TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:52:02
epoch 0  | loss: 0.7199  | val_0_rmse: 0.75034 | val_1_rmse: 0.7403  |  0:00:05s
epoch 1  | loss: 0.47423 | val_0_rmse: 0.66114 | val_1_rmse: 0.66584 |  0:00:06s
epoch 2  | loss: 0.4368  | val_0_rmse: 0.63097 | val_1_rmse: 0.61681 |  0:00:07s
epoch 3  | loss: 0.40915 | val_0_rmse: 0.61216 | val_1_rmse: 0.60338 |  0:00:09s
epoch 4  | loss: 0.38826 | val_0_rmse: 0.59855 | val_1_rmse: 0.59041 |  0:00:10s
epoch 5  | loss: 0.38178 | val_0_rmse: 0.59632 | val_1_rmse: 0.59208 |  0:00:12s
epoch 6  | loss: 0.36925 | val_0_rmse: 0.60225 | val_1_rmse: 0.59309 |  0:00:13s
epoch 7  | loss: 0.36559 | val_0_rmse: 0.59712 | val_1_rmse: 0.59207 |  0:00:15s
epoch 8  | loss: 0.35709 | val_0_rmse: 0.60156 | val_1_rmse: 0.59182 |  0:00:16s
epoch 9  | loss: 0.36382 | val_0_rmse: 0.57815 | val_1_rmse: 0.57461 |  0:00:18s
epoch 10 | loss: 0.34339 | val_0_rmse: 0.60346 | val_1_rmse: 0.59948 |  0:00:19s
epoch 11 | loss: 0.35057 | val_0_rmse: 0.57565 | val_1_rmse: 0.56758 |  0:00:20s
epoch 12 | loss: 0.3512  | val_0_rmse: 0.58381 | val_1_rmse: 0.57471 |  0:00:22s
epoch 13 | loss: 0.35389 | val_0_rmse: 0.57146 | val_1_rmse: 0.57177 |  0:00:23s
epoch 14 | loss: 0.34527 | val_0_rmse: 0.5668  | val_1_rmse: 0.56027 |  0:00:25s
epoch 15 | loss: 0.34291 | val_0_rmse: 0.57891 | val_1_rmse: 0.56496 |  0:00:26s
epoch 16 | loss: 0.33036 | val_0_rmse: 0.55497 | val_1_rmse: 0.5441  |  0:00:28s
epoch 17 | loss: 0.33102 | val_0_rmse: 0.55991 | val_1_rmse: 0.55191 |  0:00:29s
epoch 18 | loss: 0.33155 | val_0_rmse: 0.56211 | val_1_rmse: 0.55857 |  0:00:30s
epoch 19 | loss: 0.32922 | val_0_rmse: 0.56536 | val_1_rmse: 0.55397 |  0:00:32s
epoch 20 | loss: 0.3286  | val_0_rmse: 0.54918 | val_1_rmse: 0.54124 |  0:00:33s
epoch 21 | loss: 0.32674 | val_0_rmse: 0.54826 | val_1_rmse: 0.54354 |  0:00:35s
epoch 22 | loss: 0.33984 | val_0_rmse: 0.54791 | val_1_rmse: 0.54314 |  0:00:36s
epoch 23 | loss: 0.33182 | val_0_rmse: 0.57021 | val_1_rmse: 0.56474 |  0:00:38s
epoch 24 | loss: 0.32475 | val_0_rmse: 0.54687 | val_1_rmse: 0.5408  |  0:00:39s
epoch 25 | loss: 0.32493 | val_0_rmse: 0.54155 | val_1_rmse: 0.53634 |  0:00:41s
epoch 26 | loss: 0.31938 | val_0_rmse: 0.57605 | val_1_rmse: 0.56986 |  0:00:42s
epoch 27 | loss: 0.32306 | val_0_rmse: 0.58262 | val_1_rmse: 0.58029 |  0:00:43s
epoch 28 | loss: 0.32008 | val_0_rmse: 0.56709 | val_1_rmse: 0.56236 |  0:00:45s
epoch 29 | loss: 0.32671 | val_0_rmse: 0.54687 | val_1_rmse: 0.54521 |  0:00:46s
epoch 30 | loss: 0.31782 | val_0_rmse: 0.55196 | val_1_rmse: 0.55423 |  0:00:48s
epoch 31 | loss: 0.32466 | val_0_rmse: 0.57853 | val_1_rmse: 0.57131 |  0:00:49s
epoch 32 | loss: 0.3269  | val_0_rmse: 0.64928 | val_1_rmse: 0.62815 |  0:00:51s
epoch 33 | loss: 0.35789 | val_0_rmse: 0.55862 | val_1_rmse: 0.54861 |  0:00:52s
epoch 34 | loss: 0.33451 | val_0_rmse: 0.55419 | val_1_rmse: 0.54731 |  0:00:53s
epoch 35 | loss: 0.33917 | val_0_rmse: 0.55939 | val_1_rmse: 0.56083 |  0:00:55s
epoch 36 | loss: 0.32914 | val_0_rmse: 0.54977 | val_1_rmse: 0.54177 |  0:00:56s
epoch 37 | loss: 0.32115 | val_0_rmse: 0.53576 | val_1_rmse: 0.52322 |  0:00:58s
epoch 38 | loss: 0.32441 | val_0_rmse: 0.54474 | val_1_rmse: 0.53918 |  0:00:59s
epoch 39 | loss: 0.32038 | val_0_rmse: 0.56537 | val_1_rmse: 0.56028 |  0:01:01s
epoch 40 | loss: 0.34015 | val_0_rmse: 0.56297 | val_1_rmse: 0.55571 |  0:01:02s
epoch 41 | loss: 0.33089 | val_0_rmse: 0.56715 | val_1_rmse: 0.56326 |  0:01:03s
epoch 42 | loss: 0.33309 | val_0_rmse: 0.56128 | val_1_rmse: 0.54897 |  0:01:05s
epoch 43 | loss: 0.3279  | val_0_rmse: 0.56019 | val_1_rmse: 0.55187 |  0:01:06s
epoch 44 | loss: 0.32065 | val_0_rmse: 0.54695 | val_1_rmse: 0.53744 |  0:01:08s
epoch 45 | loss: 0.31155 | val_0_rmse: 0.53802 | val_1_rmse: 0.5319  |  0:01:09s
epoch 46 | loss: 0.31149 | val_0_rmse: 0.54591 | val_1_rmse: 0.54439 |  0:01:11s
epoch 47 | loss: 0.32012 | val_0_rmse: 0.59713 | val_1_rmse: 0.59108 |  0:01:12s
epoch 48 | loss: 0.32568 | val_0_rmse: 0.5658  | val_1_rmse: 0.55768 |  0:01:13s
epoch 49 | loss: 0.31218 | val_0_rmse: 0.53375 | val_1_rmse: 0.53945 |  0:01:15s
epoch 50 | loss: 0.31066 | val_0_rmse: 0.54061 | val_1_rmse: 0.53512 |  0:01:16s
epoch 51 | loss: 0.31704 | val_0_rmse: 0.56565 | val_1_rmse: 0.55828 |  0:01:18s
epoch 52 | loss: 0.3166  | val_0_rmse: 0.64393 | val_1_rmse: 0.64712 |  0:01:19s
epoch 53 | loss: 0.31925 | val_0_rmse: 0.53345 | val_1_rmse: 0.53096 |  0:01:21s
epoch 54 | loss: 0.31498 | val_0_rmse: 0.55227 | val_1_rmse: 0.55275 |  0:01:22s
epoch 55 | loss: 0.31165 | val_0_rmse: 0.52534 | val_1_rmse: 0.51925 |  0:01:23s
epoch 56 | loss: 0.3112  | val_0_rmse: 0.53999 | val_1_rmse: 0.53795 |  0:01:25s
epoch 57 | loss: 0.30938 | val_0_rmse: 0.53846 | val_1_rmse: 0.53975 |  0:01:26s
epoch 58 | loss: 0.31144 | val_0_rmse: 0.53741 | val_1_rmse: 0.53451 |  0:01:28s
epoch 59 | loss: 0.30672 | val_0_rmse: 0.56248 | val_1_rmse: 0.56087 |  0:01:29s
epoch 60 | loss: 0.29856 | val_0_rmse: 0.61361 | val_1_rmse: 0.61854 |  0:01:31s
epoch 61 | loss: 0.30489 | val_0_rmse: 0.56495 | val_1_rmse: 0.56175 |  0:01:32s
epoch 62 | loss: 0.30235 | val_0_rmse: 0.52309 | val_1_rmse: 0.52    |  0:01:33s
epoch 63 | loss: 0.30374 | val_0_rmse: 0.53183 | val_1_rmse: 0.53352 |  0:01:35s
epoch 64 | loss: 0.30686 | val_0_rmse: 0.57082 | val_1_rmse: 0.56813 |  0:01:36s
epoch 65 | loss: 0.31108 | val_0_rmse: 0.55146 | val_1_rmse: 0.54482 |  0:01:38s
epoch 66 | loss: 0.30201 | val_0_rmse: 0.53093 | val_1_rmse: 0.53501 |  0:01:39s
epoch 67 | loss: 0.30278 | val_0_rmse: 0.58256 | val_1_rmse: 0.58799 |  0:01:41s
epoch 68 | loss: 0.30374 | val_0_rmse: 0.61206 | val_1_rmse: 0.61167 |  0:01:42s
epoch 69 | loss: 0.30597 | val_0_rmse: 0.55769 | val_1_rmse: 0.55364 |  0:01:43s
epoch 70 | loss: 0.29959 | val_0_rmse: 0.54257 | val_1_rmse: 0.53084 |  0:01:45s
epoch 71 | loss: 0.29934 | val_0_rmse: 0.53293 | val_1_rmse: 0.53099 |  0:01:46s
epoch 72 | loss: 0.29572 | val_0_rmse: 0.53729 | val_1_rmse: 0.53014 |  0:01:48s
epoch 73 | loss: 0.29799 | val_0_rmse: 0.55043 | val_1_rmse: 0.55133 |  0:01:49s
epoch 74 | loss: 0.29685 | val_0_rmse: 0.52494 | val_1_rmse: 0.51994 |  0:01:51s
epoch 75 | loss: 0.29974 | val_0_rmse: 0.53612 | val_1_rmse: 0.54278 |  0:01:52s
epoch 76 | loss: 0.30322 | val_0_rmse: 0.52076 | val_1_rmse: 0.52082 |  0:01:53s
epoch 77 | loss: 0.29474 | val_0_rmse: 0.52303 | val_1_rmse: 0.5251  |  0:01:55s
epoch 78 | loss: 0.30318 | val_0_rmse: 0.54389 | val_1_rmse: 0.55055 |  0:01:56s
epoch 79 | loss: 0.29942 | val_0_rmse: 0.53882 | val_1_rmse: 0.53071 |  0:01:58s
epoch 80 | loss: 0.30052 | val_0_rmse: 0.54485 | val_1_rmse: 0.54745 |  0:01:59s
epoch 81 | loss: 0.29891 | val_0_rmse: 0.53485 | val_1_rmse: 0.52571 |  0:02:01s
epoch 82 | loss: 0.303   | val_0_rmse: 0.5356  | val_1_rmse: 0.53238 |  0:02:02s
epoch 83 | loss: 0.2936  | val_0_rmse: 0.52303 | val_1_rmse: 0.52582 |  0:02:03s
epoch 84 | loss: 0.28902 | val_0_rmse: 0.6251  | val_1_rmse: 0.63351 |  0:02:05s
epoch 85 | loss: 0.29671 | val_0_rmse: 0.64315 | val_1_rmse: 0.63453 |  0:02:06s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.51925
Best weights from best epoch are automatically used!
ended training at: 04:54:09
Feature importance:
[('Area', 0.24727252523876106), ('Baths', 0.0), ('Beds', 0.024675024976340772), ('Latitude', 0.3167023330200237), ('Longitude', 0.25145764487227823), ('Month', 0.003242450448910208), ('Year', 0.156650021443686)]
Mean squared error is of 6387734853.720607
Mean absolute error:54935.50785987144
MAPE:0.17670960890274764
R2 score:0.7139755461492576
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:54:10
epoch 0  | loss: 0.78012 | val_0_rmse: 0.76048 | val_1_rmse: 0.76529 |  0:00:01s
epoch 1  | loss: 0.45474 | val_0_rmse: 0.65072 | val_1_rmse: 0.64678 |  0:00:02s
epoch 2  | loss: 0.43402 | val_0_rmse: 0.62015 | val_1_rmse: 0.61448 |  0:00:04s
epoch 3  | loss: 0.40663 | val_0_rmse: 0.61391 | val_1_rmse: 0.61015 |  0:00:05s
epoch 4  | loss: 0.38892 | val_0_rmse: 0.59709 | val_1_rmse: 0.59026 |  0:00:07s
epoch 5  | loss: 0.37475 | val_0_rmse: 0.59938 | val_1_rmse: 0.59923 |  0:00:08s
epoch 6  | loss: 0.37221 | val_0_rmse: 0.58584 | val_1_rmse: 0.58045 |  0:00:10s
epoch 7  | loss: 0.36676 | val_0_rmse: 0.58979 | val_1_rmse: 0.58598 |  0:00:11s
epoch 8  | loss: 0.37297 | val_0_rmse: 0.58624 | val_1_rmse: 0.5814  |  0:00:13s
epoch 9  | loss: 0.36205 | val_0_rmse: 0.58606 | val_1_rmse: 0.58162 |  0:00:14s
epoch 10 | loss: 0.37042 | val_0_rmse: 0.61898 | val_1_rmse: 0.61469 |  0:00:15s
epoch 11 | loss: 0.36499 | val_0_rmse: 0.57865 | val_1_rmse: 0.57604 |  0:00:17s
epoch 12 | loss: 0.35277 | val_0_rmse: 0.58176 | val_1_rmse: 0.57396 |  0:00:18s
epoch 13 | loss: 0.36548 | val_0_rmse: 0.62125 | val_1_rmse: 0.61347 |  0:00:20s
epoch 14 | loss: 0.36449 | val_0_rmse: 0.58045 | val_1_rmse: 0.57522 |  0:00:21s
epoch 15 | loss: 0.35733 | val_0_rmse: 0.59941 | val_1_rmse: 0.59391 |  0:00:23s
epoch 16 | loss: 0.35104 | val_0_rmse: 0.61034 | val_1_rmse: 0.60375 |  0:00:24s
epoch 17 | loss: 0.35725 | val_0_rmse: 0.58427 | val_1_rmse: 0.57605 |  0:00:26s
epoch 18 | loss: 0.34318 | val_0_rmse: 0.55889 | val_1_rmse: 0.55125 |  0:00:27s
epoch 19 | loss: 0.33757 | val_0_rmse: 0.55666 | val_1_rmse: 0.554   |  0:00:29s
epoch 20 | loss: 0.33728 | val_0_rmse: 0.57977 | val_1_rmse: 0.57456 |  0:00:30s
epoch 21 | loss: 0.34121 | val_0_rmse: 0.5636  | val_1_rmse: 0.55951 |  0:00:31s
epoch 22 | loss: 0.34812 | val_0_rmse: 0.58516 | val_1_rmse: 0.58192 |  0:00:33s
epoch 23 | loss: 0.33497 | val_0_rmse: 0.55119 | val_1_rmse: 0.54878 |  0:00:34s
epoch 24 | loss: 0.33638 | val_0_rmse: 0.55518 | val_1_rmse: 0.54929 |  0:00:36s
epoch 25 | loss: 0.32822 | val_0_rmse: 0.56299 | val_1_rmse: 0.55674 |  0:00:37s
epoch 26 | loss: 0.33763 | val_0_rmse: 0.57629 | val_1_rmse: 0.57213 |  0:00:39s
epoch 27 | loss: 0.32883 | val_0_rmse: 0.56028 | val_1_rmse: 0.5601  |  0:00:40s
epoch 28 | loss: 0.33468 | val_0_rmse: 0.55238 | val_1_rmse: 0.5529  |  0:00:42s
epoch 29 | loss: 0.32739 | val_0_rmse: 0.55135 | val_1_rmse: 0.55156 |  0:00:43s
epoch 30 | loss: 0.32444 | val_0_rmse: 0.54215 | val_1_rmse: 0.5395  |  0:00:44s
epoch 31 | loss: 0.32058 | val_0_rmse: 0.5469  | val_1_rmse: 0.5458  |  0:00:46s
epoch 32 | loss: 0.32676 | val_0_rmse: 0.64721 | val_1_rmse: 0.64939 |  0:00:47s
epoch 33 | loss: 0.34052 | val_0_rmse: 0.5873  | val_1_rmse: 0.58825 |  0:00:49s
epoch 34 | loss: 0.33057 | val_0_rmse: 0.56205 | val_1_rmse: 0.55973 |  0:00:50s
epoch 35 | loss: 0.34    | val_0_rmse: 0.54884 | val_1_rmse: 0.54583 |  0:00:52s
epoch 36 | loss: 0.32146 | val_0_rmse: 0.55437 | val_1_rmse: 0.55286 |  0:00:53s
epoch 37 | loss: 0.32668 | val_0_rmse: 0.57892 | val_1_rmse: 0.57681 |  0:00:55s
epoch 38 | loss: 0.33016 | val_0_rmse: 0.54965 | val_1_rmse: 0.54684 |  0:00:56s
epoch 39 | loss: 0.31684 | val_0_rmse: 0.56232 | val_1_rmse: 0.56508 |  0:00:57s
epoch 40 | loss: 0.32208 | val_0_rmse: 0.5342  | val_1_rmse: 0.53239 |  0:00:59s
epoch 41 | loss: 0.31928 | val_0_rmse: 0.54888 | val_1_rmse: 0.54496 |  0:01:00s
epoch 42 | loss: 0.32406 | val_0_rmse: 0.55541 | val_1_rmse: 0.5503  |  0:01:02s
epoch 43 | loss: 0.33095 | val_0_rmse: 0.54361 | val_1_rmse: 0.54215 |  0:01:03s
epoch 44 | loss: 0.32031 | val_0_rmse: 0.54889 | val_1_rmse: 0.54855 |  0:01:05s
epoch 45 | loss: 0.32014 | val_0_rmse: 0.54363 | val_1_rmse: 0.53796 |  0:01:06s
epoch 46 | loss: 0.31221 | val_0_rmse: 0.56352 | val_1_rmse: 0.56337 |  0:01:08s
epoch 47 | loss: 0.31863 | val_0_rmse: 0.53363 | val_1_rmse: 0.53095 |  0:01:09s
epoch 48 | loss: 0.31313 | val_0_rmse: 0.54205 | val_1_rmse: 0.53975 |  0:01:10s
epoch 49 | loss: 0.31468 | val_0_rmse: 0.5511  | val_1_rmse: 0.54851 |  0:01:12s
epoch 50 | loss: 0.31975 | val_0_rmse: 0.55623 | val_1_rmse: 0.55591 |  0:01:13s
epoch 51 | loss: 0.31928 | val_0_rmse: 0.55309 | val_1_rmse: 0.55437 |  0:01:15s
epoch 52 | loss: 0.31201 | val_0_rmse: 0.54306 | val_1_rmse: 0.53975 |  0:01:16s
epoch 53 | loss: 0.31559 | val_0_rmse: 0.53538 | val_1_rmse: 0.53626 |  0:01:18s
epoch 54 | loss: 0.31615 | val_0_rmse: 0.54083 | val_1_rmse: 0.54064 |  0:01:19s
epoch 55 | loss: 0.31619 | val_0_rmse: 0.53203 | val_1_rmse: 0.53137 |  0:01:21s
epoch 56 | loss: 0.30979 | val_0_rmse: 0.54062 | val_1_rmse: 0.54321 |  0:01:22s
epoch 57 | loss: 0.3119  | val_0_rmse: 0.54782 | val_1_rmse: 0.55311 |  0:01:23s
epoch 58 | loss: 0.30721 | val_0_rmse: 0.5339  | val_1_rmse: 0.5334  |  0:01:25s
epoch 59 | loss: 0.30901 | val_0_rmse: 0.56484 | val_1_rmse: 0.56831 |  0:01:26s
epoch 60 | loss: 0.31056 | val_0_rmse: 0.55622 | val_1_rmse: 0.55501 |  0:01:28s
epoch 61 | loss: 0.32443 | val_0_rmse: 0.54303 | val_1_rmse: 0.54251 |  0:01:29s
epoch 62 | loss: 0.32034 | val_0_rmse: 0.54798 | val_1_rmse: 0.55088 |  0:01:31s
epoch 63 | loss: 0.31681 | val_0_rmse: 0.54128 | val_1_rmse: 0.54155 |  0:01:32s
epoch 64 | loss: 0.31821 | val_0_rmse: 0.56917 | val_1_rmse: 0.57394 |  0:01:34s
epoch 65 | loss: 0.31993 | val_0_rmse: 0.5332  | val_1_rmse: 0.53441 |  0:01:35s
epoch 66 | loss: 0.31092 | val_0_rmse: 0.53321 | val_1_rmse: 0.53186 |  0:01:36s
epoch 67 | loss: 0.31091 | val_0_rmse: 0.53189 | val_1_rmse: 0.53274 |  0:01:38s
epoch 68 | loss: 0.30078 | val_0_rmse: 0.52874 | val_1_rmse: 0.53028 |  0:01:39s
epoch 69 | loss: 0.30209 | val_0_rmse: 0.53209 | val_1_rmse: 0.53022 |  0:01:41s
epoch 70 | loss: 0.30366 | val_0_rmse: 0.53204 | val_1_rmse: 0.53267 |  0:01:42s
epoch 71 | loss: 0.31387 | val_0_rmse: 0.53144 | val_1_rmse: 0.53053 |  0:01:44s
epoch 72 | loss: 0.30724 | val_0_rmse: 0.53792 | val_1_rmse: 0.53904 |  0:01:45s
epoch 73 | loss: 0.3125  | val_0_rmse: 0.54166 | val_1_rmse: 0.54833 |  0:01:47s
epoch 74 | loss: 0.30861 | val_0_rmse: 0.53159 | val_1_rmse: 0.53319 |  0:01:48s
epoch 75 | loss: 0.31125 | val_0_rmse: 0.56571 | val_1_rmse: 0.56389 |  0:01:49s
epoch 76 | loss: 0.31844 | val_0_rmse: 0.52928 | val_1_rmse: 0.53014 |  0:01:51s
epoch 77 | loss: 0.30639 | val_0_rmse: 0.52375 | val_1_rmse: 0.52617 |  0:01:52s
epoch 78 | loss: 0.30268 | val_0_rmse: 0.5325  | val_1_rmse: 0.5396  |  0:01:54s
epoch 79 | loss: 0.31236 | val_0_rmse: 0.52165 | val_1_rmse: 0.52314 |  0:01:55s
epoch 80 | loss: 0.30716 | val_0_rmse: 0.54223 | val_1_rmse: 0.54249 |  0:01:57s
epoch 81 | loss: 0.30467 | val_0_rmse: 0.53964 | val_1_rmse: 0.537   |  0:01:58s
epoch 82 | loss: 0.30788 | val_0_rmse: 0.60138 | val_1_rmse: 0.6028  |  0:02:00s
epoch 83 | loss: 0.30994 | val_0_rmse: 0.53756 | val_1_rmse: 0.53551 |  0:02:01s
epoch 84 | loss: 0.30143 | val_0_rmse: 0.53215 | val_1_rmse: 0.53217 |  0:02:02s
epoch 85 | loss: 0.30712 | val_0_rmse: 0.52545 | val_1_rmse: 0.52454 |  0:02:04s
epoch 86 | loss: 0.30382 | val_0_rmse: 0.56353 | val_1_rmse: 0.57046 |  0:02:05s
epoch 87 | loss: 0.30882 | val_0_rmse: 0.54621 | val_1_rmse: 0.54579 |  0:02:07s
epoch 88 | loss: 0.30556 | val_0_rmse: 0.53621 | val_1_rmse: 0.53644 |  0:02:08s
epoch 89 | loss: 0.30012 | val_0_rmse: 0.54817 | val_1_rmse: 0.55079 |  0:02:10s
epoch 90 | loss: 0.30914 | val_0_rmse: 0.57036 | val_1_rmse: 0.56931 |  0:02:11s
epoch 91 | loss: 0.29914 | val_0_rmse: 0.51872 | val_1_rmse: 0.5224  |  0:02:13s
epoch 92 | loss: 0.29326 | val_0_rmse: 0.53728 | val_1_rmse: 0.53735 |  0:02:14s
epoch 93 | loss: 0.29873 | val_0_rmse: 0.55288 | val_1_rmse: 0.548   |  0:02:15s
epoch 94 | loss: 0.31136 | val_0_rmse: 0.55453 | val_1_rmse: 0.55972 |  0:02:17s
epoch 95 | loss: 0.29668 | val_0_rmse: 0.51848 | val_1_rmse: 0.52223 |  0:02:18s
epoch 96 | loss: 0.29318 | val_0_rmse: 0.5258  | val_1_rmse: 0.52539 |  0:02:20s
epoch 97 | loss: 0.30056 | val_0_rmse: 0.52621 | val_1_rmse: 0.52976 |  0:02:21s
epoch 98 | loss: 0.30453 | val_0_rmse: 0.52149 | val_1_rmse: 0.52475 |  0:02:23s
epoch 99 | loss: 0.29605 | val_0_rmse: 0.52385 | val_1_rmse: 0.53117 |  0:02:24s
epoch 100| loss: 0.29205 | val_0_rmse: 0.53061 | val_1_rmse: 0.53221 |  0:02:26s
epoch 101| loss: 0.29071 | val_0_rmse: 0.51906 | val_1_rmse: 0.52426 |  0:02:27s
epoch 102| loss: 0.29049 | val_0_rmse: 0.52857 | val_1_rmse: 0.53437 |  0:02:28s
epoch 103| loss: 0.29139 | val_0_rmse: 0.5242  | val_1_rmse: 0.52932 |  0:02:30s
epoch 104| loss: 0.29433 | val_0_rmse: 0.52475 | val_1_rmse: 0.53153 |  0:02:31s
epoch 105| loss: 0.2937  | val_0_rmse: 0.52073 | val_1_rmse: 0.52869 |  0:02:33s
epoch 106| loss: 0.29405 | val_0_rmse: 0.51158 | val_1_rmse: 0.52067 |  0:02:34s
epoch 107| loss: 0.29175 | val_0_rmse: 0.54447 | val_1_rmse: 0.55221 |  0:02:36s
epoch 108| loss: 0.29685 | val_0_rmse: 0.5394  | val_1_rmse: 0.55012 |  0:02:37s
epoch 109| loss: 0.29826 | val_0_rmse: 0.53719 | val_1_rmse: 0.53731 |  0:02:39s
epoch 110| loss: 0.2996  | val_0_rmse: 0.54304 | val_1_rmse: 0.54193 |  0:02:40s
epoch 111| loss: 0.30476 | val_0_rmse: 0.57525 | val_1_rmse: 0.57598 |  0:02:41s
epoch 112| loss: 0.29102 | val_0_rmse: 0.5249  | val_1_rmse: 0.52758 |  0:02:43s
epoch 113| loss: 0.29513 | val_0_rmse: 0.51562 | val_1_rmse: 0.52072 |  0:02:44s
epoch 114| loss: 0.28735 | val_0_rmse: 0.52958 | val_1_rmse: 0.53173 |  0:02:46s
epoch 115| loss: 0.29406 | val_0_rmse: 0.53069 | val_1_rmse: 0.53585 |  0:02:47s
epoch 116| loss: 0.28765 | val_0_rmse: 0.51543 | val_1_rmse: 0.52578 |  0:02:49s
epoch 117| loss: 0.28495 | val_0_rmse: 0.50909 | val_1_rmse: 0.51644 |  0:02:50s
epoch 118| loss: 0.29153 | val_0_rmse: 0.51652 | val_1_rmse: 0.52507 |  0:02:52s
epoch 119| loss: 0.29037 | val_0_rmse: 0.51136 | val_1_rmse: 0.52092 |  0:02:53s
epoch 120| loss: 0.28243 | val_0_rmse: 0.50936 | val_1_rmse: 0.51866 |  0:02:54s
epoch 121| loss: 0.28886 | val_0_rmse: 0.52738 | val_1_rmse: 0.53257 |  0:02:56s
epoch 122| loss: 0.28273 | val_0_rmse: 0.51162 | val_1_rmse: 0.52288 |  0:02:57s
epoch 123| loss: 0.29275 | val_0_rmse: 0.51977 | val_1_rmse: 0.53023 |  0:02:59s
epoch 124| loss: 0.2893  | val_0_rmse: 0.51282 | val_1_rmse: 0.51915 |  0:03:00s
epoch 125| loss: 0.28653 | val_0_rmse: 0.53356 | val_1_rmse: 0.54546 |  0:03:02s
epoch 126| loss: 0.29024 | val_0_rmse: 0.50599 | val_1_rmse: 0.51656 |  0:03:03s
epoch 127| loss: 0.28364 | val_0_rmse: 0.52949 | val_1_rmse: 0.54388 |  0:03:05s
epoch 128| loss: 0.28285 | val_0_rmse: 0.51965 | val_1_rmse: 0.52734 |  0:03:06s
epoch 129| loss: 0.29031 | val_0_rmse: 0.51961 | val_1_rmse: 0.52933 |  0:03:07s
epoch 130| loss: 0.28521 | val_0_rmse: 0.50737 | val_1_rmse: 0.5156  |  0:03:09s
epoch 131| loss: 0.28603 | val_0_rmse: 0.51231 | val_1_rmse: 0.51813 |  0:03:10s
epoch 132| loss: 0.28604 | val_0_rmse: 0.52033 | val_1_rmse: 0.52856 |  0:03:12s
epoch 133| loss: 0.28171 | val_0_rmse: 0.51424 | val_1_rmse: 0.52205 |  0:03:13s
epoch 134| loss: 0.29539 | val_0_rmse: 0.5301  | val_1_rmse: 0.53036 |  0:03:15s
epoch 135| loss: 0.30743 | val_0_rmse: 0.51763 | val_1_rmse: 0.52361 |  0:03:16s
epoch 136| loss: 0.29252 | val_0_rmse: 0.5147  | val_1_rmse: 0.52364 |  0:03:18s
epoch 137| loss: 0.29338 | val_0_rmse: 0.52851 | val_1_rmse: 0.53704 |  0:03:19s
epoch 138| loss: 0.29173 | val_0_rmse: 0.52067 | val_1_rmse: 0.53106 |  0:03:20s
epoch 139| loss: 0.28961 | val_0_rmse: 0.50543 | val_1_rmse: 0.5164  |  0:03:22s
epoch 140| loss: 0.2828  | val_0_rmse: 0.5591  | val_1_rmse: 0.57312 |  0:03:23s
epoch 141| loss: 0.29296 | val_0_rmse: 0.52287 | val_1_rmse: 0.52923 |  0:03:25s
epoch 142| loss: 0.28756 | val_0_rmse: 0.51305 | val_1_rmse: 0.52642 |  0:03:26s
epoch 143| loss: 0.28826 | val_0_rmse: 0.53087 | val_1_rmse: 0.54001 |  0:03:28s
epoch 144| loss: 0.28611 | val_0_rmse: 0.50954 | val_1_rmse: 0.522   |  0:03:29s
epoch 145| loss: 0.28541 | val_0_rmse: 0.50872 | val_1_rmse: 0.51778 |  0:03:31s
epoch 146| loss: 0.28804 | val_0_rmse: 0.50366 | val_1_rmse: 0.5128  |  0:03:32s
epoch 147| loss: 0.27958 | val_0_rmse: 0.55234 | val_1_rmse: 0.56047 |  0:03:33s
epoch 148| loss: 0.28126 | val_0_rmse: 0.51152 | val_1_rmse: 0.52231 |  0:03:35s
epoch 149| loss: 0.2883  | val_0_rmse: 0.51339 | val_1_rmse: 0.52475 |  0:03:36s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.5128
Best weights from best epoch are automatically used!
ended training at: 04:57:48
Feature importance:
[('Area', 0.2603410586343703), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.2642442565241172), ('Longitude', 0.3259531296794924), ('Month', 0.0040339331189243), ('Year', 0.14542762204309584)]
Mean squared error is of 6169346013.807502
Mean absolute error:54566.52920167561
MAPE:0.18047385848341163
R2 score:0.7242788344842463
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:57:48
epoch 0  | loss: 0.79037 | val_0_rmse: 0.78864 | val_1_rmse: 0.79884 |  0:00:01s
epoch 1  | loss: 0.52394 | val_0_rmse: 0.69717 | val_1_rmse: 0.70071 |  0:00:02s
epoch 2  | loss: 0.4483  | val_0_rmse: 0.64581 | val_1_rmse: 0.64703 |  0:00:04s
epoch 3  | loss: 0.40163 | val_0_rmse: 0.60108 | val_1_rmse: 0.61273 |  0:00:05s
epoch 4  | loss: 0.39227 | val_0_rmse: 0.60107 | val_1_rmse: 0.61613 |  0:00:07s
epoch 5  | loss: 0.38026 | val_0_rmse: 0.61758 | val_1_rmse: 0.62751 |  0:00:08s
epoch 6  | loss: 0.38322 | val_0_rmse: 0.58245 | val_1_rmse: 0.5936  |  0:00:10s
epoch 7  | loss: 0.36108 | val_0_rmse: 0.60744 | val_1_rmse: 0.61158 |  0:00:11s
epoch 8  | loss: 0.35417 | val_0_rmse: 0.56197 | val_1_rmse: 0.57444 |  0:00:13s
epoch 9  | loss: 0.35525 | val_0_rmse: 0.58975 | val_1_rmse: 0.59814 |  0:00:14s
epoch 10 | loss: 0.36493 | val_0_rmse: 0.60212 | val_1_rmse: 0.61166 |  0:00:15s
epoch 11 | loss: 0.34889 | val_0_rmse: 0.63838 | val_1_rmse: 0.64889 |  0:00:17s
epoch 12 | loss: 0.34202 | val_0_rmse: 0.55828 | val_1_rmse: 0.56624 |  0:00:18s
epoch 13 | loss: 0.33586 | val_0_rmse: 0.56742 | val_1_rmse: 0.5731  |  0:00:20s
epoch 14 | loss: 0.33767 | val_0_rmse: 0.54822 | val_1_rmse: 0.55468 |  0:00:21s
epoch 15 | loss: 0.34598 | val_0_rmse: 0.5562  | val_1_rmse: 0.56524 |  0:00:23s
epoch 16 | loss: 0.334   | val_0_rmse: 0.55899 | val_1_rmse: 0.57032 |  0:00:24s
epoch 17 | loss: 0.33521 | val_0_rmse: 0.56458 | val_1_rmse: 0.57303 |  0:00:26s
epoch 18 | loss: 0.33696 | val_0_rmse: 0.54518 | val_1_rmse: 0.54934 |  0:00:27s
epoch 19 | loss: 0.33527 | val_0_rmse: 0.56146 | val_1_rmse: 0.56994 |  0:00:28s
epoch 20 | loss: 0.33026 | val_0_rmse: 0.57014 | val_1_rmse: 0.57446 |  0:00:30s
epoch 21 | loss: 0.3378  | val_0_rmse: 0.5879  | val_1_rmse: 0.59167 |  0:00:31s
epoch 22 | loss: 0.33844 | val_0_rmse: 0.55709 | val_1_rmse: 0.56176 |  0:00:33s
epoch 23 | loss: 0.32662 | val_0_rmse: 0.55778 | val_1_rmse: 0.563   |  0:00:34s
epoch 24 | loss: 0.33666 | val_0_rmse: 0.58548 | val_1_rmse: 0.60338 |  0:00:36s
epoch 25 | loss: 0.33486 | val_0_rmse: 0.56442 | val_1_rmse: 0.57852 |  0:00:37s
epoch 26 | loss: 0.32138 | val_0_rmse: 0.57118 | val_1_rmse: 0.58352 |  0:00:39s
epoch 27 | loss: 0.32335 | val_0_rmse: 0.55125 | val_1_rmse: 0.55672 |  0:00:40s
epoch 28 | loss: 0.34382 | val_0_rmse: 0.56453 | val_1_rmse: 0.56627 |  0:00:41s
epoch 29 | loss: 0.34308 | val_0_rmse: 0.64092 | val_1_rmse: 0.64285 |  0:00:43s
epoch 30 | loss: 0.35276 | val_0_rmse: 0.57412 | val_1_rmse: 0.58054 |  0:00:44s
epoch 31 | loss: 0.33734 | val_0_rmse: 0.56257 | val_1_rmse: 0.56613 |  0:00:46s
epoch 32 | loss: 0.33756 | val_0_rmse: 0.54875 | val_1_rmse: 0.552   |  0:00:47s
epoch 33 | loss: 0.32724 | val_0_rmse: 0.5574  | val_1_rmse: 0.56795 |  0:00:49s
epoch 34 | loss: 0.33285 | val_0_rmse: 0.54722 | val_1_rmse: 0.56189 |  0:00:50s
epoch 35 | loss: 0.32142 | val_0_rmse: 0.54079 | val_1_rmse: 0.55612 |  0:00:52s
epoch 36 | loss: 0.32649 | val_0_rmse: 0.5609  | val_1_rmse: 0.57146 |  0:00:53s
epoch 37 | loss: 0.31871 | val_0_rmse: 0.53824 | val_1_rmse: 0.54607 |  0:00:54s
epoch 38 | loss: 0.32296 | val_0_rmse: 0.53342 | val_1_rmse: 0.54139 |  0:00:56s
epoch 39 | loss: 0.31702 | val_0_rmse: 0.53792 | val_1_rmse: 0.54958 |  0:00:57s
epoch 40 | loss: 0.32256 | val_0_rmse: 0.55428 | val_1_rmse: 0.55808 |  0:00:59s
epoch 41 | loss: 0.32912 | val_0_rmse: 0.60418 | val_1_rmse: 0.61307 |  0:01:00s
epoch 42 | loss: 0.32383 | val_0_rmse: 0.53602 | val_1_rmse: 0.54161 |  0:01:02s
epoch 43 | loss: 0.32235 | val_0_rmse: 0.55677 | val_1_rmse: 0.56047 |  0:01:03s
epoch 44 | loss: 0.32489 | val_0_rmse: 0.61856 | val_1_rmse: 0.62301 |  0:01:05s
epoch 45 | loss: 0.32738 | val_0_rmse: 0.55482 | val_1_rmse: 0.55963 |  0:01:06s
epoch 46 | loss: 0.32127 | val_0_rmse: 0.53627 | val_1_rmse: 0.53984 |  0:01:07s
epoch 47 | loss: 0.31211 | val_0_rmse: 0.55467 | val_1_rmse: 0.56555 |  0:01:09s
epoch 48 | loss: 0.31947 | val_0_rmse: 0.53457 | val_1_rmse: 0.54083 |  0:01:10s
epoch 49 | loss: 0.31272 | val_0_rmse: 0.52877 | val_1_rmse: 0.53683 |  0:01:12s
epoch 50 | loss: 0.30863 | val_0_rmse: 0.53217 | val_1_rmse: 0.5376  |  0:01:13s
epoch 51 | loss: 0.30855 | val_0_rmse: 0.5356  | val_1_rmse: 0.54204 |  0:01:15s
epoch 52 | loss: 0.30759 | val_0_rmse: 0.52577 | val_1_rmse: 0.53671 |  0:01:16s
epoch 53 | loss: 0.30383 | val_0_rmse: 0.53314 | val_1_rmse: 0.54393 |  0:01:18s
epoch 54 | loss: 0.31013 | val_0_rmse: 0.54232 | val_1_rmse: 0.54967 |  0:01:19s
epoch 55 | loss: 0.29934 | val_0_rmse: 0.52792 | val_1_rmse: 0.53766 |  0:01:20s
epoch 56 | loss: 0.29845 | val_0_rmse: 0.55239 | val_1_rmse: 0.55702 |  0:01:22s
epoch 57 | loss: 0.31043 | val_0_rmse: 0.58742 | val_1_rmse: 0.60238 |  0:01:23s
epoch 58 | loss: 0.30543 | val_0_rmse: 0.52751 | val_1_rmse: 0.53602 |  0:01:25s
epoch 59 | loss: 0.29923 | val_0_rmse: 0.51226 | val_1_rmse: 0.5243  |  0:01:26s
epoch 60 | loss: 0.29906 | val_0_rmse: 0.52934 | val_1_rmse: 0.54332 |  0:01:28s
epoch 61 | loss: 0.30127 | val_0_rmse: 0.54296 | val_1_rmse: 0.54646 |  0:01:29s
epoch 62 | loss: 0.3003  | val_0_rmse: 0.58985 | val_1_rmse: 0.59641 |  0:01:31s
epoch 63 | loss: 0.29874 | val_0_rmse: 0.53609 | val_1_rmse: 0.54556 |  0:01:32s
epoch 64 | loss: 0.30228 | val_0_rmse: 0.60155 | val_1_rmse: 0.60975 |  0:01:33s
epoch 65 | loss: 0.29492 | val_0_rmse: 0.57677 | val_1_rmse: 0.58428 |  0:01:35s
epoch 66 | loss: 0.30092 | val_0_rmse: 0.57754 | val_1_rmse: 0.58792 |  0:01:36s
epoch 67 | loss: 0.30072 | val_0_rmse: 0.52022 | val_1_rmse: 0.52695 |  0:01:38s
epoch 68 | loss: 0.30151 | val_0_rmse: 0.53865 | val_1_rmse: 0.54429 |  0:01:39s
epoch 69 | loss: 0.30211 | val_0_rmse: 0.50831 | val_1_rmse: 0.52098 |  0:01:41s
epoch 70 | loss: 0.30242 | val_0_rmse: 0.52291 | val_1_rmse: 0.53235 |  0:01:42s
epoch 71 | loss: 0.29724 | val_0_rmse: 0.54876 | val_1_rmse: 0.56061 |  0:01:43s
epoch 72 | loss: 0.2933  | val_0_rmse: 0.53148 | val_1_rmse: 0.53611 |  0:01:45s
epoch 73 | loss: 0.29088 | val_0_rmse: 0.52045 | val_1_rmse: 0.53168 |  0:01:46s
epoch 74 | loss: 0.3027  | val_0_rmse: 0.53587 | val_1_rmse: 0.54868 |  0:01:48s
epoch 75 | loss: 0.29955 | val_0_rmse: 0.69772 | val_1_rmse: 0.70222 |  0:01:49s
epoch 76 | loss: 0.30328 | val_0_rmse: 0.64644 | val_1_rmse: 0.65216 |  0:01:51s
epoch 77 | loss: 0.30112 | val_0_rmse: 0.56991 | val_1_rmse: 0.57623 |  0:01:52s
epoch 78 | loss: 0.29677 | val_0_rmse: 0.54376 | val_1_rmse: 0.54927 |  0:01:54s
epoch 79 | loss: 0.30406 | val_0_rmse: 0.54128 | val_1_rmse: 0.54786 |  0:01:55s
epoch 80 | loss: 0.30485 | val_0_rmse: 0.56271 | val_1_rmse: 0.57391 |  0:01:56s
epoch 81 | loss: 0.30724 | val_0_rmse: 0.53355 | val_1_rmse: 0.54421 |  0:01:58s
epoch 82 | loss: 0.29158 | val_0_rmse: 0.53347 | val_1_rmse: 0.53746 |  0:01:59s
epoch 83 | loss: 0.29347 | val_0_rmse: 0.52812 | val_1_rmse: 0.54261 |  0:02:01s
epoch 84 | loss: 0.29611 | val_0_rmse: 0.52778 | val_1_rmse: 0.53034 |  0:02:02s
epoch 85 | loss: 0.29103 | val_0_rmse: 0.53096 | val_1_rmse: 0.53554 |  0:02:04s
epoch 86 | loss: 0.29668 | val_0_rmse: 0.56611 | val_1_rmse: 0.58014 |  0:02:05s
epoch 87 | loss: 0.30639 | val_0_rmse: 0.55039 | val_1_rmse: 0.56327 |  0:02:07s
epoch 88 | loss: 0.29749 | val_0_rmse: 0.51528 | val_1_rmse: 0.5236  |  0:02:08s
epoch 89 | loss: 0.29808 | val_0_rmse: 0.54877 | val_1_rmse: 0.55869 |  0:02:09s
epoch 90 | loss: 0.29046 | val_0_rmse: 0.51644 | val_1_rmse: 0.52258 |  0:02:11s
epoch 91 | loss: 0.28939 | val_0_rmse: 0.52241 | val_1_rmse: 0.53257 |  0:02:12s
epoch 92 | loss: 0.29015 | val_0_rmse: 0.5107  | val_1_rmse: 0.52168 |  0:02:14s
epoch 93 | loss: 0.2887  | val_0_rmse: 0.54583 | val_1_rmse: 0.55262 |  0:02:15s
epoch 94 | loss: 0.29988 | val_0_rmse: 0.52709 | val_1_rmse: 0.53779 |  0:02:17s
epoch 95 | loss: 0.29448 | val_0_rmse: 0.58785 | val_1_rmse: 0.5981  |  0:02:18s
epoch 96 | loss: 0.29092 | val_0_rmse: 0.60922 | val_1_rmse: 0.6152  |  0:02:20s
epoch 97 | loss: 0.29356 | val_0_rmse: 0.61342 | val_1_rmse: 0.61904 |  0:02:21s
epoch 98 | loss: 0.29558 | val_0_rmse: 0.5458  | val_1_rmse: 0.55079 |  0:02:22s
epoch 99 | loss: 0.30911 | val_0_rmse: 0.58973 | val_1_rmse: 0.5964  |  0:02:24s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.52098
Best weights from best epoch are automatically used!
ended training at: 05:00:13
Feature importance:
[('Area', 0.302154212080192), ('Baths', 0.0), ('Beds', 0.002379810513377069), ('Latitude', 0.2970076870914163), ('Longitude', 0.3161819743466787), ('Month', 0.002269172830484049), ('Year', 0.08000714313785187)]
Mean squared error is of 5999426163.166642
Mean absolute error:53151.341716066476
MAPE:0.17671529531841412
R2 score:0.7255938325729616
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:00:13
epoch 0  | loss: 0.78625 | val_0_rmse: 0.77513 | val_1_rmse: 0.79188 |  0:00:01s
epoch 1  | loss: 0.49462 | val_0_rmse: 0.67366 | val_1_rmse: 0.6837  |  0:00:02s
epoch 2  | loss: 0.43567 | val_0_rmse: 0.62252 | val_1_rmse: 0.63783 |  0:00:04s
epoch 3  | loss: 0.40788 | val_0_rmse: 0.63165 | val_1_rmse: 0.64566 |  0:00:05s
epoch 4  | loss: 0.3948  | val_0_rmse: 0.6113  | val_1_rmse: 0.63045 |  0:00:07s
epoch 5  | loss: 0.40391 | val_0_rmse: 0.61933 | val_1_rmse: 0.63372 |  0:00:08s
epoch 6  | loss: 0.38864 | val_0_rmse: 0.60392 | val_1_rmse: 0.61781 |  0:00:10s
epoch 7  | loss: 0.38158 | val_0_rmse: 0.61262 | val_1_rmse: 0.62696 |  0:00:11s
epoch 8  | loss: 0.37704 | val_0_rmse: 0.6037  | val_1_rmse: 0.6168  |  0:00:13s
epoch 9  | loss: 0.38522 | val_0_rmse: 0.60173 | val_1_rmse: 0.6156  |  0:00:14s
epoch 10 | loss: 0.38031 | val_0_rmse: 0.59617 | val_1_rmse: 0.60816 |  0:00:15s
epoch 11 | loss: 0.37762 | val_0_rmse: 0.59968 | val_1_rmse: 0.61053 |  0:00:17s
epoch 12 | loss: 0.36799 | val_0_rmse: 0.57222 | val_1_rmse: 0.58191 |  0:00:18s
epoch 13 | loss: 0.3633  | val_0_rmse: 0.58646 | val_1_rmse: 0.60105 |  0:00:20s
epoch 14 | loss: 0.34952 | val_0_rmse: 0.58477 | val_1_rmse: 0.60093 |  0:00:21s
epoch 15 | loss: 0.35558 | val_0_rmse: 0.56728 | val_1_rmse: 0.57895 |  0:00:23s
epoch 16 | loss: 0.35058 | val_0_rmse: 0.61663 | val_1_rmse: 0.63313 |  0:00:24s
epoch 17 | loss: 0.35292 | val_0_rmse: 0.5703  | val_1_rmse: 0.57694 |  0:00:26s
epoch 18 | loss: 0.35301 | val_0_rmse: 0.57477 | val_1_rmse: 0.58129 |  0:00:27s
epoch 19 | loss: 0.34284 | val_0_rmse: 0.5636  | val_1_rmse: 0.57596 |  0:00:28s
epoch 20 | loss: 0.34309 | val_0_rmse: 0.56304 | val_1_rmse: 0.57097 |  0:00:30s
epoch 21 | loss: 0.34305 | val_0_rmse: 0.56642 | val_1_rmse: 0.57669 |  0:00:31s
epoch 22 | loss: 0.33809 | val_0_rmse: 0.56157 | val_1_rmse: 0.57514 |  0:00:33s
epoch 23 | loss: 0.34622 | val_0_rmse: 0.56942 | val_1_rmse: 0.58378 |  0:00:34s
epoch 24 | loss: 0.33732 | val_0_rmse: 0.58348 | val_1_rmse: 0.60217 |  0:00:36s
epoch 25 | loss: 0.34547 | val_0_rmse: 0.56924 | val_1_rmse: 0.58358 |  0:00:37s
epoch 26 | loss: 0.33131 | val_0_rmse: 0.55439 | val_1_rmse: 0.56425 |  0:00:39s
epoch 27 | loss: 0.33029 | val_0_rmse: 0.56556 | val_1_rmse: 0.57606 |  0:00:40s
epoch 28 | loss: 0.33326 | val_0_rmse: 0.55793 | val_1_rmse: 0.56972 |  0:00:42s
epoch 29 | loss: 0.32899 | val_0_rmse: 0.55071 | val_1_rmse: 0.56138 |  0:00:43s
epoch 30 | loss: 0.33296 | val_0_rmse: 0.54609 | val_1_rmse: 0.55874 |  0:00:44s
epoch 31 | loss: 0.33263 | val_0_rmse: 0.57675 | val_1_rmse: 0.5827  |  0:00:46s
epoch 32 | loss: 0.32817 | val_0_rmse: 0.55121 | val_1_rmse: 0.56241 |  0:00:47s
epoch 33 | loss: 0.33868 | val_0_rmse: 0.55714 | val_1_rmse: 0.56718 |  0:00:49s
epoch 34 | loss: 0.32937 | val_0_rmse: 0.56007 | val_1_rmse: 0.56707 |  0:00:50s
epoch 35 | loss: 0.3275  | val_0_rmse: 0.56084 | val_1_rmse: 0.56788 |  0:00:52s
epoch 36 | loss: 0.32423 | val_0_rmse: 0.54937 | val_1_rmse: 0.56225 |  0:00:53s
epoch 37 | loss: 0.3267  | val_0_rmse: 0.55803 | val_1_rmse: 0.56849 |  0:00:54s
epoch 38 | loss: 0.31939 | val_0_rmse: 0.54388 | val_1_rmse: 0.55469 |  0:00:56s
epoch 39 | loss: 0.32165 | val_0_rmse: 0.54744 | val_1_rmse: 0.55419 |  0:00:57s
epoch 40 | loss: 0.33369 | val_0_rmse: 0.54298 | val_1_rmse: 0.55751 |  0:00:59s
epoch 41 | loss: 0.32289 | val_0_rmse: 0.53549 | val_1_rmse: 0.54712 |  0:01:00s
epoch 42 | loss: 0.31581 | val_0_rmse: 0.54292 | val_1_rmse: 0.54969 |  0:01:02s
epoch 43 | loss: 0.31843 | val_0_rmse: 0.53916 | val_1_rmse: 0.55388 |  0:01:03s
epoch 44 | loss: 0.318   | val_0_rmse: 0.544   | val_1_rmse: 0.55446 |  0:01:05s
epoch 45 | loss: 0.31767 | val_0_rmse: 0.53528 | val_1_rmse: 0.54734 |  0:01:06s
epoch 46 | loss: 0.32381 | val_0_rmse: 0.5464  | val_1_rmse: 0.5562  |  0:01:08s
epoch 47 | loss: 0.31658 | val_0_rmse: 0.53539 | val_1_rmse: 0.54449 |  0:01:09s
epoch 48 | loss: 0.3142  | val_0_rmse: 0.54546 | val_1_rmse: 0.55137 |  0:01:10s
epoch 49 | loss: 0.31885 | val_0_rmse: 0.54953 | val_1_rmse: 0.55757 |  0:01:12s
epoch 50 | loss: 0.31873 | val_0_rmse: 0.56625 | val_1_rmse: 0.57469 |  0:01:13s
epoch 51 | loss: 0.32046 | val_0_rmse: 0.5662  | val_1_rmse: 0.58087 |  0:01:15s
epoch 52 | loss: 0.31197 | val_0_rmse: 0.54769 | val_1_rmse: 0.55505 |  0:01:16s
epoch 53 | loss: 0.31567 | val_0_rmse: 0.54055 | val_1_rmse: 0.55207 |  0:01:18s
epoch 54 | loss: 0.32032 | val_0_rmse: 0.54764 | val_1_rmse: 0.55966 |  0:01:19s
epoch 55 | loss: 0.31627 | val_0_rmse: 0.54859 | val_1_rmse: 0.56027 |  0:01:21s
epoch 56 | loss: 0.32048 | val_0_rmse: 0.53924 | val_1_rmse: 0.5484  |  0:01:22s
epoch 57 | loss: 0.31567 | val_0_rmse: 0.54301 | val_1_rmse: 0.55303 |  0:01:23s
epoch 58 | loss: 0.31201 | val_0_rmse: 0.529   | val_1_rmse: 0.54537 |  0:01:25s
epoch 59 | loss: 0.3065  | val_0_rmse: 0.54242 | val_1_rmse: 0.55647 |  0:01:26s
epoch 60 | loss: 0.31846 | val_0_rmse: 0.53575 | val_1_rmse: 0.54803 |  0:01:28s
epoch 61 | loss: 0.32046 | val_0_rmse: 0.54901 | val_1_rmse: 0.56189 |  0:01:29s
epoch 62 | loss: 0.32423 | val_0_rmse: 0.58052 | val_1_rmse: 0.58938 |  0:01:31s
epoch 63 | loss: 0.33622 | val_0_rmse: 0.56329 | val_1_rmse: 0.57805 |  0:01:32s
epoch 64 | loss: 0.31641 | val_0_rmse: 0.53993 | val_1_rmse: 0.55097 |  0:01:33s
epoch 65 | loss: 0.31304 | val_0_rmse: 0.5399  | val_1_rmse: 0.55834 |  0:01:35s
epoch 66 | loss: 0.31446 | val_0_rmse: 0.53494 | val_1_rmse: 0.54267 |  0:01:36s
epoch 67 | loss: 0.30992 | val_0_rmse: 0.54591 | val_1_rmse: 0.55484 |  0:01:38s
epoch 68 | loss: 0.3093  | val_0_rmse: 0.5338  | val_1_rmse: 0.54363 |  0:01:39s
epoch 69 | loss: 0.30842 | val_0_rmse: 0.55744 | val_1_rmse: 0.56494 |  0:01:41s
epoch 70 | loss: 0.31526 | val_0_rmse: 0.54204 | val_1_rmse: 0.55339 |  0:01:42s
epoch 71 | loss: 0.31089 | val_0_rmse: 0.55471 | val_1_rmse: 0.56125 |  0:01:44s
epoch 72 | loss: 0.31705 | val_0_rmse: 0.54072 | val_1_rmse: 0.55253 |  0:01:45s
epoch 73 | loss: 0.31238 | val_0_rmse: 0.53799 | val_1_rmse: 0.54864 |  0:01:46s
epoch 74 | loss: 0.31065 | val_0_rmse: 0.5347  | val_1_rmse: 0.54699 |  0:01:48s
epoch 75 | loss: 0.30897 | val_0_rmse: 0.54491 | val_1_rmse: 0.56223 |  0:01:49s
epoch 76 | loss: 0.31137 | val_0_rmse: 0.53363 | val_1_rmse: 0.54374 |  0:01:51s
epoch 77 | loss: 0.3158  | val_0_rmse: 0.55492 | val_1_rmse: 0.5631  |  0:01:52s
epoch 78 | loss: 0.3196  | val_0_rmse: 0.55364 | val_1_rmse: 0.56454 |  0:01:54s
epoch 79 | loss: 0.30974 | val_0_rmse: 0.53779 | val_1_rmse: 0.54499 |  0:01:55s
epoch 80 | loss: 0.30662 | val_0_rmse: 0.54313 | val_1_rmse: 0.55413 |  0:01:56s
epoch 81 | loss: 0.31196 | val_0_rmse: 0.55346 | val_1_rmse: 0.56943 |  0:01:58s
epoch 82 | loss: 0.3095  | val_0_rmse: 0.53409 | val_1_rmse: 0.54433 |  0:01:59s
epoch 83 | loss: 0.31424 | val_0_rmse: 0.55683 | val_1_rmse: 0.56649 |  0:02:01s
epoch 84 | loss: 0.30742 | val_0_rmse: 0.54914 | val_1_rmse: 0.56281 |  0:02:02s
epoch 85 | loss: 0.30878 | val_0_rmse: 0.5401  | val_1_rmse: 0.55019 |  0:02:04s
epoch 86 | loss: 0.30097 | val_0_rmse: 0.52456 | val_1_rmse: 0.53755 |  0:02:05s
epoch 87 | loss: 0.30631 | val_0_rmse: 0.53867 | val_1_rmse: 0.55343 |  0:02:07s
epoch 88 | loss: 0.29992 | val_0_rmse: 0.53454 | val_1_rmse: 0.54545 |  0:02:08s
epoch 89 | loss: 0.29903 | val_0_rmse: 0.54095 | val_1_rmse: 0.55495 |  0:02:09s
epoch 90 | loss: 0.30196 | val_0_rmse: 0.52429 | val_1_rmse: 0.539   |  0:02:11s
epoch 91 | loss: 0.31016 | val_0_rmse: 0.52593 | val_1_rmse: 0.53687 |  0:02:12s
epoch 92 | loss: 0.30053 | val_0_rmse: 0.5328  | val_1_rmse: 0.54596 |  0:02:14s
epoch 93 | loss: 0.3094  | val_0_rmse: 0.53117 | val_1_rmse: 0.54479 |  0:02:15s
epoch 94 | loss: 0.30139 | val_0_rmse: 0.52313 | val_1_rmse: 0.53963 |  0:02:17s
epoch 95 | loss: 0.29302 | val_0_rmse: 0.52627 | val_1_rmse: 0.54157 |  0:02:18s
epoch 96 | loss: 0.30558 | val_0_rmse: 0.54177 | val_1_rmse: 0.55438 |  0:02:20s
epoch 97 | loss: 0.30819 | val_0_rmse: 0.54919 | val_1_rmse: 0.55858 |  0:02:21s
epoch 98 | loss: 0.30001 | val_0_rmse: 0.56191 | val_1_rmse: 0.57021 |  0:02:22s
epoch 99 | loss: 0.30216 | val_0_rmse: 0.53926 | val_1_rmse: 0.55836 |  0:02:24s
epoch 100| loss: 0.30157 | val_0_rmse: 0.53029 | val_1_rmse: 0.54512 |  0:02:25s
epoch 101| loss: 0.30413 | val_0_rmse: 0.53071 | val_1_rmse: 0.54279 |  0:02:27s
epoch 102| loss: 0.30833 | val_0_rmse: 0.53157 | val_1_rmse: 0.54373 |  0:02:28s
epoch 103| loss: 0.30307 | val_0_rmse: 0.52676 | val_1_rmse: 0.53452 |  0:02:30s
epoch 104| loss: 0.30153 | val_0_rmse: 0.52318 | val_1_rmse: 0.53277 |  0:02:31s
epoch 105| loss: 0.30037 | val_0_rmse: 0.53156 | val_1_rmse: 0.5375  |  0:02:33s
epoch 106| loss: 0.30718 | val_0_rmse: 0.53442 | val_1_rmse: 0.54101 |  0:02:34s
epoch 107| loss: 0.30604 | val_0_rmse: 0.52905 | val_1_rmse: 0.54607 |  0:02:35s
epoch 108| loss: 0.31155 | val_0_rmse: 0.53435 | val_1_rmse: 0.54148 |  0:02:37s
epoch 109| loss: 0.29793 | val_0_rmse: 0.52412 | val_1_rmse: 0.53571 |  0:02:38s
epoch 110| loss: 0.29647 | val_0_rmse: 0.5288  | val_1_rmse: 0.53962 |  0:02:40s
epoch 111| loss: 0.30159 | val_0_rmse: 0.53895 | val_1_rmse: 0.54891 |  0:02:41s
epoch 112| loss: 0.30413 | val_0_rmse: 0.55574 | val_1_rmse: 0.56482 |  0:02:43s
epoch 113| loss: 0.30732 | val_0_rmse: 0.52727 | val_1_rmse: 0.54199 |  0:02:44s
epoch 114| loss: 0.29741 | val_0_rmse: 0.5218  | val_1_rmse: 0.53644 |  0:02:45s
epoch 115| loss: 0.2982  | val_0_rmse: 0.5335  | val_1_rmse: 0.54339 |  0:02:47s
epoch 116| loss: 0.29827 | val_0_rmse: 0.52928 | val_1_rmse: 0.53992 |  0:02:48s
epoch 117| loss: 0.2972  | val_0_rmse: 0.53187 | val_1_rmse: 0.54293 |  0:02:50s
epoch 118| loss: 0.301   | val_0_rmse: 0.55085 | val_1_rmse: 0.56618 |  0:02:51s
epoch 119| loss: 0.30184 | val_0_rmse: 0.53287 | val_1_rmse: 0.54757 |  0:02:53s
epoch 120| loss: 0.30902 | val_0_rmse: 0.5336  | val_1_rmse: 0.54678 |  0:02:54s
epoch 121| loss: 0.29583 | val_0_rmse: 0.51935 | val_1_rmse: 0.5299  |  0:02:56s
epoch 122| loss: 0.29459 | val_0_rmse: 0.52155 | val_1_rmse: 0.53522 |  0:02:57s
epoch 123| loss: 0.30025 | val_0_rmse: 0.5211  | val_1_rmse: 0.53285 |  0:02:58s
epoch 124| loss: 0.29686 | val_0_rmse: 0.52242 | val_1_rmse: 0.53744 |  0:03:00s
epoch 125| loss: 0.29443 | val_0_rmse: 0.52971 | val_1_rmse: 0.54543 |  0:03:01s
epoch 126| loss: 0.295   | val_0_rmse: 0.52309 | val_1_rmse: 0.5414  |  0:03:03s
epoch 127| loss: 0.29324 | val_0_rmse: 0.51654 | val_1_rmse: 0.5301  |  0:03:04s
epoch 128| loss: 0.29226 | val_0_rmse: 0.51273 | val_1_rmse: 0.5286  |  0:03:06s
epoch 129| loss: 0.29343 | val_0_rmse: 0.53753 | val_1_rmse: 0.55226 |  0:03:07s
epoch 130| loss: 0.29598 | val_0_rmse: 0.52051 | val_1_rmse: 0.53091 |  0:03:08s
epoch 131| loss: 0.29482 | val_0_rmse: 0.51514 | val_1_rmse: 0.53559 |  0:03:10s
epoch 132| loss: 0.29036 | val_0_rmse: 0.51585 | val_1_rmse: 0.53506 |  0:03:11s
epoch 133| loss: 0.29547 | val_0_rmse: 0.52034 | val_1_rmse: 0.53704 |  0:03:13s
epoch 134| loss: 0.30107 | val_0_rmse: 0.52755 | val_1_rmse: 0.54709 |  0:03:14s
epoch 135| loss: 0.3075  | val_0_rmse: 0.53138 | val_1_rmse: 0.54296 |  0:03:16s
epoch 136| loss: 0.30389 | val_0_rmse: 0.54642 | val_1_rmse: 0.55932 |  0:03:17s
epoch 137| loss: 0.29858 | val_0_rmse: 0.53095 | val_1_rmse: 0.54653 |  0:03:19s
epoch 138| loss: 0.30142 | val_0_rmse: 0.52927 | val_1_rmse: 0.54353 |  0:03:20s
epoch 139| loss: 0.30477 | val_0_rmse: 0.52834 | val_1_rmse: 0.54588 |  0:03:21s
epoch 140| loss: 0.29572 | val_0_rmse: 0.51234 | val_1_rmse: 0.52703 |  0:03:23s
epoch 141| loss: 0.29446 | val_0_rmse: 0.53295 | val_1_rmse: 0.54852 |  0:03:24s
epoch 142| loss: 0.29316 | val_0_rmse: 0.52786 | val_1_rmse: 0.5415  |  0:03:26s
epoch 143| loss: 0.28733 | val_0_rmse: 0.51116 | val_1_rmse: 0.52491 |  0:03:27s
epoch 144| loss: 0.28847 | val_0_rmse: 0.54038 | val_1_rmse: 0.5562  |  0:03:29s
epoch 145| loss: 0.29143 | val_0_rmse: 0.53937 | val_1_rmse: 0.55508 |  0:03:30s
epoch 146| loss: 0.29492 | val_0_rmse: 0.5254  | val_1_rmse: 0.54586 |  0:03:31s
epoch 147| loss: 0.29653 | val_0_rmse: 0.5236  | val_1_rmse: 0.54311 |  0:03:33s
epoch 148| loss: 0.28599 | val_0_rmse: 0.51174 | val_1_rmse: 0.5289  |  0:03:34s
epoch 149| loss: 0.2882  | val_0_rmse: 0.50962 | val_1_rmse: 0.5255  |  0:03:36s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.52491
Best weights from best epoch are automatically used!
ended training at: 05:03:50
Feature importance:
[('Area', 0.381788162124834), ('Baths', 0.03374245281334184), ('Beds', 0.04133828597832697), ('Latitude', 0.24281305083719062), ('Longitude', 0.204285692604641), ('Month', 0.0), ('Year', 0.09603235564166557)]
Mean squared error is of 6176254166.126457
Mean absolute error:53789.68754220094
MAPE:0.1778815065703072
R2 score:0.7313463252257022
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:03:50
epoch 0  | loss: 0.7341  | val_0_rmse: 0.77045 | val_1_rmse: 0.76375 |  0:00:01s
epoch 1  | loss: 0.47249 | val_0_rmse: 0.63923 | val_1_rmse: 0.63166 |  0:00:02s
epoch 2  | loss: 0.43114 | val_0_rmse: 0.68157 | val_1_rmse: 0.68098 |  0:00:04s
epoch 3  | loss: 0.41773 | val_0_rmse: 0.6295  | val_1_rmse: 0.6318  |  0:00:05s
epoch 4  | loss: 0.39611 | val_0_rmse: 0.62722 | val_1_rmse: 0.61988 |  0:00:07s
epoch 5  | loss: 0.42048 | val_0_rmse: 0.61851 | val_1_rmse: 0.62111 |  0:00:08s
epoch 6  | loss: 0.39873 | val_0_rmse: 0.60138 | val_1_rmse: 0.59862 |  0:00:10s
epoch 7  | loss: 0.39301 | val_0_rmse: 0.59361 | val_1_rmse: 0.59171 |  0:00:11s
epoch 8  | loss: 0.37699 | val_0_rmse: 0.60658 | val_1_rmse: 0.61217 |  0:00:12s
epoch 9  | loss: 0.38326 | val_0_rmse: 0.59668 | val_1_rmse: 0.59604 |  0:00:14s
epoch 10 | loss: 0.37453 | val_0_rmse: 0.61604 | val_1_rmse: 0.6102  |  0:00:15s
epoch 11 | loss: 0.38411 | val_0_rmse: 0.59238 | val_1_rmse: 0.58377 |  0:00:17s
epoch 12 | loss: 0.36556 | val_0_rmse: 0.57827 | val_1_rmse: 0.5742  |  0:00:18s
epoch 13 | loss: 0.35624 | val_0_rmse: 0.58211 | val_1_rmse: 0.58125 |  0:00:20s
epoch 14 | loss: 0.36041 | val_0_rmse: 0.57944 | val_1_rmse: 0.57873 |  0:00:21s
epoch 15 | loss: 0.36221 | val_0_rmse: 0.59006 | val_1_rmse: 0.59062 |  0:00:23s
epoch 16 | loss: 0.37103 | val_0_rmse: 0.59344 | val_1_rmse: 0.59194 |  0:00:24s
epoch 17 | loss: 0.36193 | val_0_rmse: 0.58557 | val_1_rmse: 0.58402 |  0:00:25s
epoch 18 | loss: 0.35642 | val_0_rmse: 0.58324 | val_1_rmse: 0.58107 |  0:00:27s
epoch 19 | loss: 0.35052 | val_0_rmse: 0.57277 | val_1_rmse: 0.57127 |  0:00:28s
epoch 20 | loss: 0.35525 | val_0_rmse: 0.58914 | val_1_rmse: 0.58315 |  0:00:30s
epoch 21 | loss: 0.35029 | val_0_rmse: 0.57432 | val_1_rmse: 0.56885 |  0:00:31s
epoch 22 | loss: 0.35078 | val_0_rmse: 0.56495 | val_1_rmse: 0.56143 |  0:00:33s
epoch 23 | loss: 0.34124 | val_0_rmse: 0.58158 | val_1_rmse: 0.57466 |  0:00:34s
epoch 24 | loss: 0.341   | val_0_rmse: 0.57541 | val_1_rmse: 0.57463 |  0:00:36s
epoch 25 | loss: 0.34009 | val_0_rmse: 0.58958 | val_1_rmse: 0.58316 |  0:00:37s
epoch 26 | loss: 0.34922 | val_0_rmse: 0.57512 | val_1_rmse: 0.57097 |  0:00:38s
epoch 27 | loss: 0.34248 | val_0_rmse: 0.56305 | val_1_rmse: 0.55841 |  0:00:40s
epoch 28 | loss: 0.34656 | val_0_rmse: 0.57614 | val_1_rmse: 0.56807 |  0:00:41s
epoch 29 | loss: 0.33981 | val_0_rmse: 0.55506 | val_1_rmse: 0.55073 |  0:00:43s
epoch 30 | loss: 0.33067 | val_0_rmse: 0.5453  | val_1_rmse: 0.54035 |  0:00:44s
epoch 31 | loss: 0.3337  | val_0_rmse: 0.5876  | val_1_rmse: 0.58203 |  0:00:46s
epoch 32 | loss: 0.33761 | val_0_rmse: 0.56134 | val_1_rmse: 0.55403 |  0:00:47s
epoch 33 | loss: 0.33365 | val_0_rmse: 0.59197 | val_1_rmse: 0.58453 |  0:00:49s
epoch 34 | loss: 0.32499 | val_0_rmse: 0.56282 | val_1_rmse: 0.55761 |  0:00:50s
epoch 35 | loss: 0.3284  | val_0_rmse: 0.54885 | val_1_rmse: 0.54393 |  0:00:52s
epoch 36 | loss: 0.33101 | val_0_rmse: 0.5434  | val_1_rmse: 0.53708 |  0:00:53s
epoch 37 | loss: 0.3287  | val_0_rmse: 0.59157 | val_1_rmse: 0.58822 |  0:00:55s
epoch 38 | loss: 0.33083 | val_0_rmse: 0.54616 | val_1_rmse: 0.54164 |  0:00:56s
epoch 39 | loss: 0.3202  | val_0_rmse: 0.54669 | val_1_rmse: 0.5422  |  0:00:57s
epoch 40 | loss: 0.32125 | val_0_rmse: 0.53968 | val_1_rmse: 0.53889 |  0:00:59s
epoch 41 | loss: 0.31346 | val_0_rmse: 0.54844 | val_1_rmse: 0.54427 |  0:01:00s
epoch 42 | loss: 0.31076 | val_0_rmse: 0.53942 | val_1_rmse: 0.53873 |  0:01:02s
epoch 43 | loss: 0.31401 | val_0_rmse: 0.54475 | val_1_rmse: 0.5475  |  0:01:03s
epoch 44 | loss: 0.31574 | val_0_rmse: 0.54323 | val_1_rmse: 0.5425  |  0:01:05s
epoch 45 | loss: 0.31472 | val_0_rmse: 0.56158 | val_1_rmse: 0.55935 |  0:01:06s
epoch 46 | loss: 0.32498 | val_0_rmse: 0.54289 | val_1_rmse: 0.53955 |  0:01:07s
epoch 47 | loss: 0.31909 | val_0_rmse: 0.541   | val_1_rmse: 0.53823 |  0:01:09s
epoch 48 | loss: 0.31419 | val_0_rmse: 0.54265 | val_1_rmse: 0.53816 |  0:01:10s
epoch 49 | loss: 0.31214 | val_0_rmse: 0.55737 | val_1_rmse: 0.55774 |  0:01:12s
epoch 50 | loss: 0.31415 | val_0_rmse: 0.59699 | val_1_rmse: 0.59298 |  0:01:13s
epoch 51 | loss: 0.31631 | val_0_rmse: 0.53546 | val_1_rmse: 0.53635 |  0:01:15s
epoch 52 | loss: 0.31202 | val_0_rmse: 0.54902 | val_1_rmse: 0.54976 |  0:01:16s
epoch 53 | loss: 0.31665 | val_0_rmse: 0.54656 | val_1_rmse: 0.54758 |  0:01:18s
epoch 54 | loss: 0.33179 | val_0_rmse: 0.56324 | val_1_rmse: 0.56054 |  0:01:19s
epoch 55 | loss: 0.32309 | val_0_rmse: 0.56021 | val_1_rmse: 0.55892 |  0:01:20s
epoch 56 | loss: 0.31218 | val_0_rmse: 0.55663 | val_1_rmse: 0.55576 |  0:01:22s
epoch 57 | loss: 0.31844 | val_0_rmse: 0.56709 | val_1_rmse: 0.57136 |  0:01:23s
epoch 58 | loss: 0.31072 | val_0_rmse: 0.58706 | val_1_rmse: 0.58387 |  0:01:25s
epoch 59 | loss: 0.30597 | val_0_rmse: 0.55195 | val_1_rmse: 0.55119 |  0:01:26s
epoch 60 | loss: 0.31197 | val_0_rmse: 0.53082 | val_1_rmse: 0.53651 |  0:01:28s
epoch 61 | loss: 0.31142 | val_0_rmse: 0.53807 | val_1_rmse: 0.53765 |  0:01:29s
epoch 62 | loss: 0.30357 | val_0_rmse: 0.52343 | val_1_rmse: 0.52557 |  0:01:31s
epoch 63 | loss: 0.30901 | val_0_rmse: 0.57376 | val_1_rmse: 0.57694 |  0:01:32s
epoch 64 | loss: 0.31224 | val_0_rmse: 0.59906 | val_1_rmse: 0.59582 |  0:01:34s
epoch 65 | loss: 0.31076 | val_0_rmse: 0.56208 | val_1_rmse: 0.56428 |  0:01:35s
epoch 66 | loss: 0.30603 | val_0_rmse: 0.5245  | val_1_rmse: 0.53051 |  0:01:36s
epoch 67 | loss: 0.3056  | val_0_rmse: 0.53511 | val_1_rmse: 0.53809 |  0:01:38s
epoch 68 | loss: 0.30145 | val_0_rmse: 0.53968 | val_1_rmse: 0.53882 |  0:01:39s
epoch 69 | loss: 0.2983  | val_0_rmse: 0.53827 | val_1_rmse: 0.53722 |  0:01:41s
epoch 70 | loss: 0.30004 | val_0_rmse: 0.55226 | val_1_rmse: 0.55676 |  0:01:42s
epoch 71 | loss: 0.30563 | val_0_rmse: 0.53263 | val_1_rmse: 0.53693 |  0:01:44s
epoch 72 | loss: 0.30529 | val_0_rmse: 0.54244 | val_1_rmse: 0.54274 |  0:01:45s
epoch 73 | loss: 0.31286 | val_0_rmse: 0.54025 | val_1_rmse: 0.54127 |  0:01:46s
epoch 74 | loss: 0.30149 | val_0_rmse: 0.54389 | val_1_rmse: 0.53956 |  0:01:48s
epoch 75 | loss: 0.30707 | val_0_rmse: 0.54575 | val_1_rmse: 0.54938 |  0:01:49s
epoch 76 | loss: 0.30406 | val_0_rmse: 0.52319 | val_1_rmse: 0.52521 |  0:01:51s
epoch 77 | loss: 0.29366 | val_0_rmse: 0.52224 | val_1_rmse: 0.52458 |  0:01:52s
epoch 78 | loss: 0.29667 | val_0_rmse: 0.52245 | val_1_rmse: 0.52354 |  0:01:54s
epoch 79 | loss: 0.30666 | val_0_rmse: 0.52996 | val_1_rmse: 0.52842 |  0:01:55s
epoch 80 | loss: 0.29912 | val_0_rmse: 0.52423 | val_1_rmse: 0.52938 |  0:01:57s
epoch 81 | loss: 0.2982  | val_0_rmse: 0.51879 | val_1_rmse: 0.52172 |  0:01:58s
epoch 82 | loss: 0.3014  | val_0_rmse: 0.52419 | val_1_rmse: 0.52607 |  0:02:00s
epoch 83 | loss: 0.30323 | val_0_rmse: 0.52945 | val_1_rmse: 0.53379 |  0:02:01s
epoch 84 | loss: 0.30348 | val_0_rmse: 0.55025 | val_1_rmse: 0.55762 |  0:02:02s
epoch 85 | loss: 0.30309 | val_0_rmse: 0.51852 | val_1_rmse: 0.52353 |  0:02:04s
epoch 86 | loss: 0.29385 | val_0_rmse: 0.52815 | val_1_rmse: 0.5282  |  0:02:05s
epoch 87 | loss: 0.29902 | val_0_rmse: 0.53843 | val_1_rmse: 0.53912 |  0:02:07s
epoch 88 | loss: 0.30254 | val_0_rmse: 0.5164  | val_1_rmse: 0.51963 |  0:02:08s
epoch 89 | loss: 0.29987 | val_0_rmse: 0.51542 | val_1_rmse: 0.51757 |  0:02:10s
epoch 90 | loss: 0.29364 | val_0_rmse: 0.52647 | val_1_rmse: 0.52961 |  0:02:11s
epoch 91 | loss: 0.2941  | val_0_rmse: 0.58571 | val_1_rmse: 0.59132 |  0:02:13s
epoch 92 | loss: 0.30208 | val_0_rmse: 0.53589 | val_1_rmse: 0.54546 |  0:02:14s
epoch 93 | loss: 0.29236 | val_0_rmse: 0.52863 | val_1_rmse: 0.53476 |  0:02:15s
epoch 94 | loss: 0.28789 | val_0_rmse: 0.52025 | val_1_rmse: 0.52551 |  0:02:17s
epoch 95 | loss: 0.29679 | val_0_rmse: 0.51604 | val_1_rmse: 0.52405 |  0:02:18s
epoch 96 | loss: 0.29212 | val_0_rmse: 0.5118  | val_1_rmse: 0.51785 |  0:02:20s
epoch 97 | loss: 0.28705 | val_0_rmse: 0.5252  | val_1_rmse: 0.53365 |  0:02:21s
epoch 98 | loss: 0.29452 | val_0_rmse: 0.53978 | val_1_rmse: 0.54777 |  0:02:23s
epoch 99 | loss: 0.30253 | val_0_rmse: 0.52064 | val_1_rmse: 0.52208 |  0:02:24s
epoch 100| loss: 0.29496 | val_0_rmse: 0.53082 | val_1_rmse: 0.53552 |  0:02:25s
epoch 101| loss: 0.29643 | val_0_rmse: 0.53156 | val_1_rmse: 0.53937 |  0:02:27s
epoch 102| loss: 0.28982 | val_0_rmse: 0.513   | val_1_rmse: 0.52437 |  0:02:28s
epoch 103| loss: 0.28749 | val_0_rmse: 0.51225 | val_1_rmse: 0.52042 |  0:02:30s
epoch 104| loss: 0.2895  | val_0_rmse: 0.51939 | val_1_rmse: 0.52885 |  0:02:31s
epoch 105| loss: 0.28913 | val_0_rmse: 0.5078  | val_1_rmse: 0.5203  |  0:02:33s
epoch 106| loss: 0.28366 | val_0_rmse: 0.52189 | val_1_rmse: 0.53227 |  0:02:34s
epoch 107| loss: 0.29039 | val_0_rmse: 0.52822 | val_1_rmse: 0.5398  |  0:02:36s
epoch 108| loss: 0.28876 | val_0_rmse: 0.53095 | val_1_rmse: 0.54517 |  0:02:37s
epoch 109| loss: 0.29105 | val_0_rmse: 0.51002 | val_1_rmse: 0.51773 |  0:02:38s
epoch 110| loss: 0.2866  | val_0_rmse: 0.52014 | val_1_rmse: 0.52724 |  0:02:40s
epoch 111| loss: 0.28314 | val_0_rmse: 0.52743 | val_1_rmse: 0.53402 |  0:02:41s
epoch 112| loss: 0.29715 | val_0_rmse: 0.51713 | val_1_rmse: 0.52575 |  0:02:43s
epoch 113| loss: 0.29074 | val_0_rmse: 0.51291 | val_1_rmse: 0.52101 |  0:02:44s
epoch 114| loss: 0.30132 | val_0_rmse: 0.56266 | val_1_rmse: 0.57073 |  0:02:46s
epoch 115| loss: 0.29664 | val_0_rmse: 0.52065 | val_1_rmse: 0.53073 |  0:02:47s
epoch 116| loss: 0.29796 | val_0_rmse: 0.53847 | val_1_rmse: 0.54912 |  0:02:48s
epoch 117| loss: 0.29518 | val_0_rmse: 0.52103 | val_1_rmse: 0.52867 |  0:02:50s
epoch 118| loss: 0.28561 | val_0_rmse: 0.51027 | val_1_rmse: 0.52587 |  0:02:51s
epoch 119| loss: 0.29056 | val_0_rmse: 0.53657 | val_1_rmse: 0.54707 |  0:02:53s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.51757
Best weights from best epoch are automatically used!
ended training at: 05:06:43
Feature importance:
[('Area', 0.25133372457195086), ('Baths', 0.0589384331965264), ('Beds', 0.0), ('Latitude', 0.2999598028278498), ('Longitude', 0.1887029039392678), ('Month', 0.04706954111300756), ('Year', 0.15399559435139765)]
Mean squared error is of 6095180062.967593
Mean absolute error:53638.796372220095
MAPE:0.17636886953914552
R2 score:0.7258911474661411
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:06:44
epoch 0  | loss: 0.80422 | val_0_rmse: 0.79345 | val_1_rmse: 0.80744 |  0:00:01s
epoch 1  | loss: 0.57188 | val_0_rmse: 0.71947 | val_1_rmse: 0.73661 |  0:00:02s
epoch 2  | loss: 0.48456 | val_0_rmse: 0.66279 | val_1_rmse: 0.66019 |  0:00:04s
epoch 3  | loss: 0.42881 | val_0_rmse: 0.63754 | val_1_rmse: 0.64555 |  0:00:05s
epoch 4  | loss: 0.41442 | val_0_rmse: 0.65768 | val_1_rmse: 0.66088 |  0:00:07s
epoch 5  | loss: 0.43605 | val_0_rmse: 0.67248 | val_1_rmse: 0.68626 |  0:00:08s
epoch 6  | loss: 0.41558 | val_0_rmse: 0.59927 | val_1_rmse: 0.60381 |  0:00:10s
epoch 7  | loss: 0.3984  | val_0_rmse: 0.61953 | val_1_rmse: 0.62251 |  0:00:11s
epoch 8  | loss: 0.38541 | val_0_rmse: 0.61215 | val_1_rmse: 0.61651 |  0:00:13s
epoch 9  | loss: 0.36542 | val_0_rmse: 0.5881  | val_1_rmse: 0.58264 |  0:00:14s
epoch 10 | loss: 0.35741 | val_0_rmse: 0.57111 | val_1_rmse: 0.56928 |  0:00:15s
epoch 11 | loss: 0.35755 | val_0_rmse: 0.61332 | val_1_rmse: 0.6091  |  0:00:17s
epoch 12 | loss: 0.3497  | val_0_rmse: 0.56216 | val_1_rmse: 0.56597 |  0:00:18s
epoch 13 | loss: 0.33906 | val_0_rmse: 0.56457 | val_1_rmse: 0.57029 |  0:00:20s
epoch 14 | loss: 0.34335 | val_0_rmse: 0.56613 | val_1_rmse: 0.56989 |  0:00:21s
epoch 15 | loss: 0.34093 | val_0_rmse: 0.5759  | val_1_rmse: 0.5855  |  0:00:23s
epoch 16 | loss: 0.33553 | val_0_rmse: 0.57455 | val_1_rmse: 0.58297 |  0:00:24s
epoch 17 | loss: 0.34023 | val_0_rmse: 0.56903 | val_1_rmse: 0.57384 |  0:00:26s
epoch 18 | loss: 0.33658 | val_0_rmse: 0.56293 | val_1_rmse: 0.57403 |  0:00:27s
epoch 19 | loss: 0.33722 | val_0_rmse: 0.59357 | val_1_rmse: 0.60545 |  0:00:28s
epoch 20 | loss: 0.33264 | val_0_rmse: 0.55767 | val_1_rmse: 0.55678 |  0:00:30s
epoch 21 | loss: 0.32809 | val_0_rmse: 0.54618 | val_1_rmse: 0.55384 |  0:00:31s
epoch 22 | loss: 0.33149 | val_0_rmse: 0.5471  | val_1_rmse: 0.55575 |  0:00:33s
epoch 23 | loss: 0.32603 | val_0_rmse: 0.53917 | val_1_rmse: 0.54448 |  0:00:34s
epoch 24 | loss: 0.31791 | val_0_rmse: 0.56811 | val_1_rmse: 0.57551 |  0:00:36s
epoch 25 | loss: 0.32797 | val_0_rmse: 0.5371  | val_1_rmse: 0.53828 |  0:00:37s
epoch 26 | loss: 0.32372 | val_0_rmse: 0.54103 | val_1_rmse: 0.5449  |  0:00:39s
epoch 27 | loss: 0.32821 | val_0_rmse: 0.63011 | val_1_rmse: 0.63152 |  0:00:40s
epoch 28 | loss: 0.32518 | val_0_rmse: 0.56997 | val_1_rmse: 0.57541 |  0:00:42s
epoch 29 | loss: 0.31827 | val_0_rmse: 0.53465 | val_1_rmse: 0.54302 |  0:00:43s
epoch 30 | loss: 0.31382 | val_0_rmse: 0.54835 | val_1_rmse: 0.55006 |  0:00:44s
epoch 31 | loss: 0.31471 | val_0_rmse: 0.52795 | val_1_rmse: 0.53469 |  0:00:46s
epoch 32 | loss: 0.3218  | val_0_rmse: 0.54378 | val_1_rmse: 0.54228 |  0:00:47s
epoch 33 | loss: 0.31204 | val_0_rmse: 0.54374 | val_1_rmse: 0.54039 |  0:00:49s
epoch 34 | loss: 0.31449 | val_0_rmse: 0.53858 | val_1_rmse: 0.54412 |  0:00:50s
epoch 35 | loss: 0.30873 | val_0_rmse: 0.53785 | val_1_rmse: 0.54626 |  0:00:52s
epoch 36 | loss: 0.31479 | val_0_rmse: 0.53385 | val_1_rmse: 0.54066 |  0:00:53s
epoch 37 | loss: 0.30923 | val_0_rmse: 0.52426 | val_1_rmse: 0.5301  |  0:00:55s
epoch 38 | loss: 0.30722 | val_0_rmse: 0.54375 | val_1_rmse: 0.54669 |  0:00:56s
epoch 39 | loss: 0.32153 | val_0_rmse: 0.60397 | val_1_rmse: 0.60283 |  0:00:57s
epoch 40 | loss: 0.3228  | val_0_rmse: 0.59943 | val_1_rmse: 0.60142 |  0:00:59s
epoch 41 | loss: 0.31224 | val_0_rmse: 0.57228 | val_1_rmse: 0.57169 |  0:01:00s
epoch 42 | loss: 0.31472 | val_0_rmse: 0.57386 | val_1_rmse: 0.57926 |  0:01:02s
epoch 43 | loss: 0.31732 | val_0_rmse: 0.54499 | val_1_rmse: 0.5492  |  0:01:03s
epoch 44 | loss: 0.30894 | val_0_rmse: 0.53487 | val_1_rmse: 0.53822 |  0:01:05s
epoch 45 | loss: 0.30824 | val_0_rmse: 0.56678 | val_1_rmse: 0.56759 |  0:01:06s
epoch 46 | loss: 0.31203 | val_0_rmse: 0.53153 | val_1_rmse: 0.53412 |  0:01:07s
epoch 47 | loss: 0.30789 | val_0_rmse: 0.53811 | val_1_rmse: 0.5391  |  0:01:09s
epoch 48 | loss: 0.31102 | val_0_rmse: 0.65166 | val_1_rmse: 0.64752 |  0:01:10s
epoch 49 | loss: 0.31052 | val_0_rmse: 0.52691 | val_1_rmse: 0.53168 |  0:01:12s
epoch 50 | loss: 0.29796 | val_0_rmse: 0.51494 | val_1_rmse: 0.52231 |  0:01:13s
epoch 51 | loss: 0.30381 | val_0_rmse: 0.53091 | val_1_rmse: 0.54003 |  0:01:15s
epoch 52 | loss: 0.30129 | val_0_rmse: 0.53252 | val_1_rmse: 0.54728 |  0:01:16s
epoch 53 | loss: 0.30922 | val_0_rmse: 0.58768 | val_1_rmse: 0.58449 |  0:01:18s
epoch 54 | loss: 0.31524 | val_0_rmse: 0.59322 | val_1_rmse: 0.60272 |  0:01:19s
epoch 55 | loss: 0.30861 | val_0_rmse: 0.52806 | val_1_rmse: 0.53959 |  0:01:20s
epoch 56 | loss: 0.30713 | val_0_rmse: 0.53358 | val_1_rmse: 0.54158 |  0:01:22s
epoch 57 | loss: 0.29919 | val_0_rmse: 0.53302 | val_1_rmse: 0.54228 |  0:01:23s
epoch 58 | loss: 0.3079  | val_0_rmse: 0.51288 | val_1_rmse: 0.52086 |  0:01:25s
epoch 59 | loss: 0.3152  | val_0_rmse: 0.53886 | val_1_rmse: 0.54737 |  0:01:26s
epoch 60 | loss: 0.30772 | val_0_rmse: 0.54219 | val_1_rmse: 0.55379 |  0:01:28s
epoch 61 | loss: 0.3002  | val_0_rmse: 0.51966 | val_1_rmse: 0.52748 |  0:01:29s
epoch 62 | loss: 0.30673 | val_0_rmse: 0.54951 | val_1_rmse: 0.55447 |  0:01:31s
epoch 63 | loss: 0.30892 | val_0_rmse: 0.54188 | val_1_rmse: 0.54825 |  0:01:32s
epoch 64 | loss: 0.30145 | val_0_rmse: 0.5358  | val_1_rmse: 0.54569 |  0:01:33s
epoch 65 | loss: 0.29856 | val_0_rmse: 0.52523 | val_1_rmse: 0.53535 |  0:01:35s
epoch 66 | loss: 0.30191 | val_0_rmse: 0.53504 | val_1_rmse: 0.54702 |  0:01:36s
epoch 67 | loss: 0.31128 | val_0_rmse: 0.73293 | val_1_rmse: 0.7271  |  0:01:38s
epoch 68 | loss: 0.30203 | val_0_rmse: 0.53381 | val_1_rmse: 0.5441  |  0:01:39s
epoch 69 | loss: 0.29529 | val_0_rmse: 0.60262 | val_1_rmse: 0.60357 |  0:01:41s
epoch 70 | loss: 0.3013  | val_0_rmse: 0.51763 | val_1_rmse: 0.52269 |  0:01:42s
epoch 71 | loss: 0.30181 | val_0_rmse: 0.5487  | val_1_rmse: 0.55118 |  0:01:43s
epoch 72 | loss: 0.30812 | val_0_rmse: 0.5498  | val_1_rmse: 0.55269 |  0:01:45s
epoch 73 | loss: 0.30956 | val_0_rmse: 0.53379 | val_1_rmse: 0.54575 |  0:01:46s
epoch 74 | loss: 0.30905 | val_0_rmse: 0.60504 | val_1_rmse: 0.60321 |  0:01:48s
epoch 75 | loss: 0.30226 | val_0_rmse: 0.52074 | val_1_rmse: 0.53361 |  0:01:49s
epoch 76 | loss: 0.29301 | val_0_rmse: 0.55373 | val_1_rmse: 0.56992 |  0:01:51s
epoch 77 | loss: 0.30659 | val_0_rmse: 0.70584 | val_1_rmse: 0.70644 |  0:01:52s
epoch 78 | loss: 0.30969 | val_0_rmse: 0.54744 | val_1_rmse: 0.55386 |  0:01:54s
epoch 79 | loss: 0.29964 | val_0_rmse: 0.6063  | val_1_rmse: 0.6125  |  0:01:55s
epoch 80 | loss: 0.30207 | val_0_rmse: 0.51527 | val_1_rmse: 0.52311 |  0:01:56s
epoch 81 | loss: 0.29618 | val_0_rmse: 0.53427 | val_1_rmse: 0.54391 |  0:01:58s
epoch 82 | loss: 0.29914 | val_0_rmse: 0.53972 | val_1_rmse: 0.55552 |  0:01:59s
epoch 83 | loss: 0.29296 | val_0_rmse: 0.52472 | val_1_rmse: 0.53771 |  0:02:01s
epoch 84 | loss: 0.29533 | val_0_rmse: 0.70276 | val_1_rmse: 0.70278 |  0:02:02s
epoch 85 | loss: 0.29375 | val_0_rmse: 0.64411 | val_1_rmse: 0.63853 |  0:02:04s
epoch 86 | loss: 0.29443 | val_0_rmse: 0.56556 | val_1_rmse: 0.57115 |  0:02:05s
epoch 87 | loss: 0.30855 | val_0_rmse: 0.58371 | val_1_rmse: 0.58234 |  0:02:06s
epoch 88 | loss: 0.3038  | val_0_rmse: 0.53158 | val_1_rmse: 0.54149 |  0:02:08s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.52086
Best weights from best epoch are automatically used!
ended training at: 05:08:52
Feature importance:
[('Area', 0.29252843976740506), ('Baths', 0.04249549687650315), ('Beds', 0.0059469384369825505), ('Latitude', 0.2793369021794562), ('Longitude', 0.30111186929610545), ('Month', 0.004095187263378093), ('Year', 0.0744851661801695)]
Mean squared error is of 6130545745.823517
Mean absolute error:54676.65260072019
MAPE:0.17563670087173666
R2 score:0.7219257500096274
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:08:53
epoch 0  | loss: 0.7496  | val_0_rmse: 0.7951  | val_1_rmse: 0.77119 |  0:00:01s
epoch 1  | loss: 0.47714 | val_0_rmse: 0.64159 | val_1_rmse: 0.62221 |  0:00:02s
epoch 2  | loss: 0.41457 | val_0_rmse: 0.61136 | val_1_rmse: 0.5903  |  0:00:04s
epoch 3  | loss: 0.38987 | val_0_rmse: 0.59768 | val_1_rmse: 0.58507 |  0:00:05s
epoch 4  | loss: 0.38653 | val_0_rmse: 0.60932 | val_1_rmse: 0.58558 |  0:00:07s
epoch 5  | loss: 0.36299 | val_0_rmse: 0.60835 | val_1_rmse: 0.59074 |  0:00:08s
epoch 6  | loss: 0.37214 | val_0_rmse: 0.6004  | val_1_rmse: 0.5761  |  0:00:10s
epoch 7  | loss: 0.36442 | val_0_rmse: 0.57599 | val_1_rmse: 0.55058 |  0:00:11s
epoch 8  | loss: 0.36148 | val_0_rmse: 0.57732 | val_1_rmse: 0.55154 |  0:00:13s
epoch 9  | loss: 0.35527 | val_0_rmse: 0.57357 | val_1_rmse: 0.54547 |  0:00:14s
epoch 10 | loss: 0.34664 | val_0_rmse: 0.588   | val_1_rmse: 0.56428 |  0:00:15s
epoch 11 | loss: 0.34582 | val_0_rmse: 0.56491 | val_1_rmse: 0.54739 |  0:00:17s
epoch 12 | loss: 0.33647 | val_0_rmse: 0.55856 | val_1_rmse: 0.5332  |  0:00:18s
epoch 13 | loss: 0.33177 | val_0_rmse: 0.60009 | val_1_rmse: 0.58064 |  0:00:20s
epoch 14 | loss: 0.34624 | val_0_rmse: 0.57588 | val_1_rmse: 0.55411 |  0:00:21s
epoch 15 | loss: 0.33994 | val_0_rmse: 0.56356 | val_1_rmse: 0.53859 |  0:00:23s
epoch 16 | loss: 0.34276 | val_0_rmse: 0.58598 | val_1_rmse: 0.56506 |  0:00:24s
epoch 17 | loss: 0.34348 | val_0_rmse: 0.56628 | val_1_rmse: 0.54076 |  0:00:26s
epoch 18 | loss: 0.35151 | val_0_rmse: 0.55942 | val_1_rmse: 0.53584 |  0:00:27s
epoch 19 | loss: 0.33108 | val_0_rmse: 0.59022 | val_1_rmse: 0.56734 |  0:00:29s
epoch 20 | loss: 0.34814 | val_0_rmse: 0.57826 | val_1_rmse: 0.55621 |  0:00:30s
epoch 21 | loss: 0.33172 | val_0_rmse: 0.55344 | val_1_rmse: 0.52759 |  0:00:31s
epoch 22 | loss: 0.33956 | val_0_rmse: 0.55667 | val_1_rmse: 0.54178 |  0:00:33s
epoch 23 | loss: 0.32954 | val_0_rmse: 0.5636  | val_1_rmse: 0.54357 |  0:00:34s
epoch 24 | loss: 0.33321 | val_0_rmse: 0.57568 | val_1_rmse: 0.56107 |  0:00:36s
epoch 25 | loss: 0.33855 | val_0_rmse: 0.58305 | val_1_rmse: 0.55711 |  0:00:37s
epoch 26 | loss: 0.34291 | val_0_rmse: 0.54987 | val_1_rmse: 0.52804 |  0:00:39s
epoch 27 | loss: 0.35041 | val_0_rmse: 0.58497 | val_1_rmse: 0.56878 |  0:00:40s
epoch 28 | loss: 0.3416  | val_0_rmse: 0.56763 | val_1_rmse: 0.54712 |  0:00:41s
epoch 29 | loss: 0.33991 | val_0_rmse: 0.55431 | val_1_rmse: 0.52696 |  0:00:43s
epoch 30 | loss: 0.32662 | val_0_rmse: 0.58226 | val_1_rmse: 0.56235 |  0:00:44s
epoch 31 | loss: 0.33138 | val_0_rmse: 0.5735  | val_1_rmse: 0.55792 |  0:00:46s
epoch 32 | loss: 0.32147 | val_0_rmse: 0.55569 | val_1_rmse: 0.53711 |  0:00:47s
epoch 33 | loss: 0.32117 | val_0_rmse: 0.54264 | val_1_rmse: 0.52556 |  0:00:49s
epoch 34 | loss: 0.31636 | val_0_rmse: 0.55753 | val_1_rmse: 0.54195 |  0:00:50s
epoch 35 | loss: 0.33914 | val_0_rmse: 0.55751 | val_1_rmse: 0.53604 |  0:00:52s
epoch 36 | loss: 0.32301 | val_0_rmse: 0.54615 | val_1_rmse: 0.5269  |  0:00:53s
epoch 37 | loss: 0.32811 | val_0_rmse: 0.56068 | val_1_rmse: 0.53419 |  0:00:54s
epoch 38 | loss: 0.31905 | val_0_rmse: 0.54488 | val_1_rmse: 0.52648 |  0:00:56s
epoch 39 | loss: 0.31673 | val_0_rmse: 0.53641 | val_1_rmse: 0.51252 |  0:00:57s
epoch 40 | loss: 0.31084 | val_0_rmse: 0.53523 | val_1_rmse: 0.51588 |  0:00:59s
epoch 41 | loss: 0.31674 | val_0_rmse: 0.53874 | val_1_rmse: 0.51783 |  0:01:00s
epoch 42 | loss: 0.3111  | val_0_rmse: 0.53411 | val_1_rmse: 0.51565 |  0:01:02s
epoch 43 | loss: 0.31053 | val_0_rmse: 0.53889 | val_1_rmse: 0.5203  |  0:01:03s
epoch 44 | loss: 0.31049 | val_0_rmse: 0.53482 | val_1_rmse: 0.51721 |  0:01:05s
epoch 45 | loss: 0.31722 | val_0_rmse: 0.54175 | val_1_rmse: 0.51801 |  0:01:06s
epoch 46 | loss: 0.30894 | val_0_rmse: 0.57541 | val_1_rmse: 0.55605 |  0:01:07s
epoch 47 | loss: 0.31113 | val_0_rmse: 0.53198 | val_1_rmse: 0.51244 |  0:01:09s
epoch 48 | loss: 0.30394 | val_0_rmse: 0.53557 | val_1_rmse: 0.52495 |  0:01:10s
epoch 49 | loss: 0.32218 | val_0_rmse: 0.58304 | val_1_rmse: 0.56975 |  0:01:12s
epoch 50 | loss: 0.32991 | val_0_rmse: 0.57009 | val_1_rmse: 0.55121 |  0:01:13s
epoch 51 | loss: 0.32122 | val_0_rmse: 0.5473  | val_1_rmse: 0.53931 |  0:01:15s
epoch 52 | loss: 0.31562 | val_0_rmse: 0.54379 | val_1_rmse: 0.52893 |  0:01:16s
epoch 53 | loss: 0.30448 | val_0_rmse: 0.53146 | val_1_rmse: 0.51507 |  0:01:17s
epoch 54 | loss: 0.30859 | val_0_rmse: 0.54353 | val_1_rmse: 0.53681 |  0:01:19s
epoch 55 | loss: 0.30681 | val_0_rmse: 0.55232 | val_1_rmse: 0.53358 |  0:01:20s
epoch 56 | loss: 0.30769 | val_0_rmse: 0.5426  | val_1_rmse: 0.52513 |  0:01:22s
epoch 57 | loss: 0.30243 | val_0_rmse: 0.52216 | val_1_rmse: 0.50164 |  0:01:23s
epoch 58 | loss: 0.29686 | val_0_rmse: 0.54458 | val_1_rmse: 0.53065 |  0:01:25s
epoch 59 | loss: 0.30628 | val_0_rmse: 0.52507 | val_1_rmse: 0.51337 |  0:01:26s
epoch 60 | loss: 0.30038 | val_0_rmse: 0.52267 | val_1_rmse: 0.50393 |  0:01:28s
epoch 61 | loss: 0.29688 | val_0_rmse: 0.53911 | val_1_rmse: 0.52342 |  0:01:29s
epoch 62 | loss: 0.2944  | val_0_rmse: 0.52385 | val_1_rmse: 0.50584 |  0:01:30s
epoch 63 | loss: 0.29597 | val_0_rmse: 0.52447 | val_1_rmse: 0.51051 |  0:01:32s
epoch 64 | loss: 0.28964 | val_0_rmse: 0.5633  | val_1_rmse: 0.55689 |  0:01:33s
epoch 65 | loss: 0.29409 | val_0_rmse: 0.5916  | val_1_rmse: 0.57511 |  0:01:35s
epoch 66 | loss: 0.30139 | val_0_rmse: 0.54812 | val_1_rmse: 0.53928 |  0:01:36s
epoch 67 | loss: 0.29802 | val_0_rmse: 0.54357 | val_1_rmse: 0.53729 |  0:01:38s
epoch 68 | loss: 0.3011  | val_0_rmse: 0.54486 | val_1_rmse: 0.52538 |  0:01:39s
epoch 69 | loss: 0.29886 | val_0_rmse: 0.52666 | val_1_rmse: 0.51377 |  0:01:40s
epoch 70 | loss: 0.29692 | val_0_rmse: 0.54158 | val_1_rmse: 0.52952 |  0:01:42s
epoch 71 | loss: 0.29652 | val_0_rmse: 0.54117 | val_1_rmse: 0.52811 |  0:01:43s
epoch 72 | loss: 0.29578 | val_0_rmse: 0.5382  | val_1_rmse: 0.52641 |  0:01:45s
epoch 73 | loss: 0.30487 | val_0_rmse: 0.52867 | val_1_rmse: 0.51225 |  0:01:46s
epoch 74 | loss: 0.29215 | val_0_rmse: 0.50897 | val_1_rmse: 0.49402 |  0:01:48s
epoch 75 | loss: 0.2874  | val_0_rmse: 0.56088 | val_1_rmse: 0.55294 |  0:01:49s
epoch 76 | loss: 0.2937  | val_0_rmse: 0.52125 | val_1_rmse: 0.50661 |  0:01:51s
epoch 77 | loss: 0.28556 | val_0_rmse: 0.51007 | val_1_rmse: 0.49299 |  0:01:52s
epoch 78 | loss: 0.29138 | val_0_rmse: 0.52059 | val_1_rmse: 0.50443 |  0:01:54s
epoch 79 | loss: 0.29407 | val_0_rmse: 0.50911 | val_1_rmse: 0.488   |  0:01:55s
epoch 80 | loss: 0.28441 | val_0_rmse: 0.53443 | val_1_rmse: 0.52285 |  0:01:56s
epoch 81 | loss: 0.29035 | val_0_rmse: 0.52367 | val_1_rmse: 0.50582 |  0:01:58s
epoch 82 | loss: 0.29399 | val_0_rmse: 0.52386 | val_1_rmse: 0.51319 |  0:01:59s
epoch 83 | loss: 0.28015 | val_0_rmse: 0.51575 | val_1_rmse: 0.50091 |  0:02:01s
epoch 84 | loss: 0.28544 | val_0_rmse: 0.50715 | val_1_rmse: 0.49322 |  0:02:02s
epoch 85 | loss: 0.28372 | val_0_rmse: 0.51138 | val_1_rmse: 0.49886 |  0:02:04s
epoch 86 | loss: 0.28877 | val_0_rmse: 0.52666 | val_1_rmse: 0.51682 |  0:02:05s
epoch 87 | loss: 0.28636 | val_0_rmse: 0.53132 | val_1_rmse: 0.5198  |  0:02:07s
epoch 88 | loss: 0.28652 | val_0_rmse: 0.54104 | val_1_rmse: 0.52622 |  0:02:08s
epoch 89 | loss: 0.28463 | val_0_rmse: 0.52701 | val_1_rmse: 0.51562 |  0:02:09s
epoch 90 | loss: 0.27697 | val_0_rmse: 0.54021 | val_1_rmse: 0.52969 |  0:02:11s
epoch 91 | loss: 0.2861  | val_0_rmse: 0.50826 | val_1_rmse: 0.50364 |  0:02:12s
epoch 92 | loss: 0.28394 | val_0_rmse: 0.50503 | val_1_rmse: 0.49261 |  0:02:14s
epoch 93 | loss: 0.29494 | val_0_rmse: 0.52486 | val_1_rmse: 0.51094 |  0:02:15s
epoch 94 | loss: 0.28086 | val_0_rmse: 0.51035 | val_1_rmse: 0.50748 |  0:02:17s
epoch 95 | loss: 0.2794  | val_0_rmse: 0.5086  | val_1_rmse: 0.50145 |  0:02:18s
epoch 96 | loss: 0.28537 | val_0_rmse: 0.50581 | val_1_rmse: 0.49821 |  0:02:20s
epoch 97 | loss: 0.27886 | val_0_rmse: 0.53961 | val_1_rmse: 0.53624 |  0:02:21s
epoch 98 | loss: 0.28974 | val_0_rmse: 0.51706 | val_1_rmse: 0.49959 |  0:02:22s
epoch 99 | loss: 0.29087 | val_0_rmse: 0.51123 | val_1_rmse: 0.50151 |  0:02:24s
epoch 100| loss: 0.29736 | val_0_rmse: 0.62184 | val_1_rmse: 0.61504 |  0:02:25s
epoch 101| loss: 0.28281 | val_0_rmse: 0.51014 | val_1_rmse: 0.50202 |  0:02:27s
epoch 102| loss: 0.28549 | val_0_rmse: 0.53589 | val_1_rmse: 0.52465 |  0:02:28s
epoch 103| loss: 0.28087 | val_0_rmse: 0.52689 | val_1_rmse: 0.51637 |  0:02:30s
epoch 104| loss: 0.28078 | val_0_rmse: 0.63734 | val_1_rmse: 0.63457 |  0:02:31s
epoch 105| loss: 0.29108 | val_0_rmse: 0.55311 | val_1_rmse: 0.54098 |  0:02:33s
epoch 106| loss: 0.27872 | val_0_rmse: 0.52229 | val_1_rmse: 0.51079 |  0:02:34s
epoch 107| loss: 0.27885 | val_0_rmse: 0.51104 | val_1_rmse: 0.50943 |  0:02:35s
epoch 108| loss: 0.27789 | val_0_rmse: 0.51237 | val_1_rmse: 0.50334 |  0:02:37s
epoch 109| loss: 0.27445 | val_0_rmse: 0.51205 | val_1_rmse: 0.51022 |  0:02:38s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.488
Best weights from best epoch are automatically used!
ended training at: 05:11:32
Feature importance:
[('Area', 0.24396576562777203), ('Baths', 0.029697937891203063), ('Beds', 0.02513467593531824), ('Latitude', 0.35917479916285777), ('Longitude', 0.27600234552822206), ('Month', 0.0), ('Year', 0.06602447585462681)]
Mean squared error is of 5909527301.545556
Mean absolute error:53108.80316739115
MAPE:0.16529135181180157
R2 score:0.7406799131972708
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:11:32
epoch 0  | loss: 0.78451 | val_0_rmse: 0.83681 | val_1_rmse: 0.82682 |  0:00:01s
epoch 1  | loss: 0.49063 | val_0_rmse: 0.66166 | val_1_rmse: 0.65665 |  0:00:02s
epoch 2  | loss: 0.40874 | val_0_rmse: 0.61611 | val_1_rmse: 0.62233 |  0:00:04s
epoch 3  | loss: 0.38393 | val_0_rmse: 0.59896 | val_1_rmse: 0.59542 |  0:00:05s
epoch 4  | loss: 0.37278 | val_0_rmse: 0.62857 | val_1_rmse: 0.62997 |  0:00:07s
epoch 5  | loss: 0.37478 | val_0_rmse: 0.59394 | val_1_rmse: 0.59498 |  0:00:08s
epoch 6  | loss: 0.36358 | val_0_rmse: 0.60036 | val_1_rmse: 0.60028 |  0:00:10s
epoch 7  | loss: 0.37151 | val_0_rmse: 0.58534 | val_1_rmse: 0.58939 |  0:00:11s
epoch 8  | loss: 0.35054 | val_0_rmse: 0.58571 | val_1_rmse: 0.58317 |  0:00:13s
epoch 9  | loss: 0.34899 | val_0_rmse: 0.58984 | val_1_rmse: 0.60356 |  0:00:14s
epoch 10 | loss: 0.35903 | val_0_rmse: 0.59073 | val_1_rmse: 0.58889 |  0:00:15s
epoch 11 | loss: 0.34421 | val_0_rmse: 0.59558 | val_1_rmse: 0.60786 |  0:00:17s
epoch 12 | loss: 0.33199 | val_0_rmse: 0.57373 | val_1_rmse: 0.57716 |  0:00:18s
epoch 13 | loss: 0.32963 | val_0_rmse: 0.57413 | val_1_rmse: 0.5732  |  0:00:20s
epoch 14 | loss: 0.34042 | val_0_rmse: 0.59339 | val_1_rmse: 0.59666 |  0:00:21s
epoch 15 | loss: 0.33134 | val_0_rmse: 0.56277 | val_1_rmse: 0.55828 |  0:00:23s
epoch 16 | loss: 0.33527 | val_0_rmse: 0.56219 | val_1_rmse: 0.56279 |  0:00:24s
epoch 17 | loss: 0.33653 | val_0_rmse: 0.58721 | val_1_rmse: 0.593   |  0:00:26s
epoch 18 | loss: 0.33169 | val_0_rmse: 0.54824 | val_1_rmse: 0.55323 |  0:00:27s
epoch 19 | loss: 0.32504 | val_0_rmse: 0.54681 | val_1_rmse: 0.54867 |  0:00:28s
epoch 20 | loss: 0.33838 | val_0_rmse: 0.56037 | val_1_rmse: 0.55995 |  0:00:30s
epoch 21 | loss: 0.32549 | val_0_rmse: 0.54966 | val_1_rmse: 0.5521  |  0:00:31s
epoch 22 | loss: 0.31764 | val_0_rmse: 0.53729 | val_1_rmse: 0.53949 |  0:00:33s
epoch 23 | loss: 0.32035 | val_0_rmse: 0.57269 | val_1_rmse: 0.57127 |  0:00:34s
epoch 24 | loss: 0.319   | val_0_rmse: 0.62592 | val_1_rmse: 0.62786 |  0:00:36s
epoch 25 | loss: 0.33539 | val_0_rmse: 0.58317 | val_1_rmse: 0.58279 |  0:00:37s
epoch 26 | loss: 0.32337 | val_0_rmse: 0.53476 | val_1_rmse: 0.54151 |  0:00:39s
epoch 27 | loss: 0.31938 | val_0_rmse: 0.53748 | val_1_rmse: 0.54282 |  0:00:40s
epoch 28 | loss: 0.31213 | val_0_rmse: 0.5487  | val_1_rmse: 0.54815 |  0:00:41s
epoch 29 | loss: 0.3076  | val_0_rmse: 0.53063 | val_1_rmse: 0.53513 |  0:00:43s
epoch 30 | loss: 0.31204 | val_0_rmse: 0.53062 | val_1_rmse: 0.53596 |  0:00:44s
epoch 31 | loss: 0.31493 | val_0_rmse: 0.54943 | val_1_rmse: 0.55704 |  0:00:46s
epoch 32 | loss: 0.31039 | val_0_rmse: 0.56926 | val_1_rmse: 0.57191 |  0:00:47s
epoch 33 | loss: 0.34074 | val_0_rmse: 0.5709  | val_1_rmse: 0.56771 |  0:00:49s
epoch 34 | loss: 0.33826 | val_0_rmse: 0.54879 | val_1_rmse: 0.54978 |  0:00:50s
epoch 35 | loss: 0.32321 | val_0_rmse: 0.55553 | val_1_rmse: 0.55921 |  0:00:52s
epoch 36 | loss: 0.31464 | val_0_rmse: 0.5269  | val_1_rmse: 0.52923 |  0:00:53s
epoch 37 | loss: 0.31348 | val_0_rmse: 0.55627 | val_1_rmse: 0.56147 |  0:00:54s
epoch 38 | loss: 0.31526 | val_0_rmse: 0.52957 | val_1_rmse: 0.52901 |  0:00:56s
epoch 39 | loss: 0.31619 | val_0_rmse: 0.56457 | val_1_rmse: 0.55697 |  0:00:57s
epoch 40 | loss: 0.31474 | val_0_rmse: 0.57015 | val_1_rmse: 0.56949 |  0:00:59s
epoch 41 | loss: 0.32478 | val_0_rmse: 0.55361 | val_1_rmse: 0.55944 |  0:01:00s
epoch 42 | loss: 0.3077  | val_0_rmse: 0.55    | val_1_rmse: 0.55567 |  0:01:02s
epoch 43 | loss: 0.31851 | val_0_rmse: 0.53236 | val_1_rmse: 0.53067 |  0:01:03s
epoch 44 | loss: 0.31152 | val_0_rmse: 0.54408 | val_1_rmse: 0.54468 |  0:01:05s
epoch 45 | loss: 0.31481 | val_0_rmse: 0.69425 | val_1_rmse: 0.70144 |  0:01:06s
epoch 46 | loss: 0.32448 | val_0_rmse: 0.57623 | val_1_rmse: 0.577   |  0:01:07s
epoch 47 | loss: 0.32463 | val_0_rmse: 0.54977 | val_1_rmse: 0.55277 |  0:01:09s
epoch 48 | loss: 0.31926 | val_0_rmse: 0.53378 | val_1_rmse: 0.53923 |  0:01:10s
epoch 49 | loss: 0.30736 | val_0_rmse: 0.55588 | val_1_rmse: 0.56294 |  0:01:12s
epoch 50 | loss: 0.30361 | val_0_rmse: 0.53468 | val_1_rmse: 0.54178 |  0:01:13s
epoch 51 | loss: 0.30989 | val_0_rmse: 0.53725 | val_1_rmse: 0.5392  |  0:01:15s
epoch 52 | loss: 0.31053 | val_0_rmse: 0.57189 | val_1_rmse: 0.56789 |  0:01:16s
epoch 53 | loss: 0.31771 | val_0_rmse: 0.54555 | val_1_rmse: 0.54294 |  0:01:17s
epoch 54 | loss: 0.32178 | val_0_rmse: 0.55556 | val_1_rmse: 0.5652  |  0:01:19s
epoch 55 | loss: 0.33124 | val_0_rmse: 0.5625  | val_1_rmse: 0.56254 |  0:01:20s
epoch 56 | loss: 0.31966 | val_0_rmse: 0.58612 | val_1_rmse: 0.58925 |  0:01:22s
epoch 57 | loss: 0.32406 | val_0_rmse: 0.54745 | val_1_rmse: 0.55181 |  0:01:23s
epoch 58 | loss: 0.32284 | val_0_rmse: 0.53412 | val_1_rmse: 0.5348  |  0:01:25s
epoch 59 | loss: 0.3098  | val_0_rmse: 0.53951 | val_1_rmse: 0.54734 |  0:01:26s
epoch 60 | loss: 0.30632 | val_0_rmse: 0.53837 | val_1_rmse: 0.54589 |  0:01:28s
epoch 61 | loss: 0.31463 | val_0_rmse: 0.55537 | val_1_rmse: 0.56138 |  0:01:29s
epoch 62 | loss: 0.31121 | val_0_rmse: 0.53047 | val_1_rmse: 0.52565 |  0:01:30s
epoch 63 | loss: 0.31022 | val_0_rmse: 0.53267 | val_1_rmse: 0.53358 |  0:01:32s
epoch 64 | loss: 0.31232 | val_0_rmse: 0.54151 | val_1_rmse: 0.5391  |  0:01:33s
epoch 65 | loss: 0.30759 | val_0_rmse: 0.52413 | val_1_rmse: 0.5234  |  0:01:35s
epoch 66 | loss: 0.32325 | val_0_rmse: 0.56942 | val_1_rmse: 0.56592 |  0:01:36s
epoch 67 | loss: 0.31442 | val_0_rmse: 0.52853 | val_1_rmse: 0.52966 |  0:01:38s
epoch 68 | loss: 0.31126 | val_0_rmse: 0.53775 | val_1_rmse: 0.54223 |  0:01:39s
epoch 69 | loss: 0.30301 | val_0_rmse: 0.54203 | val_1_rmse: 0.54854 |  0:01:41s
epoch 70 | loss: 0.30555 | val_0_rmse: 0.58522 | val_1_rmse: 0.59398 |  0:01:42s
epoch 71 | loss: 0.30708 | val_0_rmse: 0.56182 | val_1_rmse: 0.56465 |  0:01:43s
epoch 72 | loss: 0.30204 | val_0_rmse: 0.52192 | val_1_rmse: 0.52546 |  0:01:45s
epoch 73 | loss: 0.30059 | val_0_rmse: 0.52703 | val_1_rmse: 0.52638 |  0:01:46s
epoch 74 | loss: 0.30744 | val_0_rmse: 0.57324 | val_1_rmse: 0.56716 |  0:01:48s
epoch 75 | loss: 0.32095 | val_0_rmse: 0.60131 | val_1_rmse: 0.5904  |  0:01:49s
epoch 76 | loss: 0.31163 | val_0_rmse: 0.53584 | val_1_rmse: 0.53684 |  0:01:51s
epoch 77 | loss: 0.3029  | val_0_rmse: 0.52345 | val_1_rmse: 0.52539 |  0:01:52s
epoch 78 | loss: 0.31279 | val_0_rmse: 0.61041 | val_1_rmse: 0.60376 |  0:01:54s
epoch 79 | loss: 0.31369 | val_0_rmse: 0.60886 | val_1_rmse: 0.61589 |  0:01:55s
epoch 80 | loss: 0.31491 | val_0_rmse: 0.64125 | val_1_rmse: 0.63202 |  0:01:56s
epoch 81 | loss: 0.30869 | val_0_rmse: 0.56781 | val_1_rmse: 0.57498 |  0:01:58s
epoch 82 | loss: 0.31117 | val_0_rmse: 0.55502 | val_1_rmse: 0.55361 |  0:01:59s
epoch 83 | loss: 0.31707 | val_0_rmse: 0.55486 | val_1_rmse: 0.55776 |  0:02:01s
epoch 84 | loss: 0.30784 | val_0_rmse: 0.55912 | val_1_rmse: 0.55541 |  0:02:02s
epoch 85 | loss: 0.32755 | val_0_rmse: 0.54745 | val_1_rmse: 0.54693 |  0:02:04s
epoch 86 | loss: 0.32954 | val_0_rmse: 0.66292 | val_1_rmse: 0.65208 |  0:02:05s
epoch 87 | loss: 0.32603 | val_0_rmse: 0.53712 | val_1_rmse: 0.53813 |  0:02:06s
epoch 88 | loss: 0.34278 | val_0_rmse: 0.61202 | val_1_rmse: 0.60998 |  0:02:08s
epoch 89 | loss: 0.35966 | val_0_rmse: 0.57846 | val_1_rmse: 0.56919 |  0:02:09s
epoch 90 | loss: 0.34222 | val_0_rmse: 0.61467 | val_1_rmse: 0.61516 |  0:02:11s
epoch 91 | loss: 0.33349 | val_0_rmse: 0.55606 | val_1_rmse: 0.55483 |  0:02:12s
epoch 92 | loss: 0.33218 | val_0_rmse: 0.55478 | val_1_rmse: 0.54913 |  0:02:14s
epoch 93 | loss: 0.32043 | val_0_rmse: 0.55457 | val_1_rmse: 0.55352 |  0:02:15s
epoch 94 | loss: 0.33156 | val_0_rmse: 0.55422 | val_1_rmse: 0.55075 |  0:02:17s
epoch 95 | loss: 0.33513 | val_0_rmse: 0.56837 | val_1_rmse: 0.56058 |  0:02:18s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.5234
Best weights from best epoch are automatically used!
ended training at: 05:13:51
Feature importance:
[('Area', 0.27409291568403116), ('Baths', 0.1006684109446683), ('Beds', 0.004728025707237825), ('Latitude', 0.271685607092213), ('Longitude', 0.21758011476821776), ('Month', 0.00014199849077935333), ('Year', 0.13110292731285259)]
Mean squared error is of 6687745241.056364
Mean absolute error:56536.521146868916
MAPE:0.18763825392428268
R2 score:0.7087878419373882
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:51
epoch 0  | loss: 0.78182 | val_0_rmse: 0.81343 | val_1_rmse: 0.80282 |  0:00:01s
epoch 1  | loss: 0.50475 | val_0_rmse: 0.67753 | val_1_rmse: 0.66241 |  0:00:02s
epoch 2  | loss: 0.43742 | val_0_rmse: 0.62874 | val_1_rmse: 0.60621 |  0:00:04s
epoch 3  | loss: 0.40338 | val_0_rmse: 0.67672 | val_1_rmse: 0.67427 |  0:00:05s
epoch 4  | loss: 0.40468 | val_0_rmse: 0.60275 | val_1_rmse: 0.58118 |  0:00:07s
epoch 5  | loss: 0.39481 | val_0_rmse: 0.65159 | val_1_rmse: 0.63926 |  0:00:08s
epoch 6  | loss: 0.4261  | val_0_rmse: 0.62045 | val_1_rmse: 0.60383 |  0:00:10s
epoch 7  | loss: 0.39736 | val_0_rmse: 0.59771 | val_1_rmse: 0.59255 |  0:00:11s
epoch 8  | loss: 0.37223 | val_0_rmse: 0.59145 | val_1_rmse: 0.57913 |  0:00:13s
epoch 9  | loss: 0.39837 | val_0_rmse: 0.6098  | val_1_rmse: 0.60187 |  0:00:14s
epoch 10 | loss: 0.40104 | val_0_rmse: 0.60097 | val_1_rmse: 0.5908  |  0:00:15s
epoch 11 | loss: 0.38744 | val_0_rmse: 0.61468 | val_1_rmse: 0.60605 |  0:00:17s
epoch 12 | loss: 0.37291 | val_0_rmse: 0.59141 | val_1_rmse: 0.57689 |  0:00:18s
epoch 13 | loss: 0.35666 | val_0_rmse: 0.56531 | val_1_rmse: 0.55057 |  0:00:20s
epoch 14 | loss: 0.35607 | val_0_rmse: 0.57281 | val_1_rmse: 0.55473 |  0:00:21s
epoch 15 | loss: 0.36468 | val_0_rmse: 0.56744 | val_1_rmse: 0.55307 |  0:00:23s
epoch 16 | loss: 0.35827 | val_0_rmse: 0.581   | val_1_rmse: 0.57133 |  0:00:24s
epoch 17 | loss: 0.35275 | val_0_rmse: 0.57934 | val_1_rmse: 0.57179 |  0:00:26s
epoch 18 | loss: 0.34945 | val_0_rmse: 0.55313 | val_1_rmse: 0.53981 |  0:00:27s
epoch 19 | loss: 0.34327 | val_0_rmse: 0.55966 | val_1_rmse: 0.54126 |  0:00:28s
epoch 20 | loss: 0.33791 | val_0_rmse: 0.61803 | val_1_rmse: 0.60649 |  0:00:30s
epoch 21 | loss: 0.33787 | val_0_rmse: 0.58974 | val_1_rmse: 0.58459 |  0:00:31s
epoch 22 | loss: 0.33194 | val_0_rmse: 0.56072 | val_1_rmse: 0.54355 |  0:00:33s
epoch 23 | loss: 0.32973 | val_0_rmse: 0.58326 | val_1_rmse: 0.57058 |  0:00:34s
epoch 24 | loss: 0.33971 | val_0_rmse: 0.55159 | val_1_rmse: 0.53927 |  0:00:36s
epoch 25 | loss: 0.33264 | val_0_rmse: 0.55873 | val_1_rmse: 0.54986 |  0:00:37s
epoch 26 | loss: 0.33281 | val_0_rmse: 0.56084 | val_1_rmse: 0.55026 |  0:00:39s
epoch 27 | loss: 0.33529 | val_0_rmse: 0.55263 | val_1_rmse: 0.54508 |  0:00:40s
epoch 28 | loss: 0.33863 | val_0_rmse: 0.55511 | val_1_rmse: 0.54838 |  0:00:41s
epoch 29 | loss: 0.32979 | val_0_rmse: 0.5608  | val_1_rmse: 0.55464 |  0:00:43s
epoch 30 | loss: 0.31804 | val_0_rmse: 0.54662 | val_1_rmse: 0.54091 |  0:00:44s
epoch 31 | loss: 0.33464 | val_0_rmse: 0.55557 | val_1_rmse: 0.54764 |  0:00:46s
epoch 32 | loss: 0.3206  | val_0_rmse: 0.55051 | val_1_rmse: 0.53997 |  0:00:47s
epoch 33 | loss: 0.31793 | val_0_rmse: 0.53882 | val_1_rmse: 0.53648 |  0:00:49s
epoch 34 | loss: 0.32413 | val_0_rmse: 0.56087 | val_1_rmse: 0.54965 |  0:00:50s
epoch 35 | loss: 0.32269 | val_0_rmse: 0.53858 | val_1_rmse: 0.53431 |  0:00:52s
epoch 36 | loss: 0.32368 | val_0_rmse: 0.53948 | val_1_rmse: 0.52911 |  0:00:53s
epoch 37 | loss: 0.31723 | val_0_rmse: 0.6225  | val_1_rmse: 0.60939 |  0:00:54s
epoch 38 | loss: 0.32556 | val_0_rmse: 0.54683 | val_1_rmse: 0.53598 |  0:00:56s
epoch 39 | loss: 0.31442 | val_0_rmse: 0.5572  | val_1_rmse: 0.54504 |  0:00:57s
epoch 40 | loss: 0.31204 | val_0_rmse: 0.55743 | val_1_rmse: 0.54253 |  0:00:59s
epoch 41 | loss: 0.31417 | val_0_rmse: 0.53101 | val_1_rmse: 0.52479 |  0:01:00s
epoch 42 | loss: 0.31982 | val_0_rmse: 0.54755 | val_1_rmse: 0.53617 |  0:01:02s
epoch 43 | loss: 0.30893 | val_0_rmse: 0.54188 | val_1_rmse: 0.53179 |  0:01:03s
epoch 44 | loss: 0.3079  | val_0_rmse: 0.5468  | val_1_rmse: 0.54292 |  0:01:05s
epoch 45 | loss: 0.30487 | val_0_rmse: 0.58592 | val_1_rmse: 0.57703 |  0:01:06s
epoch 46 | loss: 0.31728 | val_0_rmse: 0.53913 | val_1_rmse: 0.53037 |  0:01:07s
epoch 47 | loss: 0.30802 | val_0_rmse: 0.53613 | val_1_rmse: 0.53288 |  0:01:09s
epoch 48 | loss: 0.31663 | val_0_rmse: 0.54671 | val_1_rmse: 0.53529 |  0:01:10s
epoch 49 | loss: 0.31139 | val_0_rmse: 0.56761 | val_1_rmse: 0.55796 |  0:01:12s
epoch 50 | loss: 0.31061 | val_0_rmse: 0.54387 | val_1_rmse: 0.53486 |  0:01:13s
epoch 51 | loss: 0.30479 | val_0_rmse: 0.53671 | val_1_rmse: 0.52875 |  0:01:15s
epoch 52 | loss: 0.30624 | val_0_rmse: 0.53814 | val_1_rmse: 0.53148 |  0:01:16s
epoch 53 | loss: 0.31081 | val_0_rmse: 0.54482 | val_1_rmse: 0.54199 |  0:01:18s
epoch 54 | loss: 0.30573 | val_0_rmse: 0.53184 | val_1_rmse: 0.52186 |  0:01:19s
epoch 55 | loss: 0.30652 | val_0_rmse: 0.52892 | val_1_rmse: 0.52497 |  0:01:20s
epoch 56 | loss: 0.30292 | val_0_rmse: 0.52713 | val_1_rmse: 0.51946 |  0:01:22s
epoch 57 | loss: 0.30732 | val_0_rmse: 0.56633 | val_1_rmse: 0.56161 |  0:01:23s
epoch 58 | loss: 0.3198  | val_0_rmse: 0.54747 | val_1_rmse: 0.53389 |  0:01:25s
epoch 59 | loss: 0.30983 | val_0_rmse: 0.60316 | val_1_rmse: 0.59667 |  0:01:26s
epoch 60 | loss: 0.30107 | val_0_rmse: 0.53006 | val_1_rmse: 0.52278 |  0:01:28s
epoch 61 | loss: 0.30269 | val_0_rmse: 0.52158 | val_1_rmse: 0.51233 |  0:01:29s
epoch 62 | loss: 0.2971  | val_0_rmse: 0.5296  | val_1_rmse: 0.52389 |  0:01:31s
epoch 63 | loss: 0.2985  | val_0_rmse: 0.53594 | val_1_rmse: 0.52726 |  0:01:32s
epoch 64 | loss: 0.29893 | val_0_rmse: 0.5316  | val_1_rmse: 0.52428 |  0:01:33s
epoch 65 | loss: 0.30387 | val_0_rmse: 0.5462  | val_1_rmse: 0.54433 |  0:01:35s
epoch 66 | loss: 0.29898 | val_0_rmse: 0.53502 | val_1_rmse: 0.53064 |  0:01:36s
epoch 67 | loss: 0.30166 | val_0_rmse: 0.52665 | val_1_rmse: 0.51821 |  0:01:38s
epoch 68 | loss: 0.29448 | val_0_rmse: 0.51367 | val_1_rmse: 0.5118  |  0:01:39s
epoch 69 | loss: 0.29766 | val_0_rmse: 0.57787 | val_1_rmse: 0.56832 |  0:01:41s
epoch 70 | loss: 0.29909 | val_0_rmse: 0.5298  | val_1_rmse: 0.52681 |  0:01:42s
epoch 71 | loss: 0.30196 | val_0_rmse: 0.55835 | val_1_rmse: 0.55698 |  0:01:43s
epoch 72 | loss: 0.30375 | val_0_rmse: 0.57751 | val_1_rmse: 0.57118 |  0:01:45s
epoch 73 | loss: 0.30326 | val_0_rmse: 0.52672 | val_1_rmse: 0.52928 |  0:01:46s
epoch 74 | loss: 0.3016  | val_0_rmse: 0.51908 | val_1_rmse: 0.51737 |  0:01:48s
epoch 75 | loss: 0.29911 | val_0_rmse: 0.51921 | val_1_rmse: 0.51357 |  0:01:49s
epoch 76 | loss: 0.29275 | val_0_rmse: 0.52793 | val_1_rmse: 0.52075 |  0:01:51s
epoch 77 | loss: 0.29864 | val_0_rmse: 0.54988 | val_1_rmse: 0.54879 |  0:01:52s
epoch 78 | loss: 0.2963  | val_0_rmse: 0.53458 | val_1_rmse: 0.5327  |  0:01:54s
epoch 79 | loss: 0.29574 | val_0_rmse: 0.59621 | val_1_rmse: 0.5971  |  0:01:55s
epoch 80 | loss: 0.30509 | val_0_rmse: 0.54334 | val_1_rmse: 0.53962 |  0:01:56s
epoch 81 | loss: 0.30164 | val_0_rmse: 0.53637 | val_1_rmse: 0.53364 |  0:01:58s
epoch 82 | loss: 0.28851 | val_0_rmse: 0.52708 | val_1_rmse: 0.52388 |  0:01:59s
epoch 83 | loss: 0.28778 | val_0_rmse: 0.51229 | val_1_rmse: 0.50806 |  0:02:01s
epoch 84 | loss: 0.29627 | val_0_rmse: 0.53077 | val_1_rmse: 0.5251  |  0:02:02s
epoch 85 | loss: 0.29623 | val_0_rmse: 0.55244 | val_1_rmse: 0.54937 |  0:02:04s
epoch 86 | loss: 0.2953  | val_0_rmse: 0.52857 | val_1_rmse: 0.52834 |  0:02:05s
epoch 87 | loss: 0.29265 | val_0_rmse: 0.58227 | val_1_rmse: 0.58023 |  0:02:06s
epoch 88 | loss: 0.29705 | val_0_rmse: 0.5282  | val_1_rmse: 0.52351 |  0:02:08s
epoch 89 | loss: 0.29509 | val_0_rmse: 0.53329 | val_1_rmse: 0.53067 |  0:02:09s
epoch 90 | loss: 0.29635 | val_0_rmse: 0.53429 | val_1_rmse: 0.53289 |  0:02:11s
epoch 91 | loss: 0.29434 | val_0_rmse: 0.53388 | val_1_rmse: 0.5283  |  0:02:12s
epoch 92 | loss: 0.29479 | val_0_rmse: 0.5338  | val_1_rmse: 0.52947 |  0:02:14s
epoch 93 | loss: 0.29535 | val_0_rmse: 0.51713 | val_1_rmse: 0.5117  |  0:02:15s
epoch 94 | loss: 0.29567 | val_0_rmse: 0.51376 | val_1_rmse: 0.50969 |  0:02:17s
epoch 95 | loss: 0.28689 | val_0_rmse: 0.52188 | val_1_rmse: 0.51936 |  0:02:18s
epoch 96 | loss: 0.28151 | val_0_rmse: 0.5114  | val_1_rmse: 0.50797 |  0:02:19s
epoch 97 | loss: 0.29831 | val_0_rmse: 0.52171 | val_1_rmse: 0.52349 |  0:02:21s
epoch 98 | loss: 0.29672 | val_0_rmse: 0.52919 | val_1_rmse: 0.52835 |  0:02:22s
epoch 99 | loss: 0.29104 | val_0_rmse: 0.52689 | val_1_rmse: 0.52565 |  0:02:24s
epoch 100| loss: 0.28844 | val_0_rmse: 0.5296  | val_1_rmse: 0.53161 |  0:02:25s
epoch 101| loss: 0.29294 | val_0_rmse: 0.52175 | val_1_rmse: 0.52039 |  0:02:27s
epoch 102| loss: 0.30516 | val_0_rmse: 0.5304  | val_1_rmse: 0.52831 |  0:02:28s
epoch 103| loss: 0.30321 | val_0_rmse: 0.52647 | val_1_rmse: 0.5233  |  0:02:29s
epoch 104| loss: 0.30217 | val_0_rmse: 0.52404 | val_1_rmse: 0.52139 |  0:02:31s
epoch 105| loss: 0.29692 | val_0_rmse: 0.54254 | val_1_rmse: 0.53966 |  0:02:32s
epoch 106| loss: 0.29167 | val_0_rmse: 0.57117 | val_1_rmse: 0.56025 |  0:02:34s
epoch 107| loss: 0.29248 | val_0_rmse: 0.52607 | val_1_rmse: 0.52291 |  0:02:35s
epoch 108| loss: 0.28681 | val_0_rmse: 0.51661 | val_1_rmse: 0.5157  |  0:02:37s
epoch 109| loss: 0.29118 | val_0_rmse: 0.59438 | val_1_rmse: 0.59418 |  0:02:38s
epoch 110| loss: 0.30119 | val_0_rmse: 0.57359 | val_1_rmse: 0.56131 |  0:02:40s
epoch 111| loss: 0.29111 | val_0_rmse: 0.52381 | val_1_rmse: 0.52089 |  0:02:41s
epoch 112| loss: 0.29839 | val_0_rmse: 0.63732 | val_1_rmse: 0.63372 |  0:02:42s
epoch 113| loss: 0.29844 | val_0_rmse: 0.5766  | val_1_rmse: 0.57306 |  0:02:44s
epoch 114| loss: 0.30609 | val_0_rmse: 0.54906 | val_1_rmse: 0.54309 |  0:02:45s
epoch 115| loss: 0.29708 | val_0_rmse: 0.52339 | val_1_rmse: 0.51734 |  0:02:47s
epoch 116| loss: 0.30157 | val_0_rmse: 0.51902 | val_1_rmse: 0.51105 |  0:02:48s
epoch 117| loss: 0.29689 | val_0_rmse: 0.5254  | val_1_rmse: 0.51791 |  0:02:50s
epoch 118| loss: 0.29974 | val_0_rmse: 0.52109 | val_1_rmse: 0.5191  |  0:02:51s
epoch 119| loss: 0.30064 | val_0_rmse: 0.54189 | val_1_rmse: 0.53879 |  0:02:53s
epoch 120| loss: 0.29055 | val_0_rmse: 0.51351 | val_1_rmse: 0.51282 |  0:02:54s
epoch 121| loss: 0.29451 | val_0_rmse: 0.52781 | val_1_rmse: 0.5177  |  0:02:55s
epoch 122| loss: 0.28875 | val_0_rmse: 0.5984  | val_1_rmse: 0.58757 |  0:02:57s
epoch 123| loss: 0.28922 | val_0_rmse: 0.5433  | val_1_rmse: 0.53992 |  0:02:58s
epoch 124| loss: 0.29522 | val_0_rmse: 0.50986 | val_1_rmse: 0.50427 |  0:03:00s
epoch 125| loss: 0.2834  | val_0_rmse: 0.51417 | val_1_rmse: 0.51443 |  0:03:01s
epoch 126| loss: 0.28418 | val_0_rmse: 0.52801 | val_1_rmse: 0.52792 |  0:03:03s
epoch 127| loss: 0.28094 | val_0_rmse: 0.52656 | val_1_rmse: 0.52424 |  0:03:04s
epoch 128| loss: 0.29728 | val_0_rmse: 0.54999 | val_1_rmse: 0.55353 |  0:03:05s
epoch 129| loss: 0.30206 | val_0_rmse: 0.53083 | val_1_rmse: 0.52058 |  0:03:07s
epoch 130| loss: 0.32066 | val_0_rmse: 0.56739 | val_1_rmse: 0.56647 |  0:03:08s
epoch 131| loss: 0.35747 | val_0_rmse: 0.62592 | val_1_rmse: 0.61778 |  0:03:10s
epoch 132| loss: 0.33516 | val_0_rmse: 0.58479 | val_1_rmse: 0.56897 |  0:03:11s
epoch 133| loss: 0.31028 | val_0_rmse: 0.60221 | val_1_rmse: 0.59751 |  0:03:13s
epoch 134| loss: 0.30094 | val_0_rmse: 0.54005 | val_1_rmse: 0.53079 |  0:03:14s
epoch 135| loss: 0.28835 | val_0_rmse: 0.52474 | val_1_rmse: 0.51778 |  0:03:15s
epoch 136| loss: 0.30466 | val_0_rmse: 0.56682 | val_1_rmse: 0.56125 |  0:03:17s
epoch 137| loss: 0.31416 | val_0_rmse: 0.57691 | val_1_rmse: 0.57532 |  0:03:18s
epoch 138| loss: 0.3077  | val_0_rmse: 0.5352  | val_1_rmse: 0.53131 |  0:03:20s
epoch 139| loss: 0.3027  | val_0_rmse: 0.52362 | val_1_rmse: 0.51898 |  0:03:21s
epoch 140| loss: 0.30221 | val_0_rmse: 0.67504 | val_1_rmse: 0.6748  |  0:03:23s
epoch 141| loss: 0.32455 | val_0_rmse: 0.56068 | val_1_rmse: 0.55803 |  0:03:24s
epoch 142| loss: 0.30063 | val_0_rmse: 0.52979 | val_1_rmse: 0.52231 |  0:03:26s
epoch 143| loss: 0.30053 | val_0_rmse: 0.52394 | val_1_rmse: 0.51829 |  0:03:27s
epoch 144| loss: 0.30149 | val_0_rmse: 0.55928 | val_1_rmse: 0.55206 |  0:03:28s
epoch 145| loss: 0.30802 | val_0_rmse: 0.59235 | val_1_rmse: 0.58387 |  0:03:30s
epoch 146| loss: 0.30079 | val_0_rmse: 0.55862 | val_1_rmse: 0.55368 |  0:03:31s
epoch 147| loss: 0.29595 | val_0_rmse: 0.56418 | val_1_rmse: 0.55397 |  0:03:33s
epoch 148| loss: 0.29992 | val_0_rmse: 0.52343 | val_1_rmse: 0.51328 |  0:03:34s
epoch 149| loss: 0.29802 | val_0_rmse: 0.56043 | val_1_rmse: 0.55135 |  0:03:36s
Stop training because you reached max_epochs = 150 with best_epoch = 124 and best_val_1_rmse = 0.50427
Best weights from best epoch are automatically used!
ended training at: 05:17:28
Feature importance:
[('Area', 0.380848248656998), ('Baths', 0.0), ('Beds', 0.021013371033743504), ('Latitude', 0.26174628360083374), ('Longitude', 0.19667459588054495), ('Month', 4.92412807068346e-07), ('Year', 0.13971700841507273)]
Mean squared error is of 6230446571.649594
Mean absolute error:54732.63532727921
MAPE:0.1833794016304816
R2 score:0.7144634465803348
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:17:28
epoch 0  | loss: 0.75679 | val_0_rmse: 0.78819 | val_1_rmse: 0.77343 |  0:00:01s
epoch 1  | loss: 0.5045  | val_0_rmse: 0.66775 | val_1_rmse: 0.67065 |  0:00:02s
epoch 2  | loss: 0.45874 | val_0_rmse: 0.64863 | val_1_rmse: 0.65575 |  0:00:04s
epoch 3  | loss: 0.43623 | val_0_rmse: 0.65234 | val_1_rmse: 0.65757 |  0:00:05s
epoch 4  | loss: 0.42121 | val_0_rmse: 0.63441 | val_1_rmse: 0.64279 |  0:00:07s
epoch 5  | loss: 0.39911 | val_0_rmse: 0.61037 | val_1_rmse: 0.62497 |  0:00:08s
epoch 6  | loss: 0.38556 | val_0_rmse: 0.59339 | val_1_rmse: 0.60085 |  0:00:10s
epoch 7  | loss: 0.37865 | val_0_rmse: 0.60058 | val_1_rmse: 0.61255 |  0:00:11s
epoch 8  | loss: 0.37194 | val_0_rmse: 0.5898  | val_1_rmse: 0.60182 |  0:00:13s
epoch 9  | loss: 0.36546 | val_0_rmse: 0.58222 | val_1_rmse: 0.58733 |  0:00:14s
epoch 10 | loss: 0.35937 | val_0_rmse: 0.58217 | val_1_rmse: 0.58904 |  0:00:15s
epoch 11 | loss: 0.36581 | val_0_rmse: 0.59888 | val_1_rmse: 0.60463 |  0:00:17s
epoch 12 | loss: 0.39079 | val_0_rmse: 0.58673 | val_1_rmse: 0.59623 |  0:00:18s
epoch 13 | loss: 0.36632 | val_0_rmse: 0.58314 | val_1_rmse: 0.58961 |  0:00:20s
epoch 14 | loss: 0.3567  | val_0_rmse: 0.57848 | val_1_rmse: 0.58457 |  0:00:21s
epoch 15 | loss: 0.35312 | val_0_rmse: 0.57685 | val_1_rmse: 0.59134 |  0:00:23s
epoch 16 | loss: 0.34971 | val_0_rmse: 0.56515 | val_1_rmse: 0.57366 |  0:00:24s
epoch 17 | loss: 0.33812 | val_0_rmse: 0.56956 | val_1_rmse: 0.57876 |  0:00:26s
epoch 18 | loss: 0.34521 | val_0_rmse: 0.57926 | val_1_rmse: 0.58454 |  0:00:27s
epoch 19 | loss: 0.34224 | val_0_rmse: 0.57022 | val_1_rmse: 0.58395 |  0:00:29s
epoch 20 | loss: 0.35636 | val_0_rmse: 0.58772 | val_1_rmse: 0.59312 |  0:00:30s
epoch 21 | loss: 0.35013 | val_0_rmse: 0.57772 | val_1_rmse: 0.59177 |  0:00:31s
epoch 22 | loss: 0.35065 | val_0_rmse: 0.57194 | val_1_rmse: 0.58416 |  0:00:33s
epoch 23 | loss: 0.34165 | val_0_rmse: 0.55791 | val_1_rmse: 0.56758 |  0:00:34s
epoch 24 | loss: 0.33542 | val_0_rmse: 0.56144 | val_1_rmse: 0.57634 |  0:00:36s
epoch 25 | loss: 0.33842 | val_0_rmse: 0.56011 | val_1_rmse: 0.57129 |  0:00:37s
epoch 26 | loss: 0.33685 | val_0_rmse: 0.55797 | val_1_rmse: 0.56812 |  0:00:39s
epoch 27 | loss: 0.33541 | val_0_rmse: 0.57934 | val_1_rmse: 0.59762 |  0:00:40s
epoch 28 | loss: 0.34726 | val_0_rmse: 0.57348 | val_1_rmse: 0.58738 |  0:00:42s
epoch 29 | loss: 0.33892 | val_0_rmse: 0.57659 | val_1_rmse: 0.5868  |  0:00:43s
epoch 30 | loss: 0.32814 | val_0_rmse: 0.55073 | val_1_rmse: 0.56133 |  0:00:44s
epoch 31 | loss: 0.32545 | val_0_rmse: 0.54895 | val_1_rmse: 0.56338 |  0:00:46s
epoch 32 | loss: 0.32634 | val_0_rmse: 0.54877 | val_1_rmse: 0.56477 |  0:00:47s
epoch 33 | loss: 0.32618 | val_0_rmse: 0.56153 | val_1_rmse: 0.58446 |  0:00:49s
epoch 34 | loss: 0.325   | val_0_rmse: 0.54598 | val_1_rmse: 0.56101 |  0:00:50s
epoch 35 | loss: 0.32013 | val_0_rmse: 0.55907 | val_1_rmse: 0.57114 |  0:00:52s
epoch 36 | loss: 0.3233  | val_0_rmse: 0.54512 | val_1_rmse: 0.56311 |  0:00:53s
epoch 37 | loss: 0.32138 | val_0_rmse: 0.55429 | val_1_rmse: 0.56862 |  0:00:55s
epoch 38 | loss: 0.31679 | val_0_rmse: 0.56324 | val_1_rmse: 0.57537 |  0:00:56s
epoch 39 | loss: 0.31837 | val_0_rmse: 0.5455  | val_1_rmse: 0.56104 |  0:00:57s
epoch 40 | loss: 0.3107  | val_0_rmse: 0.55206 | val_1_rmse: 0.56578 |  0:00:59s
epoch 41 | loss: 0.31451 | val_0_rmse: 0.54434 | val_1_rmse: 0.55614 |  0:01:00s
epoch 42 | loss: 0.32969 | val_0_rmse: 0.54586 | val_1_rmse: 0.56501 |  0:01:02s
epoch 43 | loss: 0.31566 | val_0_rmse: 0.54742 | val_1_rmse: 0.56639 |  0:01:03s
epoch 44 | loss: 0.31431 | val_0_rmse: 0.54999 | val_1_rmse: 0.56689 |  0:01:05s
epoch 45 | loss: 0.32227 | val_0_rmse: 0.53541 | val_1_rmse: 0.55037 |  0:01:06s
epoch 46 | loss: 0.31363 | val_0_rmse: 0.54776 | val_1_rmse: 0.55822 |  0:01:08s
epoch 47 | loss: 0.30691 | val_0_rmse: 0.52951 | val_1_rmse: 0.54513 |  0:01:09s
epoch 48 | loss: 0.30651 | val_0_rmse: 0.55184 | val_1_rmse: 0.56658 |  0:01:10s
epoch 49 | loss: 0.31545 | val_0_rmse: 0.55975 | val_1_rmse: 0.57875 |  0:01:12s
epoch 50 | loss: 0.32072 | val_0_rmse: 0.55186 | val_1_rmse: 0.56988 |  0:01:13s
epoch 51 | loss: 0.32049 | val_0_rmse: 0.56034 | val_1_rmse: 0.5742  |  0:01:15s
epoch 52 | loss: 0.31213 | val_0_rmse: 0.52915 | val_1_rmse: 0.54355 |  0:01:16s
epoch 53 | loss: 0.30744 | val_0_rmse: 0.53459 | val_1_rmse: 0.55105 |  0:01:18s
epoch 54 | loss: 0.30969 | val_0_rmse: 0.55032 | val_1_rmse: 0.56631 |  0:01:19s
epoch 55 | loss: 0.31536 | val_0_rmse: 0.5513  | val_1_rmse: 0.56611 |  0:01:21s
epoch 56 | loss: 0.30363 | val_0_rmse: 0.53907 | val_1_rmse: 0.55345 |  0:01:22s
epoch 57 | loss: 0.30844 | val_0_rmse: 0.55535 | val_1_rmse: 0.57506 |  0:01:23s
epoch 58 | loss: 0.30035 | val_0_rmse: 0.53403 | val_1_rmse: 0.55044 |  0:01:25s
epoch 59 | loss: 0.30065 | val_0_rmse: 0.52574 | val_1_rmse: 0.54315 |  0:01:26s
epoch 60 | loss: 0.3046  | val_0_rmse: 0.53164 | val_1_rmse: 0.54461 |  0:01:28s
epoch 61 | loss: 0.3017  | val_0_rmse: 0.56122 | val_1_rmse: 0.5715  |  0:01:29s
epoch 62 | loss: 0.3091  | val_0_rmse: 0.52848 | val_1_rmse: 0.5447  |  0:01:31s
epoch 63 | loss: 0.29769 | val_0_rmse: 0.53614 | val_1_rmse: 0.55323 |  0:01:32s
epoch 64 | loss: 0.29763 | val_0_rmse: 0.535   | val_1_rmse: 0.54927 |  0:01:33s
epoch 65 | loss: 0.30083 | val_0_rmse: 0.54455 | val_1_rmse: 0.55746 |  0:01:35s
epoch 66 | loss: 0.30349 | val_0_rmse: 0.52393 | val_1_rmse: 0.54175 |  0:01:36s
epoch 67 | loss: 0.29805 | val_0_rmse: 0.53239 | val_1_rmse: 0.54886 |  0:01:38s
epoch 68 | loss: 0.29881 | val_0_rmse: 0.52937 | val_1_rmse: 0.55034 |  0:01:39s
epoch 69 | loss: 0.301   | val_0_rmse: 0.53818 | val_1_rmse: 0.55666 |  0:01:41s
epoch 70 | loss: 0.31138 | val_0_rmse: 0.53747 | val_1_rmse: 0.55404 |  0:01:42s
epoch 71 | loss: 0.31109 | val_0_rmse: 0.542   | val_1_rmse: 0.55558 |  0:01:44s
epoch 72 | loss: 0.30972 | val_0_rmse: 0.53479 | val_1_rmse: 0.56019 |  0:01:45s
epoch 73 | loss: 0.29924 | val_0_rmse: 0.52911 | val_1_rmse: 0.54544 |  0:01:47s
epoch 74 | loss: 0.29549 | val_0_rmse: 0.52443 | val_1_rmse: 0.54595 |  0:01:48s
epoch 75 | loss: 0.29628 | val_0_rmse: 0.51903 | val_1_rmse: 0.53753 |  0:01:49s
epoch 76 | loss: 0.29696 | val_0_rmse: 0.53778 | val_1_rmse: 0.55822 |  0:01:51s
epoch 77 | loss: 0.2981  | val_0_rmse: 0.52767 | val_1_rmse: 0.54267 |  0:01:52s
epoch 78 | loss: 0.29658 | val_0_rmse: 0.51007 | val_1_rmse: 0.53109 |  0:01:54s
epoch 79 | loss: 0.28685 | val_0_rmse: 0.51568 | val_1_rmse: 0.53784 |  0:01:55s
epoch 80 | loss: 0.29049 | val_0_rmse: 0.52279 | val_1_rmse: 0.53873 |  0:01:57s
epoch 81 | loss: 0.29737 | val_0_rmse: 0.52736 | val_1_rmse: 0.54993 |  0:01:58s
epoch 82 | loss: 0.29619 | val_0_rmse: 0.51691 | val_1_rmse: 0.53449 |  0:02:00s
epoch 83 | loss: 0.29175 | val_0_rmse: 0.51248 | val_1_rmse: 0.53068 |  0:02:01s
epoch 84 | loss: 0.28531 | val_0_rmse: 0.51456 | val_1_rmse: 0.5377  |  0:02:02s
epoch 85 | loss: 0.29327 | val_0_rmse: 0.52664 | val_1_rmse: 0.55069 |  0:02:04s
epoch 86 | loss: 0.28878 | val_0_rmse: 0.51701 | val_1_rmse: 0.53877 |  0:02:05s
epoch 87 | loss: 0.28524 | val_0_rmse: 0.53799 | val_1_rmse: 0.55528 |  0:02:07s
epoch 88 | loss: 0.28652 | val_0_rmse: 0.51417 | val_1_rmse: 0.52973 |  0:02:08s
epoch 89 | loss: 0.28989 | val_0_rmse: 0.51378 | val_1_rmse: 0.53118 |  0:02:10s
epoch 90 | loss: 0.2932  | val_0_rmse: 0.51919 | val_1_rmse: 0.53882 |  0:02:11s
epoch 91 | loss: 0.29034 | val_0_rmse: 0.52617 | val_1_rmse: 0.54486 |  0:02:13s
epoch 92 | loss: 0.29401 | val_0_rmse: 0.50897 | val_1_rmse: 0.52939 |  0:02:14s
epoch 93 | loss: 0.28618 | val_0_rmse: 0.51234 | val_1_rmse: 0.53063 |  0:02:16s
epoch 94 | loss: 0.29249 | val_0_rmse: 0.51624 | val_1_rmse: 0.53768 |  0:02:17s
epoch 95 | loss: 0.28053 | val_0_rmse: 0.51454 | val_1_rmse: 0.53014 |  0:02:18s
epoch 96 | loss: 0.28693 | val_0_rmse: 0.51395 | val_1_rmse: 0.53282 |  0:02:20s
epoch 97 | loss: 0.28963 | val_0_rmse: 0.54943 | val_1_rmse: 0.57089 |  0:02:21s
epoch 98 | loss: 0.29432 | val_0_rmse: 0.5269  | val_1_rmse: 0.54908 |  0:02:23s
epoch 99 | loss: 0.28929 | val_0_rmse: 0.52083 | val_1_rmse: 0.54279 |  0:02:24s
epoch 100| loss: 0.28823 | val_0_rmse: 0.54323 | val_1_rmse: 0.56627 |  0:02:26s
epoch 101| loss: 0.28725 | val_0_rmse: 0.5007  | val_1_rmse: 0.52498 |  0:02:27s
epoch 102| loss: 0.28411 | val_0_rmse: 0.5109  | val_1_rmse: 0.52934 |  0:02:29s
epoch 103| loss: 0.28287 | val_0_rmse: 0.52074 | val_1_rmse: 0.5357  |  0:02:30s
epoch 104| loss: 0.28587 | val_0_rmse: 0.55162 | val_1_rmse: 0.57184 |  0:02:32s
epoch 105| loss: 0.29243 | val_0_rmse: 0.51216 | val_1_rmse: 0.53083 |  0:02:33s
epoch 106| loss: 0.28964 | val_0_rmse: 0.50996 | val_1_rmse: 0.52941 |  0:02:34s
epoch 107| loss: 0.28895 | val_0_rmse: 0.51791 | val_1_rmse: 0.53601 |  0:02:36s
epoch 108| loss: 0.29177 | val_0_rmse: 0.504   | val_1_rmse: 0.52338 |  0:02:37s
epoch 109| loss: 0.28821 | val_0_rmse: 0.51238 | val_1_rmse: 0.53315 |  0:02:39s
epoch 110| loss: 0.28374 | val_0_rmse: 0.53465 | val_1_rmse: 0.55306 |  0:02:40s
epoch 111| loss: 0.28711 | val_0_rmse: 0.52814 | val_1_rmse: 0.54995 |  0:02:42s
epoch 112| loss: 0.28729 | val_0_rmse: 0.50685 | val_1_rmse: 0.53126 |  0:02:43s
epoch 113| loss: 0.28414 | val_0_rmse: 0.50337 | val_1_rmse: 0.52925 |  0:02:45s
epoch 114| loss: 0.28599 | val_0_rmse: 0.50284 | val_1_rmse: 0.52893 |  0:02:46s
epoch 115| loss: 0.28229 | val_0_rmse: 0.51296 | val_1_rmse: 0.536   |  0:02:47s
epoch 116| loss: 0.27902 | val_0_rmse: 0.5055  | val_1_rmse: 0.53141 |  0:02:49s
epoch 117| loss: 0.2791  | val_0_rmse: 0.51667 | val_1_rmse: 0.53902 |  0:02:50s
epoch 118| loss: 0.28341 | val_0_rmse: 0.50442 | val_1_rmse: 0.5267  |  0:02:52s
epoch 119| loss: 0.27691 | val_0_rmse: 0.50298 | val_1_rmse: 0.53036 |  0:02:53s
epoch 120| loss: 0.27143 | val_0_rmse: 0.5016  | val_1_rmse: 0.52305 |  0:02:55s
epoch 121| loss: 0.27536 | val_0_rmse: 0.5056  | val_1_rmse: 0.53216 |  0:02:56s
epoch 122| loss: 0.27506 | val_0_rmse: 0.50415 | val_1_rmse: 0.52608 |  0:02:58s
epoch 123| loss: 0.28207 | val_0_rmse: 0.50517 | val_1_rmse: 0.52781 |  0:02:59s
epoch 124| loss: 0.27618 | val_0_rmse: 0.50837 | val_1_rmse: 0.52892 |  0:03:00s
epoch 125| loss: 0.27775 | val_0_rmse: 0.51525 | val_1_rmse: 0.53237 |  0:03:02s
epoch 126| loss: 0.28788 | val_0_rmse: 0.50255 | val_1_rmse: 0.52718 |  0:03:03s
epoch 127| loss: 0.27914 | val_0_rmse: 0.50233 | val_1_rmse: 0.52495 |  0:03:05s
epoch 128| loss: 0.27785 | val_0_rmse: 0.50408 | val_1_rmse: 0.52884 |  0:03:06s
epoch 129| loss: 0.27893 | val_0_rmse: 0.50666 | val_1_rmse: 0.53024 |  0:03:08s
epoch 130| loss: 0.27438 | val_0_rmse: 0.50256 | val_1_rmse: 0.52283 |  0:03:09s
epoch 131| loss: 0.27632 | val_0_rmse: 0.49967 | val_1_rmse: 0.52637 |  0:03:11s
epoch 132| loss: 0.28225 | val_0_rmse: 0.51753 | val_1_rmse: 0.53907 |  0:03:12s
epoch 133| loss: 0.28035 | val_0_rmse: 0.51482 | val_1_rmse: 0.54098 |  0:03:13s
epoch 134| loss: 0.27496 | val_0_rmse: 0.49529 | val_1_rmse: 0.51835 |  0:03:15s
epoch 135| loss: 0.27356 | val_0_rmse: 0.49485 | val_1_rmse: 0.52409 |  0:03:16s
epoch 136| loss: 0.27896 | val_0_rmse: 0.49574 | val_1_rmse: 0.52399 |  0:03:18s
epoch 137| loss: 0.27291 | val_0_rmse: 0.51907 | val_1_rmse: 0.54436 |  0:03:19s
epoch 138| loss: 0.28241 | val_0_rmse: 0.50726 | val_1_rmse: 0.52829 |  0:03:21s
epoch 139| loss: 0.27438 | val_0_rmse: 0.49406 | val_1_rmse: 0.52042 |  0:03:22s
epoch 140| loss: 0.26987 | val_0_rmse: 0.49833 | val_1_rmse: 0.5228  |  0:03:24s
epoch 141| loss: 0.27458 | val_0_rmse: 0.50416 | val_1_rmse: 0.52959 |  0:03:25s
epoch 142| loss: 0.27288 | val_0_rmse: 0.49653 | val_1_rmse: 0.52266 |  0:03:26s
epoch 143| loss: 0.2737  | val_0_rmse: 0.50364 | val_1_rmse: 0.52951 |  0:03:28s
epoch 144| loss: 0.26761 | val_0_rmse: 0.49412 | val_1_rmse: 0.51731 |  0:03:29s
epoch 145| loss: 0.27728 | val_0_rmse: 0.50513 | val_1_rmse: 0.52156 |  0:03:31s
epoch 146| loss: 0.27892 | val_0_rmse: 0.50757 | val_1_rmse: 0.53964 |  0:03:32s
epoch 147| loss: 0.27586 | val_0_rmse: 0.49825 | val_1_rmse: 0.52498 |  0:03:34s
epoch 148| loss: 0.26978 | val_0_rmse: 0.49719 | val_1_rmse: 0.52074 |  0:03:35s
epoch 149| loss: 0.27366 | val_0_rmse: 0.51646 | val_1_rmse: 0.54426 |  0:03:37s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.51731
Best weights from best epoch are automatically used!
ended training at: 05:21:05
Feature importance:
[('Area', 0.27653192495086987), ('Baths', 0.048250815447616106), ('Beds', 0.01736410806155317), ('Latitude', 0.2863044300130261), ('Longitude', 0.2722893863555247), ('Month', 0.009965402593665799), ('Year', 0.08929393257774426)]
Mean squared error is of 5993907688.756021
Mean absolute error:53298.725153511
MAPE:0.17038651847654343
R2 score:0.7375732460271651
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:21:06
epoch 0  | loss: 0.61051 | val_0_rmse: 0.72681 | val_1_rmse: 0.72899 |  0:00:05s
epoch 1  | loss: 0.49813 | val_0_rmse: 0.68953 | val_1_rmse: 0.69165 |  0:00:10s
epoch 2  | loss: 0.47924 | val_0_rmse: 0.6965  | val_1_rmse: 0.69955 |  0:00:16s
epoch 3  | loss: 0.4584  | val_0_rmse: 0.71428 | val_1_rmse: 0.7216  |  0:00:21s
epoch 4  | loss: 0.3971  | val_0_rmse: 0.61798 | val_1_rmse: 0.62539 |  0:00:27s
epoch 5  | loss: 0.37599 | val_0_rmse: 0.71165 | val_1_rmse: 0.71922 |  0:00:32s
epoch 6  | loss: 0.3807  | val_0_rmse: 0.61502 | val_1_rmse: 0.62341 |  0:00:38s
epoch 7  | loss: 0.37852 | val_0_rmse: 0.66992 | val_1_rmse: 0.67821 |  0:00:43s
epoch 8  | loss: 0.37108 | val_0_rmse: 0.62309 | val_1_rmse: 0.6289  |  0:00:49s
epoch 9  | loss: 0.3657  | val_0_rmse: 0.99295 | val_1_rmse: 1.00167 |  0:00:54s
epoch 10 | loss: 0.36355 | val_0_rmse: 0.67677 | val_1_rmse: 0.68431 |  0:01:00s
epoch 11 | loss: 0.36383 | val_0_rmse: 0.73423 | val_1_rmse: 0.74117 |  0:01:05s
epoch 12 | loss: 0.35526 | val_0_rmse: 0.71283 | val_1_rmse: 0.71998 |  0:01:11s
epoch 13 | loss: 0.35469 | val_0_rmse: 0.64564 | val_1_rmse: 0.65281 |  0:01:16s
epoch 14 | loss: 0.35055 | val_0_rmse: 0.64106 | val_1_rmse: 0.64757 |  0:01:21s
epoch 15 | loss: 0.35169 | val_0_rmse: 0.88711 | val_1_rmse: 0.89402 |  0:01:27s
epoch 16 | loss: 0.35132 | val_0_rmse: 0.74126 | val_1_rmse: 0.74857 |  0:01:32s
epoch 17 | loss: 0.35061 | val_0_rmse: 1.0178  | val_1_rmse: 1.02707 |  0:01:38s
epoch 18 | loss: 0.34898 | val_0_rmse: 0.77614 | val_1_rmse: 0.78635 |  0:01:43s
epoch 19 | loss: 0.34944 | val_0_rmse: 1.0908  | val_1_rmse: 1.10126 |  0:01:49s
epoch 20 | loss: 0.34865 | val_0_rmse: 0.66381 | val_1_rmse: 0.67017 |  0:01:54s
epoch 21 | loss: 0.35048 | val_0_rmse: 0.72644 | val_1_rmse: 0.73387 |  0:02:00s
epoch 22 | loss: 0.34887 | val_0_rmse: 0.5881  | val_1_rmse: 0.59456 |  0:02:05s
epoch 23 | loss: 0.34501 | val_0_rmse: 0.64891 | val_1_rmse: 0.65424 |  0:02:11s
epoch 24 | loss: 0.34534 | val_0_rmse: 0.62442 | val_1_rmse: 0.6343  |  0:02:16s
epoch 25 | loss: 0.34783 | val_0_rmse: 0.75619 | val_1_rmse: 0.7629  |  0:02:22s
epoch 26 | loss: 0.34608 | val_0_rmse: 1.28897 | val_1_rmse: 1.29991 |  0:02:27s
epoch 27 | loss: 0.34774 | val_0_rmse: 0.89445 | val_1_rmse: 0.90166 |  0:02:33s
epoch 28 | loss: 0.34546 | val_0_rmse: 0.82081 | val_1_rmse: 0.82695 |  0:02:38s
epoch 29 | loss: 0.34896 | val_0_rmse: 0.99527 | val_1_rmse: 1.0057  |  0:02:43s
epoch 30 | loss: 0.3467  | val_0_rmse: 0.69529 | val_1_rmse: 0.70139 |  0:02:49s
epoch 31 | loss: 0.34983 | val_0_rmse: 0.71119 | val_1_rmse: 0.71793 |  0:02:54s
epoch 32 | loss: 0.35442 | val_0_rmse: 1.81455 | val_1_rmse: 1.82479 |  0:03:00s
epoch 33 | loss: 0.34921 | val_0_rmse: 0.60664 | val_1_rmse: 0.61429 |  0:03:05s
epoch 34 | loss: 0.34833 | val_0_rmse: 0.60949 | val_1_rmse: 0.61749 |  0:03:11s
epoch 35 | loss: 0.34726 | val_0_rmse: 0.60165 | val_1_rmse: 0.60714 |  0:03:16s
epoch 36 | loss: 0.35421 | val_0_rmse: 0.59098 | val_1_rmse: 0.59642 |  0:03:22s
epoch 37 | loss: 0.35114 | val_0_rmse: 0.95664 | val_1_rmse: 0.96602 |  0:03:27s
epoch 38 | loss: 0.35171 | val_0_rmse: 0.63312 | val_1_rmse: 0.6405  |  0:03:33s
epoch 39 | loss: 0.35031 | val_0_rmse: 1.40235 | val_1_rmse: 1.41402 |  0:03:38s
epoch 40 | loss: 0.35525 | val_0_rmse: 1.87345 | val_1_rmse: 1.88533 |  0:03:44s
epoch 41 | loss: 0.35179 | val_0_rmse: 0.83419 | val_1_rmse: 0.84213 |  0:03:49s
epoch 42 | loss: 0.35108 | val_0_rmse: 0.59172 | val_1_rmse: 0.5993  |  0:03:54s
epoch 43 | loss: 0.35099 | val_0_rmse: 0.64389 | val_1_rmse: 0.6508  |  0:04:00s
epoch 44 | loss: 0.35004 | val_0_rmse: 0.96237 | val_1_rmse: 0.97241 |  0:04:05s
epoch 45 | loss: 0.34851 | val_0_rmse: 0.75731 | val_1_rmse: 0.76331 |  0:04:11s
epoch 46 | loss: 0.34572 | val_0_rmse: 0.71236 | val_1_rmse: 0.72007 |  0:04:16s
epoch 47 | loss: 0.3554  | val_0_rmse: 0.6271  | val_1_rmse: 0.63661 |  0:04:22s
epoch 48 | loss: 0.34999 | val_0_rmse: 0.94385 | val_1_rmse: 0.95327 |  0:04:27s
epoch 49 | loss: 0.35111 | val_0_rmse: 0.69167 | val_1_rmse: 0.70009 |  0:04:33s
epoch 50 | loss: 0.34634 | val_0_rmse: 0.78722 | val_1_rmse: 0.79396 |  0:04:38s
epoch 51 | loss: 0.34586 | val_0_rmse: 0.65916 | val_1_rmse: 0.6663  |  0:04:44s
epoch 52 | loss: 0.34384 | val_0_rmse: 0.8519  | val_1_rmse: 0.85905 |  0:04:49s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.59456
Best weights from best epoch are automatically used!
ended training at: 05:25:57
Feature importance:
[('Area', 0.30261074124743986), ('Baths', 0.22215852198737068), ('Beds', 0.0), ('Latitude', 0.2571953851094338), ('Longitude', 0.2180353516557557), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2351749082.4072185
Mean absolute error:35394.35528578274
MAPE:0.36342124202051135
R2 score:0.6567491793682028
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:25:58
epoch 0  | loss: 0.61002 | val_0_rmse: 0.72414 | val_1_rmse: 0.71752 |  0:00:05s
epoch 1  | loss: 0.50709 | val_0_rmse: 0.72853 | val_1_rmse: 0.72193 |  0:00:11s
epoch 2  | loss: 0.48238 | val_0_rmse: 0.6891  | val_1_rmse: 0.68672 |  0:00:17s
epoch 3  | loss: 0.46941 | val_0_rmse: 0.6927  | val_1_rmse: 0.68924 |  0:00:22s
epoch 4  | loss: 0.47353 | val_0_rmse: 0.70353 | val_1_rmse: 0.69902 |  0:00:27s
epoch 5  | loss: 0.46641 | val_0_rmse: 0.67567 | val_1_rmse: 0.6728  |  0:00:33s
epoch 6  | loss: 0.45855 | val_0_rmse: 0.67575 | val_1_rmse: 0.67357 |  0:00:38s
epoch 7  | loss: 0.45805 | val_0_rmse: 0.67433 | val_1_rmse: 0.67152 |  0:00:44s
epoch 8  | loss: 0.45347 | val_0_rmse: 0.69454 | val_1_rmse: 0.68883 |  0:00:49s
epoch 9  | loss: 0.45367 | val_0_rmse: 0.66921 | val_1_rmse: 0.66632 |  0:00:55s
epoch 10 | loss: 0.44674 | val_0_rmse: 0.71713 | val_1_rmse: 0.71429 |  0:01:00s
epoch 11 | loss: 0.43373 | val_0_rmse: 0.66755 | val_1_rmse: 0.66467 |  0:01:06s
epoch 12 | loss: 0.4323  | val_0_rmse: 0.6721  | val_1_rmse: 0.66962 |  0:01:11s
epoch 13 | loss: 0.42544 | val_0_rmse: 0.7038  | val_1_rmse: 0.70218 |  0:01:17s
epoch 14 | loss: 0.42788 | val_0_rmse: 0.65213 | val_1_rmse: 0.6526  |  0:01:22s
epoch 15 | loss: 0.42116 | val_0_rmse: 0.64613 | val_1_rmse: 0.64436 |  0:01:28s
epoch 16 | loss: 0.42106 | val_0_rmse: 0.65488 | val_1_rmse: 0.65072 |  0:01:33s
epoch 17 | loss: 0.41981 | val_0_rmse: 0.72663 | val_1_rmse: 0.72443 |  0:01:38s
epoch 18 | loss: 0.41863 | val_0_rmse: 0.74956 | val_1_rmse: 0.74861 |  0:01:44s
epoch 19 | loss: 0.41751 | val_0_rmse: 0.66434 | val_1_rmse: 0.66428 |  0:01:49s
epoch 20 | loss: 0.41685 | val_0_rmse: 0.67337 | val_1_rmse: 0.66734 |  0:01:55s
epoch 21 | loss: 0.41728 | val_0_rmse: 0.6401  | val_1_rmse: 0.63993 |  0:02:00s
epoch 22 | loss: 0.41341 | val_0_rmse: 0.6528  | val_1_rmse: 0.64907 |  0:02:06s
epoch 23 | loss: 0.41275 | val_0_rmse: 0.65086 | val_1_rmse: 0.64772 |  0:02:11s
epoch 24 | loss: 0.41325 | val_0_rmse: 0.68387 | val_1_rmse: 0.68428 |  0:02:17s
epoch 25 | loss: 0.40905 | val_0_rmse: 0.65004 | val_1_rmse: 0.64861 |  0:02:22s
epoch 26 | loss: 0.4081  | val_0_rmse: 0.675   | val_1_rmse: 0.6728  |  0:02:28s
epoch 27 | loss: 0.4074  | val_0_rmse: 0.71122 | val_1_rmse: 0.71175 |  0:02:33s
epoch 28 | loss: 0.40717 | val_0_rmse: 0.63359 | val_1_rmse: 0.63043 |  0:02:38s
epoch 29 | loss: 0.40488 | val_0_rmse: 0.70574 | val_1_rmse: 0.70559 |  0:02:44s
epoch 30 | loss: 0.404   | val_0_rmse: 0.64321 | val_1_rmse: 0.64369 |  0:02:49s
epoch 31 | loss: 0.40363 | val_0_rmse: 0.65669 | val_1_rmse: 0.65677 |  0:02:55s
epoch 32 | loss: 0.40428 | val_0_rmse: 0.69922 | val_1_rmse: 0.69853 |  0:03:00s
epoch 33 | loss: 0.40445 | val_0_rmse: 0.68259 | val_1_rmse: 0.68148 |  0:03:06s
epoch 34 | loss: 0.39909 | val_0_rmse: 0.6595  | val_1_rmse: 0.65671 |  0:03:11s
epoch 35 | loss: 0.38439 | val_0_rmse: 0.68618 | val_1_rmse: 0.6883  |  0:03:17s
epoch 36 | loss: 0.37536 | val_0_rmse: 0.64068 | val_1_rmse: 0.64018 |  0:03:22s
epoch 37 | loss: 0.37112 | val_0_rmse: 0.59147 | val_1_rmse: 0.59036 |  0:03:28s
epoch 38 | loss: 0.37042 | val_0_rmse: 0.60909 | val_1_rmse: 0.61027 |  0:03:33s
epoch 39 | loss: 0.36718 | val_0_rmse: 0.62127 | val_1_rmse: 0.61984 |  0:03:39s
epoch 40 | loss: 0.36443 | val_0_rmse: 0.58907 | val_1_rmse: 0.58845 |  0:03:44s
epoch 41 | loss: 0.36018 | val_0_rmse: 0.61041 | val_1_rmse: 0.6114  |  0:03:50s
epoch 42 | loss: 0.36437 | val_0_rmse: 0.5919  | val_1_rmse: 0.59233 |  0:03:55s
epoch 43 | loss: 0.35993 | val_0_rmse: 0.62135 | val_1_rmse: 0.6188  |  0:04:01s
epoch 44 | loss: 0.35821 | val_0_rmse: 0.65562 | val_1_rmse: 0.65491 |  0:04:06s
epoch 45 | loss: 0.35988 | val_0_rmse: 0.63922 | val_1_rmse: 0.63941 |  0:04:11s
epoch 46 | loss: 0.35784 | val_0_rmse: 0.69751 | val_1_rmse: 0.69511 |  0:04:17s
epoch 47 | loss: 0.3614  | val_0_rmse: 0.62782 | val_1_rmse: 0.6272  |  0:04:22s
epoch 48 | loss: 0.35546 | val_0_rmse: 0.61035 | val_1_rmse: 0.60914 |  0:04:28s
epoch 49 | loss: 0.36165 | val_0_rmse: 0.64033 | val_1_rmse: 0.64135 |  0:04:33s
epoch 50 | loss: 0.35307 | val_0_rmse: 0.62159 | val_1_rmse: 0.62394 |  0:04:39s
epoch 51 | loss: 0.35479 | val_0_rmse: 0.63428 | val_1_rmse: 0.63708 |  0:04:44s
epoch 52 | loss: 0.35229 | val_0_rmse: 0.62301 | val_1_rmse: 0.62496 |  0:04:50s
epoch 53 | loss: 0.36307 | val_0_rmse: 0.61833 | val_1_rmse: 0.61865 |  0:04:55s
epoch 54 | loss: 0.36281 | val_0_rmse: 0.67869 | val_1_rmse: 0.67903 |  0:05:01s
epoch 55 | loss: 0.35024 | val_0_rmse: 0.62214 | val_1_rmse: 0.62305 |  0:05:06s
epoch 56 | loss: 0.35266 | val_0_rmse: 0.61382 | val_1_rmse: 0.61256 |  0:05:12s
epoch 57 | loss: 0.34556 | val_0_rmse: 0.61053 | val_1_rmse: 0.60946 |  0:05:17s
epoch 58 | loss: 0.346   | val_0_rmse: 0.61206 | val_1_rmse: 0.61313 |  0:05:23s
epoch 59 | loss: 0.34662 | val_0_rmse: 0.65943 | val_1_rmse: 0.6603  |  0:05:28s
epoch 60 | loss: 0.3475  | val_0_rmse: 0.67159 | val_1_rmse: 0.66836 |  0:05:33s
epoch 61 | loss: 0.34359 | val_0_rmse: 0.62062 | val_1_rmse: 0.62081 |  0:05:39s
epoch 62 | loss: 0.35133 | val_0_rmse: 0.65613 | val_1_rmse: 0.65825 |  0:05:44s
epoch 63 | loss: 0.34592 | val_0_rmse: 0.62376 | val_1_rmse: 0.62307 |  0:05:50s
epoch 64 | loss: 0.34438 | val_0_rmse: 0.59334 | val_1_rmse: 0.59426 |  0:05:55s
epoch 65 | loss: 0.34466 | val_0_rmse: 0.6031  | val_1_rmse: 0.60464 |  0:06:01s
epoch 66 | loss: 0.34123 | val_0_rmse: 0.65174 | val_1_rmse: 0.65445 |  0:06:06s
epoch 67 | loss: 0.34584 | val_0_rmse: 0.73738 | val_1_rmse: 0.73625 |  0:06:12s
epoch 68 | loss: 0.33933 | val_0_rmse: 0.64487 | val_1_rmse: 0.64536 |  0:06:17s
epoch 69 | loss: 0.34208 | val_0_rmse: 0.70836 | val_1_rmse: 0.70921 |  0:06:23s
epoch 70 | loss: 0.3681  | val_0_rmse: 0.644   | val_1_rmse: 0.6428  |  0:06:28s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.58845
Best weights from best epoch are automatically used!
ended training at: 05:32:28
Feature importance:
[('Area', 0.3324950307305118), ('Baths', 0.2386469221435262), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.3844444302341872), ('Month', 0.0), ('Year', 0.04441361689177488)]
Mean squared error is of 2428227186.6195436
Mean absolute error:35728.06009249013
MAPE:0.35831299360116614
R2 score:0.6472371595956136
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:32:29
epoch 0  | loss: 0.58873 | val_0_rmse: 0.71197 | val_1_rmse: 0.72078 |  0:00:05s
epoch 1  | loss: 0.46589 | val_0_rmse: 0.66073 | val_1_rmse: 0.67114 |  0:00:11s
epoch 2  | loss: 0.40082 | val_0_rmse: 0.64718 | val_1_rmse: 0.655   |  0:00:16s
epoch 3  | loss: 0.39112 | val_0_rmse: 0.61108 | val_1_rmse: 0.61744 |  0:00:21s
epoch 4  | loss: 0.38341 | val_0_rmse: 0.61681 | val_1_rmse: 0.62677 |  0:00:27s
epoch 5  | loss: 0.38637 | val_0_rmse: 0.67355 | val_1_rmse: 0.68077 |  0:00:32s
epoch 6  | loss: 0.37252 | val_0_rmse: 0.67248 | val_1_rmse: 0.68127 |  0:00:38s
epoch 7  | loss: 0.3714  | val_0_rmse: 0.97907 | val_1_rmse: 0.97781 |  0:00:43s
epoch 8  | loss: 0.36786 | val_0_rmse: 0.7416  | val_1_rmse: 0.74905 |  0:00:49s
epoch 9  | loss: 0.36434 | val_0_rmse: 0.62743 | val_1_rmse: 0.6372  |  0:00:54s
epoch 10 | loss: 0.36485 | val_0_rmse: 0.58811 | val_1_rmse: 0.59711 |  0:01:00s
epoch 11 | loss: 0.35996 | val_0_rmse: 0.64206 | val_1_rmse: 0.65229 |  0:01:05s
epoch 12 | loss: 0.35769 | val_0_rmse: 0.58963 | val_1_rmse: 0.59902 |  0:01:11s
epoch 13 | loss: 0.35275 | val_0_rmse: 0.59374 | val_1_rmse: 0.60068 |  0:01:16s
epoch 14 | loss: 0.35251 | val_0_rmse: 0.63484 | val_1_rmse: 0.64167 |  0:01:22s
epoch 15 | loss: 0.35308 | val_0_rmse: 0.64011 | val_1_rmse: 0.6513  |  0:01:27s
epoch 16 | loss: 0.35112 | val_0_rmse: 0.65568 | val_1_rmse: 0.66655 |  0:01:32s
epoch 17 | loss: 0.34731 | val_0_rmse: 0.62873 | val_1_rmse: 0.63433 |  0:01:38s
epoch 18 | loss: 0.34666 | val_0_rmse: 0.58297 | val_1_rmse: 0.58895 |  0:01:43s
epoch 19 | loss: 0.34182 | val_0_rmse: 0.65004 | val_1_rmse: 0.65639 |  0:01:49s
epoch 20 | loss: 0.34359 | val_0_rmse: 0.60461 | val_1_rmse: 0.61477 |  0:01:54s
epoch 21 | loss: 0.34067 | val_0_rmse: 0.6189  | val_1_rmse: 0.62881 |  0:02:00s
epoch 22 | loss: 0.34287 | val_0_rmse: 0.61233 | val_1_rmse: 0.62136 |  0:02:05s
epoch 23 | loss: 0.34145 | val_0_rmse: 0.65345 | val_1_rmse: 0.66352 |  0:02:11s
epoch 24 | loss: 0.33965 | val_0_rmse: 0.60091 | val_1_rmse: 0.61077 |  0:02:16s
epoch 25 | loss: 0.33812 | val_0_rmse: 0.6391  | val_1_rmse: 0.64695 |  0:02:22s
epoch 26 | loss: 0.33485 | val_0_rmse: 0.58113 | val_1_rmse: 0.59097 |  0:02:27s
epoch 27 | loss: 0.33615 | val_0_rmse: 0.72068 | val_1_rmse: 0.72571 |  0:02:33s
epoch 28 | loss: 0.3372  | val_0_rmse: 0.6026  | val_1_rmse: 0.61231 |  0:02:38s
epoch 29 | loss: 0.33327 | val_0_rmse: 0.75101 | val_1_rmse: 0.75879 |  0:02:44s
epoch 30 | loss: 0.33587 | val_0_rmse: 0.73058 | val_1_rmse: 0.7341  |  0:02:49s
epoch 31 | loss: 0.3353  | val_0_rmse: 0.59825 | val_1_rmse: 0.60708 |  0:02:55s
epoch 32 | loss: 0.33456 | val_0_rmse: 0.77737 | val_1_rmse: 0.7781  |  0:03:00s
epoch 33 | loss: 0.33319 | val_0_rmse: 0.58332 | val_1_rmse: 0.59266 |  0:03:05s
epoch 34 | loss: 0.3317  | val_0_rmse: 0.76017 | val_1_rmse: 0.77072 |  0:03:11s
epoch 35 | loss: 0.3283  | val_0_rmse: 0.60092 | val_1_rmse: 0.60819 |  0:03:16s
epoch 36 | loss: 0.33273 | val_0_rmse: 0.60741 | val_1_rmse: 0.61623 |  0:03:22s
epoch 37 | loss: 0.3312  | val_0_rmse: 0.57284 | val_1_rmse: 0.58124 |  0:03:27s
epoch 38 | loss: 0.33085 | val_0_rmse: 0.57495 | val_1_rmse: 0.58428 |  0:03:33s
epoch 39 | loss: 0.32917 | val_0_rmse: 0.58939 | val_1_rmse: 0.59765 |  0:03:38s
epoch 40 | loss: 0.33035 | val_0_rmse: 0.59825 | val_1_rmse: 0.60626 |  0:03:44s
epoch 41 | loss: 0.32681 | val_0_rmse: 0.75684 | val_1_rmse: 0.7595  |  0:03:49s
epoch 42 | loss: 0.33152 | val_0_rmse: 0.6042  | val_1_rmse: 0.61304 |  0:03:55s
epoch 43 | loss: 0.32831 | val_0_rmse: 0.64388 | val_1_rmse: 0.64793 |  0:04:00s
epoch 44 | loss: 0.3258  | val_0_rmse: 0.60616 | val_1_rmse: 0.61593 |  0:04:06s
epoch 45 | loss: 0.33108 | val_0_rmse: 0.70745 | val_1_rmse: 0.71583 |  0:04:11s
epoch 46 | loss: 0.34246 | val_0_rmse: 0.5898  | val_1_rmse: 0.59768 |  0:04:17s
epoch 47 | loss: 0.33757 | val_0_rmse: 0.70761 | val_1_rmse: 0.71703 |  0:04:22s
epoch 48 | loss: 0.33665 | val_0_rmse: 0.583   | val_1_rmse: 0.59264 |  0:04:27s
epoch 49 | loss: 0.33025 | val_0_rmse: 0.60318 | val_1_rmse: 0.61245 |  0:04:33s
epoch 50 | loss: 0.33205 | val_0_rmse: 0.71799 | val_1_rmse: 0.72631 |  0:04:38s
epoch 51 | loss: 0.32928 | val_0_rmse: 0.65689 | val_1_rmse: 0.66667 |  0:04:44s
epoch 52 | loss: 0.3292  | val_0_rmse: 0.64501 | val_1_rmse: 0.65407 |  0:04:49s
epoch 53 | loss: 0.3311  | val_0_rmse: 0.65077 | val_1_rmse: 0.65983 |  0:04:55s
epoch 54 | loss: 0.32914 | val_0_rmse: 0.62221 | val_1_rmse: 0.63144 |  0:05:00s
epoch 55 | loss: 0.32675 | val_0_rmse: 0.64009 | val_1_rmse: 0.64755 |  0:05:06s
epoch 56 | loss: 0.32565 | val_0_rmse: 0.63458 | val_1_rmse: 0.64363 |  0:05:11s
epoch 57 | loss: 0.33177 | val_0_rmse: 0.61136 | val_1_rmse: 0.61994 |  0:05:17s
epoch 58 | loss: 0.33006 | val_0_rmse: 0.64491 | val_1_rmse: 0.65424 |  0:05:22s
epoch 59 | loss: 0.32724 | val_0_rmse: 0.75149 | val_1_rmse: 0.76076 |  0:05:27s
epoch 60 | loss: 0.32773 | val_0_rmse: 0.61947 | val_1_rmse: 0.62984 |  0:05:33s
epoch 61 | loss: 0.32644 | val_0_rmse: 0.75007 | val_1_rmse: 0.7626  |  0:05:38s
epoch 62 | loss: 0.32867 | val_0_rmse: 0.69638 | val_1_rmse: 0.70781 |  0:05:44s
epoch 63 | loss: 0.33166 | val_0_rmse: 0.58307 | val_1_rmse: 0.59356 |  0:05:49s
epoch 64 | loss: 0.32695 | val_0_rmse: 0.66149 | val_1_rmse: 0.66673 |  0:05:55s
epoch 65 | loss: 0.32625 | val_0_rmse: 0.81984 | val_1_rmse: 0.82824 |  0:06:00s
epoch 66 | loss: 0.32722 | val_0_rmse: 0.64147 | val_1_rmse: 0.65104 |  0:06:06s
epoch 67 | loss: 0.32542 | val_0_rmse: 0.76882 | val_1_rmse: 0.77922 |  0:06:11s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.58124
Best weights from best epoch are automatically used!
ended training at: 05:38:42
Feature importance:
[('Area', 0.357673544586376), ('Baths', 0.1926386798509537), ('Beds', 0.0), ('Latitude', 0.23141847320696451), ('Longitude', 0.19473094514255324), ('Month', 0.023538357213152554), ('Year', 0.0)]
Mean squared error is of 2322177550.9604716
Mean absolute error:35093.349329665
MAPE:0.35521263039204315
R2 score:0.6579679048706045
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:38:43
epoch 0  | loss: 0.6091  | val_0_rmse: 0.72942 | val_1_rmse: 0.72797 |  0:00:05s
epoch 1  | loss: 0.51297 | val_0_rmse: 0.69262 | val_1_rmse: 0.69558 |  0:00:11s
epoch 2  | loss: 0.48138 | val_0_rmse: 0.68731 | val_1_rmse: 0.68918 |  0:00:16s
epoch 3  | loss: 0.47106 | val_0_rmse: 0.7188  | val_1_rmse: 0.72078 |  0:00:22s
epoch 4  | loss: 0.46432 | val_0_rmse: 0.67519 | val_1_rmse: 0.67663 |  0:00:27s
epoch 5  | loss: 0.46961 | val_0_rmse: 0.67404 | val_1_rmse: 0.67527 |  0:00:32s
epoch 6  | loss: 0.4626  | val_0_rmse: 0.70813 | val_1_rmse: 0.70864 |  0:00:38s
epoch 7  | loss: 0.46082 | val_0_rmse: 0.71304 | val_1_rmse: 0.71448 |  0:00:43s
epoch 8  | loss: 0.45992 | val_0_rmse: 0.70781 | val_1_rmse: 0.70839 |  0:00:49s
epoch 9  | loss: 0.45746 | val_0_rmse: 0.68258 | val_1_rmse: 0.68373 |  0:00:54s
epoch 10 | loss: 0.45865 | val_0_rmse: 0.66983 | val_1_rmse: 0.67198 |  0:01:00s
epoch 11 | loss: 0.45688 | val_0_rmse: 0.67498 | val_1_rmse: 0.67794 |  0:01:05s
epoch 12 | loss: 0.45574 | val_0_rmse: 0.67869 | val_1_rmse: 0.68274 |  0:01:11s
epoch 13 | loss: 0.45142 | val_0_rmse: 0.6703  | val_1_rmse: 0.67025 |  0:01:16s
epoch 14 | loss: 0.44713 | val_0_rmse: 0.68325 | val_1_rmse: 0.68451 |  0:01:22s
epoch 15 | loss: 0.44284 | val_0_rmse: 0.69302 | val_1_rmse: 0.69357 |  0:01:27s
epoch 16 | loss: 0.43786 | val_0_rmse: 0.71512 | val_1_rmse: 0.71563 |  0:01:32s
epoch 17 | loss: 0.44071 | val_0_rmse: 0.69633 | val_1_rmse: 0.69538 |  0:01:38s
epoch 18 | loss: 0.43853 | val_0_rmse: 0.66169 | val_1_rmse: 0.66093 |  0:01:43s
epoch 19 | loss: 0.44016 | val_0_rmse: 0.74095 | val_1_rmse: 0.74453 |  0:01:49s
epoch 20 | loss: 0.43885 | val_0_rmse: 0.67097 | val_1_rmse: 0.67252 |  0:01:54s
epoch 21 | loss: 0.43628 | val_0_rmse: 0.68263 | val_1_rmse: 0.68433 |  0:02:00s
epoch 22 | loss: 0.43509 | val_0_rmse: 0.65259 | val_1_rmse: 0.652   |  0:02:05s
epoch 23 | loss: 0.44317 | val_0_rmse: 0.69697 | val_1_rmse: 0.69796 |  0:02:11s
epoch 24 | loss: 0.44976 | val_0_rmse: 0.68905 | val_1_rmse: 0.68698 |  0:02:16s
epoch 25 | loss: 0.44595 | val_0_rmse: 0.72608 | val_1_rmse: 0.72423 |  0:02:22s
epoch 26 | loss: 0.44327 | val_0_rmse: 0.72259 | val_1_rmse: 0.72093 |  0:02:27s
epoch 27 | loss: 0.41145 | val_0_rmse: 0.62586 | val_1_rmse: 0.62496 |  0:02:33s
epoch 28 | loss: 0.39374 | val_0_rmse: 0.61925 | val_1_rmse: 0.61757 |  0:02:38s
epoch 29 | loss: 0.39    | val_0_rmse: 0.63593 | val_1_rmse: 0.63768 |  0:02:44s
epoch 30 | loss: 0.38357 | val_0_rmse: 0.64496 | val_1_rmse: 0.64626 |  0:02:49s
epoch 31 | loss: 0.37982 | val_0_rmse: 0.61947 | val_1_rmse: 0.62002 |  0:02:54s
epoch 32 | loss: 0.37975 | val_0_rmse: 0.60847 | val_1_rmse: 0.60868 |  0:03:00s
epoch 33 | loss: 0.37638 | val_0_rmse: 0.81207 | val_1_rmse: 0.8045  |  0:03:05s
epoch 34 | loss: 0.37669 | val_0_rmse: 0.63787 | val_1_rmse: 0.63747 |  0:03:11s
epoch 35 | loss: 0.37653 | val_0_rmse: 0.61714 | val_1_rmse: 0.61846 |  0:03:16s
epoch 36 | loss: 0.37267 | val_0_rmse: 0.63072 | val_1_rmse: 0.62964 |  0:03:22s
epoch 37 | loss: 0.3888  | val_0_rmse: 0.62858 | val_1_rmse: 0.62748 |  0:03:27s
epoch 38 | loss: 0.38982 | val_0_rmse: 0.64351 | val_1_rmse: 0.64306 |  0:03:33s
epoch 39 | loss: 0.37994 | val_0_rmse: 0.62098 | val_1_rmse: 0.62362 |  0:03:38s
epoch 40 | loss: 0.37066 | val_0_rmse: 0.66128 | val_1_rmse: 0.66321 |  0:03:44s
epoch 41 | loss: 0.36388 | val_0_rmse: 0.65428 | val_1_rmse: 0.6558  |  0:03:49s
epoch 42 | loss: 0.36376 | val_0_rmse: 0.67878 | val_1_rmse: 0.67958 |  0:03:55s
epoch 43 | loss: 0.36318 | val_0_rmse: 0.65993 | val_1_rmse: 0.66063 |  0:04:00s
epoch 44 | loss: 0.36496 | val_0_rmse: 0.64612 | val_1_rmse: 0.64723 |  0:04:06s
epoch 45 | loss: 0.36263 | val_0_rmse: 0.60052 | val_1_rmse: 0.59968 |  0:04:11s
epoch 46 | loss: 0.36007 | val_0_rmse: 0.69034 | val_1_rmse: 0.68959 |  0:04:17s
epoch 47 | loss: 0.35837 | val_0_rmse: 0.75958 | val_1_rmse: 0.75719 |  0:04:22s
epoch 48 | loss: 0.3565  | val_0_rmse: 0.59241 | val_1_rmse: 0.59422 |  0:04:28s
epoch 49 | loss: 0.35543 | val_0_rmse: 0.64286 | val_1_rmse: 0.64356 |  0:04:33s
epoch 50 | loss: 0.35504 | val_0_rmse: 0.59182 | val_1_rmse: 0.59425 |  0:04:38s
epoch 51 | loss: 0.35498 | val_0_rmse: 0.61968 | val_1_rmse: 0.6196  |  0:04:44s
epoch 52 | loss: 0.35263 | val_0_rmse: 0.6887  | val_1_rmse: 0.68947 |  0:04:49s
epoch 53 | loss: 0.35287 | val_0_rmse: 0.62932 | val_1_rmse: 0.63227 |  0:04:55s
epoch 54 | loss: 0.34836 | val_0_rmse: 0.61558 | val_1_rmse: 0.61469 |  0:05:00s
epoch 55 | loss: 0.34815 | val_0_rmse: 0.6028  | val_1_rmse: 0.60449 |  0:05:06s
epoch 56 | loss: 0.34725 | val_0_rmse: 0.6247  | val_1_rmse: 0.62251 |  0:05:11s
epoch 57 | loss: 0.3479  | val_0_rmse: 0.65273 | val_1_rmse: 0.64723 |  0:05:17s
epoch 58 | loss: 0.34569 | val_0_rmse: 0.60198 | val_1_rmse: 0.59968 |  0:05:22s
epoch 59 | loss: 0.34723 | val_0_rmse: 0.62668 | val_1_rmse: 0.62822 |  0:05:28s
epoch 60 | loss: 0.3431  | val_0_rmse: 0.59442 | val_1_rmse: 0.59459 |  0:05:33s
epoch 61 | loss: 0.34234 | val_0_rmse: 0.64408 | val_1_rmse: 0.64406 |  0:05:39s
epoch 62 | loss: 0.34458 | val_0_rmse: 0.6534  | val_1_rmse: 0.65716 |  0:05:44s
epoch 63 | loss: 0.34114 | val_0_rmse: 0.63349 | val_1_rmse: 0.63406 |  0:05:50s
epoch 64 | loss: 0.34302 | val_0_rmse: 0.61594 | val_1_rmse: 0.61751 |  0:05:55s
epoch 65 | loss: 0.34213 | val_0_rmse: 0.67409 | val_1_rmse: 0.67601 |  0:06:00s
epoch 66 | loss: 0.3404  | val_0_rmse: 0.62297 | val_1_rmse: 0.62332 |  0:06:06s
epoch 67 | loss: 0.33996 | val_0_rmse: 0.60459 | val_1_rmse: 0.60254 |  0:06:11s
epoch 68 | loss: 0.34094 | val_0_rmse: 0.62658 | val_1_rmse: 0.62959 |  0:06:17s
epoch 69 | loss: 0.3377  | val_0_rmse: 0.6153  | val_1_rmse: 0.61624 |  0:06:22s
epoch 70 | loss: 0.33934 | val_0_rmse: 0.63169 | val_1_rmse: 0.63412 |  0:06:28s
epoch 71 | loss: 0.33834 | val_0_rmse: 0.59834 | val_1_rmse: 0.59676 |  0:06:33s
epoch 72 | loss: 0.33657 | val_0_rmse: 0.60894 | val_1_rmse: 0.60715 |  0:06:39s
epoch 73 | loss: 0.33836 | val_0_rmse: 0.65403 | val_1_rmse: 0.65087 |  0:06:44s
epoch 74 | loss: 0.33634 | val_0_rmse: 0.70567 | val_1_rmse: 0.7093  |  0:06:50s
epoch 75 | loss: 0.3358  | val_0_rmse: 0.65257 | val_1_rmse: 0.65524 |  0:06:55s
epoch 76 | loss: 0.33816 | val_0_rmse: 0.62884 | val_1_rmse: 0.62562 |  0:07:01s
epoch 77 | loss: 0.3343  | val_0_rmse: 0.643   | val_1_rmse: 0.6463  |  0:07:06s
epoch 78 | loss: 0.33402 | val_0_rmse: 0.66072 | val_1_rmse: 0.66357 |  0:07:12s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.59422
Best weights from best epoch are automatically used!
ended training at: 05:45:57
Feature importance:
[('Area', 0.3511849765077331), ('Baths', 0.33603665699713703), ('Beds', 0.0), ('Latitude', 0.3127783664951299), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2447444540.307888
Mean absolute error:35517.427116097606
MAPE:0.32812469907491
R2 score:0.6422955327012068
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:57
epoch 0  | loss: 0.60776 | val_0_rmse: 0.72    | val_1_rmse: 0.71808 |  0:00:05s
epoch 1  | loss: 0.50059 | val_0_rmse: 0.68809 | val_1_rmse: 0.68532 |  0:00:11s
epoch 2  | loss: 0.45286 | val_0_rmse: 0.66191 | val_1_rmse: 0.65762 |  0:00:16s
epoch 3  | loss: 0.41464 | val_0_rmse: 0.70852 | val_1_rmse: 0.70287 |  0:00:22s
epoch 4  | loss: 0.40791 | val_0_rmse: 0.64651 | val_1_rmse: 0.64312 |  0:00:27s
epoch 5  | loss: 0.39955 | val_0_rmse: 0.62036 | val_1_rmse: 0.61404 |  0:00:33s
epoch 6  | loss: 0.39244 | val_0_rmse: 0.64081 | val_1_rmse: 0.63367 |  0:00:38s
epoch 7  | loss: 0.38616 | val_0_rmse: 0.60939 | val_1_rmse: 0.60211 |  0:00:43s
epoch 8  | loss: 0.3857  | val_0_rmse: 0.62016 | val_1_rmse: 0.61449 |  0:00:49s
epoch 9  | loss: 0.38151 | val_0_rmse: 0.62416 | val_1_rmse: 0.62015 |  0:00:54s
epoch 10 | loss: 0.37937 | val_0_rmse: 0.6042  | val_1_rmse: 0.60017 |  0:01:00s
epoch 11 | loss: 0.37451 | val_0_rmse: 0.67529 | val_1_rmse: 0.66823 |  0:01:05s
epoch 12 | loss: 0.38124 | val_0_rmse: 0.64375 | val_1_rmse: 0.63552 |  0:01:11s
epoch 13 | loss: 0.37447 | val_0_rmse: 0.62845 | val_1_rmse: 0.62099 |  0:01:16s
epoch 14 | loss: 0.37218 | val_0_rmse: 0.66153 | val_1_rmse: 0.65466 |  0:01:22s
epoch 15 | loss: 0.37425 | val_0_rmse: 0.61531 | val_1_rmse: 0.61139 |  0:01:27s
epoch 16 | loss: 0.37134 | val_0_rmse: 0.67379 | val_1_rmse: 0.66667 |  0:01:33s
epoch 17 | loss: 0.36907 | val_0_rmse: 0.62884 | val_1_rmse: 0.62347 |  0:01:38s
epoch 18 | loss: 0.36958 | val_0_rmse: 0.63085 | val_1_rmse: 0.62909 |  0:01:44s
epoch 19 | loss: 0.37085 | val_0_rmse: 0.6691  | val_1_rmse: 0.66497 |  0:01:49s
epoch 20 | loss: 0.36765 | val_0_rmse: 0.63455 | val_1_rmse: 0.63076 |  0:01:55s
epoch 21 | loss: 0.36433 | val_0_rmse: 0.6208  | val_1_rmse: 0.61437 |  0:02:00s
epoch 22 | loss: 0.36027 | val_0_rmse: 0.62558 | val_1_rmse: 0.62002 |  0:02:06s
epoch 23 | loss: 0.36518 | val_0_rmse: 0.64533 | val_1_rmse: 0.63997 |  0:02:11s
epoch 24 | loss: 0.36057 | val_0_rmse: 0.6367  | val_1_rmse: 0.62972 |  0:02:17s
epoch 25 | loss: 0.36165 | val_0_rmse: 0.60437 | val_1_rmse: 0.5987  |  0:02:22s
epoch 26 | loss: 0.36952 | val_0_rmse: 0.73918 | val_1_rmse: 0.73243 |  0:02:28s
epoch 27 | loss: 0.36073 | val_0_rmse: 0.7359  | val_1_rmse: 0.73394 |  0:02:33s
epoch 28 | loss: 0.35736 | val_0_rmse: 0.63301 | val_1_rmse: 0.62839 |  0:02:38s
epoch 29 | loss: 0.35508 | val_0_rmse: 0.63747 | val_1_rmse: 0.6331  |  0:02:44s
epoch 30 | loss: 0.35562 | val_0_rmse: 0.86851 | val_1_rmse: 0.86288 |  0:02:49s
epoch 31 | loss: 0.36009 | val_0_rmse: 0.60744 | val_1_rmse: 0.60186 |  0:02:55s
epoch 32 | loss: 0.35435 | val_0_rmse: 0.6404  | val_1_rmse: 0.63517 |  0:03:00s
epoch 33 | loss: 0.35827 | val_0_rmse: 0.61056 | val_1_rmse: 0.60503 |  0:03:06s
epoch 34 | loss: 0.35753 | val_0_rmse: 0.72525 | val_1_rmse: 0.72308 |  0:03:11s
epoch 35 | loss: 0.36059 | val_0_rmse: 0.64149 | val_1_rmse: 0.63647 |  0:03:17s
epoch 36 | loss: 0.35375 | val_0_rmse: 0.61047 | val_1_rmse: 0.60492 |  0:03:22s
epoch 37 | loss: 0.3505  | val_0_rmse: 0.62349 | val_1_rmse: 0.61654 |  0:03:28s
epoch 38 | loss: 0.35097 | val_0_rmse: 0.60797 | val_1_rmse: 0.60279 |  0:03:33s
epoch 39 | loss: 0.34622 | val_0_rmse: 0.59593 | val_1_rmse: 0.59191 |  0:03:39s
epoch 40 | loss: 0.34664 | val_0_rmse: 0.59322 | val_1_rmse: 0.58529 |  0:03:44s
epoch 41 | loss: 0.34419 | val_0_rmse: 0.67457 | val_1_rmse: 0.66846 |  0:03:50s
epoch 42 | loss: 0.34688 | val_0_rmse: 0.73384 | val_1_rmse: 0.72857 |  0:03:55s
epoch 43 | loss: 0.34804 | val_0_rmse: 0.60423 | val_1_rmse: 0.59559 |  0:04:01s
epoch 44 | loss: 0.34698 | val_0_rmse: 0.60647 | val_1_rmse: 0.59963 |  0:04:06s
epoch 45 | loss: 0.34676 | val_0_rmse: 0.68306 | val_1_rmse: 0.67712 |  0:04:12s
epoch 46 | loss: 0.34695 | val_0_rmse: 0.62381 | val_1_rmse: 0.6209  |  0:04:17s
epoch 47 | loss: 0.34303 | val_0_rmse: 0.6191  | val_1_rmse: 0.61433 |  0:04:22s
epoch 48 | loss: 0.34428 | val_0_rmse: 0.64126 | val_1_rmse: 0.63314 |  0:04:28s
epoch 49 | loss: 0.34338 | val_0_rmse: 0.69728 | val_1_rmse: 0.69212 |  0:04:33s
epoch 50 | loss: 0.34677 | val_0_rmse: 0.60936 | val_1_rmse: 0.60353 |  0:04:39s
epoch 51 | loss: 0.34576 | val_0_rmse: 0.73924 | val_1_rmse: 0.73623 |  0:04:44s
epoch 52 | loss: 0.34282 | val_0_rmse: 0.62936 | val_1_rmse: 0.62584 |  0:04:50s
epoch 53 | loss: 0.33682 | val_0_rmse: 0.72638 | val_1_rmse: 0.72102 |  0:04:55s
epoch 54 | loss: 0.3549  | val_0_rmse: 0.75887 | val_1_rmse: 0.75494 |  0:05:01s
epoch 55 | loss: 0.34849 | val_0_rmse: 0.77578 | val_1_rmse: 0.77236 |  0:05:06s
epoch 56 | loss: 0.34755 | val_0_rmse: 0.61368 | val_1_rmse: 0.60779 |  0:05:12s
epoch 57 | loss: 0.33763 | val_0_rmse: 0.61254 | val_1_rmse: 0.60618 |  0:05:17s
epoch 58 | loss: 0.3409  | val_0_rmse: 0.6111  | val_1_rmse: 0.60561 |  0:05:23s
epoch 59 | loss: 0.33873 | val_0_rmse: 0.59082 | val_1_rmse: 0.5858  |  0:05:28s
epoch 60 | loss: 0.34411 | val_0_rmse: 0.89361 | val_1_rmse: 0.88761 |  0:05:33s
epoch 61 | loss: 0.34149 | val_0_rmse: 0.59829 | val_1_rmse: 0.59142 |  0:05:39s
epoch 62 | loss: 0.34468 | val_0_rmse: 0.63546 | val_1_rmse: 0.62821 |  0:05:44s
epoch 63 | loss: 0.34158 | val_0_rmse: 0.64557 | val_1_rmse: 0.64012 |  0:05:50s
epoch 64 | loss: 0.34154 | val_0_rmse: 0.641   | val_1_rmse: 0.63495 |  0:05:55s
epoch 65 | loss: 0.33993 | val_0_rmse: 0.5986  | val_1_rmse: 0.5955  |  0:06:01s
epoch 66 | loss: 0.33336 | val_0_rmse: 0.61014 | val_1_rmse: 0.60562 |  0:06:06s
epoch 67 | loss: 0.34245 | val_0_rmse: 0.70839 | val_1_rmse: 0.70299 |  0:06:12s
epoch 68 | loss: 0.33994 | val_0_rmse: 0.63415 | val_1_rmse: 0.62824 |  0:06:17s
epoch 69 | loss: 0.33786 | val_0_rmse: 0.70324 | val_1_rmse: 0.69434 |  0:06:23s
epoch 70 | loss: 0.33429 | val_0_rmse: 0.71165 | val_1_rmse: 0.70503 |  0:06:28s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.58529
Best weights from best epoch are automatically used!
ended training at: 05:52:27
Feature importance:
[('Area', 0.2704680224893464), ('Baths', 0.24974088912192202), ('Beds', 0.0), ('Latitude', 0.2737802668622547), ('Longitude', 0.19817161554216206), ('Month', 0.0), ('Year', 0.00783920598431476)]
Mean squared error is of 2399004764.6437936
Mean absolute error:36055.1597086411
MAPE:0.367176179572481
R2 score:0.651970121342505
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:28
epoch 0  | loss: 0.61052 | val_0_rmse: 0.72798 | val_1_rmse: 0.72796 |  0:00:05s
epoch 1  | loss: 0.50238 | val_0_rmse: 0.68852 | val_1_rmse: 0.6882  |  0:00:10s
epoch 2  | loss: 0.45828 | val_0_rmse: 0.70767 | val_1_rmse: 0.70768 |  0:00:16s
epoch 3  | loss: 0.43456 | val_0_rmse: 0.6575  | val_1_rmse: 0.65527 |  0:00:21s
epoch 4  | loss: 0.42415 | val_0_rmse: 0.63687 | val_1_rmse: 0.63646 |  0:00:27s
epoch 5  | loss: 0.41667 | val_0_rmse: 0.68731 | val_1_rmse: 0.68993 |  0:00:32s
epoch 6  | loss: 0.41679 | val_0_rmse: 0.6397  | val_1_rmse: 0.64072 |  0:00:38s
epoch 7  | loss: 0.41247 | val_0_rmse: 0.68475 | val_1_rmse: 0.6849  |  0:00:43s
epoch 8  | loss: 0.39958 | val_0_rmse: 0.6504  | val_1_rmse: 0.65085 |  0:00:49s
epoch 9  | loss: 0.39787 | val_0_rmse: 0.70599 | val_1_rmse: 0.70559 |  0:00:54s
epoch 10 | loss: 0.4068  | val_0_rmse: 0.68778 | val_1_rmse: 0.68611 |  0:01:00s
epoch 11 | loss: 0.39748 | val_0_rmse: 0.62049 | val_1_rmse: 0.61935 |  0:01:05s
epoch 12 | loss: 0.38213 | val_0_rmse: 0.66628 | val_1_rmse: 0.66601 |  0:01:11s
epoch 13 | loss: 0.38086 | val_0_rmse: 0.60105 | val_1_rmse: 0.60047 |  0:01:16s
epoch 14 | loss: 0.38171 | val_0_rmse: 0.61129 | val_1_rmse: 0.60885 |  0:01:21s
epoch 15 | loss: 0.3736  | val_0_rmse: 0.6816  | val_1_rmse: 0.68146 |  0:01:27s
epoch 16 | loss: 0.36689 | val_0_rmse: 0.61849 | val_1_rmse: 0.61714 |  0:01:32s
epoch 17 | loss: 0.36236 | val_0_rmse: 0.60951 | val_1_rmse: 0.60984 |  0:01:38s
epoch 18 | loss: 0.366   | val_0_rmse: 0.64075 | val_1_rmse: 0.6381  |  0:01:43s
epoch 19 | loss: 0.35663 | val_0_rmse: 0.6987  | val_1_rmse: 0.69497 |  0:01:49s
epoch 20 | loss: 0.35149 | val_0_rmse: 0.58788 | val_1_rmse: 0.58397 |  0:01:54s
epoch 21 | loss: 0.35193 | val_0_rmse: 0.63111 | val_1_rmse: 0.63136 |  0:02:00s
epoch 22 | loss: 0.35353 | val_0_rmse: 0.61749 | val_1_rmse: 0.61753 |  0:02:05s
epoch 23 | loss: 0.35012 | val_0_rmse: 0.62344 | val_1_rmse: 0.62329 |  0:02:11s
epoch 24 | loss: 0.35551 | val_0_rmse: 0.60927 | val_1_rmse: 0.60837 |  0:02:16s
epoch 25 | loss: 0.35711 | val_0_rmse: 0.61531 | val_1_rmse: 0.61411 |  0:02:22s
epoch 26 | loss: 0.35621 | val_0_rmse: 0.61667 | val_1_rmse: 0.61446 |  0:02:27s
epoch 27 | loss: 0.35086 | val_0_rmse: 0.62343 | val_1_rmse: 0.62224 |  0:02:32s
epoch 28 | loss: 0.3487  | val_0_rmse: 0.58855 | val_1_rmse: 0.58858 |  0:02:38s
epoch 29 | loss: 0.34765 | val_0_rmse: 0.64164 | val_1_rmse: 0.63998 |  0:02:43s
epoch 30 | loss: 0.344   | val_0_rmse: 0.65878 | val_1_rmse: 0.65924 |  0:02:49s
epoch 31 | loss: 0.34467 | val_0_rmse: 0.59097 | val_1_rmse: 0.58829 |  0:02:54s
epoch 32 | loss: 0.34577 | val_0_rmse: 0.62207 | val_1_rmse: 0.62019 |  0:03:00s
epoch 33 | loss: 0.34362 | val_0_rmse: 0.60431 | val_1_rmse: 0.60195 |  0:03:05s
epoch 34 | loss: 0.33883 | val_0_rmse: 0.60807 | val_1_rmse: 0.60643 |  0:03:11s
epoch 35 | loss: 0.33881 | val_0_rmse: 0.61784 | val_1_rmse: 0.61749 |  0:03:16s
epoch 36 | loss: 0.33703 | val_0_rmse: 0.57952 | val_1_rmse: 0.5791  |  0:03:22s
epoch 37 | loss: 0.33911 | val_0_rmse: 0.6264  | val_1_rmse: 0.62669 |  0:03:27s
epoch 38 | loss: 0.34022 | val_0_rmse: 0.63145 | val_1_rmse: 0.62824 |  0:03:32s
epoch 39 | loss: 0.33714 | val_0_rmse: 0.58097 | val_1_rmse: 0.57987 |  0:03:38s
epoch 40 | loss: 0.33797 | val_0_rmse: 0.62426 | val_1_rmse: 0.62274 |  0:03:43s
epoch 41 | loss: 0.33679 | val_0_rmse: 0.62101 | val_1_rmse: 0.61801 |  0:03:49s
epoch 42 | loss: 0.33798 | val_0_rmse: 0.62353 | val_1_rmse: 0.62212 |  0:03:54s
epoch 43 | loss: 0.33403 | val_0_rmse: 0.65339 | val_1_rmse: 0.65587 |  0:04:00s
epoch 44 | loss: 0.33269 | val_0_rmse: 0.6011  | val_1_rmse: 0.59963 |  0:04:05s
epoch 45 | loss: 0.33694 | val_0_rmse: 0.62847 | val_1_rmse: 0.6282  |  0:04:11s
epoch 46 | loss: 0.33957 | val_0_rmse: 0.57693 | val_1_rmse: 0.57762 |  0:04:16s
epoch 47 | loss: 0.33724 | val_0_rmse: 0.59721 | val_1_rmse: 0.59634 |  0:04:22s
epoch 48 | loss: 0.33763 | val_0_rmse: 0.59521 | val_1_rmse: 0.59303 |  0:04:27s
epoch 49 | loss: 0.33302 | val_0_rmse: 0.57624 | val_1_rmse: 0.57502 |  0:04:33s
epoch 50 | loss: 0.33509 | val_0_rmse: 0.6023  | val_1_rmse: 0.60076 |  0:04:38s
epoch 51 | loss: 0.33605 | val_0_rmse: 0.64331 | val_1_rmse: 0.64087 |  0:04:44s
epoch 52 | loss: 0.33095 | val_0_rmse: 0.62685 | val_1_rmse: 0.62645 |  0:04:49s
epoch 53 | loss: 0.32997 | val_0_rmse: 0.84643 | val_1_rmse: 0.84816 |  0:04:55s
epoch 54 | loss: 0.35019 | val_0_rmse: 0.58819 | val_1_rmse: 0.58606 |  0:05:00s
epoch 55 | loss: 0.34046 | val_0_rmse: 0.58611 | val_1_rmse: 0.58722 |  0:05:06s
epoch 56 | loss: 0.3419  | val_0_rmse: 0.63869 | val_1_rmse: 0.63836 |  0:05:11s
epoch 57 | loss: 0.33936 | val_0_rmse: 0.62511 | val_1_rmse: 0.62261 |  0:05:16s
epoch 58 | loss: 0.336   | val_0_rmse: 0.62332 | val_1_rmse: 0.62324 |  0:05:22s
epoch 59 | loss: 0.33395 | val_0_rmse: 0.71843 | val_1_rmse: 0.71791 |  0:05:27s
epoch 60 | loss: 0.33464 | val_0_rmse: 0.59692 | val_1_rmse: 0.59599 |  0:05:33s
epoch 61 | loss: 0.33192 | val_0_rmse: 0.61183 | val_1_rmse: 0.61037 |  0:05:38s
epoch 62 | loss: 0.33283 | val_0_rmse: 0.68419 | val_1_rmse: 0.68438 |  0:05:44s
epoch 63 | loss: 0.33074 | val_0_rmse: 0.64962 | val_1_rmse: 0.6506  |  0:05:49s
epoch 64 | loss: 0.33245 | val_0_rmse: 0.61393 | val_1_rmse: 0.61218 |  0:05:55s
epoch 65 | loss: 0.34218 | val_0_rmse: 0.78844 | val_1_rmse: 0.79091 |  0:06:00s
epoch 66 | loss: 0.33736 | val_0_rmse: 0.6136  | val_1_rmse: 0.61417 |  0:06:06s
epoch 67 | loss: 0.33886 | val_0_rmse: 0.71613 | val_1_rmse: 0.71418 |  0:06:11s
epoch 68 | loss: 0.33152 | val_0_rmse: 0.71972 | val_1_rmse: 0.7167  |  0:06:17s
epoch 69 | loss: 0.33262 | val_0_rmse: 0.57618 | val_1_rmse: 0.57641 |  0:06:22s
epoch 70 | loss: 0.34109 | val_0_rmse: 0.92318 | val_1_rmse: 0.91853 |  0:06:28s
epoch 71 | loss: 0.35933 | val_0_rmse: 0.64175 | val_1_rmse: 0.63913 |  0:06:33s
epoch 72 | loss: 0.34501 | val_0_rmse: 0.60537 | val_1_rmse: 0.60301 |  0:06:39s
epoch 73 | loss: 0.33824 | val_0_rmse: 0.6537  | val_1_rmse: 0.65188 |  0:06:44s
epoch 74 | loss: 0.33832 | val_0_rmse: 0.59549 | val_1_rmse: 0.59552 |  0:06:49s
epoch 75 | loss: 0.34162 | val_0_rmse: 0.59864 | val_1_rmse: 0.59795 |  0:06:55s
epoch 76 | loss: 0.33432 | val_0_rmse: 0.59116 | val_1_rmse: 0.59142 |  0:07:00s
epoch 77 | loss: 0.33164 | val_0_rmse: 0.63781 | val_1_rmse: 0.63533 |  0:07:06s
epoch 78 | loss: 0.33169 | val_0_rmse: 0.6421  | val_1_rmse: 0.64162 |  0:07:11s
epoch 79 | loss: 0.33278 | val_0_rmse: 0.5978  | val_1_rmse: 0.59918 |  0:07:17s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.57502
Best weights from best epoch are automatically used!
ended training at: 05:59:47
Feature importance:
[('Area', 0.4042360836066208), ('Baths', 0.10654165471228863), ('Beds', 0.0025266194486547907), ('Latitude', 0.29792040450256047), ('Longitude', 0.18765342201189467), ('Month', 0.0011218157179806692), ('Year', 0.0)]
Mean squared error is of 2311785615.2501493
Mean absolute error:34342.61780329505
MAPE:0.32875301823426617
R2 score:0.6611545545460794
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:59:48
epoch 0  | loss: 0.60052 | val_0_rmse: 0.7226  | val_1_rmse: 0.71855 |  0:00:05s
epoch 1  | loss: 0.5146  | val_0_rmse: 0.71225 | val_1_rmse: 0.70723 |  0:00:10s
epoch 2  | loss: 0.46307 | val_0_rmse: 0.69392 | val_1_rmse: 0.68906 |  0:00:16s
epoch 3  | loss: 0.42906 | val_0_rmse: 0.70637 | val_1_rmse: 0.70385 |  0:00:21s
epoch 4  | loss: 0.41208 | val_0_rmse: 0.62753 | val_1_rmse: 0.6247  |  0:00:27s
epoch 5  | loss: 0.39706 | val_0_rmse: 0.61518 | val_1_rmse: 0.61352 |  0:00:32s
epoch 6  | loss: 0.38797 | val_0_rmse: 0.63159 | val_1_rmse: 0.63038 |  0:00:38s
epoch 7  | loss: 0.38407 | val_0_rmse: 0.64413 | val_1_rmse: 0.64369 |  0:00:43s
epoch 8  | loss: 0.38303 | val_0_rmse: 0.61966 | val_1_rmse: 0.61744 |  0:00:49s
epoch 9  | loss: 0.37593 | val_0_rmse: 0.61041 | val_1_rmse: 0.61085 |  0:00:54s
epoch 10 | loss: 0.36902 | val_0_rmse: 0.63616 | val_1_rmse: 0.63496 |  0:01:00s
epoch 11 | loss: 0.3662  | val_0_rmse: 0.72788 | val_1_rmse: 0.72376 |  0:01:05s
epoch 12 | loss: 0.37304 | val_0_rmse: 0.61245 | val_1_rmse: 0.61108 |  0:01:10s
epoch 13 | loss: 0.36521 | val_0_rmse: 0.62201 | val_1_rmse: 0.62129 |  0:01:16s
epoch 14 | loss: 0.36393 | val_0_rmse: 0.68754 | val_1_rmse: 0.68696 |  0:01:21s
epoch 15 | loss: 0.3581  | val_0_rmse: 0.59248 | val_1_rmse: 0.59105 |  0:01:27s
epoch 16 | loss: 0.35741 | val_0_rmse: 0.62317 | val_1_rmse: 0.61724 |  0:01:32s
epoch 17 | loss: 0.35037 | val_0_rmse: 0.6278  | val_1_rmse: 0.6236  |  0:01:38s
epoch 18 | loss: 0.35215 | val_0_rmse: 0.61241 | val_1_rmse: 0.61505 |  0:01:43s
epoch 19 | loss: 0.35587 | val_0_rmse: 0.63181 | val_1_rmse: 0.62767 |  0:01:49s
epoch 20 | loss: 0.34755 | val_0_rmse: 0.60673 | val_1_rmse: 0.60763 |  0:01:54s
epoch 21 | loss: 0.34841 | val_0_rmse: 0.69977 | val_1_rmse: 0.69673 |  0:01:59s
epoch 22 | loss: 0.34841 | val_0_rmse: 0.61479 | val_1_rmse: 0.61555 |  0:02:05s
epoch 23 | loss: 0.34713 | val_0_rmse: 0.67056 | val_1_rmse: 0.67314 |  0:02:10s
epoch 24 | loss: 0.34609 | val_0_rmse: 0.61195 | val_1_rmse: 0.61414 |  0:02:16s
epoch 25 | loss: 0.34148 | val_0_rmse: 0.66157 | val_1_rmse: 0.66442 |  0:02:21s
epoch 26 | loss: 0.34077 | val_0_rmse: 0.60615 | val_1_rmse: 0.60832 |  0:02:27s
epoch 27 | loss: 0.33851 | val_0_rmse: 0.64008 | val_1_rmse: 0.64241 |  0:02:32s
epoch 28 | loss: 0.33682 | val_0_rmse: 0.62143 | val_1_rmse: 0.62076 |  0:02:38s
epoch 29 | loss: 0.34204 | val_0_rmse: 0.5846  | val_1_rmse: 0.58518 |  0:02:43s
epoch 30 | loss: 0.33544 | val_0_rmse: 0.63439 | val_1_rmse: 0.63772 |  0:02:48s
epoch 31 | loss: 0.33768 | val_0_rmse: 0.61935 | val_1_rmse: 0.62145 |  0:02:54s
epoch 32 | loss: 0.33555 | val_0_rmse: 0.72689 | val_1_rmse: 0.72541 |  0:02:59s
epoch 33 | loss: 0.33372 | val_0_rmse: 0.59641 | val_1_rmse: 0.59759 |  0:03:05s
epoch 34 | loss: 0.33317 | val_0_rmse: 0.63507 | val_1_rmse: 0.6395  |  0:03:10s
epoch 35 | loss: 0.33397 | val_0_rmse: 0.64734 | val_1_rmse: 0.65034 |  0:03:16s
epoch 36 | loss: 0.33102 | val_0_rmse: 0.64567 | val_1_rmse: 0.64913 |  0:03:21s
epoch 37 | loss: 0.33186 | val_0_rmse: 0.65976 | val_1_rmse: 0.66252 |  0:03:27s
epoch 38 | loss: 0.33108 | val_0_rmse: 0.57254 | val_1_rmse: 0.57507 |  0:03:32s
epoch 39 | loss: 0.3298  | val_0_rmse: 0.56934 | val_1_rmse: 0.5735  |  0:03:38s
epoch 40 | loss: 0.33018 | val_0_rmse: 0.60182 | val_1_rmse: 0.60414 |  0:03:43s
epoch 41 | loss: 0.3313  | val_0_rmse: 0.6078  | val_1_rmse: 0.60898 |  0:03:48s
epoch 42 | loss: 0.33299 | val_0_rmse: 0.6617  | val_1_rmse: 0.66089 |  0:03:54s
epoch 43 | loss: 0.33133 | val_0_rmse: 0.58833 | val_1_rmse: 0.59189 |  0:03:59s
epoch 44 | loss: 0.32615 | val_0_rmse: 0.65491 | val_1_rmse: 0.65729 |  0:04:05s
epoch 45 | loss: 0.32571 | val_0_rmse: 0.59289 | val_1_rmse: 0.59775 |  0:04:10s
epoch 46 | loss: 0.32351 | val_0_rmse: 0.60458 | val_1_rmse: 0.60723 |  0:04:16s
epoch 47 | loss: 0.32712 | val_0_rmse: 0.57896 | val_1_rmse: 0.5829  |  0:04:21s
epoch 48 | loss: 0.32634 | val_0_rmse: 0.61746 | val_1_rmse: 0.6204  |  0:04:27s
epoch 49 | loss: 0.3319  | val_0_rmse: 0.642   | val_1_rmse: 0.63965 |  0:04:32s
epoch 50 | loss: 0.33048 | val_0_rmse: 0.6238  | val_1_rmse: 0.62456 |  0:04:38s
epoch 51 | loss: 0.32885 | val_0_rmse: 0.69841 | val_1_rmse: 0.69789 |  0:04:43s
epoch 52 | loss: 0.33096 | val_0_rmse: 0.60297 | val_1_rmse: 0.607   |  0:04:49s
epoch 53 | loss: 0.32619 | val_0_rmse: 0.57998 | val_1_rmse: 0.58317 |  0:04:54s
epoch 54 | loss: 0.3244  | val_0_rmse: 0.63847 | val_1_rmse: 0.64374 |  0:05:00s
epoch 55 | loss: 0.32594 | val_0_rmse: 0.59788 | val_1_rmse: 0.59906 |  0:05:05s
epoch 56 | loss: 0.32625 | val_0_rmse: 0.85549 | val_1_rmse: 0.85731 |  0:05:11s
epoch 57 | loss: 0.32483 | val_0_rmse: 0.65306 | val_1_rmse: 0.65784 |  0:05:16s
epoch 58 | loss: 0.32374 | val_0_rmse: 0.6331  | val_1_rmse: 0.63727 |  0:05:22s
epoch 59 | loss: 0.32389 | val_0_rmse: 0.68265 | val_1_rmse: 0.68726 |  0:05:27s
epoch 60 | loss: 0.32368 | val_0_rmse: 0.62434 | val_1_rmse: 0.62609 |  0:05:32s
epoch 61 | loss: 0.32698 | val_0_rmse: 0.60177 | val_1_rmse: 0.60472 |  0:05:38s
epoch 62 | loss: 0.32404 | val_0_rmse: 0.58996 | val_1_rmse: 0.59056 |  0:05:43s
epoch 63 | loss: 0.32316 | val_0_rmse: 0.75542 | val_1_rmse: 0.75403 |  0:05:49s
epoch 64 | loss: 0.32468 | val_0_rmse: 0.56831 | val_1_rmse: 0.57334 |  0:05:54s
epoch 65 | loss: 0.32391 | val_0_rmse: 0.70751 | val_1_rmse: 0.70485 |  0:06:00s
epoch 66 | loss: 0.32443 | val_0_rmse: 0.60826 | val_1_rmse: 0.60949 |  0:06:05s
epoch 67 | loss: 0.32554 | val_0_rmse: 0.65242 | val_1_rmse: 0.65811 |  0:06:11s
epoch 68 | loss: 0.32423 | val_0_rmse: 0.64865 | val_1_rmse: 0.65038 |  0:06:16s
epoch 69 | loss: 0.32841 | val_0_rmse: 0.67238 | val_1_rmse: 0.6741  |  0:06:22s
epoch 70 | loss: 0.32214 | val_0_rmse: 0.64605 | val_1_rmse: 0.64831 |  0:06:27s
epoch 71 | loss: 0.31814 | val_0_rmse: 0.70557 | val_1_rmse: 0.70642 |  0:06:32s
epoch 72 | loss: 0.32045 | val_0_rmse: 0.64954 | val_1_rmse: 0.65489 |  0:06:38s
epoch 73 | loss: 0.31998 | val_0_rmse: 0.63031 | val_1_rmse: 0.63706 |  0:06:43s
epoch 74 | loss: 0.33049 | val_0_rmse: 0.61451 | val_1_rmse: 0.61856 |  0:06:49s
epoch 75 | loss: 0.32209 | val_0_rmse: 0.6515  | val_1_rmse: 0.65578 |  0:06:54s
epoch 76 | loss: 0.32119 | val_0_rmse: 0.56311 | val_1_rmse: 0.56692 |  0:07:00s
epoch 77 | loss: 0.32127 | val_0_rmse: 0.65348 | val_1_rmse: 0.65367 |  0:07:05s
epoch 78 | loss: 0.32137 | val_0_rmse: 0.61324 | val_1_rmse: 0.61443 |  0:07:11s
epoch 79 | loss: 0.31984 | val_0_rmse: 0.72929 | val_1_rmse: 0.73343 |  0:07:16s
epoch 80 | loss: 0.32095 | val_0_rmse: 0.6656  | val_1_rmse: 0.66652 |  0:07:22s
epoch 81 | loss: 0.32415 | val_0_rmse: 0.63833 | val_1_rmse: 0.6391  |  0:07:27s
epoch 82 | loss: 0.31979 | val_0_rmse: 0.76054 | val_1_rmse: 0.75977 |  0:07:32s
epoch 83 | loss: 0.31786 | val_0_rmse: 0.59669 | val_1_rmse: 0.60083 |  0:07:38s
epoch 84 | loss: 0.31836 | val_0_rmse: 0.61922 | val_1_rmse: 0.62371 |  0:07:43s
epoch 85 | loss: 0.31959 | val_0_rmse: 0.83541 | val_1_rmse: 0.83613 |  0:07:49s
epoch 86 | loss: 0.32059 | val_0_rmse: 0.71276 | val_1_rmse: 0.71126 |  0:07:54s
epoch 87 | loss: 0.31843 | val_0_rmse: 0.58903 | val_1_rmse: 0.59238 |  0:08:00s
epoch 88 | loss: 0.31722 | val_0_rmse: 1.10881 | val_1_rmse: 1.11204 |  0:08:05s
epoch 89 | loss: 0.31842 | val_0_rmse: 0.68122 | val_1_rmse: 0.6829  |  0:08:11s
epoch 90 | loss: 0.32049 | val_0_rmse: 0.71872 | val_1_rmse: 0.72422 |  0:08:16s
epoch 91 | loss: 0.31922 | val_0_rmse: 0.58945 | val_1_rmse: 0.59407 |  0:08:21s
epoch 92 | loss: 0.3182  | val_0_rmse: 0.61199 | val_1_rmse: 0.61728 |  0:08:27s
epoch 93 | loss: 0.31706 | val_0_rmse: 0.63933 | val_1_rmse: 0.64076 |  0:08:32s
epoch 94 | loss: 0.31737 | val_0_rmse: 0.55476 | val_1_rmse: 0.5605  |  0:08:38s
epoch 95 | loss: 0.31849 | val_0_rmse: 0.59545 | val_1_rmse: 0.59979 |  0:08:43s
epoch 96 | loss: 0.32158 | val_0_rmse: 0.63928 | val_1_rmse: 0.64105 |  0:08:49s
epoch 97 | loss: 0.32052 | val_0_rmse: 0.66994 | val_1_rmse: 0.67487 |  0:08:54s
epoch 98 | loss: 0.32222 | val_0_rmse: 0.6925  | val_1_rmse: 0.69593 |  0:08:59s
epoch 99 | loss: 0.31768 | val_0_rmse: 0.71395 | val_1_rmse: 0.71722 |  0:09:05s
epoch 100| loss: 0.31902 | val_0_rmse: 0.58384 | val_1_rmse: 0.58874 |  0:09:10s
epoch 101| loss: 0.31647 | val_0_rmse: 0.56513 | val_1_rmse: 0.57023 |  0:09:16s
epoch 102| loss: 0.31504 | val_0_rmse: 0.74879 | val_1_rmse: 0.75305 |  0:09:21s
epoch 103| loss: 0.31841 | val_0_rmse: 0.72246 | val_1_rmse: 0.72625 |  0:09:27s
epoch 104| loss: 0.3178  | val_0_rmse: 0.6783  | val_1_rmse: 0.67913 |  0:09:32s
epoch 105| loss: 0.31745 | val_0_rmse: 0.61721 | val_1_rmse: 0.61954 |  0:09:37s
epoch 106| loss: 0.31664 | val_0_rmse: 0.59873 | val_1_rmse: 0.60087 |  0:09:43s
epoch 107| loss: 0.31786 | val_0_rmse: 0.77314 | val_1_rmse: 0.77061 |  0:09:48s
epoch 108| loss: 0.31696 | val_0_rmse: 0.67551 | val_1_rmse: 0.67805 |  0:09:54s
epoch 109| loss: 0.32062 | val_0_rmse: 0.65865 | val_1_rmse: 0.6621  |  0:09:59s
epoch 110| loss: 0.31768 | val_0_rmse: 0.66768 | val_1_rmse: 0.67007 |  0:10:05s
epoch 111| loss: 0.31845 | val_0_rmse: 0.64811 | val_1_rmse: 0.64893 |  0:10:10s
epoch 112| loss: 0.3164  | val_0_rmse: 0.65676 | val_1_rmse: 0.65793 |  0:10:15s
epoch 113| loss: 0.31674 | val_0_rmse: 0.62761 | val_1_rmse: 0.63006 |  0:10:21s
epoch 114| loss: 0.3152  | val_0_rmse: 0.56156 | val_1_rmse: 0.56745 |  0:10:26s
epoch 115| loss: 0.31446 | val_0_rmse: 0.6432  | val_1_rmse: 0.646   |  0:10:32s
epoch 116| loss: 0.31539 | val_0_rmse: 1.56416 | val_1_rmse: 1.58613 |  0:10:37s
epoch 117| loss: 0.31742 | val_0_rmse: 0.58415 | val_1_rmse: 0.58843 |  0:10:43s
epoch 118| loss: 0.31543 | val_0_rmse: 0.60714 | val_1_rmse: 0.60914 |  0:10:48s
epoch 119| loss: 0.31617 | val_0_rmse: 0.86043 | val_1_rmse: 0.86059 |  0:10:54s
epoch 120| loss: 0.31629 | val_0_rmse: 0.57424 | val_1_rmse: 0.57729 |  0:10:59s
epoch 121| loss: 0.31854 | val_0_rmse: 0.55516 | val_1_rmse: 0.55972 |  0:11:04s
epoch 122| loss: 0.31373 | val_0_rmse: 0.568   | val_1_rmse: 0.5727  |  0:11:10s
epoch 123| loss: 0.31386 | val_0_rmse: 0.63766 | val_1_rmse: 0.63869 |  0:11:15s
epoch 124| loss: 0.31618 | val_0_rmse: 0.63713 | val_1_rmse: 0.63862 |  0:11:21s
epoch 125| loss: 0.31513 | val_0_rmse: 1.14972 | val_1_rmse: 1.14872 |  0:11:26s
epoch 126| loss: 0.31556 | val_0_rmse: 0.7609  | val_1_rmse: 0.76053 |  0:11:32s
epoch 127| loss: 0.31073 | val_0_rmse: 0.66622 | val_1_rmse: 0.67055 |  0:11:37s
epoch 128| loss: 0.31518 | val_0_rmse: 0.79322 | val_1_rmse: 0.79897 |  0:11:43s
epoch 129| loss: 0.31381 | val_0_rmse: 0.65204 | val_1_rmse: 0.65358 |  0:11:48s
epoch 130| loss: 0.31535 | val_0_rmse: 0.57417 | val_1_rmse: 0.57721 |  0:11:53s
epoch 131| loss: 0.31338 | val_0_rmse: 0.63888 | val_1_rmse: 0.64483 |  0:11:59s
epoch 132| loss: 0.313   | val_0_rmse: 0.61348 | val_1_rmse: 0.6165  |  0:12:04s
epoch 133| loss: 0.31445 | val_0_rmse: 0.62718 | val_1_rmse: 0.62862 |  0:12:10s
epoch 134| loss: 0.31423 | val_0_rmse: 0.73181 | val_1_rmse: 0.73382 |  0:12:15s
epoch 135| loss: 0.31982 | val_0_rmse: 0.58122 | val_1_rmse: 0.58618 |  0:12:21s
epoch 136| loss: 0.31659 | val_0_rmse: 0.61844 | val_1_rmse: 0.61877 |  0:12:26s
epoch 137| loss: 0.31561 | val_0_rmse: 0.64082 | val_1_rmse: 0.64134 |  0:12:32s
epoch 138| loss: 0.31636 | val_0_rmse: 0.72302 | val_1_rmse: 0.72639 |  0:12:37s
epoch 139| loss: 0.3143  | val_0_rmse: 0.68569 | val_1_rmse: 0.69138 |  0:12:42s
epoch 140| loss: 0.3121  | val_0_rmse: 0.6442  | val_1_rmse: 0.6386  |  0:12:48s
epoch 141| loss: 0.31538 | val_0_rmse: 1.09801 | val_1_rmse: 1.10519 |  0:12:53s
epoch 142| loss: 0.31551 | val_0_rmse: 0.5787  | val_1_rmse: 0.58693 |  0:12:59s
epoch 143| loss: 0.31064 | val_0_rmse: 0.57271 | val_1_rmse: 0.57629 |  0:13:04s
epoch 144| loss: 0.31352 | val_0_rmse: 0.60319 | val_1_rmse: 0.6103  |  0:13:10s
epoch 145| loss: 0.31174 | val_0_rmse: 0.61047 | val_1_rmse: 0.61422 |  0:13:15s
epoch 146| loss: 0.31164 | val_0_rmse: 0.61867 | val_1_rmse: 0.62248 |  0:13:21s
epoch 147| loss: 0.31182 | val_0_rmse: 0.63199 | val_1_rmse: 0.63441 |  0:13:26s
epoch 148| loss: 0.31257 | val_0_rmse: 0.67698 | val_1_rmse: 0.68236 |  0:13:32s
epoch 149| loss: 0.31222 | val_0_rmse: 0.61    | val_1_rmse: 0.6124  |  0:13:37s
Stop training because you reached max_epochs = 150 with best_epoch = 121 and best_val_1_rmse = 0.55972
Best weights from best epoch are automatically used!
ended training at: 06:13:27
Feature importance:
[('Area', 0.2659929360049022), ('Baths', 0.2437922309304433), ('Beds', 0.01575954702798513), ('Latitude', 0.20908036076933958), ('Longitude', 0.17863336787051076), ('Month', 0.0), ('Year', 0.08674155739681902)]
Mean squared error is of 2074233518.104689
Mean absolute error:32863.142076264296
MAPE:0.31396251759857935
R2 score:0.6919785987789653
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:13:27
epoch 0  | loss: 0.59148 | val_0_rmse: 0.70849 | val_1_rmse: 0.70823 |  0:00:05s
epoch 1  | loss: 0.46301 | val_0_rmse: 0.76102 | val_1_rmse: 0.75859 |  0:00:10s
epoch 2  | loss: 0.41244 | val_0_rmse: 0.68038 | val_1_rmse: 0.67893 |  0:00:16s
epoch 3  | loss: 0.38704 | val_0_rmse: 0.64858 | val_1_rmse: 0.64629 |  0:00:21s
epoch 4  | loss: 0.38416 | val_0_rmse: 0.6727  | val_1_rmse: 0.6708  |  0:00:27s
epoch 5  | loss: 0.37055 | val_0_rmse: 0.63873 | val_1_rmse: 0.63847 |  0:00:32s
epoch 6  | loss: 0.37163 | val_0_rmse: 0.66175 | val_1_rmse: 0.665   |  0:00:38s
epoch 7  | loss: 0.36356 | val_0_rmse: 0.5938  | val_1_rmse: 0.59566 |  0:00:43s
epoch 8  | loss: 0.36634 | val_0_rmse: 0.69819 | val_1_rmse: 0.69822 |  0:00:49s
epoch 9  | loss: 0.35876 | val_0_rmse: 0.63275 | val_1_rmse: 0.62991 |  0:00:54s
epoch 10 | loss: 0.35735 | val_0_rmse: 0.64686 | val_1_rmse: 0.64505 |  0:01:00s
epoch 11 | loss: 0.35758 | val_0_rmse: 0.66434 | val_1_rmse: 0.66137 |  0:01:05s
epoch 12 | loss: 0.35422 | val_0_rmse: 0.66388 | val_1_rmse: 0.66752 |  0:01:11s
epoch 13 | loss: 0.35151 | val_0_rmse: 0.59961 | val_1_rmse: 0.60197 |  0:01:16s
epoch 14 | loss: 0.34968 | val_0_rmse: 0.59677 | val_1_rmse: 0.59954 |  0:01:22s
epoch 15 | loss: 0.34869 | val_0_rmse: 0.64121 | val_1_rmse: 0.64288 |  0:01:27s
epoch 16 | loss: 0.34492 | val_0_rmse: 0.66597 | val_1_rmse: 0.66165 |  0:01:32s
epoch 17 | loss: 0.34753 | val_0_rmse: 0.66422 | val_1_rmse: 0.66311 |  0:01:38s
epoch 18 | loss: 0.3419  | val_0_rmse: 0.59084 | val_1_rmse: 0.59226 |  0:01:43s
epoch 19 | loss: 0.34291 | val_0_rmse: 0.65348 | val_1_rmse: 0.65352 |  0:01:49s
epoch 20 | loss: 0.33881 | val_0_rmse: 0.65682 | val_1_rmse: 0.66278 |  0:01:54s
epoch 21 | loss: 0.34289 | val_0_rmse: 0.58122 | val_1_rmse: 0.58657 |  0:02:00s
epoch 22 | loss: 0.34289 | val_0_rmse: 0.6418  | val_1_rmse: 0.64421 |  0:02:05s
epoch 23 | loss: 0.35187 | val_0_rmse: 0.6394  | val_1_rmse: 0.63883 |  0:02:11s
epoch 24 | loss: 0.34597 | val_0_rmse: 0.62734 | val_1_rmse: 0.63019 |  0:02:16s
epoch 25 | loss: 0.34206 | val_0_rmse: 0.63058 | val_1_rmse: 0.6289  |  0:02:22s
epoch 26 | loss: 0.34524 | val_0_rmse: 0.73669 | val_1_rmse: 0.73888 |  0:02:27s
epoch 27 | loss: 0.3402  | val_0_rmse: 0.64688 | val_1_rmse: 0.64779 |  0:02:33s
epoch 28 | loss: 0.33744 | val_0_rmse: 0.60922 | val_1_rmse: 0.6104  |  0:02:38s
epoch 29 | loss: 0.33917 | val_0_rmse: 0.58235 | val_1_rmse: 0.58272 |  0:02:44s
epoch 30 | loss: 0.34651 | val_0_rmse: 0.64685 | val_1_rmse: 0.64448 |  0:02:49s
epoch 31 | loss: 0.34414 | val_0_rmse: 0.59242 | val_1_rmse: 0.59323 |  0:02:54s
epoch 32 | loss: 0.34087 | val_0_rmse: 0.59489 | val_1_rmse: 0.59692 |  0:03:00s
epoch 33 | loss: 0.33624 | val_0_rmse: 0.61715 | val_1_rmse: 0.61312 |  0:03:05s
epoch 34 | loss: 0.33423 | val_0_rmse: 0.73762 | val_1_rmse: 0.73493 |  0:03:11s
epoch 35 | loss: 0.3361  | val_0_rmse: 0.59416 | val_1_rmse: 0.59591 |  0:03:16s
epoch 36 | loss: 0.33161 | val_0_rmse: 0.62794 | val_1_rmse: 0.62568 |  0:03:22s
epoch 37 | loss: 0.33205 | val_0_rmse: 0.64829 | val_1_rmse: 0.6437  |  0:03:27s
epoch 38 | loss: 0.33398 | val_0_rmse: 0.62648 | val_1_rmse: 0.62746 |  0:03:33s
epoch 39 | loss: 0.33359 | val_0_rmse: 0.61984 | val_1_rmse: 0.61802 |  0:03:38s
epoch 40 | loss: 0.3321  | val_0_rmse: 0.59985 | val_1_rmse: 0.60497 |  0:03:44s
epoch 41 | loss: 0.33144 | val_0_rmse: 0.58731 | val_1_rmse: 0.59199 |  0:03:49s
epoch 42 | loss: 0.33063 | val_0_rmse: 0.66878 | val_1_rmse: 0.6727  |  0:03:55s
epoch 43 | loss: 0.33413 | val_0_rmse: 0.59077 | val_1_rmse: 0.59571 |  0:04:00s
epoch 44 | loss: 0.3331  | val_0_rmse: 0.58115 | val_1_rmse: 0.58531 |  0:04:06s
epoch 45 | loss: 0.32949 | val_0_rmse: 0.65934 | val_1_rmse: 0.65373 |  0:04:11s
epoch 46 | loss: 0.33189 | val_0_rmse: 0.61648 | val_1_rmse: 0.62014 |  0:04:16s
epoch 47 | loss: 0.33196 | val_0_rmse: 0.57343 | val_1_rmse: 0.57585 |  0:04:22s
epoch 48 | loss: 0.32776 | val_0_rmse: 0.59209 | val_1_rmse: 0.59693 |  0:04:28s
epoch 49 | loss: 0.32794 | val_0_rmse: 0.66595 | val_1_rmse: 0.67007 |  0:04:33s
epoch 50 | loss: 0.3257  | val_0_rmse: 0.60147 | val_1_rmse: 0.60707 |  0:04:39s
epoch 51 | loss: 0.33141 | val_0_rmse: 0.81954 | val_1_rmse: 0.8203  |  0:04:44s
epoch 52 | loss: 0.33443 | val_0_rmse: 0.63281 | val_1_rmse: 0.63193 |  0:04:50s
epoch 53 | loss: 0.33566 | val_0_rmse: 0.59975 | val_1_rmse: 0.59987 |  0:04:55s
epoch 54 | loss: 0.32926 | val_0_rmse: 0.60325 | val_1_rmse: 0.60948 |  0:05:01s
epoch 55 | loss: 0.32833 | val_0_rmse: 0.60331 | val_1_rmse: 0.60249 |  0:05:06s
epoch 56 | loss: 0.32564 | val_0_rmse: 0.58502 | val_1_rmse: 0.58846 |  0:05:11s
epoch 57 | loss: 0.32359 | val_0_rmse: 0.59995 | val_1_rmse: 0.59946 |  0:05:17s
epoch 58 | loss: 0.32396 | val_0_rmse: 0.58492 | val_1_rmse: 0.58681 |  0:05:22s
epoch 59 | loss: 0.32715 | val_0_rmse: 0.57282 | val_1_rmse: 0.57568 |  0:05:28s
epoch 60 | loss: 0.32858 | val_0_rmse: 0.60329 | val_1_rmse: 0.60392 |  0:05:33s
epoch 61 | loss: 0.3261  | val_0_rmse: 0.57376 | val_1_rmse: 0.57648 |  0:05:39s
epoch 62 | loss: 0.33591 | val_0_rmse: 0.59571 | val_1_rmse: 0.59838 |  0:05:44s
epoch 63 | loss: 0.32931 | val_0_rmse: 0.6612  | val_1_rmse: 0.65663 |  0:05:50s
epoch 64 | loss: 0.32845 | val_0_rmse: 0.59309 | val_1_rmse: 0.59589 |  0:05:55s
epoch 65 | loss: 0.33084 | val_0_rmse: 0.62515 | val_1_rmse: 0.62303 |  0:06:01s
epoch 66 | loss: 0.32679 | val_0_rmse: 0.68269 | val_1_rmse: 0.68027 |  0:06:06s
epoch 67 | loss: 0.32732 | val_0_rmse: 0.61408 | val_1_rmse: 0.61925 |  0:06:12s
epoch 68 | loss: 0.32533 | val_0_rmse: 0.69874 | val_1_rmse: 0.7017  |  0:06:17s
epoch 69 | loss: 0.32255 | val_0_rmse: 0.81526 | val_1_rmse: 0.81932 |  0:06:23s
epoch 70 | loss: 0.3244  | val_0_rmse: 0.61811 | val_1_rmse: 0.62261 |  0:06:28s
epoch 71 | loss: 0.32234 | val_0_rmse: 0.67066 | val_1_rmse: 0.66774 |  0:06:34s
epoch 72 | loss: 0.32838 | val_0_rmse: 0.88804 | val_1_rmse: 0.88923 |  0:06:39s
epoch 73 | loss: 0.32455 | val_0_rmse: 0.72912 | val_1_rmse: 0.7242  |  0:06:44s
epoch 74 | loss: 0.32238 | val_0_rmse: 1.00793 | val_1_rmse: 1.01073 |  0:06:50s
epoch 75 | loss: 0.32068 | val_0_rmse: 0.58987 | val_1_rmse: 0.59165 |  0:06:55s
epoch 76 | loss: 0.32346 | val_0_rmse: 0.58429 | val_1_rmse: 0.58982 |  0:07:01s
epoch 77 | loss: 0.3234  | val_0_rmse: 0.62151 | val_1_rmse: 0.62676 |  0:07:06s
epoch 78 | loss: 0.32404 | val_0_rmse: 0.57676 | val_1_rmse: 0.5804  |  0:07:12s
epoch 79 | loss: 0.32447 | val_0_rmse: 0.61051 | val_1_rmse: 0.6097  |  0:07:17s
epoch 80 | loss: 0.32196 | val_0_rmse: 0.64682 | val_1_rmse: 0.64335 |  0:07:23s
epoch 81 | loss: 0.31977 | val_0_rmse: 0.64254 | val_1_rmse: 0.63999 |  0:07:28s
epoch 82 | loss: 0.32098 | val_0_rmse: 0.71058 | val_1_rmse: 0.7048  |  0:07:34s
epoch 83 | loss: 0.31988 | val_0_rmse: 0.61396 | val_1_rmse: 0.61653 |  0:07:39s
epoch 84 | loss: 0.33012 | val_0_rmse: 0.62301 | val_1_rmse: 0.62736 |  0:07:44s
epoch 85 | loss: 0.32433 | val_0_rmse: 0.60666 | val_1_rmse: 0.60796 |  0:07:50s
epoch 86 | loss: 0.32114 | val_0_rmse: 0.58346 | val_1_rmse: 0.58573 |  0:07:55s
epoch 87 | loss: 0.32031 | val_0_rmse: 0.59113 | val_1_rmse: 0.59125 |  0:08:01s
epoch 88 | loss: 0.31736 | val_0_rmse: 0.60983 | val_1_rmse: 0.60952 |  0:08:06s
epoch 89 | loss: 0.3216  | val_0_rmse: 0.7102  | val_1_rmse: 0.71115 |  0:08:12s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.57568
Best weights from best epoch are automatically used!
ended training at: 06:21:41
Feature importance:
[('Area', 0.43790625043190295), ('Baths', 0.2091708029383742), ('Beds', 0.06639374563375956), ('Latitude', 0.1429445663253167), ('Longitude', 0.13505008385349546), ('Month', 0.0), ('Year', 0.008534550817151093)]
Mean squared error is of 2268490234.5926757
Mean absolute error:34571.37907515997
MAPE:0.3508596922912237
R2 score:0.6685339808096598
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:42
epoch 0  | loss: 0.60175 | val_0_rmse: 0.73422 | val_1_rmse: 0.73644 |  0:00:05s
epoch 1  | loss: 0.50562 | val_0_rmse: 0.71904 | val_1_rmse: 0.72107 |  0:00:10s
epoch 2  | loss: 0.47877 | val_0_rmse: 0.688   | val_1_rmse: 0.68992 |  0:00:16s
epoch 3  | loss: 0.44752 | val_0_rmse: 0.63876 | val_1_rmse: 0.64268 |  0:00:21s
epoch 4  | loss: 0.41787 | val_0_rmse: 0.6468  | val_1_rmse: 0.65072 |  0:00:27s
epoch 5  | loss: 0.40674 | val_0_rmse: 0.62277 | val_1_rmse: 0.62675 |  0:00:32s
epoch 6  | loss: 0.4014  | val_0_rmse: 0.63261 | val_1_rmse: 0.6373  |  0:00:38s
epoch 7  | loss: 0.40861 | val_0_rmse: 0.65038 | val_1_rmse: 0.65457 |  0:00:43s
epoch 8  | loss: 0.40244 | val_0_rmse: 0.64309 | val_1_rmse: 0.64764 |  0:00:49s
epoch 9  | loss: 0.39798 | val_0_rmse: 0.64683 | val_1_rmse: 0.65178 |  0:00:54s
epoch 10 | loss: 0.39817 | val_0_rmse: 0.62068 | val_1_rmse: 0.62407 |  0:01:00s
epoch 11 | loss: 0.38893 | val_0_rmse: 0.61408 | val_1_rmse: 0.61788 |  0:01:05s
epoch 12 | loss: 0.38883 | val_0_rmse: 0.62012 | val_1_rmse: 0.62366 |  0:01:10s
epoch 13 | loss: 0.38872 | val_0_rmse: 0.65243 | val_1_rmse: 0.65664 |  0:01:16s
epoch 14 | loss: 0.38665 | val_0_rmse: 0.63427 | val_1_rmse: 0.6378  |  0:01:21s
epoch 15 | loss: 0.38392 | val_0_rmse: 0.65767 | val_1_rmse: 0.66315 |  0:01:27s
epoch 16 | loss: 0.38719 | val_0_rmse: 0.67548 | val_1_rmse: 0.68161 |  0:01:32s
epoch 17 | loss: 0.37996 | val_0_rmse: 0.61593 | val_1_rmse: 0.62007 |  0:01:38s
epoch 18 | loss: 0.38235 | val_0_rmse: 0.62841 | val_1_rmse: 0.63376 |  0:01:43s
epoch 19 | loss: 0.38014 | val_0_rmse: 0.59924 | val_1_rmse: 0.60296 |  0:01:49s
epoch 20 | loss: 0.37548 | val_0_rmse: 0.60076 | val_1_rmse: 0.60467 |  0:01:54s
epoch 21 | loss: 0.37543 | val_0_rmse: 0.61189 | val_1_rmse: 0.61405 |  0:02:00s
epoch 22 | loss: 0.38119 | val_0_rmse: 0.69666 | val_1_rmse: 0.69984 |  0:02:05s
epoch 23 | loss: 0.37658 | val_0_rmse: 0.60035 | val_1_rmse: 0.6045  |  0:02:11s
epoch 24 | loss: 0.37171 | val_0_rmse: 0.61273 | val_1_rmse: 0.61995 |  0:02:16s
epoch 25 | loss: 0.37461 | val_0_rmse: 0.59175 | val_1_rmse: 0.59749 |  0:02:22s
epoch 26 | loss: 0.36958 | val_0_rmse: 0.63552 | val_1_rmse: 0.64189 |  0:02:27s
epoch 27 | loss: 0.36745 | val_0_rmse: 0.61205 | val_1_rmse: 0.61486 |  0:02:32s
epoch 28 | loss: 0.36388 | val_0_rmse: 0.62314 | val_1_rmse: 0.62759 |  0:02:38s
epoch 29 | loss: 0.36281 | val_0_rmse: 0.59968 | val_1_rmse: 0.60429 |  0:02:43s
epoch 30 | loss: 0.35665 | val_0_rmse: 0.59768 | val_1_rmse: 0.60117 |  0:02:49s
epoch 31 | loss: 0.35815 | val_0_rmse: 0.59956 | val_1_rmse: 0.6014  |  0:02:54s
epoch 32 | loss: 0.35503 | val_0_rmse: 0.59647 | val_1_rmse: 0.60047 |  0:03:00s
epoch 33 | loss: 0.3557  | val_0_rmse: 0.63761 | val_1_rmse: 0.64542 |  0:03:05s
epoch 34 | loss: 0.37706 | val_0_rmse: 0.60632 | val_1_rmse: 0.60969 |  0:03:11s
epoch 35 | loss: 0.36251 | val_0_rmse: 0.63052 | val_1_rmse: 0.63291 |  0:03:16s
epoch 36 | loss: 0.35551 | val_0_rmse: 0.81286 | val_1_rmse: 0.8119  |  0:03:21s
epoch 37 | loss: 0.37113 | val_0_rmse: 0.7788  | val_1_rmse: 0.78552 |  0:03:27s
epoch 38 | loss: 0.36029 | val_0_rmse: 0.585   | val_1_rmse: 0.58901 |  0:03:32s
epoch 39 | loss: 0.35488 | val_0_rmse: 0.63269 | val_1_rmse: 0.63423 |  0:03:38s
epoch 40 | loss: 0.351   | val_0_rmse: 0.87403 | val_1_rmse: 0.88293 |  0:03:43s
epoch 41 | loss: 0.35166 | val_0_rmse: 0.65364 | val_1_rmse: 0.65833 |  0:03:49s
epoch 42 | loss: 0.35187 | val_0_rmse: 0.67692 | val_1_rmse: 0.67644 |  0:03:54s
epoch 43 | loss: 0.40096 | val_0_rmse: 0.68304 | val_1_rmse: 0.68801 |  0:03:59s
epoch 44 | loss: 0.38778 | val_0_rmse: 0.68821 | val_1_rmse: 0.69185 |  0:04:05s
epoch 45 | loss: 0.40179 | val_0_rmse: 0.86373 | val_1_rmse: 0.86923 |  0:04:10s
epoch 46 | loss: 0.3944  | val_0_rmse: 0.66268 | val_1_rmse: 0.66444 |  0:04:16s
epoch 47 | loss: 0.38516 | val_0_rmse: 0.63363 | val_1_rmse: 0.63784 |  0:04:21s
epoch 48 | loss: 0.3942  | val_0_rmse: 0.72602 | val_1_rmse: 0.73101 |  0:04:27s
epoch 49 | loss: 0.44399 | val_0_rmse: 0.64689 | val_1_rmse: 0.64823 |  0:04:32s
epoch 50 | loss: 0.42605 | val_0_rmse: 0.67329 | val_1_rmse: 0.6755  |  0:04:38s
epoch 51 | loss: 0.42552 | val_0_rmse: 0.6925  | val_1_rmse: 0.69649 |  0:04:43s
epoch 52 | loss: 0.41906 | val_0_rmse: 0.67585 | val_1_rmse: 0.67976 |  0:04:49s
epoch 53 | loss: 0.41834 | val_0_rmse: 0.68806 | val_1_rmse: 0.69137 |  0:04:54s
epoch 54 | loss: 0.41478 | val_0_rmse: 0.70984 | val_1_rmse: 0.711   |  0:05:00s
epoch 55 | loss: 0.41212 | val_0_rmse: 0.65293 | val_1_rmse: 0.65553 |  0:05:05s
epoch 56 | loss: 0.40481 | val_0_rmse: 0.68226 | val_1_rmse: 0.68602 |  0:05:10s
epoch 57 | loss: 0.40182 | val_0_rmse: 0.6629  | val_1_rmse: 0.66487 |  0:05:16s
epoch 58 | loss: 0.40007 | val_0_rmse: 0.63879 | val_1_rmse: 0.64109 |  0:05:21s
epoch 59 | loss: 0.3938  | val_0_rmse: 0.61464 | val_1_rmse: 0.61739 |  0:05:27s
epoch 60 | loss: 0.38096 | val_0_rmse: 0.64004 | val_1_rmse: 0.64337 |  0:05:32s
epoch 61 | loss: 0.3816  | val_0_rmse: 0.66177 | val_1_rmse: 0.66487 |  0:05:38s
epoch 62 | loss: 0.38048 | val_0_rmse: 0.62154 | val_1_rmse: 0.62382 |  0:05:43s
epoch 63 | loss: 0.38177 | val_0_rmse: 0.61854 | val_1_rmse: 0.62102 |  0:05:49s
epoch 64 | loss: 0.37926 | val_0_rmse: 0.62594 | val_1_rmse: 0.62803 |  0:05:54s
epoch 65 | loss: 0.38776 | val_0_rmse: 0.64705 | val_1_rmse: 0.65119 |  0:06:00s
epoch 66 | loss: 0.38396 | val_0_rmse: 0.61904 | val_1_rmse: 0.62368 |  0:06:05s
epoch 67 | loss: 0.38026 | val_0_rmse: 0.62594 | val_1_rmse: 0.62745 |  0:06:11s
epoch 68 | loss: 0.37962 | val_0_rmse: 0.62546 | val_1_rmse: 0.62731 |  0:06:16s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.58901
Best weights from best epoch are automatically used!
ended training at: 06:28:00
Feature importance:
[('Area', 0.38187804205587095), ('Baths', 0.13540580868331809), ('Beds', 0.0), ('Latitude', 0.25552953035616355), ('Longitude', 0.21638232484170727), ('Month', 0.0), ('Year', 0.010804294062940145)]
Mean squared error is of 2260448624.8327675
Mean absolute error:34785.8102944791
MAPE:0.34480273463324573
R2 score:0.6662817074510781
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:01
epoch 0  | loss: 0.60906 | val_0_rmse: 0.7235  | val_1_rmse: 0.72311 |  0:00:05s
epoch 1  | loss: 0.51142 | val_0_rmse: 0.69126 | val_1_rmse: 0.69124 |  0:00:11s
epoch 2  | loss: 0.46238 | val_0_rmse: 0.72881 | val_1_rmse: 0.73111 |  0:00:16s
epoch 3  | loss: 0.41777 | val_0_rmse: 0.62536 | val_1_rmse: 0.62679 |  0:00:21s
epoch 4  | loss: 0.39728 | val_0_rmse: 0.64141 | val_1_rmse: 0.64162 |  0:00:27s
epoch 5  | loss: 0.39458 | val_0_rmse: 0.71792 | val_1_rmse: 0.72222 |  0:00:32s
epoch 6  | loss: 0.38322 | val_0_rmse: 0.63103 | val_1_rmse: 0.63093 |  0:00:38s
epoch 7  | loss: 0.37386 | val_0_rmse: 0.67067 | val_1_rmse: 0.67321 |  0:00:43s
epoch 8  | loss: 0.3674  | val_0_rmse: 0.59989 | val_1_rmse: 0.6006  |  0:00:49s
epoch 9  | loss: 0.36503 | val_0_rmse: 0.6275  | val_1_rmse: 0.62777 |  0:00:54s
epoch 10 | loss: 0.36728 | val_0_rmse: 0.64385 | val_1_rmse: 0.64282 |  0:01:00s
epoch 11 | loss: 0.36317 | val_0_rmse: 0.6606  | val_1_rmse: 0.6609  |  0:01:05s
epoch 12 | loss: 0.36156 | val_0_rmse: 0.66367 | val_1_rmse: 0.66743 |  0:01:11s
epoch 13 | loss: 0.35858 | val_0_rmse: 0.68466 | val_1_rmse: 0.68494 |  0:01:16s
epoch 14 | loss: 0.35857 | val_0_rmse: 0.63331 | val_1_rmse: 0.63295 |  0:01:22s
epoch 15 | loss: 0.35793 | val_0_rmse: 0.89164 | val_1_rmse: 0.90041 |  0:01:27s
epoch 16 | loss: 0.35691 | val_0_rmse: 0.63934 | val_1_rmse: 0.63971 |  0:01:32s
epoch 17 | loss: 0.34952 | val_0_rmse: 0.74101 | val_1_rmse: 0.741   |  0:01:38s
epoch 18 | loss: 0.35559 | val_0_rmse: 0.68411 | val_1_rmse: 0.68386 |  0:01:43s
epoch 19 | loss: 0.35073 | val_0_rmse: 0.61948 | val_1_rmse: 0.62373 |  0:01:49s
epoch 20 | loss: 0.35276 | val_0_rmse: 0.62875 | val_1_rmse: 0.63347 |  0:01:54s
epoch 21 | loss: 0.34728 | val_0_rmse: 0.65647 | val_1_rmse: 0.66232 |  0:02:00s
epoch 22 | loss: 0.35061 | val_0_rmse: 1.41278 | val_1_rmse: 1.41086 |  0:02:05s
epoch 23 | loss: 0.34745 | val_0_rmse: 0.67904 | val_1_rmse: 0.68584 |  0:02:11s
epoch 24 | loss: 0.34506 | val_0_rmse: 0.61094 | val_1_rmse: 0.61567 |  0:02:16s
epoch 25 | loss: 0.34633 | val_0_rmse: 0.69815 | val_1_rmse: 0.70561 |  0:02:22s
epoch 26 | loss: 0.34447 | val_0_rmse: 0.61417 | val_1_rmse: 0.618   |  0:02:27s
epoch 27 | loss: 0.34682 | val_0_rmse: 0.61331 | val_1_rmse: 0.61795 |  0:02:32s
epoch 28 | loss: 0.34832 | val_0_rmse: 0.61559 | val_1_rmse: 0.61568 |  0:02:38s
epoch 29 | loss: 0.34648 | val_0_rmse: 0.71685 | val_1_rmse: 0.72255 |  0:02:43s
epoch 30 | loss: 0.34401 | val_0_rmse: 0.60496 | val_1_rmse: 0.60675 |  0:02:49s
epoch 31 | loss: 0.34491 | val_0_rmse: 0.86228 | val_1_rmse: 0.86244 |  0:02:54s
epoch 32 | loss: 0.34372 | val_0_rmse: 0.7988  | val_1_rmse: 0.79728 |  0:03:00s
epoch 33 | loss: 0.34733 | val_0_rmse: 0.769   | val_1_rmse: 0.77564 |  0:03:05s
epoch 34 | loss: 0.34702 | val_0_rmse: 0.86519 | val_1_rmse: 0.86429 |  0:03:11s
epoch 35 | loss: 0.34488 | val_0_rmse: 0.62312 | val_1_rmse: 0.62311 |  0:03:16s
epoch 36 | loss: 0.34748 | val_0_rmse: 0.71334 | val_1_rmse: 0.71309 |  0:03:22s
epoch 37 | loss: 0.34423 | val_0_rmse: 0.72801 | val_1_rmse: 0.72851 |  0:03:27s
epoch 38 | loss: 0.34414 | val_0_rmse: 0.58061 | val_1_rmse: 0.58221 |  0:03:33s
epoch 39 | loss: 0.34294 | val_0_rmse: 1.56842 | val_1_rmse: 1.5711  |  0:03:38s
epoch 40 | loss: 0.34504 | val_0_rmse: 0.65572 | val_1_rmse: 0.65835 |  0:03:43s
epoch 41 | loss: 0.34142 | val_0_rmse: 0.65057 | val_1_rmse: 0.65668 |  0:03:49s
epoch 42 | loss: 0.34022 | val_0_rmse: 0.71015 | val_1_rmse: 0.71596 |  0:03:54s
epoch 43 | loss: 0.34179 | val_0_rmse: 0.57989 | val_1_rmse: 0.58381 |  0:04:00s
epoch 44 | loss: 0.34325 | val_0_rmse: 0.76603 | val_1_rmse: 0.7648  |  0:04:05s
epoch 45 | loss: 0.34296 | val_0_rmse: 0.84321 | val_1_rmse: 0.84549 |  0:04:11s
epoch 46 | loss: 0.33912 | val_0_rmse: 0.75902 | val_1_rmse: 0.75657 |  0:04:16s
epoch 47 | loss: 0.33878 | val_0_rmse: 0.76253 | val_1_rmse: 0.76485 |  0:04:22s
epoch 48 | loss: 0.33778 | val_0_rmse: 0.77236 | val_1_rmse: 0.78083 |  0:04:27s
epoch 49 | loss: 0.3411  | val_0_rmse: 1.02395 | val_1_rmse: 1.02253 |  0:04:33s
epoch 50 | loss: 0.33705 | val_0_rmse: 0.59364 | val_1_rmse: 0.59983 |  0:04:38s
epoch 51 | loss: 0.33905 | val_0_rmse: 0.65324 | val_1_rmse: 0.66076 |  0:04:43s
epoch 52 | loss: 0.34339 | val_0_rmse: 0.60262 | val_1_rmse: 0.6072  |  0:04:49s
epoch 53 | loss: 0.3428  | val_0_rmse: 0.79033 | val_1_rmse: 0.79866 |  0:04:54s
epoch 54 | loss: 0.34067 | val_0_rmse: 0.70838 | val_1_rmse: 0.713   |  0:05:00s
epoch 55 | loss: 0.33824 | val_0_rmse: 0.57819 | val_1_rmse: 0.5803  |  0:05:05s
epoch 56 | loss: 0.33985 | val_0_rmse: 0.79569 | val_1_rmse: 0.80343 |  0:05:11s
epoch 57 | loss: 0.33826 | val_0_rmse: 0.61138 | val_1_rmse: 0.61672 |  0:05:16s
epoch 58 | loss: 0.34143 | val_0_rmse: 0.76664 | val_1_rmse: 0.77363 |  0:05:22s
epoch 59 | loss: 0.33817 | val_0_rmse: 0.80831 | val_1_rmse: 0.80834 |  0:05:27s
epoch 60 | loss: 0.33631 | val_0_rmse: 0.59333 | val_1_rmse: 0.59783 |  0:05:32s
epoch 61 | loss: 0.33544 | val_0_rmse: 0.67427 | val_1_rmse: 0.68159 |  0:05:38s
epoch 62 | loss: 0.33379 | val_0_rmse: 1.01335 | val_1_rmse: 1.01244 |  0:05:43s
epoch 63 | loss: 0.33435 | val_0_rmse: 0.57359 | val_1_rmse: 0.57488 |  0:05:49s
epoch 64 | loss: 0.33377 | val_0_rmse: 0.67667 | val_1_rmse: 0.67785 |  0:05:54s
epoch 65 | loss: 0.33689 | val_0_rmse: 0.59057 | val_1_rmse: 0.5953  |  0:06:00s
epoch 66 | loss: 0.3363  | val_0_rmse: 0.67344 | val_1_rmse: 0.68181 |  0:06:05s
epoch 67 | loss: 0.33516 | val_0_rmse: 0.81654 | val_1_rmse: 0.82622 |  0:06:11s
epoch 68 | loss: 0.33696 | val_0_rmse: 0.60271 | val_1_rmse: 0.60501 |  0:06:16s
epoch 69 | loss: 0.33808 | val_0_rmse: 0.75269 | val_1_rmse: 0.75997 |  0:06:22s
epoch 70 | loss: 0.3362  | val_0_rmse: 0.68759 | val_1_rmse: 0.69478 |  0:06:27s
epoch 71 | loss: 0.33728 | val_0_rmse: 0.81882 | val_1_rmse: 0.82686 |  0:06:33s
epoch 72 | loss: 0.33436 | val_0_rmse: 1.32265 | val_1_rmse: 1.32295 |  0:06:38s
epoch 73 | loss: 0.33495 | val_0_rmse: 0.64448 | val_1_rmse: 0.65363 |  0:06:44s
epoch 74 | loss: 0.33454 | val_0_rmse: 0.86528 | val_1_rmse: 0.87265 |  0:06:49s
epoch 75 | loss: 0.34102 | val_0_rmse: 0.6914  | val_1_rmse: 0.69089 |  0:06:55s
epoch 76 | loss: 0.33611 | val_0_rmse: 0.86016 | val_1_rmse: 0.86243 |  0:07:00s
epoch 77 | loss: 0.33577 | val_0_rmse: 0.7374  | val_1_rmse: 0.74219 |  0:07:06s
epoch 78 | loss: 0.33857 | val_0_rmse: 1.28445 | val_1_rmse: 1.285   |  0:07:11s
epoch 79 | loss: 0.33667 | val_0_rmse: 0.69808 | val_1_rmse: 0.70586 |  0:07:17s
epoch 80 | loss: 0.33337 | val_0_rmse: 0.71299 | val_1_rmse: 0.72132 |  0:07:22s
epoch 81 | loss: 0.33511 | val_0_rmse: 0.70063 | val_1_rmse: 0.70773 |  0:07:28s
epoch 82 | loss: 0.33488 | val_0_rmse: 0.66385 | val_1_rmse: 0.66715 |  0:07:33s
epoch 83 | loss: 0.33092 | val_0_rmse: 0.67658 | val_1_rmse: 0.68546 |  0:07:38s
epoch 84 | loss: 0.33475 | val_0_rmse: 0.97565 | val_1_rmse: 0.97569 |  0:07:44s
epoch 85 | loss: 0.33182 | val_0_rmse: 1.55334 | val_1_rmse: 1.55462 |  0:07:49s
epoch 86 | loss: 0.33376 | val_0_rmse: 0.7183  | val_1_rmse: 0.72595 |  0:07:55s
epoch 87 | loss: 0.3316  | val_0_rmse: 0.90065 | val_1_rmse: 0.90069 |  0:08:00s
epoch 88 | loss: 0.33901 | val_0_rmse: 0.6637  | val_1_rmse: 0.66482 |  0:08:06s
epoch 89 | loss: 0.33126 | val_0_rmse: 1.03705 | val_1_rmse: 1.03647 |  0:08:11s
epoch 90 | loss: 0.33267 | val_0_rmse: 0.88602 | val_1_rmse: 0.88676 |  0:08:17s
epoch 91 | loss: 0.33254 | val_0_rmse: 0.88852 | val_1_rmse: 0.89662 |  0:08:22s
epoch 92 | loss: 0.33122 | val_0_rmse: 0.7348  | val_1_rmse: 0.74055 |  0:08:28s
epoch 93 | loss: 0.33267 | val_0_rmse: 1.03508 | val_1_rmse: 1.03629 |  0:08:33s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.57488
Best weights from best epoch are automatically used!
ended training at: 06:36:36
Feature importance:
[('Area', 0.3103822408274661), ('Baths', 0.1010350584617442), ('Beds', 0.0), ('Latitude', 0.3156123806713256), ('Longitude', 0.2629435080038543), ('Month', 0.0), ('Year', 0.010026812035609758)]
Mean squared error is of 2293604161.101068
Mean absolute error:34655.57556420757
MAPE:0.3358579842835321
R2 score:0.6668717583622207
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:36:37
epoch 0  | loss: 0.5399  | val_0_rmse: 0.59809 | val_1_rmse: 0.60414 |  0:00:02s
epoch 1  | loss: 0.35559 | val_0_rmse: 0.5786  | val_1_rmse: 0.5893  |  0:00:04s
epoch 2  | loss: 0.34333 | val_0_rmse: 0.57972 | val_1_rmse: 0.58596 |  0:00:06s
epoch 3  | loss: 0.33929 | val_0_rmse: 0.5669  | val_1_rmse: 0.57647 |  0:00:08s
epoch 4  | loss: 0.33132 | val_0_rmse: 0.55852 | val_1_rmse: 0.56732 |  0:00:10s
epoch 5  | loss: 0.32265 | val_0_rmse: 0.576   | val_1_rmse: 0.5802  |  0:00:12s
epoch 6  | loss: 0.31943 | val_0_rmse: 0.5435  | val_1_rmse: 0.55521 |  0:00:14s
epoch 7  | loss: 0.30979 | val_0_rmse: 0.59047 | val_1_rmse: 0.60203 |  0:00:15s
epoch 8  | loss: 0.30825 | val_0_rmse: 0.54312 | val_1_rmse: 0.55257 |  0:00:17s
epoch 9  | loss: 0.28713 | val_0_rmse: 0.56784 | val_1_rmse: 0.57464 |  0:00:19s
epoch 10 | loss: 0.28944 | val_0_rmse: 0.53022 | val_1_rmse: 0.53911 |  0:00:21s
epoch 11 | loss: 0.28755 | val_0_rmse: 0.52478 | val_1_rmse: 0.53737 |  0:00:23s
epoch 12 | loss: 0.2823  | val_0_rmse: 0.54731 | val_1_rmse: 0.55295 |  0:00:25s
epoch 13 | loss: 0.27897 | val_0_rmse: 0.51955 | val_1_rmse: 0.52679 |  0:00:27s
epoch 14 | loss: 0.27553 | val_0_rmse: 0.50747 | val_1_rmse: 0.5176  |  0:00:29s
epoch 15 | loss: 0.2699  | val_0_rmse: 0.52246 | val_1_rmse: 0.5323  |  0:00:31s
epoch 16 | loss: 0.27656 | val_0_rmse: 0.5216  | val_1_rmse: 0.53534 |  0:00:33s
epoch 17 | loss: 0.27544 | val_0_rmse: 0.51129 | val_1_rmse: 0.51914 |  0:00:35s
epoch 18 | loss: 0.2679  | val_0_rmse: 0.52743 | val_1_rmse: 0.53276 |  0:00:37s
epoch 19 | loss: 0.26738 | val_0_rmse: 0.51323 | val_1_rmse: 0.52293 |  0:00:39s
epoch 20 | loss: 0.26358 | val_0_rmse: 0.50007 | val_1_rmse: 0.50809 |  0:00:41s
epoch 21 | loss: 0.27037 | val_0_rmse: 0.52909 | val_1_rmse: 0.53637 |  0:00:43s
epoch 22 | loss: 0.26352 | val_0_rmse: 0.53495 | val_1_rmse: 0.54    |  0:00:45s
epoch 23 | loss: 0.26071 | val_0_rmse: 0.49923 | val_1_rmse: 0.50858 |  0:00:47s
epoch 24 | loss: 0.25677 | val_0_rmse: 0.54138 | val_1_rmse: 0.54669 |  0:00:49s
epoch 25 | loss: 0.25683 | val_0_rmse: 0.50313 | val_1_rmse: 0.51475 |  0:00:51s
epoch 26 | loss: 0.26068 | val_0_rmse: 0.53228 | val_1_rmse: 0.54185 |  0:00:53s
epoch 27 | loss: 0.25984 | val_0_rmse: 0.52077 | val_1_rmse: 0.53322 |  0:00:55s
epoch 28 | loss: 0.26102 | val_0_rmse: 0.60572 | val_1_rmse: 0.60681 |  0:00:57s
epoch 29 | loss: 0.25739 | val_0_rmse: 0.56734 | val_1_rmse: 0.57141 |  0:00:59s
epoch 30 | loss: 0.25437 | val_0_rmse: 0.59905 | val_1_rmse: 0.59977 |  0:01:01s
epoch 31 | loss: 0.25329 | val_0_rmse: 0.52291 | val_1_rmse: 0.53437 |  0:01:03s
epoch 32 | loss: 0.25397 | val_0_rmse: 0.51316 | val_1_rmse: 0.52406 |  0:01:05s
epoch 33 | loss: 0.25448 | val_0_rmse: 0.50644 | val_1_rmse: 0.5137  |  0:01:07s
epoch 34 | loss: 0.25804 | val_0_rmse: 0.50458 | val_1_rmse: 0.51116 |  0:01:09s
epoch 35 | loss: 0.25476 | val_0_rmse: 0.50541 | val_1_rmse: 0.51544 |  0:01:11s
epoch 36 | loss: 0.2576  | val_0_rmse: 0.673   | val_1_rmse: 0.67335 |  0:01:13s
epoch 37 | loss: 0.25533 | val_0_rmse: 0.50133 | val_1_rmse: 0.5102  |  0:01:15s
epoch 38 | loss: 0.25553 | val_0_rmse: 0.55074 | val_1_rmse: 0.5622  |  0:01:17s
epoch 39 | loss: 0.25228 | val_0_rmse: 0.5019  | val_1_rmse: 0.51249 |  0:01:19s
epoch 40 | loss: 0.24997 | val_0_rmse: 0.56959 | val_1_rmse: 0.57162 |  0:01:21s
epoch 41 | loss: 0.25031 | val_0_rmse: 0.53654 | val_1_rmse: 0.54436 |  0:01:23s
epoch 42 | loss: 0.2498  | val_0_rmse: 0.4901  | val_1_rmse: 0.5002  |  0:01:25s
epoch 43 | loss: 0.25016 | val_0_rmse: 0.52829 | val_1_rmse: 0.5375  |  0:01:27s
epoch 44 | loss: 0.24539 | val_0_rmse: 0.55665 | val_1_rmse: 0.56427 |  0:01:29s
epoch 45 | loss: 0.24906 | val_0_rmse: 0.51234 | val_1_rmse: 0.52182 |  0:01:31s
epoch 46 | loss: 0.24736 | val_0_rmse: 0.50974 | val_1_rmse: 0.51694 |  0:01:33s
epoch 47 | loss: 0.24536 | val_0_rmse: 0.49076 | val_1_rmse: 0.50151 |  0:01:35s
epoch 48 | loss: 0.2456  | val_0_rmse: 0.54379 | val_1_rmse: 0.55052 |  0:01:37s
epoch 49 | loss: 0.24603 | val_0_rmse: 0.49815 | val_1_rmse: 0.50599 |  0:01:39s
epoch 50 | loss: 0.24703 | val_0_rmse: 0.51263 | val_1_rmse: 0.52273 |  0:01:41s
epoch 51 | loss: 0.24534 | val_0_rmse: 0.52497 | val_1_rmse: 0.52994 |  0:01:43s
epoch 52 | loss: 0.24759 | val_0_rmse: 0.51125 | val_1_rmse: 0.52058 |  0:01:45s
epoch 53 | loss: 0.24596 | val_0_rmse: 0.49118 | val_1_rmse: 0.5041  |  0:01:47s
epoch 54 | loss: 0.24564 | val_0_rmse: 0.51204 | val_1_rmse: 0.51948 |  0:01:49s
epoch 55 | loss: 0.24532 | val_0_rmse: 0.48862 | val_1_rmse: 0.50223 |  0:01:51s
epoch 56 | loss: 0.24133 | val_0_rmse: 0.5164  | val_1_rmse: 0.52564 |  0:01:53s
epoch 57 | loss: 0.24445 | val_0_rmse: 0.58979 | val_1_rmse: 0.5922  |  0:01:55s
epoch 58 | loss: 0.24575 | val_0_rmse: 0.50335 | val_1_rmse: 0.51418 |  0:01:57s
epoch 59 | loss: 0.24231 | val_0_rmse: 0.50122 | val_1_rmse: 0.51544 |  0:01:59s
epoch 60 | loss: 0.24758 | val_0_rmse: 0.48769 | val_1_rmse: 0.4995  |  0:02:01s
epoch 61 | loss: 0.24227 | val_0_rmse: 0.54636 | val_1_rmse: 0.55369 |  0:02:03s
epoch 62 | loss: 0.24646 | val_0_rmse: 0.53089 | val_1_rmse: 0.5413  |  0:02:05s
epoch 63 | loss: 0.24082 | val_0_rmse: 0.57819 | val_1_rmse: 0.58354 |  0:02:07s
epoch 64 | loss: 0.23992 | val_0_rmse: 0.49777 | val_1_rmse: 0.51015 |  0:02:09s
epoch 65 | loss: 0.23997 | val_0_rmse: 0.49493 | val_1_rmse: 0.50646 |  0:02:11s
epoch 66 | loss: 0.23867 | val_0_rmse: 0.48749 | val_1_rmse: 0.50207 |  0:02:13s
epoch 67 | loss: 0.24193 | val_0_rmse: 0.52461 | val_1_rmse: 0.53324 |  0:02:15s
epoch 68 | loss: 0.24752 | val_0_rmse: 0.59206 | val_1_rmse: 0.595   |  0:02:17s
epoch 69 | loss: 0.24278 | val_0_rmse: 0.48437 | val_1_rmse: 0.49353 |  0:02:19s
epoch 70 | loss: 0.24177 | val_0_rmse: 0.56683 | val_1_rmse: 0.57173 |  0:02:21s
epoch 71 | loss: 0.25004 | val_0_rmse: 0.49205 | val_1_rmse: 0.50146 |  0:02:23s
epoch 72 | loss: 0.24003 | val_0_rmse: 0.55829 | val_1_rmse: 0.56658 |  0:02:25s
epoch 73 | loss: 0.24092 | val_0_rmse: 0.58915 | val_1_rmse: 0.58774 |  0:02:27s
epoch 74 | loss: 0.23975 | val_0_rmse: 0.50215 | val_1_rmse: 0.51674 |  0:02:29s
epoch 75 | loss: 0.2454  | val_0_rmse: 0.55317 | val_1_rmse: 0.55967 |  0:02:31s
epoch 76 | loss: 0.24014 | val_0_rmse: 0.48334 | val_1_rmse: 0.49399 |  0:02:33s
epoch 77 | loss: 0.23979 | val_0_rmse: 0.52777 | val_1_rmse: 0.53408 |  0:02:35s
epoch 78 | loss: 0.24089 | val_0_rmse: 0.4898  | val_1_rmse: 0.50456 |  0:02:37s
epoch 79 | loss: 0.24171 | val_0_rmse: 0.50575 | val_1_rmse: 0.51592 |  0:02:39s
epoch 80 | loss: 0.24208 | val_0_rmse: 0.50772 | val_1_rmse: 0.51831 |  0:02:41s
epoch 81 | loss: 0.24422 | val_0_rmse: 0.65554 | val_1_rmse: 0.65138 |  0:02:43s
epoch 82 | loss: 0.24586 | val_0_rmse: 0.53962 | val_1_rmse: 0.55177 |  0:02:45s
epoch 83 | loss: 0.24801 | val_0_rmse: 0.50638 | val_1_rmse: 0.5168  |  0:02:47s
epoch 84 | loss: 0.25336 | val_0_rmse: 0.68259 | val_1_rmse: 0.69014 |  0:02:49s
epoch 85 | loss: 0.24137 | val_0_rmse: 0.64158 | val_1_rmse: 0.63712 |  0:02:51s
epoch 86 | loss: 0.24227 | val_0_rmse: 0.50466 | val_1_rmse: 0.51579 |  0:02:53s
epoch 87 | loss: 0.2372  | val_0_rmse: 0.48476 | val_1_rmse: 0.49917 |  0:02:55s
epoch 88 | loss: 0.2387  | val_0_rmse: 0.51193 | val_1_rmse: 0.51738 |  0:02:57s
epoch 89 | loss: 0.23769 | val_0_rmse: 0.51923 | val_1_rmse: 0.52859 |  0:02:59s
epoch 90 | loss: 0.24256 | val_0_rmse: 0.50868 | val_1_rmse: 0.5197  |  0:03:01s
epoch 91 | loss: 0.24262 | val_0_rmse: 0.49899 | val_1_rmse: 0.51053 |  0:03:03s
epoch 92 | loss: 0.23805 | val_0_rmse: 0.57392 | val_1_rmse: 0.58118 |  0:03:05s
epoch 93 | loss: 0.24068 | val_0_rmse: 0.5157  | val_1_rmse: 0.53025 |  0:03:07s
epoch 94 | loss: 0.2369  | val_0_rmse: 0.59273 | val_1_rmse: 0.59614 |  0:03:09s
epoch 95 | loss: 0.24096 | val_0_rmse: 0.56662 | val_1_rmse: 0.57884 |  0:03:11s
epoch 96 | loss: 0.23751 | val_0_rmse: 0.58243 | val_1_rmse: 0.59348 |  0:03:13s
epoch 97 | loss: 0.23676 | val_0_rmse: 0.49448 | val_1_rmse: 0.50628 |  0:03:15s
epoch 98 | loss: 0.23825 | val_0_rmse: 0.49867 | val_1_rmse: 0.51178 |  0:03:17s
epoch 99 | loss: 0.24338 | val_0_rmse: 0.62618 | val_1_rmse: 0.6335  |  0:03:19s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.49353
Best weights from best epoch are automatically used!
ended training at: 06:39:57
Feature importance:
[('Area', 0.30429416854724023), ('Baths', 0.15253157932691014), ('Beds', 0.04047748120904993), ('Latitude', 0.287360751682962), ('Longitude', 0.1890375023590643), ('Month', 5.146160266278895e-07), ('Year', 0.026298002258746768)]
Mean squared error is of 943472335.3119688
Mean absolute error:21276.066783617178
MAPE:0.2681442389267432
R2 score:0.7607435426196945
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:39:57
epoch 0  | loss: 0.57769 | val_0_rmse: 0.64896 | val_1_rmse: 0.65582 |  0:00:01s
epoch 1  | loss: 0.35499 | val_0_rmse: 0.56921 | val_1_rmse: 0.56858 |  0:00:03s
epoch 2  | loss: 0.32299 | val_0_rmse: 0.54897 | val_1_rmse: 0.54646 |  0:00:05s
epoch 3  | loss: 0.32636 | val_0_rmse: 0.56413 | val_1_rmse: 0.56211 |  0:00:07s
epoch 4  | loss: 0.32042 | val_0_rmse: 0.55278 | val_1_rmse: 0.54987 |  0:00:09s
epoch 5  | loss: 0.32379 | val_0_rmse: 0.55328 | val_1_rmse: 0.55319 |  0:00:11s
epoch 6  | loss: 0.31355 | val_0_rmse: 0.55744 | val_1_rmse: 0.55698 |  0:00:13s
epoch 7  | loss: 0.3106  | val_0_rmse: 0.55989 | val_1_rmse: 0.5559  |  0:00:15s
epoch 8  | loss: 0.29898 | val_0_rmse: 0.60488 | val_1_rmse: 0.60077 |  0:00:17s
epoch 9  | loss: 0.28687 | val_0_rmse: 0.58383 | val_1_rmse: 0.58389 |  0:00:19s
epoch 10 | loss: 0.27955 | val_0_rmse: 0.51859 | val_1_rmse: 0.51622 |  0:00:21s
epoch 11 | loss: 0.2835  | val_0_rmse: 0.53906 | val_1_rmse: 0.53455 |  0:00:23s
epoch 12 | loss: 0.28584 | val_0_rmse: 0.57141 | val_1_rmse: 0.57441 |  0:00:25s
epoch 13 | loss: 0.27893 | val_0_rmse: 0.53357 | val_1_rmse: 0.53455 |  0:00:27s
epoch 14 | loss: 0.27607 | val_0_rmse: 0.51375 | val_1_rmse: 0.51255 |  0:00:29s
epoch 15 | loss: 0.27673 | val_0_rmse: 0.59553 | val_1_rmse: 0.59988 |  0:00:31s
epoch 16 | loss: 0.27394 | val_0_rmse: 0.50907 | val_1_rmse: 0.50927 |  0:00:33s
epoch 17 | loss: 0.27478 | val_0_rmse: 0.58208 | val_1_rmse: 0.58478 |  0:00:35s
epoch 18 | loss: 0.27372 | val_0_rmse: 0.52232 | val_1_rmse: 0.5231  |  0:00:37s
epoch 19 | loss: 0.27208 | val_0_rmse: 0.57493 | val_1_rmse: 0.57366 |  0:00:39s
epoch 20 | loss: 0.27186 | val_0_rmse: 0.51833 | val_1_rmse: 0.51978 |  0:00:41s
epoch 21 | loss: 0.27154 | val_0_rmse: 0.52274 | val_1_rmse: 0.52059 |  0:00:43s
epoch 22 | loss: 0.27592 | val_0_rmse: 0.552   | val_1_rmse: 0.55281 |  0:00:45s
epoch 23 | loss: 0.27106 | val_0_rmse: 0.61976 | val_1_rmse: 0.61699 |  0:00:47s
epoch 24 | loss: 0.26334 | val_0_rmse: 0.51643 | val_1_rmse: 0.5182  |  0:00:49s
epoch 25 | loss: 0.26607 | val_0_rmse: 0.62216 | val_1_rmse: 0.62057 |  0:00:51s
epoch 26 | loss: 0.26477 | val_0_rmse: 0.53455 | val_1_rmse: 0.53614 |  0:00:53s
epoch 27 | loss: 0.26217 | val_0_rmse: 0.50364 | val_1_rmse: 0.50477 |  0:00:55s
epoch 28 | loss: 0.26905 | val_0_rmse: 0.50988 | val_1_rmse: 0.50985 |  0:00:57s
epoch 29 | loss: 0.26921 | val_0_rmse: 0.51133 | val_1_rmse: 0.51367 |  0:00:59s
epoch 30 | loss: 0.26524 | val_0_rmse: 0.52172 | val_1_rmse: 0.52234 |  0:01:01s
epoch 31 | loss: 0.26212 | val_0_rmse: 0.51454 | val_1_rmse: 0.51813 |  0:01:03s
epoch 32 | loss: 0.26241 | val_0_rmse: 0.52783 | val_1_rmse: 0.53042 |  0:01:05s
epoch 33 | loss: 0.25827 | val_0_rmse: 0.57691 | val_1_rmse: 0.58066 |  0:01:07s
epoch 34 | loss: 0.26276 | val_0_rmse: 0.50831 | val_1_rmse: 0.51052 |  0:01:09s
epoch 35 | loss: 0.26161 | val_0_rmse: 0.5441  | val_1_rmse: 0.54757 |  0:01:11s
epoch 36 | loss: 0.2612  | val_0_rmse: 0.50543 | val_1_rmse: 0.50591 |  0:01:13s
epoch 37 | loss: 0.26245 | val_0_rmse: 0.58928 | val_1_rmse: 0.58924 |  0:01:15s
epoch 38 | loss: 0.25532 | val_0_rmse: 0.53616 | val_1_rmse: 0.53931 |  0:01:17s
epoch 39 | loss: 0.25891 | val_0_rmse: 0.57196 | val_1_rmse: 0.56941 |  0:01:19s
epoch 40 | loss: 0.26242 | val_0_rmse: 0.5216  | val_1_rmse: 0.5236  |  0:01:21s
epoch 41 | loss: 0.25864 | val_0_rmse: 0.5013  | val_1_rmse: 0.5011  |  0:01:23s
epoch 42 | loss: 0.25906 | val_0_rmse: 0.50647 | val_1_rmse: 0.50941 |  0:01:25s
epoch 43 | loss: 0.26339 | val_0_rmse: 0.64182 | val_1_rmse: 0.63804 |  0:01:27s
epoch 44 | loss: 0.26129 | val_0_rmse: 0.50276 | val_1_rmse: 0.50328 |  0:01:29s
epoch 45 | loss: 0.25758 | val_0_rmse: 0.56665 | val_1_rmse: 0.5664  |  0:01:31s
epoch 46 | loss: 0.25972 | val_0_rmse: 0.53458 | val_1_rmse: 0.53712 |  0:01:33s
epoch 47 | loss: 0.25679 | val_0_rmse: 0.5076  | val_1_rmse: 0.50636 |  0:01:35s
epoch 48 | loss: 0.25447 | val_0_rmse: 0.59471 | val_1_rmse: 0.59877 |  0:01:36s
epoch 49 | loss: 0.25352 | val_0_rmse: 0.58061 | val_1_rmse: 0.5847  |  0:01:38s
epoch 50 | loss: 0.25279 | val_0_rmse: 0.55481 | val_1_rmse: 0.55824 |  0:01:40s
epoch 51 | loss: 0.25407 | val_0_rmse: 0.5044  | val_1_rmse: 0.50609 |  0:01:42s
epoch 52 | loss: 0.25964 | val_0_rmse: 0.58204 | val_1_rmse: 0.58586 |  0:01:44s
epoch 53 | loss: 0.26663 | val_0_rmse: 0.60927 | val_1_rmse: 0.61307 |  0:01:46s
epoch 54 | loss: 0.26037 | val_0_rmse: 0.51317 | val_1_rmse: 0.51181 |  0:01:48s
epoch 55 | loss: 0.25674 | val_0_rmse: 0.5098  | val_1_rmse: 0.51025 |  0:01:50s
epoch 56 | loss: 0.25757 | val_0_rmse: 0.52089 | val_1_rmse: 0.52368 |  0:01:52s
epoch 57 | loss: 0.25003 | val_0_rmse: 0.52342 | val_1_rmse: 0.52467 |  0:01:54s
epoch 58 | loss: 0.25097 | val_0_rmse: 0.49525 | val_1_rmse: 0.49479 |  0:01:56s
epoch 59 | loss: 0.24899 | val_0_rmse: 0.51084 | val_1_rmse: 0.51045 |  0:01:58s
epoch 60 | loss: 0.24947 | val_0_rmse: 0.50925 | val_1_rmse: 0.5134  |  0:02:00s
epoch 61 | loss: 0.25188 | val_0_rmse: 0.50988 | val_1_rmse: 0.50921 |  0:02:02s
epoch 62 | loss: 0.25061 | val_0_rmse: 0.48974 | val_1_rmse: 0.48818 |  0:02:04s
epoch 63 | loss: 0.24933 | val_0_rmse: 0.5908  | val_1_rmse: 0.59593 |  0:02:06s
epoch 64 | loss: 0.25136 | val_0_rmse: 0.49556 | val_1_rmse: 0.49639 |  0:02:08s
epoch 65 | loss: 0.25028 | val_0_rmse: 0.60935 | val_1_rmse: 0.6158  |  0:02:10s
epoch 66 | loss: 0.24661 | val_0_rmse: 0.5355  | val_1_rmse: 0.53546 |  0:02:12s
epoch 67 | loss: 0.25509 | val_0_rmse: 0.49256 | val_1_rmse: 0.49165 |  0:02:14s
epoch 68 | loss: 0.24911 | val_0_rmse: 0.50286 | val_1_rmse: 0.50591 |  0:02:16s
epoch 69 | loss: 0.24578 | val_0_rmse: 0.51474 | val_1_rmse: 0.517   |  0:02:18s
epoch 70 | loss: 0.24601 | val_0_rmse: 0.50654 | val_1_rmse: 0.50668 |  0:02:20s
epoch 71 | loss: 0.24838 | val_0_rmse: 0.66489 | val_1_rmse: 0.6707  |  0:02:22s
epoch 72 | loss: 0.25657 | val_0_rmse: 0.48457 | val_1_rmse: 0.484   |  0:02:24s
epoch 73 | loss: 0.24707 | val_0_rmse: 0.52694 | val_1_rmse: 0.52791 |  0:02:26s
epoch 74 | loss: 0.24502 | val_0_rmse: 0.51452 | val_1_rmse: 0.51359 |  0:02:28s
epoch 75 | loss: 0.25258 | val_0_rmse: 0.65162 | val_1_rmse: 0.65526 |  0:02:30s
epoch 76 | loss: 0.25092 | val_0_rmse: 0.49438 | val_1_rmse: 0.49215 |  0:02:32s
epoch 77 | loss: 0.24891 | val_0_rmse: 0.50366 | val_1_rmse: 0.50472 |  0:02:34s
epoch 78 | loss: 0.24663 | val_0_rmse: 0.6     | val_1_rmse: 0.59574 |  0:02:36s
epoch 79 | loss: 0.24667 | val_0_rmse: 0.602   | val_1_rmse: 0.6077  |  0:02:38s
epoch 80 | loss: 0.24493 | val_0_rmse: 0.53762 | val_1_rmse: 0.54074 |  0:02:40s
epoch 81 | loss: 0.24719 | val_0_rmse: 0.53843 | val_1_rmse: 0.54123 |  0:02:42s
epoch 82 | loss: 0.24427 | val_0_rmse: 0.50741 | val_1_rmse: 0.50847 |  0:02:44s
epoch 83 | loss: 0.24199 | val_0_rmse: 0.49422 | val_1_rmse: 0.49427 |  0:02:46s
epoch 84 | loss: 0.24262 | val_0_rmse: 0.49507 | val_1_rmse: 0.49564 |  0:02:48s
epoch 85 | loss: 0.24667 | val_0_rmse: 0.54359 | val_1_rmse: 0.54518 |  0:02:50s
epoch 86 | loss: 0.24903 | val_0_rmse: 0.60798 | val_1_rmse: 0.6133  |  0:02:52s
epoch 87 | loss: 0.24265 | val_0_rmse: 0.51304 | val_1_rmse: 0.5154  |  0:02:54s
epoch 88 | loss: 0.24514 | val_0_rmse: 0.57051 | val_1_rmse: 0.57448 |  0:02:56s
epoch 89 | loss: 0.24259 | val_0_rmse: 0.54431 | val_1_rmse: 0.54135 |  0:02:58s
epoch 90 | loss: 0.24386 | val_0_rmse: 0.60271 | val_1_rmse: 0.6085  |  0:03:00s
epoch 91 | loss: 0.2448  | val_0_rmse: 0.61898 | val_1_rmse: 0.62529 |  0:03:02s
epoch 92 | loss: 0.24502 | val_0_rmse: 0.6486  | val_1_rmse: 0.65592 |  0:03:04s
epoch 93 | loss: 0.24004 | val_0_rmse: 0.48968 | val_1_rmse: 0.49254 |  0:03:06s
epoch 94 | loss: 0.23934 | val_0_rmse: 0.50106 | val_1_rmse: 0.50127 |  0:03:08s
epoch 95 | loss: 0.23959 | val_0_rmse: 0.52948 | val_1_rmse: 0.52695 |  0:03:10s
epoch 96 | loss: 0.24291 | val_0_rmse: 0.54266 | val_1_rmse: 0.54228 |  0:03:12s
epoch 97 | loss: 0.24591 | val_0_rmse: 0.51241 | val_1_rmse: 0.51352 |  0:03:14s
epoch 98 | loss: 0.24933 | val_0_rmse: 0.54021 | val_1_rmse: 0.53906 |  0:03:16s
epoch 99 | loss: 0.26116 | val_0_rmse: 0.56898 | val_1_rmse: 0.57031 |  0:03:18s
epoch 100| loss: 0.24771 | val_0_rmse: 0.49211 | val_1_rmse: 0.49348 |  0:03:20s
epoch 101| loss: 0.24502 | val_0_rmse: 0.48924 | val_1_rmse: 0.48904 |  0:03:22s
epoch 102| loss: 0.24327 | val_0_rmse: 0.53441 | val_1_rmse: 0.53032 |  0:03:23s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.484
Best weights from best epoch are automatically used!
ended training at: 06:43:22
Feature importance:
[('Area', 0.44386308705882194), ('Baths', 0.09012442117956156), ('Beds', 0.15913644222350434), ('Latitude', 0.10152603531871308), ('Longitude', 0.2027027286324964), ('Month', 0.002647285586902684), ('Year', 0.0)]
Mean squared error is of 923221071.7095433
Mean absolute error:20425.657951065707
MAPE:0.2444999275981825
R2 score:0.770479472619866
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:43:22
epoch 0  | loss: 0.55048 | val_0_rmse: 0.6156  | val_1_rmse: 0.62819 |  0:00:01s
epoch 1  | loss: 0.33998 | val_0_rmse: 0.5614  | val_1_rmse: 0.57019 |  0:00:03s
epoch 2  | loss: 0.31941 | val_0_rmse: 0.55318 | val_1_rmse: 0.56023 |  0:00:05s
epoch 3  | loss: 0.31296 | val_0_rmse: 0.57559 | val_1_rmse: 0.58182 |  0:00:07s
epoch 4  | loss: 0.31227 | val_0_rmse: 0.54654 | val_1_rmse: 0.5533  |  0:00:09s
epoch 5  | loss: 0.30729 | val_0_rmse: 0.54594 | val_1_rmse: 0.55266 |  0:00:11s
epoch 6  | loss: 0.30288 | val_0_rmse: 0.5422  | val_1_rmse: 0.54886 |  0:00:13s
epoch 7  | loss: 0.31072 | val_0_rmse: 0.54145 | val_1_rmse: 0.54894 |  0:00:15s
epoch 8  | loss: 0.30206 | val_0_rmse: 0.53951 | val_1_rmse: 0.54799 |  0:00:17s
epoch 9  | loss: 0.29861 | val_0_rmse: 0.54238 | val_1_rmse: 0.54882 |  0:00:19s
epoch 10 | loss: 0.29517 | val_0_rmse: 0.54188 | val_1_rmse: 0.54932 |  0:00:21s
epoch 11 | loss: 0.29407 | val_0_rmse: 0.54282 | val_1_rmse: 0.5516  |  0:00:23s
epoch 12 | loss: 0.29661 | val_0_rmse: 0.53869 | val_1_rmse: 0.54742 |  0:00:25s
epoch 13 | loss: 0.29847 | val_0_rmse: 0.60856 | val_1_rmse: 0.61092 |  0:00:27s
epoch 14 | loss: 0.29672 | val_0_rmse: 0.53133 | val_1_rmse: 0.53835 |  0:00:29s
epoch 15 | loss: 0.28797 | val_0_rmse: 0.52699 | val_1_rmse: 0.5346  |  0:00:31s
epoch 16 | loss: 0.29103 | val_0_rmse: 0.54004 | val_1_rmse: 0.54694 |  0:00:33s
epoch 17 | loss: 0.2834  | val_0_rmse: 0.52284 | val_1_rmse: 0.53105 |  0:00:35s
epoch 18 | loss: 0.28158 | val_0_rmse: 0.51959 | val_1_rmse: 0.52963 |  0:00:37s
epoch 19 | loss: 0.28009 | val_0_rmse: 0.52842 | val_1_rmse: 0.53707 |  0:00:39s
epoch 20 | loss: 0.27743 | val_0_rmse: 0.51881 | val_1_rmse: 0.52659 |  0:00:41s
epoch 21 | loss: 0.27751 | val_0_rmse: 0.51691 | val_1_rmse: 0.52574 |  0:00:43s
epoch 22 | loss: 0.27639 | val_0_rmse: 0.52345 | val_1_rmse: 0.53312 |  0:00:45s
epoch 23 | loss: 0.27672 | val_0_rmse: 0.52135 | val_1_rmse: 0.52969 |  0:00:47s
epoch 24 | loss: 0.27967 | val_0_rmse: 0.5247  | val_1_rmse: 0.53173 |  0:00:49s
epoch 25 | loss: 0.27563 | val_0_rmse: 0.54039 | val_1_rmse: 0.54865 |  0:00:51s
epoch 26 | loss: 0.27913 | val_0_rmse: 0.51551 | val_1_rmse: 0.52621 |  0:00:53s
epoch 27 | loss: 0.27016 | val_0_rmse: 0.51359 | val_1_rmse: 0.52469 |  0:00:55s
epoch 28 | loss: 0.271   | val_0_rmse: 0.55583 | val_1_rmse: 0.56281 |  0:00:57s
epoch 29 | loss: 0.27427 | val_0_rmse: 0.60948 | val_1_rmse: 0.62284 |  0:00:59s
epoch 30 | loss: 0.27612 | val_0_rmse: 0.5344  | val_1_rmse: 0.54584 |  0:01:01s
epoch 31 | loss: 0.2693  | val_0_rmse: 0.51157 | val_1_rmse: 0.5211  |  0:01:03s
epoch 32 | loss: 0.26157 | val_0_rmse: 0.51873 | val_1_rmse: 0.52578 |  0:01:05s
epoch 33 | loss: 0.2666  | val_0_rmse: 0.50583 | val_1_rmse: 0.51483 |  0:01:07s
epoch 34 | loss: 0.26589 | val_0_rmse: 0.53478 | val_1_rmse: 0.5455  |  0:01:09s
epoch 35 | loss: 0.26019 | val_0_rmse: 0.54625 | val_1_rmse: 0.55384 |  0:01:11s
epoch 36 | loss: 0.25476 | val_0_rmse: 0.51608 | val_1_rmse: 0.52459 |  0:01:13s
epoch 37 | loss: 0.25528 | val_0_rmse: 0.54926 | val_1_rmse: 0.55639 |  0:01:15s
epoch 38 | loss: 0.25737 | val_0_rmse: 0.51041 | val_1_rmse: 0.52127 |  0:01:17s
epoch 39 | loss: 0.25245 | val_0_rmse: 0.52521 | val_1_rmse: 0.53479 |  0:01:19s
epoch 40 | loss: 0.25169 | val_0_rmse: 0.49722 | val_1_rmse: 0.50709 |  0:01:21s
epoch 41 | loss: 0.25234 | val_0_rmse: 0.48831 | val_1_rmse: 0.50068 |  0:01:23s
epoch 42 | loss: 0.25083 | val_0_rmse: 0.52612 | val_1_rmse: 0.5347  |  0:01:25s
epoch 43 | loss: 0.25546 | val_0_rmse: 0.49945 | val_1_rmse: 0.51108 |  0:01:27s
epoch 44 | loss: 0.25303 | val_0_rmse: 0.54889 | val_1_rmse: 0.55757 |  0:01:29s
epoch 45 | loss: 0.2526  | val_0_rmse: 0.50397 | val_1_rmse: 0.51806 |  0:01:31s
epoch 46 | loss: 0.24762 | val_0_rmse: 0.51796 | val_1_rmse: 0.52998 |  0:01:33s
epoch 47 | loss: 0.24794 | val_0_rmse: 0.50384 | val_1_rmse: 0.51556 |  0:01:35s
epoch 48 | loss: 0.25048 | val_0_rmse: 0.52769 | val_1_rmse: 0.53805 |  0:01:37s
epoch 49 | loss: 0.25359 | val_0_rmse: 0.51767 | val_1_rmse: 0.5315  |  0:01:39s
epoch 50 | loss: 0.25184 | val_0_rmse: 0.52215 | val_1_rmse: 0.53247 |  0:01:41s
epoch 51 | loss: 0.24575 | val_0_rmse: 0.58753 | val_1_rmse: 0.59532 |  0:01:43s
epoch 52 | loss: 0.24638 | val_0_rmse: 0.52047 | val_1_rmse: 0.5333  |  0:01:45s
epoch 53 | loss: 0.2504  | val_0_rmse: 0.52612 | val_1_rmse: 0.53647 |  0:01:47s
epoch 54 | loss: 0.25062 | val_0_rmse: 0.52797 | val_1_rmse: 0.53863 |  0:01:49s
epoch 55 | loss: 0.24469 | val_0_rmse: 0.4833  | val_1_rmse: 0.4954  |  0:01:51s
epoch 56 | loss: 0.24628 | val_0_rmse: 0.53122 | val_1_rmse: 0.5411  |  0:01:53s
epoch 57 | loss: 0.2446  | val_0_rmse: 0.55005 | val_1_rmse: 0.56041 |  0:01:55s
epoch 58 | loss: 0.24612 | val_0_rmse: 0.52661 | val_1_rmse: 0.5367  |  0:01:57s
epoch 59 | loss: 0.24929 | val_0_rmse: 0.52052 | val_1_rmse: 0.52848 |  0:01:59s
epoch 60 | loss: 0.24515 | val_0_rmse: 0.49796 | val_1_rmse: 0.50989 |  0:02:00s
epoch 61 | loss: 0.24539 | val_0_rmse: 0.51698 | val_1_rmse: 0.52857 |  0:02:02s
epoch 62 | loss: 0.24327 | val_0_rmse: 0.50392 | val_1_rmse: 0.51433 |  0:02:04s
epoch 63 | loss: 0.24121 | val_0_rmse: 0.48417 | val_1_rmse: 0.49631 |  0:02:06s
epoch 64 | loss: 0.24827 | val_0_rmse: 0.48167 | val_1_rmse: 0.49373 |  0:02:08s
epoch 65 | loss: 0.24419 | val_0_rmse: 0.56457 | val_1_rmse: 0.57486 |  0:02:10s
epoch 66 | loss: 0.24193 | val_0_rmse: 0.48181 | val_1_rmse: 0.49373 |  0:02:12s
epoch 67 | loss: 0.24566 | val_0_rmse: 0.52994 | val_1_rmse: 0.53767 |  0:02:14s
epoch 68 | loss: 0.24403 | val_0_rmse: 0.48416 | val_1_rmse: 0.49674 |  0:02:16s
epoch 69 | loss: 0.24147 | val_0_rmse: 0.49199 | val_1_rmse: 0.50876 |  0:02:18s
epoch 70 | loss: 0.24266 | val_0_rmse: 0.48449 | val_1_rmse: 0.49889 |  0:02:20s
epoch 71 | loss: 0.24285 | val_0_rmse: 0.49272 | val_1_rmse: 0.50904 |  0:02:22s
epoch 72 | loss: 0.24212 | val_0_rmse: 0.49598 | val_1_rmse: 0.51084 |  0:02:24s
epoch 73 | loss: 0.23943 | val_0_rmse: 0.49166 | val_1_rmse: 0.50425 |  0:02:26s
epoch 74 | loss: 0.24569 | val_0_rmse: 0.5432  | val_1_rmse: 0.55291 |  0:02:28s
epoch 75 | loss: 0.2418  | val_0_rmse: 0.51139 | val_1_rmse: 0.52151 |  0:02:30s
epoch 76 | loss: 0.24098 | val_0_rmse: 0.50973 | val_1_rmse: 0.52197 |  0:02:32s
epoch 77 | loss: 0.24485 | val_0_rmse: 0.50425 | val_1_rmse: 0.51765 |  0:02:34s
epoch 78 | loss: 0.24749 | val_0_rmse: 0.50896 | val_1_rmse: 0.52126 |  0:02:36s
epoch 79 | loss: 0.2428  | val_0_rmse: 0.5652  | val_1_rmse: 0.57299 |  0:02:38s
epoch 80 | loss: 0.23907 | val_0_rmse: 0.50659 | val_1_rmse: 0.51923 |  0:02:40s
epoch 81 | loss: 0.25244 | val_0_rmse: 0.53508 | val_1_rmse: 0.54414 |  0:02:42s
epoch 82 | loss: 0.24709 | val_0_rmse: 0.48196 | val_1_rmse: 0.49624 |  0:02:44s
epoch 83 | loss: 0.24462 | val_0_rmse: 0.49516 | val_1_rmse: 0.50755 |  0:02:46s
epoch 84 | loss: 0.24145 | val_0_rmse: 0.47926 | val_1_rmse: 0.49356 |  0:02:48s
epoch 85 | loss: 0.24218 | val_0_rmse: 0.49511 | val_1_rmse: 0.5115  |  0:02:50s
epoch 86 | loss: 0.23952 | val_0_rmse: 0.53212 | val_1_rmse: 0.54159 |  0:02:52s
epoch 87 | loss: 0.24272 | val_0_rmse: 0.57323 | val_1_rmse: 0.58437 |  0:02:54s
epoch 88 | loss: 0.24008 | val_0_rmse: 0.53265 | val_1_rmse: 0.5453  |  0:02:56s
epoch 89 | loss: 0.24059 | val_0_rmse: 0.47732 | val_1_rmse: 0.49283 |  0:02:58s
epoch 90 | loss: 0.24032 | val_0_rmse: 0.49101 | val_1_rmse: 0.50548 |  0:03:00s
epoch 91 | loss: 0.24237 | val_0_rmse: 0.51196 | val_1_rmse: 0.5234  |  0:03:02s
epoch 92 | loss: 0.23933 | val_0_rmse: 0.59319 | val_1_rmse: 0.60272 |  0:03:04s
epoch 93 | loss: 0.24294 | val_0_rmse: 0.48507 | val_1_rmse: 0.50112 |  0:03:06s
epoch 94 | loss: 0.23753 | val_0_rmse: 0.551   | val_1_rmse: 0.56106 |  0:03:08s
epoch 95 | loss: 0.23814 | val_0_rmse: 0.50053 | val_1_rmse: 0.51173 |  0:03:10s
epoch 96 | loss: 0.23982 | val_0_rmse: 0.49599 | val_1_rmse: 0.50906 |  0:03:12s
epoch 97 | loss: 0.23871 | val_0_rmse: 0.48591 | val_1_rmse: 0.50062 |  0:03:14s
epoch 98 | loss: 0.23822 | val_0_rmse: 0.53371 | val_1_rmse: 0.54575 |  0:03:16s
epoch 99 | loss: 0.23963 | val_0_rmse: 0.52523 | val_1_rmse: 0.53847 |  0:03:18s
epoch 100| loss: 0.23908 | val_0_rmse: 0.51005 | val_1_rmse: 0.52277 |  0:03:20s
epoch 101| loss: 0.23997 | val_0_rmse: 0.50942 | val_1_rmse: 0.52345 |  0:03:22s
epoch 102| loss: 0.23656 | val_0_rmse: 0.55236 | val_1_rmse: 0.56219 |  0:03:24s
epoch 103| loss: 0.23951 | val_0_rmse: 0.52945 | val_1_rmse: 0.54428 |  0:03:26s
epoch 104| loss: 0.23861 | val_0_rmse: 0.51922 | val_1_rmse: 0.53188 |  0:03:28s
epoch 105| loss: 0.23597 | val_0_rmse: 0.54279 | val_1_rmse: 0.55467 |  0:03:30s
epoch 106| loss: 0.23977 | val_0_rmse: 0.50132 | val_1_rmse: 0.51987 |  0:03:32s
epoch 107| loss: 0.23728 | val_0_rmse: 0.50575 | val_1_rmse: 0.52294 |  0:03:34s
epoch 108| loss: 0.23588 | val_0_rmse: 0.53253 | val_1_rmse: 0.54468 |  0:03:36s
epoch 109| loss: 0.23667 | val_0_rmse: 0.48015 | val_1_rmse: 0.49713 |  0:03:38s
epoch 110| loss: 0.24112 | val_0_rmse: 0.5081  | val_1_rmse: 0.52305 |  0:03:40s
epoch 111| loss: 0.2351  | val_0_rmse: 0.4947  | val_1_rmse: 0.51197 |  0:03:42s
epoch 112| loss: 0.23468 | val_0_rmse: 0.49179 | val_1_rmse: 0.50885 |  0:03:44s
epoch 113| loss: 0.24232 | val_0_rmse: 0.51079 | val_1_rmse: 0.52258 |  0:03:46s
epoch 114| loss: 0.2354  | val_0_rmse: 0.5278  | val_1_rmse: 0.54139 |  0:03:48s
epoch 115| loss: 0.23451 | val_0_rmse: 0.55683 | val_1_rmse: 0.56742 |  0:03:49s
epoch 116| loss: 0.23648 | val_0_rmse: 0.47856 | val_1_rmse: 0.49827 |  0:03:51s
epoch 117| loss: 0.23639 | val_0_rmse: 0.51643 | val_1_rmse: 0.5306  |  0:03:53s
epoch 118| loss: 0.23601 | val_0_rmse: 0.48804 | val_1_rmse: 0.50472 |  0:03:55s
epoch 119| loss: 0.23701 | val_0_rmse: 0.52573 | val_1_rmse: 0.53983 |  0:03:57s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.49283
Best weights from best epoch are automatically used!
ended training at: 06:47:21
Feature importance:
[('Area', 0.46976631093264604), ('Baths', 0.0), ('Beds', 0.2091666826575778), ('Latitude', 0.0017985477826068755), ('Longitude', 0.22303973535341926), ('Month', 0.0), ('Year', 0.09622872327375005)]
Mean squared error is of 924306477.9006288
Mean absolute error:20659.053035521247
MAPE:0.25136465293596766
R2 score:0.768836491684055
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:47:21
epoch 0  | loss: 0.57541 | val_0_rmse: 0.65909 | val_1_rmse: 0.64829 |  0:00:01s
epoch 1  | loss: 0.38568 | val_0_rmse: 0.58925 | val_1_rmse: 0.57793 |  0:00:04s
epoch 2  | loss: 0.34996 | val_0_rmse: 0.58094 | val_1_rmse: 0.57556 |  0:00:05s
epoch 3  | loss: 0.34148 | val_0_rmse: 0.57568 | val_1_rmse: 0.57096 |  0:00:08s
epoch 4  | loss: 0.3245  | val_0_rmse: 0.56293 | val_1_rmse: 0.5609  |  0:00:10s
epoch 5  | loss: 0.31589 | val_0_rmse: 0.54998 | val_1_rmse: 0.54862 |  0:00:12s
epoch 6  | loss: 0.30722 | val_0_rmse: 0.54394 | val_1_rmse: 0.53961 |  0:00:13s
epoch 7  | loss: 0.30618 | val_0_rmse: 0.54996 | val_1_rmse: 0.54499 |  0:00:15s
epoch 8  | loss: 0.29667 | val_0_rmse: 0.55909 | val_1_rmse: 0.55708 |  0:00:17s
epoch 9  | loss: 0.31611 | val_0_rmse: 0.54786 | val_1_rmse: 0.54252 |  0:00:19s
epoch 10 | loss: 0.29826 | val_0_rmse: 0.53551 | val_1_rmse: 0.53003 |  0:00:21s
epoch 11 | loss: 0.30291 | val_0_rmse: 0.54283 | val_1_rmse: 0.53664 |  0:00:23s
epoch 12 | loss: 0.29268 | val_0_rmse: 0.53052 | val_1_rmse: 0.52666 |  0:00:25s
epoch 13 | loss: 0.29087 | val_0_rmse: 0.54713 | val_1_rmse: 0.54666 |  0:00:27s
epoch 14 | loss: 0.29223 | val_0_rmse: 0.56121 | val_1_rmse: 0.55419 |  0:00:29s
epoch 15 | loss: 0.294   | val_0_rmse: 0.53922 | val_1_rmse: 0.53074 |  0:00:31s
epoch 16 | loss: 0.28845 | val_0_rmse: 0.5281  | val_1_rmse: 0.52094 |  0:00:33s
epoch 17 | loss: 0.2885  | val_0_rmse: 0.52796 | val_1_rmse: 0.52666 |  0:00:35s
epoch 18 | loss: 0.28702 | val_0_rmse: 0.52577 | val_1_rmse: 0.52389 |  0:00:37s
epoch 19 | loss: 0.28478 | val_0_rmse: 0.52052 | val_1_rmse: 0.51626 |  0:00:39s
epoch 20 | loss: 0.28589 | val_0_rmse: 0.52328 | val_1_rmse: 0.518   |  0:00:41s
epoch 21 | loss: 0.28178 | val_0_rmse: 0.52059 | val_1_rmse: 0.51673 |  0:00:43s
epoch 22 | loss: 0.28184 | val_0_rmse: 0.53738 | val_1_rmse: 0.53733 |  0:00:45s
epoch 23 | loss: 0.28213 | val_0_rmse: 0.52499 | val_1_rmse: 0.5255  |  0:00:47s
epoch 24 | loss: 0.28388 | val_0_rmse: 0.52016 | val_1_rmse: 0.51482 |  0:00:49s
epoch 25 | loss: 0.28008 | val_0_rmse: 0.52662 | val_1_rmse: 0.52143 |  0:00:51s
epoch 26 | loss: 0.28059 | val_0_rmse: 0.53675 | val_1_rmse: 0.53715 |  0:00:53s
epoch 27 | loss: 0.27839 | val_0_rmse: 0.52147 | val_1_rmse: 0.51928 |  0:00:55s
epoch 28 | loss: 0.27879 | val_0_rmse: 0.5363  | val_1_rmse: 0.53557 |  0:00:57s
epoch 29 | loss: 0.28248 | val_0_rmse: 0.51824 | val_1_rmse: 0.51357 |  0:00:59s
epoch 30 | loss: 0.27483 | val_0_rmse: 0.55513 | val_1_rmse: 0.54707 |  0:01:01s
epoch 31 | loss: 0.26906 | val_0_rmse: 0.53757 | val_1_rmse: 0.53379 |  0:01:03s
epoch 32 | loss: 0.28575 | val_0_rmse: 0.53559 | val_1_rmse: 0.52999 |  0:01:05s
epoch 33 | loss: 0.26671 | val_0_rmse: 0.49823 | val_1_rmse: 0.49388 |  0:01:07s
epoch 34 | loss: 0.2645  | val_0_rmse: 0.51014 | val_1_rmse: 0.50308 |  0:01:09s
epoch 35 | loss: 0.26371 | val_0_rmse: 0.50913 | val_1_rmse: 0.5039  |  0:01:11s
epoch 36 | loss: 0.26387 | val_0_rmse: 0.51751 | val_1_rmse: 0.51194 |  0:01:13s
epoch 37 | loss: 0.2653  | val_0_rmse: 0.53827 | val_1_rmse: 0.53579 |  0:01:15s
epoch 38 | loss: 0.26381 | val_0_rmse: 0.51218 | val_1_rmse: 0.51157 |  0:01:17s
epoch 39 | loss: 0.2624  | val_0_rmse: 0.50815 | val_1_rmse: 0.50922 |  0:01:19s
epoch 40 | loss: 0.26623 | val_0_rmse: 0.51242 | val_1_rmse: 0.50675 |  0:01:21s
epoch 41 | loss: 0.26007 | val_0_rmse: 0.54872 | val_1_rmse: 0.54685 |  0:01:23s
epoch 42 | loss: 0.26623 | val_0_rmse: 0.524   | val_1_rmse: 0.51802 |  0:01:25s
epoch 43 | loss: 0.26369 | val_0_rmse: 0.53216 | val_1_rmse: 0.53104 |  0:01:27s
epoch 44 | loss: 0.27129 | val_0_rmse: 0.50282 | val_1_rmse: 0.49812 |  0:01:29s
epoch 45 | loss: 0.26457 | val_0_rmse: 0.51247 | val_1_rmse: 0.51314 |  0:01:31s
epoch 46 | loss: 0.25588 | val_0_rmse: 0.52161 | val_1_rmse: 0.52158 |  0:01:33s
epoch 47 | loss: 0.25963 | val_0_rmse: 0.54668 | val_1_rmse: 0.54918 |  0:01:35s
epoch 48 | loss: 0.25981 | val_0_rmse: 0.52793 | val_1_rmse: 0.52289 |  0:01:37s
epoch 49 | loss: 0.25857 | val_0_rmse: 0.51418 | val_1_rmse: 0.51289 |  0:01:39s
epoch 50 | loss: 0.26046 | val_0_rmse: 0.51945 | val_1_rmse: 0.51387 |  0:01:41s
epoch 51 | loss: 0.25776 | val_0_rmse: 0.54888 | val_1_rmse: 0.54895 |  0:01:43s
epoch 52 | loss: 0.25943 | val_0_rmse: 0.50016 | val_1_rmse: 0.49916 |  0:01:45s
epoch 53 | loss: 0.2559  | val_0_rmse: 0.53237 | val_1_rmse: 0.53146 |  0:01:47s
epoch 54 | loss: 0.26342 | val_0_rmse: 0.50906 | val_1_rmse: 0.50502 |  0:01:49s
epoch 55 | loss: 0.25786 | val_0_rmse: 0.50414 | val_1_rmse: 0.50128 |  0:01:51s
epoch 56 | loss: 0.25861 | val_0_rmse: 0.524   | val_1_rmse: 0.52276 |  0:01:53s
epoch 57 | loss: 0.2613  | val_0_rmse: 0.52661 | val_1_rmse: 0.5263  |  0:01:55s
epoch 58 | loss: 0.2608  | val_0_rmse: 0.51028 | val_1_rmse: 0.51239 |  0:01:57s
epoch 59 | loss: 0.25774 | val_0_rmse: 0.5055  | val_1_rmse: 0.50589 |  0:01:59s
epoch 60 | loss: 0.25742 | val_0_rmse: 0.55201 | val_1_rmse: 0.55272 |  0:02:01s
epoch 61 | loss: 0.25297 | val_0_rmse: 0.53931 | val_1_rmse: 0.53311 |  0:02:03s
epoch 62 | loss: 0.25193 | val_0_rmse: 0.50329 | val_1_rmse: 0.50207 |  0:02:04s
epoch 63 | loss: 0.24979 | val_0_rmse: 0.49962 | val_1_rmse: 0.49786 |  0:02:06s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.49388
Best weights from best epoch are automatically used!
ended training at: 06:49:28
Feature importance:
[('Area', 0.39317424880348684), ('Baths', 0.11680041920258878), ('Beds', 0.11786876615816012), ('Latitude', 0.01204828208453478), ('Longitude', 0.179538674360217), ('Month', 0.1399055568157804), ('Year', 0.04066405257523204)]
Mean squared error is of 965502686.5735972
Mean absolute error:21294.331422151965
MAPE:0.26118084202882313
R2 score:0.7586987058983549
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:49:29
epoch 0  | loss: 0.55237 | val_0_rmse: 0.60579 | val_1_rmse: 0.60369 |  0:00:01s
epoch 1  | loss: 0.34213 | val_0_rmse: 0.58812 | val_1_rmse: 0.58244 |  0:00:03s
epoch 2  | loss: 0.33441 | val_0_rmse: 0.55804 | val_1_rmse: 0.55289 |  0:00:05s
epoch 3  | loss: 0.32295 | val_0_rmse: 0.55407 | val_1_rmse: 0.54798 |  0:00:07s
epoch 4  | loss: 0.31266 | val_0_rmse: 0.55824 | val_1_rmse: 0.55405 |  0:00:09s
epoch 5  | loss: 0.3102  | val_0_rmse: 0.55618 | val_1_rmse: 0.55243 |  0:00:11s
epoch 6  | loss: 0.30562 | val_0_rmse: 0.54636 | val_1_rmse: 0.53945 |  0:00:13s
epoch 7  | loss: 0.29828 | val_0_rmse: 0.5324  | val_1_rmse: 0.52828 |  0:00:15s
epoch 8  | loss: 0.30557 | val_0_rmse: 0.5632  | val_1_rmse: 0.56441 |  0:00:18s
epoch 9  | loss: 0.3218  | val_0_rmse: 0.54722 | val_1_rmse: 0.54014 |  0:00:20s
epoch 10 | loss: 0.29974 | val_0_rmse: 0.53078 | val_1_rmse: 0.52484 |  0:00:22s
epoch 11 | loss: 0.29169 | val_0_rmse: 0.5292  | val_1_rmse: 0.52387 |  0:00:24s
epoch 12 | loss: 0.28896 | val_0_rmse: 0.527   | val_1_rmse: 0.52298 |  0:00:26s
epoch 13 | loss: 0.29329 | val_0_rmse: 0.52792 | val_1_rmse: 0.52234 |  0:00:28s
epoch 14 | loss: 0.28874 | val_0_rmse: 0.52913 | val_1_rmse: 0.52561 |  0:00:30s
epoch 15 | loss: 0.28512 | val_0_rmse: 0.52183 | val_1_rmse: 0.51941 |  0:00:32s
epoch 16 | loss: 0.28155 | val_0_rmse: 0.52164 | val_1_rmse: 0.51591 |  0:00:34s
epoch 17 | loss: 0.28295 | val_0_rmse: 0.51809 | val_1_rmse: 0.51148 |  0:00:36s
epoch 18 | loss: 0.28268 | val_0_rmse: 0.52502 | val_1_rmse: 0.52096 |  0:00:38s
epoch 19 | loss: 0.27999 | val_0_rmse: 0.52272 | val_1_rmse: 0.5207  |  0:00:40s
epoch 20 | loss: 0.29443 | val_0_rmse: 0.52759 | val_1_rmse: 0.52506 |  0:00:41s
epoch 21 | loss: 0.28304 | val_0_rmse: 0.52311 | val_1_rmse: 0.51953 |  0:00:43s
epoch 22 | loss: 0.2829  | val_0_rmse: 0.5227  | val_1_rmse: 0.52094 |  0:00:45s
epoch 23 | loss: 0.28201 | val_0_rmse: 0.52165 | val_1_rmse: 0.51883 |  0:00:47s
epoch 24 | loss: 0.28423 | val_0_rmse: 0.52466 | val_1_rmse: 0.5219  |  0:00:49s
epoch 25 | loss: 0.28239 | val_0_rmse: 0.51794 | val_1_rmse: 0.51618 |  0:00:51s
epoch 26 | loss: 0.28662 | val_0_rmse: 0.53657 | val_1_rmse: 0.53345 |  0:00:53s
epoch 27 | loss: 0.28511 | val_0_rmse: 0.53399 | val_1_rmse: 0.53085 |  0:00:55s
epoch 28 | loss: 0.27633 | val_0_rmse: 0.51608 | val_1_rmse: 0.51144 |  0:00:57s
epoch 29 | loss: 0.27629 | val_0_rmse: 0.51812 | val_1_rmse: 0.51479 |  0:00:59s
epoch 30 | loss: 0.27887 | val_0_rmse: 0.52198 | val_1_rmse: 0.51884 |  0:01:01s
epoch 31 | loss: 0.27911 | val_0_rmse: 0.51778 | val_1_rmse: 0.51324 |  0:01:03s
epoch 32 | loss: 0.27514 | val_0_rmse: 0.52243 | val_1_rmse: 0.519   |  0:01:05s
epoch 33 | loss: 0.27657 | val_0_rmse: 0.51586 | val_1_rmse: 0.51188 |  0:01:07s
epoch 34 | loss: 0.27067 | val_0_rmse: 0.50334 | val_1_rmse: 0.49417 |  0:01:09s
epoch 35 | loss: 0.27419 | val_0_rmse: 0.53338 | val_1_rmse: 0.5289  |  0:01:11s
epoch 36 | loss: 0.27253 | val_0_rmse: 0.52185 | val_1_rmse: 0.51794 |  0:01:13s
epoch 37 | loss: 0.27106 | val_0_rmse: 0.50382 | val_1_rmse: 0.49948 |  0:01:15s
epoch 38 | loss: 0.26695 | val_0_rmse: 0.55886 | val_1_rmse: 0.55327 |  0:01:17s
epoch 39 | loss: 0.26342 | val_0_rmse: 0.50674 | val_1_rmse: 0.50164 |  0:01:19s
epoch 40 | loss: 0.26649 | val_0_rmse: 0.51093 | val_1_rmse: 0.50522 |  0:01:21s
epoch 41 | loss: 0.26748 | val_0_rmse: 0.56628 | val_1_rmse: 0.56293 |  0:01:23s
epoch 42 | loss: 0.26588 | val_0_rmse: 0.50204 | val_1_rmse: 0.49639 |  0:01:25s
epoch 43 | loss: 0.26706 | val_0_rmse: 0.56479 | val_1_rmse: 0.56152 |  0:01:27s
epoch 44 | loss: 0.26606 | val_0_rmse: 0.5508  | val_1_rmse: 0.54812 |  0:01:29s
epoch 45 | loss: 0.27056 | val_0_rmse: 0.52552 | val_1_rmse: 0.51945 |  0:01:31s
epoch 46 | loss: 0.26989 | val_0_rmse: 0.51938 | val_1_rmse: 0.51112 |  0:01:33s
epoch 47 | loss: 0.26574 | val_0_rmse: 0.50549 | val_1_rmse: 0.49928 |  0:01:35s
epoch 48 | loss: 0.26136 | val_0_rmse: 0.49733 | val_1_rmse: 0.49404 |  0:01:37s
epoch 49 | loss: 0.26067 | val_0_rmse: 0.50561 | val_1_rmse: 0.50178 |  0:01:39s
epoch 50 | loss: 0.25688 | val_0_rmse: 0.51069 | val_1_rmse: 0.50515 |  0:01:41s
epoch 51 | loss: 0.25548 | val_0_rmse: 0.53348 | val_1_rmse: 0.53192 |  0:01:43s
epoch 52 | loss: 0.25237 | val_0_rmse: 0.51464 | val_1_rmse: 0.51116 |  0:01:45s
epoch 53 | loss: 0.25039 | val_0_rmse: 0.5729  | val_1_rmse: 0.57354 |  0:01:47s
epoch 54 | loss: 0.24794 | val_0_rmse: 0.53705 | val_1_rmse: 0.53383 |  0:01:49s
epoch 55 | loss: 0.25649 | val_0_rmse: 0.60292 | val_1_rmse: 0.60013 |  0:01:51s
epoch 56 | loss: 0.25918 | val_0_rmse: 0.50766 | val_1_rmse: 0.5041  |  0:01:53s
epoch 57 | loss: 0.25229 | val_0_rmse: 0.54124 | val_1_rmse: 0.53666 |  0:01:55s
epoch 58 | loss: 0.25687 | val_0_rmse: 0.50824 | val_1_rmse: 0.50391 |  0:01:57s
epoch 59 | loss: 0.25911 | val_0_rmse: 0.64661 | val_1_rmse: 0.64385 |  0:01:59s
epoch 60 | loss: 0.25463 | val_0_rmse: 0.49621 | val_1_rmse: 0.49253 |  0:02:01s
epoch 61 | loss: 0.24823 | val_0_rmse: 0.48645 | val_1_rmse: 0.48221 |  0:02:03s
epoch 62 | loss: 0.24782 | val_0_rmse: 0.56621 | val_1_rmse: 0.56552 |  0:02:05s
epoch 63 | loss: 0.24344 | val_0_rmse: 0.60702 | val_1_rmse: 0.60404 |  0:02:07s
epoch 64 | loss: 0.24491 | val_0_rmse: 0.57213 | val_1_rmse: 0.57007 |  0:02:08s
epoch 65 | loss: 0.2471  | val_0_rmse: 0.59491 | val_1_rmse: 0.59237 |  0:02:10s
epoch 66 | loss: 0.24478 | val_0_rmse: 0.55771 | val_1_rmse: 0.55365 |  0:02:12s
epoch 67 | loss: 0.24363 | val_0_rmse: 0.53832 | val_1_rmse: 0.53451 |  0:02:14s
epoch 68 | loss: 0.25071 | val_0_rmse: 0.5228  | val_1_rmse: 0.5223  |  0:02:16s
epoch 69 | loss: 0.24879 | val_0_rmse: 0.51439 | val_1_rmse: 0.51375 |  0:02:18s
epoch 70 | loss: 0.24633 | val_0_rmse: 0.5261  | val_1_rmse: 0.52159 |  0:02:20s
epoch 71 | loss: 0.24573 | val_0_rmse: 0.59904 | val_1_rmse: 0.59316 |  0:02:22s
epoch 72 | loss: 0.24647 | val_0_rmse: 0.51059 | val_1_rmse: 0.50632 |  0:02:24s
epoch 73 | loss: 0.24242 | val_0_rmse: 0.56962 | val_1_rmse: 0.56603 |  0:02:26s
epoch 74 | loss: 0.24333 | val_0_rmse: 0.51188 | val_1_rmse: 0.50902 |  0:02:28s
epoch 75 | loss: 0.24186 | val_0_rmse: 0.58008 | val_1_rmse: 0.57751 |  0:02:30s
epoch 76 | loss: 0.24326 | val_0_rmse: 0.56181 | val_1_rmse: 0.55796 |  0:02:32s
epoch 77 | loss: 0.26346 | val_0_rmse: 0.59433 | val_1_rmse: 0.59232 |  0:02:34s
epoch 78 | loss: 0.25929 | val_0_rmse: 0.50511 | val_1_rmse: 0.49766 |  0:02:36s
epoch 79 | loss: 0.25702 | val_0_rmse: 0.5273  | val_1_rmse: 0.523   |  0:02:38s
epoch 80 | loss: 0.25085 | val_0_rmse: 0.58397 | val_1_rmse: 0.5903  |  0:02:40s
epoch 81 | loss: 0.24503 | val_0_rmse: 0.60345 | val_1_rmse: 0.59815 |  0:02:42s
epoch 82 | loss: 0.24459 | val_0_rmse: 0.55274 | val_1_rmse: 0.54927 |  0:02:44s
epoch 83 | loss: 0.24639 | val_0_rmse: 0.56037 | val_1_rmse: 0.55883 |  0:02:46s
epoch 84 | loss: 0.24513 | val_0_rmse: 0.51836 | val_1_rmse: 0.51284 |  0:02:48s
epoch 85 | loss: 0.24269 | val_0_rmse: 0.50859 | val_1_rmse: 0.50439 |  0:02:50s
epoch 86 | loss: 0.24081 | val_0_rmse: 0.60831 | val_1_rmse: 0.6056  |  0:02:52s
epoch 87 | loss: 0.24164 | val_0_rmse: 0.57078 | val_1_rmse: 0.57033 |  0:02:54s
epoch 88 | loss: 0.24886 | val_0_rmse: 0.50088 | val_1_rmse: 0.49657 |  0:02:56s
epoch 89 | loss: 0.2404  | val_0_rmse: 0.57491 | val_1_rmse: 0.5702  |  0:02:58s
epoch 90 | loss: 0.23805 | val_0_rmse: 0.55623 | val_1_rmse: 0.56038 |  0:03:00s
epoch 91 | loss: 0.24134 | val_0_rmse: 0.54334 | val_1_rmse: 0.54076 |  0:03:02s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.48221
Best weights from best epoch are automatically used!
ended training at: 06:52:32
Feature importance:
[('Area', 0.47669294449738225), ('Baths', 0.09795830618141582), ('Beds', 0.06702189851120958), ('Latitude', 0.2040632431852543), ('Longitude', 0.15426360762473804), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 1057983932.7200409
Mean absolute error:21783.446333161028
MAPE:0.2600819507147516
R2 score:0.7404932546431693
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:52:32
epoch 0  | loss: 0.56811 | val_0_rmse: 0.64487 | val_1_rmse: 0.64387 |  0:00:02s
epoch 1  | loss: 0.35884 | val_0_rmse: 0.59099 | val_1_rmse: 0.59874 |  0:00:03s
epoch 2  | loss: 0.33981 | val_0_rmse: 0.5734  | val_1_rmse: 0.58078 |  0:00:05s
epoch 3  | loss: 0.32421 | val_0_rmse: 0.54677 | val_1_rmse: 0.54849 |  0:00:07s
epoch 4  | loss: 0.30722 | val_0_rmse: 0.53835 | val_1_rmse: 0.5428  |  0:00:09s
epoch 5  | loss: 0.29844 | val_0_rmse: 0.54209 | val_1_rmse: 0.54599 |  0:00:11s
epoch 6  | loss: 0.28924 | val_0_rmse: 0.5235  | val_1_rmse: 0.52629 |  0:00:13s
epoch 7  | loss: 0.28535 | val_0_rmse: 0.51826 | val_1_rmse: 0.51848 |  0:00:15s
epoch 8  | loss: 0.28599 | val_0_rmse: 0.5212  | val_1_rmse: 0.52329 |  0:00:17s
epoch 9  | loss: 0.28773 | val_0_rmse: 0.65682 | val_1_rmse: 0.66108 |  0:00:19s
epoch 10 | loss: 0.27442 | val_0_rmse: 0.51526 | val_1_rmse: 0.51412 |  0:00:21s
epoch 11 | loss: 0.2717  | val_0_rmse: 0.54072 | val_1_rmse: 0.54558 |  0:00:23s
epoch 12 | loss: 0.27211 | val_0_rmse: 0.52978 | val_1_rmse: 0.53898 |  0:00:25s
epoch 13 | loss: 0.26931 | val_0_rmse: 0.50214 | val_1_rmse: 0.50508 |  0:00:27s
epoch 14 | loss: 0.28128 | val_0_rmse: 0.66311 | val_1_rmse: 0.67332 |  0:00:29s
epoch 15 | loss: 0.26803 | val_0_rmse: 0.5319  | val_1_rmse: 0.53234 |  0:00:31s
epoch 16 | loss: 0.27202 | val_0_rmse: 0.50762 | val_1_rmse: 0.51301 |  0:00:33s
epoch 17 | loss: 0.26169 | val_0_rmse: 0.50569 | val_1_rmse: 0.50522 |  0:00:35s
epoch 18 | loss: 0.26717 | val_0_rmse: 0.90351 | val_1_rmse: 0.91911 |  0:00:37s
epoch 19 | loss: 0.26778 | val_0_rmse: 0.59058 | val_1_rmse: 0.58835 |  0:00:39s
epoch 20 | loss: 0.26562 | val_0_rmse: 0.51366 | val_1_rmse: 0.51518 |  0:00:41s
epoch 21 | loss: 0.2641  | val_0_rmse: 0.55066 | val_1_rmse: 0.54699 |  0:00:43s
epoch 22 | loss: 0.26415 | val_0_rmse: 0.49307 | val_1_rmse: 0.49479 |  0:00:45s
epoch 23 | loss: 0.26262 | val_0_rmse: 0.55365 | val_1_rmse: 0.55943 |  0:00:47s
epoch 24 | loss: 0.25835 | val_0_rmse: 0.5666  | val_1_rmse: 0.56525 |  0:00:49s
epoch 25 | loss: 0.26217 | val_0_rmse: 0.50731 | val_1_rmse: 0.50555 |  0:00:51s
epoch 26 | loss: 0.2618  | val_0_rmse: 0.50472 | val_1_rmse: 0.50999 |  0:00:53s
epoch 27 | loss: 0.26562 | val_0_rmse: 0.56992 | val_1_rmse: 0.56716 |  0:00:55s
epoch 28 | loss: 0.2614  | val_0_rmse: 0.63531 | val_1_rmse: 0.64632 |  0:00:57s
epoch 29 | loss: 0.26332 | val_0_rmse: 0.54584 | val_1_rmse: 0.54626 |  0:00:59s
epoch 30 | loss: 0.26106 | val_0_rmse: 0.50145 | val_1_rmse: 0.50894 |  0:01:01s
epoch 31 | loss: 0.26002 | val_0_rmse: 0.4963  | val_1_rmse: 0.49547 |  0:01:03s
epoch 32 | loss: 0.25659 | val_0_rmse: 0.54918 | val_1_rmse: 0.54846 |  0:01:05s
epoch 33 | loss: 0.25811 | val_0_rmse: 0.54339 | val_1_rmse: 0.55169 |  0:01:07s
epoch 34 | loss: 0.25555 | val_0_rmse: 0.50847 | val_1_rmse: 0.50665 |  0:01:09s
epoch 35 | loss: 0.25709 | val_0_rmse: 0.50327 | val_1_rmse: 0.50737 |  0:01:11s
epoch 36 | loss: 0.25573 | val_0_rmse: 0.49432 | val_1_rmse: 0.49686 |  0:01:13s
epoch 37 | loss: 0.25742 | val_0_rmse: 0.50843 | val_1_rmse: 0.51316 |  0:01:15s
epoch 38 | loss: 0.26247 | val_0_rmse: 0.65339 | val_1_rmse: 0.66216 |  0:01:17s
epoch 39 | loss: 0.25977 | val_0_rmse: 0.53784 | val_1_rmse: 0.53802 |  0:01:19s
epoch 40 | loss: 0.25549 | val_0_rmse: 0.49243 | val_1_rmse: 0.49328 |  0:01:21s
epoch 41 | loss: 0.25436 | val_0_rmse: 0.49689 | val_1_rmse: 0.49792 |  0:01:23s
epoch 42 | loss: 0.25747 | val_0_rmse: 0.54557 | val_1_rmse: 0.55299 |  0:01:25s
epoch 43 | loss: 0.25433 | val_0_rmse: 0.49341 | val_1_rmse: 0.49524 |  0:01:27s
epoch 44 | loss: 0.25886 | val_0_rmse: 0.50316 | val_1_rmse: 0.50263 |  0:01:29s
epoch 45 | loss: 0.25665 | val_0_rmse: 0.58207 | val_1_rmse: 0.58051 |  0:01:31s
epoch 46 | loss: 0.25584 | val_0_rmse: 0.52935 | val_1_rmse: 0.52779 |  0:01:33s
epoch 47 | loss: 0.25455 | val_0_rmse: 0.50558 | val_1_rmse: 0.50632 |  0:01:35s
epoch 48 | loss: 0.25547 | val_0_rmse: 0.49901 | val_1_rmse: 0.50122 |  0:01:36s
epoch 49 | loss: 0.25972 | val_0_rmse: 0.49361 | val_1_rmse: 0.49633 |  0:01:38s
epoch 50 | loss: 0.25448 | val_0_rmse: 0.50691 | val_1_rmse: 0.5074  |  0:01:40s
epoch 51 | loss: 0.25107 | val_0_rmse: 0.53496 | val_1_rmse: 0.54261 |  0:01:42s
epoch 52 | loss: 0.25501 | val_0_rmse: 0.53568 | val_1_rmse: 0.53373 |  0:01:44s
epoch 53 | loss: 0.26008 | val_0_rmse: 0.59825 | val_1_rmse: 0.5956  |  0:01:46s
epoch 54 | loss: 0.2585  | val_0_rmse: 0.50378 | val_1_rmse: 0.5072  |  0:01:48s
epoch 55 | loss: 0.253   | val_0_rmse: 0.57235 | val_1_rmse: 0.56812 |  0:01:50s
epoch 56 | loss: 0.25716 | val_0_rmse: 0.49301 | val_1_rmse: 0.49729 |  0:01:52s
epoch 57 | loss: 0.2498  | val_0_rmse: 0.49857 | val_1_rmse: 0.50021 |  0:01:54s
epoch 58 | loss: 0.25065 | val_0_rmse: 0.53931 | val_1_rmse: 0.53919 |  0:01:56s
epoch 59 | loss: 0.24839 | val_0_rmse: 0.53696 | val_1_rmse: 0.53806 |  0:01:58s
epoch 60 | loss: 0.24893 | val_0_rmse: 0.49088 | val_1_rmse: 0.49514 |  0:02:00s
epoch 61 | loss: 0.25245 | val_0_rmse: 0.49242 | val_1_rmse: 0.49687 |  0:02:02s
epoch 62 | loss: 0.25118 | val_0_rmse: 0.5049  | val_1_rmse: 0.51061 |  0:02:04s
epoch 63 | loss: 0.25045 | val_0_rmse: 0.53608 | val_1_rmse: 0.5482  |  0:02:06s
epoch 64 | loss: 0.251   | val_0_rmse: 0.48888 | val_1_rmse: 0.49313 |  0:02:08s
epoch 65 | loss: 0.24595 | val_0_rmse: 0.51296 | val_1_rmse: 0.51343 |  0:02:10s
epoch 66 | loss: 0.24495 | val_0_rmse: 0.48895 | val_1_rmse: 0.49381 |  0:02:12s
epoch 67 | loss: 0.24817 | val_0_rmse: 0.49231 | val_1_rmse: 0.49377 |  0:02:14s
epoch 68 | loss: 0.2506  | val_0_rmse: 0.5044  | val_1_rmse: 0.50622 |  0:02:16s
epoch 69 | loss: 0.24904 | val_0_rmse: 0.50978 | val_1_rmse: 0.51119 |  0:02:18s
epoch 70 | loss: 0.24705 | val_0_rmse: 0.48639 | val_1_rmse: 0.48922 |  0:02:20s
epoch 71 | loss: 0.24868 | val_0_rmse: 0.48696 | val_1_rmse: 0.49087 |  0:02:22s
epoch 72 | loss: 0.24645 | val_0_rmse: 0.5898  | val_1_rmse: 0.58826 |  0:02:24s
epoch 73 | loss: 0.24866 | val_0_rmse: 0.54049 | val_1_rmse: 0.54221 |  0:02:26s
epoch 74 | loss: 0.24825 | val_0_rmse: 0.49251 | val_1_rmse: 0.49826 |  0:02:28s
epoch 75 | loss: 0.24969 | val_0_rmse: 0.49823 | val_1_rmse: 0.49949 |  0:02:30s
epoch 76 | loss: 0.24854 | val_0_rmse: 0.5895  | val_1_rmse: 0.58914 |  0:02:32s
epoch 77 | loss: 0.24775 | val_0_rmse: 0.48478 | val_1_rmse: 0.48913 |  0:02:34s
epoch 78 | loss: 0.25054 | val_0_rmse: 0.50086 | val_1_rmse: 0.50336 |  0:02:36s
epoch 79 | loss: 0.24684 | val_0_rmse: 0.48714 | val_1_rmse: 0.49053 |  0:02:38s
epoch 80 | loss: 0.24873 | val_0_rmse: 0.50339 | val_1_rmse: 0.50292 |  0:02:40s
epoch 81 | loss: 0.259   | val_0_rmse: 0.51311 | val_1_rmse: 0.51766 |  0:02:42s
epoch 82 | loss: 0.26126 | val_0_rmse: 0.52719 | val_1_rmse: 0.52375 |  0:02:44s
epoch 83 | loss: 0.25592 | val_0_rmse: 0.51315 | val_1_rmse: 0.5134  |  0:02:46s
epoch 84 | loss: 0.25754 | val_0_rmse: 0.54558 | val_1_rmse: 0.54544 |  0:02:48s
epoch 85 | loss: 0.25532 | val_0_rmse: 0.4872  | val_1_rmse: 0.48996 |  0:02:50s
epoch 86 | loss: 0.25107 | val_0_rmse: 0.62625 | val_1_rmse: 0.63923 |  0:02:52s
epoch 87 | loss: 0.25494 | val_0_rmse: 0.50812 | val_1_rmse: 0.51113 |  0:02:54s
epoch 88 | loss: 0.25343 | val_0_rmse: 0.55295 | val_1_rmse: 0.551   |  0:02:56s
epoch 89 | loss: 0.25444 | val_0_rmse: 0.50345 | val_1_rmse: 0.5063  |  0:02:58s
epoch 90 | loss: 0.24947 | val_0_rmse: 0.54066 | val_1_rmse: 0.53851 |  0:03:00s
epoch 91 | loss: 0.25087 | val_0_rmse: 0.4942  | val_1_rmse: 0.49849 |  0:03:02s
epoch 92 | loss: 0.24855 | val_0_rmse: 0.49611 | val_1_rmse: 0.49533 |  0:03:04s
epoch 93 | loss: 0.24928 | val_0_rmse: 0.51149 | val_1_rmse: 0.51585 |  0:03:06s
epoch 94 | loss: 0.24996 | val_0_rmse: 0.56021 | val_1_rmse: 0.56774 |  0:03:08s
epoch 95 | loss: 0.25403 | val_0_rmse: 0.51513 | val_1_rmse: 0.51369 |  0:03:10s
epoch 96 | loss: 0.25081 | val_0_rmse: 0.49078 | val_1_rmse: 0.4938  |  0:03:12s
epoch 97 | loss: 0.24686 | val_0_rmse: 0.51426 | val_1_rmse: 0.5173  |  0:03:14s
epoch 98 | loss: 0.24723 | val_0_rmse: 0.50488 | val_1_rmse: 0.5083  |  0:03:16s
epoch 99 | loss: 0.24452 | val_0_rmse: 0.52139 | val_1_rmse: 0.52378 |  0:03:18s
epoch 100| loss: 0.25484 | val_0_rmse: 0.53713 | val_1_rmse: 0.5478  |  0:03:20s
epoch 101| loss: 0.25039 | val_0_rmse: 0.55802 | val_1_rmse: 0.55403 |  0:03:22s
epoch 102| loss: 0.24517 | val_0_rmse: 0.5016  | val_1_rmse: 0.50169 |  0:03:24s
epoch 103| loss: 0.24668 | val_0_rmse: 0.50374 | val_1_rmse: 0.50884 |  0:03:26s
epoch 104| loss: 0.24956 | val_0_rmse: 0.50381 | val_1_rmse: 0.50392 |  0:03:28s
epoch 105| loss: 0.24343 | val_0_rmse: 0.54275 | val_1_rmse: 0.54249 |  0:03:30s
epoch 106| loss: 0.24432 | val_0_rmse: 0.49706 | val_1_rmse: 0.50089 |  0:03:32s
epoch 107| loss: 0.2479  | val_0_rmse: 0.49495 | val_1_rmse: 0.50092 |  0:03:33s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.48913
Best weights from best epoch are automatically used!
ended training at: 06:56:07
Feature importance:
[('Area', 0.4062690806104566), ('Baths', 0.076086723611614), ('Beds', 0.06922290744909194), ('Latitude', 0.0794124853347776), ('Longitude', 0.17209988544844587), ('Month', 0.11627095108154821), ('Year', 0.08063796646406576)]
Mean squared error is of 1018099386.5541039
Mean absolute error:21679.89949677066
MAPE:0.2594812569874521
R2 score:0.7470674294108762
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:56:07
epoch 0  | loss: 0.54652 | val_0_rmse: 0.62791 | val_1_rmse: 0.63687 |  0:00:02s
epoch 1  | loss: 0.34032 | val_0_rmse: 0.56217 | val_1_rmse: 0.56669 |  0:00:03s
epoch 2  | loss: 0.32734 | val_0_rmse: 0.56287 | val_1_rmse: 0.57029 |  0:00:05s
epoch 3  | loss: 0.31852 | val_0_rmse: 0.54814 | val_1_rmse: 0.55527 |  0:00:07s
epoch 4  | loss: 0.3052  | val_0_rmse: 0.53712 | val_1_rmse: 0.54279 |  0:00:09s
epoch 5  | loss: 0.30762 | val_0_rmse: 0.55628 | val_1_rmse: 0.56756 |  0:00:11s
epoch 6  | loss: 0.30556 | val_0_rmse: 0.54773 | val_1_rmse: 0.55538 |  0:00:13s
epoch 7  | loss: 0.29654 | val_0_rmse: 0.52918 | val_1_rmse: 0.53761 |  0:00:15s
epoch 8  | loss: 0.28602 | val_0_rmse: 0.52132 | val_1_rmse: 0.52663 |  0:00:17s
epoch 9  | loss: 0.30332 | val_0_rmse: 0.53534 | val_1_rmse: 0.54207 |  0:00:19s
epoch 10 | loss: 0.28939 | val_0_rmse: 0.5165  | val_1_rmse: 0.51823 |  0:00:21s
epoch 11 | loss: 0.28057 | val_0_rmse: 0.51621 | val_1_rmse: 0.52146 |  0:00:23s
epoch 12 | loss: 0.28024 | val_0_rmse: 0.50918 | val_1_rmse: 0.51479 |  0:00:25s
epoch 13 | loss: 0.27723 | val_0_rmse: 0.6158  | val_1_rmse: 0.61894 |  0:00:27s
epoch 14 | loss: 0.28809 | val_0_rmse: 0.51954 | val_1_rmse: 0.52493 |  0:00:29s
epoch 15 | loss: 0.27305 | val_0_rmse: 0.52507 | val_1_rmse: 0.53072 |  0:00:31s
epoch 16 | loss: 0.29964 | val_0_rmse: 0.59901 | val_1_rmse: 0.59971 |  0:00:33s
epoch 17 | loss: 0.2795  | val_0_rmse: 0.51188 | val_1_rmse: 0.51638 |  0:00:35s
epoch 18 | loss: 0.27368 | val_0_rmse: 0.51681 | val_1_rmse: 0.5249  |  0:00:37s
epoch 19 | loss: 0.26488 | val_0_rmse: 0.54535 | val_1_rmse: 0.55469 |  0:00:39s
epoch 20 | loss: 0.25818 | val_0_rmse: 0.49421 | val_1_rmse: 0.50061 |  0:00:41s
epoch 21 | loss: 0.25597 | val_0_rmse: 0.52733 | val_1_rmse: 0.53078 |  0:00:43s
epoch 22 | loss: 0.26231 | val_0_rmse: 0.51716 | val_1_rmse: 0.52706 |  0:00:45s
epoch 23 | loss: 0.25615 | val_0_rmse: 0.53149 | val_1_rmse: 0.53995 |  0:00:47s
epoch 24 | loss: 0.26016 | val_0_rmse: 0.50295 | val_1_rmse: 0.5092  |  0:00:49s
epoch 25 | loss: 0.26047 | val_0_rmse: 0.52349 | val_1_rmse: 0.52977 |  0:00:51s
epoch 26 | loss: 0.25921 | val_0_rmse: 0.51402 | val_1_rmse: 0.52055 |  0:00:53s
epoch 27 | loss: 0.25542 | val_0_rmse: 0.54584 | val_1_rmse: 0.55367 |  0:00:55s
epoch 28 | loss: 0.25011 | val_0_rmse: 0.53077 | val_1_rmse: 0.54065 |  0:00:57s
epoch 29 | loss: 0.25177 | val_0_rmse: 0.51962 | val_1_rmse: 0.53082 |  0:00:59s
epoch 30 | loss: 0.25255 | val_0_rmse: 0.51973 | val_1_rmse: 0.52614 |  0:01:01s
epoch 31 | loss: 0.26093 | val_0_rmse: 0.52753 | val_1_rmse: 0.53402 |  0:01:03s
epoch 32 | loss: 0.25244 | val_0_rmse: 0.49812 | val_1_rmse: 0.50351 |  0:01:05s
epoch 33 | loss: 0.25091 | val_0_rmse: 0.52027 | val_1_rmse: 0.52852 |  0:01:07s
epoch 34 | loss: 0.25122 | val_0_rmse: 0.49462 | val_1_rmse: 0.4987  |  0:01:09s
epoch 35 | loss: 0.24955 | val_0_rmse: 0.51633 | val_1_rmse: 0.52477 |  0:01:11s
epoch 36 | loss: 0.24577 | val_0_rmse: 0.52831 | val_1_rmse: 0.5369  |  0:01:13s
epoch 37 | loss: 0.24514 | val_0_rmse: 0.5072  | val_1_rmse: 0.51609 |  0:01:15s
epoch 38 | loss: 0.24572 | val_0_rmse: 0.53285 | val_1_rmse: 0.54003 |  0:01:17s
epoch 39 | loss: 0.25012 | val_0_rmse: 0.5244  | val_1_rmse: 0.52956 |  0:01:19s
epoch 40 | loss: 0.24307 | val_0_rmse: 0.49605 | val_1_rmse: 0.50504 |  0:01:21s
epoch 41 | loss: 0.24511 | val_0_rmse: 0.48249 | val_1_rmse: 0.49284 |  0:01:23s
epoch 42 | loss: 0.24103 | val_0_rmse: 0.4875  | val_1_rmse: 0.49833 |  0:01:25s
epoch 43 | loss: 0.24393 | val_0_rmse: 0.5193  | val_1_rmse: 0.53156 |  0:01:27s
epoch 44 | loss: 0.24729 | val_0_rmse: 0.52495 | val_1_rmse: 0.53363 |  0:01:29s
epoch 45 | loss: 0.24507 | val_0_rmse: 0.48648 | val_1_rmse: 0.49482 |  0:01:31s
epoch 46 | loss: 0.24294 | val_0_rmse: 0.50233 | val_1_rmse: 0.50985 |  0:01:33s
epoch 47 | loss: 0.24204 | val_0_rmse: 0.55689 | val_1_rmse: 0.56978 |  0:01:35s
epoch 48 | loss: 0.24442 | val_0_rmse: 0.51942 | val_1_rmse: 0.53278 |  0:01:37s
epoch 49 | loss: 0.24216 | val_0_rmse: 0.53004 | val_1_rmse: 0.53971 |  0:01:39s
epoch 50 | loss: 0.23997 | val_0_rmse: 0.47877 | val_1_rmse: 0.49015 |  0:01:41s
epoch 51 | loss: 0.24342 | val_0_rmse: 0.49556 | val_1_rmse: 0.50676 |  0:01:43s
epoch 52 | loss: 0.24001 | val_0_rmse: 0.5636  | val_1_rmse: 0.5765  |  0:01:45s
epoch 53 | loss: 0.24615 | val_0_rmse: 0.48604 | val_1_rmse: 0.49472 |  0:01:47s
epoch 54 | loss: 0.24171 | val_0_rmse: 0.48488 | val_1_rmse: 0.4955  |  0:01:49s
epoch 55 | loss: 0.2393  | val_0_rmse: 0.48794 | val_1_rmse: 0.49652 |  0:01:51s
epoch 56 | loss: 0.24124 | val_0_rmse: 0.52718 | val_1_rmse: 0.54029 |  0:01:53s
epoch 57 | loss: 0.2436  | val_0_rmse: 0.49232 | val_1_rmse: 0.50539 |  0:01:55s
epoch 58 | loss: 0.2446  | val_0_rmse: 0.49921 | val_1_rmse: 0.51004 |  0:01:57s
epoch 59 | loss: 0.24615 | val_0_rmse: 0.52566 | val_1_rmse: 0.53388 |  0:01:59s
epoch 60 | loss: 0.23946 | val_0_rmse: 0.52079 | val_1_rmse: 0.53259 |  0:02:01s
epoch 61 | loss: 0.24252 | val_0_rmse: 0.50648 | val_1_rmse: 0.52071 |  0:02:03s
epoch 62 | loss: 0.23941 | val_0_rmse: 0.48364 | val_1_rmse: 0.49553 |  0:02:05s
epoch 63 | loss: 0.23642 | val_0_rmse: 0.50675 | val_1_rmse: 0.51968 |  0:02:07s
epoch 64 | loss: 0.23858 | val_0_rmse: 0.5242  | val_1_rmse: 0.53676 |  0:02:09s
epoch 65 | loss: 0.23586 | val_0_rmse: 0.56993 | val_1_rmse: 0.5754  |  0:02:11s
epoch 66 | loss: 0.23511 | val_0_rmse: 0.50402 | val_1_rmse: 0.51983 |  0:02:12s
epoch 67 | loss: 0.2372  | val_0_rmse: 0.47928 | val_1_rmse: 0.49371 |  0:02:14s
epoch 68 | loss: 0.24163 | val_0_rmse: 0.51283 | val_1_rmse: 0.5255  |  0:02:16s
epoch 69 | loss: 0.23775 | val_0_rmse: 0.51755 | val_1_rmse: 0.52948 |  0:02:18s
epoch 70 | loss: 0.24305 | val_0_rmse: 0.48398 | val_1_rmse: 0.49469 |  0:02:20s
epoch 71 | loss: 0.2383  | val_0_rmse: 0.51533 | val_1_rmse: 0.52818 |  0:02:22s
epoch 72 | loss: 0.23788 | val_0_rmse: 0.53668 | val_1_rmse: 0.54475 |  0:02:24s
epoch 73 | loss: 0.24069 | val_0_rmse: 0.5386  | val_1_rmse: 0.55573 |  0:02:26s
epoch 74 | loss: 0.23613 | val_0_rmse: 0.47638 | val_1_rmse: 0.48828 |  0:02:28s
epoch 75 | loss: 0.23938 | val_0_rmse: 0.48835 | val_1_rmse: 0.5     |  0:02:30s
epoch 76 | loss: 0.2413  | val_0_rmse: 0.50438 | val_1_rmse: 0.51731 |  0:02:32s
epoch 77 | loss: 0.2397  | val_0_rmse: 0.50305 | val_1_rmse: 0.51913 |  0:02:34s
epoch 78 | loss: 0.23516 | val_0_rmse: 0.51555 | val_1_rmse: 0.52752 |  0:02:36s
epoch 79 | loss: 0.23606 | val_0_rmse: 0.51926 | val_1_rmse: 0.53468 |  0:02:38s
epoch 80 | loss: 0.23955 | val_0_rmse: 0.48042 | val_1_rmse: 0.49677 |  0:02:40s
epoch 81 | loss: 0.23889 | val_0_rmse: 0.52173 | val_1_rmse: 0.53269 |  0:02:42s
epoch 82 | loss: 0.23563 | val_0_rmse: 0.53269 | val_1_rmse: 0.54816 |  0:02:44s
epoch 83 | loss: 0.23845 | val_0_rmse: 0.52062 | val_1_rmse: 0.52755 |  0:02:46s
epoch 84 | loss: 0.23586 | val_0_rmse: 0.52251 | val_1_rmse: 0.53766 |  0:02:48s
epoch 85 | loss: 0.23459 | val_0_rmse: 0.50091 | val_1_rmse: 0.5111  |  0:02:50s
epoch 86 | loss: 0.23564 | val_0_rmse: 0.53064 | val_1_rmse: 0.54456 |  0:02:52s
epoch 87 | loss: 0.23623 | val_0_rmse: 0.51988 | val_1_rmse: 0.52518 |  0:02:54s
epoch 88 | loss: 0.23668 | val_0_rmse: 0.48343 | val_1_rmse: 0.4964  |  0:02:56s
epoch 89 | loss: 0.23302 | val_0_rmse: 0.50935 | val_1_rmse: 0.51751 |  0:02:58s
epoch 90 | loss: 0.23283 | val_0_rmse: 0.4857  | val_1_rmse: 0.50157 |  0:03:00s
epoch 91 | loss: 0.23614 | val_0_rmse: 0.50656 | val_1_rmse: 0.52404 |  0:03:02s
epoch 92 | loss: 0.23071 | val_0_rmse: 0.54891 | val_1_rmse: 0.56097 |  0:03:04s
epoch 93 | loss: 0.23714 | val_0_rmse: 0.54135 | val_1_rmse: 0.55081 |  0:03:06s
epoch 94 | loss: 0.23698 | val_0_rmse: 0.49585 | val_1_rmse: 0.51485 |  0:03:08s
epoch 95 | loss: 0.24088 | val_0_rmse: 0.58393 | val_1_rmse: 0.59187 |  0:03:10s
epoch 96 | loss: 0.2406  | val_0_rmse: 0.53711 | val_1_rmse: 0.55138 |  0:03:12s
epoch 97 | loss: 0.23285 | val_0_rmse: 0.49792 | val_1_rmse: 0.51137 |  0:03:14s
epoch 98 | loss: 0.23517 | val_0_rmse: 0.54263 | val_1_rmse: 0.55053 |  0:03:16s
epoch 99 | loss: 0.2375  | val_0_rmse: 0.5136  | val_1_rmse: 0.53081 |  0:03:18s
epoch 100| loss: 0.23656 | val_0_rmse: 0.49596 | val_1_rmse: 0.51257 |  0:03:20s
epoch 101| loss: 0.23204 | val_0_rmse: 0.50514 | val_1_rmse: 0.5243  |  0:03:22s
epoch 102| loss: 0.23219 | val_0_rmse: 0.482   | val_1_rmse: 0.49839 |  0:03:24s
epoch 103| loss: 0.23384 | val_0_rmse: 0.52241 | val_1_rmse: 0.53697 |  0:03:26s
epoch 104| loss: 0.2348  | val_0_rmse: 0.5068  | val_1_rmse: 0.51747 |  0:03:28s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.48828
Best weights from best epoch are automatically used!
ended training at: 06:59:36
Feature importance:
[('Area', 0.36815202983188905), ('Baths', 0.18407786928968403), ('Beds', 0.060376133608898215), ('Latitude', 0.204863540587978), ('Longitude', 0.1129413742461057), ('Month', 0.002234426125595848), ('Year', 0.06735462630984915)]
Mean squared error is of 926604656.8246791
Mean absolute error:20616.760162778963
MAPE:0.26024932595427613
R2 score:0.7638476345382672
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:59:36
epoch 0  | loss: 0.54113 | val_0_rmse: 0.60868 | val_1_rmse: 0.60971 |  0:00:02s
epoch 1  | loss: 0.35443 | val_0_rmse: 0.56765 | val_1_rmse: 0.56871 |  0:00:04s
epoch 2  | loss: 0.3227  | val_0_rmse: 0.57782 | val_1_rmse: 0.58308 |  0:00:05s
epoch 3  | loss: 0.32138 | val_0_rmse: 0.57434 | val_1_rmse: 0.5766  |  0:00:07s
epoch 4  | loss: 0.31575 | val_0_rmse: 0.58329 | val_1_rmse: 0.59186 |  0:00:09s
epoch 5  | loss: 0.31064 | val_0_rmse: 0.53935 | val_1_rmse: 0.54502 |  0:00:11s
epoch 6  | loss: 0.2987  | val_0_rmse: 0.53356 | val_1_rmse: 0.54161 |  0:00:13s
epoch 7  | loss: 0.2998  | val_0_rmse: 0.54009 | val_1_rmse: 0.55009 |  0:00:15s
epoch 8  | loss: 0.29858 | val_0_rmse: 0.53451 | val_1_rmse: 0.54332 |  0:00:17s
epoch 9  | loss: 0.29412 | val_0_rmse: 0.53048 | val_1_rmse: 0.53783 |  0:00:19s
epoch 10 | loss: 0.29434 | val_0_rmse: 0.57504 | val_1_rmse: 0.57857 |  0:00:21s
epoch 11 | loss: 0.31751 | val_0_rmse: 0.55215 | val_1_rmse: 0.55797 |  0:00:23s
epoch 12 | loss: 0.30699 | val_0_rmse: 0.5606  | val_1_rmse: 0.56359 |  0:00:25s
epoch 13 | loss: 0.30522 | val_0_rmse: 0.54595 | val_1_rmse: 0.55094 |  0:00:27s
epoch 14 | loss: 0.31354 | val_0_rmse: 0.55104 | val_1_rmse: 0.55678 |  0:00:29s
epoch 15 | loss: 0.31269 | val_0_rmse: 0.54333 | val_1_rmse: 0.54548 |  0:00:31s
epoch 16 | loss: 0.29474 | val_0_rmse: 0.55617 | val_1_rmse: 0.56061 |  0:00:33s
epoch 17 | loss: 0.28489 | val_0_rmse: 0.51544 | val_1_rmse: 0.52387 |  0:00:35s
epoch 18 | loss: 0.28107 | val_0_rmse: 0.75952 | val_1_rmse: 0.76475 |  0:00:37s
epoch 19 | loss: 0.29016 | val_0_rmse: 0.51727 | val_1_rmse: 0.52967 |  0:00:39s
epoch 20 | loss: 0.28322 | val_0_rmse: 0.52644 | val_1_rmse: 0.53655 |  0:00:41s
epoch 21 | loss: 0.28666 | val_0_rmse: 0.55229 | val_1_rmse: 0.56375 |  0:00:43s
epoch 22 | loss: 0.28809 | val_0_rmse: 0.53332 | val_1_rmse: 0.54302 |  0:00:45s
epoch 23 | loss: 0.27557 | val_0_rmse: 0.61496 | val_1_rmse: 0.6191  |  0:00:47s
epoch 24 | loss: 0.28069 | val_0_rmse: 0.57322 | val_1_rmse: 0.58035 |  0:00:49s
epoch 25 | loss: 0.30843 | val_0_rmse: 0.53835 | val_1_rmse: 0.5455  |  0:00:51s
epoch 26 | loss: 0.29433 | val_0_rmse: 0.53151 | val_1_rmse: 0.53849 |  0:00:53s
epoch 27 | loss: 0.32243 | val_0_rmse: 0.5465  | val_1_rmse: 0.55071 |  0:00:55s
epoch 28 | loss: 0.29614 | val_0_rmse: 0.52854 | val_1_rmse: 0.5312  |  0:00:57s
epoch 29 | loss: 0.28421 | val_0_rmse: 0.53901 | val_1_rmse: 0.5471  |  0:00:59s
epoch 30 | loss: 0.28237 | val_0_rmse: 0.53685 | val_1_rmse: 0.5481  |  0:01:01s
epoch 31 | loss: 0.28209 | val_0_rmse: 0.55703 | val_1_rmse: 0.56502 |  0:01:03s
epoch 32 | loss: 0.28443 | val_0_rmse: 0.5365  | val_1_rmse: 0.54462 |  0:01:05s
epoch 33 | loss: 0.29194 | val_0_rmse: 0.53144 | val_1_rmse: 0.53471 |  0:01:07s
epoch 34 | loss: 0.27993 | val_0_rmse: 0.59408 | val_1_rmse: 0.59773 |  0:01:09s
epoch 35 | loss: 0.2749  | val_0_rmse: 0.51428 | val_1_rmse: 0.52139 |  0:01:11s
epoch 36 | loss: 0.26954 | val_0_rmse: 0.51159 | val_1_rmse: 0.52302 |  0:01:13s
epoch 37 | loss: 0.26953 | val_0_rmse: 0.528   | val_1_rmse: 0.53357 |  0:01:15s
epoch 38 | loss: 0.27812 | val_0_rmse: 0.55966 | val_1_rmse: 0.56529 |  0:01:17s
epoch 39 | loss: 0.27413 | val_0_rmse: 0.51117 | val_1_rmse: 0.51967 |  0:01:19s
epoch 40 | loss: 0.26942 | val_0_rmse: 0.58533 | val_1_rmse: 0.59124 |  0:01:21s
epoch 41 | loss: 0.26821 | val_0_rmse: 0.53259 | val_1_rmse: 0.54378 |  0:01:23s
epoch 42 | loss: 0.26727 | val_0_rmse: 0.61324 | val_1_rmse: 0.61643 |  0:01:25s
epoch 43 | loss: 0.25892 | val_0_rmse: 0.54436 | val_1_rmse: 0.55297 |  0:01:27s
epoch 44 | loss: 0.25618 | val_0_rmse: 0.50158 | val_1_rmse: 0.51345 |  0:01:29s
epoch 45 | loss: 0.25782 | val_0_rmse: 0.50864 | val_1_rmse: 0.51884 |  0:01:31s
epoch 46 | loss: 0.2641  | val_0_rmse: 0.533   | val_1_rmse: 0.54507 |  0:01:33s
epoch 47 | loss: 0.25676 | val_0_rmse: 0.51471 | val_1_rmse: 0.52604 |  0:01:35s
epoch 48 | loss: 0.25738 | val_0_rmse: 0.50638 | val_1_rmse: 0.51775 |  0:01:37s
epoch 49 | loss: 0.25822 | val_0_rmse: 0.62903 | val_1_rmse: 0.62951 |  0:01:39s
epoch 50 | loss: 0.26464 | val_0_rmse: 0.49851 | val_1_rmse: 0.51061 |  0:01:41s
epoch 51 | loss: 0.2542  | val_0_rmse: 0.62003 | val_1_rmse: 0.62157 |  0:01:43s
epoch 52 | loss: 0.2571  | val_0_rmse: 0.54208 | val_1_rmse: 0.54825 |  0:01:45s
epoch 53 | loss: 0.25594 | val_0_rmse: 0.51253 | val_1_rmse: 0.52406 |  0:01:47s
epoch 54 | loss: 0.25636 | val_0_rmse: 0.58145 | val_1_rmse: 0.58848 |  0:01:49s
epoch 55 | loss: 0.25751 | val_0_rmse: 0.65306 | val_1_rmse: 0.65426 |  0:01:51s
epoch 56 | loss: 0.25769 | val_0_rmse: 0.5023  | val_1_rmse: 0.51461 |  0:01:53s
epoch 57 | loss: 0.25568 | val_0_rmse: 0.50244 | val_1_rmse: 0.51252 |  0:01:55s
epoch 58 | loss: 0.25396 | val_0_rmse: 0.50257 | val_1_rmse: 0.51411 |  0:01:57s
epoch 59 | loss: 0.25377 | val_0_rmse: 0.59592 | val_1_rmse: 0.60186 |  0:01:59s
epoch 60 | loss: 0.2535  | val_0_rmse: 0.51452 | val_1_rmse: 0.52592 |  0:02:01s
epoch 61 | loss: 0.25718 | val_0_rmse: 0.50726 | val_1_rmse: 0.52044 |  0:02:03s
epoch 62 | loss: 0.25536 | val_0_rmse: 0.51795 | val_1_rmse: 0.52655 |  0:02:05s
epoch 63 | loss: 0.25214 | val_0_rmse: 0.49509 | val_1_rmse: 0.50735 |  0:02:07s
epoch 64 | loss: 0.25401 | val_0_rmse: 0.49548 | val_1_rmse: 0.50948 |  0:02:09s
epoch 65 | loss: 0.2538  | val_0_rmse: 0.56205 | val_1_rmse: 0.56845 |  0:02:11s
epoch 66 | loss: 0.25537 | val_0_rmse: 0.53091 | val_1_rmse: 0.54058 |  0:02:13s
epoch 67 | loss: 0.25256 | val_0_rmse: 0.51207 | val_1_rmse: 0.52497 |  0:02:15s
epoch 68 | loss: 0.25357 | val_0_rmse: 0.5184  | val_1_rmse: 0.52596 |  0:02:17s
epoch 69 | loss: 0.2569  | val_0_rmse: 0.49147 | val_1_rmse: 0.50334 |  0:02:19s
epoch 70 | loss: 0.25261 | val_0_rmse: 0.53819 | val_1_rmse: 0.54778 |  0:02:21s
epoch 71 | loss: 0.25719 | val_0_rmse: 0.52159 | val_1_rmse: 0.53321 |  0:02:23s
epoch 72 | loss: 0.25165 | val_0_rmse: 0.49182 | val_1_rmse: 0.50578 |  0:02:25s
epoch 73 | loss: 0.24918 | val_0_rmse: 0.50172 | val_1_rmse: 0.51595 |  0:02:26s
epoch 74 | loss: 0.25228 | val_0_rmse: 0.5174  | val_1_rmse: 0.52738 |  0:02:29s
epoch 75 | loss: 0.24959 | val_0_rmse: 0.56528 | val_1_rmse: 0.57458 |  0:02:30s
epoch 76 | loss: 0.24967 | val_0_rmse: 0.55583 | val_1_rmse: 0.56731 |  0:02:32s
epoch 77 | loss: 0.25092 | val_0_rmse: 0.49541 | val_1_rmse: 0.50795 |  0:02:34s
epoch 78 | loss: 0.24974 | val_0_rmse: 0.49986 | val_1_rmse: 0.512   |  0:02:36s
epoch 79 | loss: 0.24846 | val_0_rmse: 0.56828 | val_1_rmse: 0.57886 |  0:02:38s
epoch 80 | loss: 0.2489  | val_0_rmse: 0.50653 | val_1_rmse: 0.52071 |  0:02:40s
epoch 81 | loss: 0.25138 | val_0_rmse: 0.55402 | val_1_rmse: 0.56754 |  0:02:42s
epoch 82 | loss: 0.25231 | val_0_rmse: 0.50014 | val_1_rmse: 0.51237 |  0:02:44s
epoch 83 | loss: 0.24693 | val_0_rmse: 0.56021 | val_1_rmse: 0.57118 |  0:02:46s
epoch 84 | loss: 0.25433 | val_0_rmse: 0.50839 | val_1_rmse: 0.52298 |  0:02:48s
epoch 85 | loss: 0.25011 | val_0_rmse: 0.51226 | val_1_rmse: 0.52778 |  0:02:50s
epoch 86 | loss: 0.25093 | val_0_rmse: 0.53594 | val_1_rmse: 0.54507 |  0:02:52s
epoch 87 | loss: 0.24609 | val_0_rmse: 0.50393 | val_1_rmse: 0.51775 |  0:02:54s
epoch 88 | loss: 0.24882 | val_0_rmse: 0.49894 | val_1_rmse: 0.52009 |  0:02:56s
epoch 89 | loss: 0.24656 | val_0_rmse: 0.49654 | val_1_rmse: 0.51094 |  0:02:58s
epoch 90 | loss: 0.2502  | val_0_rmse: 0.48882 | val_1_rmse: 0.50552 |  0:03:00s
epoch 91 | loss: 0.24858 | val_0_rmse: 0.59156 | val_1_rmse: 0.60059 |  0:03:02s
epoch 92 | loss: 0.2463  | val_0_rmse: 0.51335 | val_1_rmse: 0.52536 |  0:03:04s
epoch 93 | loss: 0.24627 | val_0_rmse: 0.49572 | val_1_rmse: 0.51352 |  0:03:06s
epoch 94 | loss: 0.24502 | val_0_rmse: 0.53679 | val_1_rmse: 0.5479  |  0:03:08s
epoch 95 | loss: 0.24738 | val_0_rmse: 0.49857 | val_1_rmse: 0.51422 |  0:03:10s
epoch 96 | loss: 0.24574 | val_0_rmse: 0.50809 | val_1_rmse: 0.52273 |  0:03:12s
epoch 97 | loss: 0.24537 | val_0_rmse: 0.49862 | val_1_rmse: 0.51446 |  0:03:14s
epoch 98 | loss: 0.24619 | val_0_rmse: 0.49055 | val_1_rmse: 0.50983 |  0:03:16s
epoch 99 | loss: 0.24557 | val_0_rmse: 0.60419 | val_1_rmse: 0.61176 |  0:03:18s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.50334
Best weights from best epoch are automatically used!
ended training at: 07:02:55
Feature importance:
[('Area', 0.3768701183759434), ('Baths', 0.13941914388866578), ('Beds', 0.09850097319748374), ('Latitude', 0.13092270461283836), ('Longitude', 0.17047860289578526), ('Month', 0.010071340334993125), ('Year', 0.07373711669429031)]
Mean squared error is of 974845091.8538196
Mean absolute error:21390.998069422152
MAPE:0.2656465828906008
R2 score:0.7534503211976127
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:02:55
epoch 0  | loss: 0.58346 | val_0_rmse: 0.65762 | val_1_rmse: 0.65618 |  0:00:01s
epoch 1  | loss: 0.37085 | val_0_rmse: 0.60033 | val_1_rmse: 0.61042 |  0:00:03s
epoch 2  | loss: 0.34447 | val_0_rmse: 0.56207 | val_1_rmse: 0.56748 |  0:00:05s
epoch 3  | loss: 0.33273 | val_0_rmse: 0.57523 | val_1_rmse: 0.58002 |  0:00:07s
epoch 4  | loss: 0.34881 | val_0_rmse: 0.57441 | val_1_rmse: 0.57999 |  0:00:09s
epoch 5  | loss: 0.34228 | val_0_rmse: 0.56555 | val_1_rmse: 0.56731 |  0:00:11s
epoch 6  | loss: 0.32726 | val_0_rmse: 0.55223 | val_1_rmse: 0.55602 |  0:00:13s
epoch 7  | loss: 0.31429 | val_0_rmse: 0.54556 | val_1_rmse: 0.54851 |  0:00:15s
epoch 8  | loss: 0.30532 | val_0_rmse: 0.53843 | val_1_rmse: 0.54138 |  0:00:17s
epoch 9  | loss: 0.30363 | val_0_rmse: 0.53838 | val_1_rmse: 0.54056 |  0:00:19s
epoch 10 | loss: 0.30341 | val_0_rmse: 0.54852 | val_1_rmse: 0.55012 |  0:00:21s
epoch 11 | loss: 0.30184 | val_0_rmse: 0.54205 | val_1_rmse: 0.54847 |  0:00:23s
epoch 12 | loss: 0.30088 | val_0_rmse: 0.54108 | val_1_rmse: 0.5493  |  0:00:25s
epoch 13 | loss: 0.3039  | val_0_rmse: 0.53922 | val_1_rmse: 0.54439 |  0:00:27s
epoch 14 | loss: 0.29685 | val_0_rmse: 0.53821 | val_1_rmse: 0.54172 |  0:00:29s
epoch 15 | loss: 0.29566 | val_0_rmse: 0.54812 | val_1_rmse: 0.55094 |  0:00:31s
epoch 16 | loss: 0.29593 | val_0_rmse: 0.53406 | val_1_rmse: 0.53893 |  0:00:33s
epoch 17 | loss: 0.29364 | val_0_rmse: 0.54784 | val_1_rmse: 0.55096 |  0:00:35s
epoch 18 | loss: 0.29433 | val_0_rmse: 0.53065 | val_1_rmse: 0.53716 |  0:00:37s
epoch 19 | loss: 0.29466 | val_0_rmse: 0.53645 | val_1_rmse: 0.53838 |  0:00:39s
epoch 20 | loss: 0.29028 | val_0_rmse: 0.52616 | val_1_rmse: 0.53041 |  0:00:41s
epoch 21 | loss: 0.28794 | val_0_rmse: 0.52618 | val_1_rmse: 0.53174 |  0:00:43s
epoch 22 | loss: 0.28807 | val_0_rmse: 0.52844 | val_1_rmse: 0.53313 |  0:00:45s
epoch 23 | loss: 0.29247 | val_0_rmse: 0.52695 | val_1_rmse: 0.52988 |  0:00:47s
epoch 24 | loss: 0.28679 | val_0_rmse: 0.52129 | val_1_rmse: 0.52631 |  0:00:49s
epoch 25 | loss: 0.28122 | val_0_rmse: 0.52814 | val_1_rmse: 0.53025 |  0:00:51s
epoch 26 | loss: 0.28178 | val_0_rmse: 0.52423 | val_1_rmse: 0.53036 |  0:00:53s
epoch 27 | loss: 0.28867 | val_0_rmse: 0.55334 | val_1_rmse: 0.56048 |  0:00:55s
epoch 28 | loss: 0.28697 | val_0_rmse: 0.5313  | val_1_rmse: 0.53973 |  0:00:57s
epoch 29 | loss: 0.28137 | val_0_rmse: 0.52217 | val_1_rmse: 0.53052 |  0:00:59s
epoch 30 | loss: 0.28005 | val_0_rmse: 0.52434 | val_1_rmse: 0.52908 |  0:01:01s
epoch 31 | loss: 0.28477 | val_0_rmse: 0.52534 | val_1_rmse: 0.52884 |  0:01:03s
epoch 32 | loss: 0.2816  | val_0_rmse: 0.53116 | val_1_rmse: 0.53922 |  0:01:05s
epoch 33 | loss: 0.27918 | val_0_rmse: 0.5256  | val_1_rmse: 0.5296  |  0:01:07s
epoch 34 | loss: 0.2838  | val_0_rmse: 0.52782 | val_1_rmse: 0.53435 |  0:01:09s
epoch 35 | loss: 0.28273 | val_0_rmse: 0.54534 | val_1_rmse: 0.5543  |  0:01:11s
epoch 36 | loss: 0.28217 | val_0_rmse: 0.52705 | val_1_rmse: 0.5313  |  0:01:13s
epoch 37 | loss: 0.27702 | val_0_rmse: 0.52153 | val_1_rmse: 0.52839 |  0:01:15s
epoch 38 | loss: 0.27824 | val_0_rmse: 0.52234 | val_1_rmse: 0.52788 |  0:01:17s
epoch 39 | loss: 0.27215 | val_0_rmse: 0.528   | val_1_rmse: 0.52864 |  0:01:19s
epoch 40 | loss: 0.27526 | val_0_rmse: 0.55001 | val_1_rmse: 0.55083 |  0:01:21s
epoch 41 | loss: 0.27639 | val_0_rmse: 0.51476 | val_1_rmse: 0.51897 |  0:01:23s
epoch 42 | loss: 0.2701  | val_0_rmse: 0.51579 | val_1_rmse: 0.51671 |  0:01:25s
epoch 43 | loss: 0.26691 | val_0_rmse: 0.52037 | val_1_rmse: 0.52459 |  0:01:27s
epoch 44 | loss: 0.26622 | val_0_rmse: 0.50665 | val_1_rmse: 0.51118 |  0:01:29s
epoch 45 | loss: 0.26286 | val_0_rmse: 0.49955 | val_1_rmse: 0.50268 |  0:01:31s
epoch 46 | loss: 0.26327 | val_0_rmse: 0.52159 | val_1_rmse: 0.52469 |  0:01:33s
epoch 47 | loss: 0.2658  | val_0_rmse: 0.51907 | val_1_rmse: 0.52124 |  0:01:35s
epoch 48 | loss: 0.26394 | val_0_rmse: 0.54322 | val_1_rmse: 0.54553 |  0:01:37s
epoch 49 | loss: 0.26305 | val_0_rmse: 0.51076 | val_1_rmse: 0.51295 |  0:01:39s
epoch 50 | loss: 0.25382 | val_0_rmse: 0.49529 | val_1_rmse: 0.50001 |  0:01:41s
epoch 51 | loss: 0.25865 | val_0_rmse: 0.55029 | val_1_rmse: 0.55443 |  0:01:43s
epoch 52 | loss: 0.27844 | val_0_rmse: 0.52794 | val_1_rmse: 0.53118 |  0:01:45s
epoch 53 | loss: 0.27534 | val_0_rmse: 0.5157  | val_1_rmse: 0.51637 |  0:01:47s
epoch 54 | loss: 0.26686 | val_0_rmse: 0.52501 | val_1_rmse: 0.52554 |  0:01:49s
epoch 55 | loss: 0.2609  | val_0_rmse: 0.51158 | val_1_rmse: 0.51243 |  0:01:51s
epoch 56 | loss: 0.26118 | val_0_rmse: 0.5012  | val_1_rmse: 0.50331 |  0:01:53s
epoch 57 | loss: 0.25771 | val_0_rmse: 0.4987  | val_1_rmse: 0.50112 |  0:01:55s
epoch 58 | loss: 0.26048 | val_0_rmse: 0.51939 | val_1_rmse: 0.52146 |  0:01:57s
epoch 59 | loss: 0.25598 | val_0_rmse: 0.5166  | val_1_rmse: 0.5189  |  0:01:59s
epoch 60 | loss: 0.25711 | val_0_rmse: 0.50865 | val_1_rmse: 0.50881 |  0:02:01s
epoch 61 | loss: 0.25865 | val_0_rmse: 0.516   | val_1_rmse: 0.51615 |  0:02:03s
epoch 62 | loss: 0.25473 | val_0_rmse: 0.5008  | val_1_rmse: 0.50423 |  0:02:05s
epoch 63 | loss: 0.25466 | val_0_rmse: 0.49957 | val_1_rmse: 0.5016  |  0:02:07s
epoch 64 | loss: 0.25134 | val_0_rmse: 0.49912 | val_1_rmse: 0.50131 |  0:02:09s
epoch 65 | loss: 0.25984 | val_0_rmse: 0.51793 | val_1_rmse: 0.51784 |  0:02:11s
epoch 66 | loss: 0.25674 | val_0_rmse: 0.50293 | val_1_rmse: 0.50635 |  0:02:13s
epoch 67 | loss: 0.25447 | val_0_rmse: 0.57954 | val_1_rmse: 0.58236 |  0:02:15s
epoch 68 | loss: 0.25091 | val_0_rmse: 0.5642  | val_1_rmse: 0.57225 |  0:02:17s
epoch 69 | loss: 0.25664 | val_0_rmse: 0.49718 | val_1_rmse: 0.50029 |  0:02:19s
epoch 70 | loss: 0.2535  | val_0_rmse: 0.52006 | val_1_rmse: 0.52364 |  0:02:21s
epoch 71 | loss: 0.25202 | val_0_rmse: 0.4878  | val_1_rmse: 0.49205 |  0:02:22s
epoch 72 | loss: 0.25066 | val_0_rmse: 0.49372 | val_1_rmse: 0.4975  |  0:02:25s
epoch 73 | loss: 0.24838 | val_0_rmse: 0.49014 | val_1_rmse: 0.49517 |  0:02:26s
epoch 74 | loss: 0.25249 | val_0_rmse: 0.53309 | val_1_rmse: 0.53593 |  0:02:28s
epoch 75 | loss: 0.24998 | val_0_rmse: 0.49804 | val_1_rmse: 0.50093 |  0:02:30s
epoch 76 | loss: 0.24802 | val_0_rmse: 0.49193 | val_1_rmse: 0.49515 |  0:02:32s
epoch 77 | loss: 0.25206 | val_0_rmse: 0.54434 | val_1_rmse: 0.54562 |  0:02:34s
epoch 78 | loss: 0.24935 | val_0_rmse: 0.50452 | val_1_rmse: 0.50791 |  0:02:36s
epoch 79 | loss: 0.2506  | val_0_rmse: 0.49556 | val_1_rmse: 0.49824 |  0:02:38s
epoch 80 | loss: 0.2487  | val_0_rmse: 0.50939 | val_1_rmse: 0.51098 |  0:02:40s
epoch 81 | loss: 0.24674 | val_0_rmse: 0.5147  | val_1_rmse: 0.51734 |  0:02:42s
epoch 82 | loss: 0.24899 | val_0_rmse: 0.51504 | val_1_rmse: 0.51843 |  0:02:44s
epoch 83 | loss: 0.25413 | val_0_rmse: 0.54006 | val_1_rmse: 0.54483 |  0:02:46s
epoch 84 | loss: 0.24991 | val_0_rmse: 0.4942  | val_1_rmse: 0.49526 |  0:02:48s
epoch 85 | loss: 0.24856 | val_0_rmse: 0.51982 | val_1_rmse: 0.52283 |  0:02:50s
epoch 86 | loss: 0.25094 | val_0_rmse: 0.5023  | val_1_rmse: 0.50578 |  0:02:52s
epoch 87 | loss: 0.24788 | val_0_rmse: 0.50701 | val_1_rmse: 0.50838 |  0:02:54s
epoch 88 | loss: 0.25057 | val_0_rmse: 0.51627 | val_1_rmse: 0.52102 |  0:02:56s
epoch 89 | loss: 0.24757 | val_0_rmse: 0.51381 | val_1_rmse: 0.51591 |  0:02:58s
epoch 90 | loss: 0.24982 | val_0_rmse: 0.5067  | val_1_rmse: 0.50944 |  0:03:00s
epoch 91 | loss: 0.24708 | val_0_rmse: 0.49135 | val_1_rmse: 0.49551 |  0:03:02s
epoch 92 | loss: 0.24591 | val_0_rmse: 0.5028  | val_1_rmse: 0.50387 |  0:03:04s
epoch 93 | loss: 0.24662 | val_0_rmse: 0.50063 | val_1_rmse: 0.50397 |  0:03:06s
epoch 94 | loss: 0.25555 | val_0_rmse: 0.49751 | val_1_rmse: 0.50035 |  0:03:08s
epoch 95 | loss: 0.25096 | val_0_rmse: 0.54953 | val_1_rmse: 0.55054 |  0:03:10s
epoch 96 | loss: 0.25016 | val_0_rmse: 0.49797 | val_1_rmse: 0.50508 |  0:03:12s
epoch 97 | loss: 0.25321 | val_0_rmse: 0.50091 | val_1_rmse: 0.50664 |  0:03:14s
epoch 98 | loss: 0.24863 | val_0_rmse: 0.49699 | val_1_rmse: 0.50213 |  0:03:16s
epoch 99 | loss: 0.24746 | val_0_rmse: 0.49114 | val_1_rmse: 0.4942  |  0:03:18s
epoch 100| loss: 0.24482 | val_0_rmse: 0.48791 | val_1_rmse: 0.4937  |  0:03:20s
epoch 101| loss: 0.24603 | val_0_rmse: 0.49928 | val_1_rmse: 0.50235 |  0:03:22s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.49205
Best weights from best epoch are automatically used!
ended training at: 07:06:18
Feature importance:
[('Area', 0.30243336371873086), ('Baths', 0.13326363318025974), ('Beds', 0.17331650370304072), ('Latitude', 0.0), ('Longitude', 0.3165011532617954), ('Month', 0.009371227003906772), ('Year', 0.06511411913226649)]
Mean squared error is of 964239591.4742136
Mean absolute error:21006.712016149617
MAPE:0.2598867051166138
R2 score:0.7564404347263906
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:06:19
epoch 0  | loss: 0.55228 | val_0_rmse: 0.63602 | val_1_rmse: 0.6452  |  0:00:02s
epoch 1  | loss: 0.34789 | val_0_rmse: 0.65677 | val_1_rmse: 0.66327 |  0:00:03s
epoch 2  | loss: 0.33324 | val_0_rmse: 0.5676  | val_1_rmse: 0.57485 |  0:00:05s
epoch 3  | loss: 0.32642 | val_0_rmse: 0.56076 | val_1_rmse: 0.56198 |  0:00:08s
epoch 4  | loss: 0.31843 | val_0_rmse: 0.55843 | val_1_rmse: 0.56268 |  0:00:09s
epoch 5  | loss: 0.30559 | val_0_rmse: 0.54568 | val_1_rmse: 0.54799 |  0:00:11s
epoch 6  | loss: 0.2948  | val_0_rmse: 0.53445 | val_1_rmse: 0.53813 |  0:00:13s
epoch 7  | loss: 0.29746 | val_0_rmse: 0.52988 | val_1_rmse: 0.5326  |  0:00:15s
epoch 8  | loss: 0.29274 | val_0_rmse: 0.52853 | val_1_rmse: 0.53343 |  0:00:17s
epoch 9  | loss: 0.29181 | val_0_rmse: 0.53537 | val_1_rmse: 0.538   |  0:00:19s
epoch 10 | loss: 0.28942 | val_0_rmse: 0.53816 | val_1_rmse: 0.54089 |  0:00:21s
epoch 11 | loss: 0.29064 | val_0_rmse: 0.53289 | val_1_rmse: 0.53875 |  0:00:23s
epoch 12 | loss: 0.29092 | val_0_rmse: 0.53294 | val_1_rmse: 0.53252 |  0:00:25s
epoch 13 | loss: 0.29133 | val_0_rmse: 0.53712 | val_1_rmse: 0.54499 |  0:00:27s
epoch 14 | loss: 0.28812 | val_0_rmse: 0.53279 | val_1_rmse: 0.53785 |  0:00:29s
epoch 15 | loss: 0.28708 | val_0_rmse: 0.52239 | val_1_rmse: 0.52814 |  0:00:31s
epoch 16 | loss: 0.2843  | val_0_rmse: 0.52825 | val_1_rmse: 0.53307 |  0:00:33s
epoch 17 | loss: 0.28423 | val_0_rmse: 0.52204 | val_1_rmse: 0.52629 |  0:00:35s
epoch 18 | loss: 0.28049 | val_0_rmse: 0.51847 | val_1_rmse: 0.52412 |  0:00:37s
epoch 19 | loss: 0.28105 | val_0_rmse: 0.54074 | val_1_rmse: 0.54132 |  0:00:39s
epoch 20 | loss: 0.28582 | val_0_rmse: 0.52029 | val_1_rmse: 0.52481 |  0:00:41s
epoch 21 | loss: 0.28636 | val_0_rmse: 0.52093 | val_1_rmse: 0.52727 |  0:00:43s
epoch 22 | loss: 0.28183 | val_0_rmse: 0.52716 | val_1_rmse: 0.53237 |  0:00:45s
epoch 23 | loss: 0.28974 | val_0_rmse: 0.53385 | val_1_rmse: 0.54242 |  0:00:47s
epoch 24 | loss: 0.28893 | val_0_rmse: 0.52241 | val_1_rmse: 0.52754 |  0:00:49s
epoch 25 | loss: 0.28235 | val_0_rmse: 0.53439 | val_1_rmse: 0.53501 |  0:00:51s
epoch 26 | loss: 0.27922 | val_0_rmse: 0.57082 | val_1_rmse: 0.57005 |  0:00:53s
epoch 27 | loss: 0.27736 | val_0_rmse: 0.52098 | val_1_rmse: 0.52664 |  0:00:55s
epoch 28 | loss: 0.27679 | val_0_rmse: 0.51999 | val_1_rmse: 0.52621 |  0:00:57s
epoch 29 | loss: 0.27533 | val_0_rmse: 0.52047 | val_1_rmse: 0.52155 |  0:00:59s
epoch 30 | loss: 0.27291 | val_0_rmse: 0.52304 | val_1_rmse: 0.52546 |  0:01:01s
epoch 31 | loss: 0.27469 | val_0_rmse: 0.51278 | val_1_rmse: 0.51864 |  0:01:03s
epoch 32 | loss: 0.27517 | val_0_rmse: 0.5674  | val_1_rmse: 0.58029 |  0:01:05s
epoch 33 | loss: 0.27755 | val_0_rmse: 0.56387 | val_1_rmse: 0.56666 |  0:01:07s
epoch 34 | loss: 0.27492 | val_0_rmse: 0.52322 | val_1_rmse: 0.52485 |  0:01:09s
epoch 35 | loss: 0.27513 | val_0_rmse: 0.52283 | val_1_rmse: 0.52269 |  0:01:11s
epoch 36 | loss: 0.27138 | val_0_rmse: 0.52671 | val_1_rmse: 0.53283 |  0:01:13s
epoch 37 | loss: 0.27078 | val_0_rmse: 0.57315 | val_1_rmse: 0.57229 |  0:01:15s
epoch 38 | loss: 0.26982 | val_0_rmse: 0.51211 | val_1_rmse: 0.51991 |  0:01:17s
epoch 39 | loss: 0.27001 | val_0_rmse: 0.50477 | val_1_rmse: 0.51214 |  0:01:19s
epoch 40 | loss: 0.26752 | val_0_rmse: 0.55197 | val_1_rmse: 0.5563  |  0:01:21s
epoch 41 | loss: 0.26936 | val_0_rmse: 0.50849 | val_1_rmse: 0.51368 |  0:01:23s
epoch 42 | loss: 0.27188 | val_0_rmse: 0.51667 | val_1_rmse: 0.52743 |  0:01:25s
epoch 43 | loss: 0.26589 | val_0_rmse: 0.51162 | val_1_rmse: 0.51576 |  0:01:27s
epoch 44 | loss: 0.26243 | val_0_rmse: 0.51143 | val_1_rmse: 0.51452 |  0:01:29s
epoch 45 | loss: 0.26884 | val_0_rmse: 0.51026 | val_1_rmse: 0.51521 |  0:01:31s
epoch 46 | loss: 0.26151 | val_0_rmse: 0.512   | val_1_rmse: 0.51131 |  0:01:33s
epoch 47 | loss: 0.26138 | val_0_rmse: 0.51755 | val_1_rmse: 0.51975 |  0:01:35s
epoch 48 | loss: 0.26547 | val_0_rmse: 0.51742 | val_1_rmse: 0.5278  |  0:01:37s
epoch 49 | loss: 0.26294 | val_0_rmse: 0.50998 | val_1_rmse: 0.51145 |  0:01:39s
epoch 50 | loss: 0.2606  | val_0_rmse: 0.52512 | val_1_rmse: 0.52816 |  0:01:41s
epoch 51 | loss: 0.25895 | val_0_rmse: 0.50362 | val_1_rmse: 0.50678 |  0:01:43s
epoch 52 | loss: 0.26161 | val_0_rmse: 0.50015 | val_1_rmse: 0.50362 |  0:01:45s
epoch 53 | loss: 0.26167 | val_0_rmse: 0.5307  | val_1_rmse: 0.53836 |  0:01:47s
epoch 54 | loss: 0.25791 | val_0_rmse: 0.53266 | val_1_rmse: 0.53102 |  0:01:49s
epoch 55 | loss: 0.25511 | val_0_rmse: 0.50388 | val_1_rmse: 0.50491 |  0:01:51s
epoch 56 | loss: 0.25739 | val_0_rmse: 0.53003 | val_1_rmse: 0.53201 |  0:01:53s
epoch 57 | loss: 0.25344 | val_0_rmse: 0.55222 | val_1_rmse: 0.55046 |  0:01:55s
epoch 58 | loss: 0.25555 | val_0_rmse: 0.50908 | val_1_rmse: 0.50783 |  0:01:57s
epoch 59 | loss: 0.25076 | val_0_rmse: 0.53254 | val_1_rmse: 0.54074 |  0:01:59s
epoch 60 | loss: 0.25493 | val_0_rmse: 0.53689 | val_1_rmse: 0.53469 |  0:02:01s
epoch 61 | loss: 0.25539 | val_0_rmse: 0.49933 | val_1_rmse: 0.5019  |  0:02:03s
epoch 62 | loss: 0.25417 | val_0_rmse: 0.51244 | val_1_rmse: 0.51682 |  0:02:05s
epoch 63 | loss: 0.2506  | val_0_rmse: 0.48785 | val_1_rmse: 0.49238 |  0:02:07s
epoch 64 | loss: 0.25353 | val_0_rmse: 0.49263 | val_1_rmse: 0.49705 |  0:02:09s
epoch 65 | loss: 0.24824 | val_0_rmse: 0.50882 | val_1_rmse: 0.51117 |  0:02:11s
epoch 66 | loss: 0.2581  | val_0_rmse: 0.57119 | val_1_rmse: 0.56989 |  0:02:13s
epoch 67 | loss: 0.25496 | val_0_rmse: 0.49824 | val_1_rmse: 0.50315 |  0:02:15s
epoch 68 | loss: 0.2515  | val_0_rmse: 0.59279 | val_1_rmse: 0.59118 |  0:02:17s
epoch 69 | loss: 0.25047 | val_0_rmse: 0.50341 | val_1_rmse: 0.50574 |  0:02:19s
epoch 70 | loss: 0.24791 | val_0_rmse: 0.49769 | val_1_rmse: 0.49708 |  0:02:21s
epoch 71 | loss: 0.25159 | val_0_rmse: 0.50808 | val_1_rmse: 0.5152  |  0:02:22s
epoch 72 | loss: 0.24892 | val_0_rmse: 0.51563 | val_1_rmse: 0.51653 |  0:02:25s
epoch 73 | loss: 0.24851 | val_0_rmse: 0.49544 | val_1_rmse: 0.49457 |  0:02:26s
epoch 74 | loss: 0.24753 | val_0_rmse: 0.49099 | val_1_rmse: 0.49166 |  0:02:28s
epoch 75 | loss: 0.24879 | val_0_rmse: 0.49655 | val_1_rmse: 0.50189 |  0:02:30s
epoch 76 | loss: 0.2507  | val_0_rmse: 0.49205 | val_1_rmse: 0.49477 |  0:02:32s
epoch 77 | loss: 0.24767 | val_0_rmse: 0.57373 | val_1_rmse: 0.57237 |  0:02:34s
epoch 78 | loss: 0.24578 | val_0_rmse: 0.51457 | val_1_rmse: 0.5128  |  0:02:36s
epoch 79 | loss: 0.24562 | val_0_rmse: 0.5319  | val_1_rmse: 0.53236 |  0:02:38s
epoch 80 | loss: 0.24812 | val_0_rmse: 0.49729 | val_1_rmse: 0.49668 |  0:02:40s
epoch 81 | loss: 0.25081 | val_0_rmse: 0.55904 | val_1_rmse: 0.56143 |  0:02:42s
epoch 82 | loss: 0.24255 | val_0_rmse: 0.52185 | val_1_rmse: 0.52881 |  0:02:44s
epoch 83 | loss: 0.24564 | val_0_rmse: 0.48758 | val_1_rmse: 0.49456 |  0:02:46s
epoch 84 | loss: 0.24398 | val_0_rmse: 0.51717 | val_1_rmse: 0.52174 |  0:02:48s
epoch 85 | loss: 0.24677 | val_0_rmse: 0.49066 | val_1_rmse: 0.49656 |  0:02:50s
epoch 86 | loss: 0.24408 | val_0_rmse: 0.47811 | val_1_rmse: 0.48401 |  0:02:52s
epoch 87 | loss: 0.2439  | val_0_rmse: 0.48578 | val_1_rmse: 0.48936 |  0:02:54s
epoch 88 | loss: 0.2413  | val_0_rmse: 0.49332 | val_1_rmse: 0.49701 |  0:02:56s
epoch 89 | loss: 0.24652 | val_0_rmse: 0.50139 | val_1_rmse: 0.50708 |  0:02:58s
epoch 90 | loss: 0.24519 | val_0_rmse: 0.50596 | val_1_rmse: 0.5075  |  0:03:00s
epoch 91 | loss: 0.24702 | val_0_rmse: 0.50158 | val_1_rmse: 0.50455 |  0:03:02s
epoch 92 | loss: 0.24529 | val_0_rmse: 0.48005 | val_1_rmse: 0.48727 |  0:03:04s
epoch 93 | loss: 0.24478 | val_0_rmse: 0.5084  | val_1_rmse: 0.51467 |  0:03:06s
epoch 94 | loss: 0.24134 | val_0_rmse: 0.51663 | val_1_rmse: 0.5238  |  0:03:08s
epoch 95 | loss: 0.2413  | val_0_rmse: 0.489   | val_1_rmse: 0.49351 |  0:03:10s
epoch 96 | loss: 0.24184 | val_0_rmse: 0.48345 | val_1_rmse: 0.49267 |  0:03:12s
epoch 97 | loss: 0.24393 | val_0_rmse: 0.51171 | val_1_rmse: 0.51854 |  0:03:14s
epoch 98 | loss: 0.24231 | val_0_rmse: 0.4849  | val_1_rmse: 0.49183 |  0:03:16s
epoch 99 | loss: 0.23747 | val_0_rmse: 0.48545 | val_1_rmse: 0.49222 |  0:03:18s
epoch 100| loss: 0.24293 | val_0_rmse: 0.50133 | val_1_rmse: 0.50503 |  0:03:20s
epoch 101| loss: 0.24158 | val_0_rmse: 0.47795 | val_1_rmse: 0.49039 |  0:03:22s
epoch 102| loss: 0.23965 | val_0_rmse: 0.4834  | val_1_rmse: 0.48873 |  0:03:24s
epoch 103| loss: 0.24419 | val_0_rmse: 0.48814 | val_1_rmse: 0.49274 |  0:03:26s
epoch 104| loss: 0.2404  | val_0_rmse: 0.47732 | val_1_rmse: 0.48746 |  0:03:28s
epoch 105| loss: 0.24212 | val_0_rmse: 0.48926 | val_1_rmse: 0.49817 |  0:03:30s
epoch 106| loss: 0.23792 | val_0_rmse: 0.48003 | val_1_rmse: 0.48878 |  0:03:32s
epoch 107| loss: 0.23744 | val_0_rmse: 0.47945 | val_1_rmse: 0.4917  |  0:03:34s
epoch 108| loss: 0.24358 | val_0_rmse: 0.55824 | val_1_rmse: 0.56418 |  0:03:36s
epoch 109| loss: 0.24255 | val_0_rmse: 0.48104 | val_1_rmse: 0.4923  |  0:03:38s
epoch 110| loss: 0.24381 | val_0_rmse: 0.50082 | val_1_rmse: 0.50472 |  0:03:40s
epoch 111| loss: 0.23954 | val_0_rmse: 0.53194 | val_1_rmse: 0.53676 |  0:03:42s
epoch 112| loss: 0.23985 | val_0_rmse: 0.52936 | val_1_rmse: 0.5371  |  0:03:44s
epoch 113| loss: 0.23569 | val_0_rmse: 0.47546 | val_1_rmse: 0.48536 |  0:03:46s
epoch 114| loss: 0.23738 | val_0_rmse: 0.51365 | val_1_rmse: 0.52224 |  0:03:48s
epoch 115| loss: 0.24279 | val_0_rmse: 0.482   | val_1_rmse: 0.48961 |  0:03:50s
epoch 116| loss: 0.24342 | val_0_rmse: 0.50585 | val_1_rmse: 0.5117  |  0:03:52s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.48401
Best weights from best epoch are automatically used!
ended training at: 07:10:12
Feature importance:
[('Area', 0.28471258365066954), ('Baths', 0.25981183339854136), ('Beds', 0.09900372343148077), ('Latitude', 0.0007755557930896193), ('Longitude', 0.35149107628305176), ('Month', 0.0), ('Year', 0.004205227443166936)]
Mean squared error is of 929722640.3626155
Mean absolute error:20696.980750425042
MAPE:0.253981755977356
R2 score:0.7658607424402253
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:10:12
epoch 0  | loss: 0.43371 | val_0_rmse: 0.54279 | val_1_rmse: 0.54709 |  0:00:03s
epoch 1  | loss: 0.28072 | val_0_rmse: 0.50862 | val_1_rmse: 0.5134  |  0:00:06s
epoch 2  | loss: 0.26133 | val_0_rmse: 0.49448 | val_1_rmse: 0.49558 |  0:00:09s
epoch 3  | loss: 0.25239 | val_0_rmse: 0.47918 | val_1_rmse: 0.48208 |  0:00:12s
epoch 4  | loss: 0.24546 | val_0_rmse: 0.47315 | val_1_rmse: 0.47592 |  0:00:15s
epoch 5  | loss: 0.24017 | val_0_rmse: 0.46775 | val_1_rmse: 0.46753 |  0:00:19s
epoch 6  | loss: 0.2355  | val_0_rmse: 0.49979 | val_1_rmse: 0.50467 |  0:00:22s
epoch 7  | loss: 0.23386 | val_0_rmse: 0.46048 | val_1_rmse: 0.46341 |  0:00:25s
epoch 8  | loss: 0.22859 | val_0_rmse: 0.44819 | val_1_rmse: 0.44835 |  0:00:28s
epoch 9  | loss: 0.22599 | val_0_rmse: 0.45798 | val_1_rmse: 0.45932 |  0:00:31s
epoch 10 | loss: 0.22584 | val_0_rmse: 0.45715 | val_1_rmse: 0.46165 |  0:00:35s
epoch 11 | loss: 0.21942 | val_0_rmse: 0.44712 | val_1_rmse: 0.44824 |  0:00:38s
epoch 12 | loss: 0.22305 | val_0_rmse: 0.46079 | val_1_rmse: 0.46248 |  0:00:41s
epoch 13 | loss: 0.21975 | val_0_rmse: 0.43518 | val_1_rmse: 0.43538 |  0:00:44s
epoch 14 | loss: 0.21548 | val_0_rmse: 0.43476 | val_1_rmse: 0.43662 |  0:00:47s
epoch 15 | loss: 0.20806 | val_0_rmse: 0.44444 | val_1_rmse: 0.44319 |  0:00:51s
epoch 16 | loss: 0.20886 | val_0_rmse: 0.43958 | val_1_rmse: 0.43982 |  0:00:54s
epoch 17 | loss: 0.21297 | val_0_rmse: 0.44426 | val_1_rmse: 0.44478 |  0:00:57s
epoch 18 | loss: 0.20733 | val_0_rmse: 0.43808 | val_1_rmse: 0.4372  |  0:01:00s
epoch 19 | loss: 0.20608 | val_0_rmse: 0.44378 | val_1_rmse: 0.44802 |  0:01:03s
epoch 20 | loss: 0.20585 | val_0_rmse: 0.43261 | val_1_rmse: 0.43531 |  0:01:07s
epoch 21 | loss: 0.20912 | val_0_rmse: 0.43443 | val_1_rmse: 0.43429 |  0:01:10s
epoch 22 | loss: 0.20196 | val_0_rmse: 0.43872 | val_1_rmse: 0.43764 |  0:01:13s
epoch 23 | loss: 0.1994  | val_0_rmse: 0.4349  | val_1_rmse: 0.43761 |  0:01:16s
epoch 24 | loss: 0.20487 | val_0_rmse: 0.44131 | val_1_rmse: 0.44418 |  0:01:19s
epoch 25 | loss: 0.20299 | val_0_rmse: 0.4305  | val_1_rmse: 0.43333 |  0:01:22s
epoch 26 | loss: 0.19988 | val_0_rmse: 0.4274  | val_1_rmse: 0.43006 |  0:01:26s
epoch 27 | loss: 0.20315 | val_0_rmse: 0.4216  | val_1_rmse: 0.42444 |  0:01:29s
epoch 28 | loss: 0.19848 | val_0_rmse: 0.43285 | val_1_rmse: 0.43217 |  0:01:32s
epoch 29 | loss: 0.19563 | val_0_rmse: 0.42028 | val_1_rmse: 0.42125 |  0:01:35s
epoch 30 | loss: 0.2012  | val_0_rmse: 0.44221 | val_1_rmse: 0.44159 |  0:01:38s
epoch 31 | loss: 0.19722 | val_0_rmse: 0.4281  | val_1_rmse: 0.43062 |  0:01:42s
epoch 32 | loss: 0.1957  | val_0_rmse: 0.42059 | val_1_rmse: 0.42076 |  0:01:45s
epoch 33 | loss: 0.19897 | val_0_rmse: 0.42951 | val_1_rmse: 0.43057 |  0:01:48s
epoch 34 | loss: 0.19639 | val_0_rmse: 0.44167 | val_1_rmse: 0.44427 |  0:01:51s
epoch 35 | loss: 0.19372 | val_0_rmse: 0.42351 | val_1_rmse: 0.4251  |  0:01:54s
epoch 36 | loss: 0.19345 | val_0_rmse: 0.42142 | val_1_rmse: 0.42467 |  0:01:57s
epoch 37 | loss: 0.19888 | val_0_rmse: 0.41357 | val_1_rmse: 0.4156  |  0:02:01s
epoch 38 | loss: 0.19313 | val_0_rmse: 0.44837 | val_1_rmse: 0.45176 |  0:02:04s
epoch 39 | loss: 0.19781 | val_0_rmse: 0.43662 | val_1_rmse: 0.4396  |  0:02:07s
epoch 40 | loss: 0.19744 | val_0_rmse: 0.41447 | val_1_rmse: 0.41715 |  0:02:10s
epoch 41 | loss: 0.18968 | val_0_rmse: 0.42101 | val_1_rmse: 0.42477 |  0:02:13s
epoch 42 | loss: 0.19513 | val_0_rmse: 0.46652 | val_1_rmse: 0.46708 |  0:02:17s
epoch 43 | loss: 0.19522 | val_0_rmse: 0.44826 | val_1_rmse: 0.44738 |  0:02:20s
epoch 44 | loss: 0.19332 | val_0_rmse: 0.4223  | val_1_rmse: 0.42445 |  0:02:23s
epoch 45 | loss: 0.19312 | val_0_rmse: 0.42208 | val_1_rmse: 0.42377 |  0:02:26s
epoch 46 | loss: 0.18947 | val_0_rmse: 0.42327 | val_1_rmse: 0.42577 |  0:02:29s
epoch 47 | loss: 0.19308 | val_0_rmse: 0.42396 | val_1_rmse: 0.42568 |  0:02:32s
epoch 48 | loss: 0.19424 | val_0_rmse: 0.42253 | val_1_rmse: 0.424   |  0:02:35s
epoch 49 | loss: 0.18821 | val_0_rmse: 0.41264 | val_1_rmse: 0.41667 |  0:02:39s
epoch 50 | loss: 0.18827 | val_0_rmse: 0.42104 | val_1_rmse: 0.42508 |  0:02:42s
epoch 51 | loss: 0.19098 | val_0_rmse: 0.429   | val_1_rmse: 0.43177 |  0:02:45s
epoch 52 | loss: 0.19075 | val_0_rmse: 0.41437 | val_1_rmse: 0.41853 |  0:02:48s
epoch 53 | loss: 0.19047 | val_0_rmse: 0.41475 | val_1_rmse: 0.41696 |  0:02:51s
epoch 54 | loss: 0.18956 | val_0_rmse: 0.41676 | val_1_rmse: 0.41961 |  0:02:54s
epoch 55 | loss: 0.18974 | val_0_rmse: 0.42011 | val_1_rmse: 0.42386 |  0:02:58s
epoch 56 | loss: 0.1941  | val_0_rmse: 0.41288 | val_1_rmse: 0.4168  |  0:03:01s
epoch 57 | loss: 0.18741 | val_0_rmse: 0.43185 | val_1_rmse: 0.43115 |  0:03:04s
epoch 58 | loss: 0.18825 | val_0_rmse: 0.42851 | val_1_rmse: 0.43316 |  0:03:07s
epoch 59 | loss: 0.1869  | val_0_rmse: 0.41657 | val_1_rmse: 0.41809 |  0:03:10s
epoch 60 | loss: 0.19233 | val_0_rmse: 0.44152 | val_1_rmse: 0.44358 |  0:03:14s
epoch 61 | loss: 0.18801 | val_0_rmse: 0.42489 | val_1_rmse: 0.42918 |  0:03:17s
epoch 62 | loss: 0.18599 | val_0_rmse: 0.43759 | val_1_rmse: 0.44226 |  0:03:20s
epoch 63 | loss: 0.18817 | val_0_rmse: 0.4105  | val_1_rmse: 0.41782 |  0:03:23s
epoch 64 | loss: 0.18883 | val_0_rmse: 0.42233 | val_1_rmse: 0.42293 |  0:03:26s
epoch 65 | loss: 0.18853 | val_0_rmse: 0.41074 | val_1_rmse: 0.41676 |  0:03:29s
epoch 66 | loss: 0.18698 | val_0_rmse: 0.41413 | val_1_rmse: 0.41816 |  0:03:33s
epoch 67 | loss: 0.18471 | val_0_rmse: 0.42234 | val_1_rmse: 0.42489 |  0:03:36s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.4156
Best weights from best epoch are automatically used!
ended training at: 07:13:49
Feature importance:
[('Area', 0.1489062979809923), ('Baths', 0.13661120397561774), ('Beds', 0.10916919667342782), ('Latitude', 0.1714612907122281), ('Longitude', 0.1246885291558352), ('Month', 0.02310591901221213), ('Year', 0.2860575624896867)]
Mean squared error is of 10464356810.992445
Mean absolute error:70630.39728447581
MAPE:0.30088135791406223
R2 score:0.820874897575133
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:13:50
epoch 0  | loss: 0.45764 | val_0_rmse: 0.56182 | val_1_rmse: 0.5574  |  0:00:03s
epoch 1  | loss: 0.31331 | val_0_rmse: 0.53546 | val_1_rmse: 0.53205 |  0:00:06s
epoch 2  | loss: 0.28284 | val_0_rmse: 0.4926  | val_1_rmse: 0.48733 |  0:00:09s
epoch 3  | loss: 0.25723 | val_0_rmse: 0.4882  | val_1_rmse: 0.48422 |  0:00:12s
epoch 4  | loss: 0.2517  | val_0_rmse: 0.49278 | val_1_rmse: 0.49046 |  0:00:16s
epoch 5  | loss: 0.24356 | val_0_rmse: 0.45742 | val_1_rmse: 0.45424 |  0:00:19s
epoch 6  | loss: 0.22959 | val_0_rmse: 0.45149 | val_1_rmse: 0.44799 |  0:00:22s
epoch 7  | loss: 0.23159 | val_0_rmse: 0.48483 | val_1_rmse: 0.48291 |  0:00:25s
epoch 8  | loss: 0.2285  | val_0_rmse: 0.45606 | val_1_rmse: 0.4539  |  0:00:28s
epoch 9  | loss: 0.22567 | val_0_rmse: 0.45139 | val_1_rmse: 0.45274 |  0:00:31s
epoch 10 | loss: 0.23004 | val_0_rmse: 0.45983 | val_1_rmse: 0.4601  |  0:00:35s
epoch 11 | loss: 0.22339 | val_0_rmse: 0.44352 | val_1_rmse: 0.4431  |  0:00:38s
epoch 12 | loss: 0.22181 | val_0_rmse: 0.46475 | val_1_rmse: 0.46572 |  0:00:41s
epoch 13 | loss: 0.22045 | val_0_rmse: 0.44355 | val_1_rmse: 0.44442 |  0:00:44s
epoch 14 | loss: 0.2169  | val_0_rmse: 0.51408 | val_1_rmse: 0.51412 |  0:00:47s
epoch 15 | loss: 0.21476 | val_0_rmse: 0.44701 | val_1_rmse: 0.44871 |  0:00:51s
epoch 16 | loss: 0.2116  | val_0_rmse: 0.45309 | val_1_rmse: 0.45379 |  0:00:54s
epoch 17 | loss: 0.20994 | val_0_rmse: 0.44635 | val_1_rmse: 0.44826 |  0:00:57s
epoch 18 | loss: 0.20725 | val_0_rmse: 0.45719 | val_1_rmse: 0.45995 |  0:01:00s
epoch 19 | loss: 0.21525 | val_0_rmse: 0.43203 | val_1_rmse: 0.4331  |  0:01:03s
epoch 20 | loss: 0.2099  | val_0_rmse: 0.43846 | val_1_rmse: 0.44141 |  0:01:07s
epoch 21 | loss: 0.2076  | val_0_rmse: 0.44786 | val_1_rmse: 0.45133 |  0:01:10s
epoch 22 | loss: 0.21617 | val_0_rmse: 0.44201 | val_1_rmse: 0.44431 |  0:01:13s
epoch 23 | loss: 0.21005 | val_0_rmse: 0.4357  | val_1_rmse: 0.43518 |  0:01:16s
epoch 24 | loss: 0.20345 | val_0_rmse: 0.42373 | val_1_rmse: 0.42815 |  0:01:19s
epoch 25 | loss: 0.209   | val_0_rmse: 0.43292 | val_1_rmse: 0.43564 |  0:01:23s
epoch 26 | loss: 0.20269 | val_0_rmse: 0.44507 | val_1_rmse: 0.448   |  0:01:26s
epoch 27 | loss: 0.20613 | val_0_rmse: 0.42523 | val_1_rmse: 0.43058 |  0:01:29s
epoch 28 | loss: 0.20566 | val_0_rmse: 0.43756 | val_1_rmse: 0.44037 |  0:01:32s
epoch 29 | loss: 0.2035  | val_0_rmse: 0.42547 | val_1_rmse: 0.42871 |  0:01:35s
epoch 30 | loss: 0.20632 | val_0_rmse: 0.42558 | val_1_rmse: 0.43168 |  0:01:38s
epoch 31 | loss: 0.2042  | val_0_rmse: 0.4735  | val_1_rmse: 0.47694 |  0:01:42s
epoch 32 | loss: 0.20067 | val_0_rmse: 0.4338  | val_1_rmse: 0.4367  |  0:01:45s
epoch 33 | loss: 0.20127 | val_0_rmse: 0.45113 | val_1_rmse: 0.45529 |  0:01:48s
epoch 34 | loss: 0.20303 | val_0_rmse: 0.43432 | val_1_rmse: 0.43915 |  0:01:51s
epoch 35 | loss: 0.20162 | val_0_rmse: 0.42221 | val_1_rmse: 0.42775 |  0:01:54s
epoch 36 | loss: 0.19873 | val_0_rmse: 0.43675 | val_1_rmse: 0.44204 |  0:01:58s
epoch 37 | loss: 0.19709 | val_0_rmse: 0.43497 | val_1_rmse: 0.43998 |  0:02:01s
epoch 38 | loss: 0.2008  | val_0_rmse: 0.43651 | val_1_rmse: 0.44024 |  0:02:04s
epoch 39 | loss: 0.20232 | val_0_rmse: 0.42258 | val_1_rmse: 0.42872 |  0:02:07s
epoch 40 | loss: 0.19979 | val_0_rmse: 0.45218 | val_1_rmse: 0.45524 |  0:02:10s
epoch 41 | loss: 0.20016 | val_0_rmse: 0.42557 | val_1_rmse: 0.43198 |  0:02:13s
epoch 42 | loss: 0.20046 | val_0_rmse: 0.42204 | val_1_rmse: 0.42765 |  0:02:17s
epoch 43 | loss: 0.19997 | val_0_rmse: 0.44011 | val_1_rmse: 0.44344 |  0:02:20s
epoch 44 | loss: 0.2017  | val_0_rmse: 0.43188 | val_1_rmse: 0.43595 |  0:02:23s
epoch 45 | loss: 0.19258 | val_0_rmse: 0.42704 | val_1_rmse: 0.43346 |  0:02:26s
epoch 46 | loss: 0.19517 | val_0_rmse: 0.43444 | val_1_rmse: 0.43803 |  0:02:30s
epoch 47 | loss: 0.19759 | val_0_rmse: 0.43215 | val_1_rmse: 0.436   |  0:02:33s
epoch 48 | loss: 0.20123 | val_0_rmse: 0.44167 | val_1_rmse: 0.44349 |  0:02:36s
epoch 49 | loss: 0.19825 | val_0_rmse: 0.42855 | val_1_rmse: 0.43473 |  0:02:39s
epoch 50 | loss: 0.19669 | val_0_rmse: 0.4297  | val_1_rmse: 0.43525 |  0:02:42s
epoch 51 | loss: 0.20312 | val_0_rmse: 0.43405 | val_1_rmse: 0.43526 |  0:02:45s
epoch 52 | loss: 0.19937 | val_0_rmse: 0.42578 | val_1_rmse: 0.42953 |  0:02:49s
epoch 53 | loss: 0.19773 | val_0_rmse: 0.42914 | val_1_rmse: 0.43398 |  0:02:52s
epoch 54 | loss: 0.19658 | val_0_rmse: 0.42049 | val_1_rmse: 0.42586 |  0:02:55s
epoch 55 | loss: 0.19749 | val_0_rmse: 0.43336 | val_1_rmse: 0.43746 |  0:02:58s
epoch 56 | loss: 0.19219 | val_0_rmse: 0.43443 | val_1_rmse: 0.44121 |  0:03:01s
epoch 57 | loss: 0.19123 | val_0_rmse: 0.4318  | val_1_rmse: 0.43768 |  0:03:05s
epoch 58 | loss: 0.19832 | val_0_rmse: 0.42139 | val_1_rmse: 0.42282 |  0:03:08s
epoch 59 | loss: 0.20182 | val_0_rmse: 0.42373 | val_1_rmse: 0.42781 |  0:03:11s
epoch 60 | loss: 0.19549 | val_0_rmse: 0.45462 | val_1_rmse: 0.4604  |  0:03:14s
epoch 61 | loss: 0.19633 | val_0_rmse: 0.44736 | val_1_rmse: 0.45322 |  0:03:17s
epoch 62 | loss: 0.19211 | val_0_rmse: 0.45308 | val_1_rmse: 0.45794 |  0:03:21s
epoch 63 | loss: 0.1969  | val_0_rmse: 0.41879 | val_1_rmse: 0.42471 |  0:03:24s
epoch 64 | loss: 0.18839 | val_0_rmse: 0.43152 | val_1_rmse: 0.4357  |  0:03:27s
epoch 65 | loss: 0.19609 | val_0_rmse: 0.42289 | val_1_rmse: 0.42885 |  0:03:30s
epoch 66 | loss: 0.19305 | val_0_rmse: 0.41766 | val_1_rmse: 0.4217  |  0:03:34s
epoch 67 | loss: 0.19857 | val_0_rmse: 0.45687 | val_1_rmse: 0.45988 |  0:03:37s
epoch 68 | loss: 0.19979 | val_0_rmse: 0.43018 | val_1_rmse: 0.43164 |  0:03:40s
epoch 69 | loss: 0.19573 | val_0_rmse: 0.42073 | val_1_rmse: 0.42539 |  0:03:43s
epoch 70 | loss: 0.19068 | val_0_rmse: 0.42688 | val_1_rmse: 0.43151 |  0:03:46s
epoch 71 | loss: 0.20043 | val_0_rmse: 0.44417 | val_1_rmse: 0.44625 |  0:03:49s
epoch 72 | loss: 0.19497 | val_0_rmse: 0.4194  | val_1_rmse: 0.42413 |  0:03:53s
epoch 73 | loss: 0.19356 | val_0_rmse: 0.46439 | val_1_rmse: 0.46922 |  0:03:56s
epoch 74 | loss: 0.19267 | val_0_rmse: 0.42361 | val_1_rmse: 0.42821 |  0:03:59s
epoch 75 | loss: 0.19581 | val_0_rmse: 0.42351 | val_1_rmse: 0.42846 |  0:04:02s
epoch 76 | loss: 0.18777 | val_0_rmse: 0.41397 | val_1_rmse: 0.42047 |  0:04:05s
epoch 77 | loss: 0.18884 | val_0_rmse: 0.41205 | val_1_rmse: 0.41768 |  0:04:09s
epoch 78 | loss: 0.18759 | val_0_rmse: 0.41288 | val_1_rmse: 0.42014 |  0:04:12s
epoch 79 | loss: 0.18892 | val_0_rmse: 0.41268 | val_1_rmse: 0.41872 |  0:04:15s
epoch 80 | loss: 0.1868  | val_0_rmse: 0.42277 | val_1_rmse: 0.42874 |  0:04:18s
epoch 81 | loss: 0.18992 | val_0_rmse: 0.41182 | val_1_rmse: 0.41808 |  0:04:21s
epoch 82 | loss: 0.18711 | val_0_rmse: 0.43612 | val_1_rmse: 0.44211 |  0:04:24s
epoch 83 | loss: 0.19001 | val_0_rmse: 0.41931 | val_1_rmse: 0.42529 |  0:04:28s
epoch 84 | loss: 0.18654 | val_0_rmse: 0.42511 | val_1_rmse: 0.43194 |  0:04:31s
epoch 85 | loss: 0.18897 | val_0_rmse: 0.43533 | val_1_rmse: 0.43976 |  0:04:34s
epoch 86 | loss: 0.18687 | val_0_rmse: 0.4922  | val_1_rmse: 0.49668 |  0:04:37s
epoch 87 | loss: 0.19036 | val_0_rmse: 0.41146 | val_1_rmse: 0.41818 |  0:04:40s
epoch 88 | loss: 0.18608 | val_0_rmse: 0.42238 | val_1_rmse: 0.42875 |  0:04:44s
epoch 89 | loss: 0.19279 | val_0_rmse: 0.41144 | val_1_rmse: 0.41937 |  0:04:47s
epoch 90 | loss: 0.18737 | val_0_rmse: 0.4327  | val_1_rmse: 0.43743 |  0:04:50s
epoch 91 | loss: 0.19478 | val_0_rmse: 0.43032 | val_1_rmse: 0.43655 |  0:04:53s
epoch 92 | loss: 0.18933 | val_0_rmse: 0.42347 | val_1_rmse: 0.42813 |  0:04:56s
epoch 93 | loss: 0.18906 | val_0_rmse: 0.41669 | val_1_rmse: 0.42231 |  0:05:00s
epoch 94 | loss: 0.1935  | val_0_rmse: 0.41757 | val_1_rmse: 0.42242 |  0:05:03s
epoch 95 | loss: 0.19541 | val_0_rmse: 0.43823 | val_1_rmse: 0.44185 |  0:05:06s
epoch 96 | loss: 0.19227 | val_0_rmse: 0.41955 | val_1_rmse: 0.42528 |  0:05:09s
epoch 97 | loss: 0.18666 | val_0_rmse: 0.44422 | val_1_rmse: 0.44873 |  0:05:12s
epoch 98 | loss: 0.18764 | val_0_rmse: 0.41955 | val_1_rmse: 0.42741 |  0:05:15s
epoch 99 | loss: 0.18689 | val_0_rmse: 0.4176  | val_1_rmse: 0.42527 |  0:05:19s
epoch 100| loss: 0.18643 | val_0_rmse: 0.43196 | val_1_rmse: 0.44171 |  0:05:22s
epoch 101| loss: 0.18651 | val_0_rmse: 0.41139 | val_1_rmse: 0.41958 |  0:05:25s
epoch 102| loss: 0.18451 | val_0_rmse: 0.41056 | val_1_rmse: 0.41938 |  0:05:28s
epoch 103| loss: 0.18358 | val_0_rmse: 0.40639 | val_1_rmse: 0.41418 |  0:05:32s
epoch 104| loss: 0.18167 | val_0_rmse: 0.40637 | val_1_rmse: 0.41562 |  0:05:35s
epoch 105| loss: 0.18413 | val_0_rmse: 0.40907 | val_1_rmse: 0.41752 |  0:05:38s
epoch 106| loss: 0.18529 | val_0_rmse: 0.42063 | val_1_rmse: 0.42904 |  0:05:41s
epoch 107| loss: 0.18642 | val_0_rmse: 0.4121  | val_1_rmse: 0.42097 |  0:05:44s
epoch 108| loss: 0.18413 | val_0_rmse: 0.44184 | val_1_rmse: 0.45109 |  0:05:47s
epoch 109| loss: 0.18621 | val_0_rmse: 0.42221 | val_1_rmse: 0.42905 |  0:05:51s
epoch 110| loss: 0.19708 | val_0_rmse: 0.48411 | val_1_rmse: 0.48964 |  0:05:54s
epoch 111| loss: 0.19196 | val_0_rmse: 0.42408 | val_1_rmse: 0.42941 |  0:05:57s
epoch 112| loss: 0.18821 | val_0_rmse: 0.41049 | val_1_rmse: 0.41811 |  0:06:00s
epoch 113| loss: 0.18432 | val_0_rmse: 0.40464 | val_1_rmse: 0.41391 |  0:06:04s
epoch 114| loss: 0.18658 | val_0_rmse: 0.41162 | val_1_rmse: 0.41909 |  0:06:07s
epoch 115| loss: 0.18902 | val_0_rmse: 0.40753 | val_1_rmse: 0.41497 |  0:06:10s
epoch 116| loss: 0.18463 | val_0_rmse: 0.43255 | val_1_rmse: 0.43905 |  0:06:13s
epoch 117| loss: 0.19792 | val_0_rmse: 0.41161 | val_1_rmse: 0.41727 |  0:06:16s
epoch 118| loss: 0.18922 | val_0_rmse: 0.42089 | val_1_rmse: 0.42274 |  0:06:19s
epoch 119| loss: 0.18838 | val_0_rmse: 0.41837 | val_1_rmse: 0.42547 |  0:06:23s
epoch 120| loss: 0.18581 | val_0_rmse: 0.41904 | val_1_rmse: 0.42576 |  0:06:26s
epoch 121| loss: 0.18145 | val_0_rmse: 0.40726 | val_1_rmse: 0.41677 |  0:06:29s
epoch 122| loss: 0.18121 | val_0_rmse: 0.40592 | val_1_rmse: 0.41339 |  0:06:32s
epoch 123| loss: 0.18193 | val_0_rmse: 0.40825 | val_1_rmse: 0.4195  |  0:06:35s
epoch 124| loss: 0.18024 | val_0_rmse: 0.41044 | val_1_rmse: 0.41996 |  0:06:39s
epoch 125| loss: 0.18185 | val_0_rmse: 0.41032 | val_1_rmse: 0.41915 |  0:06:42s
epoch 126| loss: 0.18101 | val_0_rmse: 0.39816 | val_1_rmse: 0.40629 |  0:06:45s
epoch 127| loss: 0.17787 | val_0_rmse: 0.40319 | val_1_rmse: 0.41274 |  0:06:48s
epoch 128| loss: 0.1778  | val_0_rmse: 0.40486 | val_1_rmse: 0.41397 |  0:06:51s
epoch 129| loss: 0.17957 | val_0_rmse: 0.40389 | val_1_rmse: 0.41387 |  0:06:55s
epoch 130| loss: 0.17799 | val_0_rmse: 0.40256 | val_1_rmse: 0.41317 |  0:06:58s
epoch 131| loss: 0.17792 | val_0_rmse: 0.4153  | val_1_rmse: 0.42336 |  0:07:01s
epoch 132| loss: 0.17838 | val_0_rmse: 0.40929 | val_1_rmse: 0.41829 |  0:07:04s
epoch 133| loss: 0.17849 | val_0_rmse: 0.4067  | val_1_rmse: 0.4169  |  0:07:07s
epoch 134| loss: 0.17972 | val_0_rmse: 0.43193 | val_1_rmse: 0.43743 |  0:07:10s
epoch 135| loss: 0.18108 | val_0_rmse: 0.40795 | val_1_rmse: 0.41897 |  0:07:14s
epoch 136| loss: 0.18346 | val_0_rmse: 0.41498 | val_1_rmse: 0.427   |  0:07:17s
epoch 137| loss: 0.1788  | val_0_rmse: 0.40991 | val_1_rmse: 0.4202  |  0:07:20s
epoch 138| loss: 0.1812  | val_0_rmse: 0.42008 | val_1_rmse: 0.43363 |  0:07:23s
epoch 139| loss: 0.17672 | val_0_rmse: 0.41221 | val_1_rmse: 0.4235  |  0:07:26s
epoch 140| loss: 0.17733 | val_0_rmse: 0.41758 | val_1_rmse: 0.42909 |  0:07:30s
epoch 141| loss: 0.17663 | val_0_rmse: 0.40912 | val_1_rmse: 0.42086 |  0:07:33s
epoch 142| loss: 0.17682 | val_0_rmse: 0.41394 | val_1_rmse: 0.42411 |  0:07:36s
epoch 143| loss: 0.18189 | val_0_rmse: 0.40056 | val_1_rmse: 0.4139  |  0:07:39s
epoch 144| loss: 0.176   | val_0_rmse: 0.41724 | val_1_rmse: 0.42922 |  0:07:42s
epoch 145| loss: 0.17621 | val_0_rmse: 0.40991 | val_1_rmse: 0.42174 |  0:07:46s
epoch 146| loss: 0.17857 | val_0_rmse: 0.40699 | val_1_rmse: 0.41934 |  0:07:49s
epoch 147| loss: 0.18089 | val_0_rmse: 0.39975 | val_1_rmse: 0.41066 |  0:07:52s
epoch 148| loss: 0.17882 | val_0_rmse: 0.40363 | val_1_rmse: 0.41613 |  0:07:55s
epoch 149| loss: 0.17912 | val_0_rmse: 0.3987  | val_1_rmse: 0.41056 |  0:07:58s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.40629
Best weights from best epoch are automatically used!
ended training at: 07:21:50
Feature importance:
[('Area', 0.060819600608657036), ('Baths', 0.1282237579046144), ('Beds', 0.11943563211182692), ('Latitude', 0.07133104045009289), ('Longitude', 0.36754179209467974), ('Month', 0.0), ('Year', 0.252648176830129)]
Mean squared error is of 9641647672.75175
Mean absolute error:68309.13317018787
MAPE:0.28591330719171715
R2 score:0.8320345619544594
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:21:50
epoch 0  | loss: 0.44373 | val_0_rmse: 0.54802 | val_1_rmse: 0.55112 |  0:00:03s
epoch 1  | loss: 0.28859 | val_0_rmse: 0.51599 | val_1_rmse: 0.5203  |  0:00:06s
epoch 2  | loss: 0.26702 | val_0_rmse: 0.51309 | val_1_rmse: 0.51379 |  0:00:09s
epoch 3  | loss: 0.25559 | val_0_rmse: 0.50615 | val_1_rmse: 0.51181 |  0:00:12s
epoch 4  | loss: 0.23797 | val_0_rmse: 0.46196 | val_1_rmse: 0.46377 |  0:00:15s
epoch 5  | loss: 0.23822 | val_0_rmse: 0.45642 | val_1_rmse: 0.45969 |  0:00:19s
epoch 6  | loss: 0.22143 | val_0_rmse: 0.46099 | val_1_rmse: 0.46081 |  0:00:22s
epoch 7  | loss: 0.22381 | val_0_rmse: 0.44147 | val_1_rmse: 0.44217 |  0:00:25s
epoch 8  | loss: 0.21816 | val_0_rmse: 0.45311 | val_1_rmse: 0.4553  |  0:00:28s
epoch 9  | loss: 0.21237 | val_0_rmse: 0.43352 | val_1_rmse: 0.43345 |  0:00:31s
epoch 10 | loss: 0.20828 | val_0_rmse: 0.44487 | val_1_rmse: 0.44467 |  0:00:35s
epoch 11 | loss: 0.21012 | val_0_rmse: 0.43875 | val_1_rmse: 0.44077 |  0:00:38s
epoch 12 | loss: 0.20768 | val_0_rmse: 0.44478 | val_1_rmse: 0.45142 |  0:00:41s
epoch 13 | loss: 0.21061 | val_0_rmse: 0.43308 | val_1_rmse: 0.43601 |  0:00:44s
epoch 14 | loss: 0.20381 | val_0_rmse: 0.43035 | val_1_rmse: 0.43196 |  0:00:47s
epoch 15 | loss: 0.20337 | val_0_rmse: 0.43084 | val_1_rmse: 0.4299  |  0:00:51s
epoch 16 | loss: 0.1994  | val_0_rmse: 0.44566 | val_1_rmse: 0.45124 |  0:00:54s
epoch 17 | loss: 0.20153 | val_0_rmse: 0.42508 | val_1_rmse: 0.42792 |  0:00:57s
epoch 18 | loss: 0.20665 | val_0_rmse: 0.43366 | val_1_rmse: 0.43341 |  0:01:00s
epoch 19 | loss: 0.20187 | val_0_rmse: 0.43447 | val_1_rmse: 0.43802 |  0:01:03s
epoch 20 | loss: 0.20076 | val_0_rmse: 0.42799 | val_1_rmse: 0.43426 |  0:01:06s
epoch 21 | loss: 0.1988  | val_0_rmse: 0.43669 | val_1_rmse: 0.43975 |  0:01:10s
epoch 22 | loss: 0.19867 | val_0_rmse: 0.43238 | val_1_rmse: 0.43433 |  0:01:13s
epoch 23 | loss: 0.19585 | val_0_rmse: 0.42566 | val_1_rmse: 0.42641 |  0:01:16s
epoch 24 | loss: 0.19587 | val_0_rmse: 0.42574 | val_1_rmse: 0.42921 |  0:01:19s
epoch 25 | loss: 0.19596 | val_0_rmse: 0.4239  | val_1_rmse: 0.42673 |  0:01:22s
epoch 26 | loss: 0.19477 | val_0_rmse: 0.42862 | val_1_rmse: 0.42958 |  0:01:26s
epoch 27 | loss: 0.19947 | val_0_rmse: 0.44449 | val_1_rmse: 0.45068 |  0:01:29s
epoch 28 | loss: 0.19453 | val_0_rmse: 0.41701 | val_1_rmse: 0.41951 |  0:01:32s
epoch 29 | loss: 0.19487 | val_0_rmse: 0.424   | val_1_rmse: 0.42574 |  0:01:35s
epoch 30 | loss: 0.19641 | val_0_rmse: 0.42538 | val_1_rmse: 0.42986 |  0:01:38s
epoch 31 | loss: 0.19571 | val_0_rmse: 0.42866 | val_1_rmse: 0.4342  |  0:01:42s
epoch 32 | loss: 0.19837 | val_0_rmse: 0.42247 | val_1_rmse: 0.42648 |  0:01:45s
epoch 33 | loss: 0.19561 | val_0_rmse: 0.42538 | val_1_rmse: 0.43167 |  0:01:48s
epoch 34 | loss: 0.19212 | val_0_rmse: 0.42523 | val_1_rmse: 0.43035 |  0:01:51s
epoch 35 | loss: 0.1913  | val_0_rmse: 0.42576 | val_1_rmse: 0.42528 |  0:01:54s
epoch 36 | loss: 0.19637 | val_0_rmse: 0.44748 | val_1_rmse: 0.4503  |  0:01:58s
epoch 37 | loss: 0.19492 | val_0_rmse: 0.42082 | val_1_rmse: 0.42671 |  0:02:01s
epoch 38 | loss: 0.19203 | val_0_rmse: 0.42562 | val_1_rmse: 0.42915 |  0:02:04s
epoch 39 | loss: 0.19384 | val_0_rmse: 0.42132 | val_1_rmse: 0.4276  |  0:02:07s
epoch 40 | loss: 0.19135 | val_0_rmse: 0.44645 | val_1_rmse: 0.4488  |  0:02:10s
epoch 41 | loss: 0.19334 | val_0_rmse: 0.43988 | val_1_rmse: 0.44645 |  0:02:13s
epoch 42 | loss: 0.19703 | val_0_rmse: 0.42843 | val_1_rmse: 0.43233 |  0:02:17s
epoch 43 | loss: 0.19078 | val_0_rmse: 0.41848 | val_1_rmse: 0.42335 |  0:02:20s
epoch 44 | loss: 0.19423 | val_0_rmse: 0.42845 | val_1_rmse: 0.43499 |  0:02:23s
epoch 45 | loss: 0.18801 | val_0_rmse: 0.42068 | val_1_rmse: 0.42607 |  0:02:26s
epoch 46 | loss: 0.19012 | val_0_rmse: 0.42362 | val_1_rmse: 0.42742 |  0:02:29s
epoch 47 | loss: 0.19222 | val_0_rmse: 0.42341 | val_1_rmse: 0.4266  |  0:02:33s
epoch 48 | loss: 0.18806 | val_0_rmse: 0.41983 | val_1_rmse: 0.42362 |  0:02:36s
epoch 49 | loss: 0.18858 | val_0_rmse: 0.41426 | val_1_rmse: 0.41747 |  0:02:39s
epoch 50 | loss: 0.18828 | val_0_rmse: 0.43645 | val_1_rmse: 0.4411  |  0:02:42s
epoch 51 | loss: 0.19097 | val_0_rmse: 0.41993 | val_1_rmse: 0.42205 |  0:02:45s
epoch 52 | loss: 0.18528 | val_0_rmse: 0.41746 | val_1_rmse: 0.42269 |  0:02:49s
epoch 53 | loss: 0.18715 | val_0_rmse: 0.41724 | val_1_rmse: 0.42284 |  0:02:52s
epoch 54 | loss: 0.18877 | val_0_rmse: 0.41562 | val_1_rmse: 0.42226 |  0:02:55s
epoch 55 | loss: 0.18672 | val_0_rmse: 0.41227 | val_1_rmse: 0.41822 |  0:02:58s
epoch 56 | loss: 0.18804 | val_0_rmse: 0.41639 | val_1_rmse: 0.42263 |  0:03:01s
epoch 57 | loss: 0.18645 | val_0_rmse: 0.42097 | val_1_rmse: 0.42863 |  0:03:05s
epoch 58 | loss: 0.18673 | val_0_rmse: 0.42092 | val_1_rmse: 0.425   |  0:03:08s
epoch 59 | loss: 0.18749 | val_0_rmse: 0.4258  | val_1_rmse: 0.43267 |  0:03:11s
epoch 60 | loss: 0.18894 | val_0_rmse: 0.42044 | val_1_rmse: 0.42525 |  0:03:14s
epoch 61 | loss: 0.18614 | val_0_rmse: 0.41853 | val_1_rmse: 0.42154 |  0:03:17s
epoch 62 | loss: 0.18485 | val_0_rmse: 0.42888 | val_1_rmse: 0.43179 |  0:03:21s
epoch 63 | loss: 0.18677 | val_0_rmse: 0.42175 | val_1_rmse: 0.4266  |  0:03:24s
epoch 64 | loss: 0.18453 | val_0_rmse: 0.41786 | val_1_rmse: 0.4263  |  0:03:27s
epoch 65 | loss: 0.18372 | val_0_rmse: 0.42733 | val_1_rmse: 0.43473 |  0:03:30s
epoch 66 | loss: 0.18693 | val_0_rmse: 0.41765 | val_1_rmse: 0.42666 |  0:03:33s
epoch 67 | loss: 0.18554 | val_0_rmse: 0.41579 | val_1_rmse: 0.42231 |  0:03:36s
epoch 68 | loss: 0.18262 | val_0_rmse: 0.42728 | val_1_rmse: 0.43459 |  0:03:40s
epoch 69 | loss: 0.18265 | val_0_rmse: 0.41669 | val_1_rmse: 0.42144 |  0:03:43s
epoch 70 | loss: 0.18092 | val_0_rmse: 0.42392 | val_1_rmse: 0.43406 |  0:03:46s
epoch 71 | loss: 0.18298 | val_0_rmse: 0.42987 | val_1_rmse: 0.43611 |  0:03:49s
epoch 72 | loss: 0.18552 | val_0_rmse: 0.41618 | val_1_rmse: 0.42022 |  0:03:52s
epoch 73 | loss: 0.18321 | val_0_rmse: 0.4075  | val_1_rmse: 0.41461 |  0:03:56s
epoch 74 | loss: 0.18394 | val_0_rmse: 0.42286 | val_1_rmse: 0.43065 |  0:03:59s
epoch 75 | loss: 0.18427 | val_0_rmse: 0.42922 | val_1_rmse: 0.43564 |  0:04:02s
epoch 76 | loss: 0.18575 | val_0_rmse: 0.42309 | val_1_rmse: 0.42717 |  0:04:05s
epoch 77 | loss: 0.18517 | val_0_rmse: 0.41598 | val_1_rmse: 0.4239  |  0:04:08s
epoch 78 | loss: 0.18984 | val_0_rmse: 0.40858 | val_1_rmse: 0.4166  |  0:04:12s
epoch 79 | loss: 0.18132 | val_0_rmse: 0.41428 | val_1_rmse: 0.42429 |  0:04:15s
epoch 80 | loss: 0.18565 | val_0_rmse: 0.42825 | val_1_rmse: 0.43753 |  0:04:18s
epoch 81 | loss: 0.18225 | val_0_rmse: 0.41315 | val_1_rmse: 0.42274 |  0:04:21s
epoch 82 | loss: 0.18211 | val_0_rmse: 0.45176 | val_1_rmse: 0.45883 |  0:04:24s
epoch 83 | loss: 0.18231 | val_0_rmse: 0.41522 | val_1_rmse: 0.42133 |  0:04:27s
epoch 84 | loss: 0.17975 | val_0_rmse: 0.42011 | val_1_rmse: 0.42647 |  0:04:31s
epoch 85 | loss: 0.18273 | val_0_rmse: 0.41698 | val_1_rmse: 0.42329 |  0:04:34s
epoch 86 | loss: 0.18396 | val_0_rmse: 0.41301 | val_1_rmse: 0.41942 |  0:04:37s
epoch 87 | loss: 0.18049 | val_0_rmse: 0.40749 | val_1_rmse: 0.41539 |  0:04:40s
epoch 88 | loss: 0.1796  | val_0_rmse: 0.41403 | val_1_rmse: 0.41819 |  0:04:43s
epoch 89 | loss: 0.18014 | val_0_rmse: 0.42102 | val_1_rmse: 0.42405 |  0:04:47s
epoch 90 | loss: 0.18072 | val_0_rmse: 0.41953 | val_1_rmse: 0.42582 |  0:04:50s
epoch 91 | loss: 0.18239 | val_0_rmse: 0.41924 | val_1_rmse: 0.42811 |  0:04:53s
epoch 92 | loss: 0.18106 | val_0_rmse: 0.41754 | val_1_rmse: 0.42981 |  0:04:56s
epoch 93 | loss: 0.17877 | val_0_rmse: 0.40723 | val_1_rmse: 0.41549 |  0:04:59s
epoch 94 | loss: 0.1816  | val_0_rmse: 0.4136  | val_1_rmse: 0.42426 |  0:05:03s
epoch 95 | loss: 0.18307 | val_0_rmse: 0.40935 | val_1_rmse: 0.41598 |  0:05:06s
epoch 96 | loss: 0.17846 | val_0_rmse: 0.42143 | val_1_rmse: 0.43006 |  0:05:09s
epoch 97 | loss: 0.18646 | val_0_rmse: 0.41926 | val_1_rmse: 0.42495 |  0:05:12s
epoch 98 | loss: 0.19634 | val_0_rmse: 0.42346 | val_1_rmse: 0.42599 |  0:05:15s
epoch 99 | loss: 0.2021  | val_0_rmse: 0.44824 | val_1_rmse: 0.45004 |  0:05:19s
epoch 100| loss: 0.25386 | val_0_rmse: 0.54277 | val_1_rmse: 0.54415 |  0:05:22s
epoch 101| loss: 0.2251  | val_0_rmse: 0.45975 | val_1_rmse: 0.46019 |  0:05:25s
epoch 102| loss: 0.20818 | val_0_rmse: 0.43884 | val_1_rmse: 0.44019 |  0:05:28s
epoch 103| loss: 0.20531 | val_0_rmse: 0.43884 | val_1_rmse: 0.44102 |  0:05:31s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.41461
Best weights from best epoch are automatically used!
ended training at: 07:27:23
Feature importance:
[('Area', 0.04907187254785025), ('Baths', 0.13957175994855484), ('Beds', 0.045615379766444794), ('Latitude', 0.17963058316668976), ('Longitude', 0.25670024761413973), ('Month', 0.08230357567740475), ('Year', 0.24710658127891588)]
Mean squared error is of 9698139601.975805
Mean absolute error:68120.37235741461
MAPE:0.26165455342650884
R2 score:0.832521727432078
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:27:23
epoch 0  | loss: 0.42528 | val_0_rmse: 0.53445 | val_1_rmse: 0.55087 |  0:00:03s
epoch 1  | loss: 0.28788 | val_0_rmse: 0.52016 | val_1_rmse: 0.53634 |  0:00:06s
epoch 2  | loss: 0.2678  | val_0_rmse: 0.50583 | val_1_rmse: 0.52138 |  0:00:09s
epoch 3  | loss: 0.27054 | val_0_rmse: 0.48346 | val_1_rmse: 0.49586 |  0:00:12s
epoch 4  | loss: 0.24782 | val_0_rmse: 0.47556 | val_1_rmse: 0.49489 |  0:00:15s
epoch 5  | loss: 0.23853 | val_0_rmse: 0.47303 | val_1_rmse: 0.49037 |  0:00:19s
epoch 6  | loss: 0.24005 | val_0_rmse: 0.50215 | val_1_rmse: 0.51939 |  0:00:22s
epoch 7  | loss: 0.23063 | val_0_rmse: 0.47308 | val_1_rmse: 0.48876 |  0:00:25s
epoch 8  | loss: 0.2218  | val_0_rmse: 0.44923 | val_1_rmse: 0.46562 |  0:00:28s
epoch 9  | loss: 0.21734 | val_0_rmse: 0.44706 | val_1_rmse: 0.46382 |  0:00:31s
epoch 10 | loss: 0.21454 | val_0_rmse: 0.46707 | val_1_rmse: 0.48048 |  0:00:35s
epoch 11 | loss: 0.21857 | val_0_rmse: 0.45644 | val_1_rmse: 0.47352 |  0:00:38s
epoch 12 | loss: 0.21593 | val_0_rmse: 0.44968 | val_1_rmse: 0.46398 |  0:00:41s
epoch 13 | loss: 0.21544 | val_0_rmse: 0.43847 | val_1_rmse: 0.45454 |  0:00:44s
epoch 14 | loss: 0.21191 | val_0_rmse: 0.45051 | val_1_rmse: 0.47222 |  0:00:47s
epoch 15 | loss: 0.20638 | val_0_rmse: 0.44697 | val_1_rmse: 0.46457 |  0:00:50s
epoch 16 | loss: 0.20481 | val_0_rmse: 0.43962 | val_1_rmse: 0.45278 |  0:00:54s
epoch 17 | loss: 0.2059  | val_0_rmse: 0.45492 | val_1_rmse: 0.47515 |  0:00:57s
epoch 18 | loss: 0.20842 | val_0_rmse: 0.43688 | val_1_rmse: 0.45411 |  0:01:00s
epoch 19 | loss: 0.21286 | val_0_rmse: 0.43899 | val_1_rmse: 0.45501 |  0:01:03s
epoch 20 | loss: 0.21547 | val_0_rmse: 0.44512 | val_1_rmse: 0.46375 |  0:01:06s
epoch 21 | loss: 0.21077 | val_0_rmse: 0.44227 | val_1_rmse: 0.45428 |  0:01:10s
epoch 22 | loss: 0.21062 | val_0_rmse: 0.45389 | val_1_rmse: 0.46664 |  0:01:13s
epoch 23 | loss: 0.20748 | val_0_rmse: 0.46974 | val_1_rmse: 0.48742 |  0:01:16s
epoch 24 | loss: 0.20716 | val_0_rmse: 0.43627 | val_1_rmse: 0.45536 |  0:01:19s
epoch 25 | loss: 0.19788 | val_0_rmse: 0.44497 | val_1_rmse: 0.46361 |  0:01:22s
epoch 26 | loss: 0.19975 | val_0_rmse: 0.42758 | val_1_rmse: 0.445   |  0:01:26s
epoch 27 | loss: 0.19795 | val_0_rmse: 0.43971 | val_1_rmse: 0.45536 |  0:01:29s
epoch 28 | loss: 0.19636 | val_0_rmse: 0.43712 | val_1_rmse: 0.45063 |  0:01:32s
epoch 29 | loss: 0.1948  | val_0_rmse: 0.43436 | val_1_rmse: 0.45306 |  0:01:35s
epoch 30 | loss: 0.19214 | val_0_rmse: 0.42283 | val_1_rmse: 0.44237 |  0:01:38s
epoch 31 | loss: 0.19454 | val_0_rmse: 0.42673 | val_1_rmse: 0.44523 |  0:01:41s
epoch 32 | loss: 0.19763 | val_0_rmse: 0.43468 | val_1_rmse: 0.45465 |  0:01:45s
epoch 33 | loss: 0.19403 | val_0_rmse: 0.42029 | val_1_rmse: 0.43949 |  0:01:48s
epoch 34 | loss: 0.18779 | val_0_rmse: 0.4202  | val_1_rmse: 0.44132 |  0:01:51s
epoch 35 | loss: 0.19201 | val_0_rmse: 0.41968 | val_1_rmse: 0.43616 |  0:01:54s
epoch 36 | loss: 0.18805 | val_0_rmse: 0.41221 | val_1_rmse: 0.43288 |  0:01:57s
epoch 37 | loss: 0.18501 | val_0_rmse: 0.43034 | val_1_rmse: 0.44914 |  0:02:01s
epoch 38 | loss: 0.18894 | val_0_rmse: 0.41175 | val_1_rmse: 0.43211 |  0:02:04s
epoch 39 | loss: 0.18787 | val_0_rmse: 0.41787 | val_1_rmse: 0.43913 |  0:02:07s
epoch 40 | loss: 0.19037 | val_0_rmse: 0.41733 | val_1_rmse: 0.43899 |  0:02:10s
epoch 41 | loss: 0.18339 | val_0_rmse: 0.41148 | val_1_rmse: 0.43478 |  0:02:13s
epoch 42 | loss: 0.18212 | val_0_rmse: 0.41029 | val_1_rmse: 0.42854 |  0:02:17s
epoch 43 | loss: 0.18799 | val_0_rmse: 0.4231  | val_1_rmse: 0.44205 |  0:02:20s
epoch 44 | loss: 0.1869  | val_0_rmse: 0.4282  | val_1_rmse: 0.44944 |  0:02:23s
epoch 45 | loss: 0.18773 | val_0_rmse: 0.41847 | val_1_rmse: 0.4403  |  0:02:26s
epoch 46 | loss: 0.18323 | val_0_rmse: 0.41823 | val_1_rmse: 0.43477 |  0:02:29s
epoch 47 | loss: 0.18309 | val_0_rmse: 0.42418 | val_1_rmse: 0.44765 |  0:02:32s
epoch 48 | loss: 0.18986 | val_0_rmse: 0.42509 | val_1_rmse: 0.44512 |  0:02:36s
epoch 49 | loss: 0.18652 | val_0_rmse: 0.42268 | val_1_rmse: 0.44048 |  0:02:39s
epoch 50 | loss: 0.18584 | val_0_rmse: 0.41916 | val_1_rmse: 0.43916 |  0:02:42s
epoch 51 | loss: 0.18693 | val_0_rmse: 0.40992 | val_1_rmse: 0.42952 |  0:02:45s
epoch 52 | loss: 0.18124 | val_0_rmse: 0.43486 | val_1_rmse: 0.45237 |  0:02:48s
epoch 53 | loss: 0.18296 | val_0_rmse: 0.40545 | val_1_rmse: 0.42697 |  0:02:52s
epoch 54 | loss: 0.19472 | val_0_rmse: 0.43175 | val_1_rmse: 0.4514  |  0:02:55s
epoch 55 | loss: 0.1893  | val_0_rmse: 0.42145 | val_1_rmse: 0.44176 |  0:02:58s
epoch 56 | loss: 0.19156 | val_0_rmse: 0.42822 | val_1_rmse: 0.44475 |  0:03:01s
epoch 57 | loss: 0.1907  | val_0_rmse: 0.41332 | val_1_rmse: 0.4309  |  0:03:04s
epoch 58 | loss: 0.18749 | val_0_rmse: 0.4207  | val_1_rmse: 0.44033 |  0:03:07s
epoch 59 | loss: 0.18699 | val_0_rmse: 0.43424 | val_1_rmse: 0.45197 |  0:03:11s
epoch 60 | loss: 0.18723 | val_0_rmse: 0.41504 | val_1_rmse: 0.43514 |  0:03:14s
epoch 61 | loss: 0.1859  | val_0_rmse: 0.42915 | val_1_rmse: 0.45049 |  0:03:17s
epoch 62 | loss: 0.184   | val_0_rmse: 0.42084 | val_1_rmse: 0.43841 |  0:03:20s
epoch 63 | loss: 0.18694 | val_0_rmse: 0.42553 | val_1_rmse: 0.43976 |  0:03:23s
epoch 64 | loss: 0.18776 | val_0_rmse: 0.41884 | val_1_rmse: 0.43866 |  0:03:26s
epoch 65 | loss: 0.18846 | val_0_rmse: 0.40972 | val_1_rmse: 0.42835 |  0:03:30s
epoch 66 | loss: 0.18373 | val_0_rmse: 0.41745 | val_1_rmse: 0.43774 |  0:03:33s
epoch 67 | loss: 0.18243 | val_0_rmse: 0.46697 | val_1_rmse: 0.4891  |  0:03:36s
epoch 68 | loss: 0.18359 | val_0_rmse: 0.42179 | val_1_rmse: 0.44214 |  0:03:39s
epoch 69 | loss: 0.1806  | val_0_rmse: 0.42039 | val_1_rmse: 0.44396 |  0:03:42s
epoch 70 | loss: 0.17986 | val_0_rmse: 0.41049 | val_1_rmse: 0.43147 |  0:03:46s
epoch 71 | loss: 0.18027 | val_0_rmse: 0.40958 | val_1_rmse: 0.432   |  0:03:49s
epoch 72 | loss: 0.1776  | val_0_rmse: 0.40228 | val_1_rmse: 0.42408 |  0:03:52s
epoch 73 | loss: 0.17936 | val_0_rmse: 0.40133 | val_1_rmse: 0.42422 |  0:03:55s
epoch 74 | loss: 0.17728 | val_0_rmse: 0.41416 | val_1_rmse: 0.43446 |  0:03:58s
epoch 75 | loss: 0.17831 | val_0_rmse: 0.42317 | val_1_rmse: 0.44452 |  0:04:01s
epoch 76 | loss: 0.18805 | val_0_rmse: 0.44441 | val_1_rmse: 0.46966 |  0:04:05s
epoch 77 | loss: 0.18411 | val_0_rmse: 0.44334 | val_1_rmse: 0.46311 |  0:04:08s
epoch 78 | loss: 0.19931 | val_0_rmse: 0.45038 | val_1_rmse: 0.46572 |  0:04:11s
epoch 79 | loss: 0.18553 | val_0_rmse: 0.41487 | val_1_rmse: 0.43717 |  0:04:14s
epoch 80 | loss: 0.18042 | val_0_rmse: 0.41248 | val_1_rmse: 0.43515 |  0:04:17s
epoch 81 | loss: 0.18289 | val_0_rmse: 0.40778 | val_1_rmse: 0.43004 |  0:04:20s
epoch 82 | loss: 0.17784 | val_0_rmse: 0.42927 | val_1_rmse: 0.44636 |  0:04:24s
epoch 83 | loss: 0.18214 | val_0_rmse: 0.40883 | val_1_rmse: 0.4307  |  0:04:27s
epoch 84 | loss: 0.17962 | val_0_rmse: 0.41566 | val_1_rmse: 0.43557 |  0:04:30s
epoch 85 | loss: 0.17639 | val_0_rmse: 0.39809 | val_1_rmse: 0.42121 |  0:04:33s
epoch 86 | loss: 0.17961 | val_0_rmse: 0.41905 | val_1_rmse: 0.44493 |  0:04:36s
epoch 87 | loss: 0.17525 | val_0_rmse: 0.40113 | val_1_rmse: 0.42419 |  0:04:39s
epoch 88 | loss: 0.17628 | val_0_rmse: 0.40875 | val_1_rmse: 0.43012 |  0:04:43s
epoch 89 | loss: 0.17793 | val_0_rmse: 0.43435 | val_1_rmse: 0.45538 |  0:04:46s
epoch 90 | loss: 0.17419 | val_0_rmse: 0.41639 | val_1_rmse: 0.43959 |  0:04:49s
epoch 91 | loss: 0.18336 | val_0_rmse: 0.41054 | val_1_rmse: 0.43314 |  0:04:52s
epoch 92 | loss: 0.17678 | val_0_rmse: 0.41842 | val_1_rmse: 0.44267 |  0:04:55s
epoch 93 | loss: 0.17686 | val_0_rmse: 0.43038 | val_1_rmse: 0.45088 |  0:04:58s
epoch 94 | loss: 0.17853 | val_0_rmse: 0.40891 | val_1_rmse: 0.43265 |  0:05:02s
epoch 95 | loss: 0.17777 | val_0_rmse: 0.40976 | val_1_rmse: 0.43285 |  0:05:05s
epoch 96 | loss: 0.17378 | val_0_rmse: 0.43084 | val_1_rmse: 0.45258 |  0:05:08s
epoch 97 | loss: 0.17405 | val_0_rmse: 0.40521 | val_1_rmse: 0.43113 |  0:05:11s
epoch 98 | loss: 0.17586 | val_0_rmse: 0.39788 | val_1_rmse: 0.42096 |  0:05:14s
epoch 99 | loss: 0.17791 | val_0_rmse: 0.41767 | val_1_rmse: 0.43829 |  0:05:18s
epoch 100| loss: 0.17704 | val_0_rmse: 0.42243 | val_1_rmse: 0.44563 |  0:05:21s
epoch 101| loss: 0.18088 | val_0_rmse: 0.42432 | val_1_rmse: 0.44772 |  0:05:24s
epoch 102| loss: 0.18161 | val_0_rmse: 0.44143 | val_1_rmse: 0.46241 |  0:05:27s
epoch 103| loss: 0.17576 | val_0_rmse: 0.395   | val_1_rmse: 0.42073 |  0:05:30s
epoch 104| loss: 0.17657 | val_0_rmse: 0.40546 | val_1_rmse: 0.42926 |  0:05:33s
epoch 105| loss: 0.17667 | val_0_rmse: 0.40823 | val_1_rmse: 0.43194 |  0:05:37s
epoch 106| loss: 0.18088 | val_0_rmse: 0.4067  | val_1_rmse: 0.42796 |  0:05:40s
epoch 107| loss: 0.17824 | val_0_rmse: 0.41406 | val_1_rmse: 0.4358  |  0:05:43s
epoch 108| loss: 0.1803  | val_0_rmse: 0.40835 | val_1_rmse: 0.42939 |  0:05:46s
epoch 109| loss: 0.17766 | val_0_rmse: 0.3934  | val_1_rmse: 0.41737 |  0:05:49s
epoch 110| loss: 0.17486 | val_0_rmse: 0.40752 | val_1_rmse: 0.43102 |  0:05:52s
epoch 111| loss: 0.17838 | val_0_rmse: 0.41498 | val_1_rmse: 0.4416  |  0:05:56s
epoch 112| loss: 0.17389 | val_0_rmse: 0.39704 | val_1_rmse: 0.42222 |  0:05:59s
epoch 113| loss: 0.17188 | val_0_rmse: 0.41171 | val_1_rmse: 0.4368  |  0:06:02s
epoch 114| loss: 0.17516 | val_0_rmse: 0.39989 | val_1_rmse: 0.42456 |  0:06:05s
epoch 115| loss: 0.17675 | val_0_rmse: 0.40856 | val_1_rmse: 0.43234 |  0:06:08s
epoch 116| loss: 0.17652 | val_0_rmse: 0.4056  | val_1_rmse: 0.42951 |  0:06:11s
epoch 117| loss: 0.17426 | val_0_rmse: 0.40026 | val_1_rmse: 0.42568 |  0:06:15s
epoch 118| loss: 0.17641 | val_0_rmse: 0.40272 | val_1_rmse: 0.42901 |  0:06:18s
epoch 119| loss: 0.17829 | val_0_rmse: 0.40833 | val_1_rmse: 0.4289  |  0:06:21s
epoch 120| loss: 0.17464 | val_0_rmse: 0.40044 | val_1_rmse: 0.42284 |  0:06:24s
epoch 121| loss: 0.17571 | val_0_rmse: 0.40697 | val_1_rmse: 0.43098 |  0:06:27s
epoch 122| loss: 0.17489 | val_0_rmse: 0.41848 | val_1_rmse: 0.43632 |  0:06:30s
epoch 123| loss: 0.17278 | val_0_rmse: 0.40363 | val_1_rmse: 0.42497 |  0:06:34s
epoch 124| loss: 0.17557 | val_0_rmse: 0.42941 | val_1_rmse: 0.44872 |  0:06:37s
epoch 125| loss: 0.17474 | val_0_rmse: 0.40365 | val_1_rmse: 0.42642 |  0:06:40s
epoch 126| loss: 0.17997 | val_0_rmse: 0.4159  | val_1_rmse: 0.44412 |  0:06:43s
epoch 127| loss: 0.17528 | val_0_rmse: 0.42836 | val_1_rmse: 0.4504  |  0:06:46s
epoch 128| loss: 0.17894 | val_0_rmse: 0.39812 | val_1_rmse: 0.4211  |  0:06:50s
epoch 129| loss: 0.17689 | val_0_rmse: 0.42734 | val_1_rmse: 0.44825 |  0:06:53s
epoch 130| loss: 0.18089 | val_0_rmse: 0.41297 | val_1_rmse: 0.43548 |  0:06:56s
epoch 131| loss: 0.17457 | val_0_rmse: 0.43962 | val_1_rmse: 0.4584  |  0:06:59s
epoch 132| loss: 0.17961 | val_0_rmse: 0.42008 | val_1_rmse: 0.44072 |  0:07:02s
epoch 133| loss: 0.17662 | val_0_rmse: 0.4011  | val_1_rmse: 0.42501 |  0:07:05s
epoch 134| loss: 0.17261 | val_0_rmse: 0.40002 | val_1_rmse: 0.42134 |  0:07:09s
epoch 135| loss: 0.17408 | val_0_rmse: 0.39598 | val_1_rmse: 0.42025 |  0:07:12s
epoch 136| loss: 0.17256 | val_0_rmse: 0.42756 | val_1_rmse: 0.44601 |  0:07:15s
epoch 137| loss: 0.17119 | val_0_rmse: 0.39884 | val_1_rmse: 0.42517 |  0:07:18s
epoch 138| loss: 0.17357 | val_0_rmse: 0.41982 | val_1_rmse: 0.44032 |  0:07:21s
epoch 139| loss: 0.17849 | val_0_rmse: 0.40466 | val_1_rmse: 0.42786 |  0:07:25s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.41737
Best weights from best epoch are automatically used!
ended training at: 07:34:49
Feature importance:
[('Area', 0.0), ('Baths', 0.1152726702362254), ('Beds', 0.15168879260301846), ('Latitude', 0.1616890817521504), ('Longitude', 0.2361029709919807), ('Month', 0.06347675587733748), ('Year', 0.27176972853928755)]
Mean squared error is of 10008650805.986813
Mean absolute error:68880.80841672355
MAPE:0.2895822920296811
R2 score:0.8247867428666693
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:34:50
epoch 0  | loss: 0.43034 | val_0_rmse: 0.56568 | val_1_rmse: 0.56682 |  0:00:03s
epoch 1  | loss: 0.28924 | val_0_rmse: 0.50412 | val_1_rmse: 0.50035 |  0:00:06s
epoch 2  | loss: 0.25225 | val_0_rmse: 0.50589 | val_1_rmse: 0.50252 |  0:00:09s
epoch 3  | loss: 0.25078 | val_0_rmse: 0.47291 | val_1_rmse: 0.47278 |  0:00:12s
epoch 4  | loss: 0.23812 | val_0_rmse: 0.47499 | val_1_rmse: 0.47549 |  0:00:15s
epoch 5  | loss: 0.23459 | val_0_rmse: 0.46303 | val_1_rmse: 0.46307 |  0:00:19s
epoch 6  | loss: 0.23064 | val_0_rmse: 0.45578 | val_1_rmse: 0.45607 |  0:00:22s
epoch 7  | loss: 0.22658 | val_0_rmse: 0.46761 | val_1_rmse: 0.46525 |  0:00:25s
epoch 8  | loss: 0.22245 | val_0_rmse: 0.44215 | val_1_rmse: 0.43846 |  0:00:28s
epoch 9  | loss: 0.22179 | val_0_rmse: 0.4531  | val_1_rmse: 0.45384 |  0:00:31s
epoch 10 | loss: 0.22014 | val_0_rmse: 0.44676 | val_1_rmse: 0.44497 |  0:00:35s
epoch 11 | loss: 0.21395 | val_0_rmse: 0.46166 | val_1_rmse: 0.45952 |  0:00:38s
epoch 12 | loss: 0.21604 | val_0_rmse: 0.44884 | val_1_rmse: 0.44516 |  0:00:41s
epoch 13 | loss: 0.21043 | val_0_rmse: 0.44553 | val_1_rmse: 0.44374 |  0:00:44s
epoch 14 | loss: 0.21585 | val_0_rmse: 0.44439 | val_1_rmse: 0.44363 |  0:00:47s
epoch 15 | loss: 0.21162 | val_0_rmse: 0.43626 | val_1_rmse: 0.4338  |  0:00:50s
epoch 16 | loss: 0.20819 | val_0_rmse: 0.43557 | val_1_rmse: 0.43453 |  0:00:54s
epoch 17 | loss: 0.20592 | val_0_rmse: 0.44589 | val_1_rmse: 0.44412 |  0:00:57s
epoch 18 | loss: 0.21289 | val_0_rmse: 0.45577 | val_1_rmse: 0.45422 |  0:01:00s
epoch 19 | loss: 0.20653 | val_0_rmse: 0.43375 | val_1_rmse: 0.43356 |  0:01:03s
epoch 20 | loss: 0.20785 | val_0_rmse: 0.44619 | val_1_rmse: 0.4456  |  0:01:06s
epoch 21 | loss: 0.20805 | val_0_rmse: 0.43077 | val_1_rmse: 0.43001 |  0:01:10s
epoch 22 | loss: 0.20525 | val_0_rmse: 0.42872 | val_1_rmse: 0.42754 |  0:01:13s
epoch 23 | loss: 0.202   | val_0_rmse: 0.42766 | val_1_rmse: 0.42695 |  0:01:16s
epoch 24 | loss: 0.20004 | val_0_rmse: 0.43781 | val_1_rmse: 0.43699 |  0:01:19s
epoch 25 | loss: 0.20182 | val_0_rmse: 0.43188 | val_1_rmse: 0.43154 |  0:01:22s
epoch 26 | loss: 0.20023 | val_0_rmse: 0.42572 | val_1_rmse: 0.4257  |  0:01:25s
epoch 27 | loss: 0.19965 | val_0_rmse: 0.44529 | val_1_rmse: 0.44318 |  0:01:29s
epoch 28 | loss: 0.20079 | val_0_rmse: 0.43722 | val_1_rmse: 0.43649 |  0:01:32s
epoch 29 | loss: 0.19891 | val_0_rmse: 0.4481  | val_1_rmse: 0.44804 |  0:01:35s
epoch 30 | loss: 0.20038 | val_0_rmse: 0.42223 | val_1_rmse: 0.42158 |  0:01:38s
epoch 31 | loss: 0.19843 | val_0_rmse: 0.43546 | val_1_rmse: 0.43474 |  0:01:41s
epoch 32 | loss: 0.19854 | val_0_rmse: 0.4288  | val_1_rmse: 0.42681 |  0:01:45s
epoch 33 | loss: 0.20031 | val_0_rmse: 0.42779 | val_1_rmse: 0.42898 |  0:01:48s
epoch 34 | loss: 0.20055 | val_0_rmse: 0.43773 | val_1_rmse: 0.43665 |  0:01:51s
epoch 35 | loss: 0.19532 | val_0_rmse: 0.42799 | val_1_rmse: 0.42702 |  0:01:54s
epoch 36 | loss: 0.19883 | val_0_rmse: 0.42548 | val_1_rmse: 0.4259  |  0:01:57s
epoch 37 | loss: 0.19807 | val_0_rmse: 0.47529 | val_1_rmse: 0.47548 |  0:02:00s
epoch 38 | loss: 0.2001  | val_0_rmse: 0.42299 | val_1_rmse: 0.42234 |  0:02:04s
epoch 39 | loss: 0.19614 | val_0_rmse: 0.42693 | val_1_rmse: 0.42788 |  0:02:07s
epoch 40 | loss: 0.19378 | val_0_rmse: 0.4337  | val_1_rmse: 0.4337  |  0:02:10s
epoch 41 | loss: 0.19542 | val_0_rmse: 0.42552 | val_1_rmse: 0.42517 |  0:02:13s
epoch 42 | loss: 0.19432 | val_0_rmse: 0.42145 | val_1_rmse: 0.41966 |  0:02:16s
epoch 43 | loss: 0.19244 | val_0_rmse: 0.44573 | val_1_rmse: 0.44521 |  0:02:20s
epoch 44 | loss: 0.1948  | val_0_rmse: 0.42567 | val_1_rmse: 0.42668 |  0:02:23s
epoch 45 | loss: 0.19091 | val_0_rmse: 0.43258 | val_1_rmse: 0.43457 |  0:02:26s
epoch 46 | loss: 0.19475 | val_0_rmse: 0.42179 | val_1_rmse: 0.4226  |  0:02:29s
epoch 47 | loss: 0.19354 | val_0_rmse: 0.42174 | val_1_rmse: 0.42209 |  0:02:32s
epoch 48 | loss: 0.1951  | val_0_rmse: 0.43063 | val_1_rmse: 0.43225 |  0:02:35s
epoch 49 | loss: 0.19328 | val_0_rmse: 0.42019 | val_1_rmse: 0.4206  |  0:02:39s
epoch 50 | loss: 0.18924 | val_0_rmse: 0.41546 | val_1_rmse: 0.4165  |  0:02:42s
epoch 51 | loss: 0.19027 | val_0_rmse: 0.41848 | val_1_rmse: 0.41901 |  0:02:45s
epoch 52 | loss: 0.19622 | val_0_rmse: 0.44928 | val_1_rmse: 0.45215 |  0:02:48s
epoch 53 | loss: 0.19153 | val_0_rmse: 0.42988 | val_1_rmse: 0.43064 |  0:02:51s
epoch 54 | loss: 0.18913 | val_0_rmse: 0.41322 | val_1_rmse: 0.41491 |  0:02:54s
epoch 55 | loss: 0.18867 | val_0_rmse: 0.4161  | val_1_rmse: 0.41728 |  0:02:58s
epoch 56 | loss: 0.19681 | val_0_rmse: 0.43521 | val_1_rmse: 0.43531 |  0:03:01s
epoch 57 | loss: 0.19304 | val_0_rmse: 0.41273 | val_1_rmse: 0.41337 |  0:03:04s
epoch 58 | loss: 0.18874 | val_0_rmse: 0.44953 | val_1_rmse: 0.45282 |  0:03:07s
epoch 59 | loss: 0.22435 | val_0_rmse: 0.43638 | val_1_rmse: 0.4357  |  0:03:10s
epoch 60 | loss: 0.20249 | val_0_rmse: 0.42994 | val_1_rmse: 0.42993 |  0:03:14s
epoch 61 | loss: 0.20362 | val_0_rmse: 0.45707 | val_1_rmse: 0.45506 |  0:03:17s
epoch 62 | loss: 0.21467 | val_0_rmse: 0.43    | val_1_rmse: 0.42946 |  0:03:20s
epoch 63 | loss: 0.20795 | val_0_rmse: 0.44597 | val_1_rmse: 0.44673 |  0:03:23s
epoch 64 | loss: 0.23193 | val_0_rmse: 0.47372 | val_1_rmse: 0.47093 |  0:03:26s
epoch 65 | loss: 0.23442 | val_0_rmse: 0.45274 | val_1_rmse: 0.45086 |  0:03:29s
epoch 66 | loss: 0.21196 | val_0_rmse: 0.44127 | val_1_rmse: 0.43911 |  0:03:33s
epoch 67 | loss: 0.20389 | val_0_rmse: 0.44321 | val_1_rmse: 0.44087 |  0:03:36s
epoch 68 | loss: 0.2039  | val_0_rmse: 0.43233 | val_1_rmse: 0.4297  |  0:03:39s
epoch 69 | loss: 0.20363 | val_0_rmse: 0.4372  | val_1_rmse: 0.43722 |  0:03:42s
epoch 70 | loss: 0.19956 | val_0_rmse: 0.42684 | val_1_rmse: 0.42517 |  0:03:45s
epoch 71 | loss: 0.19651 | val_0_rmse: 0.44136 | val_1_rmse: 0.44036 |  0:03:48s
epoch 72 | loss: 0.20146 | val_0_rmse: 0.48193 | val_1_rmse: 0.48267 |  0:03:52s
epoch 73 | loss: 0.21042 | val_0_rmse: 0.44389 | val_1_rmse: 0.4442  |  0:03:55s
epoch 74 | loss: 0.19975 | val_0_rmse: 0.43918 | val_1_rmse: 0.44039 |  0:03:58s
epoch 75 | loss: 0.19494 | val_0_rmse: 0.42284 | val_1_rmse: 0.425   |  0:04:01s
epoch 76 | loss: 0.19256 | val_0_rmse: 0.41863 | val_1_rmse: 0.41937 |  0:04:04s
epoch 77 | loss: 0.19091 | val_0_rmse: 0.42104 | val_1_rmse: 0.42226 |  0:04:08s
epoch 78 | loss: 0.19674 | val_0_rmse: 0.42517 | val_1_rmse: 0.42688 |  0:04:11s
epoch 79 | loss: 0.19311 | val_0_rmse: 0.4301  | val_1_rmse: 0.43139 |  0:04:14s
epoch 80 | loss: 0.18846 | val_0_rmse: 0.42416 | val_1_rmse: 0.42578 |  0:04:17s
epoch 81 | loss: 0.18905 | val_0_rmse: 0.41783 | val_1_rmse: 0.41788 |  0:04:20s
epoch 82 | loss: 0.18771 | val_0_rmse: 0.41312 | val_1_rmse: 0.41481 |  0:04:23s
epoch 83 | loss: 0.19041 | val_0_rmse: 0.42501 | val_1_rmse: 0.42721 |  0:04:27s
epoch 84 | loss: 0.19639 | val_0_rmse: 0.48959 | val_1_rmse: 0.49197 |  0:04:30s
epoch 85 | loss: 0.2036  | val_0_rmse: 0.42781 | val_1_rmse: 0.42975 |  0:04:33s
epoch 86 | loss: 0.19493 | val_0_rmse: 0.46061 | val_1_rmse: 0.46163 |  0:04:36s
epoch 87 | loss: 0.19588 | val_0_rmse: 0.42981 | val_1_rmse: 0.42841 |  0:04:39s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.41337
Best weights from best epoch are automatically used!
ended training at: 07:39:31
Feature importance:
[('Area', 0.054048132508353296), ('Baths', 0.14004259086814877), ('Beds', 0.2272148831481799), ('Latitude', 0.08198208762709668), ('Longitude', 0.1758898091602493), ('Month', 0.01101492515049576), ('Year', 0.3098075715374763)]
Mean squared error is of 10016191003.173431
Mean absolute error:69064.66974501524
MAPE:0.3016722104742018
R2 score:0.8270457020550797
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:39:31
epoch 0  | loss: 0.44879 | val_0_rmse: 0.53636 | val_1_rmse: 0.53868 |  0:00:03s
epoch 1  | loss: 0.2784  | val_0_rmse: 0.48981 | val_1_rmse: 0.49642 |  0:00:06s
epoch 2  | loss: 0.25472 | val_0_rmse: 0.48908 | val_1_rmse: 0.49252 |  0:00:09s
epoch 3  | loss: 0.24458 | val_0_rmse: 0.47899 | val_1_rmse: 0.48255 |  0:00:12s
epoch 4  | loss: 0.23708 | val_0_rmse: 0.45443 | val_1_rmse: 0.45735 |  0:00:15s
epoch 5  | loss: 0.23199 | val_0_rmse: 0.44943 | val_1_rmse: 0.45329 |  0:00:19s
epoch 6  | loss: 0.22713 | val_0_rmse: 0.46112 | val_1_rmse: 0.46949 |  0:00:22s
epoch 7  | loss: 0.22408 | val_0_rmse: 0.44953 | val_1_rmse: 0.45449 |  0:00:25s
epoch 8  | loss: 0.22432 | val_0_rmse: 0.44232 | val_1_rmse: 0.44635 |  0:00:28s
epoch 9  | loss: 0.21877 | val_0_rmse: 0.46573 | val_1_rmse: 0.4698  |  0:00:31s
epoch 10 | loss: 0.21929 | val_0_rmse: 0.44418 | val_1_rmse: 0.44652 |  0:00:35s
epoch 11 | loss: 0.21896 | val_0_rmse: 0.43971 | val_1_rmse: 0.44436 |  0:00:38s
epoch 12 | loss: 0.21203 | val_0_rmse: 0.45103 | val_1_rmse: 0.45685 |  0:00:41s
epoch 13 | loss: 0.21375 | val_0_rmse: 0.44803 | val_1_rmse: 0.45249 |  0:00:44s
epoch 14 | loss: 0.21503 | val_0_rmse: 0.44626 | val_1_rmse: 0.45138 |  0:00:47s
epoch 15 | loss: 0.21535 | val_0_rmse: 0.4459  | val_1_rmse: 0.45192 |  0:00:50s
epoch 16 | loss: 0.21415 | val_0_rmse: 0.46982 | val_1_rmse: 0.47402 |  0:00:54s
epoch 17 | loss: 0.21119 | val_0_rmse: 0.44018 | val_1_rmse: 0.44482 |  0:00:57s
epoch 18 | loss: 0.2099  | val_0_rmse: 0.46494 | val_1_rmse: 0.46711 |  0:01:00s
epoch 19 | loss: 0.20825 | val_0_rmse: 0.44324 | val_1_rmse: 0.45109 |  0:01:03s
epoch 20 | loss: 0.2114  | val_0_rmse: 0.44873 | val_1_rmse: 0.45554 |  0:01:06s
epoch 21 | loss: 0.20623 | val_0_rmse: 0.4557  | val_1_rmse: 0.45937 |  0:01:10s
epoch 22 | loss: 0.21338 | val_0_rmse: 0.46929 | val_1_rmse: 0.47911 |  0:01:13s
epoch 23 | loss: 0.20493 | val_0_rmse: 0.45364 | val_1_rmse: 0.45943 |  0:01:16s
epoch 24 | loss: 0.20346 | val_0_rmse: 0.43224 | val_1_rmse: 0.43865 |  0:01:19s
epoch 25 | loss: 0.20428 | val_0_rmse: 0.43069 | val_1_rmse: 0.43905 |  0:01:22s
epoch 26 | loss: 0.2015  | val_0_rmse: 0.44996 | val_1_rmse: 0.45694 |  0:01:26s
epoch 27 | loss: 0.20214 | val_0_rmse: 0.43115 | val_1_rmse: 0.43816 |  0:01:29s
epoch 28 | loss: 0.2002  | val_0_rmse: 0.4274  | val_1_rmse: 0.43632 |  0:01:32s
epoch 29 | loss: 0.20218 | val_0_rmse: 0.43645 | val_1_rmse: 0.44268 |  0:01:35s
epoch 30 | loss: 0.19773 | val_0_rmse: 0.42412 | val_1_rmse: 0.43034 |  0:01:38s
epoch 31 | loss: 0.19729 | val_0_rmse: 0.41761 | val_1_rmse: 0.42532 |  0:01:41s
epoch 32 | loss: 0.19941 | val_0_rmse: 0.4387  | val_1_rmse: 0.44316 |  0:01:45s
epoch 33 | loss: 0.19795 | val_0_rmse: 0.42903 | val_1_rmse: 0.43928 |  0:01:48s
epoch 34 | loss: 0.19829 | val_0_rmse: 0.41913 | val_1_rmse: 0.42747 |  0:01:51s
epoch 35 | loss: 0.19816 | val_0_rmse: 0.44346 | val_1_rmse: 0.44869 |  0:01:54s
epoch 36 | loss: 0.2035  | val_0_rmse: 0.44309 | val_1_rmse: 0.45081 |  0:01:57s
epoch 37 | loss: 0.19393 | val_0_rmse: 0.42269 | val_1_rmse: 0.4331  |  0:02:01s
epoch 38 | loss: 0.19414 | val_0_rmse: 0.42519 | val_1_rmse: 0.43345 |  0:02:04s
epoch 39 | loss: 0.19533 | val_0_rmse: 0.55242 | val_1_rmse: 0.56302 |  0:02:07s
epoch 40 | loss: 0.19777 | val_0_rmse: 0.45694 | val_1_rmse: 0.46131 |  0:02:10s
epoch 41 | loss: 0.19625 | val_0_rmse: 0.42368 | val_1_rmse: 0.43226 |  0:02:13s
epoch 42 | loss: 0.19653 | val_0_rmse: 0.44479 | val_1_rmse: 0.45156 |  0:02:16s
epoch 43 | loss: 0.19999 | val_0_rmse: 0.41397 | val_1_rmse: 0.42145 |  0:02:20s
epoch 44 | loss: 0.1997  | val_0_rmse: 0.41842 | val_1_rmse: 0.42612 |  0:02:23s
epoch 45 | loss: 0.18963 | val_0_rmse: 0.4194  | val_1_rmse: 0.42571 |  0:02:26s
epoch 46 | loss: 0.19447 | val_0_rmse: 0.43217 | val_1_rmse: 0.44243 |  0:02:29s
epoch 47 | loss: 0.19428 | val_0_rmse: 0.41611 | val_1_rmse: 0.42354 |  0:02:32s
epoch 48 | loss: 0.19134 | val_0_rmse: 0.41727 | val_1_rmse: 0.42576 |  0:02:36s
epoch 49 | loss: 0.1915  | val_0_rmse: 0.41643 | val_1_rmse: 0.42586 |  0:02:39s
epoch 50 | loss: 0.19012 | val_0_rmse: 0.41265 | val_1_rmse: 0.42098 |  0:02:42s
epoch 51 | loss: 0.19402 | val_0_rmse: 0.48218 | val_1_rmse: 0.4912  |  0:02:45s
epoch 52 | loss: 0.18996 | val_0_rmse: 0.41779 | val_1_rmse: 0.42655 |  0:02:48s
epoch 53 | loss: 0.19168 | val_0_rmse: 0.42918 | val_1_rmse: 0.43954 |  0:02:51s
epoch 54 | loss: 0.19136 | val_0_rmse: 0.41247 | val_1_rmse: 0.42295 |  0:02:55s
epoch 55 | loss: 0.18913 | val_0_rmse: 0.42342 | val_1_rmse: 0.4346  |  0:02:58s
epoch 56 | loss: 0.18987 | val_0_rmse: 0.41785 | val_1_rmse: 0.42685 |  0:03:01s
epoch 57 | loss: 0.1912  | val_0_rmse: 0.4282  | val_1_rmse: 0.43524 |  0:03:04s
epoch 58 | loss: 0.18902 | val_0_rmse: 0.40969 | val_1_rmse: 0.42041 |  0:03:07s
epoch 59 | loss: 0.18739 | val_0_rmse: 0.42183 | val_1_rmse: 0.43301 |  0:03:11s
epoch 60 | loss: 0.18711 | val_0_rmse: 0.41927 | val_1_rmse: 0.42557 |  0:03:14s
epoch 61 | loss: 0.19302 | val_0_rmse: 0.49177 | val_1_rmse: 0.50341 |  0:03:17s
epoch 62 | loss: 0.19692 | val_0_rmse: 0.4379  | val_1_rmse: 0.44543 |  0:03:20s
epoch 63 | loss: 0.18821 | val_0_rmse: 0.42288 | val_1_rmse: 0.43074 |  0:03:23s
epoch 64 | loss: 0.18591 | val_0_rmse: 0.42389 | val_1_rmse: 0.43212 |  0:03:26s
epoch 65 | loss: 0.18943 | val_0_rmse: 0.43861 | val_1_rmse: 0.44907 |  0:03:30s
epoch 66 | loss: 0.19128 | val_0_rmse: 0.41136 | val_1_rmse: 0.42112 |  0:03:33s
epoch 67 | loss: 0.1904  | val_0_rmse: 0.41217 | val_1_rmse: 0.42452 |  0:03:36s
epoch 68 | loss: 0.18801 | val_0_rmse: 0.43536 | val_1_rmse: 0.44672 |  0:03:39s
epoch 69 | loss: 0.19108 | val_0_rmse: 0.41792 | val_1_rmse: 0.42808 |  0:03:42s
epoch 70 | loss: 0.19171 | val_0_rmse: 0.43734 | val_1_rmse: 0.44881 |  0:03:45s
epoch 71 | loss: 0.18664 | val_0_rmse: 0.42622 | val_1_rmse: 0.43678 |  0:03:49s
epoch 72 | loss: 0.18421 | val_0_rmse: 0.4419  | val_1_rmse: 0.45012 |  0:03:52s
epoch 73 | loss: 0.18311 | val_0_rmse: 0.40726 | val_1_rmse: 0.41875 |  0:03:55s
epoch 74 | loss: 0.18344 | val_0_rmse: 0.41957 | val_1_rmse: 0.42745 |  0:03:58s
epoch 75 | loss: 0.18575 | val_0_rmse: 0.42798 | val_1_rmse: 0.43789 |  0:04:01s
epoch 76 | loss: 0.18461 | val_0_rmse: 0.40905 | val_1_rmse: 0.42141 |  0:04:05s
epoch 77 | loss: 0.18509 | val_0_rmse: 0.4216  | val_1_rmse: 0.42876 |  0:04:08s
epoch 78 | loss: 0.19438 | val_0_rmse: 0.42289 | val_1_rmse: 0.43427 |  0:04:11s
epoch 79 | loss: 0.18927 | val_0_rmse: 0.42137 | val_1_rmse: 0.43238 |  0:04:14s
epoch 80 | loss: 0.18533 | val_0_rmse: 0.41542 | val_1_rmse: 0.42969 |  0:04:17s
epoch 81 | loss: 0.19025 | val_0_rmse: 0.41229 | val_1_rmse: 0.42509 |  0:04:20s
epoch 82 | loss: 0.18625 | val_0_rmse: 0.44833 | val_1_rmse: 0.46227 |  0:04:24s
epoch 83 | loss: 0.18947 | val_0_rmse: 0.41698 | val_1_rmse: 0.42828 |  0:04:27s
epoch 84 | loss: 0.18489 | val_0_rmse: 0.41343 | val_1_rmse: 0.4272  |  0:04:30s
epoch 85 | loss: 0.18769 | val_0_rmse: 0.41475 | val_1_rmse: 0.42663 |  0:04:33s
epoch 86 | loss: 0.18356 | val_0_rmse: 0.41198 | val_1_rmse: 0.42312 |  0:04:36s
epoch 87 | loss: 0.18899 | val_0_rmse: 0.45619 | val_1_rmse: 0.46454 |  0:04:39s
epoch 88 | loss: 0.19053 | val_0_rmse: 0.40732 | val_1_rmse: 0.41801 |  0:04:43s
epoch 89 | loss: 0.1857  | val_0_rmse: 0.44405 | val_1_rmse: 0.45241 |  0:04:46s
epoch 90 | loss: 0.18326 | val_0_rmse: 0.42288 | val_1_rmse: 0.43473 |  0:04:49s
epoch 91 | loss: 0.18593 | val_0_rmse: 0.41954 | val_1_rmse: 0.431   |  0:04:52s
epoch 92 | loss: 0.18458 | val_0_rmse: 0.40721 | val_1_rmse: 0.42066 |  0:04:55s
epoch 93 | loss: 0.1833  | val_0_rmse: 0.41536 | val_1_rmse: 0.42605 |  0:04:58s
epoch 94 | loss: 0.18687 | val_0_rmse: 0.40772 | val_1_rmse: 0.41991 |  0:05:02s
epoch 95 | loss: 0.18611 | val_0_rmse: 0.40415 | val_1_rmse: 0.41606 |  0:05:05s
epoch 96 | loss: 0.18597 | val_0_rmse: 0.4248  | val_1_rmse: 0.44042 |  0:05:08s
epoch 97 | loss: 0.18713 | val_0_rmse: 0.42378 | val_1_rmse: 0.43581 |  0:05:11s
epoch 98 | loss: 0.18246 | val_0_rmse: 0.40745 | val_1_rmse: 0.41708 |  0:05:14s
epoch 99 | loss: 0.1861  | val_0_rmse: 0.44535 | val_1_rmse: 0.45316 |  0:05:18s
epoch 100| loss: 0.18049 | val_0_rmse: 0.42499 | val_1_rmse: 0.43608 |  0:05:21s
epoch 101| loss: 0.18209 | val_0_rmse: 0.41557 | val_1_rmse: 0.42505 |  0:05:24s
epoch 102| loss: 0.1817  | val_0_rmse: 0.4163  | val_1_rmse: 0.42651 |  0:05:27s
epoch 103| loss: 0.18409 | val_0_rmse: 0.41259 | val_1_rmse: 0.42586 |  0:05:30s
epoch 104| loss: 0.18488 | val_0_rmse: 0.4293  | val_1_rmse: 0.44125 |  0:05:33s
epoch 105| loss: 0.1815  | val_0_rmse: 0.41769 | val_1_rmse: 0.42757 |  0:05:37s
epoch 106| loss: 0.18123 | val_0_rmse: 0.42333 | val_1_rmse: 0.43809 |  0:05:40s
epoch 107| loss: 0.18081 | val_0_rmse: 0.43224 | val_1_rmse: 0.44383 |  0:05:43s
epoch 108| loss: 0.18296 | val_0_rmse: 0.42382 | val_1_rmse: 0.4342  |  0:05:46s
epoch 109| loss: 0.18257 | val_0_rmse: 0.41577 | val_1_rmse: 0.43195 |  0:05:49s
epoch 110| loss: 0.17843 | val_0_rmse: 0.41622 | val_1_rmse: 0.42685 |  0:05:52s
epoch 111| loss: 0.17986 | val_0_rmse: 0.45013 | val_1_rmse: 0.46566 |  0:05:56s
epoch 112| loss: 0.18305 | val_0_rmse: 0.41271 | val_1_rmse: 0.42637 |  0:05:59s
epoch 113| loss: 0.18197 | val_0_rmse: 0.42418 | val_1_rmse: 0.43579 |  0:06:02s
epoch 114| loss: 0.19092 | val_0_rmse: 0.41097 | val_1_rmse: 0.42425 |  0:06:05s
epoch 115| loss: 0.18243 | val_0_rmse: 0.43872 | val_1_rmse: 0.45331 |  0:06:08s
epoch 116| loss: 0.17963 | val_0_rmse: 0.40987 | val_1_rmse: 0.42508 |  0:06:12s
epoch 117| loss: 0.18481 | val_0_rmse: 0.41812 | val_1_rmse: 0.42689 |  0:06:15s
epoch 118| loss: 0.17771 | val_0_rmse: 0.41943 | val_1_rmse: 0.43591 |  0:06:18s
epoch 119| loss: 0.18231 | val_0_rmse: 0.39875 | val_1_rmse: 0.41368 |  0:06:21s
epoch 120| loss: 0.18362 | val_0_rmse: 0.40198 | val_1_rmse: 0.41708 |  0:06:24s
epoch 121| loss: 0.1848  | val_0_rmse: 0.40221 | val_1_rmse: 0.41858 |  0:06:27s
epoch 122| loss: 0.1825  | val_0_rmse: 0.43245 | val_1_rmse: 0.44416 |  0:06:31s
epoch 123| loss: 0.18066 | val_0_rmse: 0.40817 | val_1_rmse: 0.42346 |  0:06:34s
epoch 124| loss: 0.17903 | val_0_rmse: 0.41364 | val_1_rmse: 0.43146 |  0:06:37s
epoch 125| loss: 0.183   | val_0_rmse: 0.40145 | val_1_rmse: 0.41696 |  0:06:40s
epoch 126| loss: 0.18244 | val_0_rmse: 0.41085 | val_1_rmse: 0.42505 |  0:06:43s
epoch 127| loss: 0.18936 | val_0_rmse: 0.44096 | val_1_rmse: 0.45014 |  0:06:46s
epoch 128| loss: 0.18105 | val_0_rmse: 0.40852 | val_1_rmse: 0.42361 |  0:06:50s
epoch 129| loss: 0.19429 | val_0_rmse: 0.43514 | val_1_rmse: 0.44435 |  0:06:53s
epoch 130| loss: 0.1852  | val_0_rmse: 0.40535 | val_1_rmse: 0.42025 |  0:06:56s
epoch 131| loss: 0.18147 | val_0_rmse: 0.45216 | val_1_rmse: 0.46289 |  0:06:59s
epoch 132| loss: 0.1988  | val_0_rmse: 0.43151 | val_1_rmse: 0.44468 |  0:07:02s
epoch 133| loss: 0.19548 | val_0_rmse: 0.43992 | val_1_rmse: 0.45162 |  0:07:06s
epoch 134| loss: 0.1855  | val_0_rmse: 0.40427 | val_1_rmse: 0.41796 |  0:07:09s
epoch 135| loss: 0.18194 | val_0_rmse: 0.41889 | val_1_rmse: 0.42954 |  0:07:12s
epoch 136| loss: 0.17952 | val_0_rmse: 0.43514 | val_1_rmse: 0.44735 |  0:07:15s
epoch 137| loss: 0.17771 | val_0_rmse: 0.41633 | val_1_rmse: 0.42896 |  0:07:18s
epoch 138| loss: 0.19019 | val_0_rmse: 0.41908 | val_1_rmse: 0.43042 |  0:07:21s
epoch 139| loss: 0.18579 | val_0_rmse: 0.41198 | val_1_rmse: 0.42492 |  0:07:25s
epoch 140| loss: 0.18561 | val_0_rmse: 0.41745 | val_1_rmse: 0.43011 |  0:07:28s
epoch 141| loss: 0.1835  | val_0_rmse: 0.41636 | val_1_rmse: 0.43316 |  0:07:31s
epoch 142| loss: 0.17999 | val_0_rmse: 0.45088 | val_1_rmse: 0.46632 |  0:07:34s
epoch 143| loss: 0.18846 | val_0_rmse: 0.40437 | val_1_rmse: 0.41908 |  0:07:37s
epoch 144| loss: 0.17616 | val_0_rmse: 0.42744 | val_1_rmse: 0.44232 |  0:07:41s
epoch 145| loss: 0.17808 | val_0_rmse: 0.40773 | val_1_rmse: 0.42246 |  0:07:44s
epoch 146| loss: 0.17535 | val_0_rmse: 0.41384 | val_1_rmse: 0.43056 |  0:07:47s
epoch 147| loss: 0.17758 | val_0_rmse: 0.45509 | val_1_rmse: 0.46667 |  0:07:50s
epoch 148| loss: 0.17947 | val_0_rmse: 0.43703 | val_1_rmse: 0.44695 |  0:07:53s
epoch 149| loss: 0.18072 | val_0_rmse: 0.40451 | val_1_rmse: 0.4189  |  0:07:56s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.41368
Best weights from best epoch are automatically used!
ended training at: 07:47:29
Feature importance:
[('Area', 0.04062765118680839), ('Baths', 0.20739762709841206), ('Beds', 0.0974358766569798), ('Latitude', 0.12159746018638588), ('Longitude', 0.1974827016559509), ('Month', 0.0003014544541532797), ('Year', 0.3351572287613097)]
Mean squared error is of 9419328506.3649
Mean absolute error:67715.10850613506
MAPE:0.2865707804558347
R2 score:0.83637733284361
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:47:29
epoch 0  | loss: 0.43669 | val_0_rmse: 0.5739  | val_1_rmse: 0.57667 |  0:00:03s
epoch 1  | loss: 0.28833 | val_0_rmse: 0.49811 | val_1_rmse: 0.50471 |  0:00:06s
epoch 2  | loss: 0.24964 | val_0_rmse: 0.46619 | val_1_rmse: 0.46988 |  0:00:09s
epoch 3  | loss: 0.23714 | val_0_rmse: 0.46235 | val_1_rmse: 0.46999 |  0:00:12s
epoch 4  | loss: 0.2319  | val_0_rmse: 0.46092 | val_1_rmse: 0.47006 |  0:00:16s
epoch 5  | loss: 0.22432 | val_0_rmse: 0.48542 | val_1_rmse: 0.4923  |  0:00:19s
epoch 6  | loss: 0.22041 | val_0_rmse: 0.44462 | val_1_rmse: 0.45256 |  0:00:22s
epoch 7  | loss: 0.21727 | val_0_rmse: 0.44139 | val_1_rmse: 0.44596 |  0:00:25s
epoch 8  | loss: 0.21831 | val_0_rmse: 0.45124 | val_1_rmse: 0.45376 |  0:00:28s
epoch 9  | loss: 0.21348 | val_0_rmse: 0.4501  | val_1_rmse: 0.45679 |  0:00:31s
epoch 10 | loss: 0.20888 | val_0_rmse: 0.44065 | val_1_rmse: 0.44643 |  0:00:35s
epoch 11 | loss: 0.21016 | val_0_rmse: 0.44882 | val_1_rmse: 0.45184 |  0:00:38s
epoch 12 | loss: 0.21018 | val_0_rmse: 0.45327 | val_1_rmse: 0.46114 |  0:00:41s
epoch 13 | loss: 0.21178 | val_0_rmse: 0.44148 | val_1_rmse: 0.44775 |  0:00:44s
epoch 14 | loss: 0.20961 | val_0_rmse: 0.43736 | val_1_rmse: 0.44506 |  0:00:47s
epoch 15 | loss: 0.20542 | val_0_rmse: 0.43456 | val_1_rmse: 0.44255 |  0:00:51s
epoch 16 | loss: 0.20962 | val_0_rmse: 0.44435 | val_1_rmse: 0.4493  |  0:00:54s
epoch 17 | loss: 0.2044  | val_0_rmse: 0.43255 | val_1_rmse: 0.43988 |  0:00:57s
epoch 18 | loss: 0.20406 | val_0_rmse: 0.42986 | val_1_rmse: 0.43809 |  0:01:00s
epoch 19 | loss: 0.20368 | val_0_rmse: 0.43872 | val_1_rmse: 0.4462  |  0:01:03s
epoch 20 | loss: 0.20342 | val_0_rmse: 0.44305 | val_1_rmse: 0.44786 |  0:01:07s
epoch 21 | loss: 0.19998 | val_0_rmse: 0.4286  | val_1_rmse: 0.43591 |  0:01:10s
epoch 22 | loss: 0.19803 | val_0_rmse: 0.43602 | val_1_rmse: 0.44547 |  0:01:13s
epoch 23 | loss: 0.199   | val_0_rmse: 0.43879 | val_1_rmse: 0.44741 |  0:01:16s
epoch 24 | loss: 0.20159 | val_0_rmse: 0.43639 | val_1_rmse: 0.44557 |  0:01:19s
epoch 25 | loss: 0.19767 | val_0_rmse: 0.42214 | val_1_rmse: 0.43234 |  0:01:23s
epoch 26 | loss: 0.19708 | val_0_rmse: 0.44141 | val_1_rmse: 0.44988 |  0:01:26s
epoch 27 | loss: 0.19939 | val_0_rmse: 0.42484 | val_1_rmse: 0.43586 |  0:01:29s
epoch 28 | loss: 0.19662 | val_0_rmse: 0.42252 | val_1_rmse: 0.43385 |  0:01:32s
epoch 29 | loss: 0.19723 | val_0_rmse: 0.4292  | val_1_rmse: 0.43764 |  0:01:35s
epoch 30 | loss: 0.19878 | val_0_rmse: 0.42632 | val_1_rmse: 0.4343  |  0:01:39s
epoch 31 | loss: 0.19657 | val_0_rmse: 0.4316  | val_1_rmse: 0.44114 |  0:01:42s
epoch 32 | loss: 0.1927  | val_0_rmse: 0.42024 | val_1_rmse: 0.4283  |  0:01:45s
epoch 33 | loss: 0.19455 | val_0_rmse: 0.4247  | val_1_rmse: 0.43158 |  0:01:48s
epoch 34 | loss: 0.19489 | val_0_rmse: 0.42932 | val_1_rmse: 0.44    |  0:01:51s
epoch 35 | loss: 0.19698 | val_0_rmse: 0.42105 | val_1_rmse: 0.42839 |  0:01:54s
epoch 36 | loss: 0.1953  | val_0_rmse: 0.42629 | val_1_rmse: 0.43369 |  0:01:58s
epoch 37 | loss: 0.19693 | val_0_rmse: 0.42391 | val_1_rmse: 0.43354 |  0:02:01s
epoch 38 | loss: 0.19484 | val_0_rmse: 0.43494 | val_1_rmse: 0.4453  |  0:02:04s
epoch 39 | loss: 0.19301 | val_0_rmse: 0.41741 | val_1_rmse: 0.42713 |  0:02:07s
epoch 40 | loss: 0.19514 | val_0_rmse: 0.42579 | val_1_rmse: 0.43375 |  0:02:10s
epoch 41 | loss: 0.19021 | val_0_rmse: 0.41717 | val_1_rmse: 0.42643 |  0:02:14s
epoch 42 | loss: 0.18733 | val_0_rmse: 0.42209 | val_1_rmse: 0.43613 |  0:02:17s
epoch 43 | loss: 0.19362 | val_0_rmse: 0.41861 | val_1_rmse: 0.42858 |  0:02:20s
epoch 44 | loss: 0.19132 | val_0_rmse: 0.41728 | val_1_rmse: 0.42669 |  0:02:23s
epoch 45 | loss: 0.19143 | val_0_rmse: 0.41393 | val_1_rmse: 0.42469 |  0:02:26s
epoch 46 | loss: 0.18841 | val_0_rmse: 0.41948 | val_1_rmse: 0.42856 |  0:02:30s
epoch 47 | loss: 0.1913  | val_0_rmse: 0.42781 | val_1_rmse: 0.43739 |  0:02:33s
epoch 48 | loss: 0.18875 | val_0_rmse: 0.42666 | val_1_rmse: 0.43689 |  0:02:36s
epoch 49 | loss: 0.18993 | val_0_rmse: 0.43656 | val_1_rmse: 0.44633 |  0:02:39s
epoch 50 | loss: 0.19097 | val_0_rmse: 0.43458 | val_1_rmse: 0.44267 |  0:02:42s
epoch 51 | loss: 0.19066 | val_0_rmse: 0.41668 | val_1_rmse: 0.42853 |  0:02:46s
epoch 52 | loss: 0.18838 | val_0_rmse: 0.42057 | val_1_rmse: 0.42937 |  0:02:49s
epoch 53 | loss: 0.1885  | val_0_rmse: 0.41432 | val_1_rmse: 0.42552 |  0:02:52s
epoch 54 | loss: 0.18945 | val_0_rmse: 0.428   | val_1_rmse: 0.43749 |  0:02:55s
epoch 55 | loss: 0.18803 | val_0_rmse: 0.41628 | val_1_rmse: 0.42444 |  0:02:58s
epoch 56 | loss: 0.18697 | val_0_rmse: 0.41725 | val_1_rmse: 0.42924 |  0:03:02s
epoch 57 | loss: 0.1916  | val_0_rmse: 0.42166 | val_1_rmse: 0.43156 |  0:03:05s
epoch 58 | loss: 0.19001 | val_0_rmse: 0.41212 | val_1_rmse: 0.42482 |  0:03:08s
epoch 59 | loss: 0.18699 | val_0_rmse: 0.42091 | val_1_rmse: 0.42976 |  0:03:11s
epoch 60 | loss: 0.18374 | val_0_rmse: 0.42663 | val_1_rmse: 0.43914 |  0:03:14s
epoch 61 | loss: 0.18723 | val_0_rmse: 0.42753 | val_1_rmse: 0.43904 |  0:03:17s
epoch 62 | loss: 0.18492 | val_0_rmse: 0.41515 | val_1_rmse: 0.42818 |  0:03:21s
epoch 63 | loss: 0.18636 | val_0_rmse: 0.40764 | val_1_rmse: 0.41992 |  0:03:24s
epoch 64 | loss: 0.18547 | val_0_rmse: 0.40886 | val_1_rmse: 0.4198  |  0:03:27s
epoch 65 | loss: 0.18913 | val_0_rmse: 0.42439 | val_1_rmse: 0.43497 |  0:03:30s
epoch 66 | loss: 0.1879  | val_0_rmse: 0.42262 | val_1_rmse: 0.43594 |  0:03:33s
epoch 67 | loss: 0.18626 | val_0_rmse: 0.41153 | val_1_rmse: 0.42001 |  0:03:37s
epoch 68 | loss: 0.18517 | val_0_rmse: 0.42205 | val_1_rmse: 0.43547 |  0:03:40s
epoch 69 | loss: 0.18704 | val_0_rmse: 0.41426 | val_1_rmse: 0.42354 |  0:03:43s
epoch 70 | loss: 0.187   | val_0_rmse: 0.42672 | val_1_rmse: 0.43694 |  0:03:46s
epoch 71 | loss: 0.18432 | val_0_rmse: 0.40941 | val_1_rmse: 0.42183 |  0:03:49s
epoch 72 | loss: 0.18584 | val_0_rmse: 0.41553 | val_1_rmse: 0.4285  |  0:03:52s
epoch 73 | loss: 0.18526 | val_0_rmse: 0.40967 | val_1_rmse: 0.42317 |  0:03:56s
epoch 74 | loss: 0.18776 | val_0_rmse: 0.44878 | val_1_rmse: 0.46119 |  0:03:59s
epoch 75 | loss: 0.18361 | val_0_rmse: 0.42362 | val_1_rmse: 0.43711 |  0:04:02s
epoch 76 | loss: 0.1844  | val_0_rmse: 0.40668 | val_1_rmse: 0.41959 |  0:04:05s
epoch 77 | loss: 0.18438 | val_0_rmse: 0.41336 | val_1_rmse: 0.42501 |  0:04:08s
epoch 78 | loss: 0.1834  | val_0_rmse: 0.41059 | val_1_rmse: 0.42066 |  0:04:12s
epoch 79 | loss: 0.18128 | val_0_rmse: 0.41669 | val_1_rmse: 0.42876 |  0:04:15s
epoch 80 | loss: 0.1811  | val_0_rmse: 0.40718 | val_1_rmse: 0.41689 |  0:04:18s
epoch 81 | loss: 0.1826  | val_0_rmse: 0.40821 | val_1_rmse: 0.42109 |  0:04:21s
epoch 82 | loss: 0.18429 | val_0_rmse: 0.41209 | val_1_rmse: 0.42521 |  0:04:24s
epoch 83 | loss: 0.18193 | val_0_rmse: 0.40758 | val_1_rmse: 0.41839 |  0:04:28s
epoch 84 | loss: 0.18348 | val_0_rmse: 0.4036  | val_1_rmse: 0.41615 |  0:04:31s
epoch 85 | loss: 0.18355 | val_0_rmse: 0.40373 | val_1_rmse: 0.41561 |  0:04:34s
epoch 86 | loss: 0.18281 | val_0_rmse: 0.41126 | val_1_rmse: 0.42115 |  0:04:37s
epoch 87 | loss: 0.1815  | val_0_rmse: 0.42366 | val_1_rmse: 0.43496 |  0:04:40s
epoch 88 | loss: 0.18217 | val_0_rmse: 0.43087 | val_1_rmse: 0.44665 |  0:04:44s
epoch 89 | loss: 0.18069 | val_0_rmse: 0.40034 | val_1_rmse: 0.41352 |  0:04:47s
epoch 90 | loss: 0.18187 | val_0_rmse: 0.4719  | val_1_rmse: 0.48112 |  0:04:50s
epoch 91 | loss: 0.18406 | val_0_rmse: 0.40807 | val_1_rmse: 0.42015 |  0:04:53s
epoch 92 | loss: 0.18154 | val_0_rmse: 0.40187 | val_1_rmse: 0.41685 |  0:04:56s
epoch 93 | loss: 0.18113 | val_0_rmse: 0.40717 | val_1_rmse: 0.42018 |  0:04:59s
epoch 94 | loss: 0.17845 | val_0_rmse: 0.40911 | val_1_rmse: 0.42079 |  0:05:03s
epoch 95 | loss: 0.18012 | val_0_rmse: 0.39911 | val_1_rmse: 0.41103 |  0:05:06s
epoch 96 | loss: 0.17832 | val_0_rmse: 0.40356 | val_1_rmse: 0.41672 |  0:05:09s
epoch 97 | loss: 0.17648 | val_0_rmse: 0.40175 | val_1_rmse: 0.41512 |  0:05:12s
epoch 98 | loss: 0.17866 | val_0_rmse: 0.40308 | val_1_rmse: 0.41823 |  0:05:15s
epoch 99 | loss: 0.17892 | val_0_rmse: 0.40208 | val_1_rmse: 0.41449 |  0:05:19s
epoch 100| loss: 0.17767 | val_0_rmse: 0.40209 | val_1_rmse: 0.41518 |  0:05:22s
epoch 101| loss: 0.17826 | val_0_rmse: 0.40118 | val_1_rmse: 0.41586 |  0:05:25s
epoch 102| loss: 0.17893 | val_0_rmse: 0.41707 | val_1_rmse: 0.42998 |  0:05:28s
epoch 103| loss: 0.18301 | val_0_rmse: 0.40916 | val_1_rmse: 0.42394 |  0:05:31s
epoch 104| loss: 0.18016 | val_0_rmse: 0.40938 | val_1_rmse: 0.42467 |  0:05:35s
epoch 105| loss: 0.18205 | val_0_rmse: 0.42395 | val_1_rmse: 0.4356  |  0:05:38s
epoch 106| loss: 0.18263 | val_0_rmse: 0.407   | val_1_rmse: 0.41904 |  0:05:41s
epoch 107| loss: 0.17709 | val_0_rmse: 0.44532 | val_1_rmse: 0.45791 |  0:05:44s
epoch 108| loss: 0.17903 | val_0_rmse: 0.40153 | val_1_rmse: 0.41586 |  0:05:47s
epoch 109| loss: 0.17943 | val_0_rmse: 0.41006 | val_1_rmse: 0.42561 |  0:05:50s
epoch 110| loss: 0.17684 | val_0_rmse: 0.41009 | val_1_rmse: 0.42664 |  0:05:54s
epoch 111| loss: 0.18509 | val_0_rmse: 0.41048 | val_1_rmse: 0.42232 |  0:05:57s
epoch 112| loss: 0.18259 | val_0_rmse: 0.40529 | val_1_rmse: 0.41693 |  0:06:00s
epoch 113| loss: 0.1775  | val_0_rmse: 0.39623 | val_1_rmse: 0.40855 |  0:06:03s
epoch 114| loss: 0.1775  | val_0_rmse: 0.41381 | val_1_rmse: 0.43008 |  0:06:06s
epoch 115| loss: 0.18187 | val_0_rmse: 0.40427 | val_1_rmse: 0.41639 |  0:06:10s
epoch 116| loss: 0.1807  | val_0_rmse: 0.40611 | val_1_rmse: 0.41913 |  0:06:13s
epoch 117| loss: 0.18293 | val_0_rmse: 0.40604 | val_1_rmse: 0.41597 |  0:06:16s
epoch 118| loss: 0.18032 | val_0_rmse: 0.40688 | val_1_rmse: 0.41961 |  0:06:19s
epoch 119| loss: 0.17746 | val_0_rmse: 0.40184 | val_1_rmse: 0.41568 |  0:06:22s
epoch 120| loss: 0.17964 | val_0_rmse: 0.41186 | val_1_rmse: 0.42478 |  0:06:26s
epoch 121| loss: 0.1786  | val_0_rmse: 0.40405 | val_1_rmse: 0.41848 |  0:06:29s
epoch 122| loss: 0.17711 | val_0_rmse: 0.40568 | val_1_rmse: 0.42058 |  0:06:32s
epoch 123| loss: 0.17688 | val_0_rmse: 0.40683 | val_1_rmse: 0.42016 |  0:06:35s
epoch 124| loss: 0.17832 | val_0_rmse: 0.42375 | val_1_rmse: 0.43485 |  0:06:38s
epoch 125| loss: 0.1792  | val_0_rmse: 0.40552 | val_1_rmse: 0.41923 |  0:06:41s
epoch 126| loss: 0.18322 | val_0_rmse: 0.40824 | val_1_rmse: 0.42131 |  0:06:45s
epoch 127| loss: 0.17589 | val_0_rmse: 0.4073  | val_1_rmse: 0.42367 |  0:06:48s
epoch 128| loss: 0.17833 | val_0_rmse: 0.39547 | val_1_rmse: 0.41228 |  0:06:51s
epoch 129| loss: 0.17341 | val_0_rmse: 0.39442 | val_1_rmse: 0.41092 |  0:06:54s
epoch 130| loss: 0.17789 | val_0_rmse: 0.39942 | val_1_rmse: 0.41459 |  0:06:57s
epoch 131| loss: 0.17622 | val_0_rmse: 0.40861 | val_1_rmse: 0.42128 |  0:07:01s
epoch 132| loss: 0.18096 | val_0_rmse: 0.40334 | val_1_rmse: 0.41862 |  0:07:04s
epoch 133| loss: 0.17775 | val_0_rmse: 0.42221 | val_1_rmse: 0.4386  |  0:07:07s
epoch 134| loss: 0.17874 | val_0_rmse: 0.40435 | val_1_rmse: 0.4204  |  0:07:10s
epoch 135| loss: 0.17658 | val_0_rmse: 0.42944 | val_1_rmse: 0.44156 |  0:07:13s
epoch 136| loss: 0.17931 | val_0_rmse: 0.41503 | val_1_rmse: 0.43359 |  0:07:17s
epoch 137| loss: 0.18217 | val_0_rmse: 0.40275 | val_1_rmse: 0.41639 |  0:07:20s
epoch 138| loss: 0.18022 | val_0_rmse: 0.42121 | val_1_rmse: 0.43834 |  0:07:23s
epoch 139| loss: 0.178   | val_0_rmse: 0.40313 | val_1_rmse: 0.41697 |  0:07:26s
epoch 140| loss: 0.17674 | val_0_rmse: 0.3968  | val_1_rmse: 0.4132  |  0:07:29s
epoch 141| loss: 0.17467 | val_0_rmse: 0.42919 | val_1_rmse: 0.44077 |  0:07:33s
epoch 142| loss: 0.18288 | val_0_rmse: 0.43341 | val_1_rmse: 0.44574 |  0:07:36s
epoch 143| loss: 0.18776 | val_0_rmse: 0.41004 | val_1_rmse: 0.4234  |  0:07:39s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.40855
Best weights from best epoch are automatically used!
ended training at: 07:55:10
Feature importance:
[('Area', 0.0689295428936556), ('Baths', 0.08910741311698073), ('Beds', 0.12839879666069984), ('Latitude', 0.08557914585831461), ('Longitude', 0.2456149441522172), ('Month', 0.03023464177251179), ('Year', 0.3521355155456202)]
Mean squared error is of 9581803079.359077
Mean absolute error:67441.60576313917
MAPE:0.2637669817602727
R2 score:0.8338194117324158
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:55:10
epoch 0  | loss: 0.449   | val_0_rmse: 0.57982 | val_1_rmse: 0.5787  |  0:00:03s
epoch 1  | loss: 0.29928 | val_0_rmse: 0.53821 | val_1_rmse: 0.52704 |  0:00:06s
epoch 2  | loss: 0.26708 | val_0_rmse: 0.49598 | val_1_rmse: 0.49055 |  0:00:09s
epoch 3  | loss: 0.24853 | val_0_rmse: 0.49578 | val_1_rmse: 0.48843 |  0:00:12s
epoch 4  | loss: 0.23452 | val_0_rmse: 0.48271 | val_1_rmse: 0.47609 |  0:00:16s
epoch 5  | loss: 0.22867 | val_0_rmse: 0.47575 | val_1_rmse: 0.4691  |  0:00:19s
epoch 6  | loss: 0.22668 | val_0_rmse: 0.45451 | val_1_rmse: 0.4477  |  0:00:22s
epoch 7  | loss: 0.22183 | val_0_rmse: 0.44817 | val_1_rmse: 0.44474 |  0:00:25s
epoch 8  | loss: 0.21729 | val_0_rmse: 0.45558 | val_1_rmse: 0.45015 |  0:00:28s
epoch 9  | loss: 0.21414 | val_0_rmse: 0.44465 | val_1_rmse: 0.43955 |  0:00:32s
epoch 10 | loss: 0.21746 | val_0_rmse: 0.43948 | val_1_rmse: 0.43771 |  0:00:35s
epoch 11 | loss: 0.21133 | val_0_rmse: 0.43587 | val_1_rmse: 0.43217 |  0:00:38s
epoch 12 | loss: 0.21016 | val_0_rmse: 0.44522 | val_1_rmse: 0.44058 |  0:00:41s
epoch 13 | loss: 0.20922 | val_0_rmse: 0.42721 | val_1_rmse: 0.4232  |  0:00:44s
epoch 14 | loss: 0.20843 | val_0_rmse: 0.438   | val_1_rmse: 0.43407 |  0:00:48s
epoch 15 | loss: 0.20303 | val_0_rmse: 0.43125 | val_1_rmse: 0.4269  |  0:00:51s
epoch 16 | loss: 0.20486 | val_0_rmse: 0.43126 | val_1_rmse: 0.42797 |  0:00:54s
epoch 17 | loss: 0.20524 | val_0_rmse: 0.45037 | val_1_rmse: 0.44672 |  0:00:57s
epoch 18 | loss: 0.20103 | val_0_rmse: 0.43562 | val_1_rmse: 0.43208 |  0:01:00s
epoch 19 | loss: 0.19937 | val_0_rmse: 0.43845 | val_1_rmse: 0.43467 |  0:01:03s
epoch 20 | loss: 0.20362 | val_0_rmse: 0.43246 | val_1_rmse: 0.43177 |  0:01:07s
epoch 21 | loss: 0.20186 | val_0_rmse: 0.43707 | val_1_rmse: 0.43396 |  0:01:10s
epoch 22 | loss: 0.20028 | val_0_rmse: 0.42784 | val_1_rmse: 0.42636 |  0:01:13s
epoch 23 | loss: 0.19926 | val_0_rmse: 0.43062 | val_1_rmse: 0.42932 |  0:01:16s
epoch 24 | loss: 0.20286 | val_0_rmse: 0.45925 | val_1_rmse: 0.456   |  0:01:19s
epoch 25 | loss: 0.20228 | val_0_rmse: 0.42993 | val_1_rmse: 0.43032 |  0:01:23s
epoch 26 | loss: 0.19627 | val_0_rmse: 0.42657 | val_1_rmse: 0.42731 |  0:01:26s
epoch 27 | loss: 0.19598 | val_0_rmse: 0.42012 | val_1_rmse: 0.41913 |  0:01:29s
epoch 28 | loss: 0.19597 | val_0_rmse: 0.42586 | val_1_rmse: 0.42472 |  0:01:32s
epoch 29 | loss: 0.19741 | val_0_rmse: 0.41923 | val_1_rmse: 0.41842 |  0:01:35s
epoch 30 | loss: 0.19974 | val_0_rmse: 0.43023 | val_1_rmse: 0.42904 |  0:01:39s
epoch 31 | loss: 0.19283 | val_0_rmse: 0.41527 | val_1_rmse: 0.41433 |  0:01:42s
epoch 32 | loss: 0.19401 | val_0_rmse: 0.41924 | val_1_rmse: 0.41927 |  0:01:45s
epoch 33 | loss: 0.19344 | val_0_rmse: 0.41934 | val_1_rmse: 0.41733 |  0:01:48s
epoch 34 | loss: 0.1926  | val_0_rmse: 0.44301 | val_1_rmse: 0.44172 |  0:01:51s
epoch 35 | loss: 0.19607 | val_0_rmse: 0.42087 | val_1_rmse: 0.42317 |  0:01:55s
epoch 36 | loss: 0.19387 | val_0_rmse: 0.42015 | val_1_rmse: 0.42    |  0:01:58s
epoch 37 | loss: 0.19475 | val_0_rmse: 0.44128 | val_1_rmse: 0.44115 |  0:02:01s
epoch 38 | loss: 0.19613 | val_0_rmse: 0.4279  | val_1_rmse: 0.42922 |  0:02:04s
epoch 39 | loss: 0.19048 | val_0_rmse: 0.41939 | val_1_rmse: 0.41846 |  0:02:07s
epoch 40 | loss: 0.19034 | val_0_rmse: 0.41883 | val_1_rmse: 0.41925 |  0:02:11s
epoch 41 | loss: 0.19728 | val_0_rmse: 0.41776 | val_1_rmse: 0.41785 |  0:02:14s
epoch 42 | loss: 0.18799 | val_0_rmse: 0.41521 | val_1_rmse: 0.41428 |  0:02:17s
epoch 43 | loss: 0.19338 | val_0_rmse: 0.42236 | val_1_rmse: 0.42121 |  0:02:20s
epoch 44 | loss: 0.19204 | val_0_rmse: 0.42989 | val_1_rmse: 0.42941 |  0:02:23s
epoch 45 | loss: 0.19507 | val_0_rmse: 0.42466 | val_1_rmse: 0.42256 |  0:02:27s
epoch 46 | loss: 0.19757 | val_0_rmse: 0.43385 | val_1_rmse: 0.43124 |  0:02:30s
epoch 47 | loss: 0.19427 | val_0_rmse: 0.42274 | val_1_rmse: 0.42288 |  0:02:33s
epoch 48 | loss: 0.19119 | val_0_rmse: 0.43226 | val_1_rmse: 0.43063 |  0:02:36s
epoch 49 | loss: 0.19086 | val_0_rmse: 0.42558 | val_1_rmse: 0.4249  |  0:02:39s
epoch 50 | loss: 0.19043 | val_0_rmse: 0.41987 | val_1_rmse: 0.42    |  0:02:43s
epoch 51 | loss: 0.1926  | val_0_rmse: 0.41993 | val_1_rmse: 0.41738 |  0:02:46s
epoch 52 | loss: 0.19186 | val_0_rmse: 0.41991 | val_1_rmse: 0.42188 |  0:02:49s
epoch 53 | loss: 0.19653 | val_0_rmse: 0.42756 | val_1_rmse: 0.42694 |  0:02:52s
epoch 54 | loss: 0.19264 | val_0_rmse: 0.42227 | val_1_rmse: 0.42054 |  0:02:55s
epoch 55 | loss: 0.18955 | val_0_rmse: 0.43639 | val_1_rmse: 0.43458 |  0:02:58s
epoch 56 | loss: 0.1918  | val_0_rmse: 0.44306 | val_1_rmse: 0.44402 |  0:03:02s
epoch 57 | loss: 0.19269 | val_0_rmse: 0.41658 | val_1_rmse: 0.41691 |  0:03:05s
epoch 58 | loss: 0.19322 | val_0_rmse: 0.4328  | val_1_rmse: 0.432   |  0:03:08s
epoch 59 | loss: 0.19015 | val_0_rmse: 0.42739 | val_1_rmse: 0.42689 |  0:03:11s
epoch 60 | loss: 0.19206 | val_0_rmse: 0.4211  | val_1_rmse: 0.42302 |  0:03:14s
epoch 61 | loss: 0.19184 | val_0_rmse: 0.45031 | val_1_rmse: 0.44882 |  0:03:17s
epoch 62 | loss: 0.20782 | val_0_rmse: 0.42427 | val_1_rmse: 0.42419 |  0:03:21s
epoch 63 | loss: 0.20127 | val_0_rmse: 0.46692 | val_1_rmse: 0.46132 |  0:03:24s
epoch 64 | loss: 0.2143  | val_0_rmse: 0.45011 | val_1_rmse: 0.44756 |  0:03:27s
epoch 65 | loss: 0.20183 | val_0_rmse: 0.42507 | val_1_rmse: 0.42567 |  0:03:30s
epoch 66 | loss: 0.20828 | val_0_rmse: 0.48367 | val_1_rmse: 0.47808 |  0:03:33s
epoch 67 | loss: 0.20915 | val_0_rmse: 0.44138 | val_1_rmse: 0.43785 |  0:03:37s
epoch 68 | loss: 0.20082 | val_0_rmse: 0.4263  | val_1_rmse: 0.42237 |  0:03:40s
epoch 69 | loss: 0.19907 | val_0_rmse: 0.42427 | val_1_rmse: 0.42026 |  0:03:43s
epoch 70 | loss: 0.2024  | val_0_rmse: 0.43379 | val_1_rmse: 0.43234 |  0:03:46s
epoch 71 | loss: 0.19934 | val_0_rmse: 0.44928 | val_1_rmse: 0.44696 |  0:03:49s
epoch 72 | loss: 0.20826 | val_0_rmse: 0.45645 | val_1_rmse: 0.451   |  0:03:52s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.41428
Best weights from best epoch are automatically used!
ended training at: 07:59:04
Feature importance:
[('Area', 0.024203565590181716), ('Baths', 0.12682452295033836), ('Beds', 0.15257481639421974), ('Latitude', 0.19573161455024482), ('Longitude', 0.25790611010531783), ('Month', 0.003938767467263281), ('Year', 0.23882060294243426)]
Mean squared error is of 10060952574.021984
Mean absolute error:69558.45519820607
MAPE:0.2993293351662181
R2 score:0.8286543498626224
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:59:05
epoch 0  | loss: 0.44868 | val_0_rmse: 0.54354 | val_1_rmse: 0.54515 |  0:00:03s
epoch 1  | loss: 0.27152 | val_0_rmse: 0.48429 | val_1_rmse: 0.48618 |  0:00:06s
epoch 2  | loss: 0.24969 | val_0_rmse: 0.48668 | val_1_rmse: 0.48665 |  0:00:09s
epoch 3  | loss: 0.23571 | val_0_rmse: 0.48897 | val_1_rmse: 0.49152 |  0:00:12s
epoch 4  | loss: 0.2277  | val_0_rmse: 0.45137 | val_1_rmse: 0.45482 |  0:00:15s
epoch 5  | loss: 0.22289 | val_0_rmse: 0.46105 | val_1_rmse: 0.46649 |  0:00:19s
epoch 6  | loss: 0.22411 | val_0_rmse: 0.44725 | val_1_rmse: 0.45081 |  0:00:22s
epoch 7  | loss: 0.21558 | val_0_rmse: 0.44524 | val_1_rmse: 0.44561 |  0:00:25s
epoch 8  | loss: 0.21286 | val_0_rmse: 0.43473 | val_1_rmse: 0.43864 |  0:00:28s
epoch 9  | loss: 0.21352 | val_0_rmse: 0.43994 | val_1_rmse: 0.44146 |  0:00:31s
epoch 10 | loss: 0.21092 | val_0_rmse: 0.45658 | val_1_rmse: 0.45889 |  0:00:35s
epoch 11 | loss: 0.20995 | val_0_rmse: 0.44472 | val_1_rmse: 0.451   |  0:00:38s
epoch 12 | loss: 0.21357 | val_0_rmse: 0.45633 | val_1_rmse: 0.45757 |  0:00:41s
epoch 13 | loss: 0.20481 | val_0_rmse: 0.42846 | val_1_rmse: 0.43319 |  0:00:44s
epoch 14 | loss: 0.20494 | val_0_rmse: 0.43619 | val_1_rmse: 0.43969 |  0:00:47s
epoch 15 | loss: 0.20142 | val_0_rmse: 0.42644 | val_1_rmse: 0.43061 |  0:00:51s
epoch 16 | loss: 0.20678 | val_0_rmse: 0.43205 | val_1_rmse: 0.43665 |  0:00:54s
epoch 17 | loss: 0.20485 | val_0_rmse: 0.4439  | val_1_rmse: 0.44574 |  0:00:57s
epoch 18 | loss: 0.20061 | val_0_rmse: 0.42585 | val_1_rmse: 0.43035 |  0:01:00s
epoch 19 | loss: 0.20002 | val_0_rmse: 0.43722 | val_1_rmse: 0.44151 |  0:01:03s
epoch 20 | loss: 0.20425 | val_0_rmse: 0.44091 | val_1_rmse: 0.44585 |  0:01:06s
epoch 21 | loss: 0.19891 | val_0_rmse: 0.44226 | val_1_rmse: 0.44881 |  0:01:10s
epoch 22 | loss: 0.19891 | val_0_rmse: 0.42617 | val_1_rmse: 0.43025 |  0:01:13s
epoch 23 | loss: 0.20152 | val_0_rmse: 0.43541 | val_1_rmse: 0.43838 |  0:01:16s
epoch 24 | loss: 0.20009 | val_0_rmse: 0.42506 | val_1_rmse: 0.43282 |  0:01:19s
epoch 25 | loss: 0.19783 | val_0_rmse: 0.45587 | val_1_rmse: 0.46181 |  0:01:22s
epoch 26 | loss: 0.2016  | val_0_rmse: 0.42928 | val_1_rmse: 0.43393 |  0:01:25s
epoch 27 | loss: 0.20077 | val_0_rmse: 0.43065 | val_1_rmse: 0.43545 |  0:01:29s
epoch 28 | loss: 0.19435 | val_0_rmse: 0.42667 | val_1_rmse: 0.43144 |  0:01:32s
epoch 29 | loss: 0.19744 | val_0_rmse: 0.44229 | val_1_rmse: 0.44798 |  0:01:35s
epoch 30 | loss: 0.19809 | val_0_rmse: 0.42762 | val_1_rmse: 0.43301 |  0:01:38s
epoch 31 | loss: 0.19544 | val_0_rmse: 0.41742 | val_1_rmse: 0.42282 |  0:01:41s
epoch 32 | loss: 0.19673 | val_0_rmse: 0.4526  | val_1_rmse: 0.45808 |  0:01:45s
epoch 33 | loss: 0.21475 | val_0_rmse: 0.43892 | val_1_rmse: 0.44354 |  0:01:48s
epoch 34 | loss: 0.20202 | val_0_rmse: 0.43113 | val_1_rmse: 0.43289 |  0:01:51s
epoch 35 | loss: 0.1956  | val_0_rmse: 0.4513  | val_1_rmse: 0.45817 |  0:01:54s
epoch 36 | loss: 0.1989  | val_0_rmse: 0.42033 | val_1_rmse: 0.42496 |  0:01:57s
epoch 37 | loss: 0.19513 | val_0_rmse: 0.4284  | val_1_rmse: 0.43418 |  0:02:01s
epoch 38 | loss: 0.19522 | val_0_rmse: 0.45902 | val_1_rmse: 0.46369 |  0:02:04s
epoch 39 | loss: 0.19435 | val_0_rmse: 0.42365 | val_1_rmse: 0.42967 |  0:02:07s
epoch 40 | loss: 0.19197 | val_0_rmse: 0.42655 | val_1_rmse: 0.4312  |  0:02:10s
epoch 41 | loss: 0.19468 | val_0_rmse: 0.42421 | val_1_rmse: 0.43014 |  0:02:13s
epoch 42 | loss: 0.19232 | val_0_rmse: 0.42225 | val_1_rmse: 0.42965 |  0:02:17s
epoch 43 | loss: 0.19301 | val_0_rmse: 0.4179  | val_1_rmse: 0.42545 |  0:02:20s
epoch 44 | loss: 0.19518 | val_0_rmse: 0.41794 | val_1_rmse: 0.42475 |  0:02:23s
epoch 45 | loss: 0.19455 | val_0_rmse: 0.43908 | val_1_rmse: 0.44586 |  0:02:26s
epoch 46 | loss: 0.19394 | val_0_rmse: 0.42732 | val_1_rmse: 0.43376 |  0:02:29s
epoch 47 | loss: 0.19671 | val_0_rmse: 0.42117 | val_1_rmse: 0.42594 |  0:02:33s
epoch 48 | loss: 0.19978 | val_0_rmse: 0.44653 | val_1_rmse: 0.45204 |  0:02:36s
epoch 49 | loss: 0.19215 | val_0_rmse: 0.43342 | val_1_rmse: 0.43682 |  0:02:39s
epoch 50 | loss: 0.19354 | val_0_rmse: 0.42203 | val_1_rmse: 0.43085 |  0:02:42s
epoch 51 | loss: 0.19259 | val_0_rmse: 0.41754 | val_1_rmse: 0.42319 |  0:02:45s
epoch 52 | loss: 0.1886  | val_0_rmse: 0.43151 | val_1_rmse: 0.43807 |  0:02:48s
epoch 53 | loss: 0.1934  | val_0_rmse: 0.42366 | val_1_rmse: 0.43103 |  0:02:52s
epoch 54 | loss: 0.19249 | val_0_rmse: 0.41954 | val_1_rmse: 0.42779 |  0:02:55s
epoch 55 | loss: 0.19115 | val_0_rmse: 0.42077 | val_1_rmse: 0.42743 |  0:02:58s
epoch 56 | loss: 0.18873 | val_0_rmse: 0.41654 | val_1_rmse: 0.42367 |  0:03:01s
epoch 57 | loss: 0.19283 | val_0_rmse: 0.44161 | val_1_rmse: 0.4482  |  0:03:04s
epoch 58 | loss: 0.19722 | val_0_rmse: 0.42007 | val_1_rmse: 0.42698 |  0:03:08s
epoch 59 | loss: 0.18882 | val_0_rmse: 0.41703 | val_1_rmse: 0.4235  |  0:03:11s
epoch 60 | loss: 0.18973 | val_0_rmse: 0.43162 | val_1_rmse: 0.43862 |  0:03:14s
epoch 61 | loss: 0.19148 | val_0_rmse: 0.41735 | val_1_rmse: 0.423   |  0:03:17s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.42282
Best weights from best epoch are automatically used!
ended training at: 08:02:23
Feature importance:
[('Area', 0.0), ('Baths', 0.2102287949232353), ('Beds', 0.13525493844492076), ('Latitude', 0.10391939766886873), ('Longitude', 0.19633682628285387), ('Month', 0.10095214093984771), ('Year', 0.2533079017402736)]
Mean squared error is of 10375745572.986755
Mean absolute error:70744.52897371139
MAPE:0.28536715210396973
R2 score:0.8202988316653823
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:23
epoch 0  | loss: 0.48155 | val_0_rmse: 0.59156 | val_1_rmse: 0.58724 |  0:00:03s
epoch 1  | loss: 0.31082 | val_0_rmse: 0.51253 | val_1_rmse: 0.50714 |  0:00:06s
epoch 2  | loss: 0.27745 | val_0_rmse: 0.51478 | val_1_rmse: 0.5114  |  0:00:09s
epoch 3  | loss: 0.26036 | val_0_rmse: 0.49499 | val_1_rmse: 0.49618 |  0:00:12s
epoch 4  | loss: 0.24273 | val_0_rmse: 0.46749 | val_1_rmse: 0.46802 |  0:00:15s
epoch 5  | loss: 0.23912 | val_0_rmse: 0.47863 | val_1_rmse: 0.48208 |  0:00:19s
epoch 6  | loss: 0.23206 | val_0_rmse: 0.51354 | val_1_rmse: 0.51343 |  0:00:22s
epoch 7  | loss: 0.23026 | val_0_rmse: 0.45065 | val_1_rmse: 0.45398 |  0:00:25s
epoch 8  | loss: 0.22619 | val_0_rmse: 0.44545 | val_1_rmse: 0.44725 |  0:00:28s
epoch 9  | loss: 0.22259 | val_0_rmse: 0.46856 | val_1_rmse: 0.46859 |  0:00:31s
epoch 10 | loss: 0.22433 | val_0_rmse: 0.45576 | val_1_rmse: 0.45562 |  0:00:35s
epoch 11 | loss: 0.2166  | val_0_rmse: 0.43979 | val_1_rmse: 0.43987 |  0:00:38s
epoch 12 | loss: 0.21776 | val_0_rmse: 0.44899 | val_1_rmse: 0.44957 |  0:00:41s
epoch 13 | loss: 0.21951 | val_0_rmse: 0.43988 | val_1_rmse: 0.44067 |  0:00:44s
epoch 14 | loss: 0.21744 | val_0_rmse: 0.45484 | val_1_rmse: 0.45586 |  0:00:47s
epoch 15 | loss: 0.21125 | val_0_rmse: 0.43375 | val_1_rmse: 0.43436 |  0:00:50s
epoch 16 | loss: 0.21302 | val_0_rmse: 0.44961 | val_1_rmse: 0.45015 |  0:00:54s
epoch 17 | loss: 0.21052 | val_0_rmse: 0.43354 | val_1_rmse: 0.4344  |  0:00:57s
epoch 18 | loss: 0.20932 | val_0_rmse: 0.45184 | val_1_rmse: 0.4542  |  0:01:00s
epoch 19 | loss: 0.20392 | val_0_rmse: 0.44374 | val_1_rmse: 0.44569 |  0:01:03s
epoch 20 | loss: 0.20247 | val_0_rmse: 0.44908 | val_1_rmse: 0.45008 |  0:01:06s
epoch 21 | loss: 0.20609 | val_0_rmse: 0.43615 | val_1_rmse: 0.44298 |  0:01:10s
epoch 22 | loss: 0.20187 | val_0_rmse: 0.44743 | val_1_rmse: 0.45298 |  0:01:13s
epoch 23 | loss: 0.20317 | val_0_rmse: 0.42891 | val_1_rmse: 0.43037 |  0:01:16s
epoch 24 | loss: 0.1999  | val_0_rmse: 0.42559 | val_1_rmse: 0.43072 |  0:01:19s
epoch 25 | loss: 0.20222 | val_0_rmse: 0.43174 | val_1_rmse: 0.43832 |  0:01:22s
epoch 26 | loss: 0.2025  | val_0_rmse: 0.43267 | val_1_rmse: 0.43401 |  0:01:25s
epoch 27 | loss: 0.20025 | val_0_rmse: 0.44278 | val_1_rmse: 0.45035 |  0:01:29s
epoch 28 | loss: 0.19477 | val_0_rmse: 0.44754 | val_1_rmse: 0.45149 |  0:01:32s
epoch 29 | loss: 0.20236 | val_0_rmse: 0.43506 | val_1_rmse: 0.43924 |  0:01:35s
epoch 30 | loss: 0.1995  | val_0_rmse: 0.4418  | val_1_rmse: 0.44242 |  0:01:38s
epoch 31 | loss: 0.19631 | val_0_rmse: 0.43471 | val_1_rmse: 0.43689 |  0:01:41s
epoch 32 | loss: 0.1993  | val_0_rmse: 0.42501 | val_1_rmse: 0.42818 |  0:01:45s
epoch 33 | loss: 0.19823 | val_0_rmse: 0.42283 | val_1_rmse: 0.43038 |  0:01:48s
epoch 34 | loss: 0.19755 | val_0_rmse: 0.42302 | val_1_rmse: 0.42645 |  0:01:51s
epoch 35 | loss: 0.19859 | val_0_rmse: 0.4234  | val_1_rmse: 0.42785 |  0:01:54s
epoch 36 | loss: 0.19509 | val_0_rmse: 0.43446 | val_1_rmse: 0.43853 |  0:01:57s
epoch 37 | loss: 0.1958  | val_0_rmse: 0.41465 | val_1_rmse: 0.42047 |  0:02:01s
epoch 38 | loss: 0.19336 | val_0_rmse: 0.41665 | val_1_rmse: 0.42342 |  0:02:04s
epoch 39 | loss: 0.19539 | val_0_rmse: 0.42875 | val_1_rmse: 0.43399 |  0:02:07s
epoch 40 | loss: 0.19439 | val_0_rmse: 0.4218  | val_1_rmse: 0.42795 |  0:02:10s
epoch 41 | loss: 0.1912  | val_0_rmse: 0.41912 | val_1_rmse: 0.42509 |  0:02:13s
epoch 42 | loss: 0.1926  | val_0_rmse: 0.43181 | val_1_rmse: 0.43715 |  0:02:16s
epoch 43 | loss: 0.19561 | val_0_rmse: 0.42696 | val_1_rmse: 0.43498 |  0:02:20s
epoch 44 | loss: 0.19233 | val_0_rmse: 0.41738 | val_1_rmse: 0.42311 |  0:02:23s
epoch 45 | loss: 0.18929 | val_0_rmse: 0.41313 | val_1_rmse: 0.41916 |  0:02:26s
epoch 46 | loss: 0.19201 | val_0_rmse: 0.42097 | val_1_rmse: 0.4278  |  0:02:29s
epoch 47 | loss: 0.19019 | val_0_rmse: 0.4407  | val_1_rmse: 0.44382 |  0:02:33s
epoch 48 | loss: 0.19447 | val_0_rmse: 0.41748 | val_1_rmse: 0.42424 |  0:02:36s
epoch 49 | loss: 0.19387 | val_0_rmse: 0.41455 | val_1_rmse: 0.4206  |  0:02:39s
epoch 50 | loss: 0.18944 | val_0_rmse: 0.41135 | val_1_rmse: 0.41655 |  0:02:42s
epoch 51 | loss: 0.19359 | val_0_rmse: 0.4177  | val_1_rmse: 0.4267  |  0:02:45s
epoch 52 | loss: 0.1886  | val_0_rmse: 0.43037 | val_1_rmse: 0.43526 |  0:02:49s
epoch 53 | loss: 0.18867 | val_0_rmse: 0.41701 | val_1_rmse: 0.42197 |  0:02:52s
epoch 54 | loss: 0.18969 | val_0_rmse: 0.41244 | val_1_rmse: 0.42311 |  0:02:55s
epoch 55 | loss: 0.19041 | val_0_rmse: 0.41819 | val_1_rmse: 0.42278 |  0:02:58s
epoch 56 | loss: 0.1885  | val_0_rmse: 0.41775 | val_1_rmse: 0.42344 |  0:03:01s
epoch 57 | loss: 0.18699 | val_0_rmse: 0.41614 | val_1_rmse: 0.42326 |  0:03:05s
epoch 58 | loss: 0.19312 | val_0_rmse: 0.42795 | val_1_rmse: 0.43829 |  0:03:08s
epoch 59 | loss: 0.18931 | val_0_rmse: 0.40999 | val_1_rmse: 0.42047 |  0:03:11s
epoch 60 | loss: 0.18857 | val_0_rmse: 0.41242 | val_1_rmse: 0.41985 |  0:03:14s
epoch 61 | loss: 0.18669 | val_0_rmse: 0.4215  | val_1_rmse: 0.42587 |  0:03:17s
epoch 62 | loss: 0.18816 | val_0_rmse: 0.41341 | val_1_rmse: 0.421   |  0:03:20s
epoch 63 | loss: 0.1846  | val_0_rmse: 0.41427 | val_1_rmse: 0.42295 |  0:03:24s
epoch 64 | loss: 0.18509 | val_0_rmse: 0.42291 | val_1_rmse: 0.42978 |  0:03:27s
epoch 65 | loss: 0.1855  | val_0_rmse: 0.41227 | val_1_rmse: 0.41701 |  0:03:30s
epoch 66 | loss: 0.18678 | val_0_rmse: 0.41739 | val_1_rmse: 0.4226  |  0:03:33s
epoch 67 | loss: 0.18673 | val_0_rmse: 0.41221 | val_1_rmse: 0.41743 |  0:03:36s
epoch 68 | loss: 0.18367 | val_0_rmse: 0.43883 | val_1_rmse: 0.44845 |  0:03:39s
epoch 69 | loss: 0.18496 | val_0_rmse: 0.41038 | val_1_rmse: 0.41722 |  0:03:43s
epoch 70 | loss: 0.18411 | val_0_rmse: 0.4233  | val_1_rmse: 0.42891 |  0:03:46s
epoch 71 | loss: 0.1873  | val_0_rmse: 0.4158  | val_1_rmse: 0.423   |  0:03:49s
epoch 72 | loss: 0.18428 | val_0_rmse: 0.41919 | val_1_rmse: 0.42541 |  0:03:52s
epoch 73 | loss: 0.18813 | val_0_rmse: 0.40512 | val_1_rmse: 0.4153  |  0:03:55s
epoch 74 | loss: 0.18321 | val_0_rmse: 0.4059  | val_1_rmse: 0.41578 |  0:03:59s
epoch 75 | loss: 0.18373 | val_0_rmse: 0.42196 | val_1_rmse: 0.42974 |  0:04:02s
epoch 76 | loss: 0.18879 | val_0_rmse: 0.42113 | val_1_rmse: 0.42993 |  0:04:05s
epoch 77 | loss: 0.18431 | val_0_rmse: 0.41008 | val_1_rmse: 0.42272 |  0:04:08s
epoch 78 | loss: 0.18479 | val_0_rmse: 0.40881 | val_1_rmse: 0.41859 |  0:04:11s
epoch 79 | loss: 0.18407 | val_0_rmse: 0.4085  | val_1_rmse: 0.41474 |  0:04:15s
epoch 80 | loss: 0.18409 | val_0_rmse: 0.4272  | val_1_rmse: 0.4329  |  0:04:18s
epoch 81 | loss: 0.18554 | val_0_rmse: 0.40915 | val_1_rmse: 0.41981 |  0:04:21s
epoch 82 | loss: 0.1832  | val_0_rmse: 0.41265 | val_1_rmse: 0.42343 |  0:04:24s
epoch 83 | loss: 0.18335 | val_0_rmse: 0.4089  | val_1_rmse: 0.42033 |  0:04:27s
epoch 84 | loss: 0.18006 | val_0_rmse: 0.4044  | val_1_rmse: 0.41511 |  0:04:30s
epoch 85 | loss: 0.18135 | val_0_rmse: 0.40278 | val_1_rmse: 0.41445 |  0:04:34s
epoch 86 | loss: 0.18318 | val_0_rmse: 0.41681 | val_1_rmse: 0.42657 |  0:04:37s
epoch 87 | loss: 0.18689 | val_0_rmse: 0.42445 | val_1_rmse: 0.435   |  0:04:40s
epoch 88 | loss: 0.18661 | val_0_rmse: 0.40746 | val_1_rmse: 0.41774 |  0:04:43s
epoch 89 | loss: 0.18473 | val_0_rmse: 0.40632 | val_1_rmse: 0.41663 |  0:04:46s
epoch 90 | loss: 0.18531 | val_0_rmse: 0.41144 | val_1_rmse: 0.42045 |  0:04:50s
epoch 91 | loss: 0.18188 | val_0_rmse: 0.40263 | val_1_rmse: 0.41222 |  0:04:53s
epoch 92 | loss: 0.18053 | val_0_rmse: 0.4071  | val_1_rmse: 0.41545 |  0:04:56s
epoch 93 | loss: 0.18379 | val_0_rmse: 0.4389  | val_1_rmse: 0.44935 |  0:04:59s
epoch 94 | loss: 0.18385 | val_0_rmse: 0.41411 | val_1_rmse: 0.42386 |  0:05:02s
epoch 95 | loss: 0.18197 | val_0_rmse: 0.41414 | val_1_rmse: 0.4229  |  0:05:06s
epoch 96 | loss: 0.18286 | val_0_rmse: 0.40504 | val_1_rmse: 0.41366 |  0:05:09s
epoch 97 | loss: 0.18084 | val_0_rmse: 0.41541 | val_1_rmse: 0.42716 |  0:05:12s
epoch 98 | loss: 0.18051 | val_0_rmse: 0.421   | val_1_rmse: 0.43179 |  0:05:15s
epoch 99 | loss: 0.18079 | val_0_rmse: 0.41522 | val_1_rmse: 0.42539 |  0:05:18s
epoch 100| loss: 0.17938 | val_0_rmse: 0.41404 | val_1_rmse: 0.42718 |  0:05:22s
epoch 101| loss: 0.17644 | val_0_rmse: 0.39473 | val_1_rmse: 0.4057  |  0:05:25s
epoch 102| loss: 0.17783 | val_0_rmse: 0.40054 | val_1_rmse: 0.41473 |  0:05:28s
epoch 103| loss: 0.18231 | val_0_rmse: 0.4299  | val_1_rmse: 0.44202 |  0:05:31s
epoch 104| loss: 0.17993 | val_0_rmse: 0.39713 | val_1_rmse: 0.4094  |  0:05:34s
epoch 105| loss: 0.17877 | val_0_rmse: 0.4123  | val_1_rmse: 0.42082 |  0:05:38s
epoch 106| loss: 0.17656 | val_0_rmse: 0.41132 | val_1_rmse: 0.42435 |  0:05:41s
epoch 107| loss: 0.18024 | val_0_rmse: 0.40787 | val_1_rmse: 0.4162  |  0:05:44s
epoch 108| loss: 0.17797 | val_0_rmse: 0.40002 | val_1_rmse: 0.40888 |  0:05:47s
epoch 109| loss: 0.17617 | val_0_rmse: 0.40217 | val_1_rmse: 0.41237 |  0:05:50s
epoch 110| loss: 0.17685 | val_0_rmse: 0.40854 | val_1_rmse: 0.4186  |  0:05:53s
epoch 111| loss: 0.18339 | val_0_rmse: 0.41973 | val_1_rmse: 0.42809 |  0:05:57s
epoch 112| loss: 0.18327 | val_0_rmse: 0.40554 | val_1_rmse: 0.41564 |  0:06:00s
epoch 113| loss: 0.18253 | val_0_rmse: 0.41312 | val_1_rmse: 0.4223  |  0:06:03s
epoch 114| loss: 0.18171 | val_0_rmse: 0.41792 | val_1_rmse: 0.43131 |  0:06:06s
epoch 115| loss: 0.17608 | val_0_rmse: 0.41114 | val_1_rmse: 0.42478 |  0:06:09s
epoch 116| loss: 0.18094 | val_0_rmse: 0.42946 | val_1_rmse: 0.43808 |  0:06:13s
epoch 117| loss: 0.18681 | val_0_rmse: 0.43276 | val_1_rmse: 0.44197 |  0:06:16s
epoch 118| loss: 0.22248 | val_0_rmse: 0.47643 | val_1_rmse: 0.47649 |  0:06:19s
epoch 119| loss: 0.20387 | val_0_rmse: 0.41714 | val_1_rmse: 0.42124 |  0:06:22s
epoch 120| loss: 0.19346 | val_0_rmse: 0.42433 | val_1_rmse: 0.43002 |  0:06:25s
epoch 121| loss: 0.19383 | val_0_rmse: 0.42853 | val_1_rmse: 0.43591 |  0:06:28s
epoch 122| loss: 0.18765 | val_0_rmse: 0.41432 | val_1_rmse: 0.4188  |  0:06:32s
epoch 123| loss: 0.18576 | val_0_rmse: 0.4175  | val_1_rmse: 0.42289 |  0:06:35s
epoch 124| loss: 0.18439 | val_0_rmse: 0.41508 | val_1_rmse: 0.42222 |  0:06:38s
epoch 125| loss: 0.21591 | val_0_rmse: 0.4445  | val_1_rmse: 0.44716 |  0:06:41s
epoch 126| loss: 0.19421 | val_0_rmse: 0.41446 | val_1_rmse: 0.42091 |  0:06:44s
epoch 127| loss: 0.18916 | val_0_rmse: 0.42675 | val_1_rmse: 0.43244 |  0:06:48s
epoch 128| loss: 0.19049 | val_0_rmse: 0.41336 | val_1_rmse: 0.4197  |  0:06:51s
epoch 129| loss: 0.18493 | val_0_rmse: 0.41598 | val_1_rmse: 0.42229 |  0:06:54s
epoch 130| loss: 0.18192 | val_0_rmse: 0.40315 | val_1_rmse: 0.41138 |  0:06:57s
epoch 131| loss: 0.18228 | val_0_rmse: 0.40258 | val_1_rmse: 0.41126 |  0:07:00s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.4057
Best weights from best epoch are automatically used!
ended training at: 08:09:25
Feature importance:
[('Area', 0.09569835391327702), ('Baths', 0.2846460109537597), ('Beds', 0.08471574568789346), ('Latitude', 0.09909735329471092), ('Longitude', 0.18311222590204113), ('Month', 0.0035786501790175404), ('Year', 0.24915166006930023)]
Mean squared error is of 9759637946.437717
Mean absolute error:67987.79654392302
MAPE:0.28127312633978696
R2 score:0.8312353081026663
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:09:26
epoch 0  | loss: 0.68027 | val_0_rmse: 0.70612 | val_1_rmse: 0.7128  |  0:00:00s
epoch 1  | loss: 0.37244 | val_0_rmse: 0.62726 | val_1_rmse: 0.62085 |  0:00:01s
epoch 2  | loss: 0.31943 | val_0_rmse: 0.55811 | val_1_rmse: 0.56144 |  0:00:02s
epoch 3  | loss: 0.29815 | val_0_rmse: 0.54593 | val_1_rmse: 0.53417 |  0:00:03s
epoch 4  | loss: 0.29117 | val_0_rmse: 0.54515 | val_1_rmse: 0.54007 |  0:00:04s
epoch 5  | loss: 0.29284 | val_0_rmse: 0.52356 | val_1_rmse: 0.51644 |  0:00:05s
epoch 6  | loss: 0.27836 | val_0_rmse: 0.54187 | val_1_rmse: 0.53368 |  0:00:06s
epoch 7  | loss: 0.27441 | val_0_rmse: 0.50646 | val_1_rmse: 0.50543 |  0:00:07s
epoch 8  | loss: 0.27109 | val_0_rmse: 0.5065  | val_1_rmse: 0.507   |  0:00:08s
epoch 9  | loss: 0.27307 | val_0_rmse: 0.50153 | val_1_rmse: 0.49977 |  0:00:09s
epoch 10 | loss: 0.27168 | val_0_rmse: 0.50479 | val_1_rmse: 0.50457 |  0:00:10s
epoch 11 | loss: 0.28383 | val_0_rmse: 0.51882 | val_1_rmse: 0.51243 |  0:00:11s
epoch 12 | loss: 0.27036 | val_0_rmse: 0.52351 | val_1_rmse: 0.51513 |  0:00:12s
epoch 13 | loss: 0.27427 | val_0_rmse: 0.50319 | val_1_rmse: 0.49802 |  0:00:13s
epoch 14 | loss: 0.27002 | val_0_rmse: 0.50068 | val_1_rmse: 0.49521 |  0:00:14s
epoch 15 | loss: 0.26284 | val_0_rmse: 0.50738 | val_1_rmse: 0.49824 |  0:00:15s
epoch 16 | loss: 0.26056 | val_0_rmse: 0.49387 | val_1_rmse: 0.49221 |  0:00:16s
epoch 17 | loss: 0.25891 | val_0_rmse: 0.50058 | val_1_rmse: 0.49542 |  0:00:17s
epoch 18 | loss: 0.2627  | val_0_rmse: 0.51399 | val_1_rmse: 0.51551 |  0:00:18s
epoch 19 | loss: 0.2652  | val_0_rmse: 0.50367 | val_1_rmse: 0.50648 |  0:00:18s
epoch 20 | loss: 0.26614 | val_0_rmse: 0.50315 | val_1_rmse: 0.49976 |  0:00:19s
epoch 21 | loss: 0.26322 | val_0_rmse: 0.49576 | val_1_rmse: 0.49115 |  0:00:20s
epoch 22 | loss: 0.25708 | val_0_rmse: 0.48976 | val_1_rmse: 0.48973 |  0:00:21s
epoch 23 | loss: 0.25602 | val_0_rmse: 0.5071  | val_1_rmse: 0.50108 |  0:00:22s
epoch 24 | loss: 0.25461 | val_0_rmse: 0.48934 | val_1_rmse: 0.4871  |  0:00:23s
epoch 25 | loss: 0.256   | val_0_rmse: 0.49076 | val_1_rmse: 0.48973 |  0:00:24s
epoch 26 | loss: 0.25336 | val_0_rmse: 0.48627 | val_1_rmse: 0.484   |  0:00:25s
epoch 27 | loss: 0.25336 | val_0_rmse: 0.48452 | val_1_rmse: 0.4828  |  0:00:26s
epoch 28 | loss: 0.2495  | val_0_rmse: 0.48295 | val_1_rmse: 0.48212 |  0:00:27s
epoch 29 | loss: 0.24848 | val_0_rmse: 0.48317 | val_1_rmse: 0.48298 |  0:00:28s
epoch 30 | loss: 0.25336 | val_0_rmse: 0.48577 | val_1_rmse: 0.48444 |  0:00:29s
epoch 31 | loss: 0.25299 | val_0_rmse: 0.49026 | val_1_rmse: 0.48911 |  0:00:30s
epoch 32 | loss: 0.24812 | val_0_rmse: 0.48808 | val_1_rmse: 0.48916 |  0:00:31s
epoch 33 | loss: 0.24441 | val_0_rmse: 0.51964 | val_1_rmse: 0.51578 |  0:00:32s
epoch 34 | loss: 0.2582  | val_0_rmse: 0.48219 | val_1_rmse: 0.48291 |  0:00:33s
epoch 35 | loss: 0.25234 | val_0_rmse: 0.49038 | val_1_rmse: 0.48552 |  0:00:34s
epoch 36 | loss: 0.25047 | val_0_rmse: 0.48716 | val_1_rmse: 0.48887 |  0:00:35s
epoch 37 | loss: 0.24686 | val_0_rmse: 0.47903 | val_1_rmse: 0.47532 |  0:00:35s
epoch 38 | loss: 0.24443 | val_0_rmse: 0.48051 | val_1_rmse: 0.47838 |  0:00:36s
epoch 39 | loss: 0.25056 | val_0_rmse: 0.47753 | val_1_rmse: 0.47441 |  0:00:37s
epoch 40 | loss: 0.24298 | val_0_rmse: 0.48742 | val_1_rmse: 0.47965 |  0:00:38s
epoch 41 | loss: 0.24762 | val_0_rmse: 0.48221 | val_1_rmse: 0.48036 |  0:00:39s
epoch 42 | loss: 0.24387 | val_0_rmse: 0.4783  | val_1_rmse: 0.4764  |  0:00:40s
epoch 43 | loss: 0.25516 | val_0_rmse: 0.47818 | val_1_rmse: 0.47643 |  0:00:41s
epoch 44 | loss: 0.25519 | val_0_rmse: 0.5024  | val_1_rmse: 0.49549 |  0:00:42s
epoch 45 | loss: 0.24997 | val_0_rmse: 0.47584 | val_1_rmse: 0.47269 |  0:00:43s
epoch 46 | loss: 0.24104 | val_0_rmse: 0.47294 | val_1_rmse: 0.47254 |  0:00:44s
epoch 47 | loss: 0.24052 | val_0_rmse: 0.49646 | val_1_rmse: 0.48868 |  0:00:45s
epoch 48 | loss: 0.24095 | val_0_rmse: 0.48495 | val_1_rmse: 0.47925 |  0:00:46s
epoch 49 | loss: 0.24034 | val_0_rmse: 0.47675 | val_1_rmse: 0.47298 |  0:00:47s
epoch 50 | loss: 0.24255 | val_0_rmse: 0.47274 | val_1_rmse: 0.467   |  0:00:48s
epoch 51 | loss: 0.24085 | val_0_rmse: 0.48488 | val_1_rmse: 0.48006 |  0:00:49s
epoch 52 | loss: 0.23901 | val_0_rmse: 0.47449 | val_1_rmse: 0.47272 |  0:00:50s
epoch 53 | loss: 0.23976 | val_0_rmse: 0.46854 | val_1_rmse: 0.46803 |  0:00:51s
epoch 54 | loss: 0.23483 | val_0_rmse: 0.46487 | val_1_rmse: 0.46301 |  0:00:52s
epoch 55 | loss: 0.23787 | val_0_rmse: 0.47087 | val_1_rmse: 0.47212 |  0:00:52s
epoch 56 | loss: 0.2307  | val_0_rmse: 0.46487 | val_1_rmse: 0.46642 |  0:00:53s
epoch 57 | loss: 0.23664 | val_0_rmse: 0.46524 | val_1_rmse: 0.46165 |  0:00:54s
epoch 58 | loss: 0.24177 | val_0_rmse: 0.47447 | val_1_rmse: 0.47033 |  0:00:55s
epoch 59 | loss: 0.23862 | val_0_rmse: 0.46525 | val_1_rmse: 0.46444 |  0:00:56s
epoch 60 | loss: 0.23854 | val_0_rmse: 0.47903 | val_1_rmse: 0.48536 |  0:00:57s
epoch 61 | loss: 0.23275 | val_0_rmse: 0.46474 | val_1_rmse: 0.4659  |  0:00:58s
epoch 62 | loss: 0.2331  | val_0_rmse: 0.47664 | val_1_rmse: 0.4747  |  0:00:59s
epoch 63 | loss: 0.22936 | val_0_rmse: 0.46009 | val_1_rmse: 0.46369 |  0:01:00s
epoch 64 | loss: 0.22762 | val_0_rmse: 0.46967 | val_1_rmse: 0.46524 |  0:01:01s
epoch 65 | loss: 0.23496 | val_0_rmse: 0.48196 | val_1_rmse: 0.48013 |  0:01:02s
epoch 66 | loss: 0.22607 | val_0_rmse: 0.47108 | val_1_rmse: 0.47512 |  0:01:03s
epoch 67 | loss: 0.23114 | val_0_rmse: 0.45553 | val_1_rmse: 0.4558  |  0:01:04s
epoch 68 | loss: 0.22706 | val_0_rmse: 0.46708 | val_1_rmse: 0.46754 |  0:01:05s
epoch 69 | loss: 0.22989 | val_0_rmse: 0.45652 | val_1_rmse: 0.45731 |  0:01:06s
epoch 70 | loss: 0.22492 | val_0_rmse: 0.44997 | val_1_rmse: 0.45388 |  0:01:07s
epoch 71 | loss: 0.22318 | val_0_rmse: 0.46304 | val_1_rmse: 0.46349 |  0:01:08s
epoch 72 | loss: 0.22845 | val_0_rmse: 0.45544 | val_1_rmse: 0.45891 |  0:01:09s
epoch 73 | loss: 0.22916 | val_0_rmse: 0.46487 | val_1_rmse: 0.46955 |  0:01:10s
epoch 74 | loss: 0.22828 | val_0_rmse: 0.47073 | val_1_rmse: 0.47064 |  0:01:10s
epoch 75 | loss: 0.22735 | val_0_rmse: 0.45285 | val_1_rmse: 0.45388 |  0:01:11s
epoch 76 | loss: 0.22578 | val_0_rmse: 0.4582  | val_1_rmse: 0.46232 |  0:01:12s
epoch 77 | loss: 0.22371 | val_0_rmse: 0.46023 | val_1_rmse: 0.46286 |  0:01:13s
epoch 78 | loss: 0.22355 | val_0_rmse: 0.45283 | val_1_rmse: 0.4576  |  0:01:14s
epoch 79 | loss: 0.23408 | val_0_rmse: 0.45178 | val_1_rmse: 0.45174 |  0:01:15s
epoch 80 | loss: 0.22311 | val_0_rmse: 0.45784 | val_1_rmse: 0.45952 |  0:01:16s
epoch 81 | loss: 0.22738 | val_0_rmse: 0.45605 | val_1_rmse: 0.46219 |  0:01:17s
epoch 82 | loss: 0.22102 | val_0_rmse: 0.45274 | val_1_rmse: 0.45544 |  0:01:18s
epoch 83 | loss: 0.22222 | val_0_rmse: 0.46387 | val_1_rmse: 0.46428 |  0:01:19s
epoch 84 | loss: 0.22353 | val_0_rmse: 0.45356 | val_1_rmse: 0.45251 |  0:01:20s
epoch 85 | loss: 0.22027 | val_0_rmse: 0.45331 | val_1_rmse: 0.45789 |  0:01:21s
epoch 86 | loss: 0.21821 | val_0_rmse: 0.45637 | val_1_rmse: 0.46103 |  0:01:22s
epoch 87 | loss: 0.22083 | val_0_rmse: 0.45943 | val_1_rmse: 0.46342 |  0:01:23s
epoch 88 | loss: 0.21957 | val_0_rmse: 0.45826 | val_1_rmse: 0.46139 |  0:01:24s
epoch 89 | loss: 0.227   | val_0_rmse: 0.46534 | val_1_rmse: 0.46737 |  0:01:25s
epoch 90 | loss: 0.21849 | val_0_rmse: 0.44747 | val_1_rmse: 0.45129 |  0:01:26s
epoch 91 | loss: 0.21937 | val_0_rmse: 0.45483 | val_1_rmse: 0.45463 |  0:01:26s
epoch 92 | loss: 0.21514 | val_0_rmse: 0.46055 | val_1_rmse: 0.46233 |  0:01:27s
epoch 93 | loss: 0.21861 | val_0_rmse: 0.44137 | val_1_rmse: 0.44656 |  0:01:28s
epoch 94 | loss: 0.21535 | val_0_rmse: 0.44715 | val_1_rmse: 0.45167 |  0:01:29s
epoch 95 | loss: 0.22056 | val_0_rmse: 0.4615  | val_1_rmse: 0.46003 |  0:01:30s
epoch 96 | loss: 0.21597 | val_0_rmse: 0.44961 | val_1_rmse: 0.45481 |  0:01:31s
epoch 97 | loss: 0.21852 | val_0_rmse: 0.4545  | val_1_rmse: 0.4597  |  0:01:32s
epoch 98 | loss: 0.21925 | val_0_rmse: 0.45023 | val_1_rmse: 0.45199 |  0:01:33s
epoch 99 | loss: 0.21829 | val_0_rmse: 0.44651 | val_1_rmse: 0.45219 |  0:01:34s
epoch 100| loss: 0.21695 | val_0_rmse: 0.45591 | val_1_rmse: 0.46759 |  0:01:35s
epoch 101| loss: 0.21967 | val_0_rmse: 0.45263 | val_1_rmse: 0.46549 |  0:01:36s
epoch 102| loss: 0.20922 | val_0_rmse: 0.44955 | val_1_rmse: 0.45731 |  0:01:37s
epoch 103| loss: 0.21582 | val_0_rmse: 0.44651 | val_1_rmse: 0.45702 |  0:01:38s
epoch 104| loss: 0.21749 | val_0_rmse: 0.43942 | val_1_rmse: 0.44528 |  0:01:39s
epoch 105| loss: 0.21158 | val_0_rmse: 0.44435 | val_1_rmse: 0.45375 |  0:01:40s
epoch 106| loss: 0.21196 | val_0_rmse: 0.45074 | val_1_rmse: 0.45527 |  0:01:41s
epoch 107| loss: 0.21207 | val_0_rmse: 0.43855 | val_1_rmse: 0.45147 |  0:01:42s
epoch 108| loss: 0.22002 | val_0_rmse: 0.44884 | val_1_rmse: 0.45489 |  0:01:42s
epoch 109| loss: 0.20896 | val_0_rmse: 0.4434  | val_1_rmse: 0.45186 |  0:01:43s
epoch 110| loss: 0.21034 | val_0_rmse: 0.46501 | val_1_rmse: 0.48328 |  0:01:44s
epoch 111| loss: 0.21368 | val_0_rmse: 0.44404 | val_1_rmse: 0.45804 |  0:01:45s
epoch 112| loss: 0.20843 | val_0_rmse: 0.44923 | val_1_rmse: 0.46436 |  0:01:46s
epoch 113| loss: 0.21029 | val_0_rmse: 0.45425 | val_1_rmse: 0.46154 |  0:01:47s
epoch 114| loss: 0.22465 | val_0_rmse: 0.44614 | val_1_rmse: 0.45754 |  0:01:48s
epoch 115| loss: 0.21124 | val_0_rmse: 0.44551 | val_1_rmse: 0.45599 |  0:01:49s
epoch 116| loss: 0.20461 | val_0_rmse: 0.43492 | val_1_rmse: 0.44946 |  0:01:50s
epoch 117| loss: 0.20952 | val_0_rmse: 0.45176 | val_1_rmse: 0.46052 |  0:01:51s
epoch 118| loss: 0.21074 | val_0_rmse: 0.43865 | val_1_rmse: 0.44899 |  0:01:52s
epoch 119| loss: 0.21471 | val_0_rmse: 0.46211 | val_1_rmse: 0.47194 |  0:01:53s
epoch 120| loss: 0.21194 | val_0_rmse: 0.45507 | val_1_rmse: 0.46774 |  0:01:54s
epoch 121| loss: 0.21121 | val_0_rmse: 0.44704 | val_1_rmse: 0.45841 |  0:01:55s
epoch 122| loss: 0.20936 | val_0_rmse: 0.45241 | val_1_rmse: 0.46214 |  0:01:56s
epoch 123| loss: 0.21105 | val_0_rmse: 0.44164 | val_1_rmse: 0.45327 |  0:01:57s
epoch 124| loss: 0.21302 | val_0_rmse: 0.46112 | val_1_rmse: 0.47098 |  0:01:58s
epoch 125| loss: 0.21519 | val_0_rmse: 0.43995 | val_1_rmse: 0.44937 |  0:01:58s
epoch 126| loss: 0.20836 | val_0_rmse: 0.44557 | val_1_rmse: 0.45437 |  0:01:59s
epoch 127| loss: 0.21645 | val_0_rmse: 0.44209 | val_1_rmse: 0.45399 |  0:02:00s
epoch 128| loss: 0.20896 | val_0_rmse: 0.43936 | val_1_rmse: 0.45363 |  0:02:01s
epoch 129| loss: 0.20742 | val_0_rmse: 0.44043 | val_1_rmse: 0.45059 |  0:02:02s
epoch 130| loss: 0.20839 | val_0_rmse: 0.4374  | val_1_rmse: 0.44859 |  0:02:03s
epoch 131| loss: 0.2112  | val_0_rmse: 0.43392 | val_1_rmse: 0.44647 |  0:02:04s
epoch 132| loss: 0.21015 | val_0_rmse: 0.44168 | val_1_rmse: 0.45402 |  0:02:05s
epoch 133| loss: 0.2039  | val_0_rmse: 0.43623 | val_1_rmse: 0.45009 |  0:02:06s
epoch 134| loss: 0.20738 | val_0_rmse: 0.43647 | val_1_rmse: 0.44384 |  0:02:07s
epoch 135| loss: 0.20611 | val_0_rmse: 0.43222 | val_1_rmse: 0.44843 |  0:02:08s
epoch 136| loss: 0.20571 | val_0_rmse: 0.46113 | val_1_rmse: 0.46317 |  0:02:09s
epoch 137| loss: 0.20921 | val_0_rmse: 0.44585 | val_1_rmse: 0.46105 |  0:02:10s
epoch 138| loss: 0.20889 | val_0_rmse: 0.43263 | val_1_rmse: 0.44817 |  0:02:11s
epoch 139| loss: 0.21429 | val_0_rmse: 0.44504 | val_1_rmse: 0.44809 |  0:02:12s
epoch 140| loss: 0.21194 | val_0_rmse: 0.44759 | val_1_rmse: 0.45711 |  0:02:13s
epoch 141| loss: 0.20083 | val_0_rmse: 0.43004 | val_1_rmse: 0.44323 |  0:02:14s
epoch 142| loss: 0.20162 | val_0_rmse: 0.4319  | val_1_rmse: 0.45257 |  0:02:14s
epoch 143| loss: 0.2013  | val_0_rmse: 0.4399  | val_1_rmse: 0.46294 |  0:02:15s
epoch 144| loss: 0.1986  | val_0_rmse: 0.44141 | val_1_rmse: 0.45335 |  0:02:16s
epoch 145| loss: 0.21634 | val_0_rmse: 0.46558 | val_1_rmse: 0.47334 |  0:02:17s
epoch 146| loss: 0.21545 | val_0_rmse: 0.44261 | val_1_rmse: 0.45788 |  0:02:18s
epoch 147| loss: 0.21349 | val_0_rmse: 0.45745 | val_1_rmse: 0.47067 |  0:02:19s
epoch 148| loss: 0.21324 | val_0_rmse: 0.43974 | val_1_rmse: 0.46295 |  0:02:20s
epoch 149| loss: 0.2093  | val_0_rmse: 0.42862 | val_1_rmse: 0.4393  |  0:02:21s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.4393
Best weights from best epoch are automatically used!
ended training at: 08:11:48
Feature importance:
[('Area', 0.44307690610657874), ('Baths', 3.6592234806440645e-06), ('Beds', 0.030805807535946524), ('Latitude', 0.3204840134950689), ('Longitude', 0.14503054873401014), ('Month', 0.010707692392274723), ('Year', 0.04989137251264032)]
Mean squared error is of 6025391866.398599
Mean absolute error:55744.87656476473
MAPE:0.14508570857103986
R2 score:0.8059182423695807
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:11:48
epoch 0  | loss: 0.66428 | val_0_rmse: 0.72764 | val_1_rmse: 0.73761 |  0:00:00s
epoch 1  | loss: 0.34034 | val_0_rmse: 0.58021 | val_1_rmse: 0.58878 |  0:00:01s
epoch 2  | loss: 0.30825 | val_0_rmse: 0.59048 | val_1_rmse: 0.60006 |  0:00:02s
epoch 3  | loss: 0.31292 | val_0_rmse: 0.55794 | val_1_rmse: 0.55991 |  0:00:03s
epoch 4  | loss: 0.30723 | val_0_rmse: 0.54481 | val_1_rmse: 0.55529 |  0:00:04s
epoch 5  | loss: 0.3048  | val_0_rmse: 0.5325  | val_1_rmse: 0.54129 |  0:00:05s
epoch 6  | loss: 0.30029 | val_0_rmse: 0.52927 | val_1_rmse: 0.53808 |  0:00:06s
epoch 7  | loss: 0.2914  | val_0_rmse: 0.51927 | val_1_rmse: 0.5269  |  0:00:07s
epoch 8  | loss: 0.28396 | val_0_rmse: 0.5253  | val_1_rmse: 0.53461 |  0:00:08s
epoch 9  | loss: 0.2768  | val_0_rmse: 0.52109 | val_1_rmse: 0.52504 |  0:00:09s
epoch 10 | loss: 0.28914 | val_0_rmse: 0.54576 | val_1_rmse: 0.54499 |  0:00:10s
epoch 11 | loss: 0.2848  | val_0_rmse: 0.51323 | val_1_rmse: 0.52406 |  0:00:11s
epoch 12 | loss: 0.27855 | val_0_rmse: 0.52672 | val_1_rmse: 0.53955 |  0:00:12s
epoch 13 | loss: 0.28552 | val_0_rmse: 0.51995 | val_1_rmse: 0.52408 |  0:00:13s
epoch 14 | loss: 0.27828 | val_0_rmse: 0.51009 | val_1_rmse: 0.51177 |  0:00:14s
epoch 15 | loss: 0.27954 | val_0_rmse: 0.53148 | val_1_rmse: 0.52951 |  0:00:15s
epoch 16 | loss: 0.28756 | val_0_rmse: 0.53842 | val_1_rmse: 0.54497 |  0:00:16s
epoch 17 | loss: 0.27978 | val_0_rmse: 0.50842 | val_1_rmse: 0.51279 |  0:00:17s
epoch 18 | loss: 0.2823  | val_0_rmse: 0.51929 | val_1_rmse: 0.52462 |  0:00:18s
epoch 19 | loss: 0.28997 | val_0_rmse: 0.54021 | val_1_rmse: 0.54697 |  0:00:19s
epoch 20 | loss: 0.28882 | val_0_rmse: 0.50843 | val_1_rmse: 0.52015 |  0:00:19s
epoch 21 | loss: 0.27349 | val_0_rmse: 0.51295 | val_1_rmse: 0.52526 |  0:00:20s
epoch 22 | loss: 0.27349 | val_0_rmse: 0.52628 | val_1_rmse: 0.53169 |  0:00:21s
epoch 23 | loss: 0.27321 | val_0_rmse: 0.54635 | val_1_rmse: 0.54943 |  0:00:22s
epoch 24 | loss: 0.28153 | val_0_rmse: 0.54129 | val_1_rmse: 0.54398 |  0:00:23s
epoch 25 | loss: 0.26738 | val_0_rmse: 0.49241 | val_1_rmse: 0.49555 |  0:00:24s
epoch 26 | loss: 0.25855 | val_0_rmse: 0.48967 | val_1_rmse: 0.48865 |  0:00:25s
epoch 27 | loss: 0.25198 | val_0_rmse: 0.48508 | val_1_rmse: 0.48487 |  0:00:26s
epoch 28 | loss: 0.24669 | val_0_rmse: 0.47961 | val_1_rmse: 0.47915 |  0:00:27s
epoch 29 | loss: 0.24076 | val_0_rmse: 0.48267 | val_1_rmse: 0.48875 |  0:00:28s
epoch 30 | loss: 0.241   | val_0_rmse: 0.47594 | val_1_rmse: 0.48138 |  0:00:29s
epoch 31 | loss: 0.24542 | val_0_rmse: 0.47586 | val_1_rmse: 0.47497 |  0:00:30s
epoch 32 | loss: 0.24054 | val_0_rmse: 0.47796 | val_1_rmse: 0.4804  |  0:00:31s
epoch 33 | loss: 0.24017 | val_0_rmse: 0.46821 | val_1_rmse: 0.47364 |  0:00:32s
epoch 34 | loss: 0.24677 | val_0_rmse: 0.48618 | val_1_rmse: 0.48779 |  0:00:33s
epoch 35 | loss: 0.24081 | val_0_rmse: 0.47381 | val_1_rmse: 0.48158 |  0:00:34s
epoch 36 | loss: 0.2418  | val_0_rmse: 0.47409 | val_1_rmse: 0.48054 |  0:00:35s
epoch 37 | loss: 0.23614 | val_0_rmse: 0.47531 | val_1_rmse: 0.47743 |  0:00:36s
epoch 38 | loss: 0.23712 | val_0_rmse: 0.45995 | val_1_rmse: 0.46253 |  0:00:37s
epoch 39 | loss: 0.23128 | val_0_rmse: 0.459   | val_1_rmse: 0.4637  |  0:00:38s
epoch 40 | loss: 0.22872 | val_0_rmse: 0.46476 | val_1_rmse: 0.47497 |  0:00:38s
epoch 41 | loss: 0.22676 | val_0_rmse: 0.47509 | val_1_rmse: 0.47962 |  0:00:39s
epoch 42 | loss: 0.24256 | val_0_rmse: 0.47164 | val_1_rmse: 0.48245 |  0:00:40s
epoch 43 | loss: 0.23146 | val_0_rmse: 0.46159 | val_1_rmse: 0.47282 |  0:00:41s
epoch 44 | loss: 0.22623 | val_0_rmse: 0.45572 | val_1_rmse: 0.45837 |  0:00:42s
epoch 45 | loss: 0.23132 | val_0_rmse: 0.45641 | val_1_rmse: 0.46536 |  0:00:43s
epoch 46 | loss: 0.22949 | val_0_rmse: 0.4839  | val_1_rmse: 0.48789 |  0:00:44s
epoch 47 | loss: 0.23604 | val_0_rmse: 0.46099 | val_1_rmse: 0.46731 |  0:00:45s
epoch 48 | loss: 0.22635 | val_0_rmse: 0.45873 | val_1_rmse: 0.46218 |  0:00:46s
epoch 49 | loss: 0.22488 | val_0_rmse: 0.45756 | val_1_rmse: 0.46668 |  0:00:47s
epoch 50 | loss: 0.22901 | val_0_rmse: 0.46997 | val_1_rmse: 0.4733  |  0:00:48s
epoch 51 | loss: 0.23782 | val_0_rmse: 0.49115 | val_1_rmse: 0.49955 |  0:00:49s
epoch 52 | loss: 0.22504 | val_0_rmse: 0.45621 | val_1_rmse: 0.46135 |  0:00:50s
epoch 53 | loss: 0.22575 | val_0_rmse: 0.4508  | val_1_rmse: 0.45844 |  0:00:51s
epoch 54 | loss: 0.2304  | val_0_rmse: 0.44949 | val_1_rmse: 0.45561 |  0:00:52s
epoch 55 | loss: 0.21973 | val_0_rmse: 0.46123 | val_1_rmse: 0.47271 |  0:00:53s
epoch 56 | loss: 0.22403 | val_0_rmse: 0.45045 | val_1_rmse: 0.46082 |  0:00:54s
epoch 57 | loss: 0.22077 | val_0_rmse: 0.44813 | val_1_rmse: 0.45893 |  0:00:55s
epoch 58 | loss: 0.21967 | val_0_rmse: 0.44132 | val_1_rmse: 0.4523  |  0:00:56s
epoch 59 | loss: 0.21425 | val_0_rmse: 0.44189 | val_1_rmse: 0.44867 |  0:00:57s
epoch 60 | loss: 0.21836 | val_0_rmse: 0.45559 | val_1_rmse: 0.46804 |  0:00:57s
epoch 61 | loss: 0.22265 | val_0_rmse: 0.4542  | val_1_rmse: 0.46376 |  0:00:58s
epoch 62 | loss: 0.22383 | val_0_rmse: 0.44671 | val_1_rmse: 0.45341 |  0:00:59s
epoch 63 | loss: 0.2173  | val_0_rmse: 0.44601 | val_1_rmse: 0.45595 |  0:01:00s
epoch 64 | loss: 0.2199  | val_0_rmse: 0.44452 | val_1_rmse: 0.45567 |  0:01:01s
epoch 65 | loss: 0.21557 | val_0_rmse: 0.45159 | val_1_rmse: 0.45922 |  0:01:02s
epoch 66 | loss: 0.2211  | val_0_rmse: 0.45188 | val_1_rmse: 0.46155 |  0:01:03s
epoch 67 | loss: 0.21745 | val_0_rmse: 0.44118 | val_1_rmse: 0.45076 |  0:01:04s
epoch 68 | loss: 0.21717 | val_0_rmse: 0.46261 | val_1_rmse: 0.46758 |  0:01:05s
epoch 69 | loss: 0.21607 | val_0_rmse: 0.44778 | val_1_rmse: 0.4559  |  0:01:06s
epoch 70 | loss: 0.22254 | val_0_rmse: 0.46547 | val_1_rmse: 0.4696  |  0:01:07s
epoch 71 | loss: 0.21971 | val_0_rmse: 0.45613 | val_1_rmse: 0.46217 |  0:01:08s
epoch 72 | loss: 0.22616 | val_0_rmse: 0.44685 | val_1_rmse: 0.4565  |  0:01:09s
epoch 73 | loss: 0.22118 | val_0_rmse: 0.44982 | val_1_rmse: 0.45495 |  0:01:10s
epoch 74 | loss: 0.22222 | val_0_rmse: 0.44862 | val_1_rmse: 0.45639 |  0:01:11s
epoch 75 | loss: 0.21508 | val_0_rmse: 0.43644 | val_1_rmse: 0.4483  |  0:01:12s
epoch 76 | loss: 0.21118 | val_0_rmse: 0.45117 | val_1_rmse: 0.45942 |  0:01:13s
epoch 77 | loss: 0.21421 | val_0_rmse: 0.445   | val_1_rmse: 0.45228 |  0:01:14s
epoch 78 | loss: 0.21827 | val_0_rmse: 0.448   | val_1_rmse: 0.45587 |  0:01:15s
epoch 79 | loss: 0.2148  | val_0_rmse: 0.4522  | val_1_rmse: 0.46023 |  0:01:16s
epoch 80 | loss: 0.2117  | val_0_rmse: 0.43889 | val_1_rmse: 0.44916 |  0:01:16s
epoch 81 | loss: 0.21144 | val_0_rmse: 0.44764 | val_1_rmse: 0.45417 |  0:01:17s
epoch 82 | loss: 0.21852 | val_0_rmse: 0.45417 | val_1_rmse: 0.46002 |  0:01:18s
epoch 83 | loss: 0.22609 | val_0_rmse: 0.45544 | val_1_rmse: 0.46049 |  0:01:19s
epoch 84 | loss: 0.22088 | val_0_rmse: 0.44838 | val_1_rmse: 0.44958 |  0:01:20s
epoch 85 | loss: 0.21097 | val_0_rmse: 0.45112 | val_1_rmse: 0.46151 |  0:01:21s
epoch 86 | loss: 0.21163 | val_0_rmse: 0.43873 | val_1_rmse: 0.45128 |  0:01:22s
epoch 87 | loss: 0.21512 | val_0_rmse: 0.43871 | val_1_rmse: 0.44595 |  0:01:23s
epoch 88 | loss: 0.21389 | val_0_rmse: 0.45819 | val_1_rmse: 0.46173 |  0:01:24s
epoch 89 | loss: 0.21264 | val_0_rmse: 0.44372 | val_1_rmse: 0.45544 |  0:01:25s
epoch 90 | loss: 0.20948 | val_0_rmse: 0.44306 | val_1_rmse: 0.44975 |  0:01:26s
epoch 91 | loss: 0.20869 | val_0_rmse: 0.44198 | val_1_rmse: 0.45126 |  0:01:27s
epoch 92 | loss: 0.20907 | val_0_rmse: 0.44433 | val_1_rmse: 0.45628 |  0:01:28s
epoch 93 | loss: 0.21288 | val_0_rmse: 0.44272 | val_1_rmse: 0.4493  |  0:01:29s
epoch 94 | loss: 0.21593 | val_0_rmse: 0.43757 | val_1_rmse: 0.44568 |  0:01:30s
epoch 95 | loss: 0.21993 | val_0_rmse: 0.44594 | val_1_rmse: 0.45997 |  0:01:31s
epoch 96 | loss: 0.23358 | val_0_rmse: 0.44843 | val_1_rmse: 0.46056 |  0:01:32s
epoch 97 | loss: 0.22238 | val_0_rmse: 0.45068 | val_1_rmse: 0.46303 |  0:01:33s
epoch 98 | loss: 0.21725 | val_0_rmse: 0.44371 | val_1_rmse: 0.45745 |  0:01:34s
epoch 99 | loss: 0.21603 | val_0_rmse: 0.46376 | val_1_rmse: 0.46909 |  0:01:35s
epoch 100| loss: 0.21788 | val_0_rmse: 0.45142 | val_1_rmse: 0.46012 |  0:01:35s
epoch 101| loss: 0.21862 | val_0_rmse: 0.45381 | val_1_rmse: 0.46443 |  0:01:36s
epoch 102| loss: 0.21908 | val_0_rmse: 0.43835 | val_1_rmse: 0.44663 |  0:01:37s
epoch 103| loss: 0.2176  | val_0_rmse: 0.44575 | val_1_rmse: 0.45361 |  0:01:38s
epoch 104| loss: 0.21007 | val_0_rmse: 0.44263 | val_1_rmse: 0.4507  |  0:01:39s
epoch 105| loss: 0.20884 | val_0_rmse: 0.45973 | val_1_rmse: 0.46583 |  0:01:40s
epoch 106| loss: 0.22196 | val_0_rmse: 0.45202 | val_1_rmse: 0.45633 |  0:01:41s
epoch 107| loss: 0.21982 | val_0_rmse: 0.44594 | val_1_rmse: 0.45642 |  0:01:42s
epoch 108| loss: 0.21499 | val_0_rmse: 0.44266 | val_1_rmse: 0.45098 |  0:01:43s
epoch 109| loss: 0.21144 | val_0_rmse: 0.437   | val_1_rmse: 0.44237 |  0:01:44s
epoch 110| loss: 0.21528 | val_0_rmse: 0.45196 | val_1_rmse: 0.45984 |  0:01:45s
epoch 111| loss: 0.21175 | val_0_rmse: 0.43277 | val_1_rmse: 0.44558 |  0:01:46s
epoch 112| loss: 0.20729 | val_0_rmse: 0.43196 | val_1_rmse: 0.44448 |  0:01:47s
epoch 113| loss: 0.20705 | val_0_rmse: 0.43735 | val_1_rmse: 0.44977 |  0:01:48s
epoch 114| loss: 0.20991 | val_0_rmse: 0.43055 | val_1_rmse: 0.44278 |  0:01:49s
epoch 115| loss: 0.2088  | val_0_rmse: 0.44944 | val_1_rmse: 0.45895 |  0:01:50s
epoch 116| loss: 0.2109  | val_0_rmse: 0.43121 | val_1_rmse: 0.4428  |  0:01:51s
epoch 117| loss: 0.20656 | val_0_rmse: 0.42821 | val_1_rmse: 0.4417  |  0:01:52s
epoch 118| loss: 0.2016  | val_0_rmse: 0.43065 | val_1_rmse: 0.44285 |  0:01:53s
epoch 119| loss: 0.20877 | val_0_rmse: 0.45322 | val_1_rmse: 0.46512 |  0:01:53s
epoch 120| loss: 0.20785 | val_0_rmse: 0.44223 | val_1_rmse: 0.46129 |  0:01:54s
epoch 121| loss: 0.21249 | val_0_rmse: 0.43621 | val_1_rmse: 0.45217 |  0:01:55s
epoch 122| loss: 0.20657 | val_0_rmse: 0.43206 | val_1_rmse: 0.44481 |  0:01:56s
epoch 123| loss: 0.21364 | val_0_rmse: 0.45103 | val_1_rmse: 0.46583 |  0:01:57s
epoch 124| loss: 0.21381 | val_0_rmse: 0.44005 | val_1_rmse: 0.45022 |  0:01:58s
epoch 125| loss: 0.21208 | val_0_rmse: 0.45318 | val_1_rmse: 0.46312 |  0:01:59s
epoch 126| loss: 0.20759 | val_0_rmse: 0.43779 | val_1_rmse: 0.45004 |  0:02:00s
epoch 127| loss: 0.20853 | val_0_rmse: 0.43597 | val_1_rmse: 0.44864 |  0:02:01s
epoch 128| loss: 0.20436 | val_0_rmse: 0.46487 | val_1_rmse: 0.47666 |  0:02:02s
epoch 129| loss: 0.2185  | val_0_rmse: 0.43605 | val_1_rmse: 0.44665 |  0:02:03s
epoch 130| loss: 0.21237 | val_0_rmse: 0.43957 | val_1_rmse: 0.44777 |  0:02:04s
epoch 131| loss: 0.21727 | val_0_rmse: 0.43613 | val_1_rmse: 0.44793 |  0:02:05s
epoch 132| loss: 0.21092 | val_0_rmse: 0.43905 | val_1_rmse: 0.45239 |  0:02:06s
epoch 133| loss: 0.2117  | val_0_rmse: 0.43154 | val_1_rmse: 0.44721 |  0:02:07s
epoch 134| loss: 0.21365 | val_0_rmse: 0.43489 | val_1_rmse: 0.4515  |  0:02:08s
epoch 135| loss: 0.20997 | val_0_rmse: 0.44071 | val_1_rmse: 0.45039 |  0:02:09s
epoch 136| loss: 0.21018 | val_0_rmse: 0.44687 | val_1_rmse: 0.4582  |  0:02:10s
epoch 137| loss: 0.20488 | val_0_rmse: 0.4317  | val_1_rmse: 0.44618 |  0:02:11s
epoch 138| loss: 0.20163 | val_0_rmse: 0.43757 | val_1_rmse: 0.44874 |  0:02:11s
epoch 139| loss: 0.20712 | val_0_rmse: 0.42564 | val_1_rmse: 0.44666 |  0:02:12s
epoch 140| loss: 0.20706 | val_0_rmse: 0.4275  | val_1_rmse: 0.44652 |  0:02:13s
epoch 141| loss: 0.20467 | val_0_rmse: 0.43045 | val_1_rmse: 0.44915 |  0:02:14s
epoch 142| loss: 0.21101 | val_0_rmse: 0.43168 | val_1_rmse: 0.44376 |  0:02:15s
epoch 143| loss: 0.20949 | val_0_rmse: 0.43449 | val_1_rmse: 0.44568 |  0:02:16s
epoch 144| loss: 0.20743 | val_0_rmse: 0.43687 | val_1_rmse: 0.45193 |  0:02:17s
epoch 145| loss: 0.2016  | val_0_rmse: 0.433   | val_1_rmse: 0.44913 |  0:02:18s
epoch 146| loss: 0.20436 | val_0_rmse: 0.43754 | val_1_rmse: 0.45326 |  0:02:19s
epoch 147| loss: 0.20642 | val_0_rmse: 0.44583 | val_1_rmse: 0.459   |  0:02:20s

Early stopping occured at epoch 147 with best_epoch = 117 and best_val_1_rmse = 0.4417
Best weights from best epoch are automatically used!
ended training at: 08:14:09
Feature importance:
[('Area', 0.4025619319343606), ('Baths', 0.05894161028799722), ('Beds', 0.034916829508093564), ('Latitude', 0.2952595384483915), ('Longitude', 0.09389638067494101), ('Month', 0.012346063967489005), ('Year', 0.10207764517872711)]
Mean squared error is of 6396512932.987707
Mean absolute error:57021.24863997232
MAPE:0.14986005085391294
R2 score:0.7945028856468431
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:14:09
epoch 0  | loss: 0.6947  | val_0_rmse: 0.71784 | val_1_rmse: 0.70827 |  0:00:00s
epoch 1  | loss: 0.36612 | val_0_rmse: 0.66877 | val_1_rmse: 0.67394 |  0:00:01s
epoch 2  | loss: 0.33228 | val_0_rmse: 0.55534 | val_1_rmse: 0.55629 |  0:00:02s
epoch 3  | loss: 0.31189 | val_0_rmse: 0.54132 | val_1_rmse: 0.54216 |  0:00:03s
epoch 4  | loss: 0.3062  | val_0_rmse: 0.55387 | val_1_rmse: 0.55217 |  0:00:04s
epoch 5  | loss: 0.29648 | val_0_rmse: 0.53398 | val_1_rmse: 0.53161 |  0:00:05s
epoch 6  | loss: 0.29798 | val_0_rmse: 0.51921 | val_1_rmse: 0.51641 |  0:00:06s
epoch 7  | loss: 0.2858  | val_0_rmse: 0.51649 | val_1_rmse: 0.51158 |  0:00:07s
epoch 8  | loss: 0.28009 | val_0_rmse: 0.52484 | val_1_rmse: 0.5241  |  0:00:08s
epoch 9  | loss: 0.28384 | val_0_rmse: 0.53677 | val_1_rmse: 0.53565 |  0:00:09s
epoch 10 | loss: 0.28595 | val_0_rmse: 0.51266 | val_1_rmse: 0.51207 |  0:00:10s
epoch 11 | loss: 0.2804  | val_0_rmse: 0.51885 | val_1_rmse: 0.51736 |  0:00:11s
epoch 12 | loss: 0.27502 | val_0_rmse: 0.52719 | val_1_rmse: 0.52616 |  0:00:12s
epoch 13 | loss: 0.28276 | val_0_rmse: 0.51269 | val_1_rmse: 0.50849 |  0:00:13s
epoch 14 | loss: 0.27682 | val_0_rmse: 0.52111 | val_1_rmse: 0.52063 |  0:00:14s
epoch 15 | loss: 0.27382 | val_0_rmse: 0.52993 | val_1_rmse: 0.52688 |  0:00:15s
epoch 16 | loss: 0.27679 | val_0_rmse: 0.50472 | val_1_rmse: 0.50597 |  0:00:16s
epoch 17 | loss: 0.27102 | val_0_rmse: 0.51012 | val_1_rmse: 0.50613 |  0:00:17s
epoch 18 | loss: 0.2747  | val_0_rmse: 0.51122 | val_1_rmse: 0.50786 |  0:00:18s
epoch 19 | loss: 0.27152 | val_0_rmse: 0.51898 | val_1_rmse: 0.51455 |  0:00:19s
epoch 20 | loss: 0.27744 | val_0_rmse: 0.50753 | val_1_rmse: 0.50582 |  0:00:19s
epoch 21 | loss: 0.27106 | val_0_rmse: 0.51686 | val_1_rmse: 0.51294 |  0:00:20s
epoch 22 | loss: 0.27461 | val_0_rmse: 0.51409 | val_1_rmse: 0.51291 |  0:00:21s
epoch 23 | loss: 0.27359 | val_0_rmse: 0.506   | val_1_rmse: 0.50501 |  0:00:22s
epoch 24 | loss: 0.26711 | val_0_rmse: 0.50237 | val_1_rmse: 0.50033 |  0:00:23s
epoch 25 | loss: 0.26608 | val_0_rmse: 0.50363 | val_1_rmse: 0.50388 |  0:00:24s
epoch 26 | loss: 0.26565 | val_0_rmse: 0.50412 | val_1_rmse: 0.50429 |  0:00:25s
epoch 27 | loss: 0.2654  | val_0_rmse: 0.50417 | val_1_rmse: 0.50253 |  0:00:26s
epoch 28 | loss: 0.26262 | val_0_rmse: 0.49921 | val_1_rmse: 0.49664 |  0:00:27s
epoch 29 | loss: 0.26914 | val_0_rmse: 0.49866 | val_1_rmse: 0.49667 |  0:00:28s
epoch 30 | loss: 0.26426 | val_0_rmse: 0.50244 | val_1_rmse: 0.50061 |  0:00:29s
epoch 31 | loss: 0.26111 | val_0_rmse: 0.49157 | val_1_rmse: 0.4948  |  0:00:30s
epoch 32 | loss: 0.25887 | val_0_rmse: 0.50099 | val_1_rmse: 0.50229 |  0:00:31s
epoch 33 | loss: 0.26329 | val_0_rmse: 0.52049 | val_1_rmse: 0.518   |  0:00:32s
epoch 34 | loss: 0.26134 | val_0_rmse: 0.51196 | val_1_rmse: 0.51428 |  0:00:33s
epoch 35 | loss: 0.2589  | val_0_rmse: 0.4986  | val_1_rmse: 0.49834 |  0:00:34s
epoch 36 | loss: 0.25617 | val_0_rmse: 0.50185 | val_1_rmse: 0.50087 |  0:00:35s
epoch 37 | loss: 0.24958 | val_0_rmse: 0.49756 | val_1_rmse: 0.50207 |  0:00:36s
epoch 38 | loss: 0.25464 | val_0_rmse: 0.49115 | val_1_rmse: 0.49242 |  0:00:37s
epoch 39 | loss: 0.25201 | val_0_rmse: 0.48242 | val_1_rmse: 0.48265 |  0:00:38s
epoch 40 | loss: 0.2507  | val_0_rmse: 0.48143 | val_1_rmse: 0.48184 |  0:00:38s
epoch 41 | loss: 0.24421 | val_0_rmse: 0.49626 | val_1_rmse: 0.49755 |  0:00:39s
epoch 42 | loss: 0.24143 | val_0_rmse: 0.49011 | val_1_rmse: 0.49257 |  0:00:40s
epoch 43 | loss: 0.24396 | val_0_rmse: 0.47721 | val_1_rmse: 0.47628 |  0:00:41s
epoch 44 | loss: 0.23647 | val_0_rmse: 0.47293 | val_1_rmse: 0.47144 |  0:00:42s
epoch 45 | loss: 0.24057 | val_0_rmse: 0.47573 | val_1_rmse: 0.4735  |  0:00:43s
epoch 46 | loss: 0.234   | val_0_rmse: 0.47452 | val_1_rmse: 0.47407 |  0:00:44s
epoch 47 | loss: 0.23311 | val_0_rmse: 0.4648  | val_1_rmse: 0.46328 |  0:00:45s
epoch 48 | loss: 0.23655 | val_0_rmse: 0.47401 | val_1_rmse: 0.47411 |  0:00:46s
epoch 49 | loss: 0.2376  | val_0_rmse: 0.48341 | val_1_rmse: 0.48655 |  0:00:47s
epoch 50 | loss: 0.23746 | val_0_rmse: 0.4783  | val_1_rmse: 0.47911 |  0:00:48s
epoch 51 | loss: 0.22989 | val_0_rmse: 0.47029 | val_1_rmse: 0.47314 |  0:00:49s
epoch 52 | loss: 0.23029 | val_0_rmse: 0.45812 | val_1_rmse: 0.46336 |  0:00:50s
epoch 53 | loss: 0.24034 | val_0_rmse: 0.49889 | val_1_rmse: 0.49776 |  0:00:51s
epoch 54 | loss: 0.23966 | val_0_rmse: 0.46865 | val_1_rmse: 0.46914 |  0:00:52s
epoch 55 | loss: 0.2302  | val_0_rmse: 0.4753  | val_1_rmse: 0.47591 |  0:00:53s
epoch 56 | loss: 0.23154 | val_0_rmse: 0.47887 | val_1_rmse: 0.48145 |  0:00:54s
epoch 57 | loss: 0.23781 | val_0_rmse: 0.47568 | val_1_rmse: 0.46959 |  0:00:55s
epoch 58 | loss: 0.23089 | val_0_rmse: 0.46251 | val_1_rmse: 0.45754 |  0:00:55s
epoch 59 | loss: 0.23382 | val_0_rmse: 0.47848 | val_1_rmse: 0.47419 |  0:00:56s
epoch 60 | loss: 0.23544 | val_0_rmse: 0.47151 | val_1_rmse: 0.46952 |  0:00:57s
epoch 61 | loss: 0.23277 | val_0_rmse: 0.46183 | val_1_rmse: 0.46192 |  0:00:58s
epoch 62 | loss: 0.22837 | val_0_rmse: 0.47741 | val_1_rmse: 0.47237 |  0:00:59s
epoch 63 | loss: 0.22417 | val_0_rmse: 0.46307 | val_1_rmse: 0.46557 |  0:01:00s
epoch 64 | loss: 0.22969 | val_0_rmse: 0.48045 | val_1_rmse: 0.479   |  0:01:01s
epoch 65 | loss: 0.23876 | val_0_rmse: 0.45537 | val_1_rmse: 0.45604 |  0:01:02s
epoch 66 | loss: 0.22736 | val_0_rmse: 0.47054 | val_1_rmse: 0.47197 |  0:01:03s
epoch 67 | loss: 0.22718 | val_0_rmse: 0.47489 | val_1_rmse: 0.47986 |  0:01:04s
epoch 68 | loss: 0.23543 | val_0_rmse: 0.46435 | val_1_rmse: 0.46846 |  0:01:05s
epoch 69 | loss: 0.23091 | val_0_rmse: 0.47367 | val_1_rmse: 0.47548 |  0:01:06s
epoch 70 | loss: 0.22534 | val_0_rmse: 0.47497 | val_1_rmse: 0.47671 |  0:01:07s
epoch 71 | loss: 0.22438 | val_0_rmse: 0.46095 | val_1_rmse: 0.46324 |  0:01:08s
epoch 72 | loss: 0.22169 | val_0_rmse: 0.48409 | val_1_rmse: 0.48104 |  0:01:09s
epoch 73 | loss: 0.22895 | val_0_rmse: 0.46252 | val_1_rmse: 0.4656  |  0:01:10s
epoch 74 | loss: 0.22582 | val_0_rmse: 0.45602 | val_1_rmse: 0.45756 |  0:01:11s
epoch 75 | loss: 0.23107 | val_0_rmse: 0.46635 | val_1_rmse: 0.47068 |  0:01:12s
epoch 76 | loss: 0.22759 | val_0_rmse: 0.47941 | val_1_rmse: 0.4786  |  0:01:13s
epoch 77 | loss: 0.22381 | val_0_rmse: 0.46057 | val_1_rmse: 0.46189 |  0:01:13s
epoch 78 | loss: 0.22625 | val_0_rmse: 0.48576 | val_1_rmse: 0.49114 |  0:01:14s
epoch 79 | loss: 0.22759 | val_0_rmse: 0.4584  | val_1_rmse: 0.4632  |  0:01:15s
epoch 80 | loss: 0.22326 | val_0_rmse: 0.4662  | val_1_rmse: 0.47343 |  0:01:16s
epoch 81 | loss: 0.22136 | val_0_rmse: 0.45758 | val_1_rmse: 0.45984 |  0:01:17s
epoch 82 | loss: 0.22225 | val_0_rmse: 0.44819 | val_1_rmse: 0.45592 |  0:01:18s
epoch 83 | loss: 0.22062 | val_0_rmse: 0.46151 | val_1_rmse: 0.46451 |  0:01:19s
epoch 84 | loss: 0.22364 | val_0_rmse: 0.45233 | val_1_rmse: 0.45536 |  0:01:20s
epoch 85 | loss: 0.21641 | val_0_rmse: 0.44666 | val_1_rmse: 0.45164 |  0:01:21s
epoch 86 | loss: 0.21833 | val_0_rmse: 0.45782 | val_1_rmse: 0.46497 |  0:01:22s
epoch 87 | loss: 0.22238 | val_0_rmse: 0.45139 | val_1_rmse: 0.46066 |  0:01:23s
epoch 88 | loss: 0.21793 | val_0_rmse: 0.45731 | val_1_rmse: 0.46348 |  0:01:24s
epoch 89 | loss: 0.22486 | val_0_rmse: 0.44743 | val_1_rmse: 0.44965 |  0:01:25s
epoch 90 | loss: 0.22792 | val_0_rmse: 0.44637 | val_1_rmse: 0.45071 |  0:01:26s
epoch 91 | loss: 0.2229  | val_0_rmse: 0.46737 | val_1_rmse: 0.47234 |  0:01:27s
epoch 92 | loss: 0.22101 | val_0_rmse: 0.46698 | val_1_rmse: 0.46811 |  0:01:28s
epoch 93 | loss: 0.22282 | val_0_rmse: 0.45069 | val_1_rmse: 0.45284 |  0:01:29s
epoch 94 | loss: 0.2215  | val_0_rmse: 0.47194 | val_1_rmse: 0.47235 |  0:01:30s
epoch 95 | loss: 0.23301 | val_0_rmse: 0.47061 | val_1_rmse: 0.46968 |  0:01:31s
epoch 96 | loss: 0.22092 | val_0_rmse: 0.45546 | val_1_rmse: 0.4579  |  0:01:32s
epoch 97 | loss: 0.22163 | val_0_rmse: 0.45295 | val_1_rmse: 0.45991 |  0:01:32s
epoch 98 | loss: 0.21755 | val_0_rmse: 0.44696 | val_1_rmse: 0.44989 |  0:01:33s
epoch 99 | loss: 0.21939 | val_0_rmse: 0.45779 | val_1_rmse: 0.46656 |  0:01:34s
epoch 100| loss: 0.22291 | val_0_rmse: 0.44093 | val_1_rmse: 0.45003 |  0:01:35s
epoch 101| loss: 0.21558 | val_0_rmse: 0.44852 | val_1_rmse: 0.45344 |  0:01:36s
epoch 102| loss: 0.21877 | val_0_rmse: 0.45601 | val_1_rmse: 0.4604  |  0:01:37s
epoch 103| loss: 0.22445 | val_0_rmse: 0.44801 | val_1_rmse: 0.45211 |  0:01:38s
epoch 104| loss: 0.21582 | val_0_rmse: 0.44729 | val_1_rmse: 0.45096 |  0:01:39s
epoch 105| loss: 0.21358 | val_0_rmse: 0.44836 | val_1_rmse: 0.45752 |  0:01:40s
epoch 106| loss: 0.21996 | val_0_rmse: 0.46491 | val_1_rmse: 0.46777 |  0:01:41s
epoch 107| loss: 0.22347 | val_0_rmse: 0.47967 | val_1_rmse: 0.48508 |  0:01:42s
epoch 108| loss: 0.22301 | val_0_rmse: 0.46736 | val_1_rmse: 0.47423 |  0:01:43s
epoch 109| loss: 0.22307 | val_0_rmse: 0.46111 | val_1_rmse: 0.47505 |  0:01:44s
epoch 110| loss: 0.22077 | val_0_rmse: 0.45855 | val_1_rmse: 0.47196 |  0:01:45s
epoch 111| loss: 0.22511 | val_0_rmse: 0.45474 | val_1_rmse: 0.46416 |  0:01:46s
epoch 112| loss: 0.22103 | val_0_rmse: 0.45302 | val_1_rmse: 0.46061 |  0:01:47s
epoch 113| loss: 0.21761 | val_0_rmse: 0.45608 | val_1_rmse: 0.45928 |  0:01:48s
epoch 114| loss: 0.2209  | val_0_rmse: 0.46711 | val_1_rmse: 0.46951 |  0:01:49s
epoch 115| loss: 0.21306 | val_0_rmse: 0.45145 | val_1_rmse: 0.46002 |  0:01:50s
epoch 116| loss: 0.21622 | val_0_rmse: 0.45536 | val_1_rmse: 0.4684  |  0:01:50s
epoch 117| loss: 0.21359 | val_0_rmse: 0.45786 | val_1_rmse: 0.4638  |  0:01:51s
epoch 118| loss: 0.21002 | val_0_rmse: 0.44484 | val_1_rmse: 0.4512  |  0:01:52s
epoch 119| loss: 0.21431 | val_0_rmse: 0.45908 | val_1_rmse: 0.46798 |  0:01:53s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.44965
Best weights from best epoch are automatically used!
ended training at: 08:16:03
Feature importance:
[('Area', 0.4318074481184164), ('Baths', 0.1025736885610311), ('Beds', 0.018109636065276442), ('Latitude', 0.2966270445046301), ('Longitude', 0.15088218275064588), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 6160353146.221044
Mean absolute error:56410.911700396326
MAPE:0.14906962819294514
R2 score:0.7998042656367247
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:16:03
epoch 0  | loss: 0.62405 | val_0_rmse: 0.69727 | val_1_rmse: 0.7205  |  0:00:00s
epoch 1  | loss: 0.36421 | val_0_rmse: 0.60927 | val_1_rmse: 0.62682 |  0:00:01s
epoch 2  | loss: 0.33289 | val_0_rmse: 0.56737 | val_1_rmse: 0.59059 |  0:00:02s
epoch 3  | loss: 0.31095 | val_0_rmse: 0.53581 | val_1_rmse: 0.55835 |  0:00:03s
epoch 4  | loss: 0.30052 | val_0_rmse: 0.53051 | val_1_rmse: 0.54804 |  0:00:04s
epoch 5  | loss: 0.29321 | val_0_rmse: 0.53015 | val_1_rmse: 0.54868 |  0:00:05s
epoch 6  | loss: 0.29056 | val_0_rmse: 0.52055 | val_1_rmse: 0.53654 |  0:00:06s
epoch 7  | loss: 0.28625 | val_0_rmse: 0.53803 | val_1_rmse: 0.55303 |  0:00:07s
epoch 8  | loss: 0.27916 | val_0_rmse: 0.53022 | val_1_rmse: 0.54658 |  0:00:08s
epoch 9  | loss: 0.27786 | val_0_rmse: 0.51479 | val_1_rmse: 0.52948 |  0:00:09s
epoch 10 | loss: 0.27328 | val_0_rmse: 0.51502 | val_1_rmse: 0.53072 |  0:00:10s
epoch 11 | loss: 0.26761 | val_0_rmse: 0.51938 | val_1_rmse: 0.53246 |  0:00:11s
epoch 12 | loss: 0.26738 | val_0_rmse: 0.50535 | val_1_rmse: 0.52151 |  0:00:12s
epoch 13 | loss: 0.26031 | val_0_rmse: 0.50006 | val_1_rmse: 0.5156  |  0:00:13s
epoch 14 | loss: 0.26568 | val_0_rmse: 0.52006 | val_1_rmse: 0.53028 |  0:00:14s
epoch 15 | loss: 0.26441 | val_0_rmse: 0.49479 | val_1_rmse: 0.50887 |  0:00:15s
epoch 16 | loss: 0.25623 | val_0_rmse: 0.49693 | val_1_rmse: 0.51217 |  0:00:16s
epoch 17 | loss: 0.2627  | val_0_rmse: 0.49773 | val_1_rmse: 0.50906 |  0:00:17s
epoch 18 | loss: 0.25724 | val_0_rmse: 0.5006  | val_1_rmse: 0.51247 |  0:00:18s
epoch 19 | loss: 0.25797 | val_0_rmse: 0.49588 | val_1_rmse: 0.50701 |  0:00:19s
epoch 20 | loss: 0.25697 | val_0_rmse: 0.48599 | val_1_rmse: 0.49773 |  0:00:19s
epoch 21 | loss: 0.25044 | val_0_rmse: 0.48232 | val_1_rmse: 0.49276 |  0:00:20s
epoch 22 | loss: 0.24982 | val_0_rmse: 0.48154 | val_1_rmse: 0.49504 |  0:00:21s
epoch 23 | loss: 0.25081 | val_0_rmse: 0.49371 | val_1_rmse: 0.50791 |  0:00:22s
epoch 24 | loss: 0.24971 | val_0_rmse: 0.49241 | val_1_rmse: 0.50642 |  0:00:23s
epoch 25 | loss: 0.24289 | val_0_rmse: 0.48085 | val_1_rmse: 0.49801 |  0:00:24s
epoch 26 | loss: 0.24658 | val_0_rmse: 0.47473 | val_1_rmse: 0.48749 |  0:00:25s
epoch 27 | loss: 0.23572 | val_0_rmse: 0.47242 | val_1_rmse: 0.4871  |  0:00:26s
epoch 28 | loss: 0.23796 | val_0_rmse: 0.47673 | val_1_rmse: 0.49269 |  0:00:27s
epoch 29 | loss: 0.23162 | val_0_rmse: 0.46156 | val_1_rmse: 0.48011 |  0:00:28s
epoch 30 | loss: 0.23231 | val_0_rmse: 0.45975 | val_1_rmse: 0.47312 |  0:00:29s
epoch 31 | loss: 0.22521 | val_0_rmse: 0.45961 | val_1_rmse: 0.47336 |  0:00:30s
epoch 32 | loss: 0.22208 | val_0_rmse: 0.46781 | val_1_rmse: 0.48138 |  0:00:31s
epoch 33 | loss: 0.22357 | val_0_rmse: 0.45322 | val_1_rmse: 0.47257 |  0:00:32s
epoch 34 | loss: 0.23375 | val_0_rmse: 0.44935 | val_1_rmse: 0.4699  |  0:00:33s
epoch 35 | loss: 0.22673 | val_0_rmse: 0.44446 | val_1_rmse: 0.46049 |  0:00:34s
epoch 36 | loss: 0.21862 | val_0_rmse: 0.45469 | val_1_rmse: 0.46882 |  0:00:35s
epoch 37 | loss: 0.21917 | val_0_rmse: 0.44812 | val_1_rmse: 0.47115 |  0:00:36s
epoch 38 | loss: 0.21744 | val_0_rmse: 0.43937 | val_1_rmse: 0.45636 |  0:00:37s
epoch 39 | loss: 0.21711 | val_0_rmse: 0.48576 | val_1_rmse: 0.49652 |  0:00:37s
epoch 40 | loss: 0.22534 | val_0_rmse: 0.45747 | val_1_rmse: 0.47326 |  0:00:38s
epoch 41 | loss: 0.23888 | val_0_rmse: 0.48648 | val_1_rmse: 0.50213 |  0:00:39s
epoch 42 | loss: 0.2342  | val_0_rmse: 0.45084 | val_1_rmse: 0.4714  |  0:00:40s
epoch 43 | loss: 0.22171 | val_0_rmse: 0.44746 | val_1_rmse: 0.46776 |  0:00:41s
epoch 44 | loss: 0.22171 | val_0_rmse: 0.44359 | val_1_rmse: 0.46558 |  0:00:42s
epoch 45 | loss: 0.21796 | val_0_rmse: 0.45069 | val_1_rmse: 0.46714 |  0:00:43s
epoch 46 | loss: 0.21912 | val_0_rmse: 0.44874 | val_1_rmse: 0.46635 |  0:00:44s
epoch 47 | loss: 0.215   | val_0_rmse: 0.45651 | val_1_rmse: 0.47109 |  0:00:45s
epoch 48 | loss: 0.22076 | val_0_rmse: 0.45595 | val_1_rmse: 0.47812 |  0:00:46s
epoch 49 | loss: 0.22096 | val_0_rmse: 0.46283 | val_1_rmse: 0.47787 |  0:00:47s
epoch 50 | loss: 0.21439 | val_0_rmse: 0.44434 | val_1_rmse: 0.46251 |  0:00:48s
epoch 51 | loss: 0.2124  | val_0_rmse: 0.44462 | val_1_rmse: 0.46641 |  0:00:49s
epoch 52 | loss: 0.21462 | val_0_rmse: 0.44669 | val_1_rmse: 0.46625 |  0:00:50s
epoch 53 | loss: 0.21857 | val_0_rmse: 0.43963 | val_1_rmse: 0.46183 |  0:00:51s
epoch 54 | loss: 0.21277 | val_0_rmse: 0.44461 | val_1_rmse: 0.46291 |  0:00:52s
epoch 55 | loss: 0.21009 | val_0_rmse: 0.45407 | val_1_rmse: 0.47187 |  0:00:53s
epoch 56 | loss: 0.21178 | val_0_rmse: 0.44588 | val_1_rmse: 0.46491 |  0:00:53s
epoch 57 | loss: 0.21461 | val_0_rmse: 0.44266 | val_1_rmse: 0.4619  |  0:00:54s
epoch 58 | loss: 0.21042 | val_0_rmse: 0.42949 | val_1_rmse: 0.45266 |  0:00:55s
epoch 59 | loss: 0.20765 | val_0_rmse: 0.43484 | val_1_rmse: 0.45355 |  0:00:56s
epoch 60 | loss: 0.21011 | val_0_rmse: 0.44784 | val_1_rmse: 0.46431 |  0:00:57s
epoch 61 | loss: 0.21656 | val_0_rmse: 0.45217 | val_1_rmse: 0.47264 |  0:00:58s
epoch 62 | loss: 0.21014 | val_0_rmse: 0.43901 | val_1_rmse: 0.46289 |  0:00:59s
epoch 63 | loss: 0.20842 | val_0_rmse: 0.43451 | val_1_rmse: 0.45937 |  0:01:00s
epoch 64 | loss: 0.21436 | val_0_rmse: 0.43255 | val_1_rmse: 0.45226 |  0:01:01s
epoch 65 | loss: 0.2083  | val_0_rmse: 0.4466  | val_1_rmse: 0.46651 |  0:01:02s
epoch 66 | loss: 0.21318 | val_0_rmse: 0.47576 | val_1_rmse: 0.49245 |  0:01:03s
epoch 67 | loss: 0.2188  | val_0_rmse: 0.45067 | val_1_rmse: 0.46906 |  0:01:04s
epoch 68 | loss: 0.21341 | val_0_rmse: 0.43841 | val_1_rmse: 0.46284 |  0:01:05s
epoch 69 | loss: 0.21004 | val_0_rmse: 0.42937 | val_1_rmse: 0.45058 |  0:01:06s
epoch 70 | loss: 0.20397 | val_0_rmse: 0.44255 | val_1_rmse: 0.46363 |  0:01:07s
epoch 71 | loss: 0.2095  | val_0_rmse: 0.44034 | val_1_rmse: 0.46131 |  0:01:08s
epoch 72 | loss: 0.2102  | val_0_rmse: 0.44285 | val_1_rmse: 0.46838 |  0:01:09s
epoch 73 | loss: 0.20909 | val_0_rmse: 0.44361 | val_1_rmse: 0.46984 |  0:01:10s
epoch 74 | loss: 0.21313 | val_0_rmse: 0.44197 | val_1_rmse: 0.46375 |  0:01:11s
epoch 75 | loss: 0.20938 | val_0_rmse: 0.43518 | val_1_rmse: 0.4574  |  0:01:11s
epoch 76 | loss: 0.20879 | val_0_rmse: 0.43835 | val_1_rmse: 0.45969 |  0:01:12s
epoch 77 | loss: 0.20703 | val_0_rmse: 0.42712 | val_1_rmse: 0.45071 |  0:01:13s
epoch 78 | loss: 0.20611 | val_0_rmse: 0.4332  | val_1_rmse: 0.45343 |  0:01:14s
epoch 79 | loss: 0.20111 | val_0_rmse: 0.43249 | val_1_rmse: 0.45668 |  0:01:15s
epoch 80 | loss: 0.2102  | val_0_rmse: 0.43258 | val_1_rmse: 0.46294 |  0:01:16s
epoch 81 | loss: 0.21416 | val_0_rmse: 0.44713 | val_1_rmse: 0.47362 |  0:01:17s
epoch 82 | loss: 0.20669 | val_0_rmse: 0.4239  | val_1_rmse: 0.44712 |  0:01:18s
epoch 83 | loss: 0.21087 | val_0_rmse: 0.42832 | val_1_rmse: 0.45632 |  0:01:19s
epoch 84 | loss: 0.21334 | val_0_rmse: 0.43329 | val_1_rmse: 0.45646 |  0:01:20s
epoch 85 | loss: 0.20876 | val_0_rmse: 0.43905 | val_1_rmse: 0.45786 |  0:01:21s
epoch 86 | loss: 0.20789 | val_0_rmse: 0.43144 | val_1_rmse: 0.4587  |  0:01:22s
epoch 87 | loss: 0.20683 | val_0_rmse: 0.43065 | val_1_rmse: 0.45721 |  0:01:23s
epoch 88 | loss: 0.20366 | val_0_rmse: 0.4385  | val_1_rmse: 0.46454 |  0:01:24s
epoch 89 | loss: 0.19939 | val_0_rmse: 0.42651 | val_1_rmse: 0.45092 |  0:01:25s
epoch 90 | loss: 0.20038 | val_0_rmse: 0.42408 | val_1_rmse: 0.45134 |  0:01:26s
epoch 91 | loss: 0.1982  | val_0_rmse: 0.44033 | val_1_rmse: 0.46332 |  0:01:27s
epoch 92 | loss: 0.19713 | val_0_rmse: 0.42646 | val_1_rmse: 0.45364 |  0:01:27s
epoch 93 | loss: 0.20185 | val_0_rmse: 0.4266  | val_1_rmse: 0.45677 |  0:01:28s
epoch 94 | loss: 0.20492 | val_0_rmse: 0.42982 | val_1_rmse: 0.45502 |  0:01:29s
epoch 95 | loss: 0.20482 | val_0_rmse: 0.43699 | val_1_rmse: 0.46334 |  0:01:30s
epoch 96 | loss: 0.19797 | val_0_rmse: 0.42935 | val_1_rmse: 0.45746 |  0:01:31s
epoch 97 | loss: 0.20761 | val_0_rmse: 0.42552 | val_1_rmse: 0.45051 |  0:01:32s
epoch 98 | loss: 0.21213 | val_0_rmse: 0.42473 | val_1_rmse: 0.45096 |  0:01:33s
epoch 99 | loss: 0.20394 | val_0_rmse: 0.4486  | val_1_rmse: 0.47601 |  0:01:34s
epoch 100| loss: 0.19944 | val_0_rmse: 0.42547 | val_1_rmse: 0.45333 |  0:01:35s
epoch 101| loss: 0.19954 | val_0_rmse: 0.43743 | val_1_rmse: 0.46587 |  0:01:36s
epoch 102| loss: 0.20512 | val_0_rmse: 0.43161 | val_1_rmse: 0.45801 |  0:01:37s
epoch 103| loss: 0.20073 | val_0_rmse: 0.4257  | val_1_rmse: 0.4508  |  0:01:38s
epoch 104| loss: 0.19876 | val_0_rmse: 0.42432 | val_1_rmse: 0.45241 |  0:01:39s
epoch 105| loss: 0.20034 | val_0_rmse: 0.42709 | val_1_rmse: 0.45464 |  0:01:40s
epoch 106| loss: 0.19904 | val_0_rmse: 0.42192 | val_1_rmse: 0.45303 |  0:01:41s
epoch 107| loss: 0.20021 | val_0_rmse: 0.41803 | val_1_rmse: 0.44696 |  0:01:42s
epoch 108| loss: 0.1967  | val_0_rmse: 0.43079 | val_1_rmse: 0.45723 |  0:01:43s
epoch 109| loss: 0.19836 | val_0_rmse: 0.44051 | val_1_rmse: 0.47271 |  0:01:44s
epoch 110| loss: 0.20314 | val_0_rmse: 0.44219 | val_1_rmse: 0.46723 |  0:01:44s
epoch 111| loss: 0.20295 | val_0_rmse: 0.42487 | val_1_rmse: 0.4519  |  0:01:45s
epoch 112| loss: 0.19805 | val_0_rmse: 0.42314 | val_1_rmse: 0.45296 |  0:01:46s
epoch 113| loss: 0.20527 | val_0_rmse: 0.42794 | val_1_rmse: 0.45286 |  0:01:47s
epoch 114| loss: 0.20502 | val_0_rmse: 0.44834 | val_1_rmse: 0.46985 |  0:01:48s
epoch 115| loss: 0.20879 | val_0_rmse: 0.42115 | val_1_rmse: 0.44977 |  0:01:49s
epoch 116| loss: 0.19662 | val_0_rmse: 0.42705 | val_1_rmse: 0.45226 |  0:01:50s
epoch 117| loss: 0.20069 | val_0_rmse: 0.42391 | val_1_rmse: 0.45804 |  0:01:51s
epoch 118| loss: 0.20064 | val_0_rmse: 0.45808 | val_1_rmse: 0.48866 |  0:01:52s
epoch 119| loss: 0.20754 | val_0_rmse: 0.42192 | val_1_rmse: 0.45403 |  0:01:53s
epoch 120| loss: 0.19998 | val_0_rmse: 0.42263 | val_1_rmse: 0.45758 |  0:01:54s
epoch 121| loss: 0.19501 | val_0_rmse: 0.42667 | val_1_rmse: 0.45936 |  0:01:55s
epoch 122| loss: 0.19615 | val_0_rmse: 0.42412 | val_1_rmse: 0.45407 |  0:01:56s
epoch 123| loss: 0.20199 | val_0_rmse: 0.42853 | val_1_rmse: 0.45805 |  0:01:57s
epoch 124| loss: 0.19631 | val_0_rmse: 0.4239  | val_1_rmse: 0.45702 |  0:01:58s
epoch 125| loss: 0.20103 | val_0_rmse: 0.43004 | val_1_rmse: 0.46498 |  0:01:59s
epoch 126| loss: 0.1982  | val_0_rmse: 0.42193 | val_1_rmse: 0.45894 |  0:02:00s
epoch 127| loss: 0.19531 | val_0_rmse: 0.43368 | val_1_rmse: 0.46019 |  0:02:00s
epoch 128| loss: 0.20251 | val_0_rmse: 0.4238  | val_1_rmse: 0.45356 |  0:02:01s
epoch 129| loss: 0.20078 | val_0_rmse: 0.41997 | val_1_rmse: 0.45849 |  0:02:02s
epoch 130| loss: 0.1971  | val_0_rmse: 0.42491 | val_1_rmse: 0.45983 |  0:02:03s
epoch 131| loss: 0.19762 | val_0_rmse: 0.44773 | val_1_rmse: 0.47365 |  0:02:04s
epoch 132| loss: 0.19675 | val_0_rmse: 0.42734 | val_1_rmse: 0.46428 |  0:02:05s
epoch 133| loss: 0.19256 | val_0_rmse: 0.418   | val_1_rmse: 0.4565  |  0:02:06s
epoch 134| loss: 0.20406 | val_0_rmse: 0.41941 | val_1_rmse: 0.45296 |  0:02:07s
epoch 135| loss: 0.19522 | val_0_rmse: 0.41852 | val_1_rmse: 0.45181 |  0:02:08s
epoch 136| loss: 0.19686 | val_0_rmse: 0.43111 | val_1_rmse: 0.46754 |  0:02:09s
epoch 137| loss: 0.19129 | val_0_rmse: 0.42939 | val_1_rmse: 0.46325 |  0:02:10s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.44696
Best weights from best epoch are automatically used!
ended training at: 08:18:14
Feature importance:
[('Area', 0.4459239802471648), ('Baths', 0.039758090222163665), ('Beds', 0.03081200550537866), ('Latitude', 0.23494421069936142), ('Longitude', 0.05435063782792838), ('Month', 0.03622609749054543), ('Year', 0.1579849780074577)]
Mean squared error is of 5965410380.652924
Mean absolute error:54302.97976003397
MAPE:0.1392430114004081
R2 score:0.8044707648295066
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:18:14
epoch 0  | loss: 0.63838 | val_0_rmse: 0.68545 | val_1_rmse: 0.70725 |  0:00:00s
epoch 1  | loss: 0.37059 | val_0_rmse: 0.61082 | val_1_rmse: 0.6355  |  0:00:01s
epoch 2  | loss: 0.32531 | val_0_rmse: 0.55666 | val_1_rmse: 0.576   |  0:00:02s
epoch 3  | loss: 0.30387 | val_0_rmse: 0.53995 | val_1_rmse: 0.55771 |  0:00:03s
epoch 4  | loss: 0.29671 | val_0_rmse: 0.53253 | val_1_rmse: 0.5512  |  0:00:04s
epoch 5  | loss: 0.29956 | val_0_rmse: 0.54207 | val_1_rmse: 0.54925 |  0:00:05s
epoch 6  | loss: 0.29467 | val_0_rmse: 0.54218 | val_1_rmse: 0.54706 |  0:00:06s
epoch 7  | loss: 0.28991 | val_0_rmse: 0.54327 | val_1_rmse: 0.54685 |  0:00:07s
epoch 8  | loss: 0.29954 | val_0_rmse: 0.53383 | val_1_rmse: 0.54929 |  0:00:08s
epoch 9  | loss: 0.28529 | val_0_rmse: 0.52141 | val_1_rmse: 0.53126 |  0:00:09s
epoch 10 | loss: 0.27987 | val_0_rmse: 0.52675 | val_1_rmse: 0.53377 |  0:00:10s
epoch 11 | loss: 0.2855  | val_0_rmse: 0.5376  | val_1_rmse: 0.54494 |  0:00:11s
epoch 12 | loss: 0.27799 | val_0_rmse: 0.51322 | val_1_rmse: 0.5233  |  0:00:12s
epoch 13 | loss: 0.28017 | val_0_rmse: 0.51066 | val_1_rmse: 0.51849 |  0:00:13s
epoch 14 | loss: 0.27335 | val_0_rmse: 0.50769 | val_1_rmse: 0.51308 |  0:00:14s
epoch 15 | loss: 0.26203 | val_0_rmse: 0.4961  | val_1_rmse: 0.49776 |  0:00:15s
epoch 16 | loss: 0.26406 | val_0_rmse: 0.50079 | val_1_rmse: 0.50459 |  0:00:16s
epoch 17 | loss: 0.26043 | val_0_rmse: 0.49023 | val_1_rmse: 0.49432 |  0:00:17s
epoch 18 | loss: 0.25367 | val_0_rmse: 0.48625 | val_1_rmse: 0.48705 |  0:00:18s
epoch 19 | loss: 0.25307 | val_0_rmse: 0.48375 | val_1_rmse: 0.49105 |  0:00:19s
epoch 20 | loss: 0.25927 | val_0_rmse: 0.49275 | val_1_rmse: 0.49801 |  0:00:20s
epoch 21 | loss: 0.25229 | val_0_rmse: 0.48862 | val_1_rmse: 0.49906 |  0:00:20s
epoch 22 | loss: 0.25464 | val_0_rmse: 0.48636 | val_1_rmse: 0.49661 |  0:00:21s
epoch 23 | loss: 0.24758 | val_0_rmse: 0.48341 | val_1_rmse: 0.49122 |  0:00:22s
epoch 24 | loss: 0.24445 | val_0_rmse: 0.48029 | val_1_rmse: 0.48407 |  0:00:23s
epoch 25 | loss: 0.24525 | val_0_rmse: 0.48285 | val_1_rmse: 0.48646 |  0:00:24s
epoch 26 | loss: 0.2452  | val_0_rmse: 0.47839 | val_1_rmse: 0.48512 |  0:00:25s
epoch 27 | loss: 0.25154 | val_0_rmse: 0.47217 | val_1_rmse: 0.47929 |  0:00:26s
epoch 28 | loss: 0.24902 | val_0_rmse: 0.49941 | val_1_rmse: 0.50696 |  0:00:27s
epoch 29 | loss: 0.24186 | val_0_rmse: 0.47113 | val_1_rmse: 0.47911 |  0:00:28s
epoch 30 | loss: 0.23756 | val_0_rmse: 0.47854 | val_1_rmse: 0.48887 |  0:00:29s
epoch 31 | loss: 0.24177 | val_0_rmse: 0.48135 | val_1_rmse: 0.4891  |  0:00:30s
epoch 32 | loss: 0.24108 | val_0_rmse: 0.47011 | val_1_rmse: 0.47874 |  0:00:31s
epoch 33 | loss: 0.24004 | val_0_rmse: 0.47599 | val_1_rmse: 0.48321 |  0:00:32s
epoch 34 | loss: 0.2364  | val_0_rmse: 0.47911 | val_1_rmse: 0.48265 |  0:00:33s
epoch 35 | loss: 0.24544 | val_0_rmse: 0.4786  | val_1_rmse: 0.48683 |  0:00:34s
epoch 36 | loss: 0.2438  | val_0_rmse: 0.47502 | val_1_rmse: 0.48621 |  0:00:35s
epoch 37 | loss: 0.23384 | val_0_rmse: 0.46721 | val_1_rmse: 0.4762  |  0:00:36s
epoch 38 | loss: 0.22757 | val_0_rmse: 0.45857 | val_1_rmse: 0.46564 |  0:00:37s
epoch 39 | loss: 0.22929 | val_0_rmse: 0.46612 | val_1_rmse: 0.47853 |  0:00:38s
epoch 40 | loss: 0.23271 | val_0_rmse: 0.45798 | val_1_rmse: 0.46702 |  0:00:39s
epoch 41 | loss: 0.23215 | val_0_rmse: 0.471   | val_1_rmse: 0.48432 |  0:00:39s
epoch 42 | loss: 0.23027 | val_0_rmse: 0.49026 | val_1_rmse: 0.50216 |  0:00:40s
epoch 43 | loss: 0.23893 | val_0_rmse: 0.50523 | val_1_rmse: 0.51707 |  0:00:41s
epoch 44 | loss: 0.23389 | val_0_rmse: 0.4591  | val_1_rmse: 0.46708 |  0:00:42s
epoch 45 | loss: 0.22739 | val_0_rmse: 0.45727 | val_1_rmse: 0.46696 |  0:00:43s
epoch 46 | loss: 0.22893 | val_0_rmse: 0.46212 | val_1_rmse: 0.47406 |  0:00:44s
epoch 47 | loss: 0.22612 | val_0_rmse: 0.46366 | val_1_rmse: 0.47501 |  0:00:45s
epoch 48 | loss: 0.22938 | val_0_rmse: 0.46563 | val_1_rmse: 0.47204 |  0:00:46s
epoch 49 | loss: 0.23172 | val_0_rmse: 0.46233 | val_1_rmse: 0.47296 |  0:00:47s
epoch 50 | loss: 0.22851 | val_0_rmse: 0.49311 | val_1_rmse: 0.50071 |  0:00:48s
epoch 51 | loss: 0.22783 | val_0_rmse: 0.46626 | val_1_rmse: 0.47794 |  0:00:49s
epoch 52 | loss: 0.2262  | val_0_rmse: 0.47322 | val_1_rmse: 0.48709 |  0:00:50s
epoch 53 | loss: 0.22994 | val_0_rmse: 0.46059 | val_1_rmse: 0.47033 |  0:00:51s
epoch 54 | loss: 0.22772 | val_0_rmse: 0.46317 | val_1_rmse: 0.47806 |  0:00:52s
epoch 55 | loss: 0.22339 | val_0_rmse: 0.45473 | val_1_rmse: 0.46628 |  0:00:53s
epoch 56 | loss: 0.22586 | val_0_rmse: 0.48272 | val_1_rmse: 0.49178 |  0:00:54s
epoch 57 | loss: 0.23122 | val_0_rmse: 0.498   | val_1_rmse: 0.5077  |  0:00:55s
epoch 58 | loss: 0.23725 | val_0_rmse: 0.48636 | val_1_rmse: 0.5002  |  0:00:56s
epoch 59 | loss: 0.23355 | val_0_rmse: 0.45282 | val_1_rmse: 0.46698 |  0:00:57s
epoch 60 | loss: 0.22397 | val_0_rmse: 0.4575  | val_1_rmse: 0.47027 |  0:00:57s
epoch 61 | loss: 0.22212 | val_0_rmse: 0.47262 | val_1_rmse: 0.47883 |  0:00:58s
epoch 62 | loss: 0.22664 | val_0_rmse: 0.46869 | val_1_rmse: 0.47725 |  0:00:59s
epoch 63 | loss: 0.22672 | val_0_rmse: 0.53142 | val_1_rmse: 0.55402 |  0:01:00s
epoch 64 | loss: 0.23536 | val_0_rmse: 0.46797 | val_1_rmse: 0.47935 |  0:01:01s
epoch 65 | loss: 0.22386 | val_0_rmse: 0.47783 | val_1_rmse: 0.495   |  0:01:02s
epoch 66 | loss: 0.22672 | val_0_rmse: 0.48319 | val_1_rmse: 0.4915  |  0:01:03s
epoch 67 | loss: 0.23477 | val_0_rmse: 0.47784 | val_1_rmse: 0.49334 |  0:01:04s
epoch 68 | loss: 0.23009 | val_0_rmse: 0.46509 | val_1_rmse: 0.47755 |  0:01:05s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.46564
Best weights from best epoch are automatically used!
ended training at: 08:19:20
Feature importance:
[('Area', 0.4920974257722815), ('Baths', 0.022956898230591983), ('Beds', 0.0), ('Latitude', 0.3472030425339606), ('Longitude', 0.09662588121330269), ('Month', 0.0), ('Year', 0.04111675224986322)]
Mean squared error is of 6710994001.378221
Mean absolute error:58871.34523383241
MAPE:0.1608257131589134
R2 score:0.7831164641324005
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:19:20
epoch 0  | loss: 0.71035 | val_0_rmse: 0.73933 | val_1_rmse: 0.72404 |  0:00:00s
epoch 1  | loss: 0.40234 | val_0_rmse: 0.61475 | val_1_rmse: 0.60902 |  0:00:01s
epoch 2  | loss: 0.34114 | val_0_rmse: 0.56699 | val_1_rmse: 0.56723 |  0:00:02s
epoch 3  | loss: 0.32556 | val_0_rmse: 0.58532 | val_1_rmse: 0.57188 |  0:00:03s
epoch 4  | loss: 0.3149  | val_0_rmse: 0.5475  | val_1_rmse: 0.53799 |  0:00:04s
epoch 5  | loss: 0.30806 | val_0_rmse: 0.5374  | val_1_rmse: 0.52866 |  0:00:05s
epoch 6  | loss: 0.30472 | val_0_rmse: 0.54077 | val_1_rmse: 0.53568 |  0:00:06s
epoch 7  | loss: 0.29262 | val_0_rmse: 0.53438 | val_1_rmse: 0.52649 |  0:00:07s
epoch 8  | loss: 0.28837 | val_0_rmse: 0.52056 | val_1_rmse: 0.51595 |  0:00:08s
epoch 9  | loss: 0.28944 | val_0_rmse: 0.51745 | val_1_rmse: 0.51372 |  0:00:09s
epoch 10 | loss: 0.28372 | val_0_rmse: 0.51869 | val_1_rmse: 0.51811 |  0:00:10s
epoch 11 | loss: 0.28447 | val_0_rmse: 0.52365 | val_1_rmse: 0.52129 |  0:00:11s
epoch 12 | loss: 0.28384 | val_0_rmse: 0.5234  | val_1_rmse: 0.51253 |  0:00:12s
epoch 13 | loss: 0.27954 | val_0_rmse: 0.52759 | val_1_rmse: 0.52289 |  0:00:13s
epoch 14 | loss: 0.27737 | val_0_rmse: 0.52446 | val_1_rmse: 0.52417 |  0:00:14s
epoch 15 | loss: 0.27756 | val_0_rmse: 0.51281 | val_1_rmse: 0.50408 |  0:00:15s
epoch 16 | loss: 0.2776  | val_0_rmse: 0.51264 | val_1_rmse: 0.50714 |  0:00:16s
epoch 17 | loss: 0.2803  | val_0_rmse: 0.51182 | val_1_rmse: 0.50605 |  0:00:17s
epoch 18 | loss: 0.27705 | val_0_rmse: 0.51409 | val_1_rmse: 0.50899 |  0:00:18s
epoch 19 | loss: 0.27745 | val_0_rmse: 0.50128 | val_1_rmse: 0.49491 |  0:00:18s
epoch 20 | loss: 0.27043 | val_0_rmse: 0.5221  | val_1_rmse: 0.51696 |  0:00:19s
epoch 21 | loss: 0.27998 | val_0_rmse: 0.51153 | val_1_rmse: 0.50677 |  0:00:20s
epoch 22 | loss: 0.27407 | val_0_rmse: 0.50295 | val_1_rmse: 0.49643 |  0:00:21s
epoch 23 | loss: 0.264   | val_0_rmse: 0.50092 | val_1_rmse: 0.49454 |  0:00:22s
epoch 24 | loss: 0.26844 | val_0_rmse: 0.50215 | val_1_rmse: 0.49354 |  0:00:23s
epoch 25 | loss: 0.26398 | val_0_rmse: 0.50929 | val_1_rmse: 0.49949 |  0:00:24s
epoch 26 | loss: 0.26958 | val_0_rmse: 0.49787 | val_1_rmse: 0.48909 |  0:00:25s
epoch 27 | loss: 0.26634 | val_0_rmse: 0.52321 | val_1_rmse: 0.52648 |  0:00:26s
epoch 28 | loss: 0.27713 | val_0_rmse: 0.50086 | val_1_rmse: 0.49661 |  0:00:27s
epoch 29 | loss: 0.26249 | val_0_rmse: 0.49692 | val_1_rmse: 0.49345 |  0:00:28s
epoch 30 | loss: 0.25309 | val_0_rmse: 0.49461 | val_1_rmse: 0.48756 |  0:00:29s
epoch 31 | loss: 0.25776 | val_0_rmse: 0.49303 | val_1_rmse: 0.48796 |  0:00:30s
epoch 32 | loss: 0.26335 | val_0_rmse: 0.49396 | val_1_rmse: 0.48847 |  0:00:31s
epoch 33 | loss: 0.26222 | val_0_rmse: 0.50984 | val_1_rmse: 0.50068 |  0:00:32s
epoch 34 | loss: 0.25946 | val_0_rmse: 0.49682 | val_1_rmse: 0.49513 |  0:00:33s
epoch 35 | loss: 0.26015 | val_0_rmse: 0.50381 | val_1_rmse: 0.49739 |  0:00:34s
epoch 36 | loss: 0.26729 | val_0_rmse: 0.49479 | val_1_rmse: 0.48907 |  0:00:35s
epoch 37 | loss: 0.26137 | val_0_rmse: 0.4839  | val_1_rmse: 0.4804  |  0:00:36s
epoch 38 | loss: 0.25763 | val_0_rmse: 0.4975  | val_1_rmse: 0.49656 |  0:00:37s
epoch 39 | loss: 0.26779 | val_0_rmse: 0.50126 | val_1_rmse: 0.49724 |  0:00:38s
epoch 40 | loss: 0.25907 | val_0_rmse: 0.50833 | val_1_rmse: 0.50153 |  0:00:38s
epoch 41 | loss: 0.2552  | val_0_rmse: 0.48202 | val_1_rmse: 0.4806  |  0:00:39s
epoch 42 | loss: 0.2442  | val_0_rmse: 0.47857 | val_1_rmse: 0.48002 |  0:00:40s
epoch 43 | loss: 0.24554 | val_0_rmse: 0.48819 | val_1_rmse: 0.48368 |  0:00:41s
epoch 44 | loss: 0.25597 | val_0_rmse: 0.48429 | val_1_rmse: 0.48576 |  0:00:42s
epoch 45 | loss: 0.25038 | val_0_rmse: 0.48569 | val_1_rmse: 0.4828  |  0:00:43s
epoch 46 | loss: 0.25127 | val_0_rmse: 0.48606 | val_1_rmse: 0.48483 |  0:00:44s
epoch 47 | loss: 0.24759 | val_0_rmse: 0.48795 | val_1_rmse: 0.4882  |  0:00:45s
epoch 48 | loss: 0.25629 | val_0_rmse: 0.48438 | val_1_rmse: 0.48416 |  0:00:46s
epoch 49 | loss: 0.24809 | val_0_rmse: 0.48284 | val_1_rmse: 0.48098 |  0:00:47s
epoch 50 | loss: 0.24928 | val_0_rmse: 0.48995 | val_1_rmse: 0.48795 |  0:00:48s
epoch 51 | loss: 0.2529  | val_0_rmse: 0.48102 | val_1_rmse: 0.47971 |  0:00:49s
epoch 52 | loss: 0.25629 | val_0_rmse: 0.52086 | val_1_rmse: 0.50767 |  0:00:50s
epoch 53 | loss: 0.25807 | val_0_rmse: 0.48268 | val_1_rmse: 0.47928 |  0:00:51s
epoch 54 | loss: 0.25385 | val_0_rmse: 0.48819 | val_1_rmse: 0.48594 |  0:00:52s
epoch 55 | loss: 0.24501 | val_0_rmse: 0.49061 | val_1_rmse: 0.48681 |  0:00:53s
epoch 56 | loss: 0.26089 | val_0_rmse: 0.48869 | val_1_rmse: 0.48447 |  0:00:54s
epoch 57 | loss: 0.24941 | val_0_rmse: 0.48637 | val_1_rmse: 0.48157 |  0:00:54s
epoch 58 | loss: 0.24758 | val_0_rmse: 0.50052 | val_1_rmse: 0.50126 |  0:00:55s
epoch 59 | loss: 0.25111 | val_0_rmse: 0.48285 | val_1_rmse: 0.48385 |  0:00:56s
epoch 60 | loss: 0.25063 | val_0_rmse: 0.47574 | val_1_rmse: 0.47678 |  0:00:57s
epoch 61 | loss: 0.24022 | val_0_rmse: 0.47502 | val_1_rmse: 0.47472 |  0:00:58s
epoch 62 | loss: 0.24238 | val_0_rmse: 0.48162 | val_1_rmse: 0.47988 |  0:00:59s
epoch 63 | loss: 0.24412 | val_0_rmse: 0.47941 | val_1_rmse: 0.47791 |  0:01:00s
epoch 64 | loss: 0.24649 | val_0_rmse: 0.47865 | val_1_rmse: 0.47531 |  0:01:01s
epoch 65 | loss: 0.24753 | val_0_rmse: 0.48361 | val_1_rmse: 0.48171 |  0:01:02s
epoch 66 | loss: 0.24705 | val_0_rmse: 0.49337 | val_1_rmse: 0.49008 |  0:01:03s
epoch 67 | loss: 0.24846 | val_0_rmse: 0.47919 | val_1_rmse: 0.48083 |  0:01:04s
epoch 68 | loss: 0.25238 | val_0_rmse: 0.48455 | val_1_rmse: 0.48437 |  0:01:05s
epoch 69 | loss: 0.24876 | val_0_rmse: 0.49654 | val_1_rmse: 0.49572 |  0:01:06s
epoch 70 | loss: 0.24728 | val_0_rmse: 0.48183 | val_1_rmse: 0.48357 |  0:01:07s
epoch 71 | loss: 0.24469 | val_0_rmse: 0.47803 | val_1_rmse: 0.47981 |  0:01:08s
epoch 72 | loss: 0.2392  | val_0_rmse: 0.47104 | val_1_rmse: 0.47303 |  0:01:09s
epoch 73 | loss: 0.23696 | val_0_rmse: 0.47613 | val_1_rmse: 0.47627 |  0:01:10s
epoch 74 | loss: 0.24627 | val_0_rmse: 0.47919 | val_1_rmse: 0.47812 |  0:01:11s
epoch 75 | loss: 0.24276 | val_0_rmse: 0.47333 | val_1_rmse: 0.47551 |  0:01:12s
epoch 76 | loss: 0.23982 | val_0_rmse: 0.46991 | val_1_rmse: 0.4702  |  0:01:12s
epoch 77 | loss: 0.24154 | val_0_rmse: 0.48273 | val_1_rmse: 0.47884 |  0:01:13s
epoch 78 | loss: 0.23834 | val_0_rmse: 0.48441 | val_1_rmse: 0.48202 |  0:01:14s
epoch 79 | loss: 0.23961 | val_0_rmse: 0.47303 | val_1_rmse: 0.47677 |  0:01:15s
epoch 80 | loss: 0.23374 | val_0_rmse: 0.47231 | val_1_rmse: 0.47257 |  0:01:16s
epoch 81 | loss: 0.23529 | val_0_rmse: 0.46745 | val_1_rmse: 0.47077 |  0:01:17s
epoch 82 | loss: 0.23388 | val_0_rmse: 0.46311 | val_1_rmse: 0.46265 |  0:01:18s
epoch 83 | loss: 0.24731 | val_0_rmse: 0.49096 | val_1_rmse: 0.48723 |  0:01:19s
epoch 84 | loss: 0.25628 | val_0_rmse: 0.47891 | val_1_rmse: 0.48482 |  0:01:20s
epoch 85 | loss: 0.24232 | val_0_rmse: 0.47671 | val_1_rmse: 0.4761  |  0:01:21s
epoch 86 | loss: 0.24936 | val_0_rmse: 0.4734  | val_1_rmse: 0.4744  |  0:01:22s
epoch 87 | loss: 0.23293 | val_0_rmse: 0.46191 | val_1_rmse: 0.46519 |  0:01:23s
epoch 88 | loss: 0.23379 | val_0_rmse: 0.47468 | val_1_rmse: 0.4757  |  0:01:24s
epoch 89 | loss: 0.24013 | val_0_rmse: 0.49463 | val_1_rmse: 0.49646 |  0:01:25s
epoch 90 | loss: 0.23853 | val_0_rmse: 0.46748 | val_1_rmse: 0.47112 |  0:01:26s
epoch 91 | loss: 0.23762 | val_0_rmse: 0.47331 | val_1_rmse: 0.47336 |  0:01:27s
epoch 92 | loss: 0.24109 | val_0_rmse: 0.48412 | val_1_rmse: 0.4803  |  0:01:27s
epoch 93 | loss: 0.23302 | val_0_rmse: 0.47188 | val_1_rmse: 0.47706 |  0:01:28s
epoch 94 | loss: 0.23319 | val_0_rmse: 0.46281 | val_1_rmse: 0.4644  |  0:01:29s
epoch 95 | loss: 0.22974 | val_0_rmse: 0.46767 | val_1_rmse: 0.46483 |  0:01:30s
epoch 96 | loss: 0.23018 | val_0_rmse: 0.45821 | val_1_rmse: 0.46504 |  0:01:31s
epoch 97 | loss: 0.23344 | val_0_rmse: 0.45395 | val_1_rmse: 0.45978 |  0:01:32s
epoch 98 | loss: 0.22544 | val_0_rmse: 0.45961 | val_1_rmse: 0.46404 |  0:01:33s
epoch 99 | loss: 0.23225 | val_0_rmse: 0.47727 | val_1_rmse: 0.47768 |  0:01:34s
epoch 100| loss: 0.24698 | val_0_rmse: 0.49174 | val_1_rmse: 0.4884  |  0:01:35s
epoch 101| loss: 0.24887 | val_0_rmse: 0.48121 | val_1_rmse: 0.47965 |  0:01:36s
epoch 102| loss: 0.23904 | val_0_rmse: 0.47259 | val_1_rmse: 0.47611 |  0:01:37s
epoch 103| loss: 0.24117 | val_0_rmse: 0.47082 | val_1_rmse: 0.46635 |  0:01:38s
epoch 104| loss: 0.24435 | val_0_rmse: 0.47986 | val_1_rmse: 0.48436 |  0:01:39s
epoch 105| loss: 0.2403  | val_0_rmse: 0.47064 | val_1_rmse: 0.46959 |  0:01:40s
epoch 106| loss: 0.23846 | val_0_rmse: 0.48816 | val_1_rmse: 0.48621 |  0:01:41s
epoch 107| loss: 0.2389  | val_0_rmse: 0.46685 | val_1_rmse: 0.46241 |  0:01:42s
epoch 108| loss: 0.22594 | val_0_rmse: 0.45757 | val_1_rmse: 0.46169 |  0:01:43s
epoch 109| loss: 0.22914 | val_0_rmse: 0.46251 | val_1_rmse: 0.46302 |  0:01:44s
epoch 110| loss: 0.23308 | val_0_rmse: 0.46608 | val_1_rmse: 0.46576 |  0:01:44s
epoch 111| loss: 0.23106 | val_0_rmse: 0.46053 | val_1_rmse: 0.46649 |  0:01:45s
epoch 112| loss: 0.23317 | val_0_rmse: 0.46053 | val_1_rmse: 0.46403 |  0:01:46s
epoch 113| loss: 0.23501 | val_0_rmse: 0.48004 | val_1_rmse: 0.48476 |  0:01:47s
epoch 114| loss: 0.23848 | val_0_rmse: 0.49386 | val_1_rmse: 0.48928 |  0:01:48s
epoch 115| loss: 0.23948 | val_0_rmse: 0.46268 | val_1_rmse: 0.46369 |  0:01:49s
epoch 116| loss: 0.23245 | val_0_rmse: 0.48    | val_1_rmse: 0.4783  |  0:01:50s
epoch 117| loss: 0.23589 | val_0_rmse: 0.46033 | val_1_rmse: 0.46266 |  0:01:51s
epoch 118| loss: 0.23272 | val_0_rmse: 0.45515 | val_1_rmse: 0.45784 |  0:01:52s
epoch 119| loss: 0.22776 | val_0_rmse: 0.47216 | val_1_rmse: 0.47014 |  0:01:53s
epoch 120| loss: 0.22602 | val_0_rmse: 0.45256 | val_1_rmse: 0.45474 |  0:01:54s
epoch 121| loss: 0.23173 | val_0_rmse: 0.47409 | val_1_rmse: 0.47725 |  0:01:55s
epoch 122| loss: 0.23489 | val_0_rmse: 0.47654 | val_1_rmse: 0.47482 |  0:01:56s
epoch 123| loss: 0.22803 | val_0_rmse: 0.46404 | val_1_rmse: 0.46564 |  0:01:57s
epoch 124| loss: 0.23063 | val_0_rmse: 0.45461 | val_1_rmse: 0.45366 |  0:01:58s
epoch 125| loss: 0.2334  | val_0_rmse: 0.45969 | val_1_rmse: 0.46121 |  0:01:59s
epoch 126| loss: 0.23407 | val_0_rmse: 0.4608  | val_1_rmse: 0.46439 |  0:02:00s
epoch 127| loss: 0.22799 | val_0_rmse: 0.4635  | val_1_rmse: 0.4635  |  0:02:01s
epoch 128| loss: 0.22303 | val_0_rmse: 0.46087 | val_1_rmse: 0.466   |  0:02:01s
epoch 129| loss: 0.22696 | val_0_rmse: 0.47276 | val_1_rmse: 0.47861 |  0:02:02s
epoch 130| loss: 0.22858 | val_0_rmse: 0.46953 | val_1_rmse: 0.47349 |  0:02:03s
epoch 131| loss: 0.23085 | val_0_rmse: 0.47762 | val_1_rmse: 0.4791  |  0:02:04s
epoch 132| loss: 0.22467 | val_0_rmse: 0.46752 | val_1_rmse: 0.47849 |  0:02:05s
epoch 133| loss: 0.23166 | val_0_rmse: 0.45369 | val_1_rmse: 0.45597 |  0:02:06s
epoch 134| loss: 0.22321 | val_0_rmse: 0.44836 | val_1_rmse: 0.45246 |  0:02:07s
epoch 135| loss: 0.22347 | val_0_rmse: 0.45447 | val_1_rmse: 0.45976 |  0:02:08s
epoch 136| loss: 0.2207  | val_0_rmse: 0.44953 | val_1_rmse: 0.45216 |  0:02:09s
epoch 137| loss: 0.22043 | val_0_rmse: 0.45142 | val_1_rmse: 0.45964 |  0:02:10s
epoch 138| loss: 0.21893 | val_0_rmse: 0.45939 | val_1_rmse: 0.46506 |  0:02:11s
epoch 139| loss: 0.22199 | val_0_rmse: 0.4501  | val_1_rmse: 0.44906 |  0:02:12s
epoch 140| loss: 0.22943 | val_0_rmse: 0.46333 | val_1_rmse: 0.46509 |  0:02:13s
epoch 141| loss: 0.23143 | val_0_rmse: 0.45821 | val_1_rmse: 0.45959 |  0:02:14s
epoch 142| loss: 0.22416 | val_0_rmse: 0.46018 | val_1_rmse: 0.45865 |  0:02:15s
epoch 143| loss: 0.23074 | val_0_rmse: 0.45903 | val_1_rmse: 0.46582 |  0:02:16s
epoch 144| loss: 0.22062 | val_0_rmse: 0.45898 | val_1_rmse: 0.46714 |  0:02:17s
epoch 145| loss: 0.22605 | val_0_rmse: 0.45746 | val_1_rmse: 0.46122 |  0:02:18s
epoch 146| loss: 0.24252 | val_0_rmse: 0.47734 | val_1_rmse: 0.47975 |  0:02:19s
epoch 147| loss: 0.23306 | val_0_rmse: 0.45508 | val_1_rmse: 0.45653 |  0:02:19s
epoch 148| loss: 0.22551 | val_0_rmse: 0.46253 | val_1_rmse: 0.46363 |  0:02:20s
epoch 149| loss: 0.22604 | val_0_rmse: 0.44822 | val_1_rmse: 0.45327 |  0:02:21s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.44906
Best weights from best epoch are automatically used!
ended training at: 08:21:42
Feature importance:
[('Area', 0.3818894543005825), ('Baths', 0.0), ('Beds', 0.006008310457126469), ('Latitude', 0.4422578816043541), ('Longitude', 0.08642748882646661), ('Month', 0.0), ('Year', 0.08341686481147031)]
Mean squared error is of 6120138035.993095
Mean absolute error:55625.98890510191
MAPE:0.14524549553244578
R2 score:0.8016547026380609
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:21:42
epoch 0  | loss: 0.62371 | val_0_rmse: 0.72266 | val_1_rmse: 0.71544 |  0:00:00s
epoch 1  | loss: 0.35278 | val_0_rmse: 0.6048  | val_1_rmse: 0.59914 |  0:00:01s
epoch 2  | loss: 0.3308  | val_0_rmse: 0.60538 | val_1_rmse: 0.59822 |  0:00:02s
epoch 3  | loss: 0.33327 | val_0_rmse: 0.56163 | val_1_rmse: 0.55556 |  0:00:03s
epoch 4  | loss: 0.30855 | val_0_rmse: 0.54792 | val_1_rmse: 0.55344 |  0:00:04s
epoch 5  | loss: 0.30516 | val_0_rmse: 0.53065 | val_1_rmse: 0.53508 |  0:00:05s
epoch 6  | loss: 0.29491 | val_0_rmse: 0.55311 | val_1_rmse: 0.55265 |  0:00:06s
epoch 7  | loss: 0.30922 | val_0_rmse: 0.60953 | val_1_rmse: 0.60634 |  0:00:07s
epoch 8  | loss: 0.29946 | val_0_rmse: 0.55195 | val_1_rmse: 0.55506 |  0:00:08s
epoch 9  | loss: 0.2883  | val_0_rmse: 0.53007 | val_1_rmse: 0.53046 |  0:00:09s
epoch 10 | loss: 0.28579 | val_0_rmse: 0.52386 | val_1_rmse: 0.52422 |  0:00:10s
epoch 11 | loss: 0.2794  | val_0_rmse: 0.51267 | val_1_rmse: 0.51385 |  0:00:11s
epoch 12 | loss: 0.27724 | val_0_rmse: 0.51684 | val_1_rmse: 0.51941 |  0:00:12s
epoch 13 | loss: 0.28188 | val_0_rmse: 0.53757 | val_1_rmse: 0.53654 |  0:00:13s
epoch 14 | loss: 0.27623 | val_0_rmse: 0.50991 | val_1_rmse: 0.50768 |  0:00:14s
epoch 15 | loss: 0.26843 | val_0_rmse: 0.51195 | val_1_rmse: 0.51165 |  0:00:15s
epoch 16 | loss: 0.27082 | val_0_rmse: 0.50841 | val_1_rmse: 0.50687 |  0:00:16s
epoch 17 | loss: 0.27007 | val_0_rmse: 0.51236 | val_1_rmse: 0.51154 |  0:00:17s
epoch 18 | loss: 0.27356 | val_0_rmse: 0.51382 | val_1_rmse: 0.51877 |  0:00:18s
epoch 19 | loss: 0.26718 | val_0_rmse: 0.51231 | val_1_rmse: 0.51359 |  0:00:19s
epoch 20 | loss: 0.26824 | val_0_rmse: 0.50882 | val_1_rmse: 0.50949 |  0:00:20s
epoch 21 | loss: 0.26666 | val_0_rmse: 0.50828 | val_1_rmse: 0.50828 |  0:00:21s
epoch 22 | loss: 0.26517 | val_0_rmse: 0.50418 | val_1_rmse: 0.50624 |  0:00:21s
epoch 23 | loss: 0.2684  | val_0_rmse: 0.50762 | val_1_rmse: 0.50941 |  0:00:22s
epoch 24 | loss: 0.2665  | val_0_rmse: 0.50323 | val_1_rmse: 0.50227 |  0:00:23s
epoch 25 | loss: 0.26154 | val_0_rmse: 0.4937  | val_1_rmse: 0.49448 |  0:00:24s
epoch 26 | loss: 0.26149 | val_0_rmse: 0.51719 | val_1_rmse: 0.51795 |  0:00:25s
epoch 27 | loss: 0.26614 | val_0_rmse: 0.49147 | val_1_rmse: 0.49274 |  0:00:26s
epoch 28 | loss: 0.2575  | val_0_rmse: 0.49895 | val_1_rmse: 0.50458 |  0:00:27s
epoch 29 | loss: 0.26027 | val_0_rmse: 0.49389 | val_1_rmse: 0.4971  |  0:00:28s
epoch 30 | loss: 0.25378 | val_0_rmse: 0.49119 | val_1_rmse: 0.49291 |  0:00:29s
epoch 31 | loss: 0.25843 | val_0_rmse: 0.50592 | val_1_rmse: 0.50936 |  0:00:30s
epoch 32 | loss: 0.25987 | val_0_rmse: 0.48764 | val_1_rmse: 0.49051 |  0:00:31s
epoch 33 | loss: 0.2525  | val_0_rmse: 0.48667 | val_1_rmse: 0.48814 |  0:00:32s
epoch 34 | loss: 0.25134 | val_0_rmse: 0.48344 | val_1_rmse: 0.48409 |  0:00:33s
epoch 35 | loss: 0.24949 | val_0_rmse: 0.49306 | val_1_rmse: 0.50058 |  0:00:34s
epoch 36 | loss: 0.25589 | val_0_rmse: 0.49863 | val_1_rmse: 0.50193 |  0:00:35s
epoch 37 | loss: 0.25687 | val_0_rmse: 0.48811 | val_1_rmse: 0.49071 |  0:00:36s
epoch 38 | loss: 0.25732 | val_0_rmse: 0.48478 | val_1_rmse: 0.48739 |  0:00:37s
epoch 39 | loss: 0.25118 | val_0_rmse: 0.48896 | val_1_rmse: 0.49158 |  0:00:38s
epoch 40 | loss: 0.24643 | val_0_rmse: 0.48876 | val_1_rmse: 0.48971 |  0:00:39s
epoch 41 | loss: 0.24599 | val_0_rmse: 0.481   | val_1_rmse: 0.48679 |  0:00:40s
epoch 42 | loss: 0.24394 | val_0_rmse: 0.49111 | val_1_rmse: 0.49094 |  0:00:41s
epoch 43 | loss: 0.24408 | val_0_rmse: 0.4806  | val_1_rmse: 0.48526 |  0:00:41s
epoch 44 | loss: 0.24155 | val_0_rmse: 0.47835 | val_1_rmse: 0.48491 |  0:00:42s
epoch 45 | loss: 0.23999 | val_0_rmse: 0.47369 | val_1_rmse: 0.47974 |  0:00:43s
epoch 46 | loss: 0.23903 | val_0_rmse: 0.49522 | val_1_rmse: 0.50093 |  0:00:44s
epoch 47 | loss: 0.24429 | val_0_rmse: 0.4709  | val_1_rmse: 0.47664 |  0:00:45s
epoch 48 | loss: 0.24152 | val_0_rmse: 0.47664 | val_1_rmse: 0.47996 |  0:00:46s
epoch 49 | loss: 0.23932 | val_0_rmse: 0.48422 | val_1_rmse: 0.49421 |  0:00:47s
epoch 50 | loss: 0.23948 | val_0_rmse: 0.48159 | val_1_rmse: 0.48775 |  0:00:48s
epoch 51 | loss: 0.23987 | val_0_rmse: 0.52332 | val_1_rmse: 0.52868 |  0:00:49s
epoch 52 | loss: 0.24407 | val_0_rmse: 0.47228 | val_1_rmse: 0.4797  |  0:00:50s
epoch 53 | loss: 0.23341 | val_0_rmse: 0.46415 | val_1_rmse: 0.46992 |  0:00:51s
epoch 54 | loss: 0.24005 | val_0_rmse: 0.47446 | val_1_rmse: 0.4835  |  0:00:52s
epoch 55 | loss: 0.23792 | val_0_rmse: 0.47697 | val_1_rmse: 0.48431 |  0:00:53s
epoch 56 | loss: 0.23598 | val_0_rmse: 0.47225 | val_1_rmse: 0.477   |  0:00:54s
epoch 57 | loss: 0.23512 | val_0_rmse: 0.47052 | val_1_rmse: 0.47597 |  0:00:55s
epoch 58 | loss: 0.22681 | val_0_rmse: 0.47257 | val_1_rmse: 0.48096 |  0:00:56s
epoch 59 | loss: 0.22789 | val_0_rmse: 0.47024 | val_1_rmse: 0.4736  |  0:00:57s
epoch 60 | loss: 0.23176 | val_0_rmse: 0.45404 | val_1_rmse: 0.45879 |  0:00:58s
epoch 61 | loss: 0.22529 | val_0_rmse: 0.46286 | val_1_rmse: 0.46538 |  0:00:59s
epoch 62 | loss: 0.22533 | val_0_rmse: 0.45822 | val_1_rmse: 0.46292 |  0:00:59s
epoch 63 | loss: 0.2213  | val_0_rmse: 0.45666 | val_1_rmse: 0.46395 |  0:01:00s
epoch 64 | loss: 0.22751 | val_0_rmse: 0.46304 | val_1_rmse: 0.46736 |  0:01:01s
epoch 65 | loss: 0.22196 | val_0_rmse: 0.45933 | val_1_rmse: 0.46307 |  0:01:02s
epoch 66 | loss: 0.22064 | val_0_rmse: 0.45079 | val_1_rmse: 0.45596 |  0:01:03s
epoch 67 | loss: 0.2165  | val_0_rmse: 0.45698 | val_1_rmse: 0.45963 |  0:01:04s
epoch 68 | loss: 0.22216 | val_0_rmse: 0.46519 | val_1_rmse: 0.47003 |  0:01:05s
epoch 69 | loss: 0.22615 | val_0_rmse: 0.45714 | val_1_rmse: 0.46228 |  0:01:06s
epoch 70 | loss: 0.2271  | val_0_rmse: 0.44977 | val_1_rmse: 0.45546 |  0:01:07s
epoch 71 | loss: 0.2211  | val_0_rmse: 0.46552 | val_1_rmse: 0.469   |  0:01:08s
epoch 72 | loss: 0.23267 | val_0_rmse: 0.45691 | val_1_rmse: 0.45881 |  0:01:09s
epoch 73 | loss: 0.22387 | val_0_rmse: 0.44665 | val_1_rmse: 0.45228 |  0:01:10s
epoch 74 | loss: 0.21597 | val_0_rmse: 0.4426  | val_1_rmse: 0.44776 |  0:01:11s
epoch 75 | loss: 0.21618 | val_0_rmse: 0.46719 | val_1_rmse: 0.47212 |  0:01:12s
epoch 76 | loss: 0.21599 | val_0_rmse: 0.44241 | val_1_rmse: 0.44872 |  0:01:13s
epoch 77 | loss: 0.21443 | val_0_rmse: 0.44745 | val_1_rmse: 0.45205 |  0:01:14s
epoch 78 | loss: 0.22123 | val_0_rmse: 0.45339 | val_1_rmse: 0.46107 |  0:01:15s
epoch 79 | loss: 0.21423 | val_0_rmse: 0.44475 | val_1_rmse: 0.45201 |  0:01:16s
epoch 80 | loss: 0.21354 | val_0_rmse: 0.45104 | val_1_rmse: 0.45706 |  0:01:17s
epoch 81 | loss: 0.22052 | val_0_rmse: 0.46301 | val_1_rmse: 0.46648 |  0:01:18s
epoch 82 | loss: 0.22556 | val_0_rmse: 0.47712 | val_1_rmse: 0.48013 |  0:01:18s
epoch 83 | loss: 0.22198 | val_0_rmse: 0.45855 | val_1_rmse: 0.46589 |  0:01:19s
epoch 84 | loss: 0.21627 | val_0_rmse: 0.44502 | val_1_rmse: 0.45055 |  0:01:20s
epoch 85 | loss: 0.21174 | val_0_rmse: 0.45228 | val_1_rmse: 0.45847 |  0:01:21s
epoch 86 | loss: 0.21645 | val_0_rmse: 0.44266 | val_1_rmse: 0.44951 |  0:01:22s
epoch 87 | loss: 0.21169 | val_0_rmse: 0.44881 | val_1_rmse: 0.45949 |  0:01:23s
epoch 88 | loss: 0.21498 | val_0_rmse: 0.44583 | val_1_rmse: 0.45096 |  0:01:24s
epoch 89 | loss: 0.21463 | val_0_rmse: 0.44876 | val_1_rmse: 0.45382 |  0:01:25s
epoch 90 | loss: 0.21764 | val_0_rmse: 0.44651 | val_1_rmse: 0.45283 |  0:01:26s
epoch 91 | loss: 0.2158  | val_0_rmse: 0.43517 | val_1_rmse: 0.44303 |  0:01:27s
epoch 92 | loss: 0.21191 | val_0_rmse: 0.45153 | val_1_rmse: 0.46119 |  0:01:28s
epoch 93 | loss: 0.21248 | val_0_rmse: 0.4358  | val_1_rmse: 0.44315 |  0:01:29s
epoch 94 | loss: 0.21415 | val_0_rmse: 0.43913 | val_1_rmse: 0.4488  |  0:01:30s
epoch 95 | loss: 0.2098  | val_0_rmse: 0.44464 | val_1_rmse: 0.45581 |  0:01:31s
epoch 96 | loss: 0.22616 | val_0_rmse: 0.45623 | val_1_rmse: 0.46563 |  0:01:32s
epoch 97 | loss: 0.22334 | val_0_rmse: 0.44056 | val_1_rmse: 0.44854 |  0:01:33s
epoch 98 | loss: 0.21224 | val_0_rmse: 0.44277 | val_1_rmse: 0.45109 |  0:01:34s
epoch 99 | loss: 0.2099  | val_0_rmse: 0.44379 | val_1_rmse: 0.45314 |  0:01:35s
epoch 100| loss: 0.21373 | val_0_rmse: 0.4463  | val_1_rmse: 0.45326 |  0:01:36s
epoch 101| loss: 0.22041 | val_0_rmse: 0.46241 | val_1_rmse: 0.46677 |  0:01:36s
epoch 102| loss: 0.21405 | val_0_rmse: 0.44403 | val_1_rmse: 0.4521  |  0:01:37s
epoch 103| loss: 0.2089  | val_0_rmse: 0.43461 | val_1_rmse: 0.44245 |  0:01:38s
epoch 104| loss: 0.2095  | val_0_rmse: 0.43679 | val_1_rmse: 0.44389 |  0:01:39s
epoch 105| loss: 0.2095  | val_0_rmse: 0.43679 | val_1_rmse: 0.44963 |  0:01:40s
epoch 106| loss: 0.20818 | val_0_rmse: 0.42984 | val_1_rmse: 0.44281 |  0:01:41s
epoch 107| loss: 0.20872 | val_0_rmse: 0.44195 | val_1_rmse: 0.45198 |  0:01:42s
epoch 108| loss: 0.21204 | val_0_rmse: 0.43577 | val_1_rmse: 0.44926 |  0:01:43s
epoch 109| loss: 0.2098  | val_0_rmse: 0.44216 | val_1_rmse: 0.44962 |  0:01:44s
epoch 110| loss: 0.21499 | val_0_rmse: 0.44694 | val_1_rmse: 0.45463 |  0:01:45s
epoch 111| loss: 0.2107  | val_0_rmse: 0.43676 | val_1_rmse: 0.44403 |  0:01:46s
epoch 112| loss: 0.20801 | val_0_rmse: 0.43842 | val_1_rmse: 0.44611 |  0:01:47s
epoch 113| loss: 0.21187 | val_0_rmse: 0.43155 | val_1_rmse: 0.44307 |  0:01:48s
epoch 114| loss: 0.20454 | val_0_rmse: 0.45404 | val_1_rmse: 0.46475 |  0:01:49s
epoch 115| loss: 0.21208 | val_0_rmse: 0.45243 | val_1_rmse: 0.46107 |  0:01:50s
epoch 116| loss: 0.21359 | val_0_rmse: 0.45639 | val_1_rmse: 0.47002 |  0:01:51s
epoch 117| loss: 0.20277 | val_0_rmse: 0.4333  | val_1_rmse: 0.44072 |  0:01:52s
epoch 118| loss: 0.20892 | val_0_rmse: 0.44076 | val_1_rmse: 0.44851 |  0:01:53s
epoch 119| loss: 0.20449 | val_0_rmse: 0.43611 | val_1_rmse: 0.44689 |  0:01:54s
epoch 120| loss: 0.20871 | val_0_rmse: 0.44724 | val_1_rmse: 0.46016 |  0:01:55s
epoch 121| loss: 0.20732 | val_0_rmse: 0.43289 | val_1_rmse: 0.44614 |  0:01:56s
epoch 122| loss: 0.21747 | val_0_rmse: 0.43964 | val_1_rmse: 0.45136 |  0:01:56s
epoch 123| loss: 0.20603 | val_0_rmse: 0.44424 | val_1_rmse: 0.4566  |  0:01:57s
epoch 124| loss: 0.20696 | val_0_rmse: 0.43286 | val_1_rmse: 0.44101 |  0:01:58s
epoch 125| loss: 0.20729 | val_0_rmse: 0.43616 | val_1_rmse: 0.44677 |  0:01:59s
epoch 126| loss: 0.20088 | val_0_rmse: 0.43253 | val_1_rmse: 0.44301 |  0:02:00s
epoch 127| loss: 0.20136 | val_0_rmse: 0.43811 | val_1_rmse: 0.45058 |  0:02:01s
epoch 128| loss: 0.20572 | val_0_rmse: 0.428   | val_1_rmse: 0.4421  |  0:02:02s
epoch 129| loss: 0.20025 | val_0_rmse: 0.42806 | val_1_rmse: 0.44167 |  0:02:03s
epoch 130| loss: 0.20113 | val_0_rmse: 0.42582 | val_1_rmse: 0.44033 |  0:02:04s
epoch 131| loss: 0.20479 | val_0_rmse: 0.43178 | val_1_rmse: 0.44179 |  0:02:05s
epoch 132| loss: 0.20321 | val_0_rmse: 0.42048 | val_1_rmse: 0.4362  |  0:02:06s
epoch 133| loss: 0.20052 | val_0_rmse: 0.43356 | val_1_rmse: 0.43819 |  0:02:07s
epoch 134| loss: 0.20818 | val_0_rmse: 0.44795 | val_1_rmse: 0.4626  |  0:02:08s
epoch 135| loss: 0.2029  | val_0_rmse: 0.42834 | val_1_rmse: 0.4396  |  0:02:09s
epoch 136| loss: 0.20684 | val_0_rmse: 0.43382 | val_1_rmse: 0.44375 |  0:02:10s
epoch 137| loss: 0.21042 | val_0_rmse: 0.44086 | val_1_rmse: 0.45147 |  0:02:11s
epoch 138| loss: 0.20402 | val_0_rmse: 0.43399 | val_1_rmse: 0.44775 |  0:02:12s
epoch 139| loss: 0.20334 | val_0_rmse: 0.43809 | val_1_rmse: 0.45474 |  0:02:13s
epoch 140| loss: 0.20278 | val_0_rmse: 0.43065 | val_1_rmse: 0.44208 |  0:02:14s
epoch 141| loss: 0.20231 | val_0_rmse: 0.43557 | val_1_rmse: 0.45132 |  0:02:14s
epoch 142| loss: 0.20689 | val_0_rmse: 0.43739 | val_1_rmse: 0.45641 |  0:02:15s
epoch 143| loss: 0.20389 | val_0_rmse: 0.43444 | val_1_rmse: 0.44981 |  0:02:16s
epoch 144| loss: 0.20838 | val_0_rmse: 0.44353 | val_1_rmse: 0.45367 |  0:02:17s
epoch 145| loss: 0.21168 | val_0_rmse: 0.43541 | val_1_rmse: 0.4465  |  0:02:18s
epoch 146| loss: 0.20345 | val_0_rmse: 0.43202 | val_1_rmse: 0.44668 |  0:02:19s
epoch 147| loss: 0.19948 | val_0_rmse: 0.43188 | val_1_rmse: 0.44472 |  0:02:20s
epoch 148| loss: 0.20115 | val_0_rmse: 0.4303  | val_1_rmse: 0.44583 |  0:02:21s
epoch 149| loss: 0.19918 | val_0_rmse: 0.426   | val_1_rmse: 0.44166 |  0:02:22s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.4362
Best weights from best epoch are automatically used!
ended training at: 08:24:05
Feature importance:
[('Area', 0.39645292038726193), ('Baths', 0.09831400580709732), ('Beds', 0.0), ('Latitude', 0.38726168111894677), ('Longitude', 0.0), ('Month', 0.061175410265505735), ('Year', 0.05679598242118821)]
Mean squared error is of 5844814273.031739
Mean absolute error:54023.24569020508
MAPE:0.1449973786849035
R2 score:0.8051889116958468
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:24:05
epoch 0  | loss: 0.6611  | val_0_rmse: 0.6698  | val_1_rmse: 0.66936 |  0:00:00s
epoch 1  | loss: 0.37689 | val_0_rmse: 0.59316 | val_1_rmse: 0.58786 |  0:00:01s
epoch 2  | loss: 0.32282 | val_0_rmse: 0.56901 | val_1_rmse: 0.5504  |  0:00:02s
epoch 3  | loss: 0.29403 | val_0_rmse: 0.52993 | val_1_rmse: 0.51463 |  0:00:03s
epoch 4  | loss: 0.28631 | val_0_rmse: 0.54182 | val_1_rmse: 0.52877 |  0:00:04s
epoch 5  | loss: 0.28614 | val_0_rmse: 0.51419 | val_1_rmse: 0.50185 |  0:00:05s
epoch 6  | loss: 0.27702 | val_0_rmse: 0.52565 | val_1_rmse: 0.5139  |  0:00:06s
epoch 7  | loss: 0.28296 | val_0_rmse: 0.53422 | val_1_rmse: 0.52621 |  0:00:07s
epoch 8  | loss: 0.29026 | val_0_rmse: 0.51931 | val_1_rmse: 0.50815 |  0:00:08s
epoch 9  | loss: 0.28315 | val_0_rmse: 0.51817 | val_1_rmse: 0.50419 |  0:00:09s
epoch 10 | loss: 0.28082 | val_0_rmse: 0.52484 | val_1_rmse: 0.50802 |  0:00:10s
epoch 11 | loss: 0.27868 | val_0_rmse: 0.51615 | val_1_rmse: 0.496   |  0:00:11s
epoch 12 | loss: 0.28119 | val_0_rmse: 0.51368 | val_1_rmse: 0.49485 |  0:00:12s
epoch 13 | loss: 0.27778 | val_0_rmse: 0.50768 | val_1_rmse: 0.49097 |  0:00:13s
epoch 14 | loss: 0.27507 | val_0_rmse: 0.51148 | val_1_rmse: 0.48978 |  0:00:14s
epoch 15 | loss: 0.27067 | val_0_rmse: 0.50663 | val_1_rmse: 0.49044 |  0:00:15s
epoch 16 | loss: 0.27053 | val_0_rmse: 0.50478 | val_1_rmse: 0.49034 |  0:00:16s
epoch 17 | loss: 0.28723 | val_0_rmse: 0.51896 | val_1_rmse: 0.50446 |  0:00:17s
epoch 18 | loss: 0.27149 | val_0_rmse: 0.51187 | val_1_rmse: 0.49313 |  0:00:18s
epoch 19 | loss: 0.26874 | val_0_rmse: 0.51255 | val_1_rmse: 0.49223 |  0:00:19s
epoch 20 | loss: 0.2681  | val_0_rmse: 0.50478 | val_1_rmse: 0.49088 |  0:00:19s
epoch 21 | loss: 0.26856 | val_0_rmse: 0.51184 | val_1_rmse: 0.50075 |  0:00:20s
epoch 22 | loss: 0.27264 | val_0_rmse: 0.50282 | val_1_rmse: 0.48917 |  0:00:21s
epoch 23 | loss: 0.2701  | val_0_rmse: 0.51815 | val_1_rmse: 0.50113 |  0:00:22s
epoch 24 | loss: 0.2669  | val_0_rmse: 0.50202 | val_1_rmse: 0.49064 |  0:00:23s
epoch 25 | loss: 0.26861 | val_0_rmse: 0.50592 | val_1_rmse: 0.49159 |  0:00:24s
epoch 26 | loss: 0.26689 | val_0_rmse: 0.50621 | val_1_rmse: 0.49412 |  0:00:25s
epoch 27 | loss: 0.26778 | val_0_rmse: 0.50033 | val_1_rmse: 0.48447 |  0:00:26s
epoch 28 | loss: 0.26899 | val_0_rmse: 0.50826 | val_1_rmse: 0.49305 |  0:00:27s
epoch 29 | loss: 0.27088 | val_0_rmse: 0.51179 | val_1_rmse: 0.49628 |  0:00:28s
epoch 30 | loss: 0.2722  | val_0_rmse: 0.50409 | val_1_rmse: 0.48926 |  0:00:29s
epoch 31 | loss: 0.2723  | val_0_rmse: 0.49955 | val_1_rmse: 0.48873 |  0:00:30s
epoch 32 | loss: 0.27062 | val_0_rmse: 0.50705 | val_1_rmse: 0.49171 |  0:00:31s
epoch 33 | loss: 0.26337 | val_0_rmse: 0.50707 | val_1_rmse: 0.49376 |  0:00:32s
epoch 34 | loss: 0.27166 | val_0_rmse: 0.50249 | val_1_rmse: 0.48742 |  0:00:33s
epoch 35 | loss: 0.2717  | val_0_rmse: 0.51398 | val_1_rmse: 0.50546 |  0:00:34s
epoch 36 | loss: 0.27461 | val_0_rmse: 0.51366 | val_1_rmse: 0.50327 |  0:00:35s
epoch 37 | loss: 0.26908 | val_0_rmse: 0.5068  | val_1_rmse: 0.49806 |  0:00:36s
epoch 38 | loss: 0.26732 | val_0_rmse: 0.50376 | val_1_rmse: 0.49072 |  0:00:37s
epoch 39 | loss: 0.26953 | val_0_rmse: 0.51077 | val_1_rmse: 0.49403 |  0:00:38s
epoch 40 | loss: 0.26804 | val_0_rmse: 0.50066 | val_1_rmse: 0.48793 |  0:00:38s
epoch 41 | loss: 0.26092 | val_0_rmse: 0.50708 | val_1_rmse: 0.49497 |  0:00:39s
epoch 42 | loss: 0.26846 | val_0_rmse: 0.50316 | val_1_rmse: 0.48777 |  0:00:40s
epoch 43 | loss: 0.26665 | val_0_rmse: 0.50148 | val_1_rmse: 0.48877 |  0:00:41s
epoch 44 | loss: 0.26724 | val_0_rmse: 0.49661 | val_1_rmse: 0.48581 |  0:00:42s
epoch 45 | loss: 0.2749  | val_0_rmse: 0.5006  | val_1_rmse: 0.48534 |  0:00:43s
epoch 46 | loss: 0.26249 | val_0_rmse: 0.51444 | val_1_rmse: 0.49555 |  0:00:44s
epoch 47 | loss: 0.25947 | val_0_rmse: 0.49108 | val_1_rmse: 0.47771 |  0:00:45s
epoch 48 | loss: 0.26314 | val_0_rmse: 0.50197 | val_1_rmse: 0.48795 |  0:00:46s
epoch 49 | loss: 0.25751 | val_0_rmse: 0.50179 | val_1_rmse: 0.48561 |  0:00:47s
epoch 50 | loss: 0.25223 | val_0_rmse: 0.49221 | val_1_rmse: 0.48046 |  0:00:48s
epoch 51 | loss: 0.2543  | val_0_rmse: 0.4879  | val_1_rmse: 0.47284 |  0:00:49s
epoch 52 | loss: 0.25244 | val_0_rmse: 0.49283 | val_1_rmse: 0.47357 |  0:00:50s
epoch 53 | loss: 0.25472 | val_0_rmse: 0.49829 | val_1_rmse: 0.48589 |  0:00:51s
epoch 54 | loss: 0.24931 | val_0_rmse: 0.50542 | val_1_rmse: 0.49465 |  0:00:52s
epoch 55 | loss: 0.25975 | val_0_rmse: 0.50322 | val_1_rmse: 0.48941 |  0:00:53s
epoch 56 | loss: 0.25119 | val_0_rmse: 0.48834 | val_1_rmse: 0.47735 |  0:00:54s
epoch 57 | loss: 0.25004 | val_0_rmse: 0.49597 | val_1_rmse: 0.48318 |  0:00:55s
epoch 58 | loss: 0.25058 | val_0_rmse: 0.48803 | val_1_rmse: 0.47824 |  0:00:56s
epoch 59 | loss: 0.24881 | val_0_rmse: 0.49335 | val_1_rmse: 0.4812  |  0:00:57s
epoch 60 | loss: 0.2471  | val_0_rmse: 0.48163 | val_1_rmse: 0.4706  |  0:00:57s
epoch 61 | loss: 0.24412 | val_0_rmse: 0.4824  | val_1_rmse: 0.47315 |  0:00:58s
epoch 62 | loss: 0.24263 | val_0_rmse: 0.48413 | val_1_rmse: 0.47832 |  0:00:59s
epoch 63 | loss: 0.24534 | val_0_rmse: 0.50064 | val_1_rmse: 0.49655 |  0:01:00s
epoch 64 | loss: 0.25786 | val_0_rmse: 0.48836 | val_1_rmse: 0.482   |  0:01:01s
epoch 65 | loss: 0.2435  | val_0_rmse: 0.4874  | val_1_rmse: 0.47956 |  0:01:02s
epoch 66 | loss: 0.24189 | val_0_rmse: 0.47861 | val_1_rmse: 0.46983 |  0:01:03s
epoch 67 | loss: 0.24232 | val_0_rmse: 0.48194 | val_1_rmse: 0.47396 |  0:01:04s
epoch 68 | loss: 0.23996 | val_0_rmse: 0.47719 | val_1_rmse: 0.46857 |  0:01:05s
epoch 69 | loss: 0.237   | val_0_rmse: 0.47357 | val_1_rmse: 0.46537 |  0:01:06s
epoch 70 | loss: 0.23851 | val_0_rmse: 0.48192 | val_1_rmse: 0.47294 |  0:01:07s
epoch 71 | loss: 0.24292 | val_0_rmse: 0.48416 | val_1_rmse: 0.47679 |  0:01:08s
epoch 72 | loss: 0.23818 | val_0_rmse: 0.48315 | val_1_rmse: 0.47483 |  0:01:09s
epoch 73 | loss: 0.24585 | val_0_rmse: 0.48152 | val_1_rmse: 0.47159 |  0:01:10s
epoch 74 | loss: 0.24009 | val_0_rmse: 0.48428 | val_1_rmse: 0.4755  |  0:01:11s
epoch 75 | loss: 0.24236 | val_0_rmse: 0.47305 | val_1_rmse: 0.46127 |  0:01:12s
epoch 76 | loss: 0.23473 | val_0_rmse: 0.46855 | val_1_rmse: 0.46071 |  0:01:13s
epoch 77 | loss: 0.2362  | val_0_rmse: 0.47083 | val_1_rmse: 0.46051 |  0:01:14s
epoch 78 | loss: 0.23828 | val_0_rmse: 0.48017 | val_1_rmse: 0.46678 |  0:01:14s
epoch 79 | loss: 0.2347  | val_0_rmse: 0.47561 | val_1_rmse: 0.46353 |  0:01:15s
epoch 80 | loss: 0.23281 | val_0_rmse: 0.47388 | val_1_rmse: 0.46384 |  0:01:16s
epoch 81 | loss: 0.23426 | val_0_rmse: 0.46238 | val_1_rmse: 0.45125 |  0:01:17s
epoch 82 | loss: 0.23353 | val_0_rmse: 0.47595 | val_1_rmse: 0.4705  |  0:01:18s
epoch 83 | loss: 0.23301 | val_0_rmse: 0.46653 | val_1_rmse: 0.45619 |  0:01:19s
epoch 84 | loss: 0.23476 | val_0_rmse: 0.47984 | val_1_rmse: 0.46624 |  0:01:20s
epoch 85 | loss: 0.23942 | val_0_rmse: 0.47324 | val_1_rmse: 0.45757 |  0:01:21s
epoch 86 | loss: 0.23203 | val_0_rmse: 0.47667 | val_1_rmse: 0.46293 |  0:01:22s
epoch 87 | loss: 0.23529 | val_0_rmse: 0.48576 | val_1_rmse: 0.47908 |  0:01:23s
epoch 88 | loss: 0.23958 | val_0_rmse: 0.47083 | val_1_rmse: 0.45666 |  0:01:24s
epoch 89 | loss: 0.243   | val_0_rmse: 0.47831 | val_1_rmse: 0.46929 |  0:01:25s
epoch 90 | loss: 0.23681 | val_0_rmse: 0.487   | val_1_rmse: 0.47813 |  0:01:26s
epoch 91 | loss: 0.23652 | val_0_rmse: 0.46507 | val_1_rmse: 0.45447 |  0:01:27s
epoch 92 | loss: 0.23585 | val_0_rmse: 0.47333 | val_1_rmse: 0.46118 |  0:01:28s
epoch 93 | loss: 0.22991 | val_0_rmse: 0.46658 | val_1_rmse: 0.45928 |  0:01:29s
epoch 94 | loss: 0.22672 | val_0_rmse: 0.47087 | val_1_rmse: 0.46029 |  0:01:30s
epoch 95 | loss: 0.2281  | val_0_rmse: 0.46974 | val_1_rmse: 0.46189 |  0:01:30s
epoch 96 | loss: 0.23289 | val_0_rmse: 0.46627 | val_1_rmse: 0.45377 |  0:01:31s
epoch 97 | loss: 0.22485 | val_0_rmse: 0.4546  | val_1_rmse: 0.44532 |  0:01:32s
epoch 98 | loss: 0.22484 | val_0_rmse: 0.46287 | val_1_rmse: 0.4494  |  0:01:33s
epoch 99 | loss: 0.22268 | val_0_rmse: 0.46832 | val_1_rmse: 0.46265 |  0:01:34s
epoch 100| loss: 0.23645 | val_0_rmse: 0.47565 | val_1_rmse: 0.46699 |  0:01:35s
epoch 101| loss: 0.23477 | val_0_rmse: 0.45949 | val_1_rmse: 0.45047 |  0:01:36s
epoch 102| loss: 0.24051 | val_0_rmse: 0.47363 | val_1_rmse: 0.46104 |  0:01:37s
epoch 103| loss: 0.23272 | val_0_rmse: 0.46808 | val_1_rmse: 0.45494 |  0:01:38s
epoch 104| loss: 0.23257 | val_0_rmse: 0.46518 | val_1_rmse: 0.45874 |  0:01:39s
epoch 105| loss: 0.22811 | val_0_rmse: 0.46525 | val_1_rmse: 0.4535  |  0:01:40s
epoch 106| loss: 0.23083 | val_0_rmse: 0.46446 | val_1_rmse: 0.45363 |  0:01:41s
epoch 107| loss: 0.23414 | val_0_rmse: 0.4741  | val_1_rmse: 0.46476 |  0:01:42s
epoch 108| loss: 0.22961 | val_0_rmse: 0.46208 | val_1_rmse: 0.45924 |  0:01:43s
epoch 109| loss: 0.22564 | val_0_rmse: 0.4639  | val_1_rmse: 0.4524  |  0:01:44s
epoch 110| loss: 0.22206 | val_0_rmse: 0.46149 | val_1_rmse: 0.45853 |  0:01:45s
epoch 111| loss: 0.22881 | val_0_rmse: 0.46363 | val_1_rmse: 0.45935 |  0:01:46s
epoch 112| loss: 0.22339 | val_0_rmse: 0.45159 | val_1_rmse: 0.43937 |  0:01:47s
epoch 113| loss: 0.22053 | val_0_rmse: 0.4559  | val_1_rmse: 0.44642 |  0:01:47s
epoch 114| loss: 0.22527 | val_0_rmse: 0.45651 | val_1_rmse: 0.44878 |  0:01:48s
epoch 115| loss: 0.22505 | val_0_rmse: 0.45565 | val_1_rmse: 0.44902 |  0:01:49s
epoch 116| loss: 0.22931 | val_0_rmse: 0.45817 | val_1_rmse: 0.45257 |  0:01:50s
epoch 117| loss: 0.22557 | val_0_rmse: 0.47387 | val_1_rmse: 0.46729 |  0:01:51s
epoch 118| loss: 0.24997 | val_0_rmse: 0.48873 | val_1_rmse: 0.48028 |  0:01:52s
epoch 119| loss: 0.25192 | val_0_rmse: 0.48035 | val_1_rmse: 0.46628 |  0:01:53s
epoch 120| loss: 0.24366 | val_0_rmse: 0.47311 | val_1_rmse: 0.46301 |  0:01:54s
epoch 121| loss: 0.24356 | val_0_rmse: 0.47035 | val_1_rmse: 0.45651 |  0:01:55s
epoch 122| loss: 0.23652 | val_0_rmse: 0.48101 | val_1_rmse: 0.47302 |  0:01:56s
epoch 123| loss: 0.2397  | val_0_rmse: 0.48173 | val_1_rmse: 0.46659 |  0:01:57s
epoch 124| loss: 0.24078 | val_0_rmse: 0.46904 | val_1_rmse: 0.45113 |  0:01:58s
epoch 125| loss: 0.23872 | val_0_rmse: 0.46496 | val_1_rmse: 0.45538 |  0:01:59s
epoch 126| loss: 0.23276 | val_0_rmse: 0.46451 | val_1_rmse: 0.45455 |  0:02:00s
epoch 127| loss: 0.22861 | val_0_rmse: 0.459   | val_1_rmse: 0.4492  |  0:02:01s
epoch 128| loss: 0.23237 | val_0_rmse: 0.46399 | val_1_rmse: 0.45757 |  0:02:02s
epoch 129| loss: 0.23066 | val_0_rmse: 0.46718 | val_1_rmse: 0.45722 |  0:02:03s
epoch 130| loss: 0.23738 | val_0_rmse: 0.47813 | val_1_rmse: 0.47104 |  0:02:04s
epoch 131| loss: 0.24314 | val_0_rmse: 0.4763  | val_1_rmse: 0.47347 |  0:02:05s
epoch 132| loss: 0.2402  | val_0_rmse: 0.46423 | val_1_rmse: 0.45803 |  0:02:05s
epoch 133| loss: 0.22749 | val_0_rmse: 0.4701  | val_1_rmse: 0.46175 |  0:02:06s
epoch 134| loss: 0.22962 | val_0_rmse: 0.47876 | val_1_rmse: 0.47007 |  0:02:07s
epoch 135| loss: 0.23386 | val_0_rmse: 0.47288 | val_1_rmse: 0.46187 |  0:02:08s
epoch 136| loss: 0.23155 | val_0_rmse: 0.45968 | val_1_rmse: 0.44723 |  0:02:09s
epoch 137| loss: 0.22458 | val_0_rmse: 0.45539 | val_1_rmse: 0.44722 |  0:02:10s
epoch 138| loss: 0.22571 | val_0_rmse: 0.45314 | val_1_rmse: 0.44558 |  0:02:11s
epoch 139| loss: 0.22371 | val_0_rmse: 0.47991 | val_1_rmse: 0.46798 |  0:02:12s
epoch 140| loss: 0.22808 | val_0_rmse: 0.46958 | val_1_rmse: 0.46704 |  0:02:13s
epoch 141| loss: 0.24048 | val_0_rmse: 0.4812  | val_1_rmse: 0.47239 |  0:02:14s
epoch 142| loss: 0.24614 | val_0_rmse: 0.47901 | val_1_rmse: 0.47205 |  0:02:15s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.43937
Best weights from best epoch are automatically used!
ended training at: 08:26:21
Feature importance:
[('Area', 0.4012286512979747), ('Baths', 0.020743856066435768), ('Beds', 0.03955779951925143), ('Latitude', 0.41965696698570715), ('Longitude', 0.02654396220273561), ('Month', 0.0), ('Year', 0.09226876392789535)]
Mean squared error is of 6348809792.6074705
Mean absolute error:56950.819081970316
MAPE:0.15104138232920586
R2 score:0.7937536230952811
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:26:21
epoch 0  | loss: 0.65858 | val_0_rmse: 0.67153 | val_1_rmse: 0.66302 |  0:00:00s
epoch 1  | loss: 0.36891 | val_0_rmse: 0.61171 | val_1_rmse: 0.61391 |  0:00:01s
epoch 2  | loss: 0.32656 | val_0_rmse: 0.54976 | val_1_rmse: 0.55401 |  0:00:02s
epoch 3  | loss: 0.30844 | val_0_rmse: 0.55786 | val_1_rmse: 0.56528 |  0:00:03s
epoch 4  | loss: 0.30077 | val_0_rmse: 0.53493 | val_1_rmse: 0.54713 |  0:00:04s
epoch 5  | loss: 0.28914 | val_0_rmse: 0.53267 | val_1_rmse: 0.54885 |  0:00:05s
epoch 6  | loss: 0.28065 | val_0_rmse: 0.50966 | val_1_rmse: 0.52083 |  0:00:06s
epoch 7  | loss: 0.27438 | val_0_rmse: 0.50216 | val_1_rmse: 0.51261 |  0:00:07s
epoch 8  | loss: 0.27025 | val_0_rmse: 0.49913 | val_1_rmse: 0.51061 |  0:00:08s
epoch 9  | loss: 0.26461 | val_0_rmse: 0.51289 | val_1_rmse: 0.52133 |  0:00:09s
epoch 10 | loss: 0.2579  | val_0_rmse: 0.48645 | val_1_rmse: 0.49933 |  0:00:10s
epoch 11 | loss: 0.25276 | val_0_rmse: 0.48196 | val_1_rmse: 0.49399 |  0:00:11s
epoch 12 | loss: 0.25197 | val_0_rmse: 0.47835 | val_1_rmse: 0.49131 |  0:00:12s
epoch 13 | loss: 0.25034 | val_0_rmse: 0.49771 | val_1_rmse: 0.50993 |  0:00:13s
epoch 14 | loss: 0.25096 | val_0_rmse: 0.49446 | val_1_rmse: 0.4986  |  0:00:14s
epoch 15 | loss: 0.25046 | val_0_rmse: 0.48147 | val_1_rmse: 0.49372 |  0:00:15s
epoch 16 | loss: 0.24919 | val_0_rmse: 0.47852 | val_1_rmse: 0.48769 |  0:00:16s
epoch 17 | loss: 0.24402 | val_0_rmse: 0.47493 | val_1_rmse: 0.48961 |  0:00:16s
epoch 18 | loss: 0.23779 | val_0_rmse: 0.48225 | val_1_rmse: 0.49566 |  0:00:17s
epoch 19 | loss: 0.23639 | val_0_rmse: 0.46992 | val_1_rmse: 0.48206 |  0:00:18s
epoch 20 | loss: 0.23218 | val_0_rmse: 0.45857 | val_1_rmse: 0.47428 |  0:00:19s
epoch 21 | loss: 0.23143 | val_0_rmse: 0.45934 | val_1_rmse: 0.47274 |  0:00:20s
epoch 22 | loss: 0.22606 | val_0_rmse: 0.45318 | val_1_rmse: 0.46764 |  0:00:21s
epoch 23 | loss: 0.22648 | val_0_rmse: 0.45501 | val_1_rmse: 0.47139 |  0:00:22s
epoch 24 | loss: 0.23187 | val_0_rmse: 0.46728 | val_1_rmse: 0.4835  |  0:00:23s
epoch 25 | loss: 0.22669 | val_0_rmse: 0.46098 | val_1_rmse: 0.47278 |  0:00:24s
epoch 26 | loss: 0.2254  | val_0_rmse: 0.45576 | val_1_rmse: 0.47423 |  0:00:25s
epoch 27 | loss: 0.22839 | val_0_rmse: 0.47375 | val_1_rmse: 0.48966 |  0:00:26s
epoch 28 | loss: 0.22853 | val_0_rmse: 0.45168 | val_1_rmse: 0.46877 |  0:00:27s
epoch 29 | loss: 0.22286 | val_0_rmse: 0.46908 | val_1_rmse: 0.48965 |  0:00:28s
epoch 30 | loss: 0.22194 | val_0_rmse: 0.45726 | val_1_rmse: 0.47883 |  0:00:29s
epoch 31 | loss: 0.22399 | val_0_rmse: 0.45234 | val_1_rmse: 0.47169 |  0:00:30s
epoch 32 | loss: 0.22678 | val_0_rmse: 0.45663 | val_1_rmse: 0.47224 |  0:00:31s
epoch 33 | loss: 0.21611 | val_0_rmse: 0.44537 | val_1_rmse: 0.46417 |  0:00:32s
epoch 34 | loss: 0.22766 | val_0_rmse: 0.44404 | val_1_rmse: 0.46203 |  0:00:33s
epoch 35 | loss: 0.22479 | val_0_rmse: 0.45012 | val_1_rmse: 0.47093 |  0:00:34s
epoch 36 | loss: 0.22346 | val_0_rmse: 0.46035 | val_1_rmse: 0.4799  |  0:00:35s
epoch 37 | loss: 0.21892 | val_0_rmse: 0.45181 | val_1_rmse: 0.4746  |  0:00:35s
epoch 38 | loss: 0.22321 | val_0_rmse: 0.45109 | val_1_rmse: 0.46674 |  0:00:36s
epoch 39 | loss: 0.222   | val_0_rmse: 0.45229 | val_1_rmse: 0.47515 |  0:00:37s
epoch 40 | loss: 0.2257  | val_0_rmse: 0.45622 | val_1_rmse: 0.47245 |  0:00:38s
epoch 41 | loss: 0.22321 | val_0_rmse: 0.44856 | val_1_rmse: 0.4675  |  0:00:39s
epoch 42 | loss: 0.21935 | val_0_rmse: 0.43715 | val_1_rmse: 0.45863 |  0:00:40s
epoch 43 | loss: 0.2181  | val_0_rmse: 0.4618  | val_1_rmse: 0.48355 |  0:00:41s
epoch 44 | loss: 0.21782 | val_0_rmse: 0.4412  | val_1_rmse: 0.46401 |  0:00:42s
epoch 45 | loss: 0.21318 | val_0_rmse: 0.44275 | val_1_rmse: 0.46116 |  0:00:43s
epoch 46 | loss: 0.21179 | val_0_rmse: 0.4461  | val_1_rmse: 0.46971 |  0:00:44s
epoch 47 | loss: 0.22033 | val_0_rmse: 0.44657 | val_1_rmse: 0.46779 |  0:00:45s
epoch 48 | loss: 0.21683 | val_0_rmse: 0.4507  | val_1_rmse: 0.46928 |  0:00:46s
epoch 49 | loss: 0.21591 | val_0_rmse: 0.44857 | val_1_rmse: 0.4701  |  0:00:47s
epoch 50 | loss: 0.21416 | val_0_rmse: 0.44918 | val_1_rmse: 0.46986 |  0:00:48s
epoch 51 | loss: 0.21981 | val_0_rmse: 0.44047 | val_1_rmse: 0.46268 |  0:00:49s
epoch 52 | loss: 0.21424 | val_0_rmse: 0.44345 | val_1_rmse: 0.46379 |  0:00:50s
epoch 53 | loss: 0.21293 | val_0_rmse: 0.45665 | val_1_rmse: 0.47644 |  0:00:51s
epoch 54 | loss: 0.2159  | val_0_rmse: 0.44912 | val_1_rmse: 0.47047 |  0:00:52s
epoch 55 | loss: 0.21364 | val_0_rmse: 0.44691 | val_1_rmse: 0.47228 |  0:00:53s
epoch 56 | loss: 0.2178  | val_0_rmse: 0.44185 | val_1_rmse: 0.46079 |  0:00:53s
epoch 57 | loss: 0.21953 | val_0_rmse: 0.45008 | val_1_rmse: 0.47048 |  0:00:54s
epoch 58 | loss: 0.21546 | val_0_rmse: 0.43057 | val_1_rmse: 0.45192 |  0:00:55s
epoch 59 | loss: 0.21613 | val_0_rmse: 0.44858 | val_1_rmse: 0.47045 |  0:00:56s
epoch 60 | loss: 0.21074 | val_0_rmse: 0.43393 | val_1_rmse: 0.45803 |  0:00:57s
epoch 61 | loss: 0.21583 | val_0_rmse: 0.44384 | val_1_rmse: 0.46496 |  0:00:58s
epoch 62 | loss: 0.21115 | val_0_rmse: 0.4468  | val_1_rmse: 0.46459 |  0:00:59s
epoch 63 | loss: 0.21778 | val_0_rmse: 0.43706 | val_1_rmse: 0.46174 |  0:01:00s
epoch 64 | loss: 0.20849 | val_0_rmse: 0.44046 | val_1_rmse: 0.46502 |  0:01:01s
epoch 65 | loss: 0.20739 | val_0_rmse: 0.43315 | val_1_rmse: 0.45803 |  0:01:02s
epoch 66 | loss: 0.20631 | val_0_rmse: 0.43745 | val_1_rmse: 0.46029 |  0:01:03s
epoch 67 | loss: 0.21192 | val_0_rmse: 0.44276 | val_1_rmse: 0.46854 |  0:01:04s
epoch 68 | loss: 0.2117  | val_0_rmse: 0.45053 | val_1_rmse: 0.46749 |  0:01:05s
epoch 69 | loss: 0.21918 | val_0_rmse: 0.46924 | val_1_rmse: 0.48269 |  0:01:06s
epoch 70 | loss: 0.21506 | val_0_rmse: 0.44051 | val_1_rmse: 0.46049 |  0:01:07s
epoch 71 | loss: 0.20819 | val_0_rmse: 0.44965 | val_1_rmse: 0.47406 |  0:01:08s
epoch 72 | loss: 0.21006 | val_0_rmse: 0.44712 | val_1_rmse: 0.46654 |  0:01:09s
epoch 73 | loss: 0.20946 | val_0_rmse: 0.44577 | val_1_rmse: 0.4656  |  0:01:09s
epoch 74 | loss: 0.21706 | val_0_rmse: 0.44806 | val_1_rmse: 0.46957 |  0:01:10s
epoch 75 | loss: 0.21892 | val_0_rmse: 0.44227 | val_1_rmse: 0.46678 |  0:01:11s
epoch 76 | loss: 0.20952 | val_0_rmse: 0.44102 | val_1_rmse: 0.46556 |  0:01:12s
epoch 77 | loss: 0.20369 | val_0_rmse: 0.4371  | val_1_rmse: 0.46238 |  0:01:13s
epoch 78 | loss: 0.21418 | val_0_rmse: 0.44658 | val_1_rmse: 0.4701  |  0:01:14s
epoch 79 | loss: 0.21159 | val_0_rmse: 0.45014 | val_1_rmse: 0.47353 |  0:01:15s
epoch 80 | loss: 0.20827 | val_0_rmse: 0.46879 | val_1_rmse: 0.48443 |  0:01:16s
epoch 81 | loss: 0.21194 | val_0_rmse: 0.43683 | val_1_rmse: 0.46012 |  0:01:17s
epoch 82 | loss: 0.20693 | val_0_rmse: 0.43397 | val_1_rmse: 0.45453 |  0:01:18s
epoch 83 | loss: 0.21474 | val_0_rmse: 0.43246 | val_1_rmse: 0.45384 |  0:01:19s
epoch 84 | loss: 0.2112  | val_0_rmse: 0.44387 | val_1_rmse: 0.4684  |  0:01:20s
epoch 85 | loss: 0.21569 | val_0_rmse: 0.44932 | val_1_rmse: 0.46721 |  0:01:21s
epoch 86 | loss: 0.21754 | val_0_rmse: 0.45005 | val_1_rmse: 0.47073 |  0:01:22s
epoch 87 | loss: 0.21169 | val_0_rmse: 0.44172 | val_1_rmse: 0.45887 |  0:01:23s
epoch 88 | loss: 0.20961 | val_0_rmse: 0.45098 | val_1_rmse: 0.47243 |  0:01:24s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.45192
Best weights from best epoch are automatically used!
ended training at: 08:27:45
Feature importance:
[('Area', 0.3475705937847175), ('Baths', 0.07122841630300597), ('Beds', 0.038847786024694124), ('Latitude', 0.3634350959374996), ('Longitude', 0.024256001897625487), ('Month', 0.007670184147399283), ('Year', 0.14699192190505803)]
Mean squared error is of 6056960661.0321665
Mean absolute error:55038.15495288123
MAPE:0.145739499919833
R2 score:0.7997570387094132
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:27:46
epoch 0  | loss: 0.69597 | val_0_rmse: 0.8185  | val_1_rmse: 0.82417 |  0:00:00s
epoch 1  | loss: 0.3894  | val_0_rmse: 0.60481 | val_1_rmse: 0.61126 |  0:00:01s
epoch 2  | loss: 0.33868 | val_0_rmse: 0.57161 | val_1_rmse: 0.56937 |  0:00:02s
epoch 3  | loss: 0.31905 | val_0_rmse: 0.5441  | val_1_rmse: 0.54861 |  0:00:03s
epoch 4  | loss: 0.30174 | val_0_rmse: 0.55463 | val_1_rmse: 0.55974 |  0:00:04s
epoch 5  | loss: 0.30751 | val_0_rmse: 0.53452 | val_1_rmse: 0.53585 |  0:00:05s
epoch 6  | loss: 0.30217 | val_0_rmse: 0.55623 | val_1_rmse: 0.56465 |  0:00:06s
epoch 7  | loss: 0.31098 | val_0_rmse: 0.54599 | val_1_rmse: 0.54745 |  0:00:07s
epoch 8  | loss: 0.29969 | val_0_rmse: 0.55075 | val_1_rmse: 0.55223 |  0:00:08s
epoch 9  | loss: 0.30068 | val_0_rmse: 0.53383 | val_1_rmse: 0.53358 |  0:00:09s
epoch 10 | loss: 0.30154 | val_0_rmse: 0.53107 | val_1_rmse: 0.53196 |  0:00:10s
epoch 11 | loss: 0.29564 | val_0_rmse: 0.5281  | val_1_rmse: 0.53163 |  0:00:11s
epoch 12 | loss: 0.29461 | val_0_rmse: 0.52499 | val_1_rmse: 0.52818 |  0:00:12s
epoch 13 | loss: 0.28726 | val_0_rmse: 0.52289 | val_1_rmse: 0.52662 |  0:00:13s
epoch 14 | loss: 0.2907  | val_0_rmse: 0.52644 | val_1_rmse: 0.53168 |  0:00:14s
epoch 15 | loss: 0.29005 | val_0_rmse: 0.5319  | val_1_rmse: 0.53593 |  0:00:15s
epoch 16 | loss: 0.29297 | val_0_rmse: 0.52771 | val_1_rmse: 0.53194 |  0:00:16s
epoch 17 | loss: 0.28898 | val_0_rmse: 0.51993 | val_1_rmse: 0.52212 |  0:00:17s
epoch 18 | loss: 0.28282 | val_0_rmse: 0.52014 | val_1_rmse: 0.52066 |  0:00:18s
epoch 19 | loss: 0.28813 | val_0_rmse: 0.53845 | val_1_rmse: 0.54149 |  0:00:18s
epoch 20 | loss: 0.2821  | val_0_rmse: 0.51501 | val_1_rmse: 0.5191  |  0:00:19s
epoch 21 | loss: 0.27991 | val_0_rmse: 0.51164 | val_1_rmse: 0.51603 |  0:00:20s
epoch 22 | loss: 0.27544 | val_0_rmse: 0.51339 | val_1_rmse: 0.5151  |  0:00:21s
epoch 23 | loss: 0.27825 | val_0_rmse: 0.50901 | val_1_rmse: 0.51217 |  0:00:22s
epoch 24 | loss: 0.27865 | val_0_rmse: 0.53876 | val_1_rmse: 0.54494 |  0:00:23s
epoch 25 | loss: 0.28744 | val_0_rmse: 0.51968 | val_1_rmse: 0.5203  |  0:00:24s
epoch 26 | loss: 0.27534 | val_0_rmse: 0.5235  | val_1_rmse: 0.52327 |  0:00:25s
epoch 27 | loss: 0.28207 | val_0_rmse: 0.52604 | val_1_rmse: 0.5292  |  0:00:26s
epoch 28 | loss: 0.27656 | val_0_rmse: 0.51778 | val_1_rmse: 0.5198  |  0:00:27s
epoch 29 | loss: 0.28152 | val_0_rmse: 0.51605 | val_1_rmse: 0.5182  |  0:00:28s
epoch 30 | loss: 0.27399 | val_0_rmse: 0.5141  | val_1_rmse: 0.51498 |  0:00:29s
epoch 31 | loss: 0.27349 | val_0_rmse: 0.51923 | val_1_rmse: 0.52269 |  0:00:30s
epoch 32 | loss: 0.28447 | val_0_rmse: 0.51204 | val_1_rmse: 0.51411 |  0:00:31s
epoch 33 | loss: 0.2801  | val_0_rmse: 0.51861 | val_1_rmse: 0.52393 |  0:00:32s
epoch 34 | loss: 0.27837 | val_0_rmse: 0.50738 | val_1_rmse: 0.51181 |  0:00:33s
epoch 35 | loss: 0.27302 | val_0_rmse: 0.50685 | val_1_rmse: 0.50772 |  0:00:34s
epoch 36 | loss: 0.26956 | val_0_rmse: 0.50645 | val_1_rmse: 0.50702 |  0:00:35s
epoch 37 | loss: 0.27319 | val_0_rmse: 0.50668 | val_1_rmse: 0.50859 |  0:00:35s
epoch 38 | loss: 0.26647 | val_0_rmse: 0.50933 | val_1_rmse: 0.50856 |  0:00:36s
epoch 39 | loss: 0.26975 | val_0_rmse: 0.50927 | val_1_rmse: 0.50778 |  0:00:37s
epoch 40 | loss: 0.27182 | val_0_rmse: 0.50991 | val_1_rmse: 0.50832 |  0:00:38s
epoch 41 | loss: 0.26897 | val_0_rmse: 0.50142 | val_1_rmse: 0.49938 |  0:00:39s
epoch 42 | loss: 0.26036 | val_0_rmse: 0.49988 | val_1_rmse: 0.49889 |  0:00:40s
epoch 43 | loss: 0.25788 | val_0_rmse: 0.50259 | val_1_rmse: 0.50278 |  0:00:41s
epoch 44 | loss: 0.26259 | val_0_rmse: 0.497   | val_1_rmse: 0.49587 |  0:00:42s
epoch 45 | loss: 0.26018 | val_0_rmse: 0.50121 | val_1_rmse: 0.50154 |  0:00:43s
epoch 46 | loss: 0.26    | val_0_rmse: 0.4874  | val_1_rmse: 0.48853 |  0:00:44s
epoch 47 | loss: 0.24982 | val_0_rmse: 0.51104 | val_1_rmse: 0.50932 |  0:00:45s
epoch 48 | loss: 0.24959 | val_0_rmse: 0.49664 | val_1_rmse: 0.4978  |  0:00:46s
epoch 49 | loss: 0.24654 | val_0_rmse: 0.47417 | val_1_rmse: 0.47355 |  0:00:47s
epoch 50 | loss: 0.24196 | val_0_rmse: 0.48019 | val_1_rmse: 0.48225 |  0:00:48s
epoch 51 | loss: 0.24598 | val_0_rmse: 0.47699 | val_1_rmse: 0.47995 |  0:00:49s
epoch 52 | loss: 0.23628 | val_0_rmse: 0.4734  | val_1_rmse: 0.47713 |  0:00:50s
epoch 53 | loss: 0.24437 | val_0_rmse: 0.47905 | val_1_rmse: 0.48143 |  0:00:51s
epoch 54 | loss: 0.25342 | val_0_rmse: 0.49486 | val_1_rmse: 0.49461 |  0:00:51s
epoch 55 | loss: 0.24468 | val_0_rmse: 0.49616 | val_1_rmse: 0.49909 |  0:00:52s
epoch 56 | loss: 0.23735 | val_0_rmse: 0.48918 | val_1_rmse: 0.49532 |  0:00:53s
epoch 57 | loss: 0.23599 | val_0_rmse: 0.46511 | val_1_rmse: 0.46768 |  0:00:54s
epoch 58 | loss: 0.23798 | val_0_rmse: 0.47119 | val_1_rmse: 0.47015 |  0:00:55s
epoch 59 | loss: 0.22873 | val_0_rmse: 0.4701  | val_1_rmse: 0.47788 |  0:00:56s
epoch 60 | loss: 0.23209 | val_0_rmse: 0.47527 | val_1_rmse: 0.47408 |  0:00:57s
epoch 61 | loss: 0.23774 | val_0_rmse: 0.50258 | val_1_rmse: 0.50684 |  0:00:58s
epoch 62 | loss: 0.23883 | val_0_rmse: 0.51237 | val_1_rmse: 0.51344 |  0:00:59s
epoch 63 | loss: 0.24209 | val_0_rmse: 0.49099 | val_1_rmse: 0.49255 |  0:01:00s
epoch 64 | loss: 0.24334 | val_0_rmse: 0.4787  | val_1_rmse: 0.47928 |  0:01:01s
epoch 65 | loss: 0.23337 | val_0_rmse: 0.47401 | val_1_rmse: 0.47498 |  0:01:02s
epoch 66 | loss: 0.24399 | val_0_rmse: 0.48132 | val_1_rmse: 0.48152 |  0:01:03s
epoch 67 | loss: 0.2367  | val_0_rmse: 0.48934 | val_1_rmse: 0.49638 |  0:01:04s
epoch 68 | loss: 0.23781 | val_0_rmse: 0.48559 | val_1_rmse: 0.48374 |  0:01:05s
epoch 69 | loss: 0.23518 | val_0_rmse: 0.47474 | val_1_rmse: 0.47459 |  0:01:06s
epoch 70 | loss: 0.23049 | val_0_rmse: 0.4609  | val_1_rmse: 0.46534 |  0:01:07s
epoch 71 | loss: 0.24101 | val_0_rmse: 0.50291 | val_1_rmse: 0.50643 |  0:01:07s
epoch 72 | loss: 0.23847 | val_0_rmse: 0.46616 | val_1_rmse: 0.46769 |  0:01:08s
epoch 73 | loss: 0.23032 | val_0_rmse: 0.46041 | val_1_rmse: 0.4649  |  0:01:09s
epoch 74 | loss: 0.23269 | val_0_rmse: 0.46418 | val_1_rmse: 0.46958 |  0:01:10s
epoch 75 | loss: 0.22753 | val_0_rmse: 0.46991 | val_1_rmse: 0.47192 |  0:01:11s
epoch 76 | loss: 0.23098 | val_0_rmse: 0.49747 | val_1_rmse: 0.50492 |  0:01:12s
epoch 77 | loss: 0.23296 | val_0_rmse: 0.46872 | val_1_rmse: 0.47553 |  0:01:13s
epoch 78 | loss: 0.23135 | val_0_rmse: 0.45887 | val_1_rmse: 0.45799 |  0:01:14s
epoch 79 | loss: 0.23318 | val_0_rmse: 0.46064 | val_1_rmse: 0.46536 |  0:01:15s
epoch 80 | loss: 0.23683 | val_0_rmse: 0.48833 | val_1_rmse: 0.48777 |  0:01:16s
epoch 81 | loss: 0.23073 | val_0_rmse: 0.46682 | val_1_rmse: 0.46743 |  0:01:17s
epoch 82 | loss: 0.23252 | val_0_rmse: 0.45649 | val_1_rmse: 0.46075 |  0:01:18s
epoch 83 | loss: 0.22831 | val_0_rmse: 0.49395 | val_1_rmse: 0.49487 |  0:01:19s
epoch 84 | loss: 0.23039 | val_0_rmse: 0.45873 | val_1_rmse: 0.46298 |  0:01:20s
epoch 85 | loss: 0.22086 | val_0_rmse: 0.45552 | val_1_rmse: 0.45945 |  0:01:21s
epoch 86 | loss: 0.22535 | val_0_rmse: 0.46014 | val_1_rmse: 0.46014 |  0:01:22s
epoch 87 | loss: 0.2296  | val_0_rmse: 0.46678 | val_1_rmse: 0.47037 |  0:01:23s
epoch 88 | loss: 0.23397 | val_0_rmse: 0.47712 | val_1_rmse: 0.48504 |  0:01:24s
epoch 89 | loss: 0.23627 | val_0_rmse: 0.47809 | val_1_rmse: 0.47422 |  0:01:25s
epoch 90 | loss: 0.23636 | val_0_rmse: 0.47307 | val_1_rmse: 0.47352 |  0:01:25s
epoch 91 | loss: 0.22869 | val_0_rmse: 0.46458 | val_1_rmse: 0.46786 |  0:01:26s
epoch 92 | loss: 0.23071 | val_0_rmse: 0.48664 | val_1_rmse: 0.48598 |  0:01:27s
epoch 93 | loss: 0.23038 | val_0_rmse: 0.46177 | val_1_rmse: 0.46408 |  0:01:28s
epoch 94 | loss: 0.23072 | val_0_rmse: 0.47795 | val_1_rmse: 0.48097 |  0:01:29s
epoch 95 | loss: 0.23051 | val_0_rmse: 0.46453 | val_1_rmse: 0.468   |  0:01:30s
epoch 96 | loss: 0.2273  | val_0_rmse: 0.45881 | val_1_rmse: 0.46334 |  0:01:31s
epoch 97 | loss: 0.23036 | val_0_rmse: 0.45888 | val_1_rmse: 0.45889 |  0:01:32s
epoch 98 | loss: 0.22565 | val_0_rmse: 0.47191 | val_1_rmse: 0.47407 |  0:01:33s
epoch 99 | loss: 0.22799 | val_0_rmse: 0.45932 | val_1_rmse: 0.4631  |  0:01:34s
epoch 100| loss: 0.22847 | val_0_rmse: 0.45826 | val_1_rmse: 0.46376 |  0:01:35s
epoch 101| loss: 0.22659 | val_0_rmse: 0.46969 | val_1_rmse: 0.46751 |  0:01:36s
epoch 102| loss: 0.23784 | val_0_rmse: 0.4581  | val_1_rmse: 0.45497 |  0:01:37s
epoch 103| loss: 0.22372 | val_0_rmse: 0.4572  | val_1_rmse: 0.45842 |  0:01:38s
epoch 104| loss: 0.21988 | val_0_rmse: 0.48211 | val_1_rmse: 0.4847  |  0:01:39s
epoch 105| loss: 0.2211  | val_0_rmse: 0.45496 | val_1_rmse: 0.4585  |  0:01:40s
epoch 106| loss: 0.22447 | val_0_rmse: 0.45734 | val_1_rmse: 0.45555 |  0:01:41s
epoch 107| loss: 0.22526 | val_0_rmse: 0.46409 | val_1_rmse: 0.46849 |  0:01:41s
epoch 108| loss: 0.22408 | val_0_rmse: 0.51239 | val_1_rmse: 0.51193 |  0:01:42s
epoch 109| loss: 0.22792 | val_0_rmse: 0.46632 | val_1_rmse: 0.4688  |  0:01:43s
epoch 110| loss: 0.22713 | val_0_rmse: 0.46092 | val_1_rmse: 0.46233 |  0:01:44s
epoch 111| loss: 0.23042 | val_0_rmse: 0.46881 | val_1_rmse: 0.47276 |  0:01:45s
epoch 112| loss: 0.2361  | val_0_rmse: 0.49685 | val_1_rmse: 0.50114 |  0:01:46s
epoch 113| loss: 0.23487 | val_0_rmse: 0.48439 | val_1_rmse: 0.4839  |  0:01:47s
epoch 114| loss: 0.22961 | val_0_rmse: 0.4682  | val_1_rmse: 0.47257 |  0:01:48s
epoch 115| loss: 0.22906 | val_0_rmse: 0.46319 | val_1_rmse: 0.46333 |  0:01:49s
epoch 116| loss: 0.23348 | val_0_rmse: 0.46119 | val_1_rmse: 0.46771 |  0:01:50s
epoch 117| loss: 0.21903 | val_0_rmse: 0.45902 | val_1_rmse: 0.45565 |  0:01:51s
epoch 118| loss: 0.22271 | val_0_rmse: 0.4556  | val_1_rmse: 0.45791 |  0:01:52s
epoch 119| loss: 0.22186 | val_0_rmse: 0.45371 | val_1_rmse: 0.4567  |  0:01:53s
epoch 120| loss: 0.21838 | val_0_rmse: 0.45417 | val_1_rmse: 0.45586 |  0:01:54s
epoch 121| loss: 0.22753 | val_0_rmse: 0.46616 | val_1_rmse: 0.46526 |  0:01:55s
epoch 122| loss: 0.22917 | val_0_rmse: 0.48594 | val_1_rmse: 0.48846 |  0:01:56s
epoch 123| loss: 0.23449 | val_0_rmse: 0.47406 | val_1_rmse: 0.47709 |  0:01:57s
epoch 124| loss: 0.23601 | val_0_rmse: 0.48086 | val_1_rmse: 0.4778  |  0:01:58s
epoch 125| loss: 0.23195 | val_0_rmse: 0.50053 | val_1_rmse: 0.50054 |  0:01:58s
epoch 126| loss: 0.22641 | val_0_rmse: 0.46715 | val_1_rmse: 0.46583 |  0:01:59s
epoch 127| loss: 0.23125 | val_0_rmse: 0.459   | val_1_rmse: 0.46463 |  0:02:00s
epoch 128| loss: 0.22716 | val_0_rmse: 0.46512 | val_1_rmse: 0.46394 |  0:02:01s
epoch 129| loss: 0.22606 | val_0_rmse: 0.45525 | val_1_rmse: 0.45726 |  0:02:02s
epoch 130| loss: 0.22394 | val_0_rmse: 0.45975 | val_1_rmse: 0.46112 |  0:02:03s
epoch 131| loss: 0.22755 | val_0_rmse: 0.46356 | val_1_rmse: 0.46439 |  0:02:04s
epoch 132| loss: 0.22847 | val_0_rmse: 0.47358 | val_1_rmse: 0.47162 |  0:02:05s

Early stopping occured at epoch 132 with best_epoch = 102 and best_val_1_rmse = 0.45497
Best weights from best epoch are automatically used!
ended training at: 08:29:51
Feature importance:
[('Area', 0.37884143021379546), ('Baths', 0.1823031087154112), ('Beds', 0.018697926101312646), ('Latitude', 0.31522320370397383), ('Longitude', 0.029474247909191214), ('Month', 0.026432861505901822), ('Year', 0.04902722185041384)]
Mean squared error is of 6018849035.6496
Mean absolute error:55496.9390533153
MAPE:0.14887352272763346
R2 score:0.7987728374943638
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:29:52
epoch 0  | loss: 0.97783 | val_0_rmse: 1.01853 | val_1_rmse: 1.01304 |  0:00:00s
epoch 1  | loss: 0.53082 | val_0_rmse: 0.95188 | val_1_rmse: 0.9525  |  0:00:00s
epoch 2  | loss: 0.44001 | val_0_rmse: 0.79133 | val_1_rmse: 0.7934  |  0:00:01s
epoch 3  | loss: 0.38642 | val_0_rmse: 0.6715  | val_1_rmse: 0.6787  |  0:00:01s
epoch 4  | loss: 0.37968 | val_0_rmse: 0.69259 | val_1_rmse: 0.70044 |  0:00:02s
epoch 5  | loss: 0.38071 | val_0_rmse: 0.63901 | val_1_rmse: 0.64384 |  0:00:02s
epoch 6  | loss: 0.36652 | val_0_rmse: 0.61688 | val_1_rmse: 0.62274 |  0:00:03s
epoch 7  | loss: 0.34669 | val_0_rmse: 0.59762 | val_1_rmse: 0.5939  |  0:00:03s
epoch 8  | loss: 0.34216 | val_0_rmse: 0.56575 | val_1_rmse: 0.5646  |  0:00:04s
epoch 9  | loss: 0.3408  | val_0_rmse: 0.58735 | val_1_rmse: 0.59449 |  0:00:04s
epoch 10 | loss: 0.32854 | val_0_rmse: 0.55802 | val_1_rmse: 0.55958 |  0:00:05s
epoch 11 | loss: 0.3202  | val_0_rmse: 0.55657 | val_1_rmse: 0.56081 |  0:00:05s
epoch 12 | loss: 0.31783 | val_0_rmse: 0.55491 | val_1_rmse: 0.56138 |  0:00:05s
epoch 13 | loss: 0.33105 | val_0_rmse: 0.54685 | val_1_rmse: 0.55245 |  0:00:06s
epoch 14 | loss: 0.32183 | val_0_rmse: 0.55542 | val_1_rmse: 0.56442 |  0:00:06s
epoch 15 | loss: 0.32842 | val_0_rmse: 0.5528  | val_1_rmse: 0.55924 |  0:00:07s
epoch 16 | loss: 0.31646 | val_0_rmse: 0.53838 | val_1_rmse: 0.54268 |  0:00:07s
epoch 17 | loss: 0.30727 | val_0_rmse: 0.54402 | val_1_rmse: 0.55059 |  0:00:08s
epoch 18 | loss: 0.30723 | val_0_rmse: 0.55577 | val_1_rmse: 0.55818 |  0:00:08s
epoch 19 | loss: 0.31317 | val_0_rmse: 0.55098 | val_1_rmse: 0.5576  |  0:00:09s
epoch 20 | loss: 0.30595 | val_0_rmse: 0.5292  | val_1_rmse: 0.53681 |  0:00:09s
epoch 21 | loss: 0.30837 | val_0_rmse: 0.54435 | val_1_rmse: 0.55239 |  0:00:10s
epoch 22 | loss: 0.31085 | val_0_rmse: 0.54668 | val_1_rmse: 0.55826 |  0:00:10s
epoch 23 | loss: 0.31064 | val_0_rmse: 0.52756 | val_1_rmse: 0.53681 |  0:00:10s
epoch 24 | loss: 0.30243 | val_0_rmse: 0.52724 | val_1_rmse: 0.53809 |  0:00:11s
epoch 25 | loss: 0.29023 | val_0_rmse: 0.51665 | val_1_rmse: 0.52347 |  0:00:11s
epoch 26 | loss: 0.28914 | val_0_rmse: 0.52071 | val_1_rmse: 0.52087 |  0:00:12s
epoch 27 | loss: 0.29403 | val_0_rmse: 0.52405 | val_1_rmse: 0.5331  |  0:00:12s
epoch 28 | loss: 0.28858 | val_0_rmse: 0.5254  | val_1_rmse: 0.53631 |  0:00:13s
epoch 29 | loss: 0.29187 | val_0_rmse: 0.51975 | val_1_rmse: 0.52232 |  0:00:13s
epoch 30 | loss: 0.29186 | val_0_rmse: 0.5179  | val_1_rmse: 0.52035 |  0:00:14s
epoch 31 | loss: 0.28795 | val_0_rmse: 0.52336 | val_1_rmse: 0.53308 |  0:00:14s
epoch 32 | loss: 0.28408 | val_0_rmse: 0.50589 | val_1_rmse: 0.51561 |  0:00:15s
epoch 33 | loss: 0.28523 | val_0_rmse: 0.51174 | val_1_rmse: 0.52314 |  0:00:15s
epoch 34 | loss: 0.28145 | val_0_rmse: 0.50594 | val_1_rmse: 0.52176 |  0:00:15s
epoch 35 | loss: 0.28545 | val_0_rmse: 0.53908 | val_1_rmse: 0.55409 |  0:00:16s
epoch 36 | loss: 0.28706 | val_0_rmse: 0.5307  | val_1_rmse: 0.53845 |  0:00:16s
epoch 37 | loss: 0.29487 | val_0_rmse: 0.5301  | val_1_rmse: 0.54288 |  0:00:17s
epoch 38 | loss: 0.28101 | val_0_rmse: 0.55094 | val_1_rmse: 0.56314 |  0:00:17s
epoch 39 | loss: 0.28076 | val_0_rmse: 0.52266 | val_1_rmse: 0.53017 |  0:00:18s
epoch 40 | loss: 0.27799 | val_0_rmse: 0.50824 | val_1_rmse: 0.51733 |  0:00:18s
epoch 41 | loss: 0.2809  | val_0_rmse: 0.51857 | val_1_rmse: 0.52526 |  0:00:19s
epoch 42 | loss: 0.28208 | val_0_rmse: 0.50565 | val_1_rmse: 0.5156  |  0:00:19s
epoch 43 | loss: 0.28229 | val_0_rmse: 0.50065 | val_1_rmse: 0.51311 |  0:00:19s
epoch 44 | loss: 0.27408 | val_0_rmse: 0.51556 | val_1_rmse: 0.52504 |  0:00:20s
epoch 45 | loss: 0.27583 | val_0_rmse: 0.52492 | val_1_rmse: 0.53318 |  0:00:20s
epoch 46 | loss: 0.27397 | val_0_rmse: 0.50244 | val_1_rmse: 0.5198  |  0:00:21s
epoch 47 | loss: 0.27298 | val_0_rmse: 0.49925 | val_1_rmse: 0.5108  |  0:00:21s
epoch 48 | loss: 0.2652  | val_0_rmse: 0.49435 | val_1_rmse: 0.50602 |  0:00:22s
epoch 49 | loss: 0.26737 | val_0_rmse: 0.50237 | val_1_rmse: 0.51314 |  0:00:22s
epoch 50 | loss: 0.26783 | val_0_rmse: 0.50163 | val_1_rmse: 0.51068 |  0:00:23s
epoch 51 | loss: 0.26981 | val_0_rmse: 0.50015 | val_1_rmse: 0.51338 |  0:00:23s
epoch 52 | loss: 0.26443 | val_0_rmse: 0.4974  | val_1_rmse: 0.50721 |  0:00:24s
epoch 53 | loss: 0.26842 | val_0_rmse: 0.49412 | val_1_rmse: 0.50865 |  0:00:24s
epoch 54 | loss: 0.26055 | val_0_rmse: 0.49374 | val_1_rmse: 0.51039 |  0:00:24s
epoch 55 | loss: 0.26191 | val_0_rmse: 0.49166 | val_1_rmse: 0.50587 |  0:00:25s
epoch 56 | loss: 0.26421 | val_0_rmse: 0.49858 | val_1_rmse: 0.51219 |  0:00:25s
epoch 57 | loss: 0.27189 | val_0_rmse: 0.51368 | val_1_rmse: 0.52516 |  0:00:26s
epoch 58 | loss: 0.27551 | val_0_rmse: 0.5163  | val_1_rmse: 0.53299 |  0:00:26s
epoch 59 | loss: 0.27341 | val_0_rmse: 0.52395 | val_1_rmse: 0.53295 |  0:00:27s
epoch 60 | loss: 0.26751 | val_0_rmse: 0.5023  | val_1_rmse: 0.5187  |  0:00:27s
epoch 61 | loss: 0.26108 | val_0_rmse: 0.4889  | val_1_rmse: 0.50413 |  0:00:28s
epoch 62 | loss: 0.25816 | val_0_rmse: 0.49217 | val_1_rmse: 0.50741 |  0:00:28s
epoch 63 | loss: 0.26641 | val_0_rmse: 0.49222 | val_1_rmse: 0.50476 |  0:00:29s
epoch 64 | loss: 0.25914 | val_0_rmse: 0.50291 | val_1_rmse: 0.51326 |  0:00:29s
epoch 65 | loss: 0.26171 | val_0_rmse: 0.49277 | val_1_rmse: 0.50712 |  0:00:29s
epoch 66 | loss: 0.25991 | val_0_rmse: 0.49466 | val_1_rmse: 0.50784 |  0:00:30s
epoch 67 | loss: 0.26264 | val_0_rmse: 0.48917 | val_1_rmse: 0.50453 |  0:00:30s
epoch 68 | loss: 0.25667 | val_0_rmse: 0.4853  | val_1_rmse: 0.50322 |  0:00:31s
epoch 69 | loss: 0.26263 | val_0_rmse: 0.49373 | val_1_rmse: 0.50573 |  0:00:31s
epoch 70 | loss: 0.2675  | val_0_rmse: 0.49129 | val_1_rmse: 0.50672 |  0:00:32s
epoch 71 | loss: 0.26005 | val_0_rmse: 0.50423 | val_1_rmse: 0.52111 |  0:00:32s
epoch 72 | loss: 0.26819 | val_0_rmse: 0.49165 | val_1_rmse: 0.50726 |  0:00:33s
epoch 73 | loss: 0.26114 | val_0_rmse: 0.48679 | val_1_rmse: 0.50414 |  0:00:33s
epoch 74 | loss: 0.26636 | val_0_rmse: 0.49552 | val_1_rmse: 0.51342 |  0:00:33s
epoch 75 | loss: 0.26023 | val_0_rmse: 0.49308 | val_1_rmse: 0.50348 |  0:00:34s
epoch 76 | loss: 0.25667 | val_0_rmse: 0.47953 | val_1_rmse: 0.50011 |  0:00:34s
epoch 77 | loss: 0.2493  | val_0_rmse: 0.48329 | val_1_rmse: 0.50199 |  0:00:35s
epoch 78 | loss: 0.2494  | val_0_rmse: 0.47755 | val_1_rmse: 0.49639 |  0:00:35s
epoch 79 | loss: 0.25561 | val_0_rmse: 0.4849  | val_1_rmse: 0.50117 |  0:00:36s
epoch 80 | loss: 0.25209 | val_0_rmse: 0.48538 | val_1_rmse: 0.50331 |  0:00:36s
epoch 81 | loss: 0.25059 | val_0_rmse: 0.48461 | val_1_rmse: 0.49771 |  0:00:37s
epoch 82 | loss: 0.25114 | val_0_rmse: 0.47896 | val_1_rmse: 0.49516 |  0:00:37s
epoch 83 | loss: 0.25065 | val_0_rmse: 0.48361 | val_1_rmse: 0.50163 |  0:00:38s
epoch 84 | loss: 0.25714 | val_0_rmse: 0.49108 | val_1_rmse: 0.51042 |  0:00:38s
epoch 85 | loss: 0.25444 | val_0_rmse: 0.48379 | val_1_rmse: 0.5035  |  0:00:38s
epoch 86 | loss: 0.25301 | val_0_rmse: 0.48269 | val_1_rmse: 0.50129 |  0:00:39s
epoch 87 | loss: 0.25559 | val_0_rmse: 0.53163 | val_1_rmse: 0.54604 |  0:00:39s
epoch 88 | loss: 0.26333 | val_0_rmse: 0.49312 | val_1_rmse: 0.51408 |  0:00:40s
epoch 89 | loss: 0.25575 | val_0_rmse: 0.48705 | val_1_rmse: 0.50402 |  0:00:40s
epoch 90 | loss: 0.24972 | val_0_rmse: 0.49042 | val_1_rmse: 0.51082 |  0:00:41s
epoch 91 | loss: 0.25835 | val_0_rmse: 0.49846 | val_1_rmse: 0.51894 |  0:00:41s
epoch 92 | loss: 0.25946 | val_0_rmse: 0.48321 | val_1_rmse: 0.50558 |  0:00:42s
epoch 93 | loss: 0.25192 | val_0_rmse: 0.49394 | val_1_rmse: 0.51494 |  0:00:42s
epoch 94 | loss: 0.26983 | val_0_rmse: 0.51852 | val_1_rmse: 0.53348 |  0:00:42s
epoch 95 | loss: 0.25824 | val_0_rmse: 0.49051 | val_1_rmse: 0.50844 |  0:00:43s
epoch 96 | loss: 0.257   | val_0_rmse: 0.48134 | val_1_rmse: 0.50233 |  0:00:43s
epoch 97 | loss: 0.25016 | val_0_rmse: 0.4886  | val_1_rmse: 0.51117 |  0:00:44s
epoch 98 | loss: 0.24893 | val_0_rmse: 0.47453 | val_1_rmse: 0.49693 |  0:00:44s
epoch 99 | loss: 0.24864 | val_0_rmse: 0.49535 | val_1_rmse: 0.51192 |  0:00:45s
epoch 100| loss: 0.24674 | val_0_rmse: 0.47721 | val_1_rmse: 0.50008 |  0:00:45s
epoch 101| loss: 0.24812 | val_0_rmse: 0.47915 | val_1_rmse: 0.50029 |  0:00:46s
epoch 102| loss: 0.2486  | val_0_rmse: 0.47659 | val_1_rmse: 0.50262 |  0:00:46s
epoch 103| loss: 0.24038 | val_0_rmse: 0.48602 | val_1_rmse: 0.51139 |  0:00:46s
epoch 104| loss: 0.25034 | val_0_rmse: 0.47229 | val_1_rmse: 0.49518 |  0:00:47s
epoch 105| loss: 0.24754 | val_0_rmse: 0.48945 | val_1_rmse: 0.51433 |  0:00:47s
epoch 106| loss: 0.24648 | val_0_rmse: 0.47726 | val_1_rmse: 0.50272 |  0:00:48s
epoch 107| loss: 0.24849 | val_0_rmse: 0.47768 | val_1_rmse: 0.50104 |  0:00:48s
epoch 108| loss: 0.24652 | val_0_rmse: 0.47732 | val_1_rmse: 0.50324 |  0:00:49s
epoch 109| loss: 0.24596 | val_0_rmse: 0.46888 | val_1_rmse: 0.49739 |  0:00:49s
epoch 110| loss: 0.25219 | val_0_rmse: 0.47434 | val_1_rmse: 0.49824 |  0:00:50s
epoch 111| loss: 0.24342 | val_0_rmse: 0.46865 | val_1_rmse: 0.49688 |  0:00:50s
epoch 112| loss: 0.24273 | val_0_rmse: 0.48067 | val_1_rmse: 0.51092 |  0:00:51s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.49516
Best weights from best epoch are automatically used!
ended training at: 08:30:43
Feature importance:
[('Area', 0.2035088733505903), ('Baths', 0.11487405365329278), ('Beds', 0.05282615883283733), ('Latitude', 0.17434420376915186), ('Longitude', 0.38476064413999356), ('Month', 0.04247924361401431), ('Year', 0.02720682264011982)]
Mean squared error is of 20631736015.735607
Mean absolute error:104069.39726426132
MAPE:0.17347425580509665
R2 score:0.7564887006869647
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:30:43
epoch 0  | loss: 1.03481 | val_0_rmse: 0.86334 | val_1_rmse: 0.83952 |  0:00:00s
epoch 1  | loss: 0.57264 | val_0_rmse: 0.73071 | val_1_rmse: 0.74156 |  0:00:00s
epoch 2  | loss: 0.46106 | val_0_rmse: 0.73139 | val_1_rmse: 0.7419  |  0:00:01s
epoch 3  | loss: 0.41513 | val_0_rmse: 0.65073 | val_1_rmse: 0.66401 |  0:00:01s
epoch 4  | loss: 0.3786  | val_0_rmse: 0.62682 | val_1_rmse: 0.64506 |  0:00:02s
epoch 5  | loss: 0.37701 | val_0_rmse: 0.59576 | val_1_rmse: 0.60425 |  0:00:02s
epoch 6  | loss: 0.35622 | val_0_rmse: 0.58416 | val_1_rmse: 0.59505 |  0:00:03s
epoch 7  | loss: 0.34915 | val_0_rmse: 0.5785  | val_1_rmse: 0.58932 |  0:00:03s
epoch 8  | loss: 0.34507 | val_0_rmse: 0.56144 | val_1_rmse: 0.5797  |  0:00:04s
epoch 9  | loss: 0.32853 | val_0_rmse: 0.55473 | val_1_rmse: 0.57708 |  0:00:04s
epoch 10 | loss: 0.33303 | val_0_rmse: 0.55473 | val_1_rmse: 0.56251 |  0:00:04s
epoch 11 | loss: 0.32843 | val_0_rmse: 0.57448 | val_1_rmse: 0.60914 |  0:00:05s
epoch 12 | loss: 0.32234 | val_0_rmse: 0.55649 | val_1_rmse: 0.5896  |  0:00:05s
epoch 13 | loss: 0.3221  | val_0_rmse: 0.54469 | val_1_rmse: 0.56214 |  0:00:06s
epoch 14 | loss: 0.30391 | val_0_rmse: 0.53121 | val_1_rmse: 0.55952 |  0:00:06s
epoch 15 | loss: 0.30362 | val_0_rmse: 0.52861 | val_1_rmse: 0.55385 |  0:00:07s
epoch 16 | loss: 0.29619 | val_0_rmse: 0.52978 | val_1_rmse: 0.55434 |  0:00:07s
epoch 17 | loss: 0.29295 | val_0_rmse: 0.52838 | val_1_rmse: 0.55304 |  0:00:08s
epoch 18 | loss: 0.30278 | val_0_rmse: 0.51998 | val_1_rmse: 0.54732 |  0:00:08s
epoch 19 | loss: 0.29147 | val_0_rmse: 0.53511 | val_1_rmse: 0.55806 |  0:00:09s
epoch 20 | loss: 0.29942 | val_0_rmse: 0.51447 | val_1_rmse: 0.53524 |  0:00:09s
epoch 21 | loss: 0.29611 | val_0_rmse: 0.51768 | val_1_rmse: 0.53929 |  0:00:09s
epoch 22 | loss: 0.29464 | val_0_rmse: 0.53882 | val_1_rmse: 0.55478 |  0:00:10s
epoch 23 | loss: 0.29372 | val_0_rmse: 0.52141 | val_1_rmse: 0.54441 |  0:00:10s
epoch 24 | loss: 0.29097 | val_0_rmse: 0.52281 | val_1_rmse: 0.5383  |  0:00:11s
epoch 25 | loss: 0.2906  | val_0_rmse: 0.51717 | val_1_rmse: 0.53691 |  0:00:11s
epoch 26 | loss: 0.29349 | val_0_rmse: 0.50457 | val_1_rmse: 0.52196 |  0:00:12s
epoch 27 | loss: 0.28861 | val_0_rmse: 0.50519 | val_1_rmse: 0.52523 |  0:00:12s
epoch 28 | loss: 0.27599 | val_0_rmse: 0.50213 | val_1_rmse: 0.52274 |  0:00:13s
epoch 29 | loss: 0.27584 | val_0_rmse: 0.50104 | val_1_rmse: 0.52152 |  0:00:13s
epoch 30 | loss: 0.27643 | val_0_rmse: 0.50356 | val_1_rmse: 0.52853 |  0:00:14s
epoch 31 | loss: 0.27363 | val_0_rmse: 0.50746 | val_1_rmse: 0.52237 |  0:00:14s
epoch 32 | loss: 0.27811 | val_0_rmse: 0.49995 | val_1_rmse: 0.52138 |  0:00:14s
epoch 33 | loss: 0.27288 | val_0_rmse: 0.53105 | val_1_rmse: 0.55344 |  0:00:15s
epoch 34 | loss: 0.28621 | val_0_rmse: 0.51353 | val_1_rmse: 0.52947 |  0:00:15s
epoch 35 | loss: 0.28699 | val_0_rmse: 0.50197 | val_1_rmse: 0.52378 |  0:00:16s
epoch 36 | loss: 0.27514 | val_0_rmse: 0.50727 | val_1_rmse: 0.52263 |  0:00:16s
epoch 37 | loss: 0.26729 | val_0_rmse: 0.49683 | val_1_rmse: 0.5202  |  0:00:17s
epoch 38 | loss: 0.27653 | val_0_rmse: 0.51116 | val_1_rmse: 0.52743 |  0:00:17s
epoch 39 | loss: 0.27071 | val_0_rmse: 0.49611 | val_1_rmse: 0.51684 |  0:00:18s
epoch 40 | loss: 0.26714 | val_0_rmse: 0.5006  | val_1_rmse: 0.52186 |  0:00:18s
epoch 41 | loss: 0.27677 | val_0_rmse: 0.49897 | val_1_rmse: 0.51832 |  0:00:19s
epoch 42 | loss: 0.28968 | val_0_rmse: 0.52686 | val_1_rmse: 0.53474 |  0:00:19s
epoch 43 | loss: 0.29164 | val_0_rmse: 0.52738 | val_1_rmse: 0.54747 |  0:00:19s
epoch 44 | loss: 0.28442 | val_0_rmse: 0.49333 | val_1_rmse: 0.51531 |  0:00:20s
epoch 45 | loss: 0.27196 | val_0_rmse: 0.50945 | val_1_rmse: 0.52629 |  0:00:20s
epoch 46 | loss: 0.27661 | val_0_rmse: 0.50696 | val_1_rmse: 0.52719 |  0:00:21s
epoch 47 | loss: 0.2675  | val_0_rmse: 0.49628 | val_1_rmse: 0.511   |  0:00:21s
epoch 48 | loss: 0.27134 | val_0_rmse: 0.49528 | val_1_rmse: 0.51237 |  0:00:22s
epoch 49 | loss: 0.2682  | val_0_rmse: 0.48921 | val_1_rmse: 0.51348 |  0:00:22s
epoch 50 | loss: 0.26326 | val_0_rmse: 0.4995  | val_1_rmse: 0.52384 |  0:00:23s
epoch 51 | loss: 0.26476 | val_0_rmse: 0.49117 | val_1_rmse: 0.51644 |  0:00:23s
epoch 52 | loss: 0.26507 | val_0_rmse: 0.48436 | val_1_rmse: 0.51012 |  0:00:23s
epoch 53 | loss: 0.25904 | val_0_rmse: 0.48696 | val_1_rmse: 0.50951 |  0:00:24s
epoch 54 | loss: 0.25881 | val_0_rmse: 0.48243 | val_1_rmse: 0.50197 |  0:00:24s
epoch 55 | loss: 0.25424 | val_0_rmse: 0.48882 | val_1_rmse: 0.51325 |  0:00:25s
epoch 56 | loss: 0.26153 | val_0_rmse: 0.48819 | val_1_rmse: 0.51114 |  0:00:25s
epoch 57 | loss: 0.2592  | val_0_rmse: 0.50381 | val_1_rmse: 0.5226  |  0:00:26s
epoch 58 | loss: 0.26524 | val_0_rmse: 0.4893  | val_1_rmse: 0.51878 |  0:00:26s
epoch 59 | loss: 0.25646 | val_0_rmse: 0.49426 | val_1_rmse: 0.51928 |  0:00:27s
epoch 60 | loss: 0.26832 | val_0_rmse: 0.48518 | val_1_rmse: 0.51145 |  0:00:27s
epoch 61 | loss: 0.25784 | val_0_rmse: 0.48481 | val_1_rmse: 0.51306 |  0:00:27s
epoch 62 | loss: 0.25656 | val_0_rmse: 0.48307 | val_1_rmse: 0.51124 |  0:00:28s
epoch 63 | loss: 0.25922 | val_0_rmse: 0.48374 | val_1_rmse: 0.50706 |  0:00:28s
epoch 64 | loss: 0.2565  | val_0_rmse: 0.49939 | val_1_rmse: 0.52545 |  0:00:29s
epoch 65 | loss: 0.25113 | val_0_rmse: 0.48532 | val_1_rmse: 0.51266 |  0:00:29s
epoch 66 | loss: 0.25989 | val_0_rmse: 0.48759 | val_1_rmse: 0.50864 |  0:00:30s
epoch 67 | loss: 0.25927 | val_0_rmse: 0.49267 | val_1_rmse: 0.5228  |  0:00:30s
epoch 68 | loss: 0.25959 | val_0_rmse: 0.50083 | val_1_rmse: 0.51731 |  0:00:31s
epoch 69 | loss: 0.25557 | val_0_rmse: 0.48545 | val_1_rmse: 0.50251 |  0:00:31s
epoch 70 | loss: 0.25471 | val_0_rmse: 0.48636 | val_1_rmse: 0.50151 |  0:00:31s
epoch 71 | loss: 0.25937 | val_0_rmse: 0.49233 | val_1_rmse: 0.51513 |  0:00:32s
epoch 72 | loss: 0.2612  | val_0_rmse: 0.48761 | val_1_rmse: 0.51301 |  0:00:32s
epoch 73 | loss: 0.25185 | val_0_rmse: 0.48488 | val_1_rmse: 0.50624 |  0:00:33s
epoch 74 | loss: 0.25818 | val_0_rmse: 0.49075 | val_1_rmse: 0.51448 |  0:00:33s
epoch 75 | loss: 0.25999 | val_0_rmse: 0.48205 | val_1_rmse: 0.50665 |  0:00:34s
epoch 76 | loss: 0.25308 | val_0_rmse: 0.47993 | val_1_rmse: 0.50893 |  0:00:34s
epoch 77 | loss: 0.25069 | val_0_rmse: 0.47664 | val_1_rmse: 0.49991 |  0:00:35s
epoch 78 | loss: 0.25247 | val_0_rmse: 0.48038 | val_1_rmse: 0.51149 |  0:00:35s
epoch 79 | loss: 0.24947 | val_0_rmse: 0.49207 | val_1_rmse: 0.52081 |  0:00:36s
epoch 80 | loss: 0.24889 | val_0_rmse: 0.47782 | val_1_rmse: 0.50666 |  0:00:36s
epoch 81 | loss: 0.2443  | val_0_rmse: 0.4716  | val_1_rmse: 0.49698 |  0:00:36s
epoch 82 | loss: 0.25664 | val_0_rmse: 0.47856 | val_1_rmse: 0.49542 |  0:00:37s
epoch 83 | loss: 0.25432 | val_0_rmse: 0.50102 | val_1_rmse: 0.51849 |  0:00:37s
epoch 84 | loss: 0.25125 | val_0_rmse: 0.48317 | val_1_rmse: 0.50896 |  0:00:38s
epoch 85 | loss: 0.25692 | val_0_rmse: 0.47932 | val_1_rmse: 0.50667 |  0:00:38s
epoch 86 | loss: 0.25147 | val_0_rmse: 0.48459 | val_1_rmse: 0.51906 |  0:00:39s
epoch 87 | loss: 0.25474 | val_0_rmse: 0.49222 | val_1_rmse: 0.52048 |  0:00:39s
epoch 88 | loss: 0.25718 | val_0_rmse: 0.47671 | val_1_rmse: 0.50392 |  0:00:40s
epoch 89 | loss: 0.25042 | val_0_rmse: 0.48043 | val_1_rmse: 0.51105 |  0:00:40s
epoch 90 | loss: 0.24508 | val_0_rmse: 0.47505 | val_1_rmse: 0.5029  |  0:00:40s
epoch 91 | loss: 0.2488  | val_0_rmse: 0.4781  | val_1_rmse: 0.50985 |  0:00:41s
epoch 92 | loss: 0.25223 | val_0_rmse: 0.47043 | val_1_rmse: 0.50339 |  0:00:41s
epoch 93 | loss: 0.24586 | val_0_rmse: 0.48422 | val_1_rmse: 0.51477 |  0:00:42s
epoch 94 | loss: 0.25346 | val_0_rmse: 0.48007 | val_1_rmse: 0.51062 |  0:00:42s
epoch 95 | loss: 0.25657 | val_0_rmse: 0.49002 | val_1_rmse: 0.52717 |  0:00:43s
epoch 96 | loss: 0.263   | val_0_rmse: 0.49274 | val_1_rmse: 0.51595 |  0:00:43s
epoch 97 | loss: 0.25152 | val_0_rmse: 0.48675 | val_1_rmse: 0.5159  |  0:00:44s
epoch 98 | loss: 0.25054 | val_0_rmse: 0.48143 | val_1_rmse: 0.50229 |  0:00:44s
epoch 99 | loss: 0.25105 | val_0_rmse: 0.47052 | val_1_rmse: 0.49332 |  0:00:44s
epoch 100| loss: 0.25322 | val_0_rmse: 0.48728 | val_1_rmse: 0.52198 |  0:00:45s
epoch 101| loss: 0.26008 | val_0_rmse: 0.48027 | val_1_rmse: 0.51158 |  0:00:45s
epoch 102| loss: 0.26183 | val_0_rmse: 0.50075 | val_1_rmse: 0.52607 |  0:00:46s
epoch 103| loss: 0.25663 | val_0_rmse: 0.47666 | val_1_rmse: 0.50562 |  0:00:46s
epoch 104| loss: 0.242   | val_0_rmse: 0.4695  | val_1_rmse: 0.50385 |  0:00:47s
epoch 105| loss: 0.2375  | val_0_rmse: 0.47088 | val_1_rmse: 0.50244 |  0:00:47s
epoch 106| loss: 0.24354 | val_0_rmse: 0.47029 | val_1_rmse: 0.50192 |  0:00:48s
epoch 107| loss: 0.24281 | val_0_rmse: 0.46572 | val_1_rmse: 0.50237 |  0:00:48s
epoch 108| loss: 0.23763 | val_0_rmse: 0.46875 | val_1_rmse: 0.50949 |  0:00:49s
epoch 109| loss: 0.24104 | val_0_rmse: 0.46551 | val_1_rmse: 0.50559 |  0:00:49s
epoch 110| loss: 0.23988 | val_0_rmse: 0.464   | val_1_rmse: 0.4989  |  0:00:49s
epoch 111| loss: 0.23875 | val_0_rmse: 0.46313 | val_1_rmse: 0.50055 |  0:00:50s
epoch 112| loss: 0.23942 | val_0_rmse: 0.47001 | val_1_rmse: 0.50852 |  0:00:50s
epoch 113| loss: 0.23893 | val_0_rmse: 0.48261 | val_1_rmse: 0.51074 |  0:00:51s
epoch 114| loss: 0.2478  | val_0_rmse: 0.47899 | val_1_rmse: 0.51623 |  0:00:51s
epoch 115| loss: 0.24984 | val_0_rmse: 0.46882 | val_1_rmse: 0.50169 |  0:00:52s
epoch 116| loss: 0.23794 | val_0_rmse: 0.46568 | val_1_rmse: 0.5014  |  0:00:52s
epoch 117| loss: 0.24367 | val_0_rmse: 0.47353 | val_1_rmse: 0.51457 |  0:00:53s
epoch 118| loss: 0.2426  | val_0_rmse: 0.47423 | val_1_rmse: 0.50847 |  0:00:53s
epoch 119| loss: 0.25506 | val_0_rmse: 0.4801  | val_1_rmse: 0.52298 |  0:00:53s
epoch 120| loss: 0.24421 | val_0_rmse: 0.46676 | val_1_rmse: 0.51209 |  0:00:54s
epoch 121| loss: 0.24356 | val_0_rmse: 0.47011 | val_1_rmse: 0.51279 |  0:00:54s
epoch 122| loss: 0.23464 | val_0_rmse: 0.47149 | val_1_rmse: 0.50274 |  0:00:55s
epoch 123| loss: 0.24194 | val_0_rmse: 0.46873 | val_1_rmse: 0.50163 |  0:00:55s
epoch 124| loss: 0.23496 | val_0_rmse: 0.46792 | val_1_rmse: 0.50177 |  0:00:56s
epoch 125| loss: 0.24396 | val_0_rmse: 0.46672 | val_1_rmse: 0.50319 |  0:00:56s
epoch 126| loss: 0.24641 | val_0_rmse: 0.47591 | val_1_rmse: 0.51132 |  0:00:57s
epoch 127| loss: 0.25127 | val_0_rmse: 0.46705 | val_1_rmse: 0.50065 |  0:00:57s
epoch 128| loss: 0.23849 | val_0_rmse: 0.46609 | val_1_rmse: 0.506   |  0:00:57s
epoch 129| loss: 0.23623 | val_0_rmse: 0.48299 | val_1_rmse: 0.52408 |  0:00:58s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.49332
Best weights from best epoch are automatically used!
ended training at: 08:31:42
Feature importance:
[('Area', 0.3150149564113137), ('Baths', 0.06248349235121973), ('Beds', 0.09829753455494436), ('Latitude', 0.23319059152561827), ('Longitude', 0.23796772497039534), ('Month', 0.013966957797379546), ('Year', 0.03907874238912905)]
Mean squared error is of 21313675163.284218
Mean absolute error:103917.40542369874
MAPE:0.17987084334375808
R2 score:0.742653776099003
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:31:42
epoch 0  | loss: 0.97346 | val_0_rmse: 1.003   | val_1_rmse: 1.00671 |  0:00:00s
epoch 1  | loss: 0.56139 | val_0_rmse: 0.83174 | val_1_rmse: 0.84647 |  0:00:00s
epoch 2  | loss: 0.46957 | val_0_rmse: 0.70132 | val_1_rmse: 0.69593 |  0:00:01s
epoch 3  | loss: 0.41977 | val_0_rmse: 0.65369 | val_1_rmse: 0.65842 |  0:00:01s
epoch 4  | loss: 0.39598 | val_0_rmse: 0.61092 | val_1_rmse: 0.60378 |  0:00:02s
epoch 5  | loss: 0.37758 | val_0_rmse: 0.59153 | val_1_rmse: 0.59312 |  0:00:02s
epoch 6  | loss: 0.35903 | val_0_rmse: 0.57222 | val_1_rmse: 0.57975 |  0:00:03s
epoch 7  | loss: 0.34797 | val_0_rmse: 0.5693  | val_1_rmse: 0.57686 |  0:00:03s
epoch 8  | loss: 0.33862 | val_0_rmse: 0.56432 | val_1_rmse: 0.58422 |  0:00:04s
epoch 9  | loss: 0.32364 | val_0_rmse: 0.56619 | val_1_rmse: 0.57363 |  0:00:04s
epoch 10 | loss: 0.32126 | val_0_rmse: 0.54932 | val_1_rmse: 0.55424 |  0:00:05s
epoch 11 | loss: 0.32365 | val_0_rmse: 0.54782 | val_1_rmse: 0.53993 |  0:00:05s
epoch 12 | loss: 0.31303 | val_0_rmse: 0.55796 | val_1_rmse: 0.54907 |  0:00:05s
epoch 13 | loss: 0.31049 | val_0_rmse: 0.54116 | val_1_rmse: 0.53201 |  0:00:06s
epoch 14 | loss: 0.3053  | val_0_rmse: 0.53342 | val_1_rmse: 0.52922 |  0:00:06s
epoch 15 | loss: 0.31258 | val_0_rmse: 0.54565 | val_1_rmse: 0.53664 |  0:00:07s
epoch 16 | loss: 0.30586 | val_0_rmse: 0.53479 | val_1_rmse: 0.53018 |  0:00:07s
epoch 17 | loss: 0.32363 | val_0_rmse: 0.56116 | val_1_rmse: 0.55448 |  0:00:08s
epoch 18 | loss: 0.31402 | val_0_rmse: 0.54425 | val_1_rmse: 0.54126 |  0:00:08s
epoch 19 | loss: 0.30429 | val_0_rmse: 0.52237 | val_1_rmse: 0.51995 |  0:00:09s
epoch 20 | loss: 0.29486 | val_0_rmse: 0.52577 | val_1_rmse: 0.52317 |  0:00:09s
epoch 21 | loss: 0.29715 | val_0_rmse: 0.52319 | val_1_rmse: 0.52035 |  0:00:09s
epoch 22 | loss: 0.2954  | val_0_rmse: 0.52165 | val_1_rmse: 0.52085 |  0:00:10s
epoch 23 | loss: 0.2963  | val_0_rmse: 0.51596 | val_1_rmse: 0.51621 |  0:00:10s
epoch 24 | loss: 0.28876 | val_0_rmse: 0.52293 | val_1_rmse: 0.52333 |  0:00:11s
epoch 25 | loss: 0.28333 | val_0_rmse: 0.51809 | val_1_rmse: 0.51915 |  0:00:11s
epoch 26 | loss: 0.29435 | val_0_rmse: 0.51293 | val_1_rmse: 0.51724 |  0:00:12s
epoch 27 | loss: 0.28487 | val_0_rmse: 0.51641 | val_1_rmse: 0.52137 |  0:00:12s
epoch 28 | loss: 0.28285 | val_0_rmse: 0.51706 | val_1_rmse: 0.52081 |  0:00:13s
epoch 29 | loss: 0.29089 | val_0_rmse: 0.52854 | val_1_rmse: 0.53176 |  0:00:13s
epoch 30 | loss: 0.29325 | val_0_rmse: 0.52125 | val_1_rmse: 0.5311  |  0:00:14s
epoch 31 | loss: 0.28447 | val_0_rmse: 0.52342 | val_1_rmse: 0.52444 |  0:00:14s
epoch 32 | loss: 0.28203 | val_0_rmse: 0.52237 | val_1_rmse: 0.52654 |  0:00:14s
epoch 33 | loss: 0.2852  | val_0_rmse: 0.5128  | val_1_rmse: 0.5152  |  0:00:15s
epoch 34 | loss: 0.29066 | val_0_rmse: 0.51324 | val_1_rmse: 0.5186  |  0:00:15s
epoch 35 | loss: 0.28926 | val_0_rmse: 0.51721 | val_1_rmse: 0.51733 |  0:00:16s
epoch 36 | loss: 0.28305 | val_0_rmse: 0.5337  | val_1_rmse: 0.53306 |  0:00:16s
epoch 37 | loss: 0.29037 | val_0_rmse: 0.51537 | val_1_rmse: 0.51612 |  0:00:17s
epoch 38 | loss: 0.28168 | val_0_rmse: 0.5124  | val_1_rmse: 0.51922 |  0:00:17s
epoch 39 | loss: 0.27416 | val_0_rmse: 0.51199 | val_1_rmse: 0.51696 |  0:00:18s
epoch 40 | loss: 0.27937 | val_0_rmse: 0.52261 | val_1_rmse: 0.52732 |  0:00:18s
epoch 41 | loss: 0.27726 | val_0_rmse: 0.50933 | val_1_rmse: 0.51374 |  0:00:19s
epoch 42 | loss: 0.27553 | val_0_rmse: 0.53226 | val_1_rmse: 0.53638 |  0:00:19s
epoch 43 | loss: 0.2788  | val_0_rmse: 0.51233 | val_1_rmse: 0.51741 |  0:00:19s
epoch 44 | loss: 0.27166 | val_0_rmse: 0.50501 | val_1_rmse: 0.50973 |  0:00:20s
epoch 45 | loss: 0.27792 | val_0_rmse: 0.51263 | val_1_rmse: 0.51628 |  0:00:20s
epoch 46 | loss: 0.27303 | val_0_rmse: 0.51208 | val_1_rmse: 0.51693 |  0:00:21s
epoch 47 | loss: 0.26967 | val_0_rmse: 0.50454 | val_1_rmse: 0.51072 |  0:00:21s
epoch 48 | loss: 0.27708 | val_0_rmse: 0.50962 | val_1_rmse: 0.51456 |  0:00:22s
epoch 49 | loss: 0.27349 | val_0_rmse: 0.51069 | val_1_rmse: 0.51737 |  0:00:22s
epoch 50 | loss: 0.27334 | val_0_rmse: 0.51366 | val_1_rmse: 0.52037 |  0:00:23s
epoch 51 | loss: 0.27486 | val_0_rmse: 0.51404 | val_1_rmse: 0.52396 |  0:00:23s
epoch 52 | loss: 0.26918 | val_0_rmse: 0.50014 | val_1_rmse: 0.50378 |  0:00:23s
epoch 53 | loss: 0.26756 | val_0_rmse: 0.52007 | val_1_rmse: 0.52562 |  0:00:24s
epoch 54 | loss: 0.27316 | val_0_rmse: 0.49791 | val_1_rmse: 0.50528 |  0:00:24s
epoch 55 | loss: 0.26468 | val_0_rmse: 0.49493 | val_1_rmse: 0.50413 |  0:00:25s
epoch 56 | loss: 0.26624 | val_0_rmse: 0.49796 | val_1_rmse: 0.50625 |  0:00:25s
epoch 57 | loss: 0.26419 | val_0_rmse: 0.51626 | val_1_rmse: 0.52382 |  0:00:26s
epoch 58 | loss: 0.26963 | val_0_rmse: 0.50058 | val_1_rmse: 0.51503 |  0:00:26s
epoch 59 | loss: 0.27634 | val_0_rmse: 0.50484 | val_1_rmse: 0.51339 |  0:00:27s
epoch 60 | loss: 0.27517 | val_0_rmse: 0.50305 | val_1_rmse: 0.50884 |  0:00:27s
epoch 61 | loss: 0.27303 | val_0_rmse: 0.51061 | val_1_rmse: 0.51876 |  0:00:27s
epoch 62 | loss: 0.27354 | val_0_rmse: 0.50873 | val_1_rmse: 0.51664 |  0:00:28s
epoch 63 | loss: 0.27456 | val_0_rmse: 0.50016 | val_1_rmse: 0.50821 |  0:00:28s
epoch 64 | loss: 0.27108 | val_0_rmse: 0.50266 | val_1_rmse: 0.51107 |  0:00:29s
epoch 65 | loss: 0.26712 | val_0_rmse: 0.50914 | val_1_rmse: 0.51701 |  0:00:29s
epoch 66 | loss: 0.27182 | val_0_rmse: 0.49477 | val_1_rmse: 0.50738 |  0:00:30s
epoch 67 | loss: 0.26524 | val_0_rmse: 0.50096 | val_1_rmse: 0.51147 |  0:00:30s
epoch 68 | loss: 0.27123 | val_0_rmse: 0.50225 | val_1_rmse: 0.50926 |  0:00:31s
epoch 69 | loss: 0.26125 | val_0_rmse: 0.50418 | val_1_rmse: 0.52279 |  0:00:31s
epoch 70 | loss: 0.26295 | val_0_rmse: 0.4901  | val_1_rmse: 0.50586 |  0:00:32s
epoch 71 | loss: 0.25438 | val_0_rmse: 0.49364 | val_1_rmse: 0.50716 |  0:00:32s
epoch 72 | loss: 0.26475 | val_0_rmse: 0.49431 | val_1_rmse: 0.51038 |  0:00:32s
epoch 73 | loss: 0.26538 | val_0_rmse: 0.50835 | val_1_rmse: 0.52043 |  0:00:33s
epoch 74 | loss: 0.25995 | val_0_rmse: 0.50608 | val_1_rmse: 0.52385 |  0:00:33s
epoch 75 | loss: 0.26795 | val_0_rmse: 0.50514 | val_1_rmse: 0.52449 |  0:00:34s
epoch 76 | loss: 0.2618  | val_0_rmse: 0.50209 | val_1_rmse: 0.51556 |  0:00:34s
epoch 77 | loss: 0.26377 | val_0_rmse: 0.49939 | val_1_rmse: 0.51749 |  0:00:35s
epoch 78 | loss: 0.2657  | val_0_rmse: 0.50265 | val_1_rmse: 0.5138  |  0:00:35s
epoch 79 | loss: 0.26693 | val_0_rmse: 0.49995 | val_1_rmse: 0.51593 |  0:00:36s
epoch 80 | loss: 0.26285 | val_0_rmse: 0.49833 | val_1_rmse: 0.512   |  0:00:36s
epoch 81 | loss: 0.26413 | val_0_rmse: 0.49974 | val_1_rmse: 0.51535 |  0:00:36s
epoch 82 | loss: 0.26381 | val_0_rmse: 0.49887 | val_1_rmse: 0.50481 |  0:00:37s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.50378
Best weights from best epoch are automatically used!
ended training at: 08:32:20
Feature importance:
[('Area', 0.27745388124814874), ('Baths', 0.06698062534615229), ('Beds', 0.10493038515028448), ('Latitude', 0.2515165205500991), ('Longitude', 0.24846182140728998), ('Month', 0.007549912660731307), ('Year', 0.043106853637294114)]
Mean squared error is of 21380308659.31103
Mean absolute error:106782.14457945584
MAPE:0.18408437709240538
R2 score:0.737785770537245
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:32:20
epoch 0  | loss: 1.06255 | val_0_rmse: 0.88439 | val_1_rmse: 0.88475 |  0:00:00s
epoch 1  | loss: 0.59815 | val_0_rmse: 0.8436  | val_1_rmse: 0.90784 |  0:00:00s
epoch 2  | loss: 0.51465 | val_0_rmse: 0.73608 | val_1_rmse: 0.76912 |  0:00:01s
epoch 3  | loss: 0.45433 | val_0_rmse: 0.68906 | val_1_rmse: 0.72435 |  0:00:01s
epoch 4  | loss: 0.42649 | val_0_rmse: 0.64746 | val_1_rmse: 0.67119 |  0:00:02s
epoch 5  | loss: 0.37898 | val_0_rmse: 0.61955 | val_1_rmse: 0.63284 |  0:00:02s
epoch 6  | loss: 0.36737 | val_0_rmse: 0.61229 | val_1_rmse: 0.63131 |  0:00:03s
epoch 7  | loss: 0.34845 | val_0_rmse: 0.61659 | val_1_rmse: 0.64779 |  0:00:03s
epoch 8  | loss: 0.33257 | val_0_rmse: 0.56208 | val_1_rmse: 0.58198 |  0:00:04s
epoch 9  | loss: 0.33221 | val_0_rmse: 0.54697 | val_1_rmse: 0.56391 |  0:00:04s
epoch 10 | loss: 0.32264 | val_0_rmse: 0.5402  | val_1_rmse: 0.55962 |  0:00:05s
epoch 11 | loss: 0.31015 | val_0_rmse: 0.53327 | val_1_rmse: 0.54797 |  0:00:05s
epoch 12 | loss: 0.30553 | val_0_rmse: 0.5412  | val_1_rmse: 0.56213 |  0:00:05s
epoch 13 | loss: 0.30761 | val_0_rmse: 0.53472 | val_1_rmse: 0.54791 |  0:00:06s
epoch 14 | loss: 0.30913 | val_0_rmse: 0.52773 | val_1_rmse: 0.54457 |  0:00:06s
epoch 15 | loss: 0.30417 | val_0_rmse: 0.52654 | val_1_rmse: 0.53652 |  0:00:07s
epoch 16 | loss: 0.3023  | val_0_rmse: 0.52483 | val_1_rmse: 0.53739 |  0:00:07s
epoch 17 | loss: 0.29646 | val_0_rmse: 0.52894 | val_1_rmse: 0.53842 |  0:00:08s
epoch 18 | loss: 0.3048  | val_0_rmse: 0.52895 | val_1_rmse: 0.54407 |  0:00:08s
epoch 19 | loss: 0.3015  | val_0_rmse: 0.52953 | val_1_rmse: 0.54051 |  0:00:09s
epoch 20 | loss: 0.30213 | val_0_rmse: 0.52302 | val_1_rmse: 0.53638 |  0:00:09s
epoch 21 | loss: 0.29502 | val_0_rmse: 0.51886 | val_1_rmse: 0.52873 |  0:00:09s
epoch 22 | loss: 0.29273 | val_0_rmse: 0.51695 | val_1_rmse: 0.52687 |  0:00:10s
epoch 23 | loss: 0.29067 | val_0_rmse: 0.53127 | val_1_rmse: 0.54888 |  0:00:10s
epoch 24 | loss: 0.29382 | val_0_rmse: 0.52333 | val_1_rmse: 0.53767 |  0:00:11s
epoch 25 | loss: 0.2884  | val_0_rmse: 0.51316 | val_1_rmse: 0.53029 |  0:00:11s
epoch 26 | loss: 0.2757  | val_0_rmse: 0.50817 | val_1_rmse: 0.52003 |  0:00:12s
epoch 27 | loss: 0.27855 | val_0_rmse: 0.50356 | val_1_rmse: 0.51963 |  0:00:12s
epoch 28 | loss: 0.27962 | val_0_rmse: 0.5024  | val_1_rmse: 0.51627 |  0:00:13s
epoch 29 | loss: 0.27309 | val_0_rmse: 0.50497 | val_1_rmse: 0.52016 |  0:00:13s
epoch 30 | loss: 0.27632 | val_0_rmse: 0.49972 | val_1_rmse: 0.51312 |  0:00:14s
epoch 31 | loss: 0.27189 | val_0_rmse: 0.52852 | val_1_rmse: 0.54509 |  0:00:14s
epoch 32 | loss: 0.28419 | val_0_rmse: 0.5076  | val_1_rmse: 0.52657 |  0:00:14s
epoch 33 | loss: 0.27907 | val_0_rmse: 0.5006  | val_1_rmse: 0.5249  |  0:00:15s
epoch 34 | loss: 0.28628 | val_0_rmse: 0.50373 | val_1_rmse: 0.52285 |  0:00:15s
epoch 35 | loss: 0.28299 | val_0_rmse: 0.51923 | val_1_rmse: 0.53939 |  0:00:16s
epoch 36 | loss: 0.28541 | val_0_rmse: 0.49772 | val_1_rmse: 0.5151  |  0:00:16s
epoch 37 | loss: 0.27292 | val_0_rmse: 0.50629 | val_1_rmse: 0.52624 |  0:00:17s
epoch 38 | loss: 0.26281 | val_0_rmse: 0.52402 | val_1_rmse: 0.54184 |  0:00:17s
epoch 39 | loss: 0.2692  | val_0_rmse: 0.49019 | val_1_rmse: 0.5137  |  0:00:18s
epoch 40 | loss: 0.26827 | val_0_rmse: 0.50045 | val_1_rmse: 0.52565 |  0:00:18s
epoch 41 | loss: 0.26831 | val_0_rmse: 0.49343 | val_1_rmse: 0.51919 |  0:00:19s
epoch 42 | loss: 0.27474 | val_0_rmse: 0.49116 | val_1_rmse: 0.51511 |  0:00:19s
epoch 43 | loss: 0.26411 | val_0_rmse: 0.49434 | val_1_rmse: 0.51542 |  0:00:19s
epoch 44 | loss: 0.26189 | val_0_rmse: 0.49665 | val_1_rmse: 0.51824 |  0:00:20s
epoch 45 | loss: 0.26478 | val_0_rmse: 0.49483 | val_1_rmse: 0.516   |  0:00:20s
epoch 46 | loss: 0.27047 | val_0_rmse: 0.49464 | val_1_rmse: 0.51808 |  0:00:21s
epoch 47 | loss: 0.2698  | val_0_rmse: 0.49387 | val_1_rmse: 0.51815 |  0:00:21s
epoch 48 | loss: 0.26454 | val_0_rmse: 0.48821 | val_1_rmse: 0.51246 |  0:00:22s
epoch 49 | loss: 0.26304 | val_0_rmse: 0.48655 | val_1_rmse: 0.514   |  0:00:22s
epoch 50 | loss: 0.26493 | val_0_rmse: 0.49356 | val_1_rmse: 0.51673 |  0:00:23s
epoch 51 | loss: 0.25839 | val_0_rmse: 0.49076 | val_1_rmse: 0.51497 |  0:00:23s
epoch 52 | loss: 0.26646 | val_0_rmse: 0.48509 | val_1_rmse: 0.51468 |  0:00:24s
epoch 53 | loss: 0.26524 | val_0_rmse: 0.51191 | val_1_rmse: 0.53452 |  0:00:24s
epoch 54 | loss: 0.26236 | val_0_rmse: 0.48884 | val_1_rmse: 0.51834 |  0:00:24s
epoch 55 | loss: 0.26056 | val_0_rmse: 0.48467 | val_1_rmse: 0.51505 |  0:00:25s
epoch 56 | loss: 0.26396 | val_0_rmse: 0.48489 | val_1_rmse: 0.51873 |  0:00:25s
epoch 57 | loss: 0.25651 | val_0_rmse: 0.49848 | val_1_rmse: 0.53338 |  0:00:26s
epoch 58 | loss: 0.25916 | val_0_rmse: 0.47937 | val_1_rmse: 0.51007 |  0:00:26s
epoch 59 | loss: 0.25253 | val_0_rmse: 0.48274 | val_1_rmse: 0.51352 |  0:00:27s
epoch 60 | loss: 0.25522 | val_0_rmse: 0.48309 | val_1_rmse: 0.51539 |  0:00:27s
epoch 61 | loss: 0.26473 | val_0_rmse: 0.48407 | val_1_rmse: 0.51685 |  0:00:28s
epoch 62 | loss: 0.24897 | val_0_rmse: 0.47897 | val_1_rmse: 0.51458 |  0:00:28s
epoch 63 | loss: 0.25212 | val_0_rmse: 0.4807  | val_1_rmse: 0.51611 |  0:00:28s
epoch 64 | loss: 0.24891 | val_0_rmse: 0.48277 | val_1_rmse: 0.52258 |  0:00:29s
epoch 65 | loss: 0.26064 | val_0_rmse: 0.50101 | val_1_rmse: 0.53616 |  0:00:29s
epoch 66 | loss: 0.25553 | val_0_rmse: 0.48258 | val_1_rmse: 0.5219  |  0:00:30s
epoch 67 | loss: 0.25367 | val_0_rmse: 0.49596 | val_1_rmse: 0.52648 |  0:00:30s
epoch 68 | loss: 0.26151 | val_0_rmse: 0.48148 | val_1_rmse: 0.51953 |  0:00:31s
epoch 69 | loss: 0.2617  | val_0_rmse: 0.48386 | val_1_rmse: 0.5233  |  0:00:31s
epoch 70 | loss: 0.25899 | val_0_rmse: 0.48592 | val_1_rmse: 0.52698 |  0:00:32s
epoch 71 | loss: 0.25089 | val_0_rmse: 0.47977 | val_1_rmse: 0.52197 |  0:00:32s
epoch 72 | loss: 0.25505 | val_0_rmse: 0.47854 | val_1_rmse: 0.51907 |  0:00:32s
epoch 73 | loss: 0.25585 | val_0_rmse: 0.49345 | val_1_rmse: 0.52923 |  0:00:33s
epoch 74 | loss: 0.2567  | val_0_rmse: 0.47383 | val_1_rmse: 0.51179 |  0:00:33s
epoch 75 | loss: 0.25203 | val_0_rmse: 0.49371 | val_1_rmse: 0.53218 |  0:00:34s
epoch 76 | loss: 0.25386 | val_0_rmse: 0.47772 | val_1_rmse: 0.52073 |  0:00:34s
epoch 77 | loss: 0.25515 | val_0_rmse: 0.48112 | val_1_rmse: 0.51957 |  0:00:35s
epoch 78 | loss: 0.25378 | val_0_rmse: 0.48477 | val_1_rmse: 0.52076 |  0:00:35s
epoch 79 | loss: 0.26682 | val_0_rmse: 0.51142 | val_1_rmse: 0.53306 |  0:00:36s
epoch 80 | loss: 0.25834 | val_0_rmse: 0.47838 | val_1_rmse: 0.51951 |  0:00:36s
epoch 81 | loss: 0.24399 | val_0_rmse: 0.47927 | val_1_rmse: 0.5189  |  0:00:36s
epoch 82 | loss: 0.24792 | val_0_rmse: 0.47408 | val_1_rmse: 0.51316 |  0:00:37s
epoch 83 | loss: 0.24582 | val_0_rmse: 0.47093 | val_1_rmse: 0.51099 |  0:00:37s
epoch 84 | loss: 0.24987 | val_0_rmse: 0.46914 | val_1_rmse: 0.508   |  0:00:38s
epoch 85 | loss: 0.24787 | val_0_rmse: 0.4733  | val_1_rmse: 0.52089 |  0:00:38s
epoch 86 | loss: 0.2422  | val_0_rmse: 0.47074 | val_1_rmse: 0.51573 |  0:00:39s
epoch 87 | loss: 0.25125 | val_0_rmse: 0.47134 | val_1_rmse: 0.51841 |  0:00:39s
epoch 88 | loss: 0.25121 | val_0_rmse: 0.47536 | val_1_rmse: 0.51727 |  0:00:40s
epoch 89 | loss: 0.25675 | val_0_rmse: 0.49138 | val_1_rmse: 0.53012 |  0:00:40s
epoch 90 | loss: 0.24669 | val_0_rmse: 0.47707 | val_1_rmse: 0.52328 |  0:00:41s
epoch 91 | loss: 0.24978 | val_0_rmse: 0.48074 | val_1_rmse: 0.52497 |  0:00:41s
epoch 92 | loss: 0.2463  | val_0_rmse: 0.48867 | val_1_rmse: 0.5303  |  0:00:41s
epoch 93 | loss: 0.24506 | val_0_rmse: 0.47269 | val_1_rmse: 0.52131 |  0:00:42s
epoch 94 | loss: 0.24469 | val_0_rmse: 0.48083 | val_1_rmse: 0.52354 |  0:00:42s
epoch 95 | loss: 0.23985 | val_0_rmse: 0.46999 | val_1_rmse: 0.52305 |  0:00:43s
epoch 96 | loss: 0.24555 | val_0_rmse: 0.4732  | val_1_rmse: 0.51723 |  0:00:43s
epoch 97 | loss: 0.25198 | val_0_rmse: 0.47411 | val_1_rmse: 0.52347 |  0:00:44s
epoch 98 | loss: 0.25231 | val_0_rmse: 0.47424 | val_1_rmse: 0.52037 |  0:00:44s
epoch 99 | loss: 0.24622 | val_0_rmse: 0.47448 | val_1_rmse: 0.51954 |  0:00:45s
epoch 100| loss: 0.24481 | val_0_rmse: 0.47094 | val_1_rmse: 0.51901 |  0:00:45s
epoch 101| loss: 0.24781 | val_0_rmse: 0.47543 | val_1_rmse: 0.51834 |  0:00:45s
epoch 102| loss: 0.25487 | val_0_rmse: 0.49727 | val_1_rmse: 0.53975 |  0:00:46s
epoch 103| loss: 0.24539 | val_0_rmse: 0.47889 | val_1_rmse: 0.52111 |  0:00:46s
epoch 104| loss: 0.24186 | val_0_rmse: 0.46594 | val_1_rmse: 0.51418 |  0:00:47s
epoch 105| loss: 0.24558 | val_0_rmse: 0.47551 | val_1_rmse: 0.51928 |  0:00:47s
epoch 106| loss: 0.24241 | val_0_rmse: 0.46888 | val_1_rmse: 0.51322 |  0:00:48s
epoch 107| loss: 0.24635 | val_0_rmse: 0.46876 | val_1_rmse: 0.51651 |  0:00:48s
epoch 108| loss: 0.24485 | val_0_rmse: 0.46644 | val_1_rmse: 0.52048 |  0:00:49s
epoch 109| loss: 0.24102 | val_0_rmse: 0.47425 | val_1_rmse: 0.52587 |  0:00:49s
epoch 110| loss: 0.24987 | val_0_rmse: 0.47173 | val_1_rmse: 0.52315 |  0:00:49s
epoch 111| loss: 0.24584 | val_0_rmse: 0.47067 | val_1_rmse: 0.5177  |  0:00:50s
epoch 112| loss: 0.24066 | val_0_rmse: 0.46658 | val_1_rmse: 0.52518 |  0:00:50s
epoch 113| loss: 0.24682 | val_0_rmse: 0.48174 | val_1_rmse: 0.53667 |  0:00:51s
epoch 114| loss: 0.23463 | val_0_rmse: 0.46512 | val_1_rmse: 0.52288 |  0:00:51s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.508
Best weights from best epoch are automatically used!
ended training at: 08:33:11
Feature importance:
[('Area', 0.22910290136508382), ('Baths', 0.12238830170956504), ('Beds', 0.11394846010353517), ('Latitude', 0.0867732558973672), ('Longitude', 0.2769909477188976), ('Month', 0.06202198017256402), ('Year', 0.10877415303298718)]
Mean squared error is of 20060855984.185623
Mean absolute error:102804.75430881965
MAPE:0.1829708958794401
R2 score:0.7597189069823028
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:33:12
epoch 0  | loss: 1.08187 | val_0_rmse: 0.85917 | val_1_rmse: 0.83645 |  0:00:00s
epoch 1  | loss: 0.58154 | val_0_rmse: 0.90746 | val_1_rmse: 0.87695 |  0:00:00s
epoch 2  | loss: 0.45939 | val_0_rmse: 0.79102 | val_1_rmse: 0.79621 |  0:00:01s
epoch 3  | loss: 0.39092 | val_0_rmse: 0.69998 | val_1_rmse: 0.68256 |  0:00:01s
epoch 4  | loss: 0.37622 | val_0_rmse: 0.64126 | val_1_rmse: 0.62379 |  0:00:02s
epoch 5  | loss: 0.35654 | val_0_rmse: 0.6285  | val_1_rmse: 0.61645 |  0:00:02s
epoch 6  | loss: 0.33381 | val_0_rmse: 0.59108 | val_1_rmse: 0.57077 |  0:00:03s
epoch 7  | loss: 0.32387 | val_0_rmse: 0.55765 | val_1_rmse: 0.55345 |  0:00:03s
epoch 8  | loss: 0.32053 | val_0_rmse: 0.56337 | val_1_rmse: 0.55909 |  0:00:04s
epoch 9  | loss: 0.31869 | val_0_rmse: 0.53872 | val_1_rmse: 0.53137 |  0:00:04s
epoch 10 | loss: 0.30811 | val_0_rmse: 0.53308 | val_1_rmse: 0.52498 |  0:00:05s
epoch 11 | loss: 0.31464 | val_0_rmse: 0.53187 | val_1_rmse: 0.52661 |  0:00:05s
epoch 12 | loss: 0.29995 | val_0_rmse: 0.5275  | val_1_rmse: 0.51954 |  0:00:05s
epoch 13 | loss: 0.29586 | val_0_rmse: 0.52542 | val_1_rmse: 0.51657 |  0:00:06s
epoch 14 | loss: 0.29656 | val_0_rmse: 0.52117 | val_1_rmse: 0.51635 |  0:00:06s
epoch 15 | loss: 0.29068 | val_0_rmse: 0.51936 | val_1_rmse: 0.51351 |  0:00:07s
epoch 16 | loss: 0.2937  | val_0_rmse: 0.52363 | val_1_rmse: 0.51447 |  0:00:07s
epoch 17 | loss: 0.28789 | val_0_rmse: 0.52708 | val_1_rmse: 0.51714 |  0:00:08s
epoch 18 | loss: 0.29754 | val_0_rmse: 0.5352  | val_1_rmse: 0.52477 |  0:00:08s
epoch 19 | loss: 0.29116 | val_0_rmse: 0.52978 | val_1_rmse: 0.52287 |  0:00:09s
epoch 20 | loss: 0.29436 | val_0_rmse: 0.51999 | val_1_rmse: 0.51115 |  0:00:09s
epoch 21 | loss: 0.28794 | val_0_rmse: 0.51817 | val_1_rmse: 0.51717 |  0:00:10s
epoch 22 | loss: 0.29584 | val_0_rmse: 0.51246 | val_1_rmse: 0.51197 |  0:00:10s
epoch 23 | loss: 0.2933  | val_0_rmse: 0.52998 | val_1_rmse: 0.53112 |  0:00:10s
epoch 24 | loss: 0.3055  | val_0_rmse: 0.50987 | val_1_rmse: 0.50692 |  0:00:11s
epoch 25 | loss: 0.28652 | val_0_rmse: 0.51006 | val_1_rmse: 0.50398 |  0:00:11s
epoch 26 | loss: 0.27915 | val_0_rmse: 0.50682 | val_1_rmse: 0.50692 |  0:00:12s
epoch 27 | loss: 0.27884 | val_0_rmse: 0.51166 | val_1_rmse: 0.51195 |  0:00:12s
epoch 28 | loss: 0.2795  | val_0_rmse: 0.50795 | val_1_rmse: 0.50773 |  0:00:13s
epoch 29 | loss: 0.27967 | val_0_rmse: 0.50787 | val_1_rmse: 0.50669 |  0:00:13s
epoch 30 | loss: 0.27316 | val_0_rmse: 0.52803 | val_1_rmse: 0.53093 |  0:00:14s
epoch 31 | loss: 0.28366 | val_0_rmse: 0.51221 | val_1_rmse: 0.51102 |  0:00:14s
epoch 32 | loss: 0.27447 | val_0_rmse: 0.51143 | val_1_rmse: 0.50606 |  0:00:14s
epoch 33 | loss: 0.28456 | val_0_rmse: 0.50528 | val_1_rmse: 0.50421 |  0:00:15s
epoch 34 | loss: 0.2721  | val_0_rmse: 0.50975 | val_1_rmse: 0.50468 |  0:00:15s
epoch 35 | loss: 0.27487 | val_0_rmse: 0.51242 | val_1_rmse: 0.51469 |  0:00:16s
epoch 36 | loss: 0.27661 | val_0_rmse: 0.5132  | val_1_rmse: 0.51365 |  0:00:16s
epoch 37 | loss: 0.27411 | val_0_rmse: 0.50229 | val_1_rmse: 0.49835 |  0:00:17s
epoch 38 | loss: 0.27207 | val_0_rmse: 0.49858 | val_1_rmse: 0.49449 |  0:00:17s
epoch 39 | loss: 0.28608 | val_0_rmse: 0.5072  | val_1_rmse: 0.50676 |  0:00:18s
epoch 40 | loss: 0.27915 | val_0_rmse: 0.50297 | val_1_rmse: 0.49808 |  0:00:18s
epoch 41 | loss: 0.2893  | val_0_rmse: 0.50569 | val_1_rmse: 0.50922 |  0:00:19s
epoch 42 | loss: 0.28031 | val_0_rmse: 0.5106  | val_1_rmse: 0.51159 |  0:00:19s
epoch 43 | loss: 0.27251 | val_0_rmse: 0.50127 | val_1_rmse: 0.50069 |  0:00:19s
epoch 44 | loss: 0.27243 | val_0_rmse: 0.49945 | val_1_rmse: 0.50206 |  0:00:20s
epoch 45 | loss: 0.26215 | val_0_rmse: 0.49231 | val_1_rmse: 0.49474 |  0:00:20s
epoch 46 | loss: 0.26397 | val_0_rmse: 0.49635 | val_1_rmse: 0.50037 |  0:00:21s
epoch 47 | loss: 0.26238 | val_0_rmse: 0.49247 | val_1_rmse: 0.49417 |  0:00:21s
epoch 48 | loss: 0.26569 | val_0_rmse: 0.50326 | val_1_rmse: 0.50641 |  0:00:22s
epoch 49 | loss: 0.26572 | val_0_rmse: 0.4994  | val_1_rmse: 0.49902 |  0:00:22s
epoch 50 | loss: 0.27217 | val_0_rmse: 0.50311 | val_1_rmse: 0.5096  |  0:00:23s
epoch 51 | loss: 0.27654 | val_0_rmse: 0.50513 | val_1_rmse: 0.50474 |  0:00:23s
epoch 52 | loss: 0.27409 | val_0_rmse: 0.5037  | val_1_rmse: 0.50943 |  0:00:24s
epoch 53 | loss: 0.27167 | val_0_rmse: 0.49424 | val_1_rmse: 0.49524 |  0:00:24s
epoch 54 | loss: 0.26462 | val_0_rmse: 0.49286 | val_1_rmse: 0.48767 |  0:00:24s
epoch 55 | loss: 0.26658 | val_0_rmse: 0.4952  | val_1_rmse: 0.50361 |  0:00:25s
epoch 56 | loss: 0.26527 | val_0_rmse: 0.51346 | val_1_rmse: 0.51859 |  0:00:25s
epoch 57 | loss: 0.27198 | val_0_rmse: 0.50748 | val_1_rmse: 0.51143 |  0:00:26s
epoch 58 | loss: 0.26711 | val_0_rmse: 0.49291 | val_1_rmse: 0.49364 |  0:00:26s
epoch 59 | loss: 0.25689 | val_0_rmse: 0.49196 | val_1_rmse: 0.49662 |  0:00:27s
epoch 60 | loss: 0.25681 | val_0_rmse: 0.48892 | val_1_rmse: 0.49186 |  0:00:27s
epoch 61 | loss: 0.25829 | val_0_rmse: 0.48871 | val_1_rmse: 0.49185 |  0:00:28s
epoch 62 | loss: 0.26267 | val_0_rmse: 0.48522 | val_1_rmse: 0.49201 |  0:00:28s
epoch 63 | loss: 0.26035 | val_0_rmse: 0.49088 | val_1_rmse: 0.49528 |  0:00:28s
epoch 64 | loss: 0.26435 | val_0_rmse: 0.51555 | val_1_rmse: 0.51801 |  0:00:29s
epoch 65 | loss: 0.26351 | val_0_rmse: 0.50114 | val_1_rmse: 0.50894 |  0:00:29s
epoch 66 | loss: 0.27471 | val_0_rmse: 0.49447 | val_1_rmse: 0.50196 |  0:00:30s
epoch 67 | loss: 0.2721  | val_0_rmse: 0.51546 | val_1_rmse: 0.52408 |  0:00:30s
epoch 68 | loss: 0.26322 | val_0_rmse: 0.48189 | val_1_rmse: 0.49288 |  0:00:31s
epoch 69 | loss: 0.25296 | val_0_rmse: 0.4939  | val_1_rmse: 0.50367 |  0:00:31s
epoch 70 | loss: 0.25439 | val_0_rmse: 0.48589 | val_1_rmse: 0.49222 |  0:00:32s
epoch 71 | loss: 0.25426 | val_0_rmse: 0.48743 | val_1_rmse: 0.50046 |  0:00:32s
epoch 72 | loss: 0.25976 | val_0_rmse: 0.48995 | val_1_rmse: 0.50167 |  0:00:32s
epoch 73 | loss: 0.25472 | val_0_rmse: 0.49146 | val_1_rmse: 0.49766 |  0:00:33s
epoch 74 | loss: 0.26179 | val_0_rmse: 0.50766 | val_1_rmse: 0.51483 |  0:00:33s
epoch 75 | loss: 0.26805 | val_0_rmse: 0.50343 | val_1_rmse: 0.51734 |  0:00:34s
epoch 76 | loss: 0.26191 | val_0_rmse: 0.53083 | val_1_rmse: 0.5284  |  0:00:34s
epoch 77 | loss: 0.2644  | val_0_rmse: 0.48531 | val_1_rmse: 0.48912 |  0:00:35s
epoch 78 | loss: 0.25245 | val_0_rmse: 0.48826 | val_1_rmse: 0.49488 |  0:00:35s
epoch 79 | loss: 0.25368 | val_0_rmse: 0.47853 | val_1_rmse: 0.48827 |  0:00:36s
epoch 80 | loss: 0.24878 | val_0_rmse: 0.48691 | val_1_rmse: 0.49761 |  0:00:36s
epoch 81 | loss: 0.25777 | val_0_rmse: 0.48218 | val_1_rmse: 0.49558 |  0:00:37s
epoch 82 | loss: 0.25103 | val_0_rmse: 0.48137 | val_1_rmse: 0.49177 |  0:00:37s
epoch 83 | loss: 0.26842 | val_0_rmse: 0.48567 | val_1_rmse: 0.5026  |  0:00:37s
epoch 84 | loss: 0.261   | val_0_rmse: 0.48375 | val_1_rmse: 0.49346 |  0:00:38s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.48767
Best weights from best epoch are automatically used!
ended training at: 08:33:50
Feature importance:
[('Area', 0.21948761260296548), ('Baths', 0.07038506493201754), ('Beds', 0.08973463809281754), ('Latitude', 0.211454373349132), ('Longitude', 0.299495424500652), ('Month', 0.008096064170386084), ('Year', 0.1013468223520294)]
Mean squared error is of 20127870989.265736
Mean absolute error:102429.08385410093
MAPE:0.1749498985749385
R2 score:0.7527301593061819
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:33:50
epoch 0  | loss: 1.04844 | val_0_rmse: 0.91078 | val_1_rmse: 0.89775 |  0:00:00s
epoch 1  | loss: 0.53911 | val_0_rmse: 0.90554 | val_1_rmse: 0.91377 |  0:00:00s
epoch 2  | loss: 0.45134 | val_0_rmse: 0.69438 | val_1_rmse: 0.70015 |  0:00:01s
epoch 3  | loss: 0.41052 | val_0_rmse: 0.70684 | val_1_rmse: 0.72022 |  0:00:01s
epoch 4  | loss: 0.37404 | val_0_rmse: 0.61509 | val_1_rmse: 0.64299 |  0:00:02s
epoch 5  | loss: 0.36121 | val_0_rmse: 0.58823 | val_1_rmse: 0.60636 |  0:00:02s
epoch 6  | loss: 0.34995 | val_0_rmse: 0.56855 | val_1_rmse: 0.58908 |  0:00:03s
epoch 7  | loss: 0.33252 | val_0_rmse: 0.56055 | val_1_rmse: 0.57767 |  0:00:03s
epoch 8  | loss: 0.32458 | val_0_rmse: 0.55401 | val_1_rmse: 0.57199 |  0:00:04s
epoch 9  | loss: 0.31753 | val_0_rmse: 0.54986 | val_1_rmse: 0.5661  |  0:00:04s
epoch 10 | loss: 0.31571 | val_0_rmse: 0.5427  | val_1_rmse: 0.55651 |  0:00:05s
epoch 11 | loss: 0.31388 | val_0_rmse: 0.54222 | val_1_rmse: 0.56738 |  0:00:05s
epoch 12 | loss: 0.3072  | val_0_rmse: 0.53458 | val_1_rmse: 0.55744 |  0:00:05s
epoch 13 | loss: 0.30788 | val_0_rmse: 0.53281 | val_1_rmse: 0.55547 |  0:00:06s
epoch 14 | loss: 0.30237 | val_0_rmse: 0.5338  | val_1_rmse: 0.5542  |  0:00:06s
epoch 15 | loss: 0.29679 | val_0_rmse: 0.5372  | val_1_rmse: 0.56204 |  0:00:07s
epoch 16 | loss: 0.29361 | val_0_rmse: 0.52598 | val_1_rmse: 0.55308 |  0:00:07s
epoch 17 | loss: 0.28567 | val_0_rmse: 0.51595 | val_1_rmse: 0.54657 |  0:00:08s
epoch 18 | loss: 0.28537 | val_0_rmse: 0.51948 | val_1_rmse: 0.54656 |  0:00:08s
epoch 19 | loss: 0.2842  | val_0_rmse: 0.52257 | val_1_rmse: 0.5533  |  0:00:09s
epoch 20 | loss: 0.28412 | val_0_rmse: 0.51983 | val_1_rmse: 0.54517 |  0:00:09s
epoch 21 | loss: 0.28531 | val_0_rmse: 0.51255 | val_1_rmse: 0.54352 |  0:00:09s
epoch 22 | loss: 0.30293 | val_0_rmse: 0.53318 | val_1_rmse: 0.56301 |  0:00:10s
epoch 23 | loss: 0.31494 | val_0_rmse: 0.54254 | val_1_rmse: 0.56805 |  0:00:10s
epoch 24 | loss: 0.2932  | val_0_rmse: 0.5233  | val_1_rmse: 0.55113 |  0:00:11s
epoch 25 | loss: 0.2941  | val_0_rmse: 0.51944 | val_1_rmse: 0.54432 |  0:00:11s
epoch 26 | loss: 0.28853 | val_0_rmse: 0.52362 | val_1_rmse: 0.55946 |  0:00:12s
epoch 27 | loss: 0.28714 | val_0_rmse: 0.53277 | val_1_rmse: 0.55954 |  0:00:12s
epoch 28 | loss: 0.29483 | val_0_rmse: 0.52933 | val_1_rmse: 0.56849 |  0:00:13s
epoch 29 | loss: 0.28935 | val_0_rmse: 0.51864 | val_1_rmse: 0.54838 |  0:00:13s
epoch 30 | loss: 0.2797  | val_0_rmse: 0.50918 | val_1_rmse: 0.55211 |  0:00:14s
epoch 31 | loss: 0.27642 | val_0_rmse: 0.5023  | val_1_rmse: 0.54108 |  0:00:14s
epoch 32 | loss: 0.27467 | val_0_rmse: 0.5073  | val_1_rmse: 0.54249 |  0:00:14s
epoch 33 | loss: 0.27496 | val_0_rmse: 0.49893 | val_1_rmse: 0.5365  |  0:00:15s
epoch 34 | loss: 0.27245 | val_0_rmse: 0.49495 | val_1_rmse: 0.53607 |  0:00:15s
epoch 35 | loss: 0.26469 | val_0_rmse: 0.4964  | val_1_rmse: 0.5351  |  0:00:16s
epoch 36 | loss: 0.26772 | val_0_rmse: 0.49664 | val_1_rmse: 0.54137 |  0:00:16s
epoch 37 | loss: 0.2779  | val_0_rmse: 0.49879 | val_1_rmse: 0.53876 |  0:00:17s
epoch 38 | loss: 0.26795 | val_0_rmse: 0.49209 | val_1_rmse: 0.53635 |  0:00:17s
epoch 39 | loss: 0.26479 | val_0_rmse: 0.49874 | val_1_rmse: 0.54487 |  0:00:18s
epoch 40 | loss: 0.26198 | val_0_rmse: 0.49451 | val_1_rmse: 0.53721 |  0:00:18s
epoch 41 | loss: 0.26827 | val_0_rmse: 0.49413 | val_1_rmse: 0.539   |  0:00:19s
epoch 42 | loss: 0.26232 | val_0_rmse: 0.50435 | val_1_rmse: 0.54462 |  0:00:19s
epoch 43 | loss: 0.27222 | val_0_rmse: 0.49817 | val_1_rmse: 0.53924 |  0:00:19s
epoch 44 | loss: 0.2799  | val_0_rmse: 0.49336 | val_1_rmse: 0.53517 |  0:00:20s
epoch 45 | loss: 0.26821 | val_0_rmse: 0.49498 | val_1_rmse: 0.53236 |  0:00:20s
epoch 46 | loss: 0.26323 | val_0_rmse: 0.49708 | val_1_rmse: 0.53708 |  0:00:21s
epoch 47 | loss: 0.26548 | val_0_rmse: 0.49715 | val_1_rmse: 0.53948 |  0:00:21s
epoch 48 | loss: 0.26149 | val_0_rmse: 0.49309 | val_1_rmse: 0.53381 |  0:00:22s
epoch 49 | loss: 0.25873 | val_0_rmse: 0.49155 | val_1_rmse: 0.53047 |  0:00:22s
epoch 50 | loss: 0.26781 | val_0_rmse: 0.50195 | val_1_rmse: 0.54213 |  0:00:23s
epoch 51 | loss: 0.26846 | val_0_rmse: 0.49444 | val_1_rmse: 0.53602 |  0:00:23s
epoch 52 | loss: 0.26511 | val_0_rmse: 0.49283 | val_1_rmse: 0.53704 |  0:00:24s
epoch 53 | loss: 0.2607  | val_0_rmse: 0.4879  | val_1_rmse: 0.53934 |  0:00:24s
epoch 54 | loss: 0.2632  | val_0_rmse: 0.49292 | val_1_rmse: 0.53881 |  0:00:24s
epoch 55 | loss: 0.25848 | val_0_rmse: 0.49061 | val_1_rmse: 0.53539 |  0:00:25s
epoch 56 | loss: 0.25875 | val_0_rmse: 0.49574 | val_1_rmse: 0.53165 |  0:00:25s
epoch 57 | loss: 0.25849 | val_0_rmse: 0.49632 | val_1_rmse: 0.54333 |  0:00:26s
epoch 58 | loss: 0.2547  | val_0_rmse: 0.48494 | val_1_rmse: 0.53071 |  0:00:26s
epoch 59 | loss: 0.25409 | val_0_rmse: 0.48769 | val_1_rmse: 0.52951 |  0:00:27s
epoch 60 | loss: 0.2575  | val_0_rmse: 0.49062 | val_1_rmse: 0.53604 |  0:00:27s
epoch 61 | loss: 0.25818 | val_0_rmse: 0.49254 | val_1_rmse: 0.53618 |  0:00:28s
epoch 62 | loss: 0.25876 | val_0_rmse: 0.49116 | val_1_rmse: 0.53882 |  0:00:28s
epoch 63 | loss: 0.25743 | val_0_rmse: 0.48327 | val_1_rmse: 0.53172 |  0:00:28s
epoch 64 | loss: 0.26045 | val_0_rmse: 0.48479 | val_1_rmse: 0.53006 |  0:00:29s
epoch 65 | loss: 0.26452 | val_0_rmse: 0.48599 | val_1_rmse: 0.53513 |  0:00:29s
epoch 66 | loss: 0.25327 | val_0_rmse: 0.48584 | val_1_rmse: 0.52948 |  0:00:30s
epoch 67 | loss: 0.24903 | val_0_rmse: 0.48941 | val_1_rmse: 0.53067 |  0:00:30s
epoch 68 | loss: 0.25958 | val_0_rmse: 0.48395 | val_1_rmse: 0.52791 |  0:00:31s
epoch 69 | loss: 0.25449 | val_0_rmse: 0.48927 | val_1_rmse: 0.53658 |  0:00:31s
epoch 70 | loss: 0.25919 | val_0_rmse: 0.49811 | val_1_rmse: 0.5423  |  0:00:32s
epoch 71 | loss: 0.27155 | val_0_rmse: 0.49171 | val_1_rmse: 0.54121 |  0:00:32s
epoch 72 | loss: 0.25763 | val_0_rmse: 0.49119 | val_1_rmse: 0.53904 |  0:00:33s
epoch 73 | loss: 0.26047 | val_0_rmse: 0.49153 | val_1_rmse: 0.5367  |  0:00:33s
epoch 74 | loss: 0.25547 | val_0_rmse: 0.50382 | val_1_rmse: 0.55341 |  0:00:33s
epoch 75 | loss: 0.25718 | val_0_rmse: 0.48862 | val_1_rmse: 0.5385  |  0:00:34s
epoch 76 | loss: 0.25251 | val_0_rmse: 0.48497 | val_1_rmse: 0.53951 |  0:00:34s
epoch 77 | loss: 0.25352 | val_0_rmse: 0.47738 | val_1_rmse: 0.53207 |  0:00:35s
epoch 78 | loss: 0.24899 | val_0_rmse: 0.47813 | val_1_rmse: 0.53681 |  0:00:35s
epoch 79 | loss: 0.25488 | val_0_rmse: 0.47666 | val_1_rmse: 0.53051 |  0:00:36s
epoch 80 | loss: 0.2449  | val_0_rmse: 0.4751  | val_1_rmse: 0.5311  |  0:00:36s
epoch 81 | loss: 0.24779 | val_0_rmse: 0.47576 | val_1_rmse: 0.53238 |  0:00:37s
epoch 82 | loss: 0.2558  | val_0_rmse: 0.48502 | val_1_rmse: 0.52793 |  0:00:37s
epoch 83 | loss: 0.25283 | val_0_rmse: 0.47853 | val_1_rmse: 0.53389 |  0:00:37s
epoch 84 | loss: 0.25313 | val_0_rmse: 0.47625 | val_1_rmse: 0.5303  |  0:00:38s
epoch 85 | loss: 0.2506  | val_0_rmse: 0.47641 | val_1_rmse: 0.53589 |  0:00:38s
epoch 86 | loss: 0.24661 | val_0_rmse: 0.47178 | val_1_rmse: 0.53069 |  0:00:39s
epoch 87 | loss: 0.2421  | val_0_rmse: 0.4728  | val_1_rmse: 0.52402 |  0:00:39s
epoch 88 | loss: 0.24939 | val_0_rmse: 0.47835 | val_1_rmse: 0.53783 |  0:00:40s
epoch 89 | loss: 0.24969 | val_0_rmse: 0.47709 | val_1_rmse: 0.53137 |  0:00:40s
epoch 90 | loss: 0.24766 | val_0_rmse: 0.48989 | val_1_rmse: 0.54398 |  0:00:41s
epoch 91 | loss: 0.25398 | val_0_rmse: 0.48848 | val_1_rmse: 0.53628 |  0:00:41s
epoch 92 | loss: 0.25274 | val_0_rmse: 0.48441 | val_1_rmse: 0.55102 |  0:00:42s
epoch 93 | loss: 0.2475  | val_0_rmse: 0.4826  | val_1_rmse: 0.53702 |  0:00:42s
epoch 94 | loss: 0.24782 | val_0_rmse: 0.47273 | val_1_rmse: 0.53004 |  0:00:42s
epoch 95 | loss: 0.23857 | val_0_rmse: 0.47139 | val_1_rmse: 0.53111 |  0:00:43s
epoch 96 | loss: 0.24383 | val_0_rmse: 0.47552 | val_1_rmse: 0.53952 |  0:00:43s
epoch 97 | loss: 0.25024 | val_0_rmse: 0.47666 | val_1_rmse: 0.53554 |  0:00:44s
epoch 98 | loss: 0.24754 | val_0_rmse: 0.48353 | val_1_rmse: 0.5294  |  0:00:44s
epoch 99 | loss: 0.24573 | val_0_rmse: 0.46963 | val_1_rmse: 0.51503 |  0:00:45s
epoch 100| loss: 0.24675 | val_0_rmse: 0.47937 | val_1_rmse: 0.52838 |  0:00:45s
epoch 101| loss: 0.25462 | val_0_rmse: 0.46649 | val_1_rmse: 0.51779 |  0:00:46s
epoch 102| loss: 0.2379  | val_0_rmse: 0.47218 | val_1_rmse: 0.5289  |  0:00:46s
epoch 103| loss: 0.23926 | val_0_rmse: 0.46535 | val_1_rmse: 0.52256 |  0:00:46s
epoch 104| loss: 0.23949 | val_0_rmse: 0.46824 | val_1_rmse: 0.52742 |  0:00:47s
epoch 105| loss: 0.2493  | val_0_rmse: 0.47918 | val_1_rmse: 0.54852 |  0:00:47s
epoch 106| loss: 0.23965 | val_0_rmse: 0.4782  | val_1_rmse: 0.52923 |  0:00:48s
epoch 107| loss: 0.2438  | val_0_rmse: 0.4721  | val_1_rmse: 0.52926 |  0:00:48s
epoch 108| loss: 0.24037 | val_0_rmse: 0.47111 | val_1_rmse: 0.52257 |  0:00:49s
epoch 109| loss: 0.24741 | val_0_rmse: 0.47059 | val_1_rmse: 0.53288 |  0:00:49s
epoch 110| loss: 0.24003 | val_0_rmse: 0.46757 | val_1_rmse: 0.52464 |  0:00:50s
epoch 111| loss: 0.24131 | val_0_rmse: 0.47261 | val_1_rmse: 0.52727 |  0:00:50s
epoch 112| loss: 0.23985 | val_0_rmse: 0.47544 | val_1_rmse: 0.53308 |  0:00:50s
epoch 113| loss: 0.23425 | val_0_rmse: 0.46868 | val_1_rmse: 0.52012 |  0:00:51s
epoch 114| loss: 0.23814 | val_0_rmse: 0.46968 | val_1_rmse: 0.53043 |  0:00:51s
epoch 115| loss: 0.2378  | val_0_rmse: 0.47643 | val_1_rmse: 0.53554 |  0:00:52s
epoch 116| loss: 0.24525 | val_0_rmse: 0.47569 | val_1_rmse: 0.53119 |  0:00:52s
epoch 117| loss: 0.25264 | val_0_rmse: 0.4749  | val_1_rmse: 0.52737 |  0:00:53s
epoch 118| loss: 0.24687 | val_0_rmse: 0.47384 | val_1_rmse: 0.53652 |  0:00:53s
epoch 119| loss: 0.24308 | val_0_rmse: 0.46839 | val_1_rmse: 0.5232  |  0:00:54s
epoch 120| loss: 0.23933 | val_0_rmse: 0.47025 | val_1_rmse: 0.52898 |  0:00:54s
epoch 121| loss: 0.24003 | val_0_rmse: 0.46957 | val_1_rmse: 0.53092 |  0:00:55s
epoch 122| loss: 0.23417 | val_0_rmse: 0.45985 | val_1_rmse: 0.52143 |  0:00:55s
epoch 123| loss: 0.2389  | val_0_rmse: 0.47283 | val_1_rmse: 0.53294 |  0:00:55s
epoch 124| loss: 0.24663 | val_0_rmse: 0.47232 | val_1_rmse: 0.53458 |  0:00:56s
epoch 125| loss: 0.24181 | val_0_rmse: 0.46992 | val_1_rmse: 0.53913 |  0:00:56s
epoch 126| loss: 0.23366 | val_0_rmse: 0.46188 | val_1_rmse: 0.53011 |  0:00:57s
epoch 127| loss: 0.23297 | val_0_rmse: 0.46658 | val_1_rmse: 0.52939 |  0:00:57s
epoch 128| loss: 0.23787 | val_0_rmse: 0.46599 | val_1_rmse: 0.53314 |  0:00:58s
epoch 129| loss: 0.23742 | val_0_rmse: 0.46805 | val_1_rmse: 0.52635 |  0:00:58s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.51503
Best weights from best epoch are automatically used!
ended training at: 08:34:49
Feature importance:
[('Area', 0.21675060002449012), ('Baths', 0.010375052966274072), ('Beds', 0.08879705429259019), ('Latitude', 0.2743908161145169), ('Longitude', 0.3455200735412095), ('Month', 0.0072994906996410045), ('Year', 0.05686691236127823)]
Mean squared error is of 20413762997.374302
Mean absolute error:103776.23438130914
MAPE:0.17813815576041217
R2 score:0.751021981783901
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:34:49
epoch 0  | loss: 1.06155 | val_0_rmse: 1.04021 | val_1_rmse: 1.06018 |  0:00:00s
epoch 1  | loss: 0.53005 | val_0_rmse: 0.87769 | val_1_rmse: 0.89873 |  0:00:00s
epoch 2  | loss: 0.41175 | val_0_rmse: 0.73901 | val_1_rmse: 0.7401  |  0:00:01s
epoch 3  | loss: 0.36426 | val_0_rmse: 0.67225 | val_1_rmse: 0.68034 |  0:00:01s
epoch 4  | loss: 0.32912 | val_0_rmse: 0.63502 | val_1_rmse: 0.6443  |  0:00:02s
epoch 5  | loss: 0.30718 | val_0_rmse: 0.56891 | val_1_rmse: 0.58741 |  0:00:02s
epoch 6  | loss: 0.30751 | val_0_rmse: 0.549   | val_1_rmse: 0.57249 |  0:00:03s
epoch 7  | loss: 0.30075 | val_0_rmse: 0.54653 | val_1_rmse: 0.56591 |  0:00:03s
epoch 8  | loss: 0.29907 | val_0_rmse: 0.51836 | val_1_rmse: 0.54538 |  0:00:04s
epoch 9  | loss: 0.28558 | val_0_rmse: 0.518   | val_1_rmse: 0.53826 |  0:00:04s
epoch 10 | loss: 0.29481 | val_0_rmse: 0.51913 | val_1_rmse: 0.54447 |  0:00:05s
epoch 11 | loss: 0.28847 | val_0_rmse: 0.51518 | val_1_rmse: 0.54392 |  0:00:05s
epoch 12 | loss: 0.28631 | val_0_rmse: 0.51735 | val_1_rmse: 0.53658 |  0:00:05s
epoch 13 | loss: 0.29453 | val_0_rmse: 0.52005 | val_1_rmse: 0.55064 |  0:00:06s
epoch 14 | loss: 0.2841  | val_0_rmse: 0.5195  | val_1_rmse: 0.5448  |  0:00:06s
epoch 15 | loss: 0.27836 | val_0_rmse: 0.50453 | val_1_rmse: 0.53352 |  0:00:07s
epoch 16 | loss: 0.2774  | val_0_rmse: 0.49662 | val_1_rmse: 0.5253  |  0:00:07s
epoch 17 | loss: 0.26353 | val_0_rmse: 0.4995  | val_1_rmse: 0.52545 |  0:00:08s
epoch 18 | loss: 0.26911 | val_0_rmse: 0.50329 | val_1_rmse: 0.53106 |  0:00:08s
epoch 19 | loss: 0.26906 | val_0_rmse: 0.49377 | val_1_rmse: 0.52582 |  0:00:09s
epoch 20 | loss: 0.26508 | val_0_rmse: 0.4912  | val_1_rmse: 0.52812 |  0:00:09s
epoch 21 | loss: 0.27588 | val_0_rmse: 0.49573 | val_1_rmse: 0.52666 |  0:00:10s
epoch 22 | loss: 0.27101 | val_0_rmse: 0.49709 | val_1_rmse: 0.52909 |  0:00:10s
epoch 23 | loss: 0.26763 | val_0_rmse: 0.50815 | val_1_rmse: 0.54041 |  0:00:11s
epoch 24 | loss: 0.2642  | val_0_rmse: 0.4973  | val_1_rmse: 0.53106 |  0:00:11s
epoch 25 | loss: 0.26031 | val_0_rmse: 0.49209 | val_1_rmse: 0.52813 |  0:00:11s
epoch 26 | loss: 0.27037 | val_0_rmse: 0.50406 | val_1_rmse: 0.53494 |  0:00:12s
epoch 27 | loss: 0.27387 | val_0_rmse: 0.49396 | val_1_rmse: 0.53019 |  0:00:12s
epoch 28 | loss: 0.25955 | val_0_rmse: 0.49125 | val_1_rmse: 0.52571 |  0:00:13s
epoch 29 | loss: 0.26678 | val_0_rmse: 0.49934 | val_1_rmse: 0.52815 |  0:00:13s
epoch 30 | loss: 0.26732 | val_0_rmse: 0.48957 | val_1_rmse: 0.52439 |  0:00:14s
epoch 31 | loss: 0.26759 | val_0_rmse: 0.4939  | val_1_rmse: 0.5276  |  0:00:14s
epoch 32 | loss: 0.26163 | val_0_rmse: 0.49433 | val_1_rmse: 0.52505 |  0:00:15s
epoch 33 | loss: 0.25868 | val_0_rmse: 0.48638 | val_1_rmse: 0.52725 |  0:00:15s
epoch 34 | loss: 0.25593 | val_0_rmse: 0.48257 | val_1_rmse: 0.52105 |  0:00:15s
epoch 35 | loss: 0.24799 | val_0_rmse: 0.4859  | val_1_rmse: 0.52381 |  0:00:16s
epoch 36 | loss: 0.25596 | val_0_rmse: 0.48721 | val_1_rmse: 0.5237  |  0:00:16s
epoch 37 | loss: 0.26023 | val_0_rmse: 0.5176  | val_1_rmse: 0.55417 |  0:00:17s
epoch 38 | loss: 0.27702 | val_0_rmse: 0.49161 | val_1_rmse: 0.53124 |  0:00:17s
epoch 39 | loss: 0.26792 | val_0_rmse: 0.48223 | val_1_rmse: 0.52231 |  0:00:18s
epoch 40 | loss: 0.25274 | val_0_rmse: 0.4814  | val_1_rmse: 0.52529 |  0:00:18s
epoch 41 | loss: 0.25305 | val_0_rmse: 0.48448 | val_1_rmse: 0.52273 |  0:00:19s
epoch 42 | loss: 0.25372 | val_0_rmse: 0.4857  | val_1_rmse: 0.53223 |  0:00:19s
epoch 43 | loss: 0.25632 | val_0_rmse: 0.48228 | val_1_rmse: 0.52594 |  0:00:19s
epoch 44 | loss: 0.25254 | val_0_rmse: 0.48412 | val_1_rmse: 0.52394 |  0:00:20s
epoch 45 | loss: 0.25907 | val_0_rmse: 0.48896 | val_1_rmse: 0.53381 |  0:00:20s
epoch 46 | loss: 0.25494 | val_0_rmse: 0.48504 | val_1_rmse: 0.52131 |  0:00:21s
epoch 47 | loss: 0.25482 | val_0_rmse: 0.4798  | val_1_rmse: 0.52905 |  0:00:21s
epoch 48 | loss: 0.25097 | val_0_rmse: 0.4818  | val_1_rmse: 0.52382 |  0:00:22s
epoch 49 | loss: 0.25281 | val_0_rmse: 0.47652 | val_1_rmse: 0.51635 |  0:00:22s
epoch 50 | loss: 0.24658 | val_0_rmse: 0.48873 | val_1_rmse: 0.53751 |  0:00:23s
epoch 51 | loss: 0.25103 | val_0_rmse: 0.49564 | val_1_rmse: 0.53507 |  0:00:23s
epoch 52 | loss: 0.25036 | val_0_rmse: 0.47891 | val_1_rmse: 0.5292  |  0:00:23s
epoch 53 | loss: 0.25242 | val_0_rmse: 0.47608 | val_1_rmse: 0.52731 |  0:00:24s
epoch 54 | loss: 0.2639  | val_0_rmse: 0.48416 | val_1_rmse: 0.52999 |  0:00:24s
epoch 55 | loss: 0.24901 | val_0_rmse: 0.50301 | val_1_rmse: 0.54569 |  0:00:25s
epoch 56 | loss: 0.25429 | val_0_rmse: 0.48184 | val_1_rmse: 0.52562 |  0:00:25s
epoch 57 | loss: 0.24302 | val_0_rmse: 0.47504 | val_1_rmse: 0.5222  |  0:00:26s
epoch 58 | loss: 0.24745 | val_0_rmse: 0.4798  | val_1_rmse: 0.5329  |  0:00:26s
epoch 59 | loss: 0.24848 | val_0_rmse: 0.49493 | val_1_rmse: 0.53833 |  0:00:27s
epoch 60 | loss: 0.25605 | val_0_rmse: 0.4951  | val_1_rmse: 0.53823 |  0:00:27s
epoch 61 | loss: 0.24736 | val_0_rmse: 0.47437 | val_1_rmse: 0.5249  |  0:00:28s
epoch 62 | loss: 0.24897 | val_0_rmse: 0.46868 | val_1_rmse: 0.51763 |  0:00:28s
epoch 63 | loss: 0.24305 | val_0_rmse: 0.48477 | val_1_rmse: 0.53981 |  0:00:28s
epoch 64 | loss: 0.25181 | val_0_rmse: 0.48048 | val_1_rmse: 0.53405 |  0:00:29s
epoch 65 | loss: 0.24766 | val_0_rmse: 0.47993 | val_1_rmse: 0.53051 |  0:00:29s
epoch 66 | loss: 0.24213 | val_0_rmse: 0.47109 | val_1_rmse: 0.5232  |  0:00:30s
epoch 67 | loss: 0.24432 | val_0_rmse: 0.48516 | val_1_rmse: 0.54533 |  0:00:30s
epoch 68 | loss: 0.24213 | val_0_rmse: 0.46927 | val_1_rmse: 0.52722 |  0:00:31s
epoch 69 | loss: 0.23596 | val_0_rmse: 0.46828 | val_1_rmse: 0.52278 |  0:00:31s
epoch 70 | loss: 0.24792 | val_0_rmse: 0.46993 | val_1_rmse: 0.52642 |  0:00:32s
epoch 71 | loss: 0.24458 | val_0_rmse: 0.47292 | val_1_rmse: 0.53256 |  0:00:32s
epoch 72 | loss: 0.24378 | val_0_rmse: 0.46864 | val_1_rmse: 0.51576 |  0:00:32s
epoch 73 | loss: 0.24067 | val_0_rmse: 0.46511 | val_1_rmse: 0.52093 |  0:00:33s
epoch 74 | loss: 0.24006 | val_0_rmse: 0.46994 | val_1_rmse: 0.53091 |  0:00:33s
epoch 75 | loss: 0.24402 | val_0_rmse: 0.46673 | val_1_rmse: 0.52721 |  0:00:34s
epoch 76 | loss: 0.23912 | val_0_rmse: 0.47219 | val_1_rmse: 0.52559 |  0:00:34s
epoch 77 | loss: 0.2447  | val_0_rmse: 0.48793 | val_1_rmse: 0.53905 |  0:00:35s
epoch 78 | loss: 0.2389  | val_0_rmse: 0.46458 | val_1_rmse: 0.52407 |  0:00:35s
epoch 79 | loss: 0.23738 | val_0_rmse: 0.47064 | val_1_rmse: 0.53176 |  0:00:36s
epoch 80 | loss: 0.23595 | val_0_rmse: 0.46666 | val_1_rmse: 0.52856 |  0:00:36s
epoch 81 | loss: 0.24909 | val_0_rmse: 0.48616 | val_1_rmse: 0.54521 |  0:00:37s
epoch 82 | loss: 0.23569 | val_0_rmse: 0.46617 | val_1_rmse: 0.52115 |  0:00:37s
epoch 83 | loss: 0.24456 | val_0_rmse: 0.50046 | val_1_rmse: 0.55551 |  0:00:37s
epoch 84 | loss: 0.25213 | val_0_rmse: 0.51429 | val_1_rmse: 0.55597 |  0:00:38s
epoch 85 | loss: 0.24895 | val_0_rmse: 0.47409 | val_1_rmse: 0.53354 |  0:00:38s
epoch 86 | loss: 0.25116 | val_0_rmse: 0.46296 | val_1_rmse: 0.52937 |  0:00:39s
epoch 87 | loss: 0.24465 | val_0_rmse: 0.47448 | val_1_rmse: 0.53489 |  0:00:39s
epoch 88 | loss: 0.23713 | val_0_rmse: 0.46968 | val_1_rmse: 0.52802 |  0:00:40s
epoch 89 | loss: 0.23944 | val_0_rmse: 0.47137 | val_1_rmse: 0.53951 |  0:00:40s
epoch 90 | loss: 0.24064 | val_0_rmse: 0.46724 | val_1_rmse: 0.52639 |  0:00:41s
epoch 91 | loss: 0.236   | val_0_rmse: 0.46484 | val_1_rmse: 0.52625 |  0:00:41s
epoch 92 | loss: 0.23527 | val_0_rmse: 0.46807 | val_1_rmse: 0.53671 |  0:00:41s
epoch 93 | loss: 0.23125 | val_0_rmse: 0.46003 | val_1_rmse: 0.52767 |  0:00:42s
epoch 94 | loss: 0.23126 | val_0_rmse: 0.47589 | val_1_rmse: 0.54381 |  0:00:42s
epoch 95 | loss: 0.22588 | val_0_rmse: 0.45763 | val_1_rmse: 0.53273 |  0:00:43s
epoch 96 | loss: 0.23482 | val_0_rmse: 0.48355 | val_1_rmse: 0.5472  |  0:00:43s
epoch 97 | loss: 0.23357 | val_0_rmse: 0.46679 | val_1_rmse: 0.53303 |  0:00:44s
epoch 98 | loss: 0.235   | val_0_rmse: 0.45992 | val_1_rmse: 0.52723 |  0:00:44s
epoch 99 | loss: 0.22992 | val_0_rmse: 0.46322 | val_1_rmse: 0.53145 |  0:00:45s
epoch 100| loss: 0.23239 | val_0_rmse: 0.45859 | val_1_rmse: 0.53319 |  0:00:45s
epoch 101| loss: 0.2287  | val_0_rmse: 0.45591 | val_1_rmse: 0.52742 |  0:00:46s
epoch 102| loss: 0.23054 | val_0_rmse: 0.45166 | val_1_rmse: 0.52312 |  0:00:46s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.51576
Best weights from best epoch are automatically used!
ended training at: 08:35:36
Feature importance:
[('Area', 0.35227502353600393), ('Baths', 0.022342478544818776), ('Beds', 0.12620764260605), ('Latitude', 0.22169434513406353), ('Longitude', 0.22395522602036902), ('Month', 0.024885540295536863), ('Year', 0.028639743863157877)]
Mean squared error is of 21588125227.108498
Mean absolute error:105183.5310861593
MAPE:0.1748758130063353
R2 score:0.7521219304007892
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:35:36
epoch 0  | loss: 1.06921 | val_0_rmse: 0.87452 | val_1_rmse: 0.84605 |  0:00:00s
epoch 1  | loss: 0.60071 | val_0_rmse: 0.82501 | val_1_rmse: 0.795   |  0:00:00s
epoch 2  | loss: 0.46279 | val_0_rmse: 0.7198  | val_1_rmse: 0.68896 |  0:00:01s
epoch 3  | loss: 0.40856 | val_0_rmse: 0.6826  | val_1_rmse: 0.65705 |  0:00:01s
epoch 4  | loss: 0.37142 | val_0_rmse: 0.61524 | val_1_rmse: 0.60526 |  0:00:02s
epoch 5  | loss: 0.35203 | val_0_rmse: 0.57332 | val_1_rmse: 0.57178 |  0:00:02s
epoch 6  | loss: 0.34642 | val_0_rmse: 0.57257 | val_1_rmse: 0.56569 |  0:00:03s
epoch 7  | loss: 0.34386 | val_0_rmse: 0.56177 | val_1_rmse: 0.54483 |  0:00:03s
epoch 8  | loss: 0.32482 | val_0_rmse: 0.55317 | val_1_rmse: 0.54675 |  0:00:04s
epoch 9  | loss: 0.32878 | val_0_rmse: 0.54937 | val_1_rmse: 0.53627 |  0:00:04s
epoch 10 | loss: 0.32042 | val_0_rmse: 0.55636 | val_1_rmse: 0.54652 |  0:00:05s
epoch 11 | loss: 0.33175 | val_0_rmse: 0.58706 | val_1_rmse: 0.5756  |  0:00:05s
epoch 12 | loss: 0.32064 | val_0_rmse: 0.54542 | val_1_rmse: 0.54205 |  0:00:05s
epoch 13 | loss: 0.3107  | val_0_rmse: 0.53814 | val_1_rmse: 0.53127 |  0:00:06s
epoch 14 | loss: 0.3042  | val_0_rmse: 0.53941 | val_1_rmse: 0.5286  |  0:00:06s
epoch 15 | loss: 0.30106 | val_0_rmse: 0.53401 | val_1_rmse: 0.52818 |  0:00:07s
epoch 16 | loss: 0.30481 | val_0_rmse: 0.53509 | val_1_rmse: 0.53974 |  0:00:07s
epoch 17 | loss: 0.31554 | val_0_rmse: 0.55896 | val_1_rmse: 0.55624 |  0:00:08s
epoch 18 | loss: 0.31897 | val_0_rmse: 0.53587 | val_1_rmse: 0.52971 |  0:00:08s
epoch 19 | loss: 0.30864 | val_0_rmse: 0.53082 | val_1_rmse: 0.5326  |  0:00:09s
epoch 20 | loss: 0.30612 | val_0_rmse: 0.52518 | val_1_rmse: 0.51929 |  0:00:09s
epoch 21 | loss: 0.3064  | val_0_rmse: 0.52562 | val_1_rmse: 0.51938 |  0:00:09s
epoch 22 | loss: 0.30582 | val_0_rmse: 0.53418 | val_1_rmse: 0.53337 |  0:00:10s
epoch 23 | loss: 0.29458 | val_0_rmse: 0.51675 | val_1_rmse: 0.51206 |  0:00:10s
epoch 24 | loss: 0.28578 | val_0_rmse: 0.51954 | val_1_rmse: 0.52079 |  0:00:11s
epoch 25 | loss: 0.28297 | val_0_rmse: 0.51208 | val_1_rmse: 0.51408 |  0:00:11s
epoch 26 | loss: 0.27756 | val_0_rmse: 0.50585 | val_1_rmse: 0.51444 |  0:00:12s
epoch 27 | loss: 0.28112 | val_0_rmse: 0.51574 | val_1_rmse: 0.51773 |  0:00:12s
epoch 28 | loss: 0.28199 | val_0_rmse: 0.52877 | val_1_rmse: 0.53592 |  0:00:13s
epoch 29 | loss: 0.27894 | val_0_rmse: 0.51011 | val_1_rmse: 0.51345 |  0:00:13s
epoch 30 | loss: 0.28007 | val_0_rmse: 0.52429 | val_1_rmse: 0.53032 |  0:00:13s
epoch 31 | loss: 0.28023 | val_0_rmse: 0.50825 | val_1_rmse: 0.5088  |  0:00:14s
epoch 32 | loss: 0.28537 | val_0_rmse: 0.51361 | val_1_rmse: 0.5136  |  0:00:14s
epoch 33 | loss: 0.27466 | val_0_rmse: 0.51119 | val_1_rmse: 0.50825 |  0:00:15s
epoch 34 | loss: 0.2752  | val_0_rmse: 0.50148 | val_1_rmse: 0.50226 |  0:00:15s
epoch 35 | loss: 0.26811 | val_0_rmse: 0.50789 | val_1_rmse: 0.50543 |  0:00:16s
epoch 36 | loss: 0.27734 | val_0_rmse: 0.50455 | val_1_rmse: 0.50457 |  0:00:16s
epoch 37 | loss: 0.27176 | val_0_rmse: 0.49118 | val_1_rmse: 0.49713 |  0:00:17s
epoch 38 | loss: 0.26544 | val_0_rmse: 0.50746 | val_1_rmse: 0.51589 |  0:00:17s
epoch 39 | loss: 0.26812 | val_0_rmse: 0.50544 | val_1_rmse: 0.50134 |  0:00:18s
epoch 40 | loss: 0.28224 | val_0_rmse: 0.52207 | val_1_rmse: 0.52628 |  0:00:18s
epoch 41 | loss: 0.27905 | val_0_rmse: 0.51048 | val_1_rmse: 0.50831 |  0:00:18s
epoch 42 | loss: 0.28392 | val_0_rmse: 0.50236 | val_1_rmse: 0.5031  |  0:00:19s
epoch 43 | loss: 0.27197 | val_0_rmse: 0.50321 | val_1_rmse: 0.50214 |  0:00:19s
epoch 44 | loss: 0.26842 | val_0_rmse: 0.49535 | val_1_rmse: 0.4976  |  0:00:20s
epoch 45 | loss: 0.26532 | val_0_rmse: 0.49097 | val_1_rmse: 0.49879 |  0:00:20s
epoch 46 | loss: 0.26314 | val_0_rmse: 0.49056 | val_1_rmse: 0.49545 |  0:00:21s
epoch 47 | loss: 0.25625 | val_0_rmse: 0.49422 | val_1_rmse: 0.49072 |  0:00:21s
epoch 48 | loss: 0.26696 | val_0_rmse: 0.49909 | val_1_rmse: 0.50269 |  0:00:22s
epoch 49 | loss: 0.26027 | val_0_rmse: 0.50452 | val_1_rmse: 0.5146  |  0:00:22s
epoch 50 | loss: 0.26598 | val_0_rmse: 0.48849 | val_1_rmse: 0.49505 |  0:00:23s
epoch 51 | loss: 0.2606  | val_0_rmse: 0.48495 | val_1_rmse: 0.49576 |  0:00:23s
epoch 52 | loss: 0.25932 | val_0_rmse: 0.51073 | val_1_rmse: 0.51513 |  0:00:23s
epoch 53 | loss: 0.269   | val_0_rmse: 0.49303 | val_1_rmse: 0.5105  |  0:00:24s
epoch 54 | loss: 0.2588  | val_0_rmse: 0.49726 | val_1_rmse: 0.50131 |  0:00:24s
epoch 55 | loss: 0.25564 | val_0_rmse: 0.49321 | val_1_rmse: 0.50876 |  0:00:25s
epoch 56 | loss: 0.26184 | val_0_rmse: 0.48578 | val_1_rmse: 0.48903 |  0:00:25s
epoch 57 | loss: 0.26326 | val_0_rmse: 0.48727 | val_1_rmse: 0.50191 |  0:00:26s
epoch 58 | loss: 0.25904 | val_0_rmse: 0.49871 | val_1_rmse: 0.49935 |  0:00:26s
epoch 59 | loss: 0.26017 | val_0_rmse: 0.48244 | val_1_rmse: 0.49749 |  0:00:27s
epoch 60 | loss: 0.25248 | val_0_rmse: 0.48412 | val_1_rmse: 0.49025 |  0:00:27s
epoch 61 | loss: 0.25352 | val_0_rmse: 0.4962  | val_1_rmse: 0.49819 |  0:00:27s
epoch 62 | loss: 0.25598 | val_0_rmse: 0.485   | val_1_rmse: 0.49328 |  0:00:28s
epoch 63 | loss: 0.25785 | val_0_rmse: 0.48423 | val_1_rmse: 0.49597 |  0:00:28s
epoch 64 | loss: 0.24876 | val_0_rmse: 0.49023 | val_1_rmse: 0.4955  |  0:00:29s
epoch 65 | loss: 0.25392 | val_0_rmse: 0.48475 | val_1_rmse: 0.50091 |  0:00:29s
epoch 66 | loss: 0.24744 | val_0_rmse: 0.47975 | val_1_rmse: 0.49818 |  0:00:30s
epoch 67 | loss: 0.24694 | val_0_rmse: 0.48634 | val_1_rmse: 0.50078 |  0:00:30s
epoch 68 | loss: 0.25287 | val_0_rmse: 0.47962 | val_1_rmse: 0.4863  |  0:00:31s
epoch 69 | loss: 0.25363 | val_0_rmse: 0.47841 | val_1_rmse: 0.48661 |  0:00:31s
epoch 70 | loss: 0.25045 | val_0_rmse: 0.47964 | val_1_rmse: 0.49768 |  0:00:32s
epoch 71 | loss: 0.24813 | val_0_rmse: 0.48054 | val_1_rmse: 0.48913 |  0:00:32s
epoch 72 | loss: 0.25331 | val_0_rmse: 0.47405 | val_1_rmse: 0.49315 |  0:00:32s
epoch 73 | loss: 0.25013 | val_0_rmse: 0.49346 | val_1_rmse: 0.50485 |  0:00:33s
epoch 74 | loss: 0.25078 | val_0_rmse: 0.48574 | val_1_rmse: 0.49894 |  0:00:33s
epoch 75 | loss: 0.24911 | val_0_rmse: 0.47127 | val_1_rmse: 0.49387 |  0:00:34s
epoch 76 | loss: 0.24514 | val_0_rmse: 0.47284 | val_1_rmse: 0.49101 |  0:00:34s
epoch 77 | loss: 0.25502 | val_0_rmse: 0.49723 | val_1_rmse: 0.51191 |  0:00:35s
epoch 78 | loss: 0.25097 | val_0_rmse: 0.49224 | val_1_rmse: 0.49268 |  0:00:35s
epoch 79 | loss: 0.25788 | val_0_rmse: 0.48425 | val_1_rmse: 0.50582 |  0:00:36s
epoch 80 | loss: 0.2472  | val_0_rmse: 0.4743  | val_1_rmse: 0.4872  |  0:00:36s
epoch 81 | loss: 0.24314 | val_0_rmse: 0.48953 | val_1_rmse: 0.50776 |  0:00:36s
epoch 82 | loss: 0.24795 | val_0_rmse: 0.47194 | val_1_rmse: 0.48713 |  0:00:37s
epoch 83 | loss: 0.23938 | val_0_rmse: 0.46931 | val_1_rmse: 0.49598 |  0:00:37s
epoch 84 | loss: 0.24433 | val_0_rmse: 0.47698 | val_1_rmse: 0.49512 |  0:00:38s
epoch 85 | loss: 0.24585 | val_0_rmse: 0.4703  | val_1_rmse: 0.49303 |  0:00:38s
epoch 86 | loss: 0.24575 | val_0_rmse: 0.47843 | val_1_rmse: 0.49461 |  0:00:39s
epoch 87 | loss: 0.24542 | val_0_rmse: 0.47465 | val_1_rmse: 0.48521 |  0:00:39s
epoch 88 | loss: 0.24743 | val_0_rmse: 0.48567 | val_1_rmse: 0.50129 |  0:00:40s
epoch 89 | loss: 0.24735 | val_0_rmse: 0.48451 | val_1_rmse: 0.50417 |  0:00:40s
epoch 90 | loss: 0.24601 | val_0_rmse: 0.47748 | val_1_rmse: 0.49338 |  0:00:40s
epoch 91 | loss: 0.2418  | val_0_rmse: 0.46996 | val_1_rmse: 0.49396 |  0:00:41s
epoch 92 | loss: 0.23679 | val_0_rmse: 0.47936 | val_1_rmse: 0.50637 |  0:00:41s
epoch 93 | loss: 0.24199 | val_0_rmse: 0.47125 | val_1_rmse: 0.49691 |  0:00:42s
epoch 94 | loss: 0.24112 | val_0_rmse: 0.47719 | val_1_rmse: 0.4957  |  0:00:42s
epoch 95 | loss: 0.24416 | val_0_rmse: 0.4714  | val_1_rmse: 0.48875 |  0:00:43s
epoch 96 | loss: 0.2427  | val_0_rmse: 0.46925 | val_1_rmse: 0.49142 |  0:00:43s
epoch 97 | loss: 0.24984 | val_0_rmse: 0.4839  | val_1_rmse: 0.50512 |  0:00:44s
epoch 98 | loss: 0.26017 | val_0_rmse: 0.49453 | val_1_rmse: 0.52658 |  0:00:44s
epoch 99 | loss: 0.24918 | val_0_rmse: 0.47148 | val_1_rmse: 0.49329 |  0:00:44s
epoch 100| loss: 0.24096 | val_0_rmse: 0.47851 | val_1_rmse: 0.49972 |  0:00:45s
epoch 101| loss: 0.24514 | val_0_rmse: 0.48444 | val_1_rmse: 0.49166 |  0:00:45s
epoch 102| loss: 0.24454 | val_0_rmse: 0.47839 | val_1_rmse: 0.49413 |  0:00:46s
epoch 103| loss: 0.25413 | val_0_rmse: 0.48118 | val_1_rmse: 0.49833 |  0:00:46s
epoch 104| loss: 0.24954 | val_0_rmse: 0.47645 | val_1_rmse: 0.49712 |  0:00:47s
epoch 105| loss: 0.23931 | val_0_rmse: 0.48531 | val_1_rmse: 0.50696 |  0:00:47s
epoch 106| loss: 0.24682 | val_0_rmse: 0.46529 | val_1_rmse: 0.49347 |  0:00:48s
epoch 107| loss: 0.24257 | val_0_rmse: 0.47994 | val_1_rmse: 0.49909 |  0:00:48s
epoch 108| loss: 0.24714 | val_0_rmse: 0.4676  | val_1_rmse: 0.48605 |  0:00:48s
epoch 109| loss: 0.23893 | val_0_rmse: 0.4704  | val_1_rmse: 0.48918 |  0:00:49s
epoch 110| loss: 0.23611 | val_0_rmse: 0.45947 | val_1_rmse: 0.48256 |  0:00:49s
epoch 111| loss: 0.23405 | val_0_rmse: 0.46881 | val_1_rmse: 0.49944 |  0:00:50s
epoch 112| loss: 0.22943 | val_0_rmse: 0.4766  | val_1_rmse: 0.49716 |  0:00:50s
epoch 113| loss: 0.24067 | val_0_rmse: 0.46482 | val_1_rmse: 0.48626 |  0:00:51s
epoch 114| loss: 0.23288 | val_0_rmse: 0.45756 | val_1_rmse: 0.48957 |  0:00:51s
epoch 115| loss: 0.23559 | val_0_rmse: 0.47018 | val_1_rmse: 0.49628 |  0:00:52s
epoch 116| loss: 0.23426 | val_0_rmse: 0.46213 | val_1_rmse: 0.49176 |  0:00:52s
epoch 117| loss: 0.23341 | val_0_rmse: 0.46307 | val_1_rmse: 0.48656 |  0:00:52s
epoch 118| loss: 0.23513 | val_0_rmse: 0.46503 | val_1_rmse: 0.4918  |  0:00:53s
epoch 119| loss: 0.23893 | val_0_rmse: 0.46234 | val_1_rmse: 0.49648 |  0:00:53s
epoch 120| loss: 0.24002 | val_0_rmse: 0.47468 | val_1_rmse: 0.49885 |  0:00:54s
epoch 121| loss: 0.23753 | val_0_rmse: 0.46001 | val_1_rmse: 0.49642 |  0:00:54s
epoch 122| loss: 0.23221 | val_0_rmse: 0.45522 | val_1_rmse: 0.48806 |  0:00:55s
epoch 123| loss: 0.23074 | val_0_rmse: 0.46978 | val_1_rmse: 0.50868 |  0:00:55s
epoch 124| loss: 0.23142 | val_0_rmse: 0.46018 | val_1_rmse: 0.495   |  0:00:56s
epoch 125| loss: 0.2307  | val_0_rmse: 0.46401 | val_1_rmse: 0.4886  |  0:00:56s
epoch 126| loss: 0.22847 | val_0_rmse: 0.46234 | val_1_rmse: 0.49511 |  0:00:57s
epoch 127| loss: 0.23419 | val_0_rmse: 0.46823 | val_1_rmse: 0.49421 |  0:00:57s
epoch 128| loss: 0.23929 | val_0_rmse: 0.47548 | val_1_rmse: 0.50788 |  0:00:57s
epoch 129| loss: 0.23871 | val_0_rmse: 0.46674 | val_1_rmse: 0.49979 |  0:00:58s
epoch 130| loss: 0.23284 | val_0_rmse: 0.459   | val_1_rmse: 0.48929 |  0:00:58s
epoch 131| loss: 0.22629 | val_0_rmse: 0.46315 | val_1_rmse: 0.49156 |  0:00:59s
epoch 132| loss: 0.22895 | val_0_rmse: 0.45296 | val_1_rmse: 0.49098 |  0:00:59s
epoch 133| loss: 0.22164 | val_0_rmse: 0.45894 | val_1_rmse: 0.4874  |  0:01:00s
epoch 134| loss: 0.22713 | val_0_rmse: 0.45316 | val_1_rmse: 0.48942 |  0:01:00s
epoch 135| loss: 0.22597 | val_0_rmse: 0.45401 | val_1_rmse: 0.47835 |  0:01:01s
epoch 136| loss: 0.22665 | val_0_rmse: 0.47326 | val_1_rmse: 0.50505 |  0:01:01s
epoch 137| loss: 0.23339 | val_0_rmse: 0.45782 | val_1_rmse: 0.49308 |  0:01:01s
epoch 138| loss: 0.22554 | val_0_rmse: 0.4573  | val_1_rmse: 0.4911  |  0:01:02s
epoch 139| loss: 0.23366 | val_0_rmse: 0.46804 | val_1_rmse: 0.49576 |  0:01:02s
epoch 140| loss: 0.23546 | val_0_rmse: 0.4552  | val_1_rmse: 0.49118 |  0:01:03s
epoch 141| loss: 0.23224 | val_0_rmse: 0.45904 | val_1_rmse: 0.4918  |  0:01:03s
epoch 142| loss: 0.22604 | val_0_rmse: 0.45408 | val_1_rmse: 0.48594 |  0:01:04s
epoch 143| loss: 0.23479 | val_0_rmse: 0.46657 | val_1_rmse: 0.49648 |  0:01:04s
epoch 144| loss: 0.23136 | val_0_rmse: 0.45274 | val_1_rmse: 0.48757 |  0:01:05s
epoch 145| loss: 0.22793 | val_0_rmse: 0.45617 | val_1_rmse: 0.49327 |  0:01:05s
epoch 146| loss: 0.22809 | val_0_rmse: 0.45072 | val_1_rmse: 0.48593 |  0:01:05s
epoch 147| loss: 0.22387 | val_0_rmse: 0.4564  | val_1_rmse: 0.4941  |  0:01:06s
epoch 148| loss: 0.23401 | val_0_rmse: 0.45428 | val_1_rmse: 0.48978 |  0:01:06s
epoch 149| loss: 0.22473 | val_0_rmse: 0.45808 | val_1_rmse: 0.50075 |  0:01:07s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.47835
Best weights from best epoch are automatically used!
ended training at: 08:36:43
Feature importance:
[('Area', 0.2764135764257195), ('Baths', 0.025666145952800708), ('Beds', 0.10417356959471676), ('Latitude', 0.2586766008335053), ('Longitude', 0.2678953720604136), ('Month', 0.06022424130678018), ('Year', 0.006950493826063985)]
Mean squared error is of 20933530914.357883
Mean absolute error:104888.45126932177
MAPE:0.17300666516377916
R2 score:0.7511507791635725
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:36:43
epoch 0  | loss: 1.06215 | val_0_rmse: 0.92358 | val_1_rmse: 0.93796 |  0:00:00s
epoch 1  | loss: 0.54334 | val_0_rmse: 0.83749 | val_1_rmse: 0.87601 |  0:00:00s
epoch 2  | loss: 0.41004 | val_0_rmse: 0.73276 | val_1_rmse: 0.74547 |  0:00:01s
epoch 3  | loss: 0.37225 | val_0_rmse: 0.759   | val_1_rmse: 0.76698 |  0:00:01s
epoch 4  | loss: 0.34531 | val_0_rmse: 0.68765 | val_1_rmse: 0.69631 |  0:00:02s
epoch 5  | loss: 0.33639 | val_0_rmse: 0.60046 | val_1_rmse: 0.61344 |  0:00:02s
epoch 6  | loss: 0.32436 | val_0_rmse: 0.56129 | val_1_rmse: 0.5692  |  0:00:03s
epoch 7  | loss: 0.31852 | val_0_rmse: 0.54776 | val_1_rmse: 0.562   |  0:00:03s
epoch 8  | loss: 0.3145  | val_0_rmse: 0.54974 | val_1_rmse: 0.55874 |  0:00:04s
epoch 9  | loss: 0.3041  | val_0_rmse: 0.5497  | val_1_rmse: 0.56027 |  0:00:04s
epoch 10 | loss: 0.3029  | val_0_rmse: 0.53255 | val_1_rmse: 0.54709 |  0:00:04s
epoch 11 | loss: 0.29462 | val_0_rmse: 0.52791 | val_1_rmse: 0.54186 |  0:00:05s
epoch 12 | loss: 0.29384 | val_0_rmse: 0.53439 | val_1_rmse: 0.55395 |  0:00:05s
epoch 13 | loss: 0.30004 | val_0_rmse: 0.52461 | val_1_rmse: 0.54428 |  0:00:06s
epoch 14 | loss: 0.29046 | val_0_rmse: 0.53313 | val_1_rmse: 0.54826 |  0:00:06s
epoch 15 | loss: 0.28946 | val_0_rmse: 0.51708 | val_1_rmse: 0.53579 |  0:00:07s
epoch 16 | loss: 0.29046 | val_0_rmse: 0.52452 | val_1_rmse: 0.53233 |  0:00:07s
epoch 17 | loss: 0.30401 | val_0_rmse: 0.52252 | val_1_rmse: 0.53898 |  0:00:08s
epoch 18 | loss: 0.30224 | val_0_rmse: 0.5291  | val_1_rmse: 0.55097 |  0:00:08s
epoch 19 | loss: 0.29808 | val_0_rmse: 0.52171 | val_1_rmse: 0.53455 |  0:00:09s
epoch 20 | loss: 0.28907 | val_0_rmse: 0.52136 | val_1_rmse: 0.54252 |  0:00:09s
epoch 21 | loss: 0.2856  | val_0_rmse: 0.50944 | val_1_rmse: 0.52079 |  0:00:09s
epoch 22 | loss: 0.28253 | val_0_rmse: 0.51813 | val_1_rmse: 0.52587 |  0:00:10s
epoch 23 | loss: 0.28751 | val_0_rmse: 0.50877 | val_1_rmse: 0.52322 |  0:00:10s
epoch 24 | loss: 0.28191 | val_0_rmse: 0.5052  | val_1_rmse: 0.51761 |  0:00:11s
epoch 25 | loss: 0.27966 | val_0_rmse: 0.5194  | val_1_rmse: 0.53513 |  0:00:11s
epoch 26 | loss: 0.27811 | val_0_rmse: 0.50963 | val_1_rmse: 0.52306 |  0:00:12s
epoch 27 | loss: 0.26682 | val_0_rmse: 0.50215 | val_1_rmse: 0.52434 |  0:00:12s
epoch 28 | loss: 0.27937 | val_0_rmse: 0.50804 | val_1_rmse: 0.5227  |  0:00:13s
epoch 29 | loss: 0.27577 | val_0_rmse: 0.50547 | val_1_rmse: 0.52125 |  0:00:13s
epoch 30 | loss: 0.28277 | val_0_rmse: 0.51784 | val_1_rmse: 0.53173 |  0:00:14s
epoch 31 | loss: 0.27887 | val_0_rmse: 0.51712 | val_1_rmse: 0.53164 |  0:00:14s
epoch 32 | loss: 0.27568 | val_0_rmse: 0.50653 | val_1_rmse: 0.52799 |  0:00:14s
epoch 33 | loss: 0.27502 | val_0_rmse: 0.50301 | val_1_rmse: 0.52567 |  0:00:15s
epoch 34 | loss: 0.26312 | val_0_rmse: 0.50441 | val_1_rmse: 0.5308  |  0:00:15s
epoch 35 | loss: 0.27458 | val_0_rmse: 0.50188 | val_1_rmse: 0.52098 |  0:00:16s
epoch 36 | loss: 0.26828 | val_0_rmse: 0.50863 | val_1_rmse: 0.53314 |  0:00:16s
epoch 37 | loss: 0.26706 | val_0_rmse: 0.4949  | val_1_rmse: 0.52086 |  0:00:17s
epoch 38 | loss: 0.26913 | val_0_rmse: 0.50474 | val_1_rmse: 0.52028 |  0:00:17s
epoch 39 | loss: 0.26795 | val_0_rmse: 0.50335 | val_1_rmse: 0.52051 |  0:00:18s
epoch 40 | loss: 0.26696 | val_0_rmse: 0.5005  | val_1_rmse: 0.51612 |  0:00:18s
epoch 41 | loss: 0.27307 | val_0_rmse: 0.52177 | val_1_rmse: 0.53737 |  0:00:18s
epoch 42 | loss: 0.26299 | val_0_rmse: 0.50472 | val_1_rmse: 0.53178 |  0:00:19s
epoch 43 | loss: 0.25916 | val_0_rmse: 0.4981  | val_1_rmse: 0.51442 |  0:00:19s
epoch 44 | loss: 0.25369 | val_0_rmse: 0.49381 | val_1_rmse: 0.51732 |  0:00:20s
epoch 45 | loss: 0.2611  | val_0_rmse: 0.49635 | val_1_rmse: 0.53076 |  0:00:20s
epoch 46 | loss: 0.2669  | val_0_rmse: 0.50096 | val_1_rmse: 0.52524 |  0:00:21s
epoch 47 | loss: 0.25848 | val_0_rmse: 0.488   | val_1_rmse: 0.50536 |  0:00:21s
epoch 48 | loss: 0.25858 | val_0_rmse: 0.48499 | val_1_rmse: 0.50368 |  0:00:22s
epoch 49 | loss: 0.25502 | val_0_rmse: 0.48718 | val_1_rmse: 0.51619 |  0:00:22s
epoch 50 | loss: 0.26015 | val_0_rmse: 0.48653 | val_1_rmse: 0.51797 |  0:00:22s
epoch 51 | loss: 0.25753 | val_0_rmse: 0.49397 | val_1_rmse: 0.51907 |  0:00:23s
epoch 52 | loss: 0.25645 | val_0_rmse: 0.48782 | val_1_rmse: 0.51189 |  0:00:23s
epoch 53 | loss: 0.25614 | val_0_rmse: 0.48693 | val_1_rmse: 0.50982 |  0:00:24s
epoch 54 | loss: 0.25081 | val_0_rmse: 0.48321 | val_1_rmse: 0.50604 |  0:00:24s
epoch 55 | loss: 0.24524 | val_0_rmse: 0.49074 | val_1_rmse: 0.51393 |  0:00:25s
epoch 56 | loss: 0.26279 | val_0_rmse: 0.49247 | val_1_rmse: 0.52564 |  0:00:25s
epoch 57 | loss: 0.25652 | val_0_rmse: 0.4917  | val_1_rmse: 0.52902 |  0:00:26s
epoch 58 | loss: 0.26176 | val_0_rmse: 0.49161 | val_1_rmse: 0.52441 |  0:00:26s
epoch 59 | loss: 0.25238 | val_0_rmse: 0.49918 | val_1_rmse: 0.52056 |  0:00:27s
epoch 60 | loss: 0.25247 | val_0_rmse: 0.49605 | val_1_rmse: 0.5271  |  0:00:27s
epoch 61 | loss: 0.2594  | val_0_rmse: 0.48578 | val_1_rmse: 0.51632 |  0:00:27s
epoch 62 | loss: 0.25235 | val_0_rmse: 0.48658 | val_1_rmse: 0.51861 |  0:00:28s
epoch 63 | loss: 0.25913 | val_0_rmse: 0.4841  | val_1_rmse: 0.51458 |  0:00:28s
epoch 64 | loss: 0.24832 | val_0_rmse: 0.4892  | val_1_rmse: 0.5128  |  0:00:29s
epoch 65 | loss: 0.2556  | val_0_rmse: 0.48761 | val_1_rmse: 0.51422 |  0:00:29s
epoch 66 | loss: 0.24374 | val_0_rmse: 0.47762 | val_1_rmse: 0.51639 |  0:00:30s
epoch 67 | loss: 0.25561 | val_0_rmse: 0.49182 | val_1_rmse: 0.52908 |  0:00:30s
epoch 68 | loss: 0.25137 | val_0_rmse: 0.487   | val_1_rmse: 0.51283 |  0:00:31s
epoch 69 | loss: 0.25092 | val_0_rmse: 0.48619 | val_1_rmse: 0.51743 |  0:00:31s
epoch 70 | loss: 0.25373 | val_0_rmse: 0.50297 | val_1_rmse: 0.53367 |  0:00:31s
epoch 71 | loss: 0.25001 | val_0_rmse: 0.47957 | val_1_rmse: 0.50242 |  0:00:32s
epoch 72 | loss: 0.24368 | val_0_rmse: 0.47406 | val_1_rmse: 0.51177 |  0:00:32s
epoch 73 | loss: 0.24893 | val_0_rmse: 0.49984 | val_1_rmse: 0.53888 |  0:00:33s
epoch 74 | loss: 0.26009 | val_0_rmse: 0.48211 | val_1_rmse: 0.50869 |  0:00:33s
epoch 75 | loss: 0.2641  | val_0_rmse: 0.47987 | val_1_rmse: 0.51556 |  0:00:34s
epoch 76 | loss: 0.26034 | val_0_rmse: 0.48073 | val_1_rmse: 0.51798 |  0:00:34s
epoch 77 | loss: 0.248   | val_0_rmse: 0.48271 | val_1_rmse: 0.51464 |  0:00:35s
epoch 78 | loss: 0.2487  | val_0_rmse: 0.46874 | val_1_rmse: 0.50926 |  0:00:35s
epoch 79 | loss: 0.24585 | val_0_rmse: 0.47277 | val_1_rmse: 0.51406 |  0:00:36s
epoch 80 | loss: 0.2528  | val_0_rmse: 0.47425 | val_1_rmse: 0.50576 |  0:00:36s
epoch 81 | loss: 0.25192 | val_0_rmse: 0.49855 | val_1_rmse: 0.52639 |  0:00:36s
epoch 82 | loss: 0.25215 | val_0_rmse: 0.48042 | val_1_rmse: 0.50943 |  0:00:37s
epoch 83 | loss: 0.24546 | val_0_rmse: 0.47405 | val_1_rmse: 0.51255 |  0:00:37s
epoch 84 | loss: 0.24282 | val_0_rmse: 0.48514 | val_1_rmse: 0.52516 |  0:00:38s
epoch 85 | loss: 0.24072 | val_0_rmse: 0.47455 | val_1_rmse: 0.50943 |  0:00:38s
epoch 86 | loss: 0.24125 | val_0_rmse: 0.47498 | val_1_rmse: 0.50913 |  0:00:39s
epoch 87 | loss: 0.24324 | val_0_rmse: 0.47074 | val_1_rmse: 0.51318 |  0:00:39s
epoch 88 | loss: 0.24674 | val_0_rmse: 0.4779  | val_1_rmse: 0.52309 |  0:00:40s
epoch 89 | loss: 0.24634 | val_0_rmse: 0.48061 | val_1_rmse: 0.51194 |  0:00:40s
epoch 90 | loss: 0.24068 | val_0_rmse: 0.4708  | val_1_rmse: 0.50463 |  0:00:40s
epoch 91 | loss: 0.24186 | val_0_rmse: 0.47407 | val_1_rmse: 0.50564 |  0:00:41s
epoch 92 | loss: 0.24099 | val_0_rmse: 0.4789  | val_1_rmse: 0.51584 |  0:00:41s
epoch 93 | loss: 0.24027 | val_0_rmse: 0.47211 | val_1_rmse: 0.51702 |  0:00:42s
epoch 94 | loss: 0.24339 | val_0_rmse: 0.46677 | val_1_rmse: 0.5122  |  0:00:42s
epoch 95 | loss: 0.24372 | val_0_rmse: 0.48289 | val_1_rmse: 0.52346 |  0:00:43s
epoch 96 | loss: 0.24703 | val_0_rmse: 0.48225 | val_1_rmse: 0.52081 |  0:00:43s
epoch 97 | loss: 0.25101 | val_0_rmse: 0.46278 | val_1_rmse: 0.50813 |  0:00:44s
epoch 98 | loss: 0.23847 | val_0_rmse: 0.46533 | val_1_rmse: 0.50703 |  0:00:44s
epoch 99 | loss: 0.2371  | val_0_rmse: 0.47549 | val_1_rmse: 0.51878 |  0:00:44s
epoch 100| loss: 0.23853 | val_0_rmse: 0.48492 | val_1_rmse: 0.53447 |  0:00:45s
epoch 101| loss: 0.24223 | val_0_rmse: 0.4742  | val_1_rmse: 0.51641 |  0:00:45s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.50242
Best weights from best epoch are automatically used!
ended training at: 08:37:29
Feature importance:
[('Area', 0.19976009445888926), ('Baths', 0.004319250255032578), ('Beds', 0.13634352216758105), ('Latitude', 0.3331593823711351), ('Longitude', 0.29232705547030785), ('Month', 0.026816141976678993), ('Year', 0.00727455330037515)]
Mean squared error is of 24438818604.922173
Mean absolute error:113973.54096516826
MAPE:0.20536747830382682
R2 score:0.7038626856229582
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:37:29
epoch 0  | loss: 1.09058 | val_0_rmse: 0.90454 | val_1_rmse: 0.888   |  0:00:00s
epoch 1  | loss: 0.61131 | val_0_rmse: 0.88876 | val_1_rmse: 0.9019  |  0:00:00s
epoch 2  | loss: 0.52011 | val_0_rmse: 0.8577  | val_1_rmse: 0.86361 |  0:00:01s
epoch 3  | loss: 0.42593 | val_0_rmse: 0.71225 | val_1_rmse: 0.72218 |  0:00:01s
epoch 4  | loss: 0.37298 | val_0_rmse: 0.60288 | val_1_rmse: 0.61117 |  0:00:02s
epoch 5  | loss: 0.34539 | val_0_rmse: 0.57604 | val_1_rmse: 0.58008 |  0:00:02s
epoch 6  | loss: 0.32883 | val_0_rmse: 0.54952 | val_1_rmse: 0.55186 |  0:00:03s
epoch 7  | loss: 0.31278 | val_0_rmse: 0.54108 | val_1_rmse: 0.5414  |  0:00:03s
epoch 8  | loss: 0.31222 | val_0_rmse: 0.53708 | val_1_rmse: 0.54503 |  0:00:04s
epoch 9  | loss: 0.30934 | val_0_rmse: 0.52274 | val_1_rmse: 0.53054 |  0:00:04s
epoch 10 | loss: 0.30255 | val_0_rmse: 0.52077 | val_1_rmse: 0.52947 |  0:00:04s
epoch 11 | loss: 0.29741 | val_0_rmse: 0.51584 | val_1_rmse: 0.52389 |  0:00:05s
epoch 12 | loss: 0.29624 | val_0_rmse: 0.51088 | val_1_rmse: 0.51703 |  0:00:05s
epoch 13 | loss: 0.29321 | val_0_rmse: 0.52658 | val_1_rmse: 0.53728 |  0:00:06s
epoch 14 | loss: 0.29517 | val_0_rmse: 0.51214 | val_1_rmse: 0.52451 |  0:00:06s
epoch 15 | loss: 0.29637 | val_0_rmse: 0.51921 | val_1_rmse: 0.53299 |  0:00:07s
epoch 16 | loss: 0.29071 | val_0_rmse: 0.513   | val_1_rmse: 0.52704 |  0:00:07s
epoch 17 | loss: 0.28937 | val_0_rmse: 0.51795 | val_1_rmse: 0.53027 |  0:00:08s
epoch 18 | loss: 0.3     | val_0_rmse: 0.50266 | val_1_rmse: 0.5166  |  0:00:08s
epoch 19 | loss: 0.27884 | val_0_rmse: 0.51602 | val_1_rmse: 0.53074 |  0:00:09s
epoch 20 | loss: 0.29616 | val_0_rmse: 0.50487 | val_1_rmse: 0.51755 |  0:00:09s
epoch 21 | loss: 0.27571 | val_0_rmse: 0.50329 | val_1_rmse: 0.52148 |  0:00:09s
epoch 22 | loss: 0.27951 | val_0_rmse: 0.51042 | val_1_rmse: 0.52359 |  0:00:10s
epoch 23 | loss: 0.27996 | val_0_rmse: 0.49628 | val_1_rmse: 0.51393 |  0:00:10s
epoch 24 | loss: 0.28115 | val_0_rmse: 0.50031 | val_1_rmse: 0.51572 |  0:00:11s
epoch 25 | loss: 0.26361 | val_0_rmse: 0.49897 | val_1_rmse: 0.52114 |  0:00:11s
epoch 26 | loss: 0.27135 | val_0_rmse: 0.50417 | val_1_rmse: 0.52366 |  0:00:12s
epoch 27 | loss: 0.26905 | val_0_rmse: 0.49322 | val_1_rmse: 0.50901 |  0:00:12s
epoch 28 | loss: 0.27326 | val_0_rmse: 0.49989 | val_1_rmse: 0.51534 |  0:00:13s
epoch 29 | loss: 0.26402 | val_0_rmse: 0.4941  | val_1_rmse: 0.51335 |  0:00:13s
epoch 30 | loss: 0.26354 | val_0_rmse: 0.4854  | val_1_rmse: 0.50582 |  0:00:13s
epoch 31 | loss: 0.25814 | val_0_rmse: 0.48627 | val_1_rmse: 0.50973 |  0:00:14s
epoch 32 | loss: 0.25455 | val_0_rmse: 0.48649 | val_1_rmse: 0.50982 |  0:00:14s
epoch 33 | loss: 0.25969 | val_0_rmse: 0.48331 | val_1_rmse: 0.50391 |  0:00:15s
epoch 34 | loss: 0.26291 | val_0_rmse: 0.5072  | val_1_rmse: 0.52793 |  0:00:15s
epoch 35 | loss: 0.27218 | val_0_rmse: 0.51117 | val_1_rmse: 0.53302 |  0:00:16s
epoch 36 | loss: 0.27213 | val_0_rmse: 0.48229 | val_1_rmse: 0.50815 |  0:00:16s
epoch 37 | loss: 0.25878 | val_0_rmse: 0.49136 | val_1_rmse: 0.51427 |  0:00:17s
epoch 38 | loss: 0.2572  | val_0_rmse: 0.48444 | val_1_rmse: 0.50594 |  0:00:17s
epoch 39 | loss: 0.25971 | val_0_rmse: 0.48662 | val_1_rmse: 0.5041  |  0:00:18s
epoch 40 | loss: 0.26292 | val_0_rmse: 0.48809 | val_1_rmse: 0.50121 |  0:00:18s
epoch 41 | loss: 0.25882 | val_0_rmse: 0.48542 | val_1_rmse: 0.50687 |  0:00:18s
epoch 42 | loss: 0.25935 | val_0_rmse: 0.49111 | val_1_rmse: 0.5116  |  0:00:19s
epoch 43 | loss: 0.26551 | val_0_rmse: 0.48818 | val_1_rmse: 0.51213 |  0:00:19s
epoch 44 | loss: 0.26089 | val_0_rmse: 0.48724 | val_1_rmse: 0.51432 |  0:00:20s
epoch 45 | loss: 0.25089 | val_0_rmse: 0.48566 | val_1_rmse: 0.50492 |  0:00:20s
epoch 46 | loss: 0.25482 | val_0_rmse: 0.48007 | val_1_rmse: 0.50518 |  0:00:21s
epoch 47 | loss: 0.25245 | val_0_rmse: 0.48641 | val_1_rmse: 0.519   |  0:00:21s
epoch 48 | loss: 0.25173 | val_0_rmse: 0.48163 | val_1_rmse: 0.50115 |  0:00:22s
epoch 49 | loss: 0.25879 | val_0_rmse: 0.49623 | val_1_rmse: 0.52004 |  0:00:22s
epoch 50 | loss: 0.25369 | val_0_rmse: 0.48719 | val_1_rmse: 0.50763 |  0:00:22s
epoch 51 | loss: 0.2581  | val_0_rmse: 0.49331 | val_1_rmse: 0.51805 |  0:00:23s
epoch 52 | loss: 0.25323 | val_0_rmse: 0.48367 | val_1_rmse: 0.50329 |  0:00:23s
epoch 53 | loss: 0.25378 | val_0_rmse: 0.48881 | val_1_rmse: 0.51234 |  0:00:24s
epoch 54 | loss: 0.26415 | val_0_rmse: 0.47451 | val_1_rmse: 0.49812 |  0:00:24s
epoch 55 | loss: 0.25868 | val_0_rmse: 0.49802 | val_1_rmse: 0.53005 |  0:00:25s
epoch 56 | loss: 0.25092 | val_0_rmse: 0.48465 | val_1_rmse: 0.50905 |  0:00:25s
epoch 57 | loss: 0.26421 | val_0_rmse: 0.47406 | val_1_rmse: 0.49838 |  0:00:26s
epoch 58 | loss: 0.25395 | val_0_rmse: 0.48509 | val_1_rmse: 0.51119 |  0:00:26s
epoch 59 | loss: 0.24114 | val_0_rmse: 0.47189 | val_1_rmse: 0.49958 |  0:00:26s
epoch 60 | loss: 0.25308 | val_0_rmse: 0.48009 | val_1_rmse: 0.50292 |  0:00:27s
epoch 61 | loss: 0.25163 | val_0_rmse: 0.47545 | val_1_rmse: 0.49688 |  0:00:27s
epoch 62 | loss: 0.24963 | val_0_rmse: 0.47836 | val_1_rmse: 0.50503 |  0:00:28s
epoch 63 | loss: 0.25571 | val_0_rmse: 0.49829 | val_1_rmse: 0.52586 |  0:00:28s
epoch 64 | loss: 0.26396 | val_0_rmse: 0.48598 | val_1_rmse: 0.51065 |  0:00:29s
epoch 65 | loss: 0.25857 | val_0_rmse: 0.47974 | val_1_rmse: 0.51358 |  0:00:29s
epoch 66 | loss: 0.24337 | val_0_rmse: 0.47381 | val_1_rmse: 0.50283 |  0:00:30s
epoch 67 | loss: 0.25106 | val_0_rmse: 0.47102 | val_1_rmse: 0.50094 |  0:00:30s
epoch 68 | loss: 0.23957 | val_0_rmse: 0.46857 | val_1_rmse: 0.49507 |  0:00:30s
epoch 69 | loss: 0.238   | val_0_rmse: 0.46898 | val_1_rmse: 0.50085 |  0:00:31s
epoch 70 | loss: 0.23991 | val_0_rmse: 0.48333 | val_1_rmse: 0.50702 |  0:00:31s
epoch 71 | loss: 0.25121 | val_0_rmse: 0.4696  | val_1_rmse: 0.50615 |  0:00:32s
epoch 72 | loss: 0.24915 | val_0_rmse: 0.4772  | val_1_rmse: 0.50517 |  0:00:32s
epoch 73 | loss: 0.24497 | val_0_rmse: 0.4678  | val_1_rmse: 0.499   |  0:00:33s
epoch 74 | loss: 0.24468 | val_0_rmse: 0.49577 | val_1_rmse: 0.52074 |  0:00:33s
epoch 75 | loss: 0.25177 | val_0_rmse: 0.46562 | val_1_rmse: 0.49347 |  0:00:34s
epoch 76 | loss: 0.24066 | val_0_rmse: 0.47441 | val_1_rmse: 0.50665 |  0:00:34s
epoch 77 | loss: 0.24361 | val_0_rmse: 0.47133 | val_1_rmse: 0.50668 |  0:00:35s
epoch 78 | loss: 0.24404 | val_0_rmse: 0.48954 | val_1_rmse: 0.52108 |  0:00:35s
epoch 79 | loss: 0.24398 | val_0_rmse: 0.4758  | val_1_rmse: 0.50406 |  0:00:35s
epoch 80 | loss: 0.24624 | val_0_rmse: 0.47406 | val_1_rmse: 0.50521 |  0:00:36s
epoch 81 | loss: 0.23782 | val_0_rmse: 0.47308 | val_1_rmse: 0.50272 |  0:00:36s
epoch 82 | loss: 0.24339 | val_0_rmse: 0.46631 | val_1_rmse: 0.49645 |  0:00:37s
epoch 83 | loss: 0.2442  | val_0_rmse: 0.47038 | val_1_rmse: 0.50103 |  0:00:37s
epoch 84 | loss: 0.24597 | val_0_rmse: 0.48399 | val_1_rmse: 0.51625 |  0:00:38s
epoch 85 | loss: 0.24042 | val_0_rmse: 0.47314 | val_1_rmse: 0.50502 |  0:00:38s
epoch 86 | loss: 0.24288 | val_0_rmse: 0.46696 | val_1_rmse: 0.50234 |  0:00:39s
epoch 87 | loss: 0.24225 | val_0_rmse: 0.46357 | val_1_rmse: 0.49154 |  0:00:39s
epoch 88 | loss: 0.23874 | val_0_rmse: 0.4624  | val_1_rmse: 0.50115 |  0:00:39s
epoch 89 | loss: 0.23681 | val_0_rmse: 0.46921 | val_1_rmse: 0.50099 |  0:00:40s
epoch 90 | loss: 0.23879 | val_0_rmse: 0.47231 | val_1_rmse: 0.50656 |  0:00:40s
epoch 91 | loss: 0.23697 | val_0_rmse: 0.45801 | val_1_rmse: 0.49502 |  0:00:41s
epoch 92 | loss: 0.2363  | val_0_rmse: 0.46612 | val_1_rmse: 0.49953 |  0:00:41s
epoch 93 | loss: 0.23543 | val_0_rmse: 0.45813 | val_1_rmse: 0.49707 |  0:00:42s
epoch 94 | loss: 0.23671 | val_0_rmse: 0.47382 | val_1_rmse: 0.50851 |  0:00:42s
epoch 95 | loss: 0.24484 | val_0_rmse: 0.48829 | val_1_rmse: 0.52787 |  0:00:43s
epoch 96 | loss: 0.25168 | val_0_rmse: 0.48669 | val_1_rmse: 0.52164 |  0:00:43s
epoch 97 | loss: 0.24263 | val_0_rmse: 0.4618  | val_1_rmse: 0.49495 |  0:00:43s
epoch 98 | loss: 0.23931 | val_0_rmse: 0.4687  | val_1_rmse: 0.49995 |  0:00:44s
epoch 99 | loss: 0.23964 | val_0_rmse: 0.46719 | val_1_rmse: 0.50036 |  0:00:44s
epoch 100| loss: 0.24509 | val_0_rmse: 0.47855 | val_1_rmse: 0.51991 |  0:00:45s
epoch 101| loss: 0.24145 | val_0_rmse: 0.47419 | val_1_rmse: 0.5118  |  0:00:45s
epoch 102| loss: 0.23998 | val_0_rmse: 0.46153 | val_1_rmse: 0.49669 |  0:00:46s
epoch 103| loss: 0.2381  | val_0_rmse: 0.46001 | val_1_rmse: 0.4988  |  0:00:46s
epoch 104| loss: 0.23138 | val_0_rmse: 0.46295 | val_1_rmse: 0.50399 |  0:00:47s
epoch 105| loss: 0.23328 | val_0_rmse: 0.47074 | val_1_rmse: 0.5133  |  0:00:47s
epoch 106| loss: 0.23502 | val_0_rmse: 0.4628  | val_1_rmse: 0.50158 |  0:00:48s
epoch 107| loss: 0.23508 | val_0_rmse: 0.46187 | val_1_rmse: 0.49801 |  0:00:48s
epoch 108| loss: 0.2361  | val_0_rmse: 0.47364 | val_1_rmse: 0.51728 |  0:00:48s
epoch 109| loss: 0.23289 | val_0_rmse: 0.46384 | val_1_rmse: 0.50755 |  0:00:49s
epoch 110| loss: 0.24902 | val_0_rmse: 0.45838 | val_1_rmse: 0.50004 |  0:00:49s
epoch 111| loss: 0.25347 | val_0_rmse: 0.4835  | val_1_rmse: 0.53351 |  0:00:50s
epoch 112| loss: 0.24295 | val_0_rmse: 0.47516 | val_1_rmse: 0.51952 |  0:00:50s
epoch 113| loss: 0.2378  | val_0_rmse: 0.4582  | val_1_rmse: 0.504   |  0:00:51s
epoch 114| loss: 0.24264 | val_0_rmse: 0.46411 | val_1_rmse: 0.50992 |  0:00:51s
epoch 115| loss: 0.23974 | val_0_rmse: 0.48355 | val_1_rmse: 0.52719 |  0:00:51s
epoch 116| loss: 0.24524 | val_0_rmse: 0.46946 | val_1_rmse: 0.51609 |  0:00:52s
epoch 117| loss: 0.24476 | val_0_rmse: 0.48613 | val_1_rmse: 0.52964 |  0:00:52s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.49154
Best weights from best epoch are automatically used!
ended training at: 08:38:22
Feature importance:
[('Area', 0.33526638310218), ('Baths', 0.024170695011047655), ('Beds', 0.1006501549070572), ('Latitude', 0.14309638201347646), ('Longitude', 0.2511270677313953), ('Month', 0.03289212426821546), ('Year', 0.11279719296662793)]
Mean squared error is of 20300114915.597595
Mean absolute error:101839.01102865404
MAPE:0.17667015502420072
R2 score:0.7471443549294338
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:38:22
epoch 0  | loss: 1.58194 | val_0_rmse: 1.32743 | val_1_rmse: 1.29424 |  0:00:00s
epoch 1  | loss: 0.89222 | val_0_rmse: 1.02495 | val_1_rmse: 1.02097 |  0:00:00s
epoch 2  | loss: 0.6599  | val_0_rmse: 0.91841 | val_1_rmse: 0.92639 |  0:00:00s
epoch 3  | loss: 0.5581  | val_0_rmse: 0.80861 | val_1_rmse: 0.77061 |  0:00:00s
epoch 4  | loss: 0.53618 | val_0_rmse: 0.85135 | val_1_rmse: 0.87924 |  0:00:00s
epoch 5  | loss: 0.50034 | val_0_rmse: 0.80347 | val_1_rmse: 0.85723 |  0:00:00s
epoch 6  | loss: 0.48421 | val_0_rmse: 0.77335 | val_1_rmse: 0.8173  |  0:00:00s
epoch 7  | loss: 0.47346 | val_0_rmse: 0.757   | val_1_rmse: 0.80742 |  0:00:01s
epoch 8  | loss: 0.45562 | val_0_rmse: 0.72993 | val_1_rmse: 0.77672 |  0:00:01s
epoch 9  | loss: 0.4538  | val_0_rmse: 0.70553 | val_1_rmse: 0.67075 |  0:00:01s
epoch 10 | loss: 0.45018 | val_0_rmse: 0.68848 | val_1_rmse: 0.65054 |  0:00:01s
epoch 11 | loss: 0.46279 | val_0_rmse: 0.67365 | val_1_rmse: 0.64319 |  0:00:01s
epoch 12 | loss: 0.45332 | val_0_rmse: 0.68958 | val_1_rmse: 0.64666 |  0:00:01s
epoch 13 | loss: 0.4497  | val_0_rmse: 0.69796 | val_1_rmse: 0.64925 |  0:00:01s
epoch 14 | loss: 0.44316 | val_0_rmse: 0.67503 | val_1_rmse: 0.63673 |  0:00:01s
epoch 15 | loss: 0.43893 | val_0_rmse: 0.65888 | val_1_rmse: 0.62359 |  0:00:02s
epoch 16 | loss: 0.43406 | val_0_rmse: 0.65398 | val_1_rmse: 0.62726 |  0:00:02s
epoch 17 | loss: 0.42301 | val_0_rmse: 0.64393 | val_1_rmse: 0.62648 |  0:00:02s
epoch 18 | loss: 0.42888 | val_0_rmse: 0.65081 | val_1_rmse: 0.63181 |  0:00:02s
epoch 19 | loss: 0.41913 | val_0_rmse: 0.64681 | val_1_rmse: 0.63189 |  0:00:02s
epoch 20 | loss: 0.42696 | val_0_rmse: 0.64024 | val_1_rmse: 0.63204 |  0:00:02s
epoch 21 | loss: 0.41116 | val_0_rmse: 0.64274 | val_1_rmse: 0.63608 |  0:00:02s
epoch 22 | loss: 0.41864 | val_0_rmse: 0.63832 | val_1_rmse: 0.64108 |  0:00:03s
epoch 23 | loss: 0.42987 | val_0_rmse: 0.64148 | val_1_rmse: 0.64402 |  0:00:03s
epoch 24 | loss: 0.41761 | val_0_rmse: 0.63562 | val_1_rmse: 0.63002 |  0:00:03s
epoch 25 | loss: 0.41468 | val_0_rmse: 0.64386 | val_1_rmse: 0.62684 |  0:00:03s
epoch 26 | loss: 0.41474 | val_0_rmse: 0.64456 | val_1_rmse: 0.6311  |  0:00:03s
epoch 27 | loss: 0.42446 | val_0_rmse: 0.63458 | val_1_rmse: 0.61721 |  0:00:03s
epoch 28 | loss: 0.42547 | val_0_rmse: 0.64298 | val_1_rmse: 0.61759 |  0:00:03s
epoch 29 | loss: 0.41846 | val_0_rmse: 0.63709 | val_1_rmse: 0.62375 |  0:00:03s
epoch 30 | loss: 0.42058 | val_0_rmse: 0.63607 | val_1_rmse: 0.62299 |  0:00:04s
epoch 31 | loss: 0.40818 | val_0_rmse: 0.64268 | val_1_rmse: 0.62023 |  0:00:04s
epoch 32 | loss: 0.42376 | val_0_rmse: 0.63634 | val_1_rmse: 0.61447 |  0:00:04s
epoch 33 | loss: 0.41147 | val_0_rmse: 0.64137 | val_1_rmse: 0.61949 |  0:00:04s
epoch 34 | loss: 0.40629 | val_0_rmse: 0.64201 | val_1_rmse: 0.61873 |  0:00:04s
epoch 35 | loss: 0.41352 | val_0_rmse: 0.64794 | val_1_rmse: 0.61532 |  0:00:04s
epoch 36 | loss: 0.43156 | val_0_rmse: 0.64235 | val_1_rmse: 0.6129  |  0:00:04s
epoch 37 | loss: 0.42135 | val_0_rmse: 0.63763 | val_1_rmse: 0.61357 |  0:00:04s
epoch 38 | loss: 0.42598 | val_0_rmse: 0.65163 | val_1_rmse: 0.62608 |  0:00:05s
epoch 39 | loss: 0.4332  | val_0_rmse: 0.64804 | val_1_rmse: 0.62723 |  0:00:05s
epoch 40 | loss: 0.42719 | val_0_rmse: 0.64284 | val_1_rmse: 0.61998 |  0:00:05s
epoch 41 | loss: 0.42454 | val_0_rmse: 0.63997 | val_1_rmse: 0.6109  |  0:00:05s
epoch 42 | loss: 0.42007 | val_0_rmse: 0.65073 | val_1_rmse: 0.6231  |  0:00:05s
epoch 43 | loss: 0.42663 | val_0_rmse: 0.64773 | val_1_rmse: 0.62752 |  0:00:05s
epoch 44 | loss: 0.42625 | val_0_rmse: 0.64055 | val_1_rmse: 0.61708 |  0:00:05s
epoch 45 | loss: 0.42628 | val_0_rmse: 0.63701 | val_1_rmse: 0.61589 |  0:00:06s
epoch 46 | loss: 0.41027 | val_0_rmse: 0.64311 | val_1_rmse: 0.63809 |  0:00:06s
epoch 47 | loss: 0.41348 | val_0_rmse: 0.64246 | val_1_rmse: 0.63503 |  0:00:06s
epoch 48 | loss: 0.4075  | val_0_rmse: 0.63855 | val_1_rmse: 0.62988 |  0:00:06s
epoch 49 | loss: 0.41183 | val_0_rmse: 0.63408 | val_1_rmse: 0.62164 |  0:00:06s
epoch 50 | loss: 0.41402 | val_0_rmse: 0.62981 | val_1_rmse: 0.62078 |  0:00:06s
epoch 51 | loss: 0.41571 | val_0_rmse: 0.64582 | val_1_rmse: 0.64544 |  0:00:06s
epoch 52 | loss: 0.41376 | val_0_rmse: 0.62954 | val_1_rmse: 0.61703 |  0:00:06s
epoch 53 | loss: 0.40222 | val_0_rmse: 0.69535 | val_1_rmse: 0.62322 |  0:00:07s
epoch 54 | loss: 0.40577 | val_0_rmse: 0.65118 | val_1_rmse: 0.62881 |  0:00:07s
epoch 55 | loss: 0.41024 | val_0_rmse: 0.62716 | val_1_rmse: 0.61726 |  0:00:07s
epoch 56 | loss: 0.39256 | val_0_rmse: 0.62252 | val_1_rmse: 0.61685 |  0:00:07s
epoch 57 | loss: 0.4016  | val_0_rmse: 0.62497 | val_1_rmse: 0.61159 |  0:00:07s
epoch 58 | loss: 0.4031  | val_0_rmse: 0.62802 | val_1_rmse: 0.61107 |  0:00:07s
epoch 59 | loss: 0.38527 | val_0_rmse: 0.62326 | val_1_rmse: 0.61644 |  0:00:07s
epoch 60 | loss: 0.38268 | val_0_rmse: 0.62286 | val_1_rmse: 0.61646 |  0:00:08s
epoch 61 | loss: 0.38903 | val_0_rmse: 0.63242 | val_1_rmse: 0.62634 |  0:00:08s
epoch 62 | loss: 0.38743 | val_0_rmse: 0.63668 | val_1_rmse: 0.62718 |  0:00:08s
epoch 63 | loss: 0.3901  | val_0_rmse: 0.64038 | val_1_rmse: 0.62925 |  0:00:08s
epoch 64 | loss: 0.39366 | val_0_rmse: 0.63502 | val_1_rmse: 0.6289  |  0:00:08s
epoch 65 | loss: 0.39256 | val_0_rmse: 0.62794 | val_1_rmse: 0.62174 |  0:00:08s
epoch 66 | loss: 0.38374 | val_0_rmse: 0.64031 | val_1_rmse: 0.63946 |  0:00:08s
epoch 67 | loss: 0.39685 | val_0_rmse: 0.68607 | val_1_rmse: 0.6614  |  0:00:08s
epoch 68 | loss: 0.39548 | val_0_rmse: 0.63784 | val_1_rmse: 0.62819 |  0:00:09s
epoch 69 | loss: 0.37539 | val_0_rmse: 0.60404 | val_1_rmse: 0.60987 |  0:00:09s
epoch 70 | loss: 0.35866 | val_0_rmse: 0.71843 | val_1_rmse: 0.73836 |  0:00:09s
epoch 71 | loss: 0.365   | val_0_rmse: 0.81591 | val_1_rmse: 0.84424 |  0:00:09s
epoch 72 | loss: 0.36859 | val_0_rmse: 0.77893 | val_1_rmse: 0.80611 |  0:00:09s
epoch 73 | loss: 0.35594 | val_0_rmse: 0.64306 | val_1_rmse: 0.65503 |  0:00:09s
epoch 74 | loss: 0.35621 | val_0_rmse: 0.61061 | val_1_rmse: 0.61211 |  0:00:09s
epoch 75 | loss: 0.36859 | val_0_rmse: 0.6001  | val_1_rmse: 0.60417 |  0:00:09s
epoch 76 | loss: 0.36368 | val_0_rmse: 0.60935 | val_1_rmse: 0.60833 |  0:00:10s
epoch 77 | loss: 0.364   | val_0_rmse: 0.63877 | val_1_rmse: 0.63982 |  0:00:10s
epoch 78 | loss: 0.36365 | val_0_rmse: 0.63033 | val_1_rmse: 0.63028 |  0:00:10s
epoch 79 | loss: 0.34784 | val_0_rmse: 0.58244 | val_1_rmse: 0.57588 |  0:00:10s
epoch 80 | loss: 0.3424  | val_0_rmse: 0.58774 | val_1_rmse: 0.58568 |  0:00:10s
epoch 81 | loss: 0.35196 | val_0_rmse: 0.58625 | val_1_rmse: 0.58584 |  0:00:10s
epoch 82 | loss: 0.34837 | val_0_rmse: 0.58986 | val_1_rmse: 0.59143 |  0:00:10s
epoch 83 | loss: 0.34186 | val_0_rmse: 0.58045 | val_1_rmse: 0.58499 |  0:00:10s
epoch 84 | loss: 0.3559  | val_0_rmse: 0.5802  | val_1_rmse: 0.58566 |  0:00:11s
epoch 85 | loss: 0.33667 | val_0_rmse: 0.6922  | val_1_rmse: 0.70177 |  0:00:11s
epoch 86 | loss: 0.34446 | val_0_rmse: 0.70072 | val_1_rmse: 0.70789 |  0:00:11s
epoch 87 | loss: 0.35138 | val_0_rmse: 0.57858 | val_1_rmse: 0.5846  |  0:00:11s
epoch 88 | loss: 0.33547 | val_0_rmse: 0.57431 | val_1_rmse: 0.57842 |  0:00:11s
epoch 89 | loss: 0.33135 | val_0_rmse: 0.58189 | val_1_rmse: 0.58712 |  0:00:11s
epoch 90 | loss: 0.33612 | val_0_rmse: 0.59272 | val_1_rmse: 0.59868 |  0:00:11s
epoch 91 | loss: 0.32915 | val_0_rmse: 0.58498 | val_1_rmse: 0.58907 |  0:00:12s
epoch 92 | loss: 0.33416 | val_0_rmse: 0.57906 | val_1_rmse: 0.57932 |  0:00:12s
epoch 93 | loss: 0.33052 | val_0_rmse: 0.59021 | val_1_rmse: 0.58828 |  0:00:12s
epoch 94 | loss: 0.33723 | val_0_rmse: 0.63737 | val_1_rmse: 0.63926 |  0:00:12s
epoch 95 | loss: 0.32109 | val_0_rmse: 0.63599 | val_1_rmse: 0.64578 |  0:00:12s
epoch 96 | loss: 0.31968 | val_0_rmse: 0.62047 | val_1_rmse: 0.62335 |  0:00:12s
epoch 97 | loss: 0.31781 | val_0_rmse: 0.58525 | val_1_rmse: 0.58687 |  0:00:12s
epoch 98 | loss: 0.3152  | val_0_rmse: 0.57188 | val_1_rmse: 0.57138 |  0:00:12s
epoch 99 | loss: 0.31634 | val_0_rmse: 0.63021 | val_1_rmse: 0.63431 |  0:00:13s
epoch 100| loss: 0.31278 | val_0_rmse: 0.58368 | val_1_rmse: 0.58775 |  0:00:13s
epoch 101| loss: 0.32032 | val_0_rmse: 0.55623 | val_1_rmse: 0.55454 |  0:00:13s
epoch 102| loss: 0.31003 | val_0_rmse: 0.54929 | val_1_rmse: 0.54744 |  0:00:13s
epoch 103| loss: 0.31765 | val_0_rmse: 0.55187 | val_1_rmse: 0.55691 |  0:00:13s
epoch 104| loss: 0.30301 | val_0_rmse: 0.55139 | val_1_rmse: 0.56306 |  0:00:13s
epoch 105| loss: 0.30351 | val_0_rmse: 0.56534 | val_1_rmse: 0.57803 |  0:00:13s
epoch 106| loss: 0.30975 | val_0_rmse: 0.55431 | val_1_rmse: 0.56737 |  0:00:14s
epoch 107| loss: 0.31846 | val_0_rmse: 0.57991 | val_1_rmse: 0.58696 |  0:00:14s
epoch 108| loss: 0.3323  | val_0_rmse: 0.62273 | val_1_rmse: 0.62861 |  0:00:14s
epoch 109| loss: 0.3071  | val_0_rmse: 0.55265 | val_1_rmse: 0.55369 |  0:00:14s
epoch 110| loss: 0.30418 | val_0_rmse: 0.57514 | val_1_rmse: 0.57215 |  0:00:14s
epoch 111| loss: 0.31997 | val_0_rmse: 0.5605  | val_1_rmse: 0.55439 |  0:00:14s
epoch 112| loss: 0.32089 | val_0_rmse: 0.56027 | val_1_rmse: 0.55375 |  0:00:14s
epoch 113| loss: 0.32753 | val_0_rmse: 0.54659 | val_1_rmse: 0.54207 |  0:00:14s
epoch 114| loss: 0.30093 | val_0_rmse: 0.5832  | val_1_rmse: 0.57406 |  0:00:15s
epoch 115| loss: 0.30965 | val_0_rmse: 0.5779  | val_1_rmse: 0.57964 |  0:00:15s
epoch 116| loss: 0.31818 | val_0_rmse: 0.54507 | val_1_rmse: 0.5458  |  0:00:15s
epoch 117| loss: 0.30638 | val_0_rmse: 0.62848 | val_1_rmse: 0.63106 |  0:00:15s
epoch 118| loss: 0.32746 | val_0_rmse: 0.56542 | val_1_rmse: 0.57128 |  0:00:15s
epoch 119| loss: 0.31003 | val_0_rmse: 0.61225 | val_1_rmse: 0.5988  |  0:00:15s
epoch 120| loss: 0.31932 | val_0_rmse: 0.58823 | val_1_rmse: 0.57214 |  0:00:15s
epoch 121| loss: 0.32612 | val_0_rmse: 0.55037 | val_1_rmse: 0.53552 |  0:00:15s
epoch 122| loss: 0.30836 | val_0_rmse: 0.5383  | val_1_rmse: 0.52809 |  0:00:16s
epoch 123| loss: 0.32444 | val_0_rmse: 0.58198 | val_1_rmse: 0.57527 |  0:00:16s
epoch 124| loss: 0.30194 | val_0_rmse: 0.62299 | val_1_rmse: 0.62269 |  0:00:16s
epoch 125| loss: 0.31335 | val_0_rmse: 0.55911 | val_1_rmse: 0.55321 |  0:00:16s
epoch 126| loss: 0.30056 | val_0_rmse: 0.55134 | val_1_rmse: 0.55166 |  0:00:16s
epoch 127| loss: 0.31336 | val_0_rmse: 0.6056  | val_1_rmse: 0.6157  |  0:00:16s
epoch 128| loss: 0.30584 | val_0_rmse: 0.63942 | val_1_rmse: 0.65679 |  0:00:16s
epoch 129| loss: 0.30184 | val_0_rmse: 0.60885 | val_1_rmse: 0.62832 |  0:00:17s
epoch 130| loss: 0.29741 | val_0_rmse: 0.6152  | val_1_rmse: 0.63638 |  0:00:17s
epoch 131| loss: 0.3044  | val_0_rmse: 0.75222 | val_1_rmse: 0.77647 |  0:00:17s
epoch 132| loss: 0.30287 | val_0_rmse: 0.85001 | val_1_rmse: 0.86449 |  0:00:17s
epoch 133| loss: 0.30537 | val_0_rmse: 0.75714 | val_1_rmse: 0.78242 |  0:00:17s
epoch 134| loss: 0.30887 | val_0_rmse: 0.70531 | val_1_rmse: 0.72991 |  0:00:17s
epoch 135| loss: 0.29473 | val_0_rmse: 0.68549 | val_1_rmse: 0.70815 |  0:00:17s
epoch 136| loss: 0.2892  | val_0_rmse: 0.62965 | val_1_rmse: 0.64165 |  0:00:17s
epoch 137| loss: 0.29198 | val_0_rmse: 0.59835 | val_1_rmse: 0.60117 |  0:00:18s
epoch 138| loss: 0.31336 | val_0_rmse: 0.56559 | val_1_rmse: 0.56635 |  0:00:18s
epoch 139| loss: 0.29976 | val_0_rmse: 0.57922 | val_1_rmse: 0.58512 |  0:00:18s
epoch 140| loss: 0.31193 | val_0_rmse: 0.59845 | val_1_rmse: 0.60377 |  0:00:18s
epoch 141| loss: 0.29574 | val_0_rmse: 0.53893 | val_1_rmse: 0.52671 |  0:00:18s
epoch 142| loss: 0.29218 | val_0_rmse: 0.58638 | val_1_rmse: 0.57319 |  0:00:18s
epoch 143| loss: 0.29253 | val_0_rmse: 0.64822 | val_1_rmse: 0.64498 |  0:00:18s
epoch 144| loss: 0.28225 | val_0_rmse: 0.64799 | val_1_rmse: 0.65301 |  0:00:18s
epoch 145| loss: 0.29825 | val_0_rmse: 0.65931 | val_1_rmse: 0.66055 |  0:00:19s
epoch 146| loss: 0.28403 | val_0_rmse: 0.59053 | val_1_rmse: 0.59215 |  0:00:19s
epoch 147| loss: 0.28446 | val_0_rmse: 0.52919 | val_1_rmse: 0.54462 |  0:00:19s
epoch 148| loss: 0.28945 | val_0_rmse: 0.52177 | val_1_rmse: 0.53876 |  0:00:19s
epoch 149| loss: 0.2807  | val_0_rmse: 0.52673 | val_1_rmse: 0.54027 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.52671
Best weights from best epoch are automatically used!
ended training at: 08:38:42
Feature importance:
[('Area', 0.2793594329761155), ('Baths', 0.17603935104035476), ('Beds', 0.0), ('Latitude', 0.48970893359327905), ('Longitude', 0.03476883702523221), ('Month', 0.0008221161817871214), ('Year', 0.019301329183231387)]
Mean squared error is of 2703020145.96623
Mean absolute error:36447.35616401099
MAPE:0.3000323890644014
R2 score:0.6554171606061245
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:38:42
epoch 0  | loss: 1.57161 | val_0_rmse: 1.33639 | val_1_rmse: 1.09242 |  0:00:00s
epoch 1  | loss: 1.0895  | val_0_rmse: 1.50639 | val_1_rmse: 1.11381 |  0:00:00s
epoch 2  | loss: 0.77206 | val_0_rmse: 0.99432 | val_1_rmse: 0.96154 |  0:00:00s
epoch 3  | loss: 0.65015 | val_0_rmse: 0.96565 | val_1_rmse: 0.95842 |  0:00:00s
epoch 4  | loss: 0.59377 | val_0_rmse: 1.3784  | val_1_rmse: 0.9349  |  0:00:00s
epoch 5  | loss: 0.54835 | val_0_rmse: 1.21462 | val_1_rmse: 0.8835  |  0:00:00s
epoch 6  | loss: 0.505   | val_0_rmse: 1.05095 | val_1_rmse: 0.85287 |  0:00:00s
epoch 7  | loss: 0.49352 | val_0_rmse: 0.7873  | val_1_rmse: 0.79632 |  0:00:01s
epoch 8  | loss: 0.46417 | val_0_rmse: 0.69419 | val_1_rmse: 0.77905 |  0:00:01s
epoch 9  | loss: 0.45579 | val_0_rmse: 0.70275 | val_1_rmse: 0.78102 |  0:00:01s
epoch 10 | loss: 0.46406 | val_0_rmse: 0.68662 | val_1_rmse: 0.7603  |  0:00:01s
epoch 11 | loss: 0.44995 | val_0_rmse: 0.68071 | val_1_rmse: 0.75758 |  0:00:01s
epoch 12 | loss: 0.44561 | val_0_rmse: 0.69345 | val_1_rmse: 0.7804  |  0:00:01s
epoch 13 | loss: 0.43421 | val_0_rmse: 0.68622 | val_1_rmse: 0.76444 |  0:00:01s
epoch 14 | loss: 0.43725 | val_0_rmse: 0.6689  | val_1_rmse: 0.74424 |  0:00:01s
epoch 15 | loss: 0.42959 | val_0_rmse: 0.67387 | val_1_rmse: 0.74764 |  0:00:02s
epoch 16 | loss: 0.43032 | val_0_rmse: 0.67894 | val_1_rmse: 0.76307 |  0:00:02s
epoch 17 | loss: 0.43643 | val_0_rmse: 0.67104 | val_1_rmse: 0.7535  |  0:00:02s
epoch 18 | loss: 0.43368 | val_0_rmse: 0.65469 | val_1_rmse: 0.73522 |  0:00:02s
epoch 19 | loss: 0.42969 | val_0_rmse: 0.66826 | val_1_rmse: 0.75294 |  0:00:02s
epoch 20 | loss: 0.42721 | val_0_rmse: 0.65322 | val_1_rmse: 0.73073 |  0:00:02s
epoch 21 | loss: 0.41564 | val_0_rmse: 0.67268 | val_1_rmse: 0.74983 |  0:00:02s
epoch 22 | loss: 0.42741 | val_0_rmse: 0.64915 | val_1_rmse: 0.72368 |  0:00:03s
epoch 23 | loss: 0.41102 | val_0_rmse: 0.63649 | val_1_rmse: 0.70935 |  0:00:03s
epoch 24 | loss: 0.42004 | val_0_rmse: 0.63341 | val_1_rmse: 0.70995 |  0:00:03s
epoch 25 | loss: 0.42166 | val_0_rmse: 0.62779 | val_1_rmse: 0.7109  |  0:00:03s
epoch 26 | loss: 0.40257 | val_0_rmse: 0.64031 | val_1_rmse: 0.73077 |  0:00:03s
epoch 27 | loss: 0.40876 | val_0_rmse: 0.62812 | val_1_rmse: 0.72355 |  0:00:03s
epoch 28 | loss: 0.40452 | val_0_rmse: 0.63062 | val_1_rmse: 0.72894 |  0:00:03s
epoch 29 | loss: 0.4173  | val_0_rmse: 0.62889 | val_1_rmse: 0.72885 |  0:00:03s
epoch 30 | loss: 0.40989 | val_0_rmse: 0.62404 | val_1_rmse: 0.72322 |  0:00:04s
epoch 31 | loss: 0.39842 | val_0_rmse: 0.62415 | val_1_rmse: 0.72115 |  0:00:04s
epoch 32 | loss: 0.39759 | val_0_rmse: 0.61908 | val_1_rmse: 0.71447 |  0:00:04s
epoch 33 | loss: 0.40598 | val_0_rmse: 0.62332 | val_1_rmse: 0.71938 |  0:00:04s
epoch 34 | loss: 0.39679 | val_0_rmse: 0.6236  | val_1_rmse: 0.71857 |  0:00:04s
epoch 35 | loss: 0.40342 | val_0_rmse: 0.61858 | val_1_rmse: 0.714   |  0:00:04s
epoch 36 | loss: 0.4002  | val_0_rmse: 0.61725 | val_1_rmse: 0.71491 |  0:00:04s
epoch 37 | loss: 0.39516 | val_0_rmse: 0.62012 | val_1_rmse: 0.71429 |  0:00:04s
epoch 38 | loss: 0.39539 | val_0_rmse: 0.61862 | val_1_rmse: 0.70977 |  0:00:05s
epoch 39 | loss: 0.39258 | val_0_rmse: 0.61571 | val_1_rmse: 0.70901 |  0:00:05s
epoch 40 | loss: 0.38538 | val_0_rmse: 0.6096  | val_1_rmse: 0.71087 |  0:00:05s
epoch 41 | loss: 0.38436 | val_0_rmse: 0.61453 | val_1_rmse: 0.72282 |  0:00:05s
epoch 42 | loss: 0.37937 | val_0_rmse: 0.60893 | val_1_rmse: 0.71563 |  0:00:05s
epoch 43 | loss: 0.38922 | val_0_rmse: 0.61169 | val_1_rmse: 0.71446 |  0:00:05s
epoch 44 | loss: 0.37856 | val_0_rmse: 0.61809 | val_1_rmse: 0.72346 |  0:00:05s
epoch 45 | loss: 0.38091 | val_0_rmse: 0.60883 | val_1_rmse: 0.71268 |  0:00:06s
epoch 46 | loss: 0.37528 | val_0_rmse: 0.60692 | val_1_rmse: 0.70195 |  0:00:06s
epoch 47 | loss: 0.37662 | val_0_rmse: 0.60957 | val_1_rmse: 0.70308 |  0:00:06s
epoch 48 | loss: 0.38663 | val_0_rmse: 0.61552 | val_1_rmse: 0.71511 |  0:00:06s
epoch 49 | loss: 0.38137 | val_0_rmse: 0.59931 | val_1_rmse: 0.71009 |  0:00:06s
epoch 50 | loss: 0.38425 | val_0_rmse: 0.59921 | val_1_rmse: 0.70675 |  0:00:06s
epoch 51 | loss: 0.37584 | val_0_rmse: 0.61508 | val_1_rmse: 0.71714 |  0:00:06s
epoch 52 | loss: 0.39332 | val_0_rmse: 0.60815 | val_1_rmse: 0.70758 |  0:00:06s
epoch 53 | loss: 0.3857  | val_0_rmse: 0.61932 | val_1_rmse: 0.7126  |  0:00:07s
epoch 54 | loss: 0.38647 | val_0_rmse: 0.61411 | val_1_rmse: 0.7032  |  0:00:07s
epoch 55 | loss: 0.38719 | val_0_rmse: 0.60961 | val_1_rmse: 0.70235 |  0:00:07s
epoch 56 | loss: 0.38995 | val_0_rmse: 0.60351 | val_1_rmse: 0.69433 |  0:00:07s
epoch 57 | loss: 0.3862  | val_0_rmse: 0.61414 | val_1_rmse: 0.71632 |  0:00:07s
epoch 58 | loss: 0.37902 | val_0_rmse: 0.68011 | val_1_rmse: 0.79749 |  0:00:07s
epoch 59 | loss: 0.37579 | val_0_rmse: 0.6387  | val_1_rmse: 0.74276 |  0:00:07s
epoch 60 | loss: 0.38311 | val_0_rmse: 0.62333 | val_1_rmse: 0.70758 |  0:00:08s
epoch 61 | loss: 0.40511 | val_0_rmse: 0.63321 | val_1_rmse: 0.71792 |  0:00:08s
epoch 62 | loss: 0.39094 | val_0_rmse: 0.61792 | val_1_rmse: 0.71132 |  0:00:08s
epoch 63 | loss: 0.37998 | val_0_rmse: 0.62239 | val_1_rmse: 0.72289 |  0:00:08s
epoch 64 | loss: 0.38285 | val_0_rmse: 0.62902 | val_1_rmse: 0.73155 |  0:00:08s
epoch 65 | loss: 0.38456 | val_0_rmse: 0.62413 | val_1_rmse: 0.73197 |  0:00:08s
epoch 66 | loss: 0.38435 | val_0_rmse: 0.61921 | val_1_rmse: 0.72821 |  0:00:08s
epoch 67 | loss: 0.3858  | val_0_rmse: 0.61685 | val_1_rmse: 0.72601 |  0:00:08s
epoch 68 | loss: 0.37645 | val_0_rmse: 0.62171 | val_1_rmse: 0.7323  |  0:00:09s
epoch 69 | loss: 0.38262 | val_0_rmse: 0.60048 | val_1_rmse: 0.71638 |  0:00:09s
epoch 70 | loss: 0.36251 | val_0_rmse: 0.60648 | val_1_rmse: 0.71998 |  0:00:09s
epoch 71 | loss: 0.36648 | val_0_rmse: 0.59252 | val_1_rmse: 0.71167 |  0:00:09s
epoch 72 | loss: 0.36874 | val_0_rmse: 0.587   | val_1_rmse: 0.70656 |  0:00:09s
epoch 73 | loss: 0.36911 | val_0_rmse: 0.62457 | val_1_rmse: 0.73643 |  0:00:09s
epoch 74 | loss: 0.37441 | val_0_rmse: 0.62616 | val_1_rmse: 0.73268 |  0:00:09s
epoch 75 | loss: 0.36096 | val_0_rmse: 0.60196 | val_1_rmse: 0.70835 |  0:00:09s
epoch 76 | loss: 0.37054 | val_0_rmse: 0.59054 | val_1_rmse: 0.70168 |  0:00:10s
epoch 77 | loss: 0.36749 | val_0_rmse: 0.59309 | val_1_rmse: 0.70506 |  0:00:10s
epoch 78 | loss: 0.35954 | val_0_rmse: 0.58234 | val_1_rmse: 0.70152 |  0:00:10s
epoch 79 | loss: 0.35133 | val_0_rmse: 0.58585 | val_1_rmse: 0.70002 |  0:00:10s
epoch 80 | loss: 0.3552  | val_0_rmse: 0.5955  | val_1_rmse: 0.70722 |  0:00:10s
epoch 81 | loss: 0.35196 | val_0_rmse: 0.60203 | val_1_rmse: 0.71493 |  0:00:10s
epoch 82 | loss: 0.36416 | val_0_rmse: 0.59859 | val_1_rmse: 0.71089 |  0:00:10s
epoch 83 | loss: 0.35178 | val_0_rmse: 0.5957  | val_1_rmse: 0.70772 |  0:00:10s
epoch 84 | loss: 0.35305 | val_0_rmse: 0.59163 | val_1_rmse: 0.70788 |  0:00:11s
epoch 85 | loss: 0.34757 | val_0_rmse: 0.58672 | val_1_rmse: 0.70657 |  0:00:11s
epoch 86 | loss: 0.34529 | val_0_rmse: 0.59042 | val_1_rmse: 0.70978 |  0:00:11s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.69433
Best weights from best epoch are automatically used!
ended training at: 08:38:54
Feature importance:
[('Area', 0.36731149751558145), ('Baths', 0.2345178597518266), ('Beds', 0.057131234781398664), ('Latitude', 0.1586642762120164), ('Longitude', 0.11854127645375973), ('Month', 0.06126188369522526), ('Year', 0.0025719715901919064)]
Mean squared error is of 2852914617.7481403
Mean absolute error:39671.768246703294
MAPE:0.3613180004778808
R2 score:0.6045358607099727
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:38:54
epoch 0  | loss: 1.56453 | val_0_rmse: 1.31915 | val_1_rmse: 1.25994 |  0:00:00s
epoch 1  | loss: 0.86337 | val_0_rmse: 1.01379 | val_1_rmse: 1.0865  |  0:00:00s
epoch 2  | loss: 0.61091 | val_0_rmse: 0.87747 | val_1_rmse: 0.84369 |  0:00:00s
epoch 3  | loss: 0.59379 | val_0_rmse: 0.76963 | val_1_rmse: 0.82395 |  0:00:00s
epoch 4  | loss: 0.53862 | val_0_rmse: 0.75766 | val_1_rmse: 0.78596 |  0:00:00s
epoch 5  | loss: 0.51768 | val_0_rmse: 0.73125 | val_1_rmse: 0.78836 |  0:00:00s
epoch 6  | loss: 0.49241 | val_0_rmse: 0.75237 | val_1_rmse: 0.79849 |  0:00:00s
epoch 7  | loss: 0.48606 | val_0_rmse: 0.765   | val_1_rmse: 0.76389 |  0:00:01s
epoch 8  | loss: 0.4714  | val_0_rmse: 0.75507 | val_1_rmse: 0.7443  |  0:00:01s
epoch 9  | loss: 0.47794 | val_0_rmse: 0.72591 | val_1_rmse: 0.72151 |  0:00:01s
epoch 10 | loss: 0.46393 | val_0_rmse: 0.72069 | val_1_rmse: 0.71862 |  0:00:01s
epoch 11 | loss: 0.46846 | val_0_rmse: 0.71182 | val_1_rmse: 0.73131 |  0:00:01s
epoch 12 | loss: 0.45328 | val_0_rmse: 0.68048 | val_1_rmse: 0.72914 |  0:00:01s
epoch 13 | loss: 0.43381 | val_0_rmse: 0.68887 | val_1_rmse: 0.73687 |  0:00:01s
epoch 14 | loss: 0.44874 | val_0_rmse: 0.67853 | val_1_rmse: 0.74131 |  0:00:02s
epoch 15 | loss: 0.43435 | val_0_rmse: 0.66715 | val_1_rmse: 0.72799 |  0:00:02s
epoch 16 | loss: 0.43857 | val_0_rmse: 0.66126 | val_1_rmse: 0.71218 |  0:00:02s
epoch 17 | loss: 0.43287 | val_0_rmse: 0.65701 | val_1_rmse: 0.6854  |  0:00:02s
epoch 18 | loss: 0.43992 | val_0_rmse: 0.64758 | val_1_rmse: 0.66872 |  0:00:02s
epoch 19 | loss: 0.42994 | val_0_rmse: 0.6471  | val_1_rmse: 0.67362 |  0:00:02s
epoch 20 | loss: 0.42377 | val_0_rmse: 0.64635 | val_1_rmse: 0.67977 |  0:00:02s
epoch 21 | loss: 0.41608 | val_0_rmse: 0.63844 | val_1_rmse: 0.66479 |  0:00:02s
epoch 22 | loss: 0.42614 | val_0_rmse: 0.64247 | val_1_rmse: 0.67452 |  0:00:03s
epoch 23 | loss: 0.42694 | val_0_rmse: 0.63186 | val_1_rmse: 0.67514 |  0:00:03s
epoch 24 | loss: 0.41245 | val_0_rmse: 0.63161 | val_1_rmse: 0.6738  |  0:00:03s
epoch 25 | loss: 0.41216 | val_0_rmse: 0.64522 | val_1_rmse: 0.68943 |  0:00:03s
epoch 26 | loss: 0.40939 | val_0_rmse: 0.63424 | val_1_rmse: 0.68078 |  0:00:03s
epoch 27 | loss: 0.41342 | val_0_rmse: 0.63885 | val_1_rmse: 0.68098 |  0:00:03s
epoch 28 | loss: 0.40341 | val_0_rmse: 0.63096 | val_1_rmse: 0.67688 |  0:00:03s
epoch 29 | loss: 0.40972 | val_0_rmse: 0.62981 | val_1_rmse: 0.68072 |  0:00:04s
epoch 30 | loss: 0.40378 | val_0_rmse: 0.62791 | val_1_rmse: 0.67847 |  0:00:04s
epoch 31 | loss: 0.40179 | val_0_rmse: 0.62511 | val_1_rmse: 0.67105 |  0:00:04s
epoch 32 | loss: 0.39588 | val_0_rmse: 0.62151 | val_1_rmse: 0.67895 |  0:00:04s
epoch 33 | loss: 0.39537 | val_0_rmse: 0.63323 | val_1_rmse: 0.69505 |  0:00:04s
epoch 34 | loss: 0.38829 | val_0_rmse: 0.62086 | val_1_rmse: 0.6777  |  0:00:04s
epoch 35 | loss: 0.39264 | val_0_rmse: 0.61302 | val_1_rmse: 0.66912 |  0:00:04s
epoch 36 | loss: 0.38254 | val_0_rmse: 0.61387 | val_1_rmse: 0.6602  |  0:00:04s
epoch 37 | loss: 0.38777 | val_0_rmse: 0.60314 | val_1_rmse: 0.64158 |  0:00:05s
epoch 38 | loss: 0.37524 | val_0_rmse: 0.60288 | val_1_rmse: 0.6462  |  0:00:05s
epoch 39 | loss: 0.38617 | val_0_rmse: 0.59894 | val_1_rmse: 0.63821 |  0:00:05s
epoch 40 | loss: 0.36444 | val_0_rmse: 0.62065 | val_1_rmse: 0.64231 |  0:00:05s
epoch 41 | loss: 0.38329 | val_0_rmse: 0.6116  | val_1_rmse: 0.63599 |  0:00:05s
epoch 42 | loss: 0.36154 | val_0_rmse: 0.59117 | val_1_rmse: 0.62301 |  0:00:05s
epoch 43 | loss: 0.35357 | val_0_rmse: 0.59041 | val_1_rmse: 0.62338 |  0:00:05s
epoch 44 | loss: 0.35244 | val_0_rmse: 0.59777 | val_1_rmse: 0.63126 |  0:00:06s
epoch 45 | loss: 0.35784 | val_0_rmse: 0.58812 | val_1_rmse: 0.62629 |  0:00:06s
epoch 46 | loss: 0.35376 | val_0_rmse: 0.58951 | val_1_rmse: 0.63748 |  0:00:06s
epoch 47 | loss: 0.34601 | val_0_rmse: 0.58648 | val_1_rmse: 0.63282 |  0:00:06s
epoch 48 | loss: 0.34958 | val_0_rmse: 0.57927 | val_1_rmse: 0.61459 |  0:00:06s
epoch 49 | loss: 0.35645 | val_0_rmse: 0.58452 | val_1_rmse: 0.61098 |  0:00:06s
epoch 50 | loss: 0.35984 | val_0_rmse: 0.58435 | val_1_rmse: 0.60844 |  0:00:06s
epoch 51 | loss: 0.37687 | val_0_rmse: 0.5861  | val_1_rmse: 0.60973 |  0:00:06s
epoch 52 | loss: 0.36634 | val_0_rmse: 0.5857  | val_1_rmse: 0.60656 |  0:00:07s
epoch 53 | loss: 0.3619  | val_0_rmse: 0.58953 | val_1_rmse: 0.62011 |  0:00:07s
epoch 54 | loss: 0.34918 | val_0_rmse: 0.58172 | val_1_rmse: 0.62911 |  0:00:07s
epoch 55 | loss: 0.34271 | val_0_rmse: 0.59271 | val_1_rmse: 0.64154 |  0:00:07s
epoch 56 | loss: 0.33744 | val_0_rmse: 0.58262 | val_1_rmse: 0.62902 |  0:00:07s
epoch 57 | loss: 0.34102 | val_0_rmse: 0.58505 | val_1_rmse: 0.62374 |  0:00:07s
epoch 58 | loss: 0.34511 | val_0_rmse: 0.58141 | val_1_rmse: 0.61794 |  0:00:07s
epoch 59 | loss: 0.33637 | val_0_rmse: 0.58111 | val_1_rmse: 0.62263 |  0:00:07s
epoch 60 | loss: 0.33557 | val_0_rmse: 0.58048 | val_1_rmse: 0.62412 |  0:00:08s
epoch 61 | loss: 0.34117 | val_0_rmse: 0.57712 | val_1_rmse: 0.62096 |  0:00:08s
epoch 62 | loss: 0.32197 | val_0_rmse: 0.57813 | val_1_rmse: 0.61977 |  0:00:08s
epoch 63 | loss: 0.34767 | val_0_rmse: 0.57324 | val_1_rmse: 0.6231  |  0:00:08s
epoch 64 | loss: 0.31991 | val_0_rmse: 0.62406 | val_1_rmse: 0.67086 |  0:00:08s
epoch 65 | loss: 0.33429 | val_0_rmse: 0.63928 | val_1_rmse: 0.67872 |  0:00:08s
epoch 66 | loss: 0.33065 | val_0_rmse: 0.58212 | val_1_rmse: 0.62484 |  0:00:08s
epoch 67 | loss: 0.31783 | val_0_rmse: 0.56432 | val_1_rmse: 0.61597 |  0:00:08s
epoch 68 | loss: 0.32778 | val_0_rmse: 0.57344 | val_1_rmse: 0.62798 |  0:00:09s
epoch 69 | loss: 0.33193 | val_0_rmse: 0.5832  | val_1_rmse: 0.63274 |  0:00:09s
epoch 70 | loss: 0.32878 | val_0_rmse: 0.58388 | val_1_rmse: 0.63246 |  0:00:09s
epoch 71 | loss: 0.31966 | val_0_rmse: 0.55624 | val_1_rmse: 0.60748 |  0:00:09s
epoch 72 | loss: 0.3275  | val_0_rmse: 0.56103 | val_1_rmse: 0.61301 |  0:00:09s
epoch 73 | loss: 0.31103 | val_0_rmse: 0.57175 | val_1_rmse: 0.62398 |  0:00:09s
epoch 74 | loss: 0.30959 | val_0_rmse: 0.54266 | val_1_rmse: 0.6041  |  0:00:09s
epoch 75 | loss: 0.31224 | val_0_rmse: 0.54627 | val_1_rmse: 0.61155 |  0:00:10s
epoch 76 | loss: 0.30781 | val_0_rmse: 0.56217 | val_1_rmse: 0.62179 |  0:00:10s
epoch 77 | loss: 0.32278 | val_0_rmse: 0.54448 | val_1_rmse: 0.59689 |  0:00:10s
epoch 78 | loss: 0.30543 | val_0_rmse: 0.54426 | val_1_rmse: 0.59126 |  0:00:10s
epoch 79 | loss: 0.30138 | val_0_rmse: 0.55837 | val_1_rmse: 0.60305 |  0:00:10s
epoch 80 | loss: 0.30733 | val_0_rmse: 0.56051 | val_1_rmse: 0.6007  |  0:00:10s
epoch 81 | loss: 0.32257 | val_0_rmse: 0.57027 | val_1_rmse: 0.61221 |  0:00:10s
epoch 82 | loss: 0.30403 | val_0_rmse: 0.54614 | val_1_rmse: 0.60353 |  0:00:10s
epoch 83 | loss: 0.30563 | val_0_rmse: 0.54056 | val_1_rmse: 0.60336 |  0:00:11s
epoch 84 | loss: 0.30134 | val_0_rmse: 0.54552 | val_1_rmse: 0.60523 |  0:00:11s
epoch 85 | loss: 0.30126 | val_0_rmse: 0.53452 | val_1_rmse: 0.59888 |  0:00:11s
epoch 86 | loss: 0.28986 | val_0_rmse: 0.5368  | val_1_rmse: 0.60163 |  0:00:11s
epoch 87 | loss: 0.29107 | val_0_rmse: 0.5371  | val_1_rmse: 0.59782 |  0:00:11s
epoch 88 | loss: 0.31528 | val_0_rmse: 0.53917 | val_1_rmse: 0.60824 |  0:00:11s
epoch 89 | loss: 0.29214 | val_0_rmse: 0.58671 | val_1_rmse: 0.642   |  0:00:11s
epoch 90 | loss: 0.30609 | val_0_rmse: 0.54129 | val_1_rmse: 0.59725 |  0:00:12s
epoch 91 | loss: 0.29324 | val_0_rmse: 0.53339 | val_1_rmse: 0.59725 |  0:00:12s
epoch 92 | loss: 0.30001 | val_0_rmse: 0.54804 | val_1_rmse: 0.61125 |  0:00:12s
epoch 93 | loss: 0.30349 | val_0_rmse: 0.53633 | val_1_rmse: 0.59985 |  0:00:12s
epoch 94 | loss: 0.29425 | val_0_rmse: 0.5788  | val_1_rmse: 0.62056 |  0:00:12s
epoch 95 | loss: 0.30167 | val_0_rmse: 0.60436 | val_1_rmse: 0.62802 |  0:00:12s
epoch 96 | loss: 0.30946 | val_0_rmse: 0.5889  | val_1_rmse: 0.61917 |  0:00:12s
epoch 97 | loss: 0.29863 | val_0_rmse: 0.61786 | val_1_rmse: 0.6564  |  0:00:12s
epoch 98 | loss: 0.30475 | val_0_rmse: 0.56742 | val_1_rmse: 0.60627 |  0:00:13s
epoch 99 | loss: 0.30097 | val_0_rmse: 0.55419 | val_1_rmse: 0.60825 |  0:00:13s
epoch 100| loss: 0.29901 | val_0_rmse: 0.55767 | val_1_rmse: 0.61399 |  0:00:13s
epoch 101| loss: 0.31099 | val_0_rmse: 0.55567 | val_1_rmse: 0.60264 |  0:00:13s
epoch 102| loss: 0.30107 | val_0_rmse: 0.56872 | val_1_rmse: 0.61695 |  0:00:13s
epoch 103| loss: 0.30097 | val_0_rmse: 0.553   | val_1_rmse: 0.613   |  0:00:13s
epoch 104| loss: 0.30438 | val_0_rmse: 0.57674 | val_1_rmse: 0.63973 |  0:00:13s
epoch 105| loss: 0.30062 | val_0_rmse: 0.55904 | val_1_rmse: 0.62952 |  0:00:13s
epoch 106| loss: 0.29295 | val_0_rmse: 0.55424 | val_1_rmse: 0.62689 |  0:00:14s
epoch 107| loss: 0.29852 | val_0_rmse: 0.55248 | val_1_rmse: 0.61489 |  0:00:14s
epoch 108| loss: 0.28163 | val_0_rmse: 0.53796 | val_1_rmse: 0.59619 |  0:00:14s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.59126
Best weights from best epoch are automatically used!
ended training at: 08:39:08
Feature importance:
[('Area', 0.2589732039247865), ('Baths', 0.18518386681909646), ('Beds', 0.06975280301901847), ('Latitude', 0.4123593849709456), ('Longitude', 0.055839303534821666), ('Month', 0.01175077348392182), ('Year', 0.0061406642474094815)]
Mean squared error is of 2499442115.6704035
Mean absolute error:36118.93530267857
MAPE:0.32461573072387906
R2 score:0.6411132640722522
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:39:08
epoch 0  | loss: 1.53082 | val_0_rmse: 1.58886 | val_1_rmse: 0.95921 |  0:00:00s
epoch 1  | loss: 0.85433 | val_0_rmse: 0.96601 | val_1_rmse: 0.93132 |  0:00:00s
epoch 2  | loss: 0.66889 | val_0_rmse: 0.91397 | val_1_rmse: 0.85636 |  0:00:00s
epoch 3  | loss: 0.59784 | val_0_rmse: 0.8496  | val_1_rmse: 0.81292 |  0:00:00s
epoch 4  | loss: 0.5475  | val_0_rmse: 0.9914  | val_1_rmse: 0.93761 |  0:00:00s
epoch 5  | loss: 0.49923 | val_0_rmse: 0.83328 | val_1_rmse: 0.84551 |  0:00:00s
epoch 6  | loss: 0.47559 | val_0_rmse: 0.80808 | val_1_rmse: 0.79753 |  0:00:00s
epoch 7  | loss: 0.47759 | val_0_rmse: 0.80991 | val_1_rmse: 0.78196 |  0:00:01s
epoch 8  | loss: 0.45592 | val_0_rmse: 0.77148 | val_1_rmse: 0.75336 |  0:00:01s
epoch 9  | loss: 0.46868 | val_0_rmse: 0.72617 | val_1_rmse: 0.73475 |  0:00:01s
epoch 10 | loss: 0.46482 | val_0_rmse: 0.71471 | val_1_rmse: 0.72669 |  0:00:01s
epoch 11 | loss: 0.45478 | val_0_rmse: 0.69952 | val_1_rmse: 0.69443 |  0:00:01s
epoch 12 | loss: 0.43527 | val_0_rmse: 0.71796 | val_1_rmse: 0.72337 |  0:00:01s
epoch 13 | loss: 0.43906 | val_0_rmse: 0.70153 | val_1_rmse: 0.71248 |  0:00:01s
epoch 14 | loss: 0.45271 | val_0_rmse: 0.67407 | val_1_rmse: 0.68301 |  0:00:02s
epoch 15 | loss: 0.44265 | val_0_rmse: 0.67821 | val_1_rmse: 0.69184 |  0:00:02s
epoch 16 | loss: 0.44003 | val_0_rmse: 0.67892 | val_1_rmse: 0.69178 |  0:00:02s
epoch 17 | loss: 0.42142 | val_0_rmse: 0.66423 | val_1_rmse: 0.67601 |  0:00:02s
epoch 18 | loss: 0.43744 | val_0_rmse: 0.65663 | val_1_rmse: 0.67026 |  0:00:02s
epoch 19 | loss: 0.4203  | val_0_rmse: 0.65739 | val_1_rmse: 0.6621  |  0:00:02s
epoch 20 | loss: 0.42557 | val_0_rmse: 0.6562  | val_1_rmse: 0.66011 |  0:00:02s
epoch 21 | loss: 0.42098 | val_0_rmse: 0.65888 | val_1_rmse: 0.65805 |  0:00:02s
epoch 22 | loss: 0.40892 | val_0_rmse: 0.6527  | val_1_rmse: 0.65145 |  0:00:03s
epoch 23 | loss: 0.42317 | val_0_rmse: 0.64665 | val_1_rmse: 0.65242 |  0:00:03s
epoch 24 | loss: 0.42872 | val_0_rmse: 0.65367 | val_1_rmse: 0.65417 |  0:00:03s
epoch 25 | loss: 0.44261 | val_0_rmse: 0.64441 | val_1_rmse: 0.64531 |  0:00:03s
epoch 26 | loss: 0.41168 | val_0_rmse: 0.65205 | val_1_rmse: 0.6562  |  0:00:03s
epoch 27 | loss: 0.41841 | val_0_rmse: 0.64155 | val_1_rmse: 0.65219 |  0:00:03s
epoch 28 | loss: 0.41942 | val_0_rmse: 0.63749 | val_1_rmse: 0.6544  |  0:00:03s
epoch 29 | loss: 0.403   | val_0_rmse: 0.64433 | val_1_rmse: 0.65543 |  0:00:04s
epoch 30 | loss: 0.40919 | val_0_rmse: 0.63084 | val_1_rmse: 0.64869 |  0:00:04s
epoch 31 | loss: 0.39693 | val_0_rmse: 0.62666 | val_1_rmse: 0.65016 |  0:00:04s
epoch 32 | loss: 0.41089 | val_0_rmse: 0.62536 | val_1_rmse: 0.65264 |  0:00:04s
epoch 33 | loss: 0.41886 | val_0_rmse: 0.62913 | val_1_rmse: 0.64986 |  0:00:04s
epoch 34 | loss: 0.40103 | val_0_rmse: 0.63344 | val_1_rmse: 0.63725 |  0:00:04s
epoch 35 | loss: 0.39066 | val_0_rmse: 0.63321 | val_1_rmse: 0.63474 |  0:00:04s
epoch 36 | loss: 0.39768 | val_0_rmse: 0.63927 | val_1_rmse: 0.64223 |  0:00:04s
epoch 37 | loss: 0.40772 | val_0_rmse: 0.63019 | val_1_rmse: 0.641   |  0:00:05s
epoch 38 | loss: 0.39397 | val_0_rmse: 0.62743 | val_1_rmse: 0.64819 |  0:00:05s
epoch 39 | loss: 0.40753 | val_0_rmse: 0.631   | val_1_rmse: 0.65233 |  0:00:05s
epoch 40 | loss: 0.40011 | val_0_rmse: 0.62692 | val_1_rmse: 0.65618 |  0:00:05s
epoch 41 | loss: 0.41111 | val_0_rmse: 0.6314  | val_1_rmse: 0.66118 |  0:00:05s
epoch 42 | loss: 0.40748 | val_0_rmse: 0.63969 | val_1_rmse: 0.65116 |  0:00:05s
epoch 43 | loss: 0.42554 | val_0_rmse: 0.63554 | val_1_rmse: 0.64596 |  0:00:05s
epoch 44 | loss: 0.41509 | val_0_rmse: 0.63558 | val_1_rmse: 0.65189 |  0:00:06s
epoch 45 | loss: 0.41255 | val_0_rmse: 0.6349  | val_1_rmse: 0.64406 |  0:00:06s
epoch 46 | loss: 0.40076 | val_0_rmse: 0.63516 | val_1_rmse: 0.64251 |  0:00:06s
epoch 47 | loss: 0.4104  | val_0_rmse: 0.63642 | val_1_rmse: 0.64472 |  0:00:06s
epoch 48 | loss: 0.40877 | val_0_rmse: 0.63114 | val_1_rmse: 0.65174 |  0:00:06s
epoch 49 | loss: 0.4106  | val_0_rmse: 0.63541 | val_1_rmse: 0.6486  |  0:00:06s
epoch 50 | loss: 0.3976  | val_0_rmse: 0.62993 | val_1_rmse: 0.64633 |  0:00:06s
epoch 51 | loss: 0.40331 | val_0_rmse: 0.63065 | val_1_rmse: 0.66547 |  0:00:06s
epoch 52 | loss: 0.40315 | val_0_rmse: 0.62647 | val_1_rmse: 0.66651 |  0:00:07s
epoch 53 | loss: 0.40828 | val_0_rmse: 0.62505 | val_1_rmse: 0.6629  |  0:00:07s
epoch 54 | loss: 0.39196 | val_0_rmse: 0.62602 | val_1_rmse: 0.66391 |  0:00:07s
epoch 55 | loss: 0.4064  | val_0_rmse: 0.62027 | val_1_rmse: 0.66521 |  0:00:07s
epoch 56 | loss: 0.40405 | val_0_rmse: 0.63341 | val_1_rmse: 0.65837 |  0:00:07s
epoch 57 | loss: 0.4028  | val_0_rmse: 0.63993 | val_1_rmse: 0.66174 |  0:00:07s
epoch 58 | loss: 0.40965 | val_0_rmse: 0.63107 | val_1_rmse: 0.65735 |  0:00:07s
epoch 59 | loss: 0.42294 | val_0_rmse: 0.63024 | val_1_rmse: 0.64262 |  0:00:07s
epoch 60 | loss: 0.42613 | val_0_rmse: 0.63395 | val_1_rmse: 0.6395  |  0:00:08s
epoch 61 | loss: 0.41062 | val_0_rmse: 0.6563  | val_1_rmse: 0.67355 |  0:00:08s
epoch 62 | loss: 0.42346 | val_0_rmse: 0.63698 | val_1_rmse: 0.65099 |  0:00:08s
epoch 63 | loss: 0.41824 | val_0_rmse: 0.64616 | val_1_rmse: 0.64584 |  0:00:08s
epoch 64 | loss: 0.43136 | val_0_rmse: 0.63429 | val_1_rmse: 0.65012 |  0:00:08s
epoch 65 | loss: 0.40908 | val_0_rmse: 0.63078 | val_1_rmse: 0.6561  |  0:00:08s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.63474
Best weights from best epoch are automatically used!
ended training at: 08:39:17
Feature importance:
[('Area', 0.39851491688122326), ('Baths', 0.19986221564262863), ('Beds', 0.034173558172113136), ('Latitude', 0.2075839433017664), ('Longitude', 0.08335654384397359), ('Month', 0.032769865355414186), ('Year', 0.04373895680288077)]
Mean squared error is of 3457211981.444044
Mean absolute error:42214.35701689561
MAPE:0.3541619977793723
R2 score:0.5358638569223493
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:39:17
epoch 0  | loss: 1.64247 | val_0_rmse: 1.83718 | val_1_rmse: 1.72235 |  0:00:00s
epoch 1  | loss: 1.00951 | val_0_rmse: 1.02649 | val_1_rmse: 0.91635 |  0:00:00s
epoch 2  | loss: 0.70454 | val_0_rmse: 0.86996 | val_1_rmse: 0.77012 |  0:00:00s
epoch 3  | loss: 0.57317 | val_0_rmse: 0.84191 | val_1_rmse: 0.79927 |  0:00:00s
epoch 4  | loss: 0.53441 | val_0_rmse: 0.83141 | val_1_rmse: 0.81668 |  0:00:00s
epoch 5  | loss: 0.50137 | val_0_rmse: 0.83022 | val_1_rmse: 0.78034 |  0:00:00s
epoch 6  | loss: 0.5006  | val_0_rmse: 0.76051 | val_1_rmse: 0.718   |  0:00:00s
epoch 7  | loss: 0.49609 | val_0_rmse: 0.72833 | val_1_rmse: 0.67533 |  0:00:01s
epoch 8  | loss: 0.47529 | val_0_rmse: 0.7099  | val_1_rmse: 0.67297 |  0:00:01s
epoch 9  | loss: 0.47179 | val_0_rmse: 0.68862 | val_1_rmse: 0.66778 |  0:00:01s
epoch 10 | loss: 0.46924 | val_0_rmse: 0.68369 | val_1_rmse: 0.6645  |  0:00:01s
epoch 11 | loss: 0.45467 | val_0_rmse: 0.67984 | val_1_rmse: 0.66356 |  0:00:01s
epoch 12 | loss: 0.45352 | val_0_rmse: 0.68361 | val_1_rmse: 0.68369 |  0:00:01s
epoch 13 | loss: 0.43522 | val_0_rmse: 0.68673 | val_1_rmse: 0.70008 |  0:00:01s
epoch 14 | loss: 0.45734 | val_0_rmse: 0.66169 | val_1_rmse: 0.68247 |  0:00:01s
epoch 15 | loss: 0.44071 | val_0_rmse: 0.6566  | val_1_rmse: 0.67601 |  0:00:02s
epoch 16 | loss: 0.43782 | val_0_rmse: 0.65108 | val_1_rmse: 0.67894 |  0:00:02s
epoch 17 | loss: 0.44595 | val_0_rmse: 0.65127 | val_1_rmse: 0.67697 |  0:00:02s
epoch 18 | loss: 0.42834 | val_0_rmse: 0.64826 | val_1_rmse: 0.66938 |  0:00:02s
epoch 19 | loss: 0.43523 | val_0_rmse: 0.64543 | val_1_rmse: 0.67058 |  0:00:02s
epoch 20 | loss: 0.42027 | val_0_rmse: 0.64548 | val_1_rmse: 0.67117 |  0:00:02s
epoch 21 | loss: 0.41522 | val_0_rmse: 0.64095 | val_1_rmse: 0.67091 |  0:00:02s
epoch 22 | loss: 0.42247 | val_0_rmse: 0.63962 | val_1_rmse: 0.67897 |  0:00:03s
epoch 23 | loss: 0.43252 | val_0_rmse: 0.6433  | val_1_rmse: 0.69731 |  0:00:03s
epoch 24 | loss: 0.4205  | val_0_rmse: 0.66035 | val_1_rmse: 0.70859 |  0:00:03s
epoch 25 | loss: 0.4194  | val_0_rmse: 0.64023 | val_1_rmse: 0.6821  |  0:00:03s
epoch 26 | loss: 0.41045 | val_0_rmse: 0.6389  | val_1_rmse: 0.68225 |  0:00:03s
epoch 27 | loss: 0.41613 | val_0_rmse: 0.63574 | val_1_rmse: 0.68573 |  0:00:03s
epoch 28 | loss: 0.41641 | val_0_rmse: 0.62924 | val_1_rmse: 0.67931 |  0:00:03s
epoch 29 | loss: 0.40104 | val_0_rmse: 0.62845 | val_1_rmse: 0.6797  |  0:00:03s
epoch 30 | loss: 0.41567 | val_0_rmse: 0.62545 | val_1_rmse: 0.67711 |  0:00:04s
epoch 31 | loss: 0.39651 | val_0_rmse: 0.63267 | val_1_rmse: 0.67648 |  0:00:04s
epoch 32 | loss: 0.39378 | val_0_rmse: 0.6213  | val_1_rmse: 0.66127 |  0:00:04s
epoch 33 | loss: 0.40454 | val_0_rmse: 0.61734 | val_1_rmse: 0.64884 |  0:00:04s
epoch 34 | loss: 0.38081 | val_0_rmse: 0.63025 | val_1_rmse: 0.66169 |  0:00:04s
epoch 35 | loss: 0.39107 | val_0_rmse: 0.60998 | val_1_rmse: 0.6348  |  0:00:04s
epoch 36 | loss: 0.38749 | val_0_rmse: 0.62086 | val_1_rmse: 0.63556 |  0:00:04s
epoch 37 | loss: 0.39284 | val_0_rmse: 0.64287 | val_1_rmse: 0.69651 |  0:00:05s
epoch 38 | loss: 0.396   | val_0_rmse: 0.62431 | val_1_rmse: 0.69252 |  0:00:05s
epoch 39 | loss: 0.37814 | val_0_rmse: 0.62175 | val_1_rmse: 0.67091 |  0:00:05s
epoch 40 | loss: 0.39422 | val_0_rmse: 0.66098 | val_1_rmse: 0.6921  |  0:00:05s
epoch 41 | loss: 0.42417 | val_0_rmse: 0.65267 | val_1_rmse: 0.69189 |  0:00:05s
epoch 42 | loss: 0.39815 | val_0_rmse: 0.6288  | val_1_rmse: 0.6687  |  0:00:05s
epoch 43 | loss: 0.38095 | val_0_rmse: 0.62498 | val_1_rmse: 0.65854 |  0:00:05s
epoch 44 | loss: 0.38812 | val_0_rmse: 0.62388 | val_1_rmse: 0.66512 |  0:00:05s
epoch 45 | loss: 0.39259 | val_0_rmse: 0.60681 | val_1_rmse: 0.63975 |  0:00:06s
epoch 46 | loss: 0.39136 | val_0_rmse: 0.75878 | val_1_rmse: 0.78177 |  0:00:06s
epoch 47 | loss: 0.37476 | val_0_rmse: 0.7949  | val_1_rmse: 0.84256 |  0:00:06s
epoch 48 | loss: 0.37084 | val_0_rmse: 0.81845 | val_1_rmse: 0.86034 |  0:00:06s
epoch 49 | loss: 0.38007 | val_0_rmse: 0.7233  | val_1_rmse: 0.7634  |  0:00:06s
epoch 50 | loss: 0.35533 | val_0_rmse: 0.68296 | val_1_rmse: 0.73151 |  0:00:06s
epoch 51 | loss: 0.37935 | val_0_rmse: 0.69578 | val_1_rmse: 0.7303  |  0:00:06s
epoch 52 | loss: 0.37367 | val_0_rmse: 0.5975  | val_1_rmse: 0.65134 |  0:00:06s
epoch 53 | loss: 0.36187 | val_0_rmse: 0.5931  | val_1_rmse: 0.64936 |  0:00:07s
epoch 54 | loss: 0.3727  | val_0_rmse: 0.59587 | val_1_rmse: 0.64515 |  0:00:07s
epoch 55 | loss: 0.35996 | val_0_rmse: 0.60269 | val_1_rmse: 0.65329 |  0:00:07s
epoch 56 | loss: 0.36817 | val_0_rmse: 0.60576 | val_1_rmse: 0.65067 |  0:00:07s
epoch 57 | loss: 0.3664  | val_0_rmse: 0.59762 | val_1_rmse: 0.6368  |  0:00:07s
epoch 58 | loss: 0.3509  | val_0_rmse: 0.60091 | val_1_rmse: 0.6407  |  0:00:07s
epoch 59 | loss: 0.3625  | val_0_rmse: 0.59233 | val_1_rmse: 0.62976 |  0:00:07s
epoch 60 | loss: 0.36342 | val_0_rmse: 0.60191 | val_1_rmse: 0.63627 |  0:00:08s
epoch 61 | loss: 0.36361 | val_0_rmse: 0.60004 | val_1_rmse: 0.63478 |  0:00:08s
epoch 62 | loss: 0.36706 | val_0_rmse: 0.60866 | val_1_rmse: 0.63543 |  0:00:08s
epoch 63 | loss: 0.37313 | val_0_rmse: 0.611   | val_1_rmse: 0.64218 |  0:00:08s
epoch 64 | loss: 0.36529 | val_0_rmse: 0.6082  | val_1_rmse: 0.65085 |  0:00:08s
epoch 65 | loss: 0.3712  | val_0_rmse: 0.59758 | val_1_rmse: 0.65482 |  0:00:08s
epoch 66 | loss: 0.37124 | val_0_rmse: 0.61266 | val_1_rmse: 0.68454 |  0:00:08s
epoch 67 | loss: 0.36805 | val_0_rmse: 0.6308  | val_1_rmse: 0.69819 |  0:00:08s
epoch 68 | loss: 0.38747 | val_0_rmse: 0.61977 | val_1_rmse: 0.67114 |  0:00:09s
epoch 69 | loss: 0.35931 | val_0_rmse: 0.60998 | val_1_rmse: 0.6553  |  0:00:09s
epoch 70 | loss: 0.35894 | val_0_rmse: 0.64335 | val_1_rmse: 0.6761  |  0:00:09s
epoch 71 | loss: 0.34705 | val_0_rmse: 0.80424 | val_1_rmse: 0.82381 |  0:00:09s
epoch 72 | loss: 0.34449 | val_0_rmse: 0.9464  | val_1_rmse: 0.96236 |  0:00:09s
epoch 73 | loss: 0.34631 | val_0_rmse: 0.88431 | val_1_rmse: 0.89946 |  0:00:09s
epoch 74 | loss: 0.34608 | val_0_rmse: 0.81815 | val_1_rmse: 0.83716 |  0:00:09s
epoch 75 | loss: 0.33966 | val_0_rmse: 0.83085 | val_1_rmse: 0.85159 |  0:00:09s
epoch 76 | loss: 0.34737 | val_0_rmse: 0.79677 | val_1_rmse: 0.82069 |  0:00:10s
epoch 77 | loss: 0.3366  | val_0_rmse: 0.79234 | val_1_rmse: 0.81449 |  0:00:10s
epoch 78 | loss: 0.35984 | val_0_rmse: 0.68188 | val_1_rmse: 0.71198 |  0:00:10s
epoch 79 | loss: 0.32541 | val_0_rmse: 0.66068 | val_1_rmse: 0.70387 |  0:00:10s
epoch 80 | loss: 0.33483 | val_0_rmse: 0.60508 | val_1_rmse: 0.65161 |  0:00:10s
epoch 81 | loss: 0.3409  | val_0_rmse: 0.57239 | val_1_rmse: 0.62317 |  0:00:10s
epoch 82 | loss: 0.34375 | val_0_rmse: 0.62865 | val_1_rmse: 0.69112 |  0:00:10s
epoch 83 | loss: 0.32734 | val_0_rmse: 0.59839 | val_1_rmse: 0.66046 |  0:00:11s
epoch 84 | loss: 0.34253 | val_0_rmse: 0.60064 | val_1_rmse: 0.67196 |  0:00:11s
epoch 85 | loss: 0.3386  | val_0_rmse: 0.63684 | val_1_rmse: 0.70059 |  0:00:11s
epoch 86 | loss: 0.34595 | val_0_rmse: 0.66773 | val_1_rmse: 0.7167  |  0:00:11s
epoch 87 | loss: 0.34028 | val_0_rmse: 0.66448 | val_1_rmse: 0.71988 |  0:00:11s
epoch 88 | loss: 0.33321 | val_0_rmse: 0.62864 | val_1_rmse: 0.69157 |  0:00:11s
epoch 89 | loss: 0.33939 | val_0_rmse: 0.65135 | val_1_rmse: 0.6943  |  0:00:11s
epoch 90 | loss: 0.33135 | val_0_rmse: 0.70279 | val_1_rmse: 0.73835 |  0:00:11s
epoch 91 | loss: 0.34751 | val_0_rmse: 0.69184 | val_1_rmse: 0.72642 |  0:00:12s
epoch 92 | loss: 0.3403  | val_0_rmse: 0.68803 | val_1_rmse: 0.72277 |  0:00:12s
epoch 93 | loss: 0.34343 | val_0_rmse: 0.69706 | val_1_rmse: 0.73101 |  0:00:12s
epoch 94 | loss: 0.33576 | val_0_rmse: 0.7115  | val_1_rmse: 0.73393 |  0:00:12s
epoch 95 | loss: 0.32927 | val_0_rmse: 0.72252 | val_1_rmse: 0.74633 |  0:00:12s
epoch 96 | loss: 0.3289  | val_0_rmse: 0.73151 | val_1_rmse: 0.75627 |  0:00:12s
epoch 97 | loss: 0.32462 | val_0_rmse: 0.73006 | val_1_rmse: 0.75273 |  0:00:12s
epoch 98 | loss: 0.33708 | val_0_rmse: 0.73619 | val_1_rmse: 0.76404 |  0:00:12s
epoch 99 | loss: 0.32148 | val_0_rmse: 0.698   | val_1_rmse: 0.74384 |  0:00:13s
epoch 100| loss: 0.32732 | val_0_rmse: 0.57465 | val_1_rmse: 0.6457  |  0:00:13s
epoch 101| loss: 0.3189  | val_0_rmse: 0.63381 | val_1_rmse: 0.69853 |  0:00:13s
epoch 102| loss: 0.32306 | val_0_rmse: 0.64146 | val_1_rmse: 0.67675 |  0:00:13s
epoch 103| loss: 0.33512 | val_0_rmse: 0.59917 | val_1_rmse: 0.62263 |  0:00:13s
epoch 104| loss: 0.31807 | val_0_rmse: 0.71153 | val_1_rmse: 0.7378  |  0:00:13s
epoch 105| loss: 0.31933 | val_0_rmse: 0.69109 | val_1_rmse: 0.7221  |  0:00:13s
epoch 106| loss: 0.33616 | val_0_rmse: 0.74934 | val_1_rmse: 0.77746 |  0:00:13s
epoch 107| loss: 0.32734 | val_0_rmse: 0.88345 | val_1_rmse: 0.90914 |  0:00:14s
epoch 108| loss: 0.34093 | val_0_rmse: 0.87486 | val_1_rmse: 0.90078 |  0:00:14s
epoch 109| loss: 0.32032 | val_0_rmse: 0.90846 | val_1_rmse: 0.92406 |  0:00:14s
epoch 110| loss: 0.32253 | val_0_rmse: 0.87055 | val_1_rmse: 0.88177 |  0:00:14s
epoch 111| loss: 0.3206  | val_0_rmse: 0.70457 | val_1_rmse: 0.71668 |  0:00:14s
epoch 112| loss: 0.34952 | val_0_rmse: 0.63394 | val_1_rmse: 0.68069 |  0:00:14s
epoch 113| loss: 0.32452 | val_0_rmse: 0.74693 | val_1_rmse: 0.76176 |  0:00:14s
epoch 114| loss: 0.31512 | val_0_rmse: 0.74725 | val_1_rmse: 0.76131 |  0:00:15s
epoch 115| loss: 0.32866 | val_0_rmse: 0.7606  | val_1_rmse: 0.7811  |  0:00:15s
epoch 116| loss: 0.31719 | val_0_rmse: 0.72946 | val_1_rmse: 0.76812 |  0:00:15s
epoch 117| loss: 0.33813 | val_0_rmse: 0.72887 | val_1_rmse: 0.76102 |  0:00:15s
epoch 118| loss: 0.3191  | val_0_rmse: 0.75309 | val_1_rmse: 0.76634 |  0:00:15s
epoch 119| loss: 0.32482 | val_0_rmse: 0.69329 | val_1_rmse: 0.73988 |  0:00:15s
epoch 120| loss: 0.31419 | val_0_rmse: 0.5513  | val_1_rmse: 0.62154 |  0:00:15s
epoch 121| loss: 0.31442 | val_0_rmse: 0.54182 | val_1_rmse: 0.60391 |  0:00:15s
epoch 122| loss: 0.31873 | val_0_rmse: 0.54716 | val_1_rmse: 0.60745 |  0:00:16s
epoch 123| loss: 0.30746 | val_0_rmse: 0.62724 | val_1_rmse: 0.67117 |  0:00:16s
epoch 124| loss: 0.30308 | val_0_rmse: 0.57501 | val_1_rmse: 0.62715 |  0:00:16s
epoch 125| loss: 0.32809 | val_0_rmse: 0.54751 | val_1_rmse: 0.60927 |  0:00:16s
epoch 126| loss: 0.32199 | val_0_rmse: 0.53659 | val_1_rmse: 0.59293 |  0:00:16s
epoch 127| loss: 0.30992 | val_0_rmse: 0.57466 | val_1_rmse: 0.6157  |  0:00:16s
epoch 128| loss: 0.31271 | val_0_rmse: 0.54387 | val_1_rmse: 0.60308 |  0:00:16s
epoch 129| loss: 0.32454 | val_0_rmse: 0.58923 | val_1_rmse: 0.64905 |  0:00:16s
epoch 130| loss: 0.31931 | val_0_rmse: 0.63484 | val_1_rmse: 0.69224 |  0:00:17s
epoch 131| loss: 0.32167 | val_0_rmse: 0.67586 | val_1_rmse: 0.74243 |  0:00:17s
epoch 132| loss: 0.3215  | val_0_rmse: 0.73517 | val_1_rmse: 0.80156 |  0:00:17s
epoch 133| loss: 0.34346 | val_0_rmse: 0.67137 | val_1_rmse: 0.72621 |  0:00:17s
epoch 134| loss: 0.34321 | val_0_rmse: 0.67452 | val_1_rmse: 0.71747 |  0:00:17s
epoch 135| loss: 0.33624 | val_0_rmse: 0.58247 | val_1_rmse: 0.6366  |  0:00:17s
epoch 136| loss: 0.32687 | val_0_rmse: 0.58899 | val_1_rmse: 0.63578 |  0:00:17s
epoch 137| loss: 0.34092 | val_0_rmse: 0.56794 | val_1_rmse: 0.62729 |  0:00:17s
epoch 138| loss: 0.32906 | val_0_rmse: 0.7538  | val_1_rmse: 0.77401 |  0:00:18s
epoch 139| loss: 0.34498 | val_0_rmse: 0.82803 | val_1_rmse: 0.82747 |  0:00:18s
epoch 140| loss: 0.32352 | val_0_rmse: 0.81453 | val_1_rmse: 0.80476 |  0:00:18s
epoch 141| loss: 0.33013 | val_0_rmse: 0.82516 | val_1_rmse: 0.82369 |  0:00:18s
epoch 142| loss: 0.34941 | val_0_rmse: 0.80222 | val_1_rmse: 0.81402 |  0:00:18s
epoch 143| loss: 0.34581 | val_0_rmse: 0.69685 | val_1_rmse: 0.74591 |  0:00:18s
epoch 144| loss: 0.31903 | val_0_rmse: 0.57576 | val_1_rmse: 0.62949 |  0:00:18s
epoch 145| loss: 0.3384  | val_0_rmse: 0.58775 | val_1_rmse: 0.62925 |  0:00:19s
epoch 146| loss: 0.35084 | val_0_rmse: 0.57733 | val_1_rmse: 0.61915 |  0:00:19s
epoch 147| loss: 0.32624 | val_0_rmse: 0.59878 | val_1_rmse: 0.64453 |  0:00:19s
epoch 148| loss: 0.33797 | val_0_rmse: 0.58819 | val_1_rmse: 0.61547 |  0:00:19s
epoch 149| loss: 0.33688 | val_0_rmse: 0.6371  | val_1_rmse: 0.65056 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.59293
Best weights from best epoch are automatically used!
ended training at: 08:39:37
Feature importance:
[('Area', 0.22802131895579786), ('Baths', 0.14218300950318724), ('Beds', 0.11129194265901042), ('Latitude', 0.3930431183507908), ('Longitude', 0.09516194953142654), ('Month', 0.0268284013915553), ('Year', 0.0034702596082318137)]
Mean squared error is of 2426260111.9124937
Mean absolute error:35315.46887405134
MAPE:0.30620757591178277
R2 score:0.6470859491149827
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:39:37
epoch 0  | loss: 1.63896 | val_0_rmse: 2.16941 | val_1_rmse: 1.27147 |  0:00:00s
epoch 1  | loss: 0.85708 | val_0_rmse: 1.08159 | val_1_rmse: 1.09557 |  0:00:00s
epoch 2  | loss: 0.61403 | val_0_rmse: 0.84678 | val_1_rmse: 0.82708 |  0:00:00s
epoch 3  | loss: 0.58417 | val_0_rmse: 0.83665 | val_1_rmse: 0.81651 |  0:00:00s
epoch 4  | loss: 0.52358 | val_0_rmse: 0.90646 | val_1_rmse: 0.90677 |  0:00:00s
epoch 5  | loss: 0.53142 | val_0_rmse: 0.79939 | val_1_rmse: 0.82041 |  0:00:00s
epoch 6  | loss: 0.49704 | val_0_rmse: 0.73843 | val_1_rmse: 0.76208 |  0:00:00s
epoch 7  | loss: 0.49818 | val_0_rmse: 0.71385 | val_1_rmse: 0.75042 |  0:00:01s
epoch 8  | loss: 0.49206 | val_0_rmse: 0.70216 | val_1_rmse: 0.73521 |  0:00:01s
epoch 9  | loss: 0.47997 | val_0_rmse: 0.71347 | val_1_rmse: 0.73016 |  0:00:01s
epoch 10 | loss: 0.47181 | val_0_rmse: 0.71281 | val_1_rmse: 0.72039 |  0:00:01s
epoch 11 | loss: 0.47195 | val_0_rmse: 0.69076 | val_1_rmse: 0.71952 |  0:00:01s
epoch 12 | loss: 0.46291 | val_0_rmse: 0.68894 | val_1_rmse: 0.71815 |  0:00:01s
epoch 13 | loss: 0.46356 | val_0_rmse: 0.6891  | val_1_rmse: 0.72734 |  0:00:01s
epoch 14 | loss: 0.46104 | val_0_rmse: 0.68523 | val_1_rmse: 0.71556 |  0:00:01s
epoch 15 | loss: 0.46397 | val_0_rmse: 0.67461 | val_1_rmse: 0.70821 |  0:00:02s
epoch 16 | loss: 0.45762 | val_0_rmse: 0.66671 | val_1_rmse: 0.71404 |  0:00:02s
epoch 17 | loss: 0.43848 | val_0_rmse: 0.66237 | val_1_rmse: 0.70648 |  0:00:02s
epoch 18 | loss: 0.44188 | val_0_rmse: 0.66633 | val_1_rmse: 0.70896 |  0:00:02s
epoch 19 | loss: 0.45758 | val_0_rmse: 0.65935 | val_1_rmse: 0.7058  |  0:00:02s
epoch 20 | loss: 0.42691 | val_0_rmse: 0.65554 | val_1_rmse: 0.70125 |  0:00:02s
epoch 21 | loss: 0.44344 | val_0_rmse: 0.64951 | val_1_rmse: 0.69611 |  0:00:02s
epoch 22 | loss: 0.43222 | val_0_rmse: 0.67052 | val_1_rmse: 0.6909  |  0:00:03s
epoch 23 | loss: 0.42797 | val_0_rmse: 0.66401 | val_1_rmse: 0.69191 |  0:00:03s
epoch 24 | loss: 0.42464 | val_0_rmse: 0.64516 | val_1_rmse: 0.69858 |  0:00:03s
epoch 25 | loss: 0.42737 | val_0_rmse: 0.64745 | val_1_rmse: 0.70637 |  0:00:03s
epoch 26 | loss: 0.4176  | val_0_rmse: 0.64533 | val_1_rmse: 0.70717 |  0:00:03s
epoch 27 | loss: 0.42268 | val_0_rmse: 0.63848 | val_1_rmse: 0.69725 |  0:00:03s
epoch 28 | loss: 0.41507 | val_0_rmse: 0.63228 | val_1_rmse: 0.69425 |  0:00:03s
epoch 29 | loss: 0.41436 | val_0_rmse: 0.63767 | val_1_rmse: 0.70307 |  0:00:03s
epoch 30 | loss: 0.41812 | val_0_rmse: 0.64096 | val_1_rmse: 0.69601 |  0:00:04s
epoch 31 | loss: 0.41183 | val_0_rmse: 0.63532 | val_1_rmse: 0.70582 |  0:00:04s
epoch 32 | loss: 0.41108 | val_0_rmse: 0.63901 | val_1_rmse: 0.71911 |  0:00:04s
epoch 33 | loss: 0.41361 | val_0_rmse: 0.64367 | val_1_rmse: 0.7197  |  0:00:04s
epoch 34 | loss: 0.41779 | val_0_rmse: 0.63872 | val_1_rmse: 0.71287 |  0:00:04s
epoch 35 | loss: 0.41563 | val_0_rmse: 0.63415 | val_1_rmse: 0.71188 |  0:00:04s
epoch 36 | loss: 0.41472 | val_0_rmse: 0.63659 | val_1_rmse: 0.71387 |  0:00:04s
epoch 37 | loss: 0.42281 | val_0_rmse: 0.63515 | val_1_rmse: 0.71529 |  0:00:05s
epoch 38 | loss: 0.40959 | val_0_rmse: 0.63269 | val_1_rmse: 0.71954 |  0:00:05s
epoch 39 | loss: 0.40773 | val_0_rmse: 0.64357 | val_1_rmse: 0.72973 |  0:00:05s
epoch 40 | loss: 0.42253 | val_0_rmse: 0.62262 | val_1_rmse: 0.71242 |  0:00:05s
epoch 41 | loss: 0.39667 | val_0_rmse: 0.63215 | val_1_rmse: 0.71353 |  0:00:05s
epoch 42 | loss: 0.4153  | val_0_rmse: 0.63331 | val_1_rmse: 0.71062 |  0:00:05s
epoch 43 | loss: 0.40499 | val_0_rmse: 0.63012 | val_1_rmse: 0.70448 |  0:00:05s
epoch 44 | loss: 0.41026 | val_0_rmse: 0.6288  | val_1_rmse: 0.70981 |  0:00:05s
epoch 45 | loss: 0.40255 | val_0_rmse: 0.62248 | val_1_rmse: 0.70709 |  0:00:06s
epoch 46 | loss: 0.39623 | val_0_rmse: 0.62508 | val_1_rmse: 0.70801 |  0:00:06s
epoch 47 | loss: 0.39589 | val_0_rmse: 0.63115 | val_1_rmse: 0.70704 |  0:00:06s
epoch 48 | loss: 0.40556 | val_0_rmse: 0.63692 | val_1_rmse: 0.70262 |  0:00:06s
epoch 49 | loss: 0.4045  | val_0_rmse: 0.64052 | val_1_rmse: 0.70115 |  0:00:06s
epoch 50 | loss: 0.40983 | val_0_rmse: 0.63428 | val_1_rmse: 0.69852 |  0:00:06s
epoch 51 | loss: 0.41587 | val_0_rmse: 0.62955 | val_1_rmse: 0.7152  |  0:00:06s
epoch 52 | loss: 0.3999  | val_0_rmse: 0.6297  | val_1_rmse: 0.71527 |  0:00:07s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.6909
Best weights from best epoch are automatically used!
ended training at: 08:39:44
Feature importance:
[('Area', 0.42638040255968973), ('Baths', 0.12099843696608092), ('Beds', 0.06621839850576026), ('Latitude', 0.24322825013808255), ('Longitude', 0.04488111207730316), ('Month', 0.06492306218396343), ('Year', 0.03337033756911995)]
Mean squared error is of 3123180449.255648
Mean absolute error:41067.655891964285
MAPE:0.3796041779563538
R2 score:0.5533066347071498
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:39:44
epoch 0  | loss: 1.54822 | val_0_rmse: 1.22721 | val_1_rmse: 1.31197 |  0:00:00s
epoch 1  | loss: 0.81245 | val_0_rmse: 0.88814 | val_1_rmse: 1.11597 |  0:00:00s
epoch 2  | loss: 0.58641 | val_0_rmse: 1.35732 | val_1_rmse: 0.79858 |  0:00:00s
epoch 3  | loss: 0.52358 | val_0_rmse: 0.8153  | val_1_rmse: 0.88052 |  0:00:00s
epoch 4  | loss: 0.48274 | val_0_rmse: 0.70445 | val_1_rmse: 0.81706 |  0:00:00s
epoch 5  | loss: 0.45611 | val_0_rmse: 0.68335 | val_1_rmse: 0.7787  |  0:00:00s
epoch 6  | loss: 0.44797 | val_0_rmse: 0.66612 | val_1_rmse: 0.77511 |  0:00:00s
epoch 7  | loss: 0.43467 | val_0_rmse: 0.67409 | val_1_rmse: 0.75899 |  0:00:01s
epoch 8  | loss: 0.42703 | val_0_rmse: 0.64724 | val_1_rmse: 0.71475 |  0:00:01s
epoch 9  | loss: 0.41296 | val_0_rmse: 0.65008 | val_1_rmse: 0.72412 |  0:00:01s
epoch 10 | loss: 0.4005  | val_0_rmse: 0.66382 | val_1_rmse: 0.74023 |  0:00:01s
epoch 11 | loss: 0.40848 | val_0_rmse: 0.7     | val_1_rmse: 0.79467 |  0:00:01s
epoch 12 | loss: 0.41942 | val_0_rmse: 0.68946 | val_1_rmse: 0.79293 |  0:00:01s
epoch 13 | loss: 0.40587 | val_0_rmse: 0.68523 | val_1_rmse: 0.76342 |  0:00:01s
epoch 14 | loss: 0.42371 | val_0_rmse: 0.69866 | val_1_rmse: 0.8012  |  0:00:01s
epoch 15 | loss: 0.40801 | val_0_rmse: 0.73834 | val_1_rmse: 0.84465 |  0:00:02s
epoch 16 | loss: 0.40174 | val_0_rmse: 0.7014  | val_1_rmse: 0.80435 |  0:00:02s
epoch 17 | loss: 0.41074 | val_0_rmse: 0.67087 | val_1_rmse: 0.76936 |  0:00:02s
epoch 18 | loss: 0.39099 | val_0_rmse: 0.66157 | val_1_rmse: 0.75561 |  0:00:02s
epoch 19 | loss: 0.39324 | val_0_rmse: 0.65275 | val_1_rmse: 0.74935 |  0:00:02s
epoch 20 | loss: 0.38515 | val_0_rmse: 0.67856 | val_1_rmse: 0.76662 |  0:00:02s
epoch 21 | loss: 0.3835  | val_0_rmse: 0.63318 | val_1_rmse: 0.72519 |  0:00:02s
epoch 22 | loss: 0.38859 | val_0_rmse: 0.62262 | val_1_rmse: 0.7056  |  0:00:03s
epoch 23 | loss: 0.39112 | val_0_rmse: 0.62164 | val_1_rmse: 0.70246 |  0:00:03s
epoch 24 | loss: 0.40507 | val_0_rmse: 0.62872 | val_1_rmse: 0.69524 |  0:00:03s
epoch 25 | loss: 0.3876  | val_0_rmse: 0.62746 | val_1_rmse: 0.69692 |  0:00:03s
epoch 26 | loss: 0.38774 | val_0_rmse: 0.61681 | val_1_rmse: 0.70439 |  0:00:03s
epoch 27 | loss: 0.37699 | val_0_rmse: 0.62161 | val_1_rmse: 0.72246 |  0:00:03s
epoch 28 | loss: 0.38312 | val_0_rmse: 0.61951 | val_1_rmse: 0.71124 |  0:00:03s
epoch 29 | loss: 0.37769 | val_0_rmse: 0.61002 | val_1_rmse: 0.70903 |  0:00:03s
epoch 30 | loss: 0.36317 | val_0_rmse: 0.59303 | val_1_rmse: 0.70265 |  0:00:04s
epoch 31 | loss: 0.37015 | val_0_rmse: 0.59249 | val_1_rmse: 0.7216  |  0:00:04s
epoch 32 | loss: 0.37648 | val_0_rmse: 0.60442 | val_1_rmse: 0.68815 |  0:00:04s
epoch 33 | loss: 0.38264 | val_0_rmse: 0.61695 | val_1_rmse: 0.69385 |  0:00:04s
epoch 34 | loss: 0.36884 | val_0_rmse: 0.61241 | val_1_rmse: 0.69675 |  0:00:04s
epoch 35 | loss: 0.36375 | val_0_rmse: 0.66213 | val_1_rmse: 0.76226 |  0:00:04s
epoch 36 | loss: 0.36952 | val_0_rmse: 0.63212 | val_1_rmse: 0.73943 |  0:00:04s
epoch 37 | loss: 0.35065 | val_0_rmse: 0.59595 | val_1_rmse: 0.70618 |  0:00:04s
epoch 38 | loss: 0.35776 | val_0_rmse: 0.59421 | val_1_rmse: 0.70432 |  0:00:05s
epoch 39 | loss: 0.34857 | val_0_rmse: 0.59459 | val_1_rmse: 0.72596 |  0:00:05s
epoch 40 | loss: 0.36152 | val_0_rmse: 0.59348 | val_1_rmse: 0.69222 |  0:00:05s
epoch 41 | loss: 0.36541 | val_0_rmse: 0.61593 | val_1_rmse: 0.69211 |  0:00:05s
epoch 42 | loss: 0.37154 | val_0_rmse: 0.65956 | val_1_rmse: 0.72288 |  0:00:05s
epoch 43 | loss: 0.36176 | val_0_rmse: 0.60878 | val_1_rmse: 0.66887 |  0:00:05s
epoch 44 | loss: 0.37975 | val_0_rmse: 0.61152 | val_1_rmse: 0.65882 |  0:00:05s
epoch 45 | loss: 0.36689 | val_0_rmse: 0.59388 | val_1_rmse: 0.65951 |  0:00:06s
epoch 46 | loss: 0.36971 | val_0_rmse: 0.61161 | val_1_rmse: 0.70285 |  0:00:06s
epoch 47 | loss: 0.35119 | val_0_rmse: 0.61389 | val_1_rmse: 0.71071 |  0:00:06s
epoch 48 | loss: 0.35064 | val_0_rmse: 0.588   | val_1_rmse: 0.67521 |  0:00:06s
epoch 49 | loss: 0.34357 | val_0_rmse: 0.58288 | val_1_rmse: 0.6663  |  0:00:06s
epoch 50 | loss: 0.34518 | val_0_rmse: 0.57966 | val_1_rmse: 0.6676  |  0:00:06s
epoch 51 | loss: 0.3347  | val_0_rmse: 0.59312 | val_1_rmse: 0.68154 |  0:00:06s
epoch 52 | loss: 0.33517 | val_0_rmse: 0.60296 | val_1_rmse: 0.69704 |  0:00:06s
epoch 53 | loss: 0.32985 | val_0_rmse: 0.57464 | val_1_rmse: 0.66583 |  0:00:07s
epoch 54 | loss: 0.33349 | val_0_rmse: 0.57736 | val_1_rmse: 0.68801 |  0:00:07s
epoch 55 | loss: 0.33827 | val_0_rmse: 0.56875 | val_1_rmse: 0.67865 |  0:00:07s
epoch 56 | loss: 0.33952 | val_0_rmse: 0.5682  | val_1_rmse: 0.68565 |  0:00:07s
epoch 57 | loss: 0.33927 | val_0_rmse: 0.58073 | val_1_rmse: 0.70906 |  0:00:07s
epoch 58 | loss: 0.32074 | val_0_rmse: 0.56423 | val_1_rmse: 0.687   |  0:00:07s
epoch 59 | loss: 0.32584 | val_0_rmse: 0.56122 | val_1_rmse: 0.67037 |  0:00:07s
epoch 60 | loss: 0.31485 | val_0_rmse: 0.56353 | val_1_rmse: 0.67478 |  0:00:08s
epoch 61 | loss: 0.3248  | val_0_rmse: 0.55899 | val_1_rmse: 0.72201 |  0:00:08s
epoch 62 | loss: 0.31334 | val_0_rmse: 0.55871 | val_1_rmse: 0.70282 |  0:00:08s
epoch 63 | loss: 0.33341 | val_0_rmse: 0.56666 | val_1_rmse: 0.72052 |  0:00:08s
epoch 64 | loss: 0.34969 | val_0_rmse: 0.59968 | val_1_rmse: 0.72643 |  0:00:08s
epoch 65 | loss: 0.3415  | val_0_rmse: 0.59416 | val_1_rmse: 0.72587 |  0:00:08s
epoch 66 | loss: 0.33311 | val_0_rmse: 0.57304 | val_1_rmse: 0.77416 |  0:00:08s
epoch 67 | loss: 0.33153 | val_0_rmse: 0.5785  | val_1_rmse: 0.74422 |  0:00:08s
epoch 68 | loss: 0.33097 | val_0_rmse: 0.56369 | val_1_rmse: 0.74018 |  0:00:09s
epoch 69 | loss: 0.33928 | val_0_rmse: 0.56185 | val_1_rmse: 0.73853 |  0:00:09s
epoch 70 | loss: 0.32171 | val_0_rmse: 0.58509 | val_1_rmse: 0.81633 |  0:00:09s
epoch 71 | loss: 0.3394  | val_0_rmse: 0.58287 | val_1_rmse: 0.81744 |  0:00:09s
epoch 72 | loss: 0.33636 | val_0_rmse: 0.56881 | val_1_rmse: 0.76181 |  0:00:09s
epoch 73 | loss: 0.334   | val_0_rmse: 0.57175 | val_1_rmse: 0.70015 |  0:00:09s
epoch 74 | loss: 0.33413 | val_0_rmse: 0.56768 | val_1_rmse: 0.67484 |  0:00:09s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.65882
Best weights from best epoch are automatically used!
ended training at: 08:39:54
Feature importance:
[('Area', 0.31571173681357784), ('Baths', 0.1398689792417477), ('Beds', 0.09459056888541621), ('Latitude', 0.21296180097548498), ('Longitude', 0.19051632894314358), ('Month', 0.02313609899967268), ('Year', 0.02321448614095701)]
Mean squared error is of 3398938412.4126773
Mean absolute error:43320.483217513734
MAPE:0.3563944915798277
R2 score:0.5349415758924478
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:39:54
epoch 0  | loss: 1.69369 | val_0_rmse: 1.60781 | val_1_rmse: 1.41454 |  0:00:00s
epoch 1  | loss: 1.03124 | val_0_rmse: 0.90309 | val_1_rmse: 0.87326 |  0:00:00s
epoch 2  | loss: 0.70374 | val_0_rmse: 0.80761 | val_1_rmse: 0.81303 |  0:00:00s
epoch 3  | loss: 0.58365 | val_0_rmse: 0.81873 | val_1_rmse: 0.82855 |  0:00:00s
epoch 4  | loss: 0.55705 | val_0_rmse: 0.80103 | val_1_rmse: 0.76338 |  0:00:00s
epoch 5  | loss: 0.56037 | val_0_rmse: 0.77284 | val_1_rmse: 0.74167 |  0:00:00s
epoch 6  | loss: 0.53176 | val_0_rmse: 0.78623 | val_1_rmse: 0.76258 |  0:00:00s
epoch 7  | loss: 0.50403 | val_0_rmse: 0.79817 | val_1_rmse: 0.75751 |  0:00:01s
epoch 8  | loss: 0.511   | val_0_rmse: 0.73662 | val_1_rmse: 0.703   |  0:00:01s
epoch 9  | loss: 0.49656 | val_0_rmse: 0.72913 | val_1_rmse: 0.68471 |  0:00:01s
epoch 10 | loss: 0.49031 | val_0_rmse: 0.71683 | val_1_rmse: 0.68182 |  0:00:01s
epoch 11 | loss: 0.47877 | val_0_rmse: 0.71941 | val_1_rmse: 0.68956 |  0:00:01s
epoch 12 | loss: 0.47178 | val_0_rmse: 0.7225  | val_1_rmse: 0.70067 |  0:00:01s
epoch 13 | loss: 0.47417 | val_0_rmse: 0.71384 | val_1_rmse: 0.68926 |  0:00:01s
epoch 14 | loss: 0.45337 | val_0_rmse: 0.69967 | val_1_rmse: 0.67811 |  0:00:01s
epoch 15 | loss: 0.44728 | val_0_rmse: 0.70529 | val_1_rmse: 0.67706 |  0:00:02s
epoch 16 | loss: 0.46166 | val_0_rmse: 0.71442 | val_1_rmse: 0.67605 |  0:00:02s
epoch 17 | loss: 0.45387 | val_0_rmse: 0.66964 | val_1_rmse: 0.65536 |  0:00:02s
epoch 18 | loss: 0.46696 | val_0_rmse: 0.66435 | val_1_rmse: 0.64509 |  0:00:02s
epoch 19 | loss: 0.4582  | val_0_rmse: 0.65903 | val_1_rmse: 0.64241 |  0:00:02s
epoch 20 | loss: 0.44523 | val_0_rmse: 0.67969 | val_1_rmse: 0.68937 |  0:00:02s
epoch 21 | loss: 0.45802 | val_0_rmse: 0.67623 | val_1_rmse: 0.68618 |  0:00:02s
epoch 22 | loss: 0.45595 | val_0_rmse: 0.66399 | val_1_rmse: 0.67024 |  0:00:03s
epoch 23 | loss: 0.45168 | val_0_rmse: 0.65783 | val_1_rmse: 0.66743 |  0:00:03s
epoch 24 | loss: 0.43213 | val_0_rmse: 0.65946 | val_1_rmse: 0.65981 |  0:00:03s
epoch 25 | loss: 0.44829 | val_0_rmse: 0.65907 | val_1_rmse: 0.64944 |  0:00:03s
epoch 26 | loss: 0.43504 | val_0_rmse: 0.65717 | val_1_rmse: 0.63938 |  0:00:03s
epoch 27 | loss: 0.43523 | val_0_rmse: 0.65057 | val_1_rmse: 0.63575 |  0:00:03s
epoch 28 | loss: 0.43538 | val_0_rmse: 0.64906 | val_1_rmse: 0.64171 |  0:00:03s
epoch 29 | loss: 0.43824 | val_0_rmse: 0.64856 | val_1_rmse: 0.63891 |  0:00:03s
epoch 30 | loss: 0.42916 | val_0_rmse: 0.6457  | val_1_rmse: 0.63948 |  0:00:04s
epoch 31 | loss: 0.43141 | val_0_rmse: 0.64555 | val_1_rmse: 0.64894 |  0:00:04s
epoch 32 | loss: 0.42435 | val_0_rmse: 0.64372 | val_1_rmse: 0.6393  |  0:00:04s
epoch 33 | loss: 0.41081 | val_0_rmse: 0.65497 | val_1_rmse: 0.64333 |  0:00:04s
epoch 34 | loss: 0.42169 | val_0_rmse: 0.65039 | val_1_rmse: 0.6437  |  0:00:04s
epoch 35 | loss: 0.42426 | val_0_rmse: 0.64644 | val_1_rmse: 0.64268 |  0:00:04s
epoch 36 | loss: 0.42191 | val_0_rmse: 0.66555 | val_1_rmse: 0.66084 |  0:00:04s
epoch 37 | loss: 0.4242  | val_0_rmse: 0.66189 | val_1_rmse: 0.65753 |  0:00:05s
epoch 38 | loss: 0.40879 | val_0_rmse: 0.64943 | val_1_rmse: 0.63674 |  0:00:05s
epoch 39 | loss: 0.41086 | val_0_rmse: 0.6412  | val_1_rmse: 0.63357 |  0:00:05s
epoch 40 | loss: 0.40905 | val_0_rmse: 0.63768 | val_1_rmse: 0.62743 |  0:00:05s
epoch 41 | loss: 0.40879 | val_0_rmse: 0.63691 | val_1_rmse: 0.62988 |  0:00:05s
epoch 42 | loss: 0.42093 | val_0_rmse: 0.6397  | val_1_rmse: 0.63881 |  0:00:05s
epoch 43 | loss: 0.39554 | val_0_rmse: 0.62219 | val_1_rmse: 0.63076 |  0:00:05s
epoch 44 | loss: 0.40412 | val_0_rmse: 0.62168 | val_1_rmse: 0.62896 |  0:00:05s
epoch 45 | loss: 0.36987 | val_0_rmse: 0.60835 | val_1_rmse: 0.61519 |  0:00:06s
epoch 46 | loss: 0.37539 | val_0_rmse: 0.60497 | val_1_rmse: 0.61154 |  0:00:06s
epoch 47 | loss: 0.36945 | val_0_rmse: 0.61155 | val_1_rmse: 0.63793 |  0:00:06s
epoch 48 | loss: 0.37213 | val_0_rmse: 0.61472 | val_1_rmse: 0.63546 |  0:00:06s
epoch 49 | loss: 0.37235 | val_0_rmse: 0.61392 | val_1_rmse: 0.6332  |  0:00:06s
epoch 50 | loss: 0.39593 | val_0_rmse: 0.62312 | val_1_rmse: 0.63646 |  0:00:06s
epoch 51 | loss: 0.38665 | val_0_rmse: 0.60849 | val_1_rmse: 0.62604 |  0:00:06s
epoch 52 | loss: 0.36983 | val_0_rmse: 0.63846 | val_1_rmse: 0.6603  |  0:00:07s
epoch 53 | loss: 0.37401 | val_0_rmse: 0.59994 | val_1_rmse: 0.63056 |  0:00:07s
epoch 54 | loss: 0.39845 | val_0_rmse: 0.59102 | val_1_rmse: 0.62069 |  0:00:07s
epoch 55 | loss: 0.38218 | val_0_rmse: 0.60569 | val_1_rmse: 0.62454 |  0:00:07s
epoch 56 | loss: 0.36006 | val_0_rmse: 0.59091 | val_1_rmse: 0.59468 |  0:00:07s
epoch 57 | loss: 0.37766 | val_0_rmse: 0.59753 | val_1_rmse: 0.59757 |  0:00:07s
epoch 58 | loss: 0.38906 | val_0_rmse: 0.59883 | val_1_rmse: 0.61315 |  0:00:07s
epoch 59 | loss: 0.37556 | val_0_rmse: 0.60935 | val_1_rmse: 0.62922 |  0:00:07s
epoch 60 | loss: 0.34831 | val_0_rmse: 0.60344 | val_1_rmse: 0.61858 |  0:00:08s
epoch 61 | loss: 0.36406 | val_0_rmse: 0.59194 | val_1_rmse: 0.60475 |  0:00:08s
epoch 62 | loss: 0.35186 | val_0_rmse: 0.61231 | val_1_rmse: 0.61818 |  0:00:08s
epoch 63 | loss: 0.34141 | val_0_rmse: 0.65122 | val_1_rmse: 0.65784 |  0:00:08s
epoch 64 | loss: 0.34813 | val_0_rmse: 0.61677 | val_1_rmse: 0.62236 |  0:00:08s
epoch 65 | loss: 0.34393 | val_0_rmse: 0.58771 | val_1_rmse: 0.59467 |  0:00:08s
epoch 66 | loss: 0.3425  | val_0_rmse: 0.57298 | val_1_rmse: 0.59092 |  0:00:08s
epoch 67 | loss: 0.34304 | val_0_rmse: 0.57928 | val_1_rmse: 0.59712 |  0:00:08s
epoch 68 | loss: 0.34675 | val_0_rmse: 0.57077 | val_1_rmse: 0.58405 |  0:00:09s
epoch 69 | loss: 0.33866 | val_0_rmse: 0.56977 | val_1_rmse: 0.57858 |  0:00:09s
epoch 70 | loss: 0.33252 | val_0_rmse: 0.57224 | val_1_rmse: 0.58337 |  0:00:09s
epoch 71 | loss: 0.33413 | val_0_rmse: 0.57312 | val_1_rmse: 0.58981 |  0:00:09s
epoch 72 | loss: 0.32292 | val_0_rmse: 0.58285 | val_1_rmse: 0.59784 |  0:00:09s
epoch 73 | loss: 0.33235 | val_0_rmse: 0.56664 | val_1_rmse: 0.58028 |  0:00:09s
epoch 74 | loss: 0.336   | val_0_rmse: 0.56855 | val_1_rmse: 0.58538 |  0:00:09s
epoch 75 | loss: 0.34578 | val_0_rmse: 0.57584 | val_1_rmse: 0.59489 |  0:00:09s
epoch 76 | loss: 0.33249 | val_0_rmse: 0.56746 | val_1_rmse: 0.58766 |  0:00:10s
epoch 77 | loss: 0.32221 | val_0_rmse: 0.67658 | val_1_rmse: 0.70017 |  0:00:10s
epoch 78 | loss: 0.32326 | val_0_rmse: 0.64818 | val_1_rmse: 0.66696 |  0:00:10s
epoch 79 | loss: 0.33782 | val_0_rmse: 0.56767 | val_1_rmse: 0.58191 |  0:00:10s
epoch 80 | loss: 0.32668 | val_0_rmse: 0.59002 | val_1_rmse: 0.61238 |  0:00:10s
epoch 81 | loss: 0.35337 | val_0_rmse: 0.5866  | val_1_rmse: 0.61557 |  0:00:10s
epoch 82 | loss: 0.33972 | val_0_rmse: 0.57186 | val_1_rmse: 0.60093 |  0:00:10s
epoch 83 | loss: 0.33801 | val_0_rmse: 0.57147 | val_1_rmse: 0.59246 |  0:00:11s
epoch 84 | loss: 0.3365  | val_0_rmse: 0.61835 | val_1_rmse: 0.63965 |  0:00:11s
epoch 85 | loss: 0.33798 | val_0_rmse: 0.6239  | val_1_rmse: 0.65091 |  0:00:11s
epoch 86 | loss: 0.35463 | val_0_rmse: 0.60661 | val_1_rmse: 0.63281 |  0:00:11s
epoch 87 | loss: 0.35382 | val_0_rmse: 0.59145 | val_1_rmse: 0.60854 |  0:00:11s
epoch 88 | loss: 0.35002 | val_0_rmse: 0.60589 | val_1_rmse: 0.62182 |  0:00:11s
epoch 89 | loss: 0.3718  | val_0_rmse: 0.67759 | val_1_rmse: 0.69195 |  0:00:11s
epoch 90 | loss: 0.36012 | val_0_rmse: 0.67955 | val_1_rmse: 0.69421 |  0:00:11s
epoch 91 | loss: 0.35175 | val_0_rmse: 0.61522 | val_1_rmse: 0.62055 |  0:00:12s
epoch 92 | loss: 0.35452 | val_0_rmse: 0.58349 | val_1_rmse: 0.58683 |  0:00:12s
epoch 93 | loss: 0.37722 | val_0_rmse: 0.5824  | val_1_rmse: 0.59493 |  0:00:12s
epoch 94 | loss: 0.35835 | val_0_rmse: 0.5778  | val_1_rmse: 0.59    |  0:00:12s
epoch 95 | loss: 0.36328 | val_0_rmse: 0.57424 | val_1_rmse: 0.58539 |  0:00:12s
epoch 96 | loss: 0.34169 | val_0_rmse: 0.57911 | val_1_rmse: 0.60215 |  0:00:12s
epoch 97 | loss: 0.33597 | val_0_rmse: 0.57097 | val_1_rmse: 0.59567 |  0:00:12s
epoch 98 | loss: 0.33427 | val_0_rmse: 0.563   | val_1_rmse: 0.57299 |  0:00:13s
epoch 99 | loss: 0.34365 | val_0_rmse: 0.57942 | val_1_rmse: 0.58379 |  0:00:13s
epoch 100| loss: 0.32813 | val_0_rmse: 0.64467 | val_1_rmse: 0.65342 |  0:00:13s
epoch 101| loss: 0.33669 | val_0_rmse: 0.68161 | val_1_rmse: 0.69853 |  0:00:13s
epoch 102| loss: 0.32976 | val_0_rmse: 0.66959 | val_1_rmse: 0.67932 |  0:00:13s
epoch 103| loss: 0.32087 | val_0_rmse: 0.60814 | val_1_rmse: 0.6204  |  0:00:13s
epoch 104| loss: 0.31496 | val_0_rmse: 0.65197 | val_1_rmse: 0.65962 |  0:00:13s
epoch 105| loss: 0.32936 | val_0_rmse: 0.68232 | val_1_rmse: 0.6877  |  0:00:13s
epoch 106| loss: 0.32261 | val_0_rmse: 0.6295  | val_1_rmse: 0.6387  |  0:00:14s
epoch 107| loss: 0.32229 | val_0_rmse: 0.56578 | val_1_rmse: 0.56855 |  0:00:14s
epoch 108| loss: 0.31776 | val_0_rmse: 0.55911 | val_1_rmse: 0.56766 |  0:00:14s
epoch 109| loss: 0.33923 | val_0_rmse: 0.59384 | val_1_rmse: 0.60089 |  0:00:14s
epoch 110| loss: 0.32472 | val_0_rmse: 0.58895 | val_1_rmse: 0.59637 |  0:00:14s
epoch 111| loss: 0.32394 | val_0_rmse: 0.60669 | val_1_rmse: 0.62661 |  0:00:14s
epoch 112| loss: 0.32408 | val_0_rmse: 0.65917 | val_1_rmse: 0.67245 |  0:00:14s
epoch 113| loss: 0.33723 | val_0_rmse: 0.59584 | val_1_rmse: 0.60612 |  0:00:14s
epoch 114| loss: 0.32233 | val_0_rmse: 0.58753 | val_1_rmse: 0.59572 |  0:00:15s
epoch 115| loss: 0.33015 | val_0_rmse: 0.66819 | val_1_rmse: 0.67255 |  0:00:15s
epoch 116| loss: 0.32137 | val_0_rmse: 0.69908 | val_1_rmse: 0.70549 |  0:00:15s
epoch 117| loss: 0.33259 | val_0_rmse: 0.70014 | val_1_rmse: 0.71201 |  0:00:15s
epoch 118| loss: 0.33541 | val_0_rmse: 0.63352 | val_1_rmse: 0.64308 |  0:00:15s
epoch 119| loss: 0.33068 | val_0_rmse: 0.57432 | val_1_rmse: 0.5872  |  0:00:15s
epoch 120| loss: 0.32184 | val_0_rmse: 0.5605  | val_1_rmse: 0.57078 |  0:00:15s
epoch 121| loss: 0.32656 | val_0_rmse: 0.56841 | val_1_rmse: 0.57346 |  0:00:15s
epoch 122| loss: 0.31864 | val_0_rmse: 0.57008 | val_1_rmse: 0.57143 |  0:00:16s
epoch 123| loss: 0.31227 | val_0_rmse: 0.5737  | val_1_rmse: 0.57585 |  0:00:16s
epoch 124| loss: 0.33571 | val_0_rmse: 0.54981 | val_1_rmse: 0.55597 |  0:00:16s
epoch 125| loss: 0.31654 | val_0_rmse: 0.57996 | val_1_rmse: 0.59585 |  0:00:16s
epoch 126| loss: 0.31528 | val_0_rmse: 0.55317 | val_1_rmse: 0.57051 |  0:00:16s
epoch 127| loss: 0.31516 | val_0_rmse: 0.54893 | val_1_rmse: 0.55913 |  0:00:16s
epoch 128| loss: 0.31255 | val_0_rmse: 0.55617 | val_1_rmse: 0.56291 |  0:00:16s
epoch 129| loss: 0.30578 | val_0_rmse: 0.54911 | val_1_rmse: 0.55274 |  0:00:17s
epoch 130| loss: 0.30952 | val_0_rmse: 0.57089 | val_1_rmse: 0.57957 |  0:00:17s
epoch 131| loss: 0.31156 | val_0_rmse: 0.59716 | val_1_rmse: 0.6171  |  0:00:17s
epoch 132| loss: 0.30808 | val_0_rmse: 0.56003 | val_1_rmse: 0.58715 |  0:00:17s
epoch 133| loss: 0.30651 | val_0_rmse: 0.54883 | val_1_rmse: 0.56574 |  0:00:17s
epoch 134| loss: 0.32239 | val_0_rmse: 0.56445 | val_1_rmse: 0.58571 |  0:00:17s
epoch 135| loss: 0.32474 | val_0_rmse: 0.60931 | val_1_rmse: 0.62684 |  0:00:17s
epoch 136| loss: 0.31048 | val_0_rmse: 0.56875 | val_1_rmse: 0.58393 |  0:00:17s
epoch 137| loss: 0.32079 | val_0_rmse: 0.55921 | val_1_rmse: 0.56937 |  0:00:18s
epoch 138| loss: 0.31211 | val_0_rmse: 0.54636 | val_1_rmse: 0.56092 |  0:00:18s
epoch 139| loss: 0.29788 | val_0_rmse: 0.55631 | val_1_rmse: 0.56607 |  0:00:18s
epoch 140| loss: 0.30676 | val_0_rmse: 0.579   | val_1_rmse: 0.57592 |  0:00:18s
epoch 141| loss: 0.33819 | val_0_rmse: 0.55742 | val_1_rmse: 0.55615 |  0:00:18s
epoch 142| loss: 0.34261 | val_0_rmse: 0.60696 | val_1_rmse: 0.60852 |  0:00:18s
epoch 143| loss: 0.33412 | val_0_rmse: 0.60642 | val_1_rmse: 0.62731 |  0:00:18s
epoch 144| loss: 0.33324 | val_0_rmse: 0.56424 | val_1_rmse: 0.59148 |  0:00:18s
epoch 145| loss: 0.32838 | val_0_rmse: 0.55956 | val_1_rmse: 0.58539 |  0:00:19s
epoch 146| loss: 0.33295 | val_0_rmse: 0.57281 | val_1_rmse: 0.59294 |  0:00:19s
epoch 147| loss: 0.31603 | val_0_rmse: 0.57654 | val_1_rmse: 0.58643 |  0:00:19s
epoch 148| loss: 0.31516 | val_0_rmse: 0.60126 | val_1_rmse: 0.61449 |  0:00:19s
epoch 149| loss: 0.3449  | val_0_rmse: 0.59454 | val_1_rmse: 0.59393 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.55274
Best weights from best epoch are automatically used!
ended training at: 08:40:13
Feature importance:
[('Area', 0.3503590251745433), ('Baths', 0.0632790011937465), ('Beds', 0.04515757084689169), ('Latitude', 0.3677048933105857), ('Longitude', 0.11161119388097764), ('Month', 0.05988987426826558), ('Year', 0.0019984413249895775)]
Mean squared error is of 2282395138.330422
Mean absolute error:35219.4281959478
MAPE:0.32646922605713097
R2 score:0.6810606412135324
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:40:13
epoch 0  | loss: 1.68679 | val_0_rmse: 1.03748 | val_1_rmse: 0.97921 |  0:00:00s
epoch 1  | loss: 0.91999 | val_0_rmse: 0.88893 | val_1_rmse: 0.92776 |  0:00:00s
epoch 2  | loss: 0.61488 | val_0_rmse: 0.84036 | val_1_rmse: 0.85633 |  0:00:00s
epoch 3  | loss: 0.53778 | val_0_rmse: 0.85162 | val_1_rmse: 0.81638 |  0:00:00s
epoch 4  | loss: 0.52259 | val_0_rmse: 0.78646 | val_1_rmse: 0.79699 |  0:00:00s
epoch 5  | loss: 0.5404  | val_0_rmse: 0.73068 | val_1_rmse: 0.78583 |  0:00:00s
epoch 6  | loss: 0.50094 | val_0_rmse: 0.72758 | val_1_rmse: 0.76429 |  0:00:01s
epoch 7  | loss: 0.48557 | val_0_rmse: 0.7881  | val_1_rmse: 0.7906  |  0:00:01s
epoch 8  | loss: 0.47103 | val_0_rmse: 0.70156 | val_1_rmse: 0.72747 |  0:00:01s
epoch 9  | loss: 0.46935 | val_0_rmse: 0.68126 | val_1_rmse: 0.70399 |  0:00:01s
epoch 10 | loss: 0.46432 | val_0_rmse: 0.68647 | val_1_rmse: 0.7108  |  0:00:01s
epoch 11 | loss: 0.44945 | val_0_rmse: 0.76754 | val_1_rmse: 0.74634 |  0:00:01s
epoch 12 | loss: 0.45955 | val_0_rmse: 0.76143 | val_1_rmse: 0.73906 |  0:00:01s
epoch 13 | loss: 0.44729 | val_0_rmse: 0.69518 | val_1_rmse: 0.7036  |  0:00:01s
epoch 14 | loss: 0.44578 | val_0_rmse: 0.67516 | val_1_rmse: 0.69859 |  0:00:02s
epoch 15 | loss: 0.43667 | val_0_rmse: 0.68227 | val_1_rmse: 0.69411 |  0:00:02s
epoch 16 | loss: 0.4476  | val_0_rmse: 0.67522 | val_1_rmse: 0.68729 |  0:00:02s
epoch 17 | loss: 0.4684  | val_0_rmse: 0.66325 | val_1_rmse: 0.68788 |  0:00:02s
epoch 18 | loss: 0.43303 | val_0_rmse: 0.66225 | val_1_rmse: 0.71055 |  0:00:02s
epoch 19 | loss: 0.45107 | val_0_rmse: 0.64706 | val_1_rmse: 0.69432 |  0:00:02s
epoch 20 | loss: 0.43841 | val_0_rmse: 0.64668 | val_1_rmse: 0.69045 |  0:00:02s
epoch 21 | loss: 0.43154 | val_0_rmse: 0.65028 | val_1_rmse: 0.68948 |  0:00:02s
epoch 22 | loss: 0.43311 | val_0_rmse: 0.64705 | val_1_rmse: 0.67326 |  0:00:03s
epoch 23 | loss: 0.42324 | val_0_rmse: 0.64491 | val_1_rmse: 0.67216 |  0:00:03s
epoch 24 | loss: 0.42221 | val_0_rmse: 0.64097 | val_1_rmse: 0.66668 |  0:00:03s
epoch 25 | loss: 0.43017 | val_0_rmse: 0.63623 | val_1_rmse: 0.6561  |  0:00:03s
epoch 26 | loss: 0.41658 | val_0_rmse: 0.64772 | val_1_rmse: 0.66292 |  0:00:03s
epoch 27 | loss: 0.42705 | val_0_rmse: 0.63372 | val_1_rmse: 0.66079 |  0:00:03s
epoch 28 | loss: 0.42255 | val_0_rmse: 0.63714 | val_1_rmse: 0.66455 |  0:00:03s
epoch 29 | loss: 0.43025 | val_0_rmse: 0.64776 | val_1_rmse: 0.65793 |  0:00:04s
epoch 30 | loss: 0.42042 | val_0_rmse: 0.64701 | val_1_rmse: 0.65911 |  0:00:04s
epoch 31 | loss: 0.41628 | val_0_rmse: 0.65272 | val_1_rmse: 0.66758 |  0:00:04s
epoch 32 | loss: 0.424   | val_0_rmse: 0.65975 | val_1_rmse: 0.66676 |  0:00:04s
epoch 33 | loss: 0.41181 | val_0_rmse: 0.65792 | val_1_rmse: 0.65886 |  0:00:04s
epoch 34 | loss: 0.41965 | val_0_rmse: 0.6444  | val_1_rmse: 0.65708 |  0:00:04s
epoch 35 | loss: 0.42505 | val_0_rmse: 0.64145 | val_1_rmse: 0.66141 |  0:00:04s
epoch 36 | loss: 0.42024 | val_0_rmse: 0.64417 | val_1_rmse: 0.64821 |  0:00:04s
epoch 37 | loss: 0.41762 | val_0_rmse: 0.64582 | val_1_rmse: 0.64757 |  0:00:05s
epoch 38 | loss: 0.41601 | val_0_rmse: 0.63842 | val_1_rmse: 0.66254 |  0:00:05s
epoch 39 | loss: 0.41247 | val_0_rmse: 0.64519 | val_1_rmse: 0.67416 |  0:00:05s
epoch 40 | loss: 0.42615 | val_0_rmse: 0.64835 | val_1_rmse: 0.6809  |  0:00:05s
epoch 41 | loss: 0.4289  | val_0_rmse: 0.63694 | val_1_rmse: 0.67497 |  0:00:05s
epoch 42 | loss: 0.4338  | val_0_rmse: 0.64284 | val_1_rmse: 0.67256 |  0:00:05s
epoch 43 | loss: 0.43298 | val_0_rmse: 0.64244 | val_1_rmse: 0.66716 |  0:00:05s
epoch 44 | loss: 0.4235  | val_0_rmse: 0.63912 | val_1_rmse: 0.65407 |  0:00:05s
epoch 45 | loss: 0.422   | val_0_rmse: 0.63782 | val_1_rmse: 0.65472 |  0:00:06s
epoch 46 | loss: 0.4214  | val_0_rmse: 0.64268 | val_1_rmse: 0.66181 |  0:00:06s
epoch 47 | loss: 0.416   | val_0_rmse: 0.64538 | val_1_rmse: 0.67409 |  0:00:06s
epoch 48 | loss: 0.41308 | val_0_rmse: 0.63873 | val_1_rmse: 0.67707 |  0:00:06s
epoch 49 | loss: 0.41959 | val_0_rmse: 0.64827 | val_1_rmse: 0.68971 |  0:00:06s
epoch 50 | loss: 0.41075 | val_0_rmse: 0.64845 | val_1_rmse: 0.67407 |  0:00:06s
epoch 51 | loss: 0.4236  | val_0_rmse: 0.64463 | val_1_rmse: 0.67534 |  0:00:06s
epoch 52 | loss: 0.40159 | val_0_rmse: 0.64626 | val_1_rmse: 0.69392 |  0:00:07s
epoch 53 | loss: 0.41252 | val_0_rmse: 0.64436 | val_1_rmse: 0.6883  |  0:00:07s
epoch 54 | loss: 0.42942 | val_0_rmse: 0.63198 | val_1_rmse: 0.67491 |  0:00:07s
epoch 55 | loss: 0.41423 | val_0_rmse: 0.64841 | val_1_rmse: 0.69179 |  0:00:07s
epoch 56 | loss: 0.41475 | val_0_rmse: 0.62901 | val_1_rmse: 0.66256 |  0:00:07s
epoch 57 | loss: 0.40564 | val_0_rmse: 0.6318  | val_1_rmse: 0.65719 |  0:00:07s
epoch 58 | loss: 0.4051  | val_0_rmse: 0.63833 | val_1_rmse: 0.66376 |  0:00:07s
epoch 59 | loss: 0.402   | val_0_rmse: 0.63437 | val_1_rmse: 0.66239 |  0:00:07s
epoch 60 | loss: 0.40237 | val_0_rmse: 0.63352 | val_1_rmse: 0.65745 |  0:00:08s
epoch 61 | loss: 0.41339 | val_0_rmse: 0.63642 | val_1_rmse: 0.65945 |  0:00:08s
epoch 62 | loss: 0.42349 | val_0_rmse: 0.64174 | val_1_rmse: 0.66439 |  0:00:08s
epoch 63 | loss: 0.42135 | val_0_rmse: 0.64982 | val_1_rmse: 0.66063 |  0:00:08s
epoch 64 | loss: 0.42134 | val_0_rmse: 0.65463 | val_1_rmse: 0.65543 |  0:00:08s
epoch 65 | loss: 0.41603 | val_0_rmse: 0.64761 | val_1_rmse: 0.67435 |  0:00:08s
epoch 66 | loss: 0.42593 | val_0_rmse: 0.64509 | val_1_rmse: 0.67294 |  0:00:08s
epoch 67 | loss: 0.42605 | val_0_rmse: 0.64552 | val_1_rmse: 0.65796 |  0:00:09s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.64757
Best weights from best epoch are automatically used!
ended training at: 08:40:23
Feature importance:
[('Area', 0.4074318983683796), ('Baths', 0.14630145186714844), ('Beds', 0.06866064561988826), ('Latitude', 0.11616246809406786), ('Longitude', 0.11099325859140358), ('Month', 0.14808935238561857), ('Year', 0.002360925073493683)]
Mean squared error is of 3605947710.2448745
Mean absolute error:41939.216958035715
MAPE:0.42263029466245705
R2 score:0.509704381521596
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:40:23
epoch 0  | loss: 1.61024 | val_0_rmse: 1.04622 | val_1_rmse: 0.98441 |  0:00:00s
epoch 1  | loss: 0.96737 | val_0_rmse: 1.10323 | val_1_rmse: 1.04582 |  0:00:00s
epoch 2  | loss: 0.7129  | val_0_rmse: 0.97212 | val_1_rmse: 3.40612 |  0:00:00s
epoch 3  | loss: 0.57614 | val_0_rmse: 0.89564 | val_1_rmse: 0.83736 |  0:00:00s
epoch 4  | loss: 0.53607 | val_0_rmse: 0.8398  | val_1_rmse: 0.76395 |  0:00:00s
epoch 5  | loss: 0.5151  | val_0_rmse: 0.81037 | val_1_rmse: 0.74446 |  0:00:00s
epoch 6  | loss: 0.48921 | val_0_rmse: 0.78723 | val_1_rmse: 0.74366 |  0:00:00s
epoch 7  | loss: 0.45638 | val_0_rmse: 0.74981 | val_1_rmse: 0.72161 |  0:00:01s
epoch 8  | loss: 0.4714  | val_0_rmse: 0.74155 | val_1_rmse: 0.74064 |  0:00:01s
epoch 9  | loss: 0.45222 | val_0_rmse: 0.69882 | val_1_rmse: 0.71445 |  0:00:01s
epoch 10 | loss: 0.44036 | val_0_rmse: 0.68162 | val_1_rmse: 0.70285 |  0:00:01s
epoch 11 | loss: 0.44835 | val_0_rmse: 0.67052 | val_1_rmse: 0.70715 |  0:00:01s
epoch 12 | loss: 0.43903 | val_0_rmse: 0.66655 | val_1_rmse: 0.72279 |  0:00:01s
epoch 13 | loss: 0.43043 | val_0_rmse: 0.65402 | val_1_rmse: 0.69962 |  0:00:01s
epoch 14 | loss: 0.41247 | val_0_rmse: 0.64951 | val_1_rmse: 0.68826 |  0:00:02s
epoch 15 | loss: 0.41402 | val_0_rmse: 0.63759 | val_1_rmse: 0.69468 |  0:00:02s
epoch 16 | loss: 0.40909 | val_0_rmse: 0.6274  | val_1_rmse: 0.68476 |  0:00:02s
epoch 17 | loss: 0.40731 | val_0_rmse: 0.63207 | val_1_rmse: 0.68839 |  0:00:02s
epoch 18 | loss: 0.41216 | val_0_rmse: 0.6552  | val_1_rmse: 0.74595 |  0:00:02s
epoch 19 | loss: 0.40598 | val_0_rmse: 0.67568 | val_1_rmse: 0.76875 |  0:00:02s
epoch 20 | loss: 0.40673 | val_0_rmse: 0.64308 | val_1_rmse: 0.70752 |  0:00:02s
epoch 21 | loss: 0.40842 | val_0_rmse: 0.63231 | val_1_rmse: 0.74684 |  0:00:02s
epoch 22 | loss: 0.40034 | val_0_rmse: 0.63901 | val_1_rmse: 0.78085 |  0:00:03s
epoch 23 | loss: 0.39446 | val_0_rmse: 0.62331 | val_1_rmse: 0.77752 |  0:00:03s
epoch 24 | loss: 0.39262 | val_0_rmse: 0.62864 | val_1_rmse: 0.76547 |  0:00:03s
epoch 25 | loss: 0.38144 | val_0_rmse: 0.65083 | val_1_rmse: 0.76626 |  0:00:03s
epoch 26 | loss: 0.39544 | val_0_rmse: 0.63927 | val_1_rmse: 0.75391 |  0:00:03s
epoch 27 | loss: 0.39183 | val_0_rmse: 0.63293 | val_1_rmse: 0.74834 |  0:00:03s
epoch 28 | loss: 0.37431 | val_0_rmse: 0.65828 | val_1_rmse: 0.76924 |  0:00:03s
epoch 29 | loss: 0.39739 | val_0_rmse: 0.62476 | val_1_rmse: 0.72728 |  0:00:03s
epoch 30 | loss: 0.3793  | val_0_rmse: 0.61362 | val_1_rmse: 0.69155 |  0:00:04s
epoch 31 | loss: 0.36181 | val_0_rmse: 0.61301 | val_1_rmse: 0.6956  |  0:00:04s
epoch 32 | loss: 0.37543 | val_0_rmse: 0.60874 | val_1_rmse: 0.69321 |  0:00:04s
epoch 33 | loss: 0.38141 | val_0_rmse: 0.61533 | val_1_rmse: 0.69074 |  0:00:04s
epoch 34 | loss: 0.38867 | val_0_rmse: 0.61117 | val_1_rmse: 0.70603 |  0:00:04s
epoch 35 | loss: 0.3669  | val_0_rmse: 0.62976 | val_1_rmse: 0.71026 |  0:00:04s
epoch 36 | loss: 0.37696 | val_0_rmse: 0.62796 | val_1_rmse: 0.699   |  0:00:04s
epoch 37 | loss: 0.37265 | val_0_rmse: 0.61858 | val_1_rmse: 0.68285 |  0:00:04s
epoch 38 | loss: 0.37413 | val_0_rmse: 0.59329 | val_1_rmse: 0.64252 |  0:00:05s
epoch 39 | loss: 0.36579 | val_0_rmse: 0.59391 | val_1_rmse: 0.64049 |  0:00:05s
epoch 40 | loss: 0.37092 | val_0_rmse: 0.59672 | val_1_rmse: 0.65131 |  0:00:05s
epoch 41 | loss: 0.36195 | val_0_rmse: 0.59421 | val_1_rmse: 0.64893 |  0:00:05s
epoch 42 | loss: 0.36251 | val_0_rmse: 0.58603 | val_1_rmse: 0.63354 |  0:00:05s
epoch 43 | loss: 0.36392 | val_0_rmse: 0.58505 | val_1_rmse: 0.8983  |  0:00:05s
epoch 44 | loss: 0.35259 | val_0_rmse: 0.59448 | val_1_rmse: 1.01558 |  0:00:05s
epoch 45 | loss: 0.35525 | val_0_rmse: 0.60388 | val_1_rmse: 0.6391  |  0:00:06s
epoch 46 | loss: 0.37005 | val_0_rmse: 0.59951 | val_1_rmse: 0.63844 |  0:00:06s
epoch 47 | loss: 0.37883 | val_0_rmse: 0.60622 | val_1_rmse: 0.77981 |  0:00:06s
epoch 48 | loss: 0.37955 | val_0_rmse: 0.60597 | val_1_rmse: 0.63267 |  0:00:06s
epoch 49 | loss: 0.36878 | val_0_rmse: 0.59751 | val_1_rmse: 0.63096 |  0:00:06s
epoch 50 | loss: 0.35049 | val_0_rmse: 0.59161 | val_1_rmse: 0.63566 |  0:00:06s
epoch 51 | loss: 0.35304 | val_0_rmse: 0.58859 | val_1_rmse: 0.64022 |  0:00:06s
epoch 52 | loss: 0.35408 | val_0_rmse: 0.58618 | val_1_rmse: 0.6381  |  0:00:07s
epoch 53 | loss: 0.33993 | val_0_rmse: 0.58239 | val_1_rmse: 0.62287 |  0:00:07s
epoch 54 | loss: 0.35614 | val_0_rmse: 0.58987 | val_1_rmse: 0.63004 |  0:00:07s
epoch 55 | loss: 0.35041 | val_0_rmse: 0.58527 | val_1_rmse: 0.61996 |  0:00:07s
epoch 56 | loss: 0.341   | val_0_rmse: 0.58825 | val_1_rmse: 0.62086 |  0:00:07s
epoch 57 | loss: 0.34814 | val_0_rmse: 0.57837 | val_1_rmse: 0.61666 |  0:00:07s
epoch 58 | loss: 0.36498 | val_0_rmse: 0.57739 | val_1_rmse: 0.62079 |  0:00:07s
epoch 59 | loss: 0.35103 | val_0_rmse: 0.59465 | val_1_rmse: 0.62436 |  0:00:07s
epoch 60 | loss: 0.35586 | val_0_rmse: 0.60134 | val_1_rmse: 0.62087 |  0:00:08s
epoch 61 | loss: 0.34993 | val_0_rmse: 0.60538 | val_1_rmse: 0.61226 |  0:00:08s
epoch 62 | loss: 0.35321 | val_0_rmse: 0.62019 | val_1_rmse: 0.6225  |  0:00:08s
epoch 63 | loss: 0.34956 | val_0_rmse: 0.60016 | val_1_rmse: 0.60314 |  0:00:08s
epoch 64 | loss: 0.3404  | val_0_rmse: 0.59426 | val_1_rmse: 0.60304 |  0:00:08s
epoch 65 | loss: 0.33584 | val_0_rmse: 0.57694 | val_1_rmse: 0.60282 |  0:00:08s
epoch 66 | loss: 0.33569 | val_0_rmse: 0.57559 | val_1_rmse: 0.59799 |  0:00:08s
epoch 67 | loss: 0.35634 | val_0_rmse: 0.58184 | val_1_rmse: 0.60792 |  0:00:09s
epoch 68 | loss: 0.34344 | val_0_rmse: 0.5739  | val_1_rmse: 0.59431 |  0:00:09s
epoch 69 | loss: 0.33064 | val_0_rmse: 0.57596 | val_1_rmse: 0.57204 |  0:00:09s
epoch 70 | loss: 0.33081 | val_0_rmse: 0.57915 | val_1_rmse: 0.57959 |  0:00:09s
epoch 71 | loss: 0.33604 | val_0_rmse: 0.56455 | val_1_rmse: 0.59197 |  0:00:09s
epoch 72 | loss: 0.33628 | val_0_rmse: 0.56637 | val_1_rmse: 0.59824 |  0:00:09s
epoch 73 | loss: 0.32542 | val_0_rmse: 0.57625 | val_1_rmse: 0.5936  |  0:00:09s
epoch 74 | loss: 0.32272 | val_0_rmse: 0.56361 | val_1_rmse: 0.59322 |  0:00:09s
epoch 75 | loss: 0.33532 | val_0_rmse: 0.57246 | val_1_rmse: 0.61152 |  0:00:10s
epoch 76 | loss: 0.33913 | val_0_rmse: 0.57067 | val_1_rmse: 0.6088  |  0:00:10s
epoch 77 | loss: 0.34406 | val_0_rmse: 0.59329 | val_1_rmse: 0.62582 |  0:00:10s
epoch 78 | loss: 0.32909 | val_0_rmse: 0.71256 | val_1_rmse: 0.72132 |  0:00:10s
epoch 79 | loss: 0.32682 | val_0_rmse: 0.73016 | val_1_rmse: 0.73721 |  0:00:10s
epoch 80 | loss: 0.34568 | val_0_rmse: 0.62991 | val_1_rmse: 0.66723 |  0:00:10s
epoch 81 | loss: 0.34259 | val_0_rmse: 0.57879 | val_1_rmse: 0.604   |  0:00:10s
epoch 82 | loss: 0.33958 | val_0_rmse: 0.56254 | val_1_rmse: 0.59563 |  0:00:10s
epoch 83 | loss: 0.32935 | val_0_rmse: 0.5633  | val_1_rmse: 0.58908 |  0:00:11s
epoch 84 | loss: 0.32541 | val_0_rmse: 0.62853 | val_1_rmse: 0.62373 |  0:00:11s
epoch 85 | loss: 0.33762 | val_0_rmse: 0.68606 | val_1_rmse: 0.67088 |  0:00:11s
epoch 86 | loss: 0.32962 | val_0_rmse: 0.56918 | val_1_rmse: 0.59027 |  0:00:11s
epoch 87 | loss: 0.32497 | val_0_rmse: 0.64051 | val_1_rmse: 0.68151 |  0:00:11s
epoch 88 | loss: 0.3368  | val_0_rmse: 0.67793 | val_1_rmse: 0.70832 |  0:00:11s
epoch 89 | loss: 0.33189 | val_0_rmse: 0.57026 | val_1_rmse: 0.60908 |  0:00:11s
epoch 90 | loss: 0.32971 | val_0_rmse: 0.56152 | val_1_rmse: 0.60388 |  0:00:11s
epoch 91 | loss: 0.33904 | val_0_rmse: 0.61963 | val_1_rmse: 0.66766 |  0:00:12s
epoch 92 | loss: 0.33311 | val_0_rmse: 0.7144  | val_1_rmse: 0.73974 |  0:00:12s
epoch 93 | loss: 0.32076 | val_0_rmse: 0.70459 | val_1_rmse: 0.73185 |  0:00:12s
epoch 94 | loss: 0.32325 | val_0_rmse: 0.56932 | val_1_rmse: 0.6207  |  0:00:12s
epoch 95 | loss: 0.31726 | val_0_rmse: 0.58953 | val_1_rmse: 0.64039 |  0:00:12s
epoch 96 | loss: 0.31628 | val_0_rmse: 0.57237 | val_1_rmse: 0.62229 |  0:00:12s
epoch 97 | loss: 0.31089 | val_0_rmse: 0.56127 | val_1_rmse: 0.6085  |  0:00:12s
epoch 98 | loss: 0.3244  | val_0_rmse: 0.54849 | val_1_rmse: 0.59373 |  0:00:13s
epoch 99 | loss: 0.31297 | val_0_rmse: 0.56597 | val_1_rmse: 0.60485 |  0:00:13s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.57204
Best weights from best epoch are automatically used!
ended training at: 08:40:36
Feature importance:
[('Area', 0.25852791145904896), ('Baths', 0.10068176538035793), ('Beds', 0.08936715286108297), ('Latitude', 0.41482129278011787), ('Longitude', 0.08850286042227722), ('Month', 0.04526068715098479), ('Year', 0.0028383299461303157)]
Mean squared error is of 2862493934.0639057
Mean absolute error:39027.495766552194
MAPE:0.34278346844596713
R2 score:0.6116505759303574
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:40:36
epoch 0  | loss: 1.18957 | val_0_rmse: 1.10596 | val_1_rmse: 1.11532 |  0:00:00s
epoch 1  | loss: 0.64417 | val_0_rmse: 0.84721 | val_1_rmse: 0.82108 |  0:00:00s
epoch 2  | loss: 0.56621 | val_0_rmse: 0.75691 | val_1_rmse: 0.76478 |  0:00:00s
epoch 3  | loss: 0.51195 | val_0_rmse: 0.72965 | val_1_rmse: 0.76408 |  0:00:01s
epoch 4  | loss: 0.49415 | val_0_rmse: 0.76809 | val_1_rmse: 0.8057  |  0:00:01s
epoch 5  | loss: 0.47806 | val_0_rmse: 0.71298 | val_1_rmse: 0.7434  |  0:00:01s
epoch 6  | loss: 0.47632 | val_0_rmse: 0.73812 | val_1_rmse: 0.74488 |  0:00:02s
epoch 7  | loss: 0.4765  | val_0_rmse: 0.7004  | val_1_rmse: 0.72383 |  0:00:02s
epoch 8  | loss: 0.47599 | val_0_rmse: 0.68466 | val_1_rmse: 0.71824 |  0:00:02s
epoch 9  | loss: 0.46717 | val_0_rmse: 0.68051 | val_1_rmse: 0.72427 |  0:00:02s
epoch 10 | loss: 0.4694  | val_0_rmse: 0.6698  | val_1_rmse: 0.7073  |  0:00:03s
epoch 11 | loss: 0.46492 | val_0_rmse: 0.66649 | val_1_rmse: 0.70131 |  0:00:03s
epoch 12 | loss: 0.45079 | val_0_rmse: 0.66841 | val_1_rmse: 0.70719 |  0:00:03s
epoch 13 | loss: 0.45408 | val_0_rmse: 0.67706 | val_1_rmse: 0.72137 |  0:00:04s
epoch 14 | loss: 0.45076 | val_0_rmse: 0.65984 | val_1_rmse: 0.69935 |  0:00:04s
epoch 15 | loss: 0.44457 | val_0_rmse: 0.65597 | val_1_rmse: 0.68616 |  0:00:04s
epoch 16 | loss: 0.44543 | val_0_rmse: 0.66904 | val_1_rmse: 0.69534 |  0:00:04s
epoch 17 | loss: 0.44249 | val_0_rmse: 0.66548 | val_1_rmse: 0.69308 |  0:00:05s
epoch 18 | loss: 0.44916 | val_0_rmse: 0.65995 | val_1_rmse: 0.69063 |  0:00:05s
epoch 19 | loss: 0.44752 | val_0_rmse: 0.67758 | val_1_rmse: 0.7076  |  0:00:05s
epoch 20 | loss: 0.44319 | val_0_rmse: 0.66693 | val_1_rmse: 0.70173 |  0:00:06s
epoch 21 | loss: 0.438   | val_0_rmse: 0.67227 | val_1_rmse: 0.72911 |  0:00:06s
epoch 22 | loss: 0.4284  | val_0_rmse: 0.64922 | val_1_rmse: 0.70451 |  0:00:06s
epoch 23 | loss: 0.42833 | val_0_rmse: 0.71164 | val_1_rmse: 0.74689 |  0:00:07s
epoch 24 | loss: 0.43137 | val_0_rmse: 0.69178 | val_1_rmse: 0.75317 |  0:00:07s
epoch 25 | loss: 0.4244  | val_0_rmse: 0.67751 | val_1_rmse: 0.73459 |  0:00:07s
epoch 26 | loss: 0.41909 | val_0_rmse: 0.66288 | val_1_rmse: 0.72662 |  0:00:07s
epoch 27 | loss: 0.42776 | val_0_rmse: 0.66682 | val_1_rmse: 0.72107 |  0:00:08s
epoch 28 | loss: 0.41597 | val_0_rmse: 0.64376 | val_1_rmse: 0.68934 |  0:00:08s
epoch 29 | loss: 0.41405 | val_0_rmse: 0.63446 | val_1_rmse: 0.68481 |  0:00:08s
epoch 30 | loss: 0.38801 | val_0_rmse: 0.64733 | val_1_rmse: 0.69412 |  0:00:08s
epoch 31 | loss: 0.40716 | val_0_rmse: 0.65832 | val_1_rmse: 0.70416 |  0:00:09s
epoch 32 | loss: 0.40222 | val_0_rmse: 0.64186 | val_1_rmse: 0.68581 |  0:00:09s
epoch 33 | loss: 0.38342 | val_0_rmse: 0.63778 | val_1_rmse: 0.69219 |  0:00:09s
epoch 34 | loss: 0.3791  | val_0_rmse: 0.61063 | val_1_rmse: 0.67449 |  0:00:10s
epoch 35 | loss: 0.38083 | val_0_rmse: 0.62796 | val_1_rmse: 0.68431 |  0:00:10s
epoch 36 | loss: 0.38412 | val_0_rmse: 0.65403 | val_1_rmse: 0.70705 |  0:00:10s
epoch 37 | loss: 0.39304 | val_0_rmse: 0.60717 | val_1_rmse: 0.65791 |  0:00:10s
epoch 38 | loss: 0.38311 | val_0_rmse: 0.69984 | val_1_rmse: 0.75519 |  0:00:11s
epoch 39 | loss: 0.39543 | val_0_rmse: 0.67236 | val_1_rmse: 0.72377 |  0:00:11s
epoch 40 | loss: 0.38817 | val_0_rmse: 0.68831 | val_1_rmse: 0.77954 |  0:00:11s
epoch 41 | loss: 0.3803  | val_0_rmse: 0.63155 | val_1_rmse: 0.67526 |  0:00:12s
epoch 42 | loss: 0.39044 | val_0_rmse: 0.65679 | val_1_rmse: 0.68827 |  0:00:12s
epoch 43 | loss: 0.3817  | val_0_rmse: 0.65663 | val_1_rmse: 0.69145 |  0:00:12s
epoch 44 | loss: 0.39145 | val_0_rmse: 0.64242 | val_1_rmse: 0.68921 |  0:00:13s
epoch 45 | loss: 0.39422 | val_0_rmse: 0.61455 | val_1_rmse: 0.65516 |  0:00:13s
epoch 46 | loss: 0.3858  | val_0_rmse: 0.60965 | val_1_rmse: 0.64516 |  0:00:13s
epoch 47 | loss: 0.367   | val_0_rmse: 0.61391 | val_1_rmse: 0.65081 |  0:00:13s
epoch 48 | loss: 0.38021 | val_0_rmse: 0.62163 | val_1_rmse: 0.66356 |  0:00:14s
epoch 49 | loss: 0.38214 | val_0_rmse: 0.6072  | val_1_rmse: 0.64563 |  0:00:14s
epoch 50 | loss: 0.37476 | val_0_rmse: 0.61515 | val_1_rmse: 0.65762 |  0:00:14s
epoch 51 | loss: 0.37854 | val_0_rmse: 0.59879 | val_1_rmse: 0.64472 |  0:00:15s
epoch 52 | loss: 0.36697 | val_0_rmse: 0.59557 | val_1_rmse: 0.63965 |  0:00:15s
epoch 53 | loss: 0.36571 | val_0_rmse: 0.58685 | val_1_rmse: 0.62488 |  0:00:15s
epoch 54 | loss: 0.37248 | val_0_rmse: 0.61846 | val_1_rmse: 0.66046 |  0:00:15s
epoch 55 | loss: 0.37074 | val_0_rmse: 0.60241 | val_1_rmse: 0.63725 |  0:00:16s
epoch 56 | loss: 0.37746 | val_0_rmse: 0.61899 | val_1_rmse: 0.65706 |  0:00:16s
epoch 57 | loss: 0.36135 | val_0_rmse: 0.59274 | val_1_rmse: 0.62974 |  0:00:16s
epoch 58 | loss: 0.35571 | val_0_rmse: 0.59602 | val_1_rmse: 0.63114 |  0:00:17s
epoch 59 | loss: 0.35559 | val_0_rmse: 0.59841 | val_1_rmse: 0.63958 |  0:00:17s
epoch 60 | loss: 0.35843 | val_0_rmse: 0.6256  | val_1_rmse: 0.66429 |  0:00:17s
epoch 61 | loss: 0.34643 | val_0_rmse: 0.64094 | val_1_rmse: 0.68468 |  0:00:17s
epoch 62 | loss: 0.34837 | val_0_rmse: 0.60274 | val_1_rmse: 0.64958 |  0:00:18s
epoch 63 | loss: 0.33941 | val_0_rmse: 0.59631 | val_1_rmse: 0.63772 |  0:00:18s
epoch 64 | loss: 0.34417 | val_0_rmse: 0.58891 | val_1_rmse: 0.62863 |  0:00:18s
epoch 65 | loss: 0.35403 | val_0_rmse: 0.586   | val_1_rmse: 0.6283  |  0:00:19s
epoch 66 | loss: 0.35275 | val_0_rmse: 0.58057 | val_1_rmse: 0.63482 |  0:00:19s
epoch 67 | loss: 0.34941 | val_0_rmse: 0.61062 | val_1_rmse: 0.66932 |  0:00:19s
epoch 68 | loss: 0.34734 | val_0_rmse: 0.60667 | val_1_rmse: 0.6561  |  0:00:19s
epoch 69 | loss: 0.33853 | val_0_rmse: 0.63849 | val_1_rmse: 0.66814 |  0:00:20s
epoch 70 | loss: 0.35475 | val_0_rmse: 0.62995 | val_1_rmse: 0.66544 |  0:00:20s
epoch 71 | loss: 0.34973 | val_0_rmse: 0.64433 | val_1_rmse: 0.69257 |  0:00:20s
epoch 72 | loss: 0.35056 | val_0_rmse: 0.63236 | val_1_rmse: 0.68141 |  0:00:21s
epoch 73 | loss: 0.33865 | val_0_rmse: 0.60577 | val_1_rmse: 0.65284 |  0:00:21s
epoch 74 | loss: 0.34384 | val_0_rmse: 0.5949  | val_1_rmse: 0.63678 |  0:00:21s
epoch 75 | loss: 0.34837 | val_0_rmse: 0.5933  | val_1_rmse: 0.63539 |  0:00:21s
epoch 76 | loss: 0.34077 | val_0_rmse: 0.60387 | val_1_rmse: 0.63945 |  0:00:22s
epoch 77 | loss: 0.35089 | val_0_rmse: 0.6048  | val_1_rmse: 0.64154 |  0:00:22s
epoch 78 | loss: 0.34282 | val_0_rmse: 0.61394 | val_1_rmse: 0.66325 |  0:00:22s
epoch 79 | loss: 0.33386 | val_0_rmse: 0.60876 | val_1_rmse: 0.64678 |  0:00:23s
epoch 80 | loss: 0.33577 | val_0_rmse: 0.58238 | val_1_rmse: 0.622   |  0:00:23s
epoch 81 | loss: 0.33937 | val_0_rmse: 0.59822 | val_1_rmse: 0.64022 |  0:00:23s
epoch 82 | loss: 0.34283 | val_0_rmse: 0.60617 | val_1_rmse: 0.65317 |  0:00:23s
epoch 83 | loss: 0.34116 | val_0_rmse: 0.59882 | val_1_rmse: 0.64039 |  0:00:24s
epoch 84 | loss: 0.34193 | val_0_rmse: 0.60342 | val_1_rmse: 0.64608 |  0:00:24s
epoch 85 | loss: 0.3354  | val_0_rmse: 0.60146 | val_1_rmse: 0.6459  |  0:00:24s
epoch 86 | loss: 0.34461 | val_0_rmse: 0.6112  | val_1_rmse: 0.67149 |  0:00:25s
epoch 87 | loss: 0.35091 | val_0_rmse: 0.60951 | val_1_rmse: 0.6629  |  0:00:25s
epoch 88 | loss: 0.35149 | val_0_rmse: 0.65863 | val_1_rmse: 0.70767 |  0:00:25s
epoch 89 | loss: 0.33972 | val_0_rmse: 0.61444 | val_1_rmse: 0.6573  |  0:00:25s
epoch 90 | loss: 0.3332  | val_0_rmse: 0.6128  | val_1_rmse: 0.66044 |  0:00:26s
epoch 91 | loss: 0.34245 | val_0_rmse: 0.59578 | val_1_rmse: 0.64103 |  0:00:26s
epoch 92 | loss: 0.36302 | val_0_rmse: 0.62613 | val_1_rmse: 0.67403 |  0:00:26s
epoch 93 | loss: 0.35778 | val_0_rmse: 0.62    | val_1_rmse: 0.65936 |  0:00:27s
epoch 94 | loss: 0.34099 | val_0_rmse: 0.59438 | val_1_rmse: 0.63688 |  0:00:27s
epoch 95 | loss: 0.34464 | val_0_rmse: 0.64107 | val_1_rmse: 0.68426 |  0:00:27s
epoch 96 | loss: 0.34295 | val_0_rmse: 0.6175  | val_1_rmse: 0.65491 |  0:00:27s
epoch 97 | loss: 0.3429  | val_0_rmse: 0.60959 | val_1_rmse: 0.65632 |  0:00:28s
epoch 98 | loss: 0.34818 | val_0_rmse: 0.61481 | val_1_rmse: 0.67823 |  0:00:28s
epoch 99 | loss: 0.34827 | val_0_rmse: 0.5912  | val_1_rmse: 0.63612 |  0:00:28s
epoch 100| loss: 0.33768 | val_0_rmse: 0.60919 | val_1_rmse: 0.66543 |  0:00:29s
epoch 101| loss: 0.33985 | val_0_rmse: 0.5893  | val_1_rmse: 0.64257 |  0:00:29s
epoch 102| loss: 0.33247 | val_0_rmse: 0.62259 | val_1_rmse: 0.66775 |  0:00:29s
epoch 103| loss: 0.33597 | val_0_rmse: 0.59774 | val_1_rmse: 0.64326 |  0:00:29s
epoch 104| loss: 0.33695 | val_0_rmse: 0.61461 | val_1_rmse: 0.66792 |  0:00:30s
epoch 105| loss: 0.35408 | val_0_rmse: 0.60925 | val_1_rmse: 0.65777 |  0:00:30s
epoch 106| loss: 0.35772 | val_0_rmse: 0.60388 | val_1_rmse: 0.6506  |  0:00:30s
epoch 107| loss: 0.34035 | val_0_rmse: 0.60766 | val_1_rmse: 0.66178 |  0:00:31s
epoch 108| loss: 0.33309 | val_0_rmse: 0.6134  | val_1_rmse: 0.6622  |  0:00:31s
epoch 109| loss: 0.33641 | val_0_rmse: 0.65039 | val_1_rmse: 0.69239 |  0:00:31s
epoch 110| loss: 0.3445  | val_0_rmse: 0.60584 | val_1_rmse: 0.65515 |  0:00:31s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.622
Best weights from best epoch are automatically used!
ended training at: 08:41:08
Feature importance:
[('Area', 0.28058750901153995), ('Baths', 0.18897577070934074), ('Beds', 7.340337814353172e-06), ('Latitude', 0.18115472205510683), ('Longitude', 0.30071305642888485), ('Month', 0.045934898191847685), ('Year', 0.0026267032654656327)]
Mean squared error is of 2737855503.470263
Mean absolute error:36516.930291158205
MAPE:0.2922954775347454
R2 score:0.6761168317646318
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:41:08
epoch 0  | loss: 1.13372 | val_0_rmse: 1.35133 | val_1_rmse: 1.23901 |  0:00:00s
epoch 1  | loss: 0.72542 | val_0_rmse: 0.93015 | val_1_rmse: 0.90979 |  0:00:00s
epoch 2  | loss: 0.61642 | val_0_rmse: 0.79394 | val_1_rmse: 0.75866 |  0:00:00s
epoch 3  | loss: 0.55114 | val_0_rmse: 0.78549 | val_1_rmse: 0.74361 |  0:00:01s
epoch 4  | loss: 0.55744 | val_0_rmse: 0.76172 | val_1_rmse: 0.73045 |  0:00:01s
epoch 5  | loss: 0.53322 | val_0_rmse: 0.73881 | val_1_rmse: 0.72858 |  0:00:01s
epoch 6  | loss: 0.53109 | val_0_rmse: 0.7044  | val_1_rmse: 0.65791 |  0:00:02s
epoch 7  | loss: 0.50137 | val_0_rmse: 0.71138 | val_1_rmse: 0.69648 |  0:00:02s
epoch 8  | loss: 0.48323 | val_0_rmse: 0.68482 | val_1_rmse: 0.64684 |  0:00:02s
epoch 9  | loss: 0.47751 | val_0_rmse: 0.68217 | val_1_rmse: 0.63819 |  0:00:02s
epoch 10 | loss: 0.47089 | val_0_rmse: 0.68706 | val_1_rmse: 0.63752 |  0:00:03s
epoch 11 | loss: 0.45761 | val_0_rmse: 0.67793 | val_1_rmse: 0.61842 |  0:00:03s
epoch 12 | loss: 0.44783 | val_0_rmse: 0.67982 | val_1_rmse: 0.62644 |  0:00:03s
epoch 13 | loss: 0.43635 | val_0_rmse: 0.70803 | val_1_rmse: 0.65277 |  0:00:04s
epoch 14 | loss: 0.4602  | val_0_rmse: 0.6754  | val_1_rmse: 0.63658 |  0:00:04s
epoch 15 | loss: 0.4549  | val_0_rmse: 0.6661  | val_1_rmse: 0.65095 |  0:00:04s
epoch 16 | loss: 0.4557  | val_0_rmse: 0.6887  | val_1_rmse: 0.65825 |  0:00:04s
epoch 17 | loss: 0.44652 | val_0_rmse: 0.68017 | val_1_rmse: 0.65866 |  0:00:05s
epoch 18 | loss: 0.44203 | val_0_rmse: 0.66432 | val_1_rmse: 0.63565 |  0:00:05s
epoch 19 | loss: 0.43692 | val_0_rmse: 0.65818 | val_1_rmse: 0.61731 |  0:00:05s
epoch 20 | loss: 0.43375 | val_0_rmse: 0.68692 | val_1_rmse: 0.64347 |  0:00:06s
epoch 21 | loss: 0.44067 | val_0_rmse: 0.64723 | val_1_rmse: 0.6178  |  0:00:06s
epoch 22 | loss: 0.42198 | val_0_rmse: 0.64124 | val_1_rmse: 0.60582 |  0:00:06s
epoch 23 | loss: 0.40698 | val_0_rmse: 0.66015 | val_1_rmse: 0.61724 |  0:00:07s
epoch 24 | loss: 0.40383 | val_0_rmse: 0.63644 | val_1_rmse: 0.59007 |  0:00:07s
epoch 25 | loss: 0.4067  | val_0_rmse: 0.6952  | val_1_rmse: 0.62418 |  0:00:07s
epoch 26 | loss: 0.40445 | val_0_rmse: 0.76527 | val_1_rmse: 0.7035  |  0:00:07s
epoch 27 | loss: 0.40495 | val_0_rmse: 0.74577 | val_1_rmse: 0.70626 |  0:00:08s
epoch 28 | loss: 0.41528 | val_0_rmse: 0.66787 | val_1_rmse: 0.63458 |  0:00:08s
epoch 29 | loss: 0.41386 | val_0_rmse: 0.65813 | val_1_rmse: 0.61807 |  0:00:08s
epoch 30 | loss: 0.41221 | val_0_rmse: 0.68056 | val_1_rmse: 0.64655 |  0:00:09s
epoch 31 | loss: 0.40147 | val_0_rmse: 0.6394  | val_1_rmse: 0.60973 |  0:00:09s
epoch 32 | loss: 0.3945  | val_0_rmse: 0.63212 | val_1_rmse: 0.60023 |  0:00:09s
epoch 33 | loss: 0.39252 | val_0_rmse: 0.66708 | val_1_rmse: 0.62662 |  0:00:09s
epoch 34 | loss: 0.39414 | val_0_rmse: 0.62638 | val_1_rmse: 0.60677 |  0:00:10s
epoch 35 | loss: 0.39284 | val_0_rmse: 0.68827 | val_1_rmse: 0.63414 |  0:00:10s
epoch 36 | loss: 0.39573 | val_0_rmse: 0.64203 | val_1_rmse: 0.619   |  0:00:10s
epoch 37 | loss: 0.39242 | val_0_rmse: 0.69673 | val_1_rmse: 0.64161 |  0:00:11s
epoch 38 | loss: 0.40253 | val_0_rmse: 0.64594 | val_1_rmse: 0.61011 |  0:00:11s
epoch 39 | loss: 0.38353 | val_0_rmse: 0.669   | val_1_rmse: 0.62    |  0:00:11s
epoch 40 | loss: 0.38059 | val_0_rmse: 0.65697 | val_1_rmse: 0.61367 |  0:00:11s
epoch 41 | loss: 0.38301 | val_0_rmse: 0.64245 | val_1_rmse: 0.60831 |  0:00:12s
epoch 42 | loss: 0.37319 | val_0_rmse: 0.65224 | val_1_rmse: 0.60528 |  0:00:12s
epoch 43 | loss: 0.38734 | val_0_rmse: 0.65413 | val_1_rmse: 0.61177 |  0:00:12s
epoch 44 | loss: 0.37025 | val_0_rmse: 0.68157 | val_1_rmse: 0.61891 |  0:00:13s
epoch 45 | loss: 0.37308 | val_0_rmse: 0.67679 | val_1_rmse: 0.60024 |  0:00:13s
epoch 46 | loss: 0.3762  | val_0_rmse: 0.67163 | val_1_rmse: 0.60142 |  0:00:13s
epoch 47 | loss: 0.37815 | val_0_rmse: 0.62432 | val_1_rmse: 0.59115 |  0:00:13s
epoch 48 | loss: 0.36875 | val_0_rmse: 0.61776 | val_1_rmse: 0.59744 |  0:00:14s
epoch 49 | loss: 0.3877  | val_0_rmse: 0.68874 | val_1_rmse: 0.65672 |  0:00:14s
epoch 50 | loss: 0.392   | val_0_rmse: 0.63994 | val_1_rmse: 0.60682 |  0:00:14s
epoch 51 | loss: 0.38835 | val_0_rmse: 0.67846 | val_1_rmse: 0.6403  |  0:00:14s
epoch 52 | loss: 0.38429 | val_0_rmse: 0.61623 | val_1_rmse: 0.58699 |  0:00:15s
epoch 53 | loss: 0.36437 | val_0_rmse: 0.63887 | val_1_rmse: 0.60851 |  0:00:15s
epoch 54 | loss: 0.3679  | val_0_rmse: 0.62027 | val_1_rmse: 0.60275 |  0:00:15s
epoch 55 | loss: 0.37188 | val_0_rmse: 0.62062 | val_1_rmse: 0.59267 |  0:00:16s
epoch 56 | loss: 0.36945 | val_0_rmse: 0.62415 | val_1_rmse: 0.59885 |  0:00:16s
epoch 57 | loss: 0.36634 | val_0_rmse: 0.6215  | val_1_rmse: 0.59834 |  0:00:16s
epoch 58 | loss: 0.36626 | val_0_rmse: 0.64757 | val_1_rmse: 0.6196  |  0:00:17s
epoch 59 | loss: 0.35717 | val_0_rmse: 0.61963 | val_1_rmse: 0.60109 |  0:00:17s
epoch 60 | loss: 0.35662 | val_0_rmse: 0.63425 | val_1_rmse: 0.61672 |  0:00:17s
epoch 61 | loss: 0.36103 | val_0_rmse: 0.6216  | val_1_rmse: 0.59843 |  0:00:17s
epoch 62 | loss: 0.35374 | val_0_rmse: 0.61354 | val_1_rmse: 0.58383 |  0:00:18s
epoch 63 | loss: 0.34917 | val_0_rmse: 0.59689 | val_1_rmse: 0.59547 |  0:00:18s
epoch 64 | loss: 0.35843 | val_0_rmse: 0.62535 | val_1_rmse: 0.61193 |  0:00:18s
epoch 65 | loss: 0.34665 | val_0_rmse: 0.63535 | val_1_rmse: 0.61321 |  0:00:19s
epoch 66 | loss: 0.3521  | val_0_rmse: 0.68524 | val_1_rmse: 0.65952 |  0:00:19s
epoch 67 | loss: 0.35054 | val_0_rmse: 0.61618 | val_1_rmse: 0.60054 |  0:00:19s
epoch 68 | loss: 0.35435 | val_0_rmse: 0.61825 | val_1_rmse: 0.60156 |  0:00:19s
epoch 69 | loss: 0.35235 | val_0_rmse: 0.63371 | val_1_rmse: 0.6052  |  0:00:20s
epoch 70 | loss: 0.35982 | val_0_rmse: 0.61017 | val_1_rmse: 0.58458 |  0:00:20s
epoch 71 | loss: 0.34636 | val_0_rmse: 0.64516 | val_1_rmse: 0.62049 |  0:00:20s
epoch 72 | loss: 0.34661 | val_0_rmse: 0.64391 | val_1_rmse: 0.62794 |  0:00:21s
epoch 73 | loss: 0.35499 | val_0_rmse: 0.62534 | val_1_rmse: 0.60198 |  0:00:21s
epoch 74 | loss: 0.35598 | val_0_rmse: 0.62202 | val_1_rmse: 0.60002 |  0:00:21s
epoch 75 | loss: 0.35161 | val_0_rmse: 0.60603 | val_1_rmse: 0.5854  |  0:00:21s
epoch 76 | loss: 0.35975 | val_0_rmse: 0.63902 | val_1_rmse: 0.61325 |  0:00:22s
epoch 77 | loss: 0.3641  | val_0_rmse: 0.61551 | val_1_rmse: 0.59955 |  0:00:22s
epoch 78 | loss: 0.36009 | val_0_rmse: 0.62904 | val_1_rmse: 0.62047 |  0:00:22s
epoch 79 | loss: 0.3539  | val_0_rmse: 0.62553 | val_1_rmse: 0.61148 |  0:00:23s
epoch 80 | loss: 0.3547  | val_0_rmse: 0.65386 | val_1_rmse: 0.63975 |  0:00:23s
epoch 81 | loss: 0.35528 | val_0_rmse: 0.65017 | val_1_rmse: 0.62391 |  0:00:23s
epoch 82 | loss: 0.35688 | val_0_rmse: 0.62601 | val_1_rmse: 0.60312 |  0:00:23s
epoch 83 | loss: 0.35441 | val_0_rmse: 0.62097 | val_1_rmse: 0.59778 |  0:00:24s
epoch 84 | loss: 0.36806 | val_0_rmse: 0.62895 | val_1_rmse: 0.60901 |  0:00:24s
epoch 85 | loss: 0.35878 | val_0_rmse: 0.6291  | val_1_rmse: 0.60722 |  0:00:24s
epoch 86 | loss: 0.35582 | val_0_rmse: 0.61089 | val_1_rmse: 0.59378 |  0:00:25s
epoch 87 | loss: 0.34488 | val_0_rmse: 0.61657 | val_1_rmse: 0.59902 |  0:00:25s
epoch 88 | loss: 0.35912 | val_0_rmse: 0.63476 | val_1_rmse: 0.62025 |  0:00:25s
epoch 89 | loss: 0.35888 | val_0_rmse: 0.60632 | val_1_rmse: 0.59447 |  0:00:25s
epoch 90 | loss: 0.35443 | val_0_rmse: 0.61951 | val_1_rmse: 0.60557 |  0:00:26s
epoch 91 | loss: 0.34866 | val_0_rmse: 0.63005 | val_1_rmse: 0.61717 |  0:00:26s
epoch 92 | loss: 0.34485 | val_0_rmse: 0.62671 | val_1_rmse: 0.61486 |  0:00:26s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.58383
Best weights from best epoch are automatically used!
ended training at: 08:41:35
Feature importance:
[('Area', 0.2891531530930267), ('Baths', 0.12056201547077608), ('Beds', 0.029674526103383114), ('Latitude', 0.312089838559416), ('Longitude', 0.24080778680627726), ('Month', 5.549539061445674e-07), ('Year', 0.007712125013214689)]
Mean squared error is of 3707483228.35528
Mean absolute error:39910.0661733871
MAPE:0.3484712329931323
R2 score:0.5722305983247228
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:41:35
epoch 0  | loss: 1.1987  | val_0_rmse: 1.01991 | val_1_rmse: 1.02592 |  0:00:00s
epoch 1  | loss: 0.61229 | val_0_rmse: 0.87426 | val_1_rmse: 0.92077 |  0:00:00s
epoch 2  | loss: 0.55286 | val_0_rmse: 0.77227 | val_1_rmse: 0.81533 |  0:00:00s
epoch 3  | loss: 0.50127 | val_0_rmse: 0.75558 | val_1_rmse: 0.80922 |  0:00:01s
epoch 4  | loss: 0.49792 | val_0_rmse: 0.72285 | val_1_rmse: 0.78471 |  0:00:01s
epoch 5  | loss: 0.48732 | val_0_rmse: 0.70592 | val_1_rmse: 0.76518 |  0:00:01s
epoch 6  | loss: 0.48318 | val_0_rmse: 0.68965 | val_1_rmse: 0.73375 |  0:00:02s
epoch 7  | loss: 0.46943 | val_0_rmse: 0.66443 | val_1_rmse: 0.69498 |  0:00:02s
epoch 8  | loss: 0.46274 | val_0_rmse: 0.67011 | val_1_rmse: 0.70513 |  0:00:02s
epoch 9  | loss: 0.45587 | val_0_rmse: 0.66515 | val_1_rmse: 0.68673 |  0:00:02s
epoch 10 | loss: 0.44003 | val_0_rmse: 0.67162 | val_1_rmse: 0.69591 |  0:00:03s
epoch 11 | loss: 0.43293 | val_0_rmse: 0.69281 | val_1_rmse: 0.6743  |  0:00:03s
epoch 12 | loss: 0.4397  | val_0_rmse: 0.71957 | val_1_rmse: 0.71307 |  0:00:03s
epoch 13 | loss: 0.45817 | val_0_rmse: 0.65703 | val_1_rmse: 0.68243 |  0:00:04s
epoch 14 | loss: 0.42661 | val_0_rmse: 0.65465 | val_1_rmse: 0.68004 |  0:00:04s
epoch 15 | loss: 0.41931 | val_0_rmse: 0.69437 | val_1_rmse: 0.70614 |  0:00:04s
epoch 16 | loss: 0.43311 | val_0_rmse: 0.63057 | val_1_rmse: 0.65503 |  0:00:04s
epoch 17 | loss: 0.41238 | val_0_rmse: 0.64806 | val_1_rmse: 0.67314 |  0:00:05s
epoch 18 | loss: 0.43128 | val_0_rmse: 0.65471 | val_1_rmse: 0.68343 |  0:00:05s
epoch 19 | loss: 0.42241 | val_0_rmse: 0.64346 | val_1_rmse: 0.67743 |  0:00:05s
epoch 20 | loss: 0.40142 | val_0_rmse: 0.68438 | val_1_rmse: 0.70192 |  0:00:06s
epoch 21 | loss: 0.39912 | val_0_rmse: 0.63623 | val_1_rmse: 0.66406 |  0:00:06s
epoch 22 | loss: 0.39279 | val_0_rmse: 0.6312  | val_1_rmse: 0.66582 |  0:00:06s
epoch 23 | loss: 0.39629 | val_0_rmse: 0.62751 | val_1_rmse: 0.65545 |  0:00:07s
epoch 24 | loss: 0.3894  | val_0_rmse: 0.69377 | val_1_rmse: 0.70306 |  0:00:07s
epoch 25 | loss: 0.39284 | val_0_rmse: 0.63521 | val_1_rmse: 0.65076 |  0:00:07s
epoch 26 | loss: 0.41049 | val_0_rmse: 0.65769 | val_1_rmse: 0.67596 |  0:00:07s
epoch 27 | loss: 0.37502 | val_0_rmse: 0.60773 | val_1_rmse: 0.62987 |  0:00:08s
epoch 28 | loss: 0.37682 | val_0_rmse: 0.64493 | val_1_rmse: 0.65645 |  0:00:08s
epoch 29 | loss: 0.38177 | val_0_rmse: 0.62317 | val_1_rmse: 0.64455 |  0:00:08s
epoch 30 | loss: 0.39813 | val_0_rmse: 0.60414 | val_1_rmse: 0.61792 |  0:00:09s
epoch 31 | loss: 0.37594 | val_0_rmse: 0.63602 | val_1_rmse: 0.64379 |  0:00:09s
epoch 32 | loss: 0.38156 | val_0_rmse: 0.61975 | val_1_rmse: 0.62607 |  0:00:09s
epoch 33 | loss: 0.378   | val_0_rmse: 0.61817 | val_1_rmse: 0.63334 |  0:00:09s
epoch 34 | loss: 0.38421 | val_0_rmse: 0.61801 | val_1_rmse: 0.63014 |  0:00:10s
epoch 35 | loss: 0.38554 | val_0_rmse: 0.66774 | val_1_rmse: 0.66213 |  0:00:10s
epoch 36 | loss: 0.39235 | val_0_rmse: 0.6164  | val_1_rmse: 0.61423 |  0:00:10s
epoch 37 | loss: 0.37803 | val_0_rmse: 0.59617 | val_1_rmse: 0.60835 |  0:00:11s
epoch 38 | loss: 0.36248 | val_0_rmse: 0.60598 | val_1_rmse: 0.60916 |  0:00:11s
epoch 39 | loss: 0.3644  | val_0_rmse: 0.60827 | val_1_rmse: 0.6079  |  0:00:11s
epoch 40 | loss: 0.37302 | val_0_rmse: 0.60242 | val_1_rmse: 0.59532 |  0:00:11s
epoch 41 | loss: 0.36627 | val_0_rmse: 0.59913 | val_1_rmse: 0.59695 |  0:00:12s
epoch 42 | loss: 0.36248 | val_0_rmse: 0.6164  | val_1_rmse: 0.61589 |  0:00:12s
epoch 43 | loss: 0.37826 | val_0_rmse: 0.62048 | val_1_rmse: 0.62243 |  0:00:12s
epoch 44 | loss: 0.37915 | val_0_rmse: 0.65492 | val_1_rmse: 0.65961 |  0:00:13s
epoch 45 | loss: 0.37267 | val_0_rmse: 0.67469 | val_1_rmse: 0.66627 |  0:00:13s
epoch 46 | loss: 0.37583 | val_0_rmse: 0.6192  | val_1_rmse: 0.62323 |  0:00:13s
epoch 47 | loss: 0.38155 | val_0_rmse: 0.61115 | val_1_rmse: 0.62035 |  0:00:14s
epoch 48 | loss: 0.37291 | val_0_rmse: 0.65598 | val_1_rmse: 0.65438 |  0:00:14s
epoch 49 | loss: 0.36327 | val_0_rmse: 0.71369 | val_1_rmse: 0.71205 |  0:00:14s
epoch 50 | loss: 0.35256 | val_0_rmse: 0.70587 | val_1_rmse: 0.69884 |  0:00:14s
epoch 51 | loss: 0.35714 | val_0_rmse: 0.63049 | val_1_rmse: 0.61668 |  0:00:15s
epoch 52 | loss: 0.35498 | val_0_rmse: 0.64751 | val_1_rmse: 0.6381  |  0:00:15s
epoch 53 | loss: 0.37319 | val_0_rmse: 0.61517 | val_1_rmse: 0.61602 |  0:00:15s
epoch 54 | loss: 0.35537 | val_0_rmse: 0.61623 | val_1_rmse: 0.61574 |  0:00:16s
epoch 55 | loss: 0.34647 | val_0_rmse: 0.62601 | val_1_rmse: 0.63111 |  0:00:16s
epoch 56 | loss: 0.37107 | val_0_rmse: 0.60473 | val_1_rmse: 0.59932 |  0:00:16s
epoch 57 | loss: 0.36576 | val_0_rmse: 0.61157 | val_1_rmse: 0.61359 |  0:00:16s
epoch 58 | loss: 0.36616 | val_0_rmse: 0.60054 | val_1_rmse: 0.61131 |  0:00:17s
epoch 59 | loss: 0.36444 | val_0_rmse: 0.60952 | val_1_rmse: 0.60807 |  0:00:17s
epoch 60 | loss: 0.36116 | val_0_rmse: 0.5937  | val_1_rmse: 0.59804 |  0:00:17s
epoch 61 | loss: 0.36452 | val_0_rmse: 0.61729 | val_1_rmse: 0.61628 |  0:00:18s
epoch 62 | loss: 0.37263 | val_0_rmse: 0.60221 | val_1_rmse: 0.59784 |  0:00:18s
epoch 63 | loss: 0.36623 | val_0_rmse: 0.63366 | val_1_rmse: 0.62791 |  0:00:18s
epoch 64 | loss: 0.3677  | val_0_rmse: 0.6108  | val_1_rmse: 0.60954 |  0:00:18s
epoch 65 | loss: 0.37683 | val_0_rmse: 0.60677 | val_1_rmse: 0.60866 |  0:00:19s
epoch 66 | loss: 0.37696 | val_0_rmse: 0.61024 | val_1_rmse: 0.61601 |  0:00:19s
epoch 67 | loss: 0.35896 | val_0_rmse: 0.60866 | val_1_rmse: 0.61321 |  0:00:19s
epoch 68 | loss: 0.36246 | val_0_rmse: 0.58995 | val_1_rmse: 0.59367 |  0:00:20s
epoch 69 | loss: 0.3578  | val_0_rmse: 0.58852 | val_1_rmse: 0.59001 |  0:00:20s
epoch 70 | loss: 0.35392 | val_0_rmse: 0.60277 | val_1_rmse: 0.60357 |  0:00:20s
epoch 71 | loss: 0.35402 | val_0_rmse: 0.6061  | val_1_rmse: 0.60535 |  0:00:20s
epoch 72 | loss: 0.35338 | val_0_rmse: 0.60074 | val_1_rmse: 0.59554 |  0:00:21s
epoch 73 | loss: 0.36064 | val_0_rmse: 0.62303 | val_1_rmse: 0.61999 |  0:00:21s
epoch 74 | loss: 0.36185 | val_0_rmse: 0.58898 | val_1_rmse: 0.58767 |  0:00:21s
epoch 75 | loss: 0.34714 | val_0_rmse: 0.58471 | val_1_rmse: 0.5889  |  0:00:22s
epoch 76 | loss: 0.35049 | val_0_rmse: 0.61274 | val_1_rmse: 0.62137 |  0:00:22s
epoch 77 | loss: 0.34508 | val_0_rmse: 0.61352 | val_1_rmse: 0.61591 |  0:00:22s
epoch 78 | loss: 0.3478  | val_0_rmse: 0.61581 | val_1_rmse: 0.61927 |  0:00:22s
epoch 79 | loss: 0.33882 | val_0_rmse: 0.59232 | val_1_rmse: 0.59546 |  0:00:23s
epoch 80 | loss: 0.3387  | val_0_rmse: 0.6368  | val_1_rmse: 0.63654 |  0:00:23s
epoch 81 | loss: 0.34457 | val_0_rmse: 0.60907 | val_1_rmse: 0.6171  |  0:00:23s
epoch 82 | loss: 0.33383 | val_0_rmse: 0.60991 | val_1_rmse: 0.6113  |  0:00:24s
epoch 83 | loss: 0.3443  | val_0_rmse: 0.60116 | val_1_rmse: 0.60953 |  0:00:24s
epoch 84 | loss: 0.34189 | val_0_rmse: 0.58942 | val_1_rmse: 0.6028  |  0:00:24s
epoch 85 | loss: 0.34088 | val_0_rmse: 0.60097 | val_1_rmse: 0.61097 |  0:00:24s
epoch 86 | loss: 0.34926 | val_0_rmse: 0.61711 | val_1_rmse: 0.62535 |  0:00:25s
epoch 87 | loss: 0.35148 | val_0_rmse: 0.66393 | val_1_rmse: 0.66543 |  0:00:25s
epoch 88 | loss: 0.35505 | val_0_rmse: 0.63527 | val_1_rmse: 0.63519 |  0:00:25s
epoch 89 | loss: 0.35939 | val_0_rmse: 0.74022 | val_1_rmse: 0.72766 |  0:00:26s
epoch 90 | loss: 0.351   | val_0_rmse: 0.76205 | val_1_rmse: 0.75812 |  0:00:26s
epoch 91 | loss: 0.35864 | val_0_rmse: 0.71256 | val_1_rmse: 0.71039 |  0:00:26s
epoch 92 | loss: 0.34841 | val_0_rmse: 0.66852 | val_1_rmse: 0.65952 |  0:00:27s
epoch 93 | loss: 0.34604 | val_0_rmse: 0.64201 | val_1_rmse: 0.63905 |  0:00:27s
epoch 94 | loss: 0.34865 | val_0_rmse: 0.60476 | val_1_rmse: 0.60211 |  0:00:27s
epoch 95 | loss: 0.34338 | val_0_rmse: 0.59036 | val_1_rmse: 0.59488 |  0:00:27s
epoch 96 | loss: 0.33994 | val_0_rmse: 0.59137 | val_1_rmse: 0.60742 |  0:00:28s
epoch 97 | loss: 0.33385 | val_0_rmse: 0.59833 | val_1_rmse: 0.60601 |  0:00:28s
epoch 98 | loss: 0.34402 | val_0_rmse: 0.60843 | val_1_rmse: 0.61357 |  0:00:28s
epoch 99 | loss: 0.34991 | val_0_rmse: 0.58741 | val_1_rmse: 0.58485 |  0:00:29s
epoch 100| loss: 0.34843 | val_0_rmse: 0.60035 | val_1_rmse: 0.61268 |  0:00:29s
epoch 101| loss: 0.35447 | val_0_rmse: 0.58793 | val_1_rmse: 0.58849 |  0:00:29s
epoch 102| loss: 0.33709 | val_0_rmse: 0.61788 | val_1_rmse: 0.61195 |  0:00:29s
epoch 103| loss: 0.35575 | val_0_rmse: 0.58854 | val_1_rmse: 0.59714 |  0:00:30s
epoch 104| loss: 0.33648 | val_0_rmse: 0.60463 | val_1_rmse: 0.60826 |  0:00:30s
epoch 105| loss: 0.34353 | val_0_rmse: 0.59056 | val_1_rmse: 0.59319 |  0:00:30s
epoch 106| loss: 0.33304 | val_0_rmse: 0.61912 | val_1_rmse: 0.61756 |  0:00:31s
epoch 107| loss: 0.33882 | val_0_rmse: 0.61046 | val_1_rmse: 0.61252 |  0:00:31s
epoch 108| loss: 0.35183 | val_0_rmse: 0.63238 | val_1_rmse: 0.62833 |  0:00:31s
epoch 109| loss: 0.38318 | val_0_rmse: 0.69904 | val_1_rmse: 0.69435 |  0:00:31s
epoch 110| loss: 0.36469 | val_0_rmse: 0.66782 | val_1_rmse: 0.6704  |  0:00:32s
epoch 111| loss: 0.35587 | val_0_rmse: 0.62729 | val_1_rmse: 0.63482 |  0:00:32s
epoch 112| loss: 0.34854 | val_0_rmse: 0.61463 | val_1_rmse: 0.62495 |  0:00:32s
epoch 113| loss: 0.3425  | val_0_rmse: 0.60962 | val_1_rmse: 0.6121  |  0:00:33s
epoch 114| loss: 0.34915 | val_0_rmse: 0.59318 | val_1_rmse: 0.596   |  0:00:33s
epoch 115| loss: 0.34352 | val_0_rmse: 0.61183 | val_1_rmse: 0.62331 |  0:00:33s
epoch 116| loss: 0.34058 | val_0_rmse: 0.62979 | val_1_rmse: 0.62563 |  0:00:33s
epoch 117| loss: 0.36431 | val_0_rmse: 0.64111 | val_1_rmse: 0.64684 |  0:00:34s
epoch 118| loss: 0.35629 | val_0_rmse: 0.63175 | val_1_rmse: 0.63863 |  0:00:34s
epoch 119| loss: 0.35473 | val_0_rmse: 0.63326 | val_1_rmse: 0.64218 |  0:00:34s
epoch 120| loss: 0.35042 | val_0_rmse: 0.63121 | val_1_rmse: 0.63559 |  0:00:35s
epoch 121| loss: 0.35169 | val_0_rmse: 0.6275  | val_1_rmse: 0.63146 |  0:00:35s
epoch 122| loss: 0.34618 | val_0_rmse: 0.62213 | val_1_rmse: 0.62881 |  0:00:35s
epoch 123| loss: 0.34271 | val_0_rmse: 0.64186 | val_1_rmse: 0.6408  |  0:00:35s
epoch 124| loss: 0.34097 | val_0_rmse: 0.62763 | val_1_rmse: 0.63374 |  0:00:36s
epoch 125| loss: 0.36289 | val_0_rmse: 0.65225 | val_1_rmse: 0.66131 |  0:00:36s
epoch 126| loss: 0.34949 | val_0_rmse: 0.63716 | val_1_rmse: 0.64712 |  0:00:36s
epoch 127| loss: 0.36722 | val_0_rmse: 0.67143 | val_1_rmse: 0.66714 |  0:00:37s
epoch 128| loss: 0.34874 | val_0_rmse: 0.64421 | val_1_rmse: 0.65054 |  0:00:37s
epoch 129| loss: 0.34889 | val_0_rmse: 0.63949 | val_1_rmse: 0.65242 |  0:00:37s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.58485
Best weights from best epoch are automatically used!
ended training at: 08:42:13
Feature importance:
[('Area', 0.24797025982467488), ('Baths', 0.10105537837162419), ('Beds', 0.0015449000969159664), ('Latitude', 0.25872408275198044), ('Longitude', 0.2973504242137088), ('Month', 0.05957021656686655), ('Year', 0.03378473817422921)]
Mean squared error is of 2849576190.264222
Mean absolute error:36696.32337929754
MAPE:0.3312644524164457
R2 score:0.6399839609714593
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:42:13
epoch 0  | loss: 1.18883 | val_0_rmse: 0.97092 | val_1_rmse: 0.9428  |  0:00:00s
epoch 1  | loss: 0.6478  | val_0_rmse: 1.04715 | val_1_rmse: 0.98313 |  0:00:00s
epoch 2  | loss: 0.56623 | val_0_rmse: 0.82668 | val_1_rmse: 0.78147 |  0:00:00s
epoch 3  | loss: 0.54239 | val_0_rmse: 0.79436 | val_1_rmse: 0.76159 |  0:00:01s
epoch 4  | loss: 0.5175  | val_0_rmse: 0.74599 | val_1_rmse: 0.68164 |  0:00:01s
epoch 5  | loss: 0.51394 | val_0_rmse: 0.72388 | val_1_rmse: 0.68457 |  0:00:01s
epoch 6  | loss: 0.51241 | val_0_rmse: 0.71317 | val_1_rmse: 0.67755 |  0:00:02s
epoch 7  | loss: 0.49878 | val_0_rmse: 0.70394 | val_1_rmse: 0.66864 |  0:00:02s
epoch 8  | loss: 0.50361 | val_0_rmse: 0.69942 | val_1_rmse: 0.65811 |  0:00:02s
epoch 9  | loss: 0.48858 | val_0_rmse: 0.70102 | val_1_rmse: 0.65434 |  0:00:02s
epoch 10 | loss: 0.48868 | val_0_rmse: 0.70408 | val_1_rmse: 0.66578 |  0:00:03s
epoch 11 | loss: 0.4876  | val_0_rmse: 0.6951  | val_1_rmse: 0.66395 |  0:00:03s
epoch 12 | loss: 0.48039 | val_0_rmse: 0.68378 | val_1_rmse: 0.64776 |  0:00:03s
epoch 13 | loss: 0.47057 | val_0_rmse: 0.67826 | val_1_rmse: 0.64075 |  0:00:04s
epoch 14 | loss: 0.46628 | val_0_rmse: 0.67359 | val_1_rmse: 0.62278 |  0:00:04s
epoch 15 | loss: 0.46015 | val_0_rmse: 0.67775 | val_1_rmse: 0.62535 |  0:00:04s
epoch 16 | loss: 0.46474 | val_0_rmse: 0.66811 | val_1_rmse: 0.6267  |  0:00:05s
epoch 17 | loss: 0.44983 | val_0_rmse: 0.6605  | val_1_rmse: 0.614   |  0:00:05s
epoch 18 | loss: 0.43872 | val_0_rmse: 0.66151 | val_1_rmse: 0.61709 |  0:00:05s
epoch 19 | loss: 0.43449 | val_0_rmse: 0.64533 | val_1_rmse: 0.59993 |  0:00:05s
epoch 20 | loss: 0.42862 | val_0_rmse: 0.64651 | val_1_rmse: 0.60194 |  0:00:06s
epoch 21 | loss: 0.42466 | val_0_rmse: 0.67861 | val_1_rmse: 0.62741 |  0:00:06s
epoch 22 | loss: 0.43195 | val_0_rmse: 0.64804 | val_1_rmse: 0.60434 |  0:00:06s
epoch 23 | loss: 0.41425 | val_0_rmse: 0.6747  | val_1_rmse: 0.61665 |  0:00:07s
epoch 24 | loss: 0.40294 | val_0_rmse: 0.72561 | val_1_rmse: 0.68145 |  0:00:07s
epoch 25 | loss: 0.42156 | val_0_rmse: 0.64464 | val_1_rmse: 0.60465 |  0:00:07s
epoch 26 | loss: 0.4206  | val_0_rmse: 0.63529 | val_1_rmse: 0.59383 |  0:00:07s
epoch 27 | loss: 0.41783 | val_0_rmse: 0.65805 | val_1_rmse: 0.61572 |  0:00:08s
epoch 28 | loss: 0.40783 | val_0_rmse: 0.61984 | val_1_rmse: 0.56688 |  0:00:08s
epoch 29 | loss: 0.40429 | val_0_rmse: 0.66621 | val_1_rmse: 0.61691 |  0:00:08s
epoch 30 | loss: 0.41124 | val_0_rmse: 0.62868 | val_1_rmse: 0.59035 |  0:00:09s
epoch 31 | loss: 0.38925 | val_0_rmse: 0.62794 | val_1_rmse: 0.59005 |  0:00:09s
epoch 32 | loss: 0.39686 | val_0_rmse: 0.6162  | val_1_rmse: 0.57754 |  0:00:09s
epoch 33 | loss: 0.38188 | val_0_rmse: 0.6135  | val_1_rmse: 0.57516 |  0:00:09s
epoch 34 | loss: 0.38694 | val_0_rmse: 0.63724 | val_1_rmse: 0.59051 |  0:00:10s
epoch 35 | loss: 0.38559 | val_0_rmse: 0.65041 | val_1_rmse: 0.60537 |  0:00:10s
epoch 36 | loss: 0.38821 | val_0_rmse: 0.62643 | val_1_rmse: 0.58986 |  0:00:10s
epoch 37 | loss: 0.38621 | val_0_rmse: 0.61619 | val_1_rmse: 0.57618 |  0:00:11s
epoch 38 | loss: 0.37215 | val_0_rmse: 0.62821 | val_1_rmse: 0.59786 |  0:00:11s
epoch 39 | loss: 0.37318 | val_0_rmse: 0.6148  | val_1_rmse: 0.58086 |  0:00:11s
epoch 40 | loss: 0.38046 | val_0_rmse: 0.60686 | val_1_rmse: 0.56103 |  0:00:11s
epoch 41 | loss: 0.37808 | val_0_rmse: 0.61776 | val_1_rmse: 0.57332 |  0:00:12s
epoch 42 | loss: 0.3807  | val_0_rmse: 0.61316 | val_1_rmse: 0.56479 |  0:00:12s
epoch 43 | loss: 0.37984 | val_0_rmse: 0.60709 | val_1_rmse: 0.5658  |  0:00:12s
epoch 44 | loss: 0.38162 | val_0_rmse: 0.61677 | val_1_rmse: 0.56491 |  0:00:13s
epoch 45 | loss: 0.38858 | val_0_rmse: 0.62642 | val_1_rmse: 0.57663 |  0:00:13s
epoch 46 | loss: 0.3956  | val_0_rmse: 0.63049 | val_1_rmse: 0.58976 |  0:00:13s
epoch 47 | loss: 0.38728 | val_0_rmse: 0.64421 | val_1_rmse: 0.61098 |  0:00:13s
epoch 48 | loss: 0.3767  | val_0_rmse: 0.63985 | val_1_rmse: 0.60169 |  0:00:14s
epoch 49 | loss: 0.37262 | val_0_rmse: 0.63009 | val_1_rmse: 0.59982 |  0:00:14s
epoch 50 | loss: 0.37624 | val_0_rmse: 0.61599 | val_1_rmse: 0.57676 |  0:00:14s
epoch 51 | loss: 0.36324 | val_0_rmse: 0.62646 | val_1_rmse: 0.58177 |  0:00:15s
epoch 52 | loss: 0.36373 | val_0_rmse: 0.61597 | val_1_rmse: 0.57769 |  0:00:15s
epoch 53 | loss: 0.36823 | val_0_rmse: 0.61033 | val_1_rmse: 0.5724  |  0:00:15s
epoch 54 | loss: 0.36218 | val_0_rmse: 0.60568 | val_1_rmse: 0.56704 |  0:00:15s
epoch 55 | loss: 0.36413 | val_0_rmse: 0.60773 | val_1_rmse: 0.57104 |  0:00:16s
epoch 56 | loss: 0.35783 | val_0_rmse: 0.60606 | val_1_rmse: 0.56303 |  0:00:16s
epoch 57 | loss: 0.36018 | val_0_rmse: 0.60115 | val_1_rmse: 0.56346 |  0:00:16s
epoch 58 | loss: 0.36473 | val_0_rmse: 0.60521 | val_1_rmse: 0.5685  |  0:00:17s
epoch 59 | loss: 0.35324 | val_0_rmse: 0.59961 | val_1_rmse: 0.5627  |  0:00:17s
epoch 60 | loss: 0.36006 | val_0_rmse: 0.60619 | val_1_rmse: 0.57106 |  0:00:17s
epoch 61 | loss: 0.35488 | val_0_rmse: 0.60368 | val_1_rmse: 0.56876 |  0:00:17s
epoch 62 | loss: 0.36299 | val_0_rmse: 0.60646 | val_1_rmse: 0.57219 |  0:00:18s
epoch 63 | loss: 0.36449 | val_0_rmse: 0.60622 | val_1_rmse: 0.57295 |  0:00:18s
epoch 64 | loss: 0.35222 | val_0_rmse: 0.60926 | val_1_rmse: 0.57932 |  0:00:18s
epoch 65 | loss: 0.35399 | val_0_rmse: 0.61049 | val_1_rmse: 0.58082 |  0:00:19s
epoch 66 | loss: 0.35368 | val_0_rmse: 0.61751 | val_1_rmse: 0.58626 |  0:00:19s
epoch 67 | loss: 0.36023 | val_0_rmse: 0.6083  | val_1_rmse: 0.58356 |  0:00:19s
epoch 68 | loss: 0.35298 | val_0_rmse: 0.60154 | val_1_rmse: 0.57513 |  0:00:19s
epoch 69 | loss: 0.35512 | val_0_rmse: 0.59189 | val_1_rmse: 0.56583 |  0:00:20s
epoch 70 | loss: 0.3599  | val_0_rmse: 0.60283 | val_1_rmse: 0.57513 |  0:00:20s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.56103
Best weights from best epoch are automatically used!
ended training at: 08:42:34
Feature importance:
[('Area', 0.20141746622966417), ('Baths', 0.17681449934985477), ('Beds', 0.01634274090172641), ('Latitude', 0.18304207816612356), ('Longitude', 0.2990163450984019), ('Month', 0.09724480205291433), ('Year', 0.026122068201314876)]
Mean squared error is of 3322345620.737576
Mean absolute error:38654.81150919195
MAPE:0.3387545869610987
R2 score:0.5871625268347536
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:42:34
epoch 0  | loss: 1.19821 | val_0_rmse: 1.3535  | val_1_rmse: 1.33857 |  0:00:00s
epoch 1  | loss: 0.63892 | val_0_rmse: 0.96068 | val_1_rmse: 0.90857 |  0:00:00s
epoch 2  | loss: 0.5546  | val_0_rmse: 0.87215 | val_1_rmse: 0.88689 |  0:00:00s
epoch 3  | loss: 0.52093 | val_0_rmse: 0.76502 | val_1_rmse: 0.73258 |  0:00:01s
epoch 4  | loss: 0.51241 | val_0_rmse: 0.75335 | val_1_rmse: 0.73185 |  0:00:01s
epoch 5  | loss: 0.49403 | val_0_rmse: 0.72423 | val_1_rmse: 0.6994  |  0:00:01s
epoch 6  | loss: 0.49917 | val_0_rmse: 0.73248 | val_1_rmse: 0.71381 |  0:00:02s
epoch 7  | loss: 0.48968 | val_0_rmse: 0.7085  | val_1_rmse: 0.69899 |  0:00:02s
epoch 8  | loss: 0.49107 | val_0_rmse: 0.69766 | val_1_rmse: 0.67608 |  0:00:02s
epoch 9  | loss: 0.48167 | val_0_rmse: 0.69466 | val_1_rmse: 0.67674 |  0:00:02s
epoch 10 | loss: 0.47798 | val_0_rmse: 0.6894  | val_1_rmse: 0.67759 |  0:00:03s
epoch 11 | loss: 0.46867 | val_0_rmse: 0.68371 | val_1_rmse: 0.67191 |  0:00:03s
epoch 12 | loss: 0.46943 | val_0_rmse: 0.69804 | val_1_rmse: 0.67602 |  0:00:03s
epoch 13 | loss: 0.47524 | val_0_rmse: 0.69049 | val_1_rmse: 0.6667  |  0:00:04s
epoch 14 | loss: 0.46654 | val_0_rmse: 0.68349 | val_1_rmse: 0.66813 |  0:00:04s
epoch 15 | loss: 0.47976 | val_0_rmse: 0.68161 | val_1_rmse: 0.66864 |  0:00:04s
epoch 16 | loss: 0.46834 | val_0_rmse: 0.71274 | val_1_rmse: 0.69695 |  0:00:04s
epoch 17 | loss: 0.47931 | val_0_rmse: 0.68334 | val_1_rmse: 0.6667  |  0:00:05s
epoch 18 | loss: 0.48191 | val_0_rmse: 0.6835  | val_1_rmse: 0.66137 |  0:00:05s
epoch 19 | loss: 0.46972 | val_0_rmse: 0.6818  | val_1_rmse: 0.65758 |  0:00:05s
epoch 20 | loss: 0.46107 | val_0_rmse: 0.68562 | val_1_rmse: 0.66397 |  0:00:06s
epoch 21 | loss: 0.45866 | val_0_rmse: 0.70891 | val_1_rmse: 0.68197 |  0:00:06s
epoch 22 | loss: 0.45867 | val_0_rmse: 0.68528 | val_1_rmse: 0.66236 |  0:00:06s
epoch 23 | loss: 0.4508  | val_0_rmse: 0.68065 | val_1_rmse: 0.6636  |  0:00:06s
epoch 24 | loss: 0.44963 | val_0_rmse: 0.67705 | val_1_rmse: 0.66345 |  0:00:07s
epoch 25 | loss: 0.44319 | val_0_rmse: 0.68207 | val_1_rmse: 0.67957 |  0:00:07s
epoch 26 | loss: 0.4401  | val_0_rmse: 0.69394 | val_1_rmse: 0.68345 |  0:00:07s
epoch 27 | loss: 0.44334 | val_0_rmse: 0.665   | val_1_rmse: 0.66178 |  0:00:08s
epoch 28 | loss: 0.43392 | val_0_rmse: 0.67343 | val_1_rmse: 0.66599 |  0:00:08s
epoch 29 | loss: 0.44187 | val_0_rmse: 0.66663 | val_1_rmse: 0.675   |  0:00:08s
epoch 30 | loss: 0.44108 | val_0_rmse: 0.66053 | val_1_rmse: 0.67894 |  0:00:08s
epoch 31 | loss: 0.43337 | val_0_rmse: 0.67113 | val_1_rmse: 0.66608 |  0:00:09s
epoch 32 | loss: 0.44051 | val_0_rmse: 0.65893 | val_1_rmse: 0.64843 |  0:00:09s
epoch 33 | loss: 0.43022 | val_0_rmse: 0.65493 | val_1_rmse: 0.64438 |  0:00:09s
epoch 34 | loss: 0.43067 | val_0_rmse: 0.66928 | val_1_rmse: 0.65793 |  0:00:10s
epoch 35 | loss: 0.42194 | val_0_rmse: 0.67015 | val_1_rmse: 0.66105 |  0:00:10s
epoch 36 | loss: 0.42634 | val_0_rmse: 0.64865 | val_1_rmse: 0.63669 |  0:00:10s
epoch 37 | loss: 0.41281 | val_0_rmse: 0.66301 | val_1_rmse: 0.65111 |  0:00:11s
epoch 38 | loss: 0.42127 | val_0_rmse: 0.6526  | val_1_rmse: 0.64289 |  0:00:11s
epoch 39 | loss: 0.41546 | val_0_rmse: 0.6204  | val_1_rmse: 0.61576 |  0:00:11s
epoch 40 | loss: 0.39859 | val_0_rmse: 0.66809 | val_1_rmse: 0.65866 |  0:00:11s
epoch 41 | loss: 0.39832 | val_0_rmse: 0.71324 | val_1_rmse: 0.71301 |  0:00:12s
epoch 42 | loss: 0.40575 | val_0_rmse: 0.69959 | val_1_rmse: 0.6893  |  0:00:12s
epoch 43 | loss: 0.40031 | val_0_rmse: 0.67204 | val_1_rmse: 0.65894 |  0:00:12s
epoch 44 | loss: 0.40607 | val_0_rmse: 0.6485  | val_1_rmse: 0.63396 |  0:00:13s
epoch 45 | loss: 0.4011  | val_0_rmse: 0.68528 | val_1_rmse: 0.6693  |  0:00:13s
epoch 46 | loss: 0.40114 | val_0_rmse: 0.64267 | val_1_rmse: 0.62898 |  0:00:13s
epoch 47 | loss: 0.39462 | val_0_rmse: 0.62446 | val_1_rmse: 0.61091 |  0:00:13s
epoch 48 | loss: 0.38995 | val_0_rmse: 0.62314 | val_1_rmse: 0.61001 |  0:00:14s
epoch 49 | loss: 0.38005 | val_0_rmse: 0.63718 | val_1_rmse: 0.62891 |  0:00:14s
epoch 50 | loss: 0.38179 | val_0_rmse: 0.62132 | val_1_rmse: 0.616   |  0:00:14s
epoch 51 | loss: 0.38403 | val_0_rmse: 0.59999 | val_1_rmse: 0.59469 |  0:00:15s
epoch 52 | loss: 0.38868 | val_0_rmse: 0.6259  | val_1_rmse: 0.6285  |  0:00:15s
epoch 53 | loss: 0.39058 | val_0_rmse: 0.63638 | val_1_rmse: 0.63124 |  0:00:15s
epoch 54 | loss: 0.3845  | val_0_rmse: 0.61541 | val_1_rmse: 0.60795 |  0:00:15s
epoch 55 | loss: 0.3775  | val_0_rmse: 0.61602 | val_1_rmse: 0.60703 |  0:00:16s
epoch 56 | loss: 0.37138 | val_0_rmse: 0.61899 | val_1_rmse: 0.61632 |  0:00:16s
epoch 57 | loss: 0.37943 | val_0_rmse: 0.65339 | val_1_rmse: 0.6397  |  0:00:16s
epoch 58 | loss: 0.37098 | val_0_rmse: 0.60876 | val_1_rmse: 0.61063 |  0:00:17s
epoch 59 | loss: 0.36467 | val_0_rmse: 0.61013 | val_1_rmse: 0.60455 |  0:00:17s
epoch 60 | loss: 0.35612 | val_0_rmse: 0.60069 | val_1_rmse: 0.60355 |  0:00:17s
epoch 61 | loss: 0.36077 | val_0_rmse: 0.59343 | val_1_rmse: 0.5989  |  0:00:17s
epoch 62 | loss: 0.36142 | val_0_rmse: 0.60121 | val_1_rmse: 0.59931 |  0:00:18s
epoch 63 | loss: 0.35992 | val_0_rmse: 0.61037 | val_1_rmse: 0.60139 |  0:00:18s
epoch 64 | loss: 0.36269 | val_0_rmse: 0.60981 | val_1_rmse: 0.61424 |  0:00:18s
epoch 65 | loss: 0.35505 | val_0_rmse: 0.61943 | val_1_rmse: 0.6231  |  0:00:19s
epoch 66 | loss: 0.36411 | val_0_rmse: 0.65806 | val_1_rmse: 0.65296 |  0:00:19s
epoch 67 | loss: 0.38143 | val_0_rmse: 0.60991 | val_1_rmse: 0.60941 |  0:00:19s
epoch 68 | loss: 0.36773 | val_0_rmse: 0.62128 | val_1_rmse: 0.62379 |  0:00:19s
epoch 69 | loss: 0.38635 | val_0_rmse: 0.61103 | val_1_rmse: 0.61469 |  0:00:20s
epoch 70 | loss: 0.38473 | val_0_rmse: 0.61651 | val_1_rmse: 0.62254 |  0:00:20s
epoch 71 | loss: 0.36651 | val_0_rmse: 0.64725 | val_1_rmse: 0.6566  |  0:00:20s
epoch 72 | loss: 0.3665  | val_0_rmse: 0.63946 | val_1_rmse: 0.64276 |  0:00:21s
epoch 73 | loss: 0.3758  | val_0_rmse: 0.67646 | val_1_rmse: 0.67766 |  0:00:21s
epoch 74 | loss: 0.36351 | val_0_rmse: 0.59826 | val_1_rmse: 0.61265 |  0:00:21s
epoch 75 | loss: 0.36806 | val_0_rmse: 0.59272 | val_1_rmse: 0.6154  |  0:00:21s
epoch 76 | loss: 0.3495  | val_0_rmse: 0.60117 | val_1_rmse: 0.60562 |  0:00:22s
epoch 77 | loss: 0.34383 | val_0_rmse: 0.71186 | val_1_rmse: 0.69591 |  0:00:22s
epoch 78 | loss: 0.34855 | val_0_rmse: 0.59154 | val_1_rmse: 0.59563 |  0:00:22s
epoch 79 | loss: 0.3503  | val_0_rmse: 0.62686 | val_1_rmse: 0.62571 |  0:00:23s
epoch 80 | loss: 0.34268 | val_0_rmse: 0.59143 | val_1_rmse: 0.59916 |  0:00:23s
epoch 81 | loss: 0.33965 | val_0_rmse: 0.60231 | val_1_rmse: 0.61464 |  0:00:23s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.59469
Best weights from best epoch are automatically used!
ended training at: 08:42:58
Feature importance:
[('Area', 0.2519069790979019), ('Baths', 0.38751639455171627), ('Beds', 0.04969683059765063), ('Latitude', 0.16867945789712135), ('Longitude', 0.05680770820608576), ('Month', 0.042515984313812476), ('Year', 0.04287664533571166)]
Mean squared error is of 2974443596.564091
Mean absolute error:36226.43866417607
MAPE:0.3406622395190454
R2 score:0.610498136044964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:42:58
epoch 0  | loss: 1.16938 | val_0_rmse: 1.06901 | val_1_rmse: 1.06108 |  0:00:00s
epoch 1  | loss: 0.69278 | val_0_rmse: 0.93734 | val_1_rmse: 0.95757 |  0:00:00s
epoch 2  | loss: 0.65842 | val_0_rmse: 0.95174 | val_1_rmse: 0.96231 |  0:00:00s
epoch 3  | loss: 0.61267 | val_0_rmse: 0.76454 | val_1_rmse: 0.7724  |  0:00:01s
epoch 4  | loss: 0.58041 | val_0_rmse: 0.79543 | val_1_rmse: 0.81367 |  0:00:01s
epoch 5  | loss: 0.5419  | val_0_rmse: 0.72448 | val_1_rmse: 0.75487 |  0:00:01s
epoch 6  | loss: 0.53357 | val_0_rmse: 0.74364 | val_1_rmse: 0.75509 |  0:00:02s
epoch 7  | loss: 0.52897 | val_0_rmse: 0.72417 | val_1_rmse: 0.74528 |  0:00:02s
epoch 8  | loss: 0.54199 | val_0_rmse: 0.73646 | val_1_rmse: 0.7423  |  0:00:02s
epoch 9  | loss: 0.52615 | val_0_rmse: 0.72066 | val_1_rmse: 0.76704 |  0:00:02s
epoch 10 | loss: 0.5003  | val_0_rmse: 0.70362 | val_1_rmse: 0.74865 |  0:00:03s
epoch 11 | loss: 0.4955  | val_0_rmse: 0.69283 | val_1_rmse: 0.74037 |  0:00:03s
epoch 12 | loss: 0.48735 | val_0_rmse: 0.70511 | val_1_rmse: 0.73061 |  0:00:03s
epoch 13 | loss: 0.49726 | val_0_rmse: 0.69872 | val_1_rmse: 0.72329 |  0:00:04s
epoch 14 | loss: 0.48053 | val_0_rmse: 0.68503 | val_1_rmse: 0.70413 |  0:00:04s
epoch 15 | loss: 0.46692 | val_0_rmse: 0.68737 | val_1_rmse: 0.71113 |  0:00:04s
epoch 16 | loss: 0.47949 | val_0_rmse: 0.68882 | val_1_rmse: 0.71525 |  0:00:04s
epoch 17 | loss: 0.47107 | val_0_rmse: 0.69045 | val_1_rmse: 0.72966 |  0:00:05s
epoch 18 | loss: 0.47254 | val_0_rmse: 0.68327 | val_1_rmse: 0.72049 |  0:00:05s
epoch 19 | loss: 0.47574 | val_0_rmse: 0.67755 | val_1_rmse: 0.70306 |  0:00:05s
epoch 20 | loss: 0.47062 | val_0_rmse: 0.72576 | val_1_rmse: 0.76874 |  0:00:06s
epoch 21 | loss: 0.46897 | val_0_rmse: 0.67831 | val_1_rmse: 0.72851 |  0:00:06s
epoch 22 | loss: 0.48109 | val_0_rmse: 0.70177 | val_1_rmse: 0.72939 |  0:00:06s
epoch 23 | loss: 0.46185 | val_0_rmse: 0.69204 | val_1_rmse: 0.71678 |  0:00:06s
epoch 24 | loss: 0.46455 | val_0_rmse: 0.67603 | val_1_rmse: 0.69923 |  0:00:07s
epoch 25 | loss: 0.45361 | val_0_rmse: 0.69199 | val_1_rmse: 0.70426 |  0:00:07s
epoch 26 | loss: 0.45303 | val_0_rmse: 0.66243 | val_1_rmse: 0.67414 |  0:00:07s
epoch 27 | loss: 0.4541  | val_0_rmse: 0.66829 | val_1_rmse: 0.66583 |  0:00:08s
epoch 28 | loss: 0.47389 | val_0_rmse: 0.66729 | val_1_rmse: 0.67805 |  0:00:08s
epoch 29 | loss: 0.45376 | val_0_rmse: 0.66894 | val_1_rmse: 0.68469 |  0:00:08s
epoch 30 | loss: 0.43499 | val_0_rmse: 0.65465 | val_1_rmse: 0.67305 |  0:00:09s
epoch 31 | loss: 0.43305 | val_0_rmse: 0.64829 | val_1_rmse: 0.67206 |  0:00:09s
epoch 32 | loss: 0.42588 | val_0_rmse: 0.65596 | val_1_rmse: 0.66687 |  0:00:09s
epoch 33 | loss: 0.43206 | val_0_rmse: 0.65245 | val_1_rmse: 0.66657 |  0:00:09s
epoch 34 | loss: 0.42876 | val_0_rmse: 0.65338 | val_1_rmse: 0.65547 |  0:00:10s
epoch 35 | loss: 0.43434 | val_0_rmse: 0.6626  | val_1_rmse: 0.65369 |  0:00:10s
epoch 36 | loss: 0.42452 | val_0_rmse: 0.65745 | val_1_rmse: 0.65555 |  0:00:10s
epoch 37 | loss: 0.42178 | val_0_rmse: 0.63779 | val_1_rmse: 0.63118 |  0:00:11s
epoch 38 | loss: 0.40925 | val_0_rmse: 0.64873 | val_1_rmse: 0.64052 |  0:00:11s
epoch 39 | loss: 0.3943  | val_0_rmse: 0.66749 | val_1_rmse: 0.65934 |  0:00:11s
epoch 40 | loss: 0.38803 | val_0_rmse: 0.67901 | val_1_rmse: 0.67356 |  0:00:11s
epoch 41 | loss: 0.39021 | val_0_rmse: 0.63027 | val_1_rmse: 0.6274  |  0:00:12s
epoch 42 | loss: 0.37951 | val_0_rmse: 0.66898 | val_1_rmse: 0.66692 |  0:00:12s
epoch 43 | loss: 0.3914  | val_0_rmse: 0.65624 | val_1_rmse: 0.66047 |  0:00:12s
epoch 44 | loss: 0.37986 | val_0_rmse: 0.66505 | val_1_rmse: 0.67315 |  0:00:13s
epoch 45 | loss: 0.39823 | val_0_rmse: 0.63457 | val_1_rmse: 0.64658 |  0:00:13s
epoch 46 | loss: 0.39388 | val_0_rmse: 0.62268 | val_1_rmse: 0.64406 |  0:00:13s
epoch 47 | loss: 0.38744 | val_0_rmse: 0.61702 | val_1_rmse: 0.63821 |  0:00:13s
epoch 48 | loss: 0.38047 | val_0_rmse: 0.62047 | val_1_rmse: 0.63049 |  0:00:14s
epoch 49 | loss: 0.38369 | val_0_rmse: 0.6498  | val_1_rmse: 0.65544 |  0:00:14s
epoch 50 | loss: 0.38227 | val_0_rmse: 0.62515 | val_1_rmse: 0.63105 |  0:00:14s
epoch 51 | loss: 0.37848 | val_0_rmse: 0.61682 | val_1_rmse: 0.6215  |  0:00:15s
epoch 52 | loss: 0.36656 | val_0_rmse: 0.60638 | val_1_rmse: 0.61386 |  0:00:15s
epoch 53 | loss: 0.36821 | val_0_rmse: 0.62089 | val_1_rmse: 0.63276 |  0:00:15s
epoch 54 | loss: 0.38492 | val_0_rmse: 0.65817 | val_1_rmse: 0.66858 |  0:00:15s
epoch 55 | loss: 0.38305 | val_0_rmse: 0.62826 | val_1_rmse: 0.64081 |  0:00:16s
epoch 56 | loss: 0.3783  | val_0_rmse: 0.61061 | val_1_rmse: 0.62391 |  0:00:16s
epoch 57 | loss: 0.37218 | val_0_rmse: 0.59578 | val_1_rmse: 0.60413 |  0:00:16s
epoch 58 | loss: 0.35841 | val_0_rmse: 0.61233 | val_1_rmse: 0.61736 |  0:00:17s
epoch 59 | loss: 0.36606 | val_0_rmse: 0.60635 | val_1_rmse: 0.62071 |  0:00:17s
epoch 60 | loss: 0.36334 | val_0_rmse: 0.60659 | val_1_rmse: 0.62445 |  0:00:17s
epoch 61 | loss: 0.37155 | val_0_rmse: 0.60701 | val_1_rmse: 0.6192  |  0:00:17s
epoch 62 | loss: 0.37155 | val_0_rmse: 0.59042 | val_1_rmse: 0.61623 |  0:00:18s
epoch 63 | loss: 0.35765 | val_0_rmse: 0.60203 | val_1_rmse: 0.62157 |  0:00:18s
epoch 64 | loss: 0.36527 | val_0_rmse: 0.58781 | val_1_rmse: 0.61173 |  0:00:18s
epoch 65 | loss: 0.35531 | val_0_rmse: 0.59168 | val_1_rmse: 0.60573 |  0:00:19s
epoch 66 | loss: 0.36365 | val_0_rmse: 0.62322 | val_1_rmse: 0.63536 |  0:00:19s
epoch 67 | loss: 0.38257 | val_0_rmse: 0.60802 | val_1_rmse: 0.61995 |  0:00:19s
epoch 68 | loss: 0.38492 | val_0_rmse: 0.70984 | val_1_rmse: 0.72074 |  0:00:20s
epoch 69 | loss: 0.36856 | val_0_rmse: 0.60367 | val_1_rmse: 0.61851 |  0:00:20s
epoch 70 | loss: 0.37622 | val_0_rmse: 0.70089 | val_1_rmse: 0.70792 |  0:00:20s
epoch 71 | loss: 0.36668 | val_0_rmse: 0.61079 | val_1_rmse: 0.61416 |  0:00:20s
epoch 72 | loss: 0.35198 | val_0_rmse: 0.60026 | val_1_rmse: 0.60917 |  0:00:21s
epoch 73 | loss: 0.35584 | val_0_rmse: 0.5882  | val_1_rmse: 0.60107 |  0:00:21s
epoch 74 | loss: 0.3487  | val_0_rmse: 0.59271 | val_1_rmse: 0.6092  |  0:00:21s
epoch 75 | loss: 0.34387 | val_0_rmse: 0.59924 | val_1_rmse: 0.61288 |  0:00:22s
epoch 76 | loss: 0.3534  | val_0_rmse: 0.5801  | val_1_rmse: 0.59585 |  0:00:22s
epoch 77 | loss: 0.35828 | val_0_rmse: 0.66785 | val_1_rmse: 0.66658 |  0:00:22s
epoch 78 | loss: 0.35693 | val_0_rmse: 0.59375 | val_1_rmse: 0.60417 |  0:00:22s
epoch 79 | loss: 0.35689 | val_0_rmse: 0.59052 | val_1_rmse: 0.59461 |  0:00:23s
epoch 80 | loss: 0.34963 | val_0_rmse: 0.60642 | val_1_rmse: 0.6098  |  0:00:23s
epoch 81 | loss: 0.34991 | val_0_rmse: 0.5852  | val_1_rmse: 0.59182 |  0:00:23s
epoch 82 | loss: 0.35574 | val_0_rmse: 0.58254 | val_1_rmse: 0.59744 |  0:00:24s
epoch 83 | loss: 0.34693 | val_0_rmse: 0.57935 | val_1_rmse: 0.60091 |  0:00:24s
epoch 84 | loss: 0.34003 | val_0_rmse: 0.58251 | val_1_rmse: 0.59464 |  0:00:24s
epoch 85 | loss: 0.3412  | val_0_rmse: 0.57864 | val_1_rmse: 0.59592 |  0:00:24s
epoch 86 | loss: 0.33923 | val_0_rmse: 0.57665 | val_1_rmse: 0.59694 |  0:00:25s
epoch 87 | loss: 0.34054 | val_0_rmse: 0.57601 | val_1_rmse: 0.60003 |  0:00:25s
epoch 88 | loss: 0.34032 | val_0_rmse: 0.57375 | val_1_rmse: 0.59887 |  0:00:25s
epoch 89 | loss: 0.32941 | val_0_rmse: 0.60629 | val_1_rmse: 0.61751 |  0:00:26s
epoch 90 | loss: 0.35663 | val_0_rmse: 0.58053 | val_1_rmse: 0.60576 |  0:00:26s
epoch 91 | loss: 0.34336 | val_0_rmse: 0.57936 | val_1_rmse: 0.59512 |  0:00:26s
epoch 92 | loss: 0.33201 | val_0_rmse: 0.56805 | val_1_rmse: 0.58708 |  0:00:26s
epoch 93 | loss: 0.34145 | val_0_rmse: 0.58408 | val_1_rmse: 0.59375 |  0:00:27s
epoch 94 | loss: 0.33315 | val_0_rmse: 0.55628 | val_1_rmse: 0.5794  |  0:00:27s
epoch 95 | loss: 0.33838 | val_0_rmse: 0.62001 | val_1_rmse: 0.63547 |  0:00:27s
epoch 96 | loss: 0.33682 | val_0_rmse: 0.5934  | val_1_rmse: 0.61036 |  0:00:28s
epoch 97 | loss: 0.34325 | val_0_rmse: 0.59219 | val_1_rmse: 0.60834 |  0:00:28s
epoch 98 | loss: 0.32846 | val_0_rmse: 0.5904  | val_1_rmse: 0.60932 |  0:00:28s
epoch 99 | loss: 0.33467 | val_0_rmse: 0.56102 | val_1_rmse: 0.58825 |  0:00:28s
epoch 100| loss: 0.3275  | val_0_rmse: 0.57784 | val_1_rmse: 0.59572 |  0:00:29s
epoch 101| loss: 0.32303 | val_0_rmse: 0.56457 | val_1_rmse: 0.59367 |  0:00:29s
epoch 102| loss: 0.33189 | val_0_rmse: 0.57481 | val_1_rmse: 0.60476 |  0:00:29s
epoch 103| loss: 0.32817 | val_0_rmse: 0.56877 | val_1_rmse: 0.59318 |  0:00:30s
epoch 104| loss: 0.33696 | val_0_rmse: 0.57861 | val_1_rmse: 0.60628 |  0:00:30s
epoch 105| loss: 0.35776 | val_0_rmse: 0.57375 | val_1_rmse: 0.59402 |  0:00:30s
epoch 106| loss: 0.34234 | val_0_rmse: 0.57164 | val_1_rmse: 0.59021 |  0:00:30s
epoch 107| loss: 0.32986 | val_0_rmse: 0.55988 | val_1_rmse: 0.58799 |  0:00:31s
epoch 108| loss: 0.32971 | val_0_rmse: 0.56233 | val_1_rmse: 0.587   |  0:00:31s
epoch 109| loss: 0.32567 | val_0_rmse: 0.59869 | val_1_rmse: 0.61098 |  0:00:31s
epoch 110| loss: 0.32969 | val_0_rmse: 0.57682 | val_1_rmse: 0.59312 |  0:00:32s
epoch 111| loss: 0.33745 | val_0_rmse: 0.57397 | val_1_rmse: 0.58263 |  0:00:32s
epoch 112| loss: 0.33494 | val_0_rmse: 0.57992 | val_1_rmse: 0.59344 |  0:00:32s
epoch 113| loss: 0.33618 | val_0_rmse: 0.56625 | val_1_rmse: 0.58137 |  0:00:32s
epoch 114| loss: 0.33131 | val_0_rmse: 0.55844 | val_1_rmse: 0.58555 |  0:00:33s
epoch 115| loss: 0.32784 | val_0_rmse: 0.56313 | val_1_rmse: 0.58621 |  0:00:33s
epoch 116| loss: 0.32994 | val_0_rmse: 0.62455 | val_1_rmse: 0.63185 |  0:00:33s
epoch 117| loss: 0.33007 | val_0_rmse: 0.56792 | val_1_rmse: 0.58214 |  0:00:34s
epoch 118| loss: 0.33386 | val_0_rmse: 0.62129 | val_1_rmse: 0.6293  |  0:00:34s
epoch 119| loss: 0.33808 | val_0_rmse: 0.63297 | val_1_rmse: 0.63336 |  0:00:34s
epoch 120| loss: 0.32474 | val_0_rmse: 0.6521  | val_1_rmse: 0.65108 |  0:00:34s
epoch 121| loss: 0.33138 | val_0_rmse: 0.62668 | val_1_rmse: 0.61977 |  0:00:35s
epoch 122| loss: 0.33476 | val_0_rmse: 0.57423 | val_1_rmse: 0.58061 |  0:00:35s
epoch 123| loss: 0.32725 | val_0_rmse: 0.56561 | val_1_rmse: 0.57262 |  0:00:35s
epoch 124| loss: 0.3205  | val_0_rmse: 0.56682 | val_1_rmse: 0.58433 |  0:00:36s
epoch 125| loss: 0.3155  | val_0_rmse: 0.56085 | val_1_rmse: 0.57781 |  0:00:36s
epoch 126| loss: 0.32594 | val_0_rmse: 0.57516 | val_1_rmse: 0.59369 |  0:00:36s
epoch 127| loss: 0.32914 | val_0_rmse: 0.58148 | val_1_rmse: 0.59164 |  0:00:36s
epoch 128| loss: 0.31868 | val_0_rmse: 0.60395 | val_1_rmse: 0.61416 |  0:00:37s
epoch 129| loss: 0.32265 | val_0_rmse: 0.5609  | val_1_rmse: 0.57547 |  0:00:37s
epoch 130| loss: 0.31904 | val_0_rmse: 0.55417 | val_1_rmse: 0.57266 |  0:00:37s
epoch 131| loss: 0.3245  | val_0_rmse: 0.54977 | val_1_rmse: 0.57416 |  0:00:38s
epoch 132| loss: 0.32595 | val_0_rmse: 0.55896 | val_1_rmse: 0.58404 |  0:00:38s
epoch 133| loss: 0.32034 | val_0_rmse: 0.54792 | val_1_rmse: 0.57376 |  0:00:38s
epoch 134| loss: 0.321   | val_0_rmse: 0.56207 | val_1_rmse: 0.58983 |  0:00:38s
epoch 135| loss: 0.31819 | val_0_rmse: 0.55099 | val_1_rmse: 0.58382 |  0:00:39s
epoch 136| loss: 0.31206 | val_0_rmse: 0.54856 | val_1_rmse: 0.58517 |  0:00:39s
epoch 137| loss: 0.32183 | val_0_rmse: 0.60167 | val_1_rmse: 0.62224 |  0:00:39s
epoch 138| loss: 0.31615 | val_0_rmse: 0.56759 | val_1_rmse: 0.58445 |  0:00:40s
epoch 139| loss: 0.33018 | val_0_rmse: 0.60666 | val_1_rmse: 0.61784 |  0:00:40s
epoch 140| loss: 0.3182  | val_0_rmse: 0.5517  | val_1_rmse: 0.57478 |  0:00:40s
epoch 141| loss: 0.31414 | val_0_rmse: 0.55801 | val_1_rmse: 0.58538 |  0:00:41s
epoch 142| loss: 0.31252 | val_0_rmse: 0.56608 | val_1_rmse: 0.58202 |  0:00:41s
epoch 143| loss: 0.31683 | val_0_rmse: 0.56151 | val_1_rmse: 0.58429 |  0:00:41s
epoch 144| loss: 0.3151  | val_0_rmse: 0.54897 | val_1_rmse: 0.56993 |  0:00:41s
epoch 145| loss: 0.31265 | val_0_rmse: 0.55343 | val_1_rmse: 0.56843 |  0:00:42s
epoch 146| loss: 0.31966 | val_0_rmse: 0.54846 | val_1_rmse: 0.56764 |  0:00:42s
epoch 147| loss: 0.3088  | val_0_rmse: 0.54648 | val_1_rmse: 0.56653 |  0:00:42s
epoch 148| loss: 0.32326 | val_0_rmse: 0.58307 | val_1_rmse: 0.59048 |  0:00:43s
epoch 149| loss: 0.32128 | val_0_rmse: 0.56103 | val_1_rmse: 0.57228 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.56653
Best weights from best epoch are automatically used!
ended training at: 08:43:41
Feature importance:
[('Area', 0.2532866449113897), ('Baths', 0.21166591330293272), ('Beds', 0.09823618283510077), ('Latitude', 0.19821252657178923), ('Longitude', 0.21776818236488818), ('Month', 0.020830550013899454), ('Year', 0.0)]
Mean squared error is of 2264649359.722734
Mean absolute error:32533.296576214983
MAPE:0.28996674809619477
R2 score:0.728083575193633
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:43:41
epoch 0  | loss: 1.24632 | val_0_rmse: 1.07444 | val_1_rmse: 1.03651 |  0:00:00s
epoch 1  | loss: 0.6152  | val_0_rmse: 0.93215 | val_1_rmse: 0.94538 |  0:00:00s
epoch 2  | loss: 0.53864 | val_0_rmse: 0.88916 | val_1_rmse: 0.89797 |  0:00:00s
epoch 3  | loss: 0.52003 | val_0_rmse: 0.74791 | val_1_rmse: 0.8126  |  0:00:01s
epoch 4  | loss: 0.51651 | val_0_rmse: 0.71272 | val_1_rmse: 0.75809 |  0:00:01s
epoch 5  | loss: 0.4894  | val_0_rmse: 0.69818 | val_1_rmse: 0.75127 |  0:00:01s
epoch 6  | loss: 0.48579 | val_0_rmse: 0.69087 | val_1_rmse: 0.74583 |  0:00:02s
epoch 7  | loss: 0.47597 | val_0_rmse: 0.68624 | val_1_rmse: 0.74258 |  0:00:02s
epoch 8  | loss: 0.47114 | val_0_rmse: 0.68476 | val_1_rmse: 0.73791 |  0:00:02s
epoch 9  | loss: 0.46624 | val_0_rmse: 0.67957 | val_1_rmse: 0.74278 |  0:00:02s
epoch 10 | loss: 0.46996 | val_0_rmse: 0.6794  | val_1_rmse: 0.73486 |  0:00:03s
epoch 11 | loss: 0.45819 | val_0_rmse: 0.67497 | val_1_rmse: 0.719   |  0:00:03s
epoch 12 | loss: 0.46231 | val_0_rmse: 0.66787 | val_1_rmse: 0.7126  |  0:00:03s
epoch 13 | loss: 0.44045 | val_0_rmse: 0.66996 | val_1_rmse: 0.72193 |  0:00:04s
epoch 14 | loss: 0.45725 | val_0_rmse: 0.67006 | val_1_rmse: 0.71342 |  0:00:04s
epoch 15 | loss: 0.45337 | val_0_rmse: 0.67071 | val_1_rmse: 0.72387 |  0:00:04s
epoch 16 | loss: 0.44893 | val_0_rmse: 0.66147 | val_1_rmse: 0.72186 |  0:00:05s
epoch 17 | loss: 0.43237 | val_0_rmse: 0.66212 | val_1_rmse: 0.7246  |  0:00:05s
epoch 18 | loss: 0.44308 | val_0_rmse: 0.64335 | val_1_rmse: 0.71081 |  0:00:05s
epoch 19 | loss: 0.43367 | val_0_rmse: 0.66078 | val_1_rmse: 0.72453 |  0:00:05s
epoch 20 | loss: 0.42146 | val_0_rmse: 0.63962 | val_1_rmse: 0.69233 |  0:00:06s
epoch 21 | loss: 0.42931 | val_0_rmse: 0.66361 | val_1_rmse: 0.71769 |  0:00:06s
epoch 22 | loss: 0.43132 | val_0_rmse: 0.63578 | val_1_rmse: 0.70347 |  0:00:06s
epoch 23 | loss: 0.43579 | val_0_rmse: 0.66915 | val_1_rmse: 0.73252 |  0:00:07s
epoch 24 | loss: 0.42413 | val_0_rmse: 0.6439  | val_1_rmse: 0.69101 |  0:00:07s
epoch 25 | loss: 0.43351 | val_0_rmse: 0.68043 | val_1_rmse: 0.71892 |  0:00:07s
epoch 26 | loss: 0.41199 | val_0_rmse: 0.64648 | val_1_rmse: 0.70069 |  0:00:07s
epoch 27 | loss: 0.41704 | val_0_rmse: 0.63183 | val_1_rmse: 0.68673 |  0:00:08s
epoch 28 | loss: 0.3947  | val_0_rmse: 0.62535 | val_1_rmse: 0.68076 |  0:00:08s
epoch 29 | loss: 0.40321 | val_0_rmse: 0.63321 | val_1_rmse: 0.68577 |  0:00:08s
epoch 30 | loss: 0.40236 | val_0_rmse: 0.68396 | val_1_rmse: 0.72909 |  0:00:09s
epoch 31 | loss: 0.40096 | val_0_rmse: 0.62793 | val_1_rmse: 0.68197 |  0:00:09s
epoch 32 | loss: 0.39659 | val_0_rmse: 0.62791 | val_1_rmse: 0.67925 |  0:00:09s
epoch 33 | loss: 0.39174 | val_0_rmse: 0.63072 | val_1_rmse: 0.68543 |  0:00:09s
epoch 34 | loss: 0.39214 | val_0_rmse: 0.62825 | val_1_rmse: 0.67993 |  0:00:10s
epoch 35 | loss: 0.39119 | val_0_rmse: 0.63592 | val_1_rmse: 0.6823  |  0:00:10s
epoch 36 | loss: 0.38695 | val_0_rmse: 0.62127 | val_1_rmse: 0.67709 |  0:00:10s
epoch 37 | loss: 0.39747 | val_0_rmse: 0.62931 | val_1_rmse: 0.68714 |  0:00:11s
epoch 38 | loss: 0.39176 | val_0_rmse: 0.65767 | val_1_rmse: 0.70294 |  0:00:11s
epoch 39 | loss: 0.39964 | val_0_rmse: 0.62704 | val_1_rmse: 0.67913 |  0:00:11s
epoch 40 | loss: 0.39398 | val_0_rmse: 0.64243 | val_1_rmse: 0.68471 |  0:00:12s
epoch 41 | loss: 0.37608 | val_0_rmse: 0.61283 | val_1_rmse: 0.67001 |  0:00:12s
epoch 42 | loss: 0.37617 | val_0_rmse: 0.62102 | val_1_rmse: 0.67734 |  0:00:12s
epoch 43 | loss: 0.37971 | val_0_rmse: 0.63631 | val_1_rmse: 0.68442 |  0:00:12s
epoch 44 | loss: 0.38819 | val_0_rmse: 0.62151 | val_1_rmse: 0.67249 |  0:00:13s
epoch 45 | loss: 0.38592 | val_0_rmse: 0.64253 | val_1_rmse: 0.68452 |  0:00:13s
epoch 46 | loss: 0.38121 | val_0_rmse: 0.61265 | val_1_rmse: 0.66718 |  0:00:13s
epoch 47 | loss: 0.36683 | val_0_rmse: 0.62664 | val_1_rmse: 0.68782 |  0:00:14s
epoch 48 | loss: 0.38435 | val_0_rmse: 0.61889 | val_1_rmse: 0.67813 |  0:00:14s
epoch 49 | loss: 0.40454 | val_0_rmse: 0.62534 | val_1_rmse: 0.6759  |  0:00:14s
epoch 50 | loss: 0.37718 | val_0_rmse: 0.62221 | val_1_rmse: 0.6788  |  0:00:14s
epoch 51 | loss: 0.38621 | val_0_rmse: 0.61557 | val_1_rmse: 0.66469 |  0:00:15s
epoch 52 | loss: 0.38679 | val_0_rmse: 0.61544 | val_1_rmse: 0.65968 |  0:00:15s
epoch 53 | loss: 0.37105 | val_0_rmse: 0.60676 | val_1_rmse: 0.65182 |  0:00:15s
epoch 54 | loss: 0.37696 | val_0_rmse: 0.66392 | val_1_rmse: 0.71662 |  0:00:16s
epoch 55 | loss: 0.38583 | val_0_rmse: 0.61589 | val_1_rmse: 0.66816 |  0:00:16s
epoch 56 | loss: 0.3864  | val_0_rmse: 0.6201  | val_1_rmse: 0.65939 |  0:00:16s
epoch 57 | loss: 0.37695 | val_0_rmse: 0.61327 | val_1_rmse: 0.66025 |  0:00:16s
epoch 58 | loss: 0.38631 | val_0_rmse: 0.63892 | val_1_rmse: 0.68644 |  0:00:17s
epoch 59 | loss: 0.37357 | val_0_rmse: 0.61208 | val_1_rmse: 0.65747 |  0:00:17s
epoch 60 | loss: 0.3693  | val_0_rmse: 0.66407 | val_1_rmse: 0.69963 |  0:00:17s
epoch 61 | loss: 0.37903 | val_0_rmse: 0.62893 | val_1_rmse: 0.67829 |  0:00:18s
epoch 62 | loss: 0.3809  | val_0_rmse: 0.6291  | val_1_rmse: 0.67483 |  0:00:18s
epoch 63 | loss: 0.36884 | val_0_rmse: 0.62207 | val_1_rmse: 0.67664 |  0:00:18s
epoch 64 | loss: 0.37285 | val_0_rmse: 0.65793 | val_1_rmse: 0.69458 |  0:00:18s
epoch 65 | loss: 0.36565 | val_0_rmse: 0.61845 | val_1_rmse: 0.657   |  0:00:19s
epoch 66 | loss: 0.36842 | val_0_rmse: 0.62037 | val_1_rmse: 0.66268 |  0:00:19s
epoch 67 | loss: 0.37462 | val_0_rmse: 0.6013  | val_1_rmse: 0.64424 |  0:00:19s
epoch 68 | loss: 0.36737 | val_0_rmse: 0.61928 | val_1_rmse: 0.65907 |  0:00:20s
epoch 69 | loss: 0.35808 | val_0_rmse: 0.61512 | val_1_rmse: 0.65278 |  0:00:20s
epoch 70 | loss: 0.35917 | val_0_rmse: 0.62811 | val_1_rmse: 0.67084 |  0:00:20s
epoch 71 | loss: 0.36033 | val_0_rmse: 0.61835 | val_1_rmse: 0.66866 |  0:00:20s
epoch 72 | loss: 0.36486 | val_0_rmse: 0.64096 | val_1_rmse: 0.68986 |  0:00:21s
epoch 73 | loss: 0.36552 | val_0_rmse: 0.59979 | val_1_rmse: 0.65285 |  0:00:21s
epoch 74 | loss: 0.35449 | val_0_rmse: 0.61849 | val_1_rmse: 0.67338 |  0:00:21s
epoch 75 | loss: 0.35693 | val_0_rmse: 0.59823 | val_1_rmse: 0.65963 |  0:00:22s
epoch 76 | loss: 0.34942 | val_0_rmse: 0.60708 | val_1_rmse: 0.6636  |  0:00:22s
epoch 77 | loss: 0.34644 | val_0_rmse: 0.60603 | val_1_rmse: 0.65335 |  0:00:22s
epoch 78 | loss: 0.34753 | val_0_rmse: 0.62983 | val_1_rmse: 0.67619 |  0:00:22s
epoch 79 | loss: 0.35356 | val_0_rmse: 0.61909 | val_1_rmse: 0.66562 |  0:00:23s
epoch 80 | loss: 0.35359 | val_0_rmse: 0.6544  | val_1_rmse: 0.69529 |  0:00:23s
epoch 81 | loss: 0.3513  | val_0_rmse: 0.59008 | val_1_rmse: 0.64947 |  0:00:23s
epoch 82 | loss: 0.35077 | val_0_rmse: 0.61435 | val_1_rmse: 0.66952 |  0:00:24s
epoch 83 | loss: 0.35604 | val_0_rmse: 0.58925 | val_1_rmse: 0.64093 |  0:00:24s
epoch 84 | loss: 0.34436 | val_0_rmse: 0.64534 | val_1_rmse: 0.69092 |  0:00:24s
epoch 85 | loss: 0.33609 | val_0_rmse: 0.59175 | val_1_rmse: 0.64173 |  0:00:25s
epoch 86 | loss: 0.33263 | val_0_rmse: 0.60223 | val_1_rmse: 0.64697 |  0:00:25s
epoch 87 | loss: 0.33134 | val_0_rmse: 0.61021 | val_1_rmse: 0.64506 |  0:00:25s
epoch 88 | loss: 0.33514 | val_0_rmse: 0.58753 | val_1_rmse: 0.62279 |  0:00:25s
epoch 89 | loss: 0.33715 | val_0_rmse: 0.57843 | val_1_rmse: 0.6191  |  0:00:26s
epoch 90 | loss: 0.32851 | val_0_rmse: 0.61042 | val_1_rmse: 0.64193 |  0:00:26s
epoch 91 | loss: 0.3296  | val_0_rmse: 0.58263 | val_1_rmse: 0.61531 |  0:00:26s
epoch 92 | loss: 0.33939 | val_0_rmse: 0.62297 | val_1_rmse: 0.65735 |  0:00:27s
epoch 93 | loss: 0.33959 | val_0_rmse: 0.58952 | val_1_rmse: 0.63181 |  0:00:27s
epoch 94 | loss: 0.32406 | val_0_rmse: 0.59519 | val_1_rmse: 0.63419 |  0:00:27s
epoch 95 | loss: 0.32836 | val_0_rmse: 0.59875 | val_1_rmse: 0.64068 |  0:00:27s
epoch 96 | loss: 0.32644 | val_0_rmse: 0.58416 | val_1_rmse: 0.62612 |  0:00:28s
epoch 97 | loss: 0.33808 | val_0_rmse: 0.59143 | val_1_rmse: 0.62567 |  0:00:28s
epoch 98 | loss: 0.3415  | val_0_rmse: 0.60579 | val_1_rmse: 0.64593 |  0:00:28s
epoch 99 | loss: 0.3501  | val_0_rmse: 0.61651 | val_1_rmse: 0.65785 |  0:00:29s
epoch 100| loss: 0.3406  | val_0_rmse: 0.58248 | val_1_rmse: 0.62522 |  0:00:29s
epoch 101| loss: 0.33871 | val_0_rmse: 0.64046 | val_1_rmse: 0.6819  |  0:00:29s
epoch 102| loss: 0.33941 | val_0_rmse: 0.64481 | val_1_rmse: 0.68047 |  0:00:29s
epoch 103| loss: 0.33335 | val_0_rmse: 0.59323 | val_1_rmse: 0.63048 |  0:00:30s
epoch 104| loss: 0.3327  | val_0_rmse: 0.59734 | val_1_rmse: 0.63098 |  0:00:30s
epoch 105| loss: 0.33835 | val_0_rmse: 0.62608 | val_1_rmse: 0.66097 |  0:00:30s
epoch 106| loss: 0.33108 | val_0_rmse: 0.56762 | val_1_rmse: 0.6087  |  0:00:31s
epoch 107| loss: 0.32952 | val_0_rmse: 0.62477 | val_1_rmse: 0.66274 |  0:00:31s
epoch 108| loss: 0.32329 | val_0_rmse: 0.59575 | val_1_rmse: 0.63309 |  0:00:31s
epoch 109| loss: 0.32212 | val_0_rmse: 0.59691 | val_1_rmse: 0.63698 |  0:00:31s
epoch 110| loss: 0.31971 | val_0_rmse: 0.59084 | val_1_rmse: 0.62628 |  0:00:32s
epoch 111| loss: 0.3148  | val_0_rmse: 0.58641 | val_1_rmse: 0.62038 |  0:00:32s
epoch 112| loss: 0.31254 | val_0_rmse: 0.59077 | val_1_rmse: 0.62843 |  0:00:32s
epoch 113| loss: 0.31223 | val_0_rmse: 0.5726  | val_1_rmse: 0.62758 |  0:00:33s
epoch 114| loss: 0.32001 | val_0_rmse: 0.5697  | val_1_rmse: 0.6224  |  0:00:33s
epoch 115| loss: 0.32208 | val_0_rmse: 0.58656 | val_1_rmse: 0.63429 |  0:00:33s
epoch 116| loss: 0.31439 | val_0_rmse: 0.60907 | val_1_rmse: 0.65595 |  0:00:33s
epoch 117| loss: 0.31673 | val_0_rmse: 0.61608 | val_1_rmse: 0.66061 |  0:00:34s
epoch 118| loss: 0.32131 | val_0_rmse: 0.57103 | val_1_rmse: 0.61941 |  0:00:34s
epoch 119| loss: 0.31466 | val_0_rmse: 0.58066 | val_1_rmse: 0.62217 |  0:00:34s
epoch 120| loss: 0.31826 | val_0_rmse: 0.58162 | val_1_rmse: 0.6243  |  0:00:35s
epoch 121| loss: 0.31828 | val_0_rmse: 0.5796  | val_1_rmse: 0.62496 |  0:00:35s
epoch 122| loss: 0.31985 | val_0_rmse: 0.57976 | val_1_rmse: 0.62552 |  0:00:35s
epoch 123| loss: 0.31335 | val_0_rmse: 0.58946 | val_1_rmse: 0.64038 |  0:00:35s
epoch 124| loss: 0.31067 | val_0_rmse: 0.57538 | val_1_rmse: 0.63149 |  0:00:36s
epoch 125| loss: 0.33039 | val_0_rmse: 0.59444 | val_1_rmse: 0.65033 |  0:00:36s
epoch 126| loss: 0.30808 | val_0_rmse: 0.57707 | val_1_rmse: 0.62868 |  0:00:36s
epoch 127| loss: 0.31917 | val_0_rmse: 0.58015 | val_1_rmse: 0.62352 |  0:00:37s
epoch 128| loss: 0.31717 | val_0_rmse: 0.58254 | val_1_rmse: 0.644   |  0:00:37s
epoch 129| loss: 0.32979 | val_0_rmse: 0.60377 | val_1_rmse: 0.66746 |  0:00:37s
epoch 130| loss: 0.32449 | val_0_rmse: 0.62465 | val_1_rmse: 0.67654 |  0:00:37s
epoch 131| loss: 0.31467 | val_0_rmse: 0.60884 | val_1_rmse: 0.65384 |  0:00:38s
epoch 132| loss: 0.31254 | val_0_rmse: 0.56765 | val_1_rmse: 0.61874 |  0:00:38s
epoch 133| loss: 0.30986 | val_0_rmse: 0.57333 | val_1_rmse: 0.63792 |  0:00:38s
epoch 134| loss: 0.3091  | val_0_rmse: 0.57682 | val_1_rmse: 0.63331 |  0:00:39s
epoch 135| loss: 0.30782 | val_0_rmse: 0.59092 | val_1_rmse: 0.66076 |  0:00:39s
epoch 136| loss: 0.30719 | val_0_rmse: 0.56614 | val_1_rmse: 0.64193 |  0:00:39s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.6087
Best weights from best epoch are automatically used!
ended training at: 08:44:21
Feature importance:
[('Area', 0.2655798967052813), ('Baths', 0.18233500824696824), ('Beds', 0.012209498528137476), ('Latitude', 0.2606354593754753), ('Longitude', 0.205134982130028), ('Month', 0.0028897800578288175), ('Year', 0.07121537495628086)]
Mean squared error is of 2809870282.2112565
Mean absolute error:36367.736800695035
MAPE:0.33656385855024706
R2 score:0.6550084182359845
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:44:21
epoch 0  | loss: 1.23624 | val_0_rmse: 1.03923 | val_1_rmse: 0.92998 |  0:00:00s
epoch 1  | loss: 0.60276 | val_0_rmse: 0.84096 | val_1_rmse: 0.94127 |  0:00:00s
epoch 2  | loss: 0.56118 | val_0_rmse: 0.79296 | val_1_rmse: 0.85284 |  0:00:00s
epoch 3  | loss: 0.52888 | val_0_rmse: 0.76025 | val_1_rmse: 0.76132 |  0:00:01s
epoch 4  | loss: 0.51287 | val_0_rmse: 0.73379 | val_1_rmse: 0.74214 |  0:00:01s
epoch 5  | loss: 0.50687 | val_0_rmse: 0.70223 | val_1_rmse: 0.71598 |  0:00:01s
epoch 6  | loss: 0.48866 | val_0_rmse: 0.69919 | val_1_rmse: 0.70792 |  0:00:02s
epoch 7  | loss: 0.48083 | val_0_rmse: 0.72815 | val_1_rmse: 0.7642  |  0:00:02s
epoch 8  | loss: 0.47266 | val_0_rmse: 0.68848 | val_1_rmse: 0.712   |  0:00:02s
epoch 9  | loss: 0.46904 | val_0_rmse: 0.67916 | val_1_rmse: 0.69455 |  0:00:02s
epoch 10 | loss: 0.47827 | val_0_rmse: 0.68293 | val_1_rmse: 0.6828  |  0:00:03s
epoch 11 | loss: 0.46601 | val_0_rmse: 0.67776 | val_1_rmse: 0.67538 |  0:00:03s
epoch 12 | loss: 0.46895 | val_0_rmse: 0.678   | val_1_rmse: 0.67561 |  0:00:03s
epoch 13 | loss: 0.45971 | val_0_rmse: 0.69842 | val_1_rmse: 0.70439 |  0:00:04s
epoch 14 | loss: 0.45401 | val_0_rmse: 0.66319 | val_1_rmse: 0.6757  |  0:00:04s
epoch 15 | loss: 0.43859 | val_0_rmse: 0.65588 | val_1_rmse: 0.66967 |  0:00:04s
epoch 16 | loss: 0.44262 | val_0_rmse: 0.65036 | val_1_rmse: 0.66094 |  0:00:04s
epoch 17 | loss: 0.42904 | val_0_rmse: 0.65659 | val_1_rmse: 0.6637  |  0:00:05s
epoch 18 | loss: 0.43644 | val_0_rmse: 0.65476 | val_1_rmse: 0.6654  |  0:00:05s
epoch 19 | loss: 0.43513 | val_0_rmse: 0.67474 | val_1_rmse: 0.67607 |  0:00:05s
epoch 20 | loss: 0.4466  | val_0_rmse: 0.66823 | val_1_rmse: 0.67895 |  0:00:06s
epoch 21 | loss: 0.44679 | val_0_rmse: 0.66453 | val_1_rmse: 0.66224 |  0:00:06s
epoch 22 | loss: 0.43543 | val_0_rmse: 0.65444 | val_1_rmse: 0.65599 |  0:00:06s
epoch 23 | loss: 0.43199 | val_0_rmse: 0.6605  | val_1_rmse: 0.66494 |  0:00:07s
epoch 24 | loss: 0.42686 | val_0_rmse: 0.65409 | val_1_rmse: 0.66342 |  0:00:07s
epoch 25 | loss: 0.42273 | val_0_rmse: 0.65268 | val_1_rmse: 0.66094 |  0:00:07s
epoch 26 | loss: 0.42074 | val_0_rmse: 0.65549 | val_1_rmse: 0.65996 |  0:00:07s
epoch 27 | loss: 0.42675 | val_0_rmse: 0.64486 | val_1_rmse: 0.65356 |  0:00:08s
epoch 28 | loss: 0.41304 | val_0_rmse: 0.64626 | val_1_rmse: 0.6533  |  0:00:08s
epoch 29 | loss: 0.42669 | val_0_rmse: 0.65031 | val_1_rmse: 0.65771 |  0:00:08s
epoch 30 | loss: 0.4102  | val_0_rmse: 0.63987 | val_1_rmse: 0.65341 |  0:00:09s
epoch 31 | loss: 0.41829 | val_0_rmse: 0.63949 | val_1_rmse: 0.64176 |  0:00:09s
epoch 32 | loss: 0.40724 | val_0_rmse: 0.63816 | val_1_rmse: 0.64525 |  0:00:09s
epoch 33 | loss: 0.41574 | val_0_rmse: 0.64626 | val_1_rmse: 0.64872 |  0:00:09s
epoch 34 | loss: 0.40965 | val_0_rmse: 0.69064 | val_1_rmse: 0.66046 |  0:00:10s
epoch 35 | loss: 0.4237  | val_0_rmse: 0.65478 | val_1_rmse: 0.66689 |  0:00:10s
epoch 36 | loss: 0.41333 | val_0_rmse: 0.67035 | val_1_rmse: 0.68531 |  0:00:10s
epoch 37 | loss: 0.40695 | val_0_rmse: 0.64489 | val_1_rmse: 0.66594 |  0:00:11s
epoch 38 | loss: 0.40742 | val_0_rmse: 0.65987 | val_1_rmse: 0.65775 |  0:00:11s
epoch 39 | loss: 0.415   | val_0_rmse: 0.66868 | val_1_rmse: 0.67229 |  0:00:11s
epoch 40 | loss: 0.41529 | val_0_rmse: 0.66903 | val_1_rmse: 0.67982 |  0:00:11s
epoch 41 | loss: 0.4149  | val_0_rmse: 0.64734 | val_1_rmse: 0.65923 |  0:00:12s
epoch 42 | loss: 0.4045  | val_0_rmse: 0.6571  | val_1_rmse: 0.66528 |  0:00:12s
epoch 43 | loss: 0.39045 | val_0_rmse: 0.69036 | val_1_rmse: 0.68281 |  0:00:12s
epoch 44 | loss: 0.38457 | val_0_rmse: 0.63316 | val_1_rmse: 0.63525 |  0:00:13s
epoch 45 | loss: 0.4021  | val_0_rmse: 0.81322 | val_1_rmse: 0.81794 |  0:00:13s
epoch 46 | loss: 0.39162 | val_0_rmse: 0.61404 | val_1_rmse: 0.6173  |  0:00:13s
epoch 47 | loss: 0.38289 | val_0_rmse: 0.6167  | val_1_rmse: 0.613   |  0:00:13s
epoch 48 | loss: 0.38209 | val_0_rmse: 0.61206 | val_1_rmse: 0.62408 |  0:00:14s
epoch 49 | loss: 0.36175 | val_0_rmse: 0.60409 | val_1_rmse: 0.61955 |  0:00:14s
epoch 50 | loss: 0.36389 | val_0_rmse: 0.62518 | val_1_rmse: 0.63234 |  0:00:14s
epoch 51 | loss: 0.36348 | val_0_rmse: 0.59847 | val_1_rmse: 0.59643 |  0:00:15s
epoch 52 | loss: 0.36783 | val_0_rmse: 0.63204 | val_1_rmse: 0.62566 |  0:00:15s
epoch 53 | loss: 0.36263 | val_0_rmse: 0.63185 | val_1_rmse: 0.63127 |  0:00:15s
epoch 54 | loss: 0.35696 | val_0_rmse: 0.60086 | val_1_rmse: 0.6004  |  0:00:15s
epoch 55 | loss: 0.36424 | val_0_rmse: 0.61601 | val_1_rmse: 0.61083 |  0:00:16s
epoch 56 | loss: 0.3551  | val_0_rmse: 0.59627 | val_1_rmse: 0.59292 |  0:00:16s
epoch 57 | loss: 0.36284 | val_0_rmse: 0.6148  | val_1_rmse: 0.61244 |  0:00:16s
epoch 58 | loss: 0.36375 | val_0_rmse: 0.67616 | val_1_rmse: 0.67001 |  0:00:17s
epoch 59 | loss: 0.35476 | val_0_rmse: 0.59167 | val_1_rmse: 0.58421 |  0:00:17s
epoch 60 | loss: 0.35423 | val_0_rmse: 0.60364 | val_1_rmse: 0.59039 |  0:00:17s
epoch 61 | loss: 0.33844 | val_0_rmse: 0.59605 | val_1_rmse: 0.58749 |  0:00:18s
epoch 62 | loss: 0.38405 | val_0_rmse: 0.60593 | val_1_rmse: 0.60031 |  0:00:18s
epoch 63 | loss: 0.37287 | val_0_rmse: 0.64272 | val_1_rmse: 0.63195 |  0:00:18s
epoch 64 | loss: 0.3852  | val_0_rmse: 0.61754 | val_1_rmse: 0.60935 |  0:00:18s
epoch 65 | loss: 0.36125 | val_0_rmse: 0.67664 | val_1_rmse: 0.67583 |  0:00:19s
epoch 66 | loss: 0.35466 | val_0_rmse: 0.61247 | val_1_rmse: 0.6045  |  0:00:19s
epoch 67 | loss: 0.35816 | val_0_rmse: 0.60164 | val_1_rmse: 0.59858 |  0:00:19s
epoch 68 | loss: 0.35411 | val_0_rmse: 0.61749 | val_1_rmse: 0.60901 |  0:00:20s
epoch 69 | loss: 0.35762 | val_0_rmse: 0.59694 | val_1_rmse: 0.59343 |  0:00:20s
epoch 70 | loss: 0.35476 | val_0_rmse: 0.5926  | val_1_rmse: 0.58468 |  0:00:20s
epoch 71 | loss: 0.34826 | val_0_rmse: 0.62443 | val_1_rmse: 0.61631 |  0:00:20s
epoch 72 | loss: 0.34537 | val_0_rmse: 0.63674 | val_1_rmse: 0.63327 |  0:00:21s
epoch 73 | loss: 0.33722 | val_0_rmse: 0.59191 | val_1_rmse: 0.59082 |  0:00:21s
epoch 74 | loss: 0.33612 | val_0_rmse: 0.60353 | val_1_rmse: 0.59972 |  0:00:21s
epoch 75 | loss: 0.33516 | val_0_rmse: 0.63327 | val_1_rmse: 0.62526 |  0:00:22s
epoch 76 | loss: 0.32934 | val_0_rmse: 0.59845 | val_1_rmse: 0.59068 |  0:00:22s
epoch 77 | loss: 0.3319  | val_0_rmse: 0.60893 | val_1_rmse: 0.60118 |  0:00:22s
epoch 78 | loss: 0.33309 | val_0_rmse: 0.59683 | val_1_rmse: 0.59099 |  0:00:22s
epoch 79 | loss: 0.3364  | val_0_rmse: 0.60702 | val_1_rmse: 0.6004  |  0:00:23s
epoch 80 | loss: 0.33246 | val_0_rmse: 0.62064 | val_1_rmse: 0.61074 |  0:00:23s
epoch 81 | loss: 0.33247 | val_0_rmse: 0.57201 | val_1_rmse: 0.57182 |  0:00:23s
epoch 82 | loss: 0.33171 | val_0_rmse: 0.60554 | val_1_rmse: 0.60086 |  0:00:24s
epoch 83 | loss: 0.33363 | val_0_rmse: 0.59286 | val_1_rmse: 0.59656 |  0:00:24s
epoch 84 | loss: 0.33985 | val_0_rmse: 0.61729 | val_1_rmse: 0.61158 |  0:00:24s
epoch 85 | loss: 0.35296 | val_0_rmse: 0.60842 | val_1_rmse: 0.6067  |  0:00:24s
epoch 86 | loss: 0.33573 | val_0_rmse: 0.64504 | val_1_rmse: 0.64084 |  0:00:25s
epoch 87 | loss: 0.3357  | val_0_rmse: 0.58903 | val_1_rmse: 0.59015 |  0:00:25s
epoch 88 | loss: 0.33258 | val_0_rmse: 0.56348 | val_1_rmse: 0.56264 |  0:00:25s
epoch 89 | loss: 0.33376 | val_0_rmse: 0.60212 | val_1_rmse: 0.59583 |  0:00:26s
epoch 90 | loss: 0.32347 | val_0_rmse: 0.58019 | val_1_rmse: 0.57131 |  0:00:26s
epoch 91 | loss: 0.32203 | val_0_rmse: 0.56603 | val_1_rmse: 0.55716 |  0:00:26s
epoch 92 | loss: 0.32672 | val_0_rmse: 0.63062 | val_1_rmse: 0.61846 |  0:00:26s
epoch 93 | loss: 0.31933 | val_0_rmse: 0.57308 | val_1_rmse: 0.56135 |  0:00:27s
epoch 94 | loss: 0.32018 | val_0_rmse: 0.5666  | val_1_rmse: 0.5576  |  0:00:27s
epoch 95 | loss: 0.32021 | val_0_rmse: 0.62858 | val_1_rmse: 0.62051 |  0:00:27s
epoch 96 | loss: 0.32312 | val_0_rmse: 0.59821 | val_1_rmse: 0.59089 |  0:00:28s
epoch 97 | loss: 0.31834 | val_0_rmse: 0.56338 | val_1_rmse: 0.55902 |  0:00:28s
epoch 98 | loss: 0.31851 | val_0_rmse: 0.57946 | val_1_rmse: 0.5774  |  0:00:28s
epoch 99 | loss: 0.32937 | val_0_rmse: 0.61562 | val_1_rmse: 0.60446 |  0:00:29s
epoch 100| loss: 0.32704 | val_0_rmse: 0.57114 | val_1_rmse: 0.56307 |  0:00:29s
epoch 101| loss: 0.32616 | val_0_rmse: 0.61317 | val_1_rmse: 0.60299 |  0:00:29s
epoch 102| loss: 0.3137  | val_0_rmse: 0.56281 | val_1_rmse: 0.55819 |  0:00:29s
epoch 103| loss: 0.32625 | val_0_rmse: 0.57199 | val_1_rmse: 0.55745 |  0:00:30s
epoch 104| loss: 0.3343  | val_0_rmse: 0.57889 | val_1_rmse: 0.56897 |  0:00:30s
epoch 105| loss: 0.32354 | val_0_rmse: 0.60586 | val_1_rmse: 0.59619 |  0:00:30s
epoch 106| loss: 0.33096 | val_0_rmse: 0.64397 | val_1_rmse: 0.62914 |  0:00:31s
epoch 107| loss: 0.32802 | val_0_rmse: 0.61546 | val_1_rmse: 0.60943 |  0:00:31s
epoch 108| loss: 0.31462 | val_0_rmse: 0.58751 | val_1_rmse: 0.58241 |  0:00:31s
epoch 109| loss: 0.31391 | val_0_rmse: 0.5839  | val_1_rmse: 0.58072 |  0:00:31s
epoch 110| loss: 0.30845 | val_0_rmse: 0.57731 | val_1_rmse: 0.56997 |  0:00:32s
epoch 111| loss: 0.31634 | val_0_rmse: 0.56462 | val_1_rmse: 0.55587 |  0:00:32s
epoch 112| loss: 0.31487 | val_0_rmse: 0.58703 | val_1_rmse: 0.57792 |  0:00:32s
epoch 113| loss: 0.31121 | val_0_rmse: 0.54683 | val_1_rmse: 0.55546 |  0:00:33s
epoch 114| loss: 0.3179  | val_0_rmse: 0.60671 | val_1_rmse: 0.59764 |  0:00:33s
epoch 115| loss: 0.3092  | val_0_rmse: 0.62248 | val_1_rmse: 0.62396 |  0:00:33s
epoch 116| loss: 0.31206 | val_0_rmse: 0.61364 | val_1_rmse: 0.60443 |  0:00:33s
epoch 117| loss: 0.3065  | val_0_rmse: 0.59485 | val_1_rmse: 0.58608 |  0:00:34s
epoch 118| loss: 0.30957 | val_0_rmse: 0.56844 | val_1_rmse: 0.56302 |  0:00:34s
epoch 119| loss: 0.31087 | val_0_rmse: 0.57282 | val_1_rmse: 0.56457 |  0:00:34s
epoch 120| loss: 0.30273 | val_0_rmse: 0.56704 | val_1_rmse: 0.56174 |  0:00:35s
epoch 121| loss: 0.30927 | val_0_rmse: 0.5553  | val_1_rmse: 0.54978 |  0:00:35s
epoch 122| loss: 0.30681 | val_0_rmse: 0.58583 | val_1_rmse: 0.57927 |  0:00:35s
epoch 123| loss: 0.31085 | val_0_rmse: 0.59881 | val_1_rmse: 0.59432 |  0:00:35s
epoch 124| loss: 0.30847 | val_0_rmse: 0.55961 | val_1_rmse: 0.55732 |  0:00:36s
epoch 125| loss: 0.30557 | val_0_rmse: 0.59181 | val_1_rmse: 0.58459 |  0:00:36s
epoch 126| loss: 0.30123 | val_0_rmse: 0.57991 | val_1_rmse: 0.57063 |  0:00:36s
epoch 127| loss: 0.30009 | val_0_rmse: 0.56874 | val_1_rmse: 0.56178 |  0:00:37s
epoch 128| loss: 0.30557 | val_0_rmse: 0.61997 | val_1_rmse: 0.61346 |  0:00:37s
epoch 129| loss: 0.30746 | val_0_rmse: 0.61302 | val_1_rmse: 0.60442 |  0:00:37s
epoch 130| loss: 0.31002 | val_0_rmse: 0.5823  | val_1_rmse: 0.57524 |  0:00:37s
epoch 131| loss: 0.31041 | val_0_rmse: 0.56639 | val_1_rmse: 0.57552 |  0:00:38s
epoch 132| loss: 0.3181  | val_0_rmse: 0.5519  | val_1_rmse: 0.5563  |  0:00:38s
epoch 133| loss: 0.30913 | val_0_rmse: 0.56337 | val_1_rmse: 0.55806 |  0:00:38s
epoch 134| loss: 0.3014  | val_0_rmse: 0.54731 | val_1_rmse: 0.54723 |  0:00:39s
epoch 135| loss: 0.30236 | val_0_rmse: 0.56296 | val_1_rmse: 0.55647 |  0:00:39s
epoch 136| loss: 0.30945 | val_0_rmse: 0.59749 | val_1_rmse: 0.59506 |  0:00:39s
epoch 137| loss: 0.30512 | val_0_rmse: 0.60422 | val_1_rmse: 0.6002  |  0:00:39s
epoch 138| loss: 0.29762 | val_0_rmse: 0.62131 | val_1_rmse: 0.61249 |  0:00:40s
epoch 139| loss: 0.29631 | val_0_rmse: 0.60059 | val_1_rmse: 0.59501 |  0:00:40s
epoch 140| loss: 0.30389 | val_0_rmse: 0.55828 | val_1_rmse: 0.55061 |  0:00:40s
epoch 141| loss: 0.30016 | val_0_rmse: 0.5969  | val_1_rmse: 0.5853  |  0:00:41s
epoch 142| loss: 0.31711 | val_0_rmse: 0.57715 | val_1_rmse: 0.56581 |  0:00:41s
epoch 143| loss: 0.30643 | val_0_rmse: 0.61095 | val_1_rmse: 0.59329 |  0:00:41s
epoch 144| loss: 0.30006 | val_0_rmse: 0.61407 | val_1_rmse: 0.60422 |  0:00:41s
epoch 145| loss: 0.3019  | val_0_rmse: 0.56753 | val_1_rmse: 0.56283 |  0:00:42s
epoch 146| loss: 0.29431 | val_0_rmse: 0.56678 | val_1_rmse: 0.55787 |  0:00:42s
epoch 147| loss: 0.30783 | val_0_rmse: 0.53981 | val_1_rmse: 0.53791 |  0:00:42s
epoch 148| loss: 0.30788 | val_0_rmse: 0.53976 | val_1_rmse: 0.53786 |  0:00:43s
epoch 149| loss: 0.30144 | val_0_rmse: 0.56524 | val_1_rmse: 0.56368 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.53786
Best weights from best epoch are automatically used!
ended training at: 08:45:05
Feature importance:
[('Area', 0.2074361258181651), ('Baths', 0.2492791063280569), ('Beds', 0.01763979231236533), ('Latitude', 0.2665702102103509), ('Longitude', 0.2553382386511635), ('Month', 0.0), ('Year', 0.003736526679898347)]
Mean squared error is of 2930730758.9502606
Mean absolute error:35429.437859210026
MAPE:0.25493474173610525
R2 score:0.6337263870667562
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:45:05
epoch 0  | loss: 1.18254 | val_0_rmse: 0.91317 | val_1_rmse: 0.93852 |  0:00:00s
epoch 1  | loss: 0.59329 | val_0_rmse: 1.05764 | val_1_rmse: 1.0775  |  0:00:00s
epoch 2  | loss: 0.53999 | val_0_rmse: 0.74555 | val_1_rmse: 0.78869 |  0:00:00s
epoch 3  | loss: 0.50281 | val_0_rmse: 0.79769 | val_1_rmse: 0.87276 |  0:00:01s
epoch 4  | loss: 0.4978  | val_0_rmse: 0.75578 | val_1_rmse: 0.84095 |  0:00:01s
epoch 5  | loss: 0.50425 | val_0_rmse: 0.70872 | val_1_rmse: 0.77357 |  0:00:01s
epoch 6  | loss: 0.49574 | val_0_rmse: 0.71791 | val_1_rmse: 0.78114 |  0:00:02s
epoch 7  | loss: 0.4744  | val_0_rmse: 0.68218 | val_1_rmse: 0.72987 |  0:00:02s
epoch 8  | loss: 0.46951 | val_0_rmse: 0.69064 | val_1_rmse: 0.73374 |  0:00:02s
epoch 9  | loss: 0.45571 | val_0_rmse: 0.68647 | val_1_rmse: 0.73796 |  0:00:02s
epoch 10 | loss: 0.4638  | val_0_rmse: 0.69326 | val_1_rmse: 0.74104 |  0:00:03s
epoch 11 | loss: 0.45381 | val_0_rmse: 0.66283 | val_1_rmse: 0.70195 |  0:00:03s
epoch 12 | loss: 0.45361 | val_0_rmse: 0.67472 | val_1_rmse: 0.73122 |  0:00:03s
epoch 13 | loss: 0.46512 | val_0_rmse: 0.66377 | val_1_rmse: 0.72276 |  0:00:04s
epoch 14 | loss: 0.45212 | val_0_rmse: 0.66339 | val_1_rmse: 0.71992 |  0:00:04s
epoch 15 | loss: 0.45215 | val_0_rmse: 0.68663 | val_1_rmse: 0.74263 |  0:00:04s
epoch 16 | loss: 0.44626 | val_0_rmse: 0.68203 | val_1_rmse: 0.73198 |  0:00:04s
epoch 17 | loss: 0.43394 | val_0_rmse: 0.65424 | val_1_rmse: 0.71088 |  0:00:05s
epoch 18 | loss: 0.42211 | val_0_rmse: 0.64255 | val_1_rmse: 0.70601 |  0:00:05s
epoch 19 | loss: 0.42219 | val_0_rmse: 0.6544  | val_1_rmse: 0.67678 |  0:00:05s
epoch 20 | loss: 0.41814 | val_0_rmse: 0.64141 | val_1_rmse: 0.69015 |  0:00:06s
epoch 21 | loss: 0.4026  | val_0_rmse: 0.63558 | val_1_rmse: 0.69067 |  0:00:06s
epoch 22 | loss: 0.41303 | val_0_rmse: 0.66951 | val_1_rmse: 0.72516 |  0:00:06s
epoch 23 | loss: 0.40283 | val_0_rmse: 0.6352  | val_1_rmse: 0.67559 |  0:00:06s
epoch 24 | loss: 0.40322 | val_0_rmse: 0.62214 | val_1_rmse: 0.66029 |  0:00:07s
epoch 25 | loss: 0.405   | val_0_rmse: 0.61517 | val_1_rmse: 0.66703 |  0:00:07s
epoch 26 | loss: 0.38521 | val_0_rmse: 0.62317 | val_1_rmse: 0.65436 |  0:00:07s
epoch 27 | loss: 0.39333 | val_0_rmse: 0.62421 | val_1_rmse: 0.6828  |  0:00:08s
epoch 28 | loss: 0.39493 | val_0_rmse: 0.60104 | val_1_rmse: 0.64772 |  0:00:08s
epoch 29 | loss: 0.38004 | val_0_rmse: 0.61214 | val_1_rmse: 0.64624 |  0:00:08s
epoch 30 | loss: 0.38089 | val_0_rmse: 0.61962 | val_1_rmse: 0.65159 |  0:00:09s
epoch 31 | loss: 0.38287 | val_0_rmse: 0.6292  | val_1_rmse: 0.68103 |  0:00:09s
epoch 32 | loss: 0.38795 | val_0_rmse: 0.62734 | val_1_rmse: 0.67909 |  0:00:09s
epoch 33 | loss: 0.38983 | val_0_rmse: 0.63112 | val_1_rmse: 0.68561 |  0:00:09s
epoch 34 | loss: 0.38143 | val_0_rmse: 0.61939 | val_1_rmse: 0.67848 |  0:00:10s
epoch 35 | loss: 0.38053 | val_0_rmse: 0.61711 | val_1_rmse: 0.67023 |  0:00:10s
epoch 36 | loss: 0.3841  | val_0_rmse: 0.62598 | val_1_rmse: 0.68166 |  0:00:10s
epoch 37 | loss: 0.37073 | val_0_rmse: 0.61322 | val_1_rmse: 0.65265 |  0:00:11s
epoch 38 | loss: 0.37627 | val_0_rmse: 0.62046 | val_1_rmse: 0.67908 |  0:00:11s
epoch 39 | loss: 0.36289 | val_0_rmse: 0.61051 | val_1_rmse: 0.6708  |  0:00:11s
epoch 40 | loss: 0.36462 | val_0_rmse: 0.61766 | val_1_rmse: 0.67125 |  0:00:11s
epoch 41 | loss: 0.35942 | val_0_rmse: 0.61316 | val_1_rmse: 0.66492 |  0:00:12s
epoch 42 | loss: 0.36109 | val_0_rmse: 0.60567 | val_1_rmse: 0.64901 |  0:00:12s
epoch 43 | loss: 0.37026 | val_0_rmse: 0.60896 | val_1_rmse: 0.66286 |  0:00:12s
epoch 44 | loss: 0.35793 | val_0_rmse: 0.6009  | val_1_rmse: 0.64698 |  0:00:13s
epoch 45 | loss: 0.35397 | val_0_rmse: 0.63589 | val_1_rmse: 0.70504 |  0:00:13s
epoch 46 | loss: 0.35971 | val_0_rmse: 0.60403 | val_1_rmse: 0.66019 |  0:00:13s
epoch 47 | loss: 0.36014 | val_0_rmse: 0.61804 | val_1_rmse: 0.67357 |  0:00:13s
epoch 48 | loss: 0.35014 | val_0_rmse: 0.62758 | val_1_rmse: 0.70157 |  0:00:14s
epoch 49 | loss: 0.3647  | val_0_rmse: 0.66982 | val_1_rmse: 0.74321 |  0:00:14s
epoch 50 | loss: 0.35817 | val_0_rmse: 0.62987 | val_1_rmse: 0.69426 |  0:00:14s
epoch 51 | loss: 0.36204 | val_0_rmse: 0.60194 | val_1_rmse: 0.65774 |  0:00:15s
epoch 52 | loss: 0.36997 | val_0_rmse: 0.61684 | val_1_rmse: 0.67433 |  0:00:15s
epoch 53 | loss: 0.3707  | val_0_rmse: 0.63324 | val_1_rmse: 0.6865  |  0:00:15s
epoch 54 | loss: 0.34561 | val_0_rmse: 0.63147 | val_1_rmse: 0.69616 |  0:00:15s
epoch 55 | loss: 0.34626 | val_0_rmse: 0.63991 | val_1_rmse: 0.70411 |  0:00:16s
epoch 56 | loss: 0.33974 | val_0_rmse: 0.64579 | val_1_rmse: 0.70837 |  0:00:16s
epoch 57 | loss: 0.35553 | val_0_rmse: 0.64185 | val_1_rmse: 0.69828 |  0:00:16s
epoch 58 | loss: 0.34178 | val_0_rmse: 0.63727 | val_1_rmse: 0.69887 |  0:00:17s
epoch 59 | loss: 0.33409 | val_0_rmse: 0.62044 | val_1_rmse: 0.68208 |  0:00:17s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.64624
Best weights from best epoch are automatically used!
ended training at: 08:45:22
Feature importance:
[('Area', 0.4048318639254866), ('Baths', 0.07708128902400666), ('Beds', 0.043986146104608576), ('Latitude', 0.1499354982505653), ('Longitude', 0.19112627180149255), ('Month', 0.082634198242222), ('Year', 0.05040473265161834)]
Mean squared error is of 3529973054.6873183
Mean absolute error:41527.94486739641
MAPE:0.38264196374180526
R2 score:0.5840814214890246
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:45:22
epoch 0  | loss: 1.11925 | val_0_rmse: 1.24022 | val_1_rmse: 1.2232  |  0:00:00s
epoch 1  | loss: 0.67923 | val_0_rmse: 0.86865 | val_1_rmse: 0.85695 |  0:00:00s
epoch 2  | loss: 0.60842 | val_0_rmse: 0.75797 | val_1_rmse: 0.73046 |  0:00:00s
epoch 3  | loss: 0.5474  | val_0_rmse: 0.74246 | val_1_rmse: 0.72076 |  0:00:01s
epoch 4  | loss: 0.51113 | val_0_rmse: 0.71734 | val_1_rmse: 0.72092 |  0:00:01s
epoch 5  | loss: 0.4974  | val_0_rmse: 0.6933  | val_1_rmse: 0.69208 |  0:00:01s
epoch 6  | loss: 0.49715 | val_0_rmse: 0.68828 | val_1_rmse: 0.67207 |  0:00:02s
epoch 7  | loss: 0.47926 | val_0_rmse: 0.67587 | val_1_rmse: 0.65475 |  0:00:02s
epoch 8  | loss: 0.47417 | val_0_rmse: 0.69769 | val_1_rmse: 0.69607 |  0:00:02s
epoch 9  | loss: 0.45576 | val_0_rmse: 0.67231 | val_1_rmse: 0.66752 |  0:00:02s
epoch 10 | loss: 0.4576  | val_0_rmse: 0.66464 | val_1_rmse: 0.66817 |  0:00:03s
epoch 11 | loss: 0.45297 | val_0_rmse: 0.69032 | val_1_rmse: 0.6854  |  0:00:03s
epoch 12 | loss: 0.46724 | val_0_rmse: 0.68269 | val_1_rmse: 0.66051 |  0:00:03s
epoch 13 | loss: 0.46564 | val_0_rmse: 0.68236 | val_1_rmse: 0.66445 |  0:00:04s
epoch 14 | loss: 0.45057 | val_0_rmse: 0.66863 | val_1_rmse: 0.63752 |  0:00:04s
epoch 15 | loss: 0.45208 | val_0_rmse: 0.67575 | val_1_rmse: 0.64553 |  0:00:04s
epoch 16 | loss: 0.4467  | val_0_rmse: 0.69404 | val_1_rmse: 0.64337 |  0:00:04s
epoch 17 | loss: 0.44739 | val_0_rmse: 0.66425 | val_1_rmse: 0.61907 |  0:00:05s
epoch 18 | loss: 0.44367 | val_0_rmse: 0.66149 | val_1_rmse: 0.62918 |  0:00:05s
epoch 19 | loss: 0.42841 | val_0_rmse: 0.64527 | val_1_rmse: 0.60941 |  0:00:05s
epoch 20 | loss: 0.41262 | val_0_rmse: 0.64247 | val_1_rmse: 0.60281 |  0:00:06s
epoch 21 | loss: 0.41118 | val_0_rmse: 0.63802 | val_1_rmse: 0.6033  |  0:00:06s
epoch 22 | loss: 0.41106 | val_0_rmse: 0.64975 | val_1_rmse: 0.62516 |  0:00:06s
epoch 23 | loss: 0.40717 | val_0_rmse: 0.62626 | val_1_rmse: 0.60098 |  0:00:07s
epoch 24 | loss: 0.39985 | val_0_rmse: 0.62376 | val_1_rmse: 0.59196 |  0:00:07s
epoch 25 | loss: 0.40134 | val_0_rmse: 0.63089 | val_1_rmse: 0.60921 |  0:00:07s
epoch 26 | loss: 0.39675 | val_0_rmse: 0.64429 | val_1_rmse: 0.61905 |  0:00:07s
epoch 27 | loss: 0.40518 | val_0_rmse: 0.62877 | val_1_rmse: 0.60232 |  0:00:08s
epoch 28 | loss: 0.38653 | val_0_rmse: 0.64662 | val_1_rmse: 0.62425 |  0:00:08s
epoch 29 | loss: 0.38757 | val_0_rmse: 0.61311 | val_1_rmse: 0.59469 |  0:00:08s
epoch 30 | loss: 0.38074 | val_0_rmse: 0.63663 | val_1_rmse: 0.62147 |  0:00:09s
epoch 31 | loss: 0.39839 | val_0_rmse: 0.64317 | val_1_rmse: 0.65401 |  0:00:09s
epoch 32 | loss: 0.39558 | val_0_rmse: 0.63341 | val_1_rmse: 0.64702 |  0:00:09s
epoch 33 | loss: 0.40433 | val_0_rmse: 0.62339 | val_1_rmse: 0.61867 |  0:00:09s
epoch 34 | loss: 0.40899 | val_0_rmse: 0.62677 | val_1_rmse: 0.61859 |  0:00:10s
epoch 35 | loss: 0.38927 | val_0_rmse: 0.62236 | val_1_rmse: 0.59071 |  0:00:10s
epoch 36 | loss: 0.38822 | val_0_rmse: 0.61572 | val_1_rmse: 0.59166 |  0:00:10s
epoch 37 | loss: 0.38256 | val_0_rmse: 0.614   | val_1_rmse: 0.60114 |  0:00:11s
epoch 38 | loss: 0.3819  | val_0_rmse: 0.60443 | val_1_rmse: 0.58528 |  0:00:11s
epoch 39 | loss: 0.37679 | val_0_rmse: 0.61157 | val_1_rmse: 0.59508 |  0:00:11s
epoch 40 | loss: 0.37471 | val_0_rmse: 0.6122  | val_1_rmse: 0.58674 |  0:00:11s
epoch 41 | loss: 0.37394 | val_0_rmse: 0.63942 | val_1_rmse: 0.62512 |  0:00:12s
epoch 42 | loss: 0.37142 | val_0_rmse: 0.6168  | val_1_rmse: 0.60793 |  0:00:12s
epoch 43 | loss: 0.38231 | val_0_rmse: 0.61712 | val_1_rmse: 0.60096 |  0:00:12s
epoch 44 | loss: 0.38084 | val_0_rmse: 0.61286 | val_1_rmse: 0.59401 |  0:00:13s
epoch 45 | loss: 0.3853  | val_0_rmse: 0.63315 | val_1_rmse: 0.61905 |  0:00:13s
epoch 46 | loss: 0.37435 | val_0_rmse: 0.60974 | val_1_rmse: 0.59795 |  0:00:13s
epoch 47 | loss: 0.36568 | val_0_rmse: 0.64305 | val_1_rmse: 0.63634 |  0:00:13s
epoch 48 | loss: 0.37657 | val_0_rmse: 0.60531 | val_1_rmse: 0.59429 |  0:00:14s
epoch 49 | loss: 0.36764 | val_0_rmse: 0.62764 | val_1_rmse: 0.61397 |  0:00:14s
epoch 50 | loss: 0.35816 | val_0_rmse: 0.62685 | val_1_rmse: 0.615   |  0:00:14s
epoch 51 | loss: 0.36147 | val_0_rmse: 0.606   | val_1_rmse: 0.59358 |  0:00:15s
epoch 52 | loss: 0.34978 | val_0_rmse: 0.6109  | val_1_rmse: 0.60172 |  0:00:15s
epoch 53 | loss: 0.34668 | val_0_rmse: 0.59366 | val_1_rmse: 0.58357 |  0:00:15s
epoch 54 | loss: 0.35069 | val_0_rmse: 0.59127 | val_1_rmse: 0.58267 |  0:00:16s
epoch 55 | loss: 0.35963 | val_0_rmse: 0.62935 | val_1_rmse: 0.62089 |  0:00:16s
epoch 56 | loss: 0.35553 | val_0_rmse: 0.60543 | val_1_rmse: 0.59049 |  0:00:16s
epoch 57 | loss: 0.36001 | val_0_rmse: 0.61331 | val_1_rmse: 0.59992 |  0:00:16s
epoch 58 | loss: 0.35153 | val_0_rmse: 0.59711 | val_1_rmse: 0.5841  |  0:00:17s
epoch 59 | loss: 0.35412 | val_0_rmse: 0.62389 | val_1_rmse: 0.61882 |  0:00:17s
epoch 60 | loss: 0.34888 | val_0_rmse: 0.60618 | val_1_rmse: 0.59907 |  0:00:17s
epoch 61 | loss: 0.35309 | val_0_rmse: 0.60981 | val_1_rmse: 0.59902 |  0:00:18s
epoch 62 | loss: 0.34679 | val_0_rmse: 0.61721 | val_1_rmse: 0.61139 |  0:00:18s
epoch 63 | loss: 0.34836 | val_0_rmse: 0.61617 | val_1_rmse: 0.6094  |  0:00:18s
epoch 64 | loss: 0.34695 | val_0_rmse: 0.59478 | val_1_rmse: 0.58163 |  0:00:18s
epoch 65 | loss: 0.35297 | val_0_rmse: 0.64445 | val_1_rmse: 0.64263 |  0:00:19s
epoch 66 | loss: 0.35128 | val_0_rmse: 0.59719 | val_1_rmse: 0.58312 |  0:00:19s
epoch 67 | loss: 0.35429 | val_0_rmse: 0.60383 | val_1_rmse: 0.58909 |  0:00:19s
epoch 68 | loss: 0.34712 | val_0_rmse: 0.61482 | val_1_rmse: 0.60438 |  0:00:20s
epoch 69 | loss: 0.34631 | val_0_rmse: 0.60395 | val_1_rmse: 0.59739 |  0:00:20s
epoch 70 | loss: 0.34578 | val_0_rmse: 0.6157  | val_1_rmse: 0.60357 |  0:00:20s
epoch 71 | loss: 0.34144 | val_0_rmse: 0.64021 | val_1_rmse: 0.60625 |  0:00:20s
epoch 72 | loss: 0.34253 | val_0_rmse: 0.60132 | val_1_rmse: 0.59076 |  0:00:21s
epoch 73 | loss: 0.34226 | val_0_rmse: 0.61451 | val_1_rmse: 0.60203 |  0:00:21s
epoch 74 | loss: 0.34414 | val_0_rmse: 0.64113 | val_1_rmse: 0.63831 |  0:00:21s
epoch 75 | loss: 0.35094 | val_0_rmse: 0.62671 | val_1_rmse: 0.62163 |  0:00:22s
epoch 76 | loss: 0.33986 | val_0_rmse: 0.62654 | val_1_rmse: 0.62111 |  0:00:22s
epoch 77 | loss: 0.343   | val_0_rmse: 0.61516 | val_1_rmse: 0.60879 |  0:00:22s
epoch 78 | loss: 0.34525 | val_0_rmse: 0.60666 | val_1_rmse: 0.59576 |  0:00:22s
epoch 79 | loss: 0.34992 | val_0_rmse: 0.6221  | val_1_rmse: 0.61104 |  0:00:23s
epoch 80 | loss: 0.3544  | val_0_rmse: 0.63698 | val_1_rmse: 0.63057 |  0:00:23s
epoch 81 | loss: 0.36151 | val_0_rmse: 0.62655 | val_1_rmse: 0.61635 |  0:00:23s
epoch 82 | loss: 0.35586 | val_0_rmse: 0.61653 | val_1_rmse: 0.60455 |  0:00:24s
epoch 83 | loss: 0.35265 | val_0_rmse: 0.62485 | val_1_rmse: 0.61043 |  0:00:24s
epoch 84 | loss: 0.3453  | val_0_rmse: 0.58817 | val_1_rmse: 0.56511 |  0:00:24s
epoch 85 | loss: 0.34384 | val_0_rmse: 0.61239 | val_1_rmse: 0.60018 |  0:00:24s
epoch 86 | loss: 0.34179 | val_0_rmse: 0.58992 | val_1_rmse: 0.57057 |  0:00:25s
epoch 87 | loss: 0.34238 | val_0_rmse: 0.60265 | val_1_rmse: 0.58958 |  0:00:25s
epoch 88 | loss: 0.33976 | val_0_rmse: 0.59182 | val_1_rmse: 0.55641 |  0:00:25s
epoch 89 | loss: 0.34281 | val_0_rmse: 0.59144 | val_1_rmse: 0.56401 |  0:00:26s
epoch 90 | loss: 0.33439 | val_0_rmse: 0.60806 | val_1_rmse: 0.58339 |  0:00:26s
epoch 91 | loss: 0.33745 | val_0_rmse: 0.60557 | val_1_rmse: 0.58886 |  0:00:26s
epoch 92 | loss: 0.33915 | val_0_rmse: 0.61396 | val_1_rmse: 0.60036 |  0:00:27s
epoch 93 | loss: 0.33995 | val_0_rmse: 0.61432 | val_1_rmse: 0.60012 |  0:00:27s
epoch 94 | loss: 0.33142 | val_0_rmse: 0.61621 | val_1_rmse: 0.60772 |  0:00:27s
epoch 95 | loss: 0.33705 | val_0_rmse: 0.60455 | val_1_rmse: 0.59196 |  0:00:27s
epoch 96 | loss: 0.33764 | val_0_rmse: 0.63262 | val_1_rmse: 0.6256  |  0:00:28s
epoch 97 | loss: 0.3405  | val_0_rmse: 0.60933 | val_1_rmse: 0.59805 |  0:00:28s
epoch 98 | loss: 0.33892 | val_0_rmse: 0.60973 | val_1_rmse: 0.59335 |  0:00:28s
epoch 99 | loss: 0.34452 | val_0_rmse: 0.64582 | val_1_rmse: 0.63743 |  0:00:29s
epoch 100| loss: 0.34102 | val_0_rmse: 0.5846  | val_1_rmse: 0.56802 |  0:00:29s
epoch 101| loss: 0.33559 | val_0_rmse: 0.61173 | val_1_rmse: 0.60814 |  0:00:29s
epoch 102| loss: 0.33303 | val_0_rmse: 0.57478 | val_1_rmse: 0.55806 |  0:00:29s
epoch 103| loss: 0.3357  | val_0_rmse: 0.58407 | val_1_rmse: 0.56615 |  0:00:30s
epoch 104| loss: 0.34147 | val_0_rmse: 0.58209 | val_1_rmse: 0.56063 |  0:00:30s
epoch 105| loss: 0.3301  | val_0_rmse: 0.59085 | val_1_rmse: 0.56973 |  0:00:30s
epoch 106| loss: 0.33437 | val_0_rmse: 0.60672 | val_1_rmse: 0.60217 |  0:00:31s
epoch 107| loss: 0.33363 | val_0_rmse: 0.61717 | val_1_rmse: 0.61257 |  0:00:31s
epoch 108| loss: 0.33712 | val_0_rmse: 0.59564 | val_1_rmse: 0.5835  |  0:00:31s
epoch 109| loss: 0.33784 | val_0_rmse: 0.58223 | val_1_rmse: 0.55903 |  0:00:31s
epoch 110| loss: 0.34491 | val_0_rmse: 0.60365 | val_1_rmse: 0.59068 |  0:00:32s
epoch 111| loss: 0.34617 | val_0_rmse: 0.6034  | val_1_rmse: 0.59367 |  0:00:32s
epoch 112| loss: 0.33657 | val_0_rmse: 0.58598 | val_1_rmse: 0.56887 |  0:00:32s
epoch 113| loss: 0.33175 | val_0_rmse: 0.5942  | val_1_rmse: 0.577   |  0:00:33s
epoch 114| loss: 0.33216 | val_0_rmse: 0.58593 | val_1_rmse: 0.57399 |  0:00:33s
epoch 115| loss: 0.33221 | val_0_rmse: 0.57617 | val_1_rmse: 0.56291 |  0:00:33s
epoch 116| loss: 0.3466  | val_0_rmse: 0.59387 | val_1_rmse: 0.58548 |  0:00:33s
epoch 117| loss: 0.33271 | val_0_rmse: 0.59625 | val_1_rmse: 0.58642 |  0:00:34s
epoch 118| loss: 0.32911 | val_0_rmse: 0.60486 | val_1_rmse: 0.59054 |  0:00:34s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.55641
Best weights from best epoch are automatically used!
ended training at: 08:45:57
Feature importance:
[('Area', 0.206504566659076), ('Baths', 0.2473634967053162), ('Beds', 0.00228720117332197), ('Latitude', 0.22859744790280456), ('Longitude', 0.30899120844684436), ('Month', 0.006239508271145184), ('Year', 1.657084149174307e-05)]
Mean squared error is of 2908670203.2435703
Mean absolute error:36697.29071482385
MAPE:0.32374538015413135
R2 score:0.6568342783887118
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:45:57
epoch 0  | loss: 0.44656 | val_0_rmse: 0.65944 | val_1_rmse: 0.6558  |  0:00:13s
epoch 1  | loss: 0.36767 | val_0_rmse: 0.59719 | val_1_rmse: 0.59285 |  0:00:27s
epoch 2  | loss: 0.35669 | val_0_rmse: 0.61803 | val_1_rmse: 0.61417 |  0:00:41s
epoch 3  | loss: 0.35076 | val_0_rmse: 0.58591 | val_1_rmse: 0.58384 |  0:00:55s
epoch 4  | loss: 0.34716 | val_0_rmse: 0.58552 | val_1_rmse: 0.58176 |  0:01:08s
epoch 5  | loss: 0.34559 | val_0_rmse: 0.58114 | val_1_rmse: 0.5766  |  0:01:22s
epoch 6  | loss: 0.35524 | val_0_rmse: 0.59071 | val_1_rmse: 0.5855  |  0:01:36s
epoch 7  | loss: 0.34661 | val_0_rmse: 0.58167 | val_1_rmse: 0.57781 |  0:01:50s
epoch 8  | loss: 0.34362 | val_0_rmse: 0.57226 | val_1_rmse: 0.5678  |  0:02:03s
epoch 9  | loss: 0.33832 | val_0_rmse: 0.6219  | val_1_rmse: 0.61956 |  0:02:17s
epoch 10 | loss: 0.33676 | val_0_rmse: 0.57451 | val_1_rmse: 0.5695  |  0:02:31s
epoch 11 | loss: 0.33315 | val_0_rmse: 0.56997 | val_1_rmse: 0.565   |  0:02:45s
epoch 12 | loss: 0.34602 | val_0_rmse: 0.59089 | val_1_rmse: 0.58543 |  0:02:59s
epoch 13 | loss: 0.34769 | val_0_rmse: 0.57836 | val_1_rmse: 0.57517 |  0:03:13s
epoch 14 | loss: 0.34026 | val_0_rmse: 0.5767  | val_1_rmse: 0.57079 |  0:03:27s
epoch 15 | loss: 0.33808 | val_0_rmse: 0.57526 | val_1_rmse: 0.56895 |  0:03:41s
epoch 16 | loss: 0.34742 | val_0_rmse: 0.67492 | val_1_rmse: 0.66796 |  0:03:54s
epoch 17 | loss: 0.34336 | val_0_rmse: 0.62877 | val_1_rmse: 0.62643 |  0:04:08s
epoch 18 | loss: 0.33691 | val_0_rmse: 0.5743  | val_1_rmse: 0.56964 |  0:04:22s
epoch 19 | loss: 0.33568 | val_0_rmse: 0.61024 | val_1_rmse: 0.60639 |  0:04:36s
epoch 20 | loss: 0.34008 | val_0_rmse: 0.59799 | val_1_rmse: 0.59432 |  0:04:50s
epoch 21 | loss: 0.33368 | val_0_rmse: 0.57408 | val_1_rmse: 0.56965 |  0:05:04s
epoch 22 | loss: 0.33299 | val_0_rmse: 0.57325 | val_1_rmse: 0.56892 |  0:05:17s
epoch 23 | loss: 0.33901 | val_0_rmse: 0.58513 | val_1_rmse: 0.58041 |  0:05:31s
epoch 24 | loss: 0.33741 | val_0_rmse: 0.59889 | val_1_rmse: 0.59609 |  0:05:45s
epoch 25 | loss: 0.33306 | val_0_rmse: 0.58567 | val_1_rmse: 0.58308 |  0:05:59s
epoch 26 | loss: 0.33182 | val_0_rmse: 0.58068 | val_1_rmse: 0.5773  |  0:06:12s
epoch 27 | loss: 0.3355  | val_0_rmse: 0.7037  | val_1_rmse: 0.70311 |  0:06:26s
epoch 28 | loss: 0.3335  | val_0_rmse: 0.57221 | val_1_rmse: 0.56953 |  0:06:40s
epoch 29 | loss: 0.33042 | val_0_rmse: 0.57968 | val_1_rmse: 0.57774 |  0:06:54s
epoch 30 | loss: 0.33105 | val_0_rmse: 0.56809 | val_1_rmse: 0.56366 |  0:07:08s
epoch 31 | loss: 0.3304  | val_0_rmse: 0.5774  | val_1_rmse: 0.57474 |  0:07:21s
epoch 32 | loss: 0.32925 | val_0_rmse: 0.57219 | val_1_rmse: 0.57045 |  0:07:35s
epoch 33 | loss: 0.32963 | val_0_rmse: 0.56535 | val_1_rmse: 0.56188 |  0:07:49s
epoch 34 | loss: 0.32863 | val_0_rmse: 0.58022 | val_1_rmse: 0.57844 |  0:08:03s
epoch 35 | loss: 0.3274  | val_0_rmse: 0.56461 | val_1_rmse: 0.56251 |  0:08:17s
epoch 36 | loss: 0.32741 | val_0_rmse: 0.57263 | val_1_rmse: 0.56941 |  0:08:31s
epoch 37 | loss: 0.32826 | val_0_rmse: 0.5741  | val_1_rmse: 0.56989 |  0:08:44s
epoch 38 | loss: 0.32673 | val_0_rmse: 0.57443 | val_1_rmse: 0.57051 |  0:08:58s
epoch 39 | loss: 0.32675 | val_0_rmse: 0.59847 | val_1_rmse: 0.59399 |  0:09:12s
epoch 40 | loss: 0.32744 | val_0_rmse: 0.57395 | val_1_rmse: 0.57099 |  0:09:26s
epoch 41 | loss: 0.32659 | val_0_rmse: 0.5735  | val_1_rmse: 0.57018 |  0:09:40s
epoch 42 | loss: 0.32636 | val_0_rmse: 0.57353 | val_1_rmse: 0.57033 |  0:09:54s
epoch 43 | loss: 0.3264  | val_0_rmse: 0.58359 | val_1_rmse: 0.58098 |  0:10:07s
epoch 44 | loss: 0.32752 | val_0_rmse: 0.63622 | val_1_rmse: 0.63412 |  0:10:21s
epoch 45 | loss: 0.32669 | val_0_rmse: 0.56965 | val_1_rmse: 0.56816 |  0:10:35s
epoch 46 | loss: 0.32568 | val_0_rmse: 0.57739 | val_1_rmse: 0.57532 |  0:10:49s
epoch 47 | loss: 0.32349 | val_0_rmse: 0.57049 | val_1_rmse: 0.56821 |  0:11:03s
epoch 48 | loss: 0.3232  | val_0_rmse: 0.56515 | val_1_rmse: 0.56292 |  0:11:16s
epoch 49 | loss: 0.32379 | val_0_rmse: 0.57933 | val_1_rmse: 0.57761 |  0:11:30s
epoch 50 | loss: 0.33436 | val_0_rmse: 0.5942  | val_1_rmse: 0.5941  |  0:11:44s
epoch 51 | loss: 0.33229 | val_0_rmse: 0.57527 | val_1_rmse: 0.57242 |  0:11:58s
epoch 52 | loss: 0.32928 | val_0_rmse: 0.57356 | val_1_rmse: 0.57151 |  0:12:12s
epoch 53 | loss: 0.32851 | val_0_rmse: 0.61241 | val_1_rmse: 0.61317 |  0:12:25s
epoch 54 | loss: 0.32782 | val_0_rmse: 0.58583 | val_1_rmse: 0.58431 |  0:12:39s
epoch 55 | loss: 0.3263  | val_0_rmse: 0.62645 | val_1_rmse: 0.62652 |  0:12:53s
epoch 56 | loss: 0.32716 | val_0_rmse: 0.60001 | val_1_rmse: 0.60085 |  0:13:07s
epoch 57 | loss: 0.32625 | val_0_rmse: 0.56433 | val_1_rmse: 0.56214 |  0:13:21s
epoch 58 | loss: 0.32495 | val_0_rmse: 0.58094 | val_1_rmse: 0.58128 |  0:13:35s
epoch 59 | loss: 0.32503 | val_0_rmse: 0.56438 | val_1_rmse: 0.56213 |  0:13:48s
epoch 60 | loss: 0.32413 | val_0_rmse: 0.5797  | val_1_rmse: 0.57924 |  0:14:02s
epoch 61 | loss: 0.32855 | val_0_rmse: 0.59862 | val_1_rmse: 0.59861 |  0:14:16s
epoch 62 | loss: 0.32589 | val_0_rmse: 0.67611 | val_1_rmse: 0.67519 |  0:14:30s
epoch 63 | loss: 0.32373 | val_0_rmse: 0.57511 | val_1_rmse: 0.57627 |  0:14:44s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.56188
Best weights from best epoch are automatically used!
ended training at: 09:00:45
Feature importance:
[('Area', 0.13691374979629056), ('Baths', 0.08375403311868017), ('Beds', 0.058615940427830075), ('Latitude', 0.4423677841843659), ('Longitude', 0.2567617185786993), ('Month', 0.0), ('Year', 0.021586773894134)]
Mean squared error is of 13888266759.647312
Mean absolute error:78109.22535507365
MAPE:0.43446212883934937
R2 score:0.6797485992519476
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:00:48
epoch 0  | loss: 0.42478 | val_0_rmse: 0.63509 | val_1_rmse: 0.63908 |  0:00:14s
epoch 1  | loss: 0.36559 | val_0_rmse: 0.6051  | val_1_rmse: 0.60822 |  0:00:27s
epoch 2  | loss: 0.35425 | val_0_rmse: 0.59816 | val_1_rmse: 0.59463 |  0:00:41s
epoch 3  | loss: 0.35102 | val_0_rmse: 0.59664 | val_1_rmse: 0.59542 |  0:00:55s
epoch 4  | loss: 0.34888 | val_0_rmse: 0.59379 | val_1_rmse: 0.59325 |  0:01:09s
epoch 5  | loss: 0.34534 | val_0_rmse: 0.58351 | val_1_rmse: 0.58026 |  0:01:23s
epoch 6  | loss: 0.34492 | val_0_rmse: 0.57965 | val_1_rmse: 0.57823 |  0:01:37s
epoch 7  | loss: 0.34174 | val_0_rmse: 0.57868 | val_1_rmse: 0.57731 |  0:01:50s
epoch 8  | loss: 0.34019 | val_0_rmse: 0.5802  | val_1_rmse: 0.57875 |  0:02:04s
epoch 9  | loss: 0.34046 | val_0_rmse: 0.58127 | val_1_rmse: 0.58063 |  0:02:18s
epoch 10 | loss: 0.33859 | val_0_rmse: 0.5796  | val_1_rmse: 0.57876 |  0:02:32s
epoch 11 | loss: 0.34738 | val_0_rmse: 0.58778 | val_1_rmse: 0.5866  |  0:02:46s
epoch 12 | loss: 0.33988 | val_0_rmse: 0.57793 | val_1_rmse: 0.57721 |  0:03:00s
epoch 13 | loss: 0.33955 | val_0_rmse: 0.59076 | val_1_rmse: 0.58922 |  0:03:14s
epoch 14 | loss: 0.34428 | val_0_rmse: 0.5823  | val_1_rmse: 0.58228 |  0:03:27s
epoch 15 | loss: 0.34256 | val_0_rmse: 0.70015 | val_1_rmse: 0.70015 |  0:03:41s
epoch 16 | loss: 0.34277 | val_0_rmse: 0.58927 | val_1_rmse: 0.58943 |  0:03:55s
epoch 17 | loss: 0.34006 | val_0_rmse: 0.5881  | val_1_rmse: 0.58788 |  0:04:09s
epoch 18 | loss: 0.33727 | val_0_rmse: 0.59228 | val_1_rmse: 0.59413 |  0:04:23s
epoch 19 | loss: 0.34318 | val_0_rmse: 0.59075 | val_1_rmse: 0.59197 |  0:04:37s
epoch 20 | loss: 0.33627 | val_0_rmse: 0.58184 | val_1_rmse: 0.58135 |  0:04:51s
epoch 21 | loss: 0.33402 | val_0_rmse: 0.58063 | val_1_rmse: 0.58024 |  0:05:04s
epoch 22 | loss: 0.33481 | val_0_rmse: 0.57424 | val_1_rmse: 0.57346 |  0:05:18s
epoch 23 | loss: 0.33596 | val_0_rmse: 0.57494 | val_1_rmse: 0.57373 |  0:05:32s
epoch 24 | loss: 0.33272 | val_0_rmse: 0.57426 | val_1_rmse: 0.57275 |  0:05:46s
epoch 25 | loss: 0.33228 | val_0_rmse: 0.58059 | val_1_rmse: 0.57977 |  0:06:00s
epoch 26 | loss: 0.33191 | val_0_rmse: 0.57939 | val_1_rmse: 0.57873 |  0:06:14s
epoch 27 | loss: 0.3326  | val_0_rmse: 0.57632 | val_1_rmse: 0.5734  |  0:06:28s
epoch 28 | loss: 0.35158 | val_0_rmse: 0.58209 | val_1_rmse: 0.57822 |  0:06:41s
epoch 29 | loss: 0.36502 | val_0_rmse: 0.58974 | val_1_rmse: 0.58715 |  0:06:55s
epoch 30 | loss: 0.35799 | val_0_rmse: 0.58786 | val_1_rmse: 0.58472 |  0:07:09s
epoch 31 | loss: 0.35311 | val_0_rmse: 0.60059 | val_1_rmse: 0.59709 |  0:07:23s
epoch 32 | loss: 0.35358 | val_0_rmse: 0.59074 | val_1_rmse: 0.58875 |  0:07:37s
epoch 33 | loss: 0.35264 | val_0_rmse: 0.5903  | val_1_rmse: 0.58818 |  0:07:51s
epoch 34 | loss: 0.35037 | val_0_rmse: 0.58911 | val_1_rmse: 0.58632 |  0:08:05s
epoch 35 | loss: 0.3649  | val_0_rmse: 0.60893 | val_1_rmse: 0.60944 |  0:08:19s
epoch 36 | loss: 0.36641 | val_0_rmse: 0.60027 | val_1_rmse: 0.60049 |  0:08:32s
epoch 37 | loss: 0.36244 | val_0_rmse: 0.60303 | val_1_rmse: 0.60305 |  0:08:46s
epoch 38 | loss: 0.36131 | val_0_rmse: 0.61534 | val_1_rmse: 0.61553 |  0:09:00s
epoch 39 | loss: 0.3602  | val_0_rmse: 0.61833 | val_1_rmse: 0.62151 |  0:09:14s
epoch 40 | loss: 0.35971 | val_0_rmse: 0.59742 | val_1_rmse: 0.59833 |  0:09:28s
epoch 41 | loss: 0.35993 | val_0_rmse: 0.59967 | val_1_rmse: 0.60188 |  0:09:42s
epoch 42 | loss: 0.36816 | val_0_rmse: 0.58265 | val_1_rmse: 0.58057 |  0:09:56s
epoch 43 | loss: 0.34126 | val_0_rmse: 0.59153 | val_1_rmse: 0.59139 |  0:10:10s
epoch 44 | loss: 0.33623 | val_0_rmse: 0.6078  | val_1_rmse: 0.6111  |  0:10:23s
epoch 45 | loss: 0.33397 | val_0_rmse: 0.58009 | val_1_rmse: 0.57748 |  0:10:37s
epoch 46 | loss: 0.34876 | val_0_rmse: 0.58559 | val_1_rmse: 0.58475 |  0:10:51s
epoch 47 | loss: 0.35114 | val_0_rmse: 0.58156 | val_1_rmse: 0.5802  |  0:11:05s
epoch 48 | loss: 0.34694 | val_0_rmse: 0.58262 | val_1_rmse: 0.58018 |  0:11:19s
epoch 49 | loss: 0.34934 | val_0_rmse: 0.60849 | val_1_rmse: 0.60984 |  0:11:33s
epoch 50 | loss: 0.34484 | val_0_rmse: 0.5784  | val_1_rmse: 0.57601 |  0:11:46s
epoch 51 | loss: 0.33997 | val_0_rmse: 0.5779  | val_1_rmse: 0.57635 |  0:12:00s
epoch 52 | loss: 0.33907 | val_0_rmse: 0.58521 | val_1_rmse: 0.58447 |  0:12:14s
epoch 53 | loss: 0.33773 | val_0_rmse: 0.57865 | val_1_rmse: 0.57753 |  0:12:28s
epoch 54 | loss: 0.33559 | val_0_rmse: 0.63245 | val_1_rmse: 0.63712 |  0:12:42s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.57275
Best weights from best epoch are automatically used!
ended training at: 09:13:34
Feature importance:
[('Area', 0.1937379438018233), ('Baths', 0.24013235842804162), ('Beds', 0.0), ('Latitude', 0.07021503036088185), ('Longitude', 0.19709583522576468), ('Month', 0.0), ('Year', 0.29881883218348854)]
Mean squared error is of 14064667788.655598
Mean absolute error:77905.36629694834
MAPE:0.42702084692930814
R2 score:0.6701718993282186
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:13:36
epoch 0  | loss: 0.44697 | val_0_rmse: 0.61775 | val_1_rmse: 0.61853 |  0:00:14s
epoch 1  | loss: 0.37312 | val_0_rmse: 0.60586 | val_1_rmse: 0.60481 |  0:00:27s
epoch 2  | loss: 0.36675 | val_0_rmse: 0.60176 | val_1_rmse: 0.60032 |  0:00:41s
epoch 3  | loss: 0.36041 | val_0_rmse: 0.58719 | val_1_rmse: 0.58685 |  0:00:55s
epoch 4  | loss: 0.35565 | val_0_rmse: 0.62042 | val_1_rmse: 0.61976 |  0:01:09s
epoch 5  | loss: 0.35852 | val_0_rmse: 0.58706 | val_1_rmse: 0.58594 |  0:01:22s
epoch 6  | loss: 0.34699 | val_0_rmse: 0.59502 | val_1_rmse: 0.59318 |  0:01:36s
epoch 7  | loss: 0.34857 | val_0_rmse: 0.5849  | val_1_rmse: 0.58471 |  0:01:50s
epoch 8  | loss: 0.34465 | val_0_rmse: 0.58352 | val_1_rmse: 0.5838  |  0:02:04s
epoch 9  | loss: 0.34335 | val_0_rmse: 0.58871 | val_1_rmse: 0.58745 |  0:02:18s
epoch 10 | loss: 0.39379 | val_0_rmse: 0.64982 | val_1_rmse: 0.64772 |  0:02:32s
epoch 11 | loss: 0.385   | val_0_rmse: 0.60834 | val_1_rmse: 0.60641 |  0:02:45s
epoch 12 | loss: 0.37948 | val_0_rmse: 0.60136 | val_1_rmse: 0.59958 |  0:02:59s
epoch 13 | loss: 0.37551 | val_0_rmse: 0.60571 | val_1_rmse: 0.60413 |  0:03:13s
epoch 14 | loss: 0.37238 | val_0_rmse: 0.60025 | val_1_rmse: 0.59822 |  0:03:27s
epoch 15 | loss: 0.37082 | val_0_rmse: 0.61685 | val_1_rmse: 0.61418 |  0:03:41s
epoch 16 | loss: 0.36988 | val_0_rmse: 0.61678 | val_1_rmse: 0.6141  |  0:03:55s
epoch 17 | loss: 0.36877 | val_0_rmse: 0.61502 | val_1_rmse: 0.61436 |  0:04:08s
epoch 18 | loss: 0.36762 | val_0_rmse: 0.60246 | val_1_rmse: 0.59905 |  0:04:22s
epoch 19 | loss: 0.35258 | val_0_rmse: 0.59265 | val_1_rmse: 0.59162 |  0:04:36s
epoch 20 | loss: 0.34536 | val_0_rmse: 0.57821 | val_1_rmse: 0.57804 |  0:04:50s
epoch 21 | loss: 0.34054 | val_0_rmse: 0.58536 | val_1_rmse: 0.5839  |  0:05:03s
epoch 22 | loss: 0.33966 | val_0_rmse: 0.64406 | val_1_rmse: 0.64333 |  0:05:17s
epoch 23 | loss: 0.33974 | val_0_rmse: 0.6051  | val_1_rmse: 0.60416 |  0:05:31s
epoch 24 | loss: 0.34253 | val_0_rmse: 0.58361 | val_1_rmse: 0.58202 |  0:05:45s
epoch 25 | loss: 0.3407  | val_0_rmse: 0.56952 | val_1_rmse: 0.56875 |  0:05:59s
epoch 26 | loss: 0.3376  | val_0_rmse: 0.58105 | val_1_rmse: 0.57975 |  0:06:12s
epoch 27 | loss: 0.33728 | val_0_rmse: 0.73487 | val_1_rmse: 0.73291 |  0:06:26s
epoch 28 | loss: 0.33847 | val_0_rmse: 0.60245 | val_1_rmse: 0.60078 |  0:06:40s
epoch 29 | loss: 0.34053 | val_0_rmse: 0.57307 | val_1_rmse: 0.57227 |  0:06:54s
epoch 30 | loss: 0.33755 | val_0_rmse: 0.57786 | val_1_rmse: 0.57692 |  0:07:08s
epoch 31 | loss: 0.33439 | val_0_rmse: 0.57026 | val_1_rmse: 0.56963 |  0:07:22s
epoch 32 | loss: 0.33218 | val_0_rmse: 0.58205 | val_1_rmse: 0.58116 |  0:07:35s
epoch 33 | loss: 0.35545 | val_0_rmse: 0.59949 | val_1_rmse: 0.59776 |  0:07:49s
epoch 34 | loss: 0.35815 | val_0_rmse: 0.614   | val_1_rmse: 0.61274 |  0:08:03s
epoch 35 | loss: 0.33972 | val_0_rmse: 0.5721  | val_1_rmse: 0.57104 |  0:08:17s
epoch 36 | loss: 0.3343  | val_0_rmse: 0.58035 | val_1_rmse: 0.57852 |  0:08:31s
epoch 37 | loss: 0.33524 | val_0_rmse: 0.59445 | val_1_rmse: 0.59359 |  0:08:44s
epoch 38 | loss: 0.3374  | val_0_rmse: 0.62985 | val_1_rmse: 0.62854 |  0:08:58s
epoch 39 | loss: 0.3799  | val_0_rmse: 0.60881 | val_1_rmse: 0.60546 |  0:09:12s
epoch 40 | loss: 0.36654 | val_0_rmse: 0.60258 | val_1_rmse: 0.60053 |  0:09:26s
epoch 41 | loss: 0.3605  | val_0_rmse: 0.60345 | val_1_rmse: 0.60136 |  0:09:40s
epoch 42 | loss: 0.36147 | val_0_rmse: 0.61706 | val_1_rmse: 0.61589 |  0:09:53s
epoch 43 | loss: 0.36187 | val_0_rmse: 0.64034 | val_1_rmse: 0.63988 |  0:10:07s
epoch 44 | loss: 0.38115 | val_0_rmse: 0.61541 | val_1_rmse: 0.61421 |  0:10:21s
epoch 45 | loss: 0.34245 | val_0_rmse: 0.57431 | val_1_rmse: 0.57361 |  0:10:35s
epoch 46 | loss: 0.3421  | val_0_rmse: 0.57969 | val_1_rmse: 0.57927 |  0:10:49s
epoch 47 | loss: 0.34028 | val_0_rmse: 0.57364 | val_1_rmse: 0.5728  |  0:11:03s
epoch 48 | loss: 0.33967 | val_0_rmse: 0.57213 | val_1_rmse: 0.57064 |  0:11:17s
epoch 49 | loss: 0.33883 | val_0_rmse: 0.57438 | val_1_rmse: 0.57265 |  0:11:30s
epoch 50 | loss: 0.3379  | val_0_rmse: 0.61233 | val_1_rmse: 0.61152 |  0:11:44s
epoch 51 | loss: 0.34039 | val_0_rmse: 0.57892 | val_1_rmse: 0.57827 |  0:11:58s
epoch 52 | loss: 0.33723 | val_0_rmse: 0.57518 | val_1_rmse: 0.57424 |  0:12:12s
epoch 53 | loss: 0.33621 | val_0_rmse: 0.57964 | val_1_rmse: 0.57854 |  0:12:26s
epoch 54 | loss: 0.33873 | val_0_rmse: 0.5764  | val_1_rmse: 0.57582 |  0:12:40s
epoch 55 | loss: 0.33822 | val_0_rmse: 0.57399 | val_1_rmse: 0.57299 |  0:12:53s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.56875
Best weights from best epoch are automatically used!
ended training at: 09:26:34
Feature importance:
[('Area', 0.14671241796111972), ('Baths', 0.22588168247356616), ('Beds', 0.0), ('Latitude', 0.28629926652220455), ('Longitude', 0.049755142117028216), ('Month', 0.0), ('Year', 0.2913514909260813)]
Mean squared error is of 13982391062.290882
Mean absolute error:78085.82506231194
MAPE:0.4613224098836524
R2 score:0.6745924612967746
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:26:36
epoch 0  | loss: 0.42465 | val_0_rmse: 0.60638 | val_1_rmse: 0.60624 |  0:00:14s
epoch 1  | loss: 0.37305 | val_0_rmse: 0.60363 | val_1_rmse: 0.60309 |  0:00:27s
epoch 2  | loss: 0.36977 | val_0_rmse: 0.63896 | val_1_rmse: 0.63994 |  0:00:41s
epoch 3  | loss: 0.36599 | val_0_rmse: 0.63758 | val_1_rmse: 0.63557 |  0:00:55s
epoch 4  | loss: 0.36414 | val_0_rmse: 0.64951 | val_1_rmse: 0.64683 |  0:01:09s
epoch 5  | loss: 0.35026 | val_0_rmse: 0.5805  | val_1_rmse: 0.57875 |  0:01:23s
epoch 6  | loss: 0.34326 | val_0_rmse: 0.6027  | val_1_rmse: 0.5998  |  0:01:36s
epoch 7  | loss: 0.3419  | val_0_rmse: 0.58171 | val_1_rmse: 0.58081 |  0:01:50s
epoch 8  | loss: 0.34161 | val_0_rmse: 0.58398 | val_1_rmse: 0.58278 |  0:02:04s
epoch 9  | loss: 0.34091 | val_0_rmse: 0.58677 | val_1_rmse: 0.58661 |  0:02:18s
epoch 10 | loss: 0.34082 | val_0_rmse: 0.59924 | val_1_rmse: 0.59878 |  0:02:32s
epoch 11 | loss: 0.33875 | val_0_rmse: 0.57914 | val_1_rmse: 0.57844 |  0:02:46s
epoch 12 | loss: 0.37688 | val_0_rmse: 0.61819 | val_1_rmse: 0.61602 |  0:02:59s
epoch 13 | loss: 0.36735 | val_0_rmse: 0.59118 | val_1_rmse: 0.59079 |  0:03:13s
epoch 14 | loss: 0.34766 | val_0_rmse: 0.5996  | val_1_rmse: 0.60013 |  0:03:27s
epoch 15 | loss: 0.34451 | val_0_rmse: 0.74895 | val_1_rmse: 0.74759 |  0:03:41s
epoch 16 | loss: 0.34061 | val_0_rmse: 0.57921 | val_1_rmse: 0.57942 |  0:03:55s
epoch 17 | loss: 0.34349 | val_0_rmse: 0.57588 | val_1_rmse: 0.57531 |  0:04:08s
epoch 18 | loss: 0.33674 | val_0_rmse: 0.58938 | val_1_rmse: 0.58881 |  0:04:22s
epoch 19 | loss: 0.33846 | val_0_rmse: 0.58914 | val_1_rmse: 0.58881 |  0:04:36s
epoch 20 | loss: 0.3455  | val_0_rmse: 0.58526 | val_1_rmse: 0.5847  |  0:04:50s
epoch 21 | loss: 0.33767 | val_0_rmse: 0.59297 | val_1_rmse: 0.5909  |  0:05:04s
epoch 22 | loss: 0.33418 | val_0_rmse: 0.59302 | val_1_rmse: 0.59201 |  0:05:18s
epoch 23 | loss: 0.33133 | val_0_rmse: 0.58573 | val_1_rmse: 0.58598 |  0:05:31s
epoch 24 | loss: 0.33053 | val_0_rmse: 0.59085 | val_1_rmse: 0.59092 |  0:05:45s
epoch 25 | loss: 0.33231 | val_0_rmse: 0.59174 | val_1_rmse: 0.5915  |  0:05:59s
epoch 26 | loss: 0.32922 | val_0_rmse: 0.58136 | val_1_rmse: 0.58073 |  0:06:13s
epoch 27 | loss: 0.32941 | val_0_rmse: 0.58128 | val_1_rmse: 0.58196 |  0:06:27s
epoch 28 | loss: 0.33445 | val_0_rmse: 0.58103 | val_1_rmse: 0.58142 |  0:06:40s
epoch 29 | loss: 0.33256 | val_0_rmse: 0.61796 | val_1_rmse: 0.61586 |  0:06:54s
epoch 30 | loss: 0.33152 | val_0_rmse: 0.58229 | val_1_rmse: 0.5827  |  0:07:08s
epoch 31 | loss: 0.33067 | val_0_rmse: 0.58123 | val_1_rmse: 0.58257 |  0:07:22s
epoch 32 | loss: 0.32926 | val_0_rmse: 0.57111 | val_1_rmse: 0.57163 |  0:07:36s
epoch 33 | loss: 0.32911 | val_0_rmse: 0.59167 | val_1_rmse: 0.59227 |  0:07:49s
epoch 34 | loss: 0.32824 | val_0_rmse: 0.58295 | val_1_rmse: 0.58332 |  0:08:03s
epoch 35 | loss: 0.32931 | val_0_rmse: 0.60366 | val_1_rmse: 0.60344 |  0:08:17s
epoch 36 | loss: 0.32928 | val_0_rmse: 0.58158 | val_1_rmse: 0.58269 |  0:08:31s
epoch 37 | loss: 0.3292  | val_0_rmse: 0.58532 | val_1_rmse: 0.58615 |  0:08:44s
epoch 38 | loss: 0.32896 | val_0_rmse: 0.58479 | val_1_rmse: 0.58566 |  0:08:58s
epoch 39 | loss: 0.32797 | val_0_rmse: 0.5913  | val_1_rmse: 0.59334 |  0:09:12s
epoch 40 | loss: 0.32828 | val_0_rmse: 0.58138 | val_1_rmse: 0.58163 |  0:09:26s
epoch 41 | loss: 0.32717 | val_0_rmse: 0.58235 | val_1_rmse: 0.58144 |  0:09:40s
epoch 42 | loss: 0.3272  | val_0_rmse: 0.57864 | val_1_rmse: 0.58035 |  0:09:53s
epoch 43 | loss: 0.33407 | val_0_rmse: 0.58155 | val_1_rmse: 0.58157 |  0:10:07s
epoch 44 | loss: 0.32907 | val_0_rmse: 0.59017 | val_1_rmse: 0.5915  |  0:10:21s
epoch 45 | loss: 0.32938 | val_0_rmse: 0.57977 | val_1_rmse: 0.58114 |  0:10:35s
epoch 46 | loss: 0.32722 | val_0_rmse: 0.60029 | val_1_rmse: 0.60078 |  0:10:49s
epoch 47 | loss: 0.32476 | val_0_rmse: 0.57939 | val_1_rmse: 0.58111 |  0:11:03s
epoch 48 | loss: 0.32468 | val_0_rmse: 0.59488 | val_1_rmse: 0.59417 |  0:11:16s
epoch 49 | loss: 0.32407 | val_0_rmse: 0.57599 | val_1_rmse: 0.57835 |  0:11:30s
epoch 50 | loss: 0.32444 | val_0_rmse: 0.60552 | val_1_rmse: 0.60463 |  0:11:44s
epoch 51 | loss: 0.32603 | val_0_rmse: 0.57856 | val_1_rmse: 0.58023 |  0:11:58s
epoch 52 | loss: 0.32728 | val_0_rmse: 0.59068 | val_1_rmse: 0.59145 |  0:12:12s
epoch 53 | loss: 0.32444 | val_0_rmse: 0.58854 | val_1_rmse: 0.58891 |  0:12:25s
epoch 54 | loss: 0.32502 | val_0_rmse: 0.57453 | val_1_rmse: 0.57514 |  0:12:39s
epoch 55 | loss: 0.32344 | val_0_rmse: 0.60509 | val_1_rmse: 0.60484 |  0:12:53s
epoch 56 | loss: 0.32439 | val_0_rmse: 0.5999  | val_1_rmse: 0.59923 |  0:13:07s
epoch 57 | loss: 0.32197 | val_0_rmse: 0.59315 | val_1_rmse: 0.59262 |  0:13:20s
epoch 58 | loss: 0.32349 | val_0_rmse: 0.57944 | val_1_rmse: 0.58046 |  0:13:34s
epoch 59 | loss: 0.32455 | val_0_rmse: 0.60539 | val_1_rmse: 0.60467 |  0:13:48s
epoch 60 | loss: 0.32383 | val_0_rmse: 0.59754 | val_1_rmse: 0.59689 |  0:14:02s
epoch 61 | loss: 0.32356 | val_0_rmse: 0.5827  | val_1_rmse: 0.58499 |  0:14:16s
epoch 62 | loss: 0.32155 | val_0_rmse: 0.60505 | val_1_rmse: 0.60588 |  0:14:30s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.57163
Best weights from best epoch are automatically used!
ended training at: 09:41:10
Feature importance:
[('Area', 0.23210832367037992), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.33457616386778793), ('Longitude', 0.4333059848686221), ('Month', 0.0), ('Year', 9.527593209973294e-06)]
Mean squared error is of 14004037952.177063
Mean absolute error:77268.6168119803
MAPE:0.4497986408060629
R2 score:0.6703509112278805
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:41:12
epoch 0  | loss: 0.4335  | val_0_rmse: 0.58692 | val_1_rmse: 0.58294 |  0:00:14s
epoch 1  | loss: 0.36279 | val_0_rmse: 0.59314 | val_1_rmse: 0.59009 |  0:00:28s
epoch 2  | loss: 0.35577 | val_0_rmse: 0.58039 | val_1_rmse: 0.5797  |  0:00:41s
epoch 3  | loss: 0.35307 | val_0_rmse: 0.59875 | val_1_rmse: 0.59822 |  0:00:55s
epoch 4  | loss: 0.34926 | val_0_rmse: 0.58226 | val_1_rmse: 0.58121 |  0:01:09s
epoch 5  | loss: 0.34653 | val_0_rmse: 0.6137  | val_1_rmse: 0.61204 |  0:01:23s
epoch 6  | loss: 0.34581 | val_0_rmse: 0.59141 | val_1_rmse: 0.58851 |  0:01:37s
epoch 7  | loss: 0.34475 | val_0_rmse: 0.61819 | val_1_rmse: 0.61754 |  0:01:51s
epoch 8  | loss: 0.34301 | val_0_rmse: 0.57434 | val_1_rmse: 0.57403 |  0:02:05s
epoch 9  | loss: 0.34519 | val_0_rmse: 0.58985 | val_1_rmse: 0.58688 |  0:02:19s
epoch 10 | loss: 0.3452  | val_0_rmse: 0.58371 | val_1_rmse: 0.58166 |  0:02:32s
epoch 11 | loss: 0.35329 | val_0_rmse: 0.5956  | val_1_rmse: 0.59233 |  0:02:46s
epoch 12 | loss: 0.34794 | val_0_rmse: 0.58181 | val_1_rmse: 0.58067 |  0:03:00s
epoch 13 | loss: 0.34031 | val_0_rmse: 0.60894 | val_1_rmse: 0.60813 |  0:03:14s
epoch 14 | loss: 0.34022 | val_0_rmse: 0.61313 | val_1_rmse: 0.61189 |  0:03:28s
epoch 15 | loss: 0.33711 | val_0_rmse: 0.61352 | val_1_rmse: 0.6136  |  0:03:42s
epoch 16 | loss: 0.33716 | val_0_rmse: 0.58601 | val_1_rmse: 0.58484 |  0:03:56s
epoch 17 | loss: 0.33514 | val_0_rmse: 0.58752 | val_1_rmse: 0.58584 |  0:04:09s
epoch 18 | loss: 0.33536 | val_0_rmse: 0.58126 | val_1_rmse: 0.57983 |  0:04:23s
epoch 19 | loss: 0.33407 | val_0_rmse: 0.5889  | val_1_rmse: 0.58874 |  0:04:37s
epoch 20 | loss: 0.33496 | val_0_rmse: 0.57161 | val_1_rmse: 0.57092 |  0:04:51s
epoch 21 | loss: 0.33322 | val_0_rmse: 0.61421 | val_1_rmse: 0.61374 |  0:05:05s
epoch 22 | loss: 0.33586 | val_0_rmse: 0.59242 | val_1_rmse: 0.59127 |  0:05:18s
epoch 23 | loss: 0.33547 | val_0_rmse: 0.58219 | val_1_rmse: 0.58123 |  0:05:32s
epoch 24 | loss: 0.334   | val_0_rmse: 0.58075 | val_1_rmse: 0.57972 |  0:05:46s
epoch 25 | loss: 0.33188 | val_0_rmse: 0.58479 | val_1_rmse: 0.58527 |  0:06:00s
epoch 26 | loss: 0.33341 | val_0_rmse: 0.57018 | val_1_rmse: 0.57052 |  0:06:13s
epoch 27 | loss: 0.3314  | val_0_rmse: 0.65772 | val_1_rmse: 0.65469 |  0:06:27s
epoch 28 | loss: 0.33251 | val_0_rmse: 0.60276 | val_1_rmse: 0.60273 |  0:06:41s
epoch 29 | loss: 0.3295  | val_0_rmse: 0.62214 | val_1_rmse: 0.62205 |  0:06:55s
epoch 30 | loss: 0.32917 | val_0_rmse: 0.61135 | val_1_rmse: 0.61049 |  0:07:09s
epoch 31 | loss: 0.32879 | val_0_rmse: 0.60908 | val_1_rmse: 0.60774 |  0:07:23s
epoch 32 | loss: 0.32838 | val_0_rmse: 0.6059  | val_1_rmse: 0.60706 |  0:07:37s
epoch 33 | loss: 0.32821 | val_0_rmse: 0.57095 | val_1_rmse: 0.57103 |  0:07:50s
epoch 34 | loss: 0.32834 | val_0_rmse: 0.6406  | val_1_rmse: 0.64051 |  0:08:04s
epoch 35 | loss: 0.32817 | val_0_rmse: 0.60184 | val_1_rmse: 0.60061 |  0:08:18s
epoch 36 | loss: 0.3289  | val_0_rmse: 0.72009 | val_1_rmse: 0.71708 |  0:08:32s
epoch 37 | loss: 0.32811 | val_0_rmse: 0.59246 | val_1_rmse: 0.59279 |  0:08:46s
epoch 38 | loss: 0.32799 | val_0_rmse: 0.5864  | val_1_rmse: 0.58661 |  0:09:00s
epoch 39 | loss: 0.32634 | val_0_rmse: 0.5967  | val_1_rmse: 0.59676 |  0:09:14s
epoch 40 | loss: 0.3264  | val_0_rmse: 0.59223 | val_1_rmse: 0.59355 |  0:09:28s
epoch 41 | loss: 0.32984 | val_0_rmse: 0.58733 | val_1_rmse: 0.58806 |  0:09:42s
epoch 42 | loss: 0.32766 | val_0_rmse: 0.56763 | val_1_rmse: 0.56885 |  0:09:56s
epoch 43 | loss: 0.32617 | val_0_rmse: 0.59825 | val_1_rmse: 0.59931 |  0:10:09s
epoch 44 | loss: 0.3255  | val_0_rmse: 0.58856 | val_1_rmse: 0.58822 |  0:10:23s
epoch 45 | loss: 0.3244  | val_0_rmse: 0.58728 | val_1_rmse: 0.58722 |  0:10:37s
epoch 46 | loss: 0.34163 | val_0_rmse: 0.6469  | val_1_rmse: 0.64572 |  0:10:51s
epoch 47 | loss: 0.34389 | val_0_rmse: 0.58571 | val_1_rmse: 0.58451 |  0:11:05s
epoch 48 | loss: 0.33417 | val_0_rmse: 0.57984 | val_1_rmse: 0.57912 |  0:11:19s
epoch 49 | loss: 0.32569 | val_0_rmse: 0.58703 | val_1_rmse: 0.58733 |  0:11:33s
epoch 50 | loss: 0.32557 | val_0_rmse: 0.57858 | val_1_rmse: 0.57999 |  0:11:47s
epoch 51 | loss: 0.32569 | val_0_rmse: 0.57333 | val_1_rmse: 0.57313 |  0:12:00s
epoch 52 | loss: 0.32408 | val_0_rmse: 0.5866  | val_1_rmse: 0.58671 |  0:12:14s
epoch 53 | loss: 0.3245  | val_0_rmse: 0.57107 | val_1_rmse: 0.57337 |  0:12:28s
epoch 54 | loss: 0.32424 | val_0_rmse: 0.58642 | val_1_rmse: 0.58782 |  0:12:42s
epoch 55 | loss: 0.32475 | val_0_rmse: 0.59325 | val_1_rmse: 0.59421 |  0:12:56s
epoch 56 | loss: 0.32341 | val_0_rmse: 0.5952  | val_1_rmse: 0.59508 |  0:13:10s
epoch 57 | loss: 0.3246  | val_0_rmse: 0.57434 | val_1_rmse: 0.57426 |  0:13:23s
epoch 58 | loss: 0.32234 | val_0_rmse: 0.58254 | val_1_rmse: 0.58416 |  0:13:37s
epoch 59 | loss: 0.32259 | val_0_rmse: 0.58256 | val_1_rmse: 0.58416 |  0:13:51s
epoch 60 | loss: 0.32349 | val_0_rmse: 0.59257 | val_1_rmse: 0.59359 |  0:14:05s
epoch 61 | loss: 0.32407 | val_0_rmse: 0.58998 | val_1_rmse: 0.59109 |  0:14:19s
epoch 62 | loss: 0.32373 | val_0_rmse: 0.57068 | val_1_rmse: 0.57111 |  0:14:33s
epoch 63 | loss: 0.32167 | val_0_rmse: 0.58391 | val_1_rmse: 0.58556 |  0:14:47s
epoch 64 | loss: 0.32187 | val_0_rmse: 0.58051 | val_1_rmse: 0.58197 |  0:15:01s
epoch 65 | loss: 0.32315 | val_0_rmse: 0.58298 | val_1_rmse: 0.58435 |  0:15:14s
epoch 66 | loss: 0.32285 | val_0_rmse: 0.57234 | val_1_rmse: 0.57379 |  0:15:28s
epoch 67 | loss: 0.32186 | val_0_rmse: 0.58516 | val_1_rmse: 0.58602 |  0:15:42s
epoch 68 | loss: 0.32251 | val_0_rmse: 0.57961 | val_1_rmse: 0.5828  |  0:15:56s
epoch 69 | loss: 0.3208  | val_0_rmse: 0.60926 | val_1_rmse: 0.61082 |  0:16:10s
epoch 70 | loss: 0.32239 | val_0_rmse: 0.57112 | val_1_rmse: 0.57194 |  0:16:23s
epoch 71 | loss: 0.32023 | val_0_rmse: 0.60619 | val_1_rmse: 0.60721 |  0:16:37s
epoch 72 | loss: 0.32196 | val_0_rmse: 0.56878 | val_1_rmse: 0.57149 |  0:16:51s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.56885
Best weights from best epoch are automatically used!
ended training at: 09:58:07
Feature importance:
[('Area', 0.40248963430035817), ('Baths', 0.2236583828155812), ('Beds', 0.06777215214486348), ('Latitude', 0.17906914815553499), ('Longitude', 0.12701068258366216), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 14068183480.20336
Mean absolute error:78721.55041449724
MAPE:0.4350563273663958
R2 score:0.6763554040398276
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:58:09
epoch 0  | loss: 0.43087 | val_0_rmse: 0.6144  | val_1_rmse: 0.60627 |  0:00:14s
epoch 1  | loss: 0.3689  | val_0_rmse: 0.60669 | val_1_rmse: 0.60082 |  0:00:27s
epoch 2  | loss: 0.36012 | val_0_rmse: 0.5902  | val_1_rmse: 0.58081 |  0:00:41s
epoch 3  | loss: 0.35955 | val_0_rmse: 0.59857 | val_1_rmse: 0.59099 |  0:00:55s
epoch 4  | loss: 0.36235 | val_0_rmse: 0.59304 | val_1_rmse: 0.58356 |  0:01:09s
epoch 5  | loss: 0.35676 | val_0_rmse: 0.68755 | val_1_rmse: 0.68353 |  0:01:23s
epoch 6  | loss: 0.35848 | val_0_rmse: 0.58839 | val_1_rmse: 0.58176 |  0:01:36s
epoch 7  | loss: 0.34815 | val_0_rmse: 0.60883 | val_1_rmse: 0.60164 |  0:01:50s
epoch 8  | loss: 0.35678 | val_0_rmse: 0.67266 | val_1_rmse: 0.6622  |  0:02:04s
epoch 9  | loss: 0.36267 | val_0_rmse: 0.59545 | val_1_rmse: 0.58717 |  0:02:18s
epoch 10 | loss: 0.35837 | val_0_rmse: 0.59309 | val_1_rmse: 0.58476 |  0:02:32s
epoch 11 | loss: 0.35679 | val_0_rmse: 0.6763  | val_1_rmse: 0.66361 |  0:02:45s
epoch 12 | loss: 0.35346 | val_0_rmse: 0.60038 | val_1_rmse: 0.59112 |  0:02:59s
epoch 13 | loss: 0.35565 | val_0_rmse: 0.61792 | val_1_rmse: 0.60517 |  0:03:13s
epoch 14 | loss: 0.35206 | val_0_rmse: 0.63868 | val_1_rmse: 0.62664 |  0:03:27s
epoch 15 | loss: 0.35352 | val_0_rmse: 0.6329  | val_1_rmse: 0.62211 |  0:03:40s
epoch 16 | loss: 0.35287 | val_0_rmse: 0.60582 | val_1_rmse: 0.59883 |  0:03:54s
epoch 17 | loss: 0.3521  | val_0_rmse: 0.60401 | val_1_rmse: 0.59581 |  0:04:08s
epoch 18 | loss: 0.35292 | val_0_rmse: 0.59918 | val_1_rmse: 0.58954 |  0:04:22s
epoch 19 | loss: 0.352   | val_0_rmse: 0.6203  | val_1_rmse: 0.61539 |  0:04:35s
epoch 20 | loss: 0.35138 | val_0_rmse: 0.61273 | val_1_rmse: 0.60322 |  0:04:49s
epoch 21 | loss: 0.34322 | val_0_rmse: 0.60968 | val_1_rmse: 0.60271 |  0:05:03s
epoch 22 | loss: 0.3367  | val_0_rmse: 0.5955  | val_1_rmse: 0.58677 |  0:05:17s
epoch 23 | loss: 0.33619 | val_0_rmse: 0.58673 | val_1_rmse: 0.57724 |  0:05:31s
epoch 24 | loss: 0.33396 | val_0_rmse: 0.59025 | val_1_rmse: 0.58173 |  0:05:44s
epoch 25 | loss: 0.33336 | val_0_rmse: 0.58677 | val_1_rmse: 0.57972 |  0:05:58s
epoch 26 | loss: 0.33294 | val_0_rmse: 0.58025 | val_1_rmse: 0.5753  |  0:06:12s
epoch 27 | loss: 0.33275 | val_0_rmse: 0.64143 | val_1_rmse: 0.63372 |  0:06:26s
epoch 28 | loss: 0.33246 | val_0_rmse: 0.5731  | val_1_rmse: 0.56521 |  0:06:40s
epoch 29 | loss: 0.3345  | val_0_rmse: 0.57356 | val_1_rmse: 0.56706 |  0:06:53s
epoch 30 | loss: 0.3326  | val_0_rmse: 0.63591 | val_1_rmse: 0.62558 |  0:07:08s
epoch 31 | loss: 0.33147 | val_0_rmse: 0.60384 | val_1_rmse: 0.59879 |  0:07:21s
epoch 32 | loss: 0.33344 | val_0_rmse: 0.5889  | val_1_rmse: 0.58279 |  0:07:35s
epoch 33 | loss: 0.33037 | val_0_rmse: 0.59277 | val_1_rmse: 0.58849 |  0:07:49s
epoch 34 | loss: 0.32865 | val_0_rmse: 0.59427 | val_1_rmse: 0.58757 |  0:08:03s
epoch 35 | loss: 0.32902 | val_0_rmse: 0.59085 | val_1_rmse: 0.58256 |  0:08:17s
epoch 36 | loss: 0.3284  | val_0_rmse: 0.59897 | val_1_rmse: 0.59315 |  0:08:30s
epoch 37 | loss: 0.32808 | val_0_rmse: 0.58547 | val_1_rmse: 0.57576 |  0:08:44s
epoch 38 | loss: 0.32858 | val_0_rmse: 0.59436 | val_1_rmse: 0.58826 |  0:08:58s
epoch 39 | loss: 0.32669 | val_0_rmse: 0.57575 | val_1_rmse: 0.56751 |  0:09:12s
epoch 40 | loss: 0.3265  | val_0_rmse: 0.61988 | val_1_rmse: 0.61312 |  0:09:26s
epoch 41 | loss: 0.32889 | val_0_rmse: 0.57263 | val_1_rmse: 0.56559 |  0:09:39s
epoch 42 | loss: 0.32784 | val_0_rmse: 0.57712 | val_1_rmse: 0.57047 |  0:09:53s
epoch 43 | loss: 0.327   | val_0_rmse: 0.57929 | val_1_rmse: 0.57303 |  0:10:07s
epoch 44 | loss: 0.32574 | val_0_rmse: 0.58294 | val_1_rmse: 0.57529 |  0:10:21s
epoch 45 | loss: 0.32629 | val_0_rmse: 0.57408 | val_1_rmse: 0.56553 |  0:10:35s
epoch 46 | loss: 0.32403 | val_0_rmse: 0.57499 | val_1_rmse: 0.56872 |  0:10:48s
epoch 47 | loss: 0.32565 | val_0_rmse: 0.59478 | val_1_rmse: 0.59083 |  0:11:02s
epoch 48 | loss: 0.32412 | val_0_rmse: 0.58423 | val_1_rmse: 0.57741 |  0:11:16s
epoch 49 | loss: 0.3246  | val_0_rmse: 0.5735  | val_1_rmse: 0.5667  |  0:11:30s
epoch 50 | loss: 0.3241  | val_0_rmse: 0.58746 | val_1_rmse: 0.57951 |  0:11:44s
epoch 51 | loss: 0.32237 | val_0_rmse: 0.60273 | val_1_rmse: 0.59783 |  0:11:58s
epoch 52 | loss: 0.32234 | val_0_rmse: 0.5924  | val_1_rmse: 0.58586 |  0:12:12s
epoch 53 | loss: 0.323   | val_0_rmse: 0.5931  | val_1_rmse: 0.5873  |  0:12:26s
epoch 54 | loss: 0.32475 | val_0_rmse: 0.58251 | val_1_rmse: 0.57345 |  0:12:39s
epoch 55 | loss: 0.32231 | val_0_rmse: 0.58711 | val_1_rmse: 0.57907 |  0:12:53s
epoch 56 | loss: 0.32514 | val_0_rmse: 0.69981 | val_1_rmse: 0.69895 |  0:13:07s
epoch 57 | loss: 0.32548 | val_0_rmse: 0.57375 | val_1_rmse: 0.56762 |  0:13:21s
epoch 58 | loss: 0.32221 | val_0_rmse: 0.6389  | val_1_rmse: 0.63496 |  0:13:35s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.56521
Best weights from best epoch are automatically used!
ended training at: 10:11:49
Feature importance:
[('Area', 0.24047513697279713), ('Baths', 0.11915652310180369), ('Beds', 0.0), ('Latitude', 0.26363474716750634), ('Longitude', 0.06929185246224524), ('Month', 0.12935619625776376), ('Year', 0.1780855440378838)]
Mean squared error is of 14300646041.500858
Mean absolute error:79313.51026220426
MAPE:0.47050889383886224
R2 score:0.673524456333758
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:11:50
epoch 0  | loss: 0.42641 | val_0_rmse: 0.60573 | val_1_rmse: 0.61005 |  0:00:14s
epoch 1  | loss: 0.36244 | val_0_rmse: 0.60949 | val_1_rmse: 0.6111  |  0:00:27s
epoch 2  | loss: 0.35414 | val_0_rmse: 0.58608 | val_1_rmse: 0.58811 |  0:00:41s
epoch 3  | loss: 0.35213 | val_0_rmse: 0.59469 | val_1_rmse: 0.59716 |  0:00:55s
epoch 4  | loss: 0.34932 | val_0_rmse: 0.62967 | val_1_rmse: 0.63    |  0:01:09s
epoch 5  | loss: 0.34779 | val_0_rmse: 0.64631 | val_1_rmse: 0.64596 |  0:01:23s
epoch 6  | loss: 0.34312 | val_0_rmse: 0.58068 | val_1_rmse: 0.58121 |  0:01:37s
epoch 7  | loss: 0.34094 | val_0_rmse: 0.58671 | val_1_rmse: 0.58773 |  0:01:51s
epoch 8  | loss: 0.34062 | val_0_rmse: 0.61768 | val_1_rmse: 0.6193  |  0:02:05s
epoch 9  | loss: 0.33969 | val_0_rmse: 0.59159 | val_1_rmse: 0.59354 |  0:02:19s
epoch 10 | loss: 0.33718 | val_0_rmse: 0.57744 | val_1_rmse: 0.57867 |  0:02:32s
epoch 11 | loss: 0.34093 | val_0_rmse: 0.60238 | val_1_rmse: 0.60486 |  0:02:46s
epoch 12 | loss: 0.34195 | val_0_rmse: 0.57654 | val_1_rmse: 0.57762 |  0:03:00s
epoch 13 | loss: 0.3376  | val_0_rmse: 0.5961  | val_1_rmse: 0.59782 |  0:03:14s
epoch 14 | loss: 0.33452 | val_0_rmse: 0.58104 | val_1_rmse: 0.58208 |  0:03:28s
epoch 15 | loss: 0.33701 | val_0_rmse: 0.61052 | val_1_rmse: 0.6101  |  0:03:42s
epoch 16 | loss: 0.3337  | val_0_rmse: 0.59354 | val_1_rmse: 0.59447 |  0:03:56s
epoch 17 | loss: 0.33921 | val_0_rmse: 0.61386 | val_1_rmse: 0.61563 |  0:04:09s
epoch 18 | loss: 0.3368  | val_0_rmse: 0.58187 | val_1_rmse: 0.58458 |  0:04:23s
epoch 19 | loss: 0.33688 | val_0_rmse: 0.59077 | val_1_rmse: 0.59262 |  0:04:37s
epoch 20 | loss: 0.33431 | val_0_rmse: 0.62105 | val_1_rmse: 0.6217  |  0:04:51s
epoch 21 | loss: 0.33149 | val_0_rmse: 0.57334 | val_1_rmse: 0.57486 |  0:05:05s
epoch 22 | loss: 0.33082 | val_0_rmse: 0.57446 | val_1_rmse: 0.57646 |  0:05:19s
epoch 23 | loss: 0.33143 | val_0_rmse: 0.57616 | val_1_rmse: 0.57825 |  0:05:33s
epoch 24 | loss: 0.33239 | val_0_rmse: 0.59499 | val_1_rmse: 0.5973  |  0:05:47s
epoch 25 | loss: 0.33372 | val_0_rmse: 0.61222 | val_1_rmse: 0.61388 |  0:06:01s
epoch 26 | loss: 0.33587 | val_0_rmse: 0.61234 | val_1_rmse: 0.61198 |  0:06:15s
epoch 27 | loss: 0.33245 | val_0_rmse: 0.59475 | val_1_rmse: 0.59571 |  0:06:28s
epoch 28 | loss: 0.33461 | val_0_rmse: 0.58827 | val_1_rmse: 0.58774 |  0:06:42s
epoch 29 | loss: 0.33467 | val_0_rmse: 0.58603 | val_1_rmse: 0.58658 |  0:06:56s
epoch 30 | loss: 0.32912 | val_0_rmse: 0.57144 | val_1_rmse: 0.57268 |  0:07:10s
epoch 31 | loss: 0.32974 | val_0_rmse: 0.59572 | val_1_rmse: 0.59752 |  0:07:24s
epoch 32 | loss: 0.33169 | val_0_rmse: 0.62821 | val_1_rmse: 0.62876 |  0:07:38s
epoch 33 | loss: 0.33058 | val_0_rmse: 0.59875 | val_1_rmse: 0.60182 |  0:07:52s
epoch 34 | loss: 0.32822 | val_0_rmse: 0.5932  | val_1_rmse: 0.59426 |  0:08:06s
epoch 35 | loss: 0.33022 | val_0_rmse: 0.78011 | val_1_rmse: 0.78262 |  0:08:20s
epoch 36 | loss: 0.33277 | val_0_rmse: 0.59941 | val_1_rmse: 0.60091 |  0:08:34s
epoch 37 | loss: 0.33363 | val_0_rmse: 0.57943 | val_1_rmse: 0.58159 |  0:08:48s
epoch 38 | loss: 0.32987 | val_0_rmse: 0.57257 | val_1_rmse: 0.57487 |  0:09:02s
epoch 39 | loss: 0.32944 | val_0_rmse: 0.60285 | val_1_rmse: 0.60366 |  0:09:16s
epoch 40 | loss: 0.32728 | val_0_rmse: 0.56694 | val_1_rmse: 0.56822 |  0:09:30s
epoch 41 | loss: 0.32736 | val_0_rmse: 0.63885 | val_1_rmse: 0.63965 |  0:09:44s
epoch 42 | loss: 0.32687 | val_0_rmse: 0.58329 | val_1_rmse: 0.58611 |  0:09:59s
epoch 43 | loss: 0.32872 | val_0_rmse: 0.58049 | val_1_rmse: 0.58186 |  0:10:13s
epoch 44 | loss: 0.32738 | val_0_rmse: 0.59665 | val_1_rmse: 0.59738 |  0:10:28s
epoch 45 | loss: 0.32774 | val_0_rmse: 0.58365 | val_1_rmse: 0.58429 |  0:10:42s
epoch 46 | loss: 0.32816 | val_0_rmse: 0.60302 | val_1_rmse: 0.60444 |  0:10:55s
epoch 47 | loss: 0.32839 | val_0_rmse: 0.59322 | val_1_rmse: 0.59573 |  0:11:09s
epoch 48 | loss: 0.32912 | val_0_rmse: 0.59206 | val_1_rmse: 0.59438 |  0:11:23s
epoch 49 | loss: 0.32817 | val_0_rmse: 0.58409 | val_1_rmse: 0.58593 |  0:11:37s
epoch 50 | loss: 0.35209 | val_0_rmse: 0.61098 | val_1_rmse: 0.61369 |  0:11:51s
epoch 51 | loss: 0.34157 | val_0_rmse: 0.59031 | val_1_rmse: 0.59312 |  0:12:05s
epoch 52 | loss: 0.33388 | val_0_rmse: 0.59348 | val_1_rmse: 0.59516 |  0:12:19s
epoch 53 | loss: 0.3329  | val_0_rmse: 0.59046 | val_1_rmse: 0.59387 |  0:12:33s
epoch 54 | loss: 0.32972 | val_0_rmse: 0.59259 | val_1_rmse: 0.59502 |  0:12:46s
epoch 55 | loss: 0.32565 | val_0_rmse: 0.58356 | val_1_rmse: 0.58606 |  0:13:00s
epoch 56 | loss: 0.3248  | val_0_rmse: 0.5752  | val_1_rmse: 0.57714 |  0:13:14s
epoch 57 | loss: 0.32484 | val_0_rmse: 0.5861  | val_1_rmse: 0.58841 |  0:13:28s
epoch 58 | loss: 0.33638 | val_0_rmse: 0.58679 | val_1_rmse: 0.58886 |  0:13:42s
epoch 59 | loss: 0.32646 | val_0_rmse: 0.57745 | val_1_rmse: 0.57948 |  0:13:56s
epoch 60 | loss: 0.32602 | val_0_rmse: 0.59541 | val_1_rmse: 0.59683 |  0:14:10s
epoch 61 | loss: 0.32689 | val_0_rmse: 0.59259 | val_1_rmse: 0.59477 |  0:14:24s
epoch 62 | loss: 0.32565 | val_0_rmse: 0.58758 | val_1_rmse: 0.58988 |  0:14:37s
epoch 63 | loss: 0.32865 | val_0_rmse: 0.5752  | val_1_rmse: 0.57692 |  0:14:51s
epoch 64 | loss: 0.32756 | val_0_rmse: 0.5705  | val_1_rmse: 0.5729  |  0:15:05s
epoch 65 | loss: 0.32582 | val_0_rmse: 0.60189 | val_1_rmse: 0.60254 |  0:15:19s
epoch 66 | loss: 0.32491 | val_0_rmse: 0.57701 | val_1_rmse: 0.57977 |  0:15:33s
epoch 67 | loss: 0.32422 | val_0_rmse: 0.57027 | val_1_rmse: 0.57191 |  0:15:47s
epoch 68 | loss: 0.32363 | val_0_rmse: 0.56453 | val_1_rmse: 0.56631 |  0:16:01s
epoch 69 | loss: 0.32323 | val_0_rmse: 0.5649  | val_1_rmse: 0.56738 |  0:16:15s
epoch 70 | loss: 0.32282 | val_0_rmse: 0.5688  | val_1_rmse: 0.57102 |  0:16:29s
epoch 71 | loss: 0.32465 | val_0_rmse: 0.58934 | val_1_rmse: 0.59071 |  0:16:43s
epoch 72 | loss: 0.32403 | val_0_rmse: 0.58676 | val_1_rmse: 0.58852 |  0:16:57s
epoch 73 | loss: 0.32522 | val_0_rmse: 0.59639 | val_1_rmse: 0.59741 |  0:17:11s
epoch 74 | loss: 0.32553 | val_0_rmse: 0.58385 | val_1_rmse: 0.58705 |  0:17:24s
epoch 75 | loss: 0.3235  | val_0_rmse: 0.74477 | val_1_rmse: 0.7478  |  0:17:38s
epoch 76 | loss: 0.32413 | val_0_rmse: 0.59346 | val_1_rmse: 0.59605 |  0:17:52s
epoch 77 | loss: 0.32308 | val_0_rmse: 0.59488 | val_1_rmse: 0.59589 |  0:18:06s
epoch 78 | loss: 0.32318 | val_0_rmse: 0.57206 | val_1_rmse: 0.57517 |  0:18:20s
epoch 79 | loss: 0.3222  | val_0_rmse: 0.58172 | val_1_rmse: 0.58419 |  0:18:34s
epoch 80 | loss: 0.3232  | val_0_rmse: 0.56879 | val_1_rmse: 0.57151 |  0:18:49s
epoch 81 | loss: 0.32138 | val_0_rmse: 0.58739 | val_1_rmse: 0.58915 |  0:19:04s
epoch 82 | loss: 0.32211 | val_0_rmse: 0.56681 | val_1_rmse: 0.56869 |  0:19:18s
epoch 83 | loss: 0.32082 | val_0_rmse: 0.57207 | val_1_rmse: 0.57455 |  0:19:31s
epoch 84 | loss: 0.32235 | val_0_rmse: 0.58031 | val_1_rmse: 0.5836  |  0:19:45s
epoch 85 | loss: 0.32169 | val_0_rmse: 0.57633 | val_1_rmse: 0.57905 |  0:19:59s
epoch 86 | loss: 0.32161 | val_0_rmse: 0.57856 | val_1_rmse: 0.58164 |  0:20:13s
epoch 87 | loss: 0.32263 | val_0_rmse: 0.5777  | val_1_rmse: 0.58073 |  0:20:27s
epoch 88 | loss: 0.32376 | val_0_rmse: 0.57294 | val_1_rmse: 0.57535 |  0:20:41s
epoch 89 | loss: 0.32568 | val_0_rmse: 0.67758 | val_1_rmse: 0.67978 |  0:20:54s
epoch 90 | loss: 0.3305  | val_0_rmse: 0.6371  | val_1_rmse: 0.64032 |  0:21:08s
epoch 91 | loss: 0.32798 | val_0_rmse: 0.58474 | val_1_rmse: 0.58812 |  0:21:22s
epoch 92 | loss: 0.32177 | val_0_rmse: 0.58123 | val_1_rmse: 0.58496 |  0:21:36s
epoch 93 | loss: 0.32107 | val_0_rmse: 0.58169 | val_1_rmse: 0.58483 |  0:21:50s
epoch 94 | loss: 0.32479 | val_0_rmse: 0.5816  | val_1_rmse: 0.58466 |  0:22:04s
epoch 95 | loss: 0.32748 | val_0_rmse: 0.58614 | val_1_rmse: 0.58942 |  0:22:17s
epoch 96 | loss: 0.32512 | val_0_rmse: 0.57901 | val_1_rmse: 0.58157 |  0:22:31s
epoch 97 | loss: 0.32358 | val_0_rmse: 0.5789  | val_1_rmse: 0.58178 |  0:22:45s
epoch 98 | loss: 0.32385 | val_0_rmse: 0.58837 | val_1_rmse: 0.59022 |  0:22:59s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.56631
Best weights from best epoch are automatically used!
ended training at: 10:34:54
Feature importance:
[('Area', 0.309042557452027), ('Baths', 0.21285531032968566), ('Beds', 0.0), ('Latitude', 0.19962540002711077), ('Longitude', 0.21390529292065805), ('Month', 0.0), ('Year', 0.06457143927051848)]
Mean squared error is of 13809293669.819458
Mean absolute error:76903.29959356107
MAPE:0.4439574661720545
R2 score:0.6764431820698761
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:34:56
epoch 0  | loss: 0.42344 | val_0_rmse: 0.58982 | val_1_rmse: 0.58776 |  0:00:14s
epoch 1  | loss: 0.35877 | val_0_rmse: 0.59145 | val_1_rmse: 0.58779 |  0:00:28s
epoch 2  | loss: 0.38454 | val_0_rmse: 0.60856 | val_1_rmse: 0.61058 |  0:00:42s
epoch 3  | loss: 0.36869 | val_0_rmse: 0.63921 | val_1_rmse: 0.64235 |  0:00:55s
epoch 4  | loss: 0.35961 | val_0_rmse: 0.59648 | val_1_rmse: 0.59364 |  0:01:09s
epoch 5  | loss: 0.34727 | val_0_rmse: 0.57948 | val_1_rmse: 0.57807 |  0:01:23s
epoch 6  | loss: 0.35028 | val_0_rmse: 0.58198 | val_1_rmse: 0.58102 |  0:01:37s
epoch 7  | loss: 0.34451 | val_0_rmse: 0.58846 | val_1_rmse: 0.58451 |  0:01:51s
epoch 8  | loss: 0.3402  | val_0_rmse: 0.5942  | val_1_rmse: 0.59274 |  0:02:05s
epoch 9  | loss: 0.34044 | val_0_rmse: 0.58465 | val_1_rmse: 0.58412 |  0:02:19s
epoch 10 | loss: 0.3396  | val_0_rmse: 0.57954 | val_1_rmse: 0.57936 |  0:02:33s
epoch 11 | loss: 0.34299 | val_0_rmse: 0.59417 | val_1_rmse: 0.59307 |  0:02:47s
epoch 12 | loss: 0.34291 | val_0_rmse: 0.5771  | val_1_rmse: 0.57341 |  0:03:00s
epoch 13 | loss: 0.33779 | val_0_rmse: 0.5807  | val_1_rmse: 0.58032 |  0:03:14s
epoch 14 | loss: 0.33753 | val_0_rmse: 0.57659 | val_1_rmse: 0.57435 |  0:03:28s
epoch 15 | loss: 0.33464 | val_0_rmse: 0.58664 | val_1_rmse: 0.58295 |  0:03:42s
epoch 16 | loss: 0.33549 | val_0_rmse: 0.57451 | val_1_rmse: 0.57317 |  0:03:56s
epoch 17 | loss: 0.33591 | val_0_rmse: 0.61221 | val_1_rmse: 0.61357 |  0:04:10s
epoch 18 | loss: 0.33433 | val_0_rmse: 0.584   | val_1_rmse: 0.5823  |  0:04:24s
epoch 19 | loss: 0.33261 | val_0_rmse: 0.57091 | val_1_rmse: 0.56943 |  0:04:38s
epoch 20 | loss: 0.33665 | val_0_rmse: 0.60994 | val_1_rmse: 0.61092 |  0:04:52s
epoch 21 | loss: 0.33774 | val_0_rmse: 0.5706  | val_1_rmse: 0.56754 |  0:05:06s
epoch 22 | loss: 0.33294 | val_0_rmse: 0.60663 | val_1_rmse: 0.60769 |  0:05:19s
epoch 23 | loss: 0.33175 | val_0_rmse: 0.6105  | val_1_rmse: 0.60632 |  0:05:33s
epoch 24 | loss: 0.33203 | val_0_rmse: 0.57718 | val_1_rmse: 0.57584 |  0:05:47s
epoch 25 | loss: 0.33275 | val_0_rmse: 0.60162 | val_1_rmse: 0.60048 |  0:06:01s
epoch 26 | loss: 0.33933 | val_0_rmse: 0.58923 | val_1_rmse: 0.58576 |  0:06:15s
epoch 27 | loss: 0.33348 | val_0_rmse: 0.56884 | val_1_rmse: 0.56663 |  0:06:29s
epoch 28 | loss: 0.34043 | val_0_rmse: 0.57982 | val_1_rmse: 0.57675 |  0:06:43s
epoch 29 | loss: 0.33366 | val_0_rmse: 0.60415 | val_1_rmse: 0.60337 |  0:06:57s
epoch 30 | loss: 0.33068 | val_0_rmse: 0.60407 | val_1_rmse: 0.60584 |  0:07:11s
epoch 31 | loss: 0.32892 | val_0_rmse: 0.59103 | val_1_rmse: 0.58807 |  0:07:25s
epoch 32 | loss: 0.34765 | val_0_rmse: 0.62204 | val_1_rmse: 0.61763 |  0:07:38s
epoch 33 | loss: 0.34448 | val_0_rmse: 0.59286 | val_1_rmse: 0.58906 |  0:07:52s
epoch 34 | loss: 0.3571  | val_0_rmse: 0.62618 | val_1_rmse: 0.62368 |  0:08:06s
epoch 35 | loss: 0.37118 | val_0_rmse: 0.59498 | val_1_rmse: 0.59197 |  0:08:20s
epoch 36 | loss: 0.35422 | val_0_rmse: 0.58015 | val_1_rmse: 0.57763 |  0:08:34s
epoch 37 | loss: 0.35253 | val_0_rmse: 0.59907 | val_1_rmse: 0.59674 |  0:08:47s
epoch 38 | loss: 0.3427  | val_0_rmse: 0.58916 | val_1_rmse: 0.58635 |  0:09:01s
epoch 39 | loss: 0.33709 | val_0_rmse: 0.58078 | val_1_rmse: 0.57783 |  0:09:15s
epoch 40 | loss: 0.33612 | val_0_rmse: 0.59514 | val_1_rmse: 0.59342 |  0:09:29s
epoch 41 | loss: 0.33479 | val_0_rmse: 0.58334 | val_1_rmse: 0.58101 |  0:09:43s
epoch 42 | loss: 0.33447 | val_0_rmse: 0.58206 | val_1_rmse: 0.58031 |  0:09:57s
epoch 43 | loss: 0.33416 | val_0_rmse: 0.5776  | val_1_rmse: 0.57647 |  0:10:10s
epoch 44 | loss: 0.33283 | val_0_rmse: 0.58907 | val_1_rmse: 0.58664 |  0:10:24s
epoch 45 | loss: 0.33056 | val_0_rmse: 0.58736 | val_1_rmse: 0.58669 |  0:10:38s
epoch 46 | loss: 0.33038 | val_0_rmse: 0.5795  | val_1_rmse: 0.57767 |  0:10:52s
epoch 47 | loss: 0.32916 | val_0_rmse: 0.59165 | val_1_rmse: 0.58945 |  0:11:06s
epoch 48 | loss: 0.3288  | val_0_rmse: 0.58119 | val_1_rmse: 0.58042 |  0:11:20s
epoch 49 | loss: 0.33002 | val_0_rmse: 0.60692 | val_1_rmse: 0.60395 |  0:11:34s
epoch 50 | loss: 0.33342 | val_0_rmse: 0.59868 | val_1_rmse: 0.59698 |  0:11:48s
epoch 51 | loss: 0.33392 | val_0_rmse: 0.60069 | val_1_rmse: 0.59919 |  0:12:01s
epoch 52 | loss: 0.34509 | val_0_rmse: 0.59227 | val_1_rmse: 0.58997 |  0:12:15s
epoch 53 | loss: 0.34124 | val_0_rmse: 0.61433 | val_1_rmse: 0.6107  |  0:12:29s
epoch 54 | loss: 0.33934 | val_0_rmse: 0.59666 | val_1_rmse: 0.59492 |  0:12:43s
epoch 55 | loss: 0.33811 | val_0_rmse: 0.58847 | val_1_rmse: 0.58658 |  0:12:57s
epoch 56 | loss: 0.34521 | val_0_rmse: 0.58118 | val_1_rmse: 0.57989 |  0:13:11s
epoch 57 | loss: 0.34817 | val_0_rmse: 0.59374 | val_1_rmse: 0.59115 |  0:13:25s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.56663
Best weights from best epoch are automatically used!
ended training at: 10:48:25
Feature importance:
[('Area', 0.10636822229890532), ('Baths', 0.2444915769186996), ('Beds', 0.08237767326102986), ('Latitude', 0.19997332548603158), ('Longitude', 0.28583250081531936), ('Month', 0.0), ('Year', 0.0809567012200143)]
Mean squared error is of 14104351461.537413
Mean absolute error:80383.36091082336
MAPE:0.5020867366765494
R2 score:0.6733913497868967
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:48:27
epoch 0  | loss: 0.43769 | val_0_rmse: 0.61107 | val_1_rmse: 0.60698 |  0:00:14s
epoch 1  | loss: 0.36893 | val_0_rmse: 0.61097 | val_1_rmse: 0.60905 |  0:00:28s
epoch 2  | loss: 0.35913 | val_0_rmse: 0.58468 | val_1_rmse: 0.58527 |  0:00:42s
epoch 3  | loss: 0.34911 | val_0_rmse: 0.58293 | val_1_rmse: 0.58267 |  0:00:55s
epoch 4  | loss: 0.35083 | val_0_rmse: 0.58094 | val_1_rmse: 0.58067 |  0:01:09s
epoch 5  | loss: 0.34616 | val_0_rmse: 0.69174 | val_1_rmse: 0.68888 |  0:01:23s
epoch 6  | loss: 0.34563 | val_0_rmse: 0.57879 | val_1_rmse: 0.57788 |  0:01:37s
epoch 7  | loss: 0.34146 | val_0_rmse: 0.66734 | val_1_rmse: 0.66321 |  0:01:51s
epoch 8  | loss: 0.34368 | val_0_rmse: 0.64284 | val_1_rmse: 0.64198 |  0:02:05s
epoch 9  | loss: 0.34969 | val_0_rmse: 0.58204 | val_1_rmse: 0.58155 |  0:02:19s
epoch 10 | loss: 0.34609 | val_0_rmse: 0.57958 | val_1_rmse: 0.58038 |  0:02:33s
epoch 11 | loss: 0.34762 | val_0_rmse: 0.58298 | val_1_rmse: 0.58311 |  0:02:47s
epoch 12 | loss: 0.34128 | val_0_rmse: 0.57383 | val_1_rmse: 0.57467 |  0:03:00s
epoch 13 | loss: 0.33703 | val_0_rmse: 0.5891  | val_1_rmse: 0.59228 |  0:03:14s
epoch 14 | loss: 0.3379  | val_0_rmse: 0.59403 | val_1_rmse: 0.59491 |  0:03:28s
epoch 15 | loss: 0.33639 | val_0_rmse: 0.58479 | val_1_rmse: 0.58547 |  0:03:42s
epoch 16 | loss: 0.33922 | val_0_rmse: 0.57374 | val_1_rmse: 0.57673 |  0:03:56s
epoch 17 | loss: 0.33585 | val_0_rmse: 0.57456 | val_1_rmse: 0.5751  |  0:04:10s
epoch 18 | loss: 0.34923 | val_0_rmse: 0.59439 | val_1_rmse: 0.5944  |  0:04:23s
epoch 19 | loss: 0.34737 | val_0_rmse: 0.5916  | val_1_rmse: 0.59169 |  0:04:37s
epoch 20 | loss: 0.33918 | val_0_rmse: 0.60992 | val_1_rmse: 0.61312 |  0:04:51s
epoch 21 | loss: 0.33542 | val_0_rmse: 0.59154 | val_1_rmse: 0.59251 |  0:05:05s
epoch 22 | loss: 0.33449 | val_0_rmse: 0.57293 | val_1_rmse: 0.57462 |  0:05:19s
epoch 23 | loss: 0.33459 | val_0_rmse: 0.58236 | val_1_rmse: 0.58438 |  0:05:33s
epoch 24 | loss: 0.33391 | val_0_rmse: 0.61295 | val_1_rmse: 0.61437 |  0:05:46s
epoch 25 | loss: 0.33471 | val_0_rmse: 0.59282 | val_1_rmse: 0.59368 |  0:06:00s
epoch 26 | loss: 0.33247 | val_0_rmse: 0.5763  | val_1_rmse: 0.57785 |  0:06:14s
epoch 27 | loss: 0.33359 | val_0_rmse: 0.57351 | val_1_rmse: 0.57464 |  0:06:28s
epoch 28 | loss: 0.33223 | val_0_rmse: 0.57264 | val_1_rmse: 0.57504 |  0:06:42s
epoch 29 | loss: 0.33115 | val_0_rmse: 0.56997 | val_1_rmse: 0.57159 |  0:06:56s
epoch 30 | loss: 0.33063 | val_0_rmse: 0.58914 | val_1_rmse: 0.5917  |  0:07:09s
epoch 31 | loss: 0.32817 | val_0_rmse: 0.57248 | val_1_rmse: 0.57408 |  0:07:23s
epoch 32 | loss: 0.32874 | val_0_rmse: 0.56856 | val_1_rmse: 0.57045 |  0:07:37s
epoch 33 | loss: 0.32891 | val_0_rmse: 0.56549 | val_1_rmse: 0.56882 |  0:07:51s
epoch 34 | loss: 0.32943 | val_0_rmse: 0.57401 | val_1_rmse: 0.57464 |  0:08:05s
epoch 35 | loss: 0.33122 | val_0_rmse: 0.61289 | val_1_rmse: 0.61085 |  0:08:19s
epoch 36 | loss: 0.32888 | val_0_rmse: 0.58827 | val_1_rmse: 0.59035 |  0:08:32s
epoch 37 | loss: 0.32782 | val_0_rmse: 0.58802 | val_1_rmse: 0.58846 |  0:08:47s
epoch 38 | loss: 0.32736 | val_0_rmse: 0.56684 | val_1_rmse: 0.56838 |  0:09:00s
epoch 39 | loss: 0.33115 | val_0_rmse: 0.59952 | val_1_rmse: 0.59874 |  0:09:14s
epoch 40 | loss: 0.33844 | val_0_rmse: 0.62336 | val_1_rmse: 0.62061 |  0:09:28s
epoch 41 | loss: 0.33222 | val_0_rmse: 0.56777 | val_1_rmse: 0.56973 |  0:09:42s
epoch 42 | loss: 0.33031 | val_0_rmse: 0.56968 | val_1_rmse: 0.57058 |  0:09:56s
epoch 43 | loss: 0.32958 | val_0_rmse: 0.57161 | val_1_rmse: 0.57426 |  0:10:10s
epoch 44 | loss: 0.32841 | val_0_rmse: 0.57358 | val_1_rmse: 0.57631 |  0:10:23s
epoch 45 | loss: 0.32847 | val_0_rmse: 0.58087 | val_1_rmse: 0.5807  |  0:10:37s
epoch 46 | loss: 0.32911 | val_0_rmse: 0.58152 | val_1_rmse: 0.58187 |  0:10:51s
epoch 47 | loss: 0.33107 | val_0_rmse: 0.56964 | val_1_rmse: 0.57415 |  0:11:05s
epoch 48 | loss: 0.33141 | val_0_rmse: 0.60457 | val_1_rmse: 0.60517 |  0:11:19s
epoch 49 | loss: 0.33035 | val_0_rmse: 0.57012 | val_1_rmse: 0.57461 |  0:11:33s
epoch 50 | loss: 0.32897 | val_0_rmse: 0.57812 | val_1_rmse: 0.58176 |  0:11:46s
epoch 51 | loss: 0.3282  | val_0_rmse: 0.56557 | val_1_rmse: 0.56842 |  0:12:00s
epoch 52 | loss: 0.32705 | val_0_rmse: 0.57077 | val_1_rmse: 0.57296 |  0:12:14s
epoch 53 | loss: 0.32621 | val_0_rmse: 0.5642  | val_1_rmse: 0.56707 |  0:12:28s
epoch 54 | loss: 0.3274  | val_0_rmse: 0.57356 | val_1_rmse: 0.57573 |  0:12:42s
epoch 55 | loss: 0.32686 | val_0_rmse: 0.57856 | val_1_rmse: 0.58077 |  0:12:55s
epoch 56 | loss: 0.32618 | val_0_rmse: 0.58782 | val_1_rmse: 0.58923 |  0:13:09s
epoch 57 | loss: 0.32593 | val_0_rmse: 0.56756 | val_1_rmse: 0.57015 |  0:13:23s
epoch 58 | loss: 0.32589 | val_0_rmse: 0.56507 | val_1_rmse: 0.56787 |  0:13:37s
epoch 59 | loss: 0.32533 | val_0_rmse: 0.57526 | val_1_rmse: 0.57717 |  0:13:51s
epoch 60 | loss: 0.32689 | val_0_rmse: 0.5804  | val_1_rmse: 0.58394 |  0:14:04s
epoch 61 | loss: 0.32739 | val_0_rmse: 0.61349 | val_1_rmse: 0.61406 |  0:14:18s
epoch 62 | loss: 0.32691 | val_0_rmse: 0.62504 | val_1_rmse: 0.62484 |  0:14:32s
epoch 63 | loss: 0.32598 | val_0_rmse: 0.56194 | val_1_rmse: 0.56515 |  0:14:46s
epoch 64 | loss: 0.32509 | val_0_rmse: 0.56464 | val_1_rmse: 0.56786 |  0:15:00s
epoch 65 | loss: 0.32464 | val_0_rmse: 0.56917 | val_1_rmse: 0.57177 |  0:15:13s
epoch 66 | loss: 0.32465 | val_0_rmse: 0.57506 | val_1_rmse: 0.57978 |  0:15:27s
epoch 67 | loss: 0.32563 | val_0_rmse: 0.60921 | val_1_rmse: 0.61167 |  0:15:41s
epoch 68 | loss: 0.32829 | val_0_rmse: 0.5638  | val_1_rmse: 0.56676 |  0:15:55s
epoch 69 | loss: 0.3239  | val_0_rmse: 0.5847  | val_1_rmse: 0.58679 |  0:16:09s
epoch 70 | loss: 0.32397 | val_0_rmse: 0.56688 | val_1_rmse: 0.56859 |  0:16:23s
epoch 71 | loss: 0.32446 | val_0_rmse: 0.56886 | val_1_rmse: 0.5709  |  0:16:37s
epoch 72 | loss: 0.32383 | val_0_rmse: 0.57115 | val_1_rmse: 0.57364 |  0:16:51s
epoch 73 | loss: 0.32229 | val_0_rmse: 0.56621 | val_1_rmse: 0.56968 |  0:17:04s
epoch 74 | loss: 0.32305 | val_0_rmse: 0.57137 | val_1_rmse: 0.57643 |  0:17:18s
epoch 75 | loss: 0.32309 | val_0_rmse: 0.58255 | val_1_rmse: 0.58596 |  0:17:32s
epoch 76 | loss: 0.32273 | val_0_rmse: 0.5616  | val_1_rmse: 0.56534 |  0:17:46s
epoch 77 | loss: 0.32295 | val_0_rmse: 0.57283 | val_1_rmse: 0.57694 |  0:18:00s
epoch 78 | loss: 0.32268 | val_0_rmse: 0.57254 | val_1_rmse: 0.57616 |  0:18:14s
epoch 79 | loss: 0.3228  | val_0_rmse: 0.57129 | val_1_rmse: 0.57425 |  0:18:28s
epoch 80 | loss: 0.32145 | val_0_rmse: 0.56245 | val_1_rmse: 0.56594 |  0:18:42s
epoch 81 | loss: 0.32219 | val_0_rmse: 0.58069 | val_1_rmse: 0.58577 |  0:18:56s
epoch 82 | loss: 0.32321 | val_0_rmse: 0.58882 | val_1_rmse: 0.59183 |  0:19:09s
epoch 83 | loss: 0.3221  | val_0_rmse: 0.56608 | val_1_rmse: 0.56945 |  0:19:23s
epoch 84 | loss: 0.3258  | val_0_rmse: 0.59354 | val_1_rmse: 0.59663 |  0:19:37s
epoch 85 | loss: 0.32911 | val_0_rmse: 0.57777 | val_1_rmse: 0.57704 |  0:19:51s
epoch 86 | loss: 0.32618 | val_0_rmse: 0.56324 | val_1_rmse: 0.56521 |  0:20:05s
epoch 87 | loss: 0.32483 | val_0_rmse: 0.56231 | val_1_rmse: 0.56477 |  0:20:19s
epoch 88 | loss: 0.32356 | val_0_rmse: 0.56302 | val_1_rmse: 0.56624 |  0:20:33s
epoch 89 | loss: 0.32864 | val_0_rmse: 0.56733 | val_1_rmse: 0.56971 |  0:20:47s
epoch 90 | loss: 0.32707 | val_0_rmse: 0.56261 | val_1_rmse: 0.56451 |  0:21:01s
epoch 91 | loss: 0.3238  | val_0_rmse: 0.56319 | val_1_rmse: 0.56695 |  0:21:14s
epoch 92 | loss: 0.32331 | val_0_rmse: 0.56365 | val_1_rmse: 0.56635 |  0:21:28s
epoch 93 | loss: 0.32117 | val_0_rmse: 0.56973 | val_1_rmse: 0.57223 |  0:21:42s
epoch 94 | loss: 0.32155 | val_0_rmse: 0.56668 | val_1_rmse: 0.57027 |  0:21:56s
epoch 95 | loss: 0.32537 | val_0_rmse: 0.56387 | val_1_rmse: 0.567   |  0:22:10s
epoch 96 | loss: 0.32261 | val_0_rmse: 0.57554 | val_1_rmse: 0.58072 |  0:22:23s
epoch 97 | loss: 0.32182 | val_0_rmse: 0.5643  | val_1_rmse: 0.56789 |  0:22:37s
epoch 98 | loss: 0.33524 | val_0_rmse: 0.5925  | val_1_rmse: 0.59395 |  0:22:51s
epoch 99 | loss: 0.33758 | val_0_rmse: 0.57128 | val_1_rmse: 0.57322 |  0:23:05s
epoch 100| loss: 0.3274  | val_0_rmse: 0.5658  | val_1_rmse: 0.56804 |  0:23:19s
epoch 101| loss: 0.3247  | val_0_rmse: 0.56561 | val_1_rmse: 0.56919 |  0:23:32s
epoch 102| loss: 0.32383 | val_0_rmse: 0.56178 | val_1_rmse: 0.56546 |  0:23:46s
epoch 103| loss: 0.32283 | val_0_rmse: 0.56552 | val_1_rmse: 0.56934 |  0:24:00s
epoch 104| loss: 0.32187 | val_0_rmse: 0.55967 | val_1_rmse: 0.56343 |  0:24:14s
epoch 105| loss: 0.32256 | val_0_rmse: 0.57523 | val_1_rmse: 0.57897 |  0:24:28s
epoch 106| loss: 0.32155 | val_0_rmse: 0.55933 | val_1_rmse: 0.56353 |  0:24:42s
epoch 107| loss: 0.3217  | val_0_rmse: 0.56647 | val_1_rmse: 0.57051 |  0:24:56s
epoch 108| loss: 0.32089 | val_0_rmse: 0.57193 | val_1_rmse: 0.57529 |  0:25:09s
epoch 109| loss: 0.32088 | val_0_rmse: 0.56667 | val_1_rmse: 0.56991 |  0:25:23s
epoch 110| loss: 0.3209  | val_0_rmse: 0.56617 | val_1_rmse: 0.56894 |  0:25:37s
epoch 111| loss: 0.31984 | val_0_rmse: 0.56227 | val_1_rmse: 0.56716 |  0:25:51s
epoch 112| loss: 0.31982 | val_0_rmse: 0.55943 | val_1_rmse: 0.56306 |  0:26:05s
epoch 113| loss: 0.31999 | val_0_rmse: 0.56168 | val_1_rmse: 0.56499 |  0:26:19s
epoch 114| loss: 0.31863 | val_0_rmse: 0.56953 | val_1_rmse: 0.57349 |  0:26:33s
epoch 115| loss: 0.31941 | val_0_rmse: 0.56126 | val_1_rmse: 0.56578 |  0:26:47s
epoch 116| loss: 0.32004 | val_0_rmse: 0.56231 | val_1_rmse: 0.56642 |  0:27:01s
epoch 117| loss: 0.31999 | val_0_rmse: 0.56939 | val_1_rmse: 0.57212 |  0:27:14s
epoch 118| loss: 0.32033 | val_0_rmse: 0.56688 | val_1_rmse: 0.57113 |  0:27:28s
epoch 119| loss: 0.32057 | val_0_rmse: 0.55775 | val_1_rmse: 0.56235 |  0:27:42s
epoch 120| loss: 0.31953 | val_0_rmse: 0.55938 | val_1_rmse: 0.56444 |  0:27:56s
epoch 121| loss: 0.3193  | val_0_rmse: 0.55945 | val_1_rmse: 0.56383 |  0:28:10s
epoch 122| loss: 0.3196  | val_0_rmse: 0.5671  | val_1_rmse: 0.57089 |  0:28:24s
epoch 123| loss: 0.3187  | val_0_rmse: 0.55988 | val_1_rmse: 0.56434 |  0:28:38s
epoch 124| loss: 0.31989 | val_0_rmse: 0.56728 | val_1_rmse: 0.57055 |  0:28:52s
epoch 125| loss: 0.31963 | val_0_rmse: 0.56338 | val_1_rmse: 0.5666  |  0:29:06s
epoch 126| loss: 0.31974 | val_0_rmse: 0.55931 | val_1_rmse: 0.56319 |  0:29:20s
epoch 127| loss: 0.31912 | val_0_rmse: 0.56225 | val_1_rmse: 0.56593 |  0:29:34s
epoch 128| loss: 0.31902 | val_0_rmse: 0.58346 | val_1_rmse: 0.58672 |  0:29:47s
epoch 129| loss: 0.31809 | val_0_rmse: 0.57267 | val_1_rmse: 0.57651 |  0:30:01s
epoch 130| loss: 0.31939 | val_0_rmse: 0.56055 | val_1_rmse: 0.5639  |  0:30:15s
epoch 131| loss: 0.31921 | val_0_rmse: 0.56984 | val_1_rmse: 0.5739  |  0:30:29s
epoch 132| loss: 0.31774 | val_0_rmse: 0.57    | val_1_rmse: 0.57601 |  0:30:43s
epoch 133| loss: 0.31826 | val_0_rmse: 0.55753 | val_1_rmse: 0.56334 |  0:30:57s
epoch 134| loss: 0.31897 | val_0_rmse: 0.56269 | val_1_rmse: 0.56742 |  0:31:11s
epoch 135| loss: 0.3184  | val_0_rmse: 0.55707 | val_1_rmse: 0.56243 |  0:31:25s
epoch 136| loss: 0.31844 | val_0_rmse: 0.58556 | val_1_rmse: 0.58831 |  0:31:39s
epoch 137| loss: 0.31763 | val_0_rmse: 0.55721 | val_1_rmse: 0.56176 |  0:31:53s
epoch 138| loss: 0.31784 | val_0_rmse: 0.56281 | val_1_rmse: 0.5674  |  0:32:07s
epoch 139| loss: 0.31805 | val_0_rmse: 0.55971 | val_1_rmse: 0.564   |  0:32:21s
epoch 140| loss: 0.31927 | val_0_rmse: 0.65638 | val_1_rmse: 0.65878 |  0:32:34s
epoch 141| loss: 0.31852 | val_0_rmse: 0.56522 | val_1_rmse: 0.57135 |  0:32:48s
epoch 142| loss: 0.31863 | val_0_rmse: 0.56793 | val_1_rmse: 0.57186 |  0:33:02s
epoch 143| loss: 0.31665 | val_0_rmse: 0.55727 | val_1_rmse: 0.56158 |  0:33:16s
epoch 144| loss: 0.31923 | val_0_rmse: 0.56731 | val_1_rmse: 0.57053 |  0:33:30s
epoch 145| loss: 0.31909 | val_0_rmse: 0.55925 | val_1_rmse: 0.56316 |  0:33:44s
epoch 146| loss: 0.31818 | val_0_rmse: 0.56089 | val_1_rmse: 0.5665  |  0:33:58s
epoch 147| loss: 0.31612 | val_0_rmse: 0.557   | val_1_rmse: 0.56259 |  0:34:11s
epoch 148| loss: 0.31538 | val_0_rmse: 0.55603 | val_1_rmse: 0.56132 |  0:34:25s
epoch 149| loss: 0.3167  | val_0_rmse: 0.55683 | val_1_rmse: 0.56297 |  0:34:39s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.56132
Best weights from best epoch are automatically used!
ended training at: 11:23:11
Feature importance:
[('Area', 0.2309506994511672), ('Baths', 0.014719436272660301), ('Beds', 0.16655708793045262), ('Latitude', 0.4116492092014877), ('Longitude', 0.17612356714423222), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 13669227256.268532
Mean absolute error:77204.99147017974
MAPE:0.446343348372287
R2 score:0.680538850974474
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:23:12
epoch 0  | loss: 0.45635 | val_0_rmse: 0.64459 | val_1_rmse: 0.645   |  0:00:13s
epoch 1  | loss: 0.38893 | val_0_rmse: 0.67211 | val_1_rmse: 0.67517 |  0:00:27s
epoch 2  | loss: 0.38194 | val_0_rmse: 0.6159  | val_1_rmse: 0.61776 |  0:00:41s
epoch 3  | loss: 0.38112 | val_0_rmse: 0.60782 | val_1_rmse: 0.60678 |  0:00:55s
epoch 4  | loss: 0.3777  | val_0_rmse: 0.61101 | val_1_rmse: 0.61167 |  0:01:09s
epoch 5  | loss: 0.37387 | val_0_rmse: 0.60177 | val_1_rmse: 0.60128 |  0:01:23s
epoch 6  | loss: 0.37122 | val_0_rmse: 0.60953 | val_1_rmse: 0.60882 |  0:01:36s
epoch 7  | loss: 0.37604 | val_0_rmse: 0.61401 | val_1_rmse: 0.61586 |  0:01:50s
epoch 8  | loss: 0.35268 | val_0_rmse: 0.58063 | val_1_rmse: 0.58159 |  0:02:05s
epoch 9  | loss: 0.34589 | val_0_rmse: 0.57673 | val_1_rmse: 0.5785  |  0:02:19s
epoch 10 | loss: 0.34298 | val_0_rmse: 0.57951 | val_1_rmse: 0.58002 |  0:02:32s
epoch 11 | loss: 0.34077 | val_0_rmse: 0.57496 | val_1_rmse: 0.57655 |  0:02:46s
epoch 12 | loss: 0.33819 | val_0_rmse: 0.87567 | val_1_rmse: 0.87787 |  0:03:00s
epoch 13 | loss: 0.33703 | val_0_rmse: 0.5803  | val_1_rmse: 0.58244 |  0:03:14s
epoch 14 | loss: 0.33703 | val_0_rmse: 0.58828 | val_1_rmse: 0.59065 |  0:03:28s
epoch 15 | loss: 0.33489 | val_0_rmse: 0.57538 | val_1_rmse: 0.57668 |  0:03:42s
epoch 16 | loss: 0.33505 | val_0_rmse: 0.57244 | val_1_rmse: 0.57422 |  0:03:55s
epoch 17 | loss: 0.33294 | val_0_rmse: 0.59258 | val_1_rmse: 0.59486 |  0:04:09s
epoch 18 | loss: 0.33356 | val_0_rmse: 0.57965 | val_1_rmse: 0.58047 |  0:04:23s
epoch 19 | loss: 0.33275 | val_0_rmse: 0.58382 | val_1_rmse: 0.58408 |  0:04:37s
epoch 20 | loss: 0.33291 | val_0_rmse: 0.57869 | val_1_rmse: 0.58109 |  0:04:51s
epoch 21 | loss: 0.33149 | val_0_rmse: 0.59895 | val_1_rmse: 0.59936 |  0:05:05s
epoch 22 | loss: 0.32902 | val_0_rmse: 0.60487 | val_1_rmse: 0.60607 |  0:05:18s
epoch 23 | loss: 0.32988 | val_0_rmse: 0.56842 | val_1_rmse: 0.57044 |  0:05:32s
epoch 24 | loss: 0.33502 | val_0_rmse: 0.59587 | val_1_rmse: 0.59571 |  0:05:46s
epoch 25 | loss: 0.33052 | val_0_rmse: 0.56912 | val_1_rmse: 0.57098 |  0:06:00s
epoch 26 | loss: 0.32952 | val_0_rmse: 0.57692 | val_1_rmse: 0.57796 |  0:06:14s
epoch 27 | loss: 0.32831 | val_0_rmse: 0.56793 | val_1_rmse: 0.56925 |  0:06:28s
epoch 28 | loss: 0.32679 | val_0_rmse: 0.5698  | val_1_rmse: 0.57103 |  0:06:42s
epoch 29 | loss: 0.32813 | val_0_rmse: 0.60889 | val_1_rmse: 0.60902 |  0:06:56s
epoch 30 | loss: 0.32665 | val_0_rmse: 0.56346 | val_1_rmse: 0.56453 |  0:07:10s
epoch 31 | loss: 0.32857 | val_0_rmse: 0.58303 | val_1_rmse: 0.58476 |  0:07:24s
epoch 32 | loss: 0.32565 | val_0_rmse: 0.57136 | val_1_rmse: 0.57265 |  0:07:38s
epoch 33 | loss: 0.32617 | val_0_rmse: 0.57092 | val_1_rmse: 0.57345 |  0:07:52s
epoch 34 | loss: 0.32753 | val_0_rmse: 0.57172 | val_1_rmse: 0.57407 |  0:08:06s
epoch 35 | loss: 0.32574 | val_0_rmse: 0.59232 | val_1_rmse: 0.5951  |  0:08:20s
epoch 36 | loss: 0.32481 | val_0_rmse: 0.58098 | val_1_rmse: 0.58369 |  0:08:34s
epoch 37 | loss: 0.32474 | val_0_rmse: 0.56919 | val_1_rmse: 0.57053 |  0:08:47s
epoch 38 | loss: 0.3238  | val_0_rmse: 0.58065 | val_1_rmse: 0.58216 |  0:09:01s
epoch 39 | loss: 0.32425 | val_0_rmse: 0.57533 | val_1_rmse: 0.57642 |  0:09:15s
epoch 40 | loss: 0.32479 | val_0_rmse: 0.56817 | val_1_rmse: 0.57098 |  0:09:29s
epoch 41 | loss: 0.32386 | val_0_rmse: 0.56518 | val_1_rmse: 0.56786 |  0:09:43s
epoch 42 | loss: 0.32662 | val_0_rmse: 0.57357 | val_1_rmse: 0.57578 |  0:09:57s
epoch 43 | loss: 0.32349 | val_0_rmse: 0.56873 | val_1_rmse: 0.5716  |  0:10:11s
epoch 44 | loss: 0.3239  | val_0_rmse: 0.56736 | val_1_rmse: 0.5696  |  0:10:25s
epoch 45 | loss: 0.32303 | val_0_rmse: 0.58395 | val_1_rmse: 0.58647 |  0:10:38s
epoch 46 | loss: 0.32309 | val_0_rmse: 0.65026 | val_1_rmse: 0.65084 |  0:10:52s
epoch 47 | loss: 0.3229  | val_0_rmse: 0.61668 | val_1_rmse: 0.6164  |  0:11:06s
epoch 48 | loss: 0.32407 | val_0_rmse: 0.574   | val_1_rmse: 0.575   |  0:11:20s
epoch 49 | loss: 0.32286 | val_0_rmse: 0.57022 | val_1_rmse: 0.57318 |  0:11:34s
epoch 50 | loss: 0.32272 | val_0_rmse: 0.56537 | val_1_rmse: 0.56839 |  0:11:48s
epoch 51 | loss: 0.3225  | val_0_rmse: 0.56619 | val_1_rmse: 0.5697  |  0:12:02s
epoch 52 | loss: 0.32246 | val_0_rmse: 0.56339 | val_1_rmse: 0.56593 |  0:12:16s
epoch 53 | loss: 0.3232  | val_0_rmse: 0.56261 | val_1_rmse: 0.56509 |  0:12:30s
epoch 54 | loss: 0.32199 | val_0_rmse: 0.60171 | val_1_rmse: 0.60413 |  0:12:43s
epoch 55 | loss: 0.3224  | val_0_rmse: 0.59229 | val_1_rmse: 0.5941  |  0:12:57s
epoch 56 | loss: 0.32179 | val_0_rmse: 0.56385 | val_1_rmse: 0.56756 |  0:13:11s
epoch 57 | loss: 0.32264 | val_0_rmse: 0.56463 | val_1_rmse: 0.56666 |  0:13:25s
epoch 58 | loss: 0.32165 | val_0_rmse: 0.5693  | val_1_rmse: 0.57191 |  0:13:39s
epoch 59 | loss: 0.32165 | val_0_rmse: 0.57059 | val_1_rmse: 0.57358 |  0:13:53s
epoch 60 | loss: 0.32126 | val_0_rmse: 0.5669  | val_1_rmse: 0.56999 |  0:14:07s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.56453
Best weights from best epoch are automatically used!
ended training at: 11:37:23
Feature importance:
[('Area', 0.21482815400896565), ('Baths', 0.049534533722585435), ('Beds', 0.1525007883059516), ('Latitude', 0.5333374811310606), ('Longitude', 0.0), ('Month', 0.016919041012594265), ('Year', 0.032880001818842465)]
Mean squared error is of 13696852085.729635
Mean absolute error:76379.2585759708
MAPE:0.4206160715108786
R2 score:0.6803151123262657
------------------------------------------------------------------
