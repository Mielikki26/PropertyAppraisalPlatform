TabNet Logs:

Saving copy of script...
In this script the datasets were geopy augmented.This is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:43:13
epoch 0  | loss: 1.20038 | val_0_rmse: 0.99428 | val_1_rmse: 0.99706 |  0:00:03s
epoch 1  | loss: 0.76153 | val_0_rmse: 0.82262 | val_1_rmse: 0.8317  |  0:00:05s
epoch 2  | loss: 0.52272 | val_0_rmse: 0.80084 | val_1_rmse: 0.80543 |  0:00:07s
epoch 3  | loss: 0.42058 | val_0_rmse: 0.8036  | val_1_rmse: 0.80557 |  0:00:09s
epoch 4  | loss: 0.38915 | val_0_rmse: 0.79163 | val_1_rmse: 0.79499 |  0:00:11s
epoch 5  | loss: 0.37401 | val_0_rmse: 0.77854 | val_1_rmse: 0.78299 |  0:00:13s
epoch 6  | loss: 0.35607 | val_0_rmse: 0.7461  | val_1_rmse: 0.74859 |  0:00:15s
epoch 7  | loss: 0.33863 | val_0_rmse: 0.74268 | val_1_rmse: 0.74453 |  0:00:17s
epoch 8  | loss: 0.31864 | val_0_rmse: 0.71865 | val_1_rmse: 0.72817 |  0:00:19s
epoch 9  | loss: 0.30837 | val_0_rmse: 0.69256 | val_1_rmse: 0.69903 |  0:00:21s
epoch 10 | loss: 0.29118 | val_0_rmse: 0.65703 | val_1_rmse: 0.66791 |  0:00:23s
epoch 11 | loss: 0.27786 | val_0_rmse: 0.65346 | val_1_rmse: 0.66767 |  0:00:25s
epoch 12 | loss: 0.26892 | val_0_rmse: 0.63507 | val_1_rmse: 0.65327 |  0:00:27s
epoch 13 | loss: 0.26614 | val_0_rmse: 0.6041  | val_1_rmse: 0.61841 |  0:00:29s
epoch 14 | loss: 0.26954 | val_0_rmse: 0.59497 | val_1_rmse: 0.61644 |  0:00:31s
epoch 15 | loss: 0.26603 | val_0_rmse: 0.58569 | val_1_rmse: 0.60297 |  0:00:33s
epoch 16 | loss: 0.25588 | val_0_rmse: 0.55735 | val_1_rmse: 0.58282 |  0:00:35s
epoch 17 | loss: 0.25345 | val_0_rmse: 0.55621 | val_1_rmse: 0.58153 |  0:00:36s
epoch 18 | loss: 0.25465 | val_0_rmse: 0.55234 | val_1_rmse: 0.57901 |  0:00:38s
epoch 19 | loss: 0.27117 | val_0_rmse: 0.56147 | val_1_rmse: 0.59282 |  0:00:40s
epoch 20 | loss: 0.26195 | val_0_rmse: 0.55439 | val_1_rmse: 0.58364 |  0:00:42s
epoch 21 | loss: 0.25161 | val_0_rmse: 0.516   | val_1_rmse: 0.54691 |  0:00:44s
epoch 22 | loss: 0.25007 | val_0_rmse: 0.54462 | val_1_rmse: 0.56609 |  0:00:46s
epoch 23 | loss: 0.26852 | val_0_rmse: 0.50159 | val_1_rmse: 0.52878 |  0:00:48s
epoch 24 | loss: 0.25925 | val_0_rmse: 0.55153 | val_1_rmse: 0.57295 |  0:00:50s
epoch 25 | loss: 0.26167 | val_0_rmse: 0.49463 | val_1_rmse: 0.53201 |  0:00:52s
epoch 26 | loss: 0.2493  | val_0_rmse: 0.47922 | val_1_rmse: 0.512   |  0:00:54s
epoch 27 | loss: 0.25067 | val_0_rmse: 0.48648 | val_1_rmse: 0.52166 |  0:00:56s
epoch 28 | loss: 0.24663 | val_0_rmse: 0.473   | val_1_rmse: 0.50702 |  0:00:58s
epoch 29 | loss: 0.24134 | val_0_rmse: 0.46998 | val_1_rmse: 0.50854 |  0:01:00s
epoch 30 | loss: 0.24465 | val_0_rmse: 0.47262 | val_1_rmse: 0.51614 |  0:01:02s
epoch 31 | loss: 0.24043 | val_0_rmse: 0.46706 | val_1_rmse: 0.51195 |  0:01:04s
epoch 32 | loss: 0.23911 | val_0_rmse: 0.46674 | val_1_rmse: 0.50477 |  0:01:06s
epoch 33 | loss: 0.23944 | val_0_rmse: 0.49431 | val_1_rmse: 0.53944 |  0:01:08s
epoch 34 | loss: 0.30337 | val_0_rmse: 0.51883 | val_1_rmse: 0.55014 |  0:01:10s
epoch 35 | loss: 0.31601 | val_0_rmse: 0.52389 | val_1_rmse: 0.55085 |  0:01:12s
epoch 36 | loss: 0.28496 | val_0_rmse: 0.49608 | val_1_rmse: 0.52819 |  0:01:14s
epoch 37 | loss: 0.27567 | val_0_rmse: 0.48434 | val_1_rmse: 0.52113 |  0:01:15s
epoch 38 | loss: 0.26352 | val_0_rmse: 0.50345 | val_1_rmse: 0.53924 |  0:01:17s
epoch 39 | loss: 0.25617 | val_0_rmse: 0.48001 | val_1_rmse: 0.51775 |  0:01:19s
epoch 40 | loss: 0.25549 | val_0_rmse: 0.48875 | val_1_rmse: 0.52704 |  0:01:21s
epoch 41 | loss: 0.24651 | val_0_rmse: 0.47431 | val_1_rmse: 0.51581 |  0:01:23s
epoch 42 | loss: 0.24154 | val_0_rmse: 0.4681  | val_1_rmse: 0.50575 |  0:01:25s
epoch 43 | loss: 0.23495 | val_0_rmse: 0.46462 | val_1_rmse: 0.50806 |  0:01:27s
epoch 44 | loss: 0.23089 | val_0_rmse: 0.46109 | val_1_rmse: 0.50826 |  0:01:29s
epoch 45 | loss: 0.23097 | val_0_rmse: 0.46269 | val_1_rmse: 0.50443 |  0:01:31s
epoch 46 | loss: 0.22733 | val_0_rmse: 0.48662 | val_1_rmse: 0.52697 |  0:01:33s
epoch 47 | loss: 0.22579 | val_0_rmse: 0.46105 | val_1_rmse: 0.50504 |  0:01:35s
epoch 48 | loss: 0.22906 | val_0_rmse: 0.45611 | val_1_rmse: 0.50051 |  0:01:37s
epoch 49 | loss: 0.223   | val_0_rmse: 0.44758 | val_1_rmse: 0.49523 |  0:01:39s
epoch 50 | loss: 0.22458 | val_0_rmse: 0.45233 | val_1_rmse: 0.50081 |  0:01:41s
epoch 51 | loss: 0.22055 | val_0_rmse: 0.45634 | val_1_rmse: 0.50548 |  0:01:43s
epoch 52 | loss: 0.22451 | val_0_rmse: 0.46575 | val_1_rmse: 0.51115 |  0:01:45s
epoch 53 | loss: 0.22013 | val_0_rmse: 0.44783 | val_1_rmse: 0.49658 |  0:01:47s
epoch 54 | loss: 0.21592 | val_0_rmse: 0.44269 | val_1_rmse: 0.50241 |  0:01:49s
epoch 55 | loss: 0.21004 | val_0_rmse: 0.44112 | val_1_rmse: 0.4935  |  0:01:50s
epoch 56 | loss: 0.21312 | val_0_rmse: 0.44699 | val_1_rmse: 0.49997 |  0:01:52s
epoch 57 | loss: 0.21548 | val_0_rmse: 0.44444 | val_1_rmse: 0.50348 |  0:01:54s
epoch 58 | loss: 0.21733 | val_0_rmse: 0.45382 | val_1_rmse: 0.51695 |  0:01:56s
epoch 59 | loss: 0.21426 | val_0_rmse: 0.44388 | val_1_rmse: 0.50284 |  0:01:58s
epoch 60 | loss: 0.21528 | val_0_rmse: 0.44614 | val_1_rmse: 0.50343 |  0:02:00s
epoch 61 | loss: 0.21518 | val_0_rmse: 0.43669 | val_1_rmse: 0.50093 |  0:02:02s
epoch 62 | loss: 0.20966 | val_0_rmse: 0.43538 | val_1_rmse: 0.50267 |  0:02:04s
epoch 63 | loss: 0.21077 | val_0_rmse: 0.43476 | val_1_rmse: 0.49902 |  0:02:06s
epoch 64 | loss: 0.20644 | val_0_rmse: 0.43145 | val_1_rmse: 0.50169 |  0:02:08s
epoch 65 | loss: 0.21141 | val_0_rmse: 0.4348  | val_1_rmse: 0.50837 |  0:02:10s
epoch 66 | loss: 0.20856 | val_0_rmse: 0.44777 | val_1_rmse: 0.50908 |  0:02:12s
epoch 67 | loss: 0.20877 | val_0_rmse: 0.44441 | val_1_rmse: 0.51141 |  0:02:14s
epoch 68 | loss: 0.20806 | val_0_rmse: 0.42791 | val_1_rmse: 0.50025 |  0:02:16s
epoch 69 | loss: 0.202   | val_0_rmse: 0.42502 | val_1_rmse: 0.50233 |  0:02:18s
epoch 70 | loss: 0.20581 | val_0_rmse: 0.43073 | val_1_rmse: 0.50374 |  0:02:19s
epoch 71 | loss: 0.19964 | val_0_rmse: 0.42515 | val_1_rmse: 0.50579 |  0:02:21s
epoch 72 | loss: 0.20485 | val_0_rmse: 0.44327 | val_1_rmse: 0.51997 |  0:02:23s
epoch 73 | loss: 0.20222 | val_0_rmse: 0.42343 | val_1_rmse: 0.50158 |  0:02:25s
epoch 74 | loss: 0.19385 | val_0_rmse: 0.41138 | val_1_rmse: 0.50252 |  0:02:27s
epoch 75 | loss: 0.19336 | val_0_rmse: 0.41144 | val_1_rmse: 0.50453 |  0:02:29s
epoch 76 | loss: 0.19391 | val_0_rmse: 0.41897 | val_1_rmse: 0.50312 |  0:02:31s
epoch 77 | loss: 0.19047 | val_0_rmse: 0.41495 | val_1_rmse: 0.50811 |  0:02:33s
epoch 78 | loss: 0.18854 | val_0_rmse: 0.40822 | val_1_rmse: 0.49962 |  0:02:35s
epoch 79 | loss: 0.18623 | val_0_rmse: 0.40469 | val_1_rmse: 0.50361 |  0:02:37s
epoch 80 | loss: 0.18408 | val_0_rmse: 0.40582 | val_1_rmse: 0.50866 |  0:02:39s
epoch 81 | loss: 0.18802 | val_0_rmse: 0.41162 | val_1_rmse: 0.51141 |  0:02:41s
epoch 82 | loss: 0.1861  | val_0_rmse: 0.41695 | val_1_rmse: 0.51228 |  0:02:43s
epoch 83 | loss: 0.18561 | val_0_rmse: 0.40456 | val_1_rmse: 0.50485 |  0:02:45s
epoch 84 | loss: 0.18157 | val_0_rmse: 0.40436 | val_1_rmse: 0.50155 |  0:02:47s
epoch 85 | loss: 0.18075 | val_0_rmse: 0.40262 | val_1_rmse: 0.50432 |  0:02:49s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.4935
Best weights from best epoch are automatically used!
ended training at: 14:46:03
Feature importance:
Mean squared error is of 5421054275.015081
Mean absolute error:50057.043288573026
MAPE:0.16610884501695586
R2 score:0.7619054484544803
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:46:04
epoch 0  | loss: 1.22519 | val_0_rmse: 0.93772 | val_1_rmse: 0.93375 |  0:00:01s
epoch 1  | loss: 0.75529 | val_0_rmse: 0.85714 | val_1_rmse: 0.84679 |  0:00:03s
epoch 2  | loss: 0.62749 | val_0_rmse: 0.81583 | val_1_rmse: 0.8111  |  0:00:05s
epoch 3  | loss: 0.52    | val_0_rmse: 0.80278 | val_1_rmse: 0.80181 |  0:00:07s
epoch 4  | loss: 0.45552 | val_0_rmse: 0.81393 | val_1_rmse: 0.8106  |  0:00:09s
epoch 5  | loss: 0.419   | val_0_rmse: 0.81757 | val_1_rmse: 0.81514 |  0:00:11s
epoch 6  | loss: 0.40111 | val_0_rmse: 0.80528 | val_1_rmse: 0.80091 |  0:00:13s
epoch 7  | loss: 0.36797 | val_0_rmse: 0.77305 | val_1_rmse: 0.77217 |  0:00:15s
epoch 8  | loss: 0.35652 | val_0_rmse: 0.75825 | val_1_rmse: 0.7609  |  0:00:17s
epoch 9  | loss: 0.34959 | val_0_rmse: 0.76306 | val_1_rmse: 0.76529 |  0:00:19s
epoch 10 | loss: 0.33168 | val_0_rmse: 0.74036 | val_1_rmse: 0.74427 |  0:00:21s
epoch 11 | loss: 0.32632 | val_0_rmse: 0.70879 | val_1_rmse: 0.71362 |  0:00:23s
epoch 12 | loss: 0.32059 | val_0_rmse: 0.69355 | val_1_rmse: 0.69757 |  0:00:25s
epoch 13 | loss: 0.31572 | val_0_rmse: 0.65967 | val_1_rmse: 0.66879 |  0:00:27s
epoch 14 | loss: 0.30641 | val_0_rmse: 0.64685 | val_1_rmse: 0.6557  |  0:00:28s
epoch 15 | loss: 0.29142 | val_0_rmse: 0.6292  | val_1_rmse: 0.63743 |  0:00:30s
epoch 16 | loss: 0.29051 | val_0_rmse: 0.58663 | val_1_rmse: 0.5978  |  0:00:32s
epoch 17 | loss: 0.28772 | val_0_rmse: 0.56938 | val_1_rmse: 0.57898 |  0:00:34s
epoch 18 | loss: 0.28921 | val_0_rmse: 0.57145 | val_1_rmse: 0.58436 |  0:00:36s
epoch 19 | loss: 0.2803  | val_0_rmse: 0.58121 | val_1_rmse: 0.59018 |  0:00:38s
epoch 20 | loss: 0.29041 | val_0_rmse: 0.55173 | val_1_rmse: 0.55707 |  0:00:40s
epoch 21 | loss: 0.28327 | val_0_rmse: 0.53274 | val_1_rmse: 0.53938 |  0:00:42s
epoch 22 | loss: 0.27792 | val_0_rmse: 0.5326  | val_1_rmse: 0.53775 |  0:00:44s
epoch 23 | loss: 0.27576 | val_0_rmse: 0.51087 | val_1_rmse: 0.521   |  0:00:46s
epoch 24 | loss: 0.26667 | val_0_rmse: 0.51051 | val_1_rmse: 0.52137 |  0:00:48s
epoch 25 | loss: 0.26423 | val_0_rmse: 0.4916  | val_1_rmse: 0.5082  |  0:00:50s
epoch 26 | loss: 0.25914 | val_0_rmse: 0.49601 | val_1_rmse: 0.51419 |  0:00:52s
epoch 27 | loss: 0.26036 | val_0_rmse: 0.49529 | val_1_rmse: 0.50756 |  0:00:54s
epoch 28 | loss: 0.25548 | val_0_rmse: 0.48485 | val_1_rmse: 0.5035  |  0:00:56s
epoch 29 | loss: 0.25469 | val_0_rmse: 0.48555 | val_1_rmse: 0.50531 |  0:00:57s
epoch 30 | loss: 0.25296 | val_0_rmse: 0.47882 | val_1_rmse: 0.50087 |  0:00:59s
epoch 31 | loss: 0.24898 | val_0_rmse: 0.47911 | val_1_rmse: 0.50195 |  0:01:01s
epoch 32 | loss: 0.24649 | val_0_rmse: 0.47866 | val_1_rmse: 0.50631 |  0:01:03s
epoch 33 | loss: 0.2447  | val_0_rmse: 0.48277 | val_1_rmse: 0.50112 |  0:01:05s
epoch 34 | loss: 0.24417 | val_0_rmse: 0.47119 | val_1_rmse: 0.49528 |  0:01:07s
epoch 35 | loss: 0.24416 | val_0_rmse: 0.47395 | val_1_rmse: 0.50099 |  0:01:09s
epoch 36 | loss: 0.24096 | val_0_rmse: 0.47047 | val_1_rmse: 0.49413 |  0:01:11s
epoch 37 | loss: 0.23646 | val_0_rmse: 0.4657  | val_1_rmse: 0.49417 |  0:01:13s
epoch 38 | loss: 0.23369 | val_0_rmse: 0.4643  | val_1_rmse: 0.49401 |  0:01:15s
epoch 39 | loss: 0.23019 | val_0_rmse: 0.46729 | val_1_rmse: 0.5015  |  0:01:17s
epoch 40 | loss: 0.22891 | val_0_rmse: 0.45961 | val_1_rmse: 0.50169 |  0:01:19s
epoch 41 | loss: 0.22894 | val_0_rmse: 0.45778 | val_1_rmse: 0.49228 |  0:01:21s
epoch 42 | loss: 0.22763 | val_0_rmse: 0.45807 | val_1_rmse: 0.49512 |  0:01:23s
epoch 43 | loss: 0.22924 | val_0_rmse: 0.45834 | val_1_rmse: 0.50772 |  0:01:24s
epoch 44 | loss: 0.22708 | val_0_rmse: 0.45596 | val_1_rmse: 0.49929 |  0:01:26s
epoch 45 | loss: 0.23048 | val_0_rmse: 0.46201 | val_1_rmse: 0.49688 |  0:01:28s
epoch 46 | loss: 0.23149 | val_0_rmse: 0.47433 | val_1_rmse: 0.51468 |  0:01:30s
epoch 47 | loss: 0.23082 | val_0_rmse: 0.45849 | val_1_rmse: 0.49916 |  0:01:32s
epoch 48 | loss: 0.22705 | val_0_rmse: 0.4601  | val_1_rmse: 0.50313 |  0:01:34s
epoch 49 | loss: 0.23106 | val_0_rmse: 0.46938 | val_1_rmse: 0.5118  |  0:01:36s
epoch 50 | loss: 0.23491 | val_0_rmse: 0.4651  | val_1_rmse: 0.50203 |  0:01:38s
epoch 51 | loss: 0.22997 | val_0_rmse: 0.45454 | val_1_rmse: 0.50106 |  0:01:40s
epoch 52 | loss: 0.22642 | val_0_rmse: 0.44943 | val_1_rmse: 0.49728 |  0:01:42s
epoch 53 | loss: 0.22037 | val_0_rmse: 0.45427 | val_1_rmse: 0.50344 |  0:01:44s
epoch 54 | loss: 0.23242 | val_0_rmse: 0.47367 | val_1_rmse: 0.51642 |  0:01:46s
epoch 55 | loss: 0.23377 | val_0_rmse: 0.46856 | val_1_rmse: 0.50131 |  0:01:48s
epoch 56 | loss: 0.23187 | val_0_rmse: 0.46251 | val_1_rmse: 0.49837 |  0:01:49s
epoch 57 | loss: 0.22355 | val_0_rmse: 0.45913 | val_1_rmse: 0.49538 |  0:01:51s
epoch 58 | loss: 0.22223 | val_0_rmse: 0.45392 | val_1_rmse: 0.50342 |  0:01:53s
epoch 59 | loss: 0.22084 | val_0_rmse: 0.44919 | val_1_rmse: 0.50336 |  0:01:55s
epoch 60 | loss: 0.22179 | val_0_rmse: 0.44631 | val_1_rmse: 0.49587 |  0:01:57s
epoch 61 | loss: 0.21965 | val_0_rmse: 0.45178 | val_1_rmse: 0.5025  |  0:01:59s
epoch 62 | loss: 0.21719 | val_0_rmse: 0.44832 | val_1_rmse: 0.50201 |  0:02:01s
epoch 63 | loss: 0.21333 | val_0_rmse: 0.45366 | val_1_rmse: 0.50555 |  0:02:03s
epoch 64 | loss: 0.21742 | val_0_rmse: 0.44591 | val_1_rmse: 0.49341 |  0:02:05s
epoch 65 | loss: 0.20932 | val_0_rmse: 0.43944 | val_1_rmse: 0.49825 |  0:02:07s
epoch 66 | loss: 0.21403 | val_0_rmse: 0.44444 | val_1_rmse: 0.49972 |  0:02:09s
epoch 67 | loss: 0.21394 | val_0_rmse: 0.44137 | val_1_rmse: 0.49934 |  0:02:11s
epoch 68 | loss: 0.21014 | val_0_rmse: 0.4372  | val_1_rmse: 0.49884 |  0:02:13s
epoch 69 | loss: 0.21354 | val_0_rmse: 0.46604 | val_1_rmse: 0.52008 |  0:02:15s
epoch 70 | loss: 0.20987 | val_0_rmse: 0.44015 | val_1_rmse: 0.48836 |  0:02:16s
epoch 71 | loss: 0.20914 | val_0_rmse: 0.43564 | val_1_rmse: 0.49844 |  0:02:18s
epoch 72 | loss: 0.20528 | val_0_rmse: 0.43071 | val_1_rmse: 0.49328 |  0:02:20s
epoch 73 | loss: 0.20246 | val_0_rmse: 0.43324 | val_1_rmse: 0.49848 |  0:02:22s
epoch 74 | loss: 0.20944 | val_0_rmse: 0.43717 | val_1_rmse: 0.50673 |  0:02:24s
epoch 75 | loss: 0.20053 | val_0_rmse: 0.42713 | val_1_rmse: 0.49383 |  0:02:26s
epoch 76 | loss: 0.20246 | val_0_rmse: 0.43314 | val_1_rmse: 0.49856 |  0:02:28s
epoch 77 | loss: 0.20247 | val_0_rmse: 0.43221 | val_1_rmse: 0.50337 |  0:02:30s
epoch 78 | loss: 0.20142 | val_0_rmse: 0.42721 | val_1_rmse: 0.49349 |  0:02:32s
epoch 79 | loss: 0.20089 | val_0_rmse: 0.4249  | val_1_rmse: 0.50014 |  0:02:34s
epoch 80 | loss: 0.19538 | val_0_rmse: 0.42694 | val_1_rmse: 0.50395 |  0:02:36s
epoch 81 | loss: 0.19719 | val_0_rmse: 0.42501 | val_1_rmse: 0.50307 |  0:02:38s
epoch 82 | loss: 0.2053  | val_0_rmse: 0.42827 | val_1_rmse: 0.49761 |  0:02:40s
epoch 83 | loss: 0.19762 | val_0_rmse: 0.42931 | val_1_rmse: 0.4981  |  0:02:41s
epoch 84 | loss: 0.19471 | val_0_rmse: 0.4252  | val_1_rmse: 0.50236 |  0:02:43s
epoch 85 | loss: 0.19602 | val_0_rmse: 0.43713 | val_1_rmse: 0.50073 |  0:02:45s
epoch 86 | loss: 0.19292 | val_0_rmse: 0.42971 | val_1_rmse: 0.50331 |  0:02:47s
epoch 87 | loss: 0.19998 | val_0_rmse: 0.42047 | val_1_rmse: 0.50013 |  0:02:49s
epoch 88 | loss: 0.19226 | val_0_rmse: 0.41404 | val_1_rmse: 0.49797 |  0:02:51s
epoch 89 | loss: 0.18816 | val_0_rmse: 0.41296 | val_1_rmse: 0.50239 |  0:02:53s
epoch 90 | loss: 0.19389 | val_0_rmse: 0.42785 | val_1_rmse: 0.5011  |  0:02:55s
epoch 91 | loss: 0.20389 | val_0_rmse: 0.423   | val_1_rmse: 0.50175 |  0:02:57s
epoch 92 | loss: 0.1979  | val_0_rmse: 0.41353 | val_1_rmse: 0.49825 |  0:02:59s
epoch 93 | loss: 0.19081 | val_0_rmse: 0.43309 | val_1_rmse: 0.51396 |  0:03:01s
epoch 94 | loss: 0.19033 | val_0_rmse: 0.41297 | val_1_rmse: 0.50144 |  0:03:03s
epoch 95 | loss: 0.18894 | val_0_rmse: 0.42532 | val_1_rmse: 0.50179 |  0:03:05s
epoch 96 | loss: 0.18724 | val_0_rmse: 0.41358 | val_1_rmse: 0.4996  |  0:03:07s
epoch 97 | loss: 0.18388 | val_0_rmse: 0.40531 | val_1_rmse: 0.49862 |  0:03:08s
epoch 98 | loss: 0.18494 | val_0_rmse: 0.42045 | val_1_rmse: 0.51026 |  0:03:10s
epoch 99 | loss: 0.18432 | val_0_rmse: 0.40828 | val_1_rmse: 0.50229 |  0:03:12s
epoch 100| loss: 0.18193 | val_0_rmse: 0.40534 | val_1_rmse: 0.49655 |  0:03:14s

Early stopping occured at epoch 100 with best_epoch = 70 and best_val_1_rmse = 0.48836
Best weights from best epoch are automatically used!
ended training at: 14:49:19
Feature importance:
Mean squared error is of 5479156784.792504
Mean absolute error:49718.34110594078
MAPE:0.16128684770797078
R2 score:0.7592426646563177
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:49:19
epoch 0  | loss: 1.22728 | val_0_rmse: 0.97779 | val_1_rmse: 0.97073 |  0:00:01s
epoch 1  | loss: 0.76085 | val_0_rmse: 0.8758  | val_1_rmse: 0.87838 |  0:00:03s
epoch 2  | loss: 0.60472 | val_0_rmse: 0.81554 | val_1_rmse: 0.82633 |  0:00:05s
epoch 3  | loss: 0.46963 | val_0_rmse: 0.79445 | val_1_rmse: 0.80187 |  0:00:07s
epoch 4  | loss: 0.41772 | val_0_rmse: 0.79385 | val_1_rmse: 0.79802 |  0:00:09s
epoch 5  | loss: 0.37367 | val_0_rmse: 0.75017 | val_1_rmse: 0.75864 |  0:00:11s
epoch 6  | loss: 0.36161 | val_0_rmse: 0.72351 | val_1_rmse: 0.73081 |  0:00:13s
epoch 7  | loss: 0.33037 | val_0_rmse: 0.73225 | val_1_rmse: 0.73607 |  0:00:15s
epoch 8  | loss: 0.31337 | val_0_rmse: 0.70066 | val_1_rmse: 0.71008 |  0:00:17s
epoch 9  | loss: 0.30554 | val_0_rmse: 0.67402 | val_1_rmse: 0.68457 |  0:00:19s
epoch 10 | loss: 0.30037 | val_0_rmse: 0.65342 | val_1_rmse: 0.66042 |  0:00:21s
epoch 11 | loss: 0.2998  | val_0_rmse: 0.67703 | val_1_rmse: 0.68003 |  0:00:23s
epoch 12 | loss: 0.29511 | val_0_rmse: 0.63454 | val_1_rmse: 0.64751 |  0:00:25s
epoch 13 | loss: 0.27637 | val_0_rmse: 0.59809 | val_1_rmse: 0.61471 |  0:00:27s
epoch 14 | loss: 0.26987 | val_0_rmse: 0.59642 | val_1_rmse: 0.61328 |  0:00:29s
epoch 15 | loss: 0.25798 | val_0_rmse: 0.58317 | val_1_rmse: 0.59952 |  0:00:30s
epoch 16 | loss: 0.27403 | val_0_rmse: 0.60529 | val_1_rmse: 0.62236 |  0:00:32s
epoch 17 | loss: 0.29555 | val_0_rmse: 0.58091 | val_1_rmse: 0.60285 |  0:00:34s
epoch 18 | loss: 0.27933 | val_0_rmse: 0.56477 | val_1_rmse: 0.57876 |  0:00:36s
epoch 19 | loss: 0.26685 | val_0_rmse: 0.53955 | val_1_rmse: 0.55609 |  0:00:38s
epoch 20 | loss: 0.26208 | val_0_rmse: 0.52868 | val_1_rmse: 0.54531 |  0:00:40s
epoch 21 | loss: 0.25241 | val_0_rmse: 0.51199 | val_1_rmse: 0.53218 |  0:00:42s
epoch 22 | loss: 0.2475  | val_0_rmse: 0.50237 | val_1_rmse: 0.52495 |  0:00:44s
epoch 23 | loss: 0.24634 | val_0_rmse: 0.49672 | val_1_rmse: 0.51802 |  0:00:46s
epoch 24 | loss: 0.24876 | val_0_rmse: 0.49814 | val_1_rmse: 0.52128 |  0:00:48s
epoch 25 | loss: 0.24235 | val_0_rmse: 0.48149 | val_1_rmse: 0.50953 |  0:00:50s
epoch 26 | loss: 0.2339  | val_0_rmse: 0.48483 | val_1_rmse: 0.51351 |  0:00:52s
epoch 27 | loss: 0.23432 | val_0_rmse: 0.47472 | val_1_rmse: 0.50046 |  0:00:54s
epoch 28 | loss: 0.23358 | val_0_rmse: 0.46349 | val_1_rmse: 0.50134 |  0:00:56s
epoch 29 | loss: 0.23398 | val_0_rmse: 0.45992 | val_1_rmse: 0.49843 |  0:00:58s
epoch 30 | loss: 0.2303  | val_0_rmse: 0.45761 | val_1_rmse: 0.49304 |  0:00:59s
epoch 31 | loss: 0.22745 | val_0_rmse: 0.45256 | val_1_rmse: 0.49241 |  0:01:01s
epoch 32 | loss: 0.22323 | val_0_rmse: 0.45123 | val_1_rmse: 0.49012 |  0:01:03s
epoch 33 | loss: 0.228   | val_0_rmse: 0.45631 | val_1_rmse: 0.50361 |  0:01:05s
epoch 34 | loss: 0.22687 | val_0_rmse: 0.45316 | val_1_rmse: 0.49632 |  0:01:07s
epoch 35 | loss: 0.22233 | val_0_rmse: 0.44234 | val_1_rmse: 0.49003 |  0:01:09s
epoch 36 | loss: 0.21869 | val_0_rmse: 0.44881 | val_1_rmse: 0.49085 |  0:01:11s
epoch 37 | loss: 0.22062 | val_0_rmse: 0.46361 | val_1_rmse: 0.51017 |  0:01:13s
epoch 38 | loss: 0.2176  | val_0_rmse: 0.44501 | val_1_rmse: 0.49685 |  0:01:15s
epoch 39 | loss: 0.21936 | val_0_rmse: 0.44352 | val_1_rmse: 0.49519 |  0:01:17s
epoch 40 | loss: 0.21841 | val_0_rmse: 0.44867 | val_1_rmse: 0.50526 |  0:01:19s
epoch 41 | loss: 0.21192 | val_0_rmse: 0.45849 | val_1_rmse: 0.51092 |  0:01:21s
epoch 42 | loss: 0.21354 | val_0_rmse: 0.4371  | val_1_rmse: 0.49184 |  0:01:23s
epoch 43 | loss: 0.21248 | val_0_rmse: 0.43415 | val_1_rmse: 0.49171 |  0:01:25s
epoch 44 | loss: 0.20787 | val_0_rmse: 0.4322  | val_1_rmse: 0.49233 |  0:01:26s
epoch 45 | loss: 0.2052  | val_0_rmse: 0.43248 | val_1_rmse: 0.49525 |  0:01:28s
epoch 46 | loss: 0.20746 | val_0_rmse: 0.44871 | val_1_rmse: 0.51452 |  0:01:30s
epoch 47 | loss: 0.20652 | val_0_rmse: 0.4355  | val_1_rmse: 0.50237 |  0:01:32s
epoch 48 | loss: 0.20343 | val_0_rmse: 0.43029 | val_1_rmse: 0.5021  |  0:01:34s
epoch 49 | loss: 0.20635 | val_0_rmse: 0.43181 | val_1_rmse: 0.50402 |  0:01:36s
epoch 50 | loss: 0.19877 | val_0_rmse: 0.42574 | val_1_rmse: 0.49795 |  0:01:38s
epoch 51 | loss: 0.19995 | val_0_rmse: 0.43783 | val_1_rmse: 0.50233 |  0:01:40s
epoch 52 | loss: 0.19915 | val_0_rmse: 0.44052 | val_1_rmse: 0.50368 |  0:01:42s
epoch 53 | loss: 0.20097 | val_0_rmse: 0.4225  | val_1_rmse: 0.49484 |  0:01:44s
epoch 54 | loss: 0.19609 | val_0_rmse: 0.42645 | val_1_rmse: 0.50445 |  0:01:46s
epoch 55 | loss: 0.19596 | val_0_rmse: 0.42667 | val_1_rmse: 0.51091 |  0:01:48s
epoch 56 | loss: 0.19639 | val_0_rmse: 0.41815 | val_1_rmse: 0.49345 |  0:01:50s
epoch 57 | loss: 0.19506 | val_0_rmse: 0.42037 | val_1_rmse: 0.49968 |  0:01:52s
epoch 58 | loss: 0.1965  | val_0_rmse: 0.41858 | val_1_rmse: 0.49736 |  0:01:54s
epoch 59 | loss: 0.19343 | val_0_rmse: 0.41544 | val_1_rmse: 0.50073 |  0:01:56s
epoch 60 | loss: 0.19396 | val_0_rmse: 0.41752 | val_1_rmse: 0.50292 |  0:01:58s
epoch 61 | loss: 0.19174 | val_0_rmse: 0.41174 | val_1_rmse: 0.49409 |  0:02:00s
epoch 62 | loss: 0.18818 | val_0_rmse: 0.41565 | val_1_rmse: 0.50425 |  0:02:02s
epoch 63 | loss: 0.1901  | val_0_rmse: 0.41166 | val_1_rmse: 0.50226 |  0:02:03s
epoch 64 | loss: 0.18645 | val_0_rmse: 0.40933 | val_1_rmse: 0.50185 |  0:02:05s
epoch 65 | loss: 0.18641 | val_0_rmse: 0.42403 | val_1_rmse: 0.50271 |  0:02:07s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.49003
Best weights from best epoch are automatically used!
ended training at: 14:51:28
Feature importance:
Mean squared error is of 5278619492.444138
Mean absolute error:48960.19568541342
MAPE:0.1587058094840937
R2 score:0.7609609503976699
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:51:28
epoch 0  | loss: 1.21791 | val_0_rmse: 0.99819 | val_1_rmse: 0.99355 |  0:00:01s
epoch 1  | loss: 0.94538 | val_0_rmse: 1.05796 | val_1_rmse: 1.05644 |  0:00:03s
epoch 2  | loss: 0.76149 | val_0_rmse: 0.92689 | val_1_rmse: 0.93414 |  0:00:05s
epoch 3  | loss: 0.54051 | val_0_rmse: 0.81072 | val_1_rmse: 0.8177  |  0:00:07s
epoch 4  | loss: 0.42616 | val_0_rmse: 0.77528 | val_1_rmse: 0.78556 |  0:00:09s
epoch 5  | loss: 0.37146 | val_0_rmse: 0.75985 | val_1_rmse: 0.77195 |  0:00:11s
epoch 6  | loss: 0.33011 | val_0_rmse: 0.74845 | val_1_rmse: 0.75797 |  0:00:13s
epoch 7  | loss: 0.31105 | val_0_rmse: 0.72669 | val_1_rmse: 0.73735 |  0:00:15s
epoch 8  | loss: 0.30688 | val_0_rmse: 0.71266 | val_1_rmse: 0.72542 |  0:00:17s
epoch 9  | loss: 0.29156 | val_0_rmse: 0.69878 | val_1_rmse: 0.7112  |  0:00:19s
epoch 10 | loss: 0.28083 | val_0_rmse: 0.68713 | val_1_rmse: 0.69944 |  0:00:21s
epoch 11 | loss: 0.27208 | val_0_rmse: 0.70699 | val_1_rmse: 0.72209 |  0:00:23s
epoch 12 | loss: 0.27986 | val_0_rmse: 0.67312 | val_1_rmse: 0.68336 |  0:00:25s
epoch 13 | loss: 0.26851 | val_0_rmse: 0.61608 | val_1_rmse: 0.62902 |  0:00:27s
epoch 14 | loss: 0.25411 | val_0_rmse: 0.60587 | val_1_rmse: 0.61538 |  0:00:29s
epoch 15 | loss: 0.24866 | val_0_rmse: 0.57495 | val_1_rmse: 0.58521 |  0:00:30s
epoch 16 | loss: 0.24135 | val_0_rmse: 0.58572 | val_1_rmse: 0.59751 |  0:00:32s
epoch 17 | loss: 0.24043 | val_0_rmse: 0.58748 | val_1_rmse: 0.60036 |  0:00:34s
epoch 18 | loss: 0.23541 | val_0_rmse: 0.55868 | val_1_rmse: 0.57235 |  0:00:36s
epoch 19 | loss: 0.23392 | val_0_rmse: 0.53486 | val_1_rmse: 0.5505  |  0:00:38s
epoch 20 | loss: 0.23247 | val_0_rmse: 0.52526 | val_1_rmse: 0.54318 |  0:00:40s
epoch 21 | loss: 0.22679 | val_0_rmse: 0.52044 | val_1_rmse: 0.53953 |  0:00:42s
epoch 22 | loss: 0.23969 | val_0_rmse: 0.48893 | val_1_rmse: 0.51752 |  0:00:44s
epoch 23 | loss: 0.23123 | val_0_rmse: 0.51437 | val_1_rmse: 0.53642 |  0:00:46s
epoch 24 | loss: 0.23234 | val_0_rmse: 0.48199 | val_1_rmse: 0.50671 |  0:00:48s
epoch 25 | loss: 0.22612 | val_0_rmse: 0.46081 | val_1_rmse: 0.49565 |  0:00:50s
epoch 26 | loss: 0.22673 | val_0_rmse: 0.46011 | val_1_rmse: 0.49206 |  0:00:52s
epoch 27 | loss: 0.22458 | val_0_rmse: 0.45776 | val_1_rmse: 0.4909  |  0:00:54s
epoch 28 | loss: 0.2197  | val_0_rmse: 0.46656 | val_1_rmse: 0.49884 |  0:00:56s
epoch 29 | loss: 0.2206  | val_0_rmse: 0.45468 | val_1_rmse: 0.4941  |  0:00:58s
epoch 30 | loss: 0.21737 | val_0_rmse: 0.44348 | val_1_rmse: 0.48872 |  0:00:59s
epoch 31 | loss: 0.21461 | val_0_rmse: 0.44119 | val_1_rmse: 0.48536 |  0:01:01s
epoch 32 | loss: 0.21665 | val_0_rmse: 0.44681 | val_1_rmse: 0.49601 |  0:01:03s
epoch 33 | loss: 0.21511 | val_0_rmse: 0.44376 | val_1_rmse: 0.49567 |  0:01:05s
epoch 34 | loss: 0.21143 | val_0_rmse: 0.43947 | val_1_rmse: 0.48875 |  0:01:07s
epoch 35 | loss: 0.21086 | val_0_rmse: 0.42924 | val_1_rmse: 0.4804  |  0:01:09s
epoch 36 | loss: 0.20801 | val_0_rmse: 0.42893 | val_1_rmse: 0.48214 |  0:01:11s
epoch 37 | loss: 0.20673 | val_0_rmse: 0.43808 | val_1_rmse: 0.49431 |  0:01:13s
epoch 38 | loss: 0.20822 | val_0_rmse: 0.44334 | val_1_rmse: 0.49512 |  0:01:15s
epoch 39 | loss: 0.20999 | val_0_rmse: 0.44389 | val_1_rmse: 0.49299 |  0:01:17s
epoch 40 | loss: 0.20499 | val_0_rmse: 0.43274 | val_1_rmse: 0.48381 |  0:01:19s
epoch 41 | loss: 0.20405 | val_0_rmse: 0.43134 | val_1_rmse: 0.49194 |  0:01:21s
epoch 42 | loss: 0.20349 | val_0_rmse: 0.42524 | val_1_rmse: 0.48669 |  0:01:23s
epoch 43 | loss: 0.20122 | val_0_rmse: 0.42687 | val_1_rmse: 0.4834  |  0:01:25s
epoch 44 | loss: 0.20184 | val_0_rmse: 0.42367 | val_1_rmse: 0.4848  |  0:01:26s
epoch 45 | loss: 0.19659 | val_0_rmse: 0.42169 | val_1_rmse: 0.48523 |  0:01:28s
epoch 46 | loss: 0.20287 | val_0_rmse: 0.43049 | val_1_rmse: 0.49303 |  0:01:30s
epoch 47 | loss: 0.20056 | val_0_rmse: 0.42143 | val_1_rmse: 0.48279 |  0:01:32s
epoch 48 | loss: 0.1995  | val_0_rmse: 0.42167 | val_1_rmse: 0.4907  |  0:01:34s
epoch 49 | loss: 0.1984  | val_0_rmse: 0.41779 | val_1_rmse: 0.48062 |  0:01:36s
epoch 50 | loss: 0.19536 | val_0_rmse: 0.42477 | val_1_rmse: 0.48956 |  0:01:38s
epoch 51 | loss: 0.19589 | val_0_rmse: 0.41992 | val_1_rmse: 0.49695 |  0:01:40s
epoch 52 | loss: 0.19402 | val_0_rmse: 0.41351 | val_1_rmse: 0.48243 |  0:01:42s
epoch 53 | loss: 0.1951  | val_0_rmse: 0.60024 | val_1_rmse: 0.63556 |  0:01:44s
epoch 54 | loss: 0.19045 | val_0_rmse: 0.4116  | val_1_rmse: 0.48275 |  0:01:46s
epoch 55 | loss: 0.19059 | val_0_rmse: 0.40691 | val_1_rmse: 0.48456 |  0:01:48s
epoch 56 | loss: 0.19218 | val_0_rmse: 0.43603 | val_1_rmse: 0.51243 |  0:01:50s
epoch 57 | loss: 0.18686 | val_0_rmse: 0.4038  | val_1_rmse: 0.483   |  0:01:51s
epoch 58 | loss: 0.18672 | val_0_rmse: 0.39979 | val_1_rmse: 0.49305 |  0:01:53s
epoch 59 | loss: 0.18257 | val_0_rmse: 0.40684 | val_1_rmse: 0.49238 |  0:01:55s
epoch 60 | loss: 0.1869  | val_0_rmse: 0.40788 | val_1_rmse: 0.48623 |  0:01:57s
epoch 61 | loss: 0.1842  | val_0_rmse: 0.40459 | val_1_rmse: 0.49099 |  0:01:59s
epoch 62 | loss: 0.18757 | val_0_rmse: 0.40998 | val_1_rmse: 0.49485 |  0:02:01s
epoch 63 | loss: 0.18362 | val_0_rmse: 0.40559 | val_1_rmse: 0.49565 |  0:02:03s
epoch 64 | loss: 0.18968 | val_0_rmse: 0.43291 | val_1_rmse: 0.52736 |  0:02:05s
epoch 65 | loss: 0.19027 | val_0_rmse: 0.40225 | val_1_rmse: 0.49325 |  0:02:07s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.4804
Best weights from best epoch are automatically used!
ended training at: 14:53:36
Feature importance:
Mean squared error is of 5536634989.392414
Mean absolute error:49404.65336643808
MAPE:0.16305114640842963
R2 score:0.7592617066019796
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:53:37
epoch 0  | loss: 1.25488 | val_0_rmse: 0.94617 | val_1_rmse: 0.94163 |  0:00:01s
epoch 1  | loss: 0.79772 | val_0_rmse: 0.85909 | val_1_rmse: 0.86857 |  0:00:03s
epoch 2  | loss: 0.70098 | val_0_rmse: 0.85584 | val_1_rmse: 0.8593  |  0:00:05s
epoch 3  | loss: 0.60647 | val_0_rmse: 0.81339 | val_1_rmse: 0.81759 |  0:00:07s
epoch 4  | loss: 0.46452 | val_0_rmse: 0.79002 | val_1_rmse: 0.79432 |  0:00:09s
epoch 5  | loss: 0.405   | val_0_rmse: 0.75748 | val_1_rmse: 0.76452 |  0:00:11s
epoch 6  | loss: 0.35948 | val_0_rmse: 0.74088 | val_1_rmse: 0.74596 |  0:00:13s
epoch 7  | loss: 0.33584 | val_0_rmse: 0.71424 | val_1_rmse: 0.71869 |  0:00:15s
epoch 8  | loss: 0.31715 | val_0_rmse: 0.71586 | val_1_rmse: 0.71835 |  0:00:17s
epoch 9  | loss: 0.30747 | val_0_rmse: 0.70235 | val_1_rmse: 0.70643 |  0:00:19s
epoch 10 | loss: 0.30175 | val_0_rmse: 0.6894  | val_1_rmse: 0.69302 |  0:00:21s
epoch 11 | loss: 0.28924 | val_0_rmse: 0.67689 | val_1_rmse: 0.67977 |  0:00:23s
epoch 12 | loss: 0.2945  | val_0_rmse: 0.6718  | val_1_rmse: 0.67853 |  0:00:25s
epoch 13 | loss: 0.28594 | val_0_rmse: 0.65079 | val_1_rmse: 0.66043 |  0:00:27s
epoch 14 | loss: 0.28791 | val_0_rmse: 0.64156 | val_1_rmse: 0.65236 |  0:00:28s
epoch 15 | loss: 0.28065 | val_0_rmse: 0.61104 | val_1_rmse: 0.62417 |  0:00:30s
epoch 16 | loss: 0.27795 | val_0_rmse: 0.60507 | val_1_rmse: 0.62516 |  0:00:32s
epoch 17 | loss: 0.27608 | val_0_rmse: 0.59218 | val_1_rmse: 0.61248 |  0:00:34s
epoch 18 | loss: 0.27252 | val_0_rmse: 0.56346 | val_1_rmse: 0.57925 |  0:00:36s
epoch 19 | loss: 0.26037 | val_0_rmse: 0.55415 | val_1_rmse: 0.58064 |  0:00:38s
epoch 20 | loss: 0.25866 | val_0_rmse: 0.54135 | val_1_rmse: 0.57081 |  0:00:40s
epoch 21 | loss: 0.26118 | val_0_rmse: 0.52951 | val_1_rmse: 0.55689 |  0:00:42s
epoch 22 | loss: 0.25813 | val_0_rmse: 0.51073 | val_1_rmse: 0.53241 |  0:00:44s
epoch 23 | loss: 0.25407 | val_0_rmse: 0.5012  | val_1_rmse: 0.524   |  0:00:46s
epoch 24 | loss: 0.2531  | val_0_rmse: 0.49945 | val_1_rmse: 0.52321 |  0:00:48s
epoch 25 | loss: 0.25174 | val_0_rmse: 0.49269 | val_1_rmse: 0.51682 |  0:00:50s
epoch 26 | loss: 0.2431  | val_0_rmse: 0.47315 | val_1_rmse: 0.50109 |  0:00:52s
epoch 27 | loss: 0.2408  | val_0_rmse: 0.47675 | val_1_rmse: 0.49917 |  0:00:54s
epoch 28 | loss: 0.2427  | val_0_rmse: 0.47703 | val_1_rmse: 0.50506 |  0:00:56s
epoch 29 | loss: 0.23817 | val_0_rmse: 0.46181 | val_1_rmse: 0.49922 |  0:00:57s
epoch 30 | loss: 0.23406 | val_0_rmse: 0.4605  | val_1_rmse: 0.49815 |  0:00:59s
epoch 31 | loss: 0.23398 | val_0_rmse: 0.45433 | val_1_rmse: 0.49289 |  0:01:01s
epoch 32 | loss: 0.2323  | val_0_rmse: 0.45153 | val_1_rmse: 0.49238 |  0:01:03s
epoch 33 | loss: 0.22594 | val_0_rmse: 0.45157 | val_1_rmse: 0.49383 |  0:01:05s
epoch 34 | loss: 0.23809 | val_0_rmse: 0.49341 | val_1_rmse: 0.53392 |  0:01:07s
epoch 35 | loss: 0.25521 | val_0_rmse: 0.49451 | val_1_rmse: 0.52261 |  0:01:09s
epoch 36 | loss: 0.24574 | val_0_rmse: 0.47506 | val_1_rmse: 0.50154 |  0:01:11s
epoch 37 | loss: 0.2323  | val_0_rmse: 0.46022 | val_1_rmse: 0.49702 |  0:01:13s
epoch 38 | loss: 0.22582 | val_0_rmse: 0.45233 | val_1_rmse: 0.49125 |  0:01:15s
epoch 39 | loss: 0.22159 | val_0_rmse: 0.44389 | val_1_rmse: 0.48772 |  0:01:17s
epoch 40 | loss: 0.21888 | val_0_rmse: 0.45004 | val_1_rmse: 0.49006 |  0:01:19s
epoch 41 | loss: 0.21703 | val_0_rmse: 0.44423 | val_1_rmse: 0.49204 |  0:01:21s
epoch 42 | loss: 0.21699 | val_0_rmse: 0.44847 | val_1_rmse: 0.49369 |  0:01:23s
epoch 43 | loss: 0.22067 | val_0_rmse: 0.44229 | val_1_rmse: 0.48747 |  0:01:24s
epoch 44 | loss: 0.22093 | val_0_rmse: 0.44341 | val_1_rmse: 0.49022 |  0:01:26s
epoch 45 | loss: 0.21438 | val_0_rmse: 0.44437 | val_1_rmse: 0.49962 |  0:01:28s
epoch 46 | loss: 0.22029 | val_0_rmse: 0.44117 | val_1_rmse: 0.49368 |  0:01:30s
epoch 47 | loss: 0.21338 | val_0_rmse: 0.46233 | val_1_rmse: 0.50894 |  0:01:32s
epoch 48 | loss: 0.21098 | val_0_rmse: 0.43864 | val_1_rmse: 0.49658 |  0:01:34s
epoch 49 | loss: 0.2065  | val_0_rmse: 0.43347 | val_1_rmse: 0.48874 |  0:01:36s
epoch 50 | loss: 0.20519 | val_0_rmse: 0.43117 | val_1_rmse: 0.4985  |  0:01:38s
epoch 51 | loss: 0.20323 | val_0_rmse: 0.42705 | val_1_rmse: 0.49246 |  0:01:40s
epoch 52 | loss: 0.20142 | val_0_rmse: 0.42649 | val_1_rmse: 0.48793 |  0:01:42s
epoch 53 | loss: 0.20311 | val_0_rmse: 0.42789 | val_1_rmse: 0.48864 |  0:01:44s
epoch 54 | loss: 0.20186 | val_0_rmse: 0.43237 | val_1_rmse: 0.49647 |  0:01:46s
epoch 55 | loss: 0.19955 | val_0_rmse: 0.42618 | val_1_rmse: 0.49443 |  0:01:48s
epoch 56 | loss: 0.20173 | val_0_rmse: 0.43782 | val_1_rmse: 0.49643 |  0:01:50s
epoch 57 | loss: 0.22307 | val_0_rmse: 0.43883 | val_1_rmse: 0.49878 |  0:01:51s
epoch 58 | loss: 0.21295 | val_0_rmse: 0.54221 | val_1_rmse: 0.57586 |  0:01:53s
epoch 59 | loss: 0.20614 | val_0_rmse: 0.43313 | val_1_rmse: 0.49766 |  0:01:55s
epoch 60 | loss: 0.20565 | val_0_rmse: 0.44042 | val_1_rmse: 0.50714 |  0:01:57s
epoch 61 | loss: 0.20435 | val_0_rmse: 0.42969 | val_1_rmse: 0.49908 |  0:01:59s
epoch 62 | loss: 0.20258 | val_0_rmse: 0.4364  | val_1_rmse: 0.50342 |  0:02:01s
epoch 63 | loss: 0.20612 | val_0_rmse: 0.42688 | val_1_rmse: 0.49264 |  0:02:03s
epoch 64 | loss: 0.20119 | val_0_rmse: 0.42703 | val_1_rmse: 0.50242 |  0:02:05s
epoch 65 | loss: 0.19796 | val_0_rmse: 0.41712 | val_1_rmse: 0.49811 |  0:02:07s
epoch 66 | loss: 0.19416 | val_0_rmse: 0.42825 | val_1_rmse: 0.50403 |  0:02:09s
epoch 67 | loss: 0.19371 | val_0_rmse: 0.41533 | val_1_rmse: 0.49898 |  0:02:11s
epoch 68 | loss: 0.19399 | val_0_rmse: 0.41434 | val_1_rmse: 0.49295 |  0:02:13s
epoch 69 | loss: 0.19005 | val_0_rmse: 0.40863 | val_1_rmse: 0.49214 |  0:02:15s
epoch 70 | loss: 0.19022 | val_0_rmse: 0.40798 | val_1_rmse: 0.49581 |  0:02:16s
epoch 71 | loss: 0.1869  | val_0_rmse: 0.40965 | val_1_rmse: 0.49956 |  0:02:18s
epoch 72 | loss: 0.18826 | val_0_rmse: 0.41226 | val_1_rmse: 0.50499 |  0:02:20s
epoch 73 | loss: 0.18774 | val_0_rmse: 0.40782 | val_1_rmse: 0.49977 |  0:02:22s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.48747
Best weights from best epoch are automatically used!
ended training at: 14:56:00
Feature importance:
Mean squared error is of 5556751873.835104
Mean absolute error:50368.13553202605
MAPE:0.1683903424912594
R2 score:0.751947048562599
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:56:04
epoch 0  | loss: 0.82996 | val_0_rmse: 0.77056 | val_1_rmse: 0.76332 |  0:00:11s
epoch 1  | loss: 0.39179 | val_0_rmse: 0.67887 | val_1_rmse: 0.67482 |  0:00:22s
epoch 2  | loss: 0.3393  | val_0_rmse: 0.65618 | val_1_rmse: 0.65776 |  0:00:33s
epoch 3  | loss: 0.30859 | val_0_rmse: 0.6295  | val_1_rmse: 0.63099 |  0:00:44s
epoch 4  | loss: 0.2935  | val_0_rmse: 0.61478 | val_1_rmse: 0.6198  |  0:00:55s
epoch 5  | loss: 0.2785  | val_0_rmse: 0.56769 | val_1_rmse: 0.57949 |  0:01:06s
epoch 6  | loss: 0.27277 | val_0_rmse: 0.54178 | val_1_rmse: 0.55046 |  0:01:18s
epoch 7  | loss: 0.26903 | val_0_rmse: 0.51463 | val_1_rmse: 0.52418 |  0:01:29s
epoch 8  | loss: 0.26841 | val_0_rmse: 0.49719 | val_1_rmse: 0.50981 |  0:01:41s
epoch 9  | loss: 0.25964 | val_0_rmse: 0.48855 | val_1_rmse: 0.50536 |  0:01:52s
epoch 10 | loss: 0.25629 | val_0_rmse: 0.50137 | val_1_rmse: 0.51985 |  0:02:04s
epoch 11 | loss: 0.25696 | val_0_rmse: 0.49353 | val_1_rmse: 0.51636 |  0:02:15s
epoch 12 | loss: 0.25017 | val_0_rmse: 0.48831 | val_1_rmse: 0.51196 |  0:02:26s
epoch 13 | loss: 0.25226 | val_0_rmse: 0.50035 | val_1_rmse: 0.52608 |  0:02:38s
epoch 14 | loss: 0.24783 | val_0_rmse: 0.51014 | val_1_rmse: 0.53217 |  0:02:49s
epoch 15 | loss: 0.24433 | val_0_rmse: 0.49169 | val_1_rmse: 0.51903 |  0:03:00s
epoch 16 | loss: 0.24509 | val_0_rmse: 0.49488 | val_1_rmse: 0.52353 |  0:03:12s
epoch 17 | loss: 0.24241 | val_0_rmse: 0.49678 | val_1_rmse: 0.52816 |  0:03:23s
epoch 18 | loss: 0.23883 | val_0_rmse: 0.4882  | val_1_rmse: 0.52211 |  0:03:34s
epoch 19 | loss: 0.23611 | val_0_rmse: 0.50373 | val_1_rmse: 0.53981 |  0:03:46s
epoch 20 | loss: 0.23864 | val_0_rmse: 0.5591  | val_1_rmse: 0.59155 |  0:03:57s
epoch 21 | loss: 0.23458 | val_0_rmse: 0.48222 | val_1_rmse: 0.51975 |  0:04:09s
epoch 22 | loss: 0.23067 | val_0_rmse: 0.51808 | val_1_rmse: 0.55688 |  0:04:20s
epoch 23 | loss: 0.2318  | val_0_rmse: 0.50044 | val_1_rmse: 0.5417  |  0:04:32s
epoch 24 | loss: 0.228   | val_0_rmse: 0.51181 | val_1_rmse: 0.54166 |  0:04:43s
epoch 25 | loss: 0.22747 | val_0_rmse: 0.47976 | val_1_rmse: 0.52493 |  0:04:54s
epoch 26 | loss: 0.23053 | val_0_rmse: 0.77686 | val_1_rmse: 0.78404 |  0:05:06s
epoch 27 | loss: 0.22888 | val_0_rmse: 0.48    | val_1_rmse: 0.51806 |  0:05:17s
epoch 28 | loss: 0.22585 | val_0_rmse: 0.50237 | val_1_rmse: 0.53896 |  0:05:28s
epoch 29 | loss: 0.22265 | val_0_rmse: 0.49857 | val_1_rmse: 0.53901 |  0:05:40s
epoch 30 | loss: 0.22185 | val_0_rmse: 0.48927 | val_1_rmse: 0.53767 |  0:05:51s
epoch 31 | loss: 0.21928 | val_0_rmse: 0.47908 | val_1_rmse: 0.52031 |  0:06:02s
epoch 32 | loss: 0.21891 | val_0_rmse: 0.46824 | val_1_rmse: 0.5167  |  0:06:14s
epoch 33 | loss: 0.21883 | val_0_rmse: 0.46781 | val_1_rmse: 0.51527 |  0:06:25s
epoch 34 | loss: 0.21563 | val_0_rmse: 0.5024  | val_1_rmse: 0.55764 |  0:06:37s
epoch 35 | loss: 0.2138  | val_0_rmse: 0.48487 | val_1_rmse: 0.54477 |  0:06:48s
epoch 36 | loss: 0.21193 | val_0_rmse: 0.47176 | val_1_rmse: 0.52772 |  0:06:59s
epoch 37 | loss: 0.21164 | val_0_rmse: 0.46733 | val_1_rmse: 0.52307 |  0:07:11s
epoch 38 | loss: 0.21245 | val_0_rmse: 0.46157 | val_1_rmse: 0.52452 |  0:07:22s
epoch 39 | loss: 0.20939 | val_0_rmse: 0.46129 | val_1_rmse: 0.52092 |  0:07:33s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.50536
Best weights from best epoch are automatically used!
ended training at: 15:03:43
Feature importance:
Mean squared error is of 1717887261.199652
Mean absolute error:29900.149748186785
MAPE:0.2874465528251071
R2 score:0.7429509226630346
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:03:47
epoch 0  | loss: 0.80114 | val_0_rmse: 0.89751 | val_1_rmse: 0.89133 |  0:00:11s
epoch 1  | loss: 0.48148 | val_0_rmse: 0.67948 | val_1_rmse: 0.67665 |  0:00:22s
epoch 2  | loss: 0.361   | val_0_rmse: 0.65953 | val_1_rmse: 0.65996 |  0:00:33s
epoch 3  | loss: 0.32408 | val_0_rmse: 0.64058 | val_1_rmse: 0.63911 |  0:00:45s
epoch 4  | loss: 0.3022  | val_0_rmse: 0.61404 | val_1_rmse: 0.61674 |  0:00:56s
epoch 5  | loss: 0.28943 | val_0_rmse: 0.58724 | val_1_rmse: 0.59019 |  0:01:07s
epoch 6  | loss: 0.28089 | val_0_rmse: 0.55199 | val_1_rmse: 0.55958 |  0:01:18s
epoch 7  | loss: 0.27649 | val_0_rmse: 0.51982 | val_1_rmse: 0.5315  |  0:01:30s
epoch 8  | loss: 0.26736 | val_0_rmse: 0.52541 | val_1_rmse: 0.54236 |  0:01:41s
epoch 9  | loss: 0.26212 | val_0_rmse: 0.51184 | val_1_rmse: 0.53185 |  0:01:52s
epoch 10 | loss: 0.25942 | val_0_rmse: 0.49042 | val_1_rmse: 0.51443 |  0:02:03s
epoch 11 | loss: 0.25545 | val_0_rmse: 0.49457 | val_1_rmse: 0.52062 |  0:02:16s
epoch 12 | loss: 0.2515  | val_0_rmse: 0.50493 | val_1_rmse: 0.5312  |  0:02:27s
epoch 13 | loss: 0.24904 | val_0_rmse: 0.49865 | val_1_rmse: 0.52688 |  0:02:39s
epoch 14 | loss: 0.24749 | val_0_rmse: 0.51325 | val_1_rmse: 0.54444 |  0:02:50s
epoch 15 | loss: 0.24548 | val_0_rmse: 0.5402  | val_1_rmse: 0.55149 |  0:03:01s
epoch 16 | loss: 0.24441 | val_0_rmse: 0.4937  | val_1_rmse: 0.52393 |  0:03:12s
epoch 17 | loss: 0.24141 | val_0_rmse: 0.52225 | val_1_rmse: 0.55644 |  0:03:24s
epoch 18 | loss: 0.24049 | val_0_rmse: 0.48925 | val_1_rmse: 0.52426 |  0:03:35s
epoch 19 | loss: 0.23933 | val_0_rmse: 0.48855 | val_1_rmse: 0.52926 |  0:03:46s
epoch 20 | loss: 0.24031 | val_0_rmse: 0.52351 | val_1_rmse: 0.55899 |  0:03:58s
epoch 21 | loss: 0.25209 | val_0_rmse: 0.56284 | val_1_rmse: 0.58168 |  0:04:09s
epoch 22 | loss: 0.25431 | val_0_rmse: 0.54094 | val_1_rmse: 0.566   |  0:04:20s
epoch 23 | loss: 0.24554 | val_0_rmse: 0.50726 | val_1_rmse: 0.54248 |  0:04:31s
epoch 24 | loss: 0.2479  | val_0_rmse: 0.50774 | val_1_rmse: 0.54507 |  0:04:43s
epoch 25 | loss: 0.23973 | val_0_rmse: 0.49828 | val_1_rmse: 0.53891 |  0:04:54s
epoch 26 | loss: 0.24589 | val_0_rmse: 0.53451 | val_1_rmse: 0.5762  |  0:05:06s
epoch 27 | loss: 0.23973 | val_0_rmse: 0.53406 | val_1_rmse: 0.57807 |  0:05:17s
epoch 28 | loss: 0.23632 | val_0_rmse: 0.4864  | val_1_rmse: 0.53082 |  0:05:28s
epoch 29 | loss: 0.24581 | val_0_rmse: 0.76408 | val_1_rmse: 0.5435  |  0:05:40s
epoch 30 | loss: 0.24639 | val_0_rmse: 0.5606  | val_1_rmse: 0.58759 |  0:05:51s
epoch 31 | loss: 0.32595 | val_0_rmse: 1.06205 | val_1_rmse: 0.71437 |  0:06:02s
epoch 32 | loss: 0.33998 | val_0_rmse: 1.88443 | val_1_rmse: 0.58752 |  0:06:14s
epoch 33 | loss: 0.26738 | val_0_rmse: 0.53344 | val_1_rmse: 0.56788 |  0:06:25s
epoch 34 | loss: 0.25413 | val_0_rmse: 0.59182 | val_1_rmse: 0.61049 |  0:06:37s
epoch 35 | loss: 0.24392 | val_0_rmse: 0.50219 | val_1_rmse: 0.5387  |  0:06:48s
epoch 36 | loss: 0.24032 | val_0_rmse: 0.54399 | val_1_rmse: 0.58396 |  0:06:59s
epoch 37 | loss: 0.23605 | val_0_rmse: 0.4903  | val_1_rmse: 0.53199 |  0:07:11s
epoch 38 | loss: 0.23189 | val_0_rmse: 0.48618 | val_1_rmse: 0.52869 |  0:07:22s
epoch 39 | loss: 0.22752 | val_0_rmse: 0.48981 | val_1_rmse: 0.53438 |  0:07:33s
epoch 40 | loss: 0.22551 | val_0_rmse: 0.51069 | val_1_rmse: 0.54071 |  0:07:45s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.51443
Best weights from best epoch are automatically used!
ended training at: 15:11:38
Feature importance:
Mean squared error is of 1754878759.6867516
Mean absolute error:30022.277246542737
MAPE:0.28790912720139505
R2 score:0.7435613881179608
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:11:41
epoch 0  | loss: 0.81649 | val_0_rmse: 0.80928 | val_1_rmse: 0.80553 |  0:00:11s
epoch 1  | loss: 0.40506 | val_0_rmse: 0.69788 | val_1_rmse: 0.70176 |  0:00:22s
epoch 2  | loss: 0.32636 | val_0_rmse: 0.66975 | val_1_rmse: 0.6705  |  0:00:33s
epoch 3  | loss: 0.30168 | val_0_rmse: 0.64885 | val_1_rmse: 0.65559 |  0:00:44s
epoch 4  | loss: 0.28632 | val_0_rmse: 0.59314 | val_1_rmse: 0.60198 |  0:00:56s
epoch 5  | loss: 0.27544 | val_0_rmse: 0.55407 | val_1_rmse: 0.56611 |  0:01:07s
epoch 6  | loss: 0.27513 | val_0_rmse: 0.53961 | val_1_rmse: 0.55692 |  0:01:18s
epoch 7  | loss: 0.26926 | val_0_rmse: 0.51853 | val_1_rmse: 0.53977 |  0:01:30s
epoch 8  | loss: 0.27727 | val_0_rmse: 0.51591 | val_1_rmse: 0.53855 |  0:01:41s
epoch 9  | loss: 0.27017 | val_0_rmse: 0.52335 | val_1_rmse: 0.54708 |  0:01:52s
epoch 10 | loss: 0.26628 | val_0_rmse: 0.50271 | val_1_rmse: 0.52718 |  0:02:04s
epoch 11 | loss: 0.25996 | val_0_rmse: 0.50572 | val_1_rmse: 0.5387  |  0:02:15s
epoch 12 | loss: 0.25256 | val_0_rmse: 0.48465 | val_1_rmse: 0.51876 |  0:02:26s
epoch 13 | loss: 0.25021 | val_0_rmse: 0.51371 | val_1_rmse: 0.55457 |  0:02:38s
epoch 14 | loss: 0.25111 | val_0_rmse: 0.49556 | val_1_rmse: 0.52859 |  0:02:49s
epoch 15 | loss: 0.24721 | val_0_rmse: 0.49689 | val_1_rmse: 0.53206 |  0:03:01s
epoch 16 | loss: 0.25282 | val_0_rmse: 0.50268 | val_1_rmse: 0.53687 |  0:03:12s
epoch 17 | loss: 0.25394 | val_0_rmse: 0.52351 | val_1_rmse: 0.56704 |  0:03:23s
epoch 18 | loss: 0.25244 | val_0_rmse: 0.50272 | val_1_rmse: 0.53885 |  0:03:35s
epoch 19 | loss: 0.25253 | val_0_rmse: 0.51514 | val_1_rmse: 0.54982 |  0:03:46s
epoch 20 | loss: 0.26113 | val_0_rmse: 0.5467  | val_1_rmse: 0.59321 |  0:03:58s
epoch 21 | loss: 0.25336 | val_0_rmse: 0.52235 | val_1_rmse: 0.55825 |  0:04:09s
epoch 22 | loss: 0.25257 | val_0_rmse: 1.08799 | val_1_rmse: 1.10844 |  0:04:21s
epoch 23 | loss: 0.28347 | val_0_rmse: 0.55095 | val_1_rmse: 0.58551 |  0:04:32s
epoch 24 | loss: 0.26864 | val_0_rmse: 0.52178 | val_1_rmse: 0.56446 |  0:04:43s
epoch 25 | loss: 0.25764 | val_0_rmse: 0.5216  | val_1_rmse: 0.55623 |  0:04:55s
epoch 26 | loss: 0.24741 | val_0_rmse: 0.49388 | val_1_rmse: 0.55781 |  0:05:06s
epoch 27 | loss: 0.24257 | val_0_rmse: 0.52972 | val_1_rmse: 0.58055 |  0:05:18s
epoch 28 | loss: 0.24047 | val_0_rmse: 0.53389 | val_1_rmse: 0.58078 |  0:05:29s
epoch 29 | loss: 0.24876 | val_0_rmse: 0.49281 | val_1_rmse: 0.53335 |  0:05:41s
epoch 30 | loss: 0.2409  | val_0_rmse: 0.49839 | val_1_rmse: 0.53889 |  0:05:52s
epoch 31 | loss: 0.23815 | val_0_rmse: 0.49421 | val_1_rmse: 0.53263 |  0:06:03s
epoch 32 | loss: 0.23601 | val_0_rmse: 0.53886 | val_1_rmse: 0.5891  |  0:06:15s
epoch 33 | loss: 0.231   | val_0_rmse: 0.48404 | val_1_rmse: 0.53463 |  0:06:26s
epoch 34 | loss: 0.22902 | val_0_rmse: 0.48267 | val_1_rmse: 0.52642 |  0:06:38s
epoch 35 | loss: 0.23424 | val_0_rmse: 0.50129 | val_1_rmse: 0.81527 |  0:06:49s
epoch 36 | loss: 0.22823 | val_0_rmse: 0.53474 | val_1_rmse: 0.58521 |  0:07:01s
epoch 37 | loss: 0.22442 | val_0_rmse: 0.49244 | val_1_rmse: 0.54338 |  0:07:12s
epoch 38 | loss: 0.22389 | val_0_rmse: 0.48673 | val_1_rmse: 0.53755 |  0:07:23s
epoch 39 | loss: 0.22083 | val_0_rmse: 0.49118 | val_1_rmse: 0.54267 |  0:07:35s
epoch 40 | loss: 0.21995 | val_0_rmse: 0.54009 | val_1_rmse: 0.59795 |  0:07:46s
epoch 41 | loss: 0.22613 | val_0_rmse: 0.48888 | val_1_rmse: 0.54226 |  0:07:58s
epoch 42 | loss: 0.22078 | val_0_rmse: 0.50089 | val_1_rmse: 0.56431 |  0:08:09s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.51876
Best weights from best epoch are automatically used!
ended training at: 15:19:56
Feature importance:
Mean squared error is of 1780092753.400096
Mean absolute error:30460.137205132647
MAPE:0.2928477489148165
R2 score:0.7354253245952475
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:19:59
epoch 0  | loss: 0.74124 | val_0_rmse: 0.73711 | val_1_rmse: 0.75095 |  0:00:11s
epoch 1  | loss: 0.34486 | val_0_rmse: 0.70657 | val_1_rmse: 0.71814 |  0:00:22s
epoch 2  | loss: 0.30165 | val_0_rmse: 0.66286 | val_1_rmse: 0.67799 |  0:00:33s
epoch 3  | loss: 0.27898 | val_0_rmse: 0.62322 | val_1_rmse: 0.63795 |  0:00:44s
epoch 4  | loss: 0.27231 | val_0_rmse: 0.59314 | val_1_rmse: 0.60926 |  0:00:56s
epoch 5  | loss: 0.26515 | val_0_rmse: 0.5609  | val_1_rmse: 0.58094 |  0:01:08s
epoch 6  | loss: 0.26356 | val_0_rmse: 0.54023 | val_1_rmse: 0.56205 |  0:01:19s
epoch 7  | loss: 0.26057 | val_0_rmse: 0.49683 | val_1_rmse: 0.5235  |  0:01:30s
epoch 8  | loss: 0.25634 | val_0_rmse: 0.51129 | val_1_rmse: 0.54204 |  0:01:42s
epoch 9  | loss: 0.25048 | val_0_rmse: 0.48293 | val_1_rmse: 0.51821 |  0:01:53s
epoch 10 | loss: 0.2478  | val_0_rmse: 0.47918 | val_1_rmse: 0.51377 |  0:02:04s
epoch 11 | loss: 0.24514 | val_0_rmse: 0.48567 | val_1_rmse: 0.52518 |  0:02:15s
epoch 12 | loss: 0.24109 | val_0_rmse: 0.48479 | val_1_rmse: 0.52422 |  0:02:27s
epoch 13 | loss: 0.24176 | val_0_rmse: 0.50366 | val_1_rmse: 0.54311 |  0:02:38s
epoch 14 | loss: 0.2402  | val_0_rmse: 0.48687 | val_1_rmse: 0.52859 |  0:02:49s
epoch 15 | loss: 0.23584 | val_0_rmse: 0.50648 | val_1_rmse: 0.55029 |  0:03:00s
epoch 16 | loss: 0.23376 | val_0_rmse: 0.48873 | val_1_rmse: 0.53307 |  0:03:12s
epoch 17 | loss: 0.23075 | val_0_rmse: 0.48686 | val_1_rmse: 0.53785 |  0:03:23s
epoch 18 | loss: 0.23216 | val_0_rmse: 0.48271 | val_1_rmse: 0.52668 |  0:03:34s
epoch 19 | loss: 0.22936 | val_0_rmse: 0.50078 | val_1_rmse: 0.54824 |  0:03:46s
epoch 20 | loss: 0.22615 | val_0_rmse: 0.48946 | val_1_rmse: 0.5429  |  0:03:57s
epoch 21 | loss: 0.22389 | val_0_rmse: 0.47644 | val_1_rmse: 0.52804 |  0:04:08s
epoch 22 | loss: 0.22259 | val_0_rmse: 0.46922 | val_1_rmse: 0.52035 |  0:04:19s
epoch 23 | loss: 0.22057 | val_0_rmse: 0.47611 | val_1_rmse: 0.52936 |  0:04:31s
epoch 24 | loss: 0.21826 | val_0_rmse: 0.4685  | val_1_rmse: 0.52702 |  0:04:42s
epoch 25 | loss: 0.21594 | val_0_rmse: 0.47349 | val_1_rmse: 0.53525 |  0:04:53s
epoch 26 | loss: 0.21657 | val_0_rmse: 0.53541 | val_1_rmse: 0.63217 |  0:05:04s
epoch 27 | loss: 0.21457 | val_0_rmse: 0.48699 | val_1_rmse: 0.53776 |  0:05:16s
epoch 28 | loss: 0.214   | val_0_rmse: 0.5942  | val_1_rmse: 0.66781 |  0:05:27s
epoch 29 | loss: 0.21029 | val_0_rmse: 0.46865 | val_1_rmse: 0.52721 |  0:05:38s
epoch 30 | loss: 0.21055 | val_0_rmse: 0.45735 | val_1_rmse: 0.52107 |  0:05:49s
epoch 31 | loss: 0.20969 | val_0_rmse: 0.79009 | val_1_rmse: 0.85524 |  0:06:01s
epoch 32 | loss: 0.21091 | val_0_rmse: 0.46995 | val_1_rmse: 0.53518 |  0:06:12s
epoch 33 | loss: 0.20611 | val_0_rmse: 0.45324 | val_1_rmse: 0.51993 |  0:06:23s
epoch 34 | loss: 0.20561 | val_0_rmse: 0.50132 | val_1_rmse: 0.56321 |  0:06:34s
epoch 35 | loss: 0.20321 | val_0_rmse: 0.45823 | val_1_rmse: 0.53232 |  0:06:46s
epoch 36 | loss: 0.20101 | val_0_rmse: 0.44789 | val_1_rmse: 0.52192 |  0:06:57s
epoch 37 | loss: 0.19985 | val_0_rmse: 0.45877 | val_1_rmse: 0.53519 |  0:07:08s
epoch 38 | loss: 0.20094 | val_0_rmse: 0.44986 | val_1_rmse: 0.52945 |  0:07:19s
epoch 39 | loss: 0.19945 | val_0_rmse: 0.45089 | val_1_rmse: 0.53169 |  0:07:31s
epoch 40 | loss: 0.19639 | val_0_rmse: 0.4447  | val_1_rmse: 0.5193  |  0:07:42s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.51377
Best weights from best epoch are automatically used!
ended training at: 15:27:47
Feature importance:
Mean squared error is of 1785499127.3938437
Mean absolute error:30541.634650791464
MAPE:0.29381096716764
R2 score:0.7365898027113904
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:27:50
epoch 0  | loss: 0.75425 | val_0_rmse: 0.7798  | val_1_rmse: 0.77534 |  0:00:11s
epoch 1  | loss: 0.36188 | val_0_rmse: 0.68836 | val_1_rmse: 0.687   |  0:00:22s
epoch 2  | loss: 0.31053 | val_0_rmse: 0.65038 | val_1_rmse: 0.65034 |  0:00:33s
epoch 3  | loss: 0.28487 | val_0_rmse: 0.62143 | val_1_rmse: 0.62644 |  0:00:45s
epoch 4  | loss: 0.27827 | val_0_rmse: 0.58027 | val_1_rmse: 0.58624 |  0:00:56s
epoch 5  | loss: 0.26907 | val_0_rmse: 0.55271 | val_1_rmse: 0.56201 |  0:01:07s
epoch 6  | loss: 0.26409 | val_0_rmse: 0.54816 | val_1_rmse: 0.56108 |  0:01:18s
epoch 7  | loss: 0.26039 | val_0_rmse: 0.50068 | val_1_rmse: 0.51958 |  0:01:30s
epoch 8  | loss: 0.25719 | val_0_rmse: 0.48777 | val_1_rmse: 0.50991 |  0:01:41s
epoch 9  | loss: 0.25322 | val_0_rmse: 0.48733 | val_1_rmse: 0.51142 |  0:01:52s
epoch 10 | loss: 0.25042 | val_0_rmse: 0.47787 | val_1_rmse: 0.50593 |  0:02:03s
epoch 11 | loss: 0.24779 | val_0_rmse: 0.47907 | val_1_rmse: 0.51281 |  0:02:15s
epoch 12 | loss: 0.24266 | val_0_rmse: 0.49051 | val_1_rmse: 0.52528 |  0:02:26s
epoch 13 | loss: 0.24196 | val_0_rmse: 0.48445 | val_1_rmse: 0.51687 |  0:02:37s
epoch 14 | loss: 0.23888 | val_0_rmse: 0.49796 | val_1_rmse: 0.53356 |  0:02:49s
epoch 15 | loss: 0.2397  | val_0_rmse: 0.52623 | val_1_rmse: 0.57311 |  0:03:00s
epoch 16 | loss: 0.23718 | val_0_rmse: 0.48705 | val_1_rmse: 0.52245 |  0:03:12s
epoch 17 | loss: 0.23704 | val_0_rmse: 0.48245 | val_1_rmse: 0.51839 |  0:03:23s
epoch 18 | loss: 0.23404 | val_0_rmse: 0.47727 | val_1_rmse: 0.51269 |  0:03:34s
epoch 19 | loss: 0.23337 | val_0_rmse: 0.51535 | val_1_rmse: 0.56033 |  0:03:46s
epoch 20 | loss: 0.23182 | val_0_rmse: 0.58909 | val_1_rmse: 0.52479 |  0:03:57s
epoch 21 | loss: 0.22813 | val_0_rmse: 0.49997 | val_1_rmse: 0.53497 |  0:04:08s
epoch 22 | loss: 0.22976 | val_0_rmse: 0.54459 | val_1_rmse: 0.60434 |  0:04:20s
epoch 23 | loss: 0.22811 | val_0_rmse: 0.50198 | val_1_rmse: 0.64531 |  0:04:31s
epoch 24 | loss: 0.22649 | val_0_rmse: 0.48597 | val_1_rmse: 0.55658 |  0:04:42s
epoch 25 | loss: 0.22273 | val_0_rmse: 0.49922 | val_1_rmse: 0.56505 |  0:04:54s
epoch 26 | loss: 0.22245 | val_0_rmse: 0.49766 | val_1_rmse: 0.55175 |  0:05:05s
epoch 27 | loss: 0.22311 | val_0_rmse: 0.88552 | val_1_rmse: 0.89173 |  0:05:16s
epoch 28 | loss: 0.22213 | val_0_rmse: 0.48615 | val_1_rmse: 0.57294 |  0:05:28s
epoch 29 | loss: 0.21967 | val_0_rmse: 0.47535 | val_1_rmse: 0.58053 |  0:05:39s
epoch 30 | loss: 0.21878 | val_0_rmse: 0.66877 | val_1_rmse: 0.64822 |  0:05:51s
epoch 31 | loss: 0.22068 | val_0_rmse: 0.52209 | val_1_rmse: 0.59739 |  0:06:02s
epoch 32 | loss: 0.21613 | val_0_rmse: 0.58475 | val_1_rmse: 0.53125 |  0:06:13s
epoch 33 | loss: 0.21563 | val_0_rmse: 0.48866 | val_1_rmse: 0.53442 |  0:06:25s
epoch 34 | loss: 0.21422 | val_0_rmse: 0.64143 | val_1_rmse: 0.52231 |  0:06:36s
epoch 35 | loss: 0.21252 | val_0_rmse: 0.57887 | val_1_rmse: 0.56982 |  0:06:47s
epoch 36 | loss: 0.21452 | val_0_rmse: 0.48452 | val_1_rmse: 0.55548 |  0:06:59s
epoch 37 | loss: 0.20931 | val_0_rmse: 0.47274 | val_1_rmse: 0.61049 |  0:07:10s
epoch 38 | loss: 0.21322 | val_0_rmse: 0.46774 | val_1_rmse: 0.52974 |  0:07:22s
epoch 39 | loss: 0.21016 | val_0_rmse: 0.47885 | val_1_rmse: 0.5496  |  0:07:33s
epoch 40 | loss: 0.20809 | val_0_rmse: 0.47079 | val_1_rmse: 0.54129 |  0:07:44s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.50593
Best weights from best epoch are automatically used!
ended training at: 15:35:41
Feature importance:
Mean squared error is of 1724377208.891886
Mean absolute error:29642.322331343566
MAPE:0.2776039858122589
R2 score:0.746990710888551
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:35:43
epoch 0  | loss: 1.414   | val_0_rmse: 0.77415 | val_1_rmse: 0.76552 |  0:00:01s
epoch 1  | loss: 0.48289 | val_0_rmse: 0.68671 | val_1_rmse: 0.68272 |  0:00:03s
epoch 2  | loss: 0.34419 | val_0_rmse: 0.71683 | val_1_rmse: 0.70956 |  0:00:04s
epoch 3  | loss: 0.30365 | val_0_rmse: 0.66641 | val_1_rmse: 0.6618  |  0:00:06s
epoch 4  | loss: 0.28906 | val_0_rmse: 0.69777 | val_1_rmse: 0.69253 |  0:00:08s
epoch 5  | loss: 0.27349 | val_0_rmse: 0.71571 | val_1_rmse: 0.7029  |  0:00:10s
epoch 6  | loss: 0.25031 | val_0_rmse: 0.62278 | val_1_rmse: 0.62342 |  0:00:11s
epoch 7  | loss: 0.24812 | val_0_rmse: 0.5991  | val_1_rmse: 0.59603 |  0:00:13s
epoch 8  | loss: 0.24518 | val_0_rmse: 0.59452 | val_1_rmse: 0.59096 |  0:00:14s
epoch 9  | loss: 0.23669 | val_0_rmse: 0.60351 | val_1_rmse: 0.59881 |  0:00:16s
epoch 10 | loss: 0.22916 | val_0_rmse: 0.58249 | val_1_rmse: 0.58329 |  0:00:18s
epoch 11 | loss: 0.23542 | val_0_rmse: 0.5642  | val_1_rmse: 0.56649 |  0:00:19s
epoch 12 | loss: 0.23841 | val_0_rmse: 0.53655 | val_1_rmse: 0.53713 |  0:00:21s
epoch 13 | loss: 0.23261 | val_0_rmse: 0.54005 | val_1_rmse: 0.54106 |  0:00:23s
epoch 14 | loss: 0.21687 | val_0_rmse: 0.50401 | val_1_rmse: 0.50875 |  0:00:24s
epoch 15 | loss: 0.21427 | val_0_rmse: 0.50596 | val_1_rmse: 0.51366 |  0:00:26s
epoch 16 | loss: 0.21065 | val_0_rmse: 0.48141 | val_1_rmse: 0.48614 |  0:00:28s
epoch 17 | loss: 0.20957 | val_0_rmse: 0.50733 | val_1_rmse: 0.51787 |  0:00:30s
epoch 18 | loss: 0.20986 | val_0_rmse: 0.48948 | val_1_rmse: 0.49867 |  0:00:31s
epoch 19 | loss: 0.20493 | val_0_rmse: 0.47463 | val_1_rmse: 0.48833 |  0:00:33s
epoch 20 | loss: 0.20563 | val_0_rmse: 0.47023 | val_1_rmse: 0.49099 |  0:00:35s
epoch 21 | loss: 0.20694 | val_0_rmse: 0.46847 | val_1_rmse: 0.48405 |  0:00:36s
epoch 22 | loss: 0.20222 | val_0_rmse: 0.44812 | val_1_rmse: 0.47116 |  0:00:38s
epoch 23 | loss: 0.20497 | val_0_rmse: 0.45843 | val_1_rmse: 0.48289 |  0:00:40s
epoch 24 | loss: 0.20658 | val_0_rmse: 0.45401 | val_1_rmse: 0.47581 |  0:00:41s
epoch 25 | loss: 0.20349 | val_0_rmse: 0.47843 | val_1_rmse: 0.50056 |  0:00:43s
epoch 26 | loss: 0.21381 | val_0_rmse: 0.4563  | val_1_rmse: 0.47688 |  0:00:45s
epoch 27 | loss: 0.20704 | val_0_rmse: 0.44693 | val_1_rmse: 0.47528 |  0:00:46s
epoch 28 | loss: 0.20238 | val_0_rmse: 0.44613 | val_1_rmse: 0.47427 |  0:00:48s
epoch 29 | loss: 0.19903 | val_0_rmse: 0.43279 | val_1_rmse: 0.46602 |  0:00:50s
epoch 30 | loss: 0.19628 | val_0_rmse: 0.45053 | val_1_rmse: 0.48038 |  0:00:51s
epoch 31 | loss: 0.19359 | val_0_rmse: 0.43443 | val_1_rmse: 0.46734 |  0:00:53s
epoch 32 | loss: 0.19407 | val_0_rmse: 0.42417 | val_1_rmse: 0.46075 |  0:00:54s
epoch 33 | loss: 0.19213 | val_0_rmse: 0.42592 | val_1_rmse: 0.46108 |  0:00:56s
epoch 34 | loss: 0.19063 | val_0_rmse: 0.42809 | val_1_rmse: 0.45932 |  0:00:58s
epoch 35 | loss: 0.19582 | val_0_rmse: 0.42972 | val_1_rmse: 0.46515 |  0:00:59s
epoch 36 | loss: 0.19451 | val_0_rmse: 0.42727 | val_1_rmse: 0.46174 |  0:01:01s
epoch 37 | loss: 0.18953 | val_0_rmse: 0.43002 | val_1_rmse: 0.46482 |  0:01:03s
epoch 38 | loss: 0.18949 | val_0_rmse: 0.41914 | val_1_rmse: 0.45663 |  0:01:04s
epoch 39 | loss: 0.18853 | val_0_rmse: 0.42614 | val_1_rmse: 0.4584  |  0:01:06s
epoch 40 | loss: 0.19038 | val_0_rmse: 0.41716 | val_1_rmse: 0.45625 |  0:01:08s
epoch 41 | loss: 0.18744 | val_0_rmse: 0.41735 | val_1_rmse: 0.45669 |  0:01:09s
epoch 42 | loss: 0.18452 | val_0_rmse: 0.41609 | val_1_rmse: 0.45491 |  0:01:11s
epoch 43 | loss: 0.18389 | val_0_rmse: 0.42574 | val_1_rmse: 0.46534 |  0:01:13s
epoch 44 | loss: 0.18668 | val_0_rmse: 0.41687 | val_1_rmse: 0.45437 |  0:01:14s
epoch 45 | loss: 0.18284 | val_0_rmse: 0.42195 | val_1_rmse: 0.46161 |  0:01:16s
epoch 46 | loss: 0.18805 | val_0_rmse: 0.41982 | val_1_rmse: 0.45916 |  0:01:18s
epoch 47 | loss: 0.18805 | val_0_rmse: 0.41214 | val_1_rmse: 0.45354 |  0:01:19s
epoch 48 | loss: 0.18484 | val_0_rmse: 0.41629 | val_1_rmse: 0.45629 |  0:01:21s
epoch 49 | loss: 0.18363 | val_0_rmse: 0.44499 | val_1_rmse: 0.49374 |  0:01:23s
epoch 50 | loss: 0.18037 | val_0_rmse: 0.41477 | val_1_rmse: 0.46124 |  0:01:24s
epoch 51 | loss: 0.17921 | val_0_rmse: 0.41501 | val_1_rmse: 0.45936 |  0:01:26s
epoch 52 | loss: 0.18129 | val_0_rmse: 0.43093 | val_1_rmse: 0.47335 |  0:01:28s
epoch 53 | loss: 0.19444 | val_0_rmse: 0.43372 | val_1_rmse: 0.48226 |  0:01:29s
epoch 54 | loss: 0.18576 | val_0_rmse: 0.42876 | val_1_rmse: 0.49135 |  0:01:31s
epoch 55 | loss: 0.18177 | val_0_rmse: 0.43052 | val_1_rmse: 0.49278 |  0:01:33s
epoch 56 | loss: 0.17761 | val_0_rmse: 0.44482 | val_1_rmse: 0.51631 |  0:01:34s
epoch 57 | loss: 0.18028 | val_0_rmse: 0.44803 | val_1_rmse: 0.51689 |  0:01:36s
epoch 58 | loss: 0.17958 | val_0_rmse: 0.43236 | val_1_rmse: 0.51275 |  0:01:38s
epoch 59 | loss: 0.18523 | val_0_rmse: 0.42539 | val_1_rmse: 0.47853 |  0:01:39s
epoch 60 | loss: 0.18536 | val_0_rmse: 0.44639 | val_1_rmse: 0.50593 |  0:01:41s
epoch 61 | loss: 0.19896 | val_0_rmse: 0.44909 | val_1_rmse: 0.49335 |  0:01:43s
epoch 62 | loss: 0.18969 | val_0_rmse: 0.4515  | val_1_rmse: 0.48719 |  0:01:44s
epoch 63 | loss: 0.18797 | val_0_rmse: 0.41694 | val_1_rmse: 0.46547 |  0:01:46s
epoch 64 | loss: 0.19475 | val_0_rmse: 0.43543 | val_1_rmse: 0.46869 |  0:01:48s
epoch 65 | loss: 0.19784 | val_0_rmse: 0.44272 | val_1_rmse: 0.47902 |  0:01:49s
epoch 66 | loss: 0.18689 | val_0_rmse: 0.42232 | val_1_rmse: 0.46127 |  0:01:51s
epoch 67 | loss: 0.18185 | val_0_rmse: 0.4158  | val_1_rmse: 0.45801 |  0:01:53s
epoch 68 | loss: 0.18299 | val_0_rmse: 0.44779 | val_1_rmse: 0.48105 |  0:01:54s
epoch 69 | loss: 0.18707 | val_0_rmse: 0.42466 | val_1_rmse: 0.46107 |  0:01:56s
epoch 70 | loss: 0.18032 | val_0_rmse: 0.41906 | val_1_rmse: 0.46491 |  0:01:58s
epoch 71 | loss: 0.17843 | val_0_rmse: 0.41448 | val_1_rmse: 0.46039 |  0:01:59s
epoch 72 | loss: 0.17565 | val_0_rmse: 0.40965 | val_1_rmse: 0.45828 |  0:02:01s
epoch 73 | loss: 0.17612 | val_0_rmse: 0.40938 | val_1_rmse: 0.45512 |  0:02:02s
epoch 74 | loss: 0.1764  | val_0_rmse: 0.4146  | val_1_rmse: 0.46616 |  0:02:04s
epoch 75 | loss: 0.17302 | val_0_rmse: 0.40694 | val_1_rmse: 0.48241 |  0:02:06s
epoch 76 | loss: 0.17519 | val_0_rmse: 0.41469 | val_1_rmse: 0.49188 |  0:02:07s
epoch 77 | loss: 0.17377 | val_0_rmse: 0.42113 | val_1_rmse: 0.47533 |  0:02:09s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.45354
Best weights from best epoch are automatically used!
ended training at: 15:37:53
Feature importance:
Mean squared error is of 885949834.3502724
Mean absolute error:20368.11174641155
MAPE:0.241699995891504
R2 score:0.8090467687575782
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:37:54
epoch 0  | loss: 1.8061  | val_0_rmse: 0.97979 | val_1_rmse: 0.98453 |  0:00:01s
epoch 1  | loss: 0.56616 | val_0_rmse: 0.73885 | val_1_rmse: 0.74708 |  0:00:03s
epoch 2  | loss: 0.34407 | val_0_rmse: 0.8101  | val_1_rmse: 0.81294 |  0:00:05s
epoch 3  | loss: 0.31085 | val_0_rmse: 0.72015 | val_1_rmse: 0.72701 |  0:00:06s
epoch 4  | loss: 0.29223 | val_0_rmse: 0.57385 | val_1_rmse: 0.57587 |  0:00:08s
epoch 5  | loss: 0.27522 | val_0_rmse: 0.5858  | val_1_rmse: 0.58861 |  0:00:10s
epoch 6  | loss: 0.26353 | val_0_rmse: 0.58795 | val_1_rmse: 0.59117 |  0:00:11s
epoch 7  | loss: 0.25806 | val_0_rmse: 0.56815 | val_1_rmse: 0.569   |  0:00:13s
epoch 8  | loss: 0.25708 | val_0_rmse: 0.59719 | val_1_rmse: 0.59475 |  0:00:15s
epoch 9  | loss: 0.2472  | val_0_rmse: 0.5466  | val_1_rmse: 0.5471  |  0:00:16s
epoch 10 | loss: 0.25076 | val_0_rmse: 0.57223 | val_1_rmse: 0.5783  |  0:00:18s
epoch 11 | loss: 0.24914 | val_0_rmse: 0.51639 | val_1_rmse: 0.52132 |  0:00:20s
epoch 12 | loss: 0.23926 | val_0_rmse: 0.52369 | val_1_rmse: 0.52709 |  0:00:21s
epoch 13 | loss: 0.24992 | val_0_rmse: 0.54008 | val_1_rmse: 0.54874 |  0:00:23s
epoch 14 | loss: 0.23554 | val_0_rmse: 0.49699 | val_1_rmse: 0.50553 |  0:00:25s
epoch 15 | loss: 0.23214 | val_0_rmse: 0.49566 | val_1_rmse: 0.50183 |  0:00:26s
epoch 16 | loss: 0.2273  | val_0_rmse: 0.50289 | val_1_rmse: 0.50691 |  0:00:28s
epoch 17 | loss: 0.22763 | val_0_rmse: 0.48046 | val_1_rmse: 0.48546 |  0:00:30s
epoch 18 | loss: 0.22364 | val_0_rmse: 0.47853 | val_1_rmse: 0.48498 |  0:00:31s
epoch 19 | loss: 0.22205 | val_0_rmse: 0.50617 | val_1_rmse: 0.50866 |  0:00:33s
epoch 20 | loss: 0.21567 | val_0_rmse: 0.47318 | val_1_rmse: 0.47925 |  0:00:35s
epoch 21 | loss: 0.21998 | val_0_rmse: 0.47637 | val_1_rmse: 0.48542 |  0:00:36s
epoch 22 | loss: 0.22251 | val_0_rmse: 0.46451 | val_1_rmse: 0.47346 |  0:00:38s
epoch 23 | loss: 0.21682 | val_0_rmse: 0.45765 | val_1_rmse: 0.46917 |  0:00:39s
epoch 24 | loss: 0.21534 | val_0_rmse: 0.47693 | val_1_rmse: 0.4901  |  0:00:41s
epoch 25 | loss: 0.2178  | val_0_rmse: 0.45916 | val_1_rmse: 0.47604 |  0:00:43s
epoch 26 | loss: 0.22229 | val_0_rmse: 0.47418 | val_1_rmse: 0.48601 |  0:00:44s
epoch 27 | loss: 0.22366 | val_0_rmse: 0.46046 | val_1_rmse: 0.47732 |  0:00:46s
epoch 28 | loss: 0.21572 | val_0_rmse: 0.47941 | val_1_rmse: 0.4969  |  0:00:48s
epoch 29 | loss: 0.21574 | val_0_rmse: 0.45678 | val_1_rmse: 0.4738  |  0:00:49s
epoch 30 | loss: 0.21139 | val_0_rmse: 0.45499 | val_1_rmse: 0.47593 |  0:00:51s
epoch 31 | loss: 0.21715 | val_0_rmse: 0.45163 | val_1_rmse: 0.4693  |  0:00:53s
epoch 32 | loss: 0.21324 | val_0_rmse: 0.46436 | val_1_rmse: 0.48063 |  0:00:54s
epoch 33 | loss: 0.21369 | val_0_rmse: 0.44936 | val_1_rmse: 0.47158 |  0:00:56s
epoch 34 | loss: 0.21099 | val_0_rmse: 0.44705 | val_1_rmse: 0.46843 |  0:00:58s
epoch 35 | loss: 0.21118 | val_0_rmse: 0.44956 | val_1_rmse: 0.47539 |  0:00:59s
epoch 36 | loss: 0.20516 | val_0_rmse: 0.44293 | val_1_rmse: 0.46566 |  0:01:01s
epoch 37 | loss: 0.20593 | val_0_rmse: 0.44812 | val_1_rmse: 0.47523 |  0:01:03s
epoch 38 | loss: 0.20603 | val_0_rmse: 0.44705 | val_1_rmse: 0.47563 |  0:01:04s
epoch 39 | loss: 0.20772 | val_0_rmse: 0.44943 | val_1_rmse: 0.4768  |  0:01:06s
epoch 40 | loss: 0.20883 | val_0_rmse: 0.45369 | val_1_rmse: 0.47937 |  0:01:08s
epoch 41 | loss: 0.21394 | val_0_rmse: 0.45521 | val_1_rmse: 0.47841 |  0:01:09s
epoch 42 | loss: 0.21204 | val_0_rmse: 0.45178 | val_1_rmse: 0.47282 |  0:01:11s
epoch 43 | loss: 0.21526 | val_0_rmse: 0.44714 | val_1_rmse: 0.47576 |  0:01:13s
epoch 44 | loss: 0.2111  | val_0_rmse: 0.45034 | val_1_rmse: 0.4749  |  0:01:14s
epoch 45 | loss: 0.20443 | val_0_rmse: 0.45244 | val_1_rmse: 0.47789 |  0:01:16s
epoch 46 | loss: 0.20269 | val_0_rmse: 0.4514  | val_1_rmse: 0.48285 |  0:01:18s
epoch 47 | loss: 0.20517 | val_0_rmse: 0.43969 | val_1_rmse: 0.47118 |  0:01:19s
epoch 48 | loss: 0.21164 | val_0_rmse: 0.4473  | val_1_rmse: 0.47532 |  0:01:21s
epoch 49 | loss: 0.20347 | val_0_rmse: 0.44063 | val_1_rmse: 0.46735 |  0:01:23s
epoch 50 | loss: 0.20319 | val_0_rmse: 0.44001 | val_1_rmse: 0.4718  |  0:01:24s
epoch 51 | loss: 0.20066 | val_0_rmse: 0.46258 | val_1_rmse: 0.49164 |  0:01:26s
epoch 52 | loss: 0.20283 | val_0_rmse: 0.43718 | val_1_rmse: 0.46005 |  0:01:28s
epoch 53 | loss: 0.20232 | val_0_rmse: 0.44742 | val_1_rmse: 0.47298 |  0:01:29s
epoch 54 | loss: 0.20661 | val_0_rmse: 0.44514 | val_1_rmse: 0.46505 |  0:01:31s
epoch 55 | loss: 0.20371 | val_0_rmse: 0.44163 | val_1_rmse: 0.47615 |  0:01:33s
epoch 56 | loss: 0.20259 | val_0_rmse: 0.44638 | val_1_rmse: 0.47877 |  0:01:34s
epoch 57 | loss: 0.20133 | val_0_rmse: 0.44922 | val_1_rmse: 0.48068 |  0:01:36s
epoch 58 | loss: 0.20471 | val_0_rmse: 0.43567 | val_1_rmse: 0.46422 |  0:01:38s
epoch 59 | loss: 0.20021 | val_0_rmse: 0.43713 | val_1_rmse: 0.46721 |  0:01:39s
epoch 60 | loss: 0.19723 | val_0_rmse: 0.43735 | val_1_rmse: 0.4709  |  0:01:41s
epoch 61 | loss: 0.19886 | val_0_rmse: 0.43162 | val_1_rmse: 0.46554 |  0:01:43s
epoch 62 | loss: 0.19598 | val_0_rmse: 0.43934 | val_1_rmse: 0.47337 |  0:01:44s
epoch 63 | loss: 0.19686 | val_0_rmse: 0.43665 | val_1_rmse: 0.46559 |  0:01:46s
epoch 64 | loss: 0.19624 | val_0_rmse: 0.45452 | val_1_rmse: 0.48272 |  0:01:48s
epoch 65 | loss: 0.20158 | val_0_rmse: 0.45403 | val_1_rmse: 0.47619 |  0:01:49s
epoch 66 | loss: 0.21625 | val_0_rmse: 0.47672 | val_1_rmse: 0.50773 |  0:01:51s
epoch 67 | loss: 0.21185 | val_0_rmse: 0.45663 | val_1_rmse: 0.48406 |  0:01:53s
epoch 68 | loss: 0.20721 | val_0_rmse: 0.47319 | val_1_rmse: 0.49819 |  0:01:54s
epoch 69 | loss: 0.20254 | val_0_rmse: 0.44102 | val_1_rmse: 0.46769 |  0:01:56s
epoch 70 | loss: 0.19839 | val_0_rmse: 0.43221 | val_1_rmse: 0.45942 |  0:01:58s
epoch 71 | loss: 0.19516 | val_0_rmse: 0.43272 | val_1_rmse: 0.46447 |  0:01:59s
epoch 72 | loss: 0.19029 | val_0_rmse: 0.4261  | val_1_rmse: 0.45668 |  0:02:01s
epoch 73 | loss: 0.19019 | val_0_rmse: 0.4258  | val_1_rmse: 0.46508 |  0:02:03s
epoch 74 | loss: 0.18848 | val_0_rmse: 0.42484 | val_1_rmse: 0.46264 |  0:02:04s
epoch 75 | loss: 0.19062 | val_0_rmse: 0.43421 | val_1_rmse: 0.46763 |  0:02:06s
epoch 76 | loss: 0.189   | val_0_rmse: 0.42303 | val_1_rmse: 0.45947 |  0:02:08s
epoch 77 | loss: 0.18941 | val_0_rmse: 0.44011 | val_1_rmse: 0.4702  |  0:02:09s
epoch 78 | loss: 0.1914  | val_0_rmse: 0.42723 | val_1_rmse: 0.46793 |  0:02:11s
epoch 79 | loss: 0.18866 | val_0_rmse: 0.43085 | val_1_rmse: 0.47085 |  0:02:13s
epoch 80 | loss: 0.19088 | val_0_rmse: 0.4404  | val_1_rmse: 0.4764  |  0:02:14s
epoch 81 | loss: 0.18977 | val_0_rmse: 0.42123 | val_1_rmse: 0.4561  |  0:02:16s
epoch 82 | loss: 0.18317 | val_0_rmse: 0.41797 | val_1_rmse: 0.45332 |  0:02:18s
epoch 83 | loss: 0.1824  | val_0_rmse: 0.42227 | val_1_rmse: 0.45847 |  0:02:19s
epoch 84 | loss: 0.18382 | val_0_rmse: 0.42146 | val_1_rmse: 0.46093 |  0:02:21s
epoch 85 | loss: 0.18435 | val_0_rmse: 0.41719 | val_1_rmse: 0.45322 |  0:02:23s
epoch 86 | loss: 0.18237 | val_0_rmse: 0.42093 | val_1_rmse: 0.45804 |  0:02:24s
epoch 87 | loss: 0.18318 | val_0_rmse: 0.44567 | val_1_rmse: 0.4822  |  0:02:26s
epoch 88 | loss: 0.18103 | val_0_rmse: 0.41898 | val_1_rmse: 0.45717 |  0:02:28s
epoch 89 | loss: 0.18079 | val_0_rmse: 0.42619 | val_1_rmse: 0.46932 |  0:02:29s
epoch 90 | loss: 0.1816  | val_0_rmse: 0.41941 | val_1_rmse: 0.46349 |  0:02:31s
epoch 91 | loss: 0.17805 | val_0_rmse: 0.40949 | val_1_rmse: 0.45535 |  0:02:33s
epoch 92 | loss: 0.1759  | val_0_rmse: 0.41029 | val_1_rmse: 0.45418 |  0:02:34s
epoch 93 | loss: 0.17775 | val_0_rmse: 0.41247 | val_1_rmse: 0.45573 |  0:02:36s
epoch 94 | loss: 0.17746 | val_0_rmse: 0.4214  | val_1_rmse: 0.46678 |  0:02:38s
epoch 95 | loss: 0.18024 | val_0_rmse: 0.41423 | val_1_rmse: 0.45638 |  0:02:39s
epoch 96 | loss: 0.17903 | val_0_rmse: 0.41343 | val_1_rmse: 0.46029 |  0:02:41s
epoch 97 | loss: 0.17973 | val_0_rmse: 0.41042 | val_1_rmse: 0.45375 |  0:02:43s
epoch 98 | loss: 0.17391 | val_0_rmse: 0.41576 | val_1_rmse: 0.46228 |  0:02:44s
epoch 99 | loss: 0.17635 | val_0_rmse: 0.41554 | val_1_rmse: 0.4657  |  0:02:46s
epoch 100| loss: 0.17436 | val_0_rmse: 0.40748 | val_1_rmse: 0.4585  |  0:02:48s
epoch 101| loss: 0.17407 | val_0_rmse: 0.41588 | val_1_rmse: 0.46435 |  0:02:49s
epoch 102| loss: 0.18614 | val_0_rmse: 0.47003 | val_1_rmse: 0.50821 |  0:02:51s
epoch 103| loss: 0.18669 | val_0_rmse: 0.4233  | val_1_rmse: 0.46752 |  0:02:53s
epoch 104| loss: 0.18162 | val_0_rmse: 0.41397 | val_1_rmse: 0.45917 |  0:02:54s
epoch 105| loss: 0.18328 | val_0_rmse: 0.41367 | val_1_rmse: 0.45989 |  0:02:56s
epoch 106| loss: 0.17724 | val_0_rmse: 0.41402 | val_1_rmse: 0.46143 |  0:02:58s
epoch 107| loss: 0.17941 | val_0_rmse: 0.41178 | val_1_rmse: 0.45587 |  0:02:59s
epoch 108| loss: 0.17912 | val_0_rmse: 0.4094  | val_1_rmse: 0.45349 |  0:03:01s
epoch 109| loss: 0.17579 | val_0_rmse: 0.42288 | val_1_rmse: 0.46442 |  0:03:03s
epoch 110| loss: 0.17747 | val_0_rmse: 0.41214 | val_1_rmse: 0.458   |  0:03:04s
epoch 111| loss: 0.18509 | val_0_rmse: 0.42237 | val_1_rmse: 0.46291 |  0:03:06s
epoch 112| loss: 0.18065 | val_0_rmse: 0.4236  | val_1_rmse: 0.46408 |  0:03:08s
epoch 113| loss: 0.18211 | val_0_rmse: 0.41798 | val_1_rmse: 0.45465 |  0:03:09s
epoch 114| loss: 0.17551 | val_0_rmse: 0.48977 | val_1_rmse: 0.53143 |  0:03:11s
epoch 115| loss: 0.17609 | val_0_rmse: 0.4036  | val_1_rmse: 0.4561  |  0:03:13s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.45322
Best weights from best epoch are automatically used!
ended training at: 15:41:07
Feature importance:
Mean squared error is of 879717322.5877901
Mean absolute error:20221.12507390035
MAPE:0.24566673731166835
R2 score:0.8021321254266247
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:41:07
epoch 0  | loss: 1.51179 | val_0_rmse: 0.98196 | val_1_rmse: 0.99128 |  0:00:01s
epoch 1  | loss: 0.66857 | val_0_rmse: 0.73259 | val_1_rmse: 0.74849 |  0:00:03s
epoch 2  | loss: 0.37802 | val_0_rmse: 0.74876 | val_1_rmse: 0.75622 |  0:00:04s
epoch 3  | loss: 0.31585 | val_0_rmse: 0.58824 | val_1_rmse: 0.59724 |  0:00:06s
epoch 4  | loss: 0.29459 | val_0_rmse: 0.59843 | val_1_rmse: 0.60857 |  0:00:08s
epoch 5  | loss: 0.27102 | val_0_rmse: 0.55523 | val_1_rmse: 0.56415 |  0:00:09s
epoch 6  | loss: 0.25523 | val_0_rmse: 0.54974 | val_1_rmse: 0.56018 |  0:00:11s
epoch 7  | loss: 0.24002 | val_0_rmse: 0.52701 | val_1_rmse: 0.53416 |  0:00:13s
epoch 8  | loss: 0.22683 | val_0_rmse: 0.51829 | val_1_rmse: 0.52744 |  0:00:15s
epoch 9  | loss: 0.21933 | val_0_rmse: 0.49632 | val_1_rmse: 0.50279 |  0:00:16s
epoch 10 | loss: 0.21305 | val_0_rmse: 0.50383 | val_1_rmse: 0.50989 |  0:00:18s
epoch 11 | loss: 0.20577 | val_0_rmse: 0.493   | val_1_rmse: 0.49919 |  0:00:20s
epoch 12 | loss: 0.20814 | val_0_rmse: 0.48311 | val_1_rmse: 0.4896  |  0:00:21s
epoch 13 | loss: 0.20531 | val_0_rmse: 0.48136 | val_1_rmse: 0.48722 |  0:00:23s
epoch 14 | loss: 0.20213 | val_0_rmse: 0.48984 | val_1_rmse: 0.49741 |  0:00:25s
epoch 15 | loss: 0.2009  | val_0_rmse: 0.48002 | val_1_rmse: 0.48843 |  0:00:26s
epoch 16 | loss: 0.2002  | val_0_rmse: 0.47879 | val_1_rmse: 0.48546 |  0:00:28s
epoch 17 | loss: 0.19886 | val_0_rmse: 0.45987 | val_1_rmse: 0.46739 |  0:00:30s
epoch 18 | loss: 0.19693 | val_0_rmse: 0.50838 | val_1_rmse: 0.51527 |  0:00:31s
epoch 19 | loss: 0.19531 | val_0_rmse: 0.45132 | val_1_rmse: 0.46272 |  0:00:33s
epoch 20 | loss: 0.194   | val_0_rmse: 0.44832 | val_1_rmse: 0.45949 |  0:00:35s
epoch 21 | loss: 0.19173 | val_0_rmse: 0.44984 | val_1_rmse: 0.46226 |  0:00:36s
epoch 22 | loss: 0.19204 | val_0_rmse: 0.44544 | val_1_rmse: 0.4576  |  0:00:38s
epoch 23 | loss: 0.18704 | val_0_rmse: 0.43448 | val_1_rmse: 0.45292 |  0:00:39s
epoch 24 | loss: 0.18282 | val_0_rmse: 0.43605 | val_1_rmse: 0.45596 |  0:00:41s
epoch 25 | loss: 0.18455 | val_0_rmse: 0.43506 | val_1_rmse: 0.46066 |  0:00:43s
epoch 26 | loss: 0.18852 | val_0_rmse: 0.44207 | val_1_rmse: 0.46505 |  0:00:44s
epoch 27 | loss: 0.18436 | val_0_rmse: 0.44187 | val_1_rmse: 0.47225 |  0:00:46s
epoch 28 | loss: 0.18736 | val_0_rmse: 0.42749 | val_1_rmse: 0.4722  |  0:00:48s
epoch 29 | loss: 0.19013 | val_0_rmse: 0.42985 | val_1_rmse: 0.48035 |  0:00:49s
epoch 30 | loss: 0.18766 | val_0_rmse: 0.42106 | val_1_rmse: 0.47424 |  0:00:51s
epoch 31 | loss: 0.18236 | val_0_rmse: 0.42161 | val_1_rmse: 0.50591 |  0:00:53s
epoch 32 | loss: 0.18786 | val_0_rmse: 0.4106  | val_1_rmse: 0.48972 |  0:00:54s
epoch 33 | loss: 0.18227 | val_0_rmse: 0.41258 | val_1_rmse: 0.48842 |  0:00:56s
epoch 34 | loss: 0.18004 | val_0_rmse: 0.40689 | val_1_rmse: 0.54902 |  0:00:58s
epoch 35 | loss: 0.17974 | val_0_rmse: 0.41858 | val_1_rmse: 0.51807 |  0:00:59s
epoch 36 | loss: 0.17941 | val_0_rmse: 0.4115  | val_1_rmse: 0.50958 |  0:01:01s
epoch 37 | loss: 0.17771 | val_0_rmse: 0.4062  | val_1_rmse: 0.54894 |  0:01:03s
epoch 38 | loss: 0.17703 | val_0_rmse: 0.4016  | val_1_rmse: 0.56752 |  0:01:04s
epoch 39 | loss: 0.17307 | val_0_rmse: 0.39732 | val_1_rmse: 0.56635 |  0:01:06s
epoch 40 | loss: 0.17114 | val_0_rmse: 0.39998 | val_1_rmse: 0.73874 |  0:01:08s
epoch 41 | loss: 0.17211 | val_0_rmse: 0.4001  | val_1_rmse: 0.61794 |  0:01:09s
epoch 42 | loss: 0.17789 | val_0_rmse: 0.40497 | val_1_rmse: 0.56393 |  0:01:11s
epoch 43 | loss: 0.1764  | val_0_rmse: 0.39475 | val_1_rmse: 0.73892 |  0:01:13s
epoch 44 | loss: 0.17016 | val_0_rmse: 0.39592 | val_1_rmse: 0.7078  |  0:01:14s
epoch 45 | loss: 0.17449 | val_0_rmse: 0.39253 | val_1_rmse: 0.63473 |  0:01:16s
epoch 46 | loss: 0.16964 | val_0_rmse: 0.39614 | val_1_rmse: 0.79964 |  0:01:18s
epoch 47 | loss: 0.16929 | val_0_rmse: 0.38793 | val_1_rmse: 1.06842 |  0:01:19s
epoch 48 | loss: 0.16544 | val_0_rmse: 0.39704 | val_1_rmse: 0.59567 |  0:01:21s
epoch 49 | loss: 0.17602 | val_0_rmse: 0.44137 | val_1_rmse: 0.52929 |  0:01:23s
epoch 50 | loss: 0.17033 | val_0_rmse: 0.39232 | val_1_rmse: 0.4463  |  0:01:24s
epoch 51 | loss: 0.164   | val_0_rmse: 0.39062 | val_1_rmse: 0.45462 |  0:01:26s
epoch 52 | loss: 0.16492 | val_0_rmse: 0.38659 | val_1_rmse: 0.45307 |  0:01:28s
epoch 53 | loss: 0.16342 | val_0_rmse: 0.38898 | val_1_rmse: 0.44822 |  0:01:29s
epoch 54 | loss: 0.16471 | val_0_rmse: 0.40389 | val_1_rmse: 0.46615 |  0:01:31s
epoch 55 | loss: 0.16676 | val_0_rmse: 0.38715 | val_1_rmse: 0.45093 |  0:01:33s
epoch 56 | loss: 0.16723 | val_0_rmse: 0.38465 | val_1_rmse: 0.44558 |  0:01:34s
epoch 57 | loss: 0.16089 | val_0_rmse: 0.38545 | val_1_rmse: 0.45138 |  0:01:36s
epoch 58 | loss: 0.16388 | val_0_rmse: 0.39313 | val_1_rmse: 0.45836 |  0:01:38s
epoch 59 | loss: 0.16227 | val_0_rmse: 0.37961 | val_1_rmse: 0.4537  |  0:01:39s
epoch 60 | loss: 0.16335 | val_0_rmse: 0.40556 | val_1_rmse: 0.4773  |  0:01:41s
epoch 61 | loss: 0.16104 | val_0_rmse: 0.38243 | val_1_rmse: 0.4519  |  0:01:43s
epoch 62 | loss: 0.16261 | val_0_rmse: 0.39326 | val_1_rmse: 0.4667  |  0:01:44s
epoch 63 | loss: 0.16144 | val_0_rmse: 0.38487 | val_1_rmse: 0.45253 |  0:01:46s
epoch 64 | loss: 0.16102 | val_0_rmse: 0.38686 | val_1_rmse: 0.45513 |  0:01:48s
epoch 65 | loss: 0.16065 | val_0_rmse: 0.40282 | val_1_rmse: 0.47043 |  0:01:49s
epoch 66 | loss: 0.16076 | val_0_rmse: 0.38441 | val_1_rmse: 0.45412 |  0:01:51s
epoch 67 | loss: 0.15679 | val_0_rmse: 0.38498 | val_1_rmse: 0.45411 |  0:01:53s
epoch 68 | loss: 0.15705 | val_0_rmse: 0.39163 | val_1_rmse: 0.46034 |  0:01:54s
epoch 69 | loss: 0.16284 | val_0_rmse: 0.38456 | val_1_rmse: 0.4563  |  0:01:56s
epoch 70 | loss: 0.16219 | val_0_rmse: 0.37897 | val_1_rmse: 0.45001 |  0:01:58s
epoch 71 | loss: 0.15762 | val_0_rmse: 0.39541 | val_1_rmse: 0.4638  |  0:01:59s
epoch 72 | loss: 0.15793 | val_0_rmse: 0.37486 | val_1_rmse: 0.4532  |  0:02:01s
epoch 73 | loss: 0.15577 | val_0_rmse: 0.38206 | val_1_rmse: 0.45649 |  0:02:03s
epoch 74 | loss: 0.15781 | val_0_rmse: 0.37978 | val_1_rmse: 0.46411 |  0:02:04s
epoch 75 | loss: 0.15398 | val_0_rmse: 0.37807 | val_1_rmse: 0.4578  |  0:02:06s
epoch 76 | loss: 0.15343 | val_0_rmse: 0.37487 | val_1_rmse: 0.45876 |  0:02:08s
epoch 77 | loss: 0.152   | val_0_rmse: 0.37893 | val_1_rmse: 0.4558  |  0:02:09s
epoch 78 | loss: 0.15165 | val_0_rmse: 0.36979 | val_1_rmse: 0.45572 |  0:02:11s
epoch 79 | loss: 0.14937 | val_0_rmse: 0.373   | val_1_rmse: 0.46345 |  0:02:13s
epoch 80 | loss: 0.15327 | val_0_rmse: 0.37488 | val_1_rmse: 0.45616 |  0:02:14s
epoch 81 | loss: 0.1515  | val_0_rmse: 0.37289 | val_1_rmse: 0.45327 |  0:02:16s
epoch 82 | loss: 0.14869 | val_0_rmse: 0.37114 | val_1_rmse: 0.45845 |  0:02:18s
epoch 83 | loss: 0.15246 | val_0_rmse: 0.37263 | val_1_rmse: 0.46426 |  0:02:19s
epoch 84 | loss: 0.14899 | val_0_rmse: 0.37089 | val_1_rmse: 0.45696 |  0:02:21s
epoch 85 | loss: 0.15382 | val_0_rmse: 0.3717  | val_1_rmse: 0.46486 |  0:02:23s
epoch 86 | loss: 0.15107 | val_0_rmse: 0.36629 | val_1_rmse: 0.45864 |  0:02:24s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.44558
Best weights from best epoch are automatically used!
ended training at: 15:43:33
Feature importance:
Mean squared error is of 883079355.6728692
Mean absolute error:19971.144115592833
MAPE:0.2234185584012191
R2 score:0.8063389005790798
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:43:33
epoch 0  | loss: 1.59924 | val_0_rmse: 0.96423 | val_1_rmse: 0.98035 |  0:00:01s
epoch 1  | loss: 0.6291  | val_0_rmse: 0.70643 | val_1_rmse: 0.71698 |  0:00:03s
epoch 2  | loss: 0.41533 | val_0_rmse: 0.643   | val_1_rmse: 0.65445 |  0:00:04s
epoch 3  | loss: 0.34145 | val_0_rmse: 0.61516 | val_1_rmse: 0.62543 |  0:00:06s
epoch 4  | loss: 0.29769 | val_0_rmse: 0.58629 | val_1_rmse: 0.59133 |  0:00:08s
epoch 5  | loss: 0.26305 | val_0_rmse: 0.59479 | val_1_rmse: 0.60489 |  0:00:09s
epoch 6  | loss: 0.24796 | val_0_rmse: 0.55842 | val_1_rmse: 0.56793 |  0:00:11s
epoch 7  | loss: 0.23176 | val_0_rmse: 0.55522 | val_1_rmse: 0.56401 |  0:00:13s
epoch 8  | loss: 0.22987 | val_0_rmse: 0.52551 | val_1_rmse: 0.53103 |  0:00:14s
epoch 9  | loss: 0.21766 | val_0_rmse: 0.5368  | val_1_rmse: 0.54767 |  0:00:16s
epoch 10 | loss: 0.2222  | val_0_rmse: 0.50957 | val_1_rmse: 0.51704 |  0:00:18s
epoch 11 | loss: 0.21288 | val_0_rmse: 0.52382 | val_1_rmse: 0.53302 |  0:00:19s
epoch 12 | loss: 0.20355 | val_0_rmse: 0.50672 | val_1_rmse: 0.51864 |  0:00:21s
epoch 13 | loss: 0.20236 | val_0_rmse: 0.49295 | val_1_rmse: 0.50195 |  0:00:23s
epoch 14 | loss: 0.20149 | val_0_rmse: 0.49517 | val_1_rmse: 0.5059  |  0:00:24s
epoch 15 | loss: 0.19782 | val_0_rmse: 0.49299 | val_1_rmse: 0.50393 |  0:00:26s
epoch 16 | loss: 0.19398 | val_0_rmse: 0.47052 | val_1_rmse: 0.47929 |  0:00:28s
epoch 17 | loss: 0.19517 | val_0_rmse: 0.49109 | val_1_rmse: 0.50405 |  0:00:29s
epoch 18 | loss: 0.19332 | val_0_rmse: 0.46203 | val_1_rmse: 0.47382 |  0:00:31s
epoch 19 | loss: 0.19297 | val_0_rmse: 0.48428 | val_1_rmse: 0.49685 |  0:00:33s
epoch 20 | loss: 0.1882  | val_0_rmse: 0.46294 | val_1_rmse: 0.47939 |  0:00:34s
epoch 21 | loss: 0.18708 | val_0_rmse: 0.45125 | val_1_rmse: 0.46693 |  0:00:36s
epoch 22 | loss: 0.1852  | val_0_rmse: 0.44439 | val_1_rmse: 0.46348 |  0:00:38s
epoch 23 | loss: 0.18834 | val_0_rmse: 0.49771 | val_1_rmse: 0.51386 |  0:00:39s
epoch 24 | loss: 0.18454 | val_0_rmse: 0.45161 | val_1_rmse: 0.47139 |  0:00:41s
epoch 25 | loss: 0.18589 | val_0_rmse: 0.43109 | val_1_rmse: 0.45253 |  0:00:43s
epoch 26 | loss: 0.18323 | val_0_rmse: 0.43532 | val_1_rmse: 0.46004 |  0:00:44s
epoch 27 | loss: 0.18093 | val_0_rmse: 0.42464 | val_1_rmse: 0.45027 |  0:00:46s
epoch 28 | loss: 0.17922 | val_0_rmse: 0.41698 | val_1_rmse: 0.44769 |  0:00:48s
epoch 29 | loss: 0.17959 | val_0_rmse: 0.41499 | val_1_rmse: 0.4468  |  0:00:49s
epoch 30 | loss: 0.18193 | val_0_rmse: 0.41562 | val_1_rmse: 0.45204 |  0:00:51s
epoch 31 | loss: 0.18041 | val_0_rmse: 0.41054 | val_1_rmse: 0.44984 |  0:00:53s
epoch 32 | loss: 0.17956 | val_0_rmse: 0.44493 | val_1_rmse: 0.47964 |  0:00:54s
epoch 33 | loss: 0.17813 | val_0_rmse: 0.40293 | val_1_rmse: 0.44364 |  0:00:56s
epoch 34 | loss: 0.17586 | val_0_rmse: 0.40739 | val_1_rmse: 0.45189 |  0:00:58s
epoch 35 | loss: 0.17613 | val_0_rmse: 0.40175 | val_1_rmse: 0.45008 |  0:00:59s
epoch 36 | loss: 0.17766 | val_0_rmse: 0.40067 | val_1_rmse: 0.45056 |  0:01:01s
epoch 37 | loss: 0.17236 | val_0_rmse: 0.39296 | val_1_rmse: 0.4446  |  0:01:03s
epoch 38 | loss: 0.17261 | val_0_rmse: 0.39495 | val_1_rmse: 0.44796 |  0:01:04s
epoch 39 | loss: 0.17258 | val_0_rmse: 0.39725 | val_1_rmse: 0.45021 |  0:01:06s
epoch 40 | loss: 0.17144 | val_0_rmse: 0.39626 | val_1_rmse: 0.4547  |  0:01:08s
epoch 41 | loss: 0.17227 | val_0_rmse: 0.38991 | val_1_rmse: 0.44581 |  0:01:09s
epoch 42 | loss: 0.17071 | val_0_rmse: 0.39911 | val_1_rmse: 0.45912 |  0:01:11s
epoch 43 | loss: 0.1721  | val_0_rmse: 0.38778 | val_1_rmse: 0.44577 |  0:01:13s
epoch 44 | loss: 0.16704 | val_0_rmse: 0.38755 | val_1_rmse: 0.45063 |  0:01:14s
epoch 45 | loss: 0.16678 | val_0_rmse: 0.39252 | val_1_rmse: 0.45077 |  0:01:16s
epoch 46 | loss: 0.17055 | val_0_rmse: 0.38628 | val_1_rmse: 0.44917 |  0:01:18s
epoch 47 | loss: 0.16933 | val_0_rmse: 0.40367 | val_1_rmse: 0.45576 |  0:01:19s
epoch 48 | loss: 0.16606 | val_0_rmse: 0.40545 | val_1_rmse: 0.46018 |  0:01:21s
epoch 49 | loss: 0.1662  | val_0_rmse: 0.38664 | val_1_rmse: 0.45115 |  0:01:23s
epoch 50 | loss: 0.16572 | val_0_rmse: 0.38307 | val_1_rmse: 0.4471  |  0:01:24s
epoch 51 | loss: 0.16513 | val_0_rmse: 0.38425 | val_1_rmse: 0.45006 |  0:01:26s
epoch 52 | loss: 0.16321 | val_0_rmse: 0.38186 | val_1_rmse: 0.44849 |  0:01:28s
epoch 53 | loss: 0.16456 | val_0_rmse: 0.38019 | val_1_rmse: 0.45217 |  0:01:29s
epoch 54 | loss: 0.16247 | val_0_rmse: 0.38828 | val_1_rmse: 0.4575  |  0:01:31s
epoch 55 | loss: 0.16932 | val_0_rmse: 0.41639 | val_1_rmse: 0.47641 |  0:01:33s
epoch 56 | loss: 0.16779 | val_0_rmse: 0.38988 | val_1_rmse: 0.45386 |  0:01:34s
epoch 57 | loss: 0.16536 | val_0_rmse: 0.38094 | val_1_rmse: 0.45045 |  0:01:36s
epoch 58 | loss: 0.16632 | val_0_rmse: 0.39161 | val_1_rmse: 0.46365 |  0:01:38s
epoch 59 | loss: 0.16116 | val_0_rmse: 0.38096 | val_1_rmse: 0.4536  |  0:01:39s
epoch 60 | loss: 0.1587  | val_0_rmse: 0.39159 | val_1_rmse: 0.46198 |  0:01:41s
epoch 61 | loss: 0.16022 | val_0_rmse: 0.39039 | val_1_rmse: 0.46218 |  0:01:43s
epoch 62 | loss: 0.15913 | val_0_rmse: 0.38458 | val_1_rmse: 0.46571 |  0:01:44s
epoch 63 | loss: 0.16774 | val_0_rmse: 0.37396 | val_1_rmse: 0.4473  |  0:01:46s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.44364
Best weights from best epoch are automatically used!
ended training at: 15:45:20
Feature importance:
Mean squared error is of 819656301.0888617
Mean absolute error:19586.207451468978
MAPE:0.222627249840813
R2 score:0.8194356699021408
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:45:20
epoch 0  | loss: 1.54817 | val_0_rmse: 0.94297 | val_1_rmse: 0.95725 |  0:00:01s
epoch 1  | loss: 0.63221 | val_0_rmse: 0.93672 | val_1_rmse: 0.95005 |  0:00:03s
epoch 2  | loss: 0.39946 | val_0_rmse: 0.77573 | val_1_rmse: 0.79451 |  0:00:05s
epoch 3  | loss: 0.31152 | val_0_rmse: 0.62816 | val_1_rmse: 0.64568 |  0:00:06s
epoch 4  | loss: 0.29183 | val_0_rmse: 0.60416 | val_1_rmse: 0.61947 |  0:00:08s
epoch 5  | loss: 0.27159 | val_0_rmse: 0.58645 | val_1_rmse: 0.60007 |  0:00:10s
epoch 6  | loss: 0.24753 | val_0_rmse: 0.5512  | val_1_rmse: 0.56468 |  0:00:11s
epoch 7  | loss: 0.24571 | val_0_rmse: 0.55102 | val_1_rmse: 0.56132 |  0:00:13s
epoch 8  | loss: 0.2449  | val_0_rmse: 0.52316 | val_1_rmse: 0.53735 |  0:00:15s
epoch 9  | loss: 0.23556 | val_0_rmse: 0.54776 | val_1_rmse: 0.55943 |  0:00:16s
epoch 10 | loss: 0.23401 | val_0_rmse: 0.54321 | val_1_rmse: 0.55791 |  0:00:18s
epoch 11 | loss: 0.21986 | val_0_rmse: 0.51313 | val_1_rmse: 0.52659 |  0:00:20s
epoch 12 | loss: 0.21674 | val_0_rmse: 0.49555 | val_1_rmse: 0.51231 |  0:00:21s
epoch 13 | loss: 0.22284 | val_0_rmse: 0.57578 | val_1_rmse: 0.5915  |  0:00:23s
epoch 14 | loss: 0.22064 | val_0_rmse: 0.51218 | val_1_rmse: 0.5292  |  0:00:24s
epoch 15 | loss: 0.21331 | val_0_rmse: 0.48135 | val_1_rmse: 0.49831 |  0:00:26s
epoch 16 | loss: 0.22039 | val_0_rmse: 0.51488 | val_1_rmse: 0.52915 |  0:00:28s
epoch 17 | loss: 0.23602 | val_0_rmse: 0.48905 | val_1_rmse: 0.5014  |  0:00:29s
epoch 18 | loss: 0.22438 | val_0_rmse: 0.49819 | val_1_rmse: 0.51278 |  0:00:31s
epoch 19 | loss: 0.22564 | val_0_rmse: 0.47145 | val_1_rmse: 0.48852 |  0:00:33s
epoch 20 | loss: 0.22173 | val_0_rmse: 0.48747 | val_1_rmse: 0.50328 |  0:00:34s
epoch 21 | loss: 0.21156 | val_0_rmse: 0.46921 | val_1_rmse: 0.48906 |  0:00:36s
epoch 22 | loss: 0.21098 | val_0_rmse: 0.46902 | val_1_rmse: 0.48834 |  0:00:38s
epoch 23 | loss: 0.20877 | val_0_rmse: 0.45264 | val_1_rmse: 0.47264 |  0:00:39s
epoch 24 | loss: 0.20566 | val_0_rmse: 0.46745 | val_1_rmse: 0.48631 |  0:00:41s
epoch 25 | loss: 0.20104 | val_0_rmse: 0.45898 | val_1_rmse: 0.47693 |  0:00:43s
epoch 26 | loss: 0.20277 | val_0_rmse: 0.43913 | val_1_rmse: 0.46241 |  0:00:44s
epoch 27 | loss: 0.20069 | val_0_rmse: 0.4343  | val_1_rmse: 0.46062 |  0:00:46s
epoch 28 | loss: 0.1981  | val_0_rmse: 0.43184 | val_1_rmse: 0.4582  |  0:00:48s
epoch 29 | loss: 0.19849 | val_0_rmse: 0.43168 | val_1_rmse: 0.4613  |  0:00:49s
epoch 30 | loss: 0.19693 | val_0_rmse: 0.43202 | val_1_rmse: 0.46027 |  0:00:51s
epoch 31 | loss: 0.19205 | val_0_rmse: 0.42183 | val_1_rmse: 0.44934 |  0:00:53s
epoch 32 | loss: 0.19614 | val_0_rmse: 0.45057 | val_1_rmse: 0.47858 |  0:00:54s
epoch 33 | loss: 0.18874 | val_0_rmse: 0.42328 | val_1_rmse: 0.45634 |  0:00:56s
epoch 34 | loss: 0.18789 | val_0_rmse: 0.41944 | val_1_rmse: 0.45461 |  0:00:58s
epoch 35 | loss: 0.1852  | val_0_rmse: 0.42544 | val_1_rmse: 0.45683 |  0:00:59s
epoch 36 | loss: 0.18514 | val_0_rmse: 0.4177  | val_1_rmse: 0.45137 |  0:01:01s
epoch 37 | loss: 0.18391 | val_0_rmse: 0.41511 | val_1_rmse: 0.44695 |  0:01:03s
epoch 38 | loss: 0.18396 | val_0_rmse: 0.41309 | val_1_rmse: 0.44954 |  0:01:04s
epoch 39 | loss: 0.18285 | val_0_rmse: 0.41916 | val_1_rmse: 0.45352 |  0:01:06s
epoch 40 | loss: 0.18469 | val_0_rmse: 0.41149 | val_1_rmse: 0.44998 |  0:01:08s
epoch 41 | loss: 0.18437 | val_0_rmse: 0.41836 | val_1_rmse: 0.45391 |  0:01:09s
epoch 42 | loss: 0.18073 | val_0_rmse: 0.40881 | val_1_rmse: 0.44381 |  0:01:11s
epoch 43 | loss: 0.17869 | val_0_rmse: 0.41168 | val_1_rmse: 0.4512  |  0:01:13s
epoch 44 | loss: 0.18045 | val_0_rmse: 0.4187  | val_1_rmse: 0.45682 |  0:01:14s
epoch 45 | loss: 0.17874 | val_0_rmse: 0.41216 | val_1_rmse: 0.45245 |  0:01:16s
epoch 46 | loss: 0.17798 | val_0_rmse: 0.41092 | val_1_rmse: 0.45277 |  0:01:18s
epoch 47 | loss: 0.1824  | val_0_rmse: 0.42431 | val_1_rmse: 0.46091 |  0:01:19s
epoch 48 | loss: 0.18189 | val_0_rmse: 0.4124  | val_1_rmse: 0.45007 |  0:01:21s
epoch 49 | loss: 0.18011 | val_0_rmse: 0.41498 | val_1_rmse: 0.45261 |  0:01:23s
epoch 50 | loss: 0.17749 | val_0_rmse: 0.41252 | val_1_rmse: 0.4542  |  0:01:24s
epoch 51 | loss: 0.17737 | val_0_rmse: 0.41173 | val_1_rmse: 0.44966 |  0:01:26s
epoch 52 | loss: 0.17967 | val_0_rmse: 0.41472 | val_1_rmse: 0.45443 |  0:01:28s
epoch 53 | loss: 0.18561 | val_0_rmse: 0.43295 | val_1_rmse: 0.47607 |  0:01:29s
epoch 54 | loss: 0.18085 | val_0_rmse: 0.42188 | val_1_rmse: 0.46864 |  0:01:31s
epoch 55 | loss: 0.17921 | val_0_rmse: 0.40653 | val_1_rmse: 0.44905 |  0:01:33s
epoch 56 | loss: 0.17559 | val_0_rmse: 0.41678 | val_1_rmse: 0.46014 |  0:01:34s
epoch 57 | loss: 0.17522 | val_0_rmse: 0.41855 | val_1_rmse: 0.46649 |  0:01:36s
epoch 58 | loss: 0.18103 | val_0_rmse: 0.42167 | val_1_rmse: 0.46805 |  0:01:38s
epoch 59 | loss: 0.17952 | val_0_rmse: 0.40892 | val_1_rmse: 0.45699 |  0:01:39s
epoch 60 | loss: 0.17683 | val_0_rmse: 0.42048 | val_1_rmse: 0.46137 |  0:01:41s
epoch 61 | loss: 0.17724 | val_0_rmse: 0.41104 | val_1_rmse: 0.45402 |  0:01:43s
epoch 62 | loss: 0.17805 | val_0_rmse: 0.42397 | val_1_rmse: 0.4709  |  0:01:44s
epoch 63 | loss: 0.17502 | val_0_rmse: 0.40864 | val_1_rmse: 0.45513 |  0:01:46s
epoch 64 | loss: 0.17455 | val_0_rmse: 0.42417 | val_1_rmse: 0.46468 |  0:01:48s
epoch 65 | loss: 0.17424 | val_0_rmse: 0.40332 | val_1_rmse: 0.45317 |  0:01:49s
epoch 66 | loss: 0.17356 | val_0_rmse: 0.40315 | val_1_rmse: 0.45066 |  0:01:51s
epoch 67 | loss: 0.1691  | val_0_rmse: 0.40492 | val_1_rmse: 0.4568  |  0:01:53s
epoch 68 | loss: 0.17215 | val_0_rmse: 0.40642 | val_1_rmse: 0.45784 |  0:01:54s
epoch 69 | loss: 0.1777  | val_0_rmse: 0.43024 | val_1_rmse: 0.47933 |  0:01:56s
epoch 70 | loss: 0.18187 | val_0_rmse: 0.41648 | val_1_rmse: 0.47066 |  0:01:58s
epoch 71 | loss: 0.17697 | val_0_rmse: 0.40635 | val_1_rmse: 0.45767 |  0:01:59s
epoch 72 | loss: 0.17259 | val_0_rmse: 0.48768 | val_1_rmse: 0.4589  |  0:02:01s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.44381
Best weights from best epoch are automatically used!
ended training at: 15:47:22
Feature importance:
Mean squared error is of 893876997.5279703
Mean absolute error:20566.27851649948
MAPE:0.24204051455730277
R2 score:0.8045697972385457
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:47:22
epoch 0  | loss: 0.71729 | val_0_rmse: 0.74543 | val_1_rmse: 0.75186 |  0:00:03s
epoch 1  | loss: 0.39438 | val_0_rmse: 0.7365  | val_1_rmse: 0.74243 |  0:00:06s
epoch 2  | loss: 0.31604 | val_0_rmse: 0.70465 | val_1_rmse: 0.70714 |  0:00:10s
epoch 3  | loss: 0.29003 | val_0_rmse: 0.62093 | val_1_rmse: 0.62348 |  0:00:13s
epoch 4  | loss: 0.27278 | val_0_rmse: 0.59595 | val_1_rmse: 0.60036 |  0:00:17s
epoch 5  | loss: 0.26489 | val_0_rmse: 0.54796 | val_1_rmse: 0.55262 |  0:00:20s
epoch 6  | loss: 0.24458 | val_0_rmse: 0.56284 | val_1_rmse: 0.56371 |  0:00:24s
epoch 7  | loss: 0.25472 | val_0_rmse: 0.50168 | val_1_rmse: 0.50262 |  0:00:27s
epoch 8  | loss: 0.2386  | val_0_rmse: 0.4908  | val_1_rmse: 0.49499 |  0:00:31s
epoch 9  | loss: 0.22547 | val_0_rmse: 0.45502 | val_1_rmse: 0.45905 |  0:00:34s
epoch 10 | loss: 0.22223 | val_0_rmse: 0.45923 | val_1_rmse: 0.45892 |  0:00:37s
epoch 11 | loss: 0.21605 | val_0_rmse: 0.43598 | val_1_rmse: 0.4386  |  0:00:41s
epoch 12 | loss: 0.21616 | val_0_rmse: 0.45594 | val_1_rmse: 0.45739 |  0:00:44s
epoch 13 | loss: 0.21959 | val_0_rmse: 0.49303 | val_1_rmse: 0.4942  |  0:00:48s
epoch 14 | loss: 0.21488 | val_0_rmse: 0.43859 | val_1_rmse: 0.44175 |  0:00:51s
epoch 15 | loss: 0.21103 | val_0_rmse: 0.43031 | val_1_rmse: 0.43108 |  0:00:55s
epoch 16 | loss: 0.20437 | val_0_rmse: 0.42893 | val_1_rmse: 0.42832 |  0:00:58s
epoch 17 | loss: 0.19687 | val_0_rmse: 0.42064 | val_1_rmse: 0.4224  |  0:01:02s
epoch 18 | loss: 0.20245 | val_0_rmse: 0.42697 | val_1_rmse: 0.43083 |  0:01:05s
epoch 19 | loss: 0.20801 | val_0_rmse: 0.51214 | val_1_rmse: 0.50798 |  0:01:08s
epoch 20 | loss: 0.20691 | val_0_rmse: 0.4591  | val_1_rmse: 0.46019 |  0:01:12s
epoch 21 | loss: 0.19657 | val_0_rmse: 0.4206  | val_1_rmse: 0.42461 |  0:01:15s
epoch 22 | loss: 0.19482 | val_0_rmse: 0.46065 | val_1_rmse: 0.46183 |  0:01:19s
epoch 23 | loss: 0.19674 | val_0_rmse: 0.42154 | val_1_rmse: 0.42369 |  0:01:22s
epoch 24 | loss: 0.19877 | val_0_rmse: 0.4468  | val_1_rmse: 0.44196 |  0:01:26s
epoch 25 | loss: 0.20151 | val_0_rmse: 0.42494 | val_1_rmse: 0.42981 |  0:01:29s
epoch 26 | loss: 0.20588 | val_0_rmse: 0.47236 | val_1_rmse: 0.47779 |  0:01:33s
epoch 27 | loss: 0.23144 | val_0_rmse: 0.46115 | val_1_rmse: 0.46727 |  0:01:36s
epoch 28 | loss: 0.25646 | val_0_rmse: 0.49558 | val_1_rmse: 0.49616 |  0:01:39s
epoch 29 | loss: 0.26095 | val_0_rmse: 0.50816 | val_1_rmse: 0.51298 |  0:01:43s
epoch 30 | loss: 0.22866 | val_0_rmse: 0.46053 | val_1_rmse: 0.46596 |  0:01:46s
epoch 31 | loss: 0.23363 | val_0_rmse: 0.49356 | val_1_rmse: 0.4985  |  0:01:50s
epoch 32 | loss: 0.21497 | val_0_rmse: 0.43088 | val_1_rmse: 0.43404 |  0:01:53s
epoch 33 | loss: 0.19874 | val_0_rmse: 0.42253 | val_1_rmse: 0.42103 |  0:01:57s
epoch 34 | loss: 0.19733 | val_0_rmse: 0.42865 | val_1_rmse: 0.42952 |  0:02:00s
epoch 35 | loss: 0.19342 | val_0_rmse: 0.42274 | val_1_rmse: 0.4265  |  0:02:04s
epoch 36 | loss: 0.1941  | val_0_rmse: 0.41744 | val_1_rmse: 0.421   |  0:02:07s
epoch 37 | loss: 0.19836 | val_0_rmse: 0.42297 | val_1_rmse: 0.42508 |  0:02:10s
epoch 38 | loss: 0.19029 | val_0_rmse: 0.412   | val_1_rmse: 0.41467 |  0:02:14s
epoch 39 | loss: 0.18953 | val_0_rmse: 0.41742 | val_1_rmse: 0.41911 |  0:02:17s
epoch 40 | loss: 0.18988 | val_0_rmse: 0.42624 | val_1_rmse: 0.43084 |  0:02:21s
epoch 41 | loss: 0.18376 | val_0_rmse: 0.4119  | val_1_rmse: 0.41463 |  0:02:24s
epoch 42 | loss: 0.18437 | val_0_rmse: 0.41685 | val_1_rmse: 0.4204  |  0:02:28s
epoch 43 | loss: 0.18628 | val_0_rmse: 0.40901 | val_1_rmse: 0.41539 |  0:02:31s
epoch 44 | loss: 0.18383 | val_0_rmse: 0.41185 | val_1_rmse: 0.41699 |  0:02:35s
epoch 45 | loss: 0.18663 | val_0_rmse: 0.41391 | val_1_rmse: 0.41824 |  0:02:38s
epoch 46 | loss: 0.18721 | val_0_rmse: 0.41585 | val_1_rmse: 0.41811 |  0:02:41s
epoch 47 | loss: 0.18464 | val_0_rmse: 0.40619 | val_1_rmse: 0.40858 |  0:02:45s
epoch 48 | loss: 0.18027 | val_0_rmse: 0.40607 | val_1_rmse: 0.4105  |  0:02:48s
epoch 49 | loss: 0.18183 | val_0_rmse: 0.42471 | val_1_rmse: 0.42763 |  0:02:52s
epoch 50 | loss: 0.17846 | val_0_rmse: 0.41358 | val_1_rmse: 0.41616 |  0:02:55s
epoch 51 | loss: 0.17796 | val_0_rmse: 0.40078 | val_1_rmse: 0.40421 |  0:02:59s
epoch 52 | loss: 0.1796  | val_0_rmse: 0.40606 | val_1_rmse: 0.41152 |  0:03:02s
epoch 53 | loss: 0.17726 | val_0_rmse: 0.40064 | val_1_rmse: 0.4034  |  0:03:06s
epoch 54 | loss: 0.17835 | val_0_rmse: 0.40421 | val_1_rmse: 0.41126 |  0:03:09s
epoch 55 | loss: 0.17944 | val_0_rmse: 0.40967 | val_1_rmse: 0.41477 |  0:03:12s
epoch 56 | loss: 0.17459 | val_0_rmse: 0.41231 | val_1_rmse: 0.41552 |  0:03:16s
epoch 57 | loss: 0.17638 | val_0_rmse: 0.39666 | val_1_rmse: 0.40255 |  0:03:19s
epoch 58 | loss: 0.17598 | val_0_rmse: 0.39995 | val_1_rmse: 0.40508 |  0:03:23s
epoch 59 | loss: 0.17974 | val_0_rmse: 0.41416 | val_1_rmse: 0.41875 |  0:03:26s
epoch 60 | loss: 0.17738 | val_0_rmse: 0.40107 | val_1_rmse: 0.40896 |  0:03:30s
epoch 61 | loss: 0.17609 | val_0_rmse: 0.40396 | val_1_rmse: 0.40957 |  0:03:33s
epoch 62 | loss: 0.17428 | val_0_rmse: 0.425   | val_1_rmse: 0.43228 |  0:03:37s
epoch 63 | loss: 0.17714 | val_0_rmse: 0.40218 | val_1_rmse: 0.40828 |  0:03:40s
epoch 64 | loss: 0.17455 | val_0_rmse: 0.39528 | val_1_rmse: 0.40124 |  0:03:43s
epoch 65 | loss: 0.17261 | val_0_rmse: 0.39815 | val_1_rmse: 0.40521 |  0:03:47s
epoch 66 | loss: 0.1695  | val_0_rmse: 0.39059 | val_1_rmse: 0.397   |  0:03:51s
epoch 67 | loss: 0.17427 | val_0_rmse: 0.39554 | val_1_rmse: 0.40233 |  0:03:54s
epoch 68 | loss: 0.17288 | val_0_rmse: 0.40021 | val_1_rmse: 0.40649 |  0:03:57s
epoch 69 | loss: 0.17201 | val_0_rmse: 0.3956  | val_1_rmse: 0.40287 |  0:04:01s
epoch 70 | loss: 0.17151 | val_0_rmse: 0.4099  | val_1_rmse: 0.41651 |  0:04:04s
epoch 71 | loss: 0.17192 | val_0_rmse: 0.39263 | val_1_rmse: 0.40063 |  0:04:08s
epoch 72 | loss: 0.16952 | val_0_rmse: 0.39962 | val_1_rmse: 0.40854 |  0:04:11s
epoch 73 | loss: 0.17049 | val_0_rmse: 0.39019 | val_1_rmse: 0.40078 |  0:04:15s
epoch 74 | loss: 0.16878 | val_0_rmse: 0.38895 | val_1_rmse: 0.39895 |  0:04:18s
epoch 75 | loss: 0.17033 | val_0_rmse: 0.39006 | val_1_rmse: 0.39895 |  0:04:21s
epoch 76 | loss: 0.17336 | val_0_rmse: 0.42109 | val_1_rmse: 0.42984 |  0:04:25s
epoch 77 | loss: 0.17065 | val_0_rmse: 0.39193 | val_1_rmse: 0.40222 |  0:04:28s
epoch 78 | loss: 0.1705  | val_0_rmse: 0.39984 | val_1_rmse: 0.40835 |  0:04:32s
epoch 79 | loss: 0.16846 | val_0_rmse: 0.39802 | val_1_rmse: 0.40714 |  0:04:35s
epoch 80 | loss: 0.16917 | val_0_rmse: 0.39008 | val_1_rmse: 0.39692 |  0:04:39s
epoch 81 | loss: 0.16777 | val_0_rmse: 0.3893  | val_1_rmse: 0.40071 |  0:04:42s
epoch 82 | loss: 0.17083 | val_0_rmse: 0.39551 | val_1_rmse: 0.40463 |  0:04:46s
epoch 83 | loss: 0.16656 | val_0_rmse: 0.40364 | val_1_rmse: 0.41546 |  0:04:49s
epoch 84 | loss: 0.16882 | val_0_rmse: 0.3971  | val_1_rmse: 0.40531 |  0:04:52s
epoch 85 | loss: 0.16904 | val_0_rmse: 0.38828 | val_1_rmse: 0.39861 |  0:04:56s
epoch 86 | loss: 0.17236 | val_0_rmse: 0.39365 | val_1_rmse: 0.40674 |  0:04:59s
epoch 87 | loss: 0.1669  | val_0_rmse: 0.39621 | val_1_rmse: 0.40517 |  0:05:03s
epoch 88 | loss: 0.16629 | val_0_rmse: 0.38596 | val_1_rmse: 0.39565 |  0:05:06s
epoch 89 | loss: 0.16574 | val_0_rmse: 0.38895 | val_1_rmse: 0.3972  |  0:05:10s
epoch 90 | loss: 0.16625 | val_0_rmse: 0.38648 | val_1_rmse: 0.39618 |  0:05:13s
epoch 91 | loss: 0.16983 | val_0_rmse: 0.3947  | val_1_rmse: 0.4053  |  0:05:16s
epoch 92 | loss: 0.17045 | val_0_rmse: 0.42678 | val_1_rmse: 0.43824 |  0:05:20s
epoch 93 | loss: 0.1667  | val_0_rmse: 0.39092 | val_1_rmse: 0.4008  |  0:05:23s
epoch 94 | loss: 0.16348 | val_0_rmse: 0.39818 | val_1_rmse: 0.40786 |  0:05:27s
epoch 95 | loss: 0.16939 | val_0_rmse: 0.40833 | val_1_rmse: 0.41893 |  0:05:30s
epoch 96 | loss: 0.16653 | val_0_rmse: 0.38402 | val_1_rmse: 0.39699 |  0:05:34s
epoch 97 | loss: 0.16464 | val_0_rmse: 0.38573 | val_1_rmse: 0.39655 |  0:05:37s
epoch 98 | loss: 0.16678 | val_0_rmse: 0.3868  | val_1_rmse: 0.39941 |  0:05:41s
epoch 99 | loss: 0.16595 | val_0_rmse: 0.38313 | val_1_rmse: 0.39692 |  0:05:44s
epoch 100| loss: 0.16521 | val_0_rmse: 0.38837 | val_1_rmse: 0.39957 |  0:05:47s
epoch 101| loss: 0.16602 | val_0_rmse: 0.38812 | val_1_rmse: 0.40158 |  0:05:51s
epoch 102| loss: 0.1629  | val_0_rmse: 0.39219 | val_1_rmse: 0.40455 |  0:05:54s
epoch 103| loss: 0.16392 | val_0_rmse: 0.38605 | val_1_rmse: 0.3994  |  0:05:58s
epoch 104| loss: 0.16472 | val_0_rmse: 0.38448 | val_1_rmse: 0.3994  |  0:06:01s
epoch 105| loss: 0.16349 | val_0_rmse: 0.3891  | val_1_rmse: 0.40229 |  0:06:05s
epoch 106| loss: 0.16146 | val_0_rmse: 0.38361 | val_1_rmse: 0.39708 |  0:06:08s
epoch 107| loss: 0.16341 | val_0_rmse: 0.383   | val_1_rmse: 0.39998 |  0:06:11s
epoch 108| loss: 0.16256 | val_0_rmse: 0.38391 | val_1_rmse: 0.39816 |  0:06:15s
epoch 109| loss: 0.1626  | val_0_rmse: 0.40348 | val_1_rmse: 0.41421 |  0:06:18s
epoch 110| loss: 0.16662 | val_0_rmse: 0.39285 | val_1_rmse: 0.40727 |  0:06:22s
epoch 111| loss: 0.16524 | val_0_rmse: 0.38906 | val_1_rmse: 0.40421 |  0:06:25s
epoch 112| loss: 0.16677 | val_0_rmse: 0.38142 | val_1_rmse: 0.39619 |  0:06:29s
epoch 113| loss: 0.16243 | val_0_rmse: 0.38172 | val_1_rmse: 0.39494 |  0:06:32s
epoch 114| loss: 0.16459 | val_0_rmse: 0.38995 | val_1_rmse: 0.40377 |  0:06:36s
epoch 115| loss: 0.16397 | val_0_rmse: 0.38689 | val_1_rmse: 0.40184 |  0:06:39s
epoch 116| loss: 0.16389 | val_0_rmse: 0.38945 | val_1_rmse: 0.40369 |  0:06:42s
epoch 117| loss: 0.16481 | val_0_rmse: 0.38628 | val_1_rmse: 0.40128 |  0:06:46s
epoch 118| loss: 0.16279 | val_0_rmse: 0.38773 | val_1_rmse: 0.40347 |  0:06:49s
epoch 119| loss: 0.16226 | val_0_rmse: 0.38623 | val_1_rmse: 0.40185 |  0:06:53s
epoch 120| loss: 0.16119 | val_0_rmse: 0.39557 | val_1_rmse: 0.40917 |  0:06:56s
epoch 121| loss: 0.1616  | val_0_rmse: 0.39071 | val_1_rmse: 0.40409 |  0:07:00s
epoch 122| loss: 0.16264 | val_0_rmse: 0.40589 | val_1_rmse: 0.41867 |  0:07:03s
epoch 123| loss: 0.15889 | val_0_rmse: 0.38026 | val_1_rmse: 0.39822 |  0:07:07s
epoch 124| loss: 0.16172 | val_0_rmse: 0.39124 | val_1_rmse: 0.40815 |  0:07:10s
epoch 125| loss: 0.16356 | val_0_rmse: 0.37919 | val_1_rmse: 0.39751 |  0:07:13s
epoch 126| loss: 0.16171 | val_0_rmse: 0.38326 | val_1_rmse: 0.39793 |  0:07:17s
epoch 127| loss: 0.16121 | val_0_rmse: 0.37834 | val_1_rmse: 0.39499 |  0:07:20s
epoch 128| loss: 0.16045 | val_0_rmse: 0.38939 | val_1_rmse: 0.4037  |  0:07:24s
epoch 129| loss: 0.16494 | val_0_rmse: 0.41383 | val_1_rmse: 0.42815 |  0:07:27s
epoch 130| loss: 0.16698 | val_0_rmse: 0.39093 | val_1_rmse: 0.405   |  0:07:31s
epoch 131| loss: 0.16441 | val_0_rmse: 0.38183 | val_1_rmse: 0.39765 |  0:07:34s
epoch 132| loss: 0.16287 | val_0_rmse: 0.38244 | val_1_rmse: 0.40048 |  0:07:38s
epoch 133| loss: 0.16141 | val_0_rmse: 0.38787 | val_1_rmse: 0.40478 |  0:07:41s
epoch 134| loss: 0.16006 | val_0_rmse: 0.38937 | val_1_rmse: 0.40934 |  0:07:44s
epoch 135| loss: 0.16079 | val_0_rmse: 0.39054 | val_1_rmse: 0.40414 |  0:07:48s
epoch 136| loss: 0.16052 | val_0_rmse: 0.37905 | val_1_rmse: 0.39959 |  0:07:51s
epoch 137| loss: 0.16095 | val_0_rmse: 0.39021 | val_1_rmse: 0.40926 |  0:07:55s
epoch 138| loss: 0.16163 | val_0_rmse: 0.38112 | val_1_rmse: 0.39931 |  0:07:58s
epoch 139| loss: 0.1617  | val_0_rmse: 0.38923 | val_1_rmse: 0.40685 |  0:08:02s
epoch 140| loss: 0.16079 | val_0_rmse: 0.37808 | val_1_rmse: 0.39766 |  0:08:05s
epoch 141| loss: 0.16146 | val_0_rmse: 0.38375 | val_1_rmse: 0.40588 |  0:08:08s
epoch 142| loss: 0.15948 | val_0_rmse: 0.3872  | val_1_rmse: 0.40168 |  0:08:12s
epoch 143| loss: 0.15903 | val_0_rmse: 0.37727 | val_1_rmse: 0.39483 |  0:08:15s
epoch 144| loss: 0.15891 | val_0_rmse: 0.38338 | val_1_rmse: 0.3981  |  0:08:19s
epoch 145| loss: 0.15925 | val_0_rmse: 0.42004 | val_1_rmse: 0.43861 |  0:08:22s
epoch 146| loss: 0.17283 | val_0_rmse: 0.415   | val_1_rmse: 0.42809 |  0:08:26s
epoch 147| loss: 0.16823 | val_0_rmse: 0.38694 | val_1_rmse: 0.40259 |  0:08:29s
epoch 148| loss: 0.16608 | val_0_rmse: 0.3929  | val_1_rmse: 0.40495 |  0:08:33s
epoch 149| loss: 0.16762 | val_0_rmse: 0.38989 | val_1_rmse: 0.40383 |  0:08:36s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.39483
Best weights from best epoch are automatically used!
ended training at: 15:56:00
Feature importance:
Mean squared error is of 9598922142.47592
Mean absolute error:66693.61430175816
MAPE:0.28431996520044595
R2 score:0.8330069730107486
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:56:01
epoch 0  | loss: 0.63934 | val_0_rmse: 0.65944 | val_1_rmse: 0.65927 |  0:00:03s
epoch 1  | loss: 0.3306  | val_0_rmse: 0.58273 | val_1_rmse: 0.58054 |  0:00:06s
epoch 2  | loss: 0.28822 | val_0_rmse: 0.55106 | val_1_rmse: 0.54732 |  0:00:10s
epoch 3  | loss: 0.2649  | val_0_rmse: 0.56746 | val_1_rmse: 0.55683 |  0:00:13s
epoch 4  | loss: 0.25531 | val_0_rmse: 0.56667 | val_1_rmse: 0.56274 |  0:00:17s
epoch 5  | loss: 0.25862 | val_0_rmse: 0.51542 | val_1_rmse: 0.50955 |  0:00:20s
epoch 6  | loss: 0.25764 | val_0_rmse: 0.48189 | val_1_rmse: 0.47842 |  0:00:24s
epoch 7  | loss: 0.23511 | val_0_rmse: 0.48171 | val_1_rmse: 0.48275 |  0:00:27s
epoch 8  | loss: 0.25353 | val_0_rmse: 0.50459 | val_1_rmse: 0.5059  |  0:00:31s
epoch 9  | loss: 0.23289 | val_0_rmse: 0.49055 | val_1_rmse: 0.48812 |  0:00:34s
epoch 10 | loss: 0.23241 | val_0_rmse: 0.47984 | val_1_rmse: 0.47854 |  0:00:38s
epoch 11 | loss: 0.22338 | val_0_rmse: 0.45294 | val_1_rmse: 0.45213 |  0:00:41s
epoch 12 | loss: 0.21384 | val_0_rmse: 0.45413 | val_1_rmse: 0.45075 |  0:00:44s
epoch 13 | loss: 0.21882 | val_0_rmse: 0.44907 | val_1_rmse: 0.44735 |  0:00:48s
epoch 14 | loss: 0.20954 | val_0_rmse: 0.44303 | val_1_rmse: 0.44331 |  0:00:51s
epoch 15 | loss: 0.21319 | val_0_rmse: 0.45875 | val_1_rmse: 0.45752 |  0:00:55s
epoch 16 | loss: 0.21742 | val_0_rmse: 0.45189 | val_1_rmse: 0.45306 |  0:00:58s
epoch 17 | loss: 0.21623 | val_0_rmse: 0.45988 | val_1_rmse: 0.46081 |  0:01:02s
epoch 18 | loss: 0.21028 | val_0_rmse: 0.45356 | val_1_rmse: 0.45264 |  0:01:05s
epoch 19 | loss: 0.21168 | val_0_rmse: 0.4458  | val_1_rmse: 0.44981 |  0:01:09s
epoch 20 | loss: 0.21719 | val_0_rmse: 0.49353 | val_1_rmse: 0.49271 |  0:01:12s
epoch 21 | loss: 0.21957 | val_0_rmse: 0.43362 | val_1_rmse: 0.43554 |  0:01:16s
epoch 22 | loss: 0.20707 | val_0_rmse: 0.45578 | val_1_rmse: 0.45634 |  0:01:19s
epoch 23 | loss: 0.20688 | val_0_rmse: 0.44643 | val_1_rmse: 0.44546 |  0:01:22s
epoch 24 | loss: 0.20506 | val_0_rmse: 0.46862 | val_1_rmse: 0.4682  |  0:01:26s
epoch 25 | loss: 0.22433 | val_0_rmse: 0.45058 | val_1_rmse: 0.45114 |  0:01:29s
epoch 26 | loss: 0.22296 | val_0_rmse: 0.45825 | val_1_rmse: 0.45683 |  0:01:33s
epoch 27 | loss: 0.21154 | val_0_rmse: 0.4614  | val_1_rmse: 0.46047 |  0:01:36s
epoch 28 | loss: 0.21674 | val_0_rmse: 0.4788  | val_1_rmse: 0.48298 |  0:01:40s
epoch 29 | loss: 0.22487 | val_0_rmse: 0.45578 | val_1_rmse: 0.4569  |  0:01:43s
epoch 30 | loss: 0.21736 | val_0_rmse: 0.44765 | val_1_rmse: 0.44628 |  0:01:47s
epoch 31 | loss: 0.21977 | val_0_rmse: 0.46405 | val_1_rmse: 0.46858 |  0:01:50s
epoch 32 | loss: 0.21947 | val_0_rmse: 0.451   | val_1_rmse: 0.4501  |  0:01:54s
epoch 33 | loss: 0.21359 | val_0_rmse: 0.4527  | val_1_rmse: 0.45377 |  0:01:57s
epoch 34 | loss: 0.21017 | val_0_rmse: 0.44315 | val_1_rmse: 0.44394 |  0:02:00s
epoch 35 | loss: 0.2024  | val_0_rmse: 0.43783 | val_1_rmse: 0.43737 |  0:02:04s
epoch 36 | loss: 0.20518 | val_0_rmse: 0.45023 | val_1_rmse: 0.45284 |  0:02:07s
epoch 37 | loss: 0.20761 | val_0_rmse: 0.43621 | val_1_rmse: 0.43954 |  0:02:11s
epoch 38 | loss: 0.20864 | val_0_rmse: 0.46523 | val_1_rmse: 0.47075 |  0:02:14s
epoch 39 | loss: 0.20463 | val_0_rmse: 0.44685 | val_1_rmse: 0.4508  |  0:02:18s
epoch 40 | loss: 0.20981 | val_0_rmse: 0.49205 | val_1_rmse: 0.49371 |  0:02:21s
epoch 41 | loss: 0.22951 | val_0_rmse: 0.45132 | val_1_rmse: 0.45239 |  0:02:25s
epoch 42 | loss: 0.21102 | val_0_rmse: 0.45953 | val_1_rmse: 0.46467 |  0:02:28s
epoch 43 | loss: 0.2143  | val_0_rmse: 0.4471  | val_1_rmse: 0.45329 |  0:02:32s
epoch 44 | loss: 0.21015 | val_0_rmse: 0.47204 | val_1_rmse: 0.47654 |  0:02:35s
epoch 45 | loss: 0.22649 | val_0_rmse: 0.48448 | val_1_rmse: 0.48731 |  0:02:38s
epoch 46 | loss: 0.23356 | val_0_rmse: 0.50733 | val_1_rmse: 0.50843 |  0:02:42s
epoch 47 | loss: 0.23801 | val_0_rmse: 0.48163 | val_1_rmse: 0.47805 |  0:02:45s
epoch 48 | loss: 0.23065 | val_0_rmse: 0.46663 | val_1_rmse: 0.46629 |  0:02:49s
epoch 49 | loss: 0.21658 | val_0_rmse: 0.45765 | val_1_rmse: 0.45405 |  0:02:52s
epoch 50 | loss: 0.2157  | val_0_rmse: 0.46097 | val_1_rmse: 0.46338 |  0:02:56s
epoch 51 | loss: 0.21258 | val_0_rmse: 0.44835 | val_1_rmse: 0.45207 |  0:02:59s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.43554
Best weights from best epoch are automatically used!
ended training at: 15:59:02
Feature importance:
Mean squared error is of 11265266420.054163
Mean absolute error:74415.85516053686
MAPE:0.308701353123744
R2 score:0.8066304888966956
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:59:02
epoch 0  | loss: 0.60438 | val_0_rmse: 0.65705 | val_1_rmse: 0.65245 |  0:00:03s
epoch 1  | loss: 0.32286 | val_0_rmse: 0.65805 | val_1_rmse: 0.6547  |  0:00:06s
epoch 2  | loss: 0.28336 | val_0_rmse: 0.55526 | val_1_rmse: 0.5577  |  0:00:10s
epoch 3  | loss: 0.27187 | val_0_rmse: 0.5184  | val_1_rmse: 0.52077 |  0:00:13s
epoch 4  | loss: 0.25028 | val_0_rmse: 0.51707 | val_1_rmse: 0.51684 |  0:00:17s
epoch 5  | loss: 0.23415 | val_0_rmse: 0.50372 | val_1_rmse: 0.50178 |  0:00:20s
epoch 6  | loss: 0.23053 | val_0_rmse: 0.48322 | val_1_rmse: 0.48165 |  0:00:24s
epoch 7  | loss: 0.22483 | val_0_rmse: 0.46332 | val_1_rmse: 0.45881 |  0:00:27s
epoch 8  | loss: 0.218   | val_0_rmse: 0.45409 | val_1_rmse: 0.45276 |  0:00:31s
epoch 9  | loss: 0.21775 | val_0_rmse: 0.45613 | val_1_rmse: 0.45028 |  0:00:34s
epoch 10 | loss: 0.21293 | val_0_rmse: 0.4598  | val_1_rmse: 0.4577  |  0:00:37s
epoch 11 | loss: 0.20674 | val_0_rmse: 0.43312 | val_1_rmse: 0.43436 |  0:00:41s
epoch 12 | loss: 0.20718 | val_0_rmse: 0.43313 | val_1_rmse: 0.43505 |  0:00:44s
epoch 13 | loss: 0.20191 | val_0_rmse: 0.42696 | val_1_rmse: 0.42704 |  0:00:48s
epoch 14 | loss: 0.19837 | val_0_rmse: 0.44017 | val_1_rmse: 0.44045 |  0:00:51s
epoch 15 | loss: 0.19878 | val_0_rmse: 0.43127 | val_1_rmse: 0.4325  |  0:00:55s
epoch 16 | loss: 0.19646 | val_0_rmse: 0.43168 | val_1_rmse: 0.43496 |  0:00:58s
epoch 17 | loss: 0.19612 | val_0_rmse: 0.45325 | val_1_rmse: 0.45787 |  0:01:02s
epoch 18 | loss: 0.19399 | val_0_rmse: 0.42933 | val_1_rmse: 0.43394 |  0:01:05s
epoch 19 | loss: 0.1961  | val_0_rmse: 0.42608 | val_1_rmse: 0.43012 |  0:01:09s
epoch 20 | loss: 0.20244 | val_0_rmse: 0.43379 | val_1_rmse: 0.43734 |  0:01:12s
epoch 21 | loss: 0.19682 | val_0_rmse: 0.42513 | val_1_rmse: 0.42691 |  0:01:15s
epoch 22 | loss: 0.19421 | val_0_rmse: 0.43202 | val_1_rmse: 0.43537 |  0:01:19s
epoch 23 | loss: 0.19142 | val_0_rmse: 0.41582 | val_1_rmse: 0.41912 |  0:01:22s
epoch 24 | loss: 0.18851 | val_0_rmse: 0.41985 | val_1_rmse: 0.42379 |  0:01:26s
epoch 25 | loss: 0.18666 | val_0_rmse: 0.41301 | val_1_rmse: 0.41851 |  0:01:29s
epoch 26 | loss: 0.18723 | val_0_rmse: 0.41966 | val_1_rmse: 0.42364 |  0:01:33s
epoch 27 | loss: 0.18602 | val_0_rmse: 0.41932 | val_1_rmse: 0.42677 |  0:01:36s
epoch 28 | loss: 0.18304 | val_0_rmse: 0.41299 | val_1_rmse: 0.41865 |  0:01:40s
epoch 29 | loss: 0.18477 | val_0_rmse: 0.43659 | val_1_rmse: 0.44203 |  0:01:43s
epoch 30 | loss: 0.18515 | val_0_rmse: 0.41468 | val_1_rmse: 0.42247 |  0:01:46s
epoch 31 | loss: 0.18051 | val_0_rmse: 0.40853 | val_1_rmse: 0.41422 |  0:01:50s
epoch 32 | loss: 0.18134 | val_0_rmse: 0.41123 | val_1_rmse: 0.41802 |  0:01:53s
epoch 33 | loss: 0.18124 | val_0_rmse: 0.41121 | val_1_rmse: 0.41825 |  0:01:57s
epoch 34 | loss: 0.17944 | val_0_rmse: 0.42729 | val_1_rmse: 0.43572 |  0:02:00s
epoch 35 | loss: 0.18196 | val_0_rmse: 0.41137 | val_1_rmse: 0.41836 |  0:02:04s
epoch 36 | loss: 0.18264 | val_0_rmse: 0.41216 | val_1_rmse: 0.41947 |  0:02:07s
epoch 37 | loss: 0.18145 | val_0_rmse: 0.42594 | val_1_rmse: 0.43094 |  0:02:11s
epoch 38 | loss: 0.1787  | val_0_rmse: 0.40294 | val_1_rmse: 0.41143 |  0:02:14s
epoch 39 | loss: 0.17631 | val_0_rmse: 0.40646 | val_1_rmse: 0.41492 |  0:02:18s
epoch 40 | loss: 0.17733 | val_0_rmse: 0.39794 | val_1_rmse: 0.41004 |  0:02:21s
epoch 41 | loss: 0.17391 | val_0_rmse: 0.40157 | val_1_rmse: 0.41075 |  0:02:24s
epoch 42 | loss: 0.17493 | val_0_rmse: 0.40532 | val_1_rmse: 0.41525 |  0:02:28s
epoch 43 | loss: 0.1744  | val_0_rmse: 0.4001  | val_1_rmse: 0.41108 |  0:02:31s
epoch 44 | loss: 0.17407 | val_0_rmse: 0.40131 | val_1_rmse: 0.41163 |  0:02:35s
epoch 45 | loss: 0.1727  | val_0_rmse: 0.39517 | val_1_rmse: 0.40553 |  0:02:38s
epoch 46 | loss: 0.17225 | val_0_rmse: 0.40318 | val_1_rmse: 0.41361 |  0:02:42s
epoch 47 | loss: 0.1733  | val_0_rmse: 0.40532 | val_1_rmse: 0.41668 |  0:02:45s
epoch 48 | loss: 0.17505 | val_0_rmse: 0.40388 | val_1_rmse: 0.41424 |  0:02:49s
epoch 49 | loss: 0.17215 | val_0_rmse: 0.39893 | val_1_rmse: 0.41257 |  0:02:52s
epoch 50 | loss: 0.17291 | val_0_rmse: 0.39555 | val_1_rmse: 0.40756 |  0:02:56s
epoch 51 | loss: 0.1707  | val_0_rmse: 0.40534 | val_1_rmse: 0.41692 |  0:02:59s
epoch 52 | loss: 0.17241 | val_0_rmse: 0.40571 | val_1_rmse: 0.41936 |  0:03:02s
epoch 53 | loss: 0.17463 | val_0_rmse: 0.40938 | val_1_rmse: 0.41939 |  0:03:06s
epoch 54 | loss: 0.17914 | val_0_rmse: 0.40951 | val_1_rmse: 0.41812 |  0:03:09s
epoch 55 | loss: 0.19239 | val_0_rmse: 0.41692 | val_1_rmse: 0.42543 |  0:03:13s
epoch 56 | loss: 0.18215 | val_0_rmse: 0.43467 | val_1_rmse: 0.44395 |  0:03:16s
epoch 57 | loss: 0.18302 | val_0_rmse: 0.4237  | val_1_rmse: 0.43504 |  0:03:20s
epoch 58 | loss: 0.17764 | val_0_rmse: 0.4112  | val_1_rmse: 0.42083 |  0:03:23s
epoch 59 | loss: 0.17935 | val_0_rmse: 0.40356 | val_1_rmse: 0.41548 |  0:03:27s
epoch 60 | loss: 0.17449 | val_0_rmse: 0.40318 | val_1_rmse: 0.41575 |  0:03:30s
epoch 61 | loss: 0.17214 | val_0_rmse: 0.40636 | val_1_rmse: 0.42139 |  0:03:33s
epoch 62 | loss: 0.17451 | val_0_rmse: 0.40874 | val_1_rmse: 0.42135 |  0:03:37s
epoch 63 | loss: 0.17532 | val_0_rmse: 0.43953 | val_1_rmse: 0.44805 |  0:03:40s
epoch 64 | loss: 0.18781 | val_0_rmse: 0.42642 | val_1_rmse: 0.43244 |  0:03:44s
epoch 65 | loss: 0.21584 | val_0_rmse: 0.48605 | val_1_rmse: 0.49008 |  0:03:47s
epoch 66 | loss: 0.19372 | val_0_rmse: 0.41798 | val_1_rmse: 0.42521 |  0:03:51s
epoch 67 | loss: 0.18726 | val_0_rmse: 0.41702 | val_1_rmse: 0.42375 |  0:03:54s
epoch 68 | loss: 0.1853  | val_0_rmse: 0.40972 | val_1_rmse: 0.41764 |  0:03:58s
epoch 69 | loss: 0.18226 | val_0_rmse: 0.40792 | val_1_rmse: 0.41721 |  0:04:01s
epoch 70 | loss: 0.18264 | val_0_rmse: 0.40223 | val_1_rmse: 0.41204 |  0:04:05s
epoch 71 | loss: 0.18127 | val_0_rmse: 0.4153  | val_1_rmse: 0.42721 |  0:04:08s
epoch 72 | loss: 0.1786  | val_0_rmse: 0.41581 | val_1_rmse: 0.4268  |  0:04:11s
epoch 73 | loss: 0.17741 | val_0_rmse: 0.40232 | val_1_rmse: 0.41577 |  0:04:15s
epoch 74 | loss: 0.17713 | val_0_rmse: 0.40268 | val_1_rmse: 0.41593 |  0:04:18s
epoch 75 | loss: 0.18452 | val_0_rmse: 0.44771 | val_1_rmse: 0.45619 |  0:04:22s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.40553
Best weights from best epoch are automatically used!
ended training at: 16:03:25
Feature importance:
Mean squared error is of 9199939193.267035
Mean absolute error:66552.52307380139
MAPE:0.27794838212808654
R2 score:0.8403848119654316
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:03:26
epoch 0  | loss: 0.60634 | val_0_rmse: 0.69154 | val_1_rmse: 0.69086 |  0:00:03s
epoch 1  | loss: 0.33407 | val_0_rmse: 0.62233 | val_1_rmse: 0.62423 |  0:00:06s
epoch 2  | loss: 0.27629 | val_0_rmse: 0.63875 | val_1_rmse: 0.63773 |  0:00:10s
epoch 3  | loss: 0.25661 | val_0_rmse: 0.58869 | val_1_rmse: 0.58565 |  0:00:13s
epoch 4  | loss: 0.24046 | val_0_rmse: 0.54232 | val_1_rmse: 0.54447 |  0:00:17s
epoch 5  | loss: 0.23414 | val_0_rmse: 0.52079 | val_1_rmse: 0.52182 |  0:00:20s
epoch 6  | loss: 0.22818 | val_0_rmse: 0.48848 | val_1_rmse: 0.49176 |  0:00:24s
epoch 7  | loss: 0.22248 | val_0_rmse: 0.46254 | val_1_rmse: 0.46864 |  0:00:27s
epoch 8  | loss: 0.21759 | val_0_rmse: 0.46282 | val_1_rmse: 0.47098 |  0:00:31s
epoch 9  | loss: 0.21155 | val_0_rmse: 0.44132 | val_1_rmse: 0.44945 |  0:00:34s
epoch 10 | loss: 0.21104 | val_0_rmse: 0.44427 | val_1_rmse: 0.45059 |  0:00:38s
epoch 11 | loss: 0.21024 | val_0_rmse: 0.44246 | val_1_rmse: 0.44959 |  0:00:41s
epoch 12 | loss: 0.22349 | val_0_rmse: 0.4563  | val_1_rmse: 0.4594  |  0:00:45s
epoch 13 | loss: 0.26344 | val_0_rmse: 0.48015 | val_1_rmse: 0.49046 |  0:00:48s
epoch 14 | loss: 0.227   | val_0_rmse: 0.44791 | val_1_rmse: 0.4563  |  0:00:51s
epoch 15 | loss: 0.21487 | val_0_rmse: 0.45751 | val_1_rmse: 0.46339 |  0:00:55s
epoch 16 | loss: 0.2057  | val_0_rmse: 0.44183 | val_1_rmse: 0.45092 |  0:00:58s
epoch 17 | loss: 0.20401 | val_0_rmse: 0.42867 | val_1_rmse: 0.43847 |  0:01:02s
epoch 18 | loss: 0.19842 | val_0_rmse: 0.42678 | val_1_rmse: 0.43784 |  0:01:05s
epoch 19 | loss: 0.20023 | val_0_rmse: 0.43202 | val_1_rmse: 0.44188 |  0:01:09s
epoch 20 | loss: 0.19935 | val_0_rmse: 0.42573 | val_1_rmse: 0.43454 |  0:01:12s
epoch 21 | loss: 0.19283 | val_0_rmse: 0.41514 | val_1_rmse: 0.4244  |  0:01:16s
epoch 22 | loss: 0.19075 | val_0_rmse: 0.4265  | val_1_rmse: 0.43562 |  0:01:19s
epoch 23 | loss: 0.19342 | val_0_rmse: 0.4291  | val_1_rmse: 0.44071 |  0:01:23s
epoch 24 | loss: 0.19244 | val_0_rmse: 0.42352 | val_1_rmse: 0.43887 |  0:01:26s
epoch 25 | loss: 0.19158 | val_0_rmse: 0.41475 | val_1_rmse: 0.42768 |  0:01:29s
epoch 26 | loss: 0.18955 | val_0_rmse: 0.41635 | val_1_rmse: 0.42822 |  0:01:33s
epoch 27 | loss: 0.18919 | val_0_rmse: 0.41171 | val_1_rmse: 0.42382 |  0:01:36s
epoch 28 | loss: 0.18648 | val_0_rmse: 0.41499 | val_1_rmse: 0.42933 |  0:01:40s
epoch 29 | loss: 0.18407 | val_0_rmse: 0.41465 | val_1_rmse: 0.42947 |  0:01:43s
epoch 30 | loss: 0.18388 | val_0_rmse: 0.40952 | val_1_rmse: 0.42597 |  0:01:47s
epoch 31 | loss: 0.18224 | val_0_rmse: 0.41196 | val_1_rmse: 0.42636 |  0:01:50s
epoch 32 | loss: 0.18206 | val_0_rmse: 0.40578 | val_1_rmse: 0.41871 |  0:01:54s
epoch 33 | loss: 0.17962 | val_0_rmse: 0.44502 | val_1_rmse: 0.45454 |  0:01:57s
epoch 34 | loss: 0.17971 | val_0_rmse: 0.40747 | val_1_rmse: 0.42045 |  0:02:01s
epoch 35 | loss: 0.17932 | val_0_rmse: 0.40614 | val_1_rmse: 0.42254 |  0:02:04s
epoch 36 | loss: 0.17989 | val_0_rmse: 0.40111 | val_1_rmse: 0.41689 |  0:02:07s
epoch 37 | loss: 0.17944 | val_0_rmse: 0.40649 | val_1_rmse: 0.42325 |  0:02:11s
epoch 38 | loss: 0.18171 | val_0_rmse: 0.40005 | val_1_rmse: 0.41653 |  0:02:14s
epoch 39 | loss: 0.17565 | val_0_rmse: 0.39995 | val_1_rmse: 0.41552 |  0:02:18s
epoch 40 | loss: 0.17267 | val_0_rmse: 0.40153 | val_1_rmse: 0.41714 |  0:02:21s
epoch 41 | loss: 0.17364 | val_0_rmse: 0.42989 | val_1_rmse: 0.44134 |  0:02:25s
epoch 42 | loss: 0.18086 | val_0_rmse: 0.44778 | val_1_rmse: 0.46583 |  0:02:28s
epoch 43 | loss: 0.18535 | val_0_rmse: 0.44955 | val_1_rmse: 0.46278 |  0:02:32s
epoch 44 | loss: 0.18625 | val_0_rmse: 0.43766 | val_1_rmse: 0.44904 |  0:02:35s
epoch 45 | loss: 0.18373 | val_0_rmse: 0.41839 | val_1_rmse: 0.43186 |  0:02:38s
epoch 46 | loss: 0.18432 | val_0_rmse: 0.44452 | val_1_rmse: 0.46161 |  0:02:42s
epoch 47 | loss: 0.18424 | val_0_rmse: 0.42996 | val_1_rmse: 0.4426  |  0:02:45s
epoch 48 | loss: 0.17613 | val_0_rmse: 0.41436 | val_1_rmse: 0.4268  |  0:02:49s
epoch 49 | loss: 0.17327 | val_0_rmse: 0.40611 | val_1_rmse: 0.42002 |  0:02:53s
epoch 50 | loss: 0.17518 | val_0_rmse: 0.4028  | val_1_rmse: 0.41644 |  0:02:56s
epoch 51 | loss: 0.17447 | val_0_rmse: 0.39987 | val_1_rmse: 0.41583 |  0:02:59s
epoch 52 | loss: 0.17627 | val_0_rmse: 0.40793 | val_1_rmse: 0.42252 |  0:03:03s
epoch 53 | loss: 0.17888 | val_0_rmse: 0.41261 | val_1_rmse: 0.42712 |  0:03:06s
epoch 54 | loss: 0.17452 | val_0_rmse: 0.40414 | val_1_rmse: 0.42098 |  0:03:10s
epoch 55 | loss: 0.1702  | val_0_rmse: 0.41571 | val_1_rmse: 0.43132 |  0:03:13s
epoch 56 | loss: 0.17328 | val_0_rmse: 0.40709 | val_1_rmse: 0.42562 |  0:03:17s
epoch 57 | loss: 0.17229 | val_0_rmse: 0.39791 | val_1_rmse: 0.41604 |  0:03:20s
epoch 58 | loss: 0.18004 | val_0_rmse: 0.42182 | val_1_rmse: 0.43489 |  0:03:24s
epoch 59 | loss: 0.1996  | val_0_rmse: 0.46771 | val_1_rmse: 0.48381 |  0:03:27s
epoch 60 | loss: 0.2094  | val_0_rmse: 0.46843 | val_1_rmse: 0.4808  |  0:03:30s
epoch 61 | loss: 0.23085 | val_0_rmse: 0.51489 | val_1_rmse: 0.52366 |  0:03:34s
epoch 62 | loss: 0.28907 | val_0_rmse: 0.51908 | val_1_rmse: 0.52611 |  0:03:37s
epoch 63 | loss: 0.27128 | val_0_rmse: 0.55997 | val_1_rmse: 0.56278 |  0:03:41s
epoch 64 | loss: 0.26648 | val_0_rmse: 0.48291 | val_1_rmse: 0.48919 |  0:03:44s
epoch 65 | loss: 0.23947 | val_0_rmse: 0.4706  | val_1_rmse: 0.47988 |  0:03:48s
epoch 66 | loss: 0.21865 | val_0_rmse: 0.46272 | val_1_rmse: 0.46794 |  0:03:51s
epoch 67 | loss: 0.22663 | val_0_rmse: 0.47497 | val_1_rmse: 0.48127 |  0:03:55s
epoch 68 | loss: 0.20997 | val_0_rmse: 0.45254 | val_1_rmse: 0.45868 |  0:03:58s
epoch 69 | loss: 0.20435 | val_0_rmse: 0.44763 | val_1_rmse: 0.4566  |  0:04:02s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.41552
Best weights from best epoch are automatically used!
ended training at: 16:07:29
Feature importance:
Mean squared error is of 9616064109.152506
Mean absolute error:68172.55832430464
MAPE:0.2862458519082318
R2 score:0.8327837944875096
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:07:29
epoch 0  | loss: 0.66689 | val_0_rmse: 0.82913 | val_1_rmse: 0.83083 |  0:00:03s
epoch 1  | loss: 0.36911 | val_0_rmse: 0.63678 | val_1_rmse: 0.64694 |  0:00:06s
epoch 2  | loss: 0.29892 | val_0_rmse: 0.56662 | val_1_rmse: 0.57586 |  0:00:10s
epoch 3  | loss: 0.26727 | val_0_rmse: 0.59594 | val_1_rmse: 0.60083 |  0:00:13s
epoch 4  | loss: 0.24595 | val_0_rmse: 0.50318 | val_1_rmse: 0.5086  |  0:00:17s
epoch 5  | loss: 0.24159 | val_0_rmse: 0.50634 | val_1_rmse: 0.51053 |  0:00:20s
epoch 6  | loss: 0.22581 | val_0_rmse: 0.47099 | val_1_rmse: 0.47524 |  0:00:24s
epoch 7  | loss: 0.21784 | val_0_rmse: 0.48826 | val_1_rmse: 0.49176 |  0:00:27s
epoch 8  | loss: 0.21923 | val_0_rmse: 0.4692  | val_1_rmse: 0.47622 |  0:00:31s
epoch 9  | loss: 0.20971 | val_0_rmse: 0.45626 | val_1_rmse: 0.46453 |  0:00:34s
epoch 10 | loss: 0.20991 | val_0_rmse: 0.46575 | val_1_rmse: 0.47368 |  0:00:37s
epoch 11 | loss: 0.21799 | val_0_rmse: 0.45792 | val_1_rmse: 0.46932 |  0:00:41s
epoch 12 | loss: 0.21422 | val_0_rmse: 0.44841 | val_1_rmse: 0.45955 |  0:00:44s
epoch 13 | loss: 0.20371 | val_0_rmse: 0.44572 | val_1_rmse: 0.45526 |  0:00:48s
epoch 14 | loss: 0.2031  | val_0_rmse: 0.43463 | val_1_rmse: 0.44889 |  0:00:51s
epoch 15 | loss: 0.20294 | val_0_rmse: 0.43886 | val_1_rmse: 0.45225 |  0:00:55s
epoch 16 | loss: 0.2007  | val_0_rmse: 0.42232 | val_1_rmse: 0.43726 |  0:00:58s
epoch 17 | loss: 0.19533 | val_0_rmse: 0.4324  | val_1_rmse: 0.44761 |  0:01:02s
epoch 18 | loss: 0.19098 | val_0_rmse: 0.43066 | val_1_rmse: 0.44558 |  0:01:05s
epoch 19 | loss: 0.19139 | val_0_rmse: 0.4312  | val_1_rmse: 0.44727 |  0:01:08s
epoch 20 | loss: 0.188   | val_0_rmse: 0.41848 | val_1_rmse: 0.43438 |  0:01:12s
epoch 21 | loss: 0.1857  | val_0_rmse: 0.41399 | val_1_rmse: 0.42973 |  0:01:15s
epoch 22 | loss: 0.18701 | val_0_rmse: 0.44824 | val_1_rmse: 0.46201 |  0:01:19s
epoch 23 | loss: 0.18433 | val_0_rmse: 0.42551 | val_1_rmse: 0.44052 |  0:01:22s
epoch 24 | loss: 0.18807 | val_0_rmse: 0.43903 | val_1_rmse: 0.45365 |  0:01:26s
epoch 25 | loss: 0.18513 | val_0_rmse: 0.43835 | val_1_rmse: 0.45525 |  0:01:29s
epoch 26 | loss: 0.18509 | val_0_rmse: 0.42085 | val_1_rmse: 0.43834 |  0:01:33s
epoch 27 | loss: 0.18633 | val_0_rmse: 0.4149  | val_1_rmse: 0.43155 |  0:01:36s
epoch 28 | loss: 0.18655 | val_0_rmse: 0.42522 | val_1_rmse: 0.44033 |  0:01:40s
epoch 29 | loss: 0.18192 | val_0_rmse: 0.40519 | val_1_rmse: 0.42309 |  0:01:43s
epoch 30 | loss: 0.18186 | val_0_rmse: 0.41431 | val_1_rmse: 0.43244 |  0:01:47s
epoch 31 | loss: 0.17772 | val_0_rmse: 0.41387 | val_1_rmse: 0.43186 |  0:01:50s
epoch 32 | loss: 0.18005 | val_0_rmse: 0.39982 | val_1_rmse: 0.41807 |  0:01:53s
epoch 33 | loss: 0.17864 | val_0_rmse: 0.40323 | val_1_rmse: 0.42139 |  0:01:57s
epoch 34 | loss: 0.17677 | val_0_rmse: 0.39992 | val_1_rmse: 0.41931 |  0:02:00s
epoch 35 | loss: 0.1777  | val_0_rmse: 0.40422 | val_1_rmse: 0.42415 |  0:02:04s
epoch 36 | loss: 0.18769 | val_0_rmse: 0.43138 | val_1_rmse: 0.44894 |  0:02:07s
epoch 37 | loss: 0.17922 | val_0_rmse: 0.42221 | val_1_rmse: 0.43979 |  0:02:11s
epoch 38 | loss: 0.17766 | val_0_rmse: 0.44889 | val_1_rmse: 0.46696 |  0:02:14s
epoch 39 | loss: 0.17683 | val_0_rmse: 0.41776 | val_1_rmse: 0.43754 |  0:02:18s
epoch 40 | loss: 0.17436 | val_0_rmse: 0.40175 | val_1_rmse: 0.42306 |  0:02:21s
epoch 41 | loss: 0.17532 | val_0_rmse: 0.41861 | val_1_rmse: 0.43938 |  0:02:24s
epoch 42 | loss: 0.172   | val_0_rmse: 0.41874 | val_1_rmse: 0.438   |  0:02:28s
epoch 43 | loss: 0.17282 | val_0_rmse: 0.41844 | val_1_rmse: 0.43743 |  0:02:31s
epoch 44 | loss: 0.17486 | val_0_rmse: 0.40982 | val_1_rmse: 0.42999 |  0:02:35s
epoch 45 | loss: 0.17666 | val_0_rmse: 0.40671 | val_1_rmse: 0.42679 |  0:02:38s
epoch 46 | loss: 0.17173 | val_0_rmse: 0.40226 | val_1_rmse: 0.42452 |  0:02:42s
epoch 47 | loss: 0.17524 | val_0_rmse: 0.39812 | val_1_rmse: 0.41881 |  0:02:45s
epoch 48 | loss: 0.17011 | val_0_rmse: 0.40748 | val_1_rmse: 0.43063 |  0:02:49s
epoch 49 | loss: 0.16962 | val_0_rmse: 0.39627 | val_1_rmse: 0.4195  |  0:02:52s
epoch 50 | loss: 0.16952 | val_0_rmse: 0.4145  | val_1_rmse: 0.43521 |  0:02:56s
epoch 51 | loss: 0.1733  | val_0_rmse: 0.42133 | val_1_rmse: 0.44557 |  0:02:59s
epoch 52 | loss: 0.17142 | val_0_rmse: 0.41447 | val_1_rmse: 0.43822 |  0:03:02s
epoch 53 | loss: 0.16931 | val_0_rmse: 0.38927 | val_1_rmse: 0.41514 |  0:03:06s
epoch 54 | loss: 0.16956 | val_0_rmse: 0.39673 | val_1_rmse: 0.42207 |  0:03:09s
epoch 55 | loss: 0.16925 | val_0_rmse: 0.40279 | val_1_rmse: 0.42774 |  0:03:13s
epoch 56 | loss: 0.16847 | val_0_rmse: 0.39304 | val_1_rmse: 0.41821 |  0:03:16s
epoch 57 | loss: 0.17249 | val_0_rmse: 0.39437 | val_1_rmse: 0.41727 |  0:03:20s
epoch 58 | loss: 0.16829 | val_0_rmse: 0.38926 | val_1_rmse: 0.41388 |  0:03:23s
epoch 59 | loss: 0.16698 | val_0_rmse: 0.39437 | val_1_rmse: 0.42059 |  0:03:27s
epoch 60 | loss: 0.1652  | val_0_rmse: 0.4105  | val_1_rmse: 0.43619 |  0:03:30s
epoch 61 | loss: 0.17039 | val_0_rmse: 0.40404 | val_1_rmse: 0.42718 |  0:03:34s
epoch 62 | loss: 0.16601 | val_0_rmse: 0.39906 | val_1_rmse: 0.42366 |  0:03:37s
epoch 63 | loss: 0.16502 | val_0_rmse: 0.39615 | val_1_rmse: 0.42204 |  0:03:40s
epoch 64 | loss: 0.16251 | val_0_rmse: 0.38539 | val_1_rmse: 0.41443 |  0:03:44s
epoch 65 | loss: 0.1659  | val_0_rmse: 0.41925 | val_1_rmse: 0.44389 |  0:03:47s
epoch 66 | loss: 0.16527 | val_0_rmse: 0.3896  | val_1_rmse: 0.41437 |  0:03:51s
epoch 67 | loss: 0.1687  | val_0_rmse: 0.38973 | val_1_rmse: 0.41816 |  0:03:54s
epoch 68 | loss: 0.16209 | val_0_rmse: 0.39016 | val_1_rmse: 0.4168  |  0:03:58s
epoch 69 | loss: 0.16366 | val_0_rmse: 0.42204 | val_1_rmse: 0.44826 |  0:04:01s
epoch 70 | loss: 0.16634 | val_0_rmse: 0.39839 | val_1_rmse: 0.4294  |  0:04:05s
epoch 71 | loss: 0.16482 | val_0_rmse: 0.39212 | val_1_rmse: 0.41865 |  0:04:08s
epoch 72 | loss: 0.1652  | val_0_rmse: 0.40663 | val_1_rmse: 0.43167 |  0:04:12s
epoch 73 | loss: 0.16491 | val_0_rmse: 0.38764 | val_1_rmse: 0.41549 |  0:04:15s
epoch 74 | loss: 0.16224 | val_0_rmse: 0.39193 | val_1_rmse: 0.4208  |  0:04:18s
epoch 75 | loss: 0.16204 | val_0_rmse: 0.39989 | val_1_rmse: 0.42906 |  0:04:22s
epoch 76 | loss: 0.16437 | val_0_rmse: 0.37993 | val_1_rmse: 0.4104  |  0:04:25s
epoch 77 | loss: 0.16463 | val_0_rmse: 0.42047 | val_1_rmse: 0.44708 |  0:04:29s
epoch 78 | loss: 0.16278 | val_0_rmse: 0.40739 | val_1_rmse: 0.43502 |  0:04:32s
epoch 79 | loss: 0.16258 | val_0_rmse: 0.42229 | val_1_rmse: 0.44977 |  0:04:36s
epoch 80 | loss: 0.16268 | val_0_rmse: 0.38569 | val_1_rmse: 0.41671 |  0:04:39s
epoch 81 | loss: 0.16069 | val_0_rmse: 0.38314 | val_1_rmse: 0.41375 |  0:04:43s
epoch 82 | loss: 0.15903 | val_0_rmse: 0.38131 | val_1_rmse: 0.41343 |  0:04:46s
epoch 83 | loss: 0.15958 | val_0_rmse: 0.39344 | val_1_rmse: 0.42028 |  0:04:49s
epoch 84 | loss: 0.16533 | val_0_rmse: 0.40625 | val_1_rmse: 0.43382 |  0:04:53s
epoch 85 | loss: 0.16143 | val_0_rmse: 0.38233 | val_1_rmse: 0.41178 |  0:04:56s
epoch 86 | loss: 0.16273 | val_0_rmse: 0.39028 | val_1_rmse: 0.4236  |  0:05:00s
epoch 87 | loss: 0.15874 | val_0_rmse: 0.40459 | val_1_rmse: 0.43737 |  0:05:03s
epoch 88 | loss: 0.15668 | val_0_rmse: 0.3872  | val_1_rmse: 0.41974 |  0:05:07s
epoch 89 | loss: 0.16046 | val_0_rmse: 0.37644 | val_1_rmse: 0.41053 |  0:05:10s
epoch 90 | loss: 0.15882 | val_0_rmse: 0.38405 | val_1_rmse: 0.41756 |  0:05:14s
epoch 91 | loss: 0.1584  | val_0_rmse: 0.40708 | val_1_rmse: 0.43845 |  0:05:17s
epoch 92 | loss: 0.15773 | val_0_rmse: 0.37956 | val_1_rmse: 0.41341 |  0:05:20s
epoch 93 | loss: 0.15899 | val_0_rmse: 0.37561 | val_1_rmse: 0.40984 |  0:05:24s
epoch 94 | loss: 0.15683 | val_0_rmse: 0.37736 | val_1_rmse: 0.4133  |  0:05:27s
epoch 95 | loss: 0.15954 | val_0_rmse: 0.40114 | val_1_rmse: 0.43476 |  0:05:31s
epoch 96 | loss: 0.15718 | val_0_rmse: 0.37613 | val_1_rmse: 0.411   |  0:05:35s
epoch 97 | loss: 0.16134 | val_0_rmse: 0.39754 | val_1_rmse: 0.42906 |  0:05:38s
epoch 98 | loss: 0.15694 | val_0_rmse: 0.39684 | val_1_rmse: 0.43122 |  0:05:42s
epoch 99 | loss: 0.15571 | val_0_rmse: 0.38831 | val_1_rmse: 0.42011 |  0:05:45s
epoch 100| loss: 0.15861 | val_0_rmse: 0.38813 | val_1_rmse: 0.42128 |  0:05:48s
epoch 101| loss: 0.15835 | val_0_rmse: 0.38408 | val_1_rmse: 0.41605 |  0:05:52s
epoch 102| loss: 0.15728 | val_0_rmse: 0.39075 | val_1_rmse: 0.42559 |  0:05:55s
epoch 103| loss: 0.1553  | val_0_rmse: 0.37815 | val_1_rmse: 0.4128  |  0:05:59s
epoch 104| loss: 0.16082 | val_0_rmse: 0.40164 | val_1_rmse: 0.43375 |  0:06:02s
epoch 105| loss: 0.15802 | val_0_rmse: 0.37751 | val_1_rmse: 0.41217 |  0:06:06s
epoch 106| loss: 0.15638 | val_0_rmse: 0.3954  | val_1_rmse: 0.43062 |  0:06:09s
epoch 107| loss: 0.15425 | val_0_rmse: 0.38783 | val_1_rmse: 0.42218 |  0:06:13s
epoch 108| loss: 0.15463 | val_0_rmse: 0.37838 | val_1_rmse: 0.41522 |  0:06:16s
epoch 109| loss: 0.15926 | val_0_rmse: 0.3768  | val_1_rmse: 0.41143 |  0:06:19s
epoch 110| loss: 0.15579 | val_0_rmse: 0.38979 | val_1_rmse: 0.42553 |  0:06:23s
epoch 111| loss: 0.15602 | val_0_rmse: 0.37919 | val_1_rmse: 0.41614 |  0:06:26s
epoch 112| loss: 0.15514 | val_0_rmse: 0.39095 | val_1_rmse: 0.42544 |  0:06:30s
epoch 113| loss: 0.15647 | val_0_rmse: 0.3725  | val_1_rmse: 0.40922 |  0:06:33s
epoch 114| loss: 0.1565  | val_0_rmse: 0.39154 | val_1_rmse: 0.42601 |  0:06:37s
epoch 115| loss: 0.15281 | val_0_rmse: 0.40075 | val_1_rmse: 0.43698 |  0:06:40s
epoch 116| loss: 0.15584 | val_0_rmse: 0.39279 | val_1_rmse: 0.42678 |  0:06:44s
epoch 117| loss: 0.1552  | val_0_rmse: 0.40279 | val_1_rmse: 0.43991 |  0:06:47s
epoch 118| loss: 0.15641 | val_0_rmse: 0.37645 | val_1_rmse: 0.41188 |  0:06:51s
epoch 119| loss: 0.15443 | val_0_rmse: 0.38956 | val_1_rmse: 0.42578 |  0:06:54s
epoch 120| loss: 0.15799 | val_0_rmse: 0.38942 | val_1_rmse: 0.4265  |  0:06:58s
epoch 121| loss: 0.15508 | val_0_rmse: 0.37325 | val_1_rmse: 0.41081 |  0:07:01s
epoch 122| loss: 0.15251 | val_0_rmse: 0.3731  | val_1_rmse: 0.411   |  0:07:04s
epoch 123| loss: 0.15139 | val_0_rmse: 0.38044 | val_1_rmse: 0.41772 |  0:07:08s
epoch 124| loss: 0.15442 | val_0_rmse: 0.37253 | val_1_rmse: 0.4113  |  0:07:11s
epoch 125| loss: 0.15347 | val_0_rmse: 0.37159 | val_1_rmse: 0.40877 |  0:07:15s
epoch 126| loss: 0.15236 | val_0_rmse: 0.37657 | val_1_rmse: 0.4102  |  0:07:18s
epoch 127| loss: 0.15277 | val_0_rmse: 0.3913  | val_1_rmse: 0.42697 |  0:07:22s
epoch 128| loss: 0.1512  | val_0_rmse: 0.39397 | val_1_rmse: 0.4313  |  0:07:25s
epoch 129| loss: 0.15557 | val_0_rmse: 0.40114 | val_1_rmse: 0.43836 |  0:07:29s
epoch 130| loss: 0.15463 | val_0_rmse: 0.36921 | val_1_rmse: 0.40824 |  0:07:32s
epoch 131| loss: 0.15197 | val_0_rmse: 0.3717  | val_1_rmse: 0.41401 |  0:07:36s
epoch 132| loss: 0.15228 | val_0_rmse: 0.40879 | val_1_rmse: 0.44348 |  0:07:39s
epoch 133| loss: 0.15202 | val_0_rmse: 0.38614 | val_1_rmse: 0.42246 |  0:07:42s
epoch 134| loss: 0.15113 | val_0_rmse: 0.36834 | val_1_rmse: 0.4116  |  0:07:46s
epoch 135| loss: 0.15262 | val_0_rmse: 0.36934 | val_1_rmse: 0.40966 |  0:07:49s
epoch 136| loss: 0.15227 | val_0_rmse: 0.38792 | val_1_rmse: 0.42588 |  0:07:53s
epoch 137| loss: 0.15275 | val_0_rmse: 0.36491 | val_1_rmse: 0.40661 |  0:07:56s
epoch 138| loss: 0.15162 | val_0_rmse: 0.38594 | val_1_rmse: 0.43036 |  0:08:00s
epoch 139| loss: 0.1521  | val_0_rmse: 0.38977 | val_1_rmse: 0.43053 |  0:08:03s
epoch 140| loss: 0.15494 | val_0_rmse: 0.37335 | val_1_rmse: 0.41707 |  0:08:07s
epoch 141| loss: 0.15201 | val_0_rmse: 0.39452 | val_1_rmse: 0.42712 |  0:08:10s
epoch 142| loss: 0.17532 | val_0_rmse: 0.45644 | val_1_rmse: 0.48449 |  0:08:13s
epoch 143| loss: 0.16257 | val_0_rmse: 0.41429 | val_1_rmse: 0.44883 |  0:08:17s
epoch 144| loss: 0.16524 | val_0_rmse: 0.40041 | val_1_rmse: 0.43344 |  0:08:20s
epoch 145| loss: 0.16469 | val_0_rmse: 0.41199 | val_1_rmse: 0.44474 |  0:08:24s
epoch 146| loss: 0.15955 | val_0_rmse: 0.40188 | val_1_rmse: 0.43918 |  0:08:27s
epoch 147| loss: 0.16051 | val_0_rmse: 0.38695 | val_1_rmse: 0.42362 |  0:08:31s
epoch 148| loss: 0.15609 | val_0_rmse: 0.37787 | val_1_rmse: 0.41662 |  0:08:34s
epoch 149| loss: 0.15962 | val_0_rmse: 0.40655 | val_1_rmse: 0.44189 |  0:08:38s
Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_val_1_rmse = 0.40661
Best weights from best epoch are automatically used!
ended training at: 16:16:08
Feature importance:
Mean squared error is of 8945210440.617126
Mean absolute error:64624.18462025985
MAPE:0.26658575258579226
R2 score:0.8442228966845806
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:16:09
epoch 0  | loss: 1.26167 | val_0_rmse: 0.99151 | val_1_rmse: 1.00229 |  0:00:01s
epoch 1  | loss: 0.75537 | val_0_rmse: 0.91005 | val_1_rmse: 0.91287 |  0:00:02s
epoch 2  | loss: 0.49727 | val_0_rmse: 0.78796 | val_1_rmse: 0.77561 |  0:00:03s
epoch 3  | loss: 0.36472 | val_0_rmse: 0.64783 | val_1_rmse: 0.63875 |  0:00:04s
epoch 4  | loss: 0.32458 | val_0_rmse: 0.63597 | val_1_rmse: 0.6258  |  0:00:05s
epoch 5  | loss: 0.30386 | val_0_rmse: 0.6437  | val_1_rmse: 0.6327  |  0:00:06s
epoch 6  | loss: 0.28939 | val_0_rmse: 0.62007 | val_1_rmse: 0.61142 |  0:00:08s
epoch 7  | loss: 0.26928 | val_0_rmse: 0.58965 | val_1_rmse: 0.58698 |  0:00:09s
epoch 8  | loss: 0.26163 | val_0_rmse: 0.60439 | val_1_rmse: 0.59906 |  0:00:10s
epoch 9  | loss: 0.2547  | val_0_rmse: 0.5884  | val_1_rmse: 0.57788 |  0:00:11s
epoch 10 | loss: 0.24915 | val_0_rmse: 0.5679  | val_1_rmse: 0.5623  |  0:00:12s
epoch 11 | loss: 0.2449  | val_0_rmse: 0.57431 | val_1_rmse: 0.57134 |  0:00:13s
epoch 12 | loss: 0.24066 | val_0_rmse: 0.56674 | val_1_rmse: 0.55593 |  0:00:14s
epoch 13 | loss: 0.24132 | val_0_rmse: 0.55532 | val_1_rmse: 0.55053 |  0:00:16s
epoch 14 | loss: 0.23977 | val_0_rmse: 0.55175 | val_1_rmse: 0.55369 |  0:00:17s
epoch 15 | loss: 0.24056 | val_0_rmse: 0.54868 | val_1_rmse: 0.5483  |  0:00:18s
epoch 16 | loss: 0.23241 | val_0_rmse: 0.52899 | val_1_rmse: 0.5305  |  0:00:19s
epoch 17 | loss: 0.23233 | val_0_rmse: 0.53301 | val_1_rmse: 0.53666 |  0:00:20s
epoch 18 | loss: 0.23885 | val_0_rmse: 0.52992 | val_1_rmse: 0.52828 |  0:00:21s
epoch 19 | loss: 0.23223 | val_0_rmse: 0.5127  | val_1_rmse: 0.51384 |  0:00:22s
epoch 20 | loss: 0.22913 | val_0_rmse: 0.51246 | val_1_rmse: 0.51177 |  0:00:24s
epoch 21 | loss: 0.22842 | val_0_rmse: 0.51075 | val_1_rmse: 0.50999 |  0:00:25s
epoch 22 | loss: 0.22992 | val_0_rmse: 0.49602 | val_1_rmse: 0.49446 |  0:00:26s
epoch 23 | loss: 0.2325  | val_0_rmse: 0.52028 | val_1_rmse: 0.52517 |  0:00:27s
epoch 24 | loss: 0.23371 | val_0_rmse: 0.5076  | val_1_rmse: 0.50832 |  0:00:28s
epoch 25 | loss: 0.23426 | val_0_rmse: 0.49108 | val_1_rmse: 0.49001 |  0:00:29s
epoch 26 | loss: 0.22895 | val_0_rmse: 0.48551 | val_1_rmse: 0.48683 |  0:00:30s
epoch 27 | loss: 0.23586 | val_0_rmse: 0.48889 | val_1_rmse: 0.48928 |  0:00:32s
epoch 28 | loss: 0.23416 | val_0_rmse: 0.48431 | val_1_rmse: 0.48758 |  0:00:33s
epoch 29 | loss: 0.23365 | val_0_rmse: 0.48372 | val_1_rmse: 0.48296 |  0:00:34s
epoch 30 | loss: 0.23478 | val_0_rmse: 0.48455 | val_1_rmse: 0.48617 |  0:00:35s
epoch 31 | loss: 0.24412 | val_0_rmse: 0.48409 | val_1_rmse: 0.48174 |  0:00:36s
epoch 32 | loss: 0.23519 | val_0_rmse: 0.48074 | val_1_rmse: 0.47711 |  0:00:37s
epoch 33 | loss: 0.23521 | val_0_rmse: 0.48162 | val_1_rmse: 0.48008 |  0:00:38s
epoch 34 | loss: 0.23295 | val_0_rmse: 0.47597 | val_1_rmse: 0.47862 |  0:00:40s
epoch 35 | loss: 0.23831 | val_0_rmse: 0.47716 | val_1_rmse: 0.47327 |  0:00:41s
epoch 36 | loss: 0.23165 | val_0_rmse: 0.47311 | val_1_rmse: 0.47427 |  0:00:42s
epoch 37 | loss: 0.23122 | val_0_rmse: 0.46338 | val_1_rmse: 0.46369 |  0:00:43s
epoch 38 | loss: 0.22976 | val_0_rmse: 0.46267 | val_1_rmse: 0.46221 |  0:00:44s
epoch 39 | loss: 0.22758 | val_0_rmse: 0.4688  | val_1_rmse: 0.47046 |  0:00:45s
epoch 40 | loss: 0.2205  | val_0_rmse: 0.45828 | val_1_rmse: 0.4584  |  0:00:46s
epoch 41 | loss: 0.21715 | val_0_rmse: 0.4658  | val_1_rmse: 0.46366 |  0:00:48s
epoch 42 | loss: 0.22275 | val_0_rmse: 0.45697 | val_1_rmse: 0.45874 |  0:00:49s
epoch 43 | loss: 0.22013 | val_0_rmse: 0.45089 | val_1_rmse: 0.45243 |  0:00:50s
epoch 44 | loss: 0.22722 | val_0_rmse: 0.4734  | val_1_rmse: 0.47363 |  0:00:51s
epoch 45 | loss: 0.22868 | val_0_rmse: 0.46199 | val_1_rmse: 0.46092 |  0:00:52s
epoch 46 | loss: 0.22494 | val_0_rmse: 0.46243 | val_1_rmse: 0.45911 |  0:00:53s
epoch 47 | loss: 0.22528 | val_0_rmse: 0.45961 | val_1_rmse: 0.45582 |  0:00:54s
epoch 48 | loss: 0.22478 | val_0_rmse: 0.46108 | val_1_rmse: 0.45685 |  0:00:56s
epoch 49 | loss: 0.2232  | val_0_rmse: 0.45674 | val_1_rmse: 0.45207 |  0:00:57s
epoch 50 | loss: 0.2215  | val_0_rmse: 0.46669 | val_1_rmse: 0.46566 |  0:00:58s
epoch 51 | loss: 0.22216 | val_0_rmse: 0.46572 | val_1_rmse: 0.45907 |  0:00:59s
epoch 52 | loss: 0.22055 | val_0_rmse: 0.45325 | val_1_rmse: 0.45243 |  0:01:00s
epoch 53 | loss: 0.21844 | val_0_rmse: 0.46909 | val_1_rmse: 0.47307 |  0:01:01s
epoch 54 | loss: 0.22635 | val_0_rmse: 0.46722 | val_1_rmse: 0.47121 |  0:01:02s
epoch 55 | loss: 0.23011 | val_0_rmse: 0.4751  | val_1_rmse: 0.4768  |  0:01:04s
epoch 56 | loss: 0.23151 | val_0_rmse: 0.47066 | val_1_rmse: 0.46825 |  0:01:05s
epoch 57 | loss: 0.22244 | val_0_rmse: 0.48709 | val_1_rmse: 0.48761 |  0:01:06s
epoch 58 | loss: 0.22653 | val_0_rmse: 0.46008 | val_1_rmse: 0.46196 |  0:01:07s
epoch 59 | loss: 0.21843 | val_0_rmse: 0.45752 | val_1_rmse: 0.45652 |  0:01:08s
epoch 60 | loss: 0.22011 | val_0_rmse: 0.45386 | val_1_rmse: 0.45768 |  0:01:09s
epoch 61 | loss: 0.21963 | val_0_rmse: 0.45704 | val_1_rmse: 0.45455 |  0:01:10s
epoch 62 | loss: 0.2143  | val_0_rmse: 0.45463 | val_1_rmse: 0.45181 |  0:01:12s
epoch 63 | loss: 0.22644 | val_0_rmse: 0.47325 | val_1_rmse: 0.47514 |  0:01:13s
epoch 64 | loss: 0.22238 | val_0_rmse: 0.46943 | val_1_rmse: 0.4754  |  0:01:14s
epoch 65 | loss: 0.22158 | val_0_rmse: 0.46339 | val_1_rmse: 0.46288 |  0:01:15s
epoch 66 | loss: 0.21935 | val_0_rmse: 0.47299 | val_1_rmse: 0.46912 |  0:01:16s
epoch 67 | loss: 0.21532 | val_0_rmse: 0.45537 | val_1_rmse: 0.45494 |  0:01:17s
epoch 68 | loss: 0.21613 | val_0_rmse: 0.4485  | val_1_rmse: 0.44799 |  0:01:18s
epoch 69 | loss: 0.21091 | val_0_rmse: 0.45771 | val_1_rmse: 0.45855 |  0:01:20s
epoch 70 | loss: 0.21226 | val_0_rmse: 0.447   | val_1_rmse: 0.44772 |  0:01:21s
epoch 71 | loss: 0.20992 | val_0_rmse: 0.44741 | val_1_rmse: 0.44981 |  0:01:22s
epoch 72 | loss: 0.20852 | val_0_rmse: 0.44832 | val_1_rmse: 0.45071 |  0:01:23s
epoch 73 | loss: 0.21064 | val_0_rmse: 0.44537 | val_1_rmse: 0.44723 |  0:01:24s
epoch 74 | loss: 0.20781 | val_0_rmse: 0.44993 | val_1_rmse: 0.45707 |  0:01:25s
epoch 75 | loss: 0.20864 | val_0_rmse: 0.44681 | val_1_rmse: 0.45618 |  0:01:27s
epoch 76 | loss: 0.20805 | val_0_rmse: 0.44284 | val_1_rmse: 0.44516 |  0:01:28s
epoch 77 | loss: 0.20794 | val_0_rmse: 0.44504 | val_1_rmse: 0.44595 |  0:01:29s
epoch 78 | loss: 0.20599 | val_0_rmse: 0.44169 | val_1_rmse: 0.44672 |  0:01:30s
epoch 79 | loss: 0.20495 | val_0_rmse: 0.45143 | val_1_rmse: 0.45471 |  0:01:31s
epoch 80 | loss: 0.20894 | val_0_rmse: 0.44632 | val_1_rmse: 0.45974 |  0:01:32s
epoch 81 | loss: 0.20747 | val_0_rmse: 0.4527  | val_1_rmse: 0.45542 |  0:01:33s
epoch 82 | loss: 0.20643 | val_0_rmse: 0.44397 | val_1_rmse: 0.44768 |  0:01:34s
epoch 83 | loss: 0.2104  | val_0_rmse: 0.44952 | val_1_rmse: 0.45705 |  0:01:36s
epoch 84 | loss: 0.21651 | val_0_rmse: 0.45313 | val_1_rmse: 0.4573  |  0:01:37s
epoch 85 | loss: 0.20949 | val_0_rmse: 0.44894 | val_1_rmse: 0.44897 |  0:01:38s
epoch 86 | loss: 0.21381 | val_0_rmse: 0.44153 | val_1_rmse: 0.44509 |  0:01:39s
epoch 87 | loss: 0.20687 | val_0_rmse: 0.44059 | val_1_rmse: 0.44778 |  0:01:40s
epoch 88 | loss: 0.21262 | val_0_rmse: 0.50883 | val_1_rmse: 0.50782 |  0:01:41s
epoch 89 | loss: 0.22606 | val_0_rmse: 0.46787 | val_1_rmse: 0.46116 |  0:01:42s
epoch 90 | loss: 0.22843 | val_0_rmse: 0.46164 | val_1_rmse: 0.46009 |  0:01:44s
epoch 91 | loss: 0.21987 | val_0_rmse: 0.45374 | val_1_rmse: 0.4522  |  0:01:45s
epoch 92 | loss: 0.21408 | val_0_rmse: 0.45047 | val_1_rmse: 0.45422 |  0:01:46s
epoch 93 | loss: 0.2121  | val_0_rmse: 0.44822 | val_1_rmse: 0.4494  |  0:01:47s
epoch 94 | loss: 0.21012 | val_0_rmse: 0.45511 | val_1_rmse: 0.46677 |  0:01:48s
epoch 95 | loss: 0.21113 | val_0_rmse: 0.46482 | val_1_rmse: 0.47434 |  0:01:49s
epoch 96 | loss: 0.22152 | val_0_rmse: 0.45867 | val_1_rmse: 0.46435 |  0:01:50s
epoch 97 | loss: 0.22137 | val_0_rmse: 0.46667 | val_1_rmse: 0.45711 |  0:01:52s
epoch 98 | loss: 0.22249 | val_0_rmse: 0.48162 | val_1_rmse: 0.47798 |  0:01:53s
epoch 99 | loss: 0.21974 | val_0_rmse: 0.47185 | val_1_rmse: 0.47411 |  0:01:54s
epoch 100| loss: 0.21951 | val_0_rmse: 0.46105 | val_1_rmse: 0.45524 |  0:01:55s
epoch 101| loss: 0.21243 | val_0_rmse: 0.45586 | val_1_rmse: 0.45415 |  0:01:56s
epoch 102| loss: 0.21347 | val_0_rmse: 0.45397 | val_1_rmse: 0.45285 |  0:01:57s
epoch 103| loss: 0.21227 | val_0_rmse: 0.44763 | val_1_rmse: 0.45041 |  0:01:58s
epoch 104| loss: 0.21007 | val_0_rmse: 0.44602 | val_1_rmse: 0.45158 |  0:02:00s
epoch 105| loss: 0.20708 | val_0_rmse: 0.44217 | val_1_rmse: 0.44849 |  0:02:01s
epoch 106| loss: 0.20627 | val_0_rmse: 0.44877 | val_1_rmse: 0.45217 |  0:02:02s
epoch 107| loss: 0.20758 | val_0_rmse: 0.44238 | val_1_rmse: 0.44928 |  0:02:03s
epoch 108| loss: 0.20603 | val_0_rmse: 0.44537 | val_1_rmse: 0.45532 |  0:02:04s
epoch 109| loss: 0.20503 | val_0_rmse: 0.44045 | val_1_rmse: 0.4475  |  0:02:05s
epoch 110| loss: 0.20802 | val_0_rmse: 0.45405 | val_1_rmse: 0.45771 |  0:02:06s
epoch 111| loss: 0.20502 | val_0_rmse: 0.451   | val_1_rmse: 0.45886 |  0:02:08s
epoch 112| loss: 0.20298 | val_0_rmse: 0.44879 | val_1_rmse: 0.45809 |  0:02:09s
epoch 113| loss: 0.20573 | val_0_rmse: 0.44779 | val_1_rmse: 0.45287 |  0:02:10s
epoch 114| loss: 0.21287 | val_0_rmse: 0.44745 | val_1_rmse: 0.45633 |  0:02:11s
epoch 115| loss: 0.20355 | val_0_rmse: 0.43666 | val_1_rmse: 0.45089 |  0:02:12s
epoch 116| loss: 0.19884 | val_0_rmse: 0.43973 | val_1_rmse: 0.45819 |  0:02:13s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.44509
Best weights from best epoch are automatically used!
ended training at: 16:18:23
Feature importance:
Mean squared error is of 6421498348.967017
Mean absolute error:57771.72724815061
MAPE:0.15571869212106293
R2 score:0.7933418330545969
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:18:23
epoch 0  | loss: 1.34205 | val_0_rmse: 0.9872  | val_1_rmse: 0.97844 |  0:00:01s
epoch 1  | loss: 0.70698 | val_0_rmse: 0.91316 | val_1_rmse: 0.89073 |  0:00:02s
epoch 2  | loss: 0.51961 | val_0_rmse: 0.71626 | val_1_rmse: 0.70208 |  0:00:03s
epoch 3  | loss: 0.45877 | val_0_rmse: 0.62296 | val_1_rmse: 0.60929 |  0:00:04s
epoch 4  | loss: 0.39047 | val_0_rmse: 0.61332 | val_1_rmse: 0.60458 |  0:00:05s
epoch 5  | loss: 0.3359  | val_0_rmse: 0.59101 | val_1_rmse: 0.58697 |  0:00:06s
epoch 6  | loss: 0.30283 | val_0_rmse: 0.68633 | val_1_rmse: 0.67324 |  0:00:08s
epoch 7  | loss: 0.33122 | val_0_rmse: 0.58672 | val_1_rmse: 0.57653 |  0:00:09s
epoch 8  | loss: 0.28238 | val_0_rmse: 0.53577 | val_1_rmse: 0.52195 |  0:00:10s
epoch 9  | loss: 0.26414 | val_0_rmse: 0.5515  | val_1_rmse: 0.53327 |  0:00:11s
epoch 10 | loss: 0.25101 | val_0_rmse: 0.53903 | val_1_rmse: 0.52427 |  0:00:12s
epoch 11 | loss: 0.24508 | val_0_rmse: 0.58132 | val_1_rmse: 0.56703 |  0:00:13s
epoch 12 | loss: 0.23538 | val_0_rmse: 0.5665  | val_1_rmse: 0.55264 |  0:00:14s
epoch 13 | loss: 0.23301 | val_0_rmse: 0.53603 | val_1_rmse: 0.52316 |  0:00:16s
epoch 14 | loss: 0.23026 | val_0_rmse: 0.52989 | val_1_rmse: 0.52056 |  0:00:17s
epoch 15 | loss: 0.22738 | val_0_rmse: 0.52449 | val_1_rmse: 0.5162  |  0:00:18s
epoch 16 | loss: 0.22951 | val_0_rmse: 0.5468  | val_1_rmse: 0.53233 |  0:00:19s
epoch 17 | loss: 0.22752 | val_0_rmse: 0.51716 | val_1_rmse: 0.50633 |  0:00:20s
epoch 18 | loss: 0.22443 | val_0_rmse: 0.52218 | val_1_rmse: 0.51389 |  0:00:21s
epoch 19 | loss: 0.23063 | val_0_rmse: 0.51478 | val_1_rmse: 0.50917 |  0:00:22s
epoch 20 | loss: 0.22299 | val_0_rmse: 0.51743 | val_1_rmse: 0.508   |  0:00:24s
epoch 21 | loss: 0.21819 | val_0_rmse: 0.49713 | val_1_rmse: 0.49188 |  0:00:25s
epoch 22 | loss: 0.21898 | val_0_rmse: 0.53724 | val_1_rmse: 0.52603 |  0:00:26s
epoch 23 | loss: 0.21679 | val_0_rmse: 0.49091 | val_1_rmse: 0.4876  |  0:00:27s
epoch 24 | loss: 0.21415 | val_0_rmse: 0.48662 | val_1_rmse: 0.48084 |  0:00:28s
epoch 25 | loss: 0.21797 | val_0_rmse: 0.48698 | val_1_rmse: 0.48034 |  0:00:29s
epoch 26 | loss: 0.21239 | val_0_rmse: 0.47187 | val_1_rmse: 0.46753 |  0:00:30s
epoch 27 | loss: 0.21991 | val_0_rmse: 0.47926 | val_1_rmse: 0.47506 |  0:00:32s
epoch 28 | loss: 0.21603 | val_0_rmse: 0.50247 | val_1_rmse: 0.49507 |  0:00:33s
epoch 29 | loss: 0.21987 | val_0_rmse: 0.4706  | val_1_rmse: 0.46626 |  0:00:34s
epoch 30 | loss: 0.21822 | val_0_rmse: 0.4654  | val_1_rmse: 0.46355 |  0:00:35s
epoch 31 | loss: 0.2133  | val_0_rmse: 0.45252 | val_1_rmse: 0.45343 |  0:00:36s
epoch 32 | loss: 0.21219 | val_0_rmse: 0.45264 | val_1_rmse: 0.45255 |  0:00:37s
epoch 33 | loss: 0.20671 | val_0_rmse: 0.45414 | val_1_rmse: 0.45296 |  0:00:38s
epoch 34 | loss: 0.2092  | val_0_rmse: 0.45533 | val_1_rmse: 0.45455 |  0:00:40s
epoch 35 | loss: 0.22022 | val_0_rmse: 0.45957 | val_1_rmse: 0.46331 |  0:00:41s
epoch 36 | loss: 0.20945 | val_0_rmse: 0.46015 | val_1_rmse: 0.45991 |  0:00:42s
epoch 37 | loss: 0.2099  | val_0_rmse: 0.44826 | val_1_rmse: 0.44811 |  0:00:43s
epoch 38 | loss: 0.21557 | val_0_rmse: 0.46003 | val_1_rmse: 0.46602 |  0:00:44s
epoch 39 | loss: 0.21467 | val_0_rmse: 0.45441 | val_1_rmse: 0.45924 |  0:00:45s
epoch 40 | loss: 0.21962 | val_0_rmse: 0.44386 | val_1_rmse: 0.44882 |  0:00:46s
epoch 41 | loss: 0.22032 | val_0_rmse: 0.43862 | val_1_rmse: 0.44079 |  0:00:48s
epoch 42 | loss: 0.20831 | val_0_rmse: 0.44014 | val_1_rmse: 0.44509 |  0:00:49s
epoch 43 | loss: 0.2084  | val_0_rmse: 0.43569 | val_1_rmse: 0.44257 |  0:00:50s
epoch 44 | loss: 0.20798 | val_0_rmse: 0.44042 | val_1_rmse: 0.44717 |  0:00:51s
epoch 45 | loss: 0.21068 | val_0_rmse: 0.44207 | val_1_rmse: 0.4492  |  0:00:52s
epoch 46 | loss: 0.20447 | val_0_rmse: 0.4421  | val_1_rmse: 0.44599 |  0:00:53s
epoch 47 | loss: 0.20371 | val_0_rmse: 0.4362  | val_1_rmse: 0.44376 |  0:00:55s
epoch 48 | loss: 0.20514 | val_0_rmse: 0.43597 | val_1_rmse: 0.44303 |  0:00:56s
epoch 49 | loss: 0.20319 | val_0_rmse: 0.44196 | val_1_rmse: 0.4464  |  0:00:57s
epoch 50 | loss: 0.20631 | val_0_rmse: 0.44012 | val_1_rmse: 0.45119 |  0:00:58s
epoch 51 | loss: 0.20193 | val_0_rmse: 0.43152 | val_1_rmse: 0.44506 |  0:00:59s
epoch 52 | loss: 0.20118 | val_0_rmse: 0.43275 | val_1_rmse: 0.44546 |  0:01:00s
epoch 53 | loss: 0.20184 | val_0_rmse: 0.43309 | val_1_rmse: 0.44308 |  0:01:01s
epoch 54 | loss: 0.19989 | val_0_rmse: 0.43002 | val_1_rmse: 0.44391 |  0:01:03s
epoch 55 | loss: 0.19746 | val_0_rmse: 0.43109 | val_1_rmse: 0.44293 |  0:01:04s
epoch 56 | loss: 0.19714 | val_0_rmse: 0.42837 | val_1_rmse: 0.44103 |  0:01:05s
epoch 57 | loss: 0.19746 | val_0_rmse: 0.43168 | val_1_rmse: 0.44136 |  0:01:06s
epoch 58 | loss: 0.19434 | val_0_rmse: 0.43509 | val_1_rmse: 0.4512  |  0:01:07s
epoch 59 | loss: 0.19569 | val_0_rmse: 0.43997 | val_1_rmse: 0.45085 |  0:01:08s
epoch 60 | loss: 0.19849 | val_0_rmse: 0.42536 | val_1_rmse: 0.44142 |  0:01:09s
epoch 61 | loss: 0.20145 | val_0_rmse: 0.43779 | val_1_rmse: 0.45217 |  0:01:10s
epoch 62 | loss: 0.19696 | val_0_rmse: 0.43014 | val_1_rmse: 0.44901 |  0:01:12s
epoch 63 | loss: 0.19599 | val_0_rmse: 0.42572 | val_1_rmse: 0.44309 |  0:01:13s
epoch 64 | loss: 0.19423 | val_0_rmse: 0.4259  | val_1_rmse: 0.44125 |  0:01:14s
epoch 65 | loss: 0.20045 | val_0_rmse: 0.44873 | val_1_rmse: 0.46926 |  0:01:15s
epoch 66 | loss: 0.19312 | val_0_rmse: 0.42188 | val_1_rmse: 0.44131 |  0:01:16s
epoch 67 | loss: 0.19502 | val_0_rmse: 0.42145 | val_1_rmse: 0.44354 |  0:01:17s
epoch 68 | loss: 0.19437 | val_0_rmse: 0.43162 | val_1_rmse: 0.44991 |  0:01:18s
epoch 69 | loss: 0.19639 | val_0_rmse: 0.42188 | val_1_rmse: 0.44069 |  0:01:20s
epoch 70 | loss: 0.19166 | val_0_rmse: 0.42326 | val_1_rmse: 0.44004 |  0:01:21s
epoch 71 | loss: 0.192   | val_0_rmse: 0.42235 | val_1_rmse: 0.44136 |  0:01:22s
epoch 72 | loss: 0.19428 | val_0_rmse: 0.42036 | val_1_rmse: 0.44223 |  0:01:23s
epoch 73 | loss: 0.19129 | val_0_rmse: 0.42478 | val_1_rmse: 0.44421 |  0:01:24s
epoch 74 | loss: 0.19096 | val_0_rmse: 0.42033 | val_1_rmse: 0.44347 |  0:01:25s
epoch 75 | loss: 0.19135 | val_0_rmse: 0.42403 | val_1_rmse: 0.44348 |  0:01:26s
epoch 76 | loss: 0.19025 | val_0_rmse: 0.42361 | val_1_rmse: 0.44771 |  0:01:28s
epoch 77 | loss: 0.19814 | val_0_rmse: 0.43013 | val_1_rmse: 0.45313 |  0:01:29s
epoch 78 | loss: 0.19523 | val_0_rmse: 0.42586 | val_1_rmse: 0.4437  |  0:01:30s
epoch 79 | loss: 0.19413 | val_0_rmse: 0.42313 | val_1_rmse: 0.44499 |  0:01:31s
epoch 80 | loss: 0.19238 | val_0_rmse: 0.42127 | val_1_rmse: 0.44455 |  0:01:32s
epoch 81 | loss: 0.18642 | val_0_rmse: 0.42415 | val_1_rmse: 0.44682 |  0:01:33s
epoch 82 | loss: 0.18986 | val_0_rmse: 0.41903 | val_1_rmse: 0.44409 |  0:01:35s
epoch 83 | loss: 0.19082 | val_0_rmse: 0.41752 | val_1_rmse: 0.44166 |  0:01:36s
epoch 84 | loss: 0.18773 | val_0_rmse: 0.41667 | val_1_rmse: 0.44285 |  0:01:37s
epoch 85 | loss: 0.1877  | val_0_rmse: 0.4137  | val_1_rmse: 0.44512 |  0:01:38s
epoch 86 | loss: 0.19625 | val_0_rmse: 0.42364 | val_1_rmse: 0.44707 |  0:01:39s
epoch 87 | loss: 0.19334 | val_0_rmse: 0.42351 | val_1_rmse: 0.45151 |  0:01:40s
epoch 88 | loss: 0.19048 | val_0_rmse: 0.41878 | val_1_rmse: 0.44362 |  0:01:41s
epoch 89 | loss: 0.19278 | val_0_rmse: 0.42701 | val_1_rmse: 0.45169 |  0:01:43s
epoch 90 | loss: 0.1895  | val_0_rmse: 0.41988 | val_1_rmse: 0.44273 |  0:01:44s
epoch 91 | loss: 0.18934 | val_0_rmse: 0.41579 | val_1_rmse: 0.44619 |  0:01:45s
epoch 92 | loss: 0.18619 | val_0_rmse: 0.42135 | val_1_rmse: 0.44796 |  0:01:46s
epoch 93 | loss: 0.19212 | val_0_rmse: 0.41276 | val_1_rmse: 0.44705 |  0:01:47s
epoch 94 | loss: 0.18712 | val_0_rmse: 0.4142  | val_1_rmse: 0.44117 |  0:01:48s
epoch 95 | loss: 0.18592 | val_0_rmse: 0.41328 | val_1_rmse: 0.43945 |  0:01:49s
epoch 96 | loss: 0.18621 | val_0_rmse: 0.42565 | val_1_rmse: 0.4585  |  0:01:50s
epoch 97 | loss: 0.18592 | val_0_rmse: 0.41018 | val_1_rmse: 0.44228 |  0:01:52s
epoch 98 | loss: 0.18396 | val_0_rmse: 0.41813 | val_1_rmse: 0.45113 |  0:01:53s
epoch 99 | loss: 0.18949 | val_0_rmse: 0.42414 | val_1_rmse: 0.45853 |  0:01:54s
epoch 100| loss: 0.19066 | val_0_rmse: 0.41973 | val_1_rmse: 0.45249 |  0:01:55s
epoch 101| loss: 0.18981 | val_0_rmse: 0.42405 | val_1_rmse: 0.45972 |  0:01:56s
epoch 102| loss: 0.18503 | val_0_rmse: 0.42245 | val_1_rmse: 0.45213 |  0:01:57s
epoch 103| loss: 0.18205 | val_0_rmse: 0.41013 | val_1_rmse: 0.4466  |  0:01:59s
epoch 104| loss: 0.18063 | val_0_rmse: 0.40581 | val_1_rmse: 0.44532 |  0:02:00s
epoch 105| loss: 0.18783 | val_0_rmse: 0.42448 | val_1_rmse: 0.46004 |  0:02:01s
epoch 106| loss: 0.18466 | val_0_rmse: 0.409   | val_1_rmse: 0.44904 |  0:02:02s
epoch 107| loss: 0.18551 | val_0_rmse: 0.40651 | val_1_rmse: 0.4503  |  0:02:03s
epoch 108| loss: 0.17875 | val_0_rmse: 0.40639 | val_1_rmse: 0.45478 |  0:02:04s
epoch 109| loss: 0.17611 | val_0_rmse: 0.40277 | val_1_rmse: 0.44691 |  0:02:06s
epoch 110| loss: 0.17897 | val_0_rmse: 0.40903 | val_1_rmse: 0.44575 |  0:02:07s
epoch 111| loss: 0.17844 | val_0_rmse: 0.40132 | val_1_rmse: 0.45077 |  0:02:08s
epoch 112| loss: 0.17558 | val_0_rmse: 0.40155 | val_1_rmse: 0.44719 |  0:02:09s
epoch 113| loss: 0.17808 | val_0_rmse: 0.41255 | val_1_rmse: 0.45501 |  0:02:11s
epoch 114| loss: 0.17855 | val_0_rmse: 0.39901 | val_1_rmse: 0.44338 |  0:02:12s
epoch 115| loss: 0.17817 | val_0_rmse: 0.40009 | val_1_rmse: 0.44766 |  0:02:13s
epoch 116| loss: 0.1762  | val_0_rmse: 0.40623 | val_1_rmse: 0.45371 |  0:02:14s
epoch 117| loss: 0.17638 | val_0_rmse: 0.41436 | val_1_rmse: 0.46154 |  0:02:15s
epoch 118| loss: 0.17463 | val_0_rmse: 0.40198 | val_1_rmse: 0.45837 |  0:02:16s
epoch 119| loss: 0.17039 | val_0_rmse: 0.39961 | val_1_rmse: 0.4561  |  0:02:18s
epoch 120| loss: 0.17387 | val_0_rmse: 0.40331 | val_1_rmse: 0.46241 |  0:02:19s
epoch 121| loss: 0.17225 | val_0_rmse: 0.39447 | val_1_rmse: 0.44732 |  0:02:20s
epoch 122| loss: 0.17988 | val_0_rmse: 0.41105 | val_1_rmse: 0.45897 |  0:02:21s
epoch 123| loss: 0.17944 | val_0_rmse: 0.40231 | val_1_rmse: 0.45242 |  0:02:22s
epoch 124| loss: 0.17551 | val_0_rmse: 0.39723 | val_1_rmse: 0.4555  |  0:02:23s
epoch 125| loss: 0.17464 | val_0_rmse: 0.39851 | val_1_rmse: 0.45621 |  0:02:24s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.43945
Best weights from best epoch are automatically used!
ended training at: 16:20:49
Feature importance:
Mean squared error is of 6426598791.388966
Mean absolute error:57238.750290244054
MAPE:0.1500976689657005
R2 score:0.7922677035124225
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:20:49
epoch 0  | loss: 1.33891 | val_0_rmse: 0.99186 | val_1_rmse: 1.009   |  0:00:01s
epoch 1  | loss: 0.72294 | val_0_rmse: 0.82352 | val_1_rmse: 0.82974 |  0:00:02s
epoch 2  | loss: 0.48168 | val_0_rmse: 0.64779 | val_1_rmse: 0.66238 |  0:00:03s
epoch 3  | loss: 0.376   | val_0_rmse: 0.63827 | val_1_rmse: 0.66087 |  0:00:04s
epoch 4  | loss: 0.31631 | val_0_rmse: 0.5576  | val_1_rmse: 0.57453 |  0:00:05s
epoch 5  | loss: 0.30306 | val_0_rmse: 0.55276 | val_1_rmse: 0.56764 |  0:00:06s
epoch 6  | loss: 0.29621 | val_0_rmse: 0.55779 | val_1_rmse: 0.57558 |  0:00:08s
epoch 7  | loss: 0.29262 | val_0_rmse: 0.56967 | val_1_rmse: 0.58853 |  0:00:09s
epoch 8  | loss: 0.28785 | val_0_rmse: 0.55299 | val_1_rmse: 0.56822 |  0:00:10s
epoch 9  | loss: 0.28571 | val_0_rmse: 0.53638 | val_1_rmse: 0.5555  |  0:00:11s
epoch 10 | loss: 0.27539 | val_0_rmse: 0.57783 | val_1_rmse: 0.5969  |  0:00:12s
epoch 11 | loss: 0.25961 | val_0_rmse: 0.54979 | val_1_rmse: 0.56654 |  0:00:13s
epoch 12 | loss: 0.25561 | val_0_rmse: 0.53273 | val_1_rmse: 0.54346 |  0:00:14s
epoch 13 | loss: 0.25628 | val_0_rmse: 0.5201  | val_1_rmse: 0.53349 |  0:00:16s
epoch 14 | loss: 0.25342 | val_0_rmse: 0.54807 | val_1_rmse: 0.56337 |  0:00:17s
epoch 15 | loss: 0.25518 | val_0_rmse: 0.50904 | val_1_rmse: 0.52328 |  0:00:18s
epoch 16 | loss: 0.24685 | val_0_rmse: 0.53438 | val_1_rmse: 0.55236 |  0:00:19s
epoch 17 | loss: 0.23818 | val_0_rmse: 0.53443 | val_1_rmse: 0.55214 |  0:00:20s
epoch 18 | loss: 0.24693 | val_0_rmse: 0.50193 | val_1_rmse: 0.51646 |  0:00:21s
epoch 19 | loss: 0.24068 | val_0_rmse: 0.52516 | val_1_rmse: 0.54088 |  0:00:22s
epoch 20 | loss: 0.2397  | val_0_rmse: 0.50679 | val_1_rmse: 0.52282 |  0:00:24s
epoch 21 | loss: 0.2345  | val_0_rmse: 0.49552 | val_1_rmse: 0.5105  |  0:00:25s
epoch 22 | loss: 0.23586 | val_0_rmse: 0.53639 | val_1_rmse: 0.55552 |  0:00:26s
epoch 23 | loss: 0.22596 | val_0_rmse: 0.47346 | val_1_rmse: 0.48887 |  0:00:27s
epoch 24 | loss: 0.22836 | val_0_rmse: 0.52589 | val_1_rmse: 0.54273 |  0:00:28s
epoch 25 | loss: 0.24009 | val_0_rmse: 0.48812 | val_1_rmse: 0.50046 |  0:00:29s
epoch 26 | loss: 0.2373  | val_0_rmse: 0.56372 | val_1_rmse: 0.57684 |  0:00:30s
epoch 27 | loss: 0.23166 | val_0_rmse: 0.47016 | val_1_rmse: 0.48174 |  0:00:32s
epoch 28 | loss: 0.23486 | val_0_rmse: 0.48289 | val_1_rmse: 0.49808 |  0:00:33s
epoch 29 | loss: 0.23368 | val_0_rmse: 0.49865 | val_1_rmse: 0.51356 |  0:00:34s
epoch 30 | loss: 0.24929 | val_0_rmse: 0.48174 | val_1_rmse: 0.49793 |  0:00:35s
epoch 31 | loss: 0.24905 | val_0_rmse: 0.47828 | val_1_rmse: 0.49506 |  0:00:36s
epoch 32 | loss: 0.23913 | val_0_rmse: 0.49177 | val_1_rmse: 0.50537 |  0:00:37s
epoch 33 | loss: 0.24073 | val_0_rmse: 0.48458 | val_1_rmse: 0.49721 |  0:00:38s
epoch 34 | loss: 0.22743 | val_0_rmse: 0.47509 | val_1_rmse: 0.49314 |  0:00:40s
epoch 35 | loss: 0.2278  | val_0_rmse: 0.4625  | val_1_rmse: 0.48144 |  0:00:41s
epoch 36 | loss: 0.22857 | val_0_rmse: 0.45631 | val_1_rmse: 0.47589 |  0:00:42s
epoch 37 | loss: 0.2239  | val_0_rmse: 0.4561  | val_1_rmse: 0.47399 |  0:00:43s
epoch 38 | loss: 0.22424 | val_0_rmse: 0.46366 | val_1_rmse: 0.48014 |  0:00:44s
epoch 39 | loss: 0.22562 | val_0_rmse: 0.45737 | val_1_rmse: 0.47859 |  0:00:45s
epoch 40 | loss: 0.22289 | val_0_rmse: 0.46013 | val_1_rmse: 0.48292 |  0:00:46s
epoch 41 | loss: 0.22573 | val_0_rmse: 0.45203 | val_1_rmse: 0.46977 |  0:00:48s
epoch 42 | loss: 0.21706 | val_0_rmse: 0.46278 | val_1_rmse: 0.48337 |  0:00:49s
epoch 43 | loss: 0.21686 | val_0_rmse: 0.45195 | val_1_rmse: 0.4715  |  0:00:50s
epoch 44 | loss: 0.21871 | val_0_rmse: 0.45136 | val_1_rmse: 0.47054 |  0:00:51s
epoch 45 | loss: 0.21635 | val_0_rmse: 0.45231 | val_1_rmse: 0.46882 |  0:00:52s
epoch 46 | loss: 0.21627 | val_0_rmse: 0.45528 | val_1_rmse: 0.46858 |  0:00:53s
epoch 47 | loss: 0.22183 | val_0_rmse: 0.45148 | val_1_rmse: 0.46885 |  0:00:54s
epoch 48 | loss: 0.2146  | val_0_rmse: 0.4489  | val_1_rmse: 0.46484 |  0:00:56s
epoch 49 | loss: 0.21225 | val_0_rmse: 0.44353 | val_1_rmse: 0.4629  |  0:00:57s
epoch 50 | loss: 0.21077 | val_0_rmse: 0.4482  | val_1_rmse: 0.47128 |  0:00:58s
epoch 51 | loss: 0.21158 | val_0_rmse: 0.44671 | val_1_rmse: 0.4642  |  0:00:59s
epoch 52 | loss: 0.20945 | val_0_rmse: 0.44652 | val_1_rmse: 0.46491 |  0:01:00s
epoch 53 | loss: 0.21069 | val_0_rmse: 0.45023 | val_1_rmse: 0.47043 |  0:01:01s
epoch 54 | loss: 0.20598 | val_0_rmse: 0.44801 | val_1_rmse: 0.46942 |  0:01:02s
epoch 55 | loss: 0.20659 | val_0_rmse: 0.43911 | val_1_rmse: 0.4587  |  0:01:04s
epoch 56 | loss: 0.2101  | val_0_rmse: 0.44032 | val_1_rmse: 0.46066 |  0:01:05s
epoch 57 | loss: 0.20989 | val_0_rmse: 0.445   | val_1_rmse: 0.46439 |  0:01:06s
epoch 58 | loss: 0.2045  | val_0_rmse: 0.46812 | val_1_rmse: 0.49154 |  0:01:07s
epoch 59 | loss: 0.2138  | val_0_rmse: 0.44528 | val_1_rmse: 0.46622 |  0:01:08s
epoch 60 | loss: 0.20565 | val_0_rmse: 0.44833 | val_1_rmse: 0.46935 |  0:01:09s
epoch 61 | loss: 0.20337 | val_0_rmse: 0.44239 | val_1_rmse: 0.46233 |  0:01:10s
epoch 62 | loss: 0.20429 | val_0_rmse: 0.44305 | val_1_rmse: 0.46485 |  0:01:12s
epoch 63 | loss: 0.20633 | val_0_rmse: 0.4365  | val_1_rmse: 0.45787 |  0:01:13s
epoch 64 | loss: 0.20463 | val_0_rmse: 0.45005 | val_1_rmse: 0.48465 |  0:01:14s
epoch 65 | loss: 0.20919 | val_0_rmse: 0.46146 | val_1_rmse: 0.48818 |  0:01:15s
epoch 66 | loss: 0.21144 | val_0_rmse: 0.45105 | val_1_rmse: 0.47066 |  0:01:16s
epoch 67 | loss: 0.20808 | val_0_rmse: 0.44814 | val_1_rmse: 0.47376 |  0:01:17s
epoch 68 | loss: 0.20277 | val_0_rmse: 0.45261 | val_1_rmse: 0.4798  |  0:01:18s
epoch 69 | loss: 0.20751 | val_0_rmse: 0.44038 | val_1_rmse: 0.46898 |  0:01:20s
epoch 70 | loss: 0.20786 | val_0_rmse: 0.4347  | val_1_rmse: 0.46522 |  0:01:21s
epoch 71 | loss: 0.20368 | val_0_rmse: 0.45168 | val_1_rmse: 0.47911 |  0:01:22s
epoch 72 | loss: 0.20337 | val_0_rmse: 0.43504 | val_1_rmse: 0.46852 |  0:01:23s
epoch 73 | loss: 0.1992  | val_0_rmse: 0.45434 | val_1_rmse: 0.47774 |  0:01:24s
epoch 74 | loss: 0.21292 | val_0_rmse: 0.45099 | val_1_rmse: 0.47807 |  0:01:25s
epoch 75 | loss: 0.2076  | val_0_rmse: 0.44887 | val_1_rmse: 0.4684  |  0:01:26s
epoch 76 | loss: 0.20629 | val_0_rmse: 0.44109 | val_1_rmse: 0.46503 |  0:01:28s
epoch 77 | loss: 0.20863 | val_0_rmse: 0.4456  | val_1_rmse: 0.46834 |  0:01:29s
epoch 78 | loss: 0.21589 | val_0_rmse: 0.44561 | val_1_rmse: 0.46905 |  0:01:30s
epoch 79 | loss: 0.20998 | val_0_rmse: 0.4456  | val_1_rmse: 0.4702  |  0:01:31s
epoch 80 | loss: 0.20645 | val_0_rmse: 0.43317 | val_1_rmse: 0.45981 |  0:01:32s
epoch 81 | loss: 0.19976 | val_0_rmse: 0.43099 | val_1_rmse: 0.45705 |  0:01:33s
epoch 82 | loss: 0.20014 | val_0_rmse: 0.44089 | val_1_rmse: 0.46409 |  0:01:34s
epoch 83 | loss: 0.20766 | val_0_rmse: 0.43806 | val_1_rmse: 0.46311 |  0:01:36s
epoch 84 | loss: 0.20222 | val_0_rmse: 0.43291 | val_1_rmse: 0.46105 |  0:01:37s
epoch 85 | loss: 0.20415 | val_0_rmse: 0.42949 | val_1_rmse: 0.45869 |  0:01:38s
epoch 86 | loss: 0.20454 | val_0_rmse: 0.445   | val_1_rmse: 0.47078 |  0:01:39s
epoch 87 | loss: 0.21078 | val_0_rmse: 0.45163 | val_1_rmse: 0.47119 |  0:01:40s
epoch 88 | loss: 0.21311 | val_0_rmse: 0.45976 | val_1_rmse: 0.48826 |  0:01:41s
epoch 89 | loss: 0.21765 | val_0_rmse: 0.44568 | val_1_rmse: 0.47338 |  0:01:42s
epoch 90 | loss: 0.21086 | val_0_rmse: 0.43658 | val_1_rmse: 0.46493 |  0:01:43s
epoch 91 | loss: 0.20687 | val_0_rmse: 0.46544 | val_1_rmse: 0.49226 |  0:01:45s
epoch 92 | loss: 0.22668 | val_0_rmse: 0.4664  | val_1_rmse: 0.50549 |  0:01:46s
epoch 93 | loss: 0.2804  | val_0_rmse: 0.55669 | val_1_rmse: 0.60007 |  0:01:47s
epoch 94 | loss: 0.27775 | val_0_rmse: 0.52086 | val_1_rmse: 0.52968 |  0:01:48s
epoch 95 | loss: 0.26123 | val_0_rmse: 0.49973 | val_1_rmse: 0.52398 |  0:01:49s
epoch 96 | loss: 0.25119 | val_0_rmse: 0.48269 | val_1_rmse: 0.49798 |  0:01:50s
epoch 97 | loss: 0.2407  | val_0_rmse: 0.47401 | val_1_rmse: 0.49176 |  0:01:51s
epoch 98 | loss: 0.23529 | val_0_rmse: 0.48426 | val_1_rmse: 0.50922 |  0:01:53s
epoch 99 | loss: 0.23367 | val_0_rmse: 0.46407 | val_1_rmse: 0.48383 |  0:01:54s
epoch 100| loss: 0.23337 | val_0_rmse: 0.47163 | val_1_rmse: 0.4907  |  0:01:55s
epoch 101| loss: 0.2285  | val_0_rmse: 0.46004 | val_1_rmse: 0.48392 |  0:01:56s
epoch 102| loss: 0.22526 | val_0_rmse: 0.45466 | val_1_rmse: 0.47692 |  0:01:57s
epoch 103| loss: 0.21861 | val_0_rmse: 0.4564  | val_1_rmse: 0.47955 |  0:01:58s
epoch 104| loss: 0.21693 | val_0_rmse: 0.44908 | val_1_rmse: 0.47556 |  0:01:59s
epoch 105| loss: 0.21875 | val_0_rmse: 0.45106 | val_1_rmse: 0.47067 |  0:02:01s
epoch 106| loss: 0.21547 | val_0_rmse: 0.44646 | val_1_rmse: 0.46701 |  0:02:02s
epoch 107| loss: 0.2094  | val_0_rmse: 0.44581 | val_1_rmse: 0.46681 |  0:02:03s
epoch 108| loss: 0.21159 | val_0_rmse: 0.44297 | val_1_rmse: 0.4668  |  0:02:04s
epoch 109| loss: 0.2106  | val_0_rmse: 0.44674 | val_1_rmse: 0.47234 |  0:02:05s
epoch 110| loss: 0.2107  | val_0_rmse: 0.45017 | val_1_rmse: 0.47646 |  0:02:06s
epoch 111| loss: 0.20527 | val_0_rmse: 0.44209 | val_1_rmse: 0.46587 |  0:02:07s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.45705
Best weights from best epoch are automatically used!
ended training at: 16:22:57
Feature importance:
Mean squared error is of 6321639111.637277
Mean absolute error:56849.21109932979
MAPE:0.1522587978992804
R2 score:0.7932580321710986
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:22:57
epoch 0  | loss: 1.25565 | val_0_rmse: 0.86619 | val_1_rmse: 0.86514 |  0:00:01s
epoch 1  | loss: 0.4559  | val_0_rmse: 0.65135 | val_1_rmse: 0.65636 |  0:00:02s
epoch 2  | loss: 0.35093 | val_0_rmse: 0.64589 | val_1_rmse: 0.6475  |  0:00:03s
epoch 3  | loss: 0.33149 | val_0_rmse: 0.57528 | val_1_rmse: 0.57216 |  0:00:04s
epoch 4  | loss: 0.32578 | val_0_rmse: 0.55049 | val_1_rmse: 0.55372 |  0:00:05s
epoch 5  | loss: 0.31319 | val_0_rmse: 0.56452 | val_1_rmse: 0.5706  |  0:00:06s
epoch 6  | loss: 0.31415 | val_0_rmse: 0.55831 | val_1_rmse: 0.56658 |  0:00:08s
epoch 7  | loss: 0.30833 | val_0_rmse: 0.54526 | val_1_rmse: 0.55256 |  0:00:09s
epoch 8  | loss: 0.30245 | val_0_rmse: 0.57179 | val_1_rmse: 0.57548 |  0:00:10s
epoch 9  | loss: 0.30755 | val_0_rmse: 0.54983 | val_1_rmse: 0.55203 |  0:00:11s
epoch 10 | loss: 0.31153 | val_0_rmse: 0.61075 | val_1_rmse: 0.60672 |  0:00:12s
epoch 11 | loss: 0.30286 | val_0_rmse: 0.54844 | val_1_rmse: 0.54843 |  0:00:13s
epoch 12 | loss: 0.2864  | val_0_rmse: 0.55195 | val_1_rmse: 0.55416 |  0:00:14s
epoch 13 | loss: 0.2852  | val_0_rmse: 0.53159 | val_1_rmse: 0.53878 |  0:00:16s
epoch 14 | loss: 0.26818 | val_0_rmse: 0.52163 | val_1_rmse: 0.53213 |  0:00:17s
epoch 15 | loss: 0.26824 | val_0_rmse: 0.52173 | val_1_rmse: 0.52823 |  0:00:18s
epoch 16 | loss: 0.26174 | val_0_rmse: 0.52018 | val_1_rmse: 0.52846 |  0:00:19s
epoch 17 | loss: 0.25661 | val_0_rmse: 0.50267 | val_1_rmse: 0.51319 |  0:00:20s
epoch 18 | loss: 0.24824 | val_0_rmse: 0.53114 | val_1_rmse: 0.5328  |  0:00:21s
epoch 19 | loss: 0.24609 | val_0_rmse: 0.49469 | val_1_rmse: 0.5019  |  0:00:22s
epoch 20 | loss: 0.24996 | val_0_rmse: 0.4959  | val_1_rmse: 0.50264 |  0:00:24s
epoch 21 | loss: 0.25032 | val_0_rmse: 0.54468 | val_1_rmse: 0.55697 |  0:00:25s
epoch 22 | loss: 0.25664 | val_0_rmse: 0.49242 | val_1_rmse: 0.49669 |  0:00:26s
epoch 23 | loss: 0.24408 | val_0_rmse: 0.48715 | val_1_rmse: 0.49402 |  0:00:27s
epoch 24 | loss: 0.24471 | val_0_rmse: 0.50591 | val_1_rmse: 0.51098 |  0:00:28s
epoch 25 | loss: 0.25025 | val_0_rmse: 0.49186 | val_1_rmse: 0.49715 |  0:00:29s
epoch 26 | loss: 0.24631 | val_0_rmse: 0.48704 | val_1_rmse: 0.49223 |  0:00:30s
epoch 27 | loss: 0.24001 | val_0_rmse: 0.48462 | val_1_rmse: 0.48687 |  0:00:32s
epoch 28 | loss: 0.23796 | val_0_rmse: 0.49414 | val_1_rmse: 0.5026  |  0:00:33s
epoch 29 | loss: 0.24544 | val_0_rmse: 0.4796  | val_1_rmse: 0.49072 |  0:00:34s
epoch 30 | loss: 0.23786 | val_0_rmse: 0.48262 | val_1_rmse: 0.49425 |  0:00:35s
epoch 31 | loss: 0.23722 | val_0_rmse: 0.52553 | val_1_rmse: 0.53095 |  0:00:36s
epoch 32 | loss: 0.24249 | val_0_rmse: 0.54236 | val_1_rmse: 0.552   |  0:00:37s
epoch 33 | loss: 0.24794 | val_0_rmse: 0.4904  | val_1_rmse: 0.50224 |  0:00:38s
epoch 34 | loss: 0.23546 | val_0_rmse: 0.46836 | val_1_rmse: 0.47818 |  0:00:40s
epoch 35 | loss: 0.23193 | val_0_rmse: 0.47086 | val_1_rmse: 0.47754 |  0:00:41s
epoch 36 | loss: 0.22965 | val_0_rmse: 0.46821 | val_1_rmse: 0.4717  |  0:00:42s
epoch 37 | loss: 0.22672 | val_0_rmse: 0.4657  | val_1_rmse: 0.47847 |  0:00:43s
epoch 38 | loss: 0.22669 | val_0_rmse: 0.46022 | val_1_rmse: 0.47319 |  0:00:44s
epoch 39 | loss: 0.22682 | val_0_rmse: 0.46659 | val_1_rmse: 0.47868 |  0:00:45s
epoch 40 | loss: 0.22908 | val_0_rmse: 0.45924 | val_1_rmse: 0.4721  |  0:00:46s
epoch 41 | loss: 0.22555 | val_0_rmse: 0.47238 | val_1_rmse: 0.48314 |  0:00:48s
epoch 42 | loss: 0.22541 | val_0_rmse: 0.4623  | val_1_rmse: 0.47573 |  0:00:49s
epoch 43 | loss: 0.22706 | val_0_rmse: 0.47426 | val_1_rmse: 0.48858 |  0:00:50s
epoch 44 | loss: 0.2215  | val_0_rmse: 0.45595 | val_1_rmse: 0.47153 |  0:00:51s
epoch 45 | loss: 0.22147 | val_0_rmse: 0.4644  | val_1_rmse: 0.47784 |  0:00:52s
epoch 46 | loss: 0.21933 | val_0_rmse: 0.46271 | val_1_rmse: 0.47852 |  0:00:53s
epoch 47 | loss: 0.22032 | val_0_rmse: 0.45341 | val_1_rmse: 0.47115 |  0:00:54s
epoch 48 | loss: 0.22218 | val_0_rmse: 0.45319 | val_1_rmse: 0.4713  |  0:00:56s
epoch 49 | loss: 0.22067 | val_0_rmse: 0.46675 | val_1_rmse: 0.48083 |  0:00:57s
epoch 50 | loss: 0.22889 | val_0_rmse: 0.4658  | val_1_rmse: 0.47992 |  0:00:58s
epoch 51 | loss: 0.22492 | val_0_rmse: 0.47473 | val_1_rmse: 0.48683 |  0:00:59s
epoch 52 | loss: 0.22948 | val_0_rmse: 0.48564 | val_1_rmse: 0.49377 |  0:01:00s
epoch 53 | loss: 0.23433 | val_0_rmse: 0.47408 | val_1_rmse: 0.48489 |  0:01:01s
epoch 54 | loss: 0.23525 | val_0_rmse: 0.51886 | val_1_rmse: 0.53662 |  0:01:02s
epoch 55 | loss: 0.22695 | val_0_rmse: 0.47887 | val_1_rmse: 0.48975 |  0:01:04s
epoch 56 | loss: 0.22353 | val_0_rmse: 0.46947 | val_1_rmse: 0.4839  |  0:01:05s
epoch 57 | loss: 0.22717 | val_0_rmse: 0.46821 | val_1_rmse: 0.48412 |  0:01:06s
epoch 58 | loss: 0.22938 | val_0_rmse: 0.47109 | val_1_rmse: 0.48226 |  0:01:07s
epoch 59 | loss: 0.2253  | val_0_rmse: 0.46395 | val_1_rmse: 0.47704 |  0:01:08s
epoch 60 | loss: 0.22456 | val_0_rmse: 0.46968 | val_1_rmse: 0.48661 |  0:01:09s
epoch 61 | loss: 0.22258 | val_0_rmse: 0.46072 | val_1_rmse: 0.47562 |  0:01:10s
epoch 62 | loss: 0.23715 | val_0_rmse: 0.48235 | val_1_rmse: 0.49893 |  0:01:12s
epoch 63 | loss: 0.23539 | val_0_rmse: 0.46916 | val_1_rmse: 0.48179 |  0:01:13s
epoch 64 | loss: 0.22522 | val_0_rmse: 0.44812 | val_1_rmse: 0.46103 |  0:01:14s
epoch 65 | loss: 0.23023 | val_0_rmse: 0.45801 | val_1_rmse: 0.46678 |  0:01:15s
epoch 66 | loss: 0.2219  | val_0_rmse: 0.44999 | val_1_rmse: 0.46464 |  0:01:16s
epoch 67 | loss: 0.22992 | val_0_rmse: 0.45369 | val_1_rmse: 0.47217 |  0:01:17s
epoch 68 | loss: 0.23305 | val_0_rmse: 0.47187 | val_1_rmse: 0.48796 |  0:01:18s
epoch 69 | loss: 0.22871 | val_0_rmse: 0.45837 | val_1_rmse: 0.47566 |  0:01:20s
epoch 70 | loss: 0.21735 | val_0_rmse: 0.45833 | val_1_rmse: 0.47715 |  0:01:21s
epoch 71 | loss: 0.21848 | val_0_rmse: 0.45504 | val_1_rmse: 0.47254 |  0:01:22s
epoch 72 | loss: 0.22062 | val_0_rmse: 0.45771 | val_1_rmse: 0.47615 |  0:01:23s
epoch 73 | loss: 0.24163 | val_0_rmse: 0.48096 | val_1_rmse: 0.4922  |  0:01:24s
epoch 74 | loss: 0.23791 | val_0_rmse: 0.49526 | val_1_rmse: 0.5035  |  0:01:25s
epoch 75 | loss: 0.23642 | val_0_rmse: 0.49038 | val_1_rmse: 0.50246 |  0:01:26s
epoch 76 | loss: 0.22292 | val_0_rmse: 0.47612 | val_1_rmse: 0.48574 |  0:01:28s
epoch 77 | loss: 0.22287 | val_0_rmse: 0.45403 | val_1_rmse: 0.46266 |  0:01:29s
epoch 78 | loss: 0.2164  | val_0_rmse: 0.45242 | val_1_rmse: 0.46329 |  0:01:30s
epoch 79 | loss: 0.21253 | val_0_rmse: 0.45523 | val_1_rmse: 0.46373 |  0:01:31s
epoch 80 | loss: 0.21524 | val_0_rmse: 0.45192 | val_1_rmse: 0.46011 |  0:01:32s
epoch 81 | loss: 0.20795 | val_0_rmse: 0.44642 | val_1_rmse: 0.45546 |  0:01:33s
epoch 82 | loss: 0.20938 | val_0_rmse: 0.44143 | val_1_rmse: 0.45236 |  0:01:34s
epoch 83 | loss: 0.20437 | val_0_rmse: 0.44005 | val_1_rmse: 0.45092 |  0:01:36s
epoch 84 | loss: 0.20567 | val_0_rmse: 0.44327 | val_1_rmse: 0.45467 |  0:01:37s
epoch 85 | loss: 0.20684 | val_0_rmse: 0.44365 | val_1_rmse: 0.45692 |  0:01:38s
epoch 86 | loss: 0.20587 | val_0_rmse: 0.43939 | val_1_rmse: 0.45294 |  0:01:39s
epoch 87 | loss: 0.20767 | val_0_rmse: 0.44049 | val_1_rmse: 0.4555  |  0:01:40s
epoch 88 | loss: 0.20646 | val_0_rmse: 0.43979 | val_1_rmse: 0.45645 |  0:01:41s
epoch 89 | loss: 0.20802 | val_0_rmse: 0.45304 | val_1_rmse: 0.46709 |  0:01:42s
epoch 90 | loss: 0.20181 | val_0_rmse: 0.44616 | val_1_rmse: 0.45968 |  0:01:44s
epoch 91 | loss: 0.20217 | val_0_rmse: 0.43174 | val_1_rmse: 0.4463  |  0:01:45s
epoch 92 | loss: 0.20083 | val_0_rmse: 0.43687 | val_1_rmse: 0.45255 |  0:01:46s
epoch 93 | loss: 0.19877 | val_0_rmse: 0.43905 | val_1_rmse: 0.45571 |  0:01:47s
epoch 94 | loss: 0.19766 | val_0_rmse: 0.43757 | val_1_rmse: 0.45393 |  0:01:48s
epoch 95 | loss: 0.19769 | val_0_rmse: 0.43284 | val_1_rmse: 0.44999 |  0:01:49s
epoch 96 | loss: 0.20243 | val_0_rmse: 0.43485 | val_1_rmse: 0.45318 |  0:01:50s
epoch 97 | loss: 0.20248 | val_0_rmse: 0.4303  | val_1_rmse: 0.44819 |  0:01:52s
epoch 98 | loss: 0.20429 | val_0_rmse: 0.43391 | val_1_rmse: 0.45463 |  0:01:53s
epoch 99 | loss: 0.19955 | val_0_rmse: 0.44438 | val_1_rmse: 0.46482 |  0:01:54s
epoch 100| loss: 0.2021  | val_0_rmse: 0.4282  | val_1_rmse: 0.45304 |  0:01:55s
epoch 101| loss: 0.19275 | val_0_rmse: 0.42505 | val_1_rmse: 0.44894 |  0:01:56s
epoch 102| loss: 0.19272 | val_0_rmse: 0.43187 | val_1_rmse: 0.45494 |  0:01:57s
epoch 103| loss: 0.19971 | val_0_rmse: 0.43964 | val_1_rmse: 0.46133 |  0:01:58s
epoch 104| loss: 0.20284 | val_0_rmse: 0.43451 | val_1_rmse: 0.45715 |  0:02:00s
epoch 105| loss: 0.19462 | val_0_rmse: 0.43146 | val_1_rmse: 0.45578 |  0:02:01s
epoch 106| loss: 0.19243 | val_0_rmse: 0.43728 | val_1_rmse: 0.46001 |  0:02:02s
epoch 107| loss: 0.19179 | val_0_rmse: 0.43652 | val_1_rmse: 0.46122 |  0:02:03s
epoch 108| loss: 0.1978  | val_0_rmse: 0.44344 | val_1_rmse: 0.46683 |  0:02:04s
epoch 109| loss: 0.19618 | val_0_rmse: 0.42326 | val_1_rmse: 0.44779 |  0:02:05s
epoch 110| loss: 0.19162 | val_0_rmse: 0.43106 | val_1_rmse: 0.45583 |  0:02:06s
epoch 111| loss: 0.1929  | val_0_rmse: 0.42501 | val_1_rmse: 0.45292 |  0:02:08s
epoch 112| loss: 0.19669 | val_0_rmse: 0.42603 | val_1_rmse: 0.45371 |  0:02:09s
epoch 113| loss: 0.19181 | val_0_rmse: 0.42051 | val_1_rmse: 0.44844 |  0:02:10s
epoch 114| loss: 0.19729 | val_0_rmse: 0.42764 | val_1_rmse: 0.45474 |  0:02:11s
epoch 115| loss: 0.19033 | val_0_rmse: 0.42074 | val_1_rmse: 0.447   |  0:02:12s
epoch 116| loss: 0.1886  | val_0_rmse: 0.41893 | val_1_rmse: 0.44586 |  0:02:13s
epoch 117| loss: 0.18702 | val_0_rmse: 0.41738 | val_1_rmse: 0.44655 |  0:02:14s
epoch 118| loss: 0.1941  | val_0_rmse: 0.42664 | val_1_rmse: 0.45595 |  0:02:16s
epoch 119| loss: 0.20402 | val_0_rmse: 0.43089 | val_1_rmse: 0.46576 |  0:02:17s
epoch 120| loss: 0.1997  | val_0_rmse: 0.42453 | val_1_rmse: 0.45592 |  0:02:18s
epoch 121| loss: 0.19285 | val_0_rmse: 0.42932 | val_1_rmse: 0.45629 |  0:02:19s
epoch 122| loss: 0.18852 | val_0_rmse: 0.42105 | val_1_rmse: 0.44642 |  0:02:20s
epoch 123| loss: 0.18679 | val_0_rmse: 0.41856 | val_1_rmse: 0.44546 |  0:02:21s
epoch 124| loss: 0.18647 | val_0_rmse: 0.43234 | val_1_rmse: 0.46167 |  0:02:23s
epoch 125| loss: 0.19778 | val_0_rmse: 0.41482 | val_1_rmse: 0.44643 |  0:02:24s
epoch 126| loss: 0.19209 | val_0_rmse: 0.42123 | val_1_rmse: 0.45171 |  0:02:25s
epoch 127| loss: 0.1911  | val_0_rmse: 0.41572 | val_1_rmse: 0.44636 |  0:02:26s
epoch 128| loss: 0.18631 | val_0_rmse: 0.41635 | val_1_rmse: 0.44632 |  0:02:27s
epoch 129| loss: 0.18314 | val_0_rmse: 0.41246 | val_1_rmse: 0.44353 |  0:02:28s
epoch 130| loss: 0.183   | val_0_rmse: 0.41259 | val_1_rmse: 0.44572 |  0:02:29s
epoch 131| loss: 0.184   | val_0_rmse: 0.42003 | val_1_rmse: 0.45318 |  0:02:30s
epoch 132| loss: 0.18589 | val_0_rmse: 0.41439 | val_1_rmse: 0.44832 |  0:02:32s
epoch 133| loss: 0.18397 | val_0_rmse: 0.42369 | val_1_rmse: 0.45942 |  0:02:33s
epoch 134| loss: 0.18426 | val_0_rmse: 0.40979 | val_1_rmse: 0.44332 |  0:02:34s
epoch 135| loss: 0.18052 | val_0_rmse: 0.40832 | val_1_rmse: 0.4454  |  0:02:35s
epoch 136| loss: 0.18667 | val_0_rmse: 0.41855 | val_1_rmse: 0.45206 |  0:02:36s
epoch 137| loss: 0.1811  | val_0_rmse: 0.41264 | val_1_rmse: 0.44886 |  0:02:37s
epoch 138| loss: 0.17983 | val_0_rmse: 0.41978 | val_1_rmse: 0.45704 |  0:02:38s
epoch 139| loss: 0.17996 | val_0_rmse: 0.40558 | val_1_rmse: 0.44746 |  0:02:40s
epoch 140| loss: 0.1771  | val_0_rmse: 0.41802 | val_1_rmse: 0.4576  |  0:02:41s
epoch 141| loss: 0.17727 | val_0_rmse: 0.41198 | val_1_rmse: 0.44932 |  0:02:42s
epoch 142| loss: 0.18224 | val_0_rmse: 0.41461 | val_1_rmse: 0.45533 |  0:02:43s
epoch 143| loss: 0.17783 | val_0_rmse: 0.40919 | val_1_rmse: 0.4487  |  0:02:44s
epoch 144| loss: 0.17607 | val_0_rmse: 0.40691 | val_1_rmse: 0.44855 |  0:02:45s
epoch 145| loss: 0.21886 | val_0_rmse: 0.59773 | val_1_rmse: 0.6135  |  0:02:46s
epoch 146| loss: 0.30281 | val_0_rmse: 0.59511 | val_1_rmse: 0.6102  |  0:02:48s
epoch 147| loss: 0.33791 | val_0_rmse: 0.64482 | val_1_rmse: 0.64955 |  0:02:49s
epoch 148| loss: 0.32014 | val_0_rmse: 0.52799 | val_1_rmse: 0.53594 |  0:02:50s
epoch 149| loss: 0.28449 | val_0_rmse: 0.53428 | val_1_rmse: 0.54209 |  0:02:51s
Stop training because you reached max_epochs = 150 with best_epoch = 134 and best_val_1_rmse = 0.44332
Best weights from best epoch are automatically used!
ended training at: 16:25:49
Feature importance:
Mean squared error is of 6344156330.684191
Mean absolute error:56174.38305778957
MAPE:0.1515841355120541
R2 score:0.7896809954000751
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:25:49
epoch 0  | loss: 1.20307 | val_0_rmse: 0.85842 | val_1_rmse: 0.85454 |  0:00:01s
epoch 1  | loss: 0.56094 | val_0_rmse: 0.71962 | val_1_rmse: 0.72458 |  0:00:02s
epoch 2  | loss: 0.39088 | val_0_rmse: 0.64028 | val_1_rmse: 0.65478 |  0:00:03s
epoch 3  | loss: 0.31494 | val_0_rmse: 0.61217 | val_1_rmse: 0.62434 |  0:00:04s
epoch 4  | loss: 0.27962 | val_0_rmse: 0.59443 | val_1_rmse: 0.60783 |  0:00:05s
epoch 5  | loss: 0.26838 | val_0_rmse: 0.59417 | val_1_rmse: 0.60889 |  0:00:06s
epoch 6  | loss: 0.26468 | val_0_rmse: 0.61654 | val_1_rmse: 0.63016 |  0:00:07s
epoch 7  | loss: 0.25522 | val_0_rmse: 0.5939  | val_1_rmse: 0.6075  |  0:00:09s
epoch 8  | loss: 0.24875 | val_0_rmse: 0.59803 | val_1_rmse: 0.61329 |  0:00:10s
epoch 9  | loss: 0.24882 | val_0_rmse: 0.57405 | val_1_rmse: 0.5932  |  0:00:11s
epoch 10 | loss: 0.2472  | val_0_rmse: 0.56311 | val_1_rmse: 0.57883 |  0:00:12s
epoch 11 | loss: 0.2412  | val_0_rmse: 0.55441 | val_1_rmse: 0.57014 |  0:00:13s
epoch 12 | loss: 0.23424 | val_0_rmse: 0.54528 | val_1_rmse: 0.55868 |  0:00:14s
epoch 13 | loss: 0.24299 | val_0_rmse: 0.5744  | val_1_rmse: 0.59418 |  0:00:15s
epoch 14 | loss: 0.24056 | val_0_rmse: 0.56914 | val_1_rmse: 0.58821 |  0:00:17s
epoch 15 | loss: 0.23445 | val_0_rmse: 0.55947 | val_1_rmse: 0.57099 |  0:00:18s
epoch 16 | loss: 0.24033 | val_0_rmse: 0.52911 | val_1_rmse: 0.54527 |  0:00:19s
epoch 17 | loss: 0.2361  | val_0_rmse: 0.54569 | val_1_rmse: 0.55865 |  0:00:20s
epoch 18 | loss: 0.24313 | val_0_rmse: 0.55773 | val_1_rmse: 0.57236 |  0:00:21s
epoch 19 | loss: 0.25048 | val_0_rmse: 0.53578 | val_1_rmse: 0.55512 |  0:00:22s
epoch 20 | loss: 0.24456 | val_0_rmse: 0.5179  | val_1_rmse: 0.53534 |  0:00:23s
epoch 21 | loss: 0.23473 | val_0_rmse: 0.49574 | val_1_rmse: 0.51317 |  0:00:25s
epoch 22 | loss: 0.23238 | val_0_rmse: 0.48839 | val_1_rmse: 0.50681 |  0:00:26s
epoch 23 | loss: 0.23336 | val_0_rmse: 0.5109  | val_1_rmse: 0.53253 |  0:00:27s
epoch 24 | loss: 0.23857 | val_0_rmse: 0.50141 | val_1_rmse: 0.5243  |  0:00:28s
epoch 25 | loss: 0.22773 | val_0_rmse: 0.47282 | val_1_rmse: 0.49465 |  0:00:29s
epoch 26 | loss: 0.22577 | val_0_rmse: 0.48253 | val_1_rmse: 0.5077  |  0:00:30s
epoch 27 | loss: 0.23045 | val_0_rmse: 0.47302 | val_1_rmse: 0.49898 |  0:00:31s
epoch 28 | loss: 0.22255 | val_0_rmse: 0.46917 | val_1_rmse: 0.49514 |  0:00:33s
epoch 29 | loss: 0.22231 | val_0_rmse: 0.46994 | val_1_rmse: 0.4951  |  0:00:34s
epoch 30 | loss: 0.21995 | val_0_rmse: 0.46549 | val_1_rmse: 0.48929 |  0:00:35s
epoch 31 | loss: 0.2169  | val_0_rmse: 0.47149 | val_1_rmse: 0.49382 |  0:00:36s
epoch 32 | loss: 0.22157 | val_0_rmse: 0.46791 | val_1_rmse: 0.49191 |  0:00:37s
epoch 33 | loss: 0.21936 | val_0_rmse: 0.47189 | val_1_rmse: 0.49027 |  0:00:38s
epoch 34 | loss: 0.21914 | val_0_rmse: 0.46822 | val_1_rmse: 0.48748 |  0:00:39s
epoch 35 | loss: 0.21784 | val_0_rmse: 0.45216 | val_1_rmse: 0.47893 |  0:00:41s
epoch 36 | loss: 0.21266 | val_0_rmse: 0.45289 | val_1_rmse: 0.47884 |  0:00:42s
epoch 37 | loss: 0.2118  | val_0_rmse: 0.44589 | val_1_rmse: 0.47223 |  0:00:43s
epoch 38 | loss: 0.20793 | val_0_rmse: 0.44723 | val_1_rmse: 0.47341 |  0:00:44s
epoch 39 | loss: 0.21117 | val_0_rmse: 0.46365 | val_1_rmse: 0.49358 |  0:00:45s
epoch 40 | loss: 0.22326 | val_0_rmse: 0.46514 | val_1_rmse: 0.49776 |  0:00:46s
epoch 41 | loss: 0.21566 | val_0_rmse: 0.44489 | val_1_rmse: 0.47357 |  0:00:47s
epoch 42 | loss: 0.21079 | val_0_rmse: 0.44848 | val_1_rmse: 0.47661 |  0:00:49s
epoch 43 | loss: 0.21332 | val_0_rmse: 0.46181 | val_1_rmse: 0.49129 |  0:00:50s
epoch 44 | loss: 0.21415 | val_0_rmse: 0.44506 | val_1_rmse: 0.47546 |  0:00:51s
epoch 45 | loss: 0.21285 | val_0_rmse: 0.44869 | val_1_rmse: 0.47516 |  0:00:52s
epoch 46 | loss: 0.20733 | val_0_rmse: 0.44157 | val_1_rmse: 0.47074 |  0:00:53s
epoch 47 | loss: 0.20691 | val_0_rmse: 0.4367  | val_1_rmse: 0.47481 |  0:00:54s
epoch 48 | loss: 0.20571 | val_0_rmse: 0.44028 | val_1_rmse: 0.47726 |  0:00:55s
epoch 49 | loss: 0.19993 | val_0_rmse: 0.44001 | val_1_rmse: 0.47681 |  0:00:57s
epoch 50 | loss: 0.19721 | val_0_rmse: 0.43073 | val_1_rmse: 0.46366 |  0:00:58s
epoch 51 | loss: 0.20275 | val_0_rmse: 0.43328 | val_1_rmse: 0.46767 |  0:00:59s
epoch 52 | loss: 0.19893 | val_0_rmse: 0.43001 | val_1_rmse: 0.46767 |  0:01:00s
epoch 53 | loss: 0.19855 | val_0_rmse: 0.42688 | val_1_rmse: 0.46526 |  0:01:01s
epoch 54 | loss: 0.19768 | val_0_rmse: 0.42827 | val_1_rmse: 0.46411 |  0:01:02s
epoch 55 | loss: 0.19721 | val_0_rmse: 0.42408 | val_1_rmse: 0.46318 |  0:01:03s
epoch 56 | loss: 0.19399 | val_0_rmse: 0.43384 | val_1_rmse: 0.47272 |  0:01:05s
epoch 57 | loss: 0.19328 | val_0_rmse: 0.42411 | val_1_rmse: 0.46449 |  0:01:06s
epoch 58 | loss: 0.19439 | val_0_rmse: 0.44173 | val_1_rmse: 0.48533 |  0:01:07s
epoch 59 | loss: 0.19493 | val_0_rmse: 0.42482 | val_1_rmse: 0.4675  |  0:01:08s
epoch 60 | loss: 0.19065 | val_0_rmse: 0.4221  | val_1_rmse: 0.46391 |  0:01:09s
epoch 61 | loss: 0.19276 | val_0_rmse: 0.42526 | val_1_rmse: 0.47087 |  0:01:10s
epoch 62 | loss: 0.19489 | val_0_rmse: 0.42395 | val_1_rmse: 0.47406 |  0:01:11s
epoch 63 | loss: 0.18744 | val_0_rmse: 0.41843 | val_1_rmse: 0.46365 |  0:01:13s
epoch 64 | loss: 0.19002 | val_0_rmse: 0.41844 | val_1_rmse: 0.46677 |  0:01:14s
epoch 65 | loss: 0.18924 | val_0_rmse: 0.41781 | val_1_rmse: 0.46994 |  0:01:15s
epoch 66 | loss: 0.19204 | val_0_rmse: 0.41524 | val_1_rmse: 0.46912 |  0:01:16s
epoch 67 | loss: 0.18634 | val_0_rmse: 0.41605 | val_1_rmse: 0.46178 |  0:01:17s
epoch 68 | loss: 0.18629 | val_0_rmse: 0.41447 | val_1_rmse: 0.46632 |  0:01:18s
epoch 69 | loss: 0.18407 | val_0_rmse: 0.41645 | val_1_rmse: 0.46923 |  0:01:19s
epoch 70 | loss: 0.18379 | val_0_rmse: 0.41915 | val_1_rmse: 0.47176 |  0:01:20s
epoch 71 | loss: 0.19046 | val_0_rmse: 0.42016 | val_1_rmse: 0.47304 |  0:01:22s
epoch 72 | loss: 0.18434 | val_0_rmse: 0.41279 | val_1_rmse: 0.46991 |  0:01:23s
epoch 73 | loss: 0.18587 | val_0_rmse: 0.41177 | val_1_rmse: 0.46494 |  0:01:24s
epoch 74 | loss: 0.17948 | val_0_rmse: 0.40865 | val_1_rmse: 0.46838 |  0:01:25s
epoch 75 | loss: 0.18188 | val_0_rmse: 0.41902 | val_1_rmse: 0.4691  |  0:01:26s
epoch 76 | loss: 0.18822 | val_0_rmse: 0.41163 | val_1_rmse: 0.46967 |  0:01:27s
epoch 77 | loss: 0.18618 | val_0_rmse: 0.42611 | val_1_rmse: 0.47732 |  0:01:28s
epoch 78 | loss: 0.18476 | val_0_rmse: 0.41262 | val_1_rmse: 0.46748 |  0:01:30s
epoch 79 | loss: 0.18014 | val_0_rmse: 0.40751 | val_1_rmse: 0.46612 |  0:01:31s
epoch 80 | loss: 0.1768  | val_0_rmse: 0.40629 | val_1_rmse: 0.46474 |  0:01:32s
epoch 81 | loss: 0.17782 | val_0_rmse: 0.4117  | val_1_rmse: 0.46757 |  0:01:33s
epoch 82 | loss: 0.17776 | val_0_rmse: 0.40407 | val_1_rmse: 0.46926 |  0:01:34s
epoch 83 | loss: 0.17791 | val_0_rmse: 0.41128 | val_1_rmse: 0.46895 |  0:01:35s
epoch 84 | loss: 0.17998 | val_0_rmse: 0.40867 | val_1_rmse: 0.47538 |  0:01:36s
epoch 85 | loss: 0.17709 | val_0_rmse: 0.40253 | val_1_rmse: 0.46776 |  0:01:38s
epoch 86 | loss: 0.17634 | val_0_rmse: 0.40088 | val_1_rmse: 0.4672  |  0:01:39s
epoch 87 | loss: 0.17551 | val_0_rmse: 0.40065 | val_1_rmse: 0.46588 |  0:01:40s
epoch 88 | loss: 0.1755  | val_0_rmse: 0.40036 | val_1_rmse: 0.47041 |  0:01:41s
epoch 89 | loss: 0.17672 | val_0_rmse: 0.40185 | val_1_rmse: 0.46699 |  0:01:42s
epoch 90 | loss: 0.17876 | val_0_rmse: 0.40665 | val_1_rmse: 0.4649  |  0:01:43s
epoch 91 | loss: 0.17693 | val_0_rmse: 0.41765 | val_1_rmse: 0.48189 |  0:01:44s
epoch 92 | loss: 0.17588 | val_0_rmse: 0.4008  | val_1_rmse: 0.46429 |  0:01:46s
epoch 93 | loss: 0.17377 | val_0_rmse: 0.39823 | val_1_rmse: 0.46456 |  0:01:47s
epoch 94 | loss: 0.17272 | val_0_rmse: 0.3967  | val_1_rmse: 0.46497 |  0:01:48s
epoch 95 | loss: 0.16879 | val_0_rmse: 0.39445 | val_1_rmse: 0.46675 |  0:01:49s
epoch 96 | loss: 0.1696  | val_0_rmse: 0.39422 | val_1_rmse: 0.46644 |  0:01:50s
epoch 97 | loss: 0.16891 | val_0_rmse: 0.39426 | val_1_rmse: 0.46262 |  0:01:51s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.46178
Best weights from best epoch are automatically used!
ended training at: 16:27:41
Feature importance:
Mean squared error is of 6197997304.520169
Mean absolute error:55572.14916187721
MAPE:0.1494023327734704
R2 score:0.8011911645706656
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:27:41
epoch 0  | loss: 1.90503 | val_0_rmse: 0.99615 | val_1_rmse: 1.01794 |  0:00:00s
epoch 1  | loss: 1.15405 | val_0_rmse: 0.99647 | val_1_rmse: 1.01833 |  0:00:01s
epoch 2  | loss: 1.01684 | val_0_rmse: 0.99516 | val_1_rmse: 1.01572 |  0:00:01s
epoch 3  | loss: 0.9986  | val_0_rmse: 0.99203 | val_1_rmse: 1.01308 |  0:00:02s
epoch 4  | loss: 0.97897 | val_0_rmse: 0.98651 | val_1_rmse: 1.00635 |  0:00:03s
epoch 5  | loss: 0.95814 | val_0_rmse: 0.97524 | val_1_rmse: 0.99178 |  0:00:03s
epoch 6  | loss: 0.91049 | val_0_rmse: 0.97231 | val_1_rmse: 0.98864 |  0:00:04s
epoch 7  | loss: 0.83758 | val_0_rmse: 0.91878 | val_1_rmse: 0.92951 |  0:00:05s
epoch 8  | loss: 0.73172 | val_0_rmse: 0.97467 | val_1_rmse: 0.97173 |  0:00:05s
epoch 9  | loss: 0.58455 | val_0_rmse: 1.00872 | val_1_rmse: 1.00776 |  0:00:06s
epoch 10 | loss: 0.49014 | val_0_rmse: 0.92562 | val_1_rmse: 0.92605 |  0:00:06s
epoch 11 | loss: 0.42923 | val_0_rmse: 0.88927 | val_1_rmse: 0.89161 |  0:00:07s
epoch 12 | loss: 0.37777 | val_0_rmse: 0.82413 | val_1_rmse: 0.82499 |  0:00:08s
epoch 13 | loss: 0.35438 | val_0_rmse: 0.81749 | val_1_rmse: 0.8161  |  0:00:08s
epoch 14 | loss: 0.33236 | val_0_rmse: 0.82179 | val_1_rmse: 0.82028 |  0:00:09s
epoch 15 | loss: 0.32133 | val_0_rmse: 0.80673 | val_1_rmse: 0.81022 |  0:00:10s
epoch 16 | loss: 0.30919 | val_0_rmse: 0.82112 | val_1_rmse: 0.82021 |  0:00:10s
epoch 17 | loss: 0.29522 | val_0_rmse: 0.83848 | val_1_rmse: 0.83275 |  0:00:11s
epoch 18 | loss: 0.27154 | val_0_rmse: 0.79721 | val_1_rmse: 0.79967 |  0:00:11s
epoch 19 | loss: 0.26638 | val_0_rmse: 0.79632 | val_1_rmse: 0.79185 |  0:00:12s
epoch 20 | loss: 0.25756 | val_0_rmse: 0.78754 | val_1_rmse: 0.78516 |  0:00:13s
epoch 21 | loss: 0.25296 | val_0_rmse: 0.80612 | val_1_rmse: 0.80843 |  0:00:13s
epoch 22 | loss: 0.24748 | val_0_rmse: 0.80486 | val_1_rmse: 0.8073  |  0:00:14s
epoch 23 | loss: 0.24155 | val_0_rmse: 0.77529 | val_1_rmse: 0.77821 |  0:00:15s
epoch 24 | loss: 0.24077 | val_0_rmse: 0.77913 | val_1_rmse: 0.7792  |  0:00:15s
epoch 25 | loss: 0.23181 | val_0_rmse: 0.78133 | val_1_rmse: 0.78453 |  0:00:16s
epoch 26 | loss: 0.22814 | val_0_rmse: 0.75989 | val_1_rmse: 0.76165 |  0:00:16s
epoch 27 | loss: 0.22637 | val_0_rmse: 0.76766 | val_1_rmse: 0.76964 |  0:00:17s
epoch 28 | loss: 0.22297 | val_0_rmse: 0.75882 | val_1_rmse: 0.76079 |  0:00:18s
epoch 29 | loss: 0.21425 | val_0_rmse: 0.74594 | val_1_rmse: 0.74737 |  0:00:18s
epoch 30 | loss: 0.21366 | val_0_rmse: 0.74192 | val_1_rmse: 0.74348 |  0:00:19s
epoch 31 | loss: 0.21227 | val_0_rmse: 0.72978 | val_1_rmse: 0.73233 |  0:00:20s
epoch 32 | loss: 0.22266 | val_0_rmse: 0.76083 | val_1_rmse: 0.75587 |  0:00:20s
epoch 33 | loss: 0.20977 | val_0_rmse: 0.72608 | val_1_rmse: 0.73083 |  0:00:21s
epoch 34 | loss: 0.20571 | val_0_rmse: 0.73457 | val_1_rmse: 0.73556 |  0:00:22s
epoch 35 | loss: 0.21669 | val_0_rmse: 0.72322 | val_1_rmse: 0.73271 |  0:00:22s
epoch 36 | loss: 0.21813 | val_0_rmse: 0.7314  | val_1_rmse: 0.73342 |  0:00:23s
epoch 37 | loss: 0.20507 | val_0_rmse: 0.69475 | val_1_rmse: 0.70108 |  0:00:23s
epoch 38 | loss: 0.20641 | val_0_rmse: 0.69448 | val_1_rmse: 0.70666 |  0:00:24s
epoch 39 | loss: 0.20785 | val_0_rmse: 0.68955 | val_1_rmse: 0.69605 |  0:00:25s
epoch 40 | loss: 0.20131 | val_0_rmse: 0.68099 | val_1_rmse: 0.69472 |  0:00:25s
epoch 41 | loss: 0.19974 | val_0_rmse: 0.68008 | val_1_rmse: 0.68876 |  0:00:26s
epoch 42 | loss: 0.19823 | val_0_rmse: 0.67918 | val_1_rmse: 0.69358 |  0:00:27s
epoch 43 | loss: 0.19731 | val_0_rmse: 0.6691  | val_1_rmse: 0.6768  |  0:00:27s
epoch 44 | loss: 0.19945 | val_0_rmse: 0.65434 | val_1_rmse: 0.67117 |  0:00:28s
epoch 45 | loss: 0.19629 | val_0_rmse: 0.6634  | val_1_rmse: 0.66739 |  0:00:29s
epoch 46 | loss: 0.20255 | val_0_rmse: 0.64548 | val_1_rmse: 0.65341 |  0:00:29s
epoch 47 | loss: 0.19275 | val_0_rmse: 0.62585 | val_1_rmse: 0.63685 |  0:00:30s
epoch 48 | loss: 0.18686 | val_0_rmse: 0.62425 | val_1_rmse: 0.63824 |  0:00:30s
epoch 49 | loss: 0.18995 | val_0_rmse: 0.62704 | val_1_rmse: 0.64391 |  0:00:31s
epoch 50 | loss: 0.19183 | val_0_rmse: 0.61712 | val_1_rmse: 0.63033 |  0:00:32s
epoch 51 | loss: 0.19766 | val_0_rmse: 0.6189  | val_1_rmse: 0.6407  |  0:00:32s
epoch 52 | loss: 0.19217 | val_0_rmse: 0.60231 | val_1_rmse: 0.62256 |  0:00:33s
epoch 53 | loss: 0.19631 | val_0_rmse: 0.58508 | val_1_rmse: 0.61344 |  0:00:34s
epoch 54 | loss: 0.18383 | val_0_rmse: 0.57509 | val_1_rmse: 0.60089 |  0:00:34s
epoch 55 | loss: 0.1777  | val_0_rmse: 0.56364 | val_1_rmse: 0.59351 |  0:00:35s
epoch 56 | loss: 0.1766  | val_0_rmse: 0.55411 | val_1_rmse: 0.59036 |  0:00:35s
epoch 57 | loss: 0.18166 | val_0_rmse: 0.55912 | val_1_rmse: 0.59519 |  0:00:36s
epoch 58 | loss: 0.17712 | val_0_rmse: 0.54254 | val_1_rmse: 0.58074 |  0:00:37s
epoch 59 | loss: 0.18225 | val_0_rmse: 0.55131 | val_1_rmse: 0.59298 |  0:00:37s
epoch 60 | loss: 0.1746  | val_0_rmse: 0.53444 | val_1_rmse: 0.58143 |  0:00:38s
epoch 61 | loss: 0.17638 | val_0_rmse: 0.52982 | val_1_rmse: 0.58299 |  0:00:39s
epoch 62 | loss: 0.1721  | val_0_rmse: 0.51435 | val_1_rmse: 0.57304 |  0:00:39s
epoch 63 | loss: 0.16968 | val_0_rmse: 0.50563 | val_1_rmse: 0.57119 |  0:00:40s
epoch 64 | loss: 0.16494 | val_0_rmse: 0.50094 | val_1_rmse: 0.56763 |  0:00:40s
epoch 65 | loss: 0.17162 | val_0_rmse: 0.49266 | val_1_rmse: 0.56178 |  0:00:41s
epoch 66 | loss: 0.17302 | val_0_rmse: 0.49188 | val_1_rmse: 0.56201 |  0:00:42s
epoch 67 | loss: 0.17515 | val_0_rmse: 0.49837 | val_1_rmse: 0.57241 |  0:00:42s
epoch 68 | loss: 0.17247 | val_0_rmse: 0.48593 | val_1_rmse: 0.56242 |  0:00:43s
epoch 69 | loss: 0.16641 | val_0_rmse: 0.46472 | val_1_rmse: 0.55769 |  0:00:44s
epoch 70 | loss: 0.16002 | val_0_rmse: 0.47736 | val_1_rmse: 0.56785 |  0:00:44s
epoch 71 | loss: 0.15952 | val_0_rmse: 0.44626 | val_1_rmse: 0.55309 |  0:00:45s
epoch 72 | loss: 0.15461 | val_0_rmse: 0.4565  | val_1_rmse: 0.56241 |  0:00:45s
epoch 73 | loss: 0.1626  | val_0_rmse: 0.45133 | val_1_rmse: 0.5565  |  0:00:46s
epoch 74 | loss: 0.16263 | val_0_rmse: 0.4297  | val_1_rmse: 0.54363 |  0:00:47s
epoch 75 | loss: 0.15534 | val_0_rmse: 0.44233 | val_1_rmse: 0.55036 |  0:00:47s
epoch 76 | loss: 0.14918 | val_0_rmse: 0.41221 | val_1_rmse: 0.54929 |  0:00:48s
epoch 77 | loss: 0.15268 | val_0_rmse: 0.45338 | val_1_rmse: 0.56057 |  0:00:49s
epoch 78 | loss: 0.1538  | val_0_rmse: 0.40797 | val_1_rmse: 0.54872 |  0:00:49s
epoch 79 | loss: 0.14903 | val_0_rmse: 0.3982  | val_1_rmse: 0.54503 |  0:00:50s
epoch 80 | loss: 0.14684 | val_0_rmse: 0.40978 | val_1_rmse: 0.54913 |  0:00:51s
epoch 81 | loss: 0.14696 | val_0_rmse: 0.40188 | val_1_rmse: 0.55313 |  0:00:51s
epoch 82 | loss: 0.15238 | val_0_rmse: 0.40139 | val_1_rmse: 0.54534 |  0:00:52s
epoch 83 | loss: 0.15639 | val_0_rmse: 0.38293 | val_1_rmse: 0.54257 |  0:00:52s
epoch 84 | loss: 0.14651 | val_0_rmse: 0.39119 | val_1_rmse: 0.54679 |  0:00:53s
epoch 85 | loss: 0.14123 | val_0_rmse: 0.37941 | val_1_rmse: 0.53807 |  0:00:54s
epoch 86 | loss: 0.14552 | val_0_rmse: 0.40199 | val_1_rmse: 0.55461 |  0:00:54s
epoch 87 | loss: 0.14456 | val_0_rmse: 0.3773  | val_1_rmse: 0.5371  |  0:00:55s
epoch 88 | loss: 0.14232 | val_0_rmse: 0.35957 | val_1_rmse: 0.54096 |  0:00:56s
epoch 89 | loss: 0.14289 | val_0_rmse: 0.36999 | val_1_rmse: 0.53775 |  0:00:56s
epoch 90 | loss: 0.14197 | val_0_rmse: 0.36532 | val_1_rmse: 0.54502 |  0:00:57s
epoch 91 | loss: 0.14217 | val_0_rmse: 0.34719 | val_1_rmse: 0.53913 |  0:00:57s
epoch 92 | loss: 0.13919 | val_0_rmse: 0.35441 | val_1_rmse: 0.53696 |  0:00:58s
epoch 93 | loss: 0.13521 | val_0_rmse: 0.35003 | val_1_rmse: 0.54645 |  0:00:59s
epoch 94 | loss: 0.14084 | val_0_rmse: 0.34872 | val_1_rmse: 0.54139 |  0:00:59s
epoch 95 | loss: 0.13823 | val_0_rmse: 0.339   | val_1_rmse: 0.54269 |  0:01:00s
epoch 96 | loss: 0.13394 | val_0_rmse: 0.33857 | val_1_rmse: 0.54392 |  0:01:01s
epoch 97 | loss: 0.13964 | val_0_rmse: 0.33142 | val_1_rmse: 0.55263 |  0:01:01s
epoch 98 | loss: 0.13781 | val_0_rmse: 0.34226 | val_1_rmse: 0.54616 |  0:01:02s
epoch 99 | loss: 0.13036 | val_0_rmse: 0.32582 | val_1_rmse: 0.5555  |  0:01:03s
epoch 100| loss: 0.13738 | val_0_rmse: 0.33035 | val_1_rmse: 0.56073 |  0:01:03s
epoch 101| loss: 0.13777 | val_0_rmse: 0.33818 | val_1_rmse: 0.55547 |  0:01:04s
epoch 102| loss: 0.12786 | val_0_rmse: 0.32279 | val_1_rmse: 0.55806 |  0:01:04s
epoch 103| loss: 0.13079 | val_0_rmse: 0.32478 | val_1_rmse: 0.5685  |  0:01:05s
epoch 104| loss: 0.1318  | val_0_rmse: 0.32465 | val_1_rmse: 0.55326 |  0:01:06s
epoch 105| loss: 0.12686 | val_0_rmse: 0.31748 | val_1_rmse: 0.5542  |  0:01:06s
epoch 106| loss: 0.12868 | val_0_rmse: 0.33319 | val_1_rmse: 0.55417 |  0:01:07s
epoch 107| loss: 0.12696 | val_0_rmse: 0.31518 | val_1_rmse: 0.54885 |  0:01:07s
epoch 108| loss: 0.13256 | val_0_rmse: 0.32497 | val_1_rmse: 0.54492 |  0:01:08s
epoch 109| loss: 0.13231 | val_0_rmse: 0.32849 | val_1_rmse: 0.55144 |  0:01:09s
epoch 110| loss: 0.13046 | val_0_rmse: 0.3098  | val_1_rmse: 0.56125 |  0:01:09s
epoch 111| loss: 0.12701 | val_0_rmse: 0.31904 | val_1_rmse: 0.54707 |  0:01:10s
epoch 112| loss: 0.13155 | val_0_rmse: 0.3264  | val_1_rmse: 0.55659 |  0:01:11s
epoch 113| loss: 0.1288  | val_0_rmse: 0.31856 | val_1_rmse: 0.54485 |  0:01:11s
epoch 114| loss: 0.12915 | val_0_rmse: 0.31438 | val_1_rmse: 0.55887 |  0:01:12s
epoch 115| loss: 0.12688 | val_0_rmse: 0.31524 | val_1_rmse: 0.55103 |  0:01:13s
epoch 116| loss: 0.12399 | val_0_rmse: 0.30172 | val_1_rmse: 0.55945 |  0:01:13s
epoch 117| loss: 0.11958 | val_0_rmse: 0.31098 | val_1_rmse: 0.54709 |  0:01:14s
epoch 118| loss: 0.1195  | val_0_rmse: 0.30104 | val_1_rmse: 0.56851 |  0:01:14s
epoch 119| loss: 0.1164  | val_0_rmse: 0.31441 | val_1_rmse: 0.55604 |  0:01:15s
epoch 120| loss: 0.12439 | val_0_rmse: 0.32639 | val_1_rmse: 0.56467 |  0:01:16s
epoch 121| loss: 0.11905 | val_0_rmse: 0.30105 | val_1_rmse: 0.55865 |  0:01:16s
epoch 122| loss: 0.12032 | val_0_rmse: 0.3072  | val_1_rmse: 0.55188 |  0:01:17s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.53696
Best weights from best epoch are automatically used!
ended training at: 16:28:59
Feature importance:
Mean squared error is of 23015517170.226402
Mean absolute error:107042.67900690064
MAPE:0.17502083545544
R2 score:0.7165054968823577
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:29:00
epoch 0  | loss: 1.82834 | val_0_rmse: 1.00663 | val_1_rmse: 1.00661 |  0:00:00s
epoch 1  | loss: 1.12114 | val_0_rmse: 1.00525 | val_1_rmse: 1.00429 |  0:00:01s
epoch 2  | loss: 0.99521 | val_0_rmse: 0.95503 | val_1_rmse: 0.95866 |  0:00:01s
epoch 3  | loss: 0.89371 | val_0_rmse: 0.92757 | val_1_rmse: 0.927   |  0:00:02s
epoch 4  | loss: 0.80247 | val_0_rmse: 0.93515 | val_1_rmse: 0.94969 |  0:00:03s
epoch 5  | loss: 0.74477 | val_0_rmse: 0.91802 | val_1_rmse: 0.93541 |  0:00:03s
epoch 6  | loss: 0.6814  | val_0_rmse: 0.84977 | val_1_rmse: 0.86901 |  0:00:04s
epoch 7  | loss: 0.62712 | val_0_rmse: 0.85736 | val_1_rmse: 0.87501 |  0:00:05s
epoch 8  | loss: 0.57858 | val_0_rmse: 0.82185 | val_1_rmse: 0.8416  |  0:00:05s
epoch 9  | loss: 0.50408 | val_0_rmse: 0.78018 | val_1_rmse: 0.78936 |  0:00:06s
epoch 10 | loss: 0.4713  | val_0_rmse: 0.75221 | val_1_rmse: 0.74617 |  0:00:06s
epoch 11 | loss: 0.42845 | val_0_rmse: 0.73899 | val_1_rmse: 0.73454 |  0:00:07s
epoch 12 | loss: 0.40108 | val_0_rmse: 0.7148  | val_1_rmse: 0.70702 |  0:00:08s
epoch 13 | loss: 0.38936 | val_0_rmse: 0.72767 | val_1_rmse: 0.70979 |  0:00:08s
epoch 14 | loss: 0.38937 | val_0_rmse: 0.72531 | val_1_rmse: 0.70735 |  0:00:09s
epoch 15 | loss: 0.38295 | val_0_rmse: 0.71113 | val_1_rmse: 0.7072  |  0:00:10s
epoch 16 | loss: 0.35952 | val_0_rmse: 0.70183 | val_1_rmse: 0.69415 |  0:00:10s
epoch 17 | loss: 0.34797 | val_0_rmse: 0.68792 | val_1_rmse: 0.68451 |  0:00:11s
epoch 18 | loss: 0.33332 | val_0_rmse: 0.71272 | val_1_rmse: 0.70607 |  0:00:11s
epoch 19 | loss: 0.33454 | val_0_rmse: 0.68029 | val_1_rmse: 0.67751 |  0:00:12s
epoch 20 | loss: 0.32691 | val_0_rmse: 0.64985 | val_1_rmse: 0.64059 |  0:00:13s
epoch 21 | loss: 0.32625 | val_0_rmse: 0.66863 | val_1_rmse: 0.66415 |  0:00:13s
epoch 22 | loss: 0.31716 | val_0_rmse: 0.69767 | val_1_rmse: 0.69185 |  0:00:14s
epoch 23 | loss: 0.31248 | val_0_rmse: 0.68228 | val_1_rmse: 0.67442 |  0:00:15s
epoch 24 | loss: 0.30672 | val_0_rmse: 0.6895  | val_1_rmse: 0.68609 |  0:00:15s
epoch 25 | loss: 0.31144 | val_0_rmse: 0.65719 | val_1_rmse: 0.6467  |  0:00:16s
epoch 26 | loss: 0.29058 | val_0_rmse: 0.65815 | val_1_rmse: 0.65068 |  0:00:16s
epoch 27 | loss: 0.29243 | val_0_rmse: 0.65614 | val_1_rmse: 0.65634 |  0:00:17s
epoch 28 | loss: 0.2925  | val_0_rmse: 0.65765 | val_1_rmse: 0.65627 |  0:00:18s
epoch 29 | loss: 0.28523 | val_0_rmse: 0.66573 | val_1_rmse: 0.66039 |  0:00:18s
epoch 30 | loss: 0.28604 | val_0_rmse: 0.63887 | val_1_rmse: 0.6407  |  0:00:19s
epoch 31 | loss: 0.27825 | val_0_rmse: 0.62881 | val_1_rmse: 0.6276  |  0:00:20s
epoch 32 | loss: 0.27777 | val_0_rmse: 0.63429 | val_1_rmse: 0.63411 |  0:00:20s
epoch 33 | loss: 0.26976 | val_0_rmse: 0.6204  | val_1_rmse: 0.61849 |  0:00:21s
epoch 34 | loss: 0.26464 | val_0_rmse: 0.61801 | val_1_rmse: 0.62378 |  0:00:22s
epoch 35 | loss: 0.26669 | val_0_rmse: 0.62507 | val_1_rmse: 0.62831 |  0:00:22s
epoch 36 | loss: 0.26284 | val_0_rmse: 0.61141 | val_1_rmse: 0.61634 |  0:00:23s
epoch 37 | loss: 0.25754 | val_0_rmse: 0.64427 | val_1_rmse: 0.65154 |  0:00:23s
epoch 38 | loss: 0.25454 | val_0_rmse: 0.60885 | val_1_rmse: 0.61398 |  0:00:24s
epoch 39 | loss: 0.25648 | val_0_rmse: 0.62187 | val_1_rmse: 0.62823 |  0:00:25s
epoch 40 | loss: 0.25404 | val_0_rmse: 0.60377 | val_1_rmse: 0.61287 |  0:00:25s
epoch 41 | loss: 0.25082 | val_0_rmse: 0.61375 | val_1_rmse: 0.62213 |  0:00:26s
epoch 42 | loss: 0.24954 | val_0_rmse: 0.60667 | val_1_rmse: 0.61922 |  0:00:27s
epoch 43 | loss: 0.24602 | val_0_rmse: 0.62769 | val_1_rmse: 0.64138 |  0:00:27s
epoch 44 | loss: 0.24206 | val_0_rmse: 0.60884 | val_1_rmse: 0.62141 |  0:00:28s
epoch 45 | loss: 0.23822 | val_0_rmse: 0.60092 | val_1_rmse: 0.6135  |  0:00:28s
epoch 46 | loss: 0.24499 | val_0_rmse: 0.58943 | val_1_rmse: 0.60022 |  0:00:29s
epoch 47 | loss: 0.23825 | val_0_rmse: 0.61476 | val_1_rmse: 0.62304 |  0:00:30s
epoch 48 | loss: 0.252   | val_0_rmse: 0.59069 | val_1_rmse: 0.5977  |  0:00:30s
epoch 49 | loss: 0.24666 | val_0_rmse: 0.57828 | val_1_rmse: 0.59477 |  0:00:31s
epoch 50 | loss: 0.23615 | val_0_rmse: 0.57259 | val_1_rmse: 0.59169 |  0:00:32s
epoch 51 | loss: 0.23669 | val_0_rmse: 0.56774 | val_1_rmse: 0.59014 |  0:00:32s
epoch 52 | loss: 0.22975 | val_0_rmse: 0.56158 | val_1_rmse: 0.58075 |  0:00:33s
epoch 53 | loss: 0.23017 | val_0_rmse: 0.57198 | val_1_rmse: 0.59422 |  0:00:34s
epoch 54 | loss: 0.22836 | val_0_rmse: 0.54848 | val_1_rmse: 0.57914 |  0:00:34s
epoch 55 | loss: 0.22736 | val_0_rmse: 0.55181 | val_1_rmse: 0.5807  |  0:00:35s
epoch 56 | loss: 0.23068 | val_0_rmse: 0.55133 | val_1_rmse: 0.58169 |  0:00:35s
epoch 57 | loss: 0.22891 | val_0_rmse: 0.54169 | val_1_rmse: 0.57164 |  0:00:36s
epoch 58 | loss: 0.23305 | val_0_rmse: 0.54354 | val_1_rmse: 0.57434 |  0:00:37s
epoch 59 | loss: 0.24442 | val_0_rmse: 0.54039 | val_1_rmse: 0.57042 |  0:00:37s
epoch 60 | loss: 0.23228 | val_0_rmse: 0.53769 | val_1_rmse: 0.56886 |  0:00:38s
epoch 61 | loss: 0.23553 | val_0_rmse: 0.52885 | val_1_rmse: 0.5626  |  0:00:39s
epoch 62 | loss: 0.22538 | val_0_rmse: 0.53088 | val_1_rmse: 0.56913 |  0:00:39s
epoch 63 | loss: 0.22652 | val_0_rmse: 0.52386 | val_1_rmse: 0.56388 |  0:00:40s
epoch 64 | loss: 0.22485 | val_0_rmse: 0.50519 | val_1_rmse: 0.54681 |  0:00:40s
epoch 65 | loss: 0.22822 | val_0_rmse: 0.51936 | val_1_rmse: 0.56216 |  0:00:41s
epoch 66 | loss: 0.23071 | val_0_rmse: 0.49593 | val_1_rmse: 0.54077 |  0:00:42s
epoch 67 | loss: 0.22887 | val_0_rmse: 0.54033 | val_1_rmse: 0.57554 |  0:00:42s
epoch 68 | loss: 0.22919 | val_0_rmse: 0.49282 | val_1_rmse: 0.54126 |  0:00:43s
epoch 69 | loss: 0.2185  | val_0_rmse: 0.50021 | val_1_rmse: 0.54234 |  0:00:44s
epoch 70 | loss: 0.21746 | val_0_rmse: 0.48772 | val_1_rmse: 0.53559 |  0:00:44s
epoch 71 | loss: 0.21151 | val_0_rmse: 0.48522 | val_1_rmse: 0.52973 |  0:00:45s
epoch 72 | loss: 0.21494 | val_0_rmse: 0.47334 | val_1_rmse: 0.51832 |  0:00:45s
epoch 73 | loss: 0.20799 | val_0_rmse: 0.489   | val_1_rmse: 0.53382 |  0:00:46s
epoch 74 | loss: 0.21305 | val_0_rmse: 0.47389 | val_1_rmse: 0.51903 |  0:00:47s
epoch 75 | loss: 0.21151 | val_0_rmse: 0.45839 | val_1_rmse: 0.51589 |  0:00:47s
epoch 76 | loss: 0.20154 | val_0_rmse: 0.46602 | val_1_rmse: 0.52168 |  0:00:48s
epoch 77 | loss: 0.20593 | val_0_rmse: 0.45912 | val_1_rmse: 0.51966 |  0:00:49s
epoch 78 | loss: 0.20775 | val_0_rmse: 0.46345 | val_1_rmse: 0.51924 |  0:00:49s
epoch 79 | loss: 0.20804 | val_0_rmse: 0.45775 | val_1_rmse: 0.51598 |  0:00:50s
epoch 80 | loss: 0.20253 | val_0_rmse: 0.4571  | val_1_rmse: 0.5183  |  0:00:50s
epoch 81 | loss: 0.20396 | val_0_rmse: 0.45346 | val_1_rmse: 0.51474 |  0:00:51s
epoch 82 | loss: 0.19899 | val_0_rmse: 0.44685 | val_1_rmse: 0.52328 |  0:00:52s
epoch 83 | loss: 0.19663 | val_0_rmse: 0.43875 | val_1_rmse: 0.51752 |  0:00:52s
epoch 84 | loss: 0.20159 | val_0_rmse: 0.45428 | val_1_rmse: 0.5189  |  0:00:53s
epoch 85 | loss: 0.19718 | val_0_rmse: 0.43217 | val_1_rmse: 0.50613 |  0:00:54s
epoch 86 | loss: 0.19692 | val_0_rmse: 0.43161 | val_1_rmse: 0.50744 |  0:00:54s
epoch 87 | loss: 0.19805 | val_0_rmse: 0.42936 | val_1_rmse: 0.5106  |  0:00:55s
epoch 88 | loss: 0.19464 | val_0_rmse: 0.45298 | val_1_rmse: 0.52608 |  0:00:55s
epoch 89 | loss: 0.19885 | val_0_rmse: 0.42255 | val_1_rmse: 0.51513 |  0:00:56s
epoch 90 | loss: 0.19928 | val_0_rmse: 0.45379 | val_1_rmse: 0.55319 |  0:00:57s
epoch 91 | loss: 0.21887 | val_0_rmse: 0.43201 | val_1_rmse: 0.51719 |  0:00:57s
epoch 92 | loss: 0.19856 | val_0_rmse: 0.43057 | val_1_rmse: 0.51619 |  0:00:58s
epoch 93 | loss: 0.19353 | val_0_rmse: 0.41874 | val_1_rmse: 0.51232 |  0:00:59s
epoch 94 | loss: 0.1904  | val_0_rmse: 0.41361 | val_1_rmse: 0.51378 |  0:00:59s
epoch 95 | loss: 0.19009 | val_0_rmse: 0.42509 | val_1_rmse: 0.51496 |  0:01:00s
epoch 96 | loss: 0.19079 | val_0_rmse: 0.41475 | val_1_rmse: 0.51583 |  0:01:00s
epoch 97 | loss: 0.18736 | val_0_rmse: 0.423   | val_1_rmse: 0.52561 |  0:01:01s
epoch 98 | loss: 0.18932 | val_0_rmse: 0.42119 | val_1_rmse: 0.512   |  0:01:02s
epoch 99 | loss: 0.18855 | val_0_rmse: 0.40957 | val_1_rmse: 0.52264 |  0:01:02s
epoch 100| loss: 0.18812 | val_0_rmse: 0.41787 | val_1_rmse: 0.53634 |  0:01:03s
epoch 101| loss: 0.18688 | val_0_rmse: 0.40774 | val_1_rmse: 0.51049 |  0:01:04s
epoch 102| loss: 0.18674 | val_0_rmse: 0.40229 | val_1_rmse: 0.51063 |  0:01:04s
epoch 103| loss: 0.18418 | val_0_rmse: 0.39946 | val_1_rmse: 0.52594 |  0:01:05s
epoch 104| loss: 0.18484 | val_0_rmse: 0.41167 | val_1_rmse: 0.52811 |  0:01:05s
epoch 105| loss: 0.18553 | val_0_rmse: 0.42455 | val_1_rmse: 0.55077 |  0:01:06s
epoch 106| loss: 0.17638 | val_0_rmse: 0.3978  | val_1_rmse: 0.50855 |  0:01:07s
epoch 107| loss: 0.18461 | val_0_rmse: 0.39871 | val_1_rmse: 0.50666 |  0:01:07s
epoch 108| loss: 0.18112 | val_0_rmse: 0.39658 | val_1_rmse: 0.5187  |  0:01:08s
epoch 109| loss: 0.1779  | val_0_rmse: 0.39618 | val_1_rmse: 0.51723 |  0:01:09s
epoch 110| loss: 0.17745 | val_0_rmse: 0.40848 | val_1_rmse: 0.51984 |  0:01:09s
epoch 111| loss: 0.18313 | val_0_rmse: 0.39822 | val_1_rmse: 0.52068 |  0:01:10s
epoch 112| loss: 0.18243 | val_0_rmse: 0.39396 | val_1_rmse: 0.51847 |  0:01:10s
epoch 113| loss: 0.18217 | val_0_rmse: 0.3909  | val_1_rmse: 0.52008 |  0:01:11s
epoch 114| loss: 0.17392 | val_0_rmse: 0.38273 | val_1_rmse: 0.50659 |  0:01:12s
epoch 115| loss: 0.17082 | val_0_rmse: 0.39616 | val_1_rmse: 0.5137  |  0:01:12s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.50613
Best weights from best epoch are automatically used!
ended training at: 16:30:13
Feature importance:
Mean squared error is of 21658820299.426495
Mean absolute error:107239.62370734753
MAPE:0.1896594889234386
R2 score:0.7159407717319597
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:30:13
epoch 0  | loss: 1.90299 | val_0_rmse: 0.99674 | val_1_rmse: 1.01639 |  0:00:00s
epoch 1  | loss: 1.12231 | val_0_rmse: 0.99678 | val_1_rmse: 1.01713 |  0:00:01s
epoch 2  | loss: 0.99467 | val_0_rmse: 0.99564 | val_1_rmse: 1.01545 |  0:00:01s
epoch 3  | loss: 0.98369 | val_0_rmse: 0.99753 | val_1_rmse: 1.01773 |  0:00:02s
epoch 4  | loss: 0.94136 | val_0_rmse: 0.99369 | val_1_rmse: 1.01394 |  0:00:03s
epoch 5  | loss: 0.90712 | val_0_rmse: 1.00049 | val_1_rmse: 1.01954 |  0:00:03s
epoch 6  | loss: 0.86292 | val_0_rmse: 0.95744 | val_1_rmse: 0.97039 |  0:00:04s
epoch 7  | loss: 0.78365 | val_0_rmse: 0.89324 | val_1_rmse: 0.90909 |  0:00:05s
epoch 8  | loss: 0.69275 | val_0_rmse: 0.88424 | val_1_rmse: 0.90152 |  0:00:05s
epoch 9  | loss: 0.57155 | val_0_rmse: 0.87458 | val_1_rmse: 0.89462 |  0:00:06s
epoch 10 | loss: 0.51008 | val_0_rmse: 0.80067 | val_1_rmse: 0.82615 |  0:00:06s
epoch 11 | loss: 0.45658 | val_0_rmse: 0.74857 | val_1_rmse: 0.75825 |  0:00:07s
epoch 12 | loss: 0.41159 | val_0_rmse: 0.74665 | val_1_rmse: 0.76496 |  0:00:08s
epoch 13 | loss: 0.36932 | val_0_rmse: 0.77149 | val_1_rmse: 0.79562 |  0:00:08s
epoch 14 | loss: 0.33613 | val_0_rmse: 0.73058 | val_1_rmse: 0.75024 |  0:00:09s
epoch 15 | loss: 0.3148  | val_0_rmse: 0.73083 | val_1_rmse: 0.74625 |  0:00:10s
epoch 16 | loss: 0.30201 | val_0_rmse: 0.75341 | val_1_rmse: 0.77731 |  0:00:10s
epoch 17 | loss: 0.28809 | val_0_rmse: 0.74166 | val_1_rmse: 0.7637  |  0:00:11s
epoch 18 | loss: 0.27389 | val_0_rmse: 0.74316 | val_1_rmse: 0.76203 |  0:00:11s
epoch 19 | loss: 0.27105 | val_0_rmse: 0.759   | val_1_rmse: 0.78395 |  0:00:12s
epoch 20 | loss: 0.26106 | val_0_rmse: 0.79809 | val_1_rmse: 0.82684 |  0:00:13s
epoch 21 | loss: 0.26547 | val_0_rmse: 0.75642 | val_1_rmse: 0.78555 |  0:00:13s
epoch 22 | loss: 0.24928 | val_0_rmse: 0.72441 | val_1_rmse: 0.74553 |  0:00:14s
epoch 23 | loss: 0.2448  | val_0_rmse: 0.71476 | val_1_rmse: 0.73849 |  0:00:15s
epoch 24 | loss: 0.24474 | val_0_rmse: 0.72227 | val_1_rmse: 0.74881 |  0:00:15s
epoch 25 | loss: 0.23949 | val_0_rmse: 0.71113 | val_1_rmse: 0.73563 |  0:00:16s
epoch 26 | loss: 0.23714 | val_0_rmse: 0.68931 | val_1_rmse: 0.71615 |  0:00:17s
epoch 27 | loss: 0.23754 | val_0_rmse: 0.70015 | val_1_rmse: 0.72819 |  0:00:17s
epoch 28 | loss: 0.23876 | val_0_rmse: 0.67839 | val_1_rmse: 0.70307 |  0:00:18s
epoch 29 | loss: 0.2262  | val_0_rmse: 0.715   | val_1_rmse: 0.7392  |  0:00:19s
epoch 30 | loss: 0.22496 | val_0_rmse: 0.66774 | val_1_rmse: 0.69114 |  0:00:19s
epoch 31 | loss: 0.22596 | val_0_rmse: 0.665   | val_1_rmse: 0.68705 |  0:00:20s
epoch 32 | loss: 0.22097 | val_0_rmse: 0.6836  | val_1_rmse: 0.71286 |  0:00:20s
epoch 33 | loss: 0.22876 | val_0_rmse: 0.65993 | val_1_rmse: 0.68493 |  0:00:21s
epoch 34 | loss: 0.22109 | val_0_rmse: 0.62943 | val_1_rmse: 0.65263 |  0:00:22s
epoch 35 | loss: 0.22205 | val_0_rmse: 0.64796 | val_1_rmse: 0.66737 |  0:00:22s
epoch 36 | loss: 0.21862 | val_0_rmse: 0.64665 | val_1_rmse: 0.67281 |  0:00:23s
epoch 37 | loss: 0.21781 | val_0_rmse: 0.63904 | val_1_rmse: 0.66198 |  0:00:24s
epoch 38 | loss: 0.21392 | val_0_rmse: 0.63387 | val_1_rmse: 0.65538 |  0:00:24s
epoch 39 | loss: 0.21543 | val_0_rmse: 0.63648 | val_1_rmse: 0.66332 |  0:00:25s
epoch 40 | loss: 0.21461 | val_0_rmse: 0.63095 | val_1_rmse: 0.6531  |  0:00:25s
epoch 41 | loss: 0.21292 | val_0_rmse: 0.60115 | val_1_rmse: 0.62573 |  0:00:26s
epoch 42 | loss: 0.20456 | val_0_rmse: 0.62644 | val_1_rmse: 0.65286 |  0:00:27s
epoch 43 | loss: 0.20901 | val_0_rmse: 0.60596 | val_1_rmse: 0.63319 |  0:00:27s
epoch 44 | loss: 0.20335 | val_0_rmse: 0.62431 | val_1_rmse: 0.65356 |  0:00:28s
epoch 45 | loss: 0.20047 | val_0_rmse: 0.5973  | val_1_rmse: 0.62348 |  0:00:29s
epoch 46 | loss: 0.19901 | val_0_rmse: 0.59771 | val_1_rmse: 0.63015 |  0:00:29s
epoch 47 | loss: 0.20338 | val_0_rmse: 0.57958 | val_1_rmse: 0.61114 |  0:00:30s
epoch 48 | loss: 0.19889 | val_0_rmse: 0.57413 | val_1_rmse: 0.60921 |  0:00:31s
epoch 49 | loss: 0.19786 | val_0_rmse: 0.56649 | val_1_rmse: 0.60029 |  0:00:31s
epoch 50 | loss: 0.19099 | val_0_rmse: 0.56402 | val_1_rmse: 0.5985  |  0:00:32s
epoch 51 | loss: 0.19242 | val_0_rmse: 0.56254 | val_1_rmse: 0.60124 |  0:00:32s
epoch 52 | loss: 0.19101 | val_0_rmse: 0.56552 | val_1_rmse: 0.59963 |  0:00:33s
epoch 53 | loss: 0.19041 | val_0_rmse: 0.54408 | val_1_rmse: 0.58133 |  0:00:34s
epoch 54 | loss: 0.19175 | val_0_rmse: 0.53611 | val_1_rmse: 0.57482 |  0:00:34s
epoch 55 | loss: 0.18718 | val_0_rmse: 0.52446 | val_1_rmse: 0.5683  |  0:00:35s
epoch 56 | loss: 0.18647 | val_0_rmse: 0.51274 | val_1_rmse: 0.55593 |  0:00:36s
epoch 57 | loss: 0.18266 | val_0_rmse: 0.52622 | val_1_rmse: 0.57223 |  0:00:36s
epoch 58 | loss: 0.1813  | val_0_rmse: 0.52647 | val_1_rmse: 0.57043 |  0:00:37s
epoch 59 | loss: 0.18222 | val_0_rmse: 0.52814 | val_1_rmse: 0.57951 |  0:00:38s
epoch 60 | loss: 0.17941 | val_0_rmse: 0.50677 | val_1_rmse: 0.56067 |  0:00:38s
epoch 61 | loss: 0.1895  | val_0_rmse: 0.50691 | val_1_rmse: 0.5596  |  0:00:39s
epoch 62 | loss: 0.17778 | val_0_rmse: 0.48744 | val_1_rmse: 0.54413 |  0:00:39s
epoch 63 | loss: 0.19004 | val_0_rmse: 0.49253 | val_1_rmse: 0.54558 |  0:00:40s
epoch 64 | loss: 0.17946 | val_0_rmse: 0.48318 | val_1_rmse: 0.54739 |  0:00:41s
epoch 65 | loss: 0.18156 | val_0_rmse: 0.48618 | val_1_rmse: 0.55493 |  0:00:41s
epoch 66 | loss: 0.17785 | val_0_rmse: 0.458   | val_1_rmse: 0.54207 |  0:00:42s
epoch 67 | loss: 0.17346 | val_0_rmse: 0.46072 | val_1_rmse: 0.54572 |  0:00:43s
epoch 68 | loss: 0.17184 | val_0_rmse: 0.48922 | val_1_rmse: 0.57291 |  0:00:43s
epoch 69 | loss: 0.17451 | val_0_rmse: 0.48686 | val_1_rmse: 0.55648 |  0:00:44s
epoch 70 | loss: 0.17907 | val_0_rmse: 0.45977 | val_1_rmse: 0.54678 |  0:00:44s
epoch 71 | loss: 0.16817 | val_0_rmse: 0.45226 | val_1_rmse: 0.55186 |  0:00:45s
epoch 72 | loss: 0.17076 | val_0_rmse: 0.48394 | val_1_rmse: 0.56009 |  0:00:46s
epoch 73 | loss: 0.17306 | val_0_rmse: 0.44054 | val_1_rmse: 0.5409  |  0:00:46s
epoch 74 | loss: 0.1833  | val_0_rmse: 0.43916 | val_1_rmse: 0.54441 |  0:00:47s
epoch 75 | loss: 0.17749 | val_0_rmse: 0.43924 | val_1_rmse: 0.54691 |  0:00:48s
epoch 76 | loss: 0.17286 | val_0_rmse: 0.44208 | val_1_rmse: 0.55247 |  0:00:48s
epoch 77 | loss: 0.17282 | val_0_rmse: 0.43083 | val_1_rmse: 0.55131 |  0:00:49s
epoch 78 | loss: 0.16809 | val_0_rmse: 0.42185 | val_1_rmse: 0.52685 |  0:00:49s
epoch 79 | loss: 0.16185 | val_0_rmse: 0.42737 | val_1_rmse: 0.54083 |  0:00:50s
epoch 80 | loss: 0.15647 | val_0_rmse: 0.39988 | val_1_rmse: 0.52071 |  0:00:51s
epoch 81 | loss: 0.16048 | val_0_rmse: 0.39685 | val_1_rmse: 0.51813 |  0:00:51s
epoch 82 | loss: 0.15388 | val_0_rmse: 0.39472 | val_1_rmse: 0.51474 |  0:00:52s
epoch 83 | loss: 0.15784 | val_0_rmse: 0.39387 | val_1_rmse: 0.51132 |  0:00:53s
epoch 84 | loss: 0.15057 | val_0_rmse: 0.38234 | val_1_rmse: 0.52414 |  0:00:53s
epoch 85 | loss: 0.15151 | val_0_rmse: 0.39547 | val_1_rmse: 0.51805 |  0:00:54s
epoch 86 | loss: 0.15251 | val_0_rmse: 0.37918 | val_1_rmse: 0.51744 |  0:00:55s
epoch 87 | loss: 0.15109 | val_0_rmse: 0.38039 | val_1_rmse: 0.53222 |  0:00:55s
epoch 88 | loss: 0.15735 | val_0_rmse: 0.39076 | val_1_rmse: 0.51734 |  0:00:56s
epoch 89 | loss: 0.14944 | val_0_rmse: 0.36655 | val_1_rmse: 0.51804 |  0:00:56s
epoch 90 | loss: 0.14463 | val_0_rmse: 0.37376 | val_1_rmse: 0.53669 |  0:00:57s
epoch 91 | loss: 0.14307 | val_0_rmse: 0.36264 | val_1_rmse: 0.52229 |  0:00:58s
epoch 92 | loss: 0.14307 | val_0_rmse: 0.36925 | val_1_rmse: 0.51821 |  0:00:58s
epoch 93 | loss: 0.14675 | val_0_rmse: 0.35851 | val_1_rmse: 0.52013 |  0:00:59s
epoch 94 | loss: 0.14018 | val_0_rmse: 0.35274 | val_1_rmse: 0.52035 |  0:01:00s
epoch 95 | loss: 0.14421 | val_0_rmse: 0.36332 | val_1_rmse: 0.51452 |  0:01:00s
epoch 96 | loss: 0.1422  | val_0_rmse: 0.34328 | val_1_rmse: 0.52519 |  0:01:01s
epoch 97 | loss: 0.13947 | val_0_rmse: 0.34486 | val_1_rmse: 0.51971 |  0:01:01s
epoch 98 | loss: 0.13346 | val_0_rmse: 0.33262 | val_1_rmse: 0.51087 |  0:01:02s
epoch 99 | loss: 0.13159 | val_0_rmse: 0.3369  | val_1_rmse: 0.52457 |  0:01:03s
epoch 100| loss: 0.13262 | val_0_rmse: 0.35225 | val_1_rmse: 0.5358  |  0:01:03s
epoch 101| loss: 0.13455 | val_0_rmse: 0.34072 | val_1_rmse: 0.5239  |  0:01:04s
epoch 102| loss: 0.14045 | val_0_rmse: 0.34592 | val_1_rmse: 0.52876 |  0:01:05s
epoch 103| loss: 0.13329 | val_0_rmse: 0.33696 | val_1_rmse: 0.51103 |  0:01:05s
epoch 104| loss: 0.12995 | val_0_rmse: 0.32533 | val_1_rmse: 0.51748 |  0:01:06s
epoch 105| loss: 0.12822 | val_0_rmse: 0.3266  | val_1_rmse: 0.51819 |  0:01:07s
epoch 106| loss: 0.12628 | val_0_rmse: 0.32157 | val_1_rmse: 0.52273 |  0:01:07s
epoch 107| loss: 0.12656 | val_0_rmse: 0.31757 | val_1_rmse: 0.52634 |  0:01:08s
epoch 108| loss: 0.12011 | val_0_rmse: 0.3177  | val_1_rmse: 0.54122 |  0:01:08s
epoch 109| loss: 0.1241  | val_0_rmse: 0.32186 | val_1_rmse: 0.52822 |  0:01:09s
epoch 110| loss: 0.12012 | val_0_rmse: 0.31851 | val_1_rmse: 0.54914 |  0:01:10s
epoch 111| loss: 0.11703 | val_0_rmse: 0.3047  | val_1_rmse: 0.52329 |  0:01:10s
epoch 112| loss: 0.11506 | val_0_rmse: 0.30051 | val_1_rmse: 0.53398 |  0:01:11s
epoch 113| loss: 0.12026 | val_0_rmse: 0.30685 | val_1_rmse: 0.5341  |  0:01:12s
epoch 114| loss: 0.12089 | val_0_rmse: 0.30638 | val_1_rmse: 0.53605 |  0:01:12s
epoch 115| loss: 0.11542 | val_0_rmse: 0.29944 | val_1_rmse: 0.53641 |  0:01:13s
epoch 116| loss: 0.11752 | val_0_rmse: 0.30254 | val_1_rmse: 0.54319 |  0:01:13s
epoch 117| loss: 0.1166  | val_0_rmse: 0.30036 | val_1_rmse: 0.53514 |  0:01:14s
epoch 118| loss: 0.1129  | val_0_rmse: 0.29531 | val_1_rmse: 0.54477 |  0:01:15s
epoch 119| loss: 0.12023 | val_0_rmse: 0.29384 | val_1_rmse: 0.53444 |  0:01:15s
epoch 120| loss: 0.11218 | val_0_rmse: 0.29385 | val_1_rmse: 0.55321 |  0:01:16s
epoch 121| loss: 0.11849 | val_0_rmse: 0.30585 | val_1_rmse: 0.54337 |  0:01:17s
epoch 122| loss: 0.12017 | val_0_rmse: 0.31678 | val_1_rmse: 0.54653 |  0:01:17s
epoch 123| loss: 0.12105 | val_0_rmse: 0.31321 | val_1_rmse: 0.54516 |  0:01:18s
epoch 124| loss: 0.11964 | val_0_rmse: 0.31605 | val_1_rmse: 0.55962 |  0:01:18s
epoch 125| loss: 0.12051 | val_0_rmse: 0.31636 | val_1_rmse: 0.54594 |  0:01:19s
epoch 126| loss: 0.11631 | val_0_rmse: 0.2956  | val_1_rmse: 0.55524 |  0:01:20s
epoch 127| loss: 0.11485 | val_0_rmse: 0.29319 | val_1_rmse: 0.54632 |  0:01:20s
epoch 128| loss: 0.11718 | val_0_rmse: 0.29078 | val_1_rmse: 0.5524  |  0:01:21s

Early stopping occured at epoch 128 with best_epoch = 98 and best_val_1_rmse = 0.51087
Best weights from best epoch are automatically used!
ended training at: 16:31:35
Feature importance:
Mean squared error is of 22026982579.832516
Mean absolute error:105092.15899638539
MAPE:0.17607835671820468
R2 score:0.7278600923272058
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:31:35
epoch 0  | loss: 2.08161 | val_0_rmse: 1.00519 | val_1_rmse: 1.00136 |  0:00:00s
epoch 1  | loss: 1.19585 | val_0_rmse: 0.99825 | val_1_rmse: 0.99707 |  0:00:01s
epoch 2  | loss: 1.04934 | val_0_rmse: 0.99018 | val_1_rmse: 0.98838 |  0:00:01s
epoch 3  | loss: 0.99116 | val_0_rmse: 0.99102 | val_1_rmse: 0.9912  |  0:00:02s
epoch 4  | loss: 0.96704 | val_0_rmse: 0.95442 | val_1_rmse: 0.95299 |  0:00:03s
epoch 5  | loss: 0.89719 | val_0_rmse: 0.89704 | val_1_rmse: 0.89852 |  0:00:03s
epoch 6  | loss: 0.78834 | val_0_rmse: 0.85895 | val_1_rmse: 0.85523 |  0:00:04s
epoch 7  | loss: 0.69418 | val_0_rmse: 0.85291 | val_1_rmse: 0.85645 |  0:00:05s
epoch 8  | loss: 0.60059 | val_0_rmse: 0.85011 | val_1_rmse: 0.85891 |  0:00:05s
epoch 9  | loss: 0.50762 | val_0_rmse: 0.85006 | val_1_rmse: 0.84999 |  0:00:06s
epoch 10 | loss: 0.43598 | val_0_rmse: 0.83121 | val_1_rmse: 0.8365  |  0:00:06s
epoch 11 | loss: 0.3743  | val_0_rmse: 0.86863 | val_1_rmse: 0.8819  |  0:00:07s
epoch 12 | loss: 0.34951 | val_0_rmse: 0.80205 | val_1_rmse: 0.80749 |  0:00:08s
epoch 13 | loss: 0.34149 | val_0_rmse: 0.80939 | val_1_rmse: 0.81519 |  0:00:08s
epoch 14 | loss: 0.32544 | val_0_rmse: 0.79289 | val_1_rmse: 0.80513 |  0:00:09s
epoch 15 | loss: 0.30557 | val_0_rmse: 0.82414 | val_1_rmse: 0.83031 |  0:00:10s
epoch 16 | loss: 0.30108 | val_0_rmse: 0.7775  | val_1_rmse: 0.78841 |  0:00:10s
epoch 17 | loss: 0.29635 | val_0_rmse: 0.77453 | val_1_rmse: 0.78601 |  0:00:11s
epoch 18 | loss: 0.27305 | val_0_rmse: 0.8086  | val_1_rmse: 0.82529 |  0:00:12s
epoch 19 | loss: 0.26564 | val_0_rmse: 0.7811  | val_1_rmse: 0.79433 |  0:00:12s
epoch 20 | loss: 0.26511 | val_0_rmse: 0.75961 | val_1_rmse: 0.7741  |  0:00:13s
epoch 21 | loss: 0.25182 | val_0_rmse: 0.74585 | val_1_rmse: 0.7569  |  0:00:13s
epoch 22 | loss: 0.25303 | val_0_rmse: 0.75408 | val_1_rmse: 0.76995 |  0:00:14s
epoch 23 | loss: 0.2398  | val_0_rmse: 0.74601 | val_1_rmse: 0.76252 |  0:00:15s
epoch 24 | loss: 0.23535 | val_0_rmse: 0.72923 | val_1_rmse: 0.74172 |  0:00:15s
epoch 25 | loss: 0.23508 | val_0_rmse: 0.73251 | val_1_rmse: 0.74253 |  0:00:16s
epoch 26 | loss: 0.23217 | val_0_rmse: 0.7316  | val_1_rmse: 0.74452 |  0:00:17s
epoch 27 | loss: 0.23254 | val_0_rmse: 0.72449 | val_1_rmse: 0.73875 |  0:00:17s
epoch 28 | loss: 0.23506 | val_0_rmse: 0.72272 | val_1_rmse: 0.73794 |  0:00:18s
epoch 29 | loss: 0.22369 | val_0_rmse: 0.71762 | val_1_rmse: 0.73449 |  0:00:18s
epoch 30 | loss: 0.2165  | val_0_rmse: 0.71185 | val_1_rmse: 0.72815 |  0:00:19s
epoch 31 | loss: 0.22299 | val_0_rmse: 0.695   | val_1_rmse: 0.71053 |  0:00:20s
epoch 32 | loss: 0.21307 | val_0_rmse: 0.69545 | val_1_rmse: 0.71398 |  0:00:20s
epoch 33 | loss: 0.21615 | val_0_rmse: 0.68998 | val_1_rmse: 0.71208 |  0:00:21s
epoch 34 | loss: 0.22524 | val_0_rmse: 0.68527 | val_1_rmse: 0.69961 |  0:00:22s
epoch 35 | loss: 0.21726 | val_0_rmse: 0.68392 | val_1_rmse: 0.70775 |  0:00:22s
epoch 36 | loss: 0.21513 | val_0_rmse: 0.66363 | val_1_rmse: 0.68272 |  0:00:23s
epoch 37 | loss: 0.2099  | val_0_rmse: 0.67334 | val_1_rmse: 0.69294 |  0:00:24s
epoch 38 | loss: 0.20845 | val_0_rmse: 0.66342 | val_1_rmse: 0.684   |  0:00:24s
epoch 39 | loss: 0.20537 | val_0_rmse: 0.65845 | val_1_rmse: 0.67459 |  0:00:25s
epoch 40 | loss: 0.2024  | val_0_rmse: 0.65089 | val_1_rmse: 0.66831 |  0:00:25s
epoch 41 | loss: 0.20127 | val_0_rmse: 0.65676 | val_1_rmse: 0.67723 |  0:00:26s
epoch 42 | loss: 0.19922 | val_0_rmse: 0.64058 | val_1_rmse: 0.65814 |  0:00:27s
epoch 43 | loss: 0.20555 | val_0_rmse: 0.63201 | val_1_rmse: 0.65401 |  0:00:27s
epoch 44 | loss: 0.19757 | val_0_rmse: 0.63562 | val_1_rmse: 0.66089 |  0:00:28s
epoch 45 | loss: 0.19498 | val_0_rmse: 0.63014 | val_1_rmse: 0.65031 |  0:00:29s
epoch 46 | loss: 0.19816 | val_0_rmse: 0.62297 | val_1_rmse: 0.64898 |  0:00:29s
epoch 47 | loss: 0.19479 | val_0_rmse: 0.62232 | val_1_rmse: 0.64729 |  0:00:30s
epoch 48 | loss: 0.192   | val_0_rmse: 0.6163  | val_1_rmse: 0.65542 |  0:00:30s
epoch 49 | loss: 0.1911  | val_0_rmse: 0.61146 | val_1_rmse: 0.63591 |  0:00:31s
epoch 50 | loss: 0.19051 | val_0_rmse: 0.59774 | val_1_rmse: 0.63211 |  0:00:32s
epoch 51 | loss: 0.18144 | val_0_rmse: 0.59638 | val_1_rmse: 0.62658 |  0:00:32s
epoch 52 | loss: 0.18365 | val_0_rmse: 0.59149 | val_1_rmse: 0.62101 |  0:00:33s
epoch 53 | loss: 0.18392 | val_0_rmse: 0.58994 | val_1_rmse: 0.62453 |  0:00:34s
epoch 54 | loss: 0.18291 | val_0_rmse: 0.56492 | val_1_rmse: 0.60262 |  0:00:34s
epoch 55 | loss: 0.17831 | val_0_rmse: 0.56301 | val_1_rmse: 0.60805 |  0:00:35s
epoch 56 | loss: 0.17907 | val_0_rmse: 0.564   | val_1_rmse: 0.60329 |  0:00:36s
epoch 57 | loss: 0.17559 | val_0_rmse: 0.54704 | val_1_rmse: 0.59702 |  0:00:36s
epoch 58 | loss: 0.17468 | val_0_rmse: 0.54291 | val_1_rmse: 0.58887 |  0:00:37s
epoch 59 | loss: 0.16974 | val_0_rmse: 0.52427 | val_1_rmse: 0.57856 |  0:00:37s
epoch 60 | loss: 0.17101 | val_0_rmse: 0.52687 | val_1_rmse: 0.58595 |  0:00:38s
epoch 61 | loss: 0.16725 | val_0_rmse: 0.51578 | val_1_rmse: 0.57788 |  0:00:39s
epoch 62 | loss: 0.17122 | val_0_rmse: 0.53805 | val_1_rmse: 0.59014 |  0:00:39s
epoch 63 | loss: 0.17685 | val_0_rmse: 0.51276 | val_1_rmse: 0.57633 |  0:00:40s
epoch 64 | loss: 0.1735  | val_0_rmse: 0.50489 | val_1_rmse: 0.57279 |  0:00:41s
epoch 65 | loss: 0.16504 | val_0_rmse: 0.48482 | val_1_rmse: 0.55776 |  0:00:41s
epoch 66 | loss: 0.16375 | val_0_rmse: 0.50168 | val_1_rmse: 0.57321 |  0:00:42s
epoch 67 | loss: 0.16765 | val_0_rmse: 0.48208 | val_1_rmse: 0.55814 |  0:00:42s
epoch 68 | loss: 0.16241 | val_0_rmse: 0.47103 | val_1_rmse: 0.55222 |  0:00:43s
epoch 69 | loss: 0.16293 | val_0_rmse: 0.47071 | val_1_rmse: 0.55594 |  0:00:44s
epoch 70 | loss: 0.15418 | val_0_rmse: 0.44727 | val_1_rmse: 0.5522  |  0:00:44s
epoch 71 | loss: 0.15686 | val_0_rmse: 0.4638  | val_1_rmse: 0.56064 |  0:00:45s
epoch 72 | loss: 0.15312 | val_0_rmse: 0.46629 | val_1_rmse: 0.56325 |  0:00:46s
epoch 73 | loss: 0.15489 | val_0_rmse: 0.43653 | val_1_rmse: 0.55018 |  0:00:46s
epoch 74 | loss: 0.16091 | val_0_rmse: 0.43234 | val_1_rmse: 0.54887 |  0:00:47s
epoch 75 | loss: 0.1587  | val_0_rmse: 0.4577  | val_1_rmse: 0.55691 |  0:00:48s
epoch 76 | loss: 0.16824 | val_0_rmse: 0.41054 | val_1_rmse: 0.5454  |  0:00:48s
epoch 77 | loss: 0.15234 | val_0_rmse: 0.43484 | val_1_rmse: 0.55053 |  0:00:49s
epoch 78 | loss: 0.14893 | val_0_rmse: 0.40748 | val_1_rmse: 0.53965 |  0:00:49s
epoch 79 | loss: 0.14566 | val_0_rmse: 0.41613 | val_1_rmse: 0.53864 |  0:00:50s
epoch 80 | loss: 0.14804 | val_0_rmse: 0.40616 | val_1_rmse: 0.54819 |  0:00:51s
epoch 81 | loss: 0.1448  | val_0_rmse: 0.42023 | val_1_rmse: 0.54938 |  0:00:51s
epoch 82 | loss: 0.14721 | val_0_rmse: 0.39065 | val_1_rmse: 0.54434 |  0:00:52s
epoch 83 | loss: 0.14189 | val_0_rmse: 0.40189 | val_1_rmse: 0.54812 |  0:00:53s
epoch 84 | loss: 0.14488 | val_0_rmse: 0.37764 | val_1_rmse: 0.53167 |  0:00:53s
epoch 85 | loss: 0.14496 | val_0_rmse: 0.38049 | val_1_rmse: 0.53608 |  0:00:54s
epoch 86 | loss: 0.14108 | val_0_rmse: 0.37808 | val_1_rmse: 0.53236 |  0:00:54s
epoch 87 | loss: 0.14494 | val_0_rmse: 0.36875 | val_1_rmse: 0.52922 |  0:00:55s
epoch 88 | loss: 0.13836 | val_0_rmse: 0.36506 | val_1_rmse: 0.53929 |  0:00:56s
epoch 89 | loss: 0.14286 | val_0_rmse: 0.35424 | val_1_rmse: 0.53345 |  0:00:56s
epoch 90 | loss: 0.1357  | val_0_rmse: 0.34477 | val_1_rmse: 0.54455 |  0:00:57s
epoch 91 | loss: 0.13114 | val_0_rmse: 0.35977 | val_1_rmse: 0.54441 |  0:00:58s
epoch 92 | loss: 0.13696 | val_0_rmse: 0.34139 | val_1_rmse: 0.54233 |  0:00:58s
epoch 93 | loss: 0.13409 | val_0_rmse: 0.35038 | val_1_rmse: 0.53974 |  0:00:59s
epoch 94 | loss: 0.13208 | val_0_rmse: 0.33883 | val_1_rmse: 0.55462 |  0:00:59s
epoch 95 | loss: 0.13116 | val_0_rmse: 0.35094 | val_1_rmse: 0.54883 |  0:01:00s
epoch 96 | loss: 0.1265  | val_0_rmse: 0.32407 | val_1_rmse: 0.55058 |  0:01:01s
epoch 97 | loss: 0.13013 | val_0_rmse: 0.37111 | val_1_rmse: 0.55082 |  0:01:01s
epoch 98 | loss: 0.12689 | val_0_rmse: 0.3387  | val_1_rmse: 0.5717  |  0:01:02s
epoch 99 | loss: 0.1247  | val_0_rmse: 0.33205 | val_1_rmse: 0.53735 |  0:01:03s
epoch 100| loss: 0.12714 | val_0_rmse: 0.31823 | val_1_rmse: 0.55338 |  0:01:03s
epoch 101| loss: 0.12532 | val_0_rmse: 0.3264  | val_1_rmse: 0.54462 |  0:01:04s
epoch 102| loss: 0.12299 | val_0_rmse: 0.31184 | val_1_rmse: 0.55382 |  0:01:05s
epoch 103| loss: 0.11714 | val_0_rmse: 0.31579 | val_1_rmse: 0.54762 |  0:01:05s
epoch 104| loss: 0.1228  | val_0_rmse: 0.31528 | val_1_rmse: 0.55164 |  0:01:06s
epoch 105| loss: 0.11646 | val_0_rmse: 0.30069 | val_1_rmse: 0.55595 |  0:01:06s
epoch 106| loss: 0.11861 | val_0_rmse: 0.3124  | val_1_rmse: 0.54821 |  0:01:07s
epoch 107| loss: 0.11967 | val_0_rmse: 0.30345 | val_1_rmse: 0.546   |  0:01:08s
epoch 108| loss: 0.10952 | val_0_rmse: 0.2958  | val_1_rmse: 0.55418 |  0:01:08s
epoch 109| loss: 0.11342 | val_0_rmse: 0.29908 | val_1_rmse: 0.55696 |  0:01:09s
epoch 110| loss: 0.11698 | val_0_rmse: 0.2922  | val_1_rmse: 0.5499  |  0:01:10s
epoch 111| loss: 0.1186  | val_0_rmse: 0.29558 | val_1_rmse: 0.54928 |  0:01:10s
epoch 112| loss: 0.11353 | val_0_rmse: 0.28964 | val_1_rmse: 0.56762 |  0:01:11s
epoch 113| loss: 0.11971 | val_0_rmse: 0.30015 | val_1_rmse: 0.54918 |  0:01:11s
epoch 114| loss: 0.11397 | val_0_rmse: 0.289   | val_1_rmse: 0.55757 |  0:01:12s
epoch 115| loss: 0.10849 | val_0_rmse: 0.29468 | val_1_rmse: 0.56002 |  0:01:13s
epoch 116| loss: 0.11336 | val_0_rmse: 0.28534 | val_1_rmse: 0.56102 |  0:01:13s
epoch 117| loss: 0.11362 | val_0_rmse: 0.28348 | val_1_rmse: 0.55548 |  0:01:14s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.52922
Best weights from best epoch are automatically used!
ended training at: 16:32:49
Feature importance:
Mean squared error is of 24073357170.637478
Mean absolute error:111446.59672003154
MAPE:0.189770279777491
R2 score:0.7058519282798656
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:32:49
epoch 0  | loss: 2.06246 | val_0_rmse: 1.02589 | val_1_rmse: 0.99349 |  0:00:00s
epoch 1  | loss: 1.17613 | val_0_rmse: 1.01046 | val_1_rmse: 0.98115 |  0:00:01s
epoch 2  | loss: 1.07416 | val_0_rmse: 1.01382 | val_1_rmse: 0.98406 |  0:00:01s
epoch 3  | loss: 1.01355 | val_0_rmse: 1.00615 | val_1_rmse: 0.97575 |  0:00:02s
epoch 4  | loss: 0.98266 | val_0_rmse: 0.96304 | val_1_rmse: 0.93249 |  0:00:03s
epoch 5  | loss: 0.91003 | val_0_rmse: 0.9135  | val_1_rmse: 0.89493 |  0:00:03s
epoch 6  | loss: 0.79786 | val_0_rmse: 0.88658 | val_1_rmse: 0.8733  |  0:00:04s
epoch 7  | loss: 0.70089 | val_0_rmse: 0.94085 | val_1_rmse: 0.93083 |  0:00:05s
epoch 8  | loss: 0.59667 | val_0_rmse: 0.96184 | val_1_rmse: 0.94742 |  0:00:05s
epoch 9  | loss: 0.50946 | val_0_rmse: 0.96183 | val_1_rmse: 0.94387 |  0:00:06s
epoch 10 | loss: 0.43756 | val_0_rmse: 0.87602 | val_1_rmse: 0.85566 |  0:00:06s
epoch 11 | loss: 0.39008 | val_0_rmse: 0.87354 | val_1_rmse: 0.85703 |  0:00:07s
epoch 12 | loss: 0.36147 | val_0_rmse: 0.85073 | val_1_rmse: 0.82964 |  0:00:08s
epoch 13 | loss: 0.32851 | val_0_rmse: 0.83542 | val_1_rmse: 0.81112 |  0:00:08s
epoch 14 | loss: 0.32465 | val_0_rmse: 0.81264 | val_1_rmse: 0.78442 |  0:00:09s
epoch 15 | loss: 0.30967 | val_0_rmse: 0.80245 | val_1_rmse: 0.77795 |  0:00:10s
epoch 16 | loss: 0.2983  | val_0_rmse: 0.79215 | val_1_rmse: 0.76549 |  0:00:10s
epoch 17 | loss: 0.29239 | val_0_rmse: 0.79279 | val_1_rmse: 0.76964 |  0:00:11s
epoch 18 | loss: 0.2946  | val_0_rmse: 0.79403 | val_1_rmse: 0.76476 |  0:00:12s
epoch 19 | loss: 0.27304 | val_0_rmse: 0.77232 | val_1_rmse: 0.7446  |  0:00:12s
epoch 20 | loss: 0.26753 | val_0_rmse: 0.77018 | val_1_rmse: 0.74159 |  0:00:13s
epoch 21 | loss: 0.26984 | val_0_rmse: 0.77189 | val_1_rmse: 0.74094 |  0:00:13s
epoch 22 | loss: 0.26817 | val_0_rmse: 0.75128 | val_1_rmse: 0.72461 |  0:00:14s
epoch 23 | loss: 0.26378 | val_0_rmse: 0.73984 | val_1_rmse: 0.71036 |  0:00:15s
epoch 24 | loss: 0.2499  | val_0_rmse: 0.74867 | val_1_rmse: 0.71431 |  0:00:15s
epoch 25 | loss: 0.24385 | val_0_rmse: 0.73365 | val_1_rmse: 0.69994 |  0:00:16s
epoch 26 | loss: 0.24946 | val_0_rmse: 0.74034 | val_1_rmse: 0.71568 |  0:00:17s
epoch 27 | loss: 0.24996 | val_0_rmse: 0.72171 | val_1_rmse: 0.69574 |  0:00:17s
epoch 28 | loss: 0.24503 | val_0_rmse: 0.73945 | val_1_rmse: 0.70463 |  0:00:18s
epoch 29 | loss: 0.2474  | val_0_rmse: 0.7036  | val_1_rmse: 0.67625 |  0:00:18s
epoch 30 | loss: 0.23175 | val_0_rmse: 0.71662 | val_1_rmse: 0.68788 |  0:00:19s
epoch 31 | loss: 0.23267 | val_0_rmse: 0.71068 | val_1_rmse: 0.67816 |  0:00:20s
epoch 32 | loss: 0.2257  | val_0_rmse: 0.70286 | val_1_rmse: 0.67684 |  0:00:20s
epoch 33 | loss: 0.21739 | val_0_rmse: 0.69453 | val_1_rmse: 0.66637 |  0:00:21s
epoch 34 | loss: 0.21379 | val_0_rmse: 0.69124 | val_1_rmse: 0.66493 |  0:00:22s
epoch 35 | loss: 0.20908 | val_0_rmse: 0.6998  | val_1_rmse: 0.67575 |  0:00:22s
epoch 36 | loss: 0.21673 | val_0_rmse: 0.67281 | val_1_rmse: 0.64862 |  0:00:23s
epoch 37 | loss: 0.22067 | val_0_rmse: 0.68199 | val_1_rmse: 0.66012 |  0:00:23s
epoch 38 | loss: 0.21292 | val_0_rmse: 0.6686  | val_1_rmse: 0.643   |  0:00:24s
epoch 39 | loss: 0.22214 | val_0_rmse: 0.6681  | val_1_rmse: 0.64928 |  0:00:25s
epoch 40 | loss: 0.20941 | val_0_rmse: 0.64456 | val_1_rmse: 0.62545 |  0:00:25s
epoch 41 | loss: 0.20818 | val_0_rmse: 0.65176 | val_1_rmse: 0.62904 |  0:00:26s
epoch 42 | loss: 0.20979 | val_0_rmse: 0.6357  | val_1_rmse: 0.61221 |  0:00:27s
epoch 43 | loss: 0.20454 | val_0_rmse: 0.62883 | val_1_rmse: 0.60554 |  0:00:27s
epoch 44 | loss: 0.20675 | val_0_rmse: 0.62276 | val_1_rmse: 0.60007 |  0:00:28s
epoch 45 | loss: 0.20278 | val_0_rmse: 0.63872 | val_1_rmse: 0.6202  |  0:00:28s
epoch 46 | loss: 0.20129 | val_0_rmse: 0.63319 | val_1_rmse: 0.60736 |  0:00:29s
epoch 47 | loss: 0.19696 | val_0_rmse: 0.61831 | val_1_rmse: 0.60451 |  0:00:30s
epoch 48 | loss: 0.19889 | val_0_rmse: 0.6003  | val_1_rmse: 0.58062 |  0:00:30s
epoch 49 | loss: 0.19243 | val_0_rmse: 0.60289 | val_1_rmse: 0.58376 |  0:00:31s
epoch 50 | loss: 0.19618 | val_0_rmse: 0.59292 | val_1_rmse: 0.57506 |  0:00:32s
epoch 51 | loss: 0.19214 | val_0_rmse: 0.58343 | val_1_rmse: 0.56573 |  0:00:32s
epoch 52 | loss: 0.19231 | val_0_rmse: 0.58393 | val_1_rmse: 0.56565 |  0:00:33s
epoch 53 | loss: 0.18631 | val_0_rmse: 0.57388 | val_1_rmse: 0.55967 |  0:00:34s
epoch 54 | loss: 0.18803 | val_0_rmse: 0.58592 | val_1_rmse: 0.56613 |  0:00:34s
epoch 55 | loss: 0.18921 | val_0_rmse: 0.56362 | val_1_rmse: 0.55792 |  0:00:35s
epoch 56 | loss: 0.18864 | val_0_rmse: 0.56061 | val_1_rmse: 0.55255 |  0:00:35s
epoch 57 | loss: 0.1855  | val_0_rmse: 0.55574 | val_1_rmse: 0.54831 |  0:00:36s
epoch 58 | loss: 0.1824  | val_0_rmse: 0.53939 | val_1_rmse: 0.53666 |  0:00:37s
epoch 59 | loss: 0.18246 | val_0_rmse: 0.54552 | val_1_rmse: 0.54254 |  0:00:37s
epoch 60 | loss: 0.17972 | val_0_rmse: 0.5513  | val_1_rmse: 0.55001 |  0:00:38s
epoch 61 | loss: 0.18371 | val_0_rmse: 0.5158  | val_1_rmse: 0.52649 |  0:00:39s
epoch 62 | loss: 0.17933 | val_0_rmse: 0.54251 | val_1_rmse: 0.54062 |  0:00:39s
epoch 63 | loss: 0.18147 | val_0_rmse: 0.5125  | val_1_rmse: 0.52256 |  0:00:40s
epoch 64 | loss: 0.17661 | val_0_rmse: 0.5038  | val_1_rmse: 0.52197 |  0:00:40s
epoch 65 | loss: 0.17666 | val_0_rmse: 0.49558 | val_1_rmse: 0.51577 |  0:00:41s
epoch 66 | loss: 0.17328 | val_0_rmse: 0.49559 | val_1_rmse: 0.51508 |  0:00:42s
epoch 67 | loss: 0.16752 | val_0_rmse: 0.49521 | val_1_rmse: 0.51427 |  0:00:42s
epoch 68 | loss: 0.16972 | val_0_rmse: 0.48598 | val_1_rmse: 0.51501 |  0:00:43s
epoch 69 | loss: 0.1701  | val_0_rmse: 0.49725 | val_1_rmse: 0.52259 |  0:00:44s
epoch 70 | loss: 0.16573 | val_0_rmse: 0.47862 | val_1_rmse: 0.51071 |  0:00:44s
epoch 71 | loss: 0.16722 | val_0_rmse: 0.46841 | val_1_rmse: 0.50965 |  0:00:45s
epoch 72 | loss: 0.16273 | val_0_rmse: 0.45514 | val_1_rmse: 0.49943 |  0:00:45s
epoch 73 | loss: 0.16812 | val_0_rmse: 0.46185 | val_1_rmse: 0.50358 |  0:00:46s
epoch 74 | loss: 0.15988 | val_0_rmse: 0.42125 | val_1_rmse: 0.48726 |  0:00:47s
epoch 75 | loss: 0.16446 | val_0_rmse: 0.45262 | val_1_rmse: 0.503   |  0:00:47s
epoch 76 | loss: 0.16351 | val_0_rmse: 0.44315 | val_1_rmse: 0.50013 |  0:00:48s
epoch 77 | loss: 0.16131 | val_0_rmse: 0.43308 | val_1_rmse: 0.49822 |  0:00:49s
epoch 78 | loss: 0.15526 | val_0_rmse: 0.43196 | val_1_rmse: 0.50316 |  0:00:49s
epoch 79 | loss: 0.15941 | val_0_rmse: 0.41425 | val_1_rmse: 0.49245 |  0:00:50s
epoch 80 | loss: 0.1568  | val_0_rmse: 0.41046 | val_1_rmse: 0.49234 |  0:00:50s
epoch 81 | loss: 0.15769 | val_0_rmse: 0.40275 | val_1_rmse: 0.48838 |  0:00:51s
epoch 82 | loss: 0.15312 | val_0_rmse: 0.40949 | val_1_rmse: 0.49586 |  0:00:52s
epoch 83 | loss: 0.15237 | val_0_rmse: 0.40259 | val_1_rmse: 0.49132 |  0:00:52s
epoch 84 | loss: 0.15608 | val_0_rmse: 0.3833  | val_1_rmse: 0.48749 |  0:00:53s
epoch 85 | loss: 0.15619 | val_0_rmse: 0.38742 | val_1_rmse: 0.48482 |  0:00:54s
epoch 86 | loss: 0.15322 | val_0_rmse: 0.37291 | val_1_rmse: 0.48993 |  0:00:54s
epoch 87 | loss: 0.15203 | val_0_rmse: 0.40681 | val_1_rmse: 0.4985  |  0:00:55s
epoch 88 | loss: 0.14654 | val_0_rmse: 0.39139 | val_1_rmse: 0.49678 |  0:00:56s
epoch 89 | loss: 0.14453 | val_0_rmse: 0.36552 | val_1_rmse: 0.48104 |  0:00:56s
epoch 90 | loss: 0.1378  | val_0_rmse: 0.36224 | val_1_rmse: 0.4839  |  0:00:57s
epoch 91 | loss: 0.13981 | val_0_rmse: 0.36826 | val_1_rmse: 0.49524 |  0:00:57s
epoch 92 | loss: 0.14277 | val_0_rmse: 0.35682 | val_1_rmse: 0.49595 |  0:00:58s
epoch 93 | loss: 0.13862 | val_0_rmse: 0.35353 | val_1_rmse: 0.49223 |  0:00:59s
epoch 94 | loss: 0.14016 | val_0_rmse: 0.35708 | val_1_rmse: 0.49639 |  0:00:59s
epoch 95 | loss: 0.14107 | val_0_rmse: 0.35369 | val_1_rmse: 0.49453 |  0:01:00s
epoch 96 | loss: 0.13754 | val_0_rmse: 0.34097 | val_1_rmse: 0.49645 |  0:01:00s
epoch 97 | loss: 0.13782 | val_0_rmse: 0.35062 | val_1_rmse: 0.49575 |  0:01:01s
epoch 98 | loss: 0.13841 | val_0_rmse: 0.33883 | val_1_rmse: 0.50022 |  0:01:02s
epoch 99 | loss: 0.13261 | val_0_rmse: 0.33802 | val_1_rmse: 0.49134 |  0:01:02s
epoch 100| loss: 0.13634 | val_0_rmse: 0.34126 | val_1_rmse: 0.49657 |  0:01:03s
epoch 101| loss: 0.14368 | val_0_rmse: 0.34177 | val_1_rmse: 0.50678 |  0:01:04s
epoch 102| loss: 0.14189 | val_0_rmse: 0.36874 | val_1_rmse: 0.52049 |  0:01:04s
epoch 103| loss: 0.13633 | val_0_rmse: 0.3319  | val_1_rmse: 0.50149 |  0:01:05s
epoch 104| loss: 0.13165 | val_0_rmse: 0.32724 | val_1_rmse: 0.50825 |  0:01:06s
epoch 105| loss: 0.13598 | val_0_rmse: 0.33164 | val_1_rmse: 0.50245 |  0:01:06s
epoch 106| loss: 0.13885 | val_0_rmse: 0.33047 | val_1_rmse: 0.4904  |  0:01:07s
epoch 107| loss: 0.12956 | val_0_rmse: 0.32103 | val_1_rmse: 0.52023 |  0:01:07s
epoch 108| loss: 0.12433 | val_0_rmse: 0.32201 | val_1_rmse: 0.49236 |  0:01:08s
epoch 109| loss: 0.12598 | val_0_rmse: 0.31629 | val_1_rmse: 0.5214  |  0:01:09s
epoch 110| loss: 0.12647 | val_0_rmse: 0.31536 | val_1_rmse: 0.49945 |  0:01:09s
epoch 111| loss: 0.12381 | val_0_rmse: 0.30924 | val_1_rmse: 0.51323 |  0:01:10s
epoch 112| loss: 0.12489 | val_0_rmse: 0.30774 | val_1_rmse: 0.514   |  0:01:11s
epoch 113| loss: 0.12238 | val_0_rmse: 0.31191 | val_1_rmse: 0.52042 |  0:01:11s
epoch 114| loss: 0.12974 | val_0_rmse: 0.30614 | val_1_rmse: 0.51063 |  0:01:12s
epoch 115| loss: 0.12437 | val_0_rmse: 0.31104 | val_1_rmse: 0.52143 |  0:01:12s
epoch 116| loss: 0.11948 | val_0_rmse: 0.29894 | val_1_rmse: 0.52281 |  0:01:13s
epoch 117| loss: 0.13274 | val_0_rmse: 0.33533 | val_1_rmse: 0.54107 |  0:01:14s
epoch 118| loss: 0.13782 | val_0_rmse: 0.34639 | val_1_rmse: 0.51468 |  0:01:14s
epoch 119| loss: 0.1327  | val_0_rmse: 0.3434  | val_1_rmse: 0.5502  |  0:01:15s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.48104
Best weights from best epoch are automatically used!
ended training at: 16:34:05
Feature importance:
Mean squared error is of 20836522240.55298
Mean absolute error:103799.81309831757
MAPE:0.17953080671058835
R2 score:0.7237617031325441
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:34:05
epoch 0  | loss: 3.7634  | val_0_rmse: 1.06156 | val_1_rmse: 1.06297 |  0:00:00s
epoch 1  | loss: 2.86306 | val_0_rmse: 1.01467 | val_1_rmse: 1.02677 |  0:00:00s
epoch 2  | loss: 2.11077 | val_0_rmse: 0.99259 | val_1_rmse: 1.01191 |  0:00:00s
epoch 3  | loss: 1.37504 | val_0_rmse: 0.99565 | val_1_rmse: 1.01952 |  0:00:00s
epoch 4  | loss: 1.15847 | val_0_rmse: 1.00163 | val_1_rmse: 1.02187 |  0:00:00s
epoch 5  | loss: 1.2189  | val_0_rmse: 1.00429 | val_1_rmse: 1.02369 |  0:00:00s
epoch 6  | loss: 1.12209 | val_0_rmse: 1.00016 | val_1_rmse: 1.02126 |  0:00:01s
epoch 7  | loss: 1.0415  | val_0_rmse: 0.99678 | val_1_rmse: 1.01929 |  0:00:01s
epoch 8  | loss: 1.0257  | val_0_rmse: 0.99637 | val_1_rmse: 1.01899 |  0:00:01s
epoch 9  | loss: 1.00558 | val_0_rmse: 0.99598 | val_1_rmse: 1.01881 |  0:00:01s
epoch 10 | loss: 0.99594 | val_0_rmse: 0.99441 | val_1_rmse: 1.01722 |  0:00:01s
epoch 11 | loss: 0.95413 | val_0_rmse: 0.91058 | val_1_rmse: 0.91276 |  0:00:01s
epoch 12 | loss: 0.9245  | val_0_rmse: 0.87912 | val_1_rmse: 0.86328 |  0:00:02s
epoch 13 | loss: 0.84624 | val_0_rmse: 0.89313 | val_1_rmse: 0.8497  |  0:00:02s
epoch 14 | loss: 0.85564 | val_0_rmse: 0.90178 | val_1_rmse: 0.93712 |  0:00:02s
epoch 15 | loss: 0.81193 | val_0_rmse: 1.00891 | val_1_rmse: 1.03924 |  0:00:02s
epoch 16 | loss: 0.75657 | val_0_rmse: 0.92398 | val_1_rmse: 0.9637  |  0:00:02s
epoch 17 | loss: 0.7202  | val_0_rmse: 0.8535  | val_1_rmse: 0.84661 |  0:00:02s
epoch 18 | loss: 0.67051 | val_0_rmse: 0.86056 | val_1_rmse: 0.80958 |  0:00:03s
epoch 19 | loss: 0.6713  | val_0_rmse: 0.86391 | val_1_rmse: 0.81685 |  0:00:03s
epoch 20 | loss: 0.62519 | val_0_rmse: 0.92041 | val_1_rmse: 0.89748 |  0:00:03s
epoch 21 | loss: 0.56131 | val_0_rmse: 0.86769 | val_1_rmse: 0.84014 |  0:00:03s
epoch 22 | loss: 0.57872 | val_0_rmse: 0.79186 | val_1_rmse: 0.76916 |  0:00:03s
epoch 23 | loss: 0.51381 | val_0_rmse: 0.83178 | val_1_rmse: 0.80349 |  0:00:03s
epoch 24 | loss: 0.51826 | val_0_rmse: 0.81441 | val_1_rmse: 0.7732  |  0:00:04s
epoch 25 | loss: 0.50949 | val_0_rmse: 0.80412 | val_1_rmse: 0.75512 |  0:00:04s
epoch 26 | loss: 0.49615 | val_0_rmse: 0.8927  | val_1_rmse: 0.85056 |  0:00:04s
epoch 27 | loss: 0.50203 | val_0_rmse: 0.90566 | val_1_rmse: 0.8644  |  0:00:04s
epoch 28 | loss: 0.48515 | val_0_rmse: 0.80323 | val_1_rmse: 0.76342 |  0:00:04s
epoch 29 | loss: 0.48126 | val_0_rmse: 0.79939 | val_1_rmse: 0.75626 |  0:00:04s
epoch 30 | loss: 0.45165 | val_0_rmse: 0.90689 | val_1_rmse: 0.86879 |  0:00:05s
epoch 31 | loss: 0.44297 | val_0_rmse: 0.92155 | val_1_rmse: 0.88725 |  0:00:05s
epoch 32 | loss: 0.40846 | val_0_rmse: 0.86917 | val_1_rmse: 0.83277 |  0:00:05s
epoch 33 | loss: 0.41314 | val_0_rmse: 0.88883 | val_1_rmse: 0.85585 |  0:00:05s
epoch 34 | loss: 0.4176  | val_0_rmse: 0.85843 | val_1_rmse: 0.82138 |  0:00:05s
epoch 35 | loss: 0.40738 | val_0_rmse: 0.80616 | val_1_rmse: 0.76608 |  0:00:05s
epoch 36 | loss: 0.38153 | val_0_rmse: 0.73632 | val_1_rmse: 0.6931  |  0:00:05s
epoch 37 | loss: 0.38622 | val_0_rmse: 0.74163 | val_1_rmse: 0.69768 |  0:00:06s
epoch 38 | loss: 0.35578 | val_0_rmse: 0.79554 | val_1_rmse: 0.7544  |  0:00:06s
epoch 39 | loss: 0.35305 | val_0_rmse: 0.7784  | val_1_rmse: 0.73429 |  0:00:06s
epoch 40 | loss: 0.34859 | val_0_rmse: 0.717   | val_1_rmse: 0.66741 |  0:00:06s
epoch 41 | loss: 0.33194 | val_0_rmse: 0.73887 | val_1_rmse: 0.68962 |  0:00:06s
epoch 42 | loss: 0.33554 | val_0_rmse: 0.80507 | val_1_rmse: 0.76434 |  0:00:06s
epoch 43 | loss: 0.32563 | val_0_rmse: 0.76496 | val_1_rmse: 0.72285 |  0:00:07s
epoch 44 | loss: 0.32161 | val_0_rmse: 0.7117  | val_1_rmse: 0.66747 |  0:00:07s
epoch 45 | loss: 0.3093  | val_0_rmse: 0.71228 | val_1_rmse: 0.66808 |  0:00:07s
epoch 46 | loss: 0.30067 | val_0_rmse: 0.71783 | val_1_rmse: 0.67595 |  0:00:07s
epoch 47 | loss: 0.29644 | val_0_rmse: 0.6918  | val_1_rmse: 0.65068 |  0:00:07s
epoch 48 | loss: 0.30216 | val_0_rmse: 0.6834  | val_1_rmse: 0.64245 |  0:00:07s
epoch 49 | loss: 0.29099 | val_0_rmse: 0.70921 | val_1_rmse: 0.67233 |  0:00:08s
epoch 50 | loss: 0.31462 | val_0_rmse: 0.71437 | val_1_rmse: 0.68257 |  0:00:08s
epoch 51 | loss: 0.29592 | val_0_rmse: 0.68212 | val_1_rmse: 0.65411 |  0:00:08s
epoch 52 | loss: 0.29497 | val_0_rmse: 0.68095 | val_1_rmse: 0.65458 |  0:00:08s
epoch 53 | loss: 0.30743 | val_0_rmse: 0.69567 | val_1_rmse: 0.67066 |  0:00:08s
epoch 54 | loss: 0.30208 | val_0_rmse: 0.68021 | val_1_rmse: 0.65167 |  0:00:08s
epoch 55 | loss: 0.27926 | val_0_rmse: 0.67258 | val_1_rmse: 0.63995 |  0:00:09s
epoch 56 | loss: 0.27426 | val_0_rmse: 0.68747 | val_1_rmse: 0.64506 |  0:00:09s
epoch 57 | loss: 0.27383 | val_0_rmse: 0.70818 | val_1_rmse: 0.66074 |  0:00:09s
epoch 58 | loss: 0.26993 | val_0_rmse: 0.7019  | val_1_rmse: 0.65158 |  0:00:09s
epoch 59 | loss: 0.25088 | val_0_rmse: 0.71574 | val_1_rmse: 0.67083 |  0:00:09s
epoch 60 | loss: 0.25971 | val_0_rmse: 0.71943 | val_1_rmse: 0.6761  |  0:00:09s
epoch 61 | loss: 0.26314 | val_0_rmse: 0.73395 | val_1_rmse: 0.69543 |  0:00:09s
epoch 62 | loss: 0.25731 | val_0_rmse: 0.76371 | val_1_rmse: 0.73115 |  0:00:10s
epoch 63 | loss: 0.25385 | val_0_rmse: 0.71759 | val_1_rmse: 0.67858 |  0:00:10s
epoch 64 | loss: 0.24982 | val_0_rmse: 0.67035 | val_1_rmse: 0.62146 |  0:00:10s
epoch 65 | loss: 0.24555 | val_0_rmse: 0.69296 | val_1_rmse: 0.64454 |  0:00:10s
epoch 66 | loss: 0.24345 | val_0_rmse: 0.7835  | val_1_rmse: 0.74723 |  0:00:10s
epoch 67 | loss: 0.25466 | val_0_rmse: 0.72788 | val_1_rmse: 0.68236 |  0:00:11s
epoch 68 | loss: 0.23474 | val_0_rmse: 0.65995 | val_1_rmse: 0.60839 |  0:00:11s
epoch 69 | loss: 0.24577 | val_0_rmse: 0.65909 | val_1_rmse: 0.61716 |  0:00:11s
epoch 70 | loss: 0.25903 | val_0_rmse: 0.6832  | val_1_rmse: 0.65973 |  0:00:11s
epoch 71 | loss: 0.24101 | val_0_rmse: 0.6565  | val_1_rmse: 0.63685 |  0:00:11s
epoch 72 | loss: 0.23024 | val_0_rmse: 0.6395  | val_1_rmse: 0.61235 |  0:00:11s
epoch 73 | loss: 0.23572 | val_0_rmse: 0.64769 | val_1_rmse: 0.62044 |  0:00:11s
epoch 74 | loss: 0.22526 | val_0_rmse: 0.66311 | val_1_rmse: 0.63359 |  0:00:12s
epoch 75 | loss: 0.24576 | val_0_rmse: 0.67485 | val_1_rmse: 0.63909 |  0:00:12s
epoch 76 | loss: 0.2193  | val_0_rmse: 0.68055 | val_1_rmse: 0.63862 |  0:00:12s
epoch 77 | loss: 0.22193 | val_0_rmse: 0.66743 | val_1_rmse: 0.61982 |  0:00:12s
epoch 78 | loss: 0.21842 | val_0_rmse: 0.65615 | val_1_rmse: 0.60813 |  0:00:12s
epoch 79 | loss: 0.21455 | val_0_rmse: 0.68859 | val_1_rmse: 0.63537 |  0:00:12s
epoch 80 | loss: 0.21113 | val_0_rmse: 0.71588 | val_1_rmse: 0.672   |  0:00:13s
epoch 81 | loss: 0.21937 | val_0_rmse: 0.67404 | val_1_rmse: 0.6256  |  0:00:13s
epoch 82 | loss: 0.22231 | val_0_rmse: 0.63939 | val_1_rmse: 0.5897  |  0:00:13s
epoch 83 | loss: 0.21776 | val_0_rmse: 0.64707 | val_1_rmse: 0.60105 |  0:00:13s
epoch 84 | loss: 0.2097  | val_0_rmse: 0.66583 | val_1_rmse: 0.6191  |  0:00:13s
epoch 85 | loss: 0.21685 | val_0_rmse: 0.6543  | val_1_rmse: 0.60577 |  0:00:13s
epoch 86 | loss: 0.21536 | val_0_rmse: 0.63855 | val_1_rmse: 0.5904  |  0:00:14s
epoch 87 | loss: 0.21907 | val_0_rmse: 0.67653 | val_1_rmse: 0.63369 |  0:00:14s
epoch 88 | loss: 0.21175 | val_0_rmse: 0.69721 | val_1_rmse: 0.66048 |  0:00:14s
epoch 89 | loss: 0.19707 | val_0_rmse: 0.68513 | val_1_rmse: 0.64366 |  0:00:14s
epoch 90 | loss: 0.20789 | val_0_rmse: 0.70105 | val_1_rmse: 0.65715 |  0:00:14s
epoch 91 | loss: 0.19382 | val_0_rmse: 0.70661 | val_1_rmse: 0.66399 |  0:00:14s
epoch 92 | loss: 0.19864 | val_0_rmse: 0.6649  | val_1_rmse: 0.61485 |  0:00:14s
epoch 93 | loss: 0.19817 | val_0_rmse: 0.64716 | val_1_rmse: 0.5961  |  0:00:15s
epoch 94 | loss: 0.1973  | val_0_rmse: 0.66214 | val_1_rmse: 0.6156  |  0:00:15s
epoch 95 | loss: 0.19086 | val_0_rmse: 0.68643 | val_1_rmse: 0.64215 |  0:00:15s
epoch 96 | loss: 0.1979  | val_0_rmse: 0.66911 | val_1_rmse: 0.62121 |  0:00:15s
epoch 97 | loss: 0.19749 | val_0_rmse: 0.65401 | val_1_rmse: 0.60633 |  0:00:15s
epoch 98 | loss: 0.19419 | val_0_rmse: 0.68806 | val_1_rmse: 0.64825 |  0:00:15s
epoch 99 | loss: 0.19058 | val_0_rmse: 0.71685 | val_1_rmse: 0.68668 |  0:00:16s
epoch 100| loss: 0.19713 | val_0_rmse: 0.65994 | val_1_rmse: 0.63129 |  0:00:16s
epoch 101| loss: 0.18794 | val_0_rmse: 0.62829 | val_1_rmse: 0.60514 |  0:00:16s
epoch 102| loss: 0.20304 | val_0_rmse: 0.63696 | val_1_rmse: 0.61557 |  0:00:16s
epoch 103| loss: 0.19374 | val_0_rmse: 0.64067 | val_1_rmse: 0.61552 |  0:00:16s
epoch 104| loss: 0.19198 | val_0_rmse: 0.64344 | val_1_rmse: 0.61371 |  0:00:16s
epoch 105| loss: 0.19382 | val_0_rmse: 0.642   | val_1_rmse: 0.62602 |  0:00:17s
epoch 106| loss: 0.17874 | val_0_rmse: 0.64864 | val_1_rmse: 0.64349 |  0:00:17s
epoch 107| loss: 0.18627 | val_0_rmse: 0.61738 | val_1_rmse: 0.6073  |  0:00:17s
epoch 108| loss: 0.18997 | val_0_rmse: 0.61929 | val_1_rmse: 0.60022 |  0:00:17s
epoch 109| loss: 0.19532 | val_0_rmse: 0.64222 | val_1_rmse: 0.62118 |  0:00:17s
epoch 110| loss: 0.18241 | val_0_rmse: 0.68124 | val_1_rmse: 0.67068 |  0:00:17s
epoch 111| loss: 0.18381 | val_0_rmse: 0.67283 | val_1_rmse: 0.66287 |  0:00:17s
epoch 112| loss: 0.18368 | val_0_rmse: 0.66569 | val_1_rmse: 0.6616  |  0:00:18s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.5897
Best weights from best epoch are automatically used!
ended training at: 16:34:23
Feature importance:
Mean squared error is of 3453068992.06629
Mean absolute error:42600.88498487102
MAPE:0.4368665591846037
R2 score:0.5089492488902233
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:34:24
epoch 0  | loss: 3.66206 | val_0_rmse: 1.00691 | val_1_rmse: 1.04068 |  0:00:00s
epoch 1  | loss: 2.37557 | val_0_rmse: 0.99814 | val_1_rmse: 1.04172 |  0:00:00s
epoch 2  | loss: 1.52046 | val_0_rmse: 0.9973  | val_1_rmse: 1.03966 |  0:00:00s
epoch 3  | loss: 1.52715 | val_0_rmse: 0.99637 | val_1_rmse: 1.03633 |  0:00:00s
epoch 4  | loss: 1.18865 | val_0_rmse: 0.99642 | val_1_rmse: 1.03536 |  0:00:00s
epoch 5  | loss: 1.09834 | val_0_rmse: 0.99667 | val_1_rmse: 1.03547 |  0:00:00s
epoch 6  | loss: 1.10834 | val_0_rmse: 0.99587 | val_1_rmse: 1.03451 |  0:00:01s
epoch 7  | loss: 1.05945 | val_0_rmse: 0.99486 | val_1_rmse: 1.03283 |  0:00:01s
epoch 8  | loss: 1.01984 | val_0_rmse: 0.9946  | val_1_rmse: 1.0314  |  0:00:01s
epoch 9  | loss: 0.98742 | val_0_rmse: 0.99482 | val_1_rmse: 1.03105 |  0:00:01s
epoch 10 | loss: 1.00489 | val_0_rmse: 0.99373 | val_1_rmse: 1.03146 |  0:00:01s
epoch 11 | loss: 0.9798  | val_0_rmse: 0.99379 | val_1_rmse: 1.03179 |  0:00:01s
epoch 12 | loss: 0.9723  | val_0_rmse: 0.99271 | val_1_rmse: 1.02895 |  0:00:02s
epoch 13 | loss: 0.95167 | val_0_rmse: 0.98885 | val_1_rmse: 1.02215 |  0:00:02s
epoch 14 | loss: 0.91336 | val_0_rmse: 0.99284 | val_1_rmse: 1.0172  |  0:00:02s
epoch 15 | loss: 0.89361 | val_0_rmse: 1.01806 | val_1_rmse: 1.03463 |  0:00:02s
epoch 16 | loss: 0.81657 | val_0_rmse: 1.08563 | val_1_rmse: 1.10782 |  0:00:02s
epoch 17 | loss: 0.75915 | val_0_rmse: 1.13906 | val_1_rmse: 1.17782 |  0:00:02s
epoch 18 | loss: 0.67816 | val_0_rmse: 1.15537 | val_1_rmse: 1.20827 |  0:00:03s
epoch 19 | loss: 0.62326 | val_0_rmse: 1.26104 | val_1_rmse: 1.32755 |  0:00:03s
epoch 20 | loss: 0.51823 | val_0_rmse: 1.62704 | val_1_rmse: 1.70842 |  0:00:03s
epoch 21 | loss: 0.4885  | val_0_rmse: 1.49733 | val_1_rmse: 1.57571 |  0:00:03s
epoch 22 | loss: 0.4419  | val_0_rmse: 1.2399  | val_1_rmse: 1.29823 |  0:00:03s
epoch 23 | loss: 0.42051 | val_0_rmse: 1.21422 | val_1_rmse: 1.26207 |  0:00:03s
epoch 24 | loss: 0.39968 | val_0_rmse: 1.03813 | val_1_rmse: 1.09593 |  0:00:04s
epoch 25 | loss: 0.36733 | val_0_rmse: 0.88559 | val_1_rmse: 0.9433  |  0:00:04s
epoch 26 | loss: 0.35526 | val_0_rmse: 0.93671 | val_1_rmse: 0.99131 |  0:00:04s
epoch 27 | loss: 0.34982 | val_0_rmse: 0.90648 | val_1_rmse: 0.96032 |  0:00:04s
epoch 28 | loss: 0.32944 | val_0_rmse: 0.81374 | val_1_rmse: 0.86611 |  0:00:04s
epoch 29 | loss: 0.32443 | val_0_rmse: 0.75074 | val_1_rmse: 0.80363 |  0:00:04s
epoch 30 | loss: 0.31187 | val_0_rmse: 0.73388 | val_1_rmse: 0.78834 |  0:00:05s
epoch 31 | loss: 0.31084 | val_0_rmse: 0.68516 | val_1_rmse: 0.73939 |  0:00:05s
epoch 32 | loss: 0.29439 | val_0_rmse: 0.67757 | val_1_rmse: 0.7288  |  0:00:05s
epoch 33 | loss: 0.28489 | val_0_rmse: 0.65345 | val_1_rmse: 0.69786 |  0:00:05s
epoch 34 | loss: 0.27969 | val_0_rmse: 0.65057 | val_1_rmse: 0.69133 |  0:00:05s
epoch 35 | loss: 0.27569 | val_0_rmse: 0.64528 | val_1_rmse: 0.68779 |  0:00:05s
epoch 36 | loss: 0.26545 | val_0_rmse: 0.66147 | val_1_rmse: 0.71498 |  0:00:05s
epoch 37 | loss: 0.27167 | val_0_rmse: 0.66345 | val_1_rmse: 0.7176  |  0:00:06s
epoch 38 | loss: 0.25834 | val_0_rmse: 0.64932 | val_1_rmse: 0.69698 |  0:00:06s
epoch 39 | loss: 0.26275 | val_0_rmse: 0.64574 | val_1_rmse: 0.69528 |  0:00:06s
epoch 40 | loss: 0.25047 | val_0_rmse: 0.6464  | val_1_rmse: 0.7015  |  0:00:06s
epoch 41 | loss: 0.24481 | val_0_rmse: 0.65041 | val_1_rmse: 0.70954 |  0:00:06s
epoch 42 | loss: 0.23646 | val_0_rmse: 0.6565  | val_1_rmse: 0.71608 |  0:00:06s
epoch 43 | loss: 0.23234 | val_0_rmse: 0.65818 | val_1_rmse: 0.71514 |  0:00:07s
epoch 44 | loss: 0.24007 | val_0_rmse: 0.64264 | val_1_rmse: 0.69453 |  0:00:07s
epoch 45 | loss: 0.23163 | val_0_rmse: 0.63051 | val_1_rmse: 0.67689 |  0:00:07s
epoch 46 | loss: 0.23532 | val_0_rmse: 0.63366 | val_1_rmse: 0.67876 |  0:00:07s
epoch 47 | loss: 0.23128 | val_0_rmse: 0.62867 | val_1_rmse: 0.67995 |  0:00:07s
epoch 48 | loss: 0.24918 | val_0_rmse: 0.62651 | val_1_rmse: 0.68145 |  0:00:07s
epoch 49 | loss: 0.2243  | val_0_rmse: 0.6208  | val_1_rmse: 0.67676 |  0:00:08s
epoch 50 | loss: 0.21883 | val_0_rmse: 0.62088 | val_1_rmse: 0.67648 |  0:00:08s
epoch 51 | loss: 0.22359 | val_0_rmse: 0.62368 | val_1_rmse: 0.67969 |  0:00:08s
epoch 52 | loss: 0.23312 | val_0_rmse: 0.64038 | val_1_rmse: 0.69778 |  0:00:08s
epoch 53 | loss: 0.22972 | val_0_rmse: 0.66054 | val_1_rmse: 0.71922 |  0:00:08s
epoch 54 | loss: 0.21899 | val_0_rmse: 0.67949 | val_1_rmse: 0.74138 |  0:00:08s
epoch 55 | loss: 0.22175 | val_0_rmse: 0.65726 | val_1_rmse: 0.71707 |  0:00:09s
epoch 56 | loss: 0.21213 | val_0_rmse: 0.62194 | val_1_rmse: 0.67429 |  0:00:09s
epoch 57 | loss: 0.21008 | val_0_rmse: 0.61599 | val_1_rmse: 0.66635 |  0:00:09s
epoch 58 | loss: 0.20633 | val_0_rmse: 0.61579 | val_1_rmse: 0.66602 |  0:00:09s
epoch 59 | loss: 0.21379 | val_0_rmse: 0.61228 | val_1_rmse: 0.66696 |  0:00:09s
epoch 60 | loss: 0.19886 | val_0_rmse: 0.62315 | val_1_rmse: 0.68593 |  0:00:09s
epoch 61 | loss: 0.19368 | val_0_rmse: 0.64872 | val_1_rmse: 0.71816 |  0:00:10s
epoch 62 | loss: 0.19974 | val_0_rmse: 0.63584 | val_1_rmse: 0.70166 |  0:00:10s
epoch 63 | loss: 0.19703 | val_0_rmse: 0.61159 | val_1_rmse: 0.67216 |  0:00:10s
epoch 64 | loss: 0.18854 | val_0_rmse: 0.61201 | val_1_rmse: 0.6778  |  0:00:10s
epoch 65 | loss: 0.20629 | val_0_rmse: 0.61741 | val_1_rmse: 0.68882 |  0:00:10s
epoch 66 | loss: 0.19378 | val_0_rmse: 0.60787 | val_1_rmse: 0.6792  |  0:00:10s
epoch 67 | loss: 0.18633 | val_0_rmse: 0.60775 | val_1_rmse: 0.68029 |  0:00:10s
epoch 68 | loss: 0.17854 | val_0_rmse: 0.61562 | val_1_rmse: 0.69087 |  0:00:11s
epoch 69 | loss: 0.19999 | val_0_rmse: 0.61668 | val_1_rmse: 0.69135 |  0:00:11s
epoch 70 | loss: 0.18621 | val_0_rmse: 0.60318 | val_1_rmse: 0.67356 |  0:00:11s
epoch 71 | loss: 0.18579 | val_0_rmse: 0.60091 | val_1_rmse: 0.67694 |  0:00:11s
epoch 72 | loss: 0.18025 | val_0_rmse: 0.62255 | val_1_rmse: 0.71225 |  0:00:11s
epoch 73 | loss: 0.1821  | val_0_rmse: 0.61235 | val_1_rmse: 0.70286 |  0:00:11s
epoch 74 | loss: 0.18277 | val_0_rmse: 0.58877 | val_1_rmse: 0.6735  |  0:00:12s
epoch 75 | loss: 0.17202 | val_0_rmse: 0.59088 | val_1_rmse: 0.67871 |  0:00:12s
epoch 76 | loss: 0.17804 | val_0_rmse: 0.59849 | val_1_rmse: 0.68865 |  0:00:12s
epoch 77 | loss: 0.16799 | val_0_rmse: 0.59662 | val_1_rmse: 0.68342 |  0:00:12s
epoch 78 | loss: 0.17296 | val_0_rmse: 0.61278 | val_1_rmse: 0.69931 |  0:00:12s
epoch 79 | loss: 0.17412 | val_0_rmse: 0.64656 | val_1_rmse: 0.73424 |  0:00:12s
epoch 80 | loss: 0.18895 | val_0_rmse: 0.62207 | val_1_rmse: 0.70325 |  0:00:13s
epoch 81 | loss: 0.17177 | val_0_rmse: 0.60483 | val_1_rmse: 0.68302 |  0:00:13s
epoch 82 | loss: 0.18965 | val_0_rmse: 0.60475 | val_1_rmse: 0.68612 |  0:00:13s
epoch 83 | loss: 0.17096 | val_0_rmse: 0.63691 | val_1_rmse: 0.72725 |  0:00:13s
epoch 84 | loss: 0.17202 | val_0_rmse: 0.6313  | val_1_rmse: 0.72762 |  0:00:13s
epoch 85 | loss: 0.1641  | val_0_rmse: 0.58519 | val_1_rmse: 0.67422 |  0:00:13s
epoch 86 | loss: 0.16716 | val_0_rmse: 0.57682 | val_1_rmse: 0.65671 |  0:00:13s
epoch 87 | loss: 0.17532 | val_0_rmse: 0.57628 | val_1_rmse: 0.66175 |  0:00:14s
epoch 88 | loss: 0.16626 | val_0_rmse: 0.59485 | val_1_rmse: 0.68738 |  0:00:14s
epoch 89 | loss: 0.15909 | val_0_rmse: 0.61044 | val_1_rmse: 0.70946 |  0:00:14s
epoch 90 | loss: 0.15356 | val_0_rmse: 0.58795 | val_1_rmse: 0.68591 |  0:00:14s
epoch 91 | loss: 0.15838 | val_0_rmse: 0.58135 | val_1_rmse: 0.6688  |  0:00:14s
epoch 92 | loss: 0.16075 | val_0_rmse: 0.59705 | val_1_rmse: 0.68546 |  0:00:14s
epoch 93 | loss: 0.15607 | val_0_rmse: 0.6315  | val_1_rmse: 0.72596 |  0:00:15s
epoch 94 | loss: 0.1526  | val_0_rmse: 0.60652 | val_1_rmse: 0.7034  |  0:00:15s
epoch 95 | loss: 0.14876 | val_0_rmse: 0.57203 | val_1_rmse: 0.66667 |  0:00:15s
epoch 96 | loss: 0.15331 | val_0_rmse: 0.56171 | val_1_rmse: 0.65324 |  0:00:15s
epoch 97 | loss: 0.15558 | val_0_rmse: 0.57275 | val_1_rmse: 0.67068 |  0:00:15s
epoch 98 | loss: 0.15543 | val_0_rmse: 0.58962 | val_1_rmse: 0.69428 |  0:00:15s
epoch 99 | loss: 0.16312 | val_0_rmse: 0.57268 | val_1_rmse: 0.67637 |  0:00:16s
epoch 100| loss: 0.161   | val_0_rmse: 0.56431 | val_1_rmse: 0.65874 |  0:00:16s
epoch 101| loss: 0.15423 | val_0_rmse: 0.56531 | val_1_rmse: 0.65766 |  0:00:16s
epoch 102| loss: 0.15011 | val_0_rmse: 0.56561 | val_1_rmse: 0.66648 |  0:00:16s
epoch 103| loss: 0.14615 | val_0_rmse: 0.57003 | val_1_rmse: 0.67478 |  0:00:16s
epoch 104| loss: 0.15343 | val_0_rmse: 0.55802 | val_1_rmse: 0.65394 |  0:00:16s
epoch 105| loss: 0.15446 | val_0_rmse: 0.57063 | val_1_rmse: 0.65304 |  0:00:16s
epoch 106| loss: 0.15721 | val_0_rmse: 0.55379 | val_1_rmse: 0.64483 |  0:00:17s
epoch 107| loss: 0.15009 | val_0_rmse: 0.57738 | val_1_rmse: 0.68755 |  0:00:17s
epoch 108| loss: 0.13783 | val_0_rmse: 0.59427 | val_1_rmse: 0.70709 |  0:00:17s
epoch 109| loss: 0.13348 | val_0_rmse: 0.56651 | val_1_rmse: 0.6757  |  0:00:17s
epoch 110| loss: 0.1417  | val_0_rmse: 0.56344 | val_1_rmse: 0.66684 |  0:00:17s
epoch 111| loss: 0.1452  | val_0_rmse: 0.57927 | val_1_rmse: 0.69169 |  0:00:17s
epoch 112| loss: 0.13727 | val_0_rmse: 0.61641 | val_1_rmse: 0.73515 |  0:00:18s
epoch 113| loss: 0.15313 | val_0_rmse: 0.56864 | val_1_rmse: 0.68168 |  0:00:18s
epoch 114| loss: 0.13865 | val_0_rmse: 0.54934 | val_1_rmse: 0.66211 |  0:00:18s
epoch 115| loss: 0.13894 | val_0_rmse: 0.54403 | val_1_rmse: 0.66103 |  0:00:18s
epoch 116| loss: 0.1353  | val_0_rmse: 0.54693 | val_1_rmse: 0.67123 |  0:00:18s
epoch 117| loss: 0.13122 | val_0_rmse: 0.55702 | val_1_rmse: 0.68611 |  0:00:18s
epoch 118| loss: 0.12987 | val_0_rmse: 0.57102 | val_1_rmse: 0.69962 |  0:00:19s
epoch 119| loss: 0.12415 | val_0_rmse: 0.55097 | val_1_rmse: 0.67656 |  0:00:19s
epoch 120| loss: 0.13451 | val_0_rmse: 0.5414  | val_1_rmse: 0.66639 |  0:00:19s
epoch 121| loss: 0.12661 | val_0_rmse: 0.55174 | val_1_rmse: 0.68082 |  0:00:19s
epoch 122| loss: 0.12416 | val_0_rmse: 0.56159 | val_1_rmse: 0.69685 |  0:00:19s
epoch 123| loss: 0.13776 | val_0_rmse: 0.56478 | val_1_rmse: 0.70774 |  0:00:19s
epoch 124| loss: 0.11837 | val_0_rmse: 0.55962 | val_1_rmse: 0.69974 |  0:00:20s
epoch 125| loss: 0.13237 | val_0_rmse: 0.58372 | val_1_rmse: 0.71685 |  0:00:20s
epoch 126| loss: 0.13658 | val_0_rmse: 0.59459 | val_1_rmse: 0.72109 |  0:00:20s
epoch 127| loss: 0.12188 | val_0_rmse: 0.54261 | val_1_rmse: 0.67107 |  0:00:20s
epoch 128| loss: 0.11581 | val_0_rmse: 0.52811 | val_1_rmse: 0.65587 |  0:00:20s
epoch 129| loss: 0.1146  | val_0_rmse: 0.52353 | val_1_rmse: 0.64707 |  0:00:20s
epoch 130| loss: 0.13279 | val_0_rmse: 0.53402 | val_1_rmse: 0.6597  |  0:00:21s
epoch 131| loss: 0.12643 | val_0_rmse: 0.54115 | val_1_rmse: 0.67275 |  0:00:21s
epoch 132| loss: 0.11717 | val_0_rmse: 0.53294 | val_1_rmse: 0.67347 |  0:00:21s
epoch 133| loss: 0.11778 | val_0_rmse: 0.56519 | val_1_rmse: 0.7074  |  0:00:21s
epoch 134| loss: 0.1265  | val_0_rmse: 0.56866 | val_1_rmse: 0.71376 |  0:00:21s
epoch 135| loss: 0.13427 | val_0_rmse: 0.53591 | val_1_rmse: 0.69056 |  0:00:21s
epoch 136| loss: 0.12344 | val_0_rmse: 0.53185 | val_1_rmse: 0.69165 |  0:00:21s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.64483
Best weights from best epoch are automatically used!
ended training at: 16:34:46
Feature importance:
Mean squared error is of 2540954484.83231
Mean absolute error:37237.6317864229
MAPE:0.35813995401968174
R2 score:0.6330240962014779
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:34:46
epoch 0  | loss: 4.72688 | val_0_rmse: 1.10615 | val_1_rmse: 1.0705  |  0:00:00s
epoch 1  | loss: 2.82314 | val_0_rmse: 1.07644 | val_1_rmse: 1.04708 |  0:00:00s
epoch 2  | loss: 2.01397 | val_0_rmse: 0.99712 | val_1_rmse: 0.99014 |  0:00:00s
epoch 3  | loss: 1.51629 | val_0_rmse: 0.99629 | val_1_rmse: 0.99789 |  0:00:00s
epoch 4  | loss: 1.27194 | val_0_rmse: 1.00076 | val_1_rmse: 1.00528 |  0:00:00s
epoch 5  | loss: 1.32389 | val_0_rmse: 1.00525 | val_1_rmse: 1.01001 |  0:00:00s
epoch 6  | loss: 1.08856 | val_0_rmse: 1.00379 | val_1_rmse: 1.00546 |  0:00:01s
epoch 7  | loss: 1.01543 | val_0_rmse: 0.99126 | val_1_rmse: 0.98731 |  0:00:01s
epoch 8  | loss: 0.97983 | val_0_rmse: 0.9672  | val_1_rmse: 0.95384 |  0:00:01s
epoch 9  | loss: 0.93919 | val_0_rmse: 0.91948 | val_1_rmse: 0.89049 |  0:00:01s
epoch 10 | loss: 0.87128 | val_0_rmse: 0.81219 | val_1_rmse: 0.79229 |  0:00:01s
epoch 11 | loss: 0.76657 | val_0_rmse: 0.83123 | val_1_rmse: 0.83025 |  0:00:01s
epoch 12 | loss: 0.67413 | val_0_rmse: 0.96362 | val_1_rmse: 1.00244 |  0:00:02s
epoch 13 | loss: 0.5895  | val_0_rmse: 0.83422 | val_1_rmse: 0.88361 |  0:00:02s
epoch 14 | loss: 0.54954 | val_0_rmse: 0.743   | val_1_rmse: 0.77074 |  0:00:02s
epoch 15 | loss: 0.5465  | val_0_rmse: 0.75481 | val_1_rmse: 0.76067 |  0:00:02s
epoch 16 | loss: 0.54654 | val_0_rmse: 0.77165 | val_1_rmse: 0.77496 |  0:00:02s
epoch 17 | loss: 0.50401 | val_0_rmse: 0.76518 | val_1_rmse: 0.7804  |  0:00:02s
epoch 18 | loss: 0.51473 | val_0_rmse: 0.7271  | val_1_rmse: 0.74381 |  0:00:03s
epoch 19 | loss: 0.51943 | val_0_rmse: 0.70802 | val_1_rmse: 0.73418 |  0:00:03s
epoch 20 | loss: 0.48709 | val_0_rmse: 0.70167 | val_1_rmse: 0.74388 |  0:00:03s
epoch 21 | loss: 0.47578 | val_0_rmse: 0.70344 | val_1_rmse: 0.7628  |  0:00:03s
epoch 22 | loss: 0.48065 | val_0_rmse: 0.70674 | val_1_rmse: 0.74529 |  0:00:03s
epoch 23 | loss: 0.4651  | val_0_rmse: 0.70007 | val_1_rmse: 0.70463 |  0:00:03s
epoch 24 | loss: 0.46821 | val_0_rmse: 0.69215 | val_1_rmse: 0.69646 |  0:00:04s
epoch 25 | loss: 0.45193 | val_0_rmse: 0.70302 | val_1_rmse: 0.70894 |  0:00:04s
epoch 26 | loss: 0.46144 | val_0_rmse: 0.70054 | val_1_rmse: 0.70808 |  0:00:04s
epoch 27 | loss: 0.45602 | val_0_rmse: 0.69704 | val_1_rmse: 0.70448 |  0:00:04s
epoch 28 | loss: 0.43181 | val_0_rmse: 0.69521 | val_1_rmse: 0.70096 |  0:00:04s
epoch 29 | loss: 0.4139  | val_0_rmse: 0.7035  | val_1_rmse: 0.7161  |  0:00:04s
epoch 30 | loss: 0.41207 | val_0_rmse: 0.7051  | val_1_rmse: 0.71148 |  0:00:04s
epoch 31 | loss: 0.41299 | val_0_rmse: 0.69068 | val_1_rmse: 0.69182 |  0:00:05s
epoch 32 | loss: 0.39617 | val_0_rmse: 0.67018 | val_1_rmse: 0.67259 |  0:00:05s
epoch 33 | loss: 0.39675 | val_0_rmse: 0.66282 | val_1_rmse: 0.66749 |  0:00:05s
epoch 34 | loss: 0.38681 | val_0_rmse: 0.66359 | val_1_rmse: 0.66872 |  0:00:05s
epoch 35 | loss: 0.38883 | val_0_rmse: 0.66487 | val_1_rmse: 0.67504 |  0:00:05s
epoch 36 | loss: 0.36721 | val_0_rmse: 0.66302 | val_1_rmse: 0.6696  |  0:00:06s
epoch 37 | loss: 0.36157 | val_0_rmse: 0.67157 | val_1_rmse: 0.67433 |  0:00:06s
epoch 38 | loss: 0.38568 | val_0_rmse: 0.66798 | val_1_rmse: 0.67423 |  0:00:06s
epoch 39 | loss: 0.35138 | val_0_rmse: 0.69315 | val_1_rmse: 0.7065  |  0:00:06s
epoch 40 | loss: 0.3521  | val_0_rmse: 0.65299 | val_1_rmse: 0.66619 |  0:00:06s
epoch 41 | loss: 0.33695 | val_0_rmse: 0.67026 | val_1_rmse: 0.68567 |  0:00:06s
epoch 42 | loss: 0.34565 | val_0_rmse: 0.66336 | val_1_rmse: 0.68847 |  0:00:06s
epoch 43 | loss: 0.34738 | val_0_rmse: 0.68338 | val_1_rmse: 0.70738 |  0:00:07s
epoch 44 | loss: 0.34647 | val_0_rmse: 0.67057 | val_1_rmse: 0.69101 |  0:00:07s
epoch 45 | loss: 0.34335 | val_0_rmse: 0.6752  | val_1_rmse: 0.69638 |  0:00:07s
epoch 46 | loss: 0.33225 | val_0_rmse: 0.67808 | val_1_rmse: 0.70961 |  0:00:07s
epoch 47 | loss: 0.34054 | val_0_rmse: 0.67187 | val_1_rmse: 0.70607 |  0:00:07s
epoch 48 | loss: 0.31769 | val_0_rmse: 0.65032 | val_1_rmse: 0.68157 |  0:00:07s
epoch 49 | loss: 0.32317 | val_0_rmse: 0.65108 | val_1_rmse: 0.66448 |  0:00:08s
epoch 50 | loss: 0.32333 | val_0_rmse: 0.64893 | val_1_rmse: 0.65955 |  0:00:08s
epoch 51 | loss: 0.32181 | val_0_rmse: 0.65113 | val_1_rmse: 0.67076 |  0:00:08s
epoch 52 | loss: 0.31776 | val_0_rmse: 0.66486 | val_1_rmse: 0.68707 |  0:00:08s
epoch 53 | loss: 0.32009 | val_0_rmse: 0.6601  | val_1_rmse: 0.67765 |  0:00:08s
epoch 54 | loss: 0.29648 | val_0_rmse: 0.63945 | val_1_rmse: 0.64403 |  0:00:08s
epoch 55 | loss: 0.30081 | val_0_rmse: 0.63819 | val_1_rmse: 0.64523 |  0:00:09s
epoch 56 | loss: 0.29912 | val_0_rmse: 0.64848 | val_1_rmse: 0.66021 |  0:00:09s
epoch 57 | loss: 0.30042 | val_0_rmse: 0.65375 | val_1_rmse: 0.66887 |  0:00:09s
epoch 58 | loss: 0.29616 | val_0_rmse: 0.64801 | val_1_rmse: 0.67637 |  0:00:09s
epoch 59 | loss: 0.29874 | val_0_rmse: 0.6382  | val_1_rmse: 0.66226 |  0:00:09s
epoch 60 | loss: 0.28333 | val_0_rmse: 0.63848 | val_1_rmse: 0.66776 |  0:00:09s
epoch 61 | loss: 0.296   | val_0_rmse: 0.64318 | val_1_rmse: 0.68006 |  0:00:09s
epoch 62 | loss: 0.28855 | val_0_rmse: 0.6563  | val_1_rmse: 0.69312 |  0:00:10s
epoch 63 | loss: 0.28526 | val_0_rmse: 0.6583  | val_1_rmse: 0.69256 |  0:00:10s
epoch 64 | loss: 0.28223 | val_0_rmse: 0.64869 | val_1_rmse: 0.68795 |  0:00:10s
epoch 65 | loss: 0.29158 | val_0_rmse: 0.64879 | val_1_rmse: 0.67669 |  0:00:10s
epoch 66 | loss: 0.28994 | val_0_rmse: 0.64536 | val_1_rmse: 0.67319 |  0:00:10s
epoch 67 | loss: 0.2855  | val_0_rmse: 0.63316 | val_1_rmse: 0.6558  |  0:00:10s
epoch 68 | loss: 0.28967 | val_0_rmse: 0.64279 | val_1_rmse: 0.66417 |  0:00:11s
epoch 69 | loss: 0.28271 | val_0_rmse: 0.63686 | val_1_rmse: 0.65486 |  0:00:11s
epoch 70 | loss: 0.28197 | val_0_rmse: 0.63826 | val_1_rmse: 0.65293 |  0:00:11s
epoch 71 | loss: 0.30624 | val_0_rmse: 0.63502 | val_1_rmse: 0.64997 |  0:00:11s
epoch 72 | loss: 0.2985  | val_0_rmse: 0.64145 | val_1_rmse: 0.65711 |  0:00:11s
epoch 73 | loss: 0.30451 | val_0_rmse: 0.64637 | val_1_rmse: 0.66284 |  0:00:11s
epoch 74 | loss: 0.28946 | val_0_rmse: 0.66605 | val_1_rmse: 0.68403 |  0:00:12s
epoch 75 | loss: 0.29801 | val_0_rmse: 0.67361 | val_1_rmse: 0.68359 |  0:00:12s
epoch 76 | loss: 0.28489 | val_0_rmse: 0.66139 | val_1_rmse: 0.66639 |  0:00:12s
epoch 77 | loss: 0.27191 | val_0_rmse: 0.68075 | val_1_rmse: 0.69248 |  0:00:12s
epoch 78 | loss: 0.28435 | val_0_rmse: 0.66303 | val_1_rmse: 0.69847 |  0:00:12s
epoch 79 | loss: 0.30136 | val_0_rmse: 0.64487 | val_1_rmse: 0.68186 |  0:00:12s
epoch 80 | loss: 0.30385 | val_0_rmse: 0.64439 | val_1_rmse: 0.68423 |  0:00:12s
epoch 81 | loss: 0.2993  | val_0_rmse: 0.66764 | val_1_rmse: 0.70444 |  0:00:13s
epoch 82 | loss: 0.31386 | val_0_rmse: 0.6584  | val_1_rmse: 0.68835 |  0:00:13s
epoch 83 | loss: 0.30838 | val_0_rmse: 0.64006 | val_1_rmse: 0.66465 |  0:00:13s
epoch 84 | loss: 0.29794 | val_0_rmse: 0.64557 | val_1_rmse: 0.66497 |  0:00:13s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.64403
Best weights from best epoch are automatically used!
ended training at: 16:35:00
Feature importance:
Mean squared error is of 3750386639.5187893
Mean absolute error:44626.319420918364
MAPE:0.47594202012951475
R2 score:0.47990045646106416
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:35:00
epoch 0  | loss: 3.96978 | val_0_rmse: 1.09034 | val_1_rmse: 1.07482 |  0:00:00s
epoch 1  | loss: 2.98977 | val_0_rmse: 1.03733 | val_1_rmse: 1.01109 |  0:00:00s
epoch 2  | loss: 1.90959 | val_0_rmse: 1.01018 | val_1_rmse: 0.97596 |  0:00:00s
epoch 3  | loss: 1.50476 | val_0_rmse: 1.00706 | val_1_rmse: 0.97058 |  0:00:00s
epoch 4  | loss: 1.20281 | val_0_rmse: 1.00673 | val_1_rmse: 0.97123 |  0:00:00s
epoch 5  | loss: 1.16434 | val_0_rmse: 1.00376 | val_1_rmse: 0.96807 |  0:00:00s
epoch 6  | loss: 1.07139 | val_0_rmse: 0.99468 | val_1_rmse: 0.95982 |  0:00:01s
epoch 7  | loss: 1.03028 | val_0_rmse: 0.9872  | val_1_rmse: 0.94937 |  0:00:01s
epoch 8  | loss: 0.99736 | val_0_rmse: 0.96084 | val_1_rmse: 0.92827 |  0:00:01s
epoch 9  | loss: 0.95827 | val_0_rmse: 0.94449 | val_1_rmse: 0.9205  |  0:00:01s
epoch 10 | loss: 0.90608 | val_0_rmse: 0.90236 | val_1_rmse: 0.88978 |  0:00:01s
epoch 11 | loss: 0.89347 | val_0_rmse: 0.94224 | val_1_rmse: 0.95502 |  0:00:02s
epoch 12 | loss: 0.84769 | val_0_rmse: 1.03368 | val_1_rmse: 1.08641 |  0:00:02s
epoch 13 | loss: 0.79734 | val_0_rmse: 0.98837 | val_1_rmse: 1.03919 |  0:00:02s
epoch 14 | loss: 0.81106 | val_0_rmse: 0.8939  | val_1_rmse: 0.91955 |  0:00:02s
epoch 15 | loss: 0.79384 | val_0_rmse: 0.90202 | val_1_rmse: 0.91734 |  0:00:02s
epoch 16 | loss: 0.76817 | val_0_rmse: 0.87349 | val_1_rmse: 0.88025 |  0:00:02s
epoch 17 | loss: 0.7437  | val_0_rmse: 0.86372 | val_1_rmse: 0.887   |  0:00:02s
epoch 18 | loss: 0.72688 | val_0_rmse: 0.85262 | val_1_rmse: 0.87287 |  0:00:03s
epoch 19 | loss: 0.73887 | val_0_rmse: 0.85256 | val_1_rmse: 0.86924 |  0:00:03s
epoch 20 | loss: 0.71248 | val_0_rmse: 0.86971 | val_1_rmse: 0.89482 |  0:00:03s
epoch 21 | loss: 0.69543 | val_0_rmse: 0.86278 | val_1_rmse: 0.88282 |  0:00:03s
epoch 22 | loss: 0.75007 | val_0_rmse: 0.83577 | val_1_rmse: 0.84617 |  0:00:03s
epoch 23 | loss: 0.72561 | val_0_rmse: 0.82392 | val_1_rmse: 0.82086 |  0:00:03s
epoch 24 | loss: 0.68511 | val_0_rmse: 0.81717 | val_1_rmse: 0.81764 |  0:00:04s
epoch 25 | loss: 0.67933 | val_0_rmse: 0.8457  | val_1_rmse: 0.86859 |  0:00:04s
epoch 26 | loss: 0.65069 | val_0_rmse: 0.82618 | val_1_rmse: 0.84426 |  0:00:04s
epoch 27 | loss: 0.64222 | val_0_rmse: 0.7881  | val_1_rmse: 0.77543 |  0:00:04s
epoch 28 | loss: 0.61546 | val_0_rmse: 0.75683 | val_1_rmse: 0.74165 |  0:00:04s
epoch 29 | loss: 0.61246 | val_0_rmse: 0.70913 | val_1_rmse: 0.72585 |  0:00:04s
epoch 30 | loss: 0.5373  | val_0_rmse: 0.72009 | val_1_rmse: 0.765   |  0:00:05s
epoch 31 | loss: 0.52077 | val_0_rmse: 0.6963  | val_1_rmse: 0.74002 |  0:00:05s
epoch 32 | loss: 0.50419 | val_0_rmse: 0.7022  | val_1_rmse: 0.75359 |  0:00:05s
epoch 33 | loss: 0.4821  | val_0_rmse: 0.71667 | val_1_rmse: 0.77899 |  0:00:05s
epoch 34 | loss: 0.45505 | val_0_rmse: 0.72578 | val_1_rmse: 0.78651 |  0:00:05s
epoch 35 | loss: 0.43251 | val_0_rmse: 0.74308 | val_1_rmse: 0.8105  |  0:00:05s
epoch 36 | loss: 0.42257 | val_0_rmse: 0.74742 | val_1_rmse: 0.80699 |  0:00:06s
epoch 37 | loss: 0.42362 | val_0_rmse: 0.80629 | val_1_rmse: 0.89124 |  0:00:06s
epoch 38 | loss: 0.40938 | val_0_rmse: 0.91456 | val_1_rmse: 1.02351 |  0:00:06s
epoch 39 | loss: 0.37971 | val_0_rmse: 0.83133 | val_1_rmse: 0.9206  |  0:00:06s
epoch 40 | loss: 0.39242 | val_0_rmse: 0.77507 | val_1_rmse: 0.84636 |  0:00:06s
epoch 41 | loss: 0.35399 | val_0_rmse: 0.80717 | val_1_rmse: 0.88567 |  0:00:06s
epoch 42 | loss: 0.36478 | val_0_rmse: 0.77104 | val_1_rmse: 0.83811 |  0:00:06s
epoch 43 | loss: 0.3608  | val_0_rmse: 0.72834 | val_1_rmse: 0.77651 |  0:00:07s
epoch 44 | loss: 0.34991 | val_0_rmse: 0.70981 | val_1_rmse: 0.74529 |  0:00:07s
epoch 45 | loss: 0.3224  | val_0_rmse: 0.67353 | val_1_rmse: 0.69037 |  0:00:07s
epoch 46 | loss: 0.32047 | val_0_rmse: 0.65916 | val_1_rmse: 0.66456 |  0:00:07s
epoch 47 | loss: 0.35427 | val_0_rmse: 0.66071 | val_1_rmse: 0.66385 |  0:00:07s
epoch 48 | loss: 0.34035 | val_0_rmse: 0.67115 | val_1_rmse: 0.68962 |  0:00:07s
epoch 49 | loss: 0.31983 | val_0_rmse: 0.66318 | val_1_rmse: 0.67745 |  0:00:08s
epoch 50 | loss: 0.29454 | val_0_rmse: 0.66499 | val_1_rmse: 0.68196 |  0:00:08s
epoch 51 | loss: 0.2975  | val_0_rmse: 0.67127 | val_1_rmse: 0.69687 |  0:00:08s
epoch 52 | loss: 0.29545 | val_0_rmse: 0.66197 | val_1_rmse: 0.67821 |  0:00:08s
epoch 53 | loss: 0.29076 | val_0_rmse: 0.65992 | val_1_rmse: 0.66757 |  0:00:08s
epoch 54 | loss: 0.28078 | val_0_rmse: 0.66208 | val_1_rmse: 0.67173 |  0:00:08s
epoch 55 | loss: 0.27152 | val_0_rmse: 0.66995 | val_1_rmse: 0.68922 |  0:00:09s
epoch 56 | loss: 0.26586 | val_0_rmse: 0.66612 | val_1_rmse: 0.6826  |  0:00:09s
epoch 57 | loss: 0.25934 | val_0_rmse: 0.66195 | val_1_rmse: 0.6782  |  0:00:09s
epoch 58 | loss: 0.26601 | val_0_rmse: 0.66671 | val_1_rmse: 0.68997 |  0:00:09s
epoch 59 | loss: 0.24889 | val_0_rmse: 0.67631 | val_1_rmse: 0.70681 |  0:00:09s
epoch 60 | loss: 0.25453 | val_0_rmse: 0.67664 | val_1_rmse: 0.7104  |  0:00:09s
epoch 61 | loss: 0.25558 | val_0_rmse: 0.66409 | val_1_rmse: 0.69494 |  0:00:10s
epoch 62 | loss: 0.25715 | val_0_rmse: 0.66349 | val_1_rmse: 0.69688 |  0:00:10s
epoch 63 | loss: 0.24552 | val_0_rmse: 0.6775  | val_1_rmse: 0.71737 |  0:00:10s
epoch 64 | loss: 0.24325 | val_0_rmse: 0.67015 | val_1_rmse: 0.70498 |  0:00:10s
epoch 65 | loss: 0.23836 | val_0_rmse: 0.65057 | val_1_rmse: 0.67126 |  0:00:10s
epoch 66 | loss: 0.23642 | val_0_rmse: 0.64372 | val_1_rmse: 0.65336 |  0:00:10s
epoch 67 | loss: 0.23088 | val_0_rmse: 0.64654 | val_1_rmse: 0.66588 |  0:00:11s
epoch 68 | loss: 0.2276  | val_0_rmse: 0.65765 | val_1_rmse: 0.6891  |  0:00:11s
epoch 69 | loss: 0.22447 | val_0_rmse: 0.65724 | val_1_rmse: 0.68969 |  0:00:11s
epoch 70 | loss: 0.2439  | val_0_rmse: 0.64819 | val_1_rmse: 0.67433 |  0:00:11s
epoch 71 | loss: 0.24575 | val_0_rmse: 0.64544 | val_1_rmse: 0.67123 |  0:00:11s
epoch 72 | loss: 0.21719 | val_0_rmse: 0.65418 | val_1_rmse: 0.68826 |  0:00:11s
epoch 73 | loss: 0.22547 | val_0_rmse: 0.65601 | val_1_rmse: 0.69256 |  0:00:11s
epoch 74 | loss: 0.21669 | val_0_rmse: 0.66474 | val_1_rmse: 0.70724 |  0:00:12s
epoch 75 | loss: 0.2293  | val_0_rmse: 0.65317 | val_1_rmse: 0.68912 |  0:00:12s
epoch 76 | loss: 0.21266 | val_0_rmse: 0.65059 | val_1_rmse: 0.68506 |  0:00:12s
epoch 77 | loss: 0.21713 | val_0_rmse: 0.65419 | val_1_rmse: 0.69384 |  0:00:12s
epoch 78 | loss: 0.21293 | val_0_rmse: 0.65908 | val_1_rmse: 0.70863 |  0:00:12s
epoch 79 | loss: 0.22042 | val_0_rmse: 0.64595 | val_1_rmse: 0.69117 |  0:00:12s
epoch 80 | loss: 0.22803 | val_0_rmse: 0.6591  | val_1_rmse: 0.7152  |  0:00:13s
epoch 81 | loss: 0.21172 | val_0_rmse: 0.64355 | val_1_rmse: 0.68952 |  0:00:13s
epoch 82 | loss: 0.2093  | val_0_rmse: 0.63339 | val_1_rmse: 0.66631 |  0:00:13s
epoch 83 | loss: 0.22028 | val_0_rmse: 0.65175 | val_1_rmse: 0.70021 |  0:00:13s
epoch 84 | loss: 0.22542 | val_0_rmse: 0.67988 | val_1_rmse: 0.74088 |  0:00:13s
epoch 85 | loss: 0.24152 | val_0_rmse: 0.63624 | val_1_rmse: 0.67402 |  0:00:13s
epoch 86 | loss: 0.21008 | val_0_rmse: 0.64232 | val_1_rmse: 0.68298 |  0:00:14s
epoch 87 | loss: 0.20284 | val_0_rmse: 0.65599 | val_1_rmse: 0.70375 |  0:00:14s
epoch 88 | loss: 0.21319 | val_0_rmse: 0.63737 | val_1_rmse: 0.67638 |  0:00:14s
epoch 89 | loss: 0.19883 | val_0_rmse: 0.63073 | val_1_rmse: 0.65927 |  0:00:14s
epoch 90 | loss: 0.2008  | val_0_rmse: 0.64262 | val_1_rmse: 0.67753 |  0:00:14s
epoch 91 | loss: 0.19862 | val_0_rmse: 0.64959 | val_1_rmse: 0.69295 |  0:00:14s
epoch 92 | loss: 0.21169 | val_0_rmse: 0.6538  | val_1_rmse: 0.70409 |  0:00:14s
epoch 93 | loss: 0.20155 | val_0_rmse: 0.6483  | val_1_rmse: 0.69466 |  0:00:15s
epoch 94 | loss: 0.19993 | val_0_rmse: 0.64213 | val_1_rmse: 0.67066 |  0:00:15s
epoch 95 | loss: 0.21029 | val_0_rmse: 0.64145 | val_1_rmse: 0.66237 |  0:00:15s
epoch 96 | loss: 0.19905 | val_0_rmse: 0.6389  | val_1_rmse: 0.67685 |  0:00:15s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.65336
Best weights from best epoch are automatically used!
ended training at: 16:35:15
Feature importance:
Mean squared error is of 3031295481.4097652
Mean absolute error:41294.32757142857
MAPE:0.4261439760650054
R2 score:0.5870227530182559
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:35:15
epoch 0  | loss: 4.51391 | val_0_rmse: 1.03927 | val_1_rmse: 1.02156 |  0:00:00s
epoch 1  | loss: 2.38567 | val_0_rmse: 1.01846 | val_1_rmse: 0.98761 |  0:00:00s
epoch 2  | loss: 1.7447  | val_0_rmse: 1.01757 | val_1_rmse: 0.98508 |  0:00:00s
epoch 3  | loss: 1.5764  | val_0_rmse: 1.01878 | val_1_rmse: 0.98379 |  0:00:00s
epoch 4  | loss: 1.21605 | val_0_rmse: 1.01781 | val_1_rmse: 0.98389 |  0:00:00s
epoch 5  | loss: 1.18292 | val_0_rmse: 1.01515 | val_1_rmse: 0.98262 |  0:00:00s
epoch 6  | loss: 1.03137 | val_0_rmse: 0.99558 | val_1_rmse: 0.96735 |  0:00:01s
epoch 7  | loss: 0.97618 | val_0_rmse: 0.93636 | val_1_rmse: 0.92249 |  0:00:01s
epoch 8  | loss: 0.90454 | val_0_rmse: 0.88444 | val_1_rmse: 0.88629 |  0:00:01s
epoch 9  | loss: 0.87832 | val_0_rmse: 0.84402 | val_1_rmse: 0.867   |  0:00:01s
epoch 10 | loss: 0.79952 | val_0_rmse: 0.80958 | val_1_rmse: 0.86171 |  0:00:01s
epoch 11 | loss: 0.79768 | val_0_rmse: 0.79784 | val_1_rmse: 0.85935 |  0:00:01s
epoch 12 | loss: 0.72048 | val_0_rmse: 0.77616 | val_1_rmse: 0.84014 |  0:00:02s
epoch 13 | loss: 0.69404 | val_0_rmse: 0.76554 | val_1_rmse: 0.83475 |  0:00:02s
epoch 14 | loss: 0.6405  | val_0_rmse: 0.79743 | val_1_rmse: 0.86635 |  0:00:02s
epoch 15 | loss: 0.60694 | val_0_rmse: 0.79046 | val_1_rmse: 0.85945 |  0:00:02s
epoch 16 | loss: 0.56292 | val_0_rmse: 0.78773 | val_1_rmse: 0.85607 |  0:00:02s
epoch 17 | loss: 0.54476 | val_0_rmse: 0.76696 | val_1_rmse: 0.83185 |  0:00:02s
epoch 18 | loss: 0.51907 | val_0_rmse: 0.75145 | val_1_rmse: 0.81301 |  0:00:03s
epoch 19 | loss: 0.50905 | val_0_rmse: 0.7587  | val_1_rmse: 0.8027  |  0:00:03s
epoch 20 | loss: 0.4768  | val_0_rmse: 0.77791 | val_1_rmse: 0.81076 |  0:00:03s
epoch 21 | loss: 0.46136 | val_0_rmse: 0.74801 | val_1_rmse: 0.78204 |  0:00:03s
epoch 22 | loss: 0.43409 | val_0_rmse: 0.73575 | val_1_rmse: 0.79334 |  0:00:03s
epoch 23 | loss: 0.41212 | val_0_rmse: 0.7201  | val_1_rmse: 0.79553 |  0:00:03s
epoch 24 | loss: 0.40404 | val_0_rmse: 0.73502 | val_1_rmse: 0.81169 |  0:00:04s
epoch 25 | loss: 0.40466 | val_0_rmse: 0.73416 | val_1_rmse: 0.80388 |  0:00:04s
epoch 26 | loss: 0.37646 | val_0_rmse: 0.70585 | val_1_rmse: 0.7728  |  0:00:04s
epoch 27 | loss: 0.37524 | val_0_rmse: 0.7108  | val_1_rmse: 0.77159 |  0:00:04s
epoch 28 | loss: 0.37696 | val_0_rmse: 0.72608 | val_1_rmse: 0.78435 |  0:00:04s
epoch 29 | loss: 0.36775 | val_0_rmse: 0.71181 | val_1_rmse: 0.77165 |  0:00:04s
epoch 30 | loss: 0.34564 | val_0_rmse: 0.70594 | val_1_rmse: 0.77091 |  0:00:05s
epoch 31 | loss: 0.33633 | val_0_rmse: 0.71042 | val_1_rmse: 0.781   |  0:00:05s
epoch 32 | loss: 0.34571 | val_0_rmse: 0.71507 | val_1_rmse: 0.78579 |  0:00:05s
epoch 33 | loss: 0.32393 | val_0_rmse: 0.71823 | val_1_rmse: 0.80044 |  0:00:05s
epoch 34 | loss: 0.3308  | val_0_rmse: 0.70831 | val_1_rmse: 0.80767 |  0:00:05s
epoch 35 | loss: 0.31079 | val_0_rmse: 0.69598 | val_1_rmse: 0.7869  |  0:00:05s
epoch 36 | loss: 0.31651 | val_0_rmse: 0.69414 | val_1_rmse: 0.78617 |  0:00:06s
epoch 37 | loss: 0.2927  | val_0_rmse: 0.72322 | val_1_rmse: 0.81151 |  0:00:06s
epoch 38 | loss: 0.30304 | val_0_rmse: 0.75891 | val_1_rmse: 0.84156 |  0:00:06s
epoch 39 | loss: 0.30627 | val_0_rmse: 0.74321 | val_1_rmse: 0.82504 |  0:00:06s
epoch 40 | loss: 0.29136 | val_0_rmse: 0.70948 | val_1_rmse: 0.79542 |  0:00:06s
epoch 41 | loss: 0.27855 | val_0_rmse: 0.69555 | val_1_rmse: 0.77785 |  0:00:06s
epoch 42 | loss: 0.29146 | val_0_rmse: 0.69921 | val_1_rmse: 0.7864  |  0:00:07s
epoch 43 | loss: 0.27863 | val_0_rmse: 0.71633 | val_1_rmse: 0.81162 |  0:00:07s
epoch 44 | loss: 0.28399 | val_0_rmse: 0.70959 | val_1_rmse: 0.80893 |  0:00:07s
epoch 45 | loss: 0.27384 | val_0_rmse: 0.68933 | val_1_rmse: 0.78469 |  0:00:07s
epoch 46 | loss: 0.27375 | val_0_rmse: 0.68588 | val_1_rmse: 0.77328 |  0:00:07s
epoch 47 | loss: 0.27547 | val_0_rmse: 0.68585 | val_1_rmse: 0.77788 |  0:00:07s
epoch 48 | loss: 0.26938 | val_0_rmse: 0.69167 | val_1_rmse: 0.79291 |  0:00:07s
epoch 49 | loss: 0.26267 | val_0_rmse: 0.69604 | val_1_rmse: 0.79352 |  0:00:08s
epoch 50 | loss: 0.26873 | val_0_rmse: 0.69029 | val_1_rmse: 0.78255 |  0:00:08s
epoch 51 | loss: 0.25395 | val_0_rmse: 0.67936 | val_1_rmse: 0.7729  |  0:00:08s
epoch 52 | loss: 0.26815 | val_0_rmse: 0.67844 | val_1_rmse: 0.76822 |  0:00:08s
epoch 53 | loss: 0.25109 | val_0_rmse: 0.68201 | val_1_rmse: 0.7729  |  0:00:08s
epoch 54 | loss: 0.24381 | val_0_rmse: 0.66771 | val_1_rmse: 0.76264 |  0:00:08s
epoch 55 | loss: 0.26055 | val_0_rmse: 0.67539 | val_1_rmse: 0.75984 |  0:00:09s
epoch 56 | loss: 0.2474  | val_0_rmse: 0.68899 | val_1_rmse: 0.76585 |  0:00:09s
epoch 57 | loss: 0.24724 | val_0_rmse: 0.67913 | val_1_rmse: 0.76822 |  0:00:09s
epoch 58 | loss: 0.23565 | val_0_rmse: 0.67212 | val_1_rmse: 0.764   |  0:00:09s
epoch 59 | loss: 0.23081 | val_0_rmse: 0.66485 | val_1_rmse: 0.75147 |  0:00:09s
epoch 60 | loss: 0.23233 | val_0_rmse: 0.65198 | val_1_rmse: 0.74667 |  0:00:09s
epoch 61 | loss: 0.23888 | val_0_rmse: 0.66023 | val_1_rmse: 0.74536 |  0:00:10s
epoch 62 | loss: 0.23958 | val_0_rmse: 0.67562 | val_1_rmse: 0.75463 |  0:00:10s
epoch 63 | loss: 0.24958 | val_0_rmse: 0.67645 | val_1_rmse: 0.7628  |  0:00:10s
epoch 64 | loss: 0.24287 | val_0_rmse: 0.66143 | val_1_rmse: 0.7495  |  0:00:10s
epoch 65 | loss: 0.23493 | val_0_rmse: 0.65608 | val_1_rmse: 0.7429  |  0:00:10s
epoch 66 | loss: 0.2334  | val_0_rmse: 0.65166 | val_1_rmse: 0.74482 |  0:00:10s
epoch 67 | loss: 0.23615 | val_0_rmse: 0.65857 | val_1_rmse: 0.7548  |  0:00:11s
epoch 68 | loss: 0.22503 | val_0_rmse: 0.65567 | val_1_rmse: 0.75193 |  0:00:11s
epoch 69 | loss: 0.23258 | val_0_rmse: 0.65955 | val_1_rmse: 0.75231 |  0:00:11s
epoch 70 | loss: 0.23106 | val_0_rmse: 0.65474 | val_1_rmse: 0.73561 |  0:00:11s
epoch 71 | loss: 0.23276 | val_0_rmse: 0.65142 | val_1_rmse: 0.72433 |  0:00:11s
epoch 72 | loss: 0.23509 | val_0_rmse: 0.65722 | val_1_rmse: 0.74067 |  0:00:11s
epoch 73 | loss: 0.23949 | val_0_rmse: 0.66764 | val_1_rmse: 0.76551 |  0:00:11s
epoch 74 | loss: 0.22455 | val_0_rmse: 0.6522  | val_1_rmse: 0.7504  |  0:00:12s
epoch 75 | loss: 0.23109 | val_0_rmse: 0.647   | val_1_rmse: 0.73182 |  0:00:12s
epoch 76 | loss: 0.24295 | val_0_rmse: 0.6364  | val_1_rmse: 0.71697 |  0:00:12s
epoch 77 | loss: 0.21618 | val_0_rmse: 0.6363  | val_1_rmse: 0.72018 |  0:00:12s
epoch 78 | loss: 0.23051 | val_0_rmse: 0.63231 | val_1_rmse: 0.71972 |  0:00:12s
epoch 79 | loss: 0.21175 | val_0_rmse: 0.64038 | val_1_rmse: 0.72632 |  0:00:12s
epoch 80 | loss: 0.22623 | val_0_rmse: 0.63401 | val_1_rmse: 0.7234  |  0:00:13s
epoch 81 | loss: 0.21481 | val_0_rmse: 0.63284 | val_1_rmse: 0.7252  |  0:00:13s
epoch 82 | loss: 0.21746 | val_0_rmse: 0.6348  | val_1_rmse: 0.72515 |  0:00:13s
epoch 83 | loss: 0.23006 | val_0_rmse: 0.6218  | val_1_rmse: 0.72606 |  0:00:13s
epoch 84 | loss: 0.22313 | val_0_rmse: 0.61881 | val_1_rmse: 0.72688 |  0:00:13s
epoch 85 | loss: 0.2062  | val_0_rmse: 0.62806 | val_1_rmse: 0.72573 |  0:00:13s
epoch 86 | loss: 0.21839 | val_0_rmse: 0.63671 | val_1_rmse: 0.73318 |  0:00:14s
epoch 87 | loss: 0.19911 | val_0_rmse: 0.62149 | val_1_rmse: 0.72919 |  0:00:14s
epoch 88 | loss: 0.20051 | val_0_rmse: 0.61152 | val_1_rmse: 0.71987 |  0:00:14s
epoch 89 | loss: 0.20112 | val_0_rmse: 0.61371 | val_1_rmse: 0.7103  |  0:00:14s
epoch 90 | loss: 0.1929  | val_0_rmse: 0.62448 | val_1_rmse: 0.71144 |  0:00:14s
epoch 91 | loss: 0.18809 | val_0_rmse: 0.61051 | val_1_rmse: 0.70185 |  0:00:14s
epoch 92 | loss: 0.1921  | val_0_rmse: 0.60331 | val_1_rmse: 0.69946 |  0:00:15s
epoch 93 | loss: 0.18457 | val_0_rmse: 0.61434 | val_1_rmse: 0.709   |  0:00:15s
epoch 94 | loss: 0.19608 | val_0_rmse: 0.61169 | val_1_rmse: 0.7052  |  0:00:15s
epoch 95 | loss: 0.18813 | val_0_rmse: 0.59401 | val_1_rmse: 0.69322 |  0:00:15s
epoch 96 | loss: 0.18663 | val_0_rmse: 0.6075  | val_1_rmse: 0.69738 |  0:00:15s
epoch 97 | loss: 0.1909  | val_0_rmse: 0.62546 | val_1_rmse: 0.71293 |  0:00:15s
epoch 98 | loss: 0.18713 | val_0_rmse: 0.61025 | val_1_rmse: 0.71948 |  0:00:16s
epoch 99 | loss: 0.18995 | val_0_rmse: 0.60464 | val_1_rmse: 0.72685 |  0:00:16s
epoch 100| loss: 0.18833 | val_0_rmse: 0.59643 | val_1_rmse: 0.71354 |  0:00:16s
epoch 101| loss: 0.19166 | val_0_rmse: 0.60803 | val_1_rmse: 0.70971 |  0:00:16s
epoch 102| loss: 0.18147 | val_0_rmse: 0.59999 | val_1_rmse: 0.69958 |  0:00:16s
epoch 103| loss: 0.18065 | val_0_rmse: 0.59839 | val_1_rmse: 0.70632 |  0:00:16s
epoch 104| loss: 0.18875 | val_0_rmse: 0.59856 | val_1_rmse: 0.70784 |  0:00:16s
epoch 105| loss: 0.19075 | val_0_rmse: 0.59768 | val_1_rmse: 0.7045  |  0:00:17s
epoch 106| loss: 0.19049 | val_0_rmse: 0.59409 | val_1_rmse: 0.69422 |  0:00:17s
epoch 107| loss: 0.20965 | val_0_rmse: 0.58683 | val_1_rmse: 0.69743 |  0:00:17s
epoch 108| loss: 0.18591 | val_0_rmse: 0.58011 | val_1_rmse: 0.69754 |  0:00:17s
epoch 109| loss: 0.19227 | val_0_rmse: 0.59133 | val_1_rmse: 0.69341 |  0:00:17s
epoch 110| loss: 0.19158 | val_0_rmse: 0.59491 | val_1_rmse: 0.70254 |  0:00:17s
epoch 111| loss: 0.18477 | val_0_rmse: 0.59039 | val_1_rmse: 0.71407 |  0:00:18s
epoch 112| loss: 0.1899  | val_0_rmse: 0.58465 | val_1_rmse: 0.70808 |  0:00:18s
epoch 113| loss: 0.17862 | val_0_rmse: 0.58014 | val_1_rmse: 0.70076 |  0:00:18s
epoch 114| loss: 0.19    | val_0_rmse: 0.59777 | val_1_rmse: 0.73208 |  0:00:18s
epoch 115| loss: 0.17858 | val_0_rmse: 0.60914 | val_1_rmse: 0.74423 |  0:00:18s
epoch 116| loss: 0.18831 | val_0_rmse: 0.57158 | val_1_rmse: 0.70408 |  0:00:18s
epoch 117| loss: 0.16655 | val_0_rmse: 0.56722 | val_1_rmse: 0.70527 |  0:00:19s
epoch 118| loss: 0.17296 | val_0_rmse: 0.5664  | val_1_rmse: 0.71606 |  0:00:19s
epoch 119| loss: 0.18763 | val_0_rmse: 0.56901 | val_1_rmse: 0.71445 |  0:00:19s
epoch 120| loss: 0.1712  | val_0_rmse: 0.56766 | val_1_rmse: 0.70849 |  0:00:19s
epoch 121| loss: 0.16903 | val_0_rmse: 0.55577 | val_1_rmse: 0.69631 |  0:00:19s
epoch 122| loss: 0.17055 | val_0_rmse: 0.5563  | val_1_rmse: 0.6993  |  0:00:19s
epoch 123| loss: 0.17214 | val_0_rmse: 0.55355 | val_1_rmse: 0.68903 |  0:00:19s
epoch 124| loss: 0.17397 | val_0_rmse: 0.56693 | val_1_rmse: 0.68437 |  0:00:20s
epoch 125| loss: 0.17295 | val_0_rmse: 0.56317 | val_1_rmse: 0.68046 |  0:00:20s
epoch 126| loss: 0.16411 | val_0_rmse: 0.55946 | val_1_rmse: 0.68427 |  0:00:20s
epoch 127| loss: 0.16855 | val_0_rmse: 0.55514 | val_1_rmse: 0.68    |  0:00:20s
epoch 128| loss: 0.16661 | val_0_rmse: 0.55979 | val_1_rmse: 0.68069 |  0:00:20s
epoch 129| loss: 0.16544 | val_0_rmse: 0.55053 | val_1_rmse: 0.67465 |  0:00:20s
epoch 130| loss: 0.15933 | val_0_rmse: 0.54808 | val_1_rmse: 0.68446 |  0:00:21s
epoch 131| loss: 0.16159 | val_0_rmse: 0.55338 | val_1_rmse: 0.6909  |  0:00:21s
epoch 132| loss: 0.16757 | val_0_rmse: 0.55351 | val_1_rmse: 0.68369 |  0:00:21s
epoch 133| loss: 0.16021 | val_0_rmse: 0.55656 | val_1_rmse: 0.68119 |  0:00:21s
epoch 134| loss: 0.15776 | val_0_rmse: 0.53936 | val_1_rmse: 0.6797  |  0:00:21s
epoch 135| loss: 0.17082 | val_0_rmse: 0.53708 | val_1_rmse: 0.68584 |  0:00:21s
epoch 136| loss: 0.16424 | val_0_rmse: 0.54605 | val_1_rmse: 0.69202 |  0:00:22s
epoch 137| loss: 0.1568  | val_0_rmse: 0.54136 | val_1_rmse: 0.68612 |  0:00:22s
epoch 138| loss: 0.16585 | val_0_rmse: 0.53433 | val_1_rmse: 0.67967 |  0:00:22s
epoch 139| loss: 0.16639 | val_0_rmse: 0.52625 | val_1_rmse: 0.67847 |  0:00:22s
epoch 140| loss: 0.17726 | val_0_rmse: 0.52421 | val_1_rmse: 0.68618 |  0:00:22s
epoch 141| loss: 0.16659 | val_0_rmse: 0.52863 | val_1_rmse: 0.69709 |  0:00:22s
epoch 142| loss: 0.16208 | val_0_rmse: 0.52987 | val_1_rmse: 0.69503 |  0:00:22s
epoch 143| loss: 0.15935 | val_0_rmse: 0.51962 | val_1_rmse: 0.69328 |  0:00:23s
epoch 144| loss: 0.15397 | val_0_rmse: 0.51066 | val_1_rmse: 0.69477 |  0:00:23s
epoch 145| loss: 0.1573  | val_0_rmse: 0.50939 | val_1_rmse: 0.70587 |  0:00:23s
epoch 146| loss: 0.16243 | val_0_rmse: 0.51651 | val_1_rmse: 0.71806 |  0:00:23s
epoch 147| loss: 0.15974 | val_0_rmse: 0.5227  | val_1_rmse: 0.71297 |  0:00:23s
epoch 148| loss: 0.15618 | val_0_rmse: 0.5162  | val_1_rmse: 0.69333 |  0:00:24s
epoch 149| loss: 0.15633 | val_0_rmse: 0.51233 | val_1_rmse: 0.68535 |  0:00:24s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.67465
Best weights from best epoch are automatically used!
ended training at: 16:35:40
Feature importance:
Mean squared error is of 2600864158.1188297
Mean absolute error:37175.129413973926
MAPE:0.3318714600838414
R2 score:0.6049030383572207
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:35:40
epoch 0  | loss: 2.88334 | val_0_rmse: 1.01824 | val_1_rmse: 0.98141 |  0:00:00s
epoch 1  | loss: 1.57144 | val_0_rmse: 1.00497 | val_1_rmse: 0.9714  |  0:00:00s
epoch 2  | loss: 1.18655 | val_0_rmse: 1.03389 | val_1_rmse: 0.99251 |  0:00:01s
epoch 3  | loss: 0.9896  | val_0_rmse: 1.01663 | val_1_rmse: 0.97839 |  0:00:01s
epoch 4  | loss: 0.89599 | val_0_rmse: 1.11355 | val_1_rmse: 1.25446 |  0:00:01s
epoch 5  | loss: 0.80832 | val_0_rmse: 0.88418 | val_1_rmse: 0.84976 |  0:00:02s
epoch 6  | loss: 0.67705 | val_0_rmse: 0.77861 | val_1_rmse: 0.78911 |  0:00:02s
epoch 7  | loss: 0.5957  | val_0_rmse: 0.75623 | val_1_rmse: 0.77217 |  0:00:02s
epoch 8  | loss: 0.53899 | val_0_rmse: 0.75542 | val_1_rmse: 0.7743  |  0:00:03s
epoch 9  | loss: 0.49791 | val_0_rmse: 0.75171 | val_1_rmse: 0.77212 |  0:00:03s
epoch 10 | loss: 0.45927 | val_0_rmse: 0.74413 | val_1_rmse: 0.75974 |  0:00:03s
epoch 11 | loss: 0.45129 | val_0_rmse: 0.73449 | val_1_rmse: 0.75761 |  0:00:04s
epoch 12 | loss: 0.4419  | val_0_rmse: 0.73694 | val_1_rmse: 0.75285 |  0:00:04s
epoch 13 | loss: 0.46398 | val_0_rmse: 0.73111 | val_1_rmse: 0.75906 |  0:00:04s
epoch 14 | loss: 0.43874 | val_0_rmse: 0.71467 | val_1_rmse: 0.73123 |  0:00:05s
epoch 15 | loss: 0.43829 | val_0_rmse: 0.73317 | val_1_rmse: 0.74508 |  0:00:05s
epoch 16 | loss: 0.44709 | val_0_rmse: 0.75026 | val_1_rmse: 0.74475 |  0:00:06s
epoch 17 | loss: 0.42755 | val_0_rmse: 0.71256 | val_1_rmse: 0.72931 |  0:00:06s
epoch 18 | loss: 0.41771 | val_0_rmse: 0.7308  | val_1_rmse: 0.719   |  0:00:06s
epoch 19 | loss: 0.40287 | val_0_rmse: 0.69359 | val_1_rmse: 0.66885 |  0:00:07s
epoch 20 | loss: 0.39427 | val_0_rmse: 0.70768 | val_1_rmse: 0.70361 |  0:00:07s
epoch 21 | loss: 0.4126  | val_0_rmse: 0.70895 | val_1_rmse: 0.70927 |  0:00:07s
epoch 22 | loss: 0.40766 | val_0_rmse: 0.70988 | val_1_rmse: 0.70176 |  0:00:08s
epoch 23 | loss: 0.40699 | val_0_rmse: 0.69897 | val_1_rmse: 0.6962  |  0:00:08s
epoch 24 | loss: 0.39501 | val_0_rmse: 0.69712 | val_1_rmse: 0.70004 |  0:00:08s
epoch 25 | loss: 0.39058 | val_0_rmse: 0.68909 | val_1_rmse: 0.69479 |  0:00:09s
epoch 26 | loss: 0.35911 | val_0_rmse: 0.69919 | val_1_rmse: 0.70466 |  0:00:09s
epoch 27 | loss: 0.36148 | val_0_rmse: 0.68337 | val_1_rmse: 0.69367 |  0:00:09s
epoch 28 | loss: 0.35102 | val_0_rmse: 0.68534 | val_1_rmse: 0.68968 |  0:00:10s
epoch 29 | loss: 0.35237 | val_0_rmse: 0.66809 | val_1_rmse: 0.67552 |  0:00:10s
epoch 30 | loss: 0.35093 | val_0_rmse: 0.71113 | val_1_rmse: 0.70077 |  0:00:10s
epoch 31 | loss: 0.34592 | val_0_rmse: 0.6778  | val_1_rmse: 0.70395 |  0:00:11s
epoch 32 | loss: 0.34508 | val_0_rmse: 0.7201  | val_1_rmse: 0.70936 |  0:00:11s
epoch 33 | loss: 0.35431 | val_0_rmse: 0.68822 | val_1_rmse: 0.73182 |  0:00:11s
epoch 34 | loss: 0.3446  | val_0_rmse: 0.67511 | val_1_rmse: 0.69739 |  0:00:12s
epoch 35 | loss: 0.33906 | val_0_rmse: 0.66425 | val_1_rmse: 0.67929 |  0:00:12s
epoch 36 | loss: 0.33878 | val_0_rmse: 0.67969 | val_1_rmse: 0.66907 |  0:00:12s
epoch 37 | loss: 0.33808 | val_0_rmse: 0.65995 | val_1_rmse: 0.65615 |  0:00:13s
epoch 38 | loss: 0.33648 | val_0_rmse: 0.66193 | val_1_rmse: 0.66572 |  0:00:13s
epoch 39 | loss: 0.33295 | val_0_rmse: 0.64132 | val_1_rmse: 0.66217 |  0:00:14s
epoch 40 | loss: 0.33058 | val_0_rmse: 0.6358  | val_1_rmse: 0.65864 |  0:00:14s
epoch 41 | loss: 0.32986 | val_0_rmse: 0.64011 | val_1_rmse: 0.65388 |  0:00:14s
epoch 42 | loss: 0.32228 | val_0_rmse: 0.64572 | val_1_rmse: 0.66929 |  0:00:15s
epoch 43 | loss: 0.32112 | val_0_rmse: 0.64774 | val_1_rmse: 0.66306 |  0:00:15s
epoch 44 | loss: 0.3246  | val_0_rmse: 0.65402 | val_1_rmse: 0.66532 |  0:00:15s
epoch 45 | loss: 0.3115  | val_0_rmse: 0.66005 | val_1_rmse: 0.66492 |  0:00:16s
epoch 46 | loss: 0.31847 | val_0_rmse: 0.64706 | val_1_rmse: 0.65817 |  0:00:16s
epoch 47 | loss: 0.3145  | val_0_rmse: 0.63851 | val_1_rmse: 0.6515  |  0:00:16s
epoch 48 | loss: 0.31861 | val_0_rmse: 0.6379  | val_1_rmse: 0.65626 |  0:00:17s
epoch 49 | loss: 0.31561 | val_0_rmse: 0.63777 | val_1_rmse: 0.66494 |  0:00:17s
epoch 50 | loss: 0.31324 | val_0_rmse: 0.64359 | val_1_rmse: 0.66209 |  0:00:17s
epoch 51 | loss: 0.32398 | val_0_rmse: 0.64992 | val_1_rmse: 0.67135 |  0:00:18s
epoch 52 | loss: 0.31993 | val_0_rmse: 0.6597  | val_1_rmse: 0.6692  |  0:00:18s
epoch 53 | loss: 0.31197 | val_0_rmse: 0.64554 | val_1_rmse: 0.67543 |  0:00:18s
epoch 54 | loss: 0.31123 | val_0_rmse: 0.64796 | val_1_rmse: 0.66293 |  0:00:19s
epoch 55 | loss: 0.31362 | val_0_rmse: 0.61273 | val_1_rmse: 0.63176 |  0:00:19s
epoch 56 | loss: 0.30589 | val_0_rmse: 0.63721 | val_1_rmse: 0.64573 |  0:00:19s
epoch 57 | loss: 0.30617 | val_0_rmse: 0.61001 | val_1_rmse: 0.64866 |  0:00:20s
epoch 58 | loss: 0.31209 | val_0_rmse: 0.61878 | val_1_rmse: 0.6426  |  0:00:20s
epoch 59 | loss: 0.3009  | val_0_rmse: 0.63324 | val_1_rmse: 0.64998 |  0:00:21s
epoch 60 | loss: 0.30445 | val_0_rmse: 0.62757 | val_1_rmse: 0.65372 |  0:00:21s
epoch 61 | loss: 0.29846 | val_0_rmse: 0.60431 | val_1_rmse: 0.63644 |  0:00:21s
epoch 62 | loss: 0.29236 | val_0_rmse: 0.60956 | val_1_rmse: 0.62725 |  0:00:22s
epoch 63 | loss: 0.29579 | val_0_rmse: 0.62385 | val_1_rmse: 0.64358 |  0:00:22s
epoch 64 | loss: 0.30173 | val_0_rmse: 0.59587 | val_1_rmse: 0.62937 |  0:00:22s
epoch 65 | loss: 0.29299 | val_0_rmse: 0.61243 | val_1_rmse: 0.63342 |  0:00:23s
epoch 66 | loss: 0.30149 | val_0_rmse: 0.612   | val_1_rmse: 0.6366  |  0:00:23s
epoch 67 | loss: 0.28815 | val_0_rmse: 0.61002 | val_1_rmse: 0.64322 |  0:00:23s
epoch 68 | loss: 0.28783 | val_0_rmse: 0.60202 | val_1_rmse: 0.62291 |  0:00:24s
epoch 69 | loss: 0.30558 | val_0_rmse: 0.60499 | val_1_rmse: 0.62552 |  0:00:24s
epoch 70 | loss: 0.3105  | val_0_rmse: 0.61269 | val_1_rmse: 0.63411 |  0:00:24s
epoch 71 | loss: 0.30577 | val_0_rmse: 0.60276 | val_1_rmse: 0.62639 |  0:00:25s
epoch 72 | loss: 0.29866 | val_0_rmse: 0.59538 | val_1_rmse: 0.62136 |  0:00:25s
epoch 73 | loss: 0.29496 | val_0_rmse: 0.588   | val_1_rmse: 0.61542 |  0:00:25s
epoch 74 | loss: 0.2914  | val_0_rmse: 0.60407 | val_1_rmse: 0.63206 |  0:00:26s
epoch 75 | loss: 0.28955 | val_0_rmse: 0.58828 | val_1_rmse: 0.61582 |  0:00:26s
epoch 76 | loss: 0.29255 | val_0_rmse: 0.59964 | val_1_rmse: 0.62619 |  0:00:26s
epoch 77 | loss: 0.2822  | val_0_rmse: 0.57916 | val_1_rmse: 0.61616 |  0:00:27s
epoch 78 | loss: 0.28645 | val_0_rmse: 0.57898 | val_1_rmse: 0.61824 |  0:00:27s
epoch 79 | loss: 0.28338 | val_0_rmse: 0.58404 | val_1_rmse: 0.61265 |  0:00:28s
epoch 80 | loss: 0.28111 | val_0_rmse: 0.57278 | val_1_rmse: 0.61693 |  0:00:28s
epoch 81 | loss: 0.2751  | val_0_rmse: 0.56989 | val_1_rmse: 0.60764 |  0:00:28s
epoch 82 | loss: 0.27998 | val_0_rmse: 0.56725 | val_1_rmse: 0.61299 |  0:00:29s
epoch 83 | loss: 0.27176 | val_0_rmse: 0.5744  | val_1_rmse: 0.61308 |  0:00:29s
epoch 84 | loss: 0.27004 | val_0_rmse: 0.5702  | val_1_rmse: 0.61306 |  0:00:29s
epoch 85 | loss: 0.27218 | val_0_rmse: 0.55116 | val_1_rmse: 0.59181 |  0:00:30s
epoch 86 | loss: 0.27135 | val_0_rmse: 0.55894 | val_1_rmse: 0.60238 |  0:00:30s
epoch 87 | loss: 0.26762 | val_0_rmse: 0.57632 | val_1_rmse: 0.60884 |  0:00:30s
epoch 88 | loss: 0.26823 | val_0_rmse: 0.55691 | val_1_rmse: 0.61202 |  0:00:31s
epoch 89 | loss: 0.27327 | val_0_rmse: 0.55461 | val_1_rmse: 0.60208 |  0:00:31s
epoch 90 | loss: 0.27148 | val_0_rmse: 0.54377 | val_1_rmse: 0.58537 |  0:00:31s
epoch 91 | loss: 0.26996 | val_0_rmse: 0.55257 | val_1_rmse: 0.59735 |  0:00:32s
epoch 92 | loss: 0.26721 | val_0_rmse: 0.54372 | val_1_rmse: 0.59376 |  0:00:32s
epoch 93 | loss: 0.26875 | val_0_rmse: 0.56571 | val_1_rmse: 0.61386 |  0:00:32s
epoch 94 | loss: 0.2726  | val_0_rmse: 0.54993 | val_1_rmse: 0.61781 |  0:00:33s
epoch 95 | loss: 0.26952 | val_0_rmse: 0.5462  | val_1_rmse: 0.59685 |  0:00:33s
epoch 96 | loss: 0.26921 | val_0_rmse: 0.5328  | val_1_rmse: 0.59283 |  0:00:33s
epoch 97 | loss: 0.26253 | val_0_rmse: 0.54621 | val_1_rmse: 0.59576 |  0:00:34s
epoch 98 | loss: 0.25677 | val_0_rmse: 0.53687 | val_1_rmse: 0.60209 |  0:00:34s
epoch 99 | loss: 0.27516 | val_0_rmse: 0.54622 | val_1_rmse: 0.6018  |  0:00:35s
epoch 100| loss: 0.26338 | val_0_rmse: 0.53985 | val_1_rmse: 0.60546 |  0:00:35s
epoch 101| loss: 0.26707 | val_0_rmse: 0.53827 | val_1_rmse: 0.61519 |  0:00:35s
epoch 102| loss: 0.28003 | val_0_rmse: 0.59652 | val_1_rmse: 0.63977 |  0:00:36s
epoch 103| loss: 0.29186 | val_0_rmse: 0.53245 | val_1_rmse: 0.6111  |  0:00:36s
epoch 104| loss: 0.26206 | val_0_rmse: 0.54731 | val_1_rmse: 0.5985  |  0:00:36s
epoch 105| loss: 0.27232 | val_0_rmse: 0.52983 | val_1_rmse: 0.61494 |  0:00:37s
epoch 106| loss: 0.25294 | val_0_rmse: 0.53066 | val_1_rmse: 0.59783 |  0:00:37s
epoch 107| loss: 0.26017 | val_0_rmse: 0.52103 | val_1_rmse: 0.60557 |  0:00:37s
epoch 108| loss: 0.25506 | val_0_rmse: 0.52927 | val_1_rmse: 0.58766 |  0:00:38s
epoch 109| loss: 0.26232 | val_0_rmse: 0.51861 | val_1_rmse: 0.60288 |  0:00:38s
epoch 110| loss: 0.25246 | val_0_rmse: 0.51875 | val_1_rmse: 0.59699 |  0:00:38s
epoch 111| loss: 0.25033 | val_0_rmse: 0.50972 | val_1_rmse: 0.59998 |  0:00:39s
epoch 112| loss: 0.25328 | val_0_rmse: 0.5242  | val_1_rmse: 0.60174 |  0:00:39s
epoch 113| loss: 0.25399 | val_0_rmse: 0.50631 | val_1_rmse: 0.60004 |  0:00:39s
epoch 114| loss: 0.24603 | val_0_rmse: 0.51541 | val_1_rmse: 0.60437 |  0:00:40s
epoch 115| loss: 0.24852 | val_0_rmse: 0.50493 | val_1_rmse: 0.5902  |  0:00:40s
epoch 116| loss: 0.25022 | val_0_rmse: 0.50877 | val_1_rmse: 0.59368 |  0:00:40s
epoch 117| loss: 0.24965 | val_0_rmse: 0.52341 | val_1_rmse: 0.60624 |  0:00:41s
epoch 118| loss: 0.24727 | val_0_rmse: 0.5186  | val_1_rmse: 0.60109 |  0:00:41s
epoch 119| loss: 0.24685 | val_0_rmse: 0.52394 | val_1_rmse: 0.60589 |  0:00:42s
epoch 120| loss: 0.23938 | val_0_rmse: 0.50342 | val_1_rmse: 0.59568 |  0:00:42s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.58537
Best weights from best epoch are automatically used!
ended training at: 16:36:22
Feature importance:
Mean squared error is of 2797447433.1008935
Mean absolute error:36388.63703201932
MAPE:0.3384446269056844
R2 score:0.671888393884391
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:36:23
epoch 0  | loss: 3.83571 | val_0_rmse: 1.01419 | val_1_rmse: 0.97897 |  0:00:00s
epoch 1  | loss: 1.40733 | val_0_rmse: 0.9837  | val_1_rmse: 0.94886 |  0:00:00s
epoch 2  | loss: 1.22562 | val_0_rmse: 0.98556 | val_1_rmse: 0.94706 |  0:00:01s
epoch 3  | loss: 0.9917  | val_0_rmse: 0.90217 | val_1_rmse: 0.86689 |  0:00:01s
epoch 4  | loss: 0.80712 | val_0_rmse: 0.86451 | val_1_rmse: 0.8421  |  0:00:01s
epoch 5  | loss: 0.73023 | val_0_rmse: 0.83334 | val_1_rmse: 0.80781 |  0:00:02s
epoch 6  | loss: 0.68183 | val_0_rmse: 0.82843 | val_1_rmse: 0.7935  |  0:00:02s
epoch 7  | loss: 0.64825 | val_0_rmse: 0.84414 | val_1_rmse: 0.81014 |  0:00:02s
epoch 8  | loss: 0.625   | val_0_rmse: 0.80858 | val_1_rmse: 0.77531 |  0:00:03s
epoch 9  | loss: 0.57987 | val_0_rmse: 0.80352 | val_1_rmse: 0.77622 |  0:00:03s
epoch 10 | loss: 0.5305  | val_0_rmse: 0.75421 | val_1_rmse: 0.74052 |  0:00:03s
epoch 11 | loss: 0.50716 | val_0_rmse: 0.79654 | val_1_rmse: 0.79318 |  0:00:04s
epoch 12 | loss: 0.47989 | val_0_rmse: 0.72308 | val_1_rmse: 0.71771 |  0:00:04s
epoch 13 | loss: 0.46151 | val_0_rmse: 0.7562  | val_1_rmse: 0.75545 |  0:00:05s
epoch 14 | loss: 0.43444 | val_0_rmse: 0.70552 | val_1_rmse: 0.68386 |  0:00:05s
epoch 15 | loss: 0.42102 | val_0_rmse: 0.69397 | val_1_rmse: 0.66057 |  0:00:05s
epoch 16 | loss: 0.42273 | val_0_rmse: 0.6849  | val_1_rmse: 0.64433 |  0:00:06s
epoch 17 | loss: 0.40912 | val_0_rmse: 0.70281 | val_1_rmse: 0.66756 |  0:00:06s
epoch 18 | loss: 0.4024  | val_0_rmse: 0.71142 | val_1_rmse: 0.67627 |  0:00:06s
epoch 19 | loss: 0.39456 | val_0_rmse: 0.7105  | val_1_rmse: 0.67611 |  0:00:07s
epoch 20 | loss: 0.38705 | val_0_rmse: 0.73648 | val_1_rmse: 0.70673 |  0:00:07s
epoch 21 | loss: 0.38779 | val_0_rmse: 0.68129 | val_1_rmse: 0.64948 |  0:00:07s
epoch 22 | loss: 0.37821 | val_0_rmse: 0.7004  | val_1_rmse: 0.66823 |  0:00:08s
epoch 23 | loss: 0.38543 | val_0_rmse: 0.67429 | val_1_rmse: 0.64698 |  0:00:08s
epoch 24 | loss: 0.38052 | val_0_rmse: 0.67075 | val_1_rmse: 0.64096 |  0:00:08s
epoch 25 | loss: 0.37813 | val_0_rmse: 0.67663 | val_1_rmse: 0.65088 |  0:00:09s
epoch 26 | loss: 0.39231 | val_0_rmse: 0.66496 | val_1_rmse: 0.62625 |  0:00:09s
epoch 27 | loss: 0.38916 | val_0_rmse: 0.67928 | val_1_rmse: 0.64726 |  0:00:09s
epoch 28 | loss: 0.37681 | val_0_rmse: 0.65435 | val_1_rmse: 0.6187  |  0:00:10s
epoch 29 | loss: 0.37764 | val_0_rmse: 0.66024 | val_1_rmse: 0.62987 |  0:00:10s
epoch 30 | loss: 0.3787  | val_0_rmse: 0.65249 | val_1_rmse: 0.61489 |  0:00:11s
epoch 31 | loss: 0.37923 | val_0_rmse: 0.66228 | val_1_rmse: 0.62931 |  0:00:11s
epoch 32 | loss: 0.35558 | val_0_rmse: 0.65793 | val_1_rmse: 0.62078 |  0:00:11s
epoch 33 | loss: 0.37191 | val_0_rmse: 0.65713 | val_1_rmse: 0.62077 |  0:00:12s
epoch 34 | loss: 0.35618 | val_0_rmse: 0.66019 | val_1_rmse: 0.61715 |  0:00:12s
epoch 35 | loss: 0.35901 | val_0_rmse: 0.6598  | val_1_rmse: 0.61865 |  0:00:12s
epoch 36 | loss: 0.36992 | val_0_rmse: 0.66645 | val_1_rmse: 0.62605 |  0:00:13s
epoch 37 | loss: 0.37816 | val_0_rmse: 0.66737 | val_1_rmse: 0.63079 |  0:00:13s
epoch 38 | loss: 0.37492 | val_0_rmse: 0.66272 | val_1_rmse: 0.62939 |  0:00:13s
epoch 39 | loss: 0.37005 | val_0_rmse: 0.64151 | val_1_rmse: 0.61058 |  0:00:14s
epoch 40 | loss: 0.37676 | val_0_rmse: 0.63972 | val_1_rmse: 0.6063  |  0:00:14s
epoch 41 | loss: 0.37008 | val_0_rmse: 0.65513 | val_1_rmse: 0.63214 |  0:00:14s
epoch 42 | loss: 0.3643  | val_0_rmse: 0.64221 | val_1_rmse: 0.61693 |  0:00:15s
epoch 43 | loss: 0.35699 | val_0_rmse: 0.6433  | val_1_rmse: 0.6207  |  0:00:15s
epoch 44 | loss: 0.35394 | val_0_rmse: 0.64737 | val_1_rmse: 0.62543 |  0:00:15s
epoch 45 | loss: 0.35381 | val_0_rmse: 0.63569 | val_1_rmse: 0.61316 |  0:00:16s
epoch 46 | loss: 0.34367 | val_0_rmse: 0.65332 | val_1_rmse: 0.62877 |  0:00:16s
epoch 47 | loss: 0.34654 | val_0_rmse: 0.64071 | val_1_rmse: 0.62794 |  0:00:17s
epoch 48 | loss: 0.34054 | val_0_rmse: 0.63834 | val_1_rmse: 0.62466 |  0:00:17s
epoch 49 | loss: 0.3383  | val_0_rmse: 0.6541  | val_1_rmse: 0.64063 |  0:00:17s
epoch 50 | loss: 0.32964 | val_0_rmse: 0.63569 | val_1_rmse: 0.62215 |  0:00:18s
epoch 51 | loss: 0.32858 | val_0_rmse: 0.6281  | val_1_rmse: 0.60638 |  0:00:18s
epoch 52 | loss: 0.32933 | val_0_rmse: 0.63473 | val_1_rmse: 0.6088  |  0:00:18s
epoch 53 | loss: 0.33074 | val_0_rmse: 0.62488 | val_1_rmse: 0.60447 |  0:00:19s
epoch 54 | loss: 0.32201 | val_0_rmse: 0.65299 | val_1_rmse: 0.62876 |  0:00:19s
epoch 55 | loss: 0.32718 | val_0_rmse: 0.63132 | val_1_rmse: 0.61261 |  0:00:19s
epoch 56 | loss: 0.32725 | val_0_rmse: 0.64478 | val_1_rmse: 0.62708 |  0:00:20s
epoch 57 | loss: 0.32565 | val_0_rmse: 0.62268 | val_1_rmse: 0.59595 |  0:00:20s
epoch 58 | loss: 0.31847 | val_0_rmse: 0.6312  | val_1_rmse: 0.60212 |  0:00:20s
epoch 59 | loss: 0.31999 | val_0_rmse: 0.62122 | val_1_rmse: 0.59621 |  0:00:21s
epoch 60 | loss: 0.31147 | val_0_rmse: 0.62068 | val_1_rmse: 0.60102 |  0:00:21s
epoch 61 | loss: 0.30714 | val_0_rmse: 0.63098 | val_1_rmse: 0.60582 |  0:00:21s
epoch 62 | loss: 0.31507 | val_0_rmse: 0.60877 | val_1_rmse: 0.59018 |  0:00:22s
epoch 63 | loss: 0.30982 | val_0_rmse: 0.61023 | val_1_rmse: 0.58615 |  0:00:22s
epoch 64 | loss: 0.3017  | val_0_rmse: 0.60491 | val_1_rmse: 0.58098 |  0:00:23s
epoch 65 | loss: 0.31901 | val_0_rmse: 0.59804 | val_1_rmse: 0.5728  |  0:00:23s
epoch 66 | loss: 0.30626 | val_0_rmse: 0.61106 | val_1_rmse: 0.58306 |  0:00:23s
epoch 67 | loss: 0.31414 | val_0_rmse: 0.61196 | val_1_rmse: 0.59805 |  0:00:24s
epoch 68 | loss: 0.30698 | val_0_rmse: 0.61028 | val_1_rmse: 0.60207 |  0:00:24s
epoch 69 | loss: 0.29904 | val_0_rmse: 0.60736 | val_1_rmse: 0.59068 |  0:00:24s
epoch 70 | loss: 0.30207 | val_0_rmse: 0.61022 | val_1_rmse: 0.60413 |  0:00:25s
epoch 71 | loss: 0.30212 | val_0_rmse: 0.60648 | val_1_rmse: 0.59173 |  0:00:25s
epoch 72 | loss: 0.31029 | val_0_rmse: 0.59574 | val_1_rmse: 0.59318 |  0:00:25s
epoch 73 | loss: 0.29967 | val_0_rmse: 0.58748 | val_1_rmse: 0.57079 |  0:00:26s
epoch 74 | loss: 0.30444 | val_0_rmse: 0.62627 | val_1_rmse: 0.6022  |  0:00:26s
epoch 75 | loss: 0.30654 | val_0_rmse: 0.58023 | val_1_rmse: 0.5678  |  0:00:26s
epoch 76 | loss: 0.29751 | val_0_rmse: 0.57689 | val_1_rmse: 0.56872 |  0:00:27s
epoch 77 | loss: 0.29601 | val_0_rmse: 0.57907 | val_1_rmse: 0.57774 |  0:00:27s
epoch 78 | loss: 0.29098 | val_0_rmse: 0.58269 | val_1_rmse: 0.57356 |  0:00:27s
epoch 79 | loss: 0.2897  | val_0_rmse: 0.57403 | val_1_rmse: 0.56825 |  0:00:28s
epoch 80 | loss: 0.30407 | val_0_rmse: 0.57317 | val_1_rmse: 0.56957 |  0:00:28s
epoch 81 | loss: 0.29137 | val_0_rmse: 0.57147 | val_1_rmse: 0.57307 |  0:00:29s
epoch 82 | loss: 0.29276 | val_0_rmse: 0.56074 | val_1_rmse: 0.55692 |  0:00:29s
epoch 83 | loss: 0.29576 | val_0_rmse: 0.55872 | val_1_rmse: 0.54908 |  0:00:29s
epoch 84 | loss: 0.2974  | val_0_rmse: 0.5523  | val_1_rmse: 0.55379 |  0:00:30s
epoch 85 | loss: 0.29319 | val_0_rmse: 0.5821  | val_1_rmse: 0.57004 |  0:00:30s
epoch 86 | loss: 0.28886 | val_0_rmse: 0.57852 | val_1_rmse: 0.58099 |  0:00:30s
epoch 87 | loss: 0.29735 | val_0_rmse: 0.56942 | val_1_rmse: 0.57711 |  0:00:31s
epoch 88 | loss: 0.3005  | val_0_rmse: 0.56448 | val_1_rmse: 0.57604 |  0:00:31s
epoch 89 | loss: 0.29451 | val_0_rmse: 0.56818 | val_1_rmse: 0.56257 |  0:00:31s
epoch 90 | loss: 0.2922  | val_0_rmse: 0.56377 | val_1_rmse: 0.57664 |  0:00:32s
epoch 91 | loss: 0.28766 | val_0_rmse: 0.55265 | val_1_rmse: 0.56505 |  0:00:32s
epoch 92 | loss: 0.28389 | val_0_rmse: 0.55783 | val_1_rmse: 0.55985 |  0:00:32s
epoch 93 | loss: 0.27929 | val_0_rmse: 0.56726 | val_1_rmse: 0.56403 |  0:00:33s
epoch 94 | loss: 0.28394 | val_0_rmse: 0.55846 | val_1_rmse: 0.56418 |  0:00:33s
epoch 95 | loss: 0.2769  | val_0_rmse: 0.54426 | val_1_rmse: 0.56661 |  0:00:33s
epoch 96 | loss: 0.27356 | val_0_rmse: 0.54243 | val_1_rmse: 0.56797 |  0:00:34s
epoch 97 | loss: 0.27202 | val_0_rmse: 0.55029 | val_1_rmse: 0.56364 |  0:00:34s
epoch 98 | loss: 0.27725 | val_0_rmse: 0.53855 | val_1_rmse: 0.55997 |  0:00:34s
epoch 99 | loss: 0.27752 | val_0_rmse: 0.55047 | val_1_rmse: 0.56503 |  0:00:35s
epoch 100| loss: 0.27949 | val_0_rmse: 0.53615 | val_1_rmse: 0.56857 |  0:00:35s
epoch 101| loss: 0.27672 | val_0_rmse: 0.53378 | val_1_rmse: 0.55785 |  0:00:36s
epoch 102| loss: 0.27557 | val_0_rmse: 0.53548 | val_1_rmse: 0.55699 |  0:00:36s
epoch 103| loss: 0.27024 | val_0_rmse: 0.53642 | val_1_rmse: 0.55581 |  0:00:36s
epoch 104| loss: 0.2723  | val_0_rmse: 0.54449 | val_1_rmse: 0.56607 |  0:00:37s
epoch 105| loss: 0.26528 | val_0_rmse: 0.52478 | val_1_rmse: 0.57291 |  0:00:37s
epoch 106| loss: 0.26358 | val_0_rmse: 0.52765 | val_1_rmse: 0.56141 |  0:00:37s
epoch 107| loss: 0.27181 | val_0_rmse: 0.52364 | val_1_rmse: 0.55678 |  0:00:38s
epoch 108| loss: 0.26062 | val_0_rmse: 0.51106 | val_1_rmse: 0.55241 |  0:00:38s
epoch 109| loss: 0.25965 | val_0_rmse: 0.52222 | val_1_rmse: 0.5559  |  0:00:38s
epoch 110| loss: 0.26232 | val_0_rmse: 0.5088  | val_1_rmse: 0.55976 |  0:00:39s
epoch 111| loss: 0.2628  | val_0_rmse: 0.51515 | val_1_rmse: 0.56593 |  0:00:39s
epoch 112| loss: 0.26053 | val_0_rmse: 0.51412 | val_1_rmse: 0.54582 |  0:00:39s
epoch 113| loss: 0.26258 | val_0_rmse: 0.51327 | val_1_rmse: 0.55021 |  0:00:40s
epoch 114| loss: 0.25906 | val_0_rmse: 0.5156  | val_1_rmse: 0.56665 |  0:00:40s
epoch 115| loss: 0.26996 | val_0_rmse: 0.5111  | val_1_rmse: 0.5638  |  0:00:40s
epoch 116| loss: 0.25601 | val_0_rmse: 0.51052 | val_1_rmse: 0.55603 |  0:00:41s
epoch 117| loss: 0.26098 | val_0_rmse: 0.52443 | val_1_rmse: 0.5613  |  0:00:41s
epoch 118| loss: 0.26224 | val_0_rmse: 0.50217 | val_1_rmse: 0.55653 |  0:00:42s
epoch 119| loss: 0.25484 | val_0_rmse: 0.50315 | val_1_rmse: 0.5675  |  0:00:42s
epoch 120| loss: 0.24849 | val_0_rmse: 0.49582 | val_1_rmse: 0.55232 |  0:00:42s
epoch 121| loss: 0.25793 | val_0_rmse: 0.4976  | val_1_rmse: 0.56489 |  0:00:43s
epoch 122| loss: 0.24889 | val_0_rmse: 0.5102  | val_1_rmse: 0.57349 |  0:00:43s
epoch 123| loss: 0.25503 | val_0_rmse: 0.50739 | val_1_rmse: 0.57246 |  0:00:43s
epoch 124| loss: 0.26452 | val_0_rmse: 0.50008 | val_1_rmse: 0.5571  |  0:00:44s
epoch 125| loss: 0.25994 | val_0_rmse: 0.49577 | val_1_rmse: 0.55395 |  0:00:44s
epoch 126| loss: 0.27214 | val_0_rmse: 0.49029 | val_1_rmse: 0.55532 |  0:00:44s
epoch 127| loss: 0.2428  | val_0_rmse: 0.48828 | val_1_rmse: 0.55495 |  0:00:45s
epoch 128| loss: 0.25647 | val_0_rmse: 0.48559 | val_1_rmse: 0.55318 |  0:00:45s
epoch 129| loss: 0.25728 | val_0_rmse: 0.49342 | val_1_rmse: 0.5543  |  0:00:45s
epoch 130| loss: 0.24852 | val_0_rmse: 0.49975 | val_1_rmse: 0.57823 |  0:00:46s
epoch 131| loss: 0.26273 | val_0_rmse: 0.51651 | val_1_rmse: 0.56291 |  0:00:46s
epoch 132| loss: 0.26842 | val_0_rmse: 0.52239 | val_1_rmse: 0.5432  |  0:00:46s
epoch 133| loss: 0.26713 | val_0_rmse: 0.53957 | val_1_rmse: 0.55623 |  0:00:47s
epoch 134| loss: 0.25823 | val_0_rmse: 0.5534  | val_1_rmse: 0.57726 |  0:00:47s
epoch 135| loss: 0.25011 | val_0_rmse: 0.5074  | val_1_rmse: 0.55536 |  0:00:47s
epoch 136| loss: 0.25416 | val_0_rmse: 0.49766 | val_1_rmse: 0.55024 |  0:00:48s
epoch 137| loss: 0.24701 | val_0_rmse: 0.49563 | val_1_rmse: 0.54959 |  0:00:48s
epoch 138| loss: 0.24668 | val_0_rmse: 0.4921  | val_1_rmse: 0.55808 |  0:00:49s
epoch 139| loss: 0.26108 | val_0_rmse: 0.48685 | val_1_rmse: 0.5678  |  0:00:49s
epoch 140| loss: 0.26131 | val_0_rmse: 0.49121 | val_1_rmse: 0.5696  |  0:00:49s
epoch 141| loss: 0.25495 | val_0_rmse: 0.51703 | val_1_rmse: 0.58529 |  0:00:50s
epoch 142| loss: 0.25829 | val_0_rmse: 0.49685 | val_1_rmse: 0.57469 |  0:00:50s
epoch 143| loss: 0.24676 | val_0_rmse: 0.47989 | val_1_rmse: 0.55781 |  0:00:50s
epoch 144| loss: 0.25584 | val_0_rmse: 0.47982 | val_1_rmse: 0.56061 |  0:00:51s
epoch 145| loss: 0.25075 | val_0_rmse: 0.47826 | val_1_rmse: 0.56974 |  0:00:51s
epoch 146| loss: 0.24916 | val_0_rmse: 0.48115 | val_1_rmse: 0.56146 |  0:00:51s
epoch 147| loss: 0.24013 | val_0_rmse: 0.50044 | val_1_rmse: 0.56062 |  0:00:52s
epoch 148| loss: 0.24792 | val_0_rmse: 0.49463 | val_1_rmse: 0.55835 |  0:00:52s
epoch 149| loss: 0.2414  | val_0_rmse: 0.48562 | val_1_rmse: 0.56339 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.5432
Best weights from best epoch are automatically used!
ended training at: 16:37:16
Feature importance:
Mean squared error is of 2794802094.946139
Mean absolute error:35150.82311550297
MAPE:0.332226636427583
R2 score:0.6592685922727635
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:37:16
epoch 0  | loss: 3.19662 | val_0_rmse: 0.99686 | val_1_rmse: 1.03742 |  0:00:00s
epoch 1  | loss: 1.7145  | val_0_rmse: 0.99391 | val_1_rmse: 1.03446 |  0:00:00s
epoch 2  | loss: 1.22882 | val_0_rmse: 0.98931 | val_1_rmse: 1.03029 |  0:00:01s
epoch 3  | loss: 1.00454 | val_0_rmse: 0.90335 | val_1_rmse: 0.93904 |  0:00:01s
epoch 4  | loss: 0.85687 | val_0_rmse: 0.78545 | val_1_rmse: 0.80284 |  0:00:01s
epoch 5  | loss: 0.70532 | val_0_rmse: 0.87093 | val_1_rmse: 0.85433 |  0:00:02s
epoch 6  | loss: 0.57208 | val_0_rmse: 0.8656  | val_1_rmse: 0.85904 |  0:00:02s
epoch 7  | loss: 0.52041 | val_0_rmse: 0.83431 | val_1_rmse: 0.82739 |  0:00:02s
epoch 8  | loss: 0.49555 | val_0_rmse: 0.7375  | val_1_rmse: 0.75144 |  0:00:03s
epoch 9  | loss: 0.49407 | val_0_rmse: 0.73104 | val_1_rmse: 0.74723 |  0:00:03s
epoch 10 | loss: 0.45628 | val_0_rmse: 0.72839 | val_1_rmse: 0.74332 |  0:00:03s
epoch 11 | loss: 0.44358 | val_0_rmse: 0.72702 | val_1_rmse: 0.73128 |  0:00:04s
epoch 12 | loss: 0.4411  | val_0_rmse: 0.73021 | val_1_rmse: 0.72978 |  0:00:04s
epoch 13 | loss: 0.4309  | val_0_rmse: 0.71597 | val_1_rmse: 0.73574 |  0:00:04s
epoch 14 | loss: 0.42169 | val_0_rmse: 0.7247  | val_1_rmse: 0.74287 |  0:00:05s
epoch 15 | loss: 0.41895 | val_0_rmse: 0.73503 | val_1_rmse: 0.7598  |  0:00:05s
epoch 16 | loss: 0.41217 | val_0_rmse: 0.74487 | val_1_rmse: 0.77527 |  0:00:05s
epoch 17 | loss: 0.39386 | val_0_rmse: 0.74145 | val_1_rmse: 0.76563 |  0:00:06s
epoch 18 | loss: 0.38551 | val_0_rmse: 0.76145 | val_1_rmse: 0.79175 |  0:00:06s
epoch 19 | loss: 0.37494 | val_0_rmse: 0.75281 | val_1_rmse: 0.78311 |  0:00:07s
epoch 20 | loss: 0.39019 | val_0_rmse: 0.74994 | val_1_rmse: 0.78743 |  0:00:07s
epoch 21 | loss: 0.38797 | val_0_rmse: 0.72895 | val_1_rmse: 0.76107 |  0:00:07s
epoch 22 | loss: 0.3755  | val_0_rmse: 0.7203  | val_1_rmse: 0.74688 |  0:00:08s
epoch 23 | loss: 0.37167 | val_0_rmse: 0.71256 | val_1_rmse: 0.73988 |  0:00:08s
epoch 24 | loss: 0.3688  | val_0_rmse: 0.70744 | val_1_rmse: 0.73065 |  0:00:08s
epoch 25 | loss: 0.35589 | val_0_rmse: 0.69048 | val_1_rmse: 0.71134 |  0:00:09s
epoch 26 | loss: 0.35555 | val_0_rmse: 0.68345 | val_1_rmse: 0.70376 |  0:00:09s
epoch 27 | loss: 0.35726 | val_0_rmse: 0.69803 | val_1_rmse: 0.72452 |  0:00:09s
epoch 28 | loss: 0.35362 | val_0_rmse: 0.69083 | val_1_rmse: 0.71328 |  0:00:10s
epoch 29 | loss: 0.34762 | val_0_rmse: 0.70149 | val_1_rmse: 0.72692 |  0:00:10s
epoch 30 | loss: 0.35673 | val_0_rmse: 0.69096 | val_1_rmse: 0.69776 |  0:00:10s
epoch 31 | loss: 0.34683 | val_0_rmse: 0.68335 | val_1_rmse: 0.71369 |  0:00:11s
epoch 32 | loss: 0.35248 | val_0_rmse: 0.68242 | val_1_rmse: 0.69974 |  0:00:11s
epoch 33 | loss: 0.35578 | val_0_rmse: 0.69443 | val_1_rmse: 0.71412 |  0:00:11s
epoch 34 | loss: 0.36705 | val_0_rmse: 0.68446 | val_1_rmse: 0.69541 |  0:00:12s
epoch 35 | loss: 0.37153 | val_0_rmse: 0.67252 | val_1_rmse: 0.69336 |  0:00:12s
epoch 36 | loss: 0.36513 | val_0_rmse: 0.67319 | val_1_rmse: 0.68386 |  0:00:13s
epoch 37 | loss: 0.35387 | val_0_rmse: 0.67997 | val_1_rmse: 0.69085 |  0:00:13s
epoch 38 | loss: 0.35931 | val_0_rmse: 0.67175 | val_1_rmse: 0.67152 |  0:00:13s
epoch 39 | loss: 0.34906 | val_0_rmse: 0.66937 | val_1_rmse: 0.67922 |  0:00:14s
epoch 40 | loss: 0.35456 | val_0_rmse: 0.69139 | val_1_rmse: 0.70277 |  0:00:14s
epoch 41 | loss: 0.36044 | val_0_rmse: 0.69308 | val_1_rmse: 0.69966 |  0:00:14s
epoch 42 | loss: 0.34526 | val_0_rmse: 0.67069 | val_1_rmse: 0.68048 |  0:00:15s
epoch 43 | loss: 0.3392  | val_0_rmse: 0.67566 | val_1_rmse: 0.68757 |  0:00:15s
epoch 44 | loss: 0.3432  | val_0_rmse: 0.67767 | val_1_rmse: 0.69475 |  0:00:15s
epoch 45 | loss: 0.33834 | val_0_rmse: 0.66186 | val_1_rmse: 0.67231 |  0:00:16s
epoch 46 | loss: 0.32263 | val_0_rmse: 0.65288 | val_1_rmse: 0.66144 |  0:00:16s
epoch 47 | loss: 0.32142 | val_0_rmse: 0.65479 | val_1_rmse: 0.66252 |  0:00:16s
epoch 48 | loss: 0.32116 | val_0_rmse: 0.65368 | val_1_rmse: 0.65944 |  0:00:17s
epoch 49 | loss: 0.32624 | val_0_rmse: 0.65676 | val_1_rmse: 0.66683 |  0:00:17s
epoch 50 | loss: 0.32438 | val_0_rmse: 0.64691 | val_1_rmse: 0.65557 |  0:00:17s
epoch 51 | loss: 0.31606 | val_0_rmse: 0.64618 | val_1_rmse: 0.66514 |  0:00:18s
epoch 52 | loss: 0.31103 | val_0_rmse: 0.65382 | val_1_rmse: 0.66943 |  0:00:18s
epoch 53 | loss: 0.31977 | val_0_rmse: 0.6427  | val_1_rmse: 0.66155 |  0:00:19s
epoch 54 | loss: 0.32119 | val_0_rmse: 0.64182 | val_1_rmse: 0.65715 |  0:00:19s
epoch 55 | loss: 0.31062 | val_0_rmse: 0.63871 | val_1_rmse: 0.66454 |  0:00:19s
epoch 56 | loss: 0.31194 | val_0_rmse: 0.63968 | val_1_rmse: 0.65947 |  0:00:20s
epoch 57 | loss: 0.31059 | val_0_rmse: 0.64639 | val_1_rmse: 0.67033 |  0:00:20s
epoch 58 | loss: 0.30919 | val_0_rmse: 0.62941 | val_1_rmse: 0.64506 |  0:00:20s
epoch 59 | loss: 0.30499 | val_0_rmse: 0.64156 | val_1_rmse: 0.66231 |  0:00:21s
epoch 60 | loss: 0.30144 | val_0_rmse: 0.64427 | val_1_rmse: 0.66849 |  0:00:21s
epoch 61 | loss: 0.30712 | val_0_rmse: 0.63085 | val_1_rmse: 0.65624 |  0:00:21s
epoch 62 | loss: 0.29983 | val_0_rmse: 0.62434 | val_1_rmse: 0.64883 |  0:00:22s
epoch 63 | loss: 0.29715 | val_0_rmse: 0.62017 | val_1_rmse: 0.65357 |  0:00:22s
epoch 64 | loss: 0.29015 | val_0_rmse: 0.62105 | val_1_rmse: 0.64959 |  0:00:22s
epoch 65 | loss: 0.29763 | val_0_rmse: 0.6278  | val_1_rmse: 0.65396 |  0:00:23s
epoch 66 | loss: 0.30075 | val_0_rmse: 0.63397 | val_1_rmse: 0.6656  |  0:00:23s
epoch 67 | loss: 0.29934 | val_0_rmse: 0.62574 | val_1_rmse: 0.65721 |  0:00:23s
epoch 68 | loss: 0.30192 | val_0_rmse: 0.62265 | val_1_rmse: 0.65804 |  0:00:24s
epoch 69 | loss: 0.30418 | val_0_rmse: 0.61244 | val_1_rmse: 0.64016 |  0:00:24s
epoch 70 | loss: 0.29378 | val_0_rmse: 0.61221 | val_1_rmse: 0.65233 |  0:00:25s
epoch 71 | loss: 0.29394 | val_0_rmse: 0.60784 | val_1_rmse: 0.64024 |  0:00:25s
epoch 72 | loss: 0.28387 | val_0_rmse: 0.60766 | val_1_rmse: 0.63713 |  0:00:25s
epoch 73 | loss: 0.28892 | val_0_rmse: 0.61401 | val_1_rmse: 0.6445  |  0:00:26s
epoch 74 | loss: 0.29262 | val_0_rmse: 0.61345 | val_1_rmse: 0.64156 |  0:00:26s
epoch 75 | loss: 0.2851  | val_0_rmse: 0.60473 | val_1_rmse: 0.63903 |  0:00:26s
epoch 76 | loss: 0.29466 | val_0_rmse: 0.59876 | val_1_rmse: 0.6326  |  0:00:27s
epoch 77 | loss: 0.29703 | val_0_rmse: 0.60476 | val_1_rmse: 0.63994 |  0:00:27s
epoch 78 | loss: 0.29688 | val_0_rmse: 0.61366 | val_1_rmse: 0.64933 |  0:00:27s
epoch 79 | loss: 0.29969 | val_0_rmse: 0.59828 | val_1_rmse: 0.62279 |  0:00:28s
epoch 80 | loss: 0.30168 | val_0_rmse: 0.59782 | val_1_rmse: 0.62945 |  0:00:28s
epoch 81 | loss: 0.29818 | val_0_rmse: 0.58988 | val_1_rmse: 0.61919 |  0:00:28s
epoch 82 | loss: 0.29003 | val_0_rmse: 0.60332 | val_1_rmse: 0.6367  |  0:00:29s
epoch 83 | loss: 0.29205 | val_0_rmse: 0.58869 | val_1_rmse: 0.61624 |  0:00:29s
epoch 84 | loss: 0.28767 | val_0_rmse: 0.59638 | val_1_rmse: 0.62926 |  0:00:29s
epoch 85 | loss: 0.28758 | val_0_rmse: 0.5855  | val_1_rmse: 0.62322 |  0:00:30s
epoch 86 | loss: 0.28844 | val_0_rmse: 0.57346 | val_1_rmse: 0.61736 |  0:00:30s
epoch 87 | loss: 0.28736 | val_0_rmse: 0.60532 | val_1_rmse: 0.64433 |  0:00:31s
epoch 88 | loss: 0.2869  | val_0_rmse: 0.59612 | val_1_rmse: 0.6318  |  0:00:31s
epoch 89 | loss: 0.28043 | val_0_rmse: 0.57803 | val_1_rmse: 0.61579 |  0:00:31s
epoch 90 | loss: 0.28188 | val_0_rmse: 0.573   | val_1_rmse: 0.61273 |  0:00:32s
epoch 91 | loss: 0.28284 | val_0_rmse: 0.57888 | val_1_rmse: 0.60719 |  0:00:32s
epoch 92 | loss: 0.28846 | val_0_rmse: 0.57559 | val_1_rmse: 0.59543 |  0:00:32s
epoch 93 | loss: 0.29304 | val_0_rmse: 0.57188 | val_1_rmse: 0.59856 |  0:00:33s
epoch 94 | loss: 0.2812  | val_0_rmse: 0.57229 | val_1_rmse: 0.60662 |  0:00:33s
epoch 95 | loss: 0.29244 | val_0_rmse: 0.59188 | val_1_rmse: 0.62842 |  0:00:33s
epoch 96 | loss: 0.28562 | val_0_rmse: 0.57351 | val_1_rmse: 0.60641 |  0:00:34s
epoch 97 | loss: 0.28204 | val_0_rmse: 0.56181 | val_1_rmse: 0.60493 |  0:00:34s
epoch 98 | loss: 0.28241 | val_0_rmse: 0.56345 | val_1_rmse: 0.62417 |  0:00:34s
epoch 99 | loss: 0.28149 | val_0_rmse: 0.55495 | val_1_rmse: 0.61689 |  0:00:35s
epoch 100| loss: 0.27926 | val_0_rmse: 0.55144 | val_1_rmse: 0.61365 |  0:00:35s
epoch 101| loss: 0.28268 | val_0_rmse: 0.57952 | val_1_rmse: 0.63574 |  0:00:36s
epoch 102| loss: 0.27584 | val_0_rmse: 0.54787 | val_1_rmse: 0.6081  |  0:00:36s
epoch 103| loss: 0.27085 | val_0_rmse: 0.55233 | val_1_rmse: 0.61179 |  0:00:36s
epoch 104| loss: 0.26953 | val_0_rmse: 0.54454 | val_1_rmse: 0.60724 |  0:00:37s
epoch 105| loss: 0.26252 | val_0_rmse: 0.54631 | val_1_rmse: 0.61624 |  0:00:37s
epoch 106| loss: 0.26968 | val_0_rmse: 0.53747 | val_1_rmse: 0.60761 |  0:00:37s
epoch 107| loss: 0.26967 | val_0_rmse: 0.54814 | val_1_rmse: 0.61671 |  0:00:38s
epoch 108| loss: 0.26638 | val_0_rmse: 0.53375 | val_1_rmse: 0.60919 |  0:00:38s
epoch 109| loss: 0.27425 | val_0_rmse: 0.53527 | val_1_rmse: 0.61063 |  0:00:38s
epoch 110| loss: 0.27341 | val_0_rmse: 0.55081 | val_1_rmse: 0.61248 |  0:00:39s
epoch 111| loss: 0.26628 | val_0_rmse: 0.53148 | val_1_rmse: 0.59941 |  0:00:39s
epoch 112| loss: 0.26577 | val_0_rmse: 0.52942 | val_1_rmse: 0.61226 |  0:00:39s
epoch 113| loss: 0.26189 | val_0_rmse: 0.52539 | val_1_rmse: 0.60267 |  0:00:40s
epoch 114| loss: 0.25782 | val_0_rmse: 0.53801 | val_1_rmse: 0.6025  |  0:00:40s
epoch 115| loss: 0.26417 | val_0_rmse: 0.51807 | val_1_rmse: 0.58607 |  0:00:40s
epoch 116| loss: 0.27078 | val_0_rmse: 0.52298 | val_1_rmse: 0.59419 |  0:00:41s
epoch 117| loss: 0.26612 | val_0_rmse: 0.52019 | val_1_rmse: 0.5799  |  0:00:41s
epoch 118| loss: 0.26817 | val_0_rmse: 0.53614 | val_1_rmse: 0.60174 |  0:00:42s
epoch 119| loss: 0.26446 | val_0_rmse: 0.53355 | val_1_rmse: 0.59609 |  0:00:42s
epoch 120| loss: 0.28522 | val_0_rmse: 0.56463 | val_1_rmse: 0.62787 |  0:00:42s
epoch 121| loss: 0.31232 | val_0_rmse: 0.58535 | val_1_rmse: 0.65905 |  0:00:43s
epoch 122| loss: 0.29719 | val_0_rmse: 0.5423  | val_1_rmse: 0.62692 |  0:00:43s
epoch 123| loss: 0.28306 | val_0_rmse: 0.5519  | val_1_rmse: 0.63076 |  0:00:43s
epoch 124| loss: 0.28983 | val_0_rmse: 0.53924 | val_1_rmse: 0.60542 |  0:00:44s
epoch 125| loss: 0.28072 | val_0_rmse: 0.52955 | val_1_rmse: 0.5941  |  0:00:44s
epoch 126| loss: 0.27768 | val_0_rmse: 0.5229  | val_1_rmse: 0.57374 |  0:00:44s
epoch 127| loss: 0.27831 | val_0_rmse: 0.53562 | val_1_rmse: 0.58968 |  0:00:45s
epoch 128| loss: 0.27634 | val_0_rmse: 0.5469  | val_1_rmse: 0.60063 |  0:00:45s
epoch 129| loss: 0.28059 | val_0_rmse: 0.52914 | val_1_rmse: 0.59036 |  0:00:45s
epoch 130| loss: 0.27426 | val_0_rmse: 0.52051 | val_1_rmse: 0.58084 |  0:00:46s
epoch 131| loss: 0.27594 | val_0_rmse: 0.52176 | val_1_rmse: 0.59    |  0:00:46s
epoch 132| loss: 0.26549 | val_0_rmse: 0.50587 | val_1_rmse: 0.57703 |  0:00:46s
epoch 133| loss: 0.26763 | val_0_rmse: 0.54112 | val_1_rmse: 0.61308 |  0:00:47s
epoch 134| loss: 0.2691  | val_0_rmse: 0.52584 | val_1_rmse: 0.59088 |  0:00:47s
epoch 135| loss: 0.27271 | val_0_rmse: 0.52309 | val_1_rmse: 0.59861 |  0:00:47s
epoch 136| loss: 0.26926 | val_0_rmse: 0.49533 | val_1_rmse: 0.57644 |  0:00:48s
epoch 137| loss: 0.2622  | val_0_rmse: 0.51011 | val_1_rmse: 0.58587 |  0:00:48s
epoch 138| loss: 0.26478 | val_0_rmse: 0.49567 | val_1_rmse: 0.57498 |  0:00:48s
epoch 139| loss: 0.26404 | val_0_rmse: 0.49627 | val_1_rmse: 0.58269 |  0:00:49s
epoch 140| loss: 0.25483 | val_0_rmse: 0.4967  | val_1_rmse: 0.58479 |  0:00:49s
epoch 141| loss: 0.25203 | val_0_rmse: 0.48992 | val_1_rmse: 0.59459 |  0:00:50s
epoch 142| loss: 0.25678 | val_0_rmse: 0.50419 | val_1_rmse: 0.58659 |  0:00:50s
epoch 143| loss: 0.25577 | val_0_rmse: 0.48754 | val_1_rmse: 0.58807 |  0:00:50s
epoch 144| loss: 0.24514 | val_0_rmse: 0.48608 | val_1_rmse: 0.5822  |  0:00:51s
epoch 145| loss: 0.2508  | val_0_rmse: 0.48498 | val_1_rmse: 0.5766  |  0:00:51s
epoch 146| loss: 0.24634 | val_0_rmse: 0.47847 | val_1_rmse: 0.58587 |  0:00:51s
epoch 147| loss: 0.24516 | val_0_rmse: 0.48207 | val_1_rmse: 0.56933 |  0:00:52s
epoch 148| loss: 0.24182 | val_0_rmse: 0.48929 | val_1_rmse: 0.59225 |  0:00:52s
epoch 149| loss: 0.24053 | val_0_rmse: 0.4752  | val_1_rmse: 0.58108 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.56933
Best weights from best epoch are automatically used!
ended training at: 16:38:09
Feature importance:
Mean squared error is of 2731160726.2771373
Mean absolute error:34771.90424927048
MAPE:0.31050971380403586
R2 score:0.6718651473347893
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:38:09
epoch 0  | loss: 3.28299 | val_0_rmse: 0.99626 | val_1_rmse: 0.9924  |  0:00:00s
epoch 1  | loss: 1.45727 | val_0_rmse: 0.99205 | val_1_rmse: 0.98829 |  0:00:00s
epoch 2  | loss: 1.07458 | val_0_rmse: 0.86937 | val_1_rmse: 0.88186 |  0:00:01s
epoch 3  | loss: 0.91346 | val_0_rmse: 0.84149 | val_1_rmse: 0.85222 |  0:00:01s
epoch 4  | loss: 0.78217 | val_0_rmse: 0.85422 | val_1_rmse: 0.86756 |  0:00:01s
epoch 5  | loss: 0.66103 | val_0_rmse: 0.80989 | val_1_rmse: 0.85278 |  0:00:02s
epoch 6  | loss: 0.63372 | val_0_rmse: 0.77889 | val_1_rmse: 0.79221 |  0:00:02s
epoch 7  | loss: 0.60708 | val_0_rmse: 0.79239 | val_1_rmse: 0.80179 |  0:00:02s
epoch 8  | loss: 0.6014  | val_0_rmse: 0.77437 | val_1_rmse: 0.80009 |  0:00:03s
epoch 9  | loss: 0.56102 | val_0_rmse: 0.78246 | val_1_rmse: 0.81339 |  0:00:03s
epoch 10 | loss: 0.53404 | val_0_rmse: 0.76008 | val_1_rmse: 0.77444 |  0:00:03s
epoch 11 | loss: 0.50733 | val_0_rmse: 0.73135 | val_1_rmse: 0.75166 |  0:00:04s
epoch 12 | loss: 0.47832 | val_0_rmse: 0.71962 | val_1_rmse: 0.73989 |  0:00:04s
epoch 13 | loss: 0.4582  | val_0_rmse: 0.72921 | val_1_rmse: 0.75295 |  0:00:04s
epoch 14 | loss: 0.45256 | val_0_rmse: 0.68491 | val_1_rmse: 0.71479 |  0:00:05s
epoch 15 | loss: 0.42223 | val_0_rmse: 0.69757 | val_1_rmse: 0.72192 |  0:00:05s
epoch 16 | loss: 0.41744 | val_0_rmse: 0.68545 | val_1_rmse: 0.73507 |  0:00:06s
epoch 17 | loss: 0.40805 | val_0_rmse: 0.70285 | val_1_rmse: 0.74155 |  0:00:06s
epoch 18 | loss: 0.417   | val_0_rmse: 0.66806 | val_1_rmse: 0.71354 |  0:00:06s
epoch 19 | loss: 0.42547 | val_0_rmse: 0.68014 | val_1_rmse: 0.72256 |  0:00:07s
epoch 20 | loss: 0.39976 | val_0_rmse: 0.67568 | val_1_rmse: 0.7281  |  0:00:07s
epoch 21 | loss: 0.40547 | val_0_rmse: 0.67725 | val_1_rmse: 0.712   |  0:00:07s
epoch 22 | loss: 0.39199 | val_0_rmse: 0.68175 | val_1_rmse: 0.71539 |  0:00:08s
epoch 23 | loss: 0.39823 | val_0_rmse: 0.66615 | val_1_rmse: 0.70154 |  0:00:08s
epoch 24 | loss: 0.38804 | val_0_rmse: 0.67141 | val_1_rmse: 0.70896 |  0:00:08s
epoch 25 | loss: 0.38531 | val_0_rmse: 0.66266 | val_1_rmse: 0.70043 |  0:00:09s
epoch 26 | loss: 0.3764  | val_0_rmse: 0.66471 | val_1_rmse: 0.69684 |  0:00:09s
epoch 27 | loss: 0.36299 | val_0_rmse: 0.66218 | val_1_rmse: 0.69335 |  0:00:09s
epoch 28 | loss: 0.35905 | val_0_rmse: 0.65437 | val_1_rmse: 0.68382 |  0:00:10s
epoch 29 | loss: 0.35032 | val_0_rmse: 0.64849 | val_1_rmse: 0.67945 |  0:00:10s
epoch 30 | loss: 0.34508 | val_0_rmse: 0.65396 | val_1_rmse: 0.67934 |  0:00:11s
epoch 31 | loss: 0.3416  | val_0_rmse: 0.64331 | val_1_rmse: 0.67187 |  0:00:11s
epoch 32 | loss: 0.33391 | val_0_rmse: 0.63643 | val_1_rmse: 0.66668 |  0:00:11s
epoch 33 | loss: 0.32382 | val_0_rmse: 0.6422  | val_1_rmse: 0.66715 |  0:00:12s
epoch 34 | loss: 0.32112 | val_0_rmse: 0.63186 | val_1_rmse: 0.66457 |  0:00:12s
epoch 35 | loss: 0.3218  | val_0_rmse: 0.64715 | val_1_rmse: 0.67483 |  0:00:12s
epoch 36 | loss: 0.32042 | val_0_rmse: 0.63022 | val_1_rmse: 0.66674 |  0:00:13s
epoch 37 | loss: 0.31769 | val_0_rmse: 0.62648 | val_1_rmse: 0.65718 |  0:00:13s
epoch 38 | loss: 0.31439 | val_0_rmse: 0.63304 | val_1_rmse: 0.66198 |  0:00:13s
epoch 39 | loss: 0.30471 | val_0_rmse: 0.62423 | val_1_rmse: 0.65537 |  0:00:14s
epoch 40 | loss: 0.30427 | val_0_rmse: 0.61932 | val_1_rmse: 0.65777 |  0:00:14s
epoch 41 | loss: 0.30803 | val_0_rmse: 0.61539 | val_1_rmse: 0.64838 |  0:00:14s
epoch 42 | loss: 0.29953 | val_0_rmse: 0.6156  | val_1_rmse: 0.64486 |  0:00:15s
epoch 43 | loss: 0.30674 | val_0_rmse: 0.62475 | val_1_rmse: 0.64542 |  0:00:15s
epoch 44 | loss: 0.31139 | val_0_rmse: 0.62286 | val_1_rmse: 0.64149 |  0:00:16s
epoch 45 | loss: 0.30455 | val_0_rmse: 0.62058 | val_1_rmse: 0.63725 |  0:00:16s
epoch 46 | loss: 0.30517 | val_0_rmse: 0.62838 | val_1_rmse: 0.64324 |  0:00:16s
epoch 47 | loss: 0.3033  | val_0_rmse: 0.61732 | val_1_rmse: 0.63498 |  0:00:17s
epoch 48 | loss: 0.32455 | val_0_rmse: 0.62934 | val_1_rmse: 0.64085 |  0:00:17s
epoch 49 | loss: 0.31108 | val_0_rmse: 0.63322 | val_1_rmse: 0.6545  |  0:00:17s
epoch 50 | loss: 0.30888 | val_0_rmse: 0.61923 | val_1_rmse: 0.64452 |  0:00:18s
epoch 51 | loss: 0.29859 | val_0_rmse: 0.6411  | val_1_rmse: 0.66934 |  0:00:18s
epoch 52 | loss: 0.29862 | val_0_rmse: 0.62687 | val_1_rmse: 0.65136 |  0:00:18s
epoch 53 | loss: 0.29645 | val_0_rmse: 0.62209 | val_1_rmse: 0.65139 |  0:00:19s
epoch 54 | loss: 0.29438 | val_0_rmse: 0.61473 | val_1_rmse: 0.65262 |  0:00:19s
epoch 55 | loss: 0.29467 | val_0_rmse: 0.61294 | val_1_rmse: 0.6447  |  0:00:19s
epoch 56 | loss: 0.29609 | val_0_rmse: 0.61233 | val_1_rmse: 0.64756 |  0:00:20s
epoch 57 | loss: 0.29449 | val_0_rmse: 0.60783 | val_1_rmse: 0.64592 |  0:00:20s
epoch 58 | loss: 0.29009 | val_0_rmse: 0.60453 | val_1_rmse: 0.65124 |  0:00:20s
epoch 59 | loss: 0.28685 | val_0_rmse: 0.59867 | val_1_rmse: 0.64577 |  0:00:21s
epoch 60 | loss: 0.27899 | val_0_rmse: 0.59301 | val_1_rmse: 0.63888 |  0:00:21s
epoch 61 | loss: 0.29385 | val_0_rmse: 0.60142 | val_1_rmse: 0.64794 |  0:00:22s
epoch 62 | loss: 0.29106 | val_0_rmse: 0.59506 | val_1_rmse: 0.63572 |  0:00:22s
epoch 63 | loss: 0.29222 | val_0_rmse: 0.59856 | val_1_rmse: 0.63401 |  0:00:22s
epoch 64 | loss: 0.2868  | val_0_rmse: 0.5848  | val_1_rmse: 0.62216 |  0:00:23s
epoch 65 | loss: 0.27842 | val_0_rmse: 0.59499 | val_1_rmse: 0.63803 |  0:00:23s
epoch 66 | loss: 0.2797  | val_0_rmse: 0.58115 | val_1_rmse: 0.62084 |  0:00:23s
epoch 67 | loss: 0.28239 | val_0_rmse: 0.58271 | val_1_rmse: 0.63549 |  0:00:24s
epoch 68 | loss: 0.28423 | val_0_rmse: 0.58335 | val_1_rmse: 0.63596 |  0:00:24s
epoch 69 | loss: 0.27501 | val_0_rmse: 0.58879 | val_1_rmse: 0.63414 |  0:00:24s
epoch 70 | loss: 0.27965 | val_0_rmse: 0.58392 | val_1_rmse: 0.63676 |  0:00:25s
epoch 71 | loss: 0.27291 | val_0_rmse: 0.59192 | val_1_rmse: 0.63703 |  0:00:25s
epoch 72 | loss: 0.28405 | val_0_rmse: 0.59136 | val_1_rmse: 0.63074 |  0:00:25s
epoch 73 | loss: 0.28066 | val_0_rmse: 0.5849  | val_1_rmse: 0.62317 |  0:00:26s
epoch 74 | loss: 0.27497 | val_0_rmse: 0.58627 | val_1_rmse: 0.62526 |  0:00:26s
epoch 75 | loss: 0.27525 | val_0_rmse: 0.5976  | val_1_rmse: 0.63952 |  0:00:26s
epoch 76 | loss: 0.27127 | val_0_rmse: 0.58233 | val_1_rmse: 0.62595 |  0:00:27s
epoch 77 | loss: 0.26692 | val_0_rmse: 0.57851 | val_1_rmse: 0.62668 |  0:00:27s
epoch 78 | loss: 0.27243 | val_0_rmse: 0.57242 | val_1_rmse: 0.61831 |  0:00:27s
epoch 79 | loss: 0.26968 | val_0_rmse: 0.56943 | val_1_rmse: 0.61397 |  0:00:28s
epoch 80 | loss: 0.25966 | val_0_rmse: 0.57478 | val_1_rmse: 0.62426 |  0:00:28s
epoch 81 | loss: 0.27327 | val_0_rmse: 0.57167 | val_1_rmse: 0.62149 |  0:00:29s
epoch 82 | loss: 0.26857 | val_0_rmse: 0.5681  | val_1_rmse: 0.62206 |  0:00:29s
epoch 83 | loss: 0.26384 | val_0_rmse: 0.56388 | val_1_rmse: 0.63767 |  0:00:29s
epoch 84 | loss: 0.27456 | val_0_rmse: 0.55024 | val_1_rmse: 0.61788 |  0:00:30s
epoch 85 | loss: 0.2692  | val_0_rmse: 0.5585  | val_1_rmse: 0.62558 |  0:00:30s
epoch 86 | loss: 0.2645  | val_0_rmse: 0.55167 | val_1_rmse: 0.62239 |  0:00:30s
epoch 87 | loss: 0.26287 | val_0_rmse: 0.55664 | val_1_rmse: 0.62817 |  0:00:31s
epoch 88 | loss: 0.26347 | val_0_rmse: 0.55896 | val_1_rmse: 0.62023 |  0:00:31s
epoch 89 | loss: 0.27008 | val_0_rmse: 0.56105 | val_1_rmse: 0.61936 |  0:00:31s
epoch 90 | loss: 0.26558 | val_0_rmse: 0.56271 | val_1_rmse: 0.61406 |  0:00:32s
epoch 91 | loss: 0.27569 | val_0_rmse: 0.5723  | val_1_rmse: 0.62719 |  0:00:32s
epoch 92 | loss: 0.27122 | val_0_rmse: 0.57618 | val_1_rmse: 0.63899 |  0:00:32s
epoch 93 | loss: 0.27213 | val_0_rmse: 0.56664 | val_1_rmse: 0.63438 |  0:00:33s
epoch 94 | loss: 0.26911 | val_0_rmse: 0.55747 | val_1_rmse: 0.63329 |  0:00:33s
epoch 95 | loss: 0.28287 | val_0_rmse: 0.56006 | val_1_rmse: 0.64061 |  0:00:33s
epoch 96 | loss: 0.28505 | val_0_rmse: 0.56829 | val_1_rmse: 0.63759 |  0:00:34s
epoch 97 | loss: 0.27957 | val_0_rmse: 0.56053 | val_1_rmse: 0.63242 |  0:00:34s
epoch 98 | loss: 0.2801  | val_0_rmse: 0.55872 | val_1_rmse: 0.63938 |  0:00:34s
epoch 99 | loss: 0.28043 | val_0_rmse: 0.55519 | val_1_rmse: 0.63328 |  0:00:35s
epoch 100| loss: 0.2744  | val_0_rmse: 0.55442 | val_1_rmse: 0.63836 |  0:00:35s
epoch 101| loss: 0.27349 | val_0_rmse: 0.54676 | val_1_rmse: 0.6262  |  0:00:36s
epoch 102| loss: 0.26747 | val_0_rmse: 0.5463  | val_1_rmse: 0.62444 |  0:00:36s
epoch 103| loss: 0.26535 | val_0_rmse: 0.54964 | val_1_rmse: 0.61751 |  0:00:36s
epoch 104| loss: 0.26388 | val_0_rmse: 0.54115 | val_1_rmse: 0.6139  |  0:00:37s
epoch 105| loss: 0.26222 | val_0_rmse: 0.53312 | val_1_rmse: 0.6118  |  0:00:37s
epoch 106| loss: 0.26204 | val_0_rmse: 0.52848 | val_1_rmse: 0.61331 |  0:00:37s
epoch 107| loss: 0.25825 | val_0_rmse: 0.52944 | val_1_rmse: 0.61241 |  0:00:38s
epoch 108| loss: 0.2607  | val_0_rmse: 0.52118 | val_1_rmse: 0.60797 |  0:00:38s
epoch 109| loss: 0.26004 | val_0_rmse: 0.52672 | val_1_rmse: 0.62011 |  0:00:38s
epoch 110| loss: 0.25428 | val_0_rmse: 0.52363 | val_1_rmse: 0.61067 |  0:00:39s
epoch 111| loss: 0.25351 | val_0_rmse: 0.52187 | val_1_rmse: 0.61824 |  0:00:39s
epoch 112| loss: 0.25573 | val_0_rmse: 0.51838 | val_1_rmse: 0.61788 |  0:00:39s
epoch 113| loss: 0.24697 | val_0_rmse: 0.51567 | val_1_rmse: 0.61085 |  0:00:40s
epoch 114| loss: 0.24622 | val_0_rmse: 0.51809 | val_1_rmse: 0.61568 |  0:00:40s
epoch 115| loss: 0.25051 | val_0_rmse: 0.52588 | val_1_rmse: 0.60403 |  0:00:40s
epoch 116| loss: 0.25498 | val_0_rmse: 0.52826 | val_1_rmse: 0.62256 |  0:00:41s
epoch 117| loss: 0.25751 | val_0_rmse: 0.5257  | val_1_rmse: 0.60995 |  0:00:41s
epoch 118| loss: 0.25323 | val_0_rmse: 0.52977 | val_1_rmse: 0.61953 |  0:00:42s
epoch 119| loss: 0.24741 | val_0_rmse: 0.51611 | val_1_rmse: 0.59973 |  0:00:42s
epoch 120| loss: 0.25006 | val_0_rmse: 0.49985 | val_1_rmse: 0.59013 |  0:00:42s
epoch 121| loss: 0.25414 | val_0_rmse: 0.4961  | val_1_rmse: 0.59603 |  0:00:43s
epoch 122| loss: 0.25845 | val_0_rmse: 0.50403 | val_1_rmse: 0.59969 |  0:00:43s
epoch 123| loss: 0.27037 | val_0_rmse: 0.51442 | val_1_rmse: 0.60759 |  0:00:43s
epoch 124| loss: 0.26908 | val_0_rmse: 0.53167 | val_1_rmse: 0.62064 |  0:00:44s
epoch 125| loss: 0.26844 | val_0_rmse: 0.52583 | val_1_rmse: 0.62998 |  0:00:44s
epoch 126| loss: 0.26642 | val_0_rmse: 0.51963 | val_1_rmse: 0.61556 |  0:00:44s
epoch 127| loss: 0.27186 | val_0_rmse: 0.50635 | val_1_rmse: 0.60304 |  0:00:45s
epoch 128| loss: 0.27236 | val_0_rmse: 0.49959 | val_1_rmse: 0.59459 |  0:00:45s
epoch 129| loss: 0.26182 | val_0_rmse: 0.50427 | val_1_rmse: 0.60788 |  0:00:45s
epoch 130| loss: 0.26322 | val_0_rmse: 0.5181  | val_1_rmse: 0.61037 |  0:00:46s
epoch 131| loss: 0.25752 | val_0_rmse: 0.49726 | val_1_rmse: 0.59937 |  0:00:46s
epoch 132| loss: 0.25499 | val_0_rmse: 0.49239 | val_1_rmse: 0.60795 |  0:00:46s
epoch 133| loss: 0.25435 | val_0_rmse: 0.49046 | val_1_rmse: 0.60778 |  0:00:47s
epoch 134| loss: 0.25522 | val_0_rmse: 0.49229 | val_1_rmse: 0.60915 |  0:00:47s
epoch 135| loss: 0.25454 | val_0_rmse: 0.48731 | val_1_rmse: 0.61166 |  0:00:47s
epoch 136| loss: 0.25032 | val_0_rmse: 0.48571 | val_1_rmse: 0.60646 |  0:00:48s
epoch 137| loss: 0.24587 | val_0_rmse: 0.48274 | val_1_rmse: 0.61437 |  0:00:48s
epoch 138| loss: 0.23855 | val_0_rmse: 0.49223 | val_1_rmse: 0.60643 |  0:00:49s
epoch 139| loss: 0.25624 | val_0_rmse: 0.48426 | val_1_rmse: 0.62069 |  0:00:49s
epoch 140| loss: 0.24876 | val_0_rmse: 0.48343 | val_1_rmse: 0.61928 |  0:00:49s
epoch 141| loss: 0.25403 | val_0_rmse: 0.48654 | val_1_rmse: 0.62003 |  0:00:50s
epoch 142| loss: 0.24404 | val_0_rmse: 0.48295 | val_1_rmse: 0.62088 |  0:00:50s
epoch 143| loss: 0.25601 | val_0_rmse: 0.47824 | val_1_rmse: 0.61966 |  0:00:50s
epoch 144| loss: 0.24239 | val_0_rmse: 0.48236 | val_1_rmse: 0.61935 |  0:00:51s
epoch 145| loss: 0.25192 | val_0_rmse: 0.4822  | val_1_rmse: 0.621   |  0:00:51s
epoch 146| loss: 0.24643 | val_0_rmse: 0.48409 | val_1_rmse: 0.61766 |  0:00:51s
epoch 147| loss: 0.25082 | val_0_rmse: 0.48428 | val_1_rmse: 0.60825 |  0:00:52s
epoch 148| loss: 0.24503 | val_0_rmse: 0.48411 | val_1_rmse: 0.61734 |  0:00:52s
epoch 149| loss: 0.24407 | val_0_rmse: 0.46969 | val_1_rmse: 0.61057 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.59013
Best weights from best epoch are automatically used!
ended training at: 16:39:02
Feature importance:
Mean squared error is of 3230025269.207148
Mean absolute error:38365.27394362797
MAPE:0.3496711265921233
R2 score:0.6420240103979363
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:39:02
epoch 0  | loss: 3.73189 | val_0_rmse: 1.00121 | val_1_rmse: 1.03867 |  0:00:00s
epoch 1  | loss: 1.52519 | val_0_rmse: 0.92875 | val_1_rmse: 0.96812 |  0:00:00s
epoch 2  | loss: 0.95705 | val_0_rmse: 0.84943 | val_1_rmse: 0.86626 |  0:00:01s
epoch 3  | loss: 0.74426 | val_0_rmse: 0.7947  | val_1_rmse: 0.813   |  0:00:01s
epoch 4  | loss: 0.66458 | val_0_rmse: 0.85881 | val_1_rmse: 0.86214 |  0:00:01s
epoch 5  | loss: 0.5593  | val_0_rmse: 0.97294 | val_1_rmse: 0.98622 |  0:00:02s
epoch 6  | loss: 0.5262  | val_0_rmse: 0.87527 | val_1_rmse: 0.87181 |  0:00:02s
epoch 7  | loss: 0.50933 | val_0_rmse: 0.94921 | val_1_rmse: 0.94264 |  0:00:02s
epoch 8  | loss: 0.49411 | val_0_rmse: 0.80004 | val_1_rmse: 0.79804 |  0:00:03s
epoch 9  | loss: 0.46329 | val_0_rmse: 0.8018  | val_1_rmse: 0.79976 |  0:00:03s
epoch 10 | loss: 0.44331 | val_0_rmse: 0.70174 | val_1_rmse: 0.7171  |  0:00:03s
epoch 11 | loss: 0.41594 | val_0_rmse: 0.70298 | val_1_rmse: 0.71982 |  0:00:04s
epoch 12 | loss: 0.39997 | val_0_rmse: 0.70959 | val_1_rmse: 0.71877 |  0:00:04s
epoch 13 | loss: 0.38613 | val_0_rmse: 0.70115 | val_1_rmse: 0.70912 |  0:00:04s
epoch 14 | loss: 0.36432 | val_0_rmse: 0.69199 | val_1_rmse: 0.70664 |  0:00:05s
epoch 15 | loss: 0.35642 | val_0_rmse: 0.71314 | val_1_rmse: 0.72455 |  0:00:05s
epoch 16 | loss: 0.3495  | val_0_rmse: 0.69798 | val_1_rmse: 0.71456 |  0:00:06s
epoch 17 | loss: 0.34373 | val_0_rmse: 0.68794 | val_1_rmse: 0.70521 |  0:00:06s
epoch 18 | loss: 0.33463 | val_0_rmse: 0.74703 | val_1_rmse: 0.7556  |  0:00:06s
epoch 19 | loss: 0.33186 | val_0_rmse: 0.68734 | val_1_rmse: 0.70757 |  0:00:07s
epoch 20 | loss: 0.33435 | val_0_rmse: 0.71457 | val_1_rmse: 0.73033 |  0:00:07s
epoch 21 | loss: 0.32645 | val_0_rmse: 0.70018 | val_1_rmse: 0.71926 |  0:00:07s
epoch 22 | loss: 0.31731 | val_0_rmse: 0.69288 | val_1_rmse: 0.71182 |  0:00:08s
epoch 23 | loss: 0.3185  | val_0_rmse: 0.69975 | val_1_rmse: 0.72104 |  0:00:08s
epoch 24 | loss: 0.31308 | val_0_rmse: 0.70973 | val_1_rmse: 0.73292 |  0:00:08s
epoch 25 | loss: 0.31098 | val_0_rmse: 0.68212 | val_1_rmse: 0.71079 |  0:00:09s
epoch 26 | loss: 0.31038 | val_0_rmse: 0.69114 | val_1_rmse: 0.71368 |  0:00:09s
epoch 27 | loss: 0.30072 | val_0_rmse: 0.70956 | val_1_rmse: 0.72372 |  0:00:09s
epoch 28 | loss: 0.29641 | val_0_rmse: 0.67415 | val_1_rmse: 0.69664 |  0:00:10s
epoch 29 | loss: 0.3062  | val_0_rmse: 0.69272 | val_1_rmse: 0.71805 |  0:00:10s
epoch 30 | loss: 0.29521 | val_0_rmse: 0.67695 | val_1_rmse: 0.70062 |  0:00:11s
epoch 31 | loss: 0.29367 | val_0_rmse: 0.72228 | val_1_rmse: 0.74331 |  0:00:11s
epoch 32 | loss: 0.29995 | val_0_rmse: 0.66769 | val_1_rmse: 0.69687 |  0:00:11s
epoch 33 | loss: 0.29902 | val_0_rmse: 0.68737 | val_1_rmse: 0.70782 |  0:00:12s
epoch 34 | loss: 0.28703 | val_0_rmse: 0.68647 | val_1_rmse: 0.71159 |  0:00:12s
epoch 35 | loss: 0.2872  | val_0_rmse: 0.7001  | val_1_rmse: 0.72631 |  0:00:12s
epoch 36 | loss: 0.28796 | val_0_rmse: 0.67546 | val_1_rmse: 0.7027  |  0:00:13s
epoch 37 | loss: 0.27941 | val_0_rmse: 0.68055 | val_1_rmse: 0.70778 |  0:00:13s
epoch 38 | loss: 0.279   | val_0_rmse: 0.67158 | val_1_rmse: 0.69381 |  0:00:13s
epoch 39 | loss: 0.28801 | val_0_rmse: 0.6615  | val_1_rmse: 0.68579 |  0:00:14s
epoch 40 | loss: 0.28896 | val_0_rmse: 0.69239 | val_1_rmse: 0.71865 |  0:00:14s
epoch 41 | loss: 0.27518 | val_0_rmse: 0.65617 | val_1_rmse: 0.6839  |  0:00:14s
epoch 42 | loss: 0.28099 | val_0_rmse: 0.69173 | val_1_rmse: 0.71884 |  0:00:15s
epoch 43 | loss: 0.27159 | val_0_rmse: 0.66707 | val_1_rmse: 0.69667 |  0:00:15s
epoch 44 | loss: 0.27913 | val_0_rmse: 0.6941  | val_1_rmse: 0.72924 |  0:00:15s
epoch 45 | loss: 0.26843 | val_0_rmse: 0.65044 | val_1_rmse: 0.68902 |  0:00:16s
epoch 46 | loss: 0.27022 | val_0_rmse: 0.66588 | val_1_rmse: 0.70691 |  0:00:16s
epoch 47 | loss: 0.26732 | val_0_rmse: 0.6525  | val_1_rmse: 0.68786 |  0:00:17s
epoch 48 | loss: 0.25846 | val_0_rmse: 0.67853 | val_1_rmse: 0.71844 |  0:00:17s
epoch 49 | loss: 0.25831 | val_0_rmse: 0.64467 | val_1_rmse: 0.69076 |  0:00:17s
epoch 50 | loss: 0.26041 | val_0_rmse: 0.66193 | val_1_rmse: 0.70724 |  0:00:18s
epoch 51 | loss: 0.26163 | val_0_rmse: 0.6441  | val_1_rmse: 0.68732 |  0:00:18s
epoch 52 | loss: 0.25256 | val_0_rmse: 0.68321 | val_1_rmse: 0.73249 |  0:00:18s
epoch 53 | loss: 0.26139 | val_0_rmse: 0.6632  | val_1_rmse: 0.70355 |  0:00:19s
epoch 54 | loss: 0.25608 | val_0_rmse: 0.63061 | val_1_rmse: 0.67258 |  0:00:19s
epoch 55 | loss: 0.25723 | val_0_rmse: 0.65988 | val_1_rmse: 0.71177 |  0:00:19s
epoch 56 | loss: 0.25173 | val_0_rmse: 0.62627 | val_1_rmse: 0.67251 |  0:00:20s
epoch 57 | loss: 0.25336 | val_0_rmse: 0.64435 | val_1_rmse: 0.69232 |  0:00:20s
epoch 58 | loss: 0.25253 | val_0_rmse: 0.62752 | val_1_rmse: 0.676   |  0:00:20s
epoch 59 | loss: 0.24533 | val_0_rmse: 0.62625 | val_1_rmse: 0.68185 |  0:00:21s
epoch 60 | loss: 0.24348 | val_0_rmse: 0.61275 | val_1_rmse: 0.66497 |  0:00:21s
epoch 61 | loss: 0.25203 | val_0_rmse: 0.64259 | val_1_rmse: 0.7008  |  0:00:21s
epoch 62 | loss: 0.26085 | val_0_rmse: 0.60824 | val_1_rmse: 0.66264 |  0:00:22s
epoch 63 | loss: 0.25069 | val_0_rmse: 0.61894 | val_1_rmse: 0.67458 |  0:00:22s
epoch 64 | loss: 0.24737 | val_0_rmse: 0.61235 | val_1_rmse: 0.67286 |  0:00:23s
epoch 65 | loss: 0.2446  | val_0_rmse: 0.60985 | val_1_rmse: 0.66675 |  0:00:23s
epoch 66 | loss: 0.238   | val_0_rmse: 0.61462 | val_1_rmse: 0.67944 |  0:00:23s
epoch 67 | loss: 0.24192 | val_0_rmse: 0.59346 | val_1_rmse: 0.65813 |  0:00:24s
epoch 68 | loss: 0.23276 | val_0_rmse: 0.63663 | val_1_rmse: 0.70028 |  0:00:24s
epoch 69 | loss: 0.23925 | val_0_rmse: 0.59694 | val_1_rmse: 0.66403 |  0:00:24s
epoch 70 | loss: 0.23593 | val_0_rmse: 0.62025 | val_1_rmse: 0.69823 |  0:00:25s
epoch 71 | loss: 0.22559 | val_0_rmse: 0.58949 | val_1_rmse: 0.65672 |  0:00:25s
epoch 72 | loss: 0.22826 | val_0_rmse: 0.58342 | val_1_rmse: 0.64346 |  0:00:25s
epoch 73 | loss: 0.22759 | val_0_rmse: 0.58753 | val_1_rmse: 0.65372 |  0:00:26s
epoch 74 | loss: 0.23509 | val_0_rmse: 0.57431 | val_1_rmse: 0.64498 |  0:00:26s
epoch 75 | loss: 0.22539 | val_0_rmse: 0.5857  | val_1_rmse: 0.66498 |  0:00:26s
epoch 76 | loss: 0.22436 | val_0_rmse: 0.57742 | val_1_rmse: 0.65145 |  0:00:27s
epoch 77 | loss: 0.22101 | val_0_rmse: 0.57209 | val_1_rmse: 0.64583 |  0:00:27s
epoch 78 | loss: 0.22663 | val_0_rmse: 0.57213 | val_1_rmse: 0.65449 |  0:00:28s
epoch 79 | loss: 0.21909 | val_0_rmse: 0.58138 | val_1_rmse: 0.67157 |  0:00:28s
epoch 80 | loss: 0.22435 | val_0_rmse: 0.56541 | val_1_rmse: 0.65152 |  0:00:28s
epoch 81 | loss: 0.21362 | val_0_rmse: 0.57425 | val_1_rmse: 0.66564 |  0:00:29s
epoch 82 | loss: 0.22198 | val_0_rmse: 0.56089 | val_1_rmse: 0.65628 |  0:00:29s
epoch 83 | loss: 0.22334 | val_0_rmse: 0.55848 | val_1_rmse: 0.66029 |  0:00:29s
epoch 84 | loss: 0.23106 | val_0_rmse: 0.56973 | val_1_rmse: 0.68172 |  0:00:30s
epoch 85 | loss: 0.22144 | val_0_rmse: 0.55773 | val_1_rmse: 0.65443 |  0:00:30s
epoch 86 | loss: 0.22155 | val_0_rmse: 0.55127 | val_1_rmse: 0.66995 |  0:00:30s
epoch 87 | loss: 0.2209  | val_0_rmse: 0.54019 | val_1_rmse: 0.65221 |  0:00:31s
epoch 88 | loss: 0.2232  | val_0_rmse: 0.53744 | val_1_rmse: 0.6659  |  0:00:31s
epoch 89 | loss: 0.21675 | val_0_rmse: 0.53851 | val_1_rmse: 0.64032 |  0:00:31s
epoch 90 | loss: 0.2164  | val_0_rmse: 0.55733 | val_1_rmse: 0.68655 |  0:00:32s
epoch 91 | loss: 0.22164 | val_0_rmse: 0.52173 | val_1_rmse: 0.6247  |  0:00:32s
epoch 92 | loss: 0.21947 | val_0_rmse: 0.51874 | val_1_rmse: 0.64183 |  0:00:32s
epoch 93 | loss: 0.20293 | val_0_rmse: 0.51524 | val_1_rmse: 0.63394 |  0:00:33s
epoch 94 | loss: 0.20973 | val_0_rmse: 0.51243 | val_1_rmse: 0.65118 |  0:00:33s
epoch 95 | loss: 0.20313 | val_0_rmse: 0.51931 | val_1_rmse: 0.64066 |  0:00:33s
epoch 96 | loss: 0.20879 | val_0_rmse: 0.51057 | val_1_rmse: 0.66947 |  0:00:34s
epoch 97 | loss: 0.20326 | val_0_rmse: 0.49848 | val_1_rmse: 0.62868 |  0:00:34s
epoch 98 | loss: 0.21386 | val_0_rmse: 0.49513 | val_1_rmse: 0.6392  |  0:00:35s
epoch 99 | loss: 0.20257 | val_0_rmse: 0.49011 | val_1_rmse: 0.63859 |  0:00:35s
epoch 100| loss: 0.20839 | val_0_rmse: 0.49964 | val_1_rmse: 0.63251 |  0:00:35s
epoch 101| loss: 0.20849 | val_0_rmse: 0.51088 | val_1_rmse: 0.65024 |  0:00:36s
epoch 102| loss: 0.21106 | val_0_rmse: 0.48823 | val_1_rmse: 0.62812 |  0:00:36s
epoch 103| loss: 0.20985 | val_0_rmse: 0.4845  | val_1_rmse: 0.62768 |  0:00:36s
epoch 104| loss: 0.20437 | val_0_rmse: 0.48423 | val_1_rmse: 0.61913 |  0:00:37s
epoch 105| loss: 0.20942 | val_0_rmse: 0.48454 | val_1_rmse: 0.63182 |  0:00:37s
epoch 106| loss: 0.21055 | val_0_rmse: 0.47361 | val_1_rmse: 0.60931 |  0:00:37s
epoch 107| loss: 0.20345 | val_0_rmse: 0.48733 | val_1_rmse: 0.60765 |  0:00:38s
epoch 108| loss: 0.19678 | val_0_rmse: 0.46784 | val_1_rmse: 0.60725 |  0:00:38s
epoch 109| loss: 0.20066 | val_0_rmse: 0.47732 | val_1_rmse: 0.60704 |  0:00:38s
epoch 110| loss: 0.1986  | val_0_rmse: 0.49662 | val_1_rmse: 0.68437 |  0:00:39s
epoch 111| loss: 0.20458 | val_0_rmse: 0.47877 | val_1_rmse: 0.6164  |  0:00:39s
epoch 112| loss: 0.19751 | val_0_rmse: 0.46297 | val_1_rmse: 0.63444 |  0:00:39s
epoch 113| loss: 0.20251 | val_0_rmse: 0.45897 | val_1_rmse: 0.61684 |  0:00:40s
epoch 114| loss: 0.19642 | val_0_rmse: 0.45921 | val_1_rmse: 0.64388 |  0:00:40s
epoch 115| loss: 0.19673 | val_0_rmse: 0.46276 | val_1_rmse: 0.63511 |  0:00:41s
epoch 116| loss: 0.20976 | val_0_rmse: 0.4719  | val_1_rmse: 0.65303 |  0:00:41s
epoch 117| loss: 0.19888 | val_0_rmse: 0.4596  | val_1_rmse: 0.6162  |  0:00:41s
epoch 118| loss: 0.20017 | val_0_rmse: 0.4563  | val_1_rmse: 0.62005 |  0:00:42s
epoch 119| loss: 0.19823 | val_0_rmse: 0.46251 | val_1_rmse: 0.62197 |  0:00:42s
epoch 120| loss: 0.19606 | val_0_rmse: 0.47873 | val_1_rmse: 0.61663 |  0:00:42s
epoch 121| loss: 0.19656 | val_0_rmse: 0.43933 | val_1_rmse: 0.63711 |  0:00:43s
epoch 122| loss: 0.19906 | val_0_rmse: 0.44092 | val_1_rmse: 0.60659 |  0:00:43s
epoch 123| loss: 0.19736 | val_0_rmse: 0.44141 | val_1_rmse: 0.62061 |  0:00:43s
epoch 124| loss: 0.20391 | val_0_rmse: 0.44455 | val_1_rmse: 0.62763 |  0:00:44s
epoch 125| loss: 0.19785 | val_0_rmse: 0.45068 | val_1_rmse: 0.62308 |  0:00:44s
epoch 126| loss: 0.19978 | val_0_rmse: 0.43816 | val_1_rmse: 0.62008 |  0:00:44s
epoch 127| loss: 0.20068 | val_0_rmse: 0.42671 | val_1_rmse: 0.62008 |  0:00:45s
epoch 128| loss: 0.19423 | val_0_rmse: 0.441   | val_1_rmse: 0.61922 |  0:00:45s
epoch 129| loss: 0.19607 | val_0_rmse: 0.43683 | val_1_rmse: 0.66009 |  0:00:45s
epoch 130| loss: 0.19612 | val_0_rmse: 0.43241 | val_1_rmse: 0.61003 |  0:00:46s
epoch 131| loss: 0.19256 | val_0_rmse: 0.43602 | val_1_rmse: 0.65189 |  0:00:46s
epoch 132| loss: 0.18725 | val_0_rmse: 0.41899 | val_1_rmse: 0.61845 |  0:00:46s
epoch 133| loss: 0.18376 | val_0_rmse: 0.41964 | val_1_rmse: 0.62817 |  0:00:47s
epoch 134| loss: 0.17981 | val_0_rmse: 0.41644 | val_1_rmse: 0.61884 |  0:00:47s
epoch 135| loss: 0.17839 | val_0_rmse: 0.41881 | val_1_rmse: 0.62195 |  0:00:48s
epoch 136| loss: 0.19354 | val_0_rmse: 0.42375 | val_1_rmse: 0.61146 |  0:00:48s
epoch 137| loss: 0.19329 | val_0_rmse: 0.41857 | val_1_rmse: 0.61924 |  0:00:48s
epoch 138| loss: 0.19952 | val_0_rmse: 0.44306 | val_1_rmse: 0.61274 |  0:00:49s
epoch 139| loss: 0.19743 | val_0_rmse: 0.42267 | val_1_rmse: 0.63893 |  0:00:49s
epoch 140| loss: 0.19355 | val_0_rmse: 0.4395  | val_1_rmse: 0.62011 |  0:00:49s
epoch 141| loss: 0.19868 | val_0_rmse: 0.4183  | val_1_rmse: 0.62317 |  0:00:50s
epoch 142| loss: 0.18698 | val_0_rmse: 0.41462 | val_1_rmse: 0.62575 |  0:00:50s
epoch 143| loss: 0.18792 | val_0_rmse: 0.41559 | val_1_rmse: 0.62987 |  0:00:50s
epoch 144| loss: 0.18944 | val_0_rmse: 0.40983 | val_1_rmse: 0.61638 |  0:00:51s
epoch 145| loss: 0.18703 | val_0_rmse: 0.41242 | val_1_rmse: 0.62149 |  0:00:51s
epoch 146| loss: 0.17869 | val_0_rmse: 0.40823 | val_1_rmse: 0.64331 |  0:00:51s
epoch 147| loss: 0.18944 | val_0_rmse: 0.42759 | val_1_rmse: 0.6102  |  0:00:52s
epoch 148| loss: 0.20385 | val_0_rmse: 0.41134 | val_1_rmse: 0.63778 |  0:00:52s
epoch 149| loss: 0.18976 | val_0_rmse: 0.40781 | val_1_rmse: 0.60061 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.60061
Best weights from best epoch are automatically used!
ended training at: 16:39:55
Feature importance:
Mean squared error is of 3369112902.3225293
Mean absolute error:37639.143446439935
MAPE:0.332041202218952
R2 score:0.6250814551157016
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
erro no dataset: all_datasets
