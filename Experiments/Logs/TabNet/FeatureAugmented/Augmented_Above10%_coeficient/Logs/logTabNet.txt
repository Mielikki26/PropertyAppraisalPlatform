TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:11:09
epoch 0  | loss: 1.46025 | val_0_rmse: 1.00379 | val_1_rmse: 0.91665 |  0:00:03s
epoch 1  | loss: 1.01247 | val_0_rmse: 1.00196 | val_1_rmse: 0.9118  |  0:00:05s
epoch 2  | loss: 1.00715 | val_0_rmse: 1.00198 | val_1_rmse: 0.91173 |  0:00:07s
epoch 3  | loss: 1.00637 | val_0_rmse: 1.002   | val_1_rmse: 0.9119  |  0:00:09s
epoch 4  | loss: 1.00691 | val_0_rmse: 1.00192 | val_1_rmse: 0.91175 |  0:00:11s
epoch 5  | loss: 1.00507 | val_0_rmse: 1.00196 | val_1_rmse: 0.91185 |  0:00:12s
epoch 6  | loss: 1.00474 | val_0_rmse: 1.00194 | val_1_rmse: 0.91187 |  0:00:14s
epoch 7  | loss: 1.00459 | val_0_rmse: 1.00197 | val_1_rmse: 0.91183 |  0:00:16s
epoch 8  | loss: 1.00448 | val_0_rmse: 1.00194 | val_1_rmse: 0.91181 |  0:00:18s
epoch 9  | loss: 1.00443 | val_0_rmse: 1.00192 | val_1_rmse: 0.91176 |  0:00:20s
epoch 10 | loss: 1.00468 | val_0_rmse: 1.00198 | val_1_rmse: 0.91189 |  0:00:21s
epoch 11 | loss: 1.00398 | val_0_rmse: 1.00185 | val_1_rmse: 0.91179 |  0:00:23s
epoch 12 | loss: 1.0033  | val_0_rmse: 1.00187 | val_1_rmse: 0.91186 |  0:00:25s
epoch 13 | loss: 1.00352 | val_0_rmse: 1.00181 | val_1_rmse: 0.91173 |  0:00:27s
epoch 14 | loss: 1.00401 | val_0_rmse: 1.00082 | val_1_rmse: 0.91098 |  0:00:29s
epoch 15 | loss: 1.00133 | val_0_rmse: 0.9941  | val_1_rmse: 0.9062  |  0:00:30s
epoch 16 | loss: 0.98312 | val_0_rmse: 0.96792 | val_1_rmse: 0.88806 |  0:00:32s
epoch 17 | loss: 0.97143 | val_0_rmse: 0.9619  | val_1_rmse: 0.8795  |  0:00:34s
epoch 18 | loss: 0.94549 | val_0_rmse: 0.9747  | val_1_rmse: 0.88555 |  0:00:36s
epoch 19 | loss: 0.91072 | val_0_rmse: 0.93575 | val_1_rmse: 0.85082 |  0:00:38s
epoch 20 | loss: 0.88848 | val_0_rmse: 0.93858 | val_1_rmse: 0.8632  |  0:00:39s
epoch 21 | loss: 0.88562 | val_0_rmse: 0.93143 | val_1_rmse: 0.84977 |  0:00:41s
epoch 22 | loss: 0.86273 | val_0_rmse: 0.92251 | val_1_rmse: 0.84056 |  0:00:43s
epoch 23 | loss: 0.85667 | val_0_rmse: 0.91855 | val_1_rmse: 0.82676 |  0:00:45s
epoch 24 | loss: 0.85197 | val_0_rmse: 0.91462 | val_1_rmse: 0.82609 |  0:00:47s
epoch 25 | loss: 0.84331 | val_0_rmse: 0.91342 | val_1_rmse: 0.82534 |  0:00:48s
epoch 26 | loss: 0.84496 | val_0_rmse: 0.91508 | val_1_rmse: 0.82454 |  0:00:50s
epoch 27 | loss: 0.84211 | val_0_rmse: 0.91503 | val_1_rmse: 0.83293 |  0:00:52s
epoch 28 | loss: 0.83967 | val_0_rmse: 0.91832 | val_1_rmse: 0.82907 |  0:00:54s
epoch 29 | loss: 0.84138 | val_0_rmse: 0.91239 | val_1_rmse: 0.82765 |  0:00:56s
epoch 30 | loss: 0.83385 | val_0_rmse: 0.90774 | val_1_rmse: 0.82558 |  0:00:57s
epoch 31 | loss: 0.83083 | val_0_rmse: 0.91088 | val_1_rmse: 0.82624 |  0:00:59s
epoch 32 | loss: 0.83023 | val_0_rmse: 0.90931 | val_1_rmse: 0.82449 |  0:01:01s
epoch 33 | loss: 0.82864 | val_0_rmse: 0.90908 | val_1_rmse: 0.82382 |  0:01:03s
epoch 34 | loss: 0.82838 | val_0_rmse: 0.9057  | val_1_rmse: 0.82557 |  0:01:05s
epoch 35 | loss: 0.82812 | val_0_rmse: 0.90853 | val_1_rmse: 0.82538 |  0:01:06s
epoch 36 | loss: 0.82381 | val_0_rmse: 0.90896 | val_1_rmse: 0.82566 |  0:01:08s
epoch 37 | loss: 0.83014 | val_0_rmse: 0.90757 | val_1_rmse: 0.82504 |  0:01:10s
epoch 38 | loss: 0.82854 | val_0_rmse: 0.90509 | val_1_rmse: 0.82331 |  0:01:12s
epoch 39 | loss: 0.81951 | val_0_rmse: 0.90217 | val_1_rmse: 0.82226 |  0:01:13s
epoch 40 | loss: 0.82309 | val_0_rmse: 0.90411 | val_1_rmse: 0.82537 |  0:01:15s
epoch 41 | loss: 0.8259  | val_0_rmse: 0.90633 | val_1_rmse: 0.82819 |  0:01:17s
epoch 42 | loss: 0.8281  | val_0_rmse: 0.90716 | val_1_rmse: 0.83493 |  0:01:19s
epoch 43 | loss: 0.82589 | val_0_rmse: 0.91815 | val_1_rmse: 0.83908 |  0:01:21s
epoch 44 | loss: 0.83236 | val_0_rmse: 0.9093  | val_1_rmse: 0.83542 |  0:01:22s
epoch 45 | loss: 0.83165 | val_0_rmse: 0.90834 | val_1_rmse: 0.8335  |  0:01:24s
epoch 46 | loss: 0.81513 | val_0_rmse: 0.92013 | val_1_rmse: 0.84829 |  0:01:26s
epoch 47 | loss: 0.80711 | val_0_rmse: 0.89176 | val_1_rmse: 0.82334 |  0:01:28s
epoch 48 | loss: 0.79647 | val_0_rmse: 0.93323 | val_1_rmse: 0.88868 |  0:01:30s
epoch 49 | loss: 0.79761 | val_0_rmse: 0.89264 | val_1_rmse: 0.82157 |  0:01:31s
epoch 50 | loss: 0.78478 | val_0_rmse: 0.87594 | val_1_rmse: 0.8174  |  0:01:33s
epoch 51 | loss: 0.78312 | val_0_rmse: 0.88176 | val_1_rmse: 0.82888 |  0:01:35s
epoch 52 | loss: 0.77997 | val_0_rmse: 0.89305 | val_1_rmse: 0.82095 |  0:01:37s
epoch 53 | loss: 0.78365 | val_0_rmse: 0.89146 | val_1_rmse: 0.82599 |  0:01:39s
epoch 54 | loss: 0.77595 | val_0_rmse: 0.88356 | val_1_rmse: 0.82107 |  0:01:40s
epoch 55 | loss: 0.77485 | val_0_rmse: 0.86861 | val_1_rmse: 0.81534 |  0:01:42s
epoch 56 | loss: 0.77153 | val_0_rmse: 0.87299 | val_1_rmse: 0.81714 |  0:01:44s
epoch 57 | loss: 0.77012 | val_0_rmse: 0.8973  | val_1_rmse: 0.88447 |  0:01:46s
epoch 58 | loss: 0.76941 | val_0_rmse: 1.07852 | val_1_rmse: 1.25148 |  0:01:48s
epoch 59 | loss: 0.7634  | val_0_rmse: 0.8779  | val_1_rmse: 0.84233 |  0:01:49s
epoch 60 | loss: 0.78782 | val_0_rmse: 0.8853  | val_1_rmse: 0.85348 |  0:01:51s
epoch 61 | loss: 0.80021 | val_0_rmse: 0.96661 | val_1_rmse: 0.92519 |  0:01:53s
epoch 62 | loss: 0.79357 | val_0_rmse: 0.87771 | val_1_rmse: 0.81616 |  0:01:55s
epoch 63 | loss: 0.76724 | val_0_rmse: 0.87182 | val_1_rmse: 0.82837 |  0:01:56s
epoch 64 | loss: 0.7761  | val_0_rmse: 0.89277 | val_1_rmse: 0.83345 |  0:01:58s
epoch 65 | loss: 0.77694 | val_0_rmse: 0.88167 | val_1_rmse: 0.82447 |  0:02:00s
epoch 66 | loss: 0.7789  | val_0_rmse: 0.87742 | val_1_rmse: 0.81179 |  0:02:02s
epoch 67 | loss: 0.77315 | val_0_rmse: 0.87564 | val_1_rmse: 0.82554 |  0:02:04s
epoch 68 | loss: 0.76977 | val_0_rmse: 0.88661 | val_1_rmse: 0.83508 |  0:02:05s
epoch 69 | loss: 0.77172 | val_0_rmse: 0.8881  | val_1_rmse: 0.84361 |  0:02:07s
epoch 70 | loss: 0.77183 | val_0_rmse: 0.88288 | val_1_rmse: 0.82882 |  0:02:09s
epoch 71 | loss: 0.76755 | val_0_rmse: 0.87694 | val_1_rmse: 0.82647 |  0:02:11s
epoch 72 | loss: 0.76599 | val_0_rmse: 0.86856 | val_1_rmse: 0.81649 |  0:02:13s
epoch 73 | loss: 0.75784 | val_0_rmse: 0.96081 | val_1_rmse: 0.95521 |  0:02:14s
epoch 74 | loss: 0.76162 | val_0_rmse: 0.89302 | val_1_rmse: 0.86596 |  0:02:16s
epoch 75 | loss: 0.74921 | val_0_rmse: 0.86444 | val_1_rmse: 0.8448  |  0:02:18s
epoch 76 | loss: 0.73826 | val_0_rmse: 0.8493  | val_1_rmse: 0.81865 |  0:02:20s
epoch 77 | loss: 0.74029 | val_0_rmse: 0.84182 | val_1_rmse: 0.81315 |  0:02:22s
epoch 78 | loss: 0.72141 | val_0_rmse: 0.83667 | val_1_rmse: 0.79989 |  0:02:23s
epoch 79 | loss: 0.71849 | val_0_rmse: 0.82384 | val_1_rmse: 0.80918 |  0:02:25s
epoch 80 | loss: 0.71472 | val_0_rmse: 0.84485 | val_1_rmse: 0.82808 |  0:02:27s
epoch 81 | loss: 0.70823 | val_0_rmse: 0.94931 | val_1_rmse: 0.94284 |  0:02:29s
epoch 82 | loss: 0.71595 | val_0_rmse: 0.83724 | val_1_rmse: 0.80443 |  0:02:31s
epoch 83 | loss: 0.70211 | val_0_rmse: 0.86922 | val_1_rmse: 0.87061 |  0:02:32s
epoch 84 | loss: 0.69717 | val_0_rmse: 0.82028 | val_1_rmse: 0.80982 |  0:02:34s
epoch 85 | loss: 0.72864 | val_0_rmse: 0.84194 | val_1_rmse: 0.80783 |  0:02:36s
epoch 86 | loss: 0.71752 | val_0_rmse: 0.82572 | val_1_rmse: 0.80343 |  0:02:38s
epoch 87 | loss: 0.7038  | val_0_rmse: 0.8326  | val_1_rmse: 0.80139 |  0:02:40s
epoch 88 | loss: 0.68908 | val_0_rmse: 0.91898 | val_1_rmse: 0.95421 |  0:02:41s
epoch 89 | loss: 0.69486 | val_0_rmse: 0.81604 | val_1_rmse: 0.81741 |  0:02:43s
epoch 90 | loss: 0.6898  | val_0_rmse: 0.83292 | val_1_rmse: 0.81089 |  0:02:45s
epoch 91 | loss: 0.69763 | val_0_rmse: 0.83576 | val_1_rmse: 0.81032 |  0:02:47s
epoch 92 | loss: 0.66374 | val_0_rmse: 0.80925 | val_1_rmse: 0.81042 |  0:02:49s
epoch 93 | loss: 0.6795  | val_0_rmse: 0.86505 | val_1_rmse: 0.91654 |  0:02:50s
epoch 94 | loss: 0.65565 | val_0_rmse: 0.81663 | val_1_rmse: 0.85861 |  0:02:52s
epoch 95 | loss: 0.6573  | val_0_rmse: 1.56267 | val_1_rmse: 1.62402 |  0:02:54s
epoch 96 | loss: 0.68818 | val_0_rmse: 0.83402 | val_1_rmse: 0.8184  |  0:02:56s
epoch 97 | loss: 0.66141 | val_0_rmse: 0.80649 | val_1_rmse: 0.81824 |  0:02:57s
epoch 98 | loss: 0.64296 | val_0_rmse: 0.90315 | val_1_rmse: 0.97759 |  0:02:59s
epoch 99 | loss: 0.65368 | val_0_rmse: 0.78857 | val_1_rmse: 0.80585 |  0:03:01s
epoch 100| loss: 0.63364 | val_0_rmse: 0.87009 | val_1_rmse: 0.81079 |  0:03:03s
epoch 101| loss: 0.6148  | val_0_rmse: 0.78757 | val_1_rmse: 0.83138 |  0:03:05s
epoch 102| loss: 0.61838 | val_0_rmse: 0.83233 | val_1_rmse: 0.90578 |  0:03:06s
epoch 103| loss: 0.64464 | val_0_rmse: 0.83742 | val_1_rmse: 0.90099 |  0:03:08s
epoch 104| loss: 0.63386 | val_0_rmse: 0.85451 | val_1_rmse: 0.79738 |  0:03:10s
epoch 105| loss: 0.63758 | val_0_rmse: 0.85136 | val_1_rmse: 0.79577 |  0:03:12s
epoch 106| loss: 0.62286 | val_0_rmse: 0.81433 | val_1_rmse: 0.89122 |  0:03:14s
epoch 107| loss: 0.60446 | val_0_rmse: 0.82226 | val_1_rmse: 0.93669 |  0:03:15s
epoch 108| loss: 0.62382 | val_0_rmse: 1.09739 | val_1_rmse: 1.14867 |  0:03:17s
epoch 109| loss: 0.6426  | val_0_rmse: 0.79335 | val_1_rmse: 0.81412 |  0:03:19s
epoch 110| loss: 0.62185 | val_0_rmse: 0.76745 | val_1_rmse: 0.79786 |  0:03:21s
epoch 111| loss: 0.62424 | val_0_rmse: 0.79558 | val_1_rmse: 0.79235 |  0:03:23s
epoch 112| loss: 0.62159 | val_0_rmse: 0.95223 | val_1_rmse: 1.06528 |  0:03:24s
epoch 113| loss: 0.61633 | val_0_rmse: 0.79099 | val_1_rmse: 0.82868 |  0:03:26s
epoch 114| loss: 0.63174 | val_0_rmse: 0.87096 | val_1_rmse: 0.95247 |  0:03:28s
epoch 115| loss: 0.61034 | val_0_rmse: 0.76889 | val_1_rmse: 0.82255 |  0:03:30s
epoch 116| loss: 0.61373 | val_0_rmse: 0.7506  | val_1_rmse: 0.81723 |  0:03:32s
epoch 117| loss: 0.59344 | val_0_rmse: 0.75434 | val_1_rmse: 0.8009  |  0:03:33s
epoch 118| loss: 0.57739 | val_0_rmse: 0.73999 | val_1_rmse: 0.78907 |  0:03:35s
epoch 119| loss: 0.56752 | val_0_rmse: 0.76202 | val_1_rmse: 0.82769 |  0:03:37s
epoch 120| loss: 0.57152 | val_0_rmse: 0.84114 | val_1_rmse: 0.89333 |  0:03:39s
epoch 121| loss: 0.57947 | val_0_rmse: 0.75907 | val_1_rmse: 0.80439 |  0:03:41s
epoch 122| loss: 0.5849  | val_0_rmse: 0.80301 | val_1_rmse: 0.83953 |  0:03:42s
epoch 123| loss: 0.58475 | val_0_rmse: 0.80421 | val_1_rmse: 0.8133  |  0:03:44s
epoch 124| loss: 0.5737  | val_0_rmse: 0.75328 | val_1_rmse: 0.8003  |  0:03:46s
epoch 125| loss: 0.57229 | val_0_rmse: 0.95311 | val_1_rmse: 1.03776 |  0:03:48s
epoch 126| loss: 0.57257 | val_0_rmse: 0.84947 | val_1_rmse: 0.93184 |  0:03:49s
epoch 127| loss: 0.56116 | val_0_rmse: 0.76632 | val_1_rmse: 0.84506 |  0:03:51s
epoch 128| loss: 0.55268 | val_0_rmse: 0.72479 | val_1_rmse: 0.80766 |  0:03:53s
epoch 129| loss: 0.53829 | val_0_rmse: 0.77295 | val_1_rmse: 0.85703 |  0:03:55s
epoch 130| loss: 0.55832 | val_0_rmse: 0.86355 | val_1_rmse: 0.91315 |  0:03:57s
epoch 131| loss: 0.54274 | val_0_rmse: 0.71637 | val_1_rmse: 0.81395 |  0:03:59s
epoch 132| loss: 0.53671 | val_0_rmse: 0.73679 | val_1_rmse: 0.8002  |  0:04:00s
epoch 133| loss: 0.54513 | val_0_rmse: 0.80777 | val_1_rmse: 0.83482 |  0:04:02s
epoch 134| loss: 0.5624  | val_0_rmse: 0.81255 | val_1_rmse: 0.93919 |  0:04:04s
epoch 135| loss: 0.55305 | val_0_rmse: 0.97518 | val_1_rmse: 1.09317 |  0:04:06s
epoch 136| loss: 0.54194 | val_0_rmse: 0.75783 | val_1_rmse: 0.81756 |  0:04:08s
epoch 137| loss: 0.52855 | val_0_rmse: 0.74565 | val_1_rmse: 0.86971 |  0:04:09s
epoch 138| loss: 0.53386 | val_0_rmse: 0.7725  | val_1_rmse: 0.84557 |  0:04:11s
epoch 139| loss: 0.51934 | val_0_rmse: 0.81508 | val_1_rmse: 0.83467 |  0:04:13s
epoch 140| loss: 0.51752 | val_0_rmse: 0.73579 | val_1_rmse: 0.81519 |  0:04:15s
epoch 141| loss: 0.52448 | val_0_rmse: 0.73219 | val_1_rmse: 0.83284 |  0:04:17s
epoch 142| loss: 0.5043  | val_0_rmse: 0.75583 | val_1_rmse: 0.83736 |  0:04:18s
epoch 143| loss: 0.50881 | val_0_rmse: 0.70045 | val_1_rmse: 0.83796 |  0:04:20s
epoch 144| loss: 0.49834 | val_0_rmse: 0.75059 | val_1_rmse: 0.89426 |  0:04:22s
epoch 145| loss: 0.50367 | val_0_rmse: 0.78333 | val_1_rmse: 0.91342 |  0:04:24s
epoch 146| loss: 0.49318 | val_0_rmse: 0.74226 | val_1_rmse: 0.86501 |  0:04:26s
epoch 147| loss: 0.49629 | val_0_rmse: 0.77273 | val_1_rmse: 0.8689  |  0:04:27s
epoch 148| loss: 0.49739 | val_0_rmse: 0.77159 | val_1_rmse: 0.9082  |  0:04:29s

Early stopping occured at epoch 148 with best_epoch = 118 and best_val_1_rmse = 0.78907
Best weights from best epoch are automatically used!
ended training at: 05:15:39
Feature importance:
Mean squared error is of 0.0664408006593475
Mean absolute error:0.15453414139702984
MAPE:0.16806189733817903
R2 score:0.15813431766252317
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:15:42
epoch 0  | loss: 1.22004 | val_0_rmse: 1.00765 | val_1_rmse: 0.99674 |  0:00:06s
epoch 1  | loss: 1.01807 | val_0_rmse: 1.00688 | val_1_rmse: 0.99563 |  0:00:13s
epoch 2  | loss: 1.01565 | val_0_rmse: 1.00824 | val_1_rmse: 0.99686 |  0:00:20s
epoch 3  | loss: 1.01544 | val_0_rmse: 1.00569 | val_1_rmse: 0.99421 |  0:00:27s
epoch 4  | loss: 1.00385 | val_0_rmse: 0.97718 | val_1_rmse: 0.96807 |  0:00:34s
epoch 5  | loss: 0.94977 | val_0_rmse: 0.9669  | val_1_rmse: 0.9575  |  0:00:41s
epoch 6  | loss: 0.89911 | val_0_rmse: 0.94483 | val_1_rmse: 0.93686 |  0:00:48s
epoch 7  | loss: 0.87632 | val_0_rmse: 0.94123 | val_1_rmse: 0.9363  |  0:00:55s
epoch 8  | loss: 0.8629  | val_0_rmse: 0.92914 | val_1_rmse: 0.92825 |  0:01:02s
epoch 9  | loss: 0.84475 | val_0_rmse: 0.91794 | val_1_rmse: 0.9234  |  0:01:09s
epoch 10 | loss: 0.8356  | val_0_rmse: 0.91177 | val_1_rmse: 0.92101 |  0:01:16s
epoch 11 | loss: 0.82908 | val_0_rmse: 0.9282  | val_1_rmse: 0.93454 |  0:01:22s
epoch 12 | loss: 0.82699 | val_0_rmse: 0.90237 | val_1_rmse: 0.92174 |  0:01:29s
epoch 13 | loss: 0.81545 | val_0_rmse: 0.89925 | val_1_rmse: 0.91907 |  0:01:36s
epoch 14 | loss: 0.80508 | val_0_rmse: 0.8906  | val_1_rmse: 0.92827 |  0:01:43s
epoch 15 | loss: 0.79795 | val_0_rmse: 0.90022 | val_1_rmse: 0.93558 |  0:01:50s
epoch 16 | loss: 0.79011 | val_0_rmse: 0.88751 | val_1_rmse: 0.9215  |  0:01:57s
epoch 17 | loss: 0.78385 | val_0_rmse: 0.88356 | val_1_rmse: 0.9184  |  0:02:04s
epoch 18 | loss: 0.77424 | val_0_rmse: 0.93549 | val_1_rmse: 0.95404 |  0:02:11s
epoch 19 | loss: 0.76909 | val_0_rmse: 0.88264 | val_1_rmse: 0.91417 |  0:02:18s
epoch 20 | loss: 0.76098 | val_0_rmse: 0.88008 | val_1_rmse: 0.92687 |  0:02:24s
epoch 21 | loss: 0.76046 | val_0_rmse: 1.14726 | val_1_rmse: 0.94094 |  0:02:31s
epoch 22 | loss: 0.75887 | val_0_rmse: 0.87123 | val_1_rmse: 0.93005 |  0:02:38s
epoch 23 | loss: 0.75204 | val_0_rmse: 0.88105 | val_1_rmse: 0.92896 |  0:02:45s
epoch 24 | loss: 0.74667 | val_0_rmse: 0.86361 | val_1_rmse: 0.92721 |  0:02:52s
epoch 25 | loss: 0.74382 | val_0_rmse: 0.85991 | val_1_rmse: 0.92665 |  0:02:59s
epoch 26 | loss: 0.73452 | val_0_rmse: 0.85597 | val_1_rmse: 0.93265 |  0:03:06s
epoch 27 | loss: 0.73078 | val_0_rmse: 0.85688 | val_1_rmse: 0.93598 |  0:03:13s
epoch 28 | loss: 0.72588 | val_0_rmse: 0.84589 | val_1_rmse: 0.92452 |  0:03:20s
epoch 29 | loss: 0.72449 | val_0_rmse: 0.84501 | val_1_rmse: 0.93539 |  0:03:26s
epoch 30 | loss: 0.7291  | val_0_rmse: 0.94476 | val_1_rmse: 0.98413 |  0:03:33s
epoch 31 | loss: 0.72098 | val_0_rmse: 0.86335 | val_1_rmse: 0.91345 |  0:03:40s
epoch 32 | loss: 0.71137 | val_0_rmse: 0.91378 | val_1_rmse: 0.99796 |  0:03:47s
epoch 33 | loss: 0.71498 | val_0_rmse: 0.85986 | val_1_rmse: 0.91889 |  0:03:54s
epoch 34 | loss: 0.70602 | val_0_rmse: 0.84965 | val_1_rmse: 0.95812 |  0:04:01s
epoch 35 | loss: 0.70156 | val_0_rmse: 0.85183 | val_1_rmse: 0.93012 |  0:04:08s
epoch 36 | loss: 0.70164 | val_0_rmse: 0.86574 | val_1_rmse: 0.98647 |  0:04:15s
epoch 37 | loss: 0.69326 | val_0_rmse: 0.82847 | val_1_rmse: 0.92617 |  0:04:22s
epoch 38 | loss: 0.6912  | val_0_rmse: 0.83123 | val_1_rmse: 0.92079 |  0:04:29s
epoch 39 | loss: 0.69018 | val_0_rmse: 0.82735 | val_1_rmse: 0.93572 |  0:04:35s
epoch 40 | loss: 0.69045 | val_0_rmse: 0.83387 | val_1_rmse: 0.94264 |  0:04:42s
epoch 41 | loss: 0.68344 | val_0_rmse: 0.82735 | val_1_rmse: 0.92908 |  0:04:49s
epoch 42 | loss: 0.68218 | val_0_rmse: 0.85413 | val_1_rmse: 0.97649 |  0:04:56s
epoch 43 | loss: 0.68225 | val_0_rmse: 0.84115 | val_1_rmse: 0.92177 |  0:05:03s
epoch 44 | loss: 0.67191 | val_0_rmse: 0.84088 | val_1_rmse: 0.98312 |  0:05:10s
epoch 45 | loss: 0.67553 | val_0_rmse: 0.84486 | val_1_rmse: 0.94024 |  0:05:17s
epoch 46 | loss: 0.6712  | val_0_rmse: 0.87083 | val_1_rmse: 0.92266 |  0:05:24s
epoch 47 | loss: 0.66817 | val_0_rmse: 0.86407 | val_1_rmse: 1.05288 |  0:05:31s
epoch 48 | loss: 0.6665  | val_0_rmse: 0.8309  | val_1_rmse: 0.96369 |  0:05:38s
epoch 49 | loss: 0.66913 | val_0_rmse: 0.81597 | val_1_rmse: 0.94359 |  0:05:44s
epoch 50 | loss: 0.66523 | val_0_rmse: 0.82407 | val_1_rmse: 0.96646 |  0:05:51s
epoch 51 | loss: 0.66001 | val_0_rmse: 0.81958 | val_1_rmse: 0.95587 |  0:05:58s
epoch 52 | loss: 0.65496 | val_0_rmse: 0.83562 | val_1_rmse: 0.98047 |  0:06:05s
epoch 53 | loss: 0.65845 | val_0_rmse: 0.81523 | val_1_rmse: 0.96147 |  0:06:12s
epoch 54 | loss: 0.65282 | val_0_rmse: 0.818   | val_1_rmse: 0.93863 |  0:06:19s
epoch 55 | loss: 0.64855 | val_0_rmse: 0.8192  | val_1_rmse: 0.9732  |  0:06:26s
epoch 56 | loss: 0.64533 | val_0_rmse: 0.82533 | val_1_rmse: 0.96033 |  0:06:33s
epoch 57 | loss: 0.64746 | val_0_rmse: 0.84057 | val_1_rmse: 0.97885 |  0:06:40s
epoch 58 | loss: 0.64466 | val_0_rmse: 0.81982 | val_1_rmse: 0.96373 |  0:06:47s
epoch 59 | loss: 0.64424 | val_0_rmse: 0.85724 | val_1_rmse: 1.00329 |  0:06:53s
epoch 60 | loss: 0.64904 | val_0_rmse: 0.81378 | val_1_rmse: 0.97289 |  0:07:00s
epoch 61 | loss: 0.63336 | val_0_rmse: 0.85516 | val_1_rmse: 1.03595 |  0:07:07s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.91345
Best weights from best epoch are automatically used!
ended training at: 05:22:52
Feature importance:
Mean squared error is of 0.06243121650592398
Mean absolute error:0.1855994945605277
MAPE:0.20087674370581793
R2 score:0.1565595468956893
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:22:54
epoch 0  | loss: 2.54358 | val_0_rmse: 1.03979 | val_1_rmse: 0.94835 |  0:00:00s
epoch 1  | loss: 1.10422 | val_0_rmse: 1.03351 | val_1_rmse: 0.94225 |  0:00:02s
epoch 2  | loss: 1.05825 | val_0_rmse: 1.0127  | val_1_rmse: 0.9263  |  0:00:03s
epoch 3  | loss: 1.05143 | val_0_rmse: 1.01815 | val_1_rmse: 0.9373  |  0:00:04s
epoch 4  | loss: 1.02827 | val_0_rmse: 1.0021  | val_1_rmse: 0.90746 |  0:00:05s
epoch 5  | loss: 0.98234 | val_0_rmse: 1.02675 | val_1_rmse: 0.94061 |  0:00:06s
epoch 6  | loss: 0.94994 | val_0_rmse: 1.01917 | val_1_rmse: 0.91989 |  0:00:07s
epoch 7  | loss: 0.9419  | val_0_rmse: 1.02216 | val_1_rmse: 0.93422 |  0:00:08s
epoch 8  | loss: 0.93249 | val_0_rmse: 1.01772 | val_1_rmse: 0.9337  |  0:00:09s
epoch 9  | loss: 0.9326  | val_0_rmse: 0.97922 | val_1_rmse: 0.89316 |  0:00:10s
epoch 10 | loss: 0.93143 | val_0_rmse: 0.97995 | val_1_rmse: 0.8872  |  0:00:11s
epoch 11 | loss: 0.94446 | val_0_rmse: 1.00665 | val_1_rmse: 0.91743 |  0:00:12s
epoch 12 | loss: 0.96457 | val_0_rmse: 0.98823 | val_1_rmse: 0.89644 |  0:00:13s
epoch 13 | loss: 0.95498 | val_0_rmse: 0.98758 | val_1_rmse: 0.89096 |  0:00:14s
epoch 14 | loss: 0.95437 | val_0_rmse: 0.97321 | val_1_rmse: 0.88177 |  0:00:15s
epoch 15 | loss: 0.9287  | val_0_rmse: 0.95958 | val_1_rmse: 0.87513 |  0:00:16s
epoch 16 | loss: 0.93429 | val_0_rmse: 0.96193 | val_1_rmse: 0.87866 |  0:00:17s
epoch 17 | loss: 0.92307 | val_0_rmse: 0.95852 | val_1_rmse: 0.87575 |  0:00:18s
epoch 18 | loss: 0.90666 | val_0_rmse: 0.95594 | val_1_rmse: 0.87321 |  0:00:19s
epoch 19 | loss: 0.8963  | val_0_rmse: 0.95867 | val_1_rmse: 0.87679 |  0:00:20s
epoch 20 | loss: 0.88563 | val_0_rmse: 0.96477 | val_1_rmse: 0.88031 |  0:00:21s
epoch 21 | loss: 0.89425 | val_0_rmse: 0.94993 | val_1_rmse: 0.8663  |  0:00:22s
epoch 22 | loss: 0.88449 | val_0_rmse: 0.94037 | val_1_rmse: 0.86526 |  0:00:23s
epoch 23 | loss: 0.89783 | val_0_rmse: 0.9807  | val_1_rmse: 0.90461 |  0:00:24s
epoch 24 | loss: 0.89155 | val_0_rmse: 0.94461 | val_1_rmse: 0.86353 |  0:00:25s
epoch 25 | loss: 0.87588 | val_0_rmse: 0.938   | val_1_rmse: 0.85548 |  0:00:26s
epoch 26 | loss: 0.86801 | val_0_rmse: 0.94383 | val_1_rmse: 0.86423 |  0:00:27s
epoch 27 | loss: 0.87941 | val_0_rmse: 0.92724 | val_1_rmse: 0.87144 |  0:00:28s
epoch 28 | loss: 0.866   | val_0_rmse: 0.94727 | val_1_rmse: 0.87317 |  0:00:29s
epoch 29 | loss: 0.89284 | val_0_rmse: 0.93469 | val_1_rmse: 0.85842 |  0:00:30s
epoch 30 | loss: 0.85747 | val_0_rmse: 0.92318 | val_1_rmse: 0.86084 |  0:00:31s
epoch 31 | loss: 0.85828 | val_0_rmse: 0.90953 | val_1_rmse: 0.85731 |  0:00:32s
epoch 32 | loss: 0.84722 | val_0_rmse: 0.91438 | val_1_rmse: 0.86469 |  0:00:33s
epoch 33 | loss: 0.8385  | val_0_rmse: 0.91569 | val_1_rmse: 0.85074 |  0:00:34s
epoch 34 | loss: 0.83206 | val_0_rmse: 0.9216  | val_1_rmse: 0.85446 |  0:00:35s
epoch 35 | loss: 0.86474 | val_0_rmse: 0.94508 | val_1_rmse: 0.87113 |  0:00:36s
epoch 36 | loss: 0.87476 | val_0_rmse: 0.92881 | val_1_rmse: 0.87286 |  0:00:37s
epoch 37 | loss: 0.85032 | val_0_rmse: 0.90797 | val_1_rmse: 0.88653 |  0:00:38s
epoch 38 | loss: 0.87223 | val_0_rmse: 0.93269 | val_1_rmse: 0.85158 |  0:00:39s
epoch 39 | loss: 0.86803 | val_0_rmse: 0.93454 | val_1_rmse: 0.85009 |  0:00:40s
epoch 40 | loss: 0.88335 | val_0_rmse: 0.94894 | val_1_rmse: 0.86922 |  0:00:41s
epoch 41 | loss: 0.87285 | val_0_rmse: 0.92958 | val_1_rmse: 0.84963 |  0:00:42s
epoch 42 | loss: 0.86513 | val_0_rmse: 0.93014 | val_1_rmse: 0.84771 |  0:00:43s
epoch 43 | loss: 0.86802 | val_0_rmse: 0.92763 | val_1_rmse: 0.85232 |  0:00:44s
epoch 44 | loss: 0.86192 | val_0_rmse: 0.9222  | val_1_rmse: 0.85171 |  0:00:45s
epoch 45 | loss: 0.85146 | val_0_rmse: 0.91552 | val_1_rmse: 0.84635 |  0:00:46s
epoch 46 | loss: 0.84246 | val_0_rmse: 0.90985 | val_1_rmse: 0.8476  |  0:00:47s
epoch 47 | loss: 0.83963 | val_0_rmse: 0.90962 | val_1_rmse: 0.8485  |  0:00:48s
epoch 48 | loss: 0.82267 | val_0_rmse: 0.88945 | val_1_rmse: 0.8607  |  0:00:49s
epoch 49 | loss: 0.84979 | val_0_rmse: 0.9183  | val_1_rmse: 0.85416 |  0:00:50s
epoch 50 | loss: 0.86059 | val_0_rmse: 0.91935 | val_1_rmse: 0.84829 |  0:00:51s
epoch 51 | loss: 0.84616 | val_0_rmse: 0.91122 | val_1_rmse: 0.84177 |  0:00:52s
epoch 52 | loss: 0.83581 | val_0_rmse: 0.90885 | val_1_rmse: 0.84967 |  0:00:53s
epoch 53 | loss: 0.83789 | val_0_rmse: 0.92601 | val_1_rmse: 0.8576  |  0:00:54s
epoch 54 | loss: 0.86259 | val_0_rmse: 0.94419 | val_1_rmse: 0.86912 |  0:00:55s
epoch 55 | loss: 0.87773 | val_0_rmse: 0.92876 | val_1_rmse: 0.85396 |  0:00:56s
epoch 56 | loss: 0.86515 | val_0_rmse: 0.92438 | val_1_rmse: 0.84821 |  0:00:57s
epoch 57 | loss: 0.85866 | val_0_rmse: 0.93208 | val_1_rmse: 0.86693 |  0:00:58s
epoch 58 | loss: 0.85129 | val_0_rmse: 0.91782 | val_1_rmse: 0.85299 |  0:00:59s
epoch 59 | loss: 0.84587 | val_0_rmse: 0.91179 | val_1_rmse: 0.83944 |  0:01:00s
epoch 60 | loss: 0.8364  | val_0_rmse: 0.90609 | val_1_rmse: 0.84157 |  0:01:01s
epoch 61 | loss: 0.82998 | val_0_rmse: 0.90702 | val_1_rmse: 0.84461 |  0:01:02s
epoch 62 | loss: 0.82777 | val_0_rmse: 0.90502 | val_1_rmse: 0.84944 |  0:01:03s
epoch 63 | loss: 0.82395 | val_0_rmse: 0.89961 | val_1_rmse: 0.84288 |  0:01:04s
epoch 64 | loss: 0.81742 | val_0_rmse: 0.89695 | val_1_rmse: 0.85136 |  0:01:05s
epoch 65 | loss: 0.81659 | val_0_rmse: 0.89817 | val_1_rmse: 0.8452  |  0:01:06s
epoch 66 | loss: 0.814   | val_0_rmse: 0.8933  | val_1_rmse: 0.85859 |  0:01:07s
epoch 67 | loss: 0.81693 | val_0_rmse: 0.90253 | val_1_rmse: 0.84461 |  0:01:08s
epoch 68 | loss: 0.8365  | val_0_rmse: 0.89866 | val_1_rmse: 0.85329 |  0:01:09s
epoch 69 | loss: 0.82369 | val_0_rmse: 0.89503 | val_1_rmse: 0.8493  |  0:01:10s
epoch 70 | loss: 0.81392 | val_0_rmse: 0.8962  | val_1_rmse: 0.84234 |  0:01:11s
epoch 71 | loss: 0.80843 | val_0_rmse: 0.89468 | val_1_rmse: 0.84237 |  0:01:12s
epoch 72 | loss: 0.80099 | val_0_rmse: 0.87841 | val_1_rmse: 0.86301 |  0:01:13s
epoch 73 | loss: 0.79933 | val_0_rmse: 0.88261 | val_1_rmse: 0.84548 |  0:01:14s
epoch 74 | loss: 0.79452 | val_0_rmse: 0.87658 | val_1_rmse: 0.84534 |  0:01:15s
epoch 75 | loss: 0.79456 | val_0_rmse: 0.89969 | val_1_rmse: 0.85297 |  0:01:16s
epoch 76 | loss: 0.81609 | val_0_rmse: 0.88174 | val_1_rmse: 0.86845 |  0:01:17s
epoch 77 | loss: 0.79633 | val_0_rmse: 0.87294 | val_1_rmse: 0.87483 |  0:01:18s
epoch 78 | loss: 0.77376 | val_0_rmse: 0.88784 | val_1_rmse: 0.93003 |  0:01:19s
epoch 79 | loss: 0.78159 | val_0_rmse: 0.87627 | val_1_rmse: 0.91764 |  0:01:20s
epoch 80 | loss: 0.76001 | val_0_rmse: 0.89734 | val_1_rmse: 0.84456 |  0:01:21s
epoch 81 | loss: 0.82089 | val_0_rmse: 0.91061 | val_1_rmse: 0.85294 |  0:01:22s
epoch 82 | loss: 0.83623 | val_0_rmse: 0.90755 | val_1_rmse: 0.85609 |  0:01:23s
epoch 83 | loss: 0.83578 | val_0_rmse: 0.91028 | val_1_rmse: 0.85547 |  0:01:24s
epoch 84 | loss: 0.8284  | val_0_rmse: 0.91866 | val_1_rmse: 0.87816 |  0:01:25s
epoch 85 | loss: 0.81452 | val_0_rmse: 0.88869 | val_1_rmse: 0.85843 |  0:01:26s
epoch 86 | loss: 0.79653 | val_0_rmse: 0.88651 | val_1_rmse: 0.84974 |  0:01:27s
epoch 87 | loss: 0.79733 | val_0_rmse: 0.91976 | val_1_rmse: 0.87894 |  0:01:28s
epoch 88 | loss: 0.79441 | val_0_rmse: 0.88119 | val_1_rmse: 0.84388 |  0:01:29s
epoch 89 | loss: 0.78149 | val_0_rmse: 0.87641 | val_1_rmse: 0.84607 |  0:01:30s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.83944
Best weights from best epoch are automatically used!
ended training at: 05:24:25
Feature importance:
Mean squared error is of 0.06446345113332629
Mean absolute error:0.191753916188551
MAPE:0.21747987117793327
R2 score:0.1950328339553028
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:24:25
epoch 0  | loss: 1.45231 | val_0_rmse: 1.00984 | val_1_rmse: 0.94497 |  0:00:00s
epoch 1  | loss: 0.97015 | val_0_rmse: 0.966   | val_1_rmse: 0.90357 |  0:00:01s
epoch 2  | loss: 0.85323 | val_0_rmse: 0.93606 | val_1_rmse: 0.8809  |  0:00:02s
epoch 3  | loss: 0.8417  | val_0_rmse: 0.92143 | val_1_rmse: 0.86368 |  0:00:03s
epoch 4  | loss: 0.85761 | val_0_rmse: 0.91796 | val_1_rmse: 0.86036 |  0:00:04s
epoch 5  | loss: 0.84757 | val_0_rmse: 0.91259 | val_1_rmse: 0.85988 |  0:00:05s
epoch 6  | loss: 0.82429 | val_0_rmse: 0.89184 | val_1_rmse: 0.84647 |  0:00:06s
epoch 7  | loss: 0.78931 | val_0_rmse: 0.86353 | val_1_rmse: 0.82941 |  0:00:07s
epoch 8  | loss: 0.7316  | val_0_rmse: 0.83826 | val_1_rmse: 0.80235 |  0:00:08s
epoch 9  | loss: 0.64048 | val_0_rmse: 0.82744 | val_1_rmse: 0.79985 |  0:00:09s
epoch 10 | loss: 0.60889 | val_0_rmse: 0.81543 | val_1_rmse: 0.79086 |  0:00:10s
epoch 11 | loss: 0.60517 | val_0_rmse: 0.77876 | val_1_rmse: 0.76501 |  0:00:11s
epoch 12 | loss: 0.58094 | val_0_rmse: 0.79183 | val_1_rmse: 0.77093 |  0:00:11s
epoch 13 | loss: 0.56824 | val_0_rmse: 0.77174 | val_1_rmse: 0.75317 |  0:00:12s
epoch 14 | loss: 0.54563 | val_0_rmse: 0.73878 | val_1_rmse: 0.7357  |  0:00:13s
epoch 15 | loss: 0.54214 | val_0_rmse: 0.74621 | val_1_rmse: 0.73638 |  0:00:14s
epoch 16 | loss: 0.54478 | val_0_rmse: 0.73895 | val_1_rmse: 0.73625 |  0:00:15s
epoch 17 | loss: 0.5366  | val_0_rmse: 0.72759 | val_1_rmse: 0.73467 |  0:00:16s
epoch 18 | loss: 0.51793 | val_0_rmse: 0.72389 | val_1_rmse: 0.72687 |  0:00:17s
epoch 19 | loss: 0.5245  | val_0_rmse: 0.71781 | val_1_rmse: 0.72181 |  0:00:18s
epoch 20 | loss: 0.49519 | val_0_rmse: 0.71496 | val_1_rmse: 0.72089 |  0:00:19s
epoch 21 | loss: 0.4767  | val_0_rmse: 0.72448 | val_1_rmse: 0.72241 |  0:00:20s
epoch 22 | loss: 0.47462 | val_0_rmse: 0.69742 | val_1_rmse: 0.70584 |  0:00:21s
epoch 23 | loss: 0.46754 | val_0_rmse: 0.70864 | val_1_rmse: 0.71577 |  0:00:22s
epoch 24 | loss: 0.47492 | val_0_rmse: 0.70329 | val_1_rmse: 0.72903 |  0:00:22s
epoch 25 | loss: 0.47609 | val_0_rmse: 0.68974 | val_1_rmse: 0.69781 |  0:00:23s
epoch 26 | loss: 0.4633  | val_0_rmse: 0.69722 | val_1_rmse: 0.70465 |  0:00:24s
epoch 27 | loss: 0.48209 | val_0_rmse: 0.70532 | val_1_rmse: 0.70838 |  0:00:25s
epoch 28 | loss: 0.4811  | val_0_rmse: 0.71676 | val_1_rmse: 0.75644 |  0:00:26s
epoch 29 | loss: 0.48221 | val_0_rmse: 0.67745 | val_1_rmse: 0.7006  |  0:00:27s
epoch 30 | loss: 0.45962 | val_0_rmse: 0.67291 | val_1_rmse: 0.69514 |  0:00:28s
epoch 31 | loss: 0.46131 | val_0_rmse: 0.68952 | val_1_rmse: 0.69769 |  0:00:29s
epoch 32 | loss: 0.47388 | val_0_rmse: 0.68106 | val_1_rmse: 0.68686 |  0:00:30s
epoch 33 | loss: 0.4692  | val_0_rmse: 0.68282 | val_1_rmse: 0.69219 |  0:00:31s
epoch 34 | loss: 0.46071 | val_0_rmse: 0.6695  | val_1_rmse: 0.68036 |  0:00:32s
epoch 35 | loss: 0.45616 | val_0_rmse: 0.65119 | val_1_rmse: 0.6783  |  0:00:33s
epoch 36 | loss: 0.43992 | val_0_rmse: 0.66353 | val_1_rmse: 0.68955 |  0:00:34s
epoch 37 | loss: 0.44608 | val_0_rmse: 0.65611 | val_1_rmse: 0.67796 |  0:00:34s
epoch 38 | loss: 0.446   | val_0_rmse: 0.65746 | val_1_rmse: 0.68116 |  0:00:35s
epoch 39 | loss: 0.45215 | val_0_rmse: 0.6474  | val_1_rmse: 0.68495 |  0:00:36s
epoch 40 | loss: 0.44218 | val_0_rmse: 0.66419 | val_1_rmse: 0.69571 |  0:00:37s
epoch 41 | loss: 0.45971 | val_0_rmse: 0.66007 | val_1_rmse: 0.67843 |  0:00:38s
epoch 42 | loss: 0.44308 | val_0_rmse: 0.7481  | val_1_rmse: 0.78426 |  0:00:39s
epoch 43 | loss: 0.43472 | val_0_rmse: 0.64343 | val_1_rmse: 0.67435 |  0:00:40s
epoch 44 | loss: 0.44644 | val_0_rmse: 0.66761 | val_1_rmse: 0.69773 |  0:00:41s
epoch 45 | loss: 0.43583 | val_0_rmse: 0.63023 | val_1_rmse: 0.65482 |  0:00:42s
epoch 46 | loss: 0.42257 | val_0_rmse: 0.63234 | val_1_rmse: 0.65877 |  0:00:43s
epoch 47 | loss: 0.41134 | val_0_rmse: 0.6334  | val_1_rmse: 0.67388 |  0:00:44s
epoch 48 | loss: 0.42221 | val_0_rmse: 0.62904 | val_1_rmse: 0.659   |  0:00:45s
epoch 49 | loss: 0.41956 | val_0_rmse: 0.65732 | val_1_rmse: 0.68114 |  0:00:45s
epoch 50 | loss: 0.41897 | val_0_rmse: 0.65551 | val_1_rmse: 0.69436 |  0:00:46s
epoch 51 | loss: 0.4259  | val_0_rmse: 0.62991 | val_1_rmse: 0.67025 |  0:00:47s
epoch 52 | loss: 0.41682 | val_0_rmse: 0.62572 | val_1_rmse: 0.65774 |  0:00:48s
epoch 53 | loss: 0.41542 | val_0_rmse: 0.62759 | val_1_rmse: 0.65481 |  0:00:49s
epoch 54 | loss: 0.40946 | val_0_rmse: 0.64217 | val_1_rmse: 0.66082 |  0:00:50s
epoch 55 | loss: 0.41138 | val_0_rmse: 0.63397 | val_1_rmse: 0.66428 |  0:00:51s
epoch 56 | loss: 0.40789 | val_0_rmse: 0.62609 | val_1_rmse: 0.66703 |  0:00:52s
epoch 57 | loss: 0.39436 | val_0_rmse: 0.62504 | val_1_rmse: 0.6565  |  0:00:53s
epoch 58 | loss: 0.40021 | val_0_rmse: 0.61462 | val_1_rmse: 0.65229 |  0:00:54s
epoch 59 | loss: 0.38802 | val_0_rmse: 0.62559 | val_1_rmse: 0.65146 |  0:00:55s
epoch 60 | loss: 0.39588 | val_0_rmse: 0.64598 | val_1_rmse: 0.6879  |  0:00:56s
epoch 61 | loss: 0.42011 | val_0_rmse: 0.63455 | val_1_rmse: 0.6695  |  0:00:57s
epoch 62 | loss: 0.40831 | val_0_rmse: 0.61964 | val_1_rmse: 0.64261 |  0:00:57s
epoch 63 | loss: 0.39387 | val_0_rmse: 0.60957 | val_1_rmse: 0.64501 |  0:00:58s
epoch 64 | loss: 0.38883 | val_0_rmse: 0.63808 | val_1_rmse: 0.66133 |  0:00:59s
epoch 65 | loss: 0.4232  | val_0_rmse: 0.63542 | val_1_rmse: 0.6653  |  0:01:00s
epoch 66 | loss: 0.42125 | val_0_rmse: 0.63055 | val_1_rmse: 0.66714 |  0:01:01s
epoch 67 | loss: 0.40127 | val_0_rmse: 0.61492 | val_1_rmse: 0.64299 |  0:01:02s
epoch 68 | loss: 0.38795 | val_0_rmse: 0.60626 | val_1_rmse: 0.62794 |  0:01:03s
epoch 69 | loss: 0.39031 | val_0_rmse: 0.609   | val_1_rmse: 0.64757 |  0:01:04s
epoch 70 | loss: 0.38695 | val_0_rmse: 0.60909 | val_1_rmse: 0.6424  |  0:01:05s
epoch 71 | loss: 0.3842  | val_0_rmse: 0.60544 | val_1_rmse: 0.62942 |  0:01:06s
epoch 72 | loss: 0.37515 | val_0_rmse: 0.61226 | val_1_rmse: 0.62865 |  0:01:07s
epoch 73 | loss: 0.37859 | val_0_rmse: 0.61095 | val_1_rmse: 0.64013 |  0:01:08s
epoch 74 | loss: 0.38088 | val_0_rmse: 0.60446 | val_1_rmse: 0.63522 |  0:01:08s
epoch 75 | loss: 0.377   | val_0_rmse: 0.60651 | val_1_rmse: 0.63819 |  0:01:09s
epoch 76 | loss: 0.38644 | val_0_rmse: 0.61617 | val_1_rmse: 0.63349 |  0:01:10s
epoch 77 | loss: 0.39182 | val_0_rmse: 0.61439 | val_1_rmse: 0.63128 |  0:01:11s
epoch 78 | loss: 0.37642 | val_0_rmse: 0.6252  | val_1_rmse: 0.64743 |  0:01:12s
epoch 79 | loss: 0.36986 | val_0_rmse: 0.59187 | val_1_rmse: 0.62412 |  0:01:13s
epoch 80 | loss: 0.3654  | val_0_rmse: 0.58278 | val_1_rmse: 0.6149  |  0:01:14s
epoch 81 | loss: 0.36278 | val_0_rmse: 0.60718 | val_1_rmse: 0.64438 |  0:01:15s
epoch 82 | loss: 0.3696  | val_0_rmse: 0.58489 | val_1_rmse: 0.62446 |  0:01:16s
epoch 83 | loss: 0.35944 | val_0_rmse: 0.58718 | val_1_rmse: 0.6214  |  0:01:17s
epoch 84 | loss: 0.35924 | val_0_rmse: 0.58928 | val_1_rmse: 0.62203 |  0:01:18s
epoch 85 | loss: 0.35568 | val_0_rmse: 0.57745 | val_1_rmse: 0.60973 |  0:01:19s
epoch 86 | loss: 0.35796 | val_0_rmse: 0.58101 | val_1_rmse: 0.60839 |  0:01:19s
epoch 87 | loss: 0.36286 | val_0_rmse: 0.58394 | val_1_rmse: 0.61105 |  0:01:20s
epoch 88 | loss: 0.35774 | val_0_rmse: 0.58428 | val_1_rmse: 0.61461 |  0:01:21s
epoch 89 | loss: 0.3608  | val_0_rmse: 0.58842 | val_1_rmse: 0.62434 |  0:01:22s
epoch 90 | loss: 0.35645 | val_0_rmse: 0.58804 | val_1_rmse: 0.62608 |  0:01:23s
epoch 91 | loss: 0.36582 | val_0_rmse: 0.58347 | val_1_rmse: 0.61743 |  0:01:24s
epoch 92 | loss: 0.35183 | val_0_rmse: 0.57661 | val_1_rmse: 0.61438 |  0:01:25s
epoch 93 | loss: 0.36059 | val_0_rmse: 0.5909  | val_1_rmse: 0.63626 |  0:01:26s
epoch 94 | loss: 0.35794 | val_0_rmse: 0.57559 | val_1_rmse: 0.61915 |  0:01:27s
epoch 95 | loss: 0.34338 | val_0_rmse: 0.58478 | val_1_rmse: 0.62024 |  0:01:28s
epoch 96 | loss: 0.34574 | val_0_rmse: 0.57297 | val_1_rmse: 0.6093  |  0:01:29s
epoch 97 | loss: 0.35541 | val_0_rmse: 0.5824  | val_1_rmse: 0.6254  |  0:01:30s
epoch 98 | loss: 0.35442 | val_0_rmse: 0.58368 | val_1_rmse: 0.61582 |  0:01:30s
epoch 99 | loss: 0.3516  | val_0_rmse: 0.57319 | val_1_rmse: 0.60986 |  0:01:31s
epoch 100| loss: 0.35317 | val_0_rmse: 0.583   | val_1_rmse: 0.60699 |  0:01:32s
epoch 101| loss: 0.35434 | val_0_rmse: 0.57657 | val_1_rmse: 0.60495 |  0:01:33s
epoch 102| loss: 0.34438 | val_0_rmse: 0.58787 | val_1_rmse: 0.61562 |  0:01:34s
epoch 103| loss: 0.34945 | val_0_rmse: 0.57714 | val_1_rmse: 0.6126  |  0:01:35s
epoch 104| loss: 0.34236 | val_0_rmse: 0.57609 | val_1_rmse: 0.62199 |  0:01:36s
epoch 105| loss: 0.3629  | val_0_rmse: 0.58497 | val_1_rmse: 0.63073 |  0:01:37s
epoch 106| loss: 0.35902 | val_0_rmse: 0.58288 | val_1_rmse: 0.61173 |  0:01:38s
epoch 107| loss: 0.35192 | val_0_rmse: 0.57972 | val_1_rmse: 0.61608 |  0:01:39s
epoch 108| loss: 0.35363 | val_0_rmse: 0.56444 | val_1_rmse: 0.60418 |  0:01:40s
epoch 109| loss: 0.34638 | val_0_rmse: 0.59211 | val_1_rmse: 0.63212 |  0:01:41s
epoch 110| loss: 0.35413 | val_0_rmse: 0.58202 | val_1_rmse: 0.62904 |  0:01:41s
epoch 111| loss: 0.35277 | val_0_rmse: 0.58393 | val_1_rmse: 0.62595 |  0:01:42s
epoch 112| loss: 0.35568 | val_0_rmse: 0.57243 | val_1_rmse: 0.61871 |  0:01:43s
epoch 113| loss: 0.33765 | val_0_rmse: 0.56528 | val_1_rmse: 0.61172 |  0:01:44s
epoch 114| loss: 0.33631 | val_0_rmse: 0.56413 | val_1_rmse: 0.60953 |  0:01:45s
epoch 115| loss: 0.33554 | val_0_rmse: 0.56588 | val_1_rmse: 0.61259 |  0:01:46s
epoch 116| loss: 0.3412  | val_0_rmse: 0.56817 | val_1_rmse: 0.61572 |  0:01:47s
epoch 117| loss: 0.33302 | val_0_rmse: 0.56514 | val_1_rmse: 0.60894 |  0:01:48s
epoch 118| loss: 0.33543 | val_0_rmse: 0.55683 | val_1_rmse: 0.60062 |  0:01:49s
epoch 119| loss: 0.32283 | val_0_rmse: 0.55519 | val_1_rmse: 0.60423 |  0:01:50s
epoch 120| loss: 0.33031 | val_0_rmse: 0.55713 | val_1_rmse: 0.60596 |  0:01:51s
epoch 121| loss: 0.32797 | val_0_rmse: 0.5592  | val_1_rmse: 0.60359 |  0:01:52s
epoch 122| loss: 0.32882 | val_0_rmse: 0.56752 | val_1_rmse: 0.6162  |  0:01:52s
epoch 123| loss: 0.33574 | val_0_rmse: 0.56871 | val_1_rmse: 0.61378 |  0:01:53s
epoch 124| loss: 0.32176 | val_0_rmse: 0.56171 | val_1_rmse: 0.61678 |  0:01:54s
epoch 125| loss: 0.32681 | val_0_rmse: 0.55259 | val_1_rmse: 0.60583 |  0:01:55s
epoch 126| loss: 0.32378 | val_0_rmse: 0.55763 | val_1_rmse: 0.60535 |  0:01:56s
epoch 127| loss: 0.32807 | val_0_rmse: 0.55813 | val_1_rmse: 0.60222 |  0:01:57s
epoch 128| loss: 0.32596 | val_0_rmse: 0.5582  | val_1_rmse: 0.61752 |  0:01:58s
epoch 129| loss: 0.32727 | val_0_rmse: 0.55727 | val_1_rmse: 0.61336 |  0:01:59s
epoch 130| loss: 0.31982 | val_0_rmse: 0.55462 | val_1_rmse: 0.61033 |  0:02:00s
epoch 131| loss: 0.32071 | val_0_rmse: 0.55064 | val_1_rmse: 0.60636 |  0:02:01s
epoch 132| loss: 0.32198 | val_0_rmse: 0.55179 | val_1_rmse: 0.60697 |  0:02:02s
epoch 133| loss: 0.32148 | val_0_rmse: 0.5571  | val_1_rmse: 0.60786 |  0:02:03s
epoch 134| loss: 0.32103 | val_0_rmse: 0.55451 | val_1_rmse: 0.60638 |  0:02:03s
epoch 135| loss: 0.32921 | val_0_rmse: 0.55444 | val_1_rmse: 0.60886 |  0:02:04s
epoch 136| loss: 0.33205 | val_0_rmse: 0.57111 | val_1_rmse: 0.61599 |  0:02:05s
epoch 137| loss: 0.32982 | val_0_rmse: 0.56157 | val_1_rmse: 0.61407 |  0:02:06s
epoch 138| loss: 0.3407  | val_0_rmse: 0.57021 | val_1_rmse: 0.62409 |  0:02:07s
epoch 139| loss: 0.33398 | val_0_rmse: 0.56257 | val_1_rmse: 0.62182 |  0:02:08s
epoch 140| loss: 0.32962 | val_0_rmse: 0.56101 | val_1_rmse: 0.61209 |  0:02:09s
epoch 141| loss: 0.32535 | val_0_rmse: 0.55589 | val_1_rmse: 0.61336 |  0:02:10s
epoch 142| loss: 0.32402 | val_0_rmse: 0.55344 | val_1_rmse: 0.60927 |  0:02:11s
epoch 143| loss: 0.31958 | val_0_rmse: 0.55074 | val_1_rmse: 0.60825 |  0:02:12s
epoch 144| loss: 0.31995 | val_0_rmse: 0.55008 | val_1_rmse: 0.60504 |  0:02:13s
epoch 145| loss: 0.31883 | val_0_rmse: 0.54731 | val_1_rmse: 0.60905 |  0:02:14s
epoch 146| loss: 0.33579 | val_0_rmse: 0.55294 | val_1_rmse: 0.60999 |  0:02:14s
epoch 147| loss: 0.32487 | val_0_rmse: 0.55319 | val_1_rmse: 0.60992 |  0:02:15s
epoch 148| loss: 0.31975 | val_0_rmse: 0.54965 | val_1_rmse: 0.60707 |  0:02:16s

Early stopping occured at epoch 148 with best_epoch = 118 and best_val_1_rmse = 0.60062
Best weights from best epoch are automatically used!
ended training at: 05:26:42
Feature importance:
Mean squared error is of 0.1774139681884722
Mean absolute error:0.2772165618266333
MAPE:0.3506544580929084
R2 score:0.6396634638675522
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:26:43
epoch 0  | loss: 1.59062 | val_0_rmse: 1.01442 | val_1_rmse: 1.011   |  0:00:00s
epoch 1  | loss: 1.00862 | val_0_rmse: 0.96576 | val_1_rmse: 0.96402 |  0:00:00s
epoch 2  | loss: 0.86141 | val_0_rmse: 0.89057 | val_1_rmse: 0.8928  |  0:00:00s
epoch 3  | loss: 0.78083 | val_0_rmse: 0.87468 | val_1_rmse: 0.87513 |  0:00:01s
epoch 4  | loss: 0.73863 | val_0_rmse: 0.86549 | val_1_rmse: 0.87653 |  0:00:01s
epoch 5  | loss: 0.73665 | val_0_rmse: 0.85278 | val_1_rmse: 0.84964 |  0:00:01s
epoch 6  | loss: 0.72911 | val_0_rmse: 0.86466 | val_1_rmse: 0.87044 |  0:00:02s
epoch 7  | loss: 0.72864 | val_0_rmse: 0.85147 | val_1_rmse: 0.85444 |  0:00:02s
epoch 8  | loss: 0.72971 | val_0_rmse: 0.85881 | val_1_rmse: 0.86351 |  0:00:02s
epoch 9  | loss: 0.70572 | val_0_rmse: 0.84199 | val_1_rmse: 0.84612 |  0:00:03s
epoch 10 | loss: 0.70297 | val_0_rmse: 0.8552  | val_1_rmse: 0.86651 |  0:00:03s
epoch 11 | loss: 0.70417 | val_0_rmse: 0.86963 | val_1_rmse: 0.8789  |  0:00:03s
epoch 12 | loss: 0.70666 | val_0_rmse: 0.847   | val_1_rmse: 0.85607 |  0:00:03s
epoch 13 | loss: 0.69324 | val_0_rmse: 0.84917 | val_1_rmse: 0.85629 |  0:00:04s
epoch 14 | loss: 0.69753 | val_0_rmse: 0.8468  | val_1_rmse: 0.85663 |  0:00:04s
epoch 15 | loss: 0.68934 | val_0_rmse: 0.84183 | val_1_rmse: 0.8418  |  0:00:04s
epoch 16 | loss: 0.69111 | val_0_rmse: 0.84995 | val_1_rmse: 0.86134 |  0:00:05s
epoch 17 | loss: 0.68902 | val_0_rmse: 0.8491  | val_1_rmse: 0.85724 |  0:00:05s
epoch 18 | loss: 0.68135 | val_0_rmse: 0.84549 | val_1_rmse: 0.86135 |  0:00:05s
epoch 19 | loss: 0.68166 | val_0_rmse: 0.85476 | val_1_rmse: 0.85768 |  0:00:06s
epoch 20 | loss: 0.6887  | val_0_rmse: 0.86024 | val_1_rmse: 0.87874 |  0:00:06s
epoch 21 | loss: 0.69268 | val_0_rmse: 0.85758 | val_1_rmse: 0.8657  |  0:00:06s
epoch 22 | loss: 0.68205 | val_0_rmse: 0.83283 | val_1_rmse: 0.83977 |  0:00:06s
epoch 23 | loss: 0.68768 | val_0_rmse: 0.84507 | val_1_rmse: 0.854   |  0:00:07s
epoch 24 | loss: 0.68856 | val_0_rmse: 0.86319 | val_1_rmse: 0.87575 |  0:00:07s
epoch 25 | loss: 0.69574 | val_0_rmse: 0.84347 | val_1_rmse: 0.83953 |  0:00:07s
epoch 26 | loss: 0.68959 | val_0_rmse: 0.83442 | val_1_rmse: 0.83053 |  0:00:08s
epoch 27 | loss: 0.67679 | val_0_rmse: 0.81996 | val_1_rmse: 0.82194 |  0:00:08s
epoch 28 | loss: 0.68017 | val_0_rmse: 0.83521 | val_1_rmse: 0.82954 |  0:00:08s
epoch 29 | loss: 0.69389 | val_0_rmse: 0.82484 | val_1_rmse: 0.82432 |  0:00:09s
epoch 30 | loss: 0.68123 | val_0_rmse: 0.82553 | val_1_rmse: 0.83511 |  0:00:09s
epoch 31 | loss: 0.6774  | val_0_rmse: 0.82143 | val_1_rmse: 0.83573 |  0:00:09s
epoch 32 | loss: 0.68189 | val_0_rmse: 0.82711 | val_1_rmse: 0.83957 |  0:00:10s
epoch 33 | loss: 0.67256 | val_0_rmse: 0.82317 | val_1_rmse: 0.83302 |  0:00:10s
epoch 34 | loss: 0.66927 | val_0_rmse: 0.81891 | val_1_rmse: 0.82306 |  0:00:10s
epoch 35 | loss: 0.65395 | val_0_rmse: 0.8254  | val_1_rmse: 0.83824 |  0:00:10s
epoch 36 | loss: 0.66412 | val_0_rmse: 0.81759 | val_1_rmse: 0.8289  |  0:00:11s
epoch 37 | loss: 0.66489 | val_0_rmse: 0.83737 | val_1_rmse: 0.84486 |  0:00:11s
epoch 38 | loss: 0.65452 | val_0_rmse: 0.82886 | val_1_rmse: 0.83053 |  0:00:11s
epoch 39 | loss: 0.65825 | val_0_rmse: 0.82614 | val_1_rmse: 0.84303 |  0:00:12s
epoch 40 | loss: 0.66872 | val_0_rmse: 0.81963 | val_1_rmse: 0.82053 |  0:00:12s
epoch 41 | loss: 0.66857 | val_0_rmse: 0.81635 | val_1_rmse: 0.81622 |  0:00:12s
epoch 42 | loss: 0.66931 | val_0_rmse: 0.81493 | val_1_rmse: 0.81959 |  0:00:13s
epoch 43 | loss: 0.66684 | val_0_rmse: 0.817   | val_1_rmse: 0.82933 |  0:00:13s
epoch 44 | loss: 0.65868 | val_0_rmse: 0.81121 | val_1_rmse: 0.83097 |  0:00:13s
epoch 45 | loss: 0.65732 | val_0_rmse: 0.81453 | val_1_rmse: 0.83698 |  0:00:13s
epoch 46 | loss: 0.67052 | val_0_rmse: 0.82049 | val_1_rmse: 0.84906 |  0:00:14s
epoch 47 | loss: 0.64963 | val_0_rmse: 0.80538 | val_1_rmse: 0.81675 |  0:00:14s
epoch 48 | loss: 0.66886 | val_0_rmse: 0.82483 | val_1_rmse: 0.85309 |  0:00:14s
epoch 49 | loss: 0.66472 | val_0_rmse: 0.81501 | val_1_rmse: 0.81761 |  0:00:15s
epoch 50 | loss: 0.65362 | val_0_rmse: 0.81592 | val_1_rmse: 0.83156 |  0:00:15s
epoch 51 | loss: 0.65387 | val_0_rmse: 0.80938 | val_1_rmse: 0.81549 |  0:00:15s
epoch 52 | loss: 0.65135 | val_0_rmse: 0.81089 | val_1_rmse: 0.81953 |  0:00:16s
epoch 53 | loss: 0.64365 | val_0_rmse: 0.80766 | val_1_rmse: 0.81479 |  0:00:16s
epoch 54 | loss: 0.63809 | val_0_rmse: 0.80325 | val_1_rmse: 0.81767 |  0:00:16s
epoch 55 | loss: 0.63788 | val_0_rmse: 0.80168 | val_1_rmse: 0.82135 |  0:00:16s
epoch 56 | loss: 0.64285 | val_0_rmse: 0.80315 | val_1_rmse: 0.81309 |  0:00:17s
epoch 57 | loss: 0.63829 | val_0_rmse: 0.81049 | val_1_rmse: 0.83114 |  0:00:17s
epoch 58 | loss: 0.63757 | val_0_rmse: 0.80263 | val_1_rmse: 0.81985 |  0:00:17s
epoch 59 | loss: 0.63709 | val_0_rmse: 0.79926 | val_1_rmse: 0.82536 |  0:00:18s
epoch 60 | loss: 0.6331  | val_0_rmse: 0.79353 | val_1_rmse: 0.81772 |  0:00:18s
epoch 61 | loss: 0.62644 | val_0_rmse: 0.79711 | val_1_rmse: 0.82357 |  0:00:18s
epoch 62 | loss: 0.64343 | val_0_rmse: 0.79855 | val_1_rmse: 0.8146  |  0:00:19s
epoch 63 | loss: 0.62425 | val_0_rmse: 0.79679 | val_1_rmse: 0.81916 |  0:00:19s
epoch 64 | loss: 0.62964 | val_0_rmse: 0.7926  | val_1_rmse: 0.80879 |  0:00:19s
epoch 65 | loss: 0.62986 | val_0_rmse: 0.79217 | val_1_rmse: 0.81009 |  0:00:20s
epoch 66 | loss: 0.62509 | val_0_rmse: 0.79295 | val_1_rmse: 0.80836 |  0:00:20s
epoch 67 | loss: 0.62101 | val_0_rmse: 0.80011 | val_1_rmse: 0.81124 |  0:00:20s
epoch 68 | loss: 0.61494 | val_0_rmse: 0.79966 | val_1_rmse: 0.80889 |  0:00:20s
epoch 69 | loss: 0.62087 | val_0_rmse: 0.79766 | val_1_rmse: 0.80293 |  0:00:21s
epoch 70 | loss: 0.61847 | val_0_rmse: 0.79072 | val_1_rmse: 0.80871 |  0:00:21s
epoch 71 | loss: 0.62544 | val_0_rmse: 0.78795 | val_1_rmse: 0.81289 |  0:00:21s
epoch 72 | loss: 0.61237 | val_0_rmse: 0.79011 | val_1_rmse: 0.81619 |  0:00:22s
epoch 73 | loss: 0.61469 | val_0_rmse: 0.78806 | val_1_rmse: 0.81275 |  0:00:22s
epoch 74 | loss: 0.61113 | val_0_rmse: 0.79131 | val_1_rmse: 0.81335 |  0:00:22s
epoch 75 | loss: 0.60904 | val_0_rmse: 0.78743 | val_1_rmse: 0.81086 |  0:00:23s
epoch 76 | loss: 0.60572 | val_0_rmse: 0.78644 | val_1_rmse: 0.81738 |  0:00:23s
epoch 77 | loss: 0.60605 | val_0_rmse: 0.78947 | val_1_rmse: 0.82386 |  0:00:23s
epoch 78 | loss: 0.60595 | val_0_rmse: 0.79096 | val_1_rmse: 0.82272 |  0:00:23s
epoch 79 | loss: 0.6059  | val_0_rmse: 0.77937 | val_1_rmse: 0.81599 |  0:00:24s
epoch 80 | loss: 0.60338 | val_0_rmse: 0.78337 | val_1_rmse: 0.81924 |  0:00:24s
epoch 81 | loss: 0.60258 | val_0_rmse: 0.78419 | val_1_rmse: 0.81285 |  0:00:24s
epoch 82 | loss: 0.604   | val_0_rmse: 0.77876 | val_1_rmse: 0.80414 |  0:00:25s
epoch 83 | loss: 0.60126 | val_0_rmse: 0.79034 | val_1_rmse: 0.82322 |  0:00:25s
epoch 84 | loss: 0.60285 | val_0_rmse: 0.79307 | val_1_rmse: 0.82761 |  0:00:25s
epoch 85 | loss: 0.60308 | val_0_rmse: 0.78774 | val_1_rmse: 0.81877 |  0:00:26s
epoch 86 | loss: 0.60881 | val_0_rmse: 0.78236 | val_1_rmse: 0.81247 |  0:00:26s
epoch 87 | loss: 0.6016  | val_0_rmse: 0.80146 | val_1_rmse: 0.826   |  0:00:26s
epoch 88 | loss: 0.60318 | val_0_rmse: 0.77999 | val_1_rmse: 0.82202 |  0:00:26s
epoch 89 | loss: 0.60822 | val_0_rmse: 0.77509 | val_1_rmse: 0.81267 |  0:00:27s
epoch 90 | loss: 0.59837 | val_0_rmse: 0.77385 | val_1_rmse: 0.81467 |  0:00:27s
epoch 91 | loss: 0.59626 | val_0_rmse: 0.7752  | val_1_rmse: 0.81152 |  0:00:27s
epoch 92 | loss: 0.59026 | val_0_rmse: 0.78608 | val_1_rmse: 0.80714 |  0:00:28s
epoch 93 | loss: 0.5987  | val_0_rmse: 0.77628 | val_1_rmse: 0.81121 |  0:00:28s
epoch 94 | loss: 0.59987 | val_0_rmse: 0.77251 | val_1_rmse: 0.81199 |  0:00:28s
epoch 95 | loss: 0.59079 | val_0_rmse: 0.7713  | val_1_rmse: 0.82038 |  0:00:29s
epoch 96 | loss: 0.58584 | val_0_rmse: 0.77354 | val_1_rmse: 0.80857 |  0:00:29s
epoch 97 | loss: 0.59209 | val_0_rmse: 0.78031 | val_1_rmse: 0.81278 |  0:00:29s
epoch 98 | loss: 0.59597 | val_0_rmse: 0.7809  | val_1_rmse: 0.80941 |  0:00:30s
epoch 99 | loss: 0.60324 | val_0_rmse: 0.7795  | val_1_rmse: 0.81549 |  0:00:30s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.80293
Best weights from best epoch are automatically used!
ended training at: 05:27:13
Feature importance:
Mean squared error is of 0.04468391115545757
Mean absolute error:0.15380486660707457
MAPE:0.15769355152800577
R2 score:0.30626658445514776
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:27:14
epoch 0  | loss: 2.234   | val_0_rmse: 1.00248 | val_1_rmse: 1.00146 |  0:00:00s
epoch 1  | loss: 1.30917 | val_0_rmse: 1.0029  | val_1_rmse: 1.00239 |  0:00:01s
epoch 2  | loss: 1.08352 | val_0_rmse: 1.00202 | val_1_rmse: 1.00142 |  0:00:01s
epoch 3  | loss: 1.0275  | val_0_rmse: 1.00203 | val_1_rmse: 1.00127 |  0:00:02s
epoch 4  | loss: 1.00966 | val_0_rmse: 1.00234 | val_1_rmse: 1.00184 |  0:00:03s
epoch 5  | loss: 1.00436 | val_0_rmse: 0.99993 | val_1_rmse: 0.99858 |  0:00:03s
epoch 6  | loss: 0.994   | val_0_rmse: 0.98208 | val_1_rmse: 0.97059 |  0:00:04s
epoch 7  | loss: 0.98064 | val_0_rmse: 0.98071 | val_1_rmse: 0.96604 |  0:00:04s
epoch 8  | loss: 0.96725 | val_0_rmse: 0.9752  | val_1_rmse: 0.95856 |  0:00:05s
epoch 9  | loss: 0.95853 | val_0_rmse: 0.97559 | val_1_rmse: 0.95954 |  0:00:06s
epoch 10 | loss: 0.95292 | val_0_rmse: 0.97507 | val_1_rmse: 0.95763 |  0:00:06s
epoch 11 | loss: 0.95701 | val_0_rmse: 0.97731 | val_1_rmse: 0.95687 |  0:00:07s
epoch 12 | loss: 0.95628 | val_0_rmse: 0.98034 | val_1_rmse: 0.96683 |  0:00:08s
epoch 13 | loss: 0.95549 | val_0_rmse: 0.97006 | val_1_rmse: 0.95635 |  0:00:08s
epoch 14 | loss: 0.95389 | val_0_rmse: 0.9692  | val_1_rmse: 0.95409 |  0:00:09s
epoch 15 | loss: 0.95613 | val_0_rmse: 0.96781 | val_1_rmse: 0.95403 |  0:00:09s
epoch 16 | loss: 0.95155 | val_0_rmse: 0.96963 | val_1_rmse: 0.95736 |  0:00:10s
epoch 17 | loss: 0.94831 | val_0_rmse: 0.96787 | val_1_rmse: 0.95285 |  0:00:11s
epoch 18 | loss: 0.94701 | val_0_rmse: 0.96562 | val_1_rmse: 0.95237 |  0:00:11s
epoch 19 | loss: 0.94249 | val_0_rmse: 0.96962 | val_1_rmse: 0.95908 |  0:00:12s
epoch 20 | loss: 0.94149 | val_0_rmse: 0.96695 | val_1_rmse: 0.95195 |  0:00:12s
epoch 21 | loss: 0.94092 | val_0_rmse: 0.96742 | val_1_rmse: 0.95371 |  0:00:13s
epoch 22 | loss: 0.94141 | val_0_rmse: 0.97129 | val_1_rmse: 0.96198 |  0:00:14s
epoch 23 | loss: 0.94103 | val_0_rmse: 0.96946 | val_1_rmse: 0.95659 |  0:00:14s
epoch 24 | loss: 0.9341  | val_0_rmse: 0.96798 | val_1_rmse: 0.95639 |  0:00:15s
epoch 25 | loss: 0.93092 | val_0_rmse: 0.9677  | val_1_rmse: 0.95373 |  0:00:16s
epoch 26 | loss: 0.93289 | val_0_rmse: 0.96484 | val_1_rmse: 0.95471 |  0:00:16s
epoch 27 | loss: 0.9277  | val_0_rmse: 0.96408 | val_1_rmse: 0.95633 |  0:00:17s
epoch 28 | loss: 0.92655 | val_0_rmse: 0.96134 | val_1_rmse: 0.95332 |  0:00:17s
epoch 29 | loss: 0.92483 | val_0_rmse: 0.96291 | val_1_rmse: 0.95758 |  0:00:18s
epoch 30 | loss: 0.92028 | val_0_rmse: 0.95474 | val_1_rmse: 0.94826 |  0:00:19s
epoch 31 | loss: 0.92259 | val_0_rmse: 0.94528 | val_1_rmse: 0.93959 |  0:00:19s
epoch 32 | loss: 0.91187 | val_0_rmse: 0.95079 | val_1_rmse: 0.94714 |  0:00:20s
epoch 33 | loss: 0.90845 | val_0_rmse: 0.94892 | val_1_rmse: 0.94804 |  0:00:20s
epoch 34 | loss: 0.90347 | val_0_rmse: 0.94534 | val_1_rmse: 0.95142 |  0:00:21s
epoch 35 | loss: 0.89469 | val_0_rmse: 0.93542 | val_1_rmse: 0.94796 |  0:00:22s
epoch 36 | loss: 0.87777 | val_0_rmse: 0.93064 | val_1_rmse: 0.94474 |  0:00:22s
epoch 37 | loss: 0.87171 | val_0_rmse: 0.91933 | val_1_rmse: 0.94476 |  0:00:23s
epoch 38 | loss: 0.85337 | val_0_rmse: 0.92103 | val_1_rmse: 0.93853 |  0:00:24s
epoch 39 | loss: 0.82856 | val_0_rmse: 0.91054 | val_1_rmse: 0.92637 |  0:00:24s
epoch 40 | loss: 0.86264 | val_0_rmse: 0.90563 | val_1_rmse: 0.94432 |  0:00:25s
epoch 41 | loss: 0.85818 | val_0_rmse: 0.93504 | val_1_rmse: 0.94407 |  0:00:25s
epoch 42 | loss: 0.87617 | val_0_rmse: 0.91604 | val_1_rmse: 0.90853 |  0:00:26s
epoch 43 | loss: 0.86227 | val_0_rmse: 0.90668 | val_1_rmse: 0.94353 |  0:00:27s
epoch 44 | loss: 0.85038 | val_0_rmse: 0.89732 | val_1_rmse: 0.9455  |  0:00:27s
epoch 45 | loss: 0.83441 | val_0_rmse: 0.92959 | val_1_rmse: 0.9293  |  0:00:28s
epoch 46 | loss: 0.98662 | val_0_rmse: 0.89181 | val_1_rmse: 0.94952 |  0:00:28s
epoch 47 | loss: 0.87795 | val_0_rmse: 0.94186 | val_1_rmse: 0.94045 |  0:00:29s
epoch 48 | loss: 0.89282 | val_0_rmse: 0.92917 | val_1_rmse: 0.9337  |  0:00:30s
epoch 49 | loss: 0.89153 | val_0_rmse: 0.91833 | val_1_rmse: 0.92173 |  0:00:30s
epoch 50 | loss: 0.87629 | val_0_rmse: 0.92448 | val_1_rmse: 0.92514 |  0:00:31s
epoch 51 | loss: 0.8736  | val_0_rmse: 0.91943 | val_1_rmse: 0.91427 |  0:00:32s
epoch 52 | loss: 0.86771 | val_0_rmse: 0.92313 | val_1_rmse: 0.94361 |  0:00:32s
epoch 53 | loss: 0.87764 | val_0_rmse: 0.93193 | val_1_rmse: 0.94333 |  0:00:33s
epoch 54 | loss: 0.88306 | val_0_rmse: 0.93138 | val_1_rmse: 0.94201 |  0:00:33s
epoch 55 | loss: 0.87181 | val_0_rmse: 0.92606 | val_1_rmse: 0.95057 |  0:00:34s
epoch 56 | loss: 0.88438 | val_0_rmse: 0.92732 | val_1_rmse: 0.9407  |  0:00:35s
epoch 57 | loss: 0.87771 | val_0_rmse: 0.92909 | val_1_rmse: 0.93871 |  0:00:35s
epoch 58 | loss: 0.88159 | val_0_rmse: 0.99857 | val_1_rmse: 1.00065 |  0:00:36s
epoch 59 | loss: 0.86411 | val_0_rmse: 0.91558 | val_1_rmse: 0.94473 |  0:00:36s
epoch 60 | loss: 0.83076 | val_0_rmse: 0.90232 | val_1_rmse: 0.94392 |  0:00:37s
epoch 61 | loss: 0.81076 | val_0_rmse: 0.86694 | val_1_rmse: 0.96075 |  0:00:38s
epoch 62 | loss: 0.7699  | val_0_rmse: 0.87082 | val_1_rmse: 0.95894 |  0:00:38s
epoch 63 | loss: 0.796   | val_0_rmse: 0.88552 | val_1_rmse: 0.9538  |  0:00:39s
epoch 64 | loss: 0.77155 | val_0_rmse: 0.99033 | val_1_rmse: 1.0442  |  0:00:40s
epoch 65 | loss: 0.80316 | val_0_rmse: 0.87675 | val_1_rmse: 1.09183 |  0:00:40s
epoch 66 | loss: 0.77794 | val_0_rmse: 0.86092 | val_1_rmse: 0.98771 |  0:00:41s
epoch 67 | loss: 0.7757  | val_0_rmse: 0.86176 | val_1_rmse: 1.01469 |  0:00:41s
epoch 68 | loss: 0.75014 | val_0_rmse: 0.86727 | val_1_rmse: 0.97457 |  0:00:42s
epoch 69 | loss: 0.74495 | val_0_rmse: 0.96671 | val_1_rmse: 1.0622  |  0:00:43s
epoch 70 | loss: 0.72131 | val_0_rmse: 0.95439 | val_1_rmse: 0.95694 |  0:00:43s
epoch 71 | loss: 0.81656 | val_0_rmse: 0.91021 | val_1_rmse: 0.96125 |  0:00:44s
epoch 72 | loss: 0.75859 | val_0_rmse: 0.87781 | val_1_rmse: 0.96848 |  0:00:44s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.90853
Best weights from best epoch are automatically used!
ended training at: 05:27:59
Feature importance:
Mean squared error is of 0.07915603438936583
Mean absolute error:0.1926164183069579
MAPE:0.20119011874371243
R2 score:0.12063220102917616
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:28:00
epoch 0  | loss: 3.37529 | val_0_rmse: 0.99444 | val_1_rmse: 1.11852 |  0:00:00s
epoch 1  | loss: 2.20545 | val_0_rmse: 0.99354 | val_1_rmse: 1.12128 |  0:00:00s
epoch 2  | loss: 2.14403 | val_0_rmse: 0.99431 | val_1_rmse: 1.13219 |  0:00:00s
epoch 3  | loss: 1.56378 | val_0_rmse: 0.99284 | val_1_rmse: 1.1299  |  0:00:00s
epoch 4  | loss: 1.39538 | val_0_rmse: 0.99272 | val_1_rmse: 1.13024 |  0:00:00s
epoch 5  | loss: 1.24177 | val_0_rmse: 0.99346 | val_1_rmse: 1.12994 |  0:00:00s
epoch 6  | loss: 1.18288 | val_0_rmse: 0.9945  | val_1_rmse: 1.13219 |  0:00:00s
epoch 7  | loss: 1.43566 | val_0_rmse: 0.99431 | val_1_rmse: 1.13117 |  0:00:00s
epoch 8  | loss: 1.29305 | val_0_rmse: 0.993   | val_1_rmse: 1.12946 |  0:00:00s
epoch 9  | loss: 1.17576 | val_0_rmse: 0.99268 | val_1_rmse: 1.12858 |  0:00:00s
epoch 10 | loss: 1.11684 | val_0_rmse: 0.99274 | val_1_rmse: 1.1292  |  0:00:00s
epoch 11 | loss: 1.10179 | val_0_rmse: 0.99353 | val_1_rmse: 1.13159 |  0:00:00s
epoch 12 | loss: 1.01815 | val_0_rmse: 0.99272 | val_1_rmse: 1.12997 |  0:00:01s
epoch 13 | loss: 1.06674 | val_0_rmse: 0.99241 | val_1_rmse: 1.12767 |  0:00:01s
epoch 14 | loss: 1.07036 | val_0_rmse: 0.99211 | val_1_rmse: 1.12647 |  0:00:01s
epoch 15 | loss: 1.00157 | val_0_rmse: 0.99197 | val_1_rmse: 1.12826 |  0:00:01s
epoch 16 | loss: 1.04315 | val_0_rmse: 0.99379 | val_1_rmse: 1.13315 |  0:00:01s
epoch 17 | loss: 1.0023  | val_0_rmse: 0.9947  | val_1_rmse: 1.13508 |  0:00:01s
epoch 18 | loss: 1.01669 | val_0_rmse: 0.99316 | val_1_rmse: 1.13199 |  0:00:01s
epoch 19 | loss: 0.98595 | val_0_rmse: 0.99189 | val_1_rmse: 1.12744 |  0:00:01s
epoch 20 | loss: 0.98035 | val_0_rmse: 0.99135 | val_1_rmse: 1.1242  |  0:00:01s
epoch 21 | loss: 1.00032 | val_0_rmse: 0.99117 | val_1_rmse: 1.12291 |  0:00:01s
epoch 22 | loss: 0.98258 | val_0_rmse: 0.99108 | val_1_rmse: 1.12168 |  0:00:01s
epoch 23 | loss: 0.96427 | val_0_rmse: 0.99102 | val_1_rmse: 1.12082 |  0:00:01s
epoch 24 | loss: 0.96676 | val_0_rmse: 0.99091 | val_1_rmse: 1.1201  |  0:00:02s
epoch 25 | loss: 0.97193 | val_0_rmse: 0.99061 | val_1_rmse: 1.11991 |  0:00:02s
epoch 26 | loss: 0.97211 | val_0_rmse: 0.99027 | val_1_rmse: 1.11972 |  0:00:02s
epoch 27 | loss: 0.9728  | val_0_rmse: 0.99012 | val_1_rmse: 1.12064 |  0:00:02s
epoch 28 | loss: 0.95499 | val_0_rmse: 0.99003 | val_1_rmse: 1.12158 |  0:00:02s
epoch 29 | loss: 0.95073 | val_0_rmse: 0.99001 | val_1_rmse: 1.12292 |  0:00:02s
epoch 30 | loss: 0.96404 | val_0_rmse: 0.98965 | val_1_rmse: 1.12326 |  0:00:02s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 1.11852
Best weights from best epoch are automatically used!
ended training at: 05:28:02
Feature importance:
Mean squared error is of 0.07564050091644897
Mean absolute error:0.19888667472821964
MAPE:0.23014514935769367
R2 score:-0.022059625861035714
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:28:03
epoch 0  | loss: 1.87377 | val_0_rmse: 0.99152 | val_1_rmse: 0.99462 |  0:00:00s
epoch 1  | loss: 1.13061 | val_0_rmse: 0.98154 | val_1_rmse: 0.98041 |  0:00:00s
epoch 2  | loss: 1.01663 | val_0_rmse: 0.98219 | val_1_rmse: 0.98251 |  0:00:00s
epoch 3  | loss: 0.98103 | val_0_rmse: 0.97563 | val_1_rmse: 0.96644 |  0:00:01s
epoch 4  | loss: 0.96408 | val_0_rmse: 0.97822 | val_1_rmse: 0.96767 |  0:00:01s
epoch 5  | loss: 0.94527 | val_0_rmse: 0.96274 | val_1_rmse: 0.95278 |  0:00:01s
epoch 6  | loss: 0.92918 | val_0_rmse: 0.96374 | val_1_rmse: 0.95182 |  0:00:01s
epoch 7  | loss: 0.92159 | val_0_rmse: 0.95707 | val_1_rmse: 0.94721 |  0:00:02s
epoch 8  | loss: 0.88757 | val_0_rmse: 0.91939 | val_1_rmse: 0.91739 |  0:00:02s
epoch 9  | loss: 0.85983 | val_0_rmse: 0.91656 | val_1_rmse: 0.91351 |  0:00:02s
epoch 10 | loss: 0.84425 | val_0_rmse: 0.89797 | val_1_rmse: 0.89997 |  0:00:03s
epoch 11 | loss: 0.82236 | val_0_rmse: 0.89258 | val_1_rmse: 0.89852 |  0:00:03s
epoch 12 | loss: 0.83537 | val_0_rmse: 0.8949  | val_1_rmse: 0.89468 |  0:00:03s
epoch 13 | loss: 0.83267 | val_0_rmse: 0.90008 | val_1_rmse: 0.90411 |  0:00:03s
epoch 14 | loss: 0.83762 | val_0_rmse: 0.88983 | val_1_rmse: 0.8876  |  0:00:04s
epoch 15 | loss: 0.80358 | val_0_rmse: 0.89007 | val_1_rmse: 0.88559 |  0:00:04s
epoch 16 | loss: 0.79436 | val_0_rmse: 0.89391 | val_1_rmse: 0.89823 |  0:00:04s
epoch 17 | loss: 0.80173 | val_0_rmse: 0.89265 | val_1_rmse: 0.89383 |  0:00:05s
epoch 18 | loss: 0.79129 | val_0_rmse: 0.89082 | val_1_rmse: 0.89284 |  0:00:05s
epoch 19 | loss: 0.78571 | val_0_rmse: 0.88674 | val_1_rmse: 0.89252 |  0:00:05s
epoch 20 | loss: 0.78273 | val_0_rmse: 0.88829 | val_1_rmse: 0.89404 |  0:00:06s
epoch 21 | loss: 0.78061 | val_0_rmse: 0.88715 | val_1_rmse: 0.892   |  0:00:06s
epoch 22 | loss: 0.78485 | val_0_rmse: 0.8891  | val_1_rmse: 0.89568 |  0:00:06s
epoch 23 | loss: 0.77619 | val_0_rmse: 0.88818 | val_1_rmse: 0.89195 |  0:00:06s
epoch 24 | loss: 0.77959 | val_0_rmse: 0.89039 | val_1_rmse: 0.89806 |  0:00:07s
epoch 25 | loss: 0.78287 | val_0_rmse: 0.88492 | val_1_rmse: 0.88901 |  0:00:07s
epoch 26 | loss: 0.78417 | val_0_rmse: 0.89043 | val_1_rmse: 0.89038 |  0:00:07s
epoch 27 | loss: 0.77781 | val_0_rmse: 0.89769 | val_1_rmse: 0.90266 |  0:00:08s
epoch 28 | loss: 0.78873 | val_0_rmse: 0.8924  | val_1_rmse: 0.8897  |  0:00:08s
epoch 29 | loss: 0.78283 | val_0_rmse: 0.89293 | val_1_rmse: 0.88958 |  0:00:08s
epoch 30 | loss: 0.77505 | val_0_rmse: 0.89165 | val_1_rmse: 0.89355 |  0:00:08s
epoch 31 | loss: 0.77324 | val_0_rmse: 0.89634 | val_1_rmse: 0.89357 |  0:00:09s
epoch 32 | loss: 0.77945 | val_0_rmse: 0.89129 | val_1_rmse: 0.8908  |  0:00:09s
epoch 33 | loss: 0.77464 | val_0_rmse: 0.89249 | val_1_rmse: 0.89645 |  0:00:09s
epoch 34 | loss: 0.77259 | val_0_rmse: 0.89332 | val_1_rmse: 0.89362 |  0:00:09s
epoch 35 | loss: 0.77427 | val_0_rmse: 0.88568 | val_1_rmse: 0.8865  |  0:00:10s
epoch 36 | loss: 0.77064 | val_0_rmse: 0.88455 | val_1_rmse: 0.89282 |  0:00:10s
epoch 37 | loss: 0.76363 | val_0_rmse: 0.88334 | val_1_rmse: 0.88977 |  0:00:10s
epoch 38 | loss: 0.76755 | val_0_rmse: 0.88435 | val_1_rmse: 0.89205 |  0:00:11s
epoch 39 | loss: 0.75938 | val_0_rmse: 0.88431 | val_1_rmse: 0.88848 |  0:00:11s
epoch 40 | loss: 0.76351 | val_0_rmse: 0.88243 | val_1_rmse: 0.88213 |  0:00:11s
epoch 41 | loss: 0.764   | val_0_rmse: 0.87919 | val_1_rmse: 0.88656 |  0:00:12s
epoch 42 | loss: 0.76694 | val_0_rmse: 0.87804 | val_1_rmse: 0.88475 |  0:00:12s
epoch 43 | loss: 0.76048 | val_0_rmse: 0.8791  | val_1_rmse: 0.88327 |  0:00:12s
epoch 44 | loss: 0.76067 | val_0_rmse: 0.88295 | val_1_rmse: 0.8883  |  0:00:12s
epoch 45 | loss: 0.76394 | val_0_rmse: 0.88602 | val_1_rmse: 0.89525 |  0:00:13s
epoch 46 | loss: 0.74826 | val_0_rmse: 0.88547 | val_1_rmse: 0.89466 |  0:00:13s
epoch 47 | loss: 0.7585  | val_0_rmse: 0.89145 | val_1_rmse: 0.90459 |  0:00:13s
epoch 48 | loss: 0.75695 | val_0_rmse: 0.89061 | val_1_rmse: 0.89827 |  0:00:13s
epoch 49 | loss: 0.75894 | val_0_rmse: 0.88936 | val_1_rmse: 0.89834 |  0:00:14s
epoch 50 | loss: 0.76668 | val_0_rmse: 0.88015 | val_1_rmse: 0.88636 |  0:00:14s
epoch 51 | loss: 0.75706 | val_0_rmse: 0.87876 | val_1_rmse: 0.88338 |  0:00:14s
epoch 52 | loss: 0.76493 | val_0_rmse: 0.88566 | val_1_rmse: 0.88859 |  0:00:15s
epoch 53 | loss: 0.76475 | val_0_rmse: 0.88016 | val_1_rmse: 0.88309 |  0:00:15s
epoch 54 | loss: 0.76079 | val_0_rmse: 0.88111 | val_1_rmse: 0.88895 |  0:00:15s
epoch 55 | loss: 0.75783 | val_0_rmse: 0.88315 | val_1_rmse: 0.89354 |  0:00:15s
epoch 56 | loss: 0.75777 | val_0_rmse: 0.90752 | val_1_rmse: 0.88537 |  0:00:16s
epoch 57 | loss: 0.75848 | val_0_rmse: 0.90832 | val_1_rmse: 0.88719 |  0:00:16s
epoch 58 | loss: 0.74758 | val_0_rmse: 0.91261 | val_1_rmse: 0.89414 |  0:00:16s
epoch 59 | loss: 0.76167 | val_0_rmse: 0.93062 | val_1_rmse: 0.88747 |  0:00:17s
epoch 60 | loss: 0.7674  | val_0_rmse: 0.87892 | val_1_rmse: 0.88556 |  0:00:17s
epoch 61 | loss: 0.76281 | val_0_rmse: 0.87855 | val_1_rmse: 0.88436 |  0:00:17s
epoch 62 | loss: 0.75599 | val_0_rmse: 0.87455 | val_1_rmse: 0.87852 |  0:00:17s
epoch 63 | loss: 0.75804 | val_0_rmse: 0.86989 | val_1_rmse: 0.87513 |  0:00:18s
epoch 64 | loss: 0.75441 | val_0_rmse: 0.86984 | val_1_rmse: 0.87351 |  0:00:18s
epoch 65 | loss: 0.74987 | val_0_rmse: 0.87067 | val_1_rmse: 0.87481 |  0:00:18s
epoch 66 | loss: 0.75248 | val_0_rmse: 0.86926 | val_1_rmse: 0.87642 |  0:00:19s
epoch 67 | loss: 0.75129 | val_0_rmse: 0.87263 | val_1_rmse: 0.88333 |  0:00:19s
epoch 68 | loss: 0.75151 | val_0_rmse: 0.87248 | val_1_rmse: 0.88179 |  0:00:19s
epoch 69 | loss: 0.75456 | val_0_rmse: 0.87099 | val_1_rmse: 0.88012 |  0:00:19s
epoch 70 | loss: 0.7477  | val_0_rmse: 0.8699  | val_1_rmse: 0.87987 |  0:00:20s
epoch 71 | loss: 0.74769 | val_0_rmse: 0.87458 | val_1_rmse: 0.88307 |  0:00:20s
epoch 72 | loss: 0.74849 | val_0_rmse: 0.87329 | val_1_rmse: 0.88556 |  0:00:20s
epoch 73 | loss: 0.74776 | val_0_rmse: 0.87528 | val_1_rmse: 0.88554 |  0:00:21s
epoch 74 | loss: 0.74481 | val_0_rmse: 0.87348 | val_1_rmse: 0.88323 |  0:00:21s
epoch 75 | loss: 0.74768 | val_0_rmse: 0.87237 | val_1_rmse: 0.88119 |  0:00:21s
epoch 76 | loss: 0.74399 | val_0_rmse: 0.87157 | val_1_rmse: 0.87934 |  0:00:21s
epoch 77 | loss: 0.75093 | val_0_rmse: 0.87129 | val_1_rmse: 0.88368 |  0:00:22s
epoch 78 | loss: 0.75827 | val_0_rmse: 0.87031 | val_1_rmse: 0.88119 |  0:00:22s
epoch 79 | loss: 0.74763 | val_0_rmse: 0.87077 | val_1_rmse: 0.88068 |  0:00:22s
epoch 80 | loss: 0.74287 | val_0_rmse: 0.86738 | val_1_rmse: 0.88395 |  0:00:23s
epoch 81 | loss: 0.75411 | val_0_rmse: 0.86732 | val_1_rmse: 0.8846  |  0:00:23s
epoch 82 | loss: 0.74987 | val_0_rmse: 0.87411 | val_1_rmse: 0.8843  |  0:00:23s
epoch 83 | loss: 0.7523  | val_0_rmse: 0.87189 | val_1_rmse: 0.88905 |  0:00:23s
epoch 84 | loss: 0.74613 | val_0_rmse: 0.87187 | val_1_rmse: 0.88832 |  0:00:24s
epoch 85 | loss: 0.74472 | val_0_rmse: 0.86909 | val_1_rmse: 0.88082 |  0:00:24s
epoch 86 | loss: 0.74348 | val_0_rmse: 0.86845 | val_1_rmse: 0.8758  |  0:00:24s
epoch 87 | loss: 0.73995 | val_0_rmse: 0.86704 | val_1_rmse: 0.87952 |  0:00:25s
epoch 88 | loss: 0.74372 | val_0_rmse: 0.86714 | val_1_rmse: 0.88553 |  0:00:25s
epoch 89 | loss: 0.74151 | val_0_rmse: 0.86642 | val_1_rmse: 0.88588 |  0:00:25s
epoch 90 | loss: 0.74422 | val_0_rmse: 0.86425 | val_1_rmse: 0.88333 |  0:00:25s
epoch 91 | loss: 0.74009 | val_0_rmse: 0.8633  | val_1_rmse: 0.87802 |  0:00:26s
epoch 92 | loss: 0.7352  | val_0_rmse: 0.86586 | val_1_rmse: 0.87801 |  0:00:26s
epoch 93 | loss: 0.73827 | val_0_rmse: 0.86769 | val_1_rmse: 0.88515 |  0:00:26s
epoch 94 | loss: 0.74704 | val_0_rmse: 0.86403 | val_1_rmse: 0.87898 |  0:00:27s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.87351
Best weights from best epoch are automatically used!
ended training at: 05:28:30
Feature importance:
Mean squared error is of 0.07650688256660935
Mean absolute error:0.2013456569161232
MAPE:0.22090328158045205
R2 score:0.2174272593748504
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:28:39
epoch 0  | loss: 1.06756 | val_0_rmse: 1.00509 | val_1_rmse: 0.96933 |  0:00:22s
epoch 1  | loss: 0.9234  | val_0_rmse: 0.91571 | val_1_rmse: 0.8829  |  0:00:43s
epoch 2  | loss: 0.82321 | val_0_rmse: 0.87734 | val_1_rmse: 0.84996 |  0:01:05s
epoch 3  | loss: 0.7633  | val_0_rmse: 0.85614 | val_1_rmse: 0.83414 |  0:01:26s
epoch 4  | loss: 0.74955 | val_0_rmse: 0.89696 | val_1_rmse: 0.88071 |  0:01:48s
epoch 5  | loss: 0.72829 | val_0_rmse: 0.8292  | val_1_rmse: 0.82566 |  0:02:10s
epoch 6  | loss: 0.69941 | val_0_rmse: 0.82481 | val_1_rmse: 0.84318 |  0:02:31s
epoch 7  | loss: 0.69655 | val_0_rmse: 0.81529 | val_1_rmse: 0.86228 |  0:02:53s
epoch 8  | loss: 0.67375 | val_0_rmse: 0.81052 | val_1_rmse: 0.99026 |  0:03:14s
epoch 9  | loss: 0.66934 | val_0_rmse: 0.81328 | val_1_rmse: 1.52922 |  0:03:36s
epoch 10 | loss: 0.65627 | val_0_rmse: 0.80995 | val_1_rmse: 2.70327 |  0:03:57s
epoch 11 | loss: 0.65359 | val_0_rmse: 0.80504 | val_1_rmse: 3.28391 |  0:04:18s
epoch 12 | loss: 0.64776 | val_0_rmse: 0.80366 | val_1_rmse: 4.37518 |  0:04:39s
epoch 13 | loss: 0.64204 | val_0_rmse: 0.81289 | val_1_rmse: 4.89753 |  0:05:01s
epoch 14 | loss: 0.64589 | val_0_rmse: 0.79862 | val_1_rmse: 6.34566 |  0:05:22s
epoch 15 | loss: 0.63032 | val_0_rmse: 0.79778 | val_1_rmse: 4.9341  |  0:05:43s
epoch 16 | loss: 0.62372 | val_0_rmse: 0.78626 | val_1_rmse: 5.32622 |  0:06:05s
epoch 17 | loss: 0.61995 | val_0_rmse: 0.78568 | val_1_rmse: 7.20061 |  0:06:26s
epoch 18 | loss: 0.62888 | val_0_rmse: 0.80273 | val_1_rmse: 7.71992 |  0:06:47s
epoch 19 | loss: 0.6202  | val_0_rmse: 1.07754 | val_1_rmse: 0.99152 |  0:07:09s
epoch 20 | loss: 0.64073 | val_0_rmse: 0.79919 | val_1_rmse: 0.82886 |  0:07:30s
epoch 21 | loss: 0.61895 | val_0_rmse: 0.80659 | val_1_rmse: 0.81632 |  0:07:52s
epoch 22 | loss: 0.60889 | val_0_rmse: 0.81628 | val_1_rmse: 0.85661 |  0:08:13s
epoch 23 | loss: 0.60999 | val_0_rmse: 0.78172 | val_1_rmse: 0.81355 |  0:08:34s
epoch 24 | loss: 0.62085 | val_0_rmse: 0.81569 | val_1_rmse: 0.83222 |  0:08:56s
epoch 25 | loss: 0.60874 | val_0_rmse: 0.78868 | val_1_rmse: 0.83114 |  0:09:17s
epoch 26 | loss: 0.602   | val_0_rmse: 0.78974 | val_1_rmse: 0.82804 |  0:09:38s
epoch 27 | loss: 0.6044  | val_0_rmse: 0.80971 | val_1_rmse: 0.85878 |  0:09:59s
epoch 28 | loss: 0.59767 | val_0_rmse: 0.77845 | val_1_rmse: 0.81486 |  0:10:21s
epoch 29 | loss: 0.58866 | val_0_rmse: 0.83749 | val_1_rmse: 0.89447 |  0:10:42s
epoch 30 | loss: 0.59233 | val_0_rmse: 0.78502 | val_1_rmse: 0.83276 |  0:11:03s
epoch 31 | loss: 0.58868 | val_0_rmse: 0.90213 | val_1_rmse: 0.96991 |  0:11:25s
epoch 32 | loss: 0.58515 | val_0_rmse: 0.86287 | val_1_rmse: 0.91492 |  0:11:46s
epoch 33 | loss: 0.58339 | val_0_rmse: 0.80788 | val_1_rmse: 0.85859 |  0:12:07s
epoch 34 | loss: 0.59323 | val_0_rmse: 0.77557 | val_1_rmse: 0.82599 |  0:12:29s
epoch 35 | loss: 0.58526 | val_0_rmse: 0.76704 | val_1_rmse: 0.82768 |  0:12:50s
epoch 36 | loss: 0.57251 | val_0_rmse: 0.78013 | val_1_rmse: 0.81788 |  0:13:11s
epoch 37 | loss: 0.56865 | val_0_rmse: 0.76714 | val_1_rmse: 0.83201 |  0:13:33s
epoch 38 | loss: 0.56621 | val_0_rmse: 0.77519 | val_1_rmse: 0.81911 |  0:13:54s
epoch 39 | loss: 0.56481 | val_0_rmse: 0.77326 | val_1_rmse: 0.81906 |  0:14:15s
epoch 40 | loss: 0.56373 | val_0_rmse: 0.80579 | val_1_rmse: 0.84044 |  0:14:37s
epoch 41 | loss: 0.56313 | val_0_rmse: 0.80616 | val_1_rmse: 0.87855 |  0:14:58s
epoch 42 | loss: 0.55824 | val_0_rmse: 0.78795 | val_1_rmse: 0.85406 |  0:15:19s
epoch 43 | loss: 0.56083 | val_0_rmse: 0.77724 | val_1_rmse: 0.83552 |  0:15:41s
epoch 44 | loss: 0.58945 | val_0_rmse: 1.31375 | val_1_rmse: 0.85511 |  0:16:03s
epoch 45 | loss: 0.58009 | val_0_rmse: 0.83368 | val_1_rmse: 0.87149 |  0:16:25s
epoch 46 | loss: 0.57277 | val_0_rmse: 0.785   | val_1_rmse: 0.84759 |  0:16:46s
epoch 47 | loss: 0.55756 | val_0_rmse: 0.79631 | val_1_rmse: 0.84877 |  0:17:08s
epoch 48 | loss: 0.55909 | val_0_rmse: 0.84485 | val_1_rmse: 0.92643 |  0:17:29s
epoch 49 | loss: 0.55471 | val_0_rmse: 0.7887  | val_1_rmse: 0.88113 |  0:17:51s
epoch 50 | loss: 0.55049 | val_0_rmse: 0.77835 | val_1_rmse: 0.84759 |  0:18:12s
epoch 51 | loss: 0.54393 | val_0_rmse: 0.77898 | val_1_rmse: 0.88389 |  0:18:33s
epoch 52 | loss: 0.54525 | val_0_rmse: 0.77106 | val_1_rmse: 0.83612 |  0:18:55s
epoch 53 | loss: 0.54706 | val_0_rmse: 0.94987 | val_1_rmse: 0.84088 |  0:19:16s

Early stopping occured at epoch 53 with best_epoch = 23 and best_val_1_rmse = 0.81355
Best weights from best epoch are automatically used!
ended training at: 05:48:06
Feature importance:
Mean squared error is of 0.08792183703732194
Mean absolute error:0.2038447612515629
MAPE:0.23291344076840556
R2 score:0.28888226313902476
------------------------------------------------------------------
