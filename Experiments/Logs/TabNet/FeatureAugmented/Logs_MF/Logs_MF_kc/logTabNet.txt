TabNet Logs:

Saving copy of script...
In this script only the kc house data dataset is used and its a continuation of the augmentation testsHere the test done is to test the improvement that the new 10 features provide
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:44:47
epoch 0  | loss: 1.26518 | val_0_rmse: 0.90927 | val_1_rmse: 0.82239 |  0:00:05s
epoch 1  | loss: 0.40945 | val_0_rmse: 0.6765  | val_1_rmse: 0.65593 |  0:00:07s
epoch 2  | loss: 0.32082 | val_0_rmse: 0.56284 | val_1_rmse: 0.54632 |  0:00:08s
epoch 3  | loss: 0.2898  | val_0_rmse: 0.58917 | val_1_rmse: 0.58894 |  0:00:10s
epoch 4  | loss: 0.29305 | val_0_rmse: 0.52924 | val_1_rmse: 0.52567 |  0:00:12s
epoch 5  | loss: 0.27164 | val_0_rmse: 0.5298  | val_1_rmse: 0.52078 |  0:00:13s
epoch 6  | loss: 0.26502 | val_0_rmse: 0.50559 | val_1_rmse: 0.50297 |  0:00:15s
epoch 7  | loss: 0.25238 | val_0_rmse: 0.52154 | val_1_rmse: 0.52172 |  0:00:17s
epoch 8  | loss: 0.25468 | val_0_rmse: 0.54636 | val_1_rmse: 0.55444 |  0:00:18s
epoch 9  | loss: 0.2529  | val_0_rmse: 0.47665 | val_1_rmse: 0.47521 |  0:00:20s
epoch 10 | loss: 0.23392 | val_0_rmse: 0.46518 | val_1_rmse: 0.46461 |  0:00:21s
epoch 11 | loss: 0.23132 | val_0_rmse: 0.49723 | val_1_rmse: 0.49508 |  0:00:23s
epoch 12 | loss: 0.24109 | val_0_rmse: 0.4689  | val_1_rmse: 0.46659 |  0:00:24s
epoch 13 | loss: 0.23753 | val_0_rmse: 0.46331 | val_1_rmse: 0.45849 |  0:00:26s
epoch 14 | loss: 0.22976 | val_0_rmse: 0.47535 | val_1_rmse: 0.47319 |  0:00:27s
epoch 15 | loss: 0.22572 | val_0_rmse: 0.46739 | val_1_rmse: 0.46015 |  0:00:29s
epoch 16 | loss: 0.21687 | val_0_rmse: 0.45072 | val_1_rmse: 0.44518 |  0:00:30s
epoch 17 | loss: 0.21519 | val_0_rmse: 0.44101 | val_1_rmse: 0.43236 |  0:00:31s
epoch 18 | loss: 0.21383 | val_0_rmse: 0.44604 | val_1_rmse: 0.43427 |  0:00:33s
epoch 19 | loss: 0.20217 | val_0_rmse: 0.43236 | val_1_rmse: 0.42444 |  0:00:34s
epoch 20 | loss: 0.20304 | val_0_rmse: 0.44041 | val_1_rmse: 0.43172 |  0:00:35s
epoch 21 | loss: 0.2012  | val_0_rmse: 0.42649 | val_1_rmse: 0.42199 |  0:00:37s
epoch 22 | loss: 0.19475 | val_0_rmse: 0.43105 | val_1_rmse: 0.42672 |  0:00:38s
epoch 23 | loss: 0.20089 | val_0_rmse: 0.43099 | val_1_rmse: 0.42297 |  0:00:40s
epoch 24 | loss: 0.19606 | val_0_rmse: 0.42329 | val_1_rmse: 0.4139  |  0:00:41s
epoch 25 | loss: 0.1905  | val_0_rmse: 0.41703 | val_1_rmse: 0.40854 |  0:00:42s
epoch 26 | loss: 0.19107 | val_0_rmse: 0.42471 | val_1_rmse: 0.41986 |  0:00:44s
epoch 27 | loss: 0.18815 | val_0_rmse: 0.41431 | val_1_rmse: 0.41106 |  0:00:45s
epoch 28 | loss: 0.19141 | val_0_rmse: 0.41368 | val_1_rmse: 0.41448 |  0:00:46s
epoch 29 | loss: 0.18713 | val_0_rmse: 0.41135 | val_1_rmse: 0.40841 |  0:00:48s
epoch 30 | loss: 0.18789 | val_0_rmse: 0.4098  | val_1_rmse: 0.4036  |  0:00:49s
epoch 31 | loss: 0.18084 | val_0_rmse: 0.40754 | val_1_rmse: 0.40837 |  0:00:50s
epoch 32 | loss: 0.1807  | val_0_rmse: 0.40488 | val_1_rmse: 0.40511 |  0:00:52s
epoch 33 | loss: 0.1849  | val_0_rmse: 0.41259 | val_1_rmse: 0.41008 |  0:00:53s
epoch 34 | loss: 0.18526 | val_0_rmse: 0.40721 | val_1_rmse: 0.40714 |  0:00:54s
epoch 35 | loss: 0.17902 | val_0_rmse: 0.406   | val_1_rmse: 0.40453 |  0:00:56s
epoch 36 | loss: 0.17677 | val_0_rmse: 0.39401 | val_1_rmse: 0.39284 |  0:00:57s
epoch 37 | loss: 0.17639 | val_0_rmse: 0.39728 | val_1_rmse: 0.40112 |  0:00:58s
epoch 38 | loss: 0.18025 | val_0_rmse: 0.39333 | val_1_rmse: 0.39432 |  0:01:00s
epoch 39 | loss: 0.17086 | val_0_rmse: 0.39144 | val_1_rmse: 0.3938  |  0:01:01s
epoch 40 | loss: 0.17332 | val_0_rmse: 0.40281 | val_1_rmse: 0.4056  |  0:01:03s
epoch 41 | loss: 0.18018 | val_0_rmse: 0.38723 | val_1_rmse: 0.39012 |  0:01:04s
epoch 42 | loss: 0.17986 | val_0_rmse: 0.40754 | val_1_rmse: 0.40028 |  0:01:05s
epoch 43 | loss: 0.17937 | val_0_rmse: 0.41503 | val_1_rmse: 0.42546 |  0:01:07s
epoch 44 | loss: 0.1758  | val_0_rmse: 0.41309 | val_1_rmse: 0.40976 |  0:01:08s
epoch 45 | loss: 0.17761 | val_0_rmse: 0.42579 | val_1_rmse: 0.42578 |  0:01:09s
epoch 46 | loss: 0.1833  | val_0_rmse: 0.40564 | val_1_rmse: 0.41161 |  0:01:10s
epoch 47 | loss: 0.17516 | val_0_rmse: 0.40083 | val_1_rmse: 0.40414 |  0:01:12s
epoch 48 | loss: 0.1736  | val_0_rmse: 0.40666 | val_1_rmse: 0.41536 |  0:01:13s
epoch 49 | loss: 0.16713 | val_0_rmse: 0.3879  | val_1_rmse: 0.38815 |  0:01:14s
epoch 50 | loss: 0.17013 | val_0_rmse: 0.4008  | val_1_rmse: 0.39711 |  0:01:15s
epoch 51 | loss: 0.18153 | val_0_rmse: 0.39808 | val_1_rmse: 0.39881 |  0:01:17s
epoch 52 | loss: 0.17815 | val_0_rmse: 0.40011 | val_1_rmse: 0.40481 |  0:01:18s
epoch 53 | loss: 0.17584 | val_0_rmse: 0.39223 | val_1_rmse: 0.38749 |  0:01:19s
epoch 54 | loss: 0.17098 | val_0_rmse: 0.43387 | val_1_rmse: 0.43057 |  0:01:21s
epoch 55 | loss: 0.1874  | val_0_rmse: 0.4265  | val_1_rmse: 0.42115 |  0:01:22s
epoch 56 | loss: 0.17831 | val_0_rmse: 0.41814 | val_1_rmse: 0.42276 |  0:01:23s
epoch 57 | loss: 0.17576 | val_0_rmse: 0.38915 | val_1_rmse: 0.38821 |  0:01:24s
epoch 58 | loss: 0.1724  | val_0_rmse: 0.40284 | val_1_rmse: 0.39836 |  0:01:26s
epoch 59 | loss: 0.16848 | val_0_rmse: 0.38572 | val_1_rmse: 0.3871  |  0:01:27s
epoch 60 | loss: 0.16781 | val_0_rmse: 0.38271 | val_1_rmse: 0.38655 |  0:01:28s
epoch 61 | loss: 0.1661  | val_0_rmse: 0.38209 | val_1_rmse: 0.39075 |  0:01:29s
epoch 62 | loss: 0.16112 | val_0_rmse: 0.39032 | val_1_rmse: 0.39197 |  0:01:31s
epoch 63 | loss: 0.16226 | val_0_rmse: 0.38862 | val_1_rmse: 0.39564 |  0:01:32s
epoch 64 | loss: 0.16137 | val_0_rmse: 0.37889 | val_1_rmse: 0.38562 |  0:01:33s
epoch 65 | loss: 0.16521 | val_0_rmse: 0.39176 | val_1_rmse: 0.39923 |  0:01:34s
epoch 66 | loss: 0.16319 | val_0_rmse: 0.3926  | val_1_rmse: 0.39188 |  0:01:36s
epoch 67 | loss: 0.16416 | val_0_rmse: 0.38191 | val_1_rmse: 0.38759 |  0:01:37s
epoch 68 | loss: 0.15924 | val_0_rmse: 0.38301 | val_1_rmse: 0.39221 |  0:01:38s
epoch 69 | loss: 0.15852 | val_0_rmse: 0.37433 | val_1_rmse: 0.38264 |  0:01:39s
epoch 70 | loss: 0.1601  | val_0_rmse: 0.38225 | val_1_rmse: 0.38429 |  0:01:41s
epoch 71 | loss: 0.1601  | val_0_rmse: 0.38734 | val_1_rmse: 0.39599 |  0:01:42s
epoch 72 | loss: 0.16168 | val_0_rmse: 0.38407 | val_1_rmse: 0.39424 |  0:01:43s
epoch 73 | loss: 0.16054 | val_0_rmse: 0.37585 | val_1_rmse: 0.38568 |  0:01:45s
epoch 74 | loss: 0.16314 | val_0_rmse: 0.37999 | val_1_rmse: 0.38746 |  0:01:46s
epoch 75 | loss: 0.16337 | val_0_rmse: 0.37603 | val_1_rmse: 0.38524 |  0:01:47s
epoch 76 | loss: 0.16406 | val_0_rmse: 0.38021 | val_1_rmse: 0.38752 |  0:01:48s
epoch 77 | loss: 0.16555 | val_0_rmse: 0.41615 | val_1_rmse: 0.41755 |  0:01:50s
epoch 78 | loss: 0.16416 | val_0_rmse: 0.38466 | val_1_rmse: 0.39154 |  0:01:51s
epoch 79 | loss: 0.16416 | val_0_rmse: 0.37785 | val_1_rmse: 0.38469 |  0:01:52s
epoch 80 | loss: 0.1617  | val_0_rmse: 0.37103 | val_1_rmse: 0.38117 |  0:01:53s
epoch 81 | loss: 0.15476 | val_0_rmse: 0.37846 | val_1_rmse: 0.38533 |  0:01:55s
epoch 82 | loss: 0.1562  | val_0_rmse: 0.37685 | val_1_rmse: 0.38428 |  0:01:56s
epoch 83 | loss: 0.15347 | val_0_rmse: 0.37002 | val_1_rmse: 0.37336 |  0:01:57s
epoch 84 | loss: 0.15109 | val_0_rmse: 0.36239 | val_1_rmse: 0.3724  |  0:01:58s
epoch 85 | loss: 0.15056 | val_0_rmse: 0.36886 | val_1_rmse: 0.38282 |  0:02:00s
epoch 86 | loss: 0.15221 | val_0_rmse: 0.37022 | val_1_rmse: 0.37632 |  0:02:01s
epoch 87 | loss: 0.15897 | val_0_rmse: 0.38387 | val_1_rmse: 0.38746 |  0:02:02s
epoch 88 | loss: 0.16293 | val_0_rmse: 0.37798 | val_1_rmse: 0.3881  |  0:02:04s
epoch 89 | loss: 0.1549  | val_0_rmse: 0.3671  | val_1_rmse: 0.37309 |  0:02:05s
epoch 90 | loss: 0.14785 | val_0_rmse: 0.373   | val_1_rmse: 0.38509 |  0:02:06s
epoch 91 | loss: 0.15217 | val_0_rmse: 0.36972 | val_1_rmse: 0.38343 |  0:02:07s
epoch 92 | loss: 0.15324 | val_0_rmse: 0.36347 | val_1_rmse: 0.37435 |  0:02:09s
epoch 93 | loss: 0.14682 | val_0_rmse: 0.36715 | val_1_rmse: 0.37568 |  0:02:10s
epoch 94 | loss: 0.15573 | val_0_rmse: 0.3672  | val_1_rmse: 0.38216 |  0:02:11s
epoch 95 | loss: 0.14931 | val_0_rmse: 0.35874 | val_1_rmse: 0.37269 |  0:02:12s
epoch 96 | loss: 0.14715 | val_0_rmse: 0.36526 | val_1_rmse: 0.37813 |  0:02:14s
epoch 97 | loss: 0.14729 | val_0_rmse: 0.37215 | val_1_rmse: 0.38238 |  0:02:15s
epoch 98 | loss: 0.16643 | val_0_rmse: 0.47394 | val_1_rmse: 0.47117 |  0:02:16s
epoch 99 | loss: 0.18118 | val_0_rmse: 0.47331 | val_1_rmse: 0.47613 |  0:02:17s
epoch 100| loss: 0.19344 | val_0_rmse: 0.43201 | val_1_rmse: 0.44086 |  0:02:19s
epoch 101| loss: 0.1835  | val_0_rmse: 0.42659 | val_1_rmse: 0.42725 |  0:02:20s
epoch 102| loss: 0.17929 | val_0_rmse: 0.40344 | val_1_rmse: 0.40776 |  0:02:21s
epoch 103| loss: 0.17589 | val_0_rmse: 0.47211 | val_1_rmse: 0.47141 |  0:02:22s
epoch 104| loss: 0.21556 | val_0_rmse: 0.46491 | val_1_rmse: 0.45577 |  0:02:24s
epoch 105| loss: 0.22894 | val_0_rmse: 0.47229 | val_1_rmse: 0.46606 |  0:02:25s
epoch 106| loss: 0.23018 | val_0_rmse: 0.45364 | val_1_rmse: 0.43977 |  0:02:26s
epoch 107| loss: 0.21748 | val_0_rmse: 0.44368 | val_1_rmse: 0.43453 |  0:02:27s
epoch 108| loss: 0.20467 | val_0_rmse: 0.45176 | val_1_rmse: 0.44342 |  0:02:29s
epoch 109| loss: 0.19787 | val_0_rmse: 0.43984 | val_1_rmse: 0.42958 |  0:02:30s
epoch 110| loss: 0.19358 | val_0_rmse: 0.41285 | val_1_rmse: 0.40688 |  0:02:31s
epoch 111| loss: 0.18242 | val_0_rmse: 0.41439 | val_1_rmse: 0.40944 |  0:02:32s
epoch 112| loss: 0.18189 | val_0_rmse: 0.4245  | val_1_rmse: 0.41896 |  0:02:34s
epoch 113| loss: 0.18884 | val_0_rmse: 0.42327 | val_1_rmse: 0.41706 |  0:02:35s
epoch 114| loss: 0.17856 | val_0_rmse: 0.40239 | val_1_rmse: 0.39722 |  0:02:36s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.3724
Best weights from best epoch are automatically used!
ended training at: 04:47:24
Feature importance:
[('Area', 0.17043437815977086), ('Baths', 0.020507441308310826), ('Beds', 0.1043304764501904), ('Latitude', 0.24576499312733804), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.0), ('sqft_lot', 0.16042439108477155), ('floors', 0.01901771066443973), ('waterfront', 0.029338718002834187), ('view', 0.044483981286306686), ('condition', 0.0), ('grade', 0.14287349099627183), ('sqft_above', 0.0001926912531036515), ('sqft_basement', 5.0548409454449646e-05), ('yr_renovated', 0.0), ('zipcode', 0.0625811792572078)]
Mean squared error is of 4538233766.327893
Mean absolute error:47830.44804711878
MAPE:0.12642291289845795
R2 score:0.8498846178142039
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:47:25
epoch 0  | loss: 1.19653 | val_0_rmse: 0.73964 | val_1_rmse: 0.74406 |  0:00:01s
epoch 1  | loss: 0.37342 | val_0_rmse: 0.74472 | val_1_rmse: 0.74454 |  0:00:02s
epoch 2  | loss: 0.30975 | val_0_rmse: 0.56195 | val_1_rmse: 0.5494  |  0:00:03s
epoch 3  | loss: 0.30098 | val_0_rmse: 0.5145  | val_1_rmse: 0.51788 |  0:00:05s
epoch 4  | loss: 0.29164 | val_0_rmse: 0.56116 | val_1_rmse: 0.55988 |  0:00:06s
epoch 5  | loss: 0.28165 | val_0_rmse: 0.51713 | val_1_rmse: 0.51795 |  0:00:07s
epoch 6  | loss: 0.27644 | val_0_rmse: 0.50332 | val_1_rmse: 0.50361 |  0:00:08s
epoch 7  | loss: 0.25814 | val_0_rmse: 0.48638 | val_1_rmse: 0.49152 |  0:00:10s
epoch 8  | loss: 0.24483 | val_0_rmse: 0.48177 | val_1_rmse: 0.49123 |  0:00:11s
epoch 9  | loss: 0.23653 | val_0_rmse: 0.47299 | val_1_rmse: 0.48366 |  0:00:12s
epoch 10 | loss: 0.23554 | val_0_rmse: 0.46218 | val_1_rmse: 0.48343 |  0:00:14s
epoch 11 | loss: 0.22717 | val_0_rmse: 0.46509 | val_1_rmse: 0.48532 |  0:00:15s
epoch 12 | loss: 0.22476 | val_0_rmse: 0.46256 | val_1_rmse: 0.47239 |  0:00:16s
epoch 13 | loss: 0.23315 | val_0_rmse: 0.4698  | val_1_rmse: 0.48335 |  0:00:17s
epoch 14 | loss: 0.23017 | val_0_rmse: 0.449   | val_1_rmse: 0.46424 |  0:00:19s
epoch 15 | loss: 0.21367 | val_0_rmse: 0.46584 | val_1_rmse: 0.46808 |  0:00:20s
epoch 16 | loss: 0.2162  | val_0_rmse: 0.44061 | val_1_rmse: 0.46363 |  0:00:21s
epoch 17 | loss: 0.22095 | val_0_rmse: 0.48293 | val_1_rmse: 0.49759 |  0:00:22s
epoch 18 | loss: 0.21764 | val_0_rmse: 0.43515 | val_1_rmse: 0.46393 |  0:00:24s
epoch 19 | loss: 0.20138 | val_0_rmse: 0.43092 | val_1_rmse: 0.46179 |  0:00:25s
epoch 20 | loss: 0.20287 | val_0_rmse: 0.43375 | val_1_rmse: 0.44397 |  0:00:26s
epoch 21 | loss: 0.20506 | val_0_rmse: 0.4313  | val_1_rmse: 0.4369  |  0:00:28s
epoch 22 | loss: 0.19818 | val_0_rmse: 0.42744 | val_1_rmse: 0.43107 |  0:00:29s
epoch 23 | loss: 0.18977 | val_0_rmse: 0.4293  | val_1_rmse: 0.43499 |  0:00:30s
epoch 24 | loss: 0.18926 | val_0_rmse: 0.4106  | val_1_rmse: 0.4228  |  0:00:31s
epoch 25 | loss: 0.18433 | val_0_rmse: 0.41522 | val_1_rmse: 0.43401 |  0:00:33s
epoch 26 | loss: 0.19053 | val_0_rmse: 0.42584 | val_1_rmse: 0.43965 |  0:00:34s
epoch 27 | loss: 0.19356 | val_0_rmse: 0.41419 | val_1_rmse: 0.43462 |  0:00:35s
epoch 28 | loss: 0.19035 | val_0_rmse: 0.41544 | val_1_rmse: 0.4314  |  0:00:37s
epoch 29 | loss: 0.18813 | val_0_rmse: 0.41712 | val_1_rmse: 0.43961 |  0:00:38s
epoch 30 | loss: 0.18491 | val_0_rmse: 0.43062 | val_1_rmse: 0.45011 |  0:00:39s
epoch 31 | loss: 0.19504 | val_0_rmse: 0.42832 | val_1_rmse: 0.44016 |  0:00:40s
epoch 32 | loss: 0.19087 | val_0_rmse: 0.41713 | val_1_rmse: 0.43426 |  0:00:42s
epoch 33 | loss: 0.18328 | val_0_rmse: 0.41007 | val_1_rmse: 0.43051 |  0:00:43s
epoch 34 | loss: 0.18004 | val_0_rmse: 0.40138 | val_1_rmse: 0.42092 |  0:00:44s
epoch 35 | loss: 0.17295 | val_0_rmse: 0.39257 | val_1_rmse: 0.41035 |  0:00:46s
epoch 36 | loss: 0.17207 | val_0_rmse: 0.40727 | val_1_rmse: 0.4244  |  0:00:47s
epoch 37 | loss: 0.16725 | val_0_rmse: 0.38957 | val_1_rmse: 0.40846 |  0:00:48s
epoch 38 | loss: 0.17119 | val_0_rmse: 0.39193 | val_1_rmse: 0.4096  |  0:00:49s
epoch 39 | loss: 0.17485 | val_0_rmse: 0.39499 | val_1_rmse: 0.4157  |  0:00:51s
epoch 40 | loss: 0.17985 | val_0_rmse: 0.3985  | val_1_rmse: 0.42098 |  0:00:52s
epoch 41 | loss: 0.1723  | val_0_rmse: 0.39704 | val_1_rmse: 0.41273 |  0:00:53s
epoch 42 | loss: 0.16703 | val_0_rmse: 0.38091 | val_1_rmse: 0.39725 |  0:00:55s
epoch 43 | loss: 0.16283 | val_0_rmse: 0.37819 | val_1_rmse: 0.39603 |  0:00:56s
epoch 44 | loss: 0.16005 | val_0_rmse: 0.38665 | val_1_rmse: 0.4084  |  0:00:57s
epoch 45 | loss: 0.16095 | val_0_rmse: 0.39171 | val_1_rmse: 0.41077 |  0:00:58s
epoch 46 | loss: 0.16991 | val_0_rmse: 0.38974 | val_1_rmse: 0.40776 |  0:01:00s
epoch 47 | loss: 0.16512 | val_0_rmse: 0.38041 | val_1_rmse: 0.40278 |  0:01:01s
epoch 48 | loss: 0.16562 | val_0_rmse: 0.40449 | val_1_rmse: 0.43013 |  0:01:02s
epoch 49 | loss: 0.16618 | val_0_rmse: 0.39408 | val_1_rmse: 0.41286 |  0:01:04s
epoch 50 | loss: 0.15852 | val_0_rmse: 0.38469 | val_1_rmse: 0.40689 |  0:01:05s
epoch 51 | loss: 0.15945 | val_0_rmse: 0.37631 | val_1_rmse: 0.3958  |  0:01:06s
epoch 52 | loss: 0.15975 | val_0_rmse: 0.38304 | val_1_rmse: 0.40042 |  0:01:07s
epoch 53 | loss: 0.16711 | val_0_rmse: 0.38347 | val_1_rmse: 0.40231 |  0:01:09s
epoch 54 | loss: 0.16097 | val_0_rmse: 0.40125 | val_1_rmse: 0.42445 |  0:01:10s
epoch 55 | loss: 0.16901 | val_0_rmse: 0.38271 | val_1_rmse: 0.40369 |  0:01:11s
epoch 56 | loss: 0.15758 | val_0_rmse: 0.38582 | val_1_rmse: 0.40896 |  0:01:12s
epoch 57 | loss: 0.16089 | val_0_rmse: 0.39642 | val_1_rmse: 0.4197  |  0:01:14s
epoch 58 | loss: 0.16366 | val_0_rmse: 0.37563 | val_1_rmse: 0.39785 |  0:01:15s
epoch 59 | loss: 0.15615 | val_0_rmse: 0.37638 | val_1_rmse: 0.40195 |  0:01:16s
epoch 60 | loss: 0.15516 | val_0_rmse: 0.37962 | val_1_rmse: 0.40579 |  0:01:17s
epoch 61 | loss: 0.15583 | val_0_rmse: 0.39531 | val_1_rmse: 0.41572 |  0:01:19s
epoch 62 | loss: 0.16468 | val_0_rmse: 0.38233 | val_1_rmse: 0.40168 |  0:01:20s
epoch 63 | loss: 0.15961 | val_0_rmse: 0.4089  | val_1_rmse: 0.43075 |  0:01:21s
epoch 64 | loss: 0.16012 | val_0_rmse: 0.37817 | val_1_rmse: 0.40471 |  0:01:23s
epoch 65 | loss: 0.15579 | val_0_rmse: 0.37191 | val_1_rmse: 0.39572 |  0:01:24s
epoch 66 | loss: 0.16385 | val_0_rmse: 0.38609 | val_1_rmse: 0.41134 |  0:01:25s
epoch 67 | loss: 0.15131 | val_0_rmse: 0.37625 | val_1_rmse: 0.39784 |  0:01:26s
epoch 68 | loss: 0.15443 | val_0_rmse: 0.38168 | val_1_rmse: 0.4055  |  0:01:28s
epoch 69 | loss: 0.16151 | val_0_rmse: 0.37441 | val_1_rmse: 0.39735 |  0:01:29s
epoch 70 | loss: 0.15679 | val_0_rmse: 0.39903 | val_1_rmse: 0.42288 |  0:01:30s
epoch 71 | loss: 0.16444 | val_0_rmse: 0.38014 | val_1_rmse: 0.40358 |  0:01:31s
epoch 72 | loss: 0.15177 | val_0_rmse: 0.37388 | val_1_rmse: 0.40002 |  0:01:33s
epoch 73 | loss: 0.15067 | val_0_rmse: 0.37024 | val_1_rmse: 0.3915  |  0:01:34s
epoch 74 | loss: 0.15003 | val_0_rmse: 0.36932 | val_1_rmse: 0.39632 |  0:01:35s
epoch 75 | loss: 0.15506 | val_0_rmse: 0.3746  | val_1_rmse: 0.40004 |  0:01:36s
epoch 76 | loss: 0.14871 | val_0_rmse: 0.36536 | val_1_rmse: 0.39385 |  0:01:38s
epoch 77 | loss: 0.15014 | val_0_rmse: 0.36516 | val_1_rmse: 0.39272 |  0:01:39s
epoch 78 | loss: 0.15511 | val_0_rmse: 0.37757 | val_1_rmse: 0.40191 |  0:01:40s
epoch 79 | loss: 0.15516 | val_0_rmse: 0.3775  | val_1_rmse: 0.39711 |  0:01:42s
epoch 80 | loss: 0.1487  | val_0_rmse: 0.36424 | val_1_rmse: 0.39274 |  0:01:43s
epoch 81 | loss: 0.14814 | val_0_rmse: 0.37464 | val_1_rmse: 0.40455 |  0:01:44s
epoch 82 | loss: 0.14885 | val_0_rmse: 0.37255 | val_1_rmse: 0.40929 |  0:01:45s
epoch 83 | loss: 0.1513  | val_0_rmse: 0.37849 | val_1_rmse: 0.40837 |  0:01:47s
epoch 84 | loss: 0.15047 | val_0_rmse: 0.36377 | val_1_rmse: 0.39425 |  0:01:48s
epoch 85 | loss: 0.14786 | val_0_rmse: 0.36703 | val_1_rmse: 0.40143 |  0:01:49s
epoch 86 | loss: 0.14854 | val_0_rmse: 0.37216 | val_1_rmse: 0.40059 |  0:01:51s
epoch 87 | loss: 0.15056 | val_0_rmse: 0.37677 | val_1_rmse: 0.41162 |  0:01:52s
epoch 88 | loss: 0.15696 | val_0_rmse: 0.36819 | val_1_rmse: 0.39534 |  0:01:53s
epoch 89 | loss: 0.14844 | val_0_rmse: 0.37293 | val_1_rmse: 0.40699 |  0:01:54s
epoch 90 | loss: 0.14786 | val_0_rmse: 0.36729 | val_1_rmse: 0.39685 |  0:01:56s
epoch 91 | loss: 0.15831 | val_0_rmse: 0.39972 | val_1_rmse: 0.42801 |  0:01:57s
epoch 92 | loss: 0.16082 | val_0_rmse: 0.39441 | val_1_rmse: 0.42622 |  0:01:58s
epoch 93 | loss: 0.1507  | val_0_rmse: 0.36466 | val_1_rmse: 0.3951  |  0:01:59s
epoch 94 | loss: 0.14344 | val_0_rmse: 0.35548 | val_1_rmse: 0.38788 |  0:02:01s
epoch 95 | loss: 0.14384 | val_0_rmse: 0.35841 | val_1_rmse: 0.39736 |  0:02:02s
epoch 96 | loss: 0.14317 | val_0_rmse: 0.36846 | val_1_rmse: 0.40135 |  0:02:03s
epoch 97 | loss: 0.14651 | val_0_rmse: 0.36736 | val_1_rmse: 0.40268 |  0:02:04s
epoch 98 | loss: 0.14192 | val_0_rmse: 0.35779 | val_1_rmse: 0.3928  |  0:02:06s
epoch 99 | loss: 0.14518 | val_0_rmse: 0.37391 | val_1_rmse: 0.40254 |  0:02:07s
epoch 100| loss: 0.14342 | val_0_rmse: 0.35858 | val_1_rmse: 0.39147 |  0:02:08s
epoch 101| loss: 0.14827 | val_0_rmse: 0.36899 | val_1_rmse: 0.40159 |  0:02:10s
epoch 102| loss: 0.14318 | val_0_rmse: 0.36149 | val_1_rmse: 0.39295 |  0:02:11s
epoch 103| loss: 0.14208 | val_0_rmse: 0.35271 | val_1_rmse: 0.38949 |  0:02:12s
epoch 104| loss: 0.14381 | val_0_rmse: 0.35532 | val_1_rmse: 0.39223 |  0:02:13s
epoch 105| loss: 0.14175 | val_0_rmse: 0.35815 | val_1_rmse: 0.39536 |  0:02:15s
epoch 106| loss: 0.14339 | val_0_rmse: 0.38304 | val_1_rmse: 0.41811 |  0:02:16s
epoch 107| loss: 0.14278 | val_0_rmse: 0.35845 | val_1_rmse: 0.39803 |  0:02:17s
epoch 108| loss: 0.13896 | val_0_rmse: 0.34717 | val_1_rmse: 0.38559 |  0:02:18s
epoch 109| loss: 0.13649 | val_0_rmse: 0.35157 | val_1_rmse: 0.39618 |  0:02:20s
epoch 110| loss: 0.1396  | val_0_rmse: 0.35553 | val_1_rmse: 0.40053 |  0:02:21s
epoch 111| loss: 0.14904 | val_0_rmse: 0.36139 | val_1_rmse: 0.3988  |  0:02:22s
epoch 112| loss: 0.14164 | val_0_rmse: 0.35515 | val_1_rmse: 0.39457 |  0:02:24s
epoch 113| loss: 0.1396  | val_0_rmse: 0.37451 | val_1_rmse: 0.42358 |  0:02:25s
epoch 114| loss: 0.15085 | val_0_rmse: 0.371   | val_1_rmse: 0.40493 |  0:02:26s
epoch 115| loss: 0.14647 | val_0_rmse: 0.3578  | val_1_rmse: 0.39017 |  0:02:27s
epoch 116| loss: 0.14755 | val_0_rmse: 0.37726 | val_1_rmse: 0.41076 |  0:02:29s
epoch 117| loss: 0.14241 | val_0_rmse: 0.35396 | val_1_rmse: 0.39339 |  0:02:30s
epoch 118| loss: 0.14359 | val_0_rmse: 0.36265 | val_1_rmse: 0.39509 |  0:02:31s
epoch 119| loss: 0.13901 | val_0_rmse: 0.35937 | val_1_rmse: 0.40234 |  0:02:33s
epoch 120| loss: 0.13624 | val_0_rmse: 0.35095 | val_1_rmse: 0.3901  |  0:02:34s
epoch 121| loss: 0.13386 | val_0_rmse: 0.34805 | val_1_rmse: 0.38949 |  0:02:35s
epoch 122| loss: 0.13513 | val_0_rmse: 0.34869 | val_1_rmse: 0.39261 |  0:02:36s
epoch 123| loss: 0.13941 | val_0_rmse: 0.34544 | val_1_rmse: 0.39338 |  0:02:38s
epoch 124| loss: 0.14598 | val_0_rmse: 0.3609  | val_1_rmse: 0.40559 |  0:02:39s
epoch 125| loss: 0.13718 | val_0_rmse: 0.36887 | val_1_rmse: 0.41367 |  0:02:40s
epoch 126| loss: 0.14073 | val_0_rmse: 0.35928 | val_1_rmse: 0.39294 |  0:02:41s
epoch 127| loss: 0.15094 | val_0_rmse: 0.37233 | val_1_rmse: 0.41027 |  0:02:43s
epoch 128| loss: 0.14728 | val_0_rmse: 0.36416 | val_1_rmse: 0.40188 |  0:02:44s
epoch 129| loss: 0.14386 | val_0_rmse: 0.34914 | val_1_rmse: 0.38772 |  0:02:45s
epoch 130| loss: 0.14202 | val_0_rmse: 0.36547 | val_1_rmse: 0.40262 |  0:02:46s
epoch 131| loss: 0.1411  | val_0_rmse: 0.36304 | val_1_rmse: 0.40391 |  0:02:48s
epoch 132| loss: 0.13889 | val_0_rmse: 0.35449 | val_1_rmse: 0.40167 |  0:02:49s
epoch 133| loss: 0.1454  | val_0_rmse: 0.3541  | val_1_rmse: 0.39603 |  0:02:50s
epoch 134| loss: 0.13826 | val_0_rmse: 0.35811 | val_1_rmse: 0.40188 |  0:02:51s
epoch 135| loss: 0.13517 | val_0_rmse: 0.34953 | val_1_rmse: 0.38884 |  0:02:53s
epoch 136| loss: 0.1357  | val_0_rmse: 0.35288 | val_1_rmse: 0.39332 |  0:02:54s
epoch 137| loss: 0.14136 | val_0_rmse: 0.36739 | val_1_rmse: 0.40621 |  0:02:55s
epoch 138| loss: 0.14071 | val_0_rmse: 0.36141 | val_1_rmse: 0.40352 |  0:02:56s

Early stopping occured at epoch 138 with best_epoch = 108 and best_val_1_rmse = 0.38559
Best weights from best epoch are automatically used!
ended training at: 04:50:22
Feature importance:
[('Area', 0.2461254572138822), ('Baths', 0.0), ('Beds', 0.02372057762122363), ('Latitude', 0.3556430986961314), ('Longitude', 0.08272974237867035), ('Month', 0.0), ('Year', 0.0), ('sqft_lot', 0.0), ('floors', 0.008533482848038867), ('waterfront', 0.09402736409627518), ('view', 0.0007902103806057555), ('condition', 0.0), ('grade', 0.1418466782808713), ('sqft_above', 0.0), ('sqft_basement', 0.00185983113878173), ('yr_renovated', 0.0), ('zipcode', 0.044723557345519555)]
Mean squared error is of 4203131837.603653
Mean absolute error:46154.27393264028
MAPE:0.12351163317152822
R2 score:0.8550481019790656
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:50:23
epoch 0  | loss: 1.18441 | val_0_rmse: 0.78729 | val_1_rmse: 0.81325 |  0:00:01s
epoch 1  | loss: 0.41544 | val_0_rmse: 0.64252 | val_1_rmse: 0.62459 |  0:00:02s
epoch 2  | loss: 0.34063 | val_0_rmse: 0.55434 | val_1_rmse: 0.54503 |  0:00:03s
epoch 3  | loss: 0.30169 | val_0_rmse: 0.52406 | val_1_rmse: 0.51328 |  0:00:05s
epoch 4  | loss: 0.27645 | val_0_rmse: 0.51095 | val_1_rmse: 0.50081 |  0:00:06s
epoch 5  | loss: 0.25948 | val_0_rmse: 0.49076 | val_1_rmse: 0.48211 |  0:00:07s
epoch 6  | loss: 0.25514 | val_0_rmse: 0.48646 | val_1_rmse: 0.4797  |  0:00:08s
epoch 7  | loss: 0.25172 | val_0_rmse: 0.48273 | val_1_rmse: 0.47327 |  0:00:10s
epoch 8  | loss: 0.25073 | val_0_rmse: 0.49015 | val_1_rmse: 0.47575 |  0:00:11s
epoch 9  | loss: 0.24363 | val_0_rmse: 0.48375 | val_1_rmse: 0.47649 |  0:00:12s
epoch 10 | loss: 0.23659 | val_0_rmse: 0.46456 | val_1_rmse: 0.46099 |  0:00:13s
epoch 11 | loss: 0.23283 | val_0_rmse: 0.46327 | val_1_rmse: 0.45011 |  0:00:15s
epoch 12 | loss: 0.22937 | val_0_rmse: 0.45783 | val_1_rmse: 0.45269 |  0:00:16s
epoch 13 | loss: 0.2246  | val_0_rmse: 0.45178 | val_1_rmse: 0.44047 |  0:00:17s
epoch 14 | loss: 0.2182  | val_0_rmse: 0.45353 | val_1_rmse: 0.44448 |  0:00:18s
epoch 15 | loss: 0.2215  | val_0_rmse: 0.44954 | val_1_rmse: 0.4437  |  0:00:20s
epoch 16 | loss: 0.21579 | val_0_rmse: 0.50877 | val_1_rmse: 0.48977 |  0:00:21s
epoch 17 | loss: 0.21321 | val_0_rmse: 0.43289 | val_1_rmse: 0.42901 |  0:00:22s
epoch 18 | loss: 0.20665 | val_0_rmse: 0.44141 | val_1_rmse: 0.4316  |  0:00:23s
epoch 19 | loss: 0.20474 | val_0_rmse: 0.43325 | val_1_rmse: 0.4287  |  0:00:25s
epoch 20 | loss: 0.20325 | val_0_rmse: 0.42841 | val_1_rmse: 0.41864 |  0:00:26s
epoch 21 | loss: 0.20564 | val_0_rmse: 0.43193 | val_1_rmse: 0.42201 |  0:00:27s
epoch 22 | loss: 0.19456 | val_0_rmse: 0.44166 | val_1_rmse: 0.4387  |  0:00:28s
epoch 23 | loss: 0.2     | val_0_rmse: 0.47532 | val_1_rmse: 0.45836 |  0:00:30s
epoch 24 | loss: 0.20681 | val_0_rmse: 0.422   | val_1_rmse: 0.41379 |  0:00:31s
epoch 25 | loss: 0.19825 | val_0_rmse: 0.42707 | val_1_rmse: 0.41641 |  0:00:32s
epoch 26 | loss: 0.19026 | val_0_rmse: 0.4216  | val_1_rmse: 0.41478 |  0:00:34s
epoch 27 | loss: 0.18994 | val_0_rmse: 0.41976 | val_1_rmse: 0.41041 |  0:00:35s
epoch 28 | loss: 0.19135 | val_0_rmse: 0.4161  | val_1_rmse: 0.41242 |  0:00:36s
epoch 29 | loss: 0.18451 | val_0_rmse: 0.41489 | val_1_rmse: 0.41365 |  0:00:37s
epoch 30 | loss: 0.18254 | val_0_rmse: 0.42132 | val_1_rmse: 0.41163 |  0:00:39s
epoch 31 | loss: 0.18145 | val_0_rmse: 0.40911 | val_1_rmse: 0.4063  |  0:00:40s
epoch 32 | loss: 0.18582 | val_0_rmse: 0.40955 | val_1_rmse: 0.40467 |  0:00:41s
epoch 33 | loss: 0.18688 | val_0_rmse: 0.41053 | val_1_rmse: 0.39923 |  0:00:42s
epoch 34 | loss: 0.18243 | val_0_rmse: 0.40793 | val_1_rmse: 0.40341 |  0:00:44s
epoch 35 | loss: 0.18489 | val_0_rmse: 0.43159 | val_1_rmse: 0.41536 |  0:00:45s
epoch 36 | loss: 0.17587 | val_0_rmse: 0.3937  | val_1_rmse: 0.3839  |  0:00:46s
epoch 37 | loss: 0.17289 | val_0_rmse: 0.39474 | val_1_rmse: 0.38744 |  0:00:47s
epoch 38 | loss: 0.16843 | val_0_rmse: 0.39393 | val_1_rmse: 0.38612 |  0:00:49s
epoch 39 | loss: 0.16811 | val_0_rmse: 0.40749 | val_1_rmse: 0.41511 |  0:00:50s
epoch 40 | loss: 0.1678  | val_0_rmse: 0.39401 | val_1_rmse: 0.38671 |  0:00:51s
epoch 41 | loss: 0.1683  | val_0_rmse: 0.39172 | val_1_rmse: 0.39619 |  0:00:52s
epoch 42 | loss: 0.16457 | val_0_rmse: 0.38878 | val_1_rmse: 0.38733 |  0:00:54s
epoch 43 | loss: 0.16688 | val_0_rmse: 0.39009 | val_1_rmse: 0.39248 |  0:00:55s
epoch 44 | loss: 0.17184 | val_0_rmse: 0.38402 | val_1_rmse: 0.3884  |  0:00:56s
epoch 45 | loss: 0.16424 | val_0_rmse: 0.39679 | val_1_rmse: 0.39492 |  0:00:57s
epoch 46 | loss: 0.16399 | val_0_rmse: 0.38111 | val_1_rmse: 0.38539 |  0:00:59s
epoch 47 | loss: 0.15987 | val_0_rmse: 0.38608 | val_1_rmse: 0.39508 |  0:01:00s
epoch 48 | loss: 0.16053 | val_0_rmse: 0.39122 | val_1_rmse: 0.39632 |  0:01:01s
epoch 49 | loss: 0.17223 | val_0_rmse: 0.38244 | val_1_rmse: 0.38313 |  0:01:03s
epoch 50 | loss: 0.16376 | val_0_rmse: 0.39672 | val_1_rmse: 0.40894 |  0:01:04s
epoch 51 | loss: 0.1571  | val_0_rmse: 0.3802  | val_1_rmse: 0.38544 |  0:01:05s
epoch 52 | loss: 0.15907 | val_0_rmse: 0.37202 | val_1_rmse: 0.37361 |  0:01:06s
epoch 53 | loss: 0.15121 | val_0_rmse: 0.36662 | val_1_rmse: 0.37326 |  0:01:08s
epoch 54 | loss: 0.15376 | val_0_rmse: 0.3798  | val_1_rmse: 0.39095 |  0:01:09s
epoch 55 | loss: 0.15549 | val_0_rmse: 0.38327 | val_1_rmse: 0.38005 |  0:01:10s
epoch 56 | loss: 0.15653 | val_0_rmse: 0.37584 | val_1_rmse: 0.38521 |  0:01:12s
epoch 57 | loss: 0.16147 | val_0_rmse: 0.38446 | val_1_rmse: 0.38393 |  0:01:13s
epoch 58 | loss: 0.15795 | val_0_rmse: 0.37188 | val_1_rmse: 0.37481 |  0:01:14s
epoch 59 | loss: 0.15212 | val_0_rmse: 0.36839 | val_1_rmse: 0.38726 |  0:01:15s
epoch 60 | loss: 0.16752 | val_0_rmse: 0.41907 | val_1_rmse: 0.41562 |  0:01:17s
epoch 61 | loss: 0.16608 | val_0_rmse: 0.39213 | val_1_rmse: 0.3982  |  0:01:18s
epoch 62 | loss: 0.16841 | val_0_rmse: 0.38712 | val_1_rmse: 0.38707 |  0:01:19s
epoch 63 | loss: 0.16051 | val_0_rmse: 0.37492 | val_1_rmse: 0.38164 |  0:01:20s
epoch 64 | loss: 0.16596 | val_0_rmse: 0.40434 | val_1_rmse: 0.41037 |  0:01:22s
epoch 65 | loss: 0.16196 | val_0_rmse: 0.38653 | val_1_rmse: 0.38983 |  0:01:23s
epoch 66 | loss: 0.15598 | val_0_rmse: 0.36763 | val_1_rmse: 0.375   |  0:01:24s
epoch 67 | loss: 0.15566 | val_0_rmse: 0.37225 | val_1_rmse: 0.37222 |  0:01:25s
epoch 68 | loss: 0.15347 | val_0_rmse: 0.36385 | val_1_rmse: 0.36872 |  0:01:27s
epoch 69 | loss: 0.15233 | val_0_rmse: 0.38423 | val_1_rmse: 0.3813  |  0:01:28s
epoch 70 | loss: 0.15695 | val_0_rmse: 0.36795 | val_1_rmse: 0.38009 |  0:01:29s
epoch 71 | loss: 0.15009 | val_0_rmse: 0.38833 | val_1_rmse: 0.38913 |  0:01:30s
epoch 72 | loss: 0.15308 | val_0_rmse: 0.36173 | val_1_rmse: 0.37099 |  0:01:32s
epoch 73 | loss: 0.15205 | val_0_rmse: 0.36811 | val_1_rmse: 0.38415 |  0:01:33s
epoch 74 | loss: 0.15067 | val_0_rmse: 0.36463 | val_1_rmse: 0.37824 |  0:01:34s
epoch 75 | loss: 0.14733 | val_0_rmse: 0.37114 | val_1_rmse: 0.37672 |  0:01:36s
epoch 76 | loss: 0.15293 | val_0_rmse: 0.36047 | val_1_rmse: 0.37053 |  0:01:37s
epoch 77 | loss: 0.15003 | val_0_rmse: 0.39597 | val_1_rmse: 0.40327 |  0:01:38s
epoch 78 | loss: 0.15101 | val_0_rmse: 0.35803 | val_1_rmse: 0.36901 |  0:01:39s
epoch 79 | loss: 0.15256 | val_0_rmse: 0.36827 | val_1_rmse: 0.38341 |  0:01:41s
epoch 80 | loss: 0.15128 | val_0_rmse: 0.37317 | val_1_rmse: 0.38137 |  0:01:42s
epoch 81 | loss: 0.14927 | val_0_rmse: 0.37108 | val_1_rmse: 0.38082 |  0:01:43s
epoch 82 | loss: 0.14371 | val_0_rmse: 0.35802 | val_1_rmse: 0.37337 |  0:01:44s
epoch 83 | loss: 0.14966 | val_0_rmse: 0.36615 | val_1_rmse: 0.37865 |  0:01:46s
epoch 84 | loss: 0.15333 | val_0_rmse: 0.3573  | val_1_rmse: 0.36589 |  0:01:47s
epoch 85 | loss: 0.15253 | val_0_rmse: 0.38118 | val_1_rmse: 0.38603 |  0:01:48s
epoch 86 | loss: 0.15782 | val_0_rmse: 0.38303 | val_1_rmse: 0.38703 |  0:01:49s
epoch 87 | loss: 0.15128 | val_0_rmse: 0.38311 | val_1_rmse: 0.39328 |  0:01:51s
epoch 88 | loss: 0.15112 | val_0_rmse: 0.35804 | val_1_rmse: 0.37043 |  0:01:52s
epoch 89 | loss: 0.14321 | val_0_rmse: 0.3562  | val_1_rmse: 0.37139 |  0:01:53s
epoch 90 | loss: 0.14175 | val_0_rmse: 0.35451 | val_1_rmse: 0.3641  |  0:01:54s
epoch 91 | loss: 0.14335 | val_0_rmse: 0.36013 | val_1_rmse: 0.37413 |  0:01:56s
epoch 92 | loss: 0.14934 | val_0_rmse: 0.36621 | val_1_rmse: 0.37291 |  0:01:57s
epoch 93 | loss: 0.15016 | val_0_rmse: 0.35742 | val_1_rmse: 0.36922 |  0:01:58s
epoch 94 | loss: 0.1424  | val_0_rmse: 0.35054 | val_1_rmse: 0.36579 |  0:01:59s
epoch 95 | loss: 0.13724 | val_0_rmse: 0.35326 | val_1_rmse: 0.36454 |  0:02:01s
epoch 96 | loss: 0.1446  | val_0_rmse: 0.37996 | val_1_rmse: 0.40487 |  0:02:02s
epoch 97 | loss: 0.14694 | val_0_rmse: 0.36009 | val_1_rmse: 0.37568 |  0:02:03s
epoch 98 | loss: 0.14453 | val_0_rmse: 0.35362 | val_1_rmse: 0.37199 |  0:02:05s
epoch 99 | loss: 0.14084 | val_0_rmse: 0.36459 | val_1_rmse: 0.38381 |  0:02:06s
epoch 100| loss: 0.14823 | val_0_rmse: 0.36424 | val_1_rmse: 0.38862 |  0:02:07s
epoch 101| loss: 0.14397 | val_0_rmse: 0.35588 | val_1_rmse: 0.37581 |  0:02:08s
epoch 102| loss: 0.14069 | val_0_rmse: 0.36785 | val_1_rmse: 0.3812  |  0:02:10s
epoch 103| loss: 0.14176 | val_0_rmse: 0.35822 | val_1_rmse: 0.36814 |  0:02:11s
epoch 104| loss: 0.14461 | val_0_rmse: 0.35742 | val_1_rmse: 0.37383 |  0:02:12s
epoch 105| loss: 0.13903 | val_0_rmse: 0.35292 | val_1_rmse: 0.38053 |  0:02:13s
epoch 106| loss: 0.14211 | val_0_rmse: 0.35    | val_1_rmse: 0.36846 |  0:02:15s
epoch 107| loss: 0.13692 | val_0_rmse: 0.35887 | val_1_rmse: 0.37239 |  0:02:16s
epoch 108| loss: 0.14036 | val_0_rmse: 0.36674 | val_1_rmse: 0.37489 |  0:02:17s
epoch 109| loss: 0.14648 | val_0_rmse: 0.35055 | val_1_rmse: 0.36564 |  0:02:18s
epoch 110| loss: 0.13762 | val_0_rmse: 0.35361 | val_1_rmse: 0.36752 |  0:02:20s
epoch 111| loss: 0.13605 | val_0_rmse: 0.35198 | val_1_rmse: 0.37526 |  0:02:21s
epoch 112| loss: 0.13899 | val_0_rmse: 0.35555 | val_1_rmse: 0.37221 |  0:02:22s
epoch 113| loss: 0.1388  | val_0_rmse: 0.35195 | val_1_rmse: 0.37113 |  0:02:24s
epoch 114| loss: 0.13454 | val_0_rmse: 0.35075 | val_1_rmse: 0.37607 |  0:02:25s
epoch 115| loss: 0.13566 | val_0_rmse: 0.34346 | val_1_rmse: 0.37032 |  0:02:26s
epoch 116| loss: 0.13564 | val_0_rmse: 0.36019 | val_1_rmse: 0.37908 |  0:02:27s
epoch 117| loss: 0.1374  | val_0_rmse: 0.35066 | val_1_rmse: 0.37956 |  0:02:29s
epoch 118| loss: 0.14142 | val_0_rmse: 0.34809 | val_1_rmse: 0.37002 |  0:02:30s
epoch 119| loss: 0.13901 | val_0_rmse: 0.34115 | val_1_rmse: 0.36719 |  0:02:31s
epoch 120| loss: 0.13554 | val_0_rmse: 0.35089 | val_1_rmse: 0.38462 |  0:02:32s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.3641
Best weights from best epoch are automatically used!
ended training at: 04:52:56
Feature importance:
[('Area', 0.15363375535509702), ('Baths', 0.0), ('Beds', 1.4890868375362667e-05), ('Latitude', 0.29772926090776036), ('Longitude', 0.0), ('Month', 1.1714366118976208e-06), ('Year', 5.274507041448673e-06), ('sqft_lot', 0.018214252796698555), ('floors', 0.0026855040701498196), ('waterfront', 0.0), ('view', 0.008021160800767053), ('condition', 1.8219081972321333e-06), ('grade', 0.20523454114261885), ('sqft_above', 0.27226814002312594), ('sqft_basement', 0.04219022618355642), ('yr_renovated', 0.0), ('zipcode', 0.0)]
Mean squared error is of 4609334349.195567
Mean absolute error:47589.61405293784
MAPE:0.12765734509594523
R2 score:0.8516202135764238
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:52:56
epoch 0  | loss: 1.25115 | val_0_rmse: 0.76299 | val_1_rmse: 0.77532 |  0:00:01s
epoch 1  | loss: 0.45778 | val_0_rmse: 0.68445 | val_1_rmse: 0.68828 |  0:00:02s
epoch 2  | loss: 0.35928 | val_0_rmse: 0.59743 | val_1_rmse: 0.59113 |  0:00:03s
epoch 3  | loss: 0.31023 | val_0_rmse: 0.52712 | val_1_rmse: 0.52273 |  0:00:05s
epoch 4  | loss: 0.28529 | val_0_rmse: 0.50688 | val_1_rmse: 0.50363 |  0:00:06s
epoch 5  | loss: 0.26999 | val_0_rmse: 0.49941 | val_1_rmse: 0.49431 |  0:00:07s
epoch 6  | loss: 0.25889 | val_0_rmse: 0.49047 | val_1_rmse: 0.48631 |  0:00:08s
epoch 7  | loss: 0.25682 | val_0_rmse: 0.48379 | val_1_rmse: 0.48216 |  0:00:10s
epoch 8  | loss: 0.24629 | val_0_rmse: 0.47972 | val_1_rmse: 0.47531 |  0:00:11s
epoch 9  | loss: 0.25574 | val_0_rmse: 0.50732 | val_1_rmse: 0.5036  |  0:00:12s
epoch 10 | loss: 0.25975 | val_0_rmse: 0.47409 | val_1_rmse: 0.47414 |  0:00:14s
epoch 11 | loss: 0.23688 | val_0_rmse: 0.47427 | val_1_rmse: 0.47156 |  0:00:15s
epoch 12 | loss: 0.22935 | val_0_rmse: 0.46853 | val_1_rmse: 0.47097 |  0:00:16s
epoch 13 | loss: 0.23168 | val_0_rmse: 0.45487 | val_1_rmse: 0.44931 |  0:00:17s
epoch 14 | loss: 0.22927 | val_0_rmse: 0.45789 | val_1_rmse: 0.45253 |  0:00:19s
epoch 15 | loss: 0.21313 | val_0_rmse: 0.44711 | val_1_rmse: 0.44999 |  0:00:20s
epoch 16 | loss: 0.21054 | val_0_rmse: 0.44265 | val_1_rmse: 0.44586 |  0:00:21s
epoch 17 | loss: 0.20975 | val_0_rmse: 0.44142 | val_1_rmse: 0.44661 |  0:00:23s
epoch 18 | loss: 0.2057  | val_0_rmse: 0.43372 | val_1_rmse: 0.43643 |  0:00:24s
epoch 19 | loss: 0.19831 | val_0_rmse: 0.42944 | val_1_rmse: 0.4394  |  0:00:25s
epoch 20 | loss: 0.20092 | val_0_rmse: 0.42206 | val_1_rmse: 0.42843 |  0:00:26s
epoch 21 | loss: 0.19805 | val_0_rmse: 0.43169 | val_1_rmse: 0.43172 |  0:00:28s
epoch 22 | loss: 0.19382 | val_0_rmse: 0.41779 | val_1_rmse: 0.42231 |  0:00:29s
epoch 23 | loss: 0.19295 | val_0_rmse: 0.42113 | val_1_rmse: 0.42038 |  0:00:30s
epoch 24 | loss: 0.19349 | val_0_rmse: 0.41691 | val_1_rmse: 0.42548 |  0:00:31s
epoch 25 | loss: 0.18854 | val_0_rmse: 0.44102 | val_1_rmse: 0.44046 |  0:00:33s
epoch 26 | loss: 0.20102 | val_0_rmse: 0.42845 | val_1_rmse: 0.42623 |  0:00:34s
epoch 27 | loss: 0.19513 | val_0_rmse: 0.42153 | val_1_rmse: 0.41816 |  0:00:35s
epoch 28 | loss: 0.18922 | val_0_rmse: 0.40846 | val_1_rmse: 0.41396 |  0:00:36s
epoch 29 | loss: 0.18202 | val_0_rmse: 0.39957 | val_1_rmse: 0.40402 |  0:00:38s
epoch 30 | loss: 0.17651 | val_0_rmse: 0.40816 | val_1_rmse: 0.41458 |  0:00:39s
epoch 31 | loss: 0.17933 | val_0_rmse: 0.39834 | val_1_rmse: 0.40223 |  0:00:40s
epoch 32 | loss: 0.18639 | val_0_rmse: 0.4881  | val_1_rmse: 0.50578 |  0:00:42s
epoch 33 | loss: 0.18793 | val_0_rmse: 0.41843 | val_1_rmse: 0.41311 |  0:00:43s
epoch 34 | loss: 0.18479 | val_0_rmse: 0.40628 | val_1_rmse: 0.414   |  0:00:44s
epoch 35 | loss: 0.17772 | val_0_rmse: 0.41734 | val_1_rmse: 0.41267 |  0:00:45s
epoch 36 | loss: 0.17435 | val_0_rmse: 0.39684 | val_1_rmse: 0.40111 |  0:00:47s
epoch 37 | loss: 0.17288 | val_0_rmse: 0.47076 | val_1_rmse: 0.46056 |  0:00:48s
epoch 38 | loss: 0.18492 | val_0_rmse: 0.39587 | val_1_rmse: 0.4025  |  0:00:49s
epoch 39 | loss: 0.17453 | val_0_rmse: 0.40033 | val_1_rmse: 0.40483 |  0:00:50s
epoch 40 | loss: 0.1669  | val_0_rmse: 0.39601 | val_1_rmse: 0.40132 |  0:00:52s
epoch 41 | loss: 0.17087 | val_0_rmse: 0.39203 | val_1_rmse: 0.39544 |  0:00:53s
epoch 42 | loss: 0.1651  | val_0_rmse: 0.41447 | val_1_rmse: 0.41268 |  0:00:54s
epoch 43 | loss: 0.16912 | val_0_rmse: 0.39056 | val_1_rmse: 0.3988  |  0:00:55s
epoch 44 | loss: 0.17075 | val_0_rmse: 0.40434 | val_1_rmse: 0.4039  |  0:00:57s
epoch 45 | loss: 0.1664  | val_0_rmse: 0.3823  | val_1_rmse: 0.39136 |  0:00:58s
epoch 46 | loss: 0.15823 | val_0_rmse: 0.39668 | val_1_rmse: 0.3968  |  0:00:59s
epoch 47 | loss: 0.16181 | val_0_rmse: 0.38554 | val_1_rmse: 0.39027 |  0:01:00s
epoch 48 | loss: 0.15652 | val_0_rmse: 0.39401 | val_1_rmse: 0.40555 |  0:01:02s
epoch 49 | loss: 0.16168 | val_0_rmse: 0.41632 | val_1_rmse: 0.41133 |  0:01:03s
epoch 50 | loss: 0.16278 | val_0_rmse: 0.38122 | val_1_rmse: 0.3909  |  0:01:04s
epoch 51 | loss: 0.15505 | val_0_rmse: 0.38621 | val_1_rmse: 0.38795 |  0:01:06s
epoch 52 | loss: 0.15617 | val_0_rmse: 0.39126 | val_1_rmse: 0.39201 |  0:01:07s
epoch 53 | loss: 0.15949 | val_0_rmse: 0.38448 | val_1_rmse: 0.39627 |  0:01:08s
epoch 54 | loss: 0.16198 | val_0_rmse: 0.40298 | val_1_rmse: 0.40322 |  0:01:09s
epoch 55 | loss: 0.15656 | val_0_rmse: 0.38036 | val_1_rmse: 0.39511 |  0:01:11s
epoch 56 | loss: 0.15902 | val_0_rmse: 0.4004  | val_1_rmse: 0.41134 |  0:01:12s
epoch 57 | loss: 0.16481 | val_0_rmse: 0.38787 | val_1_rmse: 0.39164 |  0:01:13s
epoch 58 | loss: 0.15818 | val_0_rmse: 0.37209 | val_1_rmse: 0.37849 |  0:01:15s
epoch 59 | loss: 0.15404 | val_0_rmse: 0.39886 | val_1_rmse: 0.41114 |  0:01:16s
epoch 60 | loss: 0.15021 | val_0_rmse: 0.36625 | val_1_rmse: 0.37625 |  0:01:17s
epoch 61 | loss: 0.15381 | val_0_rmse: 0.37205 | val_1_rmse: 0.37958 |  0:01:18s
epoch 62 | loss: 0.15662 | val_0_rmse: 0.36742 | val_1_rmse: 0.37462 |  0:01:20s
epoch 63 | loss: 0.15151 | val_0_rmse: 0.37839 | val_1_rmse: 0.3895  |  0:01:21s
epoch 64 | loss: 0.15489 | val_0_rmse: 0.36704 | val_1_rmse: 0.37819 |  0:01:22s
epoch 65 | loss: 0.15126 | val_0_rmse: 0.36679 | val_1_rmse: 0.3769  |  0:01:24s
epoch 66 | loss: 0.16126 | val_0_rmse: 0.36375 | val_1_rmse: 0.37645 |  0:01:25s
epoch 67 | loss: 0.1522  | val_0_rmse: 0.38168 | val_1_rmse: 0.39239 |  0:01:26s
epoch 68 | loss: 0.15148 | val_0_rmse: 0.37175 | val_1_rmse: 0.3883  |  0:01:27s
epoch 69 | loss: 0.15039 | val_0_rmse: 0.36748 | val_1_rmse: 0.37968 |  0:01:29s
epoch 70 | loss: 0.14686 | val_0_rmse: 0.3709  | val_1_rmse: 0.38735 |  0:01:30s
epoch 71 | loss: 0.14758 | val_0_rmse: 0.3654  | val_1_rmse: 0.38539 |  0:01:31s
epoch 72 | loss: 0.15111 | val_0_rmse: 0.36948 | val_1_rmse: 0.38301 |  0:01:32s
epoch 73 | loss: 0.14751 | val_0_rmse: 0.37294 | val_1_rmse: 0.38805 |  0:01:34s
epoch 74 | loss: 0.14956 | val_0_rmse: 0.37789 | val_1_rmse: 0.39035 |  0:01:35s
epoch 75 | loss: 0.14831 | val_0_rmse: 0.3746  | val_1_rmse: 0.39747 |  0:01:36s
epoch 76 | loss: 0.14557 | val_0_rmse: 0.38042 | val_1_rmse: 0.39132 |  0:01:38s
epoch 77 | loss: 0.14784 | val_0_rmse: 0.38539 | val_1_rmse: 0.40065 |  0:01:39s
epoch 78 | loss: 0.15531 | val_0_rmse: 0.36578 | val_1_rmse: 0.38116 |  0:01:40s
epoch 79 | loss: 0.15007 | val_0_rmse: 0.37229 | val_1_rmse: 0.39514 |  0:01:41s
epoch 80 | loss: 0.14721 | val_0_rmse: 0.3712  | val_1_rmse: 0.38628 |  0:01:43s
epoch 81 | loss: 0.14455 | val_0_rmse: 0.36239 | val_1_rmse: 0.39034 |  0:01:44s
epoch 82 | loss: 0.14172 | val_0_rmse: 0.36178 | val_1_rmse: 0.38433 |  0:01:45s
epoch 83 | loss: 0.14494 | val_0_rmse: 0.35243 | val_1_rmse: 0.37631 |  0:01:46s
epoch 84 | loss: 0.14505 | val_0_rmse: 0.35508 | val_1_rmse: 0.37897 |  0:01:48s
epoch 85 | loss: 0.14368 | val_0_rmse: 0.35143 | val_1_rmse: 0.37871 |  0:01:49s
epoch 86 | loss: 0.1399  | val_0_rmse: 0.36101 | val_1_rmse: 0.38009 |  0:01:50s
epoch 87 | loss: 0.14079 | val_0_rmse: 0.40286 | val_1_rmse: 0.41775 |  0:01:51s
epoch 88 | loss: 0.1396  | val_0_rmse: 0.36239 | val_1_rmse: 0.39331 |  0:01:53s
epoch 89 | loss: 0.14546 | val_0_rmse: 0.37763 | val_1_rmse: 0.39769 |  0:01:54s
epoch 90 | loss: 0.15003 | val_0_rmse: 0.3614  | val_1_rmse: 0.39139 |  0:01:55s
epoch 91 | loss: 0.14674 | val_0_rmse: 0.35474 | val_1_rmse: 0.37934 |  0:01:57s
epoch 92 | loss: 0.14715 | val_0_rmse: 0.36446 | val_1_rmse: 0.38572 |  0:01:58s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.37462
Best weights from best epoch are automatically used!
ended training at: 04:54:55
Feature importance:
[('Area', 0.0), ('Baths', 4.2656782550547455e-06), ('Beds', 0.0), ('Latitude', 0.41982197842869556), ('Longitude', 0.11170391325806997), ('Month', 1.2217332568418616e-06), ('Year', 4.153112325124912e-07), ('sqft_lot', 0.0), ('floors', 0.08807156175373633), ('waterfront', 0.00010963169436294767), ('view', 0.0), ('condition', 0.0), ('grade', 0.23568665063831903), ('sqft_above', 0.13321553176346698), ('sqft_basement', 0.0), ('yr_renovated', 0.0001309205282248257), ('zipcode', 0.01125390921237994)]
Mean squared error is of 4250549803.458733
Mean absolute error:47212.38243312783
MAPE:0.12969621592453134
R2 score:0.859971876634473
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:54:55
epoch 0  | loss: 1.25198 | val_0_rmse: 0.82386 | val_1_rmse: 0.79402 |  0:00:01s
epoch 1  | loss: 0.41653 | val_0_rmse: 0.62649 | val_1_rmse: 0.60311 |  0:00:02s
epoch 2  | loss: 0.35489 | val_0_rmse: 0.57677 | val_1_rmse: 0.55149 |  0:00:03s
epoch 3  | loss: 0.31649 | val_0_rmse: 0.56077 | val_1_rmse: 0.53079 |  0:00:05s
epoch 4  | loss: 0.31426 | val_0_rmse: 0.54551 | val_1_rmse: 0.5193  |  0:00:06s
epoch 5  | loss: 0.30391 | val_0_rmse: 0.52374 | val_1_rmse: 0.4997  |  0:00:07s
epoch 6  | loss: 0.2887  | val_0_rmse: 0.52976 | val_1_rmse: 0.50482 |  0:00:09s
epoch 7  | loss: 0.28105 | val_0_rmse: 0.50142 | val_1_rmse: 0.47456 |  0:00:10s
epoch 8  | loss: 0.26569 | val_0_rmse: 0.50261 | val_1_rmse: 0.47513 |  0:00:11s
epoch 9  | loss: 0.2612  | val_0_rmse: 0.49912 | val_1_rmse: 0.47028 |  0:00:12s
epoch 10 | loss: 0.2504  | val_0_rmse: 0.50203 | val_1_rmse: 0.4764  |  0:00:14s
epoch 11 | loss: 0.24266 | val_0_rmse: 0.47845 | val_1_rmse: 0.45543 |  0:00:15s
epoch 12 | loss: 0.23755 | val_0_rmse: 0.47031 | val_1_rmse: 0.4447  |  0:00:16s
epoch 13 | loss: 0.23469 | val_0_rmse: 0.48241 | val_1_rmse: 0.45988 |  0:00:17s
epoch 14 | loss: 0.23187 | val_0_rmse: 0.47129 | val_1_rmse: 0.44701 |  0:00:19s
epoch 15 | loss: 0.22979 | val_0_rmse: 0.47672 | val_1_rmse: 0.45299 |  0:00:20s
epoch 16 | loss: 0.22737 | val_0_rmse: 0.48495 | val_1_rmse: 0.4586  |  0:00:21s
epoch 17 | loss: 0.23361 | val_0_rmse: 0.48953 | val_1_rmse: 0.4628  |  0:00:23s
epoch 18 | loss: 0.24339 | val_0_rmse: 0.48248 | val_1_rmse: 0.45998 |  0:00:24s
epoch 19 | loss: 0.23852 | val_0_rmse: 0.49999 | val_1_rmse: 0.47476 |  0:00:25s
epoch 20 | loss: 0.2472  | val_0_rmse: 0.4803  | val_1_rmse: 0.45294 |  0:00:26s
epoch 21 | loss: 0.27191 | val_0_rmse: 0.50678 | val_1_rmse: 0.48275 |  0:00:28s
epoch 22 | loss: 0.24139 | val_0_rmse: 0.47225 | val_1_rmse: 0.44987 |  0:00:29s
epoch 23 | loss: 0.22899 | val_0_rmse: 0.46357 | val_1_rmse: 0.43999 |  0:00:30s
epoch 24 | loss: 0.22043 | val_0_rmse: 0.44576 | val_1_rmse: 0.42152 |  0:00:31s
epoch 25 | loss: 0.21539 | val_0_rmse: 0.44917 | val_1_rmse: 0.42619 |  0:00:33s
epoch 26 | loss: 0.20926 | val_0_rmse: 0.44581 | val_1_rmse: 0.42418 |  0:00:34s
epoch 27 | loss: 0.21676 | val_0_rmse: 0.46908 | val_1_rmse: 0.44997 |  0:00:35s
epoch 28 | loss: 0.21174 | val_0_rmse: 0.45877 | val_1_rmse: 0.43516 |  0:00:36s
epoch 29 | loss: 0.2129  | val_0_rmse: 0.44805 | val_1_rmse: 0.42133 |  0:00:38s
epoch 30 | loss: 0.20905 | val_0_rmse: 0.48506 | val_1_rmse: 0.46722 |  0:00:39s
epoch 31 | loss: 0.22467 | val_0_rmse: 0.45471 | val_1_rmse: 0.43342 |  0:00:40s
epoch 32 | loss: 0.2177  | val_0_rmse: 0.45192 | val_1_rmse: 0.43107 |  0:00:42s
epoch 33 | loss: 0.2098  | val_0_rmse: 0.45823 | val_1_rmse: 0.43423 |  0:00:43s
epoch 34 | loss: 0.20452 | val_0_rmse: 0.46395 | val_1_rmse: 0.44208 |  0:00:44s
epoch 35 | loss: 0.20067 | val_0_rmse: 0.44024 | val_1_rmse: 0.41783 |  0:00:45s
epoch 36 | loss: 0.20322 | val_0_rmse: 0.43727 | val_1_rmse: 0.41762 |  0:00:47s
epoch 37 | loss: 0.20456 | val_0_rmse: 0.45926 | val_1_rmse: 0.43405 |  0:00:48s
epoch 38 | loss: 0.20306 | val_0_rmse: 0.43676 | val_1_rmse: 0.41318 |  0:00:49s
epoch 39 | loss: 0.20422 | val_0_rmse: 0.43494 | val_1_rmse: 0.41622 |  0:00:50s
epoch 40 | loss: 0.20241 | val_0_rmse: 0.44208 | val_1_rmse: 0.42015 |  0:00:52s
epoch 41 | loss: 0.20846 | val_0_rmse: 0.46652 | val_1_rmse: 0.44662 |  0:00:53s
epoch 42 | loss: 0.21122 | val_0_rmse: 0.44279 | val_1_rmse: 0.42211 |  0:00:54s
epoch 43 | loss: 0.2067  | val_0_rmse: 0.46261 | val_1_rmse: 0.44127 |  0:00:56s
epoch 44 | loss: 0.20267 | val_0_rmse: 0.42696 | val_1_rmse: 0.41356 |  0:00:57s
epoch 45 | loss: 0.20196 | val_0_rmse: 0.434   | val_1_rmse: 0.40812 |  0:00:58s
epoch 46 | loss: 0.20142 | val_0_rmse: 0.42841 | val_1_rmse: 0.40406 |  0:00:59s
epoch 47 | loss: 0.2018  | val_0_rmse: 0.43186 | val_1_rmse: 0.41039 |  0:01:01s
epoch 48 | loss: 0.20123 | val_0_rmse: 0.43737 | val_1_rmse: 0.41566 |  0:01:02s
epoch 49 | loss: 0.19837 | val_0_rmse: 0.46334 | val_1_rmse: 0.45592 |  0:01:03s
epoch 50 | loss: 0.21528 | val_0_rmse: 0.44356 | val_1_rmse: 0.42689 |  0:01:05s
epoch 51 | loss: 0.20808 | val_0_rmse: 0.42716 | val_1_rmse: 0.40792 |  0:01:06s
epoch 52 | loss: 0.19869 | val_0_rmse: 0.42479 | val_1_rmse: 0.40306 |  0:01:07s
epoch 53 | loss: 0.19785 | val_0_rmse: 0.44057 | val_1_rmse: 0.42027 |  0:01:08s
epoch 54 | loss: 0.20334 | val_0_rmse: 0.45052 | val_1_rmse: 0.42836 |  0:01:10s
epoch 55 | loss: 0.2064  | val_0_rmse: 0.42898 | val_1_rmse: 0.41221 |  0:01:11s
epoch 56 | loss: 0.19476 | val_0_rmse: 0.42279 | val_1_rmse: 0.40972 |  0:01:12s
epoch 57 | loss: 0.19462 | val_0_rmse: 0.43474 | val_1_rmse: 0.41393 |  0:01:14s
epoch 58 | loss: 0.21796 | val_0_rmse: 0.45509 | val_1_rmse: 0.44245 |  0:01:15s
epoch 59 | loss: 0.21284 | val_0_rmse: 0.42649 | val_1_rmse: 0.41218 |  0:01:16s
epoch 60 | loss: 0.19719 | val_0_rmse: 0.44124 | val_1_rmse: 0.42015 |  0:01:17s
epoch 61 | loss: 0.19746 | val_0_rmse: 0.43316 | val_1_rmse: 0.41404 |  0:01:19s
epoch 62 | loss: 0.20033 | val_0_rmse: 0.42439 | val_1_rmse: 0.40651 |  0:01:20s
epoch 63 | loss: 0.19663 | val_0_rmse: 0.44319 | val_1_rmse: 0.42714 |  0:01:21s
epoch 64 | loss: 0.20216 | val_0_rmse: 0.45056 | val_1_rmse: 0.43246 |  0:01:22s
epoch 65 | loss: 0.21084 | val_0_rmse: 0.43963 | val_1_rmse: 0.4163  |  0:01:24s
epoch 66 | loss: 0.20728 | val_0_rmse: 0.43637 | val_1_rmse: 0.41475 |  0:01:25s
epoch 67 | loss: 0.1968  | val_0_rmse: 0.43155 | val_1_rmse: 0.40982 |  0:01:26s
epoch 68 | loss: 0.19855 | val_0_rmse: 0.4293  | val_1_rmse: 0.40693 |  0:01:28s
epoch 69 | loss: 0.20018 | val_0_rmse: 0.43529 | val_1_rmse: 0.41247 |  0:01:29s
epoch 70 | loss: 0.19681 | val_0_rmse: 0.4433  | val_1_rmse: 0.42389 |  0:01:30s
epoch 71 | loss: 0.19968 | val_0_rmse: 0.4301  | val_1_rmse: 0.41373 |  0:01:31s
epoch 72 | loss: 0.19475 | val_0_rmse: 0.42895 | val_1_rmse: 0.41224 |  0:01:33s
epoch 73 | loss: 0.19028 | val_0_rmse: 0.4182  | val_1_rmse: 0.40015 |  0:01:34s
epoch 74 | loss: 0.18846 | val_0_rmse: 0.44105 | val_1_rmse: 0.42878 |  0:01:35s
epoch 75 | loss: 0.19644 | val_0_rmse: 0.47    | val_1_rmse: 0.4493  |  0:01:37s
epoch 76 | loss: 0.22393 | val_0_rmse: 0.45998 | val_1_rmse: 0.43903 |  0:01:38s
epoch 77 | loss: 0.22519 | val_0_rmse: 0.48392 | val_1_rmse: 0.46961 |  0:01:39s
epoch 78 | loss: 0.2241  | val_0_rmse: 0.4591  | val_1_rmse: 0.43569 |  0:01:40s
epoch 79 | loss: 0.21314 | val_0_rmse: 0.46108 | val_1_rmse: 0.4399  |  0:01:42s
epoch 80 | loss: 0.23563 | val_0_rmse: 0.46204 | val_1_rmse: 0.43767 |  0:01:43s
epoch 81 | loss: 0.22618 | val_0_rmse: 0.46278 | val_1_rmse: 0.43394 |  0:01:44s
epoch 82 | loss: 0.22996 | val_0_rmse: 0.4595  | val_1_rmse: 0.43208 |  0:01:45s
epoch 83 | loss: 0.22457 | val_0_rmse: 0.45761 | val_1_rmse: 0.43523 |  0:01:47s
epoch 84 | loss: 0.21875 | val_0_rmse: 0.45077 | val_1_rmse: 0.42831 |  0:01:48s
epoch 85 | loss: 0.22621 | val_0_rmse: 0.47862 | val_1_rmse: 0.45421 |  0:01:49s
epoch 86 | loss: 0.22324 | val_0_rmse: 0.46046 | val_1_rmse: 0.43551 |  0:01:50s
epoch 87 | loss: 0.22406 | val_0_rmse: 0.46897 | val_1_rmse: 0.4433  |  0:01:52s
epoch 88 | loss: 0.21672 | val_0_rmse: 0.45956 | val_1_rmse: 0.4355  |  0:01:53s
epoch 89 | loss: 0.22996 | val_0_rmse: 0.48553 | val_1_rmse: 0.4677  |  0:01:54s
epoch 90 | loss: 0.22201 | val_0_rmse: 0.45135 | val_1_rmse: 0.4286  |  0:01:56s
epoch 91 | loss: 0.21625 | val_0_rmse: 0.45778 | val_1_rmse: 0.43624 |  0:01:57s
epoch 92 | loss: 0.22501 | val_0_rmse: 0.47385 | val_1_rmse: 0.4478  |  0:01:58s
epoch 93 | loss: 0.24544 | val_0_rmse: 0.4903  | val_1_rmse: 0.46608 |  0:01:59s
epoch 94 | loss: 0.23745 | val_0_rmse: 0.46666 | val_1_rmse: 0.44135 |  0:02:01s
epoch 95 | loss: 0.23014 | val_0_rmse: 0.52989 | val_1_rmse: 0.5053  |  0:02:02s
epoch 96 | loss: 0.22677 | val_0_rmse: 0.46971 | val_1_rmse: 0.44491 |  0:02:03s
epoch 97 | loss: 0.22279 | val_0_rmse: 0.46792 | val_1_rmse: 0.442   |  0:02:04s
epoch 98 | loss: 0.23803 | val_0_rmse: 0.48961 | val_1_rmse: 0.46275 |  0:02:06s
epoch 99 | loss: 0.24198 | val_0_rmse: 0.47997 | val_1_rmse: 0.45603 |  0:02:07s
epoch 100| loss: 0.24219 | val_0_rmse: 0.49618 | val_1_rmse: 0.4698  |  0:02:08s
epoch 101| loss: 0.22877 | val_0_rmse: 0.47005 | val_1_rmse: 0.44409 |  0:02:10s
epoch 102| loss: 0.22612 | val_0_rmse: 0.47047 | val_1_rmse: 0.44204 |  0:02:11s
epoch 103| loss: 0.22411 | val_0_rmse: 0.46131 | val_1_rmse: 0.43414 |  0:02:12s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.40015
Best weights from best epoch are automatically used!
ended training at: 04:57:08
Feature importance:
[('Area', 0.31425969733829945), ('Baths', 0.0), ('Beds', 0.1036829798731645), ('Latitude', 0.39301211782555495), ('Longitude', 0.0), ('Month', 0.0), ('Year', 9.234336485397054e-05), ('sqft_lot', 0.00037942137444424293), ('floors', 7.776676881425511e-07), ('waterfront', 0.06623991103685406), ('view', 0.0), ('condition', 0.0), ('grade', 0.1055570451605505), ('sqft_above', 0.014814518984772545), ('sqft_basement', 0.0019611873738176345), ('yr_renovated', 0.0), ('zipcode', 0.0)]
Mean squared error is of 5196843627.681744
Mean absolute error:52374.460112654124
MAPE:0.13962714340379134
R2 score:0.8293318387662157
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:57:08
epoch 0  | loss: 1.25643 | val_0_rmse: 0.86625 | val_1_rmse: 0.85472 |  0:00:01s
epoch 1  | loss: 0.41836 | val_0_rmse: 0.78745 | val_1_rmse: 0.7949  |  0:00:02s
epoch 2  | loss: 0.32839 | val_0_rmse: 0.55222 | val_1_rmse: 0.55342 |  0:00:03s
epoch 3  | loss: 0.29872 | val_0_rmse: 0.52424 | val_1_rmse: 0.5159  |  0:00:05s
epoch 4  | loss: 0.28312 | val_0_rmse: 0.52015 | val_1_rmse: 0.51306 |  0:00:06s
epoch 5  | loss: 0.27517 | val_0_rmse: 0.51841 | val_1_rmse: 0.5091  |  0:00:07s
epoch 6  | loss: 0.26321 | val_0_rmse: 0.5107  | val_1_rmse: 0.51344 |  0:00:08s
epoch 7  | loss: 0.24628 | val_0_rmse: 0.47402 | val_1_rmse: 0.47575 |  0:00:09s
epoch 8  | loss: 0.2471  | val_0_rmse: 0.47874 | val_1_rmse: 0.48783 |  0:00:10s
epoch 9  | loss: 0.23432 | val_0_rmse: 0.45801 | val_1_rmse: 0.46246 |  0:00:12s
epoch 10 | loss: 0.22499 | val_0_rmse: 0.4677  | val_1_rmse: 0.47184 |  0:00:13s
epoch 11 | loss: 0.22478 | val_0_rmse: 0.45424 | val_1_rmse: 0.45592 |  0:00:14s
epoch 12 | loss: 0.22017 | val_0_rmse: 0.4521  | val_1_rmse: 0.45404 |  0:00:15s
epoch 13 | loss: 0.2139  | val_0_rmse: 0.44273 | val_1_rmse: 0.44805 |  0:00:16s
epoch 14 | loss: 0.21117 | val_0_rmse: 0.45858 | val_1_rmse: 0.46268 |  0:00:17s
epoch 15 | loss: 0.20988 | val_0_rmse: 0.44552 | val_1_rmse: 0.45079 |  0:00:19s
epoch 16 | loss: 0.20789 | val_0_rmse: 0.43942 | val_1_rmse: 0.44456 |  0:00:20s
epoch 17 | loss: 0.20617 | val_0_rmse: 0.42781 | val_1_rmse: 0.43167 |  0:00:21s
epoch 18 | loss: 0.20272 | val_0_rmse: 0.43639 | val_1_rmse: 0.43592 |  0:00:22s
epoch 19 | loss: 0.20343 | val_0_rmse: 0.43583 | val_1_rmse: 0.43893 |  0:00:23s
epoch 20 | loss: 0.199   | val_0_rmse: 0.42695 | val_1_rmse: 0.43061 |  0:00:24s
epoch 21 | loss: 0.19843 | val_0_rmse: 0.42187 | val_1_rmse: 0.42354 |  0:00:26s
epoch 22 | loss: 0.19179 | val_0_rmse: 0.4253  | val_1_rmse: 0.42768 |  0:00:27s
epoch 23 | loss: 0.19211 | val_0_rmse: 0.42769 | val_1_rmse: 0.43523 |  0:00:28s
epoch 24 | loss: 0.19418 | val_0_rmse: 0.4192  | val_1_rmse: 0.42044 |  0:00:29s
epoch 25 | loss: 0.18882 | val_0_rmse: 0.41515 | val_1_rmse: 0.41975 |  0:00:30s
epoch 26 | loss: 0.19072 | val_0_rmse: 0.41403 | val_1_rmse: 0.41722 |  0:00:31s
epoch 27 | loss: 0.18093 | val_0_rmse: 0.41393 | val_1_rmse: 0.41892 |  0:00:32s
epoch 28 | loss: 0.18346 | val_0_rmse: 0.40549 | val_1_rmse: 0.41085 |  0:00:34s
epoch 29 | loss: 0.18217 | val_0_rmse: 0.41618 | val_1_rmse: 0.42713 |  0:00:35s
epoch 30 | loss: 0.18184 | val_0_rmse: 0.41444 | val_1_rmse: 0.42172 |  0:00:36s
epoch 31 | loss: 0.18359 | val_0_rmse: 0.40456 | val_1_rmse: 0.41336 |  0:00:37s
epoch 32 | loss: 0.17928 | val_0_rmse: 0.40416 | val_1_rmse: 0.41525 |  0:00:38s
epoch 33 | loss: 0.18218 | val_0_rmse: 0.40969 | val_1_rmse: 0.4203  |  0:00:39s
epoch 34 | loss: 0.17517 | val_0_rmse: 0.40146 | val_1_rmse: 0.41198 |  0:00:41s
epoch 35 | loss: 0.17604 | val_0_rmse: 0.39563 | val_1_rmse: 0.4037  |  0:00:42s
epoch 36 | loss: 0.17562 | val_0_rmse: 0.41212 | val_1_rmse: 0.41968 |  0:00:43s
epoch 37 | loss: 0.18119 | val_0_rmse: 0.40402 | val_1_rmse: 0.41346 |  0:00:44s
epoch 38 | loss: 0.17308 | val_0_rmse: 0.39386 | val_1_rmse: 0.39968 |  0:00:45s
epoch 39 | loss: 0.17255 | val_0_rmse: 0.41236 | val_1_rmse: 0.42291 |  0:00:46s
epoch 40 | loss: 0.17447 | val_0_rmse: 0.40799 | val_1_rmse: 0.41782 |  0:00:47s
epoch 41 | loss: 0.1792  | val_0_rmse: 0.40577 | val_1_rmse: 0.41468 |  0:00:49s
epoch 42 | loss: 0.16981 | val_0_rmse: 0.39314 | val_1_rmse: 0.40663 |  0:00:50s
epoch 43 | loss: 0.17543 | val_0_rmse: 0.41408 | val_1_rmse: 0.42221 |  0:00:51s
epoch 44 | loss: 0.17858 | val_0_rmse: 0.4026  | val_1_rmse: 0.41379 |  0:00:52s
epoch 45 | loss: 0.17469 | val_0_rmse: 0.39934 | val_1_rmse: 0.41114 |  0:00:53s
epoch 46 | loss: 0.17487 | val_0_rmse: 0.39852 | val_1_rmse: 0.4101  |  0:00:54s
epoch 47 | loss: 0.17439 | val_0_rmse: 0.39705 | val_1_rmse: 0.40292 |  0:00:55s
epoch 48 | loss: 0.17019 | val_0_rmse: 0.39077 | val_1_rmse: 0.40239 |  0:00:57s
epoch 49 | loss: 0.16659 | val_0_rmse: 0.39319 | val_1_rmse: 0.40378 |  0:00:58s
epoch 50 | loss: 0.1721  | val_0_rmse: 0.39318 | val_1_rmse: 0.40588 |  0:00:59s
epoch 51 | loss: 0.17029 | val_0_rmse: 0.39795 | val_1_rmse: 0.40881 |  0:01:00s
epoch 52 | loss: 0.1718  | val_0_rmse: 0.38914 | val_1_rmse: 0.40213 |  0:01:01s
epoch 53 | loss: 0.17188 | val_0_rmse: 0.40255 | val_1_rmse: 0.41518 |  0:01:02s
epoch 54 | loss: 0.17033 | val_0_rmse: 0.40305 | val_1_rmse: 0.4136  |  0:01:03s
epoch 55 | loss: 0.16535 | val_0_rmse: 0.38137 | val_1_rmse: 0.39258 |  0:01:05s
epoch 56 | loss: 0.1706  | val_0_rmse: 0.38873 | val_1_rmse: 0.40412 |  0:01:06s
epoch 57 | loss: 0.16407 | val_0_rmse: 0.38764 | val_1_rmse: 0.4014  |  0:01:07s
epoch 58 | loss: 0.16527 | val_0_rmse: 0.39143 | val_1_rmse: 0.4053  |  0:01:08s
epoch 59 | loss: 0.16429 | val_0_rmse: 0.38675 | val_1_rmse: 0.40431 |  0:01:09s
epoch 60 | loss: 0.16342 | val_0_rmse: 0.37906 | val_1_rmse: 0.39367 |  0:01:10s
epoch 61 | loss: 0.16864 | val_0_rmse: 0.38579 | val_1_rmse: 0.40559 |  0:01:12s
epoch 62 | loss: 0.16598 | val_0_rmse: 0.38109 | val_1_rmse: 0.39656 |  0:01:13s
epoch 63 | loss: 0.16608 | val_0_rmse: 0.38799 | val_1_rmse: 0.40103 |  0:01:14s
epoch 64 | loss: 0.1657  | val_0_rmse: 0.38585 | val_1_rmse: 0.40193 |  0:01:15s
epoch 65 | loss: 0.16277 | val_0_rmse: 0.4008  | val_1_rmse: 0.41575 |  0:01:16s
epoch 66 | loss: 0.17113 | val_0_rmse: 0.38348 | val_1_rmse: 0.40182 |  0:01:17s
epoch 67 | loss: 0.16514 | val_0_rmse: 0.38835 | val_1_rmse: 0.40881 |  0:01:18s
epoch 68 | loss: 0.16621 | val_0_rmse: 0.38396 | val_1_rmse: 0.40086 |  0:01:20s
epoch 69 | loss: 0.16512 | val_0_rmse: 0.38211 | val_1_rmse: 0.39858 |  0:01:21s
epoch 70 | loss: 0.16799 | val_0_rmse: 0.3902  | val_1_rmse: 0.40468 |  0:01:22s
epoch 71 | loss: 0.16689 | val_0_rmse: 0.38845 | val_1_rmse: 0.40318 |  0:01:23s
epoch 72 | loss: 0.15954 | val_0_rmse: 0.37865 | val_1_rmse: 0.39776 |  0:01:24s
epoch 73 | loss: 0.16386 | val_0_rmse: 0.37468 | val_1_rmse: 0.3966  |  0:01:25s
epoch 74 | loss: 0.15581 | val_0_rmse: 0.37988 | val_1_rmse: 0.398   |  0:01:26s
epoch 75 | loss: 0.15764 | val_0_rmse: 0.3794  | val_1_rmse: 0.39937 |  0:01:28s
epoch 76 | loss: 0.16173 | val_0_rmse: 0.38377 | val_1_rmse: 0.40095 |  0:01:29s
epoch 77 | loss: 0.16172 | val_0_rmse: 0.38387 | val_1_rmse: 0.40469 |  0:01:30s
epoch 78 | loss: 0.15826 | val_0_rmse: 0.37513 | val_1_rmse: 0.38948 |  0:01:31s
epoch 79 | loss: 0.1558  | val_0_rmse: 0.37346 | val_1_rmse: 0.39003 |  0:01:32s
epoch 80 | loss: 0.16222 | val_0_rmse: 0.38646 | val_1_rmse: 0.41138 |  0:01:33s
epoch 81 | loss: 0.16287 | val_0_rmse: 0.38265 | val_1_rmse: 0.40153 |  0:01:35s
epoch 82 | loss: 0.15975 | val_0_rmse: 0.39679 | val_1_rmse: 0.4154  |  0:01:36s
epoch 83 | loss: 0.16257 | val_0_rmse: 0.38358 | val_1_rmse: 0.40343 |  0:01:37s
epoch 84 | loss: 0.15884 | val_0_rmse: 0.38031 | val_1_rmse: 0.40308 |  0:01:38s
epoch 85 | loss: 0.16061 | val_0_rmse: 0.38058 | val_1_rmse: 0.40105 |  0:01:39s
epoch 86 | loss: 0.1638  | val_0_rmse: 0.41581 | val_1_rmse: 0.43197 |  0:01:40s
epoch 87 | loss: 0.15846 | val_0_rmse: 0.37278 | val_1_rmse: 0.3958  |  0:01:41s
epoch 88 | loss: 0.16081 | val_0_rmse: 0.36897 | val_1_rmse: 0.39339 |  0:01:43s
epoch 89 | loss: 0.15469 | val_0_rmse: 0.37311 | val_1_rmse: 0.39522 |  0:01:44s
epoch 90 | loss: 0.15962 | val_0_rmse: 0.36637 | val_1_rmse: 0.39189 |  0:01:45s
epoch 91 | loss: 0.15646 | val_0_rmse: 0.38065 | val_1_rmse: 0.40016 |  0:01:46s
epoch 92 | loss: 0.15145 | val_0_rmse: 0.37578 | val_1_rmse: 0.40387 |  0:01:47s
epoch 93 | loss: 0.15488 | val_0_rmse: 0.37184 | val_1_rmse: 0.39732 |  0:01:48s
epoch 94 | loss: 0.15234 | val_0_rmse: 0.37891 | val_1_rmse: 0.40375 |  0:01:50s
epoch 95 | loss: 0.15465 | val_0_rmse: 0.39512 | val_1_rmse: 0.42014 |  0:01:51s
epoch 96 | loss: 0.15217 | val_0_rmse: 0.36996 | val_1_rmse: 0.38943 |  0:01:52s
epoch 97 | loss: 0.15759 | val_0_rmse: 0.38392 | val_1_rmse: 0.4043  |  0:01:53s
epoch 98 | loss: 0.15311 | val_0_rmse: 0.37167 | val_1_rmse: 0.39434 |  0:01:54s
epoch 99 | loss: 0.15258 | val_0_rmse: 0.38073 | val_1_rmse: 0.40458 |  0:01:55s
epoch 100| loss: 0.1539  | val_0_rmse: 0.38227 | val_1_rmse: 0.40305 |  0:01:57s
epoch 101| loss: 0.15408 | val_0_rmse: 0.37102 | val_1_rmse: 0.39222 |  0:01:58s
epoch 102| loss: 0.15942 | val_0_rmse: 0.37744 | val_1_rmse: 0.40269 |  0:01:59s
epoch 103| loss: 0.15901 | val_0_rmse: 0.38002 | val_1_rmse: 0.40968 |  0:02:00s
epoch 104| loss: 0.15234 | val_0_rmse: 0.3697  | val_1_rmse: 0.39164 |  0:02:01s
epoch 105| loss: 0.15251 | val_0_rmse: 0.37454 | val_1_rmse: 0.40086 |  0:02:02s
epoch 106| loss: 0.15095 | val_0_rmse: 0.36423 | val_1_rmse: 0.38537 |  0:02:03s
epoch 107| loss: 0.15062 | val_0_rmse: 0.36412 | val_1_rmse: 0.38945 |  0:02:04s
epoch 108| loss: 0.14819 | val_0_rmse: 0.37442 | val_1_rmse: 0.39776 |  0:02:06s
epoch 109| loss: 0.14967 | val_0_rmse: 0.38094 | val_1_rmse: 0.40183 |  0:02:07s
epoch 110| loss: 0.14797 | val_0_rmse: 0.36612 | val_1_rmse: 0.39385 |  0:02:08s
epoch 111| loss: 0.14562 | val_0_rmse: 0.35887 | val_1_rmse: 0.38715 |  0:02:09s
epoch 112| loss: 0.14864 | val_0_rmse: 0.36212 | val_1_rmse: 0.39151 |  0:02:10s
epoch 113| loss: 0.15112 | val_0_rmse: 0.3563  | val_1_rmse: 0.38524 |  0:02:11s
epoch 114| loss: 0.14544 | val_0_rmse: 0.36138 | val_1_rmse: 0.38951 |  0:02:13s
epoch 115| loss: 0.15045 | val_0_rmse: 0.36957 | val_1_rmse: 0.39857 |  0:02:14s
epoch 116| loss: 0.14378 | val_0_rmse: 0.36232 | val_1_rmse: 0.39328 |  0:02:15s
epoch 117| loss: 0.14488 | val_0_rmse: 0.38167 | val_1_rmse: 0.40702 |  0:02:16s
epoch 118| loss: 0.14793 | val_0_rmse: 0.37138 | val_1_rmse: 0.40237 |  0:02:17s
epoch 119| loss: 0.1469  | val_0_rmse: 0.35349 | val_1_rmse: 0.38242 |  0:02:18s
epoch 120| loss: 0.14429 | val_0_rmse: 0.36466 | val_1_rmse: 0.38959 |  0:02:19s
epoch 121| loss: 0.13997 | val_0_rmse: 0.35829 | val_1_rmse: 0.38863 |  0:02:21s
epoch 122| loss: 0.1436  | val_0_rmse: 0.35748 | val_1_rmse: 0.38639 |  0:02:22s
epoch 123| loss: 0.13923 | val_0_rmse: 0.35169 | val_1_rmse: 0.38001 |  0:02:23s
epoch 124| loss: 0.13798 | val_0_rmse: 0.34932 | val_1_rmse: 0.3831  |  0:02:24s
epoch 125| loss: 0.14492 | val_0_rmse: 0.36462 | val_1_rmse: 0.39263 |  0:02:25s
epoch 126| loss: 0.14828 | val_0_rmse: 0.37112 | val_1_rmse: 0.39534 |  0:02:26s
epoch 127| loss: 0.15031 | val_0_rmse: 0.36502 | val_1_rmse: 0.38262 |  0:02:27s
epoch 128| loss: 0.14582 | val_0_rmse: 0.36961 | val_1_rmse: 0.39286 |  0:02:29s
epoch 129| loss: 0.14878 | val_0_rmse: 0.37241 | val_1_rmse: 0.39667 |  0:02:30s
epoch 130| loss: 0.14942 | val_0_rmse: 0.35796 | val_1_rmse: 0.38373 |  0:02:31s
epoch 131| loss: 0.14431 | val_0_rmse: 0.36658 | val_1_rmse: 0.39133 |  0:02:32s
epoch 132| loss: 0.14616 | val_0_rmse: 0.35065 | val_1_rmse: 0.37502 |  0:02:33s
epoch 133| loss: 0.14584 | val_0_rmse: 0.3591  | val_1_rmse: 0.38953 |  0:02:34s
epoch 134| loss: 0.14734 | val_0_rmse: 0.3565  | val_1_rmse: 0.38389 |  0:02:35s
epoch 135| loss: 0.14016 | val_0_rmse: 0.38329 | val_1_rmse: 0.41254 |  0:02:37s
epoch 136| loss: 0.14354 | val_0_rmse: 0.35548 | val_1_rmse: 0.39068 |  0:02:38s
epoch 137| loss: 0.13555 | val_0_rmse: 0.34768 | val_1_rmse: 0.38605 |  0:02:39s
epoch 138| loss: 0.13387 | val_0_rmse: 0.34365 | val_1_rmse: 0.37623 |  0:02:40s
epoch 139| loss: 0.13429 | val_0_rmse: 0.34449 | val_1_rmse: 0.37498 |  0:02:41s
epoch 140| loss: 0.14297 | val_0_rmse: 0.37442 | val_1_rmse: 0.40668 |  0:02:42s
epoch 141| loss: 0.14219 | val_0_rmse: 0.36733 | val_1_rmse: 0.40004 |  0:02:44s
epoch 142| loss: 0.14157 | val_0_rmse: 0.35413 | val_1_rmse: 0.38338 |  0:02:45s
epoch 143| loss: 0.13787 | val_0_rmse: 0.35291 | val_1_rmse: 0.38722 |  0:02:46s
epoch 144| loss: 0.13877 | val_0_rmse: 0.35298 | val_1_rmse: 0.38657 |  0:02:47s
epoch 145| loss: 0.13671 | val_0_rmse: 0.36349 | val_1_rmse: 0.39844 |  0:02:48s
epoch 146| loss: 0.13671 | val_0_rmse: 0.34996 | val_1_rmse: 0.38732 |  0:02:49s
epoch 147| loss: 0.13332 | val_0_rmse: 0.3461  | val_1_rmse: 0.38336 |  0:02:50s
epoch 148| loss: 0.13118 | val_0_rmse: 0.34283 | val_1_rmse: 0.38264 |  0:02:52s
epoch 149| loss: 0.13434 | val_0_rmse: 0.34415 | val_1_rmse: 0.38284 |  0:02:53s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.37498
Best weights from best epoch are automatically used!
ended training at: 05:00:02
Feature importance:
[('Area', 0.062429938473238016), ('Baths', 0.01328211156851623), ('Beds', 0.0), ('Latitude', 0.33897294522611254), ('Longitude', 0.032632993746574475), ('Month', 0.0), ('Year', 0.08338289932636393), ('sqft_lot', 0.0), ('floors', 1.2585149805651598e-05), ('waterfront', 0.0038556957749858287), ('view', 0.017668238744518564), ('condition', 0.0), ('grade', 0.17806807447928402), ('sqft_above', 0.05172813549787088), ('sqft_basement', 0.14532176372642225), ('yr_renovated', 0.0), ('zipcode', 0.0726446182863076)]
Mean squared error is of 4319155244.928148
Mean absolute error:46704.03990919099
MAPE:0.12839208541346506
R2 score:0.8544708359110617
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:00:02
epoch 0  | loss: 1.22849 | val_0_rmse: 0.73862 | val_1_rmse: 0.73412 |  0:00:01s
epoch 1  | loss: 0.42342 | val_0_rmse: 0.66137 | val_1_rmse: 0.63526 |  0:00:02s
epoch 2  | loss: 0.35873 | val_0_rmse: 0.58459 | val_1_rmse: 0.5853  |  0:00:03s
epoch 3  | loss: 0.31799 | val_0_rmse: 0.54686 | val_1_rmse: 0.54449 |  0:00:04s
epoch 4  | loss: 0.30387 | val_0_rmse: 0.51505 | val_1_rmse: 0.51236 |  0:00:05s
epoch 5  | loss: 0.28112 | val_0_rmse: 0.50654 | val_1_rmse: 0.50766 |  0:00:06s
epoch 6  | loss: 0.28884 | val_0_rmse: 0.53171 | val_1_rmse: 0.53974 |  0:00:08s
epoch 7  | loss: 0.28604 | val_0_rmse: 0.52204 | val_1_rmse: 0.52611 |  0:00:09s
epoch 8  | loss: 0.26201 | val_0_rmse: 0.50834 | val_1_rmse: 0.51021 |  0:00:10s
epoch 9  | loss: 0.26085 | val_0_rmse: 0.50809 | val_1_rmse: 0.50618 |  0:00:11s
epoch 10 | loss: 0.25768 | val_0_rmse: 0.49027 | val_1_rmse: 0.4906  |  0:00:12s
epoch 11 | loss: 0.25673 | val_0_rmse: 0.49175 | val_1_rmse: 0.49418 |  0:00:13s
epoch 12 | loss: 0.25517 | val_0_rmse: 0.49466 | val_1_rmse: 0.50076 |  0:00:15s
epoch 13 | loss: 0.26193 | val_0_rmse: 0.49444 | val_1_rmse: 0.49039 |  0:00:16s
epoch 14 | loss: 0.24567 | val_0_rmse: 0.47106 | val_1_rmse: 0.47208 |  0:00:17s
epoch 15 | loss: 0.23862 | val_0_rmse: 0.48127 | val_1_rmse: 0.49198 |  0:00:18s
epoch 16 | loss: 0.23841 | val_0_rmse: 0.48842 | val_1_rmse: 0.48417 |  0:00:19s
epoch 17 | loss: 0.23965 | val_0_rmse: 0.48894 | val_1_rmse: 0.48879 |  0:00:20s
epoch 18 | loss: 0.24337 | val_0_rmse: 0.46797 | val_1_rmse: 0.46449 |  0:00:21s
epoch 19 | loss: 0.22553 | val_0_rmse: 0.46092 | val_1_rmse: 0.46404 |  0:00:23s
epoch 20 | loss: 0.22707 | val_0_rmse: 0.45146 | val_1_rmse: 0.45642 |  0:00:24s
epoch 21 | loss: 0.21979 | val_0_rmse: 0.48007 | val_1_rmse: 0.48733 |  0:00:25s
epoch 22 | loss: 0.22146 | val_0_rmse: 0.47169 | val_1_rmse: 0.4766  |  0:00:26s
epoch 23 | loss: 0.22051 | val_0_rmse: 0.45028 | val_1_rmse: 0.45529 |  0:00:27s
epoch 24 | loss: 0.21687 | val_0_rmse: 0.46461 | val_1_rmse: 0.46742 |  0:00:28s
epoch 25 | loss: 0.21667 | val_0_rmse: 0.45293 | val_1_rmse: 0.45409 |  0:00:30s
epoch 26 | loss: 0.23209 | val_0_rmse: 0.48537 | val_1_rmse: 0.47817 |  0:00:31s
epoch 27 | loss: 0.24351 | val_0_rmse: 0.46012 | val_1_rmse: 0.46113 |  0:00:32s
epoch 28 | loss: 0.22645 | val_0_rmse: 0.45643 | val_1_rmse: 0.45533 |  0:00:33s
epoch 29 | loss: 0.24244 | val_0_rmse: 0.49423 | val_1_rmse: 0.48923 |  0:00:34s
epoch 30 | loss: 0.22565 | val_0_rmse: 0.46222 | val_1_rmse: 0.46588 |  0:00:35s
epoch 31 | loss: 0.21607 | val_0_rmse: 0.45032 | val_1_rmse: 0.45634 |  0:00:36s
epoch 32 | loss: 0.21885 | val_0_rmse: 0.45895 | val_1_rmse: 0.46039 |  0:00:38s
epoch 33 | loss: 0.22076 | val_0_rmse: 0.46515 | val_1_rmse: 0.46174 |  0:00:39s
epoch 34 | loss: 0.21416 | val_0_rmse: 0.444   | val_1_rmse: 0.44087 |  0:00:40s
epoch 35 | loss: 0.21303 | val_0_rmse: 0.44344 | val_1_rmse: 0.4385  |  0:00:41s
epoch 36 | loss: 0.21639 | val_0_rmse: 0.46529 | val_1_rmse: 0.46414 |  0:00:42s
epoch 37 | loss: 0.22034 | val_0_rmse: 0.46433 | val_1_rmse: 0.45845 |  0:00:43s
epoch 38 | loss: 0.21007 | val_0_rmse: 0.4636  | val_1_rmse: 0.45905 |  0:00:44s
epoch 39 | loss: 0.21624 | val_0_rmse: 0.43618 | val_1_rmse: 0.43686 |  0:00:46s
epoch 40 | loss: 0.21782 | val_0_rmse: 0.46307 | val_1_rmse: 0.47556 |  0:00:47s
epoch 41 | loss: 0.22148 | val_0_rmse: 0.44821 | val_1_rmse: 0.45206 |  0:00:48s
epoch 42 | loss: 0.21632 | val_0_rmse: 0.44488 | val_1_rmse: 0.45212 |  0:00:49s
epoch 43 | loss: 0.20626 | val_0_rmse: 0.44726 | val_1_rmse: 0.45657 |  0:00:50s
epoch 44 | loss: 0.20235 | val_0_rmse: 0.44746 | val_1_rmse: 0.45013 |  0:00:51s
epoch 45 | loss: 0.20827 | val_0_rmse: 0.45078 | val_1_rmse: 0.45824 |  0:00:52s
epoch 46 | loss: 0.20408 | val_0_rmse: 0.42734 | val_1_rmse: 0.43699 |  0:00:54s
epoch 47 | loss: 0.19982 | val_0_rmse: 0.43277 | val_1_rmse: 0.44363 |  0:00:55s
epoch 48 | loss: 0.19415 | val_0_rmse: 0.42387 | val_1_rmse: 0.43766 |  0:00:56s
epoch 49 | loss: 0.19233 | val_0_rmse: 0.42615 | val_1_rmse: 0.43452 |  0:00:57s
epoch 50 | loss: 0.19087 | val_0_rmse: 0.42802 | val_1_rmse: 0.43651 |  0:00:58s
epoch 51 | loss: 0.19009 | val_0_rmse: 0.42708 | val_1_rmse: 0.43189 |  0:00:59s
epoch 52 | loss: 0.19299 | val_0_rmse: 0.41666 | val_1_rmse: 0.42126 |  0:01:00s
epoch 53 | loss: 0.19257 | val_0_rmse: 0.43915 | val_1_rmse: 0.44362 |  0:01:02s
epoch 54 | loss: 0.18905 | val_0_rmse: 0.42054 | val_1_rmse: 0.4256  |  0:01:03s
epoch 55 | loss: 0.18615 | val_0_rmse: 0.41931 | val_1_rmse: 0.42359 |  0:01:04s
epoch 56 | loss: 0.18487 | val_0_rmse: 0.41855 | val_1_rmse: 0.42633 |  0:01:05s
epoch 57 | loss: 0.19129 | val_0_rmse: 0.42557 | val_1_rmse: 0.42625 |  0:01:06s
epoch 58 | loss: 0.18838 | val_0_rmse: 0.42545 | val_1_rmse: 0.42543 |  0:01:07s
epoch 59 | loss: 0.19793 | val_0_rmse: 0.42187 | val_1_rmse: 0.42232 |  0:01:09s
epoch 60 | loss: 0.18932 | val_0_rmse: 0.42259 | val_1_rmse: 0.42849 |  0:01:10s
epoch 61 | loss: 0.18402 | val_0_rmse: 0.42299 | val_1_rmse: 0.42336 |  0:01:11s
epoch 62 | loss: 0.18731 | val_0_rmse: 0.41092 | val_1_rmse: 0.41758 |  0:01:12s
epoch 63 | loss: 0.18304 | val_0_rmse: 0.40941 | val_1_rmse: 0.41706 |  0:01:13s
epoch 64 | loss: 0.18442 | val_0_rmse: 0.41355 | val_1_rmse: 0.41595 |  0:01:14s
epoch 65 | loss: 0.18332 | val_0_rmse: 0.41616 | val_1_rmse: 0.422   |  0:01:15s
epoch 66 | loss: 0.18489 | val_0_rmse: 0.41293 | val_1_rmse: 0.42155 |  0:01:17s
epoch 67 | loss: 0.18522 | val_0_rmse: 0.40666 | val_1_rmse: 0.41651 |  0:01:18s
epoch 68 | loss: 0.17797 | val_0_rmse: 0.41041 | val_1_rmse: 0.4193  |  0:01:19s
epoch 69 | loss: 0.18274 | val_0_rmse: 0.41112 | val_1_rmse: 0.41719 |  0:01:20s
epoch 70 | loss: 0.17979 | val_0_rmse: 0.41118 | val_1_rmse: 0.4145  |  0:01:21s
epoch 71 | loss: 0.18395 | val_0_rmse: 0.41126 | val_1_rmse: 0.41444 |  0:01:22s
epoch 72 | loss: 0.18938 | val_0_rmse: 0.41559 | val_1_rmse: 0.42683 |  0:01:24s
epoch 73 | loss: 0.1827  | val_0_rmse: 0.41212 | val_1_rmse: 0.42388 |  0:01:25s
epoch 74 | loss: 0.18026 | val_0_rmse: 0.41716 | val_1_rmse: 0.42706 |  0:01:26s
epoch 75 | loss: 0.18391 | val_0_rmse: 0.40843 | val_1_rmse: 0.41451 |  0:01:27s
epoch 76 | loss: 0.18537 | val_0_rmse: 0.40579 | val_1_rmse: 0.41485 |  0:01:28s
epoch 77 | loss: 0.17824 | val_0_rmse: 0.40532 | val_1_rmse: 0.41578 |  0:01:29s
epoch 78 | loss: 0.17805 | val_0_rmse: 0.41509 | val_1_rmse: 0.42009 |  0:01:30s
epoch 79 | loss: 0.1821  | val_0_rmse: 0.4117  | val_1_rmse: 0.41636 |  0:01:32s
epoch 80 | loss: 0.18547 | val_0_rmse: 0.40517 | val_1_rmse: 0.4157  |  0:01:33s
epoch 81 | loss: 0.18094 | val_0_rmse: 0.40375 | val_1_rmse: 0.41482 |  0:01:34s
epoch 82 | loss: 0.17959 | val_0_rmse: 0.40725 | val_1_rmse: 0.41651 |  0:01:35s
epoch 83 | loss: 0.17183 | val_0_rmse: 0.39924 | val_1_rmse: 0.41085 |  0:01:36s
epoch 84 | loss: 0.1723  | val_0_rmse: 0.39936 | val_1_rmse: 0.40844 |  0:01:37s
epoch 85 | loss: 0.17317 | val_0_rmse: 0.4074  | val_1_rmse: 0.41547 |  0:01:38s
epoch 86 | loss: 0.18626 | val_0_rmse: 0.40974 | val_1_rmse: 0.42074 |  0:01:40s
epoch 87 | loss: 0.17746 | val_0_rmse: 0.41432 | val_1_rmse: 0.42195 |  0:01:41s
epoch 88 | loss: 0.18645 | val_0_rmse: 0.4597  | val_1_rmse: 0.46896 |  0:01:42s
epoch 89 | loss: 0.18484 | val_0_rmse: 0.39614 | val_1_rmse: 0.4047  |  0:01:43s
epoch 90 | loss: 0.17084 | val_0_rmse: 0.39892 | val_1_rmse: 0.40683 |  0:01:44s
epoch 91 | loss: 0.16904 | val_0_rmse: 0.39601 | val_1_rmse: 0.40259 |  0:01:45s
epoch 92 | loss: 0.16964 | val_0_rmse: 0.39897 | val_1_rmse: 0.40934 |  0:01:46s
epoch 93 | loss: 0.17709 | val_0_rmse: 0.40756 | val_1_rmse: 0.41731 |  0:01:48s
epoch 94 | loss: 0.17555 | val_0_rmse: 0.40267 | val_1_rmse: 0.40736 |  0:01:49s
epoch 95 | loss: 0.18512 | val_0_rmse: 0.40954 | val_1_rmse: 0.41849 |  0:01:50s
epoch 96 | loss: 0.17723 | val_0_rmse: 0.39689 | val_1_rmse: 0.40067 |  0:01:51s
epoch 97 | loss: 0.17013 | val_0_rmse: 0.41144 | val_1_rmse: 0.42022 |  0:01:52s
epoch 98 | loss: 0.16813 | val_0_rmse: 0.39048 | val_1_rmse: 0.39543 |  0:01:53s
epoch 99 | loss: 0.16568 | val_0_rmse: 0.38669 | val_1_rmse: 0.3978  |  0:01:55s
epoch 100| loss: 0.17455 | val_0_rmse: 0.40881 | val_1_rmse: 0.42173 |  0:01:56s
epoch 101| loss: 0.17337 | val_0_rmse: 0.39515 | val_1_rmse: 0.40469 |  0:01:57s
epoch 102| loss: 0.16772 | val_0_rmse: 0.3992  | val_1_rmse: 0.40419 |  0:01:58s
epoch 103| loss: 0.16771 | val_0_rmse: 0.38509 | val_1_rmse: 0.39684 |  0:01:59s
epoch 104| loss: 0.16266 | val_0_rmse: 0.38639 | val_1_rmse: 0.39411 |  0:02:00s
epoch 105| loss: 0.16902 | val_0_rmse: 0.41061 | val_1_rmse: 0.41529 |  0:02:01s
epoch 106| loss: 0.1713  | val_0_rmse: 0.39959 | val_1_rmse: 0.40225 |  0:02:03s
epoch 107| loss: 0.17025 | val_0_rmse: 0.40246 | val_1_rmse: 0.40388 |  0:02:04s
epoch 108| loss: 0.16488 | val_0_rmse: 0.40697 | val_1_rmse: 0.40996 |  0:02:05s
epoch 109| loss: 0.16132 | val_0_rmse: 0.39801 | val_1_rmse: 0.41417 |  0:02:06s
epoch 110| loss: 0.16494 | val_0_rmse: 0.3828  | val_1_rmse: 0.39432 |  0:02:07s
epoch 111| loss: 0.15915 | val_0_rmse: 0.38564 | val_1_rmse: 0.39941 |  0:02:08s
epoch 112| loss: 0.1667  | val_0_rmse: 0.40281 | val_1_rmse: 0.41709 |  0:02:09s
epoch 113| loss: 0.17716 | val_0_rmse: 0.4054  | val_1_rmse: 0.41808 |  0:02:11s
epoch 114| loss: 0.17173 | val_0_rmse: 0.43787 | val_1_rmse: 0.44814 |  0:02:12s
epoch 115| loss: 0.17059 | val_0_rmse: 0.41082 | val_1_rmse: 0.41146 |  0:02:13s
epoch 116| loss: 0.16814 | val_0_rmse: 0.39175 | val_1_rmse: 0.40054 |  0:02:14s
epoch 117| loss: 0.17147 | val_0_rmse: 0.39507 | val_1_rmse: 0.40629 |  0:02:15s
epoch 118| loss: 0.16465 | val_0_rmse: 0.39698 | val_1_rmse: 0.39959 |  0:02:16s
epoch 119| loss: 0.15989 | val_0_rmse: 0.38795 | val_1_rmse: 0.39452 |  0:02:17s
epoch 120| loss: 0.15928 | val_0_rmse: 0.39818 | val_1_rmse: 0.39966 |  0:02:19s
epoch 121| loss: 0.16827 | val_0_rmse: 0.39083 | val_1_rmse: 0.40209 |  0:02:20s
epoch 122| loss: 0.16353 | val_0_rmse: 0.37922 | val_1_rmse: 0.38417 |  0:02:21s
epoch 123| loss: 0.16325 | val_0_rmse: 0.39294 | val_1_rmse: 0.40441 |  0:02:22s
epoch 124| loss: 0.15997 | val_0_rmse: 0.38106 | val_1_rmse: 0.39443 |  0:02:23s
epoch 125| loss: 0.16307 | val_0_rmse: 0.39367 | val_1_rmse: 0.40652 |  0:02:24s
epoch 126| loss: 0.16338 | val_0_rmse: 0.38353 | val_1_rmse: 0.39742 |  0:02:26s
epoch 127| loss: 0.15982 | val_0_rmse: 0.40307 | val_1_rmse: 0.41684 |  0:02:27s
epoch 128| loss: 0.16491 | val_0_rmse: 0.39194 | val_1_rmse: 0.40453 |  0:02:28s
epoch 129| loss: 0.15731 | val_0_rmse: 0.40964 | val_1_rmse: 0.41372 |  0:02:29s
epoch 130| loss: 0.15722 | val_0_rmse: 0.37974 | val_1_rmse: 0.39483 |  0:02:30s
epoch 131| loss: 0.15838 | val_0_rmse: 0.37851 | val_1_rmse: 0.38676 |  0:02:31s
epoch 132| loss: 0.1766  | val_0_rmse: 0.43696 | val_1_rmse: 0.44201 |  0:02:32s
epoch 133| loss: 0.19518 | val_0_rmse: 0.46662 | val_1_rmse: 0.46268 |  0:02:34s
epoch 134| loss: 0.19108 | val_0_rmse: 0.42127 | val_1_rmse: 0.42183 |  0:02:35s
epoch 135| loss: 0.17834 | val_0_rmse: 0.46016 | val_1_rmse: 0.45715 |  0:02:36s
epoch 136| loss: 0.17415 | val_0_rmse: 0.39341 | val_1_rmse: 0.40074 |  0:02:37s
epoch 137| loss: 0.17251 | val_0_rmse: 0.44186 | val_1_rmse: 0.44251 |  0:02:38s
epoch 138| loss: 0.17633 | val_0_rmse: 0.40681 | val_1_rmse: 0.40377 |  0:02:39s
epoch 139| loss: 0.17577 | val_0_rmse: 0.4142  | val_1_rmse: 0.41496 |  0:02:41s
epoch 140| loss: 0.17184 | val_0_rmse: 0.39345 | val_1_rmse: 0.39337 |  0:02:42s
epoch 141| loss: 0.16242 | val_0_rmse: 0.38865 | val_1_rmse: 0.39063 |  0:02:43s
epoch 142| loss: 0.16172 | val_0_rmse: 0.37968 | val_1_rmse: 0.38296 |  0:02:44s
epoch 143| loss: 0.15989 | val_0_rmse: 0.3975  | val_1_rmse: 0.40304 |  0:02:45s
epoch 144| loss: 0.15862 | val_0_rmse: 0.38167 | val_1_rmse: 0.3853  |  0:02:46s
epoch 145| loss: 0.15621 | val_0_rmse: 0.40047 | val_1_rmse: 0.40643 |  0:02:48s
epoch 146| loss: 0.15545 | val_0_rmse: 0.38551 | val_1_rmse: 0.39114 |  0:02:49s
epoch 147| loss: 0.15621 | val_0_rmse: 0.37146 | val_1_rmse: 0.37861 |  0:02:50s
epoch 148| loss: 0.15519 | val_0_rmse: 0.37652 | val_1_rmse: 0.3847  |  0:02:51s
epoch 149| loss: 0.15817 | val_0_rmse: 0.37604 | val_1_rmse: 0.38794 |  0:02:52s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.37861
Best weights from best epoch are automatically used!
ended training at: 05:02:55
Feature importance:
[('Area', 0.2491555861100936), ('Baths', 0.004751221232131514), ('Beds', 0.0006669442622353421), ('Latitude', 0.3742796323481808), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.07431255491835512), ('sqft_lot', 0.0), ('floors', 0.0), ('waterfront', 0.13401002986183205), ('view', 0.0), ('condition', 0.0), ('grade', 2.202173166791533e-06), ('sqft_above', 0.059277907143593676), ('sqft_basement', 0.00793451975728062), ('yr_renovated', 0.0), ('zipcode', 0.09560940219313044)]
Mean squared error is of 4565004625.910355
Mean absolute error:47716.68694821024
MAPE:0.12886996300099232
R2 score:0.8483919692640667
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:02:55
epoch 0  | loss: 1.27767 | val_0_rmse: 0.81357 | val_1_rmse: 0.81992 |  0:00:01s
epoch 1  | loss: 0.43317 | val_0_rmse: 0.59991 | val_1_rmse: 0.59658 |  0:00:02s
epoch 2  | loss: 0.34681 | val_0_rmse: 0.66199 | val_1_rmse: 0.66493 |  0:00:03s
epoch 3  | loss: 0.29515 | val_0_rmse: 0.56756 | val_1_rmse: 0.56751 |  0:00:04s
epoch 4  | loss: 0.29087 | val_0_rmse: 0.50935 | val_1_rmse: 0.5107  |  0:00:05s
epoch 5  | loss: 0.26783 | val_0_rmse: 0.49568 | val_1_rmse: 0.49498 |  0:00:06s
epoch 6  | loss: 0.25848 | val_0_rmse: 0.47987 | val_1_rmse: 0.47477 |  0:00:08s
epoch 7  | loss: 0.24309 | val_0_rmse: 0.4781  | val_1_rmse: 0.47022 |  0:00:09s
epoch 8  | loss: 0.23085 | val_0_rmse: 0.45625 | val_1_rmse: 0.45322 |  0:00:10s
epoch 9  | loss: 0.23169 | val_0_rmse: 0.45989 | val_1_rmse: 0.4515  |  0:00:11s
epoch 10 | loss: 0.21307 | val_0_rmse: 0.44428 | val_1_rmse: 0.4395  |  0:00:12s
epoch 11 | loss: 0.20625 | val_0_rmse: 0.455   | val_1_rmse: 0.44771 |  0:00:13s
epoch 12 | loss: 0.20332 | val_0_rmse: 0.44128 | val_1_rmse: 0.43826 |  0:00:14s
epoch 13 | loss: 0.20209 | val_0_rmse: 0.43011 | val_1_rmse: 0.4252  |  0:00:16s
epoch 14 | loss: 0.20407 | val_0_rmse: 0.47236 | val_1_rmse: 0.46763 |  0:00:17s
epoch 15 | loss: 0.20343 | val_0_rmse: 0.45349 | val_1_rmse: 0.44899 |  0:00:18s
epoch 16 | loss: 0.20522 | val_0_rmse: 0.43692 | val_1_rmse: 0.43169 |  0:00:19s
epoch 17 | loss: 0.19225 | val_0_rmse: 0.42038 | val_1_rmse: 0.41683 |  0:00:20s
epoch 18 | loss: 0.18731 | val_0_rmse: 0.41144 | val_1_rmse: 0.41005 |  0:00:21s
epoch 19 | loss: 0.19021 | val_0_rmse: 0.41163 | val_1_rmse: 0.4094  |  0:00:22s
epoch 20 | loss: 0.18514 | val_0_rmse: 0.41435 | val_1_rmse: 0.41469 |  0:00:24s
epoch 21 | loss: 0.19354 | val_0_rmse: 0.43869 | val_1_rmse: 0.4408  |  0:00:25s
epoch 22 | loss: 0.18469 | val_0_rmse: 0.40586 | val_1_rmse: 0.40381 |  0:00:26s
epoch 23 | loss: 0.17726 | val_0_rmse: 0.4093  | val_1_rmse: 0.40608 |  0:00:27s
epoch 24 | loss: 0.17582 | val_0_rmse: 0.39497 | val_1_rmse: 0.39305 |  0:00:28s
epoch 25 | loss: 0.17715 | val_0_rmse: 0.39834 | val_1_rmse: 0.39656 |  0:00:29s
epoch 26 | loss: 0.17812 | val_0_rmse: 0.40629 | val_1_rmse: 0.40239 |  0:00:31s
epoch 27 | loss: 0.1766  | val_0_rmse: 0.40283 | val_1_rmse: 0.39692 |  0:00:32s
epoch 28 | loss: 0.17509 | val_0_rmse: 0.4088  | val_1_rmse: 0.40757 |  0:00:33s
epoch 29 | loss: 0.17221 | val_0_rmse: 0.39734 | val_1_rmse: 0.3942  |  0:00:34s
epoch 30 | loss: 0.16997 | val_0_rmse: 0.39578 | val_1_rmse: 0.39443 |  0:00:35s
epoch 31 | loss: 0.17168 | val_0_rmse: 0.40173 | val_1_rmse: 0.39943 |  0:00:36s
epoch 32 | loss: 0.17527 | val_0_rmse: 0.39366 | val_1_rmse: 0.39451 |  0:00:37s
epoch 33 | loss: 0.16927 | val_0_rmse: 0.3915  | val_1_rmse: 0.39108 |  0:00:39s
epoch 34 | loss: 0.16773 | val_0_rmse: 0.40733 | val_1_rmse: 0.41087 |  0:00:40s
epoch 35 | loss: 0.16968 | val_0_rmse: 0.40317 | val_1_rmse: 0.39896 |  0:00:41s
epoch 36 | loss: 0.16716 | val_0_rmse: 0.38589 | val_1_rmse: 0.38279 |  0:00:42s
epoch 37 | loss: 0.16592 | val_0_rmse: 0.38481 | val_1_rmse: 0.38789 |  0:00:43s
epoch 38 | loss: 0.16634 | val_0_rmse: 0.38008 | val_1_rmse: 0.38028 |  0:00:44s
epoch 39 | loss: 0.1629  | val_0_rmse: 0.37914 | val_1_rmse: 0.38264 |  0:00:45s
epoch 40 | loss: 0.1634  | val_0_rmse: 0.39432 | val_1_rmse: 0.39788 |  0:00:47s
epoch 41 | loss: 0.16101 | val_0_rmse: 0.3801  | val_1_rmse: 0.37917 |  0:00:48s
epoch 42 | loss: 0.15841 | val_0_rmse: 0.3826  | val_1_rmse: 0.38351 |  0:00:49s
epoch 43 | loss: 0.16614 | val_0_rmse: 0.37964 | val_1_rmse: 0.37964 |  0:00:50s
epoch 44 | loss: 0.1599  | val_0_rmse: 0.39284 | val_1_rmse: 0.3932  |  0:00:51s
epoch 45 | loss: 0.15446 | val_0_rmse: 0.37957 | val_1_rmse: 0.38324 |  0:00:52s
epoch 46 | loss: 0.1638  | val_0_rmse: 0.38909 | val_1_rmse: 0.39289 |  0:00:53s
epoch 47 | loss: 0.1622  | val_0_rmse: 0.37233 | val_1_rmse: 0.3796  |  0:00:55s
epoch 48 | loss: 0.16278 | val_0_rmse: 0.37326 | val_1_rmse: 0.38248 |  0:00:56s
epoch 49 | loss: 0.1589  | val_0_rmse: 0.38216 | val_1_rmse: 0.39015 |  0:00:57s
epoch 50 | loss: 0.16334 | val_0_rmse: 0.38121 | val_1_rmse: 0.3876  |  0:00:58s
epoch 51 | loss: 0.15799 | val_0_rmse: 0.39546 | val_1_rmse: 0.39811 |  0:00:59s
epoch 52 | loss: 0.15876 | val_0_rmse: 0.40516 | val_1_rmse: 0.4134  |  0:01:00s
epoch 53 | loss: 0.16204 | val_0_rmse: 0.42119 | val_1_rmse: 0.43298 |  0:01:02s
epoch 54 | loss: 0.16305 | val_0_rmse: 0.3819  | val_1_rmse: 0.39383 |  0:01:03s
epoch 55 | loss: 0.15814 | val_0_rmse: 0.37833 | val_1_rmse: 0.38643 |  0:01:04s
epoch 56 | loss: 0.15609 | val_0_rmse: 0.3823  | val_1_rmse: 0.39279 |  0:01:05s
epoch 57 | loss: 0.15742 | val_0_rmse: 0.36889 | val_1_rmse: 0.37889 |  0:01:06s
epoch 58 | loss: 0.15366 | val_0_rmse: 0.38136 | val_1_rmse: 0.39288 |  0:01:07s
epoch 59 | loss: 0.15095 | val_0_rmse: 0.36791 | val_1_rmse: 0.37954 |  0:01:08s
epoch 60 | loss: 0.15325 | val_0_rmse: 0.36912 | val_1_rmse: 0.38391 |  0:01:10s
epoch 61 | loss: 0.16219 | val_0_rmse: 0.38835 | val_1_rmse: 0.40409 |  0:01:11s
epoch 62 | loss: 0.1625  | val_0_rmse: 0.39916 | val_1_rmse: 0.41091 |  0:01:12s
epoch 63 | loss: 0.15082 | val_0_rmse: 0.37508 | val_1_rmse: 0.38728 |  0:01:13s
epoch 64 | loss: 0.15193 | val_0_rmse: 0.37426 | val_1_rmse: 0.39075 |  0:01:14s
epoch 65 | loss: 0.14984 | val_0_rmse: 0.37417 | val_1_rmse: 0.38968 |  0:01:15s
epoch 66 | loss: 0.15607 | val_0_rmse: 0.38253 | val_1_rmse: 0.39569 |  0:01:16s
epoch 67 | loss: 0.15214 | val_0_rmse: 0.36572 | val_1_rmse: 0.38077 |  0:01:18s
epoch 68 | loss: 0.15326 | val_0_rmse: 0.38724 | val_1_rmse: 0.4034  |  0:01:19s
epoch 69 | loss: 0.14691 | val_0_rmse: 0.37336 | val_1_rmse: 0.39083 |  0:01:20s
epoch 70 | loss: 0.14942 | val_0_rmse: 0.38485 | val_1_rmse: 0.40088 |  0:01:21s
epoch 71 | loss: 0.15452 | val_0_rmse: 0.35894 | val_1_rmse: 0.37845 |  0:01:22s
epoch 72 | loss: 0.14476 | val_0_rmse: 0.36954 | val_1_rmse: 0.3883  |  0:01:23s
epoch 73 | loss: 0.14497 | val_0_rmse: 0.37446 | val_1_rmse: 0.39256 |  0:01:24s
epoch 74 | loss: 0.15085 | val_0_rmse: 0.39023 | val_1_rmse: 0.40483 |  0:01:26s
epoch 75 | loss: 0.1449  | val_0_rmse: 0.37134 | val_1_rmse: 0.38685 |  0:01:27s
epoch 76 | loss: 0.1456  | val_0_rmse: 0.36438 | val_1_rmse: 0.38308 |  0:01:28s
epoch 77 | loss: 0.14742 | val_0_rmse: 0.38821 | val_1_rmse: 0.40378 |  0:01:29s
epoch 78 | loss: 0.14561 | val_0_rmse: 0.37191 | val_1_rmse: 0.38995 |  0:01:30s
epoch 79 | loss: 0.14342 | val_0_rmse: 0.35971 | val_1_rmse: 0.37628 |  0:01:31s
epoch 80 | loss: 0.14429 | val_0_rmse: 0.36143 | val_1_rmse: 0.38327 |  0:01:33s
epoch 81 | loss: 0.14937 | val_0_rmse: 0.35677 | val_1_rmse: 0.37583 |  0:01:34s
epoch 82 | loss: 0.14589 | val_0_rmse: 0.36228 | val_1_rmse: 0.37927 |  0:01:35s
epoch 83 | loss: 0.14584 | val_0_rmse: 0.38352 | val_1_rmse: 0.39744 |  0:01:36s
epoch 84 | loss: 0.15258 | val_0_rmse: 0.37206 | val_1_rmse: 0.38628 |  0:01:37s
epoch 85 | loss: 0.14862 | val_0_rmse: 0.36762 | val_1_rmse: 0.38314 |  0:01:38s
epoch 86 | loss: 0.14505 | val_0_rmse: 0.36564 | val_1_rmse: 0.38288 |  0:01:39s
epoch 87 | loss: 0.14635 | val_0_rmse: 0.35518 | val_1_rmse: 0.3761  |  0:01:41s
epoch 88 | loss: 0.13973 | val_0_rmse: 0.35679 | val_1_rmse: 0.37513 |  0:01:42s
epoch 89 | loss: 0.14455 | val_0_rmse: 0.3653  | val_1_rmse: 0.38607 |  0:01:43s
epoch 90 | loss: 0.14361 | val_0_rmse: 0.35179 | val_1_rmse: 0.36904 |  0:01:44s
epoch 91 | loss: 0.14283 | val_0_rmse: 0.37544 | val_1_rmse: 0.39626 |  0:01:45s
epoch 92 | loss: 0.14939 | val_0_rmse: 0.36794 | val_1_rmse: 0.39213 |  0:01:46s
epoch 93 | loss: 0.1449  | val_0_rmse: 0.37258 | val_1_rmse: 0.38912 |  0:01:47s
epoch 94 | loss: 0.14724 | val_0_rmse: 0.36041 | val_1_rmse: 0.38005 |  0:01:49s
epoch 95 | loss: 0.14244 | val_0_rmse: 0.34933 | val_1_rmse: 0.37086 |  0:01:50s
epoch 96 | loss: 0.14144 | val_0_rmse: 0.36657 | val_1_rmse: 0.38758 |  0:01:51s
epoch 97 | loss: 0.14161 | val_0_rmse: 0.35569 | val_1_rmse: 0.37707 |  0:01:52s
epoch 98 | loss: 0.13801 | val_0_rmse: 0.3622  | val_1_rmse: 0.38171 |  0:01:53s
epoch 99 | loss: 0.13899 | val_0_rmse: 0.35695 | val_1_rmse: 0.37416 |  0:01:54s
epoch 100| loss: 0.14006 | val_0_rmse: 0.35365 | val_1_rmse: 0.37863 |  0:01:55s
epoch 101| loss: 0.14096 | val_0_rmse: 0.35501 | val_1_rmse: 0.37841 |  0:01:56s
epoch 102| loss: 0.1411  | val_0_rmse: 0.35034 | val_1_rmse: 0.37265 |  0:01:58s
epoch 103| loss: 0.13926 | val_0_rmse: 0.34888 | val_1_rmse: 0.37336 |  0:01:59s
epoch 104| loss: 0.13646 | val_0_rmse: 0.34669 | val_1_rmse: 0.36776 |  0:02:00s
epoch 105| loss: 0.14344 | val_0_rmse: 0.37294 | val_1_rmse: 0.39226 |  0:02:01s
epoch 106| loss: 0.15177 | val_0_rmse: 0.38491 | val_1_rmse: 0.39781 |  0:02:02s
epoch 107| loss: 0.15007 | val_0_rmse: 0.35978 | val_1_rmse: 0.37433 |  0:02:03s
epoch 108| loss: 0.14561 | val_0_rmse: 0.35624 | val_1_rmse: 0.37332 |  0:02:04s
epoch 109| loss: 0.14292 | val_0_rmse: 0.36735 | val_1_rmse: 0.38203 |  0:02:06s
epoch 110| loss: 0.14328 | val_0_rmse: 0.36989 | val_1_rmse: 0.38954 |  0:02:07s
epoch 111| loss: 0.14318 | val_0_rmse: 0.35983 | val_1_rmse: 0.37867 |  0:02:08s
epoch 112| loss: 0.13896 | val_0_rmse: 0.35256 | val_1_rmse: 0.37538 |  0:02:09s
epoch 113| loss: 0.13713 | val_0_rmse: 0.37291 | val_1_rmse: 0.39181 |  0:02:10s
epoch 114| loss: 0.13865 | val_0_rmse: 0.34658 | val_1_rmse: 0.36841 |  0:02:11s
epoch 115| loss: 0.13412 | val_0_rmse: 0.35346 | val_1_rmse: 0.37283 |  0:02:12s
epoch 116| loss: 0.13653 | val_0_rmse: 0.3447  | val_1_rmse: 0.3664  |  0:02:14s
epoch 117| loss: 0.13486 | val_0_rmse: 0.34738 | val_1_rmse: 0.36913 |  0:02:15s
epoch 118| loss: 0.13718 | val_0_rmse: 0.35612 | val_1_rmse: 0.37832 |  0:02:16s
epoch 119| loss: 0.13471 | val_0_rmse: 0.35268 | val_1_rmse: 0.3763  |  0:02:17s
epoch 120| loss: 0.13205 | val_0_rmse: 0.34357 | val_1_rmse: 0.36739 |  0:02:18s
epoch 121| loss: 0.13622 | val_0_rmse: 0.37208 | val_1_rmse: 0.39874 |  0:02:19s
epoch 122| loss: 0.13965 | val_0_rmse: 0.36481 | val_1_rmse: 0.38587 |  0:02:21s
epoch 123| loss: 0.13752 | val_0_rmse: 0.3479  | val_1_rmse: 0.36759 |  0:02:22s
epoch 124| loss: 0.13424 | val_0_rmse: 0.34779 | val_1_rmse: 0.37537 |  0:02:23s
epoch 125| loss: 0.13288 | val_0_rmse: 0.33839 | val_1_rmse: 0.36606 |  0:02:24s
epoch 126| loss: 0.13637 | val_0_rmse: 0.34597 | val_1_rmse: 0.37214 |  0:02:25s
epoch 127| loss: 0.13867 | val_0_rmse: 0.35001 | val_1_rmse: 0.37657 |  0:02:26s
epoch 128| loss: 0.13817 | val_0_rmse: 0.34539 | val_1_rmse: 0.37345 |  0:02:27s
epoch 129| loss: 0.13526 | val_0_rmse: 0.34597 | val_1_rmse: 0.37424 |  0:02:29s
epoch 130| loss: 0.13636 | val_0_rmse: 0.34109 | val_1_rmse: 0.36724 |  0:02:30s
epoch 131| loss: 0.13709 | val_0_rmse: 0.36709 | val_1_rmse: 0.3956  |  0:02:31s
epoch 132| loss: 0.13416 | val_0_rmse: 0.37646 | val_1_rmse: 0.40923 |  0:02:32s
epoch 133| loss: 0.13646 | val_0_rmse: 0.33902 | val_1_rmse: 0.36952 |  0:02:33s
epoch 134| loss: 0.13987 | val_0_rmse: 0.35826 | val_1_rmse: 0.38362 |  0:02:34s
epoch 135| loss: 0.14099 | val_0_rmse: 0.35537 | val_1_rmse: 0.3819  |  0:02:36s
epoch 136| loss: 0.13257 | val_0_rmse: 0.33906 | val_1_rmse: 0.37215 |  0:02:37s
epoch 137| loss: 0.13486 | val_0_rmse: 0.35352 | val_1_rmse: 0.38402 |  0:02:38s
epoch 138| loss: 0.13178 | val_0_rmse: 0.34416 | val_1_rmse: 0.37825 |  0:02:39s
epoch 139| loss: 0.13039 | val_0_rmse: 0.33893 | val_1_rmse: 0.3681  |  0:02:40s
epoch 140| loss: 0.13109 | val_0_rmse: 0.34911 | val_1_rmse: 0.38151 |  0:02:41s
epoch 141| loss: 0.1388  | val_0_rmse: 0.34871 | val_1_rmse: 0.3783  |  0:02:42s
epoch 142| loss: 0.13269 | val_0_rmse: 0.34233 | val_1_rmse: 0.38094 |  0:02:44s
epoch 143| loss: 0.13533 | val_0_rmse: 0.34674 | val_1_rmse: 0.38153 |  0:02:45s
epoch 144| loss: 0.13503 | val_0_rmse: 0.34094 | val_1_rmse: 0.37708 |  0:02:46s
epoch 145| loss: 0.13631 | val_0_rmse: 0.33498 | val_1_rmse: 0.37417 |  0:02:47s
epoch 146| loss: 0.12996 | val_0_rmse: 0.33467 | val_1_rmse: 0.36703 |  0:02:48s
epoch 147| loss: 0.12895 | val_0_rmse: 0.35464 | val_1_rmse: 0.3944  |  0:02:49s
epoch 148| loss: 0.1337  | val_0_rmse: 0.34784 | val_1_rmse: 0.39185 |  0:02:50s
epoch 149| loss: 0.12897 | val_0_rmse: 0.34408 | val_1_rmse: 0.39061 |  0:02:51s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.36606
Best weights from best epoch are automatically used!
ended training at: 05:05:47
Feature importance:
[('Area', 0.26846586368221753), ('Baths', 0.001239776293301501), ('Beds', 0.000171965380370904), ('Latitude', 0.28857595955967497), ('Longitude', 0.0), ('Month', 0.0025108471358059833), ('Year', 3.7773485254378152e-06), ('sqft_lot', 0.11869356030627229), ('floors', 1.5654849231521152e-07), ('waterfront', 0.0), ('view', 0.0330727711688299), ('condition', 0.0), ('grade', 0.17746374228239237), ('sqft_above', 0.0), ('sqft_basement', 0.0), ('yr_renovated', 0.07242578479250185), ('zipcode', 0.03737579550161496)]
Mean squared error is of 4415918672.72323
Mean absolute error:46770.78876437468
MAPE:0.12404942988709082
R2 score:0.8522656714592923
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:05:47
epoch 0  | loss: 1.22012 | val_0_rmse: 0.80189 | val_1_rmse: 0.79757 |  0:00:01s
epoch 1  | loss: 0.41997 | val_0_rmse: 0.68145 | val_1_rmse: 0.68158 |  0:00:02s
epoch 2  | loss: 0.32864 | val_0_rmse: 0.56329 | val_1_rmse: 0.57231 |  0:00:03s
epoch 3  | loss: 0.29883 | val_0_rmse: 0.5236  | val_1_rmse: 0.52511 |  0:00:04s
epoch 4  | loss: 0.28803 | val_0_rmse: 0.51976 | val_1_rmse: 0.52713 |  0:00:05s
epoch 5  | loss: 0.28089 | val_0_rmse: 0.51002 | val_1_rmse: 0.51401 |  0:00:06s
epoch 6  | loss: 0.27356 | val_0_rmse: 0.50236 | val_1_rmse: 0.5111  |  0:00:08s
epoch 7  | loss: 0.26081 | val_0_rmse: 0.50591 | val_1_rmse: 0.51058 |  0:00:09s
epoch 8  | loss: 0.25492 | val_0_rmse: 0.48537 | val_1_rmse: 0.4968  |  0:00:10s
epoch 9  | loss: 0.24023 | val_0_rmse: 0.47051 | val_1_rmse: 0.4745  |  0:00:11s
epoch 10 | loss: 0.23695 | val_0_rmse: 0.46793 | val_1_rmse: 0.46955 |  0:00:12s
epoch 11 | loss: 0.2349  | val_0_rmse: 0.46412 | val_1_rmse: 0.47593 |  0:00:13s
epoch 12 | loss: 0.22929 | val_0_rmse: 0.45883 | val_1_rmse: 0.46324 |  0:00:14s
epoch 13 | loss: 0.21798 | val_0_rmse: 0.45417 | val_1_rmse: 0.4588  |  0:00:16s
epoch 14 | loss: 0.2321  | val_0_rmse: 0.45969 | val_1_rmse: 0.46984 |  0:00:17s
epoch 15 | loss: 0.23095 | val_0_rmse: 0.47436 | val_1_rmse: 0.48338 |  0:00:18s
epoch 16 | loss: 0.22399 | val_0_rmse: 0.46239 | val_1_rmse: 0.47045 |  0:00:19s
epoch 17 | loss: 0.21108 | val_0_rmse: 0.45271 | val_1_rmse: 0.45809 |  0:00:20s
epoch 18 | loss: 0.21004 | val_0_rmse: 0.43282 | val_1_rmse: 0.44206 |  0:00:21s
epoch 19 | loss: 0.20424 | val_0_rmse: 0.43096 | val_1_rmse: 0.43736 |  0:00:22s
epoch 20 | loss: 0.20153 | val_0_rmse: 0.42945 | val_1_rmse: 0.43178 |  0:00:24s
epoch 21 | loss: 0.19638 | val_0_rmse: 0.42764 | val_1_rmse: 0.4375  |  0:00:25s
epoch 22 | loss: 0.19274 | val_0_rmse: 0.42416 | val_1_rmse: 0.42939 |  0:00:26s
epoch 23 | loss: 0.19121 | val_0_rmse: 0.4226  | val_1_rmse: 0.42586 |  0:00:27s
epoch 24 | loss: 0.19025 | val_0_rmse: 0.43246 | val_1_rmse: 0.43942 |  0:00:28s
epoch 25 | loss: 0.19846 | val_0_rmse: 0.4317  | val_1_rmse: 0.44309 |  0:00:29s
epoch 26 | loss: 0.18574 | val_0_rmse: 0.42251 | val_1_rmse: 0.43255 |  0:00:30s
epoch 27 | loss: 0.18909 | val_0_rmse: 0.43514 | val_1_rmse: 0.4417  |  0:00:31s
epoch 28 | loss: 0.18784 | val_0_rmse: 0.46419 | val_1_rmse: 0.4724  |  0:00:33s
epoch 29 | loss: 0.18982 | val_0_rmse: 0.42342 | val_1_rmse: 0.43195 |  0:00:34s
epoch 30 | loss: 0.18105 | val_0_rmse: 0.40696 | val_1_rmse: 0.41606 |  0:00:35s
epoch 31 | loss: 0.1785  | val_0_rmse: 0.40798 | val_1_rmse: 0.4155  |  0:00:36s
epoch 32 | loss: 0.17577 | val_0_rmse: 0.40062 | val_1_rmse: 0.40879 |  0:00:37s
epoch 33 | loss: 0.17556 | val_0_rmse: 0.40539 | val_1_rmse: 0.41658 |  0:00:38s
epoch 34 | loss: 0.18559 | val_0_rmse: 0.43829 | val_1_rmse: 0.44931 |  0:00:39s
epoch 35 | loss: 0.18696 | val_0_rmse: 0.41238 | val_1_rmse: 0.42248 |  0:00:41s
epoch 36 | loss: 0.17777 | val_0_rmse: 0.40672 | val_1_rmse: 0.41667 |  0:00:42s
epoch 37 | loss: 0.18716 | val_0_rmse: 0.40445 | val_1_rmse: 0.41758 |  0:00:43s
epoch 38 | loss: 0.17896 | val_0_rmse: 0.40988 | val_1_rmse: 0.42323 |  0:00:44s
epoch 39 | loss: 0.17914 | val_0_rmse: 0.40624 | val_1_rmse: 0.42123 |  0:00:45s
epoch 40 | loss: 0.17916 | val_0_rmse: 0.41424 | val_1_rmse: 0.42701 |  0:00:46s
epoch 41 | loss: 0.17577 | val_0_rmse: 0.40308 | val_1_rmse: 0.41507 |  0:00:47s
epoch 42 | loss: 0.17144 | val_0_rmse: 0.40211 | val_1_rmse: 0.41578 |  0:00:49s
epoch 43 | loss: 0.1685  | val_0_rmse: 0.39076 | val_1_rmse: 0.41081 |  0:00:50s
epoch 44 | loss: 0.17636 | val_0_rmse: 0.40619 | val_1_rmse: 0.4187  |  0:00:51s
epoch 45 | loss: 0.17545 | val_0_rmse: 0.3956  | val_1_rmse: 0.41054 |  0:00:52s
epoch 46 | loss: 0.17383 | val_0_rmse: 0.39735 | val_1_rmse: 0.40719 |  0:00:53s
epoch 47 | loss: 0.16605 | val_0_rmse: 0.39145 | val_1_rmse: 0.40568 |  0:00:54s
epoch 48 | loss: 0.16789 | val_0_rmse: 0.40442 | val_1_rmse: 0.41431 |  0:00:55s
epoch 49 | loss: 0.17839 | val_0_rmse: 0.41407 | val_1_rmse: 0.42629 |  0:00:57s
epoch 50 | loss: 0.17447 | val_0_rmse: 0.40124 | val_1_rmse: 0.41706 |  0:00:58s
epoch 51 | loss: 0.17519 | val_0_rmse: 0.39755 | val_1_rmse: 0.4133  |  0:00:59s
epoch 52 | loss: 0.17127 | val_0_rmse: 0.3994  | val_1_rmse: 0.41985 |  0:01:00s
epoch 53 | loss: 0.17235 | val_0_rmse: 0.39834 | val_1_rmse: 0.40622 |  0:01:01s
epoch 54 | loss: 0.16754 | val_0_rmse: 0.38704 | val_1_rmse: 0.40386 |  0:01:02s
epoch 55 | loss: 0.16552 | val_0_rmse: 0.39815 | val_1_rmse: 0.41197 |  0:01:03s
epoch 56 | loss: 0.16628 | val_0_rmse: 0.39685 | val_1_rmse: 0.41272 |  0:01:05s
epoch 57 | loss: 0.1684  | val_0_rmse: 0.38333 | val_1_rmse: 0.39627 |  0:01:06s
epoch 58 | loss: 0.15932 | val_0_rmse: 0.39567 | val_1_rmse: 0.41134 |  0:01:07s
epoch 59 | loss: 0.15902 | val_0_rmse: 0.39373 | val_1_rmse: 0.40729 |  0:01:08s
epoch 60 | loss: 0.16377 | val_0_rmse: 0.3879  | val_1_rmse: 0.4045  |  0:01:09s
epoch 61 | loss: 0.1633  | val_0_rmse: 0.40139 | val_1_rmse: 0.41812 |  0:01:10s
epoch 62 | loss: 0.16224 | val_0_rmse: 0.37716 | val_1_rmse: 0.3975  |  0:01:11s
epoch 63 | loss: 0.15579 | val_0_rmse: 0.3784  | val_1_rmse: 0.39526 |  0:01:13s
epoch 64 | loss: 0.15585 | val_0_rmse: 0.38376 | val_1_rmse: 0.40166 |  0:01:14s
epoch 65 | loss: 0.1578  | val_0_rmse: 0.38643 | val_1_rmse: 0.40481 |  0:01:15s
epoch 66 | loss: 0.16165 | val_0_rmse: 0.38091 | val_1_rmse: 0.39908 |  0:01:16s
epoch 67 | loss: 0.15752 | val_0_rmse: 0.38424 | val_1_rmse: 0.4018  |  0:01:17s
epoch 68 | loss: 0.16048 | val_0_rmse: 0.37744 | val_1_rmse: 0.39403 |  0:01:18s
epoch 69 | loss: 0.15758 | val_0_rmse: 0.39465 | val_1_rmse: 0.41433 |  0:01:20s
epoch 70 | loss: 0.16396 | val_0_rmse: 0.38272 | val_1_rmse: 0.39738 |  0:01:21s
epoch 71 | loss: 0.15691 | val_0_rmse: 0.37436 | val_1_rmse: 0.39401 |  0:01:22s
epoch 72 | loss: 0.15682 | val_0_rmse: 0.39928 | val_1_rmse: 0.42298 |  0:01:23s
epoch 73 | loss: 0.15806 | val_0_rmse: 0.40309 | val_1_rmse: 0.41538 |  0:01:24s
epoch 74 | loss: 0.1647  | val_0_rmse: 0.46823 | val_1_rmse: 0.48781 |  0:01:25s
epoch 75 | loss: 0.16455 | val_0_rmse: 0.41845 | val_1_rmse: 0.42726 |  0:01:26s
epoch 76 | loss: 0.16786 | val_0_rmse: 0.38344 | val_1_rmse: 0.39336 |  0:01:27s
epoch 77 | loss: 0.1561  | val_0_rmse: 0.40501 | val_1_rmse: 0.42108 |  0:01:29s
epoch 78 | loss: 0.15919 | val_0_rmse: 0.39201 | val_1_rmse: 0.40347 |  0:01:30s
epoch 79 | loss: 0.15797 | val_0_rmse: 0.39172 | val_1_rmse: 0.41063 |  0:01:31s
epoch 80 | loss: 0.15277 | val_0_rmse: 0.36503 | val_1_rmse: 0.38805 |  0:01:32s
epoch 81 | loss: 0.14777 | val_0_rmse: 0.3714  | val_1_rmse: 0.38982 |  0:01:33s
epoch 82 | loss: 0.14889 | val_0_rmse: 0.36447 | val_1_rmse: 0.38272 |  0:01:34s
epoch 83 | loss: 0.14625 | val_0_rmse: 0.36817 | val_1_rmse: 0.38681 |  0:01:36s
epoch 84 | loss: 0.14874 | val_0_rmse: 0.35983 | val_1_rmse: 0.38696 |  0:01:37s
epoch 85 | loss: 0.14707 | val_0_rmse: 0.36353 | val_1_rmse: 0.38966 |  0:01:38s
epoch 86 | loss: 0.14861 | val_0_rmse: 0.36786 | val_1_rmse: 0.39543 |  0:01:39s
epoch 87 | loss: 0.1462  | val_0_rmse: 0.37925 | val_1_rmse: 0.40122 |  0:01:40s
epoch 88 | loss: 0.14874 | val_0_rmse: 0.36221 | val_1_rmse: 0.38924 |  0:01:41s
epoch 89 | loss: 0.14472 | val_0_rmse: 0.36395 | val_1_rmse: 0.39219 |  0:01:42s
epoch 90 | loss: 0.14591 | val_0_rmse: 0.35813 | val_1_rmse: 0.38546 |  0:01:44s
epoch 91 | loss: 0.14569 | val_0_rmse: 0.36612 | val_1_rmse: 0.3957  |  0:01:45s
epoch 92 | loss: 0.14922 | val_0_rmse: 0.36484 | val_1_rmse: 0.39176 |  0:01:46s
epoch 93 | loss: 0.15383 | val_0_rmse: 0.36676 | val_1_rmse: 0.39139 |  0:01:47s
epoch 94 | loss: 0.14192 | val_0_rmse: 0.37125 | val_1_rmse: 0.40342 |  0:01:48s
epoch 95 | loss: 0.14496 | val_0_rmse: 0.36999 | val_1_rmse: 0.39481 |  0:01:49s
epoch 96 | loss: 0.14571 | val_0_rmse: 0.3613  | val_1_rmse: 0.38964 |  0:01:50s
epoch 97 | loss: 0.14627 | val_0_rmse: 0.36445 | val_1_rmse: 0.39257 |  0:01:51s
epoch 98 | loss: 0.14154 | val_0_rmse: 0.35213 | val_1_rmse: 0.38165 |  0:01:53s
epoch 99 | loss: 0.14079 | val_0_rmse: 0.35505 | val_1_rmse: 0.38693 |  0:01:54s
epoch 100| loss: 0.14132 | val_0_rmse: 0.37691 | val_1_rmse: 0.40649 |  0:01:55s
epoch 101| loss: 0.14808 | val_0_rmse: 0.36643 | val_1_rmse: 0.39118 |  0:01:56s
epoch 102| loss: 0.14059 | val_0_rmse: 0.35407 | val_1_rmse: 0.39078 |  0:01:57s
epoch 103| loss: 0.13765 | val_0_rmse: 0.35296 | val_1_rmse: 0.38571 |  0:01:58s
epoch 104| loss: 0.13726 | val_0_rmse: 0.36085 | val_1_rmse: 0.39028 |  0:01:59s
epoch 105| loss: 0.14589 | val_0_rmse: 0.35397 | val_1_rmse: 0.38696 |  0:02:01s
epoch 106| loss: 0.14776 | val_0_rmse: 0.36165 | val_1_rmse: 0.39768 |  0:02:02s
epoch 107| loss: 0.14414 | val_0_rmse: 0.35443 | val_1_rmse: 0.38462 |  0:02:03s
epoch 108| loss: 0.14174 | val_0_rmse: 0.3774  | val_1_rmse: 0.41028 |  0:02:04s
epoch 109| loss: 0.14083 | val_0_rmse: 0.37077 | val_1_rmse: 0.40368 |  0:02:05s
epoch 110| loss: 0.14119 | val_0_rmse: 0.35417 | val_1_rmse: 0.38814 |  0:02:06s
epoch 111| loss: 0.14607 | val_0_rmse: 0.37451 | val_1_rmse: 0.41048 |  0:02:08s
epoch 112| loss: 0.1426  | val_0_rmse: 0.35323 | val_1_rmse: 0.39165 |  0:02:09s
epoch 113| loss: 0.13812 | val_0_rmse: 0.35571 | val_1_rmse: 0.38958 |  0:02:10s
epoch 114| loss: 0.14346 | val_0_rmse: 0.36127 | val_1_rmse: 0.4026  |  0:02:11s
epoch 115| loss: 0.13812 | val_0_rmse: 0.35664 | val_1_rmse: 0.39279 |  0:02:12s
epoch 116| loss: 0.13661 | val_0_rmse: 0.34766 | val_1_rmse: 0.38664 |  0:02:13s
epoch 117| loss: 0.13836 | val_0_rmse: 0.366   | val_1_rmse: 0.39706 |  0:02:14s
epoch 118| loss: 0.1381  | val_0_rmse: 0.37657 | val_1_rmse: 0.41792 |  0:02:16s
epoch 119| loss: 0.14448 | val_0_rmse: 0.36533 | val_1_rmse: 0.40155 |  0:02:17s
epoch 120| loss: 0.14494 | val_0_rmse: 0.34894 | val_1_rmse: 0.3851  |  0:02:18s
epoch 121| loss: 0.13715 | val_0_rmse: 0.3463  | val_1_rmse: 0.38615 |  0:02:19s
epoch 122| loss: 0.13658 | val_0_rmse: 0.3479  | val_1_rmse: 0.3876  |  0:02:20s
epoch 123| loss: 0.13792 | val_0_rmse: 0.35925 | val_1_rmse: 0.41301 |  0:02:21s
epoch 124| loss: 0.13683 | val_0_rmse: 0.34321 | val_1_rmse: 0.38508 |  0:02:22s
epoch 125| loss: 0.13616 | val_0_rmse: 0.35715 | val_1_rmse: 0.39386 |  0:02:23s
epoch 126| loss: 0.13315 | val_0_rmse: 0.34257 | val_1_rmse: 0.38802 |  0:02:25s
epoch 127| loss: 0.13604 | val_0_rmse: 0.34098 | val_1_rmse: 0.38252 |  0:02:26s
epoch 128| loss: 0.13492 | val_0_rmse: 0.35026 | val_1_rmse: 0.38947 |  0:02:27s

Early stopping occured at epoch 128 with best_epoch = 98 and best_val_1_rmse = 0.38165
Best weights from best epoch are automatically used!
ended training at: 05:08:15
Feature importance:
[('Area', 0.21526791174354543), ('Baths', 3.0436491779401684e-07), ('Beds', 0.00019980916855559788), ('Latitude', 0.36999309156362137), ('Longitude', 0.0), ('Month', 1.2936406774253184e-06), ('Year', 0.0), ('sqft_lot', 0.0), ('floors', 0.0), ('waterfront', 0.0), ('view', 0.0377715001321828), ('condition', 0.06858126413038101), ('grade', 0.18839532461646527), ('sqft_above', 3.8109507557195785e-07), ('sqft_basement', 0.03043949871717859), ('yr_renovated', 0.0), ('zipcode', 0.08934962082739911)]
Mean squared error is of 4394990616.793104
Mean absolute error:46998.798916724336
MAPE:0.12522253068902228
R2 score:0.8582329160460673
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:08:15
epoch 0  | loss: 1.18259 | val_0_rmse: 0.76219 | val_1_rmse: 0.78118 |  0:00:01s
epoch 1  | loss: 0.42913 | val_0_rmse: 0.64077 | val_1_rmse: 0.655   |  0:00:02s
epoch 2  | loss: 0.33934 | val_0_rmse: 0.6258  | val_1_rmse: 0.63983 |  0:00:03s
epoch 3  | loss: 0.30722 | val_0_rmse: 0.5385  | val_1_rmse: 0.54359 |  0:00:04s
epoch 4  | loss: 0.279   | val_0_rmse: 0.50987 | val_1_rmse: 0.51596 |  0:00:05s
epoch 5  | loss: 0.25925 | val_0_rmse: 0.48559 | val_1_rmse: 0.49375 |  0:00:06s
epoch 6  | loss: 0.26537 | val_0_rmse: 0.52003 | val_1_rmse: 0.5312  |  0:00:08s
epoch 7  | loss: 0.25851 | val_0_rmse: 0.48534 | val_1_rmse: 0.49958 |  0:00:09s
epoch 8  | loss: 0.24812 | val_0_rmse: 0.4803  | val_1_rmse: 0.49638 |  0:00:10s
epoch 9  | loss: 0.24124 | val_0_rmse: 0.47495 | val_1_rmse: 0.48452 |  0:00:11s
epoch 10 | loss: 0.23516 | val_0_rmse: 0.48685 | val_1_rmse: 0.50275 |  0:00:12s
epoch 11 | loss: 0.23729 | val_0_rmse: 0.51724 | val_1_rmse: 0.54087 |  0:00:13s
epoch 12 | loss: 0.24204 | val_0_rmse: 0.47033 | val_1_rmse: 0.47784 |  0:00:14s
epoch 13 | loss: 0.22975 | val_0_rmse: 0.46346 | val_1_rmse: 0.47475 |  0:00:16s
epoch 14 | loss: 0.2255  | val_0_rmse: 0.45421 | val_1_rmse: 0.46768 |  0:00:17s
epoch 15 | loss: 0.21835 | val_0_rmse: 0.45515 | val_1_rmse: 0.46245 |  0:00:18s
epoch 16 | loss: 0.2235  | val_0_rmse: 0.46423 | val_1_rmse: 0.48056 |  0:00:19s
epoch 17 | loss: 0.21299 | val_0_rmse: 0.44301 | val_1_rmse: 0.45226 |  0:00:20s
epoch 18 | loss: 0.20563 | val_0_rmse: 0.44755 | val_1_rmse: 0.45321 |  0:00:21s
epoch 19 | loss: 0.21352 | val_0_rmse: 0.43862 | val_1_rmse: 0.44487 |  0:00:22s
epoch 20 | loss: 0.21797 | val_0_rmse: 0.4391  | val_1_rmse: 0.44972 |  0:00:24s
epoch 21 | loss: 0.20147 | val_0_rmse: 0.43194 | val_1_rmse: 0.43932 |  0:00:25s
epoch 22 | loss: 0.20022 | val_0_rmse: 0.42863 | val_1_rmse: 0.44318 |  0:00:26s
epoch 23 | loss: 0.19569 | val_0_rmse: 0.43034 | val_1_rmse: 0.44253 |  0:00:27s
epoch 24 | loss: 0.19749 | val_0_rmse: 0.4356  | val_1_rmse: 0.44276 |  0:00:28s
epoch 25 | loss: 0.19502 | val_0_rmse: 0.42456 | val_1_rmse: 0.43779 |  0:00:29s
epoch 26 | loss: 0.1936  | val_0_rmse: 0.42882 | val_1_rmse: 0.44427 |  0:00:30s
epoch 27 | loss: 0.19476 | val_0_rmse: 0.42187 | val_1_rmse: 0.43484 |  0:00:32s
epoch 28 | loss: 0.19219 | val_0_rmse: 0.4223  | val_1_rmse: 0.43554 |  0:00:33s
epoch 29 | loss: 0.1911  | val_0_rmse: 0.41406 | val_1_rmse: 0.43142 |  0:00:34s
epoch 30 | loss: 0.18546 | val_0_rmse: 0.41008 | val_1_rmse: 0.42861 |  0:00:35s
epoch 31 | loss: 0.17913 | val_0_rmse: 0.40997 | val_1_rmse: 0.43028 |  0:00:36s
epoch 32 | loss: 0.18261 | val_0_rmse: 0.41987 | val_1_rmse: 0.4378  |  0:00:37s
epoch 33 | loss: 0.18472 | val_0_rmse: 0.40477 | val_1_rmse: 0.42368 |  0:00:38s
epoch 34 | loss: 0.17992 | val_0_rmse: 0.41459 | val_1_rmse: 0.43106 |  0:00:40s
epoch 35 | loss: 0.18381 | val_0_rmse: 0.42226 | val_1_rmse: 0.44807 |  0:00:41s
epoch 36 | loss: 0.18543 | val_0_rmse: 0.41373 | val_1_rmse: 0.43365 |  0:00:42s
epoch 37 | loss: 0.1823  | val_0_rmse: 0.41013 | val_1_rmse: 0.43047 |  0:00:43s
epoch 38 | loss: 0.17818 | val_0_rmse: 0.40677 | val_1_rmse: 0.42684 |  0:00:44s
epoch 39 | loss: 0.17721 | val_0_rmse: 0.4052  | val_1_rmse: 0.42821 |  0:00:45s
epoch 40 | loss: 0.17712 | val_0_rmse: 0.40867 | val_1_rmse: 0.42756 |  0:00:46s
epoch 41 | loss: 0.17332 | val_0_rmse: 0.39268 | val_1_rmse: 0.41478 |  0:00:48s
epoch 42 | loss: 0.17482 | val_0_rmse: 0.38927 | val_1_rmse: 0.41635 |  0:00:49s
epoch 43 | loss: 0.16711 | val_0_rmse: 0.38472 | val_1_rmse: 0.4102  |  0:00:50s
epoch 44 | loss: 0.16951 | val_0_rmse: 0.395   | val_1_rmse: 0.42098 |  0:00:51s
epoch 45 | loss: 0.16905 | val_0_rmse: 0.39424 | val_1_rmse: 0.42381 |  0:00:52s
epoch 46 | loss: 0.17978 | val_0_rmse: 0.43592 | val_1_rmse: 0.45844 |  0:00:53s
epoch 47 | loss: 0.18727 | val_0_rmse: 0.42065 | val_1_rmse: 0.44427 |  0:00:54s
epoch 48 | loss: 0.17844 | val_0_rmse: 0.40886 | val_1_rmse: 0.43723 |  0:00:56s
epoch 49 | loss: 0.17398 | val_0_rmse: 0.38989 | val_1_rmse: 0.41409 |  0:00:57s
epoch 50 | loss: 0.16759 | val_0_rmse: 0.39384 | val_1_rmse: 0.41939 |  0:00:58s
epoch 51 | loss: 0.16022 | val_0_rmse: 0.38589 | val_1_rmse: 0.4148  |  0:00:59s
epoch 52 | loss: 0.16073 | val_0_rmse: 0.38936 | val_1_rmse: 0.41397 |  0:01:00s
epoch 53 | loss: 0.16095 | val_0_rmse: 0.38113 | val_1_rmse: 0.41103 |  0:01:01s
epoch 54 | loss: 0.15721 | val_0_rmse: 0.37305 | val_1_rmse: 0.4039  |  0:01:02s
epoch 55 | loss: 0.15414 | val_0_rmse: 0.37507 | val_1_rmse: 0.40528 |  0:01:04s
epoch 56 | loss: 0.16354 | val_0_rmse: 0.42249 | val_1_rmse: 0.44966 |  0:01:05s
epoch 57 | loss: 0.16332 | val_0_rmse: 0.39503 | val_1_rmse: 0.42571 |  0:01:06s
epoch 58 | loss: 0.15464 | val_0_rmse: 0.37478 | val_1_rmse: 0.40834 |  0:01:07s
epoch 59 | loss: 0.15859 | val_0_rmse: 0.37227 | val_1_rmse: 0.40624 |  0:01:08s
epoch 60 | loss: 0.15055 | val_0_rmse: 0.3767  | val_1_rmse: 0.41553 |  0:01:09s
epoch 61 | loss: 0.15223 | val_0_rmse: 0.41179 | val_1_rmse: 0.43603 |  0:01:10s
epoch 62 | loss: 0.16555 | val_0_rmse: 0.38546 | val_1_rmse: 0.41517 |  0:01:11s
epoch 63 | loss: 0.15896 | val_0_rmse: 0.37122 | val_1_rmse: 0.40794 |  0:01:13s
epoch 64 | loss: 0.15663 | val_0_rmse: 0.36602 | val_1_rmse: 0.40719 |  0:01:14s
epoch 65 | loss: 0.15429 | val_0_rmse: 0.37191 | val_1_rmse: 0.40357 |  0:01:15s
epoch 66 | loss: 0.15003 | val_0_rmse: 0.36794 | val_1_rmse: 0.40627 |  0:01:16s
epoch 67 | loss: 0.15095 | val_0_rmse: 0.36544 | val_1_rmse: 0.39938 |  0:01:17s
epoch 68 | loss: 0.15573 | val_0_rmse: 0.39795 | val_1_rmse: 0.42514 |  0:01:18s
epoch 69 | loss: 0.16161 | val_0_rmse: 0.38174 | val_1_rmse: 0.41474 |  0:01:19s
epoch 70 | loss: 0.15914 | val_0_rmse: 0.37994 | val_1_rmse: 0.41228 |  0:01:21s
epoch 71 | loss: 0.15729 | val_0_rmse: 0.37371 | val_1_rmse: 0.40709 |  0:01:22s
epoch 72 | loss: 0.15281 | val_0_rmse: 0.37372 | val_1_rmse: 0.40501 |  0:01:23s
epoch 73 | loss: 0.14991 | val_0_rmse: 0.37893 | val_1_rmse: 0.41091 |  0:01:24s
epoch 74 | loss: 0.15601 | val_0_rmse: 0.37979 | val_1_rmse: 0.41144 |  0:01:25s
epoch 75 | loss: 0.15408 | val_0_rmse: 0.37577 | val_1_rmse: 0.41018 |  0:01:26s
epoch 76 | loss: 0.15041 | val_0_rmse: 0.37657 | val_1_rmse: 0.40994 |  0:01:27s
epoch 77 | loss: 0.15033 | val_0_rmse: 0.36712 | val_1_rmse: 0.40216 |  0:01:29s
epoch 78 | loss: 0.14776 | val_0_rmse: 0.36683 | val_1_rmse: 0.40306 |  0:01:30s
epoch 79 | loss: 0.14552 | val_0_rmse: 0.36359 | val_1_rmse: 0.4016  |  0:01:31s
epoch 80 | loss: 0.14467 | val_0_rmse: 0.37024 | val_1_rmse: 0.4041  |  0:01:32s
epoch 81 | loss: 0.14976 | val_0_rmse: 0.36485 | val_1_rmse: 0.40241 |  0:01:33s
epoch 82 | loss: 0.14469 | val_0_rmse: 0.36631 | val_1_rmse: 0.39943 |  0:01:34s
epoch 83 | loss: 0.14479 | val_0_rmse: 0.36869 | val_1_rmse: 0.40664 |  0:01:35s
epoch 84 | loss: 0.14281 | val_0_rmse: 0.35785 | val_1_rmse: 0.39861 |  0:01:37s
epoch 85 | loss: 0.1451  | val_0_rmse: 0.36688 | val_1_rmse: 0.40648 |  0:01:38s
epoch 86 | loss: 0.1445  | val_0_rmse: 0.36626 | val_1_rmse: 0.40169 |  0:01:39s
epoch 87 | loss: 0.14808 | val_0_rmse: 0.37734 | val_1_rmse: 0.41181 |  0:01:40s
epoch 88 | loss: 0.14793 | val_0_rmse: 0.37079 | val_1_rmse: 0.41004 |  0:01:41s
epoch 89 | loss: 0.14801 | val_0_rmse: 0.36719 | val_1_rmse: 0.40739 |  0:01:42s
epoch 90 | loss: 0.14653 | val_0_rmse: 0.36201 | val_1_rmse: 0.39867 |  0:01:43s
epoch 91 | loss: 0.14156 | val_0_rmse: 0.36404 | val_1_rmse: 0.40531 |  0:01:45s
epoch 92 | loss: 0.14498 | val_0_rmse: 0.36422 | val_1_rmse: 0.40194 |  0:01:46s
epoch 93 | loss: 0.15446 | val_0_rmse: 0.35613 | val_1_rmse: 0.39772 |  0:01:47s
epoch 94 | loss: 0.14723 | val_0_rmse: 0.35512 | val_1_rmse: 0.39455 |  0:01:48s
epoch 95 | loss: 0.14078 | val_0_rmse: 0.35884 | val_1_rmse: 0.39832 |  0:01:49s
epoch 96 | loss: 0.1433  | val_0_rmse: 0.36434 | val_1_rmse: 0.40091 |  0:01:50s
epoch 97 | loss: 0.15017 | val_0_rmse: 0.36727 | val_1_rmse: 0.40496 |  0:01:51s
epoch 98 | loss: 0.14361 | val_0_rmse: 0.36884 | val_1_rmse: 0.40943 |  0:01:53s
epoch 99 | loss: 0.14911 | val_0_rmse: 0.40006 | val_1_rmse: 0.43759 |  0:01:54s
epoch 100| loss: 0.1482  | val_0_rmse: 0.35867 | val_1_rmse: 0.40026 |  0:01:55s
epoch 101| loss: 0.14387 | val_0_rmse: 0.35704 | val_1_rmse: 0.40028 |  0:01:56s
epoch 102| loss: 0.14429 | val_0_rmse: 0.35888 | val_1_rmse: 0.4019  |  0:01:57s
epoch 103| loss: 0.14435 | val_0_rmse: 0.3677  | val_1_rmse: 0.41267 |  0:01:58s
epoch 104| loss: 0.14287 | val_0_rmse: 0.35792 | val_1_rmse: 0.39808 |  0:01:59s
epoch 105| loss: 0.14071 | val_0_rmse: 0.36308 | val_1_rmse: 0.3961  |  0:02:01s
epoch 106| loss: 0.15604 | val_0_rmse: 0.38596 | val_1_rmse: 0.40975 |  0:02:02s
epoch 107| loss: 0.15252 | val_0_rmse: 0.3888  | val_1_rmse: 0.41921 |  0:02:03s
epoch 108| loss: 0.1624  | val_0_rmse: 0.39816 | val_1_rmse: 0.42396 |  0:02:04s
epoch 109| loss: 0.15888 | val_0_rmse: 0.37328 | val_1_rmse: 0.40182 |  0:02:05s
epoch 110| loss: 0.15182 | val_0_rmse: 0.36958 | val_1_rmse: 0.39813 |  0:02:06s
epoch 111| loss: 0.14795 | val_0_rmse: 0.36807 | val_1_rmse: 0.39403 |  0:02:07s
epoch 112| loss: 0.14685 | val_0_rmse: 0.35946 | val_1_rmse: 0.39077 |  0:02:09s
epoch 113| loss: 0.16656 | val_0_rmse: 0.48457 | val_1_rmse: 0.51059 |  0:02:10s
epoch 114| loss: 0.18444 | val_0_rmse: 0.39929 | val_1_rmse: 0.42649 |  0:02:11s
epoch 115| loss: 0.16291 | val_0_rmse: 0.38723 | val_1_rmse: 0.41607 |  0:02:12s
epoch 116| loss: 0.15471 | val_0_rmse: 0.36974 | val_1_rmse: 0.39554 |  0:02:13s
epoch 117| loss: 0.15391 | val_0_rmse: 0.37261 | val_1_rmse: 0.39607 |  0:02:14s
epoch 118| loss: 0.1484  | val_0_rmse: 0.36839 | val_1_rmse: 0.39553 |  0:02:15s
epoch 119| loss: 0.14871 | val_0_rmse: 0.35833 | val_1_rmse: 0.38987 |  0:02:16s
epoch 120| loss: 0.14395 | val_0_rmse: 0.35821 | val_1_rmse: 0.3899  |  0:02:18s
epoch 121| loss: 0.14423 | val_0_rmse: 0.36421 | val_1_rmse: 0.3912  |  0:02:19s
epoch 122| loss: 0.14905 | val_0_rmse: 0.36947 | val_1_rmse: 0.39683 |  0:02:20s
epoch 123| loss: 0.14621 | val_0_rmse: 0.36524 | val_1_rmse: 0.39702 |  0:02:21s
epoch 124| loss: 0.14051 | val_0_rmse: 0.35601 | val_1_rmse: 0.39239 |  0:02:22s
epoch 125| loss: 0.14136 | val_0_rmse: 0.35101 | val_1_rmse: 0.38397 |  0:02:23s
epoch 126| loss: 0.14025 | val_0_rmse: 0.35357 | val_1_rmse: 0.39049 |  0:02:24s
epoch 127| loss: 0.1402  | val_0_rmse: 0.35535 | val_1_rmse: 0.39032 |  0:02:26s
epoch 128| loss: 0.14522 | val_0_rmse: 0.34857 | val_1_rmse: 0.38659 |  0:02:27s
epoch 129| loss: 0.14062 | val_0_rmse: 0.35903 | val_1_rmse: 0.39846 |  0:02:28s
epoch 130| loss: 0.13922 | val_0_rmse: 0.35334 | val_1_rmse: 0.39091 |  0:02:29s
epoch 131| loss: 0.13868 | val_0_rmse: 0.34656 | val_1_rmse: 0.38825 |  0:02:30s
epoch 132| loss: 0.13931 | val_0_rmse: 0.35244 | val_1_rmse: 0.39571 |  0:02:31s
epoch 133| loss: 0.13986 | val_0_rmse: 0.35045 | val_1_rmse: 0.39448 |  0:02:32s
epoch 134| loss: 0.13937 | val_0_rmse: 0.34592 | val_1_rmse: 0.39181 |  0:02:34s
epoch 135| loss: 0.13524 | val_0_rmse: 0.34698 | val_1_rmse: 0.38774 |  0:02:35s
epoch 136| loss: 0.14382 | val_0_rmse: 0.47037 | val_1_rmse: 0.49788 |  0:02:36s
epoch 137| loss: 0.17031 | val_0_rmse: 0.43245 | val_1_rmse: 0.45482 |  0:02:37s
epoch 138| loss: 0.17437 | val_0_rmse: 0.38727 | val_1_rmse: 0.41855 |  0:02:38s
epoch 139| loss: 0.16807 | val_0_rmse: 0.38988 | val_1_rmse: 0.42332 |  0:02:39s
epoch 140| loss: 0.16228 | val_0_rmse: 0.37406 | val_1_rmse: 0.4057  |  0:02:40s
epoch 141| loss: 0.16006 | val_0_rmse: 0.39108 | val_1_rmse: 0.41936 |  0:02:42s
epoch 142| loss: 0.15192 | val_0_rmse: 0.38895 | val_1_rmse: 0.41783 |  0:02:43s
epoch 143| loss: 0.15056 | val_0_rmse: 0.36049 | val_1_rmse: 0.3931  |  0:02:44s
epoch 144| loss: 0.14757 | val_0_rmse: 0.37133 | val_1_rmse: 0.40495 |  0:02:45s
epoch 145| loss: 0.14431 | val_0_rmse: 0.3633  | val_1_rmse: 0.39996 |  0:02:46s
epoch 146| loss: 0.14775 | val_0_rmse: 0.36994 | val_1_rmse: 0.40848 |  0:02:47s
epoch 147| loss: 0.1504  | val_0_rmse: 0.38114 | val_1_rmse: 0.41713 |  0:02:48s
epoch 148| loss: 0.1477  | val_0_rmse: 0.3668  | val_1_rmse: 0.3998  |  0:02:50s
epoch 149| loss: 0.14123 | val_0_rmse: 0.35985 | val_1_rmse: 0.3996  |  0:02:51s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.38397
Best weights from best epoch are automatically used!
ended training at: 05:11:07
Feature importance:
[('Area', 0.05598841727664933), ('Baths', 5.943807812812217e-06), ('Beds', 0.0), ('Latitude', 0.3483676158954911), ('Longitude', 0.0), ('Month', 8.971878077303621e-07), ('Year', 0.0005368477907009657), ('sqft_lot', 0.25550709073803607), ('floors', 0.0), ('waterfront', 0.20256652503470893), ('view', 6.492467995903956e-06), ('condition', 0.0004020894291155353), ('grade', 0.1265742500852578), ('sqft_above', 0.010031069496838262), ('sqft_basement', 1.2760789585605667e-05), ('yr_renovated', 0.0), ('zipcode', 0.0)]
Mean squared error is of 4404190695.099024
Mean absolute error:47618.2218841218
MAPE:0.1296073443852842
R2 score:0.8595564363267381
------------------------------------------------------------------
