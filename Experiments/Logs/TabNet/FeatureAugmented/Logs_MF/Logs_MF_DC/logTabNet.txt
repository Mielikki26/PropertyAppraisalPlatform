TabNet Logs:

Saving copy of script...
In this script only the DC dataset is used and its a continuation of the augmentation testsHere the test done is to test the improvement that the new 14 features provide
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:44:53
epoch 0  | loss: 0.62081 | val_0_rmse: 0.65647 | val_1_rmse: 0.64666 |  0:00:09s
epoch 1  | loss: 0.3143  | val_0_rmse: 0.52583 | val_1_rmse: 0.52469 |  0:00:14s
epoch 2  | loss: 0.24611 | val_0_rmse: 0.47781 | val_1_rmse: 0.47346 |  0:00:19s
epoch 3  | loss: 0.22135 | val_0_rmse: 0.44985 | val_1_rmse: 0.44835 |  0:00:24s
epoch 4  | loss: 0.20703 | val_0_rmse: 0.46014 | val_1_rmse: 0.46081 |  0:00:29s
epoch 5  | loss: 0.19924 | val_0_rmse: 0.42902 | val_1_rmse: 0.42923 |  0:00:33s
epoch 6  | loss: 0.19128 | val_0_rmse: 0.42139 | val_1_rmse: 0.42182 |  0:00:38s
epoch 7  | loss: 0.18598 | val_0_rmse: 0.41903 | val_1_rmse: 0.41462 |  0:00:42s
epoch 8  | loss: 0.18066 | val_0_rmse: 0.42692 | val_1_rmse: 0.42706 |  0:00:47s
epoch 9  | loss: 0.18735 | val_0_rmse: 0.41381 | val_1_rmse: 0.41223 |  0:00:51s
epoch 10 | loss: 0.17797 | val_0_rmse: 0.41737 | val_1_rmse: 0.42072 |  0:00:56s
epoch 11 | loss: 0.17248 | val_0_rmse: 0.40367 | val_1_rmse: 0.40299 |  0:01:01s
epoch 12 | loss: 0.16803 | val_0_rmse: 0.39826 | val_1_rmse: 0.3984  |  0:01:05s
epoch 13 | loss: 0.1653  | val_0_rmse: 0.38668 | val_1_rmse: 0.38611 |  0:01:09s
epoch 14 | loss: 0.17026 | val_0_rmse: 0.39488 | val_1_rmse: 0.39415 |  0:01:13s
epoch 15 | loss: 0.16651 | val_0_rmse: 0.38879 | val_1_rmse: 0.38988 |  0:01:18s
epoch 16 | loss: 0.17269 | val_0_rmse: 0.40121 | val_1_rmse: 0.40237 |  0:01:22s
epoch 17 | loss: 0.16362 | val_0_rmse: 0.38948 | val_1_rmse: 0.38991 |  0:01:26s
epoch 18 | loss: 0.16253 | val_0_rmse: 0.40104 | val_1_rmse: 0.40062 |  0:01:30s
epoch 19 | loss: 0.16268 | val_0_rmse: 0.39147 | val_1_rmse: 0.39165 |  0:01:34s
epoch 20 | loss: 0.15436 | val_0_rmse: 0.38751 | val_1_rmse: 0.39093 |  0:01:39s
epoch 21 | loss: 0.15828 | val_0_rmse: 0.38619 | val_1_rmse: 0.38664 |  0:01:43s
epoch 22 | loss: 0.15856 | val_0_rmse: 0.37991 | val_1_rmse: 0.38126 |  0:01:47s
epoch 23 | loss: 0.15564 | val_0_rmse: 0.37467 | val_1_rmse: 0.37837 |  0:01:51s
epoch 24 | loss: 0.1532  | val_0_rmse: 0.39235 | val_1_rmse: 0.39265 |  0:01:56s
epoch 25 | loss: 0.1527  | val_0_rmse: 0.38467 | val_1_rmse: 0.38681 |  0:02:00s
epoch 26 | loss: 0.15396 | val_0_rmse: 0.37925 | val_1_rmse: 0.37941 |  0:02:04s
epoch 27 | loss: 0.15212 | val_0_rmse: 0.37635 | val_1_rmse: 0.37501 |  0:02:08s
epoch 28 | loss: 0.15146 | val_0_rmse: 0.37593 | val_1_rmse: 0.38152 |  0:02:12s
epoch 29 | loss: 0.15298 | val_0_rmse: 0.38552 | val_1_rmse: 0.38456 |  0:02:17s
epoch 30 | loss: 0.15055 | val_0_rmse: 0.38196 | val_1_rmse: 0.38157 |  0:02:21s
epoch 31 | loss: 0.15066 | val_0_rmse: 0.3733  | val_1_rmse: 0.37655 |  0:02:25s
epoch 32 | loss: 0.15251 | val_0_rmse: 0.38526 | val_1_rmse: 0.38822 |  0:02:29s
epoch 33 | loss: 0.15524 | val_0_rmse: 0.38301 | val_1_rmse: 0.38593 |  0:02:34s
epoch 34 | loss: 0.15017 | val_0_rmse: 0.3639  | val_1_rmse: 0.36903 |  0:02:38s
epoch 35 | loss: 0.14651 | val_0_rmse: 0.36915 | val_1_rmse: 0.37369 |  0:02:42s
epoch 36 | loss: 0.1493  | val_0_rmse: 0.36678 | val_1_rmse: 0.37011 |  0:02:46s
epoch 37 | loss: 0.14805 | val_0_rmse: 0.36662 | val_1_rmse: 0.36915 |  0:02:51s
epoch 38 | loss: 0.1433  | val_0_rmse: 0.36687 | val_1_rmse: 0.37261 |  0:02:55s
epoch 39 | loss: 0.14646 | val_0_rmse: 0.3669  | val_1_rmse: 0.36907 |  0:02:59s
epoch 40 | loss: 0.14686 | val_0_rmse: 0.37237 | val_1_rmse: 0.37804 |  0:03:03s
epoch 41 | loss: 0.14388 | val_0_rmse: 0.38058 | val_1_rmse: 0.38608 |  0:03:08s
epoch 42 | loss: 0.14258 | val_0_rmse: 0.35951 | val_1_rmse: 0.36259 |  0:03:12s
epoch 43 | loss: 0.14168 | val_0_rmse: 0.41504 | val_1_rmse: 0.41843 |  0:03:16s
epoch 44 | loss: 0.14148 | val_0_rmse: 0.35963 | val_1_rmse: 0.36477 |  0:03:20s
epoch 45 | loss: 0.14092 | val_0_rmse: 0.36143 | val_1_rmse: 0.36565 |  0:03:25s
epoch 46 | loss: 0.14334 | val_0_rmse: 0.38952 | val_1_rmse: 0.39665 |  0:03:29s
epoch 47 | loss: 0.13717 | val_0_rmse: 0.36009 | val_1_rmse: 0.36723 |  0:03:33s
epoch 48 | loss: 0.14029 | val_0_rmse: 0.35908 | val_1_rmse: 0.36709 |  0:03:37s
epoch 49 | loss: 0.13832 | val_0_rmse: 0.35659 | val_1_rmse: 0.36263 |  0:03:41s
epoch 50 | loss: 0.14052 | val_0_rmse: 0.362   | val_1_rmse: 0.36872 |  0:03:46s
epoch 51 | loss: 0.13707 | val_0_rmse: 0.36146 | val_1_rmse: 0.36991 |  0:03:50s
epoch 52 | loss: 0.13928 | val_0_rmse: 0.38399 | val_1_rmse: 0.39044 |  0:03:54s
epoch 53 | loss: 0.13757 | val_0_rmse: 0.36881 | val_1_rmse: 0.37624 |  0:03:58s
epoch 54 | loss: 0.14364 | val_0_rmse: 0.3558  | val_1_rmse: 0.36269 |  0:04:03s
epoch 55 | loss: 0.13697 | val_0_rmse: 0.35658 | val_1_rmse: 0.36548 |  0:04:07s
epoch 56 | loss: 0.14006 | val_0_rmse: 0.35634 | val_1_rmse: 0.36588 |  0:04:11s
epoch 57 | loss: 0.13856 | val_0_rmse: 0.35879 | val_1_rmse: 0.36685 |  0:04:15s
epoch 58 | loss: 0.13614 | val_0_rmse: 0.35319 | val_1_rmse: 0.362   |  0:04:20s
epoch 59 | loss: 0.13689 | val_0_rmse: 0.35189 | val_1_rmse: 0.35943 |  0:04:24s
epoch 60 | loss: 0.13383 | val_0_rmse: 0.36128 | val_1_rmse: 0.36889 |  0:04:28s
epoch 61 | loss: 0.13515 | val_0_rmse: 0.35494 | val_1_rmse: 0.36247 |  0:04:32s
epoch 62 | loss: 0.13476 | val_0_rmse: 0.35536 | val_1_rmse: 0.36336 |  0:04:36s
epoch 63 | loss: 0.13213 | val_0_rmse: 0.36521 | val_1_rmse: 0.37344 |  0:04:41s
epoch 64 | loss: 0.13603 | val_0_rmse: 0.34905 | val_1_rmse: 0.36033 |  0:04:45s
epoch 65 | loss: 0.13383 | val_0_rmse: 0.34725 | val_1_rmse: 0.35687 |  0:04:49s
epoch 66 | loss: 0.13832 | val_0_rmse: 0.35607 | val_1_rmse: 0.36806 |  0:04:54s
epoch 67 | loss: 0.13672 | val_0_rmse: 0.35415 | val_1_rmse: 0.36596 |  0:04:58s
epoch 68 | loss: 0.13185 | val_0_rmse: 0.34963 | val_1_rmse: 0.36237 |  0:05:02s
epoch 69 | loss: 0.13524 | val_0_rmse: 0.35293 | val_1_rmse: 0.36233 |  0:05:06s
epoch 70 | loss: 0.13351 | val_0_rmse: 0.35377 | val_1_rmse: 0.36581 |  0:05:10s
epoch 71 | loss: 0.1349  | val_0_rmse: 0.35093 | val_1_rmse: 0.36323 |  0:05:15s
epoch 72 | loss: 0.13348 | val_0_rmse: 0.39066 | val_1_rmse: 0.39824 |  0:05:19s
epoch 73 | loss: 0.1403  | val_0_rmse: 0.3563  | val_1_rmse: 0.36963 |  0:05:23s
epoch 74 | loss: 0.13316 | val_0_rmse: 0.35978 | val_1_rmse: 0.37056 |  0:05:27s
epoch 75 | loss: 0.1315  | val_0_rmse: 0.35486 | val_1_rmse: 0.36621 |  0:05:31s
epoch 76 | loss: 0.12984 | val_0_rmse: 0.36713 | val_1_rmse: 0.37811 |  0:05:36s
epoch 77 | loss: 0.13093 | val_0_rmse: 0.34894 | val_1_rmse: 0.36259 |  0:05:40s
epoch 78 | loss: 0.13245 | val_0_rmse: 0.34363 | val_1_rmse: 0.35597 |  0:05:44s
epoch 79 | loss: 0.1325  | val_0_rmse: 0.35433 | val_1_rmse: 0.36724 |  0:05:48s
epoch 80 | loss: 0.1299  | val_0_rmse: 0.34898 | val_1_rmse: 0.36209 |  0:05:53s
epoch 81 | loss: 0.128   | val_0_rmse: 0.36631 | val_1_rmse: 0.37936 |  0:05:57s
epoch 82 | loss: 0.13238 | val_0_rmse: 0.35195 | val_1_rmse: 0.36891 |  0:06:01s
epoch 83 | loss: 0.13465 | val_0_rmse: 0.34387 | val_1_rmse: 0.35689 |  0:06:05s
epoch 84 | loss: 0.13013 | val_0_rmse: 0.35317 | val_1_rmse: 0.36722 |  0:06:09s
epoch 85 | loss: 0.13013 | val_0_rmse: 0.36141 | val_1_rmse: 0.37562 |  0:06:14s
epoch 86 | loss: 0.12804 | val_0_rmse: 0.35206 | val_1_rmse: 0.36586 |  0:06:18s
epoch 87 | loss: 0.14477 | val_0_rmse: 0.36088 | val_1_rmse: 0.37256 |  0:06:22s
epoch 88 | loss: 0.13418 | val_0_rmse: 0.35468 | val_1_rmse: 0.36542 |  0:06:26s
epoch 89 | loss: 0.13113 | val_0_rmse: 0.35593 | val_1_rmse: 0.36677 |  0:06:31s
epoch 90 | loss: 0.1359  | val_0_rmse: 0.34846 | val_1_rmse: 0.36228 |  0:06:35s
epoch 91 | loss: 0.13168 | val_0_rmse: 0.34462 | val_1_rmse: 0.35732 |  0:06:39s
epoch 92 | loss: 0.12759 | val_0_rmse: 0.34328 | val_1_rmse: 0.36014 |  0:06:43s
epoch 93 | loss: 0.12853 | val_0_rmse: 0.34496 | val_1_rmse: 0.36168 |  0:06:48s
epoch 94 | loss: 0.12932 | val_0_rmse: 0.33805 | val_1_rmse: 0.35369 |  0:06:52s
epoch 95 | loss: 0.12773 | val_0_rmse: 0.34369 | val_1_rmse: 0.3616  |  0:06:56s
epoch 96 | loss: 0.12827 | val_0_rmse: 0.34789 | val_1_rmse: 0.3636  |  0:07:00s
epoch 97 | loss: 0.12564 | val_0_rmse: 0.34286 | val_1_rmse: 0.36137 |  0:07:05s
epoch 98 | loss: 0.12954 | val_0_rmse: 0.36093 | val_1_rmse: 0.37784 |  0:07:09s
epoch 99 | loss: 0.13195 | val_0_rmse: 0.36071 | val_1_rmse: 0.37603 |  0:07:13s
epoch 100| loss: 0.128   | val_0_rmse: 0.36119 | val_1_rmse: 0.37607 |  0:07:17s
epoch 101| loss: 0.12663 | val_0_rmse: 0.34267 | val_1_rmse: 0.36068 |  0:07:21s
epoch 102| loss: 0.12464 | val_0_rmse: 0.34773 | val_1_rmse: 0.36357 |  0:07:25s
epoch 103| loss: 0.1271  | val_0_rmse: 0.3411  | val_1_rmse: 0.35812 |  0:07:30s
epoch 104| loss: 0.12463 | val_0_rmse: 0.33904 | val_1_rmse: 0.3581  |  0:07:34s
epoch 105| loss: 0.12566 | val_0_rmse: 0.39563 | val_1_rmse: 0.41064 |  0:07:38s
epoch 106| loss: 0.12936 | val_0_rmse: 0.34705 | val_1_rmse: 0.3645  |  0:07:42s
epoch 107| loss: 0.1263  | val_0_rmse: 0.36797 | val_1_rmse: 0.38625 |  0:07:46s
epoch 108| loss: 0.12497 | val_0_rmse: 0.33723 | val_1_rmse: 0.35664 |  0:07:51s
epoch 109| loss: 0.12523 | val_0_rmse: 0.34247 | val_1_rmse: 0.36291 |  0:07:55s
epoch 110| loss: 0.12342 | val_0_rmse: 0.34207 | val_1_rmse: 0.36373 |  0:07:59s
epoch 111| loss: 0.12412 | val_0_rmse: 0.33955 | val_1_rmse: 0.35965 |  0:08:03s
epoch 112| loss: 0.12335 | val_0_rmse: 0.33396 | val_1_rmse: 0.35376 |  0:08:07s
epoch 113| loss: 0.12355 | val_0_rmse: 0.34601 | val_1_rmse: 0.36631 |  0:08:12s
epoch 114| loss: 0.13309 | val_0_rmse: 0.34408 | val_1_rmse: 0.36299 |  0:08:16s
epoch 115| loss: 0.12828 | val_0_rmse: 0.34002 | val_1_rmse: 0.35906 |  0:08:20s
epoch 116| loss: 0.12514 | val_0_rmse: 0.33113 | val_1_rmse: 0.35028 |  0:08:24s
epoch 117| loss: 0.12445 | val_0_rmse: 0.3736  | val_1_rmse: 0.3925  |  0:08:29s
epoch 118| loss: 0.12505 | val_0_rmse: 0.36593 | val_1_rmse: 0.38371 |  0:08:33s
epoch 119| loss: 0.12173 | val_0_rmse: 0.34046 | val_1_rmse: 0.36009 |  0:08:37s
epoch 120| loss: 0.12472 | val_0_rmse: 0.34121 | val_1_rmse: 0.36258 |  0:08:41s
epoch 121| loss: 0.12557 | val_0_rmse: 0.34142 | val_1_rmse: 0.35959 |  0:08:45s
epoch 122| loss: 0.12316 | val_0_rmse: 0.33605 | val_1_rmse: 0.35742 |  0:08:49s
epoch 123| loss: 0.12412 | val_0_rmse: 0.35161 | val_1_rmse: 0.37332 |  0:08:54s
epoch 124| loss: 0.12464 | val_0_rmse: 0.34282 | val_1_rmse: 0.36365 |  0:08:58s
epoch 125| loss: 0.13801 | val_0_rmse: 0.34268 | val_1_rmse: 0.35848 |  0:09:02s
epoch 126| loss: 0.12938 | val_0_rmse: 0.35835 | val_1_rmse: 0.37665 |  0:09:06s
epoch 127| loss: 0.12624 | val_0_rmse: 0.34588 | val_1_rmse: 0.36434 |  0:09:11s
epoch 128| loss: 0.1232  | val_0_rmse: 0.35445 | val_1_rmse: 0.37103 |  0:09:15s
epoch 129| loss: 0.12425 | val_0_rmse: 0.33693 | val_1_rmse: 0.35792 |  0:09:19s
epoch 130| loss: 0.12356 | val_0_rmse: 0.36923 | val_1_rmse: 0.38874 |  0:09:23s
epoch 131| loss: 0.12792 | val_0_rmse: 0.35153 | val_1_rmse: 0.37219 |  0:09:27s
epoch 132| loss: 0.12745 | val_0_rmse: 0.36048 | val_1_rmse: 0.37908 |  0:09:32s
epoch 133| loss: 0.12401 | val_0_rmse: 0.36069 | val_1_rmse: 0.37896 |  0:09:36s
epoch 134| loss: 0.12495 | val_0_rmse: 0.35008 | val_1_rmse: 0.36909 |  0:09:40s
epoch 135| loss: 0.125   | val_0_rmse: 0.35965 | val_1_rmse: 0.38083 |  0:09:44s
epoch 136| loss: 0.12285 | val_0_rmse: 0.34021 | val_1_rmse: 0.36255 |  0:09:49s
epoch 137| loss: 0.12339 | val_0_rmse: 0.33748 | val_1_rmse: 0.35817 |  0:09:53s
epoch 138| loss: 0.12117 | val_0_rmse: 0.33738 | val_1_rmse: 0.36226 |  0:09:57s
epoch 139| loss: 0.12163 | val_0_rmse: 0.34696 | val_1_rmse: 0.36928 |  0:10:01s
epoch 140| loss: 0.12586 | val_0_rmse: 0.35148 | val_1_rmse: 0.37233 |  0:10:05s
epoch 141| loss: 0.12436 | val_0_rmse: 0.37166 | val_1_rmse: 0.39022 |  0:10:10s
epoch 142| loss: 0.12263 | val_0_rmse: 0.35238 | val_1_rmse: 0.37312 |  0:10:14s
epoch 143| loss: 0.11857 | val_0_rmse: 0.33727 | val_1_rmse: 0.36308 |  0:10:18s
epoch 144| loss: 0.12188 | val_0_rmse: 0.33917 | val_1_rmse: 0.36161 |  0:10:22s
epoch 145| loss: 0.12018 | val_0_rmse: 0.33945 | val_1_rmse: 0.35964 |  0:10:27s
epoch 146| loss: 0.11896 | val_0_rmse: 0.34709 | val_1_rmse: 0.36849 |  0:10:31s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.35028
Best weights from best epoch are automatically used!
ended training at: 04:55:26
Feature importance:
[('Area', 0.0), ('Baths', 0.23134511031765212), ('Beds', 0.03245757362404426), ('Latitude', 0.0), ('Longitude', 0.21558235162306566), ('Month', 0.0), ('Year', 0.2354425993815185), ('HF_BATHRM', 0.0), ('AC', 0.0), ('NUM_UNITS', 0.0), ('ROOMS', 0.0), ('AYB', 0.0), ('YR_RMDL', 7.271037749966619e-07), ('EYB', 0.0), ('STORIES', 0.0), ('QUALIFIED', 0.0), ('SALE_NUM', 0.0), ('KITCHENS', 0.0), ('FIREPLACES', 0.18074222301138881), ('SOURCE', 0.10442941493855562), ('ZIPCODE', 0.0)]
Mean squared error is of 7288529037.550111
Mean absolute error:57827.761406692065
MAPE:0.24140761097638636
R2 score:0.8733297531172243
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:55:27
epoch 0  | loss: 0.59289 | val_0_rmse: 0.63046 | val_1_rmse: 0.61168 |  0:00:04s
epoch 1  | loss: 0.30922 | val_0_rmse: 0.54285 | val_1_rmse: 0.53627 |  0:00:08s
epoch 2  | loss: 0.2444  | val_0_rmse: 0.48343 | val_1_rmse: 0.47734 |  0:00:12s
epoch 3  | loss: 0.21131 | val_0_rmse: 0.46257 | val_1_rmse: 0.45365 |  0:00:17s
epoch 4  | loss: 0.19388 | val_0_rmse: 0.43562 | val_1_rmse: 0.42891 |  0:00:21s
epoch 5  | loss: 0.19038 | val_0_rmse: 0.4405  | val_1_rmse: 0.43306 |  0:00:25s
epoch 6  | loss: 0.18655 | val_0_rmse: 0.43082 | val_1_rmse: 0.42641 |  0:00:30s
epoch 7  | loss: 0.18386 | val_0_rmse: 0.41422 | val_1_rmse: 0.40917 |  0:00:34s
epoch 8  | loss: 0.17872 | val_0_rmse: 0.41882 | val_1_rmse: 0.41387 |  0:00:38s
epoch 9  | loss: 0.17515 | val_0_rmse: 0.41005 | val_1_rmse: 0.40384 |  0:00:43s
epoch 10 | loss: 0.18025 | val_0_rmse: 0.40872 | val_1_rmse: 0.40366 |  0:00:47s
epoch 11 | loss: 0.17566 | val_0_rmse: 0.41269 | val_1_rmse: 0.40593 |  0:00:51s
epoch 12 | loss: 0.18049 | val_0_rmse: 0.40031 | val_1_rmse: 0.39768 |  0:00:55s
epoch 13 | loss: 0.17564 | val_0_rmse: 0.40159 | val_1_rmse: 0.39545 |  0:01:00s
epoch 14 | loss: 0.17466 | val_0_rmse: 0.39443 | val_1_rmse: 0.38824 |  0:01:04s
epoch 15 | loss: 0.17195 | val_0_rmse: 0.41383 | val_1_rmse: 0.40728 |  0:01:08s
epoch 16 | loss: 0.1653  | val_0_rmse: 0.39186 | val_1_rmse: 0.38781 |  0:01:12s
epoch 17 | loss: 0.16252 | val_0_rmse: 0.38512 | val_1_rmse: 0.3804  |  0:01:16s
epoch 18 | loss: 0.17184 | val_0_rmse: 0.39148 | val_1_rmse: 0.38654 |  0:01:21s
epoch 19 | loss: 0.16362 | val_0_rmse: 0.39133 | val_1_rmse: 0.38783 |  0:01:25s
epoch 20 | loss: 0.16531 | val_0_rmse: 0.39364 | val_1_rmse: 0.39091 |  0:01:29s
epoch 21 | loss: 0.1604  | val_0_rmse: 0.45333 | val_1_rmse: 0.45026 |  0:01:34s
epoch 22 | loss: 0.15893 | val_0_rmse: 0.37751 | val_1_rmse: 0.37448 |  0:01:38s
epoch 23 | loss: 0.15284 | val_0_rmse: 0.38379 | val_1_rmse: 0.37908 |  0:01:42s
epoch 24 | loss: 0.15212 | val_0_rmse: 0.37364 | val_1_rmse: 0.3721  |  0:01:47s
epoch 25 | loss: 0.15232 | val_0_rmse: 0.38266 | val_1_rmse: 0.38227 |  0:01:50s
epoch 26 | loss: 0.15073 | val_0_rmse: 0.38693 | val_1_rmse: 0.38601 |  0:01:54s
epoch 27 | loss: 0.14894 | val_0_rmse: 0.36545 | val_1_rmse: 0.36309 |  0:01:58s
epoch 28 | loss: 0.1504  | val_0_rmse: 0.36703 | val_1_rmse: 0.36512 |  0:02:02s
epoch 29 | loss: 0.14889 | val_0_rmse: 0.36367 | val_1_rmse: 0.3625  |  0:02:06s
epoch 30 | loss: 0.14628 | val_0_rmse: 0.36303 | val_1_rmse: 0.36276 |  0:02:10s
epoch 31 | loss: 0.14576 | val_0_rmse: 0.37074 | val_1_rmse: 0.37161 |  0:02:14s
epoch 32 | loss: 0.1474  | val_0_rmse: 0.36982 | val_1_rmse: 0.37149 |  0:02:18s
epoch 33 | loss: 0.1436  | val_0_rmse: 0.36422 | val_1_rmse: 0.36523 |  0:02:21s
epoch 34 | loss: 0.14585 | val_0_rmse: 0.36646 | val_1_rmse: 0.36531 |  0:02:25s
epoch 35 | loss: 0.14337 | val_0_rmse: 0.37318 | val_1_rmse: 0.37315 |  0:02:29s
epoch 36 | loss: 0.14498 | val_0_rmse: 0.37493 | val_1_rmse: 0.37657 |  0:02:33s
epoch 37 | loss: 0.14549 | val_0_rmse: 0.36256 | val_1_rmse: 0.36487 |  0:02:37s
epoch 38 | loss: 0.14289 | val_0_rmse: 0.36034 | val_1_rmse: 0.36104 |  0:02:41s
epoch 39 | loss: 0.14182 | val_0_rmse: 0.35044 | val_1_rmse: 0.35224 |  0:02:45s
epoch 40 | loss: 0.13954 | val_0_rmse: 0.35835 | val_1_rmse: 0.36115 |  0:02:48s
epoch 41 | loss: 0.14004 | val_0_rmse: 0.35261 | val_1_rmse: 0.35493 |  0:02:52s
epoch 42 | loss: 0.14468 | val_0_rmse: 0.35232 | val_1_rmse: 0.35616 |  0:02:56s
epoch 43 | loss: 0.14091 | val_0_rmse: 0.35517 | val_1_rmse: 0.35995 |  0:03:00s
epoch 44 | loss: 0.13977 | val_0_rmse: 0.3507  | val_1_rmse: 0.35386 |  0:03:04s
epoch 45 | loss: 0.14056 | val_0_rmse: 0.35794 | val_1_rmse: 0.36177 |  0:03:08s
epoch 46 | loss: 0.13716 | val_0_rmse: 0.35932 | val_1_rmse: 0.36054 |  0:03:12s
epoch 47 | loss: 0.13915 | val_0_rmse: 0.35737 | val_1_rmse: 0.36135 |  0:03:16s
epoch 48 | loss: 0.13844 | val_0_rmse: 0.37069 | val_1_rmse: 0.37672 |  0:03:19s
epoch 49 | loss: 0.13765 | val_0_rmse: 0.3781  | val_1_rmse: 0.38558 |  0:03:23s
epoch 50 | loss: 0.13827 | val_0_rmse: 0.35497 | val_1_rmse: 0.36067 |  0:03:27s
epoch 51 | loss: 0.13662 | val_0_rmse: 0.35045 | val_1_rmse: 0.35568 |  0:03:31s
epoch 52 | loss: 0.13501 | val_0_rmse: 0.36162 | val_1_rmse: 0.36775 |  0:03:35s
epoch 53 | loss: 0.13784 | val_0_rmse: 0.35413 | val_1_rmse: 0.35946 |  0:03:39s
epoch 54 | loss: 0.1385  | val_0_rmse: 0.34703 | val_1_rmse: 0.35273 |  0:03:43s
epoch 55 | loss: 0.13902 | val_0_rmse: 0.35495 | val_1_rmse: 0.36214 |  0:03:46s
epoch 56 | loss: 0.13659 | val_0_rmse: 0.34753 | val_1_rmse: 0.3534  |  0:03:50s
epoch 57 | loss: 0.13532 | val_0_rmse: 0.35973 | val_1_rmse: 0.36665 |  0:03:54s
epoch 58 | loss: 0.13343 | val_0_rmse: 0.3448  | val_1_rmse: 0.35214 |  0:03:58s
epoch 59 | loss: 0.13578 | val_0_rmse: 0.34134 | val_1_rmse: 0.34959 |  0:04:02s
epoch 60 | loss: 0.13332 | val_0_rmse: 0.35114 | val_1_rmse: 0.35892 |  0:04:06s
epoch 61 | loss: 0.13403 | val_0_rmse: 0.35474 | val_1_rmse: 0.36025 |  0:04:10s
epoch 62 | loss: 0.13219 | val_0_rmse: 0.35312 | val_1_rmse: 0.36253 |  0:04:14s
epoch 63 | loss: 0.13243 | val_0_rmse: 0.35039 | val_1_rmse: 0.36005 |  0:04:17s
epoch 64 | loss: 0.13283 | val_0_rmse: 0.35574 | val_1_rmse: 0.36518 |  0:04:21s
epoch 65 | loss: 0.13433 | val_0_rmse: 0.34651 | val_1_rmse: 0.35683 |  0:04:25s
epoch 66 | loss: 0.13329 | val_0_rmse: 0.3577  | val_1_rmse: 0.36463 |  0:04:29s
epoch 67 | loss: 0.13503 | val_0_rmse: 0.3423  | val_1_rmse: 0.35249 |  0:04:33s
epoch 68 | loss: 0.13275 | val_0_rmse: 0.38401 | val_1_rmse: 0.39229 |  0:04:37s
epoch 69 | loss: 0.13596 | val_0_rmse: 0.34907 | val_1_rmse: 0.35855 |  0:04:41s
epoch 70 | loss: 0.13256 | val_0_rmse: 0.34761 | val_1_rmse: 0.35799 |  0:04:45s
epoch 71 | loss: 0.13129 | val_0_rmse: 0.3722  | val_1_rmse: 0.38134 |  0:04:48s
epoch 72 | loss: 0.13235 | val_0_rmse: 0.34265 | val_1_rmse: 0.35294 |  0:04:52s
epoch 73 | loss: 0.13119 | val_0_rmse: 0.38419 | val_1_rmse: 0.39152 |  0:04:56s
epoch 74 | loss: 0.13387 | val_0_rmse: 0.35669 | val_1_rmse: 0.3705  |  0:05:00s
epoch 75 | loss: 0.13381 | val_0_rmse: 0.34519 | val_1_rmse: 0.35558 |  0:05:04s
epoch 76 | loss: 0.13111 | val_0_rmse: 0.34685 | val_1_rmse: 0.35571 |  0:05:08s
epoch 77 | loss: 0.12836 | val_0_rmse: 0.33801 | val_1_rmse: 0.35148 |  0:05:11s
epoch 78 | loss: 0.12995 | val_0_rmse: 0.33914 | val_1_rmse: 0.35461 |  0:05:15s
epoch 79 | loss: 0.12778 | val_0_rmse: 0.3404  | val_1_rmse: 0.35231 |  0:05:19s
epoch 80 | loss: 0.13262 | val_0_rmse: 0.34435 | val_1_rmse: 0.35575 |  0:05:23s
epoch 81 | loss: 0.13218 | val_0_rmse: 0.3479  | val_1_rmse: 0.35931 |  0:05:27s
epoch 82 | loss: 0.13038 | val_0_rmse: 0.34146 | val_1_rmse: 0.35398 |  0:05:31s
epoch 83 | loss: 0.13002 | val_0_rmse: 0.36947 | val_1_rmse: 0.37981 |  0:05:35s
epoch 84 | loss: 0.1296  | val_0_rmse: 0.3382  | val_1_rmse: 0.3493  |  0:05:39s
epoch 85 | loss: 0.12821 | val_0_rmse: 0.34826 | val_1_rmse: 0.36285 |  0:05:42s
epoch 86 | loss: 0.12763 | val_0_rmse: 0.34058 | val_1_rmse: 0.35337 |  0:05:46s
epoch 87 | loss: 0.13077 | val_0_rmse: 0.34738 | val_1_rmse: 0.35742 |  0:05:50s
epoch 88 | loss: 0.12987 | val_0_rmse: 0.33894 | val_1_rmse: 0.35346 |  0:05:54s
epoch 89 | loss: 0.13007 | val_0_rmse: 0.35195 | val_1_rmse: 0.36239 |  0:05:58s
epoch 90 | loss: 0.1302  | val_0_rmse: 0.34227 | val_1_rmse: 0.35486 |  0:06:01s
epoch 91 | loss: 0.12726 | val_0_rmse: 0.34605 | val_1_rmse: 0.35893 |  0:06:05s
epoch 92 | loss: 0.12561 | val_0_rmse: 0.36153 | val_1_rmse: 0.37208 |  0:06:09s
epoch 93 | loss: 0.12692 | val_0_rmse: 0.35773 | val_1_rmse: 0.36954 |  0:06:13s
epoch 94 | loss: 0.13133 | val_0_rmse: 0.34774 | val_1_rmse: 0.3594  |  0:06:17s
epoch 95 | loss: 0.13364 | val_0_rmse: 0.39988 | val_1_rmse: 0.41231 |  0:06:21s
epoch 96 | loss: 0.12944 | val_0_rmse: 0.346   | val_1_rmse: 0.36013 |  0:06:25s
epoch 97 | loss: 0.12895 | val_0_rmse: 0.34965 | val_1_rmse: 0.36213 |  0:06:28s
epoch 98 | loss: 0.12682 | val_0_rmse: 0.33815 | val_1_rmse: 0.35191 |  0:06:32s
epoch 99 | loss: 0.12741 | val_0_rmse: 0.33723 | val_1_rmse: 0.34873 |  0:06:36s
epoch 100| loss: 0.12867 | val_0_rmse: 0.35262 | val_1_rmse: 0.36657 |  0:06:40s
epoch 101| loss: 0.12341 | val_0_rmse: 0.33797 | val_1_rmse: 0.35159 |  0:06:44s
epoch 102| loss: 0.12724 | val_0_rmse: 0.33893 | val_1_rmse: 0.35254 |  0:06:48s
epoch 103| loss: 0.12744 | val_0_rmse: 0.34636 | val_1_rmse: 0.35791 |  0:06:52s
epoch 104| loss: 0.12497 | val_0_rmse: 0.33735 | val_1_rmse: 0.34909 |  0:06:55s
epoch 105| loss: 0.12774 | val_0_rmse: 0.34722 | val_1_rmse: 0.35961 |  0:06:59s
epoch 106| loss: 0.12615 | val_0_rmse: 0.34508 | val_1_rmse: 0.36196 |  0:07:03s
epoch 107| loss: 0.12328 | val_0_rmse: 0.33619 | val_1_rmse: 0.35201 |  0:07:07s
epoch 108| loss: 0.12923 | val_0_rmse: 0.36045 | val_1_rmse: 0.37077 |  0:07:11s
epoch 109| loss: 0.13225 | val_0_rmse: 0.3492  | val_1_rmse: 0.36445 |  0:07:15s
epoch 110| loss: 0.12726 | val_0_rmse: 0.35064 | val_1_rmse: 0.36605 |  0:07:19s
epoch 111| loss: 0.1274  | val_0_rmse: 0.35061 | val_1_rmse: 0.36444 |  0:07:22s
epoch 112| loss: 0.12632 | val_0_rmse: 0.33244 | val_1_rmse: 0.34852 |  0:07:26s
epoch 113| loss: 0.12391 | val_0_rmse: 0.34059 | val_1_rmse: 0.35782 |  0:07:30s
epoch 114| loss: 0.12596 | val_0_rmse: 0.33684 | val_1_rmse: 0.35559 |  0:07:34s
epoch 115| loss: 0.12422 | val_0_rmse: 0.3369  | val_1_rmse: 0.35274 |  0:07:38s
epoch 116| loss: 0.12061 | val_0_rmse: 0.35219 | val_1_rmse: 0.36903 |  0:07:42s
epoch 117| loss: 0.12408 | val_0_rmse: 0.34016 | val_1_rmse: 0.35738 |  0:07:46s
epoch 118| loss: 0.12158 | val_0_rmse: 0.33235 | val_1_rmse: 0.34862 |  0:07:49s
epoch 119| loss: 0.1215  | val_0_rmse: 0.32945 | val_1_rmse: 0.3498  |  0:07:53s
epoch 120| loss: 0.122   | val_0_rmse: 0.33494 | val_1_rmse: 0.35152 |  0:07:57s
epoch 121| loss: 0.12505 | val_0_rmse: 0.32844 | val_1_rmse: 0.34588 |  0:08:01s
epoch 122| loss: 0.12297 | val_0_rmse: 0.33996 | val_1_rmse: 0.35755 |  0:08:05s
epoch 123| loss: 0.12183 | val_0_rmse: 0.32848 | val_1_rmse: 0.35214 |  0:08:09s
epoch 124| loss: 0.12214 | val_0_rmse: 0.3289  | val_1_rmse: 0.34843 |  0:08:13s
epoch 125| loss: 0.12729 | val_0_rmse: 0.33957 | val_1_rmse: 0.3601  |  0:08:16s
epoch 126| loss: 0.12364 | val_0_rmse: 0.33748 | val_1_rmse: 0.35565 |  0:08:20s
epoch 127| loss: 0.12408 | val_0_rmse: 0.33177 | val_1_rmse: 0.35061 |  0:08:24s
epoch 128| loss: 0.12084 | val_0_rmse: 0.33293 | val_1_rmse: 0.35336 |  0:08:28s
epoch 129| loss: 0.12021 | val_0_rmse: 0.33355 | val_1_rmse: 0.35578 |  0:08:32s
epoch 130| loss: 0.12322 | val_0_rmse: 0.32937 | val_1_rmse: 0.34834 |  0:08:36s
epoch 131| loss: 0.12535 | val_0_rmse: 0.37068 | val_1_rmse: 0.38619 |  0:08:40s
epoch 132| loss: 0.11866 | val_0_rmse: 0.33646 | val_1_rmse: 0.35902 |  0:08:43s
epoch 133| loss: 0.11991 | val_0_rmse: 0.33237 | val_1_rmse: 0.35153 |  0:08:47s
epoch 134| loss: 0.12082 | val_0_rmse: 0.32524 | val_1_rmse: 0.34677 |  0:08:51s
epoch 135| loss: 0.11881 | val_0_rmse: 0.34711 | val_1_rmse: 0.36643 |  0:08:55s
epoch 136| loss: 0.11639 | val_0_rmse: 0.31981 | val_1_rmse: 0.34409 |  0:08:59s
epoch 137| loss: 0.11561 | val_0_rmse: 0.33616 | val_1_rmse: 0.35851 |  0:09:02s
epoch 138| loss: 0.11784 | val_0_rmse: 0.33333 | val_1_rmse: 0.35453 |  0:09:06s
epoch 139| loss: 0.11996 | val_0_rmse: 0.33924 | val_1_rmse: 0.36134 |  0:09:10s
epoch 140| loss: 0.12485 | val_0_rmse: 0.36266 | val_1_rmse: 0.3806  |  0:09:14s
epoch 141| loss: 0.12593 | val_0_rmse: 0.33098 | val_1_rmse: 0.35133 |  0:09:18s
epoch 142| loss: 0.12367 | val_0_rmse: 0.33465 | val_1_rmse: 0.35491 |  0:09:22s
epoch 143| loss: 0.11864 | val_0_rmse: 0.33305 | val_1_rmse: 0.35286 |  0:09:26s
epoch 144| loss: 0.11798 | val_0_rmse: 0.32697 | val_1_rmse: 0.34955 |  0:09:29s
epoch 145| loss: 0.12332 | val_0_rmse: 0.34157 | val_1_rmse: 0.35842 |  0:09:33s
epoch 146| loss: 0.12344 | val_0_rmse: 0.3343  | val_1_rmse: 0.35467 |  0:09:37s
epoch 147| loss: 0.12304 | val_0_rmse: 0.35991 | val_1_rmse: 0.37456 |  0:09:41s
epoch 148| loss: 0.1256  | val_0_rmse: 0.33057 | val_1_rmse: 0.35075 |  0:09:45s
epoch 149| loss: 0.1227  | val_0_rmse: 0.32222 | val_1_rmse: 0.34497 |  0:09:49s
Stop training because you reached max_epochs = 150 with best_epoch = 136 and best_val_1_rmse = 0.34409
Best weights from best epoch are automatically used!
ended training at: 05:05:18
Feature importance:
[('Area', 0.0), ('Baths', 0.09381390036905582), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.25771400645112597), ('HF_BATHRM', 0.008759741229642102), ('AC', 0.05834011720440896), ('NUM_UNITS', 0.0), ('ROOMS', 0.10008952882303979), ('AYB', 0.0), ('YR_RMDL', 0.0), ('EYB', 0.05050676271551204), ('STORIES', 0.0), ('QUALIFIED', 0.11764971137144058), ('SALE_NUM', 0.09293217185905987), ('KITCHENS', 0.0), ('FIREPLACES', 0.16388579662577515), ('SOURCE', 0.056054481997457675), ('ZIPCODE', 0.00025378135348202914)]
Mean squared error is of 7479789356.4472275
Mean absolute error:56876.18718659076
MAPE:0.2331972909423437
R2 score:0.8704773469237747
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:05:18
epoch 0  | loss: 0.64206 | val_0_rmse: 0.7061  | val_1_rmse: 0.83493 |  0:00:03s
epoch 1  | loss: 0.41458 | val_0_rmse: 0.66087 | val_1_rmse: 0.81853 |  0:00:07s
epoch 2  | loss: 0.32491 | val_0_rmse: 0.55558 | val_1_rmse: 0.55338 |  0:00:11s
epoch 3  | loss: 0.2636  | val_0_rmse: 0.48876 | val_1_rmse: 0.53305 |  0:00:15s
epoch 4  | loss: 0.23593 | val_0_rmse: 0.47919 | val_1_rmse: 0.83649 |  0:00:19s
epoch 5  | loss: 0.2057  | val_0_rmse: 0.43884 | val_1_rmse: 1.07347 |  0:00:23s
epoch 6  | loss: 0.1985  | val_0_rmse: 0.41847 | val_1_rmse: 0.88397 |  0:00:27s
epoch 7  | loss: 0.19246 | val_0_rmse: 0.428   | val_1_rmse: 1.06687 |  0:00:30s
epoch 8  | loss: 0.1811  | val_0_rmse: 0.43617 | val_1_rmse: 1.17815 |  0:00:34s
epoch 9  | loss: 0.17817 | val_0_rmse: 0.40675 | val_1_rmse: 0.60014 |  0:00:38s
epoch 10 | loss: 0.17563 | val_0_rmse: 0.39891 | val_1_rmse: 0.40432 |  0:00:42s
epoch 11 | loss: 0.1735  | val_0_rmse: 0.4094  | val_1_rmse: 0.40854 |  0:00:46s
epoch 12 | loss: 0.17017 | val_0_rmse: 0.40054 | val_1_rmse: 0.40392 |  0:00:50s
epoch 13 | loss: 0.17288 | val_0_rmse: 0.38679 | val_1_rmse: 0.39193 |  0:00:53s
epoch 14 | loss: 0.16578 | val_0_rmse: 0.41281 | val_1_rmse: 0.416   |  0:00:57s
epoch 15 | loss: 0.16253 | val_0_rmse: 0.40084 | val_1_rmse: 0.40486 |  0:01:01s
epoch 16 | loss: 0.16967 | val_0_rmse: 0.3796  | val_1_rmse: 0.38454 |  0:01:05s
epoch 17 | loss: 0.16169 | val_0_rmse: 0.37656 | val_1_rmse: 0.38256 |  0:01:09s
epoch 18 | loss: 0.16307 | val_0_rmse: 0.39473 | val_1_rmse: 0.40167 |  0:01:13s
epoch 19 | loss: 0.16046 | val_0_rmse: 0.40791 | val_1_rmse: 0.41392 |  0:01:17s
epoch 20 | loss: 0.15801 | val_0_rmse: 0.38981 | val_1_rmse: 0.39892 |  0:01:20s
epoch 21 | loss: 0.15497 | val_0_rmse: 0.36434 | val_1_rmse: 0.37072 |  0:01:24s
epoch 22 | loss: 0.15104 | val_0_rmse: 0.3613  | val_1_rmse: 0.37024 |  0:01:28s
epoch 23 | loss: 0.15087 | val_0_rmse: 0.37027 | val_1_rmse: 0.37545 |  0:01:32s
epoch 24 | loss: 0.15267 | val_0_rmse: 0.37051 | val_1_rmse: 0.37954 |  0:01:36s
epoch 25 | loss: 0.14924 | val_0_rmse: 0.3606  | val_1_rmse: 0.37139 |  0:01:40s
epoch 26 | loss: 0.14735 | val_0_rmse: 0.36458 | val_1_rmse: 0.37376 |  0:01:44s
epoch 27 | loss: 0.14642 | val_0_rmse: 0.37088 | val_1_rmse: 0.38133 |  0:01:47s
epoch 28 | loss: 0.14965 | val_0_rmse: 0.36606 | val_1_rmse: 0.37613 |  0:01:51s
epoch 29 | loss: 0.14749 | val_0_rmse: 0.36085 | val_1_rmse: 0.37049 |  0:01:55s
epoch 30 | loss: 0.14597 | val_0_rmse: 0.36173 | val_1_rmse: 0.372   |  0:01:59s
epoch 31 | loss: 0.14277 | val_0_rmse: 0.35729 | val_1_rmse: 0.36915 |  0:02:03s
epoch 32 | loss: 0.14653 | val_0_rmse: 0.35544 | val_1_rmse: 0.36778 |  0:02:07s
epoch 33 | loss: 0.14179 | val_0_rmse: 0.35428 | val_1_rmse: 0.3661  |  0:02:10s
epoch 34 | loss: 0.14055 | val_0_rmse: 0.38331 | val_1_rmse: 0.39553 |  0:02:14s
epoch 35 | loss: 0.144   | val_0_rmse: 0.36592 | val_1_rmse: 0.37572 |  0:02:18s
epoch 36 | loss: 0.14269 | val_0_rmse: 0.35292 | val_1_rmse: 0.36468 |  0:02:22s
epoch 37 | loss: 0.14005 | val_0_rmse: 0.34637 | val_1_rmse: 0.35776 |  0:02:26s
epoch 38 | loss: 0.13902 | val_0_rmse: 0.35924 | val_1_rmse: 0.37165 |  0:02:29s
epoch 39 | loss: 0.13918 | val_0_rmse: 0.34476 | val_1_rmse: 0.36032 |  0:02:33s
epoch 40 | loss: 0.1371  | val_0_rmse: 0.34753 | val_1_rmse: 0.36312 |  0:02:37s
epoch 41 | loss: 0.13587 | val_0_rmse: 0.35597 | val_1_rmse: 0.3683  |  0:02:41s
epoch 42 | loss: 0.13918 | val_0_rmse: 0.35912 | val_1_rmse: 0.37306 |  0:02:45s
epoch 43 | loss: 0.13598 | val_0_rmse: 0.35788 | val_1_rmse: 0.37285 |  0:02:49s
epoch 44 | loss: 0.13405 | val_0_rmse: 0.35195 | val_1_rmse: 0.36752 |  0:02:53s
epoch 45 | loss: 0.13569 | val_0_rmse: 0.35947 | val_1_rmse: 0.37668 |  0:02:56s
epoch 46 | loss: 0.13767 | val_0_rmse: 0.37046 | val_1_rmse: 0.38364 |  0:03:00s
epoch 47 | loss: 0.139   | val_0_rmse: 0.34484 | val_1_rmse: 0.36354 |  0:03:04s
epoch 48 | loss: 0.13447 | val_0_rmse: 0.35582 | val_1_rmse: 0.37134 |  0:03:08s
epoch 49 | loss: 0.1341  | val_0_rmse: 0.34396 | val_1_rmse: 0.36122 |  0:03:12s
epoch 50 | loss: 0.13784 | val_0_rmse: 0.34924 | val_1_rmse: 0.36809 |  0:03:16s
epoch 51 | loss: 0.13527 | val_0_rmse: 0.34824 | val_1_rmse: 0.36787 |  0:03:19s
epoch 52 | loss: 0.13315 | val_0_rmse: 0.33923 | val_1_rmse: 0.35745 |  0:03:23s
epoch 53 | loss: 0.13274 | val_0_rmse: 0.34363 | val_1_rmse: 0.36506 |  0:03:27s
epoch 54 | loss: 0.13326 | val_0_rmse: 0.34347 | val_1_rmse: 0.36682 |  0:03:31s
epoch 55 | loss: 0.13262 | val_0_rmse: 0.33771 | val_1_rmse: 0.35664 |  0:03:35s
epoch 56 | loss: 0.13374 | val_0_rmse: 0.34581 | val_1_rmse: 0.36503 |  0:03:39s
epoch 57 | loss: 0.13133 | val_0_rmse: 0.34655 | val_1_rmse: 0.36642 |  0:03:42s
epoch 58 | loss: 0.13293 | val_0_rmse: 0.33546 | val_1_rmse: 0.35729 |  0:03:46s
epoch 59 | loss: 0.13027 | val_0_rmse: 0.34978 | val_1_rmse: 0.36788 |  0:03:50s
epoch 60 | loss: 0.1341  | val_0_rmse: 0.35135 | val_1_rmse: 0.37517 |  0:03:54s
epoch 61 | loss: 0.13503 | val_0_rmse: 0.35837 | val_1_rmse: 0.37516 |  0:03:58s
epoch 62 | loss: 0.13775 | val_0_rmse: 0.3376  | val_1_rmse: 0.35877 |  0:04:02s
epoch 63 | loss: 0.1331  | val_0_rmse: 0.34911 | val_1_rmse: 0.37125 |  0:04:06s
epoch 64 | loss: 0.13354 | val_0_rmse: 0.33516 | val_1_rmse: 0.35615 |  0:04:09s
epoch 65 | loss: 0.13182 | val_0_rmse: 0.33855 | val_1_rmse: 0.36161 |  0:04:13s
epoch 66 | loss: 0.1292  | val_0_rmse: 0.33505 | val_1_rmse: 0.35889 |  0:04:17s
epoch 67 | loss: 0.12917 | val_0_rmse: 0.33857 | val_1_rmse: 0.36188 |  0:04:21s
epoch 68 | loss: 0.12954 | val_0_rmse: 0.33952 | val_1_rmse: 0.36232 |  0:04:25s
epoch 69 | loss: 0.13017 | val_0_rmse: 0.33708 | val_1_rmse: 0.36129 |  0:04:29s
epoch 70 | loss: 0.13111 | val_0_rmse: 0.33485 | val_1_rmse: 0.35646 |  0:04:32s
epoch 71 | loss: 0.12776 | val_0_rmse: 0.33942 | val_1_rmse: 0.36183 |  0:04:36s
epoch 72 | loss: 0.12775 | val_0_rmse: 0.34309 | val_1_rmse: 0.36864 |  0:04:40s
epoch 73 | loss: 0.12808 | val_0_rmse: 0.33406 | val_1_rmse: 0.36102 |  0:04:44s
epoch 74 | loss: 0.12819 | val_0_rmse: 0.33983 | val_1_rmse: 0.3646  |  0:04:48s
epoch 75 | loss: 0.13039 | val_0_rmse: 0.3443  | val_1_rmse: 0.36999 |  0:04:52s
epoch 76 | loss: 0.13013 | val_0_rmse: 0.3452  | val_1_rmse: 0.36695 |  0:04:55s
epoch 77 | loss: 0.1305  | val_0_rmse: 0.33757 | val_1_rmse: 0.36141 |  0:04:59s
epoch 78 | loss: 0.12896 | val_0_rmse: 0.32848 | val_1_rmse: 0.3565  |  0:05:03s
epoch 79 | loss: 0.12817 | val_0_rmse: 0.34781 | val_1_rmse: 0.36966 |  0:05:07s
epoch 80 | loss: 0.12872 | val_0_rmse: 0.33474 | val_1_rmse: 0.36239 |  0:05:11s
epoch 81 | loss: 0.12894 | val_0_rmse: 0.3579  | val_1_rmse: 0.38004 |  0:05:15s
epoch 82 | loss: 0.12956 | val_0_rmse: 0.35507 | val_1_rmse: 0.37731 |  0:05:19s
epoch 83 | loss: 0.12806 | val_0_rmse: 0.32839 | val_1_rmse: 0.35759 |  0:05:22s
epoch 84 | loss: 0.12905 | val_0_rmse: 0.34397 | val_1_rmse: 0.3684  |  0:05:26s
epoch 85 | loss: 0.12641 | val_0_rmse: 0.345   | val_1_rmse: 0.37126 |  0:05:30s
epoch 86 | loss: 0.12788 | val_0_rmse: 0.33651 | val_1_rmse: 0.36506 |  0:05:34s
epoch 87 | loss: 0.12691 | val_0_rmse: 0.33362 | val_1_rmse: 0.36144 |  0:05:38s
epoch 88 | loss: 0.12638 | val_0_rmse: 0.33358 | val_1_rmse: 0.36412 |  0:05:42s
epoch 89 | loss: 0.12473 | val_0_rmse: 0.34035 | val_1_rmse: 0.36506 |  0:05:46s
epoch 90 | loss: 0.12777 | val_0_rmse: 0.33913 | val_1_rmse: 0.36792 |  0:05:49s
epoch 91 | loss: 0.12633 | val_0_rmse: 0.32893 | val_1_rmse: 0.36325 |  0:05:53s
epoch 92 | loss: 0.12684 | val_0_rmse: 0.35964 | val_1_rmse: 0.38639 |  0:05:57s
epoch 93 | loss: 0.12814 | val_0_rmse: 0.3301  | val_1_rmse: 0.35895 |  0:06:01s
epoch 94 | loss: 0.12555 | val_0_rmse: 0.33048 | val_1_rmse: 0.35998 |  0:06:04s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.35615
Best weights from best epoch are automatically used!
ended training at: 05:11:24
Feature importance:
[('Area', 0.0), ('Baths', 0.11048025449750952), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 4.074656870387542e-06), ('Month', 0.0), ('Year', 0.5234496951868302), ('HF_BATHRM', 2.8185769273715746e-06), ('AC', 3.60997166415049e-05), ('NUM_UNITS', 0.039949034370093076), ('ROOMS', 0.037874277151058594), ('AYB', 0.01092094494126121), ('YR_RMDL', 0.0), ('EYB', 0.0), ('STORIES', 0.0), ('QUALIFIED', 0.005133782497149947), ('SALE_NUM', 0.04300723360398553), ('KITCHENS', 0.0007087286368986602), ('FIREPLACES', 0.21741476647752764), ('SOURCE', 0.0), ('ZIPCODE', 0.011018289687246458)]
Mean squared error is of 7523024747.590402
Mean absolute error:58895.06471981187
MAPE:0.25887669846611566
R2 score:0.8685469414226131
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:11:24
epoch 0  | loss: 0.65105 | val_0_rmse: 0.71273 | val_1_rmse: 0.7071  |  0:00:03s
epoch 1  | loss: 0.38709 | val_0_rmse: 0.58915 | val_1_rmse: 0.59031 |  0:00:07s
epoch 2  | loss: 0.30244 | val_0_rmse: 0.53244 | val_1_rmse: 0.52847 |  0:00:10s
epoch 3  | loss: 0.25367 | val_0_rmse: 0.48181 | val_1_rmse: 0.48325 |  0:00:14s
epoch 4  | loss: 0.22771 | val_0_rmse: 0.47129 | val_1_rmse: 0.46811 |  0:00:18s
epoch 5  | loss: 0.21266 | val_0_rmse: 0.46029 | val_1_rmse: 0.4578  |  0:00:21s
epoch 6  | loss: 0.2025  | val_0_rmse: 0.46677 | val_1_rmse: 0.46571 |  0:00:25s
epoch 7  | loss: 0.19707 | val_0_rmse: 0.47091 | val_1_rmse: 0.47333 |  0:00:29s
epoch 8  | loss: 0.19488 | val_0_rmse: 0.44688 | val_1_rmse: 0.44403 |  0:00:32s
epoch 9  | loss: 0.19281 | val_0_rmse: 0.41686 | val_1_rmse: 0.41715 |  0:00:36s
epoch 10 | loss: 0.18211 | val_0_rmse: 0.41189 | val_1_rmse: 0.4094  |  0:00:40s
epoch 11 | loss: 0.17383 | val_0_rmse: 0.42081 | val_1_rmse: 0.42088 |  0:00:43s
epoch 12 | loss: 0.18104 | val_0_rmse: 0.46046 | val_1_rmse: 0.45876 |  0:00:47s
epoch 13 | loss: 0.17478 | val_0_rmse: 0.40771 | val_1_rmse: 0.40459 |  0:00:51s
epoch 14 | loss: 0.16708 | val_0_rmse: 0.42379 | val_1_rmse: 0.40608 |  0:00:54s
epoch 15 | loss: 0.16462 | val_0_rmse: 0.39656 | val_1_rmse: 0.39753 |  0:00:58s
epoch 16 | loss: 0.16914 | val_0_rmse: 0.4172  | val_1_rmse: 0.40195 |  0:01:02s
epoch 17 | loss: 0.16382 | val_0_rmse: 0.41975 | val_1_rmse: 0.39222 |  0:01:05s
epoch 18 | loss: 0.16968 | val_0_rmse: 0.4296  | val_1_rmse: 0.42836 |  0:01:09s
epoch 19 | loss: 0.16771 | val_0_rmse: 0.40254 | val_1_rmse: 0.39767 |  0:01:13s
epoch 20 | loss: 0.16297 | val_0_rmse: 0.43246 | val_1_rmse: 0.42903 |  0:01:16s
epoch 21 | loss: 0.1647  | val_0_rmse: 0.40773 | val_1_rmse: 0.40724 |  0:01:20s
epoch 22 | loss: 0.16942 | val_0_rmse: 0.41209 | val_1_rmse: 0.39517 |  0:01:24s
epoch 23 | loss: 0.16659 | val_0_rmse: 0.4107  | val_1_rmse: 0.40634 |  0:01:27s
epoch 24 | loss: 0.15941 | val_0_rmse: 0.3977  | val_1_rmse: 0.3933  |  0:01:31s
epoch 25 | loss: 0.15987 | val_0_rmse: 0.39043 | val_1_rmse: 0.39257 |  0:01:35s
epoch 26 | loss: 0.16356 | val_0_rmse: 0.39963 | val_1_rmse: 0.40402 |  0:01:38s
epoch 27 | loss: 0.15946 | val_0_rmse: 0.39207 | val_1_rmse: 0.39457 |  0:01:42s
epoch 28 | loss: 0.16013 | val_0_rmse: 0.40706 | val_1_rmse: 0.40971 |  0:01:45s
epoch 29 | loss: 0.16328 | val_0_rmse: 0.38674 | val_1_rmse: 0.38959 |  0:01:49s
epoch 30 | loss: 0.16018 | val_0_rmse: 0.3843  | val_1_rmse: 0.38568 |  0:01:53s
epoch 31 | loss: 0.15669 | val_0_rmse: 0.41339 | val_1_rmse: 0.41432 |  0:01:56s
epoch 32 | loss: 0.16628 | val_0_rmse: 0.41807 | val_1_rmse: 0.41511 |  0:02:00s
epoch 33 | loss: 0.15873 | val_0_rmse: 0.38886 | val_1_rmse: 0.39298 |  0:02:04s
epoch 34 | loss: 0.15352 | val_0_rmse: 0.39689 | val_1_rmse: 0.39815 |  0:02:07s
epoch 35 | loss: 0.15369 | val_0_rmse: 0.40116 | val_1_rmse: 0.40453 |  0:02:11s
epoch 36 | loss: 0.14899 | val_0_rmse: 0.37855 | val_1_rmse: 0.38564 |  0:02:15s
epoch 37 | loss: 0.1544  | val_0_rmse: 0.39912 | val_1_rmse: 0.40062 |  0:02:18s
epoch 38 | loss: 0.14819 | val_0_rmse: 0.37178 | val_1_rmse: 0.37698 |  0:02:22s
epoch 39 | loss: 0.14775 | val_0_rmse: 0.38571 | val_1_rmse: 0.39157 |  0:02:26s
epoch 40 | loss: 0.14733 | val_0_rmse: 0.39621 | val_1_rmse: 0.39669 |  0:02:29s
epoch 41 | loss: 0.1484  | val_0_rmse: 0.37346 | val_1_rmse: 0.37858 |  0:02:33s
epoch 42 | loss: 0.14585 | val_0_rmse: 0.37488 | val_1_rmse: 0.38176 |  0:02:37s
epoch 43 | loss: 0.14475 | val_0_rmse: 0.3777  | val_1_rmse: 0.385   |  0:02:40s
epoch 44 | loss: 0.14824 | val_0_rmse: 0.38403 | val_1_rmse: 0.39152 |  0:02:44s
epoch 45 | loss: 0.1458  | val_0_rmse: 0.41164 | val_1_rmse: 0.41747 |  0:02:48s
epoch 46 | loss: 0.14427 | val_0_rmse: 0.37663 | val_1_rmse: 0.38008 |  0:02:51s
epoch 47 | loss: 0.14612 | val_0_rmse: 0.37927 | val_1_rmse: 0.38461 |  0:02:55s
epoch 48 | loss: 0.14093 | val_0_rmse: 0.37828 | val_1_rmse: 0.38887 |  0:02:59s
epoch 49 | loss: 0.14651 | val_0_rmse: 0.39496 | val_1_rmse: 0.40273 |  0:03:02s
epoch 50 | loss: 0.14254 | val_0_rmse: 0.38727 | val_1_rmse: 0.39199 |  0:03:06s
epoch 51 | loss: 0.1438  | val_0_rmse: 0.3716  | val_1_rmse: 0.37947 |  0:03:10s
epoch 52 | loss: 0.14212 | val_0_rmse: 0.42768 | val_1_rmse: 0.43912 |  0:03:13s
epoch 53 | loss: 0.13879 | val_0_rmse: 0.35773 | val_1_rmse: 0.36928 |  0:03:17s
epoch 54 | loss: 0.13787 | val_0_rmse: 0.37544 | val_1_rmse: 0.38491 |  0:03:21s
epoch 55 | loss: 0.13686 | val_0_rmse: 0.40358 | val_1_rmse: 0.4124  |  0:03:24s
epoch 56 | loss: 0.13907 | val_0_rmse: 0.38284 | val_1_rmse: 0.38398 |  0:03:28s
epoch 57 | loss: 0.13951 | val_0_rmse: 0.36295 | val_1_rmse: 0.37191 |  0:03:32s
epoch 58 | loss: 0.13801 | val_0_rmse: 0.38334 | val_1_rmse: 0.39355 |  0:03:35s
epoch 59 | loss: 0.14071 | val_0_rmse: 0.37031 | val_1_rmse: 0.37661 |  0:03:39s
epoch 60 | loss: 0.13981 | val_0_rmse: 0.35757 | val_1_rmse: 0.37097 |  0:03:42s
epoch 61 | loss: 0.14112 | val_0_rmse: 0.38292 | val_1_rmse: 0.39253 |  0:03:46s
epoch 62 | loss: 0.13726 | val_0_rmse: 0.391   | val_1_rmse: 0.39877 |  0:03:50s
epoch 63 | loss: 0.14156 | val_0_rmse: 0.40321 | val_1_rmse: 0.41312 |  0:03:53s
epoch 64 | loss: 0.13773 | val_0_rmse: 0.363   | val_1_rmse: 0.3736  |  0:03:57s
epoch 65 | loss: 0.13899 | val_0_rmse: 0.36923 | val_1_rmse: 0.3791  |  0:04:01s
epoch 66 | loss: 0.13729 | val_0_rmse: 0.35838 | val_1_rmse: 0.37198 |  0:04:04s
epoch 67 | loss: 0.13606 | val_0_rmse: 0.39284 | val_1_rmse: 0.40648 |  0:04:08s
epoch 68 | loss: 0.13436 | val_0_rmse: 0.3577  | val_1_rmse: 0.37236 |  0:04:12s
epoch 69 | loss: 0.135   | val_0_rmse: 0.39045 | val_1_rmse: 0.40615 |  0:04:15s
epoch 70 | loss: 0.13552 | val_0_rmse: 0.36762 | val_1_rmse: 0.38002 |  0:04:19s
epoch 71 | loss: 0.13475 | val_0_rmse: 0.35098 | val_1_rmse: 0.36621 |  0:04:23s
epoch 72 | loss: 0.13489 | val_0_rmse: 0.35366 | val_1_rmse: 0.37102 |  0:04:26s
epoch 73 | loss: 0.13567 | val_0_rmse: 0.36604 | val_1_rmse: 0.38178 |  0:04:30s
epoch 74 | loss: 0.13662 | val_0_rmse: 0.38597 | val_1_rmse: 0.40233 |  0:04:34s
epoch 75 | loss: 0.13444 | val_0_rmse: 0.39909 | val_1_rmse: 0.41252 |  0:04:37s
epoch 76 | loss: 0.13693 | val_0_rmse: 0.36954 | val_1_rmse: 0.38484 |  0:04:41s
epoch 77 | loss: 0.13637 | val_0_rmse: 0.37786 | val_1_rmse: 0.39326 |  0:04:45s
epoch 78 | loss: 0.13726 | val_0_rmse: 0.37786 | val_1_rmse: 0.39209 |  0:04:48s
epoch 79 | loss: 0.13482 | val_0_rmse: 0.36178 | val_1_rmse: 0.37336 |  0:04:52s
epoch 80 | loss: 0.13261 | val_0_rmse: 0.45624 | val_1_rmse: 0.4085  |  0:04:56s
epoch 81 | loss: 0.13022 | val_0_rmse: 0.43166 | val_1_rmse: 0.38377 |  0:04:59s
epoch 82 | loss: 0.13543 | val_0_rmse: 0.36638 | val_1_rmse: 0.37732 |  0:05:03s
epoch 83 | loss: 0.13087 | val_0_rmse: 0.38028 | val_1_rmse: 0.39593 |  0:05:07s
epoch 84 | loss: 0.13469 | val_0_rmse: 0.36969 | val_1_rmse: 0.38217 |  0:05:10s
epoch 85 | loss: 0.13687 | val_0_rmse: 0.38018 | val_1_rmse: 0.3938  |  0:05:14s
epoch 86 | loss: 0.13219 | val_0_rmse: 0.3626  | val_1_rmse: 0.381   |  0:05:18s
epoch 87 | loss: 0.13572 | val_0_rmse: 0.38061 | val_1_rmse: 0.3974  |  0:05:21s
epoch 88 | loss: 0.13303 | val_0_rmse: 0.39889 | val_1_rmse: 0.41382 |  0:05:25s
epoch 89 | loss: 0.13322 | val_0_rmse: 0.35823 | val_1_rmse: 0.37633 |  0:05:29s
epoch 90 | loss: 0.13314 | val_0_rmse: 0.36815 | val_1_rmse: 0.38293 |  0:05:32s
epoch 91 | loss: 0.1339  | val_0_rmse: 0.37622 | val_1_rmse: 0.39229 |  0:05:36s
epoch 92 | loss: 0.13028 | val_0_rmse: 0.37216 | val_1_rmse: 0.39074 |  0:05:39s
epoch 93 | loss: 0.13182 | val_0_rmse: 0.35896 | val_1_rmse: 0.37439 |  0:05:43s
epoch 94 | loss: 0.13009 | val_0_rmse: 0.34263 | val_1_rmse: 0.36061 |  0:05:47s
epoch 95 | loss: 0.12951 | val_0_rmse: 0.3548  | val_1_rmse: 0.37503 |  0:05:50s
epoch 96 | loss: 0.13051 | val_0_rmse: 0.34829 | val_1_rmse: 0.36519 |  0:05:54s
epoch 97 | loss: 0.13035 | val_0_rmse: 0.36702 | val_1_rmse: 0.36857 |  0:05:58s
epoch 98 | loss: 0.13372 | val_0_rmse: 0.61932 | val_1_rmse: 0.62577 |  0:06:01s
epoch 99 | loss: 0.15501 | val_0_rmse: 0.51184 | val_1_rmse: 0.52707 |  0:06:05s
epoch 100| loss: 0.15934 | val_0_rmse: 0.40346 | val_1_rmse: 0.40796 |  0:06:09s
epoch 101| loss: 0.1429  | val_0_rmse: 0.39845 | val_1_rmse: 0.37833 |  0:06:12s
epoch 102| loss: 0.13947 | val_0_rmse: 0.3602  | val_1_rmse: 0.37811 |  0:06:16s
epoch 103| loss: 0.13834 | val_0_rmse: 0.38574 | val_1_rmse: 0.37949 |  0:06:20s
epoch 104| loss: 0.13562 | val_0_rmse: 0.37798 | val_1_rmse: 0.39782 |  0:06:23s
epoch 105| loss: 0.13725 | val_0_rmse: 0.36271 | val_1_rmse: 0.38137 |  0:06:27s
epoch 106| loss: 0.13631 | val_0_rmse: 0.37703 | val_1_rmse: 0.396   |  0:06:31s
epoch 107| loss: 0.13258 | val_0_rmse: 0.35868 | val_1_rmse: 0.37776 |  0:06:34s
epoch 108| loss: 0.1318  | val_0_rmse: 0.36314 | val_1_rmse: 0.38104 |  0:06:38s
epoch 109| loss: 0.13323 | val_0_rmse: 0.37064 | val_1_rmse: 0.38213 |  0:06:42s
epoch 110| loss: 0.13165 | val_0_rmse: 0.3728  | val_1_rmse: 0.39013 |  0:06:45s
epoch 111| loss: 0.13191 | val_0_rmse: 0.34741 | val_1_rmse: 0.36739 |  0:06:49s
epoch 112| loss: 0.13052 | val_0_rmse: 0.35239 | val_1_rmse: 0.37084 |  0:06:53s
epoch 113| loss: 0.13455 | val_0_rmse: 0.35603 | val_1_rmse: 0.37422 |  0:06:56s
epoch 114| loss: 0.13285 | val_0_rmse: 0.35988 | val_1_rmse: 0.37902 |  0:07:00s
epoch 115| loss: 0.12906 | val_0_rmse: 0.35682 | val_1_rmse: 0.37598 |  0:07:04s
epoch 116| loss: 0.13187 | val_0_rmse: 0.36872 | val_1_rmse: 0.38852 |  0:07:07s
epoch 117| loss: 0.1283  | val_0_rmse: 0.3869  | val_1_rmse: 0.40404 |  0:07:11s
epoch 118| loss: 0.12733 | val_0_rmse: 0.35849 | val_1_rmse: 0.37811 |  0:07:15s
epoch 119| loss: 0.13319 | val_0_rmse: 0.4089  | val_1_rmse: 0.42506 |  0:07:18s
epoch 120| loss: 0.13586 | val_0_rmse: 0.36962 | val_1_rmse: 0.38689 |  0:07:22s
epoch 121| loss: 0.13611 | val_0_rmse: 0.39482 | val_1_rmse: 0.39567 |  0:07:26s
epoch 122| loss: 0.13445 | val_0_rmse: 0.36001 | val_1_rmse: 0.37524 |  0:07:29s
epoch 123| loss: 0.13114 | val_0_rmse: 0.36898 | val_1_rmse: 0.38601 |  0:07:33s
epoch 124| loss: 0.14458 | val_0_rmse: 0.39011 | val_1_rmse: 0.39847 |  0:07:36s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.36061
Best weights from best epoch are automatically used!
ended training at: 05:19:02
Feature importance:
[('Area', 0.0), ('Baths', 0.12228056576909221), ('Beds', 0.04853569558672626), ('Latitude', 0.22423277900421332), ('Longitude', 0.0), ('Month', 0.028846758423181736), ('Year', 0.22379585504130106), ('HF_BATHRM', 0.0), ('AC', 0.0), ('NUM_UNITS', 0.0), ('ROOMS', 1.6849500167805196e-06), ('AYB', 0.0), ('YR_RMDL', 0.08470598654097704), ('EYB', 0.0), ('STORIES', 0.016098672606194353), ('QUALIFIED', 3.4658000801309142e-06), ('SALE_NUM', 0.0), ('KITCHENS', 0.1958622507687511), ('FIREPLACES', 0.05317958201497568), ('SOURCE', 0.0), ('ZIPCODE', 0.0024567034944902887)]
Mean squared error is of 7539953552.755479
Mean absolute error:57999.65560031719
MAPE:0.23008717934538084
R2 score:0.8697808388080386
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:19:03
epoch 0  | loss: 0.61391 | val_0_rmse: 0.62375 | val_1_rmse: 0.59718 |  0:00:03s
epoch 1  | loss: 0.28916 | val_0_rmse: 0.53035 | val_1_rmse: 0.52602 |  0:00:07s
epoch 2  | loss: 0.25559 | val_0_rmse: 0.48429 | val_1_rmse: 0.48285 |  0:00:10s
epoch 3  | loss: 0.22087 | val_0_rmse: 0.44311 | val_1_rmse: 0.4426  |  0:00:14s
epoch 4  | loss: 0.20177 | val_0_rmse: 0.43553 | val_1_rmse: 0.43406 |  0:00:18s
epoch 5  | loss: 0.19038 | val_0_rmse: 0.42918 | val_1_rmse: 0.42671 |  0:00:21s
epoch 6  | loss: 0.18519 | val_0_rmse: 0.40989 | val_1_rmse: 0.41232 |  0:00:25s
epoch 7  | loss: 0.18497 | val_0_rmse: 0.40978 | val_1_rmse: 0.41372 |  0:00:29s
epoch 8  | loss: 0.17797 | val_0_rmse: 0.40858 | val_1_rmse: 0.40956 |  0:00:32s
epoch 9  | loss: 0.17906 | val_0_rmse: 0.41801 | val_1_rmse: 0.42069 |  0:00:36s
epoch 10 | loss: 0.17936 | val_0_rmse: 0.41466 | val_1_rmse: 0.41543 |  0:00:40s
epoch 11 | loss: 0.18399 | val_0_rmse: 0.40745 | val_1_rmse: 0.41109 |  0:00:43s
epoch 12 | loss: 0.17522 | val_0_rmse: 0.46062 | val_1_rmse: 0.46505 |  0:00:47s
epoch 13 | loss: 0.1868  | val_0_rmse: 0.40731 | val_1_rmse: 0.41199 |  0:00:51s
epoch 14 | loss: 0.18544 | val_0_rmse: 0.41264 | val_1_rmse: 0.41798 |  0:00:54s
epoch 15 | loss: 0.18588 | val_0_rmse: 0.42436 | val_1_rmse: 0.43081 |  0:00:58s
epoch 16 | loss: 0.18189 | val_0_rmse: 0.44725 | val_1_rmse: 0.45608 |  0:01:02s
epoch 17 | loss: 0.17284 | val_0_rmse: 0.39878 | val_1_rmse: 0.40171 |  0:01:05s
epoch 18 | loss: 0.17291 | val_0_rmse: 0.41275 | val_1_rmse: 0.41727 |  0:01:09s
epoch 19 | loss: 0.19639 | val_0_rmse: 0.4862  | val_1_rmse: 0.49158 |  0:01:12s
epoch 20 | loss: 0.18001 | val_0_rmse: 0.42994 | val_1_rmse: 0.43341 |  0:01:16s
epoch 21 | loss: 0.17424 | val_0_rmse: 0.39589 | val_1_rmse: 0.40188 |  0:01:20s
epoch 22 | loss: 0.17031 | val_0_rmse: 0.39915 | val_1_rmse: 0.40445 |  0:01:23s
epoch 23 | loss: 0.16898 | val_0_rmse: 0.42283 | val_1_rmse: 0.4258  |  0:01:27s
epoch 24 | loss: 0.17989 | val_0_rmse: 0.4355  | val_1_rmse: 0.43761 |  0:01:31s
epoch 25 | loss: 0.16777 | val_0_rmse: 0.38643 | val_1_rmse: 0.39107 |  0:01:35s
epoch 26 | loss: 0.16703 | val_0_rmse: 0.43527 | val_1_rmse: 0.44049 |  0:01:38s
epoch 27 | loss: 0.17409 | val_0_rmse: 0.43909 | val_1_rmse: 0.44111 |  0:01:42s
epoch 28 | loss: 0.17328 | val_0_rmse: 0.42751 | val_1_rmse: 0.4373  |  0:01:46s
epoch 29 | loss: 0.18757 | val_0_rmse: 0.40055 | val_1_rmse: 0.40163 |  0:01:49s
epoch 30 | loss: 0.16938 | val_0_rmse: 0.39219 | val_1_rmse: 0.39572 |  0:01:53s
epoch 31 | loss: 0.1641  | val_0_rmse: 0.39934 | val_1_rmse: 0.40548 |  0:01:56s
epoch 32 | loss: 0.17503 | val_0_rmse: 0.41646 | val_1_rmse: 0.42063 |  0:02:00s
epoch 33 | loss: 0.16245 | val_0_rmse: 0.38632 | val_1_rmse: 0.39112 |  0:02:04s
epoch 34 | loss: 0.15984 | val_0_rmse: 0.37788 | val_1_rmse: 0.38269 |  0:02:07s
epoch 35 | loss: 0.15728 | val_0_rmse: 0.38682 | val_1_rmse: 0.38934 |  0:02:11s
epoch 36 | loss: 0.15704 | val_0_rmse: 0.37765 | val_1_rmse: 0.38198 |  0:02:15s
epoch 37 | loss: 0.15807 | val_0_rmse: 0.38406 | val_1_rmse: 0.38809 |  0:02:18s
epoch 38 | loss: 0.15652 | val_0_rmse: 0.40531 | val_1_rmse: 0.41071 |  0:02:22s
epoch 39 | loss: 0.15718 | val_0_rmse: 0.39537 | val_1_rmse: 0.40152 |  0:02:26s
epoch 40 | loss: 0.15361 | val_0_rmse: 0.3779  | val_1_rmse: 0.38203 |  0:02:29s
epoch 41 | loss: 0.15455 | val_0_rmse: 0.37652 | val_1_rmse: 0.38189 |  0:02:33s
epoch 42 | loss: 0.15329 | val_0_rmse: 0.37975 | val_1_rmse: 0.38567 |  0:02:36s
epoch 43 | loss: 0.15142 | val_0_rmse: 0.37246 | val_1_rmse: 0.37807 |  0:02:39s
epoch 44 | loss: 0.15549 | val_0_rmse: 0.37186 | val_1_rmse: 0.3779  |  0:02:42s
epoch 45 | loss: 0.15189 | val_0_rmse: 0.37961 | val_1_rmse: 0.38599 |  0:02:46s
epoch 46 | loss: 0.14958 | val_0_rmse: 0.40262 | val_1_rmse: 0.40671 |  0:02:49s
epoch 47 | loss: 0.15098 | val_0_rmse: 0.39099 | val_1_rmse: 0.39785 |  0:02:52s
epoch 48 | loss: 0.15371 | val_0_rmse: 0.40521 | val_1_rmse: 0.41077 |  0:02:55s
epoch 49 | loss: 0.1547  | val_0_rmse: 0.38305 | val_1_rmse: 0.38806 |  0:02:59s
epoch 50 | loss: 0.15528 | val_0_rmse: 0.3714  | val_1_rmse: 0.37603 |  0:03:02s
epoch 51 | loss: 0.14728 | val_0_rmse: 0.38726 | val_1_rmse: 0.39156 |  0:03:05s
epoch 52 | loss: 0.15825 | val_0_rmse: 0.37216 | val_1_rmse: 0.37826 |  0:03:08s
epoch 53 | loss: 0.1504  | val_0_rmse: 0.38948 | val_1_rmse: 0.39799 |  0:03:11s
epoch 54 | loss: 0.15253 | val_0_rmse: 0.38219 | val_1_rmse: 0.39031 |  0:03:15s
epoch 55 | loss: 0.15569 | val_0_rmse: 0.38936 | val_1_rmse: 0.3948  |  0:03:18s
epoch 56 | loss: 0.14783 | val_0_rmse: 0.36566 | val_1_rmse: 0.37326 |  0:03:21s
epoch 57 | loss: 0.15015 | val_0_rmse: 0.36537 | val_1_rmse: 0.37406 |  0:03:24s
epoch 58 | loss: 0.14873 | val_0_rmse: 0.39398 | val_1_rmse: 0.40133 |  0:03:28s
epoch 59 | loss: 0.15098 | val_0_rmse: 0.3683  | val_1_rmse: 0.37503 |  0:03:31s
epoch 60 | loss: 0.14329 | val_0_rmse: 0.37978 | val_1_rmse: 0.38725 |  0:03:34s
epoch 61 | loss: 0.14657 | val_0_rmse: 0.36531 | val_1_rmse: 0.37243 |  0:03:37s
epoch 62 | loss: 0.14609 | val_0_rmse: 0.37393 | val_1_rmse: 0.38012 |  0:03:40s
epoch 63 | loss: 0.14372 | val_0_rmse: 0.38389 | val_1_rmse: 0.39231 |  0:03:44s
epoch 64 | loss: 0.14789 | val_0_rmse: 0.38378 | val_1_rmse: 0.38891 |  0:03:47s
epoch 65 | loss: 0.15029 | val_0_rmse: 0.41913 | val_1_rmse: 0.42393 |  0:03:50s
epoch 66 | loss: 0.15182 | val_0_rmse: 0.39725 | val_1_rmse: 0.37813 |  0:03:53s
epoch 67 | loss: 0.14494 | val_0_rmse: 0.38999 | val_1_rmse: 0.37969 |  0:03:56s
epoch 68 | loss: 0.14525 | val_0_rmse: 0.37915 | val_1_rmse: 0.3814  |  0:04:00s
epoch 69 | loss: 0.14277 | val_0_rmse: 0.37823 | val_1_rmse: 0.37035 |  0:04:03s
epoch 70 | loss: 0.14173 | val_0_rmse: 0.3975  | val_1_rmse: 0.38542 |  0:04:06s
epoch 71 | loss: 0.14925 | val_0_rmse: 0.39355 | val_1_rmse: 0.38435 |  0:04:09s
epoch 72 | loss: 0.14699 | val_0_rmse: 0.43224 | val_1_rmse: 0.43825 |  0:04:12s
epoch 73 | loss: 0.15021 | val_0_rmse: 0.40619 | val_1_rmse: 0.41338 |  0:04:16s
epoch 74 | loss: 0.14702 | val_0_rmse: 0.37456 | val_1_rmse: 0.38064 |  0:04:19s
epoch 75 | loss: 0.16328 | val_0_rmse: 0.39654 | val_1_rmse: 0.40541 |  0:04:22s
epoch 76 | loss: 0.1591  | val_0_rmse: 0.36602 | val_1_rmse: 0.37532 |  0:04:25s
epoch 77 | loss: 0.15478 | val_0_rmse: 0.37359 | val_1_rmse: 0.37851 |  0:04:29s
epoch 78 | loss: 0.1537  | val_0_rmse: 0.37553 | val_1_rmse: 0.38318 |  0:04:32s
epoch 79 | loss: 0.14688 | val_0_rmse: 0.40866 | val_1_rmse: 0.41508 |  0:04:35s
epoch 80 | loss: 0.15967 | val_0_rmse: 0.37738 | val_1_rmse: 0.38573 |  0:04:38s
epoch 81 | loss: 0.14626 | val_0_rmse: 0.37882 | val_1_rmse: 0.38811 |  0:04:41s
epoch 82 | loss: 0.17486 | val_0_rmse: 0.47208 | val_1_rmse: 0.46854 |  0:04:45s
epoch 83 | loss: 0.19948 | val_0_rmse: 0.43473 | val_1_rmse: 0.43475 |  0:04:48s
epoch 84 | loss: 0.16029 | val_0_rmse: 0.37691 | val_1_rmse: 0.38238 |  0:04:51s
epoch 85 | loss: 0.15397 | val_0_rmse: 0.38829 | val_1_rmse: 0.39725 |  0:04:54s
epoch 86 | loss: 0.15232 | val_0_rmse: 0.38878 | val_1_rmse: 0.39448 |  0:04:57s
epoch 87 | loss: 0.14932 | val_0_rmse: 0.37813 | val_1_rmse: 0.38502 |  0:05:01s
epoch 88 | loss: 0.15443 | val_0_rmse: 0.38471 | val_1_rmse: 0.39128 |  0:05:04s
epoch 89 | loss: 0.15996 | val_0_rmse: 0.40034 | val_1_rmse: 0.4002  |  0:05:07s
epoch 90 | loss: 0.17189 | val_0_rmse: 0.59077 | val_1_rmse: 0.60084 |  0:05:10s
epoch 91 | loss: 0.16797 | val_0_rmse: 0.3889  | val_1_rmse: 0.39749 |  0:05:13s
epoch 92 | loss: 0.14947 | val_0_rmse: 0.40027 | val_1_rmse: 0.40764 |  0:05:17s
epoch 93 | loss: 0.14803 | val_0_rmse: 0.53137 | val_1_rmse: 0.53145 |  0:05:20s
epoch 94 | loss: 0.16419 | val_0_rmse: 0.39254 | val_1_rmse: 0.39889 |  0:05:23s
epoch 95 | loss: 0.15133 | val_0_rmse: 0.37096 | val_1_rmse: 0.3751  |  0:05:26s
epoch 96 | loss: 0.14762 | val_0_rmse: 0.40157 | val_1_rmse: 0.38817 |  0:05:30s
epoch 97 | loss: 0.14606 | val_0_rmse: 0.40602 | val_1_rmse: 0.41048 |  0:05:33s
epoch 98 | loss: 0.1432  | val_0_rmse: 0.36237 | val_1_rmse: 0.36947 |  0:05:36s
epoch 99 | loss: 0.14408 | val_0_rmse: 0.37366 | val_1_rmse: 0.38083 |  0:05:39s
epoch 100| loss: 0.14284 | val_0_rmse: 0.5316  | val_1_rmse: 0.54402 |  0:05:42s
epoch 101| loss: 0.1674  | val_0_rmse: 0.38846 | val_1_rmse: 0.39163 |  0:05:46s
epoch 102| loss: 0.15136 | val_0_rmse: 0.37018 | val_1_rmse: 0.37557 |  0:05:49s
epoch 103| loss: 0.14419 | val_0_rmse: 0.3654  | val_1_rmse: 0.37258 |  0:05:52s
epoch 104| loss: 0.1481  | val_0_rmse: 0.36991 | val_1_rmse: 0.37625 |  0:05:55s
epoch 105| loss: 0.14342 | val_0_rmse: 0.36868 | val_1_rmse: 0.37529 |  0:05:58s
epoch 106| loss: 0.14113 | val_0_rmse: 0.36269 | val_1_rmse: 0.37034 |  0:06:02s
epoch 107| loss: 0.14036 | val_0_rmse: 0.36756 | val_1_rmse: 0.3764  |  0:06:05s
epoch 108| loss: 0.14048 | val_0_rmse: 0.35916 | val_1_rmse: 0.36784 |  0:06:08s
epoch 109| loss: 0.13942 | val_0_rmse: 0.3875  | val_1_rmse: 0.37999 |  0:06:11s
epoch 110| loss: 0.14194 | val_0_rmse: 0.36574 | val_1_rmse: 0.37343 |  0:06:14s
epoch 111| loss: 0.14003 | val_0_rmse: 0.37567 | val_1_rmse: 0.38503 |  0:06:18s
epoch 112| loss: 0.14401 | val_0_rmse: 0.37858 | val_1_rmse: 0.37199 |  0:06:21s
epoch 113| loss: 0.13899 | val_0_rmse: 0.36717 | val_1_rmse: 0.36536 |  0:06:24s
epoch 114| loss: 0.13792 | val_0_rmse: 0.40376 | val_1_rmse: 0.37245 |  0:06:27s
epoch 115| loss: 0.13831 | val_0_rmse: 0.40731 | val_1_rmse: 0.3688  |  0:06:31s
epoch 116| loss: 0.1356  | val_0_rmse: 0.3971  | val_1_rmse: 0.36911 |  0:06:34s
epoch 117| loss: 0.13729 | val_0_rmse: 0.36018 | val_1_rmse: 0.36962 |  0:06:37s
epoch 118| loss: 0.13539 | val_0_rmse: 0.35918 | val_1_rmse: 0.36686 |  0:06:40s
epoch 119| loss: 0.13979 | val_0_rmse: 0.4295  | val_1_rmse: 0.38366 |  0:06:43s
epoch 120| loss: 0.13794 | val_0_rmse: 0.40058 | val_1_rmse: 0.37067 |  0:06:47s
epoch 121| loss: 0.13734 | val_0_rmse: 0.38549 | val_1_rmse: 0.36629 |  0:06:50s
epoch 122| loss: 0.13741 | val_0_rmse: 0.40965 | val_1_rmse: 0.36799 |  0:06:53s
epoch 123| loss: 0.13895 | val_0_rmse: 0.40333 | val_1_rmse: 0.37144 |  0:06:56s
epoch 124| loss: 0.14398 | val_0_rmse: 0.3977  | val_1_rmse: 0.39946 |  0:06:59s
epoch 125| loss: 0.1755  | val_0_rmse: 0.41059 | val_1_rmse: 0.41824 |  0:07:03s
epoch 126| loss: 0.15518 | val_0_rmse: 0.39581 | val_1_rmse: 0.40371 |  0:07:06s
epoch 127| loss: 0.15027 | val_0_rmse: 0.37936 | val_1_rmse: 0.38436 |  0:07:09s
epoch 128| loss: 0.15194 | val_0_rmse: 0.39689 | val_1_rmse: 0.40414 |  0:07:12s
epoch 129| loss: 0.14889 | val_0_rmse: 0.40259 | val_1_rmse: 0.38674 |  0:07:15s
epoch 130| loss: 0.14309 | val_0_rmse: 0.40559 | val_1_rmse: 0.39633 |  0:07:19s
epoch 131| loss: 0.15039 | val_0_rmse: 0.39056 | val_1_rmse: 0.38116 |  0:07:22s
epoch 132| loss: 0.14446 | val_0_rmse: 0.37376 | val_1_rmse: 0.38274 |  0:07:25s
epoch 133| loss: 0.14074 | val_0_rmse: 0.37895 | val_1_rmse: 0.38782 |  0:07:28s
epoch 134| loss: 0.1434  | val_0_rmse: 0.39183 | val_1_rmse: 0.38602 |  0:07:31s
epoch 135| loss: 0.14859 | val_0_rmse: 0.46616 | val_1_rmse: 0.45248 |  0:07:35s
epoch 136| loss: 0.14929 | val_0_rmse: 0.40099 | val_1_rmse: 0.38598 |  0:07:38s
epoch 137| loss: 0.14455 | val_0_rmse: 0.40002 | val_1_rmse: 0.39987 |  0:07:41s
epoch 138| loss: 0.13916 | val_0_rmse: 0.39775 | val_1_rmse: 0.39796 |  0:07:44s
epoch 139| loss: 0.13773 | val_0_rmse: 0.3866  | val_1_rmse: 0.36878 |  0:07:48s
epoch 140| loss: 0.13964 | val_0_rmse: 0.39554 | val_1_rmse: 0.39394 |  0:07:51s
epoch 141| loss: 0.13677 | val_0_rmse: 0.37623 | val_1_rmse: 0.36613 |  0:07:54s
epoch 142| loss: 0.13733 | val_0_rmse: 0.3795  | val_1_rmse: 0.367   |  0:07:57s
epoch 143| loss: 0.13596 | val_0_rmse: 0.36938 | val_1_rmse: 0.37787 |  0:08:00s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.36536
Best weights from best epoch are automatically used!
ended training at: 05:27:05
Feature importance:
[('Area', 0.0), ('Baths', 0.022035697275526456), ('Beds', 2.3105001281482792e-07), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.3376638360618498), ('HF_BATHRM', 0.0), ('AC', 0.0), ('NUM_UNITS', 2.5426461593194023e-07), ('ROOMS', 0.012761237071410296), ('AYB', 0.0), ('YR_RMDL', 1.240428350316819e-05), ('EYB', 7.2500834009513105e-06), ('STORIES', 0.18600371861796208), ('QUALIFIED', 0.0), ('SALE_NUM', 0.11242987768859548), ('KITCHENS', 0.1651503950522146), ('FIREPLACES', 3.06815943364605e-05), ('SOURCE', 0.0), ('ZIPCODE', 0.1639044169565719)]
Mean squared error is of 7563393064.654733
Mean absolute error:59145.84038643676
MAPE:0.23099383019282374
R2 score:0.8670275205122077
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:27:05
epoch 0  | loss: 0.62192 | val_0_rmse: 0.62443 | val_1_rmse: 0.63262 |  0:00:03s
epoch 1  | loss: 0.29668 | val_0_rmse: 0.51162 | val_1_rmse: 0.52103 |  0:00:06s
epoch 2  | loss: 0.24194 | val_0_rmse: 0.47109 | val_1_rmse: 0.49719 |  0:00:09s
epoch 3  | loss: 0.22415 | val_0_rmse: 0.44854 | val_1_rmse: 0.48198 |  0:00:12s
epoch 4  | loss: 0.20334 | val_0_rmse: 0.46643 | val_1_rmse: 0.47807 |  0:00:16s
epoch 5  | loss: 0.19523 | val_0_rmse: 0.42074 | val_1_rmse: 0.455   |  0:00:19s
epoch 6  | loss: 0.18827 | val_0_rmse: 0.42181 | val_1_rmse: 0.43627 |  0:00:22s
epoch 7  | loss: 0.18577 | val_0_rmse: 0.41146 | val_1_rmse: 0.42389 |  0:00:25s
epoch 8  | loss: 0.18081 | val_0_rmse: 0.39945 | val_1_rmse: 0.43056 |  0:00:28s
epoch 9  | loss: 0.18475 | val_0_rmse: 0.43555 | val_1_rmse: 0.44795 |  0:00:32s
epoch 10 | loss: 0.18253 | val_0_rmse: 0.42029 | val_1_rmse: 0.43457 |  0:00:35s
epoch 11 | loss: 0.17653 | val_0_rmse: 0.4092  | val_1_rmse: 0.44161 |  0:00:38s
epoch 12 | loss: 0.17234 | val_0_rmse: 0.42365 | val_1_rmse: 0.44253 |  0:00:41s
epoch 13 | loss: 0.17277 | val_0_rmse: 0.40938 | val_1_rmse: 0.42334 |  0:00:44s
epoch 14 | loss: 0.16494 | val_0_rmse: 0.39124 | val_1_rmse: 0.41152 |  0:00:48s
epoch 15 | loss: 0.16313 | val_0_rmse: 0.39704 | val_1_rmse: 0.41498 |  0:00:51s
epoch 16 | loss: 0.15999 | val_0_rmse: 0.39194 | val_1_rmse: 0.40415 |  0:00:54s
epoch 17 | loss: 0.15881 | val_0_rmse: 0.45911 | val_1_rmse: 0.46959 |  0:00:57s
epoch 18 | loss: 0.15951 | val_0_rmse: 0.38565 | val_1_rmse: 0.40188 |  0:01:00s
epoch 19 | loss: 0.15833 | val_0_rmse: 0.38729 | val_1_rmse: 0.40286 |  0:01:04s
epoch 20 | loss: 0.15919 | val_0_rmse: 0.38502 | val_1_rmse: 0.40037 |  0:01:07s
epoch 21 | loss: 0.15874 | val_0_rmse: 0.37571 | val_1_rmse: 0.3939  |  0:01:10s
epoch 22 | loss: 0.15449 | val_0_rmse: 0.38995 | val_1_rmse: 0.40431 |  0:01:13s
epoch 23 | loss: 0.15619 | val_0_rmse: 0.37177 | val_1_rmse: 0.38892 |  0:01:17s
epoch 24 | loss: 0.14957 | val_0_rmse: 0.36914 | val_1_rmse: 0.38416 |  0:01:20s
epoch 25 | loss: 0.15204 | val_0_rmse: 0.38871 | val_1_rmse: 0.40258 |  0:01:23s
epoch 26 | loss: 0.15296 | val_0_rmse: 0.39925 | val_1_rmse: 0.41828 |  0:01:26s
epoch 27 | loss: 0.15173 | val_0_rmse: 0.37212 | val_1_rmse: 0.38771 |  0:01:29s
epoch 28 | loss: 0.15104 | val_0_rmse: 0.39749 | val_1_rmse: 0.41049 |  0:01:33s
epoch 29 | loss: 0.15086 | val_0_rmse: 0.38491 | val_1_rmse: 0.39895 |  0:01:36s
epoch 30 | loss: 0.14832 | val_0_rmse: 0.38808 | val_1_rmse: 0.40073 |  0:01:39s
epoch 31 | loss: 0.15031 | val_0_rmse: 0.38422 | val_1_rmse: 0.39725 |  0:01:42s
epoch 32 | loss: 0.14629 | val_0_rmse: 0.36774 | val_1_rmse: 0.39117 |  0:01:46s
epoch 33 | loss: 0.15051 | val_0_rmse: 0.40675 | val_1_rmse: 0.49882 |  0:01:49s
epoch 34 | loss: 0.15549 | val_0_rmse: 0.38629 | val_1_rmse: 0.40409 |  0:01:52s
epoch 35 | loss: 0.14766 | val_0_rmse: 0.37237 | val_1_rmse: 0.38689 |  0:01:55s
epoch 36 | loss: 0.14566 | val_0_rmse: 0.37041 | val_1_rmse: 0.39088 |  0:01:58s
epoch 37 | loss: 0.14336 | val_0_rmse: 0.38115 | val_1_rmse: 0.39411 |  0:02:02s
epoch 38 | loss: 0.14376 | val_0_rmse: 0.37394 | val_1_rmse: 0.38985 |  0:02:05s
epoch 39 | loss: 0.14478 | val_0_rmse: 0.35787 | val_1_rmse: 0.37564 |  0:02:08s
epoch 40 | loss: 0.14313 | val_0_rmse: 0.36673 | val_1_rmse: 0.38567 |  0:02:11s
epoch 41 | loss: 0.14745 | val_0_rmse: 0.36578 | val_1_rmse: 0.38627 |  0:02:14s
epoch 42 | loss: 0.15027 | val_0_rmse: 0.36547 | val_1_rmse: 0.38209 |  0:02:18s
epoch 43 | loss: 0.14379 | val_0_rmse: 0.36252 | val_1_rmse: 0.38329 |  0:02:21s
epoch 44 | loss: 0.14269 | val_0_rmse: 0.36393 | val_1_rmse: 0.38608 |  0:02:24s
epoch 45 | loss: 0.14111 | val_0_rmse: 0.37071 | val_1_rmse: 0.40123 |  0:02:27s
epoch 46 | loss: 0.14356 | val_0_rmse: 0.36637 | val_1_rmse: 0.42072 |  0:02:30s
epoch 47 | loss: 0.14087 | val_0_rmse: 0.38683 | val_1_rmse: 0.44408 |  0:02:34s
epoch 48 | loss: 0.14883 | val_0_rmse: 0.39884 | val_1_rmse: 0.41587 |  0:02:37s
epoch 49 | loss: 0.14437 | val_0_rmse: 0.36054 | val_1_rmse: 0.37906 |  0:02:40s
epoch 50 | loss: 0.14477 | val_0_rmse: 0.37632 | val_1_rmse: 0.39661 |  0:02:43s
epoch 51 | loss: 0.13953 | val_0_rmse: 0.36158 | val_1_rmse: 0.4004  |  0:02:47s
epoch 52 | loss: 0.14402 | val_0_rmse: 0.37976 | val_1_rmse: 0.40299 |  0:02:50s
epoch 53 | loss: 0.1424  | val_0_rmse: 0.35973 | val_1_rmse: 0.38713 |  0:02:53s
epoch 54 | loss: 0.13787 | val_0_rmse: 0.35795 | val_1_rmse: 0.40111 |  0:02:56s
epoch 55 | loss: 0.14063 | val_0_rmse: 0.39411 | val_1_rmse: 0.41679 |  0:02:59s
epoch 56 | loss: 0.14391 | val_0_rmse: 0.37263 | val_1_rmse: 0.42433 |  0:03:03s
epoch 57 | loss: 0.13858 | val_0_rmse: 0.37755 | val_1_rmse: 0.4368  |  0:03:06s
epoch 58 | loss: 0.14095 | val_0_rmse: 0.39462 | val_1_rmse: 0.66072 |  0:03:09s
epoch 59 | loss: 0.14325 | val_0_rmse: 0.40095 | val_1_rmse: 0.45248 |  0:03:12s
epoch 60 | loss: 0.14316 | val_0_rmse: 0.39129 | val_1_rmse: 0.59248 |  0:03:15s
epoch 61 | loss: 0.1361  | val_0_rmse: 0.36882 | val_1_rmse: 0.44767 |  0:03:19s
epoch 62 | loss: 0.13673 | val_0_rmse: 0.38083 | val_1_rmse: 0.45775 |  0:03:22s
epoch 63 | loss: 0.13793 | val_0_rmse: 0.38187 | val_1_rmse: 0.50061 |  0:03:25s
epoch 64 | loss: 0.14079 | val_0_rmse: 0.3697  | val_1_rmse: 0.407   |  0:03:28s
epoch 65 | loss: 0.13449 | val_0_rmse: 0.40635 | val_1_rmse: 1.18395 |  0:03:31s
epoch 66 | loss: 0.1365  | val_0_rmse: 0.3809  | val_1_rmse: 0.85326 |  0:03:35s
epoch 67 | loss: 0.13515 | val_0_rmse: 0.37631 | val_1_rmse: 0.60568 |  0:03:38s
epoch 68 | loss: 0.13427 | val_0_rmse: 0.36892 | val_1_rmse: 0.39111 |  0:03:41s
epoch 69 | loss: 0.14183 | val_0_rmse: 0.38935 | val_1_rmse: 0.66293 |  0:03:44s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.37564
Best weights from best epoch are automatically used!
ended training at: 05:30:51
Feature importance:
[('Area', 2.3697675020042436e-05), ('Baths', 0.07127449812308097), ('Beds', 0.03674629413898278), ('Latitude', 0.08047171439170002), ('Longitude', 0.11891646326999634), ('Month', 1.6806827639843504e-06), ('Year', 0.23985752873184105), ('HF_BATHRM', 0.11053651100385839), ('AC', 0.0044417780170560904), ('NUM_UNITS', 0.06436993615505054), ('ROOMS', 0.046834725758292464), ('AYB', 0.052949091725712855), ('YR_RMDL', 5.3079336414609404e-05), ('EYB', 0.005989182365841641), ('STORIES', 0.0), ('QUALIFIED', 0.0078666628642631), ('SALE_NUM', 2.5800304791782462e-06), ('KITCHENS', 0.10891635048195462), ('FIREPLACES', 0.05074822524769133), ('SOURCE', 0.0), ('ZIPCODE', 0.0)]
Mean squared error is of 7901794437.05942
Mean absolute error:59973.037618570954
MAPE:0.2326965716045784
R2 score:0.8629632922398506
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:30:51
epoch 0  | loss: 0.61328 | val_0_rmse: 0.64453 | val_1_rmse: 0.64875 |  0:00:03s
epoch 1  | loss: 0.31571 | val_0_rmse: 0.51038 | val_1_rmse: 0.51862 |  0:00:06s
epoch 2  | loss: 0.24705 | val_0_rmse: 0.49986 | val_1_rmse: 0.50342 |  0:00:09s
epoch 3  | loss: 0.21263 | val_0_rmse: 0.4725  | val_1_rmse: 0.47352 |  0:00:12s
epoch 4  | loss: 0.19601 | val_0_rmse: 0.44467 | val_1_rmse: 0.44197 |  0:00:16s
epoch 5  | loss: 0.18474 | val_0_rmse: 0.41313 | val_1_rmse: 0.41828 |  0:00:19s
epoch 6  | loss: 0.18122 | val_0_rmse: 0.42868 | val_1_rmse: 0.42859 |  0:00:22s
epoch 7  | loss: 0.18537 | val_0_rmse: 0.50243 | val_1_rmse: 0.51066 |  0:00:25s
epoch 8  | loss: 0.19859 | val_0_rmse: 0.40872 | val_1_rmse: 0.41419 |  0:00:28s
epoch 9  | loss: 0.1778  | val_0_rmse: 0.41527 | val_1_rmse: 0.41855 |  0:00:32s
epoch 10 | loss: 0.17445 | val_0_rmse: 0.39317 | val_1_rmse: 0.39868 |  0:00:35s
epoch 11 | loss: 0.16721 | val_0_rmse: 0.43322 | val_1_rmse: 0.44324 |  0:00:38s
epoch 12 | loss: 0.16652 | val_0_rmse: 0.42957 | val_1_rmse: 0.43449 |  0:00:41s
epoch 13 | loss: 0.16326 | val_0_rmse: 0.40434 | val_1_rmse: 0.41033 |  0:00:45s
epoch 14 | loss: 0.16575 | val_0_rmse: 0.38604 | val_1_rmse: 0.3922  |  0:00:48s
epoch 15 | loss: 0.16078 | val_0_rmse: 0.38596 | val_1_rmse: 0.39326 |  0:00:51s
epoch 16 | loss: 0.16121 | val_0_rmse: 0.39862 | val_1_rmse: 0.40428 |  0:00:54s
epoch 17 | loss: 0.16104 | val_0_rmse: 0.38497 | val_1_rmse: 0.39477 |  0:00:57s
epoch 18 | loss: 0.15765 | val_0_rmse: 0.38298 | val_1_rmse: 0.38699 |  0:01:01s
epoch 19 | loss: 0.16028 | val_0_rmse: 0.4065  | val_1_rmse: 0.41282 |  0:01:04s
epoch 20 | loss: 0.15871 | val_0_rmse: 0.39015 | val_1_rmse: 0.39858 |  0:01:07s
epoch 21 | loss: 0.15632 | val_0_rmse: 0.38888 | val_1_rmse: 0.39642 |  0:01:10s
epoch 22 | loss: 0.15759 | val_0_rmse: 0.38651 | val_1_rmse: 0.39656 |  0:01:13s
epoch 23 | loss: 0.15522 | val_0_rmse: 0.37327 | val_1_rmse: 0.38045 |  0:01:17s
epoch 24 | loss: 0.15306 | val_0_rmse: 0.38021 | val_1_rmse: 0.38785 |  0:01:20s
epoch 25 | loss: 0.1555  | val_0_rmse: 0.40167 | val_1_rmse: 0.40762 |  0:01:23s
epoch 26 | loss: 0.15285 | val_0_rmse: 0.37443 | val_1_rmse: 0.38353 |  0:01:26s
epoch 27 | loss: 0.15194 | val_0_rmse: 0.37665 | val_1_rmse: 0.38735 |  0:01:29s
epoch 28 | loss: 0.15329 | val_0_rmse: 0.41563 | val_1_rmse: 0.4287  |  0:01:33s
epoch 29 | loss: 0.15026 | val_0_rmse: 0.37098 | val_1_rmse: 0.38062 |  0:01:36s
epoch 30 | loss: 0.14936 | val_0_rmse: 0.3647  | val_1_rmse: 0.37713 |  0:01:39s
epoch 31 | loss: 0.14949 | val_0_rmse: 0.4107  | val_1_rmse: 0.42209 |  0:01:42s
epoch 32 | loss: 0.16804 | val_0_rmse: 0.38596 | val_1_rmse: 0.39719 |  0:01:46s
epoch 33 | loss: 0.15609 | val_0_rmse: 0.37335 | val_1_rmse: 0.38229 |  0:01:49s
epoch 34 | loss: 0.14771 | val_0_rmse: 0.36753 | val_1_rmse: 0.37762 |  0:01:52s
epoch 35 | loss: 0.14601 | val_0_rmse: 0.37522 | val_1_rmse: 0.38679 |  0:01:55s
epoch 36 | loss: 0.15333 | val_0_rmse: 0.39949 | val_1_rmse: 0.41011 |  0:01:58s
epoch 37 | loss: 0.15539 | val_0_rmse: 0.38228 | val_1_rmse: 0.38791 |  0:02:02s
epoch 38 | loss: 0.15076 | val_0_rmse: 0.37604 | val_1_rmse: 0.3854  |  0:02:05s
epoch 39 | loss: 0.15421 | val_0_rmse: 0.39524 | val_1_rmse: 0.40324 |  0:02:08s
epoch 40 | loss: 0.15594 | val_0_rmse: 0.38227 | val_1_rmse: 0.39123 |  0:02:11s
epoch 41 | loss: 0.15337 | val_0_rmse: 0.36953 | val_1_rmse: 0.37848 |  0:02:14s
epoch 42 | loss: 0.14919 | val_0_rmse: 0.36479 | val_1_rmse: 0.37558 |  0:02:18s
epoch 43 | loss: 0.14759 | val_0_rmse: 0.37244 | val_1_rmse: 0.38372 |  0:02:21s
epoch 44 | loss: 0.14491 | val_0_rmse: 0.35842 | val_1_rmse: 0.3707  |  0:02:24s
epoch 45 | loss: 0.14242 | val_0_rmse: 0.38126 | val_1_rmse: 0.39215 |  0:02:27s
epoch 46 | loss: 0.14209 | val_0_rmse: 0.35698 | val_1_rmse: 0.3701  |  0:02:30s
epoch 47 | loss: 0.13884 | val_0_rmse: 0.38472 | val_1_rmse: 0.39901 |  0:02:34s
epoch 48 | loss: 0.14411 | val_0_rmse: 0.38726 | val_1_rmse: 0.39871 |  0:02:37s
epoch 49 | loss: 0.14256 | val_0_rmse: 0.36551 | val_1_rmse: 0.38253 |  0:02:40s
epoch 50 | loss: 0.14103 | val_0_rmse: 0.35719 | val_1_rmse: 0.37488 |  0:02:43s
epoch 51 | loss: 0.14338 | val_0_rmse: 0.3625  | val_1_rmse: 0.37716 |  0:02:47s
epoch 52 | loss: 0.14097 | val_0_rmse: 0.35712 | val_1_rmse: 0.37222 |  0:02:50s
epoch 53 | loss: 0.14459 | val_0_rmse: 0.36986 | val_1_rmse: 0.38528 |  0:02:53s
epoch 54 | loss: 0.14278 | val_0_rmse: 0.37302 | val_1_rmse: 0.38975 |  0:02:56s
epoch 55 | loss: 0.13772 | val_0_rmse: 0.3769  | val_1_rmse: 0.39518 |  0:02:59s
epoch 56 | loss: 0.13647 | val_0_rmse: 0.36807 | val_1_rmse: 0.38367 |  0:03:03s
epoch 57 | loss: 0.13832 | val_0_rmse: 0.36917 | val_1_rmse: 0.38378 |  0:03:06s
epoch 58 | loss: 0.14198 | val_0_rmse: 0.35314 | val_1_rmse: 0.36893 |  0:03:09s
epoch 59 | loss: 0.14005 | val_0_rmse: 0.35115 | val_1_rmse: 0.36791 |  0:03:12s
epoch 60 | loss: 0.14123 | val_0_rmse: 0.35432 | val_1_rmse: 0.37164 |  0:03:15s
epoch 61 | loss: 0.14053 | val_0_rmse: 0.35492 | val_1_rmse: 0.37316 |  0:03:19s
epoch 62 | loss: 0.15214 | val_0_rmse: 0.38294 | val_1_rmse: 0.394   |  0:03:22s
epoch 63 | loss: 0.14341 | val_0_rmse: 0.36892 | val_1_rmse: 0.38581 |  0:03:25s
epoch 64 | loss: 0.14299 | val_0_rmse: 0.35933 | val_1_rmse: 0.37575 |  0:03:28s
epoch 65 | loss: 0.13944 | val_0_rmse: 0.38611 | val_1_rmse: 0.4049  |  0:03:32s
epoch 66 | loss: 0.15296 | val_0_rmse: 0.37307 | val_1_rmse: 0.38813 |  0:03:35s
epoch 67 | loss: 0.14623 | val_0_rmse: 0.36262 | val_1_rmse: 0.37994 |  0:03:38s
epoch 68 | loss: 0.14134 | val_0_rmse: 0.36412 | val_1_rmse: 0.37989 |  0:03:41s
epoch 69 | loss: 0.14184 | val_0_rmse: 0.36462 | val_1_rmse: 0.38082 |  0:03:44s
epoch 70 | loss: 0.14499 | val_0_rmse: 0.43079 | val_1_rmse: 0.45246 |  0:03:48s
epoch 71 | loss: 0.17288 | val_0_rmse: 0.423   | val_1_rmse: 0.43225 |  0:03:51s
epoch 72 | loss: 0.15068 | val_0_rmse: 0.36962 | val_1_rmse: 0.38225 |  0:03:54s
epoch 73 | loss: 0.14833 | val_0_rmse: 0.37014 | val_1_rmse: 0.38402 |  0:03:57s
epoch 74 | loss: 0.14527 | val_0_rmse: 0.37168 | val_1_rmse: 0.3846  |  0:04:01s
epoch 75 | loss: 0.14943 | val_0_rmse: 0.4192  | val_1_rmse: 0.42973 |  0:04:04s
epoch 76 | loss: 0.14801 | val_0_rmse: 0.36948 | val_1_rmse: 0.3878  |  0:04:07s
epoch 77 | loss: 0.14583 | val_0_rmse: 0.36647 | val_1_rmse: 0.3851  |  0:04:10s
epoch 78 | loss: 0.14386 | val_0_rmse: 0.42606 | val_1_rmse: 0.43781 |  0:04:13s
epoch 79 | loss: 0.1539  | val_0_rmse: 0.37412 | val_1_rmse: 0.38755 |  0:04:17s
epoch 80 | loss: 0.16071 | val_0_rmse: 0.39767 | val_1_rmse: 0.40876 |  0:04:20s
epoch 81 | loss: 0.15902 | val_0_rmse: 0.48975 | val_1_rmse: 0.50336 |  0:04:23s
epoch 82 | loss: 0.16152 | val_0_rmse: 0.36887 | val_1_rmse: 0.38305 |  0:04:26s
epoch 83 | loss: 0.15251 | val_0_rmse: 0.38532 | val_1_rmse: 0.39528 |  0:04:30s
epoch 84 | loss: 0.16572 | val_0_rmse: 0.50062 | val_1_rmse: 0.51785 |  0:04:33s
epoch 85 | loss: 0.1924  | val_0_rmse: 0.43543 | val_1_rmse: 0.45194 |  0:04:36s
epoch 86 | loss: 0.16307 | val_0_rmse: 0.38361 | val_1_rmse: 0.39596 |  0:04:39s
epoch 87 | loss: 0.15244 | val_0_rmse: 0.40324 | val_1_rmse: 0.41513 |  0:04:43s
epoch 88 | loss: 0.16069 | val_0_rmse: 0.39068 | val_1_rmse: 0.39834 |  0:04:46s
epoch 89 | loss: 0.1554  | val_0_rmse: 0.38526 | val_1_rmse: 0.39711 |  0:04:49s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.36791
Best weights from best epoch are automatically used!
ended training at: 05:35:41
Feature importance:
[('Area', 0.022537947822451852), ('Baths', 0.2001499307241163), ('Beds', 0.03454102272307066), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.0013124745439894074), ('Year', 0.25616124107281846), ('HF_BATHRM', 0.0), ('AC', 0.0), ('NUM_UNITS', 0.028552332463135894), ('ROOMS', 0.24911456243593272), ('AYB', 0.18614236129000863), ('YR_RMDL', 1.536861747694508e-07), ('EYB', 2.762224432104994e-06), ('STORIES', 0.011490950212130788), ('QUALIFIED', 0.0025301165930898583), ('SALE_NUM', 0.0), ('KITCHENS', 0.0), ('FIREPLACES', 0.0), ('SOURCE', 0.0019586978403483175), ('ZIPCODE', 0.0055054463683002364)]
Mean squared error is of 25379259932.321064
Mean absolute error:59225.476388963834
MAPE:0.231778892418714
R2 score:0.5642072109087067
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:35:42
epoch 0  | loss: 0.57909 | val_0_rmse: 0.71847 | val_1_rmse: 0.70779 |  0:00:03s
epoch 1  | loss: 0.31628 | val_0_rmse: 0.66388 | val_1_rmse: 0.65437 |  0:00:06s
epoch 2  | loss: 0.26226 | val_0_rmse: 0.66997 | val_1_rmse: 0.65856 |  0:00:09s
epoch 3  | loss: 0.22396 | val_0_rmse: 0.53943 | val_1_rmse: 0.53207 |  0:00:12s
epoch 4  | loss: 0.20516 | val_0_rmse: 0.5517  | val_1_rmse: 0.54427 |  0:00:16s
epoch 5  | loss: 0.19726 | val_0_rmse: 0.56818 | val_1_rmse: 0.56249 |  0:00:19s
epoch 6  | loss: 0.1942  | val_0_rmse: 0.48754 | val_1_rmse: 0.48139 |  0:00:22s
epoch 7  | loss: 0.1845  | val_0_rmse: 0.48707 | val_1_rmse: 0.48244 |  0:00:25s
epoch 8  | loss: 0.18383 | val_0_rmse: 0.47874 | val_1_rmse: 0.47607 |  0:00:28s
epoch 9  | loss: 0.18116 | val_0_rmse: 0.49847 | val_1_rmse: 0.49376 |  0:00:32s
epoch 10 | loss: 0.18081 | val_0_rmse: 0.46714 | val_1_rmse: 0.46606 |  0:00:35s
epoch 11 | loss: 0.17721 | val_0_rmse: 0.48009 | val_1_rmse: 0.47652 |  0:00:38s
epoch 12 | loss: 0.17262 | val_0_rmse: 0.43978 | val_1_rmse: 0.43533 |  0:00:41s
epoch 13 | loss: 0.17659 | val_0_rmse: 0.45804 | val_1_rmse: 0.45645 |  0:00:45s
epoch 14 | loss: 0.16935 | val_0_rmse: 0.43283 | val_1_rmse: 0.43119 |  0:00:48s
epoch 15 | loss: 0.17526 | val_0_rmse: 0.45066 | val_1_rmse: 0.44831 |  0:00:51s
epoch 16 | loss: 0.16869 | val_0_rmse: 0.41377 | val_1_rmse: 0.40814 |  0:00:54s
epoch 17 | loss: 0.17992 | val_0_rmse: 0.41746 | val_1_rmse: 0.41625 |  0:00:57s
epoch 18 | loss: 0.17419 | val_0_rmse: 0.42117 | val_1_rmse: 0.41798 |  0:01:01s
epoch 19 | loss: 0.17072 | val_0_rmse: 0.42686 | val_1_rmse: 0.42347 |  0:01:04s
epoch 20 | loss: 0.16222 | val_0_rmse: 0.40764 | val_1_rmse: 0.4041  |  0:01:07s
epoch 21 | loss: 0.16071 | val_0_rmse: 0.41846 | val_1_rmse: 0.41834 |  0:01:10s
epoch 22 | loss: 0.16091 | val_0_rmse: 0.41033 | val_1_rmse: 0.40986 |  0:01:13s
epoch 23 | loss: 0.15969 | val_0_rmse: 0.39966 | val_1_rmse: 0.40147 |  0:01:17s
epoch 24 | loss: 0.1611  | val_0_rmse: 0.3961  | val_1_rmse: 0.38968 |  0:01:20s
epoch 25 | loss: 0.15826 | val_0_rmse: 0.4007  | val_1_rmse: 0.39856 |  0:01:23s
epoch 26 | loss: 0.16046 | val_0_rmse: 0.39224 | val_1_rmse: 0.38949 |  0:01:26s
epoch 27 | loss: 0.16089 | val_0_rmse: 0.41401 | val_1_rmse: 0.41333 |  0:01:30s
epoch 28 | loss: 0.1566  | val_0_rmse: 0.38704 | val_1_rmse: 0.38486 |  0:01:33s
epoch 29 | loss: 0.16331 | val_0_rmse: 0.43328 | val_1_rmse: 0.43259 |  0:01:36s
epoch 30 | loss: 0.16374 | val_0_rmse: 0.41803 | val_1_rmse: 0.41356 |  0:01:39s
epoch 31 | loss: 0.15857 | val_0_rmse: 0.39248 | val_1_rmse: 0.39392 |  0:01:42s
epoch 32 | loss: 0.15131 | val_0_rmse: 0.3958  | val_1_rmse: 0.39491 |  0:01:46s
epoch 33 | loss: 0.15206 | val_0_rmse: 0.3958  | val_1_rmse: 0.39185 |  0:01:49s
epoch 34 | loss: 0.15179 | val_0_rmse: 0.37969 | val_1_rmse: 0.37907 |  0:01:52s
epoch 35 | loss: 0.15144 | val_0_rmse: 0.39051 | val_1_rmse: 0.39037 |  0:01:55s
epoch 36 | loss: 0.15022 | val_0_rmse: 0.39509 | val_1_rmse: 0.3967  |  0:01:59s
epoch 37 | loss: 0.15052 | val_0_rmse: 0.38589 | val_1_rmse: 0.38887 |  0:02:02s
epoch 38 | loss: 0.15096 | val_0_rmse: 0.37876 | val_1_rmse: 0.37796 |  0:02:05s
epoch 39 | loss: 0.15002 | val_0_rmse: 0.39937 | val_1_rmse: 0.3832  |  0:02:08s
epoch 40 | loss: 0.14911 | val_0_rmse: 0.4103  | val_1_rmse: 0.38067 |  0:02:11s
epoch 41 | loss: 0.14508 | val_0_rmse: 0.4328  | val_1_rmse: 0.40821 |  0:02:15s
epoch 42 | loss: 0.15373 | val_0_rmse: 0.4266  | val_1_rmse: 0.37964 |  0:02:18s
epoch 43 | loss: 0.14591 | val_0_rmse: 0.38725 | val_1_rmse: 0.38074 |  0:02:21s
epoch 44 | loss: 0.14704 | val_0_rmse: 0.41454 | val_1_rmse: 0.38632 |  0:02:24s
epoch 45 | loss: 0.14818 | val_0_rmse: 0.38989 | val_1_rmse: 0.39203 |  0:02:27s
epoch 46 | loss: 0.14584 | val_0_rmse: 0.39682 | val_1_rmse: 0.39863 |  0:02:31s
epoch 47 | loss: 0.15154 | val_0_rmse: 0.38582 | val_1_rmse: 0.38551 |  0:02:34s
epoch 48 | loss: 0.14782 | val_0_rmse: 0.37988 | val_1_rmse: 0.38361 |  0:02:37s
epoch 49 | loss: 0.14157 | val_0_rmse: 0.3885  | val_1_rmse: 0.39351 |  0:02:40s
epoch 50 | loss: 0.15164 | val_0_rmse: 0.39131 | val_1_rmse: 0.39156 |  0:02:43s
epoch 51 | loss: 0.14277 | val_0_rmse: 0.39536 | val_1_rmse: 0.39574 |  0:02:47s
epoch 52 | loss: 0.14186 | val_0_rmse: 0.37861 | val_1_rmse: 0.37905 |  0:02:50s
epoch 53 | loss: 0.14445 | val_0_rmse: 0.39693 | val_1_rmse: 0.3955  |  0:02:53s
epoch 54 | loss: 0.15669 | val_0_rmse: 0.38017 | val_1_rmse: 0.38506 |  0:02:56s
epoch 55 | loss: 0.13983 | val_0_rmse: 0.41166 | val_1_rmse: 0.415   |  0:02:59s
epoch 56 | loss: 0.16433 | val_0_rmse: 0.40793 | val_1_rmse: 0.41273 |  0:03:03s
epoch 57 | loss: 0.16789 | val_0_rmse: 0.41587 | val_1_rmse: 0.41583 |  0:03:06s
epoch 58 | loss: 0.16288 | val_0_rmse: 0.37516 | val_1_rmse: 0.37866 |  0:03:09s
epoch 59 | loss: 0.15238 | val_0_rmse: 0.41939 | val_1_rmse: 0.42118 |  0:03:12s
epoch 60 | loss: 0.15379 | val_0_rmse: 0.41007 | val_1_rmse: 0.40724 |  0:03:16s
epoch 61 | loss: 0.17685 | val_0_rmse: 0.40718 | val_1_rmse: 0.40502 |  0:03:19s
epoch 62 | loss: 0.15551 | val_0_rmse: 0.41867 | val_1_rmse: 0.41897 |  0:03:22s
epoch 63 | loss: 0.15024 | val_0_rmse: 0.41147 | val_1_rmse: 0.41022 |  0:03:25s
epoch 64 | loss: 0.14792 | val_0_rmse: 0.37641 | val_1_rmse: 0.38082 |  0:03:28s
epoch 65 | loss: 0.15214 | val_0_rmse: 0.37595 | val_1_rmse: 0.37432 |  0:03:32s
epoch 66 | loss: 0.15088 | val_0_rmse: 0.39199 | val_1_rmse: 0.38796 |  0:03:35s
epoch 67 | loss: 0.1437  | val_0_rmse: 0.38443 | val_1_rmse: 0.38638 |  0:03:38s
epoch 68 | loss: 0.1437  | val_0_rmse: 0.39978 | val_1_rmse: 0.38099 |  0:03:41s
epoch 69 | loss: 0.13854 | val_0_rmse: 0.39255 | val_1_rmse: 0.38293 |  0:03:44s
epoch 70 | loss: 0.14224 | val_0_rmse: 0.50576 | val_1_rmse: 0.44527 |  0:03:48s
epoch 71 | loss: 0.14443 | val_0_rmse: 0.41537 | val_1_rmse: 0.40921 |  0:03:51s
epoch 72 | loss: 0.1376  | val_0_rmse: 0.38457 | val_1_rmse: 0.38927 |  0:03:54s
epoch 73 | loss: 0.13839 | val_0_rmse: 0.41582 | val_1_rmse: 0.41591 |  0:03:57s
epoch 74 | loss: 0.13933 | val_0_rmse: 0.42083 | val_1_rmse: 0.4251  |  0:04:00s
epoch 75 | loss: 0.14139 | val_0_rmse: 0.41689 | val_1_rmse: 0.41876 |  0:04:04s
epoch 76 | loss: 0.13721 | val_0_rmse: 0.47088 | val_1_rmse: 0.4747  |  0:04:07s
epoch 77 | loss: 0.13926 | val_0_rmse: 0.39951 | val_1_rmse: 0.40667 |  0:04:10s
epoch 78 | loss: 0.14196 | val_0_rmse: 0.38313 | val_1_rmse: 0.38983 |  0:04:13s
epoch 79 | loss: 0.13505 | val_0_rmse: 0.43352 | val_1_rmse: 0.44109 |  0:04:17s
epoch 80 | loss: 0.13606 | val_0_rmse: 0.42592 | val_1_rmse: 0.43044 |  0:04:20s
epoch 81 | loss: 0.13502 | val_0_rmse: 0.37036 | val_1_rmse: 0.37919 |  0:04:23s
epoch 82 | loss: 0.13936 | val_0_rmse: 0.38853 | val_1_rmse: 0.39475 |  0:04:26s
epoch 83 | loss: 0.13439 | val_0_rmse: 0.37786 | val_1_rmse: 0.38181 |  0:04:29s
epoch 84 | loss: 0.133   | val_0_rmse: 0.38742 | val_1_rmse: 0.396   |  0:04:33s
epoch 85 | loss: 0.13369 | val_0_rmse: 0.40737 | val_1_rmse: 0.4151  |  0:04:36s
epoch 86 | loss: 0.13417 | val_0_rmse: 0.39042 | val_1_rmse: 0.40089 |  0:04:39s
epoch 87 | loss: 0.13518 | val_0_rmse: 0.41136 | val_1_rmse: 0.40565 |  0:04:42s
epoch 88 | loss: 0.13285 | val_0_rmse: 0.4142  | val_1_rmse: 0.41938 |  0:04:45s
epoch 89 | loss: 0.1317  | val_0_rmse: 0.40096 | val_1_rmse: 0.41016 |  0:04:49s
epoch 90 | loss: 0.13024 | val_0_rmse: 0.39114 | val_1_rmse: 0.40201 |  0:04:52s
epoch 91 | loss: 0.1316  | val_0_rmse: 0.45467 | val_1_rmse: 0.4612  |  0:04:55s
epoch 92 | loss: 0.13533 | val_0_rmse: 0.39926 | val_1_rmse: 0.40902 |  0:04:58s
epoch 93 | loss: 0.12958 | val_0_rmse: 0.4681  | val_1_rmse: 0.4734  |  0:05:02s
epoch 94 | loss: 0.13044 | val_0_rmse: 0.49484 | val_1_rmse: 0.49841 |  0:05:05s
epoch 95 | loss: 0.12963 | val_0_rmse: 0.4819  | val_1_rmse: 0.48675 |  0:05:08s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.37432
Best weights from best epoch are automatically used!
ended training at: 05:40:51
Feature importance:
[('Area', 0.0), ('Baths', 0.07525357732176832), ('Beds', 0.1021349014427452), ('Latitude', 0.056962498816741265), ('Longitude', 3.993653510715971e-05), ('Month', 0.013286801744963739), ('Year', 0.18020445463065515), ('HF_BATHRM', 0.0), ('AC', 0.0), ('NUM_UNITS', 0.0), ('ROOMS', 0.0987812628647961), ('AYB', 0.17435150228264007), ('YR_RMDL', 2.7257060045321427e-06), ('EYB', 0.0), ('STORIES', 0.0), ('QUALIFIED', 0.0), ('SALE_NUM', 4.5192906287231455e-06), ('KITCHENS', 0.0), ('FIREPLACES', 0.1989300107777014), ('SOURCE', 0.10003757545597344), ('ZIPCODE', 1.0233130274871586e-05)]
Mean squared error is of 8248348653.792754
Mean absolute error:64424.39736706103
MAPE:0.30286747661101027
R2 score:0.858014062631226
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:40:51
epoch 0  | loss: 0.6421  | val_0_rmse: 0.70235 | val_1_rmse: 0.69203 |  0:00:03s
epoch 1  | loss: 0.35052 | val_0_rmse: 0.58242 | val_1_rmse: 0.5773  |  0:00:06s
epoch 2  | loss: 0.26478 | val_0_rmse: 0.56245 | val_1_rmse: 0.55393 |  0:00:09s
epoch 3  | loss: 0.2229  | val_0_rmse: 0.50416 | val_1_rmse: 0.49889 |  0:00:12s
epoch 4  | loss: 0.19784 | val_0_rmse: 0.48143 | val_1_rmse: 0.47362 |  0:00:16s
epoch 5  | loss: 0.19354 | val_0_rmse: 0.44956 | val_1_rmse: 0.44424 |  0:00:19s
epoch 6  | loss: 0.18873 | val_0_rmse: 0.44608 | val_1_rmse: 0.44157 |  0:00:22s
epoch 7  | loss: 0.17948 | val_0_rmse: 0.45264 | val_1_rmse: 0.44891 |  0:00:25s
epoch 8  | loss: 0.18016 | val_0_rmse: 0.4473  | val_1_rmse: 0.43994 |  0:00:28s
epoch 9  | loss: 0.17425 | val_0_rmse: 0.44371 | val_1_rmse: 0.43779 |  0:00:32s
epoch 10 | loss: 0.17111 | val_0_rmse: 0.45554 | val_1_rmse: 0.44819 |  0:00:35s
epoch 11 | loss: 0.17071 | val_0_rmse: 0.4318  | val_1_rmse: 0.43247 |  0:00:38s
epoch 12 | loss: 0.16914 | val_0_rmse: 0.43743 | val_1_rmse: 0.43777 |  0:00:41s
epoch 13 | loss: 0.1626  | val_0_rmse: 0.44865 | val_1_rmse: 0.42315 |  0:00:44s
epoch 14 | loss: 0.17031 | val_0_rmse: 0.42519 | val_1_rmse: 0.42466 |  0:00:48s
epoch 15 | loss: 0.16685 | val_0_rmse: 0.44421 | val_1_rmse: 0.44249 |  0:00:51s
epoch 16 | loss: 0.16663 | val_0_rmse: 0.47096 | val_1_rmse: 0.4671  |  0:00:54s
epoch 17 | loss: 0.1703  | val_0_rmse: 0.45043 | val_1_rmse: 0.44457 |  0:00:57s
epoch 18 | loss: 0.16855 | val_0_rmse: 0.46101 | val_1_rmse: 0.45669 |  0:01:01s
epoch 19 | loss: 0.16419 | val_0_rmse: 0.45256 | val_1_rmse: 0.44808 |  0:01:04s
epoch 20 | loss: 0.16164 | val_0_rmse: 0.43389 | val_1_rmse: 0.43164 |  0:01:07s
epoch 21 | loss: 0.16031 | val_0_rmse: 0.42022 | val_1_rmse: 0.41899 |  0:01:10s
epoch 22 | loss: 0.16212 | val_0_rmse: 0.43015 | val_1_rmse: 0.42865 |  0:01:13s
epoch 23 | loss: 0.15985 | val_0_rmse: 0.45905 | val_1_rmse: 0.45423 |  0:01:17s
epoch 24 | loss: 0.17079 | val_0_rmse: 0.51294 | val_1_rmse: 0.51011 |  0:01:20s
epoch 25 | loss: 0.16085 | val_0_rmse: 0.449   | val_1_rmse: 0.44593 |  0:01:23s
epoch 26 | loss: 0.17061 | val_0_rmse: 0.50598 | val_1_rmse: 0.50668 |  0:01:26s
epoch 27 | loss: 0.16275 | val_0_rmse: 0.46308 | val_1_rmse: 0.46048 |  0:01:29s
epoch 28 | loss: 0.16469 | val_0_rmse: 0.45782 | val_1_rmse: 0.45576 |  0:01:33s
epoch 29 | loss: 0.16516 | val_0_rmse: 0.46126 | val_1_rmse: 0.45447 |  0:01:36s
epoch 30 | loss: 0.16175 | val_0_rmse: 0.46003 | val_1_rmse: 0.45845 |  0:01:39s
epoch 31 | loss: 0.17012 | val_0_rmse: 0.43135 | val_1_rmse: 0.43046 |  0:01:42s
epoch 32 | loss: 0.16512 | val_0_rmse: 0.42396 | val_1_rmse: 0.42299 |  0:01:45s
epoch 33 | loss: 0.15762 | val_0_rmse: 0.41615 | val_1_rmse: 0.41505 |  0:01:49s
epoch 34 | loss: 0.16366 | val_0_rmse: 0.39698 | val_1_rmse: 0.39839 |  0:01:52s
epoch 35 | loss: 0.15407 | val_0_rmse: 0.39395 | val_1_rmse: 0.39225 |  0:01:55s
epoch 36 | loss: 0.15446 | val_0_rmse: 0.40086 | val_1_rmse: 0.40122 |  0:01:58s
epoch 37 | loss: 0.1481  | val_0_rmse: 0.39091 | val_1_rmse: 0.39371 |  0:02:01s
epoch 38 | loss: 0.15136 | val_0_rmse: 0.3923  | val_1_rmse: 0.39338 |  0:02:05s
epoch 39 | loss: 0.14892 | val_0_rmse: 0.39368 | val_1_rmse: 0.39604 |  0:02:08s
epoch 40 | loss: 0.14718 | val_0_rmse: 0.39575 | val_1_rmse: 0.39617 |  0:02:11s
epoch 41 | loss: 0.14712 | val_0_rmse: 0.41516 | val_1_rmse: 0.41884 |  0:02:14s
epoch 42 | loss: 0.14681 | val_0_rmse: 0.39743 | val_1_rmse: 0.4001  |  0:02:18s
epoch 43 | loss: 0.14812 | val_0_rmse: 0.3953  | val_1_rmse: 0.3956  |  0:02:21s
epoch 44 | loss: 0.14886 | val_0_rmse: 0.42622 | val_1_rmse: 0.42682 |  0:02:24s
epoch 45 | loss: 0.14776 | val_0_rmse: 0.40532 | val_1_rmse: 0.40774 |  0:02:27s
epoch 46 | loss: 0.14417 | val_0_rmse: 0.40482 | val_1_rmse: 0.40687 |  0:02:30s
epoch 47 | loss: 0.14665 | val_0_rmse: 0.42831 | val_1_rmse: 0.428   |  0:02:34s
epoch 48 | loss: 0.14778 | val_0_rmse: 0.415   | val_1_rmse: 0.41335 |  0:02:37s
epoch 49 | loss: 0.14827 | val_0_rmse: 0.39855 | val_1_rmse: 0.39825 |  0:02:40s
epoch 50 | loss: 0.1462  | val_0_rmse: 0.40737 | val_1_rmse: 0.40575 |  0:02:43s
epoch 51 | loss: 0.1468  | val_0_rmse: 0.42102 | val_1_rmse: 0.42155 |  0:02:46s
epoch 52 | loss: 0.14686 | val_0_rmse: 0.41964 | val_1_rmse: 0.41899 |  0:02:50s
epoch 53 | loss: 0.14809 | val_0_rmse: 0.41328 | val_1_rmse: 0.41658 |  0:02:53s
epoch 54 | loss: 0.1485  | val_0_rmse: 0.42717 | val_1_rmse: 0.42733 |  0:02:56s
epoch 55 | loss: 0.14409 | val_0_rmse: 0.42503 | val_1_rmse: 0.42632 |  0:02:59s
epoch 56 | loss: 0.14537 | val_0_rmse: 0.4062  | val_1_rmse: 0.40484 |  0:03:03s
epoch 57 | loss: 0.14251 | val_0_rmse: 0.44649 | val_1_rmse: 0.44879 |  0:03:06s
epoch 58 | loss: 0.14479 | val_0_rmse: 0.436   | val_1_rmse: 0.43909 |  0:03:09s
epoch 59 | loss: 0.14706 | val_0_rmse: 0.43593 | val_1_rmse: 0.43491 |  0:03:12s
epoch 60 | loss: 0.15288 | val_0_rmse: 0.37691 | val_1_rmse: 0.38011 |  0:03:15s
epoch 61 | loss: 0.14391 | val_0_rmse: 0.40775 | val_1_rmse: 0.41008 |  0:03:19s
epoch 62 | loss: 0.14606 | val_0_rmse: 0.38275 | val_1_rmse: 0.38661 |  0:03:22s
epoch 63 | loss: 0.14863 | val_0_rmse: 0.41377 | val_1_rmse: 0.41577 |  0:03:25s
epoch 64 | loss: 0.14605 | val_0_rmse: 0.4346  | val_1_rmse: 0.43616 |  0:03:28s
epoch 65 | loss: 0.14354 | val_0_rmse: 0.38684 | val_1_rmse: 0.38989 |  0:03:31s
epoch 66 | loss: 0.14316 | val_0_rmse: 0.41937 | val_1_rmse: 0.42237 |  0:03:35s
epoch 67 | loss: 0.14165 | val_0_rmse: 0.39188 | val_1_rmse: 0.39586 |  0:03:38s
epoch 68 | loss: 0.14003 | val_0_rmse: 0.39205 | val_1_rmse: 0.39655 |  0:03:41s
epoch 69 | loss: 0.13874 | val_0_rmse: 0.41728 | val_1_rmse: 0.41861 |  0:03:44s
epoch 70 | loss: 0.13777 | val_0_rmse: 0.37751 | val_1_rmse: 0.38583 |  0:03:47s
epoch 71 | loss: 0.13974 | val_0_rmse: 0.38615 | val_1_rmse: 0.39147 |  0:03:51s
epoch 72 | loss: 0.13829 | val_0_rmse: 0.41095 | val_1_rmse: 0.41259 |  0:03:54s
epoch 73 | loss: 0.13971 | val_0_rmse: 0.45234 | val_1_rmse: 0.4561  |  0:03:57s
epoch 74 | loss: 0.13938 | val_0_rmse: 0.38281 | val_1_rmse: 0.38994 |  0:04:00s
epoch 75 | loss: 0.1382  | val_0_rmse: 0.45303 | val_1_rmse: 0.4531  |  0:04:03s
epoch 76 | loss: 0.13493 | val_0_rmse: 0.4     | val_1_rmse: 0.40516 |  0:04:07s
epoch 77 | loss: 0.14111 | val_0_rmse: 0.40646 | val_1_rmse: 0.41088 |  0:04:10s
epoch 78 | loss: 0.13897 | val_0_rmse: 0.4145  | val_1_rmse: 0.41936 |  0:04:13s
epoch 79 | loss: 0.1364  | val_0_rmse: 0.38098 | val_1_rmse: 0.38661 |  0:04:16s
epoch 80 | loss: 0.14553 | val_0_rmse: 0.40858 | val_1_rmse: 0.3995  |  0:04:19s
epoch 81 | loss: 0.14731 | val_0_rmse: 0.37915 | val_1_rmse: 0.38346 |  0:04:23s
epoch 82 | loss: 0.1441  | val_0_rmse: 0.38105 | val_1_rmse: 0.38484 |  0:04:26s
epoch 83 | loss: 0.14378 | val_0_rmse: 0.44028 | val_1_rmse: 0.44063 |  0:04:29s
epoch 84 | loss: 0.14179 | val_0_rmse: 0.40072 | val_1_rmse: 0.40522 |  0:04:32s
epoch 85 | loss: 0.14076 | val_0_rmse: 0.39657 | val_1_rmse: 0.39927 |  0:04:36s
epoch 86 | loss: 0.15113 | val_0_rmse: 0.42898 | val_1_rmse: 0.43122 |  0:04:39s
epoch 87 | loss: 0.15571 | val_0_rmse: 0.42097 | val_1_rmse: 0.4206  |  0:04:42s
epoch 88 | loss: 0.15118 | val_0_rmse: 0.4129  | val_1_rmse: 0.41377 |  0:04:45s
epoch 89 | loss: 0.14203 | val_0_rmse: 0.42065 | val_1_rmse: 0.42093 |  0:04:48s
epoch 90 | loss: 0.14373 | val_0_rmse: 0.42888 | val_1_rmse: 0.42901 |  0:04:52s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.38011
Best weights from best epoch are automatically used!
ended training at: 05:45:44
Feature importance:
[('Area', 0.0), ('Baths', 0.11319256322918893), ('Beds', 0.2371068108477066), ('Latitude', 1.2710307851601785e-06), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.27117138502337246), ('HF_BATHRM', 1.4004764204497725e-05), ('AC', 0.0), ('NUM_UNITS', 0.0), ('ROOMS', 0.0), ('AYB', 0.0), ('YR_RMDL', 0.18772961641418814), ('EYB', 0.0), ('STORIES', 0.0), ('QUALIFIED', 4.025936614655037e-07), ('SALE_NUM', 0.0), ('KITCHENS', 0.0), ('FIREPLACES', 0.1559873405554751), ('SOURCE', 0.02859899338322703), ('ZIPCODE', 0.0061976121581906)]
Mean squared error is of 10948042102.830954
Mean absolute error:65273.01189593663
MAPE:0.29421962120749123
R2 score:0.8090808427540189
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:45
epoch 0  | loss: 0.64518 | val_0_rmse: 0.66079 | val_1_rmse: 0.66593 |  0:00:03s
epoch 1  | loss: 0.35126 | val_0_rmse: 0.57405 | val_1_rmse: 0.57441 |  0:00:06s
epoch 2  | loss: 0.2635  | val_0_rmse: 0.52313 | val_1_rmse: 0.52383 |  0:00:09s
epoch 3  | loss: 0.21938 | val_0_rmse: 0.50359 | val_1_rmse: 0.50793 |  0:00:12s
epoch 4  | loss: 0.19972 | val_0_rmse: 0.44693 | val_1_rmse: 0.4495  |  0:00:16s
epoch 5  | loss: 0.18621 | val_0_rmse: 0.43851 | val_1_rmse: 0.44107 |  0:00:19s
epoch 6  | loss: 0.18277 | val_0_rmse: 0.43479 | val_1_rmse: 0.43615 |  0:00:22s
epoch 7  | loss: 0.18238 | val_0_rmse: 0.48346 | val_1_rmse: 0.48329 |  0:00:25s
epoch 8  | loss: 0.17981 | val_0_rmse: 0.43656 | val_1_rmse: 0.43956 |  0:00:29s
epoch 9  | loss: 0.17209 | val_0_rmse: 0.41653 | val_1_rmse: 0.41813 |  0:00:32s
epoch 10 | loss: 0.17254 | val_0_rmse: 0.42054 | val_1_rmse: 0.42769 |  0:00:35s
epoch 11 | loss: 0.16924 | val_0_rmse: 0.41408 | val_1_rmse: 0.41924 |  0:00:38s
epoch 12 | loss: 0.17214 | val_0_rmse: 0.40791 | val_1_rmse: 0.41623 |  0:00:41s
epoch 13 | loss: 0.1652  | val_0_rmse: 0.41605 | val_1_rmse: 0.41918 |  0:00:45s
epoch 14 | loss: 0.16501 | val_0_rmse: 0.40157 | val_1_rmse: 0.40742 |  0:00:48s
epoch 15 | loss: 0.17234 | val_0_rmse: 0.42815 | val_1_rmse: 0.43337 |  0:00:51s
epoch 16 | loss: 0.163   | val_0_rmse: 0.40172 | val_1_rmse: 0.40522 |  0:00:54s
epoch 17 | loss: 0.15891 | val_0_rmse: 0.38463 | val_1_rmse: 0.3923  |  0:00:57s
epoch 18 | loss: 0.15818 | val_0_rmse: 0.40835 | val_1_rmse: 0.41507 |  0:01:01s
epoch 19 | loss: 0.15949 | val_0_rmse: 0.40467 | val_1_rmse: 0.41193 |  0:01:04s
epoch 20 | loss: 0.16268 | val_0_rmse: 0.41219 | val_1_rmse: 0.41953 |  0:01:07s
epoch 21 | loss: 0.15358 | val_0_rmse: 0.40456 | val_1_rmse: 0.41144 |  0:01:10s
epoch 22 | loss: 0.15507 | val_0_rmse: 0.39683 | val_1_rmse: 0.40253 |  0:01:14s
epoch 23 | loss: 0.1586  | val_0_rmse: 0.39991 | val_1_rmse: 0.40816 |  0:01:17s
epoch 24 | loss: 0.15298 | val_0_rmse: 0.41377 | val_1_rmse: 0.41934 |  0:01:20s
epoch 25 | loss: 0.15266 | val_0_rmse: 0.38488 | val_1_rmse: 0.39328 |  0:01:23s
epoch 26 | loss: 0.15197 | val_0_rmse: 0.39406 | val_1_rmse: 0.40102 |  0:01:26s
epoch 27 | loss: 0.15299 | val_0_rmse: 0.39751 | val_1_rmse: 0.40524 |  0:01:30s
epoch 28 | loss: 0.1507  | val_0_rmse: 0.39306 | val_1_rmse: 0.39994 |  0:01:33s
epoch 29 | loss: 0.14957 | val_0_rmse: 0.40759 | val_1_rmse: 0.41689 |  0:01:36s
epoch 30 | loss: 0.15296 | val_0_rmse: 0.39443 | val_1_rmse: 0.4002  |  0:01:39s
epoch 31 | loss: 0.1514  | val_0_rmse: 0.39953 | val_1_rmse: 0.40662 |  0:01:42s
epoch 32 | loss: 0.15235 | val_0_rmse: 0.40655 | val_1_rmse: 0.41383 |  0:01:46s
epoch 33 | loss: 0.15068 | val_0_rmse: 0.42855 | val_1_rmse: 0.43533 |  0:01:49s
epoch 34 | loss: 0.14978 | val_0_rmse: 0.39555 | val_1_rmse: 0.40936 |  0:01:52s
epoch 35 | loss: 0.15026 | val_0_rmse: 0.40737 | val_1_rmse: 0.42135 |  0:01:55s
epoch 36 | loss: 0.14519 | val_0_rmse: 0.38344 | val_1_rmse: 0.39637 |  0:01:58s
epoch 37 | loss: 0.14883 | val_0_rmse: 0.39147 | val_1_rmse: 0.40057 |  0:02:02s
epoch 38 | loss: 0.1464  | val_0_rmse: 0.39631 | val_1_rmse: 0.40269 |  0:02:05s
epoch 39 | loss: 0.14661 | val_0_rmse: 0.41068 | val_1_rmse: 0.42043 |  0:02:08s
epoch 40 | loss: 0.14547 | val_0_rmse: 0.37445 | val_1_rmse: 0.38454 |  0:02:11s
epoch 41 | loss: 0.15112 | val_0_rmse: 0.41066 | val_1_rmse: 0.41841 |  0:02:15s
epoch 42 | loss: 0.14907 | val_0_rmse: 0.40152 | val_1_rmse: 0.41155 |  0:02:18s
epoch 43 | loss: 0.14367 | val_0_rmse: 0.38563 | val_1_rmse: 0.39466 |  0:02:21s
epoch 44 | loss: 0.14383 | val_0_rmse: 0.38222 | val_1_rmse: 0.3908  |  0:02:24s
epoch 45 | loss: 0.1447  | val_0_rmse: 0.38583 | val_1_rmse: 0.39646 |  0:02:27s
epoch 46 | loss: 0.14271 | val_0_rmse: 0.38247 | val_1_rmse: 0.39178 |  0:02:31s
epoch 47 | loss: 0.14731 | val_0_rmse: 0.39265 | val_1_rmse: 0.40367 |  0:02:34s
epoch 48 | loss: 0.14521 | val_0_rmse: 0.40538 | val_1_rmse: 0.41763 |  0:02:37s
epoch 49 | loss: 0.14115 | val_0_rmse: 0.38007 | val_1_rmse: 0.38992 |  0:02:40s
epoch 50 | loss: 0.14261 | val_0_rmse: 0.4003  | val_1_rmse: 0.41184 |  0:02:44s
epoch 51 | loss: 0.14719 | val_0_rmse: 0.37152 | val_1_rmse: 0.38135 |  0:02:47s
epoch 52 | loss: 0.14153 | val_0_rmse: 0.372   | val_1_rmse: 0.38481 |  0:02:50s
epoch 53 | loss: 0.13802 | val_0_rmse: 0.38234 | val_1_rmse: 0.39542 |  0:02:53s
epoch 54 | loss: 0.13794 | val_0_rmse: 0.38499 | val_1_rmse: 0.3985  |  0:02:56s
epoch 55 | loss: 0.13675 | val_0_rmse: 0.37914 | val_1_rmse: 0.39057 |  0:03:00s
epoch 56 | loss: 0.1381  | val_0_rmse: 0.3673  | val_1_rmse: 0.3803  |  0:03:03s
epoch 57 | loss: 0.13915 | val_0_rmse: 0.36695 | val_1_rmse: 0.37901 |  0:03:06s
epoch 58 | loss: 0.13659 | val_0_rmse: 0.37685 | val_1_rmse: 0.39008 |  0:03:09s
epoch 59 | loss: 0.1373  | val_0_rmse: 0.38246 | val_1_rmse: 0.39492 |  0:03:12s
epoch 60 | loss: 0.13464 | val_0_rmse: 0.36191 | val_1_rmse: 0.37501 |  0:03:16s
epoch 61 | loss: 0.1373  | val_0_rmse: 0.39033 | val_1_rmse: 0.40242 |  0:03:19s
epoch 62 | loss: 0.13557 | val_0_rmse: 0.36999 | val_1_rmse: 0.38181 |  0:03:22s
epoch 63 | loss: 0.1365  | val_0_rmse: 0.3797  | val_1_rmse: 0.39264 |  0:03:25s
epoch 64 | loss: 0.1327  | val_0_rmse: 0.36489 | val_1_rmse: 0.37773 |  0:03:29s
epoch 65 | loss: 0.1383  | val_0_rmse: 0.3702  | val_1_rmse: 0.38472 |  0:03:32s
epoch 66 | loss: 0.13439 | val_0_rmse: 0.38799 | val_1_rmse: 0.40248 |  0:03:35s
epoch 67 | loss: 0.1357  | val_0_rmse: 0.38206 | val_1_rmse: 0.39506 |  0:03:38s
epoch 68 | loss: 0.13321 | val_0_rmse: 0.37007 | val_1_rmse: 0.38379 |  0:03:41s
epoch 69 | loss: 0.13711 | val_0_rmse: 0.35798 | val_1_rmse: 0.37183 |  0:03:45s
epoch 70 | loss: 0.14103 | val_0_rmse: 0.40167 | val_1_rmse: 0.41677 |  0:03:48s
epoch 71 | loss: 0.13843 | val_0_rmse: 0.36632 | val_1_rmse: 0.37872 |  0:03:51s
epoch 72 | loss: 0.13574 | val_0_rmse: 0.37536 | val_1_rmse: 0.38832 |  0:03:54s
epoch 73 | loss: 0.13393 | val_0_rmse: 0.40368 | val_1_rmse: 0.41483 |  0:03:57s
epoch 74 | loss: 0.13837 | val_0_rmse: 0.37417 | val_1_rmse: 0.38791 |  0:04:01s
epoch 75 | loss: 0.13397 | val_0_rmse: 0.37331 | val_1_rmse: 0.38662 |  0:04:04s
epoch 76 | loss: 0.13159 | val_0_rmse: 0.39451 | val_1_rmse: 0.40696 |  0:04:07s
epoch 77 | loss: 0.13226 | val_0_rmse: 0.37419 | val_1_rmse: 0.38779 |  0:04:10s
epoch 78 | loss: 0.13154 | val_0_rmse: 0.35607 | val_1_rmse: 0.37166 |  0:04:13s
epoch 79 | loss: 0.13541 | val_0_rmse: 0.3686  | val_1_rmse: 0.38483 |  0:04:17s
epoch 80 | loss: 0.13339 | val_0_rmse: 0.36491 | val_1_rmse: 0.3798  |  0:04:20s
epoch 81 | loss: 0.13621 | val_0_rmse: 0.37029 | val_1_rmse: 0.38462 |  0:04:23s
epoch 82 | loss: 0.1433  | val_0_rmse: 0.38791 | val_1_rmse: 0.40054 |  0:04:26s
epoch 83 | loss: 0.15246 | val_0_rmse: 0.39108 | val_1_rmse: 0.40098 |  0:04:30s
epoch 84 | loss: 0.14659 | val_0_rmse: 0.3905  | val_1_rmse: 0.40208 |  0:04:33s
epoch 85 | loss: 0.13943 | val_0_rmse: 0.38378 | val_1_rmse: 0.39575 |  0:04:36s
epoch 86 | loss: 0.13535 | val_0_rmse: 0.37673 | val_1_rmse: 0.39044 |  0:04:39s
epoch 87 | loss: 0.13449 | val_0_rmse: 0.38092 | val_1_rmse: 0.39514 |  0:04:42s
epoch 88 | loss: 0.13641 | val_0_rmse: 0.37533 | val_1_rmse: 0.39091 |  0:04:46s
epoch 89 | loss: 0.13463 | val_0_rmse: 0.38396 | val_1_rmse: 0.39891 |  0:04:49s
epoch 90 | loss: 0.13348 | val_0_rmse: 0.36394 | val_1_rmse: 0.37797 |  0:04:52s
epoch 91 | loss: 0.13321 | val_0_rmse: 0.38738 | val_1_rmse: 0.40054 |  0:04:55s
epoch 92 | loss: 0.13577 | val_0_rmse: 0.36481 | val_1_rmse: 0.37917 |  0:04:59s
epoch 93 | loss: 0.13553 | val_0_rmse: 0.413   | val_1_rmse: 0.4282  |  0:05:02s
epoch 94 | loss: 0.13236 | val_0_rmse: 0.35988 | val_1_rmse: 0.3766  |  0:05:05s
epoch 95 | loss: 0.1336  | val_0_rmse: 0.40065 | val_1_rmse: 0.41394 |  0:05:08s
epoch 96 | loss: 0.13191 | val_0_rmse: 0.38773 | val_1_rmse: 0.40181 |  0:05:11s
epoch 97 | loss: 0.13283 | val_0_rmse: 0.36412 | val_1_rmse: 0.38032 |  0:05:15s
epoch 98 | loss: 0.13645 | val_0_rmse: 0.37752 | val_1_rmse: 0.39224 |  0:05:18s
epoch 99 | loss: 0.13028 | val_0_rmse: 0.37642 | val_1_rmse: 0.39016 |  0:05:21s
epoch 100| loss: 0.13012 | val_0_rmse: 0.39328 | val_1_rmse: 0.40667 |  0:05:24s
epoch 101| loss: 0.13368 | val_0_rmse: 0.36993 | val_1_rmse: 0.38612 |  0:05:27s
epoch 102| loss: 0.13673 | val_0_rmse: 0.3878  | val_1_rmse: 0.40223 |  0:05:31s
epoch 103| loss: 0.13402 | val_0_rmse: 0.36472 | val_1_rmse: 0.38039 |  0:05:34s
epoch 104| loss: 0.12857 | val_0_rmse: 0.37348 | val_1_rmse: 0.39041 |  0:05:37s
epoch 105| loss: 0.13174 | val_0_rmse: 0.36833 | val_1_rmse: 0.38296 |  0:05:40s
epoch 106| loss: 0.13049 | val_0_rmse: 0.37367 | val_1_rmse: 0.39174 |  0:05:43s
epoch 107| loss: 0.13452 | val_0_rmse: 0.38463 | val_1_rmse: 0.40175 |  0:05:47s
epoch 108| loss: 0.1329  | val_0_rmse: 0.37875 | val_1_rmse: 0.39623 |  0:05:50s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.37166
Best weights from best epoch are automatically used!
ended training at: 05:51:36
Feature importance:
[('Area', 0.0), ('Baths', 0.017651509837075446), ('Beds', 0.03976833633178403), ('Latitude', 0.018561303161167497), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.37453276840085215), ('HF_BATHRM', 0.0), ('AC', 0.03405379234406533), ('NUM_UNITS', 0.042296881262588217), ('ROOMS', 0.06756877328806524), ('AYB', 0.0), ('YR_RMDL', 0.12034545638808744), ('EYB', 0.0), ('STORIES', 0.0841563216520269), ('QUALIFIED', 0.10032309784997603), ('SALE_NUM', 0.0011373389672164195), ('KITCHENS', 0.016381219775410672), ('FIREPLACES', 0.006275585791970603), ('SOURCE', 0.00959175317173753), ('ZIPCODE', 0.06735586177797649)]
Mean squared error is of 8032439192.011802
Mean absolute error:60710.98266277276
MAPE:0.25932880537192554
R2 score:0.8609434632507097
------------------------------------------------------------------
