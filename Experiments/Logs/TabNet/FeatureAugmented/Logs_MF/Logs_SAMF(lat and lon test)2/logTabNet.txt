TabNet Logs:

Saving copy of script...
In this script only the South American datasets and its a countinuation of the latitude/longitude testHere the test done is to test the improvement that the new related to lat/lon features provideThe idea is: The new features should be reflected on the lat/lon, so does them existing provide improvements?Shouldnt the neuralnet be able to obtained them automatically?
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:49:45
epoch 0  | loss: 1.00538 | val_0_rmse: 0.96945 | val_1_rmse: 0.97076 |  0:00:11s
epoch 1  | loss: 0.78023 | val_0_rmse: 0.9495  | val_1_rmse: 0.95059 |  0:00:18s
epoch 2  | loss: 0.76003 | val_0_rmse: 0.90701 | val_1_rmse: 0.9078  |  0:00:27s
epoch 3  | loss: 0.7554  | val_0_rmse: 0.87983 | val_1_rmse: 0.88212 |  0:00:35s
epoch 4  | loss: 0.75324 | val_0_rmse: 0.87955 | val_1_rmse: 0.88262 |  0:00:43s
epoch 5  | loss: 0.74705 | val_0_rmse: 0.86356 | val_1_rmse: 0.86616 |  0:00:52s
epoch 6  | loss: 0.74625 | val_0_rmse: 0.86378 | val_1_rmse: 0.86713 |  0:01:01s
epoch 7  | loss: 0.74527 | val_0_rmse: 0.86381 | val_1_rmse: 0.86647 |  0:01:09s
epoch 8  | loss: 0.74525 | val_0_rmse: 0.90118 | val_1_rmse: 0.9023  |  0:01:19s
epoch 9  | loss: 0.74688 | val_0_rmse: 0.87485 | val_1_rmse: 0.87739 |  0:01:28s
epoch 10 | loss: 0.7456  | val_0_rmse: 0.86504 | val_1_rmse: 0.86668 |  0:01:37s
epoch 11 | loss: 0.74607 | val_0_rmse: 0.86024 | val_1_rmse: 0.86317 |  0:01:47s
epoch 12 | loss: 0.74254 | val_0_rmse: 0.86388 | val_1_rmse: 0.86762 |  0:01:57s
epoch 13 | loss: 0.7425  | val_0_rmse: 0.87054 | val_1_rmse: 0.8728  |  0:02:07s
epoch 14 | loss: 0.73983 | val_0_rmse: 0.86684 | val_1_rmse: 0.87069 |  0:02:16s
epoch 15 | loss: 0.73863 | val_0_rmse: 0.8734  | val_1_rmse: 0.87607 |  0:02:26s
epoch 16 | loss: 0.73877 | val_0_rmse: 0.8603  | val_1_rmse: 0.86331 |  0:02:36s
epoch 17 | loss: 0.7388  | val_0_rmse: 0.913   | val_1_rmse: 0.91436 |  0:02:46s
epoch 18 | loss: 0.73738 | val_0_rmse: 0.85847 | val_1_rmse: 0.86329 |  0:02:56s
epoch 19 | loss: 0.73762 | val_0_rmse: 0.86065 | val_1_rmse: 0.86554 |  0:03:07s
epoch 20 | loss: 0.73736 | val_0_rmse: 0.87572 | val_1_rmse: 0.87898 |  0:03:17s
epoch 21 | loss: 0.73805 | val_0_rmse: 0.85979 | val_1_rmse: 0.8641  |  0:03:27s
epoch 22 | loss: 0.73883 | val_0_rmse: 0.86936 | val_1_rmse: 0.87018 |  0:03:37s
epoch 23 | loss: 0.7378  | val_0_rmse: 0.86256 | val_1_rmse: 0.86558 |  0:03:47s
epoch 24 | loss: 0.73797 | val_0_rmse: 0.8653  | val_1_rmse: 0.86866 |  0:03:58s
epoch 25 | loss: 0.7381  | val_0_rmse: 0.89635 | val_1_rmse: 0.89725 |  0:04:08s
epoch 26 | loss: 0.73786 | val_0_rmse: 0.86067 | val_1_rmse: 0.86444 |  0:04:18s
epoch 27 | loss: 0.73827 | val_0_rmse: 0.8902  | val_1_rmse: 0.89139 |  0:04:29s
epoch 28 | loss: 0.73913 | val_0_rmse: 0.86831 | val_1_rmse: 0.87293 |  0:04:39s
epoch 29 | loss: 0.73942 | val_0_rmse: 0.85854 | val_1_rmse: 0.86219 |  0:04:49s
epoch 30 | loss: 0.73595 | val_0_rmse: 0.87443 | val_1_rmse: 0.87723 |  0:05:00s
epoch 31 | loss: 0.7386  | val_0_rmse: 0.86631 | val_1_rmse: 0.86986 |  0:05:10s
epoch 32 | loss: 0.73705 | val_0_rmse: 0.86178 | val_1_rmse: 0.86484 |  0:05:20s
epoch 33 | loss: 0.73632 | val_0_rmse: 0.86385 | val_1_rmse: 0.86831 |  0:05:30s
epoch 34 | loss: 0.73607 | val_0_rmse: 0.86104 | val_1_rmse: 0.86463 |  0:05:41s
epoch 35 | loss: 0.73461 | val_0_rmse: 0.85888 | val_1_rmse: 0.86318 |  0:05:51s
epoch 36 | loss: 0.73558 | val_0_rmse: 0.85961 | val_1_rmse: 0.86339 |  0:06:02s
epoch 37 | loss: 0.73696 | val_0_rmse: 0.85948 | val_1_rmse: 0.86314 |  0:06:12s
epoch 38 | loss: 0.73562 | val_0_rmse: 0.86183 | val_1_rmse: 0.8658  |  0:06:22s
epoch 39 | loss: 0.73695 | val_0_rmse: 0.86271 | val_1_rmse: 0.86744 |  0:06:33s
epoch 40 | loss: 0.73665 | val_0_rmse: 0.85966 | val_1_rmse: 0.86447 |  0:06:43s
epoch 41 | loss: 0.73412 | val_0_rmse: 0.85831 | val_1_rmse: 0.86266 |  0:06:54s
epoch 42 | loss: 0.73459 | val_0_rmse: 0.8561  | val_1_rmse: 0.86086 |  0:07:04s
epoch 43 | loss: 0.73477 | val_0_rmse: 0.85644 | val_1_rmse: 0.86168 |  0:07:14s
epoch 44 | loss: 0.7347  | val_0_rmse: 0.85595 | val_1_rmse: 0.86183 |  0:07:25s
epoch 45 | loss: 0.73429 | val_0_rmse: 0.85532 | val_1_rmse: 0.86076 |  0:07:35s
epoch 46 | loss: 0.73541 | val_0_rmse: 0.85674 | val_1_rmse: 0.86084 |  0:07:46s
epoch 47 | loss: 0.73803 | val_0_rmse: 0.86205 | val_1_rmse: 0.86585 |  0:07:56s
epoch 48 | loss: 0.73668 | val_0_rmse: 0.86076 | val_1_rmse: 0.86479 |  0:08:06s
epoch 49 | loss: 0.73528 | val_0_rmse: 0.86274 | val_1_rmse: 0.86644 |  0:08:17s
epoch 50 | loss: 0.73728 | val_0_rmse: 0.89092 | val_1_rmse: 0.8945  |  0:08:27s
epoch 51 | loss: 0.74033 | val_0_rmse: 0.90146 | val_1_rmse: 0.91083 |  0:08:38s
epoch 52 | loss: 0.7431  | val_0_rmse: 0.85732 | val_1_rmse: 0.90098 |  0:08:48s
epoch 53 | loss: 0.73581 | val_0_rmse: 0.85978 | val_1_rmse: 0.89653 |  0:08:58s
epoch 54 | loss: 0.73638 | val_0_rmse: 0.85816 | val_1_rmse: 0.90219 |  0:09:09s
epoch 55 | loss: 0.73657 | val_0_rmse: 0.85956 | val_1_rmse: 0.89662 |  0:09:19s
epoch 56 | loss: 0.73606 | val_0_rmse: 0.85679 | val_1_rmse: 0.86055 |  0:09:29s
epoch 57 | loss: 0.73642 | val_0_rmse: 0.85827 | val_1_rmse: 0.86198 |  0:09:40s
epoch 58 | loss: 0.73704 | val_0_rmse: 0.85764 | val_1_rmse: 0.89903 |  0:09:50s
epoch 59 | loss: 0.736   | val_0_rmse: 0.85937 | val_1_rmse: 0.86331 |  0:10:00s
epoch 60 | loss: 0.73675 | val_0_rmse: 1.9574  | val_1_rmse: 1.89398 |  0:10:11s
epoch 61 | loss: 0.73774 | val_0_rmse: 0.85776 | val_1_rmse: 0.86464 |  0:10:21s
epoch 62 | loss: 0.73771 | val_0_rmse: 0.86007 | val_1_rmse: 0.86582 |  0:10:31s
epoch 63 | loss: 0.73737 | val_0_rmse: 0.85811 | val_1_rmse: 0.86325 |  0:10:42s
epoch 64 | loss: 0.73666 | val_0_rmse: 0.85908 | val_1_rmse: 0.86563 |  0:10:52s
epoch 65 | loss: 0.73878 | val_0_rmse: 0.85962 | val_1_rmse: 0.86371 |  0:11:02s
epoch 66 | loss: 0.73871 | val_0_rmse: 0.85843 | val_1_rmse: 0.862   |  0:11:13s
epoch 67 | loss: 0.73866 | val_0_rmse: 0.85925 | val_1_rmse: 0.87722 |  0:11:23s
epoch 68 | loss: 0.73718 | val_0_rmse: 0.85931 | val_1_rmse: 0.87444 |  0:11:33s
epoch 69 | loss: 0.73701 | val_0_rmse: 0.85793 | val_1_rmse: 0.86264 |  0:11:44s
epoch 70 | loss: 0.73752 | val_0_rmse: 0.85864 | val_1_rmse: 0.86271 |  0:11:54s
epoch 71 | loss: 0.7374  | val_0_rmse: 0.85817 | val_1_rmse: 0.86217 |  0:12:05s
epoch 72 | loss: 0.73796 | val_0_rmse: 0.85775 | val_1_rmse: 0.86201 |  0:12:15s
epoch 73 | loss: 0.73655 | val_0_rmse: 0.85757 | val_1_rmse: 0.86194 |  0:12:25s
epoch 74 | loss: 0.73607 | val_0_rmse: 0.8581  | val_1_rmse: 0.86225 |  0:12:36s
epoch 75 | loss: 0.73555 | val_0_rmse: 0.85756 | val_1_rmse: 0.86164 |  0:12:46s
epoch 76 | loss: 0.73722 | val_0_rmse: 0.85755 | val_1_rmse: 0.86272 |  0:12:56s
epoch 77 | loss: 0.73766 | val_0_rmse: 0.8577  | val_1_rmse: 0.8629  |  0:13:07s
epoch 78 | loss: 0.73695 | val_0_rmse: 0.85754 | val_1_rmse: 0.86201 |  0:13:17s
epoch 79 | loss: 0.73845 | val_0_rmse: 0.86026 | val_1_rmse: 0.86403 |  0:13:27s
epoch 80 | loss: 0.73713 | val_0_rmse: 0.85643 | val_1_rmse: 0.86075 |  0:13:38s
epoch 81 | loss: 0.73605 | val_0_rmse: 0.85784 | val_1_rmse: 0.86299 |  0:13:48s
epoch 82 | loss: 0.73598 | val_0_rmse: 0.8579  | val_1_rmse: 0.86312 |  0:13:58s
epoch 83 | loss: 0.73893 | val_0_rmse: 0.85764 | val_1_rmse: 0.86295 |  0:14:09s
epoch 84 | loss: 0.73946 | val_0_rmse: 0.85919 | val_1_rmse: 0.86339 |  0:14:19s
epoch 85 | loss: 0.73738 | val_0_rmse: 0.86023 | val_1_rmse: 0.86453 |  0:14:29s
epoch 86 | loss: 0.73647 | val_0_rmse: 0.85668 | val_1_rmse: 0.86158 |  0:14:40s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.86055
Best weights from best epoch are automatically used!
ended training at: 04:04:29
Feature importance:
Mean squared error is of 4988493048.923596
Mean absolute error:54737.55360982321
MAPE:0.53603898391544
R2 score:0.25995979626526955
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:04:32
epoch 0  | loss: 0.96254 | val_0_rmse: 0.94271 | val_1_rmse: 0.94817 |  0:00:10s
epoch 1  | loss: 0.77945 | val_0_rmse: 0.9196  | val_1_rmse: 0.9263  |  0:00:20s
epoch 2  | loss: 0.76996 | val_0_rmse: 0.89447 | val_1_rmse: 0.8994  |  0:00:31s
epoch 3  | loss: 0.76683 | val_0_rmse: 0.88068 | val_1_rmse: 0.88638 |  0:00:41s
epoch 4  | loss: 0.75683 | val_0_rmse: 0.88177 | val_1_rmse: 0.88805 |  0:00:52s
epoch 5  | loss: 0.75221 | val_0_rmse: 0.86358 | val_1_rmse: 0.86785 |  0:01:02s
epoch 6  | loss: 0.75228 | val_0_rmse: 0.87146 | val_1_rmse: 0.8773  |  0:01:13s
epoch 7  | loss: 0.75099 | val_0_rmse: 0.8632  | val_1_rmse: 0.86899 |  0:01:23s
epoch 8  | loss: 0.74596 | val_0_rmse: 0.86    | val_1_rmse: 0.86545 |  0:01:34s
epoch 9  | loss: 0.7417  | val_0_rmse: 0.85902 | val_1_rmse: 0.86547 |  0:01:44s
epoch 10 | loss: 0.74166 | val_0_rmse: 0.87072 | val_1_rmse: 0.88053 |  0:01:55s
epoch 11 | loss: 0.73962 | val_0_rmse: 0.86985 | val_1_rmse: 0.87964 |  0:02:05s
epoch 12 | loss: 0.7383  | val_0_rmse: 0.85903 | val_1_rmse: 0.86571 |  0:02:16s
epoch 13 | loss: 0.73825 | val_0_rmse: 0.88483 | val_1_rmse: 0.89904 |  0:02:26s
epoch 14 | loss: 0.74072 | val_0_rmse: 0.86292 | val_1_rmse: 0.9226  |  0:02:37s
epoch 15 | loss: 0.74193 | val_0_rmse: 0.88269 | val_1_rmse: 0.9598  |  0:02:47s
epoch 16 | loss: 0.74107 | val_0_rmse: 0.86012 | val_1_rmse: 0.94859 |  0:02:58s
epoch 17 | loss: 0.74312 | val_0_rmse: 0.86168 | val_1_rmse: 0.86663 |  0:03:08s
epoch 18 | loss: 0.74077 | val_0_rmse: 0.86753 | val_1_rmse: 0.87385 |  0:03:18s
epoch 19 | loss: 0.75563 | val_0_rmse: 0.86961 | val_1_rmse: 0.87827 |  0:03:29s
epoch 20 | loss: 0.75169 | val_0_rmse: 0.86474 | val_1_rmse: 0.87063 |  0:03:39s
epoch 21 | loss: 0.74943 | val_0_rmse: 0.86312 | val_1_rmse: 0.881   |  0:03:50s
epoch 22 | loss: 0.75144 | val_0_rmse: 0.86309 | val_1_rmse: 0.90308 |  0:04:00s
epoch 23 | loss: 0.74729 | val_0_rmse: 0.86367 | val_1_rmse: 0.90826 |  0:04:11s
epoch 24 | loss: 0.74401 | val_0_rmse: 0.86069 | val_1_rmse: 0.89731 |  0:04:21s
epoch 25 | loss: 0.74339 | val_0_rmse: 0.86239 | val_1_rmse: 0.90203 |  0:04:31s
epoch 26 | loss: 0.74522 | val_0_rmse: 0.86606 | val_1_rmse: 0.89657 |  0:04:42s
epoch 27 | loss: 0.74515 | val_0_rmse: 0.8604  | val_1_rmse: 0.86643 |  0:04:52s
epoch 28 | loss: 0.74267 | val_0_rmse: 0.86096 | val_1_rmse: 0.86742 |  0:05:03s
epoch 29 | loss: 0.74227 | val_0_rmse: 0.86265 | val_1_rmse: 0.86931 |  0:05:13s
epoch 30 | loss: 0.74178 | val_0_rmse: 0.86014 | val_1_rmse: 0.86698 |  0:05:24s
epoch 31 | loss: 0.74104 | val_0_rmse: 0.86596 | val_1_rmse: 0.87284 |  0:05:34s
epoch 32 | loss: 0.74831 | val_0_rmse: 0.86429 | val_1_rmse: 0.88318 |  0:05:45s
epoch 33 | loss: 0.74714 | val_0_rmse: 0.86588 | val_1_rmse: 0.87375 |  0:05:55s
epoch 34 | loss: 0.74546 | val_0_rmse: 0.86942 | val_1_rmse: 0.87745 |  0:06:06s
epoch 35 | loss: 0.75348 | val_0_rmse: 0.86691 | val_1_rmse: 0.87453 |  0:06:16s
epoch 36 | loss: 0.75359 | val_0_rmse: 0.86743 | val_1_rmse: 0.87545 |  0:06:27s
epoch 37 | loss: 0.75256 | val_0_rmse: 0.86821 | val_1_rmse: 0.87649 |  0:06:37s
epoch 38 | loss: 0.75241 | val_0_rmse: 0.87771 | val_1_rmse: 0.8888  |  0:06:48s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.86545
Best weights from best epoch are automatically used!
ended training at: 04:11:24
Feature importance:
Mean squared error is of 5030120301.058863
Mean absolute error:55980.71000162382
MAPE:0.5657944324366372
R2 score:0.2534809379979106
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:11:26
epoch 0  | loss: 0.95866 | val_0_rmse: 0.98966 | val_1_rmse: 0.9914  |  0:00:10s
epoch 1  | loss: 0.77006 | val_0_rmse: 0.9453  | val_1_rmse: 0.94896 |  0:00:21s
epoch 2  | loss: 0.75699 | val_0_rmse: 0.91594 | val_1_rmse: 0.91916 |  0:00:31s
epoch 3  | loss: 0.75127 | val_0_rmse: 0.88981 | val_1_rmse: 0.89349 |  0:00:42s
epoch 4  | loss: 0.74507 | val_0_rmse: 0.87155 | val_1_rmse: 0.87648 |  0:00:52s
epoch 5  | loss: 0.7431  | val_0_rmse: 0.8623  | val_1_rmse: 0.8678  |  0:01:03s
epoch 6  | loss: 0.74129 | val_0_rmse: 0.86386 | val_1_rmse: 0.86996 |  0:01:13s
epoch 7  | loss: 0.73945 | val_0_rmse: 0.85814 | val_1_rmse: 0.86402 |  0:01:24s
epoch 8  | loss: 0.73826 | val_0_rmse: 0.85655 | val_1_rmse: 0.8622  |  0:01:34s
epoch 9  | loss: 0.73804 | val_0_rmse: 0.85731 | val_1_rmse: 0.86467 |  0:01:45s
epoch 10 | loss: 0.73716 | val_0_rmse: 0.86386 | val_1_rmse: 0.90573 |  0:01:55s
epoch 11 | loss: 0.73635 | val_0_rmse: 0.85853 | val_1_rmse: 0.87277 |  0:02:06s
epoch 12 | loss: 0.73586 | val_0_rmse: 0.8775  | val_1_rmse: 0.9707  |  0:02:16s
epoch 13 | loss: 0.73532 | val_0_rmse: 0.88048 | val_1_rmse: 0.99554 |  0:02:27s
epoch 14 | loss: 0.73563 | val_0_rmse: 0.88261 | val_1_rmse: 1.00792 |  0:02:37s
epoch 15 | loss: 0.73624 | val_0_rmse: 0.86266 | val_1_rmse: 0.89676 |  0:02:47s
epoch 16 | loss: 0.73631 | val_0_rmse: 0.85849 | val_1_rmse: 0.86472 |  0:02:58s
epoch 17 | loss: 0.73551 | val_0_rmse: 0.85499 | val_1_rmse: 0.86109 |  0:03:08s
epoch 18 | loss: 0.73608 | val_0_rmse: 0.86081 | val_1_rmse: 0.86813 |  0:03:19s
epoch 19 | loss: 0.73545 | val_0_rmse: 0.86558 | val_1_rmse: 0.87161 |  0:03:29s
epoch 20 | loss: 0.73803 | val_0_rmse: 0.8718  | val_1_rmse: 0.87792 |  0:03:39s
epoch 21 | loss: 0.73636 | val_0_rmse: 0.94422 | val_1_rmse: 0.94318 |  0:03:50s
epoch 22 | loss: 0.73595 | val_0_rmse: 0.85494 | val_1_rmse: 0.86152 |  0:04:00s
epoch 23 | loss: 0.73439 | val_0_rmse: 0.85738 | val_1_rmse: 0.86373 |  0:04:11s
epoch 24 | loss: 0.73319 | val_0_rmse: 0.85776 | val_1_rmse: 0.86306 |  0:04:21s
epoch 25 | loss: 0.73676 | val_0_rmse: 0.8581  | val_1_rmse: 0.86488 |  0:04:32s
epoch 26 | loss: 0.73524 | val_0_rmse: 0.85689 | val_1_rmse: 0.8624  |  0:04:42s
epoch 27 | loss: 0.73545 | val_0_rmse: 0.85804 | val_1_rmse: 0.86413 |  0:04:53s
epoch 28 | loss: 0.74108 | val_0_rmse: 0.85912 | val_1_rmse: 0.86517 |  0:05:03s
epoch 29 | loss: 0.73448 | val_0_rmse: 0.85774 | val_1_rmse: 0.86332 |  0:05:14s
epoch 30 | loss: 0.73245 | val_0_rmse: 0.85545 | val_1_rmse: 0.86136 |  0:05:24s
epoch 31 | loss: 0.73307 | val_0_rmse: 0.85561 | val_1_rmse: 0.86236 |  0:05:35s
epoch 32 | loss: 0.73237 | val_0_rmse: 0.85554 | val_1_rmse: 0.86134 |  0:05:45s
epoch 33 | loss: 0.73239 | val_0_rmse: 0.85647 | val_1_rmse: 0.86203 |  0:05:56s
epoch 34 | loss: 0.73315 | val_0_rmse: 0.85548 | val_1_rmse: 0.86155 |  0:06:06s
epoch 35 | loss: 0.73163 | val_0_rmse: 0.85521 | val_1_rmse: 0.86185 |  0:06:17s
epoch 36 | loss: 0.73167 | val_0_rmse: 0.8545  | val_1_rmse: 0.86066 |  0:06:27s
epoch 37 | loss: 0.73173 | val_0_rmse: 0.85541 | val_1_rmse: 0.86107 |  0:06:38s
epoch 38 | loss: 0.73194 | val_0_rmse: 0.85763 | val_1_rmse: 0.86415 |  0:06:48s
epoch 39 | loss: 0.73236 | val_0_rmse: 0.85584 | val_1_rmse: 0.86233 |  0:06:59s
epoch 40 | loss: 0.73126 | val_0_rmse: 0.85539 | val_1_rmse: 0.8615  |  0:07:09s
epoch 41 | loss: 0.73092 | val_0_rmse: 0.85405 | val_1_rmse: 0.86026 |  0:07:20s
epoch 42 | loss: 0.73088 | val_0_rmse: 0.85436 | val_1_rmse: 0.86094 |  0:07:30s
epoch 43 | loss: 0.73189 | val_0_rmse: 0.85522 | val_1_rmse: 0.86187 |  0:07:41s
epoch 44 | loss: 0.73176 | val_0_rmse: 0.85507 | val_1_rmse: 0.86122 |  0:07:51s
epoch 45 | loss: 0.73356 | val_0_rmse: 0.85441 | val_1_rmse: 0.86085 |  0:08:02s
epoch 46 | loss: 0.73491 | val_0_rmse: 0.85489 | val_1_rmse: 0.86072 |  0:08:12s
epoch 47 | loss: 0.73481 | val_0_rmse: 0.85532 | val_1_rmse: 0.86221 |  0:08:23s
epoch 48 | loss: 0.73403 | val_0_rmse: 0.85537 | val_1_rmse: 0.86162 |  0:08:33s
epoch 49 | loss: 0.73321 | val_0_rmse: 0.85542 | val_1_rmse: 0.86235 |  0:08:44s
epoch 50 | loss: 0.73296 | val_0_rmse: 0.85642 | val_1_rmse: 0.86338 |  0:08:54s
epoch 51 | loss: 0.73503 | val_0_rmse: 0.85969 | val_1_rmse: 0.86538 |  0:09:05s
epoch 52 | loss: 0.73395 | val_0_rmse: 0.86329 | val_1_rmse: 0.86944 |  0:09:16s
epoch 53 | loss: 0.73222 | val_0_rmse: 0.85762 | val_1_rmse: 0.86368 |  0:09:26s
epoch 54 | loss: 0.73181 | val_0_rmse: 0.86087 | val_1_rmse: 0.86802 |  0:09:37s
epoch 55 | loss: 0.73234 | val_0_rmse: 0.85533 | val_1_rmse: 0.86204 |  0:09:47s
epoch 56 | loss: 0.73235 | val_0_rmse: 0.85493 | val_1_rmse: 0.86109 |  0:09:58s
epoch 57 | loss: 0.73487 | val_0_rmse: 0.86457 | val_1_rmse: 0.87467 |  0:10:08s
epoch 58 | loss: 0.73396 | val_0_rmse: 0.85491 | val_1_rmse: 0.86128 |  0:10:19s
epoch 59 | loss: 0.73295 | val_0_rmse: 0.85532 | val_1_rmse: 0.86135 |  0:10:29s
epoch 60 | loss: 0.7363  | val_0_rmse: 0.85942 | val_1_rmse: 0.86434 |  0:10:40s
epoch 61 | loss: 0.73588 | val_0_rmse: 0.85798 | val_1_rmse: 0.86426 |  0:10:50s
epoch 62 | loss: 0.73782 | val_0_rmse: 0.85804 | val_1_rmse: 0.86394 |  0:11:00s
epoch 63 | loss: 0.73636 | val_0_rmse: 0.85715 | val_1_rmse: 0.8629  |  0:11:11s
epoch 64 | loss: 0.73709 | val_0_rmse: 0.85885 | val_1_rmse: 0.86423 |  0:11:21s
epoch 65 | loss: 0.73715 | val_0_rmse: 0.85868 | val_1_rmse: 0.86438 |  0:11:32s
epoch 66 | loss: 0.7387  | val_0_rmse: 0.85804 | val_1_rmse: 0.86371 |  0:11:42s
epoch 67 | loss: 0.73901 | val_0_rmse: 0.85817 | val_1_rmse: 0.86399 |  0:11:53s
epoch 68 | loss: 0.73903 | val_0_rmse: 0.87724 | val_1_rmse: 0.88223 |  0:12:03s
epoch 69 | loss: 0.74525 | val_0_rmse: 0.8598  | val_1_rmse: 0.86628 |  0:12:14s
epoch 70 | loss: 0.73622 | val_0_rmse: 0.85755 | val_1_rmse: 0.86325 |  0:12:24s
epoch 71 | loss: 0.73651 | val_0_rmse: 0.85789 | val_1_rmse: 0.86416 |  0:12:35s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.86026
Best weights from best epoch are automatically used!
ended training at: 04:24:05
Feature importance:
Mean squared error is of 5024074178.726191
Mean absolute error:54859.06139921672
MAPE:0.5298665085834413
R2 score:0.2669426370054143
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:24:06
epoch 0  | loss: 1.01121 | val_0_rmse: 0.95242 | val_1_rmse: 0.94967 |  0:00:10s
epoch 1  | loss: 0.79565 | val_0_rmse: 0.93429 | val_1_rmse: 0.92952 |  0:00:21s
epoch 2  | loss: 0.77486 | val_0_rmse: 0.92427 | val_1_rmse: 0.91999 |  0:00:31s
epoch 3  | loss: 0.76471 | val_0_rmse: 0.90586 | val_1_rmse: 0.90073 |  0:00:42s
epoch 4  | loss: 0.76183 | val_0_rmse: 0.88288 | val_1_rmse: 0.88013 |  0:00:52s
epoch 5  | loss: 0.75821 | val_0_rmse: 0.87609 | val_1_rmse: 0.87404 |  0:01:03s
epoch 6  | loss: 0.75224 | val_0_rmse: 0.86504 | val_1_rmse: 0.86299 |  0:01:13s
epoch 7  | loss: 0.75393 | val_0_rmse: 0.86378 | val_1_rmse: 0.86043 |  0:01:24s
epoch 8  | loss: 0.75136 | val_0_rmse: 0.86517 | val_1_rmse: 0.86336 |  0:01:34s
epoch 9  | loss: 0.7487  | val_0_rmse: 0.86426 | val_1_rmse: 0.8627  |  0:01:45s
epoch 10 | loss: 0.74868 | val_0_rmse: 0.86201 | val_1_rmse: 0.86754 |  0:01:55s
epoch 11 | loss: 0.74885 | val_0_rmse: 0.86343 | val_1_rmse: 0.86511 |  0:02:05s
epoch 12 | loss: 0.74715 | val_0_rmse: 0.86268 | val_1_rmse: 0.87243 |  0:02:16s
epoch 13 | loss: 0.74779 | val_0_rmse: 0.86704 | val_1_rmse: 0.91308 |  0:02:27s
epoch 14 | loss: 0.74632 | val_0_rmse: 0.86697 | val_1_rmse: 0.95038 |  0:02:37s
epoch 15 | loss: 0.74601 | val_0_rmse: 0.86874 | val_1_rmse: 0.8664  |  0:02:48s
epoch 16 | loss: 0.74488 | val_0_rmse: 0.8609  | val_1_rmse: 0.86018 |  0:02:58s
epoch 17 | loss: 0.744   | val_0_rmse: 0.86292 | val_1_rmse: 0.86159 |  0:03:09s
epoch 18 | loss: 0.74413 | val_0_rmse: 0.86151 | val_1_rmse: 0.85853 |  0:03:19s
epoch 19 | loss: 0.74208 | val_0_rmse: 0.86173 | val_1_rmse: 0.86073 |  0:03:30s
epoch 20 | loss: 0.74234 | val_0_rmse: 0.86044 | val_1_rmse: 0.85892 |  0:03:40s
epoch 21 | loss: 0.74332 | val_0_rmse: 0.86728 | val_1_rmse: 0.86072 |  0:03:51s
epoch 22 | loss: 0.74427 | val_0_rmse: 0.86625 | val_1_rmse: 0.86507 |  0:04:01s
epoch 23 | loss: 0.74391 | val_0_rmse: 1.04607 | val_1_rmse: 0.85901 |  0:04:12s
epoch 24 | loss: 0.74287 | val_0_rmse: 0.86107 | val_1_rmse: 0.86034 |  0:04:22s
epoch 25 | loss: 0.74179 | val_0_rmse: 0.95434 | val_1_rmse: 0.86063 |  0:04:33s
epoch 26 | loss: 0.74459 | val_0_rmse: 0.8648  | val_1_rmse: 0.98548 |  0:04:43s
epoch 27 | loss: 0.74451 | val_0_rmse: 0.86916 | val_1_rmse: 0.86817 |  0:04:54s
epoch 28 | loss: 0.74359 | val_0_rmse: 0.8616  | val_1_rmse: 0.86023 |  0:05:04s
epoch 29 | loss: 0.74561 | val_0_rmse: 0.86284 | val_1_rmse: 1.05819 |  0:05:15s
epoch 30 | loss: 0.74858 | val_0_rmse: 0.86157 | val_1_rmse: 0.86017 |  0:05:25s
epoch 31 | loss: 0.748   | val_0_rmse: 0.86359 | val_1_rmse: 0.86242 |  0:05:36s
epoch 32 | loss: 0.74721 | val_0_rmse: 0.86224 | val_1_rmse: 0.86151 |  0:05:46s
epoch 33 | loss: 0.74546 | val_0_rmse: 0.862   | val_1_rmse: 0.86109 |  0:05:57s
epoch 34 | loss: 0.74394 | val_0_rmse: 0.86025 | val_1_rmse: 0.85804 |  0:06:07s
epoch 35 | loss: 0.74317 | val_0_rmse: 0.86221 | val_1_rmse: 0.85958 |  0:06:18s
epoch 36 | loss: 0.7484  | val_0_rmse: 0.86395 | val_1_rmse: 0.86294 |  0:06:28s
epoch 37 | loss: 0.75111 | val_0_rmse: 0.86811 | val_1_rmse: 0.86661 |  0:06:39s
epoch 38 | loss: 0.75335 | val_0_rmse: 0.87008 | val_1_rmse: 0.86887 |  0:06:49s
epoch 39 | loss: 0.74972 | val_0_rmse: 0.86249 | val_1_rmse: 0.86119 |  0:07:00s
epoch 40 | loss: 0.74612 | val_0_rmse: 0.86243 | val_1_rmse: 0.86084 |  0:07:10s
epoch 41 | loss: 0.74487 | val_0_rmse: 0.86569 | val_1_rmse: 0.86453 |  0:07:21s
epoch 42 | loss: 0.74416 | val_0_rmse: 0.86273 | val_1_rmse: 0.86187 |  0:07:32s
epoch 43 | loss: 0.74393 | val_0_rmse: 0.86269 | val_1_rmse: 0.86146 |  0:07:42s
epoch 44 | loss: 0.74418 | val_0_rmse: 0.8639  | val_1_rmse: 0.86249 |  0:07:53s
epoch 45 | loss: 0.74324 | val_0_rmse: 0.86219 | val_1_rmse: 0.86195 |  0:08:03s
epoch 46 | loss: 0.74381 | val_0_rmse: 0.86179 | val_1_rmse: 0.86119 |  0:08:13s
epoch 47 | loss: 0.74374 | val_0_rmse: 0.86169 | val_1_rmse: 0.86085 |  0:08:24s
epoch 48 | loss: 0.7444  | val_0_rmse: 0.86117 | val_1_rmse: 0.85986 |  0:08:34s
epoch 49 | loss: 0.74357 | val_0_rmse: 0.86204 | val_1_rmse: 0.86032 |  0:08:45s
epoch 50 | loss: 0.7435  | val_0_rmse: 0.86062 | val_1_rmse: 0.8593  |  0:08:55s
epoch 51 | loss: 0.74177 | val_0_rmse: 0.86091 | val_1_rmse: 0.85962 |  0:09:06s
epoch 52 | loss: 0.74234 | val_0_rmse: 0.86803 | val_1_rmse: 0.86674 |  0:09:16s
epoch 53 | loss: 0.74392 | val_0_rmse: 0.86091 | val_1_rmse: 0.85889 |  0:09:27s
epoch 54 | loss: 0.74285 | val_0_rmse: 0.86104 | val_1_rmse: 0.85915 |  0:09:37s
epoch 55 | loss: 0.74197 | val_0_rmse: 0.86081 | val_1_rmse: 0.85922 |  0:09:48s
epoch 56 | loss: 0.74445 | val_0_rmse: 0.86198 | val_1_rmse: 0.86201 |  0:09:59s
epoch 57 | loss: 0.74195 | val_0_rmse: 0.86075 | val_1_rmse: 0.86017 |  0:10:09s
epoch 58 | loss: 0.74307 | val_0_rmse: 0.85952 | val_1_rmse: 0.85862 |  0:10:20s
epoch 59 | loss: 0.74118 | val_0_rmse: 0.85925 | val_1_rmse: 0.85769 |  0:10:30s
epoch 60 | loss: 0.74327 | val_0_rmse: 0.8591  | val_1_rmse: 0.85785 |  0:10:41s
epoch 61 | loss: 0.73981 | val_0_rmse: 0.86133 | val_1_rmse: 0.86076 |  0:10:51s
epoch 62 | loss: 0.74062 | val_0_rmse: 0.85949 | val_1_rmse: 0.85818 |  0:11:02s
epoch 63 | loss: 0.73861 | val_0_rmse: 0.85937 | val_1_rmse: 0.85842 |  0:11:12s
epoch 64 | loss: 0.73988 | val_0_rmse: 0.85893 | val_1_rmse: 0.85811 |  0:11:23s
epoch 65 | loss: 0.73909 | val_0_rmse: 0.86926 | val_1_rmse: 0.86766 |  0:11:33s
epoch 66 | loss: 0.73968 | val_0_rmse: 0.86037 | val_1_rmse: 0.85978 |  0:11:44s
epoch 67 | loss: 0.73836 | val_0_rmse: 0.85775 | val_1_rmse: 0.85652 |  0:11:55s
epoch 68 | loss: 0.74045 | val_0_rmse: 0.85928 | val_1_rmse: 0.85843 |  0:12:05s
epoch 69 | loss: 0.73968 | val_0_rmse: 0.85886 | val_1_rmse: 0.85788 |  0:12:15s
epoch 70 | loss: 0.73746 | val_0_rmse: 0.85801 | val_1_rmse: 0.85695 |  0:12:26s
epoch 71 | loss: 0.73797 | val_0_rmse: 0.85828 | val_1_rmse: 0.85782 |  0:12:36s
epoch 72 | loss: 0.73854 | val_0_rmse: 0.8581  | val_1_rmse: 0.85723 |  0:12:47s
epoch 73 | loss: 0.73877 | val_0_rmse: 0.85808 | val_1_rmse: 0.85713 |  0:12:57s
epoch 74 | loss: 0.73883 | val_0_rmse: 0.85868 | val_1_rmse: 0.85722 |  0:13:08s
epoch 75 | loss: 0.74081 | val_0_rmse: 0.85927 | val_1_rmse: 0.85839 |  0:13:18s
epoch 76 | loss: 0.73883 | val_0_rmse: 0.86034 | val_1_rmse: 0.8598  |  0:13:29s
epoch 77 | loss: 0.74047 | val_0_rmse: 0.85938 | val_1_rmse: 0.85829 |  0:13:39s
epoch 78 | loss: 0.7435  | val_0_rmse: 0.86241 | val_1_rmse: 0.86076 |  0:13:50s
epoch 79 | loss: 0.74149 | val_0_rmse: 0.86222 | val_1_rmse: 0.86129 |  0:14:00s
epoch 80 | loss: 0.74168 | val_0_rmse: 0.86017 | val_1_rmse: 0.85882 |  0:14:11s
epoch 81 | loss: 0.74153 | val_0_rmse: 0.86075 | val_1_rmse: 0.8594  |  0:14:21s
epoch 82 | loss: 0.74083 | val_0_rmse: 0.86005 | val_1_rmse: 0.85906 |  0:14:32s
epoch 83 | loss: 0.74175 | val_0_rmse: 0.86119 | val_1_rmse: 0.8593  |  0:14:42s
epoch 84 | loss: 0.74193 | val_0_rmse: 0.85925 | val_1_rmse: 0.85751 |  0:14:53s
epoch 85 | loss: 0.74214 | val_0_rmse: 0.86112 | val_1_rmse: 0.85927 |  0:15:03s
epoch 86 | loss: 0.74031 | val_0_rmse: 0.85995 | val_1_rmse: 0.85885 |  0:15:14s
epoch 87 | loss: 0.74258 | val_0_rmse: 0.86132 | val_1_rmse: 0.86007 |  0:15:24s
epoch 88 | loss: 0.74116 | val_0_rmse: 0.85927 | val_1_rmse: 0.85801 |  0:15:35s
epoch 89 | loss: 0.74047 | val_0_rmse: 0.86104 | val_1_rmse: 0.8594  |  0:15:45s
epoch 90 | loss: 0.73989 | val_0_rmse: 0.85821 | val_1_rmse: 0.85715 |  0:15:56s
epoch 91 | loss: 0.73856 | val_0_rmse: 0.85937 | val_1_rmse: 0.85781 |  0:16:06s
epoch 92 | loss: 0.73917 | val_0_rmse: 0.85923 | val_1_rmse: 0.85784 |  0:16:17s
epoch 93 | loss: 0.74061 | val_0_rmse: 0.86011 | val_1_rmse: 0.85879 |  0:16:27s
epoch 94 | loss: 0.73955 | val_0_rmse: 0.85903 | val_1_rmse: 0.85804 |  0:16:38s
epoch 95 | loss: 0.73957 | val_0_rmse: 0.86055 | val_1_rmse: 0.85936 |  0:16:48s
epoch 96 | loss: 0.73975 | val_0_rmse: 0.85937 | val_1_rmse: 0.85829 |  0:16:59s
epoch 97 | loss: 0.74058 | val_0_rmse: 0.8609  | val_1_rmse: 0.8604  |  0:17:09s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.85652
Best weights from best epoch are automatically used!
ended training at: 04:41:20
Feature importance:
Mean squared error is of 4946954302.923058
Mean absolute error:54559.223749290904
MAPE:0.5276589492280269
R2 score:0.2644049821835971
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:41:21
epoch 0  | loss: 0.97076 | val_0_rmse: 0.92718 | val_1_rmse: 0.92482 |  0:00:10s
epoch 1  | loss: 0.77018 | val_0_rmse: 0.89992 | val_1_rmse: 0.89721 |  0:00:21s
epoch 2  | loss: 0.75776 | val_0_rmse: 0.89517 | val_1_rmse: 0.89265 |  0:00:31s
epoch 3  | loss: 0.75186 | val_0_rmse: 0.88561 | val_1_rmse: 0.8826  |  0:00:42s
epoch 4  | loss: 0.75075 | val_0_rmse: 0.87134 | val_1_rmse: 0.8712  |  0:00:52s
epoch 5  | loss: 0.74828 | val_0_rmse: 0.8631  | val_1_rmse: 0.86374 |  0:01:03s
epoch 6  | loss: 0.75246 | val_0_rmse: 0.85961 | val_1_rmse: 0.86197 |  0:01:13s
epoch 7  | loss: 0.74316 | val_0_rmse: 0.85943 | val_1_rmse: 0.86263 |  0:01:24s
epoch 8  | loss: 0.74358 | val_0_rmse: 0.85871 | val_1_rmse: 0.86153 |  0:01:34s
epoch 9  | loss: 0.74064 | val_0_rmse: 0.85988 | val_1_rmse: 0.86301 |  0:01:45s
epoch 10 | loss: 0.74049 | val_0_rmse: 0.85878 | val_1_rmse: 0.86209 |  0:01:55s
epoch 11 | loss: 0.74054 | val_0_rmse: 0.85712 | val_1_rmse: 0.86064 |  0:02:06s
epoch 12 | loss: 0.73856 | val_0_rmse: 0.85726 | val_1_rmse: 0.86018 |  0:02:16s
epoch 13 | loss: 0.73841 | val_0_rmse: 0.85799 | val_1_rmse: 0.86055 |  0:02:27s
epoch 14 | loss: 0.73907 | val_0_rmse: 0.86127 | val_1_rmse: 0.86327 |  0:02:37s
epoch 15 | loss: 0.74    | val_0_rmse: 0.86382 | val_1_rmse: 0.86181 |  0:02:48s
epoch 16 | loss: 0.73801 | val_0_rmse: 0.96873 | val_1_rmse: 0.8648  |  0:02:58s
epoch 17 | loss: 0.73872 | val_0_rmse: 0.96584 | val_1_rmse: 0.86281 |  0:03:09s
epoch 18 | loss: 0.73829 | val_0_rmse: 0.89942 | val_1_rmse: 0.86569 |  0:03:20s
epoch 19 | loss: 0.73796 | val_0_rmse: 0.90301 | val_1_rmse: 0.86179 |  0:03:30s
epoch 20 | loss: 0.73757 | val_0_rmse: 1.04704 | val_1_rmse: 0.86188 |  0:03:40s
epoch 21 | loss: 0.73761 | val_0_rmse: 1.04201 | val_1_rmse: 0.86053 |  0:03:51s
epoch 22 | loss: 0.73891 | val_0_rmse: 0.87155 | val_1_rmse: 0.86186 |  0:04:02s
epoch 23 | loss: 0.73995 | val_0_rmse: 0.95475 | val_1_rmse: 0.8598  |  0:04:12s
epoch 24 | loss: 0.73808 | val_0_rmse: 0.86161 | val_1_rmse: 0.86389 |  0:04:23s
epoch 25 | loss: 0.7379  | val_0_rmse: 0.8581  | val_1_rmse: 0.86182 |  0:04:33s
epoch 26 | loss: 0.73713 | val_0_rmse: 0.86057 | val_1_rmse: 1.02392 |  0:04:43s
epoch 27 | loss: 0.73611 | val_0_rmse: 0.85622 | val_1_rmse: 1.05545 |  0:04:54s
epoch 28 | loss: 0.73626 | val_0_rmse: 0.85989 | val_1_rmse: 0.871   |  0:05:04s
epoch 29 | loss: 0.73573 | val_0_rmse: 0.85686 | val_1_rmse: 0.86963 |  0:05:15s
epoch 30 | loss: 0.74242 | val_0_rmse: 0.86572 | val_1_rmse: 0.86892 |  0:05:25s
epoch 31 | loss: 0.74294 | val_0_rmse: 0.85941 | val_1_rmse: 0.86192 |  0:05:36s
epoch 32 | loss: 0.73887 | val_0_rmse: 0.85827 | val_1_rmse: 0.86096 |  0:05:46s
epoch 33 | loss: 0.73879 | val_0_rmse: 0.85827 | val_1_rmse: 0.8618  |  0:05:57s
epoch 34 | loss: 0.74517 | val_0_rmse: 0.86526 | val_1_rmse: 0.8678  |  0:06:07s
epoch 35 | loss: 0.74078 | val_0_rmse: 0.85866 | val_1_rmse: 0.86174 |  0:06:18s
epoch 36 | loss: 0.73933 | val_0_rmse: 0.85858 | val_1_rmse: 0.86155 |  0:06:28s
epoch 37 | loss: 0.73748 | val_0_rmse: 0.8572  | val_1_rmse: 0.86031 |  0:06:39s
epoch 38 | loss: 0.73705 | val_0_rmse: 0.90765 | val_1_rmse: 0.86367 |  0:06:49s
epoch 39 | loss: 0.73734 | val_0_rmse: 0.86005 | val_1_rmse: 0.86335 |  0:06:59s
epoch 40 | loss: 0.73901 | val_0_rmse: 0.85918 | val_1_rmse: 0.86326 |  0:07:10s
epoch 41 | loss: 0.74011 | val_0_rmse: 0.92118 | val_1_rmse: 0.86422 |  0:07:20s
epoch 42 | loss: 0.73839 | val_0_rmse: 0.87783 | val_1_rmse: 0.8974  |  0:07:31s
epoch 43 | loss: 0.73885 | val_0_rmse: 0.85952 | val_1_rmse: 0.97465 |  0:07:42s
epoch 44 | loss: 0.7391  | val_0_rmse: 0.85855 | val_1_rmse: 0.85991 |  0:07:52s
epoch 45 | loss: 0.73853 | val_0_rmse: 0.86997 | val_1_rmse: 0.88189 |  0:08:03s
epoch 46 | loss: 0.73966 | val_0_rmse: 0.90162 | val_1_rmse: 0.85969 |  0:08:13s
epoch 47 | loss: 0.74048 | val_0_rmse: 0.85911 | val_1_rmse: 0.86061 |  0:08:24s
epoch 48 | loss: 0.7391  | val_0_rmse: 0.8602  | val_1_rmse: 0.86213 |  0:08:34s
epoch 49 | loss: 0.73746 | val_0_rmse: 0.86068 | val_1_rmse: 0.86423 |  0:08:45s
epoch 50 | loss: 0.74394 | val_0_rmse: 0.86002 | val_1_rmse: 0.86286 |  0:08:55s
epoch 51 | loss: 0.7413  | val_0_rmse: 0.8605  | val_1_rmse: 0.86224 |  0:09:06s
epoch 52 | loss: 0.7403  | val_0_rmse: 0.85996 | val_1_rmse: 0.86285 |  0:09:16s
epoch 53 | loss: 0.73914 | val_0_rmse: 0.85984 | val_1_rmse: 0.86192 |  0:09:26s
epoch 54 | loss: 0.74085 | val_0_rmse: 0.85913 | val_1_rmse: 0.86244 |  0:09:37s
epoch 55 | loss: 0.73756 | val_0_rmse: 0.85872 | val_1_rmse: 0.8616  |  0:09:48s
epoch 56 | loss: 0.7371  | val_0_rmse: 0.85973 | val_1_rmse: 0.86312 |  0:09:58s
epoch 57 | loss: 0.73813 | val_0_rmse: 0.85893 | val_1_rmse: 0.86153 |  0:10:08s
epoch 58 | loss: 0.73699 | val_0_rmse: 0.85987 | val_1_rmse: 0.86238 |  0:10:19s
epoch 59 | loss: 0.73859 | val_0_rmse: 0.85921 | val_1_rmse: 0.86164 |  0:10:29s
epoch 60 | loss: 0.73663 | val_0_rmse: 0.857   | val_1_rmse: 0.85996 |  0:10:40s
epoch 61 | loss: 0.7348  | val_0_rmse: 0.8604  | val_1_rmse: 0.86328 |  0:10:50s
epoch 62 | loss: 0.73628 | val_0_rmse: 0.85746 | val_1_rmse: 0.86064 |  0:11:01s
epoch 63 | loss: 0.73634 | val_0_rmse: 1.05581 | val_1_rmse: 0.86499 |  0:11:11s
epoch 64 | loss: 0.73892 | val_0_rmse: 1.06866 | val_1_rmse: 0.90936 |  0:11:22s
epoch 65 | loss: 0.73911 | val_0_rmse: 0.85878 | val_1_rmse: 0.87028 |  0:11:32s
epoch 66 | loss: 0.73731 | val_0_rmse: 0.85852 | val_1_rmse: 0.8619  |  0:11:43s
epoch 67 | loss: 0.73912 | val_0_rmse: 0.87952 | val_1_rmse: 0.86316 |  0:11:53s
epoch 68 | loss: 0.7385  | val_0_rmse: 0.87012 | val_1_rmse: 0.86164 |  0:12:04s
epoch 69 | loss: 0.73869 | val_0_rmse: 0.89882 | val_1_rmse: 0.86457 |  0:12:14s
epoch 70 | loss: 0.73929 | val_0_rmse: 0.88384 | val_1_rmse: 0.86305 |  0:12:25s
epoch 71 | loss: 0.74131 | val_0_rmse: 0.90829 | val_1_rmse: 0.87348 |  0:12:35s
epoch 72 | loss: 0.74009 | val_0_rmse: 0.87326 | val_1_rmse: 0.86536 |  0:12:46s
epoch 73 | loss: 0.74135 | val_0_rmse: 0.8601  | val_1_rmse: 0.871   |  0:12:56s
epoch 74 | loss: 0.74057 | val_0_rmse: 0.85888 | val_1_rmse: 0.8623  |  0:13:07s
epoch 75 | loss: 0.73954 | val_0_rmse: 0.87712 | val_1_rmse: 0.87005 |  0:13:17s
epoch 76 | loss: 0.73922 | val_0_rmse: 0.87705 | val_1_rmse: 0.86267 |  0:13:28s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.85969
Best weights from best epoch are automatically used!
ended training at: 04:54:53
Feature importance:
Mean squared error is of 4978789282.760867
Mean absolute error:54340.27578561412
MAPE:0.5152632210485605
R2 score:0.25967899275691975
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:54:55
epoch 0  | loss: 0.97705 | val_0_rmse: 0.96377 | val_1_rmse: 0.96359 |  0:00:10s
epoch 1  | loss: 0.7694  | val_0_rmse: 0.92963 | val_1_rmse: 0.92895 |  0:00:21s
epoch 2  | loss: 0.75689 | val_0_rmse: 0.91327 | val_1_rmse: 0.90971 |  0:00:31s
epoch 3  | loss: 0.75277 | val_0_rmse: 0.88766 | val_1_rmse: 0.88985 |  0:00:42s
epoch 4  | loss: 0.75045 | val_0_rmse: 0.87762 | val_1_rmse: 0.87896 |  0:00:52s
epoch 5  | loss: 0.75093 | val_0_rmse: 0.86652 | val_1_rmse: 0.87049 |  0:01:03s
epoch 6  | loss: 0.74738 | val_0_rmse: 0.86298 | val_1_rmse: 0.86682 |  0:01:14s
epoch 7  | loss: 0.74521 | val_0_rmse: 0.85842 | val_1_rmse: 0.86254 |  0:01:24s
epoch 8  | loss: 0.74272 | val_0_rmse: 0.85894 | val_1_rmse: 0.86276 |  0:01:35s
epoch 9  | loss: 0.74229 | val_0_rmse: 0.85881 | val_1_rmse: 0.86327 |  0:01:45s
epoch 10 | loss: 0.74117 | val_0_rmse: 0.86139 | val_1_rmse: 0.86468 |  0:01:56s
epoch 11 | loss: 0.7403  | val_0_rmse: 0.85929 | val_1_rmse: 0.86271 |  0:02:06s
epoch 12 | loss: 0.7401  | val_0_rmse: 0.86008 | val_1_rmse: 0.86428 |  0:02:17s
epoch 13 | loss: 0.74038 | val_0_rmse: 0.85885 | val_1_rmse: 0.86178 |  0:02:28s
epoch 14 | loss: 0.74028 | val_0_rmse: 0.85977 | val_1_rmse: 0.86237 |  0:02:38s
epoch 15 | loss: 0.73862 | val_0_rmse: 0.85941 | val_1_rmse: 0.86263 |  0:02:49s
epoch 16 | loss: 0.74219 | val_0_rmse: 0.86012 | val_1_rmse: 0.86342 |  0:02:59s
epoch 17 | loss: 0.7399  | val_0_rmse: 0.8592  | val_1_rmse: 0.86266 |  0:03:10s
epoch 18 | loss: 0.73946 | val_0_rmse: 0.85906 | val_1_rmse: 0.86186 |  0:03:20s
epoch 19 | loss: 0.7393  | val_0_rmse: 0.85902 | val_1_rmse: 0.8624  |  0:03:31s
epoch 20 | loss: 0.73963 | val_0_rmse: 0.85839 | val_1_rmse: 0.86203 |  0:03:41s
epoch 21 | loss: 0.73966 | val_0_rmse: 0.8611  | val_1_rmse: 0.86411 |  0:03:52s
epoch 22 | loss: 0.74366 | val_0_rmse: 0.86133 | val_1_rmse: 0.86336 |  0:04:02s
epoch 23 | loss: 0.74322 | val_0_rmse: 0.85912 | val_1_rmse: 0.86187 |  0:04:13s
epoch 24 | loss: 0.7414  | val_0_rmse: 0.85999 | val_1_rmse: 0.86171 |  0:04:23s
epoch 25 | loss: 0.73964 | val_0_rmse: 0.85813 | val_1_rmse: 0.8606  |  0:04:34s
epoch 26 | loss: 0.73952 | val_0_rmse: 0.85849 | val_1_rmse: 0.86158 |  0:04:44s
epoch 27 | loss: 0.73944 | val_0_rmse: 0.85971 | val_1_rmse: 0.86195 |  0:04:55s
epoch 28 | loss: 0.73991 | val_0_rmse: 0.85998 | val_1_rmse: 0.86251 |  0:05:05s
epoch 29 | loss: 0.74131 | val_0_rmse: 0.86049 | val_1_rmse: 0.86344 |  0:05:16s
epoch 30 | loss: 0.73939 | val_0_rmse: 0.86209 | val_1_rmse: 0.86612 |  0:05:26s
epoch 31 | loss: 0.74102 | val_0_rmse: 0.86013 | val_1_rmse: 0.86262 |  0:05:37s
epoch 32 | loss: 0.741   | val_0_rmse: 0.86002 | val_1_rmse: 0.86312 |  0:05:48s
epoch 33 | loss: 0.74028 | val_0_rmse: 0.85957 | val_1_rmse: 0.87263 |  0:05:58s
epoch 34 | loss: 0.73918 | val_0_rmse: 0.85777 | val_1_rmse: 0.86222 |  0:06:09s
epoch 35 | loss: 0.73859 | val_0_rmse: 0.8577  | val_1_rmse: 0.86105 |  0:06:19s
epoch 36 | loss: 0.73868 | val_0_rmse: 0.85816 | val_1_rmse: 0.86164 |  0:06:30s
epoch 37 | loss: 0.73958 | val_0_rmse: 0.86272 | val_1_rmse: 0.86549 |  0:06:40s
epoch 38 | loss: 0.73703 | val_0_rmse: 0.85803 | val_1_rmse: 0.86089 |  0:06:51s
epoch 39 | loss: 0.73653 | val_0_rmse: 0.86177 | val_1_rmse: 0.86365 |  0:07:01s
epoch 40 | loss: 0.73697 | val_0_rmse: 0.85716 | val_1_rmse: 0.86078 |  0:07:12s
epoch 41 | loss: 0.73704 | val_0_rmse: 0.85841 | val_1_rmse: 0.86088 |  0:07:22s
epoch 42 | loss: 0.73652 | val_0_rmse: 0.85795 | val_1_rmse: 0.86178 |  0:07:33s
epoch 43 | loss: 0.73674 | val_0_rmse: 0.85991 | val_1_rmse: 0.86288 |  0:07:43s
epoch 44 | loss: 0.73797 | val_0_rmse: 0.85905 | val_1_rmse: 0.86271 |  0:07:54s
epoch 45 | loss: 0.73742 | val_0_rmse: 0.85856 | val_1_rmse: 0.86215 |  0:08:04s
epoch 46 | loss: 0.73835 | val_0_rmse: 0.86338 | val_1_rmse: 0.86668 |  0:08:15s
epoch 47 | loss: 0.7389  | val_0_rmse: 0.86276 | val_1_rmse: 0.86739 |  0:08:25s
epoch 48 | loss: 0.74437 | val_0_rmse: 0.87076 | val_1_rmse: 0.87106 |  0:08:36s
epoch 49 | loss: 0.74504 | val_0_rmse: 0.88381 | val_1_rmse: 0.88746 |  0:08:46s
epoch 50 | loss: 0.74048 | val_0_rmse: 0.86898 | val_1_rmse: 0.87197 |  0:08:57s
epoch 51 | loss: 0.7493  | val_0_rmse: 1.14495 | val_1_rmse: 1.14018 |  0:09:07s
epoch 52 | loss: 0.75042 | val_0_rmse: 0.86498 | val_1_rmse: 0.8679  |  0:09:18s
epoch 53 | loss: 0.7447  | val_0_rmse: 0.85985 | val_1_rmse: 0.86301 |  0:09:28s
epoch 54 | loss: 0.74085 | val_0_rmse: 0.86117 | val_1_rmse: 0.86196 |  0:09:39s
epoch 55 | loss: 0.73925 | val_0_rmse: 0.86082 | val_1_rmse: 0.8628  |  0:09:49s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.8606
Best weights from best epoch are automatically used!
ended training at: 05:04:48
Feature importance:
Mean squared error is of 4953627170.940807
Mean absolute error:54315.24354129739
MAPE:0.5230449532919006
R2 score:0.2605374535037799
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:04:50
epoch 0  | loss: 0.97368 | val_0_rmse: 1.00124 | val_1_rmse: 1.00661 |  0:00:10s
epoch 1  | loss: 0.76877 | val_0_rmse: 0.98092 | val_1_rmse: 0.98845 |  0:00:21s
epoch 2  | loss: 0.75701 | val_0_rmse: 0.91241 | val_1_rmse: 0.92006 |  0:00:31s
epoch 3  | loss: 0.74775 | val_0_rmse: 0.89987 | val_1_rmse: 0.91052 |  0:00:42s
epoch 4  | loss: 0.74533 | val_0_rmse: 0.87192 | val_1_rmse: 0.88521 |  0:00:52s
epoch 5  | loss: 0.74297 | val_0_rmse: 0.86591 | val_1_rmse: 0.88132 |  0:01:03s
epoch 6  | loss: 0.7401  | val_0_rmse: 0.85894 | val_1_rmse: 0.87234 |  0:01:13s
epoch 7  | loss: 0.74058 | val_0_rmse: 0.8598  | val_1_rmse: 0.87454 |  0:01:23s
epoch 8  | loss: 0.74049 | val_0_rmse: 0.85724 | val_1_rmse: 0.8718  |  0:01:34s
epoch 9  | loss: 0.7386  | val_0_rmse: 0.85616 | val_1_rmse: 0.87027 |  0:01:45s
epoch 10 | loss: 0.73611 | val_0_rmse: 0.85775 | val_1_rmse: 0.87312 |  0:01:55s
epoch 11 | loss: 0.73733 | val_0_rmse: 0.85924 | val_1_rmse: 0.87786 |  0:02:06s
epoch 12 | loss: 0.7365  | val_0_rmse: 0.85716 | val_1_rmse: 0.89095 |  0:02:16s
epoch 13 | loss: 0.7356  | val_0_rmse: 0.85882 | val_1_rmse: 0.88323 |  0:02:27s
epoch 14 | loss: 0.73679 | val_0_rmse: 0.87263 | val_1_rmse: 0.93088 |  0:02:37s
epoch 15 | loss: 0.73425 | val_0_rmse: 0.87347 | val_1_rmse: 0.90832 |  0:02:48s
epoch 16 | loss: 0.73424 | val_0_rmse: 2.61204 | val_1_rmse: 0.86982 |  0:02:58s
epoch 17 | loss: 0.73381 | val_0_rmse: 1.04455 | val_1_rmse: 1.48944 |  0:03:09s
epoch 18 | loss: 0.73327 | val_0_rmse: 0.85438 | val_1_rmse: 1.51354 |  0:03:19s
epoch 19 | loss: 0.73264 | val_0_rmse: 0.919   | val_1_rmse: 1.10685 |  0:03:30s
epoch 20 | loss: 0.73164 | val_0_rmse: 0.85427 | val_1_rmse: 0.8699  |  0:03:40s
epoch 21 | loss: 0.73256 | val_0_rmse: 0.85669 | val_1_rmse: 0.87161 |  0:03:51s
epoch 22 | loss: 0.73406 | val_0_rmse: 0.85408 | val_1_rmse: 0.86999 |  0:04:01s
epoch 23 | loss: 0.73461 | val_0_rmse: 0.85596 | val_1_rmse: 0.87079 |  0:04:12s
epoch 24 | loss: 0.73457 | val_0_rmse: 0.93621 | val_1_rmse: 0.97229 |  0:04:22s
epoch 25 | loss: 0.73393 | val_0_rmse: 0.87688 | val_1_rmse: 0.95087 |  0:04:33s
epoch 26 | loss: 0.73357 | val_0_rmse: 0.88277 | val_1_rmse: 0.9698  |  0:04:43s
epoch 27 | loss: 0.73516 | val_0_rmse: 0.89167 | val_1_rmse: 1.00343 |  0:04:54s
epoch 28 | loss: 0.73175 | val_0_rmse: 0.90436 | val_1_rmse: 1.05067 |  0:05:04s
epoch 29 | loss: 0.73155 | val_0_rmse: 0.88348 | val_1_rmse: 0.97397 |  0:05:15s
epoch 30 | loss: 0.73411 | val_0_rmse: 0.88428 | val_1_rmse: 0.96942 |  0:05:25s
epoch 31 | loss: 0.73443 | val_0_rmse: 0.8877  | val_1_rmse: 0.98435 |  0:05:36s
epoch 32 | loss: 0.73439 | val_0_rmse: 0.87573 | val_1_rmse: 0.94562 |  0:05:46s
epoch 33 | loss: 0.73374 | val_0_rmse: 0.87986 | val_1_rmse: 0.95815 |  0:05:57s
epoch 34 | loss: 0.73352 | val_0_rmse: 0.87162 | val_1_rmse: 0.92313 |  0:06:07s
epoch 35 | loss: 0.73241 | val_0_rmse: 0.92493 | val_1_rmse: 1.11485 |  0:06:17s
epoch 36 | loss: 0.7321  | val_0_rmse: 0.95016 | val_1_rmse: 1.19746 |  0:06:28s
epoch 37 | loss: 0.7374  | val_0_rmse: 0.86449 | val_1_rmse: 0.89191 |  0:06:38s
epoch 38 | loss: 0.73598 | val_0_rmse: 0.87436 | val_1_rmse: 0.89719 |  0:06:49s
epoch 39 | loss: 0.73346 | val_0_rmse: 0.86397 | val_1_rmse: 0.88541 |  0:06:59s
epoch 40 | loss: 0.73216 | val_0_rmse: 0.85846 | val_1_rmse: 0.87745 |  0:07:10s
epoch 41 | loss: 0.73401 | val_0_rmse: 0.86769 | val_1_rmse: 0.88856 |  0:07:20s
epoch 42 | loss: 0.73629 | val_0_rmse: 0.86018 | val_1_rmse: 0.88552 |  0:07:31s
epoch 43 | loss: 0.73346 | val_0_rmse: 0.86016 | val_1_rmse: 0.88228 |  0:07:42s
epoch 44 | loss: 0.7342  | val_0_rmse: 0.86057 | val_1_rmse: 0.88868 |  0:07:52s
epoch 45 | loss: 0.73355 | val_0_rmse: 0.8619  | val_1_rmse: 0.88849 |  0:08:03s
epoch 46 | loss: 0.73441 | val_0_rmse: 0.86017 | val_1_rmse: 0.88555 |  0:08:13s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.86982
Best weights from best epoch are automatically used!
ended training at: 05:13:07
Feature importance:
Mean squared error is of 4986191540.448665
Mean absolute error:54467.42235248919
MAPE:0.5206336149980091
R2 score:0.273369211514516
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:08
epoch 0  | loss: 1.02303 | val_0_rmse: 0.95338 | val_1_rmse: 0.95521 |  0:00:10s
epoch 1  | loss: 0.77767 | val_0_rmse: 0.92402 | val_1_rmse: 0.92397 |  0:00:21s
epoch 2  | loss: 0.76363 | val_0_rmse: 0.89417 | val_1_rmse: 0.89495 |  0:00:31s
epoch 3  | loss: 0.75665 | val_0_rmse: 0.8835  | val_1_rmse: 0.8842  |  0:00:42s
epoch 4  | loss: 0.75349 | val_0_rmse: 0.8724  | val_1_rmse: 0.87277 |  0:00:52s
epoch 5  | loss: 0.74842 | val_0_rmse: 0.86719 | val_1_rmse: 0.87031 |  0:01:03s
epoch 6  | loss: 0.7468  | val_0_rmse: 0.86074 | val_1_rmse: 0.86242 |  0:01:13s
epoch 7  | loss: 0.74506 | val_0_rmse: 0.85977 | val_1_rmse: 0.86169 |  0:01:24s
epoch 8  | loss: 0.74569 | val_0_rmse: 0.8627  | val_1_rmse: 0.86525 |  0:01:34s
epoch 9  | loss: 0.74571 | val_0_rmse: 0.86236 | val_1_rmse: 0.8661  |  0:01:45s
epoch 10 | loss: 0.74249 | val_0_rmse: 0.88025 | val_1_rmse: 0.87416 |  0:01:56s
epoch 11 | loss: 0.74312 | val_0_rmse: 0.86029 | val_1_rmse: 0.86586 |  0:02:06s
epoch 12 | loss: 0.73926 | val_0_rmse: 0.85873 | val_1_rmse: 0.86212 |  0:02:17s
epoch 13 | loss: 0.73829 | val_0_rmse: 0.86881 | val_1_rmse: 0.86808 |  0:02:27s
epoch 14 | loss: 0.73884 | val_0_rmse: 0.86254 | val_1_rmse: 0.86049 |  0:02:38s
epoch 15 | loss: 0.73774 | val_0_rmse: 0.85664 | val_1_rmse: 0.85868 |  0:02:48s
epoch 16 | loss: 0.73814 | val_0_rmse: 0.86211 | val_1_rmse: 0.85937 |  0:02:59s
epoch 17 | loss: 0.73674 | val_0_rmse: 0.86832 | val_1_rmse: 0.90111 |  0:03:09s
epoch 18 | loss: 0.7379  | val_0_rmse: 0.85668 | val_1_rmse: 0.85941 |  0:03:20s
epoch 19 | loss: 0.73685 | val_0_rmse: 0.85729 | val_1_rmse: 0.85966 |  0:03:30s
epoch 20 | loss: 0.73725 | val_0_rmse: 0.85802 | val_1_rmse: 0.86111 |  0:03:41s
epoch 21 | loss: 0.73758 | val_0_rmse: 0.85927 | val_1_rmse: 0.86198 |  0:03:51s
epoch 22 | loss: 0.73698 | val_0_rmse: 0.87598 | val_1_rmse: 0.88512 |  0:04:02s
epoch 23 | loss: 0.73729 | val_0_rmse: 0.85874 | val_1_rmse: 0.87938 |  0:04:13s
epoch 24 | loss: 0.73672 | val_0_rmse: 0.85612 | val_1_rmse: 0.85843 |  0:04:23s
epoch 25 | loss: 0.7348  | val_0_rmse: 0.85631 | val_1_rmse: 0.85951 |  0:04:34s
epoch 26 | loss: 0.735   | val_0_rmse: 0.85814 | val_1_rmse: 0.86157 |  0:04:44s
epoch 27 | loss: 0.73337 | val_0_rmse: 0.85503 | val_1_rmse: 0.86227 |  0:04:55s
epoch 28 | loss: 0.73369 | val_0_rmse: 0.85519 | val_1_rmse: 0.86453 |  0:05:05s
epoch 29 | loss: 0.73503 | val_0_rmse: 0.85576 | val_1_rmse: 0.85867 |  0:05:16s
epoch 30 | loss: 0.74074 | val_0_rmse: 0.85839 | val_1_rmse: 0.99727 |  0:05:27s
epoch 31 | loss: 0.73835 | val_0_rmse: 0.86087 | val_1_rmse: 0.951   |  0:05:37s
epoch 32 | loss: 0.73923 | val_0_rmse: 0.86375 | val_1_rmse: 0.86236 |  0:05:47s
epoch 33 | loss: 0.73753 | val_0_rmse: 0.85728 | val_1_rmse: 0.86012 |  0:05:58s
epoch 34 | loss: 0.73626 | val_0_rmse: 0.85729 | val_1_rmse: 0.86049 |  0:06:08s
epoch 35 | loss: 0.73664 | val_0_rmse: 0.85821 | val_1_rmse: 0.86225 |  0:06:19s
epoch 36 | loss: 0.73746 | val_0_rmse: 0.87034 | val_1_rmse: 0.86111 |  0:06:29s
epoch 37 | loss: 0.7376  | val_0_rmse: 0.85789 | val_1_rmse: 0.86116 |  0:06:40s
epoch 38 | loss: 0.73669 | val_0_rmse: 0.85796 | val_1_rmse: 0.85984 |  0:06:50s
epoch 39 | loss: 0.73608 | val_0_rmse: 0.85712 | val_1_rmse: 0.86028 |  0:07:01s
epoch 40 | loss: 0.73324 | val_0_rmse: 0.8562  | val_1_rmse: 0.85858 |  0:07:11s
epoch 41 | loss: 0.73579 | val_0_rmse: 0.85682 | val_1_rmse: 0.8613  |  0:07:22s
epoch 42 | loss: 0.73397 | val_0_rmse: 0.85462 | val_1_rmse: 0.85716 |  0:07:32s
epoch 43 | loss: 0.7328  | val_0_rmse: 0.85518 | val_1_rmse: 0.8582  |  0:07:43s
epoch 44 | loss: 0.73307 | val_0_rmse: 0.85852 | val_1_rmse: 0.86067 |  0:07:53s
epoch 45 | loss: 0.7354  | val_0_rmse: 0.855   | val_1_rmse: 0.85775 |  0:08:04s
epoch 46 | loss: 0.7332  | val_0_rmse: 0.9988  | val_1_rmse: 0.95752 |  0:08:14s
epoch 47 | loss: 0.73368 | val_0_rmse: 0.85505 | val_1_rmse: 0.85765 |  0:08:24s
epoch 48 | loss: 0.73452 | val_0_rmse: 0.8554  | val_1_rmse: 0.86205 |  0:08:35s
epoch 49 | loss: 0.73392 | val_0_rmse: 0.85519 | val_1_rmse: 0.86314 |  0:08:45s
epoch 50 | loss: 0.73247 | val_0_rmse: 0.8563  | val_1_rmse: 0.85868 |  0:08:56s
epoch 51 | loss: 0.73326 | val_0_rmse: 0.85608 | val_1_rmse: 0.85909 |  0:09:06s
epoch 52 | loss: 0.73393 | val_0_rmse: 0.85508 | val_1_rmse: 0.85881 |  0:09:17s
epoch 53 | loss: 0.73386 | val_0_rmse: 0.85593 | val_1_rmse: 0.85962 |  0:09:27s
epoch 54 | loss: 0.73279 | val_0_rmse: 0.85429 | val_1_rmse: 0.85672 |  0:09:38s
epoch 55 | loss: 0.73294 | val_0_rmse: 0.85515 | val_1_rmse: 0.85738 |  0:09:48s
epoch 56 | loss: 0.73573 | val_0_rmse: 0.86428 | val_1_rmse: 0.8657  |  0:09:59s
epoch 57 | loss: 0.74562 | val_0_rmse: 0.86187 | val_1_rmse: 0.86883 |  0:10:09s
epoch 58 | loss: 0.74204 | val_0_rmse: 0.86337 | val_1_rmse: 0.86488 |  0:10:20s
epoch 59 | loss: 0.73925 | val_0_rmse: 0.85708 | val_1_rmse: 0.85935 |  0:10:30s
epoch 60 | loss: 0.73672 | val_0_rmse: 0.85714 | val_1_rmse: 0.85982 |  0:10:40s
epoch 61 | loss: 0.73527 | val_0_rmse: 0.85603 | val_1_rmse: 0.8582  |  0:10:51s
epoch 62 | loss: 0.7352  | val_0_rmse: 0.86086 | val_1_rmse: 0.85893 |  0:11:02s
epoch 63 | loss: 0.73429 | val_0_rmse: 0.86151 | val_1_rmse: 0.85763 |  0:11:12s
epoch 64 | loss: 0.73347 | val_0_rmse: 0.86231 | val_1_rmse: 0.85696 |  0:11:23s
epoch 65 | loss: 0.73311 | val_0_rmse: 0.86074 | val_1_rmse: 0.85928 |  0:11:33s
epoch 66 | loss: 0.73267 | val_0_rmse: 0.85882 | val_1_rmse: 0.85696 |  0:11:44s
epoch 67 | loss: 0.73224 | val_0_rmse: 0.85918 | val_1_rmse: 0.85778 |  0:11:54s
epoch 68 | loss: 0.733   | val_0_rmse: 0.8598  | val_1_rmse: 0.85851 |  0:12:04s
epoch 69 | loss: 0.7326  | val_0_rmse: 0.85798 | val_1_rmse: 0.85734 |  0:12:15s
epoch 70 | loss: 0.73296 | val_0_rmse: 0.85535 | val_1_rmse: 0.85756 |  0:12:25s
epoch 71 | loss: 0.7335  | val_0_rmse: 0.85664 | val_1_rmse: 1.03591 |  0:12:36s
epoch 72 | loss: 0.73342 | val_0_rmse: 0.85563 | val_1_rmse: 1.03809 |  0:12:46s
epoch 73 | loss: 0.73407 | val_0_rmse: 0.85662 | val_1_rmse: 0.87771 |  0:12:57s
epoch 74 | loss: 0.73327 | val_0_rmse: 0.85444 | val_1_rmse: 0.86396 |  0:13:07s
epoch 75 | loss: 0.73259 | val_0_rmse: 0.85572 | val_1_rmse: 0.85652 |  0:13:18s
epoch 76 | loss: 0.73328 | val_0_rmse: 0.85456 | val_1_rmse: 0.85651 |  0:13:28s
epoch 77 | loss: 0.7323  | val_0_rmse: 0.85496 | val_1_rmse: 0.90481 |  0:13:39s
epoch 78 | loss: 0.73304 | val_0_rmse: 0.85608 | val_1_rmse: 0.92715 |  0:13:49s
epoch 79 | loss: 0.73202 | val_0_rmse: 0.85629 | val_1_rmse: 0.90966 |  0:14:00s
epoch 80 | loss: 0.73272 | val_0_rmse: 0.85498 | val_1_rmse: 0.94443 |  0:14:10s
epoch 81 | loss: 0.73212 | val_0_rmse: 0.85478 | val_1_rmse: 0.91919 |  0:14:21s
epoch 82 | loss: 0.73186 | val_0_rmse: 0.85471 | val_1_rmse: 0.9309  |  0:14:31s
epoch 83 | loss: 0.73207 | val_0_rmse: 0.85471 | val_1_rmse: 0.9244  |  0:14:42s
epoch 84 | loss: 0.73172 | val_0_rmse: 0.85473 | val_1_rmse: 0.93757 |  0:14:52s
epoch 85 | loss: 0.73159 | val_0_rmse: 0.86084 | val_1_rmse: 0.86225 |  0:15:03s
epoch 86 | loss: 0.73162 | val_0_rmse: 0.86238 | val_1_rmse: 0.97323 |  0:15:13s
epoch 87 | loss: 0.73139 | val_0_rmse: 0.85451 | val_1_rmse: 0.94353 |  0:15:24s
epoch 88 | loss: 0.73152 | val_0_rmse: 0.85522 | val_1_rmse: 0.87665 |  0:15:34s
epoch 89 | loss: 0.73205 | val_0_rmse: 0.85474 | val_1_rmse: 0.98642 |  0:15:45s
epoch 90 | loss: 0.73358 | val_0_rmse: 0.85514 | val_1_rmse: 0.98103 |  0:15:56s
epoch 91 | loss: 0.73314 | val_0_rmse: 0.85627 | val_1_rmse: 1.24968 |  0:16:06s
epoch 92 | loss: 0.73192 | val_0_rmse: 0.85483 | val_1_rmse: 1.07753 |  0:16:17s
epoch 93 | loss: 0.73198 | val_0_rmse: 0.85486 | val_1_rmse: 0.88878 |  0:16:27s
epoch 94 | loss: 0.73197 | val_0_rmse: 0.85443 | val_1_rmse: 0.87518 |  0:16:38s
epoch 95 | loss: 0.73246 | val_0_rmse: 0.85469 | val_1_rmse: 0.87963 |  0:16:48s
epoch 96 | loss: 0.73207 | val_0_rmse: 0.85869 | val_1_rmse: 0.93021 |  0:16:59s
epoch 97 | loss: 0.73583 | val_0_rmse: 0.8593  | val_1_rmse: 0.86177 |  0:17:09s
epoch 98 | loss: 0.73647 | val_0_rmse: 0.85669 | val_1_rmse: 1.29246 |  0:17:20s
epoch 99 | loss: 0.73555 | val_0_rmse: 0.8582  | val_1_rmse: 0.99089 |  0:17:30s
epoch 100| loss: 0.73501 | val_0_rmse: 0.85666 | val_1_rmse: 0.95834 |  0:17:41s
epoch 101| loss: 0.73467 | val_0_rmse: 0.85662 | val_1_rmse: 0.99781 |  0:17:51s
epoch 102| loss: 0.73573 | val_0_rmse: 0.85727 | val_1_rmse: 1.14042 |  0:18:02s
epoch 103| loss: 0.73624 | val_0_rmse: 0.85605 | val_1_rmse: 1.14377 |  0:18:13s
epoch 104| loss: 0.73456 | val_0_rmse: 0.85625 | val_1_rmse: 1.15427 |  0:18:23s
epoch 105| loss: 0.73457 | val_0_rmse: 0.856   | val_1_rmse: 1.14726 |  0:18:34s
epoch 106| loss: 0.73573 | val_0_rmse: 0.85623 | val_1_rmse: 1.15326 |  0:18:44s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.85651
Best weights from best epoch are automatically used!
ended training at: 05:31:57
Feature importance:
Mean squared error is of 5038756293.72876
Mean absolute error:55059.029297749636
MAPE:0.5293816843800766
R2 score:0.26406855166988985
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:31:58
epoch 0  | loss: 0.94208 | val_0_rmse: 0.99258 | val_1_rmse: 0.98519 |  0:00:10s
epoch 1  | loss: 0.76919 | val_0_rmse: 0.92647 | val_1_rmse: 0.91734 |  0:00:21s
epoch 2  | loss: 0.75787 | val_0_rmse: 0.89835 | val_1_rmse: 0.88856 |  0:00:31s
epoch 3  | loss: 0.75346 | val_0_rmse: 0.89026 | val_1_rmse: 0.88034 |  0:00:42s
epoch 4  | loss: 0.75205 | val_0_rmse: 0.87315 | val_1_rmse: 0.86273 |  0:00:52s
epoch 5  | loss: 0.75082 | val_0_rmse: 0.86948 | val_1_rmse: 0.85934 |  0:01:03s
epoch 6  | loss: 0.74765 | val_0_rmse: 0.86234 | val_1_rmse: 0.85069 |  0:01:13s
epoch 7  | loss: 0.75445 | val_0_rmse: 0.89201 | val_1_rmse: 0.88218 |  0:01:24s
epoch 8  | loss: 0.75879 | val_0_rmse: 0.86277 | val_1_rmse: 0.85062 |  0:01:35s
epoch 9  | loss: 0.74893 | val_0_rmse: 0.86662 | val_1_rmse: 0.85263 |  0:01:45s
epoch 10 | loss: 0.74685 | val_0_rmse: 0.86534 | val_1_rmse: 0.85226 |  0:01:56s
epoch 11 | loss: 0.74479 | val_0_rmse: 0.87135 | val_1_rmse: 0.85812 |  0:02:06s
epoch 12 | loss: 0.74333 | val_0_rmse: 0.86849 | val_1_rmse: 0.85703 |  0:02:17s
epoch 13 | loss: 0.74233 | val_0_rmse: 0.86142 | val_1_rmse: 0.84846 |  0:02:27s
epoch 14 | loss: 0.74262 | val_0_rmse: 0.86258 | val_1_rmse: 0.85045 |  0:02:38s
epoch 15 | loss: 0.74111 | val_0_rmse: 0.86568 | val_1_rmse: 0.8593  |  0:02:48s
epoch 16 | loss: 0.74204 | val_0_rmse: 0.86921 | val_1_rmse: 0.85208 |  0:02:59s
epoch 17 | loss: 0.74227 | val_0_rmse: 0.86116 | val_1_rmse: 0.8504  |  0:03:09s
epoch 18 | loss: 0.74221 | val_0_rmse: 0.86169 | val_1_rmse: 0.85201 |  0:03:20s
epoch 19 | loss: 0.74109 | val_0_rmse: 0.86027 | val_1_rmse: 0.84958 |  0:03:30s
epoch 20 | loss: 0.74147 | val_0_rmse: 0.85996 | val_1_rmse: 0.84923 |  0:03:41s
epoch 21 | loss: 0.74067 | val_0_rmse: 0.86076 | val_1_rmse: 0.84962 |  0:03:51s
epoch 22 | loss: 0.74134 | val_0_rmse: 0.86135 | val_1_rmse: 0.85068 |  0:04:02s
epoch 23 | loss: 0.74223 | val_0_rmse: 0.85964 | val_1_rmse: 0.84939 |  0:04:12s
epoch 24 | loss: 0.73974 | val_0_rmse: 0.86614 | val_1_rmse: 0.85744 |  0:04:23s
epoch 25 | loss: 0.74175 | val_0_rmse: 0.87504 | val_1_rmse: 0.86838 |  0:04:33s
epoch 26 | loss: 0.74123 | val_0_rmse: 0.94917 | val_1_rmse: 0.96875 |  0:04:44s
epoch 27 | loss: 0.74043 | val_0_rmse: 0.87264 | val_1_rmse: 0.86634 |  0:04:54s
epoch 28 | loss: 0.74145 | val_0_rmse: 0.87035 | val_1_rmse: 0.86069 |  0:05:05s
epoch 29 | loss: 0.75486 | val_0_rmse: 0.86953 | val_1_rmse: 0.86063 |  0:05:16s
epoch 30 | loss: 0.74667 | val_0_rmse: 0.8725  | val_1_rmse: 0.86642 |  0:05:26s
epoch 31 | loss: 0.74571 | val_0_rmse: 0.86247 | val_1_rmse: 0.85286 |  0:05:37s
epoch 32 | loss: 0.74346 | val_0_rmse: 0.86281 | val_1_rmse: 0.85346 |  0:05:47s
epoch 33 | loss: 0.7414  | val_0_rmse: 0.86172 | val_1_rmse: 0.85208 |  0:05:58s
epoch 34 | loss: 0.74166 | val_0_rmse: 0.8608  | val_1_rmse: 0.85109 |  0:06:08s
epoch 35 | loss: 0.74155 | val_0_rmse: 0.86247 | val_1_rmse: 0.85495 |  0:06:19s
epoch 36 | loss: 0.73974 | val_0_rmse: 0.85991 | val_1_rmse: 0.85007 |  0:06:29s
epoch 37 | loss: 0.73832 | val_0_rmse: 0.85918 | val_1_rmse: 0.84877 |  0:06:40s
epoch 38 | loss: 0.739   | val_0_rmse: 0.86028 | val_1_rmse: 0.85023 |  0:06:50s
epoch 39 | loss: 0.7382  | val_0_rmse: 0.85832 | val_1_rmse: 0.84774 |  0:07:01s
epoch 40 | loss: 0.73838 | val_0_rmse: 0.86252 | val_1_rmse: 0.84997 |  0:07:12s
epoch 41 | loss: 0.73769 | val_0_rmse: 0.86162 | val_1_rmse: 0.85703 |  0:07:22s
epoch 42 | loss: 0.73783 | val_0_rmse: 0.86775 | val_1_rmse: 0.86534 |  0:07:33s
epoch 43 | loss: 0.73796 | val_0_rmse: 0.85871 | val_1_rmse: 0.84944 |  0:07:43s
epoch 44 | loss: 0.73823 | val_0_rmse: 0.86257 | val_1_rmse: 0.85469 |  0:07:54s
epoch 45 | loss: 0.73854 | val_0_rmse: 0.85894 | val_1_rmse: 0.84924 |  0:08:04s
epoch 46 | loss: 0.73961 | val_0_rmse: 0.85963 | val_1_rmse: 0.84982 |  0:08:15s
epoch 47 | loss: 0.74063 | val_0_rmse: 0.86104 | val_1_rmse: 0.85166 |  0:08:25s
epoch 48 | loss: 0.74071 | val_0_rmse: 0.86191 | val_1_rmse: 0.85209 |  0:08:36s
epoch 49 | loss: 0.73969 | val_0_rmse: 0.85912 | val_1_rmse: 0.84868 |  0:08:47s
epoch 50 | loss: 0.74061 | val_0_rmse: 0.85904 | val_1_rmse: 0.84878 |  0:08:57s
epoch 51 | loss: 0.73912 | val_0_rmse: 0.86185 | val_1_rmse: 0.85175 |  0:09:08s
epoch 52 | loss: 0.74013 | val_0_rmse: 0.85953 | val_1_rmse: 0.85025 |  0:09:18s
epoch 53 | loss: 0.73951 | val_0_rmse: 0.86109 | val_1_rmse: 0.8521  |  0:09:29s
epoch 54 | loss: 0.73972 | val_0_rmse: 0.86091 | val_1_rmse: 0.8507  |  0:09:40s
epoch 55 | loss: 0.73955 | val_0_rmse: 0.8595  | val_1_rmse: 0.84992 |  0:09:50s
epoch 56 | loss: 0.73936 | val_0_rmse: 0.85996 | val_1_rmse: 0.84954 |  0:10:01s
epoch 57 | loss: 0.7392  | val_0_rmse: 0.85946 | val_1_rmse: 0.84904 |  0:10:11s
epoch 58 | loss: 0.73942 | val_0_rmse: 0.85917 | val_1_rmse: 0.84882 |  0:10:22s
epoch 59 | loss: 0.74024 | val_0_rmse: 0.86111 | val_1_rmse: 0.85077 |  0:10:33s
epoch 60 | loss: 0.73984 | val_0_rmse: 0.86033 | val_1_rmse: 0.85054 |  0:10:43s
epoch 61 | loss: 0.73911 | val_0_rmse: 0.859   | val_1_rmse: 0.84838 |  0:10:53s
epoch 62 | loss: 0.73885 | val_0_rmse: 0.85887 | val_1_rmse: 0.84779 |  0:11:04s
epoch 63 | loss: 0.74065 | val_0_rmse: 0.8595  | val_1_rmse: 0.84884 |  0:11:15s
epoch 64 | loss: 0.74072 | val_0_rmse: 0.86939 | val_1_rmse: 0.87443 |  0:11:25s
epoch 65 | loss: 0.7405  | val_0_rmse: 0.8626  | val_1_rmse: 0.85987 |  0:11:36s
epoch 66 | loss: 0.74063 | val_0_rmse: 0.89399 | val_1_rmse: 0.90178 |  0:11:46s
epoch 67 | loss: 0.74127 | val_0_rmse: 0.8623  | val_1_rmse: 0.85279 |  0:11:57s
epoch 68 | loss: 0.74041 | val_0_rmse: 0.86255 | val_1_rmse: 0.84775 |  0:12:07s
epoch 69 | loss: 0.74227 | val_0_rmse: 0.86827 | val_1_rmse: 0.84962 |  0:12:18s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.84774
Best weights from best epoch are automatically used!
ended training at: 05:44:20
Feature importance:
Mean squared error is of 5041124554.76281
Mean absolute error:54903.6714443901
MAPE:0.513524871956091
R2 score:0.25879222930106716
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:44:22
epoch 0  | loss: 0.99365 | val_0_rmse: 0.93047 | val_1_rmse: 0.93796 |  0:00:10s
epoch 1  | loss: 0.77108 | val_0_rmse: 0.91722 | val_1_rmse: 0.92401 |  0:00:21s
epoch 2  | loss: 0.75643 | val_0_rmse: 0.89168 | val_1_rmse: 0.90092 |  0:00:31s
epoch 3  | loss: 0.74963 | val_0_rmse: 0.87737 | val_1_rmse: 0.88357 |  0:00:42s
epoch 4  | loss: 0.74753 | val_0_rmse: 0.86718 | val_1_rmse: 0.8753  |  0:00:52s
epoch 5  | loss: 0.74415 | val_0_rmse: 0.86762 | val_1_rmse: 0.87486 |  0:01:03s
epoch 6  | loss: 0.74252 | val_0_rmse: 0.85786 | val_1_rmse: 0.86715 |  0:01:13s
epoch 7  | loss: 0.74125 | val_0_rmse: 0.85983 | val_1_rmse: 0.86954 |  0:01:24s
epoch 8  | loss: 0.74202 | val_0_rmse: 0.85875 | val_1_rmse: 0.8668  |  0:01:35s
epoch 9  | loss: 0.74289 | val_0_rmse: 0.86155 | val_1_rmse: 0.871   |  0:01:45s
epoch 10 | loss: 0.74085 | val_0_rmse: 0.85734 | val_1_rmse: 0.86645 |  0:01:56s
epoch 11 | loss: 0.73768 | val_0_rmse: 0.857   | val_1_rmse: 0.86655 |  0:02:07s
epoch 12 | loss: 0.73618 | val_0_rmse: 0.85565 | val_1_rmse: 0.86395 |  0:02:17s
epoch 13 | loss: 0.73596 | val_0_rmse: 0.8563  | val_1_rmse: 0.8635  |  0:02:28s
epoch 14 | loss: 0.73571 | val_0_rmse: 0.85577 | val_1_rmse: 0.86367 |  0:02:38s
epoch 15 | loss: 0.73401 | val_0_rmse: 0.85841 | val_1_rmse: 0.86595 |  0:02:49s
epoch 16 | loss: 0.73427 | val_0_rmse: 0.857   | val_1_rmse: 0.86321 |  0:03:00s
epoch 17 | loss: 0.73417 | val_0_rmse: 0.85566 | val_1_rmse: 0.8632  |  0:03:10s
epoch 18 | loss: 0.73331 | val_0_rmse: 0.85493 | val_1_rmse: 0.86279 |  0:03:21s
epoch 19 | loss: 0.73428 | val_0_rmse: 0.85603 | val_1_rmse: 0.86335 |  0:03:31s
epoch 20 | loss: 0.73436 | val_0_rmse: 0.8657  | val_1_rmse: 0.89722 |  0:03:42s
epoch 21 | loss: 0.7341  | val_0_rmse: 0.8561  | val_1_rmse: 0.86317 |  0:03:52s
epoch 22 | loss: 0.73518 | val_0_rmse: 0.85629 | val_1_rmse: 0.86412 |  0:04:03s
epoch 23 | loss: 0.73568 | val_0_rmse: 0.85663 | val_1_rmse: 0.86356 |  0:04:13s
epoch 24 | loss: 0.73399 | val_0_rmse: 0.8634  | val_1_rmse: 0.88836 |  0:04:24s
epoch 25 | loss: 0.73319 | val_0_rmse: 0.89786 | val_1_rmse: 1.00581 |  0:04:34s
epoch 26 | loss: 0.73723 | val_0_rmse: 0.85932 | val_1_rmse: 0.86712 |  0:04:45s
epoch 27 | loss: 0.73609 | val_0_rmse: 0.85832 | val_1_rmse: 0.86354 |  0:04:56s
epoch 28 | loss: 0.73343 | val_0_rmse: 0.86355 | val_1_rmse: 0.86338 |  0:05:06s
epoch 29 | loss: 0.73344 | val_0_rmse: 0.86151 | val_1_rmse: 0.86269 |  0:05:17s
epoch 30 | loss: 0.73287 | val_0_rmse: 0.85792 | val_1_rmse: 0.86424 |  0:05:27s
epoch 31 | loss: 0.73357 | val_0_rmse: 0.85731 | val_1_rmse: 0.86305 |  0:05:38s
epoch 32 | loss: 0.73321 | val_0_rmse: 0.8607  | val_1_rmse: 0.86762 |  0:05:48s
epoch 33 | loss: 0.73814 | val_0_rmse: 0.86448 | val_1_rmse: 0.86686 |  0:05:59s
epoch 34 | loss: 0.7363  | val_0_rmse: 0.8587  | val_1_rmse: 0.86609 |  0:06:09s
epoch 35 | loss: 0.73639 | val_0_rmse: 0.85641 | val_1_rmse: 0.86495 |  0:06:20s
epoch 36 | loss: 0.73661 | val_0_rmse: 0.85799 | val_1_rmse: 0.86601 |  0:06:30s
epoch 37 | loss: 0.73557 | val_0_rmse: 0.85637 | val_1_rmse: 0.86563 |  0:06:41s
epoch 38 | loss: 0.73599 | val_0_rmse: 0.85629 | val_1_rmse: 0.86611 |  0:06:51s
epoch 39 | loss: 0.73671 | val_0_rmse: 0.85669 | val_1_rmse: 0.86472 |  0:07:02s
epoch 40 | loss: 0.73507 | val_0_rmse: 0.85727 | val_1_rmse: 0.86615 |  0:07:13s
epoch 41 | loss: 0.7387  | val_0_rmse: 0.85938 | val_1_rmse: 0.86861 |  0:07:23s
epoch 42 | loss: 0.73687 | val_0_rmse: 0.90279 | val_1_rmse: 0.86805 |  0:07:34s
epoch 43 | loss: 0.73972 | val_0_rmse: 0.85814 | val_1_rmse: 0.86689 |  0:07:44s
epoch 44 | loss: 0.73754 | val_0_rmse: 0.85798 | val_1_rmse: 0.86646 |  0:07:55s
epoch 45 | loss: 0.73501 | val_0_rmse: 0.85636 | val_1_rmse: 0.86622 |  0:08:06s
epoch 46 | loss: 0.73498 | val_0_rmse: 0.85683 | val_1_rmse: 0.86598 |  0:08:16s
epoch 47 | loss: 0.73622 | val_0_rmse: 0.86028 | val_1_rmse: 0.86869 |  0:08:27s
epoch 48 | loss: 0.73644 | val_0_rmse: 0.85814 | val_1_rmse: 0.86629 |  0:08:37s
epoch 49 | loss: 0.73358 | val_0_rmse: 0.85831 | val_1_rmse: 0.86464 |  0:08:48s
epoch 50 | loss: 0.73365 | val_0_rmse: 0.85939 | val_1_rmse: 0.86525 |  0:08:59s
epoch 51 | loss: 0.73399 | val_0_rmse: 0.86011 | val_1_rmse: 0.86572 |  0:09:09s
epoch 52 | loss: 0.73598 | val_0_rmse: 0.9217  | val_1_rmse: 0.86726 |  0:09:20s
epoch 53 | loss: 0.737   | val_0_rmse: 0.85694 | val_1_rmse: 0.86508 |  0:09:30s
epoch 54 | loss: 0.73433 | val_0_rmse: 0.8671  | val_1_rmse: 0.86539 |  0:09:41s
epoch 55 | loss: 0.73431 | val_0_rmse: 0.85701 | val_1_rmse: 0.86627 |  0:09:51s
epoch 56 | loss: 0.73264 | val_0_rmse: 0.857   | val_1_rmse: 0.86379 |  0:10:02s
epoch 57 | loss: 0.73167 | val_0_rmse: 0.8554  | val_1_rmse: 0.86451 |  0:10:13s
epoch 58 | loss: 0.73259 | val_0_rmse: 0.91477 | val_1_rmse: 0.86389 |  0:10:23s
epoch 59 | loss: 0.73356 | val_0_rmse: 0.87593 | val_1_rmse: 0.86483 |  0:10:34s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.86269
Best weights from best epoch are automatically used!
ended training at: 05:55:00
Feature importance:
Mean squared error is of 5107777659.855262
Mean absolute error:54818.812867014894
MAPE:0.5364047352587843
R2 score:0.2415205231016988
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:55:01
epoch 0  | loss: 1.07244 | val_0_rmse: 0.9601  | val_1_rmse: 0.95194 |  0:00:03s
epoch 1  | loss: 0.85825 | val_0_rmse: 0.94016 | val_1_rmse: 0.93515 |  0:00:06s
epoch 2  | loss: 0.83772 | val_0_rmse: 0.94515 | val_1_rmse: 0.93898 |  0:00:10s
epoch 3  | loss: 0.82533 | val_0_rmse: 0.96085 | val_1_rmse: 0.95567 |  0:00:13s
epoch 4  | loss: 0.81613 | val_0_rmse: 0.94588 | val_1_rmse: 0.93994 |  0:00:17s
epoch 5  | loss: 0.80873 | val_0_rmse: 0.94009 | val_1_rmse: 0.93524 |  0:00:20s
epoch 6  | loss: 0.80735 | val_0_rmse: 0.93365 | val_1_rmse: 0.92825 |  0:00:23s
epoch 7  | loss: 0.80381 | val_0_rmse: 0.93288 | val_1_rmse: 0.92568 |  0:00:27s
epoch 8  | loss: 0.80482 | val_0_rmse: 0.92701 | val_1_rmse: 0.9236  |  0:00:30s
epoch 9  | loss: 0.79986 | val_0_rmse: 0.91109 | val_1_rmse: 0.90516 |  0:00:34s
epoch 10 | loss: 0.80203 | val_0_rmse: 0.90729 | val_1_rmse: 0.90071 |  0:00:37s
epoch 11 | loss: 0.80155 | val_0_rmse: 0.91368 | val_1_rmse: 0.90847 |  0:00:41s
epoch 12 | loss: 0.79937 | val_0_rmse: 0.90706 | val_1_rmse: 0.90083 |  0:00:44s
epoch 13 | loss: 0.78494 | val_0_rmse: 1.25235 | val_1_rmse: 1.25108 |  0:00:47s
epoch 14 | loss: 0.76539 | val_0_rmse: 0.94083 | val_1_rmse: 0.93742 |  0:00:51s
epoch 15 | loss: 0.76332 | val_0_rmse: 0.90382 | val_1_rmse: 0.90028 |  0:00:54s
epoch 16 | loss: 0.75946 | val_0_rmse: 0.93897 | val_1_rmse: 0.93769 |  0:00:58s
epoch 17 | loss: 0.75527 | val_0_rmse: 0.95974 | val_1_rmse: 0.95795 |  0:01:01s
epoch 18 | loss: 0.75101 | val_0_rmse: 0.8671  | val_1_rmse: 0.85798 |  0:01:05s
epoch 19 | loss: 0.75122 | val_0_rmse: 1.06945 | val_1_rmse: 1.05369 |  0:01:08s
epoch 20 | loss: 0.75426 | val_0_rmse: 1.09407 | val_1_rmse: 1.07713 |  0:01:12s
epoch 21 | loss: 0.75331 | val_0_rmse: 0.99406 | val_1_rmse: 0.97992 |  0:01:15s
epoch 22 | loss: 0.74882 | val_0_rmse: 0.87268 | val_1_rmse: 0.86546 |  0:01:18s
epoch 23 | loss: 0.74771 | val_0_rmse: 0.86906 | val_1_rmse: 0.86307 |  0:01:22s
epoch 24 | loss: 0.75023 | val_0_rmse: 0.89483 | val_1_rmse: 0.8896  |  0:01:25s
epoch 25 | loss: 0.74655 | val_0_rmse: 0.86058 | val_1_rmse: 0.85074 |  0:01:29s
epoch 26 | loss: 0.74435 | val_0_rmse: 1.05189 | val_1_rmse: 1.04488 |  0:01:32s
epoch 27 | loss: 0.75012 | val_0_rmse: 0.90948 | val_1_rmse: 0.90129 |  0:01:36s
epoch 28 | loss: 0.7487  | val_0_rmse: 1.07345 | val_1_rmse: 1.06391 |  0:01:39s
epoch 29 | loss: 0.7502  | val_0_rmse: 0.89355 | val_1_rmse: 0.88671 |  0:01:43s
epoch 30 | loss: 0.7479  | val_0_rmse: 0.97667 | val_1_rmse: 0.96186 |  0:01:46s
epoch 31 | loss: 0.74754 | val_0_rmse: 0.89728 | val_1_rmse: 0.89584 |  0:01:49s
epoch 32 | loss: 0.74562 | val_0_rmse: 0.86181 | val_1_rmse: 0.8542  |  0:01:53s
epoch 33 | loss: 0.74492 | val_0_rmse: 1.04138 | val_1_rmse: 1.03763 |  0:01:56s
epoch 34 | loss: 0.74507 | val_0_rmse: 0.9148  | val_1_rmse: 0.90044 |  0:02:00s
epoch 35 | loss: 0.74395 | val_0_rmse: 0.97176 | val_1_rmse: 0.96355 |  0:02:03s
epoch 36 | loss: 0.74143 | val_0_rmse: 0.89334 | val_1_rmse: 0.8788  |  0:02:07s
epoch 37 | loss: 0.74393 | val_0_rmse: 0.86196 | val_1_rmse: 0.85068 |  0:02:10s
epoch 38 | loss: 0.74108 | val_0_rmse: 0.93639 | val_1_rmse: 0.92638 |  0:02:14s
epoch 39 | loss: 0.73917 | val_0_rmse: 0.86875 | val_1_rmse: 0.85991 |  0:02:17s
epoch 40 | loss: 0.73738 | val_0_rmse: 0.86341 | val_1_rmse: 0.85507 |  0:02:20s
epoch 41 | loss: 0.73846 | val_0_rmse: 0.89284 | val_1_rmse: 0.87765 |  0:02:24s
epoch 42 | loss: 0.74141 | val_0_rmse: 0.91285 | val_1_rmse: 0.88432 |  0:02:28s
epoch 43 | loss: 0.74822 | val_0_rmse: 1.21611 | val_1_rmse: 1.19835 |  0:02:31s
epoch 44 | loss: 0.74006 | val_0_rmse: 0.87581 | val_1_rmse: 0.86616 |  0:02:35s
epoch 45 | loss: 0.73723 | val_0_rmse: 0.8914  | val_1_rmse: 0.87999 |  0:02:38s
epoch 46 | loss: 0.73711 | val_0_rmse: 0.88412 | val_1_rmse: 0.87485 |  0:02:41s
epoch 47 | loss: 0.74142 | val_0_rmse: 0.94259 | val_1_rmse: 0.94221 |  0:02:45s
epoch 48 | loss: 0.73804 | val_0_rmse: 0.90021 | val_1_rmse: 0.88859 |  0:02:48s
epoch 49 | loss: 0.7359  | val_0_rmse: 0.86147 | val_1_rmse: 0.85309 |  0:02:52s
epoch 50 | loss: 0.73665 | val_0_rmse: 0.8888  | val_1_rmse: 0.87645 |  0:02:55s
epoch 51 | loss: 0.7384  | val_0_rmse: 0.93975 | val_1_rmse: 0.92889 |  0:02:58s
epoch 52 | loss: 0.73835 | val_0_rmse: 1.02201 | val_1_rmse: 1.00371 |  0:03:02s
epoch 53 | loss: 0.73735 | val_0_rmse: 0.90707 | val_1_rmse: 0.89736 |  0:03:05s
epoch 54 | loss: 0.7354  | val_0_rmse: 0.9235  | val_1_rmse: 0.91385 |  0:03:09s
epoch 55 | loss: 0.73336 | val_0_rmse: 0.8576  | val_1_rmse: 0.84941 |  0:03:12s
epoch 56 | loss: 0.73965 | val_0_rmse: 0.91012 | val_1_rmse: 0.89691 |  0:03:16s
epoch 57 | loss: 0.73752 | val_0_rmse: 0.85756 | val_1_rmse: 0.84894 |  0:03:19s
epoch 58 | loss: 0.73665 | val_0_rmse: 0.86289 | val_1_rmse: 0.85372 |  0:03:23s
epoch 59 | loss: 0.736   | val_0_rmse: 0.87451 | val_1_rmse: 0.84869 |  0:03:26s
epoch 60 | loss: 0.73703 | val_0_rmse: 0.90588 | val_1_rmse: 0.89266 |  0:03:29s
epoch 61 | loss: 0.73672 | val_0_rmse: 0.98208 | val_1_rmse: 0.97375 |  0:03:33s
epoch 62 | loss: 0.7407  | val_0_rmse: 0.89626 | val_1_rmse: 0.88341 |  0:03:36s
epoch 63 | loss: 0.73605 | val_0_rmse: 1.04322 | val_1_rmse: 1.04003 |  0:03:40s
epoch 64 | loss: 0.7373  | val_0_rmse: 1.06813 | val_1_rmse: 1.05391 |  0:03:43s
epoch 65 | loss: 0.73988 | val_0_rmse: 1.09681 | val_1_rmse: 1.08418 |  0:03:46s
epoch 66 | loss: 0.7361  | val_0_rmse: 0.85948 | val_1_rmse: 0.84961 |  0:03:50s
epoch 67 | loss: 0.73481 | val_0_rmse: 0.88118 | val_1_rmse: 0.87153 |  0:03:53s
epoch 68 | loss: 0.73414 | val_0_rmse: 0.90034 | val_1_rmse: 0.88647 |  0:03:57s
epoch 69 | loss: 0.73459 | val_0_rmse: 0.95713 | val_1_rmse: 0.94127 |  0:04:00s
epoch 70 | loss: 0.73573 | val_0_rmse: 0.85685 | val_1_rmse: 0.84985 |  0:04:04s
epoch 71 | loss: 0.7371  | val_0_rmse: 0.8695  | val_1_rmse: 0.86002 |  0:04:07s
epoch 72 | loss: 0.73858 | val_0_rmse: 0.87184 | val_1_rmse: 0.86442 |  0:04:10s
epoch 73 | loss: 0.74089 | val_0_rmse: 0.92873 | val_1_rmse: 0.91585 |  0:04:14s
epoch 74 | loss: 0.73646 | val_0_rmse: 0.95317 | val_1_rmse: 0.94273 |  0:04:17s
epoch 75 | loss: 0.73366 | val_0_rmse: 0.87576 | val_1_rmse: 0.86789 |  0:04:21s
epoch 76 | loss: 0.73977 | val_0_rmse: 1.92495 | val_1_rmse: 1.19176 |  0:04:24s
epoch 77 | loss: 0.73933 | val_0_rmse: 1.16112 | val_1_rmse: 1.14633 |  0:04:27s
epoch 78 | loss: 0.73961 | val_0_rmse: 1.1244  | val_1_rmse: 1.07714 |  0:04:31s
epoch 79 | loss: 0.73661 | val_0_rmse: 0.97085 | val_1_rmse: 0.95936 |  0:04:34s
epoch 80 | loss: 0.73411 | val_0_rmse: 0.90254 | val_1_rmse: 0.88704 |  0:04:38s
epoch 81 | loss: 0.7396  | val_0_rmse: 0.92348 | val_1_rmse: 0.90515 |  0:04:41s
epoch 82 | loss: 0.73406 | val_0_rmse: 1.01344 | val_1_rmse: 0.96701 |  0:04:44s
epoch 83 | loss: 0.73422 | val_0_rmse: 0.9389  | val_1_rmse: 0.86454 |  0:04:48s
epoch 84 | loss: 0.73616 | val_0_rmse: 0.985   | val_1_rmse: 0.93041 |  0:04:51s
epoch 85 | loss: 0.73506 | val_0_rmse: 0.92958 | val_1_rmse: 0.86591 |  0:04:55s
epoch 86 | loss: 0.73915 | val_0_rmse: 1.23252 | val_1_rmse: 1.16987 |  0:04:58s
epoch 87 | loss: 0.7382  | val_0_rmse: 0.9449  | val_1_rmse: 0.88618 |  0:05:01s
epoch 88 | loss: 0.73465 | val_0_rmse: 1.00508 | val_1_rmse: 0.94845 |  0:05:05s
epoch 89 | loss: 0.73509 | val_0_rmse: 0.90192 | val_1_rmse: 0.85262 |  0:05:08s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.84869
Best weights from best epoch are automatically used!
ended training at: 06:00:11
Feature importance:
Mean squared error is of 2980976612.7821565
Mean absolute error:40961.79658407534
MAPE:0.5692449307655281
R2 score:0.2527346155478415
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:00:12
epoch 0  | loss: 1.00691 | val_0_rmse: 0.92128 | val_1_rmse: 0.9089  |  0:00:03s
epoch 1  | loss: 0.80174 | val_0_rmse: 0.91344 | val_1_rmse: 0.90539 |  0:00:06s
epoch 2  | loss: 0.78179 | val_0_rmse: 0.91734 | val_1_rmse: 0.90558 |  0:00:10s
epoch 3  | loss: 0.7741  | val_0_rmse: 1.0323  | val_1_rmse: 1.0113  |  0:00:13s
epoch 4  | loss: 0.76896 | val_0_rmse: 0.89835 | val_1_rmse: 0.88641 |  0:00:17s
epoch 5  | loss: 0.76675 | val_0_rmse: 0.91154 | val_1_rmse: 0.89892 |  0:00:20s
epoch 6  | loss: 0.76615 | val_0_rmse: 0.91427 | val_1_rmse: 0.90105 |  0:00:23s
epoch 7  | loss: 0.76271 | val_0_rmse: 1.01652 | val_1_rmse: 0.99597 |  0:00:27s
epoch 8  | loss: 0.76294 | val_0_rmse: 0.89635 | val_1_rmse: 0.88015 |  0:00:30s
epoch 9  | loss: 0.76278 | val_0_rmse: 0.89482 | val_1_rmse: 0.88328 |  0:00:34s
epoch 10 | loss: 0.76441 | val_0_rmse: 0.88959 | val_1_rmse: 0.87758 |  0:00:37s
epoch 11 | loss: 0.7605  | val_0_rmse: 0.93197 | val_1_rmse: 0.91308 |  0:00:41s
epoch 12 | loss: 0.76308 | val_0_rmse: 0.88313 | val_1_rmse: 0.8713  |  0:00:44s
epoch 13 | loss: 0.75947 | val_0_rmse: 0.91023 | val_1_rmse: 0.89378 |  0:00:47s
epoch 14 | loss: 0.75886 | val_0_rmse: 0.88505 | val_1_rmse: 0.87231 |  0:00:51s
epoch 15 | loss: 0.75711 | val_0_rmse: 0.89267 | val_1_rmse: 0.87966 |  0:00:54s
epoch 16 | loss: 0.75281 | val_0_rmse: 0.88149 | val_1_rmse: 0.8677  |  0:00:57s
epoch 17 | loss: 0.75436 | val_0_rmse: 0.87061 | val_1_rmse: 0.86162 |  0:01:01s
epoch 18 | loss: 0.7567  | val_0_rmse: 0.89015 | val_1_rmse: 0.884   |  0:01:04s
epoch 19 | loss: 0.75776 | val_0_rmse: 0.87012 | val_1_rmse: 0.86545 |  0:01:08s
epoch 20 | loss: 0.75519 | val_0_rmse: 0.92822 | val_1_rmse: 0.91049 |  0:01:11s
epoch 21 | loss: 0.75333 | val_0_rmse: 0.89392 | val_1_rmse: 0.88321 |  0:01:15s
epoch 22 | loss: 0.75302 | val_0_rmse: 0.86977 | val_1_rmse: 0.86155 |  0:01:18s
epoch 23 | loss: 0.75205 | val_0_rmse: 0.88335 | val_1_rmse: 0.87355 |  0:01:22s
epoch 24 | loss: 0.75813 | val_0_rmse: 0.88531 | val_1_rmse: 0.87558 |  0:01:25s
epoch 25 | loss: 0.75395 | val_0_rmse: 0.86653 | val_1_rmse: 0.85922 |  0:01:29s
epoch 26 | loss: 0.75518 | val_0_rmse: 0.88089 | val_1_rmse: 0.87012 |  0:01:32s
epoch 27 | loss: 0.7517  | val_0_rmse: 0.86583 | val_1_rmse: 0.85763 |  0:01:35s
epoch 28 | loss: 0.74962 | val_0_rmse: 0.88386 | val_1_rmse: 0.87243 |  0:01:39s
epoch 29 | loss: 0.75242 | val_0_rmse: 0.8727  | val_1_rmse: 0.86587 |  0:01:42s
epoch 30 | loss: 0.75876 | val_0_rmse: 0.87886 | val_1_rmse: 0.86749 |  0:01:46s
epoch 31 | loss: 0.7603  | val_0_rmse: 0.97968 | val_1_rmse: 0.95937 |  0:01:49s
epoch 32 | loss: 0.76448 | val_0_rmse: 0.87735 | val_1_rmse: 0.87204 |  0:01:52s
epoch 33 | loss: 0.75748 | val_0_rmse: 0.88519 | val_1_rmse: 0.87238 |  0:01:56s
epoch 34 | loss: 0.75484 | val_0_rmse: 0.86781 | val_1_rmse: 0.86039 |  0:01:59s
epoch 35 | loss: 0.75578 | val_0_rmse: 0.89949 | val_1_rmse: 0.89081 |  0:02:03s
epoch 36 | loss: 0.76974 | val_0_rmse: 1.07599 | val_1_rmse: 1.05785 |  0:02:06s
epoch 37 | loss: 0.76779 | val_0_rmse: 0.89328 | val_1_rmse: 0.88175 |  0:02:09s
epoch 38 | loss: 0.76684 | val_0_rmse: 0.92805 | val_1_rmse: 0.91318 |  0:02:13s
epoch 39 | loss: 0.76697 | val_0_rmse: 0.87843 | val_1_rmse: 0.87485 |  0:02:16s
epoch 40 | loss: 0.77011 | val_0_rmse: 0.8772  | val_1_rmse: 0.87524 |  0:02:20s
epoch 41 | loss: 0.76665 | val_0_rmse: 0.88902 | val_1_rmse: 0.88032 |  0:02:23s
epoch 42 | loss: 0.76269 | val_0_rmse: 0.8972  | val_1_rmse: 0.88605 |  0:02:27s
epoch 43 | loss: 0.75871 | val_0_rmse: 0.88439 | val_1_rmse: 0.87087 |  0:02:30s
epoch 44 | loss: 0.75767 | val_0_rmse: 0.8876  | val_1_rmse: 0.87759 |  0:02:34s
epoch 45 | loss: 0.75677 | val_0_rmse: 0.88292 | val_1_rmse: 0.87266 |  0:02:37s
epoch 46 | loss: 0.75056 | val_0_rmse: 0.86876 | val_1_rmse: 0.8614  |  0:02:40s
epoch 47 | loss: 0.7523  | val_0_rmse: 0.89147 | val_1_rmse: 0.87945 |  0:02:44s
epoch 48 | loss: 0.75083 | val_0_rmse: 0.87901 | val_1_rmse: 0.86771 |  0:02:47s
epoch 49 | loss: 0.75107 | val_0_rmse: 0.89013 | val_1_rmse: 0.87891 |  0:02:51s
epoch 50 | loss: 0.75216 | val_0_rmse: 0.88566 | val_1_rmse: 0.87663 |  0:02:54s
epoch 51 | loss: 0.75241 | val_0_rmse: 0.90337 | val_1_rmse: 0.898   |  0:02:58s
epoch 52 | loss: 0.75338 | val_0_rmse: 0.88694 | val_1_rmse: 0.87576 |  0:03:01s
epoch 53 | loss: 0.75066 | val_0_rmse: 0.88991 | val_1_rmse: 0.87853 |  0:03:04s
epoch 54 | loss: 0.75069 | val_0_rmse: 0.87584 | val_1_rmse: 0.86654 |  0:03:08s
epoch 55 | loss: 0.75283 | val_0_rmse: 0.91004 | val_1_rmse: 0.89595 |  0:03:11s
epoch 56 | loss: 0.75257 | val_0_rmse: 0.88189 | val_1_rmse: 0.87327 |  0:03:15s
epoch 57 | loss: 0.75336 | val_0_rmse: 0.87919 | val_1_rmse: 0.86991 |  0:03:18s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.85763
Best weights from best epoch are automatically used!
ended training at: 06:03:32
Feature importance:
Mean squared error is of 2963934215.7647195
Mean absolute error:40321.318410043474
MAPE:0.5510488295959625
R2 score:0.26886628071855034
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:03:33
epoch 0  | loss: 1.09268 | val_0_rmse: 0.98548 | val_1_rmse: 0.97832 |  0:00:03s
epoch 1  | loss: 0.8336  | val_0_rmse: 0.94213 | val_1_rmse: 0.93469 |  0:00:06s
epoch 2  | loss: 0.83118 | val_0_rmse: 0.95411 | val_1_rmse: 0.94684 |  0:00:10s
epoch 3  | loss: 0.81254 | val_0_rmse: 0.93367 | val_1_rmse: 0.92805 |  0:00:13s
epoch 4  | loss: 0.79311 | val_0_rmse: 0.91912 | val_1_rmse: 0.91361 |  0:00:17s
epoch 5  | loss: 0.77844 | val_0_rmse: 0.98233 | val_1_rmse: 0.9688  |  0:00:20s
epoch 6  | loss: 0.77267 | val_0_rmse: 1.24604 | val_1_rmse: 1.23767 |  0:00:23s
epoch 7  | loss: 0.76972 | val_0_rmse: 0.99051 | val_1_rmse: 0.98225 |  0:00:27s
epoch 8  | loss: 0.76533 | val_0_rmse: 0.92146 | val_1_rmse: 0.91773 |  0:00:30s
epoch 9  | loss: 0.78018 | val_0_rmse: 0.91476 | val_1_rmse: 0.90803 |  0:00:34s
epoch 10 | loss: 0.7792  | val_0_rmse: 0.90179 | val_1_rmse: 0.89942 |  0:00:37s
epoch 11 | loss: 0.78936 | val_0_rmse: 0.88234 | val_1_rmse: 0.8794  |  0:00:41s
epoch 12 | loss: 0.77495 | val_0_rmse: 0.90328 | val_1_rmse: 0.90381 |  0:00:44s
epoch 13 | loss: 0.77256 | val_0_rmse: 1.00243 | val_1_rmse: 1.00049 |  0:00:48s
epoch 14 | loss: 0.7694  | val_0_rmse: 0.93062 | val_1_rmse: 0.92743 |  0:00:51s
epoch 15 | loss: 0.76848 | val_0_rmse: 0.89921 | val_1_rmse: 0.89649 |  0:00:54s
epoch 16 | loss: 0.76712 | val_0_rmse: 0.88541 | val_1_rmse: 0.88246 |  0:00:58s
epoch 17 | loss: 0.76897 | val_0_rmse: 0.90293 | val_1_rmse: 0.90639 |  0:01:01s
epoch 18 | loss: 0.77557 | val_0_rmse: 0.88053 | val_1_rmse: 0.87951 |  0:01:05s
epoch 19 | loss: 0.76892 | val_0_rmse: 0.98347 | val_1_rmse: 0.97861 |  0:01:08s
epoch 20 | loss: 0.76573 | val_0_rmse: 1.07294 | val_1_rmse: 1.07163 |  0:01:12s
epoch 21 | loss: 0.75647 | val_0_rmse: 0.87173 | val_1_rmse: 0.87303 |  0:01:15s
epoch 22 | loss: 0.76142 | val_0_rmse: 0.94569 | val_1_rmse: 0.94388 |  0:01:18s
epoch 23 | loss: 0.7573  | val_0_rmse: 0.86783 | val_1_rmse: 0.87071 |  0:01:22s
epoch 24 | loss: 0.75821 | val_0_rmse: 0.88752 | val_1_rmse: 0.889   |  0:01:25s
epoch 25 | loss: 0.76352 | val_0_rmse: 0.87895 | val_1_rmse: 0.87932 |  0:01:29s
epoch 26 | loss: 0.76055 | val_0_rmse: 0.87368 | val_1_rmse: 0.87295 |  0:01:32s
epoch 27 | loss: 0.7563  | val_0_rmse: 0.8702  | val_1_rmse: 0.87318 |  0:01:36s
epoch 28 | loss: 0.75667 | val_0_rmse: 0.87088 | val_1_rmse: 0.87366 |  0:01:39s
epoch 29 | loss: 0.75152 | val_0_rmse: 0.86802 | val_1_rmse: 0.87199 |  0:01:42s
epoch 30 | loss: 0.75387 | val_0_rmse: 0.87379 | val_1_rmse: 0.87963 |  0:01:46s
epoch 31 | loss: 0.75612 | val_0_rmse: 0.9098  | val_1_rmse: 0.91027 |  0:01:49s
epoch 32 | loss: 0.75573 | val_0_rmse: 0.87978 | val_1_rmse: 0.88204 |  0:01:53s
epoch 33 | loss: 0.74958 | val_0_rmse: 0.86527 | val_1_rmse: 0.86803 |  0:01:56s
epoch 34 | loss: 0.75115 | val_0_rmse: 0.86962 | val_1_rmse: 0.87234 |  0:01:59s
epoch 35 | loss: 0.7514  | val_0_rmse: 0.93612 | val_1_rmse: 0.93675 |  0:02:03s
epoch 36 | loss: 0.74949 | val_0_rmse: 0.86233 | val_1_rmse: 0.86432 |  0:02:06s
epoch 37 | loss: 0.75122 | val_0_rmse: 0.90085 | val_1_rmse: 0.9002  |  0:02:10s
epoch 38 | loss: 0.74914 | val_0_rmse: 0.94139 | val_1_rmse: 0.94127 |  0:02:13s
epoch 39 | loss: 0.74751 | val_0_rmse: 0.86373 | val_1_rmse: 0.86602 |  0:02:16s
epoch 40 | loss: 0.74716 | val_0_rmse: 0.86597 | val_1_rmse: 0.86677 |  0:02:20s
epoch 41 | loss: 0.74711 | val_0_rmse: 0.8658  | val_1_rmse: 0.86903 |  0:02:23s
epoch 42 | loss: 0.74541 | val_0_rmse: 0.88719 | val_1_rmse: 0.88953 |  0:02:27s
epoch 43 | loss: 0.74748 | val_0_rmse: 0.96376 | val_1_rmse: 0.96465 |  0:02:30s
epoch 44 | loss: 0.74557 | val_0_rmse: 0.91909 | val_1_rmse: 0.92102 |  0:02:34s
epoch 45 | loss: 0.74487 | val_0_rmse: 0.98791 | val_1_rmse: 0.99006 |  0:02:37s
epoch 46 | loss: 0.74587 | val_0_rmse: 0.86006 | val_1_rmse: 0.86234 |  0:02:40s
epoch 47 | loss: 0.74485 | val_0_rmse: 0.88393 | val_1_rmse: 0.88402 |  0:02:44s
epoch 48 | loss: 0.74683 | val_0_rmse: 0.93951 | val_1_rmse: 0.94165 |  0:02:47s
epoch 49 | loss: 0.74759 | val_0_rmse: 0.87702 | val_1_rmse: 0.87598 |  0:02:51s
epoch 50 | loss: 0.75024 | val_0_rmse: 0.87057 | val_1_rmse: 0.87105 |  0:02:54s
epoch 51 | loss: 0.74996 | val_0_rmse: 0.86896 | val_1_rmse: 0.86989 |  0:02:58s
epoch 52 | loss: 0.7523  | val_0_rmse: 0.95017 | val_1_rmse: 0.94877 |  0:03:01s
epoch 53 | loss: 0.75809 | val_0_rmse: 0.89857 | val_1_rmse: 0.89777 |  0:03:04s
epoch 54 | loss: 0.75975 | val_0_rmse: 0.86865 | val_1_rmse: 0.86738 |  0:03:08s
epoch 55 | loss: 0.75193 | val_0_rmse: 0.86802 | val_1_rmse: 0.86613 |  0:03:11s
epoch 56 | loss: 0.75341 | val_0_rmse: 0.8711  | val_1_rmse: 0.87048 |  0:03:15s
epoch 57 | loss: 0.75167 | val_0_rmse: 0.87284 | val_1_rmse: 0.87264 |  0:03:18s
epoch 58 | loss: 0.74962 | val_0_rmse: 0.87193 | val_1_rmse: 0.87289 |  0:03:22s
epoch 59 | loss: 0.74945 | val_0_rmse: 0.86887 | val_1_rmse: 0.86995 |  0:03:25s
epoch 60 | loss: 0.74854 | val_0_rmse: 0.86773 | val_1_rmse: 0.86836 |  0:03:29s
epoch 61 | loss: 0.74775 | val_0_rmse: 0.86972 | val_1_rmse: 0.87016 |  0:03:32s
epoch 62 | loss: 0.74755 | val_0_rmse: 0.87931 | val_1_rmse: 0.88005 |  0:03:35s
epoch 63 | loss: 0.75014 | val_0_rmse: 0.86329 | val_1_rmse: 0.86383 |  0:03:39s
epoch 64 | loss: 0.74875 | val_0_rmse: 0.88051 | val_1_rmse: 0.8817  |  0:03:42s
epoch 65 | loss: 0.74835 | val_0_rmse: 0.88362 | val_1_rmse: 0.88463 |  0:03:46s
epoch 66 | loss: 0.74852 | val_0_rmse: 0.868   | val_1_rmse: 0.86766 |  0:03:49s
epoch 67 | loss: 0.74925 | val_0_rmse: 0.86491 | val_1_rmse: 0.86611 |  0:03:53s
epoch 68 | loss: 0.74743 | val_0_rmse: 0.86226 | val_1_rmse: 0.86269 |  0:03:56s
epoch 69 | loss: 0.74784 | val_0_rmse: 0.864   | val_1_rmse: 0.86415 |  0:03:59s
epoch 70 | loss: 0.74565 | val_0_rmse: 1.03641 | val_1_rmse: 0.95027 |  0:04:03s
epoch 71 | loss: 0.74679 | val_0_rmse: 0.86524 | val_1_rmse: 0.86492 |  0:04:06s
epoch 72 | loss: 0.74656 | val_0_rmse: 0.89444 | val_1_rmse: 0.89356 |  0:04:10s
epoch 73 | loss: 0.76113 | val_0_rmse: 0.8831  | val_1_rmse: 0.8846  |  0:04:13s
epoch 74 | loss: 0.75777 | val_0_rmse: 0.87569 | val_1_rmse: 0.87539 |  0:04:17s
epoch 75 | loss: 0.75585 | val_0_rmse: 0.89583 | val_1_rmse: 0.89587 |  0:04:20s
epoch 76 | loss: 0.75126 | val_0_rmse: 0.86282 | val_1_rmse: 0.86335 |  0:04:23s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.86234
Best weights from best epoch are automatically used!
ended training at: 06:07:58
Feature importance:
Mean squared error is of 3070998540.633072
Mean absolute error:41938.774964449425
MAPE:0.597978811958842
R2 score:0.23682192324703966
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:07:58
epoch 0  | loss: 1.07461 | val_0_rmse: 0.936   | val_1_rmse: 0.93202 |  0:00:03s
epoch 1  | loss: 0.84989 | val_0_rmse: 0.95541 | val_1_rmse: 0.95424 |  0:00:06s
epoch 2  | loss: 0.82888 | val_0_rmse: 0.96153 | val_1_rmse: 0.95849 |  0:00:10s
epoch 3  | loss: 0.82004 | val_0_rmse: 0.9456  | val_1_rmse: 0.94183 |  0:00:13s
epoch 4  | loss: 0.80929 | val_0_rmse: 0.93469 | val_1_rmse: 0.9306  |  0:00:17s
epoch 5  | loss: 0.80966 | val_0_rmse: 0.93784 | val_1_rmse: 0.9313  |  0:00:20s
epoch 6  | loss: 0.80635 | val_0_rmse: 0.92707 | val_1_rmse: 0.92256 |  0:00:23s
epoch 7  | loss: 0.80412 | val_0_rmse: 0.92397 | val_1_rmse: 0.91933 |  0:00:27s
epoch 8  | loss: 0.80438 | val_0_rmse: 0.92419 | val_1_rmse: 0.92192 |  0:00:30s
epoch 9  | loss: 0.80162 | val_0_rmse: 0.91835 | val_1_rmse: 0.91677 |  0:00:34s
epoch 10 | loss: 0.8007  | val_0_rmse: 0.91728 | val_1_rmse: 0.91354 |  0:00:37s
epoch 11 | loss: 0.80205 | val_0_rmse: 0.90976 | val_1_rmse: 0.90416 |  0:00:41s
epoch 12 | loss: 0.79767 | val_0_rmse: 1.00381 | val_1_rmse: 0.99642 |  0:00:44s
epoch 13 | loss: 0.77899 | val_0_rmse: 0.90714 | val_1_rmse: 0.89761 |  0:00:47s
epoch 14 | loss: 0.77325 | val_0_rmse: 0.96499 | val_1_rmse: 0.95807 |  0:00:51s
epoch 15 | loss: 0.77262 | val_0_rmse: 0.95869 | val_1_rmse: 0.94942 |  0:00:54s
epoch 16 | loss: 0.76501 | val_0_rmse: 0.88356 | val_1_rmse: 0.87239 |  0:00:58s
epoch 17 | loss: 0.76057 | val_0_rmse: 0.90621 | val_1_rmse: 0.89466 |  0:01:01s
epoch 18 | loss: 0.75843 | val_0_rmse: 0.86968 | val_1_rmse: 0.85884 |  0:01:05s
epoch 19 | loss: 0.76206 | val_0_rmse: 0.98541 | val_1_rmse: 0.97722 |  0:01:08s
epoch 20 | loss: 0.75854 | val_0_rmse: 0.87924 | val_1_rmse: 0.87068 |  0:01:11s
epoch 21 | loss: 0.76016 | val_0_rmse: 0.88221 | val_1_rmse: 0.87395 |  0:01:15s
epoch 22 | loss: 0.76292 | val_0_rmse: 0.87407 | val_1_rmse: 0.86571 |  0:01:18s
epoch 23 | loss: 0.76042 | val_0_rmse: 0.88638 | val_1_rmse: 0.87898 |  0:01:22s
epoch 24 | loss: 0.76015 | val_0_rmse: 0.94525 | val_1_rmse: 0.93825 |  0:01:25s
epoch 25 | loss: 0.75678 | val_0_rmse: 0.99187 | val_1_rmse: 0.98262 |  0:01:29s
epoch 26 | loss: 0.75628 | val_0_rmse: 0.89984 | val_1_rmse: 0.89076 |  0:01:32s
epoch 27 | loss: 0.75494 | val_0_rmse: 0.87061 | val_1_rmse: 0.85917 |  0:01:36s
epoch 28 | loss: 0.7488  | val_0_rmse: 0.98253 | val_1_rmse: 0.97372 |  0:01:39s
epoch 29 | loss: 0.75138 | val_0_rmse: 1.17223 | val_1_rmse: 1.16422 |  0:01:42s
epoch 30 | loss: 0.74845 | val_0_rmse: 0.86525 | val_1_rmse: 0.85508 |  0:01:46s
epoch 31 | loss: 0.75316 | val_0_rmse: 0.99292 | val_1_rmse: 0.98613 |  0:01:49s
epoch 32 | loss: 0.74664 | val_0_rmse: 0.88637 | val_1_rmse: 0.87696 |  0:01:53s
epoch 33 | loss: 0.75105 | val_0_rmse: 0.96086 | val_1_rmse: 0.95296 |  0:01:56s
epoch 34 | loss: 0.75298 | val_0_rmse: 0.90635 | val_1_rmse: 0.89673 |  0:02:00s
epoch 35 | loss: 0.75042 | val_0_rmse: 0.89555 | val_1_rmse: 0.88668 |  0:02:03s
epoch 36 | loss: 0.74787 | val_0_rmse: 0.88625 | val_1_rmse: 0.87709 |  0:02:06s
epoch 37 | loss: 0.75791 | val_0_rmse: 0.91189 | val_1_rmse: 0.9047  |  0:02:10s
epoch 38 | loss: 0.76559 | val_0_rmse: 0.88348 | val_1_rmse: 0.87608 |  0:02:13s
epoch 39 | loss: 0.74876 | val_0_rmse: 0.95404 | val_1_rmse: 0.94749 |  0:02:17s
epoch 40 | loss: 0.74699 | val_0_rmse: 1.02957 | val_1_rmse: 1.02349 |  0:02:20s
epoch 41 | loss: 0.74434 | val_0_rmse: 0.96522 | val_1_rmse: 0.95982 |  0:02:24s
epoch 42 | loss: 0.74786 | val_0_rmse: 0.89942 | val_1_rmse: 0.89299 |  0:02:27s
epoch 43 | loss: 0.74082 | val_0_rmse: 0.85999 | val_1_rmse: 0.85221 |  0:02:30s
epoch 44 | loss: 0.74152 | val_0_rmse: 0.89213 | val_1_rmse: 0.88304 |  0:02:34s
epoch 45 | loss: 0.74355 | val_0_rmse: 0.89736 | val_1_rmse: 0.88984 |  0:02:37s
epoch 46 | loss: 0.74408 | val_0_rmse: 0.86662 | val_1_rmse: 0.85544 |  0:02:41s
epoch 47 | loss: 0.73921 | val_0_rmse: 0.92199 | val_1_rmse: 0.90901 |  0:02:44s
epoch 48 | loss: 0.74206 | val_0_rmse: 0.86424 | val_1_rmse: 0.85477 |  0:02:48s
epoch 49 | loss: 0.74181 | val_0_rmse: 0.87745 | val_1_rmse: 0.86915 |  0:02:51s
epoch 50 | loss: 0.74276 | val_0_rmse: 0.89176 | val_1_rmse: 0.88341 |  0:02:54s
epoch 51 | loss: 0.74327 | val_0_rmse: 0.85601 | val_1_rmse: 0.84676 |  0:02:58s
epoch 52 | loss: 0.73896 | val_0_rmse: 0.88133 | val_1_rmse: 0.87109 |  0:03:01s
epoch 53 | loss: 0.74019 | val_0_rmse: 0.92455 | val_1_rmse: 0.91462 |  0:03:05s
epoch 54 | loss: 0.74187 | val_0_rmse: 0.89212 | val_1_rmse: 0.88442 |  0:03:08s
epoch 55 | loss: 0.73977 | val_0_rmse: 0.8749  | val_1_rmse: 0.86602 |  0:03:12s
epoch 56 | loss: 0.74036 | val_0_rmse: 0.9147  | val_1_rmse: 0.90391 |  0:03:15s
epoch 57 | loss: 0.73833 | val_0_rmse: 0.86976 | val_1_rmse: 0.85885 |  0:03:18s
epoch 58 | loss: 0.7425  | val_0_rmse: 0.91985 | val_1_rmse: 0.90667 |  0:03:22s
epoch 59 | loss: 0.73941 | val_0_rmse: 0.89794 | val_1_rmse: 0.88801 |  0:03:25s
epoch 60 | loss: 0.73858 | val_0_rmse: 0.91305 | val_1_rmse: 0.90505 |  0:03:29s
epoch 61 | loss: 0.73777 | val_0_rmse: 0.98767 | val_1_rmse: 0.97843 |  0:03:32s
epoch 62 | loss: 0.73909 | val_0_rmse: 0.9879  | val_1_rmse: 0.9784  |  0:03:36s
epoch 63 | loss: 0.74673 | val_0_rmse: 0.91286 | val_1_rmse: 0.90485 |  0:03:39s
epoch 64 | loss: 0.74011 | val_0_rmse: 0.89784 | val_1_rmse: 0.8898  |  0:03:42s
epoch 65 | loss: 0.74142 | val_0_rmse: 0.88966 | val_1_rmse: 0.87864 |  0:03:46s
epoch 66 | loss: 0.73973 | val_0_rmse: 0.89832 | val_1_rmse: 0.88955 |  0:03:49s
epoch 67 | loss: 0.73695 | val_0_rmse: 0.94071 | val_1_rmse: 0.93103 |  0:03:53s
epoch 68 | loss: 0.73649 | val_0_rmse: 0.8921  | val_1_rmse: 0.88079 |  0:03:56s
epoch 69 | loss: 0.73858 | val_0_rmse: 0.91784 | val_1_rmse: 0.90684 |  0:04:00s
epoch 70 | loss: 0.73832 | val_0_rmse: 0.96212 | val_1_rmse: 0.95283 |  0:04:03s
epoch 71 | loss: 0.74009 | val_0_rmse: 0.87682 | val_1_rmse: 0.86436 |  0:04:06s
epoch 72 | loss: 0.73331 | val_0_rmse: 0.93633 | val_1_rmse: 0.92762 |  0:04:10s
epoch 73 | loss: 0.73403 | val_0_rmse: 0.89748 | val_1_rmse: 0.889   |  0:04:13s
epoch 74 | loss: 0.73234 | val_0_rmse: 0.94489 | val_1_rmse: 0.93458 |  0:04:17s
epoch 75 | loss: 0.73388 | val_0_rmse: 0.85309 | val_1_rmse: 0.84214 |  0:04:20s
epoch 76 | loss: 0.73301 | val_0_rmse: 0.91443 | val_1_rmse: 0.90443 |  0:04:24s
epoch 77 | loss: 0.73099 | val_0_rmse: 0.91611 | val_1_rmse: 0.90478 |  0:04:27s
epoch 78 | loss: 0.73061 | val_0_rmse: 0.92403 | val_1_rmse: 0.91385 |  0:04:30s
epoch 79 | loss: 0.73307 | val_0_rmse: 0.85398 | val_1_rmse: 0.84168 |  0:04:34s
epoch 80 | loss: 0.73071 | val_0_rmse: 0.85323 | val_1_rmse: 0.84275 |  0:04:37s
epoch 81 | loss: 0.73294 | val_0_rmse: 0.90749 | val_1_rmse: 0.89772 |  0:04:41s
epoch 82 | loss: 0.73522 | val_0_rmse: 0.85652 | val_1_rmse: 0.8462  |  0:04:44s
epoch 83 | loss: 0.73098 | val_0_rmse: 0.85206 | val_1_rmse: 0.84164 |  0:04:48s
epoch 84 | loss: 0.73013 | val_0_rmse: 0.86726 | val_1_rmse: 0.85695 |  0:04:51s
epoch 85 | loss: 0.73442 | val_0_rmse: 0.92855 | val_1_rmse: 0.91867 |  0:04:55s
epoch 86 | loss: 0.73024 | val_0_rmse: 0.86135 | val_1_rmse: 0.85047 |  0:04:58s
epoch 87 | loss: 0.73178 | val_0_rmse: 0.87255 | val_1_rmse: 0.86422 |  0:05:01s
epoch 88 | loss: 0.7319  | val_0_rmse: 0.92769 | val_1_rmse: 0.91674 |  0:05:05s
epoch 89 | loss: 0.73574 | val_0_rmse: 0.85995 | val_1_rmse: 0.85042 |  0:05:08s
epoch 90 | loss: 0.73633 | val_0_rmse: 0.88429 | val_1_rmse: 0.87712 |  0:05:12s
epoch 91 | loss: 0.73693 | val_0_rmse: 0.89227 | val_1_rmse: 0.88838 |  0:05:15s
epoch 92 | loss: 0.73397 | val_0_rmse: 0.94528 | val_1_rmse: 0.93486 |  0:05:18s
epoch 93 | loss: 0.73652 | val_0_rmse: 0.96177 | val_1_rmse: 0.95534 |  0:05:22s
epoch 94 | loss: 0.74088 | val_0_rmse: 1.01389 | val_1_rmse: 1.00122 |  0:05:25s
epoch 95 | loss: 0.73597 | val_0_rmse: 0.8655  | val_1_rmse: 0.85496 |  0:05:29s
epoch 96 | loss: 0.73438 | val_0_rmse: 0.85582 | val_1_rmse: 0.84422 |  0:05:32s
epoch 97 | loss: 0.73808 | val_0_rmse: 0.85658 | val_1_rmse: 0.84562 |  0:05:36s
epoch 98 | loss: 0.74098 | val_0_rmse: 1.04275 | val_1_rmse: 1.03547 |  0:05:39s
epoch 99 | loss: 0.73508 | val_0_rmse: 0.9322  | val_1_rmse: 0.92211 |  0:05:42s
epoch 100| loss: 0.7378  | val_0_rmse: 0.86486 | val_1_rmse: 0.85537 |  0:05:46s
epoch 101| loss: 0.73199 | val_0_rmse: 0.95    | val_1_rmse: 0.94299 |  0:05:49s
epoch 102| loss: 0.73446 | val_0_rmse: 0.86118 | val_1_rmse: 0.8527  |  0:05:53s
epoch 103| loss: 0.73268 | val_0_rmse: 1.10709 | val_1_rmse: 1.09917 |  0:05:56s
epoch 104| loss: 0.73401 | val_0_rmse: 0.88097 | val_1_rmse: 0.87049 |  0:05:59s
epoch 105| loss: 0.73404 | val_0_rmse: 0.86239 | val_1_rmse: 0.85325 |  0:06:03s
epoch 106| loss: 0.73362 | val_0_rmse: 0.86118 | val_1_rmse: 0.85295 |  0:06:06s
epoch 107| loss: 0.73121 | val_0_rmse: 0.93603 | val_1_rmse: 0.92572 |  0:06:10s
epoch 108| loss: 0.73302 | val_0_rmse: 0.92742 | val_1_rmse: 0.91663 |  0:06:13s
epoch 109| loss: 0.73277 | val_0_rmse: 0.85816 | val_1_rmse: 0.84329 |  0:06:16s
epoch 110| loss: 0.73309 | val_0_rmse: 0.93358 | val_1_rmse: 0.92188 |  0:06:20s
epoch 111| loss: 0.7299  | val_0_rmse: 0.91088 | val_1_rmse: 0.90006 |  0:06:23s
epoch 112| loss: 0.73201 | val_0_rmse: 0.85053 | val_1_rmse: 0.83953 |  0:06:27s
epoch 113| loss: 0.72819 | val_0_rmse: 0.92611 | val_1_rmse: 0.91532 |  0:06:30s
epoch 114| loss: 0.72892 | val_0_rmse: 0.90646 | val_1_rmse: 0.89698 |  0:06:34s
epoch 115| loss: 0.72994 | val_0_rmse: 0.94883 | val_1_rmse: 0.93914 |  0:06:37s
epoch 116| loss: 0.72971 | val_0_rmse: 0.93318 | val_1_rmse: 0.92316 |  0:06:41s
epoch 117| loss: 0.73548 | val_0_rmse: 0.9118  | val_1_rmse: 0.90152 |  0:06:44s
epoch 118| loss: 0.73255 | val_0_rmse: 0.9321  | val_1_rmse: 0.92275 |  0:06:48s
epoch 119| loss: 0.73129 | val_0_rmse: 0.92187 | val_1_rmse: 0.91229 |  0:06:51s
epoch 120| loss: 0.73177 | val_0_rmse: 0.90696 | val_1_rmse: 0.89556 |  0:06:54s
epoch 121| loss: 0.73012 | val_0_rmse: 0.85258 | val_1_rmse: 0.84441 |  0:06:58s
epoch 122| loss: 0.73105 | val_0_rmse: 0.87986 | val_1_rmse: 0.86913 |  0:07:01s
epoch 123| loss: 0.72999 | val_0_rmse: 0.97185 | val_1_rmse: 0.96306 |  0:07:05s
epoch 124| loss: 0.73271 | val_0_rmse: 0.92429 | val_1_rmse: 0.91466 |  0:07:08s
epoch 125| loss: 0.73217 | val_0_rmse: 0.92553 | val_1_rmse: 0.91493 |  0:07:12s
epoch 126| loss: 0.73583 | val_0_rmse: 0.86829 | val_1_rmse: 0.85716 |  0:07:15s
epoch 127| loss: 0.7337  | val_0_rmse: 0.93024 | val_1_rmse: 0.91909 |  0:07:18s
epoch 128| loss: 0.73434 | val_0_rmse: 0.85287 | val_1_rmse: 0.84084 |  0:07:22s
epoch 129| loss: 0.73495 | val_0_rmse: 0.89582 | val_1_rmse: 0.88623 |  0:07:25s
epoch 130| loss: 0.73149 | val_0_rmse: 0.88904 | val_1_rmse: 0.87858 |  0:07:29s
epoch 131| loss: 0.7304  | val_0_rmse: 0.85471 | val_1_rmse: 0.84388 |  0:07:32s
epoch 132| loss: 0.73188 | val_0_rmse: 0.96653 | val_1_rmse: 0.95742 |  0:07:36s
epoch 133| loss: 0.72955 | val_0_rmse: 0.89053 | val_1_rmse: 0.87945 |  0:07:39s
epoch 134| loss: 0.72831 | val_0_rmse: 0.92408 | val_1_rmse: 0.91364 |  0:07:42s
epoch 135| loss: 0.72888 | val_0_rmse: 0.9241  | val_1_rmse: 0.91287 |  0:07:46s
epoch 136| loss: 0.72922 | val_0_rmse: 0.85121 | val_1_rmse: 0.84094 |  0:07:49s
epoch 137| loss: 0.72954 | val_0_rmse: 0.86962 | val_1_rmse: 0.85912 |  0:07:53s
epoch 138| loss: 0.73101 | val_0_rmse: 0.85115 | val_1_rmse: 0.84209 |  0:07:56s
epoch 139| loss: 0.72955 | val_0_rmse: 0.92292 | val_1_rmse: 0.91302 |  0:08:00s
epoch 140| loss: 0.73135 | val_0_rmse: 0.92084 | val_1_rmse: 0.91191 |  0:08:03s
epoch 141| loss: 0.73095 | val_0_rmse: 0.8964  | val_1_rmse: 0.88756 |  0:08:06s
epoch 142| loss: 0.73644 | val_0_rmse: 0.9452  | val_1_rmse: 0.93574 |  0:08:10s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.83953
Best weights from best epoch are automatically used!
ended training at: 06:16:10
Feature importance:
Mean squared error is of 2983362475.6181602
Mean absolute error:40733.42122211685
MAPE:0.5623367359821203
R2 score:0.2654448301667469
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:16:10
epoch 0  | loss: 1.07242 | val_0_rmse: 0.94538 | val_1_rmse: 0.94217 |  0:00:03s
epoch 1  | loss: 0.83437 | val_0_rmse: 0.94139 | val_1_rmse: 0.93806 |  0:00:06s
epoch 2  | loss: 0.81802 | val_0_rmse: 0.96638 | val_1_rmse: 0.96608 |  0:00:10s
epoch 3  | loss: 0.8149  | val_0_rmse: 0.97    | val_1_rmse: 0.97134 |  0:00:13s
epoch 4  | loss: 0.8101  | val_0_rmse: 0.92738 | val_1_rmse: 0.92773 |  0:00:17s
epoch 5  | loss: 0.80609 | val_0_rmse: 0.94631 | val_1_rmse: 0.94918 |  0:00:20s
epoch 6  | loss: 0.80547 | val_0_rmse: 0.93948 | val_1_rmse: 0.93844 |  0:00:24s
epoch 7  | loss: 0.8018  | val_0_rmse: 0.93003 | val_1_rmse: 0.92921 |  0:00:27s
epoch 8  | loss: 0.8041  | val_0_rmse: 0.9283  | val_1_rmse: 0.92902 |  0:00:30s
epoch 9  | loss: 0.80064 | val_0_rmse: 0.91482 | val_1_rmse: 0.91394 |  0:00:34s
epoch 10 | loss: 0.80434 | val_0_rmse: 0.90238 | val_1_rmse: 0.90134 |  0:00:37s
epoch 11 | loss: 0.79981 | val_0_rmse: 0.89804 | val_1_rmse: 0.89777 |  0:00:41s
epoch 12 | loss: 0.79276 | val_0_rmse: 0.90628 | val_1_rmse: 0.90725 |  0:00:44s
epoch 13 | loss: 0.77572 | val_0_rmse: 0.92497 | val_1_rmse: 0.92841 |  0:00:48s
epoch 14 | loss: 0.7669  | val_0_rmse: 0.94736 | val_1_rmse: 0.95234 |  0:00:51s
epoch 15 | loss: 0.81051 | val_0_rmse: 0.89965 | val_1_rmse: 0.90118 |  0:00:54s
epoch 16 | loss: 0.79586 | val_0_rmse: 0.89323 | val_1_rmse: 0.89624 |  0:00:58s
epoch 17 | loss: 0.79285 | val_0_rmse: 0.89243 | val_1_rmse: 0.89531 |  0:01:01s
epoch 18 | loss: 0.80644 | val_0_rmse: 0.89676 | val_1_rmse: 0.90158 |  0:01:05s
epoch 19 | loss: 0.80396 | val_0_rmse: 0.89344 | val_1_rmse: 0.89923 |  0:01:08s
epoch 20 | loss: 0.79645 | val_0_rmse: 0.89256 | val_1_rmse: 0.89747 |  0:01:12s
epoch 21 | loss: 0.79337 | val_0_rmse: 0.88983 | val_1_rmse: 0.89325 |  0:01:15s
epoch 22 | loss: 0.79471 | val_0_rmse: 0.89026 | val_1_rmse: 0.89372 |  0:01:19s
epoch 23 | loss: 0.79187 | val_0_rmse: 0.88892 | val_1_rmse: 0.8925  |  0:01:22s
epoch 24 | loss: 0.79366 | val_0_rmse: 0.88887 | val_1_rmse: 0.89193 |  0:01:26s
epoch 25 | loss: 0.7905  | val_0_rmse: 0.888   | val_1_rmse: 0.8915  |  0:01:29s
epoch 26 | loss: 0.79066 | val_0_rmse: 0.89041 | val_1_rmse: 0.8946  |  0:01:33s
epoch 27 | loss: 0.79138 | val_0_rmse: 0.88701 | val_1_rmse: 0.89068 |  0:01:36s
epoch 28 | loss: 0.79113 | val_0_rmse: 0.88792 | val_1_rmse: 0.8919  |  0:01:40s
epoch 29 | loss: 0.79043 | val_0_rmse: 0.88923 | val_1_rmse: 0.89257 |  0:01:43s
epoch 30 | loss: 0.78974 | val_0_rmse: 0.88808 | val_1_rmse: 0.89266 |  0:01:47s
epoch 31 | loss: 0.7895  | val_0_rmse: 0.88798 | val_1_rmse: 0.89353 |  0:01:50s
epoch 32 | loss: 0.78888 | val_0_rmse: 0.88885 | val_1_rmse: 0.89423 |  0:01:53s
epoch 33 | loss: 0.78886 | val_0_rmse: 0.88858 | val_1_rmse: 0.89409 |  0:01:57s
epoch 34 | loss: 0.78776 | val_0_rmse: 0.88756 | val_1_rmse: 0.89351 |  0:02:00s
epoch 35 | loss: 0.7872  | val_0_rmse: 0.88881 | val_1_rmse: 0.89704 |  0:02:04s
epoch 36 | loss: 0.78877 | val_0_rmse: 0.8885  | val_1_rmse: 0.89597 |  0:02:07s
epoch 37 | loss: 0.78954 | val_0_rmse: 0.89043 | val_1_rmse: 0.8968  |  0:02:11s
epoch 38 | loss: 0.78941 | val_0_rmse: 0.8918  | val_1_rmse: 0.89991 |  0:02:14s
epoch 39 | loss: 0.7889  | val_0_rmse: 0.89225 | val_1_rmse: 0.892   |  0:02:18s
epoch 40 | loss: 0.79109 | val_0_rmse: 1.48577 | val_1_rmse: 2.12475 |  0:02:21s
epoch 41 | loss: 0.7897  | val_0_rmse: 1.32246 | val_1_rmse: 1.81916 |  0:02:24s
epoch 42 | loss: 0.78923 | val_0_rmse: 1.25277 | val_1_rmse: 1.68605 |  0:02:28s
epoch 43 | loss: 0.78811 | val_0_rmse: 1.40398 | val_1_rmse: 1.97603 |  0:02:31s
epoch 44 | loss: 0.78771 | val_0_rmse: 0.8878  | val_1_rmse: 0.89138 |  0:02:35s
epoch 45 | loss: 0.79146 | val_0_rmse: 0.89025 | val_1_rmse: 0.89276 |  0:02:38s
epoch 46 | loss: 0.79019 | val_0_rmse: 0.89911 | val_1_rmse: 0.91749 |  0:02:42s
epoch 47 | loss: 0.79938 | val_0_rmse: 0.89117 | val_1_rmse: 0.89623 |  0:02:45s
epoch 48 | loss: 0.80996 | val_0_rmse: 0.89345 | val_1_rmse: 0.89907 |  0:02:48s
epoch 49 | loss: 0.79261 | val_0_rmse: 0.917   | val_1_rmse: 0.95303 |  0:02:52s
epoch 50 | loss: 0.79916 | val_0_rmse: 0.89949 | val_1_rmse: 0.9106  |  0:02:55s
epoch 51 | loss: 0.79929 | val_0_rmse: 0.9403  | val_1_rmse: 1.01438 |  0:02:59s
epoch 52 | loss: 0.79807 | val_0_rmse: 0.91402 | val_1_rmse: 0.94843 |  0:03:02s
epoch 53 | loss: 0.79839 | val_0_rmse: 0.91828 | val_1_rmse: 0.95822 |  0:03:06s
epoch 54 | loss: 0.79654 | val_0_rmse: 0.89314 | val_1_rmse: 0.89787 |  0:03:09s
epoch 55 | loss: 0.79695 | val_0_rmse: 0.89034 | val_1_rmse: 0.89456 |  0:03:13s
epoch 56 | loss: 0.79621 | val_0_rmse: 0.89319 | val_1_rmse: 0.89788 |  0:03:16s
epoch 57 | loss: 0.79442 | val_0_rmse: 0.89194 | val_1_rmse: 0.89824 |  0:03:19s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.89068
Best weights from best epoch are automatically used!
ended training at: 06:19:31
Feature importance:
Mean squared error is of 3234250164.0714173
Mean absolute error:43045.04979154593
MAPE:0.6172277737499847
R2 score:0.19436948005018317
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:19:31
epoch 0  | loss: 1.08844 | val_0_rmse: 0.96222 | val_1_rmse: 0.96163 |  0:00:03s
epoch 1  | loss: 0.85982 | val_0_rmse: 0.93135 | val_1_rmse: 0.93672 |  0:00:06s
epoch 2  | loss: 0.84478 | val_0_rmse: 0.93742 | val_1_rmse: 0.93957 |  0:00:10s
epoch 3  | loss: 0.82386 | val_0_rmse: 0.96009 | val_1_rmse: 0.95949 |  0:00:13s
epoch 4  | loss: 0.81408 | val_0_rmse: 0.92167 | val_1_rmse: 0.92432 |  0:00:17s
epoch 5  | loss: 0.80807 | val_0_rmse: 0.91845 | val_1_rmse: 0.92262 |  0:00:20s
epoch 6  | loss: 0.8045  | val_0_rmse: 0.90756 | val_1_rmse: 0.91144 |  0:00:24s
epoch 7  | loss: 0.80436 | val_0_rmse: 0.90513 | val_1_rmse: 0.90844 |  0:00:27s
epoch 8  | loss: 0.80522 | val_0_rmse: 0.91459 | val_1_rmse: 0.91755 |  0:00:30s
epoch 9  | loss: 0.80078 | val_0_rmse: 0.90569 | val_1_rmse: 0.90742 |  0:00:34s
epoch 10 | loss: 0.79807 | val_0_rmse: 0.93717 | val_1_rmse: 0.93777 |  0:00:37s
epoch 11 | loss: 0.78802 | val_0_rmse: 0.97764 | val_1_rmse: 0.97751 |  0:00:41s
epoch 12 | loss: 0.79697 | val_0_rmse: 0.89672 | val_1_rmse: 0.89927 |  0:00:44s
epoch 13 | loss: 0.79632 | val_0_rmse: 0.90974 | val_1_rmse: 0.91605 |  0:00:47s
epoch 14 | loss: 0.78628 | val_0_rmse: 0.90753 | val_1_rmse: 0.91299 |  0:00:51s
epoch 15 | loss: 0.76769 | val_0_rmse: 0.98641 | val_1_rmse: 0.98968 |  0:00:54s
epoch 16 | loss: 0.76438 | val_0_rmse: 1.09465 | val_1_rmse: 1.0953  |  0:00:58s
epoch 17 | loss: 0.76186 | val_0_rmse: 1.01455 | val_1_rmse: 1.01547 |  0:01:01s
epoch 18 | loss: 0.76634 | val_0_rmse: 0.98631 | val_1_rmse: 0.9881  |  0:01:05s
epoch 19 | loss: 0.76083 | val_0_rmse: 0.99879 | val_1_rmse: 0.99905 |  0:01:08s
epoch 20 | loss: 0.75653 | val_0_rmse: 0.87945 | val_1_rmse: 0.88333 |  0:01:12s
epoch 21 | loss: 0.75444 | val_0_rmse: 0.93259 | val_1_rmse: 0.93792 |  0:01:15s
epoch 22 | loss: 0.7553  | val_0_rmse: 1.09136 | val_1_rmse: 1.09894 |  0:01:19s
epoch 23 | loss: 0.76614 | val_0_rmse: 0.87693 | val_1_rmse: 0.88299 |  0:01:22s
epoch 24 | loss: 0.76075 | val_0_rmse: 0.96794 | val_1_rmse: 0.97604 |  0:01:26s
epoch 25 | loss: 0.79555 | val_0_rmse: 0.90329 | val_1_rmse: 0.9063  |  0:01:29s
epoch 26 | loss: 0.80447 | val_0_rmse: 1.03995 | val_1_rmse: 1.04305 |  0:01:33s
epoch 27 | loss: 0.80891 | val_0_rmse: 0.90523 | val_1_rmse: 0.89993 |  0:01:36s
epoch 28 | loss: 0.80353 | val_0_rmse: 0.90161 | val_1_rmse: 0.89837 |  0:01:39s
epoch 29 | loss: 0.7995  | val_0_rmse: 0.9011  | val_1_rmse: 0.89811 |  0:01:43s
epoch 30 | loss: 0.79813 | val_0_rmse: 0.89338 | val_1_rmse: 0.89964 |  0:01:46s
epoch 31 | loss: 0.79559 | val_0_rmse: 0.9344  | val_1_rmse: 0.93146 |  0:01:49s
epoch 32 | loss: 0.77106 | val_0_rmse: 0.86815 | val_1_rmse: 0.87052 |  0:01:53s
epoch 33 | loss: 0.75952 | val_0_rmse: 0.87036 | val_1_rmse: 0.87664 |  0:01:56s
epoch 34 | loss: 0.75745 | val_0_rmse: 0.90272 | val_1_rmse: 0.91085 |  0:02:00s
epoch 35 | loss: 0.75412 | val_0_rmse: 0.91102 | val_1_rmse: 0.915   |  0:02:03s
epoch 36 | loss: 0.74998 | val_0_rmse: 1.09262 | val_1_rmse: 1.09631 |  0:02:07s
epoch 37 | loss: 0.74994 | val_0_rmse: 0.96419 | val_1_rmse: 0.96321 |  0:02:10s
epoch 38 | loss: 0.75071 | val_0_rmse: 0.88759 | val_1_rmse: 0.89249 |  0:02:14s
epoch 39 | loss: 0.74807 | val_0_rmse: 1.00844 | val_1_rmse: 1.01197 |  0:02:17s
epoch 40 | loss: 0.74525 | val_0_rmse: 0.98135 | val_1_rmse: 0.98329 |  0:02:20s
epoch 41 | loss: 0.74834 | val_0_rmse: 0.93472 | val_1_rmse: 0.94215 |  0:02:24s
epoch 42 | loss: 0.74186 | val_0_rmse: 1.11897 | val_1_rmse: 1.11952 |  0:02:27s
epoch 43 | loss: 0.74603 | val_0_rmse: 0.92787 | val_1_rmse: 0.93497 |  0:02:31s
epoch 44 | loss: 0.74605 | val_0_rmse: 0.90826 | val_1_rmse: 0.9142  |  0:02:34s
epoch 45 | loss: 0.74272 | val_0_rmse: 1.10762 | val_1_rmse: 1.10841 |  0:02:38s
epoch 46 | loss: 0.74303 | val_0_rmse: 0.86551 | val_1_rmse: 0.8676  |  0:02:41s
epoch 47 | loss: 0.74173 | val_0_rmse: 0.94228 | val_1_rmse: 0.94865 |  0:02:44s
epoch 48 | loss: 0.74938 | val_0_rmse: 0.94056 | val_1_rmse: 0.94059 |  0:02:48s
epoch 49 | loss: 0.74186 | val_0_rmse: 1.04284 | val_1_rmse: 1.05113 |  0:02:51s
epoch 50 | loss: 0.75092 | val_0_rmse: 1.10252 | val_1_rmse: 1.10298 |  0:02:55s
epoch 51 | loss: 0.7464  | val_0_rmse: 0.86173 | val_1_rmse: 0.86443 |  0:02:58s
epoch 52 | loss: 0.74149 | val_0_rmse: 0.92492 | val_1_rmse: 0.93047 |  0:03:02s
epoch 53 | loss: 0.7453  | val_0_rmse: 1.09777 | val_1_rmse: 1.0953  |  0:03:05s
epoch 54 | loss: 0.74419 | val_0_rmse: 1.00505 | val_1_rmse: 1.0043  |  0:03:08s
epoch 55 | loss: 0.74199 | val_0_rmse: 0.87065 | val_1_rmse: 0.86247 |  0:03:12s
epoch 56 | loss: 0.74164 | val_0_rmse: 0.97256 | val_1_rmse: 0.91119 |  0:03:15s
epoch 57 | loss: 0.74388 | val_0_rmse: 0.8643  | val_1_rmse: 0.86887 |  0:03:19s
epoch 58 | loss: 0.74162 | val_0_rmse: 0.92788 | val_1_rmse: 0.93104 |  0:03:22s
epoch 59 | loss: 0.74048 | val_0_rmse: 0.96056 | val_1_rmse: 0.88616 |  0:03:26s
epoch 60 | loss: 0.74012 | val_0_rmse: 1.03392 | val_1_rmse: 1.04208 |  0:03:29s
epoch 61 | loss: 0.74613 | val_0_rmse: 0.86129 | val_1_rmse: 0.86501 |  0:03:32s
epoch 62 | loss: 0.74012 | val_0_rmse: 0.87944 | val_1_rmse: 0.88813 |  0:03:36s
epoch 63 | loss: 0.73812 | val_0_rmse: 1.07403 | val_1_rmse: 1.07626 |  0:03:39s
epoch 64 | loss: 0.73815 | val_0_rmse: 0.85843 | val_1_rmse: 0.86392 |  0:03:43s
epoch 65 | loss: 0.74243 | val_0_rmse: 1.02617 | val_1_rmse: 1.03525 |  0:03:46s
epoch 66 | loss: 0.7396  | val_0_rmse: 0.88333 | val_1_rmse: 0.89229 |  0:03:50s
epoch 67 | loss: 0.74798 | val_0_rmse: 1.16336 | val_1_rmse: 1.14999 |  0:03:53s
epoch 68 | loss: 0.7481  | val_0_rmse: 1.06739 | val_1_rmse: 0.86046 |  0:03:56s
epoch 69 | loss: 0.74051 | val_0_rmse: 1.75025 | val_1_rmse: 0.88526 |  0:04:00s
epoch 70 | loss: 0.73922 | val_0_rmse: 1.12307 | val_1_rmse: 0.97724 |  0:04:03s
epoch 71 | loss: 0.74145 | val_0_rmse: 0.958   | val_1_rmse: 0.92484 |  0:04:07s
epoch 72 | loss: 0.73636 | val_0_rmse: 0.97063 | val_1_rmse: 0.89117 |  0:04:10s
epoch 73 | loss: 0.73857 | val_0_rmse: 0.87865 | val_1_rmse: 0.88079 |  0:04:14s
epoch 74 | loss: 0.73684 | val_0_rmse: 1.16087 | val_1_rmse: 1.16246 |  0:04:17s
epoch 75 | loss: 0.73766 | val_0_rmse: 1.09333 | val_1_rmse: 1.09464 |  0:04:20s
epoch 76 | loss: 0.73792 | val_0_rmse: 0.90813 | val_1_rmse: 0.91189 |  0:04:24s
epoch 77 | loss: 0.73961 | val_0_rmse: 0.9711  | val_1_rmse: 0.90135 |  0:04:27s
epoch 78 | loss: 0.73741 | val_0_rmse: 1.06082 | val_1_rmse: 0.90031 |  0:04:31s
epoch 79 | loss: 0.73606 | val_0_rmse: 1.01908 | val_1_rmse: 1.02797 |  0:04:34s
epoch 80 | loss: 0.73341 | val_0_rmse: 0.95253 | val_1_rmse: 0.95341 |  0:04:38s
epoch 81 | loss: 0.73547 | val_0_rmse: 1.04639 | val_1_rmse: 0.9285  |  0:04:41s
epoch 82 | loss: 0.73369 | val_0_rmse: 1.12243 | val_1_rmse: 0.91841 |  0:04:45s
epoch 83 | loss: 0.73466 | val_0_rmse: 0.90911 | val_1_rmse: 0.91323 |  0:04:48s
epoch 84 | loss: 0.73594 | val_0_rmse: 1.04698 | val_1_rmse: 1.04857 |  0:04:51s
epoch 85 | loss: 0.734   | val_0_rmse: 0.92602 | val_1_rmse: 0.92789 |  0:04:55s
epoch 86 | loss: 0.73198 | val_0_rmse: 0.89043 | val_1_rmse: 0.89427 |  0:04:58s
epoch 87 | loss: 0.73249 | val_0_rmse: 1.03136 | val_1_rmse: 1.03213 |  0:05:02s
epoch 88 | loss: 0.73153 | val_0_rmse: 1.00782 | val_1_rmse: 0.94473 |  0:05:05s
epoch 89 | loss: 0.73322 | val_0_rmse: 0.858   | val_1_rmse: 0.86158 |  0:05:08s
epoch 90 | loss: 0.75436 | val_0_rmse: 0.88819 | val_1_rmse: 0.89137 |  0:05:12s
epoch 91 | loss: 0.74342 | val_0_rmse: 0.85733 | val_1_rmse: 0.86222 |  0:05:15s
epoch 92 | loss: 0.74038 | val_0_rmse: 0.8541  | val_1_rmse: 0.85772 |  0:05:19s
epoch 93 | loss: 0.7374  | val_0_rmse: 0.91011 | val_1_rmse: 0.91918 |  0:05:22s
epoch 94 | loss: 0.73963 | val_0_rmse: 0.91414 | val_1_rmse: 0.91749 |  0:05:26s
epoch 95 | loss: 0.73922 | val_0_rmse: 0.85477 | val_1_rmse: 0.85794 |  0:05:29s
epoch 96 | loss: 0.7343  | val_0_rmse: 1.06162 | val_1_rmse: 1.06189 |  0:05:32s
epoch 97 | loss: 0.73462 | val_0_rmse: 0.90166 | val_1_rmse: 0.90496 |  0:05:36s
epoch 98 | loss: 0.7358  | val_0_rmse: 0.94551 | val_1_rmse: 0.95432 |  0:05:39s
epoch 99 | loss: 0.73542 | val_0_rmse: 1.08285 | val_1_rmse: 1.08369 |  0:05:43s
epoch 100| loss: 0.73464 | val_0_rmse: 1.04683 | val_1_rmse: 1.05634 |  0:05:46s
epoch 101| loss: 0.73814 | val_0_rmse: 0.9878  | val_1_rmse: 0.98993 |  0:05:50s
epoch 102| loss: 0.73636 | val_0_rmse: 0.91107 | val_1_rmse: 0.91604 |  0:05:53s
epoch 103| loss: 0.73914 | val_0_rmse: 0.8753  | val_1_rmse: 0.88253 |  0:05:56s
epoch 104| loss: 0.73446 | val_0_rmse: 1.09466 | val_1_rmse: 1.09487 |  0:06:00s
epoch 105| loss: 0.73336 | val_0_rmse: 0.85939 | val_1_rmse: 0.86349 |  0:06:03s
epoch 106| loss: 0.73447 | val_0_rmse: 0.90212 | val_1_rmse: 0.90882 |  0:06:06s
epoch 107| loss: 0.73411 | val_0_rmse: 0.89232 | val_1_rmse: 0.9007  |  0:06:10s
epoch 108| loss: 0.7322  | val_0_rmse: 0.85667 | val_1_rmse: 0.86001 |  0:06:13s
epoch 109| loss: 0.73323 | val_0_rmse: 0.88143 | val_1_rmse: 0.88598 |  0:06:17s
epoch 110| loss: 0.73441 | val_0_rmse: 0.89368 | val_1_rmse: 0.89783 |  0:06:20s
epoch 111| loss: 0.73406 | val_0_rmse: 0.95678 | val_1_rmse: 0.96268 |  0:06:24s
epoch 112| loss: 0.73471 | val_0_rmse: 0.9069  | val_1_rmse: 0.91163 |  0:06:27s
epoch 113| loss: 0.73285 | val_0_rmse: 0.90355 | val_1_rmse: 0.90917 |  0:06:31s
epoch 114| loss: 0.7312  | val_0_rmse: 0.8664  | val_1_rmse: 0.86937 |  0:06:34s
epoch 115| loss: 0.73284 | val_0_rmse: 0.86379 | val_1_rmse: 0.86851 |  0:06:37s
epoch 116| loss: 0.73365 | val_0_rmse: 0.91687 | val_1_rmse: 0.92028 |  0:06:41s
epoch 117| loss: 0.74046 | val_0_rmse: 1.19541 | val_1_rmse: 1.19744 |  0:06:44s
epoch 118| loss: 0.73645 | val_0_rmse: 1.02054 | val_1_rmse: 1.02269 |  0:06:48s
epoch 119| loss: 0.73788 | val_0_rmse: 1.21038 | val_1_rmse: 1.22422 |  0:06:51s
epoch 120| loss: 0.74171 | val_0_rmse: 0.91348 | val_1_rmse: 0.92214 |  0:06:54s
epoch 121| loss: 0.73851 | val_0_rmse: 0.91999 | val_1_rmse: 0.92299 |  0:06:58s
epoch 122| loss: 0.73856 | val_0_rmse: 0.96565 | val_1_rmse: 0.9594  |  0:07:01s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.85772
Best weights from best epoch are automatically used!
ended training at: 06:26:34
Feature importance:
Mean squared error is of 2884846690.136607
Mean absolute error:40424.980975425155
MAPE:0.5770131746670122
R2 score:0.2752049525567274
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:26:35
epoch 0  | loss: 1.07615 | val_0_rmse: 1.00057 | val_1_rmse: 0.99883 |  0:00:03s
epoch 1  | loss: 0.85695 | val_0_rmse: 0.92771 | val_1_rmse: 0.92378 |  0:00:06s
epoch 2  | loss: 0.79851 | val_0_rmse: 0.96212 | val_1_rmse: 0.9605  |  0:00:10s
epoch 3  | loss: 0.78552 | val_0_rmse: 1.1526  | val_1_rmse: 1.14778 |  0:00:13s
epoch 4  | loss: 0.77368 | val_0_rmse: 1.24016 | val_1_rmse: 1.23225 |  0:00:17s
epoch 5  | loss: 0.77125 | val_0_rmse: 1.17756 | val_1_rmse: 1.17043 |  0:00:20s
epoch 6  | loss: 0.76665 | val_0_rmse: 1.1891  | val_1_rmse: 1.18165 |  0:00:24s
epoch 7  | loss: 0.77015 | val_0_rmse: 0.91028 | val_1_rmse: 0.90991 |  0:00:27s
epoch 8  | loss: 0.76173 | val_0_rmse: 1.05632 | val_1_rmse: 1.05175 |  0:00:30s
epoch 9  | loss: 0.75931 | val_0_rmse: 1.06939 | val_1_rmse: 1.06528 |  0:00:34s
epoch 10 | loss: 0.7535  | val_0_rmse: 0.95579 | val_1_rmse: 0.95273 |  0:00:37s
epoch 11 | loss: 0.7569  | val_0_rmse: 1.19242 | val_1_rmse: 1.18462 |  0:00:41s
epoch 12 | loss: 0.75365 | val_0_rmse: 1.35739 | val_1_rmse: 1.34607 |  0:00:44s
epoch 13 | loss: 0.75456 | val_0_rmse: 1.14319 | val_1_rmse: 1.13522 |  0:00:48s
epoch 14 | loss: 0.75422 | val_0_rmse: 0.91037 | val_1_rmse: 0.90853 |  0:00:51s
epoch 15 | loss: 0.75125 | val_0_rmse: 0.88329 | val_1_rmse: 0.8814  |  0:00:55s
epoch 16 | loss: 0.74659 | val_0_rmse: 0.91521 | val_1_rmse: 0.91333 |  0:00:58s
epoch 17 | loss: 0.74555 | val_0_rmse: 0.8707  | val_1_rmse: 0.86819 |  0:01:02s
epoch 18 | loss: 0.74562 | val_0_rmse: 0.93668 | val_1_rmse: 0.93457 |  0:01:05s
epoch 19 | loss: 0.75891 | val_0_rmse: 1.60059 | val_1_rmse: 1.58409 |  0:01:09s
epoch 20 | loss: 0.75573 | val_0_rmse: 0.89551 | val_1_rmse: 0.89568 |  0:01:12s
epoch 21 | loss: 0.74931 | val_0_rmse: 0.87535 | val_1_rmse: 0.87323 |  0:01:15s
epoch 22 | loss: 0.7455  | val_0_rmse: 0.98492 | val_1_rmse: 0.98166 |  0:01:19s
epoch 23 | loss: 0.74299 | val_0_rmse: 1.08255 | val_1_rmse: 1.07668 |  0:01:22s
epoch 24 | loss: 0.74883 | val_0_rmse: 1.1685  | val_1_rmse: 1.16025 |  0:01:26s
epoch 25 | loss: 0.74819 | val_0_rmse: 0.93021 | val_1_rmse: 0.92843 |  0:01:29s
epoch 26 | loss: 0.75146 | val_0_rmse: 0.86464 | val_1_rmse: 0.8619  |  0:01:32s
epoch 27 | loss: 0.74964 | val_0_rmse: 1.72247 | val_1_rmse: 1.71514 |  0:01:36s
epoch 28 | loss: 0.74646 | val_0_rmse: 0.93044 | val_1_rmse: 0.92946 |  0:01:39s
epoch 29 | loss: 0.74343 | val_0_rmse: 0.86369 | val_1_rmse: 0.85961 |  0:01:43s
epoch 30 | loss: 0.74227 | val_0_rmse: 0.91625 | val_1_rmse: 0.91038 |  0:01:46s
epoch 31 | loss: 0.74668 | val_0_rmse: 1.02434 | val_1_rmse: 1.01852 |  0:01:50s
epoch 32 | loss: 0.74297 | val_0_rmse: 0.88135 | val_1_rmse: 0.87681 |  0:01:53s
epoch 33 | loss: 0.74134 | val_0_rmse: 1.21237 | val_1_rmse: 1.2033  |  0:01:57s
epoch 34 | loss: 0.73984 | val_0_rmse: 1.04019 | val_1_rmse: 1.03942 |  0:02:00s
epoch 35 | loss: 0.74086 | val_0_rmse: 1.11433 | val_1_rmse: 1.10702 |  0:02:03s
epoch 36 | loss: 0.74252 | val_0_rmse: 1.09519 | val_1_rmse: 1.09025 |  0:02:07s
epoch 37 | loss: 0.74149 | val_0_rmse: 0.94478 | val_1_rmse: 0.94166 |  0:02:10s
epoch 38 | loss: 0.73989 | val_0_rmse: 0.95307 | val_1_rmse: 0.94905 |  0:02:14s
epoch 39 | loss: 0.7396  | val_0_rmse: 0.94887 | val_1_rmse: 0.94526 |  0:02:17s
epoch 40 | loss: 0.74536 | val_0_rmse: 1.42813 | val_1_rmse: 1.41332 |  0:02:21s
epoch 41 | loss: 0.75519 | val_0_rmse: 2.58494 | val_1_rmse: 2.5475  |  0:02:24s
epoch 42 | loss: 0.7386  | val_0_rmse: 0.92765 | val_1_rmse: 0.92566 |  0:02:28s
epoch 43 | loss: 0.73446 | val_0_rmse: 0.99525 | val_1_rmse: 0.98945 |  0:02:31s
epoch 44 | loss: 0.73798 | val_0_rmse: 0.9452  | val_1_rmse: 0.94266 |  0:02:34s
epoch 45 | loss: 0.73384 | val_0_rmse: 1.0174  | val_1_rmse: 1.01096 |  0:02:38s
epoch 46 | loss: 0.73643 | val_0_rmse: 0.91224 | val_1_rmse: 0.91158 |  0:02:41s
epoch 47 | loss: 0.73532 | val_0_rmse: 0.89817 | val_1_rmse: 0.89729 |  0:02:45s
epoch 48 | loss: 0.73256 | val_0_rmse: 0.91576 | val_1_rmse: 0.91423 |  0:02:48s
epoch 49 | loss: 0.73222 | val_0_rmse: 1.08408 | val_1_rmse: 1.07867 |  0:02:51s
epoch 50 | loss: 0.73641 | val_0_rmse: 0.89227 | val_1_rmse: 0.89202 |  0:02:55s
epoch 51 | loss: 0.76941 | val_0_rmse: 1.0213  | val_1_rmse: 1.01692 |  0:02:58s
epoch 52 | loss: 0.85942 | val_0_rmse: 0.92858 | val_1_rmse: 0.92523 |  0:03:02s
epoch 53 | loss: 0.84947 | val_0_rmse: 0.90978 | val_1_rmse: 0.90993 |  0:03:05s
epoch 54 | loss: 0.8244  | val_0_rmse: 0.90469 | val_1_rmse: 0.90382 |  0:03:09s
epoch 55 | loss: 0.80762 | val_0_rmse: 0.91101 | val_1_rmse: 0.90853 |  0:03:12s
epoch 56 | loss: 0.79671 | val_0_rmse: 0.89069 | val_1_rmse: 0.8862  |  0:03:16s
epoch 57 | loss: 0.78948 | val_0_rmse: 0.89056 | val_1_rmse: 0.89061 |  0:03:19s
epoch 58 | loss: 0.78842 | val_0_rmse: 0.89904 | val_1_rmse: 0.88325 |  0:03:23s
epoch 59 | loss: 0.7847  | val_0_rmse: 0.91049 | val_1_rmse: 0.90412 |  0:03:26s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.85961
Best weights from best epoch are automatically used!
ended training at: 06:30:02
Feature importance:
Mean squared error is of 2937945860.6274056
Mean absolute error:41892.29467456387
MAPE:0.6260874291256625
R2 score:0.24816050646067223
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:30:03
epoch 0  | loss: 1.10289 | val_0_rmse: 0.93196 | val_1_rmse: 0.91837 |  0:00:03s
epoch 1  | loss: 0.84    | val_0_rmse: 0.92875 | val_1_rmse: 0.91348 |  0:00:06s
epoch 2  | loss: 0.82597 | val_0_rmse: 0.92629 | val_1_rmse: 0.91048 |  0:00:10s
epoch 3  | loss: 0.82138 | val_0_rmse: 0.92795 | val_1_rmse: 0.91523 |  0:00:13s
epoch 4  | loss: 0.81334 | val_0_rmse: 0.92116 | val_1_rmse: 0.90656 |  0:00:17s
epoch 5  | loss: 0.81003 | val_0_rmse: 0.92344 | val_1_rmse: 0.90929 |  0:00:20s
epoch 6  | loss: 0.80901 | val_0_rmse: 0.91848 | val_1_rmse: 0.9049  |  0:00:24s
epoch 7  | loss: 0.80859 | val_0_rmse: 0.91902 | val_1_rmse: 0.90481 |  0:00:27s
epoch 8  | loss: 0.81036 | val_0_rmse: 0.91053 | val_1_rmse: 0.89732 |  0:00:30s
epoch 9  | loss: 0.80815 | val_0_rmse: 0.90961 | val_1_rmse: 0.89564 |  0:00:34s
epoch 10 | loss: 0.80742 | val_0_rmse: 0.90705 | val_1_rmse: 0.89315 |  0:00:37s
epoch 11 | loss: 0.80679 | val_0_rmse: 0.90312 | val_1_rmse: 0.89025 |  0:00:41s
epoch 12 | loss: 0.80508 | val_0_rmse: 0.90123 | val_1_rmse: 0.8877  |  0:00:44s
epoch 13 | loss: 0.80514 | val_0_rmse: 0.9001  | val_1_rmse: 0.8877  |  0:00:48s
epoch 14 | loss: 0.80423 | val_0_rmse: 0.90044 | val_1_rmse: 0.88984 |  0:00:51s
epoch 15 | loss: 0.80592 | val_0_rmse: 0.89651 | val_1_rmse: 0.88495 |  0:00:55s
epoch 16 | loss: 0.8014  | val_0_rmse: 0.89561 | val_1_rmse: 0.88467 |  0:00:58s
epoch 17 | loss: 0.80355 | val_0_rmse: 0.89366 | val_1_rmse: 0.88167 |  0:01:02s
epoch 18 | loss: 0.80028 | val_0_rmse: 0.89498 | val_1_rmse: 0.88334 |  0:01:05s
epoch 19 | loss: 0.80182 | val_0_rmse: 0.89433 | val_1_rmse: 0.88152 |  0:01:09s
epoch 20 | loss: 0.80187 | val_0_rmse: 0.89348 | val_1_rmse: 0.88185 |  0:01:12s
epoch 21 | loss: 0.80187 | val_0_rmse: 0.89527 | val_1_rmse: 0.88449 |  0:01:16s
epoch 22 | loss: 0.80074 | val_0_rmse: 0.89348 | val_1_rmse: 0.88294 |  0:01:19s
epoch 23 | loss: 0.79928 | val_0_rmse: 0.89285 | val_1_rmse: 0.88065 |  0:01:22s
epoch 24 | loss: 0.80023 | val_0_rmse: 0.89311 | val_1_rmse: 0.88043 |  0:01:26s
epoch 25 | loss: 0.79777 | val_0_rmse: 0.89567 | val_1_rmse: 0.88422 |  0:01:29s
epoch 26 | loss: 0.79921 | val_0_rmse: 0.895   | val_1_rmse: 0.88386 |  0:01:33s
epoch 27 | loss: 0.80062 | val_0_rmse: 0.89413 | val_1_rmse: 0.88338 |  0:01:36s
epoch 28 | loss: 0.80045 | val_0_rmse: 0.89434 | val_1_rmse: 0.88161 |  0:01:39s
epoch 29 | loss: 0.79921 | val_0_rmse: 0.8941  | val_1_rmse: 0.88172 |  0:01:43s
epoch 30 | loss: 0.79912 | val_0_rmse: 0.89284 | val_1_rmse: 0.88106 |  0:01:46s
epoch 31 | loss: 0.8     | val_0_rmse: 0.8952  | val_1_rmse: 0.88254 |  0:01:50s
epoch 32 | loss: 0.80191 | val_0_rmse: 0.89495 | val_1_rmse: 0.88148 |  0:01:53s
epoch 33 | loss: 0.8012  | val_0_rmse: 0.89736 | val_1_rmse: 0.88311 |  0:01:57s
epoch 34 | loss: 0.8023  | val_0_rmse: 0.89412 | val_1_rmse: 0.88153 |  0:02:00s
epoch 35 | loss: 0.79955 | val_0_rmse: 0.91609 | val_1_rmse: 0.90665 |  0:02:04s
epoch 36 | loss: 0.79716 | val_0_rmse: 0.89344 | val_1_rmse: 0.881   |  0:02:07s
epoch 37 | loss: 0.79816 | val_0_rmse: 0.8925  | val_1_rmse: 0.87894 |  0:02:10s
epoch 38 | loss: 0.7986  | val_0_rmse: 0.89398 | val_1_rmse: 0.88152 |  0:02:14s
epoch 39 | loss: 0.79912 | val_0_rmse: 0.89373 | val_1_rmse: 0.88158 |  0:02:17s
epoch 40 | loss: 0.80173 | val_0_rmse: 1.04837 | val_1_rmse: 1.05955 |  0:02:21s
epoch 41 | loss: 0.79991 | val_0_rmse: 0.90177 | val_1_rmse: 0.88315 |  0:02:24s
epoch 42 | loss: 0.7995  | val_0_rmse: 0.89504 | val_1_rmse: 0.88104 |  0:02:28s
epoch 43 | loss: 0.79979 | val_0_rmse: 0.89375 | val_1_rmse: 0.87984 |  0:02:31s
epoch 44 | loss: 0.79895 | val_0_rmse: 0.89408 | val_1_rmse: 0.88014 |  0:02:35s
epoch 45 | loss: 0.79847 | val_0_rmse: 0.89377 | val_1_rmse: 0.88021 |  0:02:38s
epoch 46 | loss: 0.7971  | val_0_rmse: 0.89278 | val_1_rmse: 0.88007 |  0:02:41s
epoch 47 | loss: 0.79719 | val_0_rmse: 0.89239 | val_1_rmse: 0.87927 |  0:02:45s
epoch 48 | loss: 0.79776 | val_0_rmse: 0.89401 | val_1_rmse: 0.88032 |  0:02:48s
epoch 49 | loss: 0.79756 | val_0_rmse: 0.89269 | val_1_rmse: 0.8804  |  0:02:52s
epoch 50 | loss: 0.79894 | val_0_rmse: 0.89339 | val_1_rmse: 0.88004 |  0:02:55s
epoch 51 | loss: 0.80023 | val_0_rmse: 0.89379 | val_1_rmse: 0.88055 |  0:02:59s
epoch 52 | loss: 0.80145 | val_0_rmse: 0.91158 | val_1_rmse: 0.88188 |  0:03:02s
epoch 53 | loss: 0.80053 | val_0_rmse: 0.89488 | val_1_rmse: 0.88272 |  0:03:06s
epoch 54 | loss: 0.80058 | val_0_rmse: 0.90894 | val_1_rmse: 0.88357 |  0:03:09s
epoch 55 | loss: 0.80009 | val_0_rmse: 0.90303 | val_1_rmse: 0.88269 |  0:03:12s
epoch 56 | loss: 0.79968 | val_0_rmse: 0.90363 | val_1_rmse: 0.88369 |  0:03:16s
epoch 57 | loss: 0.79873 | val_0_rmse: 0.96571 | val_1_rmse: 1.11339 |  0:03:19s
epoch 58 | loss: 0.80009 | val_0_rmse: 0.96852 | val_1_rmse: 1.12652 |  0:03:23s
epoch 59 | loss: 0.79932 | val_0_rmse: 0.90003 | val_1_rmse: 0.88158 |  0:03:26s
epoch 60 | loss: 0.80044 | val_0_rmse: 0.90618 | val_1_rmse: 0.88565 |  0:03:30s
epoch 61 | loss: 0.80223 | val_0_rmse: 0.90887 | val_1_rmse: 0.88863 |  0:03:33s
epoch 62 | loss: 0.80089 | val_0_rmse: 0.90802 | val_1_rmse: 0.89758 |  0:03:37s
epoch 63 | loss: 0.80172 | val_0_rmse: 0.90348 | val_1_rmse: 0.88938 |  0:03:40s
epoch 64 | loss: 0.7983  | val_0_rmse: 0.90266 | val_1_rmse: 0.88218 |  0:03:44s
epoch 65 | loss: 0.80586 | val_0_rmse: 0.93356 | val_1_rmse: 0.90141 |  0:03:47s
epoch 66 | loss: 0.80458 | val_0_rmse: 0.9026  | val_1_rmse: 0.88259 |  0:03:50s
epoch 67 | loss: 0.80235 | val_0_rmse: 0.90434 | val_1_rmse: 0.88527 |  0:03:54s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.87894
Best weights from best epoch are automatically used!
ended training at: 06:33:58
Feature importance:
Mean squared error is of 3810635940.92602
Mean absolute error:43065.76359543453
MAPE:0.6188689764121164
R2 score:0.05752887158299769
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:33:58
epoch 0  | loss: 1.07015 | val_0_rmse: 0.9992  | val_1_rmse: 1.024   |  0:00:03s
epoch 1  | loss: 0.83773 | val_0_rmse: 0.93709 | val_1_rmse: 0.96373 |  0:00:06s
epoch 2  | loss: 0.81885 | val_0_rmse: 0.99916 | val_1_rmse: 1.02394 |  0:00:10s
epoch 3  | loss: 0.80693 | val_0_rmse: 0.93446 | val_1_rmse: 0.95987 |  0:00:13s
epoch 4  | loss: 0.79706 | val_0_rmse: 0.9172  | val_1_rmse: 0.94346 |  0:00:17s
epoch 5  | loss: 0.80052 | val_0_rmse: 0.91517 | val_1_rmse: 0.94166 |  0:00:20s
epoch 6  | loss: 0.80464 | val_0_rmse: 1.03198 | val_1_rmse: 1.05496 |  0:00:24s
epoch 7  | loss: 0.79981 | val_0_rmse: 0.90517 | val_1_rmse: 0.9305  |  0:00:27s
epoch 8  | loss: 0.79577 | val_0_rmse: 0.90074 | val_1_rmse: 0.92721 |  0:00:31s
epoch 9  | loss: 0.79051 | val_0_rmse: 0.89585 | val_1_rmse: 0.92304 |  0:00:34s
epoch 10 | loss: 0.79096 | val_0_rmse: 0.89808 | val_1_rmse: 0.92504 |  0:00:38s
epoch 11 | loss: 0.79219 | val_0_rmse: 0.89298 | val_1_rmse: 0.91912 |  0:00:41s
epoch 12 | loss: 0.79021 | val_0_rmse: 0.89103 | val_1_rmse: 0.91751 |  0:00:44s
epoch 13 | loss: 0.78924 | val_0_rmse: 0.88802 | val_1_rmse: 0.91355 |  0:00:48s
epoch 14 | loss: 0.78636 | val_0_rmse: 0.88942 | val_1_rmse: 0.91454 |  0:00:51s
epoch 15 | loss: 0.78827 | val_0_rmse: 0.88561 | val_1_rmse: 0.91012 |  0:00:55s
epoch 16 | loss: 0.7783  | val_0_rmse: 0.90342 | val_1_rmse: 0.93026 |  0:00:58s
epoch 17 | loss: 0.77351 | val_0_rmse: 0.92276 | val_1_rmse: 0.94657 |  0:01:02s
epoch 18 | loss: 0.80554 | val_0_rmse: 0.88862 | val_1_rmse: 0.91203 |  0:01:05s
epoch 19 | loss: 0.79487 | val_0_rmse: 0.88685 | val_1_rmse: 0.91102 |  0:01:09s
epoch 20 | loss: 0.79259 | val_0_rmse: 0.8883  | val_1_rmse: 0.91545 |  0:01:12s
epoch 21 | loss: 0.77875 | val_0_rmse: 0.90628 | val_1_rmse: 0.93035 |  0:01:16s
epoch 22 | loss: 0.75212 | val_0_rmse: 0.86521 | val_1_rmse: 0.89129 |  0:01:19s
epoch 23 | loss: 0.7496  | val_0_rmse: 0.86199 | val_1_rmse: 0.88641 |  0:01:22s
epoch 24 | loss: 0.74578 | val_0_rmse: 0.8641  | val_1_rmse: 0.88863 |  0:01:26s
epoch 25 | loss: 0.74463 | val_0_rmse: 0.88311 | val_1_rmse: 0.91186 |  0:01:29s
epoch 26 | loss: 0.74574 | val_0_rmse: 0.87134 | val_1_rmse: 0.89722 |  0:01:33s
epoch 27 | loss: 0.75052 | val_0_rmse: 0.86841 | val_1_rmse: 0.89486 |  0:01:36s
epoch 28 | loss: 0.75363 | val_0_rmse: 0.86279 | val_1_rmse: 0.88792 |  0:01:40s
epoch 29 | loss: 0.74548 | val_0_rmse: 0.85964 | val_1_rmse: 0.88623 |  0:01:43s
epoch 30 | loss: 0.74291 | val_0_rmse: 0.97528 | val_1_rmse: 1.00033 |  0:01:46s
epoch 31 | loss: 0.73981 | val_0_rmse: 0.96672 | val_1_rmse: 0.99079 |  0:01:50s
epoch 32 | loss: 0.73987 | val_0_rmse: 0.86051 | val_1_rmse: 0.88708 |  0:01:53s
epoch 33 | loss: 0.74104 | val_0_rmse: 0.91554 | val_1_rmse: 0.94074 |  0:01:57s
epoch 34 | loss: 0.74418 | val_0_rmse: 0.88028 | val_1_rmse: 0.9069  |  0:02:00s
epoch 35 | loss: 0.73687 | val_0_rmse: 0.90272 | val_1_rmse: 0.92771 |  0:02:04s
epoch 36 | loss: 0.73474 | val_0_rmse: 0.85888 | val_1_rmse: 0.88284 |  0:02:07s
epoch 37 | loss: 0.73905 | val_0_rmse: 0.92949 | val_1_rmse: 0.95882 |  0:02:10s
epoch 38 | loss: 0.75552 | val_0_rmse: 1.12832 | val_1_rmse: 1.1526  |  0:02:14s
epoch 39 | loss: 0.7396  | val_0_rmse: 0.8773  | val_1_rmse: 0.90442 |  0:02:17s
epoch 40 | loss: 0.74565 | val_0_rmse: 0.96926 | val_1_rmse: 0.99739 |  0:02:21s
epoch 41 | loss: 0.74146 | val_0_rmse: 0.91779 | val_1_rmse: 0.94599 |  0:02:24s
epoch 42 | loss: 0.73768 | val_0_rmse: 1.1228  | val_1_rmse: 1.14537 |  0:02:28s
epoch 43 | loss: 0.73894 | val_0_rmse: 0.93545 | val_1_rmse: 0.96355 |  0:02:31s
epoch 44 | loss: 0.73768 | val_0_rmse: 0.91094 | val_1_rmse: 0.9376  |  0:02:34s
epoch 45 | loss: 0.73194 | val_0_rmse: 0.87146 | val_1_rmse: 0.89849 |  0:02:38s
epoch 46 | loss: 0.73132 | val_0_rmse: 0.93235 | val_1_rmse: 0.95831 |  0:02:41s
epoch 47 | loss: 0.72898 | val_0_rmse: 0.92265 | val_1_rmse: 0.94825 |  0:02:45s
epoch 48 | loss: 0.73441 | val_0_rmse: 0.85848 | val_1_rmse: 0.88127 |  0:02:48s
epoch 49 | loss: 0.73263 | val_0_rmse: 0.85438 | val_1_rmse: 0.87829 |  0:02:52s
epoch 50 | loss: 0.72902 | val_0_rmse: 1.13257 | val_1_rmse: 1.15404 |  0:02:55s
epoch 51 | loss: 0.73378 | val_0_rmse: 0.95341 | val_1_rmse: 0.9779  |  0:02:58s
epoch 52 | loss: 0.74092 | val_0_rmse: 0.88678 | val_1_rmse: 0.90927 |  0:03:02s
epoch 53 | loss: 0.73169 | val_0_rmse: 0.90179 | val_1_rmse: 0.9289  |  0:03:05s
epoch 54 | loss: 0.72835 | val_0_rmse: 1.1143  | val_1_rmse: 1.13493 |  0:03:09s
epoch 55 | loss: 0.72768 | val_0_rmse: 0.94889 | val_1_rmse: 0.97045 |  0:03:12s
epoch 56 | loss: 0.72894 | val_0_rmse: 1.04144 | val_1_rmse: 1.06987 |  0:03:16s
epoch 57 | loss: 0.73353 | val_0_rmse: 1.0842  | val_1_rmse: 1.10362 |  0:03:19s
epoch 58 | loss: 0.72674 | val_0_rmse: 0.95455 | val_1_rmse: 0.98268 |  0:03:23s
epoch 59 | loss: 0.72808 | val_0_rmse: 0.86832 | val_1_rmse: 0.89217 |  0:03:26s
epoch 60 | loss: 0.73403 | val_0_rmse: 0.85174 | val_1_rmse: 0.87418 |  0:03:29s
epoch 61 | loss: 0.72968 | val_0_rmse: 0.94501 | val_1_rmse: 0.96876 |  0:03:33s
epoch 62 | loss: 0.73318 | val_0_rmse: 0.91923 | val_1_rmse: 0.94415 |  0:03:36s
epoch 63 | loss: 0.729   | val_0_rmse: 1.02627 | val_1_rmse: 1.04851 |  0:03:40s
epoch 64 | loss: 0.72981 | val_0_rmse: 1.135   | val_1_rmse: 1.15127 |  0:03:43s
epoch 65 | loss: 0.73413 | val_0_rmse: 1.11198 | val_1_rmse: 1.13456 |  0:03:47s
epoch 66 | loss: 0.73982 | val_0_rmse: 0.88595 | val_1_rmse: 0.91183 |  0:03:50s
epoch 67 | loss: 0.73288 | val_0_rmse: 0.8685  | val_1_rmse: 0.89388 |  0:03:53s
epoch 68 | loss: 0.72557 | val_0_rmse: 1.156   | val_1_rmse: 1.17821 |  0:03:57s
epoch 69 | loss: 0.73461 | val_0_rmse: 1.08202 | val_1_rmse: 1.10516 |  0:04:00s
epoch 70 | loss: 0.72885 | val_0_rmse: 0.98913 | val_1_rmse: 1.01233 |  0:04:04s
epoch 71 | loss: 0.72418 | val_0_rmse: 0.91563 | val_1_rmse: 0.94158 |  0:04:07s
epoch 72 | loss: 0.72547 | val_0_rmse: 0.95905 | val_1_rmse: 0.98883 |  0:04:11s
epoch 73 | loss: 0.72658 | val_0_rmse: 0.97873 | val_1_rmse: 1.00276 |  0:04:14s
epoch 74 | loss: 0.72694 | val_0_rmse: 0.9835  | val_1_rmse: 1.00709 |  0:04:18s
epoch 75 | loss: 0.72372 | val_0_rmse: 1.01637 | val_1_rmse: 1.0386  |  0:04:21s
epoch 76 | loss: 0.72372 | val_0_rmse: 0.89359 | val_1_rmse: 0.92165 |  0:04:24s
epoch 77 | loss: 0.72179 | val_0_rmse: 1.11721 | val_1_rmse: 1.13867 |  0:04:28s
epoch 78 | loss: 0.72456 | val_0_rmse: 0.85242 | val_1_rmse: 0.87604 |  0:04:31s
epoch 79 | loss: 0.73172 | val_0_rmse: 0.86984 | val_1_rmse: 0.89588 |  0:04:35s
epoch 80 | loss: 0.73003 | val_0_rmse: 0.86895 | val_1_rmse: 0.89225 |  0:04:38s
epoch 81 | loss: 0.73895 | val_0_rmse: 0.9199  | val_1_rmse: 0.94539 |  0:04:42s
epoch 82 | loss: 0.74018 | val_0_rmse: 0.90131 | val_1_rmse: 0.92679 |  0:04:45s
epoch 83 | loss: 0.73028 | val_0_rmse: 1.02901 | val_1_rmse: 1.04778 |  0:04:48s
epoch 84 | loss: 0.72565 | val_0_rmse: 0.97444 | val_1_rmse: 0.99772 |  0:04:52s
epoch 85 | loss: 0.79121 | val_0_rmse: 0.89873 | val_1_rmse: 0.92491 |  0:04:55s
epoch 86 | loss: 0.74065 | val_0_rmse: 0.88481 | val_1_rmse: 0.90833 |  0:04:59s
epoch 87 | loss: 0.73057 | val_0_rmse: 0.92914 | val_1_rmse: 0.95216 |  0:05:02s
epoch 88 | loss: 0.72389 | val_0_rmse: 0.85645 | val_1_rmse: 0.8793  |  0:05:05s
epoch 89 | loss: 0.72575 | val_0_rmse: 1.12978 | val_1_rmse: 1.15073 |  0:05:09s
epoch 90 | loss: 0.72843 | val_0_rmse: 0.89321 | val_1_rmse: 0.91962 |  0:05:12s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.87418
Best weights from best epoch are automatically used!
ended training at: 06:39:12
Feature importance:
Mean squared error is of 3052464100.8905663
Mean absolute error:41169.31025519845
MAPE:0.5532408355634162
R2 score:0.2495568783537364
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:39:13
epoch 0  | loss: 1.12504 | val_0_rmse: 0.98425 | val_1_rmse: 0.98973 |  0:00:03s
epoch 1  | loss: 0.82309 | val_0_rmse: 0.95663 | val_1_rmse: 0.96115 |  0:00:06s
epoch 2  | loss: 0.81254 | val_0_rmse: 0.93269 | val_1_rmse: 0.93442 |  0:00:10s
epoch 3  | loss: 0.80465 | val_0_rmse: 0.93337 | val_1_rmse: 0.93667 |  0:00:13s
epoch 4  | loss: 0.80378 | val_0_rmse: 0.93971 | val_1_rmse: 0.94164 |  0:00:17s
epoch 5  | loss: 0.80158 | val_0_rmse: 0.91812 | val_1_rmse: 0.92098 |  0:00:20s
epoch 6  | loss: 0.80187 | val_0_rmse: 0.91473 | val_1_rmse: 0.9169  |  0:00:24s
epoch 7  | loss: 0.79902 | val_0_rmse: 0.91475 | val_1_rmse: 0.91653 |  0:00:27s
epoch 8  | loss: 0.79581 | val_0_rmse: 0.91017 | val_1_rmse: 0.91149 |  0:00:30s
epoch 9  | loss: 0.79902 | val_0_rmse: 0.90817 | val_1_rmse: 0.91069 |  0:00:34s
epoch 10 | loss: 0.79767 | val_0_rmse: 0.91724 | val_1_rmse: 0.92146 |  0:00:37s
epoch 11 | loss: 0.77975 | val_0_rmse: 0.91545 | val_1_rmse: 0.91462 |  0:00:41s
epoch 12 | loss: 0.77461 | val_0_rmse: 0.91154 | val_1_rmse: 0.91068 |  0:00:44s
epoch 13 | loss: 0.76519 | val_0_rmse: 0.90761 | val_1_rmse: 0.90529 |  0:00:48s
epoch 14 | loss: 0.76241 | val_0_rmse: 0.87199 | val_1_rmse: 0.87406 |  0:00:51s
epoch 15 | loss: 0.75531 | val_0_rmse: 0.88158 | val_1_rmse: 0.8837  |  0:00:54s
epoch 16 | loss: 0.75695 | val_0_rmse: 0.87394 | val_1_rmse: 0.87339 |  0:00:58s
epoch 17 | loss: 0.76063 | val_0_rmse: 0.87288 | val_1_rmse: 0.87426 |  0:01:01s
epoch 18 | loss: 0.75644 | val_0_rmse: 0.86951 | val_1_rmse: 0.87345 |  0:01:05s
epoch 19 | loss: 0.76026 | val_0_rmse: 0.88439 | val_1_rmse: 0.88628 |  0:01:08s
epoch 20 | loss: 0.75473 | val_0_rmse: 0.87445 | val_1_rmse: 0.87742 |  0:01:12s
epoch 21 | loss: 0.75487 | val_0_rmse: 0.9117  | val_1_rmse: 0.91538 |  0:01:15s
epoch 22 | loss: 0.75782 | val_0_rmse: 0.87709 | val_1_rmse: 0.88293 |  0:01:18s
epoch 23 | loss: 0.75958 | val_0_rmse: 0.88747 | val_1_rmse: 0.88937 |  0:01:22s
epoch 24 | loss: 0.76458 | val_0_rmse: 0.87525 | val_1_rmse: 0.88043 |  0:01:25s
epoch 25 | loss: 0.76622 | val_0_rmse: 0.88174 | val_1_rmse: 0.88872 |  0:01:29s
epoch 26 | loss: 0.75815 | val_0_rmse: 0.88662 | val_1_rmse: 0.89374 |  0:01:32s
epoch 27 | loss: 0.75836 | val_0_rmse: 0.87797 | val_1_rmse: 0.8851  |  0:01:36s
epoch 28 | loss: 0.75932 | val_0_rmse: 0.88259 | val_1_rmse: 0.89561 |  0:01:39s
epoch 29 | loss: 0.75591 | val_0_rmse: 0.91949 | val_1_rmse: 0.92384 |  0:01:43s
epoch 30 | loss: 0.7534  | val_0_rmse: 0.90562 | val_1_rmse: 0.93157 |  0:01:46s
epoch 31 | loss: 0.75316 | val_0_rmse: 0.88611 | val_1_rmse: 0.91623 |  0:01:49s
epoch 32 | loss: 0.75035 | val_0_rmse: 0.93066 | val_1_rmse: 1.00558 |  0:01:53s
epoch 33 | loss: 0.74878 | val_0_rmse: 0.88788 | val_1_rmse: 0.91026 |  0:01:56s
epoch 34 | loss: 0.74728 | val_0_rmse: 0.86705 | val_1_rmse: 0.88149 |  0:02:00s
epoch 35 | loss: 0.74618 | val_0_rmse: 0.94478 | val_1_rmse: 0.9502  |  0:02:03s
epoch 36 | loss: 0.74781 | val_0_rmse: 0.93829 | val_1_rmse: 0.96004 |  0:02:07s
epoch 37 | loss: 0.75093 | val_0_rmse: 0.8892  | val_1_rmse: 0.89595 |  0:02:10s
epoch 38 | loss: 0.74954 | val_0_rmse: 0.87192 | val_1_rmse: 0.88772 |  0:02:14s
epoch 39 | loss: 0.74861 | val_0_rmse: 0.86605 | val_1_rmse: 0.87608 |  0:02:17s
epoch 40 | loss: 0.75177 | val_0_rmse: 0.87248 | val_1_rmse: 0.87474 |  0:02:21s
epoch 41 | loss: 0.7483  | val_0_rmse: 0.87509 | val_1_rmse: 0.87789 |  0:02:24s
epoch 42 | loss: 0.75097 | val_0_rmse: 0.8792  | val_1_rmse: 0.88664 |  0:02:27s
epoch 43 | loss: 0.74868 | val_0_rmse: 0.89222 | val_1_rmse: 0.89654 |  0:02:31s
epoch 44 | loss: 0.7476  | val_0_rmse: 0.87703 | val_1_rmse: 0.88053 |  0:02:34s
epoch 45 | loss: 0.7475  | val_0_rmse: 0.88217 | val_1_rmse: 0.88976 |  0:02:38s
epoch 46 | loss: 0.74337 | val_0_rmse: 0.87297 | val_1_rmse: 0.88106 |  0:02:41s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.87339
Best weights from best epoch are automatically used!
ended training at: 06:41:55
Feature importance:
Mean squared error is of 3055128538.9266644
Mean absolute error:41629.62417886591
MAPE:0.5914954298953641
R2 score:0.23295060361044873
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:41:56
epoch 0  | loss: 8.0617  | val_0_rmse: 1.01372 | val_1_rmse: 1.09843 |  0:00:00s
epoch 1  | loss: 3.93527 | val_0_rmse: 1.00995 | val_1_rmse: 1.08726 |  0:00:00s
epoch 2  | loss: 1.99643 | val_0_rmse: 0.99473 | val_1_rmse: 1.07956 |  0:00:00s
epoch 3  | loss: 1.56763 | val_0_rmse: 0.99518 | val_1_rmse: 1.08018 |  0:00:00s
epoch 4  | loss: 1.73311 | val_0_rmse: 1.00855 | val_1_rmse: 1.0939  |  0:00:01s
epoch 5  | loss: 1.2033  | val_0_rmse: 1.02427 | val_1_rmse: 1.11096 |  0:00:01s
epoch 6  | loss: 1.39729 | val_0_rmse: 1.02341 | val_1_rmse: 1.10801 |  0:00:01s
epoch 7  | loss: 1.53928 | val_0_rmse: 1.00673 | val_1_rmse: 1.09102 |  0:00:01s
epoch 8  | loss: 1.17076 | val_0_rmse: 0.99767 | val_1_rmse: 1.082   |  0:00:01s
epoch 9  | loss: 1.1487  | val_0_rmse: 1.00093 | val_1_rmse: 1.07963 |  0:00:02s
epoch 10 | loss: 1.07659 | val_0_rmse: 1.00311 | val_1_rmse: 1.08024 |  0:00:02s
epoch 11 | loss: 1.0473  | val_0_rmse: 0.99448 | val_1_rmse: 1.07324 |  0:00:02s
epoch 12 | loss: 0.99907 | val_0_rmse: 0.98933 | val_1_rmse: 1.06495 |  0:00:02s
epoch 13 | loss: 1.0244  | val_0_rmse: 0.98955 | val_1_rmse: 1.06117 |  0:00:02s
epoch 14 | loss: 0.99658 | val_0_rmse: 0.99128 | val_1_rmse: 1.06357 |  0:00:03s
epoch 15 | loss: 0.95872 | val_0_rmse: 0.99182 | val_1_rmse: 1.06578 |  0:00:03s
epoch 16 | loss: 0.98119 | val_0_rmse: 0.99287 | val_1_rmse: 1.06408 |  0:00:03s
epoch 17 | loss: 0.99589 | val_0_rmse: 0.99777 | val_1_rmse: 1.06565 |  0:00:03s
epoch 18 | loss: 0.97669 | val_0_rmse: 1.00451 | val_1_rmse: 1.07219 |  0:00:04s
epoch 19 | loss: 0.96955 | val_0_rmse: 0.99407 | val_1_rmse: 1.06316 |  0:00:04s
epoch 20 | loss: 0.92506 | val_0_rmse: 1.00538 | val_1_rmse: 1.06893 |  0:00:04s
epoch 21 | loss: 0.91517 | val_0_rmse: 1.00431 | val_1_rmse: 1.06566 |  0:00:04s
epoch 22 | loss: 0.89532 | val_0_rmse: 1.00558 | val_1_rmse: 1.0639  |  0:00:04s
epoch 23 | loss: 0.86548 | val_0_rmse: 1.05054 | val_1_rmse: 1.10226 |  0:00:05s
epoch 24 | loss: 0.8684  | val_0_rmse: 1.09881 | val_1_rmse: 1.15146 |  0:00:05s
epoch 25 | loss: 0.83237 | val_0_rmse: 1.17252 | val_1_rmse: 1.21042 |  0:00:05s
epoch 26 | loss: 0.8537  | val_0_rmse: 1.19682 | val_1_rmse: 1.23459 |  0:00:05s
epoch 27 | loss: 0.87644 | val_0_rmse: 1.19689 | val_1_rmse: 1.25171 |  0:00:05s
epoch 28 | loss: 0.92809 | val_0_rmse: 1.13452 | val_1_rmse: 1.20202 |  0:00:06s
epoch 29 | loss: 0.88979 | val_0_rmse: 1.04452 | val_1_rmse: 1.11382 |  0:00:06s
epoch 30 | loss: 0.87181 | val_0_rmse: 1.00209 | val_1_rmse: 1.08262 |  0:00:06s
epoch 31 | loss: 0.83901 | val_0_rmse: 1.00083 | val_1_rmse: 1.08032 |  0:00:06s
epoch 32 | loss: 0.80278 | val_0_rmse: 1.00237 | val_1_rmse: 1.06719 |  0:00:06s
epoch 33 | loss: 0.78851 | val_0_rmse: 1.05708 | val_1_rmse: 1.10408 |  0:00:07s
epoch 34 | loss: 0.8582  | val_0_rmse: 1.07763 | val_1_rmse: 1.1219  |  0:00:07s
epoch 35 | loss: 0.78139 | val_0_rmse: 1.0066  | val_1_rmse: 1.06715 |  0:00:07s
epoch 36 | loss: 0.7742  | val_0_rmse: 1.01625 | val_1_rmse: 1.07519 |  0:00:07s
epoch 37 | loss: 0.78321 | val_0_rmse: 1.02566 | val_1_rmse: 1.09124 |  0:00:07s
epoch 38 | loss: 0.77093 | val_0_rmse: 1.0423  | val_1_rmse: 1.11066 |  0:00:08s
epoch 39 | loss: 0.74141 | val_0_rmse: 1.03155 | val_1_rmse: 1.09865 |  0:00:08s
epoch 40 | loss: 0.7361  | val_0_rmse: 1.01456 | val_1_rmse: 1.07941 |  0:00:08s
epoch 41 | loss: 0.75965 | val_0_rmse: 1.01399 | val_1_rmse: 1.0783  |  0:00:08s
epoch 42 | loss: 0.71341 | val_0_rmse: 1.02252 | val_1_rmse: 1.08684 |  0:00:08s
epoch 43 | loss: 0.72475 | val_0_rmse: 1.02612 | val_1_rmse: 1.06682 |  0:00:09s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 1.06117
Best weights from best epoch are automatically used!
ended training at: 06:42:05
Feature importance:
Mean squared error is of 6457200140.898105
Mean absolute error:65887.15822354716
MAPE:0.6729292938631743
R2 score:0.021686475666988225
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:42:05
epoch 0  | loss: 7.30879 | val_0_rmse: 1.16329 | val_1_rmse: 1.20165 |  0:00:00s
epoch 1  | loss: 2.94204 | val_0_rmse: 1.07441 | val_1_rmse: 1.10317 |  0:00:00s
epoch 2  | loss: 2.29018 | val_0_rmse: 1.01008 | val_1_rmse: 1.02602 |  0:00:00s
epoch 3  | loss: 1.61572 | val_0_rmse: 1.00414 | val_1_rmse: 1.01448 |  0:00:00s
epoch 4  | loss: 1.26334 | val_0_rmse: 1.00826 | val_1_rmse: 1.02081 |  0:00:01s
epoch 5  | loss: 1.17734 | val_0_rmse: 1.00643 | val_1_rmse: 1.01873 |  0:00:01s
epoch 6  | loss: 1.12539 | val_0_rmse: 1.00606 | val_1_rmse: 1.01453 |  0:00:01s
epoch 7  | loss: 1.07985 | val_0_rmse: 1.01106 | val_1_rmse: 1.01825 |  0:00:01s
epoch 8  | loss: 1.05086 | val_0_rmse: 1.00681 | val_1_rmse: 1.01542 |  0:00:01s
epoch 9  | loss: 1.04767 | val_0_rmse: 1.01094 | val_1_rmse: 1.01929 |  0:00:02s
epoch 10 | loss: 1.03016 | val_0_rmse: 1.00831 | val_1_rmse: 1.01843 |  0:00:02s
epoch 11 | loss: 1.00777 | val_0_rmse: 1.005   | val_1_rmse: 1.01704 |  0:00:02s
epoch 12 | loss: 1.00556 | val_0_rmse: 1.00516 | val_1_rmse: 1.01817 |  0:00:02s
epoch 13 | loss: 0.98197 | val_0_rmse: 1.00523 | val_1_rmse: 1.01947 |  0:00:02s
epoch 14 | loss: 1.02341 | val_0_rmse: 1.00633 | val_1_rmse: 1.02139 |  0:00:03s
epoch 15 | loss: 1.01492 | val_0_rmse: 1.00927 | val_1_rmse: 1.02491 |  0:00:03s
epoch 16 | loss: 1.00379 | val_0_rmse: 1.01351 | val_1_rmse: 1.03181 |  0:00:03s
epoch 17 | loss: 1.01645 | val_0_rmse: 1.00773 | val_1_rmse: 1.02161 |  0:00:03s
epoch 18 | loss: 0.98729 | val_0_rmse: 1.00573 | val_1_rmse: 1.01767 |  0:00:03s
epoch 19 | loss: 0.97732 | val_0_rmse: 1.00894 | val_1_rmse: 1.02414 |  0:00:04s
epoch 20 | loss: 0.95071 | val_0_rmse: 1.06609 | val_1_rmse: 1.09098 |  0:00:04s
epoch 21 | loss: 0.87939 | val_0_rmse: 1.17709 | val_1_rmse: 1.20843 |  0:00:04s
epoch 22 | loss: 0.83981 | val_0_rmse: 1.30875 | val_1_rmse: 1.34624 |  0:00:04s
epoch 23 | loss: 0.9752  | val_0_rmse: 1.22199 | val_1_rmse: 1.25916 |  0:00:05s
epoch 24 | loss: 0.83954 | val_0_rmse: 1.05476 | val_1_rmse: 1.0803  |  0:00:05s
epoch 25 | loss: 0.86348 | val_0_rmse: 1.08789 | val_1_rmse: 1.1151  |  0:00:05s
epoch 26 | loss: 0.81254 | val_0_rmse: 1.23208 | val_1_rmse: 1.26907 |  0:00:05s
epoch 27 | loss: 0.83162 | val_0_rmse: 1.35006 | val_1_rmse: 1.39242 |  0:00:05s
epoch 28 | loss: 0.82302 | val_0_rmse: 1.32385 | val_1_rmse: 1.36613 |  0:00:06s
epoch 29 | loss: 0.80977 | val_0_rmse: 1.29315 | val_1_rmse: 1.33622 |  0:00:06s
epoch 30 | loss: 0.79891 | val_0_rmse: 1.32715 | val_1_rmse: 1.37224 |  0:00:06s
epoch 31 | loss: 0.8065  | val_0_rmse: 1.386   | val_1_rmse: 1.43277 |  0:00:06s
epoch 32 | loss: 0.81881 | val_0_rmse: 1.39238 | val_1_rmse: 1.439   |  0:00:06s
epoch 33 | loss: 0.82424 | val_0_rmse: 1.2924  | val_1_rmse: 1.33545 |  0:00:07s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 1.01448
Best weights from best epoch are automatically used!
ended training at: 06:42:13
Feature importance:
Mean squared error is of 6854934227.474575
Mean absolute error:67680.88982154605
MAPE:0.62786547688983
R2 score:0.0020768394489866226
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:42:13
epoch 0  | loss: 7.20409 | val_0_rmse: 1.07625 | val_1_rmse: 1.02106 |  0:00:00s
epoch 1  | loss: 4.03207 | val_0_rmse: 1.05196 | val_1_rmse: 0.9982  |  0:00:00s
epoch 2  | loss: 2.88187 | val_0_rmse: 1.04001 | val_1_rmse: 0.98396 |  0:00:00s
epoch 3  | loss: 1.82464 | val_0_rmse: 1.01185 | val_1_rmse: 0.96568 |  0:00:00s
epoch 4  | loss: 1.89837 | val_0_rmse: 1.00724 | val_1_rmse: 0.96154 |  0:00:01s
epoch 5  | loss: 2.03551 | val_0_rmse: 1.00712 | val_1_rmse: 0.95972 |  0:00:01s
epoch 6  | loss: 1.46474 | val_0_rmse: 1.01394 | val_1_rmse: 0.96826 |  0:00:01s
epoch 7  | loss: 1.38184 | val_0_rmse: 1.01632 | val_1_rmse: 0.97076 |  0:00:01s
epoch 8  | loss: 1.18786 | val_0_rmse: 1.0163  | val_1_rmse: 0.9672  |  0:00:01s
epoch 9  | loss: 1.17599 | val_0_rmse: 1.03313 | val_1_rmse: 0.98824 |  0:00:02s
epoch 10 | loss: 1.26704 | val_0_rmse: 1.03055 | val_1_rmse: 0.9858  |  0:00:02s
epoch 11 | loss: 1.14977 | val_0_rmse: 1.00655 | val_1_rmse: 0.95895 |  0:00:02s
epoch 12 | loss: 1.1228  | val_0_rmse: 1.0059  | val_1_rmse: 0.95582 |  0:00:02s
epoch 13 | loss: 1.1016  | val_0_rmse: 1.01224 | val_1_rmse: 0.96019 |  0:00:03s
epoch 14 | loss: 1.02173 | val_0_rmse: 1.00608 | val_1_rmse: 0.95748 |  0:00:03s
epoch 15 | loss: 1.02549 | val_0_rmse: 1.00369 | val_1_rmse: 0.95308 |  0:00:03s
epoch 16 | loss: 1.05791 | val_0_rmse: 1.0082  | val_1_rmse: 0.95795 |  0:00:03s
epoch 17 | loss: 0.99012 | val_0_rmse: 1.00929 | val_1_rmse: 0.95837 |  0:00:03s
epoch 18 | loss: 1.00678 | val_0_rmse: 1.00587 | val_1_rmse: 0.9552  |  0:00:04s
epoch 19 | loss: 0.98723 | val_0_rmse: 1.01074 | val_1_rmse: 0.96061 |  0:00:04s
epoch 20 | loss: 1.00653 | val_0_rmse: 1.0097  | val_1_rmse: 0.95926 |  0:00:04s
epoch 21 | loss: 0.979   | val_0_rmse: 0.99547 | val_1_rmse: 0.947   |  0:00:04s
epoch 22 | loss: 1.00989 | val_0_rmse: 0.99586 | val_1_rmse: 0.94897 |  0:00:04s
epoch 23 | loss: 0.9236  | val_0_rmse: 0.97663 | val_1_rmse: 0.92609 |  0:00:05s
epoch 24 | loss: 0.91233 | val_0_rmse: 0.95459 | val_1_rmse: 0.90371 |  0:00:05s
epoch 25 | loss: 0.87215 | val_0_rmse: 1.06958 | val_1_rmse: 1.02096 |  0:00:05s
epoch 26 | loss: 0.87254 | val_0_rmse: 1.15134 | val_1_rmse: 1.10458 |  0:00:05s
epoch 27 | loss: 0.83532 | val_0_rmse: 1.20024 | val_1_rmse: 1.151   |  0:00:05s
epoch 28 | loss: 0.83646 | val_0_rmse: 1.35382 | val_1_rmse: 1.31558 |  0:00:06s
epoch 29 | loss: 0.8342  | val_0_rmse: 1.29361 | val_1_rmse: 1.25292 |  0:00:06s
epoch 30 | loss: 0.7897  | val_0_rmse: 1.1742  | val_1_rmse: 1.13326 |  0:00:06s
epoch 31 | loss: 0.82919 | val_0_rmse: 1.16326 | val_1_rmse: 1.1248  |  0:00:06s
epoch 32 | loss: 0.80175 | val_0_rmse: 1.1833  | val_1_rmse: 1.14397 |  0:00:06s
epoch 33 | loss: 0.7999  | val_0_rmse: 1.07951 | val_1_rmse: 1.04814 |  0:00:07s
epoch 34 | loss: 0.79784 | val_0_rmse: 0.9891  | val_1_rmse: 0.94442 |  0:00:07s
epoch 35 | loss: 0.79856 | val_0_rmse: 1.00596 | val_1_rmse: 0.9699  |  0:00:07s
epoch 36 | loss: 0.77474 | val_0_rmse: 1.00459 | val_1_rmse: 0.96837 |  0:00:07s
epoch 37 | loss: 0.77562 | val_0_rmse: 1.01097 | val_1_rmse: 0.97388 |  0:00:08s
epoch 38 | loss: 0.75133 | val_0_rmse: 1.01466 | val_1_rmse: 0.97625 |  0:00:08s
epoch 39 | loss: 0.77249 | val_0_rmse: 0.89169 | val_1_rmse: 0.85989 |  0:00:08s
epoch 40 | loss: 0.76113 | val_0_rmse: 1.1517  | val_1_rmse: 1.10915 |  0:00:08s
epoch 41 | loss: 0.79423 | val_0_rmse: 1.27213 | val_1_rmse: 1.23671 |  0:00:08s
epoch 42 | loss: 0.78143 | val_0_rmse: 1.31523 | val_1_rmse: 1.27176 |  0:00:09s
epoch 43 | loss: 0.76578 | val_0_rmse: 1.2218  | val_1_rmse: 1.18326 |  0:00:09s
epoch 44 | loss: 0.79659 | val_0_rmse: 1.16617 | val_1_rmse: 1.13144 |  0:00:09s
epoch 45 | loss: 0.75511 | val_0_rmse: 1.30876 | val_1_rmse: 1.2768  |  0:00:09s
epoch 46 | loss: 0.78587 | val_0_rmse: 1.3385  | val_1_rmse: 1.30978 |  0:00:09s
epoch 47 | loss: 0.74902 | val_0_rmse: 1.22216 | val_1_rmse: 1.18868 |  0:00:10s
epoch 48 | loss: 0.74519 | val_0_rmse: 1.22107 | val_1_rmse: 1.17837 |  0:00:10s
epoch 49 | loss: 0.7747  | val_0_rmse: 1.23663 | val_1_rmse: 1.20395 |  0:00:10s
epoch 50 | loss: 0.75419 | val_0_rmse: 1.21919 | val_1_rmse: 1.18994 |  0:00:10s
epoch 51 | loss: 0.74105 | val_0_rmse: 1.2522  | val_1_rmse: 1.22125 |  0:00:10s
epoch 52 | loss: 0.73508 | val_0_rmse: 1.20731 | val_1_rmse: 1.16991 |  0:00:11s
epoch 53 | loss: 0.73746 | val_0_rmse: 1.1248  | val_1_rmse: 1.08114 |  0:00:11s
epoch 54 | loss: 0.74926 | val_0_rmse: 1.15146 | val_1_rmse: 1.10473 |  0:00:11s
epoch 55 | loss: 0.74032 | val_0_rmse: 1.2007  | val_1_rmse: 1.14815 |  0:00:11s
epoch 56 | loss: 0.72072 | val_0_rmse: 1.1117  | val_1_rmse: 1.08712 |  0:00:11s
epoch 57 | loss: 0.72841 | val_0_rmse: 1.11966 | val_1_rmse: 1.06888 |  0:00:12s
epoch 58 | loss: 0.72886 | val_0_rmse: 1.20594 | val_1_rmse: 1.16281 |  0:00:12s
epoch 59 | loss: 0.7033  | val_0_rmse: 1.19282 | val_1_rmse: 1.14325 |  0:00:12s
epoch 60 | loss: 0.72887 | val_0_rmse: 1.12073 | val_1_rmse: 1.07645 |  0:00:12s
epoch 61 | loss: 0.69853 | val_0_rmse: 0.86487 | val_1_rmse: 0.81467 |  0:00:13s
epoch 62 | loss: 0.70689 | val_0_rmse: 1.12853 | val_1_rmse: 1.07196 |  0:00:13s
epoch 63 | loss: 0.73824 | val_0_rmse: 1.21289 | val_1_rmse: 1.18024 |  0:00:13s
epoch 64 | loss: 0.75834 | val_0_rmse: 1.17497 | val_1_rmse: 1.13912 |  0:00:13s
epoch 65 | loss: 0.74912 | val_0_rmse: 1.09392 | val_1_rmse: 1.04697 |  0:00:13s
epoch 66 | loss: 0.74686 | val_0_rmse: 1.104   | val_1_rmse: 1.0508  |  0:00:14s
epoch 67 | loss: 0.72787 | val_0_rmse: 1.08156 | val_1_rmse: 1.03253 |  0:00:14s
epoch 68 | loss: 0.72111 | val_0_rmse: 1.13331 | val_1_rmse: 1.08732 |  0:00:14s
epoch 69 | loss: 0.71572 | val_0_rmse: 1.23882 | val_1_rmse: 1.19892 |  0:00:14s
epoch 70 | loss: 0.72742 | val_0_rmse: 1.20522 | val_1_rmse: 1.16582 |  0:00:14s
epoch 71 | loss: 0.72303 | val_0_rmse: 1.09886 | val_1_rmse: 1.05497 |  0:00:15s
epoch 72 | loss: 0.71528 | val_0_rmse: 1.0796  | val_1_rmse: 1.03859 |  0:00:15s
epoch 73 | loss: 0.72831 | val_0_rmse: 1.10072 | val_1_rmse: 1.05786 |  0:00:15s
epoch 74 | loss: 0.71858 | val_0_rmse: 1.12291 | val_1_rmse: 1.09921 |  0:00:15s
epoch 75 | loss: 0.71731 | val_0_rmse: 1.08245 | val_1_rmse: 1.06986 |  0:00:15s
epoch 76 | loss: 0.73371 | val_0_rmse: 1.0813  | val_1_rmse: 1.05381 |  0:00:16s
epoch 77 | loss: 0.7202  | val_0_rmse: 1.05374 | val_1_rmse: 1.02429 |  0:00:16s
epoch 78 | loss: 0.71986 | val_0_rmse: 0.85606 | val_1_rmse: 0.81743 |  0:00:16s
epoch 79 | loss: 0.72592 | val_0_rmse: 0.89114 | val_1_rmse: 0.84709 |  0:00:16s
epoch 80 | loss: 0.71235 | val_0_rmse: 0.90737 | val_1_rmse: 0.85696 |  0:00:16s
epoch 81 | loss: 0.74005 | val_0_rmse: 1.10381 | val_1_rmse: 1.07388 |  0:00:17s
epoch 82 | loss: 0.72828 | val_0_rmse: 1.12601 | val_1_rmse: 1.07701 |  0:00:17s
epoch 83 | loss: 0.69536 | val_0_rmse: 1.12599 | val_1_rmse: 1.07791 |  0:00:17s
epoch 84 | loss: 0.72021 | val_0_rmse: 1.10933 | val_1_rmse: 1.06544 |  0:00:17s
epoch 85 | loss: 0.70902 | val_0_rmse: 1.12694 | val_1_rmse: 1.08027 |  0:00:17s
epoch 86 | loss: 0.70403 | val_0_rmse: 1.14826 | val_1_rmse: 1.10233 |  0:00:18s
epoch 87 | loss: 0.704   | val_0_rmse: 1.13125 | val_1_rmse: 1.08308 |  0:00:18s
epoch 88 | loss: 0.70256 | val_0_rmse: 1.09682 | val_1_rmse: 1.04541 |  0:00:18s
epoch 89 | loss: 0.71094 | val_0_rmse: 1.09871 | val_1_rmse: 1.04888 |  0:00:18s
epoch 90 | loss: 0.68677 | val_0_rmse: 1.12666 | val_1_rmse: 1.07747 |  0:00:19s
epoch 91 | loss: 0.70046 | val_0_rmse: 1.12555 | val_1_rmse: 1.08211 |  0:00:19s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.81467
Best weights from best epoch are automatically used!
ended training at: 06:42:32
Feature importance:
Mean squared error is of 5512061000.151802
Mean absolute error:58415.25339158443
MAPE:0.5656240367475867
R2 score:0.26975883158919833
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:42:32
epoch 0  | loss: 8.37442 | val_0_rmse: 1.01152 | val_1_rmse: 1.05094 |  0:00:00s
epoch 1  | loss: 2.30187 | val_0_rmse: 1.01624 | val_1_rmse: 1.0645  |  0:00:00s
epoch 2  | loss: 2.06959 | val_0_rmse: 1.00255 | val_1_rmse: 1.04256 |  0:00:00s
epoch 3  | loss: 1.59769 | val_0_rmse: 0.9973  | val_1_rmse: 1.03381 |  0:00:00s
epoch 4  | loss: 1.39952 | val_0_rmse: 0.99245 | val_1_rmse: 1.02693 |  0:00:01s
epoch 5  | loss: 1.3481  | val_0_rmse: 0.99122 | val_1_rmse: 1.02782 |  0:00:01s
epoch 6  | loss: 1.11834 | val_0_rmse: 0.98819 | val_1_rmse: 1.02063 |  0:00:01s
epoch 7  | loss: 1.2213  | val_0_rmse: 0.99809 | val_1_rmse: 1.01969 |  0:00:01s
epoch 8  | loss: 1.09257 | val_0_rmse: 0.99321 | val_1_rmse: 1.01231 |  0:00:01s
epoch 9  | loss: 1.02115 | val_0_rmse: 0.98982 | val_1_rmse: 1.01811 |  0:00:02s
epoch 10 | loss: 0.97784 | val_0_rmse: 0.98556 | val_1_rmse: 1.01809 |  0:00:02s
epoch 11 | loss: 0.96175 | val_0_rmse: 1.00642 | val_1_rmse: 1.02159 |  0:00:02s
epoch 12 | loss: 0.9294  | val_0_rmse: 1.01894 | val_1_rmse: 1.02882 |  0:00:02s
epoch 13 | loss: 1.01614 | val_0_rmse: 1.13168 | val_1_rmse: 1.1173  |  0:00:03s
epoch 14 | loss: 0.92961 | val_0_rmse: 1.08383 | val_1_rmse: 1.07518 |  0:00:03s
epoch 15 | loss: 0.89749 | val_0_rmse: 1.044   | val_1_rmse: 1.04634 |  0:00:03s
epoch 16 | loss: 0.87905 | val_0_rmse: 1.01519 | val_1_rmse: 1.02929 |  0:00:03s
epoch 17 | loss: 0.84877 | val_0_rmse: 0.98935 | val_1_rmse: 1.02285 |  0:00:03s
epoch 18 | loss: 0.82237 | val_0_rmse: 0.98856 | val_1_rmse: 1.02163 |  0:00:04s
epoch 19 | loss: 0.80209 | val_0_rmse: 0.9888  | val_1_rmse: 1.02551 |  0:00:04s
epoch 20 | loss: 0.88339 | val_0_rmse: 0.99609 | val_1_rmse: 1.03795 |  0:00:04s
epoch 21 | loss: 0.80629 | val_0_rmse: 1.0053  | val_1_rmse: 1.05096 |  0:00:04s
epoch 22 | loss: 0.80273 | val_0_rmse: 0.99653 | val_1_rmse: 1.03852 |  0:00:04s
epoch 23 | loss: 0.81444 | val_0_rmse: 0.98176 | val_1_rmse: 1.01835 |  0:00:05s
epoch 24 | loss: 0.85774 | val_0_rmse: 0.97937 | val_1_rmse: 1.01875 |  0:00:05s
epoch 25 | loss: 0.76774 | val_0_rmse: 0.97125 | val_1_rmse: 1.00278 |  0:00:05s
epoch 26 | loss: 0.79624 | val_0_rmse: 0.97865 | val_1_rmse: 1.0091  |  0:00:05s
epoch 27 | loss: 0.78977 | val_0_rmse: 0.98099 | val_1_rmse: 1.01236 |  0:00:05s
epoch 28 | loss: 0.76807 | val_0_rmse: 0.98099 | val_1_rmse: 1.01357 |  0:00:06s
epoch 29 | loss: 0.73639 | val_0_rmse: 0.98311 | val_1_rmse: 1.01864 |  0:00:06s
epoch 30 | loss: 0.7325  | val_0_rmse: 0.99211 | val_1_rmse: 1.03467 |  0:00:06s
epoch 31 | loss: 0.71583 | val_0_rmse: 1.02083 | val_1_rmse: 1.07328 |  0:00:06s
epoch 32 | loss: 0.74068 | val_0_rmse: 1.05104 | val_1_rmse: 1.10877 |  0:00:06s
epoch 33 | loss: 0.74724 | val_0_rmse: 1.05023 | val_1_rmse: 1.10634 |  0:00:07s
epoch 34 | loss: 0.77557 | val_0_rmse: 1.00732 | val_1_rmse: 1.05302 |  0:00:07s
epoch 35 | loss: 0.72774 | val_0_rmse: 0.99132 | val_1_rmse: 1.0325  |  0:00:07s
epoch 36 | loss: 0.72339 | val_0_rmse: 0.98431 | val_1_rmse: 1.02058 |  0:00:07s
epoch 37 | loss: 0.74278 | val_0_rmse: 0.98581 | val_1_rmse: 1.02294 |  0:00:08s
epoch 38 | loss: 0.73908 | val_0_rmse: 1.02575 | val_1_rmse: 1.083   |  0:00:08s
epoch 39 | loss: 0.72826 | val_0_rmse: 1.1102  | val_1_rmse: 1.15344 |  0:00:08s
epoch 40 | loss: 0.74156 | val_0_rmse: 1.14523 | val_1_rmse: 1.1985  |  0:00:08s
epoch 41 | loss: 0.74712 | val_0_rmse: 1.14958 | val_1_rmse: 1.20039 |  0:00:08s
epoch 42 | loss: 0.71242 | val_0_rmse: 1.11236 | val_1_rmse: 1.16008 |  0:00:09s
epoch 43 | loss: 0.72471 | val_0_rmse: 1.00867 | val_1_rmse: 1.04015 |  0:00:09s
epoch 44 | loss: 0.74593 | val_0_rmse: 0.99023 | val_1_rmse: 1.02423 |  0:00:09s
epoch 45 | loss: 0.74421 | val_0_rmse: 1.16084 | val_1_rmse: 1.20832 |  0:00:09s
epoch 46 | loss: 0.70964 | val_0_rmse: 0.97256 | val_1_rmse: 0.89828 |  0:00:09s
epoch 47 | loss: 0.72355 | val_0_rmse: 1.13324 | val_1_rmse: 1.16774 |  0:00:10s
epoch 48 | loss: 0.72674 | val_0_rmse: 1.15619 | val_1_rmse: 1.20779 |  0:00:10s
epoch 49 | loss: 0.71281 | val_0_rmse: 0.99398 | val_1_rmse: 0.92196 |  0:00:10s
epoch 50 | loss: 0.69675 | val_0_rmse: 0.88067 | val_1_rmse: 0.85582 |  0:00:10s
epoch 51 | loss: 0.68934 | val_0_rmse: 1.20093 | val_1_rmse: 1.26494 |  0:00:10s
epoch 52 | loss: 0.68444 | val_0_rmse: 1.23069 | val_1_rmse: 1.31197 |  0:00:11s
epoch 53 | loss: 0.6864  | val_0_rmse: 1.23922 | val_1_rmse: 1.33482 |  0:00:11s
epoch 54 | loss: 0.68539 | val_0_rmse: 1.24004 | val_1_rmse: 1.33328 |  0:00:11s
epoch 55 | loss: 0.70693 | val_0_rmse: 1.13507 | val_1_rmse: 1.18765 |  0:00:11s
epoch 56 | loss: 0.71724 | val_0_rmse: 0.86421 | val_1_rmse: 0.82148 |  0:00:11s
epoch 57 | loss: 0.70284 | val_0_rmse: 1.04349 | val_1_rmse: 1.03032 |  0:00:12s
epoch 58 | loss: 0.72122 | val_0_rmse: 1.11464 | val_1_rmse: 1.16387 |  0:00:12s
epoch 59 | loss: 0.69281 | val_0_rmse: 1.02958 | val_1_rmse: 1.02617 |  0:00:12s
epoch 60 | loss: 0.70588 | val_0_rmse: 1.05811 | val_1_rmse: 1.05199 |  0:00:12s
epoch 61 | loss: 0.71281 | val_0_rmse: 1.07965 | val_1_rmse: 1.07481 |  0:00:13s
epoch 62 | loss: 0.69733 | val_0_rmse: 0.83288 | val_1_rmse: 0.7932  |  0:00:13s
epoch 63 | loss: 0.70102 | val_0_rmse: 1.12255 | val_1_rmse: 1.17751 |  0:00:13s
epoch 64 | loss: 0.69912 | val_0_rmse: 1.18468 | val_1_rmse: 1.26906 |  0:00:13s
epoch 65 | loss: 0.70181 | val_0_rmse: 1.19602 | val_1_rmse: 1.27633 |  0:00:13s
epoch 66 | loss: 0.71409 | val_0_rmse: 1.19296 | val_1_rmse: 1.28002 |  0:00:14s
epoch 67 | loss: 0.71506 | val_0_rmse: 1.19483 | val_1_rmse: 1.26728 |  0:00:14s
epoch 68 | loss: 0.69914 | val_0_rmse: 1.19075 | val_1_rmse: 1.26482 |  0:00:14s
epoch 69 | loss: 0.68291 | val_0_rmse: 1.21312 | val_1_rmse: 1.30882 |  0:00:14s
epoch 70 | loss: 0.69586 | val_0_rmse: 1.22864 | val_1_rmse: 1.30201 |  0:00:14s
epoch 71 | loss: 0.71255 | val_0_rmse: 1.24251 | val_1_rmse: 1.316   |  0:00:15s
epoch 72 | loss: 0.70647 | val_0_rmse: 1.23959 | val_1_rmse: 1.33819 |  0:00:15s
epoch 73 | loss: 0.67073 | val_0_rmse: 1.20334 | val_1_rmse: 1.28305 |  0:00:15s
epoch 74 | loss: 0.70917 | val_0_rmse: 1.13222 | val_1_rmse: 1.18493 |  0:00:15s
epoch 75 | loss: 0.70217 | val_0_rmse: 1.12388 | val_1_rmse: 1.17622 |  0:00:15s
epoch 76 | loss: 0.69595 | val_0_rmse: 1.15849 | val_1_rmse: 1.22688 |  0:00:16s
epoch 77 | loss: 0.69362 | val_0_rmse: 1.23321 | val_1_rmse: 1.31749 |  0:00:16s
epoch 78 | loss: 0.6874  | val_0_rmse: 1.21214 | val_1_rmse: 1.30537 |  0:00:16s
epoch 79 | loss: 0.6839  | val_0_rmse: 1.20427 | val_1_rmse: 1.30264 |  0:00:16s
epoch 80 | loss: 0.6699  | val_0_rmse: 1.2275  | val_1_rmse: 1.3284  |  0:00:16s
epoch 81 | loss: 0.69868 | val_0_rmse: 1.21999 | val_1_rmse: 1.31141 |  0:00:17s
epoch 82 | loss: 0.69452 | val_0_rmse: 1.21142 | val_1_rmse: 1.30041 |  0:00:17s
epoch 83 | loss: 0.69937 | val_0_rmse: 1.22981 | val_1_rmse: 1.32854 |  0:00:17s
epoch 84 | loss: 0.69252 | val_0_rmse: 1.26023 | val_1_rmse: 1.3482  |  0:00:17s
epoch 85 | loss: 0.69568 | val_0_rmse: 1.29423 | val_1_rmse: 1.38469 |  0:00:18s
epoch 86 | loss: 0.68727 | val_0_rmse: 1.23348 | val_1_rmse: 1.33052 |  0:00:18s
epoch 87 | loss: 0.69756 | val_0_rmse: 1.19863 | val_1_rmse: 1.26934 |  0:00:18s
epoch 88 | loss: 0.67813 | val_0_rmse: 1.09783 | val_1_rmse: 1.13742 |  0:00:18s
epoch 89 | loss: 0.67974 | val_0_rmse: 1.05896 | val_1_rmse: 1.09464 |  0:00:18s
epoch 90 | loss: 0.69587 | val_0_rmse: 1.11242 | val_1_rmse: 1.16163 |  0:00:19s
epoch 91 | loss: 0.68654 | val_0_rmse: 1.11074 | val_1_rmse: 1.15585 |  0:00:19s
epoch 92 | loss: 0.67883 | val_0_rmse: 1.05184 | val_1_rmse: 1.08812 |  0:00:19s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.7932
Best weights from best epoch are automatically used!
ended training at: 06:42:51
Feature importance:
Mean squared error is of 4841900119.295386
Mean absolute error:53772.02784594299
MAPE:0.5220232819288112
R2 score:0.30408295760335546
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:42:51
epoch 0  | loss: 7.68654 | val_0_rmse: 0.99932 | val_1_rmse: 1.06182 |  0:00:00s
epoch 1  | loss: 2.63368 | val_0_rmse: 0.98959 | val_1_rmse: 1.03631 |  0:00:00s
epoch 2  | loss: 1.94595 | val_0_rmse: 0.98936 | val_1_rmse: 1.02609 |  0:00:00s
epoch 3  | loss: 1.7313  | val_0_rmse: 0.98931 | val_1_rmse: 1.02631 |  0:00:00s
epoch 4  | loss: 1.47373 | val_0_rmse: 0.98824 | val_1_rmse: 1.02057 |  0:00:01s
epoch 5  | loss: 1.20061 | val_0_rmse: 0.98256 | val_1_rmse: 1.01821 |  0:00:01s
epoch 6  | loss: 1.12494 | val_0_rmse: 1.01313 | val_1_rmse: 1.03194 |  0:00:01s
epoch 7  | loss: 1.22628 | val_0_rmse: 1.03105 | val_1_rmse: 1.04103 |  0:00:01s
epoch 8  | loss: 1.09103 | val_0_rmse: 0.981   | val_1_rmse: 1.00927 |  0:00:01s
epoch 9  | loss: 1.02346 | val_0_rmse: 0.97763 | val_1_rmse: 1.00632 |  0:00:02s
epoch 10 | loss: 1.03518 | val_0_rmse: 0.98889 | val_1_rmse: 1.01782 |  0:00:02s
epoch 11 | loss: 0.98022 | val_0_rmse: 0.97488 | val_1_rmse: 1.01518 |  0:00:02s
epoch 12 | loss: 0.94443 | val_0_rmse: 0.96821 | val_1_rmse: 1.01511 |  0:00:02s
epoch 13 | loss: 0.95275 | val_0_rmse: 0.97052 | val_1_rmse: 1.02163 |  0:00:03s
epoch 14 | loss: 0.90786 | val_0_rmse: 0.97357 | val_1_rmse: 1.02002 |  0:00:03s
epoch 15 | loss: 0.84699 | val_0_rmse: 0.96842 | val_1_rmse: 1.01038 |  0:00:03s
epoch 16 | loss: 0.83684 | val_0_rmse: 0.9535  | val_1_rmse: 0.98419 |  0:00:03s
epoch 17 | loss: 0.84999 | val_0_rmse: 0.98577 | val_1_rmse: 1.05185 |  0:00:03s
epoch 18 | loss: 0.82545 | val_0_rmse: 1.07811 | val_1_rmse: 1.15301 |  0:00:04s
epoch 19 | loss: 0.80007 | val_0_rmse: 1.0911  | val_1_rmse: 1.16619 |  0:00:04s
epoch 20 | loss: 0.79335 | val_0_rmse: 1.08379 | val_1_rmse: 1.15851 |  0:00:04s
epoch 21 | loss: 0.78883 | val_0_rmse: 1.1098  | val_1_rmse: 1.19207 |  0:00:04s
epoch 22 | loss: 0.78076 | val_0_rmse: 1.30176 | val_1_rmse: 1.39686 |  0:00:04s
epoch 23 | loss: 0.76319 | val_0_rmse: 1.50326 | val_1_rmse: 1.60603 |  0:00:05s
epoch 24 | loss: 0.75809 | val_0_rmse: 1.45456 | val_1_rmse: 1.55558 |  0:00:05s
epoch 25 | loss: 0.72515 | val_0_rmse: 1.39919 | val_1_rmse: 1.49871 |  0:00:05s
epoch 26 | loss: 0.73344 | val_0_rmse: 1.34285 | val_1_rmse: 1.44023 |  0:00:05s
epoch 27 | loss: 0.71888 | val_0_rmse: 1.3066  | val_1_rmse: 1.40432 |  0:00:05s
epoch 28 | loss: 0.69596 | val_0_rmse: 1.33869 | val_1_rmse: 1.4371  |  0:00:06s
epoch 29 | loss: 0.70938 | val_0_rmse: 1.29827 | val_1_rmse: 1.39473 |  0:00:06s
epoch 30 | loss: 0.68857 | val_0_rmse: 1.27177 | val_1_rmse: 1.36927 |  0:00:06s
epoch 31 | loss: 0.67407 | val_0_rmse: 1.18761 | val_1_rmse: 1.28231 |  0:00:06s
epoch 32 | loss: 0.71047 | val_0_rmse: 1.15724 | val_1_rmse: 1.2604  |  0:00:06s
epoch 33 | loss: 0.71167 | val_0_rmse: 1.17784 | val_1_rmse: 1.28461 |  0:00:07s
epoch 34 | loss: 0.7161  | val_0_rmse: 1.2133  | val_1_rmse: 1.31302 |  0:00:07s
epoch 35 | loss: 0.69729 | val_0_rmse: 1.16394 | val_1_rmse: 1.27037 |  0:00:07s
epoch 36 | loss: 0.6943  | val_0_rmse: 1.1812  | val_1_rmse: 1.29513 |  0:00:07s
epoch 37 | loss: 0.69505 | val_0_rmse: 1.33838 | val_1_rmse: 1.4452  |  0:00:07s
epoch 38 | loss: 0.697   | val_0_rmse: 1.31664 | val_1_rmse: 1.41793 |  0:00:08s
epoch 39 | loss: 0.67886 | val_0_rmse: 1.32437 | val_1_rmse: 1.42291 |  0:00:08s
epoch 40 | loss: 0.68972 | val_0_rmse: 1.34945 | val_1_rmse: 1.44822 |  0:00:08s
epoch 41 | loss: 0.68424 | val_0_rmse: 1.35192 | val_1_rmse: 1.4502  |  0:00:08s
epoch 42 | loss: 0.69028 | val_0_rmse: 1.31677 | val_1_rmse: 1.41324 |  0:00:09s
epoch 43 | loss: 0.67786 | val_0_rmse: 1.35259 | val_1_rmse: 1.45243 |  0:00:09s
epoch 44 | loss: 0.67446 | val_0_rmse: 1.38791 | val_1_rmse: 1.49105 |  0:00:09s
epoch 45 | loss: 0.67232 | val_0_rmse: 1.36908 | val_1_rmse: 1.47181 |  0:00:09s
epoch 46 | loss: 0.68238 | val_0_rmse: 1.3915  | val_1_rmse: 1.49592 |  0:00:09s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.98419
Best weights from best epoch are automatically used!
ended training at: 06:43:01
Feature importance:
Mean squared error is of 7234368669.43459
Mean absolute error:70909.6352489035
MAPE:0.7263775342280859
R2 score:0.06881288013490994
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:43:01
epoch 0  | loss: 7.5156  | val_0_rmse: 1.00673 | val_1_rmse: 0.99205 |  0:00:00s
epoch 1  | loss: 3.00614 | val_0_rmse: 1.02058 | val_1_rmse: 1.00454 |  0:00:00s
epoch 2  | loss: 2.28842 | val_0_rmse: 1.00252 | val_1_rmse: 0.99379 |  0:00:00s
epoch 3  | loss: 1.43129 | val_0_rmse: 0.99937 | val_1_rmse: 0.99871 |  0:00:00s
epoch 4  | loss: 1.32475 | val_0_rmse: 1.00166 | val_1_rmse: 1.00024 |  0:00:01s
epoch 5  | loss: 1.41028 | val_0_rmse: 1.00015 | val_1_rmse: 0.99758 |  0:00:01s
epoch 6  | loss: 1.38319 | val_0_rmse: 1.0018  | val_1_rmse: 0.99973 |  0:00:01s
epoch 7  | loss: 1.29579 | val_0_rmse: 1.00218 | val_1_rmse: 1.00183 |  0:00:01s
epoch 8  | loss: 1.21159 | val_0_rmse: 1.00271 | val_1_rmse: 1.00259 |  0:00:01s
epoch 9  | loss: 1.09124 | val_0_rmse: 1.00643 | val_1_rmse: 1.00822 |  0:00:02s
epoch 10 | loss: 1.03938 | val_0_rmse: 1.00984 | val_1_rmse: 1.01202 |  0:00:02s
epoch 11 | loss: 1.02192 | val_0_rmse: 1.00159 | val_1_rmse: 1.0017  |  0:00:02s
epoch 12 | loss: 1.0219  | val_0_rmse: 1.00034 | val_1_rmse: 1.00026 |  0:00:02s
epoch 13 | loss: 1.03653 | val_0_rmse: 1.00396 | val_1_rmse: 1.00596 |  0:00:02s
epoch 14 | loss: 1.01089 | val_0_rmse: 0.9993  | val_1_rmse: 0.99726 |  0:00:03s
epoch 15 | loss: 0.98683 | val_0_rmse: 0.99937 | val_1_rmse: 0.99473 |  0:00:03s
epoch 16 | loss: 0.99116 | val_0_rmse: 0.99914 | val_1_rmse: 0.99664 |  0:00:03s
epoch 17 | loss: 0.99202 | val_0_rmse: 0.99945 | val_1_rmse: 0.99782 |  0:00:03s
epoch 18 | loss: 0.99153 | val_0_rmse: 0.99821 | val_1_rmse: 0.9954  |  0:00:03s
epoch 19 | loss: 0.97467 | val_0_rmse: 0.99769 | val_1_rmse: 0.99529 |  0:00:04s
epoch 20 | loss: 0.97379 | val_0_rmse: 0.99699 | val_1_rmse: 0.99374 |  0:00:04s
epoch 21 | loss: 0.98408 | val_0_rmse: 0.99804 | val_1_rmse: 0.99241 |  0:00:04s
epoch 22 | loss: 0.98064 | val_0_rmse: 0.99825 | val_1_rmse: 0.99369 |  0:00:04s
epoch 23 | loss: 0.98169 | val_0_rmse: 0.99883 | val_1_rmse: 0.99588 |  0:00:05s
epoch 24 | loss: 0.98982 | val_0_rmse: 0.99924 | val_1_rmse: 0.99644 |  0:00:05s
epoch 25 | loss: 0.98021 | val_0_rmse: 0.99861 | val_1_rmse: 0.99627 |  0:00:05s
epoch 26 | loss: 0.98739 | val_0_rmse: 0.99691 | val_1_rmse: 0.9933  |  0:00:05s
epoch 27 | loss: 0.98512 | val_0_rmse: 0.99589 | val_1_rmse: 0.98993 |  0:00:05s
epoch 28 | loss: 0.98554 | val_0_rmse: 0.99553 | val_1_rmse: 0.9905  |  0:00:06s
epoch 29 | loss: 0.9796  | val_0_rmse: 0.99547 | val_1_rmse: 0.99102 |  0:00:06s
epoch 30 | loss: 0.98415 | val_0_rmse: 0.9962  | val_1_rmse: 0.99205 |  0:00:06s
epoch 31 | loss: 0.99026 | val_0_rmse: 0.99858 | val_1_rmse: 0.99626 |  0:00:06s
epoch 32 | loss: 0.98507 | val_0_rmse: 0.99714 | val_1_rmse: 0.99357 |  0:00:06s
epoch 33 | loss: 0.97866 | val_0_rmse: 0.99596 | val_1_rmse: 0.99043 |  0:00:07s
epoch 34 | loss: 0.98254 | val_0_rmse: 0.99616 | val_1_rmse: 0.9912  |  0:00:07s
epoch 35 | loss: 0.98653 | val_0_rmse: 0.99831 | val_1_rmse: 0.99587 |  0:00:07s
epoch 36 | loss: 0.98806 | val_0_rmse: 0.99904 | val_1_rmse: 0.9972  |  0:00:07s
epoch 37 | loss: 0.98163 | val_0_rmse: 0.99716 | val_1_rmse: 0.99359 |  0:00:07s
epoch 38 | loss: 0.98121 | val_0_rmse: 0.99749 | val_1_rmse: 0.99438 |  0:00:08s
epoch 39 | loss: 0.97902 | val_0_rmse: 0.99957 | val_1_rmse: 0.998   |  0:00:08s
epoch 40 | loss: 0.9772  | val_0_rmse: 1.00018 | val_1_rmse: 0.99869 |  0:00:08s
epoch 41 | loss: 0.97923 | val_0_rmse: 0.99931 | val_1_rmse: 0.99702 |  0:00:08s
epoch 42 | loss: 0.97965 | val_0_rmse: 0.9986  | val_1_rmse: 0.99554 |  0:00:08s
epoch 43 | loss: 0.97434 | val_0_rmse: 1.00013 | val_1_rmse: 0.99797 |  0:00:09s
epoch 44 | loss: 0.97089 | val_0_rmse: 1.00071 | val_1_rmse: 0.99983 |  0:00:09s
epoch 45 | loss: 0.97361 | val_0_rmse: 1.00409 | val_1_rmse: 1.00399 |  0:00:09s
epoch 46 | loss: 0.97053 | val_0_rmse: 0.99969 | val_1_rmse: 0.9979  |  0:00:09s
epoch 47 | loss: 0.94262 | val_0_rmse: 0.9953  | val_1_rmse: 0.99061 |  0:00:09s
epoch 48 | loss: 0.90927 | val_0_rmse: 0.99526 | val_1_rmse: 0.98825 |  0:00:10s
epoch 49 | loss: 0.87394 | val_0_rmse: 0.9952  | val_1_rmse: 0.98817 |  0:00:10s
epoch 50 | loss: 0.89432 | val_0_rmse: 0.9998  | val_1_rmse: 0.98682 |  0:00:10s
epoch 51 | loss: 0.85794 | val_0_rmse: 0.98968 | val_1_rmse: 0.98616 |  0:00:10s
epoch 52 | loss: 0.85825 | val_0_rmse: 1.0294  | val_1_rmse: 1.02406 |  0:00:11s
epoch 53 | loss: 0.83337 | val_0_rmse: 1.0111  | val_1_rmse: 1.00146 |  0:00:11s
epoch 54 | loss: 0.80901 | val_0_rmse: 0.98695 | val_1_rmse: 0.98224 |  0:00:11s
epoch 55 | loss: 0.76291 | val_0_rmse: 0.99834 | val_1_rmse: 0.99327 |  0:00:11s
epoch 56 | loss: 0.77168 | val_0_rmse: 0.99858 | val_1_rmse: 0.99247 |  0:00:11s
epoch 57 | loss: 0.78816 | val_0_rmse: 0.9955  | val_1_rmse: 0.98891 |  0:00:12s
epoch 58 | loss: 0.86469 | val_0_rmse: 0.99303 | val_1_rmse: 0.98396 |  0:00:12s
epoch 59 | loss: 0.81952 | val_0_rmse: 0.99392 | val_1_rmse: 0.97999 |  0:00:12s
epoch 60 | loss: 0.79079 | val_0_rmse: 0.99565 | val_1_rmse: 0.98129 |  0:00:12s
epoch 61 | loss: 0.83667 | val_0_rmse: 1.09161 | val_1_rmse: 1.07798 |  0:00:12s
epoch 62 | loss: 0.79383 | val_0_rmse: 1.15845 | val_1_rmse: 1.14306 |  0:00:13s
epoch 63 | loss: 0.77484 | val_0_rmse: 1.16168 | val_1_rmse: 1.14596 |  0:00:13s
epoch 64 | loss: 0.78714 | val_0_rmse: 1.0841  | val_1_rmse: 1.09    |  0:00:13s
epoch 65 | loss: 0.80473 | val_0_rmse: 1.05457 | val_1_rmse: 1.05671 |  0:00:13s
epoch 66 | loss: 0.81159 | val_0_rmse: 1.15553 | val_1_rmse: 1.1383  |  0:00:14s
epoch 67 | loss: 0.77555 | val_0_rmse: 1.15111 | val_1_rmse: 1.14618 |  0:00:14s
epoch 68 | loss: 0.79556 | val_0_rmse: 1.1367  | val_1_rmse: 1.11774 |  0:00:14s
epoch 69 | loss: 0.76959 | val_0_rmse: 1.15122 | val_1_rmse: 1.12898 |  0:00:14s
epoch 70 | loss: 0.76487 | val_0_rmse: 1.15033 | val_1_rmse: 1.12944 |  0:00:14s
epoch 71 | loss: 0.74476 | val_0_rmse: 1.16112 | val_1_rmse: 1.14107 |  0:00:15s
epoch 72 | loss: 0.74987 | val_0_rmse: 1.20323 | val_1_rmse: 1.18077 |  0:00:15s
epoch 73 | loss: 0.76998 | val_0_rmse: 1.19864 | val_1_rmse: 1.17513 |  0:00:15s
epoch 74 | loss: 0.70783 | val_0_rmse: 1.11369 | val_1_rmse: 1.09538 |  0:00:15s
epoch 75 | loss: 0.73119 | val_0_rmse: 1.08949 | val_1_rmse: 1.06702 |  0:00:15s
epoch 76 | loss: 0.73231 | val_0_rmse: 1.07395 | val_1_rmse: 1.05023 |  0:00:16s
epoch 77 | loss: 0.75052 | val_0_rmse: 0.9564  | val_1_rmse: 0.92541 |  0:00:16s
epoch 78 | loss: 0.72765 | val_0_rmse: 0.919   | val_1_rmse: 0.88657 |  0:00:16s
epoch 79 | loss: 0.7246  | val_0_rmse: 1.11054 | val_1_rmse: 1.09283 |  0:00:16s
epoch 80 | loss: 0.71991 | val_0_rmse: 1.16159 | val_1_rmse: 1.14255 |  0:00:16s
epoch 81 | loss: 0.70879 | val_0_rmse: 1.13075 | val_1_rmse: 1.1139  |  0:00:17s
epoch 82 | loss: 0.71796 | val_0_rmse: 1.07402 | val_1_rmse: 1.06218 |  0:00:17s
epoch 83 | loss: 0.71699 | val_0_rmse: 0.986   | val_1_rmse: 0.97302 |  0:00:17s
epoch 84 | loss: 0.71188 | val_0_rmse: 1.12828 | val_1_rmse: 1.13179 |  0:00:17s
epoch 85 | loss: 0.70895 | val_0_rmse: 1.08639 | val_1_rmse: 1.09263 |  0:00:17s
epoch 86 | loss: 0.71737 | val_0_rmse: 1.1681  | val_1_rmse: 1.17361 |  0:00:18s
epoch 87 | loss: 0.70007 | val_0_rmse: 1.23468 | val_1_rmse: 1.24925 |  0:00:18s
epoch 88 | loss: 0.69901 | val_0_rmse: 1.0315  | val_1_rmse: 1.02068 |  0:00:18s
epoch 89 | loss: 0.69256 | val_0_rmse: 0.98044 | val_1_rmse: 0.95205 |  0:00:18s
epoch 90 | loss: 0.72721 | val_0_rmse: 1.06676 | val_1_rmse: 1.04747 |  0:00:18s
epoch 91 | loss: 0.70242 | val_0_rmse: 0.98664 | val_1_rmse: 0.95819 |  0:00:19s
epoch 92 | loss: 0.70607 | val_0_rmse: 0.95045 | val_1_rmse: 0.90994 |  0:00:19s
epoch 93 | loss: 0.69141 | val_0_rmse: 0.98011 | val_1_rmse: 0.95705 |  0:00:19s
epoch 94 | loss: 0.6929  | val_0_rmse: 0.92152 | val_1_rmse: 0.88746 |  0:00:19s
epoch 95 | loss: 0.70988 | val_0_rmse: 1.09471 | val_1_rmse: 1.07776 |  0:00:19s
epoch 96 | loss: 0.68777 | val_0_rmse: 1.14236 | val_1_rmse: 1.12101 |  0:00:20s
epoch 97 | loss: 0.69227 | val_0_rmse: 1.16282 | val_1_rmse: 1.13908 |  0:00:20s
epoch 98 | loss: 0.69838 | val_0_rmse: 1.11725 | val_1_rmse: 1.09522 |  0:00:20s
epoch 99 | loss: 0.69204 | val_0_rmse: 1.1111  | val_1_rmse: 1.08987 |  0:00:20s
epoch 100| loss: 0.69165 | val_0_rmse: 1.14702 | val_1_rmse: 1.12388 |  0:00:20s
epoch 101| loss: 0.71536 | val_0_rmse: 1.13377 | val_1_rmse: 1.11229 |  0:00:21s
epoch 102| loss: 0.72679 | val_0_rmse: 1.08309 | val_1_rmse: 1.06978 |  0:00:21s
epoch 103| loss: 0.71543 | val_0_rmse: 0.99768 | val_1_rmse: 0.97659 |  0:00:21s
epoch 104| loss: 0.70344 | val_0_rmse: 1.05472 | val_1_rmse: 1.03388 |  0:00:21s
epoch 105| loss: 0.71777 | val_0_rmse: 0.85287 | val_1_rmse: 0.80943 |  0:00:21s
epoch 106| loss: 0.70852 | val_0_rmse: 0.98261 | val_1_rmse: 0.96803 |  0:00:22s
epoch 107| loss: 0.69983 | val_0_rmse: 0.98737 | val_1_rmse: 0.97441 |  0:00:22s
epoch 108| loss: 0.73132 | val_0_rmse: 1.08936 | val_1_rmse: 1.09184 |  0:00:22s
epoch 109| loss: 0.71332 | val_0_rmse: 0.99565 | val_1_rmse: 0.97922 |  0:00:22s
epoch 110| loss: 0.7271  | val_0_rmse: 0.99052 | val_1_rmse: 0.97899 |  0:00:23s
epoch 111| loss: 0.69786 | val_0_rmse: 0.98829 | val_1_rmse: 0.97623 |  0:00:23s
epoch 112| loss: 0.70032 | val_0_rmse: 0.99318 | val_1_rmse: 0.9787  |  0:00:23s
epoch 113| loss: 0.73638 | val_0_rmse: 1.01491 | val_1_rmse: 0.98747 |  0:00:23s
epoch 114| loss: 0.71656 | val_0_rmse: 1.18222 | val_1_rmse: 1.18116 |  0:00:23s
epoch 115| loss: 0.69794 | val_0_rmse: 1.15501 | val_1_rmse: 1.16104 |  0:00:24s
epoch 116| loss: 0.69167 | val_0_rmse: 1.05587 | val_1_rmse: 1.04567 |  0:00:24s
epoch 117| loss: 0.69355 | val_0_rmse: 1.08677 | val_1_rmse: 1.08218 |  0:00:24s
epoch 118| loss: 0.70071 | val_0_rmse: 1.0829  | val_1_rmse: 1.081   |  0:00:24s
epoch 119| loss: 0.6848  | val_0_rmse: 1.07422 | val_1_rmse: 1.07257 |  0:00:24s
epoch 120| loss: 0.69587 | val_0_rmse: 1.08044 | val_1_rmse: 1.08069 |  0:00:25s
epoch 121| loss: 0.68831 | val_0_rmse: 1.0254  | val_1_rmse: 1.01303 |  0:00:25s
epoch 122| loss: 0.68679 | val_0_rmse: 0.86304 | val_1_rmse: 0.81252 |  0:00:25s
epoch 123| loss: 0.69861 | val_0_rmse: 0.92017 | val_1_rmse: 0.879   |  0:00:25s
epoch 124| loss: 0.69572 | val_0_rmse: 1.06437 | val_1_rmse: 1.04563 |  0:00:25s
epoch 125| loss: 0.68902 | val_0_rmse: 1.0758  | val_1_rmse: 1.05687 |  0:00:26s
epoch 126| loss: 0.69507 | val_0_rmse: 1.03743 | val_1_rmse: 1.01468 |  0:00:26s
epoch 127| loss: 0.68869 | val_0_rmse: 1.11181 | val_1_rmse: 1.10739 |  0:00:26s
epoch 128| loss: 0.68615 | val_0_rmse: 1.12449 | val_1_rmse: 1.11466 |  0:00:26s
epoch 129| loss: 0.69203 | val_0_rmse: 1.12804 | val_1_rmse: 1.1146  |  0:00:26s
epoch 130| loss: 0.71339 | val_0_rmse: 1.05536 | val_1_rmse: 1.04383 |  0:00:27s
epoch 131| loss: 0.68643 | val_0_rmse: 0.98308 | val_1_rmse: 0.9549  |  0:00:27s
epoch 132| loss: 0.68381 | val_0_rmse: 1.06178 | val_1_rmse: 1.04922 |  0:00:27s
epoch 133| loss: 0.68197 | val_0_rmse: 1.03631 | val_1_rmse: 1.01334 |  0:00:27s
epoch 134| loss: 0.6783  | val_0_rmse: 1.0499  | val_1_rmse: 1.02624 |  0:00:27s
epoch 135| loss: 0.68886 | val_0_rmse: 1.09062 | val_1_rmse: 1.07434 |  0:00:28s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.80943
Best weights from best epoch are automatically used!
ended training at: 06:43:30
Feature importance:
Mean squared error is of 5214403238.329091
Mean absolute error:55837.73938966557
MAPE:0.5265594206220963
R2 score:0.27932165267905884
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:43:30
epoch 0  | loss: 11.43041| val_0_rmse: 0.9976  | val_1_rmse: 1.00971 |  0:00:00s
epoch 1  | loss: 4.51373 | val_0_rmse: 1.00056 | val_1_rmse: 1.01149 |  0:00:00s
epoch 2  | loss: 1.98805 | val_0_rmse: 0.99551 | val_1_rmse: 1.00787 |  0:00:00s
epoch 3  | loss: 2.03977 | val_0_rmse: 1.00266 | val_1_rmse: 1.01716 |  0:00:00s
epoch 4  | loss: 1.76641 | val_0_rmse: 1.02996 | val_1_rmse: 1.04556 |  0:00:01s
epoch 5  | loss: 1.79289 | val_0_rmse: 1.02403 | val_1_rmse: 1.03838 |  0:00:01s
epoch 6  | loss: 1.42515 | val_0_rmse: 1.01487 | val_1_rmse: 1.03124 |  0:00:01s
epoch 7  | loss: 1.20235 | val_0_rmse: 1.00828 | val_1_rmse: 1.02152 |  0:00:01s
epoch 8  | loss: 1.16017 | val_0_rmse: 1.02263 | val_1_rmse: 1.03893 |  0:00:01s
epoch 9  | loss: 1.22725 | val_0_rmse: 1.03438 | val_1_rmse: 1.04998 |  0:00:02s
epoch 10 | loss: 1.06558 | val_0_rmse: 1.04905 | val_1_rmse: 1.06373 |  0:00:02s
epoch 11 | loss: 1.03604 | val_0_rmse: 1.11099 | val_1_rmse: 1.13177 |  0:00:02s
epoch 12 | loss: 0.99589 | val_0_rmse: 1.08105 | val_1_rmse: 1.10099 |  0:00:02s
epoch 13 | loss: 0.99461 | val_0_rmse: 1.08142 | val_1_rmse: 1.10569 |  0:00:02s
epoch 14 | loss: 0.96905 | val_0_rmse: 1.02048 | val_1_rmse: 1.03322 |  0:00:03s
epoch 15 | loss: 0.97617 | val_0_rmse: 0.99149 | val_1_rmse: 0.99751 |  0:00:03s
epoch 16 | loss: 0.95611 | val_0_rmse: 0.98954 | val_1_rmse: 1.00356 |  0:00:03s
epoch 17 | loss: 0.9363  | val_0_rmse: 1.03322 | val_1_rmse: 1.03896 |  0:00:03s
epoch 18 | loss: 0.92057 | val_0_rmse: 1.05466 | val_1_rmse: 1.09047 |  0:00:03s
epoch 19 | loss: 0.86683 | val_0_rmse: 1.12353 | val_1_rmse: 1.17258 |  0:00:04s
epoch 20 | loss: 0.81248 | val_0_rmse: 1.15762 | val_1_rmse: 1.20342 |  0:00:04s
epoch 21 | loss: 0.79345 | val_0_rmse: 1.13471 | val_1_rmse: 1.18146 |  0:00:04s
epoch 22 | loss: 0.74586 | val_0_rmse: 1.09472 | val_1_rmse: 1.1322  |  0:00:04s
epoch 23 | loss: 0.72499 | val_0_rmse: 1.22764 | val_1_rmse: 1.26006 |  0:00:04s
epoch 24 | loss: 0.866   | val_0_rmse: 1.28387 | val_1_rmse: 1.28771 |  0:00:05s
epoch 25 | loss: 0.83139 | val_0_rmse: 1.12845 | val_1_rmse: 1.1265  |  0:00:05s
epoch 26 | loss: 0.81026 | val_0_rmse: 1.11866 | val_1_rmse: 1.10627 |  0:00:05s
epoch 27 | loss: 0.7913  | val_0_rmse: 1.20123 | val_1_rmse: 1.18222 |  0:00:05s
epoch 28 | loss: 0.80886 | val_0_rmse: 1.3489  | val_1_rmse: 1.33032 |  0:00:06s
epoch 29 | loss: 0.77198 | val_0_rmse: 1.14529 | val_1_rmse: 1.12493 |  0:00:06s
epoch 30 | loss: 0.79263 | val_0_rmse: 1.05741 | val_1_rmse: 1.02229 |  0:00:06s
epoch 31 | loss: 0.73489 | val_0_rmse: 1.26428 | val_1_rmse: 1.23446 |  0:00:06s
epoch 32 | loss: 0.73426 | val_0_rmse: 1.15841 | val_1_rmse: 1.14004 |  0:00:06s
epoch 33 | loss: 0.73939 | val_0_rmse: 0.96565 | val_1_rmse: 0.97163 |  0:00:07s
epoch 34 | loss: 0.7265  | val_0_rmse: 1.00343 | val_1_rmse: 1.01659 |  0:00:07s
epoch 35 | loss: 0.70948 | val_0_rmse: 1.10237 | val_1_rmse: 1.11735 |  0:00:07s
epoch 36 | loss: 0.70826 | val_0_rmse: 0.94124 | val_1_rmse: 0.95111 |  0:00:07s
epoch 37 | loss: 0.7794  | val_0_rmse: 0.87459 | val_1_rmse: 0.88088 |  0:00:07s
epoch 38 | loss: 0.76483 | val_0_rmse: 1.10116 | val_1_rmse: 1.09546 |  0:00:08s
epoch 39 | loss: 0.74145 | val_0_rmse: 1.12331 | val_1_rmse: 1.11889 |  0:00:08s
epoch 40 | loss: 0.74358 | val_0_rmse: 0.95547 | val_1_rmse: 0.94678 |  0:00:08s
epoch 41 | loss: 0.72292 | val_0_rmse: 0.93157 | val_1_rmse: 0.926   |  0:00:08s
epoch 42 | loss: 0.7171  | val_0_rmse: 1.01469 | val_1_rmse: 1.008   |  0:00:08s
epoch 43 | loss: 0.72431 | val_0_rmse: 0.92839 | val_1_rmse: 0.92742 |  0:00:09s
epoch 44 | loss: 0.71185 | val_0_rmse: 1.21884 | val_1_rmse: 1.2088  |  0:00:09s
epoch 45 | loss: 0.72233 | val_0_rmse: 1.25428 | val_1_rmse: 1.2419  |  0:00:09s
epoch 46 | loss: 0.73272 | val_0_rmse: 1.28996 | val_1_rmse: 1.27425 |  0:00:09s
epoch 47 | loss: 0.70368 | val_0_rmse: 1.32302 | val_1_rmse: 1.30401 |  0:00:10s
epoch 48 | loss: 0.70969 | val_0_rmse: 1.28845 | val_1_rmse: 1.26826 |  0:00:10s
epoch 49 | loss: 0.76344 | val_0_rmse: 1.11712 | val_1_rmse: 1.10025 |  0:00:10s
epoch 50 | loss: 0.71392 | val_0_rmse: 0.94838 | val_1_rmse: 0.95973 |  0:00:10s
epoch 51 | loss: 0.71542 | val_0_rmse: 1.14175 | val_1_rmse: 1.17177 |  0:00:10s
epoch 52 | loss: 0.70854 | val_0_rmse: 0.83885 | val_1_rmse: 0.8502  |  0:00:11s
epoch 53 | loss: 0.71182 | val_0_rmse: 0.91975 | val_1_rmse: 0.91086 |  0:00:11s
epoch 54 | loss: 0.69945 | val_0_rmse: 0.95827 | val_1_rmse: 0.95238 |  0:00:11s
epoch 55 | loss: 0.71751 | val_0_rmse: 0.84135 | val_1_rmse: 0.85695 |  0:00:11s
epoch 56 | loss: 0.69422 | val_0_rmse: 1.15338 | val_1_rmse: 1.13933 |  0:00:11s
epoch 57 | loss: 0.68639 | val_0_rmse: 1.18907 | val_1_rmse: 1.17666 |  0:00:12s
epoch 58 | loss: 0.74275 | val_0_rmse: 1.25498 | val_1_rmse: 1.24195 |  0:00:12s
epoch 59 | loss: 0.70425 | val_0_rmse: 1.25665 | val_1_rmse: 1.24212 |  0:00:12s
epoch 60 | loss: 0.6931  | val_0_rmse: 1.16913 | val_1_rmse: 1.14732 |  0:00:12s
epoch 61 | loss: 0.7029  | val_0_rmse: 1.07209 | val_1_rmse: 1.07502 |  0:00:12s
epoch 62 | loss: 0.69245 | val_0_rmse: 0.95515 | val_1_rmse: 0.95076 |  0:00:13s
epoch 63 | loss: 0.70271 | val_0_rmse: 1.09187 | val_1_rmse: 1.08538 |  0:00:13s
epoch 64 | loss: 0.72309 | val_0_rmse: 1.01539 | val_1_rmse: 1.00424 |  0:00:13s
epoch 65 | loss: 0.70353 | val_0_rmse: 1.03407 | val_1_rmse: 1.04884 |  0:00:13s
epoch 66 | loss: 0.72433 | val_0_rmse: 1.11645 | val_1_rmse: 1.11741 |  0:00:14s
epoch 67 | loss: 0.74045 | val_0_rmse: 1.12517 | val_1_rmse: 1.13038 |  0:00:14s
epoch 68 | loss: 0.69441 | val_0_rmse: 1.12453 | val_1_rmse: 1.14487 |  0:00:14s
epoch 69 | loss: 0.725   | val_0_rmse: 1.02857 | val_1_rmse: 1.04627 |  0:00:14s
epoch 70 | loss: 0.70548 | val_0_rmse: 1.13968 | val_1_rmse: 1.15612 |  0:00:14s
epoch 71 | loss: 0.71229 | val_0_rmse: 1.13544 | val_1_rmse: 1.14277 |  0:00:15s
epoch 72 | loss: 0.73635 | val_0_rmse: 1.10678 | val_1_rmse: 1.12467 |  0:00:15s
epoch 73 | loss: 0.71864 | val_0_rmse: 0.88679 | val_1_rmse: 0.90725 |  0:00:15s
epoch 74 | loss: 0.69784 | val_0_rmse: 1.15329 | val_1_rmse: 1.15702 |  0:00:15s
epoch 75 | loss: 0.71428 | val_0_rmse: 0.96952 | val_1_rmse: 0.96995 |  0:00:15s
epoch 76 | loss: 0.72964 | val_0_rmse: 1.03476 | val_1_rmse: 1.05587 |  0:00:16s
epoch 77 | loss: 0.77795 | val_0_rmse: 1.11835 | val_1_rmse: 1.14416 |  0:00:16s
epoch 78 | loss: 0.72874 | val_0_rmse: 1.3764  | val_1_rmse: 1.41051 |  0:00:16s
epoch 79 | loss: 0.71578 | val_0_rmse: 1.58492 | val_1_rmse: 1.61916 |  0:00:16s
epoch 80 | loss: 0.70773 | val_0_rmse: 1.59778 | val_1_rmse: 1.64322 |  0:00:16s
epoch 81 | loss: 0.71967 | val_0_rmse: 1.38372 | val_1_rmse: 1.42828 |  0:00:17s
epoch 82 | loss: 0.69485 | val_0_rmse: 1.32624 | val_1_rmse: 1.36231 |  0:00:17s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.8502
Best weights from best epoch are automatically used!
ended training at: 06:43:47
Feature importance:
Mean squared error is of 5191948236.712927
Mean absolute error:56394.82427864583
MAPE:0.5256344439275599
R2 score:0.2778944867465799
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:43:47
epoch 0  | loss: 8.81174 | val_0_rmse: 0.9996  | val_1_rmse: 1.04681 |  0:00:00s
epoch 1  | loss: 2.57827 | val_0_rmse: 1.00338 | val_1_rmse: 1.04859 |  0:00:00s
epoch 2  | loss: 2.47156 | val_0_rmse: 0.9873  | val_1_rmse: 1.03594 |  0:00:00s
epoch 3  | loss: 1.50818 | val_0_rmse: 0.99012 | val_1_rmse: 1.03703 |  0:00:00s
epoch 4  | loss: 1.27839 | val_0_rmse: 0.98746 | val_1_rmse: 1.03852 |  0:00:01s
epoch 5  | loss: 1.48137 | val_0_rmse: 1.02435 | val_1_rmse: 1.07252 |  0:00:01s
epoch 6  | loss: 1.2485  | val_0_rmse: 1.06921 | val_1_rmse: 1.10022 |  0:00:01s
epoch 7  | loss: 1.1272  | val_0_rmse: 1.07101 | val_1_rmse: 1.09975 |  0:00:01s
epoch 8  | loss: 1.12251 | val_0_rmse: 1.0257  | val_1_rmse: 1.06801 |  0:00:01s
epoch 9  | loss: 1.04505 | val_0_rmse: 1.05219 | val_1_rmse: 1.07986 |  0:00:02s
epoch 10 | loss: 1.02615 | val_0_rmse: 1.01199 | val_1_rmse: 1.04543 |  0:00:02s
epoch 11 | loss: 0.99248 | val_0_rmse: 1.0258  | val_1_rmse: 1.04877 |  0:00:02s
epoch 12 | loss: 0.98393 | val_0_rmse: 1.14794 | val_1_rmse: 1.09702 |  0:00:02s
epoch 13 | loss: 0.99758 | val_0_rmse: 1.12721 | val_1_rmse: 1.08919 |  0:00:02s
epoch 14 | loss: 0.97464 | val_0_rmse: 1.09827 | val_1_rmse: 1.07858 |  0:00:03s
epoch 15 | loss: 0.94198 | val_0_rmse: 1.00807 | val_1_rmse: 1.04038 |  0:00:03s
epoch 16 | loss: 0.94174 | val_0_rmse: 0.99382 | val_1_rmse: 1.03559 |  0:00:03s
epoch 17 | loss: 0.91809 | val_0_rmse: 0.99649 | val_1_rmse: 1.04027 |  0:00:03s
epoch 18 | loss: 0.90033 | val_0_rmse: 1.00145 | val_1_rmse: 1.04696 |  0:00:03s
epoch 19 | loss: 0.84986 | val_0_rmse: 1.01181 | val_1_rmse: 1.05962 |  0:00:04s
epoch 20 | loss: 0.80711 | val_0_rmse: 1.01088 | val_1_rmse: 1.06026 |  0:00:04s
epoch 21 | loss: 0.76846 | val_0_rmse: 0.99162 | val_1_rmse: 1.04287 |  0:00:04s
epoch 22 | loss: 0.74421 | val_0_rmse: 0.98654 | val_1_rmse: 1.03737 |  0:00:04s
epoch 23 | loss: 0.75911 | val_0_rmse: 0.97361 | val_1_rmse: 1.02671 |  0:00:05s
epoch 24 | loss: 1.13179 | val_0_rmse: 0.98332 | val_1_rmse: 1.03429 |  0:00:05s
epoch 25 | loss: 0.9418  | val_0_rmse: 0.97611 | val_1_rmse: 1.03123 |  0:00:05s
epoch 26 | loss: 0.86661 | val_0_rmse: 0.98795 | val_1_rmse: 1.04582 |  0:00:05s
epoch 27 | loss: 0.87492 | val_0_rmse: 0.99459 | val_1_rmse: 1.04882 |  0:00:05s
epoch 28 | loss: 0.82898 | val_0_rmse: 1.00955 | val_1_rmse: 1.0651  |  0:00:06s
epoch 29 | loss: 0.79525 | val_0_rmse: 1.02088 | val_1_rmse: 1.07722 |  0:00:06s
epoch 30 | loss: 0.77695 | val_0_rmse: 1.03023 | val_1_rmse: 1.08831 |  0:00:06s
epoch 31 | loss: 0.8006  | val_0_rmse: 1.01906 | val_1_rmse: 1.07658 |  0:00:06s
epoch 32 | loss: 0.83192 | val_0_rmse: 1.00337 | val_1_rmse: 1.05872 |  0:00:06s
epoch 33 | loss: 0.81905 | val_0_rmse: 0.99144 | val_1_rmse: 1.04746 |  0:00:07s
epoch 34 | loss: 0.78863 | val_0_rmse: 0.97839 | val_1_rmse: 1.03358 |  0:00:07s
epoch 35 | loss: 0.76439 | val_0_rmse: 0.97706 | val_1_rmse: 1.03241 |  0:00:07s
epoch 36 | loss: 0.7482  | val_0_rmse: 0.983   | val_1_rmse: 1.03861 |  0:00:07s
epoch 37 | loss: 0.73796 | val_0_rmse: 0.98019 | val_1_rmse: 1.03603 |  0:00:08s
epoch 38 | loss: 0.7267  | val_0_rmse: 0.97453 | val_1_rmse: 1.0291  |  0:00:08s
epoch 39 | loss: 0.7662  | val_0_rmse: 0.9761  | val_1_rmse: 1.0277  |  0:00:08s
epoch 40 | loss: 0.72144 | val_0_rmse: 0.983   | val_1_rmse: 1.03258 |  0:00:08s
epoch 41 | loss: 0.70555 | val_0_rmse: 0.98973 | val_1_rmse: 1.03911 |  0:00:08s
epoch 42 | loss: 0.6986  | val_0_rmse: 0.9972  | val_1_rmse: 1.04598 |  0:00:09s
epoch 43 | loss: 0.68923 | val_0_rmse: 1.00542 | val_1_rmse: 1.05331 |  0:00:09s
epoch 44 | loss: 0.69682 | val_0_rmse: 1.01213 | val_1_rmse: 1.06053 |  0:00:09s
epoch 45 | loss: 0.70612 | val_0_rmse: 1.00099 | val_1_rmse: 1.04985 |  0:00:09s
epoch 46 | loss: 0.69213 | val_0_rmse: 0.99353 | val_1_rmse: 1.04254 |  0:00:09s
epoch 47 | loss: 0.70453 | val_0_rmse: 0.99524 | val_1_rmse: 1.04393 |  0:00:10s
epoch 48 | loss: 0.68441 | val_0_rmse: 0.9886  | val_1_rmse: 1.03903 |  0:00:10s
epoch 49 | loss: 0.70396 | val_0_rmse: 0.98696 | val_1_rmse: 1.03423 |  0:00:10s
epoch 50 | loss: 0.69244 | val_0_rmse: 0.98904 | val_1_rmse: 1.03634 |  0:00:10s
epoch 51 | loss: 0.66035 | val_0_rmse: 0.9978  | val_1_rmse: 1.04403 |  0:00:10s
epoch 52 | loss: 0.70152 | val_0_rmse: 0.9927  | val_1_rmse: 1.03863 |  0:00:11s
epoch 53 | loss: 0.69638 | val_0_rmse: 0.98427 | val_1_rmse: 1.02982 |  0:00:11s

Early stopping occured at epoch 53 with best_epoch = 23 and best_val_1_rmse = 1.02671
Best weights from best epoch are automatically used!
ended training at: 06:43:59
Feature importance:
Mean squared error is of 7346952099.023967
Mean absolute error:69971.3884651864
MAPE:0.6838421765141149
R2 score:0.010041013529444642
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:43:59
epoch 0  | loss: 10.61797| val_0_rmse: 1.00875 | val_1_rmse: 1.08241 |  0:00:00s
epoch 1  | loss: 4.34431 | val_0_rmse: 1.00193 | val_1_rmse: 1.07117 |  0:00:00s
epoch 2  | loss: 2.55184 | val_0_rmse: 0.99362 | val_1_rmse: 1.05165 |  0:00:00s
epoch 3  | loss: 1.69015 | val_0_rmse: 1.00785 | val_1_rmse: 1.06338 |  0:00:00s
epoch 4  | loss: 1.33851 | val_0_rmse: 1.00994 | val_1_rmse: 1.06123 |  0:00:01s
epoch 5  | loss: 1.40822 | val_0_rmse: 1.01475 | val_1_rmse: 1.07125 |  0:00:01s
epoch 6  | loss: 1.37212 | val_0_rmse: 1.00672 | val_1_rmse: 1.06166 |  0:00:01s
epoch 7  | loss: 1.11975 | val_0_rmse: 1.01075 | val_1_rmse: 1.06109 |  0:00:01s
epoch 8  | loss: 1.11532 | val_0_rmse: 1.00479 | val_1_rmse: 1.05692 |  0:00:01s
epoch 9  | loss: 1.09143 | val_0_rmse: 1.00615 | val_1_rmse: 1.05587 |  0:00:02s
epoch 10 | loss: 0.97848 | val_0_rmse: 0.99783 | val_1_rmse: 1.04979 |  0:00:02s
epoch 11 | loss: 1.10452 | val_0_rmse: 0.99648 | val_1_rmse: 1.04904 |  0:00:02s
epoch 12 | loss: 0.9965  | val_0_rmse: 0.99349 | val_1_rmse: 1.04719 |  0:00:02s
epoch 13 | loss: 1.00669 | val_0_rmse: 0.98941 | val_1_rmse: 1.04937 |  0:00:02s
epoch 14 | loss: 0.99632 | val_0_rmse: 0.98539 | val_1_rmse: 1.04855 |  0:00:03s
epoch 15 | loss: 1.01122 | val_0_rmse: 0.98627 | val_1_rmse: 1.05021 |  0:00:03s
epoch 16 | loss: 0.99288 | val_0_rmse: 0.99023 | val_1_rmse: 1.05293 |  0:00:03s
epoch 17 | loss: 0.96945 | val_0_rmse: 0.98792 | val_1_rmse: 1.05044 |  0:00:03s
epoch 18 | loss: 0.95832 | val_0_rmse: 0.98815 | val_1_rmse: 1.04837 |  0:00:04s
epoch 19 | loss: 0.94268 | val_0_rmse: 0.99054 | val_1_rmse: 1.05508 |  0:00:04s
epoch 20 | loss: 0.90045 | val_0_rmse: 1.03681 | val_1_rmse: 1.11301 |  0:00:04s
epoch 21 | loss: 0.90029 | val_0_rmse: 1.06508 | val_1_rmse: 1.14339 |  0:00:04s
epoch 22 | loss: 0.89259 | val_0_rmse: 1.0709  | val_1_rmse: 1.14678 |  0:00:04s
epoch 23 | loss: 0.842   | val_0_rmse: 1.12445 | val_1_rmse: 1.20434 |  0:00:05s
epoch 24 | loss: 0.86172 | val_0_rmse: 1.06005 | val_1_rmse: 1.14791 |  0:00:05s
epoch 25 | loss: 0.7996  | val_0_rmse: 1.15897 | val_1_rmse: 1.25012 |  0:00:05s
epoch 26 | loss: 0.80339 | val_0_rmse: 1.31437 | val_1_rmse: 1.40695 |  0:00:05s
epoch 27 | loss: 0.80981 | val_0_rmse: 1.22558 | val_1_rmse: 1.3132  |  0:00:05s
epoch 28 | loss: 0.83735 | val_0_rmse: 1.24867 | val_1_rmse: 1.33501 |  0:00:06s
epoch 29 | loss: 0.81552 | val_0_rmse: 1.436   | val_1_rmse: 1.53079 |  0:00:06s
epoch 30 | loss: 0.7877  | val_0_rmse: 1.40197 | val_1_rmse: 1.4956  |  0:00:06s
epoch 31 | loss: 0.78578 | val_0_rmse: 1.39001 | val_1_rmse: 1.48127 |  0:00:06s
epoch 32 | loss: 0.75364 | val_0_rmse: 1.50385 | val_1_rmse: 1.5971  |  0:00:06s
epoch 33 | loss: 0.75576 | val_0_rmse: 1.35448 | val_1_rmse: 1.44422 |  0:00:07s
epoch 34 | loss: 0.77065 | val_0_rmse: 1.32351 | val_1_rmse: 1.41122 |  0:00:07s
epoch 35 | loss: 0.72733 | val_0_rmse: 1.49636 | val_1_rmse: 1.59088 |  0:00:07s
epoch 36 | loss: 0.75507 | val_0_rmse: 1.34705 | val_1_rmse: 1.43172 |  0:00:07s
epoch 37 | loss: 0.7185  | val_0_rmse: 1.23437 | val_1_rmse: 1.3214  |  0:00:08s
epoch 38 | loss: 0.71981 | val_0_rmse: 1.2139  | val_1_rmse: 1.28885 |  0:00:08s
epoch 39 | loss: 0.70159 | val_0_rmse: 1.18983 | val_1_rmse: 1.27237 |  0:00:08s
epoch 40 | loss: 0.73904 | val_0_rmse: 1.1828  | val_1_rmse: 1.27735 |  0:00:08s
epoch 41 | loss: 0.73375 | val_0_rmse: 1.26469 | val_1_rmse: 1.34932 |  0:00:08s
epoch 42 | loss: 0.69467 | val_0_rmse: 1.15711 | val_1_rmse: 1.23397 |  0:00:09s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 1.04719
Best weights from best epoch are automatically used!
ended training at: 06:44:08
Feature importance:
Mean squared error is of 6503397746.890154
Mean absolute error:65834.43781304825
MAPE:0.6567121679523187
R2 score:0.02775474238022091
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:44:08
epoch 0  | loss: 8.24894 | val_0_rmse: 1.07683 | val_1_rmse: 1.00828 |  0:00:00s
epoch 1  | loss: 3.10581 | val_0_rmse: 1.10591 | val_1_rmse: 1.05376 |  0:00:00s
epoch 2  | loss: 1.76969 | val_0_rmse: 1.05075 | val_1_rmse: 0.9931  |  0:00:00s
epoch 3  | loss: 1.75942 | val_0_rmse: 1.05262 | val_1_rmse: 0.9939  |  0:00:00s
epoch 4  | loss: 1.47057 | val_0_rmse: 1.01882 | val_1_rmse: 0.96147 |  0:00:01s
epoch 5  | loss: 1.42774 | val_0_rmse: 0.99142 | val_1_rmse: 0.93805 |  0:00:01s
epoch 6  | loss: 1.32882 | val_0_rmse: 1.00054 | val_1_rmse: 0.9466  |  0:00:01s
epoch 7  | loss: 1.14426 | val_0_rmse: 1.00244 | val_1_rmse: 0.94816 |  0:00:01s
epoch 8  | loss: 1.08868 | val_0_rmse: 0.99528 | val_1_rmse: 0.94214 |  0:00:01s
epoch 9  | loss: 1.04333 | val_0_rmse: 0.99847 | val_1_rmse: 0.94604 |  0:00:02s
epoch 10 | loss: 1.01199 | val_0_rmse: 0.99575 | val_1_rmse: 0.94329 |  0:00:02s
epoch 11 | loss: 0.99029 | val_0_rmse: 0.99468 | val_1_rmse: 0.9413  |  0:00:02s
epoch 12 | loss: 0.94865 | val_0_rmse: 0.99567 | val_1_rmse: 0.94092 |  0:00:02s
epoch 13 | loss: 0.96124 | val_0_rmse: 1.00629 | val_1_rmse: 0.95368 |  0:00:02s
epoch 14 | loss: 0.93705 | val_0_rmse: 1.02049 | val_1_rmse: 0.97168 |  0:00:03s
epoch 15 | loss: 0.92887 | val_0_rmse: 1.07658 | val_1_rmse: 1.03923 |  0:00:03s
epoch 16 | loss: 0.87671 | val_0_rmse: 1.0681  | val_1_rmse: 1.03136 |  0:00:03s
epoch 17 | loss: 0.86626 | val_0_rmse: 0.96947 | val_1_rmse: 0.92142 |  0:00:03s
epoch 18 | loss: 0.84986 | val_0_rmse: 0.99378 | val_1_rmse: 0.93846 |  0:00:04s
epoch 19 | loss: 0.83392 | val_0_rmse: 0.98101 | val_1_rmse: 0.93121 |  0:00:04s
epoch 20 | loss: 0.81768 | val_0_rmse: 0.97874 | val_1_rmse: 0.92862 |  0:00:04s
epoch 21 | loss: 0.83536 | val_0_rmse: 0.98407 | val_1_rmse: 0.92723 |  0:00:04s
epoch 22 | loss: 0.8139  | val_0_rmse: 0.98757 | val_1_rmse: 0.93024 |  0:00:04s
epoch 23 | loss: 0.77472 | val_0_rmse: 0.99217 | val_1_rmse: 0.93466 |  0:00:05s
epoch 24 | loss: 0.74012 | val_0_rmse: 0.98894 | val_1_rmse: 0.93629 |  0:00:05s
epoch 25 | loss: 0.7434  | val_0_rmse: 0.98553 | val_1_rmse: 0.93238 |  0:00:05s
epoch 26 | loss: 0.72284 | val_0_rmse: 0.98262 | val_1_rmse: 0.92774 |  0:00:05s
epoch 27 | loss: 0.76779 | val_0_rmse: 0.98852 | val_1_rmse: 0.93807 |  0:00:05s
epoch 28 | loss: 0.73858 | val_0_rmse: 0.98802 | val_1_rmse: 0.93729 |  0:00:06s
epoch 29 | loss: 0.73301 | val_0_rmse: 0.98363 | val_1_rmse: 0.93215 |  0:00:06s
epoch 30 | loss: 0.72398 | val_0_rmse: 0.98211 | val_1_rmse: 0.93159 |  0:00:06s
epoch 31 | loss: 0.70449 | val_0_rmse: 0.98754 | val_1_rmse: 0.9393  |  0:00:06s
epoch 32 | loss: 0.7145  | val_0_rmse: 0.99178 | val_1_rmse: 0.94246 |  0:00:06s
epoch 33 | loss: 0.72813 | val_0_rmse: 0.99728 | val_1_rmse: 0.94699 |  0:00:07s
epoch 34 | loss: 0.71524 | val_0_rmse: 0.98908 | val_1_rmse: 0.93919 |  0:00:07s
epoch 35 | loss: 0.70595 | val_0_rmse: 0.98787 | val_1_rmse: 0.938   |  0:00:07s
epoch 36 | loss: 0.72568 | val_0_rmse: 0.98306 | val_1_rmse: 0.93101 |  0:00:07s
epoch 37 | loss: 0.73318 | val_0_rmse: 0.98345 | val_1_rmse: 0.93043 |  0:00:07s
epoch 38 | loss: 0.70416 | val_0_rmse: 0.98971 | val_1_rmse: 0.93772 |  0:00:08s
epoch 39 | loss: 0.72694 | val_0_rmse: 1.01419 | val_1_rmse: 0.96377 |  0:00:08s
epoch 40 | loss: 0.69542 | val_0_rmse: 1.02513 | val_1_rmse: 0.97882 |  0:00:08s
epoch 41 | loss: 0.69861 | val_0_rmse: 1.02742 | val_1_rmse: 0.97341 |  0:00:08s
epoch 42 | loss: 0.69872 | val_0_rmse: 1.01635 | val_1_rmse: 0.96157 |  0:00:09s
epoch 43 | loss: 0.71002 | val_0_rmse: 1.03167 | val_1_rmse: 0.97108 |  0:00:09s
epoch 44 | loss: 0.69218 | val_0_rmse: 1.0329  | val_1_rmse: 0.9988  |  0:00:09s
epoch 45 | loss: 0.68243 | val_0_rmse: 0.95194 | val_1_rmse: 0.90289 |  0:00:09s
epoch 46 | loss: 0.71961 | val_0_rmse: 1.05809 | val_1_rmse: 1.02493 |  0:00:09s
epoch 47 | loss: 0.71238 | val_0_rmse: 0.86244 | val_1_rmse: 0.83619 |  0:00:10s
epoch 48 | loss: 0.71047 | val_0_rmse: 1.05068 | val_1_rmse: 1.00833 |  0:00:10s
epoch 49 | loss: 0.70985 | val_0_rmse: 0.87481 | val_1_rmse: 0.84367 |  0:00:10s
epoch 50 | loss: 0.70034 | val_0_rmse: 1.13138 | val_1_rmse: 1.09726 |  0:00:10s
epoch 51 | loss: 0.72663 | val_0_rmse: 1.11686 | val_1_rmse: 1.08149 |  0:00:10s
epoch 52 | loss: 0.7154  | val_0_rmse: 0.90968 | val_1_rmse: 0.86176 |  0:00:11s
epoch 53 | loss: 0.69758 | val_0_rmse: 0.90819 | val_1_rmse: 0.86611 |  0:00:11s
epoch 54 | loss: 0.69849 | val_0_rmse: 0.90558 | val_1_rmse: 0.85323 |  0:00:11s
epoch 55 | loss: 0.70408 | val_0_rmse: 1.17948 | val_1_rmse: 1.144   |  0:00:11s
epoch 56 | loss: 0.69832 | val_0_rmse: 1.16652 | val_1_rmse: 1.13005 |  0:00:11s
epoch 57 | loss: 0.69589 | val_0_rmse: 1.12831 | val_1_rmse: 1.08953 |  0:00:12s
epoch 58 | loss: 0.70086 | val_0_rmse: 1.10116 | val_1_rmse: 1.06442 |  0:00:12s
epoch 59 | loss: 0.67992 | val_0_rmse: 1.11143 | val_1_rmse: 1.07384 |  0:00:12s
epoch 60 | loss: 0.70156 | val_0_rmse: 1.15055 | val_1_rmse: 1.11519 |  0:00:12s
epoch 61 | loss: 0.70095 | val_0_rmse: 1.14522 | val_1_rmse: 1.1084  |  0:00:12s
epoch 62 | loss: 0.68272 | val_0_rmse: 1.12712 | val_1_rmse: 1.08933 |  0:00:13s
epoch 63 | loss: 0.67694 | val_0_rmse: 1.12705 | val_1_rmse: 1.08745 |  0:00:13s
epoch 64 | loss: 0.69853 | val_0_rmse: 1.12753 | val_1_rmse: 1.0884  |  0:00:13s
epoch 65 | loss: 0.69287 | val_0_rmse: 1.13186 | val_1_rmse: 1.09381 |  0:00:13s
epoch 66 | loss: 0.67231 | val_0_rmse: 1.14482 | val_1_rmse: 1.1082  |  0:00:14s
epoch 67 | loss: 0.69146 | val_0_rmse: 1.13368 | val_1_rmse: 1.09659 |  0:00:14s
epoch 68 | loss: 0.68909 | val_0_rmse: 1.11476 | val_1_rmse: 1.07475 |  0:00:14s
epoch 69 | loss: 0.69832 | val_0_rmse: 1.12069 | val_1_rmse: 1.08174 |  0:00:14s
epoch 70 | loss: 0.68649 | val_0_rmse: 1.14131 | val_1_rmse: 1.10408 |  0:00:14s
epoch 71 | loss: 0.6823  | val_0_rmse: 1.14608 | val_1_rmse: 1.11062 |  0:00:15s
epoch 72 | loss: 0.68864 | val_0_rmse: 1.17932 | val_1_rmse: 1.14529 |  0:00:15s
epoch 73 | loss: 0.70434 | val_0_rmse: 1.19195 | val_1_rmse: 1.15864 |  0:00:15s
epoch 74 | loss: 0.68764 | val_0_rmse: 1.15273 | val_1_rmse: 1.11604 |  0:00:15s
epoch 75 | loss: 0.68164 | val_0_rmse: 1.13484 | val_1_rmse: 1.097   |  0:00:15s
epoch 76 | loss: 0.69109 | val_0_rmse: 1.00177 | val_1_rmse: 0.96346 |  0:00:16s
epoch 77 | loss: 0.68176 | val_0_rmse: 0.98025 | val_1_rmse: 0.94149 |  0:00:16s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.83619
Best weights from best epoch are automatically used!
ended training at: 06:44:24
Feature importance:
Mean squared error is of 6106566311.1382885
Mean absolute error:61051.01132168312
MAPE:0.6138537612759414
R2 score:0.2319382678886951
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:44:24
epoch 0  | loss: 1.63534 | val_0_rmse: 1.00546 | val_1_rmse: 1.0224  |  0:00:00s
epoch 1  | loss: 1.1039  | val_0_rmse: 0.99664 | val_1_rmse: 1.01481 |  0:00:00s
epoch 2  | loss: 0.96152 | val_0_rmse: 0.99479 | val_1_rmse: 1.0118  |  0:00:01s
epoch 3  | loss: 1.0026  | val_0_rmse: 0.99582 | val_1_rmse: 1.01318 |  0:00:01s
epoch 4  | loss: 0.9726  | val_0_rmse: 0.99135 | val_1_rmse: 1.00579 |  0:00:02s
epoch 5  | loss: 0.92232 | val_0_rmse: 1.00445 | val_1_rmse: 1.01777 |  0:00:02s
epoch 6  | loss: 0.86423 | val_0_rmse: 1.02564 | val_1_rmse: 1.02716 |  0:00:03s
epoch 7  | loss: 0.85915 | val_0_rmse: 0.99078 | val_1_rmse: 1.0001  |  0:00:03s
epoch 8  | loss: 0.83372 | val_0_rmse: 1.03557 | val_1_rmse: 1.03268 |  0:00:04s
epoch 9  | loss: 0.83576 | val_0_rmse: 1.02181 | val_1_rmse: 1.02113 |  0:00:04s
epoch 10 | loss: 0.81333 | val_0_rmse: 0.99944 | val_1_rmse: 1.00396 |  0:00:05s
epoch 11 | loss: 0.80465 | val_0_rmse: 0.94165 | val_1_rmse: 0.95672 |  0:00:05s
epoch 12 | loss: 0.79372 | val_0_rmse: 0.93124 | val_1_rmse: 0.94513 |  0:00:06s
epoch 13 | loss: 0.78585 | val_0_rmse: 0.9253  | val_1_rmse: 0.93697 |  0:00:06s
epoch 14 | loss: 0.76617 | val_0_rmse: 0.92385 | val_1_rmse: 0.93403 |  0:00:07s
epoch 15 | loss: 0.75926 | val_0_rmse: 0.93005 | val_1_rmse: 0.94343 |  0:00:07s
epoch 16 | loss: 0.75123 | val_0_rmse: 0.93071 | val_1_rmse: 0.94686 |  0:00:08s
epoch 17 | loss: 0.74476 | val_0_rmse: 0.92641 | val_1_rmse: 0.95126 |  0:00:08s
epoch 18 | loss: 0.73704 | val_0_rmse: 0.93246 | val_1_rmse: 0.94842 |  0:00:08s
epoch 19 | loss: 0.72023 | val_0_rmse: 0.91346 | val_1_rmse: 0.93068 |  0:00:09s
epoch 20 | loss: 0.72337 | val_0_rmse: 0.9301  | val_1_rmse: 0.94635 |  0:00:09s
epoch 21 | loss: 0.71842 | val_0_rmse: 0.93119 | val_1_rmse: 0.94911 |  0:00:10s
epoch 22 | loss: 0.71483 | val_0_rmse: 0.92986 | val_1_rmse: 0.94898 |  0:00:10s
epoch 23 | loss: 0.69838 | val_0_rmse: 0.93154 | val_1_rmse: 0.94902 |  0:00:11s
epoch 24 | loss: 0.7053  | val_0_rmse: 0.9256  | val_1_rmse: 0.94168 |  0:00:11s
epoch 25 | loss: 0.69952 | val_0_rmse: 0.91764 | val_1_rmse: 0.93716 |  0:00:12s
epoch 26 | loss: 0.6886  | val_0_rmse: 0.91869 | val_1_rmse: 0.93712 |  0:00:12s
epoch 27 | loss: 0.69502 | val_0_rmse: 0.92125 | val_1_rmse: 0.93808 |  0:00:13s
epoch 28 | loss: 0.69129 | val_0_rmse: 0.92629 | val_1_rmse: 0.94042 |  0:00:13s
epoch 29 | loss: 0.70065 | val_0_rmse: 0.91649 | val_1_rmse: 0.93249 |  0:00:14s
epoch 30 | loss: 0.68409 | val_0_rmse: 0.90565 | val_1_rmse: 0.92491 |  0:00:14s
epoch 31 | loss: 0.69615 | val_0_rmse: 0.94111 | val_1_rmse: 0.95125 |  0:00:14s
epoch 32 | loss: 0.68155 | val_0_rmse: 0.9087  | val_1_rmse: 0.92648 |  0:00:15s
epoch 33 | loss: 0.68947 | val_0_rmse: 0.8937  | val_1_rmse: 0.91555 |  0:00:15s
epoch 34 | loss: 0.68145 | val_0_rmse: 0.89758 | val_1_rmse: 0.92268 |  0:00:16s
epoch 35 | loss: 0.68343 | val_0_rmse: 0.88963 | val_1_rmse: 0.91651 |  0:00:16s
epoch 36 | loss: 0.68738 | val_0_rmse: 0.90871 | val_1_rmse: 0.92503 |  0:00:17s
epoch 37 | loss: 0.67759 | val_0_rmse: 0.89975 | val_1_rmse: 0.91546 |  0:00:17s
epoch 38 | loss: 0.67789 | val_0_rmse: 0.91428 | val_1_rmse: 0.92951 |  0:00:18s
epoch 39 | loss: 0.67536 | val_0_rmse: 0.93321 | val_1_rmse: 0.95031 |  0:00:18s
epoch 40 | loss: 0.6827  | val_0_rmse: 0.91544 | val_1_rmse: 0.93504 |  0:00:19s
epoch 41 | loss: 0.67046 | val_0_rmse: 0.90009 | val_1_rmse: 0.91862 |  0:00:19s
epoch 42 | loss: 0.6757  | val_0_rmse: 0.92928 | val_1_rmse: 0.94132 |  0:00:20s
epoch 43 | loss: 0.67109 | val_0_rmse: 0.90533 | val_1_rmse: 0.92264 |  0:00:20s
epoch 44 | loss: 0.66477 | val_0_rmse: 0.88997 | val_1_rmse: 0.91476 |  0:00:21s
epoch 45 | loss: 0.67215 | val_0_rmse: 0.90002 | val_1_rmse: 0.9199  |  0:00:21s
epoch 46 | loss: 0.6715  | val_0_rmse: 0.91248 | val_1_rmse: 0.93057 |  0:00:22s
epoch 47 | loss: 0.66461 | val_0_rmse: 0.91376 | val_1_rmse: 0.93158 |  0:00:22s
epoch 48 | loss: 0.67336 | val_0_rmse: 0.90492 | val_1_rmse: 0.92757 |  0:00:22s
epoch 49 | loss: 0.66874 | val_0_rmse: 0.92765 | val_1_rmse: 0.94867 |  0:00:23s
epoch 50 | loss: 0.67431 | val_0_rmse: 0.91855 | val_1_rmse: 0.93776 |  0:00:23s
epoch 51 | loss: 0.67259 | val_0_rmse: 0.92243 | val_1_rmse: 0.93992 |  0:00:24s
epoch 52 | loss: 0.66113 | val_0_rmse: 0.87073 | val_1_rmse: 0.89664 |  0:00:24s
epoch 53 | loss: 0.67475 | val_0_rmse: 0.90336 | val_1_rmse: 0.92087 |  0:00:25s
epoch 54 | loss: 0.67046 | val_0_rmse: 0.90949 | val_1_rmse: 0.92674 |  0:00:25s
epoch 55 | loss: 0.67524 | val_0_rmse: 0.8626  | val_1_rmse: 0.89114 |  0:00:26s
epoch 56 | loss: 0.67153 | val_0_rmse: 0.8652  | val_1_rmse: 0.89295 |  0:00:26s
epoch 57 | loss: 0.67272 | val_0_rmse: 0.88211 | val_1_rmse: 0.90109 |  0:00:27s
epoch 58 | loss: 0.65684 | val_0_rmse: 0.8959  | val_1_rmse: 0.9168  |  0:00:27s
epoch 59 | loss: 0.65758 | val_0_rmse: 0.84807 | val_1_rmse: 0.88135 |  0:00:28s
epoch 60 | loss: 0.65634 | val_0_rmse: 0.85031 | val_1_rmse: 0.88201 |  0:00:28s
epoch 61 | loss: 0.67114 | val_0_rmse: 0.90141 | val_1_rmse: 0.92186 |  0:00:29s
epoch 62 | loss: 0.66215 | val_0_rmse: 0.89913 | val_1_rmse: 0.92108 |  0:00:29s
epoch 63 | loss: 0.66374 | val_0_rmse: 0.88407 | val_1_rmse: 0.91282 |  0:00:30s
epoch 64 | loss: 0.66825 | val_0_rmse: 0.87319 | val_1_rmse: 0.90139 |  0:00:30s
epoch 65 | loss: 0.65778 | val_0_rmse: 0.87286 | val_1_rmse: 0.89998 |  0:00:30s
epoch 66 | loss: 0.6696  | val_0_rmse: 0.87346 | val_1_rmse: 0.90059 |  0:00:31s
epoch 67 | loss: 0.65529 | val_0_rmse: 0.86032 | val_1_rmse: 0.88898 |  0:00:31s
epoch 68 | loss: 0.66036 | val_0_rmse: 0.87317 | val_1_rmse: 0.90157 |  0:00:32s
epoch 69 | loss: 0.65036 | val_0_rmse: 0.85831 | val_1_rmse: 0.88998 |  0:00:32s
epoch 70 | loss: 0.66033 | val_0_rmse: 0.8488  | val_1_rmse: 0.88337 |  0:00:33s
epoch 71 | loss: 0.66082 | val_0_rmse: 0.8624  | val_1_rmse: 0.89619 |  0:00:33s
epoch 72 | loss: 0.65775 | val_0_rmse: 0.86428 | val_1_rmse: 0.90194 |  0:00:34s
epoch 73 | loss: 0.65904 | val_0_rmse: 0.84481 | val_1_rmse: 0.89169 |  0:00:34s
epoch 74 | loss: 0.66765 | val_0_rmse: 0.88676 | val_1_rmse: 0.92013 |  0:00:35s
epoch 75 | loss: 0.65867 | val_0_rmse: 0.88323 | val_1_rmse: 0.91445 |  0:00:35s
epoch 76 | loss: 0.6563  | val_0_rmse: 0.84858 | val_1_rmse: 0.88627 |  0:00:36s
epoch 77 | loss: 0.65817 | val_0_rmse: 0.86726 | val_1_rmse: 0.90016 |  0:00:36s
epoch 78 | loss: 0.66425 | val_0_rmse: 0.86577 | val_1_rmse: 0.89705 |  0:00:37s
epoch 79 | loss: 0.65174 | val_0_rmse: 0.84994 | val_1_rmse: 0.88139 |  0:00:37s
epoch 80 | loss: 0.65564 | val_0_rmse: 0.85992 | val_1_rmse: 0.89148 |  0:00:37s
epoch 81 | loss: 0.64869 | val_0_rmse: 0.86217 | val_1_rmse: 0.88855 |  0:00:38s
epoch 82 | loss: 0.65162 | val_0_rmse: 0.86693 | val_1_rmse: 0.8898  |  0:00:38s
epoch 83 | loss: 0.6563  | val_0_rmse: 0.85607 | val_1_rmse: 0.88227 |  0:00:39s
epoch 84 | loss: 0.65988 | val_0_rmse: 0.84951 | val_1_rmse: 0.87734 |  0:00:39s
epoch 85 | loss: 0.65076 | val_0_rmse: 0.84399 | val_1_rmse: 0.87006 |  0:00:40s
epoch 86 | loss: 0.65352 | val_0_rmse: 0.84253 | val_1_rmse: 0.87044 |  0:00:40s
epoch 87 | loss: 0.6479  | val_0_rmse: 0.86008 | val_1_rmse: 0.88403 |  0:00:41s
epoch 88 | loss: 0.64594 | val_0_rmse: 0.84864 | val_1_rmse: 0.87879 |  0:00:41s
epoch 89 | loss: 0.6469  | val_0_rmse: 0.86206 | val_1_rmse: 0.89141 |  0:00:42s
epoch 90 | loss: 0.64555 | val_0_rmse: 0.86126 | val_1_rmse: 0.89085 |  0:00:42s
epoch 91 | loss: 0.64732 | val_0_rmse: 0.86591 | val_1_rmse: 0.8993  |  0:00:43s
epoch 92 | loss: 0.65623 | val_0_rmse: 0.84522 | val_1_rmse: 0.87685 |  0:00:43s
epoch 93 | loss: 0.64288 | val_0_rmse: 0.8488  | val_1_rmse: 0.88357 |  0:00:43s
epoch 94 | loss: 0.64241 | val_0_rmse: 0.83126 | val_1_rmse: 0.86606 |  0:00:44s
epoch 95 | loss: 0.65305 | val_0_rmse: 0.83197 | val_1_rmse: 0.8653  |  0:00:44s
epoch 96 | loss: 0.65417 | val_0_rmse: 0.82721 | val_1_rmse: 0.86356 |  0:00:45s
epoch 97 | loss: 0.65572 | val_0_rmse: 0.84447 | val_1_rmse: 0.88046 |  0:00:45s
epoch 98 | loss: 0.66817 | val_0_rmse: 0.82513 | val_1_rmse: 0.86022 |  0:00:46s
epoch 99 | loss: 0.66894 | val_0_rmse: 0.82595 | val_1_rmse: 0.86021 |  0:00:46s
epoch 100| loss: 0.6695  | val_0_rmse: 0.82597 | val_1_rmse: 0.85616 |  0:00:47s
epoch 101| loss: 0.66632 | val_0_rmse: 0.84405 | val_1_rmse: 0.87066 |  0:00:47s
epoch 102| loss: 0.6593  | val_0_rmse: 0.83699 | val_1_rmse: 0.86152 |  0:00:48s
epoch 103| loss: 0.66135 | val_0_rmse: 0.82316 | val_1_rmse: 0.85061 |  0:00:48s
epoch 104| loss: 0.65635 | val_0_rmse: 0.85519 | val_1_rmse: 0.87788 |  0:00:49s
epoch 105| loss: 0.65858 | val_0_rmse: 0.85146 | val_1_rmse: 0.87024 |  0:00:49s
epoch 106| loss: 0.64985 | val_0_rmse: 0.83596 | val_1_rmse: 0.85735 |  0:00:50s
epoch 107| loss: 0.6508  | val_0_rmse: 0.84963 | val_1_rmse: 0.87144 |  0:00:50s
epoch 108| loss: 0.64774 | val_0_rmse: 0.83541 | val_1_rmse: 0.86319 |  0:00:51s
epoch 109| loss: 0.65178 | val_0_rmse: 0.86073 | val_1_rmse: 0.883   |  0:00:51s
epoch 110| loss: 0.65281 | val_0_rmse: 0.83328 | val_1_rmse: 0.864   |  0:00:51s
epoch 111| loss: 0.64242 | val_0_rmse: 0.84972 | val_1_rmse: 0.87638 |  0:00:52s
epoch 112| loss: 0.64998 | val_0_rmse: 0.8638  | val_1_rmse: 0.89382 |  0:00:52s
epoch 113| loss: 0.64847 | val_0_rmse: 0.83367 | val_1_rmse: 0.86483 |  0:00:53s
epoch 114| loss: 0.6497  | val_0_rmse: 0.8235  | val_1_rmse: 0.85198 |  0:00:53s
epoch 115| loss: 0.65074 | val_0_rmse: 0.82397 | val_1_rmse: 0.85401 |  0:00:54s
epoch 116| loss: 0.65283 | val_0_rmse: 0.81896 | val_1_rmse: 0.85124 |  0:00:54s
epoch 117| loss: 0.6515  | val_0_rmse: 0.81332 | val_1_rmse: 0.8488  |  0:00:55s
epoch 118| loss: 0.64315 | val_0_rmse: 0.81917 | val_1_rmse: 0.85501 |  0:00:55s
epoch 119| loss: 0.64465 | val_0_rmse: 0.81804 | val_1_rmse: 0.84875 |  0:00:56s
epoch 120| loss: 0.65091 | val_0_rmse: 0.83525 | val_1_rmse: 0.86501 |  0:00:56s
epoch 121| loss: 0.64991 | val_0_rmse: 0.82498 | val_1_rmse: 0.86249 |  0:00:57s
epoch 122| loss: 0.64233 | val_0_rmse: 0.82438 | val_1_rmse: 0.85998 |  0:00:57s
epoch 123| loss: 0.65343 | val_0_rmse: 0.81544 | val_1_rmse: 0.8515  |  0:00:57s
epoch 124| loss: 0.65015 | val_0_rmse: 0.81716 | val_1_rmse: 0.85169 |  0:00:58s
epoch 125| loss: 0.64766 | val_0_rmse: 0.82619 | val_1_rmse: 0.85877 |  0:00:58s
epoch 126| loss: 0.63913 | val_0_rmse: 0.82302 | val_1_rmse: 0.85734 |  0:00:59s
epoch 127| loss: 0.64494 | val_0_rmse: 0.81154 | val_1_rmse: 0.85118 |  0:00:59s
epoch 128| loss: 0.63942 | val_0_rmse: 0.80721 | val_1_rmse: 0.8419  |  0:01:00s
epoch 129| loss: 0.63971 | val_0_rmse: 0.81325 | val_1_rmse: 0.8478  |  0:01:00s
epoch 130| loss: 0.63206 | val_0_rmse: 0.8063  | val_1_rmse: 0.84198 |  0:01:01s
epoch 131| loss: 0.63837 | val_0_rmse: 0.80813 | val_1_rmse: 0.84218 |  0:01:01s
epoch 132| loss: 0.64157 | val_0_rmse: 0.80642 | val_1_rmse: 0.84433 |  0:01:02s
epoch 133| loss: 0.6465  | val_0_rmse: 0.804   | val_1_rmse: 0.84298 |  0:01:02s
epoch 134| loss: 0.64334 | val_0_rmse: 0.8211  | val_1_rmse: 0.85583 |  0:01:03s
epoch 135| loss: 0.64423 | val_0_rmse: 0.8144  | val_1_rmse: 0.85929 |  0:01:03s
epoch 136| loss: 0.63596 | val_0_rmse: 0.83485 | val_1_rmse: 0.86897 |  0:01:04s
epoch 137| loss: 0.64225 | val_0_rmse: 0.79941 | val_1_rmse: 0.83415 |  0:01:04s
epoch 138| loss: 0.64575 | val_0_rmse: 0.80714 | val_1_rmse: 0.8375  |  0:01:05s
epoch 139| loss: 0.63786 | val_0_rmse: 0.836   | val_1_rmse: 0.86393 |  0:01:05s
epoch 140| loss: 0.63064 | val_0_rmse: 0.79994 | val_1_rmse: 0.83538 |  0:01:05s
epoch 141| loss: 0.64168 | val_0_rmse: 0.81033 | val_1_rmse: 0.84178 |  0:01:06s
epoch 142| loss: 0.6366  | val_0_rmse: 0.80981 | val_1_rmse: 0.83998 |  0:01:06s
epoch 143| loss: 0.63514 | val_0_rmse: 0.81509 | val_1_rmse: 0.84171 |  0:01:07s
epoch 144| loss: 0.63753 | val_0_rmse: 0.82388 | val_1_rmse: 0.84872 |  0:01:07s
epoch 145| loss: 0.64806 | val_0_rmse: 0.8007  | val_1_rmse: 0.83472 |  0:01:08s
epoch 146| loss: 0.64833 | val_0_rmse: 0.80163 | val_1_rmse: 0.83725 |  0:01:08s
epoch 147| loss: 0.6348  | val_0_rmse: 0.80073 | val_1_rmse: 0.83628 |  0:01:09s
epoch 148| loss: 0.63695 | val_0_rmse: 0.81976 | val_1_rmse: 0.85101 |  0:01:09s
epoch 149| loss: 0.63532 | val_0_rmse: 0.81831 | val_1_rmse: 0.84186 |  0:01:10s
Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_val_1_rmse = 0.83415
Best weights from best epoch are automatically used!
ended training at: 06:45:35
Feature importance:
Mean squared error is of 5503010090.883919
Mean absolute error:54664.866481128076
MAPE:0.4449082294399705
R2 score:0.33648139998052196
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:45:35
epoch 0  | loss: 1.95763 | val_0_rmse: 0.99572 | val_1_rmse: 0.99937 |  0:00:00s
epoch 1  | loss: 1.29213 | val_0_rmse: 1.00583 | val_1_rmse: 1.01028 |  0:00:00s
epoch 2  | loss: 1.05565 | val_0_rmse: 0.98861 | val_1_rmse: 0.98834 |  0:00:01s
epoch 3  | loss: 0.98676 | val_0_rmse: 0.98098 | val_1_rmse: 0.97938 |  0:00:01s
epoch 4  | loss: 0.92366 | val_0_rmse: 0.97471 | val_1_rmse: 0.97093 |  0:00:02s
epoch 5  | loss: 0.86751 | val_0_rmse: 0.96493 | val_1_rmse: 0.95956 |  0:00:02s
epoch 6  | loss: 0.80763 | val_0_rmse: 0.97927 | val_1_rmse: 0.97401 |  0:00:03s
epoch 7  | loss: 0.78037 | val_0_rmse: 0.96286 | val_1_rmse: 0.94421 |  0:00:03s
epoch 8  | loss: 0.76117 | val_0_rmse: 0.98454 | val_1_rmse: 0.95751 |  0:00:04s
epoch 9  | loss: 0.76168 | val_0_rmse: 0.94958 | val_1_rmse: 0.92726 |  0:00:04s
epoch 10 | loss: 0.74903 | val_0_rmse: 0.94566 | val_1_rmse: 0.92234 |  0:00:05s
epoch 11 | loss: 0.75019 | val_0_rmse: 0.9536  | val_1_rmse: 0.93076 |  0:00:05s
epoch 12 | loss: 0.74575 | val_0_rmse: 0.95518 | val_1_rmse: 0.93247 |  0:00:06s
epoch 13 | loss: 0.74084 | val_0_rmse: 0.95321 | val_1_rmse: 0.93238 |  0:00:06s
epoch 14 | loss: 0.72536 | val_0_rmse: 0.93859 | val_1_rmse: 0.91551 |  0:00:07s
epoch 15 | loss: 0.72611 | val_0_rmse: 0.94367 | val_1_rmse: 0.92213 |  0:00:07s
epoch 16 | loss: 0.72301 | val_0_rmse: 0.93541 | val_1_rmse: 0.91311 |  0:00:07s
epoch 17 | loss: 0.71747 | val_0_rmse: 0.92522 | val_1_rmse: 0.9057  |  0:00:08s
epoch 18 | loss: 0.71243 | val_0_rmse: 0.92829 | val_1_rmse: 0.90342 |  0:00:08s
epoch 19 | loss: 0.72018 | val_0_rmse: 0.91711 | val_1_rmse: 0.89453 |  0:00:09s
epoch 20 | loss: 0.71029 | val_0_rmse: 0.91685 | val_1_rmse: 0.90054 |  0:00:09s
epoch 21 | loss: 0.70931 | val_0_rmse: 0.92052 | val_1_rmse: 0.90009 |  0:00:10s
epoch 22 | loss: 0.70781 | val_0_rmse: 0.92028 | val_1_rmse: 0.89838 |  0:00:10s
epoch 23 | loss: 0.69352 | val_0_rmse: 0.91298 | val_1_rmse: 0.8956  |  0:00:11s
epoch 24 | loss: 0.69016 | val_0_rmse: 0.90773 | val_1_rmse: 0.88685 |  0:00:11s
epoch 25 | loss: 0.69892 | val_0_rmse: 0.91794 | val_1_rmse: 0.89635 |  0:00:12s
epoch 26 | loss: 0.69511 | val_0_rmse: 0.91812 | val_1_rmse: 0.90016 |  0:00:12s
epoch 27 | loss: 0.6823  | val_0_rmse: 0.90336 | val_1_rmse: 0.88378 |  0:00:13s
epoch 28 | loss: 0.70201 | val_0_rmse: 0.90251 | val_1_rmse: 0.8866  |  0:00:13s
epoch 29 | loss: 0.68688 | val_0_rmse: 0.89896 | val_1_rmse: 0.88221 |  0:00:14s
epoch 30 | loss: 0.69241 | val_0_rmse: 0.88789 | val_1_rmse: 0.8719  |  0:00:14s
epoch 31 | loss: 0.68345 | val_0_rmse: 0.88796 | val_1_rmse: 0.87045 |  0:00:14s
epoch 32 | loss: 0.68374 | val_0_rmse: 0.88616 | val_1_rmse: 0.8684  |  0:00:15s
epoch 33 | loss: 0.68115 | val_0_rmse: 0.88298 | val_1_rmse: 0.86299 |  0:00:15s
epoch 34 | loss: 0.674   | val_0_rmse: 0.87964 | val_1_rmse: 0.86131 |  0:00:16s
epoch 35 | loss: 0.67769 | val_0_rmse: 0.88167 | val_1_rmse: 0.86419 |  0:00:16s
epoch 36 | loss: 0.67565 | val_0_rmse: 0.86782 | val_1_rmse: 0.85051 |  0:00:17s
epoch 37 | loss: 0.68273 | val_0_rmse: 0.87793 | val_1_rmse: 0.86068 |  0:00:17s
epoch 38 | loss: 0.67732 | val_0_rmse: 0.87824 | val_1_rmse: 0.86184 |  0:00:18s
epoch 39 | loss: 0.67435 | val_0_rmse: 0.86787 | val_1_rmse: 0.85181 |  0:00:18s
epoch 40 | loss: 0.67403 | val_0_rmse: 0.8769  | val_1_rmse: 0.85635 |  0:00:19s
epoch 41 | loss: 0.67618 | val_0_rmse: 0.87509 | val_1_rmse: 0.85324 |  0:00:19s
epoch 42 | loss: 0.66533 | val_0_rmse: 0.86608 | val_1_rmse: 0.84746 |  0:00:20s
epoch 43 | loss: 0.66823 | val_0_rmse: 0.85949 | val_1_rmse: 0.84143 |  0:00:20s
epoch 44 | loss: 0.67441 | val_0_rmse: 0.86253 | val_1_rmse: 0.8438  |  0:00:21s
epoch 45 | loss: 0.67427 | val_0_rmse: 0.87333 | val_1_rmse: 0.8528  |  0:00:21s
epoch 46 | loss: 0.66928 | val_0_rmse: 0.87633 | val_1_rmse: 0.85676 |  0:00:22s
epoch 47 | loss: 0.66362 | val_0_rmse: 0.90652 | val_1_rmse: 0.87806 |  0:00:22s
epoch 48 | loss: 0.68123 | val_0_rmse: 0.86194 | val_1_rmse: 0.8426  |  0:00:23s
epoch 49 | loss: 0.68964 | val_0_rmse: 0.8599  | val_1_rmse: 0.84021 |  0:00:23s
epoch 50 | loss: 0.67436 | val_0_rmse: 0.87208 | val_1_rmse: 0.84123 |  0:00:23s
epoch 51 | loss: 0.66334 | val_0_rmse: 0.86682 | val_1_rmse: 0.83949 |  0:00:24s
epoch 52 | loss: 0.66466 | val_0_rmse: 0.87137 | val_1_rmse: 0.84751 |  0:00:24s
epoch 53 | loss: 0.66977 | val_0_rmse: 0.85823 | val_1_rmse: 0.84179 |  0:00:25s
epoch 54 | loss: 0.66879 | val_0_rmse: 0.8552  | val_1_rmse: 0.83604 |  0:00:25s
epoch 55 | loss: 0.65994 | val_0_rmse: 0.85472 | val_1_rmse: 0.83166 |  0:00:26s
epoch 56 | loss: 0.6521  | val_0_rmse: 0.86647 | val_1_rmse: 0.84396 |  0:00:26s
epoch 57 | loss: 0.66665 | val_0_rmse: 0.86305 | val_1_rmse: 0.84294 |  0:00:27s
epoch 58 | loss: 0.66154 | val_0_rmse: 0.85953 | val_1_rmse: 0.84001 |  0:00:27s
epoch 59 | loss: 0.67238 | val_0_rmse: 0.8424  | val_1_rmse: 0.82493 |  0:00:28s
epoch 60 | loss: 0.67339 | val_0_rmse: 0.85762 | val_1_rmse: 0.84143 |  0:00:28s
epoch 61 | loss: 0.67276 | val_0_rmse: 0.86511 | val_1_rmse: 0.84123 |  0:00:29s
epoch 62 | loss: 0.67029 | val_0_rmse: 0.8641  | val_1_rmse: 0.83736 |  0:00:29s
epoch 63 | loss: 0.65724 | val_0_rmse: 0.8605  | val_1_rmse: 0.83625 |  0:00:30s
epoch 64 | loss: 0.65498 | val_0_rmse: 0.85975 | val_1_rmse: 0.82984 |  0:00:30s
epoch 65 | loss: 0.65227 | val_0_rmse: 0.84856 | val_1_rmse: 0.8184  |  0:00:30s
epoch 66 | loss: 0.65773 | val_0_rmse: 0.84302 | val_1_rmse: 0.8208  |  0:00:31s
epoch 67 | loss: 0.65266 | val_0_rmse: 0.85706 | val_1_rmse: 0.83213 |  0:00:31s
epoch 68 | loss: 0.66208 | val_0_rmse: 0.8492  | val_1_rmse: 0.82951 |  0:00:32s
epoch 69 | loss: 0.65237 | val_0_rmse: 0.82931 | val_1_rmse: 0.8108  |  0:00:32s
epoch 70 | loss: 0.64819 | val_0_rmse: 0.82805 | val_1_rmse: 0.80871 |  0:00:33s
epoch 71 | loss: 0.6518  | val_0_rmse: 0.8383  | val_1_rmse: 0.81426 |  0:00:33s
epoch 72 | loss: 0.64927 | val_0_rmse: 0.85334 | val_1_rmse: 0.84304 |  0:00:34s
epoch 73 | loss: 0.65939 | val_0_rmse: 0.84376 | val_1_rmse: 0.84206 |  0:00:34s
epoch 74 | loss: 0.65246 | val_0_rmse: 0.8251  | val_1_rmse: 0.80544 |  0:00:35s
epoch 75 | loss: 0.65021 | val_0_rmse: 0.82179 | val_1_rmse: 0.7998  |  0:00:35s
epoch 76 | loss: 0.65686 | val_0_rmse: 0.81716 | val_1_rmse: 0.80595 |  0:00:36s
epoch 77 | loss: 0.64277 | val_0_rmse: 0.81842 | val_1_rmse: 0.79582 |  0:00:36s
epoch 78 | loss: 0.6428  | val_0_rmse: 0.82304 | val_1_rmse: 0.80641 |  0:00:37s
epoch 79 | loss: 0.64263 | val_0_rmse: 0.81892 | val_1_rmse: 0.8042  |  0:00:37s
epoch 80 | loss: 0.64204 | val_0_rmse: 0.81999 | val_1_rmse: 0.80738 |  0:00:38s
epoch 81 | loss: 0.63715 | val_0_rmse: 0.83873 | val_1_rmse: 0.81479 |  0:00:38s
epoch 82 | loss: 0.65152 | val_0_rmse: 0.81517 | val_1_rmse: 0.79933 |  0:00:38s
epoch 83 | loss: 0.65347 | val_0_rmse: 0.84917 | val_1_rmse: 0.84871 |  0:00:39s
epoch 84 | loss: 0.6497  | val_0_rmse: 0.82784 | val_1_rmse: 0.80857 |  0:00:39s
epoch 85 | loss: 0.65628 | val_0_rmse: 0.84527 | val_1_rmse: 0.82698 |  0:00:40s
epoch 86 | loss: 0.64727 | val_0_rmse: 0.82316 | val_1_rmse: 0.80877 |  0:00:40s
epoch 87 | loss: 0.6501  | val_0_rmse: 0.82513 | val_1_rmse: 0.80276 |  0:00:41s
epoch 88 | loss: 0.6426  | val_0_rmse: 0.84004 | val_1_rmse: 0.81937 |  0:00:41s
epoch 89 | loss: 0.6482  | val_0_rmse: 0.81316 | val_1_rmse: 0.79591 |  0:00:42s
epoch 90 | loss: 0.64926 | val_0_rmse: 0.82205 | val_1_rmse: 0.80438 |  0:00:42s
epoch 91 | loss: 0.64642 | val_0_rmse: 0.81523 | val_1_rmse: 0.79867 |  0:00:43s
epoch 92 | loss: 0.63993 | val_0_rmse: 0.81653 | val_1_rmse: 0.80268 |  0:00:43s
epoch 93 | loss: 0.64575 | val_0_rmse: 0.82567 | val_1_rmse: 0.81006 |  0:00:44s
epoch 94 | loss: 0.6415  | val_0_rmse: 0.82619 | val_1_rmse: 0.81221 |  0:00:44s
epoch 95 | loss: 0.63459 | val_0_rmse: 0.82439 | val_1_rmse: 0.80224 |  0:00:44s
epoch 96 | loss: 0.64695 | val_0_rmse: 0.80833 | val_1_rmse: 0.78728 |  0:00:45s
epoch 97 | loss: 0.64431 | val_0_rmse: 0.82213 | val_1_rmse: 0.79924 |  0:00:45s
epoch 98 | loss: 0.64213 | val_0_rmse: 0.82148 | val_1_rmse: 0.81673 |  0:00:46s
epoch 99 | loss: 0.64306 | val_0_rmse: 0.82006 | val_1_rmse: 0.80833 |  0:00:46s
epoch 100| loss: 0.65247 | val_0_rmse: 0.822   | val_1_rmse: 0.80528 |  0:00:47s
epoch 101| loss: 0.64299 | val_0_rmse: 0.81386 | val_1_rmse: 0.79807 |  0:00:47s
epoch 102| loss: 0.64272 | val_0_rmse: 0.81275 | val_1_rmse: 0.79621 |  0:00:48s
epoch 103| loss: 0.64714 | val_0_rmse: 0.81233 | val_1_rmse: 0.79401 |  0:00:48s
epoch 104| loss: 0.6403  | val_0_rmse: 0.83172 | val_1_rmse: 0.8106  |  0:00:49s
epoch 105| loss: 0.64388 | val_0_rmse: 0.82755 | val_1_rmse: 0.80632 |  0:00:49s
epoch 106| loss: 0.64862 | val_0_rmse: 0.83472 | val_1_rmse: 0.80257 |  0:00:50s
epoch 107| loss: 0.64001 | val_0_rmse: 0.82056 | val_1_rmse: 0.79451 |  0:00:50s
epoch 108| loss: 0.63926 | val_0_rmse: 0.84616 | val_1_rmse: 0.81346 |  0:00:51s
epoch 109| loss: 0.64821 | val_0_rmse: 0.84401 | val_1_rmse: 0.82588 |  0:00:51s
epoch 110| loss: 0.65388 | val_0_rmse: 0.83472 | val_1_rmse: 0.80467 |  0:00:51s
epoch 111| loss: 0.65402 | val_0_rmse: 0.83231 | val_1_rmse: 0.81781 |  0:00:52s
epoch 112| loss: 0.64831 | val_0_rmse: 0.81806 | val_1_rmse: 0.79873 |  0:00:52s
epoch 113| loss: 0.6509  | val_0_rmse: 0.82079 | val_1_rmse: 0.80154 |  0:00:53s
epoch 114| loss: 0.65068 | val_0_rmse: 0.8188  | val_1_rmse: 0.80325 |  0:00:53s
epoch 115| loss: 0.65005 | val_0_rmse: 0.80687 | val_1_rmse: 0.79205 |  0:00:54s
epoch 116| loss: 0.64649 | val_0_rmse: 0.81612 | val_1_rmse: 0.8038  |  0:00:54s
epoch 117| loss: 0.65199 | val_0_rmse: 0.84178 | val_1_rmse: 0.82892 |  0:00:55s
epoch 118| loss: 0.65214 | val_0_rmse: 0.82639 | val_1_rmse: 0.80946 |  0:00:55s
epoch 119| loss: 0.65078 | val_0_rmse: 0.8198  | val_1_rmse: 0.81887 |  0:00:56s
epoch 120| loss: 0.6478  | val_0_rmse: 0.84259 | val_1_rmse: 0.84382 |  0:00:56s
epoch 121| loss: 0.64914 | val_0_rmse: 0.83619 | val_1_rmse: 0.83721 |  0:00:56s
epoch 122| loss: 0.64569 | val_0_rmse: 0.80838 | val_1_rmse: 0.80265 |  0:00:57s
epoch 123| loss: 0.6444  | val_0_rmse: 0.82238 | val_1_rmse: 0.8174  |  0:00:57s
epoch 124| loss: 0.6424  | val_0_rmse: 0.82046 | val_1_rmse: 0.82571 |  0:00:58s
epoch 125| loss: 0.63733 | val_0_rmse: 0.80692 | val_1_rmse: 0.80349 |  0:00:58s
epoch 126| loss: 0.64606 | val_0_rmse: 0.82114 | val_1_rmse: 0.82366 |  0:00:59s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.78728
Best weights from best epoch are automatically used!
ended training at: 06:46:35
Feature importance:
Mean squared error is of 5934461197.959369
Mean absolute error:56566.681094783715
MAPE:0.4899787059240627
R2 score:0.3130286956569561
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:46:35
epoch 0  | loss: 1.71384 | val_0_rmse: 1.01355 | val_1_rmse: 1.01081 |  0:00:00s
epoch 1  | loss: 1.15472 | val_0_rmse: 1.00946 | val_1_rmse: 1.00801 |  0:00:00s
epoch 2  | loss: 1.06145 | val_0_rmse: 1.01119 | val_1_rmse: 1.00788 |  0:00:01s
epoch 3  | loss: 1.00785 | val_0_rmse: 1.00682 | val_1_rmse: 1.00419 |  0:00:01s
epoch 4  | loss: 0.96274 | val_0_rmse: 1.00773 | val_1_rmse: 1.00582 |  0:00:02s
epoch 5  | loss: 0.95339 | val_0_rmse: 1.00368 | val_1_rmse: 1.00056 |  0:00:02s
epoch 6  | loss: 0.92875 | val_0_rmse: 0.99675 | val_1_rmse: 0.99693 |  0:00:03s
epoch 7  | loss: 0.87736 | val_0_rmse: 0.98579 | val_1_rmse: 0.9853  |  0:00:03s
epoch 8  | loss: 0.816   | val_0_rmse: 0.99338 | val_1_rmse: 0.9872  |  0:00:04s
epoch 9  | loss: 0.80602 | val_0_rmse: 0.97929 | val_1_rmse: 0.97842 |  0:00:04s
epoch 10 | loss: 0.78039 | val_0_rmse: 0.97969 | val_1_rmse: 0.98162 |  0:00:05s
epoch 11 | loss: 0.77408 | val_0_rmse: 0.96901 | val_1_rmse: 0.97678 |  0:00:05s
epoch 12 | loss: 0.76544 | val_0_rmse: 0.97569 | val_1_rmse: 0.98518 |  0:00:06s
epoch 13 | loss: 0.75574 | val_0_rmse: 0.94619 | val_1_rmse: 0.95737 |  0:00:06s
epoch 14 | loss: 0.76573 | val_0_rmse: 0.94528 | val_1_rmse: 0.95394 |  0:00:07s
epoch 15 | loss: 0.75674 | val_0_rmse: 0.9353  | val_1_rmse: 0.94446 |  0:00:07s
epoch 16 | loss: 0.76729 | val_0_rmse: 0.9357  | val_1_rmse: 0.94511 |  0:00:07s
epoch 17 | loss: 0.75767 | val_0_rmse: 0.92152 | val_1_rmse: 0.93578 |  0:00:08s
epoch 18 | loss: 0.748   | val_0_rmse: 0.92283 | val_1_rmse: 0.93413 |  0:00:08s
epoch 19 | loss: 0.7439  | val_0_rmse: 0.92028 | val_1_rmse: 0.93288 |  0:00:09s
epoch 20 | loss: 0.75303 | val_0_rmse: 0.91555 | val_1_rmse: 0.92358 |  0:00:09s
epoch 21 | loss: 0.75236 | val_0_rmse: 0.91724 | val_1_rmse: 0.93253 |  0:00:10s
epoch 22 | loss: 0.74294 | val_0_rmse: 0.908   | val_1_rmse: 0.9168  |  0:00:10s
epoch 23 | loss: 0.74974 | val_0_rmse: 0.91432 | val_1_rmse: 0.92665 |  0:00:11s
epoch 24 | loss: 0.7594  | val_0_rmse: 0.91565 | val_1_rmse: 0.92259 |  0:00:11s
epoch 25 | loss: 0.75123 | val_0_rmse: 0.90346 | val_1_rmse: 0.90991 |  0:00:12s
epoch 26 | loss: 0.73861 | val_0_rmse: 0.90743 | val_1_rmse: 0.91492 |  0:00:12s
epoch 27 | loss: 0.7295  | val_0_rmse: 0.91408 | val_1_rmse: 0.92027 |  0:00:13s
epoch 28 | loss: 0.73362 | val_0_rmse: 0.93451 | val_1_rmse: 0.93954 |  0:00:13s
epoch 29 | loss: 0.73379 | val_0_rmse: 0.93509 | val_1_rmse: 0.93722 |  0:00:14s
epoch 30 | loss: 0.73866 | val_0_rmse: 0.92523 | val_1_rmse: 0.92243 |  0:00:14s
epoch 31 | loss: 0.72833 | val_0_rmse: 0.92695 | val_1_rmse: 0.93218 |  0:00:15s
epoch 32 | loss: 0.72761 | val_0_rmse: 0.92141 | val_1_rmse: 0.92434 |  0:00:15s
epoch 33 | loss: 0.72798 | val_0_rmse: 0.93206 | val_1_rmse: 0.93445 |  0:00:15s
epoch 34 | loss: 0.72017 | val_0_rmse: 0.8977  | val_1_rmse: 0.89993 |  0:00:16s
epoch 35 | loss: 0.72202 | val_0_rmse: 0.9012  | val_1_rmse: 0.89074 |  0:00:16s
epoch 36 | loss: 0.73287 | val_0_rmse: 0.90099 | val_1_rmse: 0.89002 |  0:00:17s
epoch 37 | loss: 0.72921 | val_0_rmse: 0.91585 | val_1_rmse: 0.91033 |  0:00:17s
epoch 38 | loss: 0.71933 | val_0_rmse: 0.91584 | val_1_rmse: 0.90403 |  0:00:18s
epoch 39 | loss: 0.76085 | val_0_rmse: 0.91205 | val_1_rmse: 0.91917 |  0:00:18s
epoch 40 | loss: 0.74781 | val_0_rmse: 0.90631 | val_1_rmse: 0.91544 |  0:00:19s
epoch 41 | loss: 0.74824 | val_0_rmse: 0.89845 | val_1_rmse: 0.90589 |  0:00:19s
epoch 42 | loss: 0.73176 | val_0_rmse: 0.90934 | val_1_rmse: 0.90984 |  0:00:20s
epoch 43 | loss: 0.73085 | val_0_rmse: 0.89509 | val_1_rmse: 0.90622 |  0:00:20s
epoch 44 | loss: 0.72495 | val_0_rmse: 0.87478 | val_1_rmse: 0.88542 |  0:00:21s
epoch 45 | loss: 0.72027 | val_0_rmse: 0.91599 | val_1_rmse: 0.91709 |  0:00:21s
epoch 46 | loss: 0.79376 | val_0_rmse: 0.93056 | val_1_rmse: 0.94412 |  0:00:22s
epoch 47 | loss: 0.81121 | val_0_rmse: 0.93791 | val_1_rmse: 0.95071 |  0:00:22s
epoch 48 | loss: 0.79379 | val_0_rmse: 0.90436 | val_1_rmse: 0.91825 |  0:00:22s
epoch 49 | loss: 0.76754 | val_0_rmse: 0.92852 | val_1_rmse: 0.94436 |  0:00:23s
epoch 50 | loss: 0.75129 | val_0_rmse: 0.85716 | val_1_rmse: 0.87314 |  0:00:23s
epoch 51 | loss: 0.75223 | val_0_rmse: 0.93588 | val_1_rmse: 0.94748 |  0:00:24s
epoch 52 | loss: 0.73039 | val_0_rmse: 1.03003 | val_1_rmse: 1.03837 |  0:00:24s
epoch 53 | loss: 0.73521 | val_0_rmse: 0.93177 | val_1_rmse: 0.9413  |  0:00:25s
epoch 54 | loss: 0.70922 | val_0_rmse: 0.88775 | val_1_rmse: 0.89954 |  0:00:25s
epoch 55 | loss: 0.72223 | val_0_rmse: 0.94327 | val_1_rmse: 0.96025 |  0:00:26s
epoch 56 | loss: 0.70676 | val_0_rmse: 0.86999 | val_1_rmse: 0.8872  |  0:00:26s
epoch 57 | loss: 0.70039 | val_0_rmse: 0.90385 | val_1_rmse: 0.9173  |  0:00:27s
epoch 58 | loss: 0.69637 | val_0_rmse: 0.87331 | val_1_rmse: 0.88549 |  0:00:27s
epoch 59 | loss: 0.68681 | val_0_rmse: 0.87225 | val_1_rmse: 0.88832 |  0:00:28s
epoch 60 | loss: 0.69324 | val_0_rmse: 0.84667 | val_1_rmse: 0.86055 |  0:00:28s
epoch 61 | loss: 0.69251 | val_0_rmse: 0.8664  | val_1_rmse: 0.87633 |  0:00:29s
epoch 62 | loss: 0.68351 | val_0_rmse: 0.84526 | val_1_rmse: 0.85989 |  0:00:29s
epoch 63 | loss: 0.68485 | val_0_rmse: 0.84097 | val_1_rmse: 0.85944 |  0:00:30s
epoch 64 | loss: 0.69408 | val_0_rmse: 0.84336 | val_1_rmse: 0.85885 |  0:00:30s
epoch 65 | loss: 0.686   | val_0_rmse: 0.85478 | val_1_rmse: 0.86648 |  0:00:30s
epoch 66 | loss: 0.68653 | val_0_rmse: 0.84308 | val_1_rmse: 0.85524 |  0:00:31s
epoch 67 | loss: 0.6799  | val_0_rmse: 0.84749 | val_1_rmse: 0.86462 |  0:00:31s
epoch 68 | loss: 0.68245 | val_0_rmse: 0.84954 | val_1_rmse: 0.8629  |  0:00:32s
epoch 69 | loss: 0.68137 | val_0_rmse: 0.84404 | val_1_rmse: 0.84989 |  0:00:32s
epoch 70 | loss: 0.70226 | val_0_rmse: 0.84948 | val_1_rmse: 0.85738 |  0:00:33s
epoch 71 | loss: 0.68117 | val_0_rmse: 0.83906 | val_1_rmse: 0.849   |  0:00:33s
epoch 72 | loss: 0.6825  | val_0_rmse: 0.86196 | val_1_rmse: 0.87328 |  0:00:34s
epoch 73 | loss: 0.67822 | val_0_rmse: 0.89307 | val_1_rmse: 0.90192 |  0:00:34s
epoch 74 | loss: 0.69236 | val_0_rmse: 0.86318 | val_1_rmse: 0.87469 |  0:00:35s
epoch 75 | loss: 0.68993 | val_0_rmse: 0.87021 | val_1_rmse: 0.8805  |  0:00:35s
epoch 76 | loss: 0.67838 | val_0_rmse: 0.85293 | val_1_rmse: 0.86207 |  0:00:36s
epoch 77 | loss: 0.67963 | val_0_rmse: 0.83452 | val_1_rmse: 0.84505 |  0:00:36s
epoch 78 | loss: 0.68195 | val_0_rmse: 0.83309 | val_1_rmse: 0.84559 |  0:00:37s
epoch 79 | loss: 0.67555 | val_0_rmse: 0.84057 | val_1_rmse: 0.85509 |  0:00:37s
epoch 80 | loss: 0.68728 | val_0_rmse: 0.84545 | val_1_rmse: 0.85645 |  0:00:38s
epoch 81 | loss: 0.68232 | val_0_rmse: 0.86805 | val_1_rmse: 0.87204 |  0:00:38s
epoch 82 | loss: 0.68335 | val_0_rmse: 0.88236 | val_1_rmse: 0.89205 |  0:00:38s
epoch 83 | loss: 0.66563 | val_0_rmse: 0.83431 | val_1_rmse: 0.85152 |  0:00:39s
epoch 84 | loss: 0.68089 | val_0_rmse: 0.84878 | val_1_rmse: 0.85868 |  0:00:39s
epoch 85 | loss: 0.68688 | val_0_rmse: 0.83585 | val_1_rmse: 0.84981 |  0:00:40s
epoch 86 | loss: 0.6726  | val_0_rmse: 0.84171 | val_1_rmse: 0.8611  |  0:00:40s
epoch 87 | loss: 0.67533 | val_0_rmse: 0.83373 | val_1_rmse: 0.84577 |  0:00:41s
epoch 88 | loss: 0.67069 | val_0_rmse: 0.82085 | val_1_rmse: 0.83212 |  0:00:41s
epoch 89 | loss: 0.67429 | val_0_rmse: 0.82659 | val_1_rmse: 0.84258 |  0:00:42s
epoch 90 | loss: 0.68403 | val_0_rmse: 0.82367 | val_1_rmse: 0.83729 |  0:00:42s
epoch 91 | loss: 0.68389 | val_0_rmse: 0.83541 | val_1_rmse: 0.84085 |  0:00:43s
epoch 92 | loss: 0.67115 | val_0_rmse: 0.82934 | val_1_rmse: 0.84014 |  0:00:43s
epoch 93 | loss: 0.67585 | val_0_rmse: 0.88384 | val_1_rmse: 0.90097 |  0:00:44s
epoch 94 | loss: 0.66914 | val_0_rmse: 0.85677 | val_1_rmse: 0.87758 |  0:00:44s
epoch 95 | loss: 0.6791  | val_0_rmse: 0.83818 | val_1_rmse: 0.85703 |  0:00:44s
epoch 96 | loss: 0.67411 | val_0_rmse: 0.84317 | val_1_rmse: 0.86111 |  0:00:45s
epoch 97 | loss: 0.66165 | val_0_rmse: 0.82352 | val_1_rmse: 0.84278 |  0:00:45s
epoch 98 | loss: 0.66534 | val_0_rmse: 0.82539 | val_1_rmse: 0.84189 |  0:00:46s
epoch 99 | loss: 0.66078 | val_0_rmse: 0.82637 | val_1_rmse: 0.84485 |  0:00:46s
epoch 100| loss: 0.66577 | val_0_rmse: 0.82453 | val_1_rmse: 0.85685 |  0:00:47s
epoch 101| loss: 0.67809 | val_0_rmse: 0.82587 | val_1_rmse: 0.85308 |  0:00:47s
epoch 102| loss: 0.68019 | val_0_rmse: 0.82782 | val_1_rmse: 0.8427  |  0:00:48s
epoch 103| loss: 0.67058 | val_0_rmse: 0.88438 | val_1_rmse: 0.8963  |  0:00:48s
epoch 104| loss: 0.67104 | val_0_rmse: 0.91939 | val_1_rmse: 0.93266 |  0:00:49s
epoch 105| loss: 0.66719 | val_0_rmse: 0.89458 | val_1_rmse: 0.90571 |  0:00:49s
epoch 106| loss: 0.68011 | val_0_rmse: 0.82745 | val_1_rmse: 0.83784 |  0:00:50s
epoch 107| loss: 0.66325 | val_0_rmse: 0.81711 | val_1_rmse: 0.83095 |  0:00:50s
epoch 108| loss: 0.66514 | val_0_rmse: 0.83765 | val_1_rmse: 0.85722 |  0:00:50s
epoch 109| loss: 0.6887  | val_0_rmse: 0.84108 | val_1_rmse: 0.85375 |  0:00:51s
epoch 110| loss: 0.66864 | val_0_rmse: 0.86083 | val_1_rmse: 0.87687 |  0:00:51s
epoch 111| loss: 0.66382 | val_0_rmse: 0.99681 | val_1_rmse: 1.01717 |  0:00:52s
epoch 112| loss: 0.65942 | val_0_rmse: 0.87391 | val_1_rmse: 0.89596 |  0:00:52s
epoch 113| loss: 0.66934 | val_0_rmse: 0.81553 | val_1_rmse: 0.8205  |  0:00:53s
epoch 114| loss: 0.67616 | val_0_rmse: 0.83728 | val_1_rmse: 0.84648 |  0:00:53s
epoch 115| loss: 0.66449 | val_0_rmse: 0.81594 | val_1_rmse: 0.82905 |  0:00:54s
epoch 116| loss: 0.66932 | val_0_rmse: 0.82944 | val_1_rmse: 0.82964 |  0:00:54s
epoch 117| loss: 0.66758 | val_0_rmse: 0.84423 | val_1_rmse: 0.84878 |  0:00:55s
epoch 118| loss: 0.6777  | val_0_rmse: 0.84208 | val_1_rmse: 0.84946 |  0:00:55s
epoch 119| loss: 0.66276 | val_0_rmse: 0.82492 | val_1_rmse: 0.83193 |  0:00:56s
epoch 120| loss: 0.65751 | val_0_rmse: 0.8192  | val_1_rmse: 0.83513 |  0:00:56s
epoch 121| loss: 0.65827 | val_0_rmse: 0.83249 | val_1_rmse: 0.85068 |  0:00:57s
epoch 122| loss: 0.66542 | val_0_rmse: 0.82512 | val_1_rmse: 0.8413  |  0:00:57s
epoch 123| loss: 0.6672  | val_0_rmse: 0.81956 | val_1_rmse: 0.83643 |  0:00:57s
epoch 124| loss: 0.65952 | val_0_rmse: 0.82989 | val_1_rmse: 0.85494 |  0:00:58s
epoch 125| loss: 0.6666  | val_0_rmse: 0.811   | val_1_rmse: 0.83527 |  0:00:58s
epoch 126| loss: 0.65608 | val_0_rmse: 0.81382 | val_1_rmse: 0.83383 |  0:00:59s
epoch 127| loss: 0.65089 | val_0_rmse: 0.82927 | val_1_rmse: 0.84724 |  0:00:59s
epoch 128| loss: 0.66393 | val_0_rmse: 0.82062 | val_1_rmse: 0.84211 |  0:01:00s
epoch 129| loss: 0.65549 | val_0_rmse: 0.81462 | val_1_rmse: 0.83409 |  0:01:00s
epoch 130| loss: 0.65854 | val_0_rmse: 0.80959 | val_1_rmse: 0.83573 |  0:01:01s
epoch 131| loss: 0.64856 | val_0_rmse: 0.81208 | val_1_rmse: 0.83823 |  0:01:01s
epoch 132| loss: 0.65447 | val_0_rmse: 0.81609 | val_1_rmse: 0.83748 |  0:01:02s
epoch 133| loss: 0.64857 | val_0_rmse: 0.84254 | val_1_rmse: 0.85814 |  0:01:02s
epoch 134| loss: 0.65753 | val_0_rmse: 0.86294 | val_1_rmse: 0.87834 |  0:01:03s
epoch 135| loss: 0.65888 | val_0_rmse: 0.83892 | val_1_rmse: 0.85532 |  0:01:03s
epoch 136| loss: 0.65038 | val_0_rmse: 0.83436 | val_1_rmse: 0.8584  |  0:01:04s
epoch 137| loss: 0.6629  | val_0_rmse: 0.82644 | val_1_rmse: 0.8478  |  0:01:04s
epoch 138| loss: 0.67203 | val_0_rmse: 0.93597 | val_1_rmse: 0.89422 |  0:01:04s
epoch 139| loss: 0.68651 | val_0_rmse: 0.82555 | val_1_rmse: 0.85259 |  0:01:05s
epoch 140| loss: 0.6797  | val_0_rmse: 0.81736 | val_1_rmse: 0.83837 |  0:01:05s
epoch 141| loss: 0.67326 | val_0_rmse: 0.81799 | val_1_rmse: 0.83615 |  0:01:06s
epoch 142| loss: 0.67473 | val_0_rmse: 0.86438 | val_1_rmse: 0.91842 |  0:01:06s
epoch 143| loss: 0.6675  | val_0_rmse: 0.82303 | val_1_rmse: 0.85336 |  0:01:07s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.8205
Best weights from best epoch are automatically used!
ended training at: 06:47:42
Feature importance:
Mean squared error is of 5386753775.30592
Mean absolute error:54496.09095022264
MAPE:0.4712650774837568
R2 score:0.30125839737700355
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:47:42
epoch 0  | loss: 1.80524 | val_0_rmse: 1.0095  | val_1_rmse: 0.99843 |  0:00:00s
epoch 1  | loss: 1.22162 | val_0_rmse: 1.00208 | val_1_rmse: 0.98518 |  0:00:00s
epoch 2  | loss: 1.06    | val_0_rmse: 1.00213 | val_1_rmse: 0.98516 |  0:00:01s
epoch 3  | loss: 0.97628 | val_0_rmse: 1.00491 | val_1_rmse: 0.98767 |  0:00:01s
epoch 4  | loss: 0.95907 | val_0_rmse: 1.00061 | val_1_rmse: 0.98115 |  0:00:02s
epoch 5  | loss: 0.92418 | val_0_rmse: 1.00411 | val_1_rmse: 0.98813 |  0:00:02s
epoch 6  | loss: 0.93139 | val_0_rmse: 1.011   | val_1_rmse: 0.9948  |  0:00:03s
epoch 7  | loss: 0.90274 | val_0_rmse: 1.00456 | val_1_rmse: 0.98733 |  0:00:03s
epoch 8  | loss: 0.88289 | val_0_rmse: 1.01917 | val_1_rmse: 1.00515 |  0:00:04s
epoch 9  | loss: 0.84907 | val_0_rmse: 1.03308 | val_1_rmse: 1.02062 |  0:00:04s
epoch 10 | loss: 0.81962 | val_0_rmse: 1.01533 | val_1_rmse: 1.00384 |  0:00:05s
epoch 11 | loss: 0.79402 | val_0_rmse: 1.01594 | val_1_rmse: 1.00705 |  0:00:05s
epoch 12 | loss: 0.79739 | val_0_rmse: 1.04952 | val_1_rmse: 1.04579 |  0:00:06s
epoch 13 | loss: 0.81034 | val_0_rmse: 0.99074 | val_1_rmse: 0.98525 |  0:00:06s
epoch 14 | loss: 0.80563 | val_0_rmse: 1.05517 | val_1_rmse: 1.05167 |  0:00:07s
epoch 15 | loss: 0.78508 | val_0_rmse: 1.02858 | val_1_rmse: 1.02316 |  0:00:07s
epoch 16 | loss: 0.80558 | val_0_rmse: 1.00884 | val_1_rmse: 1.00179 |  0:00:08s
epoch 17 | loss: 0.77697 | val_0_rmse: 1.03479 | val_1_rmse: 1.02708 |  0:00:08s
epoch 18 | loss: 0.764   | val_0_rmse: 1.0425  | val_1_rmse: 1.03223 |  0:00:08s
epoch 19 | loss: 0.74952 | val_0_rmse: 0.97959 | val_1_rmse: 0.97084 |  0:00:09s
epoch 20 | loss: 0.75114 | val_0_rmse: 1.08457 | val_1_rmse: 1.08312 |  0:00:09s
epoch 21 | loss: 0.74796 | val_0_rmse: 1.05027 | val_1_rmse: 1.04158 |  0:00:10s
epoch 22 | loss: 0.75173 | val_0_rmse: 1.04198 | val_1_rmse: 1.03854 |  0:00:10s
epoch 23 | loss: 0.7511  | val_0_rmse: 1.08654 | val_1_rmse: 1.08362 |  0:00:11s
epoch 24 | loss: 0.74774 | val_0_rmse: 1.05533 | val_1_rmse: 1.04746 |  0:00:11s
epoch 25 | loss: 0.74461 | val_0_rmse: 1.12624 | val_1_rmse: 1.1208  |  0:00:12s
epoch 26 | loss: 0.74724 | val_0_rmse: 1.06988 | val_1_rmse: 1.06332 |  0:00:12s
epoch 27 | loss: 0.73075 | val_0_rmse: 1.12177 | val_1_rmse: 1.10887 |  0:00:13s
epoch 28 | loss: 0.7173  | val_0_rmse: 1.12858 | val_1_rmse: 1.11165 |  0:00:13s
epoch 29 | loss: 0.71614 | val_0_rmse: 1.04542 | val_1_rmse: 1.02874 |  0:00:13s
epoch 30 | loss: 0.71492 | val_0_rmse: 1.05098 | val_1_rmse: 1.03996 |  0:00:14s
epoch 31 | loss: 0.70143 | val_0_rmse: 1.03317 | val_1_rmse: 1.02277 |  0:00:14s
epoch 32 | loss: 0.69646 | val_0_rmse: 0.96355 | val_1_rmse: 0.94559 |  0:00:15s
epoch 33 | loss: 0.69202 | val_0_rmse: 0.95892 | val_1_rmse: 0.94106 |  0:00:15s
epoch 34 | loss: 0.69504 | val_0_rmse: 0.90834 | val_1_rmse: 0.88679 |  0:00:16s
epoch 35 | loss: 0.70343 | val_0_rmse: 0.91821 | val_1_rmse: 0.89249 |  0:00:16s
epoch 36 | loss: 0.6841  | val_0_rmse: 0.95995 | val_1_rmse: 0.94004 |  0:00:17s
epoch 37 | loss: 0.68685 | val_0_rmse: 0.92726 | val_1_rmse: 0.89958 |  0:00:17s
epoch 38 | loss: 0.69358 | val_0_rmse: 0.96655 | val_1_rmse: 0.93824 |  0:00:18s
epoch 39 | loss: 0.68643 | val_0_rmse: 0.94736 | val_1_rmse: 0.91601 |  0:00:18s
epoch 40 | loss: 0.68348 | val_0_rmse: 0.96078 | val_1_rmse: 0.9207  |  0:00:19s
epoch 41 | loss: 0.69956 | val_0_rmse: 0.91922 | val_1_rmse: 0.88564 |  0:00:19s
epoch 42 | loss: 0.70414 | val_0_rmse: 0.90746 | val_1_rmse: 0.87967 |  0:00:20s
epoch 43 | loss: 0.6949  | val_0_rmse: 0.91647 | val_1_rmse: 0.86962 |  0:00:20s
epoch 44 | loss: 0.68577 | val_0_rmse: 0.92613 | val_1_rmse: 0.87871 |  0:00:21s
epoch 45 | loss: 0.68105 | val_0_rmse: 0.89705 | val_1_rmse: 0.86252 |  0:00:21s
epoch 46 | loss: 0.67972 | val_0_rmse: 0.89042 | val_1_rmse: 0.84968 |  0:00:22s
epoch 47 | loss: 0.68559 | val_0_rmse: 0.90898 | val_1_rmse: 0.86445 |  0:00:22s
epoch 48 | loss: 0.6789  | val_0_rmse: 0.90528 | val_1_rmse: 0.86116 |  0:00:22s
epoch 49 | loss: 0.67988 | val_0_rmse: 0.94224 | val_1_rmse: 0.92062 |  0:00:23s
epoch 50 | loss: 0.68267 | val_0_rmse: 0.88738 | val_1_rmse: 0.85105 |  0:00:23s
epoch 51 | loss: 0.68338 | val_0_rmse: 0.88584 | val_1_rmse: 0.861   |  0:00:24s
epoch 52 | loss: 0.69971 | val_0_rmse: 0.94131 | val_1_rmse: 0.93299 |  0:00:24s
epoch 53 | loss: 0.72278 | val_0_rmse: 0.94847 | val_1_rmse: 0.93734 |  0:00:25s
epoch 54 | loss: 0.71365 | val_0_rmse: 0.94115 | val_1_rmse: 0.92445 |  0:00:25s
epoch 55 | loss: 0.70823 | val_0_rmse: 0.92953 | val_1_rmse: 0.90972 |  0:00:26s
epoch 56 | loss: 0.71839 | val_0_rmse: 0.89706 | val_1_rmse: 0.86906 |  0:00:26s
epoch 57 | loss: 0.70656 | val_0_rmse: 0.87981 | val_1_rmse: 0.85216 |  0:00:27s
epoch 58 | loss: 0.69806 | val_0_rmse: 0.86041 | val_1_rmse: 0.83729 |  0:00:27s
epoch 59 | loss: 0.69585 | val_0_rmse: 0.86625 | val_1_rmse: 0.84162 |  0:00:28s
epoch 60 | loss: 0.69256 | val_0_rmse: 0.86299 | val_1_rmse: 0.83713 |  0:00:28s
epoch 61 | loss: 0.69666 | val_0_rmse: 0.8655  | val_1_rmse: 0.8436  |  0:00:29s
epoch 62 | loss: 0.72601 | val_0_rmse: 0.89484 | val_1_rmse: 0.86888 |  0:00:29s
epoch 63 | loss: 0.72932 | val_0_rmse: 0.8786  | val_1_rmse: 0.85191 |  0:00:29s
epoch 64 | loss: 0.71294 | val_0_rmse: 0.87288 | val_1_rmse: 0.84832 |  0:00:30s
epoch 65 | loss: 0.69525 | val_0_rmse: 0.86232 | val_1_rmse: 0.83476 |  0:00:30s
epoch 66 | loss: 0.6834  | val_0_rmse: 0.85957 | val_1_rmse: 0.82735 |  0:00:31s
epoch 67 | loss: 0.68167 | val_0_rmse: 0.85742 | val_1_rmse: 0.8208  |  0:00:31s
epoch 68 | loss: 0.68432 | val_0_rmse: 0.88579 | val_1_rmse: 0.85243 |  0:00:32s
epoch 69 | loss: 0.67098 | val_0_rmse: 0.85196 | val_1_rmse: 0.81962 |  0:00:32s
epoch 70 | loss: 0.68399 | val_0_rmse: 0.85203 | val_1_rmse: 0.82357 |  0:00:33s
epoch 71 | loss: 0.69721 | val_0_rmse: 0.89831 | val_1_rmse: 0.88006 |  0:00:33s
epoch 72 | loss: 0.68096 | val_0_rmse: 0.95112 | val_1_rmse: 0.93079 |  0:00:34s
epoch 73 | loss: 0.69547 | val_0_rmse: 0.98225 | val_1_rmse: 0.95487 |  0:00:34s
epoch 74 | loss: 0.69436 | val_0_rmse: 0.83097 | val_1_rmse: 0.81606 |  0:00:35s
epoch 75 | loss: 0.67821 | val_0_rmse: 0.83394 | val_1_rmse: 0.83216 |  0:00:35s
epoch 76 | loss: 0.6785  | val_0_rmse: 0.83082 | val_1_rmse: 0.8149  |  0:00:36s
epoch 77 | loss: 0.66333 | val_0_rmse: 0.84797 | val_1_rmse: 0.8383  |  0:00:36s
epoch 78 | loss: 0.67541 | val_0_rmse: 0.86213 | val_1_rmse: 0.83304 |  0:00:37s
epoch 79 | loss: 0.65798 | val_0_rmse: 0.8431  | val_1_rmse: 0.82304 |  0:00:37s
epoch 80 | loss: 0.65975 | val_0_rmse: 0.84208 | val_1_rmse: 0.82004 |  0:00:37s
epoch 81 | loss: 0.6654  | val_0_rmse: 0.82941 | val_1_rmse: 0.81026 |  0:00:38s
epoch 82 | loss: 0.66658 | val_0_rmse: 0.84689 | val_1_rmse: 0.82119 |  0:00:38s
epoch 83 | loss: 0.65324 | val_0_rmse: 0.86448 | val_1_rmse: 0.84814 |  0:00:39s
epoch 84 | loss: 0.66738 | val_0_rmse: 0.84487 | val_1_rmse: 0.83452 |  0:00:39s
epoch 85 | loss: 0.65929 | val_0_rmse: 0.83691 | val_1_rmse: 0.8167  |  0:00:40s
epoch 86 | loss: 0.66879 | val_0_rmse: 0.84763 | val_1_rmse: 0.82367 |  0:00:40s
epoch 87 | loss: 0.66116 | val_0_rmse: 0.86728 | val_1_rmse: 0.83919 |  0:00:41s
epoch 88 | loss: 0.65159 | val_0_rmse: 0.93481 | val_1_rmse: 0.90286 |  0:00:41s
epoch 89 | loss: 0.65444 | val_0_rmse: 0.87995 | val_1_rmse: 0.8525  |  0:00:42s
epoch 90 | loss: 0.65122 | val_0_rmse: 0.89777 | val_1_rmse: 0.87114 |  0:00:42s
epoch 91 | loss: 0.65177 | val_0_rmse: 0.87704 | val_1_rmse: 0.85746 |  0:00:43s
epoch 92 | loss: 0.6584  | val_0_rmse: 0.8476  | val_1_rmse: 0.85023 |  0:00:43s
epoch 93 | loss: 0.65628 | val_0_rmse: 0.83905 | val_1_rmse: 0.8347  |  0:00:44s
epoch 94 | loss: 0.6554  | val_0_rmse: 0.84589 | val_1_rmse: 0.84001 |  0:00:44s
epoch 95 | loss: 0.64873 | val_0_rmse: 0.83116 | val_1_rmse: 0.81644 |  0:00:45s
epoch 96 | loss: 0.64842 | val_0_rmse: 0.87289 | val_1_rmse: 0.84581 |  0:00:45s
epoch 97 | loss: 0.65712 | val_0_rmse: 0.87656 | val_1_rmse: 0.85563 |  0:00:45s
epoch 98 | loss: 0.65004 | val_0_rmse: 0.91462 | val_1_rmse: 0.89325 |  0:00:46s
epoch 99 | loss: 0.65395 | val_0_rmse: 0.87187 | val_1_rmse: 0.85125 |  0:00:46s
epoch 100| loss: 0.64676 | val_0_rmse: 0.87311 | val_1_rmse: 0.85281 |  0:00:47s
epoch 101| loss: 0.65106 | val_0_rmse: 0.86471 | val_1_rmse: 0.84929 |  0:00:47s
epoch 102| loss: 0.65274 | val_0_rmse: 0.83435 | val_1_rmse: 0.84026 |  0:00:48s
epoch 103| loss: 0.65105 | val_0_rmse: 0.82164 | val_1_rmse: 0.83649 |  0:00:48s
epoch 104| loss: 0.6606  | val_0_rmse: 0.82482 | val_1_rmse: 0.83976 |  0:00:49s
epoch 105| loss: 0.64174 | val_0_rmse: 0.82051 | val_1_rmse: 0.81916 |  0:00:49s
epoch 106| loss: 0.65645 | val_0_rmse: 0.84424 | val_1_rmse: 0.8363  |  0:00:50s
epoch 107| loss: 0.64567 | val_0_rmse: 0.90276 | val_1_rmse: 0.88306 |  0:00:50s
epoch 108| loss: 0.65162 | val_0_rmse: 0.85967 | val_1_rmse: 0.8409  |  0:00:51s
epoch 109| loss: 0.6521  | val_0_rmse: 0.86671 | val_1_rmse: 0.85379 |  0:00:51s
epoch 110| loss: 0.64464 | val_0_rmse: 0.84262 | val_1_rmse: 0.8305  |  0:00:52s
epoch 111| loss: 0.64424 | val_0_rmse: 0.82165 | val_1_rmse: 0.82374 |  0:00:52s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.81026
Best weights from best epoch are automatically used!
ended training at: 06:48:35
Feature importance:
Mean squared error is of 5969009087.028564
Mean absolute error:56418.010486535204
MAPE:0.41984985226664834
R2 score:0.2872561786536163
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:48:35
epoch 0  | loss: 1.88366 | val_0_rmse: 1.00348 | val_1_rmse: 1.00946 |  0:00:00s
epoch 1  | loss: 1.32739 | val_0_rmse: 1.00524 | val_1_rmse: 1.00969 |  0:00:00s
epoch 2  | loss: 1.08497 | val_0_rmse: 1.00316 | val_1_rmse: 1.00796 |  0:00:01s
epoch 3  | loss: 0.99604 | val_0_rmse: 0.99838 | val_1_rmse: 1.00254 |  0:00:01s
epoch 4  | loss: 0.98179 | val_0_rmse: 0.99662 | val_1_rmse: 1.00612 |  0:00:02s
epoch 5  | loss: 0.94095 | val_0_rmse: 0.99411 | val_1_rmse: 1.00254 |  0:00:02s
epoch 6  | loss: 0.91146 | val_0_rmse: 1.01415 | val_1_rmse: 1.02517 |  0:00:03s
epoch 7  | loss: 0.85608 | val_0_rmse: 1.01109 | val_1_rmse: 1.02105 |  0:00:03s
epoch 8  | loss: 0.83107 | val_0_rmse: 1.0354  | val_1_rmse: 1.04651 |  0:00:04s
epoch 9  | loss: 0.81166 | val_0_rmse: 1.00206 | val_1_rmse: 1.01228 |  0:00:04s
epoch 10 | loss: 0.8179  | val_0_rmse: 0.97786 | val_1_rmse: 0.98787 |  0:00:05s
epoch 11 | loss: 0.79668 | val_0_rmse: 0.96605 | val_1_rmse: 0.97458 |  0:00:05s
epoch 12 | loss: 0.77467 | val_0_rmse: 0.97093 | val_1_rmse: 0.97927 |  0:00:06s
epoch 13 | loss: 0.77263 | val_0_rmse: 0.95662 | val_1_rmse: 0.96362 |  0:00:06s
epoch 14 | loss: 0.75707 | val_0_rmse: 0.94632 | val_1_rmse: 0.95276 |  0:00:07s
epoch 15 | loss: 0.7543  | val_0_rmse: 0.947   | val_1_rmse: 0.95555 |  0:00:07s
epoch 16 | loss: 0.74484 | val_0_rmse: 0.94344 | val_1_rmse: 0.95256 |  0:00:08s
epoch 17 | loss: 0.76483 | val_0_rmse: 0.92709 | val_1_rmse: 0.93868 |  0:00:08s
epoch 18 | loss: 0.7559  | val_0_rmse: 0.91876 | val_1_rmse: 0.92997 |  0:00:09s
epoch 19 | loss: 0.7521  | val_0_rmse: 0.92087 | val_1_rmse: 0.93219 |  0:00:09s
epoch 20 | loss: 0.73791 | val_0_rmse: 0.91742 | val_1_rmse: 0.92951 |  0:00:09s
epoch 21 | loss: 0.72766 | val_0_rmse: 0.91715 | val_1_rmse: 0.93048 |  0:00:10s
epoch 22 | loss: 0.72725 | val_0_rmse: 0.93634 | val_1_rmse: 0.94693 |  0:00:10s
epoch 23 | loss: 0.72517 | val_0_rmse: 0.95529 | val_1_rmse: 0.96372 |  0:00:11s
epoch 24 | loss: 0.71614 | val_0_rmse: 0.95195 | val_1_rmse: 0.96016 |  0:00:11s
epoch 25 | loss: 0.72433 | val_0_rmse: 0.92425 | val_1_rmse: 0.93532 |  0:00:12s
epoch 26 | loss: 0.72115 | val_0_rmse: 0.90639 | val_1_rmse: 0.91855 |  0:00:12s
epoch 27 | loss: 0.708   | val_0_rmse: 0.89446 | val_1_rmse: 0.90965 |  0:00:13s
epoch 28 | loss: 0.7097  | val_0_rmse: 0.89614 | val_1_rmse: 0.91081 |  0:00:13s
epoch 29 | loss: 0.72012 | val_0_rmse: 0.90348 | val_1_rmse: 0.91456 |  0:00:14s
epoch 30 | loss: 0.72635 | val_0_rmse: 0.89849 | val_1_rmse: 0.9141  |  0:00:14s
epoch 31 | loss: 0.71517 | val_0_rmse: 0.91489 | val_1_rmse: 0.93111 |  0:00:15s
epoch 32 | loss: 0.71147 | val_0_rmse: 0.92172 | val_1_rmse: 0.93367 |  0:00:15s
epoch 33 | loss: 0.70854 | val_0_rmse: 0.90044 | val_1_rmse: 0.91312 |  0:00:15s
epoch 34 | loss: 0.70734 | val_0_rmse: 0.8991  | val_1_rmse: 0.91291 |  0:00:16s
epoch 35 | loss: 0.70359 | val_0_rmse: 0.90344 | val_1_rmse: 0.91571 |  0:00:16s
epoch 36 | loss: 0.70936 | val_0_rmse: 0.89087 | val_1_rmse: 0.90533 |  0:00:17s
epoch 37 | loss: 0.70192 | val_0_rmse: 0.91322 | val_1_rmse: 0.92369 |  0:00:17s
epoch 38 | loss: 0.7037  | val_0_rmse: 0.9085  | val_1_rmse: 0.92111 |  0:00:18s
epoch 39 | loss: 0.70973 | val_0_rmse: 0.89165 | val_1_rmse: 0.90679 |  0:00:18s
epoch 40 | loss: 0.69941 | val_0_rmse: 0.897   | val_1_rmse: 0.91119 |  0:00:19s
epoch 41 | loss: 0.70886 | val_0_rmse: 0.89569 | val_1_rmse: 0.91111 |  0:00:19s
epoch 42 | loss: 0.69843 | val_0_rmse: 0.88019 | val_1_rmse: 0.89762 |  0:00:20s
epoch 43 | loss: 0.69411 | val_0_rmse: 0.88539 | val_1_rmse: 0.90486 |  0:00:20s
epoch 44 | loss: 0.69797 | val_0_rmse: 0.88887 | val_1_rmse: 0.90819 |  0:00:21s
epoch 45 | loss: 0.69684 | val_0_rmse: 0.88146 | val_1_rmse: 0.90134 |  0:00:21s
epoch 46 | loss: 0.69228 | val_0_rmse: 0.86807 | val_1_rmse: 0.88483 |  0:00:22s
epoch 47 | loss: 0.68573 | val_0_rmse: 0.86743 | val_1_rmse: 0.88522 |  0:00:22s
epoch 48 | loss: 0.69224 | val_0_rmse: 0.86709 | val_1_rmse: 0.88755 |  0:00:22s
epoch 49 | loss: 0.68322 | val_0_rmse: 0.86251 | val_1_rmse: 0.88325 |  0:00:23s
epoch 50 | loss: 0.68406 | val_0_rmse: 0.85896 | val_1_rmse: 0.87821 |  0:00:23s
epoch 51 | loss: 0.69236 | val_0_rmse: 0.85866 | val_1_rmse: 0.87628 |  0:00:24s
epoch 52 | loss: 0.69307 | val_0_rmse: 0.87806 | val_1_rmse: 0.90167 |  0:00:24s
epoch 53 | loss: 0.69191 | val_0_rmse: 0.86928 | val_1_rmse: 0.89197 |  0:00:25s
epoch 54 | loss: 0.6812  | val_0_rmse: 0.86286 | val_1_rmse: 0.8861  |  0:00:25s
epoch 55 | loss: 0.68966 | val_0_rmse: 0.85952 | val_1_rmse: 0.88099 |  0:00:26s
epoch 56 | loss: 0.6809  | val_0_rmse: 0.86107 | val_1_rmse: 0.88472 |  0:00:26s
epoch 57 | loss: 0.67652 | val_0_rmse: 0.85292 | val_1_rmse: 0.87256 |  0:00:27s
epoch 58 | loss: 0.67974 | val_0_rmse: 0.85692 | val_1_rmse: 0.88388 |  0:00:27s
epoch 59 | loss: 0.69644 | val_0_rmse: 0.86883 | val_1_rmse: 0.89116 |  0:00:28s
epoch 60 | loss: 0.68256 | val_0_rmse: 0.85831 | val_1_rmse: 0.87729 |  0:00:28s
epoch 61 | loss: 0.67366 | val_0_rmse: 0.85636 | val_1_rmse: 0.87225 |  0:00:29s
epoch 62 | loss: 0.67234 | val_0_rmse: 0.85356 | val_1_rmse: 0.87066 |  0:00:29s
epoch 63 | loss: 0.6796  | val_0_rmse: 0.87282 | val_1_rmse: 0.89498 |  0:00:29s
epoch 64 | loss: 0.68879 | val_0_rmse: 0.84922 | val_1_rmse: 0.87131 |  0:00:30s
epoch 65 | loss: 0.67516 | val_0_rmse: 0.85592 | val_1_rmse: 0.87986 |  0:00:30s
epoch 66 | loss: 0.66441 | val_0_rmse: 0.85572 | val_1_rmse: 0.88131 |  0:00:31s
epoch 67 | loss: 0.67569 | val_0_rmse: 0.84367 | val_1_rmse: 0.87192 |  0:00:31s
epoch 68 | loss: 0.674   | val_0_rmse: 0.85255 | val_1_rmse: 0.8784  |  0:00:32s
epoch 69 | loss: 0.67851 | val_0_rmse: 0.85861 | val_1_rmse: 0.88623 |  0:00:32s
epoch 70 | loss: 0.67844 | val_0_rmse: 0.85356 | val_1_rmse: 0.882   |  0:00:33s
epoch 71 | loss: 0.67808 | val_0_rmse: 0.84712 | val_1_rmse: 0.87229 |  0:00:33s
epoch 72 | loss: 0.67399 | val_0_rmse: 0.84674 | val_1_rmse: 0.87301 |  0:00:34s
epoch 73 | loss: 0.67617 | val_0_rmse: 0.85012 | val_1_rmse: 0.87493 |  0:00:34s
epoch 74 | loss: 0.67318 | val_0_rmse: 0.84755 | val_1_rmse: 0.87    |  0:00:35s
epoch 75 | loss: 0.67473 | val_0_rmse: 0.85848 | val_1_rmse: 0.87737 |  0:00:35s
epoch 76 | loss: 0.67133 | val_0_rmse: 0.84968 | val_1_rmse: 0.86861 |  0:00:36s
epoch 77 | loss: 0.67425 | val_0_rmse: 0.8642  | val_1_rmse: 0.88706 |  0:00:36s
epoch 78 | loss: 0.66825 | val_0_rmse: 0.8603  | val_1_rmse: 0.88111 |  0:00:37s
epoch 79 | loss: 0.67344 | val_0_rmse: 0.85467 | val_1_rmse: 0.87507 |  0:00:37s
epoch 80 | loss: 0.67353 | val_0_rmse: 0.85853 | val_1_rmse: 0.87956 |  0:00:38s
epoch 81 | loss: 0.66808 | val_0_rmse: 0.85487 | val_1_rmse: 0.8784  |  0:00:38s
epoch 82 | loss: 0.669   | val_0_rmse: 0.83887 | val_1_rmse: 0.8641  |  0:00:38s
epoch 83 | loss: 0.6702  | val_0_rmse: 0.84049 | val_1_rmse: 0.86302 |  0:00:39s
epoch 84 | loss: 0.66615 | val_0_rmse: 0.85471 | val_1_rmse: 0.88032 |  0:00:39s
epoch 85 | loss: 0.66664 | val_0_rmse: 0.85078 | val_1_rmse: 0.87567 |  0:00:40s
epoch 86 | loss: 0.67087 | val_0_rmse: 0.8692  | val_1_rmse: 0.89379 |  0:00:40s
epoch 87 | loss: 0.67512 | val_0_rmse: 0.84295 | val_1_rmse: 0.86485 |  0:00:41s
epoch 88 | loss: 0.69282 | val_0_rmse: 0.87258 | val_1_rmse: 0.8961  |  0:00:41s
epoch 89 | loss: 0.6825  | val_0_rmse: 0.86518 | val_1_rmse: 0.88491 |  0:00:42s
epoch 90 | loss: 0.67953 | val_0_rmse: 0.84446 | val_1_rmse: 0.86745 |  0:00:42s
epoch 91 | loss: 0.67964 | val_0_rmse: 0.85432 | val_1_rmse: 0.87098 |  0:00:43s
epoch 92 | loss: 0.67588 | val_0_rmse: 0.8369  | val_1_rmse: 0.85815 |  0:00:43s
epoch 93 | loss: 0.67467 | val_0_rmse: 0.83856 | val_1_rmse: 0.86406 |  0:00:44s
epoch 94 | loss: 0.66317 | val_0_rmse: 0.84102 | val_1_rmse: 0.86615 |  0:00:44s
epoch 95 | loss: 0.67023 | val_0_rmse: 0.85217 | val_1_rmse: 0.87886 |  0:00:44s
epoch 96 | loss: 0.66581 | val_0_rmse: 0.84417 | val_1_rmse: 0.87252 |  0:00:45s
epoch 97 | loss: 0.66689 | val_0_rmse: 0.84766 | val_1_rmse: 0.87577 |  0:00:45s
epoch 98 | loss: 0.67148 | val_0_rmse: 0.84356 | val_1_rmse: 0.86931 |  0:00:46s
epoch 99 | loss: 0.66152 | val_0_rmse: 0.83871 | val_1_rmse: 0.86951 |  0:00:46s
epoch 100| loss: 0.67182 | val_0_rmse: 0.82809 | val_1_rmse: 0.85223 |  0:00:47s
epoch 101| loss: 0.66594 | val_0_rmse: 0.85762 | val_1_rmse: 0.87936 |  0:00:47s
epoch 102| loss: 0.66732 | val_0_rmse: 0.83848 | val_1_rmse: 0.86321 |  0:00:48s
epoch 103| loss: 0.67052 | val_0_rmse: 0.83534 | val_1_rmse: 0.86339 |  0:00:48s
epoch 104| loss: 0.66734 | val_0_rmse: 0.82827 | val_1_rmse: 0.85272 |  0:00:49s
epoch 105| loss: 0.66274 | val_0_rmse: 0.84932 | val_1_rmse: 0.87379 |  0:00:49s
epoch 106| loss: 0.65662 | val_0_rmse: 0.8362  | val_1_rmse: 0.86518 |  0:00:50s
epoch 107| loss: 0.66067 | val_0_rmse: 0.84609 | val_1_rmse: 0.87305 |  0:00:50s
epoch 108| loss: 0.65585 | val_0_rmse: 0.85275 | val_1_rmse: 0.87621 |  0:00:51s
epoch 109| loss: 0.66265 | val_0_rmse: 0.85216 | val_1_rmse: 0.88137 |  0:00:51s
epoch 110| loss: 0.65946 | val_0_rmse: 0.83835 | val_1_rmse: 0.86309 |  0:00:51s
epoch 111| loss: 0.65263 | val_0_rmse: 0.84683 | val_1_rmse: 0.87446 |  0:00:52s
epoch 112| loss: 0.64906 | val_0_rmse: 0.84918 | val_1_rmse: 0.87643 |  0:00:52s
epoch 113| loss: 0.66911 | val_0_rmse: 0.84918 | val_1_rmse: 0.87611 |  0:00:53s
epoch 114| loss: 0.66248 | val_0_rmse: 0.8337  | val_1_rmse: 0.85922 |  0:00:53s
epoch 115| loss: 0.64687 | val_0_rmse: 0.84843 | val_1_rmse: 0.87555 |  0:00:54s
epoch 116| loss: 0.65213 | val_0_rmse: 0.84783 | val_1_rmse: 0.87777 |  0:00:54s
epoch 117| loss: 0.64429 | val_0_rmse: 0.8322  | val_1_rmse: 0.86047 |  0:00:55s
epoch 118| loss: 0.64238 | val_0_rmse: 0.8398  | val_1_rmse: 0.87088 |  0:00:55s
epoch 119| loss: 0.64674 | val_0_rmse: 0.83642 | val_1_rmse: 0.86934 |  0:00:56s
epoch 120| loss: 0.64642 | val_0_rmse: 0.84259 | val_1_rmse: 0.87084 |  0:00:56s
epoch 121| loss: 0.65058 | val_0_rmse: 0.84047 | val_1_rmse: 0.86764 |  0:00:57s
epoch 122| loss: 0.65333 | val_0_rmse: 0.82829 | val_1_rmse: 0.86075 |  0:00:57s
epoch 123| loss: 0.64733 | val_0_rmse: 0.8349  | val_1_rmse: 0.87116 |  0:00:58s
epoch 124| loss: 0.64607 | val_0_rmse: 0.85006 | val_1_rmse: 0.88446 |  0:00:58s
epoch 125| loss: 0.65511 | val_0_rmse: 0.83278 | val_1_rmse: 0.86838 |  0:00:59s
epoch 126| loss: 0.64554 | val_0_rmse: 0.83161 | val_1_rmse: 0.86796 |  0:00:59s
epoch 127| loss: 0.63979 | val_0_rmse: 0.83075 | val_1_rmse: 0.86769 |  0:00:59s
epoch 128| loss: 0.65158 | val_0_rmse: 0.82562 | val_1_rmse: 0.86499 |  0:01:00s
epoch 129| loss: 0.65172 | val_0_rmse: 0.82552 | val_1_rmse: 0.86594 |  0:01:00s
epoch 130| loss: 0.64433 | val_0_rmse: 0.83373 | val_1_rmse: 0.87327 |  0:01:01s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.85223
Best weights from best epoch are automatically used!
ended training at: 06:49:37
Feature importance:
Mean squared error is of 5879445843.897988
Mean absolute error:56372.060775339276
MAPE:0.4669951978967529
R2 score:0.27012531483907376
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:49:37
epoch 0  | loss: 1.88709 | val_0_rmse: 1.01511 | val_1_rmse: 0.99591 |  0:00:00s
epoch 1  | loss: 1.19246 | val_0_rmse: 1.00875 | val_1_rmse: 0.98778 |  0:00:00s
epoch 2  | loss: 1.02086 | val_0_rmse: 1.00811 | val_1_rmse: 0.98761 |  0:00:01s
epoch 3  | loss: 0.96951 | val_0_rmse: 1.01045 | val_1_rmse: 0.98905 |  0:00:01s
epoch 4  | loss: 0.9472  | val_0_rmse: 1.04614 | val_1_rmse: 1.01595 |  0:00:02s
epoch 5  | loss: 0.91081 | val_0_rmse: 1.02499 | val_1_rmse: 0.99269 |  0:00:02s
epoch 6  | loss: 0.85697 | val_0_rmse: 1.02789 | val_1_rmse: 0.99529 |  0:00:03s
epoch 7  | loss: 0.86287 | val_0_rmse: 0.99809 | val_1_rmse: 0.9718  |  0:00:03s
epoch 8  | loss: 0.81188 | val_0_rmse: 1.04122 | val_1_rmse: 1.02091 |  0:00:04s
epoch 9  | loss: 0.79023 | val_0_rmse: 1.00846 | val_1_rmse: 0.99494 |  0:00:04s
epoch 10 | loss: 0.79091 | val_0_rmse: 0.99222 | val_1_rmse: 0.97099 |  0:00:05s
epoch 11 | loss: 0.7723  | val_0_rmse: 0.94647 | val_1_rmse: 0.92171 |  0:00:05s
epoch 12 | loss: 0.76961 | val_0_rmse: 0.96917 | val_1_rmse: 0.94118 |  0:00:06s
epoch 13 | loss: 0.77118 | val_0_rmse: 0.95677 | val_1_rmse: 0.93197 |  0:00:06s
epoch 14 | loss: 0.76614 | val_0_rmse: 0.95679 | val_1_rmse: 0.93636 |  0:00:06s
epoch 15 | loss: 0.7518  | val_0_rmse: 0.9511  | val_1_rmse: 0.93178 |  0:00:07s
epoch 16 | loss: 0.7482  | val_0_rmse: 0.94946 | val_1_rmse: 0.93401 |  0:00:07s
epoch 17 | loss: 0.73742 | val_0_rmse: 0.95312 | val_1_rmse: 0.94014 |  0:00:08s
epoch 18 | loss: 0.7322  | val_0_rmse: 0.94313 | val_1_rmse: 0.93279 |  0:00:08s
epoch 19 | loss: 0.72871 | val_0_rmse: 0.95968 | val_1_rmse: 0.94917 |  0:00:09s
epoch 20 | loss: 0.71831 | val_0_rmse: 0.96762 | val_1_rmse: 0.95654 |  0:00:09s
epoch 21 | loss: 0.71335 | val_0_rmse: 0.95397 | val_1_rmse: 0.94152 |  0:00:10s
epoch 22 | loss: 0.71884 | val_0_rmse: 0.95096 | val_1_rmse: 0.94696 |  0:00:10s
epoch 23 | loss: 0.71683 | val_0_rmse: 0.93802 | val_1_rmse: 0.93198 |  0:00:11s
epoch 24 | loss: 0.7185  | val_0_rmse: 0.93384 | val_1_rmse: 0.92544 |  0:00:11s
epoch 25 | loss: 0.71167 | val_0_rmse: 0.9302  | val_1_rmse: 0.92539 |  0:00:12s
epoch 26 | loss: 0.72158 | val_0_rmse: 0.92664 | val_1_rmse: 0.92122 |  0:00:12s
epoch 27 | loss: 0.70666 | val_0_rmse: 0.94002 | val_1_rmse: 0.92861 |  0:00:13s
epoch 28 | loss: 0.71663 | val_0_rmse: 0.94585 | val_1_rmse: 0.93336 |  0:00:13s
epoch 29 | loss: 0.71184 | val_0_rmse: 0.94388 | val_1_rmse: 0.9348  |  0:00:13s
epoch 30 | loss: 0.70607 | val_0_rmse: 0.94011 | val_1_rmse: 0.92919 |  0:00:14s
epoch 31 | loss: 0.70697 | val_0_rmse: 0.93842 | val_1_rmse: 0.92775 |  0:00:14s
epoch 32 | loss: 0.70982 | val_0_rmse: 0.93703 | val_1_rmse: 0.92503 |  0:00:15s
epoch 33 | loss: 0.7017  | val_0_rmse: 0.93864 | val_1_rmse: 0.92524 |  0:00:15s
epoch 34 | loss: 0.70826 | val_0_rmse: 0.94315 | val_1_rmse: 0.93295 |  0:00:16s
epoch 35 | loss: 0.71313 | val_0_rmse: 0.94035 | val_1_rmse: 0.93288 |  0:00:16s
epoch 36 | loss: 0.70422 | val_0_rmse: 0.9207  | val_1_rmse: 0.91285 |  0:00:17s
epoch 37 | loss: 0.70159 | val_0_rmse: 0.91791 | val_1_rmse: 0.91143 |  0:00:17s
epoch 38 | loss: 0.70353 | val_0_rmse: 0.90679 | val_1_rmse: 0.89598 |  0:00:18s
epoch 39 | loss: 0.70647 | val_0_rmse: 0.89761 | val_1_rmse: 0.8902  |  0:00:18s
epoch 40 | loss: 0.70149 | val_0_rmse: 0.90204 | val_1_rmse: 0.89229 |  0:00:19s
epoch 41 | loss: 0.69938 | val_0_rmse: 0.89779 | val_1_rmse: 0.88998 |  0:00:19s
epoch 42 | loss: 0.70016 | val_0_rmse: 0.89378 | val_1_rmse: 0.88626 |  0:00:19s
epoch 43 | loss: 0.70179 | val_0_rmse: 0.90538 | val_1_rmse: 0.9069  |  0:00:20s
epoch 44 | loss: 0.69652 | val_0_rmse: 0.89236 | val_1_rmse: 0.88199 |  0:00:20s
epoch 45 | loss: 0.70853 | val_0_rmse: 0.89511 | val_1_rmse: 0.88618 |  0:00:21s
epoch 46 | loss: 0.70446 | val_0_rmse: 0.89873 | val_1_rmse: 0.89469 |  0:00:21s
epoch 47 | loss: 0.70512 | val_0_rmse: 0.90654 | val_1_rmse: 0.90403 |  0:00:22s
epoch 48 | loss: 0.70183 | val_0_rmse: 0.90691 | val_1_rmse: 0.89995 |  0:00:22s
epoch 49 | loss: 0.7052  | val_0_rmse: 0.90038 | val_1_rmse: 0.89253 |  0:00:23s
epoch 50 | loss: 0.69976 | val_0_rmse: 0.90032 | val_1_rmse: 0.89319 |  0:00:23s
epoch 51 | loss: 0.7039  | val_0_rmse: 0.90435 | val_1_rmse: 0.89546 |  0:00:24s
epoch 52 | loss: 0.69435 | val_0_rmse: 0.90334 | val_1_rmse: 0.89644 |  0:00:24s
epoch 53 | loss: 0.6946  | val_0_rmse: 0.90181 | val_1_rmse: 0.88931 |  0:00:25s
epoch 54 | loss: 0.68949 | val_0_rmse: 0.89894 | val_1_rmse: 0.88639 |  0:00:25s
epoch 55 | loss: 0.69088 | val_0_rmse: 0.90177 | val_1_rmse: 0.8931  |  0:00:26s
epoch 56 | loss: 0.68664 | val_0_rmse: 0.90564 | val_1_rmse: 0.89702 |  0:00:26s
epoch 57 | loss: 0.69029 | val_0_rmse: 0.89867 | val_1_rmse: 0.8906  |  0:00:27s
epoch 58 | loss: 0.69086 | val_0_rmse: 0.89896 | val_1_rmse: 0.89035 |  0:00:27s
epoch 59 | loss: 0.68853 | val_0_rmse: 0.89248 | val_1_rmse: 0.88574 |  0:00:27s
epoch 60 | loss: 0.6914  | val_0_rmse: 0.88954 | val_1_rmse: 0.88891 |  0:00:28s
epoch 61 | loss: 0.70371 | val_0_rmse: 0.88418 | val_1_rmse: 0.88087 |  0:00:28s
epoch 62 | loss: 0.69017 | val_0_rmse: 0.88618 | val_1_rmse: 0.8866  |  0:00:29s
epoch 63 | loss: 0.69244 | val_0_rmse: 0.89223 | val_1_rmse: 0.8963  |  0:00:29s
epoch 64 | loss: 0.70096 | val_0_rmse: 0.87895 | val_1_rmse: 0.88186 |  0:00:30s
epoch 65 | loss: 0.69917 | val_0_rmse: 0.87413 | val_1_rmse: 0.88427 |  0:00:30s
epoch 66 | loss: 0.69074 | val_0_rmse: 0.87694 | val_1_rmse: 0.88773 |  0:00:31s
epoch 67 | loss: 0.70102 | val_0_rmse: 0.88157 | val_1_rmse: 0.88827 |  0:00:31s
epoch 68 | loss: 0.69852 | val_0_rmse: 0.86807 | val_1_rmse: 0.87578 |  0:00:32s
epoch 69 | loss: 0.68969 | val_0_rmse: 0.89111 | val_1_rmse: 0.89233 |  0:00:32s
epoch 70 | loss: 0.68255 | val_0_rmse: 0.89665 | val_1_rmse: 0.89835 |  0:00:33s
epoch 71 | loss: 0.69831 | val_0_rmse: 0.8693  | val_1_rmse: 0.87078 |  0:00:33s
epoch 72 | loss: 0.69098 | val_0_rmse: 0.87795 | val_1_rmse: 0.87908 |  0:00:33s
epoch 73 | loss: 0.7003  | val_0_rmse: 0.89092 | val_1_rmse: 0.89646 |  0:00:34s
epoch 74 | loss: 0.70019 | val_0_rmse: 0.87683 | val_1_rmse: 0.87601 |  0:00:34s
epoch 75 | loss: 0.69766 | val_0_rmse: 0.85783 | val_1_rmse: 0.85656 |  0:00:35s
epoch 76 | loss: 0.69327 | val_0_rmse: 0.85868 | val_1_rmse: 0.86484 |  0:00:35s
epoch 77 | loss: 0.71232 | val_0_rmse: 0.85103 | val_1_rmse: 0.85518 |  0:00:36s
epoch 78 | loss: 0.6915  | val_0_rmse: 0.844   | val_1_rmse: 0.85597 |  0:00:36s
epoch 79 | loss: 0.70439 | val_0_rmse: 0.85206 | val_1_rmse: 0.84568 |  0:00:37s
epoch 80 | loss: 0.69721 | val_0_rmse: 0.84892 | val_1_rmse: 0.84913 |  0:00:37s
epoch 81 | loss: 0.68413 | val_0_rmse: 0.85378 | val_1_rmse: 0.86256 |  0:00:38s
epoch 82 | loss: 0.68954 | val_0_rmse: 0.85447 | val_1_rmse: 0.87044 |  0:00:38s
epoch 83 | loss: 0.6899  | val_0_rmse: 0.84649 | val_1_rmse: 0.85635 |  0:00:39s
epoch 84 | loss: 0.68223 | val_0_rmse: 0.8484  | val_1_rmse: 0.85995 |  0:00:39s
epoch 85 | loss: 0.6844  | val_0_rmse: 0.85442 | val_1_rmse: 0.86988 |  0:00:40s
epoch 86 | loss: 0.67063 | val_0_rmse: 0.86304 | val_1_rmse: 0.87027 |  0:00:40s
epoch 87 | loss: 0.7033  | val_0_rmse: 0.85652 | val_1_rmse: 0.87384 |  0:00:40s
epoch 88 | loss: 0.67895 | val_0_rmse: 0.85279 | val_1_rmse: 0.87524 |  0:00:41s
epoch 89 | loss: 0.67573 | val_0_rmse: 0.85808 | val_1_rmse: 0.87767 |  0:00:41s
epoch 90 | loss: 0.68261 | val_0_rmse: 0.85084 | val_1_rmse: 0.8656  |  0:00:42s
epoch 91 | loss: 0.67867 | val_0_rmse: 0.84725 | val_1_rmse: 0.86299 |  0:00:42s
epoch 92 | loss: 0.67092 | val_0_rmse: 0.84425 | val_1_rmse: 0.85547 |  0:00:43s
epoch 93 | loss: 0.66805 | val_0_rmse: 0.87537 | val_1_rmse: 0.88364 |  0:00:43s
epoch 94 | loss: 0.67286 | val_0_rmse: 0.88152 | val_1_rmse: 0.89316 |  0:00:44s
epoch 95 | loss: 0.67374 | val_0_rmse: 0.83786 | val_1_rmse: 0.85102 |  0:00:44s
epoch 96 | loss: 0.68685 | val_0_rmse: 0.85306 | val_1_rmse: 0.86909 |  0:00:45s
epoch 97 | loss: 0.69019 | val_0_rmse: 0.85399 | val_1_rmse: 0.87339 |  0:00:45s
epoch 98 | loss: 0.69049 | val_0_rmse: 0.83623 | val_1_rmse: 0.84566 |  0:00:46s
epoch 99 | loss: 0.6936  | val_0_rmse: 0.82868 | val_1_rmse: 0.84013 |  0:00:46s
epoch 100| loss: 0.68646 | val_0_rmse: 0.84715 | val_1_rmse: 0.86158 |  0:00:47s
epoch 101| loss: 0.68567 | val_0_rmse: 0.82654 | val_1_rmse: 0.84252 |  0:00:47s
epoch 102| loss: 0.6789  | val_0_rmse: 0.82962 | val_1_rmse: 0.85058 |  0:00:47s
epoch 103| loss: 0.67177 | val_0_rmse: 0.84049 | val_1_rmse: 0.86511 |  0:00:48s
epoch 104| loss: 0.68439 | val_0_rmse: 0.83987 | val_1_rmse: 0.85498 |  0:00:48s
epoch 105| loss: 0.66891 | val_0_rmse: 0.84722 | val_1_rmse: 0.86626 |  0:00:49s
epoch 106| loss: 0.66877 | val_0_rmse: 0.85535 | val_1_rmse: 0.87822 |  0:00:49s
epoch 107| loss: 0.67999 | val_0_rmse: 0.86588 | val_1_rmse: 0.88324 |  0:00:50s
epoch 108| loss: 0.67039 | val_0_rmse: 0.87101 | val_1_rmse: 0.89266 |  0:00:50s
epoch 109| loss: 0.66577 | val_0_rmse: 0.86017 | val_1_rmse: 0.8791  |  0:00:51s
epoch 110| loss: 0.66992 | val_0_rmse: 0.84974 | val_1_rmse: 0.86873 |  0:00:51s
epoch 111| loss: 0.66809 | val_0_rmse: 0.8469  | val_1_rmse: 0.86827 |  0:00:52s
epoch 112| loss: 0.66841 | val_0_rmse: 0.8464  | val_1_rmse: 0.86454 |  0:00:52s
epoch 113| loss: 0.66687 | val_0_rmse: 0.87373 | val_1_rmse: 0.89148 |  0:00:53s
epoch 114| loss: 0.66864 | val_0_rmse: 0.84988 | val_1_rmse: 0.86881 |  0:00:53s
epoch 115| loss: 0.67176 | val_0_rmse: 0.88645 | val_1_rmse: 0.90684 |  0:00:54s
epoch 116| loss: 0.67641 | val_0_rmse: 0.86387 | val_1_rmse: 0.88214 |  0:00:54s
epoch 117| loss: 0.66895 | val_0_rmse: 0.84877 | val_1_rmse: 0.87059 |  0:00:55s
epoch 118| loss: 0.66003 | val_0_rmse: 0.855   | val_1_rmse: 0.88029 |  0:00:55s
epoch 119| loss: 0.65965 | val_0_rmse: 0.84242 | val_1_rmse: 0.86605 |  0:00:55s
epoch 120| loss: 0.6672  | val_0_rmse: 0.85046 | val_1_rmse: 0.86852 |  0:00:56s
epoch 121| loss: 0.66083 | val_0_rmse: 0.83763 | val_1_rmse: 0.86497 |  0:00:56s
epoch 122| loss: 0.66085 | val_0_rmse: 0.83188 | val_1_rmse: 0.85089 |  0:00:57s
epoch 123| loss: 0.65251 | val_0_rmse: 0.83264 | val_1_rmse: 0.84791 |  0:00:57s
epoch 124| loss: 0.65563 | val_0_rmse: 0.85441 | val_1_rmse: 0.87689 |  0:00:58s
epoch 125| loss: 0.64937 | val_0_rmse: 0.85747 | val_1_rmse: 0.87787 |  0:00:58s
epoch 126| loss: 0.65546 | val_0_rmse: 0.86507 | val_1_rmse: 0.88184 |  0:00:59s
epoch 127| loss: 0.65754 | val_0_rmse: 0.847   | val_1_rmse: 0.86742 |  0:00:59s
epoch 128| loss: 0.65803 | val_0_rmse: 0.84624 | val_1_rmse: 0.87149 |  0:01:00s
epoch 129| loss: 0.66002 | val_0_rmse: 0.84701 | val_1_rmse: 0.87086 |  0:01:00s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.84013
Best weights from best epoch are automatically used!
ended training at: 06:50:37
Feature importance:
Mean squared error is of 5696393958.26535
Mean absolute error:54056.06535368957
MAPE:0.48667732384994417
R2 score:0.27335818589185823
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:50:37
epoch 0  | loss: 1.71977 | val_0_rmse: 0.99646 | val_1_rmse: 0.99751 |  0:00:00s
epoch 1  | loss: 1.19713 | val_0_rmse: 1.00798 | val_1_rmse: 1.00876 |  0:00:00s
epoch 2  | loss: 1.03003 | val_0_rmse: 1.00348 | val_1_rmse: 1.00498 |  0:00:01s
epoch 3  | loss: 0.91057 | val_0_rmse: 1.002   | val_1_rmse: 1.00296 |  0:00:01s
epoch 4  | loss: 0.90033 | val_0_rmse: 0.98819 | val_1_rmse: 0.99054 |  0:00:02s
epoch 5  | loss: 0.87543 | val_0_rmse: 0.98176 | val_1_rmse: 0.98375 |  0:00:02s
epoch 6  | loss: 0.82251 | val_0_rmse: 0.98052 | val_1_rmse: 0.98465 |  0:00:03s
epoch 7  | loss: 0.80179 | val_0_rmse: 0.97881 | val_1_rmse: 0.98412 |  0:00:03s
epoch 8  | loss: 0.78545 | val_0_rmse: 0.98504 | val_1_rmse: 0.98976 |  0:00:04s
epoch 9  | loss: 0.7757  | val_0_rmse: 0.9897  | val_1_rmse: 0.99364 |  0:00:04s
epoch 10 | loss: 0.76445 | val_0_rmse: 0.98883 | val_1_rmse: 0.99201 |  0:00:05s
epoch 11 | loss: 0.75539 | val_0_rmse: 0.97835 | val_1_rmse: 0.97942 |  0:00:05s
epoch 12 | loss: 0.74166 | val_0_rmse: 0.95962 | val_1_rmse: 0.95934 |  0:00:06s
epoch 13 | loss: 0.73795 | val_0_rmse: 0.95    | val_1_rmse: 0.94894 |  0:00:06s
epoch 14 | loss: 0.74055 | val_0_rmse: 0.94486 | val_1_rmse: 0.94288 |  0:00:07s
epoch 15 | loss: 0.73864 | val_0_rmse: 0.94311 | val_1_rmse: 0.94108 |  0:00:07s
epoch 16 | loss: 0.73534 | val_0_rmse: 0.9465  | val_1_rmse: 0.94402 |  0:00:08s
epoch 17 | loss: 0.72354 | val_0_rmse: 0.93629 | val_1_rmse: 0.93202 |  0:00:08s
epoch 18 | loss: 0.7208  | val_0_rmse: 0.93313 | val_1_rmse: 0.92682 |  0:00:08s
epoch 19 | loss: 0.72296 | val_0_rmse: 0.92576 | val_1_rmse: 0.91822 |  0:00:09s
epoch 20 | loss: 0.7175  | val_0_rmse: 0.91387 | val_1_rmse: 0.90361 |  0:00:09s
epoch 21 | loss: 0.72521 | val_0_rmse: 0.9147  | val_1_rmse: 0.90232 |  0:00:10s
epoch 22 | loss: 0.70821 | val_0_rmse: 0.9091  | val_1_rmse: 0.89889 |  0:00:10s
epoch 23 | loss: 0.71559 | val_0_rmse: 0.92311 | val_1_rmse: 0.91363 |  0:00:11s
epoch 24 | loss: 0.70456 | val_0_rmse: 0.90586 | val_1_rmse: 0.89667 |  0:00:11s
epoch 25 | loss: 0.69526 | val_0_rmse: 0.90528 | val_1_rmse: 0.89997 |  0:00:12s
epoch 26 | loss: 0.69589 | val_0_rmse: 0.90352 | val_1_rmse: 0.89702 |  0:00:12s
epoch 27 | loss: 0.69718 | val_0_rmse: 0.92674 | val_1_rmse: 0.91816 |  0:00:13s
epoch 28 | loss: 0.68871 | val_0_rmse: 0.91763 | val_1_rmse: 0.90673 |  0:00:13s
epoch 29 | loss: 0.69034 | val_0_rmse: 0.91856 | val_1_rmse: 0.9086  |  0:00:14s
epoch 30 | loss: 0.69928 | val_0_rmse: 0.91184 | val_1_rmse: 0.90231 |  0:00:14s
epoch 31 | loss: 0.68859 | val_0_rmse: 0.89273 | val_1_rmse: 0.88234 |  0:00:15s
epoch 32 | loss: 0.68047 | val_0_rmse: 0.88371 | val_1_rmse: 0.87195 |  0:00:15s
epoch 33 | loss: 0.68953 | val_0_rmse: 0.88993 | val_1_rmse: 0.87739 |  0:00:16s
epoch 34 | loss: 0.6791  | val_0_rmse: 0.89084 | val_1_rmse: 0.87694 |  0:00:16s
epoch 35 | loss: 0.67174 | val_0_rmse: 0.89135 | val_1_rmse: 0.87601 |  0:00:16s
epoch 36 | loss: 0.66417 | val_0_rmse: 0.87957 | val_1_rmse: 0.86173 |  0:00:17s
epoch 37 | loss: 0.66979 | val_0_rmse: 0.87988 | val_1_rmse: 0.86196 |  0:00:17s
epoch 38 | loss: 0.66916 | val_0_rmse: 0.87722 | val_1_rmse: 0.85649 |  0:00:18s
epoch 39 | loss: 0.67524 | val_0_rmse: 0.87634 | val_1_rmse: 0.86011 |  0:00:18s
epoch 40 | loss: 0.66681 | val_0_rmse: 0.88686 | val_1_rmse: 0.87224 |  0:00:19s
epoch 41 | loss: 0.66683 | val_0_rmse: 0.87267 | val_1_rmse: 0.85755 |  0:00:19s
epoch 42 | loss: 0.66133 | val_0_rmse: 0.8796  | val_1_rmse: 0.86546 |  0:00:20s
epoch 43 | loss: 0.66947 | val_0_rmse: 0.86273 | val_1_rmse: 0.84473 |  0:00:20s
epoch 44 | loss: 0.66123 | val_0_rmse: 0.86422 | val_1_rmse: 0.84755 |  0:00:21s
epoch 45 | loss: 0.67483 | val_0_rmse: 0.87807 | val_1_rmse: 0.86104 |  0:00:21s
epoch 46 | loss: 0.67336 | val_0_rmse: 0.87266 | val_1_rmse: 0.85448 |  0:00:22s
epoch 47 | loss: 0.67461 | val_0_rmse: 0.87572 | val_1_rmse: 0.85838 |  0:00:22s
epoch 48 | loss: 0.67764 | val_0_rmse: 0.86569 | val_1_rmse: 0.84714 |  0:00:23s
epoch 49 | loss: 0.66917 | val_0_rmse: 0.87623 | val_1_rmse: 0.86008 |  0:00:23s
epoch 50 | loss: 0.66501 | val_0_rmse: 0.88084 | val_1_rmse: 0.86321 |  0:00:23s
epoch 51 | loss: 0.6666  | val_0_rmse: 0.87141 | val_1_rmse: 0.85341 |  0:00:24s
epoch 52 | loss: 0.65995 | val_0_rmse: 0.87221 | val_1_rmse: 0.85072 |  0:00:24s
epoch 53 | loss: 0.66241 | val_0_rmse: 0.87196 | val_1_rmse: 0.85321 |  0:00:25s
epoch 54 | loss: 0.64916 | val_0_rmse: 0.87575 | val_1_rmse: 0.8563  |  0:00:25s
epoch 55 | loss: 0.66092 | val_0_rmse: 0.85492 | val_1_rmse: 0.83462 |  0:00:26s
epoch 56 | loss: 0.65541 | val_0_rmse: 0.85472 | val_1_rmse: 0.83784 |  0:00:26s
epoch 57 | loss: 0.65688 | val_0_rmse: 0.85654 | val_1_rmse: 0.83789 |  0:00:27s
epoch 58 | loss: 0.65053 | val_0_rmse: 0.85961 | val_1_rmse: 0.84609 |  0:00:27s
epoch 59 | loss: 0.66196 | val_0_rmse: 0.8665  | val_1_rmse: 0.85207 |  0:00:28s
epoch 60 | loss: 0.66056 | val_0_rmse: 0.87557 | val_1_rmse: 0.85774 |  0:00:28s
epoch 61 | loss: 0.64535 | val_0_rmse: 0.8703  | val_1_rmse: 0.85676 |  0:00:29s
epoch 62 | loss: 0.66994 | val_0_rmse: 0.86839 | val_1_rmse: 0.8544  |  0:00:29s
epoch 63 | loss: 0.66395 | val_0_rmse: 0.87568 | val_1_rmse: 0.85792 |  0:00:30s
epoch 64 | loss: 0.66138 | val_0_rmse: 0.86751 | val_1_rmse: 0.84924 |  0:00:30s
epoch 65 | loss: 0.66186 | val_0_rmse: 0.87299 | val_1_rmse: 0.85908 |  0:00:31s
epoch 66 | loss: 0.66314 | val_0_rmse: 0.84353 | val_1_rmse: 0.82888 |  0:00:31s
epoch 67 | loss: 0.66614 | val_0_rmse: 0.85989 | val_1_rmse: 0.84449 |  0:00:31s
epoch 68 | loss: 0.65975 | val_0_rmse: 0.84065 | val_1_rmse: 0.82927 |  0:00:32s
epoch 69 | loss: 0.6653  | val_0_rmse: 0.84604 | val_1_rmse: 0.83248 |  0:00:32s
epoch 70 | loss: 0.66017 | val_0_rmse: 0.8479  | val_1_rmse: 0.83322 |  0:00:33s
epoch 71 | loss: 0.66142 | val_0_rmse: 0.84388 | val_1_rmse: 0.8273  |  0:00:33s
epoch 72 | loss: 0.66307 | val_0_rmse: 0.85097 | val_1_rmse: 0.83261 |  0:00:34s
epoch 73 | loss: 0.64991 | val_0_rmse: 0.86538 | val_1_rmse: 0.84829 |  0:00:34s
epoch 74 | loss: 0.65217 | val_0_rmse: 0.85132 | val_1_rmse: 0.83068 |  0:00:35s
epoch 75 | loss: 0.64954 | val_0_rmse: 0.83823 | val_1_rmse: 0.81965 |  0:00:35s
epoch 76 | loss: 0.6527  | val_0_rmse: 0.83097 | val_1_rmse: 0.81072 |  0:00:36s
epoch 77 | loss: 0.65897 | val_0_rmse: 0.83607 | val_1_rmse: 0.81265 |  0:00:36s
epoch 78 | loss: 0.6536  | val_0_rmse: 0.85017 | val_1_rmse: 0.82803 |  0:00:36s
epoch 79 | loss: 0.64591 | val_0_rmse: 0.84503 | val_1_rmse: 0.82186 |  0:00:37s
epoch 80 | loss: 0.65365 | val_0_rmse: 0.82908 | val_1_rmse: 0.80823 |  0:00:37s
epoch 81 | loss: 0.64761 | val_0_rmse: 0.83022 | val_1_rmse: 0.81063 |  0:00:38s
epoch 82 | loss: 0.65288 | val_0_rmse: 0.8243  | val_1_rmse: 0.80822 |  0:00:38s
epoch 83 | loss: 0.65067 | val_0_rmse: 0.82661 | val_1_rmse: 0.80583 |  0:00:39s
epoch 84 | loss: 0.64679 | val_0_rmse: 0.85108 | val_1_rmse: 0.83151 |  0:00:39s
epoch 85 | loss: 0.6485  | val_0_rmse: 0.83027 | val_1_rmse: 0.81417 |  0:00:40s
epoch 86 | loss: 0.65946 | val_0_rmse: 0.8403  | val_1_rmse: 0.8281  |  0:00:40s
epoch 87 | loss: 0.65934 | val_0_rmse: 0.85019 | val_1_rmse: 0.83844 |  0:00:41s
epoch 88 | loss: 0.65844 | val_0_rmse: 0.81912 | val_1_rmse: 0.80996 |  0:00:41s
epoch 89 | loss: 0.6465  | val_0_rmse: 0.86557 | val_1_rmse: 0.86648 |  0:00:42s
epoch 90 | loss: 0.66595 | val_0_rmse: 0.82778 | val_1_rmse: 0.81816 |  0:00:42s
epoch 91 | loss: 0.65984 | val_0_rmse: 0.82715 | val_1_rmse: 0.82155 |  0:00:43s
epoch 92 | loss: 0.65363 | val_0_rmse: 0.83364 | val_1_rmse: 0.83593 |  0:00:43s
epoch 93 | loss: 0.64298 | val_0_rmse: 0.84601 | val_1_rmse: 0.84291 |  0:00:43s
epoch 94 | loss: 0.63949 | val_0_rmse: 0.82911 | val_1_rmse: 0.82375 |  0:00:44s
epoch 95 | loss: 0.63957 | val_0_rmse: 0.83537 | val_1_rmse: 0.82608 |  0:00:44s
epoch 96 | loss: 0.64605 | val_0_rmse: 0.81546 | val_1_rmse: 0.83266 |  0:00:45s
epoch 97 | loss: 0.64255 | val_0_rmse: 0.81008 | val_1_rmse: 0.83444 |  0:00:45s
epoch 98 | loss: 0.65816 | val_0_rmse: 0.82246 | val_1_rmse: 0.84777 |  0:00:46s
epoch 99 | loss: 0.65014 | val_0_rmse: 0.82243 | val_1_rmse: 0.84705 |  0:00:46s
epoch 100| loss: 0.65276 | val_0_rmse: 0.84054 | val_1_rmse: 0.84693 |  0:00:47s
epoch 101| loss: 0.64647 | val_0_rmse: 0.84822 | val_1_rmse: 0.8241  |  0:00:47s
epoch 102| loss: 0.64224 | val_0_rmse: 0.84418 | val_1_rmse: 0.82424 |  0:00:48s
epoch 103| loss: 0.64412 | val_0_rmse: 0.82267 | val_1_rmse: 0.80758 |  0:00:48s
epoch 104| loss: 0.64349 | val_0_rmse: 0.83415 | val_1_rmse: 0.81864 |  0:00:49s
epoch 105| loss: 0.64446 | val_0_rmse: 0.82545 | val_1_rmse: 0.81769 |  0:00:49s
epoch 106| loss: 0.63633 | val_0_rmse: 0.8317  | val_1_rmse: 0.82289 |  0:00:50s
epoch 107| loss: 0.63969 | val_0_rmse: 0.81613 | val_1_rmse: 0.81179 |  0:00:50s
epoch 108| loss: 0.64231 | val_0_rmse: 0.94956 | val_1_rmse: 0.95538 |  0:00:50s
epoch 109| loss: 0.64919 | val_0_rmse: 0.79789 | val_1_rmse: 0.79399 |  0:00:51s
epoch 110| loss: 0.63525 | val_0_rmse: 0.79668 | val_1_rmse: 0.817   |  0:00:51s
epoch 111| loss: 0.63726 | val_0_rmse: 0.81537 | val_1_rmse: 0.85364 |  0:00:52s
epoch 112| loss: 0.639   | val_0_rmse: 0.81729 | val_1_rmse: 0.85268 |  0:00:52s
epoch 113| loss: 0.63078 | val_0_rmse: 0.81407 | val_1_rmse: 0.86718 |  0:00:53s
epoch 114| loss: 0.63605 | val_0_rmse: 0.84537 | val_1_rmse: 0.89059 |  0:00:53s
epoch 115| loss: 0.63541 | val_0_rmse: 0.83836 | val_1_rmse: 0.88846 |  0:00:54s
epoch 116| loss: 0.62554 | val_0_rmse: 0.79842 | val_1_rmse: 0.88077 |  0:00:54s
epoch 117| loss: 0.63377 | val_0_rmse: 0.79427 | val_1_rmse: 0.86228 |  0:00:55s
epoch 118| loss: 0.63186 | val_0_rmse: 0.78883 | val_1_rmse: 0.8625  |  0:00:55s
epoch 119| loss: 0.6336  | val_0_rmse: 0.80393 | val_1_rmse: 0.89477 |  0:00:56s
epoch 120| loss: 0.64174 | val_0_rmse: 0.80492 | val_1_rmse: 0.88146 |  0:00:56s
epoch 121| loss: 0.63233 | val_0_rmse: 0.8254  | val_1_rmse: 0.89992 |  0:00:57s
epoch 122| loss: 0.63113 | val_0_rmse: 0.79973 | val_1_rmse: 0.90749 |  0:00:57s
epoch 123| loss: 0.63467 | val_0_rmse: 0.80295 | val_1_rmse: 0.88751 |  0:00:57s
epoch 124| loss: 0.63131 | val_0_rmse: 0.79248 | val_1_rmse: 0.8785  |  0:00:58s
epoch 125| loss: 0.63012 | val_0_rmse: 0.80495 | val_1_rmse: 0.8945  |  0:00:58s
epoch 126| loss: 0.63526 | val_0_rmse: 0.80461 | val_1_rmse: 0.88862 |  0:00:59s
epoch 127| loss: 0.62234 | val_0_rmse: 0.79638 | val_1_rmse: 0.90272 |  0:00:59s
epoch 128| loss: 0.63556 | val_0_rmse: 0.81726 | val_1_rmse: 0.90349 |  0:01:00s
epoch 129| loss: 0.62243 | val_0_rmse: 0.82521 | val_1_rmse: 0.9165  |  0:01:00s
epoch 130| loss: 0.63201 | val_0_rmse: 0.79079 | val_1_rmse: 0.91373 |  0:01:01s
epoch 131| loss: 0.63049 | val_0_rmse: 0.79521 | val_1_rmse: 0.89344 |  0:01:01s
epoch 132| loss: 0.64405 | val_0_rmse: 0.78897 | val_1_rmse: 0.96157 |  0:01:02s
epoch 133| loss: 0.62325 | val_0_rmse: 0.83288 | val_1_rmse: 0.94983 |  0:01:02s
epoch 134| loss: 0.62922 | val_0_rmse: 0.81908 | val_1_rmse: 0.91418 |  0:01:02s
epoch 135| loss: 0.62288 | val_0_rmse: 0.83103 | val_1_rmse: 0.93721 |  0:01:03s
epoch 136| loss: 0.63077 | val_0_rmse: 0.83519 | val_1_rmse: 0.96143 |  0:01:03s
epoch 137| loss: 0.63201 | val_0_rmse: 0.79844 | val_1_rmse: 0.92849 |  0:01:04s
epoch 138| loss: 0.62273 | val_0_rmse: 0.78253 | val_1_rmse: 0.89151 |  0:01:04s
epoch 139| loss: 0.62297 | val_0_rmse: 0.78562 | val_1_rmse: 0.92234 |  0:01:05s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.79399
Best weights from best epoch are automatically used!
ended training at: 06:51:43
Feature importance:
Mean squared error is of 6055804366.908036
Mean absolute error:56686.72662542409
MAPE:0.46551465110188467
R2 score:0.30195205722877005
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:51:43
epoch 0  | loss: 1.67062 | val_0_rmse: 1.02496 | val_1_rmse: 0.95639 |  0:00:00s
epoch 1  | loss: 1.1383  | val_0_rmse: 1.01528 | val_1_rmse: 0.94181 |  0:00:00s
epoch 2  | loss: 1.08151 | val_0_rmse: 1.01654 | val_1_rmse: 0.94445 |  0:00:01s
epoch 3  | loss: 0.9297  | val_0_rmse: 0.99941 | val_1_rmse: 0.92542 |  0:00:01s
epoch 4  | loss: 0.90341 | val_0_rmse: 1.02783 | val_1_rmse: 0.9625  |  0:00:02s
epoch 5  | loss: 0.8688  | val_0_rmse: 0.99787 | val_1_rmse: 0.92658 |  0:00:02s
epoch 6  | loss: 0.84429 | val_0_rmse: 1.02455 | val_1_rmse: 0.95621 |  0:00:03s
epoch 7  | loss: 0.83527 | val_0_rmse: 1.01079 | val_1_rmse: 0.93863 |  0:00:03s
epoch 8  | loss: 0.81175 | val_0_rmse: 1.00044 | val_1_rmse: 0.93214 |  0:00:04s
epoch 9  | loss: 0.79268 | val_0_rmse: 0.98665 | val_1_rmse: 0.92103 |  0:00:04s
epoch 10 | loss: 0.79331 | val_0_rmse: 0.97063 | val_1_rmse: 0.90709 |  0:00:05s
epoch 11 | loss: 0.78162 | val_0_rmse: 0.97206 | val_1_rmse: 0.91538 |  0:00:05s
epoch 12 | loss: 0.76926 | val_0_rmse: 0.98195 | val_1_rmse: 0.91778 |  0:00:06s
epoch 13 | loss: 0.79394 | val_0_rmse: 0.98649 | val_1_rmse: 0.92023 |  0:00:06s
epoch 14 | loss: 0.79339 | val_0_rmse: 0.96969 | val_1_rmse: 0.89494 |  0:00:07s
epoch 15 | loss: 0.77689 | val_0_rmse: 0.96446 | val_1_rmse: 0.89188 |  0:00:07s
epoch 16 | loss: 0.77083 | val_0_rmse: 0.96082 | val_1_rmse: 0.88774 |  0:00:07s
epoch 17 | loss: 0.7754  | val_0_rmse: 0.97044 | val_1_rmse: 0.89195 |  0:00:08s
epoch 18 | loss: 0.77697 | val_0_rmse: 0.99175 | val_1_rmse: 0.91991 |  0:00:08s
epoch 19 | loss: 0.75756 | val_0_rmse: 1.03094 | val_1_rmse: 0.93068 |  0:00:09s
epoch 20 | loss: 0.75702 | val_0_rmse: 1.04139 | val_1_rmse: 0.93912 |  0:00:09s
epoch 21 | loss: 0.7679  | val_0_rmse: 0.98602 | val_1_rmse: 0.91726 |  0:00:10s
epoch 22 | loss: 0.75561 | val_0_rmse: 0.9859  | val_1_rmse: 0.91086 |  0:00:10s
epoch 23 | loss: 0.75175 | val_0_rmse: 0.9728  | val_1_rmse: 0.89839 |  0:00:11s
epoch 24 | loss: 0.74069 | val_0_rmse: 0.96349 | val_1_rmse: 0.88843 |  0:00:11s
epoch 25 | loss: 0.72971 | val_0_rmse: 0.96262 | val_1_rmse: 0.89029 |  0:00:12s
epoch 26 | loss: 0.73306 | val_0_rmse: 0.95625 | val_1_rmse: 0.88095 |  0:00:12s
epoch 27 | loss: 0.72741 | val_0_rmse: 0.95433 | val_1_rmse: 0.87868 |  0:00:13s
epoch 28 | loss: 0.73584 | val_0_rmse: 0.97108 | val_1_rmse: 0.89326 |  0:00:13s
epoch 29 | loss: 0.74295 | val_0_rmse: 0.93508 | val_1_rmse: 0.85322 |  0:00:14s
epoch 30 | loss: 0.74555 | val_0_rmse: 0.92115 | val_1_rmse: 0.84874 |  0:00:14s
epoch 31 | loss: 0.74417 | val_0_rmse: 0.92587 | val_1_rmse: 0.84884 |  0:00:15s
epoch 32 | loss: 0.74847 | val_0_rmse: 0.93384 | val_1_rmse: 0.87171 |  0:00:15s
epoch 33 | loss: 0.74132 | val_0_rmse: 0.92899 | val_1_rmse: 0.85606 |  0:00:16s
epoch 34 | loss: 0.73481 | val_0_rmse: 0.92956 | val_1_rmse: 0.85088 |  0:00:16s
epoch 35 | loss: 0.7281  | val_0_rmse: 0.92109 | val_1_rmse: 0.84284 |  0:00:16s
epoch 36 | loss: 0.7266  | val_0_rmse: 0.92626 | val_1_rmse: 0.85023 |  0:00:17s
epoch 37 | loss: 0.72361 | val_0_rmse: 0.92528 | val_1_rmse: 0.85087 |  0:00:17s
epoch 38 | loss: 0.72735 | val_0_rmse: 0.92996 | val_1_rmse: 0.85472 |  0:00:18s
epoch 39 | loss: 0.7247  | val_0_rmse: 0.93883 | val_1_rmse: 0.8684  |  0:00:18s
epoch 40 | loss: 0.72592 | val_0_rmse: 0.95171 | val_1_rmse: 0.87909 |  0:00:19s
epoch 41 | loss: 0.74652 | val_0_rmse: 0.93552 | val_1_rmse: 0.86106 |  0:00:19s
epoch 42 | loss: 0.74341 | val_0_rmse: 0.92896 | val_1_rmse: 0.86637 |  0:00:20s
epoch 43 | loss: 0.73973 | val_0_rmse: 0.92326 | val_1_rmse: 0.86007 |  0:00:20s
epoch 44 | loss: 0.73026 | val_0_rmse: 0.93155 | val_1_rmse: 0.86098 |  0:00:21s
epoch 45 | loss: 0.73879 | val_0_rmse: 0.93362 | val_1_rmse: 0.86416 |  0:00:21s
epoch 46 | loss: 0.72676 | val_0_rmse: 0.94358 | val_1_rmse: 0.87354 |  0:00:21s
epoch 47 | loss: 0.72272 | val_0_rmse: 0.95053 | val_1_rmse: 0.87563 |  0:00:22s
epoch 48 | loss: 0.73129 | val_0_rmse: 0.94901 | val_1_rmse: 0.87349 |  0:00:22s
epoch 49 | loss: 0.72578 | val_0_rmse: 0.92853 | val_1_rmse: 0.85344 |  0:00:23s
epoch 50 | loss: 0.72126 | val_0_rmse: 0.93501 | val_1_rmse: 0.86408 |  0:00:23s
epoch 51 | loss: 0.71142 | val_0_rmse: 0.91616 | val_1_rmse: 0.8531  |  0:00:24s
epoch 52 | loss: 0.72034 | val_0_rmse: 0.90733 | val_1_rmse: 0.83922 |  0:00:24s
epoch 53 | loss: 0.70748 | val_0_rmse: 0.9169  | val_1_rmse: 0.85001 |  0:00:25s
epoch 54 | loss: 0.72033 | val_0_rmse: 0.90878 | val_1_rmse: 0.83489 |  0:00:25s
epoch 55 | loss: 0.70988 | val_0_rmse: 0.89386 | val_1_rmse: 0.82799 |  0:00:26s
epoch 56 | loss: 0.71358 | val_0_rmse: 0.89724 | val_1_rmse: 0.82519 |  0:00:26s
epoch 57 | loss: 0.71285 | val_0_rmse: 0.90372 | val_1_rmse: 0.82907 |  0:00:27s
epoch 58 | loss: 0.70695 | val_0_rmse: 0.89274 | val_1_rmse: 0.82649 |  0:00:27s
epoch 59 | loss: 0.70265 | val_0_rmse: 0.89948 | val_1_rmse: 0.83348 |  0:00:28s
epoch 60 | loss: 0.71573 | val_0_rmse: 0.9126  | val_1_rmse: 0.84941 |  0:00:28s
epoch 61 | loss: 0.70856 | val_0_rmse: 0.88768 | val_1_rmse: 0.82535 |  0:00:29s
epoch 62 | loss: 0.71552 | val_0_rmse: 0.9123  | val_1_rmse: 0.84367 |  0:00:29s
epoch 63 | loss: 0.73747 | val_0_rmse: 0.9062  | val_1_rmse: 0.85245 |  0:00:29s
epoch 64 | loss: 0.73381 | val_0_rmse: 0.89486 | val_1_rmse: 0.83637 |  0:00:30s
epoch 65 | loss: 0.73075 | val_0_rmse: 0.89757 | val_1_rmse: 0.82902 |  0:00:30s
epoch 66 | loss: 0.72889 | val_0_rmse: 0.89459 | val_1_rmse: 0.83401 |  0:00:31s
epoch 67 | loss: 0.73106 | val_0_rmse: 0.91199 | val_1_rmse: 0.83977 |  0:00:31s
epoch 68 | loss: 0.71921 | val_0_rmse: 0.9018  | val_1_rmse: 0.83631 |  0:00:32s
epoch 69 | loss: 0.73087 | val_0_rmse: 0.89807 | val_1_rmse: 0.82202 |  0:00:32s
epoch 70 | loss: 0.73149 | val_0_rmse: 0.89369 | val_1_rmse: 0.81741 |  0:00:33s
epoch 71 | loss: 0.75302 | val_0_rmse: 0.92583 | val_1_rmse: 0.84902 |  0:00:33s
epoch 72 | loss: 0.75093 | val_0_rmse: 0.89414 | val_1_rmse: 0.82216 |  0:00:34s
epoch 73 | loss: 0.73689 | val_0_rmse: 0.89827 | val_1_rmse: 0.83852 |  0:00:34s
epoch 74 | loss: 0.7631  | val_0_rmse: 0.89385 | val_1_rmse: 0.82229 |  0:00:35s
epoch 75 | loss: 0.75653 | val_0_rmse: 0.89173 | val_1_rmse: 0.81534 |  0:00:35s
epoch 76 | loss: 0.72304 | val_0_rmse: 0.88187 | val_1_rmse: 0.80514 |  0:00:35s
epoch 77 | loss: 0.72631 | val_0_rmse: 0.88801 | val_1_rmse: 0.80846 |  0:00:36s
epoch 78 | loss: 0.72159 | val_0_rmse: 0.88834 | val_1_rmse: 0.81063 |  0:00:36s
epoch 79 | loss: 0.71829 | val_0_rmse: 0.88129 | val_1_rmse: 0.80364 |  0:00:37s
epoch 80 | loss: 0.71165 | val_0_rmse: 0.90582 | val_1_rmse: 0.82581 |  0:00:37s
epoch 81 | loss: 0.70631 | val_0_rmse: 0.90086 | val_1_rmse: 0.82615 |  0:00:38s
epoch 82 | loss: 0.70656 | val_0_rmse: 0.89779 | val_1_rmse: 0.82126 |  0:00:38s
epoch 83 | loss: 0.7142  | val_0_rmse: 0.91099 | val_1_rmse: 0.82962 |  0:00:39s
epoch 84 | loss: 0.70176 | val_0_rmse: 0.88991 | val_1_rmse: 0.81333 |  0:00:39s
epoch 85 | loss: 0.69188 | val_0_rmse: 0.88932 | val_1_rmse: 0.80787 |  0:00:40s
epoch 86 | loss: 0.68668 | val_0_rmse: 0.90637 | val_1_rmse: 0.82372 |  0:00:40s
epoch 87 | loss: 0.69173 | val_0_rmse: 0.88397 | val_1_rmse: 0.80735 |  0:00:41s
epoch 88 | loss: 0.69732 | val_0_rmse: 0.88452 | val_1_rmse: 0.80583 |  0:00:41s
epoch 89 | loss: 0.68998 | val_0_rmse: 0.88244 | val_1_rmse: 0.8012  |  0:00:42s
epoch 90 | loss: 0.69113 | val_0_rmse: 0.87869 | val_1_rmse: 0.79761 |  0:00:42s
epoch 91 | loss: 0.68263 | val_0_rmse: 0.87484 | val_1_rmse: 0.80103 |  0:00:43s
epoch 92 | loss: 0.71099 | val_0_rmse: 0.87892 | val_1_rmse: 0.80059 |  0:00:43s
epoch 93 | loss: 0.73612 | val_0_rmse: 0.89069 | val_1_rmse: 0.812   |  0:00:43s
epoch 94 | loss: 0.72059 | val_0_rmse: 0.8822  | val_1_rmse: 0.81488 |  0:00:44s
epoch 95 | loss: 0.70787 | val_0_rmse: 0.8698  | val_1_rmse: 0.79786 |  0:00:44s
epoch 96 | loss: 0.69064 | val_0_rmse: 0.87001 | val_1_rmse: 0.79552 |  0:00:45s
epoch 97 | loss: 0.70071 | val_0_rmse: 0.8791  | val_1_rmse: 0.80362 |  0:00:45s
epoch 98 | loss: 0.69607 | val_0_rmse: 0.87106 | val_1_rmse: 0.8023  |  0:00:46s
epoch 99 | loss: 0.68421 | val_0_rmse: 0.86615 | val_1_rmse: 0.79583 |  0:00:46s
epoch 100| loss: 0.70061 | val_0_rmse: 0.87442 | val_1_rmse: 0.80372 |  0:00:47s
epoch 101| loss: 0.69218 | val_0_rmse: 0.86739 | val_1_rmse: 0.79394 |  0:00:47s
epoch 102| loss: 0.68599 | val_0_rmse: 0.87413 | val_1_rmse: 0.80083 |  0:00:48s
epoch 103| loss: 0.69985 | val_0_rmse: 0.87431 | val_1_rmse: 0.79482 |  0:00:48s
epoch 104| loss: 0.68428 | val_0_rmse: 0.85233 | val_1_rmse: 0.78028 |  0:00:48s
epoch 105| loss: 0.68087 | val_0_rmse: 0.86721 | val_1_rmse: 0.7878  |  0:00:49s
epoch 106| loss: 0.67642 | val_0_rmse: 0.83756 | val_1_rmse: 0.76958 |  0:00:49s
epoch 107| loss: 0.68163 | val_0_rmse: 0.84079 | val_1_rmse: 0.77215 |  0:00:50s
epoch 108| loss: 0.69043 | val_0_rmse: 0.8712  | val_1_rmse: 0.79361 |  0:00:50s
epoch 109| loss: 0.68322 | val_0_rmse: 0.85685 | val_1_rmse: 0.78779 |  0:00:51s
epoch 110| loss: 0.68356 | val_0_rmse: 0.85592 | val_1_rmse: 0.78835 |  0:00:51s
epoch 111| loss: 0.68001 | val_0_rmse: 0.86464 | val_1_rmse: 0.79233 |  0:00:52s
epoch 112| loss: 0.67958 | val_0_rmse: 0.84869 | val_1_rmse: 0.78328 |  0:00:52s
epoch 113| loss: 0.69057 | val_0_rmse: 0.84106 | val_1_rmse: 0.77264 |  0:00:53s
epoch 114| loss: 0.67104 | val_0_rmse: 0.83092 | val_1_rmse: 0.76757 |  0:00:53s
epoch 115| loss: 0.68271 | val_0_rmse: 0.83908 | val_1_rmse: 0.76896 |  0:00:54s
epoch 116| loss: 0.68215 | val_0_rmse: 0.83112 | val_1_rmse: 0.76609 |  0:00:54s
epoch 117| loss: 0.67452 | val_0_rmse: 0.83102 | val_1_rmse: 0.76851 |  0:00:55s
epoch 118| loss: 0.67686 | val_0_rmse: 0.83286 | val_1_rmse: 0.77173 |  0:00:55s
epoch 119| loss: 0.68074 | val_0_rmse: 0.84619 | val_1_rmse: 0.77698 |  0:00:55s
epoch 120| loss: 0.6822  | val_0_rmse: 0.84645 | val_1_rmse: 0.77855 |  0:00:56s
epoch 121| loss: 0.68186 | val_0_rmse: 0.84777 | val_1_rmse: 0.78209 |  0:00:56s
epoch 122| loss: 0.67685 | val_0_rmse: 0.83747 | val_1_rmse: 0.77484 |  0:00:57s
epoch 123| loss: 0.6747  | val_0_rmse: 0.82856 | val_1_rmse: 0.76824 |  0:00:57s
epoch 124| loss: 0.66925 | val_0_rmse: 0.83313 | val_1_rmse: 0.76806 |  0:00:58s
epoch 125| loss: 0.67907 | val_0_rmse: 0.82136 | val_1_rmse: 0.76092 |  0:00:58s
epoch 126| loss: 0.67434 | val_0_rmse: 0.83005 | val_1_rmse: 0.76726 |  0:00:59s
epoch 127| loss: 0.66925 | val_0_rmse: 0.83442 | val_1_rmse: 0.77155 |  0:00:59s
epoch 128| loss: 0.67038 | val_0_rmse: 0.85168 | val_1_rmse: 0.78471 |  0:01:00s
epoch 129| loss: 0.68644 | val_0_rmse: 0.86309 | val_1_rmse: 0.7968  |  0:01:00s
epoch 130| loss: 0.69429 | val_0_rmse: 0.85974 | val_1_rmse: 0.80021 |  0:01:01s
epoch 131| loss: 0.68751 | val_0_rmse: 0.84887 | val_1_rmse: 0.797   |  0:01:01s
epoch 132| loss: 0.69108 | val_0_rmse: 0.8697  | val_1_rmse: 0.806   |  0:01:01s
epoch 133| loss: 0.68415 | val_0_rmse: 0.88158 | val_1_rmse: 0.81531 |  0:01:02s
epoch 134| loss: 0.69017 | val_0_rmse: 0.86798 | val_1_rmse: 0.79729 |  0:01:02s
epoch 135| loss: 0.69026 | val_0_rmse: 0.83286 | val_1_rmse: 0.76749 |  0:01:03s
epoch 136| loss: 0.68645 | val_0_rmse: 0.83878 | val_1_rmse: 0.77993 |  0:01:03s
epoch 137| loss: 0.69013 | val_0_rmse: 0.83673 | val_1_rmse: 0.76897 |  0:01:04s
epoch 138| loss: 0.70182 | val_0_rmse: 0.84025 | val_1_rmse: 0.77307 |  0:01:04s
epoch 139| loss: 0.68532 | val_0_rmse: 0.85597 | val_1_rmse: 0.79126 |  0:01:05s
epoch 140| loss: 0.69035 | val_0_rmse: 0.85613 | val_1_rmse: 0.79408 |  0:01:05s
epoch 141| loss: 0.68336 | val_0_rmse: 0.85708 | val_1_rmse: 0.79658 |  0:01:06s
epoch 142| loss: 0.68864 | val_0_rmse: 0.85637 | val_1_rmse: 0.78952 |  0:01:06s
epoch 143| loss: 0.69319 | val_0_rmse: 0.85068 | val_1_rmse: 0.78977 |  0:01:07s
epoch 144| loss: 0.6784  | val_0_rmse: 0.86482 | val_1_rmse: 0.80225 |  0:01:07s
epoch 145| loss: 0.68698 | val_0_rmse: 0.8452  | val_1_rmse: 0.78023 |  0:01:08s
epoch 146| loss: 0.68422 | val_0_rmse: 0.85557 | val_1_rmse: 0.78928 |  0:01:08s
epoch 147| loss: 0.68423 | val_0_rmse: 0.84111 | val_1_rmse: 0.78379 |  0:01:09s
epoch 148| loss: 0.68244 | val_0_rmse: 0.83512 | val_1_rmse: 0.7739  |  0:01:09s
epoch 149| loss: 0.69116 | val_0_rmse: 0.84745 | val_1_rmse: 0.78333 |  0:01:10s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.76092
Best weights from best epoch are automatically used!
ended training at: 06:52:53
Feature importance:
Mean squared error is of 5672631334.450394
Mean absolute error:54681.95752555131
MAPE:0.45406227641097296
R2 score:0.3061991343757643
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:52:53
epoch 0  | loss: 1.73172 | val_0_rmse: 0.99763 | val_1_rmse: 1.05104 |  0:00:00s
epoch 1  | loss: 1.20371 | val_0_rmse: 0.98626 | val_1_rmse: 1.04164 |  0:00:00s
epoch 2  | loss: 1.01413 | val_0_rmse: 0.99092 | val_1_rmse: 1.04686 |  0:00:01s
epoch 3  | loss: 0.94973 | val_0_rmse: 0.98843 | val_1_rmse: 1.04531 |  0:00:01s
epoch 4  | loss: 0.91492 | val_0_rmse: 0.98961 | val_1_rmse: 1.04483 |  0:00:02s
epoch 5  | loss: 0.87082 | val_0_rmse: 0.98226 | val_1_rmse: 1.0367  |  0:00:02s
epoch 6  | loss: 0.84058 | val_0_rmse: 0.99882 | val_1_rmse: 1.04867 |  0:00:03s
epoch 7  | loss: 0.81569 | val_0_rmse: 1.01112 | val_1_rmse: 1.06065 |  0:00:03s
epoch 8  | loss: 0.80405 | val_0_rmse: 1.00757 | val_1_rmse: 1.05732 |  0:00:04s
epoch 9  | loss: 0.80291 | val_0_rmse: 0.97693 | val_1_rmse: 1.03244 |  0:00:04s
epoch 10 | loss: 0.77917 | val_0_rmse: 0.95258 | val_1_rmse: 1.00698 |  0:00:05s
epoch 11 | loss: 0.78358 | val_0_rmse: 0.94392 | val_1_rmse: 1.00428 |  0:00:05s
epoch 12 | loss: 0.76915 | val_0_rmse: 0.95257 | val_1_rmse: 1.01107 |  0:00:06s
epoch 13 | loss: 0.75479 | val_0_rmse: 0.94367 | val_1_rmse: 1.003   |  0:00:06s
epoch 14 | loss: 0.76122 | val_0_rmse: 0.95963 | val_1_rmse: 1.0166  |  0:00:07s
epoch 15 | loss: 0.75577 | val_0_rmse: 0.94841 | val_1_rmse: 1.01093 |  0:00:07s
epoch 16 | loss: 0.75106 | val_0_rmse: 0.95689 | val_1_rmse: 1.02057 |  0:00:08s
epoch 17 | loss: 0.75034 | val_0_rmse: 0.95273 | val_1_rmse: 1.01966 |  0:00:08s
epoch 18 | loss: 0.7561  | val_0_rmse: 0.94892 | val_1_rmse: 1.01061 |  0:00:09s
epoch 19 | loss: 0.73932 | val_0_rmse: 0.9474  | val_1_rmse: 1.00773 |  0:00:09s
epoch 20 | loss: 0.73685 | val_0_rmse: 0.93564 | val_1_rmse: 1.00001 |  0:00:09s
epoch 21 | loss: 0.73311 | val_0_rmse: 0.94844 | val_1_rmse: 1.0091  |  0:00:10s
epoch 22 | loss: 0.73712 | val_0_rmse: 0.94323 | val_1_rmse: 1.00261 |  0:00:10s
epoch 23 | loss: 0.72084 | val_0_rmse: 0.92185 | val_1_rmse: 0.98153 |  0:00:11s
epoch 24 | loss: 0.71821 | val_0_rmse: 0.93176 | val_1_rmse: 0.99239 |  0:00:11s
epoch 25 | loss: 0.70929 | val_0_rmse: 0.92809 | val_1_rmse: 0.98682 |  0:00:12s
epoch 26 | loss: 0.7035  | val_0_rmse: 0.91333 | val_1_rmse: 0.97376 |  0:00:12s
epoch 27 | loss: 0.70013 | val_0_rmse: 0.92785 | val_1_rmse: 0.98694 |  0:00:13s
epoch 28 | loss: 0.7093  | val_0_rmse: 0.91779 | val_1_rmse: 0.97606 |  0:00:13s
epoch 29 | loss: 0.68874 | val_0_rmse: 0.89653 | val_1_rmse: 0.95206 |  0:00:14s
epoch 30 | loss: 0.69136 | val_0_rmse: 0.89728 | val_1_rmse: 0.95265 |  0:00:14s
epoch 31 | loss: 0.68853 | val_0_rmse: 0.90563 | val_1_rmse: 0.96198 |  0:00:15s
epoch 32 | loss: 0.69025 | val_0_rmse: 0.89797 | val_1_rmse: 0.95384 |  0:00:15s
epoch 33 | loss: 0.68898 | val_0_rmse: 0.88751 | val_1_rmse: 0.94576 |  0:00:15s
epoch 34 | loss: 0.68524 | val_0_rmse: 0.88105 | val_1_rmse: 0.9367  |  0:00:16s
epoch 35 | loss: 0.67894 | val_0_rmse: 0.89348 | val_1_rmse: 0.95388 |  0:00:16s
epoch 36 | loss: 0.68185 | val_0_rmse: 0.90316 | val_1_rmse: 0.96759 |  0:00:17s
epoch 37 | loss: 0.67867 | val_0_rmse: 0.89606 | val_1_rmse: 0.9626  |  0:00:17s
epoch 38 | loss: 0.67142 | val_0_rmse: 0.91238 | val_1_rmse: 0.97601 |  0:00:18s
epoch 39 | loss: 0.67895 | val_0_rmse: 0.90964 | val_1_rmse: 0.9666  |  0:00:18s
epoch 40 | loss: 0.67975 | val_0_rmse: 0.90923 | val_1_rmse: 0.96691 |  0:00:19s
epoch 41 | loss: 0.6785  | val_0_rmse: 0.89972 | val_1_rmse: 0.96118 |  0:00:19s
epoch 42 | loss: 0.67358 | val_0_rmse: 0.89213 | val_1_rmse: 0.95127 |  0:00:20s
epoch 43 | loss: 0.67102 | val_0_rmse: 0.88807 | val_1_rmse: 0.94302 |  0:00:20s
epoch 44 | loss: 0.66677 | val_0_rmse: 0.88188 | val_1_rmse: 0.9386  |  0:00:21s
epoch 45 | loss: 0.66998 | val_0_rmse: 0.88727 | val_1_rmse: 0.94651 |  0:00:21s
epoch 46 | loss: 0.66218 | val_0_rmse: 0.88586 | val_1_rmse: 0.94558 |  0:00:22s
epoch 47 | loss: 0.66189 | val_0_rmse: 0.91127 | val_1_rmse: 0.96996 |  0:00:22s
epoch 48 | loss: 0.66808 | val_0_rmse: 0.90824 | val_1_rmse: 0.96619 |  0:00:23s
epoch 49 | loss: 0.6608  | val_0_rmse: 0.88357 | val_1_rmse: 0.93884 |  0:00:23s
epoch 50 | loss: 0.65496 | val_0_rmse: 0.88224 | val_1_rmse: 0.94141 |  0:00:24s
epoch 51 | loss: 0.6647  | val_0_rmse: 0.89746 | val_1_rmse: 0.9555  |  0:00:24s
epoch 52 | loss: 0.6613  | val_0_rmse: 0.87113 | val_1_rmse: 0.92194 |  0:00:24s
epoch 53 | loss: 0.65925 | val_0_rmse: 0.86261 | val_1_rmse: 0.9164  |  0:00:25s
epoch 54 | loss: 0.6586  | val_0_rmse: 0.87899 | val_1_rmse: 0.93586 |  0:00:25s
epoch 55 | loss: 0.64929 | val_0_rmse: 0.88642 | val_1_rmse: 0.94309 |  0:00:26s
epoch 56 | loss: 0.65389 | val_0_rmse: 0.88886 | val_1_rmse: 0.94833 |  0:00:26s
epoch 57 | loss: 0.65588 | val_0_rmse: 0.87646 | val_1_rmse: 0.93454 |  0:00:27s
epoch 58 | loss: 0.65858 | val_0_rmse: 0.8939  | val_1_rmse: 0.95431 |  0:00:27s
epoch 59 | loss: 0.65308 | val_0_rmse: 0.9069  | val_1_rmse: 0.97423 |  0:00:28s
epoch 60 | loss: 0.65068 | val_0_rmse: 0.88173 | val_1_rmse: 0.94204 |  0:00:28s
epoch 61 | loss: 0.6469  | val_0_rmse: 0.88821 | val_1_rmse: 0.93993 |  0:00:29s
epoch 62 | loss: 0.64145 | val_0_rmse: 0.87344 | val_1_rmse: 0.92294 |  0:00:29s
epoch 63 | loss: 0.64977 | val_0_rmse: 0.8683  | val_1_rmse: 0.90995 |  0:00:30s
epoch 64 | loss: 0.63835 | val_0_rmse: 0.87565 | val_1_rmse: 0.91577 |  0:00:30s
epoch 65 | loss: 0.65284 | val_0_rmse: 0.86495 | val_1_rmse: 0.90803 |  0:00:31s
epoch 66 | loss: 0.64522 | val_0_rmse: 0.86725 | val_1_rmse: 0.91471 |  0:00:31s
epoch 67 | loss: 0.64729 | val_0_rmse: 0.87751 | val_1_rmse: 0.92315 |  0:00:32s
epoch 68 | loss: 0.63618 | val_0_rmse: 0.84065 | val_1_rmse: 0.88316 |  0:00:32s
epoch 69 | loss: 0.6371  | val_0_rmse: 0.85482 | val_1_rmse: 0.89869 |  0:00:32s
epoch 70 | loss: 0.64287 | val_0_rmse: 0.87692 | val_1_rmse: 0.92262 |  0:00:33s
epoch 71 | loss: 0.65052 | val_0_rmse: 0.85825 | val_1_rmse: 0.89997 |  0:00:33s
epoch 72 | loss: 0.64953 | val_0_rmse: 0.88269 | val_1_rmse: 0.93236 |  0:00:34s
epoch 73 | loss: 0.66947 | val_0_rmse: 0.88603 | val_1_rmse: 0.93103 |  0:00:34s
epoch 74 | loss: 0.6649  | val_0_rmse: 0.87933 | val_1_rmse: 0.92984 |  0:00:35s
epoch 75 | loss: 0.66262 | val_0_rmse: 0.87114 | val_1_rmse: 0.90914 |  0:00:35s
epoch 76 | loss: 0.65385 | val_0_rmse: 0.87957 | val_1_rmse: 0.91243 |  0:00:36s
epoch 77 | loss: 0.6647  | val_0_rmse: 0.87122 | val_1_rmse: 0.90653 |  0:00:36s
epoch 78 | loss: 0.65597 | val_0_rmse: 0.84557 | val_1_rmse: 0.88737 |  0:00:37s
epoch 79 | loss: 0.65316 | val_0_rmse: 0.85381 | val_1_rmse: 0.89998 |  0:00:37s
epoch 80 | loss: 0.65241 | val_0_rmse: 0.85352 | val_1_rmse: 0.90213 |  0:00:38s
epoch 81 | loss: 0.65102 | val_0_rmse: 0.85768 | val_1_rmse: 0.90861 |  0:00:38s
epoch 82 | loss: 0.65201 | val_0_rmse: 0.84701 | val_1_rmse: 0.8971  |  0:00:39s
epoch 83 | loss: 0.65177 | val_0_rmse: 0.83561 | val_1_rmse: 0.88595 |  0:00:39s
epoch 84 | loss: 0.64508 | val_0_rmse: 0.82213 | val_1_rmse: 0.85806 |  0:00:39s
epoch 85 | loss: 0.64959 | val_0_rmse: 0.83194 | val_1_rmse: 0.87149 |  0:00:40s
epoch 86 | loss: 0.65048 | val_0_rmse: 0.81746 | val_1_rmse: 0.85853 |  0:00:40s
epoch 87 | loss: 0.64917 | val_0_rmse: 0.8358  | val_1_rmse: 0.88073 |  0:00:41s
epoch 88 | loss: 0.64679 | val_0_rmse: 0.85923 | val_1_rmse: 0.89942 |  0:00:41s
epoch 89 | loss: 0.64732 | val_0_rmse: 0.84384 | val_1_rmse: 0.87979 |  0:00:42s
epoch 90 | loss: 0.64292 | val_0_rmse: 0.83714 | val_1_rmse: 0.87176 |  0:00:42s
epoch 91 | loss: 0.64421 | val_0_rmse: 0.87891 | val_1_rmse: 0.92076 |  0:00:43s
epoch 92 | loss: 0.64062 | val_0_rmse: 0.86583 | val_1_rmse: 0.90661 |  0:00:43s
epoch 93 | loss: 0.64223 | val_0_rmse: 0.8793  | val_1_rmse: 0.91682 |  0:00:44s
epoch 94 | loss: 0.64467 | val_0_rmse: 0.88389 | val_1_rmse: 0.91865 |  0:00:44s
epoch 95 | loss: 0.64584 | val_0_rmse: 0.86282 | val_1_rmse: 0.90442 |  0:00:45s
epoch 96 | loss: 0.64357 | val_0_rmse: 0.86054 | val_1_rmse: 0.90406 |  0:00:45s
epoch 97 | loss: 0.66176 | val_0_rmse: 0.86452 | val_1_rmse: 0.89912 |  0:00:46s
epoch 98 | loss: 0.64298 | val_0_rmse: 0.86415 | val_1_rmse: 0.89862 |  0:00:46s
epoch 99 | loss: 0.64866 | val_0_rmse: 0.87207 | val_1_rmse: 0.91336 |  0:00:46s
epoch 100| loss: 0.64892 | val_0_rmse: 0.85764 | val_1_rmse: 0.89756 |  0:00:47s
epoch 101| loss: 0.64087 | val_0_rmse: 0.87113 | val_1_rmse: 0.90863 |  0:00:47s
epoch 102| loss: 0.63991 | val_0_rmse: 0.887   | val_1_rmse: 0.92379 |  0:00:48s
epoch 103| loss: 0.62914 | val_0_rmse: 0.86378 | val_1_rmse: 0.904   |  0:00:48s
epoch 104| loss: 0.63276 | val_0_rmse: 0.7988  | val_1_rmse: 0.83965 |  0:00:49s
epoch 105| loss: 0.63654 | val_0_rmse: 0.81334 | val_1_rmse: 0.85891 |  0:00:49s
epoch 106| loss: 0.63969 | val_0_rmse: 0.82712 | val_1_rmse: 0.86787 |  0:00:50s
epoch 107| loss: 0.63281 | val_0_rmse: 0.86068 | val_1_rmse: 0.89581 |  0:00:50s
epoch 108| loss: 0.64448 | val_0_rmse: 0.87789 | val_1_rmse: 0.9153  |  0:00:51s
epoch 109| loss: 0.63436 | val_0_rmse: 0.91901 | val_1_rmse: 0.95524 |  0:00:51s
epoch 110| loss: 0.63589 | val_0_rmse: 0.89739 | val_1_rmse: 0.93958 |  0:00:52s
epoch 111| loss: 0.62488 | val_0_rmse: 0.88382 | val_1_rmse: 0.92616 |  0:00:52s
epoch 112| loss: 0.62048 | val_0_rmse: 0.89893 | val_1_rmse: 0.93733 |  0:00:53s
epoch 113| loss: 0.6329  | val_0_rmse: 0.84751 | val_1_rmse: 0.89401 |  0:00:53s
epoch 114| loss: 0.62696 | val_0_rmse: 0.79227 | val_1_rmse: 0.83551 |  0:00:53s
epoch 115| loss: 0.62676 | val_0_rmse: 0.79979 | val_1_rmse: 0.83904 |  0:00:54s
epoch 116| loss: 0.62556 | val_0_rmse: 0.81224 | val_1_rmse: 0.8552  |  0:00:54s
epoch 117| loss: 0.63063 | val_0_rmse: 0.82958 | val_1_rmse: 0.88361 |  0:00:55s
epoch 118| loss: 0.64569 | val_0_rmse: 0.84847 | val_1_rmse: 0.89469 |  0:00:55s
epoch 119| loss: 0.64046 | val_0_rmse: 0.85658 | val_1_rmse: 0.88955 |  0:00:56s
epoch 120| loss: 0.63627 | val_0_rmse: 0.87381 | val_1_rmse: 0.91938 |  0:00:56s
epoch 121| loss: 0.63265 | val_0_rmse: 0.8236  | val_1_rmse: 0.86881 |  0:00:57s
epoch 122| loss: 0.63135 | val_0_rmse: 0.82757 | val_1_rmse: 0.86751 |  0:00:57s
epoch 123| loss: 0.63764 | val_0_rmse: 0.85713 | val_1_rmse: 0.89511 |  0:00:58s
epoch 124| loss: 0.62707 | val_0_rmse: 0.81142 | val_1_rmse: 0.85968 |  0:00:58s
epoch 125| loss: 0.62409 | val_0_rmse: 0.86024 | val_1_rmse: 0.90657 |  0:00:59s
epoch 126| loss: 0.62872 | val_0_rmse: 0.87793 | val_1_rmse: 0.92595 |  0:00:59s
epoch 127| loss: 0.62455 | val_0_rmse: 0.8827  | val_1_rmse: 0.93093 |  0:01:00s
epoch 128| loss: 0.63571 | val_0_rmse: 0.80168 | val_1_rmse: 0.84425 |  0:01:00s
epoch 129| loss: 0.62932 | val_0_rmse: 0.8317  | val_1_rmse: 0.8763  |  0:01:01s
epoch 130| loss: 0.62937 | val_0_rmse: 0.86974 | val_1_rmse: 0.91023 |  0:01:01s
epoch 131| loss: 0.62001 | val_0_rmse: 0.87998 | val_1_rmse: 0.91884 |  0:01:02s
epoch 132| loss: 0.62154 | val_0_rmse: 0.83116 | val_1_rmse: 0.87365 |  0:01:02s
epoch 133| loss: 0.62058 | val_0_rmse: 0.81716 | val_1_rmse: 0.86586 |  0:01:02s
epoch 134| loss: 0.63322 | val_0_rmse: 0.82199 | val_1_rmse: 0.86996 |  0:01:03s
epoch 135| loss: 0.62812 | val_0_rmse: 0.80093 | val_1_rmse: 0.84477 |  0:01:03s
epoch 136| loss: 0.61986 | val_0_rmse: 0.78956 | val_1_rmse: 0.83792 |  0:01:04s
epoch 137| loss: 0.62604 | val_0_rmse: 0.81847 | val_1_rmse: 0.86774 |  0:01:04s
epoch 138| loss: 0.62838 | val_0_rmse: 0.86028 | val_1_rmse: 0.89611 |  0:01:05s
epoch 139| loss: 0.63743 | val_0_rmse: 0.8609  | val_1_rmse: 0.90051 |  0:01:05s
epoch 140| loss: 0.61373 | val_0_rmse: 0.82075 | val_1_rmse: 0.868   |  0:01:06s
epoch 141| loss: 0.62651 | val_0_rmse: 0.81474 | val_1_rmse: 0.85951 |  0:01:06s
epoch 142| loss: 0.6288  | val_0_rmse: 0.82394 | val_1_rmse: 0.86646 |  0:01:07s
epoch 143| loss: 0.62999 | val_0_rmse: 0.81797 | val_1_rmse: 0.86416 |  0:01:07s
epoch 144| loss: 0.61979 | val_0_rmse: 0.78479 | val_1_rmse: 0.82444 |  0:01:08s
epoch 145| loss: 0.62685 | val_0_rmse: 0.80965 | val_1_rmse: 0.84035 |  0:01:08s
epoch 146| loss: 0.62935 | val_0_rmse: 0.8213  | val_1_rmse: 0.85364 |  0:01:09s
epoch 147| loss: 0.6293  | val_0_rmse: 0.79467 | val_1_rmse: 0.83394 |  0:01:09s
epoch 148| loss: 0.61995 | val_0_rmse: 0.79169 | val_1_rmse: 0.8362  |  0:01:09s
epoch 149| loss: 0.63066 | val_0_rmse: 0.80343 | val_1_rmse: 0.84547 |  0:01:10s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.82444
Best weights from best epoch are automatically used!
ended training at: 06:54:04
Feature importance:
Mean squared error is of 5941747304.178838
Mean absolute error:54883.59681218193
MAPE:0.44532148674120065
R2 score:0.26964462384925125
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:54:04
epoch 0  | loss: 1.74329 | val_0_rmse: 1.00148 | val_1_rmse: 0.98098 |  0:00:00s
epoch 1  | loss: 1.28369 | val_0_rmse: 0.99723 | val_1_rmse: 0.97427 |  0:00:00s
epoch 2  | loss: 1.09004 | val_0_rmse: 1.00138 | val_1_rmse: 0.97395 |  0:00:01s
epoch 3  | loss: 1.00097 | val_0_rmse: 0.99975 | val_1_rmse: 0.97912 |  0:00:01s
epoch 4  | loss: 0.94809 | val_0_rmse: 0.99227 | val_1_rmse: 0.97081 |  0:00:02s
epoch 5  | loss: 0.92775 | val_0_rmse: 0.98886 | val_1_rmse: 0.96805 |  0:00:02s
epoch 6  | loss: 0.88759 | val_0_rmse: 0.99257 | val_1_rmse: 0.96486 |  0:00:03s
epoch 7  | loss: 0.82378 | val_0_rmse: 0.9899  | val_1_rmse: 0.95756 |  0:00:03s
epoch 8  | loss: 0.81814 | val_0_rmse: 0.99021 | val_1_rmse: 0.95816 |  0:00:04s
epoch 9  | loss: 0.78009 | val_0_rmse: 0.98603 | val_1_rmse: 0.95533 |  0:00:04s
epoch 10 | loss: 0.76636 | val_0_rmse: 0.98464 | val_1_rmse: 0.9541  |  0:00:05s
epoch 11 | loss: 0.75201 | val_0_rmse: 0.96675 | val_1_rmse: 0.93956 |  0:00:05s
epoch 12 | loss: 0.72827 | val_0_rmse: 0.95494 | val_1_rmse: 0.92623 |  0:00:06s
epoch 13 | loss: 0.73323 | val_0_rmse: 0.94956 | val_1_rmse: 0.91726 |  0:00:06s
epoch 14 | loss: 0.74325 | val_0_rmse: 0.94834 | val_1_rmse: 0.91508 |  0:00:07s
epoch 15 | loss: 0.73061 | val_0_rmse: 0.94255 | val_1_rmse: 0.90911 |  0:00:07s
epoch 16 | loss: 0.73092 | val_0_rmse: 0.94531 | val_1_rmse: 0.90869 |  0:00:08s
epoch 17 | loss: 0.72608 | val_0_rmse: 0.94651 | val_1_rmse: 0.91179 |  0:00:08s
epoch 18 | loss: 0.73101 | val_0_rmse: 0.95149 | val_1_rmse: 0.91444 |  0:00:09s
epoch 19 | loss: 0.72453 | val_0_rmse: 0.95423 | val_1_rmse: 0.91668 |  0:00:09s
epoch 20 | loss: 0.72073 | val_0_rmse: 0.94802 | val_1_rmse: 0.90977 |  0:00:10s
epoch 21 | loss: 0.71495 | val_0_rmse: 0.93734 | val_1_rmse: 0.90139 |  0:00:10s
epoch 22 | loss: 0.7124  | val_0_rmse: 0.93471 | val_1_rmse: 0.90006 |  0:00:11s
epoch 23 | loss: 0.71281 | val_0_rmse: 0.92812 | val_1_rmse: 0.89416 |  0:00:11s
epoch 24 | loss: 0.70526 | val_0_rmse: 0.92118 | val_1_rmse: 0.88771 |  0:00:11s
epoch 25 | loss: 0.70124 | val_0_rmse: 0.92279 | val_1_rmse: 0.89271 |  0:00:12s
epoch 26 | loss: 0.70311 | val_0_rmse: 0.91927 | val_1_rmse: 0.89643 |  0:00:12s
epoch 27 | loss: 0.70646 | val_0_rmse: 0.91038 | val_1_rmse: 0.88766 |  0:00:13s
epoch 28 | loss: 0.70543 | val_0_rmse: 0.8985  | val_1_rmse: 0.88262 |  0:00:13s
epoch 29 | loss: 0.7081  | val_0_rmse: 0.89374 | val_1_rmse: 0.8787  |  0:00:14s
epoch 30 | loss: 0.70257 | val_0_rmse: 0.8876  | val_1_rmse: 0.87002 |  0:00:14s
epoch 31 | loss: 0.70061 | val_0_rmse: 0.88733 | val_1_rmse: 0.87184 |  0:00:15s
epoch 32 | loss: 0.70137 | val_0_rmse: 0.89326 | val_1_rmse: 0.88177 |  0:00:15s
epoch 33 | loss: 0.69872 | val_0_rmse: 0.88927 | val_1_rmse: 0.87053 |  0:00:16s
epoch 34 | loss: 0.7011  | val_0_rmse: 0.89292 | val_1_rmse: 0.87433 |  0:00:16s
epoch 35 | loss: 0.6997  | val_0_rmse: 0.89078 | val_1_rmse: 0.87229 |  0:00:17s
epoch 36 | loss: 0.69183 | val_0_rmse: 0.88175 | val_1_rmse: 0.86476 |  0:00:17s
epoch 37 | loss: 0.69837 | val_0_rmse: 0.87753 | val_1_rmse: 0.85982 |  0:00:18s
epoch 38 | loss: 0.69417 | val_0_rmse: 0.88519 | val_1_rmse: 0.86643 |  0:00:18s
epoch 39 | loss: 0.69209 | val_0_rmse: 0.86907 | val_1_rmse: 0.85807 |  0:00:19s
epoch 40 | loss: 0.68681 | val_0_rmse: 0.8719  | val_1_rmse: 0.85413 |  0:00:19s
epoch 41 | loss: 0.68682 | val_0_rmse: 0.87314 | val_1_rmse: 0.8592  |  0:00:20s
epoch 42 | loss: 0.69177 | val_0_rmse: 0.87556 | val_1_rmse: 0.87007 |  0:00:20s
epoch 43 | loss: 0.6898  | val_0_rmse: 0.87526 | val_1_rmse: 0.86903 |  0:00:20s
epoch 44 | loss: 0.69401 | val_0_rmse: 0.87009 | val_1_rmse: 0.86301 |  0:00:21s
epoch 45 | loss: 0.6944  | val_0_rmse: 0.86441 | val_1_rmse: 0.866   |  0:00:21s
epoch 46 | loss: 0.68679 | val_0_rmse: 0.86391 | val_1_rmse: 0.86492 |  0:00:22s
epoch 47 | loss: 0.69293 | val_0_rmse: 0.85946 | val_1_rmse: 0.86344 |  0:00:22s
epoch 48 | loss: 0.6858  | val_0_rmse: 0.85874 | val_1_rmse: 0.86614 |  0:00:23s
epoch 49 | loss: 0.6876  | val_0_rmse: 0.85541 | val_1_rmse: 0.86093 |  0:00:23s
epoch 50 | loss: 0.68279 | val_0_rmse: 0.84869 | val_1_rmse: 0.84744 |  0:00:24s
epoch 51 | loss: 0.6948  | val_0_rmse: 0.85613 | val_1_rmse: 0.86094 |  0:00:24s
epoch 52 | loss: 0.68314 | val_0_rmse: 0.85706 | val_1_rmse: 0.86207 |  0:00:25s
epoch 53 | loss: 0.68413 | val_0_rmse: 0.85495 | val_1_rmse: 0.86349 |  0:00:25s
epoch 54 | loss: 0.68956 | val_0_rmse: 0.85089 | val_1_rmse: 0.8536  |  0:00:25s
epoch 55 | loss: 0.68443 | val_0_rmse: 0.85062 | val_1_rmse: 0.85757 |  0:00:26s
epoch 56 | loss: 0.68742 | val_0_rmse: 0.84597 | val_1_rmse: 0.85161 |  0:00:26s
epoch 57 | loss: 0.68416 | val_0_rmse: 0.8514  | val_1_rmse: 0.85431 |  0:00:27s
epoch 58 | loss: 0.68576 | val_0_rmse: 0.85007 | val_1_rmse: 0.85905 |  0:00:27s
epoch 59 | loss: 0.67903 | val_0_rmse: 0.84279 | val_1_rmse: 0.85336 |  0:00:28s
epoch 60 | loss: 0.6773  | val_0_rmse: 0.84617 | val_1_rmse: 0.85067 |  0:00:28s
epoch 61 | loss: 0.68169 | val_0_rmse: 0.85787 | val_1_rmse: 0.85458 |  0:00:29s
epoch 62 | loss: 0.6812  | val_0_rmse: 0.84619 | val_1_rmse: 0.84474 |  0:00:29s
epoch 63 | loss: 0.67515 | val_0_rmse: 0.84172 | val_1_rmse: 0.84923 |  0:00:30s
epoch 64 | loss: 0.67842 | val_0_rmse: 0.84261 | val_1_rmse: 0.85512 |  0:00:30s
epoch 65 | loss: 0.67816 | val_0_rmse: 0.8445  | val_1_rmse: 0.84745 |  0:00:31s
epoch 66 | loss: 0.67364 | val_0_rmse: 0.83757 | val_1_rmse: 0.84632 |  0:00:31s
epoch 67 | loss: 0.67383 | val_0_rmse: 0.83591 | val_1_rmse: 0.84505 |  0:00:32s
epoch 68 | loss: 0.67983 | val_0_rmse: 0.8369  | val_1_rmse: 0.85145 |  0:00:32s
epoch 69 | loss: 0.67183 | val_0_rmse: 0.83322 | val_1_rmse: 0.83687 |  0:00:32s
epoch 70 | loss: 0.67182 | val_0_rmse: 0.83573 | val_1_rmse: 0.84312 |  0:00:33s
epoch 71 | loss: 0.67465 | val_0_rmse: 0.84735 | val_1_rmse: 0.85995 |  0:00:33s
epoch 72 | loss: 0.67883 | val_0_rmse: 0.83839 | val_1_rmse: 0.85125 |  0:00:34s
epoch 73 | loss: 0.67232 | val_0_rmse: 0.8367  | val_1_rmse: 0.84254 |  0:00:34s
epoch 74 | loss: 0.6746  | val_0_rmse: 0.83652 | val_1_rmse: 0.84918 |  0:00:35s
epoch 75 | loss: 0.67543 | val_0_rmse: 0.84185 | val_1_rmse: 0.85529 |  0:00:35s
epoch 76 | loss: 0.67223 | val_0_rmse: 0.83847 | val_1_rmse: 0.8386  |  0:00:36s
epoch 77 | loss: 0.67381 | val_0_rmse: 0.8351  | val_1_rmse: 0.84928 |  0:00:36s
epoch 78 | loss: 0.67514 | val_0_rmse: 0.83661 | val_1_rmse: 0.84195 |  0:00:37s
epoch 79 | loss: 0.67517 | val_0_rmse: 0.83692 | val_1_rmse: 0.84057 |  0:00:37s
epoch 80 | loss: 0.67148 | val_0_rmse: 0.82918 | val_1_rmse: 0.83361 |  0:00:38s
epoch 81 | loss: 0.67003 | val_0_rmse: 0.82871 | val_1_rmse: 0.84393 |  0:00:38s
epoch 82 | loss: 0.67368 | val_0_rmse: 0.83232 | val_1_rmse: 0.84856 |  0:00:39s
epoch 83 | loss: 0.66991 | val_0_rmse: 0.83061 | val_1_rmse: 0.83227 |  0:00:39s
epoch 84 | loss: 0.67014 | val_0_rmse: 0.83168 | val_1_rmse: 0.83991 |  0:00:39s
epoch 85 | loss: 0.66464 | val_0_rmse: 0.83127 | val_1_rmse: 0.84648 |  0:00:40s
epoch 86 | loss: 0.66062 | val_0_rmse: 0.81949 | val_1_rmse: 0.83307 |  0:00:40s
epoch 87 | loss: 0.66926 | val_0_rmse: 0.82812 | val_1_rmse: 0.83374 |  0:00:41s
epoch 88 | loss: 0.6651  | val_0_rmse: 0.81572 | val_1_rmse: 0.83211 |  0:00:41s
epoch 89 | loss: 0.65547 | val_0_rmse: 0.81261 | val_1_rmse: 0.82799 |  0:00:42s
epoch 90 | loss: 0.6671  | val_0_rmse: 0.82688 | val_1_rmse: 0.83547 |  0:00:42s
epoch 91 | loss: 0.66605 | val_0_rmse: 0.82453 | val_1_rmse: 0.84411 |  0:00:43s
epoch 92 | loss: 0.65481 | val_0_rmse: 0.81205 | val_1_rmse: 0.82953 |  0:00:43s
epoch 93 | loss: 0.64993 | val_0_rmse: 0.85019 | val_1_rmse: 0.86406 |  0:00:44s
epoch 94 | loss: 0.6646  | val_0_rmse: 0.81088 | val_1_rmse: 0.82991 |  0:00:44s
epoch 95 | loss: 0.65946 | val_0_rmse: 0.81905 | val_1_rmse: 0.83402 |  0:00:45s
epoch 96 | loss: 0.64997 | val_0_rmse: 0.8229  | val_1_rmse: 0.84405 |  0:00:45s
epoch 97 | loss: 0.65175 | val_0_rmse: 0.82783 | val_1_rmse: 0.84832 |  0:00:46s
epoch 98 | loss: 0.66494 | val_0_rmse: 0.82914 | val_1_rmse: 0.85246 |  0:00:46s
epoch 99 | loss: 0.65225 | val_0_rmse: 0.817   | val_1_rmse: 0.83889 |  0:00:46s
epoch 100| loss: 0.6519  | val_0_rmse: 0.81177 | val_1_rmse: 0.82182 |  0:00:47s
epoch 101| loss: 0.65673 | val_0_rmse: 0.8257  | val_1_rmse: 0.84268 |  0:00:47s
epoch 102| loss: 0.64834 | val_0_rmse: 0.81666 | val_1_rmse: 0.84284 |  0:00:48s
epoch 103| loss: 0.64959 | val_0_rmse: 0.80813 | val_1_rmse: 0.8259  |  0:00:48s
epoch 104| loss: 0.65713 | val_0_rmse: 0.80311 | val_1_rmse: 0.82429 |  0:00:49s
epoch 105| loss: 0.65356 | val_0_rmse: 0.82153 | val_1_rmse: 0.84836 |  0:00:49s
epoch 106| loss: 0.65333 | val_0_rmse: 0.81422 | val_1_rmse: 0.83704 |  0:00:50s
epoch 107| loss: 0.64668 | val_0_rmse: 0.80395 | val_1_rmse: 0.82349 |  0:00:50s
epoch 108| loss: 0.64695 | val_0_rmse: 0.81561 | val_1_rmse: 0.85271 |  0:00:51s
epoch 109| loss: 0.65072 | val_0_rmse: 0.81228 | val_1_rmse: 0.83231 |  0:00:51s
epoch 110| loss: 0.64575 | val_0_rmse: 0.80541 | val_1_rmse: 0.82986 |  0:00:52s
epoch 111| loss: 0.64207 | val_0_rmse: 0.8171  | val_1_rmse: 0.84396 |  0:00:52s
epoch 112| loss: 0.6315  | val_0_rmse: 0.8262  | val_1_rmse: 0.85087 |  0:00:52s
epoch 113| loss: 0.63996 | val_0_rmse: 0.81247 | val_1_rmse: 0.84211 |  0:00:53s
epoch 114| loss: 0.64143 | val_0_rmse: 0.80497 | val_1_rmse: 0.8239  |  0:00:53s
epoch 115| loss: 0.64465 | val_0_rmse: 0.80216 | val_1_rmse: 0.82282 |  0:00:54s
epoch 116| loss: 0.64463 | val_0_rmse: 0.80482 | val_1_rmse: 0.8322  |  0:00:54s
epoch 117| loss: 0.63373 | val_0_rmse: 0.80551 | val_1_rmse: 0.83197 |  0:00:55s
epoch 118| loss: 0.64275 | val_0_rmse: 0.81522 | val_1_rmse: 0.8437  |  0:00:55s
epoch 119| loss: 0.65275 | val_0_rmse: 0.80528 | val_1_rmse: 0.81183 |  0:00:56s
epoch 120| loss: 0.6457  | val_0_rmse: 0.80318 | val_1_rmse: 0.81356 |  0:00:56s
epoch 121| loss: 0.63998 | val_0_rmse: 0.80742 | val_1_rmse: 0.83311 |  0:00:57s
epoch 122| loss: 0.64162 | val_0_rmse: 0.81112 | val_1_rmse: 0.83748 |  0:00:57s
epoch 123| loss: 0.64062 | val_0_rmse: 0.80437 | val_1_rmse: 0.837   |  0:00:58s
epoch 124| loss: 0.63323 | val_0_rmse: 0.81116 | val_1_rmse: 0.84044 |  0:00:58s
epoch 125| loss: 0.63195 | val_0_rmse: 0.8116  | val_1_rmse: 0.83636 |  0:00:59s
epoch 126| loss: 0.63606 | val_0_rmse: 0.80629 | val_1_rmse: 0.83767 |  0:00:59s
epoch 127| loss: 0.63065 | val_0_rmse: 0.81071 | val_1_rmse: 0.83673 |  0:01:00s
epoch 128| loss: 0.63534 | val_0_rmse: 0.79336 | val_1_rmse: 0.8136  |  0:01:00s
epoch 129| loss: 0.62838 | val_0_rmse: 0.79034 | val_1_rmse: 0.80619 |  0:01:00s
epoch 130| loss: 0.64555 | val_0_rmse: 0.79278 | val_1_rmse: 0.8161  |  0:01:01s
epoch 131| loss: 0.63666 | val_0_rmse: 0.79985 | val_1_rmse: 0.82005 |  0:01:01s
epoch 132| loss: 0.63382 | val_0_rmse: 0.79877 | val_1_rmse: 0.80874 |  0:01:02s
epoch 133| loss: 0.63247 | val_0_rmse: 0.80772 | val_1_rmse: 0.83323 |  0:01:02s
epoch 134| loss: 0.63568 | val_0_rmse: 0.8066  | val_1_rmse: 0.82668 |  0:01:03s
epoch 135| loss: 0.62794 | val_0_rmse: 0.79729 | val_1_rmse: 0.82327 |  0:01:03s
epoch 136| loss: 0.63099 | val_0_rmse: 0.79369 | val_1_rmse: 0.82149 |  0:01:04s
epoch 137| loss: 0.62642 | val_0_rmse: 0.80521 | val_1_rmse: 0.83285 |  0:01:04s
epoch 138| loss: 0.62505 | val_0_rmse: 0.80548 | val_1_rmse: 0.8383  |  0:01:05s
epoch 139| loss: 0.62271 | val_0_rmse: 0.80263 | val_1_rmse: 0.8326  |  0:01:05s
epoch 140| loss: 0.62165 | val_0_rmse: 0.79767 | val_1_rmse: 0.8242  |  0:01:06s
epoch 141| loss: 0.63012 | val_0_rmse: 0.83354 | val_1_rmse: 0.86731 |  0:01:06s
epoch 142| loss: 0.63996 | val_0_rmse: 0.80248 | val_1_rmse: 0.81478 |  0:01:07s
epoch 143| loss: 0.62746 | val_0_rmse: 0.80988 | val_1_rmse: 0.84109 |  0:01:07s
epoch 144| loss: 0.63907 | val_0_rmse: 0.81789 | val_1_rmse: 0.84346 |  0:01:08s
epoch 145| loss: 0.64094 | val_0_rmse: 0.79816 | val_1_rmse: 0.82394 |  0:01:08s
epoch 146| loss: 0.62625 | val_0_rmse: 0.8073  | val_1_rmse: 0.83812 |  0:01:08s
epoch 147| loss: 0.63324 | val_0_rmse: 0.80326 | val_1_rmse: 0.82887 |  0:01:09s
epoch 148| loss: 0.629   | val_0_rmse: 0.80998 | val_1_rmse: 0.83101 |  0:01:09s
epoch 149| loss: 0.63648 | val_0_rmse: 0.8029  | val_1_rmse: 0.82678 |  0:01:10s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.80619
Best weights from best epoch are automatically used!
ended training at: 06:55:14
Feature importance:
Mean squared error is of 6181007195.302597
Mean absolute error:55640.51876706955
MAPE:0.4736885489526263
R2 score:0.29861326221349416
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:55:17
epoch 0  | loss: 0.87267 | val_0_rmse: 0.92099 | val_1_rmse: 0.92217 |  0:00:16s
epoch 1  | loss: 0.72698 | val_0_rmse: 0.88554 | val_1_rmse: 0.88498 |  0:00:32s
epoch 2  | loss: 0.71693 | val_0_rmse: 0.85804 | val_1_rmse: 0.85784 |  0:00:48s
epoch 3  | loss: 0.70881 | val_0_rmse: 0.84568 | val_1_rmse: 0.84495 |  0:01:04s
epoch 4  | loss: 0.70755 | val_0_rmse: 0.83748 | val_1_rmse: 0.83669 |  0:01:20s
epoch 5  | loss: 0.70412 | val_0_rmse: 0.83537 | val_1_rmse: 0.83524 |  0:01:36s
epoch 6  | loss: 0.70212 | val_0_rmse: 0.83467 | val_1_rmse: 0.83403 |  0:01:53s
epoch 7  | loss: 0.70362 | val_0_rmse: 0.83593 | val_1_rmse: 0.83645 |  0:02:09s
epoch 8  | loss: 0.70187 | val_0_rmse: 0.83525 | val_1_rmse: 0.83565 |  0:02:25s
epoch 9  | loss: 0.70007 | val_0_rmse: 0.83591 | val_1_rmse: 0.83735 |  0:02:41s
epoch 10 | loss: 0.70129 | val_0_rmse: 0.84033 | val_1_rmse: 0.9182  |  0:02:57s
epoch 11 | loss: 0.7     | val_0_rmse: 0.83464 | val_1_rmse: 0.83418 |  0:03:13s
epoch 12 | loss: 0.69855 | val_0_rmse: 0.83406 | val_1_rmse: 0.83384 |  0:03:29s
epoch 13 | loss: 0.70031 | val_0_rmse: 0.83351 | val_1_rmse: 0.83309 |  0:03:45s
epoch 14 | loss: 0.69724 | val_0_rmse: 0.83326 | val_1_rmse: 0.83216 |  0:04:01s
epoch 15 | loss: 0.69608 | val_0_rmse: 0.833   | val_1_rmse: 0.83229 |  0:04:17s
epoch 16 | loss: 0.69851 | val_0_rmse: 0.83961 | val_1_rmse: 0.84174 |  0:04:33s
epoch 17 | loss: 0.69935 | val_0_rmse: 0.83713 | val_1_rmse: 0.83452 |  0:04:50s
epoch 18 | loss: 0.69713 | val_0_rmse: 0.83329 | val_1_rmse: 0.83353 |  0:05:06s
epoch 19 | loss: 0.69816 | val_0_rmse: 0.83597 | val_1_rmse: 0.83663 |  0:05:22s
epoch 20 | loss: 0.69882 | val_0_rmse: 0.83553 | val_1_rmse: 0.84523 |  0:05:38s
epoch 21 | loss: 0.69849 | val_0_rmse: 0.83495 | val_1_rmse: 0.8358  |  0:05:54s
epoch 22 | loss: 0.70582 | val_0_rmse: 0.83996 | val_1_rmse: 0.83696 |  0:06:10s
epoch 23 | loss: 0.70583 | val_0_rmse: 0.88788 | val_1_rmse: 0.83762 |  0:06:26s
epoch 24 | loss: 0.69781 | val_0_rmse: 0.83733 | val_1_rmse: 0.83453 |  0:06:43s
epoch 25 | loss: 0.69665 | val_0_rmse: 0.83529 | val_1_rmse: 0.83463 |  0:06:59s
epoch 26 | loss: 0.69631 | val_0_rmse: 0.83357 | val_1_rmse: 0.83341 |  0:07:15s
epoch 27 | loss: 0.69683 | val_0_rmse: 0.83351 | val_1_rmse: 0.83388 |  0:07:31s
epoch 28 | loss: 0.69671 | val_0_rmse: 0.83434 | val_1_rmse: 0.83452 |  0:07:47s
epoch 29 | loss: 0.69583 | val_0_rmse: 0.83424 | val_1_rmse: 0.83405 |  0:08:03s
epoch 30 | loss: 0.69558 | val_0_rmse: 0.83268 | val_1_rmse: 0.83203 |  0:08:19s
epoch 31 | loss: 0.69619 | val_0_rmse: 0.83277 | val_1_rmse: 0.83259 |  0:08:35s
epoch 32 | loss: 0.69499 | val_0_rmse: 0.8334  | val_1_rmse: 0.833   |  0:08:51s
epoch 33 | loss: 0.69497 | val_0_rmse: 0.83267 | val_1_rmse: 0.83255 |  0:09:07s
epoch 34 | loss: 0.69471 | val_0_rmse: 0.83392 | val_1_rmse: 0.83415 |  0:09:23s
epoch 35 | loss: 0.69552 | val_0_rmse: 0.83247 | val_1_rmse: 0.83243 |  0:09:38s
epoch 36 | loss: 0.69551 | val_0_rmse: 0.83309 | val_1_rmse: 0.83257 |  0:09:54s
epoch 37 | loss: 0.69691 | val_0_rmse: 0.83398 | val_1_rmse: 0.83368 |  0:10:10s
epoch 38 | loss: 0.69801 | val_0_rmse: 0.9432  | val_1_rmse: 1.21306 |  0:10:26s
epoch 39 | loss: 0.69582 | val_0_rmse: 0.84775 | val_1_rmse: 0.83317 |  0:10:42s
epoch 40 | loss: 0.69785 | val_0_rmse: 1.30337 | val_1_rmse: 2.15306 |  0:10:59s
epoch 41 | loss: 0.71814 | val_0_rmse: 1.28197 | val_1_rmse: 0.88952 |  0:11:15s
epoch 42 | loss: 0.72114 | val_0_rmse: 0.85955 | val_1_rmse: 0.87236 |  0:11:30s
epoch 43 | loss: 0.70901 | val_0_rmse: 0.85563 | val_1_rmse: 0.87167 |  0:11:46s
epoch 44 | loss: 0.70851 | val_0_rmse: 0.84441 | val_1_rmse: 0.84976 |  0:12:03s
epoch 45 | loss: 0.70657 | val_0_rmse: 0.85711 | val_1_rmse: 0.87453 |  0:12:19s
epoch 46 | loss: 0.70497 | val_0_rmse: 0.83944 | val_1_rmse: 0.84059 |  0:12:34s
epoch 47 | loss: 0.70531 | val_0_rmse: 0.9104  | val_1_rmse: 0.97511 |  0:12:50s
epoch 48 | loss: 0.70488 | val_0_rmse: 13.42058| val_1_rmse: 0.84581 |  0:13:07s
epoch 49 | loss: 0.70804 | val_0_rmse: 0.87189 | val_1_rmse: 0.90189 |  0:13:23s
epoch 50 | loss: 0.70975 | val_0_rmse: 0.84239 | val_1_rmse: 0.84606 |  0:13:39s
epoch 51 | loss: 0.71809 | val_0_rmse: 0.86529 | val_1_rmse: 0.8872  |  0:13:55s
epoch 52 | loss: 0.71549 | val_0_rmse: 0.87058 | val_1_rmse: 0.89877 |  0:14:11s
epoch 53 | loss: 0.7144  | val_0_rmse: 0.86292 | val_1_rmse: 0.87743 |  0:14:27s
epoch 54 | loss: 0.71213 | val_0_rmse: 0.84476 | val_1_rmse: 0.84845 |  0:14:43s
epoch 55 | loss: 0.71236 | val_0_rmse: 0.85517 | val_1_rmse: 0.87125 |  0:14:59s
epoch 56 | loss: 0.71153 | val_0_rmse: 1.10138 | val_1_rmse: 1.31395 |  0:15:15s
epoch 57 | loss: 0.71088 | val_0_rmse: 0.93699 | val_1_rmse: 1.02854 |  0:15:31s
epoch 58 | loss: 0.70939 | val_0_rmse: 0.84397 | val_1_rmse: 0.84608 |  0:15:47s
epoch 59 | loss: 0.7085  | val_0_rmse: 0.88328 | val_1_rmse: 0.92928 |  0:16:03s
epoch 60 | loss: 0.70629 | val_0_rmse: 1.02151 | val_1_rmse: 1.17779 |  0:16:19s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.83203
Best weights from best epoch are automatically used!
ended training at: 07:11:43
Feature importance:
Mean squared error is of 4648497895.986057
Mean absolute error:52158.43883393169
MAPE:0.5575118153118805
R2 score:0.30308887211496016
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:11:46
epoch 0  | loss: 0.8872  | val_0_rmse: 0.93621 | val_1_rmse: 0.94254 |  0:00:15s
epoch 1  | loss: 0.72876 | val_0_rmse: 0.8996  | val_1_rmse: 0.90302 |  0:00:31s
epoch 2  | loss: 0.71759 | val_0_rmse: 0.86356 | val_1_rmse: 0.86856 |  0:00:47s
epoch 3  | loss: 0.7127  | val_0_rmse: 0.84758 | val_1_rmse: 0.85178 |  0:01:03s
epoch 4  | loss: 0.70717 | val_0_rmse: 0.83696 | val_1_rmse: 0.84158 |  0:01:19s
epoch 5  | loss: 0.70716 | val_0_rmse: 0.83857 | val_1_rmse: 0.84387 |  0:01:35s
epoch 6  | loss: 0.70458 | val_0_rmse: 0.83621 | val_1_rmse: 0.84165 |  0:01:51s
epoch 7  | loss: 0.70429 | val_0_rmse: 0.83825 | val_1_rmse: 0.8427  |  0:02:07s
epoch 8  | loss: 0.70421 | val_0_rmse: 0.84043 | val_1_rmse: 0.84173 |  0:02:23s
epoch 9  | loss: 0.70157 | val_0_rmse: 0.83449 | val_1_rmse: 0.83799 |  0:02:40s
epoch 10 | loss: 0.69853 | val_0_rmse: 0.83396 | val_1_rmse: 0.83893 |  0:02:56s
epoch 11 | loss: 0.69832 | val_0_rmse: 0.83649 | val_1_rmse: 0.84108 |  0:03:11s
epoch 12 | loss: 0.69793 | val_0_rmse: 0.83393 | val_1_rmse: 0.83857 |  0:03:27s
epoch 13 | loss: 0.69778 | val_0_rmse: 0.83375 | val_1_rmse: 0.83986 |  0:03:43s
epoch 14 | loss: 0.69721 | val_0_rmse: 0.8458  | val_1_rmse: 0.8382  |  0:03:59s
epoch 15 | loss: 0.69657 | val_0_rmse: 0.83357 | val_1_rmse: 0.83828 |  0:04:15s
epoch 16 | loss: 0.69752 | val_0_rmse: 0.8367  | val_1_rmse: 0.84421 |  0:04:31s
epoch 17 | loss: 0.69748 | val_0_rmse: 0.83235 | val_1_rmse: 0.83771 |  0:04:47s
epoch 18 | loss: 0.69667 | val_0_rmse: 0.83709 | val_1_rmse: 0.83937 |  0:05:03s
epoch 19 | loss: 0.6974  | val_0_rmse: 0.88019 | val_1_rmse: 0.83728 |  0:05:19s
epoch 20 | loss: 0.69651 | val_0_rmse: 0.84934 | val_1_rmse: 0.84755 |  0:05:35s
epoch 21 | loss: 0.69712 | val_0_rmse: 0.83339 | val_1_rmse: 0.8386  |  0:05:51s
epoch 22 | loss: 0.69769 | val_0_rmse: 0.96979 | val_1_rmse: 0.8377  |  0:06:07s
epoch 23 | loss: 0.69663 | val_0_rmse: 0.83789 | val_1_rmse: 0.8388  |  0:06:23s
epoch 24 | loss: 0.69999 | val_0_rmse: 0.83591 | val_1_rmse: 0.84107 |  0:06:39s
epoch 25 | loss: 0.69767 | val_0_rmse: 0.83478 | val_1_rmse: 0.8402  |  0:06:55s
epoch 26 | loss: 0.69636 | val_0_rmse: 0.83207 | val_1_rmse: 0.83679 |  0:07:11s
epoch 27 | loss: 0.69568 | val_0_rmse: 0.83219 | val_1_rmse: 0.83675 |  0:07:27s
epoch 28 | loss: 0.69545 | val_0_rmse: 0.83605 | val_1_rmse: 0.84084 |  0:07:43s
epoch 29 | loss: 0.69824 | val_0_rmse: 0.83388 | val_1_rmse: 0.83959 |  0:07:59s
epoch 30 | loss: 0.69732 | val_0_rmse: 0.83627 | val_1_rmse: 0.84011 |  0:08:15s
epoch 31 | loss: 0.69742 | val_0_rmse: 0.83395 | val_1_rmse: 0.83918 |  0:08:31s
epoch 32 | loss: 0.69713 | val_0_rmse: 0.834   | val_1_rmse: 0.83894 |  0:08:47s
epoch 33 | loss: 0.69904 | val_0_rmse: 0.83765 | val_1_rmse: 0.84262 |  0:09:03s
epoch 34 | loss: 0.69894 | val_0_rmse: 0.83589 | val_1_rmse: 0.84036 |  0:09:19s
epoch 35 | loss: 0.69706 | val_0_rmse: 0.83446 | val_1_rmse: 0.83924 |  0:09:35s
epoch 36 | loss: 0.69696 | val_0_rmse: 0.8514  | val_1_rmse: 0.85545 |  0:09:51s
epoch 37 | loss: 0.70336 | val_0_rmse: 0.83513 | val_1_rmse: 0.8406  |  0:10:07s
epoch 38 | loss: 0.69614 | val_0_rmse: 0.83237 | val_1_rmse: 0.83741 |  0:10:23s
epoch 39 | loss: 0.69557 | val_0_rmse: 0.83291 | val_1_rmse: 0.83808 |  0:10:39s
epoch 40 | loss: 0.6948  | val_0_rmse: 0.83238 | val_1_rmse: 0.83707 |  0:10:55s
epoch 41 | loss: 0.69568 | val_0_rmse: 0.83244 | val_1_rmse: 0.83762 |  0:11:11s
epoch 42 | loss: 0.69537 | val_0_rmse: 0.83319 | val_1_rmse: 0.83817 |  0:11:27s
epoch 43 | loss: 0.69508 | val_0_rmse: 0.83348 | val_1_rmse: 0.83821 |  0:11:43s
epoch 44 | loss: 0.69489 | val_0_rmse: 0.83417 | val_1_rmse: 0.83972 |  0:11:59s
epoch 45 | loss: 0.69561 | val_0_rmse: 0.83311 | val_1_rmse: 0.83837 |  0:12:15s
epoch 46 | loss: 0.70327 | val_0_rmse: 0.83746 | val_1_rmse: 0.843   |  0:12:30s
epoch 47 | loss: 0.69966 | val_0_rmse: 0.83502 | val_1_rmse: 0.83938 |  0:12:47s
epoch 48 | loss: 0.69815 | val_0_rmse: 0.83428 | val_1_rmse: 0.8391  |  0:13:03s
epoch 49 | loss: 0.69781 | val_0_rmse: 0.83353 | val_1_rmse: 0.8393  |  0:13:18s
epoch 50 | loss: 0.69675 | val_0_rmse: 0.83318 | val_1_rmse: 0.83826 |  0:13:34s
epoch 51 | loss: 0.69551 | val_0_rmse: 0.83272 | val_1_rmse: 0.83784 |  0:13:50s
epoch 52 | loss: 0.69577 | val_0_rmse: 0.83351 | val_1_rmse: 0.83878 |  0:14:06s
epoch 53 | loss: 0.69622 | val_0_rmse: 0.83294 | val_1_rmse: 0.83785 |  0:14:22s
epoch 54 | loss: 0.69548 | val_0_rmse: 0.83365 | val_1_rmse: 0.83867 |  0:14:38s
epoch 55 | loss: 0.70094 | val_0_rmse: 0.83728 | val_1_rmse: 0.84291 |  0:14:54s
epoch 56 | loss: 0.70144 | val_0_rmse: 0.83698 | val_1_rmse: 0.84257 |  0:15:10s
epoch 57 | loss: 0.70084 | val_0_rmse: 0.83687 | val_1_rmse: 0.84333 |  0:15:26s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.83675
Best weights from best epoch are automatically used!
ended training at: 07:27:19
Feature importance:
Mean squared error is of 4622190866.927399
Mean absolute error:52360.5921953315
MAPE:0.5633926646320366
R2 score:0.30505483204632544
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:27:22
epoch 0  | loss: 0.87449 | val_0_rmse: 0.94951 | val_1_rmse: 0.94698 |  0:00:15s
epoch 1  | loss: 0.74163 | val_0_rmse: 0.89835 | val_1_rmse: 0.89411 |  0:00:31s
epoch 2  | loss: 0.72211 | val_0_rmse: 0.86707 | val_1_rmse: 0.86361 |  0:00:47s
epoch 3  | loss: 0.71662 | val_0_rmse: 0.85175 | val_1_rmse: 0.8502  |  0:01:03s
epoch 4  | loss: 0.71549 | val_0_rmse: 0.84574 | val_1_rmse: 0.84525 |  0:01:19s
epoch 5  | loss: 0.7076  | val_0_rmse: 0.83872 | val_1_rmse: 0.8385  |  0:01:36s
epoch 6  | loss: 0.70973 | val_0_rmse: 0.84017 | val_1_rmse: 0.83985 |  0:01:51s
epoch 7  | loss: 0.70734 | val_0_rmse: 0.83865 | val_1_rmse: 0.83788 |  0:02:08s
epoch 8  | loss: 0.70568 | val_0_rmse: 0.8387  | val_1_rmse: 0.83737 |  0:02:23s
epoch 9  | loss: 0.7039  | val_0_rmse: 0.84565 | val_1_rmse: 0.83744 |  0:02:39s
epoch 10 | loss: 0.70253 | val_0_rmse: 0.83635 | val_1_rmse: 0.83621 |  0:02:55s
epoch 11 | loss: 0.7036  | val_0_rmse: 0.83843 | val_1_rmse: 0.83651 |  0:03:11s
epoch 12 | loss: 0.70044 | val_0_rmse: 0.83502 | val_1_rmse: 0.8348  |  0:03:27s
epoch 13 | loss: 0.69985 | val_0_rmse: 0.8336  | val_1_rmse: 0.83378 |  0:03:43s
epoch 14 | loss: 0.69789 | val_0_rmse: 0.83357 | val_1_rmse: 0.83296 |  0:03:59s
epoch 15 | loss: 0.69815 | val_0_rmse: 0.83421 | val_1_rmse: 0.83498 |  0:04:15s
epoch 16 | loss: 0.69716 | val_0_rmse: 0.83316 | val_1_rmse: 0.83287 |  0:04:31s
epoch 17 | loss: 0.69679 | val_0_rmse: 0.83508 | val_1_rmse: 0.83592 |  0:04:47s
epoch 18 | loss: 0.69786 | val_0_rmse: 0.85045 | val_1_rmse: 0.83849 |  0:05:03s
epoch 19 | loss: 0.69597 | val_0_rmse: 0.83926 | val_1_rmse: 0.83327 |  0:05:19s
epoch 20 | loss: 0.69968 | val_0_rmse: 0.83946 | val_1_rmse: 0.83306 |  0:05:35s
epoch 21 | loss: 0.70355 | val_0_rmse: 0.84494 | val_1_rmse: 0.84018 |  0:05:51s
epoch 22 | loss: 0.70259 | val_0_rmse: 0.84556 | val_1_rmse: 0.8388  |  0:06:07s
epoch 23 | loss: 0.70555 | val_0_rmse: 0.83819 | val_1_rmse: 0.83891 |  0:06:23s
epoch 24 | loss: 0.70321 | val_0_rmse: 0.83781 | val_1_rmse: 0.83747 |  0:06:39s
epoch 25 | loss: 0.70321 | val_0_rmse: 0.84399 | val_1_rmse: 0.84416 |  0:06:55s
epoch 26 | loss: 0.70369 | val_0_rmse: 0.84086 | val_1_rmse: 0.8383  |  0:07:11s
epoch 27 | loss: 0.70988 | val_0_rmse: 0.83881 | val_1_rmse: 0.84051 |  0:07:26s
epoch 28 | loss: 0.70898 | val_0_rmse: 0.84127 | val_1_rmse: 0.84143 |  0:07:42s
epoch 29 | loss: 0.70896 | val_0_rmse: 0.84047 | val_1_rmse: 0.84167 |  0:07:58s
epoch 30 | loss: 0.70475 | val_0_rmse: 0.83792 | val_1_rmse: 0.83975 |  0:08:14s
epoch 31 | loss: 0.70312 | val_0_rmse: 0.83719 | val_1_rmse: 0.83865 |  0:08:30s
epoch 32 | loss: 0.70357 | val_0_rmse: 0.83532 | val_1_rmse: 0.83596 |  0:08:46s
epoch 33 | loss: 0.69986 | val_0_rmse: 0.83873 | val_1_rmse: 0.83644 |  0:09:02s
epoch 34 | loss: 0.69866 | val_0_rmse: 0.83535 | val_1_rmse: 0.83688 |  0:09:18s
epoch 35 | loss: 0.69852 | val_0_rmse: 0.83364 | val_1_rmse: 0.83507 |  0:09:34s
epoch 36 | loss: 0.69638 | val_0_rmse: 0.83314 | val_1_rmse: 0.83422 |  0:09:50s
epoch 37 | loss: 0.69718 | val_0_rmse: 0.83272 | val_1_rmse: 0.83294 |  0:10:06s
epoch 38 | loss: 0.69561 | val_0_rmse: 0.83174 | val_1_rmse: 0.83267 |  0:10:21s
epoch 39 | loss: 0.69531 | val_0_rmse: 0.83293 | val_1_rmse: 0.83378 |  0:10:37s
epoch 40 | loss: 0.69802 | val_0_rmse: 0.8459  | val_1_rmse: 0.83684 |  0:10:53s
epoch 41 | loss: 0.69685 | val_0_rmse: 0.83565 | val_1_rmse: 0.83404 |  0:11:09s
epoch 42 | loss: 0.69621 | val_0_rmse: 0.83495 | val_1_rmse: 0.83481 |  0:11:25s
epoch 43 | loss: 0.69684 | val_0_rmse: 0.83494 | val_1_rmse: 0.83604 |  0:11:41s
epoch 44 | loss: 0.69466 | val_0_rmse: 0.83349 | val_1_rmse: 0.83311 |  0:11:57s
epoch 45 | loss: 0.69509 | val_0_rmse: 0.8363  | val_1_rmse: 0.8348  |  0:12:13s
epoch 46 | loss: 0.69506 | val_0_rmse: 0.83285 | val_1_rmse: 0.83411 |  0:12:29s
epoch 47 | loss: 0.69538 | val_0_rmse: 0.83448 | val_1_rmse: 0.83439 |  0:12:45s
epoch 48 | loss: 0.69537 | val_0_rmse: 0.83261 | val_1_rmse: 0.83264 |  0:13:01s
epoch 49 | loss: 0.69788 | val_0_rmse: 0.83242 | val_1_rmse: 0.83253 |  0:13:17s
epoch 50 | loss: 0.69643 | val_0_rmse: 0.83737 | val_1_rmse: 0.83817 |  0:13:32s
epoch 51 | loss: 0.6968  | val_0_rmse: 0.83362 | val_1_rmse: 0.83324 |  0:13:48s
epoch 52 | loss: 0.69596 | val_0_rmse: 0.83609 | val_1_rmse: 0.83604 |  0:14:04s
epoch 53 | loss: 0.69641 | val_0_rmse: 0.83276 | val_1_rmse: 0.83324 |  0:14:20s
epoch 54 | loss: 0.69669 | val_0_rmse: 0.83596 | val_1_rmse: 0.83678 |  0:14:36s
epoch 55 | loss: 0.697   | val_0_rmse: 0.83353 | val_1_rmse: 0.83372 |  0:14:52s
epoch 56 | loss: 0.69748 | val_0_rmse: 0.8337  | val_1_rmse: 0.83328 |  0:15:08s
epoch 57 | loss: 0.69607 | val_0_rmse: 0.83742 | val_1_rmse: 0.83651 |  0:15:24s
epoch 58 | loss: 0.6983  | val_0_rmse: 0.83928 | val_1_rmse: 0.83938 |  0:15:40s
epoch 59 | loss: 0.69793 | val_0_rmse: 0.83425 | val_1_rmse: 0.83378 |  0:15:56s
epoch 60 | loss: 0.69701 | val_0_rmse: 0.83509 | val_1_rmse: 0.83495 |  0:16:12s
epoch 61 | loss: 0.69657 | val_0_rmse: 0.83476 | val_1_rmse: 0.8353  |  0:16:28s
epoch 62 | loss: 0.69757 | val_0_rmse: 0.83777 | val_1_rmse: 0.83558 |  0:16:44s
epoch 63 | loss: 0.69839 | val_0_rmse: 0.85242 | val_1_rmse: 0.84961 |  0:17:00s
epoch 64 | loss: 0.70254 | val_0_rmse: 0.83734 | val_1_rmse: 0.83289 |  0:17:16s
epoch 65 | loss: 0.69672 | val_0_rmse: 0.83586 | val_1_rmse: 0.83271 |  0:17:32s
epoch 66 | loss: 0.6988  | val_0_rmse: 0.84117 | val_1_rmse: 0.83362 |  0:17:48s
epoch 67 | loss: 0.69708 | val_0_rmse: 0.84172 | val_1_rmse: 0.8346  |  0:18:04s
epoch 68 | loss: 0.6971  | val_0_rmse: 0.83994 | val_1_rmse: 0.83371 |  0:18:20s
epoch 69 | loss: 0.70165 | val_0_rmse: 1.05287 | val_1_rmse: 0.83715 |  0:18:36s
epoch 70 | loss: 0.6971  | val_0_rmse: 0.83907 | val_1_rmse: 0.83411 |  0:18:52s
epoch 71 | loss: 0.69586 | val_0_rmse: 0.84224 | val_1_rmse: 0.8341  |  0:19:08s
epoch 72 | loss: 0.69582 | val_0_rmse: 0.84624 | val_1_rmse: 0.83494 |  0:19:24s
epoch 73 | loss: 0.69443 | val_0_rmse: 0.84138 | val_1_rmse: 0.83208 |  0:19:40s
epoch 74 | loss: 0.69377 | val_0_rmse: 0.84959 | val_1_rmse: 0.83255 |  0:19:56s
epoch 75 | loss: 0.69611 | val_0_rmse: 0.84484 | val_1_rmse: 0.83584 |  0:20:12s
epoch 76 | loss: 0.69419 | val_0_rmse: 0.83888 | val_1_rmse: 0.83308 |  0:20:28s
epoch 77 | loss: 0.69415 | val_0_rmse: 0.83641 | val_1_rmse: 0.83242 |  0:20:44s
epoch 78 | loss: 0.69472 | val_0_rmse: 0.83431 | val_1_rmse: 0.83309 |  0:21:00s
epoch 79 | loss: 0.695   | val_0_rmse: 0.8376  | val_1_rmse: 0.83481 |  0:21:16s
epoch 80 | loss: 0.69623 | val_0_rmse: 0.84064 | val_1_rmse: 0.83445 |  0:21:31s
epoch 81 | loss: 0.69558 | val_0_rmse: 0.95346 | val_1_rmse: 0.91275 |  0:21:47s
epoch 82 | loss: 0.69526 | val_0_rmse: 0.84264 | val_1_rmse: 0.83811 |  0:22:04s
epoch 83 | loss: 0.69516 | val_0_rmse: 0.84057 | val_1_rmse: 0.83239 |  0:22:20s
epoch 84 | loss: 0.69517 | val_0_rmse: 0.83255 | val_1_rmse: 0.83191 |  0:22:35s
epoch 85 | loss: 0.69434 | val_0_rmse: 0.83235 | val_1_rmse: 0.83174 |  0:22:51s
epoch 86 | loss: 0.69452 | val_0_rmse: 0.84138 | val_1_rmse: 0.83792 |  0:23:08s
epoch 87 | loss: 0.72492 | val_0_rmse: 0.85753 | val_1_rmse: 0.85656 |  0:23:23s
epoch 88 | loss: 0.72199 | val_0_rmse: 0.85281 | val_1_rmse: 0.85155 |  0:23:39s
epoch 89 | loss: 0.70999 | val_0_rmse: 0.8502  | val_1_rmse: 0.83845 |  0:23:55s
epoch 90 | loss: 0.70656 | val_0_rmse: 0.84089 | val_1_rmse: 0.83817 |  0:24:11s
epoch 91 | loss: 0.70689 | val_0_rmse: 0.84122 | val_1_rmse: 0.8379  |  0:24:27s
epoch 92 | loss: 0.70579 | val_0_rmse: 0.8418  | val_1_rmse: 0.83841 |  0:24:43s
epoch 93 | loss: 0.70317 | val_0_rmse: 0.8385  | val_1_rmse: 0.83635 |  0:24:59s
epoch 94 | loss: 0.70094 | val_0_rmse: 0.84702 | val_1_rmse: 0.83875 |  0:25:15s
epoch 95 | loss: 0.69886 | val_0_rmse: 0.84069 | val_1_rmse: 0.83715 |  0:25:31s
epoch 96 | loss: 0.69956 | val_0_rmse: 0.84491 | val_1_rmse: 0.83982 |  0:25:47s
epoch 97 | loss: 0.69883 | val_0_rmse: 0.8378  | val_1_rmse: 0.8355  |  0:26:03s
epoch 98 | loss: 0.69997 | val_0_rmse: 0.83946 | val_1_rmse: 0.8368  |  0:26:19s
epoch 99 | loss: 0.69982 | val_0_rmse: 0.83701 | val_1_rmse: 0.83535 |  0:26:35s
epoch 100| loss: 0.70005 | val_0_rmse: 0.84401 | val_1_rmse: 0.8416  |  0:26:51s
epoch 101| loss: 0.69933 | val_0_rmse: 0.84022 | val_1_rmse: 0.83973 |  0:27:07s
epoch 102| loss: 0.69937 | val_0_rmse: 0.83789 | val_1_rmse: 0.8365  |  0:27:23s
epoch 103| loss: 0.6989  | val_0_rmse: 0.83614 | val_1_rmse: 0.83486 |  0:27:39s
epoch 104| loss: 0.69933 | val_0_rmse: 0.83803 | val_1_rmse: 0.83767 |  0:27:55s
epoch 105| loss: 0.6992  | val_0_rmse: 0.83904 | val_1_rmse: 0.83631 |  0:28:11s
epoch 106| loss: 0.69849 | val_0_rmse: 0.83742 | val_1_rmse: 0.8351  |  0:28:27s
epoch 107| loss: 0.69776 | val_0_rmse: 0.83899 | val_1_rmse: 0.83511 |  0:28:43s
epoch 108| loss: 0.6985  | val_0_rmse: 0.84089 | val_1_rmse: 0.83657 |  0:28:59s
epoch 109| loss: 0.6986  | val_0_rmse: 0.83756 | val_1_rmse: 0.83399 |  0:29:15s
epoch 110| loss: 0.69834 | val_0_rmse: 0.83766 | val_1_rmse: 0.83457 |  0:29:31s
epoch 111| loss: 0.69793 | val_0_rmse: 0.84643 | val_1_rmse: 0.84225 |  0:29:47s
epoch 112| loss: 0.69889 | val_0_rmse: 0.83802 | val_1_rmse: 0.83519 |  0:30:03s
epoch 113| loss: 0.69771 | val_0_rmse: 0.83778 | val_1_rmse: 0.83378 |  0:30:19s
epoch 114| loss: 0.69689 | val_0_rmse: 0.83999 | val_1_rmse: 0.83814 |  0:30:35s
epoch 115| loss: 0.69839 | val_0_rmse: 0.83818 | val_1_rmse: 0.83527 |  0:30:51s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.83174
Best weights from best epoch are automatically used!
ended training at: 07:58:19
Feature importance:
Mean squared error is of 4661494687.981938
Mean absolute error:52218.3383140469
MAPE:0.5554207772843354
R2 score:0.2994625737554002
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:58:22
epoch 0  | loss: 0.8895  | val_0_rmse: 0.90882 | val_1_rmse: 0.90215 |  0:00:16s
epoch 1  | loss: 0.75356 | val_0_rmse: 0.8998  | val_1_rmse: 0.893   |  0:00:32s
epoch 2  | loss: 0.73735 | val_0_rmse: 0.87632 | val_1_rmse: 0.87138 |  0:00:48s
epoch 3  | loss: 0.72733 | val_0_rmse: 0.85835 | val_1_rmse: 0.85247 |  0:01:04s
epoch 4  | loss: 0.72396 | val_0_rmse: 0.84631 | val_1_rmse: 0.84192 |  0:01:20s
epoch 5  | loss: 0.7169  | val_0_rmse: 0.8468  | val_1_rmse: 0.84401 |  0:01:35s
epoch 6  | loss: 0.71884 | val_0_rmse: 0.8429  | val_1_rmse: 0.83947 |  0:01:51s
epoch 7  | loss: 0.72847 | val_0_rmse: 0.85482 | val_1_rmse: 0.85152 |  0:02:07s
epoch 8  | loss: 0.74859 | val_0_rmse: 0.86249 | val_1_rmse: 0.86126 |  0:02:23s
epoch 9  | loss: 0.73036 | val_0_rmse: 0.84653 | val_1_rmse: 0.86456 |  0:02:39s
epoch 10 | loss: 0.71918 | val_0_rmse: 0.84625 | val_1_rmse: 0.86353 |  0:02:55s
epoch 11 | loss: 0.71744 | val_0_rmse: 0.84357 | val_1_rmse: 0.87336 |  0:03:11s
epoch 12 | loss: 0.71495 | val_0_rmse: 0.84394 | val_1_rmse: 0.87527 |  0:03:27s
epoch 13 | loss: 0.7137  | val_0_rmse: 0.84188 | val_1_rmse: 0.85453 |  0:03:43s
epoch 14 | loss: 0.71311 | val_0_rmse: 0.8441  | val_1_rmse: 0.83946 |  0:03:59s
epoch 15 | loss: 0.71269 | val_0_rmse: 0.84232 | val_1_rmse: 0.84051 |  0:04:15s
epoch 16 | loss: 0.71247 | val_0_rmse: 0.84214 | val_1_rmse: 0.83764 |  0:04:31s
epoch 17 | loss: 0.71246 | val_0_rmse: 0.84399 | val_1_rmse: 0.84021 |  0:04:47s
epoch 18 | loss: 0.7115  | val_0_rmse: 0.84334 | val_1_rmse: 0.83908 |  0:05:03s
epoch 19 | loss: 0.71081 | val_0_rmse: 0.84208 | val_1_rmse: 0.83851 |  0:05:18s
epoch 20 | loss: 0.71122 | val_0_rmse: 0.84227 | val_1_rmse: 0.83773 |  0:05:34s
epoch 21 | loss: 0.71102 | val_0_rmse: 0.84312 | val_1_rmse: 0.83946 |  0:05:50s
epoch 22 | loss: 0.71138 | val_0_rmse: 0.84164 | val_1_rmse: 0.83814 |  0:06:06s
epoch 23 | loss: 0.71119 | val_0_rmse: 0.84167 | val_1_rmse: 0.8378  |  0:06:22s
epoch 24 | loss: 0.70982 | val_0_rmse: 0.84263 | val_1_rmse: 0.83806 |  0:06:38s
epoch 25 | loss: 0.71908 | val_0_rmse: 0.86356 | val_1_rmse: 0.85214 |  0:06:54s
epoch 26 | loss: 0.73132 | val_0_rmse: 0.85391 | val_1_rmse: 0.84979 |  0:07:10s
epoch 27 | loss: 0.72484 | val_0_rmse: 0.84777 | val_1_rmse: 0.84285 |  0:07:26s
epoch 28 | loss: 0.7197  | val_0_rmse: 0.8466  | val_1_rmse: 0.84126 |  0:07:42s
epoch 29 | loss: 0.71802 | val_0_rmse: 0.84468 | val_1_rmse: 0.83955 |  0:07:58s
epoch 30 | loss: 0.71603 | val_0_rmse: 0.84562 | val_1_rmse: 0.84085 |  0:08:14s
epoch 31 | loss: 0.71536 | val_0_rmse: 0.85418 | val_1_rmse: 0.85143 |  0:08:30s
epoch 32 | loss: 0.71542 | val_0_rmse: 0.84295 | val_1_rmse: 0.83952 |  0:08:46s
epoch 33 | loss: 0.71282 | val_0_rmse: 0.84225 | val_1_rmse: 0.83877 |  0:09:02s
epoch 34 | loss: 0.71244 | val_0_rmse: 0.8427  | val_1_rmse: 0.8397  |  0:09:18s
epoch 35 | loss: 0.71195 | val_0_rmse: 0.84365 | val_1_rmse: 0.84049 |  0:09:34s
epoch 36 | loss: 0.71149 | val_0_rmse: 0.84284 | val_1_rmse: 0.83914 |  0:09:50s
epoch 37 | loss: 0.71094 | val_0_rmse: 0.84737 | val_1_rmse: 0.84579 |  0:10:06s
epoch 38 | loss: 0.70484 | val_0_rmse: 0.83769 | val_1_rmse: 0.83493 |  0:10:22s
epoch 39 | loss: 0.7034  | val_0_rmse: 0.84277 | val_1_rmse: 0.84063 |  0:10:38s
epoch 40 | loss: 0.70293 | val_0_rmse: 0.83778 | val_1_rmse: 0.83443 |  0:10:54s
epoch 41 | loss: 0.70342 | val_0_rmse: 0.83946 | val_1_rmse: 0.83745 |  0:11:10s
epoch 42 | loss: 0.70513 | val_0_rmse: 0.84331 | val_1_rmse: 0.83892 |  0:11:26s
epoch 43 | loss: 0.71221 | val_0_rmse: 0.84078 | val_1_rmse: 0.83806 |  0:11:42s
epoch 44 | loss: 0.70825 | val_0_rmse: 0.84487 | val_1_rmse: 0.84327 |  0:11:58s
epoch 45 | loss: 0.70427 | val_0_rmse: 0.83774 | val_1_rmse: 0.83402 |  0:12:14s
epoch 46 | loss: 0.7043  | val_0_rmse: 0.83781 | val_1_rmse: 0.8342  |  0:12:29s
epoch 47 | loss: 0.70414 | val_0_rmse: 0.83839 | val_1_rmse: 0.83465 |  0:12:45s
epoch 48 | loss: 0.70331 | val_0_rmse: 0.83824 | val_1_rmse: 0.83463 |  0:13:01s
epoch 49 | loss: 0.7035  | val_0_rmse: 0.8399  | val_1_rmse: 0.83591 |  0:13:17s
epoch 50 | loss: 0.7028  | val_0_rmse: 0.83965 | val_1_rmse: 0.83632 |  0:13:33s
epoch 51 | loss: 0.70186 | val_0_rmse: 0.84224 | val_1_rmse: 0.8397  |  0:13:49s
epoch 52 | loss: 0.70141 | val_0_rmse: 0.83756 | val_1_rmse: 0.83395 |  0:14:05s
epoch 53 | loss: 0.70145 | val_0_rmse: 0.83726 | val_1_rmse: 0.83384 |  0:14:21s
epoch 54 | loss: 0.70189 | val_0_rmse: 0.83583 | val_1_rmse: 0.8323  |  0:14:37s
epoch 55 | loss: 0.70161 | val_0_rmse: 0.84251 | val_1_rmse: 0.84062 |  0:14:53s
epoch 56 | loss: 0.70334 | val_0_rmse: 0.83814 | val_1_rmse: 0.83466 |  0:15:09s
epoch 57 | loss: 0.70171 | val_0_rmse: 0.8375  | val_1_rmse: 0.83388 |  0:15:25s
epoch 58 | loss: 0.70113 | val_0_rmse: 0.83714 | val_1_rmse: 0.83366 |  0:15:41s
epoch 59 | loss: 0.70157 | val_0_rmse: 0.84021 | val_1_rmse: 0.83669 |  0:15:57s
epoch 60 | loss: 0.70112 | val_0_rmse: 0.83936 | val_1_rmse: 0.83533 |  0:16:13s
epoch 61 | loss: 0.70142 | val_0_rmse: 0.83718 | val_1_rmse: 0.83427 |  0:16:29s
epoch 62 | loss: 0.70242 | val_0_rmse: 0.84026 | val_1_rmse: 0.83646 |  0:16:45s
epoch 63 | loss: 0.70277 | val_0_rmse: 0.83747 | val_1_rmse: 0.83369 |  0:17:00s
epoch 64 | loss: 0.70166 | val_0_rmse: 0.83739 | val_1_rmse: 0.83467 |  0:17:16s
epoch 65 | loss: 0.70104 | val_0_rmse: 0.8372  | val_1_rmse: 0.83403 |  0:17:32s
epoch 66 | loss: 0.70085 | val_0_rmse: 0.83929 | val_1_rmse: 0.83542 |  0:17:48s
epoch 67 | loss: 0.708   | val_0_rmse: 0.84571 | val_1_rmse: 0.83701 |  0:18:04s
epoch 68 | loss: 0.70294 | val_0_rmse: 0.83767 | val_1_rmse: 0.83456 |  0:18:20s
epoch 69 | loss: 0.70303 | val_0_rmse: 0.84257 | val_1_rmse: 0.84541 |  0:18:36s
epoch 70 | loss: 0.7074  | val_0_rmse: 0.87203 | val_1_rmse: 0.86712 |  0:18:52s
epoch 71 | loss: 0.70251 | val_0_rmse: 0.83736 | val_1_rmse: 0.83344 |  0:19:08s
epoch 72 | loss: 0.70234 | val_0_rmse: 0.83725 | val_1_rmse: 0.83278 |  0:19:24s
epoch 73 | loss: 0.70151 | val_0_rmse: 0.8363  | val_1_rmse: 0.83257 |  0:19:40s
epoch 74 | loss: 0.70082 | val_0_rmse: 0.83825 | val_1_rmse: 0.83506 |  0:19:56s
epoch 75 | loss: 0.7005  | val_0_rmse: 0.83683 | val_1_rmse: 0.83323 |  0:20:12s
epoch 76 | loss: 0.70057 | val_0_rmse: 0.83781 | val_1_rmse: 0.83421 |  0:20:27s
epoch 77 | loss: 0.70107 | val_0_rmse: 0.83998 | val_1_rmse: 0.8357  |  0:20:43s
epoch 78 | loss: 0.70101 | val_0_rmse: 0.83829 | val_1_rmse: 0.83445 |  0:20:59s
epoch 79 | loss: 0.70095 | val_0_rmse: 0.83666 | val_1_rmse: 0.83352 |  0:21:15s
epoch 80 | loss: 0.70113 | val_0_rmse: 0.83673 | val_1_rmse: 0.83304 |  0:21:31s
epoch 81 | loss: 0.69991 | val_0_rmse: 0.83764 | val_1_rmse: 0.8346  |  0:21:47s
epoch 82 | loss: 0.70012 | val_0_rmse: 0.83639 | val_1_rmse: 0.83301 |  0:22:03s
epoch 83 | loss: 0.7004  | val_0_rmse: 0.83649 | val_1_rmse: 0.83348 |  0:22:19s
epoch 84 | loss: 0.70056 | val_0_rmse: 0.83626 | val_1_rmse: 0.83328 |  0:22:35s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.8323
Best weights from best epoch are automatically used!
ended training at: 08:21:03
Feature importance:
Mean squared error is of 4518142921.535862
Mean absolute error:51383.37244005955
MAPE:0.5390841936638062
R2 score:0.3056735027646592
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:21:06
epoch 0  | loss: 0.88323 | val_0_rmse: 0.94653 | val_1_rmse: 0.94833 |  0:00:15s
epoch 1  | loss: 0.73761 | val_0_rmse: 0.90194 | val_1_rmse: 0.90439 |  0:00:31s
epoch 2  | loss: 0.72777 | val_0_rmse: 0.86249 | val_1_rmse: 0.86631 |  0:00:47s
epoch 3  | loss: 0.72437 | val_0_rmse: 0.84799 | val_1_rmse: 0.85046 |  0:01:03s
epoch 4  | loss: 0.71394 | val_0_rmse: 0.84342 | val_1_rmse: 0.84572 |  0:01:19s
epoch 5  | loss: 0.71294 | val_0_rmse: 0.83819 | val_1_rmse: 0.84182 |  0:01:35s
epoch 6  | loss: 0.70609 | val_0_rmse: 0.84219 | val_1_rmse: 0.84653 |  0:01:51s
epoch 7  | loss: 0.70579 | val_0_rmse: 0.83713 | val_1_rmse: 0.8401  |  0:02:07s
epoch 8  | loss: 0.70319 | val_0_rmse: 0.83515 | val_1_rmse: 0.83884 |  0:02:23s
epoch 9  | loss: 0.70173 | val_0_rmse: 0.83624 | val_1_rmse: 0.84073 |  0:02:39s
epoch 10 | loss: 0.70136 | val_0_rmse: 0.83657 | val_1_rmse: 0.84196 |  0:02:55s
epoch 11 | loss: 0.70018 | val_0_rmse: 0.83479 | val_1_rmse: 0.83931 |  0:03:11s
epoch 12 | loss: 0.69926 | val_0_rmse: 0.83551 | val_1_rmse: 0.83892 |  0:03:27s
epoch 13 | loss: 0.69927 | val_0_rmse: 0.83512 | val_1_rmse: 0.83958 |  0:03:43s
epoch 14 | loss: 0.69754 | val_0_rmse: 0.83546 | val_1_rmse: 0.83953 |  0:03:59s
epoch 15 | loss: 0.69804 | val_0_rmse: 0.8353  | val_1_rmse: 0.83913 |  0:04:14s
epoch 16 | loss: 0.69757 | val_0_rmse: 0.83591 | val_1_rmse: 0.84097 |  0:04:30s
epoch 17 | loss: 0.69908 | val_0_rmse: 0.8335  | val_1_rmse: 0.83939 |  0:04:46s
epoch 18 | loss: 0.69915 | val_0_rmse: 0.83611 | val_1_rmse: 0.8413  |  0:05:02s
epoch 19 | loss: 0.70024 | val_0_rmse: 0.92035 | val_1_rmse: 0.84228 |  0:05:18s
epoch 20 | loss: 0.69951 | val_0_rmse: 0.83542 | val_1_rmse: 0.83898 |  0:05:34s
epoch 21 | loss: 0.71887 | val_0_rmse: 1.1619  | val_1_rmse: 0.88989 |  0:05:50s
epoch 22 | loss: 0.72599 | val_0_rmse: 0.84876 | val_1_rmse: 0.84408 |  0:06:06s
epoch 23 | loss: 0.70445 | val_0_rmse: 0.83474 | val_1_rmse: 0.83876 |  0:06:22s
epoch 24 | loss: 0.70022 | val_0_rmse: 0.83429 | val_1_rmse: 0.83703 |  0:06:38s
epoch 25 | loss: 0.69785 | val_0_rmse: 0.83342 | val_1_rmse: 0.83626 |  0:06:54s
epoch 26 | loss: 0.69752 | val_0_rmse: 0.83279 | val_1_rmse: 0.83665 |  0:07:10s
epoch 27 | loss: 0.69582 | val_0_rmse: 0.83199 | val_1_rmse: 0.83568 |  0:07:26s
epoch 28 | loss: 0.70354 | val_0_rmse: 0.84756 | val_1_rmse: 0.86873 |  0:07:42s
epoch 29 | loss: 0.69795 | val_0_rmse: 0.86854 | val_1_rmse: 0.92388 |  0:07:58s
epoch 30 | loss: 0.69683 | val_0_rmse: 0.86456 | val_1_rmse: 0.9086  |  0:08:14s
epoch 31 | loss: 0.69707 | val_0_rmse: 0.85998 | val_1_rmse: 0.90308 |  0:08:30s
epoch 32 | loss: 0.69772 | val_0_rmse: 0.84015 | val_1_rmse: 0.84748 |  0:08:46s
epoch 33 | loss: 0.69674 | val_0_rmse: 0.86318 | val_1_rmse: 0.90805 |  0:09:02s
epoch 34 | loss: 0.69629 | val_0_rmse: 0.95942 | val_1_rmse: 1.11522 |  0:09:18s
epoch 35 | loss: 0.69607 | val_0_rmse: 0.83448 | val_1_rmse: 0.83809 |  0:09:34s
epoch 36 | loss: 0.69606 | val_0_rmse: 0.84326 | val_1_rmse: 0.83851 |  0:09:50s
epoch 37 | loss: 0.71213 | val_0_rmse: 0.84013 | val_1_rmse: 0.84539 |  0:10:06s
epoch 38 | loss: 0.70377 | val_0_rmse: 4.95596 | val_1_rmse: 7.59872 |  0:10:22s
epoch 39 | loss: 0.70293 | val_0_rmse: 0.89068 | val_1_rmse: 0.97277 |  0:10:38s
epoch 40 | loss: 0.69801 | val_0_rmse: 0.88858 | val_1_rmse: 0.94745 |  0:10:54s
epoch 41 | loss: 0.69687 | val_0_rmse: 1.95833 | val_1_rmse: 2.8063  |  0:11:10s
epoch 42 | loss: 0.69606 | val_0_rmse: 0.83551 | val_1_rmse: 0.83764 |  0:11:26s
epoch 43 | loss: 0.69796 | val_0_rmse: 0.88155 | val_1_rmse: 0.94042 |  0:11:42s
epoch 44 | loss: 0.69598 | val_0_rmse: 0.9155  | val_1_rmse: 1.02465 |  0:11:58s
epoch 45 | loss: 0.69564 | val_0_rmse: 0.84756 | val_1_rmse: 0.86968 |  0:12:13s
epoch 46 | loss: 0.69435 | val_0_rmse: 0.85037 | val_1_rmse: 0.88452 |  0:12:29s
epoch 47 | loss: 0.69329 | val_0_rmse: 0.84001 | val_1_rmse: 0.85672 |  0:12:45s
epoch 48 | loss: 0.69281 | val_0_rmse: 0.83155 | val_1_rmse: 0.83786 |  0:13:01s
epoch 49 | loss: 0.70386 | val_0_rmse: 0.85272 | val_1_rmse: 0.84215 |  0:13:17s
epoch 50 | loss: 0.70143 | val_0_rmse: 0.837   | val_1_rmse: 0.83944 |  0:13:33s
epoch 51 | loss: 0.69593 | val_0_rmse: 0.84434 | val_1_rmse: 0.83787 |  0:13:49s
epoch 52 | loss: 0.69604 | val_0_rmse: 0.85355 | val_1_rmse: 0.83787 |  0:14:05s
epoch 53 | loss: 0.69498 | val_0_rmse: 0.86442 | val_1_rmse: 0.8352  |  0:14:21s
epoch 54 | loss: 0.6941  | val_0_rmse: 0.87315 | val_1_rmse: 0.83667 |  0:14:37s
epoch 55 | loss: 0.69475 | val_0_rmse: 0.86178 | val_1_rmse: 0.83705 |  0:14:53s
epoch 56 | loss: 0.69447 | val_0_rmse: 0.85476 | val_1_rmse: 0.83748 |  0:15:09s
epoch 57 | loss: 0.69363 | val_0_rmse: 0.84471 | val_1_rmse: 0.83765 |  0:15:25s
epoch 58 | loss: 0.69382 | val_0_rmse: 0.85317 | val_1_rmse: 0.83623 |  0:15:40s
epoch 59 | loss: 0.69498 | val_0_rmse: 0.86135 | val_1_rmse: 0.83594 |  0:15:56s
epoch 60 | loss: 0.69486 | val_0_rmse: 0.85812 | val_1_rmse: 0.83649 |  0:16:12s
epoch 61 | loss: 0.69433 | val_0_rmse: 0.85916 | val_1_rmse: 0.83959 |  0:16:29s
epoch 62 | loss: 0.69542 | val_0_rmse: 0.85308 | val_1_rmse: 0.84036 |  0:16:45s
epoch 63 | loss: 0.69475 | val_0_rmse: 1.01765 | val_1_rmse: 0.83819 |  0:17:00s
epoch 64 | loss: 0.69469 | val_0_rmse: 0.83791 | val_1_rmse: 0.84193 |  0:17:16s
epoch 65 | loss: 0.69514 | val_0_rmse: 0.83651 | val_1_rmse: 0.83757 |  0:17:32s
epoch 66 | loss: 0.6948  | val_0_rmse: 0.84752 | val_1_rmse: 0.83732 |  0:17:48s
epoch 67 | loss: 0.69471 | val_0_rmse: 0.83935 | val_1_rmse: 0.8366  |  0:18:03s
epoch 68 | loss: 0.69695 | val_0_rmse: 0.83536 | val_1_rmse: 0.83829 |  0:18:18s
epoch 69 | loss: 0.70949 | val_0_rmse: 0.87934 | val_1_rmse: 0.84286 |  0:18:33s
epoch 70 | loss: 0.69961 | val_0_rmse: 0.86935 | val_1_rmse: 0.83918 |  0:18:48s
epoch 71 | loss: 0.69815 | val_0_rmse: 0.85517 | val_1_rmse: 0.83881 |  0:19:03s
epoch 72 | loss: 0.69699 | val_0_rmse: 0.86421 | val_1_rmse: 0.84274 |  0:19:18s
epoch 73 | loss: 0.69712 | val_0_rmse: 0.86737 | val_1_rmse: 0.839   |  0:19:33s
epoch 74 | loss: 0.69578 | val_0_rmse: 0.86347 | val_1_rmse: 0.8387  |  0:19:48s
epoch 75 | loss: 0.69617 | val_0_rmse: 0.85472 | val_1_rmse: 0.83881 |  0:20:03s
epoch 76 | loss: 0.69607 | val_0_rmse: 0.85748 | val_1_rmse: 0.83794 |  0:20:18s
epoch 77 | loss: 0.69551 | val_0_rmse: 0.84635 | val_1_rmse: 0.83858 |  0:20:33s
epoch 78 | loss: 0.69589 | val_0_rmse: 0.97458 | val_1_rmse: 0.83924 |  0:20:47s
epoch 79 | loss: 0.69524 | val_0_rmse: 0.92249 | val_1_rmse: 0.83933 |  0:21:03s
epoch 80 | loss: 0.69482 | val_0_rmse: 0.93385 | val_1_rmse: 0.838   |  0:21:18s
epoch 81 | loss: 0.69538 | val_0_rmse: 0.89805 | val_1_rmse: 0.83891 |  0:21:32s
epoch 82 | loss: 0.69527 | val_0_rmse: 0.91111 | val_1_rmse: 0.83885 |  0:21:47s
epoch 83 | loss: 0.69572 | val_0_rmse: 1.0794  | val_1_rmse: 0.83839 |  0:22:02s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.8352
Best weights from best epoch are automatically used!
ended training at: 08:43:14
Feature importance:
Mean squared error is of 5017720523.14025
Mean absolute error:52435.266353638886
MAPE:0.5630285456813581
R2 score:0.24244081763283531
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:43:17
epoch 0  | loss: 0.89833 | val_0_rmse: 0.91499 | val_1_rmse: 0.90971 |  0:00:14s
epoch 1  | loss: 0.73802 | val_0_rmse: 0.87973 | val_1_rmse: 0.87267 |  0:00:29s
epoch 2  | loss: 0.72815 | val_0_rmse: 0.8613  | val_1_rmse: 0.85482 |  0:00:44s
epoch 3  | loss: 0.72358 | val_0_rmse: 0.85006 | val_1_rmse: 0.84584 |  0:00:59s
epoch 4  | loss: 0.71584 | val_0_rmse: 0.85068 | val_1_rmse: 0.84623 |  0:01:14s
epoch 5  | loss: 0.71828 | val_0_rmse: 0.8415  | val_1_rmse: 0.83901 |  0:01:29s
epoch 6  | loss: 0.71111 | val_0_rmse: 0.845   | val_1_rmse: 0.84143 |  0:01:44s
epoch 7  | loss: 0.70752 | val_0_rmse: 0.84608 | val_1_rmse: 0.8438  |  0:01:59s
epoch 8  | loss: 0.70879 | val_0_rmse: 0.85357 | val_1_rmse: 0.85166 |  0:02:14s
epoch 9  | loss: 0.7147  | val_0_rmse: 0.8795  | val_1_rmse: 0.87954 |  0:02:30s
epoch 10 | loss: 0.71082 | val_0_rmse: 0.85966 | val_1_rmse: 0.93696 |  0:02:45s
epoch 11 | loss: 0.70744 | val_0_rmse: 0.86038 | val_1_rmse: 0.86599 |  0:03:00s
epoch 12 | loss: 0.70696 | val_0_rmse: 0.85495 | val_1_rmse: 0.85247 |  0:03:15s
epoch 13 | loss: 0.70612 | val_0_rmse: 0.84371 | val_1_rmse: 0.84253 |  0:03:30s
epoch 14 | loss: 0.70406 | val_0_rmse: 0.83659 | val_1_rmse: 0.83322 |  0:03:45s
epoch 15 | loss: 0.70201 | val_0_rmse: 0.83679 | val_1_rmse: 0.83314 |  0:04:00s
epoch 16 | loss: 0.70064 | val_0_rmse: 0.83537 | val_1_rmse: 0.83172 |  0:04:15s
epoch 17 | loss: 0.69941 | val_0_rmse: 0.83562 | val_1_rmse: 0.84118 |  0:04:30s
epoch 18 | loss: 0.70016 | val_0_rmse: 0.83463 | val_1_rmse: 0.83922 |  0:04:45s
epoch 19 | loss: 0.70242 | val_0_rmse: 0.83834 | val_1_rmse: 0.84538 |  0:05:00s
epoch 20 | loss: 0.70018 | val_0_rmse: 0.86164 | val_1_rmse: 0.88321 |  0:05:15s
epoch 21 | loss: 0.69998 | val_0_rmse: 0.87878 | val_1_rmse: 0.90843 |  0:05:31s
epoch 22 | loss: 0.70011 | val_0_rmse: 0.8786  | val_1_rmse: 0.90646 |  0:05:46s
epoch 23 | loss: 0.69931 | val_0_rmse: 0.83598 | val_1_rmse: 0.84071 |  0:06:01s
epoch 24 | loss: 0.69889 | val_0_rmse: 0.8347  | val_1_rmse: 0.84293 |  0:06:16s
epoch 25 | loss: 0.69898 | val_0_rmse: 0.83477 | val_1_rmse: 0.83935 |  0:06:31s
epoch 26 | loss: 0.70143 | val_0_rmse: 0.83696 | val_1_rmse: 0.83892 |  0:06:46s
epoch 27 | loss: 0.7     | val_0_rmse: 0.87136 | val_1_rmse: 0.84025 |  0:07:01s
epoch 28 | loss: 0.70018 | val_0_rmse: 0.89883 | val_1_rmse: 0.8371  |  0:07:16s
epoch 29 | loss: 0.70624 | val_0_rmse: 0.83917 | val_1_rmse: 0.8626  |  0:07:31s
epoch 30 | loss: 0.71173 | val_0_rmse: 0.84301 | val_1_rmse: 0.84027 |  0:07:46s
epoch 31 | loss: 0.70759 | val_0_rmse: 0.83777 | val_1_rmse: 0.83475 |  0:08:01s
epoch 32 | loss: 0.70103 | val_0_rmse: 0.83733 | val_1_rmse: 0.83306 |  0:08:17s
epoch 33 | loss: 0.69901 | val_0_rmse: 0.83701 | val_1_rmse: 0.83298 |  0:08:32s
epoch 34 | loss: 0.69848 | val_0_rmse: 0.83793 | val_1_rmse: 0.83214 |  0:08:47s
epoch 35 | loss: 0.69745 | val_0_rmse: 0.83687 | val_1_rmse: 0.83157 |  0:09:02s
epoch 36 | loss: 0.70487 | val_0_rmse: 0.85046 | val_1_rmse: 1.05686 |  0:09:17s
epoch 37 | loss: 0.71111 | val_0_rmse: 0.84704 | val_1_rmse: 0.84278 |  0:09:32s
epoch 38 | loss: 0.71329 | val_0_rmse: 0.84556 | val_1_rmse: 1.07546 |  0:09:47s
epoch 39 | loss: 0.71079 | val_0_rmse: 0.84435 | val_1_rmse: 0.93187 |  0:10:02s
epoch 40 | loss: 0.70236 | val_0_rmse: 0.83871 | val_1_rmse: 0.90438 |  0:10:17s
epoch 41 | loss: 0.69879 | val_0_rmse: 0.83831 | val_1_rmse: 0.93685 |  0:10:33s
epoch 42 | loss: 0.70628 | val_0_rmse: 0.8435  | val_1_rmse: 0.99294 |  0:10:48s
epoch 43 | loss: 0.70279 | val_0_rmse: 0.84007 | val_1_rmse: 0.83436 |  0:11:03s
epoch 44 | loss: 0.70014 | val_0_rmse: 0.83943 | val_1_rmse: 0.83538 |  0:11:19s
epoch 45 | loss: 0.70016 | val_0_rmse: 0.83844 | val_1_rmse: 0.92989 |  0:11:34s
epoch 46 | loss: 0.69801 | val_0_rmse: 0.83751 | val_1_rmse: 0.83268 |  0:11:49s
epoch 47 | loss: 0.69745 | val_0_rmse: 0.83859 | val_1_rmse: 0.83516 |  0:12:04s
epoch 48 | loss: 0.69777 | val_0_rmse: 0.83631 | val_1_rmse: 0.83347 |  0:12:19s
epoch 49 | loss: 0.69701 | val_0_rmse: 0.83653 | val_1_rmse: 1.05776 |  0:12:34s
epoch 50 | loss: 0.69793 | val_0_rmse: 0.83591 | val_1_rmse: 1.09058 |  0:12:49s
epoch 51 | loss: 0.69829 | val_0_rmse: 0.83539 | val_1_rmse: 0.83365 |  0:13:04s
epoch 52 | loss: 0.696   | val_0_rmse: 0.83621 | val_1_rmse: 0.83368 |  0:13:19s
epoch 53 | loss: 0.69656 | val_0_rmse: 0.8353  | val_1_rmse: 0.83301 |  0:13:34s
epoch 54 | loss: 0.69639 | val_0_rmse: 0.83773 | val_1_rmse: 0.83548 |  0:13:49s
epoch 55 | loss: 0.69861 | val_0_rmse: 0.83635 | val_1_rmse: 0.83522 |  0:14:04s
epoch 56 | loss: 0.69586 | val_0_rmse: 0.83405 | val_1_rmse: 0.83278 |  0:14:19s
epoch 57 | loss: 0.69789 | val_0_rmse: 0.83509 | val_1_rmse: 0.83414 |  0:14:34s
epoch 58 | loss: 0.69712 | val_0_rmse: 0.83479 | val_1_rmse: 0.83416 |  0:14:49s
epoch 59 | loss: 0.69602 | val_0_rmse: 0.83461 | val_1_rmse: 0.83199 |  0:15:05s
epoch 60 | loss: 0.69558 | val_0_rmse: 0.83356 | val_1_rmse: 0.83255 |  0:15:20s
epoch 61 | loss: 0.69554 | val_0_rmse: 0.83753 | val_1_rmse: 0.83749 |  0:15:35s
epoch 62 | loss: 0.69769 | val_0_rmse: 0.83494 | val_1_rmse: 0.83364 |  0:15:50s
epoch 63 | loss: 0.69973 | val_0_rmse: 0.83457 | val_1_rmse: 0.8336  |  0:16:05s
epoch 64 | loss: 0.69925 | val_0_rmse: 0.83834 | val_1_rmse: 0.83317 |  0:16:20s
epoch 65 | loss: 0.69811 | val_0_rmse: 0.83464 | val_1_rmse: 0.83311 |  0:16:35s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.83157
Best weights from best epoch are automatically used!
ended training at: 08:59:58
Feature importance:
Mean squared error is of 4648735643.500098
Mean absolute error:52160.506938080376
MAPE:0.5553645583449295
R2 score:0.30326320084020075
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:00:01
epoch 0  | loss: 0.87276 | val_0_rmse: 0.93061 | val_1_rmse: 0.92914 |  0:00:15s
epoch 1  | loss: 0.74543 | val_0_rmse: 0.9006  | val_1_rmse: 0.89767 |  0:00:30s
epoch 2  | loss: 0.76354 | val_0_rmse: 0.89395 | val_1_rmse: 0.89115 |  0:00:45s
epoch 3  | loss: 0.75525 | val_0_rmse: 0.85965 | val_1_rmse: 0.85673 |  0:01:00s
epoch 4  | loss: 0.7488  | val_0_rmse: 0.88488 | val_1_rmse: 0.88386 |  0:01:15s
epoch 5  | loss: 0.76732 | val_0_rmse: 0.86341 | val_1_rmse: 0.86035 |  0:01:30s
epoch 6  | loss: 0.74519 | val_0_rmse: 0.86535 | val_1_rmse: 0.86527 |  0:01:45s
epoch 7  | loss: 0.74763 | val_0_rmse: 0.88388 | val_1_rmse: 0.88248 |  0:02:01s
epoch 8  | loss: 0.74138 | val_0_rmse: 0.85787 | val_1_rmse: 0.85676 |  0:02:16s
epoch 9  | loss: 0.73205 | val_0_rmse: 0.85213 | val_1_rmse: 0.85239 |  0:02:31s
epoch 10 | loss: 0.7252  | val_0_rmse: 0.84929 | val_1_rmse: 0.84911 |  0:02:46s
epoch 11 | loss: 0.72566 | val_0_rmse: 0.85032 | val_1_rmse: 0.84946 |  0:03:01s
epoch 12 | loss: 0.72209 | val_0_rmse: 0.85098 | val_1_rmse: 0.85109 |  0:03:16s
epoch 13 | loss: 0.72066 | val_0_rmse: 0.84514 | val_1_rmse: 0.84524 |  0:03:31s
epoch 14 | loss: 0.72099 | val_0_rmse: 0.85463 | val_1_rmse: 0.85065 |  0:03:46s
epoch 15 | loss: 0.71754 | val_0_rmse: 0.91813 | val_1_rmse: 0.84325 |  0:04:01s
epoch 16 | loss: 0.71503 | val_0_rmse: 0.84415 | val_1_rmse: 0.84388 |  0:04:16s
epoch 17 | loss: 0.71389 | val_0_rmse: 0.84276 | val_1_rmse: 0.84296 |  0:04:31s
epoch 18 | loss: 0.71117 | val_0_rmse: 0.84903 | val_1_rmse: 0.84267 |  0:04:46s
epoch 19 | loss: 0.71028 | val_0_rmse: 0.84268 | val_1_rmse: 0.84337 |  0:05:01s
epoch 20 | loss: 0.70951 | val_0_rmse: 0.83902 | val_1_rmse: 0.83972 |  0:05:16s
epoch 21 | loss: 0.70751 | val_0_rmse: 0.83977 | val_1_rmse: 0.83985 |  0:05:31s
epoch 22 | loss: 0.70774 | val_0_rmse: 0.84382 | val_1_rmse: 0.84481 |  0:05:47s
epoch 23 | loss: 0.70796 | val_0_rmse: 0.83796 | val_1_rmse: 0.83842 |  0:06:02s
epoch 24 | loss: 0.7073  | val_0_rmse: 0.84343 | val_1_rmse: 0.84436 |  0:06:17s
epoch 25 | loss: 0.70662 | val_0_rmse: 0.84008 | val_1_rmse: 0.84086 |  0:06:32s
epoch 26 | loss: 0.70467 | val_0_rmse: 0.8403  | val_1_rmse: 0.84041 |  0:06:47s
epoch 27 | loss: 0.70428 | val_0_rmse: 0.84095 | val_1_rmse: 0.8406  |  0:07:02s
epoch 28 | loss: 0.70409 | val_0_rmse: 0.83939 | val_1_rmse: 0.83947 |  0:07:17s
epoch 29 | loss: 0.70416 | val_0_rmse: 0.84689 | val_1_rmse: 0.84862 |  0:07:33s
epoch 30 | loss: 0.70424 | val_0_rmse: 0.83799 | val_1_rmse: 0.83899 |  0:07:48s
epoch 31 | loss: 0.70341 | val_0_rmse: 0.84196 | val_1_rmse: 0.84368 |  0:08:03s
epoch 32 | loss: 0.71271 | val_0_rmse: 0.83872 | val_1_rmse: 0.83928 |  0:08:18s
epoch 33 | loss: 0.70651 | val_0_rmse: 0.8376  | val_1_rmse: 0.83808 |  0:08:33s
epoch 34 | loss: 0.70433 | val_0_rmse: 0.83784 | val_1_rmse: 0.83965 |  0:08:48s
epoch 35 | loss: 0.70566 | val_0_rmse: 0.84225 | val_1_rmse: 0.84321 |  0:09:04s
epoch 36 | loss: 0.70757 | val_0_rmse: 0.83889 | val_1_rmse: 0.84022 |  0:09:19s
epoch 37 | loss: 0.70277 | val_0_rmse: 0.83703 | val_1_rmse: 0.83907 |  0:09:34s
epoch 38 | loss: 0.70301 | val_0_rmse: 0.88992 | val_1_rmse: 0.8931  |  0:09:49s
epoch 39 | loss: 0.70428 | val_0_rmse: 0.8374  | val_1_rmse: 0.83885 |  0:10:05s
epoch 40 | loss: 0.7033  | val_0_rmse: 0.84019 | val_1_rmse: 0.84201 |  0:10:20s
epoch 41 | loss: 0.70276 | val_0_rmse: 0.83591 | val_1_rmse: 0.83674 |  0:10:35s
epoch 42 | loss: 0.70264 | val_0_rmse: 0.84652 | val_1_rmse: 0.84898 |  0:10:50s
epoch 43 | loss: 0.7021  | val_0_rmse: 0.87    | val_1_rmse: 0.86987 |  0:11:05s
epoch 44 | loss: 0.70274 | val_0_rmse: 0.83989 | val_1_rmse: 0.84193 |  0:11:20s
epoch 45 | loss: 0.70174 | val_0_rmse: 0.83852 | val_1_rmse: 0.84117 |  0:11:35s
epoch 46 | loss: 0.70041 | val_0_rmse: 0.83965 | val_1_rmse: 0.84207 |  0:11:50s
epoch 47 | loss: 0.70046 | val_0_rmse: 0.84468 | val_1_rmse: 0.84506 |  0:12:05s
epoch 48 | loss: 0.69921 | val_0_rmse: 0.8415  | val_1_rmse: 0.84287 |  0:12:20s
epoch 49 | loss: 0.69853 | val_0_rmse: 0.83801 | val_1_rmse: 0.83937 |  0:12:35s
epoch 50 | loss: 0.69877 | val_0_rmse: 0.83811 | val_1_rmse: 0.83947 |  0:12:50s
epoch 51 | loss: 0.69956 | val_0_rmse: 0.89521 | val_1_rmse: 0.90053 |  0:13:05s
epoch 52 | loss: 0.69888 | val_0_rmse: 0.83908 | val_1_rmse: 0.84181 |  0:13:20s
epoch 53 | loss: 0.70056 | val_0_rmse: 0.83756 | val_1_rmse: 0.83962 |  0:13:35s
epoch 54 | loss: 0.69808 | val_0_rmse: 0.83714 | val_1_rmse: 0.84022 |  0:13:50s
epoch 55 | loss: 0.69793 | val_0_rmse: 0.86289 | val_1_rmse: 0.86678 |  0:14:05s
epoch 56 | loss: 0.70758 | val_0_rmse: 0.92679 | val_1_rmse: 0.89116 |  0:14:20s
epoch 57 | loss: 0.71927 | val_0_rmse: 0.84824 | val_1_rmse: 0.84859 |  0:14:35s
epoch 58 | loss: 0.71567 | val_0_rmse: 0.84651 | val_1_rmse: 0.84698 |  0:14:50s
epoch 59 | loss: 0.71133 | val_0_rmse: 0.8621  | val_1_rmse: 0.8644  |  0:15:05s
epoch 60 | loss: 0.70736 | val_0_rmse: 0.8395  | val_1_rmse: 0.84056 |  0:15:20s
epoch 61 | loss: 0.70444 | val_0_rmse: 0.84881 | val_1_rmse: 0.85165 |  0:15:35s
epoch 62 | loss: 0.70232 | val_0_rmse: 0.85365 | val_1_rmse: 0.85659 |  0:15:50s
epoch 63 | loss: 0.70167 | val_0_rmse: 0.8358  | val_1_rmse: 0.83727 |  0:16:05s
epoch 64 | loss: 0.7016  | val_0_rmse: 0.84409 | val_1_rmse: 0.84649 |  0:16:20s
epoch 65 | loss: 0.70032 | val_0_rmse: 0.83787 | val_1_rmse: 0.83842 |  0:16:35s
epoch 66 | loss: 0.7005  | val_0_rmse: 0.84041 | val_1_rmse: 0.84091 |  0:16:50s
epoch 67 | loss: 0.70082 | val_0_rmse: 0.85583 | val_1_rmse: 0.85606 |  0:17:05s
epoch 68 | loss: 0.69974 | val_0_rmse: 0.83984 | val_1_rmse: 0.84033 |  0:17:20s
epoch 69 | loss: 0.69958 | val_0_rmse: 0.83828 | val_1_rmse: 0.84011 |  0:17:35s
epoch 70 | loss: 0.70031 | val_0_rmse: 0.83618 | val_1_rmse: 0.83768 |  0:17:50s
epoch 71 | loss: 0.70117 | val_0_rmse: 0.85331 | val_1_rmse: 0.8566  |  0:18:05s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.83674
Best weights from best epoch are automatically used!
ended training at: 09:18:12
Feature importance:
Mean squared error is of 4525654620.865565
Mean absolute error:51612.200204394074
MAPE:0.5446271609363068
R2 score:0.30683321574116973
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:18:14
epoch 0  | loss: 0.88369 | val_0_rmse: 0.97594 | val_1_rmse: 0.97443 |  0:00:15s
epoch 1  | loss: 0.73546 | val_0_rmse: 0.88669 | val_1_rmse: 0.88759 |  0:00:30s
epoch 2  | loss: 0.71662 | val_0_rmse: 0.86334 | val_1_rmse: 0.86277 |  0:00:45s
epoch 3  | loss: 0.71222 | val_0_rmse: 0.85467 | val_1_rmse: 0.85638 |  0:01:00s
epoch 4  | loss: 0.70953 | val_0_rmse: 0.83862 | val_1_rmse: 0.84031 |  0:01:15s
epoch 5  | loss: 0.70663 | val_0_rmse: 0.83686 | val_1_rmse: 0.83956 |  0:01:30s
epoch 6  | loss: 0.70258 | val_0_rmse: 0.83491 | val_1_rmse: 0.83758 |  0:01:45s
epoch 7  | loss: 0.7019  | val_0_rmse: 0.83603 | val_1_rmse: 0.84037 |  0:02:01s
epoch 8  | loss: 0.70295 | val_0_rmse: 0.83638 | val_1_rmse: 0.8379  |  0:02:16s
epoch 9  | loss: 0.70146 | val_0_rmse: 0.83401 | val_1_rmse: 0.83695 |  0:02:31s
epoch 10 | loss: 0.69944 | val_0_rmse: 0.98083 | val_1_rmse: 1.04388 |  0:02:46s
epoch 11 | loss: 0.6986  | val_0_rmse: 0.83426 | val_1_rmse: 0.83706 |  0:03:01s
epoch 12 | loss: 0.69859 | val_0_rmse: 0.83809 | val_1_rmse: 0.84087 |  0:03:16s
epoch 13 | loss: 0.69772 | val_0_rmse: 0.83553 | val_1_rmse: 0.83914 |  0:03:31s
epoch 14 | loss: 0.69716 | val_0_rmse: 0.83408 | val_1_rmse: 0.83679 |  0:03:46s
epoch 15 | loss: 0.69917 | val_0_rmse: 0.834   | val_1_rmse: 0.83694 |  0:04:01s
epoch 16 | loss: 0.69811 | val_0_rmse: 0.8334  | val_1_rmse: 0.83651 |  0:04:16s
epoch 17 | loss: 0.6976  | val_0_rmse: 0.90902 | val_1_rmse: 0.83709 |  0:04:31s
epoch 18 | loss: 0.69742 | val_0_rmse: 0.83417 | val_1_rmse: 0.83729 |  0:04:47s
epoch 19 | loss: 0.69699 | val_0_rmse: 0.83788 | val_1_rmse: 0.84046 |  0:05:02s
epoch 20 | loss: 0.69951 | val_0_rmse: 0.83458 | val_1_rmse: 0.83667 |  0:05:17s
epoch 21 | loss: 0.70028 | val_0_rmse: 0.83588 | val_1_rmse: 0.8381  |  0:05:32s
epoch 22 | loss: 0.69862 | val_0_rmse: 0.83555 | val_1_rmse: 0.8373  |  0:05:48s
epoch 23 | loss: 0.69903 | val_0_rmse: 0.83413 | val_1_rmse: 0.83671 |  0:06:03s
epoch 24 | loss: 0.69825 | val_0_rmse: 0.83459 | val_1_rmse: 0.83613 |  0:06:18s
epoch 25 | loss: 0.69702 | val_0_rmse: 0.83497 | val_1_rmse: 0.83668 |  0:06:33s
epoch 26 | loss: 0.69713 | val_0_rmse: 0.83418 | val_1_rmse: 0.83615 |  0:06:48s
epoch 27 | loss: 0.69618 | val_0_rmse: 0.83306 | val_1_rmse: 0.83572 |  0:07:03s
epoch 28 | loss: 0.6956  | val_0_rmse: 0.83386 | val_1_rmse: 0.83673 |  0:07:18s
epoch 29 | loss: 0.69577 | val_0_rmse: 0.83323 | val_1_rmse: 0.83542 |  0:07:33s
epoch 30 | loss: 0.69815 | val_0_rmse: 0.83443 | val_1_rmse: 0.83705 |  0:07:48s
epoch 31 | loss: 0.69866 | val_0_rmse: 0.8363  | val_1_rmse: 0.83966 |  0:08:03s
epoch 32 | loss: 0.70131 | val_0_rmse: 0.846   | val_1_rmse: 0.84687 |  0:08:18s
epoch 33 | loss: 0.70865 | val_0_rmse: 0.84059 | val_1_rmse: 0.84199 |  0:08:33s
epoch 34 | loss: 0.70529 | val_0_rmse: 0.84127 | val_1_rmse: 0.84206 |  0:08:48s
epoch 35 | loss: 0.7017  | val_0_rmse: 0.83558 | val_1_rmse: 0.83741 |  0:09:03s
epoch 36 | loss: 0.69906 | val_0_rmse: 0.83509 | val_1_rmse: 0.83758 |  0:09:18s
epoch 37 | loss: 0.69915 | val_0_rmse: 0.835   | val_1_rmse: 0.83753 |  0:09:33s
epoch 38 | loss: 0.69736 | val_0_rmse: 0.83518 | val_1_rmse: 0.8379  |  0:09:48s
epoch 39 | loss: 0.70016 | val_0_rmse: 0.8381  | val_1_rmse: 0.84054 |  0:10:04s
epoch 40 | loss: 0.69981 | val_0_rmse: 0.83512 | val_1_rmse: 0.83697 |  0:10:19s
epoch 41 | loss: 0.69977 | val_0_rmse: 0.83646 | val_1_rmse: 0.83875 |  0:10:34s
epoch 42 | loss: 0.69935 | val_0_rmse: 0.83536 | val_1_rmse: 0.83817 |  0:10:49s
epoch 43 | loss: 0.70074 | val_0_rmse: 0.83876 | val_1_rmse: 0.84072 |  0:11:04s
epoch 44 | loss: 0.70032 | val_0_rmse: 0.83627 | val_1_rmse: 0.83792 |  0:11:19s
epoch 45 | loss: 0.70038 | val_0_rmse: 0.83672 | val_1_rmse: 0.83909 |  0:11:34s
epoch 46 | loss: 0.69991 | val_0_rmse: 0.83628 | val_1_rmse: 0.83854 |  0:11:49s
epoch 47 | loss: 0.7004  | val_0_rmse: 0.83638 | val_1_rmse: 0.83893 |  0:12:04s
epoch 48 | loss: 0.70036 | val_0_rmse: 0.8411  | val_1_rmse: 0.8369  |  0:12:19s
epoch 49 | loss: 0.69807 | val_0_rmse: 0.83515 | val_1_rmse: 0.8379  |  0:12:34s
epoch 50 | loss: 0.69809 | val_0_rmse: 0.83603 | val_1_rmse: 0.83879 |  0:12:49s
epoch 51 | loss: 0.69816 | val_0_rmse: 0.83526 | val_1_rmse: 0.83776 |  0:13:04s
epoch 52 | loss: 0.69806 | val_0_rmse: 0.83624 | val_1_rmse: 0.83866 |  0:13:19s
epoch 53 | loss: 0.699   | val_0_rmse: 0.83784 | val_1_rmse: 0.8403  |  0:13:34s
epoch 54 | loss: 0.69996 | val_0_rmse: 0.83563 | val_1_rmse: 0.83761 |  0:13:49s
epoch 55 | loss: 0.69959 | val_0_rmse: 0.83611 | val_1_rmse: 0.83877 |  0:14:05s
epoch 56 | loss: 0.70016 | val_0_rmse: 0.83501 | val_1_rmse: 0.83737 |  0:14:20s
epoch 57 | loss: 0.6993  | val_0_rmse: 0.83611 | val_1_rmse: 0.83913 |  0:14:35s
epoch 58 | loss: 0.69977 | val_0_rmse: 0.83695 | val_1_rmse: 0.83992 |  0:14:49s
epoch 59 | loss: 0.69968 | val_0_rmse: 0.83497 | val_1_rmse: 0.83754 |  0:15:04s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.83542
Best weights from best epoch are automatically used!
ended training at: 09:33:25
Feature importance:
Mean squared error is of 4669508370.758536
Mean absolute error:52412.45484827196
MAPE:0.5504105795365641
R2 score:0.3037281505156497
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:33:27
epoch 0  | loss: 0.89154 | val_0_rmse: 0.96625 | val_1_rmse: 0.96352 |  0:00:14s
epoch 1  | loss: 0.73479 | val_0_rmse: 0.90133 | val_1_rmse: 0.89916 |  0:00:29s
epoch 2  | loss: 0.7171  | val_0_rmse: 0.86314 | val_1_rmse: 0.86157 |  0:00:45s
epoch 3  | loss: 0.71371 | val_0_rmse: 0.84495 | val_1_rmse: 0.84522 |  0:01:00s
epoch 4  | loss: 0.71073 | val_0_rmse: 0.83858 | val_1_rmse: 0.84016 |  0:01:15s
epoch 5  | loss: 0.7085  | val_0_rmse: 0.84359 | val_1_rmse: 0.8446  |  0:01:30s
epoch 6  | loss: 0.70717 | val_0_rmse: 0.83848 | val_1_rmse: 0.83924 |  0:01:45s
epoch 7  | loss: 0.70509 | val_0_rmse: 0.83714 | val_1_rmse: 0.83968 |  0:02:00s
epoch 8  | loss: 0.70403 | val_0_rmse: 0.83655 | val_1_rmse: 0.83973 |  0:02:15s
epoch 9  | loss: 0.702   | val_0_rmse: 0.84569 | val_1_rmse: 0.84666 |  0:02:30s
epoch 10 | loss: 0.70491 | val_0_rmse: 0.83877 | val_1_rmse: 0.84133 |  0:02:46s
epoch 11 | loss: 0.7053  | val_0_rmse: 0.83763 | val_1_rmse: 0.8411  |  0:03:01s
epoch 12 | loss: 0.7021  | val_0_rmse: 0.85595 | val_1_rmse: 0.85332 |  0:03:16s
epoch 13 | loss: 0.70236 | val_0_rmse: 0.84928 | val_1_rmse: 0.85101 |  0:03:31s
epoch 14 | loss: 0.70312 | val_0_rmse: 0.84025 | val_1_rmse: 0.84665 |  0:03:46s
epoch 15 | loss: 0.70257 | val_0_rmse: 0.84261 | val_1_rmse: 0.84519 |  0:04:01s
epoch 16 | loss: 0.70049 | val_0_rmse: 0.84888 | val_1_rmse: 0.84563 |  0:04:16s
epoch 17 | loss: 0.70182 | val_0_rmse: 0.90312 | val_1_rmse: 1.19836 |  0:04:31s
epoch 18 | loss: 0.7018  | val_0_rmse: 0.94783 | val_1_rmse: 1.3956  |  0:04:46s
epoch 19 | loss: 0.70695 | val_0_rmse: 0.98395 | val_1_rmse: 1.53791 |  0:05:01s
epoch 20 | loss: 0.70645 | val_0_rmse: 0.84144 | val_1_rmse: 0.84984 |  0:05:17s
epoch 21 | loss: 0.70552 | val_0_rmse: 0.85016 | val_1_rmse: 0.86154 |  0:05:32s
epoch 22 | loss: 0.70453 | val_0_rmse: 0.85024 | val_1_rmse: 0.85699 |  0:05:47s
epoch 23 | loss: 0.70481 | val_0_rmse: 0.85593 | val_1_rmse: 0.85719 |  0:06:02s
epoch 24 | loss: 0.70517 | val_0_rmse: 0.86903 | val_1_rmse: 0.86657 |  0:06:17s
epoch 25 | loss: 0.70278 | val_0_rmse: 0.83712 | val_1_rmse: 0.84201 |  0:06:32s
epoch 26 | loss: 0.70013 | val_0_rmse: 0.85549 | val_1_rmse: 0.85671 |  0:06:48s
epoch 27 | loss: 0.69929 | val_0_rmse: 0.84485 | val_1_rmse: 0.84801 |  0:07:03s
epoch 28 | loss: 0.6984  | val_0_rmse: 1.06748 | val_1_rmse: 1.16631 |  0:07:18s
epoch 29 | loss: 0.70419 | val_0_rmse: 0.85167 | val_1_rmse: 0.86565 |  0:07:33s
epoch 30 | loss: 0.69989 | val_0_rmse: 0.85219 | val_1_rmse: 0.86234 |  0:07:48s
epoch 31 | loss: 0.6997  | val_0_rmse: 0.87359 | val_1_rmse: 0.87811 |  0:08:03s
epoch 32 | loss: 0.70458 | val_0_rmse: 0.86024 | val_1_rmse: 0.86832 |  0:08:18s
epoch 33 | loss: 0.70553 | val_0_rmse: 0.84402 | val_1_rmse: 0.85356 |  0:08:33s
epoch 34 | loss: 0.69958 | val_0_rmse: 0.84761 | val_1_rmse: 0.8524  |  0:08:48s
epoch 35 | loss: 0.69915 | val_0_rmse: 0.84289 | val_1_rmse: 0.84983 |  0:09:03s
epoch 36 | loss: 0.69937 | val_0_rmse: 0.84725 | val_1_rmse: 0.85997 |  0:09:18s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.83924
Best weights from best epoch are automatically used!
ended training at: 09:42:51
Feature importance:
Mean squared error is of 4598680112.26765
Mean absolute error:52358.9687914487
MAPE:0.5745326607983543
R2 score:0.2977982649707558
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:42:54
epoch 0  | loss: 0.86281 | val_0_rmse: 0.90663 | val_1_rmse: 0.90489 |  0:00:15s
epoch 1  | loss: 0.72568 | val_0_rmse: 0.88597 | val_1_rmse: 0.88413 |  0:00:30s
epoch 2  | loss: 0.71627 | val_0_rmse: 0.85614 | val_1_rmse: 0.85444 |  0:00:45s
epoch 3  | loss: 0.71397 | val_0_rmse: 0.84534 | val_1_rmse: 0.84307 |  0:01:00s
epoch 4  | loss: 0.71189 | val_0_rmse: 0.84656 | val_1_rmse: 0.84467 |  0:01:15s
epoch 5  | loss: 0.71691 | val_0_rmse: 0.84346 | val_1_rmse: 0.84086 |  0:01:30s
epoch 6  | loss: 0.71012 | val_0_rmse: 0.83726 | val_1_rmse: 0.83464 |  0:01:45s
epoch 7  | loss: 0.7087  | val_0_rmse: 0.84012 | val_1_rmse: 0.83718 |  0:02:00s
epoch 8  | loss: 0.70974 | val_0_rmse: 0.84065 | val_1_rmse: 0.83832 |  0:02:15s
epoch 9  | loss: 0.7087  | val_0_rmse: 0.84331 | val_1_rmse: 0.84087 |  0:02:30s
epoch 10 | loss: 0.7053  | val_0_rmse: 0.8391  | val_1_rmse: 0.83491 |  0:02:45s
epoch 11 | loss: 0.7038  | val_0_rmse: 0.84056 | val_1_rmse: 0.83614 |  0:03:01s
epoch 12 | loss: 0.70092 | val_0_rmse: 0.85229 | val_1_rmse: 0.8449  |  0:03:16s
epoch 13 | loss: 0.69966 | val_0_rmse: 0.85932 | val_1_rmse: 0.84854 |  0:03:31s
epoch 14 | loss: 0.69946 | val_0_rmse: 0.83456 | val_1_rmse: 0.83345 |  0:03:46s
epoch 15 | loss: 0.69955 | val_0_rmse: 0.88313 | val_1_rmse: 0.86145 |  0:04:01s
epoch 16 | loss: 0.69937 | val_0_rmse: 0.91137 | val_1_rmse: 0.8787  |  0:04:16s
epoch 17 | loss: 0.69937 | val_0_rmse: 0.86195 | val_1_rmse: 0.85099 |  0:04:31s
epoch 18 | loss: 0.7017  | val_0_rmse: 0.8485  | val_1_rmse: 0.84237 |  0:04:46s
epoch 19 | loss: 0.70178 | val_0_rmse: 0.83891 | val_1_rmse: 0.83465 |  0:05:01s
epoch 20 | loss: 0.70086 | val_0_rmse: 0.87654 | val_1_rmse: 0.857   |  0:05:16s
epoch 21 | loss: 0.7     | val_0_rmse: 0.86602 | val_1_rmse: 0.85159 |  0:05:31s
epoch 22 | loss: 0.70029 | val_0_rmse: 0.85006 | val_1_rmse: 0.84194 |  0:05:46s
epoch 23 | loss: 0.70175 | val_0_rmse: 0.83945 | val_1_rmse: 0.83702 |  0:06:01s
epoch 24 | loss: 0.70299 | val_0_rmse: 0.84097 | val_1_rmse: 0.83526 |  0:06:16s
epoch 25 | loss: 0.70316 | val_0_rmse: 0.841   | val_1_rmse: 0.83651 |  0:06:32s
epoch 26 | loss: 0.70222 | val_0_rmse: 0.84062 | val_1_rmse: 0.83779 |  0:06:47s
epoch 27 | loss: 0.70243 | val_0_rmse: 0.83653 | val_1_rmse: 0.83332 |  0:07:02s
epoch 28 | loss: 0.703   | val_0_rmse: 0.85953 | val_1_rmse: 0.8579  |  0:07:17s
epoch 29 | loss: 0.70357 | val_0_rmse: 0.84171 | val_1_rmse: 0.8395  |  0:07:32s
epoch 30 | loss: 0.70662 | val_0_rmse: 0.83732 | val_1_rmse: 0.8348  |  0:07:47s
epoch 31 | loss: 0.7013  | val_0_rmse: 0.84957 | val_1_rmse: 0.8423  |  0:08:02s
epoch 32 | loss: 0.70005 | val_0_rmse: 0.83504 | val_1_rmse: 0.83334 |  0:08:17s
epoch 33 | loss: 0.6991  | val_0_rmse: 0.83614 | val_1_rmse: 0.8337  |  0:08:32s
epoch 34 | loss: 0.69906 | val_0_rmse: 0.83994 | val_1_rmse: 0.83226 |  0:08:47s
epoch 35 | loss: 0.6982  | val_0_rmse: 0.83637 | val_1_rmse: 0.83452 |  0:09:02s
epoch 36 | loss: 0.69805 | val_0_rmse: 0.83716 | val_1_rmse: 0.835   |  0:09:17s
epoch 37 | loss: 0.69874 | val_0_rmse: 0.83801 | val_1_rmse: 0.83609 |  0:09:33s
epoch 38 | loss: 0.69876 | val_0_rmse: 0.83612 | val_1_rmse: 0.83438 |  0:09:48s
epoch 39 | loss: 0.69783 | val_0_rmse: 0.84556 | val_1_rmse: 0.84369 |  0:10:03s
epoch 40 | loss: 0.69859 | val_0_rmse: 0.83593 | val_1_rmse: 0.83494 |  0:10:18s
epoch 41 | loss: 0.69823 | val_0_rmse: 0.83675 | val_1_rmse: 0.83491 |  0:10:33s
epoch 42 | loss: 0.69794 | val_0_rmse: 0.83591 | val_1_rmse: 0.8345  |  0:10:48s
epoch 43 | loss: 0.69777 | val_0_rmse: 0.83805 | val_1_rmse: 0.83566 |  0:11:03s
epoch 44 | loss: 0.69725 | val_0_rmse: 0.85369 | val_1_rmse: 0.84027 |  0:11:19s
epoch 45 | loss: 0.69764 | val_0_rmse: 0.8438  | val_1_rmse: 0.8408  |  0:11:34s
epoch 46 | loss: 0.69696 | val_0_rmse: 0.83895 | val_1_rmse: 0.83592 |  0:11:49s
epoch 47 | loss: 0.69699 | val_0_rmse: 0.83469 | val_1_rmse: 0.8324  |  0:12:04s
epoch 48 | loss: 0.69733 | val_0_rmse: 0.84573 | val_1_rmse: 0.84345 |  0:12:19s
epoch 49 | loss: 0.69994 | val_0_rmse: 0.85275 | val_1_rmse: 0.83319 |  0:12:34s
epoch 50 | loss: 0.69936 | val_0_rmse: 0.83867 | val_1_rmse: 0.83575 |  0:12:49s
epoch 51 | loss: 0.69894 | val_0_rmse: 0.83405 | val_1_rmse: 0.83144 |  0:13:05s
epoch 52 | loss: 0.6997  | val_0_rmse: 0.83448 | val_1_rmse: 0.83181 |  0:13:20s
epoch 53 | loss: 0.69846 | val_0_rmse: 0.83396 | val_1_rmse: 0.83139 |  0:13:35s
epoch 54 | loss: 0.69967 | val_0_rmse: 0.84987 | val_1_rmse: 0.84786 |  0:13:50s
epoch 55 | loss: 0.69924 | val_0_rmse: 0.83825 | val_1_rmse: 0.83679 |  0:14:05s
epoch 56 | loss: 0.70463 | val_0_rmse: 0.84019 | val_1_rmse: 0.83751 |  0:14:20s
epoch 57 | loss: 0.70525 | val_0_rmse: 0.84334 | val_1_rmse: 0.84017 |  0:14:35s
epoch 58 | loss: 0.70396 | val_0_rmse: 0.8586  | val_1_rmse: 0.8462  |  0:14:50s
epoch 59 | loss: 0.70265 | val_0_rmse: 0.91213 | val_1_rmse: 0.87915 |  0:15:05s
epoch 60 | loss: 0.70089 | val_0_rmse: 0.84187 | val_1_rmse: 0.83903 |  0:15:20s
epoch 61 | loss: 0.69955 | val_0_rmse: 0.96316 | val_1_rmse: 0.90919 |  0:15:35s
epoch 62 | loss: 0.69926 | val_0_rmse: 0.84316 | val_1_rmse: 0.84494 |  0:15:50s
epoch 63 | loss: 0.69854 | val_0_rmse: 0.92666 | val_1_rmse: 0.89057 |  0:16:05s
epoch 64 | loss: 0.69704 | val_0_rmse: 0.84374 | val_1_rmse: 0.83848 |  0:16:21s
epoch 65 | loss: 0.69813 | val_0_rmse: 0.87764 | val_1_rmse: 0.86146 |  0:16:36s
epoch 66 | loss: 0.69803 | val_0_rmse: 0.86622 | val_1_rmse: 0.87137 |  0:16:51s
epoch 67 | loss: 0.69773 | val_0_rmse: 0.90331 | val_1_rmse: 0.87129 |  0:17:06s
epoch 68 | loss: 0.69774 | val_0_rmse: 0.97035 | val_1_rmse: 0.91352 |  0:17:21s
epoch 69 | loss: 0.69918 | val_0_rmse: 0.96292 | val_1_rmse: 0.91107 |  0:17:36s
epoch 70 | loss: 0.69909 | val_0_rmse: 0.96282 | val_1_rmse: 0.90914 |  0:17:51s
epoch 71 | loss: 0.69691 | val_0_rmse: 1.01311 | val_1_rmse: 0.93929 |  0:18:06s
epoch 72 | loss: 0.6992  | val_0_rmse: 0.98672 | val_1_rmse: 0.92206 |  0:18:21s
epoch 73 | loss: 0.69708 | val_0_rmse: 1.01361 | val_1_rmse: 0.93963 |  0:18:37s
epoch 74 | loss: 0.70043 | val_0_rmse: 0.8367  | val_1_rmse: 0.83387 |  0:18:52s
epoch 75 | loss: 0.70179 | val_0_rmse: 0.84221 | val_1_rmse: 0.83993 |  0:19:07s
epoch 76 | loss: 0.70055 | val_0_rmse: 0.83837 | val_1_rmse: 0.83543 |  0:19:22s
epoch 77 | loss: 0.69694 | val_0_rmse: 0.83768 | val_1_rmse: 0.83655 |  0:19:37s
epoch 78 | loss: 0.69668 | val_0_rmse: 0.8334  | val_1_rmse: 0.83169 |  0:19:52s
epoch 79 | loss: 0.69605 | val_0_rmse: 0.83565 | val_1_rmse: 0.83231 |  0:20:07s
epoch 80 | loss: 0.6962  | val_0_rmse: 0.83476 | val_1_rmse: 0.83118 |  0:20:22s
epoch 81 | loss: 0.69723 | val_0_rmse: 0.83829 | val_1_rmse: 0.83391 |  0:20:37s
epoch 82 | loss: 0.69684 | val_0_rmse: 0.83348 | val_1_rmse: 0.83161 |  0:20:52s
epoch 83 | loss: 0.69735 | val_0_rmse: 0.88591 | val_1_rmse: 0.87631 |  0:21:07s
epoch 84 | loss: 0.69697 | val_0_rmse: 0.99684 | val_1_rmse: 0.93054 |  0:21:22s
epoch 85 | loss: 0.69893 | val_0_rmse: 0.97751 | val_1_rmse: 0.92175 |  0:21:37s
epoch 86 | loss: 0.70768 | val_0_rmse: 0.83635 | val_1_rmse: 0.83521 |  0:21:52s
epoch 87 | loss: 0.70189 | val_0_rmse: 0.84081 | val_1_rmse: 0.84006 |  0:22:07s
epoch 88 | loss: 0.70053 | val_0_rmse: 0.83522 | val_1_rmse: 0.83448 |  0:22:22s
epoch 89 | loss: 0.70023 | val_0_rmse: 0.83638 | val_1_rmse: 0.83563 |  0:22:37s
epoch 90 | loss: 0.70105 | val_0_rmse: 0.84722 | val_1_rmse: 0.84649 |  0:22:52s
epoch 91 | loss: 0.69899 | val_0_rmse: 0.83952 | val_1_rmse: 0.83929 |  0:23:08s
epoch 92 | loss: 0.69812 | val_0_rmse: 0.83332 | val_1_rmse: 0.83258 |  0:23:23s
epoch 93 | loss: 0.69724 | val_0_rmse: 0.83338 | val_1_rmse: 0.83229 |  0:23:38s
epoch 94 | loss: 0.69752 | val_0_rmse: 0.83623 | val_1_rmse: 0.83541 |  0:23:53s
epoch 95 | loss: 0.6977  | val_0_rmse: 0.83471 | val_1_rmse: 0.83413 |  0:24:08s
epoch 96 | loss: 0.69659 | val_0_rmse: 0.83701 | val_1_rmse: 0.836   |  0:24:23s
epoch 97 | loss: 0.69652 | val_0_rmse: 0.83337 | val_1_rmse: 0.83236 |  0:24:38s
epoch 98 | loss: 0.69568 | val_0_rmse: 0.83315 | val_1_rmse: 0.8326  |  0:24:53s
epoch 99 | loss: 0.69627 | val_0_rmse: 0.8343  | val_1_rmse: 0.83349 |  0:25:08s
epoch 100| loss: 0.69674 | val_0_rmse: 0.83357 | val_1_rmse: 0.83245 |  0:25:23s
epoch 101| loss: 0.69572 | val_0_rmse: 0.83394 | val_1_rmse: 0.83322 |  0:25:38s
epoch 102| loss: 0.69625 | val_0_rmse: 0.83544 | val_1_rmse: 0.83452 |  0:25:53s
epoch 103| loss: 0.69671 | val_0_rmse: 0.83678 | val_1_rmse: 0.83588 |  0:26:08s
epoch 104| loss: 0.69691 | val_0_rmse: 0.83375 | val_1_rmse: 0.83258 |  0:26:23s
epoch 105| loss: 0.69666 | val_0_rmse: 0.83366 | val_1_rmse: 0.83361 |  0:26:39s
epoch 106| loss: 0.69712 | val_0_rmse: 0.84663 | val_1_rmse: 0.84615 |  0:26:54s
epoch 107| loss: 0.697   | val_0_rmse: 0.83332 | val_1_rmse: 0.83251 |  0:27:09s
epoch 108| loss: 0.69618 | val_0_rmse: 0.83491 | val_1_rmse: 0.83431 |  0:27:24s
epoch 109| loss: 0.69613 | val_0_rmse: 0.83787 | val_1_rmse: 0.83735 |  0:27:39s
epoch 110| loss: 0.69556 | val_0_rmse: 0.8331  | val_1_rmse: 0.83226 |  0:27:54s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.83118
Best weights from best epoch are automatically used!
ended training at: 10:10:54
Feature importance:
Mean squared error is of 4783245357.566893
Mean absolute error:51793.06773557106
MAPE:0.5421555593102134
R2 score:0.27344927455320844
------------------------------------------------------------------
