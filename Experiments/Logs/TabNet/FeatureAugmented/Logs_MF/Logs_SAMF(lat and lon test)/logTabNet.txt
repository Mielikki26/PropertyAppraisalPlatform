TabNet Logs:

Saving copy of script...
In this script only the South American datasets and its a countinuation of the latitude/longitude testHere the test done is to test the improvement that the new related to lat/lon features provideThe idea is: The new features should be reflected on the lat/lon, so does them existing provide improvements?Shouldnt the neuralnet be able to obtained them automatically?
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:49:44
epoch 0  | loss: 0.94376 | val_0_rmse: 0.93509 | val_1_rmse: 0.9313  |  0:00:10s
epoch 1  | loss: 0.86024 | val_0_rmse: 0.92342 | val_1_rmse: 0.91747 |  0:00:17s
epoch 2  | loss: 0.85432 | val_0_rmse: 0.92492 | val_1_rmse: 0.91949 |  0:00:23s
epoch 3  | loss: 0.85114 | val_0_rmse: 0.92346 | val_1_rmse: 0.91763 |  0:00:30s
epoch 4  | loss: 0.85364 | val_0_rmse: 0.92244 | val_1_rmse: 0.91687 |  0:00:37s
epoch 5  | loss: 0.85252 | val_0_rmse: 0.92599 | val_1_rmse: 0.92037 |  0:00:44s
epoch 6  | loss: 0.85157 | val_0_rmse: 0.92376 | val_1_rmse: 0.91759 |  0:00:52s
epoch 7  | loss: 0.84962 | val_0_rmse: 0.92345 | val_1_rmse: 0.91884 |  0:00:59s
epoch 8  | loss: 0.85769 | val_0_rmse: 0.92407 | val_1_rmse: 0.91803 |  0:01:07s
epoch 9  | loss: 0.85506 | val_0_rmse: 0.92522 | val_1_rmse: 0.92009 |  0:01:15s
epoch 10 | loss: 0.85505 | val_0_rmse: 0.92309 | val_1_rmse: 0.9172  |  0:01:23s
epoch 11 | loss: 0.85186 | val_0_rmse: 0.92238 | val_1_rmse: 0.91677 |  0:01:31s
epoch 12 | loss: 0.85214 | val_0_rmse: 0.92141 | val_1_rmse: 0.91519 |  0:01:39s
epoch 13 | loss: 0.85134 | val_0_rmse: 0.96533 | val_1_rmse: 0.96262 |  0:01:48s
epoch 14 | loss: 0.8527  | val_0_rmse: 0.9226  | val_1_rmse: 0.91718 |  0:01:56s
epoch 15 | loss: 0.85263 | val_0_rmse: 0.92633 | val_1_rmse: 0.92123 |  0:02:05s
epoch 16 | loss: 0.85146 | val_0_rmse: 0.92181 | val_1_rmse: 0.9166  |  0:02:13s
epoch 17 | loss: 0.85057 | val_0_rmse: 0.93037 | val_1_rmse: 0.92382 |  0:02:22s
epoch 18 | loss: 0.86106 | val_0_rmse: 0.92541 | val_1_rmse: 0.92029 |  0:02:30s
epoch 19 | loss: 0.85484 | val_0_rmse: 0.92313 | val_1_rmse: 0.91702 |  0:02:39s
epoch 20 | loss: 0.85135 | val_0_rmse: 0.92203 | val_1_rmse: 0.91677 |  0:02:48s
epoch 21 | loss: 0.85075 | val_0_rmse: 0.92155 | val_1_rmse: 0.91648 |  0:02:57s
epoch 22 | loss: 0.85013 | val_0_rmse: 0.92157 | val_1_rmse: 0.91658 |  0:03:06s
epoch 23 | loss: 0.84962 | val_0_rmse: 0.92172 | val_1_rmse: 0.91657 |  0:03:14s
epoch 24 | loss: 0.84917 | val_0_rmse: 0.92154 | val_1_rmse: 0.9165  |  0:03:23s
epoch 25 | loss: 0.84929 | val_0_rmse: 0.92153 | val_1_rmse: 0.9163  |  0:03:32s
epoch 26 | loss: 0.84974 | val_0_rmse: 0.92167 | val_1_rmse: 0.91659 |  0:03:41s
epoch 27 | loss: 0.84965 | val_0_rmse: 0.92599 | val_1_rmse: 0.91994 |  0:03:50s
epoch 28 | loss: 0.84954 | val_0_rmse: 0.92221 | val_1_rmse: 0.91709 |  0:03:59s
epoch 29 | loss: 0.84938 | val_0_rmse: 0.92091 | val_1_rmse: 0.91566 |  0:04:08s
epoch 30 | loss: 0.84927 | val_0_rmse: 0.92269 | val_1_rmse: 0.91723 |  0:04:17s
epoch 31 | loss: 0.84983 | val_0_rmse: 0.92139 | val_1_rmse: 0.91615 |  0:04:26s
epoch 32 | loss: 0.84958 | val_0_rmse: 0.92236 | val_1_rmse: 0.91657 |  0:04:35s
epoch 33 | loss: 0.84882 | val_0_rmse: 0.92284 | val_1_rmse: 0.91705 |  0:04:44s
epoch 34 | loss: 0.84889 | val_0_rmse: 0.92279 | val_1_rmse: 0.91792 |  0:04:53s
epoch 35 | loss: 0.84891 | val_0_rmse: 0.92613 | val_1_rmse: 0.91997 |  0:05:02s
epoch 36 | loss: 0.84845 | val_0_rmse: 0.91845 | val_1_rmse: 0.91343 |  0:05:11s
epoch 37 | loss: 0.83828 | val_0_rmse: 0.94407 | val_1_rmse: 0.93977 |  0:05:20s
epoch 38 | loss: 0.80407 | val_0_rmse: 0.96088 | val_1_rmse: 0.95795 |  0:05:29s
epoch 39 | loss: 0.79638 | val_0_rmse: 0.88988 | val_1_rmse: 0.88647 |  0:05:38s
epoch 40 | loss: 0.79318 | val_0_rmse: 0.9502  | val_1_rmse: 0.94691 |  0:05:47s
epoch 41 | loss: 0.79606 | val_0_rmse: 0.96731 | val_1_rmse: 0.96393 |  0:05:56s
epoch 42 | loss: 0.80092 | val_0_rmse: 0.9123  | val_1_rmse: 0.90897 |  0:06:05s
epoch 43 | loss: 0.7966  | val_0_rmse: 0.91627 | val_1_rmse: 0.91311 |  0:06:14s
epoch 44 | loss: 0.79139 | val_0_rmse: 0.88991 | val_1_rmse: 0.88654 |  0:06:24s
epoch 45 | loss: 0.78874 | val_0_rmse: 0.93417 | val_1_rmse: 0.9296  |  0:06:33s
epoch 46 | loss: 0.79144 | val_0_rmse: 0.9352  | val_1_rmse: 0.93143 |  0:06:42s
epoch 47 | loss: 0.78963 | val_0_rmse: 0.88772 | val_1_rmse: 0.88557 |  0:06:51s
epoch 48 | loss: 0.78921 | val_0_rmse: 0.91388 | val_1_rmse: 0.91043 |  0:07:00s
epoch 49 | loss: 0.78897 | val_0_rmse: 0.89272 | val_1_rmse: 0.88956 |  0:07:09s
epoch 50 | loss: 0.78904 | val_0_rmse: 0.93394 | val_1_rmse: 0.92993 |  0:07:18s
epoch 51 | loss: 0.78949 | val_0_rmse: 0.90484 | val_1_rmse: 0.90115 |  0:07:27s
epoch 52 | loss: 0.79018 | val_0_rmse: 0.93184 | val_1_rmse: 0.92703 |  0:07:36s
epoch 53 | loss: 0.78862 | val_0_rmse: 0.92602 | val_1_rmse: 0.92177 |  0:07:45s
epoch 54 | loss: 0.79139 | val_0_rmse: 0.93652 | val_1_rmse: 0.9316  |  0:07:54s
epoch 55 | loss: 0.78837 | val_0_rmse: 0.9469  | val_1_rmse: 0.94389 |  0:08:03s
epoch 56 | loss: 0.78863 | val_0_rmse: 0.90316 | val_1_rmse: 0.89955 |  0:08:12s
epoch 57 | loss: 0.78923 | val_0_rmse: 0.95263 | val_1_rmse: 0.94891 |  0:08:21s
epoch 58 | loss: 0.78706 | val_0_rmse: 0.92617 | val_1_rmse: 0.92244 |  0:08:30s
epoch 59 | loss: 0.78979 | val_0_rmse: 0.89286 | val_1_rmse: 0.88957 |  0:08:39s
epoch 60 | loss: 0.78875 | val_0_rmse: 0.94619 | val_1_rmse: 0.94316 |  0:08:48s
epoch 61 | loss: 0.78523 | val_0_rmse: 0.9267  | val_1_rmse: 0.92236 |  0:08:57s
epoch 62 | loss: 0.7834  | val_0_rmse: 0.88378 | val_1_rmse: 0.88139 |  0:09:06s
epoch 63 | loss: 0.78343 | val_0_rmse: 0.91934 | val_1_rmse: 0.9153  |  0:09:16s
epoch 64 | loss: 0.7866  | val_0_rmse: 0.92985 | val_1_rmse: 0.92509 |  0:09:25s
epoch 65 | loss: 0.78519 | val_0_rmse: 0.89285 | val_1_rmse: 0.88879 |  0:09:34s
epoch 66 | loss: 0.78376 | val_0_rmse: 0.91395 | val_1_rmse: 0.91177 |  0:09:43s
epoch 67 | loss: 0.78442 | val_0_rmse: 0.88836 | val_1_rmse: 0.88581 |  0:09:52s
epoch 68 | loss: 0.78351 | val_0_rmse: 0.92288 | val_1_rmse: 0.92107 |  0:10:01s
epoch 69 | loss: 0.78054 | val_0_rmse: 0.88192 | val_1_rmse: 0.87926 |  0:10:10s
epoch 70 | loss: 0.78107 | val_0_rmse: 1.02191 | val_1_rmse: 1.01876 |  0:10:19s
epoch 71 | loss: 0.78711 | val_0_rmse: 0.91396 | val_1_rmse: 0.91094 |  0:10:28s
epoch 72 | loss: 0.78626 | val_0_rmse: 0.92734 | val_1_rmse: 0.92276 |  0:10:37s
epoch 73 | loss: 0.7809  | val_0_rmse: 0.91686 | val_1_rmse: 0.91327 |  0:10:46s
epoch 74 | loss: 0.78158 | val_0_rmse: 0.94815 | val_1_rmse: 0.94432 |  0:10:55s
epoch 75 | loss: 0.78036 | val_0_rmse: 0.95899 | val_1_rmse: 0.95659 |  0:11:04s
epoch 76 | loss: 0.7787  | val_0_rmse: 0.89552 | val_1_rmse: 0.89273 |  0:11:13s
epoch 77 | loss: 0.77842 | val_0_rmse: 0.8849  | val_1_rmse: 0.88275 |  0:11:22s
epoch 78 | loss: 0.77661 | val_0_rmse: 0.94509 | val_1_rmse: 0.94129 |  0:11:31s
epoch 79 | loss: 0.77527 | val_0_rmse: 0.88781 | val_1_rmse: 0.88415 |  0:11:40s
epoch 80 | loss: 0.77504 | val_0_rmse: 0.91923 | val_1_rmse: 0.91483 |  0:11:49s
epoch 81 | loss: 0.77402 | val_0_rmse: 0.96101 | val_1_rmse: 0.95761 |  0:11:58s
epoch 82 | loss: 0.77659 | val_0_rmse: 0.99218 | val_1_rmse: 0.98646 |  0:12:07s
epoch 83 | loss: 0.79835 | val_0_rmse: 0.94425 | val_1_rmse: 0.9393  |  0:12:16s
epoch 84 | loss: 0.79501 | val_0_rmse: 0.90823 | val_1_rmse: 0.90375 |  0:12:26s
epoch 85 | loss: 0.78264 | val_0_rmse: 0.99653 | val_1_rmse: 0.98915 |  0:12:35s
epoch 86 | loss: 0.78095 | val_0_rmse: 0.92078 | val_1_rmse: 0.91761 |  0:12:44s
epoch 87 | loss: 0.77743 | val_0_rmse: 0.88638 | val_1_rmse: 0.88169 |  0:12:53s
epoch 88 | loss: 0.77565 | val_0_rmse: 0.95437 | val_1_rmse: 0.95085 |  0:13:01s
epoch 89 | loss: 0.77259 | val_0_rmse: 0.88817 | val_1_rmse: 0.88399 |  0:13:10s
epoch 90 | loss: 0.77471 | val_0_rmse: 0.91757 | val_1_rmse: 0.91432 |  0:13:20s
epoch 91 | loss: 0.77241 | val_0_rmse: 0.90056 | val_1_rmse: 0.89667 |  0:13:29s
epoch 92 | loss: 0.77499 | val_0_rmse: 0.93478 | val_1_rmse: 0.92866 |  0:13:38s
epoch 93 | loss: 0.77792 | val_0_rmse: 0.88275 | val_1_rmse: 0.87896 |  0:13:47s
epoch 94 | loss: 0.77328 | val_0_rmse: 0.97432 | val_1_rmse: 0.96732 |  0:13:56s
epoch 95 | loss: 0.77195 | val_0_rmse: 0.87385 | val_1_rmse: 0.86868 |  0:14:05s
epoch 96 | loss: 0.76884 | val_0_rmse: 0.97905 | val_1_rmse: 0.97565 |  0:14:14s
epoch 97 | loss: 0.76975 | val_0_rmse: 0.98904 | val_1_rmse: 0.98393 |  0:14:23s
epoch 98 | loss: 0.76802 | val_0_rmse: 0.89251 | val_1_rmse: 0.88781 |  0:14:32s
epoch 99 | loss: 0.76816 | val_0_rmse: 0.87769 | val_1_rmse: 0.87164 |  0:14:41s
epoch 100| loss: 0.76888 | val_0_rmse: 0.87644 | val_1_rmse: 0.87151 |  0:14:50s
epoch 101| loss: 0.76612 | val_0_rmse: 0.96347 | val_1_rmse: 0.96001 |  0:14:59s
epoch 102| loss: 0.76745 | val_0_rmse: 0.90901 | val_1_rmse: 0.90485 |  0:15:08s
epoch 103| loss: 0.76508 | val_0_rmse: 0.914   | val_1_rmse: 0.91059 |  0:15:17s
epoch 104| loss: 0.76556 | val_0_rmse: 0.89969 | val_1_rmse: 0.89591 |  0:15:26s
epoch 105| loss: 0.76473 | val_0_rmse: 0.89801 | val_1_rmse: 0.89307 |  0:15:35s
epoch 106| loss: 0.76232 | val_0_rmse: 0.88864 | val_1_rmse: 0.88291 |  0:15:44s
epoch 107| loss: 0.76563 | val_0_rmse: 0.91008 | val_1_rmse: 0.90545 |  0:15:53s
epoch 108| loss: 0.7623  | val_0_rmse: 0.88452 | val_1_rmse: 0.87974 |  0:16:02s
epoch 109| loss: 0.76217 | val_0_rmse: 0.88094 | val_1_rmse: 0.87859 |  0:16:11s
epoch 110| loss: 0.76469 | val_0_rmse: 0.96121 | val_1_rmse: 0.96006 |  0:16:20s
epoch 111| loss: 0.76149 | val_0_rmse: 0.94641 | val_1_rmse: 0.94663 |  0:16:30s
epoch 112| loss: 0.76286 | val_0_rmse: 0.8791  | val_1_rmse: 0.87321 |  0:16:39s
epoch 113| loss: 0.76282 | val_0_rmse: 0.91488 | val_1_rmse: 0.91118 |  0:16:48s
epoch 114| loss: 0.76222 | val_0_rmse: 0.94193 | val_1_rmse: 0.95567 |  0:16:57s
epoch 115| loss: 0.76303 | val_0_rmse: 0.917   | val_1_rmse: 0.91867 |  0:17:06s
epoch 116| loss: 0.76271 | val_0_rmse: 0.93196 | val_1_rmse: 0.9495  |  0:17:15s
epoch 117| loss: 0.76208 | val_0_rmse: 0.92137 | val_1_rmse: 0.92244 |  0:17:24s
epoch 118| loss: 0.76187 | val_0_rmse: 0.91345 | val_1_rmse: 0.91594 |  0:17:33s
epoch 119| loss: 0.76235 | val_0_rmse: 0.89927 | val_1_rmse: 0.90096 |  0:17:42s
epoch 120| loss: 0.76106 | val_0_rmse: 0.88692 | val_1_rmse: 0.88338 |  0:17:51s
epoch 121| loss: 0.76209 | val_0_rmse: 0.90557 | val_1_rmse: 0.90216 |  0:18:00s
epoch 122| loss: 0.76335 | val_0_rmse: 0.89969 | val_1_rmse: 0.89762 |  0:18:09s
epoch 123| loss: 0.76195 | val_0_rmse: 0.89837 | val_1_rmse: 0.89658 |  0:18:18s
epoch 124| loss: 0.76145 | val_0_rmse: 0.88683 | val_1_rmse: 0.88284 |  0:18:27s
epoch 125| loss: 0.75871 | val_0_rmse: 0.91245 | val_1_rmse: 0.90894 |  0:18:36s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.86868
Best weights from best epoch are automatically used!
ended training at: 04:08:24
Feature importance:
Mean squared error is of 5178379133.731489
Mean absolute error:55620.25411268831
MAPE:0.5415027603094376
R2 score:0.23930118597377026
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:08:26
epoch 0  | loss: 0.93791 | val_0_rmse: 0.92876 | val_1_rmse: 0.93148 |  0:00:09s
epoch 1  | loss: 0.85649 | val_0_rmse: 0.9279  | val_1_rmse: 0.93051 |  0:00:18s
epoch 2  | loss: 0.85296 | val_0_rmse: 0.92664 | val_1_rmse: 0.92842 |  0:00:27s
epoch 3  | loss: 0.85199 | val_0_rmse: 0.92143 | val_1_rmse: 0.92391 |  0:00:36s
epoch 4  | loss: 0.85054 | val_0_rmse: 0.92085 | val_1_rmse: 0.92329 |  0:00:45s
epoch 5  | loss: 0.85094 | val_0_rmse: 0.92293 | val_1_rmse: 0.92523 |  0:00:54s
epoch 6  | loss: 0.85065 | val_0_rmse: 0.92047 | val_1_rmse: 0.92264 |  0:01:03s
epoch 7  | loss: 0.84824 | val_0_rmse: 0.9227  | val_1_rmse: 0.92524 |  0:01:13s
epoch 8  | loss: 0.85156 | val_0_rmse: 0.93166 | val_1_rmse: 0.93515 |  0:01:22s
epoch 9  | loss: 0.85072 | val_0_rmse: 0.93857 | val_1_rmse: 0.94086 |  0:01:31s
epoch 10 | loss: 0.85005 | val_0_rmse: 0.93214 | val_1_rmse: 0.93374 |  0:01:40s
epoch 11 | loss: 0.84919 | val_0_rmse: 0.92763 | val_1_rmse: 0.92936 |  0:01:49s
epoch 12 | loss: 0.84704 | val_0_rmse: 0.93016 | val_1_rmse: 0.93209 |  0:01:58s
epoch 13 | loss: 0.847   | val_0_rmse: 0.92614 | val_1_rmse: 0.92858 |  0:02:08s
epoch 14 | loss: 0.84788 | val_0_rmse: 0.93625 | val_1_rmse: 0.93712 |  0:02:17s
epoch 15 | loss: 0.84705 | val_0_rmse: 0.92271 | val_1_rmse: 0.92503 |  0:02:26s
epoch 16 | loss: 0.84717 | val_0_rmse: 0.92404 | val_1_rmse: 0.92623 |  0:02:35s
epoch 17 | loss: 0.84734 | val_0_rmse: 0.92914 | val_1_rmse: 0.93101 |  0:02:44s
epoch 18 | loss: 0.84906 | val_0_rmse: 0.93569 | val_1_rmse: 0.93724 |  0:02:53s
epoch 19 | loss: 0.85029 | val_0_rmse: 0.92836 | val_1_rmse: 0.92936 |  0:03:02s
epoch 20 | loss: 0.84898 | val_0_rmse: 0.92481 | val_1_rmse: 0.92697 |  0:03:12s
epoch 21 | loss: 0.8493  | val_0_rmse: 0.93377 | val_1_rmse: 0.93535 |  0:03:21s
epoch 22 | loss: 0.84803 | val_0_rmse: 0.92444 | val_1_rmse: 0.92575 |  0:03:30s
epoch 23 | loss: 0.8488  | val_0_rmse: 0.92674 | val_1_rmse: 0.92778 |  0:03:39s
epoch 24 | loss: 0.84875 | val_0_rmse: 0.93706 | val_1_rmse: 0.93814 |  0:03:48s
epoch 25 | loss: 0.84941 | val_0_rmse: 0.92897 | val_1_rmse: 0.93002 |  0:03:57s
epoch 26 | loss: 0.84843 | val_0_rmse: 0.92632 | val_1_rmse: 0.92872 |  0:04:06s
epoch 27 | loss: 0.8484  | val_0_rmse: 0.92746 | val_1_rmse: 0.92911 |  0:04:15s
epoch 28 | loss: 0.84887 | val_0_rmse: 0.93569 | val_1_rmse: 0.93708 |  0:04:25s
epoch 29 | loss: 0.84862 | val_0_rmse: 0.92549 | val_1_rmse: 0.92704 |  0:04:34s
epoch 30 | loss: 0.84708 | val_0_rmse: 0.9293  | val_1_rmse: 0.93113 |  0:04:43s
epoch 31 | loss: 0.84822 | val_0_rmse: 0.93109 | val_1_rmse: 0.93288 |  0:04:52s
epoch 32 | loss: 0.84847 | val_0_rmse: 0.93638 | val_1_rmse: 0.937   |  0:05:01s
epoch 33 | loss: 0.84762 | val_0_rmse: 0.93374 | val_1_rmse: 0.93457 |  0:05:10s
epoch 34 | loss: 0.84828 | val_0_rmse: 0.92981 | val_1_rmse: 0.93056 |  0:05:19s
epoch 35 | loss: 0.84803 | val_0_rmse: 0.9258  | val_1_rmse: 0.92779 |  0:05:29s
epoch 36 | loss: 0.84765 | val_0_rmse: 0.92317 | val_1_rmse: 0.92438 |  0:05:38s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.92264
Best weights from best epoch are automatically used!
ended training at: 04:14:07
Feature importance:
Mean squared error is of 5755290475.595874
Mean absolute error:59340.24210807079
MAPE:0.5804342217194052
R2 score:0.15134965673780665
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:14:08
epoch 0  | loss: 0.9257  | val_0_rmse: 0.95136 | val_1_rmse: 0.95319 |  0:00:09s
epoch 1  | loss: 0.85617 | val_0_rmse: 0.93751 | val_1_rmse: 0.93961 |  0:00:18s
epoch 2  | loss: 0.85145 | val_0_rmse: 0.92547 | val_1_rmse: 0.92792 |  0:00:27s
epoch 3  | loss: 0.84905 | val_0_rmse: 0.92202 | val_1_rmse: 0.92353 |  0:00:36s
epoch 4  | loss: 0.84982 | val_0_rmse: 0.92081 | val_1_rmse: 0.92219 |  0:00:45s
epoch 5  | loss: 0.8484  | val_0_rmse: 0.92013 | val_1_rmse: 0.92244 |  0:00:54s
epoch 6  | loss: 0.84728 | val_0_rmse: 0.92085 | val_1_rmse: 0.92254 |  0:01:03s
epoch 7  | loss: 0.8479  | val_0_rmse: 0.91977 | val_1_rmse: 0.92187 |  0:01:12s
epoch 8  | loss: 0.84731 | val_0_rmse: 0.91882 | val_1_rmse: 0.92108 |  0:01:21s
epoch 9  | loss: 0.84644 | val_0_rmse: 0.92118 | val_1_rmse: 0.924   |  0:01:30s
epoch 10 | loss: 0.84617 | val_0_rmse: 0.91826 | val_1_rmse: 0.92065 |  0:01:39s
epoch 11 | loss: 0.84715 | val_0_rmse: 0.92061 | val_1_rmse: 0.92281 |  0:01:49s
epoch 12 | loss: 0.84663 | val_0_rmse: 0.91971 | val_1_rmse: 0.92225 |  0:01:58s
epoch 13 | loss: 0.84606 | val_0_rmse: 0.91992 | val_1_rmse: 0.92214 |  0:02:07s
epoch 14 | loss: 0.84604 | val_0_rmse: 0.91925 | val_1_rmse: 0.922   |  0:02:16s
epoch 15 | loss: 0.84621 | val_0_rmse: 0.91987 | val_1_rmse: 0.9222  |  0:02:25s
epoch 16 | loss: 0.8454  | val_0_rmse: 0.91933 | val_1_rmse: 0.92267 |  0:02:34s
epoch 17 | loss: 0.84657 | val_0_rmse: 0.91998 | val_1_rmse: 0.92264 |  0:02:43s
epoch 18 | loss: 0.84658 | val_0_rmse: 0.91995 | val_1_rmse: 0.92228 |  0:02:52s
epoch 19 | loss: 0.84613 | val_0_rmse: 0.9192  | val_1_rmse: 0.92179 |  0:03:02s
epoch 20 | loss: 0.84589 | val_0_rmse: 0.92021 | val_1_rmse: 0.92254 |  0:03:11s
epoch 21 | loss: 0.84661 | val_0_rmse: 0.91937 | val_1_rmse: 0.9216  |  0:03:20s
epoch 22 | loss: 0.84663 | val_0_rmse: 0.91941 | val_1_rmse: 0.92183 |  0:03:29s
epoch 23 | loss: 0.84584 | val_0_rmse: 0.91982 | val_1_rmse: 0.92206 |  0:03:38s
epoch 24 | loss: 0.84681 | val_0_rmse: 0.91948 | val_1_rmse: 0.92191 |  0:03:47s
epoch 25 | loss: 0.84664 | val_0_rmse: 0.91943 | val_1_rmse: 0.92114 |  0:03:56s
epoch 26 | loss: 0.8456  | val_0_rmse: 0.91898 | val_1_rmse: 0.92117 |  0:04:06s
epoch 27 | loss: 0.84574 | val_0_rmse: 0.9189  | val_1_rmse: 0.92106 |  0:04:15s
epoch 28 | loss: 0.84533 | val_0_rmse: 0.91884 | val_1_rmse: 0.92087 |  0:04:24s
epoch 29 | loss: 0.84632 | val_0_rmse: 0.91951 | val_1_rmse: 0.92167 |  0:04:33s
epoch 30 | loss: 0.84741 | val_0_rmse: 0.91973 | val_1_rmse: 0.92215 |  0:04:42s
epoch 31 | loss: 0.84673 | val_0_rmse: 0.91957 | val_1_rmse: 0.9218  |  0:04:51s
epoch 32 | loss: 0.85205 | val_0_rmse: 0.92215 | val_1_rmse: 0.92439 |  0:05:00s
epoch 33 | loss: 0.84865 | val_0_rmse: 0.9202  | val_1_rmse: 0.92241 |  0:05:09s
epoch 34 | loss: 0.84894 | val_0_rmse: 0.9205  | val_1_rmse: 0.92274 |  0:05:18s
epoch 35 | loss: 0.84836 | val_0_rmse: 0.92117 | val_1_rmse: 0.92311 |  0:05:27s
epoch 36 | loss: 0.8478  | val_0_rmse: 0.92024 | val_1_rmse: 0.92241 |  0:05:36s
epoch 37 | loss: 0.84823 | val_0_rmse: 0.92003 | val_1_rmse: 0.92177 |  0:05:46s
epoch 38 | loss: 0.84723 | val_0_rmse: 0.92139 | val_1_rmse: 0.92322 |  0:05:55s
epoch 39 | loss: 0.84843 | val_0_rmse: 0.92045 | val_1_rmse: 0.92239 |  0:06:04s
epoch 40 | loss: 0.84728 | val_0_rmse: 0.9204  | val_1_rmse: 0.9226  |  0:06:13s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.92065
Best weights from best epoch are automatically used!
ended training at: 04:20:24
Feature importance:
Mean squared error is of 5772704936.748121
Mean absolute error:59115.15788645441
MAPE:0.579262526541888
R2 score:0.14820105478079126
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:20:25
epoch 0  | loss: 0.93544 | val_0_rmse: 0.97842 | val_1_rmse: 0.97985 |  0:00:09s
epoch 1  | loss: 0.85613 | val_0_rmse: 0.94044 | val_1_rmse: 0.94258 |  0:00:18s
epoch 2  | loss: 0.85295 | val_0_rmse: 0.93057 | val_1_rmse: 0.93152 |  0:00:27s
epoch 3  | loss: 0.85076 | val_0_rmse: 0.92485 | val_1_rmse: 0.92612 |  0:00:36s
epoch 4  | loss: 0.83708 | val_0_rmse: 0.98107 | val_1_rmse: 0.98033 |  0:00:45s
epoch 5  | loss: 0.80231 | val_0_rmse: 0.95103 | val_1_rmse: 0.95495 |  0:00:54s
epoch 6  | loss: 0.80084 | val_0_rmse: 0.96834 | val_1_rmse: 0.96967 |  0:01:03s
epoch 7  | loss: 0.79058 | val_0_rmse: 0.94405 | val_1_rmse: 0.94476 |  0:01:13s
epoch 8  | loss: 0.78314 | val_0_rmse: 0.92413 | val_1_rmse: 0.92457 |  0:01:22s
epoch 9  | loss: 0.7799  | val_0_rmse: 0.96981 | val_1_rmse: 0.96905 |  0:01:31s
epoch 10 | loss: 0.78274 | val_0_rmse: 0.99567 | val_1_rmse: 0.99446 |  0:01:40s
epoch 11 | loss: 0.77913 | val_0_rmse: 0.98381 | val_1_rmse: 0.98577 |  0:01:49s
epoch 12 | loss: 0.77317 | val_0_rmse: 0.94942 | val_1_rmse: 0.94963 |  0:01:58s
epoch 13 | loss: 0.76956 | val_0_rmse: 0.94713 | val_1_rmse: 0.91236 |  0:02:07s
epoch 14 | loss: 0.77436 | val_0_rmse: 0.90273 | val_1_rmse: 0.90216 |  0:02:16s
epoch 15 | loss: 0.77249 | val_0_rmse: 0.93644 | val_1_rmse: 0.93734 |  0:02:25s
epoch 16 | loss: 0.76732 | val_0_rmse: 0.98282 | val_1_rmse: 0.98409 |  0:02:35s
epoch 17 | loss: 0.77065 | val_0_rmse: 1.00683 | val_1_rmse: 1.00892 |  0:02:44s
epoch 18 | loss: 0.76731 | val_0_rmse: 0.95157 | val_1_rmse: 0.95237 |  0:02:53s
epoch 19 | loss: 0.76643 | val_0_rmse: 2.35287 | val_1_rmse: 2.34794 |  0:03:02s
epoch 20 | loss: 0.76446 | val_0_rmse: 0.93432 | val_1_rmse: 0.93439 |  0:03:11s
epoch 21 | loss: 0.76807 | val_0_rmse: 0.89664 | val_1_rmse: 0.89713 |  0:03:20s
epoch 22 | loss: 0.77031 | val_0_rmse: 0.92464 | val_1_rmse: 0.92598 |  0:03:29s
epoch 23 | loss: 0.76524 | val_0_rmse: 0.91608 | val_1_rmse: 0.91724 |  0:03:38s
epoch 24 | loss: 0.76537 | val_0_rmse: 1.01517 | val_1_rmse: 1.01763 |  0:03:48s
epoch 25 | loss: 0.76575 | val_0_rmse: 1.78941 | val_1_rmse: 1.78346 |  0:03:57s
epoch 26 | loss: 0.76449 | val_0_rmse: 0.93262 | val_1_rmse: 0.93322 |  0:04:06s
epoch 27 | loss: 0.76287 | val_0_rmse: 1.2187  | val_1_rmse: 1.21402 |  0:04:15s
epoch 28 | loss: 0.76734 | val_0_rmse: 0.91306 | val_1_rmse: 0.91528 |  0:04:24s
epoch 29 | loss: 0.76975 | val_0_rmse: 1.06527 | val_1_rmse: 1.06681 |  0:04:33s
epoch 30 | loss: 0.76305 | val_0_rmse: 0.90354 | val_1_rmse: 0.9042  |  0:04:43s
epoch 31 | loss: 0.76144 | val_0_rmse: 0.95304 | val_1_rmse: 0.95779 |  0:04:52s
epoch 32 | loss: 0.76282 | val_0_rmse: 0.933   | val_1_rmse: 0.933   |  0:05:01s
epoch 33 | loss: 0.76627 | val_0_rmse: 1.22046 | val_1_rmse: 1.2227  |  0:05:10s
epoch 34 | loss: 0.76048 | val_0_rmse: 0.89305 | val_1_rmse: 0.89369 |  0:05:19s
epoch 35 | loss: 0.76008 | val_0_rmse: 0.91554 | val_1_rmse: 0.91564 |  0:05:28s
epoch 36 | loss: 0.75967 | val_0_rmse: 2.05186 | val_1_rmse: 2.04553 |  0:05:37s
epoch 37 | loss: 0.76119 | val_0_rmse: 1.0003  | val_1_rmse: 1.00028 |  0:05:46s
epoch 38 | loss: 0.75891 | val_0_rmse: 0.91924 | val_1_rmse: 0.91803 |  0:05:56s
epoch 39 | loss: 0.75611 | val_0_rmse: 1.35444 | val_1_rmse: 1.34929 |  0:06:05s
epoch 40 | loss: 0.75907 | val_0_rmse: 0.95011 | val_1_rmse: 0.94996 |  0:06:14s
epoch 41 | loss: 0.75715 | val_0_rmse: 0.92585 | val_1_rmse: 0.92795 |  0:06:23s
epoch 42 | loss: 0.75582 | val_0_rmse: 0.91965 | val_1_rmse: 0.9201  |  0:06:32s
epoch 43 | loss: 0.75925 | val_0_rmse: 1.06247 | val_1_rmse: 1.06348 |  0:06:41s
epoch 44 | loss: 0.77278 | val_0_rmse: 1.52691 | val_1_rmse: 1.52288 |  0:06:50s
epoch 45 | loss: 0.76201 | val_0_rmse: 0.98792 | val_1_rmse: 0.98807 |  0:07:00s
epoch 46 | loss: 0.76132 | val_0_rmse: 1.01843 | val_1_rmse: 1.01904 |  0:07:09s
epoch 47 | loss: 0.75949 | val_0_rmse: 0.90961 | val_1_rmse: 0.91082 |  0:07:18s
epoch 48 | loss: 0.75878 | val_0_rmse: 0.92274 | val_1_rmse: 0.92194 |  0:07:27s
epoch 49 | loss: 0.75519 | val_0_rmse: 1.0127  | val_1_rmse: 1.01273 |  0:07:36s
epoch 50 | loss: 0.75677 | val_0_rmse: 0.88129 | val_1_rmse: 0.88334 |  0:07:45s
epoch 51 | loss: 0.77924 | val_0_rmse: 1.28839 | val_1_rmse: 1.28606 |  0:07:54s
epoch 52 | loss: 0.76752 | val_0_rmse: 0.91182 | val_1_rmse: 0.91094 |  0:08:04s
epoch 53 | loss: 0.76789 | val_0_rmse: 2.20367 | val_1_rmse: 2.19531 |  0:08:13s
epoch 54 | loss: 0.76241 | val_0_rmse: 0.88437 | val_1_rmse: 0.8867  |  0:08:22s
epoch 55 | loss: 0.75945 | val_0_rmse: 0.91604 | val_1_rmse: 0.91729 |  0:08:31s
epoch 56 | loss: 0.7624  | val_0_rmse: 0.90727 | val_1_rmse: 0.91099 |  0:08:40s
epoch 57 | loss: 0.76202 | val_0_rmse: 0.99703 | val_1_rmse: 0.99975 |  0:08:49s
epoch 58 | loss: 0.76072 | val_0_rmse: 0.92296 | val_1_rmse: 0.92378 |  0:08:59s
epoch 59 | loss: 0.76054 | val_0_rmse: 0.91837 | val_1_rmse: 0.91823 |  0:09:08s
epoch 60 | loss: 0.75768 | val_0_rmse: 1.13258 | val_1_rmse: 1.13153 |  0:09:17s
epoch 61 | loss: 0.75861 | val_0_rmse: 0.95773 | val_1_rmse: 0.9556  |  0:09:26s
epoch 62 | loss: 0.75624 | val_0_rmse: 0.91104 | val_1_rmse: 0.91368 |  0:09:35s
epoch 63 | loss: 0.75891 | val_0_rmse: 0.91821 | val_1_rmse: 0.92155 |  0:09:44s
epoch 64 | loss: 0.75705 | val_0_rmse: 1.22846 | val_1_rmse: 1.22467 |  0:09:53s
epoch 65 | loss: 0.75972 | val_0_rmse: 0.8817  | val_1_rmse: 0.88232 |  0:10:02s
epoch 66 | loss: 0.75981 | val_0_rmse: 0.9177  | val_1_rmse: 0.91759 |  0:10:12s
epoch 67 | loss: 0.75779 | val_0_rmse: 1.84656 | val_1_rmse: 1.84151 |  0:10:21s
epoch 68 | loss: 0.75689 | val_0_rmse: 0.96216 | val_1_rmse: 0.96043 |  0:10:30s
epoch 69 | loss: 0.75931 | val_0_rmse: 0.96768 | val_1_rmse: 0.96571 |  0:10:39s
epoch 70 | loss: 0.75499 | val_0_rmse: 0.88711 | val_1_rmse: 0.88837 |  0:10:48s
epoch 71 | loss: 0.7571  | val_0_rmse: 0.87473 | val_1_rmse: 0.87423 |  0:10:57s
epoch 72 | loss: 0.75623 | val_0_rmse: 1.03165 | val_1_rmse: 1.03306 |  0:11:06s
epoch 73 | loss: 0.7551  | val_0_rmse: 0.99043 | val_1_rmse: 0.98849 |  0:11:16s
epoch 74 | loss: 0.75574 | val_0_rmse: 1.24186 | val_1_rmse: 1.23966 |  0:11:25s
epoch 75 | loss: 0.76642 | val_0_rmse: 1.0018  | val_1_rmse: 1.0046  |  0:11:34s
epoch 76 | loss: 0.76055 | val_0_rmse: 1.02294 | val_1_rmse: 1.02134 |  0:11:43s
epoch 77 | loss: 0.75519 | val_0_rmse: 0.94502 | val_1_rmse: 0.94688 |  0:11:52s
epoch 78 | loss: 0.75615 | val_0_rmse: 0.90397 | val_1_rmse: 0.90386 |  0:12:01s
epoch 79 | loss: 0.75501 | val_0_rmse: 0.97214 | val_1_rmse: 0.97253 |  0:12:11s
epoch 80 | loss: 0.7565  | val_0_rmse: 1.00826 | val_1_rmse: 1.00827 |  0:12:20s
epoch 81 | loss: 0.7544  | val_0_rmse: 0.93052 | val_1_rmse: 0.92887 |  0:12:29s
epoch 82 | loss: 0.75412 | val_0_rmse: 0.99568 | val_1_rmse: 0.99522 |  0:12:38s
epoch 83 | loss: 0.75271 | val_0_rmse: 1.03958 | val_1_rmse: 1.03749 |  0:12:47s
epoch 84 | loss: 0.75337 | val_0_rmse: 1.33694 | val_1_rmse: 1.33237 |  0:12:56s
epoch 85 | loss: 0.75229 | val_0_rmse: 1.16    | val_1_rmse: 1.15667 |  0:13:06s
epoch 86 | loss: 0.77402 | val_0_rmse: 0.92286 | val_1_rmse: 0.92162 |  0:13:15s
epoch 87 | loss: 0.78404 | val_0_rmse: 0.92202 | val_1_rmse: 0.92239 |  0:13:24s
epoch 88 | loss: 0.77634 | val_0_rmse: 1.0149  | val_1_rmse: 1.01392 |  0:13:33s
epoch 89 | loss: 0.77036 | val_0_rmse: 0.89466 | val_1_rmse: 0.89402 |  0:13:42s
epoch 90 | loss: 0.76933 | val_0_rmse: 0.94375 | val_1_rmse: 0.94362 |  0:13:51s
epoch 91 | loss: 0.77166 | val_0_rmse: 1.21318 | val_1_rmse: 1.21108 |  0:14:01s
epoch 92 | loss: 0.76638 | val_0_rmse: 0.94047 | val_1_rmse: 0.94033 |  0:14:10s
epoch 93 | loss: 0.76731 | val_0_rmse: 0.99236 | val_1_rmse: 0.99407 |  0:14:19s
epoch 94 | loss: 0.76737 | val_0_rmse: 0.87827 | val_1_rmse: 0.87911 |  0:14:28s
epoch 95 | loss: 0.76664 | val_0_rmse: 0.9271  | val_1_rmse: 0.92876 |  0:14:37s
epoch 96 | loss: 0.76711 | val_0_rmse: 1.34978 | val_1_rmse: 1.34382 |  0:14:46s
epoch 97 | loss: 0.76421 | val_0_rmse: 0.9776  | val_1_rmse: 0.97517 |  0:14:56s
epoch 98 | loss: 0.76645 | val_0_rmse: 0.99741 | val_1_rmse: 0.99692 |  0:15:05s
epoch 99 | loss: 0.76245 | val_0_rmse: 0.96978 | val_1_rmse: 0.97004 |  0:15:14s
epoch 100| loss: 0.76188 | val_0_rmse: 0.90771 | val_1_rmse: 0.906   |  0:15:23s
epoch 101| loss: 0.77286 | val_0_rmse: 0.90628 | val_1_rmse: 0.90616 |  0:15:32s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.87423
Best weights from best epoch are automatically used!
ended training at: 04:36:01
Feature importance:
Mean squared error is of 5177473942.946791
Mean absolute error:55721.3076294619
MAPE:0.5363854947076916
R2 score:0.23082086142567038
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:36:02
epoch 0  | loss: 0.93647 | val_0_rmse: 0.93708 | val_1_rmse: 0.93166 |  0:00:09s
epoch 1  | loss: 0.86177 | val_0_rmse: 0.93144 | val_1_rmse: 0.92573 |  0:00:18s
epoch 2  | loss: 0.85875 | val_0_rmse: 0.92549 | val_1_rmse: 0.91888 |  0:00:27s
epoch 3  | loss: 0.85883 | val_0_rmse: 0.9253  | val_1_rmse: 0.91904 |  0:00:36s
epoch 4  | loss: 0.86538 | val_0_rmse: 0.92772 | val_1_rmse: 0.92285 |  0:00:45s
epoch 5  | loss: 0.8587  | val_0_rmse: 0.92727 | val_1_rmse: 0.92053 |  0:00:54s
epoch 6  | loss: 0.86031 | val_0_rmse: 0.92621 | val_1_rmse: 0.92042 |  0:01:03s
epoch 7  | loss: 0.85663 | val_0_rmse: 0.92415 | val_1_rmse: 0.91702 |  0:01:12s
epoch 8  | loss: 0.85522 | val_0_rmse: 0.92306 | val_1_rmse: 0.91548 |  0:01:22s
epoch 9  | loss: 0.85251 | val_0_rmse: 0.92227 | val_1_rmse: 0.91533 |  0:01:31s
epoch 10 | loss: 0.85249 | val_0_rmse: 0.922   | val_1_rmse: 0.91555 |  0:01:40s
epoch 11 | loss: 0.85228 | val_0_rmse: 0.92344 | val_1_rmse: 0.91679 |  0:01:49s
epoch 12 | loss: 0.85246 | val_0_rmse: 0.92274 | val_1_rmse: 0.91595 |  0:01:58s
epoch 13 | loss: 0.85155 | val_0_rmse: 0.92134 | val_1_rmse: 0.91454 |  0:02:07s
epoch 14 | loss: 0.85126 | val_0_rmse: 0.92257 | val_1_rmse: 0.91618 |  0:02:17s
epoch 15 | loss: 0.8506  | val_0_rmse: 0.92213 | val_1_rmse: 0.91487 |  0:02:26s
epoch 16 | loss: 0.85019 | val_0_rmse: 0.92195 | val_1_rmse: 0.9143  |  0:02:35s
epoch 17 | loss: 0.84966 | val_0_rmse: 0.92056 | val_1_rmse: 0.91333 |  0:02:44s
epoch 18 | loss: 0.84969 | val_0_rmse: 0.92333 | val_1_rmse: 0.91695 |  0:02:53s
epoch 19 | loss: 0.84981 | val_0_rmse: 0.92086 | val_1_rmse: 0.91408 |  0:03:02s
epoch 20 | loss: 0.84977 | val_0_rmse: 0.92097 | val_1_rmse: 0.91304 |  0:03:11s
epoch 21 | loss: 0.8502  | val_0_rmse: 0.92104 | val_1_rmse: 0.91408 |  0:03:20s
epoch 22 | loss: 0.84967 | val_0_rmse: 0.92065 | val_1_rmse: 0.91317 |  0:03:30s
epoch 23 | loss: 0.8495  | val_0_rmse: 0.92125 | val_1_rmse: 0.91366 |  0:03:39s
epoch 24 | loss: 0.84892 | val_0_rmse: 0.92085 | val_1_rmse: 0.91371 |  0:03:48s
epoch 25 | loss: 0.84883 | val_0_rmse: 0.91995 | val_1_rmse: 0.9122  |  0:03:57s
epoch 26 | loss: 0.8488  | val_0_rmse: 0.92697 | val_1_rmse: 0.92076 |  0:04:06s
epoch 27 | loss: 0.85627 | val_0_rmse: 0.92559 | val_1_rmse: 0.91887 |  0:04:16s
epoch 28 | loss: 0.86263 | val_0_rmse: 0.92321 | val_1_rmse: 0.91685 |  0:04:25s
epoch 29 | loss: 0.85375 | val_0_rmse: 0.9224  | val_1_rmse: 0.91556 |  0:04:34s
epoch 30 | loss: 0.85203 | val_0_rmse: 0.92356 | val_1_rmse: 0.91821 |  0:04:43s
epoch 31 | loss: 0.85265 | val_0_rmse: 0.92281 | val_1_rmse: 0.91607 |  0:04:52s
epoch 32 | loss: 0.85139 | val_0_rmse: 0.92174 | val_1_rmse: 0.91547 |  0:05:01s
epoch 33 | loss: 0.84975 | val_0_rmse: 0.91997 | val_1_rmse: 0.91371 |  0:05:10s
epoch 34 | loss: 0.85002 | val_0_rmse: 0.92176 | val_1_rmse: 0.91461 |  0:05:20s
epoch 35 | loss: 0.85075 | val_0_rmse: 0.92603 | val_1_rmse: 0.91967 |  0:05:29s
epoch 36 | loss: 0.85231 | val_0_rmse: 0.92568 | val_1_rmse: 0.91828 |  0:05:38s
epoch 37 | loss: 0.85091 | val_0_rmse: 0.92115 | val_1_rmse: 0.91459 |  0:05:47s
epoch 38 | loss: 0.8494  | val_0_rmse: 0.91969 | val_1_rmse: 0.91261 |  0:05:56s
epoch 39 | loss: 0.84997 | val_0_rmse: 0.92145 | val_1_rmse: 0.91514 |  0:06:05s
epoch 40 | loss: 0.84969 | val_0_rmse: 0.92116 | val_1_rmse: 0.9141  |  0:06:15s
epoch 41 | loss: 0.84812 | val_0_rmse: 0.91918 | val_1_rmse: 0.91246 |  0:06:24s
epoch 42 | loss: 0.84901 | val_0_rmse: 0.91894 | val_1_rmse: 0.91347 |  0:06:33s
epoch 43 | loss: 0.84666 | val_0_rmse: 0.91773 | val_1_rmse: 0.91177 |  0:06:42s
epoch 44 | loss: 0.84765 | val_0_rmse: 0.91937 | val_1_rmse: 0.91253 |  0:06:51s
epoch 45 | loss: 0.84487 | val_0_rmse: 0.91672 | val_1_rmse: 0.91097 |  0:07:00s
epoch 46 | loss: 0.84783 | val_0_rmse: 0.92436 | val_1_rmse: 0.91675 |  0:07:09s
epoch 47 | loss: 0.84634 | val_0_rmse: 0.92763 | val_1_rmse: 0.92185 |  0:07:18s
epoch 48 | loss: 0.84472 | val_0_rmse: 0.91893 | val_1_rmse: 0.9139  |  0:07:27s
epoch 49 | loss: 0.84637 | val_0_rmse: 0.92023 | val_1_rmse: 0.92446 |  0:07:37s
epoch 50 | loss: 0.84443 | val_0_rmse: 0.91585 | val_1_rmse: 0.91064 |  0:07:46s
epoch 51 | loss: 0.84523 | val_0_rmse: 0.91669 | val_1_rmse: 0.91017 |  0:07:55s
epoch 52 | loss: 0.84543 | val_0_rmse: 0.92386 | val_1_rmse: 0.91715 |  0:08:04s
epoch 53 | loss: 0.84492 | val_0_rmse: 0.92038 | val_1_rmse: 0.91347 |  0:08:13s
epoch 54 | loss: 0.84596 | val_0_rmse: 0.9234  | val_1_rmse: 0.91706 |  0:08:22s
epoch 55 | loss: 0.84409 | val_0_rmse: 0.92591 | val_1_rmse: 0.91961 |  0:08:31s
epoch 56 | loss: 0.84518 | val_0_rmse: 0.91697 | val_1_rmse: 0.91071 |  0:08:41s
epoch 57 | loss: 0.84515 | val_0_rmse: 0.91619 | val_1_rmse: 0.91035 |  0:08:50s
epoch 58 | loss: 0.84451 | val_0_rmse: 0.91677 | val_1_rmse: 0.91136 |  0:08:59s
epoch 59 | loss: 0.84394 | val_0_rmse: 0.92179 | val_1_rmse: 0.91698 |  0:09:08s
epoch 60 | loss: 0.8452  | val_0_rmse: 0.91694 | val_1_rmse: 0.91181 |  0:09:17s
epoch 61 | loss: 0.84494 | val_0_rmse: 0.91716 | val_1_rmse: 0.91128 |  0:09:26s
epoch 62 | loss: 0.84518 | val_0_rmse: 0.91871 | val_1_rmse: 0.9152  |  0:09:35s
epoch 63 | loss: 0.84436 | val_0_rmse: 0.92142 | val_1_rmse: 0.91525 |  0:09:45s
epoch 64 | loss: 0.84344 | val_0_rmse: 0.91706 | val_1_rmse: 0.91201 |  0:09:54s
epoch 65 | loss: 0.84384 | val_0_rmse: 0.9156  | val_1_rmse: 0.91009 |  0:10:03s
epoch 66 | loss: 0.84311 | val_0_rmse: 0.92235 | val_1_rmse: 0.91654 |  0:10:12s
epoch 67 | loss: 0.84535 | val_0_rmse: 0.91807 | val_1_rmse: 0.91179 |  0:10:21s
epoch 68 | loss: 0.84512 | val_0_rmse: 0.91839 | val_1_rmse: 0.9142  |  0:10:30s
epoch 69 | loss: 0.84303 | val_0_rmse: 0.92155 | val_1_rmse: 0.91552 |  0:10:39s
epoch 70 | loss: 0.84408 | val_0_rmse: 0.91924 | val_1_rmse: 0.91385 |  0:10:48s
epoch 71 | loss: 0.8432  | val_0_rmse: 0.91892 | val_1_rmse: 0.9123  |  0:10:57s
epoch 72 | loss: 0.8433  | val_0_rmse: 0.91567 | val_1_rmse: 0.91081 |  0:11:06s
epoch 73 | loss: 0.84372 | val_0_rmse: 0.91623 | val_1_rmse: 0.91034 |  0:11:15s
epoch 74 | loss: 0.84561 | val_0_rmse: 0.9213  | val_1_rmse: 0.9138  |  0:11:25s
epoch 75 | loss: 0.8443  | val_0_rmse: 0.92126 | val_1_rmse: 0.91903 |  0:11:33s
epoch 76 | loss: 0.84306 | val_0_rmse: 0.92158 | val_1_rmse: 0.91671 |  0:11:43s
epoch 77 | loss: 0.84437 | val_0_rmse: 0.91908 | val_1_rmse: 0.91379 |  0:11:52s
epoch 78 | loss: 0.86622 | val_0_rmse: 0.9358  | val_1_rmse: 0.93184 |  0:12:01s
epoch 79 | loss: 0.87431 | val_0_rmse: 0.94702 | val_1_rmse: 0.94141 |  0:12:10s
epoch 80 | loss: 0.89831 | val_0_rmse: 0.95317 | val_1_rmse: 0.94586 |  0:12:19s
epoch 81 | loss: 0.89622 | val_0_rmse: 0.94805 | val_1_rmse: 0.94187 |  0:12:28s
epoch 82 | loss: 0.89678 | val_0_rmse: 0.94721 | val_1_rmse: 0.94057 |  0:12:37s
epoch 83 | loss: 0.86336 | val_0_rmse: 0.92453 | val_1_rmse: 0.91742 |  0:12:46s
epoch 84 | loss: 0.8541  | val_0_rmse: 0.92502 | val_1_rmse: 0.91856 |  0:12:56s
epoch 85 | loss: 0.85442 | val_0_rmse: 0.9251  | val_1_rmse: 0.91786 |  0:13:05s
epoch 86 | loss: 0.85437 | val_0_rmse: 0.93292 | val_1_rmse: 0.9267  |  0:13:14s
epoch 87 | loss: 0.85937 | val_0_rmse: 0.925   | val_1_rmse: 0.91828 |  0:13:23s
epoch 88 | loss: 0.85225 | val_0_rmse: 0.92835 | val_1_rmse: 0.92133 |  0:13:32s
epoch 89 | loss: 0.85236 | val_0_rmse: 0.9264  | val_1_rmse: 0.91958 |  0:13:41s
epoch 90 | loss: 0.85174 | val_0_rmse: 0.92734 | val_1_rmse: 0.92028 |  0:13:51s
epoch 91 | loss: 0.85121 | val_0_rmse: 0.92652 | val_1_rmse: 0.91911 |  0:14:00s
epoch 92 | loss: 0.85189 | val_0_rmse: 0.92656 | val_1_rmse: 0.91954 |  0:14:09s
epoch 93 | loss: 0.851   | val_0_rmse: 0.92745 | val_1_rmse: 0.92032 |  0:14:18s
epoch 94 | loss: 0.85195 | val_0_rmse: 0.92867 | val_1_rmse: 0.92185 |  0:14:27s
epoch 95 | loss: 0.85256 | val_0_rmse: 0.92608 | val_1_rmse: 0.9193  |  0:14:36s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.91009
Best weights from best epoch are automatically used!
ended training at: 04:50:41
Feature importance:
Mean squared error is of 5633975005.537
Mean absolute error:59038.65804020775
MAPE:0.5859723241291495
R2 score:0.1675511628954307
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:50:42
epoch 0  | loss: 0.95514 | val_0_rmse: 0.93757 | val_1_rmse: 0.94615 |  0:00:09s
epoch 1  | loss: 0.85284 | val_0_rmse: 0.95322 | val_1_rmse: 0.9608  |  0:00:18s
epoch 2  | loss: 0.84759 | val_0_rmse: 1.01761 | val_1_rmse: 1.02756 |  0:00:27s
epoch 3  | loss: 0.85149 | val_0_rmse: 0.93023 | val_1_rmse: 0.93862 |  0:00:36s
epoch 4  | loss: 0.85238 | val_0_rmse: 0.92169 | val_1_rmse: 0.93148 |  0:00:45s
epoch 5  | loss: 0.84888 | val_0_rmse: 0.92257 | val_1_rmse: 0.93109 |  0:00:54s
epoch 6  | loss: 0.84834 | val_0_rmse: 0.92143 | val_1_rmse: 0.93033 |  0:01:04s
epoch 7  | loss: 0.84897 | val_0_rmse: 0.92076 | val_1_rmse: 0.93009 |  0:01:13s
epoch 8  | loss: 0.84805 | val_0_rmse: 0.92059 | val_1_rmse: 0.93024 |  0:01:22s
epoch 9  | loss: 0.84685 | val_0_rmse: 0.91983 | val_1_rmse: 0.92955 |  0:01:31s
epoch 10 | loss: 0.8487  | val_0_rmse: 0.91939 | val_1_rmse: 0.92922 |  0:01:40s
epoch 11 | loss: 0.84649 | val_0_rmse: 0.91814 | val_1_rmse: 0.92767 |  0:01:49s
epoch 12 | loss: 0.84541 | val_0_rmse: 0.92083 | val_1_rmse: 0.93039 |  0:01:58s
epoch 13 | loss: 0.84418 | val_0_rmse: 0.91998 | val_1_rmse: 0.92987 |  0:02:08s
epoch 14 | loss: 0.84441 | val_0_rmse: 0.92054 | val_1_rmse: 0.93193 |  0:02:17s
epoch 15 | loss: 0.84414 | val_0_rmse: 0.91994 | val_1_rmse: 0.93124 |  0:02:26s
epoch 16 | loss: 0.84411 | val_0_rmse: 0.9198  | val_1_rmse: 0.92987 |  0:02:35s
epoch 17 | loss: 0.84382 | val_0_rmse: 0.92066 | val_1_rmse: 0.93282 |  0:02:44s
epoch 18 | loss: 0.84426 | val_0_rmse: 0.9227  | val_1_rmse: 0.93474 |  0:02:53s
epoch 19 | loss: 0.8471  | val_0_rmse: 0.9216  | val_1_rmse: 0.93188 |  0:03:02s
epoch 20 | loss: 0.84591 | val_0_rmse: 0.91957 | val_1_rmse: 0.92953 |  0:03:11s
epoch 21 | loss: 0.84421 | val_0_rmse: 0.91912 | val_1_rmse: 0.92859 |  0:03:20s
epoch 22 | loss: 0.84463 | val_0_rmse: 0.91799 | val_1_rmse: 0.92747 |  0:03:29s
epoch 23 | loss: 0.84357 | val_0_rmse: 0.91807 | val_1_rmse: 0.9279  |  0:03:38s
epoch 24 | loss: 0.84376 | val_0_rmse: 0.91919 | val_1_rmse: 0.92861 |  0:03:48s
epoch 25 | loss: 0.84313 | val_0_rmse: 0.9175  | val_1_rmse: 0.92743 |  0:03:57s
epoch 26 | loss: 0.84284 | val_0_rmse: 0.91834 | val_1_rmse: 0.92825 |  0:04:06s
epoch 27 | loss: 0.84285 | val_0_rmse: 0.91779 | val_1_rmse: 0.92816 |  0:04:15s
epoch 28 | loss: 0.8423  | val_0_rmse: 0.91868 | val_1_rmse: 0.92824 |  0:04:25s
epoch 29 | loss: 0.843   | val_0_rmse: 0.91857 | val_1_rmse: 0.92838 |  0:04:34s
epoch 30 | loss: 0.84251 | val_0_rmse: 0.91879 | val_1_rmse: 0.92927 |  0:04:43s
epoch 31 | loss: 0.84366 | val_0_rmse: 0.91923 | val_1_rmse: 0.92836 |  0:04:52s
epoch 32 | loss: 0.84336 | val_0_rmse: 0.91826 | val_1_rmse: 0.92791 |  0:05:01s
epoch 33 | loss: 0.84274 | val_0_rmse: 0.91756 | val_1_rmse: 0.9277  |  0:05:10s
epoch 34 | loss: 0.84292 | val_0_rmse: 0.91779 | val_1_rmse: 0.92726 |  0:05:20s
epoch 35 | loss: 0.84497 | val_0_rmse: 0.91987 | val_1_rmse: 0.92953 |  0:05:29s
epoch 36 | loss: 0.84376 | val_0_rmse: 0.91801 | val_1_rmse: 0.928   |  0:05:38s
epoch 37 | loss: 0.84359 | val_0_rmse: 0.91834 | val_1_rmse: 0.92795 |  0:05:47s
epoch 38 | loss: 0.84375 | val_0_rmse: 0.91991 | val_1_rmse: 0.93008 |  0:05:56s
epoch 39 | loss: 0.84854 | val_0_rmse: 0.92091 | val_1_rmse: 0.93046 |  0:06:06s
epoch 40 | loss: 0.84892 | val_0_rmse: 0.92058 | val_1_rmse: 0.93054 |  0:06:15s
epoch 41 | loss: 0.84811 | val_0_rmse: 0.92108 | val_1_rmse: 0.93126 |  0:06:24s
epoch 42 | loss: 0.84727 | val_0_rmse: 0.91961 | val_1_rmse: 0.92987 |  0:06:33s
epoch 43 | loss: 0.84762 | val_0_rmse: 0.92098 | val_1_rmse: 0.93016 |  0:06:42s
epoch 44 | loss: 0.84968 | val_0_rmse: 0.9203  | val_1_rmse: 0.92969 |  0:06:52s
epoch 45 | loss: 0.84796 | val_0_rmse: 0.92015 | val_1_rmse: 0.92938 |  0:07:01s
epoch 46 | loss: 0.84804 | val_0_rmse: 0.92052 | val_1_rmse: 0.93047 |  0:07:10s
epoch 47 | loss: 0.84649 | val_0_rmse: 0.91964 | val_1_rmse: 0.92834 |  0:07:19s
epoch 48 | loss: 0.84545 | val_0_rmse: 0.91887 | val_1_rmse: 0.92832 |  0:07:28s
epoch 49 | loss: 0.8453  | val_0_rmse: 0.91839 | val_1_rmse: 0.92779 |  0:07:37s
epoch 50 | loss: 0.84433 | val_0_rmse: 0.91876 | val_1_rmse: 0.92802 |  0:07:47s
epoch 51 | loss: 0.84518 | val_0_rmse: 0.91927 | val_1_rmse: 0.9286  |  0:07:56s
epoch 52 | loss: 0.84514 | val_0_rmse: 0.91809 | val_1_rmse: 0.92775 |  0:08:05s
epoch 53 | loss: 0.84459 | val_0_rmse: 0.91845 | val_1_rmse: 0.92766 |  0:08:14s
epoch 54 | loss: 0.84325 | val_0_rmse: 0.91798 | val_1_rmse: 0.92766 |  0:08:23s
epoch 55 | loss: 0.84309 | val_0_rmse: 0.91832 | val_1_rmse: 0.92773 |  0:08:33s
epoch 56 | loss: 0.84479 | val_0_rmse: 0.91823 | val_1_rmse: 0.92817 |  0:08:42s
epoch 57 | loss: 0.84502 | val_0_rmse: 0.91834 | val_1_rmse: 0.92819 |  0:08:51s
epoch 58 | loss: 0.84416 | val_0_rmse: 0.91871 | val_1_rmse: 0.92886 |  0:09:00s
epoch 59 | loss: 0.84402 | val_0_rmse: 0.9181  | val_1_rmse: 0.92799 |  0:09:09s
epoch 60 | loss: 0.84336 | val_0_rmse: 0.9183  | val_1_rmse: 0.92792 |  0:09:18s
epoch 61 | loss: 0.84395 | val_0_rmse: 0.91893 | val_1_rmse: 0.92899 |  0:09:28s
epoch 62 | loss: 0.84361 | val_0_rmse: 0.91818 | val_1_rmse: 0.92795 |  0:09:37s
epoch 63 | loss: 0.8427  | val_0_rmse: 0.91808 | val_1_rmse: 0.92773 |  0:09:46s
epoch 64 | loss: 0.8434  | val_0_rmse: 0.91766 | val_1_rmse: 0.9276  |  0:09:55s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.92726
Best weights from best epoch are automatically used!
ended training at: 05:00:40
Feature importance:
Mean squared error is of 5784430636.35932
Mean absolute error:58948.07590677935
MAPE:0.5663471427259946
R2 score:0.1543555663412577
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:00:41
epoch 0  | loss: 0.93833 | val_0_rmse: 0.94691 | val_1_rmse: 0.94371 |  0:00:09s
epoch 1  | loss: 0.86253 | val_0_rmse: 0.95864 | val_1_rmse: 0.95817 |  0:00:18s
epoch 2  | loss: 0.83451 | val_0_rmse: 0.92605 | val_1_rmse: 0.9217  |  0:00:27s
epoch 3  | loss: 0.8283  | val_0_rmse: 0.91491 | val_1_rmse: 0.911   |  0:00:36s
epoch 4  | loss: 0.82399 | val_0_rmse: 0.89955 | val_1_rmse: 0.89682 |  0:00:45s
epoch 5  | loss: 0.79553 | val_0_rmse: 0.95558 | val_1_rmse: 0.95529 |  0:00:55s
epoch 6  | loss: 0.7901  | val_0_rmse: 0.91584 | val_1_rmse: 0.91043 |  0:01:04s
epoch 7  | loss: 0.79028 | val_0_rmse: 0.96522 | val_1_rmse: 0.96357 |  0:01:13s
epoch 8  | loss: 0.78801 | val_0_rmse: 0.88852 | val_1_rmse: 0.88428 |  0:01:22s
epoch 9  | loss: 0.78708 | val_0_rmse: 0.92692 | val_1_rmse: 0.92666 |  0:01:31s
epoch 10 | loss: 0.78968 | val_0_rmse: 0.91808 | val_1_rmse: 0.91444 |  0:01:40s
epoch 11 | loss: 0.78632 | val_0_rmse: 0.9325  | val_1_rmse: 0.93155 |  0:01:50s
epoch 12 | loss: 0.7853  | val_0_rmse: 0.98669 | val_1_rmse: 0.98591 |  0:01:59s
epoch 13 | loss: 0.78446 | val_0_rmse: 0.88372 | val_1_rmse: 0.88139 |  0:02:08s
epoch 14 | loss: 0.78474 | val_0_rmse: 0.92861 | val_1_rmse: 0.9258  |  0:02:17s
epoch 15 | loss: 0.78464 | val_0_rmse: 0.91553 | val_1_rmse: 0.91176 |  0:02:26s
epoch 16 | loss: 0.78384 | val_0_rmse: 0.95065 | val_1_rmse: 0.94771 |  0:02:35s
epoch 17 | loss: 0.78326 | val_0_rmse: 0.97095 | val_1_rmse: 0.96917 |  0:02:44s
epoch 18 | loss: 0.783   | val_0_rmse: 0.91597 | val_1_rmse: 0.91339 |  0:02:54s
epoch 19 | loss: 0.78376 | val_0_rmse: 0.9404  | val_1_rmse: 0.93796 |  0:03:03s
epoch 20 | loss: 0.78456 | val_0_rmse: 0.96839 | val_1_rmse: 0.96802 |  0:03:12s
epoch 21 | loss: 0.78434 | val_0_rmse: 0.9175  | val_1_rmse: 0.9148  |  0:03:21s
epoch 22 | loss: 0.78506 | val_0_rmse: 0.94322 | val_1_rmse: 0.94218 |  0:03:30s
epoch 23 | loss: 0.788   | val_0_rmse: 0.91386 | val_1_rmse: 0.91309 |  0:03:39s
epoch 24 | loss: 0.78436 | val_0_rmse: 0.88297 | val_1_rmse: 0.88015 |  0:03:48s
epoch 25 | loss: 0.78275 | val_0_rmse: 0.91396 | val_1_rmse: 0.9103  |  0:03:57s
epoch 26 | loss: 0.78461 | val_0_rmse: 0.9141  | val_1_rmse: 0.91187 |  0:04:07s
epoch 27 | loss: 0.7839  | val_0_rmse: 0.92134 | val_1_rmse: 0.91913 |  0:04:16s
epoch 28 | loss: 0.78334 | val_0_rmse: 0.92555 | val_1_rmse: 0.92426 |  0:04:25s
epoch 29 | loss: 0.78382 | val_0_rmse: 0.92514 | val_1_rmse: 0.92714 |  0:04:34s
epoch 30 | loss: 0.78403 | val_0_rmse: 1.00378 | val_1_rmse: 1.00373 |  0:04:43s
epoch 31 | loss: 0.78168 | val_0_rmse: 0.96524 | val_1_rmse: 0.96637 |  0:04:52s
epoch 32 | loss: 0.78116 | val_0_rmse: 0.92512 | val_1_rmse: 0.92372 |  0:05:01s
epoch 33 | loss: 0.78269 | val_0_rmse: 0.92704 | val_1_rmse: 0.92507 |  0:05:11s
epoch 34 | loss: 0.78526 | val_0_rmse: 0.9072  | val_1_rmse: 0.90607 |  0:05:20s
epoch 35 | loss: 0.79456 | val_0_rmse: 0.93476 | val_1_rmse: 0.93246 |  0:05:29s
epoch 36 | loss: 0.79276 | val_0_rmse: 0.91369 | val_1_rmse: 0.90935 |  0:05:38s
epoch 37 | loss: 0.79348 | val_0_rmse: 0.94918 | val_1_rmse: 0.94571 |  0:05:47s
epoch 38 | loss: 0.79542 | val_0_rmse: 0.93669 | val_1_rmse: 0.93452 |  0:05:56s
epoch 39 | loss: 0.79381 | val_0_rmse: 0.95672 | val_1_rmse: 0.95671 |  0:06:05s
epoch 40 | loss: 0.79348 | val_0_rmse: 0.9069  | val_1_rmse: 0.90233 |  0:06:14s
epoch 41 | loss: 0.79334 | val_0_rmse: 0.92589 | val_1_rmse: 0.92279 |  0:06:23s
epoch 42 | loss: 0.79253 | val_0_rmse: 0.92691 | val_1_rmse: 0.92591 |  0:06:32s
epoch 43 | loss: 0.7921  | val_0_rmse: 0.92091 | val_1_rmse: 0.91978 |  0:06:41s
epoch 44 | loss: 0.79257 | val_0_rmse: 0.92378 | val_1_rmse: 0.92326 |  0:06:51s
epoch 45 | loss: 0.79101 | val_0_rmse: 0.93001 | val_1_rmse: 0.92713 |  0:07:00s
epoch 46 | loss: 0.79006 | val_0_rmse: 0.93904 | val_1_rmse: 0.93674 |  0:07:09s
epoch 47 | loss: 0.79172 | val_0_rmse: 0.90725 | val_1_rmse: 0.90282 |  0:07:18s
epoch 48 | loss: 0.7908  | val_0_rmse: 0.94546 | val_1_rmse: 0.94251 |  0:07:27s
epoch 49 | loss: 0.78791 | val_0_rmse: 0.92756 | val_1_rmse: 0.92569 |  0:07:37s
epoch 50 | loss: 0.8019  | val_0_rmse: 0.97099 | val_1_rmse: 0.96754 |  0:07:46s
epoch 51 | loss: 0.81579 | val_0_rmse: 0.95234 | val_1_rmse: 0.94976 |  0:07:55s
epoch 52 | loss: 0.79682 | val_0_rmse: 0.91234 | val_1_rmse: 0.9076  |  0:08:04s
epoch 53 | loss: 0.79193 | val_0_rmse: 0.92605 | val_1_rmse: 0.92292 |  0:08:13s
epoch 54 | loss: 0.79032 | val_0_rmse: 0.9009  | val_1_rmse: 0.89807 |  0:08:22s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.88015
Best weights from best epoch are automatically used!
ended training at: 05:09:06
Feature importance:
Mean squared error is of 5359813663.888022
Mean absolute error:56933.81532441942
MAPE:0.5615388919506262
R2 score:0.21605380139396058
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:09:07
epoch 0  | loss: 0.93604 | val_0_rmse: 0.95395 | val_1_rmse: 0.96197 |  0:00:09s
epoch 1  | loss: 0.85911 | val_0_rmse: 0.95082 | val_1_rmse: 0.95828 |  0:00:18s
epoch 2  | loss: 0.85078 | val_0_rmse: 0.93179 | val_1_rmse: 0.93998 |  0:00:27s
epoch 3  | loss: 0.84775 | val_0_rmse: 0.92585 | val_1_rmse: 0.93341 |  0:00:36s
epoch 4  | loss: 0.84794 | val_0_rmse: 0.92216 | val_1_rmse: 0.92946 |  0:00:45s
epoch 5  | loss: 0.82788 | val_0_rmse: 0.91094 | val_1_rmse: 0.91592 |  0:00:55s
epoch 6  | loss: 0.8007  | val_0_rmse: 0.94354 | val_1_rmse: 0.95003 |  0:01:04s
epoch 7  | loss: 0.7931  | val_0_rmse: 0.88877 | val_1_rmse: 0.89144 |  0:01:13s
epoch 8  | loss: 0.79021 | val_0_rmse: 0.93012 | val_1_rmse: 0.93142 |  0:01:22s
epoch 9  | loss: 0.78683 | val_0_rmse: 0.8954  | val_1_rmse: 0.8936  |  0:01:31s
epoch 10 | loss: 0.78892 | val_0_rmse: 0.94958 | val_1_rmse: 0.93798 |  0:01:40s
epoch 11 | loss: 0.78396 | val_0_rmse: 0.92513 | val_1_rmse: 0.91463 |  0:01:49s
epoch 12 | loss: 0.78543 | val_0_rmse: 0.95457 | val_1_rmse: 0.95378 |  0:01:58s
epoch 13 | loss: 0.78385 | val_0_rmse: 0.91701 | val_1_rmse: 0.91684 |  0:02:08s
epoch 14 | loss: 0.78844 | val_0_rmse: 0.91036 | val_1_rmse: 0.91419 |  0:02:17s
epoch 15 | loss: 0.7877  | val_0_rmse: 0.97939 | val_1_rmse: 0.96441 |  0:02:26s
epoch 16 | loss: 0.78551 | val_0_rmse: 0.89786 | val_1_rmse: 0.89843 |  0:02:35s
epoch 17 | loss: 0.79027 | val_0_rmse: 0.90701 | val_1_rmse: 0.91133 |  0:02:44s
epoch 18 | loss: 0.78559 | val_0_rmse: 0.93281 | val_1_rmse: 0.93658 |  0:02:53s
epoch 19 | loss: 0.78542 | val_0_rmse: 0.95054 | val_1_rmse: 0.95425 |  0:03:02s
epoch 20 | loss: 0.78509 | val_0_rmse: 0.8967  | val_1_rmse: 0.90021 |  0:03:12s
epoch 21 | loss: 0.7842  | val_0_rmse: 0.93794 | val_1_rmse: 0.94386 |  0:03:21s
epoch 22 | loss: 0.7833  | val_0_rmse: 0.93744 | val_1_rmse: 0.92875 |  0:03:30s
epoch 23 | loss: 0.78492 | val_0_rmse: 0.97471 | val_1_rmse: 0.98221 |  0:03:39s
epoch 24 | loss: 0.78347 | val_0_rmse: 0.9656  | val_1_rmse: 0.9721  |  0:03:48s
epoch 25 | loss: 0.78349 | val_0_rmse: 0.92715 | val_1_rmse: 0.92535 |  0:03:57s
epoch 26 | loss: 0.78119 | val_0_rmse: 0.89171 | val_1_rmse: 0.89177 |  0:04:06s
epoch 27 | loss: 0.78304 | val_0_rmse: 0.99388 | val_1_rmse: 1.0028  |  0:04:15s
epoch 28 | loss: 0.78263 | val_0_rmse: 1.03395 | val_1_rmse: 1.03216 |  0:04:25s
epoch 29 | loss: 0.78271 | val_0_rmse: 0.90021 | val_1_rmse: 0.90346 |  0:04:34s
epoch 30 | loss: 0.78202 | val_0_rmse: 0.92874 | val_1_rmse: 0.9327  |  0:04:43s
epoch 31 | loss: 0.78195 | val_0_rmse: 1.01172 | val_1_rmse: 1.02867 |  0:04:52s
epoch 32 | loss: 0.7811  | val_0_rmse: 0.92853 | val_1_rmse: 0.93422 |  0:05:01s
epoch 33 | loss: 0.78259 | val_0_rmse: 0.96756 | val_1_rmse: 0.97142 |  0:05:10s
epoch 34 | loss: 0.78305 | val_0_rmse: 0.93711 | val_1_rmse: 0.94453 |  0:05:19s
epoch 35 | loss: 0.78239 | val_0_rmse: 0.88464 | val_1_rmse: 0.88715 |  0:05:28s
epoch 36 | loss: 0.78423 | val_0_rmse: 0.95953 | val_1_rmse: 0.96212 |  0:05:38s
epoch 37 | loss: 0.80096 | val_0_rmse: 0.89272 | val_1_rmse: 0.89978 |  0:05:47s
epoch 38 | loss: 0.79604 | val_0_rmse: 0.88863 | val_1_rmse: 0.89354 |  0:05:56s
epoch 39 | loss: 0.79399 | val_0_rmse: 0.97709 | val_1_rmse: 0.98717 |  0:06:05s
epoch 40 | loss: 0.79615 | val_0_rmse: 0.98129 | val_1_rmse: 0.98554 |  0:06:14s
epoch 41 | loss: 0.79213 | val_0_rmse: 0.94885 | val_1_rmse: 0.95778 |  0:06:23s
epoch 42 | loss: 0.79403 | val_0_rmse: 0.93618 | val_1_rmse: 0.94316 |  0:06:33s
epoch 43 | loss: 0.79353 | val_0_rmse: 0.93796 | val_1_rmse: 0.94572 |  0:06:42s
epoch 44 | loss: 0.79464 | val_0_rmse: 0.93257 | val_1_rmse: 0.9387  |  0:06:51s
epoch 45 | loss: 0.79302 | val_0_rmse: 0.94854 | val_1_rmse: 0.95099 |  0:07:00s
epoch 46 | loss: 0.79422 | val_0_rmse: 0.9265  | val_1_rmse: 0.93259 |  0:07:09s
epoch 47 | loss: 0.79426 | val_0_rmse: 0.88578 | val_1_rmse: 0.8911  |  0:07:19s
epoch 48 | loss: 0.79308 | val_0_rmse: 0.93402 | val_1_rmse: 0.94055 |  0:07:28s
epoch 49 | loss: 0.79539 | val_0_rmse: 0.92937 | val_1_rmse: 0.93399 |  0:07:37s
epoch 50 | loss: 0.79058 | val_0_rmse: 0.89098 | val_1_rmse: 0.89573 |  0:07:46s
epoch 51 | loss: 0.78911 | val_0_rmse: 0.92493 | val_1_rmse: 0.93013 |  0:07:55s
epoch 52 | loss: 0.78906 | val_0_rmse: 0.95277 | val_1_rmse: 0.96031 |  0:08:05s
epoch 53 | loss: 0.78566 | val_0_rmse: 0.8907  | val_1_rmse: 0.89381 |  0:08:14s
epoch 54 | loss: 0.78672 | val_0_rmse: 0.93055 | val_1_rmse: 0.93561 |  0:08:23s
epoch 55 | loss: 0.78615 | val_0_rmse: 0.93196 | val_1_rmse: 0.93828 |  0:08:32s
epoch 56 | loss: 0.78823 | val_0_rmse: 1.00395 | val_1_rmse: 1.00119 |  0:08:41s
epoch 57 | loss: 0.78837 | val_0_rmse: 1.01787 | val_1_rmse: 1.01817 |  0:08:50s
epoch 58 | loss: 0.7959  | val_0_rmse: 2.46945 | val_1_rmse: 0.9262  |  0:09:00s
epoch 59 | loss: 0.80073 | val_0_rmse: 0.93366 | val_1_rmse: 0.93371 |  0:09:09s
epoch 60 | loss: 0.79053 | val_0_rmse: 0.9524  | val_1_rmse: 0.96065 |  0:09:18s
epoch 61 | loss: 0.79119 | val_0_rmse: 0.96028 | val_1_rmse: 0.96979 |  0:09:27s
epoch 62 | loss: 0.79353 | val_0_rmse: 0.93675 | val_1_rmse: 0.94266 |  0:09:36s
epoch 63 | loss: 0.79072 | val_0_rmse: 0.91561 | val_1_rmse: 0.91866 |  0:09:45s
epoch 64 | loss: 0.79136 | val_0_rmse: 0.9817  | val_1_rmse: 0.98334 |  0:09:55s
epoch 65 | loss: 0.79666 | val_0_rmse: 0.90085 | val_1_rmse: 0.90494 |  0:10:04s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.88715
Best weights from best epoch are automatically used!
ended training at: 05:19:14
Feature importance:
Mean squared error is of 5327556742.167633
Mean absolute error:57402.84932698929
MAPE:0.5701531007582151
R2 score:0.21187827334918719
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:19:15
epoch 0  | loss: 0.9235  | val_0_rmse: 0.94147 | val_1_rmse: 0.94173 |  0:00:09s
epoch 1  | loss: 0.85781 | val_0_rmse: 0.9332  | val_1_rmse: 0.93183 |  0:00:18s
epoch 2  | loss: 0.84623 | val_0_rmse: 0.94737 | val_1_rmse: 0.94255 |  0:00:27s
epoch 3  | loss: 0.844   | val_0_rmse: 0.92243 | val_1_rmse: 0.92159 |  0:00:36s
epoch 4  | loss: 0.85275 | val_0_rmse: 0.92399 | val_1_rmse: 0.92422 |  0:00:45s
epoch 5  | loss: 0.85279 | val_0_rmse: 0.92169 | val_1_rmse: 0.9212  |  0:00:54s
epoch 6  | loss: 0.85128 | val_0_rmse: 0.92176 | val_1_rmse: 0.92049 |  0:01:03s
epoch 7  | loss: 0.85191 | val_0_rmse: 0.92385 | val_1_rmse: 0.92337 |  0:01:12s
epoch 8  | loss: 0.85162 | val_0_rmse: 0.92169 | val_1_rmse: 0.9203  |  0:01:22s
epoch 9  | loss: 0.85003 | val_0_rmse: 0.92241 | val_1_rmse: 0.92273 |  0:01:31s
epoch 10 | loss: 0.85061 | val_0_rmse: 0.92086 | val_1_rmse: 0.92024 |  0:01:40s
epoch 11 | loss: 0.84985 | val_0_rmse: 0.92161 | val_1_rmse: 0.92096 |  0:01:49s
epoch 12 | loss: 0.8461  | val_0_rmse: 0.96615 | val_1_rmse: 0.97077 |  0:01:58s
epoch 13 | loss: 0.81954 | val_0_rmse: 0.97176 | val_1_rmse: 0.97664 |  0:02:07s
epoch 14 | loss: 0.8131  | val_0_rmse: 0.93947 | val_1_rmse: 0.93576 |  0:02:16s
epoch 15 | loss: 0.80951 | val_0_rmse: 0.96505 | val_1_rmse: 0.95796 |  0:02:25s
epoch 16 | loss: 0.79636 | val_0_rmse: 1.0115  | val_1_rmse: 1.01789 |  0:02:34s
epoch 17 | loss: 0.79068 | val_0_rmse: 0.95884 | val_1_rmse: 0.95855 |  0:02:44s
epoch 18 | loss: 0.78835 | val_0_rmse: 0.9628  | val_1_rmse: 0.96794 |  0:02:53s
epoch 19 | loss: 0.78544 | val_0_rmse: 0.9827  | val_1_rmse: 0.97696 |  0:03:02s
epoch 20 | loss: 0.7876  | val_0_rmse: 0.98581 | val_1_rmse: 0.98958 |  0:03:11s
epoch 21 | loss: 0.78437 | val_0_rmse: 0.98976 | val_1_rmse: 0.99499 |  0:03:20s
epoch 22 | loss: 0.78507 | val_0_rmse: 1.01024 | val_1_rmse: 1.01332 |  0:03:29s
epoch 23 | loss: 0.78244 | val_0_rmse: 0.99812 | val_1_rmse: 0.99263 |  0:03:38s
epoch 24 | loss: 0.78238 | val_0_rmse: 0.96863 | val_1_rmse: 0.97331 |  0:03:48s
epoch 25 | loss: 0.78266 | val_0_rmse: 0.93048 | val_1_rmse: 0.92738 |  0:03:57s
epoch 26 | loss: 0.7851  | val_0_rmse: 1.01954 | val_1_rmse: 1.02595 |  0:04:06s
epoch 27 | loss: 0.78545 | val_0_rmse: 0.95532 | val_1_rmse: 0.96096 |  0:04:15s
epoch 28 | loss: 0.78197 | val_0_rmse: 0.98807 | val_1_rmse: 0.99216 |  0:04:24s
epoch 29 | loss: 0.77976 | val_0_rmse: 0.96975 | val_1_rmse: 0.96612 |  0:04:33s
epoch 30 | loss: 0.7843  | val_0_rmse: 0.9153  | val_1_rmse: 0.91405 |  0:04:43s
epoch 31 | loss: 0.78518 | val_0_rmse: 0.96743 | val_1_rmse: 0.97102 |  0:04:52s
epoch 32 | loss: 0.782   | val_0_rmse: 0.97367 | val_1_rmse: 0.97735 |  0:05:01s
epoch 33 | loss: 0.78155 | val_0_rmse: 0.99342 | val_1_rmse: 1.00025 |  0:05:10s
epoch 34 | loss: 0.78003 | val_0_rmse: 0.96559 | val_1_rmse: 0.96527 |  0:05:20s
epoch 35 | loss: 0.78088 | val_0_rmse: 0.90227 | val_1_rmse: 0.90132 |  0:05:29s
epoch 36 | loss: 0.77951 | val_0_rmse: 0.97681 | val_1_rmse: 0.97796 |  0:05:38s
epoch 37 | loss: 0.77943 | val_0_rmse: 0.9327  | val_1_rmse: 0.92907 |  0:05:47s
epoch 38 | loss: 0.77862 | val_0_rmse: 0.9333  | val_1_rmse: 0.93174 |  0:05:57s
epoch 39 | loss: 0.77994 | val_0_rmse: 0.95586 | val_1_rmse: 0.95663 |  0:06:06s
epoch 40 | loss: 0.7788  | val_0_rmse: 0.9178  | val_1_rmse: 0.91786 |  0:06:15s
epoch 41 | loss: 0.77842 | val_0_rmse: 0.93074 | val_1_rmse: 0.92925 |  0:06:24s
epoch 42 | loss: 0.77837 | val_0_rmse: 0.94101 | val_1_rmse: 0.93812 |  0:06:33s
epoch 43 | loss: 0.7781  | val_0_rmse: 0.94371 | val_1_rmse: 0.94027 |  0:06:42s
epoch 44 | loss: 0.77937 | val_0_rmse: 0.9185  | val_1_rmse: 0.91804 |  0:06:51s
epoch 45 | loss: 0.77823 | val_0_rmse: 0.91614 | val_1_rmse: 0.91534 |  0:07:00s
epoch 46 | loss: 0.77884 | val_0_rmse: 1.00261 | val_1_rmse: 1.0086  |  0:07:10s
epoch 47 | loss: 0.7787  | val_0_rmse: 1.00463 | val_1_rmse: 1.0102  |  0:07:19s
epoch 48 | loss: 0.77931 | val_0_rmse: 0.93891 | val_1_rmse: 0.94333 |  0:07:28s
epoch 49 | loss: 0.78048 | val_0_rmse: 0.92735 | val_1_rmse: 0.92421 |  0:07:37s
epoch 50 | loss: 0.78308 | val_0_rmse: 0.93945 | val_1_rmse: 0.93759 |  0:07:46s
epoch 51 | loss: 0.77987 | val_0_rmse: 0.94482 | val_1_rmse: 0.9413  |  0:07:55s
epoch 52 | loss: 0.77893 | val_0_rmse: 0.93593 | val_1_rmse: 0.93291 |  0:08:04s
epoch 53 | loss: 0.77931 | val_0_rmse: 0.96028 | val_1_rmse: 0.96076 |  0:08:14s
epoch 54 | loss: 0.78034 | val_0_rmse: 0.97698 | val_1_rmse: 0.98206 |  0:08:23s
epoch 55 | loss: 0.78056 | val_0_rmse: 0.93318 | val_1_rmse: 0.93263 |  0:08:32s
epoch 56 | loss: 0.77743 | val_0_rmse: 0.90575 | val_1_rmse: 0.90513 |  0:08:41s
epoch 57 | loss: 0.77706 | val_0_rmse: 0.9569  | val_1_rmse: 0.9553  |  0:08:50s
epoch 58 | loss: 0.77785 | val_0_rmse: 0.96964 | val_1_rmse: 0.97527 |  0:08:59s
epoch 59 | loss: 0.77914 | val_0_rmse: 0.9543  | val_1_rmse: 0.95046 |  0:09:08s
epoch 60 | loss: 0.77898 | val_0_rmse: 0.95593 | val_1_rmse: 0.9543  |  0:09:17s
epoch 61 | loss: 0.78181 | val_0_rmse: 0.9777  | val_1_rmse: 0.98275 |  0:09:26s
epoch 62 | loss: 0.77868 | val_0_rmse: 0.92912 | val_1_rmse: 0.92759 |  0:09:36s
epoch 63 | loss: 0.77677 | val_0_rmse: 0.91593 | val_1_rmse: 0.91498 |  0:09:45s
epoch 64 | loss: 0.77622 | val_0_rmse: 0.92099 | val_1_rmse: 0.91921 |  0:09:54s
epoch 65 | loss: 0.77444 | val_0_rmse: 0.9481  | val_1_rmse: 0.94412 |  0:10:03s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.90132
Best weights from best epoch are automatically used!
ended training at: 05:29:21
Feature importance:
Mean squared error is of 5545183660.741437
Mean absolute error:56662.558117189954
MAPE:0.5251286072120509
R2 score:0.18362026107528806
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:29:22
epoch 0  | loss: 0.95589 | val_0_rmse: 0.93158 | val_1_rmse: 0.92502 |  0:00:09s
epoch 1  | loss: 0.85604 | val_0_rmse: 0.93641 | val_1_rmse: 0.9279  |  0:00:18s
epoch 2  | loss: 0.84955 | val_0_rmse: 0.92075 | val_1_rmse: 0.91267 |  0:00:27s
epoch 3  | loss: 0.83766 | val_0_rmse: 0.92874 | val_1_rmse: 0.92105 |  0:00:36s
epoch 4  | loss: 0.82661 | val_0_rmse: 0.90649 | val_1_rmse: 0.89825 |  0:00:45s
epoch 5  | loss: 0.82401 | val_0_rmse: 0.91325 | val_1_rmse: 0.90397 |  0:00:54s
epoch 6  | loss: 0.82115 | val_0_rmse: 0.95359 | val_1_rmse: 0.94074 |  0:01:04s
epoch 7  | loss: 0.81686 | val_0_rmse: 1.04453 | val_1_rmse: 1.04331 |  0:01:13s
epoch 8  | loss: 0.80222 | val_0_rmse: 1.01945 | val_1_rmse: 1.00513 |  0:01:22s
epoch 9  | loss: 0.79316 | val_0_rmse: 0.97962 | val_1_rmse: 0.96488 |  0:01:31s
epoch 10 | loss: 0.78736 | val_0_rmse: 0.96935 | val_1_rmse: 0.95656 |  0:01:40s
epoch 11 | loss: 0.79013 | val_0_rmse: 0.89172 | val_1_rmse: 0.88392 |  0:01:49s
epoch 12 | loss: 0.78615 | val_0_rmse: 1.02942 | val_1_rmse: 1.03032 |  0:01:58s
epoch 13 | loss: 0.78472 | val_0_rmse: 0.9372  | val_1_rmse: 0.92537 |  0:02:07s
epoch 14 | loss: 0.7877  | val_0_rmse: 0.93413 | val_1_rmse: 0.93135 |  0:02:16s
epoch 15 | loss: 0.78425 | val_0_rmse: 0.91671 | val_1_rmse: 0.90532 |  0:02:26s
epoch 16 | loss: 0.78697 | val_0_rmse: 0.90558 | val_1_rmse: 0.90075 |  0:02:35s
epoch 17 | loss: 0.78439 | val_0_rmse: 0.97993 | val_1_rmse: 0.97926 |  0:02:44s
epoch 18 | loss: 0.78345 | val_0_rmse: 1.07073 | val_1_rmse: 1.06985 |  0:02:53s
epoch 19 | loss: 0.78209 | val_0_rmse: 0.92166 | val_1_rmse: 0.91177 |  0:03:02s
epoch 20 | loss: 0.78056 | val_0_rmse: 0.97236 | val_1_rmse: 0.97041 |  0:03:11s
epoch 21 | loss: 0.77842 | val_0_rmse: 0.95262 | val_1_rmse: 0.9404  |  0:03:21s
epoch 22 | loss: 0.78192 | val_0_rmse: 0.89087 | val_1_rmse: 0.88336 |  0:03:30s
epoch 23 | loss: 0.7784  | val_0_rmse: 0.98544 | val_1_rmse: 0.97223 |  0:03:39s
epoch 24 | loss: 0.77956 | val_0_rmse: 0.93037 | val_1_rmse: 0.919   |  0:03:48s
epoch 25 | loss: 0.78081 | val_0_rmse: 1.22668 | val_1_rmse: 1.22852 |  0:03:57s
epoch 26 | loss: 0.77671 | val_0_rmse: 0.93445 | val_1_rmse: 0.9231  |  0:04:07s
epoch 27 | loss: 0.77995 | val_0_rmse: 0.97651 | val_1_rmse: 0.96476 |  0:04:16s
epoch 28 | loss: 0.78063 | val_0_rmse: 0.91886 | val_1_rmse: 0.90918 |  0:04:25s
epoch 29 | loss: 0.78094 | val_0_rmse: 0.91385 | val_1_rmse: 0.90288 |  0:04:34s
epoch 30 | loss: 0.77535 | val_0_rmse: 0.92573 | val_1_rmse: 0.91475 |  0:04:43s
epoch 31 | loss: 0.78483 | val_0_rmse: 0.91804 | val_1_rmse: 0.90706 |  0:04:52s
epoch 32 | loss: 0.77728 | val_0_rmse: 0.93958 | val_1_rmse: 0.928   |  0:05:01s
epoch 33 | loss: 0.78577 | val_0_rmse: 0.91708 | val_1_rmse: 0.90634 |  0:05:10s
epoch 34 | loss: 0.77561 | val_0_rmse: 0.90733 | val_1_rmse: 0.89629 |  0:05:20s
epoch 35 | loss: 0.77368 | val_0_rmse: 0.88231 | val_1_rmse: 0.87612 |  0:05:29s
epoch 36 | loss: 0.77512 | val_0_rmse: 1.05369 | val_1_rmse: 1.05161 |  0:05:38s
epoch 37 | loss: 0.77199 | val_0_rmse: 0.94765 | val_1_rmse: 0.9359  |  0:05:47s
epoch 38 | loss: 0.77263 | val_0_rmse: 0.94548 | val_1_rmse: 0.93194 |  0:05:56s
epoch 39 | loss: 0.77222 | val_0_rmse: 0.90486 | val_1_rmse: 0.89391 |  0:06:05s
epoch 40 | loss: 0.77361 | val_0_rmse: 0.98434 | val_1_rmse: 0.98477 |  0:06:15s
epoch 41 | loss: 0.77147 | val_0_rmse: 0.91174 | val_1_rmse: 0.90191 |  0:06:24s
epoch 42 | loss: 0.76983 | val_0_rmse: 0.93292 | val_1_rmse: 0.92896 |  0:06:33s
epoch 43 | loss: 0.7691  | val_0_rmse: 0.91881 | val_1_rmse: 0.90796 |  0:06:42s
epoch 44 | loss: 0.77015 | val_0_rmse: 0.92179 | val_1_rmse: 0.9109  |  0:06:51s
epoch 45 | loss: 0.77033 | val_0_rmse: 0.90187 | val_1_rmse: 0.89106 |  0:07:00s
epoch 46 | loss: 0.76976 | val_0_rmse: 1.01858 | val_1_rmse: 1.01934 |  0:07:10s
epoch 47 | loss: 0.77071 | val_0_rmse: 0.88577 | val_1_rmse: 0.87952 |  0:07:19s
epoch 48 | loss: 0.76906 | val_0_rmse: 1.04194 | val_1_rmse: 1.04081 |  0:07:28s
epoch 49 | loss: 0.76967 | val_0_rmse: 0.91942 | val_1_rmse: 0.90776 |  0:07:37s
epoch 50 | loss: 0.77116 | val_0_rmse: 1.02902 | val_1_rmse: 1.03147 |  0:07:46s
epoch 51 | loss: 0.77298 | val_0_rmse: 0.94476 | val_1_rmse: 0.94165 |  0:07:55s
epoch 52 | loss: 0.7811  | val_0_rmse: 0.88049 | val_1_rmse: 0.87326 |  0:08:05s
epoch 53 | loss: 0.77605 | val_0_rmse: 0.92555 | val_1_rmse: 0.92187 |  0:08:14s
epoch 54 | loss: 0.77446 | val_0_rmse: 0.98436 | val_1_rmse: 0.98571 |  0:08:23s
epoch 55 | loss: 0.77348 | val_0_rmse: 0.93131 | val_1_rmse: 0.91947 |  0:08:32s
epoch 56 | loss: 0.76763 | val_0_rmse: 0.92206 | val_1_rmse: 0.91095 |  0:08:41s
epoch 57 | loss: 0.76848 | val_0_rmse: 0.9042  | val_1_rmse: 0.89352 |  0:08:50s
epoch 58 | loss: 0.77195 | val_0_rmse: 0.93419 | val_1_rmse: 0.9333  |  0:09:00s
epoch 59 | loss: 0.76918 | val_0_rmse: 0.99184 | val_1_rmse: 0.99135 |  0:09:09s
epoch 60 | loss: 0.76972 | val_0_rmse: 0.91204 | val_1_rmse: 0.90163 |  0:09:18s
epoch 61 | loss: 0.76702 | val_0_rmse: 0.94746 | val_1_rmse: 0.9354  |  0:09:27s
epoch 62 | loss: 0.76824 | val_0_rmse: 0.90092 | val_1_rmse: 0.89636 |  0:09:36s
epoch 63 | loss: 0.7744  | val_0_rmse: 1.05655 | val_1_rmse: 1.05097 |  0:09:45s
epoch 64 | loss: 0.7679  | val_0_rmse: 0.9701  | val_1_rmse: 0.97034 |  0:09:55s
epoch 65 | loss: 0.76539 | val_0_rmse: 0.94087 | val_1_rmse: 0.92668 |  0:10:04s
epoch 66 | loss: 0.76749 | val_0_rmse: 0.9074  | val_1_rmse: 0.90335 |  0:10:13s
epoch 67 | loss: 0.76686 | val_0_rmse: 0.91272 | val_1_rmse: 0.90108 |  0:10:22s
epoch 68 | loss: 0.76743 | val_0_rmse: 0.89175 | val_1_rmse: 0.88522 |  0:10:31s
epoch 69 | loss: 0.76591 | val_0_rmse: 0.93581 | val_1_rmse: 0.92363 |  0:10:40s
epoch 70 | loss: 0.77265 | val_0_rmse: 0.89328 | val_1_rmse: 0.88431 |  0:10:50s
epoch 71 | loss: 0.77063 | val_0_rmse: 0.87806 | val_1_rmse: 0.87183 |  0:10:59s
epoch 72 | loss: 0.76555 | val_0_rmse: 1.00699 | val_1_rmse: 1.00738 |  0:11:08s
epoch 73 | loss: 0.77111 | val_0_rmse: 0.92927 | val_1_rmse: 0.9184  |  0:11:17s
epoch 74 | loss: 0.77007 | val_0_rmse: 0.9428  | val_1_rmse: 0.93756 |  0:11:26s
epoch 75 | loss: 0.77094 | val_0_rmse: 0.90002 | val_1_rmse: 0.89133 |  0:11:36s
epoch 76 | loss: 0.76771 | val_0_rmse: 0.87273 | val_1_rmse: 0.86662 |  0:11:45s
epoch 77 | loss: 0.76419 | val_0_rmse: 0.8827  | val_1_rmse: 0.87733 |  0:11:54s
epoch 78 | loss: 0.76563 | val_0_rmse: 0.93502 | val_1_rmse: 0.92969 |  0:12:03s
epoch 79 | loss: 0.76403 | val_0_rmse: 0.89611 | val_1_rmse: 0.88539 |  0:12:13s
epoch 80 | loss: 0.76412 | val_0_rmse: 0.885   | val_1_rmse: 0.8766  |  0:12:22s
epoch 81 | loss: 0.76303 | val_0_rmse: 0.89772 | val_1_rmse: 0.89056 |  0:12:31s
epoch 82 | loss: 0.76163 | val_0_rmse: 0.89684 | val_1_rmse: 0.89482 |  0:12:40s
epoch 83 | loss: 0.76237 | val_0_rmse: 0.90736 | val_1_rmse: 0.89849 |  0:12:49s
epoch 84 | loss: 0.76039 | val_0_rmse: 0.8863  | val_1_rmse: 0.88208 |  0:12:59s
epoch 85 | loss: 0.75872 | val_0_rmse: 0.92778 | val_1_rmse: 0.92217 |  0:13:08s
epoch 86 | loss: 0.76153 | val_0_rmse: 0.98545 | val_1_rmse: 0.98536 |  0:13:17s
epoch 87 | loss: 0.76141 | val_0_rmse: 1.01711 | val_1_rmse: 1.02004 |  0:13:26s
epoch 88 | loss: 0.75687 | val_0_rmse: 0.91051 | val_1_rmse: 0.90886 |  0:13:35s
epoch 89 | loss: 0.75974 | val_0_rmse: 0.91551 | val_1_rmse: 0.90453 |  0:13:45s
epoch 90 | loss: 0.75651 | val_0_rmse: 0.90177 | val_1_rmse: 0.89537 |  0:13:54s
epoch 91 | loss: 0.76055 | val_0_rmse: 0.89079 | val_1_rmse: 0.88202 |  0:14:03s
epoch 92 | loss: 0.7594  | val_0_rmse: 0.88943 | val_1_rmse: 0.88092 |  0:14:12s
epoch 93 | loss: 0.75725 | val_0_rmse: 0.87525 | val_1_rmse: 0.869   |  0:14:22s
epoch 94 | loss: 0.75849 | val_0_rmse: 0.87784 | val_1_rmse: 0.86868 |  0:14:31s
epoch 95 | loss: 0.75673 | val_0_rmse: 0.87799 | val_1_rmse: 0.87144 |  0:14:40s
epoch 96 | loss: 0.76015 | val_0_rmse: 0.92145 | val_1_rmse: 0.91827 |  0:14:49s
epoch 97 | loss: 0.75668 | val_0_rmse: 0.88643 | val_1_rmse: 0.88067 |  0:14:58s
epoch 98 | loss: 0.75637 | val_0_rmse: 0.90389 | val_1_rmse: 0.89493 |  0:15:07s
epoch 99 | loss: 0.75731 | val_0_rmse: 0.90532 | val_1_rmse: 0.90111 |  0:15:16s
epoch 100| loss: 0.76018 | val_0_rmse: 0.91304 | val_1_rmse: 0.91015 |  0:15:26s
epoch 101| loss: 0.75744 | val_0_rmse: 0.89393 | val_1_rmse: 0.88335 |  0:15:35s
epoch 102| loss: 0.75611 | val_0_rmse: 0.95955 | val_1_rmse: 0.94615 |  0:15:44s
epoch 103| loss: 0.7612  | val_0_rmse: 0.90508 | val_1_rmse: 0.89462 |  0:15:53s
epoch 104| loss: 0.75733 | val_0_rmse: 0.92537 | val_1_rmse: 0.88711 |  0:16:02s
epoch 105| loss: 0.75888 | val_0_rmse: 1.0504  | val_1_rmse: 1.0519  |  0:16:11s
epoch 106| loss: 0.75657 | val_0_rmse: 0.89441 | val_1_rmse: 0.88713 |  0:16:20s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.86662
Best weights from best epoch are automatically used!
ended training at: 05:45:46
Feature importance:
Mean squared error is of 5200005970.506087
Mean absolute error:56169.01431234837
MAPE:0.5448789636384606
R2 score:0.2357569151613812
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:47
epoch 0  | loss: 1.15242 | val_0_rmse: 0.98077 | val_1_rmse: 0.97158 |  0:00:03s
epoch 1  | loss: 0.81249 | val_0_rmse: 1.00168 | val_1_rmse: 0.99225 |  0:00:06s
epoch 2  | loss: 0.81823 | val_0_rmse: 0.95945 | val_1_rmse: 0.94936 |  0:00:09s
epoch 3  | loss: 0.84416 | val_0_rmse: 0.97849 | val_1_rmse: 0.96825 |  0:00:13s
epoch 4  | loss: 0.83816 | val_0_rmse: 0.95565 | val_1_rmse: 0.94667 |  0:00:16s
epoch 5  | loss: 0.80767 | val_0_rmse: 0.92882 | val_1_rmse: 0.92248 |  0:00:19s
epoch 6  | loss: 0.80043 | val_0_rmse: 0.92009 | val_1_rmse: 0.91403 |  0:00:23s
epoch 7  | loss: 0.79977 | val_0_rmse: 0.92383 | val_1_rmse: 0.91815 |  0:00:26s
epoch 8  | loss: 0.79766 | val_0_rmse: 1.01224 | val_1_rmse: 1.00127 |  0:00:29s
epoch 9  | loss: 0.7917  | val_0_rmse: 0.92628 | val_1_rmse: 0.92025 |  0:00:33s
epoch 10 | loss: 0.79063 | val_0_rmse: 0.92255 | val_1_rmse: 0.91524 |  0:00:36s
epoch 11 | loss: 0.78952 | val_0_rmse: 0.9641  | val_1_rmse: 0.95522 |  0:00:39s
epoch 12 | loss: 0.78812 | val_0_rmse: 0.91201 | val_1_rmse: 0.90678 |  0:00:42s
epoch 13 | loss: 0.78532 | val_0_rmse: 1.09037 | val_1_rmse: 1.07949 |  0:00:46s
epoch 14 | loss: 0.78475 | val_0_rmse: 0.91807 | val_1_rmse: 0.91553 |  0:00:49s
epoch 15 | loss: 0.78309 | val_0_rmse: 0.91156 | val_1_rmse: 0.90801 |  0:00:52s
epoch 16 | loss: 0.7847  | val_0_rmse: 1.00208 | val_1_rmse: 0.99187 |  0:00:55s
epoch 17 | loss: 0.78187 | val_0_rmse: 0.91475 | val_1_rmse: 0.90768 |  0:00:59s
epoch 18 | loss: 0.78112 | val_0_rmse: 0.94135 | val_1_rmse: 0.9344  |  0:01:02s
epoch 19 | loss: 0.78496 | val_0_rmse: 0.93273 | val_1_rmse: 0.92501 |  0:01:05s
epoch 20 | loss: 0.77915 | val_0_rmse: 0.90871 | val_1_rmse: 0.90006 |  0:01:08s
epoch 21 | loss: 0.78137 | val_0_rmse: 0.90452 | val_1_rmse: 0.89638 |  0:01:12s
epoch 22 | loss: 0.78432 | val_0_rmse: 0.91467 | val_1_rmse: 0.9078  |  0:01:15s
epoch 23 | loss: 0.78237 | val_0_rmse: 0.92793 | val_1_rmse: 0.92628 |  0:01:18s
epoch 24 | loss: 0.78175 | val_0_rmse: 0.9294  | val_1_rmse: 0.92135 |  0:01:21s
epoch 25 | loss: 0.78255 | val_0_rmse: 0.90806 | val_1_rmse: 0.90088 |  0:01:25s
epoch 26 | loss: 0.78032 | val_0_rmse: 0.93733 | val_1_rmse: 0.93014 |  0:01:28s
epoch 27 | loss: 0.77787 | val_0_rmse: 0.89354 | val_1_rmse: 0.88411 |  0:01:31s
epoch 28 | loss: 0.77998 | val_0_rmse: 0.88181 | val_1_rmse: 0.87469 |  0:01:34s
epoch 29 | loss: 0.77735 | val_0_rmse: 0.87655 | val_1_rmse: 0.86968 |  0:01:38s
epoch 30 | loss: 0.77898 | val_0_rmse: 0.92601 | val_1_rmse: 0.91702 |  0:01:41s
epoch 31 | loss: 0.77498 | val_0_rmse: 0.99896 | val_1_rmse: 0.98659 |  0:01:44s
epoch 32 | loss: 0.77717 | val_0_rmse: 0.96707 | val_1_rmse: 0.95779 |  0:01:47s
epoch 33 | loss: 0.77845 | val_0_rmse: 0.9373  | val_1_rmse: 0.92837 |  0:01:51s
epoch 34 | loss: 0.77698 | val_0_rmse: 0.96669 | val_1_rmse: 0.95678 |  0:01:54s
epoch 35 | loss: 0.77769 | val_0_rmse: 0.88317 | val_1_rmse: 0.8754  |  0:01:57s
epoch 36 | loss: 0.77562 | val_0_rmse: 0.91354 | val_1_rmse: 0.90891 |  0:02:00s
epoch 37 | loss: 0.78048 | val_0_rmse: 0.90952 | val_1_rmse: 0.9081  |  0:02:04s
epoch 38 | loss: 0.77691 | val_0_rmse: 0.92812 | val_1_rmse: 0.92323 |  0:02:07s
epoch 39 | loss: 0.77905 | val_0_rmse: 0.93428 | val_1_rmse: 0.92785 |  0:02:10s
epoch 40 | loss: 0.77742 | val_0_rmse: 0.99727 | val_1_rmse: 1.04729 |  0:02:13s
epoch 41 | loss: 0.77929 | val_0_rmse: 0.91516 | val_1_rmse: 0.91451 |  0:02:17s
epoch 42 | loss: 0.77533 | val_0_rmse: 0.92179 | val_1_rmse: 0.91525 |  0:02:20s
epoch 43 | loss: 0.77524 | val_0_rmse: 0.9729  | val_1_rmse: 0.97309 |  0:02:23s
epoch 44 | loss: 0.77604 | val_0_rmse: 0.99041 | val_1_rmse: 1.03597 |  0:02:26s
epoch 45 | loss: 0.77643 | val_0_rmse: 0.93723 | val_1_rmse: 0.92968 |  0:02:30s
epoch 46 | loss: 0.77861 | val_0_rmse: 0.94768 | val_1_rmse: 0.93754 |  0:02:33s
epoch 47 | loss: 0.77798 | val_0_rmse: 0.94885 | val_1_rmse: 0.99214 |  0:02:36s
epoch 48 | loss: 0.77914 | val_0_rmse: 0.98662 | val_1_rmse: 1.05456 |  0:02:39s
epoch 49 | loss: 0.78192 | val_0_rmse: 0.94037 | val_1_rmse: 0.96212 |  0:02:43s
epoch 50 | loss: 0.78038 | val_0_rmse: 0.96457 | val_1_rmse: 0.98392 |  0:02:46s
epoch 51 | loss: 0.78321 | val_0_rmse: 0.97966 | val_1_rmse: 1.02316 |  0:02:49s
epoch 52 | loss: 0.77929 | val_0_rmse: 0.92604 | val_1_rmse: 0.94464 |  0:02:52s
epoch 53 | loss: 0.78172 | val_0_rmse: 0.99362 | val_1_rmse: 1.0144  |  0:02:55s
epoch 54 | loss: 0.77881 | val_0_rmse: 0.97757 | val_1_rmse: 1.00687 |  0:02:59s
epoch 55 | loss: 0.77421 | val_0_rmse: 0.97364 | val_1_rmse: 1.0319  |  0:03:02s
epoch 56 | loss: 0.77629 | val_0_rmse: 0.95372 | val_1_rmse: 0.99775 |  0:03:05s
epoch 57 | loss: 0.77516 | val_0_rmse: 0.95402 | val_1_rmse: 0.98654 |  0:03:09s
epoch 58 | loss: 0.78691 | val_0_rmse: 0.96557 | val_1_rmse: 0.99752 |  0:03:12s
epoch 59 | loss: 0.78606 | val_0_rmse: 0.9067  | val_1_rmse: 0.92731 |  0:03:15s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.86968
Best weights from best epoch are automatically used!
ended training at: 05:49:03
Feature importance:
Mean squared error is of 3133063297.332233
Mean absolute error:42463.84574098246
MAPE:0.6195814522080422
R2 score:0.231091473911212
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:49:04
epoch 0  | loss: 1.14134 | val_0_rmse: 0.96517 | val_1_rmse: 0.97662 |  0:00:03s
epoch 1  | loss: 0.85199 | val_0_rmse: 0.94747 | val_1_rmse: 0.95847 |  0:00:06s
epoch 2  | loss: 0.83875 | val_0_rmse: 0.95527 | val_1_rmse: 0.96768 |  0:00:09s
epoch 3  | loss: 0.8229  | val_0_rmse: 1.06059 | val_1_rmse: 1.06937 |  0:00:13s
epoch 4  | loss: 0.79182 | val_0_rmse: 0.90258 | val_1_rmse: 0.91401 |  0:00:16s
epoch 5  | loss: 0.78107 | val_0_rmse: 0.9794  | val_1_rmse: 0.99468 |  0:00:19s
epoch 6  | loss: 0.77815 | val_0_rmse: 0.91812 | val_1_rmse: 0.92671 |  0:00:22s
epoch 7  | loss: 0.77726 | val_0_rmse: 0.95503 | val_1_rmse: 0.97296 |  0:00:26s
epoch 8  | loss: 0.77415 | val_0_rmse: 0.92461 | val_1_rmse: 0.94146 |  0:00:29s
epoch 9  | loss: 0.77359 | val_0_rmse: 0.89485 | val_1_rmse: 0.90361 |  0:00:32s
epoch 10 | loss: 0.77513 | val_0_rmse: 0.99019 | val_1_rmse: 0.99258 |  0:00:35s
epoch 11 | loss: 0.7676  | val_0_rmse: 0.89989 | val_1_rmse: 0.90662 |  0:00:39s
epoch 12 | loss: 0.76444 | val_0_rmse: 0.89972 | val_1_rmse: 0.91221 |  0:00:42s
epoch 13 | loss: 0.76155 | val_0_rmse: 0.90192 | val_1_rmse: 0.91625 |  0:00:45s
epoch 14 | loss: 0.76098 | val_0_rmse: 0.96034 | val_1_rmse: 0.97408 |  0:00:48s
epoch 15 | loss: 0.75625 | val_0_rmse: 0.91748 | val_1_rmse: 0.9324  |  0:00:51s
epoch 16 | loss: 0.75691 | val_0_rmse: 0.90646 | val_1_rmse: 0.91846 |  0:00:55s
epoch 17 | loss: 0.75872 | val_0_rmse: 0.91291 | val_1_rmse: 0.93215 |  0:00:58s
epoch 18 | loss: 0.75536 | val_0_rmse: 0.96059 | val_1_rmse: 0.97581 |  0:01:01s
epoch 19 | loss: 0.75937 | val_0_rmse: 0.94145 | val_1_rmse: 0.94961 |  0:01:04s
epoch 20 | loss: 0.76375 | val_0_rmse: 0.88061 | val_1_rmse: 0.89217 |  0:01:08s
epoch 21 | loss: 0.75969 | val_0_rmse: 0.89634 | val_1_rmse: 0.90316 |  0:01:11s
epoch 22 | loss: 0.75737 | val_0_rmse: 0.92391 | val_1_rmse: 0.93858 |  0:01:14s
epoch 23 | loss: 0.75246 | val_0_rmse: 0.89941 | val_1_rmse: 0.91652 |  0:01:18s
epoch 24 | loss: 0.75609 | val_0_rmse: 0.92113 | val_1_rmse: 0.94137 |  0:01:21s
epoch 25 | loss: 0.75438 | val_0_rmse: 0.95877 | val_1_rmse: 0.9797  |  0:01:24s
epoch 26 | loss: 0.75051 | val_0_rmse: 0.95954 | val_1_rmse: 0.98208 |  0:01:27s
epoch 27 | loss: 0.75427 | val_0_rmse: 0.96429 | val_1_rmse: 0.97002 |  0:01:30s
epoch 28 | loss: 0.75044 | val_0_rmse: 1.0058  | val_1_rmse: 1.0255  |  0:01:34s
epoch 29 | loss: 0.75274 | val_0_rmse: 1.16164 | val_1_rmse: 1.17541 |  0:01:37s
epoch 30 | loss: 0.75849 | val_0_rmse: 0.89741 | val_1_rmse: 0.91658 |  0:01:40s
epoch 31 | loss: 0.75435 | val_0_rmse: 0.9082  | val_1_rmse: 0.91578 |  0:01:44s
epoch 32 | loss: 0.75366 | val_0_rmse: 0.95    | val_1_rmse: 0.96104 |  0:01:47s
epoch 33 | loss: 0.75288 | val_0_rmse: 0.90014 | val_1_rmse: 0.90788 |  0:01:50s
epoch 34 | loss: 0.75165 | val_0_rmse: 0.93916 | val_1_rmse: 0.95221 |  0:01:53s
epoch 35 | loss: 0.75438 | val_0_rmse: 0.90468 | val_1_rmse: 0.91226 |  0:01:57s
epoch 36 | loss: 0.7508  | val_0_rmse: 0.88909 | val_1_rmse: 0.90161 |  0:02:00s
epoch 37 | loss: 0.74753 | val_0_rmse: 1.02949 | val_1_rmse: 1.04454 |  0:02:03s
epoch 38 | loss: 0.75342 | val_0_rmse: 0.87641 | val_1_rmse: 0.88621 |  0:02:06s
epoch 39 | loss: 0.75033 | val_0_rmse: 0.91216 | val_1_rmse: 0.93299 |  0:02:10s
epoch 40 | loss: 0.75004 | val_0_rmse: 0.96875 | val_1_rmse: 0.98412 |  0:02:13s
epoch 41 | loss: 0.75117 | val_0_rmse: 0.94039 | val_1_rmse: 0.95193 |  0:02:16s
epoch 42 | loss: 0.74939 | val_0_rmse: 0.91681 | val_1_rmse: 0.92457 |  0:02:19s
epoch 43 | loss: 0.75069 | val_0_rmse: 0.89897 | val_1_rmse: 0.91507 |  0:02:23s
epoch 44 | loss: 0.75912 | val_0_rmse: 1.05236 | val_1_rmse: 1.06534 |  0:02:26s
epoch 45 | loss: 0.7587  | val_0_rmse: 0.92093 | val_1_rmse: 0.93389 |  0:02:29s
epoch 46 | loss: 0.77013 | val_0_rmse: 0.92928 | val_1_rmse: 0.93979 |  0:02:32s
epoch 47 | loss: 0.76941 | val_0_rmse: 1.00504 | val_1_rmse: 1.0219  |  0:02:36s
epoch 48 | loss: 0.76835 | val_0_rmse: 0.89485 | val_1_rmse: 0.91036 |  0:02:39s
epoch 49 | loss: 0.77142 | val_0_rmse: 0.91127 | val_1_rmse: 0.92763 |  0:02:42s
epoch 50 | loss: 0.76627 | val_0_rmse: 0.8838  | val_1_rmse: 0.89562 |  0:02:46s
epoch 51 | loss: 0.76918 | val_0_rmse: 0.93944 | val_1_rmse: 0.94648 |  0:02:49s
epoch 52 | loss: 0.76911 | val_0_rmse: 0.90906 | val_1_rmse: 0.92633 |  0:02:52s
epoch 53 | loss: 0.76859 | val_0_rmse: 1.07128 | val_1_rmse: 1.06858 |  0:02:55s
epoch 54 | loss: 0.77097 | val_0_rmse: 1.17259 | val_1_rmse: 1.15429 |  0:02:58s
epoch 55 | loss: 0.769   | val_0_rmse: 1.08931 | val_1_rmse: 1.10639 |  0:03:02s
epoch 56 | loss: 0.76776 | val_0_rmse: 0.87497 | val_1_rmse: 0.8878  |  0:03:05s
epoch 57 | loss: 0.76686 | val_0_rmse: 0.88602 | val_1_rmse: 0.89776 |  0:03:08s
epoch 58 | loss: 0.76788 | val_0_rmse: 0.90513 | val_1_rmse: 0.91449 |  0:03:12s
epoch 59 | loss: 0.76688 | val_0_rmse: 0.88952 | val_1_rmse: 0.90122 |  0:03:15s
epoch 60 | loss: 0.76533 | val_0_rmse: 0.92564 | val_1_rmse: 0.93453 |  0:03:18s
epoch 61 | loss: 0.7647  | val_0_rmse: 0.88443 | val_1_rmse: 0.89604 |  0:03:21s
epoch 62 | loss: 0.77302 | val_0_rmse: 0.87936 | val_1_rmse: 0.89275 |  0:03:25s
epoch 63 | loss: 0.77076 | val_0_rmse: 0.88008 | val_1_rmse: 0.89191 |  0:03:28s
epoch 64 | loss: 0.76442 | val_0_rmse: 0.89239 | val_1_rmse: 0.9071  |  0:03:31s
epoch 65 | loss: 0.76532 | val_0_rmse: 0.87922 | val_1_rmse: 0.89161 |  0:03:34s
epoch 66 | loss: 0.76119 | val_0_rmse: 0.91287 | val_1_rmse: 0.92407 |  0:03:38s
epoch 67 | loss: 0.76113 | val_0_rmse: 0.87943 | val_1_rmse: 0.89245 |  0:03:41s
epoch 68 | loss: 0.75941 | val_0_rmse: 0.88806 | val_1_rmse: 0.90028 |  0:03:44s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.88621
Best weights from best epoch are automatically used!
ended training at: 05:52:50
Feature importance:
Mean squared error is of 3197585992.3887897
Mean absolute error:42655.00427482803
MAPE:0.5937119974421599
R2 score:0.2259069590798447
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:50
epoch 0  | loss: 1.21443 | val_0_rmse: 0.96934 | val_1_rmse: 0.96755 |  0:00:03s
epoch 1  | loss: 0.85016 | val_0_rmse: 0.92514 | val_1_rmse: 0.92592 |  0:00:06s
epoch 2  | loss: 0.84371 | val_0_rmse: 0.92509 | val_1_rmse: 0.92614 |  0:00:09s
epoch 3  | loss: 0.83664 | val_0_rmse: 0.92217 | val_1_rmse: 0.92354 |  0:00:13s
epoch 4  | loss: 0.81098 | val_0_rmse: 0.93479 | val_1_rmse: 0.93844 |  0:00:16s
epoch 5  | loss: 0.78742 | val_0_rmse: 0.92678 | val_1_rmse: 0.92989 |  0:00:19s
epoch 6  | loss: 0.78474 | val_0_rmse: 0.89652 | val_1_rmse: 0.90056 |  0:00:22s
epoch 7  | loss: 0.78163 | val_0_rmse: 0.89031 | val_1_rmse: 0.89515 |  0:00:26s
epoch 8  | loss: 0.78135 | val_0_rmse: 0.89035 | val_1_rmse: 0.89486 |  0:00:29s
epoch 9  | loss: 0.78242 | val_0_rmse: 0.92485 | val_1_rmse: 0.92789 |  0:00:32s
epoch 10 | loss: 0.78193 | val_0_rmse: 0.91695 | val_1_rmse: 0.91885 |  0:00:35s
epoch 11 | loss: 0.78167 | val_0_rmse: 0.89196 | val_1_rmse: 0.89582 |  0:00:39s
epoch 12 | loss: 0.77809 | val_0_rmse: 0.98197 | val_1_rmse: 0.98126 |  0:00:42s
epoch 13 | loss: 0.77961 | val_0_rmse: 0.91309 | val_1_rmse: 0.91497 |  0:00:45s
epoch 14 | loss: 0.77681 | val_0_rmse: 0.88729 | val_1_rmse: 0.89043 |  0:00:49s
epoch 15 | loss: 0.77548 | val_0_rmse: 0.88255 | val_1_rmse: 0.88654 |  0:00:52s
epoch 16 | loss: 0.77614 | val_0_rmse: 0.91606 | val_1_rmse: 0.91765 |  0:00:55s
epoch 17 | loss: 0.77506 | val_0_rmse: 0.91815 | val_1_rmse: 0.92012 |  0:00:58s
epoch 18 | loss: 0.77651 | val_0_rmse: 0.9122  | val_1_rmse: 0.91605 |  0:01:02s
epoch 19 | loss: 0.7742  | val_0_rmse: 0.88234 | val_1_rmse: 0.88559 |  0:01:05s
epoch 20 | loss: 0.77623 | val_0_rmse: 0.87734 | val_1_rmse: 0.88229 |  0:01:08s
epoch 21 | loss: 0.77339 | val_0_rmse: 0.9199  | val_1_rmse: 0.92086 |  0:01:12s
epoch 22 | loss: 0.77035 | val_0_rmse: 1.04843 | val_1_rmse: 1.04468 |  0:01:15s
epoch 23 | loss: 0.7701  | val_0_rmse: 0.9     | val_1_rmse: 0.90462 |  0:01:18s
epoch 24 | loss: 0.76498 | val_0_rmse: 0.89934 | val_1_rmse: 0.9012  |  0:01:21s
epoch 25 | loss: 0.76527 | val_0_rmse: 0.89567 | val_1_rmse: 0.8947  |  0:01:25s
epoch 26 | loss: 0.76192 | val_0_rmse: 0.92286 | val_1_rmse: 0.92611 |  0:01:28s
epoch 27 | loss: 0.75616 | val_0_rmse: 0.87706 | val_1_rmse: 0.87696 |  0:01:31s
epoch 28 | loss: 0.75901 | val_0_rmse: 0.88496 | val_1_rmse: 0.88679 |  0:01:34s
epoch 29 | loss: 0.76061 | val_0_rmse: 0.88698 | val_1_rmse: 0.89207 |  0:01:38s
epoch 30 | loss: 0.75656 | val_0_rmse: 0.92025 | val_1_rmse: 0.92407 |  0:01:41s
epoch 31 | loss: 0.75673 | val_0_rmse: 0.90734 | val_1_rmse: 0.91226 |  0:01:44s
epoch 32 | loss: 0.75913 | val_0_rmse: 0.92058 | val_1_rmse: 0.92404 |  0:01:47s
epoch 33 | loss: 0.75912 | val_0_rmse: 1.03099 | val_1_rmse: 1.02969 |  0:01:50s
epoch 34 | loss: 0.75756 | val_0_rmse: 0.90185 | val_1_rmse: 0.90261 |  0:01:54s
epoch 35 | loss: 0.75677 | val_0_rmse: 0.96939 | val_1_rmse: 0.96962 |  0:01:57s
epoch 36 | loss: 0.75576 | val_0_rmse: 0.93936 | val_1_rmse: 0.94417 |  0:02:00s
epoch 37 | loss: 0.75883 | val_0_rmse: 0.94551 | val_1_rmse: 0.94735 |  0:02:04s
epoch 38 | loss: 0.75724 | val_0_rmse: 0.90124 | val_1_rmse: 0.90483 |  0:02:07s
epoch 39 | loss: 0.75437 | val_0_rmse: 0.9034  | val_1_rmse: 0.90941 |  0:02:10s
epoch 40 | loss: 0.75645 | val_0_rmse: 0.93731 | val_1_rmse: 0.94418 |  0:02:13s
epoch 41 | loss: 0.75975 | val_0_rmse: 0.90244 | val_1_rmse: 0.90748 |  0:02:17s
epoch 42 | loss: 0.75917 | val_0_rmse: 0.93977 | val_1_rmse: 0.94295 |  0:02:20s
epoch 43 | loss: 0.774   | val_0_rmse: 0.93572 | val_1_rmse: 0.93632 |  0:02:23s
epoch 44 | loss: 0.80042 | val_0_rmse: 0.9209  | val_1_rmse: 0.92208 |  0:02:26s
epoch 45 | loss: 0.77059 | val_0_rmse: 0.91152 | val_1_rmse: 0.91518 |  0:02:29s
epoch 46 | loss: 0.76494 | val_0_rmse: 0.92724 | val_1_rmse: 0.92795 |  0:02:33s
epoch 47 | loss: 0.75897 | val_0_rmse: 0.9031  | val_1_rmse: 0.90673 |  0:02:36s
epoch 48 | loss: 0.75776 | val_0_rmse: 0.90551 | val_1_rmse: 0.90519 |  0:02:39s
epoch 49 | loss: 0.75813 | val_0_rmse: 0.92104 | val_1_rmse: 0.92502 |  0:02:42s
epoch 50 | loss: 0.75431 | val_0_rmse: 0.99023 | val_1_rmse: 0.98954 |  0:02:45s
epoch 51 | loss: 0.75469 | val_0_rmse: 1.01486 | val_1_rmse: 1.01307 |  0:02:49s
epoch 52 | loss: 0.75248 | val_0_rmse: 0.88094 | val_1_rmse: 0.88393 |  0:02:52s
epoch 53 | loss: 0.75411 | val_0_rmse: 0.88427 | val_1_rmse: 0.88795 |  0:02:55s
epoch 54 | loss: 0.75322 | val_0_rmse: 0.87369 | val_1_rmse: 0.87723 |  0:02:58s
epoch 55 | loss: 0.75337 | val_0_rmse: 0.95224 | val_1_rmse: 0.95442 |  0:03:02s
epoch 56 | loss: 0.75199 | val_0_rmse: 0.90871 | val_1_rmse: 0.91177 |  0:03:05s
epoch 57 | loss: 0.75092 | val_0_rmse: 0.92248 | val_1_rmse: 0.92622 |  0:03:08s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.87696
Best weights from best epoch are automatically used!
ended training at: 05:56:00
Feature importance:
Mean squared error is of 3244980243.1289415
Mean absolute error:43005.600545322566
MAPE:0.5988881747921876
R2 score:0.2178114840442964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:56:00
epoch 0  | loss: 1.09036 | val_0_rmse: 0.96573 | val_1_rmse: 0.9527  |  0:00:03s
epoch 1  | loss: 0.83233 | val_0_rmse: 0.9669  | val_1_rmse: 0.95262 |  0:00:06s
epoch 2  | loss: 0.81294 | val_0_rmse: 0.95919 | val_1_rmse: 0.9447  |  0:00:09s
epoch 3  | loss: 0.80272 | val_0_rmse: 1.05561 | val_1_rmse: 1.03976 |  0:00:13s
epoch 4  | loss: 0.79405 | val_0_rmse: 1.02592 | val_1_rmse: 1.01045 |  0:00:16s
epoch 5  | loss: 0.79341 | val_0_rmse: 1.01524 | val_1_rmse: 0.99952 |  0:00:19s
epoch 6  | loss: 0.79288 | val_0_rmse: 1.05639 | val_1_rmse: 1.0405  |  0:00:22s
epoch 7  | loss: 0.79175 | val_0_rmse: 1.09367 | val_1_rmse: 1.07808 |  0:00:25s
epoch 8  | loss: 0.79283 | val_0_rmse: 1.01382 | val_1_rmse: 0.9988  |  0:00:29s
epoch 9  | loss: 0.78672 | val_0_rmse: 1.04577 | val_1_rmse: 1.02879 |  0:00:32s
epoch 10 | loss: 0.78586 | val_0_rmse: 0.90279 | val_1_rmse: 0.8904  |  0:00:35s
epoch 11 | loss: 0.81672 | val_0_rmse: 0.93688 | val_1_rmse: 0.93061 |  0:00:38s
epoch 12 | loss: 0.79573 | val_0_rmse: 0.90433 | val_1_rmse: 0.8936  |  0:00:42s
epoch 13 | loss: 0.78829 | val_0_rmse: 0.92195 | val_1_rmse: 0.90289 |  0:00:45s
epoch 14 | loss: 0.78486 | val_0_rmse: 0.92945 | val_1_rmse: 0.90949 |  0:00:48s
epoch 15 | loss: 0.78235 | val_0_rmse: 0.91998 | val_1_rmse: 0.90974 |  0:00:51s
epoch 16 | loss: 0.77755 | val_0_rmse: 0.91297 | val_1_rmse: 0.90182 |  0:00:55s
epoch 17 | loss: 0.77543 | val_0_rmse: 0.92452 | val_1_rmse: 0.91258 |  0:00:58s
epoch 18 | loss: 0.78496 | val_0_rmse: 0.8975  | val_1_rmse: 0.88444 |  0:01:01s
epoch 19 | loss: 0.77653 | val_0_rmse: 0.93943 | val_1_rmse: 0.91934 |  0:01:04s
epoch 20 | loss: 0.77042 | val_0_rmse: 0.93773 | val_1_rmse: 0.9255  |  0:01:08s
epoch 21 | loss: 0.77134 | val_0_rmse: 0.91592 | val_1_rmse: 0.90191 |  0:01:11s
epoch 22 | loss: 0.77609 | val_0_rmse: 0.89084 | val_1_rmse: 0.88129 |  0:01:14s
epoch 23 | loss: 0.79473 | val_0_rmse: 0.95358 | val_1_rmse: 0.9483  |  0:01:17s
epoch 24 | loss: 0.80022 | val_0_rmse: 1.05185 | val_1_rmse: 1.0367  |  0:01:21s
epoch 25 | loss: 0.78868 | val_0_rmse: 0.93027 | val_1_rmse: 0.91809 |  0:01:24s
epoch 26 | loss: 0.78676 | val_0_rmse: 0.91525 | val_1_rmse: 0.90282 |  0:01:27s
epoch 27 | loss: 0.78599 | val_0_rmse: 0.95942 | val_1_rmse: 0.94261 |  0:01:31s
epoch 28 | loss: 0.78079 | val_0_rmse: 0.88914 | val_1_rmse: 0.87844 |  0:01:34s
epoch 29 | loss: 0.77946 | val_0_rmse: 0.94665 | val_1_rmse: 0.93304 |  0:01:37s
epoch 30 | loss: 0.77878 | val_0_rmse: 0.89193 | val_1_rmse: 0.87767 |  0:01:40s
epoch 31 | loss: 0.7823  | val_0_rmse: 0.90397 | val_1_rmse: 0.89449 |  0:01:43s
epoch 32 | loss: 0.77948 | val_0_rmse: 0.88044 | val_1_rmse: 0.87227 |  0:01:47s
epoch 33 | loss: 0.77616 | val_0_rmse: 0.87813 | val_1_rmse: 0.86721 |  0:01:50s
epoch 34 | loss: 0.77546 | val_0_rmse: 0.93009 | val_1_rmse: 0.92053 |  0:01:53s
epoch 35 | loss: 0.776   | val_0_rmse: 0.88871 | val_1_rmse: 0.87281 |  0:01:56s
epoch 36 | loss: 0.77916 | val_0_rmse: 0.88389 | val_1_rmse: 0.86843 |  0:02:00s
epoch 37 | loss: 0.78837 | val_0_rmse: 0.87836 | val_1_rmse: 0.86552 |  0:02:03s
epoch 38 | loss: 0.77368 | val_0_rmse: 0.9484  | val_1_rmse: 1.19468 |  0:02:06s
epoch 39 | loss: 0.77778 | val_0_rmse: 0.90537 | val_1_rmse: 1.30148 |  0:02:09s
epoch 40 | loss: 0.77279 | val_0_rmse: 0.90623 | val_1_rmse: 1.33863 |  0:02:12s
epoch 41 | loss: 0.77492 | val_0_rmse: 0.87599 | val_1_rmse: 1.18078 |  0:02:16s
epoch 42 | loss: 0.77808 | val_0_rmse: 0.90929 | val_1_rmse: 0.89945 |  0:02:19s
epoch 43 | loss: 0.7757  | val_0_rmse: 0.89302 | val_1_rmse: 0.88218 |  0:02:22s
epoch 44 | loss: 0.77845 | val_0_rmse: 0.91533 | val_1_rmse: 1.40678 |  0:02:25s
epoch 45 | loss: 0.77252 | val_0_rmse: 0.92314 | val_1_rmse: 1.54742 |  0:02:29s
epoch 46 | loss: 0.7741  | val_0_rmse: 0.87577 | val_1_rmse: 0.86311 |  0:02:32s
epoch 47 | loss: 0.77422 | val_0_rmse: 0.93036 | val_1_rmse: 0.91812 |  0:02:35s
epoch 48 | loss: 0.77368 | val_0_rmse: 0.96992 | val_1_rmse: 0.94689 |  0:02:38s
epoch 49 | loss: 0.7718  | val_0_rmse: 0.9252  | val_1_rmse: 0.91666 |  0:02:41s
epoch 50 | loss: 0.77494 | val_0_rmse: 0.9204  | val_1_rmse: 0.907   |  0:02:44s
epoch 51 | loss: 0.77513 | val_0_rmse: 0.95763 | val_1_rmse: 0.94204 |  0:02:48s
epoch 52 | loss: 0.7747  | val_0_rmse: 0.89397 | val_1_rmse: 0.88015 |  0:02:51s
epoch 53 | loss: 0.7766  | val_0_rmse: 0.88417 | val_1_rmse: 0.87368 |  0:02:54s
epoch 54 | loss: 0.77666 | val_0_rmse: 0.87892 | val_1_rmse: 0.8643  |  0:02:57s
epoch 55 | loss: 0.77498 | val_0_rmse: 0.87772 | val_1_rmse: 0.86523 |  0:03:01s
epoch 56 | loss: 0.77848 | val_0_rmse: 0.90592 | val_1_rmse: 0.89508 |  0:03:04s
epoch 57 | loss: 0.77216 | val_0_rmse: 0.8866  | val_1_rmse: 0.87466 |  0:03:07s
epoch 58 | loss: 0.77677 | val_0_rmse: 0.90131 | val_1_rmse: 0.88803 |  0:03:10s
epoch 59 | loss: 0.77759 | val_0_rmse: 0.90402 | val_1_rmse: 0.89077 |  0:03:13s
epoch 60 | loss: 0.77648 | val_0_rmse: 0.94126 | val_1_rmse: 0.92498 |  0:03:17s
epoch 61 | loss: 0.77311 | val_0_rmse: 0.98372 | val_1_rmse: 0.96004 |  0:03:20s
epoch 62 | loss: 0.77648 | val_0_rmse: 0.87439 | val_1_rmse: 0.86005 |  0:03:23s
epoch 63 | loss: 0.77565 | val_0_rmse: 0.92695 | val_1_rmse: 0.91464 |  0:03:26s
epoch 64 | loss: 0.7788  | val_0_rmse: 0.95218 | val_1_rmse: 0.93022 |  0:03:29s
epoch 65 | loss: 0.77083 | val_0_rmse: 0.91441 | val_1_rmse: 0.90229 |  0:03:33s
epoch 66 | loss: 0.77343 | val_0_rmse: 0.9123  | val_1_rmse: 0.89768 |  0:03:36s
epoch 67 | loss: 0.77613 | val_0_rmse: 0.91407 | val_1_rmse: 0.90396 |  0:03:39s
epoch 68 | loss: 0.77316 | val_0_rmse: 0.94286 | val_1_rmse: 0.92252 |  0:03:42s
epoch 69 | loss: 0.77312 | val_0_rmse: 0.93356 | val_1_rmse: 0.92304 |  0:03:45s
epoch 70 | loss: 0.77284 | val_0_rmse: 0.93595 | val_1_rmse: 0.92217 |  0:03:49s
epoch 71 | loss: 0.77444 | val_0_rmse: 0.92946 | val_1_rmse: 0.9189  |  0:03:52s
epoch 72 | loss: 0.77211 | val_0_rmse: 0.8881  | val_1_rmse: 0.87682 |  0:03:55s
epoch 73 | loss: 0.77527 | val_0_rmse: 0.87521 | val_1_rmse: 0.86064 |  0:03:58s
epoch 74 | loss: 0.77415 | val_0_rmse: 0.90061 | val_1_rmse: 0.88988 |  0:04:02s
epoch 75 | loss: 0.77051 | val_0_rmse: 0.87666 | val_1_rmse: 0.86405 |  0:04:05s
epoch 76 | loss: 0.76984 | val_0_rmse: 0.87388 | val_1_rmse: 0.85861 |  0:04:08s
epoch 77 | loss: 0.77617 | val_0_rmse: 0.88345 | val_1_rmse: 0.86824 |  0:04:11s
epoch 78 | loss: 0.77432 | val_0_rmse: 0.89189 | val_1_rmse: 0.87866 |  0:04:14s
epoch 79 | loss: 0.7705  | val_0_rmse: 0.88567 | val_1_rmse: 0.87086 |  0:04:18s
epoch 80 | loss: 0.76973 | val_0_rmse: 0.91299 | val_1_rmse: 0.90026 |  0:04:21s
epoch 81 | loss: 0.77669 | val_0_rmse: 0.92603 | val_1_rmse: 0.91496 |  0:04:24s
epoch 82 | loss: 0.7765  | val_0_rmse: 1.04454 | val_1_rmse: 1.02126 |  0:04:27s
epoch 83 | loss: 0.77493 | val_0_rmse: 0.90056 | val_1_rmse: 0.88358 |  0:04:31s
epoch 84 | loss: 0.77085 | val_0_rmse: 0.98042 | val_1_rmse: 0.95741 |  0:04:34s
epoch 85 | loss: 0.77205 | val_0_rmse: 0.87989 | val_1_rmse: 0.86855 |  0:04:37s
epoch 86 | loss: 0.77531 | val_0_rmse: 0.89286 | val_1_rmse: 0.8795  |  0:04:40s
epoch 87 | loss: 0.77045 | val_0_rmse: 0.91455 | val_1_rmse: 0.90294 |  0:04:43s
epoch 88 | loss: 0.77184 | val_0_rmse: 0.93223 | val_1_rmse: 0.9201  |  0:04:47s
epoch 89 | loss: 0.77452 | val_0_rmse: 0.87724 | val_1_rmse: 0.86269 |  0:04:50s
epoch 90 | loss: 0.77717 | val_0_rmse: 0.91041 | val_1_rmse: 1.21618 |  0:04:53s
epoch 91 | loss: 0.77529 | val_0_rmse: 0.87302 | val_1_rmse: 0.85752 |  0:04:56s
epoch 92 | loss: 0.77344 | val_0_rmse: 0.95904 | val_1_rmse: 0.93961 |  0:04:59s
epoch 93 | loss: 0.77272 | val_0_rmse: 0.94789 | val_1_rmse: 0.93316 |  0:05:03s
epoch 94 | loss: 0.77286 | val_0_rmse: 0.93363 | val_1_rmse: 0.9207  |  0:05:06s
epoch 95 | loss: 0.77284 | val_0_rmse: 0.92858 | val_1_rmse: 0.91911 |  0:05:09s
epoch 96 | loss: 0.7706  | val_0_rmse: 0.9292  | val_1_rmse: 1.08252 |  0:05:12s
epoch 97 | loss: 0.76857 | val_0_rmse: 0.87856 | val_1_rmse: 0.86073 |  0:05:16s
epoch 98 | loss: 0.76473 | val_0_rmse: 0.90411 | val_1_rmse: 0.89402 |  0:05:19s
epoch 99 | loss: 0.76967 | val_0_rmse: 1.05578 | val_1_rmse: 1.02963 |  0:05:22s
epoch 100| loss: 0.76759 | val_0_rmse: 0.91683 | val_1_rmse: 0.89985 |  0:05:25s
epoch 101| loss: 0.76165 | val_0_rmse: 0.93365 | val_1_rmse: 0.91498 |  0:05:29s
epoch 102| loss: 0.76165 | val_0_rmse: 0.89723 | val_1_rmse: 0.88788 |  0:05:32s
epoch 103| loss: 0.75764 | val_0_rmse: 0.87918 | val_1_rmse: 0.86093 |  0:05:35s
epoch 104| loss: 0.75904 | val_0_rmse: 0.89433 | val_1_rmse: 0.8825  |  0:05:38s
epoch 105| loss: 0.76028 | val_0_rmse: 0.95313 | val_1_rmse: 0.94107 |  0:05:41s
epoch 106| loss: 0.75915 | val_0_rmse: 0.88627 | val_1_rmse: 0.86456 |  0:05:45s
epoch 107| loss: 0.7583  | val_0_rmse: 0.87625 | val_1_rmse: 0.86641 |  0:05:48s
epoch 108| loss: 0.75703 | val_0_rmse: 0.91655 | val_1_rmse: 0.90017 |  0:05:51s
epoch 109| loss: 0.76229 | val_0_rmse: 1.22095 | val_1_rmse: 1.20187 |  0:05:54s
epoch 110| loss: 0.76438 | val_0_rmse: 0.90004 | val_1_rmse: 0.88533 |  0:05:58s
epoch 111| loss: 0.75498 | val_0_rmse: 0.91096 | val_1_rmse: 0.89605 |  0:06:01s
epoch 112| loss: 0.76144 | val_0_rmse: 0.8918  | val_1_rmse: 0.86883 |  0:06:04s
epoch 113| loss: 0.76421 | val_0_rmse: 1.00512 | val_1_rmse: 0.98247 |  0:06:07s
epoch 114| loss: 0.77979 | val_0_rmse: 0.94875 | val_1_rmse: 1.17401 |  0:06:11s
epoch 115| loss: 0.76915 | val_0_rmse: 0.92475 | val_1_rmse: 1.25458 |  0:06:14s
epoch 116| loss: 0.76648 | val_0_rmse: 0.94125 | val_1_rmse: 1.082   |  0:06:17s
epoch 117| loss: 0.76636 | val_0_rmse: 0.88171 | val_1_rmse: 0.97129 |  0:06:20s
epoch 118| loss: 0.76613 | val_0_rmse: 0.95034 | val_1_rmse: 1.04226 |  0:06:23s
epoch 119| loss: 0.76887 | val_0_rmse: 0.88481 | val_1_rmse: 0.96702 |  0:06:27s
epoch 120| loss: 0.75818 | val_0_rmse: 0.95235 | val_1_rmse: 1.01999 |  0:06:30s
epoch 121| loss: 0.75821 | val_0_rmse: 0.93408 | val_1_rmse: 0.9507  |  0:06:33s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.85752
Best weights from best epoch are automatically used!
ended training at: 06:02:35
Feature importance:
Mean squared error is of 3030452755.1782026
Mean absolute error:42155.424337330965
MAPE:0.6163291149932998
R2 score:0.24679553668182697
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:02:35
epoch 0  | loss: 1.14931 | val_0_rmse: 0.98694 | val_1_rmse: 0.99113 |  0:00:03s
epoch 1  | loss: 0.85568 | val_0_rmse: 0.93931 | val_1_rmse: 0.94303 |  0:00:06s
epoch 2  | loss: 0.84822 | val_0_rmse: 0.93758 | val_1_rmse: 0.94091 |  0:00:09s
epoch 3  | loss: 0.84638 | val_0_rmse: 0.92662 | val_1_rmse: 0.9298  |  0:00:12s
epoch 4  | loss: 0.84886 | val_0_rmse: 0.92781 | val_1_rmse: 0.93138 |  0:00:16s
epoch 5  | loss: 0.84671 | val_0_rmse: 0.92617 | val_1_rmse: 0.9307  |  0:00:19s
epoch 6  | loss: 0.84655 | val_0_rmse: 0.92464 | val_1_rmse: 0.92839 |  0:00:22s
epoch 7  | loss: 0.84489 | val_0_rmse: 0.92092 | val_1_rmse: 0.92522 |  0:00:25s
epoch 8  | loss: 0.84446 | val_0_rmse: 0.92293 | val_1_rmse: 0.92636 |  0:00:29s
epoch 9  | loss: 0.84518 | val_0_rmse: 0.92293 | val_1_rmse: 0.92742 |  0:00:32s
epoch 10 | loss: 0.84484 | val_0_rmse: 0.91933 | val_1_rmse: 0.92289 |  0:00:35s
epoch 11 | loss: 0.84483 | val_0_rmse: 0.91859 | val_1_rmse: 0.92243 |  0:00:38s
epoch 12 | loss: 0.84456 | val_0_rmse: 0.91915 | val_1_rmse: 0.92306 |  0:00:42s
epoch 13 | loss: 0.84613 | val_0_rmse: 0.92014 | val_1_rmse: 0.92406 |  0:00:45s
epoch 14 | loss: 0.84877 | val_0_rmse: 0.92238 | val_1_rmse: 0.92567 |  0:00:48s
epoch 15 | loss: 0.85056 | val_0_rmse: 0.9206  | val_1_rmse: 0.92488 |  0:00:51s
epoch 16 | loss: 0.84717 | val_0_rmse: 0.94697 | val_1_rmse: 0.95243 |  0:00:55s
epoch 17 | loss: 0.8177  | val_0_rmse: 0.91067 | val_1_rmse: 0.91423 |  0:00:58s
epoch 18 | loss: 0.79677 | val_0_rmse: 0.89445 | val_1_rmse: 0.89663 |  0:01:01s
epoch 19 | loss: 0.81205 | val_0_rmse: 0.99015 | val_1_rmse: 0.99398 |  0:01:04s
epoch 20 | loss: 0.82274 | val_0_rmse: 0.91265 | val_1_rmse: 0.91577 |  0:01:08s
epoch 21 | loss: 0.80397 | val_0_rmse: 0.94434 | val_1_rmse: 0.94956 |  0:01:11s
epoch 22 | loss: 0.79718 | val_0_rmse: 0.9046  | val_1_rmse: 0.90712 |  0:01:14s
epoch 23 | loss: 0.79841 | val_0_rmse: 0.8944  | val_1_rmse: 0.89586 |  0:01:17s
epoch 24 | loss: 0.79764 | val_0_rmse: 1.0404  | val_1_rmse: 1.04526 |  0:01:21s
epoch 25 | loss: 0.79556 | val_0_rmse: 0.91082 | val_1_rmse: 0.91471 |  0:01:24s
epoch 26 | loss: 0.78953 | val_0_rmse: 0.88782 | val_1_rmse: 0.88926 |  0:01:27s
epoch 27 | loss: 0.78739 | val_0_rmse: 0.89863 | val_1_rmse: 0.90156 |  0:01:30s
epoch 28 | loss: 0.78904 | val_0_rmse: 0.90381 | val_1_rmse: 0.90622 |  0:01:34s
epoch 29 | loss: 0.7929  | val_0_rmse: 1.1024  | val_1_rmse: 1.10796 |  0:01:37s
epoch 30 | loss: 0.78913 | val_0_rmse: 0.92935 | val_1_rmse: 0.92933 |  0:01:40s
epoch 31 | loss: 0.78765 | val_0_rmse: 0.99072 | val_1_rmse: 1.00101 |  0:01:43s
epoch 32 | loss: 0.786   | val_0_rmse: 1.0266  | val_1_rmse: 1.02601 |  0:01:46s
epoch 33 | loss: 0.78854 | val_0_rmse: 0.94728 | val_1_rmse: 0.94612 |  0:01:50s
epoch 34 | loss: 0.7837  | val_0_rmse: 0.91726 | val_1_rmse: 0.92189 |  0:01:53s
epoch 35 | loss: 0.78755 | val_0_rmse: 1.05095 | val_1_rmse: 1.05814 |  0:01:56s
epoch 36 | loss: 0.78546 | val_0_rmse: 0.97397 | val_1_rmse: 0.97355 |  0:01:59s
epoch 37 | loss: 0.78873 | val_0_rmse: 1.10707 | val_1_rmse: 1.11321 |  0:02:03s
epoch 38 | loss: 0.78359 | val_0_rmse: 0.88505 | val_1_rmse: 0.88765 |  0:02:06s
epoch 39 | loss: 0.78499 | val_0_rmse: 0.92539 | val_1_rmse: 0.92711 |  0:02:09s
epoch 40 | loss: 0.78605 | val_0_rmse: 0.90901 | val_1_rmse: 0.91293 |  0:02:12s
epoch 41 | loss: 0.78648 | val_0_rmse: 0.88369 | val_1_rmse: 0.8862  |  0:02:16s
epoch 42 | loss: 0.78582 | val_0_rmse: 0.90625 | val_1_rmse: 0.90966 |  0:02:19s
epoch 43 | loss: 0.78111 | val_0_rmse: 0.91662 | val_1_rmse: 0.9207  |  0:02:22s
epoch 44 | loss: 0.7904  | val_0_rmse: 0.89726 | val_1_rmse: 0.90108 |  0:02:25s
epoch 45 | loss: 0.78331 | val_0_rmse: 0.97032 | val_1_rmse: 0.97732 |  0:02:29s
epoch 46 | loss: 0.7808  | val_0_rmse: 0.91422 | val_1_rmse: 0.91772 |  0:02:32s
epoch 47 | loss: 0.78221 | val_0_rmse: 0.8937  | val_1_rmse: 0.89594 |  0:02:35s
epoch 48 | loss: 0.77995 | val_0_rmse: 0.88384 | val_1_rmse: 0.88498 |  0:02:38s
epoch 49 | loss: 0.77986 | val_0_rmse: 0.90974 | val_1_rmse: 0.91363 |  0:02:41s
epoch 50 | loss: 0.78063 | val_0_rmse: 0.89084 | val_1_rmse: 0.89238 |  0:02:45s
epoch 51 | loss: 0.78144 | val_0_rmse: 0.92201 | val_1_rmse: 0.92278 |  0:02:48s
epoch 52 | loss: 0.78424 | val_0_rmse: 1.02426 | val_1_rmse: 1.03335 |  0:02:51s
epoch 53 | loss: 0.78537 | val_0_rmse: 1.05536 | val_1_rmse: 1.06288 |  0:02:54s
epoch 54 | loss: 0.78478 | val_0_rmse: 1.11863 | val_1_rmse: 1.12635 |  0:02:57s
epoch 55 | loss: 0.78512 | val_0_rmse: 0.91374 | val_1_rmse: 0.91487 |  0:03:01s
epoch 56 | loss: 0.78404 | val_0_rmse: 0.88144 | val_1_rmse: 0.88125 |  0:03:04s
epoch 57 | loss: 0.78114 | val_0_rmse: 0.90721 | val_1_rmse: 0.90939 |  0:03:07s
epoch 58 | loss: 0.77903 | val_0_rmse: 0.92017 | val_1_rmse: 0.92058 |  0:03:10s
epoch 59 | loss: 0.77681 | val_0_rmse: 0.88821 | val_1_rmse: 0.89029 |  0:03:14s
epoch 60 | loss: 0.7807  | val_0_rmse: 0.95244 | val_1_rmse: 0.95325 |  0:03:17s
epoch 61 | loss: 0.78409 | val_0_rmse: 0.92028 | val_1_rmse: 0.92482 |  0:03:20s
epoch 62 | loss: 0.77921 | val_0_rmse: 0.99891 | val_1_rmse: 1.00663 |  0:03:23s
epoch 63 | loss: 0.77924 | val_0_rmse: 0.88555 | val_1_rmse: 0.88819 |  0:03:27s
epoch 64 | loss: 0.77516 | val_0_rmse: 0.92652 | val_1_rmse: 0.92751 |  0:03:30s
epoch 65 | loss: 0.77771 | val_0_rmse: 1.13737 | val_1_rmse: 1.28712 |  0:03:33s
epoch 66 | loss: 0.77965 | val_0_rmse: 0.92453 | val_1_rmse: 0.93763 |  0:03:36s
epoch 67 | loss: 0.77867 | val_0_rmse: 0.98738 | val_1_rmse: 0.98609 |  0:03:39s
epoch 68 | loss: 0.77143 | val_0_rmse: 0.91582 | val_1_rmse: 0.91723 |  0:03:43s
epoch 69 | loss: 0.76811 | val_0_rmse: 0.95361 | val_1_rmse: 0.90613 |  0:03:46s
epoch 70 | loss: 0.76865 | val_0_rmse: 0.91396 | val_1_rmse: 0.91566 |  0:03:49s
epoch 71 | loss: 0.77138 | val_0_rmse: 1.09829 | val_1_rmse: 1.10471 |  0:03:52s
epoch 72 | loss: 0.77229 | val_0_rmse: 1.1404  | val_1_rmse: 1.14876 |  0:03:55s
epoch 73 | loss: 0.77004 | val_0_rmse: 0.9707  | val_1_rmse: 0.97628 |  0:03:59s
epoch 74 | loss: 0.78126 | val_0_rmse: 0.91226 | val_1_rmse: 0.91388 |  0:04:02s
epoch 75 | loss: 0.77348 | val_0_rmse: 0.90656 | val_1_rmse: 0.90852 |  0:04:05s
epoch 76 | loss: 0.76928 | val_0_rmse: 0.93573 | val_1_rmse: 0.9379  |  0:04:08s
epoch 77 | loss: 0.76625 | val_0_rmse: 0.89288 | val_1_rmse: 0.89398 |  0:04:12s
epoch 78 | loss: 0.76552 | val_0_rmse: 0.90314 | val_1_rmse: 0.90456 |  0:04:15s
epoch 79 | loss: 0.77012 | val_0_rmse: 1.1297  | val_1_rmse: 1.13523 |  0:04:18s
epoch 80 | loss: 0.76695 | val_0_rmse: 0.88511 | val_1_rmse: 0.88471 |  0:04:21s
epoch 81 | loss: 0.76329 | val_0_rmse: 0.922   | val_1_rmse: 0.92594 |  0:04:24s
epoch 82 | loss: 0.7689  | val_0_rmse: 0.9141  | val_1_rmse: 0.91782 |  0:04:28s
epoch 83 | loss: 0.76677 | val_0_rmse: 0.98246 | val_1_rmse: 0.98067 |  0:04:31s
epoch 84 | loss: 0.76774 | val_0_rmse: 0.91847 | val_1_rmse: 0.91903 |  0:04:34s
epoch 85 | loss: 0.7679  | val_0_rmse: 0.95301 | val_1_rmse: 0.93068 |  0:04:37s
epoch 86 | loss: 0.76417 | val_0_rmse: 0.92883 | val_1_rmse: 0.91139 |  0:04:40s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.88125
Best weights from best epoch are automatically used!
ended training at: 06:07:17
Feature importance:
Mean squared error is of 3076418187.353247
Mean absolute error:42278.64653570717
MAPE:0.6117452678285862
R2 score:0.21908688917321795
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:07:17
epoch 0  | loss: 1.06233 | val_0_rmse: 0.95979 | val_1_rmse: 0.95389 |  0:00:03s
epoch 1  | loss: 0.85628 | val_0_rmse: 0.92796 | val_1_rmse: 0.91783 |  0:00:06s
epoch 2  | loss: 0.81209 | val_0_rmse: 0.90875 | val_1_rmse: 0.89963 |  0:00:09s
epoch 3  | loss: 0.79913 | val_0_rmse: 0.98046 | val_1_rmse: 0.97063 |  0:00:12s
epoch 4  | loss: 0.79244 | val_0_rmse: 0.89905 | val_1_rmse: 0.89082 |  0:00:16s
epoch 5  | loss: 0.79395 | val_0_rmse: 0.89722 | val_1_rmse: 0.88967 |  0:00:19s
epoch 6  | loss: 0.79269 | val_0_rmse: 0.90587 | val_1_rmse: 0.89792 |  0:00:22s
epoch 7  | loss: 0.79948 | val_0_rmse: 1.0745  | val_1_rmse: 1.07001 |  0:00:25s
epoch 8  | loss: 0.79272 | val_0_rmse: 0.92693 | val_1_rmse: 0.91966 |  0:00:29s
epoch 9  | loss: 0.78892 | val_0_rmse: 1.0659  | val_1_rmse: 1.06163 |  0:00:32s
epoch 10 | loss: 0.78809 | val_0_rmse: 0.89706 | val_1_rmse: 0.88918 |  0:00:35s
epoch 11 | loss: 0.78872 | val_0_rmse: 0.88903 | val_1_rmse: 0.88181 |  0:00:38s
epoch 12 | loss: 0.7919  | val_0_rmse: 0.89599 | val_1_rmse: 0.88912 |  0:00:41s
epoch 13 | loss: 0.789   | val_0_rmse: 0.89637 | val_1_rmse: 0.88975 |  0:00:45s
epoch 14 | loss: 0.78875 | val_0_rmse: 0.90165 | val_1_rmse: 0.8949  |  0:00:48s
epoch 15 | loss: 0.79168 | val_0_rmse: 0.89573 | val_1_rmse: 0.88732 |  0:00:51s
epoch 16 | loss: 0.79027 | val_0_rmse: 0.89033 | val_1_rmse: 0.88378 |  0:00:54s
epoch 17 | loss: 0.79464 | val_0_rmse: 0.90435 | val_1_rmse: 0.89583 |  0:00:58s
epoch 18 | loss: 0.78868 | val_0_rmse: 0.88781 | val_1_rmse: 0.88033 |  0:01:01s
epoch 19 | loss: 0.78691 | val_0_rmse: 0.90815 | val_1_rmse: 0.89944 |  0:01:04s
epoch 20 | loss: 0.78668 | val_0_rmse: 0.88854 | val_1_rmse: 0.88022 |  0:01:07s
epoch 21 | loss: 0.78693 | val_0_rmse: 0.898   | val_1_rmse: 0.88876 |  0:01:11s
epoch 22 | loss: 0.78781 | val_0_rmse: 0.90947 | val_1_rmse: 0.89994 |  0:01:14s
epoch 23 | loss: 0.78835 | val_0_rmse: 0.91127 | val_1_rmse: 0.9019  |  0:01:17s
epoch 24 | loss: 0.78897 | val_0_rmse: 0.90126 | val_1_rmse: 0.89347 |  0:01:20s
epoch 25 | loss: 0.78672 | val_0_rmse: 0.88892 | val_1_rmse: 0.88075 |  0:01:23s
epoch 26 | loss: 0.78807 | val_0_rmse: 0.88808 | val_1_rmse: 0.87996 |  0:01:27s
epoch 27 | loss: 0.78648 | val_0_rmse: 0.88681 | val_1_rmse: 0.87895 |  0:01:30s
epoch 28 | loss: 0.78717 | val_0_rmse: 0.88969 | val_1_rmse: 0.88135 |  0:01:33s
epoch 29 | loss: 0.78761 | val_0_rmse: 0.89503 | val_1_rmse: 0.88687 |  0:01:36s
epoch 30 | loss: 0.78719 | val_0_rmse: 0.90648 | val_1_rmse: 0.89509 |  0:01:40s
epoch 31 | loss: 0.79367 | val_0_rmse: 0.89866 | val_1_rmse: 0.88794 |  0:01:43s
epoch 32 | loss: 0.789   | val_0_rmse: 0.88688 | val_1_rmse: 0.87784 |  0:01:46s
epoch 33 | loss: 0.78789 | val_0_rmse: 0.88812 | val_1_rmse: 0.88142 |  0:01:49s
epoch 34 | loss: 0.78867 | val_0_rmse: 0.88851 | val_1_rmse: 0.88163 |  0:01:52s
epoch 35 | loss: 0.79037 | val_0_rmse: 0.88923 | val_1_rmse: 0.87952 |  0:01:56s
epoch 36 | loss: 0.7895  | val_0_rmse: 0.90495 | val_1_rmse: 0.89501 |  0:01:59s
epoch 37 | loss: 0.7904  | val_0_rmse: 1.02919 | val_1_rmse: 1.02118 |  0:02:02s
epoch 38 | loss: 0.79315 | val_0_rmse: 0.89183 | val_1_rmse: 0.88201 |  0:02:05s
epoch 39 | loss: 0.78929 | val_0_rmse: 0.8998  | val_1_rmse: 0.89029 |  0:02:09s
epoch 40 | loss: 0.78666 | val_0_rmse: 0.88766 | val_1_rmse: 0.87985 |  0:02:12s
epoch 41 | loss: 0.78551 | val_0_rmse: 0.88378 | val_1_rmse: 0.87548 |  0:02:15s
epoch 42 | loss: 0.78566 | val_0_rmse: 0.89292 | val_1_rmse: 0.88543 |  0:02:18s
epoch 43 | loss: 0.78875 | val_0_rmse: 0.89865 | val_1_rmse: 0.88909 |  0:02:21s
epoch 44 | loss: 0.78863 | val_0_rmse: 0.88293 | val_1_rmse: 0.87547 |  0:02:25s
epoch 45 | loss: 0.78506 | val_0_rmse: 0.96138 | val_1_rmse: 0.95247 |  0:02:28s
epoch 46 | loss: 0.78407 | val_0_rmse: 0.88906 | val_1_rmse: 0.88281 |  0:02:31s
epoch 47 | loss: 0.78303 | val_0_rmse: 0.97599 | val_1_rmse: 0.96778 |  0:02:34s
epoch 48 | loss: 0.78508 | val_0_rmse: 0.88817 | val_1_rmse: 0.87857 |  0:02:38s
epoch 49 | loss: 0.78764 | val_0_rmse: 0.89994 | val_1_rmse: 0.89158 |  0:02:41s
epoch 50 | loss: 0.78653 | val_0_rmse: 0.89666 | val_1_rmse: 0.88759 |  0:02:44s
epoch 51 | loss: 0.78675 | val_0_rmse: 0.891   | val_1_rmse: 0.88284 |  0:02:47s
epoch 52 | loss: 0.78721 | val_0_rmse: 0.8859  | val_1_rmse: 0.87905 |  0:02:51s
epoch 53 | loss: 0.78706 | val_0_rmse: 0.88576 | val_1_rmse: 0.87759 |  0:02:54s
epoch 54 | loss: 0.78658 | val_0_rmse: 0.89413 | val_1_rmse: 0.8873  |  0:02:57s
epoch 55 | loss: 0.78746 | val_0_rmse: 0.89628 | val_1_rmse: 0.88851 |  0:03:00s
epoch 56 | loss: 0.78465 | val_0_rmse: 0.88853 | val_1_rmse: 0.8818  |  0:03:04s
epoch 57 | loss: 0.78568 | val_0_rmse: 0.88915 | val_1_rmse: 0.88144 |  0:03:07s
epoch 58 | loss: 0.78312 | val_0_rmse: 0.89059 | val_1_rmse: 0.88325 |  0:03:10s
epoch 59 | loss: 0.78385 | val_0_rmse: 0.89943 | val_1_rmse: 0.89084 |  0:03:13s
epoch 60 | loss: 0.78267 | val_0_rmse: 0.89202 | val_1_rmse: 0.88519 |  0:03:17s
epoch 61 | loss: 0.78489 | val_0_rmse: 0.89134 | val_1_rmse: 0.88316 |  0:03:20s
epoch 62 | loss: 0.78341 | val_0_rmse: 0.88171 | val_1_rmse: 0.87414 |  0:03:23s
epoch 63 | loss: 0.78803 | val_0_rmse: 0.88313 | val_1_rmse: 0.87612 |  0:03:26s
epoch 64 | loss: 0.78188 | val_0_rmse: 0.90482 | val_1_rmse: 0.89606 |  0:03:30s
epoch 65 | loss: 0.78325 | val_0_rmse: 0.89041 | val_1_rmse: 0.88374 |  0:03:33s
epoch 66 | loss: 0.78245 | val_0_rmse: 0.89103 | val_1_rmse: 0.88346 |  0:03:36s
epoch 67 | loss: 0.78944 | val_0_rmse: 0.92313 | val_1_rmse: 0.91249 |  0:03:39s
epoch 68 | loss: 0.78626 | val_0_rmse: 0.90383 | val_1_rmse: 0.89552 |  0:03:43s
epoch 69 | loss: 0.78467 | val_0_rmse: 0.90401 | val_1_rmse: 0.89442 |  0:03:46s
epoch 70 | loss: 0.78373 | val_0_rmse: 0.90853 | val_1_rmse: 0.89836 |  0:03:49s
epoch 71 | loss: 0.78611 | val_0_rmse: 0.88339 | val_1_rmse: 0.87486 |  0:03:52s
epoch 72 | loss: 0.78806 | val_0_rmse: 0.88315 | val_1_rmse: 0.87482 |  0:03:55s
epoch 73 | loss: 0.78414 | val_0_rmse: 0.89105 | val_1_rmse: 0.88331 |  0:03:59s
epoch 74 | loss: 0.78655 | val_0_rmse: 0.88633 | val_1_rmse: 0.87807 |  0:04:02s
epoch 75 | loss: 0.78501 | val_0_rmse: 0.93175 | val_1_rmse: 0.9243  |  0:04:05s
epoch 76 | loss: 0.78045 | val_0_rmse: 0.8958  | val_1_rmse: 0.89    |  0:04:08s
epoch 77 | loss: 0.77844 | val_0_rmse: 0.89893 | val_1_rmse: 0.89109 |  0:04:12s
epoch 78 | loss: 0.78744 | val_0_rmse: 0.90499 | val_1_rmse: 0.90045 |  0:04:15s
epoch 79 | loss: 0.78484 | val_0_rmse: 0.89475 | val_1_rmse: 0.89063 |  0:04:18s
epoch 80 | loss: 0.78784 | val_0_rmse: 0.89824 | val_1_rmse: 0.8925  |  0:04:21s
epoch 81 | loss: 0.78462 | val_0_rmse: 0.89264 | val_1_rmse: 0.88803 |  0:04:24s
epoch 82 | loss: 0.78938 | val_0_rmse: 0.88649 | val_1_rmse: 0.88138 |  0:04:28s
epoch 83 | loss: 0.78328 | val_0_rmse: 0.88261 | val_1_rmse: 0.87526 |  0:04:31s
epoch 84 | loss: 0.78418 | val_0_rmse: 0.89528 | val_1_rmse: 0.89014 |  0:04:34s
epoch 85 | loss: 0.7865  | val_0_rmse: 0.898   | val_1_rmse: 0.89332 |  0:04:37s
epoch 86 | loss: 0.78504 | val_0_rmse: 0.89125 | val_1_rmse: 0.88545 |  0:04:40s
epoch 87 | loss: 0.78432 | val_0_rmse: 0.89287 | val_1_rmse: 0.88851 |  0:04:44s
epoch 88 | loss: 0.78368 | val_0_rmse: 0.89968 | val_1_rmse: 0.89272 |  0:04:47s
epoch 89 | loss: 0.78148 | val_0_rmse: 0.89073 | val_1_rmse: 0.88583 |  0:04:50s
epoch 90 | loss: 0.78089 | val_0_rmse: 0.87998 | val_1_rmse: 0.87326 |  0:04:53s
epoch 91 | loss: 0.78309 | val_0_rmse: 0.88264 | val_1_rmse: 0.87658 |  0:04:57s
epoch 92 | loss: 0.77991 | val_0_rmse: 0.90749 | val_1_rmse: 0.90236 |  0:05:00s
epoch 93 | loss: 0.78054 | val_0_rmse: 0.88791 | val_1_rmse: 0.88218 |  0:05:03s
epoch 94 | loss: 0.7869  | val_0_rmse: 0.89724 | val_1_rmse: 0.89117 |  0:05:06s
epoch 95 | loss: 0.78203 | val_0_rmse: 0.89388 | val_1_rmse: 0.88595 |  0:05:10s
epoch 96 | loss: 0.78108 | val_0_rmse: 0.90486 | val_1_rmse: 0.89447 |  0:05:13s
epoch 97 | loss: 0.78296 | val_0_rmse: 0.89763 | val_1_rmse: 0.88895 |  0:05:16s
epoch 98 | loss: 0.78301 | val_0_rmse: 0.95756 | val_1_rmse: 0.95153 |  0:05:19s
epoch 99 | loss: 0.78692 | val_0_rmse: 0.89401 | val_1_rmse: 0.88573 |  0:05:23s
epoch 100| loss: 0.77974 | val_0_rmse: 0.90512 | val_1_rmse: 0.89938 |  0:05:26s
epoch 101| loss: 0.77838 | val_0_rmse: 0.89618 | val_1_rmse: 0.89016 |  0:05:29s
epoch 102| loss: 0.7791  | val_0_rmse: 0.8816  | val_1_rmse: 0.87424 |  0:05:32s
epoch 103| loss: 0.78154 | val_0_rmse: 0.89887 | val_1_rmse: 0.88872 |  0:05:36s
epoch 104| loss: 0.7826  | val_0_rmse: 0.89609 | val_1_rmse: 0.8863  |  0:05:39s
epoch 105| loss: 0.78101 | val_0_rmse: 0.88199 | val_1_rmse: 0.87428 |  0:05:42s
epoch 106| loss: 0.78023 | val_0_rmse: 0.906   | val_1_rmse: 0.8971  |  0:05:45s
epoch 107| loss: 0.77982 | val_0_rmse: 0.89206 | val_1_rmse: 0.88633 |  0:05:48s
epoch 108| loss: 0.78194 | val_0_rmse: 0.88803 | val_1_rmse: 0.88281 |  0:05:52s
epoch 109| loss: 0.77976 | val_0_rmse: 0.8933  | val_1_rmse: 0.88772 |  0:05:55s
epoch 110| loss: 0.7796  | val_0_rmse: 0.89832 | val_1_rmse: 0.89169 |  0:05:58s
epoch 111| loss: 0.78069 | val_0_rmse: 0.89805 | val_1_rmse: 0.89396 |  0:06:01s
epoch 112| loss: 0.78272 | val_0_rmse: 0.89557 | val_1_rmse: 0.88846 |  0:06:04s
epoch 113| loss: 0.78141 | val_0_rmse: 0.88316 | val_1_rmse: 0.87723 |  0:06:07s
epoch 114| loss: 0.77926 | val_0_rmse: 0.90747 | val_1_rmse: 0.89977 |  0:06:11s
epoch 115| loss: 0.78977 | val_0_rmse: 0.89663 | val_1_rmse: 0.89188 |  0:06:14s
epoch 116| loss: 0.77983 | val_0_rmse: 0.89237 | val_1_rmse: 0.88746 |  0:06:17s
epoch 117| loss: 0.78316 | val_0_rmse: 0.90967 | val_1_rmse: 0.90115 |  0:06:20s
epoch 118| loss: 0.78188 | val_0_rmse: 0.88095 | val_1_rmse: 0.87339 |  0:06:24s
epoch 119| loss: 0.78061 | val_0_rmse: 0.89577 | val_1_rmse: 0.88842 |  0:06:27s
epoch 120| loss: 0.78505 | val_0_rmse: 0.89481 | val_1_rmse: 0.88666 |  0:06:30s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.87326
Best weights from best epoch are automatically used!
ended training at: 06:13:49
Feature importance:
Mean squared error is of 3067955903.288136
Mean absolute error:41982.01377561615
MAPE:0.5859350502403757
R2 score:0.22254110987742404
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:13:49
epoch 0  | loss: 1.19307 | val_0_rmse: 0.9635  | val_1_rmse: 0.95055 |  0:00:03s
epoch 1  | loss: 0.88182 | val_0_rmse: 0.94276 | val_1_rmse: 0.92674 |  0:00:06s
epoch 2  | loss: 0.86022 | val_0_rmse: 0.96008 | val_1_rmse: 0.94113 |  0:00:09s
epoch 3  | loss: 0.85663 | val_0_rmse: 0.93176 | val_1_rmse: 0.91639 |  0:00:12s
epoch 4  | loss: 0.84912 | val_0_rmse: 0.92854 | val_1_rmse: 0.91383 |  0:00:16s
epoch 5  | loss: 0.85194 | val_0_rmse: 0.92641 | val_1_rmse: 0.91142 |  0:00:19s
epoch 6  | loss: 0.8491  | val_0_rmse: 0.92378 | val_1_rmse: 0.909   |  0:00:22s
epoch 7  | loss: 0.84799 | val_0_rmse: 0.92126 | val_1_rmse: 0.90846 |  0:00:25s
epoch 8  | loss: 0.84824 | val_0_rmse: 0.92227 | val_1_rmse: 0.90814 |  0:00:29s
epoch 9  | loss: 0.84372 | val_0_rmse: 0.92545 | val_1_rmse: 0.9099  |  0:00:32s
epoch 10 | loss: 0.81575 | val_0_rmse: 0.89408 | val_1_rmse: 0.88233 |  0:00:35s
epoch 11 | loss: 0.79869 | val_0_rmse: 0.91011 | val_1_rmse: 0.89984 |  0:00:39s
epoch 12 | loss: 0.79793 | val_0_rmse: 0.91016 | val_1_rmse: 0.90099 |  0:00:42s
epoch 13 | loss: 0.79622 | val_0_rmse: 0.92887 | val_1_rmse: 0.9156  |  0:00:45s
epoch 14 | loss: 0.79131 | val_0_rmse: 0.90787 | val_1_rmse: 0.89708 |  0:00:48s
epoch 15 | loss: 0.79087 | val_0_rmse: 0.91075 | val_1_rmse: 0.90014 |  0:00:52s
epoch 16 | loss: 0.7857  | val_0_rmse: 0.924   | val_1_rmse: 0.91101 |  0:00:55s
epoch 17 | loss: 0.77891 | val_0_rmse: 0.87774 | val_1_rmse: 0.86264 |  0:00:58s
epoch 18 | loss: 0.78148 | val_0_rmse: 0.90982 | val_1_rmse: 0.90016 |  0:01:01s
epoch 19 | loss: 0.78603 | val_0_rmse: 0.91327 | val_1_rmse: 0.90105 |  0:01:05s
epoch 20 | loss: 0.78367 | val_0_rmse: 0.87853 | val_1_rmse: 0.86359 |  0:01:08s
epoch 21 | loss: 0.78256 | val_0_rmse: 0.91798 | val_1_rmse: 0.91215 |  0:01:11s
epoch 22 | loss: 0.78274 | val_0_rmse: 0.91645 | val_1_rmse: 0.90724 |  0:01:14s
epoch 23 | loss: 0.78214 | val_0_rmse: 1.01356 | val_1_rmse: 1.00218 |  0:01:17s
epoch 24 | loss: 0.78561 | val_0_rmse: 0.88548 | val_1_rmse: 0.87194 |  0:01:21s
epoch 25 | loss: 0.78212 | val_0_rmse: 1.12089 | val_1_rmse: 1.10154 |  0:01:24s
epoch 26 | loss: 0.78334 | val_0_rmse: 0.90999 | val_1_rmse: 0.8966  |  0:01:27s
epoch 27 | loss: 0.77817 | val_0_rmse: 0.88113 | val_1_rmse: 0.86619 |  0:01:30s
epoch 28 | loss: 0.78327 | val_0_rmse: 0.92256 | val_1_rmse: 0.91427 |  0:01:34s
epoch 29 | loss: 0.78453 | val_0_rmse: 0.88947 | val_1_rmse: 0.87194 |  0:01:37s
epoch 30 | loss: 0.77947 | val_0_rmse: 0.90066 | val_1_rmse: 0.89117 |  0:01:40s
epoch 31 | loss: 0.77667 | val_0_rmse: 0.95797 | val_1_rmse: 0.94372 |  0:01:43s
epoch 32 | loss: 0.78213 | val_0_rmse: 0.91687 | val_1_rmse: 0.90309 |  0:01:47s
epoch 33 | loss: 0.78341 | val_0_rmse: 0.93484 | val_1_rmse: 0.92143 |  0:01:50s
epoch 34 | loss: 0.77986 | val_0_rmse: 0.91098 | val_1_rmse: 0.90232 |  0:01:53s
epoch 35 | loss: 0.77658 | val_0_rmse: 0.94786 | val_1_rmse: 0.93375 |  0:01:56s
epoch 36 | loss: 0.77355 | val_0_rmse: 1.12546 | val_1_rmse: 1.10989 |  0:02:00s
epoch 37 | loss: 0.77732 | val_0_rmse: 0.91368 | val_1_rmse: 0.9045  |  0:02:03s
epoch 38 | loss: 0.77553 | val_0_rmse: 0.93946 | val_1_rmse: 0.92341 |  0:02:06s
epoch 39 | loss: 0.77333 | val_0_rmse: 0.90592 | val_1_rmse: 0.89818 |  0:02:09s
epoch 40 | loss: 0.77746 | val_0_rmse: 1.09406 | val_1_rmse: 1.07839 |  0:02:12s
epoch 41 | loss: 0.77514 | val_0_rmse: 0.90201 | val_1_rmse: 0.8931  |  0:02:16s
epoch 42 | loss: 0.7714  | val_0_rmse: 0.91758 | val_1_rmse: 0.8963  |  0:02:19s
epoch 43 | loss: 0.76926 | val_0_rmse: 0.88576 | val_1_rmse: 0.87047 |  0:02:22s
epoch 44 | loss: 0.77202 | val_0_rmse: 0.9121  | val_1_rmse: 0.90224 |  0:02:25s
epoch 45 | loss: 0.77087 | val_0_rmse: 0.98105 | val_1_rmse: 0.96795 |  0:02:29s
epoch 46 | loss: 0.76969 | val_0_rmse: 0.87822 | val_1_rmse: 0.8668  |  0:02:32s
epoch 47 | loss: 0.76994 | val_0_rmse: 0.91458 | val_1_rmse: 0.90384 |  0:02:35s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.86264
Best weights from best epoch are automatically used!
ended training at: 06:16:26
Feature importance:
Mean squared error is of 3030033155.5278873
Mean absolute error:42034.852338726436
MAPE:0.6009003127443407
R2 score:0.24512641812224545
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:16:26
epoch 0  | loss: 1.11926 | val_0_rmse: 0.96512 | val_1_rmse: 0.95237 |  0:00:03s
epoch 1  | loss: 0.85684 | val_0_rmse: 0.97493 | val_1_rmse: 0.9619  |  0:00:06s
epoch 2  | loss: 0.82168 | val_0_rmse: 0.93251 | val_1_rmse: 0.91995 |  0:00:09s
epoch 3  | loss: 0.80535 | val_0_rmse: 0.93809 | val_1_rmse: 0.92496 |  0:00:12s
epoch 4  | loss: 0.79743 | val_0_rmse: 0.91881 | val_1_rmse: 0.90699 |  0:00:16s
epoch 5  | loss: 0.78796 | val_0_rmse: 0.92012 | val_1_rmse: 0.91121 |  0:00:19s
epoch 6  | loss: 0.79024 | val_0_rmse: 0.89917 | val_1_rmse: 0.88935 |  0:00:22s
epoch 7  | loss: 0.79039 | val_0_rmse: 0.89669 | val_1_rmse: 0.88657 |  0:00:25s
epoch 8  | loss: 0.78743 | val_0_rmse: 0.90669 | val_1_rmse: 0.8961  |  0:00:29s
epoch 9  | loss: 0.78723 | val_0_rmse: 0.89671 | val_1_rmse: 0.88716 |  0:00:32s
epoch 10 | loss: 0.79276 | val_0_rmse: 0.90621 | val_1_rmse: 0.89069 |  0:00:35s
epoch 11 | loss: 0.78894 | val_0_rmse: 0.90074 | val_1_rmse: 0.88706 |  0:00:38s
epoch 12 | loss: 0.78856 | val_0_rmse: 1.16824 | val_1_rmse: 1.15578 |  0:00:41s
epoch 13 | loss: 0.78853 | val_0_rmse: 0.90643 | val_1_rmse: 0.89345 |  0:00:45s
epoch 14 | loss: 0.78797 | val_0_rmse: 1.16356 | val_1_rmse: 1.15086 |  0:00:48s
epoch 15 | loss: 0.78662 | val_0_rmse: 0.88847 | val_1_rmse: 0.87959 |  0:00:51s
epoch 16 | loss: 0.78961 | val_0_rmse: 0.97509 | val_1_rmse: 0.9576  |  0:00:54s
epoch 17 | loss: 0.78844 | val_0_rmse: 0.92454 | val_1_rmse: 0.90743 |  0:00:58s
epoch 18 | loss: 0.79151 | val_0_rmse: 0.95976 | val_1_rmse: 0.95516 |  0:01:01s
epoch 19 | loss: 0.79716 | val_0_rmse: 1.06591 | val_1_rmse: 1.0499  |  0:01:04s
epoch 20 | loss: 0.78792 | val_0_rmse: 1.01825 | val_1_rmse: 1.00729 |  0:01:07s
epoch 21 | loss: 0.78879 | val_0_rmse: 0.91557 | val_1_rmse: 0.90426 |  0:01:11s
epoch 22 | loss: 0.78945 | val_0_rmse: 0.90109 | val_1_rmse: 0.88463 |  0:01:14s
epoch 23 | loss: 0.78392 | val_0_rmse: 1.02051 | val_1_rmse: 1.00806 |  0:01:17s
epoch 24 | loss: 0.79198 | val_0_rmse: 0.89396 | val_1_rmse: 0.88325 |  0:01:21s
epoch 25 | loss: 0.78809 | val_0_rmse: 0.90268 | val_1_rmse: 0.89128 |  0:01:24s
epoch 26 | loss: 0.79192 | val_0_rmse: 1.10835 | val_1_rmse: 1.09619 |  0:01:27s
epoch 27 | loss: 0.78875 | val_0_rmse: 1.08466 | val_1_rmse: 1.0684  |  0:01:30s
epoch 28 | loss: 0.78851 | val_0_rmse: 1.22218 | val_1_rmse: 1.20469 |  0:01:34s
epoch 29 | loss: 0.78952 | val_0_rmse: 0.93615 | val_1_rmse: 0.92544 |  0:01:37s
epoch 30 | loss: 0.78524 | val_0_rmse: 0.96668 | val_1_rmse: 0.9069  |  0:01:40s
epoch 31 | loss: 0.78371 | val_0_rmse: 0.8901  | val_1_rmse: 0.8726  |  0:01:43s
epoch 32 | loss: 0.78313 | val_0_rmse: 0.92029 | val_1_rmse: 0.89827 |  0:01:47s
epoch 33 | loss: 0.78491 | val_0_rmse: 0.9025  | val_1_rmse: 0.88697 |  0:01:50s
epoch 34 | loss: 0.78367 | val_0_rmse: 0.93241 | val_1_rmse: 0.8846  |  0:01:53s
epoch 35 | loss: 0.78313 | val_0_rmse: 1.02025 | val_1_rmse: 0.87281 |  0:01:56s
epoch 36 | loss: 0.7843  | val_0_rmse: 1.1394  | val_1_rmse: 1.02153 |  0:01:59s
epoch 37 | loss: 0.78221 | val_0_rmse: 1.04608 | val_1_rmse: 0.89477 |  0:02:03s
epoch 38 | loss: 0.78307 | val_0_rmse: 1.11277 | val_1_rmse: 0.98785 |  0:02:06s
epoch 39 | loss: 0.7813  | val_0_rmse: 0.90677 | val_1_rmse: 0.88818 |  0:02:09s
epoch 40 | loss: 0.78228 | val_0_rmse: 0.88299 | val_1_rmse: 0.87112 |  0:02:12s
epoch 41 | loss: 0.78012 | val_0_rmse: 0.96575 | val_1_rmse: 0.95298 |  0:02:16s
epoch 42 | loss: 0.78521 | val_0_rmse: 0.92533 | val_1_rmse: 0.89034 |  0:02:19s
epoch 43 | loss: 0.77756 | val_0_rmse: 0.90324 | val_1_rmse: 0.88795 |  0:02:22s
epoch 44 | loss: 0.77712 | val_0_rmse: 0.90187 | val_1_rmse: 0.88099 |  0:02:25s
epoch 45 | loss: 0.77808 | val_0_rmse: 0.91302 | val_1_rmse: 0.88384 |  0:02:28s
epoch 46 | loss: 0.78908 | val_0_rmse: 1.10172 | val_1_rmse: 1.07872 |  0:02:32s
epoch 47 | loss: 0.78193 | val_0_rmse: 0.90159 | val_1_rmse: 0.8809  |  0:02:35s
epoch 48 | loss: 0.77904 | val_0_rmse: 0.90482 | val_1_rmse: 0.88737 |  0:02:38s
epoch 49 | loss: 0.78178 | val_0_rmse: 0.90279 | val_1_rmse: 0.88455 |  0:02:41s
epoch 50 | loss: 0.78461 | val_0_rmse: 0.89079 | val_1_rmse: 0.87443 |  0:02:45s
epoch 51 | loss: 0.77908 | val_0_rmse: 0.90375 | val_1_rmse: 0.89151 |  0:02:48s
epoch 52 | loss: 0.77872 | val_0_rmse: 0.89408 | val_1_rmse: 0.87628 |  0:02:51s
epoch 53 | loss: 0.77879 | val_0_rmse: 1.1279  | val_1_rmse: 1.09021 |  0:02:54s
epoch 54 | loss: 0.78969 | val_0_rmse: 1.02886 | val_1_rmse: 0.97967 |  0:02:57s
epoch 55 | loss: 0.78537 | val_0_rmse: 1.06    | val_1_rmse: 0.93006 |  0:03:01s
epoch 56 | loss: 0.78497 | val_0_rmse: 1.1891  | val_1_rmse: 1.11081 |  0:03:04s
epoch 57 | loss: 0.78335 | val_0_rmse: 1.02938 | val_1_rmse: 0.91015 |  0:03:07s
epoch 58 | loss: 0.78849 | val_0_rmse: 1.08765 | val_1_rmse: 1.06814 |  0:03:10s
epoch 59 | loss: 0.78421 | val_0_rmse: 0.92473 | val_1_rmse: 0.91096 |  0:03:14s
epoch 60 | loss: 0.78177 | val_0_rmse: 0.93804 | val_1_rmse: 0.9257  |  0:03:17s
epoch 61 | loss: 0.77916 | val_0_rmse: 0.95995 | val_1_rmse: 0.93075 |  0:03:20s
epoch 62 | loss: 0.77765 | val_0_rmse: 1.15954 | val_1_rmse: 1.13129 |  0:03:23s
epoch 63 | loss: 0.7865  | val_0_rmse: 0.95933 | val_1_rmse: 0.94187 |  0:03:26s
epoch 64 | loss: 0.77973 | val_0_rmse: 0.96458 | val_1_rmse: 0.9469  |  0:03:30s
epoch 65 | loss: 0.77275 | val_0_rmse: 0.89151 | val_1_rmse: 0.86556 |  0:03:33s
epoch 66 | loss: 0.77248 | val_0_rmse: 1.13319 | val_1_rmse: 1.09103 |  0:03:36s
epoch 67 | loss: 0.77772 | val_0_rmse: 0.98678 | val_1_rmse: 0.9199  |  0:03:39s
epoch 68 | loss: 0.77653 | val_0_rmse: 1.23835 | val_1_rmse: 1.18101 |  0:03:43s
epoch 69 | loss: 0.7701  | val_0_rmse: 1.08815 | val_1_rmse: 1.05392 |  0:03:46s
epoch 70 | loss: 0.77114 | val_0_rmse: 1.03305 | val_1_rmse: 0.91339 |  0:03:49s
epoch 71 | loss: 0.77106 | val_0_rmse: 0.93352 | val_1_rmse: 0.89799 |  0:03:52s
epoch 72 | loss: 0.77165 | val_0_rmse: 0.92607 | val_1_rmse: 0.88425 |  0:03:56s
epoch 73 | loss: 0.77048 | val_0_rmse: 1.126   | val_1_rmse: 1.08513 |  0:03:59s
epoch 74 | loss: 0.76935 | val_0_rmse: 0.93106 | val_1_rmse: 0.8905  |  0:04:02s
epoch 75 | loss: 0.76978 | val_0_rmse: 0.95218 | val_1_rmse: 0.88383 |  0:04:05s
epoch 76 | loss: 0.76652 | val_0_rmse: 1.01923 | val_1_rmse: 0.97997 |  0:04:08s
epoch 77 | loss: 0.77132 | val_0_rmse: 0.94593 | val_1_rmse: 0.9173  |  0:04:12s
epoch 78 | loss: 0.76748 | val_0_rmse: 1.07837 | val_1_rmse: 1.04613 |  0:04:15s
epoch 79 | loss: 0.76325 | val_0_rmse: 0.93315 | val_1_rmse: 0.90268 |  0:04:18s
epoch 80 | loss: 0.76654 | val_0_rmse: 0.93434 | val_1_rmse: 0.88728 |  0:04:22s
epoch 81 | loss: 0.761   | val_0_rmse: 0.92147 | val_1_rmse: 0.87267 |  0:04:25s
epoch 82 | loss: 0.76145 | val_0_rmse: 1.07746 | val_1_rmse: 1.01006 |  0:04:28s
epoch 83 | loss: 0.76616 | val_0_rmse: 0.96099 | val_1_rmse: 0.91422 |  0:04:31s
epoch 84 | loss: 0.76612 | val_0_rmse: 0.9259  | val_1_rmse: 0.88931 |  0:04:35s
epoch 85 | loss: 0.76502 | val_0_rmse: 1.16185 | val_1_rmse: 1.10254 |  0:04:38s
epoch 86 | loss: 0.76193 | val_0_rmse: 0.97225 | val_1_rmse: 0.89927 |  0:04:41s
epoch 87 | loss: 0.75779 | val_0_rmse: 0.94819 | val_1_rmse: 0.88485 |  0:04:44s
epoch 88 | loss: 0.75897 | val_0_rmse: 0.95869 | val_1_rmse: 0.91786 |  0:04:48s
epoch 89 | loss: 0.75589 | val_0_rmse: 1.01643 | val_1_rmse: 0.95926 |  0:04:51s
epoch 90 | loss: 0.76081 | val_0_rmse: 0.90278 | val_1_rmse: 0.87168 |  0:04:54s
epoch 91 | loss: 0.76103 | val_0_rmse: 1.07017 | val_1_rmse: 1.04359 |  0:04:57s
epoch 92 | loss: 0.76046 | val_0_rmse: 1.17676 | val_1_rmse: 1.13256 |  0:05:00s
epoch 93 | loss: 0.76043 | val_0_rmse: 0.97958 | val_1_rmse: 0.89589 |  0:05:04s
epoch 94 | loss: 0.76004 | val_0_rmse: 0.98849 | val_1_rmse: 0.94614 |  0:05:07s
epoch 95 | loss: 0.76389 | val_0_rmse: 0.97513 | val_1_rmse: 0.92392 |  0:05:10s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.86556
Best weights from best epoch are automatically used!
ended training at: 06:21:38
Feature importance:
Mean squared error is of 3304160464.801434
Mean absolute error:41556.31251382669
MAPE:0.5806673379654229
R2 score:0.16348970243724104
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:21:38
epoch 0  | loss: 1.25959 | val_0_rmse: 0.99846 | val_1_rmse: 0.97254 |  0:00:03s
epoch 1  | loss: 0.81863 | val_0_rmse: 0.9278  | val_1_rmse: 0.90849 |  0:00:06s
epoch 2  | loss: 0.80841 | val_0_rmse: 0.95225 | val_1_rmse: 0.93822 |  0:00:09s
epoch 3  | loss: 0.80511 | val_0_rmse: 0.98615 | val_1_rmse: 0.96029 |  0:00:12s
epoch 4  | loss: 0.80041 | val_0_rmse: 0.94761 | val_1_rmse: 0.92008 |  0:00:15s
epoch 5  | loss: 0.80021 | val_0_rmse: 0.93651 | val_1_rmse: 0.91925 |  0:00:19s
epoch 6  | loss: 0.79707 | val_0_rmse: 1.02454 | val_1_rmse: 0.99454 |  0:00:22s
epoch 7  | loss: 0.79857 | val_0_rmse: 1.00215 | val_1_rmse: 0.97253 |  0:00:25s
epoch 8  | loss: 0.80188 | val_0_rmse: 1.00708 | val_1_rmse: 0.97799 |  0:00:28s
epoch 9  | loss: 0.79357 | val_0_rmse: 1.1528  | val_1_rmse: 1.11862 |  0:00:32s
epoch 10 | loss: 0.79382 | val_0_rmse: 0.95652 | val_1_rmse: 0.92877 |  0:00:35s
epoch 11 | loss: 0.79529 | val_0_rmse: 1.01933 | val_1_rmse: 0.99007 |  0:00:38s
epoch 12 | loss: 0.79481 | val_0_rmse: 0.89895 | val_1_rmse: 0.87518 |  0:00:41s
epoch 13 | loss: 0.79197 | val_0_rmse: 0.88784 | val_1_rmse: 0.86184 |  0:00:44s
epoch 14 | loss: 0.7983  | val_0_rmse: 0.98712 | val_1_rmse: 0.97282 |  0:00:48s
epoch 15 | loss: 0.79535 | val_0_rmse: 1.18066 | val_1_rmse: 1.14591 |  0:00:51s
epoch 16 | loss: 0.79091 | val_0_rmse: 0.90818 | val_1_rmse: 0.88049 |  0:00:54s
epoch 17 | loss: 0.79218 | val_0_rmse: 1.14228 | val_1_rmse: 1.1094  |  0:00:57s
epoch 18 | loss: 0.79162 | val_0_rmse: 1.14486 | val_1_rmse: 1.11004 |  0:01:01s
epoch 19 | loss: 0.7893  | val_0_rmse: 0.9049  | val_1_rmse: 0.88182 |  0:01:04s
epoch 20 | loss: 0.79193 | val_0_rmse: 1.13385 | val_1_rmse: 1.1025  |  0:01:07s
epoch 21 | loss: 0.79158 | val_0_rmse: 0.92161 | val_1_rmse: 0.90146 |  0:01:10s
epoch 22 | loss: 0.80102 | val_0_rmse: 0.95358 | val_1_rmse: 0.937   |  0:01:13s
epoch 23 | loss: 0.79397 | val_0_rmse: 1.00668 | val_1_rmse: 0.9772  |  0:01:17s
epoch 24 | loss: 0.80396 | val_0_rmse: 1.00879 | val_1_rmse: 0.97987 |  0:01:20s
epoch 25 | loss: 0.79316 | val_0_rmse: 0.93449 | val_1_rmse: 0.91657 |  0:01:23s
epoch 26 | loss: 0.78954 | val_0_rmse: 0.89854 | val_1_rmse: 0.87071 |  0:01:26s
epoch 27 | loss: 0.79407 | val_0_rmse: 0.88679 | val_1_rmse: 0.8625  |  0:01:30s
epoch 28 | loss: 0.79198 | val_0_rmse: 0.99987 | val_1_rmse: 0.97199 |  0:01:33s
epoch 29 | loss: 0.79072 | val_0_rmse: 0.9649  | val_1_rmse: 0.9469  |  0:01:36s
epoch 30 | loss: 0.78865 | val_0_rmse: 1.08901 | val_1_rmse: 1.05644 |  0:01:39s
epoch 31 | loss: 0.7912  | val_0_rmse: 0.94398 | val_1_rmse: 0.92411 |  0:01:43s
epoch 32 | loss: 0.79123 | val_0_rmse: 1.11729 | val_1_rmse: 1.0826  |  0:01:46s
epoch 33 | loss: 0.78915 | val_0_rmse: 0.93984 | val_1_rmse: 0.92156 |  0:01:49s
epoch 34 | loss: 0.79133 | val_0_rmse: 0.92991 | val_1_rmse: 0.87011 |  0:01:52s
epoch 35 | loss: 0.79113 | val_0_rmse: 0.92801 | val_1_rmse: 0.89323 |  0:01:55s
epoch 36 | loss: 0.78906 | val_0_rmse: 0.95939 | val_1_rmse: 0.93146 |  0:01:59s
epoch 37 | loss: 0.78999 | val_0_rmse: 0.89345 | val_1_rmse: 0.87675 |  0:02:02s
epoch 38 | loss: 0.789   | val_0_rmse: 0.90312 | val_1_rmse: 0.88059 |  0:02:05s
epoch 39 | loss: 0.78962 | val_0_rmse: 0.88961 | val_1_rmse: 0.87143 |  0:02:08s
epoch 40 | loss: 0.78564 | val_0_rmse: 0.93579 | val_1_rmse: 0.91214 |  0:02:12s
epoch 41 | loss: 0.78905 | val_0_rmse: 0.95759 | val_1_rmse: 0.94136 |  0:02:15s
epoch 42 | loss: 0.78958 | val_0_rmse: 0.934   | val_1_rmse: 0.91862 |  0:02:18s
epoch 43 | loss: 0.78666 | val_0_rmse: 0.88893 | val_1_rmse: 0.86291 |  0:02:21s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.86184
Best weights from best epoch are automatically used!
ended training at: 06:24:01
Feature importance:
Mean squared error is of 3138454117.1930146
Mean absolute error:42836.3308727308
MAPE:0.6187233506088006
R2 score:0.21511747131422343
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:24:01
epoch 0  | loss: 1.1227  | val_0_rmse: 1.01664 | val_1_rmse: 1.03114 |  0:00:03s
epoch 1  | loss: 0.81323 | val_0_rmse: 0.91704 | val_1_rmse: 0.92941 |  0:00:06s
epoch 2  | loss: 0.7923  | val_0_rmse: 0.90785 | val_1_rmse: 0.92095 |  0:00:09s
epoch 3  | loss: 0.79196 | val_0_rmse: 0.91406 | val_1_rmse: 0.92353 |  0:00:12s
epoch 4  | loss: 0.78857 | val_0_rmse: 0.90263 | val_1_rmse: 0.91488 |  0:00:16s
epoch 5  | loss: 0.78986 | val_0_rmse: 1.02424 | val_1_rmse: 1.03406 |  0:00:19s
epoch 6  | loss: 0.78696 | val_0_rmse: 0.91666 | val_1_rmse: 0.92723 |  0:00:22s
epoch 7  | loss: 0.78806 | val_0_rmse: 0.89453 | val_1_rmse: 0.90852 |  0:00:25s
epoch 8  | loss: 0.78401 | val_0_rmse: 0.88638 | val_1_rmse: 0.89874 |  0:00:29s
epoch 9  | loss: 0.78547 | val_0_rmse: 0.88929 | val_1_rmse: 0.90257 |  0:00:32s
epoch 10 | loss: 0.78422 | val_0_rmse: 1.02623 | val_1_rmse: 1.03686 |  0:00:35s
epoch 11 | loss: 0.7857  | val_0_rmse: 1.05167 | val_1_rmse: 1.06422 |  0:00:38s
epoch 12 | loss: 0.78196 | val_0_rmse: 1.02148 | val_1_rmse: 1.03224 |  0:00:42s
epoch 13 | loss: 0.78613 | val_0_rmse: 1.04471 | val_1_rmse: 1.05554 |  0:00:45s
epoch 14 | loss: 0.78359 | val_0_rmse: 0.99823 | val_1_rmse: 1.00955 |  0:00:48s
epoch 15 | loss: 0.7849  | val_0_rmse: 0.89226 | val_1_rmse: 0.90649 |  0:00:51s
epoch 16 | loss: 0.78609 | val_0_rmse: 0.91448 | val_1_rmse: 0.92859 |  0:00:55s
epoch 17 | loss: 0.78327 | val_0_rmse: 0.88173 | val_1_rmse: 0.89357 |  0:00:58s
epoch 18 | loss: 0.7858  | val_0_rmse: 0.89704 | val_1_rmse: 0.91039 |  0:01:01s
epoch 19 | loss: 0.78231 | val_0_rmse: 0.99948 | val_1_rmse: 1.00905 |  0:01:04s
epoch 20 | loss: 0.78033 | val_0_rmse: 0.89065 | val_1_rmse: 0.9001  |  0:01:07s
epoch 21 | loss: 0.78509 | val_0_rmse: 0.91769 | val_1_rmse: 0.92814 |  0:01:11s
epoch 22 | loss: 0.78541 | val_0_rmse: 0.89778 | val_1_rmse: 0.90676 |  0:01:14s
epoch 23 | loss: 0.78032 | val_0_rmse: 0.90872 | val_1_rmse: 0.91755 |  0:01:17s
epoch 24 | loss: 0.77971 | val_0_rmse: 0.88973 | val_1_rmse: 0.90729 |  0:01:20s
epoch 25 | loss: 0.78053 | val_0_rmse: 0.89148 | val_1_rmse: 0.90812 |  0:01:23s
epoch 26 | loss: 0.78247 | val_0_rmse: 0.88773 | val_1_rmse: 0.9008  |  0:01:27s
epoch 27 | loss: 0.78028 | val_0_rmse: 0.91625 | val_1_rmse: 0.94481 |  0:01:30s
epoch 28 | loss: 0.77971 | val_0_rmse: 0.89341 | val_1_rmse: 0.90446 |  0:01:33s
epoch 29 | loss: 0.78147 | val_0_rmse: 0.89641 | val_1_rmse: 0.90746 |  0:01:36s
epoch 30 | loss: 0.77949 | val_0_rmse: 0.88674 | val_1_rmse: 0.89541 |  0:01:40s
epoch 31 | loss: 0.78048 | val_0_rmse: 0.90214 | val_1_rmse: 0.91535 |  0:01:43s
epoch 32 | loss: 0.77794 | val_0_rmse: 0.88914 | val_1_rmse: 0.89985 |  0:01:46s
epoch 33 | loss: 0.78245 | val_0_rmse: 0.8898  | val_1_rmse: 0.89812 |  0:01:49s
epoch 34 | loss: 0.77684 | val_0_rmse: 0.94876 | val_1_rmse: 0.95761 |  0:01:53s
epoch 35 | loss: 0.77563 | val_0_rmse: 0.89826 | val_1_rmse: 0.91219 |  0:01:56s
epoch 36 | loss: 0.78025 | val_0_rmse: 0.88234 | val_1_rmse: 0.89554 |  0:01:59s
epoch 37 | loss: 0.77878 | val_0_rmse: 0.88832 | val_1_rmse: 0.89989 |  0:02:02s
epoch 38 | loss: 0.779   | val_0_rmse: 0.88686 | val_1_rmse: 0.89541 |  0:02:06s
epoch 39 | loss: 0.78185 | val_0_rmse: 0.91907 | val_1_rmse: 1.29129 |  0:02:09s
epoch 40 | loss: 0.77851 | val_0_rmse: 0.88787 | val_1_rmse: 0.89661 |  0:02:12s
epoch 41 | loss: 0.77766 | val_0_rmse: 0.89571 | val_1_rmse: 0.90386 |  0:02:15s
epoch 42 | loss: 0.78033 | val_0_rmse: 0.89225 | val_1_rmse: 0.9039  |  0:02:19s
epoch 43 | loss: 0.77784 | val_0_rmse: 0.88235 | val_1_rmse: 0.89289 |  0:02:22s
epoch 44 | loss: 0.78063 | val_0_rmse: 0.87808 | val_1_rmse: 0.88866 |  0:02:25s
epoch 45 | loss: 0.77813 | val_0_rmse: 0.88972 | val_1_rmse: 0.89777 |  0:02:28s
epoch 46 | loss: 0.77374 | val_0_rmse: 0.89408 | val_1_rmse: 0.90374 |  0:02:32s
epoch 47 | loss: 0.7818  | val_0_rmse: 0.89408 | val_1_rmse: 0.90751 |  0:02:35s
epoch 48 | loss: 0.77622 | val_0_rmse: 0.91836 | val_1_rmse: 0.92659 |  0:02:38s
epoch 49 | loss: 0.78201 | val_0_rmse: 0.89324 | val_1_rmse: 0.90509 |  0:02:41s
epoch 50 | loss: 0.77831 | val_0_rmse: 0.8918  | val_1_rmse: 0.89988 |  0:02:44s
epoch 51 | loss: 0.7768  | val_0_rmse: 0.88714 | val_1_rmse: 0.89669 |  0:02:48s
epoch 52 | loss: 0.78067 | val_0_rmse: 0.91487 | val_1_rmse: 0.92914 |  0:02:51s
epoch 53 | loss: 0.78612 | val_0_rmse: 0.90387 | val_1_rmse: 0.91042 |  0:02:54s
epoch 54 | loss: 0.78546 | val_0_rmse: 0.88695 | val_1_rmse: 0.89763 |  0:02:57s
epoch 55 | loss: 0.7855  | val_0_rmse: 0.90245 | val_1_rmse: 0.91001 |  0:03:01s
epoch 56 | loss: 0.78961 | val_0_rmse: 0.89017 | val_1_rmse: 0.91648 |  0:03:04s
epoch 57 | loss: 0.79045 | val_0_rmse: 0.89471 | val_1_rmse: 0.90784 |  0:03:07s
epoch 58 | loss: 0.78535 | val_0_rmse: 0.88656 | val_1_rmse: 0.89487 |  0:03:10s
epoch 59 | loss: 0.78523 | val_0_rmse: 0.88728 | val_1_rmse: 2.16511 |  0:03:14s
epoch 60 | loss: 0.78324 | val_0_rmse: 0.89124 | val_1_rmse: 0.9035  |  0:03:17s
epoch 61 | loss: 0.78233 | val_0_rmse: 0.8946  | val_1_rmse: 2.7934  |  0:03:20s
epoch 62 | loss: 0.78397 | val_0_rmse: 0.91075 | val_1_rmse: 1.48452 |  0:03:23s
epoch 63 | loss: 0.78187 | val_0_rmse: 0.8837  | val_1_rmse: 0.89332 |  0:03:27s
epoch 64 | loss: 0.78152 | val_0_rmse: 0.88172 | val_1_rmse: 0.89186 |  0:03:30s
epoch 65 | loss: 0.78089 | val_0_rmse: 0.88212 | val_1_rmse: 0.89391 |  0:03:33s
epoch 66 | loss: 0.77965 | val_0_rmse: 1.04139 | val_1_rmse: 1.05175 |  0:03:36s
epoch 67 | loss: 0.7797  | val_0_rmse: 0.88364 | val_1_rmse: 0.89536 |  0:03:40s
epoch 68 | loss: 0.77909 | val_0_rmse: 0.88574 | val_1_rmse: 1.52604 |  0:03:43s
epoch 69 | loss: 0.77993 | val_0_rmse: 0.89398 | val_1_rmse: 1.43218 |  0:03:46s
epoch 70 | loss: 0.78243 | val_0_rmse: 0.89308 | val_1_rmse: 0.90466 |  0:03:49s
epoch 71 | loss: 0.7824  | val_0_rmse: 0.88746 | val_1_rmse: 1.00379 |  0:03:53s
epoch 72 | loss: 0.77897 | val_0_rmse: 0.89366 | val_1_rmse: 1.81311 |  0:03:56s
epoch 73 | loss: 0.77791 | val_0_rmse: 0.88356 | val_1_rmse: 0.89375 |  0:03:59s
epoch 74 | loss: 0.78029 | val_0_rmse: 0.88279 | val_1_rmse: 0.89433 |  0:04:02s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.88866
Best weights from best epoch are automatically used!
ended training at: 06:28:05
Feature importance:
Mean squared error is of 3055650922.1216607
Mean absolute error:42114.83572475194
MAPE:0.6080184330873558
R2 score:0.22768332231425525
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:05
epoch 0  | loss: 3.64757 | val_0_rmse: 0.98862 | val_1_rmse: 1.06354 |  0:00:00s
epoch 1  | loss: 2.22303 | val_0_rmse: 1.13301 | val_1_rmse: 1.2111  |  0:00:00s
epoch 2  | loss: 1.83117 | val_0_rmse: 1.1984  | val_1_rmse: 1.28238 |  0:00:00s
epoch 3  | loss: 1.27082 | val_0_rmse: 1.26574 | val_1_rmse: 1.37777 |  0:00:00s
epoch 4  | loss: 1.24682 | val_0_rmse: 1.12781 | val_1_rmse: 1.22861 |  0:00:01s
epoch 5  | loss: 1.10207 | val_0_rmse: 1.03071 | val_1_rmse: 1.13199 |  0:00:01s
epoch 6  | loss: 1.10842 | val_0_rmse: 0.99013 | val_1_rmse: 1.09162 |  0:00:01s
epoch 7  | loss: 1.03528 | val_0_rmse: 0.98378 | val_1_rmse: 1.07518 |  0:00:01s
epoch 8  | loss: 0.99899 | val_0_rmse: 0.99255 | val_1_rmse: 1.07291 |  0:00:01s
epoch 9  | loss: 1.03349 | val_0_rmse: 1.00353 | val_1_rmse: 1.07099 |  0:00:02s
epoch 10 | loss: 1.02383 | val_0_rmse: 1.00793 | val_1_rmse: 1.06633 |  0:00:02s
epoch 11 | loss: 0.98437 | val_0_rmse: 1.00267 | val_1_rmse: 1.06258 |  0:00:02s
epoch 12 | loss: 0.96104 | val_0_rmse: 0.9906  | val_1_rmse: 1.05879 |  0:00:02s
epoch 13 | loss: 0.93652 | val_0_rmse: 0.98294 | val_1_rmse: 1.05492 |  0:00:03s
epoch 14 | loss: 0.91339 | val_0_rmse: 0.98305 | val_1_rmse: 1.05001 |  0:00:03s
epoch 15 | loss: 0.89965 | val_0_rmse: 0.97677 | val_1_rmse: 1.04309 |  0:00:03s
epoch 16 | loss: 0.86684 | val_0_rmse: 0.98187 | val_1_rmse: 1.0393  |  0:00:03s
epoch 17 | loss: 0.86939 | val_0_rmse: 0.98367 | val_1_rmse: 1.03976 |  0:00:03s
epoch 18 | loss: 0.86222 | val_0_rmse: 0.98459 | val_1_rmse: 1.04104 |  0:00:04s
epoch 19 | loss: 0.86732 | val_0_rmse: 0.96371 | val_1_rmse: 1.03171 |  0:00:04s
epoch 20 | loss: 0.82117 | val_0_rmse: 0.95874 | val_1_rmse: 1.03143 |  0:00:04s
epoch 21 | loss: 0.79738 | val_0_rmse: 0.96655 | val_1_rmse: 1.03254 |  0:00:04s
epoch 22 | loss: 0.85731 | val_0_rmse: 0.95196 | val_1_rmse: 1.02225 |  0:00:04s
epoch 23 | loss: 0.78633 | val_0_rmse: 0.93128 | val_1_rmse: 1.00915 |  0:00:05s
epoch 24 | loss: 0.78847 | val_0_rmse: 1.07539 | val_1_rmse: 1.1673  |  0:00:05s
epoch 25 | loss: 0.8166  | val_0_rmse: 1.11672 | val_1_rmse: 1.21141 |  0:00:05s
epoch 26 | loss: 0.80769 | val_0_rmse: 1.07167 | val_1_rmse: 1.16365 |  0:00:05s
epoch 27 | loss: 0.81544 | val_0_rmse: 0.99448 | val_1_rmse: 1.07996 |  0:00:06s
epoch 28 | loss: 0.81757 | val_0_rmse: 0.95943 | val_1_rmse: 1.04269 |  0:00:06s
epoch 29 | loss: 0.79943 | val_0_rmse: 1.1252  | val_1_rmse: 1.22396 |  0:00:06s
epoch 30 | loss: 0.84083 | val_0_rmse: 1.06986 | val_1_rmse: 1.1606  |  0:00:06s
epoch 31 | loss: 0.84682 | val_0_rmse: 1.05855 | val_1_rmse: 1.14868 |  0:00:06s
epoch 32 | loss: 0.77278 | val_0_rmse: 0.90569 | val_1_rmse: 0.96566 |  0:00:07s
epoch 33 | loss: 0.76794 | val_0_rmse: 0.99606 | val_1_rmse: 1.03235 |  0:00:07s
epoch 34 | loss: 0.76527 | val_0_rmse: 0.99487 | val_1_rmse: 1.05764 |  0:00:07s
epoch 35 | loss: 0.76558 | val_0_rmse: 0.9649  | val_1_rmse: 1.03441 |  0:00:07s
epoch 36 | loss: 0.74593 | val_0_rmse: 0.93432 | val_1_rmse: 0.99528 |  0:00:07s
epoch 37 | loss: 0.75436 | val_0_rmse: 0.86223 | val_1_rmse: 0.94273 |  0:00:08s
epoch 38 | loss: 0.75236 | val_0_rmse: 0.90128 | val_1_rmse: 0.99502 |  0:00:08s
epoch 39 | loss: 0.76752 | val_0_rmse: 0.92353 | val_1_rmse: 1.01516 |  0:00:08s
epoch 40 | loss: 0.72815 | val_0_rmse: 1.0377  | val_1_rmse: 1.13374 |  0:00:08s
epoch 41 | loss: 0.7606  | val_0_rmse: 1.04926 | val_1_rmse: 1.154   |  0:00:08s
epoch 42 | loss: 0.72539 | val_0_rmse: 1.06341 | val_1_rmse: 1.15475 |  0:00:09s
epoch 43 | loss: 0.70846 | val_0_rmse: 0.95691 | val_1_rmse: 1.03636 |  0:00:09s
epoch 44 | loss: 0.69601 | val_0_rmse: 0.87134 | val_1_rmse: 0.94442 |  0:00:09s
epoch 45 | loss: 0.71479 | val_0_rmse: 0.8633  | val_1_rmse: 0.92608 |  0:00:09s
epoch 46 | loss: 0.71491 | val_0_rmse: 0.96203 | val_1_rmse: 1.01283 |  0:00:10s
epoch 47 | loss: 0.71489 | val_0_rmse: 1.13448 | val_1_rmse: 1.17153 |  0:00:10s
epoch 48 | loss: 0.71634 | val_0_rmse: 1.13989 | val_1_rmse: 1.16944 |  0:00:10s
epoch 49 | loss: 0.69648 | val_0_rmse: 0.97836 | val_1_rmse: 1.00017 |  0:00:10s
epoch 50 | loss: 0.69406 | val_0_rmse: 0.94992 | val_1_rmse: 0.98408 |  0:00:10s
epoch 51 | loss: 0.74007 | val_0_rmse: 0.93389 | val_1_rmse: 0.9984  |  0:00:11s
epoch 52 | loss: 0.69729 | val_0_rmse: 0.87329 | val_1_rmse: 0.9688  |  0:00:11s
epoch 53 | loss: 0.6908  | val_0_rmse: 0.90748 | val_1_rmse: 1.00665 |  0:00:11s
epoch 54 | loss: 0.69234 | val_0_rmse: 0.99091 | val_1_rmse: 1.07662 |  0:00:11s
epoch 55 | loss: 0.69235 | val_0_rmse: 0.98395 | val_1_rmse: 1.07097 |  0:00:12s
epoch 56 | loss: 0.73228 | val_0_rmse: 0.90059 | val_1_rmse: 0.97623 |  0:00:12s
epoch 57 | loss: 0.70681 | val_0_rmse: 0.93094 | val_1_rmse: 1.02465 |  0:00:12s
epoch 58 | loss: 0.71122 | val_0_rmse: 1.0563  | val_1_rmse: 1.16429 |  0:00:12s
epoch 59 | loss: 0.71885 | val_0_rmse: 1.1505  | val_1_rmse: 1.26184 |  0:00:12s
epoch 60 | loss: 0.69786 | val_0_rmse: 0.96246 | val_1_rmse: 1.05183 |  0:00:13s
epoch 61 | loss: 0.72677 | val_0_rmse: 1.04585 | val_1_rmse: 1.0971  |  0:00:13s
epoch 62 | loss: 0.69728 | val_0_rmse: 1.037   | val_1_rmse: 1.10493 |  0:00:13s
epoch 63 | loss: 0.68272 | val_0_rmse: 1.03986 | val_1_rmse: 1.10073 |  0:00:13s
epoch 64 | loss: 0.67739 | val_0_rmse: 0.98693 | val_1_rmse: 1.05421 |  0:00:13s
epoch 65 | loss: 0.66114 | val_0_rmse: 1.01779 | val_1_rmse: 1.11056 |  0:00:14s
epoch 66 | loss: 0.66953 | val_0_rmse: 1.14062 | val_1_rmse: 1.23266 |  0:00:14s
epoch 67 | loss: 0.67926 | val_0_rmse: 1.12076 | val_1_rmse: 1.20876 |  0:00:14s
epoch 68 | loss: 0.66215 | val_0_rmse: 1.07584 | val_1_rmse: 1.16645 |  0:00:14s
epoch 69 | loss: 0.67176 | val_0_rmse: 0.82604 | val_1_rmse: 0.88378 |  0:00:14s
epoch 70 | loss: 0.65267 | val_0_rmse: 0.83099 | val_1_rmse: 0.88746 |  0:00:15s
epoch 71 | loss: 0.66107 | val_0_rmse: 1.07636 | val_1_rmse: 1.16687 |  0:00:15s
epoch 72 | loss: 0.68498 | val_0_rmse: 1.09363 | val_1_rmse: 1.19002 |  0:00:15s
epoch 73 | loss: 0.66642 | val_0_rmse: 1.12202 | val_1_rmse: 1.21892 |  0:00:15s
epoch 74 | loss: 0.6948  | val_0_rmse: 1.10154 | val_1_rmse: 1.17593 |  0:00:16s
epoch 75 | loss: 0.69359 | val_0_rmse: 1.06382 | val_1_rmse: 1.08113 |  0:00:16s
epoch 76 | loss: 0.67206 | val_0_rmse: 1.18222 | val_1_rmse: 1.10833 |  0:00:16s
epoch 77 | loss: 0.69711 | val_0_rmse: 1.20049 | val_1_rmse: 1.14984 |  0:00:16s
epoch 78 | loss: 0.69677 | val_0_rmse: 1.21681 | val_1_rmse: 1.17079 |  0:00:16s
epoch 79 | loss: 0.66525 | val_0_rmse: 1.07879 | val_1_rmse: 1.07792 |  0:00:17s
epoch 80 | loss: 0.68971 | val_0_rmse: 1.04902 | val_1_rmse: 1.08333 |  0:00:17s
epoch 81 | loss: 0.67716 | val_0_rmse: 1.01962 | val_1_rmse: 1.07001 |  0:00:17s
epoch 82 | loss: 0.63922 | val_0_rmse: 0.90692 | val_1_rmse: 0.9205  |  0:00:17s
epoch 83 | loss: 0.65321 | val_0_rmse: 0.87772 | val_1_rmse: 0.90509 |  0:00:17s
epoch 84 | loss: 0.65724 | val_0_rmse: 0.87399 | val_1_rmse: 0.90388 |  0:00:18s
epoch 85 | loss: 0.64718 | val_0_rmse: 0.9517  | val_1_rmse: 0.98598 |  0:00:18s
epoch 86 | loss: 0.66105 | val_0_rmse: 1.06884 | val_1_rmse: 1.07935 |  0:00:18s
epoch 87 | loss: 0.66363 | val_0_rmse: 1.17796 | val_1_rmse: 1.16774 |  0:00:18s
epoch 88 | loss: 0.67095 | val_0_rmse: 1.18728 | val_1_rmse: 1.15813 |  0:00:18s
epoch 89 | loss: 0.66998 | val_0_rmse: 1.03991 | val_1_rmse: 1.00163 |  0:00:19s
epoch 90 | loss: 0.66848 | val_0_rmse: 1.28423 | val_1_rmse: 1.30763 |  0:00:19s
epoch 91 | loss: 0.66486 | val_0_rmse: 1.29887 | val_1_rmse: 1.32551 |  0:00:19s
epoch 92 | loss: 0.65338 | val_0_rmse: 1.24065 | val_1_rmse: 1.24446 |  0:00:19s
epoch 93 | loss: 0.63697 | val_0_rmse: 1.30906 | val_1_rmse: 1.2829  |  0:00:20s
epoch 94 | loss: 0.66462 | val_0_rmse: 1.32147 | val_1_rmse: 1.28287 |  0:00:20s
epoch 95 | loss: 0.69734 | val_0_rmse: 1.18334 | val_1_rmse: 1.19081 |  0:00:20s
epoch 96 | loss: 0.66026 | val_0_rmse: 1.14476 | val_1_rmse: 1.17685 |  0:00:20s
epoch 97 | loss: 0.65255 | val_0_rmse: 1.17266 | val_1_rmse: 1.18519 |  0:00:20s
epoch 98 | loss: 0.66643 | val_0_rmse: 1.05897 | val_1_rmse: 1.14497 |  0:00:21s
epoch 99 | loss: 0.6864  | val_0_rmse: 1.0738  | val_1_rmse: 1.14213 |  0:00:21s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.88378
Best weights from best epoch are automatically used!
ended training at: 06:28:26
Feature importance:
Mean squared error is of 5266542789.899414
Mean absolute error:55102.26735183662
MAPE:0.519377466625149
R2 score:0.2560244941353076
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:27
epoch 0  | loss: 3.7505  | val_0_rmse: 1.41833 | val_1_rmse: 1.48077 |  0:00:00s
epoch 1  | loss: 1.5968  | val_0_rmse: 1.05581 | val_1_rmse: 1.0278  |  0:00:00s
epoch 2  | loss: 1.72193 | val_0_rmse: 1.02083 | val_1_rmse: 0.99712 |  0:00:00s
epoch 3  | loss: 1.24925 | val_0_rmse: 0.9958  | val_1_rmse: 0.9797  |  0:00:00s
epoch 4  | loss: 1.10639 | val_0_rmse: 0.98608 | val_1_rmse: 0.97785 |  0:00:01s
epoch 5  | loss: 1.14327 | val_0_rmse: 0.99378 | val_1_rmse: 0.98948 |  0:00:01s
epoch 6  | loss: 1.08028 | val_0_rmse: 0.99522 | val_1_rmse: 0.99379 |  0:00:01s
epoch 7  | loss: 1.07044 | val_0_rmse: 0.99046 | val_1_rmse: 0.98649 |  0:00:01s
epoch 8  | loss: 1.15589 | val_0_rmse: 1.01375 | val_1_rmse: 1.0164  |  0:00:01s
epoch 9  | loss: 0.98103 | val_0_rmse: 1.03103 | val_1_rmse: 1.03656 |  0:00:02s
epoch 10 | loss: 0.95545 | val_0_rmse: 0.99466 | val_1_rmse: 0.99193 |  0:00:02s
epoch 11 | loss: 0.91447 | val_0_rmse: 0.98658 | val_1_rmse: 0.97615 |  0:00:02s
epoch 12 | loss: 0.83826 | val_0_rmse: 1.00678 | val_1_rmse: 0.98868 |  0:00:02s
epoch 13 | loss: 0.80468 | val_0_rmse: 1.03728 | val_1_rmse: 1.01556 |  0:00:02s
epoch 14 | loss: 0.78787 | val_0_rmse: 1.12768 | val_1_rmse: 1.09953 |  0:00:03s
epoch 15 | loss: 0.76902 | val_0_rmse: 1.14579 | val_1_rmse: 1.11841 |  0:00:03s
epoch 16 | loss: 0.77913 | val_0_rmse: 1.05451 | val_1_rmse: 1.049   |  0:00:03s
epoch 17 | loss: 0.7611  | val_0_rmse: 1.02342 | val_1_rmse: 1.03198 |  0:00:03s
epoch 18 | loss: 0.74471 | val_0_rmse: 0.99035 | val_1_rmse: 1.00804 |  0:00:04s
epoch 19 | loss: 0.72047 | val_0_rmse: 1.05496 | val_1_rmse: 1.05498 |  0:00:04s
epoch 20 | loss: 0.70786 | val_0_rmse: 1.1228  | val_1_rmse: 1.10778 |  0:00:04s
epoch 21 | loss: 0.70231 | val_0_rmse: 1.15193 | val_1_rmse: 1.13769 |  0:00:04s
epoch 22 | loss: 0.71739 | val_0_rmse: 1.11838 | val_1_rmse: 1.1083  |  0:00:04s
epoch 23 | loss: 0.70776 | val_0_rmse: 1.01556 | val_1_rmse: 1.01446 |  0:00:05s
epoch 24 | loss: 0.70239 | val_0_rmse: 0.96692 | val_1_rmse: 0.97845 |  0:00:05s
epoch 25 | loss: 0.72587 | val_0_rmse: 1.13867 | val_1_rmse: 1.15973 |  0:00:05s
epoch 26 | loss: 0.70455 | val_0_rmse: 1.12199 | val_1_rmse: 1.161   |  0:00:05s
epoch 27 | loss: 0.69079 | val_0_rmse: 1.0553  | val_1_rmse: 1.06193 |  0:00:05s
epoch 28 | loss: 0.6909  | val_0_rmse: 1.09336 | val_1_rmse: 1.08256 |  0:00:06s
epoch 29 | loss: 0.69173 | val_0_rmse: 1.09662 | val_1_rmse: 1.07975 |  0:00:06s
epoch 30 | loss: 0.6944  | val_0_rmse: 0.93549 | val_1_rmse: 0.93203 |  0:00:06s
epoch 31 | loss: 0.68865 | val_0_rmse: 1.09411 | val_1_rmse: 1.07096 |  0:00:06s
epoch 32 | loss: 0.68421 | val_0_rmse: 1.10979 | val_1_rmse: 1.0655  |  0:00:06s
epoch 33 | loss: 0.68861 | val_0_rmse: 0.90865 | val_1_rmse: 0.91011 |  0:00:07s
epoch 34 | loss: 0.69028 | val_0_rmse: 0.92293 | val_1_rmse: 0.92916 |  0:00:07s
epoch 35 | loss: 0.68493 | val_0_rmse: 0.84449 | val_1_rmse: 0.84128 |  0:00:07s
epoch 36 | loss: 0.67977 | val_0_rmse: 1.11497 | val_1_rmse: 1.11174 |  0:00:07s
epoch 37 | loss: 0.69736 | val_0_rmse: 0.86236 | val_1_rmse: 0.87995 |  0:00:08s
epoch 38 | loss: 0.68726 | val_0_rmse: 1.0976  | val_1_rmse: 1.08695 |  0:00:08s
epoch 39 | loss: 0.66973 | val_0_rmse: 1.19844 | val_1_rmse: 1.1875  |  0:00:08s
epoch 40 | loss: 0.69359 | val_0_rmse: 1.12267 | val_1_rmse: 1.11896 |  0:00:08s
epoch 41 | loss: 0.67952 | val_0_rmse: 1.10818 | val_1_rmse: 1.10109 |  0:00:08s
epoch 42 | loss: 0.69588 | val_0_rmse: 1.10618 | val_1_rmse: 1.096   |  0:00:09s
epoch 43 | loss: 0.6814  | val_0_rmse: 1.0828  | val_1_rmse: 1.08531 |  0:00:09s
epoch 44 | loss: 0.7029  | val_0_rmse: 0.89354 | val_1_rmse: 0.9099  |  0:00:09s
epoch 45 | loss: 0.67112 | val_0_rmse: 1.10758 | val_1_rmse: 1.09449 |  0:00:09s
epoch 46 | loss: 0.67957 | val_0_rmse: 1.23774 | val_1_rmse: 1.20902 |  0:00:09s
epoch 47 | loss: 0.69052 | val_0_rmse: 1.22265 | val_1_rmse: 1.19397 |  0:00:10s
epoch 48 | loss: 0.69699 | val_0_rmse: 0.91435 | val_1_rmse: 0.92112 |  0:00:10s
epoch 49 | loss: 0.69988 | val_0_rmse: 1.04098 | val_1_rmse: 1.02497 |  0:00:10s
epoch 50 | loss: 0.70317 | val_0_rmse: 1.01179 | val_1_rmse: 1.00716 |  0:00:10s
epoch 51 | loss: 0.69492 | val_0_rmse: 0.85557 | val_1_rmse: 0.85551 |  0:00:11s
epoch 52 | loss: 0.67481 | val_0_rmse: 0.968   | val_1_rmse: 0.97029 |  0:00:11s
epoch 53 | loss: 0.6935  | val_0_rmse: 1.1808  | val_1_rmse: 1.14238 |  0:00:11s
epoch 54 | loss: 0.69258 | val_0_rmse: 1.1489  | val_1_rmse: 1.1535  |  0:00:11s
epoch 55 | loss: 0.69001 | val_0_rmse: 1.09135 | val_1_rmse: 1.11512 |  0:00:11s
epoch 56 | loss: 0.68602 | val_0_rmse: 1.03595 | val_1_rmse: 1.05416 |  0:00:12s
epoch 57 | loss: 0.70701 | val_0_rmse: 1.01242 | val_1_rmse: 1.03801 |  0:00:12s
epoch 58 | loss: 0.71342 | val_0_rmse: 0.98707 | val_1_rmse: 1.01705 |  0:00:12s
epoch 59 | loss: 0.72075 | val_0_rmse: 1.00599 | val_1_rmse: 1.02458 |  0:00:12s
epoch 60 | loss: 0.67596 | val_0_rmse: 1.07589 | val_1_rmse: 1.07874 |  0:00:12s
epoch 61 | loss: 0.7003  | val_0_rmse: 1.05504 | val_1_rmse: 1.06804 |  0:00:13s
epoch 62 | loss: 0.68919 | val_0_rmse: 1.01636 | val_1_rmse: 1.04728 |  0:00:13s
epoch 63 | loss: 0.68361 | val_0_rmse: 1.15474 | val_1_rmse: 1.12094 |  0:00:13s
epoch 64 | loss: 0.70452 | val_0_rmse: 1.06563 | val_1_rmse: 1.05616 |  0:00:13s
epoch 65 | loss: 0.6955  | val_0_rmse: 1.22829 | val_1_rmse: 1.20152 |  0:00:13s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.84128
Best weights from best epoch are automatically used!
ended training at: 06:28:41
Feature importance:
Mean squared error is of 5312782393.201832
Mean absolute error:55612.75191406249
MAPE:0.5370828552327733
R2 score:0.29497637207901906
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:41
epoch 0  | loss: 4.2418  | val_0_rmse: 1.14563 | val_1_rmse: 1.01691 |  0:00:00s
epoch 1  | loss: 3.28172 | val_0_rmse: 1.035   | val_1_rmse: 0.90469 |  0:00:00s
epoch 2  | loss: 1.63197 | val_0_rmse: 1.02781 | val_1_rmse: 0.9001  |  0:00:00s
epoch 3  | loss: 1.39103 | val_0_rmse: 1.01351 | val_1_rmse: 0.89546 |  0:00:00s
epoch 4  | loss: 1.19169 | val_0_rmse: 1.01046 | val_1_rmse: 0.89534 |  0:00:01s
epoch 5  | loss: 1.12576 | val_0_rmse: 1.00443 | val_1_rmse: 0.89481 |  0:00:01s
epoch 6  | loss: 1.06663 | val_0_rmse: 1.00783 | val_1_rmse: 0.90171 |  0:00:01s
epoch 7  | loss: 1.12456 | val_0_rmse: 1.00669 | val_1_rmse: 0.91171 |  0:00:01s
epoch 8  | loss: 1.08508 | val_0_rmse: 1.00719 | val_1_rmse: 0.92033 |  0:00:01s
epoch 9  | loss: 1.03049 | val_0_rmse: 1.00764 | val_1_rmse: 0.92113 |  0:00:02s
epoch 10 | loss: 1.00628 | val_0_rmse: 1.00246 | val_1_rmse: 0.90579 |  0:00:02s
epoch 11 | loss: 0.99018 | val_0_rmse: 1.00294 | val_1_rmse: 0.89948 |  0:00:02s
epoch 12 | loss: 0.98142 | val_0_rmse: 1.00211 | val_1_rmse: 0.89704 |  0:00:02s
epoch 13 | loss: 0.96561 | val_0_rmse: 0.98956 | val_1_rmse: 0.8995  |  0:00:03s
epoch 14 | loss: 0.9233  | val_0_rmse: 0.97858 | val_1_rmse: 0.9029  |  0:00:03s
epoch 15 | loss: 0.91166 | val_0_rmse: 1.03812 | val_1_rmse: 0.89843 |  0:00:03s
epoch 16 | loss: 0.86346 | val_0_rmse: 1.14009 | val_1_rmse: 0.98446 |  0:00:03s
epoch 17 | loss: 0.81478 | val_0_rmse: 1.12512 | val_1_rmse: 0.98035 |  0:00:03s
epoch 18 | loss: 0.8359  | val_0_rmse: 1.14335 | val_1_rmse: 0.98335 |  0:00:04s
epoch 19 | loss: 0.81264 | val_0_rmse: 1.22155 | val_1_rmse: 1.05683 |  0:00:04s
epoch 20 | loss: 0.81514 | val_0_rmse: 1.27739 | val_1_rmse: 1.11623 |  0:00:04s
epoch 21 | loss: 0.77215 | val_0_rmse: 1.09821 | val_1_rmse: 1.07384 |  0:00:04s
epoch 22 | loss: 0.77848 | val_0_rmse: 0.99398 | val_1_rmse: 0.93131 |  0:00:04s
epoch 23 | loss: 0.76178 | val_0_rmse: 1.03503 | val_1_rmse: 0.94732 |  0:00:05s
epoch 24 | loss: 0.7602  | val_0_rmse: 1.13084 | val_1_rmse: 1.11484 |  0:00:05s
epoch 25 | loss: 0.75208 | val_0_rmse: 1.06771 | val_1_rmse: 0.98171 |  0:00:05s
epoch 26 | loss: 0.7786  | val_0_rmse: 0.89311 | val_1_rmse: 0.92641 |  0:00:05s
epoch 27 | loss: 0.75719 | val_0_rmse: 1.19787 | val_1_rmse: 1.03327 |  0:00:05s
epoch 28 | loss: 0.74237 | val_0_rmse: 1.30303 | val_1_rmse: 1.13335 |  0:00:06s
epoch 29 | loss: 0.80272 | val_0_rmse: 1.19485 | val_1_rmse: 1.02992 |  0:00:06s
epoch 30 | loss: 0.75319 | val_0_rmse: 1.1767  | val_1_rmse: 1.02564 |  0:00:06s
epoch 31 | loss: 0.73074 | val_0_rmse: 1.27017 | val_1_rmse: 1.10438 |  0:00:06s
epoch 32 | loss: 0.76964 | val_0_rmse: 1.25257 | val_1_rmse: 1.09148 |  0:00:07s
epoch 33 | loss: 0.75764 | val_0_rmse: 1.1836  | val_1_rmse: 1.03693 |  0:00:07s
epoch 34 | loss: 0.73993 | val_0_rmse: 1.16151 | val_1_rmse: 1.00705 |  0:00:07s
epoch 35 | loss: 0.71707 | val_0_rmse: 1.20666 | val_1_rmse: 1.04731 |  0:00:07s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 0.89481
Best weights from best epoch are automatically used!
ended training at: 06:28:49
Feature importance:
Mean squared error is of 7614235538.145099
Mean absolute error:69473.88023410087
MAPE:0.6234774881018883
R2 score:0.01718183351256286
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:49
epoch 0  | loss: 4.59099 | val_0_rmse: 1.23767 | val_1_rmse: 1.21068 |  0:00:00s
epoch 1  | loss: 1.79885 | val_0_rmse: 1.08784 | val_1_rmse: 1.07428 |  0:00:00s
epoch 2  | loss: 1.40735 | val_0_rmse: 1.0061  | val_1_rmse: 1.00287 |  0:00:00s
epoch 3  | loss: 1.61086 | val_0_rmse: 0.99917 | val_1_rmse: 0.99979 |  0:00:00s
epoch 4  | loss: 1.18633 | val_0_rmse: 1.0127  | val_1_rmse: 1.00854 |  0:00:01s
epoch 5  | loss: 1.07526 | val_0_rmse: 1.06217 | val_1_rmse: 1.05638 |  0:00:01s
epoch 6  | loss: 1.05368 | val_0_rmse: 1.0692  | val_1_rmse: 1.06339 |  0:00:01s
epoch 7  | loss: 1.00726 | val_0_rmse: 1.10646 | val_1_rmse: 1.09905 |  0:00:01s
epoch 8  | loss: 1.02586 | val_0_rmse: 1.05107 | val_1_rmse: 1.05231 |  0:00:01s
epoch 9  | loss: 0.99242 | val_0_rmse: 1.18247 | val_1_rmse: 1.1857  |  0:00:02s
epoch 10 | loss: 0.9232  | val_0_rmse: 1.50823 | val_1_rmse: 1.50484 |  0:00:02s
epoch 11 | loss: 0.90672 | val_0_rmse: 1.48244 | val_1_rmse: 1.47561 |  0:00:02s
epoch 12 | loss: 0.89181 | val_0_rmse: 1.49441 | val_1_rmse: 1.48673 |  0:00:02s
epoch 13 | loss: 0.8802  | val_0_rmse: 1.86273 | val_1_rmse: 1.85553 |  0:00:03s
epoch 14 | loss: 0.86151 | val_0_rmse: 2.02669 | val_1_rmse: 2.0197  |  0:00:03s
epoch 15 | loss: 0.85546 | val_0_rmse: 1.90456 | val_1_rmse: 1.89232 |  0:00:03s
epoch 16 | loss: 0.83716 | val_0_rmse: 1.64927 | val_1_rmse: 1.63581 |  0:00:03s
epoch 17 | loss: 0.82972 | val_0_rmse: 1.50425 | val_1_rmse: 1.49294 |  0:00:03s
epoch 18 | loss: 0.82287 | val_0_rmse: 1.63083 | val_1_rmse: 1.61833 |  0:00:04s
epoch 19 | loss: 0.82644 | val_0_rmse: 1.54063 | val_1_rmse: 1.53083 |  0:00:04s
epoch 20 | loss: 0.8311  | val_0_rmse: 1.2267  | val_1_rmse: 1.22396 |  0:00:04s
epoch 21 | loss: 0.81953 | val_0_rmse: 1.22776 | val_1_rmse: 1.22409 |  0:00:04s
epoch 22 | loss: 0.80544 | val_0_rmse: 1.31458 | val_1_rmse: 1.3097  |  0:00:04s
epoch 23 | loss: 0.82926 | val_0_rmse: 1.18678 | val_1_rmse: 1.18359 |  0:00:05s
epoch 24 | loss: 0.80088 | val_0_rmse: 1.10901 | val_1_rmse: 1.10747 |  0:00:05s
epoch 25 | loss: 0.7833  | val_0_rmse: 1.25949 | val_1_rmse: 1.25303 |  0:00:05s
epoch 26 | loss: 0.78947 | val_0_rmse: 1.38618 | val_1_rmse: 1.37874 |  0:00:05s
epoch 27 | loss: 0.7909  | val_0_rmse: 1.21806 | val_1_rmse: 1.20778 |  0:00:06s
epoch 28 | loss: 0.78489 | val_0_rmse: 1.12165 | val_1_rmse: 1.11425 |  0:00:06s
epoch 29 | loss: 0.76732 | val_0_rmse: 1.17982 | val_1_rmse: 1.18234 |  0:00:06s
epoch 30 | loss: 0.79176 | val_0_rmse: 1.20028 | val_1_rmse: 1.20562 |  0:00:06s
epoch 31 | loss: 0.76725 | val_0_rmse: 1.15636 | val_1_rmse: 1.16736 |  0:00:06s
epoch 32 | loss: 0.76226 | val_0_rmse: 1.16912 | val_1_rmse: 1.17418 |  0:00:07s
epoch 33 | loss: 0.80183 | val_0_rmse: 1.14214 | val_1_rmse: 1.13669 |  0:00:07s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.99979
Best weights from best epoch are automatically used!
ended training at: 06:28:56
Feature importance:
Mean squared error is of 6801706317.268529
Mean absolute error:66099.22649849232
MAPE:0.6791468916637551
R2 score:0.020612209475029064
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:28:56
epoch 0  | loss: 4.10058 | val_0_rmse: 2.28752 | val_1_rmse: 1.95907 |  0:00:00s
epoch 1  | loss: 2.5987  | val_0_rmse: 1.24469 | val_1_rmse: 1.23899 |  0:00:00s
epoch 2  | loss: 2.28715 | val_0_rmse: 1.09655 | val_1_rmse: 1.08997 |  0:00:00s
epoch 3  | loss: 1.51734 | val_0_rmse: 1.03596 | val_1_rmse: 1.02404 |  0:00:00s
epoch 4  | loss: 1.30669 | val_0_rmse: 1.01553 | val_1_rmse: 0.99765 |  0:00:01s
epoch 5  | loss: 1.1163  | val_0_rmse: 0.99409 | val_1_rmse: 0.98164 |  0:00:01s
epoch 6  | loss: 1.09629 | val_0_rmse: 0.99732 | val_1_rmse: 0.98494 |  0:00:01s
epoch 7  | loss: 0.98098 | val_0_rmse: 0.99756 | val_1_rmse: 0.98265 |  0:00:01s
epoch 8  | loss: 1.01886 | val_0_rmse: 0.99331 | val_1_rmse: 0.97887 |  0:00:01s
epoch 9  | loss: 1.01112 | val_0_rmse: 0.99474 | val_1_rmse: 0.97883 |  0:00:02s
epoch 10 | loss: 1.13444 | val_0_rmse: 0.99341 | val_1_rmse: 0.97931 |  0:00:02s
epoch 11 | loss: 1.0145  | val_0_rmse: 0.99742 | val_1_rmse: 0.98309 |  0:00:02s
epoch 12 | loss: 1.02014 | val_0_rmse: 0.99361 | val_1_rmse: 0.97934 |  0:00:02s
epoch 13 | loss: 0.98699 | val_0_rmse: 0.99321 | val_1_rmse: 0.97794 |  0:00:03s
epoch 14 | loss: 0.96692 | val_0_rmse: 0.98221 | val_1_rmse: 0.97331 |  0:00:03s
epoch 15 | loss: 0.95775 | val_0_rmse: 0.9831  | val_1_rmse: 0.9809  |  0:00:03s
epoch 16 | loss: 0.95495 | val_0_rmse: 0.97636 | val_1_rmse: 0.97319 |  0:00:03s
epoch 17 | loss: 0.94319 | val_0_rmse: 0.97718 | val_1_rmse: 0.96415 |  0:00:03s
epoch 18 | loss: 0.91086 | val_0_rmse: 0.97936 | val_1_rmse: 0.96499 |  0:00:04s
epoch 19 | loss: 0.88733 | val_0_rmse: 0.96224 | val_1_rmse: 0.95349 |  0:00:04s
epoch 20 | loss: 0.84452 | val_0_rmse: 0.97497 | val_1_rmse: 0.96976 |  0:00:04s
epoch 21 | loss: 0.89714 | val_0_rmse: 0.99773 | val_1_rmse: 0.99972 |  0:00:04s
epoch 22 | loss: 0.84208 | val_0_rmse: 1.0413  | val_1_rmse: 1.038   |  0:00:04s
epoch 23 | loss: 0.85681 | val_0_rmse: 1.07577 | val_1_rmse: 1.06703 |  0:00:05s
epoch 24 | loss: 0.82376 | val_0_rmse: 1.00134 | val_1_rmse: 0.99051 |  0:00:05s
epoch 25 | loss: 0.84817 | val_0_rmse: 0.95204 | val_1_rmse: 0.94    |  0:00:05s
epoch 26 | loss: 0.8161  | val_0_rmse: 1.0282  | val_1_rmse: 1.00108 |  0:00:05s
epoch 27 | loss: 0.8094  | val_0_rmse: 1.02465 | val_1_rmse: 0.9985  |  0:00:06s
epoch 28 | loss: 0.79554 | val_0_rmse: 1.01612 | val_1_rmse: 0.99429 |  0:00:06s
epoch 29 | loss: 0.79409 | val_0_rmse: 1.04994 | val_1_rmse: 1.03488 |  0:00:06s
epoch 30 | loss: 0.86074 | val_0_rmse: 1.02042 | val_1_rmse: 1.00452 |  0:00:06s
epoch 31 | loss: 0.80446 | val_0_rmse: 0.93024 | val_1_rmse: 0.93441 |  0:00:06s
epoch 32 | loss: 0.79832 | val_0_rmse: 0.95526 | val_1_rmse: 0.96061 |  0:00:07s
epoch 33 | loss: 0.83171 | val_0_rmse: 0.93334 | val_1_rmse: 0.93083 |  0:00:07s
epoch 34 | loss: 0.80437 | val_0_rmse: 0.93794 | val_1_rmse: 0.93987 |  0:00:07s
epoch 35 | loss: 0.8008  | val_0_rmse: 1.08796 | val_1_rmse: 1.09118 |  0:00:07s
epoch 36 | loss: 0.80123 | val_0_rmse: 1.11389 | val_1_rmse: 1.11634 |  0:00:07s
epoch 37 | loss: 0.80962 | val_0_rmse: 1.08731 | val_1_rmse: 1.09198 |  0:00:08s
epoch 38 | loss: 0.80071 | val_0_rmse: 1.14947 | val_1_rmse: 1.15195 |  0:00:08s
epoch 39 | loss: 0.81173 | val_0_rmse: 0.8984  | val_1_rmse: 0.90774 |  0:00:08s
epoch 40 | loss: 0.78863 | val_0_rmse: 1.0623  | val_1_rmse: 1.07313 |  0:00:08s
epoch 41 | loss: 0.79036 | val_0_rmse: 1.10307 | val_1_rmse: 1.11931 |  0:00:09s
epoch 42 | loss: 0.81298 | val_0_rmse: 1.09437 | val_1_rmse: 1.11346 |  0:00:09s
epoch 43 | loss: 0.78415 | val_0_rmse: 1.09859 | val_1_rmse: 1.11886 |  0:00:09s
epoch 44 | loss: 0.79789 | val_0_rmse: 1.112   | val_1_rmse: 1.11957 |  0:00:09s
epoch 45 | loss: 0.80029 | val_0_rmse: 0.92366 | val_1_rmse: 0.93048 |  0:00:09s
epoch 46 | loss: 0.78899 | val_0_rmse: 0.90353 | val_1_rmse: 0.91016 |  0:00:10s
epoch 47 | loss: 0.78634 | val_0_rmse: 0.90743 | val_1_rmse: 0.91152 |  0:00:10s
epoch 48 | loss: 0.79261 | val_0_rmse: 0.897   | val_1_rmse: 0.90211 |  0:00:10s
epoch 49 | loss: 0.77927 | val_0_rmse: 0.88902 | val_1_rmse: 0.90221 |  0:00:10s
epoch 50 | loss: 0.78666 | val_0_rmse: 0.88541 | val_1_rmse: 0.90172 |  0:00:10s
epoch 51 | loss: 0.79129 | val_0_rmse: 0.92056 | val_1_rmse: 0.93029 |  0:00:11s
epoch 52 | loss: 0.79123 | val_0_rmse: 0.95216 | val_1_rmse: 0.9608  |  0:00:11s
epoch 53 | loss: 0.78381 | val_0_rmse: 0.94153 | val_1_rmse: 0.95405 |  0:00:11s
epoch 54 | loss: 0.78442 | val_0_rmse: 0.91495 | val_1_rmse: 0.92746 |  0:00:11s
epoch 55 | loss: 0.79151 | val_0_rmse: 0.94423 | val_1_rmse: 0.95601 |  0:00:11s
epoch 56 | loss: 0.78021 | val_0_rmse: 0.97274 | val_1_rmse: 0.98289 |  0:00:12s
epoch 57 | loss: 0.77619 | val_0_rmse: 0.94393 | val_1_rmse: 0.95087 |  0:00:12s
epoch 58 | loss: 0.77847 | val_0_rmse: 0.92686 | val_1_rmse: 0.93474 |  0:00:12s
epoch 59 | loss: 0.77788 | val_0_rmse: 0.89589 | val_1_rmse: 0.90841 |  0:00:12s
epoch 60 | loss: 0.77505 | val_0_rmse: 0.87962 | val_1_rmse: 0.89315 |  0:00:13s
epoch 61 | loss: 0.77295 | val_0_rmse: 0.89099 | val_1_rmse: 0.89706 |  0:00:13s
epoch 62 | loss: 0.77694 | val_0_rmse: 0.8808  | val_1_rmse: 0.89311 |  0:00:13s
epoch 63 | loss: 0.77251 | val_0_rmse: 0.92223 | val_1_rmse: 0.92901 |  0:00:13s
epoch 64 | loss: 0.77293 | val_0_rmse: 0.90696 | val_1_rmse: 0.9146  |  0:00:13s
epoch 65 | loss: 0.76721 | val_0_rmse: 0.88976 | val_1_rmse: 0.89996 |  0:00:14s
epoch 66 | loss: 0.78277 | val_0_rmse: 0.91143 | val_1_rmse: 0.91778 |  0:00:14s
epoch 67 | loss: 0.77274 | val_0_rmse: 0.89315 | val_1_rmse: 0.90257 |  0:00:14s
epoch 68 | loss: 0.78059 | val_0_rmse: 0.91761 | val_1_rmse: 0.92378 |  0:00:14s
epoch 69 | loss: 0.77017 | val_0_rmse: 0.99491 | val_1_rmse: 1.03135 |  0:00:14s
epoch 70 | loss: 0.78064 | val_0_rmse: 1.03466 | val_1_rmse: 1.07687 |  0:00:15s
epoch 71 | loss: 0.81157 | val_0_rmse: 1.1944  | val_1_rmse: 1.25562 |  0:00:15s
epoch 72 | loss: 0.77571 | val_0_rmse: 1.222   | val_1_rmse: 1.28706 |  0:00:15s
epoch 73 | loss: 0.78235 | val_0_rmse: 1.27542 | val_1_rmse: 1.35163 |  0:00:15s
epoch 74 | loss: 0.78715 | val_0_rmse: 1.24443 | val_1_rmse: 1.31672 |  0:00:15s
epoch 75 | loss: 0.77783 | val_0_rmse: 1.21197 | val_1_rmse: 1.2905  |  0:00:16s
epoch 76 | loss: 0.80864 | val_0_rmse: 1.02803 | val_1_rmse: 1.05691 |  0:00:16s
epoch 77 | loss: 0.77671 | val_0_rmse: 1.06962 | val_1_rmse: 1.10967 |  0:00:16s
epoch 78 | loss: 0.78653 | val_0_rmse: 0.99743 | val_1_rmse: 1.03354 |  0:00:16s
epoch 79 | loss: 0.78267 | val_0_rmse: 0.95627 | val_1_rmse: 0.9851  |  0:00:17s
epoch 80 | loss: 0.76248 | val_0_rmse: 0.87903 | val_1_rmse: 0.90079 |  0:00:17s
epoch 81 | loss: 0.77118 | val_0_rmse: 0.93639 | val_1_rmse: 0.94386 |  0:00:17s
epoch 82 | loss: 0.76846 | val_0_rmse: 0.94877 | val_1_rmse: 0.93567 |  0:00:17s
epoch 83 | loss: 0.77572 | val_0_rmse: 0.887   | val_1_rmse: 0.90073 |  0:00:17s
epoch 84 | loss: 0.7696  | val_0_rmse: 0.87811 | val_1_rmse: 0.89028 |  0:00:18s
epoch 85 | loss: 0.83591 | val_0_rmse: 0.90967 | val_1_rmse: 0.93328 |  0:00:18s
epoch 86 | loss: 0.76853 | val_0_rmse: 1.01019 | val_1_rmse: 1.04779 |  0:00:18s
epoch 87 | loss: 0.78603 | val_0_rmse: 1.02808 | val_1_rmse: 1.05381 |  0:00:18s
epoch 88 | loss: 0.75925 | val_0_rmse: 1.03803 | val_1_rmse: 1.05689 |  0:00:18s
epoch 89 | loss: 0.74659 | val_0_rmse: 1.06214 | val_1_rmse: 1.07617 |  0:00:19s
epoch 90 | loss: 0.74576 | val_0_rmse: 1.06604 | val_1_rmse: 1.08388 |  0:00:19s
epoch 91 | loss: 0.74332 | val_0_rmse: 1.09225 | val_1_rmse: 1.11439 |  0:00:19s
epoch 92 | loss: 0.7413  | val_0_rmse: 1.06461 | val_1_rmse: 1.10901 |  0:00:19s
epoch 93 | loss: 0.7394  | val_0_rmse: 1.02417 | val_1_rmse: 1.0897  |  0:00:20s
epoch 94 | loss: 0.73007 | val_0_rmse: 1.07855 | val_1_rmse: 1.13239 |  0:00:20s
epoch 95 | loss: 0.73742 | val_0_rmse: 1.03583 | val_1_rmse: 1.07477 |  0:00:20s
epoch 96 | loss: 0.71953 | val_0_rmse: 1.02573 | val_1_rmse: 1.07943 |  0:00:20s
epoch 97 | loss: 0.7143  | val_0_rmse: 1.00602 | val_1_rmse: 1.04528 |  0:00:20s
epoch 98 | loss: 0.73173 | val_0_rmse: 1.00475 | val_1_rmse: 1.03302 |  0:00:21s
epoch 99 | loss: 0.76035 | val_0_rmse: 0.90768 | val_1_rmse: 0.9056  |  0:00:21s
epoch 100| loss: 0.76378 | val_0_rmse: 0.84414 | val_1_rmse: 0.829   |  0:00:21s
epoch 101| loss: 0.72546 | val_0_rmse: 0.89329 | val_1_rmse: 0.86501 |  0:00:21s
epoch 102| loss: 0.71655 | val_0_rmse: 0.96141 | val_1_rmse: 0.95194 |  0:00:21s
epoch 103| loss: 0.73372 | val_0_rmse: 1.07927 | val_1_rmse: 1.08121 |  0:00:22s
epoch 104| loss: 0.69667 | val_0_rmse: 1.04345 | val_1_rmse: 1.03864 |  0:00:22s
epoch 105| loss: 0.75869 | val_0_rmse: 1.07941 | val_1_rmse: 1.12601 |  0:00:22s
epoch 106| loss: 0.79801 | val_0_rmse: 1.0128  | val_1_rmse: 1.06067 |  0:00:22s
epoch 107| loss: 0.78196 | val_0_rmse: 0.92569 | val_1_rmse: 0.91626 |  0:00:22s
epoch 108| loss: 0.74203 | val_0_rmse: 0.90569 | val_1_rmse: 0.89265 |  0:00:23s
epoch 109| loss: 0.73737 | val_0_rmse: 0.87415 | val_1_rmse: 0.8783  |  0:00:23s
epoch 110| loss: 0.73118 | val_0_rmse: 0.88489 | val_1_rmse: 0.89464 |  0:00:23s
epoch 111| loss: 0.72105 | val_0_rmse: 0.89978 | val_1_rmse: 0.93538 |  0:00:23s
epoch 112| loss: 0.72532 | val_0_rmse: 0.90259 | val_1_rmse: 0.93917 |  0:00:24s
epoch 113| loss: 0.71242 | val_0_rmse: 0.95807 | val_1_rmse: 0.98165 |  0:00:24s
epoch 114| loss: 0.73585 | val_0_rmse: 0.93347 | val_1_rmse: 0.95772 |  0:00:24s
epoch 115| loss: 0.71102 | val_0_rmse: 0.97596 | val_1_rmse: 1.02076 |  0:00:24s
epoch 116| loss: 0.715   | val_0_rmse: 1.14187 | val_1_rmse: 1.18675 |  0:00:24s
epoch 117| loss: 0.71452 | val_0_rmse: 1.08933 | val_1_rmse: 1.10488 |  0:00:25s
epoch 118| loss: 0.70412 | val_0_rmse: 0.95403 | val_1_rmse: 0.94478 |  0:00:25s
epoch 119| loss: 0.7399  | val_0_rmse: 1.07618 | val_1_rmse: 1.07112 |  0:00:25s
epoch 120| loss: 0.70095 | val_0_rmse: 1.20769 | val_1_rmse: 1.21409 |  0:00:25s
epoch 121| loss: 0.72625 | val_0_rmse: 0.98974 | val_1_rmse: 0.96753 |  0:00:25s
epoch 122| loss: 0.71241 | val_0_rmse: 1.03861 | val_1_rmse: 1.08067 |  0:00:26s
epoch 123| loss: 0.73847 | val_0_rmse: 1.07087 | val_1_rmse: 1.08957 |  0:00:26s
epoch 124| loss: 0.73395 | val_0_rmse: 1.1027  | val_1_rmse: 1.1379  |  0:00:26s
epoch 125| loss: 0.72253 | val_0_rmse: 0.98848 | val_1_rmse: 1.09248 |  0:00:26s
epoch 126| loss: 0.71985 | val_0_rmse: 1.00874 | val_1_rmse: 1.13146 |  0:00:26s
epoch 127| loss: 0.7143  | val_0_rmse: 1.03062 | val_1_rmse: 1.15309 |  0:00:27s
epoch 128| loss: 0.71581 | val_0_rmse: 1.02654 | val_1_rmse: 1.30105 |  0:00:27s
epoch 129| loss: 0.70616 | val_0_rmse: 1.04191 | val_1_rmse: 1.68615 |  0:00:27s
epoch 130| loss: 0.73741 | val_0_rmse: 1.03792 | val_1_rmse: 1.76906 |  0:00:27s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.829
Best weights from best epoch are automatically used!
ended training at: 06:29:24
Feature importance:
Mean squared error is of 6683219898.916019
Mean absolute error:58716.07549424342
MAPE:0.5106432726527493
R2 score:0.11341480685913197
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:29:24
epoch 0  | loss: 4.4902  | val_0_rmse: 1.00963 | val_1_rmse: 0.98936 |  0:00:00s
epoch 1  | loss: 2.0054  | val_0_rmse: 1.07441 | val_1_rmse: 1.05336 |  0:00:00s
epoch 2  | loss: 1.98425 | val_0_rmse: 1.01442 | val_1_rmse: 0.98949 |  0:00:00s
epoch 3  | loss: 1.71175 | val_0_rmse: 1.03972 | val_1_rmse: 1.01076 |  0:00:00s
epoch 4  | loss: 1.21464 | val_0_rmse: 1.32622 | val_1_rmse: 1.24295 |  0:00:01s
epoch 5  | loss: 1.04189 | val_0_rmse: 1.00566 | val_1_rmse: 0.98702 |  0:00:01s
epoch 6  | loss: 1.0166  | val_0_rmse: 0.99665 | val_1_rmse: 0.98001 |  0:00:01s
epoch 7  | loss: 1.01229 | val_0_rmse: 0.99178 | val_1_rmse: 0.97366 |  0:00:01s
epoch 8  | loss: 0.95774 | val_0_rmse: 1.07456 | val_1_rmse: 1.0521  |  0:00:01s
epoch 9  | loss: 0.93559 | val_0_rmse: 1.48924 | val_1_rmse: 1.46508 |  0:00:02s
epoch 10 | loss: 0.87769 | val_0_rmse: 2.41893 | val_1_rmse: 2.44856 |  0:00:02s
epoch 11 | loss: 0.86113 | val_0_rmse: 4.3531  | val_1_rmse: 4.38371 |  0:00:02s
epoch 12 | loss: 0.95882 | val_0_rmse: 3.12088 | val_1_rmse: 3.16172 |  0:00:02s
epoch 13 | loss: 0.93666 | val_0_rmse: 2.72841 | val_1_rmse: 2.75649 |  0:00:02s
epoch 14 | loss: 1.00335 | val_0_rmse: 1.29799 | val_1_rmse: 1.28017 |  0:00:03s
epoch 15 | loss: 0.90112 | val_0_rmse: 1.00409 | val_1_rmse: 0.97822 |  0:00:03s
epoch 16 | loss: 0.86934 | val_0_rmse: 0.98406 | val_1_rmse: 0.96167 |  0:00:03s
epoch 17 | loss: 0.85561 | val_0_rmse: 1.00315 | val_1_rmse: 0.98332 |  0:00:03s
epoch 18 | loss: 0.8407  | val_0_rmse: 1.00169 | val_1_rmse: 0.9761  |  0:00:04s
epoch 19 | loss: 0.80595 | val_0_rmse: 1.00372 | val_1_rmse: 0.98869 |  0:00:04s
epoch 20 | loss: 0.84114 | val_0_rmse: 0.98394 | val_1_rmse: 0.96679 |  0:00:04s
epoch 21 | loss: 0.85337 | val_0_rmse: 0.97992 | val_1_rmse: 0.96119 |  0:00:04s
epoch 22 | loss: 0.82813 | val_0_rmse: 0.91437 | val_1_rmse: 0.88918 |  0:00:04s
epoch 23 | loss: 0.78837 | val_0_rmse: 1.19358 | val_1_rmse: 1.17924 |  0:00:05s
epoch 24 | loss: 0.75314 | val_0_rmse: 1.04712 | val_1_rmse: 0.99821 |  0:00:05s
epoch 25 | loss: 0.76317 | val_0_rmse: 1.02956 | val_1_rmse: 0.97798 |  0:00:05s
epoch 26 | loss: 0.75652 | val_0_rmse: 1.58582 | val_1_rmse: 1.56147 |  0:00:05s
epoch 27 | loss: 0.74042 | val_0_rmse: 0.93755 | val_1_rmse: 0.89648 |  0:00:05s
epoch 28 | loss: 0.73348 | val_0_rmse: 1.05919 | val_1_rmse: 1.02199 |  0:00:06s
epoch 29 | loss: 0.71632 | val_0_rmse: 1.30428 | val_1_rmse: 1.27349 |  0:00:06s
epoch 30 | loss: 0.69342 | val_0_rmse: 1.31739 | val_1_rmse: 1.29063 |  0:00:06s
epoch 31 | loss: 0.73689 | val_0_rmse: 1.2148  | val_1_rmse: 1.19581 |  0:00:06s
epoch 32 | loss: 0.72669 | val_0_rmse: 1.24594 | val_1_rmse: 1.20888 |  0:00:06s
epoch 33 | loss: 0.73755 | val_0_rmse: 0.93865 | val_1_rmse: 0.92069 |  0:00:07s
epoch 34 | loss: 0.71117 | val_0_rmse: 1.13831 | val_1_rmse: 1.10852 |  0:00:07s
epoch 35 | loss: 0.72627 | val_0_rmse: 1.06368 | val_1_rmse: 1.0642  |  0:00:07s
epoch 36 | loss: 0.71342 | val_0_rmse: 1.07196 | val_1_rmse: 1.04331 |  0:00:07s
epoch 37 | loss: 0.75867 | val_0_rmse: 1.0412  | val_1_rmse: 1.02387 |  0:00:07s
epoch 38 | loss: 0.71393 | val_0_rmse: 1.05973 | val_1_rmse: 1.04267 |  0:00:08s
epoch 39 | loss: 0.70035 | val_0_rmse: 1.05566 | val_1_rmse: 1.03844 |  0:00:08s
epoch 40 | loss: 0.72497 | val_0_rmse: 1.04769 | val_1_rmse: 1.02844 |  0:00:08s
epoch 41 | loss: 0.69301 | val_0_rmse: 1.04977 | val_1_rmse: 1.03171 |  0:00:08s
epoch 42 | loss: 0.69421 | val_0_rmse: 1.04647 | val_1_rmse: 1.0349  |  0:00:09s
epoch 43 | loss: 0.714   | val_0_rmse: 1.13925 | val_1_rmse: 1.11201 |  0:00:09s
epoch 44 | loss: 0.70067 | val_0_rmse: 1.21394 | val_1_rmse: 1.19601 |  0:00:09s
epoch 45 | loss: 0.71037 | val_0_rmse: 1.13323 | val_1_rmse: 1.08148 |  0:00:09s
epoch 46 | loss: 0.68581 | val_0_rmse: 1.00772 | val_1_rmse: 0.99568 |  0:00:09s
epoch 47 | loss: 0.71471 | val_0_rmse: 1.0032  | val_1_rmse: 0.98686 |  0:00:10s
epoch 48 | loss: 0.69155 | val_0_rmse: 1.00318 | val_1_rmse: 0.98566 |  0:00:10s
epoch 49 | loss: 0.69737 | val_0_rmse: 1.01648 | val_1_rmse: 0.99766 |  0:00:10s
epoch 50 | loss: 0.70577 | val_0_rmse: 1.02534 | val_1_rmse: 1.00678 |  0:00:10s
epoch 51 | loss: 0.71112 | val_0_rmse: 0.9972  | val_1_rmse: 0.97567 |  0:00:10s
epoch 52 | loss: 0.70663 | val_0_rmse: 1.00753 | val_1_rmse: 0.98564 |  0:00:11s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.88918
Best weights from best epoch are automatically used!
ended training at: 06:29:35
Feature importance:
Mean squared error is of 6216252294.427332
Mean absolute error:62042.26186526864
MAPE:0.6019303874105111
R2 score:0.14942926491535036
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:29:35
epoch 0  | loss: 3.8549  | val_0_rmse: 1.03585 | val_1_rmse: 1.01944 |  0:00:00s
epoch 1  | loss: 2.03088 | val_0_rmse: 1.07466 | val_1_rmse: 1.06786 |  0:00:00s
epoch 2  | loss: 1.8765  | val_0_rmse: 1.07341 | val_1_rmse: 1.0642  |  0:00:00s
epoch 3  | loss: 1.58378 | val_0_rmse: 1.00544 | val_1_rmse: 0.98742 |  0:00:00s
epoch 4  | loss: 1.2858  | val_0_rmse: 1.03395 | val_1_rmse: 1.01884 |  0:00:01s
epoch 5  | loss: 1.2165  | val_0_rmse: 1.24988 | val_1_rmse: 1.22232 |  0:00:01s
epoch 6  | loss: 1.16055 | val_0_rmse: 1.25142 | val_1_rmse: 1.19668 |  0:00:01s
epoch 7  | loss: 1.0737  | val_0_rmse: 1.1056  | val_1_rmse: 1.05587 |  0:00:01s
epoch 8  | loss: 1.06314 | val_0_rmse: 1.05254 | val_1_rmse: 1.01689 |  0:00:01s
epoch 9  | loss: 1.02012 | val_0_rmse: 1.02018 | val_1_rmse: 0.99655 |  0:00:02s
epoch 10 | loss: 0.99327 | val_0_rmse: 1.00041 | val_1_rmse: 0.98092 |  0:00:02s
epoch 11 | loss: 0.98203 | val_0_rmse: 0.9969  | val_1_rmse: 0.97899 |  0:00:02s
epoch 12 | loss: 0.98567 | val_0_rmse: 1.00147 | val_1_rmse: 0.98365 |  0:00:02s
epoch 13 | loss: 0.99321 | val_0_rmse: 1.00742 | val_1_rmse: 0.98524 |  0:00:02s
epoch 14 | loss: 0.97485 | val_0_rmse: 1.0081  | val_1_rmse: 0.98525 |  0:00:03s
epoch 15 | loss: 0.96881 | val_0_rmse: 1.03436 | val_1_rmse: 1.01125 |  0:00:03s
epoch 16 | loss: 0.94316 | val_0_rmse: 1.05583 | val_1_rmse: 1.03201 |  0:00:03s
epoch 17 | loss: 0.92731 | val_0_rmse: 1.00501 | val_1_rmse: 0.99095 |  0:00:03s
epoch 18 | loss: 0.9111  | val_0_rmse: 1.04695 | val_1_rmse: 1.03244 |  0:00:04s
epoch 19 | loss: 0.91056 | val_0_rmse: 1.06488 | val_1_rmse: 1.14469 |  0:00:04s
epoch 20 | loss: 0.91201 | val_0_rmse: 1.07007 | val_1_rmse: 1.15785 |  0:00:04s
epoch 21 | loss: 0.89738 | val_0_rmse: 1.04526 | val_1_rmse: 1.14054 |  0:00:04s
epoch 22 | loss: 0.85517 | val_0_rmse: 1.02663 | val_1_rmse: 1.08667 |  0:00:04s
epoch 23 | loss: 0.87067 | val_0_rmse: 1.0097  | val_1_rmse: 1.06017 |  0:00:05s
epoch 24 | loss: 0.89877 | val_0_rmse: 0.99921 | val_1_rmse: 0.99957 |  0:00:05s
epoch 25 | loss: 0.85404 | val_0_rmse: 0.99456 | val_1_rmse: 0.98072 |  0:00:05s
epoch 26 | loss: 0.86573 | val_0_rmse: 0.99551 | val_1_rmse: 0.98309 |  0:00:05s
epoch 27 | loss: 0.84552 | val_0_rmse: 1.01426 | val_1_rmse: 1.00793 |  0:00:05s
epoch 28 | loss: 0.81014 | val_0_rmse: 1.03152 | val_1_rmse: 1.02659 |  0:00:06s
epoch 29 | loss: 0.7835  | val_0_rmse: 1.01273 | val_1_rmse: 0.99997 |  0:00:06s
epoch 30 | loss: 0.80637 | val_0_rmse: 1.01161 | val_1_rmse: 0.99667 |  0:00:06s
epoch 31 | loss: 0.80824 | val_0_rmse: 0.99616 | val_1_rmse: 0.9794  |  0:00:06s
epoch 32 | loss: 0.76183 | val_0_rmse: 0.99162 | val_1_rmse: 0.97386 |  0:00:06s
epoch 33 | loss: 0.79605 | val_0_rmse: 0.99376 | val_1_rmse: 0.97598 |  0:00:07s
epoch 34 | loss: 0.74412 | val_0_rmse: 0.99553 | val_1_rmse: 0.97782 |  0:00:07s
epoch 35 | loss: 0.74827 | val_0_rmse: 0.99768 | val_1_rmse: 0.9795  |  0:00:07s
epoch 36 | loss: 0.72969 | val_0_rmse: 0.99552 | val_1_rmse: 0.97767 |  0:00:07s
epoch 37 | loss: 0.72106 | val_0_rmse: 0.99711 | val_1_rmse: 0.98096 |  0:00:08s
epoch 38 | loss: 0.76816 | val_0_rmse: 1.00184 | val_1_rmse: 0.98478 |  0:00:08s
epoch 39 | loss: 0.77503 | val_0_rmse: 0.99376 | val_1_rmse: 0.97876 |  0:00:08s
epoch 40 | loss: 0.74379 | val_0_rmse: 0.99223 | val_1_rmse: 0.97643 |  0:00:08s
epoch 41 | loss: 0.74691 | val_0_rmse: 0.9925  | val_1_rmse: 0.97405 |  0:00:08s
epoch 42 | loss: 0.72891 | val_0_rmse: 0.99226 | val_1_rmse: 0.97393 |  0:00:09s
epoch 43 | loss: 0.71433 | val_0_rmse: 0.99126 | val_1_rmse: 0.97299 |  0:00:09s
epoch 44 | loss: 0.71035 | val_0_rmse: 0.99041 | val_1_rmse: 0.97241 |  0:00:09s
epoch 45 | loss: 0.72226 | val_0_rmse: 0.99011 | val_1_rmse: 0.97106 |  0:00:09s
epoch 46 | loss: 0.71358 | val_0_rmse: 0.98839 | val_1_rmse: 0.96993 |  0:00:09s
epoch 47 | loss: 0.71447 | val_0_rmse: 0.9877  | val_1_rmse: 0.96835 |  0:00:10s
epoch 48 | loss: 0.7354  | val_0_rmse: 0.98408 | val_1_rmse: 0.96576 |  0:00:10s
epoch 49 | loss: 0.71925 | val_0_rmse: 0.98536 | val_1_rmse: 0.96682 |  0:00:10s
epoch 50 | loss: 0.72624 | val_0_rmse: 0.98452 | val_1_rmse: 0.96588 |  0:00:10s
epoch 51 | loss: 0.71444 | val_0_rmse: 0.98236 | val_1_rmse: 0.96362 |  0:00:11s
epoch 52 | loss: 0.71152 | val_0_rmse: 0.98101 | val_1_rmse: 0.96184 |  0:00:11s
epoch 53 | loss: 0.71829 | val_0_rmse: 0.98156 | val_1_rmse: 0.96208 |  0:00:11s
epoch 54 | loss: 0.72363 | val_0_rmse: 0.98433 | val_1_rmse: 0.96534 |  0:00:11s
epoch 55 | loss: 0.73252 | val_0_rmse: 0.9831  | val_1_rmse: 0.96303 |  0:00:11s
epoch 56 | loss: 0.71445 | val_0_rmse: 0.97944 | val_1_rmse: 0.95789 |  0:00:12s
epoch 57 | loss: 0.72599 | val_0_rmse: 0.97699 | val_1_rmse: 0.95507 |  0:00:12s
epoch 58 | loss: 0.72841 | val_0_rmse: 0.97657 | val_1_rmse: 0.95457 |  0:00:12s
epoch 59 | loss: 0.69561 | val_0_rmse: 0.97688 | val_1_rmse: 0.95427 |  0:00:12s
epoch 60 | loss: 0.70999 | val_0_rmse: 0.98356 | val_1_rmse: 0.96103 |  0:00:13s
epoch 61 | loss: 0.69234 | val_0_rmse: 0.98466 | val_1_rmse: 0.96221 |  0:00:13s
epoch 62 | loss: 0.70365 | val_0_rmse: 0.97674 | val_1_rmse: 0.95356 |  0:00:13s
epoch 63 | loss: 0.73267 | val_0_rmse: 0.97414 | val_1_rmse: 0.95216 |  0:00:13s
epoch 64 | loss: 0.70854 | val_0_rmse: 0.9717  | val_1_rmse: 0.95039 |  0:00:13s
epoch 65 | loss: 0.73513 | val_0_rmse: 0.97172 | val_1_rmse: 0.95072 |  0:00:14s
epoch 66 | loss: 0.69857 | val_0_rmse: 0.97285 | val_1_rmse: 0.9513  |  0:00:14s
epoch 67 | loss: 0.71896 | val_0_rmse: 0.97332 | val_1_rmse: 0.95163 |  0:00:14s
epoch 68 | loss: 0.7112  | val_0_rmse: 0.97266 | val_1_rmse: 0.95051 |  0:00:14s
epoch 69 | loss: 0.69759 | val_0_rmse: 0.96742 | val_1_rmse: 0.94614 |  0:00:14s
epoch 70 | loss: 0.72234 | val_0_rmse: 0.96729 | val_1_rmse: 0.94608 |  0:00:15s
epoch 71 | loss: 0.71989 | val_0_rmse: 0.97371 | val_1_rmse: 0.95235 |  0:00:15s
epoch 72 | loss: 0.70853 | val_0_rmse: 0.96916 | val_1_rmse: 0.94902 |  0:00:15s
epoch 73 | loss: 0.71228 | val_0_rmse: 0.96641 | val_1_rmse: 0.945   |  0:00:15s
epoch 74 | loss: 0.70432 | val_0_rmse: 0.96955 | val_1_rmse: 0.94678 |  0:00:15s
epoch 75 | loss: 0.70405 | val_0_rmse: 0.96916 | val_1_rmse: 0.9449  |  0:00:16s
epoch 76 | loss: 0.70825 | val_0_rmse: 0.96726 | val_1_rmse: 0.94209 |  0:00:16s
epoch 77 | loss: 0.6998  | val_0_rmse: 0.96794 | val_1_rmse: 0.94269 |  0:00:16s
epoch 78 | loss: 0.71883 | val_0_rmse: 0.96916 | val_1_rmse: 0.9442  |  0:00:16s
epoch 79 | loss: 0.70134 | val_0_rmse: 0.96928 | val_1_rmse: 0.94481 |  0:00:17s
epoch 80 | loss: 0.70131 | val_0_rmse: 0.96949 | val_1_rmse: 0.94537 |  0:00:17s
epoch 81 | loss: 0.71067 | val_0_rmse: 0.96968 | val_1_rmse: 0.94634 |  0:00:17s
epoch 82 | loss: 0.68957 | val_0_rmse: 0.96894 | val_1_rmse: 0.9458  |  0:00:17s
epoch 83 | loss: 0.70667 | val_0_rmse: 0.96956 | val_1_rmse: 0.94659 |  0:00:17s
epoch 84 | loss: 0.69016 | val_0_rmse: 0.96848 | val_1_rmse: 0.94579 |  0:00:18s
epoch 85 | loss: 0.69051 | val_0_rmse: 0.9664  | val_1_rmse: 0.94436 |  0:00:18s
epoch 86 | loss: 0.68955 | val_0_rmse: 0.96509 | val_1_rmse: 0.94453 |  0:00:18s
epoch 87 | loss: 0.68784 | val_0_rmse: 0.96452 | val_1_rmse: 0.94188 |  0:00:18s
epoch 88 | loss: 0.71359 | val_0_rmse: 0.96435 | val_1_rmse: 0.94549 |  0:00:19s
epoch 89 | loss: 0.68894 | val_0_rmse: 0.96394 | val_1_rmse: 0.95101 |  0:00:19s
epoch 90 | loss: 0.6917  | val_0_rmse: 0.97017 | val_1_rmse: 0.96264 |  0:00:19s
epoch 91 | loss: 0.69592 | val_0_rmse: 0.97624 | val_1_rmse: 0.97061 |  0:00:19s
epoch 92 | loss: 0.7063  | val_0_rmse: 0.97331 | val_1_rmse: 0.97159 |  0:00:19s
epoch 93 | loss: 0.71422 | val_0_rmse: 0.97046 | val_1_rmse: 0.96951 |  0:00:20s
epoch 94 | loss: 0.70038 | val_0_rmse: 0.97722 | val_1_rmse: 0.97204 |  0:00:20s
epoch 95 | loss: 0.68045 | val_0_rmse: 0.99608 | val_1_rmse: 0.97735 |  0:00:20s
epoch 96 | loss: 0.70702 | val_0_rmse: 1.00668 | val_1_rmse: 0.9881  |  0:00:20s
epoch 97 | loss: 0.69662 | val_0_rmse: 0.99718 | val_1_rmse: 0.98196 |  0:00:20s
epoch 98 | loss: 0.71391 | val_0_rmse: 0.98407 | val_1_rmse: 0.97802 |  0:00:21s
epoch 99 | loss: 0.70087 | val_0_rmse: 0.97717 | val_1_rmse: 0.97029 |  0:00:21s
epoch 100| loss: 0.69215 | val_0_rmse: 0.97842 | val_1_rmse: 0.97447 |  0:00:21s
epoch 101| loss: 0.69679 | val_0_rmse: 0.99734 | val_1_rmse: 0.99436 |  0:00:21s
epoch 102| loss: 0.70137 | val_0_rmse: 1.00467 | val_1_rmse: 0.99977 |  0:00:21s
epoch 103| loss: 0.72812 | val_0_rmse: 1.00916 | val_1_rmse: 0.9912  |  0:00:22s
epoch 104| loss: 0.71158 | val_0_rmse: 1.02292 | val_1_rmse: 1.01427 |  0:00:22s
epoch 105| loss: 0.70505 | val_0_rmse: 1.02306 | val_1_rmse: 1.00792 |  0:00:22s
epoch 106| loss: 0.69427 | val_0_rmse: 1.03669 | val_1_rmse: 1.01827 |  0:00:22s
epoch 107| loss: 0.70522 | val_0_rmse: 1.07202 | val_1_rmse: 1.02822 |  0:00:22s
epoch 108| loss: 0.70384 | val_0_rmse: 1.09009 | val_1_rmse: 1.04131 |  0:00:23s
epoch 109| loss: 0.6841  | val_0_rmse: 1.07754 | val_1_rmse: 1.04754 |  0:00:23s
epoch 110| loss: 0.69674 | val_0_rmse: 1.06692 | val_1_rmse: 1.04667 |  0:00:23s
epoch 111| loss: 0.68856 | val_0_rmse: 1.04992 | val_1_rmse: 1.03307 |  0:00:23s
epoch 112| loss: 0.68957 | val_0_rmse: 1.03173 | val_1_rmse: 1.01907 |  0:00:23s
epoch 113| loss: 0.69973 | val_0_rmse: 1.01246 | val_1_rmse: 0.99273 |  0:00:24s
epoch 114| loss: 0.69622 | val_0_rmse: 1.01398 | val_1_rmse: 0.99408 |  0:00:24s
epoch 115| loss: 0.68895 | val_0_rmse: 1.01729 | val_1_rmse: 0.99816 |  0:00:24s
epoch 116| loss: 0.70178 | val_0_rmse: 1.019   | val_1_rmse: 0.99821 |  0:00:24s
epoch 117| loss: 0.68677 | val_0_rmse: 1.01591 | val_1_rmse: 1.0085  |  0:00:25s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.94188
Best weights from best epoch are automatically used!
ended training at: 06:30:00
Feature importance:
Mean squared error is of 6531411841.735838
Mean absolute error:66502.72328138707
MAPE:0.7199696491472011
R2 score:0.04895632178111775
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:30:00
epoch 0  | loss: 4.88757 | val_0_rmse: 1.00591 | val_1_rmse: 1.08552 |  0:00:00s
epoch 1  | loss: 2.30377 | val_0_rmse: 1.02096 | val_1_rmse: 1.09619 |  0:00:00s
epoch 2  | loss: 1.51858 | val_0_rmse: 1.00214 | val_1_rmse: 1.07644 |  0:00:00s
epoch 3  | loss: 1.32675 | val_0_rmse: 1.01249 | val_1_rmse: 1.08566 |  0:00:00s
epoch 4  | loss: 1.24405 | val_0_rmse: 1.01146 | val_1_rmse: 1.0819  |  0:00:01s
epoch 5  | loss: 1.12361 | val_0_rmse: 1.00315 | val_1_rmse: 1.06384 |  0:00:01s
epoch 6  | loss: 1.05621 | val_0_rmse: 1.00408 | val_1_rmse: 1.07181 |  0:00:01s
epoch 7  | loss: 1.02199 | val_0_rmse: 1.0866  | val_1_rmse: 1.17071 |  0:00:01s
epoch 8  | loss: 0.99487 | val_0_rmse: 1.06653 | val_1_rmse: 1.14538 |  0:00:01s
epoch 9  | loss: 0.97652 | val_0_rmse: 0.99937 | val_1_rmse: 1.06133 |  0:00:02s
epoch 10 | loss: 0.95643 | val_0_rmse: 0.9792  | val_1_rmse: 1.03936 |  0:00:02s
epoch 11 | loss: 0.94544 | val_0_rmse: 0.97836 | val_1_rmse: 1.03841 |  0:00:02s
epoch 12 | loss: 0.929   | val_0_rmse: 0.9782  | val_1_rmse: 1.0403  |  0:00:02s
epoch 13 | loss: 0.90901 | val_0_rmse: 1.0122  | val_1_rmse: 1.08209 |  0:00:03s
epoch 14 | loss: 0.88459 | val_0_rmse: 1.10049 | val_1_rmse: 1.17424 |  0:00:03s
epoch 15 | loss: 0.84702 | val_0_rmse: 1.11339 | val_1_rmse: 1.1972  |  0:00:03s
epoch 16 | loss: 0.82885 | val_0_rmse: 1.00417 | val_1_rmse: 1.0891  |  0:00:03s
epoch 17 | loss: 0.84352 | val_0_rmse: 1.03464 | val_1_rmse: 1.1222  |  0:00:03s
epoch 18 | loss: 0.81285 | val_0_rmse: 1.20219 | val_1_rmse: 1.29347 |  0:00:04s
epoch 19 | loss: 0.80668 | val_0_rmse: 1.26064 | val_1_rmse: 1.34786 |  0:00:04s
epoch 20 | loss: 0.80983 | val_0_rmse: 1.18326 | val_1_rmse: 1.27052 |  0:00:04s
epoch 21 | loss: 0.76447 | val_0_rmse: 1.11392 | val_1_rmse: 1.20126 |  0:00:04s
epoch 22 | loss: 0.70923 | val_0_rmse: 1.6141  | val_1_rmse: 1.68443 |  0:00:04s
epoch 23 | loss: 0.71795 | val_0_rmse: 1.57974 | val_1_rmse: 1.65066 |  0:00:05s
epoch 24 | loss: 0.7144  | val_0_rmse: 1.32521 | val_1_rmse: 1.39974 |  0:00:05s
epoch 25 | loss: 0.6881  | val_0_rmse: 1.26589 | val_1_rmse: 1.34129 |  0:00:05s
epoch 26 | loss: 0.71534 | val_0_rmse: 1.14231 | val_1_rmse: 1.21037 |  0:00:05s
epoch 27 | loss: 0.69474 | val_0_rmse: 1.00541 | val_1_rmse: 1.05496 |  0:00:05s
epoch 28 | loss: 0.72581 | val_0_rmse: 0.9721  | val_1_rmse: 1.0403  |  0:00:06s
epoch 29 | loss: 0.70062 | val_0_rmse: 0.98792 | val_1_rmse: 1.03591 |  0:00:06s
epoch 30 | loss: 0.66148 | val_0_rmse: 1.03987 | val_1_rmse: 1.07748 |  0:00:06s
epoch 31 | loss: 0.67699 | val_0_rmse: 0.90309 | val_1_rmse: 0.96862 |  0:00:06s
epoch 32 | loss: 0.66659 | val_0_rmse: 1.15217 | val_1_rmse: 1.24569 |  0:00:06s
epoch 33 | loss: 0.6673  | val_0_rmse: 1.14096 | val_1_rmse: 1.23269 |  0:00:07s
epoch 34 | loss: 0.67682 | val_0_rmse: 1.18131 | val_1_rmse: 1.26345 |  0:00:07s
epoch 35 | loss: 0.64785 | val_0_rmse: 1.23227 | val_1_rmse: 1.3184  |  0:00:07s
epoch 36 | loss: 0.65892 | val_0_rmse: 1.15374 | val_1_rmse: 1.24362 |  0:00:07s
epoch 37 | loss: 0.65306 | val_0_rmse: 1.13708 | val_1_rmse: 1.22831 |  0:00:08s
epoch 38 | loss: 0.67795 | val_0_rmse: 1.17629 | val_1_rmse: 1.25414 |  0:00:08s
epoch 39 | loss: 0.6729  | val_0_rmse: 1.09955 | val_1_rmse: 1.18567 |  0:00:08s
epoch 40 | loss: 0.66401 | val_0_rmse: 1.03567 | val_1_rmse: 1.09638 |  0:00:08s
epoch 41 | loss: 0.67296 | val_0_rmse: 0.96967 | val_1_rmse: 1.03557 |  0:00:08s
epoch 42 | loss: 0.6542  | val_0_rmse: 1.00309 | val_1_rmse: 1.03744 |  0:00:09s
epoch 43 | loss: 0.65163 | val_0_rmse: 1.03305 | val_1_rmse: 1.06337 |  0:00:09s
epoch 44 | loss: 0.66196 | val_0_rmse: 1.0136  | val_1_rmse: 1.0471  |  0:00:09s
epoch 45 | loss: 0.64915 | val_0_rmse: 0.92783 | val_1_rmse: 0.97252 |  0:00:09s
epoch 46 | loss: 0.66205 | val_0_rmse: 0.89981 | val_1_rmse: 0.96449 |  0:00:09s
epoch 47 | loss: 0.67782 | val_0_rmse: 1.06728 | val_1_rmse: 1.09684 |  0:00:10s
epoch 48 | loss: 0.68648 | val_0_rmse: 1.07806 | val_1_rmse: 1.11669 |  0:00:10s
epoch 49 | loss: 0.66267 | val_0_rmse: 1.0447  | val_1_rmse: 1.08415 |  0:00:10s
epoch 50 | loss: 0.66435 | val_0_rmse: 0.97292 | val_1_rmse: 1.02323 |  0:00:10s
epoch 51 | loss: 0.66497 | val_0_rmse: 0.98313 | val_1_rmse: 1.0532  |  0:00:11s
epoch 52 | loss: 0.66042 | val_0_rmse: 0.95258 | val_1_rmse: 1.02167 |  0:00:11s
epoch 53 | loss: 0.65929 | val_0_rmse: 0.95024 | val_1_rmse: 1.01387 |  0:00:11s
epoch 54 | loss: 0.67666 | val_0_rmse: 0.96737 | val_1_rmse: 1.03964 |  0:00:11s
epoch 55 | loss: 0.70652 | val_0_rmse: 0.95086 | val_1_rmse: 1.01358 |  0:00:11s
epoch 56 | loss: 0.6616  | val_0_rmse: 0.96765 | val_1_rmse: 1.01414 |  0:00:12s
epoch 57 | loss: 0.67852 | val_0_rmse: 0.97147 | val_1_rmse: 1.0166  |  0:00:12s
epoch 58 | loss: 0.67287 | val_0_rmse: 0.95082 | val_1_rmse: 1.0077  |  0:00:12s
epoch 59 | loss: 0.65514 | val_0_rmse: 0.95187 | val_1_rmse: 1.02205 |  0:00:12s
epoch 60 | loss: 0.64719 | val_0_rmse: 0.94589 | val_1_rmse: 1.01546 |  0:00:12s
epoch 61 | loss: 0.66074 | val_0_rmse: 0.94396 | val_1_rmse: 1.01386 |  0:00:13s
epoch 62 | loss: 0.6631  | val_0_rmse: 0.95106 | val_1_rmse: 1.02606 |  0:00:13s
epoch 63 | loss: 0.65827 | val_0_rmse: 0.93266 | val_1_rmse: 0.99724 |  0:00:13s
epoch 64 | loss: 0.66632 | val_0_rmse: 0.92969 | val_1_rmse: 0.99945 |  0:00:13s
epoch 65 | loss: 0.65851 | val_0_rmse: 0.95623 | val_1_rmse: 1.02753 |  0:00:13s
epoch 66 | loss: 0.65873 | val_0_rmse: 0.96321 | val_1_rmse: 1.03492 |  0:00:14s
epoch 67 | loss: 0.67645 | val_0_rmse: 0.9564  | val_1_rmse: 1.02714 |  0:00:14s
epoch 68 | loss: 0.65572 | val_0_rmse: 0.97265 | val_1_rmse: 1.04609 |  0:00:14s
epoch 69 | loss: 0.641   | val_0_rmse: 1.02802 | val_1_rmse: 1.10592 |  0:00:14s
epoch 70 | loss: 0.67247 | val_0_rmse: 1.02528 | val_1_rmse: 1.0972  |  0:00:15s
epoch 71 | loss: 0.66212 | val_0_rmse: 0.98114 | val_1_rmse: 1.04928 |  0:00:15s
epoch 72 | loss: 0.64139 | val_0_rmse: 0.94835 | val_1_rmse: 1.01144 |  0:00:15s
epoch 73 | loss: 0.65    | val_0_rmse: 0.94566 | val_1_rmse: 1.00495 |  0:00:15s
epoch 74 | loss: 0.65215 | val_0_rmse: 0.94268 | val_1_rmse: 1.00421 |  0:00:15s
epoch 75 | loss: 0.65129 | val_0_rmse: 0.95609 | val_1_rmse: 1.0279  |  0:00:16s
epoch 76 | loss: 0.64186 | val_0_rmse: 0.96643 | val_1_rmse: 1.03881 |  0:00:16s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.96449
Best weights from best epoch are automatically used!
ended training at: 06:30:17
Feature importance:
Mean squared error is of 6509753092.947609
Mean absolute error:63479.46833456688
MAPE:0.7061672360465736
R2 score:0.12096770336748142
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:30:17
epoch 0  | loss: 4.52691 | val_0_rmse: 1.17858 | val_1_rmse: 1.00218 |  0:00:00s
epoch 1  | loss: 3.1727  | val_0_rmse: 0.99981 | val_1_rmse: 0.97598 |  0:00:00s
epoch 2  | loss: 2.41608 | val_0_rmse: 1.10661 | val_1_rmse: 1.10152 |  0:00:00s
epoch 3  | loss: 1.36121 | val_0_rmse: 1.15643 | val_1_rmse: 1.15891 |  0:00:00s
epoch 4  | loss: 1.0684  | val_0_rmse: 1.045   | val_1_rmse: 1.03156 |  0:00:01s
epoch 5  | loss: 1.0233  | val_0_rmse: 0.97878 | val_1_rmse: 0.95261 |  0:00:01s
epoch 6  | loss: 1.01037 | val_0_rmse: 0.97624 | val_1_rmse: 0.94859 |  0:00:01s
epoch 7  | loss: 0.96922 | val_0_rmse: 0.98191 | val_1_rmse: 0.95764 |  0:00:01s
epoch 8  | loss: 0.99087 | val_0_rmse: 0.97487 | val_1_rmse: 0.9461  |  0:00:01s
epoch 9  | loss: 0.96953 | val_0_rmse: 0.97636 | val_1_rmse: 0.94898 |  0:00:02s
epoch 10 | loss: 0.97281 | val_0_rmse: 0.97946 | val_1_rmse: 0.95472 |  0:00:02s
epoch 11 | loss: 0.97397 | val_0_rmse: 0.97887 | val_1_rmse: 0.9534  |  0:00:02s
epoch 12 | loss: 0.96352 | val_0_rmse: 0.97686 | val_1_rmse: 0.94978 |  0:00:02s
epoch 13 | loss: 0.95791 | val_0_rmse: 0.97838 | val_1_rmse: 0.95374 |  0:00:02s
epoch 14 | loss: 0.95791 | val_0_rmse: 0.98128 | val_1_rmse: 0.95842 |  0:00:03s
epoch 15 | loss: 0.96557 | val_0_rmse: 0.97771 | val_1_rmse: 0.95152 |  0:00:03s
epoch 16 | loss: 0.95538 | val_0_rmse: 0.98223 | val_1_rmse: 0.94886 |  0:00:03s
epoch 17 | loss: 0.95571 | val_0_rmse: 0.98914 | val_1_rmse: 0.94823 |  0:00:03s
epoch 18 | loss: 0.95746 | val_0_rmse: 0.98096 | val_1_rmse: 0.94646 |  0:00:03s
epoch 19 | loss: 0.95615 | val_0_rmse: 0.97838 | val_1_rmse: 0.95204 |  0:00:04s
epoch 20 | loss: 0.95906 | val_0_rmse: 0.97942 | val_1_rmse: 0.95344 |  0:00:04s
epoch 21 | loss: 0.95059 | val_0_rmse: 0.97727 | val_1_rmse: 0.9481  |  0:00:04s
epoch 22 | loss: 0.94333 | val_0_rmse: 0.97706 | val_1_rmse: 0.94586 |  0:00:04s
epoch 23 | loss: 0.93042 | val_0_rmse: 0.98689 | val_1_rmse: 0.95533 |  0:00:05s
epoch 24 | loss: 0.90639 | val_0_rmse: 1.02297 | val_1_rmse: 0.99274 |  0:00:05s
epoch 25 | loss: 0.87537 | val_0_rmse: 1.17824 | val_1_rmse: 1.15639 |  0:00:05s
epoch 26 | loss: 0.83497 | val_0_rmse: 1.25198 | val_1_rmse: 1.23464 |  0:00:05s
epoch 27 | loss: 0.81985 | val_0_rmse: 1.29318 | val_1_rmse: 1.28201 |  0:00:05s
epoch 28 | loss: 0.79945 | val_0_rmse: 1.4138  | val_1_rmse: 1.40796 |  0:00:06s
epoch 29 | loss: 0.7867  | val_0_rmse: 1.44998 | val_1_rmse: 1.43427 |  0:00:06s
epoch 30 | loss: 0.77906 | val_0_rmse: 1.24907 | val_1_rmse: 1.22895 |  0:00:06s
epoch 31 | loss: 0.82465 | val_0_rmse: 1.37564 | val_1_rmse: 1.3583  |  0:00:06s
epoch 32 | loss: 0.77198 | val_0_rmse: 1.48007 | val_1_rmse: 1.47472 |  0:00:06s
epoch 33 | loss: 0.74818 | val_0_rmse: 1.11875 | val_1_rmse: 1.09546 |  0:00:07s
epoch 34 | loss: 0.74024 | val_0_rmse: 1.14089 | val_1_rmse: 1.08099 |  0:00:07s
epoch 35 | loss: 0.7372  | val_0_rmse: 1.63364 | val_1_rmse: 1.60291 |  0:00:07s
epoch 36 | loss: 0.70273 | val_0_rmse: 1.73214 | val_1_rmse: 1.71949 |  0:00:07s
epoch 37 | loss: 0.7062  | val_0_rmse: 1.18742 | val_1_rmse: 1.16217 |  0:00:07s
epoch 38 | loss: 0.69912 | val_0_rmse: 1.19012 | val_1_rmse: 1.14182 |  0:00:08s
epoch 39 | loss: 0.6764  | val_0_rmse: 1.23599 | val_1_rmse: 1.21261 |  0:00:08s
epoch 40 | loss: 0.68646 | val_0_rmse: 1.15734 | val_1_rmse: 1.13134 |  0:00:08s
epoch 41 | loss: 0.69993 | val_0_rmse: 1.21625 | val_1_rmse: 1.18973 |  0:00:08s
epoch 42 | loss: 0.70124 | val_0_rmse: 1.16079 | val_1_rmse: 1.12804 |  0:00:09s
epoch 43 | loss: 0.68945 | val_0_rmse: 1.11976 | val_1_rmse: 1.09576 |  0:00:09s
epoch 44 | loss: 0.69315 | val_0_rmse: 1.21505 | val_1_rmse: 1.1904  |  0:00:09s
epoch 45 | loss: 0.67586 | val_0_rmse: 1.12206 | val_1_rmse: 1.07897 |  0:00:09s
epoch 46 | loss: 0.66247 | val_0_rmse: 1.08901 | val_1_rmse: 1.05016 |  0:00:09s
epoch 47 | loss: 0.68971 | val_0_rmse: 1.18279 | val_1_rmse: 1.14854 |  0:00:10s
epoch 48 | loss: 0.70457 | val_0_rmse: 1.29556 | val_1_rmse: 1.28914 |  0:00:10s
epoch 49 | loss: 0.6654  | val_0_rmse: 1.18842 | val_1_rmse: 1.17607 |  0:00:10s
epoch 50 | loss: 0.68804 | val_0_rmse: 1.12072 | val_1_rmse: 1.10398 |  0:00:10s
epoch 51 | loss: 0.68743 | val_0_rmse: 1.18422 | val_1_rmse: 1.17256 |  0:00:10s
epoch 52 | loss: 0.66651 | val_0_rmse: 1.24873 | val_1_rmse: 1.23081 |  0:00:11s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.94586
Best weights from best epoch are automatically used!
ended training at: 06:30:28
Feature importance:
Mean squared error is of 7946130163.695811
Mean absolute error:69574.1518377193
MAPE:0.6383457169202412
R2 score:0.029433345333054084
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:30:28
epoch 0  | loss: 4.75411 | val_0_rmse: 1.01705 | val_1_rmse: 1.00575 |  0:00:00s
epoch 1  | loss: 3.14803 | val_0_rmse: 0.99826 | val_1_rmse: 0.99162 |  0:00:00s
epoch 2  | loss: 1.72584 | val_0_rmse: 1.02339 | val_1_rmse: 1.02217 |  0:00:00s
epoch 3  | loss: 1.3034  | val_0_rmse: 1.00913 | val_1_rmse: 1.00833 |  0:00:00s
epoch 4  | loss: 1.06915 | val_0_rmse: 1.00962 | val_1_rmse: 1.00599 |  0:00:01s
epoch 5  | loss: 1.07007 | val_0_rmse: 1.00394 | val_1_rmse: 0.9999  |  0:00:01s
epoch 6  | loss: 1.00503 | val_0_rmse: 1.0166  | val_1_rmse: 1.00251 |  0:00:01s
epoch 7  | loss: 1.00831 | val_0_rmse: 1.01336 | val_1_rmse: 1.00346 |  0:00:01s
epoch 8  | loss: 0.99337 | val_0_rmse: 1.00626 | val_1_rmse: 0.99916 |  0:00:01s
epoch 9  | loss: 0.94676 | val_0_rmse: 0.99702 | val_1_rmse: 0.98358 |  0:00:02s
epoch 10 | loss: 0.94641 | val_0_rmse: 1.01588 | val_1_rmse: 1.01033 |  0:00:02s
epoch 11 | loss: 0.93051 | val_0_rmse: 1.0453  | val_1_rmse: 1.04081 |  0:00:02s
epoch 12 | loss: 0.92527 | val_0_rmse: 1.03174 | val_1_rmse: 1.02747 |  0:00:02s
epoch 13 | loss: 0.91461 | val_0_rmse: 1.01182 | val_1_rmse: 1.00845 |  0:00:03s
epoch 14 | loss: 0.91059 | val_0_rmse: 0.99191 | val_1_rmse: 0.98469 |  0:00:03s
epoch 15 | loss: 0.89099 | val_0_rmse: 0.96244 | val_1_rmse: 0.96896 |  0:00:03s
epoch 16 | loss: 0.89296 | val_0_rmse: 1.06485 | val_1_rmse: 1.06367 |  0:00:03s
epoch 17 | loss: 0.89913 | val_0_rmse: 1.1073  | val_1_rmse: 1.1005  |  0:00:03s
epoch 18 | loss: 0.91325 | val_0_rmse: 1.13405 | val_1_rmse: 1.13042 |  0:00:04s
epoch 19 | loss: 0.90634 | val_0_rmse: 1.16068 | val_1_rmse: 1.1612  |  0:00:04s
epoch 20 | loss: 0.88796 | val_0_rmse: 1.03259 | val_1_rmse: 1.05161 |  0:00:04s
epoch 21 | loss: 0.88951 | val_0_rmse: 1.00884 | val_1_rmse: 1.03065 |  0:00:04s
epoch 22 | loss: 0.88976 | val_0_rmse: 1.02408 | val_1_rmse: 1.04084 |  0:00:04s
epoch 23 | loss: 0.89358 | val_0_rmse: 1.00845 | val_1_rmse: 1.03499 |  0:00:05s
epoch 24 | loss: 0.88925 | val_0_rmse: 1.01358 | val_1_rmse: 1.03941 |  0:00:05s
epoch 25 | loss: 0.89501 | val_0_rmse: 0.95476 | val_1_rmse: 0.98893 |  0:00:05s
epoch 26 | loss: 0.87422 | val_0_rmse: 0.95446 | val_1_rmse: 0.96041 |  0:00:05s
epoch 27 | loss: 0.8901  | val_0_rmse: 0.97001 | val_1_rmse: 0.99943 |  0:00:06s
epoch 28 | loss: 0.88904 | val_0_rmse: 0.94496 | val_1_rmse: 0.9746  |  0:00:06s
epoch 29 | loss: 0.89778 | val_0_rmse: 0.95048 | val_1_rmse: 0.98494 |  0:00:06s
epoch 30 | loss: 0.8893  | val_0_rmse: 0.9992  | val_1_rmse: 1.02123 |  0:00:06s
epoch 31 | loss: 0.88267 | val_0_rmse: 1.19265 | val_1_rmse: 1.19272 |  0:00:06s
epoch 32 | loss: 0.88377 | val_0_rmse: 1.30042 | val_1_rmse: 1.29879 |  0:00:07s
epoch 33 | loss: 0.87361 | val_0_rmse: 1.3748  | val_1_rmse: 1.37148 |  0:00:07s
epoch 34 | loss: 0.88338 | val_0_rmse: 1.39932 | val_1_rmse: 1.39407 |  0:00:07s
epoch 35 | loss: 0.88571 | val_0_rmse: 1.35345 | val_1_rmse: 1.34418 |  0:00:07s
epoch 36 | loss: 0.87797 | val_0_rmse: 1.334   | val_1_rmse: 1.32171 |  0:00:07s
epoch 37 | loss: 0.89202 | val_0_rmse: 1.28152 | val_1_rmse: 1.27036 |  0:00:08s
epoch 38 | loss: 0.86607 | val_0_rmse: 1.30246 | val_1_rmse: 1.29384 |  0:00:08s
epoch 39 | loss: 0.87477 | val_0_rmse: 1.37472 | val_1_rmse: 1.36933 |  0:00:08s
epoch 40 | loss: 0.8627  | val_0_rmse: 1.46122 | val_1_rmse: 1.45083 |  0:00:08s
epoch 41 | loss: 0.84593 | val_0_rmse: 1.40945 | val_1_rmse: 1.39901 |  0:00:08s
epoch 42 | loss: 0.84028 | val_0_rmse: 1.12341 | val_1_rmse: 1.12478 |  0:00:09s
epoch 43 | loss: 0.80789 | val_0_rmse: 1.04052 | val_1_rmse: 1.03992 |  0:00:09s
epoch 44 | loss: 0.82151 | val_0_rmse: 1.01285 | val_1_rmse: 1.01208 |  0:00:09s
epoch 45 | loss: 0.80543 | val_0_rmse: 1.04931 | val_1_rmse: 1.06153 |  0:00:09s
epoch 46 | loss: 0.77795 | val_0_rmse: 1.02759 | val_1_rmse: 1.03082 |  0:00:09s
epoch 47 | loss: 0.78329 | val_0_rmse: 1.03596 | val_1_rmse: 1.02189 |  0:00:10s
epoch 48 | loss: 0.80251 | val_0_rmse: 1.03015 | val_1_rmse: 1.02877 |  0:00:10s
epoch 49 | loss: 0.75424 | val_0_rmse: 1.04356 | val_1_rmse: 1.04029 |  0:00:10s
epoch 50 | loss: 0.77539 | val_0_rmse: 1.03362 | val_1_rmse: 1.02999 |  0:00:10s
epoch 51 | loss: 0.73892 | val_0_rmse: 1.0397  | val_1_rmse: 1.03907 |  0:00:10s
epoch 52 | loss: 0.73625 | val_0_rmse: 1.07366 | val_1_rmse: 1.07775 |  0:00:11s
epoch 53 | loss: 0.71995 | val_0_rmse: 1.08164 | val_1_rmse: 1.08549 |  0:00:11s
epoch 54 | loss: 0.75267 | val_0_rmse: 1.05608 | val_1_rmse: 1.05706 |  0:00:11s
epoch 55 | loss: 0.69434 | val_0_rmse: 1.02624 | val_1_rmse: 1.02346 |  0:00:11s
epoch 56 | loss: 0.71822 | val_0_rmse: 1.02371 | val_1_rmse: 1.01941 |  0:00:12s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.96041
Best weights from best epoch are automatically used!
ended training at: 06:30:40
Feature importance:
Mean squared error is of 6569803458.21685
Mean absolute error:64463.505973135965
MAPE:0.692984840482933
R2 score:0.07572196998758252
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:30:40
epoch 0  | loss: 2.3479  | val_0_rmse: 1.11646 | val_1_rmse: 1.54792 |  0:00:00s
epoch 1  | loss: 1.50326 | val_0_rmse: 1.0208  | val_1_rmse: 1.04403 |  0:00:00s
epoch 2  | loss: 1.08465 | val_0_rmse: 1.07447 | val_1_rmse: 1.07773 |  0:00:01s
epoch 3  | loss: 0.92974 | val_0_rmse: 1.03873 | val_1_rmse: 1.02332 |  0:00:01s
epoch 4  | loss: 0.92343 | val_0_rmse: 1.00886 | val_1_rmse: 0.97143 |  0:00:02s
epoch 5  | loss: 0.83869 | val_0_rmse: 0.9973  | val_1_rmse: 0.97225 |  0:00:02s
epoch 6  | loss: 0.85842 | val_0_rmse: 1.06627 | val_1_rmse: 1.07351 |  0:00:03s
epoch 7  | loss: 0.83282 | val_0_rmse: 0.96623 | val_1_rmse: 0.93506 |  0:00:03s
epoch 8  | loss: 0.83661 | val_0_rmse: 0.95486 | val_1_rmse: 0.93334 |  0:00:04s
epoch 9  | loss: 0.78575 | val_0_rmse: 0.93716 | val_1_rmse: 0.91892 |  0:00:04s
epoch 10 | loss: 0.71985 | val_0_rmse: 0.92223 | val_1_rmse: 0.90404 |  0:00:05s
epoch 11 | loss: 0.72581 | val_0_rmse: 0.95375 | val_1_rmse: 0.9395  |  0:00:05s
epoch 12 | loss: 0.73765 | val_0_rmse: 0.94263 | val_1_rmse: 0.92905 |  0:00:06s
epoch 13 | loss: 0.72255 | val_0_rmse: 0.95823 | val_1_rmse: 0.94293 |  0:00:06s
epoch 14 | loss: 0.72203 | val_0_rmse: 0.93772 | val_1_rmse: 0.91445 |  0:00:07s
epoch 15 | loss: 0.7072  | val_0_rmse: 0.94524 | val_1_rmse: 0.92686 |  0:00:07s
epoch 16 | loss: 0.72495 | val_0_rmse: 0.97408 | val_1_rmse: 0.96112 |  0:00:07s
epoch 17 | loss: 0.71615 | val_0_rmse: 0.98985 | val_1_rmse: 0.98241 |  0:00:08s
epoch 18 | loss: 0.70652 | val_0_rmse: 0.91731 | val_1_rmse: 0.8905  |  0:00:08s
epoch 19 | loss: 0.7135  | val_0_rmse: 0.90838 | val_1_rmse: 0.88557 |  0:00:09s
epoch 20 | loss: 0.72178 | val_0_rmse: 0.90761 | val_1_rmse: 0.88445 |  0:00:09s
epoch 21 | loss: 0.69229 | val_0_rmse: 0.90186 | val_1_rmse: 0.88359 |  0:00:10s
epoch 22 | loss: 0.70601 | val_0_rmse: 0.90751 | val_1_rmse: 0.89224 |  0:00:10s
epoch 23 | loss: 0.69348 | val_0_rmse: 0.93989 | val_1_rmse: 0.92695 |  0:00:11s
epoch 24 | loss: 0.69569 | val_0_rmse: 0.93474 | val_1_rmse: 0.91735 |  0:00:11s
epoch 25 | loss: 0.6788  | val_0_rmse: 0.93603 | val_1_rmse: 0.91871 |  0:00:12s
epoch 26 | loss: 0.67851 | val_0_rmse: 0.95504 | val_1_rmse: 0.94201 |  0:00:12s
epoch 27 | loss: 0.68622 | val_0_rmse: 0.94143 | val_1_rmse: 0.9336  |  0:00:13s
epoch 28 | loss: 0.69302 | val_0_rmse: 0.90988 | val_1_rmse: 0.90564 |  0:00:13s
epoch 29 | loss: 0.67927 | val_0_rmse: 0.90289 | val_1_rmse: 0.90052 |  0:00:14s
epoch 30 | loss: 0.67117 | val_0_rmse: 0.96319 | val_1_rmse: 0.96323 |  0:00:14s
epoch 31 | loss: 0.67823 | val_0_rmse: 0.91976 | val_1_rmse: 0.9178  |  0:00:14s
epoch 32 | loss: 0.67556 | val_0_rmse: 0.94194 | val_1_rmse: 0.93734 |  0:00:15s
epoch 33 | loss: 0.66953 | val_0_rmse: 0.93175 | val_1_rmse: 0.92728 |  0:00:15s
epoch 34 | loss: 0.68266 | val_0_rmse: 0.95437 | val_1_rmse: 0.95174 |  0:00:16s
epoch 35 | loss: 0.68534 | val_0_rmse: 0.90902 | val_1_rmse: 0.90386 |  0:00:16s
epoch 36 | loss: 0.6935  | val_0_rmse: 0.91161 | val_1_rmse: 0.90521 |  0:00:17s
epoch 37 | loss: 0.68213 | val_0_rmse: 0.90311 | val_1_rmse: 0.89826 |  0:00:17s
epoch 38 | loss: 0.68026 | val_0_rmse: 0.91474 | val_1_rmse: 0.91081 |  0:00:18s
epoch 39 | loss: 0.67746 | val_0_rmse: 0.91684 | val_1_rmse: 0.9104  |  0:00:18s
epoch 40 | loss: 0.68025 | val_0_rmse: 0.916   | val_1_rmse: 0.90773 |  0:00:19s
epoch 41 | loss: 0.67667 | val_0_rmse: 0.92853 | val_1_rmse: 0.91938 |  0:00:19s
epoch 42 | loss: 0.67945 | val_0_rmse: 0.95175 | val_1_rmse: 0.94467 |  0:00:20s
epoch 43 | loss: 0.6762  | val_0_rmse: 0.92303 | val_1_rmse: 0.91661 |  0:00:20s
epoch 44 | loss: 0.67128 | val_0_rmse: 0.90131 | val_1_rmse: 0.89404 |  0:00:20s
epoch 45 | loss: 0.66553 | val_0_rmse: 0.94884 | val_1_rmse: 0.93696 |  0:00:21s
epoch 46 | loss: 0.68484 | val_0_rmse: 0.95472 | val_1_rmse: 0.94606 |  0:00:21s
epoch 47 | loss: 0.67281 | val_0_rmse: 0.98078 | val_1_rmse: 0.9658  |  0:00:22s
epoch 48 | loss: 0.66413 | val_0_rmse: 0.95723 | val_1_rmse: 0.94349 |  0:00:22s
epoch 49 | loss: 0.67406 | val_0_rmse: 0.89912 | val_1_rmse: 0.89093 |  0:00:23s
epoch 50 | loss: 0.68549 | val_0_rmse: 0.89417 | val_1_rmse: 0.89065 |  0:00:23s
epoch 51 | loss: 0.67622 | val_0_rmse: 0.89124 | val_1_rmse: 0.88812 |  0:00:24s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.88359
Best weights from best epoch are automatically used!
ended training at: 06:31:05
Feature importance:
Mean squared error is of 6689034355.505551
Mean absolute error:59962.30190940416
MAPE:0.4724229035641427
R2 score:0.1937530440703753
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:31:05
epoch 0  | loss: 2.58075 | val_0_rmse: 1.09829 | val_1_rmse: 1.03559 |  0:00:00s
epoch 1  | loss: 1.65478 | val_0_rmse: 1.02443 | val_1_rmse: 0.98962 |  0:00:00s
epoch 2  | loss: 1.23574 | val_0_rmse: 1.03978 | val_1_rmse: 1.01731 |  0:00:01s
epoch 3  | loss: 1.177   | val_0_rmse: 1.04326 | val_1_rmse: 1.01538 |  0:00:01s
epoch 4  | loss: 1.0138  | val_0_rmse: 1.01206 | val_1_rmse: 0.99033 |  0:00:02s
epoch 5  | loss: 0.97243 | val_0_rmse: 1.14073 | val_1_rmse: 0.98651 |  0:00:02s
epoch 6  | loss: 0.92279 | val_0_rmse: 1.01009 | val_1_rmse: 0.98147 |  0:00:03s
epoch 7  | loss: 0.85675 | val_0_rmse: 1.06495 | val_1_rmse: 1.03821 |  0:00:03s
epoch 8  | loss: 0.84149 | val_0_rmse: 1.13181 | val_1_rmse: 1.08531 |  0:00:04s
epoch 9  | loss: 0.80052 | val_0_rmse: 1.05878 | val_1_rmse: 1.0143  |  0:00:04s
epoch 10 | loss: 0.77637 | val_0_rmse: 1.0638  | val_1_rmse: 1.014   |  0:00:05s
epoch 11 | loss: 0.78725 | val_0_rmse: 0.96785 | val_1_rmse: 0.94353 |  0:00:05s
epoch 12 | loss: 0.77062 | val_0_rmse: 0.96713 | val_1_rmse: 0.94767 |  0:00:06s
epoch 13 | loss: 0.74244 | val_0_rmse: 0.92358 | val_1_rmse: 0.9039  |  0:00:06s
epoch 14 | loss: 0.73527 | val_0_rmse: 0.98022 | val_1_rmse: 0.95747 |  0:00:07s
epoch 15 | loss: 0.7334  | val_0_rmse: 0.95167 | val_1_rmse: 0.93244 |  0:00:07s
epoch 16 | loss: 0.7258  | val_0_rmse: 0.91441 | val_1_rmse: 0.90597 |  0:00:08s
epoch 17 | loss: 0.7365  | val_0_rmse: 0.92154 | val_1_rmse: 0.91072 |  0:00:08s
epoch 18 | loss: 0.73067 | val_0_rmse: 0.90715 | val_1_rmse: 0.88604 |  0:00:08s
epoch 19 | loss: 0.73369 | val_0_rmse: 0.96173 | val_1_rmse: 0.92521 |  0:00:09s
epoch 20 | loss: 0.72391 | val_0_rmse: 0.93754 | val_1_rmse: 0.89821 |  0:00:09s
epoch 21 | loss: 0.7172  | val_0_rmse: 0.92212 | val_1_rmse: 0.88979 |  0:00:10s
epoch 22 | loss: 0.72076 | val_0_rmse: 0.91947 | val_1_rmse: 0.88732 |  0:00:10s
epoch 23 | loss: 0.71564 | val_0_rmse: 0.91472 | val_1_rmse: 0.89207 |  0:00:11s
epoch 24 | loss: 0.70933 | val_0_rmse: 0.91399 | val_1_rmse: 0.91105 |  0:00:11s
epoch 25 | loss: 0.71547 | val_0_rmse: 0.92663 | val_1_rmse: 0.90432 |  0:00:12s
epoch 26 | loss: 0.73819 | val_0_rmse: 0.94792 | val_1_rmse: 0.93207 |  0:00:12s
epoch 27 | loss: 0.7241  | val_0_rmse: 0.96131 | val_1_rmse: 0.94528 |  0:00:13s
epoch 28 | loss: 0.74329 | val_0_rmse: 0.95369 | val_1_rmse: 0.9428  |  0:00:13s
epoch 29 | loss: 0.72958 | val_0_rmse: 0.90247 | val_1_rmse: 0.88593 |  0:00:14s
epoch 30 | loss: 0.73566 | val_0_rmse: 0.93124 | val_1_rmse: 0.90856 |  0:00:14s
epoch 31 | loss: 0.7397  | val_0_rmse: 0.95546 | val_1_rmse: 0.93926 |  0:00:15s
epoch 32 | loss: 0.73842 | val_0_rmse: 0.92244 | val_1_rmse: 0.90798 |  0:00:15s
epoch 33 | loss: 0.72915 | val_0_rmse: 0.96929 | val_1_rmse: 0.95952 |  0:00:15s
epoch 34 | loss: 0.74005 | val_0_rmse: 0.96078 | val_1_rmse: 0.94782 |  0:00:16s
epoch 35 | loss: 0.72526 | val_0_rmse: 0.92375 | val_1_rmse: 0.91097 |  0:00:16s
epoch 36 | loss: 0.73218 | val_0_rmse: 0.93879 | val_1_rmse: 0.9255  |  0:00:17s
epoch 37 | loss: 0.72696 | val_0_rmse: 0.94022 | val_1_rmse: 0.92604 |  0:00:17s
epoch 38 | loss: 0.71158 | val_0_rmse: 0.88988 | val_1_rmse: 0.8782  |  0:00:18s
epoch 39 | loss: 0.71728 | val_0_rmse: 0.97473 | val_1_rmse: 0.95507 |  0:00:18s
epoch 40 | loss: 0.7205  | val_0_rmse: 1.01124 | val_1_rmse: 0.99661 |  0:00:19s
epoch 41 | loss: 0.7182  | val_0_rmse: 1.0005  | val_1_rmse: 0.98287 |  0:00:19s
epoch 42 | loss: 0.7321  | val_0_rmse: 0.97416 | val_1_rmse: 0.95758 |  0:00:20s
epoch 43 | loss: 0.711   | val_0_rmse: 0.99584 | val_1_rmse: 0.98565 |  0:00:20s
epoch 44 | loss: 0.71977 | val_0_rmse: 0.97171 | val_1_rmse: 0.95453 |  0:00:21s
epoch 45 | loss: 0.70711 | val_0_rmse: 0.96479 | val_1_rmse: 0.94532 |  0:00:21s
epoch 46 | loss: 0.70513 | val_0_rmse: 0.95291 | val_1_rmse: 0.92971 |  0:00:22s
epoch 47 | loss: 0.69973 | val_0_rmse: 0.94514 | val_1_rmse: 0.91993 |  0:00:22s
epoch 48 | loss: 0.71235 | val_0_rmse: 0.93038 | val_1_rmse: 0.90684 |  0:00:22s
epoch 49 | loss: 0.70771 | val_0_rmse: 0.93555 | val_1_rmse: 0.91474 |  0:00:23s
epoch 50 | loss: 0.69788 | val_0_rmse: 0.9363  | val_1_rmse: 0.91419 |  0:00:23s
epoch 51 | loss: 0.70448 | val_0_rmse: 0.94497 | val_1_rmse: 0.92365 |  0:00:24s
epoch 52 | loss: 0.6869  | val_0_rmse: 0.91347 | val_1_rmse: 0.89886 |  0:00:24s
epoch 53 | loss: 0.69305 | val_0_rmse: 0.92958 | val_1_rmse: 0.90451 |  0:00:25s
epoch 54 | loss: 0.68701 | val_0_rmse: 0.95595 | val_1_rmse: 0.92506 |  0:00:25s
epoch 55 | loss: 0.69178 | val_0_rmse: 0.93733 | val_1_rmse: 0.91602 |  0:00:26s
epoch 56 | loss: 0.69401 | val_0_rmse: 0.94978 | val_1_rmse: 0.93124 |  0:00:26s
epoch 57 | loss: 0.6862  | val_0_rmse: 0.94334 | val_1_rmse: 0.92786 |  0:00:27s
epoch 58 | loss: 0.68128 | val_0_rmse: 0.97846 | val_1_rmse: 0.95495 |  0:00:27s
epoch 59 | loss: 0.69594 | val_0_rmse: 0.9326  | val_1_rmse: 0.91541 |  0:00:28s
epoch 60 | loss: 0.67749 | val_0_rmse: 0.95844 | val_1_rmse: 0.93486 |  0:00:28s
epoch 61 | loss: 0.68396 | val_0_rmse: 0.92597 | val_1_rmse: 0.91096 |  0:00:28s
epoch 62 | loss: 0.68047 | val_0_rmse: 0.9143  | val_1_rmse: 0.90612 |  0:00:29s
epoch 63 | loss: 0.67854 | val_0_rmse: 0.91572 | val_1_rmse: 0.8939  |  0:00:29s
epoch 64 | loss: 0.6952  | val_0_rmse: 0.91811 | val_1_rmse: 0.90788 |  0:00:30s
epoch 65 | loss: 0.69957 | val_0_rmse: 0.90878 | val_1_rmse: 0.90407 |  0:00:30s
epoch 66 | loss: 0.7039  | val_0_rmse: 0.90004 | val_1_rmse: 0.87834 |  0:00:31s
epoch 67 | loss: 0.68769 | val_0_rmse: 0.9142  | val_1_rmse: 0.89166 |  0:00:31s
epoch 68 | loss: 0.68559 | val_0_rmse: 0.95146 | val_1_rmse: 0.9187  |  0:00:32s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.8782
Best weights from best epoch are automatically used!
ended training at: 06:31:38
Feature importance:
Mean squared error is of 5730489548.379819
Mean absolute error:55347.989675095414
MAPE:0.45659186042960076
R2 score:0.23594096850528767
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:31:38
epoch 0  | loss: 2.37464 | val_0_rmse: 1.06244 | val_1_rmse: 1.00295 |  0:00:00s
epoch 1  | loss: 1.50244 | val_0_rmse: 0.99508 | val_1_rmse: 0.92981 |  0:00:00s
epoch 2  | loss: 1.10116 | val_0_rmse: 1.0123  | val_1_rmse: 0.94134 |  0:00:01s
epoch 3  | loss: 0.96405 | val_0_rmse: 0.96504 | val_1_rmse: 0.90768 |  0:00:01s
epoch 4  | loss: 0.89298 | val_0_rmse: 0.94064 | val_1_rmse: 0.88331 |  0:00:02s
epoch 5  | loss: 0.86799 | val_0_rmse: 0.99482 | val_1_rmse: 0.9299  |  0:00:02s
epoch 6  | loss: 0.80205 | val_0_rmse: 1.013   | val_1_rmse: 0.9458  |  0:00:03s
epoch 7  | loss: 0.84256 | val_0_rmse: 0.98583 | val_1_rmse: 0.92139 |  0:00:03s
epoch 8  | loss: 0.77953 | val_0_rmse: 0.95333 | val_1_rmse: 0.88863 |  0:00:04s
epoch 9  | loss: 0.76722 | val_0_rmse: 0.94659 | val_1_rmse: 0.88796 |  0:00:04s
epoch 10 | loss: 0.743   | val_0_rmse: 0.93003 | val_1_rmse: 0.87023 |  0:00:05s
epoch 11 | loss: 0.7203  | val_0_rmse: 0.9115  | val_1_rmse: 0.85833 |  0:00:05s
epoch 12 | loss: 0.73262 | val_0_rmse: 0.94637 | val_1_rmse: 0.90071 |  0:00:06s
epoch 13 | loss: 0.71039 | val_0_rmse: 0.95637 | val_1_rmse: 0.90504 |  0:00:06s
epoch 14 | loss: 0.70416 | val_0_rmse: 0.94025 | val_1_rmse: 0.8827  |  0:00:07s
epoch 15 | loss: 0.70244 | val_0_rmse: 0.95574 | val_1_rmse: 0.89663 |  0:00:07s
epoch 16 | loss: 0.69911 | val_0_rmse: 0.9696  | val_1_rmse: 0.91708 |  0:00:07s
epoch 17 | loss: 0.69388 | val_0_rmse: 0.97105 | val_1_rmse: 0.91203 |  0:00:08s
epoch 18 | loss: 0.69537 | val_0_rmse: 0.93487 | val_1_rmse: 0.88227 |  0:00:08s
epoch 19 | loss: 0.6882  | val_0_rmse: 0.88087 | val_1_rmse: 0.83721 |  0:00:09s
epoch 20 | loss: 0.70167 | val_0_rmse: 0.94016 | val_1_rmse: 0.88741 |  0:00:09s
epoch 21 | loss: 0.68698 | val_0_rmse: 0.91432 | val_1_rmse: 0.8621  |  0:00:10s
epoch 22 | loss: 0.68465 | val_0_rmse: 0.92443 | val_1_rmse: 0.86743 |  0:00:10s
epoch 23 | loss: 0.68546 | val_0_rmse: 0.91576 | val_1_rmse: 0.85841 |  0:00:11s
epoch 24 | loss: 0.68754 | val_0_rmse: 0.92063 | val_1_rmse: 0.85911 |  0:00:11s
epoch 25 | loss: 0.69059 | val_0_rmse: 0.87871 | val_1_rmse: 0.82587 |  0:00:12s
epoch 26 | loss: 0.68253 | val_0_rmse: 0.83944 | val_1_rmse: 0.79817 |  0:00:12s
epoch 27 | loss: 0.68466 | val_0_rmse: 0.90001 | val_1_rmse: 0.8418  |  0:00:13s
epoch 28 | loss: 0.69185 | val_0_rmse: 0.91367 | val_1_rmse: 0.85707 |  0:00:13s
epoch 29 | loss: 0.68106 | val_0_rmse: 0.88925 | val_1_rmse: 0.82912 |  0:00:14s
epoch 30 | loss: 0.67774 | val_0_rmse: 0.91191 | val_1_rmse: 0.84424 |  0:00:14s
epoch 31 | loss: 0.68216 | val_0_rmse: 0.89807 | val_1_rmse: 0.82989 |  0:00:14s
epoch 32 | loss: 0.67228 | val_0_rmse: 0.90633 | val_1_rmse: 0.83928 |  0:00:15s
epoch 33 | loss: 0.68948 | val_0_rmse: 0.85156 | val_1_rmse: 0.79632 |  0:00:15s
epoch 34 | loss: 0.68105 | val_0_rmse: 0.87355 | val_1_rmse: 0.81131 |  0:00:16s
epoch 35 | loss: 0.67992 | val_0_rmse: 0.84271 | val_1_rmse: 0.78306 |  0:00:16s
epoch 36 | loss: 0.6859  | val_0_rmse: 0.9048  | val_1_rmse: 0.83774 |  0:00:17s
epoch 37 | loss: 0.67945 | val_0_rmse: 0.90277 | val_1_rmse: 0.83235 |  0:00:17s
epoch 38 | loss: 0.68038 | val_0_rmse: 0.90093 | val_1_rmse: 0.83048 |  0:00:18s
epoch 39 | loss: 0.66549 | val_0_rmse: 0.90223 | val_1_rmse: 0.83051 |  0:00:18s
epoch 40 | loss: 0.67624 | val_0_rmse: 0.88251 | val_1_rmse: 0.81572 |  0:00:19s
epoch 41 | loss: 0.67548 | val_0_rmse: 0.88731 | val_1_rmse: 0.82257 |  0:00:19s
epoch 42 | loss: 0.67482 | val_0_rmse: 0.85465 | val_1_rmse: 0.79774 |  0:00:20s
epoch 43 | loss: 0.6783  | val_0_rmse: 0.86662 | val_1_rmse: 0.80263 |  0:00:20s
epoch 44 | loss: 0.66737 | val_0_rmse: 0.85867 | val_1_rmse: 0.79757 |  0:00:21s
epoch 45 | loss: 0.6703  | val_0_rmse: 0.85945 | val_1_rmse: 0.80091 |  0:00:21s
epoch 46 | loss: 0.67347 | val_0_rmse: 0.89595 | val_1_rmse: 0.82508 |  0:00:22s
epoch 47 | loss: 0.67039 | val_0_rmse: 0.86662 | val_1_rmse: 0.8052  |  0:00:22s
epoch 48 | loss: 0.67274 | val_0_rmse: 0.87169 | val_1_rmse: 0.81181 |  0:00:22s
epoch 49 | loss: 0.67125 | val_0_rmse: 0.87646 | val_1_rmse: 0.81188 |  0:00:23s
epoch 50 | loss: 0.66859 | val_0_rmse: 0.82073 | val_1_rmse: 0.7775  |  0:00:23s
epoch 51 | loss: 0.67444 | val_0_rmse: 0.85097 | val_1_rmse: 0.79234 |  0:00:24s
epoch 52 | loss: 0.67398 | val_0_rmse: 0.85336 | val_1_rmse: 0.79309 |  0:00:24s
epoch 53 | loss: 0.66146 | val_0_rmse: 0.82575 | val_1_rmse: 0.78081 |  0:00:25s
epoch 54 | loss: 0.68243 | val_0_rmse: 0.86987 | val_1_rmse: 0.80467 |  0:00:25s
epoch 55 | loss: 0.66815 | val_0_rmse: 0.87778 | val_1_rmse: 0.8082  |  0:00:26s
epoch 56 | loss: 0.67657 | val_0_rmse: 0.87205 | val_1_rmse: 0.80169 |  0:00:26s
epoch 57 | loss: 0.67339 | val_0_rmse: 0.88424 | val_1_rmse: 0.81494 |  0:00:27s
epoch 58 | loss: 0.66415 | val_0_rmse: 0.92188 | val_1_rmse: 0.83686 |  0:00:27s
epoch 59 | loss: 0.67052 | val_0_rmse: 0.91825 | val_1_rmse: 0.83229 |  0:00:28s
epoch 60 | loss: 0.67039 | val_0_rmse: 0.92766 | val_1_rmse: 0.83907 |  0:00:28s
epoch 61 | loss: 0.66195 | val_0_rmse: 0.88785 | val_1_rmse: 0.79805 |  0:00:29s
epoch 62 | loss: 0.66794 | val_0_rmse: 0.91953 | val_1_rmse: 0.81862 |  0:00:29s
epoch 63 | loss: 0.67665 | val_0_rmse: 0.90499 | val_1_rmse: 0.81394 |  0:00:29s
epoch 64 | loss: 0.67089 | val_0_rmse: 0.8902  | val_1_rmse: 0.80409 |  0:00:30s
epoch 65 | loss: 0.66424 | val_0_rmse: 0.88428 | val_1_rmse: 0.8037  |  0:00:30s
epoch 66 | loss: 0.66553 | val_0_rmse: 0.84537 | val_1_rmse: 0.77156 |  0:00:31s
epoch 67 | loss: 0.6755  | val_0_rmse: 0.83516 | val_1_rmse: 0.76477 |  0:00:31s
epoch 68 | loss: 0.66718 | val_0_rmse: 0.83072 | val_1_rmse: 0.76641 |  0:00:32s
epoch 69 | loss: 0.66397 | val_0_rmse: 0.85065 | val_1_rmse: 0.78167 |  0:00:32s
epoch 70 | loss: 0.66514 | val_0_rmse: 0.9103  | val_1_rmse: 0.8252  |  0:00:33s
epoch 71 | loss: 0.66539 | val_0_rmse: 0.9093  | val_1_rmse: 0.82664 |  0:00:33s
epoch 72 | loss: 0.66126 | val_0_rmse: 0.92041 | val_1_rmse: 0.82871 |  0:00:34s
epoch 73 | loss: 0.66751 | val_0_rmse: 0.90218 | val_1_rmse: 0.8061  |  0:00:34s
epoch 74 | loss: 0.65938 | val_0_rmse: 0.88786 | val_1_rmse: 0.77441 |  0:00:35s
epoch 75 | loss: 0.6651  | val_0_rmse: 0.87849 | val_1_rmse: 0.78552 |  0:00:35s
epoch 76 | loss: 0.67152 | val_0_rmse: 0.90171 | val_1_rmse: 0.79168 |  0:00:35s
epoch 77 | loss: 0.67017 | val_0_rmse: 0.92    | val_1_rmse: 0.81004 |  0:00:36s
epoch 78 | loss: 0.67002 | val_0_rmse: 0.91895 | val_1_rmse: 0.80712 |  0:00:36s
epoch 79 | loss: 0.6757  | val_0_rmse: 0.92694 | val_1_rmse: 0.82049 |  0:00:37s
epoch 80 | loss: 0.66623 | val_0_rmse: 0.91971 | val_1_rmse: 0.82839 |  0:00:37s
epoch 81 | loss: 0.66167 | val_0_rmse: 0.87584 | val_1_rmse: 0.8029  |  0:00:38s
epoch 82 | loss: 0.66393 | val_0_rmse: 0.88422 | val_1_rmse: 0.80277 |  0:00:38s
epoch 83 | loss: 0.66023 | val_0_rmse: 0.91475 | val_1_rmse: 0.81126 |  0:00:39s
epoch 84 | loss: 0.66434 | val_0_rmse: 0.90002 | val_1_rmse: 0.80237 |  0:00:39s
epoch 85 | loss: 0.65864 | val_0_rmse: 0.90102 | val_1_rmse: 0.80484 |  0:00:40s
epoch 86 | loss: 0.66368 | val_0_rmse: 0.85838 | val_1_rmse: 0.77874 |  0:00:40s
epoch 87 | loss: 0.66289 | val_0_rmse: 0.88151 | val_1_rmse: 0.79951 |  0:00:41s
epoch 88 | loss: 0.65852 | val_0_rmse: 0.87821 | val_1_rmse: 0.79334 |  0:00:41s
epoch 89 | loss: 0.66222 | val_0_rmse: 0.86913 | val_1_rmse: 0.79486 |  0:00:42s
epoch 90 | loss: 0.66525 | val_0_rmse: 0.90433 | val_1_rmse: 0.82824 |  0:00:42s
epoch 91 | loss: 0.66663 | val_0_rmse: 0.82306 | val_1_rmse: 0.75686 |  0:00:42s
epoch 92 | loss: 0.6654  | val_0_rmse: 0.87443 | val_1_rmse: 0.77971 |  0:00:43s
epoch 93 | loss: 0.66752 | val_0_rmse: 0.86879 | val_1_rmse: 0.78144 |  0:00:43s
epoch 94 | loss: 0.67264 | val_0_rmse: 0.89342 | val_1_rmse: 0.79129 |  0:00:44s
epoch 95 | loss: 0.66174 | val_0_rmse: 0.94581 | val_1_rmse: 0.83072 |  0:00:44s
epoch 96 | loss: 0.65967 | val_0_rmse: 0.9111  | val_1_rmse: 0.81952 |  0:00:45s
epoch 97 | loss: 0.66519 | val_0_rmse: 0.91244 | val_1_rmse: 0.82925 |  0:00:45s
epoch 98 | loss: 0.66443 | val_0_rmse: 0.89328 | val_1_rmse: 0.80357 |  0:00:46s
epoch 99 | loss: 0.66091 | val_0_rmse: 0.88409 | val_1_rmse: 0.78815 |  0:00:46s
epoch 100| loss: 0.65527 | val_0_rmse: 0.8652  | val_1_rmse: 0.76963 |  0:00:47s
epoch 101| loss: 0.66095 | val_0_rmse: 0.89475 | val_1_rmse: 0.78337 |  0:00:47s
epoch 102| loss: 0.66391 | val_0_rmse: 0.90027 | val_1_rmse: 0.81234 |  0:00:48s
epoch 103| loss: 0.66544 | val_0_rmse: 0.86597 | val_1_rmse: 0.79247 |  0:00:48s
epoch 104| loss: 0.66592 | val_0_rmse: 0.86822 | val_1_rmse: 0.77468 |  0:00:49s
epoch 105| loss: 0.66795 | val_0_rmse: 0.89715 | val_1_rmse: 0.80313 |  0:00:49s
epoch 106| loss: 0.65582 | val_0_rmse: 0.90606 | val_1_rmse: 0.80626 |  0:00:50s
epoch 107| loss: 0.66517 | val_0_rmse: 0.93527 | val_1_rmse: 0.83477 |  0:00:50s
epoch 108| loss: 0.67188 | val_0_rmse: 0.90007 | val_1_rmse: 0.79446 |  0:00:51s
epoch 109| loss: 0.65778 | val_0_rmse: 0.88791 | val_1_rmse: 0.76354 |  0:00:51s
epoch 110| loss: 0.66518 | val_0_rmse: 0.913   | val_1_rmse: 0.79189 |  0:00:51s
epoch 111| loss: 0.66519 | val_0_rmse: 0.93224 | val_1_rmse: 0.80634 |  0:00:52s
epoch 112| loss: 0.65793 | val_0_rmse: 0.91577 | val_1_rmse: 0.81708 |  0:00:52s
epoch 113| loss: 0.66522 | val_0_rmse: 0.90524 | val_1_rmse: 0.82435 |  0:00:53s
epoch 114| loss: 0.65976 | val_0_rmse: 0.88855 | val_1_rmse: 0.80966 |  0:00:53s
epoch 115| loss: 0.66108 | val_0_rmse: 0.8701  | val_1_rmse: 0.78796 |  0:00:54s
epoch 116| loss: 0.66607 | val_0_rmse: 0.90404 | val_1_rmse: 0.80021 |  0:00:54s
epoch 117| loss: 0.65886 | val_0_rmse: 0.8849  | val_1_rmse: 0.80159 |  0:00:55s
epoch 118| loss: 0.66214 | val_0_rmse: 0.86034 | val_1_rmse: 0.79347 |  0:00:55s
epoch 119| loss: 0.66833 | val_0_rmse: 0.83598 | val_1_rmse: 0.78124 |  0:00:56s
epoch 120| loss: 0.66446 | val_0_rmse: 0.82781 | val_1_rmse: 0.77601 |  0:00:56s
epoch 121| loss: 0.66466 | val_0_rmse: 0.8107  | val_1_rmse: 0.77332 |  0:00:57s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.75686
Best weights from best epoch are automatically used!
ended training at: 06:32:35
Feature importance:
Mean squared error is of 5412160534.677462
Mean absolute error:52956.7859634754
MAPE:0.44593051236781567
R2 score:0.31479066794793586
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:32:35
epoch 0  | loss: 2.8943  | val_0_rmse: 1.0302  | val_1_rmse: 0.99421 |  0:00:00s
epoch 1  | loss: 1.42835 | val_0_rmse: 1.00521 | val_1_rmse: 0.9792  |  0:00:00s
epoch 2  | loss: 1.16724 | val_0_rmse: 1.00414 | val_1_rmse: 0.97806 |  0:00:01s
epoch 3  | loss: 1.07451 | val_0_rmse: 1.02788 | val_1_rmse: 1.00118 |  0:00:01s
epoch 4  | loss: 1.06793 | val_0_rmse: 1.01304 | val_1_rmse: 0.98696 |  0:00:02s
epoch 5  | loss: 0.95177 | val_0_rmse: 0.98633 | val_1_rmse: 0.96745 |  0:00:02s
epoch 6  | loss: 0.89468 | val_0_rmse: 0.99904 | val_1_rmse: 0.97539 |  0:00:03s
epoch 7  | loss: 0.89855 | val_0_rmse: 1.0653  | val_1_rmse: 1.03744 |  0:00:03s
epoch 8  | loss: 0.81582 | val_0_rmse: 1.00965 | val_1_rmse: 0.98356 |  0:00:04s
epoch 9  | loss: 0.76589 | val_0_rmse: 1.09851 | val_1_rmse: 1.06096 |  0:00:04s
epoch 10 | loss: 0.76783 | val_0_rmse: 1.03107 | val_1_rmse: 1.00052 |  0:00:05s
epoch 11 | loss: 0.77916 | val_0_rmse: 1.09483 | val_1_rmse: 1.06894 |  0:00:05s
epoch 12 | loss: 0.76548 | val_0_rmse: 1.14888 | val_1_rmse: 1.12585 |  0:00:06s
epoch 13 | loss: 0.7488  | val_0_rmse: 1.24681 | val_1_rmse: 1.22216 |  0:00:06s
epoch 14 | loss: 0.72917 | val_0_rmse: 1.13059 | val_1_rmse: 1.10523 |  0:00:06s
epoch 15 | loss: 0.7304  | val_0_rmse: 1.15791 | val_1_rmse: 1.13212 |  0:00:07s
epoch 16 | loss: 0.71698 | val_0_rmse: 1.13169 | val_1_rmse: 1.10548 |  0:00:07s
epoch 17 | loss: 0.73627 | val_0_rmse: 1.10447 | val_1_rmse: 1.07154 |  0:00:08s
epoch 18 | loss: 0.72501 | val_0_rmse: 1.11379 | val_1_rmse: 1.08145 |  0:00:08s
epoch 19 | loss: 0.72548 | val_0_rmse: 1.09472 | val_1_rmse: 1.06679 |  0:00:09s
epoch 20 | loss: 0.71623 | val_0_rmse: 1.10037 | val_1_rmse: 1.07259 |  0:00:09s
epoch 21 | loss: 0.71226 | val_0_rmse: 1.06764 | val_1_rmse: 1.0381  |  0:00:10s
epoch 22 | loss: 0.71692 | val_0_rmse: 1.05272 | val_1_rmse: 1.02255 |  0:00:10s
epoch 23 | loss: 0.70625 | val_0_rmse: 1.03688 | val_1_rmse: 1.0084  |  0:00:11s
epoch 24 | loss: 0.73061 | val_0_rmse: 1.03868 | val_1_rmse: 1.00901 |  0:00:11s
epoch 25 | loss: 0.71554 | val_0_rmse: 0.99053 | val_1_rmse: 0.95859 |  0:00:12s
epoch 26 | loss: 0.70286 | val_0_rmse: 1.0201  | val_1_rmse: 0.98258 |  0:00:12s
epoch 27 | loss: 0.72965 | val_0_rmse: 1.07578 | val_1_rmse: 1.03027 |  0:00:13s
epoch 28 | loss: 0.72159 | val_0_rmse: 1.11513 | val_1_rmse: 1.08818 |  0:00:13s
epoch 29 | loss: 0.70238 | val_0_rmse: 1.14237 | val_1_rmse: 1.11266 |  0:00:14s
epoch 30 | loss: 0.70689 | val_0_rmse: 1.11588 | val_1_rmse: 1.0881  |  0:00:14s
epoch 31 | loss: 0.71523 | val_0_rmse: 1.10505 | val_1_rmse: 1.07925 |  0:00:14s
epoch 32 | loss: 0.72098 | val_0_rmse: 1.07723 | val_1_rmse: 1.04665 |  0:00:15s
epoch 33 | loss: 0.71164 | val_0_rmse: 1.08894 | val_1_rmse: 1.0578  |  0:00:15s
epoch 34 | loss: 0.70857 | val_0_rmse: 1.12672 | val_1_rmse: 1.09851 |  0:00:16s
epoch 35 | loss: 0.70866 | val_0_rmse: 1.08334 | val_1_rmse: 1.0552  |  0:00:16s
epoch 36 | loss: 0.70493 | val_0_rmse: 1.07862 | val_1_rmse: 1.05012 |  0:00:17s
epoch 37 | loss: 0.70113 | val_0_rmse: 1.04679 | val_1_rmse: 1.01757 |  0:00:17s
epoch 38 | loss: 0.69708 | val_0_rmse: 1.0657  | val_1_rmse: 1.03311 |  0:00:18s
epoch 39 | loss: 0.69415 | val_0_rmse: 1.04831 | val_1_rmse: 1.01765 |  0:00:18s
epoch 40 | loss: 0.69536 | val_0_rmse: 1.0321  | val_1_rmse: 1.00363 |  0:00:19s
epoch 41 | loss: 0.6987  | val_0_rmse: 1.02371 | val_1_rmse: 1.00015 |  0:00:19s
epoch 42 | loss: 0.70261 | val_0_rmse: 0.98213 | val_1_rmse: 0.95454 |  0:00:20s
epoch 43 | loss: 0.692   | val_0_rmse: 1.01596 | val_1_rmse: 0.9899  |  0:00:20s
epoch 44 | loss: 0.68542 | val_0_rmse: 1.05134 | val_1_rmse: 1.02788 |  0:00:21s
epoch 45 | loss: 0.69755 | val_0_rmse: 1.02651 | val_1_rmse: 0.99359 |  0:00:21s
epoch 46 | loss: 0.68578 | val_0_rmse: 1.01485 | val_1_rmse: 0.98334 |  0:00:22s
epoch 47 | loss: 0.68681 | val_0_rmse: 0.97879 | val_1_rmse: 0.9445  |  0:00:22s
epoch 48 | loss: 0.69122 | val_0_rmse: 0.91838 | val_1_rmse: 0.89524 |  0:00:22s
epoch 49 | loss: 0.69596 | val_0_rmse: 0.97492 | val_1_rmse: 0.94376 |  0:00:23s
epoch 50 | loss: 0.68425 | val_0_rmse: 1.01974 | val_1_rmse: 0.98779 |  0:00:23s
epoch 51 | loss: 0.69659 | val_0_rmse: 1.00822 | val_1_rmse: 0.97295 |  0:00:24s
epoch 52 | loss: 0.70001 | val_0_rmse: 0.98277 | val_1_rmse: 0.95889 |  0:00:24s
epoch 53 | loss: 0.69478 | val_0_rmse: 0.95603 | val_1_rmse: 0.93562 |  0:00:25s
epoch 54 | loss: 0.68357 | val_0_rmse: 0.95804 | val_1_rmse: 0.93612 |  0:00:25s
epoch 55 | loss: 0.68811 | val_0_rmse: 1.00835 | val_1_rmse: 0.97591 |  0:00:26s
epoch 56 | loss: 0.68899 | val_0_rmse: 1.02859 | val_1_rmse: 0.99538 |  0:00:26s
epoch 57 | loss: 0.69469 | val_0_rmse: 1.00572 | val_1_rmse: 0.9764  |  0:00:27s
epoch 58 | loss: 0.68427 | val_0_rmse: 0.92506 | val_1_rmse: 0.88826 |  0:00:27s
epoch 59 | loss: 0.7066  | val_0_rmse: 0.92181 | val_1_rmse: 0.88893 |  0:00:28s
epoch 60 | loss: 0.7148  | val_0_rmse: 1.00756 | val_1_rmse: 0.97344 |  0:00:28s
epoch 61 | loss: 0.70047 | val_0_rmse: 1.04279 | val_1_rmse: 1.005   |  0:00:28s
epoch 62 | loss: 0.71634 | val_0_rmse: 1.06237 | val_1_rmse: 1.01983 |  0:00:29s
epoch 63 | loss: 0.72561 | val_0_rmse: 0.85794 | val_1_rmse: 0.85235 |  0:00:29s
epoch 64 | loss: 0.72244 | val_0_rmse: 0.85087 | val_1_rmse: 0.83899 |  0:00:30s
epoch 65 | loss: 0.70095 | val_0_rmse: 1.03253 | val_1_rmse: 0.99397 |  0:00:30s
epoch 66 | loss: 0.7152  | val_0_rmse: 1.01186 | val_1_rmse: 0.96832 |  0:00:31s
epoch 67 | loss: 0.70758 | val_0_rmse: 0.97762 | val_1_rmse: 0.93074 |  0:00:31s
epoch 68 | loss: 0.71031 | val_0_rmse: 1.04434 | val_1_rmse: 1.01364 |  0:00:32s
epoch 69 | loss: 0.70672 | val_0_rmse: 1.03543 | val_1_rmse: 1.00074 |  0:00:32s
epoch 70 | loss: 0.70305 | val_0_rmse: 1.01911 | val_1_rmse: 0.98035 |  0:00:33s
epoch 71 | loss: 0.70268 | val_0_rmse: 0.96681 | val_1_rmse: 0.92347 |  0:00:33s
epoch 72 | loss: 0.71053 | val_0_rmse: 0.84178 | val_1_rmse: 0.82471 |  0:00:34s
epoch 73 | loss: 0.69279 | val_0_rmse: 0.86211 | val_1_rmse: 0.83402 |  0:00:34s
epoch 74 | loss: 0.69982 | val_0_rmse: 0.97969 | val_1_rmse: 0.93115 |  0:00:35s
epoch 75 | loss: 0.70999 | val_0_rmse: 0.97917 | val_1_rmse: 0.93252 |  0:00:35s
epoch 76 | loss: 0.70228 | val_0_rmse: 0.94937 | val_1_rmse: 0.93068 |  0:00:36s
epoch 77 | loss: 0.69553 | val_0_rmse: 0.83269 | val_1_rmse: 0.81703 |  0:00:36s
epoch 78 | loss: 0.72141 | val_0_rmse: 0.86308 | val_1_rmse: 0.8325  |  0:00:36s
epoch 79 | loss: 0.73462 | val_0_rmse: 0.87897 | val_1_rmse: 0.84894 |  0:00:37s
epoch 80 | loss: 0.71784 | val_0_rmse: 1.04158 | val_1_rmse: 1.00613 |  0:00:37s
epoch 81 | loss: 0.70617 | val_0_rmse: 0.96472 | val_1_rmse: 0.92535 |  0:00:38s
epoch 82 | loss: 0.69731 | val_0_rmse: 0.96411 | val_1_rmse: 0.91761 |  0:00:38s
epoch 83 | loss: 0.72084 | val_0_rmse: 0.93086 | val_1_rmse: 0.88728 |  0:00:39s
epoch 84 | loss: 0.73432 | val_0_rmse: 0.94508 | val_1_rmse: 0.91014 |  0:00:39s
epoch 85 | loss: 0.71769 | val_0_rmse: 0.91134 | val_1_rmse: 0.87333 |  0:00:40s
epoch 86 | loss: 0.70998 | val_0_rmse: 0.85265 | val_1_rmse: 0.82546 |  0:00:40s
epoch 87 | loss: 0.69054 | val_0_rmse: 0.83785 | val_1_rmse: 0.81302 |  0:00:41s
epoch 88 | loss: 0.68709 | val_0_rmse: 0.83905 | val_1_rmse: 0.81181 |  0:00:41s
epoch 89 | loss: 0.68785 | val_0_rmse: 0.84018 | val_1_rmse: 0.81366 |  0:00:42s
epoch 90 | loss: 0.68378 | val_0_rmse: 0.88564 | val_1_rmse: 0.85305 |  0:00:42s
epoch 91 | loss: 0.69024 | val_0_rmse: 0.90266 | val_1_rmse: 0.86435 |  0:00:43s
epoch 92 | loss: 0.68117 | val_0_rmse: 0.92663 | val_1_rmse: 0.89406 |  0:00:43s
epoch 93 | loss: 0.68859 | val_0_rmse: 0.91897 | val_1_rmse: 0.88513 |  0:00:43s
epoch 94 | loss: 0.68543 | val_0_rmse: 0.93573 | val_1_rmse: 0.90141 |  0:00:44s
epoch 95 | loss: 0.69226 | val_0_rmse: 0.93239 | val_1_rmse: 0.89151 |  0:00:44s
epoch 96 | loss: 0.68147 | val_0_rmse: 0.94907 | val_1_rmse: 0.90883 |  0:00:45s
epoch 97 | loss: 0.67535 | val_0_rmse: 0.94487 | val_1_rmse: 0.90961 |  0:00:45s
epoch 98 | loss: 0.70235 | val_0_rmse: 0.9543  | val_1_rmse: 0.91908 |  0:00:46s
epoch 99 | loss: 0.70107 | val_0_rmse: 0.91525 | val_1_rmse: 0.87969 |  0:00:46s
epoch 100| loss: 0.68559 | val_0_rmse: 0.84513 | val_1_rmse: 0.82295 |  0:00:47s
epoch 101| loss: 0.69285 | val_0_rmse: 0.84442 | val_1_rmse: 0.82326 |  0:00:47s
epoch 102| loss: 0.68467 | val_0_rmse: 0.86337 | val_1_rmse: 0.83658 |  0:00:48s
epoch 103| loss: 0.68804 | val_0_rmse: 0.90611 | val_1_rmse: 0.87293 |  0:00:48s
epoch 104| loss: 0.67275 | val_0_rmse: 0.90592 | val_1_rmse: 0.87185 |  0:00:49s
epoch 105| loss: 0.6755  | val_0_rmse: 0.94892 | val_1_rmse: 0.91031 |  0:00:49s
epoch 106| loss: 0.69974 | val_0_rmse: 0.92411 | val_1_rmse: 0.88269 |  0:00:50s
epoch 107| loss: 0.67675 | val_0_rmse: 0.90398 | val_1_rmse: 0.86462 |  0:00:50s
epoch 108| loss: 0.68719 | val_0_rmse: 0.9057  | val_1_rmse: 0.86633 |  0:00:50s
epoch 109| loss: 0.67814 | val_0_rmse: 0.88821 | val_1_rmse: 0.8526  |  0:00:51s
epoch 110| loss: 0.67246 | val_0_rmse: 0.88976 | val_1_rmse: 0.85163 |  0:00:51s
epoch 111| loss: 0.66768 | val_0_rmse: 0.8796  | val_1_rmse: 0.84065 |  0:00:52s
epoch 112| loss: 0.67609 | val_0_rmse: 0.89052 | val_1_rmse: 0.84571 |  0:00:52s
epoch 113| loss: 0.67339 | val_0_rmse: 0.88734 | val_1_rmse: 0.84168 |  0:00:53s
epoch 114| loss: 0.67066 | val_0_rmse: 0.94248 | val_1_rmse: 0.89672 |  0:00:53s
epoch 115| loss: 0.67136 | val_0_rmse: 0.9246  | val_1_rmse: 0.87656 |  0:00:54s
epoch 116| loss: 0.67459 | val_0_rmse: 0.88716 | val_1_rmse: 0.83974 |  0:00:54s
epoch 117| loss: 0.66633 | val_0_rmse: 0.85184 | val_1_rmse: 0.82766 |  0:00:55s
epoch 118| loss: 0.66897 | val_0_rmse: 0.84285 | val_1_rmse: 0.82558 |  0:00:55s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.81181
Best weights from best epoch are automatically used!
ended training at: 06:33:31
Feature importance:
Mean squared error is of 6014122555.098409
Mean absolute error:55134.22950678541
MAPE:0.4269365744418534
R2 score:0.30669149929390394
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:33:31
epoch 0  | loss: 2.10702 | val_0_rmse: 1.06293 | val_1_rmse: 1.11327 |  0:00:00s
epoch 1  | loss: 2.03413 | val_0_rmse: 1.57804 | val_1_rmse: 2.26754 |  0:00:00s
epoch 2  | loss: 1.34737 | val_0_rmse: 1.10548 | val_1_rmse: 1.32711 |  0:00:01s
epoch 3  | loss: 1.06945 | val_0_rmse: 0.98536 | val_1_rmse: 1.18861 |  0:00:01s
epoch 4  | loss: 1.03503 | val_0_rmse: 1.00158 | val_1_rmse: 1.07503 |  0:00:02s
epoch 5  | loss: 1.06688 | val_0_rmse: 1.02896 | val_1_rmse: 1.12882 |  0:00:02s
epoch 6  | loss: 0.99382 | val_0_rmse: 1.00613 | val_1_rmse: 1.08108 |  0:00:03s
epoch 7  | loss: 0.97585 | val_0_rmse: 1.01586 | val_1_rmse: 1.11588 |  0:00:03s
epoch 8  | loss: 0.93873 | val_0_rmse: 1.14404 | val_1_rmse: 1.50063 |  0:00:04s
epoch 9  | loss: 0.90949 | val_0_rmse: 1.03011 | val_1_rmse: 1.21344 |  0:00:04s
epoch 10 | loss: 0.84406 | val_0_rmse: 0.95958 | val_1_rmse: 1.01122 |  0:00:05s
epoch 11 | loss: 0.80846 | val_0_rmse: 0.96091 | val_1_rmse: 1.02622 |  0:00:05s
epoch 12 | loss: 0.8289  | val_0_rmse: 0.9729  | val_1_rmse: 1.05001 |  0:00:06s
epoch 13 | loss: 0.80329 | val_0_rmse: 0.95803 | val_1_rmse: 1.00607 |  0:00:06s
epoch 14 | loss: 0.82538 | val_0_rmse: 1.0148  | val_1_rmse: 1.05629 |  0:00:07s
epoch 15 | loss: 0.77171 | val_0_rmse: 1.04931 | val_1_rmse: 1.10452 |  0:00:07s
epoch 16 | loss: 0.71352 | val_0_rmse: 1.01908 | val_1_rmse: 1.06246 |  0:00:08s
epoch 17 | loss: 0.71213 | val_0_rmse: 1.07451 | val_1_rmse: 1.10481 |  0:00:08s
epoch 18 | loss: 0.73801 | val_0_rmse: 1.04985 | val_1_rmse: 1.08963 |  0:00:08s
epoch 19 | loss: 0.72844 | val_0_rmse: 1.02472 | val_1_rmse: 1.0622  |  0:00:09s
epoch 20 | loss: 0.72127 | val_0_rmse: 1.05105 | val_1_rmse: 1.08574 |  0:00:09s
epoch 21 | loss: 0.69585 | val_0_rmse: 1.09648 | val_1_rmse: 1.12177 |  0:00:10s
epoch 22 | loss: 0.7003  | val_0_rmse: 1.03245 | val_1_rmse: 1.06982 |  0:00:10s
epoch 23 | loss: 0.69539 | val_0_rmse: 1.00414 | val_1_rmse: 1.05086 |  0:00:11s
epoch 24 | loss: 0.6722  | val_0_rmse: 1.02195 | val_1_rmse: 1.07513 |  0:00:11s
epoch 25 | loss: 0.70137 | val_0_rmse: 1.01403 | val_1_rmse: 1.0682  |  0:00:12s
epoch 26 | loss: 0.68983 | val_0_rmse: 1.01403 | val_1_rmse: 1.07024 |  0:00:12s
epoch 27 | loss: 0.67734 | val_0_rmse: 1.02441 | val_1_rmse: 1.08024 |  0:00:13s
epoch 28 | loss: 0.6738  | val_0_rmse: 1.02149 | val_1_rmse: 1.07506 |  0:00:13s
epoch 29 | loss: 0.67857 | val_0_rmse: 1.01551 | val_1_rmse: 1.06864 |  0:00:14s
epoch 30 | loss: 0.68411 | val_0_rmse: 1.01388 | val_1_rmse: 1.06025 |  0:00:14s
epoch 31 | loss: 0.66527 | val_0_rmse: 1.0305  | val_1_rmse: 1.07522 |  0:00:15s
epoch 32 | loss: 0.66894 | val_0_rmse: 1.04236 | val_1_rmse: 1.08535 |  0:00:15s
epoch 33 | loss: 0.6664  | val_0_rmse: 1.01399 | val_1_rmse: 1.06237 |  0:00:15s
epoch 34 | loss: 0.67307 | val_0_rmse: 1.03632 | val_1_rmse: 1.08536 |  0:00:16s
epoch 35 | loss: 0.66134 | val_0_rmse: 1.03611 | val_1_rmse: 1.08824 |  0:00:16s
epoch 36 | loss: 0.67331 | val_0_rmse: 1.02574 | val_1_rmse: 1.07637 |  0:00:17s
epoch 37 | loss: 0.67392 | val_0_rmse: 1.0279  | val_1_rmse: 1.07    |  0:00:17s
epoch 38 | loss: 0.67616 | val_0_rmse: 1.04893 | val_1_rmse: 1.09532 |  0:00:18s
epoch 39 | loss: 0.67674 | val_0_rmse: 1.03902 | val_1_rmse: 1.08305 |  0:00:18s
epoch 40 | loss: 0.66846 | val_0_rmse: 1.0362  | val_1_rmse: 1.07823 |  0:00:19s
epoch 41 | loss: 0.67945 | val_0_rmse: 1.05777 | val_1_rmse: 1.10278 |  0:00:19s
epoch 42 | loss: 0.67358 | val_0_rmse: 1.05815 | val_1_rmse: 1.09089 |  0:00:20s
epoch 43 | loss: 0.67898 | val_0_rmse: 1.05109 | val_1_rmse: 1.08635 |  0:00:20s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 1.00607
Best weights from best epoch are automatically used!
ended training at: 06:33:52
Feature importance:
Mean squared error is of 8037575049.5782795
Mean absolute error:65126.41018463741
MAPE:0.5319189007460501
R2 score:0.05451821858207373
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:33:52
epoch 0  | loss: 2.63169 | val_0_rmse: 1.05968 | val_1_rmse: 0.99919 |  0:00:00s
epoch 1  | loss: 1.37836 | val_0_rmse: 1.14794 | val_1_rmse: 1.04438 |  0:00:00s
epoch 2  | loss: 1.29269 | val_0_rmse: 1.35849 | val_1_rmse: 1.07631 |  0:00:01s
epoch 3  | loss: 1.06033 | val_0_rmse: 1.00909 | val_1_rmse: 0.94358 |  0:00:01s
epoch 4  | loss: 0.93968 | val_0_rmse: 1.03013 | val_1_rmse: 1.00153 |  0:00:02s
epoch 5  | loss: 0.84776 | val_0_rmse: 1.12447 | val_1_rmse: 1.04286 |  0:00:02s
epoch 6  | loss: 0.81019 | val_0_rmse: 1.24208 | val_1_rmse: 1.03019 |  0:00:03s
epoch 7  | loss: 0.77148 | val_0_rmse: 1.76522 | val_1_rmse: 1.78676 |  0:00:03s
epoch 8  | loss: 0.76543 | val_0_rmse: 2.32359 | val_1_rmse: 2.33502 |  0:00:04s
epoch 9  | loss: 0.75455 | val_0_rmse: 1.89004 | val_1_rmse: 1.88423 |  0:00:04s
epoch 10 | loss: 0.74915 | val_0_rmse: 1.32093 | val_1_rmse: 1.31946 |  0:00:05s
epoch 11 | loss: 0.74554 | val_0_rmse: 0.89828 | val_1_rmse: 0.86887 |  0:00:05s
epoch 12 | loss: 0.73541 | val_0_rmse: 1.02345 | val_1_rmse: 0.99039 |  0:00:06s
epoch 13 | loss: 0.73449 | val_0_rmse: 1.30362 | val_1_rmse: 1.32774 |  0:00:06s
epoch 14 | loss: 0.71635 | val_0_rmse: 2.1204  | val_1_rmse: 2.16854 |  0:00:07s
epoch 15 | loss: 0.73125 | val_0_rmse: 2.58233 | val_1_rmse: 2.61077 |  0:00:07s
epoch 16 | loss: 0.71778 | val_0_rmse: 2.54359 | val_1_rmse: 2.54477 |  0:00:07s
epoch 17 | loss: 0.73424 | val_0_rmse: 2.50102 | val_1_rmse: 2.5283  |  0:00:08s
epoch 18 | loss: 0.70291 | val_0_rmse: 2.55194 | val_1_rmse: 2.58396 |  0:00:08s
epoch 19 | loss: 0.742   | val_0_rmse: 2.31241 | val_1_rmse: 2.35634 |  0:00:09s
epoch 20 | loss: 0.7359  | val_0_rmse: 2.57746 | val_1_rmse: 2.62391 |  0:00:09s
epoch 21 | loss: 0.72006 | val_0_rmse: 2.56969 | val_1_rmse: 2.58144 |  0:00:10s
epoch 22 | loss: 0.72084 | val_0_rmse: 2.25363 | val_1_rmse: 2.26341 |  0:00:10s
epoch 23 | loss: 0.70284 | val_0_rmse: 2.76186 | val_1_rmse: 2.77677 |  0:00:11s
epoch 24 | loss: 0.71022 | val_0_rmse: 2.7454  | val_1_rmse: 2.78107 |  0:00:11s
epoch 25 | loss: 0.71396 | val_0_rmse: 2.23279 | val_1_rmse: 2.30969 |  0:00:12s
epoch 26 | loss: 0.71504 | val_0_rmse: 2.16275 | val_1_rmse: 2.25056 |  0:00:12s
epoch 27 | loss: 0.70016 | val_0_rmse: 1.93037 | val_1_rmse: 2.00108 |  0:00:13s
epoch 28 | loss: 0.68977 | val_0_rmse: 1.91008 | val_1_rmse: 1.97394 |  0:00:13s
epoch 29 | loss: 0.73163 | val_0_rmse: 1.51033 | val_1_rmse: 1.53662 |  0:00:14s
epoch 30 | loss: 0.71363 | val_0_rmse: 1.4685  | val_1_rmse: 1.49319 |  0:00:14s
epoch 31 | loss: 0.73279 | val_0_rmse: 0.95764 | val_1_rmse: 1.02095 |  0:00:14s
epoch 32 | loss: 0.72589 | val_0_rmse: 0.88057 | val_1_rmse: 0.86983 |  0:00:15s
epoch 33 | loss: 0.72248 | val_0_rmse: 0.85457 | val_1_rmse: 0.817   |  0:00:15s
epoch 34 | loss: 0.70508 | val_0_rmse: 0.86742 | val_1_rmse: 0.82845 |  0:00:16s
epoch 35 | loss: 0.70838 | val_0_rmse: 0.84734 | val_1_rmse: 0.8309  |  0:00:16s
epoch 36 | loss: 0.70169 | val_0_rmse: 0.93642 | val_1_rmse: 0.95025 |  0:00:17s
epoch 37 | loss: 0.7092  | val_0_rmse: 1.16342 | val_1_rmse: 1.17147 |  0:00:17s
epoch 38 | loss: 0.71387 | val_0_rmse: 1.41017 | val_1_rmse: 1.41119 |  0:00:18s
epoch 39 | loss: 0.69982 | val_0_rmse: 1.44632 | val_1_rmse: 1.45093 |  0:00:18s
epoch 40 | loss: 0.69804 | val_0_rmse: 1.41097 | val_1_rmse: 1.41466 |  0:00:19s
epoch 41 | loss: 0.69597 | val_0_rmse: 1.40133 | val_1_rmse: 1.40725 |  0:00:19s
epoch 42 | loss: 0.69718 | val_0_rmse: 1.46404 | val_1_rmse: 1.47451 |  0:00:20s
epoch 43 | loss: 0.68795 | val_0_rmse: 0.87511 | val_1_rmse: 0.87681 |  0:00:20s
epoch 44 | loss: 0.69846 | val_0_rmse: 0.87881 | val_1_rmse: 0.8387  |  0:00:21s
epoch 45 | loss: 0.68356 | val_0_rmse: 0.83685 | val_1_rmse: 0.79898 |  0:00:21s
epoch 46 | loss: 0.70005 | val_0_rmse: 0.82718 | val_1_rmse: 0.80764 |  0:00:21s
epoch 47 | loss: 0.67241 | val_0_rmse: 0.83872 | val_1_rmse: 0.80399 |  0:00:22s
epoch 48 | loss: 0.70098 | val_0_rmse: 0.89944 | val_1_rmse: 0.85852 |  0:00:22s
epoch 49 | loss: 0.71158 | val_0_rmse: 0.90126 | val_1_rmse: 0.85445 |  0:00:23s
epoch 50 | loss: 0.68639 | val_0_rmse: 0.838   | val_1_rmse: 0.79578 |  0:00:23s
epoch 51 | loss: 0.68446 | val_0_rmse: 0.82544 | val_1_rmse: 0.79608 |  0:00:24s
epoch 52 | loss: 0.67754 | val_0_rmse: 0.83665 | val_1_rmse: 0.79557 |  0:00:24s
epoch 53 | loss: 0.69615 | val_0_rmse: 0.90231 | val_1_rmse: 0.80062 |  0:00:25s
epoch 54 | loss: 0.69263 | val_0_rmse: 0.86773 | val_1_rmse: 0.82763 |  0:00:25s
epoch 55 | loss: 0.68056 | val_0_rmse: 0.90223 | val_1_rmse: 0.85036 |  0:00:26s
epoch 56 | loss: 0.68362 | val_0_rmse: 0.84902 | val_1_rmse: 0.83475 |  0:00:26s
epoch 57 | loss: 0.69163 | val_0_rmse: 0.82821 | val_1_rmse: 0.81198 |  0:00:27s
epoch 58 | loss: 0.6892  | val_0_rmse: 0.87381 | val_1_rmse: 0.83382 |  0:00:27s
epoch 59 | loss: 0.68762 | val_0_rmse: 0.89099 | val_1_rmse: 0.85238 |  0:00:28s
epoch 60 | loss: 0.6899  | val_0_rmse: 0.88275 | val_1_rmse: 0.84603 |  0:00:28s
epoch 61 | loss: 0.67918 | val_0_rmse: 0.86048 | val_1_rmse: 0.81864 |  0:00:28s
epoch 62 | loss: 0.68365 | val_0_rmse: 0.84877 | val_1_rmse: 0.80833 |  0:00:29s
epoch 63 | loss: 0.67729 | val_0_rmse: 0.86013 | val_1_rmse: 0.85334 |  0:00:29s
epoch 64 | loss: 0.67826 | val_0_rmse: 0.82874 | val_1_rmse: 0.81745 |  0:00:30s
epoch 65 | loss: 0.66966 | val_0_rmse: 1.00564 | val_1_rmse: 1.01941 |  0:00:30s
epoch 66 | loss: 0.6827  | val_0_rmse: 0.82987 | val_1_rmse: 0.80626 |  0:00:31s
epoch 67 | loss: 0.67242 | val_0_rmse: 0.88492 | val_1_rmse: 0.84652 |  0:00:31s
epoch 68 | loss: 0.67136 | val_0_rmse: 0.89615 | val_1_rmse: 0.85047 |  0:00:32s
epoch 69 | loss: 0.67205 | val_0_rmse: 0.87174 | val_1_rmse: 0.84974 |  0:00:32s
epoch 70 | loss: 0.66614 | val_0_rmse: 0.86605 | val_1_rmse: 0.83038 |  0:00:33s
epoch 71 | loss: 0.66868 | val_0_rmse: 0.89397 | val_1_rmse: 0.85294 |  0:00:33s
epoch 72 | loss: 0.66875 | val_0_rmse: 0.8307  | val_1_rmse: 0.80469 |  0:00:34s
epoch 73 | loss: 0.66987 | val_0_rmse: 0.8565  | val_1_rmse: 0.81945 |  0:00:34s
epoch 74 | loss: 0.66834 | val_0_rmse: 0.86715 | val_1_rmse: 0.8354  |  0:00:34s
epoch 75 | loss: 0.67031 | val_0_rmse: 1.00094 | val_1_rmse: 0.8297  |  0:00:35s
epoch 76 | loss: 0.66474 | val_0_rmse: 0.84799 | val_1_rmse: 0.81817 |  0:00:35s
epoch 77 | loss: 0.65847 | val_0_rmse: 0.83156 | val_1_rmse: 0.81065 |  0:00:36s
epoch 78 | loss: 0.67762 | val_0_rmse: 0.87109 | val_1_rmse: 0.83216 |  0:00:36s
epoch 79 | loss: 0.66444 | val_0_rmse: 0.87329 | val_1_rmse: 0.85119 |  0:00:37s
epoch 80 | loss: 0.67579 | val_0_rmse: 0.87087 | val_1_rmse: 0.83865 |  0:00:37s
epoch 81 | loss: 0.67996 | val_0_rmse: 0.85383 | val_1_rmse: 0.83035 |  0:00:38s
epoch 82 | loss: 0.67828 | val_0_rmse: 0.84801 | val_1_rmse: 0.84072 |  0:00:38s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.79557
Best weights from best epoch are automatically used!
ended training at: 06:34:31
Feature importance:
Mean squared error is of 5402588487.809641
Mean absolute error:52124.18522878234
MAPE:0.41206159955184407
R2 score:0.3349946914035613
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:34:31
epoch 0  | loss: 2.56739 | val_0_rmse: 1.04714 | val_1_rmse: 1.084   |  0:00:00s
epoch 1  | loss: 1.24534 | val_0_rmse: 0.99308 | val_1_rmse: 1.01848 |  0:00:00s
epoch 2  | loss: 1.08568 | val_0_rmse: 1.08808 | val_1_rmse: 1.11255 |  0:00:01s
epoch 3  | loss: 0.97972 | val_0_rmse: 1.31992 | val_1_rmse: 1.3416  |  0:00:01s
epoch 4  | loss: 0.94282 | val_0_rmse: 1.30819 | val_1_rmse: 1.33425 |  0:00:02s
epoch 5  | loss: 0.86638 | val_0_rmse: 1.0046  | val_1_rmse: 1.03754 |  0:00:02s
epoch 6  | loss: 0.88163 | val_0_rmse: 1.0402  | val_1_rmse: 1.07086 |  0:00:03s
epoch 7  | loss: 0.80362 | val_0_rmse: 0.98612 | val_1_rmse: 1.02252 |  0:00:03s
epoch 8  | loss: 0.81525 | val_0_rmse: 1.2383  | val_1_rmse: 1.25777 |  0:00:04s
epoch 9  | loss: 0.79647 | val_0_rmse: 1.15867 | val_1_rmse: 1.18002 |  0:00:04s
epoch 10 | loss: 0.8197  | val_0_rmse: 0.99908 | val_1_rmse: 1.02859 |  0:00:05s
epoch 11 | loss: 0.8184  | val_0_rmse: 1.04236 | val_1_rmse: 1.07216 |  0:00:05s
epoch 12 | loss: 0.79058 | val_0_rmse: 1.06037 | val_1_rmse: 1.09082 |  0:00:06s
epoch 13 | loss: 0.78307 | val_0_rmse: 1.04362 | val_1_rmse: 1.07502 |  0:00:06s
epoch 14 | loss: 0.73889 | val_0_rmse: 1.07719 | val_1_rmse: 1.10767 |  0:00:07s
epoch 15 | loss: 0.74156 | val_0_rmse: 1.03501 | val_1_rmse: 1.05304 |  0:00:07s
epoch 16 | loss: 0.76487 | val_0_rmse: 0.98038 | val_1_rmse: 1.00521 |  0:00:07s
epoch 17 | loss: 0.75735 | val_0_rmse: 0.92716 | val_1_rmse: 0.96349 |  0:00:08s
epoch 18 | loss: 0.74456 | val_0_rmse: 0.91533 | val_1_rmse: 0.95067 |  0:00:08s
epoch 19 | loss: 0.7296  | val_0_rmse: 0.90945 | val_1_rmse: 0.94233 |  0:00:09s
epoch 20 | loss: 0.74662 | val_0_rmse: 0.90513 | val_1_rmse: 0.95556 |  0:00:09s
epoch 21 | loss: 0.74224 | val_0_rmse: 0.92437 | val_1_rmse: 0.95617 |  0:00:10s
epoch 22 | loss: 0.74307 | val_0_rmse: 0.91642 | val_1_rmse: 0.94543 |  0:00:10s
epoch 23 | loss: 0.73673 | val_0_rmse: 0.92915 | val_1_rmse: 0.96293 |  0:00:11s
epoch 24 | loss: 0.74482 | val_0_rmse: 0.95344 | val_1_rmse: 0.98763 |  0:00:11s
epoch 25 | loss: 0.73308 | val_0_rmse: 0.91058 | val_1_rmse: 0.94872 |  0:00:12s
epoch 26 | loss: 0.73347 | val_0_rmse: 0.90026 | val_1_rmse: 0.93724 |  0:00:12s
epoch 27 | loss: 0.72053 | val_0_rmse: 0.89249 | val_1_rmse: 0.92774 |  0:00:13s
epoch 28 | loss: 0.71567 | val_0_rmse: 0.9079  | val_1_rmse: 0.94015 |  0:00:13s
epoch 29 | loss: 0.73708 | val_0_rmse: 0.89947 | val_1_rmse: 0.93201 |  0:00:14s
epoch 30 | loss: 0.72634 | val_0_rmse: 0.90717 | val_1_rmse: 0.94172 |  0:00:14s
epoch 31 | loss: 0.7136  | val_0_rmse: 0.88547 | val_1_rmse: 0.92376 |  0:00:15s
epoch 32 | loss: 0.7149  | val_0_rmse: 0.8878  | val_1_rmse: 0.92513 |  0:00:15s
epoch 33 | loss: 0.70428 | val_0_rmse: 0.89014 | val_1_rmse: 0.92926 |  0:00:16s
epoch 34 | loss: 0.69353 | val_0_rmse: 0.88862 | val_1_rmse: 0.92773 |  0:00:16s
epoch 35 | loss: 0.68591 | val_0_rmse: 0.88187 | val_1_rmse: 0.922   |  0:00:16s
epoch 36 | loss: 0.66855 | val_0_rmse: 0.87454 | val_1_rmse: 0.91277 |  0:00:17s
epoch 37 | loss: 0.69111 | val_0_rmse: 0.90813 | val_1_rmse: 0.94388 |  0:00:17s
epoch 38 | loss: 0.70963 | val_0_rmse: 0.88217 | val_1_rmse: 0.91966 |  0:00:18s
epoch 39 | loss: 0.69646 | val_0_rmse: 0.87069 | val_1_rmse: 0.91411 |  0:00:18s
epoch 40 | loss: 0.68549 | val_0_rmse: 0.866   | val_1_rmse: 0.91163 |  0:00:19s
epoch 41 | loss: 0.67856 | val_0_rmse: 0.8627  | val_1_rmse: 0.90867 |  0:00:19s
epoch 42 | loss: 0.70102 | val_0_rmse: 0.85619 | val_1_rmse: 0.90144 |  0:00:20s
epoch 43 | loss: 0.67541 | val_0_rmse: 0.8468  | val_1_rmse: 0.89514 |  0:00:20s
epoch 44 | loss: 0.67715 | val_0_rmse: 0.84418 | val_1_rmse: 0.89331 |  0:00:21s
epoch 45 | loss: 0.68368 | val_0_rmse: 0.87929 | val_1_rmse: 0.92407 |  0:00:21s
epoch 46 | loss: 0.69248 | val_0_rmse: 0.86438 | val_1_rmse: 0.90898 |  0:00:22s
epoch 47 | loss: 0.69275 | val_0_rmse: 0.87499 | val_1_rmse: 0.92061 |  0:00:22s
epoch 48 | loss: 0.67284 | val_0_rmse: 0.90472 | val_1_rmse: 0.95045 |  0:00:23s
epoch 49 | loss: 0.67843 | val_0_rmse: 0.87447 | val_1_rmse: 0.92042 |  0:00:23s
epoch 50 | loss: 0.66264 | val_0_rmse: 0.90251 | val_1_rmse: 0.94688 |  0:00:24s
epoch 51 | loss: 0.66782 | val_0_rmse: 0.85905 | val_1_rmse: 0.90335 |  0:00:24s
epoch 52 | loss: 0.67133 | val_0_rmse: 0.88486 | val_1_rmse: 0.92507 |  0:00:25s
epoch 53 | loss: 0.65911 | val_0_rmse: 0.88343 | val_1_rmse: 0.92484 |  0:00:25s
epoch 54 | loss: 0.66028 | val_0_rmse: 0.89832 | val_1_rmse: 0.93592 |  0:00:25s
epoch 55 | loss: 0.66059 | val_0_rmse: 0.89017 | val_1_rmse: 0.92999 |  0:00:26s
epoch 56 | loss: 0.66599 | val_0_rmse: 0.88922 | val_1_rmse: 0.92975 |  0:00:26s
epoch 57 | loss: 0.67851 | val_0_rmse: 0.8853  | val_1_rmse: 0.92561 |  0:00:27s
epoch 58 | loss: 0.66703 | val_0_rmse: 0.84611 | val_1_rmse: 0.89405 |  0:00:27s
epoch 59 | loss: 0.70484 | val_0_rmse: 0.88695 | val_1_rmse: 0.9349  |  0:00:28s
epoch 60 | loss: 0.67734 | val_0_rmse: 0.87447 | val_1_rmse: 0.92142 |  0:00:28s
epoch 61 | loss: 0.68122 | val_0_rmse: 0.86648 | val_1_rmse: 0.9131  |  0:00:29s
epoch 62 | loss: 0.66818 | val_0_rmse: 0.87757 | val_1_rmse: 0.92422 |  0:00:29s
epoch 63 | loss: 0.66968 | val_0_rmse: 0.86528 | val_1_rmse: 0.91103 |  0:00:30s
epoch 64 | loss: 0.65934 | val_0_rmse: 0.85997 | val_1_rmse: 0.90562 |  0:00:30s
epoch 65 | loss: 0.65566 | val_0_rmse: 0.87887 | val_1_rmse: 0.92133 |  0:00:31s
epoch 66 | loss: 0.66186 | val_0_rmse: 0.86639 | val_1_rmse: 0.91286 |  0:00:31s
epoch 67 | loss: 0.66508 | val_0_rmse: 0.89422 | val_1_rmse: 0.93708 |  0:00:31s
epoch 68 | loss: 0.66069 | val_0_rmse: 0.86288 | val_1_rmse: 0.90608 |  0:00:32s
epoch 69 | loss: 0.66009 | val_0_rmse: 0.87749 | val_1_rmse: 0.91894 |  0:00:32s
epoch 70 | loss: 0.65276 | val_0_rmse: 0.8849  | val_1_rmse: 0.93033 |  0:00:33s
epoch 71 | loss: 0.6534  | val_0_rmse: 0.87634 | val_1_rmse: 0.92179 |  0:00:33s
epoch 72 | loss: 0.65141 | val_0_rmse: 0.87707 | val_1_rmse: 0.91782 |  0:00:34s
epoch 73 | loss: 0.65663 | val_0_rmse: 0.88173 | val_1_rmse: 0.9246  |  0:00:34s
epoch 74 | loss: 0.64909 | val_0_rmse: 0.87283 | val_1_rmse: 0.91682 |  0:00:35s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.89331
Best weights from best epoch are automatically used!
ended training at: 06:35:06
Feature importance:
Mean squared error is of 6094573741.421356
Mean absolute error:57444.494130619154
MAPE:0.464089195225866
R2 score:0.2783406045020467
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:35:06
epoch 0  | loss: 2.33837 | val_0_rmse: 1.05933 | val_1_rmse: 1.10521 |  0:00:00s
epoch 1  | loss: 1.65222 | val_0_rmse: 1.05423 | val_1_rmse: 1.06384 |  0:00:00s
epoch 2  | loss: 1.1415  | val_0_rmse: 1.10511 | val_1_rmse: 1.10369 |  0:00:01s
epoch 3  | loss: 0.9807  | val_0_rmse: 1.00055 | val_1_rmse: 1.04196 |  0:00:01s
epoch 4  | loss: 0.96889 | val_0_rmse: 0.99894 | val_1_rmse: 1.03436 |  0:00:02s
epoch 5  | loss: 0.91442 | val_0_rmse: 1.00248 | val_1_rmse: 1.04785 |  0:00:02s
epoch 6  | loss: 0.80925 | val_0_rmse: 1.04419 | val_1_rmse: 1.09138 |  0:00:03s
epoch 7  | loss: 0.80893 | val_0_rmse: 1.04142 | val_1_rmse: 1.10016 |  0:00:03s
epoch 8  | loss: 0.75783 | val_0_rmse: 1.23191 | val_1_rmse: 1.29872 |  0:00:04s
epoch 9  | loss: 0.73717 | val_0_rmse: 1.08911 | val_1_rmse: 1.15082 |  0:00:04s
epoch 10 | loss: 0.70405 | val_0_rmse: 1.07235 | val_1_rmse: 1.13091 |  0:00:05s
epoch 11 | loss: 0.70151 | val_0_rmse: 1.07621 | val_1_rmse: 1.1317  |  0:00:05s
epoch 12 | loss: 0.69035 | val_0_rmse: 1.17061 | val_1_rmse: 1.22988 |  0:00:06s
epoch 13 | loss: 0.67512 | val_0_rmse: 1.15767 | val_1_rmse: 1.21979 |  0:00:06s
epoch 14 | loss: 0.68089 | val_0_rmse: 1.17031 | val_1_rmse: 1.23507 |  0:00:07s
epoch 15 | loss: 0.67168 | val_0_rmse: 1.21493 | val_1_rmse: 1.2858  |  0:00:07s
epoch 16 | loss: 0.66496 | val_0_rmse: 1.17808 | val_1_rmse: 1.2584  |  0:00:07s
epoch 17 | loss: 0.66398 | val_0_rmse: 1.12762 | val_1_rmse: 1.20729 |  0:00:08s
epoch 18 | loss: 0.6578  | val_0_rmse: 1.12531 | val_1_rmse: 1.17759 |  0:00:08s
epoch 19 | loss: 0.668   | val_0_rmse: 1.07546 | val_1_rmse: 1.15398 |  0:00:09s
epoch 20 | loss: 0.64507 | val_0_rmse: 1.03605 | val_1_rmse: 1.11918 |  0:00:09s
epoch 21 | loss: 0.65973 | val_0_rmse: 0.99791 | val_1_rmse: 1.07829 |  0:00:10s
epoch 22 | loss: 0.65359 | val_0_rmse: 0.97788 | val_1_rmse: 1.05316 |  0:00:10s
epoch 23 | loss: 0.65229 | val_0_rmse: 1.02484 | val_1_rmse: 1.09931 |  0:00:11s
epoch 24 | loss: 0.64897 | val_0_rmse: 1.03235 | val_1_rmse: 1.1103  |  0:00:11s
epoch 25 | loss: 0.66236 | val_0_rmse: 0.96423 | val_1_rmse: 1.04784 |  0:00:12s
epoch 26 | loss: 0.65348 | val_0_rmse: 0.98606 | val_1_rmse: 1.06955 |  0:00:12s
epoch 27 | loss: 0.64732 | val_0_rmse: 0.96828 | val_1_rmse: 1.05264 |  0:00:13s
epoch 28 | loss: 0.64986 | val_0_rmse: 0.96679 | val_1_rmse: 1.0523  |  0:00:13s
epoch 29 | loss: 0.64672 | val_0_rmse: 0.9537  | val_1_rmse: 1.03116 |  0:00:14s
epoch 30 | loss: 0.65035 | val_0_rmse: 0.92985 | val_1_rmse: 1.00034 |  0:00:14s
epoch 31 | loss: 0.65449 | val_0_rmse: 0.8948  | val_1_rmse: 0.97099 |  0:00:14s
epoch 32 | loss: 0.63981 | val_0_rmse: 0.88194 | val_1_rmse: 0.9372  |  0:00:15s
epoch 33 | loss: 0.6507  | val_0_rmse: 0.92164 | val_1_rmse: 0.99941 |  0:00:15s
epoch 34 | loss: 0.63744 | val_0_rmse: 0.86413 | val_1_rmse: 0.93112 |  0:00:16s
epoch 35 | loss: 0.62923 | val_0_rmse: 0.86726 | val_1_rmse: 0.91582 |  0:00:16s
epoch 36 | loss: 0.65127 | val_0_rmse: 0.89142 | val_1_rmse: 0.96393 |  0:00:17s
epoch 37 | loss: 0.65534 | val_0_rmse: 0.86898 | val_1_rmse: 0.9348  |  0:00:17s
epoch 38 | loss: 0.63922 | val_0_rmse: 0.89971 | val_1_rmse: 0.96732 |  0:00:18s
epoch 39 | loss: 0.6342  | val_0_rmse: 0.88809 | val_1_rmse: 0.95768 |  0:00:18s
epoch 40 | loss: 0.64113 | val_0_rmse: 0.87426 | val_1_rmse: 0.9466  |  0:00:19s
epoch 41 | loss: 0.64104 | val_0_rmse: 0.88229 | val_1_rmse: 0.95792 |  0:00:19s
epoch 42 | loss: 0.6344  | val_0_rmse: 0.87533 | val_1_rmse: 0.95133 |  0:00:20s
epoch 43 | loss: 0.64387 | val_0_rmse: 0.89186 | val_1_rmse: 0.96788 |  0:00:20s
epoch 44 | loss: 0.63208 | val_0_rmse: 0.86938 | val_1_rmse: 0.93819 |  0:00:21s
epoch 45 | loss: 0.63693 | val_0_rmse: 0.87094 | val_1_rmse: 0.94342 |  0:00:21s
epoch 46 | loss: 0.65023 | val_0_rmse: 0.89431 | val_1_rmse: 0.94731 |  0:00:22s
epoch 47 | loss: 0.64002 | val_0_rmse: 0.8398  | val_1_rmse: 0.88884 |  0:00:22s
epoch 48 | loss: 0.63787 | val_0_rmse: 0.9523  | val_1_rmse: 1.01742 |  0:00:22s
epoch 49 | loss: 0.64438 | val_0_rmse: 0.95124 | val_1_rmse: 0.91651 |  0:00:23s
epoch 50 | loss: 0.6814  | val_0_rmse: 0.97824 | val_1_rmse: 0.95948 |  0:00:23s
epoch 51 | loss: 0.67617 | val_0_rmse: 0.96031 | val_1_rmse: 0.99723 |  0:00:24s
epoch 52 | loss: 0.71064 | val_0_rmse: 0.98808 | val_1_rmse: 1.0549  |  0:00:24s
epoch 53 | loss: 0.72015 | val_0_rmse: 0.98006 | val_1_rmse: 1.04667 |  0:00:25s
epoch 54 | loss: 0.71961 | val_0_rmse: 0.94899 | val_1_rmse: 1.00868 |  0:00:25s
epoch 55 | loss: 0.70036 | val_0_rmse: 0.97572 | val_1_rmse: 1.04135 |  0:00:26s
epoch 56 | loss: 0.68381 | val_0_rmse: 0.93534 | val_1_rmse: 1.01103 |  0:00:26s
epoch 57 | loss: 0.6734  | val_0_rmse: 1.00143 | val_1_rmse: 1.06613 |  0:00:27s
epoch 58 | loss: 0.67475 | val_0_rmse: 0.98702 | val_1_rmse: 1.07212 |  0:00:27s
epoch 59 | loss: 0.67474 | val_0_rmse: 0.96503 | val_1_rmse: 1.0459  |  0:00:28s
epoch 60 | loss: 0.69491 | val_0_rmse: 0.99001 | val_1_rmse: 1.07455 |  0:00:28s
epoch 61 | loss: 0.67644 | val_0_rmse: 0.96767 | val_1_rmse: 1.05407 |  0:00:28s
epoch 62 | loss: 0.65964 | val_0_rmse: 0.89671 | val_1_rmse: 0.96828 |  0:00:29s
epoch 63 | loss: 0.65393 | val_0_rmse: 0.92198 | val_1_rmse: 0.99534 |  0:00:29s
epoch 64 | loss: 0.6501  | val_0_rmse: 0.90356 | val_1_rmse: 0.97559 |  0:00:30s
epoch 65 | loss: 0.66055 | val_0_rmse: 0.88625 | val_1_rmse: 0.95678 |  0:00:30s
epoch 66 | loss: 0.63749 | val_0_rmse: 0.88544 | val_1_rmse: 0.95934 |  0:00:31s
epoch 67 | loss: 0.62974 | val_0_rmse: 0.92146 | val_1_rmse: 0.99631 |  0:00:31s
epoch 68 | loss: 0.6286  | val_0_rmse: 0.88686 | val_1_rmse: 0.96355 |  0:00:32s
epoch 69 | loss: 0.62136 | val_0_rmse: 0.90139 | val_1_rmse: 0.97991 |  0:00:32s
epoch 70 | loss: 0.64039 | val_0_rmse: 0.89963 | val_1_rmse: 0.97521 |  0:00:33s
epoch 71 | loss: 0.63408 | val_0_rmse: 0.9432  | val_1_rmse: 1.01954 |  0:00:33s
epoch 72 | loss: 0.63782 | val_0_rmse: 0.88768 | val_1_rmse: 0.96092 |  0:00:34s
epoch 73 | loss: 0.64031 | val_0_rmse: 0.87006 | val_1_rmse: 0.94383 |  0:00:34s
epoch 74 | loss: 0.63518 | val_0_rmse: 0.89148 | val_1_rmse: 0.96694 |  0:00:35s
epoch 75 | loss: 0.63635 | val_0_rmse: 0.89063 | val_1_rmse: 0.95893 |  0:00:35s
epoch 76 | loss: 0.63032 | val_0_rmse: 0.91289 | val_1_rmse: 0.98237 |  0:00:36s
epoch 77 | loss: 0.63016 | val_0_rmse: 0.90407 | val_1_rmse: 0.97695 |  0:00:36s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.88884
Best weights from best epoch are automatically used!
ended training at: 06:35:43
Feature importance:
Mean squared error is of 6141955024.610013
Mean absolute error:54771.18862362171
MAPE:0.4373921855381395
R2 score:0.22344440738619653
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:35:43
epoch 0  | loss: 2.29826 | val_0_rmse: 1.00203 | val_1_rmse: 1.08574 |  0:00:00s
epoch 1  | loss: 1.42259 | val_0_rmse: 1.04033 | val_1_rmse: 1.0773  |  0:00:00s
epoch 2  | loss: 1.16309 | val_0_rmse: 0.97943 | val_1_rmse: 1.01026 |  0:00:01s
epoch 3  | loss: 0.95394 | val_0_rmse: 0.96861 | val_1_rmse: 0.98902 |  0:00:01s
epoch 4  | loss: 0.87697 | val_0_rmse: 1.04036 | val_1_rmse: 1.09112 |  0:00:02s
epoch 5  | loss: 0.77652 | val_0_rmse: 1.20039 | val_1_rmse: 1.2887  |  0:00:02s
epoch 6  | loss: 0.75275 | val_0_rmse: 1.0897  | val_1_rmse: 1.29145 |  0:00:03s
epoch 7  | loss: 0.77335 | val_0_rmse: 0.95542 | val_1_rmse: 1.06006 |  0:00:03s
epoch 8  | loss: 0.76621 | val_0_rmse: 1.01558 | val_1_rmse: 1.19011 |  0:00:04s
epoch 9  | loss: 0.71301 | val_0_rmse: 1.12577 | val_1_rmse: 1.39609 |  0:00:04s
epoch 10 | loss: 0.70719 | val_0_rmse: 1.10065 | val_1_rmse: 1.35208 |  0:00:05s
epoch 11 | loss: 0.70013 | val_0_rmse: 1.05932 | val_1_rmse: 1.19465 |  0:00:05s
epoch 12 | loss: 0.68377 | val_0_rmse: 1.01799 | val_1_rmse: 1.08243 |  0:00:06s
epoch 13 | loss: 0.68912 | val_0_rmse: 1.0054  | val_1_rmse: 1.0406  |  0:00:06s
epoch 14 | loss: 0.67726 | val_0_rmse: 1.02425 | val_1_rmse: 1.05122 |  0:00:07s
epoch 15 | loss: 0.66796 | val_0_rmse: 1.04148 | val_1_rmse: 1.06706 |  0:00:07s
epoch 16 | loss: 0.68191 | val_0_rmse: 0.97057 | val_1_rmse: 0.98    |  0:00:07s
epoch 17 | loss: 0.70126 | val_0_rmse: 0.9147  | val_1_rmse: 0.93055 |  0:00:08s
epoch 18 | loss: 0.68783 | val_0_rmse: 0.89675 | val_1_rmse: 0.92374 |  0:00:08s
epoch 19 | loss: 0.67098 | val_0_rmse: 0.94017 | val_1_rmse: 0.95526 |  0:00:09s
epoch 20 | loss: 0.67036 | val_0_rmse: 0.92337 | val_1_rmse: 0.95622 |  0:00:09s
epoch 21 | loss: 0.66632 | val_0_rmse: 0.90928 | val_1_rmse: 0.94634 |  0:00:10s
epoch 22 | loss: 0.67459 | val_0_rmse: 0.87013 | val_1_rmse: 0.91373 |  0:00:10s
epoch 23 | loss: 0.68523 | val_0_rmse: 0.89084 | val_1_rmse: 0.91666 |  0:00:11s
epoch 24 | loss: 0.69506 | val_0_rmse: 0.88756 | val_1_rmse: 0.89118 |  0:00:11s
epoch 25 | loss: 0.69526 | val_0_rmse: 0.86785 | val_1_rmse: 0.87425 |  0:00:12s
epoch 26 | loss: 0.68261 | val_0_rmse: 0.93399 | val_1_rmse: 0.93586 |  0:00:12s
epoch 27 | loss: 0.68939 | val_0_rmse: 0.90613 | val_1_rmse: 0.91012 |  0:00:13s
epoch 28 | loss: 0.67186 | val_0_rmse: 0.87941 | val_1_rmse: 0.88167 |  0:00:13s
epoch 29 | loss: 0.67174 | val_0_rmse: 0.90117 | val_1_rmse: 0.90008 |  0:00:14s
epoch 30 | loss: 0.6764  | val_0_rmse: 0.91004 | val_1_rmse: 0.92035 |  0:00:14s
epoch 31 | loss: 0.66551 | val_0_rmse: 0.96007 | val_1_rmse: 0.96524 |  0:00:15s
epoch 32 | loss: 0.66649 | val_0_rmse: 0.93057 | val_1_rmse: 0.94475 |  0:00:15s
epoch 33 | loss: 0.65808 | val_0_rmse: 0.92955 | val_1_rmse: 0.96069 |  0:00:16s
epoch 34 | loss: 0.67107 | val_0_rmse: 0.90114 | val_1_rmse: 0.94715 |  0:00:16s
epoch 35 | loss: 0.68398 | val_0_rmse: 0.87977 | val_1_rmse: 0.93253 |  0:00:16s
epoch 36 | loss: 0.66534 | val_0_rmse: 0.85414 | val_1_rmse: 0.88115 |  0:00:17s
epoch 37 | loss: 0.6676  | val_0_rmse: 0.85428 | val_1_rmse: 0.88579 |  0:00:17s
epoch 38 | loss: 0.67469 | val_0_rmse: 0.90557 | val_1_rmse: 0.93262 |  0:00:18s
epoch 39 | loss: 0.65814 | val_0_rmse: 1.01232 | val_1_rmse: 1.03356 |  0:00:18s
epoch 40 | loss: 0.67491 | val_0_rmse: 0.90666 | val_1_rmse: 0.92701 |  0:00:19s
epoch 41 | loss: 0.67065 | val_0_rmse: 0.82521 | val_1_rmse: 0.83541 |  0:00:19s
epoch 42 | loss: 0.66283 | val_0_rmse: 0.85156 | val_1_rmse: 0.8736  |  0:00:20s
epoch 43 | loss: 0.68006 | val_0_rmse: 0.84402 | val_1_rmse: 0.8572  |  0:00:20s
epoch 44 | loss: 0.66513 | val_0_rmse: 0.86391 | val_1_rmse: 0.87011 |  0:00:21s
epoch 45 | loss: 0.67353 | val_0_rmse: 0.83544 | val_1_rmse: 0.8423  |  0:00:21s
epoch 46 | loss: 0.68017 | val_0_rmse: 0.82881 | val_1_rmse: 0.83068 |  0:00:22s
epoch 47 | loss: 0.65884 | val_0_rmse: 0.82526 | val_1_rmse: 0.83598 |  0:00:22s
epoch 48 | loss: 0.66693 | val_0_rmse: 0.83912 | val_1_rmse: 0.83666 |  0:00:23s
epoch 49 | loss: 0.65489 | val_0_rmse: 0.83094 | val_1_rmse: 0.83239 |  0:00:23s
epoch 50 | loss: 0.66151 | val_0_rmse: 0.87017 | val_1_rmse: 0.8683  |  0:00:23s
epoch 51 | loss: 0.65988 | val_0_rmse: 0.84877 | val_1_rmse: 0.85097 |  0:00:24s
epoch 52 | loss: 0.65803 | val_0_rmse: 0.8696  | val_1_rmse: 0.88333 |  0:00:24s
epoch 53 | loss: 0.65227 | val_0_rmse: 0.86353 | val_1_rmse: 0.86617 |  0:00:25s
epoch 54 | loss: 0.65753 | val_0_rmse: 0.86757 | val_1_rmse: 0.88275 |  0:00:25s
epoch 55 | loss: 0.64839 | val_0_rmse: 0.86543 | val_1_rmse: 0.88223 |  0:00:26s
epoch 56 | loss: 0.64926 | val_0_rmse: 0.85282 | val_1_rmse: 0.868   |  0:00:26s
epoch 57 | loss: 0.64279 | val_0_rmse: 0.86846 | val_1_rmse: 0.86872 |  0:00:27s
epoch 58 | loss: 0.64131 | val_0_rmse: 0.91692 | val_1_rmse: 0.88667 |  0:00:27s
epoch 59 | loss: 0.6523  | val_0_rmse: 0.92165 | val_1_rmse: 0.88384 |  0:00:28s
epoch 60 | loss: 0.64618 | val_0_rmse: 0.87615 | val_1_rmse: 0.86573 |  0:00:28s
epoch 61 | loss: 0.64273 | val_0_rmse: 0.88693 | val_1_rmse: 0.87207 |  0:00:29s
epoch 62 | loss: 0.64621 | val_0_rmse: 0.97524 | val_1_rmse: 0.88007 |  0:00:29s
epoch 63 | loss: 0.64503 | val_0_rmse: 0.91671 | val_1_rmse: 0.86918 |  0:00:30s
epoch 64 | loss: 0.64301 | val_0_rmse: 0.94893 | val_1_rmse: 0.88175 |  0:00:30s
epoch 65 | loss: 0.64008 | val_0_rmse: 0.95092 | val_1_rmse: 0.89793 |  0:00:30s
epoch 66 | loss: 0.65074 | val_0_rmse: 0.90955 | val_1_rmse: 0.89176 |  0:00:31s
epoch 67 | loss: 0.64426 | val_0_rmse: 0.89007 | val_1_rmse: 0.91186 |  0:00:31s
epoch 68 | loss: 0.64618 | val_0_rmse: 0.92117 | val_1_rmse: 0.94472 |  0:00:32s
epoch 69 | loss: 0.64712 | val_0_rmse: 0.92077 | val_1_rmse: 0.94458 |  0:00:32s
epoch 70 | loss: 0.63923 | val_0_rmse: 0.86368 | val_1_rmse: 0.89128 |  0:00:33s
epoch 71 | loss: 0.64086 | val_0_rmse: 0.82553 | val_1_rmse: 0.85155 |  0:00:33s
epoch 72 | loss: 0.64669 | val_0_rmse: 0.82054 | val_1_rmse: 0.84573 |  0:00:34s
epoch 73 | loss: 0.64958 | val_0_rmse: 0.8547  | val_1_rmse: 0.88196 |  0:00:34s
epoch 74 | loss: 0.64708 | val_0_rmse: 0.88413 | val_1_rmse: 0.90993 |  0:00:35s
epoch 75 | loss: 0.63651 | val_0_rmse: 0.85687 | val_1_rmse: 0.88822 |  0:00:35s
epoch 76 | loss: 0.63923 | val_0_rmse: 0.85874 | val_1_rmse: 0.89037 |  0:00:36s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.83068
Best weights from best epoch are automatically used!
ended training at: 06:36:19
Feature importance:
Mean squared error is of 5679560162.616998
Mean absolute error:55334.033163963104
MAPE:0.45342041701520697
R2 score:0.3022151295260185
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:36:19
epoch 0  | loss: 2.27172 | val_0_rmse: 1.01174 | val_1_rmse: 1.00387 |  0:00:00s
epoch 1  | loss: 1.9349  | val_0_rmse: 1.15418 | val_1_rmse: 1.02934 |  0:00:00s
epoch 2  | loss: 1.30703 | val_0_rmse: 1.08116 | val_1_rmse: 1.04292 |  0:00:01s
epoch 3  | loss: 1.06837 | val_0_rmse: 1.01796 | val_1_rmse: 0.99969 |  0:00:01s
epoch 4  | loss: 1.00643 | val_0_rmse: 1.00743 | val_1_rmse: 0.99081 |  0:00:02s
epoch 5  | loss: 0.94508 | val_0_rmse: 1.01091 | val_1_rmse: 0.98897 |  0:00:02s
epoch 6  | loss: 0.94698 | val_0_rmse: 0.99987 | val_1_rmse: 0.98635 |  0:00:03s
epoch 7  | loss: 0.92259 | val_0_rmse: 1.0034  | val_1_rmse: 0.98874 |  0:00:03s
epoch 8  | loss: 0.89444 | val_0_rmse: 1.0237  | val_1_rmse: 1.01013 |  0:00:04s
epoch 9  | loss: 0.83351 | val_0_rmse: 1.01622 | val_1_rmse: 0.99408 |  0:00:04s
epoch 10 | loss: 0.81016 | val_0_rmse: 1.00162 | val_1_rmse: 0.97602 |  0:00:05s
epoch 11 | loss: 0.78803 | val_0_rmse: 0.99561 | val_1_rmse: 0.9695  |  0:00:05s
epoch 12 | loss: 0.78229 | val_0_rmse: 1.01501 | val_1_rmse: 0.9834  |  0:00:06s
epoch 13 | loss: 0.78494 | val_0_rmse: 1.01302 | val_1_rmse: 0.9899  |  0:00:06s
epoch 14 | loss: 0.76785 | val_0_rmse: 1.01629 | val_1_rmse: 0.97755 |  0:00:07s
epoch 15 | loss: 0.76164 | val_0_rmse: 1.11243 | val_1_rmse: 1.06648 |  0:00:07s
epoch 16 | loss: 0.76646 | val_0_rmse: 0.98972 | val_1_rmse: 0.95533 |  0:00:08s
epoch 17 | loss: 0.77894 | val_0_rmse: 0.95688 | val_1_rmse: 0.92863 |  0:00:08s
epoch 18 | loss: 0.75065 | val_0_rmse: 0.96033 | val_1_rmse: 0.93523 |  0:00:08s
epoch 19 | loss: 0.75432 | val_0_rmse: 0.98779 | val_1_rmse: 0.96766 |  0:00:09s
epoch 20 | loss: 0.75186 | val_0_rmse: 0.99766 | val_1_rmse: 0.97728 |  0:00:09s
epoch 21 | loss: 0.75392 | val_0_rmse: 0.99396 | val_1_rmse: 0.97523 |  0:00:10s
epoch 22 | loss: 0.74444 | val_0_rmse: 0.98313 | val_1_rmse: 0.96598 |  0:00:10s
epoch 23 | loss: 0.74464 | val_0_rmse: 1.01743 | val_1_rmse: 0.99838 |  0:00:11s
epoch 24 | loss: 0.73043 | val_0_rmse: 0.93658 | val_1_rmse: 0.9092  |  0:00:11s
epoch 25 | loss: 0.73777 | val_0_rmse: 0.94238 | val_1_rmse: 0.91588 |  0:00:12s
epoch 26 | loss: 0.72847 | val_0_rmse: 0.93016 | val_1_rmse: 0.90363 |  0:00:12s
epoch 27 | loss: 0.718   | val_0_rmse: 0.92162 | val_1_rmse: 0.8889  |  0:00:13s
epoch 28 | loss: 0.72046 | val_0_rmse: 0.9334  | val_1_rmse: 0.90082 |  0:00:13s
epoch 29 | loss: 0.73429 | val_0_rmse: 0.93823 | val_1_rmse: 0.90699 |  0:00:14s
epoch 30 | loss: 0.73362 | val_0_rmse: 0.93677 | val_1_rmse: 0.89831 |  0:00:14s
epoch 31 | loss: 0.7337  | val_0_rmse: 0.93035 | val_1_rmse: 0.8952  |  0:00:15s
epoch 32 | loss: 0.72217 | val_0_rmse: 0.95459 | val_1_rmse: 0.91206 |  0:00:15s
epoch 33 | loss: 0.70939 | val_0_rmse: 0.93829 | val_1_rmse: 0.89926 |  0:00:16s
epoch 34 | loss: 0.71929 | val_0_rmse: 0.92636 | val_1_rmse: 0.89296 |  0:00:16s
epoch 35 | loss: 0.71056 | val_0_rmse: 0.92349 | val_1_rmse: 0.89069 |  0:00:16s
epoch 36 | loss: 0.71094 | val_0_rmse: 0.92862 | val_1_rmse: 0.89405 |  0:00:17s
epoch 37 | loss: 0.70594 | val_0_rmse: 0.94319 | val_1_rmse: 0.90271 |  0:00:17s
epoch 38 | loss: 0.70516 | val_0_rmse: 0.92592 | val_1_rmse: 0.88922 |  0:00:18s
epoch 39 | loss: 0.70376 | val_0_rmse: 0.90379 | val_1_rmse: 0.87313 |  0:00:18s
epoch 40 | loss: 0.69598 | val_0_rmse: 0.90282 | val_1_rmse: 0.87402 |  0:00:19s
epoch 41 | loss: 0.69471 | val_0_rmse: 0.91431 | val_1_rmse: 0.94415 |  0:00:19s
epoch 42 | loss: 0.70213 | val_0_rmse: 0.90611 | val_1_rmse: 0.94948 |  0:00:20s
epoch 43 | loss: 0.70962 | val_0_rmse: 0.88849 | val_1_rmse: 0.89459 |  0:00:20s
epoch 44 | loss: 0.69871 | val_0_rmse: 0.92733 | val_1_rmse: 0.95561 |  0:00:21s
epoch 45 | loss: 0.69827 | val_0_rmse: 0.89862 | val_1_rmse: 0.9347  |  0:00:21s
epoch 46 | loss: 0.70819 | val_0_rmse: 0.88614 | val_1_rmse: 0.9483  |  0:00:22s
epoch 47 | loss: 0.70828 | val_0_rmse: 0.92046 | val_1_rmse: 0.96847 |  0:00:22s
epoch 48 | loss: 0.70809 | val_0_rmse: 0.90191 | val_1_rmse: 0.91193 |  0:00:23s
epoch 49 | loss: 0.72356 | val_0_rmse: 0.91691 | val_1_rmse: 0.88646 |  0:00:23s
epoch 50 | loss: 0.69084 | val_0_rmse: 0.90631 | val_1_rmse: 0.8678  |  0:00:23s
epoch 51 | loss: 0.70048 | val_0_rmse: 0.95829 | val_1_rmse: 0.92202 |  0:00:24s
epoch 52 | loss: 0.70859 | val_0_rmse: 0.90791 | val_1_rmse: 0.89506 |  0:00:24s
epoch 53 | loss: 0.70478 | val_0_rmse: 0.88797 | val_1_rmse: 0.91042 |  0:00:25s
epoch 54 | loss: 0.70152 | val_0_rmse: 0.86415 | val_1_rmse: 0.92076 |  0:00:25s
epoch 55 | loss: 0.70964 | val_0_rmse: 0.89433 | val_1_rmse: 0.86124 |  0:00:26s
epoch 56 | loss: 0.69043 | val_0_rmse: 0.92806 | val_1_rmse: 0.87179 |  0:00:26s
epoch 57 | loss: 0.71075 | val_0_rmse: 0.94638 | val_1_rmse: 0.88341 |  0:00:27s
epoch 58 | loss: 0.69252 | val_0_rmse: 0.90135 | val_1_rmse: 0.86788 |  0:00:27s
epoch 59 | loss: 0.69551 | val_0_rmse: 0.88291 | val_1_rmse: 0.85474 |  0:00:28s
epoch 60 | loss: 0.69824 | val_0_rmse: 0.91494 | val_1_rmse: 0.88268 |  0:00:28s
epoch 61 | loss: 0.68917 | val_0_rmse: 0.87655 | val_1_rmse: 0.85269 |  0:00:29s
epoch 62 | loss: 0.68303 | val_0_rmse: 0.85854 | val_1_rmse: 0.83717 |  0:00:29s
epoch 63 | loss: 0.69248 | val_0_rmse: 0.85099 | val_1_rmse: 0.83676 |  0:00:30s
epoch 64 | loss: 0.6918  | val_0_rmse: 0.8754  | val_1_rmse: 0.86076 |  0:00:30s
epoch 65 | loss: 0.70837 | val_0_rmse: 0.86404 | val_1_rmse: 0.8595  |  0:00:31s
epoch 66 | loss: 0.71415 | val_0_rmse: 0.88305 | val_1_rmse: 0.86787 |  0:00:31s
epoch 67 | loss: 0.69712 | val_0_rmse: 0.89291 | val_1_rmse: 0.87263 |  0:00:31s
epoch 68 | loss: 0.69998 | val_0_rmse: 0.85172 | val_1_rmse: 0.83351 |  0:00:32s
epoch 69 | loss: 0.70019 | val_0_rmse: 0.88253 | val_1_rmse: 0.86027 |  0:00:32s
epoch 70 | loss: 0.68456 | val_0_rmse: 0.90325 | val_1_rmse: 0.87423 |  0:00:33s
epoch 71 | loss: 0.69049 | val_0_rmse: 0.8774  | val_1_rmse: 0.85503 |  0:00:33s
epoch 72 | loss: 0.69262 | val_0_rmse: 0.88377 | val_1_rmse: 0.86122 |  0:00:34s
epoch 73 | loss: 0.68249 | val_0_rmse: 0.84642 | val_1_rmse: 0.82918 |  0:00:34s
epoch 74 | loss: 0.68984 | val_0_rmse: 0.868   | val_1_rmse: 0.83834 |  0:00:35s
epoch 75 | loss: 0.67683 | val_0_rmse: 0.89623 | val_1_rmse: 0.86686 |  0:00:35s
epoch 76 | loss: 0.67362 | val_0_rmse: 0.88382 | val_1_rmse: 0.85827 |  0:00:36s
epoch 77 | loss: 0.68405 | val_0_rmse: 0.86571 | val_1_rmse: 0.85342 |  0:00:36s
epoch 78 | loss: 0.68513 | val_0_rmse: 0.8778  | val_1_rmse: 0.86838 |  0:00:37s
epoch 79 | loss: 0.69782 | val_0_rmse: 0.86657 | val_1_rmse: 0.85775 |  0:00:37s
epoch 80 | loss: 0.67708 | val_0_rmse: 0.87443 | val_1_rmse: 0.85954 |  0:00:38s
epoch 81 | loss: 0.67436 | val_0_rmse: 0.90123 | val_1_rmse: 0.8806  |  0:00:38s
epoch 82 | loss: 0.68159 | val_0_rmse: 0.88183 | val_1_rmse: 0.86443 |  0:00:38s
epoch 83 | loss: 0.6709  | val_0_rmse: 0.87919 | val_1_rmse: 0.86605 |  0:00:39s
epoch 84 | loss: 0.68494 | val_0_rmse: 0.8899  | val_1_rmse: 0.86808 |  0:00:39s
epoch 85 | loss: 0.69296 | val_0_rmse: 0.88962 | val_1_rmse: 0.87018 |  0:00:40s
epoch 86 | loss: 0.67669 | val_0_rmse: 0.87469 | val_1_rmse: 0.85736 |  0:00:40s
epoch 87 | loss: 0.68429 | val_0_rmse: 0.89523 | val_1_rmse: 0.87328 |  0:00:41s
epoch 88 | loss: 0.68296 | val_0_rmse: 0.87009 | val_1_rmse: 0.8529  |  0:00:41s
epoch 89 | loss: 0.69698 | val_0_rmse: 0.86567 | val_1_rmse: 0.85388 |  0:00:42s
epoch 90 | loss: 0.68127 | val_0_rmse: 0.91444 | val_1_rmse: 0.9059  |  0:00:42s
epoch 91 | loss: 0.67766 | val_0_rmse: 0.91218 | val_1_rmse: 0.89799 |  0:00:43s
epoch 92 | loss: 0.68172 | val_0_rmse: 0.96941 | val_1_rmse: 0.95512 |  0:00:43s
epoch 93 | loss: 0.67225 | val_0_rmse: 0.87068 | val_1_rmse: 0.87136 |  0:00:44s
epoch 94 | loss: 0.69862 | val_0_rmse: 0.90192 | val_1_rmse: 0.89856 |  0:00:44s
epoch 95 | loss: 0.68976 | val_0_rmse: 0.89365 | val_1_rmse: 0.8863  |  0:00:45s
epoch 96 | loss: 0.67823 | val_0_rmse: 0.89917 | val_1_rmse: 0.88908 |  0:00:45s
epoch 97 | loss: 0.67405 | val_0_rmse: 0.9211  | val_1_rmse: 0.90918 |  0:00:46s
epoch 98 | loss: 0.67676 | val_0_rmse: 0.91479 | val_1_rmse: 0.90314 |  0:00:46s
epoch 99 | loss: 0.66992 | val_0_rmse: 0.90312 | val_1_rmse: 0.88501 |  0:00:46s
epoch 100| loss: 0.66568 | val_0_rmse: 0.89208 | val_1_rmse: 0.87518 |  0:00:47s
epoch 101| loss: 0.66807 | val_0_rmse: 0.90554 | val_1_rmse: 0.89191 |  0:00:47s
epoch 102| loss: 0.67706 | val_0_rmse: 0.87928 | val_1_rmse: 0.87095 |  0:00:48s
epoch 103| loss: 0.68251 | val_0_rmse: 0.91833 | val_1_rmse: 0.89901 |  0:00:48s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.82918
Best weights from best epoch are automatically used!
ended training at: 06:37:08
Feature importance:
Mean squared error is of 6760505427.805933
Mean absolute error:55176.66858677905
MAPE:0.4904483805775838
R2 score:0.1638499279350375
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:37:09
epoch 0  | loss: 0.85259 | val_0_rmse: 0.9183  | val_1_rmse: 0.91642 |  0:00:13s
epoch 1  | loss: 0.80759 | val_0_rmse: 0.91419 | val_1_rmse: 0.91198 |  0:00:26s
epoch 2  | loss: 0.80595 | val_0_rmse: 0.90399 | val_1_rmse: 0.90182 |  0:00:39s
epoch 3  | loss: 0.80583 | val_0_rmse: 0.89789 | val_1_rmse: 0.89525 |  0:00:52s
epoch 4  | loss: 0.80366 | val_0_rmse: 0.89504 | val_1_rmse: 0.89187 |  0:01:06s
epoch 5  | loss: 0.80278 | val_0_rmse: 0.89409 | val_1_rmse: 0.89099 |  0:01:19s
epoch 6  | loss: 0.8013  | val_0_rmse: 0.89373 | val_1_rmse: 0.89061 |  0:01:32s
epoch 7  | loss: 0.80069 | val_0_rmse: 0.89338 | val_1_rmse: 0.89034 |  0:01:45s
epoch 8  | loss: 0.80034 | val_0_rmse: 0.89402 | val_1_rmse: 0.8909  |  0:01:58s
epoch 9  | loss: 0.80039 | val_0_rmse: 0.89444 | val_1_rmse: 0.89159 |  0:02:11s
epoch 10 | loss: 0.80181 | val_0_rmse: 0.9448  | val_1_rmse: 0.94113 |  0:02:24s
epoch 11 | loss: 0.80775 | val_0_rmse: 0.90327 | val_1_rmse: 0.90265 |  0:02:38s
epoch 12 | loss: 0.83081 | val_0_rmse: 0.91025 | val_1_rmse: 0.90784 |  0:02:51s
epoch 13 | loss: 0.82932 | val_0_rmse: 0.90914 | val_1_rmse: 0.90654 |  0:03:04s
epoch 14 | loss: 0.82826 | val_0_rmse: 0.90855 | val_1_rmse: 0.90567 |  0:03:17s
epoch 15 | loss: 0.82679 | val_0_rmse: 0.90896 | val_1_rmse: 0.90599 |  0:03:30s
epoch 16 | loss: 0.81153 | val_0_rmse: 0.89764 | val_1_rmse: 0.8949  |  0:03:43s
epoch 17 | loss: 0.81022 | val_0_rmse: 0.89772 | val_1_rmse: 0.89458 |  0:03:56s
epoch 18 | loss: 0.80689 | val_0_rmse: 0.89642 | val_1_rmse: 0.89323 |  0:04:10s
epoch 19 | loss: 0.80726 | val_0_rmse: 0.89808 | val_1_rmse: 0.89527 |  0:04:23s
epoch 20 | loss: 0.80452 | val_0_rmse: 0.89717 | val_1_rmse: 0.89377 |  0:04:36s
epoch 21 | loss: 0.80342 | val_0_rmse: 0.89549 | val_1_rmse: 0.89223 |  0:04:49s
epoch 22 | loss: 0.80317 | val_0_rmse: 0.89796 | val_1_rmse: 0.89476 |  0:05:02s
epoch 23 | loss: 0.80368 | val_0_rmse: 0.89525 | val_1_rmse: 0.89195 |  0:05:14s
epoch 24 | loss: 0.80315 | val_0_rmse: 0.8955  | val_1_rmse: 0.89212 |  0:05:27s
epoch 25 | loss: 0.80347 | val_0_rmse: 0.89861 | val_1_rmse: 0.89546 |  0:05:40s
epoch 26 | loss: 0.80292 | val_0_rmse: 0.89907 | val_1_rmse: 0.89578 |  0:05:53s
epoch 27 | loss: 0.80315 | val_0_rmse: 0.89695 | val_1_rmse: 0.89899 |  0:06:06s
epoch 28 | loss: 0.80228 | val_0_rmse: 0.89617 | val_1_rmse: 0.89248 |  0:06:19s
epoch 29 | loss: 0.80286 | val_0_rmse: 0.8968  | val_1_rmse: 0.8938  |  0:06:32s
epoch 30 | loss: 0.8029  | val_0_rmse: 0.89653 | val_1_rmse: 0.89305 |  0:06:44s
epoch 31 | loss: 0.80302 | val_0_rmse: 0.89507 | val_1_rmse: 0.89194 |  0:06:57s
epoch 32 | loss: 0.80239 | val_0_rmse: 0.89504 | val_1_rmse: 0.89175 |  0:07:10s
epoch 33 | loss: 0.8016  | val_0_rmse: 0.89639 | val_1_rmse: 0.89314 |  0:07:23s
epoch 34 | loss: 0.8023  | val_0_rmse: 0.8966  | val_1_rmse: 0.89422 |  0:07:36s
epoch 35 | loss: 0.80446 | val_0_rmse: 0.89848 | val_1_rmse: 0.89543 |  0:07:49s
epoch 36 | loss: 0.80515 | val_0_rmse: 0.89679 | val_1_rmse: 0.89373 |  0:08:02s
epoch 37 | loss: 0.8036  | val_0_rmse: 0.89599 | val_1_rmse: 0.89273 |  0:08:15s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.89034
Best weights from best epoch are automatically used!
ended training at: 06:45:28
Feature importance:
Mean squared error is of 5238243510.204718
Mean absolute error:56127.97049638364
MAPE:0.6142984457256845
R2 score:0.20314541757376803
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:45:30
epoch 0  | loss: 0.8576  | val_0_rmse: 0.92645 | val_1_rmse: 0.92242 |  0:00:12s
epoch 1  | loss: 0.80656 | val_0_rmse: 0.90369 | val_1_rmse: 0.89903 |  0:00:25s
epoch 2  | loss: 0.80416 | val_0_rmse: 0.90273 | val_1_rmse: 0.89811 |  0:00:38s
epoch 3  | loss: 0.80344 | val_0_rmse: 0.89468 | val_1_rmse: 0.89027 |  0:00:51s
epoch 4  | loss: 0.8024  | val_0_rmse: 0.89547 | val_1_rmse: 0.891   |  0:01:04s
epoch 5  | loss: 0.80343 | val_0_rmse: 0.89693 | val_1_rmse: 0.89687 |  0:01:17s
epoch 6  | loss: 0.80337 | val_0_rmse: 0.8976  | val_1_rmse: 0.89287 |  0:01:30s
epoch 7  | loss: 0.80424 | val_0_rmse: 0.89905 | val_1_rmse: 0.8945  |  0:01:43s
epoch 8  | loss: 0.80343 | val_0_rmse: 0.90264 | val_1_rmse: 0.91794 |  0:01:56s
epoch 9  | loss: 0.80291 | val_0_rmse: 0.89538 | val_1_rmse: 0.89071 |  0:02:09s
epoch 10 | loss: 0.80421 | val_0_rmse: 0.89663 | val_1_rmse: 0.89228 |  0:02:22s
epoch 11 | loss: 0.80517 | val_0_rmse: 0.89502 | val_1_rmse: 0.89048 |  0:02:35s
epoch 12 | loss: 0.80187 | val_0_rmse: 0.89483 | val_1_rmse: 0.89055 |  0:02:48s
epoch 13 | loss: 0.80205 | val_0_rmse: 0.89412 | val_1_rmse: 0.88983 |  0:03:01s
epoch 14 | loss: 0.80131 | val_0_rmse: 0.895   | val_1_rmse: 0.8908  |  0:03:14s
epoch 15 | loss: 0.80105 | val_0_rmse: 0.89419 | val_1_rmse: 0.88993 |  0:03:27s
epoch 16 | loss: 0.80226 | val_0_rmse: 0.89507 | val_1_rmse: 0.89077 |  0:03:40s
epoch 17 | loss: 0.80292 | val_0_rmse: 0.90586 | val_1_rmse: 0.90064 |  0:03:52s
epoch 18 | loss: 0.81977 | val_0_rmse: 0.90464 | val_1_rmse: 0.90239 |  0:04:05s
epoch 19 | loss: 0.81969 | val_0_rmse: 0.90342 | val_1_rmse: 0.89798 |  0:04:18s
epoch 20 | loss: 0.81824 | val_0_rmse: 0.9057  | val_1_rmse: 0.90052 |  0:04:31s
epoch 21 | loss: 0.81715 | val_0_rmse: 0.90291 | val_1_rmse: 0.8974  |  0:04:44s
epoch 22 | loss: 0.81689 | val_0_rmse: 0.90508 | val_1_rmse: 0.89919 |  0:04:57s
epoch 23 | loss: 0.8167  | val_0_rmse: 0.89473 | val_1_rmse: 0.89028 |  0:05:10s
epoch 24 | loss: 0.80153 | val_0_rmse: 0.89389 | val_1_rmse: 0.88973 |  0:05:23s
epoch 25 | loss: 0.80161 | val_0_rmse: 0.89565 | val_1_rmse: 0.89105 |  0:05:36s
epoch 26 | loss: 0.80039 | val_0_rmse: 0.89474 | val_1_rmse: 0.88991 |  0:05:49s
epoch 27 | loss: 0.80015 | val_0_rmse: 0.89351 | val_1_rmse: 0.88896 |  0:06:02s
epoch 28 | loss: 0.80028 | val_0_rmse: 0.89435 | val_1_rmse: 0.88968 |  0:06:15s
epoch 29 | loss: 0.80039 | val_0_rmse: 0.89383 | val_1_rmse: 0.88932 |  0:06:28s
epoch 30 | loss: 0.80626 | val_0_rmse: 0.94377 | val_1_rmse: 0.94047 |  0:06:40s
epoch 31 | loss: 0.8353  | val_0_rmse: 0.90004 | val_1_rmse: 0.89744 |  0:06:53s
epoch 32 | loss: 0.81526 | val_0_rmse: 0.90126 | val_1_rmse: 0.8962  |  0:07:06s
epoch 33 | loss: 0.81975 | val_0_rmse: 0.90925 | val_1_rmse: 0.91031 |  0:07:19s
epoch 34 | loss: 0.82361 | val_0_rmse: 0.90926 | val_1_rmse: 0.90559 |  0:07:32s
epoch 35 | loss: 0.82027 | val_0_rmse: 0.90944 | val_1_rmse: 0.90914 |  0:07:45s
epoch 36 | loss: 0.81528 | val_0_rmse: 0.90353 | val_1_rmse: 0.90775 |  0:07:58s
epoch 37 | loss: 0.81132 | val_0_rmse: 0.90338 | val_1_rmse: 0.90542 |  0:08:12s
epoch 38 | loss: 0.81103 | val_0_rmse: 0.90203 | val_1_rmse: 0.9026  |  0:08:25s
epoch 39 | loss: 0.8105  | val_0_rmse: 0.90006 | val_1_rmse: 0.89556 |  0:08:38s
epoch 40 | loss: 0.80987 | val_0_rmse: 0.89971 | val_1_rmse: 0.89693 |  0:08:50s
epoch 41 | loss: 0.81084 | val_0_rmse: 0.90025 | val_1_rmse: 0.89484 |  0:09:03s
epoch 42 | loss: 0.8095  | val_0_rmse: 0.91787 | val_1_rmse: 0.96423 |  0:09:16s
epoch 43 | loss: 0.80989 | val_0_rmse: 0.90699 | val_1_rmse: 0.92561 |  0:09:29s
epoch 44 | loss: 0.80932 | val_0_rmse: 0.90623 | val_1_rmse: 0.9203  |  0:09:42s
epoch 45 | loss: 0.81065 | val_0_rmse: 0.90165 | val_1_rmse: 0.89716 |  0:09:56s
epoch 46 | loss: 0.80978 | val_0_rmse: 0.89937 | val_1_rmse: 0.89487 |  0:10:09s
epoch 47 | loss: 0.80924 | val_0_rmse: 0.89858 | val_1_rmse: 0.8937  |  0:10:23s
epoch 48 | loss: 0.80901 | val_0_rmse: 0.89916 | val_1_rmse: 0.89448 |  0:10:36s
epoch 49 | loss: 0.80962 | val_0_rmse: 0.89882 | val_1_rmse: 0.89395 |  0:10:50s
epoch 50 | loss: 0.80891 | val_0_rmse: 0.89904 | val_1_rmse: 0.89435 |  0:11:03s
epoch 51 | loss: 0.80963 | val_0_rmse: 0.89891 | val_1_rmse: 0.89422 |  0:11:17s
epoch 52 | loss: 0.81015 | val_0_rmse: 0.89891 | val_1_rmse: 0.89431 |  0:11:30s
epoch 53 | loss: 0.80907 | val_0_rmse: 0.89973 | val_1_rmse: 0.89476 |  0:11:44s
epoch 54 | loss: 0.8093  | val_0_rmse: 0.89938 | val_1_rmse: 0.89443 |  0:11:57s
epoch 55 | loss: 0.80919 | val_0_rmse: 0.9041  | val_1_rmse: 0.91378 |  0:12:11s
epoch 56 | loss: 0.80933 | val_0_rmse: 0.90253 | val_1_rmse: 0.9031  |  0:12:24s
epoch 57 | loss: 0.80939 | val_0_rmse: 0.90126 | val_1_rmse: 0.90323 |  0:12:38s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.88896
Best weights from best epoch are automatically used!
ended training at: 06:58:13
Feature importance:
Mean squared error is of 5199955686.864909
Mean absolute error:55765.59125213576
MAPE:0.6020199008663926
R2 score:0.20802766903676673
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:58:14
epoch 0  | loss: 0.85071 | val_0_rmse: 0.93732 | val_1_rmse: 0.93432 |  0:00:13s
epoch 1  | loss: 0.81042 | val_0_rmse: 0.91673 | val_1_rmse: 0.91169 |  0:00:26s
epoch 2  | loss: 0.80961 | val_0_rmse: 0.90277 | val_1_rmse: 0.89722 |  0:00:40s
epoch 3  | loss: 0.811   | val_0_rmse: 0.90734 | val_1_rmse: 0.90266 |  0:00:53s
epoch 4  | loss: 0.81629 | val_0_rmse: 0.90136 | val_1_rmse: 0.89406 |  0:01:07s
epoch 5  | loss: 0.80862 | val_0_rmse: 0.89639 | val_1_rmse: 0.88954 |  0:01:20s
epoch 6  | loss: 0.80659 | val_0_rmse: 0.89626 | val_1_rmse: 0.88952 |  0:01:33s
epoch 7  | loss: 0.80618 | val_0_rmse: 0.89741 | val_1_rmse: 0.89105 |  0:01:47s
epoch 8  | loss: 0.80585 | val_0_rmse: 0.89777 | val_1_rmse: 0.89095 |  0:02:00s
epoch 9  | loss: 0.80592 | val_0_rmse: 0.89645 | val_1_rmse: 0.88943 |  0:02:14s
epoch 10 | loss: 0.80456 | val_0_rmse: 0.89619 | val_1_rmse: 0.88948 |  0:02:27s
epoch 11 | loss: 0.80438 | val_0_rmse: 0.89608 | val_1_rmse: 0.88944 |  0:02:41s
epoch 12 | loss: 0.80585 | val_0_rmse: 0.89548 | val_1_rmse: 0.8889  |  0:02:54s
epoch 13 | loss: 0.80442 | val_0_rmse: 0.89739 | val_1_rmse: 0.8905  |  0:03:07s
epoch 14 | loss: 0.80426 | val_0_rmse: 0.89634 | val_1_rmse: 0.88971 |  0:03:21s
epoch 15 | loss: 0.80421 | val_0_rmse: 0.89734 | val_1_rmse: 0.8913  |  0:03:34s
epoch 16 | loss: 0.80482 | val_0_rmse: 0.89583 | val_1_rmse: 0.88906 |  0:03:48s
epoch 17 | loss: 0.80455 | val_0_rmse: 0.89696 | val_1_rmse: 0.8904  |  0:04:01s
epoch 18 | loss: 0.80466 | val_0_rmse: 0.89655 | val_1_rmse: 0.88993 |  0:04:14s
epoch 19 | loss: 0.80468 | val_0_rmse: 0.89795 | val_1_rmse: 0.89139 |  0:04:28s
epoch 20 | loss: 0.80519 | val_0_rmse: 0.90019 | val_1_rmse: 0.89278 |  0:04:41s
epoch 21 | loss: 0.8051  | val_0_rmse: 0.89735 | val_1_rmse: 0.90601 |  0:04:55s
epoch 22 | loss: 0.80485 | val_0_rmse: 0.89668 | val_1_rmse: 0.89586 |  0:05:08s
epoch 23 | loss: 0.80748 | val_0_rmse: 0.90712 | val_1_rmse: 0.89943 |  0:05:21s
epoch 24 | loss: 0.80326 | val_0_rmse: 0.89445 | val_1_rmse: 0.88823 |  0:05:35s
epoch 25 | loss: 0.80456 | val_0_rmse: 0.95655 | val_1_rmse: 0.93861 |  0:05:48s
epoch 26 | loss: 0.80506 | val_0_rmse: 0.89579 | val_1_rmse: 0.89029 |  0:06:01s
epoch 27 | loss: 0.80189 | val_0_rmse: 0.89514 | val_1_rmse: 0.88921 |  0:06:15s
epoch 28 | loss: 0.80043 | val_0_rmse: 0.89446 | val_1_rmse: 0.88842 |  0:06:28s
epoch 29 | loss: 0.80105 | val_0_rmse: 0.89374 | val_1_rmse: 0.88788 |  0:06:42s
epoch 30 | loss: 0.79929 | val_0_rmse: 0.89347 | val_1_rmse: 0.88743 |  0:06:55s
epoch 31 | loss: 0.79954 | val_0_rmse: 0.89625 | val_1_rmse: 0.89065 |  0:07:08s
epoch 32 | loss: 0.80025 | val_0_rmse: 0.90827 | val_1_rmse: 0.90379 |  0:07:21s
epoch 33 | loss: 0.79987 | val_0_rmse: 0.89312 | val_1_rmse: 0.88702 |  0:07:35s
epoch 34 | loss: 0.80119 | val_0_rmse: 0.89484 | val_1_rmse: 0.88787 |  0:07:48s
epoch 35 | loss: 0.80035 | val_0_rmse: 0.89841 | val_1_rmse: 0.89081 |  0:08:02s
epoch 36 | loss: 0.80005 | val_0_rmse: 0.89754 | val_1_rmse: 0.88878 |  0:08:15s
epoch 37 | loss: 0.79942 | val_0_rmse: 0.89597 | val_1_rmse: 0.88824 |  0:08:28s
epoch 38 | loss: 0.80047 | val_0_rmse: 0.89485 | val_1_rmse: 0.8893  |  0:08:42s
epoch 39 | loss: 0.8002  | val_0_rmse: 0.89712 | val_1_rmse: 0.89001 |  0:08:55s
epoch 40 | loss: 0.80028 | val_0_rmse: 0.89574 | val_1_rmse: 0.88861 |  0:09:08s
epoch 41 | loss: 0.8003  | val_0_rmse: 0.89797 | val_1_rmse: 0.89058 |  0:09:22s
epoch 42 | loss: 0.80062 | val_0_rmse: 0.90641 | val_1_rmse: 0.89625 |  0:09:35s
epoch 43 | loss: 0.80515 | val_0_rmse: 0.89804 | val_1_rmse: 0.89169 |  0:09:48s
epoch 44 | loss: 0.80498 | val_0_rmse: 0.9622  | val_1_rmse: 0.93583 |  0:10:02s
epoch 45 | loss: 0.80421 | val_0_rmse: 0.89767 | val_1_rmse: 0.89293 |  0:10:15s
epoch 46 | loss: 0.80189 | val_0_rmse: 0.89513 | val_1_rmse: 0.88853 |  0:10:29s
epoch 47 | loss: 0.80142 | val_0_rmse: 0.89774 | val_1_rmse: 0.89194 |  0:10:42s
epoch 48 | loss: 0.80329 | val_0_rmse: 0.90178 | val_1_rmse: 0.89954 |  0:10:56s
epoch 49 | loss: 0.80789 | val_0_rmse: 0.90122 | val_1_rmse: 0.89686 |  0:11:09s
epoch 50 | loss: 0.80717 | val_0_rmse: 0.90018 | val_1_rmse: 0.89543 |  0:11:22s
epoch 51 | loss: 0.80603 | val_0_rmse: 0.89789 | val_1_rmse: 0.89215 |  0:11:36s
epoch 52 | loss: 0.80553 | val_0_rmse: 0.89602 | val_1_rmse: 0.88945 |  0:11:49s
epoch 53 | loss: 0.80557 | val_0_rmse: 0.89706 | val_1_rmse: 0.89007 |  0:12:02s
epoch 54 | loss: 0.80371 | val_0_rmse: 0.89499 | val_1_rmse: 0.88859 |  0:12:16s
epoch 55 | loss: 0.80257 | val_0_rmse: 0.89477 | val_1_rmse: 0.88846 |  0:12:29s
epoch 56 | loss: 0.80189 | val_0_rmse: 0.89476 | val_1_rmse: 0.88804 |  0:12:42s
epoch 57 | loss: 0.80157 | val_0_rmse: 0.8944  | val_1_rmse: 0.88809 |  0:12:56s
epoch 58 | loss: 0.80145 | val_0_rmse: 0.89428 | val_1_rmse: 0.88796 |  0:13:09s
epoch 59 | loss: 0.80192 | val_0_rmse: 0.8979  | val_1_rmse: 0.89112 |  0:13:22s
epoch 60 | loss: 0.80155 | val_0_rmse: 0.89484 | val_1_rmse: 0.888   |  0:13:36s
epoch 61 | loss: 0.80068 | val_0_rmse: 0.89599 | val_1_rmse: 0.88994 |  0:13:49s
epoch 62 | loss: 0.8016  | val_0_rmse: 0.89463 | val_1_rmse: 0.99056 |  0:14:03s
epoch 63 | loss: 0.80266 | val_0_rmse: 0.89544 | val_1_rmse: 0.8891  |  0:14:16s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.88702
Best weights from best epoch are automatically used!
ended training at: 07:12:35
Feature importance:
Mean squared error is of 5311175239.153718
Mean absolute error:56451.73362859725
MAPE:0.6141986601522647
R2 score:0.2030761945438989
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:12:36
epoch 0  | loss: 0.87424 | val_0_rmse: 0.92488 | val_1_rmse: 0.9257  |  0:00:13s
epoch 1  | loss: 0.80529 | val_0_rmse: 0.90682 | val_1_rmse: 0.90713 |  0:00:26s
epoch 2  | loss: 0.80389 | val_0_rmse: 0.89683 | val_1_rmse: 0.89767 |  0:00:39s
epoch 3  | loss: 0.80509 | val_0_rmse: 0.89647 | val_1_rmse: 0.89721 |  0:00:53s
epoch 4  | loss: 0.80254 | val_0_rmse: 0.89461 | val_1_rmse: 0.89508 |  0:01:06s
epoch 5  | loss: 0.80233 | val_0_rmse: 0.89416 | val_1_rmse: 0.89451 |  0:01:20s
epoch 6  | loss: 0.8006  | val_0_rmse: 0.89451 | val_1_rmse: 0.89505 |  0:01:33s
epoch 7  | loss: 0.80018 | val_0_rmse: 0.89452 | val_1_rmse: 0.89572 |  0:01:47s
epoch 8  | loss: 0.79994 | val_0_rmse: 0.89344 | val_1_rmse: 0.89414 |  0:02:00s
epoch 9  | loss: 0.79905 | val_0_rmse: 0.89362 | val_1_rmse: 0.89428 |  0:02:13s
epoch 10 | loss: 0.7982  | val_0_rmse: 0.89627 | val_1_rmse: 0.89747 |  0:02:26s
epoch 11 | loss: 0.79993 | val_0_rmse: 0.8941  | val_1_rmse: 0.89465 |  0:02:40s
epoch 12 | loss: 0.79942 | val_0_rmse: 0.89327 | val_1_rmse: 0.89448 |  0:02:53s
epoch 13 | loss: 0.79934 | val_0_rmse: 0.89327 | val_1_rmse: 0.8936  |  0:03:06s
epoch 14 | loss: 0.80467 | val_0_rmse: 0.89429 | val_1_rmse: 0.89485 |  0:03:20s
epoch 15 | loss: 0.80136 | val_0_rmse: 0.89358 | val_1_rmse: 0.89387 |  0:03:33s
epoch 16 | loss: 0.80135 | val_0_rmse: 0.89469 | val_1_rmse: 0.89474 |  0:03:47s
epoch 17 | loss: 0.80205 | val_0_rmse: 0.89536 | val_1_rmse: 0.89593 |  0:04:00s
epoch 18 | loss: 0.80223 | val_0_rmse: 0.89535 | val_1_rmse: 0.89633 |  0:04:13s
epoch 19 | loss: 0.80357 | val_0_rmse: 0.89654 | val_1_rmse: 0.8975  |  0:04:27s
epoch 20 | loss: 0.80206 | val_0_rmse: 0.89527 | val_1_rmse: 0.89544 |  0:04:40s
epoch 21 | loss: 0.80153 | val_0_rmse: 0.89549 | val_1_rmse: 0.89604 |  0:04:53s
epoch 22 | loss: 0.80067 | val_0_rmse: 0.89456 | val_1_rmse: 0.89477 |  0:05:06s
epoch 23 | loss: 0.80068 | val_0_rmse: 0.89416 | val_1_rmse: 0.89463 |  0:05:20s
epoch 24 | loss: 0.80068 | val_0_rmse: 0.89427 | val_1_rmse: 0.89444 |  0:05:33s
epoch 25 | loss: 0.80034 | val_0_rmse: 0.89639 | val_1_rmse: 0.89717 |  0:05:47s
epoch 26 | loss: 0.80114 | val_0_rmse: 0.8946  | val_1_rmse: 0.89525 |  0:06:00s
epoch 27 | loss: 0.80009 | val_0_rmse: 0.89372 | val_1_rmse: 0.89423 |  0:06:13s
epoch 28 | loss: 0.80005 | val_0_rmse: 0.89388 | val_1_rmse: 0.89446 |  0:06:26s
epoch 29 | loss: 0.79923 | val_0_rmse: 0.89365 | val_1_rmse: 0.8944  |  0:06:40s
epoch 30 | loss: 0.79935 | val_0_rmse: 0.89392 | val_1_rmse: 0.89427 |  0:06:53s
epoch 31 | loss: 0.80014 | val_0_rmse: 0.8931  | val_1_rmse: 0.89348 |  0:07:06s
epoch 32 | loss: 0.80018 | val_0_rmse: 0.89488 | val_1_rmse: 0.89551 |  0:07:20s
epoch 33 | loss: 0.80146 | val_0_rmse: 0.89385 | val_1_rmse: 0.89845 |  0:07:33s
epoch 34 | loss: 0.80073 | val_0_rmse: 0.8944  | val_1_rmse: 0.90821 |  0:07:46s
epoch 35 | loss: 0.80054 | val_0_rmse: 0.89387 | val_1_rmse: 0.90868 |  0:08:00s
epoch 36 | loss: 0.80071 | val_0_rmse: 0.89374 | val_1_rmse: 0.92303 |  0:08:13s
epoch 37 | loss: 0.79978 | val_0_rmse: 0.8944  | val_1_rmse: 0.91355 |  0:08:27s
epoch 38 | loss: 0.8     | val_0_rmse: 0.89403 | val_1_rmse: 0.90317 |  0:08:40s
epoch 39 | loss: 0.80009 | val_0_rmse: 0.89387 | val_1_rmse: 0.90339 |  0:08:53s
epoch 40 | loss: 0.80007 | val_0_rmse: 0.89363 | val_1_rmse: 0.91107 |  0:09:07s
epoch 41 | loss: 0.80041 | val_0_rmse: 0.89591 | val_1_rmse: 0.90958 |  0:09:20s
epoch 42 | loss: 0.80108 | val_0_rmse: 0.89382 | val_1_rmse: 0.913   |  0:09:33s
epoch 43 | loss: 0.79949 | val_0_rmse: 0.89355 | val_1_rmse: 0.90778 |  0:09:47s
epoch 44 | loss: 0.80014 | val_0_rmse: 0.89378 | val_1_rmse: 0.89474 |  0:10:00s
epoch 45 | loss: 0.80277 | val_0_rmse: 0.8947  | val_1_rmse: 0.89521 |  0:10:13s
epoch 46 | loss: 0.80096 | val_0_rmse: 0.89417 | val_1_rmse: 0.91031 |  0:10:26s
epoch 47 | loss: 0.8013  | val_0_rmse: 0.89484 | val_1_rmse: 0.8958  |  0:10:40s
epoch 48 | loss: 0.80057 | val_0_rmse: 0.89521 | val_1_rmse: 0.89611 |  0:10:53s
epoch 49 | loss: 0.80087 | val_0_rmse: 0.89488 | val_1_rmse: 0.89574 |  0:11:07s
epoch 50 | loss: 0.80103 | val_0_rmse: 0.89509 | val_1_rmse: 0.92261 |  0:11:20s
epoch 51 | loss: 0.80146 | val_0_rmse: 0.89432 | val_1_rmse: 0.90786 |  0:11:33s
epoch 52 | loss: 0.80057 | val_0_rmse: 0.89409 | val_1_rmse: 0.90098 |  0:11:47s
epoch 53 | loss: 0.80196 | val_0_rmse: 0.8939  | val_1_rmse: 0.99659 |  0:12:00s
epoch 54 | loss: 0.80009 | val_0_rmse: 0.89428 | val_1_rmse: 0.98392 |  0:12:13s
epoch 55 | loss: 0.80032 | val_0_rmse: 0.89393 | val_1_rmse: 0.99071 |  0:12:27s
epoch 56 | loss: 0.80729 | val_0_rmse: 0.89424 | val_1_rmse: 1.13832 |  0:12:40s
epoch 57 | loss: 0.80025 | val_0_rmse: 0.89343 | val_1_rmse: 1.27639 |  0:12:53s
epoch 58 | loss: 0.79973 | val_0_rmse: 0.89378 | val_1_rmse: 1.02543 |  0:13:07s
epoch 59 | loss: 0.80014 | val_0_rmse: 0.89417 | val_1_rmse: 1.36217 |  0:13:20s
epoch 60 | loss: 0.8019  | val_0_rmse: 0.89559 | val_1_rmse: 1.01366 |  0:13:34s
epoch 61 | loss: 0.80253 | val_0_rmse: 0.89525 | val_1_rmse: 1.00836 |  0:13:47s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.89348
Best weights from best epoch are automatically used!
ended training at: 07:26:27
Feature importance:
Mean squared error is of 5225079862.326041
Mean absolute error:55924.453899164946
MAPE:0.6049468123455606
R2 score:0.2067762940723099
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:26:28
epoch 0  | loss: 0.86248 | val_0_rmse: 0.91984 | val_1_rmse: 0.91799 |  0:00:13s
epoch 1  | loss: 0.80332 | val_0_rmse: 0.90333 | val_1_rmse: 0.90118 |  0:00:26s
epoch 2  | loss: 0.80165 | val_0_rmse: 0.89748 | val_1_rmse: 0.89615 |  0:00:40s
epoch 3  | loss: 0.80004 | val_0_rmse: 0.89314 | val_1_rmse: 0.89113 |  0:00:53s
epoch 4  | loss: 0.7982  | val_0_rmse: 0.89253 | val_1_rmse: 0.88976 |  0:01:07s
epoch 5  | loss: 0.79747 | val_0_rmse: 0.89183 | val_1_rmse: 0.89056 |  0:01:20s
epoch 6  | loss: 0.79694 | val_0_rmse: 0.89394 | val_1_rmse: 0.89256 |  0:01:33s
epoch 7  | loss: 0.79969 | val_0_rmse: 0.89974 | val_1_rmse: 0.89895 |  0:01:47s
epoch 8  | loss: 0.80416 | val_0_rmse: 0.89388 | val_1_rmse: 0.89194 |  0:02:00s
epoch 9  | loss: 0.79839 | val_0_rmse: 0.89332 | val_1_rmse: 0.89136 |  0:02:13s
epoch 10 | loss: 0.79674 | val_0_rmse: 0.89018 | val_1_rmse: 0.88763 |  0:02:27s
epoch 11 | loss: 0.79477 | val_0_rmse: 0.89087 | val_1_rmse: 0.8889  |  0:02:40s
epoch 12 | loss: 0.79522 | val_0_rmse: 0.89065 | val_1_rmse: 0.88841 |  0:02:54s
epoch 13 | loss: 0.79643 | val_0_rmse: 0.8931  | val_1_rmse: 0.89034 |  0:03:07s
epoch 14 | loss: 0.79604 | val_0_rmse: 0.89118 | val_1_rmse: 0.889   |  0:03:20s
epoch 15 | loss: 0.79558 | val_0_rmse: 0.89059 | val_1_rmse: 0.89287 |  0:03:34s
epoch 16 | loss: 0.7942  | val_0_rmse: 0.92813 | val_1_rmse: 1.03422 |  0:03:47s
epoch 17 | loss: 0.79407 | val_0_rmse: 0.89023 | val_1_rmse: 0.88797 |  0:04:00s
epoch 18 | loss: 0.79341 | val_0_rmse: 0.89145 | val_1_rmse: 0.88961 |  0:04:13s
epoch 19 | loss: 0.79312 | val_0_rmse: 0.8923  | val_1_rmse: 0.88913 |  0:04:27s
epoch 20 | loss: 0.79383 | val_0_rmse: 0.89007 | val_1_rmse: 0.88741 |  0:04:40s
epoch 21 | loss: 0.79344 | val_0_rmse: 0.89032 | val_1_rmse: 0.88813 |  0:04:53s
epoch 22 | loss: 0.79376 | val_0_rmse: 0.89116 | val_1_rmse: 0.88908 |  0:05:07s
epoch 23 | loss: 0.79355 | val_0_rmse: 0.88989 | val_1_rmse: 0.88731 |  0:05:21s
epoch 24 | loss: 0.79312 | val_0_rmse: 0.89029 | val_1_rmse: 0.88775 |  0:05:34s
epoch 25 | loss: 0.79328 | val_0_rmse: 0.89068 | val_1_rmse: 0.88825 |  0:05:47s
epoch 26 | loss: 0.79309 | val_0_rmse: 0.88983 | val_1_rmse: 0.88758 |  0:06:01s
epoch 27 | loss: 0.79263 | val_0_rmse: 0.88994 | val_1_rmse: 0.88732 |  0:06:14s
epoch 28 | loss: 0.79347 | val_0_rmse: 0.88979 | val_1_rmse: 0.88722 |  0:06:28s
epoch 29 | loss: 0.79279 | val_0_rmse: 0.8905  | val_1_rmse: 0.88788 |  0:06:41s
epoch 30 | loss: 0.79346 | val_0_rmse: 0.89265 | val_1_rmse: 0.89045 |  0:06:54s
epoch 31 | loss: 0.79325 | val_0_rmse: 0.88948 | val_1_rmse: 0.88706 |  0:07:07s
epoch 32 | loss: 0.79201 | val_0_rmse: 0.88949 | val_1_rmse: 0.88719 |  0:07:21s
epoch 33 | loss: 0.79295 | val_0_rmse: 0.89182 | val_1_rmse: 0.88968 |  0:07:34s
epoch 34 | loss: 0.79329 | val_0_rmse: 0.89475 | val_1_rmse: 0.89358 |  0:07:47s
epoch 35 | loss: 0.79435 | val_0_rmse: 0.89032 | val_1_rmse: 0.888   |  0:08:01s
epoch 36 | loss: 0.79331 | val_0_rmse: 0.89009 | val_1_rmse: 0.88768 |  0:08:14s
epoch 37 | loss: 0.79331 | val_0_rmse: 0.88979 | val_1_rmse: 0.88716 |  0:08:27s
epoch 38 | loss: 0.79523 | val_0_rmse: 0.8917  | val_1_rmse: 0.88954 |  0:08:40s
epoch 39 | loss: 0.79503 | val_0_rmse: 0.89277 | val_1_rmse: 0.88983 |  0:08:54s
epoch 40 | loss: 0.79574 | val_0_rmse: 0.89141 | val_1_rmse: 0.88889 |  0:09:07s
epoch 41 | loss: 0.80933 | val_0_rmse: 0.89888 | val_1_rmse: 0.89673 |  0:09:20s
epoch 42 | loss: 0.80604 | val_0_rmse: 0.89678 | val_1_rmse: 0.89489 |  0:09:34s
epoch 43 | loss: 0.80455 | val_0_rmse: 0.89729 | val_1_rmse: 0.89528 |  0:09:47s
epoch 44 | loss: 0.80456 | val_0_rmse: 0.89719 | val_1_rmse: 0.89518 |  0:10:00s
epoch 45 | loss: 0.80429 | val_0_rmse: 0.89714 | val_1_rmse: 0.89508 |  0:10:13s
epoch 46 | loss: 0.80453 | val_0_rmse: 0.89741 | val_1_rmse: 0.89529 |  0:10:27s
epoch 47 | loss: 0.80432 | val_0_rmse: 0.89633 | val_1_rmse: 0.89433 |  0:10:40s
epoch 48 | loss: 0.80388 | val_0_rmse: 0.89637 | val_1_rmse: 0.89447 |  0:10:53s
epoch 49 | loss: 0.80442 | val_0_rmse: 0.89638 | val_1_rmse: 0.89458 |  0:11:07s
epoch 50 | loss: 0.80442 | val_0_rmse: 0.89626 | val_1_rmse: 0.8945  |  0:11:20s
epoch 51 | loss: 0.80375 | val_0_rmse: 0.89603 | val_1_rmse: 0.89444 |  0:11:34s
epoch 52 | loss: 0.80379 | val_0_rmse: 0.89755 | val_1_rmse: 0.89604 |  0:11:47s
epoch 53 | loss: 0.80347 | val_0_rmse: 0.8963  | val_1_rmse: 0.89486 |  0:12:00s
epoch 54 | loss: 0.80438 | val_0_rmse: 0.90434 | val_1_rmse: 0.9014  |  0:12:13s
epoch 55 | loss: 0.80754 | val_0_rmse: 0.92353 | val_1_rmse: 0.91676 |  0:12:27s
epoch 56 | loss: 0.804   | val_0_rmse: 1.05057 | val_1_rmse: 1.01817 |  0:12:40s
epoch 57 | loss: 0.80379 | val_0_rmse: 0.95417 | val_1_rmse: 0.94122 |  0:12:53s
epoch 58 | loss: 0.80305 | val_0_rmse: 0.98846 | val_1_rmse: 0.96817 |  0:13:07s
epoch 59 | loss: 0.80336 | val_0_rmse: 0.93923 | val_1_rmse: 0.92951 |  0:13:20s
epoch 60 | loss: 0.80286 | val_0_rmse: 0.91578 | val_1_rmse: 0.91078 |  0:13:34s
epoch 61 | loss: 0.80288 | val_0_rmse: 0.93766 | val_1_rmse: 0.92792 |  0:13:47s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.88706
Best weights from best epoch are automatically used!
ended training at: 07:40:20
Feature importance:
Mean squared error is of 5351377032.072993
Mean absolute error:56347.01183821216
MAPE:0.6071132453740858
R2 score:0.20175894695541807
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 5
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:40:22
epoch 0  | loss: 0.85381 | val_0_rmse: 0.93541 | val_1_rmse: 0.93914 |  0:00:13s
epoch 1  | loss: 0.80195 | val_0_rmse: 0.90312 | val_1_rmse: 0.90817 |  0:00:26s
epoch 2  | loss: 0.79935 | val_0_rmse: 0.89466 | val_1_rmse: 0.89986 |  0:00:39s
epoch 3  | loss: 0.79814 | val_0_rmse: 0.8918  | val_1_rmse: 0.89725 |  0:00:53s
epoch 4  | loss: 0.79822 | val_0_rmse: 0.89298 | val_1_rmse: 0.89862 |  0:01:06s
epoch 5  | loss: 0.79793 | val_0_rmse: 0.89661 | val_1_rmse: 0.90283 |  0:01:20s
epoch 6  | loss: 0.7982  | val_0_rmse: 0.9008  | val_1_rmse: 0.90634 |  0:01:33s
epoch 7  | loss: 0.79691 | val_0_rmse: 0.89773 | val_1_rmse: 0.90118 |  0:01:46s
epoch 8  | loss: 0.79718 | val_0_rmse: 0.9057  | val_1_rmse: 0.90963 |  0:02:00s
epoch 9  | loss: 0.79642 | val_0_rmse: 0.90031 | val_1_rmse: 0.90145 |  0:02:13s
epoch 10 | loss: 0.79661 | val_0_rmse: 0.89296 | val_1_rmse: 0.89785 |  0:02:27s
epoch 11 | loss: 0.79542 | val_0_rmse: 0.89261 | val_1_rmse: 0.89643 |  0:02:41s
epoch 12 | loss: 0.79583 | val_0_rmse: 0.89418 | val_1_rmse: 0.89731 |  0:02:54s
epoch 13 | loss: 0.79524 | val_0_rmse: 0.9106  | val_1_rmse: 0.89741 |  0:03:07s
epoch 14 | loss: 0.79515 | val_0_rmse: 0.92639 | val_1_rmse: 0.89678 |  0:03:21s
epoch 15 | loss: 0.79497 | val_0_rmse: 0.91937 | val_1_rmse: 0.89579 |  0:03:34s
epoch 16 | loss: 0.795   | val_0_rmse: 0.89499 | val_1_rmse: 0.89979 |  0:03:47s
epoch 17 | loss: 0.79535 | val_0_rmse: 0.89666 | val_1_rmse: 0.89574 |  0:04:01s
epoch 18 | loss: 0.79569 | val_0_rmse: 0.92558 | val_1_rmse: 0.89693 |  0:04:14s
epoch 19 | loss: 0.79503 | val_0_rmse: 0.89044 | val_1_rmse: 0.8957  |  0:04:27s
epoch 20 | loss: 0.79466 | val_0_rmse: 0.89819 | val_1_rmse: 0.89716 |  0:04:41s
epoch 21 | loss: 0.79481 | val_0_rmse: 0.91098 | val_1_rmse: 0.89598 |  0:04:54s
epoch 22 | loss: 0.79487 | val_0_rmse: 0.89235 | val_1_rmse: 0.89645 |  0:05:07s
epoch 23 | loss: 0.79717 | val_0_rmse: 0.89596 | val_1_rmse: 0.90101 |  0:05:21s
epoch 24 | loss: 0.81295 | val_0_rmse: 1.30969 | val_1_rmse: 0.90874 |  0:05:34s
epoch 25 | loss: 0.81457 | val_0_rmse: 0.92202 | val_1_rmse: 0.90745 |  0:05:48s
epoch 26 | loss: 0.81482 | val_0_rmse: 0.9169  | val_1_rmse: 0.90722 |  0:06:01s
epoch 27 | loss: 0.81396 | val_0_rmse: 1.19353 | val_1_rmse: 0.90727 |  0:06:15s
epoch 28 | loss: 0.81405 | val_0_rmse: 0.90171 | val_1_rmse: 0.90752 |  0:06:28s
epoch 29 | loss: 0.81331 | val_0_rmse: 0.90109 | val_1_rmse: 0.9068  |  0:06:42s
epoch 30 | loss: 0.81281 | val_0_rmse: 0.90238 | val_1_rmse: 0.90835 |  0:06:56s
epoch 31 | loss: 0.81319 | val_0_rmse: 3.24888 | val_1_rmse: 0.90652 |  0:07:09s
epoch 32 | loss: 0.81248 | val_0_rmse: 3.69344 | val_1_rmse: 0.9065  |  0:07:22s
epoch 33 | loss: 0.8094  | val_0_rmse: 1.83383 | val_1_rmse: 0.89664 |  0:07:35s
epoch 34 | loss: 0.79562 | val_0_rmse: 0.89334 | val_1_rmse: 0.8973  |  0:07:50s
epoch 35 | loss: 0.79426 | val_0_rmse: 0.90217 | val_1_rmse: 0.89562 |  0:08:04s
epoch 36 | loss: 0.79569 | val_0_rmse: 0.89735 | val_1_rmse: 0.89695 |  0:08:17s
epoch 37 | loss: 0.79597 | val_0_rmse: 0.89402 | val_1_rmse: 0.89825 |  0:08:31s
epoch 38 | loss: 0.79486 | val_0_rmse: 0.89516 | val_1_rmse: 0.90237 |  0:08:44s
epoch 39 | loss: 0.7954  | val_0_rmse: 0.89689 | val_1_rmse: 0.89793 |  0:08:57s
epoch 40 | loss: 0.79491 | val_0_rmse: 0.89585 | val_1_rmse: 0.89676 |  0:09:11s
epoch 41 | loss: 0.7965  | val_0_rmse: 0.89074 | val_1_rmse: 0.89589 |  0:09:24s
epoch 42 | loss: 0.81818 | val_0_rmse: 0.94221 | val_1_rmse: 0.90143 |  0:09:37s
epoch 43 | loss: 0.8004  | val_0_rmse: 0.89437 | val_1_rmse: 0.89963 |  0:09:51s
epoch 44 | loss: 0.79873 | val_0_rmse: 0.8935  | val_1_rmse: 0.89826 |  0:10:04s
epoch 45 | loss: 0.7975  | val_0_rmse: 0.89353 | val_1_rmse: 0.89846 |  0:10:17s
epoch 46 | loss: 0.79713 | val_0_rmse: 0.89283 | val_1_rmse: 0.89794 |  0:10:31s
epoch 47 | loss: 0.79778 | val_0_rmse: 0.89214 | val_1_rmse: 0.89722 |  0:10:44s
epoch 48 | loss: 0.79678 | val_0_rmse: 0.89202 | val_1_rmse: 0.89704 |  0:10:57s
epoch 49 | loss: 0.7966  | val_0_rmse: 0.89182 | val_1_rmse: 0.89689 |  0:11:11s
epoch 50 | loss: 0.79661 | val_0_rmse: 0.89192 | val_1_rmse: 1.19136 |  0:11:24s
epoch 51 | loss: 0.7962  | val_0_rmse: 0.91352 | val_1_rmse: 1.25965 |  0:11:37s
epoch 52 | loss: 0.79644 | val_0_rmse: 0.91139 | val_1_rmse: 0.91731 |  0:11:50s
epoch 53 | loss: 0.79658 | val_0_rmse: 0.91379 | val_1_rmse: 1.00091 |  0:12:04s
epoch 54 | loss: 0.79608 | val_0_rmse: 0.90678 | val_1_rmse: 1.17789 |  0:12:17s
epoch 55 | loss: 0.79625 | val_0_rmse: 0.90734 | val_1_rmse: 1.18453 |  0:12:30s
epoch 56 | loss: 0.79664 | val_0_rmse: 0.90407 | val_1_rmse: 1.16361 |  0:12:44s
epoch 57 | loss: 0.79613 | val_0_rmse: 0.90006 | val_1_rmse: 1.09769 |  0:12:57s
epoch 58 | loss: 0.79618 | val_0_rmse: 0.90237 | val_1_rmse: 0.94238 |  0:13:10s
epoch 59 | loss: 0.79686 | val_0_rmse: 0.89185 | val_1_rmse: 1.01514 |  0:13:23s
epoch 60 | loss: 0.7959  | val_0_rmse: 0.89289 | val_1_rmse: 1.00762 |  0:13:37s
epoch 61 | loss: 0.79635 | val_0_rmse: 0.90703 | val_1_rmse: 0.96059 |  0:13:50s
epoch 62 | loss: 0.7963  | val_0_rmse: 0.9167  | val_1_rmse: 0.98753 |  0:14:04s
epoch 63 | loss: 0.79615 | val_0_rmse: 0.92126 | val_1_rmse: 0.8981  |  0:14:17s
epoch 64 | loss: 0.79574 | val_0_rmse: 0.93621 | val_1_rmse: 0.90096 |  0:14:30s
epoch 65 | loss: 0.7955  | val_0_rmse: 0.89532 | val_1_rmse: 0.90114 |  0:14:44s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.89562
Best weights from best epoch are automatically used!
ended training at: 07:55:10
Feature importance:
Mean squared error is of 5280018652.030172
Mean absolute error:56134.807326694965
MAPE:0.6082770361699317
R2 score:0.20598541153586236
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 6
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:55:11
epoch 0  | loss: 0.85885 | val_0_rmse: 0.91506 | val_1_rmse: 0.91233 |  0:00:13s
epoch 1  | loss: 0.80747 | val_0_rmse: 0.90439 | val_1_rmse: 0.90132 |  0:00:27s
epoch 2  | loss: 0.80765 | val_0_rmse: 0.89973 | val_1_rmse: 0.89745 |  0:00:40s
epoch 3  | loss: 0.80771 | val_0_rmse: 0.90371 | val_1_rmse: 0.90095 |  0:00:54s
epoch 4  | loss: 0.80552 | val_0_rmse: 0.89814 | val_1_rmse: 0.89529 |  0:01:07s
epoch 5  | loss: 0.8033  | val_0_rmse: 0.89799 | val_1_rmse: 0.89568 |  0:01:20s
epoch 6  | loss: 0.80551 | val_0_rmse: 0.89742 | val_1_rmse: 0.89404 |  0:01:34s
epoch 7  | loss: 0.81636 | val_0_rmse: 0.90767 | val_1_rmse: 0.90722 |  0:01:47s
epoch 8  | loss: 0.81085 | val_0_rmse: 0.9042  | val_1_rmse: 0.90283 |  0:02:00s
epoch 9  | loss: 0.80808 | val_0_rmse: 0.89739 | val_1_rmse: 0.89509 |  0:02:14s
epoch 10 | loss: 0.80723 | val_0_rmse: 0.89963 | val_1_rmse: 0.89719 |  0:02:27s
epoch 11 | loss: 0.80907 | val_0_rmse: 0.89733 | val_1_rmse: 0.89556 |  0:02:40s
epoch 12 | loss: 0.80621 | val_0_rmse: 0.89708 | val_1_rmse: 0.89495 |  0:02:54s
epoch 13 | loss: 0.80682 | val_0_rmse: 0.89797 | val_1_rmse: 0.89633 |  0:03:07s
epoch 14 | loss: 0.80637 | val_0_rmse: 1.13133 | val_1_rmse: 0.89459 |  0:03:20s
epoch 15 | loss: 0.80548 | val_0_rmse: 1.22623 | val_1_rmse: 0.89357 |  0:03:34s
epoch 16 | loss: 0.80423 | val_0_rmse: 0.89732 | val_1_rmse: 0.89511 |  0:03:47s
epoch 17 | loss: 0.80564 | val_0_rmse: 1.17497 | val_1_rmse: 0.8956  |  0:04:00s
epoch 18 | loss: 0.81704 | val_0_rmse: 0.90151 | val_1_rmse: 0.90224 |  0:04:14s
epoch 19 | loss: 0.80903 | val_0_rmse: 0.98887 | val_1_rmse: 0.90511 |  0:04:27s
epoch 20 | loss: 0.80756 | val_0_rmse: 0.90224 | val_1_rmse: 0.898   |  0:04:40s
epoch 21 | loss: 0.80542 | val_0_rmse: 0.89586 | val_1_rmse: 0.89309 |  0:04:54s
epoch 22 | loss: 0.8067  | val_0_rmse: 0.9424  | val_1_rmse: 0.93957 |  0:05:07s
epoch 23 | loss: 0.80655 | val_0_rmse: 0.9057  | val_1_rmse: 0.90306 |  0:05:20s
epoch 24 | loss: 0.80554 | val_0_rmse: 0.90496 | val_1_rmse: 0.90192 |  0:05:34s
epoch 25 | loss: 0.80456 | val_0_rmse: 0.90678 | val_1_rmse: 0.90361 |  0:05:47s
epoch 26 | loss: 0.80369 | val_0_rmse: 0.89672 | val_1_rmse: 0.89275 |  0:06:00s
epoch 27 | loss: 0.8025  | val_0_rmse: 0.89567 | val_1_rmse: 0.89163 |  0:06:14s
epoch 28 | loss: 0.80275 | val_0_rmse: 0.92882 | val_1_rmse: 0.92635 |  0:06:27s
epoch 29 | loss: 0.80458 | val_0_rmse: 0.94877 | val_1_rmse: 0.94577 |  0:06:41s
epoch 30 | loss: 0.8328  | val_0_rmse: 0.9202  | val_1_rmse: 0.92841 |  0:06:54s
epoch 31 | loss: 0.83087 | val_0_rmse: 0.91201 | val_1_rmse: 0.90732 |  0:07:07s
epoch 32 | loss: 0.83029 | val_0_rmse: 0.91052 | val_1_rmse: 0.90509 |  0:07:21s
epoch 33 | loss: 0.83006 | val_0_rmse: 0.92359 | val_1_rmse: 0.91942 |  0:07:34s
epoch 34 | loss: 0.82943 | val_0_rmse: 0.91047 | val_1_rmse: 0.90577 |  0:07:48s
epoch 35 | loss: 0.82891 | val_0_rmse: 0.90926 | val_1_rmse: 0.90427 |  0:08:01s
epoch 36 | loss: 0.82866 | val_0_rmse: 0.91194 | val_1_rmse: 0.90696 |  0:08:14s
epoch 37 | loss: 0.82924 | val_0_rmse: 0.9118  | val_1_rmse: 0.90679 |  0:08:28s
epoch 38 | loss: 0.82901 | val_0_rmse: 0.91193 | val_1_rmse: 0.90661 |  0:08:41s
epoch 39 | loss: 0.82861 | val_0_rmse: 0.91046 | val_1_rmse: 0.90418 |  0:08:54s
epoch 40 | loss: 0.82832 | val_0_rmse: 0.90907 | val_1_rmse: 0.9037  |  0:09:08s
epoch 41 | loss: 0.82849 | val_0_rmse: 0.91051 | val_1_rmse: 0.90584 |  0:09:21s
epoch 42 | loss: 0.82864 | val_0_rmse: 0.90981 | val_1_rmse: 0.90397 |  0:09:35s
epoch 43 | loss: 0.82808 | val_0_rmse: 0.91298 | val_1_rmse: 0.90435 |  0:09:48s
epoch 44 | loss: 0.82909 | val_0_rmse: 0.91234 | val_1_rmse: 0.90504 |  0:10:01s
epoch 45 | loss: 0.82975 | val_0_rmse: 0.91075 | val_1_rmse: 0.90557 |  0:10:15s
epoch 46 | loss: 0.82869 | val_0_rmse: 0.91065 | val_1_rmse: 0.90575 |  0:10:28s
epoch 47 | loss: 0.82817 | val_0_rmse: 0.91055 | val_1_rmse: 0.90468 |  0:10:41s
epoch 48 | loss: 0.82868 | val_0_rmse: 0.91659 | val_1_rmse: 0.91016 |  0:10:55s
epoch 49 | loss: 0.8281  | val_0_rmse: 0.91193 | val_1_rmse: 0.90521 |  0:11:08s
epoch 50 | loss: 0.82895 | val_0_rmse: 0.91132 | val_1_rmse: 0.90509 |  0:11:21s
epoch 51 | loss: 0.82828 | val_0_rmse: 0.91244 | val_1_rmse: 0.90752 |  0:11:35s
epoch 52 | loss: 0.82869 | val_0_rmse: 0.91061 | val_1_rmse: 0.90546 |  0:11:48s
epoch 53 | loss: 0.82896 | val_0_rmse: 0.91186 | val_1_rmse: 0.90698 |  0:12:01s
epoch 54 | loss: 0.82888 | val_0_rmse: 0.90892 | val_1_rmse: 0.90415 |  0:12:15s
epoch 55 | loss: 0.82863 | val_0_rmse: 0.9097  | val_1_rmse: 0.90459 |  0:12:28s
epoch 56 | loss: 0.82861 | val_0_rmse: 0.9089  | val_1_rmse: 0.90378 |  0:12:41s
epoch 57 | loss: 0.82858 | val_0_rmse: 0.99175 | val_1_rmse: 0.97356 |  0:12:55s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.89163
Best weights from best epoch are automatically used!
ended training at: 08:08:11
Feature importance:
Mean squared error is of 5285753043.826521
Mean absolute error:56618.06331186501
MAPE:0.6253036382133054
R2 score:0.19793709018755246
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 7
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:08:12
epoch 0  | loss: 0.85975 | val_0_rmse: 0.91223 | val_1_rmse: 0.90862 |  0:00:13s
epoch 1  | loss: 0.80808 | val_0_rmse: 0.91025 | val_1_rmse: 0.90679 |  0:00:26s
epoch 2  | loss: 0.80705 | val_0_rmse: 0.90041 | val_1_rmse: 0.89597 |  0:00:40s
epoch 3  | loss: 0.80618 | val_0_rmse: 0.89758 | val_1_rmse: 0.89395 |  0:00:53s
epoch 4  | loss: 0.80476 | val_0_rmse: 0.89593 | val_1_rmse: 0.89166 |  0:01:06s
epoch 5  | loss: 0.80498 | val_0_rmse: 0.89579 | val_1_rmse: 0.89142 |  0:01:20s
epoch 6  | loss: 0.8048  | val_0_rmse: 0.89883 | val_1_rmse: 0.89413 |  0:01:33s
epoch 7  | loss: 0.80417 | val_0_rmse: 0.89629 | val_1_rmse: 0.8921  |  0:01:47s
epoch 8  | loss: 0.80424 | val_0_rmse: 0.8979  | val_1_rmse: 0.89247 |  0:02:00s
epoch 9  | loss: 0.80377 | val_0_rmse: 0.89853 | val_1_rmse: 0.89207 |  0:02:14s
epoch 10 | loss: 0.80557 | val_0_rmse: 0.9007  | val_1_rmse: 0.89278 |  0:02:27s
epoch 11 | loss: 0.804   | val_0_rmse: 0.90314 | val_1_rmse: 0.89275 |  0:02:41s
epoch 12 | loss: 0.8037  | val_0_rmse: 0.90017 | val_1_rmse: 0.8914  |  0:02:54s
epoch 13 | loss: 0.80519 | val_0_rmse: 0.89642 | val_1_rmse: 0.89287 |  0:03:08s
epoch 14 | loss: 0.80418 | val_0_rmse: 0.90785 | val_1_rmse: 0.93799 |  0:03:21s
epoch 15 | loss: 0.80521 | val_0_rmse: 0.89684 | val_1_rmse: 0.90507 |  0:03:34s
epoch 16 | loss: 0.80381 | val_0_rmse: 0.89752 | val_1_rmse: 0.89872 |  0:03:48s
epoch 17 | loss: 0.80333 | val_0_rmse: 0.89861 | val_1_rmse: 1.07057 |  0:04:01s
epoch 18 | loss: 0.80278 | val_0_rmse: 0.90152 | val_1_rmse: 0.91085 |  0:04:15s
epoch 19 | loss: 0.80412 | val_0_rmse: 0.90333 | val_1_rmse: 0.92221 |  0:04:28s
epoch 20 | loss: 0.8033  | val_0_rmse: 0.89843 | val_1_rmse: 0.89223 |  0:04:41s
epoch 21 | loss: 0.80394 | val_0_rmse: 0.89703 | val_1_rmse: 0.89224 |  0:04:54s
epoch 22 | loss: 0.80398 | val_0_rmse: 0.89689 | val_1_rmse: 0.89695 |  0:05:08s
epoch 23 | loss: 0.80312 | val_0_rmse: 0.89688 | val_1_rmse: 0.89942 |  0:05:21s
epoch 24 | loss: 0.80413 | val_0_rmse: 0.90932 | val_1_rmse: 0.89961 |  0:05:35s
epoch 25 | loss: 0.80424 | val_0_rmse: 0.89677 | val_1_rmse: 0.89233 |  0:05:48s
epoch 26 | loss: 0.80464 | val_0_rmse: 0.89628 | val_1_rmse: 0.89233 |  0:06:02s
epoch 27 | loss: 0.80365 | val_0_rmse: 0.89548 | val_1_rmse: 0.89128 |  0:06:15s
epoch 28 | loss: 0.80325 | val_0_rmse: 0.89656 | val_1_rmse: 0.89221 |  0:06:29s
epoch 29 | loss: 0.8029  | val_0_rmse: 0.89575 | val_1_rmse: 0.89143 |  0:06:42s
epoch 30 | loss: 0.80298 | val_0_rmse: 0.89653 | val_1_rmse: 0.89265 |  0:06:56s
epoch 31 | loss: 0.80484 | val_0_rmse: 0.89859 | val_1_rmse: 0.89506 |  0:07:09s
epoch 32 | loss: 0.80474 | val_0_rmse: 0.89614 | val_1_rmse: 0.89188 |  0:07:23s
epoch 33 | loss: 0.80385 | val_0_rmse: 0.89613 | val_1_rmse: 0.89181 |  0:07:36s
epoch 34 | loss: 0.8043  | val_0_rmse: 0.89642 | val_1_rmse: 0.89229 |  0:07:50s
epoch 35 | loss: 0.80392 | val_0_rmse: 0.89642 | val_1_rmse: 0.89231 |  0:08:03s
epoch 36 | loss: 0.80371 | val_0_rmse: 0.89614 | val_1_rmse: 0.8921  |  0:08:17s
epoch 37 | loss: 0.80373 | val_0_rmse: 0.89648 | val_1_rmse: 0.89233 |  0:08:31s
epoch 38 | loss: 0.8046  | val_0_rmse: 0.89643 | val_1_rmse: 0.89217 |  0:08:44s
epoch 39 | loss: 0.80675 | val_0_rmse: 0.89774 | val_1_rmse: 0.89458 |  0:08:57s
epoch 40 | loss: 0.80598 | val_0_rmse: 0.89707 | val_1_rmse: 1.2936  |  0:09:10s
epoch 41 | loss: 0.80601 | val_0_rmse: 0.89705 | val_1_rmse: 0.8988  |  0:09:24s
epoch 42 | loss: 0.80456 | val_0_rmse: 0.89752 | val_1_rmse: 0.89275 |  0:09:37s
epoch 43 | loss: 0.80446 | val_0_rmse: 0.89609 | val_1_rmse: 0.90169 |  0:09:51s
epoch 44 | loss: 0.80434 | val_0_rmse: 0.89635 | val_1_rmse: 0.89168 |  0:10:04s
epoch 45 | loss: 0.80395 | val_0_rmse: 0.89623 | val_1_rmse: 0.89155 |  0:10:17s
epoch 46 | loss: 0.80427 | val_0_rmse: 0.89647 | val_1_rmse: 0.89191 |  0:10:31s
epoch 47 | loss: 0.80505 | val_0_rmse: 0.89639 | val_1_rmse: 0.89215 |  0:10:44s
epoch 48 | loss: 0.80431 | val_0_rmse: 0.89665 | val_1_rmse: 0.89224 |  0:10:57s
epoch 49 | loss: 0.80428 | val_0_rmse: 0.89653 | val_1_rmse: 0.89208 |  0:11:10s
epoch 50 | loss: 0.80378 | val_0_rmse: 0.89674 | val_1_rmse: 0.89229 |  0:11:24s
epoch 51 | loss: 0.80376 | val_0_rmse: 0.90267 | val_1_rmse: 0.89859 |  0:11:37s
epoch 52 | loss: 0.80426 | val_0_rmse: 0.91327 | val_1_rmse: 0.91596 |  0:11:50s
epoch 53 | loss: 0.80827 | val_0_rmse: 0.9005  | val_1_rmse: 0.89834 |  0:12:04s
epoch 54 | loss: 0.80845 | val_0_rmse: 0.90827 | val_1_rmse: 0.90313 |  0:12:17s
epoch 55 | loss: 0.80442 | val_0_rmse: 0.90655 | val_1_rmse: 0.93903 |  0:12:30s
epoch 56 | loss: 0.80465 | val_0_rmse: 0.89867 | val_1_rmse: 0.93331 |  0:12:43s
epoch 57 | loss: 0.80409 | val_0_rmse: 0.89824 | val_1_rmse: 0.92573 |  0:12:57s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.89128
Best weights from best epoch are automatically used!
ended training at: 08:21:13
Feature importance:
Mean squared error is of 5161370548.101328
Mean absolute error:55654.8513189772
MAPE:0.6118053541474665
R2 score:0.20780325743602834
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 8
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:21:15
epoch 0  | loss: 0.85794 | val_0_rmse: 0.97075 | val_1_rmse: 0.96664 |  0:00:13s
epoch 1  | loss: 0.80479 | val_0_rmse: 0.91148 | val_1_rmse: 0.9078  |  0:00:26s
epoch 2  | loss: 0.80255 | val_0_rmse: 0.90096 | val_1_rmse: 0.89913 |  0:00:40s
epoch 3  | loss: 0.80071 | val_0_rmse: 0.89478 | val_1_rmse: 0.89269 |  0:00:53s
epoch 4  | loss: 0.80007 | val_0_rmse: 0.89529 | val_1_rmse: 0.89316 |  0:01:07s
epoch 5  | loss: 0.79936 | val_0_rmse: 0.89325 | val_1_rmse: 0.89153 |  0:01:20s
epoch 6  | loss: 0.79987 | val_0_rmse: 0.89454 | val_1_rmse: 0.89177 |  0:01:33s
epoch 7  | loss: 0.80046 | val_0_rmse: 0.89613 | val_1_rmse: 0.89338 |  0:01:47s
epoch 8  | loss: 0.80266 | val_0_rmse: 0.89399 | val_1_rmse: 0.89269 |  0:02:00s
epoch 9  | loss: 0.8006  | val_0_rmse: 0.89564 | val_1_rmse: 0.89398 |  0:02:14s
epoch 10 | loss: 0.80088 | val_0_rmse: 0.895   | val_1_rmse: 0.90135 |  0:02:27s
epoch 11 | loss: 0.80003 | val_0_rmse: 0.91223 | val_1_rmse: 0.9993  |  0:02:40s
epoch 12 | loss: 0.80041 | val_0_rmse: 0.89374 | val_1_rmse: 0.89221 |  0:02:53s
epoch 13 | loss: 0.79958 | val_0_rmse: 0.89369 | val_1_rmse: 0.89273 |  0:03:06s
epoch 14 | loss: 0.79986 | val_0_rmse: 0.92358 | val_1_rmse: 0.93524 |  0:03:20s
epoch 15 | loss: 0.8025  | val_0_rmse: 0.89312 | val_1_rmse: 0.89181 |  0:03:33s
epoch 16 | loss: 0.80044 | val_0_rmse: 0.8934  | val_1_rmse: 0.89167 |  0:03:46s
epoch 17 | loss: 0.79972 | val_0_rmse: 0.89432 | val_1_rmse: 0.89609 |  0:03:59s
epoch 18 | loss: 0.7994  | val_0_rmse: 0.90278 | val_1_rmse: 0.96139 |  0:04:13s
epoch 19 | loss: 0.7991  | val_0_rmse: 0.89464 | val_1_rmse: 0.89528 |  0:04:26s
epoch 20 | loss: 0.79887 | val_0_rmse: 0.8937  | val_1_rmse: 0.89201 |  0:04:39s
epoch 21 | loss: 0.79866 | val_0_rmse: 0.89335 | val_1_rmse: 0.89193 |  0:04:53s
epoch 22 | loss: 0.79972 | val_0_rmse: 0.8953  | val_1_rmse: 0.89463 |  0:05:06s
epoch 23 | loss: 0.80112 | val_0_rmse: 0.89441 | val_1_rmse: 0.89232 |  0:05:20s
epoch 24 | loss: 0.79974 | val_0_rmse: 0.89538 | val_1_rmse: 0.89492 |  0:05:33s
epoch 25 | loss: 0.80029 | val_0_rmse: 0.89437 | val_1_rmse: 0.89221 |  0:05:46s
epoch 26 | loss: 0.80012 | val_0_rmse: 0.89464 | val_1_rmse: 0.89283 |  0:06:00s
epoch 27 | loss: 0.80001 | val_0_rmse: 0.89484 | val_1_rmse: 0.89295 |  0:06:13s
epoch 28 | loss: 0.80008 | val_0_rmse: 0.89409 | val_1_rmse: 0.89253 |  0:06:27s
epoch 29 | loss: 0.79957 | val_0_rmse: 0.89491 | val_1_rmse: 0.89378 |  0:06:40s
epoch 30 | loss: 0.80005 | val_0_rmse: 0.89464 | val_1_rmse: 0.89286 |  0:06:53s
epoch 31 | loss: 0.80583 | val_0_rmse: 0.89658 | val_1_rmse: 0.89483 |  0:07:06s
epoch 32 | loss: 0.80369 | val_0_rmse: 0.89592 | val_1_rmse: 0.89503 |  0:07:20s
epoch 33 | loss: 0.80416 | val_0_rmse: 0.89739 | val_1_rmse: 0.8956  |  0:07:33s
epoch 34 | loss: 0.80592 | val_0_rmse: 0.89769 | val_1_rmse: 0.89557 |  0:07:46s
epoch 35 | loss: 0.80548 | val_0_rmse: 0.89851 | val_1_rmse: 0.89683 |  0:08:00s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 0.89153
Best weights from best epoch are automatically used!
ended training at: 08:29:19
Feature importance:
Mean squared error is of 5265204468.160269
Mean absolute error:55975.17964856607
MAPE:0.6076647342128215
R2 score:0.20435443059304437
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 9
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:29:20
epoch 0  | loss: 0.87362 | val_0_rmse: 1.01933 | val_1_rmse: 1.01126 |  0:00:13s
epoch 1  | loss: 0.80692 | val_0_rmse: 0.92462 | val_1_rmse: 0.9224  |  0:00:26s
epoch 2  | loss: 0.804   | val_0_rmse: 0.89973 | val_1_rmse: 0.89456 |  0:00:39s
epoch 3  | loss: 0.80434 | val_0_rmse: 0.95701 | val_1_rmse: 0.94689 |  0:00:53s
epoch 4  | loss: 0.81201 | val_0_rmse: 0.89709 | val_1_rmse: 0.89093 |  0:01:06s
epoch 5  | loss: 0.80831 | val_0_rmse: 0.90049 | val_1_rmse: 0.89297 |  0:01:20s
epoch 6  | loss: 0.80763 | val_0_rmse: 0.96978 | val_1_rmse: 0.91353 |  0:01:33s
epoch 7  | loss: 0.80575 | val_0_rmse: 0.89627 | val_1_rmse: 0.889   |  0:01:46s
epoch 8  | loss: 0.80502 | val_0_rmse: 0.91648 | val_1_rmse: 0.89503 |  0:02:00s
epoch 9  | loss: 0.80524 | val_0_rmse: 0.90181 | val_1_rmse: 0.89134 |  0:02:13s
epoch 10 | loss: 0.80419 | val_0_rmse: 0.89637 | val_1_rmse: 0.88845 |  0:02:27s
epoch 11 | loss: 0.80413 | val_0_rmse: 0.89667 | val_1_rmse: 0.88867 |  0:02:40s
epoch 12 | loss: 0.80402 | val_0_rmse: 0.89692 | val_1_rmse: 0.8901  |  0:02:54s
epoch 13 | loss: 0.80389 | val_0_rmse: 0.91077 | val_1_rmse: 0.90844 |  0:03:08s
epoch 14 | loss: 0.80482 | val_0_rmse: 0.90588 | val_1_rmse: 0.89327 |  0:03:22s
epoch 15 | loss: 0.80374 | val_0_rmse: 0.90244 | val_1_rmse: 0.89074 |  0:03:36s
epoch 16 | loss: 0.80352 | val_0_rmse: 0.99229 | val_1_rmse: 0.92008 |  0:03:49s
epoch 17 | loss: 0.80346 | val_0_rmse: 0.89709 | val_1_rmse: 0.88951 |  0:04:03s
epoch 18 | loss: 0.80361 | val_0_rmse: 1.13621 | val_1_rmse: 0.97059 |  0:04:17s
epoch 19 | loss: 0.80312 | val_0_rmse: 1.7052  | val_1_rmse: 1.2071  |  0:04:31s
epoch 20 | loss: 0.80373 | val_0_rmse: 3.17649 | val_1_rmse: 1.91464 |  0:04:45s
epoch 21 | loss: 0.80291 | val_0_rmse: 1.11451 | val_1_rmse: 0.96971 |  0:04:58s
epoch 22 | loss: 0.80343 | val_0_rmse: 1.36587 | val_1_rmse: 1.06196 |  0:05:11s
epoch 23 | loss: 0.80328 | val_0_rmse: 2.18044 | val_1_rmse: 1.41961 |  0:05:24s
epoch 24 | loss: 0.80299 | val_0_rmse: 1.11754 | val_1_rmse: 0.96899 |  0:05:38s
epoch 25 | loss: 0.80296 | val_0_rmse: 1.03971 | val_1_rmse: 0.93951 |  0:05:51s
epoch 26 | loss: 0.8028  | val_0_rmse: 0.9792  | val_1_rmse: 0.91588 |  0:06:04s
epoch 27 | loss: 0.80331 | val_0_rmse: 2.08487 | val_1_rmse: 1.37345 |  0:06:18s
epoch 28 | loss: 0.80416 | val_0_rmse: 1.43872 | val_1_rmse: 1.08843 |  0:06:31s
epoch 29 | loss: 0.80504 | val_0_rmse: 0.97713 | val_1_rmse: 0.91684 |  0:06:45s
epoch 30 | loss: 0.8045  | val_0_rmse: 0.89772 | val_1_rmse: 0.89006 |  0:06:59s
epoch 31 | loss: 0.80531 | val_0_rmse: 1.08762 | val_1_rmse: 0.95486 |  0:07:13s
epoch 32 | loss: 0.80435 | val_0_rmse: 1.47884 | val_1_rmse: 1.1061  |  0:07:27s
epoch 33 | loss: 0.80401 | val_0_rmse: 0.89691 | val_1_rmse: 0.88939 |  0:07:40s
epoch 34 | loss: 0.80382 | val_0_rmse: 0.91536 | val_1_rmse: 0.9062  |  0:07:54s
epoch 35 | loss: 0.8041  | val_0_rmse: 0.99349 | val_1_rmse: 0.92108 |  0:08:07s
epoch 36 | loss: 0.80401 | val_0_rmse: 1.08601 | val_1_rmse: 0.95385 |  0:08:21s
epoch 37 | loss: 0.80405 | val_0_rmse: 1.26101 | val_1_rmse: 1.01822 |  0:08:35s
epoch 38 | loss: 0.80485 | val_0_rmse: 1.15369 | val_1_rmse: 0.97827 |  0:08:49s
epoch 39 | loss: 0.80403 | val_0_rmse: 2.04443 | val_1_rmse: 1.3583  |  0:09:03s
epoch 40 | loss: 0.80744 | val_0_rmse: 0.90413 | val_1_rmse: 0.89638 |  0:09:17s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.88845
Best weights from best epoch are automatically used!
ended training at: 08:38:42
Feature importance:
Mean squared error is of 5360458864.501642
Mean absolute error:56072.02553188212
MAPE:0.5988520448445689
R2 score:0.1983598336577317
------------------------------------------------------------------
