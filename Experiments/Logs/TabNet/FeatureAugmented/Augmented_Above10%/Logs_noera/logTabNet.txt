TabNet Logs:

Saving copy of script...
In this script the datasets were geopy augmented.This is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:19:41
epoch 0  | loss: 1.22069 | val_0_rmse: 0.99912 | val_1_rmse: 1.001   |  0:00:05s
epoch 1  | loss: 0.98702 | val_0_rmse: 0.99588 | val_1_rmse: 0.99816 |  0:00:08s
epoch 2  | loss: 0.93271 | val_0_rmse: 0.97188 | val_1_rmse: 0.97197 |  0:00:10s
epoch 3  | loss: 0.70819 | val_0_rmse: 0.91373 | val_1_rmse: 0.91809 |  0:00:12s
epoch 4  | loss: 0.51986 | val_0_rmse: 0.87852 | val_1_rmse: 0.88228 |  0:00:14s
epoch 5  | loss: 0.45588 | val_0_rmse: 0.84422 | val_1_rmse: 0.84843 |  0:00:16s
epoch 6  | loss: 0.41428 | val_0_rmse: 0.78937 | val_1_rmse: 0.79384 |  0:00:18s
epoch 7  | loss: 0.37739 | val_0_rmse: 0.77244 | val_1_rmse: 0.77607 |  0:00:20s
epoch 8  | loss: 0.35807 | val_0_rmse: 0.75649 | val_1_rmse: 0.76242 |  0:00:22s
epoch 9  | loss: 0.3395  | val_0_rmse: 0.73262 | val_1_rmse: 0.73853 |  0:00:24s
epoch 10 | loss: 0.32837 | val_0_rmse: 0.77027 | val_1_rmse: 0.77088 |  0:00:26s
epoch 11 | loss: 0.33503 | val_0_rmse: 0.74747 | val_1_rmse: 0.75289 |  0:00:28s
epoch 12 | loss: 0.32339 | val_0_rmse: 0.7186  | val_1_rmse: 0.72332 |  0:00:30s
epoch 13 | loss: 0.30844 | val_0_rmse: 0.68275 | val_1_rmse: 0.68677 |  0:00:32s
epoch 14 | loss: 0.30207 | val_0_rmse: 0.66468 | val_1_rmse: 0.66974 |  0:00:34s
epoch 15 | loss: 0.30126 | val_0_rmse: 0.64221 | val_1_rmse: 0.64949 |  0:00:36s
epoch 16 | loss: 0.29257 | val_0_rmse: 0.61388 | val_1_rmse: 0.61985 |  0:00:38s
epoch 17 | loss: 0.28825 | val_0_rmse: 0.6027  | val_1_rmse: 0.61094 |  0:00:40s
epoch 18 | loss: 0.28347 | val_0_rmse: 0.57167 | val_1_rmse: 0.58068 |  0:00:42s
epoch 19 | loss: 0.27301 | val_0_rmse: 0.56434 | val_1_rmse: 0.57617 |  0:00:44s
epoch 20 | loss: 0.27552 | val_0_rmse: 0.58412 | val_1_rmse: 0.59597 |  0:00:46s
epoch 21 | loss: 0.28114 | val_0_rmse: 0.55962 | val_1_rmse: 0.57421 |  0:00:47s
epoch 22 | loss: 0.28139 | val_0_rmse: 0.52326 | val_1_rmse: 0.54498 |  0:00:50s
epoch 23 | loss: 0.27723 | val_0_rmse: 0.52198 | val_1_rmse: 0.54431 |  0:00:51s
epoch 24 | loss: 0.27363 | val_0_rmse: 0.52672 | val_1_rmse: 0.55509 |  0:00:54s
epoch 25 | loss: 0.2699  | val_0_rmse: 0.50838 | val_1_rmse: 0.53973 |  0:00:56s
epoch 26 | loss: 0.29147 | val_0_rmse: 0.52866 | val_1_rmse: 0.56016 |  0:00:57s
epoch 27 | loss: 0.27867 | val_0_rmse: 0.50474 | val_1_rmse: 0.5399  |  0:00:59s
epoch 28 | loss: 0.27012 | val_0_rmse: 0.50985 | val_1_rmse: 0.54374 |  0:01:01s
epoch 29 | loss: 0.27029 | val_0_rmse: 0.51197 | val_1_rmse: 0.54937 |  0:01:03s
epoch 30 | loss: 0.26373 | val_0_rmse: 0.50457 | val_1_rmse: 0.54142 |  0:01:05s
epoch 31 | loss: 0.26665 | val_0_rmse: 0.5308  | val_1_rmse: 0.57146 |  0:01:07s
epoch 32 | loss: 0.27883 | val_0_rmse: 0.50992 | val_1_rmse: 0.54583 |  0:01:09s
epoch 33 | loss: 0.26619 | val_0_rmse: 0.49542 | val_1_rmse: 0.53718 |  0:01:11s
epoch 34 | loss: 0.25659 | val_0_rmse: 0.48254 | val_1_rmse: 0.52949 |  0:01:13s
epoch 35 | loss: 0.25587 | val_0_rmse: 0.48415 | val_1_rmse: 0.52886 |  0:01:15s
epoch 36 | loss: 0.25809 | val_0_rmse: 0.49037 | val_1_rmse: 0.53277 |  0:01:17s
epoch 37 | loss: 0.25276 | val_0_rmse: 0.48826 | val_1_rmse: 0.53585 |  0:01:19s
epoch 38 | loss: 0.25148 | val_0_rmse: 0.4844  | val_1_rmse: 0.53119 |  0:01:21s
epoch 39 | loss: 0.24726 | val_0_rmse: 0.48224 | val_1_rmse: 0.53675 |  0:01:23s
epoch 40 | loss: 0.24477 | val_0_rmse: 0.48849 | val_1_rmse: 0.53622 |  0:01:25s
epoch 41 | loss: 0.24402 | val_0_rmse: 0.47433 | val_1_rmse: 0.52995 |  0:01:27s
epoch 42 | loss: 0.24002 | val_0_rmse: 0.47714 | val_1_rmse: 0.53194 |  0:01:29s
epoch 43 | loss: 0.24051 | val_0_rmse: 0.46728 | val_1_rmse: 0.52346 |  0:01:31s
epoch 44 | loss: 0.24513 | val_0_rmse: 0.48013 | val_1_rmse: 0.53198 |  0:01:33s
epoch 45 | loss: 0.24391 | val_0_rmse: 0.47186 | val_1_rmse: 0.53117 |  0:01:35s
epoch 46 | loss: 0.2468  | val_0_rmse: 0.47796 | val_1_rmse: 0.53541 |  0:01:37s
epoch 47 | loss: 0.24085 | val_0_rmse: 0.47995 | val_1_rmse: 0.54074 |  0:01:39s
epoch 48 | loss: 0.24021 | val_0_rmse: 0.46742 | val_1_rmse: 0.52329 |  0:01:41s
epoch 49 | loss: 0.23333 | val_0_rmse: 0.48355 | val_1_rmse: 0.54306 |  0:01:43s
epoch 50 | loss: 0.23814 | val_0_rmse: 0.4629  | val_1_rmse: 0.52593 |  0:01:45s
epoch 51 | loss: 0.23026 | val_0_rmse: 0.45977 | val_1_rmse: 0.52841 |  0:01:47s
epoch 52 | loss: 0.23413 | val_0_rmse: 0.46236 | val_1_rmse: 0.52797 |  0:01:49s
epoch 53 | loss: 0.23132 | val_0_rmse: 0.45995 | val_1_rmse: 0.52221 |  0:01:51s
epoch 54 | loss: 0.23392 | val_0_rmse: 0.47907 | val_1_rmse: 0.54204 |  0:01:53s
epoch 55 | loss: 0.23208 | val_0_rmse: 0.46268 | val_1_rmse: 0.52705 |  0:01:55s
epoch 56 | loss: 0.23254 | val_0_rmse: 0.47342 | val_1_rmse: 0.53899 |  0:01:57s
epoch 57 | loss: 0.23415 | val_0_rmse: 0.45538 | val_1_rmse: 0.52707 |  0:01:59s
epoch 58 | loss: 0.22744 | val_0_rmse: 0.46518 | val_1_rmse: 0.53252 |  0:02:01s
epoch 59 | loss: 0.22451 | val_0_rmse: 0.45381 | val_1_rmse: 0.53482 |  0:02:03s
epoch 60 | loss: 0.22625 | val_0_rmse: 0.46526 | val_1_rmse: 0.53379 |  0:02:05s
epoch 61 | loss: 0.23072 | val_0_rmse: 0.46319 | val_1_rmse: 0.53762 |  0:02:07s
epoch 62 | loss: 0.22776 | val_0_rmse: 0.45471 | val_1_rmse: 0.52891 |  0:02:09s
epoch 63 | loss: 0.22337 | val_0_rmse: 0.55826 | val_1_rmse: 0.60302 |  0:02:11s
epoch 64 | loss: 0.23252 | val_0_rmse: 0.46444 | val_1_rmse: 0.54086 |  0:02:13s
epoch 65 | loss: 0.22549 | val_0_rmse: 0.4693  | val_1_rmse: 0.54392 |  0:02:15s
epoch 66 | loss: 0.23089 | val_0_rmse: 0.46137 | val_1_rmse: 0.5376  |  0:02:17s
epoch 67 | loss: 0.23055 | val_0_rmse: 0.46178 | val_1_rmse: 0.53456 |  0:02:19s
epoch 68 | loss: 0.223   | val_0_rmse: 0.45972 | val_1_rmse: 0.54121 |  0:02:21s
epoch 69 | loss: 0.22245 | val_0_rmse: 0.45554 | val_1_rmse: 0.53663 |  0:02:23s
epoch 70 | loss: 0.22531 | val_0_rmse: 0.46673 | val_1_rmse: 0.54845 |  0:02:25s
epoch 71 | loss: 0.23194 | val_0_rmse: 0.45761 | val_1_rmse: 0.53348 |  0:02:27s
epoch 72 | loss: 0.23057 | val_0_rmse: 0.45845 | val_1_rmse: 0.53317 |  0:02:29s
epoch 73 | loss: 0.22351 | val_0_rmse: 0.45084 | val_1_rmse: 0.52978 |  0:02:31s
epoch 74 | loss: 0.22522 | val_0_rmse: 0.45129 | val_1_rmse: 0.53341 |  0:02:33s
epoch 75 | loss: 0.21805 | val_0_rmse: 0.44212 | val_1_rmse: 0.52891 |  0:02:35s
epoch 76 | loss: 0.21653 | val_0_rmse: 0.44527 | val_1_rmse: 0.53321 |  0:02:37s
epoch 77 | loss: 0.21498 | val_0_rmse: 0.44735 | val_1_rmse: 0.53534 |  0:02:39s
epoch 78 | loss: 0.22042 | val_0_rmse: 0.45714 | val_1_rmse: 0.53401 |  0:02:41s
epoch 79 | loss: 0.21665 | val_0_rmse: 0.44322 | val_1_rmse: 0.52971 |  0:02:43s
epoch 80 | loss: 0.21286 | val_0_rmse: 0.46891 | val_1_rmse: 0.56133 |  0:02:45s
epoch 81 | loss: 0.2148  | val_0_rmse: 0.45012 | val_1_rmse: 0.53802 |  0:02:47s
epoch 82 | loss: 0.22092 | val_0_rmse: 0.44794 | val_1_rmse: 0.53486 |  0:02:49s
epoch 83 | loss: 0.21328 | val_0_rmse: 0.47213 | val_1_rmse: 0.56847 |  0:02:51s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.52221
Best weights from best epoch are automatically used!
ended training at: 06:22:33
Feature importance:
Mean squared error is of 6859899708.0139885
Mean absolute error:57480.8153636048
MAPE:0.1805570866292076
R2 score:0.6935509861049712
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:34
epoch 0  | loss: 1.20527 | val_0_rmse: 0.99554 | val_1_rmse: 1.00489 |  0:00:02s
epoch 1  | loss: 0.96228 | val_0_rmse: 0.98934 | val_1_rmse: 0.99883 |  0:00:04s
epoch 2  | loss: 0.8331  | val_0_rmse: 0.95858 | val_1_rmse: 0.96763 |  0:00:06s
epoch 3  | loss: 0.63827 | val_0_rmse: 1.0047  | val_1_rmse: 1.01695 |  0:00:08s
epoch 4  | loss: 0.52979 | val_0_rmse: 0.91528 | val_1_rmse: 0.92324 |  0:00:10s
epoch 5  | loss: 0.47393 | val_0_rmse: 0.91005 | val_1_rmse: 0.91475 |  0:00:12s
epoch 6  | loss: 0.44671 | val_0_rmse: 0.87614 | val_1_rmse: 0.88093 |  0:00:14s
epoch 7  | loss: 0.40896 | val_0_rmse: 0.86522 | val_1_rmse: 0.86808 |  0:00:16s
epoch 8  | loss: 0.38677 | val_0_rmse: 0.83191 | val_1_rmse: 0.83279 |  0:00:18s
epoch 9  | loss: 0.37251 | val_0_rmse: 0.86945 | val_1_rmse: 0.87334 |  0:00:20s
epoch 10 | loss: 0.36335 | val_0_rmse: 0.83015 | val_1_rmse: 0.83258 |  0:00:22s
epoch 11 | loss: 0.34808 | val_0_rmse: 0.7945  | val_1_rmse: 0.7939  |  0:00:24s
epoch 12 | loss: 0.34304 | val_0_rmse: 0.77227 | val_1_rmse: 0.77301 |  0:00:26s
epoch 13 | loss: 0.33031 | val_0_rmse: 0.75242 | val_1_rmse: 0.7505  |  0:00:28s
epoch 14 | loss: 0.3283  | val_0_rmse: 0.741   | val_1_rmse: 0.74295 |  0:00:30s
epoch 15 | loss: 0.31896 | val_0_rmse: 0.68845 | val_1_rmse: 0.69204 |  0:00:32s
epoch 16 | loss: 0.31177 | val_0_rmse: 0.68688 | val_1_rmse: 0.68894 |  0:00:34s
epoch 17 | loss: 0.31137 | val_0_rmse: 0.67748 | val_1_rmse: 0.68515 |  0:00:36s
epoch 18 | loss: 0.30211 | val_0_rmse: 0.67188 | val_1_rmse: 0.68024 |  0:00:38s
epoch 19 | loss: 0.30192 | val_0_rmse: 0.68679 | val_1_rmse: 0.70044 |  0:00:40s
epoch 20 | loss: 0.2972  | val_0_rmse: 0.61604 | val_1_rmse: 0.63404 |  0:00:42s
epoch 21 | loss: 0.29427 | val_0_rmse: 0.5937  | val_1_rmse: 0.61769 |  0:00:44s
epoch 22 | loss: 0.28912 | val_0_rmse: 0.58509 | val_1_rmse: 0.60488 |  0:00:46s
epoch 23 | loss: 0.28401 | val_0_rmse: 0.55684 | val_1_rmse: 0.58383 |  0:00:48s
epoch 24 | loss: 0.28288 | val_0_rmse: 0.55983 | val_1_rmse: 0.59256 |  0:00:50s
epoch 25 | loss: 0.28563 | val_0_rmse: 0.5514  | val_1_rmse: 0.58269 |  0:00:52s
epoch 26 | loss: 0.27777 | val_0_rmse: 0.54232 | val_1_rmse: 0.57566 |  0:00:54s
epoch 27 | loss: 0.27459 | val_0_rmse: 0.5301  | val_1_rmse: 0.56658 |  0:00:56s
epoch 28 | loss: 0.27498 | val_0_rmse: 0.50259 | val_1_rmse: 0.54468 |  0:00:58s
epoch 29 | loss: 0.27148 | val_0_rmse: 0.50974 | val_1_rmse: 0.55789 |  0:01:00s
epoch 30 | loss: 0.26941 | val_0_rmse: 0.54655 | val_1_rmse: 0.58233 |  0:01:02s
epoch 31 | loss: 0.26848 | val_0_rmse: 0.49911 | val_1_rmse: 0.54571 |  0:01:04s
epoch 32 | loss: 0.26472 | val_0_rmse: 0.50021 | val_1_rmse: 0.55184 |  0:01:06s
epoch 33 | loss: 0.25859 | val_0_rmse: 0.49369 | val_1_rmse: 0.54296 |  0:01:08s
epoch 34 | loss: 0.25689 | val_0_rmse: 0.51355 | val_1_rmse: 0.55926 |  0:01:10s
epoch 35 | loss: 0.26057 | val_0_rmse: 0.53091 | val_1_rmse: 0.57502 |  0:01:12s
epoch 36 | loss: 0.25477 | val_0_rmse: 0.49309 | val_1_rmse: 0.5497  |  0:01:14s
epoch 37 | loss: 0.26544 | val_0_rmse: 0.50588 | val_1_rmse: 0.56075 |  0:01:16s
epoch 38 | loss: 0.26987 | val_0_rmse: 0.48786 | val_1_rmse: 0.54187 |  0:01:18s
epoch 39 | loss: 0.26366 | val_0_rmse: 0.49953 | val_1_rmse: 0.54585 |  0:01:20s
epoch 40 | loss: 0.25206 | val_0_rmse: 0.48806 | val_1_rmse: 0.54379 |  0:01:22s
epoch 41 | loss: 0.25052 | val_0_rmse: 0.48171 | val_1_rmse: 0.53874 |  0:01:24s
epoch 42 | loss: 0.24521 | val_0_rmse: 0.49286 | val_1_rmse: 0.54849 |  0:01:26s
epoch 43 | loss: 0.24955 | val_0_rmse: 0.50773 | val_1_rmse: 0.56218 |  0:01:28s
epoch 44 | loss: 0.25608 | val_0_rmse: 0.52092 | val_1_rmse: 0.58277 |  0:01:30s
epoch 45 | loss: 0.2544  | val_0_rmse: 0.52474 | val_1_rmse: 0.57423 |  0:01:32s
epoch 46 | loss: 0.25425 | val_0_rmse: 0.48212 | val_1_rmse: 0.55007 |  0:01:34s
epoch 47 | loss: 0.25114 | val_0_rmse: 0.50795 | val_1_rmse: 0.56999 |  0:01:36s
epoch 48 | loss: 0.24012 | val_0_rmse: 0.4832  | val_1_rmse: 0.55503 |  0:01:38s
epoch 49 | loss: 0.24274 | val_0_rmse: 0.49386 | val_1_rmse: 0.56672 |  0:01:40s
epoch 50 | loss: 0.23902 | val_0_rmse: 0.47738 | val_1_rmse: 0.55219 |  0:01:42s
epoch 51 | loss: 0.2375  | val_0_rmse: 0.48095 | val_1_rmse: 0.55136 |  0:01:44s
epoch 52 | loss: 0.23958 | val_0_rmse: 0.47443 | val_1_rmse: 0.54783 |  0:01:46s
epoch 53 | loss: 0.23719 | val_0_rmse: 0.47914 | val_1_rmse: 0.55085 |  0:01:48s
epoch 54 | loss: 0.23822 | val_0_rmse: 0.49212 | val_1_rmse: 0.56056 |  0:01:50s
epoch 55 | loss: 0.2372  | val_0_rmse: 0.47752 | val_1_rmse: 0.54954 |  0:01:52s
epoch 56 | loss: 0.23671 | val_0_rmse: 0.47901 | val_1_rmse: 0.55478 |  0:01:54s
epoch 57 | loss: 0.23965 | val_0_rmse: 0.48869 | val_1_rmse: 0.57954 |  0:01:56s
epoch 58 | loss: 0.23998 | val_0_rmse: 0.479   | val_1_rmse: 0.56354 |  0:01:58s
epoch 59 | loss: 0.23499 | val_0_rmse: 0.491   | val_1_rmse: 0.5784  |  0:02:00s
epoch 60 | loss: 0.23369 | val_0_rmse: 0.47019 | val_1_rmse: 0.55742 |  0:02:02s
epoch 61 | loss: 0.23254 | val_0_rmse: 0.47064 | val_1_rmse: 0.56006 |  0:02:04s
epoch 62 | loss: 0.234   | val_0_rmse: 0.48456 | val_1_rmse: 0.56971 |  0:02:06s
epoch 63 | loss: 0.23051 | val_0_rmse: 0.48441 | val_1_rmse: 0.56904 |  0:02:08s
epoch 64 | loss: 0.22773 | val_0_rmse: 0.46483 | val_1_rmse: 0.55909 |  0:02:10s
epoch 65 | loss: 0.22645 | val_0_rmse: 0.46857 | val_1_rmse: 0.56579 |  0:02:12s
epoch 66 | loss: 0.22511 | val_0_rmse: 0.46556 | val_1_rmse: 0.55629 |  0:02:14s
epoch 67 | loss: 0.2203  | val_0_rmse: 0.46686 | val_1_rmse: 0.56532 |  0:02:16s
epoch 68 | loss: 0.22191 | val_0_rmse: 0.46431 | val_1_rmse: 0.55912 |  0:02:17s
epoch 69 | loss: 0.22589 | val_0_rmse: 0.47765 | val_1_rmse: 0.57821 |  0:02:19s
epoch 70 | loss: 0.21946 | val_0_rmse: 0.4669  | val_1_rmse: 0.57194 |  0:02:21s
epoch 71 | loss: 0.22033 | val_0_rmse: 0.46569 | val_1_rmse: 0.57155 |  0:02:23s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.53874
Best weights from best epoch are automatically used!
ended training at: 06:24:59
Feature importance:
Mean squared error is of 6304713538.382107
Mean absolute error:55771.07952142216
MAPE:0.17170456789115518
R2 score:0.7199017350109211
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:24:59
epoch 0  | loss: 1.20565 | val_0_rmse: 1.00104 | val_1_rmse: 1.00729 |  0:00:01s
epoch 1  | loss: 0.94797 | val_0_rmse: 0.97949 | val_1_rmse: 0.9865  |  0:00:04s
epoch 2  | loss: 0.79989 | val_0_rmse: 0.93927 | val_1_rmse: 0.94956 |  0:00:05s
epoch 3  | loss: 0.61123 | val_0_rmse: 0.8236  | val_1_rmse: 0.82664 |  0:00:08s
epoch 4  | loss: 0.48596 | val_0_rmse: 0.80211 | val_1_rmse: 0.80325 |  0:00:10s
epoch 5  | loss: 0.41026 | val_0_rmse: 0.81848 | val_1_rmse: 0.81923 |  0:00:12s
epoch 6  | loss: 0.38205 | val_0_rmse: 0.79523 | val_1_rmse: 0.79833 |  0:00:14s
epoch 7  | loss: 0.35842 | val_0_rmse: 0.7549  | val_1_rmse: 0.75577 |  0:00:15s
epoch 8  | loss: 0.35013 | val_0_rmse: 0.77458 | val_1_rmse: 0.77794 |  0:00:17s
epoch 9  | loss: 0.33184 | val_0_rmse: 0.7486  | val_1_rmse: 0.75329 |  0:00:20s
epoch 10 | loss: 0.32465 | val_0_rmse: 0.72123 | val_1_rmse: 0.7243  |  0:00:21s
epoch 11 | loss: 0.32342 | val_0_rmse: 0.70342 | val_1_rmse: 0.70805 |  0:00:24s
epoch 12 | loss: 0.30842 | val_0_rmse: 0.72182 | val_1_rmse: 0.73139 |  0:00:26s
epoch 13 | loss: 0.29915 | val_0_rmse: 0.66761 | val_1_rmse: 0.67541 |  0:00:28s
epoch 14 | loss: 0.3011  | val_0_rmse: 0.66839 | val_1_rmse: 0.67363 |  0:00:30s
epoch 15 | loss: 0.30566 | val_0_rmse: 0.63217 | val_1_rmse: 0.63988 |  0:00:32s
epoch 16 | loss: 0.28825 | val_0_rmse: 0.63748 | val_1_rmse: 0.64333 |  0:00:34s
epoch 17 | loss: 0.28969 | val_0_rmse: 0.58911 | val_1_rmse: 0.59846 |  0:00:36s
epoch 18 | loss: 0.27955 | val_0_rmse: 0.59358 | val_1_rmse: 0.60456 |  0:00:38s
epoch 19 | loss: 0.28037 | val_0_rmse: 0.60107 | val_1_rmse: 0.61164 |  0:00:40s
epoch 20 | loss: 0.27521 | val_0_rmse: 0.59026 | val_1_rmse: 0.60521 |  0:00:42s
epoch 21 | loss: 0.27545 | val_0_rmse: 0.55197 | val_1_rmse: 0.57067 |  0:00:44s
epoch 22 | loss: 0.27108 | val_0_rmse: 0.54172 | val_1_rmse: 0.56137 |  0:00:46s
epoch 23 | loss: 0.26868 | val_0_rmse: 0.51907 | val_1_rmse: 0.54599 |  0:00:48s
epoch 24 | loss: 0.27032 | val_0_rmse: 0.51699 | val_1_rmse: 0.54558 |  0:00:50s
epoch 25 | loss: 0.26965 | val_0_rmse: 0.51181 | val_1_rmse: 0.54501 |  0:00:52s
epoch 26 | loss: 0.27156 | val_0_rmse: 0.50773 | val_1_rmse: 0.54469 |  0:00:54s
epoch 27 | loss: 0.26725 | val_0_rmse: 0.5127  | val_1_rmse: 0.54822 |  0:00:56s
epoch 28 | loss: 0.26413 | val_0_rmse: 0.49423 | val_1_rmse: 0.53371 |  0:00:58s
epoch 29 | loss: 0.25941 | val_0_rmse: 0.48634 | val_1_rmse: 0.52572 |  0:01:00s
epoch 30 | loss: 0.26135 | val_0_rmse: 0.51196 | val_1_rmse: 0.54406 |  0:01:02s
epoch 31 | loss: 0.26179 | val_0_rmse: 0.48473 | val_1_rmse: 0.52811 |  0:01:04s
epoch 32 | loss: 0.25549 | val_0_rmse: 0.48484 | val_1_rmse: 0.52471 |  0:01:06s
epoch 33 | loss: 0.25538 | val_0_rmse: 0.48201 | val_1_rmse: 0.52644 |  0:01:08s
epoch 34 | loss: 0.25842 | val_0_rmse: 0.48671 | val_1_rmse: 0.53039 |  0:01:10s
epoch 35 | loss: 0.2611  | val_0_rmse: 0.48962 | val_1_rmse: 0.53228 |  0:01:12s
epoch 36 | loss: 0.25849 | val_0_rmse: 0.48056 | val_1_rmse: 0.52664 |  0:01:14s
epoch 37 | loss: 0.25093 | val_0_rmse: 0.48481 | val_1_rmse: 0.53054 |  0:01:16s
epoch 38 | loss: 0.24982 | val_0_rmse: 0.48052 | val_1_rmse: 0.53392 |  0:01:18s
epoch 39 | loss: 0.2481  | val_0_rmse: 0.48898 | val_1_rmse: 0.53046 |  0:01:20s
epoch 40 | loss: 0.2468  | val_0_rmse: 0.47841 | val_1_rmse: 0.53683 |  0:01:21s
epoch 41 | loss: 0.24705 | val_0_rmse: 0.47815 | val_1_rmse: 0.53586 |  0:01:24s
epoch 42 | loss: 0.24398 | val_0_rmse: 0.48106 | val_1_rmse: 0.54212 |  0:01:25s
epoch 43 | loss: 0.24177 | val_0_rmse: 0.47876 | val_1_rmse: 0.53015 |  0:01:28s
epoch 44 | loss: 0.24171 | val_0_rmse: 0.46446 | val_1_rmse: 0.52936 |  0:01:29s
epoch 45 | loss: 0.23965 | val_0_rmse: 0.46925 | val_1_rmse: 0.53179 |  0:01:32s
epoch 46 | loss: 0.24155 | val_0_rmse: 0.47038 | val_1_rmse: 0.53484 |  0:01:33s
epoch 47 | loss: 0.24096 | val_0_rmse: 0.47608 | val_1_rmse: 0.53537 |  0:01:36s
epoch 48 | loss: 0.2421  | val_0_rmse: 0.47338 | val_1_rmse: 0.54706 |  0:01:38s
epoch 49 | loss: 0.24558 | val_0_rmse: 0.46561 | val_1_rmse: 0.53039 |  0:01:40s
epoch 50 | loss: 0.24055 | val_0_rmse: 0.46772 | val_1_rmse: 0.53054 |  0:01:42s
epoch 51 | loss: 0.24346 | val_0_rmse: 0.46409 | val_1_rmse: 0.53207 |  0:01:44s
epoch 52 | loss: 0.23607 | val_0_rmse: 0.48029 | val_1_rmse: 0.54804 |  0:01:46s
epoch 53 | loss: 0.23862 | val_0_rmse: 0.46345 | val_1_rmse: 0.54263 |  0:01:48s
epoch 54 | loss: 0.23351 | val_0_rmse: 0.46504 | val_1_rmse: 0.53053 |  0:01:50s
epoch 55 | loss: 0.2348  | val_0_rmse: 0.45793 | val_1_rmse: 0.53056 |  0:01:52s
epoch 56 | loss: 0.23071 | val_0_rmse: 0.45836 | val_1_rmse: 0.53362 |  0:01:54s
epoch 57 | loss: 0.23417 | val_0_rmse: 0.45326 | val_1_rmse: 0.53249 |  0:01:56s
epoch 58 | loss: 0.23149 | val_0_rmse: 0.45166 | val_1_rmse: 0.53036 |  0:01:58s
epoch 59 | loss: 0.23214 | val_0_rmse: 0.46606 | val_1_rmse: 0.53877 |  0:02:00s
epoch 60 | loss: 0.23378 | val_0_rmse: 0.46141 | val_1_rmse: 0.53779 |  0:02:02s
epoch 61 | loss: 0.22756 | val_0_rmse: 0.45035 | val_1_rmse: 0.53234 |  0:02:04s
epoch 62 | loss: 0.2276  | val_0_rmse: 0.45437 | val_1_rmse: 0.53048 |  0:02:06s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.52471
Best weights from best epoch are automatically used!
ended training at: 06:27:06
Feature importance:
Mean squared error is of 6000697697.44652
Mean absolute error:55680.05963394461
MAPE:0.1760600593690945
R2 score:0.7294443867979656
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:27:07
epoch 0  | loss: 1.21911 | val_0_rmse: 1.00057 | val_1_rmse: 1.01173 |  0:00:02s
epoch 1  | loss: 0.88258 | val_0_rmse: 0.90226 | val_1_rmse: 0.92429 |  0:00:04s
epoch 2  | loss: 0.63861 | val_0_rmse: 0.91869 | val_1_rmse: 0.94647 |  0:00:06s
epoch 3  | loss: 0.53598 | val_0_rmse: 0.91703 | val_1_rmse: 0.94587 |  0:00:08s
epoch 4  | loss: 0.48594 | val_0_rmse: 0.89502 | val_1_rmse: 0.92008 |  0:00:10s
epoch 5  | loss: 0.46152 | val_0_rmse: 0.78887 | val_1_rmse: 0.80727 |  0:00:12s
epoch 6  | loss: 0.42551 | val_0_rmse: 0.79195 | val_1_rmse: 0.80781 |  0:00:14s
epoch 7  | loss: 0.40576 | val_0_rmse: 0.80357 | val_1_rmse: 0.82812 |  0:00:16s
epoch 8  | loss: 0.39612 | val_0_rmse: 0.72591 | val_1_rmse: 0.74696 |  0:00:18s
epoch 9  | loss: 0.38905 | val_0_rmse: 0.72172 | val_1_rmse: 0.73902 |  0:00:20s
epoch 10 | loss: 0.36041 | val_0_rmse: 0.70059 | val_1_rmse: 0.71405 |  0:00:22s
epoch 11 | loss: 0.35106 | val_0_rmse: 0.69239 | val_1_rmse: 0.70145 |  0:00:24s
epoch 12 | loss: 0.34134 | val_0_rmse: 0.67675 | val_1_rmse: 0.68524 |  0:00:26s
epoch 13 | loss: 0.33723 | val_0_rmse: 0.67952 | val_1_rmse: 0.69058 |  0:00:28s
epoch 14 | loss: 0.33532 | val_0_rmse: 0.65926 | val_1_rmse: 0.67043 |  0:00:30s
epoch 15 | loss: 0.33694 | val_0_rmse: 0.64642 | val_1_rmse: 0.65773 |  0:00:32s
epoch 16 | loss: 0.34749 | val_0_rmse: 0.64353 | val_1_rmse: 0.65749 |  0:00:34s
epoch 17 | loss: 0.34502 | val_0_rmse: 0.60105 | val_1_rmse: 0.60816 |  0:00:36s
epoch 18 | loss: 0.32885 | val_0_rmse: 0.60669 | val_1_rmse: 0.62317 |  0:00:38s
epoch 19 | loss: 0.33178 | val_0_rmse: 0.5935  | val_1_rmse: 0.60684 |  0:00:40s
epoch 20 | loss: 0.32452 | val_0_rmse: 0.57159 | val_1_rmse: 0.58324 |  0:00:42s
epoch 21 | loss: 0.31417 | val_0_rmse: 0.5678  | val_1_rmse: 0.5764  |  0:00:44s
epoch 22 | loss: 0.32845 | val_0_rmse: 0.57768 | val_1_rmse: 0.58808 |  0:00:46s
epoch 23 | loss: 0.33456 | val_0_rmse: 0.57292 | val_1_rmse: 0.58433 |  0:00:48s
epoch 24 | loss: 0.32819 | val_0_rmse: 0.55864 | val_1_rmse: 0.56479 |  0:00:50s
epoch 25 | loss: 0.32664 | val_0_rmse: 0.55162 | val_1_rmse: 0.56549 |  0:00:52s
epoch 26 | loss: 0.31633 | val_0_rmse: 0.55203 | val_1_rmse: 0.56322 |  0:00:54s
epoch 27 | loss: 0.31776 | val_0_rmse: 0.55287 | val_1_rmse: 0.55923 |  0:00:56s
epoch 28 | loss: 0.32205 | val_0_rmse: 0.54737 | val_1_rmse: 0.55599 |  0:00:58s
epoch 29 | loss: 0.31124 | val_0_rmse: 0.54565 | val_1_rmse: 0.55683 |  0:01:00s
epoch 30 | loss: 0.31171 | val_0_rmse: 0.53971 | val_1_rmse: 0.5557  |  0:01:02s
epoch 31 | loss: 0.31187 | val_0_rmse: 0.55098 | val_1_rmse: 0.56572 |  0:01:04s
epoch 32 | loss: 0.32897 | val_0_rmse: 0.56395 | val_1_rmse: 0.57648 |  0:01:06s
epoch 33 | loss: 0.32513 | val_0_rmse: 0.5411  | val_1_rmse: 0.5546  |  0:01:08s
epoch 34 | loss: 0.30892 | val_0_rmse: 0.53293 | val_1_rmse: 0.55535 |  0:01:10s
epoch 35 | loss: 0.30016 | val_0_rmse: 0.53652 | val_1_rmse: 0.5577  |  0:01:12s
epoch 36 | loss: 0.30337 | val_0_rmse: 0.53144 | val_1_rmse: 0.54909 |  0:01:14s
epoch 37 | loss: 0.28976 | val_0_rmse: 0.53973 | val_1_rmse: 0.56057 |  0:01:16s
epoch 38 | loss: 0.29406 | val_0_rmse: 0.52505 | val_1_rmse: 0.54523 |  0:01:18s
epoch 39 | loss: 0.29026 | val_0_rmse: 0.52938 | val_1_rmse: 0.55469 |  0:01:20s
epoch 40 | loss: 0.28965 | val_0_rmse: 0.52198 | val_1_rmse: 0.54008 |  0:01:22s
epoch 41 | loss: 0.28765 | val_0_rmse: 0.52678 | val_1_rmse: 0.55091 |  0:01:24s
epoch 42 | loss: 0.28425 | val_0_rmse: 0.52633 | val_1_rmse: 0.55401 |  0:01:26s
epoch 43 | loss: 0.28444 | val_0_rmse: 0.51971 | val_1_rmse: 0.54604 |  0:01:28s
epoch 44 | loss: 0.28268 | val_0_rmse: 0.5205  | val_1_rmse: 0.54746 |  0:01:30s
epoch 45 | loss: 0.28148 | val_0_rmse: 0.51996 | val_1_rmse: 0.54635 |  0:01:32s
epoch 46 | loss: 0.28136 | val_0_rmse: 0.51959 | val_1_rmse: 0.54479 |  0:01:34s
epoch 47 | loss: 0.29969 | val_0_rmse: 0.55529 | val_1_rmse: 0.58923 |  0:01:36s
epoch 48 | loss: 0.2996  | val_0_rmse: 0.52988 | val_1_rmse: 0.55498 |  0:01:38s
epoch 49 | loss: 0.2995  | val_0_rmse: 0.53774 | val_1_rmse: 0.56284 |  0:01:40s
epoch 50 | loss: 0.29094 | val_0_rmse: 0.54171 | val_1_rmse: 0.56609 |  0:01:42s
epoch 51 | loss: 0.30461 | val_0_rmse: 0.54749 | val_1_rmse: 0.5684  |  0:01:44s
epoch 52 | loss: 0.29687 | val_0_rmse: 0.52896 | val_1_rmse: 0.55518 |  0:01:46s
epoch 53 | loss: 0.29554 | val_0_rmse: 0.53133 | val_1_rmse: 0.55228 |  0:01:48s
epoch 54 | loss: 0.30131 | val_0_rmse: 0.53191 | val_1_rmse: 0.55574 |  0:01:50s
epoch 55 | loss: 0.29711 | val_0_rmse: 0.52722 | val_1_rmse: 0.55635 |  0:01:52s
epoch 56 | loss: 0.2946  | val_0_rmse: 0.52254 | val_1_rmse: 0.55249 |  0:01:54s
epoch 57 | loss: 0.28372 | val_0_rmse: 0.52262 | val_1_rmse: 0.55162 |  0:01:56s
epoch 58 | loss: 0.28093 | val_0_rmse: 0.52117 | val_1_rmse: 0.5475  |  0:01:58s
epoch 59 | loss: 0.28491 | val_0_rmse: 0.52507 | val_1_rmse: 0.54765 |  0:02:00s
epoch 60 | loss: 0.29016 | val_0_rmse: 0.52407 | val_1_rmse: 0.54984 |  0:02:02s
epoch 61 | loss: 0.28364 | val_0_rmse: 0.51971 | val_1_rmse: 0.54516 |  0:02:04s
epoch 62 | loss: 0.29222 | val_0_rmse: 0.5677  | val_1_rmse: 0.59438 |  0:02:06s
epoch 63 | loss: 0.31318 | val_0_rmse: 0.5398  | val_1_rmse: 0.55636 |  0:02:08s
epoch 64 | loss: 0.30115 | val_0_rmse: 0.53941 | val_1_rmse: 0.56155 |  0:02:10s
epoch 65 | loss: 0.28984 | val_0_rmse: 0.52459 | val_1_rmse: 0.5476  |  0:02:12s
epoch 66 | loss: 0.28744 | val_0_rmse: 0.52503 | val_1_rmse: 0.54344 |  0:02:14s
epoch 67 | loss: 0.28812 | val_0_rmse: 0.53348 | val_1_rmse: 0.56132 |  0:02:15s
epoch 68 | loss: 0.28983 | val_0_rmse: 0.52164 | val_1_rmse: 0.54524 |  0:02:17s
epoch 69 | loss: 0.28422 | val_0_rmse: 0.51652 | val_1_rmse: 0.54671 |  0:02:19s
epoch 70 | loss: 0.28734 | val_0_rmse: 0.52742 | val_1_rmse: 0.55454 |  0:02:21s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.54008
Best weights from best epoch are automatically used!
ended training at: 06:29:29
Feature importance:
Mean squared error is of 6618478940.562707
Mean absolute error:57312.19070694128
MAPE:0.17651155668843382
R2 score:0.6894523131866721
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:29:30
epoch 0  | loss: 1.25329 | val_0_rmse: 1.00106 | val_1_rmse: 0.98867 |  0:00:01s
epoch 1  | loss: 0.78854 | val_0_rmse: 0.87873 | val_1_rmse: 0.85991 |  0:00:04s
epoch 2  | loss: 0.5941  | val_0_rmse: 0.82333 | val_1_rmse: 0.8062  |  0:00:05s
epoch 3  | loss: 0.47375 | val_0_rmse: 0.84918 | val_1_rmse: 0.83742 |  0:00:08s
epoch 4  | loss: 0.41979 | val_0_rmse: 0.81801 | val_1_rmse: 0.80769 |  0:00:09s
epoch 5  | loss: 0.3775  | val_0_rmse: 0.81381 | val_1_rmse: 0.80106 |  0:00:11s
epoch 6  | loss: 0.35957 | val_0_rmse: 0.79361 | val_1_rmse: 0.78331 |  0:00:13s
epoch 7  | loss: 0.34089 | val_0_rmse: 0.78976 | val_1_rmse: 0.7789  |  0:00:15s
epoch 8  | loss: 0.3399  | val_0_rmse: 0.75052 | val_1_rmse: 0.74285 |  0:00:17s
epoch 9  | loss: 0.33972 | val_0_rmse: 0.72837 | val_1_rmse: 0.72262 |  0:00:19s
epoch 10 | loss: 0.33221 | val_0_rmse: 0.7084  | val_1_rmse: 0.70374 |  0:00:21s
epoch 11 | loss: 0.31791 | val_0_rmse: 0.70367 | val_1_rmse: 0.69915 |  0:00:23s
epoch 12 | loss: 0.3102  | val_0_rmse: 0.68837 | val_1_rmse: 0.68306 |  0:00:25s
epoch 13 | loss: 0.30941 | val_0_rmse: 0.6678  | val_1_rmse: 0.67066 |  0:00:27s
epoch 14 | loss: 0.30449 | val_0_rmse: 0.66576 | val_1_rmse: 0.66427 |  0:00:29s
epoch 15 | loss: 0.29669 | val_0_rmse: 0.61567 | val_1_rmse: 0.62014 |  0:00:31s
epoch 16 | loss: 0.29363 | val_0_rmse: 0.61285 | val_1_rmse: 0.61879 |  0:00:34s
epoch 17 | loss: 0.28756 | val_0_rmse: 0.59023 | val_1_rmse: 0.59974 |  0:00:35s
epoch 18 | loss: 0.30703 | val_0_rmse: 0.59726 | val_1_rmse: 0.61187 |  0:00:37s
epoch 19 | loss: 0.31045 | val_0_rmse: 0.57842 | val_1_rmse: 0.59264 |  0:00:39s
epoch 20 | loss: 0.2958  | val_0_rmse: 0.56563 | val_1_rmse: 0.58298 |  0:00:41s
epoch 21 | loss: 0.28624 | val_0_rmse: 0.5488  | val_1_rmse: 0.56762 |  0:00:44s
epoch 22 | loss: 0.28775 | val_0_rmse: 0.55931 | val_1_rmse: 0.57676 |  0:00:45s
epoch 23 | loss: 0.28302 | val_0_rmse: 0.55019 | val_1_rmse: 0.57255 |  0:00:47s
epoch 24 | loss: 0.27815 | val_0_rmse: 0.51845 | val_1_rmse: 0.54599 |  0:00:50s
epoch 25 | loss: 0.27062 | val_0_rmse: 0.52145 | val_1_rmse: 0.54712 |  0:00:51s
epoch 26 | loss: 0.27268 | val_0_rmse: 0.5083  | val_1_rmse: 0.54048 |  0:00:54s
epoch 27 | loss: 0.26598 | val_0_rmse: 0.52557 | val_1_rmse: 0.55433 |  0:00:56s
epoch 28 | loss: 0.26924 | val_0_rmse: 0.50836 | val_1_rmse: 0.5403  |  0:00:58s
epoch 29 | loss: 0.27264 | val_0_rmse: 0.51322 | val_1_rmse: 0.54999 |  0:01:00s
epoch 30 | loss: 0.27274 | val_0_rmse: 0.50182 | val_1_rmse: 0.54608 |  0:01:02s
epoch 31 | loss: 0.26821 | val_0_rmse: 0.49809 | val_1_rmse: 0.53552 |  0:01:04s
epoch 32 | loss: 0.26802 | val_0_rmse: 0.49462 | val_1_rmse: 0.53668 |  0:01:06s
epoch 33 | loss: 0.26305 | val_0_rmse: 0.49507 | val_1_rmse: 0.53479 |  0:01:08s
epoch 34 | loss: 0.26937 | val_0_rmse: 0.49453 | val_1_rmse: 0.53254 |  0:01:10s
epoch 35 | loss: 0.25893 | val_0_rmse: 0.48955 | val_1_rmse: 0.53349 |  0:01:12s
epoch 36 | loss: 0.25268 | val_0_rmse: 0.48561 | val_1_rmse: 0.52816 |  0:01:14s
epoch 37 | loss: 0.25268 | val_0_rmse: 0.49225 | val_1_rmse: 0.53335 |  0:01:16s
epoch 38 | loss: 0.25223 | val_0_rmse: 0.48055 | val_1_rmse: 0.53355 |  0:01:18s
epoch 39 | loss: 0.25015 | val_0_rmse: 0.47744 | val_1_rmse: 0.52985 |  0:01:20s
epoch 40 | loss: 0.25313 | val_0_rmse: 0.48253 | val_1_rmse: 0.53206 |  0:01:22s
epoch 41 | loss: 0.25321 | val_0_rmse: 0.49205 | val_1_rmse: 0.54326 |  0:01:24s
epoch 42 | loss: 0.25505 | val_0_rmse: 0.49193 | val_1_rmse: 0.53576 |  0:01:26s
epoch 43 | loss: 0.25216 | val_0_rmse: 0.49672 | val_1_rmse: 0.53723 |  0:01:28s
epoch 44 | loss: 0.26048 | val_0_rmse: 0.50046 | val_1_rmse: 0.53928 |  0:01:30s
epoch 45 | loss: 0.25914 | val_0_rmse: 0.4837  | val_1_rmse: 0.53692 |  0:01:32s
epoch 46 | loss: 0.25363 | val_0_rmse: 0.48039 | val_1_rmse: 0.53241 |  0:01:34s
epoch 47 | loss: 0.25077 | val_0_rmse: 0.48089 | val_1_rmse: 0.53231 |  0:01:36s
epoch 48 | loss: 0.26499 | val_0_rmse: 0.48703 | val_1_rmse: 0.53226 |  0:01:37s
epoch 49 | loss: 0.25675 | val_0_rmse: 0.62358 | val_1_rmse: 0.63349 |  0:01:39s
epoch 50 | loss: 0.24854 | val_0_rmse: 0.48387 | val_1_rmse: 0.53443 |  0:01:41s
epoch 51 | loss: 0.2562  | val_0_rmse: 0.47936 | val_1_rmse: 0.53475 |  0:01:43s
epoch 52 | loss: 0.24829 | val_0_rmse: 0.49566 | val_1_rmse: 0.54209 |  0:01:45s
epoch 53 | loss: 0.25294 | val_0_rmse: 0.49201 | val_1_rmse: 0.54271 |  0:01:47s
epoch 54 | loss: 0.25905 | val_0_rmse: 0.4862  | val_1_rmse: 0.54451 |  0:01:50s
epoch 55 | loss: 0.25088 | val_0_rmse: 0.48642 | val_1_rmse: 0.54383 |  0:01:51s
epoch 56 | loss: 0.25085 | val_0_rmse: 0.47749 | val_1_rmse: 0.53236 |  0:01:53s
epoch 57 | loss: 0.26617 | val_0_rmse: 0.55763 | val_1_rmse: 0.59764 |  0:01:55s
epoch 58 | loss: 0.29723 | val_0_rmse: 0.50817 | val_1_rmse: 0.53666 |  0:01:58s
epoch 59 | loss: 0.26562 | val_0_rmse: 0.49151 | val_1_rmse: 0.53928 |  0:01:59s
epoch 60 | loss: 0.24956 | val_0_rmse: 0.4802  | val_1_rmse: 0.53365 |  0:02:01s
epoch 61 | loss: 0.24544 | val_0_rmse: 0.48238 | val_1_rmse: 0.53555 |  0:02:03s
epoch 62 | loss: 0.24029 | val_0_rmse: 0.47266 | val_1_rmse: 0.52848 |  0:02:05s
epoch 63 | loss: 0.24094 | val_0_rmse: 0.47417 | val_1_rmse: 0.53022 |  0:02:07s
epoch 64 | loss: 0.23703 | val_0_rmse: 0.47076 | val_1_rmse: 0.5269  |  0:02:09s
epoch 65 | loss: 0.23278 | val_0_rmse: 0.46775 | val_1_rmse: 0.52657 |  0:02:11s
epoch 66 | loss: 0.23265 | val_0_rmse: 0.46603 | val_1_rmse: 0.52433 |  0:02:13s
epoch 67 | loss: 0.24182 | val_0_rmse: 0.47677 | val_1_rmse: 0.53626 |  0:02:15s
epoch 68 | loss: 0.23586 | val_0_rmse: 0.46353 | val_1_rmse: 0.5332  |  0:02:17s
epoch 69 | loss: 0.23113 | val_0_rmse: 0.47636 | val_1_rmse: 0.53547 |  0:02:19s
epoch 70 | loss: 0.23994 | val_0_rmse: 0.46663 | val_1_rmse: 0.53652 |  0:02:21s
epoch 71 | loss: 0.23636 | val_0_rmse: 0.46749 | val_1_rmse: 0.53794 |  0:02:23s
epoch 72 | loss: 0.23594 | val_0_rmse: 0.46595 | val_1_rmse: 0.53514 |  0:02:25s
epoch 73 | loss: 0.24049 | val_0_rmse: 0.47673 | val_1_rmse: 0.53783 |  0:02:27s
epoch 74 | loss: 0.23964 | val_0_rmse: 0.46506 | val_1_rmse: 0.53556 |  0:02:29s
epoch 75 | loss: 0.22995 | val_0_rmse: 0.46131 | val_1_rmse: 0.54128 |  0:02:31s
epoch 76 | loss: 0.22764 | val_0_rmse: 0.45947 | val_1_rmse: 0.53737 |  0:02:33s
epoch 77 | loss: 0.22866 | val_0_rmse: 0.45682 | val_1_rmse: 0.53302 |  0:02:35s
epoch 78 | loss: 0.22791 | val_0_rmse: 0.46373 | val_1_rmse: 0.53368 |  0:02:37s
epoch 79 | loss: 0.22785 | val_0_rmse: 0.45276 | val_1_rmse: 0.53133 |  0:02:39s
epoch 80 | loss: 0.23326 | val_0_rmse: 0.46908 | val_1_rmse: 0.53724 |  0:02:41s
epoch 81 | loss: 0.23278 | val_0_rmse: 0.45683 | val_1_rmse: 0.53552 |  0:02:43s
epoch 82 | loss: 0.22216 | val_0_rmse: 0.45141 | val_1_rmse: 0.53129 |  0:02:45s
epoch 83 | loss: 0.22404 | val_0_rmse: 0.4532  | val_1_rmse: 0.5353  |  0:02:47s
epoch 84 | loss: 0.21939 | val_0_rmse: 0.44851 | val_1_rmse: 0.53398 |  0:02:49s
epoch 85 | loss: 0.21593 | val_0_rmse: 0.44715 | val_1_rmse: 0.53103 |  0:02:51s
epoch 86 | loss: 0.22072 | val_0_rmse: 0.45521 | val_1_rmse: 0.53788 |  0:02:53s
epoch 87 | loss: 0.21954 | val_0_rmse: 0.45518 | val_1_rmse: 0.54232 |  0:02:55s
epoch 88 | loss: 0.22009 | val_0_rmse: 0.45053 | val_1_rmse: 0.53233 |  0:02:57s
epoch 89 | loss: 0.22061 | val_0_rmse: 0.44363 | val_1_rmse: 0.53523 |  0:02:59s
epoch 90 | loss: 0.21536 | val_0_rmse: 0.44056 | val_1_rmse: 0.53289 |  0:03:01s
epoch 91 | loss: 0.21426 | val_0_rmse: 0.44414 | val_1_rmse: 0.53652 |  0:03:03s
epoch 92 | loss: 0.21408 | val_0_rmse: 0.45575 | val_1_rmse: 0.54314 |  0:03:05s
epoch 93 | loss: 0.21978 | val_0_rmse: 0.44478 | val_1_rmse: 0.53955 |  0:03:07s
epoch 94 | loss: 0.21575 | val_0_rmse: 0.44108 | val_1_rmse: 0.53796 |  0:03:09s
epoch 95 | loss: 0.21178 | val_0_rmse: 0.44001 | val_1_rmse: 0.54518 |  0:03:11s
epoch 96 | loss: 0.21706 | val_0_rmse: 0.44643 | val_1_rmse: 0.54842 |  0:03:13s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.52433
Best weights from best epoch are automatically used!
ended training at: 06:32:44
Feature importance:
Mean squared error is of 6046092403.414368
Mean absolute error:54686.38007790802
MAPE:0.17219134380077933
R2 score:0.7248454349310076
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:32:47
epoch 0  | loss: 0.95483 | val_0_rmse: 1.03426 | val_1_rmse: 1.02493 |  0:00:10s
epoch 1  | loss: 0.48412 | val_0_rmse: 0.75274 | val_1_rmse: 0.74491 |  0:00:21s
epoch 2  | loss: 0.40897 | val_0_rmse: 0.72554 | val_1_rmse: 0.7184  |  0:00:31s
epoch 3  | loss: 0.38617 | val_0_rmse: 0.69927 | val_1_rmse: 0.69322 |  0:00:42s
epoch 4  | loss: 0.37902 | val_0_rmse: 0.67011 | val_1_rmse: 0.66566 |  0:00:52s
epoch 5  | loss: 0.36863 | val_0_rmse: 0.64278 | val_1_rmse: 0.64102 |  0:01:03s
epoch 6  | loss: 0.3624  | val_0_rmse: 0.62238 | val_1_rmse: 0.62681 |  0:01:13s
epoch 7  | loss: 0.35757 | val_0_rmse: 0.60007 | val_1_rmse: 0.60722 |  0:01:24s
epoch 8  | loss: 0.35572 | val_0_rmse: 0.58547 | val_1_rmse: 0.59791 |  0:01:34s
epoch 9  | loss: 0.35283 | val_0_rmse: 0.57577 | val_1_rmse: 0.59138 |  0:01:45s
epoch 10 | loss: 0.34902 | val_0_rmse: 0.5708  | val_1_rmse: 0.59581 |  0:01:55s
epoch 11 | loss: 0.34432 | val_0_rmse: 0.57567 | val_1_rmse: 0.60042 |  0:02:06s
epoch 12 | loss: 0.33951 | val_0_rmse: 0.58162 | val_1_rmse: 0.61648 |  0:02:16s
epoch 13 | loss: 0.33965 | val_0_rmse: 0.57581 | val_1_rmse: 0.59848 |  0:02:27s
epoch 14 | loss: 0.33625 | val_0_rmse: 0.57206 | val_1_rmse: 0.60116 |  0:02:37s
epoch 15 | loss: 0.33373 | val_0_rmse: 0.57358 | val_1_rmse: 0.60671 |  0:02:48s
epoch 16 | loss: 0.33201 | val_0_rmse: 0.579   | val_1_rmse: 0.61041 |  0:02:58s
epoch 17 | loss: 0.33047 | val_0_rmse: 0.57878 | val_1_rmse: 0.60539 |  0:03:09s
epoch 18 | loss: 0.33001 | val_0_rmse: 0.57189 | val_1_rmse: 0.60918 |  0:03:20s
epoch 19 | loss: 0.32689 | val_0_rmse: 0.5701  | val_1_rmse: 0.60341 |  0:03:30s
epoch 20 | loss: 0.32587 | val_0_rmse: 0.57494 | val_1_rmse: 0.61726 |  0:03:41s
epoch 21 | loss: 0.32452 | val_0_rmse: 0.57406 | val_1_rmse: 0.61343 |  0:03:51s
epoch 22 | loss: 0.32195 | val_0_rmse: 0.58725 | val_1_rmse: 0.63215 |  0:04:02s
epoch 23 | loss: 0.31829 | val_0_rmse: 0.5666  | val_1_rmse: 0.61221 |  0:04:12s
epoch 24 | loss: 0.31767 | val_0_rmse: 0.59124 | val_1_rmse: 0.62163 |  0:04:23s
epoch 25 | loss: 0.31575 | val_0_rmse: 0.59986 | val_1_rmse: 0.6384  |  0:04:33s
epoch 26 | loss: 0.31603 | val_0_rmse: 0.57836 | val_1_rmse: 0.62281 |  0:04:44s
epoch 27 | loss: 0.31433 | val_0_rmse: 0.56499 | val_1_rmse: 0.61505 |  0:04:54s
epoch 28 | loss: 0.31141 | val_0_rmse: 0.57061 | val_1_rmse: 0.62516 |  0:05:05s
epoch 29 | loss: 0.31172 | val_0_rmse: 0.55988 | val_1_rmse: 0.61456 |  0:05:15s
epoch 30 | loss: 0.31118 | val_0_rmse: 0.55866 | val_1_rmse: 0.62136 |  0:05:26s
epoch 31 | loss: 0.31022 | val_0_rmse: 0.55652 | val_1_rmse: 0.60478 |  0:05:36s
epoch 32 | loss: 0.3117  | val_0_rmse: 0.56594 | val_1_rmse: 0.60758 |  0:05:47s
epoch 33 | loss: 0.3084  | val_0_rmse: 0.55404 | val_1_rmse: 0.60375 |  0:05:57s
epoch 34 | loss: 0.30594 | val_0_rmse: 0.56937 | val_1_rmse: 0.60681 |  0:06:08s
epoch 35 | loss: 0.30641 | val_0_rmse: 0.58047 | val_1_rmse: 0.60246 |  0:06:18s
epoch 36 | loss: 0.30408 | val_0_rmse: 0.61574 | val_1_rmse: 0.60932 |  0:06:29s
epoch 37 | loss: 0.30295 | val_0_rmse: 0.60972 | val_1_rmse: 0.60692 |  0:06:39s
epoch 38 | loss: 0.29994 | val_0_rmse: 0.58524 | val_1_rmse: 0.61487 |  0:06:50s
epoch 39 | loss: 0.30058 | val_0_rmse: 0.59718 | val_1_rmse: 0.62116 |  0:07:01s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.59138
Best weights from best epoch are automatically used!
ended training at: 06:39:53
Feature importance:
Mean squared error is of 2294594835.048977
Mean absolute error:34971.71105726314
MAPE:0.2682200326380522
R2 score:0.6469785838944339
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:39:56
epoch 0  | loss: 0.83106 | val_0_rmse: 0.79405 | val_1_rmse: 0.78906 |  0:00:10s
epoch 1  | loss: 0.46846 | val_0_rmse: 0.7349  | val_1_rmse: 0.7263  |  0:00:21s
epoch 2  | loss: 0.40971 | val_0_rmse: 0.70645 | val_1_rmse: 0.69846 |  0:00:31s
epoch 3  | loss: 0.39897 | val_0_rmse: 0.70509 | val_1_rmse: 0.69748 |  0:00:42s
epoch 4  | loss: 0.38384 | val_0_rmse: 0.6723  | val_1_rmse: 0.66857 |  0:00:52s
epoch 5  | loss: 0.37422 | val_0_rmse: 0.67277 | val_1_rmse: 0.665   |  0:01:03s
epoch 6  | loss: 0.37147 | val_0_rmse: 0.62485 | val_1_rmse: 0.63079 |  0:01:13s
epoch 7  | loss: 0.3652  | val_0_rmse: 0.59753 | val_1_rmse: 0.60492 |  0:01:24s
epoch 8  | loss: 0.36181 | val_0_rmse: 0.60312 | val_1_rmse: 0.60938 |  0:01:34s
epoch 9  | loss: 0.35937 | val_0_rmse: 0.58511 | val_1_rmse: 0.59749 |  0:01:45s
epoch 10 | loss: 0.35703 | val_0_rmse: 0.5874  | val_1_rmse: 0.60778 |  0:01:56s
epoch 11 | loss: 0.3542  | val_0_rmse: 0.57784 | val_1_rmse: 0.59812 |  0:02:07s
epoch 12 | loss: 0.34958 | val_0_rmse: 0.59643 | val_1_rmse: 0.62295 |  0:02:17s
epoch 13 | loss: 0.34735 | val_0_rmse: 0.58179 | val_1_rmse: 0.60845 |  0:02:28s
epoch 14 | loss: 0.34545 | val_0_rmse: 0.60058 | val_1_rmse: 0.62122 |  0:02:38s
epoch 15 | loss: 0.3475  | val_0_rmse: 0.58529 | val_1_rmse: 0.61304 |  0:02:49s
epoch 16 | loss: 0.34463 | val_0_rmse: 0.58642 | val_1_rmse: 0.61631 |  0:02:59s
epoch 17 | loss: 0.34122 | val_0_rmse: 0.57907 | val_1_rmse: 0.60822 |  0:03:09s
epoch 18 | loss: 0.34473 | val_0_rmse: 0.59467 | val_1_rmse: 0.62374 |  0:03:20s
epoch 19 | loss: 0.33849 | val_0_rmse: 0.57987 | val_1_rmse: 0.6089  |  0:03:30s
epoch 20 | loss: 0.34109 | val_0_rmse: 0.58628 | val_1_rmse: 0.61748 |  0:03:41s
epoch 21 | loss: 0.3409  | val_0_rmse: 0.58673 | val_1_rmse: 0.61718 |  0:03:51s
epoch 22 | loss: 0.33741 | val_0_rmse: 0.58921 | val_1_rmse: 0.63247 |  0:04:02s
epoch 23 | loss: 0.3357  | val_0_rmse: 0.60054 | val_1_rmse: 0.64231 |  0:04:12s
epoch 24 | loss: 0.33717 | val_0_rmse: 0.58786 | val_1_rmse: 0.61343 |  0:04:23s
epoch 25 | loss: 0.33592 | val_0_rmse: 0.58226 | val_1_rmse: 0.61899 |  0:04:33s
epoch 26 | loss: 0.33536 | val_0_rmse: 0.58744 | val_1_rmse: 0.62537 |  0:04:44s
epoch 27 | loss: 0.33253 | val_0_rmse: 0.58243 | val_1_rmse: 0.62244 |  0:04:54s
epoch 28 | loss: 0.32858 | val_0_rmse: 0.57454 | val_1_rmse: 0.60792 |  0:05:05s
epoch 29 | loss: 0.3263  | val_0_rmse: 0.57162 | val_1_rmse: 0.61003 |  0:05:15s
epoch 30 | loss: 0.32991 | val_0_rmse: 0.61174 | val_1_rmse: 0.63848 |  0:05:26s
epoch 31 | loss: 0.32757 | val_0_rmse: 0.60082 | val_1_rmse: 0.61664 |  0:05:36s
epoch 32 | loss: 0.3218  | val_0_rmse: 0.5614  | val_1_rmse: 0.60122 |  0:05:47s
epoch 33 | loss: 0.31889 | val_0_rmse: 0.56577 | val_1_rmse: 0.60895 |  0:05:57s
epoch 34 | loss: 0.31713 | val_0_rmse: 0.56802 | val_1_rmse: 0.61297 |  0:06:08s
epoch 35 | loss: 0.31449 | val_0_rmse: 0.56478 | val_1_rmse: 0.61289 |  0:06:18s
epoch 36 | loss: 0.31301 | val_0_rmse: 0.56044 | val_1_rmse: 0.60372 |  0:06:29s
epoch 37 | loss: 0.31121 | val_0_rmse: 0.5834  | val_1_rmse: 0.62102 |  0:06:39s
epoch 38 | loss: 0.31217 | val_0_rmse: 0.55705 | val_1_rmse: 0.60358 |  0:06:50s
epoch 39 | loss: 0.31115 | val_0_rmse: 0.55718 | val_1_rmse: 0.60679 |  0:07:00s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.59749
Best weights from best epoch are automatically used!
ended training at: 06:47:01
Feature importance:
Mean squared error is of 2403341458.870042
Mean absolute error:35577.94892659116
MAPE:0.2719033523924412
R2 score:0.6318163428477829
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:47:03
epoch 0  | loss: 0.89862 | val_0_rmse: 0.83442 | val_1_rmse: 0.83455 |  0:00:10s
epoch 1  | loss: 0.46492 | val_0_rmse: 0.75582 | val_1_rmse: 0.75399 |  0:00:21s
epoch 2  | loss: 0.4086  | val_0_rmse: 0.73001 | val_1_rmse: 0.72955 |  0:00:31s
epoch 3  | loss: 0.39388 | val_0_rmse: 0.69878 | val_1_rmse: 0.70025 |  0:00:42s
epoch 4  | loss: 0.38141 | val_0_rmse: 0.67737 | val_1_rmse: 0.68232 |  0:00:52s
epoch 5  | loss: 0.37566 | val_0_rmse: 0.64792 | val_1_rmse: 0.65407 |  0:01:03s
epoch 6  | loss: 0.36954 | val_0_rmse: 0.62189 | val_1_rmse: 0.62963 |  0:01:13s
epoch 7  | loss: 0.36212 | val_0_rmse: 0.59516 | val_1_rmse: 0.6048  |  0:01:24s
epoch 8  | loss: 0.35749 | val_0_rmse: 0.58385 | val_1_rmse: 0.59952 |  0:01:34s
epoch 9  | loss: 0.35487 | val_0_rmse: 0.58665 | val_1_rmse: 0.60564 |  0:01:45s
epoch 10 | loss: 0.35069 | val_0_rmse: 0.57971 | val_1_rmse: 0.59915 |  0:01:55s
epoch 11 | loss: 0.34876 | val_0_rmse: 0.57678 | val_1_rmse: 0.60052 |  0:02:06s
epoch 12 | loss: 0.34759 | val_0_rmse: 0.57606 | val_1_rmse: 0.59941 |  0:02:16s
epoch 13 | loss: 0.34472 | val_0_rmse: 0.57746 | val_1_rmse: 0.59546 |  0:02:27s
epoch 14 | loss: 0.34201 | val_0_rmse: 0.57322 | val_1_rmse: 0.59729 |  0:02:38s
epoch 15 | loss: 0.33889 | val_0_rmse: 0.58637 | val_1_rmse: 0.61559 |  0:02:48s
epoch 16 | loss: 0.33784 | val_0_rmse: 0.59366 | val_1_rmse: 0.61263 |  0:02:59s
epoch 17 | loss: 0.33763 | val_0_rmse: 0.61662 | val_1_rmse: 0.60023 |  0:03:09s
epoch 18 | loss: 0.33532 | val_0_rmse: 0.57525 | val_1_rmse: 0.60448 |  0:03:20s
epoch 19 | loss: 0.33293 | val_0_rmse: 0.56815 | val_1_rmse: 0.59992 |  0:03:30s
epoch 20 | loss: 0.32964 | val_0_rmse: 0.60347 | val_1_rmse: 0.64059 |  0:03:41s
epoch 21 | loss: 0.32775 | val_0_rmse: 0.61111 | val_1_rmse: 0.65173 |  0:03:51s
epoch 22 | loss: 0.32727 | val_0_rmse: 0.60354 | val_1_rmse: 0.64466 |  0:04:02s
epoch 23 | loss: 0.32752 | val_0_rmse: 0.59299 | val_1_rmse: 0.60175 |  0:04:13s
epoch 24 | loss: 0.32149 | val_0_rmse: 0.56208 | val_1_rmse: 0.60481 |  0:04:23s
epoch 25 | loss: 0.32348 | val_0_rmse: 0.57002 | val_1_rmse: 0.6021  |  0:04:34s
epoch 26 | loss: 0.32305 | val_0_rmse: 0.57323 | val_1_rmse: 0.61764 |  0:04:44s
epoch 27 | loss: 0.31916 | val_0_rmse: 0.55661 | val_1_rmse: 0.60155 |  0:04:55s
epoch 28 | loss: 0.32079 | val_0_rmse: 0.58654 | val_1_rmse: 0.63332 |  0:05:05s
epoch 29 | loss: 0.31542 | val_0_rmse: 0.55974 | val_1_rmse: 0.60617 |  0:05:16s
epoch 30 | loss: 0.31486 | val_0_rmse: 0.5879  | val_1_rmse: 0.62872 |  0:05:26s
epoch 31 | loss: 0.31617 | val_0_rmse: 0.56908 | val_1_rmse: 0.61754 |  0:05:37s
epoch 32 | loss: 0.31329 | val_0_rmse: 0.58849 | val_1_rmse: 0.6278  |  0:05:47s
epoch 33 | loss: 0.31192 | val_0_rmse: 0.56209 | val_1_rmse: 0.60878 |  0:05:58s
epoch 34 | loss: 0.31047 | val_0_rmse: 0.56575 | val_1_rmse: 0.60814 |  0:06:09s
epoch 35 | loss: 0.31394 | val_0_rmse: 0.55184 | val_1_rmse: 0.60111 |  0:06:19s
epoch 36 | loss: 0.30955 | val_0_rmse: 0.55104 | val_1_rmse: 0.60638 |  0:06:30s
epoch 37 | loss: 0.30746 | val_0_rmse: 0.54694 | val_1_rmse: 0.60194 |  0:06:40s
epoch 38 | loss: 0.30528 | val_0_rmse: 0.56594 | val_1_rmse: 0.62926 |  0:06:51s
epoch 39 | loss: 0.30396 | val_0_rmse: 0.54866 | val_1_rmse: 0.60318 |  0:07:01s
epoch 40 | loss: 0.30206 | val_0_rmse: 0.56022 | val_1_rmse: 0.62716 |  0:07:12s
epoch 41 | loss: 0.30255 | val_0_rmse: 0.93366 | val_1_rmse: 0.61975 |  0:07:23s
epoch 42 | loss: 0.30086 | val_0_rmse: 1.92501 | val_1_rmse: 0.60953 |  0:07:33s
epoch 43 | loss: 0.29943 | val_0_rmse: 1.11999 | val_1_rmse: 0.64758 |  0:07:44s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.59546
Best weights from best epoch are automatically used!
ended training at: 06:54:52
Feature importance:
Mean squared error is of 2313471318.0814714
Mean absolute error:35093.80898342796
MAPE:0.27382609242146055
R2 score:0.6372555069673742
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:54:54
epoch 0  | loss: 0.9579  | val_0_rmse: 0.83445 | val_1_rmse: 0.82357 |  0:00:10s
epoch 1  | loss: 0.50363 | val_0_rmse: 0.79669 | val_1_rmse: 0.7898  |  0:00:21s
epoch 2  | loss: 0.41718 | val_0_rmse: 0.7287  | val_1_rmse: 0.72152 |  0:00:31s
epoch 3  | loss: 0.39543 | val_0_rmse: 0.70754 | val_1_rmse: 0.7042  |  0:00:42s
epoch 4  | loss: 0.38208 | val_0_rmse: 0.68067 | val_1_rmse: 0.67614 |  0:00:52s
epoch 5  | loss: 0.37683 | val_0_rmse: 0.64787 | val_1_rmse: 0.64666 |  0:01:03s
epoch 6  | loss: 0.36825 | val_0_rmse: 0.62142 | val_1_rmse: 0.62286 |  0:01:13s
epoch 7  | loss: 0.36365 | val_0_rmse: 0.61442 | val_1_rmse: 0.61638 |  0:01:24s
epoch 8  | loss: 0.35899 | val_0_rmse: 0.58553 | val_1_rmse: 0.59555 |  0:01:34s
epoch 9  | loss: 0.35618 | val_0_rmse: 0.57636 | val_1_rmse: 0.59002 |  0:01:45s
epoch 10 | loss: 0.35345 | val_0_rmse: 0.57881 | val_1_rmse: 0.5922  |  0:01:56s
epoch 11 | loss: 0.34981 | val_0_rmse: 0.57438 | val_1_rmse: 0.59087 |  0:02:06s
epoch 12 | loss: 0.34832 | val_0_rmse: 0.58125 | val_1_rmse: 0.61103 |  0:02:17s
epoch 13 | loss: 0.34602 | val_0_rmse: 0.59909 | val_1_rmse: 0.61947 |  0:02:27s
epoch 14 | loss: 0.34392 | val_0_rmse: 0.57461 | val_1_rmse: 0.59986 |  0:02:38s
epoch 15 | loss: 0.33948 | val_0_rmse: 0.58345 | val_1_rmse: 0.61084 |  0:02:48s
epoch 16 | loss: 0.3415  | val_0_rmse: 0.57579 | val_1_rmse: 0.60138 |  0:02:58s
epoch 17 | loss: 0.33684 | val_0_rmse: 0.5788  | val_1_rmse: 0.60662 |  0:03:09s
epoch 18 | loss: 0.33951 | val_0_rmse: 0.61323 | val_1_rmse: 0.64547 |  0:03:19s
epoch 19 | loss: 0.33539 | val_0_rmse: 0.57236 | val_1_rmse: 0.60133 |  0:03:30s
epoch 20 | loss: 0.33187 | val_0_rmse: 0.5767  | val_1_rmse: 0.61072 |  0:03:40s
epoch 21 | loss: 0.33005 | val_0_rmse: 0.57322 | val_1_rmse: 0.60566 |  0:03:51s
epoch 22 | loss: 0.32676 | val_0_rmse: 0.57272 | val_1_rmse: 0.60562 |  0:04:01s
epoch 23 | loss: 0.32822 | val_0_rmse: 0.5719  | val_1_rmse: 0.60833 |  0:04:11s
epoch 24 | loss: 0.32437 | val_0_rmse: 0.57507 | val_1_rmse: 0.60839 |  0:04:22s
epoch 25 | loss: 0.32286 | val_0_rmse: 0.56422 | val_1_rmse: 0.60003 |  0:04:32s
epoch 26 | loss: 0.3195  | val_0_rmse: 0.56297 | val_1_rmse: 0.60431 |  0:04:43s
epoch 27 | loss: 0.31817 | val_0_rmse: 0.5813  | val_1_rmse: 0.62086 |  0:04:53s
epoch 28 | loss: 0.31958 | val_0_rmse: 0.6     | val_1_rmse: 0.62459 |  0:05:03s
epoch 29 | loss: 0.31563 | val_0_rmse: 0.56159 | val_1_rmse: 0.59985 |  0:05:14s
epoch 30 | loss: 0.31312 | val_0_rmse: 0.56564 | val_1_rmse: 0.61186 |  0:05:24s
epoch 31 | loss: 0.31295 | val_0_rmse: 0.55955 | val_1_rmse: 0.59638 |  0:05:35s
epoch 32 | loss: 0.31291 | val_0_rmse: 0.55638 | val_1_rmse: 0.60148 |  0:05:45s
epoch 33 | loss: 0.3102  | val_0_rmse: 0.57647 | val_1_rmse: 0.62736 |  0:05:56s
epoch 34 | loss: 0.31024 | val_0_rmse: 0.57909 | val_1_rmse: 0.61518 |  0:06:06s
epoch 35 | loss: 0.30716 | val_0_rmse: 0.56683 | val_1_rmse: 0.61451 |  0:06:16s
epoch 36 | loss: 0.30844 | val_0_rmse: 0.55857 | val_1_rmse: 0.59911 |  0:06:27s
epoch 37 | loss: 0.30824 | val_0_rmse: 0.55551 | val_1_rmse: 0.60343 |  0:06:37s
epoch 38 | loss: 0.30537 | val_0_rmse: 0.55123 | val_1_rmse: 0.59736 |  0:06:48s
epoch 39 | loss: 0.30318 | val_0_rmse: 0.55856 | val_1_rmse: 0.60369 |  0:06:58s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.59002
Best weights from best epoch are automatically used!
ended training at: 07:01:57
Feature importance:
Mean squared error is of 2241978426.062981
Mean absolute error:34487.672680602656
MAPE:0.26355655689665664
R2 score:0.651394145499044
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:01:59
epoch 0  | loss: 0.84113 | val_0_rmse: 0.82081 | val_1_rmse: 0.81757 |  0:00:10s
epoch 1  | loss: 0.47702 | val_0_rmse: 0.85497 | val_1_rmse: 0.84257 |  0:00:21s
epoch 2  | loss: 0.41627 | val_0_rmse: 0.72782 | val_1_rmse: 0.72407 |  0:00:31s
epoch 3  | loss: 0.3927  | val_0_rmse: 0.70823 | val_1_rmse: 0.70398 |  0:00:42s
epoch 4  | loss: 0.38017 | val_0_rmse: 0.68129 | val_1_rmse: 0.67913 |  0:00:52s
epoch 5  | loss: 0.37189 | val_0_rmse: 0.68458 | val_1_rmse: 0.68353 |  0:01:03s
epoch 6  | loss: 0.3718  | val_0_rmse: 0.62409 | val_1_rmse: 0.62923 |  0:01:14s
epoch 7  | loss: 0.36424 | val_0_rmse: 0.60329 | val_1_rmse: 0.61366 |  0:01:24s
epoch 8  | loss: 0.36044 | val_0_rmse: 0.59264 | val_1_rmse: 0.60478 |  0:01:35s
epoch 9  | loss: 0.35535 | val_0_rmse: 0.57726 | val_1_rmse: 0.59746 |  0:01:45s
epoch 10 | loss: 0.35512 | val_0_rmse: 0.58106 | val_1_rmse: 0.59997 |  0:01:56s
epoch 11 | loss: 0.34819 | val_0_rmse: 0.57594 | val_1_rmse: 0.59961 |  0:02:07s
epoch 12 | loss: 0.34718 | val_0_rmse: 0.5949  | val_1_rmse: 0.63054 |  0:02:17s
epoch 13 | loss: 0.3495  | val_0_rmse: 0.5898  | val_1_rmse: 0.61843 |  0:02:28s
epoch 14 | loss: 0.34857 | val_0_rmse: 0.58231 | val_1_rmse: 0.6194  |  0:02:38s
epoch 15 | loss: 0.34323 | val_0_rmse: 0.6015  | val_1_rmse: 0.62886 |  0:02:49s
epoch 16 | loss: 0.33856 | val_0_rmse: 0.5813  | val_1_rmse: 0.61306 |  0:03:00s
epoch 17 | loss: 0.3407  | val_0_rmse: 0.5786  | val_1_rmse: 0.60519 |  0:03:10s
epoch 18 | loss: 0.33626 | val_0_rmse: 0.78477 | val_1_rmse: 0.79305 |  0:03:21s
epoch 19 | loss: 0.33521 | val_0_rmse: 0.57213 | val_1_rmse: 0.60006 |  0:03:31s
epoch 20 | loss: 0.33195 | val_0_rmse: 0.57386 | val_1_rmse: 0.60427 |  0:03:42s
epoch 21 | loss: 0.33555 | val_0_rmse: 0.57592 | val_1_rmse: 0.60222 |  0:03:52s
epoch 22 | loss: 0.33865 | val_0_rmse: 0.59757 | val_1_rmse: 0.67917 |  0:04:03s
epoch 23 | loss: 0.34564 | val_0_rmse: 0.57689 | val_1_rmse: 0.60875 |  0:04:14s
epoch 24 | loss: 0.3366  | val_0_rmse: 0.58107 | val_1_rmse: 0.74336 |  0:04:24s
epoch 25 | loss: 0.33421 | val_0_rmse: 0.62879 | val_1_rmse: 0.66606 |  0:04:35s
epoch 26 | loss: 0.33067 | val_0_rmse: 0.58723 | val_1_rmse: 0.6285  |  0:04:45s
epoch 27 | loss: 0.3279  | val_0_rmse: 0.57004 | val_1_rmse: 0.64089 |  0:04:56s
epoch 28 | loss: 0.32436 | val_0_rmse: 0.58888 | val_1_rmse: 0.63235 |  0:05:07s
epoch 29 | loss: 0.32243 | val_0_rmse: 0.5926  | val_1_rmse: 0.63446 |  0:05:17s
epoch 30 | loss: 0.32049 | val_0_rmse: 0.56167 | val_1_rmse: 0.60158 |  0:05:28s
epoch 31 | loss: 0.31931 | val_0_rmse: 0.56074 | val_1_rmse: 0.60135 |  0:05:38s
epoch 32 | loss: 0.31797 | val_0_rmse: 0.5903  | val_1_rmse: 0.60938 |  0:05:49s
epoch 33 | loss: 0.31803 | val_0_rmse: 0.57429 | val_1_rmse: 0.60754 |  0:06:00s
epoch 34 | loss: 0.31522 | val_0_rmse: 0.56289 | val_1_rmse: 0.60356 |  0:06:10s
epoch 35 | loss: 0.3182  | val_0_rmse: 0.55612 | val_1_rmse: 0.60297 |  0:06:21s
epoch 36 | loss: 0.31271 | val_0_rmse: 0.58791 | val_1_rmse: 0.64051 |  0:06:31s
epoch 37 | loss: 0.31357 | val_0_rmse: 0.63879 | val_1_rmse: 0.6932  |  0:06:42s
epoch 38 | loss: 0.30986 | val_0_rmse: 0.56461 | val_1_rmse: 0.61932 |  0:06:53s
epoch 39 | loss: 0.30969 | val_0_rmse: 0.55894 | val_1_rmse: 0.60693 |  0:07:03s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.59746
Best weights from best epoch are automatically used!
ended training at: 07:09:08
Feature importance:
Mean squared error is of 2320290610.5787582
Mean absolute error:35030.53103041372
MAPE:0.2666953797891738
R2 score:0.6390333754631501
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:09:10
epoch 0  | loss: 1.62286 | val_0_rmse: 0.99692 | val_1_rmse: 0.97399 |  0:00:01s
epoch 1  | loss: 0.81074 | val_0_rmse: 0.8498  | val_1_rmse: 0.84239 |  0:00:03s
epoch 2  | loss: 0.5815  | val_0_rmse: 0.80786 | val_1_rmse: 0.8185  |  0:00:04s
epoch 3  | loss: 0.47192 | val_0_rmse: 0.80441 | val_1_rmse: 0.82233 |  0:00:06s
epoch 4  | loss: 0.42212 | val_0_rmse: 0.74226 | val_1_rmse: 0.75504 |  0:00:07s
epoch 5  | loss: 0.39527 | val_0_rmse: 0.73031 | val_1_rmse: 0.74074 |  0:00:09s
epoch 6  | loss: 0.38262 | val_0_rmse: 0.684   | val_1_rmse: 0.68785 |  0:00:10s
epoch 7  | loss: 0.37437 | val_0_rmse: 0.68842 | val_1_rmse: 0.70072 |  0:00:12s
epoch 8  | loss: 0.36694 | val_0_rmse: 0.66673 | val_1_rmse: 0.67157 |  0:00:13s
epoch 9  | loss: 0.35986 | val_0_rmse: 0.67484 | val_1_rmse: 0.67715 |  0:00:15s
epoch 10 | loss: 0.34742 | val_0_rmse: 0.65331 | val_1_rmse: 0.65626 |  0:00:16s
epoch 11 | loss: 0.34621 | val_0_rmse: 0.65655 | val_1_rmse: 0.66088 |  0:00:18s
epoch 12 | loss: 0.34423 | val_0_rmse: 0.62315 | val_1_rmse: 0.63087 |  0:00:19s
epoch 13 | loss: 0.33941 | val_0_rmse: 0.64071 | val_1_rmse: 0.63753 |  0:00:21s
epoch 14 | loss: 0.33739 | val_0_rmse: 0.61811 | val_1_rmse: 0.62727 |  0:00:23s
epoch 15 | loss: 0.3358  | val_0_rmse: 0.61573 | val_1_rmse: 0.61714 |  0:00:24s
epoch 16 | loss: 0.33875 | val_0_rmse: 0.62421 | val_1_rmse: 0.62068 |  0:00:26s
epoch 17 | loss: 0.32878 | val_0_rmse: 0.60049 | val_1_rmse: 0.60683 |  0:00:27s
epoch 18 | loss: 0.33683 | val_0_rmse: 0.62276 | val_1_rmse: 0.62627 |  0:00:29s
epoch 19 | loss: 0.32994 | val_0_rmse: 0.60469 | val_1_rmse: 0.60112 |  0:00:30s
epoch 20 | loss: 0.32614 | val_0_rmse: 0.58579 | val_1_rmse: 0.5904  |  0:00:32s
epoch 21 | loss: 0.32175 | val_0_rmse: 0.59598 | val_1_rmse: 0.5928  |  0:00:33s
epoch 22 | loss: 0.32174 | val_0_rmse: 0.58417 | val_1_rmse: 0.58694 |  0:00:35s
epoch 23 | loss: 0.32221 | val_0_rmse: 0.58518 | val_1_rmse: 0.59167 |  0:00:36s
epoch 24 | loss: 0.32177 | val_0_rmse: 0.58162 | val_1_rmse: 0.58423 |  0:00:38s
epoch 25 | loss: 0.32273 | val_0_rmse: 0.59684 | val_1_rmse: 0.60975 |  0:00:40s
epoch 26 | loss: 0.32075 | val_0_rmse: 0.58614 | val_1_rmse: 0.59265 |  0:00:41s
epoch 27 | loss: 0.31688 | val_0_rmse: 0.56335 | val_1_rmse: 0.57825 |  0:00:43s
epoch 28 | loss: 0.31222 | val_0_rmse: 0.56797 | val_1_rmse: 0.57908 |  0:00:44s
epoch 29 | loss: 0.31507 | val_0_rmse: 0.56647 | val_1_rmse: 0.57973 |  0:00:46s
epoch 30 | loss: 0.3117  | val_0_rmse: 0.55628 | val_1_rmse: 0.57085 |  0:00:47s
epoch 31 | loss: 0.31069 | val_0_rmse: 0.55126 | val_1_rmse: 0.57105 |  0:00:49s
epoch 32 | loss: 0.30692 | val_0_rmse: 0.55035 | val_1_rmse: 0.57442 |  0:00:50s
epoch 33 | loss: 0.30941 | val_0_rmse: 0.55135 | val_1_rmse: 0.57635 |  0:00:52s
epoch 34 | loss: 0.30709 | val_0_rmse: 0.55289 | val_1_rmse: 0.57175 |  0:00:53s
epoch 35 | loss: 0.30509 | val_0_rmse: 0.54332 | val_1_rmse: 0.57087 |  0:00:55s
epoch 36 | loss: 0.30257 | val_0_rmse: 0.55927 | val_1_rmse: 0.57851 |  0:00:56s
epoch 37 | loss: 0.30371 | val_0_rmse: 0.54187 | val_1_rmse: 0.56352 |  0:00:58s
epoch 38 | loss: 0.30113 | val_0_rmse: 0.54091 | val_1_rmse: 0.57481 |  0:00:59s
epoch 39 | loss: 0.30135 | val_0_rmse: 0.54278 | val_1_rmse: 0.57054 |  0:01:01s
epoch 40 | loss: 0.30098 | val_0_rmse: 0.53923 | val_1_rmse: 0.56985 |  0:01:03s
epoch 41 | loss: 0.30315 | val_0_rmse: 0.53897 | val_1_rmse: 0.57159 |  0:01:04s
epoch 42 | loss: 0.29856 | val_0_rmse: 0.53571 | val_1_rmse: 0.57206 |  0:01:06s
epoch 43 | loss: 0.2968  | val_0_rmse: 0.53341 | val_1_rmse: 0.56934 |  0:01:07s
epoch 44 | loss: 0.30052 | val_0_rmse: 0.54481 | val_1_rmse: 0.57404 |  0:01:09s
epoch 45 | loss: 0.29741 | val_0_rmse: 0.53536 | val_1_rmse: 0.57167 |  0:01:10s
epoch 46 | loss: 0.29881 | val_0_rmse: 0.53744 | val_1_rmse: 0.57766 |  0:01:12s
epoch 47 | loss: 0.29704 | val_0_rmse: 0.53255 | val_1_rmse: 0.57168 |  0:01:13s
epoch 48 | loss: 0.29778 | val_0_rmse: 0.53363 | val_1_rmse: 0.57634 |  0:01:15s
epoch 49 | loss: 0.29763 | val_0_rmse: 0.53112 | val_1_rmse: 0.57495 |  0:01:16s
epoch 50 | loss: 0.29412 | val_0_rmse: 0.53092 | val_1_rmse: 0.57567 |  0:01:18s
epoch 51 | loss: 0.29748 | val_0_rmse: 0.5312  | val_1_rmse: 0.5684  |  0:01:19s
epoch 52 | loss: 0.29303 | val_0_rmse: 0.53073 | val_1_rmse: 0.57355 |  0:01:21s
epoch 53 | loss: 0.29369 | val_0_rmse: 0.53452 | val_1_rmse: 0.57658 |  0:01:22s
epoch 54 | loss: 0.30088 | val_0_rmse: 0.53164 | val_1_rmse: 0.5764  |  0:01:24s
epoch 55 | loss: 0.29548 | val_0_rmse: 0.53554 | val_1_rmse: 0.57194 |  0:01:25s
epoch 56 | loss: 0.29216 | val_0_rmse: 0.53138 | val_1_rmse: 0.5728  |  0:01:27s
epoch 57 | loss: 0.29361 | val_0_rmse: 0.53032 | val_1_rmse: 0.57586 |  0:01:29s
epoch 58 | loss: 0.29233 | val_0_rmse: 0.53562 | val_1_rmse: 0.58676 |  0:01:30s
epoch 59 | loss: 0.2926  | val_0_rmse: 0.53311 | val_1_rmse: 0.57525 |  0:01:32s
epoch 60 | loss: 0.29054 | val_0_rmse: 0.52706 | val_1_rmse: 0.58062 |  0:01:33s
epoch 61 | loss: 0.28842 | val_0_rmse: 0.5299  | val_1_rmse: 0.574   |  0:01:35s
epoch 62 | loss: 0.29288 | val_0_rmse: 0.53003 | val_1_rmse: 0.5745  |  0:01:36s
epoch 63 | loss: 0.29105 | val_0_rmse: 0.53555 | val_1_rmse: 0.58545 |  0:01:38s
epoch 64 | loss: 0.28945 | val_0_rmse: 0.52901 | val_1_rmse: 0.57413 |  0:01:39s
epoch 65 | loss: 0.28787 | val_0_rmse: 0.52775 | val_1_rmse: 0.57001 |  0:01:41s
epoch 66 | loss: 0.28968 | val_0_rmse: 0.53101 | val_1_rmse: 0.58308 |  0:01:42s
epoch 67 | loss: 0.29176 | val_0_rmse: 0.53102 | val_1_rmse: 0.57797 |  0:01:44s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.56352
Best weights from best epoch are automatically used!
ended training at: 07:10:55
Feature importance:
Mean squared error is of 1410080969.1110911
Mean absolute error:27035.6932246401
MAPE:0.2736067701674146
R2 score:0.6753772327466705
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:10:55
epoch 0  | loss: 1.59498 | val_0_rmse: 0.99436 | val_1_rmse: 1.00464 |  0:00:01s
epoch 1  | loss: 0.81972 | val_0_rmse: 1.2227  | val_1_rmse: 1.23952 |  0:00:03s
epoch 2  | loss: 0.56436 | val_0_rmse: 0.99609 | val_1_rmse: 0.988   |  0:00:04s
epoch 3  | loss: 0.49116 | val_0_rmse: 0.77636 | val_1_rmse: 0.77    |  0:00:06s
epoch 4  | loss: 0.4454  | val_0_rmse: 0.76664 | val_1_rmse: 0.75219 |  0:00:07s
epoch 5  | loss: 0.41508 | val_0_rmse: 0.71488 | val_1_rmse: 0.70283 |  0:00:09s
epoch 6  | loss: 0.40119 | val_0_rmse: 0.74111 | val_1_rmse: 0.72753 |  0:00:10s
epoch 7  | loss: 0.39413 | val_0_rmse: 0.698   | val_1_rmse: 0.6861  |  0:00:12s
epoch 8  | loss: 0.38826 | val_0_rmse: 0.65058 | val_1_rmse: 0.64062 |  0:00:13s
epoch 9  | loss: 0.38464 | val_0_rmse: 0.65921 | val_1_rmse: 0.65059 |  0:00:15s
epoch 10 | loss: 0.37395 | val_0_rmse: 0.66653 | val_1_rmse: 0.65989 |  0:00:16s
epoch 11 | loss: 0.37164 | val_0_rmse: 0.65343 | val_1_rmse: 0.64548 |  0:00:18s
epoch 12 | loss: 0.36653 | val_0_rmse: 0.63791 | val_1_rmse: 0.63601 |  0:00:19s
epoch 13 | loss: 0.36351 | val_0_rmse: 0.62024 | val_1_rmse: 0.61826 |  0:00:21s
epoch 14 | loss: 0.36307 | val_0_rmse: 0.63335 | val_1_rmse: 0.63053 |  0:00:23s
epoch 15 | loss: 0.35249 | val_0_rmse: 0.61848 | val_1_rmse: 0.61889 |  0:00:24s
epoch 16 | loss: 0.35076 | val_0_rmse: 0.61296 | val_1_rmse: 0.60493 |  0:00:26s
epoch 17 | loss: 0.35344 | val_0_rmse: 0.64779 | val_1_rmse: 0.64533 |  0:00:27s
epoch 18 | loss: 0.35946 | val_0_rmse: 0.6191  | val_1_rmse: 0.61484 |  0:00:29s
epoch 19 | loss: 0.35713 | val_0_rmse: 0.60168 | val_1_rmse: 0.59437 |  0:00:30s
epoch 20 | loss: 0.34889 | val_0_rmse: 0.59491 | val_1_rmse: 0.5918  |  0:00:32s
epoch 21 | loss: 0.34379 | val_0_rmse: 0.5876  | val_1_rmse: 0.59024 |  0:00:33s
epoch 22 | loss: 0.33183 | val_0_rmse: 0.5879  | val_1_rmse: 0.59177 |  0:00:35s
epoch 23 | loss: 0.33045 | val_0_rmse: 0.5804  | val_1_rmse: 0.58414 |  0:00:36s
epoch 24 | loss: 0.33035 | val_0_rmse: 0.57811 | val_1_rmse: 0.58041 |  0:00:38s
epoch 25 | loss: 0.32559 | val_0_rmse: 0.58005 | val_1_rmse: 0.57946 |  0:00:40s
epoch 26 | loss: 0.32819 | val_0_rmse: 0.58375 | val_1_rmse: 0.58464 |  0:00:41s
epoch 27 | loss: 0.32484 | val_0_rmse: 0.57013 | val_1_rmse: 0.57491 |  0:00:43s
epoch 28 | loss: 0.32713 | val_0_rmse: 0.57192 | val_1_rmse: 0.57794 |  0:00:44s
epoch 29 | loss: 0.32507 | val_0_rmse: 0.57057 | val_1_rmse: 0.5784  |  0:00:46s
epoch 30 | loss: 0.32567 | val_0_rmse: 0.58372 | val_1_rmse: 0.59159 |  0:00:47s
epoch 31 | loss: 0.32591 | val_0_rmse: 0.56457 | val_1_rmse: 0.57702 |  0:00:49s
epoch 32 | loss: 0.32109 | val_0_rmse: 0.55781 | val_1_rmse: 0.56941 |  0:00:50s
epoch 33 | loss: 0.31799 | val_0_rmse: 0.59006 | val_1_rmse: 0.6021  |  0:00:52s
epoch 34 | loss: 0.31733 | val_0_rmse: 0.56163 | val_1_rmse: 0.57566 |  0:00:53s
epoch 35 | loss: 0.31693 | val_0_rmse: 0.56093 | val_1_rmse: 0.57675 |  0:00:55s
epoch 36 | loss: 0.31825 | val_0_rmse: 0.56708 | val_1_rmse: 0.58526 |  0:00:57s
epoch 37 | loss: 0.31729 | val_0_rmse: 0.55324 | val_1_rmse: 0.57096 |  0:00:58s
epoch 38 | loss: 0.31195 | val_0_rmse: 0.56307 | val_1_rmse: 0.57813 |  0:01:00s
epoch 39 | loss: 0.31391 | val_0_rmse: 0.55672 | val_1_rmse: 0.57184 |  0:01:01s
epoch 40 | loss: 0.31076 | val_0_rmse: 0.55543 | val_1_rmse: 0.57575 |  0:01:03s
epoch 41 | loss: 0.3131  | val_0_rmse: 0.5583  | val_1_rmse: 0.57543 |  0:01:04s
epoch 42 | loss: 0.31435 | val_0_rmse: 0.57071 | val_1_rmse: 0.58636 |  0:01:06s
epoch 43 | loss: 0.31205 | val_0_rmse: 0.55616 | val_1_rmse: 0.56967 |  0:01:07s
epoch 44 | loss: 0.31114 | val_0_rmse: 0.56199 | val_1_rmse: 0.576   |  0:01:09s
epoch 45 | loss: 0.31039 | val_0_rmse: 0.57636 | val_1_rmse: 0.58894 |  0:01:10s
epoch 46 | loss: 0.31001 | val_0_rmse: 0.58371 | val_1_rmse: 0.59484 |  0:01:12s
epoch 47 | loss: 0.31031 | val_0_rmse: 0.56619 | val_1_rmse: 0.58161 |  0:01:14s
epoch 48 | loss: 0.30873 | val_0_rmse: 0.59936 | val_1_rmse: 0.6055  |  0:01:15s
epoch 49 | loss: 0.31076 | val_0_rmse: 0.57587 | val_1_rmse: 0.58921 |  0:01:17s
epoch 50 | loss: 0.3113  | val_0_rmse: 0.56913 | val_1_rmse: 0.58949 |  0:01:18s
epoch 51 | loss: 0.3124  | val_0_rmse: 0.58016 | val_1_rmse: 0.59458 |  0:01:20s
epoch 52 | loss: 0.31253 | val_0_rmse: 0.57188 | val_1_rmse: 0.5884  |  0:01:21s
epoch 53 | loss: 0.31041 | val_0_rmse: 0.60229 | val_1_rmse: 0.59114 |  0:01:23s
epoch 54 | loss: 0.3052  | val_0_rmse: 0.58032 | val_1_rmse: 0.59208 |  0:01:24s
epoch 55 | loss: 0.30207 | val_0_rmse: 0.61344 | val_1_rmse: 0.6138  |  0:01:26s
epoch 56 | loss: 0.30352 | val_0_rmse: 0.61465 | val_1_rmse: 0.61325 |  0:01:27s
epoch 57 | loss: 0.30271 | val_0_rmse: 0.6077  | val_1_rmse: 0.61299 |  0:01:29s
epoch 58 | loss: 0.30184 | val_0_rmse: 0.63265 | val_1_rmse: 0.63183 |  0:01:30s
epoch 59 | loss: 0.3089  | val_0_rmse: 0.57412 | val_1_rmse: 0.59687 |  0:01:32s
epoch 60 | loss: 0.30647 | val_0_rmse: 0.62433 | val_1_rmse: 0.6138  |  0:01:33s
epoch 61 | loss: 0.30391 | val_0_rmse: 0.63876 | val_1_rmse: 0.61859 |  0:01:35s
epoch 62 | loss: 0.30543 | val_0_rmse: 0.72389 | val_1_rmse: 0.64315 |  0:01:37s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.56941
Best weights from best epoch are automatically used!
ended training at: 07:12:33
Feature importance:
Mean squared error is of 1355999326.0538797
Mean absolute error:26919.67779929911
MAPE:0.2853155544620154
R2 score:0.6867863197566442
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:12:33
epoch 0  | loss: 1.62677 | val_0_rmse: 0.99829 | val_1_rmse: 0.98399 |  0:00:01s
epoch 1  | loss: 0.79027 | val_0_rmse: 0.8451  | val_1_rmse: 0.84512 |  0:00:03s
epoch 2  | loss: 0.54289 | val_0_rmse: 0.88702 | val_1_rmse: 0.88882 |  0:00:04s
epoch 3  | loss: 0.46217 | val_0_rmse: 0.75386 | val_1_rmse: 0.75496 |  0:00:06s
epoch 4  | loss: 0.41887 | val_0_rmse: 0.74892 | val_1_rmse: 0.74864 |  0:00:07s
epoch 5  | loss: 0.39067 | val_0_rmse: 0.7611  | val_1_rmse: 0.76575 |  0:00:09s
epoch 6  | loss: 0.37945 | val_0_rmse: 0.71267 | val_1_rmse: 0.70773 |  0:00:10s
epoch 7  | loss: 0.35953 | val_0_rmse: 0.66534 | val_1_rmse: 0.66035 |  0:00:12s
epoch 8  | loss: 0.35286 | val_0_rmse: 0.66431 | val_1_rmse: 0.66227 |  0:00:13s
epoch 9  | loss: 0.34441 | val_0_rmse: 0.65364 | val_1_rmse: 0.64862 |  0:00:15s
epoch 10 | loss: 0.34164 | val_0_rmse: 0.64532 | val_1_rmse: 0.64586 |  0:00:16s
epoch 11 | loss: 0.33774 | val_0_rmse: 0.62293 | val_1_rmse: 0.62516 |  0:00:18s
epoch 12 | loss: 0.33237 | val_0_rmse: 0.64379 | val_1_rmse: 0.64978 |  0:00:19s
epoch 13 | loss: 0.32594 | val_0_rmse: 0.61567 | val_1_rmse: 0.62046 |  0:00:21s
epoch 14 | loss: 0.32138 | val_0_rmse: 0.60424 | val_1_rmse: 0.60734 |  0:00:23s
epoch 15 | loss: 0.32007 | val_0_rmse: 0.60651 | val_1_rmse: 0.61327 |  0:00:24s
epoch 16 | loss: 0.31745 | val_0_rmse: 0.6105  | val_1_rmse: 0.62044 |  0:00:26s
epoch 17 | loss: 0.31762 | val_0_rmse: 0.59529 | val_1_rmse: 0.60408 |  0:00:27s
epoch 18 | loss: 0.32005 | val_0_rmse: 0.57993 | val_1_rmse: 0.59291 |  0:00:29s
epoch 19 | loss: 0.3153  | val_0_rmse: 0.60285 | val_1_rmse: 0.62122 |  0:00:30s
epoch 20 | loss: 0.31556 | val_0_rmse: 0.57923 | val_1_rmse: 0.5911  |  0:00:32s
epoch 21 | loss: 0.31074 | val_0_rmse: 0.57626 | val_1_rmse: 0.58825 |  0:00:33s
epoch 22 | loss: 0.31287 | val_0_rmse: 0.57836 | val_1_rmse: 0.58557 |  0:00:35s
epoch 23 | loss: 0.30714 | val_0_rmse: 0.57054 | val_1_rmse: 0.58715 |  0:00:36s
epoch 24 | loss: 0.30879 | val_0_rmse: 0.56403 | val_1_rmse: 0.57962 |  0:00:38s
epoch 25 | loss: 0.3055  | val_0_rmse: 0.5705  | val_1_rmse: 0.59269 |  0:00:39s
epoch 26 | loss: 0.30368 | val_0_rmse: 0.55844 | val_1_rmse: 0.57545 |  0:00:41s
epoch 27 | loss: 0.30043 | val_0_rmse: 0.55986 | val_1_rmse: 0.58019 |  0:00:43s
epoch 28 | loss: 0.30879 | val_0_rmse: 0.55839 | val_1_rmse: 0.57932 |  0:00:44s
epoch 29 | loss: 0.30639 | val_0_rmse: 0.55297 | val_1_rmse: 0.57884 |  0:00:46s
epoch 30 | loss: 0.30276 | val_0_rmse: 0.5477  | val_1_rmse: 0.57295 |  0:00:47s
epoch 31 | loss: 0.30092 | val_0_rmse: 0.54241 | val_1_rmse: 0.57241 |  0:00:49s
epoch 32 | loss: 0.30155 | val_0_rmse: 0.54841 | val_1_rmse: 0.58525 |  0:00:50s
epoch 33 | loss: 0.30415 | val_0_rmse: 0.54365 | val_1_rmse: 0.57295 |  0:00:52s
epoch 34 | loss: 0.29778 | val_0_rmse: 0.54805 | val_1_rmse: 0.58763 |  0:00:53s
epoch 35 | loss: 0.29662 | val_0_rmse: 0.55913 | val_1_rmse: 0.58946 |  0:00:55s
epoch 36 | loss: 0.3003  | val_0_rmse: 0.53661 | val_1_rmse: 0.58318 |  0:00:56s
epoch 37 | loss: 0.29422 | val_0_rmse: 0.53264 | val_1_rmse: 0.58136 |  0:00:58s
epoch 38 | loss: 0.29396 | val_0_rmse: 0.52923 | val_1_rmse: 0.58076 |  0:00:59s
epoch 39 | loss: 0.29115 | val_0_rmse: 0.52895 | val_1_rmse: 0.57135 |  0:01:01s
epoch 40 | loss: 0.29082 | val_0_rmse: 0.52326 | val_1_rmse: 0.56983 |  0:01:02s
epoch 41 | loss: 0.28755 | val_0_rmse: 0.52325 | val_1_rmse: 0.57189 |  0:01:04s
epoch 42 | loss: 0.28681 | val_0_rmse: 0.52615 | val_1_rmse: 0.57017 |  0:01:06s
epoch 43 | loss: 0.28788 | val_0_rmse: 0.52649 | val_1_rmse: 0.57914 |  0:01:07s
epoch 44 | loss: 0.28302 | val_0_rmse: 0.53269 | val_1_rmse: 0.57916 |  0:01:09s
epoch 45 | loss: 0.28731 | val_0_rmse: 0.52402 | val_1_rmse: 0.57435 |  0:01:10s
epoch 46 | loss: 0.28612 | val_0_rmse: 0.52303 | val_1_rmse: 0.58119 |  0:01:12s
epoch 47 | loss: 0.28401 | val_0_rmse: 0.51988 | val_1_rmse: 0.57389 |  0:01:13s
epoch 48 | loss: 0.28132 | val_0_rmse: 0.51893 | val_1_rmse: 0.58188 |  0:01:15s
epoch 49 | loss: 0.28356 | val_0_rmse: 0.51416 | val_1_rmse: 0.57321 |  0:01:16s
epoch 50 | loss: 0.2831  | val_0_rmse: 0.51681 | val_1_rmse: 0.57625 |  0:01:18s
epoch 51 | loss: 0.28199 | val_0_rmse: 0.51884 | val_1_rmse: 0.57764 |  0:01:19s
epoch 52 | loss: 0.28067 | val_0_rmse: 0.52011 | val_1_rmse: 0.57865 |  0:01:21s
epoch 53 | loss: 0.27921 | val_0_rmse: 0.51832 | val_1_rmse: 0.57842 |  0:01:22s
epoch 54 | loss: 0.27666 | val_0_rmse: 0.52722 | val_1_rmse: 0.58222 |  0:01:24s
epoch 55 | loss: 0.28152 | val_0_rmse: 0.5233  | val_1_rmse: 0.58726 |  0:01:25s
epoch 56 | loss: 0.2769  | val_0_rmse: 0.52511 | val_1_rmse: 0.58125 |  0:01:27s
epoch 57 | loss: 0.2745  | val_0_rmse: 0.53328 | val_1_rmse: 0.5906  |  0:01:28s
epoch 58 | loss: 0.28096 | val_0_rmse: 0.51532 | val_1_rmse: 0.58605 |  0:01:30s
epoch 59 | loss: 0.28051 | val_0_rmse: 0.52427 | val_1_rmse: 0.57969 |  0:01:32s
epoch 60 | loss: 0.27922 | val_0_rmse: 0.53048 | val_1_rmse: 0.61197 |  0:01:33s
epoch 61 | loss: 0.2782  | val_0_rmse: 0.51348 | val_1_rmse: 0.5805  |  0:01:35s
epoch 62 | loss: 0.27346 | val_0_rmse: 0.51015 | val_1_rmse: 0.57887 |  0:01:36s
epoch 63 | loss: 0.27188 | val_0_rmse: 0.51052 | val_1_rmse: 0.57732 |  0:01:38s
epoch 64 | loss: 0.27208 | val_0_rmse: 0.51614 | val_1_rmse: 0.59071 |  0:01:39s
epoch 65 | loss: 0.26875 | val_0_rmse: 0.51243 | val_1_rmse: 0.59203 |  0:01:41s
epoch 66 | loss: 0.26794 | val_0_rmse: 0.51293 | val_1_rmse: 0.58872 |  0:01:42s
epoch 67 | loss: 0.27246 | val_0_rmse: 0.52359 | val_1_rmse: 0.60387 |  0:01:44s
epoch 68 | loss: 0.27237 | val_0_rmse: 0.51114 | val_1_rmse: 0.58633 |  0:01:45s
epoch 69 | loss: 0.27003 | val_0_rmse: 0.51453 | val_1_rmse: 0.59512 |  0:01:47s
epoch 70 | loss: 0.27093 | val_0_rmse: 0.5111  | val_1_rmse: 0.58507 |  0:01:48s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.56983
Best weights from best epoch are automatically used!
ended training at: 07:14:23
Feature importance:
Mean squared error is of 1414238912.4578483
Mean absolute error:26536.780656094823
MAPE:0.2671343163800732
R2 score:0.6811820775986959
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:14:23
epoch 0  | loss: 1.47825 | val_0_rmse: 0.99572 | val_1_rmse: 0.98375 |  0:00:01s
epoch 1  | loss: 0.86165 | val_0_rmse: 0.96848 | val_1_rmse: 0.95422 |  0:00:03s
epoch 2  | loss: 0.66041 | val_0_rmse: 0.92583 | val_1_rmse: 0.9226  |  0:00:04s
epoch 3  | loss: 0.57135 | val_0_rmse: 0.80996 | val_1_rmse: 0.7984  |  0:00:06s
epoch 4  | loss: 0.49735 | val_0_rmse: 0.78625 | val_1_rmse: 0.77065 |  0:00:07s
epoch 5  | loss: 0.4433  | val_0_rmse: 0.76314 | val_1_rmse: 0.74692 |  0:00:09s
epoch 6  | loss: 0.4113  | val_0_rmse: 0.73725 | val_1_rmse: 0.71843 |  0:00:10s
epoch 7  | loss: 0.40178 | val_0_rmse: 0.74337 | val_1_rmse: 0.72255 |  0:00:12s
epoch 8  | loss: 0.40528 | val_0_rmse: 0.70178 | val_1_rmse: 0.68514 |  0:00:13s
epoch 9  | loss: 0.3957  | val_0_rmse: 0.70347 | val_1_rmse: 0.68805 |  0:00:15s
epoch 10 | loss: 0.4012  | val_0_rmse: 0.67258 | val_1_rmse: 0.65926 |  0:00:16s
epoch 11 | loss: 0.37917 | val_0_rmse: 0.65678 | val_1_rmse: 0.64549 |  0:00:18s
epoch 12 | loss: 0.36856 | val_0_rmse: 0.6688  | val_1_rmse: 0.65505 |  0:00:20s
epoch 13 | loss: 0.36216 | val_0_rmse: 0.65742 | val_1_rmse: 0.65547 |  0:00:21s
epoch 14 | loss: 0.35849 | val_0_rmse: 0.63793 | val_1_rmse: 0.62832 |  0:00:23s
epoch 15 | loss: 0.3696  | val_0_rmse: 0.65238 | val_1_rmse: 0.6403  |  0:00:24s
epoch 16 | loss: 0.36924 | val_0_rmse: 0.65332 | val_1_rmse: 0.64622 |  0:00:26s
epoch 17 | loss: 0.35964 | val_0_rmse: 0.62144 | val_1_rmse: 0.61393 |  0:00:27s
epoch 18 | loss: 0.35668 | val_0_rmse: 0.62757 | val_1_rmse: 0.62161 |  0:00:29s
epoch 19 | loss: 0.35915 | val_0_rmse: 0.63008 | val_1_rmse: 0.62064 |  0:00:30s
epoch 20 | loss: 0.35263 | val_0_rmse: 0.6292  | val_1_rmse: 0.6171  |  0:00:32s
epoch 21 | loss: 0.34792 | val_0_rmse: 0.61904 | val_1_rmse: 0.61056 |  0:00:33s
epoch 22 | loss: 0.34044 | val_0_rmse: 0.61097 | val_1_rmse: 0.60395 |  0:00:35s
epoch 23 | loss: 0.33747 | val_0_rmse: 0.59976 | val_1_rmse: 0.59359 |  0:00:36s
epoch 24 | loss: 0.33614 | val_0_rmse: 0.60499 | val_1_rmse: 0.60023 |  0:00:38s
epoch 25 | loss: 0.33571 | val_0_rmse: 0.58459 | val_1_rmse: 0.58073 |  0:00:39s
epoch 26 | loss: 0.33354 | val_0_rmse: 0.58683 | val_1_rmse: 0.58018 |  0:00:41s
epoch 27 | loss: 0.32931 | val_0_rmse: 0.58063 | val_1_rmse: 0.57303 |  0:00:43s
epoch 28 | loss: 0.32997 | val_0_rmse: 0.58194 | val_1_rmse: 0.5727  |  0:00:44s
epoch 29 | loss: 0.33718 | val_0_rmse: 0.57657 | val_1_rmse: 0.57426 |  0:00:46s
epoch 30 | loss: 0.34051 | val_0_rmse: 0.58435 | val_1_rmse: 0.57912 |  0:00:47s
epoch 31 | loss: 0.33827 | val_0_rmse: 0.57874 | val_1_rmse: 0.57498 |  0:00:49s
epoch 32 | loss: 0.33721 | val_0_rmse: 0.56888 | val_1_rmse: 0.56914 |  0:00:50s
epoch 33 | loss: 0.32692 | val_0_rmse: 0.57247 | val_1_rmse: 0.57306 |  0:00:52s
epoch 34 | loss: 0.32789 | val_0_rmse: 0.56543 | val_1_rmse: 0.57088 |  0:00:53s
epoch 35 | loss: 0.33585 | val_0_rmse: 0.57202 | val_1_rmse: 0.57378 |  0:00:55s
epoch 36 | loss: 0.33155 | val_0_rmse: 0.56451 | val_1_rmse: 0.56821 |  0:00:56s
epoch 37 | loss: 0.33096 | val_0_rmse: 0.57832 | val_1_rmse: 0.57816 |  0:00:58s
epoch 38 | loss: 0.32796 | val_0_rmse: 0.56428 | val_1_rmse: 0.5691  |  0:00:59s
epoch 39 | loss: 0.32349 | val_0_rmse: 0.55865 | val_1_rmse: 0.56615 |  0:01:01s
epoch 40 | loss: 0.32953 | val_0_rmse: 0.56048 | val_1_rmse: 0.56928 |  0:01:03s
epoch 41 | loss: 0.32472 | val_0_rmse: 0.55291 | val_1_rmse: 0.56284 |  0:01:04s
epoch 42 | loss: 0.31927 | val_0_rmse: 0.55322 | val_1_rmse: 0.56351 |  0:01:06s
epoch 43 | loss: 0.32462 | val_0_rmse: 0.5745  | val_1_rmse: 0.57634 |  0:01:07s
epoch 44 | loss: 0.33534 | val_0_rmse: 0.57299 | val_1_rmse: 0.58153 |  0:01:09s
epoch 45 | loss: 0.34173 | val_0_rmse: 0.57246 | val_1_rmse: 0.5805  |  0:01:10s
epoch 46 | loss: 0.33118 | val_0_rmse: 0.55702 | val_1_rmse: 0.5665  |  0:01:12s
epoch 47 | loss: 0.328   | val_0_rmse: 0.56704 | val_1_rmse: 0.57786 |  0:01:13s
epoch 48 | loss: 0.32461 | val_0_rmse: 0.55256 | val_1_rmse: 0.56887 |  0:01:15s
epoch 49 | loss: 0.31884 | val_0_rmse: 0.55416 | val_1_rmse: 0.56866 |  0:01:16s
epoch 50 | loss: 0.31921 | val_0_rmse: 0.55334 | val_1_rmse: 0.56777 |  0:01:18s
epoch 51 | loss: 0.31147 | val_0_rmse: 0.54744 | val_1_rmse: 0.56276 |  0:01:20s
epoch 52 | loss: 0.31094 | val_0_rmse: 0.54593 | val_1_rmse: 0.56213 |  0:01:21s
epoch 53 | loss: 0.31988 | val_0_rmse: 0.56774 | val_1_rmse: 0.59283 |  0:01:23s
epoch 54 | loss: 0.32347 | val_0_rmse: 0.56587 | val_1_rmse: 0.58301 |  0:01:24s
epoch 55 | loss: 0.32113 | val_0_rmse: 0.54957 | val_1_rmse: 0.56741 |  0:01:26s
epoch 56 | loss: 0.31441 | val_0_rmse: 0.549   | val_1_rmse: 0.56749 |  0:01:27s
epoch 57 | loss: 0.31233 | val_0_rmse: 0.54922 | val_1_rmse: 0.56647 |  0:01:29s
epoch 58 | loss: 0.31288 | val_0_rmse: 0.54814 | val_1_rmse: 0.57083 |  0:01:30s
epoch 59 | loss: 0.31002 | val_0_rmse: 0.54465 | val_1_rmse: 0.5682  |  0:01:32s
epoch 60 | loss: 0.30449 | val_0_rmse: 0.5434  | val_1_rmse: 0.56273 |  0:01:33s
epoch 61 | loss: 0.30348 | val_0_rmse: 0.54074 | val_1_rmse: 0.56388 |  0:01:35s
epoch 62 | loss: 0.30184 | val_0_rmse: 0.54195 | val_1_rmse: 0.56968 |  0:01:36s
epoch 63 | loss: 0.30587 | val_0_rmse: 0.54408 | val_1_rmse: 0.57458 |  0:01:38s
epoch 64 | loss: 0.30393 | val_0_rmse: 0.54284 | val_1_rmse: 0.56933 |  0:01:39s
epoch 65 | loss: 0.30127 | val_0_rmse: 0.54488 | val_1_rmse: 0.56808 |  0:01:41s
epoch 66 | loss: 0.30454 | val_0_rmse: 0.54753 | val_1_rmse: 0.56555 |  0:01:43s
epoch 67 | loss: 0.30422 | val_0_rmse: 0.53833 | val_1_rmse: 0.56403 |  0:01:44s
epoch 68 | loss: 0.30214 | val_0_rmse: 0.53876 | val_1_rmse: 0.56513 |  0:01:46s
epoch 69 | loss: 0.30509 | val_0_rmse: 0.55509 | val_1_rmse: 0.57993 |  0:01:47s
epoch 70 | loss: 0.32743 | val_0_rmse: 0.60866 | val_1_rmse: 0.63537 |  0:01:49s
epoch 71 | loss: 0.32279 | val_0_rmse: 0.55736 | val_1_rmse: 0.57768 |  0:01:50s
epoch 72 | loss: 0.31045 | val_0_rmse: 0.54524 | val_1_rmse: 0.57504 |  0:01:52s
epoch 73 | loss: 0.30556 | val_0_rmse: 0.54729 | val_1_rmse: 0.57506 |  0:01:53s
epoch 74 | loss: 0.30328 | val_0_rmse: 0.54078 | val_1_rmse: 0.5775  |  0:01:55s
epoch 75 | loss: 0.30261 | val_0_rmse: 0.54323 | val_1_rmse: 0.56519 |  0:01:56s
epoch 76 | loss: 0.30288 | val_0_rmse: 0.53871 | val_1_rmse: 0.57105 |  0:01:58s
epoch 77 | loss: 0.29898 | val_0_rmse: 0.53691 | val_1_rmse: 0.56687 |  0:01:59s
epoch 78 | loss: 0.29683 | val_0_rmse: 0.54043 | val_1_rmse: 0.57422 |  0:02:01s
epoch 79 | loss: 0.29736 | val_0_rmse: 0.54176 | val_1_rmse: 0.56746 |  0:02:02s
epoch 80 | loss: 0.29524 | val_0_rmse: 0.53518 | val_1_rmse: 0.56945 |  0:02:04s
epoch 81 | loss: 0.29623 | val_0_rmse: 0.53225 | val_1_rmse: 0.55926 |  0:02:06s
epoch 82 | loss: 0.29452 | val_0_rmse: 0.53226 | val_1_rmse: 0.56641 |  0:02:07s
epoch 83 | loss: 0.28958 | val_0_rmse: 0.53105 | val_1_rmse: 0.56095 |  0:02:09s
epoch 84 | loss: 0.29417 | val_0_rmse: 0.53605 | val_1_rmse: 0.56714 |  0:02:10s
epoch 85 | loss: 0.29627 | val_0_rmse: 0.53036 | val_1_rmse: 0.56586 |  0:02:12s
epoch 86 | loss: 0.29217 | val_0_rmse: 0.52894 | val_1_rmse: 0.56148 |  0:02:13s
epoch 87 | loss: 0.28715 | val_0_rmse: 0.52923 | val_1_rmse: 0.56266 |  0:02:15s
epoch 88 | loss: 0.28776 | val_0_rmse: 0.52801 | val_1_rmse: 0.56622 |  0:02:16s
epoch 89 | loss: 0.29104 | val_0_rmse: 0.53581 | val_1_rmse: 0.57503 |  0:02:18s
epoch 90 | loss: 0.28738 | val_0_rmse: 0.52544 | val_1_rmse: 0.56473 |  0:02:19s
epoch 91 | loss: 0.28557 | val_0_rmse: 0.52564 | val_1_rmse: 0.56442 |  0:02:21s
epoch 92 | loss: 0.28473 | val_0_rmse: 0.52355 | val_1_rmse: 0.56433 |  0:02:22s
epoch 93 | loss: 0.28508 | val_0_rmse: 0.52342 | val_1_rmse: 0.56386 |  0:02:24s
epoch 94 | loss: 0.28244 | val_0_rmse: 0.52127 | val_1_rmse: 0.56308 |  0:02:26s
epoch 95 | loss: 0.28199 | val_0_rmse: 0.52629 | val_1_rmse: 0.5658  |  0:02:27s
epoch 96 | loss: 0.28413 | val_0_rmse: 0.52186 | val_1_rmse: 0.56519 |  0:02:29s
epoch 97 | loss: 0.28503 | val_0_rmse: 0.52616 | val_1_rmse: 0.56695 |  0:02:30s
epoch 98 | loss: 0.28531 | val_0_rmse: 0.52758 | val_1_rmse: 0.57296 |  0:02:32s
epoch 99 | loss: 0.28153 | val_0_rmse: 0.5229  | val_1_rmse: 0.563   |  0:02:33s
epoch 100| loss: 0.28278 | val_0_rmse: 0.52762 | val_1_rmse: 0.56706 |  0:02:35s
epoch 101| loss: 0.27883 | val_0_rmse: 0.52545 | val_1_rmse: 0.5754  |  0:02:36s
epoch 102| loss: 0.28605 | val_0_rmse: 0.52306 | val_1_rmse: 0.5666  |  0:02:38s
epoch 103| loss: 0.28484 | val_0_rmse: 0.53286 | val_1_rmse: 0.59989 |  0:02:39s
epoch 104| loss: 0.27957 | val_0_rmse: 0.51885 | val_1_rmse: 0.56741 |  0:02:41s
epoch 105| loss: 0.27808 | val_0_rmse: 0.52206 | val_1_rmse: 0.56982 |  0:02:42s
epoch 106| loss: 0.28022 | val_0_rmse: 0.51638 | val_1_rmse: 0.56692 |  0:02:44s
epoch 107| loss: 0.27927 | val_0_rmse: 0.51692 | val_1_rmse: 0.56556 |  0:02:46s
epoch 108| loss: 0.27694 | val_0_rmse: 0.56759 | val_1_rmse: 0.61618 |  0:02:47s
epoch 109| loss: 0.27629 | val_0_rmse: 0.51874 | val_1_rmse: 0.5641  |  0:02:49s
epoch 110| loss: 0.27508 | val_0_rmse: 0.51775 | val_1_rmse: 0.57174 |  0:02:50s
epoch 111| loss: 0.27676 | val_0_rmse: 0.51957 | val_1_rmse: 0.57102 |  0:02:52s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.55926
Best weights from best epoch are automatically used!
ended training at: 07:17:15
Feature importance:
Mean squared error is of 1405582731.893909
Mean absolute error:26887.37512412242
MAPE:0.2783462493075708
R2 score:0.6796787725804194
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:17:16
epoch 0  | loss: 1.62801 | val_0_rmse: 0.99801 | val_1_rmse: 0.99084 |  0:00:01s
epoch 1  | loss: 0.88331 | val_0_rmse: 0.95905 | val_1_rmse: 0.94965 |  0:00:03s
epoch 2  | loss: 0.67566 | val_0_rmse: 0.85641 | val_1_rmse: 0.85093 |  0:00:04s
epoch 3  | loss: 0.52115 | val_0_rmse: 0.89208 | val_1_rmse: 0.88486 |  0:00:06s
epoch 4  | loss: 0.45885 | val_0_rmse: 0.82386 | val_1_rmse: 0.82094 |  0:00:07s
epoch 5  | loss: 0.43965 | val_0_rmse: 0.79282 | val_1_rmse: 0.79558 |  0:00:09s
epoch 6  | loss: 0.44027 | val_0_rmse: 0.75707 | val_1_rmse: 0.76229 |  0:00:10s
epoch 7  | loss: 0.40719 | val_0_rmse: 0.73059 | val_1_rmse: 0.73701 |  0:00:12s
epoch 8  | loss: 0.39035 | val_0_rmse: 0.74154 | val_1_rmse: 0.74053 |  0:00:13s
epoch 9  | loss: 0.39993 | val_0_rmse: 0.67473 | val_1_rmse: 0.67461 |  0:00:15s
epoch 10 | loss: 0.39231 | val_0_rmse: 0.69592 | val_1_rmse: 0.69531 |  0:00:16s
epoch 11 | loss: 0.38251 | val_0_rmse: 0.68186 | val_1_rmse: 0.68103 |  0:00:18s
epoch 12 | loss: 0.38868 | val_0_rmse: 0.64742 | val_1_rmse: 0.65465 |  0:00:19s
epoch 13 | loss: 0.38975 | val_0_rmse: 0.65569 | val_1_rmse: 0.66619 |  0:00:21s
epoch 14 | loss: 0.37818 | val_0_rmse: 0.65536 | val_1_rmse: 0.66583 |  0:00:23s
epoch 15 | loss: 0.38172 | val_0_rmse: 0.66288 | val_1_rmse: 0.66996 |  0:00:24s
epoch 16 | loss: 0.37601 | val_0_rmse: 0.64906 | val_1_rmse: 0.65832 |  0:00:26s
epoch 17 | loss: 0.3653  | val_0_rmse: 0.6298  | val_1_rmse: 0.64211 |  0:00:27s
epoch 18 | loss: 0.36047 | val_0_rmse: 0.6242  | val_1_rmse: 0.63803 |  0:00:29s
epoch 19 | loss: 0.36452 | val_0_rmse: 0.61953 | val_1_rmse: 0.63042 |  0:00:30s
epoch 20 | loss: 0.35463 | val_0_rmse: 0.62495 | val_1_rmse: 0.63584 |  0:00:32s
epoch 21 | loss: 0.35922 | val_0_rmse: 0.62347 | val_1_rmse: 0.63548 |  0:00:33s
epoch 22 | loss: 0.35105 | val_0_rmse: 0.61477 | val_1_rmse: 0.62855 |  0:00:35s
epoch 23 | loss: 0.35516 | val_0_rmse: 0.61628 | val_1_rmse: 0.62539 |  0:00:36s
epoch 24 | loss: 0.36466 | val_0_rmse: 0.60377 | val_1_rmse: 0.62628 |  0:00:38s
epoch 25 | loss: 0.36109 | val_0_rmse: 0.5938  | val_1_rmse: 0.61215 |  0:00:40s
epoch 26 | loss: 0.35399 | val_0_rmse: 0.59318 | val_1_rmse: 0.61145 |  0:00:41s
epoch 27 | loss: 0.34951 | val_0_rmse: 0.58417 | val_1_rmse: 0.60297 |  0:00:43s
epoch 28 | loss: 0.34999 | val_0_rmse: 0.58502 | val_1_rmse: 0.60161 |  0:00:44s
epoch 29 | loss: 0.34833 | val_0_rmse: 0.59059 | val_1_rmse: 0.60057 |  0:00:46s
epoch 30 | loss: 0.35028 | val_0_rmse: 0.57424 | val_1_rmse: 0.59398 |  0:00:47s
epoch 31 | loss: 0.34935 | val_0_rmse: 0.63842 | val_1_rmse: 0.65914 |  0:00:49s
epoch 32 | loss: 0.37076 | val_0_rmse: 0.60042 | val_1_rmse: 0.61893 |  0:00:50s
epoch 33 | loss: 0.36218 | val_0_rmse: 0.70089 | val_1_rmse: 0.69676 |  0:00:52s
epoch 34 | loss: 0.37475 | val_0_rmse: 0.5977  | val_1_rmse: 0.62205 |  0:00:53s
epoch 35 | loss: 0.36695 | val_0_rmse: 0.61116 | val_1_rmse: 0.6166  |  0:00:55s
epoch 36 | loss: 0.36056 | val_0_rmse: 0.58949 | val_1_rmse: 0.60219 |  0:00:56s
epoch 37 | loss: 0.35638 | val_0_rmse: 0.58775 | val_1_rmse: 0.60511 |  0:00:58s
epoch 38 | loss: 0.36027 | val_0_rmse: 0.59728 | val_1_rmse: 0.61413 |  0:01:00s
epoch 39 | loss: 0.35782 | val_0_rmse: 0.59286 | val_1_rmse: 0.60628 |  0:01:01s
epoch 40 | loss: 0.3567  | val_0_rmse: 0.58283 | val_1_rmse: 0.60256 |  0:01:03s
epoch 41 | loss: 0.34875 | val_0_rmse: 0.57963 | val_1_rmse: 0.5989  |  0:01:04s
epoch 42 | loss: 0.34555 | val_0_rmse: 0.5736  | val_1_rmse: 0.59148 |  0:01:06s
epoch 43 | loss: 0.34017 | val_0_rmse: 0.57051 | val_1_rmse: 0.58895 |  0:01:07s
epoch 44 | loss: 0.34283 | val_0_rmse: 0.5911  | val_1_rmse: 0.60905 |  0:01:09s
epoch 45 | loss: 0.35876 | val_0_rmse: 0.58996 | val_1_rmse: 0.61164 |  0:01:10s
epoch 46 | loss: 0.35606 | val_0_rmse: 0.57713 | val_1_rmse: 0.5988  |  0:01:12s
epoch 47 | loss: 0.34447 | val_0_rmse: 0.57087 | val_1_rmse: 0.59158 |  0:01:13s
epoch 48 | loss: 0.33844 | val_0_rmse: 0.57121 | val_1_rmse: 0.59355 |  0:01:15s
epoch 49 | loss: 0.33681 | val_0_rmse: 0.56577 | val_1_rmse: 0.59216 |  0:01:17s
epoch 50 | loss: 0.33584 | val_0_rmse: 0.57265 | val_1_rmse: 0.59726 |  0:01:18s
epoch 51 | loss: 0.35294 | val_0_rmse: 0.58308 | val_1_rmse: 0.60907 |  0:01:20s
epoch 52 | loss: 0.34591 | val_0_rmse: 0.6328  | val_1_rmse: 0.64642 |  0:01:21s
epoch 53 | loss: 0.38822 | val_0_rmse: 0.60495 | val_1_rmse: 0.62966 |  0:01:23s
epoch 54 | loss: 0.36131 | val_0_rmse: 0.60349 | val_1_rmse: 0.63398 |  0:01:24s
epoch 55 | loss: 0.36373 | val_0_rmse: 0.59758 | val_1_rmse: 0.61805 |  0:01:26s
epoch 56 | loss: 0.36169 | val_0_rmse: 0.59417 | val_1_rmse: 0.61937 |  0:01:27s
epoch 57 | loss: 0.36806 | val_0_rmse: 0.6009  | val_1_rmse: 0.62886 |  0:01:29s
epoch 58 | loss: 0.3595  | val_0_rmse: 0.59166 | val_1_rmse: 0.6167  |  0:01:30s
epoch 59 | loss: 0.36368 | val_0_rmse: 0.59687 | val_1_rmse: 0.61656 |  0:01:32s
epoch 60 | loss: 0.36469 | val_0_rmse: 0.59779 | val_1_rmse: 0.621   |  0:01:33s
epoch 61 | loss: 0.37375 | val_0_rmse: 0.60567 | val_1_rmse: 0.62341 |  0:01:35s
epoch 62 | loss: 0.37872 | val_0_rmse: 0.60261 | val_1_rmse: 0.63043 |  0:01:37s
epoch 63 | loss: 0.36782 | val_0_rmse: 0.62304 | val_1_rmse: 0.6385  |  0:01:38s
epoch 64 | loss: 0.38747 | val_0_rmse: 0.60156 | val_1_rmse: 0.61894 |  0:01:40s
epoch 65 | loss: 0.36216 | val_0_rmse: 0.61781 | val_1_rmse: 0.63805 |  0:01:41s
epoch 66 | loss: 0.35514 | val_0_rmse: 0.58566 | val_1_rmse: 0.60176 |  0:01:43s
epoch 67 | loss: 0.34823 | val_0_rmse: 0.57769 | val_1_rmse: 0.59019 |  0:01:44s
epoch 68 | loss: 0.34156 | val_0_rmse: 0.57382 | val_1_rmse: 0.59262 |  0:01:46s
epoch 69 | loss: 0.33718 | val_0_rmse: 0.57281 | val_1_rmse: 0.59309 |  0:01:47s
epoch 70 | loss: 0.3339  | val_0_rmse: 0.56851 | val_1_rmse: 0.59029 |  0:01:49s
epoch 71 | loss: 0.33047 | val_0_rmse: 0.58156 | val_1_rmse: 0.60067 |  0:01:50s
epoch 72 | loss: 0.33319 | val_0_rmse: 0.56439 | val_1_rmse: 0.58642 |  0:01:52s
epoch 73 | loss: 0.33115 | val_0_rmse: 0.5694  | val_1_rmse: 0.59136 |  0:01:53s
epoch 74 | loss: 0.33619 | val_0_rmse: 0.5744  | val_1_rmse: 0.59403 |  0:01:55s
epoch 75 | loss: 0.33284 | val_0_rmse: 0.56411 | val_1_rmse: 0.58809 |  0:01:57s
epoch 76 | loss: 0.3291  | val_0_rmse: 0.56309 | val_1_rmse: 0.58736 |  0:01:58s
epoch 77 | loss: 0.32445 | val_0_rmse: 0.55918 | val_1_rmse: 0.58731 |  0:02:00s
epoch 78 | loss: 0.32662 | val_0_rmse: 0.56105 | val_1_rmse: 0.58854 |  0:02:01s
epoch 79 | loss: 0.32454 | val_0_rmse: 0.56032 | val_1_rmse: 0.58949 |  0:02:03s
epoch 80 | loss: 0.32197 | val_0_rmse: 0.5602  | val_1_rmse: 0.59216 |  0:02:04s
epoch 81 | loss: 0.32275 | val_0_rmse: 0.56509 | val_1_rmse: 0.59046 |  0:02:06s
epoch 82 | loss: 0.32631 | val_0_rmse: 0.56339 | val_1_rmse: 0.5919  |  0:02:07s
epoch 83 | loss: 0.32629 | val_0_rmse: 0.55807 | val_1_rmse: 0.58396 |  0:02:09s
epoch 84 | loss: 0.32588 | val_0_rmse: 0.56143 | val_1_rmse: 0.58637 |  0:02:10s
epoch 85 | loss: 0.31482 | val_0_rmse: 0.55899 | val_1_rmse: 0.59083 |  0:02:12s
epoch 86 | loss: 0.31685 | val_0_rmse: 0.55318 | val_1_rmse: 0.58736 |  0:02:13s
epoch 87 | loss: 0.3128  | val_0_rmse: 0.55026 | val_1_rmse: 0.58064 |  0:02:15s
epoch 88 | loss: 0.31118 | val_0_rmse: 0.54915 | val_1_rmse: 0.58144 |  0:02:16s
epoch 89 | loss: 0.31126 | val_0_rmse: 0.54766 | val_1_rmse: 0.58238 |  0:02:18s
epoch 90 | loss: 0.3121  | val_0_rmse: 0.55391 | val_1_rmse: 0.58288 |  0:02:20s
epoch 91 | loss: 0.30992 | val_0_rmse: 0.55318 | val_1_rmse: 0.58596 |  0:02:21s
epoch 92 | loss: 0.31624 | val_0_rmse: 0.57276 | val_1_rmse: 0.58825 |  0:02:23s
epoch 93 | loss: 0.30937 | val_0_rmse: 0.57105 | val_1_rmse: 0.58226 |  0:02:24s
epoch 94 | loss: 0.30793 | val_0_rmse: 0.54689 | val_1_rmse: 0.58106 |  0:02:26s
epoch 95 | loss: 0.30639 | val_0_rmse: 0.54689 | val_1_rmse: 0.57745 |  0:02:27s
epoch 96 | loss: 0.30937 | val_0_rmse: 0.57501 | val_1_rmse: 0.57689 |  0:02:29s
epoch 97 | loss: 0.30563 | val_0_rmse: 0.56102 | val_1_rmse: 0.57673 |  0:02:30s
epoch 98 | loss: 0.32097 | val_0_rmse: 0.57423 | val_1_rmse: 0.59573 |  0:02:32s
epoch 99 | loss: 0.32487 | val_0_rmse: 0.55792 | val_1_rmse: 0.58939 |  0:02:33s
epoch 100| loss: 0.31733 | val_0_rmse: 0.5704  | val_1_rmse: 0.59768 |  0:02:35s
epoch 101| loss: 0.32272 | val_0_rmse: 0.56198 | val_1_rmse: 0.59423 |  0:02:36s
epoch 102| loss: 0.31983 | val_0_rmse: 0.55849 | val_1_rmse: 0.58564 |  0:02:38s
epoch 103| loss: 0.32465 | val_0_rmse: 0.55923 | val_1_rmse: 0.59018 |  0:02:39s
epoch 104| loss: 0.32769 | val_0_rmse: 0.56157 | val_1_rmse: 0.58953 |  0:02:41s
epoch 105| loss: 0.3222  | val_0_rmse: 0.55655 | val_1_rmse: 0.58267 |  0:02:43s
epoch 106| loss: 0.31782 | val_0_rmse: 0.55797 | val_1_rmse: 0.58515 |  0:02:44s
epoch 107| loss: 0.32828 | val_0_rmse: 0.56571 | val_1_rmse: 0.59017 |  0:02:46s
epoch 108| loss: 0.3169  | val_0_rmse: 0.65587 | val_1_rmse: 0.68463 |  0:02:47s
epoch 109| loss: 0.31191 | val_0_rmse: 0.553   | val_1_rmse: 0.58121 |  0:02:49s
epoch 110| loss: 0.31206 | val_0_rmse: 0.56147 | val_1_rmse: 0.58751 |  0:02:50s
epoch 111| loss: 0.30883 | val_0_rmse: 0.5527  | val_1_rmse: 0.5815  |  0:02:52s
epoch 112| loss: 0.31478 | val_0_rmse: 0.54847 | val_1_rmse: 0.58218 |  0:02:53s
epoch 113| loss: 0.3097  | val_0_rmse: 0.55297 | val_1_rmse: 0.58782 |  0:02:55s
epoch 114| loss: 0.31066 | val_0_rmse: 0.54982 | val_1_rmse: 0.58113 |  0:02:56s
epoch 115| loss: 0.30857 | val_0_rmse: 0.54452 | val_1_rmse: 0.5754  |  0:02:58s
epoch 116| loss: 0.30442 | val_0_rmse: 0.54257 | val_1_rmse: 0.57887 |  0:02:59s
epoch 117| loss: 0.30476 | val_0_rmse: 0.54688 | val_1_rmse: 0.5763  |  0:03:01s
epoch 118| loss: 0.30376 | val_0_rmse: 0.54251 | val_1_rmse: 0.57702 |  0:03:03s
epoch 119| loss: 0.30286 | val_0_rmse: 0.54314 | val_1_rmse: 0.58612 |  0:03:04s
epoch 120| loss: 0.30311 | val_0_rmse: 0.54281 | val_1_rmse: 0.58225 |  0:03:06s
epoch 121| loss: 0.30163 | val_0_rmse: 0.5457  | val_1_rmse: 0.58462 |  0:03:07s
epoch 122| loss: 0.30134 | val_0_rmse: 0.5407  | val_1_rmse: 0.58064 |  0:03:09s
epoch 123| loss: 0.30149 | val_0_rmse: 0.5398  | val_1_rmse: 0.57978 |  0:03:10s
epoch 124| loss: 0.30033 | val_0_rmse: 0.54774 | val_1_rmse: 0.58182 |  0:03:12s
epoch 125| loss: 0.30279 | val_0_rmse: 0.54248 | val_1_rmse: 0.58174 |  0:03:13s
epoch 126| loss: 0.30533 | val_0_rmse: 0.54806 | val_1_rmse: 0.59107 |  0:03:15s
epoch 127| loss: 0.3078  | val_0_rmse: 0.54747 | val_1_rmse: 0.58798 |  0:03:16s
epoch 128| loss: 0.30477 | val_0_rmse: 0.54738 | val_1_rmse: 0.59152 |  0:03:18s
epoch 129| loss: 0.31245 | val_0_rmse: 0.58326 | val_1_rmse: 0.61481 |  0:03:19s
epoch 130| loss: 0.31442 | val_0_rmse: 0.55155 | val_1_rmse: 0.60169 |  0:03:21s
epoch 131| loss: 0.30584 | val_0_rmse: 0.54599 | val_1_rmse: 0.59239 |  0:03:23s
epoch 132| loss: 0.30664 | val_0_rmse: 0.54882 | val_1_rmse: 0.58766 |  0:03:24s
epoch 133| loss: 0.30804 | val_0_rmse: 0.54524 | val_1_rmse: 0.58815 |  0:03:26s
epoch 134| loss: 0.3201  | val_0_rmse: 0.6008  | val_1_rmse: 0.62814 |  0:03:27s
epoch 135| loss: 0.33749 | val_0_rmse: 0.56794 | val_1_rmse: 0.60089 |  0:03:29s
epoch 136| loss: 0.33633 | val_0_rmse: 0.567   | val_1_rmse: 0.59753 |  0:03:30s
epoch 137| loss: 0.32634 | val_0_rmse: 0.57113 | val_1_rmse: 0.5991  |  0:03:32s
epoch 138| loss: 0.32276 | val_0_rmse: 0.5667  | val_1_rmse: 0.60143 |  0:03:33s
epoch 139| loss: 0.34567 | val_0_rmse: 0.57928 | val_1_rmse: 0.61432 |  0:03:35s
epoch 140| loss: 0.33531 | val_0_rmse: 0.56795 | val_1_rmse: 0.59929 |  0:03:36s
epoch 141| loss: 0.32943 | val_0_rmse: 0.58453 | val_1_rmse: 0.62189 |  0:03:38s
epoch 142| loss: 0.33484 | val_0_rmse: 0.59202 | val_1_rmse: 0.63018 |  0:03:39s
epoch 143| loss: 0.33069 | val_0_rmse: 0.56385 | val_1_rmse: 0.59307 |  0:03:41s
epoch 144| loss: 0.32034 | val_0_rmse: 0.55383 | val_1_rmse: 0.58714 |  0:03:43s
epoch 145| loss: 0.31332 | val_0_rmse: 0.55059 | val_1_rmse: 0.58448 |  0:03:44s

Early stopping occured at epoch 145 with best_epoch = 115 and best_val_1_rmse = 0.5754
Best weights from best epoch are automatically used!
ended training at: 07:21:01
Feature importance:
Mean squared error is of 1451912540.3722389
Mean absolute error:26862.931240247493
MAPE:0.2746192737604509
R2 score:0.664335105463112
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:21:01
epoch 0  | loss: 0.71565 | val_0_rmse: 0.72596 | val_1_rmse: 0.73401 |  0:00:03s
epoch 1  | loss: 0.45879 | val_0_rmse: 0.72783 | val_1_rmse: 0.73916 |  0:00:06s
epoch 2  | loss: 0.39485 | val_0_rmse: 0.67652 | val_1_rmse: 0.6884  |  0:00:09s
epoch 3  | loss: 0.38589 | val_0_rmse: 0.66926 | val_1_rmse: 0.68011 |  0:00:12s
epoch 4  | loss: 0.36537 | val_0_rmse: 0.64348 | val_1_rmse: 0.64935 |  0:00:15s
epoch 5  | loss: 0.36858 | val_0_rmse: 0.62656 | val_1_rmse: 0.63694 |  0:00:18s
epoch 6  | loss: 0.3588  | val_0_rmse: 0.60629 | val_1_rmse: 0.61015 |  0:00:22s
epoch 7  | loss: 0.35701 | val_0_rmse: 0.59579 | val_1_rmse: 0.60478 |  0:00:25s
epoch 8  | loss: 0.35275 | val_0_rmse: 0.59401 | val_1_rmse: 0.60161 |  0:00:28s
epoch 9  | loss: 0.34788 | val_0_rmse: 0.58435 | val_1_rmse: 0.58998 |  0:00:31s
epoch 10 | loss: 0.34049 | val_0_rmse: 0.575   | val_1_rmse: 0.58172 |  0:00:34s
epoch 11 | loss: 0.33669 | val_0_rmse: 0.57509 | val_1_rmse: 0.58139 |  0:00:37s
epoch 12 | loss: 0.34084 | val_0_rmse: 0.58122 | val_1_rmse: 0.58465 |  0:00:40s
epoch 13 | loss: 0.33708 | val_0_rmse: 0.56663 | val_1_rmse: 0.57223 |  0:00:44s
epoch 14 | loss: 0.33116 | val_0_rmse: 0.57063 | val_1_rmse: 0.57452 |  0:00:47s
epoch 15 | loss: 0.33289 | val_0_rmse: 0.566   | val_1_rmse: 0.57308 |  0:00:50s
epoch 16 | loss: 0.32712 | val_0_rmse: 0.56363 | val_1_rmse: 0.56934 |  0:00:53s
epoch 17 | loss: 0.32632 | val_0_rmse: 0.56716 | val_1_rmse: 0.5767  |  0:00:56s
epoch 18 | loss: 0.32882 | val_0_rmse: 0.56389 | val_1_rmse: 0.57038 |  0:00:59s
epoch 19 | loss: 0.32487 | val_0_rmse: 0.55732 | val_1_rmse: 0.56807 |  0:01:03s
epoch 20 | loss: 0.32533 | val_0_rmse: 0.56604 | val_1_rmse: 0.57581 |  0:01:06s
epoch 21 | loss: 0.3217  | val_0_rmse: 0.55813 | val_1_rmse: 0.56817 |  0:01:09s
epoch 22 | loss: 0.32552 | val_0_rmse: 0.56922 | val_1_rmse: 0.57845 |  0:01:12s
epoch 23 | loss: 0.32498 | val_0_rmse: 0.56489 | val_1_rmse: 0.57354 |  0:01:15s
epoch 24 | loss: 0.32213 | val_0_rmse: 0.56141 | val_1_rmse: 0.57002 |  0:01:18s
epoch 25 | loss: 0.32263 | val_0_rmse: 0.56086 | val_1_rmse: 0.56948 |  0:01:21s
epoch 26 | loss: 0.32174 | val_0_rmse: 0.56206 | val_1_rmse: 0.5714  |  0:01:25s
epoch 27 | loss: 0.32132 | val_0_rmse: 0.55996 | val_1_rmse: 0.56982 |  0:01:28s
epoch 28 | loss: 0.31941 | val_0_rmse: 0.57357 | val_1_rmse: 0.58364 |  0:01:31s
epoch 29 | loss: 0.32179 | val_0_rmse: 0.55827 | val_1_rmse: 0.56869 |  0:01:34s
epoch 30 | loss: 0.31808 | val_0_rmse: 0.55544 | val_1_rmse: 0.56765 |  0:01:37s
epoch 31 | loss: 0.31712 | val_0_rmse: 0.55634 | val_1_rmse: 0.56896 |  0:01:40s
epoch 32 | loss: 0.31831 | val_0_rmse: 0.55421 | val_1_rmse: 0.56653 |  0:01:43s
epoch 33 | loss: 0.31676 | val_0_rmse: 0.55628 | val_1_rmse: 0.56769 |  0:01:47s
epoch 34 | loss: 0.31626 | val_0_rmse: 0.5573  | val_1_rmse: 0.56721 |  0:01:50s
epoch 35 | loss: 0.317   | val_0_rmse: 0.55258 | val_1_rmse: 0.56496 |  0:01:53s
epoch 36 | loss: 0.31806 | val_0_rmse: 0.55291 | val_1_rmse: 0.56184 |  0:01:56s
epoch 37 | loss: 0.31721 | val_0_rmse: 0.55249 | val_1_rmse: 0.56457 |  0:01:59s
epoch 38 | loss: 0.31281 | val_0_rmse: 0.55134 | val_1_rmse: 0.56223 |  0:02:02s
epoch 39 | loss: 0.31247 | val_0_rmse: 0.55152 | val_1_rmse: 0.56365 |  0:02:05s
epoch 40 | loss: 0.3127  | val_0_rmse: 0.55143 | val_1_rmse: 0.56443 |  0:02:09s
epoch 41 | loss: 0.31227 | val_0_rmse: 0.55171 | val_1_rmse: 0.56646 |  0:02:12s
epoch 42 | loss: 0.31147 | val_0_rmse: 0.55235 | val_1_rmse: 0.56685 |  0:02:15s
epoch 43 | loss: 0.31226 | val_0_rmse: 0.55178 | val_1_rmse: 0.56558 |  0:02:18s
epoch 44 | loss: 0.31179 | val_0_rmse: 0.55096 | val_1_rmse: 0.56375 |  0:02:21s
epoch 45 | loss: 0.30992 | val_0_rmse: 0.54761 | val_1_rmse: 0.56068 |  0:02:24s
epoch 46 | loss: 0.31041 | val_0_rmse: 0.55963 | val_1_rmse: 0.57396 |  0:02:27s
epoch 47 | loss: 0.30945 | val_0_rmse: 0.54605 | val_1_rmse: 0.55938 |  0:02:31s
epoch 48 | loss: 0.30829 | val_0_rmse: 0.55732 | val_1_rmse: 0.57081 |  0:02:34s
epoch 49 | loss: 0.31083 | val_0_rmse: 0.55191 | val_1_rmse: 0.56498 |  0:02:37s
epoch 50 | loss: 0.30878 | val_0_rmse: 0.55181 | val_1_rmse: 0.56447 |  0:02:40s
epoch 51 | loss: 0.31137 | val_0_rmse: 0.55074 | val_1_rmse: 0.56483 |  0:02:43s
epoch 52 | loss: 0.3104  | val_0_rmse: 0.54815 | val_1_rmse: 0.56265 |  0:02:46s
epoch 53 | loss: 0.30882 | val_0_rmse: 0.54528 | val_1_rmse: 0.55934 |  0:02:49s
epoch 54 | loss: 0.31036 | val_0_rmse: 0.54791 | val_1_rmse: 0.56244 |  0:02:53s
epoch 55 | loss: 0.30588 | val_0_rmse: 0.54568 | val_1_rmse: 0.56061 |  0:02:56s
epoch 56 | loss: 0.30795 | val_0_rmse: 0.5459  | val_1_rmse: 0.56014 |  0:02:59s
epoch 57 | loss: 0.3058  | val_0_rmse: 0.5431  | val_1_rmse: 0.56009 |  0:03:02s
epoch 58 | loss: 0.3072  | val_0_rmse: 0.55126 | val_1_rmse: 0.56828 |  0:03:05s
epoch 59 | loss: 0.30671 | val_0_rmse: 0.54242 | val_1_rmse: 0.56057 |  0:03:08s
epoch 60 | loss: 0.30659 | val_0_rmse: 0.54701 | val_1_rmse: 0.56405 |  0:03:11s
epoch 61 | loss: 0.30399 | val_0_rmse: 0.54377 | val_1_rmse: 0.56107 |  0:03:15s
epoch 62 | loss: 0.31085 | val_0_rmse: 0.5608  | val_1_rmse: 0.57782 |  0:03:18s
epoch 63 | loss: 0.31143 | val_0_rmse: 0.55136 | val_1_rmse: 0.56791 |  0:03:21s
epoch 64 | loss: 0.3046  | val_0_rmse: 0.54717 | val_1_rmse: 0.5656  |  0:03:24s
epoch 65 | loss: 0.30455 | val_0_rmse: 0.5453  | val_1_rmse: 0.56179 |  0:03:27s
epoch 66 | loss: 0.30431 | val_0_rmse: 0.5472  | val_1_rmse: 0.56675 |  0:03:30s
epoch 67 | loss: 0.3081  | val_0_rmse: 0.5515  | val_1_rmse: 0.56921 |  0:03:34s
epoch 68 | loss: 0.30632 | val_0_rmse: 0.55322 | val_1_rmse: 0.56653 |  0:03:37s
epoch 69 | loss: 0.30353 | val_0_rmse: 0.54145 | val_1_rmse: 0.55999 |  0:03:40s
epoch 70 | loss: 0.30371 | val_0_rmse: 0.54325 | val_1_rmse: 0.56161 |  0:03:43s
epoch 71 | loss: 0.30251 | val_0_rmse: 0.53994 | val_1_rmse: 0.56085 |  0:03:46s
epoch 72 | loss: 0.30326 | val_0_rmse: 0.54661 | val_1_rmse: 0.56618 |  0:03:49s
epoch 73 | loss: 0.30477 | val_0_rmse: 0.54587 | val_1_rmse: 0.56619 |  0:03:52s
epoch 74 | loss: 0.30419 | val_0_rmse: 0.53943 | val_1_rmse: 0.56236 |  0:03:56s
epoch 75 | loss: 0.30197 | val_0_rmse: 0.53881 | val_1_rmse: 0.56129 |  0:03:59s
epoch 76 | loss: 0.30183 | val_0_rmse: 0.53839 | val_1_rmse: 0.56179 |  0:04:02s
epoch 77 | loss: 0.30256 | val_0_rmse: 0.54015 | val_1_rmse: 0.563   |  0:04:05s
epoch 78 | loss: 0.30131 | val_0_rmse: 0.5387  | val_1_rmse: 0.56287 |  0:04:08s
epoch 79 | loss: 0.30223 | val_0_rmse: 0.54507 | val_1_rmse: 0.5687  |  0:04:11s
epoch 80 | loss: 0.3014  | val_0_rmse: 0.54126 | val_1_rmse: 0.5632  |  0:04:14s
epoch 81 | loss: 0.3071  | val_0_rmse: 0.55606 | val_1_rmse: 0.57846 |  0:04:18s
epoch 82 | loss: 0.31155 | val_0_rmse: 0.54258 | val_1_rmse: 0.56626 |  0:04:21s
epoch 83 | loss: 0.30558 | val_0_rmse: 0.54626 | val_1_rmse: 0.57125 |  0:04:24s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.55934
Best weights from best epoch are automatically used!
ended training at: 07:25:27
Feature importance:
Mean squared error is of 15791773138.332172
Mean absolute error:91349.82018876167
MAPE:0.26477640221505727
R2 score:0.6884792506208235
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:27
epoch 0  | loss: 0.75464 | val_0_rmse: 0.81321 | val_1_rmse: 0.81753 |  0:00:03s
epoch 1  | loss: 0.46394 | val_0_rmse: 0.71695 | val_1_rmse: 0.71867 |  0:00:06s
epoch 2  | loss: 0.41005 | val_0_rmse: 0.67811 | val_1_rmse: 0.6834  |  0:00:09s
epoch 3  | loss: 0.38527 | val_0_rmse: 0.65589 | val_1_rmse: 0.66232 |  0:00:12s
epoch 4  | loss: 0.36932 | val_0_rmse: 0.65235 | val_1_rmse: 0.65632 |  0:00:15s
epoch 5  | loss: 0.36158 | val_0_rmse: 0.63963 | val_1_rmse: 0.64723 |  0:00:18s
epoch 6  | loss: 0.35234 | val_0_rmse: 0.64849 | val_1_rmse: 0.65436 |  0:00:22s
epoch 7  | loss: 0.35343 | val_0_rmse: 0.59645 | val_1_rmse: 0.60357 |  0:00:25s
epoch 8  | loss: 0.34408 | val_0_rmse: 0.59466 | val_1_rmse: 0.60275 |  0:00:28s
epoch 9  | loss: 0.34243 | val_0_rmse: 0.57608 | val_1_rmse: 0.58479 |  0:00:31s
epoch 10 | loss: 0.34177 | val_0_rmse: 0.57651 | val_1_rmse: 0.5853  |  0:00:34s
epoch 11 | loss: 0.33804 | val_0_rmse: 0.56993 | val_1_rmse: 0.57767 |  0:00:37s
epoch 12 | loss: 0.33156 | val_0_rmse: 0.56859 | val_1_rmse: 0.57619 |  0:00:41s
epoch 13 | loss: 0.32969 | val_0_rmse: 0.56672 | val_1_rmse: 0.5758  |  0:00:44s
epoch 14 | loss: 0.33263 | val_0_rmse: 0.56566 | val_1_rmse: 0.57625 |  0:00:47s
epoch 15 | loss: 0.328   | val_0_rmse: 0.56129 | val_1_rmse: 0.57217 |  0:00:50s
epoch 16 | loss: 0.3306  | val_0_rmse: 0.56269 | val_1_rmse: 0.57502 |  0:00:53s
epoch 17 | loss: 0.32644 | val_0_rmse: 0.56008 | val_1_rmse: 0.57251 |  0:00:56s
epoch 18 | loss: 0.3242  | val_0_rmse: 0.55326 | val_1_rmse: 0.56406 |  0:00:59s
epoch 19 | loss: 0.32482 | val_0_rmse: 0.55341 | val_1_rmse: 0.56497 |  0:01:03s
epoch 20 | loss: 0.32607 | val_0_rmse: 0.56007 | val_1_rmse: 0.57128 |  0:01:06s
epoch 21 | loss: 0.32211 | val_0_rmse: 0.55811 | val_1_rmse: 0.57312 |  0:01:09s
epoch 22 | loss: 0.32087 | val_0_rmse: 0.55152 | val_1_rmse: 0.56496 |  0:01:12s
epoch 23 | loss: 0.32077 | val_0_rmse: 0.5548  | val_1_rmse: 0.56656 |  0:01:15s
epoch 24 | loss: 0.32023 | val_0_rmse: 0.56562 | val_1_rmse: 0.57988 |  0:01:18s
epoch 25 | loss: 0.31867 | val_0_rmse: 0.54938 | val_1_rmse: 0.56327 |  0:01:22s
epoch 26 | loss: 0.31594 | val_0_rmse: 0.54826 | val_1_rmse: 0.56546 |  0:01:25s
epoch 27 | loss: 0.31733 | val_0_rmse: 0.55032 | val_1_rmse: 0.56804 |  0:01:28s
epoch 28 | loss: 0.3151  | val_0_rmse: 0.55492 | val_1_rmse: 0.56871 |  0:01:31s
epoch 29 | loss: 0.31643 | val_0_rmse: 0.54858 | val_1_rmse: 0.5649  |  0:01:34s
epoch 30 | loss: 0.32058 | val_0_rmse: 0.55997 | val_1_rmse: 0.57111 |  0:01:37s
epoch 31 | loss: 0.31459 | val_0_rmse: 0.54749 | val_1_rmse: 0.56325 |  0:01:41s
epoch 32 | loss: 0.31268 | val_0_rmse: 0.54762 | val_1_rmse: 0.56386 |  0:01:44s
epoch 33 | loss: 0.31323 | val_0_rmse: 0.54741 | val_1_rmse: 0.56326 |  0:01:47s
epoch 34 | loss: 0.31473 | val_0_rmse: 0.54627 | val_1_rmse: 0.56098 |  0:01:50s
epoch 35 | loss: 0.31395 | val_0_rmse: 0.54894 | val_1_rmse: 0.56775 |  0:01:53s
epoch 36 | loss: 0.31807 | val_0_rmse: 0.56001 | val_1_rmse: 0.57416 |  0:01:56s
epoch 37 | loss: 0.33739 | val_0_rmse: 0.57535 | val_1_rmse: 0.58587 |  0:01:59s
epoch 38 | loss: 0.33296 | val_0_rmse: 0.55635 | val_1_rmse: 0.56859 |  0:02:03s
epoch 39 | loss: 0.31779 | val_0_rmse: 0.55477 | val_1_rmse: 0.56747 |  0:02:06s
epoch 40 | loss: 0.31942 | val_0_rmse: 0.56813 | val_1_rmse: 0.57888 |  0:02:09s
epoch 41 | loss: 0.31561 | val_0_rmse: 0.57842 | val_1_rmse: 0.58839 |  0:02:12s
epoch 42 | loss: 0.31369 | val_0_rmse: 0.55102 | val_1_rmse: 0.56504 |  0:02:15s
epoch 43 | loss: 0.31254 | val_0_rmse: 0.54369 | val_1_rmse: 0.5594  |  0:02:18s
epoch 44 | loss: 0.30925 | val_0_rmse: 0.55086 | val_1_rmse: 0.5664  |  0:02:22s
epoch 45 | loss: 0.31181 | val_0_rmse: 0.57219 | val_1_rmse: 0.5909  |  0:02:25s
epoch 46 | loss: 0.31466 | val_0_rmse: 0.5624  | val_1_rmse: 0.5813  |  0:02:28s
epoch 47 | loss: 0.32089 | val_0_rmse: 0.55262 | val_1_rmse: 0.57116 |  0:02:31s
epoch 48 | loss: 0.31408 | val_0_rmse: 0.56406 | val_1_rmse: 0.57556 |  0:02:34s
epoch 49 | loss: 0.3121  | val_0_rmse: 0.54165 | val_1_rmse: 0.55924 |  0:02:37s
epoch 50 | loss: 0.3123  | val_0_rmse: 0.54841 | val_1_rmse: 0.56869 |  0:02:41s
epoch 51 | loss: 0.3084  | val_0_rmse: 0.54468 | val_1_rmse: 0.56628 |  0:02:44s
epoch 52 | loss: 0.30861 | val_0_rmse: 0.54039 | val_1_rmse: 0.56055 |  0:02:47s
epoch 53 | loss: 0.30708 | val_0_rmse: 0.54425 | val_1_rmse: 0.56531 |  0:02:50s
epoch 54 | loss: 0.30596 | val_0_rmse: 0.5412  | val_1_rmse: 0.55941 |  0:02:53s
epoch 55 | loss: 0.30382 | val_0_rmse: 0.54531 | val_1_rmse: 0.56688 |  0:02:56s
epoch 56 | loss: 0.30501 | val_0_rmse: 0.54592 | val_1_rmse: 0.56309 |  0:02:59s
epoch 57 | loss: 0.30586 | val_0_rmse: 0.54058 | val_1_rmse: 0.56393 |  0:03:03s
epoch 58 | loss: 0.30584 | val_0_rmse: 0.53961 | val_1_rmse: 0.56035 |  0:03:06s
epoch 59 | loss: 0.30484 | val_0_rmse: 0.53834 | val_1_rmse: 0.56115 |  0:03:09s
epoch 60 | loss: 0.30459 | val_0_rmse: 0.54276 | val_1_rmse: 0.56186 |  0:03:12s
epoch 61 | loss: 0.30344 | val_0_rmse: 0.54216 | val_1_rmse: 0.56117 |  0:03:15s
epoch 62 | loss: 0.30255 | val_0_rmse: 0.53717 | val_1_rmse: 0.5605  |  0:03:18s
epoch 63 | loss: 0.30207 | val_0_rmse: 0.53722 | val_1_rmse: 0.5595  |  0:03:21s
epoch 64 | loss: 0.30421 | val_0_rmse: 0.54575 | val_1_rmse: 0.56494 |  0:03:25s
epoch 65 | loss: 0.30679 | val_0_rmse: 0.53966 | val_1_rmse: 0.56108 |  0:03:28s
epoch 66 | loss: 0.30269 | val_0_rmse: 0.53653 | val_1_rmse: 0.56023 |  0:03:31s
epoch 67 | loss: 0.3034  | val_0_rmse: 0.56218 | val_1_rmse: 0.57694 |  0:03:34s
epoch 68 | loss: 0.30523 | val_0_rmse: 0.54374 | val_1_rmse: 0.56341 |  0:03:37s
epoch 69 | loss: 0.30258 | val_0_rmse: 0.53445 | val_1_rmse: 0.55871 |  0:03:40s
epoch 70 | loss: 0.30063 | val_0_rmse: 0.53657 | val_1_rmse: 0.56065 |  0:03:43s
epoch 71 | loss: 0.30012 | val_0_rmse: 0.54053 | val_1_rmse: 0.56398 |  0:03:47s
epoch 72 | loss: 0.30045 | val_0_rmse: 0.53827 | val_1_rmse: 0.55941 |  0:03:50s
epoch 73 | loss: 0.30375 | val_0_rmse: 0.53559 | val_1_rmse: 0.56053 |  0:03:53s
epoch 74 | loss: 0.33311 | val_0_rmse: 0.5571  | val_1_rmse: 0.57166 |  0:03:56s
epoch 75 | loss: 0.3084  | val_0_rmse: 0.54079 | val_1_rmse: 0.5594  |  0:03:59s
epoch 76 | loss: 0.30489 | val_0_rmse: 0.53977 | val_1_rmse: 0.55994 |  0:04:02s
epoch 77 | loss: 0.30528 | val_0_rmse: 0.53767 | val_1_rmse: 0.55889 |  0:04:05s
epoch 78 | loss: 0.30076 | val_0_rmse: 0.53387 | val_1_rmse: 0.55805 |  0:04:09s
epoch 79 | loss: 0.30145 | val_0_rmse: 0.53773 | val_1_rmse: 0.55992 |  0:04:12s
epoch 80 | loss: 0.30476 | val_0_rmse: 0.54215 | val_1_rmse: 0.56865 |  0:04:15s
epoch 81 | loss: 0.30243 | val_0_rmse: 0.53302 | val_1_rmse: 0.55768 |  0:04:18s
epoch 82 | loss: 0.29915 | val_0_rmse: 0.53508 | val_1_rmse: 0.5611  |  0:04:21s
epoch 83 | loss: 0.30017 | val_0_rmse: 0.53338 | val_1_rmse: 0.55944 |  0:04:24s
epoch 84 | loss: 0.29817 | val_0_rmse: 0.53921 | val_1_rmse: 0.56483 |  0:04:28s
epoch 85 | loss: 0.3056  | val_0_rmse: 0.54625 | val_1_rmse: 0.57457 |  0:04:31s
epoch 86 | loss: 0.30391 | val_0_rmse: 0.55019 | val_1_rmse: 0.57394 |  0:04:34s
epoch 87 | loss: 0.31202 | val_0_rmse: 0.54164 | val_1_rmse: 0.56476 |  0:04:37s
epoch 88 | loss: 0.30415 | val_0_rmse: 0.53449 | val_1_rmse: 0.55782 |  0:04:40s
epoch 89 | loss: 0.29842 | val_0_rmse: 0.53364 | val_1_rmse: 0.56052 |  0:04:43s
epoch 90 | loss: 0.29891 | val_0_rmse: 0.53249 | val_1_rmse: 0.55987 |  0:04:46s
epoch 91 | loss: 0.29753 | val_0_rmse: 0.53375 | val_1_rmse: 0.5579  |  0:04:50s
epoch 92 | loss: 0.29848 | val_0_rmse: 0.54077 | val_1_rmse: 0.57053 |  0:04:53s
epoch 93 | loss: 0.29915 | val_0_rmse: 0.53683 | val_1_rmse: 0.5646  |  0:04:56s
epoch 94 | loss: 0.29886 | val_0_rmse: 0.53406 | val_1_rmse: 0.5642  |  0:04:59s
epoch 95 | loss: 0.29921 | val_0_rmse: 0.53304 | val_1_rmse: 0.56118 |  0:05:02s
epoch 96 | loss: 0.29709 | val_0_rmse: 0.53393 | val_1_rmse: 0.56294 |  0:05:05s
epoch 97 | loss: 0.29538 | val_0_rmse: 0.53161 | val_1_rmse: 0.55951 |  0:05:09s
epoch 98 | loss: 0.29725 | val_0_rmse: 0.53317 | val_1_rmse: 0.56372 |  0:05:12s
epoch 99 | loss: 0.29634 | val_0_rmse: 0.5353  | val_1_rmse: 0.56281 |  0:05:15s
epoch 100| loss: 0.29569 | val_0_rmse: 0.53148 | val_1_rmse: 0.56162 |  0:05:18s
epoch 101| loss: 0.29453 | val_0_rmse: 0.52992 | val_1_rmse: 0.5606  |  0:05:21s
epoch 102| loss: 0.2949  | val_0_rmse: 0.5375  | val_1_rmse: 0.57016 |  0:05:24s
epoch 103| loss: 0.29811 | val_0_rmse: 0.53959 | val_1_rmse: 0.56815 |  0:05:27s
epoch 104| loss: 0.29371 | val_0_rmse: 0.54361 | val_1_rmse: 0.57751 |  0:05:31s
epoch 105| loss: 0.29436 | val_0_rmse: 0.52821 | val_1_rmse: 0.56093 |  0:05:34s
epoch 106| loss: 0.29492 | val_0_rmse: 0.55361 | val_1_rmse: 0.57929 |  0:05:37s
epoch 107| loss: 0.29367 | val_0_rmse: 0.53196 | val_1_rmse: 0.56196 |  0:05:40s
epoch 108| loss: 0.2952  | val_0_rmse: 0.53337 | val_1_rmse: 0.56325 |  0:05:43s
epoch 109| loss: 0.29358 | val_0_rmse: 0.52774 | val_1_rmse: 0.55944 |  0:05:46s
epoch 110| loss: 0.29345 | val_0_rmse: 0.52819 | val_1_rmse: 0.56067 |  0:05:49s
epoch 111| loss: 0.29368 | val_0_rmse: 0.52789 | val_1_rmse: 0.56418 |  0:05:53s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.55768
Best weights from best epoch are automatically used!
ended training at: 07:31:22
Feature importance:
Mean squared error is of 15973348498.991549
Mean absolute error:91059.87873499541
MAPE:0.262147558644854
R2 score:0.6870961612418973
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:31:22
epoch 0  | loss: 0.79613 | val_0_rmse: 0.71737 | val_1_rmse: 0.72718 |  0:00:03s
epoch 1  | loss: 0.44505 | val_0_rmse: 0.69632 | val_1_rmse: 0.70556 |  0:00:06s
epoch 2  | loss: 0.38445 | val_0_rmse: 0.68045 | val_1_rmse: 0.6885  |  0:00:09s
epoch 3  | loss: 0.36289 | val_0_rmse: 0.66224 | val_1_rmse: 0.67045 |  0:00:12s
epoch 4  | loss: 0.3543  | val_0_rmse: 0.64292 | val_1_rmse: 0.65042 |  0:00:15s
epoch 5  | loss: 0.34685 | val_0_rmse: 0.62887 | val_1_rmse: 0.63719 |  0:00:18s
epoch 6  | loss: 0.34236 | val_0_rmse: 0.61815 | val_1_rmse: 0.62762 |  0:00:22s
epoch 7  | loss: 0.33498 | val_0_rmse: 0.63367 | val_1_rmse: 0.64038 |  0:00:25s
epoch 8  | loss: 0.33315 | val_0_rmse: 0.62862 | val_1_rmse: 0.63619 |  0:00:28s
epoch 9  | loss: 0.33003 | val_0_rmse: 0.58466 | val_1_rmse: 0.59413 |  0:00:31s
epoch 10 | loss: 0.33021 | val_0_rmse: 0.56882 | val_1_rmse: 0.58003 |  0:00:34s
epoch 11 | loss: 0.32801 | val_0_rmse: 0.57918 | val_1_rmse: 0.58945 |  0:00:37s
epoch 12 | loss: 0.3268  | val_0_rmse: 0.56311 | val_1_rmse: 0.57288 |  0:00:41s
epoch 13 | loss: 0.32182 | val_0_rmse: 0.55239 | val_1_rmse: 0.56548 |  0:00:44s
epoch 14 | loss: 0.31934 | val_0_rmse: 0.56398 | val_1_rmse: 0.57799 |  0:00:47s
epoch 15 | loss: 0.3221  | val_0_rmse: 0.55855 | val_1_rmse: 0.57231 |  0:00:50s
epoch 16 | loss: 0.32054 | val_0_rmse: 0.56601 | val_1_rmse: 0.57881 |  0:00:53s
epoch 17 | loss: 0.32099 | val_0_rmse: 0.56171 | val_1_rmse: 0.57474 |  0:00:56s
epoch 18 | loss: 0.31901 | val_0_rmse: 0.55194 | val_1_rmse: 0.56663 |  0:01:00s
epoch 19 | loss: 0.31568 | val_0_rmse: 0.57938 | val_1_rmse: 0.59107 |  0:01:03s
epoch 20 | loss: 0.31727 | val_0_rmse: 0.55231 | val_1_rmse: 0.56944 |  0:01:06s
epoch 21 | loss: 0.31578 | val_0_rmse: 0.54671 | val_1_rmse: 0.56281 |  0:01:09s
epoch 22 | loss: 0.31502 | val_0_rmse: 0.55677 | val_1_rmse: 0.57132 |  0:01:12s
epoch 23 | loss: 0.31115 | val_0_rmse: 0.5462  | val_1_rmse: 0.5644  |  0:01:15s
epoch 24 | loss: 0.31236 | val_0_rmse: 0.56024 | val_1_rmse: 0.57928 |  0:01:18s
epoch 25 | loss: 0.31075 | val_0_rmse: 0.5434  | val_1_rmse: 0.55997 |  0:01:22s
epoch 26 | loss: 0.31036 | val_0_rmse: 0.54354 | val_1_rmse: 0.56295 |  0:01:25s
epoch 27 | loss: 0.30833 | val_0_rmse: 0.55298 | val_1_rmse: 0.56913 |  0:01:28s
epoch 28 | loss: 0.30956 | val_0_rmse: 0.56914 | val_1_rmse: 0.58316 |  0:01:31s
epoch 29 | loss: 0.3119  | val_0_rmse: 0.55976 | val_1_rmse: 0.57862 |  0:01:34s
epoch 30 | loss: 0.31545 | val_0_rmse: 0.54669 | val_1_rmse: 0.56305 |  0:01:37s
epoch 31 | loss: 0.31375 | val_0_rmse: 0.54553 | val_1_rmse: 0.56402 |  0:01:41s
epoch 32 | loss: 0.30977 | val_0_rmse: 0.5423  | val_1_rmse: 0.56114 |  0:01:44s
epoch 33 | loss: 0.30967 | val_0_rmse: 0.55903 | val_1_rmse: 0.57758 |  0:01:47s
epoch 34 | loss: 0.30555 | val_0_rmse: 0.53993 | val_1_rmse: 0.55957 |  0:01:50s
epoch 35 | loss: 0.30647 | val_0_rmse: 0.56672 | val_1_rmse: 0.59006 |  0:01:53s
epoch 36 | loss: 0.30832 | val_0_rmse: 0.54587 | val_1_rmse: 0.56948 |  0:01:56s
epoch 37 | loss: 0.3053  | val_0_rmse: 0.53771 | val_1_rmse: 0.56012 |  0:01:59s
epoch 38 | loss: 0.30395 | val_0_rmse: 0.53934 | val_1_rmse: 0.56117 |  0:02:03s
epoch 39 | loss: 0.30288 | val_0_rmse: 0.54022 | val_1_rmse: 0.56092 |  0:02:06s
epoch 40 | loss: 0.305   | val_0_rmse: 0.54069 | val_1_rmse: 0.5643  |  0:02:09s
epoch 41 | loss: 0.30425 | val_0_rmse: 0.54176 | val_1_rmse: 0.56388 |  0:02:12s
epoch 42 | loss: 0.30408 | val_0_rmse: 0.53943 | val_1_rmse: 0.56271 |  0:02:15s
epoch 43 | loss: 0.30401 | val_0_rmse: 0.54492 | val_1_rmse: 0.56921 |  0:02:18s
epoch 44 | loss: 0.30327 | val_0_rmse: 0.54438 | val_1_rmse: 0.5658  |  0:02:22s
epoch 45 | loss: 0.30362 | val_0_rmse: 0.54416 | val_1_rmse: 0.57263 |  0:02:25s
epoch 46 | loss: 0.30403 | val_0_rmse: 0.54105 | val_1_rmse: 0.56233 |  0:02:28s
epoch 47 | loss: 0.3015  | val_0_rmse: 0.53704 | val_1_rmse: 0.56189 |  0:02:31s
epoch 48 | loss: 0.30259 | val_0_rmse: 0.53703 | val_1_rmse: 0.56187 |  0:02:34s
epoch 49 | loss: 0.30134 | val_0_rmse: 0.53547 | val_1_rmse: 0.55987 |  0:02:37s
epoch 50 | loss: 0.29957 | val_0_rmse: 0.53498 | val_1_rmse: 0.56233 |  0:02:40s
epoch 51 | loss: 0.30367 | val_0_rmse: 0.54402 | val_1_rmse: 0.56826 |  0:02:44s
epoch 52 | loss: 0.30309 | val_0_rmse: 0.53749 | val_1_rmse: 0.55992 |  0:02:47s
epoch 53 | loss: 0.30128 | val_0_rmse: 0.53833 | val_1_rmse: 0.56239 |  0:02:50s
epoch 54 | loss: 0.30016 | val_0_rmse: 0.53721 | val_1_rmse: 0.56087 |  0:02:53s
epoch 55 | loss: 0.29991 | val_0_rmse: 0.53623 | val_1_rmse: 0.55945 |  0:02:56s
epoch 56 | loss: 0.29795 | val_0_rmse: 0.53482 | val_1_rmse: 0.56082 |  0:02:59s
epoch 57 | loss: 0.30054 | val_0_rmse: 0.5496  | val_1_rmse: 0.57172 |  0:03:03s
epoch 58 | loss: 0.30075 | val_0_rmse: 0.54461 | val_1_rmse: 0.57084 |  0:03:06s
epoch 59 | loss: 0.29853 | val_0_rmse: 0.53363 | val_1_rmse: 0.56009 |  0:03:09s
epoch 60 | loss: 0.29895 | val_0_rmse: 0.54515 | val_1_rmse: 0.56887 |  0:03:12s
epoch 61 | loss: 0.29812 | val_0_rmse: 0.53424 | val_1_rmse: 0.56434 |  0:03:15s
epoch 62 | loss: 0.29845 | val_0_rmse: 0.53585 | val_1_rmse: 0.5649  |  0:03:18s
epoch 63 | loss: 0.30035 | val_0_rmse: 0.54386 | val_1_rmse: 0.5757  |  0:03:21s
epoch 64 | loss: 0.29974 | val_0_rmse: 0.53281 | val_1_rmse: 0.56046 |  0:03:25s
epoch 65 | loss: 0.29778 | val_0_rmse: 0.53438 | val_1_rmse: 0.56265 |  0:03:28s
epoch 66 | loss: 0.29911 | val_0_rmse: 0.53362 | val_1_rmse: 0.56271 |  0:03:31s
epoch 67 | loss: 0.29576 | val_0_rmse: 0.53906 | val_1_rmse: 0.57583 |  0:03:34s
epoch 68 | loss: 0.29588 | val_0_rmse: 0.53134 | val_1_rmse: 0.56329 |  0:03:37s
epoch 69 | loss: 0.30104 | val_0_rmse: 0.53322 | val_1_rmse: 0.56063 |  0:03:40s
epoch 70 | loss: 0.29712 | val_0_rmse: 0.53182 | val_1_rmse: 0.56177 |  0:03:44s
epoch 71 | loss: 0.29668 | val_0_rmse: 0.54866 | val_1_rmse: 0.57483 |  0:03:47s
epoch 72 | loss: 0.2971  | val_0_rmse: 0.55329 | val_1_rmse: 0.58692 |  0:03:50s
epoch 73 | loss: 0.30128 | val_0_rmse: 0.53758 | val_1_rmse: 0.56768 |  0:03:53s
epoch 74 | loss: 0.2965  | val_0_rmse: 0.53227 | val_1_rmse: 0.56495 |  0:03:56s
epoch 75 | loss: 0.29487 | val_0_rmse: 0.53051 | val_1_rmse: 0.56174 |  0:03:59s
epoch 76 | loss: 0.29592 | val_0_rmse: 0.54126 | val_1_rmse: 0.57569 |  0:04:02s
epoch 77 | loss: 0.29475 | val_0_rmse: 0.5298  | val_1_rmse: 0.564   |  0:04:06s
epoch 78 | loss: 0.2976  | val_0_rmse: 0.53525 | val_1_rmse: 0.56923 |  0:04:09s
epoch 79 | loss: 0.29695 | val_0_rmse: 0.57783 | val_1_rmse: 0.61377 |  0:04:12s
epoch 80 | loss: 0.29774 | val_0_rmse: 0.53438 | val_1_rmse: 0.56836 |  0:04:15s
epoch 81 | loss: 0.30045 | val_0_rmse: 0.56163 | val_1_rmse: 0.59589 |  0:04:18s
epoch 82 | loss: 0.2974  | val_0_rmse: 0.56045 | val_1_rmse: 0.58454 |  0:04:21s
epoch 83 | loss: 0.30412 | val_0_rmse: 0.55961 | val_1_rmse: 0.59104 |  0:04:24s
epoch 84 | loss: 0.2998  | val_0_rmse: 0.57209 | val_1_rmse: 0.60465 |  0:04:28s
epoch 85 | loss: 0.29871 | val_0_rmse: 0.53448 | val_1_rmse: 0.56435 |  0:04:31s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.55945
Best weights from best epoch are automatically used!
ended training at: 07:35:54
Feature importance:
Mean squared error is of 15734584932.198399
Mean absolute error:91315.30112551614
MAPE:0.2707824376920827
R2 score:0.6980274719079411
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:35:55
epoch 0  | loss: 0.75069 | val_0_rmse: 0.81883 | val_1_rmse: 0.82495 |  0:00:03s
epoch 1  | loss: 0.47157 | val_0_rmse: 0.88823 | val_1_rmse: 0.88565 |  0:00:06s
epoch 2  | loss: 0.41795 | val_0_rmse: 1.00321 | val_1_rmse: 0.9953  |  0:00:09s
epoch 3  | loss: 0.39836 | val_0_rmse: 0.72427 | val_1_rmse: 0.72101 |  0:00:12s
epoch 4  | loss: 0.37081 | val_0_rmse: 0.76978 | val_1_rmse: 0.76615 |  0:00:15s
epoch 5  | loss: 0.37012 | val_0_rmse: 0.69203 | val_1_rmse: 0.69765 |  0:00:18s
epoch 6  | loss: 0.35923 | val_0_rmse: 0.60989 | val_1_rmse: 0.61461 |  0:00:22s
epoch 7  | loss: 0.34891 | val_0_rmse: 0.6027  | val_1_rmse: 0.60535 |  0:00:25s
epoch 8  | loss: 0.34314 | val_0_rmse: 0.61916 | val_1_rmse: 0.62078 |  0:00:28s
epoch 9  | loss: 0.33667 | val_0_rmse: 0.57792 | val_1_rmse: 0.58498 |  0:00:31s
epoch 10 | loss: 0.33663 | val_0_rmse: 0.58254 | val_1_rmse: 0.58956 |  0:00:34s
epoch 11 | loss: 0.33484 | val_0_rmse: 0.57698 | val_1_rmse: 0.58457 |  0:00:37s
epoch 12 | loss: 0.33557 | val_0_rmse: 0.57931 | val_1_rmse: 0.58659 |  0:00:41s
epoch 13 | loss: 0.3295  | val_0_rmse: 0.56586 | val_1_rmse: 0.57506 |  0:00:44s
epoch 14 | loss: 0.32618 | val_0_rmse: 0.56345 | val_1_rmse: 0.57209 |  0:00:47s
epoch 15 | loss: 0.32494 | val_0_rmse: 0.559   | val_1_rmse: 0.573   |  0:00:50s
epoch 16 | loss: 0.32316 | val_0_rmse: 0.55581 | val_1_rmse: 0.56713 |  0:00:53s
epoch 17 | loss: 0.32569 | val_0_rmse: 0.56244 | val_1_rmse: 0.57323 |  0:00:56s
epoch 18 | loss: 0.32385 | val_0_rmse: 0.56615 | val_1_rmse: 0.57782 |  0:00:59s
epoch 19 | loss: 0.32177 | val_0_rmse: 0.55749 | val_1_rmse: 0.57135 |  0:01:03s
epoch 20 | loss: 0.32105 | val_0_rmse: 0.56393 | val_1_rmse: 0.57577 |  0:01:06s
epoch 21 | loss: 0.31892 | val_0_rmse: 0.55648 | val_1_rmse: 0.56986 |  0:01:09s
epoch 22 | loss: 0.31944 | val_0_rmse: 0.55277 | val_1_rmse: 0.56715 |  0:01:12s
epoch 23 | loss: 0.31836 | val_0_rmse: 0.56327 | val_1_rmse: 0.57816 |  0:01:15s
epoch 24 | loss: 0.31621 | val_0_rmse: 0.5563  | val_1_rmse: 0.56999 |  0:01:18s
epoch 25 | loss: 0.31747 | val_0_rmse: 0.55091 | val_1_rmse: 0.56564 |  0:01:22s
epoch 26 | loss: 0.31259 | val_0_rmse: 0.55792 | val_1_rmse: 0.57436 |  0:01:25s
epoch 27 | loss: 0.3137  | val_0_rmse: 0.5507  | val_1_rmse: 0.56907 |  0:01:28s
epoch 28 | loss: 0.31181 | val_0_rmse: 0.55671 | val_1_rmse: 0.57398 |  0:01:31s
epoch 29 | loss: 0.31353 | val_0_rmse: 0.54696 | val_1_rmse: 0.56362 |  0:01:34s
epoch 30 | loss: 0.31226 | val_0_rmse: 0.54542 | val_1_rmse: 0.56593 |  0:01:37s
epoch 31 | loss: 0.31007 | val_0_rmse: 0.54673 | val_1_rmse: 0.56483 |  0:01:40s
epoch 32 | loss: 0.31307 | val_0_rmse: 0.55578 | val_1_rmse: 0.57364 |  0:01:44s
epoch 33 | loss: 0.30999 | val_0_rmse: 0.54387 | val_1_rmse: 0.56354 |  0:01:47s
epoch 34 | loss: 0.30684 | val_0_rmse: 0.54395 | val_1_rmse: 0.56601 |  0:01:50s
epoch 35 | loss: 0.31215 | val_0_rmse: 0.54836 | val_1_rmse: 0.56742 |  0:01:53s
epoch 36 | loss: 0.31408 | val_0_rmse: 0.5554  | val_1_rmse: 0.57523 |  0:01:56s
epoch 37 | loss: 0.31086 | val_0_rmse: 0.54306 | val_1_rmse: 0.56347 |  0:01:59s
epoch 38 | loss: 0.30771 | val_0_rmse: 0.54181 | val_1_rmse: 0.56437 |  0:02:03s
epoch 39 | loss: 0.30796 | val_0_rmse: 0.54544 | val_1_rmse: 0.56742 |  0:02:06s
epoch 40 | loss: 0.30504 | val_0_rmse: 0.54138 | val_1_rmse: 0.5643  |  0:02:09s
epoch 41 | loss: 0.30551 | val_0_rmse: 0.54464 | val_1_rmse: 0.56811 |  0:02:12s
epoch 42 | loss: 0.30559 | val_0_rmse: 0.54254 | val_1_rmse: 0.56428 |  0:02:15s
epoch 43 | loss: 0.30679 | val_0_rmse: 0.54685 | val_1_rmse: 0.56968 |  0:02:18s
epoch 44 | loss: 0.30477 | val_0_rmse: 0.54389 | val_1_rmse: 0.57089 |  0:02:21s
epoch 45 | loss: 0.30415 | val_0_rmse: 0.54199 | val_1_rmse: 0.56677 |  0:02:25s
epoch 46 | loss: 0.30274 | val_0_rmse: 0.53809 | val_1_rmse: 0.56374 |  0:02:28s
epoch 47 | loss: 0.30214 | val_0_rmse: 0.54011 | val_1_rmse: 0.56556 |  0:02:31s
epoch 48 | loss: 0.30159 | val_0_rmse: 0.542   | val_1_rmse: 0.56855 |  0:02:34s
epoch 49 | loss: 0.30322 | val_0_rmse: 0.54386 | val_1_rmse: 0.56938 |  0:02:37s
epoch 50 | loss: 0.30222 | val_0_rmse: 0.53712 | val_1_rmse: 0.56587 |  0:02:40s
epoch 51 | loss: 0.30323 | val_0_rmse: 0.53652 | val_1_rmse: 0.56292 |  0:02:43s
epoch 52 | loss: 0.29888 | val_0_rmse: 0.53499 | val_1_rmse: 0.56192 |  0:02:47s
epoch 53 | loss: 0.29858 | val_0_rmse: 0.54963 | val_1_rmse: 0.57857 |  0:02:50s
epoch 54 | loss: 0.30069 | val_0_rmse: 0.5402  | val_1_rmse: 0.56612 |  0:02:53s
epoch 55 | loss: 0.29909 | val_0_rmse: 0.55668 | val_1_rmse: 0.58161 |  0:02:56s
epoch 56 | loss: 0.29986 | val_0_rmse: 0.53586 | val_1_rmse: 0.56646 |  0:02:59s
epoch 57 | loss: 0.29716 | val_0_rmse: 0.54474 | val_1_rmse: 0.57494 |  0:03:02s
epoch 58 | loss: 0.30428 | val_0_rmse: 0.54993 | val_1_rmse: 0.57567 |  0:03:06s
epoch 59 | loss: 0.29981 | val_0_rmse: 0.53677 | val_1_rmse: 0.56743 |  0:03:09s
epoch 60 | loss: 0.29906 | val_0_rmse: 0.53483 | val_1_rmse: 0.56554 |  0:03:12s
epoch 61 | loss: 0.29821 | val_0_rmse: 0.54163 | val_1_rmse: 0.56943 |  0:03:15s
epoch 62 | loss: 0.29743 | val_0_rmse: 0.53753 | val_1_rmse: 0.56936 |  0:03:18s
epoch 63 | loss: 0.29548 | val_0_rmse: 0.53219 | val_1_rmse: 0.56517 |  0:03:21s
epoch 64 | loss: 0.29686 | val_0_rmse: 0.53361 | val_1_rmse: 0.56608 |  0:03:24s
epoch 65 | loss: 0.29903 | val_0_rmse: 0.53105 | val_1_rmse: 0.56379 |  0:03:28s
epoch 66 | loss: 0.29827 | val_0_rmse: 0.56116 | val_1_rmse: 0.5892  |  0:03:31s
epoch 67 | loss: 0.29718 | val_0_rmse: 0.53906 | val_1_rmse: 0.5718  |  0:03:34s
epoch 68 | loss: 0.29537 | val_0_rmse: 0.53362 | val_1_rmse: 0.56656 |  0:03:37s
epoch 69 | loss: 0.29491 | val_0_rmse: 0.5331  | val_1_rmse: 0.56663 |  0:03:40s
epoch 70 | loss: 0.29425 | val_0_rmse: 0.53257 | val_1_rmse: 0.56612 |  0:03:43s
epoch 71 | loss: 0.29515 | val_0_rmse: 0.53293 | val_1_rmse: 0.56812 |  0:03:46s
epoch 72 | loss: 0.29455 | val_0_rmse: 0.53622 | val_1_rmse: 0.5682  |  0:03:50s
epoch 73 | loss: 0.29442 | val_0_rmse: 0.53518 | val_1_rmse: 0.56915 |  0:03:53s
epoch 74 | loss: 0.30148 | val_0_rmse: 0.55697 | val_1_rmse: 0.58999 |  0:03:56s
epoch 75 | loss: 0.31663 | val_0_rmse: 0.55299 | val_1_rmse: 0.57836 |  0:03:59s
epoch 76 | loss: 0.30188 | val_0_rmse: 0.54876 | val_1_rmse: 0.58016 |  0:04:02s
epoch 77 | loss: 0.30214 | val_0_rmse: 0.53657 | val_1_rmse: 0.56921 |  0:04:05s
epoch 78 | loss: 0.29765 | val_0_rmse: 0.5319  | val_1_rmse: 0.56425 |  0:04:09s
epoch 79 | loss: 0.295   | val_0_rmse: 0.5329  | val_1_rmse: 0.56885 |  0:04:12s
epoch 80 | loss: 0.29515 | val_0_rmse: 0.53054 | val_1_rmse: 0.56628 |  0:04:15s
epoch 81 | loss: 0.29414 | val_0_rmse: 0.52881 | val_1_rmse: 0.5644  |  0:04:18s
epoch 82 | loss: 0.29267 | val_0_rmse: 0.54374 | val_1_rmse: 0.57996 |  0:04:21s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.56192
Best weights from best epoch are automatically used!
ended training at: 07:40:17
Feature importance:
Mean squared error is of 15819634693.335852
Mean absolute error:90790.48899603769
MAPE:0.2557371063851065
R2 score:0.6952256217635505
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:40:18
epoch 0  | loss: 0.74048 | val_0_rmse: 0.80914 | val_1_rmse: 0.82544 |  0:00:03s
epoch 1  | loss: 0.4702  | val_0_rmse: 0.76372 | val_1_rmse: 0.77824 |  0:00:06s
epoch 2  | loss: 0.41048 | val_0_rmse: 0.72561 | val_1_rmse: 0.74066 |  0:00:09s
epoch 3  | loss: 0.3892  | val_0_rmse: 0.65912 | val_1_rmse: 0.67716 |  0:00:12s
epoch 4  | loss: 0.38222 | val_0_rmse: 0.67659 | val_1_rmse: 0.69145 |  0:00:15s
epoch 5  | loss: 0.37897 | val_0_rmse: 0.6243  | val_1_rmse: 0.63982 |  0:00:18s
epoch 6  | loss: 0.37079 | val_0_rmse: 0.62277 | val_1_rmse: 0.63665 |  0:00:22s
epoch 7  | loss: 0.36923 | val_0_rmse: 0.61929 | val_1_rmse: 0.63656 |  0:00:25s
epoch 8  | loss: 0.35915 | val_0_rmse: 0.6036  | val_1_rmse: 0.61709 |  0:00:28s
epoch 9  | loss: 0.36089 | val_0_rmse: 0.59686 | val_1_rmse: 0.61159 |  0:00:31s
epoch 10 | loss: 0.35448 | val_0_rmse: 0.59433 | val_1_rmse: 0.61131 |  0:00:34s
epoch 11 | loss: 0.35754 | val_0_rmse: 0.59043 | val_1_rmse: 0.60527 |  0:00:37s
epoch 12 | loss: 0.35878 | val_0_rmse: 0.59721 | val_1_rmse: 0.61157 |  0:00:41s
epoch 13 | loss: 0.34575 | val_0_rmse: 0.57785 | val_1_rmse: 0.59268 |  0:00:44s
epoch 14 | loss: 0.34711 | val_0_rmse: 0.57299 | val_1_rmse: 0.58872 |  0:00:47s
epoch 15 | loss: 0.34548 | val_0_rmse: 0.57531 | val_1_rmse: 0.59154 |  0:00:50s
epoch 16 | loss: 0.33834 | val_0_rmse: 0.58356 | val_1_rmse: 0.60099 |  0:00:53s
epoch 17 | loss: 0.33662 | val_0_rmse: 0.5624  | val_1_rmse: 0.57651 |  0:00:56s
epoch 18 | loss: 0.33119 | val_0_rmse: 0.56419 | val_1_rmse: 0.58029 |  0:00:59s
epoch 19 | loss: 0.3327  | val_0_rmse: 0.56423 | val_1_rmse: 0.581   |  0:01:03s
epoch 20 | loss: 0.3292  | val_0_rmse: 0.56995 | val_1_rmse: 0.58641 |  0:01:06s
epoch 21 | loss: 0.32854 | val_0_rmse: 0.55853 | val_1_rmse: 0.57654 |  0:01:09s
epoch 22 | loss: 0.32492 | val_0_rmse: 0.55466 | val_1_rmse: 0.57419 |  0:01:12s
epoch 23 | loss: 0.322   | val_0_rmse: 0.55804 | val_1_rmse: 0.57657 |  0:01:15s
epoch 24 | loss: 0.34053 | val_0_rmse: 0.60596 | val_1_rmse: 0.62081 |  0:01:18s
epoch 25 | loss: 0.36093 | val_0_rmse: 0.58327 | val_1_rmse: 0.59904 |  0:01:22s
epoch 26 | loss: 0.33909 | val_0_rmse: 0.575   | val_1_rmse: 0.58899 |  0:01:25s
epoch 27 | loss: 0.33917 | val_0_rmse: 0.57601 | val_1_rmse: 0.59419 |  0:01:28s
epoch 28 | loss: 0.35517 | val_0_rmse: 0.59949 | val_1_rmse: 0.61737 |  0:01:31s
epoch 29 | loss: 0.33891 | val_0_rmse: 0.57278 | val_1_rmse: 0.59147 |  0:01:34s
epoch 30 | loss: 0.34028 | val_0_rmse: 0.57766 | val_1_rmse: 0.59528 |  0:01:37s
epoch 31 | loss: 0.34469 | val_0_rmse: 0.58104 | val_1_rmse: 0.59848 |  0:01:41s
epoch 32 | loss: 0.33841 | val_0_rmse: 0.57556 | val_1_rmse: 0.59179 |  0:01:44s
epoch 33 | loss: 0.32986 | val_0_rmse: 0.56336 | val_1_rmse: 0.58027 |  0:01:47s
epoch 34 | loss: 0.33711 | val_0_rmse: 0.57377 | val_1_rmse: 0.58884 |  0:01:50s
epoch 35 | loss: 0.3427  | val_0_rmse: 0.57827 | val_1_rmse: 0.59554 |  0:01:53s
epoch 36 | loss: 0.33229 | val_0_rmse: 0.59243 | val_1_rmse: 0.60669 |  0:01:56s
epoch 37 | loss: 0.33376 | val_0_rmse: 0.56407 | val_1_rmse: 0.58274 |  0:01:59s
epoch 38 | loss: 0.32551 | val_0_rmse: 0.56202 | val_1_rmse: 0.57632 |  0:02:03s
epoch 39 | loss: 0.31922 | val_0_rmse: 0.55903 | val_1_rmse: 0.57531 |  0:02:06s
epoch 40 | loss: 0.31915 | val_0_rmse: 0.55646 | val_1_rmse: 0.57684 |  0:02:09s
epoch 41 | loss: 0.31775 | val_0_rmse: 0.55386 | val_1_rmse: 0.5718  |  0:02:12s
epoch 42 | loss: 0.31641 | val_0_rmse: 0.55702 | val_1_rmse: 0.57668 |  0:02:15s
epoch 43 | loss: 0.31995 | val_0_rmse: 0.56137 | val_1_rmse: 0.57928 |  0:02:18s
epoch 44 | loss: 0.32442 | val_0_rmse: 0.55708 | val_1_rmse: 0.57676 |  0:02:22s
epoch 45 | loss: 0.32043 | val_0_rmse: 0.56081 | val_1_rmse: 0.57908 |  0:02:25s
epoch 46 | loss: 0.31839 | val_0_rmse: 0.55383 | val_1_rmse: 0.57375 |  0:02:28s
epoch 47 | loss: 0.31548 | val_0_rmse: 0.56497 | val_1_rmse: 0.58122 |  0:02:31s
epoch 48 | loss: 0.31998 | val_0_rmse: 0.55764 | val_1_rmse: 0.57626 |  0:02:34s
epoch 49 | loss: 0.31619 | val_0_rmse: 0.55412 | val_1_rmse: 0.57551 |  0:02:37s
epoch 50 | loss: 0.31593 | val_0_rmse: 0.55432 | val_1_rmse: 0.57619 |  0:02:40s
epoch 51 | loss: 0.31476 | val_0_rmse: 0.55976 | val_1_rmse: 0.57986 |  0:02:44s
epoch 52 | loss: 0.31457 | val_0_rmse: 0.5601  | val_1_rmse: 0.58128 |  0:02:47s
epoch 53 | loss: 0.31467 | val_0_rmse: 0.55519 | val_1_rmse: 0.57475 |  0:02:50s
epoch 54 | loss: 0.31487 | val_0_rmse: 0.55364 | val_1_rmse: 0.57545 |  0:02:53s
epoch 55 | loss: 0.31461 | val_0_rmse: 0.56388 | val_1_rmse: 0.58473 |  0:02:56s
epoch 56 | loss: 0.32301 | val_0_rmse: 0.58254 | val_1_rmse: 0.60213 |  0:02:59s
epoch 57 | loss: 0.40378 | val_0_rmse: 0.67404 | val_1_rmse: 0.69379 |  0:03:03s
epoch 58 | loss: 0.35324 | val_0_rmse: 0.5733  | val_1_rmse: 0.58862 |  0:03:06s
epoch 59 | loss: 0.3395  | val_0_rmse: 0.56619 | val_1_rmse: 0.58414 |  0:03:09s
epoch 60 | loss: 0.3312  | val_0_rmse: 0.56074 | val_1_rmse: 0.5797  |  0:03:12s
epoch 61 | loss: 0.32511 | val_0_rmse: 0.56093 | val_1_rmse: 0.57902 |  0:03:15s
epoch 62 | loss: 0.32348 | val_0_rmse: 0.55562 | val_1_rmse: 0.57605 |  0:03:18s
epoch 63 | loss: 0.31963 | val_0_rmse: 0.55772 | val_1_rmse: 0.57707 |  0:03:21s
epoch 64 | loss: 0.32359 | val_0_rmse: 0.56111 | val_1_rmse: 0.58069 |  0:03:25s
epoch 65 | loss: 0.31966 | val_0_rmse: 0.55643 | val_1_rmse: 0.57712 |  0:03:28s
epoch 66 | loss: 0.32276 | val_0_rmse: 0.55312 | val_1_rmse: 0.57443 |  0:03:31s
epoch 67 | loss: 0.31918 | val_0_rmse: 0.55542 | val_1_rmse: 0.57354 |  0:03:34s
epoch 68 | loss: 0.31451 | val_0_rmse: 0.55101 | val_1_rmse: 0.57279 |  0:03:37s
epoch 69 | loss: 0.31405 | val_0_rmse: 0.57172 | val_1_rmse: 0.58778 |  0:03:40s
epoch 70 | loss: 0.31885 | val_0_rmse: 0.54971 | val_1_rmse: 0.5703  |  0:03:43s
epoch 71 | loss: 0.31158 | val_0_rmse: 0.54864 | val_1_rmse: 0.57029 |  0:03:47s
epoch 72 | loss: 0.31139 | val_0_rmse: 0.57661 | val_1_rmse: 0.59698 |  0:03:50s
epoch 73 | loss: 0.31713 | val_0_rmse: 0.55435 | val_1_rmse: 0.57508 |  0:03:53s
epoch 74 | loss: 0.30983 | val_0_rmse: 0.57108 | val_1_rmse: 0.59314 |  0:03:56s
epoch 75 | loss: 0.31272 | val_0_rmse: 0.55347 | val_1_rmse: 0.57416 |  0:03:59s
epoch 76 | loss: 0.3157  | val_0_rmse: 0.55058 | val_1_rmse: 0.57368 |  0:04:02s
epoch 77 | loss: 0.31729 | val_0_rmse: 0.56939 | val_1_rmse: 0.58876 |  0:04:06s
epoch 78 | loss: 0.31045 | val_0_rmse: 0.55258 | val_1_rmse: 0.57461 |  0:04:09s
epoch 79 | loss: 0.30922 | val_0_rmse: 0.54938 | val_1_rmse: 0.57124 |  0:04:12s
epoch 80 | loss: 0.30971 | val_0_rmse: 0.56189 | val_1_rmse: 0.58485 |  0:04:15s
epoch 81 | loss: 0.30597 | val_0_rmse: 0.54915 | val_1_rmse: 0.57142 |  0:04:18s
epoch 82 | loss: 0.30836 | val_0_rmse: 0.55128 | val_1_rmse: 0.57477 |  0:04:21s
epoch 83 | loss: 0.30702 | val_0_rmse: 0.58846 | val_1_rmse: 0.61192 |  0:04:24s
epoch 84 | loss: 0.30624 | val_0_rmse: 0.55013 | val_1_rmse: 0.57466 |  0:04:28s
epoch 85 | loss: 0.30388 | val_0_rmse: 0.54293 | val_1_rmse: 0.56769 |  0:04:31s
epoch 86 | loss: 0.30596 | val_0_rmse: 0.55128 | val_1_rmse: 0.575   |  0:04:34s
epoch 87 | loss: 0.30395 | val_0_rmse: 0.54557 | val_1_rmse: 0.56987 |  0:04:37s
epoch 88 | loss: 0.30559 | val_0_rmse: 0.55108 | val_1_rmse: 0.57789 |  0:04:40s
epoch 89 | loss: 0.30649 | val_0_rmse: 0.5649  | val_1_rmse: 0.59043 |  0:04:43s
epoch 90 | loss: 0.30502 | val_0_rmse: 0.54946 | val_1_rmse: 0.57485 |  0:04:46s
epoch 91 | loss: 0.30444 | val_0_rmse: 0.56866 | val_1_rmse: 0.58852 |  0:04:50s
epoch 92 | loss: 0.30438 | val_0_rmse: 0.55054 | val_1_rmse: 0.57507 |  0:04:53s
epoch 93 | loss: 0.30445 | val_0_rmse: 0.56918 | val_1_rmse: 0.58454 |  0:04:56s
epoch 94 | loss: 0.30212 | val_0_rmse: 0.55903 | val_1_rmse: 0.57982 |  0:04:59s
epoch 95 | loss: 0.30103 | val_0_rmse: 0.54311 | val_1_rmse: 0.56896 |  0:05:02s
epoch 96 | loss: 0.30027 | val_0_rmse: 0.57932 | val_1_rmse: 0.59144 |  0:05:05s
epoch 97 | loss: 0.30269 | val_0_rmse: 0.5614  | val_1_rmse: 0.58444 |  0:05:09s
epoch 98 | loss: 0.3088  | val_0_rmse: 0.56508 | val_1_rmse: 0.58832 |  0:05:12s
epoch 99 | loss: 0.30255 | val_0_rmse: 0.55176 | val_1_rmse: 0.57558 |  0:05:15s
epoch 100| loss: 0.30083 | val_0_rmse: 0.57046 | val_1_rmse: 0.58755 |  0:05:18s
epoch 101| loss: 0.30516 | val_0_rmse: 0.56546 | val_1_rmse: 0.58584 |  0:05:21s
epoch 102| loss: 0.30261 | val_0_rmse: 0.59658 | val_1_rmse: 0.60178 |  0:05:24s
epoch 103| loss: 0.30052 | val_0_rmse: 0.6099  | val_1_rmse: 0.60836 |  0:05:27s
epoch 104| loss: 0.30028 | val_0_rmse: 0.62357 | val_1_rmse: 0.60951 |  0:05:31s
epoch 105| loss: 0.29961 | val_0_rmse: 0.5554  | val_1_rmse: 0.58128 |  0:05:34s
epoch 106| loss: 0.30065 | val_0_rmse: 0.56488 | val_1_rmse: 0.58463 |  0:05:37s
epoch 107| loss: 0.29978 | val_0_rmse: 0.55029 | val_1_rmse: 0.57588 |  0:05:40s
epoch 108| loss: 0.29718 | val_0_rmse: 0.66557 | val_1_rmse: 0.63015 |  0:05:43s
epoch 109| loss: 0.29807 | val_0_rmse: 0.77162 | val_1_rmse: 0.68529 |  0:05:46s
epoch 110| loss: 0.30125 | val_0_rmse: 0.65563 | val_1_rmse: 0.62635 |  0:05:49s
epoch 111| loss: 0.30016 | val_0_rmse: 0.77495 | val_1_rmse: 0.68724 |  0:05:53s
epoch 112| loss: 0.29802 | val_0_rmse: 0.61341 | val_1_rmse: 0.61383 |  0:05:56s
epoch 113| loss: 0.29968 | val_0_rmse: 0.6599  | val_1_rmse: 0.6318  |  0:05:59s
epoch 114| loss: 0.29784 | val_0_rmse: 0.57437 | val_1_rmse: 0.59285 |  0:06:02s
epoch 115| loss: 0.29545 | val_0_rmse: 0.55102 | val_1_rmse: 0.57976 |  0:06:05s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.56769
Best weights from best epoch are automatically used!
ended training at: 07:46:24
Feature importance:
Mean squared error is of 15389844617.98202
Mean absolute error:91204.03726805451
MAPE:0.2685735246145246
R2 score:0.691776091328481
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:46:25
epoch 0  | loss: 1.24419 | val_0_rmse: 0.99084 | val_1_rmse: 0.99363 |  0:00:01s
epoch 1  | loss: 0.77278 | val_0_rmse: 0.88965 | val_1_rmse: 0.89136 |  0:00:02s
epoch 2  | loss: 0.51885 | val_0_rmse: 0.85086 | val_1_rmse: 0.85333 |  0:00:03s
epoch 3  | loss: 0.42905 | val_0_rmse: 0.8002  | val_1_rmse: 0.80872 |  0:00:04s
epoch 4  | loss: 0.36607 | val_0_rmse: 0.78355 | val_1_rmse: 0.79339 |  0:00:05s
epoch 5  | loss: 0.35607 | val_0_rmse: 0.77814 | val_1_rmse: 0.7852  |  0:00:07s
epoch 6  | loss: 0.34779 | val_0_rmse: 0.74365 | val_1_rmse: 0.74885 |  0:00:08s
epoch 7  | loss: 0.33799 | val_0_rmse: 0.75239 | val_1_rmse: 0.76026 |  0:00:09s
epoch 8  | loss: 0.33253 | val_0_rmse: 0.74887 | val_1_rmse: 0.75834 |  0:00:10s
epoch 9  | loss: 0.32755 | val_0_rmse: 0.74439 | val_1_rmse: 0.75112 |  0:00:11s
epoch 10 | loss: 0.32856 | val_0_rmse: 0.7401  | val_1_rmse: 0.75044 |  0:00:12s
epoch 11 | loss: 0.31762 | val_0_rmse: 0.75106 | val_1_rmse: 0.75389 |  0:00:14s
epoch 12 | loss: 0.31717 | val_0_rmse: 0.73364 | val_1_rmse: 0.73871 |  0:00:15s
epoch 13 | loss: 0.31452 | val_0_rmse: 0.70657 | val_1_rmse: 0.71105 |  0:00:16s
epoch 14 | loss: 0.31857 | val_0_rmse: 0.68834 | val_1_rmse: 0.69536 |  0:00:17s
epoch 15 | loss: 0.31639 | val_0_rmse: 0.68708 | val_1_rmse: 0.69424 |  0:00:18s
epoch 16 | loss: 0.31438 | val_0_rmse: 0.66805 | val_1_rmse: 0.68214 |  0:00:20s
epoch 17 | loss: 0.31196 | val_0_rmse: 0.66851 | val_1_rmse: 0.68075 |  0:00:21s
epoch 18 | loss: 0.30514 | val_0_rmse: 0.66501 | val_1_rmse: 0.67771 |  0:00:22s
epoch 19 | loss: 0.30283 | val_0_rmse: 0.65268 | val_1_rmse: 0.66106 |  0:00:23s
epoch 20 | loss: 0.30236 | val_0_rmse: 0.61156 | val_1_rmse: 0.62266 |  0:00:24s
epoch 21 | loss: 0.29961 | val_0_rmse: 0.60172 | val_1_rmse: 0.61212 |  0:00:26s
epoch 22 | loss: 0.29735 | val_0_rmse: 0.6161  | val_1_rmse: 0.62438 |  0:00:27s
epoch 23 | loss: 0.29675 | val_0_rmse: 0.58617 | val_1_rmse: 0.59927 |  0:00:28s
epoch 24 | loss: 0.2945  | val_0_rmse: 0.58605 | val_1_rmse: 0.59334 |  0:00:29s
epoch 25 | loss: 0.28937 | val_0_rmse: 0.57574 | val_1_rmse: 0.59264 |  0:00:30s
epoch 26 | loss: 0.29848 | val_0_rmse: 0.56172 | val_1_rmse: 0.5647  |  0:00:31s
epoch 27 | loss: 0.29611 | val_0_rmse: 0.56379 | val_1_rmse: 0.57008 |  0:00:33s
epoch 28 | loss: 0.29197 | val_0_rmse: 0.55421 | val_1_rmse: 0.55832 |  0:00:34s
epoch 29 | loss: 0.29585 | val_0_rmse: 0.55185 | val_1_rmse: 0.55749 |  0:00:35s
epoch 30 | loss: 0.29797 | val_0_rmse: 0.55511 | val_1_rmse: 0.56073 |  0:00:36s
epoch 31 | loss: 0.29496 | val_0_rmse: 0.5455  | val_1_rmse: 0.55367 |  0:00:37s
epoch 32 | loss: 0.29552 | val_0_rmse: 0.57237 | val_1_rmse: 0.58147 |  0:00:38s
epoch 33 | loss: 0.31084 | val_0_rmse: 0.54122 | val_1_rmse: 0.55016 |  0:00:40s
epoch 34 | loss: 0.30296 | val_0_rmse: 0.5358  | val_1_rmse: 0.54338 |  0:00:41s
epoch 35 | loss: 0.29802 | val_0_rmse: 0.53573 | val_1_rmse: 0.5472  |  0:00:42s
epoch 36 | loss: 0.29866 | val_0_rmse: 0.53834 | val_1_rmse: 0.54297 |  0:00:43s
epoch 37 | loss: 0.29106 | val_0_rmse: 0.53705 | val_1_rmse: 0.54172 |  0:00:44s
epoch 38 | loss: 0.28944 | val_0_rmse: 0.52787 | val_1_rmse: 0.53385 |  0:00:46s
epoch 39 | loss: 0.28546 | val_0_rmse: 0.52574 | val_1_rmse: 0.53258 |  0:00:47s
epoch 40 | loss: 0.28667 | val_0_rmse: 0.53225 | val_1_rmse: 0.54619 |  0:00:48s
epoch 41 | loss: 0.29194 | val_0_rmse: 0.53453 | val_1_rmse: 0.54027 |  0:00:49s
epoch 42 | loss: 0.29414 | val_0_rmse: 0.52568 | val_1_rmse: 0.54034 |  0:00:50s
epoch 43 | loss: 0.28916 | val_0_rmse: 0.53325 | val_1_rmse: 0.54223 |  0:00:51s
epoch 44 | loss: 0.3095  | val_0_rmse: 0.57262 | val_1_rmse: 0.58163 |  0:00:53s
epoch 45 | loss: 0.30783 | val_0_rmse: 0.55501 | val_1_rmse: 0.55442 |  0:00:54s
epoch 46 | loss: 0.3018  | val_0_rmse: 0.53972 | val_1_rmse: 0.55149 |  0:00:55s
epoch 47 | loss: 0.30351 | val_0_rmse: 0.56042 | val_1_rmse: 0.57119 |  0:00:56s
epoch 48 | loss: 0.32245 | val_0_rmse: 0.54739 | val_1_rmse: 0.55446 |  0:00:57s
epoch 49 | loss: 0.30865 | val_0_rmse: 0.54496 | val_1_rmse: 0.55254 |  0:00:59s
epoch 50 | loss: 0.30377 | val_0_rmse: 0.53421 | val_1_rmse: 0.54066 |  0:01:00s
epoch 51 | loss: 0.2947  | val_0_rmse: 0.53002 | val_1_rmse: 0.53495 |  0:01:01s
epoch 52 | loss: 0.28719 | val_0_rmse: 0.52783 | val_1_rmse: 0.53201 |  0:01:02s
epoch 53 | loss: 0.28711 | val_0_rmse: 0.52458 | val_1_rmse: 0.52943 |  0:01:03s
epoch 54 | loss: 0.29034 | val_0_rmse: 0.5238  | val_1_rmse: 0.52978 |  0:01:04s
epoch 55 | loss: 0.28471 | val_0_rmse: 0.5203  | val_1_rmse: 0.52391 |  0:01:06s
epoch 56 | loss: 0.28025 | val_0_rmse: 0.52408 | val_1_rmse: 0.53179 |  0:01:07s
epoch 57 | loss: 0.28122 | val_0_rmse: 0.52056 | val_1_rmse: 0.53192 |  0:01:08s
epoch 58 | loss: 0.28159 | val_0_rmse: 0.51998 | val_1_rmse: 0.52799 |  0:01:09s
epoch 59 | loss: 0.28214 | val_0_rmse: 0.52333 | val_1_rmse: 0.53376 |  0:01:10s
epoch 60 | loss: 0.27689 | val_0_rmse: 0.51779 | val_1_rmse: 0.53172 |  0:01:11s
epoch 61 | loss: 0.27772 | val_0_rmse: 0.51862 | val_1_rmse: 0.53107 |  0:01:13s
epoch 62 | loss: 0.27486 | val_0_rmse: 0.51449 | val_1_rmse: 0.52591 |  0:01:14s
epoch 63 | loss: 0.27707 | val_0_rmse: 0.53811 | val_1_rmse: 0.55482 |  0:01:15s
epoch 64 | loss: 0.30527 | val_0_rmse: 0.54047 | val_1_rmse: 0.55245 |  0:01:16s
epoch 65 | loss: 0.29318 | val_0_rmse: 0.53162 | val_1_rmse: 0.53629 |  0:01:17s
epoch 66 | loss: 0.28861 | val_0_rmse: 0.52797 | val_1_rmse: 0.53519 |  0:01:19s
epoch 67 | loss: 0.28994 | val_0_rmse: 0.52937 | val_1_rmse: 0.53493 |  0:01:20s
epoch 68 | loss: 0.28595 | val_0_rmse: 0.52328 | val_1_rmse: 0.53575 |  0:01:21s
epoch 69 | loss: 0.28169 | val_0_rmse: 0.51816 | val_1_rmse: 0.53382 |  0:01:22s
epoch 70 | loss: 0.28316 | val_0_rmse: 0.5182  | val_1_rmse: 0.53419 |  0:01:23s
epoch 71 | loss: 0.28118 | val_0_rmse: 0.51986 | val_1_rmse: 0.5342  |  0:01:24s
epoch 72 | loss: 0.28076 | val_0_rmse: 0.51999 | val_1_rmse: 0.53275 |  0:01:26s
epoch 73 | loss: 0.28782 | val_0_rmse: 0.54069 | val_1_rmse: 0.55403 |  0:01:27s
epoch 74 | loss: 0.30345 | val_0_rmse: 0.53723 | val_1_rmse: 0.54614 |  0:01:28s
epoch 75 | loss: 0.2922  | val_0_rmse: 0.52609 | val_1_rmse: 0.5378  |  0:01:29s
epoch 76 | loss: 0.29159 | val_0_rmse: 0.5266  | val_1_rmse: 0.53703 |  0:01:30s
epoch 77 | loss: 0.28854 | val_0_rmse: 0.52356 | val_1_rmse: 0.53176 |  0:01:31s
epoch 78 | loss: 0.28531 | val_0_rmse: 0.52641 | val_1_rmse: 0.54253 |  0:01:33s
epoch 79 | loss: 0.27745 | val_0_rmse: 0.51592 | val_1_rmse: 0.52649 |  0:01:34s
epoch 80 | loss: 0.27558 | val_0_rmse: 0.51537 | val_1_rmse: 0.53073 |  0:01:35s
epoch 81 | loss: 0.27582 | val_0_rmse: 0.53201 | val_1_rmse: 0.54642 |  0:01:36s
epoch 82 | loss: 0.29234 | val_0_rmse: 0.52816 | val_1_rmse: 0.54083 |  0:01:37s
epoch 83 | loss: 0.28467 | val_0_rmse: 0.52251 | val_1_rmse: 0.5345  |  0:01:39s
epoch 84 | loss: 0.28171 | val_0_rmse: 0.52565 | val_1_rmse: 0.53317 |  0:01:40s
epoch 85 | loss: 0.28326 | val_0_rmse: 0.52246 | val_1_rmse: 0.53041 |  0:01:41s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.52391
Best weights from best epoch are automatically used!
ended training at: 07:48:07
Feature importance:
Mean squared error is of 8188363463.279357
Mean absolute error:67304.85185333248
MAPE:0.183411363942739
R2 score:0.7292029318591249
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:48:07
epoch 0  | loss: 1.20598 | val_0_rmse: 0.92116 | val_1_rmse: 0.91828 |  0:00:01s
epoch 1  | loss: 0.66788 | val_0_rmse: 0.7874  | val_1_rmse: 0.78587 |  0:00:02s
epoch 2  | loss: 0.57033 | val_0_rmse: 0.75788 | val_1_rmse: 0.75188 |  0:00:03s
epoch 3  | loss: 0.499   | val_0_rmse: 0.73079 | val_1_rmse: 0.72266 |  0:00:04s
epoch 4  | loss: 0.43797 | val_0_rmse: 0.7592  | val_1_rmse: 0.75617 |  0:00:05s
epoch 5  | loss: 0.39708 | val_0_rmse: 0.6985  | val_1_rmse: 0.69568 |  0:00:07s
epoch 6  | loss: 0.37446 | val_0_rmse: 0.70438 | val_1_rmse: 0.70281 |  0:00:08s
epoch 7  | loss: 0.36114 | val_0_rmse: 0.70805 | val_1_rmse: 0.70497 |  0:00:09s
epoch 8  | loss: 0.35747 | val_0_rmse: 0.69715 | val_1_rmse: 0.69575 |  0:00:10s
epoch 9  | loss: 0.3529  | val_0_rmse: 0.6856  | val_1_rmse: 0.68399 |  0:00:11s
epoch 10 | loss: 0.34459 | val_0_rmse: 0.67648 | val_1_rmse: 0.67958 |  0:00:13s
epoch 11 | loss: 0.33713 | val_0_rmse: 0.67167 | val_1_rmse: 0.67104 |  0:00:14s
epoch 12 | loss: 0.33229 | val_0_rmse: 0.67019 | val_1_rmse: 0.6708  |  0:00:15s
epoch 13 | loss: 0.332   | val_0_rmse: 0.6794  | val_1_rmse: 0.6767  |  0:00:16s
epoch 14 | loss: 0.32491 | val_0_rmse: 0.64179 | val_1_rmse: 0.64094 |  0:00:17s
epoch 15 | loss: 0.32471 | val_0_rmse: 0.64707 | val_1_rmse: 0.65032 |  0:00:18s
epoch 16 | loss: 0.31833 | val_0_rmse: 0.65778 | val_1_rmse: 0.65752 |  0:00:20s
epoch 17 | loss: 0.32033 | val_0_rmse: 0.66313 | val_1_rmse: 0.66308 |  0:00:21s
epoch 18 | loss: 0.3103  | val_0_rmse: 0.613   | val_1_rmse: 0.62047 |  0:00:22s
epoch 19 | loss: 0.30682 | val_0_rmse: 0.61025 | val_1_rmse: 0.61961 |  0:00:23s
epoch 20 | loss: 0.30499 | val_0_rmse: 0.60333 | val_1_rmse: 0.61168 |  0:00:24s
epoch 21 | loss: 0.30883 | val_0_rmse: 0.59414 | val_1_rmse: 0.60311 |  0:00:26s
epoch 22 | loss: 0.30494 | val_0_rmse: 0.58186 | val_1_rmse: 0.59182 |  0:00:27s
epoch 23 | loss: 0.31184 | val_0_rmse: 0.57114 | val_1_rmse: 0.57728 |  0:00:28s
epoch 24 | loss: 0.30176 | val_0_rmse: 0.56596 | val_1_rmse: 0.57475 |  0:00:29s
epoch 25 | loss: 0.30146 | val_0_rmse: 0.56257 | val_1_rmse: 0.57252 |  0:00:30s
epoch 26 | loss: 0.2962  | val_0_rmse: 0.55793 | val_1_rmse: 0.56569 |  0:00:31s
epoch 27 | loss: 0.29375 | val_0_rmse: 0.54363 | val_1_rmse: 0.55317 |  0:00:33s
epoch 28 | loss: 0.2931  | val_0_rmse: 0.54238 | val_1_rmse: 0.54923 |  0:00:34s
epoch 29 | loss: 0.2896  | val_0_rmse: 0.53753 | val_1_rmse: 0.54398 |  0:00:35s
epoch 30 | loss: 0.28268 | val_0_rmse: 0.53034 | val_1_rmse: 0.5339  |  0:00:36s
epoch 31 | loss: 0.28548 | val_0_rmse: 0.53132 | val_1_rmse: 0.53768 |  0:00:37s
epoch 32 | loss: 0.28467 | val_0_rmse: 0.52814 | val_1_rmse: 0.53065 |  0:00:39s
epoch 33 | loss: 0.28219 | val_0_rmse: 0.52883 | val_1_rmse: 0.53586 |  0:00:40s
epoch 34 | loss: 0.28254 | val_0_rmse: 0.52587 | val_1_rmse: 0.53126 |  0:00:41s
epoch 35 | loss: 0.2847  | val_0_rmse: 0.52105 | val_1_rmse: 0.52856 |  0:00:42s
epoch 36 | loss: 0.28117 | val_0_rmse: 0.53672 | val_1_rmse: 0.54904 |  0:00:43s
epoch 37 | loss: 0.28443 | val_0_rmse: 0.53215 | val_1_rmse: 0.54329 |  0:00:44s
epoch 38 | loss: 0.27964 | val_0_rmse: 0.51862 | val_1_rmse: 0.53261 |  0:00:46s
epoch 39 | loss: 0.27549 | val_0_rmse: 0.51576 | val_1_rmse: 0.5267  |  0:00:47s
epoch 40 | loss: 0.27783 | val_0_rmse: 0.51523 | val_1_rmse: 0.53124 |  0:00:48s
epoch 41 | loss: 0.27852 | val_0_rmse: 0.51317 | val_1_rmse: 0.52765 |  0:00:49s
epoch 42 | loss: 0.27469 | val_0_rmse: 0.51993 | val_1_rmse: 0.53633 |  0:00:50s
epoch 43 | loss: 0.27122 | val_0_rmse: 0.50907 | val_1_rmse: 0.52928 |  0:00:51s
epoch 44 | loss: 0.27357 | val_0_rmse: 0.50892 | val_1_rmse: 0.52321 |  0:00:53s
epoch 45 | loss: 0.27266 | val_0_rmse: 0.51785 | val_1_rmse: 0.53357 |  0:00:54s
epoch 46 | loss: 0.27606 | val_0_rmse: 0.52501 | val_1_rmse: 0.54365 |  0:00:55s
epoch 47 | loss: 0.27158 | val_0_rmse: 0.50606 | val_1_rmse: 0.52592 |  0:00:56s
epoch 48 | loss: 0.26815 | val_0_rmse: 0.51144 | val_1_rmse: 0.52848 |  0:00:57s
epoch 49 | loss: 0.27267 | val_0_rmse: 0.51253 | val_1_rmse: 0.52948 |  0:00:59s
epoch 50 | loss: 0.27278 | val_0_rmse: 0.51006 | val_1_rmse: 0.52679 |  0:01:00s
epoch 51 | loss: 0.2721  | val_0_rmse: 0.51136 | val_1_rmse: 0.52963 |  0:01:01s
epoch 52 | loss: 0.2696  | val_0_rmse: 0.51078 | val_1_rmse: 0.532   |  0:01:02s
epoch 53 | loss: 0.27147 | val_0_rmse: 0.51066 | val_1_rmse: 0.52948 |  0:01:03s
epoch 54 | loss: 0.26923 | val_0_rmse: 0.5026  | val_1_rmse: 0.52755 |  0:01:04s
epoch 55 | loss: 0.26493 | val_0_rmse: 0.50576 | val_1_rmse: 0.53225 |  0:01:06s
epoch 56 | loss: 0.26676 | val_0_rmse: 0.5016  | val_1_rmse: 0.52721 |  0:01:07s
epoch 57 | loss: 0.268   | val_0_rmse: 0.52132 | val_1_rmse: 0.54561 |  0:01:08s
epoch 58 | loss: 0.26605 | val_0_rmse: 0.51245 | val_1_rmse: 0.53321 |  0:01:09s
epoch 59 | loss: 0.27064 | val_0_rmse: 0.50199 | val_1_rmse: 0.5258  |  0:01:10s
epoch 60 | loss: 0.26534 | val_0_rmse: 0.50374 | val_1_rmse: 0.52531 |  0:01:11s
epoch 61 | loss: 0.26517 | val_0_rmse: 0.49817 | val_1_rmse: 0.5249  |  0:01:13s
epoch 62 | loss: 0.26048 | val_0_rmse: 0.49999 | val_1_rmse: 0.52767 |  0:01:14s
epoch 63 | loss: 0.25942 | val_0_rmse: 0.49684 | val_1_rmse: 0.5233  |  0:01:15s
epoch 64 | loss: 0.25925 | val_0_rmse: 0.49804 | val_1_rmse: 0.52588 |  0:01:16s
epoch 65 | loss: 0.26043 | val_0_rmse: 0.50132 | val_1_rmse: 0.53318 |  0:01:17s
epoch 66 | loss: 0.26011 | val_0_rmse: 0.50203 | val_1_rmse: 0.52966 |  0:01:19s
epoch 67 | loss: 0.26164 | val_0_rmse: 0.49994 | val_1_rmse: 0.53404 |  0:01:20s
epoch 68 | loss: 0.26126 | val_0_rmse: 0.4959  | val_1_rmse: 0.52837 |  0:01:21s
epoch 69 | loss: 0.25814 | val_0_rmse: 0.49535 | val_1_rmse: 0.53351 |  0:01:22s
epoch 70 | loss: 0.26153 | val_0_rmse: 0.50902 | val_1_rmse: 0.54516 |  0:01:23s
epoch 71 | loss: 0.26232 | val_0_rmse: 0.5018  | val_1_rmse: 0.54064 |  0:01:24s
epoch 72 | loss: 0.26111 | val_0_rmse: 0.49851 | val_1_rmse: 0.53025 |  0:01:26s
epoch 73 | loss: 0.25901 | val_0_rmse: 0.4964  | val_1_rmse: 0.53246 |  0:01:27s
epoch 74 | loss: 0.26016 | val_0_rmse: 0.49578 | val_1_rmse: 0.53078 |  0:01:28s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.52321
Best weights from best epoch are automatically used!
ended training at: 07:49:36
Feature importance:
Mean squared error is of 8283191265.702996
Mean absolute error:67721.5394431703
MAPE:0.18334495432826456
R2 score:0.7304836443725133
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:49:36
epoch 0  | loss: 1.30488 | val_0_rmse: 0.97219 | val_1_rmse: 0.97414 |  0:00:01s
epoch 1  | loss: 0.73707 | val_0_rmse: 0.79374 | val_1_rmse: 0.81246 |  0:00:02s
epoch 2  | loss: 0.55582 | val_0_rmse: 0.76906 | val_1_rmse: 0.79237 |  0:00:03s
epoch 3  | loss: 0.47358 | val_0_rmse: 0.71207 | val_1_rmse: 0.73329 |  0:00:04s
epoch 4  | loss: 0.39261 | val_0_rmse: 0.70796 | val_1_rmse: 0.73096 |  0:00:05s
epoch 5  | loss: 0.36471 | val_0_rmse: 0.72969 | val_1_rmse: 0.75083 |  0:00:07s
epoch 6  | loss: 0.35276 | val_0_rmse: 0.73202 | val_1_rmse: 0.7537  |  0:00:08s
epoch 7  | loss: 0.34578 | val_0_rmse: 0.72917 | val_1_rmse: 0.75745 |  0:00:09s
epoch 8  | loss: 0.33058 | val_0_rmse: 0.70348 | val_1_rmse: 0.73314 |  0:00:10s
epoch 9  | loss: 0.32758 | val_0_rmse: 0.68891 | val_1_rmse: 0.71208 |  0:00:11s
epoch 10 | loss: 0.3232  | val_0_rmse: 0.6856  | val_1_rmse: 0.71198 |  0:00:13s
epoch 11 | loss: 0.31852 | val_0_rmse: 0.67468 | val_1_rmse: 0.70613 |  0:00:14s
epoch 12 | loss: 0.31277 | val_0_rmse: 0.63782 | val_1_rmse: 0.6633  |  0:00:15s
epoch 13 | loss: 0.30473 | val_0_rmse: 0.65375 | val_1_rmse: 0.68383 |  0:00:16s
epoch 14 | loss: 0.29884 | val_0_rmse: 0.65325 | val_1_rmse: 0.68319 |  0:00:17s
epoch 15 | loss: 0.30733 | val_0_rmse: 0.67096 | val_1_rmse: 0.69757 |  0:00:18s
epoch 16 | loss: 0.30297 | val_0_rmse: 0.62626 | val_1_rmse: 0.65313 |  0:00:20s
epoch 17 | loss: 0.29469 | val_0_rmse: 0.60821 | val_1_rmse: 0.63703 |  0:00:21s
epoch 18 | loss: 0.2925  | val_0_rmse: 0.60161 | val_1_rmse: 0.63018 |  0:00:22s
epoch 19 | loss: 0.29864 | val_0_rmse: 0.62342 | val_1_rmse: 0.65834 |  0:00:23s
epoch 20 | loss: 0.29438 | val_0_rmse: 0.60886 | val_1_rmse: 0.64233 |  0:00:24s
epoch 21 | loss: 0.29667 | val_0_rmse: 0.57915 | val_1_rmse: 0.60678 |  0:00:26s
epoch 22 | loss: 0.28189 | val_0_rmse: 0.57392 | val_1_rmse: 0.60013 |  0:00:27s
epoch 23 | loss: 0.28236 | val_0_rmse: 0.56273 | val_1_rmse: 0.59043 |  0:00:28s
epoch 24 | loss: 0.27997 | val_0_rmse: 0.55378 | val_1_rmse: 0.58062 |  0:00:29s
epoch 25 | loss: 0.2802  | val_0_rmse: 0.55384 | val_1_rmse: 0.58274 |  0:00:30s
epoch 26 | loss: 0.28001 | val_0_rmse: 0.53627 | val_1_rmse: 0.56535 |  0:00:31s
epoch 27 | loss: 0.28261 | val_0_rmse: 0.55867 | val_1_rmse: 0.58776 |  0:00:33s
epoch 28 | loss: 0.2804  | val_0_rmse: 0.53025 | val_1_rmse: 0.55706 |  0:00:34s
epoch 29 | loss: 0.27821 | val_0_rmse: 0.52503 | val_1_rmse: 0.55775 |  0:00:35s
epoch 30 | loss: 0.27588 | val_0_rmse: 0.52936 | val_1_rmse: 0.56027 |  0:00:36s
epoch 31 | loss: 0.27537 | val_0_rmse: 0.53478 | val_1_rmse: 0.56539 |  0:00:37s
epoch 32 | loss: 0.27193 | val_0_rmse: 0.53167 | val_1_rmse: 0.56409 |  0:00:38s
epoch 33 | loss: 0.27704 | val_0_rmse: 0.51856 | val_1_rmse: 0.55547 |  0:00:40s
epoch 34 | loss: 0.28414 | val_0_rmse: 0.56621 | val_1_rmse: 0.60162 |  0:00:41s
epoch 35 | loss: 0.29762 | val_0_rmse: 0.54722 | val_1_rmse: 0.57914 |  0:00:42s
epoch 36 | loss: 0.30047 | val_0_rmse: 0.5279  | val_1_rmse: 0.55761 |  0:00:43s
epoch 37 | loss: 0.30077 | val_0_rmse: 0.53918 | val_1_rmse: 0.56517 |  0:00:44s
epoch 38 | loss: 0.28701 | val_0_rmse: 0.5261  | val_1_rmse: 0.55109 |  0:00:45s
epoch 39 | loss: 0.28233 | val_0_rmse: 0.52071 | val_1_rmse: 0.5522  |  0:00:47s
epoch 40 | loss: 0.28637 | val_0_rmse: 0.52204 | val_1_rmse: 0.55085 |  0:00:48s
epoch 41 | loss: 0.27589 | val_0_rmse: 0.51652 | val_1_rmse: 0.54872 |  0:00:49s
epoch 42 | loss: 0.27497 | val_0_rmse: 0.5151  | val_1_rmse: 0.54707 |  0:00:50s
epoch 43 | loss: 0.27204 | val_0_rmse: 0.51246 | val_1_rmse: 0.54996 |  0:00:51s
epoch 44 | loss: 0.27126 | val_0_rmse: 0.5101  | val_1_rmse: 0.54583 |  0:00:53s
epoch 45 | loss: 0.26767 | val_0_rmse: 0.5072  | val_1_rmse: 0.54053 |  0:00:54s
epoch 46 | loss: 0.27244 | val_0_rmse: 0.50935 | val_1_rmse: 0.54618 |  0:00:55s
epoch 47 | loss: 0.26863 | val_0_rmse: 0.5093  | val_1_rmse: 0.54633 |  0:00:56s
epoch 48 | loss: 0.2682  | val_0_rmse: 0.50529 | val_1_rmse: 0.53823 |  0:00:57s
epoch 49 | loss: 0.27443 | val_0_rmse: 0.50538 | val_1_rmse: 0.54041 |  0:00:58s
epoch 50 | loss: 0.28713 | val_0_rmse: 0.52183 | val_1_rmse: 0.54794 |  0:01:00s
epoch 51 | loss: 0.27955 | val_0_rmse: 0.51789 | val_1_rmse: 0.54607 |  0:01:01s
epoch 52 | loss: 0.2811  | val_0_rmse: 0.53532 | val_1_rmse: 0.56635 |  0:01:02s
epoch 53 | loss: 0.29713 | val_0_rmse: 0.52499 | val_1_rmse: 0.55347 |  0:01:03s
epoch 54 | loss: 0.28254 | val_0_rmse: 0.51277 | val_1_rmse: 0.54391 |  0:01:04s
epoch 55 | loss: 0.2703  | val_0_rmse: 0.50818 | val_1_rmse: 0.53978 |  0:01:05s
epoch 56 | loss: 0.26798 | val_0_rmse: 0.50056 | val_1_rmse: 0.53423 |  0:01:07s
epoch 57 | loss: 0.26158 | val_0_rmse: 0.5016  | val_1_rmse: 0.53799 |  0:01:08s
epoch 58 | loss: 0.26333 | val_0_rmse: 0.50254 | val_1_rmse: 0.54413 |  0:01:09s
epoch 59 | loss: 0.25712 | val_0_rmse: 0.49579 | val_1_rmse: 0.53461 |  0:01:10s
epoch 60 | loss: 0.25378 | val_0_rmse: 0.49369 | val_1_rmse: 0.53664 |  0:01:11s
epoch 61 | loss: 0.2549  | val_0_rmse: 0.49248 | val_1_rmse: 0.53695 |  0:01:13s
epoch 62 | loss: 0.25231 | val_0_rmse: 0.49205 | val_1_rmse: 0.53707 |  0:01:14s
epoch 63 | loss: 0.25357 | val_0_rmse: 0.50115 | val_1_rmse: 0.54556 |  0:01:15s
epoch 64 | loss: 0.25436 | val_0_rmse: 0.49291 | val_1_rmse: 0.54192 |  0:01:16s
epoch 65 | loss: 0.25168 | val_0_rmse: 0.48932 | val_1_rmse: 0.53534 |  0:01:17s
epoch 66 | loss: 0.25033 | val_0_rmse: 0.49411 | val_1_rmse: 0.5389  |  0:01:18s
epoch 67 | loss: 0.25091 | val_0_rmse: 0.4928  | val_1_rmse: 0.53786 |  0:01:20s
epoch 68 | loss: 0.25135 | val_0_rmse: 0.48719 | val_1_rmse: 0.53878 |  0:01:21s
epoch 69 | loss: 0.24843 | val_0_rmse: 0.48615 | val_1_rmse: 0.53344 |  0:01:22s
epoch 70 | loss: 0.24609 | val_0_rmse: 0.49    | val_1_rmse: 0.53895 |  0:01:23s
epoch 71 | loss: 0.24737 | val_0_rmse: 0.48437 | val_1_rmse: 0.53542 |  0:01:24s
epoch 72 | loss: 0.24566 | val_0_rmse: 0.48616 | val_1_rmse: 0.53641 |  0:01:25s
epoch 73 | loss: 0.24812 | val_0_rmse: 0.48822 | val_1_rmse: 0.53885 |  0:01:27s
epoch 74 | loss: 0.24737 | val_0_rmse: 0.48632 | val_1_rmse: 0.53732 |  0:01:28s
epoch 75 | loss: 0.24743 | val_0_rmse: 0.48435 | val_1_rmse: 0.53732 |  0:01:29s
epoch 76 | loss: 0.2457  | val_0_rmse: 0.48783 | val_1_rmse: 0.54131 |  0:01:30s
epoch 77 | loss: 0.24831 | val_0_rmse: 0.48463 | val_1_rmse: 0.5405  |  0:01:31s
epoch 78 | loss: 0.24514 | val_0_rmse: 0.48372 | val_1_rmse: 0.53412 |  0:01:33s
epoch 79 | loss: 0.24719 | val_0_rmse: 0.48204 | val_1_rmse: 0.53837 |  0:01:34s
epoch 80 | loss: 0.24123 | val_0_rmse: 0.48337 | val_1_rmse: 0.53751 |  0:01:35s
epoch 81 | loss: 0.24009 | val_0_rmse: 0.48419 | val_1_rmse: 0.54148 |  0:01:36s
epoch 82 | loss: 0.23897 | val_0_rmse: 0.48723 | val_1_rmse: 0.54429 |  0:01:37s
epoch 83 | loss: 0.24033 | val_0_rmse: 0.47963 | val_1_rmse: 0.53931 |  0:01:38s
epoch 84 | loss: 0.23964 | val_0_rmse: 0.48025 | val_1_rmse: 0.53737 |  0:01:40s
epoch 85 | loss: 0.23665 | val_0_rmse: 0.47853 | val_1_rmse: 0.53446 |  0:01:41s
epoch 86 | loss: 0.23911 | val_0_rmse: 0.47613 | val_1_rmse: 0.53471 |  0:01:42s
epoch 87 | loss: 0.23834 | val_0_rmse: 0.47535 | val_1_rmse: 0.53717 |  0:01:43s
epoch 88 | loss: 0.23592 | val_0_rmse: 0.48217 | val_1_rmse: 0.54121 |  0:01:44s
epoch 89 | loss: 0.23655 | val_0_rmse: 0.47733 | val_1_rmse: 0.53379 |  0:01:45s
epoch 90 | loss: 0.23461 | val_0_rmse: 0.49882 | val_1_rmse: 0.55345 |  0:01:47s
epoch 91 | loss: 0.2501  | val_0_rmse: 0.48933 | val_1_rmse: 0.54482 |  0:01:48s
epoch 92 | loss: 0.24548 | val_0_rmse: 0.48396 | val_1_rmse: 0.53852 |  0:01:49s
epoch 93 | loss: 0.24282 | val_0_rmse: 0.47797 | val_1_rmse: 0.53824 |  0:01:50s
epoch 94 | loss: 0.23811 | val_0_rmse: 0.47917 | val_1_rmse: 0.54311 |  0:01:51s
epoch 95 | loss: 0.23719 | val_0_rmse: 0.47638 | val_1_rmse: 0.53737 |  0:01:52s
epoch 96 | loss: 0.2373  | val_0_rmse: 0.48434 | val_1_rmse: 0.54559 |  0:01:54s
epoch 97 | loss: 0.23331 | val_0_rmse: 0.47671 | val_1_rmse: 0.54508 |  0:01:55s
epoch 98 | loss: 0.2354  | val_0_rmse: 0.4737  | val_1_rmse: 0.54002 |  0:01:56s
epoch 99 | loss: 0.23525 | val_0_rmse: 0.49027 | val_1_rmse: 0.56    |  0:01:57s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.53344
Best weights from best epoch are automatically used!
ended training at: 07:51:34
Feature importance:
Mean squared error is of 8715939666.4954
Mean absolute error:68380.3530258173
MAPE:0.1799988167977059
R2 score:0.7126441584567025
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:51:34
epoch 0  | loss: 1.25247 | val_0_rmse: 0.99727 | val_1_rmse: 0.99691 |  0:00:01s
epoch 1  | loss: 0.76943 | val_0_rmse: 0.85263 | val_1_rmse: 0.87057 |  0:00:02s
epoch 2  | loss: 0.57885 | val_0_rmse: 0.83095 | val_1_rmse: 0.8541  |  0:00:03s
epoch 3  | loss: 0.47602 | val_0_rmse: 0.78547 | val_1_rmse: 0.80761 |  0:00:04s
epoch 4  | loss: 0.42753 | val_0_rmse: 0.77964 | val_1_rmse: 0.80435 |  0:00:05s
epoch 5  | loss: 0.38353 | val_0_rmse: 0.80848 | val_1_rmse: 0.83749 |  0:00:07s
epoch 6  | loss: 0.36287 | val_0_rmse: 0.79038 | val_1_rmse: 0.81386 |  0:00:08s
epoch 7  | loss: 0.34907 | val_0_rmse: 0.77201 | val_1_rmse: 0.79569 |  0:00:09s
epoch 8  | loss: 0.34787 | val_0_rmse: 0.74619 | val_1_rmse: 0.77114 |  0:00:10s
epoch 9  | loss: 0.33292 | val_0_rmse: 0.69515 | val_1_rmse: 0.71628 |  0:00:11s
epoch 10 | loss: 0.3327  | val_0_rmse: 0.74246 | val_1_rmse: 0.76489 |  0:00:13s
epoch 11 | loss: 0.31937 | val_0_rmse: 0.6931  | val_1_rmse: 0.71784 |  0:00:14s
epoch 12 | loss: 0.3162  | val_0_rmse: 0.70418 | val_1_rmse: 0.73026 |  0:00:15s
epoch 13 | loss: 0.31185 | val_0_rmse: 0.72362 | val_1_rmse: 0.7526  |  0:00:16s
epoch 14 | loss: 0.30803 | val_0_rmse: 0.71291 | val_1_rmse: 0.73941 |  0:00:17s
epoch 15 | loss: 0.30835 | val_0_rmse: 0.68022 | val_1_rmse: 0.70705 |  0:00:18s
epoch 16 | loss: 0.30623 | val_0_rmse: 0.64939 | val_1_rmse: 0.67436 |  0:00:20s
epoch 17 | loss: 0.31189 | val_0_rmse: 0.6832  | val_1_rmse: 0.7052  |  0:00:21s
epoch 18 | loss: 0.3046  | val_0_rmse: 0.65165 | val_1_rmse: 0.6761  |  0:00:22s
epoch 19 | loss: 0.30635 | val_0_rmse: 0.6324  | val_1_rmse: 0.6569  |  0:00:23s
epoch 20 | loss: 0.29733 | val_0_rmse: 0.65963 | val_1_rmse: 0.6886  |  0:00:24s
epoch 21 | loss: 0.30076 | val_0_rmse: 0.62691 | val_1_rmse: 0.65206 |  0:00:25s
epoch 22 | loss: 0.29706 | val_0_rmse: 0.63364 | val_1_rmse: 0.65575 |  0:00:27s
epoch 23 | loss: 0.29753 | val_0_rmse: 0.64495 | val_1_rmse: 0.65994 |  0:00:28s
epoch 24 | loss: 0.30441 | val_0_rmse: 0.62311 | val_1_rmse: 0.63562 |  0:00:29s
epoch 25 | loss: 0.3027  | val_0_rmse: 0.58263 | val_1_rmse: 0.59795 |  0:00:30s
epoch 26 | loss: 0.29808 | val_0_rmse: 0.57539 | val_1_rmse: 0.58842 |  0:00:31s
epoch 27 | loss: 0.29893 | val_0_rmse: 0.55739 | val_1_rmse: 0.5704  |  0:00:33s
epoch 28 | loss: 0.29532 | val_0_rmse: 0.57048 | val_1_rmse: 0.58411 |  0:00:34s
epoch 29 | loss: 0.31571 | val_0_rmse: 0.5747  | val_1_rmse: 0.59561 |  0:00:35s
epoch 30 | loss: 0.31267 | val_0_rmse: 0.55383 | val_1_rmse: 0.56938 |  0:00:36s
epoch 31 | loss: 0.31066 | val_0_rmse: 0.55297 | val_1_rmse: 0.56716 |  0:00:37s
epoch 32 | loss: 0.29484 | val_0_rmse: 0.53213 | val_1_rmse: 0.55275 |  0:00:38s
epoch 33 | loss: 0.2881  | val_0_rmse: 0.53742 | val_1_rmse: 0.55627 |  0:00:40s
epoch 34 | loss: 0.3128  | val_0_rmse: 0.55659 | val_1_rmse: 0.58424 |  0:00:41s
epoch 35 | loss: 0.30158 | val_0_rmse: 0.54039 | val_1_rmse: 0.56143 |  0:00:42s
epoch 36 | loss: 0.29758 | val_0_rmse: 0.53718 | val_1_rmse: 0.5551  |  0:00:43s
epoch 37 | loss: 0.29431 | val_0_rmse: 0.53986 | val_1_rmse: 0.56039 |  0:00:44s
epoch 38 | loss: 0.29293 | val_0_rmse: 0.53663 | val_1_rmse: 0.55169 |  0:00:45s
epoch 39 | loss: 0.30631 | val_0_rmse: 0.55902 | val_1_rmse: 0.57098 |  0:00:47s
epoch 40 | loss: 0.31641 | val_0_rmse: 0.55999 | val_1_rmse: 0.57746 |  0:00:48s
epoch 41 | loss: 0.31542 | val_0_rmse: 0.54851 | val_1_rmse: 0.57673 |  0:00:49s
epoch 42 | loss: 0.30885 | val_0_rmse: 0.54077 | val_1_rmse: 0.56258 |  0:00:50s
epoch 43 | loss: 0.29826 | val_0_rmse: 0.53058 | val_1_rmse: 0.54939 |  0:00:51s
epoch 44 | loss: 0.29126 | val_0_rmse: 0.52851 | val_1_rmse: 0.5493  |  0:00:53s
epoch 45 | loss: 0.29199 | val_0_rmse: 0.53634 | val_1_rmse: 0.55841 |  0:00:54s
epoch 46 | loss: 0.28976 | val_0_rmse: 0.52705 | val_1_rmse: 0.5517  |  0:00:55s
epoch 47 | loss: 0.28678 | val_0_rmse: 0.52356 | val_1_rmse: 0.54926 |  0:00:56s
epoch 48 | loss: 0.28378 | val_0_rmse: 0.52305 | val_1_rmse: 0.54408 |  0:00:57s
epoch 49 | loss: 0.28276 | val_0_rmse: 0.52175 | val_1_rmse: 0.54519 |  0:00:58s
epoch 50 | loss: 0.28331 | val_0_rmse: 0.51926 | val_1_rmse: 0.54393 |  0:01:00s
epoch 51 | loss: 0.28244 | val_0_rmse: 0.52359 | val_1_rmse: 0.54794 |  0:01:01s
epoch 52 | loss: 0.27998 | val_0_rmse: 0.52273 | val_1_rmse: 0.54601 |  0:01:02s
epoch 53 | loss: 0.28238 | val_0_rmse: 0.52291 | val_1_rmse: 0.54409 |  0:01:03s
epoch 54 | loss: 0.28575 | val_0_rmse: 0.52409 | val_1_rmse: 0.54761 |  0:01:04s
epoch 55 | loss: 0.29113 | val_0_rmse: 0.53601 | val_1_rmse: 0.556   |  0:01:06s
epoch 56 | loss: 0.29468 | val_0_rmse: 0.5391  | val_1_rmse: 0.55422 |  0:01:07s
epoch 57 | loss: 0.29739 | val_0_rmse: 0.54094 | val_1_rmse: 0.56398 |  0:01:08s
epoch 58 | loss: 0.2972  | val_0_rmse: 0.52811 | val_1_rmse: 0.54533 |  0:01:09s
epoch 59 | loss: 0.29063 | val_0_rmse: 0.53906 | val_1_rmse: 0.55501 |  0:01:10s
epoch 60 | loss: 0.29416 | val_0_rmse: 0.52889 | val_1_rmse: 0.54507 |  0:01:11s
epoch 61 | loss: 0.28488 | val_0_rmse: 0.52595 | val_1_rmse: 0.54299 |  0:01:13s
epoch 62 | loss: 0.28269 | val_0_rmse: 0.5281  | val_1_rmse: 0.54465 |  0:01:14s
epoch 63 | loss: 0.29624 | val_0_rmse: 0.53977 | val_1_rmse: 0.55869 |  0:01:15s
epoch 64 | loss: 0.29406 | val_0_rmse: 0.53273 | val_1_rmse: 0.55438 |  0:01:16s
epoch 65 | loss: 0.2893  | val_0_rmse: 0.53215 | val_1_rmse: 0.55215 |  0:01:17s
epoch 66 | loss: 0.29118 | val_0_rmse: 0.52383 | val_1_rmse: 0.54569 |  0:01:18s
epoch 67 | loss: 0.28088 | val_0_rmse: 0.52182 | val_1_rmse: 0.54287 |  0:01:20s
epoch 68 | loss: 0.28215 | val_0_rmse: 0.52031 | val_1_rmse: 0.53858 |  0:01:21s
epoch 69 | loss: 0.28044 | val_0_rmse: 0.52179 | val_1_rmse: 0.53838 |  0:01:22s
epoch 70 | loss: 0.27991 | val_0_rmse: 0.51819 | val_1_rmse: 0.54199 |  0:01:23s
epoch 71 | loss: 0.27803 | val_0_rmse: 0.51843 | val_1_rmse: 0.54099 |  0:01:24s
epoch 72 | loss: 0.2775  | val_0_rmse: 0.51567 | val_1_rmse: 0.53744 |  0:01:26s
epoch 73 | loss: 0.27704 | val_0_rmse: 0.51515 | val_1_rmse: 0.5381  |  0:01:27s
epoch 74 | loss: 0.27263 | val_0_rmse: 0.51191 | val_1_rmse: 0.53199 |  0:01:28s
epoch 75 | loss: 0.27227 | val_0_rmse: 0.51433 | val_1_rmse: 0.535   |  0:01:29s
epoch 76 | loss: 0.27223 | val_0_rmse: 0.51375 | val_1_rmse: 0.53413 |  0:01:30s
epoch 77 | loss: 0.2709  | val_0_rmse: 0.51116 | val_1_rmse: 0.53179 |  0:01:31s
epoch 78 | loss: 0.27134 | val_0_rmse: 0.51416 | val_1_rmse: 0.53713 |  0:01:33s
epoch 79 | loss: 0.27117 | val_0_rmse: 0.51093 | val_1_rmse: 0.53669 |  0:01:34s
epoch 80 | loss: 0.273   | val_0_rmse: 0.5164  | val_1_rmse: 0.53821 |  0:01:35s
epoch 81 | loss: 0.27274 | val_0_rmse: 0.50988 | val_1_rmse: 0.52995 |  0:01:36s
epoch 82 | loss: 0.27625 | val_0_rmse: 0.5347  | val_1_rmse: 0.5484  |  0:01:37s
epoch 83 | loss: 0.28901 | val_0_rmse: 0.52246 | val_1_rmse: 0.54327 |  0:01:38s
epoch 84 | loss: 0.27871 | val_0_rmse: 0.52181 | val_1_rmse: 0.54254 |  0:01:40s
epoch 85 | loss: 0.27975 | val_0_rmse: 0.51783 | val_1_rmse: 0.53726 |  0:01:41s
epoch 86 | loss: 0.27428 | val_0_rmse: 0.51468 | val_1_rmse: 0.53327 |  0:01:42s
epoch 87 | loss: 0.27341 | val_0_rmse: 0.51807 | val_1_rmse: 0.53746 |  0:01:43s
epoch 88 | loss: 0.27352 | val_0_rmse: 0.51344 | val_1_rmse: 0.53265 |  0:01:44s
epoch 89 | loss: 0.27154 | val_0_rmse: 0.51437 | val_1_rmse: 0.53426 |  0:01:46s
epoch 90 | loss: 0.27099 | val_0_rmse: 0.51564 | val_1_rmse: 0.53919 |  0:01:47s
epoch 91 | loss: 0.26966 | val_0_rmse: 0.50976 | val_1_rmse: 0.53332 |  0:01:48s
epoch 92 | loss: 0.27073 | val_0_rmse: 0.51301 | val_1_rmse: 0.53424 |  0:01:49s
epoch 93 | loss: 0.27398 | val_0_rmse: 0.50808 | val_1_rmse: 0.53121 |  0:01:50s
epoch 94 | loss: 0.2739  | val_0_rmse: 0.52174 | val_1_rmse: 0.54322 |  0:01:51s
epoch 95 | loss: 0.27937 | val_0_rmse: 0.51886 | val_1_rmse: 0.54168 |  0:01:53s
epoch 96 | loss: 0.27524 | val_0_rmse: 0.51383 | val_1_rmse: 0.5349  |  0:01:54s
epoch 97 | loss: 0.27457 | val_0_rmse: 0.51333 | val_1_rmse: 0.53395 |  0:01:55s
epoch 98 | loss: 0.27182 | val_0_rmse: 0.50879 | val_1_rmse: 0.53398 |  0:01:56s
epoch 99 | loss: 0.26861 | val_0_rmse: 0.50893 | val_1_rmse: 0.53481 |  0:01:57s
epoch 100| loss: 0.26896 | val_0_rmse: 0.50863 | val_1_rmse: 0.53364 |  0:01:58s
epoch 101| loss: 0.26557 | val_0_rmse: 0.50706 | val_1_rmse: 0.53475 |  0:02:00s
epoch 102| loss: 0.26629 | val_0_rmse: 0.50899 | val_1_rmse: 0.53793 |  0:02:01s
epoch 103| loss: 0.26761 | val_0_rmse: 0.50881 | val_1_rmse: 0.53822 |  0:02:02s
epoch 104| loss: 0.27333 | val_0_rmse: 0.51823 | val_1_rmse: 0.544   |  0:02:03s
epoch 105| loss: 0.2761  | val_0_rmse: 0.51727 | val_1_rmse: 0.53884 |  0:02:04s
epoch 106| loss: 0.27186 | val_0_rmse: 0.51084 | val_1_rmse: 0.53586 |  0:02:05s
epoch 107| loss: 0.27032 | val_0_rmse: 0.50705 | val_1_rmse: 0.53467 |  0:02:07s
epoch 108| loss: 0.27012 | val_0_rmse: 0.50922 | val_1_rmse: 0.53574 |  0:02:08s
epoch 109| loss: 0.27341 | val_0_rmse: 0.50988 | val_1_rmse: 0.53095 |  0:02:09s
epoch 110| loss: 0.26787 | val_0_rmse: 0.51283 | val_1_rmse: 0.53981 |  0:02:10s
epoch 111| loss: 0.26666 | val_0_rmse: 0.51044 | val_1_rmse: 0.53574 |  0:02:11s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.52995
Best weights from best epoch are automatically used!
ended training at: 07:53:47
Feature importance:
Mean squared error is of 8396399878.232228
Mean absolute error:67485.98415069058
MAPE:0.17770395874309633
R2 score:0.7289297823412424
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:53:47
epoch 0  | loss: 1.35633 | val_0_rmse: 0.96205 | val_1_rmse: 0.96351 |  0:00:01s
epoch 1  | loss: 0.81357 | val_0_rmse: 0.87278 | val_1_rmse: 0.85333 |  0:00:02s
epoch 2  | loss: 0.59467 | val_0_rmse: 0.78409 | val_1_rmse: 0.77107 |  0:00:03s
epoch 3  | loss: 0.49863 | val_0_rmse: 0.9286  | val_1_rmse: 0.92041 |  0:00:04s
epoch 4  | loss: 0.43768 | val_0_rmse: 0.87158 | val_1_rmse: 0.84148 |  0:00:05s
epoch 5  | loss: 0.41543 | val_0_rmse: 0.85346 | val_1_rmse: 0.82189 |  0:00:07s
epoch 6  | loss: 0.38644 | val_0_rmse: 0.83106 | val_1_rmse: 0.80681 |  0:00:08s
epoch 7  | loss: 0.37735 | val_0_rmse: 0.84245 | val_1_rmse: 0.81841 |  0:00:09s
epoch 8  | loss: 0.3634  | val_0_rmse: 0.79493 | val_1_rmse: 0.77431 |  0:00:10s
epoch 9  | loss: 0.34992 | val_0_rmse: 0.80953 | val_1_rmse: 0.78593 |  0:00:11s
epoch 10 | loss: 0.34317 | val_0_rmse: 0.7579  | val_1_rmse: 0.7404  |  0:00:12s
epoch 11 | loss: 0.34008 | val_0_rmse: 0.76871 | val_1_rmse: 0.75209 |  0:00:14s
epoch 12 | loss: 0.33877 | val_0_rmse: 0.75136 | val_1_rmse: 0.74129 |  0:00:15s
epoch 13 | loss: 0.33601 | val_0_rmse: 0.84475 | val_1_rmse: 0.83178 |  0:00:16s
epoch 14 | loss: 0.33826 | val_0_rmse: 0.75346 | val_1_rmse: 0.73699 |  0:00:17s
epoch 15 | loss: 0.32692 | val_0_rmse: 0.71728 | val_1_rmse: 0.69883 |  0:00:18s
epoch 16 | loss: 0.32345 | val_0_rmse: 0.6935  | val_1_rmse: 0.66989 |  0:00:20s
epoch 17 | loss: 0.319   | val_0_rmse: 0.69108 | val_1_rmse: 0.66519 |  0:00:21s
epoch 18 | loss: 0.31785 | val_0_rmse: 0.66947 | val_1_rmse: 0.65217 |  0:00:22s
epoch 19 | loss: 0.31567 | val_0_rmse: 0.68471 | val_1_rmse: 0.66822 |  0:00:23s
epoch 20 | loss: 0.30924 | val_0_rmse: 0.65798 | val_1_rmse: 0.6468  |  0:00:24s
epoch 21 | loss: 0.31788 | val_0_rmse: 0.65597 | val_1_rmse: 0.65461 |  0:00:25s
epoch 22 | loss: 0.30721 | val_0_rmse: 0.62495 | val_1_rmse: 0.61452 |  0:00:27s
epoch 23 | loss: 0.30251 | val_0_rmse: 0.62799 | val_1_rmse: 0.6242  |  0:00:28s
epoch 24 | loss: 0.30192 | val_0_rmse: 0.62276 | val_1_rmse: 0.61838 |  0:00:29s
epoch 25 | loss: 0.30618 | val_0_rmse: 0.61347 | val_1_rmse: 0.60795 |  0:00:30s
epoch 26 | loss: 0.30728 | val_0_rmse: 0.7561  | val_1_rmse: 0.76644 |  0:00:31s
epoch 27 | loss: 0.38371 | val_0_rmse: 0.62711 | val_1_rmse: 0.62233 |  0:00:33s
epoch 28 | loss: 0.34667 | val_0_rmse: 0.59353 | val_1_rmse: 0.59889 |  0:00:34s
epoch 29 | loss: 0.32401 | val_0_rmse: 0.57671 | val_1_rmse: 0.58869 |  0:00:35s
epoch 30 | loss: 0.32121 | val_0_rmse: 0.5544  | val_1_rmse: 0.56546 |  0:00:36s
epoch 31 | loss: 0.30965 | val_0_rmse: 0.55299 | val_1_rmse: 0.56157 |  0:00:37s
epoch 32 | loss: 0.32175 | val_0_rmse: 0.56969 | val_1_rmse: 0.57942 |  0:00:38s
epoch 33 | loss: 0.31666 | val_0_rmse: 0.55702 | val_1_rmse: 0.5697  |  0:00:40s
epoch 34 | loss: 0.30573 | val_0_rmse: 0.55278 | val_1_rmse: 0.56336 |  0:00:41s
epoch 35 | loss: 0.30583 | val_0_rmse: 0.68577 | val_1_rmse: 0.69523 |  0:00:42s
epoch 36 | loss: 0.3102  | val_0_rmse: 0.54366 | val_1_rmse: 0.55603 |  0:00:43s
epoch 37 | loss: 0.29815 | val_0_rmse: 0.54309 | val_1_rmse: 0.55129 |  0:00:44s
epoch 38 | loss: 0.29475 | val_0_rmse: 0.53193 | val_1_rmse: 0.54281 |  0:00:46s
epoch 39 | loss: 0.29306 | val_0_rmse: 0.53011 | val_1_rmse: 0.53951 |  0:00:47s
epoch 40 | loss: 0.29633 | val_0_rmse: 0.53729 | val_1_rmse: 0.54339 |  0:00:48s
epoch 41 | loss: 0.28791 | val_0_rmse: 0.52766 | val_1_rmse: 0.54148 |  0:00:49s
epoch 42 | loss: 0.28864 | val_0_rmse: 0.52748 | val_1_rmse: 0.53749 |  0:00:50s
epoch 43 | loss: 0.28397 | val_0_rmse: 0.52522 | val_1_rmse: 0.53421 |  0:00:51s
epoch 44 | loss: 0.28231 | val_0_rmse: 0.51988 | val_1_rmse: 0.53389 |  0:00:53s
epoch 45 | loss: 0.27989 | val_0_rmse: 0.52255 | val_1_rmse: 0.53645 |  0:00:54s
epoch 46 | loss: 0.27901 | val_0_rmse: 0.52006 | val_1_rmse: 0.53271 |  0:00:55s
epoch 47 | loss: 0.27916 | val_0_rmse: 0.52048 | val_1_rmse: 0.5345  |  0:00:56s
epoch 48 | loss: 0.27859 | val_0_rmse: 0.51941 | val_1_rmse: 0.53201 |  0:00:57s
epoch 49 | loss: 0.27866 | val_0_rmse: 0.51469 | val_1_rmse: 0.5334  |  0:00:58s
epoch 50 | loss: 0.27759 | val_0_rmse: 0.51623 | val_1_rmse: 0.53174 |  0:01:00s
epoch 51 | loss: 0.27716 | val_0_rmse: 0.51769 | val_1_rmse: 0.53477 |  0:01:01s
epoch 52 | loss: 0.27586 | val_0_rmse: 0.51151 | val_1_rmse: 0.52979 |  0:01:02s
epoch 53 | loss: 0.27069 | val_0_rmse: 0.51646 | val_1_rmse: 0.53118 |  0:01:03s
epoch 54 | loss: 0.27754 | val_0_rmse: 0.52046 | val_1_rmse: 0.54148 |  0:01:04s
epoch 55 | loss: 0.28414 | val_0_rmse: 0.51674 | val_1_rmse: 0.53451 |  0:01:06s
epoch 56 | loss: 0.27845 | val_0_rmse: 0.53729 | val_1_rmse: 0.56056 |  0:01:07s
epoch 57 | loss: 0.27795 | val_0_rmse: 0.51641 | val_1_rmse: 0.53533 |  0:01:08s
epoch 58 | loss: 0.27401 | val_0_rmse: 0.51065 | val_1_rmse: 0.5373  |  0:01:09s
epoch 59 | loss: 0.27438 | val_0_rmse: 0.51497 | val_1_rmse: 0.53558 |  0:01:10s
epoch 60 | loss: 0.2718  | val_0_rmse: 0.50926 | val_1_rmse: 0.53216 |  0:01:11s
epoch 61 | loss: 0.26721 | val_0_rmse: 0.50563 | val_1_rmse: 0.53126 |  0:01:13s
epoch 62 | loss: 0.26814 | val_0_rmse: 0.51153 | val_1_rmse: 0.53421 |  0:01:14s
epoch 63 | loss: 0.26871 | val_0_rmse: 0.50526 | val_1_rmse: 0.53082 |  0:01:15s
epoch 64 | loss: 0.26661 | val_0_rmse: 0.50775 | val_1_rmse: 0.52672 |  0:01:16s
epoch 65 | loss: 0.2658  | val_0_rmse: 0.51442 | val_1_rmse: 0.53742 |  0:01:17s
epoch 66 | loss: 0.26581 | val_0_rmse: 0.50637 | val_1_rmse: 0.52874 |  0:01:18s
epoch 67 | loss: 0.26686 | val_0_rmse: 0.50159 | val_1_rmse: 0.52645 |  0:01:20s
epoch 68 | loss: 0.26665 | val_0_rmse: 0.50022 | val_1_rmse: 0.52694 |  0:01:21s
epoch 69 | loss: 0.26396 | val_0_rmse: 0.50193 | val_1_rmse: 0.52926 |  0:01:22s
epoch 70 | loss: 0.26194 | val_0_rmse: 0.50008 | val_1_rmse: 0.52928 |  0:01:23s
epoch 71 | loss: 0.26233 | val_0_rmse: 0.50412 | val_1_rmse: 0.53208 |  0:01:24s
epoch 72 | loss: 0.26417 | val_0_rmse: 0.49698 | val_1_rmse: 0.52649 |  0:01:26s
epoch 73 | loss: 0.26056 | val_0_rmse: 0.49773 | val_1_rmse: 0.52473 |  0:01:27s
epoch 74 | loss: 0.26187 | val_0_rmse: 0.50042 | val_1_rmse: 0.52452 |  0:01:28s
epoch 75 | loss: 0.26295 | val_0_rmse: 0.49912 | val_1_rmse: 0.52796 |  0:01:29s
epoch 76 | loss: 0.26386 | val_0_rmse: 0.50043 | val_1_rmse: 0.52548 |  0:01:30s
epoch 77 | loss: 0.26806 | val_0_rmse: 0.50538 | val_1_rmse: 0.5305  |  0:01:31s
epoch 78 | loss: 0.26475 | val_0_rmse: 0.50357 | val_1_rmse: 0.53123 |  0:01:33s
epoch 79 | loss: 0.26107 | val_0_rmse: 0.49992 | val_1_rmse: 0.52858 |  0:01:34s
epoch 80 | loss: 0.25849 | val_0_rmse: 0.49626 | val_1_rmse: 0.5292  |  0:01:35s
epoch 81 | loss: 0.25865 | val_0_rmse: 0.49658 | val_1_rmse: 0.52486 |  0:01:36s
epoch 82 | loss: 0.2588  | val_0_rmse: 0.49393 | val_1_rmse: 0.52689 |  0:01:37s
epoch 83 | loss: 0.25587 | val_0_rmse: 0.49936 | val_1_rmse: 0.53673 |  0:01:38s
epoch 84 | loss: 0.2635  | val_0_rmse: 0.49747 | val_1_rmse: 0.52438 |  0:01:40s
epoch 85 | loss: 0.25593 | val_0_rmse: 0.49962 | val_1_rmse: 0.53238 |  0:01:41s
epoch 86 | loss: 0.25745 | val_0_rmse: 0.49599 | val_1_rmse: 0.52614 |  0:01:42s
epoch 87 | loss: 0.25691 | val_0_rmse: 0.49629 | val_1_rmse: 0.52601 |  0:01:43s
epoch 88 | loss: 0.25563 | val_0_rmse: 0.4964  | val_1_rmse: 0.53533 |  0:01:44s
epoch 89 | loss: 0.25809 | val_0_rmse: 0.49614 | val_1_rmse: 0.52652 |  0:01:46s
epoch 90 | loss: 0.2549  | val_0_rmse: 0.4963  | val_1_rmse: 0.52978 |  0:01:47s
epoch 91 | loss: 0.25703 | val_0_rmse: 0.49172 | val_1_rmse: 0.52766 |  0:01:48s
epoch 92 | loss: 0.25376 | val_0_rmse: 0.49302 | val_1_rmse: 0.52554 |  0:01:49s
epoch 93 | loss: 0.25261 | val_0_rmse: 0.48989 | val_1_rmse: 0.52778 |  0:01:50s
epoch 94 | loss: 0.25161 | val_0_rmse: 0.48915 | val_1_rmse: 0.52352 |  0:01:51s
epoch 95 | loss: 0.25363 | val_0_rmse: 0.49271 | val_1_rmse: 0.52762 |  0:01:53s
epoch 96 | loss: 0.25529 | val_0_rmse: 0.49245 | val_1_rmse: 0.52291 |  0:01:54s
epoch 97 | loss: 0.25161 | val_0_rmse: 0.48968 | val_1_rmse: 0.52849 |  0:01:55s
epoch 98 | loss: 0.25391 | val_0_rmse: 0.4917  | val_1_rmse: 0.5256  |  0:01:56s
epoch 99 | loss: 0.2541  | val_0_rmse: 0.4918  | val_1_rmse: 0.53058 |  0:01:57s
epoch 100| loss: 0.25654 | val_0_rmse: 0.49147 | val_1_rmse: 0.52542 |  0:01:59s
epoch 101| loss: 0.25157 | val_0_rmse: 0.4891  | val_1_rmse: 0.52357 |  0:02:00s
epoch 102| loss: 0.25201 | val_0_rmse: 0.49312 | val_1_rmse: 0.52459 |  0:02:01s
epoch 103| loss: 0.25009 | val_0_rmse: 0.48787 | val_1_rmse: 0.52574 |  0:02:02s
epoch 104| loss: 0.24715 | val_0_rmse: 0.48965 | val_1_rmse: 0.52818 |  0:02:03s
epoch 105| loss: 0.24715 | val_0_rmse: 0.4931  | val_1_rmse: 0.54038 |  0:02:04s
epoch 106| loss: 0.25058 | val_0_rmse: 0.48722 | val_1_rmse: 0.5345  |  0:02:06s
epoch 107| loss: 0.2462  | val_0_rmse: 0.4879  | val_1_rmse: 0.52962 |  0:02:07s
epoch 108| loss: 0.24673 | val_0_rmse: 0.48402 | val_1_rmse: 0.52854 |  0:02:08s
epoch 109| loss: 0.24498 | val_0_rmse: 0.48784 | val_1_rmse: 0.53547 |  0:02:09s
epoch 110| loss: 0.24678 | val_0_rmse: 0.48666 | val_1_rmse: 0.5353  |  0:02:10s
epoch 111| loss: 0.24588 | val_0_rmse: 0.48376 | val_1_rmse: 0.53045 |  0:02:11s
epoch 112| loss: 0.25005 | val_0_rmse: 0.48614 | val_1_rmse: 0.53686 |  0:02:13s
epoch 113| loss: 0.24649 | val_0_rmse: 0.48537 | val_1_rmse: 0.53147 |  0:02:14s
epoch 114| loss: 0.24536 | val_0_rmse: 0.48111 | val_1_rmse: 0.53165 |  0:02:15s
epoch 115| loss: 0.2495  | val_0_rmse: 0.48724 | val_1_rmse: 0.54081 |  0:02:16s
epoch 116| loss: 0.24667 | val_0_rmse: 0.48768 | val_1_rmse: 0.53245 |  0:02:17s
epoch 117| loss: 0.24639 | val_0_rmse: 0.48281 | val_1_rmse: 0.52449 |  0:02:18s
epoch 118| loss: 0.24573 | val_0_rmse: 0.48326 | val_1_rmse: 0.53179 |  0:02:20s
epoch 119| loss: 0.2411  | val_0_rmse: 0.47881 | val_1_rmse: 0.53458 |  0:02:21s
epoch 120| loss: 0.24368 | val_0_rmse: 0.48044 | val_1_rmse: 0.5304  |  0:02:22s
epoch 121| loss: 0.24329 | val_0_rmse: 0.48218 | val_1_rmse: 0.53031 |  0:02:23s
epoch 122| loss: 0.24573 | val_0_rmse: 0.47791 | val_1_rmse: 0.5306  |  0:02:24s
epoch 123| loss: 0.24465 | val_0_rmse: 0.4805  | val_1_rmse: 0.5341  |  0:02:26s
epoch 124| loss: 0.24725 | val_0_rmse: 0.48591 | val_1_rmse: 0.52766 |  0:02:27s
epoch 125| loss: 0.24602 | val_0_rmse: 0.48095 | val_1_rmse: 0.52967 |  0:02:28s
epoch 126| loss: 0.24384 | val_0_rmse: 0.47989 | val_1_rmse: 0.52802 |  0:02:29s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.52291
Best weights from best epoch are automatically used!
ended training at: 07:56:17
Feature importance:
Mean squared error is of 8388426261.311279
Mean absolute error:67483.77533258364
MAPE:0.18149993072403875
R2 score:0.7351638838368735
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:56:17
epoch 0  | loss: 1.89115 | val_0_rmse: 0.99331 | val_1_rmse: 0.99865 |  0:00:00s
epoch 1  | loss: 1.12464 | val_0_rmse: 0.99925 | val_1_rmse: 1.00383 |  0:00:01s
epoch 2  | loss: 1.03015 | val_0_rmse: 0.99201 | val_1_rmse: 0.99747 |  0:00:01s
epoch 3  | loss: 0.98627 | val_0_rmse: 0.98327 | val_1_rmse: 0.98783 |  0:00:02s
epoch 4  | loss: 0.95018 | val_0_rmse: 0.96211 | val_1_rmse: 0.96753 |  0:00:03s
epoch 5  | loss: 0.86391 | val_0_rmse: 0.87747 | val_1_rmse: 0.87482 |  0:00:03s
epoch 6  | loss: 0.74916 | val_0_rmse: 0.86506 | val_1_rmse: 0.85483 |  0:00:04s
epoch 7  | loss: 0.62669 | val_0_rmse: 0.98419 | val_1_rmse: 0.95962 |  0:00:05s
epoch 8  | loss: 0.54138 | val_0_rmse: 0.9455  | val_1_rmse: 0.9321  |  0:00:05s
epoch 9  | loss: 0.51274 | val_0_rmse: 0.96283 | val_1_rmse: 0.95181 |  0:00:06s
epoch 10 | loss: 0.47228 | val_0_rmse: 0.94928 | val_1_rmse: 0.93846 |  0:00:07s
epoch 11 | loss: 0.43755 | val_0_rmse: 0.94365 | val_1_rmse: 0.93574 |  0:00:07s
epoch 12 | loss: 0.4058  | val_0_rmse: 0.89841 | val_1_rmse: 0.89126 |  0:00:08s
epoch 13 | loss: 0.37725 | val_0_rmse: 0.8926  | val_1_rmse: 0.87909 |  0:00:08s
epoch 14 | loss: 0.36252 | val_0_rmse: 0.79938 | val_1_rmse: 0.7854  |  0:00:09s
epoch 15 | loss: 0.34545 | val_0_rmse: 0.80868 | val_1_rmse: 0.79153 |  0:00:10s
epoch 16 | loss: 0.33106 | val_0_rmse: 0.78671 | val_1_rmse: 0.77214 |  0:00:10s
epoch 17 | loss: 0.3251  | val_0_rmse: 0.77134 | val_1_rmse: 0.75594 |  0:00:11s
epoch 18 | loss: 0.31558 | val_0_rmse: 0.79276 | val_1_rmse: 0.77841 |  0:00:12s
epoch 19 | loss: 0.30695 | val_0_rmse: 0.77798 | val_1_rmse: 0.76087 |  0:00:12s
epoch 20 | loss: 0.29894 | val_0_rmse: 0.77021 | val_1_rmse: 0.75421 |  0:00:13s
epoch 21 | loss: 0.29174 | val_0_rmse: 0.79063 | val_1_rmse: 0.77788 |  0:00:14s
epoch 22 | loss: 0.28357 | val_0_rmse: 0.74682 | val_1_rmse: 0.73166 |  0:00:14s
epoch 23 | loss: 0.28342 | val_0_rmse: 0.75419 | val_1_rmse: 0.7408  |  0:00:15s
epoch 24 | loss: 0.276   | val_0_rmse: 0.73191 | val_1_rmse: 0.71851 |  0:00:16s
epoch 25 | loss: 0.27383 | val_0_rmse: 0.74925 | val_1_rmse: 0.73667 |  0:00:16s
epoch 26 | loss: 0.26837 | val_0_rmse: 0.73348 | val_1_rmse: 0.71594 |  0:00:17s
epoch 27 | loss: 0.26987 | val_0_rmse: 0.72988 | val_1_rmse: 0.71499 |  0:00:17s
epoch 28 | loss: 0.26892 | val_0_rmse: 0.71656 | val_1_rmse: 0.70348 |  0:00:18s
epoch 29 | loss: 0.26657 | val_0_rmse: 0.73021 | val_1_rmse: 0.72206 |  0:00:19s
epoch 30 | loss: 0.26295 | val_0_rmse: 0.7003  | val_1_rmse: 0.68354 |  0:00:19s
epoch 31 | loss: 0.25993 | val_0_rmse: 0.71152 | val_1_rmse: 0.70062 |  0:00:20s
epoch 32 | loss: 0.25353 | val_0_rmse: 0.67923 | val_1_rmse: 0.66703 |  0:00:21s
epoch 33 | loss: 0.26027 | val_0_rmse: 0.67644 | val_1_rmse: 0.66757 |  0:00:21s
epoch 34 | loss: 0.26024 | val_0_rmse: 0.69723 | val_1_rmse: 0.68999 |  0:00:22s
epoch 35 | loss: 0.25911 | val_0_rmse: 0.68058 | val_1_rmse: 0.66652 |  0:00:23s
epoch 36 | loss: 0.26743 | val_0_rmse: 0.67164 | val_1_rmse: 0.66441 |  0:00:23s
epoch 37 | loss: 0.25844 | val_0_rmse: 0.65018 | val_1_rmse: 0.644   |  0:00:24s
epoch 38 | loss: 0.26351 | val_0_rmse: 0.66838 | val_1_rmse: 0.66198 |  0:00:25s
epoch 39 | loss: 0.25615 | val_0_rmse: 0.65262 | val_1_rmse: 0.65122 |  0:00:25s
epoch 40 | loss: 0.25175 | val_0_rmse: 0.67313 | val_1_rmse: 0.6699  |  0:00:26s
epoch 41 | loss: 0.24935 | val_0_rmse: 0.63754 | val_1_rmse: 0.64424 |  0:00:27s
epoch 42 | loss: 0.25347 | val_0_rmse: 0.65964 | val_1_rmse: 0.66487 |  0:00:27s
epoch 43 | loss: 0.24901 | val_0_rmse: 0.63818 | val_1_rmse: 0.63889 |  0:00:28s
epoch 44 | loss: 0.24529 | val_0_rmse: 0.62683 | val_1_rmse: 0.63092 |  0:00:28s
epoch 45 | loss: 0.25338 | val_0_rmse: 0.62043 | val_1_rmse: 0.62899 |  0:00:29s
epoch 46 | loss: 0.24589 | val_0_rmse: 0.65095 | val_1_rmse: 0.66206 |  0:00:30s
epoch 47 | loss: 0.25158 | val_0_rmse: 0.61132 | val_1_rmse: 0.6201  |  0:00:30s
epoch 48 | loss: 0.24911 | val_0_rmse: 0.61926 | val_1_rmse: 0.63218 |  0:00:31s
epoch 49 | loss: 0.24169 | val_0_rmse: 0.60224 | val_1_rmse: 0.61459 |  0:00:32s
epoch 50 | loss: 0.24358 | val_0_rmse: 0.61588 | val_1_rmse: 0.63341 |  0:00:32s
epoch 51 | loss: 0.24626 | val_0_rmse: 0.60027 | val_1_rmse: 0.61481 |  0:00:33s
epoch 52 | loss: 0.24042 | val_0_rmse: 0.60578 | val_1_rmse: 0.62158 |  0:00:34s
epoch 53 | loss: 0.23759 | val_0_rmse: 0.59389 | val_1_rmse: 0.61044 |  0:00:34s
epoch 54 | loss: 0.23638 | val_0_rmse: 0.58072 | val_1_rmse: 0.59984 |  0:00:35s
epoch 55 | loss: 0.23222 | val_0_rmse: 0.60147 | val_1_rmse: 0.61771 |  0:00:36s
epoch 56 | loss: 0.22571 | val_0_rmse: 0.57314 | val_1_rmse: 0.59446 |  0:00:36s
epoch 57 | loss: 0.22948 | val_0_rmse: 0.59579 | val_1_rmse: 0.60252 |  0:00:37s
epoch 58 | loss: 0.23769 | val_0_rmse: 0.56435 | val_1_rmse: 0.5769  |  0:00:37s
epoch 59 | loss: 0.23839 | val_0_rmse: 0.57104 | val_1_rmse: 0.57909 |  0:00:38s
epoch 60 | loss: 0.23397 | val_0_rmse: 0.58066 | val_1_rmse: 0.59172 |  0:00:39s
epoch 61 | loss: 0.23038 | val_0_rmse: 0.55901 | val_1_rmse: 0.57166 |  0:00:40s
epoch 62 | loss: 0.23374 | val_0_rmse: 0.54178 | val_1_rmse: 0.55788 |  0:00:40s
epoch 63 | loss: 0.2316  | val_0_rmse: 0.53765 | val_1_rmse: 0.55618 |  0:00:41s
epoch 64 | loss: 0.23165 | val_0_rmse: 0.54691 | val_1_rmse: 0.56501 |  0:00:41s
epoch 65 | loss: 0.22906 | val_0_rmse: 0.54952 | val_1_rmse: 0.5602  |  0:00:42s
epoch 66 | loss: 0.2316  | val_0_rmse: 0.53739 | val_1_rmse: 0.5561  |  0:00:43s
epoch 67 | loss: 0.22752 | val_0_rmse: 0.52494 | val_1_rmse: 0.54856 |  0:00:43s
epoch 68 | loss: 0.22569 | val_0_rmse: 0.51462 | val_1_rmse: 0.54126 |  0:00:44s
epoch 69 | loss: 0.21823 | val_0_rmse: 0.5157  | val_1_rmse: 0.54602 |  0:00:45s
epoch 70 | loss: 0.21727 | val_0_rmse: 0.50408 | val_1_rmse: 0.53901 |  0:00:45s
epoch 71 | loss: 0.21118 | val_0_rmse: 0.50707 | val_1_rmse: 0.54199 |  0:00:46s
epoch 72 | loss: 0.21234 | val_0_rmse: 0.50045 | val_1_rmse: 0.53948 |  0:00:47s
epoch 73 | loss: 0.2137  | val_0_rmse: 0.48869 | val_1_rmse: 0.54017 |  0:00:47s
epoch 74 | loss: 0.20934 | val_0_rmse: 0.50183 | val_1_rmse: 0.54047 |  0:00:48s
epoch 75 | loss: 0.21268 | val_0_rmse: 0.47719 | val_1_rmse: 0.52898 |  0:00:49s
epoch 76 | loss: 0.20967 | val_0_rmse: 0.48949 | val_1_rmse: 0.53664 |  0:00:49s
epoch 77 | loss: 0.20869 | val_0_rmse: 0.47246 | val_1_rmse: 0.5314  |  0:00:50s
epoch 78 | loss: 0.2062  | val_0_rmse: 0.47439 | val_1_rmse: 0.53107 |  0:00:50s
epoch 79 | loss: 0.20518 | val_0_rmse: 0.46379 | val_1_rmse: 0.53222 |  0:00:51s
epoch 80 | loss: 0.20322 | val_0_rmse: 0.47043 | val_1_rmse: 0.53122 |  0:00:52s
epoch 81 | loss: 0.20319 | val_0_rmse: 0.4584  | val_1_rmse: 0.53069 |  0:00:52s
epoch 82 | loss: 0.20792 | val_0_rmse: 0.46722 | val_1_rmse: 0.53356 |  0:00:53s
epoch 83 | loss: 0.20522 | val_0_rmse: 0.45601 | val_1_rmse: 0.53154 |  0:00:54s
epoch 84 | loss: 0.20242 | val_0_rmse: 0.44682 | val_1_rmse: 0.54837 |  0:00:54s
epoch 85 | loss: 0.1968  | val_0_rmse: 0.44862 | val_1_rmse: 0.53285 |  0:00:55s
epoch 86 | loss: 0.19768 | val_0_rmse: 0.43528 | val_1_rmse: 0.53622 |  0:00:56s
epoch 87 | loss: 0.19895 | val_0_rmse: 0.43669 | val_1_rmse: 0.53212 |  0:00:56s
epoch 88 | loss: 0.19966 | val_0_rmse: 0.45433 | val_1_rmse: 0.54021 |  0:00:57s
epoch 89 | loss: 0.19556 | val_0_rmse: 0.43267 | val_1_rmse: 0.54885 |  0:00:57s
epoch 90 | loss: 0.1947  | val_0_rmse: 0.44061 | val_1_rmse: 0.53873 |  0:00:58s
epoch 91 | loss: 0.19627 | val_0_rmse: 0.42755 | val_1_rmse: 0.54054 |  0:00:59s
epoch 92 | loss: 0.19615 | val_0_rmse: 0.42131 | val_1_rmse: 0.53505 |  0:00:59s
epoch 93 | loss: 0.1916  | val_0_rmse: 0.43366 | val_1_rmse: 0.53984 |  0:01:00s
epoch 94 | loss: 0.19102 | val_0_rmse: 0.41932 | val_1_rmse: 0.54382 |  0:01:01s
epoch 95 | loss: 0.18729 | val_0_rmse: 0.41974 | val_1_rmse: 0.53896 |  0:01:01s
epoch 96 | loss: 0.19648 | val_0_rmse: 0.43322 | val_1_rmse: 0.54838 |  0:01:02s
epoch 97 | loss: 0.18257 | val_0_rmse: 0.4242  | val_1_rmse: 0.56551 |  0:01:03s
epoch 98 | loss: 0.19212 | val_0_rmse: 0.43189 | val_1_rmse: 0.55015 |  0:01:03s
epoch 99 | loss: 0.1904  | val_0_rmse: 0.4218  | val_1_rmse: 0.53477 |  0:01:04s
epoch 100| loss: 0.19166 | val_0_rmse: 0.41778 | val_1_rmse: 0.5476  |  0:01:05s
epoch 101| loss: 0.1894  | val_0_rmse: 0.41293 | val_1_rmse: 0.5533  |  0:01:05s
epoch 102| loss: 0.19376 | val_0_rmse: 0.43467 | val_1_rmse: 0.56305 |  0:01:06s
epoch 103| loss: 0.19433 | val_0_rmse: 0.42308 | val_1_rmse: 0.5641  |  0:01:06s
epoch 104| loss: 0.1987  | val_0_rmse: 0.42518 | val_1_rmse: 0.56237 |  0:01:07s
epoch 105| loss: 0.20594 | val_0_rmse: 0.43039 | val_1_rmse: 0.55558 |  0:01:08s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.52898
Best weights from best epoch are automatically used!
ended training at: 07:57:25
Feature importance:
Mean squared error is of 23696035182.616142
Mean absolute error:112809.73681230262
MAPE:0.19742006511755766
R2 score:0.7172318994786123
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:57:26
epoch 0  | loss: 1.96601 | val_0_rmse: 1.00268 | val_1_rmse: 1.00237 |  0:00:00s
epoch 1  | loss: 1.15867 | val_0_rmse: 0.99739 | val_1_rmse: 0.99713 |  0:00:01s
epoch 2  | loss: 1.02099 | val_0_rmse: 0.99825 | val_1_rmse: 0.99837 |  0:00:01s
epoch 3  | loss: 0.99632 | val_0_rmse: 0.99864 | val_1_rmse: 0.9987  |  0:00:02s
epoch 4  | loss: 0.97969 | val_0_rmse: 0.99732 | val_1_rmse: 0.99722 |  0:00:03s
epoch 5  | loss: 0.94907 | val_0_rmse: 0.9863  | val_1_rmse: 0.9856  |  0:00:03s
epoch 6  | loss: 0.89973 | val_0_rmse: 0.96231 | val_1_rmse: 0.95898 |  0:00:04s
epoch 7  | loss: 0.83087 | val_0_rmse: 0.96226 | val_1_rmse: 0.95479 |  0:00:05s
epoch 8  | loss: 0.70543 | val_0_rmse: 1.06447 | val_1_rmse: 1.04813 |  0:00:05s
epoch 9  | loss: 0.5891  | val_0_rmse: 1.00043 | val_1_rmse: 0.98032 |  0:00:06s
epoch 10 | loss: 0.51613 | val_0_rmse: 1.04017 | val_1_rmse: 1.02817 |  0:00:07s
epoch 11 | loss: 0.45297 | val_0_rmse: 0.91609 | val_1_rmse: 0.91155 |  0:00:07s
epoch 12 | loss: 0.39734 | val_0_rmse: 0.84539 | val_1_rmse: 0.83844 |  0:00:08s
epoch 13 | loss: 0.36764 | val_0_rmse: 0.84    | val_1_rmse: 0.83911 |  0:00:08s
epoch 14 | loss: 0.33518 | val_0_rmse: 0.78443 | val_1_rmse: 0.785   |  0:00:09s
epoch 15 | loss: 0.32512 | val_0_rmse: 0.78307 | val_1_rmse: 0.78207 |  0:00:10s
epoch 16 | loss: 0.31922 | val_0_rmse: 0.78038 | val_1_rmse: 0.7809  |  0:00:10s
epoch 17 | loss: 0.29732 | val_0_rmse: 0.76525 | val_1_rmse: 0.7638  |  0:00:11s
epoch 18 | loss: 0.29852 | val_0_rmse: 0.75347 | val_1_rmse: 0.75198 |  0:00:12s
epoch 19 | loss: 0.2922  | val_0_rmse: 0.76237 | val_1_rmse: 0.7644  |  0:00:12s
epoch 20 | loss: 0.2807  | val_0_rmse: 0.74961 | val_1_rmse: 0.74965 |  0:00:13s
epoch 21 | loss: 0.28157 | val_0_rmse: 0.75073 | val_1_rmse: 0.74924 |  0:00:14s
epoch 22 | loss: 0.28308 | val_0_rmse: 0.74053 | val_1_rmse: 0.73919 |  0:00:14s
epoch 23 | loss: 0.27436 | val_0_rmse: 0.74429 | val_1_rmse: 0.74195 |  0:00:15s
epoch 24 | loss: 0.27367 | val_0_rmse: 0.74035 | val_1_rmse: 0.74053 |  0:00:16s
epoch 25 | loss: 0.27138 | val_0_rmse: 0.73388 | val_1_rmse: 0.7363  |  0:00:16s
epoch 26 | loss: 0.25995 | val_0_rmse: 0.72719 | val_1_rmse: 0.72834 |  0:00:17s
epoch 27 | loss: 0.26343 | val_0_rmse: 0.73904 | val_1_rmse: 0.74293 |  0:00:18s
epoch 28 | loss: 0.26672 | val_0_rmse: 0.73261 | val_1_rmse: 0.73462 |  0:00:18s
epoch 29 | loss: 0.25632 | val_0_rmse: 0.72554 | val_1_rmse: 0.72949 |  0:00:19s
epoch 30 | loss: 0.25275 | val_0_rmse: 0.72188 | val_1_rmse: 0.72518 |  0:00:19s
epoch 31 | loss: 0.24419 | val_0_rmse: 0.69401 | val_1_rmse: 0.69965 |  0:00:20s
epoch 32 | loss: 0.24135 | val_0_rmse: 0.70166 | val_1_rmse: 0.70926 |  0:00:21s
epoch 33 | loss: 0.243   | val_0_rmse: 0.69193 | val_1_rmse: 0.70011 |  0:00:21s
epoch 34 | loss: 0.24324 | val_0_rmse: 0.70141 | val_1_rmse: 0.70947 |  0:00:22s
epoch 35 | loss: 0.23883 | val_0_rmse: 0.68739 | val_1_rmse: 0.69762 |  0:00:23s
epoch 36 | loss: 0.24002 | val_0_rmse: 0.68694 | val_1_rmse: 0.69799 |  0:00:23s
epoch 37 | loss: 0.23445 | val_0_rmse: 0.68655 | val_1_rmse: 0.69528 |  0:00:24s
epoch 38 | loss: 0.23561 | val_0_rmse: 0.66632 | val_1_rmse: 0.67841 |  0:00:25s
epoch 39 | loss: 0.23198 | val_0_rmse: 0.68217 | val_1_rmse: 0.69301 |  0:00:25s
epoch 40 | loss: 0.24253 | val_0_rmse: 0.65022 | val_1_rmse: 0.66242 |  0:00:26s
epoch 41 | loss: 0.23345 | val_0_rmse: 0.66303 | val_1_rmse: 0.67725 |  0:00:27s
epoch 42 | loss: 0.23119 | val_0_rmse: 0.64801 | val_1_rmse: 0.66196 |  0:00:27s
epoch 43 | loss: 0.21953 | val_0_rmse: 0.65095 | val_1_rmse: 0.66879 |  0:00:28s
epoch 44 | loss: 0.22253 | val_0_rmse: 0.63411 | val_1_rmse: 0.65186 |  0:00:28s
epoch 45 | loss: 0.22057 | val_0_rmse: 0.65327 | val_1_rmse: 0.66669 |  0:00:29s
epoch 46 | loss: 0.22199 | val_0_rmse: 0.62664 | val_1_rmse: 0.64673 |  0:00:30s
epoch 47 | loss: 0.22511 | val_0_rmse: 0.62648 | val_1_rmse: 0.644   |  0:00:30s
epoch 48 | loss: 0.22566 | val_0_rmse: 0.63743 | val_1_rmse: 0.65825 |  0:00:31s
epoch 49 | loss: 0.22931 | val_0_rmse: 0.62927 | val_1_rmse: 0.64886 |  0:00:32s
epoch 50 | loss: 0.22635 | val_0_rmse: 0.61434 | val_1_rmse: 0.63472 |  0:00:32s
epoch 51 | loss: 0.22422 | val_0_rmse: 0.64694 | val_1_rmse: 0.66387 |  0:00:33s
epoch 52 | loss: 0.21984 | val_0_rmse: 0.60027 | val_1_rmse: 0.63737 |  0:00:34s
epoch 53 | loss: 0.22125 | val_0_rmse: 0.61328 | val_1_rmse: 0.63858 |  0:00:34s
epoch 54 | loss: 0.21598 | val_0_rmse: 0.58319 | val_1_rmse: 0.61757 |  0:00:35s
epoch 55 | loss: 0.20968 | val_0_rmse: 0.5967  | val_1_rmse: 0.62443 |  0:00:36s
epoch 56 | loss: 0.2139  | val_0_rmse: 0.57371 | val_1_rmse: 0.61024 |  0:00:36s
epoch 57 | loss: 0.21415 | val_0_rmse: 0.58634 | val_1_rmse: 0.61815 |  0:00:37s
epoch 58 | loss: 0.21156 | val_0_rmse: 0.58051 | val_1_rmse: 0.61458 |  0:00:38s
epoch 59 | loss: 0.21316 | val_0_rmse: 0.56303 | val_1_rmse: 0.59908 |  0:00:38s
epoch 60 | loss: 0.20659 | val_0_rmse: 0.56272 | val_1_rmse: 0.59949 |  0:00:39s
epoch 61 | loss: 0.20862 | val_0_rmse: 0.55593 | val_1_rmse: 0.59658 |  0:00:39s
epoch 62 | loss: 0.20672 | val_0_rmse: 0.5622  | val_1_rmse: 0.59761 |  0:00:40s
epoch 63 | loss: 0.20299 | val_0_rmse: 0.53884 | val_1_rmse: 0.58652 |  0:00:41s
epoch 64 | loss: 0.20801 | val_0_rmse: 0.51864 | val_1_rmse: 0.57624 |  0:00:41s
epoch 65 | loss: 0.21012 | val_0_rmse: 0.53895 | val_1_rmse: 0.58585 |  0:00:42s
epoch 66 | loss: 0.20515 | val_0_rmse: 0.51355 | val_1_rmse: 0.57519 |  0:00:43s
epoch 67 | loss: 0.20614 | val_0_rmse: 0.51766 | val_1_rmse: 0.57813 |  0:00:43s
epoch 68 | loss: 0.20424 | val_0_rmse: 0.53621 | val_1_rmse: 0.59424 |  0:00:44s
epoch 69 | loss: 0.20159 | val_0_rmse: 0.49114 | val_1_rmse: 0.5659  |  0:00:45s
epoch 70 | loss: 0.19982 | val_0_rmse: 0.49893 | val_1_rmse: 0.57977 |  0:00:45s
epoch 71 | loss: 0.1992  | val_0_rmse: 0.49283 | val_1_rmse: 0.56787 |  0:00:46s
epoch 72 | loss: 0.20168 | val_0_rmse: 0.52764 | val_1_rmse: 0.59329 |  0:00:47s
epoch 73 | loss: 0.21391 | val_0_rmse: 0.47855 | val_1_rmse: 0.5637  |  0:00:47s
epoch 74 | loss: 0.20422 | val_0_rmse: 0.47318 | val_1_rmse: 0.56778 |  0:00:48s
epoch 75 | loss: 0.20141 | val_0_rmse: 0.4973  | val_1_rmse: 0.56674 |  0:00:48s
epoch 76 | loss: 0.19552 | val_0_rmse: 0.46692 | val_1_rmse: 0.55729 |  0:00:49s
epoch 77 | loss: 0.19408 | val_0_rmse: 0.47955 | val_1_rmse: 0.5688  |  0:00:50s
epoch 78 | loss: 0.2005  | val_0_rmse: 0.44528 | val_1_rmse: 0.55882 |  0:00:50s
epoch 79 | loss: 0.19303 | val_0_rmse: 0.47262 | val_1_rmse: 0.56533 |  0:00:51s
epoch 80 | loss: 0.19313 | val_0_rmse: 0.45205 | val_1_rmse: 0.55721 |  0:00:52s
epoch 81 | loss: 0.19225 | val_0_rmse: 0.43858 | val_1_rmse: 0.55601 |  0:00:52s
epoch 82 | loss: 0.19489 | val_0_rmse: 0.50557 | val_1_rmse: 0.5873  |  0:00:53s
epoch 83 | loss: 0.19128 | val_0_rmse: 0.44092 | val_1_rmse: 0.56482 |  0:00:54s
epoch 84 | loss: 0.19207 | val_0_rmse: 0.44005 | val_1_rmse: 0.55821 |  0:00:54s
epoch 85 | loss: 0.18848 | val_0_rmse: 0.45106 | val_1_rmse: 0.56633 |  0:00:55s
epoch 86 | loss: 0.19312 | val_0_rmse: 0.43015 | val_1_rmse: 0.55564 |  0:00:56s
epoch 87 | loss: 0.188   | val_0_rmse: 0.45093 | val_1_rmse: 0.56705 |  0:00:56s
epoch 88 | loss: 0.19098 | val_0_rmse: 0.41307 | val_1_rmse: 0.56115 |  0:00:57s
epoch 89 | loss: 0.18754 | val_0_rmse: 0.43658 | val_1_rmse: 0.56527 |  0:00:57s
epoch 90 | loss: 0.18649 | val_0_rmse: 0.41824 | val_1_rmse: 0.559   |  0:00:58s
epoch 91 | loss: 0.18616 | val_0_rmse: 0.41183 | val_1_rmse: 0.55175 |  0:00:59s
epoch 92 | loss: 0.18175 | val_0_rmse: 0.40791 | val_1_rmse: 0.55209 |  0:00:59s
epoch 93 | loss: 0.18126 | val_0_rmse: 0.41886 | val_1_rmse: 0.55612 |  0:01:00s
epoch 94 | loss: 0.17914 | val_0_rmse: 0.404   | val_1_rmse: 0.55794 |  0:01:01s
epoch 95 | loss: 0.17886 | val_0_rmse: 0.4206  | val_1_rmse: 0.55908 |  0:01:01s
epoch 96 | loss: 0.18612 | val_0_rmse: 0.40462 | val_1_rmse: 0.55558 |  0:01:02s
epoch 97 | loss: 0.17503 | val_0_rmse: 0.39997 | val_1_rmse: 0.55402 |  0:01:03s
epoch 98 | loss: 0.17614 | val_0_rmse: 0.40006 | val_1_rmse: 0.5585  |  0:01:03s
epoch 99 | loss: 0.18034 | val_0_rmse: 0.39781 | val_1_rmse: 0.55805 |  0:01:04s
epoch 100| loss: 0.18331 | val_0_rmse: 0.43254 | val_1_rmse: 0.56541 |  0:01:04s
epoch 101| loss: 0.18792 | val_0_rmse: 0.41228 | val_1_rmse: 0.57728 |  0:01:05s
epoch 102| loss: 0.17781 | val_0_rmse: 0.40926 | val_1_rmse: 0.56213 |  0:01:06s
epoch 103| loss: 0.1844  | val_0_rmse: 0.39631 | val_1_rmse: 0.56656 |  0:01:06s
epoch 104| loss: 0.17954 | val_0_rmse: 0.4069  | val_1_rmse: 0.55979 |  0:01:07s
epoch 105| loss: 0.1817  | val_0_rmse: 0.39725 | val_1_rmse: 0.55653 |  0:01:08s
epoch 106| loss: 0.17687 | val_0_rmse: 0.39173 | val_1_rmse: 0.56661 |  0:01:08s
epoch 107| loss: 0.17587 | val_0_rmse: 0.39201 | val_1_rmse: 0.56362 |  0:01:09s
epoch 108| loss: 0.17421 | val_0_rmse: 0.3831  | val_1_rmse: 0.5708  |  0:01:10s
epoch 109| loss: 0.17579 | val_0_rmse: 0.41208 | val_1_rmse: 0.57111 |  0:01:10s
epoch 110| loss: 0.17627 | val_0_rmse: 0.4078  | val_1_rmse: 0.58561 |  0:01:11s
epoch 111| loss: 0.17753 | val_0_rmse: 0.388   | val_1_rmse: 0.56807 |  0:01:12s
epoch 112| loss: 0.17601 | val_0_rmse: 0.38785 | val_1_rmse: 0.56997 |  0:01:12s
epoch 113| loss: 0.1681  | val_0_rmse: 0.38064 | val_1_rmse: 0.57435 |  0:01:13s
epoch 114| loss: 0.17062 | val_0_rmse: 0.38676 | val_1_rmse: 0.57087 |  0:01:13s
epoch 115| loss: 0.16652 | val_0_rmse: 0.38297 | val_1_rmse: 0.57146 |  0:01:14s
epoch 116| loss: 0.17083 | val_0_rmse: 0.38803 | val_1_rmse: 0.56793 |  0:01:15s
epoch 117| loss: 0.17804 | val_0_rmse: 0.41277 | val_1_rmse: 0.60583 |  0:01:15s
epoch 118| loss: 0.17078 | val_0_rmse: 0.39326 | val_1_rmse: 0.5742  |  0:01:16s
epoch 119| loss: 0.16907 | val_0_rmse: 0.37638 | val_1_rmse: 0.57397 |  0:01:17s
epoch 120| loss: 0.1721  | val_0_rmse: 0.38219 | val_1_rmse: 0.57211 |  0:01:17s
epoch 121| loss: 0.16531 | val_0_rmse: 0.39721 | val_1_rmse: 0.57544 |  0:01:18s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.55175
Best weights from best epoch are automatically used!
ended training at: 07:58:45
Feature importance:
Mean squared error is of 24294905527.233143
Mean absolute error:111200.62769282894
MAPE:0.18703694392947573
R2 score:0.7049296744651651
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:58:45
epoch 0  | loss: 1.98549 | val_0_rmse: 0.99792 | val_1_rmse: 0.99103 |  0:00:00s
epoch 1  | loss: 1.20457 | val_0_rmse: 0.99452 | val_1_rmse: 0.98813 |  0:00:01s
epoch 2  | loss: 1.02041 | val_0_rmse: 0.98811 | val_1_rmse: 0.98215 |  0:00:01s
epoch 3  | loss: 0.97603 | val_0_rmse: 0.98366 | val_1_rmse: 0.9778  |  0:00:02s
epoch 4  | loss: 0.90661 | val_0_rmse: 0.96969 | val_1_rmse: 0.9618  |  0:00:03s
epoch 5  | loss: 0.81872 | val_0_rmse: 0.8773  | val_1_rmse: 0.87238 |  0:00:03s
epoch 6  | loss: 0.7149  | val_0_rmse: 0.81891 | val_1_rmse: 0.82483 |  0:00:04s
epoch 7  | loss: 0.62738 | val_0_rmse: 0.84287 | val_1_rmse: 0.86295 |  0:00:05s
epoch 8  | loss: 0.54363 | val_0_rmse: 0.84544 | val_1_rmse: 0.87022 |  0:00:05s
epoch 9  | loss: 0.48776 | val_0_rmse: 0.81478 | val_1_rmse: 0.8366  |  0:00:06s
epoch 10 | loss: 0.43393 | val_0_rmse: 0.80657 | val_1_rmse: 0.82491 |  0:00:07s
epoch 11 | loss: 0.39451 | val_0_rmse: 0.79173 | val_1_rmse: 0.80832 |  0:00:07s
epoch 12 | loss: 0.35363 | val_0_rmse: 0.77718 | val_1_rmse: 0.79085 |  0:00:08s
epoch 13 | loss: 0.34715 | val_0_rmse: 0.77929 | val_1_rmse: 0.78812 |  0:00:09s
epoch 14 | loss: 0.3315  | val_0_rmse: 0.77614 | val_1_rmse: 0.78922 |  0:00:09s
epoch 15 | loss: 0.32046 | val_0_rmse: 0.78744 | val_1_rmse: 0.79452 |  0:00:10s
epoch 16 | loss: 0.30377 | val_0_rmse: 0.77235 | val_1_rmse: 0.78252 |  0:00:10s
epoch 17 | loss: 0.29169 | val_0_rmse: 0.77265 | val_1_rmse: 0.78292 |  0:00:11s
epoch 18 | loss: 0.28917 | val_0_rmse: 0.75413 | val_1_rmse: 0.76593 |  0:00:12s
epoch 19 | loss: 0.2955  | val_0_rmse: 0.7726  | val_1_rmse: 0.78108 |  0:00:12s
epoch 20 | loss: 0.28111 | val_0_rmse: 0.77408 | val_1_rmse: 0.78326 |  0:00:13s
epoch 21 | loss: 0.27227 | val_0_rmse: 0.75642 | val_1_rmse: 0.76737 |  0:00:14s
epoch 22 | loss: 0.26697 | val_0_rmse: 0.7379  | val_1_rmse: 0.7505  |  0:00:14s
epoch 23 | loss: 0.26205 | val_0_rmse: 0.74689 | val_1_rmse: 0.75798 |  0:00:15s
epoch 24 | loss: 0.25762 | val_0_rmse: 0.7319  | val_1_rmse: 0.74747 |  0:00:16s
epoch 25 | loss: 0.25105 | val_0_rmse: 0.73216 | val_1_rmse: 0.74858 |  0:00:16s
epoch 26 | loss: 0.24925 | val_0_rmse: 0.74018 | val_1_rmse: 0.75637 |  0:00:17s
epoch 27 | loss: 0.24908 | val_0_rmse: 0.73569 | val_1_rmse: 0.75336 |  0:00:18s
epoch 28 | loss: 0.24942 | val_0_rmse: 0.72309 | val_1_rmse: 0.74311 |  0:00:18s
epoch 29 | loss: 0.2455  | val_0_rmse: 0.72119 | val_1_rmse: 0.74356 |  0:00:19s
epoch 30 | loss: 0.24895 | val_0_rmse: 0.70562 | val_1_rmse: 0.72983 |  0:00:19s
epoch 31 | loss: 0.23888 | val_0_rmse: 0.71045 | val_1_rmse: 0.73523 |  0:00:20s
epoch 32 | loss: 0.23738 | val_0_rmse: 0.70253 | val_1_rmse: 0.72842 |  0:00:21s
epoch 33 | loss: 0.23855 | val_0_rmse: 0.71134 | val_1_rmse: 0.73927 |  0:00:21s
epoch 34 | loss: 0.23779 | val_0_rmse: 0.72312 | val_1_rmse: 0.74847 |  0:00:22s
epoch 35 | loss: 0.23139 | val_0_rmse: 0.69509 | val_1_rmse: 0.72856 |  0:00:23s
epoch 36 | loss: 0.23602 | val_0_rmse: 0.70095 | val_1_rmse: 0.72865 |  0:00:23s
epoch 37 | loss: 0.23397 | val_0_rmse: 0.68011 | val_1_rmse: 0.7157  |  0:00:24s
epoch 38 | loss: 0.22759 | val_0_rmse: 0.67424 | val_1_rmse: 0.71167 |  0:00:25s
epoch 39 | loss: 0.23178 | val_0_rmse: 0.68318 | val_1_rmse: 0.71518 |  0:00:25s
epoch 40 | loss: 0.23607 | val_0_rmse: 0.67037 | val_1_rmse: 0.70081 |  0:00:26s
epoch 41 | loss: 0.22209 | val_0_rmse: 0.65893 | val_1_rmse: 0.69633 |  0:00:27s
epoch 42 | loss: 0.22313 | val_0_rmse: 0.65321 | val_1_rmse: 0.69351 |  0:00:27s
epoch 43 | loss: 0.22948 | val_0_rmse: 0.64659 | val_1_rmse: 0.68416 |  0:00:28s
epoch 44 | loss: 0.21659 | val_0_rmse: 0.63188 | val_1_rmse: 0.67707 |  0:00:28s
epoch 45 | loss: 0.21583 | val_0_rmse: 0.63474 | val_1_rmse: 0.68796 |  0:00:29s
epoch 46 | loss: 0.21881 | val_0_rmse: 0.64891 | val_1_rmse: 0.69008 |  0:00:30s
epoch 47 | loss: 0.21897 | val_0_rmse: 0.64618 | val_1_rmse: 0.68959 |  0:00:30s
epoch 48 | loss: 0.22939 | val_0_rmse: 0.62981 | val_1_rmse: 0.68661 |  0:00:31s
epoch 49 | loss: 0.21359 | val_0_rmse: 0.61694 | val_1_rmse: 0.67049 |  0:00:32s
epoch 50 | loss: 0.21501 | val_0_rmse: 0.62035 | val_1_rmse: 0.68103 |  0:00:32s
epoch 51 | loss: 0.21576 | val_0_rmse: 0.60758 | val_1_rmse: 0.66446 |  0:00:33s
epoch 52 | loss: 0.2197  | val_0_rmse: 0.59814 | val_1_rmse: 0.66128 |  0:00:34s
epoch 53 | loss: 0.21548 | val_0_rmse: 0.60502 | val_1_rmse: 0.66222 |  0:00:34s
epoch 54 | loss: 0.21277 | val_0_rmse: 0.57487 | val_1_rmse: 0.64857 |  0:00:35s
epoch 55 | loss: 0.21145 | val_0_rmse: 0.58389 | val_1_rmse: 0.66184 |  0:00:36s
epoch 56 | loss: 0.21015 | val_0_rmse: 0.58656 | val_1_rmse: 0.66191 |  0:00:36s
epoch 57 | loss: 0.20774 | val_0_rmse: 0.56225 | val_1_rmse: 0.63966 |  0:00:37s
epoch 58 | loss: 0.20165 | val_0_rmse: 0.56405 | val_1_rmse: 0.63684 |  0:00:37s
epoch 59 | loss: 0.19937 | val_0_rmse: 0.5573  | val_1_rmse: 0.63612 |  0:00:38s
epoch 60 | loss: 0.20025 | val_0_rmse: 0.53684 | val_1_rmse: 0.62718 |  0:00:39s
epoch 61 | loss: 0.20296 | val_0_rmse: 0.53059 | val_1_rmse: 0.63795 |  0:00:39s
epoch 62 | loss: 0.20882 | val_0_rmse: 0.52682 | val_1_rmse: 0.63732 |  0:00:40s
epoch 63 | loss: 0.19553 | val_0_rmse: 0.52732 | val_1_rmse: 0.63531 |  0:00:41s
epoch 64 | loss: 0.19813 | val_0_rmse: 0.51678 | val_1_rmse: 0.62953 |  0:00:41s
epoch 65 | loss: 0.19194 | val_0_rmse: 0.51928 | val_1_rmse: 0.63016 |  0:00:42s
epoch 66 | loss: 0.19566 | val_0_rmse: 0.51932 | val_1_rmse: 0.62842 |  0:00:43s
epoch 67 | loss: 0.20042 | val_0_rmse: 0.49115 | val_1_rmse: 0.63249 |  0:00:43s
epoch 68 | loss: 0.19211 | val_0_rmse: 0.49742 | val_1_rmse: 0.6352  |  0:00:44s
epoch 69 | loss: 0.19178 | val_0_rmse: 0.48592 | val_1_rmse: 0.62978 |  0:00:45s
epoch 70 | loss: 0.19666 | val_0_rmse: 0.48867 | val_1_rmse: 0.63125 |  0:00:45s
epoch 71 | loss: 0.18986 | val_0_rmse: 0.48341 | val_1_rmse: 0.63759 |  0:00:46s
epoch 72 | loss: 0.19191 | val_0_rmse: 0.5028  | val_1_rmse: 0.63185 |  0:00:46s
epoch 73 | loss: 0.18958 | val_0_rmse: 0.46693 | val_1_rmse: 0.63497 |  0:00:47s
epoch 74 | loss: 0.18761 | val_0_rmse: 0.48804 | val_1_rmse: 0.63485 |  0:00:48s
epoch 75 | loss: 0.19527 | val_0_rmse: 0.462   | val_1_rmse: 0.63177 |  0:00:48s
epoch 76 | loss: 0.19084 | val_0_rmse: 0.45403 | val_1_rmse: 0.61755 |  0:00:49s
epoch 77 | loss: 0.18807 | val_0_rmse: 0.45786 | val_1_rmse: 0.63256 |  0:00:50s
epoch 78 | loss: 0.1866  | val_0_rmse: 0.45442 | val_1_rmse: 0.64386 |  0:00:50s
epoch 79 | loss: 0.18545 | val_0_rmse: 0.44759 | val_1_rmse: 0.64555 |  0:00:51s
epoch 80 | loss: 0.18872 | val_0_rmse: 0.43797 | val_1_rmse: 0.63085 |  0:00:52s
epoch 81 | loss: 0.18438 | val_0_rmse: 0.42945 | val_1_rmse: 0.65714 |  0:00:52s
epoch 82 | loss: 0.18627 | val_0_rmse: 0.43388 | val_1_rmse: 0.6386  |  0:00:53s
epoch 83 | loss: 0.18109 | val_0_rmse: 0.44171 | val_1_rmse: 0.6377  |  0:00:53s
epoch 84 | loss: 0.17993 | val_0_rmse: 0.42783 | val_1_rmse: 0.6361  |  0:00:54s
epoch 85 | loss: 0.17903 | val_0_rmse: 0.41337 | val_1_rmse: 0.61732 |  0:00:55s
epoch 86 | loss: 0.17454 | val_0_rmse: 0.41355 | val_1_rmse: 0.60761 |  0:00:55s
epoch 87 | loss: 0.17656 | val_0_rmse: 0.41401 | val_1_rmse: 0.60967 |  0:00:56s
epoch 88 | loss: 0.17641 | val_0_rmse: 0.41226 | val_1_rmse: 0.62775 |  0:00:57s
epoch 89 | loss: 0.17289 | val_0_rmse: 0.40489 | val_1_rmse: 0.60753 |  0:00:57s
epoch 90 | loss: 0.18062 | val_0_rmse: 0.40745 | val_1_rmse: 0.60521 |  0:00:58s
epoch 91 | loss: 0.1787  | val_0_rmse: 0.42707 | val_1_rmse: 0.60765 |  0:00:59s
epoch 92 | loss: 0.17976 | val_0_rmse: 0.39571 | val_1_rmse: 0.60206 |  0:00:59s
epoch 93 | loss: 0.17707 | val_0_rmse: 0.39541 | val_1_rmse: 0.6193  |  0:01:00s
epoch 94 | loss: 0.17567 | val_0_rmse: 0.3889  | val_1_rmse: 0.60358 |  0:01:01s
epoch 95 | loss: 0.17601 | val_0_rmse: 0.4036  | val_1_rmse: 0.60376 |  0:01:01s
epoch 96 | loss: 0.17974 | val_0_rmse: 0.39636 | val_1_rmse: 0.61384 |  0:01:02s
epoch 97 | loss: 0.18067 | val_0_rmse: 0.3983  | val_1_rmse: 0.59841 |  0:01:02s
epoch 98 | loss: 0.17352 | val_0_rmse: 0.39306 | val_1_rmse: 0.59028 |  0:01:03s
epoch 99 | loss: 0.17028 | val_0_rmse: 0.39396 | val_1_rmse: 0.59736 |  0:01:04s
epoch 100| loss: 0.16464 | val_0_rmse: 0.38254 | val_1_rmse: 0.6141  |  0:01:04s
epoch 101| loss: 0.1697  | val_0_rmse: 0.37709 | val_1_rmse: 0.62546 |  0:01:05s
epoch 102| loss: 0.16866 | val_0_rmse: 0.37934 | val_1_rmse: 0.62233 |  0:01:06s
epoch 103| loss: 0.17336 | val_0_rmse: 0.38317 | val_1_rmse: 0.59494 |  0:01:06s
epoch 104| loss: 0.16481 | val_0_rmse: 0.38887 | val_1_rmse: 0.61883 |  0:01:07s
epoch 105| loss: 0.16693 | val_0_rmse: 0.37812 | val_1_rmse: 0.61172 |  0:01:08s
epoch 106| loss: 0.16285 | val_0_rmse: 0.37244 | val_1_rmse: 0.61568 |  0:01:08s
epoch 107| loss: 0.16381 | val_0_rmse: 0.37453 | val_1_rmse: 0.60574 |  0:01:09s
epoch 108| loss: 0.16314 | val_0_rmse: 0.37963 | val_1_rmse: 0.61428 |  0:01:10s
epoch 109| loss: 0.16285 | val_0_rmse: 0.37201 | val_1_rmse: 0.61427 |  0:01:10s
epoch 110| loss: 0.16361 | val_0_rmse: 0.37019 | val_1_rmse: 0.61724 |  0:01:11s
epoch 111| loss: 0.16184 | val_0_rmse: 0.36817 | val_1_rmse: 0.60478 |  0:01:12s
epoch 112| loss: 0.1614  | val_0_rmse: 0.37548 | val_1_rmse: 0.61089 |  0:01:12s
epoch 113| loss: 0.17064 | val_0_rmse: 0.36452 | val_1_rmse: 0.60764 |  0:01:13s
epoch 114| loss: 0.16127 | val_0_rmse: 0.36182 | val_1_rmse: 0.62332 |  0:01:13s
epoch 115| loss: 0.16079 | val_0_rmse: 0.36417 | val_1_rmse: 0.61281 |  0:01:14s
epoch 116| loss: 0.16056 | val_0_rmse: 0.36389 | val_1_rmse: 0.6223  |  0:01:15s
epoch 117| loss: 0.15699 | val_0_rmse: 0.36149 | val_1_rmse: 0.6651  |  0:01:15s
epoch 118| loss: 0.15444 | val_0_rmse: 0.36156 | val_1_rmse: 0.68252 |  0:01:16s
epoch 119| loss: 0.15308 | val_0_rmse: 0.35894 | val_1_rmse: 0.65811 |  0:01:17s
epoch 120| loss: 0.15734 | val_0_rmse: 0.35921 | val_1_rmse: 0.66095 |  0:01:17s
epoch 121| loss: 0.15175 | val_0_rmse: 0.36261 | val_1_rmse: 0.69786 |  0:01:18s
epoch 122| loss: 0.15467 | val_0_rmse: 0.36009 | val_1_rmse: 0.7127  |  0:01:19s
epoch 123| loss: 0.15763 | val_0_rmse: 0.37798 | val_1_rmse: 0.65515 |  0:01:19s
epoch 124| loss: 0.15836 | val_0_rmse: 0.3673  | val_1_rmse: 0.68099 |  0:01:20s
epoch 125| loss: 0.16051 | val_0_rmse: 0.36798 | val_1_rmse: 0.66453 |  0:01:20s
epoch 126| loss: 0.16022 | val_0_rmse: 0.36763 | val_1_rmse: 0.61303 |  0:01:21s
epoch 127| loss: 0.1622  | val_0_rmse: 0.36313 | val_1_rmse: 0.66224 |  0:01:22s
epoch 128| loss: 0.16309 | val_0_rmse: 0.37066 | val_1_rmse: 0.70853 |  0:01:22s

Early stopping occured at epoch 128 with best_epoch = 98 and best_val_1_rmse = 0.59028
Best weights from best epoch are automatically used!
ended training at: 08:00:08
Feature importance:
Mean squared error is of 25292379752.735847
Mean absolute error:114736.55445065789
MAPE:0.20306944474340066
R2 score:0.6999573728533919
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:00:08
epoch 0  | loss: 2.16105 | val_0_rmse: 0.99499 | val_1_rmse: 1.00325 |  0:00:00s
epoch 1  | loss: 1.14745 | val_0_rmse: 0.99652 | val_1_rmse: 1.00415 |  0:00:01s
epoch 2  | loss: 1.01442 | val_0_rmse: 0.99414 | val_1_rmse: 1.00192 |  0:00:01s
epoch 3  | loss: 0.9894  | val_0_rmse: 0.99418 | val_1_rmse: 1.00211 |  0:00:02s
epoch 4  | loss: 0.98403 | val_0_rmse: 0.99106 | val_1_rmse: 0.99831 |  0:00:03s
epoch 5  | loss: 0.9658  | val_0_rmse: 0.98856 | val_1_rmse: 0.99549 |  0:00:03s
epoch 6  | loss: 0.95607 | val_0_rmse: 1.00181 | val_1_rmse: 1.0088  |  0:00:04s
epoch 7  | loss: 0.91867 | val_0_rmse: 1.00077 | val_1_rmse: 1.00273 |  0:00:05s
epoch 8  | loss: 0.88525 | val_0_rmse: 0.98371 | val_1_rmse: 0.99396 |  0:00:05s
epoch 9  | loss: 0.84684 | val_0_rmse: 0.97445 | val_1_rmse: 0.98103 |  0:00:06s
epoch 10 | loss: 0.80431 | val_0_rmse: 0.96367 | val_1_rmse: 0.96755 |  0:00:07s
epoch 11 | loss: 0.73788 | val_0_rmse: 0.94596 | val_1_rmse: 0.94895 |  0:00:07s
epoch 12 | loss: 0.67965 | val_0_rmse: 1.02599 | val_1_rmse: 1.01475 |  0:00:08s
epoch 13 | loss: 0.60456 | val_0_rmse: 0.99674 | val_1_rmse: 0.98814 |  0:00:09s
epoch 14 | loss: 0.54075 | val_0_rmse: 1.01688 | val_1_rmse: 1.00667 |  0:00:09s
epoch 15 | loss: 0.51505 | val_0_rmse: 0.9981  | val_1_rmse: 0.98777 |  0:00:10s
epoch 16 | loss: 0.46655 | val_0_rmse: 0.97187 | val_1_rmse: 0.96235 |  0:00:10s
epoch 17 | loss: 0.42656 | val_0_rmse: 0.98592 | val_1_rmse: 0.97375 |  0:00:11s
epoch 18 | loss: 0.40155 | val_0_rmse: 1.04068 | val_1_rmse: 1.02071 |  0:00:12s
epoch 19 | loss: 0.39397 | val_0_rmse: 0.92923 | val_1_rmse: 0.91678 |  0:00:12s
epoch 20 | loss: 0.37396 | val_0_rmse: 1.02311 | val_1_rmse: 1.01045 |  0:00:13s
epoch 21 | loss: 0.3563  | val_0_rmse: 0.89959 | val_1_rmse: 0.88683 |  0:00:14s
epoch 22 | loss: 0.34292 | val_0_rmse: 0.91706 | val_1_rmse: 0.89812 |  0:00:14s
epoch 23 | loss: 0.3333  | val_0_rmse: 0.81803 | val_1_rmse: 0.80568 |  0:00:15s
epoch 24 | loss: 0.3171  | val_0_rmse: 0.84247 | val_1_rmse: 0.83079 |  0:00:16s
epoch 25 | loss: 0.31875 | val_0_rmse: 0.82294 | val_1_rmse: 0.81413 |  0:00:16s
epoch 26 | loss: 0.31256 | val_0_rmse: 0.7829  | val_1_rmse: 0.77742 |  0:00:17s
epoch 27 | loss: 0.30879 | val_0_rmse: 0.82661 | val_1_rmse: 0.81634 |  0:00:18s
epoch 28 | loss: 0.30355 | val_0_rmse: 0.7916  | val_1_rmse: 0.78292 |  0:00:18s
epoch 29 | loss: 0.29048 | val_0_rmse: 0.7651  | val_1_rmse: 0.75745 |  0:00:19s
epoch 30 | loss: 0.27447 | val_0_rmse: 0.75561 | val_1_rmse: 0.75162 |  0:00:19s
epoch 31 | loss: 0.27478 | val_0_rmse: 0.75815 | val_1_rmse: 0.75467 |  0:00:20s
epoch 32 | loss: 0.27269 | val_0_rmse: 0.77643 | val_1_rmse: 0.76902 |  0:00:21s
epoch 33 | loss: 0.27061 | val_0_rmse: 0.73247 | val_1_rmse: 0.7285  |  0:00:21s
epoch 34 | loss: 0.26894 | val_0_rmse: 0.77883 | val_1_rmse: 0.7702  |  0:00:22s
epoch 35 | loss: 0.2663  | val_0_rmse: 0.73497 | val_1_rmse: 0.73122 |  0:00:23s
epoch 36 | loss: 0.26807 | val_0_rmse: 0.73228 | val_1_rmse: 0.72752 |  0:00:23s
epoch 37 | loss: 0.25764 | val_0_rmse: 0.71101 | val_1_rmse: 0.70844 |  0:00:24s
epoch 38 | loss: 0.26474 | val_0_rmse: 0.71758 | val_1_rmse: 0.71753 |  0:00:25s
epoch 39 | loss: 0.25153 | val_0_rmse: 0.69213 | val_1_rmse: 0.693   |  0:00:25s
epoch 40 | loss: 0.25435 | val_0_rmse: 0.68891 | val_1_rmse: 0.69117 |  0:00:26s
epoch 41 | loss: 0.24829 | val_0_rmse: 0.68485 | val_1_rmse: 0.68706 |  0:00:27s
epoch 42 | loss: 0.24724 | val_0_rmse: 0.68705 | val_1_rmse: 0.6953  |  0:00:27s
epoch 43 | loss: 0.24567 | val_0_rmse: 0.67928 | val_1_rmse: 0.68423 |  0:00:28s
epoch 44 | loss: 0.24674 | val_0_rmse: 0.68296 | val_1_rmse: 0.68924 |  0:00:28s
epoch 45 | loss: 0.25258 | val_0_rmse: 0.69303 | val_1_rmse: 0.71012 |  0:00:29s
epoch 46 | loss: 0.25401 | val_0_rmse: 0.64752 | val_1_rmse: 0.67171 |  0:00:30s
epoch 47 | loss: 0.24102 | val_0_rmse: 0.64448 | val_1_rmse: 0.66269 |  0:00:30s
epoch 48 | loss: 0.2419  | val_0_rmse: 0.63769 | val_1_rmse: 0.66068 |  0:00:31s
epoch 49 | loss: 0.2385  | val_0_rmse: 0.62906 | val_1_rmse: 0.65312 |  0:00:32s
epoch 50 | loss: 0.24413 | val_0_rmse: 0.61952 | val_1_rmse: 0.64541 |  0:00:32s
epoch 51 | loss: 0.23695 | val_0_rmse: 0.60587 | val_1_rmse: 0.63789 |  0:00:33s
epoch 52 | loss: 0.23209 | val_0_rmse: 0.60309 | val_1_rmse: 0.63554 |  0:00:34s
epoch 53 | loss: 0.23092 | val_0_rmse: 0.59821 | val_1_rmse: 0.63135 |  0:00:34s
epoch 54 | loss: 0.23106 | val_0_rmse: 0.59329 | val_1_rmse: 0.62676 |  0:00:35s
epoch 55 | loss: 0.22553 | val_0_rmse: 0.58585 | val_1_rmse: 0.62609 |  0:00:36s
epoch 56 | loss: 0.22784 | val_0_rmse: 0.58846 | val_1_rmse: 0.62768 |  0:00:36s
epoch 57 | loss: 0.22593 | val_0_rmse: 0.57907 | val_1_rmse: 0.62028 |  0:00:37s
epoch 58 | loss: 0.2254  | val_0_rmse: 0.5767  | val_1_rmse: 0.61751 |  0:00:38s
epoch 59 | loss: 0.22359 | val_0_rmse: 0.56406 | val_1_rmse: 0.61097 |  0:00:38s
epoch 60 | loss: 0.22338 | val_0_rmse: 0.56567 | val_1_rmse: 0.61703 |  0:00:39s
epoch 61 | loss: 0.22157 | val_0_rmse: 0.55346 | val_1_rmse: 0.59997 |  0:00:39s
epoch 62 | loss: 0.22114 | val_0_rmse: 0.56267 | val_1_rmse: 0.62633 |  0:00:40s
epoch 63 | loss: 0.21895 | val_0_rmse: 0.54032 | val_1_rmse: 0.5857  |  0:00:41s
epoch 64 | loss: 0.2186  | val_0_rmse: 0.5505  | val_1_rmse: 0.60748 |  0:00:41s
epoch 65 | loss: 0.2157  | val_0_rmse: 0.5407  | val_1_rmse: 0.60178 |  0:00:42s
epoch 66 | loss: 0.21476 | val_0_rmse: 0.53213 | val_1_rmse: 0.58981 |  0:00:43s
epoch 67 | loss: 0.21577 | val_0_rmse: 0.52595 | val_1_rmse: 0.58796 |  0:00:43s
epoch 68 | loss: 0.21644 | val_0_rmse: 0.52136 | val_1_rmse: 0.58262 |  0:00:44s
epoch 69 | loss: 0.21271 | val_0_rmse: 0.51028 | val_1_rmse: 0.58315 |  0:00:45s
epoch 70 | loss: 0.21182 | val_0_rmse: 0.49874 | val_1_rmse: 0.57712 |  0:00:45s
epoch 71 | loss: 0.21159 | val_0_rmse: 0.50638 | val_1_rmse: 0.57859 |  0:00:46s
epoch 72 | loss: 0.21024 | val_0_rmse: 0.49914 | val_1_rmse: 0.575   |  0:00:47s
epoch 73 | loss: 0.20882 | val_0_rmse: 0.48615 | val_1_rmse: 0.56797 |  0:00:47s
epoch 74 | loss: 0.20675 | val_0_rmse: 0.48732 | val_1_rmse: 0.57448 |  0:00:48s
epoch 75 | loss: 0.20885 | val_0_rmse: 0.4867  | val_1_rmse: 0.56517 |  0:00:49s
epoch 76 | loss: 0.20991 | val_0_rmse: 0.49616 | val_1_rmse: 0.59221 |  0:00:49s
epoch 77 | loss: 0.20974 | val_0_rmse: 0.47892 | val_1_rmse: 0.56056 |  0:00:50s
epoch 78 | loss: 0.20216 | val_0_rmse: 0.46627 | val_1_rmse: 0.57585 |  0:00:50s
epoch 79 | loss: 0.20169 | val_0_rmse: 0.46758 | val_1_rmse: 0.5639  |  0:00:51s
epoch 80 | loss: 0.20797 | val_0_rmse: 0.47899 | val_1_rmse: 0.5679  |  0:00:52s
epoch 81 | loss: 0.20741 | val_0_rmse: 0.45132 | val_1_rmse: 0.55979 |  0:00:52s
epoch 82 | loss: 0.20005 | val_0_rmse: 0.45434 | val_1_rmse: 0.5579  |  0:00:53s
epoch 83 | loss: 0.19377 | val_0_rmse: 0.44234 | val_1_rmse: 0.56111 |  0:00:54s
epoch 84 | loss: 0.19565 | val_0_rmse: 0.44453 | val_1_rmse: 0.55608 |  0:00:54s
epoch 85 | loss: 0.19486 | val_0_rmse: 0.43573 | val_1_rmse: 0.56497 |  0:00:55s
epoch 86 | loss: 0.19284 | val_0_rmse: 0.43245 | val_1_rmse: 0.55149 |  0:00:56s
epoch 87 | loss: 0.19358 | val_0_rmse: 0.42943 | val_1_rmse: 0.55142 |  0:00:56s
epoch 88 | loss: 0.1854  | val_0_rmse: 0.42727 | val_1_rmse: 0.55509 |  0:00:57s
epoch 89 | loss: 0.18665 | val_0_rmse: 0.42612 | val_1_rmse: 0.5568  |  0:00:58s
epoch 90 | loss: 0.18653 | val_0_rmse: 0.42205 | val_1_rmse: 0.551   |  0:00:58s
epoch 91 | loss: 0.19112 | val_0_rmse: 0.42505 | val_1_rmse: 0.54214 |  0:00:59s
epoch 92 | loss: 0.18841 | val_0_rmse: 0.41411 | val_1_rmse: 0.54859 |  0:01:00s
epoch 93 | loss: 0.19055 | val_0_rmse: 0.42891 | val_1_rmse: 0.55445 |  0:01:00s
epoch 94 | loss: 0.1927  | val_0_rmse: 0.41484 | val_1_rmse: 0.56216 |  0:01:01s
epoch 95 | loss: 0.19189 | val_0_rmse: 0.40796 | val_1_rmse: 0.54998 |  0:01:01s
epoch 96 | loss: 0.18882 | val_0_rmse: 0.41304 | val_1_rmse: 0.55267 |  0:01:02s
epoch 97 | loss: 0.18508 | val_0_rmse: 0.40018 | val_1_rmse: 0.54807 |  0:01:03s
epoch 98 | loss: 0.18612 | val_0_rmse: 0.40301 | val_1_rmse: 0.54541 |  0:01:03s
epoch 99 | loss: 0.18351 | val_0_rmse: 0.40401 | val_1_rmse: 0.549   |  0:01:04s
epoch 100| loss: 0.18724 | val_0_rmse: 0.40002 | val_1_rmse: 0.54989 |  0:01:05s
epoch 101| loss: 0.18469 | val_0_rmse: 0.39975 | val_1_rmse: 0.56189 |  0:01:05s
epoch 102| loss: 0.18015 | val_0_rmse: 0.40223 | val_1_rmse: 0.55381 |  0:01:06s
epoch 103| loss: 0.17489 | val_0_rmse: 0.38952 | val_1_rmse: 0.56252 |  0:01:07s
epoch 104| loss: 0.179   | val_0_rmse: 0.3879  | val_1_rmse: 0.55956 |  0:01:07s
epoch 105| loss: 0.17422 | val_0_rmse: 0.38789 | val_1_rmse: 0.56032 |  0:01:08s
epoch 106| loss: 0.17233 | val_0_rmse: 0.3892  | val_1_rmse: 0.56991 |  0:01:08s
epoch 107| loss: 0.17151 | val_0_rmse: 0.39723 | val_1_rmse: 0.55819 |  0:01:09s
epoch 108| loss: 0.18201 | val_0_rmse: 0.38438 | val_1_rmse: 0.55968 |  0:01:10s
epoch 109| loss: 0.17238 | val_0_rmse: 0.38964 | val_1_rmse: 0.56881 |  0:01:10s
epoch 110| loss: 0.17082 | val_0_rmse: 0.39724 | val_1_rmse: 0.55938 |  0:01:11s
epoch 111| loss: 0.17953 | val_0_rmse: 0.38229 | val_1_rmse: 0.56169 |  0:01:12s
epoch 112| loss: 0.17204 | val_0_rmse: 0.38107 | val_1_rmse: 0.56497 |  0:01:12s
epoch 113| loss: 0.17262 | val_0_rmse: 0.38078 | val_1_rmse: 0.55853 |  0:01:13s
epoch 114| loss: 0.16867 | val_0_rmse: 0.37802 | val_1_rmse: 0.55787 |  0:01:14s
epoch 115| loss: 0.17245 | val_0_rmse: 0.37665 | val_1_rmse: 0.55261 |  0:01:14s
epoch 116| loss: 0.16851 | val_0_rmse: 0.37319 | val_1_rmse: 0.56256 |  0:01:15s
epoch 117| loss: 0.16279 | val_0_rmse: 0.36874 | val_1_rmse: 0.55446 |  0:01:16s
epoch 118| loss: 0.16593 | val_0_rmse: 0.37262 | val_1_rmse: 0.55148 |  0:01:16s
epoch 119| loss: 0.16687 | val_0_rmse: 0.3687  | val_1_rmse: 0.55428 |  0:01:17s
epoch 120| loss: 0.16135 | val_0_rmse: 0.36529 | val_1_rmse: 0.56131 |  0:01:17s
epoch 121| loss: 0.16081 | val_0_rmse: 0.3662  | val_1_rmse: 0.55834 |  0:01:18s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.54214
Best weights from best epoch are automatically used!
ended training at: 08:01:27
Feature importance:
Mean squared error is of 27828023710.794514
Mean absolute error:119255.52460677632
MAPE:0.2014693992073113
R2 score:0.6613432456420596
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:01:27
epoch 0  | loss: 1.93974 | val_0_rmse: 1.00811 | val_1_rmse: 0.98502 |  0:00:00s
epoch 1  | loss: 1.1334  | val_0_rmse: 1.00222 | val_1_rmse: 0.97685 |  0:00:01s
epoch 2  | loss: 1.02161 | val_0_rmse: 1.00489 | val_1_rmse: 0.97974 |  0:00:01s
epoch 3  | loss: 0.99304 | val_0_rmse: 1.00098 | val_1_rmse: 0.97492 |  0:00:02s
epoch 4  | loss: 0.97338 | val_0_rmse: 0.98281 | val_1_rmse: 0.95938 |  0:00:03s
epoch 5  | loss: 0.90599 | val_0_rmse: 0.94138 | val_1_rmse: 0.9193  |  0:00:03s
epoch 6  | loss: 0.84294 | val_0_rmse: 0.92577 | val_1_rmse: 0.90097 |  0:00:04s
epoch 7  | loss: 0.81979 | val_0_rmse: 0.9245  | val_1_rmse: 0.90157 |  0:00:05s
epoch 8  | loss: 0.77231 | val_0_rmse: 0.92481 | val_1_rmse: 0.89547 |  0:00:05s
epoch 9  | loss: 0.75127 | val_0_rmse: 0.90814 | val_1_rmse: 0.87605 |  0:00:06s
epoch 10 | loss: 0.71516 | val_0_rmse: 0.8723  | val_1_rmse: 0.83866 |  0:00:07s
epoch 11 | loss: 0.68923 | val_0_rmse: 0.87135 | val_1_rmse: 0.83506 |  0:00:07s
epoch 12 | loss: 0.63163 | val_0_rmse: 0.82984 | val_1_rmse: 0.79877 |  0:00:08s
epoch 13 | loss: 0.56318 | val_0_rmse: 0.79768 | val_1_rmse: 0.77086 |  0:00:09s
epoch 14 | loss: 0.47743 | val_0_rmse: 0.79988 | val_1_rmse: 0.77537 |  0:00:09s
epoch 15 | loss: 0.42775 | val_0_rmse: 0.78454 | val_1_rmse: 0.76057 |  0:00:10s
epoch 16 | loss: 0.39179 | val_0_rmse: 0.79217 | val_1_rmse: 0.77127 |  0:00:11s
epoch 17 | loss: 0.38117 | val_0_rmse: 0.76366 | val_1_rmse: 0.73939 |  0:00:11s
epoch 18 | loss: 0.36447 | val_0_rmse: 0.76761 | val_1_rmse: 0.73603 |  0:00:12s
epoch 19 | loss: 0.33471 | val_0_rmse: 0.74875 | val_1_rmse: 0.72072 |  0:00:12s
epoch 20 | loss: 0.3255  | val_0_rmse: 0.76186 | val_1_rmse: 0.73983 |  0:00:13s
epoch 21 | loss: 0.31482 | val_0_rmse: 0.74536 | val_1_rmse: 0.73384 |  0:00:14s
epoch 22 | loss: 0.29742 | val_0_rmse: 0.76177 | val_1_rmse: 0.73617 |  0:00:14s
epoch 23 | loss: 0.29569 | val_0_rmse: 0.72572 | val_1_rmse: 0.71052 |  0:00:15s
epoch 24 | loss: 0.28851 | val_0_rmse: 0.71195 | val_1_rmse: 0.6915  |  0:00:16s
epoch 25 | loss: 0.2789  | val_0_rmse: 0.71754 | val_1_rmse: 0.69274 |  0:00:16s
epoch 26 | loss: 0.27439 | val_0_rmse: 0.7225  | val_1_rmse: 0.70012 |  0:00:17s
epoch 27 | loss: 0.27246 | val_0_rmse: 0.71645 | val_1_rmse: 0.69927 |  0:00:18s
epoch 28 | loss: 0.27005 | val_0_rmse: 0.71984 | val_1_rmse: 0.69931 |  0:00:18s
epoch 29 | loss: 0.26918 | val_0_rmse: 0.69617 | val_1_rmse: 0.67718 |  0:00:19s
epoch 30 | loss: 0.25448 | val_0_rmse: 0.69402 | val_1_rmse: 0.67977 |  0:00:20s
epoch 31 | loss: 0.2522  | val_0_rmse: 0.69408 | val_1_rmse: 0.67599 |  0:00:20s
epoch 32 | loss: 0.25133 | val_0_rmse: 0.69761 | val_1_rmse: 0.67735 |  0:00:21s
epoch 33 | loss: 0.2523  | val_0_rmse: 0.69106 | val_1_rmse: 0.67154 |  0:00:21s
epoch 34 | loss: 0.25753 | val_0_rmse: 0.68996 | val_1_rmse: 0.67297 |  0:00:22s
epoch 35 | loss: 0.25012 | val_0_rmse: 0.6905  | val_1_rmse: 0.68682 |  0:00:23s
epoch 36 | loss: 0.26313 | val_0_rmse: 0.68272 | val_1_rmse: 0.67281 |  0:00:23s
epoch 37 | loss: 0.25222 | val_0_rmse: 0.67206 | val_1_rmse: 0.65298 |  0:00:24s
epoch 38 | loss: 0.24165 | val_0_rmse: 0.67364 | val_1_rmse: 0.65721 |  0:00:25s
epoch 39 | loss: 0.23603 | val_0_rmse: 0.66503 | val_1_rmse: 0.651   |  0:00:25s
epoch 40 | loss: 0.23852 | val_0_rmse: 0.6514  | val_1_rmse: 0.6398  |  0:00:26s
epoch 41 | loss: 0.23491 | val_0_rmse: 0.65776 | val_1_rmse: 0.65137 |  0:00:27s
epoch 42 | loss: 0.22914 | val_0_rmse: 0.64535 | val_1_rmse: 0.63976 |  0:00:27s
epoch 43 | loss: 0.23426 | val_0_rmse: 0.66106 | val_1_rmse: 0.65391 |  0:00:28s
epoch 44 | loss: 0.23962 | val_0_rmse: 0.63209 | val_1_rmse: 0.63037 |  0:00:29s
epoch 45 | loss: 0.23825 | val_0_rmse: 0.62752 | val_1_rmse: 0.62573 |  0:00:29s
epoch 46 | loss: 0.24943 | val_0_rmse: 0.65098 | val_1_rmse: 0.64628 |  0:00:30s
epoch 47 | loss: 0.25235 | val_0_rmse: 0.63743 | val_1_rmse: 0.6282  |  0:00:30s
epoch 48 | loss: 0.23449 | val_0_rmse: 0.60099 | val_1_rmse: 0.59858 |  0:00:31s
epoch 49 | loss: 0.2273  | val_0_rmse: 0.62625 | val_1_rmse: 0.62102 |  0:00:32s
epoch 50 | loss: 0.22581 | val_0_rmse: 0.59942 | val_1_rmse: 0.59625 |  0:00:32s
epoch 51 | loss: 0.22313 | val_0_rmse: 0.59386 | val_1_rmse: 0.59583 |  0:00:33s
epoch 52 | loss: 0.2272  | val_0_rmse: 0.58427 | val_1_rmse: 0.58664 |  0:00:34s
epoch 53 | loss: 0.21699 | val_0_rmse: 0.57047 | val_1_rmse: 0.57911 |  0:00:34s
epoch 54 | loss: 0.221   | val_0_rmse: 0.58853 | val_1_rmse: 0.59696 |  0:00:35s
epoch 55 | loss: 0.22293 | val_0_rmse: 0.56781 | val_1_rmse: 0.5739  |  0:00:36s
epoch 56 | loss: 0.21678 | val_0_rmse: 0.57006 | val_1_rmse: 0.57824 |  0:00:36s
epoch 57 | loss: 0.214   | val_0_rmse: 0.55376 | val_1_rmse: 0.56622 |  0:00:37s
epoch 58 | loss: 0.22296 | val_0_rmse: 0.55965 | val_1_rmse: 0.57947 |  0:00:38s
epoch 59 | loss: 0.21126 | val_0_rmse: 0.55491 | val_1_rmse: 0.57244 |  0:00:38s
epoch 60 | loss: 0.20905 | val_0_rmse: 0.5528  | val_1_rmse: 0.56777 |  0:00:39s
epoch 61 | loss: 0.21305 | val_0_rmse: 0.53072 | val_1_rmse: 0.55352 |  0:00:40s
epoch 62 | loss: 0.21442 | val_0_rmse: 0.5417  | val_1_rmse: 0.56188 |  0:00:40s
epoch 63 | loss: 0.21174 | val_0_rmse: 0.52728 | val_1_rmse: 0.54756 |  0:00:41s
epoch 64 | loss: 0.21216 | val_0_rmse: 0.54517 | val_1_rmse: 0.56646 |  0:00:42s
epoch 65 | loss: 0.21881 | val_0_rmse: 0.52485 | val_1_rmse: 0.55651 |  0:00:42s
epoch 66 | loss: 0.20967 | val_0_rmse: 0.50735 | val_1_rmse: 0.54718 |  0:00:43s
epoch 67 | loss: 0.2066  | val_0_rmse: 0.51878 | val_1_rmse: 0.55378 |  0:00:43s
epoch 68 | loss: 0.20088 | val_0_rmse: 0.49329 | val_1_rmse: 0.54395 |  0:00:44s
epoch 69 | loss: 0.19872 | val_0_rmse: 0.48757 | val_1_rmse: 0.54222 |  0:00:45s
epoch 70 | loss: 0.19186 | val_0_rmse: 0.49037 | val_1_rmse: 0.54357 |  0:00:45s
epoch 71 | loss: 0.19182 | val_0_rmse: 0.4821  | val_1_rmse: 0.53911 |  0:00:46s
epoch 72 | loss: 0.19484 | val_0_rmse: 0.47601 | val_1_rmse: 0.53856 |  0:00:47s
epoch 73 | loss: 0.19503 | val_0_rmse: 0.48159 | val_1_rmse: 0.54564 |  0:00:47s
epoch 74 | loss: 0.18825 | val_0_rmse: 0.47552 | val_1_rmse: 0.54028 |  0:00:48s
epoch 75 | loss: 0.18415 | val_0_rmse: 0.46923 | val_1_rmse: 0.54134 |  0:00:49s
epoch 76 | loss: 0.18104 | val_0_rmse: 0.45835 | val_1_rmse: 0.534   |  0:00:49s
epoch 77 | loss: 0.18202 | val_0_rmse: 0.45716 | val_1_rmse: 0.53314 |  0:00:50s
epoch 78 | loss: 0.18714 | val_0_rmse: 0.44434 | val_1_rmse: 0.53155 |  0:00:50s
epoch 79 | loss: 0.18693 | val_0_rmse: 0.44644 | val_1_rmse: 0.53161 |  0:00:51s
epoch 80 | loss: 0.18717 | val_0_rmse: 0.44507 | val_1_rmse: 0.53616 |  0:00:52s
epoch 81 | loss: 0.18655 | val_0_rmse: 0.45765 | val_1_rmse: 0.54737 |  0:00:52s
epoch 82 | loss: 0.19059 | val_0_rmse: 0.4321  | val_1_rmse: 0.53183 |  0:00:53s
epoch 83 | loss: 0.19449 | val_0_rmse: 0.42697 | val_1_rmse: 0.53851 |  0:00:54s
epoch 84 | loss: 0.18254 | val_0_rmse: 0.4341  | val_1_rmse: 0.53279 |  0:00:54s
epoch 85 | loss: 0.18464 | val_0_rmse: 0.42827 | val_1_rmse: 0.53921 |  0:00:55s
epoch 86 | loss: 0.18584 | val_0_rmse: 0.42508 | val_1_rmse: 0.52934 |  0:00:56s
epoch 87 | loss: 0.1779  | val_0_rmse: 0.43034 | val_1_rmse: 0.53623 |  0:00:56s
epoch 88 | loss: 0.17863 | val_0_rmse: 0.40909 | val_1_rmse: 0.53013 |  0:00:57s
epoch 89 | loss: 0.17801 | val_0_rmse: 0.42368 | val_1_rmse: 0.53458 |  0:00:58s
epoch 90 | loss: 0.17776 | val_0_rmse: 0.41032 | val_1_rmse: 0.5376  |  0:00:58s
epoch 91 | loss: 0.18006 | val_0_rmse: 0.40497 | val_1_rmse: 0.53349 |  0:00:59s
epoch 92 | loss: 0.17567 | val_0_rmse: 0.3993  | val_1_rmse: 0.53467 |  0:00:59s
epoch 93 | loss: 0.17976 | val_0_rmse: 0.40282 | val_1_rmse: 0.54045 |  0:01:00s
epoch 94 | loss: 0.17869 | val_0_rmse: 0.39341 | val_1_rmse: 0.53846 |  0:01:01s
epoch 95 | loss: 0.17499 | val_0_rmse: 0.39018 | val_1_rmse: 0.53139 |  0:01:01s
epoch 96 | loss: 0.17103 | val_0_rmse: 0.38696 | val_1_rmse: 0.53785 |  0:01:02s
epoch 97 | loss: 0.17591 | val_0_rmse: 0.38485 | val_1_rmse: 0.5413  |  0:01:03s
epoch 98 | loss: 0.16777 | val_0_rmse: 0.38467 | val_1_rmse: 0.53756 |  0:01:03s
epoch 99 | loss: 0.165   | val_0_rmse: 0.37794 | val_1_rmse: 0.53695 |  0:01:04s
epoch 100| loss: 0.16756 | val_0_rmse: 0.39187 | val_1_rmse: 0.5584  |  0:01:05s
epoch 101| loss: 0.17121 | val_0_rmse: 0.38818 | val_1_rmse: 0.5481  |  0:01:05s
epoch 102| loss: 0.17416 | val_0_rmse: 0.3943  | val_1_rmse: 0.54891 |  0:01:06s
epoch 103| loss: 0.17704 | val_0_rmse: 0.3841  | val_1_rmse: 0.55982 |  0:01:06s
epoch 104| loss: 0.17694 | val_0_rmse: 0.37614 | val_1_rmse: 0.54661 |  0:01:07s
epoch 105| loss: 0.16669 | val_0_rmse: 0.38151 | val_1_rmse: 0.54714 |  0:01:08s
epoch 106| loss: 0.16591 | val_0_rmse: 0.3695  | val_1_rmse: 0.54567 |  0:01:08s
epoch 107| loss: 0.16272 | val_0_rmse: 0.36741 | val_1_rmse: 0.55258 |  0:01:09s
epoch 108| loss: 0.16314 | val_0_rmse: 0.36684 | val_1_rmse: 0.54691 |  0:01:10s
epoch 109| loss: 0.16309 | val_0_rmse: 0.37025 | val_1_rmse: 0.5471  |  0:01:10s
epoch 110| loss: 0.15943 | val_0_rmse: 0.36666 | val_1_rmse: 0.55793 |  0:01:11s
epoch 111| loss: 0.15917 | val_0_rmse: 0.36589 | val_1_rmse: 0.55743 |  0:01:12s
epoch 112| loss: 0.16045 | val_0_rmse: 0.36517 | val_1_rmse: 0.55063 |  0:01:12s
epoch 113| loss: 0.15815 | val_0_rmse: 0.37155 | val_1_rmse: 0.57326 |  0:01:13s
epoch 114| loss: 0.16665 | val_0_rmse: 0.37732 | val_1_rmse: 0.54816 |  0:01:14s
epoch 115| loss: 0.1655  | val_0_rmse: 0.3659  | val_1_rmse: 0.56325 |  0:01:14s
epoch 116| loss: 0.16259 | val_0_rmse: 0.36261 | val_1_rmse: 0.55118 |  0:01:15s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.52934
Best weights from best epoch are automatically used!
ended training at: 08:02:42
Feature importance:
Mean squared error is of 26082619615.621094
Mean absolute error:117587.25639092104
MAPE:0.20167972362048922
R2 score:0.6764241988566011
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:42
epoch 0  | loss: 2.55575 | val_0_rmse: 1.09116 | val_1_rmse: 1.09375 |  0:00:00s
epoch 1  | loss: 1.64738 | val_0_rmse: 1.02808 | val_1_rmse: 1.02995 |  0:00:00s
epoch 2  | loss: 1.47774 | val_0_rmse: 1.01699 | val_1_rmse: 1.01874 |  0:00:00s
epoch 3  | loss: 1.25123 | val_0_rmse: 1.01495 | val_1_rmse: 1.01691 |  0:00:00s
epoch 4  | loss: 1.13225 | val_0_rmse: 1.0141  | val_1_rmse: 1.01661 |  0:00:00s
epoch 5  | loss: 1.18276 | val_0_rmse: 1.01569 | val_1_rmse: 1.01839 |  0:00:00s
epoch 6  | loss: 1.06975 | val_0_rmse: 1.01746 | val_1_rmse: 1.01998 |  0:00:01s
epoch 7  | loss: 1.04334 | val_0_rmse: 1.01568 | val_1_rmse: 1.01814 |  0:00:01s
epoch 8  | loss: 0.97662 | val_0_rmse: 1.01542 | val_1_rmse: 1.01793 |  0:00:01s
epoch 9  | loss: 0.96033 | val_0_rmse: 1.01151 | val_1_rmse: 1.0141  |  0:00:01s
epoch 10 | loss: 0.9416  | val_0_rmse: 0.98237 | val_1_rmse: 0.98749 |  0:00:01s
epoch 11 | loss: 0.91627 | val_0_rmse: 0.93846 | val_1_rmse: 0.9474  |  0:00:01s
epoch 12 | loss: 0.87836 | val_0_rmse: 0.87043 | val_1_rmse: 0.86938 |  0:00:02s
epoch 13 | loss: 0.85007 | val_0_rmse: 0.84589 | val_1_rmse: 0.79657 |  0:00:02s
epoch 14 | loss: 0.78916 | val_0_rmse: 0.82569 | val_1_rmse: 0.78042 |  0:00:02s
epoch 15 | loss: 0.72856 | val_0_rmse: 0.82601 | val_1_rmse: 0.78913 |  0:00:02s
epoch 16 | loss: 0.70613 | val_0_rmse: 0.91076 | val_1_rmse: 0.85389 |  0:00:02s
epoch 17 | loss: 0.67653 | val_0_rmse: 0.93409 | val_1_rmse: 0.87928 |  0:00:02s
epoch 18 | loss: 0.63852 | val_0_rmse: 0.89484 | val_1_rmse: 0.86067 |  0:00:03s
epoch 19 | loss: 0.62168 | val_0_rmse: 0.90971 | val_1_rmse: 0.87346 |  0:00:03s
epoch 20 | loss: 0.64611 | val_0_rmse: 0.90982 | val_1_rmse: 0.87876 |  0:00:03s
epoch 21 | loss: 0.62389 | val_0_rmse: 0.91586 | val_1_rmse: 0.88076 |  0:00:03s
epoch 22 | loss: 0.59375 | val_0_rmse: 0.91929 | val_1_rmse: 0.87176 |  0:00:03s
epoch 23 | loss: 0.57222 | val_0_rmse: 0.86431 | val_1_rmse: 0.83503 |  0:00:03s
epoch 24 | loss: 0.55549 | val_0_rmse: 0.87935 | val_1_rmse: 0.84413 |  0:00:04s
epoch 25 | loss: 0.56124 | val_0_rmse: 0.90828 | val_1_rmse: 0.8636  |  0:00:04s
epoch 26 | loss: 0.5438  | val_0_rmse: 0.86442 | val_1_rmse: 0.83682 |  0:00:04s
epoch 27 | loss: 0.53385 | val_0_rmse: 0.84829 | val_1_rmse: 0.82652 |  0:00:04s
epoch 28 | loss: 0.52916 | val_0_rmse: 0.84784 | val_1_rmse: 0.81259 |  0:00:04s
epoch 29 | loss: 0.53042 | val_0_rmse: 0.81963 | val_1_rmse: 0.79237 |  0:00:04s
epoch 30 | loss: 0.53683 | val_0_rmse: 0.80955 | val_1_rmse: 0.78623 |  0:00:05s
epoch 31 | loss: 0.52676 | val_0_rmse: 0.81379 | val_1_rmse: 0.78727 |  0:00:05s
epoch 32 | loss: 0.51525 | val_0_rmse: 0.80662 | val_1_rmse: 0.781   |  0:00:05s
epoch 33 | loss: 0.49743 | val_0_rmse: 0.79743 | val_1_rmse: 0.77348 |  0:00:05s
epoch 34 | loss: 0.50491 | val_0_rmse: 0.78684 | val_1_rmse: 0.76697 |  0:00:05s
epoch 35 | loss: 0.51701 | val_0_rmse: 0.78613 | val_1_rmse: 0.76752 |  0:00:05s
epoch 36 | loss: 0.50715 | val_0_rmse: 0.79993 | val_1_rmse: 0.78454 |  0:00:05s
epoch 37 | loss: 0.50077 | val_0_rmse: 0.78778 | val_1_rmse: 0.77603 |  0:00:06s
epoch 38 | loss: 0.48326 | val_0_rmse: 0.77927 | val_1_rmse: 0.76434 |  0:00:06s
epoch 39 | loss: 0.48959 | val_0_rmse: 0.81369 | val_1_rmse: 0.7977  |  0:00:06s
epoch 40 | loss: 0.48984 | val_0_rmse: 0.87613 | val_1_rmse: 0.86089 |  0:00:06s
epoch 41 | loss: 0.49002 | val_0_rmse: 0.85017 | val_1_rmse: 0.84065 |  0:00:06s
epoch 42 | loss: 0.48728 | val_0_rmse: 0.81378 | val_1_rmse: 0.80158 |  0:00:06s
epoch 43 | loss: 0.48196 | val_0_rmse: 0.80785 | val_1_rmse: 0.78518 |  0:00:07s
epoch 44 | loss: 0.46794 | val_0_rmse: 0.81365 | val_1_rmse: 0.78634 |  0:00:07s
epoch 45 | loss: 0.47231 | val_0_rmse: 0.79034 | val_1_rmse: 0.76896 |  0:00:07s
epoch 46 | loss: 0.45977 | val_0_rmse: 0.79719 | val_1_rmse: 0.78077 |  0:00:07s
epoch 47 | loss: 0.44968 | val_0_rmse: 0.81349 | val_1_rmse: 0.80515 |  0:00:07s
epoch 48 | loss: 0.43039 | val_0_rmse: 0.82303 | val_1_rmse: 0.81987 |  0:00:07s
epoch 49 | loss: 0.42993 | val_0_rmse: 0.85559 | val_1_rmse: 0.8419  |  0:00:08s
epoch 50 | loss: 0.42079 | val_0_rmse: 0.87517 | val_1_rmse: 0.84306 |  0:00:08s
epoch 51 | loss: 0.40567 | val_0_rmse: 0.85156 | val_1_rmse: 0.81154 |  0:00:08s
epoch 52 | loss: 0.42118 | val_0_rmse: 0.82765 | val_1_rmse: 0.79449 |  0:00:08s
epoch 53 | loss: 0.3956  | val_0_rmse: 0.84129 | val_1_rmse: 0.81583 |  0:00:08s
epoch 54 | loss: 0.38826 | val_0_rmse: 0.85743 | val_1_rmse: 0.83404 |  0:00:08s
epoch 55 | loss: 0.37548 | val_0_rmse: 0.85703 | val_1_rmse: 0.83538 |  0:00:08s
epoch 56 | loss: 0.39293 | val_0_rmse: 0.81283 | val_1_rmse: 0.79965 |  0:00:09s
epoch 57 | loss: 0.38859 | val_0_rmse: 0.78679 | val_1_rmse: 0.78422 |  0:00:09s
epoch 58 | loss: 0.38585 | val_0_rmse: 0.83534 | val_1_rmse: 0.82855 |  0:00:09s
epoch 59 | loss: 0.3674  | val_0_rmse: 0.81594 | val_1_rmse: 0.80486 |  0:00:09s
epoch 60 | loss: 0.36748 | val_0_rmse: 0.7589  | val_1_rmse: 0.74996 |  0:00:09s
epoch 61 | loss: 0.35666 | val_0_rmse: 0.7519  | val_1_rmse: 0.74258 |  0:00:09s
epoch 62 | loss: 0.35412 | val_0_rmse: 0.75943 | val_1_rmse: 0.75024 |  0:00:10s
epoch 63 | loss: 0.34681 | val_0_rmse: 0.72017 | val_1_rmse: 0.72012 |  0:00:10s
epoch 64 | loss: 0.35059 | val_0_rmse: 0.70734 | val_1_rmse: 0.70919 |  0:00:10s
epoch 65 | loss: 0.34813 | val_0_rmse: 0.74866 | val_1_rmse: 0.73552 |  0:00:10s
epoch 66 | loss: 0.3502  | val_0_rmse: 0.75254 | val_1_rmse: 0.73837 |  0:00:10s
epoch 67 | loss: 0.33623 | val_0_rmse: 0.7203  | val_1_rmse: 0.71256 |  0:00:10s
epoch 68 | loss: 0.33371 | val_0_rmse: 0.7162  | val_1_rmse: 0.71189 |  0:00:11s
epoch 69 | loss: 0.32723 | val_0_rmse: 0.73679 | val_1_rmse: 0.73294 |  0:00:11s
epoch 70 | loss: 0.33694 | val_0_rmse: 0.72231 | val_1_rmse: 0.72216 |  0:00:11s
epoch 71 | loss: 0.34254 | val_0_rmse: 0.7049  | val_1_rmse: 0.70895 |  0:00:11s
epoch 72 | loss: 0.34856 | val_0_rmse: 0.71158 | val_1_rmse: 0.7158  |  0:00:11s
epoch 73 | loss: 0.32597 | val_0_rmse: 0.70852 | val_1_rmse: 0.71369 |  0:00:11s
epoch 74 | loss: 0.31762 | val_0_rmse: 0.6995  | val_1_rmse: 0.70661 |  0:00:11s
epoch 75 | loss: 0.3225  | val_0_rmse: 0.69873 | val_1_rmse: 0.70883 |  0:00:12s
epoch 76 | loss: 0.32174 | val_0_rmse: 0.70039 | val_1_rmse: 0.71208 |  0:00:12s
epoch 77 | loss: 0.31849 | val_0_rmse: 0.69568 | val_1_rmse: 0.707   |  0:00:12s
epoch 78 | loss: 0.31479 | val_0_rmse: 0.69558 | val_1_rmse: 0.70277 |  0:00:12s
epoch 79 | loss: 0.32859 | val_0_rmse: 0.71506 | val_1_rmse: 0.71633 |  0:00:12s
epoch 80 | loss: 0.31355 | val_0_rmse: 0.73433 | val_1_rmse: 0.7311  |  0:00:12s
epoch 81 | loss: 0.32099 | val_0_rmse: 0.71394 | val_1_rmse: 0.71156 |  0:00:13s
epoch 82 | loss: 0.31625 | val_0_rmse: 0.68141 | val_1_rmse: 0.6871  |  0:00:13s
epoch 83 | loss: 0.32792 | val_0_rmse: 0.68058 | val_1_rmse: 0.69193 |  0:00:13s
epoch 84 | loss: 0.31924 | val_0_rmse: 0.71154 | val_1_rmse: 0.7181  |  0:00:13s
epoch 85 | loss: 0.31943 | val_0_rmse: 0.72198 | val_1_rmse: 0.73122 |  0:00:13s
epoch 86 | loss: 0.33807 | val_0_rmse: 0.69109 | val_1_rmse: 0.707   |  0:00:13s
epoch 87 | loss: 0.30487 | val_0_rmse: 0.67816 | val_1_rmse: 0.69595 |  0:00:14s
epoch 88 | loss: 0.31356 | val_0_rmse: 0.68201 | val_1_rmse: 0.69678 |  0:00:14s
epoch 89 | loss: 0.30854 | val_0_rmse: 0.69514 | val_1_rmse: 0.70377 |  0:00:14s
epoch 90 | loss: 0.32144 | val_0_rmse: 0.69418 | val_1_rmse: 0.70265 |  0:00:14s
epoch 91 | loss: 0.30363 | val_0_rmse: 0.67387 | val_1_rmse: 0.68592 |  0:00:14s
epoch 92 | loss: 0.31081 | val_0_rmse: 0.66366 | val_1_rmse: 0.68085 |  0:00:14s
epoch 93 | loss: 0.30517 | val_0_rmse: 0.65715 | val_1_rmse: 0.67959 |  0:00:14s
epoch 94 | loss: 0.29864 | val_0_rmse: 0.65579 | val_1_rmse: 0.685   |  0:00:15s
epoch 95 | loss: 0.30329 | val_0_rmse: 0.65987 | val_1_rmse: 0.68528 |  0:00:15s
epoch 96 | loss: 0.30136 | val_0_rmse: 0.66276 | val_1_rmse: 0.68408 |  0:00:15s
epoch 97 | loss: 0.30086 | val_0_rmse: 0.65958 | val_1_rmse: 0.68351 |  0:00:15s
epoch 98 | loss: 0.29864 | val_0_rmse: 0.66141 | val_1_rmse: 0.69177 |  0:00:15s
epoch 99 | loss: 0.31277 | val_0_rmse: 0.65724 | val_1_rmse: 0.6854  |  0:00:15s
epoch 100| loss: 0.29791 | val_0_rmse: 0.66499 | val_1_rmse: 0.68565 |  0:00:16s
epoch 101| loss: 0.29225 | val_0_rmse: 0.67104 | val_1_rmse: 0.69224 |  0:00:16s
epoch 102| loss: 0.28875 | val_0_rmse: 0.67415 | val_1_rmse: 0.70223 |  0:00:16s
epoch 103| loss: 0.29263 | val_0_rmse: 0.67554 | val_1_rmse: 0.70632 |  0:00:16s
epoch 104| loss: 0.29601 | val_0_rmse: 0.67703 | val_1_rmse: 0.70217 |  0:00:16s
epoch 105| loss: 0.28567 | val_0_rmse: 0.70496 | val_1_rmse: 0.71775 |  0:00:16s
epoch 106| loss: 0.29722 | val_0_rmse: 0.72156 | val_1_rmse: 0.72744 |  0:00:17s
epoch 107| loss: 0.29234 | val_0_rmse: 0.68953 | val_1_rmse: 0.69911 |  0:00:17s
epoch 108| loss: 0.28291 | val_0_rmse: 0.67105 | val_1_rmse: 0.68888 |  0:00:17s
epoch 109| loss: 0.28217 | val_0_rmse: 0.67877 | val_1_rmse: 0.70311 |  0:00:17s
epoch 110| loss: 0.27931 | val_0_rmse: 0.68525 | val_1_rmse: 0.70794 |  0:00:17s
epoch 111| loss: 0.27345 | val_0_rmse: 0.67494 | val_1_rmse: 0.69457 |  0:00:17s
epoch 112| loss: 0.27239 | val_0_rmse: 0.6596  | val_1_rmse: 0.68903 |  0:00:17s
epoch 113| loss: 0.27436 | val_0_rmse: 0.65296 | val_1_rmse: 0.68579 |  0:00:18s
epoch 114| loss: 0.26276 | val_0_rmse: 0.66251 | val_1_rmse: 0.69479 |  0:00:18s
epoch 115| loss: 0.26866 | val_0_rmse: 0.67245 | val_1_rmse: 0.70016 |  0:00:18s
epoch 116| loss: 0.28666 | val_0_rmse: 0.67276 | val_1_rmse: 0.69951 |  0:00:18s
epoch 117| loss: 0.28354 | val_0_rmse: 0.68548 | val_1_rmse: 0.70849 |  0:00:18s
epoch 118| loss: 0.29955 | val_0_rmse: 0.6654  | val_1_rmse: 0.68803 |  0:00:18s
epoch 119| loss: 0.29073 | val_0_rmse: 0.67491 | val_1_rmse: 0.69071 |  0:00:19s
epoch 120| loss: 0.2982  | val_0_rmse: 0.67181 | val_1_rmse: 0.68367 |  0:00:19s
epoch 121| loss: 0.2793  | val_0_rmse: 0.65861 | val_1_rmse: 0.66731 |  0:00:19s
epoch 122| loss: 0.27373 | val_0_rmse: 0.6582  | val_1_rmse: 0.66096 |  0:00:19s
epoch 123| loss: 0.28189 | val_0_rmse: 0.66963 | val_1_rmse: 0.6758  |  0:00:19s
epoch 124| loss: 0.27588 | val_0_rmse: 0.67972 | val_1_rmse: 0.69759 |  0:00:19s
epoch 125| loss: 0.27431 | val_0_rmse: 0.66745 | val_1_rmse: 0.69493 |  0:00:20s
epoch 126| loss: 0.26106 | val_0_rmse: 0.66285 | val_1_rmse: 0.69544 |  0:00:20s
epoch 127| loss: 0.27378 | val_0_rmse: 0.66717 | val_1_rmse: 0.703   |  0:00:20s
epoch 128| loss: 0.28161 | val_0_rmse: 0.66467 | val_1_rmse: 0.71451 |  0:00:20s
epoch 129| loss: 0.2576  | val_0_rmse: 0.65473 | val_1_rmse: 0.70189 |  0:00:20s
epoch 130| loss: 0.25818 | val_0_rmse: 0.65411 | val_1_rmse: 0.69275 |  0:00:20s
epoch 131| loss: 0.26367 | val_0_rmse: 0.65649 | val_1_rmse: 0.69086 |  0:00:21s
epoch 132| loss: 0.25553 | val_0_rmse: 0.65071 | val_1_rmse: 0.6856  |  0:00:21s
epoch 133| loss: 0.25339 | val_0_rmse: 0.64694 | val_1_rmse: 0.68115 |  0:00:21s
epoch 134| loss: 0.26475 | val_0_rmse: 0.65783 | val_1_rmse: 0.67613 |  0:00:21s
epoch 135| loss: 0.26257 | val_0_rmse: 0.67682 | val_1_rmse: 0.69634 |  0:00:21s
epoch 136| loss: 0.26159 | val_0_rmse: 0.65818 | val_1_rmse: 0.69713 |  0:00:21s
epoch 137| loss: 0.26446 | val_0_rmse: 0.65487 | val_1_rmse: 0.70488 |  0:00:21s
epoch 138| loss: 0.2608  | val_0_rmse: 0.66629 | val_1_rmse: 0.72026 |  0:00:22s
epoch 139| loss: 0.26834 | val_0_rmse: 0.66217 | val_1_rmse: 0.7137  |  0:00:22s
epoch 140| loss: 0.26497 | val_0_rmse: 0.64924 | val_1_rmse: 0.69709 |  0:00:22s
epoch 141| loss: 0.26996 | val_0_rmse: 0.64813 | val_1_rmse: 0.69206 |  0:00:22s
epoch 142| loss: 0.25807 | val_0_rmse: 0.64688 | val_1_rmse: 0.69017 |  0:00:22s
epoch 143| loss: 0.25792 | val_0_rmse: 0.63359 | val_1_rmse: 0.67646 |  0:00:22s
epoch 144| loss: 0.25385 | val_0_rmse: 0.622   | val_1_rmse: 0.67045 |  0:00:23s
epoch 145| loss: 0.25101 | val_0_rmse: 0.61907 | val_1_rmse: 0.66958 |  0:00:23s
epoch 146| loss: 0.25407 | val_0_rmse: 0.61998 | val_1_rmse: 0.67243 |  0:00:23s
epoch 147| loss: 0.25734 | val_0_rmse: 0.6251  | val_1_rmse: 0.67896 |  0:00:23s
epoch 148| loss: 0.24902 | val_0_rmse: 0.627   | val_1_rmse: 0.68456 |  0:00:23s
epoch 149| loss: 0.26304 | val_0_rmse: 0.62552 | val_1_rmse: 0.68524 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.66096
Best weights from best epoch are automatically used!
ended training at: 08:03:06
Feature importance:
Mean squared error is of 3210081151.5385733
Mean absolute error:43402.2544400059
MAPE:0.4015161496747145
R2 score:0.470572577021553
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:07
epoch 0  | loss: 3.15375 | val_0_rmse: 1.03649 | val_1_rmse: 0.93502 |  0:00:00s
epoch 1  | loss: 1.79718 | val_0_rmse: 1.02816 | val_1_rmse: 0.93326 |  0:00:00s
epoch 2  | loss: 1.39995 | val_0_rmse: 1.02492 | val_1_rmse: 0.93439 |  0:00:00s
epoch 3  | loss: 1.37207 | val_0_rmse: 1.02344 | val_1_rmse: 0.93026 |  0:00:00s
epoch 4  | loss: 1.15128 | val_0_rmse: 1.02427 | val_1_rmse: 0.92712 |  0:00:00s
epoch 5  | loss: 1.09427 | val_0_rmse: 1.0244  | val_1_rmse: 0.92425 |  0:00:01s
epoch 6  | loss: 1.05658 | val_0_rmse: 1.02432 | val_1_rmse: 0.92318 |  0:00:01s
epoch 7  | loss: 1.01898 | val_0_rmse: 1.02409 | val_1_rmse: 0.92232 |  0:00:01s
epoch 8  | loss: 1.00628 | val_0_rmse: 1.02284 | val_1_rmse: 0.92266 |  0:00:01s
epoch 9  | loss: 1.00818 | val_0_rmse: 1.02116 | val_1_rmse: 0.91894 |  0:00:01s
epoch 10 | loss: 1.03293 | val_0_rmse: 1.02604 | val_1_rmse: 0.91956 |  0:00:01s
epoch 11 | loss: 0.99549 | val_0_rmse: 1.02429 | val_1_rmse: 0.92056 |  0:00:02s
epoch 12 | loss: 0.98296 | val_0_rmse: 1.02527 | val_1_rmse: 0.92207 |  0:00:02s
epoch 13 | loss: 0.93249 | val_0_rmse: 1.02402 | val_1_rmse: 0.91959 |  0:00:02s
epoch 14 | loss: 0.93349 | val_0_rmse: 1.01334 | val_1_rmse: 0.91138 |  0:00:02s
epoch 15 | loss: 0.90167 | val_0_rmse: 0.99466 | val_1_rmse: 0.89545 |  0:00:02s
epoch 16 | loss: 0.89988 | val_0_rmse: 0.9789  | val_1_rmse: 0.87536 |  0:00:02s
epoch 17 | loss: 0.84667 | val_0_rmse: 0.97546 | val_1_rmse: 0.86918 |  0:00:02s
epoch 18 | loss: 0.79805 | val_0_rmse: 0.98135 | val_1_rmse: 0.88103 |  0:00:03s
epoch 19 | loss: 0.74229 | val_0_rmse: 0.89204 | val_1_rmse: 0.80202 |  0:00:03s
epoch 20 | loss: 0.71399 | val_0_rmse: 0.83717 | val_1_rmse: 0.75418 |  0:00:03s
epoch 21 | loss: 0.70356 | val_0_rmse: 0.81439 | val_1_rmse: 0.73467 |  0:00:03s
epoch 22 | loss: 0.68185 | val_0_rmse: 0.80126 | val_1_rmse: 0.7256  |  0:00:03s
epoch 23 | loss: 0.65218 | val_0_rmse: 0.7964  | val_1_rmse: 0.72585 |  0:00:03s
epoch 24 | loss: 0.62485 | val_0_rmse: 0.7938  | val_1_rmse: 0.71812 |  0:00:04s
epoch 25 | loss: 0.61091 | val_0_rmse: 0.78846 | val_1_rmse: 0.70486 |  0:00:04s
epoch 26 | loss: 0.5944  | val_0_rmse: 0.78104 | val_1_rmse: 0.7024  |  0:00:04s
epoch 27 | loss: 0.59813 | val_0_rmse: 0.77371 | val_1_rmse: 0.69535 |  0:00:04s
epoch 28 | loss: 0.58138 | val_0_rmse: 0.77264 | val_1_rmse: 0.68331 |  0:00:04s
epoch 29 | loss: 0.55099 | val_0_rmse: 0.77625 | val_1_rmse: 0.6973  |  0:00:04s
epoch 30 | loss: 0.55032 | val_0_rmse: 0.78174 | val_1_rmse: 0.7043  |  0:00:05s
epoch 31 | loss: 0.55541 | val_0_rmse: 0.78232 | val_1_rmse: 0.70276 |  0:00:05s
epoch 32 | loss: 0.5472  | val_0_rmse: 0.78007 | val_1_rmse: 0.70091 |  0:00:05s
epoch 33 | loss: 0.53703 | val_0_rmse: 0.77776 | val_1_rmse: 0.6961  |  0:00:05s
epoch 34 | loss: 0.50278 | val_0_rmse: 0.77777 | val_1_rmse: 0.69355 |  0:00:05s
epoch 35 | loss: 0.50998 | val_0_rmse: 0.79287 | val_1_rmse: 0.70187 |  0:00:05s
epoch 36 | loss: 0.50663 | val_0_rmse: 0.8087  | val_1_rmse: 0.7141  |  0:00:06s
epoch 37 | loss: 0.49384 | val_0_rmse: 0.81142 | val_1_rmse: 0.72156 |  0:00:06s
epoch 38 | loss: 0.52408 | val_0_rmse: 0.80063 | val_1_rmse: 0.71331 |  0:00:06s
epoch 39 | loss: 0.52509 | val_0_rmse: 0.79579 | val_1_rmse: 0.71272 |  0:00:06s
epoch 40 | loss: 0.49398 | val_0_rmse: 0.79463 | val_1_rmse: 0.7285  |  0:00:06s
epoch 41 | loss: 0.49777 | val_0_rmse: 0.79859 | val_1_rmse: 0.72749 |  0:00:06s
epoch 42 | loss: 0.45657 | val_0_rmse: 0.82421 | val_1_rmse: 0.74732 |  0:00:06s
epoch 43 | loss: 0.48982 | val_0_rmse: 0.81342 | val_1_rmse: 0.74558 |  0:00:07s
epoch 44 | loss: 0.45193 | val_0_rmse: 0.80348 | val_1_rmse: 0.74764 |  0:00:07s
epoch 45 | loss: 0.47643 | val_0_rmse: 0.78609 | val_1_rmse: 0.7246  |  0:00:07s
epoch 46 | loss: 0.43998 | val_0_rmse: 0.80144 | val_1_rmse: 0.73059 |  0:00:07s
epoch 47 | loss: 0.45832 | val_0_rmse: 0.79762 | val_1_rmse: 0.72825 |  0:00:07s
epoch 48 | loss: 0.42831 | val_0_rmse: 0.79869 | val_1_rmse: 0.74087 |  0:00:07s
epoch 49 | loss: 0.42478 | val_0_rmse: 0.80393 | val_1_rmse: 0.7535  |  0:00:08s
epoch 50 | loss: 0.41019 | val_0_rmse: 0.83363 | val_1_rmse: 0.77642 |  0:00:08s
epoch 51 | loss: 0.41779 | val_0_rmse: 0.84397 | val_1_rmse: 0.79033 |  0:00:08s
epoch 52 | loss: 0.37701 | val_0_rmse: 0.82604 | val_1_rmse: 0.79434 |  0:00:08s
epoch 53 | loss: 0.39367 | val_0_rmse: 0.81653 | val_1_rmse: 0.79204 |  0:00:08s
epoch 54 | loss: 0.37409 | val_0_rmse: 0.80144 | val_1_rmse: 0.76862 |  0:00:08s
epoch 55 | loss: 0.3656  | val_0_rmse: 0.77129 | val_1_rmse: 0.73282 |  0:00:09s
epoch 56 | loss: 0.35464 | val_0_rmse: 0.75279 | val_1_rmse: 0.71228 |  0:00:09s
epoch 57 | loss: 0.35747 | val_0_rmse: 0.74656 | val_1_rmse: 0.70585 |  0:00:09s
epoch 58 | loss: 0.37107 | val_0_rmse: 0.74645 | val_1_rmse: 0.71159 |  0:00:09s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.68331
Best weights from best epoch are automatically used!
ended training at: 08:03:16
Feature importance:
Mean squared error is of 3769013634.903683
Mean absolute error:46529.61323216392
MAPE:0.39342083667204436
R2 score:0.43785867670897505
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:16
epoch 0  | loss: 2.50212 | val_0_rmse: 1.1199  | val_1_rmse: 1.05207 |  0:00:00s
epoch 1  | loss: 1.76384 | val_0_rmse: 1.03123 | val_1_rmse: 0.98952 |  0:00:00s
epoch 2  | loss: 1.40664 | val_0_rmse: 1.0225  | val_1_rmse: 0.99329 |  0:00:00s
epoch 3  | loss: 1.24623 | val_0_rmse: 1.02371 | val_1_rmse: 0.9917  |  0:00:00s
epoch 4  | loss: 1.21022 | val_0_rmse: 1.02224 | val_1_rmse: 0.99388 |  0:00:00s
epoch 5  | loss: 1.07371 | val_0_rmse: 1.02227 | val_1_rmse: 0.99675 |  0:00:01s
epoch 6  | loss: 1.06105 | val_0_rmse: 1.02282 | val_1_rmse: 0.99859 |  0:00:01s
epoch 7  | loss: 1.04077 | val_0_rmse: 1.02032 | val_1_rmse: 0.99347 |  0:00:01s
epoch 8  | loss: 1.02374 | val_0_rmse: 1.01815 | val_1_rmse: 0.99433 |  0:00:01s
epoch 9  | loss: 1.018   | val_0_rmse: 1.01787 | val_1_rmse: 0.99863 |  0:00:01s
epoch 10 | loss: 0.99668 | val_0_rmse: 1.01412 | val_1_rmse: 0.99932 |  0:00:01s
epoch 11 | loss: 0.96294 | val_0_rmse: 1.01826 | val_1_rmse: 1.01579 |  0:00:01s
epoch 12 | loss: 0.92793 | val_0_rmse: 1.00998 | val_1_rmse: 0.9976  |  0:00:02s
epoch 13 | loss: 0.85933 | val_0_rmse: 0.97387 | val_1_rmse: 0.97156 |  0:00:02s
epoch 14 | loss: 0.84638 | val_0_rmse: 0.95897 | val_1_rmse: 0.96558 |  0:00:02s
epoch 15 | loss: 0.77433 | val_0_rmse: 0.93141 | val_1_rmse: 0.93847 |  0:00:02s
epoch 16 | loss: 0.75198 | val_0_rmse: 0.89495 | val_1_rmse: 0.89719 |  0:00:02s
epoch 17 | loss: 0.69623 | val_0_rmse: 0.83634 | val_1_rmse: 0.85516 |  0:00:02s
epoch 18 | loss: 0.66863 | val_0_rmse: 0.80492 | val_1_rmse: 0.83946 |  0:00:03s
epoch 19 | loss: 0.60472 | val_0_rmse: 0.80405 | val_1_rmse: 0.84229 |  0:00:03s
epoch 20 | loss: 0.61415 | val_0_rmse: 0.78266 | val_1_rmse: 0.81924 |  0:00:03s
epoch 21 | loss: 0.57122 | val_0_rmse: 0.77462 | val_1_rmse: 0.81721 |  0:00:03s
epoch 22 | loss: 0.53226 | val_0_rmse: 0.81225 | val_1_rmse: 0.8653  |  0:00:03s
epoch 23 | loss: 0.49463 | val_0_rmse: 0.79176 | val_1_rmse: 0.8379  |  0:00:03s
epoch 24 | loss: 0.47016 | val_0_rmse: 0.77899 | val_1_rmse: 0.8167  |  0:00:04s
epoch 25 | loss: 0.44681 | val_0_rmse: 0.78726 | val_1_rmse: 0.82519 |  0:00:04s
epoch 26 | loss: 0.44891 | val_0_rmse: 0.76543 | val_1_rmse: 0.79812 |  0:00:04s
epoch 27 | loss: 0.4383  | val_0_rmse: 0.75583 | val_1_rmse: 0.78732 |  0:00:04s
epoch 28 | loss: 0.42247 | val_0_rmse: 0.80689 | val_1_rmse: 0.84296 |  0:00:04s
epoch 29 | loss: 0.42173 | val_0_rmse: 0.79395 | val_1_rmse: 0.83152 |  0:00:04s
epoch 30 | loss: 0.39417 | val_0_rmse: 0.73916 | val_1_rmse: 0.77042 |  0:00:05s
epoch 31 | loss: 0.37999 | val_0_rmse: 0.73862 | val_1_rmse: 0.77079 |  0:00:05s
epoch 32 | loss: 0.36898 | val_0_rmse: 0.74732 | val_1_rmse: 0.78382 |  0:00:05s
epoch 33 | loss: 0.39323 | val_0_rmse: 0.72622 | val_1_rmse: 0.76282 |  0:00:05s
epoch 34 | loss: 0.35865 | val_0_rmse: 0.71366 | val_1_rmse: 0.74825 |  0:00:05s
epoch 35 | loss: 0.36297 | val_0_rmse: 0.72566 | val_1_rmse: 0.76528 |  0:00:05s
epoch 36 | loss: 0.33844 | val_0_rmse: 0.74859 | val_1_rmse: 0.78945 |  0:00:05s
epoch 37 | loss: 0.35779 | val_0_rmse: 0.73302 | val_1_rmse: 0.76466 |  0:00:06s
epoch 38 | loss: 0.32845 | val_0_rmse: 0.71696 | val_1_rmse: 0.7435  |  0:00:06s
epoch 39 | loss: 0.31503 | val_0_rmse: 0.71783 | val_1_rmse: 0.74907 |  0:00:06s
epoch 40 | loss: 0.31619 | val_0_rmse: 0.73305 | val_1_rmse: 0.76785 |  0:00:06s
epoch 41 | loss: 0.33669 | val_0_rmse: 0.72234 | val_1_rmse: 0.75275 |  0:00:06s
epoch 42 | loss: 0.30147 | val_0_rmse: 0.71827 | val_1_rmse: 0.74276 |  0:00:06s
epoch 43 | loss: 0.3032  | val_0_rmse: 0.71305 | val_1_rmse: 0.73447 |  0:00:07s
epoch 44 | loss: 0.31279 | val_0_rmse: 0.70379 | val_1_rmse: 0.73252 |  0:00:07s
epoch 45 | loss: 0.29225 | val_0_rmse: 0.70849 | val_1_rmse: 0.75203 |  0:00:07s
epoch 46 | loss: 0.2934  | val_0_rmse: 0.70743 | val_1_rmse: 0.75536 |  0:00:07s
epoch 47 | loss: 0.2852  | val_0_rmse: 0.7022  | val_1_rmse: 0.75158 |  0:00:07s
epoch 48 | loss: 0.30052 | val_0_rmse: 0.70321 | val_1_rmse: 0.74853 |  0:00:07s
epoch 49 | loss: 0.28752 | val_0_rmse: 0.70861 | val_1_rmse: 0.74999 |  0:00:08s
epoch 50 | loss: 0.29561 | val_0_rmse: 0.70606 | val_1_rmse: 0.74884 |  0:00:08s
epoch 51 | loss: 0.30385 | val_0_rmse: 0.6969  | val_1_rmse: 0.74217 |  0:00:08s
epoch 52 | loss: 0.27149 | val_0_rmse: 0.69746 | val_1_rmse: 0.74596 |  0:00:08s
epoch 53 | loss: 0.28879 | val_0_rmse: 0.70817 | val_1_rmse: 0.75968 |  0:00:08s
epoch 54 | loss: 0.28447 | val_0_rmse: 0.70107 | val_1_rmse: 0.75596 |  0:00:08s
epoch 55 | loss: 0.26771 | val_0_rmse: 0.68104 | val_1_rmse: 0.73473 |  0:00:09s
epoch 56 | loss: 0.27095 | val_0_rmse: 0.67695 | val_1_rmse: 0.73125 |  0:00:09s
epoch 57 | loss: 0.27314 | val_0_rmse: 0.69051 | val_1_rmse: 0.74904 |  0:00:09s
epoch 58 | loss: 0.26742 | val_0_rmse: 0.68697 | val_1_rmse: 0.74757 |  0:00:09s
epoch 59 | loss: 0.25527 | val_0_rmse: 0.66841 | val_1_rmse: 0.72169 |  0:00:09s
epoch 60 | loss: 0.26252 | val_0_rmse: 0.67185 | val_1_rmse: 0.72531 |  0:00:09s
epoch 61 | loss: 0.26062 | val_0_rmse: 0.70262 | val_1_rmse: 0.76056 |  0:00:10s
epoch 62 | loss: 0.26265 | val_0_rmse: 0.69818 | val_1_rmse: 0.75034 |  0:00:10s
epoch 63 | loss: 0.27009 | val_0_rmse: 0.6794  | val_1_rmse: 0.72122 |  0:00:10s
epoch 64 | loss: 0.26588 | val_0_rmse: 0.67839 | val_1_rmse: 0.72364 |  0:00:10s
epoch 65 | loss: 0.25439 | val_0_rmse: 0.68489 | val_1_rmse: 0.7387  |  0:00:10s
epoch 66 | loss: 0.2425  | val_0_rmse: 0.68629 | val_1_rmse: 0.73906 |  0:00:10s
epoch 67 | loss: 0.24568 | val_0_rmse: 0.67784 | val_1_rmse: 0.72404 |  0:00:10s
epoch 68 | loss: 0.24497 | val_0_rmse: 0.67514 | val_1_rmse: 0.71818 |  0:00:11s
epoch 69 | loss: 0.24746 | val_0_rmse: 0.67106 | val_1_rmse: 0.71702 |  0:00:11s
epoch 70 | loss: 0.239   | val_0_rmse: 0.6684  | val_1_rmse: 0.71645 |  0:00:11s
epoch 71 | loss: 0.25246 | val_0_rmse: 0.66956 | val_1_rmse: 0.71425 |  0:00:11s
epoch 72 | loss: 0.231   | val_0_rmse: 0.66894 | val_1_rmse: 0.71023 |  0:00:11s
epoch 73 | loss: 0.22826 | val_0_rmse: 0.66666 | val_1_rmse: 0.70865 |  0:00:11s
epoch 74 | loss: 0.22817 | val_0_rmse: 0.66445 | val_1_rmse: 0.70413 |  0:00:12s
epoch 75 | loss: 0.22809 | val_0_rmse: 0.66142 | val_1_rmse: 0.70539 |  0:00:12s
epoch 76 | loss: 0.22672 | val_0_rmse: 0.65684 | val_1_rmse: 0.70436 |  0:00:12s
epoch 77 | loss: 0.22785 | val_0_rmse: 0.65461 | val_1_rmse: 0.70634 |  0:00:12s
epoch 78 | loss: 0.23615 | val_0_rmse: 0.65973 | val_1_rmse: 0.71764 |  0:00:12s
epoch 79 | loss: 0.222   | val_0_rmse: 0.6592  | val_1_rmse: 0.7176  |  0:00:12s
epoch 80 | loss: 0.21683 | val_0_rmse: 0.65496 | val_1_rmse: 0.7146  |  0:00:13s
epoch 81 | loss: 0.2148  | val_0_rmse: 0.65055 | val_1_rmse: 0.7103  |  0:00:13s
epoch 82 | loss: 0.23209 | val_0_rmse: 0.64679 | val_1_rmse: 0.70447 |  0:00:13s
epoch 83 | loss: 0.21374 | val_0_rmse: 0.64909 | val_1_rmse: 0.70279 |  0:00:13s
epoch 84 | loss: 0.21722 | val_0_rmse: 0.6481  | val_1_rmse: 0.70504 |  0:00:13s
epoch 85 | loss: 0.21709 | val_0_rmse: 0.64647 | val_1_rmse: 0.7146  |  0:00:13s
epoch 86 | loss: 0.20451 | val_0_rmse: 0.65347 | val_1_rmse: 0.72725 |  0:00:14s
epoch 87 | loss: 0.2074  | val_0_rmse: 0.64955 | val_1_rmse: 0.72196 |  0:00:14s
epoch 88 | loss: 0.20657 | val_0_rmse: 0.64903 | val_1_rmse: 0.72325 |  0:00:14s
epoch 89 | loss: 0.19536 | val_0_rmse: 0.65026 | val_1_rmse: 0.72965 |  0:00:14s
epoch 90 | loss: 0.21458 | val_0_rmse: 0.6451  | val_1_rmse: 0.72125 |  0:00:14s
epoch 91 | loss: 0.19585 | val_0_rmse: 0.64614 | val_1_rmse: 0.71614 |  0:00:14s
epoch 92 | loss: 0.21488 | val_0_rmse: 0.65181 | val_1_rmse: 0.72348 |  0:00:14s
epoch 93 | loss: 0.21347 | val_0_rmse: 0.65917 | val_1_rmse: 0.73306 |  0:00:15s
epoch 94 | loss: 0.20261 | val_0_rmse: 0.6457  | val_1_rmse: 0.71508 |  0:00:15s
epoch 95 | loss: 0.20981 | val_0_rmse: 0.64221 | val_1_rmse: 0.71323 |  0:00:15s
epoch 96 | loss: 0.2195  | val_0_rmse: 0.6348  | val_1_rmse: 0.72279 |  0:00:15s
epoch 97 | loss: 0.21466 | val_0_rmse: 0.63435 | val_1_rmse: 0.72136 |  0:00:15s
epoch 98 | loss: 0.20976 | val_0_rmse: 0.64434 | val_1_rmse: 0.71974 |  0:00:15s
epoch 99 | loss: 0.20139 | val_0_rmse: 0.64686 | val_1_rmse: 0.72272 |  0:00:16s
epoch 100| loss: 0.21475 | val_0_rmse: 0.63789 | val_1_rmse: 0.72696 |  0:00:16s
epoch 101| loss: 0.2023  | val_0_rmse: 0.63502 | val_1_rmse: 0.72885 |  0:00:16s
epoch 102| loss: 0.18693 | val_0_rmse: 0.64659 | val_1_rmse: 0.73913 |  0:00:16s
epoch 103| loss: 0.19847 | val_0_rmse: 0.63844 | val_1_rmse: 0.72643 |  0:00:16s
epoch 104| loss: 0.20682 | val_0_rmse: 0.6209  | val_1_rmse: 0.70186 |  0:00:16s
epoch 105| loss: 0.19447 | val_0_rmse: 0.62003 | val_1_rmse: 0.69586 |  0:00:16s
epoch 106| loss: 0.20631 | val_0_rmse: 0.61179 | val_1_rmse: 0.69792 |  0:00:17s
epoch 107| loss: 0.18687 | val_0_rmse: 0.61815 | val_1_rmse: 0.70931 |  0:00:17s
epoch 108| loss: 0.19622 | val_0_rmse: 0.6319  | val_1_rmse: 0.71536 |  0:00:17s
epoch 109| loss: 0.19789 | val_0_rmse: 0.62635 | val_1_rmse: 0.7062  |  0:00:17s
epoch 110| loss: 0.19635 | val_0_rmse: 0.61806 | val_1_rmse: 0.69899 |  0:00:17s
epoch 111| loss: 0.17931 | val_0_rmse: 0.6275  | val_1_rmse: 0.71105 |  0:00:17s
epoch 112| loss: 0.18531 | val_0_rmse: 0.63321 | val_1_rmse: 0.71882 |  0:00:18s
epoch 113| loss: 0.19189 | val_0_rmse: 0.61574 | val_1_rmse: 0.71135 |  0:00:18s
epoch 114| loss: 0.19199 | val_0_rmse: 0.60216 | val_1_rmse: 0.70766 |  0:00:18s
epoch 115| loss: 0.17948 | val_0_rmse: 0.59307 | val_1_rmse: 0.70276 |  0:00:18s
epoch 116| loss: 0.17405 | val_0_rmse: 0.59355 | val_1_rmse: 0.71105 |  0:00:18s
epoch 117| loss: 0.18046 | val_0_rmse: 0.59635 | val_1_rmse: 0.71535 |  0:00:18s
epoch 118| loss: 0.18204 | val_0_rmse: 0.60215 | val_1_rmse: 0.71147 |  0:00:19s
epoch 119| loss: 0.17461 | val_0_rmse: 0.60677 | val_1_rmse: 0.70429 |  0:00:19s
epoch 120| loss: 0.18517 | val_0_rmse: 0.60577 | val_1_rmse: 0.70585 |  0:00:19s
epoch 121| loss: 0.1874  | val_0_rmse: 0.60076 | val_1_rmse: 0.69796 |  0:00:19s
epoch 122| loss: 0.17644 | val_0_rmse: 0.59654 | val_1_rmse: 0.68865 |  0:00:19s
epoch 123| loss: 0.17553 | val_0_rmse: 0.59408 | val_1_rmse: 0.69043 |  0:00:19s
epoch 124| loss: 0.17075 | val_0_rmse: 0.61515 | val_1_rmse: 0.71393 |  0:00:20s
epoch 125| loss: 0.17312 | val_0_rmse: 0.6109  | val_1_rmse: 0.71956 |  0:00:20s
epoch 126| loss: 0.17958 | val_0_rmse: 0.59131 | val_1_rmse: 0.70822 |  0:00:20s
epoch 127| loss: 0.16035 | val_0_rmse: 0.58721 | val_1_rmse: 0.70967 |  0:00:20s
epoch 128| loss: 0.1767  | val_0_rmse: 0.58234 | val_1_rmse: 0.71748 |  0:00:20s
epoch 129| loss: 0.18881 | val_0_rmse: 0.59845 | val_1_rmse: 0.72818 |  0:00:20s
epoch 130| loss: 0.17584 | val_0_rmse: 0.59401 | val_1_rmse: 0.71511 |  0:00:21s
epoch 131| loss: 0.17599 | val_0_rmse: 0.58016 | val_1_rmse: 0.70264 |  0:00:21s
epoch 132| loss: 0.18291 | val_0_rmse: 0.58294 | val_1_rmse: 0.70571 |  0:00:21s
epoch 133| loss: 0.17328 | val_0_rmse: 0.60391 | val_1_rmse: 0.72562 |  0:00:21s
epoch 134| loss: 0.17561 | val_0_rmse: 0.60294 | val_1_rmse: 0.72375 |  0:00:21s
epoch 135| loss: 0.17564 | val_0_rmse: 0.59059 | val_1_rmse: 0.70656 |  0:00:21s
epoch 136| loss: 0.17414 | val_0_rmse: 0.58818 | val_1_rmse: 0.70444 |  0:00:21s
epoch 137| loss: 0.17695 | val_0_rmse: 0.59608 | val_1_rmse: 0.72031 |  0:00:22s
epoch 138| loss: 0.17232 | val_0_rmse: 0.5856  | val_1_rmse: 0.71508 |  0:00:22s
epoch 139| loss: 0.16785 | val_0_rmse: 0.56462 | val_1_rmse: 0.70332 |  0:00:22s
epoch 140| loss: 0.16633 | val_0_rmse: 0.56713 | val_1_rmse: 0.71644 |  0:00:22s
epoch 141| loss: 0.1717  | val_0_rmse: 0.582   | val_1_rmse: 0.73326 |  0:00:22s
epoch 142| loss: 0.16771 | val_0_rmse: 0.58443 | val_1_rmse: 0.73513 |  0:00:22s
epoch 143| loss: 0.16585 | val_0_rmse: 0.58054 | val_1_rmse: 0.72737 |  0:00:23s
epoch 144| loss: 0.16999 | val_0_rmse: 0.57971 | val_1_rmse: 0.71483 |  0:00:23s
epoch 145| loss: 0.16415 | val_0_rmse: 0.57536 | val_1_rmse: 0.71217 |  0:00:23s
epoch 146| loss: 0.15274 | val_0_rmse: 0.56442 | val_1_rmse: 0.71694 |  0:00:23s
epoch 147| loss: 0.14862 | val_0_rmse: 0.56334 | val_1_rmse: 0.72995 |  0:00:23s
epoch 148| loss: 0.17429 | val_0_rmse: 0.55533 | val_1_rmse: 0.72007 |  0:00:23s
epoch 149| loss: 0.16183 | val_0_rmse: 0.54478 | val_1_rmse: 0.71059 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.68865
Best weights from best epoch are automatically used!
ended training at: 08:03:40
Feature importance:
Mean squared error is of 3537642158.6102157
Mean absolute error:42906.22851341392
MAPE:0.36668645100029135
R2 score:0.4121898587278443
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:40
epoch 0  | loss: 2.76179 | val_0_rmse: 1.02367 | val_1_rmse: 1.01865 |  0:00:00s
epoch 1  | loss: 1.64656 | val_0_rmse: 1.00675 | val_1_rmse: 1.01049 |  0:00:00s
epoch 2  | loss: 1.70996 | val_0_rmse: 1.22114 | val_1_rmse: 1.00764 |  0:00:00s
epoch 3  | loss: 1.22327 | val_0_rmse: 1.05462 | val_1_rmse: 1.00092 |  0:00:00s
epoch 4  | loss: 1.18352 | val_0_rmse: 1.03817 | val_1_rmse: 1.00272 |  0:00:00s
epoch 5  | loss: 1.08521 | val_0_rmse: 1.00686 | val_1_rmse: 1.00439 |  0:00:00s
epoch 6  | loss: 1.04247 | val_0_rmse: 1.02288 | val_1_rmse: 1.00524 |  0:00:01s
epoch 7  | loss: 1.0254  | val_0_rmse: 1.00565 | val_1_rmse: 1.00585 |  0:00:01s
epoch 8  | loss: 1.00652 | val_0_rmse: 1.01101 | val_1_rmse: 1.0041  |  0:00:01s
epoch 9  | loss: 0.98843 | val_0_rmse: 1.02913 | val_1_rmse: 1.00214 |  0:00:01s
epoch 10 | loss: 0.94724 | val_0_rmse: 1.00315 | val_1_rmse: 0.99985 |  0:00:01s
epoch 11 | loss: 0.92685 | val_0_rmse: 0.99344 | val_1_rmse: 0.99603 |  0:00:01s
epoch 12 | loss: 0.91872 | val_0_rmse: 0.9739  | val_1_rmse: 0.97239 |  0:00:02s
epoch 13 | loss: 0.8801  | val_0_rmse: 0.95703 | val_1_rmse: 0.95892 |  0:00:02s
epoch 14 | loss: 0.84813 | val_0_rmse: 0.95235 | val_1_rmse: 0.95    |  0:00:02s
epoch 15 | loss: 0.81346 | val_0_rmse: 1.00703 | val_1_rmse: 0.93887 |  0:00:02s
epoch 16 | loss: 0.81295 | val_0_rmse: 1.0875  | val_1_rmse: 0.928   |  0:00:02s
epoch 17 | loss: 0.77886 | val_0_rmse: 1.00555 | val_1_rmse: 0.92268 |  0:00:02s
epoch 18 | loss: 0.77024 | val_0_rmse: 0.96349 | val_1_rmse: 0.91961 |  0:00:03s
epoch 19 | loss: 0.7422  | val_0_rmse: 0.91707 | val_1_rmse: 0.9258  |  0:00:03s
epoch 20 | loss: 0.73781 | val_0_rmse: 0.90357 | val_1_rmse: 0.91624 |  0:00:03s
epoch 21 | loss: 0.69249 | val_0_rmse: 0.91956 | val_1_rmse: 0.93355 |  0:00:03s
epoch 22 | loss: 0.69157 | val_0_rmse: 0.88585 | val_1_rmse: 0.89632 |  0:00:03s
epoch 23 | loss: 0.67727 | val_0_rmse: 0.86459 | val_1_rmse: 0.86646 |  0:00:03s
epoch 24 | loss: 0.66385 | val_0_rmse: 0.85974 | val_1_rmse: 0.8659  |  0:00:04s
epoch 25 | loss: 0.63068 | val_0_rmse: 0.80737 | val_1_rmse: 0.81457 |  0:00:04s
epoch 26 | loss: 0.59756 | val_0_rmse: 0.76488 | val_1_rmse: 0.76404 |  0:00:04s
epoch 27 | loss: 0.55883 | val_0_rmse: 0.75136 | val_1_rmse: 0.75212 |  0:00:04s
epoch 28 | loss: 0.53202 | val_0_rmse: 0.74713 | val_1_rmse: 0.74637 |  0:00:04s
epoch 29 | loss: 0.52418 | val_0_rmse: 0.74648 | val_1_rmse: 0.74985 |  0:00:04s
epoch 30 | loss: 0.47439 | val_0_rmse: 0.76308 | val_1_rmse: 0.772   |  0:00:05s
epoch 31 | loss: 0.4808  | val_0_rmse: 0.74806 | val_1_rmse: 0.75536 |  0:00:05s
epoch 32 | loss: 0.4587  | val_0_rmse: 0.74297 | val_1_rmse: 0.75242 |  0:00:05s
epoch 33 | loss: 0.44448 | val_0_rmse: 0.72221 | val_1_rmse: 0.72656 |  0:00:05s
epoch 34 | loss: 0.45315 | val_0_rmse: 0.73297 | val_1_rmse: 0.74882 |  0:00:05s
epoch 35 | loss: 0.42961 | val_0_rmse: 0.80002 | val_1_rmse: 0.82353 |  0:00:05s
epoch 36 | loss: 0.4294  | val_0_rmse: 0.75072 | val_1_rmse: 0.76743 |  0:00:06s
epoch 37 | loss: 0.41285 | val_0_rmse: 0.74317 | val_1_rmse: 0.75757 |  0:00:06s
epoch 38 | loss: 0.40496 | val_0_rmse: 0.73556 | val_1_rmse: 0.7529  |  0:00:06s
epoch 39 | loss: 0.38252 | val_0_rmse: 0.70758 | val_1_rmse: 0.71827 |  0:00:06s
epoch 40 | loss: 0.38228 | val_0_rmse: 0.70485 | val_1_rmse: 0.71319 |  0:00:06s
epoch 41 | loss: 0.37574 | val_0_rmse: 0.70023 | val_1_rmse: 0.7062  |  0:00:06s
epoch 42 | loss: 0.389   | val_0_rmse: 0.70341 | val_1_rmse: 0.70592 |  0:00:07s
epoch 43 | loss: 0.37263 | val_0_rmse: 0.70524 | val_1_rmse: 0.71418 |  0:00:07s
epoch 44 | loss: 0.36023 | val_0_rmse: 0.71356 | val_1_rmse: 0.72904 |  0:00:07s
epoch 45 | loss: 0.36914 | val_0_rmse: 0.71923 | val_1_rmse: 0.73803 |  0:00:07s
epoch 46 | loss: 0.34084 | val_0_rmse: 0.71913 | val_1_rmse: 0.74345 |  0:00:07s
epoch 47 | loss: 0.33954 | val_0_rmse: 0.72186 | val_1_rmse: 0.75082 |  0:00:07s
epoch 48 | loss: 0.34055 | val_0_rmse: 0.71791 | val_1_rmse: 0.74944 |  0:00:07s
epoch 49 | loss: 0.33307 | val_0_rmse: 0.70887 | val_1_rmse: 0.74201 |  0:00:08s
epoch 50 | loss: 0.32943 | val_0_rmse: 0.70879 | val_1_rmse: 0.7487  |  0:00:08s
epoch 51 | loss: 0.32304 | val_0_rmse: 0.70021 | val_1_rmse: 0.73947 |  0:00:08s
epoch 52 | loss: 0.32591 | val_0_rmse: 0.6982  | val_1_rmse: 0.72839 |  0:00:08s
epoch 53 | loss: 0.33858 | val_0_rmse: 0.71191 | val_1_rmse: 0.7375  |  0:00:08s
epoch 54 | loss: 0.32631 | val_0_rmse: 0.71159 | val_1_rmse: 0.73741 |  0:00:08s
epoch 55 | loss: 0.3111  | val_0_rmse: 0.69807 | val_1_rmse: 0.72321 |  0:00:09s
epoch 56 | loss: 0.29912 | val_0_rmse: 0.69015 | val_1_rmse: 0.71856 |  0:00:09s
epoch 57 | loss: 0.30995 | val_0_rmse: 0.69559 | val_1_rmse: 0.72507 |  0:00:09s
epoch 58 | loss: 0.29195 | val_0_rmse: 0.70947 | val_1_rmse: 0.7312  |  0:00:09s
epoch 59 | loss: 0.29264 | val_0_rmse: 0.71084 | val_1_rmse: 0.73036 |  0:00:09s
epoch 60 | loss: 0.30444 | val_0_rmse: 0.71072 | val_1_rmse: 0.73145 |  0:00:09s
epoch 61 | loss: 0.28845 | val_0_rmse: 0.71084 | val_1_rmse: 0.73617 |  0:00:10s
epoch 62 | loss: 0.29846 | val_0_rmse: 0.69255 | val_1_rmse: 0.72508 |  0:00:10s
epoch 63 | loss: 0.29066 | val_0_rmse: 0.68325 | val_1_rmse: 0.71995 |  0:00:10s
epoch 64 | loss: 0.28894 | val_0_rmse: 0.68501 | val_1_rmse: 0.72036 |  0:00:10s
epoch 65 | loss: 0.29324 | val_0_rmse: 0.68495 | val_1_rmse: 0.71303 |  0:00:10s
epoch 66 | loss: 0.29052 | val_0_rmse: 0.69271 | val_1_rmse: 0.71434 |  0:00:10s
epoch 67 | loss: 0.28003 | val_0_rmse: 0.69781 | val_1_rmse: 0.72316 |  0:00:10s
epoch 68 | loss: 0.29192 | val_0_rmse: 0.69771 | val_1_rmse: 0.72351 |  0:00:11s
epoch 69 | loss: 0.28647 | val_0_rmse: 0.6947  | val_1_rmse: 0.72156 |  0:00:11s
epoch 70 | loss: 0.28157 | val_0_rmse: 0.68885 | val_1_rmse: 0.71791 |  0:00:11s
epoch 71 | loss: 0.27389 | val_0_rmse: 0.68198 | val_1_rmse: 0.71101 |  0:00:11s
epoch 72 | loss: 0.28322 | val_0_rmse: 0.68557 | val_1_rmse: 0.72376 |  0:00:11s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.70592
Best weights from best epoch are automatically used!
ended training at: 08:03:52
Feature importance:
Mean squared error is of 3334776022.055343
Mean absolute error:41915.693904186315
MAPE:0.3538606575596718
R2 score:0.4911095514310052
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:52
epoch 0  | loss: 2.41745 | val_0_rmse: 1.03488 | val_1_rmse: 1.06337 |  0:00:00s
epoch 1  | loss: 1.54274 | val_0_rmse: 1.00057 | val_1_rmse: 1.02144 |  0:00:00s
epoch 2  | loss: 1.54998 | val_0_rmse: 0.998   | val_1_rmse: 1.01218 |  0:00:00s
epoch 3  | loss: 1.28626 | val_0_rmse: 0.99671 | val_1_rmse: 1.01139 |  0:00:00s
epoch 4  | loss: 1.20579 | val_0_rmse: 0.99197 | val_1_rmse: 1.00919 |  0:00:00s
epoch 5  | loss: 1.05566 | val_0_rmse: 0.99435 | val_1_rmse: 1.01182 |  0:00:00s
epoch 6  | loss: 1.03637 | val_0_rmse: 0.98934 | val_1_rmse: 1.00722 |  0:00:01s
epoch 7  | loss: 1.00241 | val_0_rmse: 0.99127 | val_1_rmse: 1.00856 |  0:00:01s
epoch 8  | loss: 0.96549 | val_0_rmse: 0.99019 | val_1_rmse: 1.00587 |  0:00:01s
epoch 9  | loss: 0.94223 | val_0_rmse: 0.98519 | val_1_rmse: 0.99923 |  0:00:01s
epoch 10 | loss: 0.93434 | val_0_rmse: 0.9683  | val_1_rmse: 0.98509 |  0:00:01s
epoch 11 | loss: 0.88253 | val_0_rmse: 0.94356 | val_1_rmse: 0.96252 |  0:00:02s
epoch 12 | loss: 0.85634 | val_0_rmse: 0.92608 | val_1_rmse: 0.94733 |  0:00:02s
epoch 13 | loss: 0.81644 | val_0_rmse: 0.90073 | val_1_rmse: 0.93428 |  0:00:02s
epoch 14 | loss: 0.77311 | val_0_rmse: 0.89019 | val_1_rmse: 0.92415 |  0:00:02s
epoch 15 | loss: 0.74763 | val_0_rmse: 0.86578 | val_1_rmse: 0.90379 |  0:00:02s
epoch 16 | loss: 0.70041 | val_0_rmse: 0.84509 | val_1_rmse: 0.87697 |  0:00:02s
epoch 17 | loss: 0.71556 | val_0_rmse: 0.82377 | val_1_rmse: 0.85685 |  0:00:03s
epoch 18 | loss: 0.63745 | val_0_rmse: 0.79822 | val_1_rmse: 0.84268 |  0:00:03s
epoch 19 | loss: 0.61595 | val_0_rmse: 0.78054 | val_1_rmse: 0.82198 |  0:00:03s
epoch 20 | loss: 0.60654 | val_0_rmse: 0.78307 | val_1_rmse: 0.81591 |  0:00:03s
epoch 21 | loss: 0.62275 | val_0_rmse: 0.78903 | val_1_rmse: 0.81754 |  0:00:03s
epoch 22 | loss: 0.57178 | val_0_rmse: 0.76564 | val_1_rmse: 0.79419 |  0:00:03s
epoch 23 | loss: 0.56543 | val_0_rmse: 0.75129 | val_1_rmse: 0.78313 |  0:00:03s
epoch 24 | loss: 0.52371 | val_0_rmse: 0.73638 | val_1_rmse: 0.77604 |  0:00:04s
epoch 25 | loss: 0.52533 | val_0_rmse: 0.726   | val_1_rmse: 0.77877 |  0:00:04s
epoch 26 | loss: 0.48191 | val_0_rmse: 0.73074 | val_1_rmse: 0.78115 |  0:00:04s
epoch 27 | loss: 0.47563 | val_0_rmse: 0.72492 | val_1_rmse: 0.77491 |  0:00:04s
epoch 28 | loss: 0.45228 | val_0_rmse: 0.73599 | val_1_rmse: 0.79267 |  0:00:04s
epoch 29 | loss: 0.44641 | val_0_rmse: 0.74166 | val_1_rmse: 0.7907  |  0:00:04s
epoch 30 | loss: 0.42264 | val_0_rmse: 0.73659 | val_1_rmse: 0.78165 |  0:00:05s
epoch 31 | loss: 0.43156 | val_0_rmse: 0.73216 | val_1_rmse: 0.78576 |  0:00:05s
epoch 32 | loss: 0.42943 | val_0_rmse: 0.72732 | val_1_rmse: 0.77684 |  0:00:05s
epoch 33 | loss: 0.41566 | val_0_rmse: 0.72123 | val_1_rmse: 0.76375 |  0:00:05s
epoch 34 | loss: 0.39668 | val_0_rmse: 0.72088 | val_1_rmse: 0.75835 |  0:00:05s
epoch 35 | loss: 0.3917  | val_0_rmse: 0.71456 | val_1_rmse: 0.75297 |  0:00:05s
epoch 36 | loss: 0.37939 | val_0_rmse: 0.71798 | val_1_rmse: 0.75879 |  0:00:06s
epoch 37 | loss: 0.35976 | val_0_rmse: 0.70994 | val_1_rmse: 0.74968 |  0:00:06s
epoch 38 | loss: 0.36929 | val_0_rmse: 0.70528 | val_1_rmse: 0.74611 |  0:00:06s
epoch 39 | loss: 0.35483 | val_0_rmse: 0.70675 | val_1_rmse: 0.74765 |  0:00:06s
epoch 40 | loss: 0.35366 | val_0_rmse: 0.70533 | val_1_rmse: 0.76033 |  0:00:06s
epoch 41 | loss: 0.3389  | val_0_rmse: 0.71624 | val_1_rmse: 0.76909 |  0:00:06s
epoch 42 | loss: 0.32764 | val_0_rmse: 0.71592 | val_1_rmse: 0.77615 |  0:00:07s
epoch 43 | loss: 0.3457  | val_0_rmse: 0.71432 | val_1_rmse: 0.77437 |  0:00:07s
epoch 44 | loss: 0.33281 | val_0_rmse: 0.71857 | val_1_rmse: 0.77703 |  0:00:07s
epoch 45 | loss: 0.3324  | val_0_rmse: 0.70078 | val_1_rmse: 0.76495 |  0:00:07s
epoch 46 | loss: 0.32613 | val_0_rmse: 0.71137 | val_1_rmse: 0.77735 |  0:00:07s
epoch 47 | loss: 0.34246 | val_0_rmse: 0.68861 | val_1_rmse: 0.74247 |  0:00:07s
epoch 48 | loss: 0.31218 | val_0_rmse: 0.69089 | val_1_rmse: 0.74034 |  0:00:07s
epoch 49 | loss: 0.33656 | val_0_rmse: 0.6895  | val_1_rmse: 0.73767 |  0:00:08s
epoch 50 | loss: 0.31954 | val_0_rmse: 0.68817 | val_1_rmse: 0.74028 |  0:00:08s
epoch 51 | loss: 0.31605 | val_0_rmse: 0.70459 | val_1_rmse: 0.75792 |  0:00:08s
epoch 52 | loss: 0.33626 | val_0_rmse: 0.71775 | val_1_rmse: 0.77307 |  0:00:08s
epoch 53 | loss: 0.32285 | val_0_rmse: 0.70403 | val_1_rmse: 0.75635 |  0:00:08s
epoch 54 | loss: 0.31786 | val_0_rmse: 0.69826 | val_1_rmse: 0.74909 |  0:00:08s
epoch 55 | loss: 0.30894 | val_0_rmse: 0.70025 | val_1_rmse: 0.75108 |  0:00:09s
epoch 56 | loss: 0.29961 | val_0_rmse: 0.70176 | val_1_rmse: 0.76222 |  0:00:09s
epoch 57 | loss: 0.29544 | val_0_rmse: 0.68701 | val_1_rmse: 0.74136 |  0:00:09s
epoch 58 | loss: 0.30088 | val_0_rmse: 0.68641 | val_1_rmse: 0.73867 |  0:00:09s
epoch 59 | loss: 0.2849  | val_0_rmse: 0.69889 | val_1_rmse: 0.74547 |  0:00:09s
epoch 60 | loss: 0.29506 | val_0_rmse: 0.70407 | val_1_rmse: 0.74842 |  0:00:09s
epoch 61 | loss: 0.28527 | val_0_rmse: 0.70691 | val_1_rmse: 0.74777 |  0:00:10s
epoch 62 | loss: 0.28654 | val_0_rmse: 0.69684 | val_1_rmse: 0.7395  |  0:00:10s
epoch 63 | loss: 0.28479 | val_0_rmse: 0.68928 | val_1_rmse: 0.73277 |  0:00:10s
epoch 64 | loss: 0.27298 | val_0_rmse: 0.68534 | val_1_rmse: 0.73034 |  0:00:10s
epoch 65 | loss: 0.28138 | val_0_rmse: 0.67896 | val_1_rmse: 0.72394 |  0:00:10s
epoch 66 | loss: 0.28388 | val_0_rmse: 0.67881 | val_1_rmse: 0.72556 |  0:00:10s
epoch 67 | loss: 0.27372 | val_0_rmse: 0.67853 | val_1_rmse: 0.73087 |  0:00:10s
epoch 68 | loss: 0.26865 | val_0_rmse: 0.68267 | val_1_rmse: 0.73935 |  0:00:11s
epoch 69 | loss: 0.27441 | val_0_rmse: 0.68033 | val_1_rmse: 0.73367 |  0:00:11s
epoch 70 | loss: 0.25628 | val_0_rmse: 0.67349 | val_1_rmse: 0.72523 |  0:00:11s
epoch 71 | loss: 0.2665  | val_0_rmse: 0.66217 | val_1_rmse: 0.71767 |  0:00:11s
epoch 72 | loss: 0.26824 | val_0_rmse: 0.65304 | val_1_rmse: 0.7122  |  0:00:11s
epoch 73 | loss: 0.26408 | val_0_rmse: 0.65468 | val_1_rmse: 0.71433 |  0:00:11s
epoch 74 | loss: 0.26148 | val_0_rmse: 0.65605 | val_1_rmse: 0.71335 |  0:00:12s
epoch 75 | loss: 0.25975 | val_0_rmse: 0.65801 | val_1_rmse: 0.71386 |  0:00:12s
epoch 76 | loss: 0.24312 | val_0_rmse: 0.66007 | val_1_rmse: 0.71593 |  0:00:12s
epoch 77 | loss: 0.24781 | val_0_rmse: 0.66329 | val_1_rmse: 0.72636 |  0:00:12s
epoch 78 | loss: 0.252   | val_0_rmse: 0.66807 | val_1_rmse: 0.73795 |  0:00:12s
epoch 79 | loss: 0.25757 | val_0_rmse: 0.66763 | val_1_rmse: 0.7312  |  0:00:12s
epoch 80 | loss: 0.25912 | val_0_rmse: 0.66745 | val_1_rmse: 0.73111 |  0:00:13s
epoch 81 | loss: 0.24452 | val_0_rmse: 0.66793 | val_1_rmse: 0.73798 |  0:00:13s
epoch 82 | loss: 0.24297 | val_0_rmse: 0.66632 | val_1_rmse: 0.73836 |  0:00:13s
epoch 83 | loss: 0.2408  | val_0_rmse: 0.6573  | val_1_rmse: 0.72673 |  0:00:13s
epoch 84 | loss: 0.23666 | val_0_rmse: 0.65762 | val_1_rmse: 0.72938 |  0:00:13s
epoch 85 | loss: 0.25169 | val_0_rmse: 0.66202 | val_1_rmse: 0.74317 |  0:00:13s
epoch 86 | loss: 0.23577 | val_0_rmse: 0.66151 | val_1_rmse: 0.75046 |  0:00:13s
epoch 87 | loss: 0.23346 | val_0_rmse: 0.65398 | val_1_rmse: 0.7415  |  0:00:14s
epoch 88 | loss: 0.23845 | val_0_rmse: 0.65149 | val_1_rmse: 0.73286 |  0:00:14s
epoch 89 | loss: 0.24676 | val_0_rmse: 0.64551 | val_1_rmse: 0.73177 |  0:00:14s
epoch 90 | loss: 0.24971 | val_0_rmse: 0.64574 | val_1_rmse: 0.73721 |  0:00:14s
epoch 91 | loss: 0.24853 | val_0_rmse: 0.64031 | val_1_rmse: 0.72923 |  0:00:14s
epoch 92 | loss: 0.23787 | val_0_rmse: 0.63681 | val_1_rmse: 0.72481 |  0:00:14s
epoch 93 | loss: 0.23424 | val_0_rmse: 0.63818 | val_1_rmse: 0.72557 |  0:00:15s
epoch 94 | loss: 0.23744 | val_0_rmse: 0.64543 | val_1_rmse: 0.72978 |  0:00:15s
epoch 95 | loss: 0.23518 | val_0_rmse: 0.64747 | val_1_rmse: 0.73359 |  0:00:15s
epoch 96 | loss: 0.22907 | val_0_rmse: 0.64367 | val_1_rmse: 0.73548 |  0:00:15s
epoch 97 | loss: 0.24104 | val_0_rmse: 0.64583 | val_1_rmse: 0.73456 |  0:00:15s
epoch 98 | loss: 0.22874 | val_0_rmse: 0.64634 | val_1_rmse: 0.73483 |  0:00:15s
epoch 99 | loss: 0.22212 | val_0_rmse: 0.64844 | val_1_rmse: 0.74207 |  0:00:16s
epoch 100| loss: 0.22454 | val_0_rmse: 0.65314 | val_1_rmse: 0.75426 |  0:00:16s
epoch 101| loss: 0.22325 | val_0_rmse: 0.65603 | val_1_rmse: 0.76395 |  0:00:16s
epoch 102| loss: 0.22126 | val_0_rmse: 0.65261 | val_1_rmse: 0.76223 |  0:00:16s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.7122
Best weights from best epoch are automatically used!
ended training at: 08:04:09
Feature importance:
Mean squared error is of 3814090778.1408834
Mean absolute error:46624.88543263561
MAPE:0.35356774138921565
R2 score:0.4450630155226153
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:04:09
epoch 0  | loss: 2.1577  | val_0_rmse: 1.00902 | val_1_rmse: 1.01449 |  0:00:00s
epoch 1  | loss: 1.14541 | val_0_rmse: 1.00455 | val_1_rmse: 1.00798 |  0:00:00s
epoch 2  | loss: 0.99583 | val_0_rmse: 0.99925 | val_1_rmse: 1.00441 |  0:00:01s
epoch 3  | loss: 0.83551 | val_0_rmse: 0.94385 | val_1_rmse: 0.95325 |  0:00:01s
epoch 4  | loss: 0.77309 | val_0_rmse: 0.87268 | val_1_rmse: 0.88369 |  0:00:01s
epoch 5  | loss: 0.65107 | val_0_rmse: 0.78772 | val_1_rmse: 0.79761 |  0:00:02s
epoch 6  | loss: 0.57756 | val_0_rmse: 0.75612 | val_1_rmse: 0.74767 |  0:00:02s
epoch 7  | loss: 0.54193 | val_0_rmse: 0.80066 | val_1_rmse: 0.77956 |  0:00:02s
epoch 8  | loss: 0.51468 | val_0_rmse: 0.84104 | val_1_rmse: 0.81972 |  0:00:03s
epoch 9  | loss: 0.49801 | val_0_rmse: 0.82639 | val_1_rmse: 0.80394 |  0:00:03s
epoch 10 | loss: 0.49793 | val_0_rmse: 0.78605 | val_1_rmse: 0.7623  |  0:00:03s
epoch 11 | loss: 0.4797  | val_0_rmse: 0.78618 | val_1_rmse: 0.75907 |  0:00:04s
epoch 12 | loss: 0.45456 | val_0_rmse: 0.78211 | val_1_rmse: 0.74804 |  0:00:04s
epoch 13 | loss: 0.44984 | val_0_rmse: 0.78274 | val_1_rmse: 0.75225 |  0:00:04s
epoch 14 | loss: 0.44447 | val_0_rmse: 0.79554 | val_1_rmse: 0.76253 |  0:00:05s
epoch 15 | loss: 0.42894 | val_0_rmse: 0.79559 | val_1_rmse: 0.7534  |  0:00:05s
epoch 16 | loss: 0.43465 | val_0_rmse: 0.79642 | val_1_rmse: 0.75493 |  0:00:06s
epoch 17 | loss: 0.41571 | val_0_rmse: 0.77175 | val_1_rmse: 0.73006 |  0:00:06s
epoch 18 | loss: 0.42173 | val_0_rmse: 0.77422 | val_1_rmse: 0.73055 |  0:00:06s
epoch 19 | loss: 0.40708 | val_0_rmse: 0.77476 | val_1_rmse: 0.73326 |  0:00:07s
epoch 20 | loss: 0.41101 | val_0_rmse: 0.76823 | val_1_rmse: 0.73048 |  0:00:07s
epoch 21 | loss: 0.40407 | val_0_rmse: 0.75279 | val_1_rmse: 0.71752 |  0:00:07s
epoch 22 | loss: 0.40813 | val_0_rmse: 0.77605 | val_1_rmse: 0.74768 |  0:00:08s
epoch 23 | loss: 0.409   | val_0_rmse: 0.75946 | val_1_rmse: 0.73815 |  0:00:08s
epoch 24 | loss: 0.4042  | val_0_rmse: 0.77714 | val_1_rmse: 0.7672  |  0:00:08s
epoch 25 | loss: 0.38959 | val_0_rmse: 0.7536  | val_1_rmse: 0.73655 |  0:00:09s
epoch 26 | loss: 0.38463 | val_0_rmse: 0.75206 | val_1_rmse: 0.73554 |  0:00:09s
epoch 27 | loss: 0.37543 | val_0_rmse: 0.7307  | val_1_rmse: 0.71156 |  0:00:09s
epoch 28 | loss: 0.38354 | val_0_rmse: 0.73489 | val_1_rmse: 0.71366 |  0:00:10s
epoch 29 | loss: 0.3778  | val_0_rmse: 0.75305 | val_1_rmse: 0.7364  |  0:00:10s
epoch 30 | loss: 0.37965 | val_0_rmse: 0.75447 | val_1_rmse: 0.74016 |  0:00:10s
epoch 31 | loss: 0.3829  | val_0_rmse: 0.75603 | val_1_rmse: 0.74765 |  0:00:11s
epoch 32 | loss: 0.38194 | val_0_rmse: 0.732   | val_1_rmse: 0.72375 |  0:00:11s
epoch 33 | loss: 0.37624 | val_0_rmse: 0.74398 | val_1_rmse: 0.73156 |  0:00:12s
epoch 34 | loss: 0.37056 | val_0_rmse: 0.7512  | val_1_rmse: 0.7355  |  0:00:12s
epoch 35 | loss: 0.35335 | val_0_rmse: 0.76465 | val_1_rmse: 0.7488  |  0:00:12s
epoch 36 | loss: 0.36916 | val_0_rmse: 0.73261 | val_1_rmse: 0.72184 |  0:00:13s
epoch 37 | loss: 0.374   | val_0_rmse: 0.74559 | val_1_rmse: 0.74481 |  0:00:13s
epoch 38 | loss: 0.37652 | val_0_rmse: 0.77558 | val_1_rmse: 0.77474 |  0:00:13s
epoch 39 | loss: 0.36467 | val_0_rmse: 0.70535 | val_1_rmse: 0.70041 |  0:00:14s
epoch 40 | loss: 0.36179 | val_0_rmse: 0.71261 | val_1_rmse: 0.69998 |  0:00:14s
epoch 41 | loss: 0.35878 | val_0_rmse: 0.70663 | val_1_rmse: 0.69772 |  0:00:14s
epoch 42 | loss: 0.37019 | val_0_rmse: 0.68599 | val_1_rmse: 0.68332 |  0:00:15s
epoch 43 | loss: 0.36022 | val_0_rmse: 0.69382 | val_1_rmse: 0.68895 |  0:00:15s
epoch 44 | loss: 0.37014 | val_0_rmse: 0.69728 | val_1_rmse: 0.69411 |  0:00:15s
epoch 45 | loss: 0.37431 | val_0_rmse: 0.69609 | val_1_rmse: 0.69018 |  0:00:16s
epoch 46 | loss: 0.3628  | val_0_rmse: 0.73385 | val_1_rmse: 0.72522 |  0:00:16s
epoch 47 | loss: 0.36985 | val_0_rmse: 0.72132 | val_1_rmse: 0.71519 |  0:00:17s
epoch 48 | loss: 0.36254 | val_0_rmse: 0.69467 | val_1_rmse: 0.69361 |  0:00:17s
epoch 49 | loss: 0.36203 | val_0_rmse: 0.7     | val_1_rmse: 0.69649 |  0:00:17s
epoch 50 | loss: 0.35165 | val_0_rmse: 0.68856 | val_1_rmse: 0.68916 |  0:00:18s
epoch 51 | loss: 0.34579 | val_0_rmse: 0.70238 | val_1_rmse: 0.69824 |  0:00:18s
epoch 52 | loss: 0.35401 | val_0_rmse: 0.68481 | val_1_rmse: 0.68077 |  0:00:18s
epoch 53 | loss: 0.34589 | val_0_rmse: 0.67773 | val_1_rmse: 0.67298 |  0:00:19s
epoch 54 | loss: 0.33977 | val_0_rmse: 0.67258 | val_1_rmse: 0.6694  |  0:00:19s
epoch 55 | loss: 0.34039 | val_0_rmse: 0.66717 | val_1_rmse: 0.66334 |  0:00:19s
epoch 56 | loss: 0.33812 | val_0_rmse: 0.67393 | val_1_rmse: 0.67298 |  0:00:20s
epoch 57 | loss: 0.32607 | val_0_rmse: 0.6572  | val_1_rmse: 0.65631 |  0:00:20s
epoch 58 | loss: 0.34101 | val_0_rmse: 0.66311 | val_1_rmse: 0.6625  |  0:00:20s
epoch 59 | loss: 0.33137 | val_0_rmse: 0.66345 | val_1_rmse: 0.66115 |  0:00:21s
epoch 60 | loss: 0.34579 | val_0_rmse: 0.67348 | val_1_rmse: 0.67206 |  0:00:21s
epoch 61 | loss: 0.33698 | val_0_rmse: 0.64556 | val_1_rmse: 0.6487  |  0:00:21s
epoch 62 | loss: 0.32805 | val_0_rmse: 0.63872 | val_1_rmse: 0.6436  |  0:00:22s
epoch 63 | loss: 0.33339 | val_0_rmse: 0.65676 | val_1_rmse: 0.66222 |  0:00:22s
epoch 64 | loss: 0.32277 | val_0_rmse: 0.67183 | val_1_rmse: 0.67175 |  0:00:22s
epoch 65 | loss: 0.33008 | val_0_rmse: 0.6518  | val_1_rmse: 0.65336 |  0:00:23s
epoch 66 | loss: 0.32743 | val_0_rmse: 0.63266 | val_1_rmse: 0.63161 |  0:00:23s
epoch 67 | loss: 0.32805 | val_0_rmse: 0.65679 | val_1_rmse: 0.65592 |  0:00:24s
epoch 68 | loss: 0.33171 | val_0_rmse: 0.64873 | val_1_rmse: 0.65128 |  0:00:24s
epoch 69 | loss: 0.32176 | val_0_rmse: 0.65889 | val_1_rmse: 0.6652  |  0:00:24s
epoch 70 | loss: 0.32499 | val_0_rmse: 0.63452 | val_1_rmse: 0.64514 |  0:00:25s
epoch 71 | loss: 0.33182 | val_0_rmse: 0.61387 | val_1_rmse: 0.62519 |  0:00:25s
epoch 72 | loss: 0.32364 | val_0_rmse: 0.62183 | val_1_rmse: 0.63226 |  0:00:25s
epoch 73 | loss: 0.32774 | val_0_rmse: 0.64863 | val_1_rmse: 0.65464 |  0:00:26s
epoch 74 | loss: 0.32916 | val_0_rmse: 0.61623 | val_1_rmse: 0.63474 |  0:00:26s
epoch 75 | loss: 0.31638 | val_0_rmse: 0.62107 | val_1_rmse: 0.64319 |  0:00:26s
epoch 76 | loss: 0.31503 | val_0_rmse: 0.62468 | val_1_rmse: 0.64873 |  0:00:27s
epoch 77 | loss: 0.31982 | val_0_rmse: 0.60109 | val_1_rmse: 0.63422 |  0:00:27s
epoch 78 | loss: 0.31753 | val_0_rmse: 0.66438 | val_1_rmse: 0.68239 |  0:00:27s
epoch 79 | loss: 0.33305 | val_0_rmse: 0.637   | val_1_rmse: 0.63807 |  0:00:28s
epoch 80 | loss: 0.32533 | val_0_rmse: 0.62731 | val_1_rmse: 0.63106 |  0:00:28s
epoch 81 | loss: 0.32107 | val_0_rmse: 0.61741 | val_1_rmse: 0.62862 |  0:00:28s
epoch 82 | loss: 0.31941 | val_0_rmse: 0.6127  | val_1_rmse: 0.62156 |  0:00:29s
epoch 83 | loss: 0.31574 | val_0_rmse: 0.60753 | val_1_rmse: 0.61911 |  0:00:29s
epoch 84 | loss: 0.31073 | val_0_rmse: 0.60779 | val_1_rmse: 0.62729 |  0:00:30s
epoch 85 | loss: 0.30948 | val_0_rmse: 0.60459 | val_1_rmse: 0.63065 |  0:00:30s
epoch 86 | loss: 0.30998 | val_0_rmse: 0.60029 | val_1_rmse: 0.63133 |  0:00:30s
epoch 87 | loss: 0.30242 | val_0_rmse: 0.59417 | val_1_rmse: 0.62717 |  0:00:31s
epoch 88 | loss: 0.30367 | val_0_rmse: 0.59244 | val_1_rmse: 0.6231  |  0:00:31s
epoch 89 | loss: 0.29463 | val_0_rmse: 0.59043 | val_1_rmse: 0.61597 |  0:00:31s
epoch 90 | loss: 0.29253 | val_0_rmse: 0.59989 | val_1_rmse: 0.62534 |  0:00:32s
epoch 91 | loss: 0.30036 | val_0_rmse: 0.5822  | val_1_rmse: 0.62266 |  0:00:32s
epoch 92 | loss: 0.31515 | val_0_rmse: 0.58477 | val_1_rmse: 0.62943 |  0:00:32s
epoch 93 | loss: 0.31511 | val_0_rmse: 0.58533 | val_1_rmse: 0.61864 |  0:00:33s
epoch 94 | loss: 0.31603 | val_0_rmse: 0.59801 | val_1_rmse: 0.62099 |  0:00:33s
epoch 95 | loss: 0.31403 | val_0_rmse: 0.58467 | val_1_rmse: 0.61454 |  0:00:33s
epoch 96 | loss: 0.31255 | val_0_rmse: 0.56954 | val_1_rmse: 0.60955 |  0:00:34s
epoch 97 | loss: 0.31119 | val_0_rmse: 0.56741 | val_1_rmse: 0.60896 |  0:00:34s
epoch 98 | loss: 0.30451 | val_0_rmse: 0.58566 | val_1_rmse: 0.6196  |  0:00:34s
epoch 99 | loss: 0.30656 | val_0_rmse: 0.58153 | val_1_rmse: 0.61501 |  0:00:35s
epoch 100| loss: 0.31009 | val_0_rmse: 0.56685 | val_1_rmse: 0.60571 |  0:00:35s
epoch 101| loss: 0.30021 | val_0_rmse: 0.55446 | val_1_rmse: 0.60277 |  0:00:36s
epoch 102| loss: 0.297   | val_0_rmse: 0.56642 | val_1_rmse: 0.60409 |  0:00:36s
epoch 103| loss: 0.29195 | val_0_rmse: 0.554   | val_1_rmse: 0.5931  |  0:00:36s
epoch 104| loss: 0.29483 | val_0_rmse: 0.55472 | val_1_rmse: 0.59307 |  0:00:37s
epoch 105| loss: 0.29549 | val_0_rmse: 0.56483 | val_1_rmse: 0.60472 |  0:00:37s
epoch 106| loss: 0.30261 | val_0_rmse: 0.55616 | val_1_rmse: 0.60353 |  0:00:37s
epoch 107| loss: 0.29632 | val_0_rmse: 0.56101 | val_1_rmse: 0.60217 |  0:00:38s
epoch 108| loss: 0.29128 | val_0_rmse: 0.56029 | val_1_rmse: 0.60844 |  0:00:38s
epoch 109| loss: 0.29781 | val_0_rmse: 0.56148 | val_1_rmse: 0.60351 |  0:00:38s
epoch 110| loss: 0.30116 | val_0_rmse: 0.55246 | val_1_rmse: 0.59213 |  0:00:39s
epoch 111| loss: 0.29235 | val_0_rmse: 0.55973 | val_1_rmse: 0.59613 |  0:00:39s
epoch 112| loss: 0.29379 | val_0_rmse: 0.54385 | val_1_rmse: 0.5953  |  0:00:39s
epoch 113| loss: 0.28827 | val_0_rmse: 0.54648 | val_1_rmse: 0.5952  |  0:00:40s
epoch 114| loss: 0.29573 | val_0_rmse: 0.54721 | val_1_rmse: 0.61762 |  0:00:40s
epoch 115| loss: 0.3037  | val_0_rmse: 0.54765 | val_1_rmse: 0.60608 |  0:00:40s
epoch 116| loss: 0.29883 | val_0_rmse: 0.549   | val_1_rmse: 0.60552 |  0:00:41s
epoch 117| loss: 0.29512 | val_0_rmse: 0.5437  | val_1_rmse: 0.61238 |  0:00:41s
epoch 118| loss: 0.29631 | val_0_rmse: 0.54865 | val_1_rmse: 0.60026 |  0:00:42s
epoch 119| loss: 0.2993  | val_0_rmse: 0.5561  | val_1_rmse: 0.61004 |  0:00:42s
epoch 120| loss: 0.31431 | val_0_rmse: 0.55325 | val_1_rmse: 0.62365 |  0:00:42s
epoch 121| loss: 0.31337 | val_0_rmse: 0.55849 | val_1_rmse: 0.62352 |  0:00:43s
epoch 122| loss: 0.32551 | val_0_rmse: 0.56159 | val_1_rmse: 0.61189 |  0:00:43s
epoch 123| loss: 0.32024 | val_0_rmse: 0.561   | val_1_rmse: 0.60465 |  0:00:43s
epoch 124| loss: 0.32224 | val_0_rmse: 0.56038 | val_1_rmse: 0.6025  |  0:00:44s
epoch 125| loss: 0.30303 | val_0_rmse: 0.55996 | val_1_rmse: 0.59354 |  0:00:44s
epoch 126| loss: 0.29027 | val_0_rmse: 0.53573 | val_1_rmse: 0.591   |  0:00:44s
epoch 127| loss: 0.29593 | val_0_rmse: 0.5508  | val_1_rmse: 0.59808 |  0:00:45s
epoch 128| loss: 0.30502 | val_0_rmse: 0.54141 | val_1_rmse: 0.59311 |  0:00:45s
epoch 129| loss: 0.29728 | val_0_rmse: 0.53942 | val_1_rmse: 0.59623 |  0:00:45s
epoch 130| loss: 0.30644 | val_0_rmse: 0.54103 | val_1_rmse: 0.59699 |  0:00:46s
epoch 131| loss: 0.29503 | val_0_rmse: 0.53623 | val_1_rmse: 0.59435 |  0:00:46s
epoch 132| loss: 0.29568 | val_0_rmse: 0.52813 | val_1_rmse: 0.59933 |  0:00:46s
epoch 133| loss: 0.2872  | val_0_rmse: 0.55306 | val_1_rmse: 0.61703 |  0:00:47s
epoch 134| loss: 0.31109 | val_0_rmse: 0.57753 | val_1_rmse: 0.64781 |  0:00:47s
epoch 135| loss: 0.31875 | val_0_rmse: 0.55066 | val_1_rmse: 0.64021 |  0:00:48s
epoch 136| loss: 0.3174  | val_0_rmse: 0.55195 | val_1_rmse: 0.6284  |  0:00:48s
epoch 137| loss: 0.31298 | val_0_rmse: 0.53967 | val_1_rmse: 0.60987 |  0:00:48s
epoch 138| loss: 0.30627 | val_0_rmse: 0.5501  | val_1_rmse: 0.61135 |  0:00:49s
epoch 139| loss: 0.30426 | val_0_rmse: 0.52516 | val_1_rmse: 0.61263 |  0:00:49s
epoch 140| loss: 0.29633 | val_0_rmse: 0.52623 | val_1_rmse: 0.62107 |  0:00:49s
epoch 141| loss: 0.29394 | val_0_rmse: 0.5265  | val_1_rmse: 0.61349 |  0:00:50s
epoch 142| loss: 0.29632 | val_0_rmse: 0.54473 | val_1_rmse: 0.61431 |  0:00:50s
epoch 143| loss: 0.29451 | val_0_rmse: 0.53654 | val_1_rmse: 0.61597 |  0:00:50s
epoch 144| loss: 0.31732 | val_0_rmse: 0.53169 | val_1_rmse: 0.61891 |  0:00:51s
epoch 145| loss: 0.29048 | val_0_rmse: 0.52885 | val_1_rmse: 0.61268 |  0:00:51s
epoch 146| loss: 0.29158 | val_0_rmse: 0.52548 | val_1_rmse: 0.63303 |  0:00:51s
epoch 147| loss: 0.29563 | val_0_rmse: 0.5182  | val_1_rmse: 0.61833 |  0:00:52s
epoch 148| loss: 0.28635 | val_0_rmse: 0.52088 | val_1_rmse: 0.6081  |  0:00:52s
epoch 149| loss: 0.28575 | val_0_rmse: 0.51524 | val_1_rmse: 0.6225  |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.591
Best weights from best epoch are automatically used!
ended training at: 08:05:02
Feature importance:
Mean squared error is of 3113205020.1812525
Mean absolute error:38394.94784473684
MAPE:0.25621846206054355
R2 score:0.594893824592176
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:05:02
epoch 0  | loss: 2.05123 | val_0_rmse: 0.99157 | val_1_rmse: 1.07836 |  0:00:00s
epoch 1  | loss: 1.17738 | val_0_rmse: 0.99261 | val_1_rmse: 1.07803 |  0:00:00s
epoch 2  | loss: 0.90879 | val_0_rmse: 0.98821 | val_1_rmse: 1.07771 |  0:00:01s
epoch 3  | loss: 0.7703  | val_0_rmse: 0.94523 | val_1_rmse: 1.04264 |  0:00:01s
epoch 4  | loss: 0.68359 | val_0_rmse: 0.84717 | val_1_rmse: 0.94857 |  0:00:01s
epoch 5  | loss: 0.54934 | val_0_rmse: 0.77994 | val_1_rmse: 0.85711 |  0:00:02s
epoch 6  | loss: 0.54548 | val_0_rmse: 0.75711 | val_1_rmse: 0.8244  |  0:00:02s
epoch 7  | loss: 0.4883  | val_0_rmse: 0.75701 | val_1_rmse: 0.81623 |  0:00:02s
epoch 8  | loss: 0.43581 | val_0_rmse: 0.81331 | val_1_rmse: 0.86032 |  0:00:03s
epoch 9  | loss: 0.44041 | val_0_rmse: 0.77734 | val_1_rmse: 0.83163 |  0:00:03s
epoch 10 | loss: 0.43861 | val_0_rmse: 0.81043 | val_1_rmse: 0.85091 |  0:00:03s
epoch 11 | loss: 0.42996 | val_0_rmse: 0.76507 | val_1_rmse: 0.84091 |  0:00:04s
epoch 12 | loss: 0.43971 | val_0_rmse: 0.76263 | val_1_rmse: 0.83542 |  0:00:04s
epoch 13 | loss: 0.42794 | val_0_rmse: 0.7322  | val_1_rmse: 0.8298  |  0:00:05s
epoch 14 | loss: 0.41859 | val_0_rmse: 0.72071 | val_1_rmse: 0.80819 |  0:00:05s
epoch 15 | loss: 0.40438 | val_0_rmse: 0.70436 | val_1_rmse: 0.78308 |  0:00:05s
epoch 16 | loss: 0.40128 | val_0_rmse: 0.70778 | val_1_rmse: 0.78229 |  0:00:06s
epoch 17 | loss: 0.39584 | val_0_rmse: 0.72972 | val_1_rmse: 0.79636 |  0:00:06s
epoch 18 | loss: 0.38185 | val_0_rmse: 0.7284  | val_1_rmse: 0.80095 |  0:00:06s
epoch 19 | loss: 0.37148 | val_0_rmse: 0.70288 | val_1_rmse: 0.79024 |  0:00:07s
epoch 20 | loss: 0.36739 | val_0_rmse: 0.7091  | val_1_rmse: 0.79102 |  0:00:07s
epoch 21 | loss: 0.37925 | val_0_rmse: 0.72629 | val_1_rmse: 0.80006 |  0:00:07s
epoch 22 | loss: 0.38068 | val_0_rmse: 0.73957 | val_1_rmse: 0.80367 |  0:00:08s
epoch 23 | loss: 0.37913 | val_0_rmse: 0.72421 | val_1_rmse: 0.79269 |  0:00:08s
epoch 24 | loss: 0.37465 | val_0_rmse: 0.7278  | val_1_rmse: 0.7983  |  0:00:08s
epoch 25 | loss: 0.36712 | val_0_rmse: 0.74335 | val_1_rmse: 0.81991 |  0:00:09s
epoch 26 | loss: 0.36535 | val_0_rmse: 0.73655 | val_1_rmse: 0.79879 |  0:00:09s
epoch 27 | loss: 0.36699 | val_0_rmse: 0.70958 | val_1_rmse: 0.78519 |  0:00:09s
epoch 28 | loss: 0.35247 | val_0_rmse: 0.71293 | val_1_rmse: 0.78866 |  0:00:10s
epoch 29 | loss: 0.35013 | val_0_rmse: 0.69769 | val_1_rmse: 0.78378 |  0:00:10s
epoch 30 | loss: 0.35409 | val_0_rmse: 0.69559 | val_1_rmse: 0.77583 |  0:00:10s
epoch 31 | loss: 0.34848 | val_0_rmse: 0.68794 | val_1_rmse: 0.76976 |  0:00:11s
epoch 32 | loss: 0.3445  | val_0_rmse: 0.69208 | val_1_rmse: 0.77723 |  0:00:11s
epoch 33 | loss: 0.34906 | val_0_rmse: 0.68656 | val_1_rmse: 0.76422 |  0:00:12s
epoch 34 | loss: 0.33824 | val_0_rmse: 0.68267 | val_1_rmse: 0.76311 |  0:00:12s
epoch 35 | loss: 0.33359 | val_0_rmse: 0.68153 | val_1_rmse: 0.75935 |  0:00:12s
epoch 36 | loss: 0.33762 | val_0_rmse: 0.67856 | val_1_rmse: 0.75674 |  0:00:13s
epoch 37 | loss: 0.33916 | val_0_rmse: 0.66862 | val_1_rmse: 0.75227 |  0:00:13s
epoch 38 | loss: 0.32547 | val_0_rmse: 0.67623 | val_1_rmse: 0.75584 |  0:00:13s
epoch 39 | loss: 0.31938 | val_0_rmse: 0.68585 | val_1_rmse: 0.76114 |  0:00:14s
epoch 40 | loss: 0.32088 | val_0_rmse: 0.68017 | val_1_rmse: 0.7585  |  0:00:14s
epoch 41 | loss: 0.32071 | val_0_rmse: 0.67492 | val_1_rmse: 0.75411 |  0:00:14s
epoch 42 | loss: 0.31397 | val_0_rmse: 0.66914 | val_1_rmse: 0.75139 |  0:00:15s
epoch 43 | loss: 0.31333 | val_0_rmse: 0.65955 | val_1_rmse: 0.74494 |  0:00:15s
epoch 44 | loss: 0.32013 | val_0_rmse: 0.66628 | val_1_rmse: 0.75652 |  0:00:15s
epoch 45 | loss: 0.31843 | val_0_rmse: 0.68114 | val_1_rmse: 0.75922 |  0:00:16s
epoch 46 | loss: 0.31581 | val_0_rmse: 0.6622  | val_1_rmse: 0.75205 |  0:00:16s
epoch 47 | loss: 0.31721 | val_0_rmse: 0.66205 | val_1_rmse: 0.75222 |  0:00:17s
epoch 48 | loss: 0.30987 | val_0_rmse: 0.64845 | val_1_rmse: 0.73971 |  0:00:17s
epoch 49 | loss: 0.30841 | val_0_rmse: 0.65995 | val_1_rmse: 0.74735 |  0:00:17s
epoch 50 | loss: 0.31339 | val_0_rmse: 0.65222 | val_1_rmse: 0.73713 |  0:00:18s
epoch 51 | loss: 0.3017  | val_0_rmse: 0.6543  | val_1_rmse: 0.73876 |  0:00:18s
epoch 52 | loss: 0.31047 | val_0_rmse: 0.64745 | val_1_rmse: 0.73284 |  0:00:18s
epoch 53 | loss: 0.30454 | val_0_rmse: 0.64798 | val_1_rmse: 0.7308  |  0:00:19s
epoch 54 | loss: 0.3045  | val_0_rmse: 0.65216 | val_1_rmse: 0.73633 |  0:00:19s
epoch 55 | loss: 0.30171 | val_0_rmse: 0.64724 | val_1_rmse: 0.7353  |  0:00:19s
epoch 56 | loss: 0.30389 | val_0_rmse: 0.63689 | val_1_rmse: 0.73314 |  0:00:20s
epoch 57 | loss: 0.29581 | val_0_rmse: 0.64457 | val_1_rmse: 0.73024 |  0:00:20s
epoch 58 | loss: 0.30105 | val_0_rmse: 0.64093 | val_1_rmse: 0.72937 |  0:00:20s
epoch 59 | loss: 0.29541 | val_0_rmse: 0.6378  | val_1_rmse: 0.72655 |  0:00:21s
epoch 60 | loss: 0.2966  | val_0_rmse: 0.63875 | val_1_rmse: 0.72754 |  0:00:21s
epoch 61 | loss: 0.30161 | val_0_rmse: 0.63787 | val_1_rmse: 0.72614 |  0:00:22s
epoch 62 | loss: 0.2954  | val_0_rmse: 0.63456 | val_1_rmse: 0.72645 |  0:00:22s
epoch 63 | loss: 0.29704 | val_0_rmse: 0.64091 | val_1_rmse: 0.73857 |  0:00:22s
epoch 64 | loss: 0.29563 | val_0_rmse: 0.63756 | val_1_rmse: 0.73208 |  0:00:23s
epoch 65 | loss: 0.29206 | val_0_rmse: 0.63066 | val_1_rmse: 0.73093 |  0:00:23s
epoch 66 | loss: 0.28972 | val_0_rmse: 0.62561 | val_1_rmse: 0.72887 |  0:00:23s
epoch 67 | loss: 0.28899 | val_0_rmse: 0.61521 | val_1_rmse: 0.7236  |  0:00:24s
epoch 68 | loss: 0.28382 | val_0_rmse: 0.61733 | val_1_rmse: 0.71961 |  0:00:24s
epoch 69 | loss: 0.28201 | val_0_rmse: 0.61216 | val_1_rmse: 0.71463 |  0:00:24s
epoch 70 | loss: 0.28425 | val_0_rmse: 0.60323 | val_1_rmse: 0.71347 |  0:00:25s
epoch 71 | loss: 0.28243 | val_0_rmse: 0.61034 | val_1_rmse: 0.70954 |  0:00:25s
epoch 72 | loss: 0.27658 | val_0_rmse: 0.62155 | val_1_rmse: 0.7173  |  0:00:25s
epoch 73 | loss: 0.28437 | val_0_rmse: 0.6053  | val_1_rmse: 0.70697 |  0:00:26s
epoch 74 | loss: 0.28296 | val_0_rmse: 0.60544 | val_1_rmse: 0.70961 |  0:00:26s
epoch 75 | loss: 0.28612 | val_0_rmse: 0.61414 | val_1_rmse: 0.72156 |  0:00:27s
epoch 76 | loss: 0.28527 | val_0_rmse: 0.61553 | val_1_rmse: 0.72538 |  0:00:27s
epoch 77 | loss: 0.28016 | val_0_rmse: 0.60377 | val_1_rmse: 0.72179 |  0:00:27s
epoch 78 | loss: 0.27738 | val_0_rmse: 0.60321 | val_1_rmse: 0.71655 |  0:00:28s
epoch 79 | loss: 0.28951 | val_0_rmse: 0.59304 | val_1_rmse: 0.71146 |  0:00:28s
epoch 80 | loss: 0.27956 | val_0_rmse: 0.58342 | val_1_rmse: 0.70936 |  0:00:28s
epoch 81 | loss: 0.27986 | val_0_rmse: 0.59198 | val_1_rmse: 0.70658 |  0:00:29s
epoch 82 | loss: 0.27142 | val_0_rmse: 0.57684 | val_1_rmse: 0.69899 |  0:00:29s
epoch 83 | loss: 0.28698 | val_0_rmse: 0.5843  | val_1_rmse: 0.70233 |  0:00:29s
epoch 84 | loss: 0.27566 | val_0_rmse: 0.57194 | val_1_rmse: 0.69711 |  0:00:30s
epoch 85 | loss: 0.26758 | val_0_rmse: 0.59446 | val_1_rmse: 0.71564 |  0:00:30s
epoch 86 | loss: 0.27417 | val_0_rmse: 0.57551 | val_1_rmse: 0.70277 |  0:00:30s
epoch 87 | loss: 0.27178 | val_0_rmse: 0.58643 | val_1_rmse: 0.70247 |  0:00:31s
epoch 88 | loss: 0.2695  | val_0_rmse: 0.5637  | val_1_rmse: 0.6903  |  0:00:31s
epoch 89 | loss: 0.26744 | val_0_rmse: 0.55758 | val_1_rmse: 0.68627 |  0:00:31s
epoch 90 | loss: 0.27363 | val_0_rmse: 0.57215 | val_1_rmse: 0.69823 |  0:00:32s
epoch 91 | loss: 0.27138 | val_0_rmse: 0.55859 | val_1_rmse: 0.68313 |  0:00:32s
epoch 92 | loss: 0.26368 | val_0_rmse: 0.56433 | val_1_rmse: 0.69893 |  0:00:33s
epoch 93 | loss: 0.26606 | val_0_rmse: 0.55886 | val_1_rmse: 0.69146 |  0:00:33s
epoch 94 | loss: 0.26541 | val_0_rmse: 0.55558 | val_1_rmse: 0.6931  |  0:00:33s
epoch 95 | loss: 0.26469 | val_0_rmse: 0.5645  | val_1_rmse: 0.70283 |  0:00:34s
epoch 96 | loss: 0.25932 | val_0_rmse: 0.55178 | val_1_rmse: 0.70859 |  0:00:34s
epoch 97 | loss: 0.25721 | val_0_rmse: 0.53845 | val_1_rmse: 0.69688 |  0:00:34s
epoch 98 | loss: 0.25843 | val_0_rmse: 0.53597 | val_1_rmse: 0.69262 |  0:00:35s
epoch 99 | loss: 0.26242 | val_0_rmse: 0.5484  | val_1_rmse: 0.69783 |  0:00:35s
epoch 100| loss: 0.27127 | val_0_rmse: 0.53787 | val_1_rmse: 0.6976  |  0:00:35s
epoch 101| loss: 0.26341 | val_0_rmse: 0.53366 | val_1_rmse: 0.69748 |  0:00:36s
epoch 102| loss: 0.2576  | val_0_rmse: 0.54453 | val_1_rmse: 0.6999  |  0:00:36s
epoch 103| loss: 0.25567 | val_0_rmse: 0.53312 | val_1_rmse: 0.6992  |  0:00:36s
epoch 104| loss: 0.2462  | val_0_rmse: 0.54121 | val_1_rmse: 0.69353 |  0:00:37s
epoch 105| loss: 0.24699 | val_0_rmse: 0.52617 | val_1_rmse: 0.70015 |  0:00:37s
epoch 106| loss: 0.25095 | val_0_rmse: 0.53659 | val_1_rmse: 0.71213 |  0:00:37s
epoch 107| loss: 0.2392  | val_0_rmse: 0.52153 | val_1_rmse: 0.70702 |  0:00:38s
epoch 108| loss: 0.24699 | val_0_rmse: 0.52862 | val_1_rmse: 0.70025 |  0:00:38s
epoch 109| loss: 0.24671 | val_0_rmse: 0.51495 | val_1_rmse: 0.69035 |  0:00:39s
epoch 110| loss: 0.24389 | val_0_rmse: 0.51958 | val_1_rmse: 0.69666 |  0:00:39s
epoch 111| loss: 0.24635 | val_0_rmse: 0.52469 | val_1_rmse: 0.70659 |  0:00:39s
epoch 112| loss: 0.24748 | val_0_rmse: 0.52054 | val_1_rmse: 0.69899 |  0:00:40s
epoch 113| loss: 0.25454 | val_0_rmse: 0.52373 | val_1_rmse: 0.70244 |  0:00:40s
epoch 114| loss: 0.24558 | val_0_rmse: 0.52852 | val_1_rmse: 0.7017  |  0:00:40s
epoch 115| loss: 0.25193 | val_0_rmse: 0.51531 | val_1_rmse: 0.69844 |  0:00:41s
epoch 116| loss: 0.26292 | val_0_rmse: 0.51382 | val_1_rmse: 0.6954  |  0:00:41s
epoch 117| loss: 0.25643 | val_0_rmse: 0.50814 | val_1_rmse: 0.68921 |  0:00:41s
epoch 118| loss: 0.24997 | val_0_rmse: 0.50992 | val_1_rmse: 0.70134 |  0:00:42s
epoch 119| loss: 0.24626 | val_0_rmse: 0.5201  | val_1_rmse: 0.70759 |  0:00:42s
epoch 120| loss: 0.24363 | val_0_rmse: 0.49672 | val_1_rmse: 0.69725 |  0:00:42s
epoch 121| loss: 0.24206 | val_0_rmse: 0.48917 | val_1_rmse: 0.69027 |  0:00:43s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.68313
Best weights from best epoch are automatically used!
ended training at: 08:05:46
Feature importance:
Mean squared error is of 2823077405.060314
Mean absolute error:36912.758143311396
MAPE:0.2446674325227599
R2 score:0.62178580563859
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:05:46
epoch 0  | loss: 2.24481 | val_0_rmse: 1.02458 | val_1_rmse: 0.98824 |  0:00:00s
epoch 1  | loss: 1.30362 | val_0_rmse: 1.00523 | val_1_rmse: 0.96857 |  0:00:00s
epoch 2  | loss: 1.13243 | val_0_rmse: 0.99832 | val_1_rmse: 0.96266 |  0:00:01s
epoch 3  | loss: 0.98551 | val_0_rmse: 0.96632 | val_1_rmse: 0.92682 |  0:00:01s
epoch 4  | loss: 0.87869 | val_0_rmse: 0.92784 | val_1_rmse: 0.88705 |  0:00:01s
epoch 5  | loss: 0.79614 | val_0_rmse: 0.86785 | val_1_rmse: 0.82175 |  0:00:02s
epoch 6  | loss: 0.72894 | val_0_rmse: 0.86613 | val_1_rmse: 0.83007 |  0:00:02s
epoch 7  | loss: 0.6959  | val_0_rmse: 0.88958 | val_1_rmse: 0.86407 |  0:00:02s
epoch 8  | loss: 0.68024 | val_0_rmse: 0.88707 | val_1_rmse: 0.84911 |  0:00:03s
epoch 9  | loss: 0.67867 | val_0_rmse: 0.89844 | val_1_rmse: 0.86092 |  0:00:03s
epoch 10 | loss: 0.64051 | val_0_rmse: 0.85541 | val_1_rmse: 0.81355 |  0:00:03s
epoch 11 | loss: 0.6301  | val_0_rmse: 0.80769 | val_1_rmse: 0.76299 |  0:00:04s
epoch 12 | loss: 0.59462 | val_0_rmse: 0.79012 | val_1_rmse: 0.75146 |  0:00:04s
epoch 13 | loss: 0.55493 | val_0_rmse: 0.78498 | val_1_rmse: 0.75976 |  0:00:04s
epoch 14 | loss: 0.5334  | val_0_rmse: 0.75635 | val_1_rmse: 0.7389  |  0:00:05s
epoch 15 | loss: 0.50996 | val_0_rmse: 0.77261 | val_1_rmse: 0.75978 |  0:00:05s
epoch 16 | loss: 0.49902 | val_0_rmse: 0.7716  | val_1_rmse: 0.75448 |  0:00:05s
epoch 17 | loss: 0.47635 | val_0_rmse: 0.77754 | val_1_rmse: 0.75471 |  0:00:06s
epoch 18 | loss: 0.47552 | val_0_rmse: 0.7709  | val_1_rmse: 0.75034 |  0:00:06s
epoch 19 | loss: 0.45536 | val_0_rmse: 0.77214 | val_1_rmse: 0.75462 |  0:00:07s
epoch 20 | loss: 0.46524 | val_0_rmse: 0.75881 | val_1_rmse: 0.74254 |  0:00:07s
epoch 21 | loss: 0.44192 | val_0_rmse: 0.75698 | val_1_rmse: 0.74256 |  0:00:07s
epoch 22 | loss: 0.43904 | val_0_rmse: 0.75276 | val_1_rmse: 0.74128 |  0:00:08s
epoch 23 | loss: 0.43216 | val_0_rmse: 0.75176 | val_1_rmse: 0.74142 |  0:00:08s
epoch 24 | loss: 0.43351 | val_0_rmse: 0.74235 | val_1_rmse: 0.73051 |  0:00:08s
epoch 25 | loss: 0.4249  | val_0_rmse: 0.73571 | val_1_rmse: 0.72023 |  0:00:09s
epoch 26 | loss: 0.4252  | val_0_rmse: 0.72702 | val_1_rmse: 0.71016 |  0:00:09s
epoch 27 | loss: 0.42967 | val_0_rmse: 0.7407  | val_1_rmse: 0.72381 |  0:00:09s
epoch 28 | loss: 0.40619 | val_0_rmse: 0.73826 | val_1_rmse: 0.72709 |  0:00:10s
epoch 29 | loss: 0.41487 | val_0_rmse: 0.72563 | val_1_rmse: 0.70416 |  0:00:10s
epoch 30 | loss: 0.40067 | val_0_rmse: 0.73172 | val_1_rmse: 0.71956 |  0:00:10s
epoch 31 | loss: 0.40813 | val_0_rmse: 0.7241  | val_1_rmse: 0.70291 |  0:00:11s
epoch 32 | loss: 0.41001 | val_0_rmse: 0.73083 | val_1_rmse: 0.71387 |  0:00:11s
epoch 33 | loss: 0.4023  | val_0_rmse: 0.73214 | val_1_rmse: 0.70947 |  0:00:12s
epoch 34 | loss: 0.39836 | val_0_rmse: 0.73028 | val_1_rmse: 0.71149 |  0:00:12s
epoch 35 | loss: 0.38272 | val_0_rmse: 0.72859 | val_1_rmse: 0.70829 |  0:00:12s
epoch 36 | loss: 0.38429 | val_0_rmse: 0.7252  | val_1_rmse: 0.70717 |  0:00:13s
epoch 37 | loss: 0.38298 | val_0_rmse: 0.72234 | val_1_rmse: 0.71014 |  0:00:13s
epoch 38 | loss: 0.38073 | val_0_rmse: 0.71211 | val_1_rmse: 0.6999  |  0:00:13s
epoch 39 | loss: 0.37071 | val_0_rmse: 0.72007 | val_1_rmse: 0.70362 |  0:00:14s
epoch 40 | loss: 0.37935 | val_0_rmse: 0.7099  | val_1_rmse: 0.69505 |  0:00:14s
epoch 41 | loss: 0.36922 | val_0_rmse: 0.69296 | val_1_rmse: 0.67318 |  0:00:14s
epoch 42 | loss: 0.37952 | val_0_rmse: 0.7137  | val_1_rmse: 0.69192 |  0:00:15s
epoch 43 | loss: 0.3791  | val_0_rmse: 0.71555 | val_1_rmse: 0.69251 |  0:00:15s
epoch 44 | loss: 0.37043 | val_0_rmse: 0.72018 | val_1_rmse: 0.69416 |  0:00:15s
epoch 45 | loss: 0.3681  | val_0_rmse: 0.69847 | val_1_rmse: 0.67898 |  0:00:16s
epoch 46 | loss: 0.36859 | val_0_rmse: 0.6923  | val_1_rmse: 0.6737  |  0:00:16s
epoch 47 | loss: 0.36445 | val_0_rmse: 0.68779 | val_1_rmse: 0.6746  |  0:00:16s
epoch 48 | loss: 0.36016 | val_0_rmse: 0.69945 | val_1_rmse: 0.68006 |  0:00:17s
epoch 49 | loss: 0.36105 | val_0_rmse: 0.69823 | val_1_rmse: 0.68234 |  0:00:17s
epoch 50 | loss: 0.36549 | val_0_rmse: 0.70199 | val_1_rmse: 0.69258 |  0:00:18s
epoch 51 | loss: 0.36935 | val_0_rmse: 0.68962 | val_1_rmse: 0.67641 |  0:00:18s
epoch 52 | loss: 0.37023 | val_0_rmse: 0.70276 | val_1_rmse: 0.70162 |  0:00:18s
epoch 53 | loss: 0.35692 | val_0_rmse: 0.69553 | val_1_rmse: 0.69793 |  0:00:19s
epoch 54 | loss: 0.36047 | val_0_rmse: 0.68238 | val_1_rmse: 0.67492 |  0:00:19s
epoch 55 | loss: 0.34985 | val_0_rmse: 0.67826 | val_1_rmse: 0.66478 |  0:00:19s
epoch 56 | loss: 0.34652 | val_0_rmse: 0.69939 | val_1_rmse: 0.67279 |  0:00:20s
epoch 57 | loss: 0.35324 | val_0_rmse: 0.67459 | val_1_rmse: 0.66233 |  0:00:20s
epoch 58 | loss: 0.34583 | val_0_rmse: 0.6977  | val_1_rmse: 0.67455 |  0:00:20s
epoch 59 | loss: 0.34271 | val_0_rmse: 0.66748 | val_1_rmse: 0.65714 |  0:00:21s
epoch 60 | loss: 0.34733 | val_0_rmse: 0.68337 | val_1_rmse: 0.67463 |  0:00:21s
epoch 61 | loss: 0.34626 | val_0_rmse: 0.65604 | val_1_rmse: 0.65097 |  0:00:21s
epoch 62 | loss: 0.34855 | val_0_rmse: 0.65398 | val_1_rmse: 0.64011 |  0:00:22s
epoch 63 | loss: 0.34053 | val_0_rmse: 0.66485 | val_1_rmse: 0.64868 |  0:00:22s
epoch 64 | loss: 0.34781 | val_0_rmse: 0.64493 | val_1_rmse: 0.64989 |  0:00:22s
epoch 65 | loss: 0.34424 | val_0_rmse: 0.642   | val_1_rmse: 0.64753 |  0:00:23s
epoch 66 | loss: 0.33449 | val_0_rmse: 0.6355  | val_1_rmse: 0.63809 |  0:00:23s
epoch 67 | loss: 0.32857 | val_0_rmse: 0.65059 | val_1_rmse: 0.63979 |  0:00:24s
epoch 68 | loss: 0.33106 | val_0_rmse: 0.62838 | val_1_rmse: 0.63503 |  0:00:24s
epoch 69 | loss: 0.34187 | val_0_rmse: 0.63726 | val_1_rmse: 0.62878 |  0:00:24s
epoch 70 | loss: 0.32899 | val_0_rmse: 0.63164 | val_1_rmse: 0.6488  |  0:00:25s
epoch 71 | loss: 0.33204 | val_0_rmse: 0.6298  | val_1_rmse: 0.63449 |  0:00:25s
epoch 72 | loss: 0.33118 | val_0_rmse: 0.62982 | val_1_rmse: 0.64358 |  0:00:25s
epoch 73 | loss: 0.33594 | val_0_rmse: 0.63579 | val_1_rmse: 0.65813 |  0:00:26s
epoch 74 | loss: 0.32946 | val_0_rmse: 0.64314 | val_1_rmse: 0.64759 |  0:00:26s
epoch 75 | loss: 0.33809 | val_0_rmse: 0.64138 | val_1_rmse: 0.66054 |  0:00:26s
epoch 76 | loss: 0.33118 | val_0_rmse: 0.64247 | val_1_rmse: 0.64278 |  0:00:27s
epoch 77 | loss: 0.33289 | val_0_rmse: 0.62916 | val_1_rmse: 0.62683 |  0:00:27s
epoch 78 | loss: 0.32418 | val_0_rmse: 0.62437 | val_1_rmse: 0.63151 |  0:00:27s
epoch 79 | loss: 0.32884 | val_0_rmse: 0.62665 | val_1_rmse: 0.62793 |  0:00:28s
epoch 80 | loss: 0.32551 | val_0_rmse: 0.622   | val_1_rmse: 0.63073 |  0:00:28s
epoch 81 | loss: 0.32752 | val_0_rmse: 0.62259 | val_1_rmse: 0.63421 |  0:00:28s
epoch 82 | loss: 0.32991 | val_0_rmse: 0.62338 | val_1_rmse: 0.64029 |  0:00:29s
epoch 83 | loss: 0.31658 | val_0_rmse: 0.61479 | val_1_rmse: 0.63789 |  0:00:29s
epoch 84 | loss: 0.31977 | val_0_rmse: 0.61586 | val_1_rmse: 0.6328  |  0:00:30s
epoch 85 | loss: 0.32128 | val_0_rmse: 0.65048 | val_1_rmse: 0.64986 |  0:00:30s
epoch 86 | loss: 0.3267  | val_0_rmse: 0.62872 | val_1_rmse: 0.64276 |  0:00:30s
epoch 87 | loss: 0.32738 | val_0_rmse: 0.62304 | val_1_rmse: 0.62992 |  0:00:31s
epoch 88 | loss: 0.33134 | val_0_rmse: 0.63526 | val_1_rmse: 0.66476 |  0:00:31s
epoch 89 | loss: 0.32853 | val_0_rmse: 0.60131 | val_1_rmse: 0.6214  |  0:00:31s
epoch 90 | loss: 0.31858 | val_0_rmse: 0.60315 | val_1_rmse: 0.62727 |  0:00:32s
epoch 91 | loss: 0.31216 | val_0_rmse: 0.59241 | val_1_rmse: 0.61305 |  0:00:32s
epoch 92 | loss: 0.32586 | val_0_rmse: 0.6385  | val_1_rmse: 0.6317  |  0:00:32s
epoch 93 | loss: 0.32058 | val_0_rmse: 0.59233 | val_1_rmse: 0.60098 |  0:00:33s
epoch 94 | loss: 0.30998 | val_0_rmse: 0.59012 | val_1_rmse: 0.62257 |  0:00:33s
epoch 95 | loss: 0.30898 | val_0_rmse: 0.58956 | val_1_rmse: 0.62343 |  0:00:33s
epoch 96 | loss: 0.30908 | val_0_rmse: 0.5864  | val_1_rmse: 0.61013 |  0:00:34s
epoch 97 | loss: 0.30618 | val_0_rmse: 0.59663 | val_1_rmse: 0.61148 |  0:00:34s
epoch 98 | loss: 0.30357 | val_0_rmse: 0.59217 | val_1_rmse: 0.61092 |  0:00:34s
epoch 99 | loss: 0.30183 | val_0_rmse: 0.58758 | val_1_rmse: 0.60979 |  0:00:35s
epoch 100| loss: 0.29958 | val_0_rmse: 0.58295 | val_1_rmse: 0.61835 |  0:00:35s
epoch 101| loss: 0.30209 | val_0_rmse: 0.57921 | val_1_rmse: 0.60828 |  0:00:36s
epoch 102| loss: 0.29844 | val_0_rmse: 0.58204 | val_1_rmse: 0.60645 |  0:00:36s
epoch 103| loss: 0.29341 | val_0_rmse: 0.57726 | val_1_rmse: 0.60079 |  0:00:36s
epoch 104| loss: 0.29049 | val_0_rmse: 0.56784 | val_1_rmse: 0.59799 |  0:00:37s
epoch 105| loss: 0.29428 | val_0_rmse: 0.56416 | val_1_rmse: 0.60009 |  0:00:37s
epoch 106| loss: 0.28669 | val_0_rmse: 0.56104 | val_1_rmse: 0.60348 |  0:00:37s
epoch 107| loss: 0.297   | val_0_rmse: 0.56465 | val_1_rmse: 0.60803 |  0:00:38s
epoch 108| loss: 0.29088 | val_0_rmse: 0.56192 | val_1_rmse: 0.61269 |  0:00:38s
epoch 109| loss: 0.28848 | val_0_rmse: 0.55652 | val_1_rmse: 0.60373 |  0:00:38s
epoch 110| loss: 0.28449 | val_0_rmse: 0.54128 | val_1_rmse: 0.61571 |  0:00:39s
epoch 111| loss: 0.28916 | val_0_rmse: 0.54397 | val_1_rmse: 0.61152 |  0:00:39s
epoch 112| loss: 0.2924  | val_0_rmse: 0.53848 | val_1_rmse: 0.60817 |  0:00:39s
epoch 113| loss: 0.29237 | val_0_rmse: 0.53228 | val_1_rmse: 0.60136 |  0:00:40s
epoch 114| loss: 0.28805 | val_0_rmse: 0.53643 | val_1_rmse: 0.59866 |  0:00:40s
epoch 115| loss: 0.27655 | val_0_rmse: 0.53191 | val_1_rmse: 0.60065 |  0:00:40s
epoch 116| loss: 0.29248 | val_0_rmse: 0.5575  | val_1_rmse: 0.63707 |  0:00:41s
epoch 117| loss: 0.29669 | val_0_rmse: 0.56483 | val_1_rmse: 0.6197  |  0:00:41s
epoch 118| loss: 0.29003 | val_0_rmse: 0.53335 | val_1_rmse: 0.59889 |  0:00:42s
epoch 119| loss: 0.28486 | val_0_rmse: 0.54497 | val_1_rmse: 0.60319 |  0:00:42s
epoch 120| loss: 0.28235 | val_0_rmse: 0.52247 | val_1_rmse: 0.5997  |  0:00:42s
epoch 121| loss: 0.28648 | val_0_rmse: 0.52591 | val_1_rmse: 0.59449 |  0:00:43s
epoch 122| loss: 0.28636 | val_0_rmse: 0.53305 | val_1_rmse: 0.6009  |  0:00:43s
epoch 123| loss: 0.29975 | val_0_rmse: 0.55335 | val_1_rmse: 0.61998 |  0:00:43s
epoch 124| loss: 0.30712 | val_0_rmse: 0.55907 | val_1_rmse: 0.63208 |  0:00:44s
epoch 125| loss: 0.29407 | val_0_rmse: 0.53435 | val_1_rmse: 0.6027  |  0:00:44s
epoch 126| loss: 0.28874 | val_0_rmse: 0.55657 | val_1_rmse: 0.6317  |  0:00:44s
epoch 127| loss: 0.28622 | val_0_rmse: 0.5258  | val_1_rmse: 0.61617 |  0:00:45s
epoch 128| loss: 0.28234 | val_0_rmse: 0.52135 | val_1_rmse: 0.59665 |  0:00:45s
epoch 129| loss: 0.27765 | val_0_rmse: 0.51461 | val_1_rmse: 0.59716 |  0:00:45s
epoch 130| loss: 0.27002 | val_0_rmse: 0.52037 | val_1_rmse: 0.59666 |  0:00:46s
epoch 131| loss: 0.28416 | val_0_rmse: 0.51485 | val_1_rmse: 0.59495 |  0:00:46s
epoch 132| loss: 0.27263 | val_0_rmse: 0.51143 | val_1_rmse: 0.59319 |  0:00:46s
epoch 133| loss: 0.27169 | val_0_rmse: 0.51071 | val_1_rmse: 0.59799 |  0:00:47s
epoch 134| loss: 0.2728  | val_0_rmse: 0.51267 | val_1_rmse: 0.5876  |  0:00:47s
epoch 135| loss: 0.27981 | val_0_rmse: 0.51323 | val_1_rmse: 0.58236 |  0:00:48s
epoch 136| loss: 0.26841 | val_0_rmse: 0.50435 | val_1_rmse: 0.59805 |  0:00:48s
epoch 137| loss: 0.26666 | val_0_rmse: 0.51416 | val_1_rmse: 0.59205 |  0:00:48s
epoch 138| loss: 0.27652 | val_0_rmse: 0.50973 | val_1_rmse: 0.597   |  0:00:49s
epoch 139| loss: 0.27983 | val_0_rmse: 0.51132 | val_1_rmse: 0.58565 |  0:00:49s
epoch 140| loss: 0.26847 | val_0_rmse: 0.50859 | val_1_rmse: 0.60904 |  0:00:49s
epoch 141| loss: 0.27587 | val_0_rmse: 0.5137  | val_1_rmse: 0.6046  |  0:00:50s
epoch 142| loss: 0.26892 | val_0_rmse: 0.50611 | val_1_rmse: 0.61067 |  0:00:50s
epoch 143| loss: 0.27939 | val_0_rmse: 0.53387 | val_1_rmse: 0.61822 |  0:00:50s
epoch 144| loss: 0.3024  | val_0_rmse: 0.53104 | val_1_rmse: 0.62247 |  0:00:51s
epoch 145| loss: 0.28861 | val_0_rmse: 0.52436 | val_1_rmse: 0.6083  |  0:00:51s
epoch 146| loss: 0.29635 | val_0_rmse: 0.52911 | val_1_rmse: 0.60259 |  0:00:51s
epoch 147| loss: 0.29688 | val_0_rmse: 0.53549 | val_1_rmse: 0.61811 |  0:00:52s
epoch 148| loss: 0.29343 | val_0_rmse: 0.52273 | val_1_rmse: 0.60258 |  0:00:52s
epoch 149| loss: 0.29165 | val_0_rmse: 0.53527 | val_1_rmse: 0.60743 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.58236
Best weights from best epoch are automatically used!
ended training at: 08:06:39
Feature importance:
Mean squared error is of 2726465093.1018863
Mean absolute error:35519.40963645834
MAPE:0.22512817187824016
R2 score:0.6437195618980427
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:06:39
epoch 0  | loss: 2.06298 | val_0_rmse: 0.98627 | val_1_rmse: 0.99695 |  0:00:00s
epoch 1  | loss: 1.10064 | val_0_rmse: 0.95179 | val_1_rmse: 0.98069 |  0:00:00s
epoch 2  | loss: 0.85459 | val_0_rmse: 0.86979 | val_1_rmse: 0.89327 |  0:00:01s
epoch 3  | loss: 0.7906  | val_0_rmse: 0.86187 | val_1_rmse: 0.88444 |  0:00:01s
epoch 4  | loss: 0.75555 | val_0_rmse: 0.86961 | val_1_rmse: 0.88402 |  0:00:01s
epoch 5  | loss: 0.6916  | val_0_rmse: 0.85462 | val_1_rmse: 0.86601 |  0:00:02s
epoch 6  | loss: 0.62153 | val_0_rmse: 0.83169 | val_1_rmse: 0.84466 |  0:00:02s
epoch 7  | loss: 0.5647  | val_0_rmse: 0.87118 | val_1_rmse: 0.87673 |  0:00:02s
epoch 8  | loss: 0.53256 | val_0_rmse: 0.86259 | val_1_rmse: 0.86869 |  0:00:03s
epoch 9  | loss: 0.50356 | val_0_rmse: 0.87762 | val_1_rmse: 0.87082 |  0:00:03s
epoch 10 | loss: 0.49353 | val_0_rmse: 0.83893 | val_1_rmse: 0.8403  |  0:00:03s
epoch 11 | loss: 0.503   | val_0_rmse: 0.82708 | val_1_rmse: 0.83489 |  0:00:04s
epoch 12 | loss: 0.48567 | val_0_rmse: 0.80778 | val_1_rmse: 0.81975 |  0:00:04s
epoch 13 | loss: 0.47366 | val_0_rmse: 0.80822 | val_1_rmse: 0.81866 |  0:00:04s
epoch 14 | loss: 0.46704 | val_0_rmse: 0.84782 | val_1_rmse: 0.85315 |  0:00:05s
epoch 15 | loss: 0.43025 | val_0_rmse: 0.82904 | val_1_rmse: 0.83928 |  0:00:05s
epoch 16 | loss: 0.43011 | val_0_rmse: 0.82942 | val_1_rmse: 0.84172 |  0:00:06s
epoch 17 | loss: 0.41458 | val_0_rmse: 0.8408  | val_1_rmse: 0.85426 |  0:00:06s
epoch 18 | loss: 0.40078 | val_0_rmse: 0.81752 | val_1_rmse: 0.83217 |  0:00:06s
epoch 19 | loss: 0.4147  | val_0_rmse: 0.80857 | val_1_rmse: 0.82339 |  0:00:07s
epoch 20 | loss: 0.4182  | val_0_rmse: 0.78874 | val_1_rmse: 0.80363 |  0:00:07s
epoch 21 | loss: 0.41056 | val_0_rmse: 0.77395 | val_1_rmse: 0.79433 |  0:00:07s
epoch 22 | loss: 0.39367 | val_0_rmse: 0.76075 | val_1_rmse: 0.78128 |  0:00:08s
epoch 23 | loss: 0.39293 | val_0_rmse: 0.78348 | val_1_rmse: 0.8014  |  0:00:08s
epoch 24 | loss: 0.38538 | val_0_rmse: 0.7546  | val_1_rmse: 0.77544 |  0:00:08s
epoch 25 | loss: 0.36703 | val_0_rmse: 0.74043 | val_1_rmse: 0.76233 |  0:00:09s
epoch 26 | loss: 0.36643 | val_0_rmse: 0.74942 | val_1_rmse: 0.77332 |  0:00:09s
epoch 27 | loss: 0.36954 | val_0_rmse: 0.73561 | val_1_rmse: 0.76375 |  0:00:09s
epoch 28 | loss: 0.36231 | val_0_rmse: 0.73596 | val_1_rmse: 0.76243 |  0:00:10s
epoch 29 | loss: 0.37211 | val_0_rmse: 0.73418 | val_1_rmse: 0.76153 |  0:00:10s
epoch 30 | loss: 0.36091 | val_0_rmse: 0.74258 | val_1_rmse: 0.76726 |  0:00:11s
epoch 31 | loss: 0.35975 | val_0_rmse: 0.73218 | val_1_rmse: 0.75922 |  0:00:11s
epoch 32 | loss: 0.36282 | val_0_rmse: 0.72844 | val_1_rmse: 0.75496 |  0:00:11s
epoch 33 | loss: 0.36034 | val_0_rmse: 0.73029 | val_1_rmse: 0.76463 |  0:00:12s
epoch 34 | loss: 0.35823 | val_0_rmse: 0.7404  | val_1_rmse: 0.76552 |  0:00:12s
epoch 35 | loss: 0.36231 | val_0_rmse: 0.71529 | val_1_rmse: 0.74674 |  0:00:12s
epoch 36 | loss: 0.34972 | val_0_rmse: 0.71212 | val_1_rmse: 0.74291 |  0:00:13s
epoch 37 | loss: 0.34965 | val_0_rmse: 0.73367 | val_1_rmse: 0.75639 |  0:00:13s
epoch 38 | loss: 0.33747 | val_0_rmse: 0.72276 | val_1_rmse: 0.74944 |  0:00:13s
epoch 39 | loss: 0.33718 | val_0_rmse: 0.71106 | val_1_rmse: 0.74871 |  0:00:14s
epoch 40 | loss: 0.33119 | val_0_rmse: 0.69826 | val_1_rmse: 0.73957 |  0:00:14s
epoch 41 | loss: 0.33176 | val_0_rmse: 0.68588 | val_1_rmse: 0.72989 |  0:00:14s
epoch 42 | loss: 0.32662 | val_0_rmse: 0.69886 | val_1_rmse: 0.73921 |  0:00:15s
epoch 43 | loss: 0.3288  | val_0_rmse: 0.6917  | val_1_rmse: 0.731   |  0:00:15s
epoch 44 | loss: 0.32962 | val_0_rmse: 0.68026 | val_1_rmse: 0.72065 |  0:00:15s
epoch 45 | loss: 0.32336 | val_0_rmse: 0.67996 | val_1_rmse: 0.71756 |  0:00:16s
epoch 46 | loss: 0.32906 | val_0_rmse: 0.6938  | val_1_rmse: 0.72895 |  0:00:16s
epoch 47 | loss: 0.32599 | val_0_rmse: 0.69498 | val_1_rmse: 0.73221 |  0:00:17s
epoch 48 | loss: 0.32552 | val_0_rmse: 0.6753  | val_1_rmse: 0.71852 |  0:00:17s
epoch 49 | loss: 0.32619 | val_0_rmse: 0.66991 | val_1_rmse: 0.72223 |  0:00:17s
epoch 50 | loss: 0.31934 | val_0_rmse: 0.67807 | val_1_rmse: 0.73412 |  0:00:18s
epoch 51 | loss: 0.32088 | val_0_rmse: 0.67603 | val_1_rmse: 0.73022 |  0:00:18s
epoch 52 | loss: 0.31779 | val_0_rmse: 0.67789 | val_1_rmse: 0.72385 |  0:00:18s
epoch 53 | loss: 0.31377 | val_0_rmse: 0.66708 | val_1_rmse: 0.71473 |  0:00:19s
epoch 54 | loss: 0.31865 | val_0_rmse: 0.65892 | val_1_rmse: 0.71077 |  0:00:19s
epoch 55 | loss: 0.31513 | val_0_rmse: 0.65791 | val_1_rmse: 0.71112 |  0:00:19s
epoch 56 | loss: 0.3165  | val_0_rmse: 0.66395 | val_1_rmse: 0.7028  |  0:00:20s
epoch 57 | loss: 0.32282 | val_0_rmse: 0.65345 | val_1_rmse: 0.70399 |  0:00:20s
epoch 58 | loss: 0.32047 | val_0_rmse: 0.64519 | val_1_rmse: 0.7029  |  0:00:20s
epoch 59 | loss: 0.30937 | val_0_rmse: 0.64904 | val_1_rmse: 0.69548 |  0:00:21s
epoch 60 | loss: 0.31043 | val_0_rmse: 0.6493  | val_1_rmse: 0.70127 |  0:00:21s
epoch 61 | loss: 0.31257 | val_0_rmse: 0.64591 | val_1_rmse: 0.7058  |  0:00:22s
epoch 62 | loss: 0.31122 | val_0_rmse: 0.65268 | val_1_rmse: 0.701   |  0:00:22s
epoch 63 | loss: 0.31385 | val_0_rmse: 0.63076 | val_1_rmse: 0.69225 |  0:00:22s
epoch 64 | loss: 0.3036  | val_0_rmse: 0.63149 | val_1_rmse: 0.69145 |  0:00:23s
epoch 65 | loss: 0.30144 | val_0_rmse: 0.62288 | val_1_rmse: 0.68221 |  0:00:23s
epoch 66 | loss: 0.30361 | val_0_rmse: 0.62469 | val_1_rmse: 0.68608 |  0:00:23s
epoch 67 | loss: 0.30453 | val_0_rmse: 0.62144 | val_1_rmse: 0.68867 |  0:00:24s
epoch 68 | loss: 0.30195 | val_0_rmse: 0.62298 | val_1_rmse: 0.69323 |  0:00:24s
epoch 69 | loss: 0.29646 | val_0_rmse: 0.62368 | val_1_rmse: 0.68611 |  0:00:24s
epoch 70 | loss: 0.2962  | val_0_rmse: 0.6211  | val_1_rmse: 0.6855  |  0:00:25s
epoch 71 | loss: 0.29707 | val_0_rmse: 0.61188 | val_1_rmse: 0.68142 |  0:00:25s
epoch 72 | loss: 0.29569 | val_0_rmse: 0.60849 | val_1_rmse: 0.67626 |  0:00:25s
epoch 73 | loss: 0.28933 | val_0_rmse: 0.6008  | val_1_rmse: 0.67686 |  0:00:26s
epoch 74 | loss: 0.29026 | val_0_rmse: 0.59247 | val_1_rmse: 0.67639 |  0:00:26s
epoch 75 | loss: 0.28402 | val_0_rmse: 0.59794 | val_1_rmse: 0.68111 |  0:00:26s
epoch 76 | loss: 0.27894 | val_0_rmse: 0.59424 | val_1_rmse: 0.68859 |  0:00:27s
epoch 77 | loss: 0.28553 | val_0_rmse: 0.58925 | val_1_rmse: 0.67443 |  0:00:27s
epoch 78 | loss: 0.27933 | val_0_rmse: 0.57849 | val_1_rmse: 0.67544 |  0:00:27s
epoch 79 | loss: 0.28626 | val_0_rmse: 0.59911 | val_1_rmse: 0.67907 |  0:00:28s
epoch 80 | loss: 0.29358 | val_0_rmse: 0.59105 | val_1_rmse: 0.67492 |  0:00:28s
epoch 81 | loss: 0.28188 | val_0_rmse: 0.5774  | val_1_rmse: 0.67412 |  0:00:29s
epoch 82 | loss: 0.28514 | val_0_rmse: 0.5824  | val_1_rmse: 0.66633 |  0:00:29s
epoch 83 | loss: 0.28138 | val_0_rmse: 0.58181 | val_1_rmse: 0.67317 |  0:00:29s
epoch 84 | loss: 0.28228 | val_0_rmse: 0.57718 | val_1_rmse: 0.65947 |  0:00:30s
epoch 85 | loss: 0.28637 | val_0_rmse: 0.57196 | val_1_rmse: 0.65326 |  0:00:30s
epoch 86 | loss: 0.27889 | val_0_rmse: 0.57006 | val_1_rmse: 0.661   |  0:00:30s
epoch 87 | loss: 0.28104 | val_0_rmse: 0.57883 | val_1_rmse: 0.66304 |  0:00:31s
epoch 88 | loss: 0.27656 | val_0_rmse: 0.56549 | val_1_rmse: 0.66348 |  0:00:31s
epoch 89 | loss: 0.28037 | val_0_rmse: 0.56509 | val_1_rmse: 0.66647 |  0:00:31s
epoch 90 | loss: 0.27327 | val_0_rmse: 0.56541 | val_1_rmse: 0.66031 |  0:00:32s
epoch 91 | loss: 0.27548 | val_0_rmse: 0.5732  | val_1_rmse: 0.67359 |  0:00:32s
epoch 92 | loss: 0.28366 | val_0_rmse: 0.59134 | val_1_rmse: 0.67597 |  0:00:32s
epoch 93 | loss: 0.28254 | val_0_rmse: 0.56446 | val_1_rmse: 0.6647  |  0:00:33s
epoch 94 | loss: 0.28343 | val_0_rmse: 0.55242 | val_1_rmse: 0.66746 |  0:00:33s
epoch 95 | loss: 0.26759 | val_0_rmse: 0.57644 | val_1_rmse: 0.6752  |  0:00:34s
epoch 96 | loss: 0.26538 | val_0_rmse: 0.55182 | val_1_rmse: 0.6783  |  0:00:34s
epoch 97 | loss: 0.26466 | val_0_rmse: 0.54523 | val_1_rmse: 0.65874 |  0:00:34s
epoch 98 | loss: 0.26841 | val_0_rmse: 0.54944 | val_1_rmse: 0.67006 |  0:00:35s
epoch 99 | loss: 0.26391 | val_0_rmse: 0.53485 | val_1_rmse: 0.65376 |  0:00:35s
epoch 100| loss: 0.27099 | val_0_rmse: 0.5351  | val_1_rmse: 0.659   |  0:00:35s
epoch 101| loss: 0.26812 | val_0_rmse: 0.52466 | val_1_rmse: 0.65701 |  0:00:36s
epoch 102| loss: 0.26111 | val_0_rmse: 0.52721 | val_1_rmse: 0.66101 |  0:00:36s
epoch 103| loss: 0.26652 | val_0_rmse: 0.53281 | val_1_rmse: 0.66147 |  0:00:36s
epoch 104| loss: 0.25697 | val_0_rmse: 0.52803 | val_1_rmse: 0.65492 |  0:00:37s
epoch 105| loss: 0.2652  | val_0_rmse: 0.53305 | val_1_rmse: 0.6508  |  0:00:37s
epoch 106| loss: 0.26163 | val_0_rmse: 0.52482 | val_1_rmse: 0.65574 |  0:00:37s
epoch 107| loss: 0.26757 | val_0_rmse: 0.52461 | val_1_rmse: 0.65507 |  0:00:38s
epoch 108| loss: 0.26043 | val_0_rmse: 0.54274 | val_1_rmse: 0.65705 |  0:00:38s
epoch 109| loss: 0.2628  | val_0_rmse: 0.53547 | val_1_rmse: 0.66033 |  0:00:38s
epoch 110| loss: 0.26643 | val_0_rmse: 0.54213 | val_1_rmse: 0.65389 |  0:00:39s
epoch 111| loss: 0.2633  | val_0_rmse: 0.53041 | val_1_rmse: 0.65958 |  0:00:39s
epoch 112| loss: 0.26701 | val_0_rmse: 0.52939 | val_1_rmse: 0.65561 |  0:00:40s
epoch 113| loss: 0.26611 | val_0_rmse: 0.52453 | val_1_rmse: 0.65435 |  0:00:40s
epoch 114| loss: 0.26332 | val_0_rmse: 0.50656 | val_1_rmse: 0.65766 |  0:00:40s
epoch 115| loss: 0.26187 | val_0_rmse: 0.51947 | val_1_rmse: 0.65089 |  0:00:41s
epoch 116| loss: 0.25467 | val_0_rmse: 0.50293 | val_1_rmse: 0.66466 |  0:00:41s
epoch 117| loss: 0.26459 | val_0_rmse: 0.49772 | val_1_rmse: 0.65057 |  0:00:41s
epoch 118| loss: 0.2579  | val_0_rmse: 0.5017  | val_1_rmse: 0.64458 |  0:00:42s
epoch 119| loss: 0.25307 | val_0_rmse: 0.52035 | val_1_rmse: 0.70064 |  0:00:42s
epoch 120| loss: 0.25819 | val_0_rmse: 0.49963 | val_1_rmse: 0.65035 |  0:00:42s
epoch 121| loss: 0.25319 | val_0_rmse: 0.49397 | val_1_rmse: 0.64978 |  0:00:43s
epoch 122| loss: 0.25054 | val_0_rmse: 0.49677 | val_1_rmse: 0.66211 |  0:00:43s
epoch 123| loss: 0.24881 | val_0_rmse: 0.51343 | val_1_rmse: 0.64322 |  0:00:43s
epoch 124| loss: 0.24997 | val_0_rmse: 0.48802 | val_1_rmse: 0.65196 |  0:00:44s
epoch 125| loss: 0.25008 | val_0_rmse: 0.48809 | val_1_rmse: 0.63995 |  0:00:44s
epoch 126| loss: 0.25427 | val_0_rmse: 0.49192 | val_1_rmse: 0.6357  |  0:00:44s
epoch 127| loss: 0.24808 | val_0_rmse: 0.48518 | val_1_rmse: 0.64852 |  0:00:45s
epoch 128| loss: 0.24479 | val_0_rmse: 0.49078 | val_1_rmse: 0.63757 |  0:00:45s
epoch 129| loss: 0.23874 | val_0_rmse: 0.48287 | val_1_rmse: 0.63968 |  0:00:46s
epoch 130| loss: 0.24428 | val_0_rmse: 0.47894 | val_1_rmse: 0.63451 |  0:00:46s
epoch 131| loss: 0.23877 | val_0_rmse: 0.50212 | val_1_rmse: 0.6365  |  0:00:46s
epoch 132| loss: 0.24701 | val_0_rmse: 0.48595 | val_1_rmse: 0.63852 |  0:00:47s
epoch 133| loss: 0.24603 | val_0_rmse: 0.48938 | val_1_rmse: 0.64699 |  0:00:47s
epoch 134| loss: 0.24975 | val_0_rmse: 0.47779 | val_1_rmse: 0.65071 |  0:00:47s
epoch 135| loss: 0.23985 | val_0_rmse: 0.48356 | val_1_rmse: 0.64952 |  0:00:48s
epoch 136| loss: 0.24506 | val_0_rmse: 0.48494 | val_1_rmse: 0.65858 |  0:00:48s
epoch 137| loss: 0.2508  | val_0_rmse: 0.4802  | val_1_rmse: 0.67819 |  0:00:48s
epoch 138| loss: 0.24304 | val_0_rmse: 0.47342 | val_1_rmse: 0.65878 |  0:00:49s
epoch 139| loss: 0.24673 | val_0_rmse: 0.48047 | val_1_rmse: 0.63288 |  0:00:49s
epoch 140| loss: 0.23752 | val_0_rmse: 0.47088 | val_1_rmse: 0.64236 |  0:00:49s
epoch 141| loss: 0.24494 | val_0_rmse: 0.51071 | val_1_rmse: 0.64655 |  0:00:50s
epoch 142| loss: 0.23916 | val_0_rmse: 0.46738 | val_1_rmse: 0.63658 |  0:00:50s
epoch 143| loss: 0.238   | val_0_rmse: 0.47335 | val_1_rmse: 0.66049 |  0:00:50s
epoch 144| loss: 0.23748 | val_0_rmse: 0.46947 | val_1_rmse: 0.64188 |  0:00:51s
epoch 145| loss: 0.23325 | val_0_rmse: 0.466   | val_1_rmse: 0.65115 |  0:00:51s
epoch 146| loss: 0.23055 | val_0_rmse: 0.4638  | val_1_rmse: 0.63545 |  0:00:52s
epoch 147| loss: 0.23365 | val_0_rmse: 0.47008 | val_1_rmse: 0.63321 |  0:00:52s
epoch 148| loss: 0.2306  | val_0_rmse: 0.46793 | val_1_rmse: 0.65195 |  0:00:52s
epoch 149| loss: 0.23651 | val_0_rmse: 0.46172 | val_1_rmse: 0.63475 |  0:00:53s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.63288
Best weights from best epoch are automatically used!
ended training at: 08:07:32
Feature importance:
Mean squared error is of 3172684466.7883787
Mean absolute error:38630.93959528508
MAPE:0.23859636513314691
R2 score:0.6007992037378671
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:07:32
epoch 0  | loss: 2.11708 | val_0_rmse: 0.99198 | val_1_rmse: 1.04309 |  0:00:00s
epoch 1  | loss: 1.1276  | val_0_rmse: 0.98783 | val_1_rmse: 1.05023 |  0:00:00s
epoch 2  | loss: 1.01296 | val_0_rmse: 1.06983 | val_1_rmse: 1.0382  |  0:00:01s
epoch 3  | loss: 0.94356 | val_0_rmse: 0.99398 | val_1_rmse: 1.03793 |  0:00:01s
epoch 4  | loss: 0.87631 | val_0_rmse: 0.95562 | val_1_rmse: 1.00853 |  0:00:01s
epoch 5  | loss: 0.81694 | val_0_rmse: 0.93437 | val_1_rmse: 0.99039 |  0:00:02s
epoch 6  | loss: 0.73121 | val_0_rmse: 0.88967 | val_1_rmse: 0.93394 |  0:00:02s
epoch 7  | loss: 0.63121 | val_0_rmse: 0.9001  | val_1_rmse: 0.9398  |  0:00:02s
epoch 8  | loss: 0.57611 | val_0_rmse: 0.809   | val_1_rmse: 0.85952 |  0:00:03s
epoch 9  | loss: 0.5406  | val_0_rmse: 0.78036 | val_1_rmse: 0.83271 |  0:00:03s
epoch 10 | loss: 0.50928 | val_0_rmse: 0.79972 | val_1_rmse: 0.85952 |  0:00:03s
epoch 11 | loss: 0.47548 | val_0_rmse: 0.77516 | val_1_rmse: 0.83535 |  0:00:04s
epoch 12 | loss: 0.45832 | val_0_rmse: 0.79279 | val_1_rmse: 0.8583  |  0:00:04s
epoch 13 | loss: 0.44608 | val_0_rmse: 0.77921 | val_1_rmse: 0.84274 |  0:00:04s
epoch 14 | loss: 0.4305  | val_0_rmse: 0.79119 | val_1_rmse: 0.8663  |  0:00:05s
epoch 15 | loss: 0.42115 | val_0_rmse: 0.77726 | val_1_rmse: 0.85791 |  0:00:05s
epoch 16 | loss: 0.41077 | val_0_rmse: 0.7568  | val_1_rmse: 0.83828 |  0:00:05s
epoch 17 | loss: 0.38837 | val_0_rmse: 0.78224 | val_1_rmse: 0.86459 |  0:00:06s
epoch 18 | loss: 0.38239 | val_0_rmse: 0.79399 | val_1_rmse: 0.88241 |  0:00:06s
epoch 19 | loss: 0.36825 | val_0_rmse: 0.76646 | val_1_rmse: 0.85777 |  0:00:07s
epoch 20 | loss: 0.36423 | val_0_rmse: 0.76811 | val_1_rmse: 0.86444 |  0:00:07s
epoch 21 | loss: 0.3487  | val_0_rmse: 0.75476 | val_1_rmse: 0.85367 |  0:00:07s
epoch 22 | loss: 0.34678 | val_0_rmse: 0.76086 | val_1_rmse: 0.86012 |  0:00:08s
epoch 23 | loss: 0.34408 | val_0_rmse: 0.74845 | val_1_rmse: 0.84522 |  0:00:08s
epoch 24 | loss: 0.34168 | val_0_rmse: 0.73087 | val_1_rmse: 0.82715 |  0:00:08s
epoch 25 | loss: 0.34303 | val_0_rmse: 0.7351  | val_1_rmse: 0.82794 |  0:00:09s
epoch 26 | loss: 0.34615 | val_0_rmse: 0.73114 | val_1_rmse: 0.82753 |  0:00:09s
epoch 27 | loss: 0.34003 | val_0_rmse: 0.74328 | val_1_rmse: 0.84035 |  0:00:09s
epoch 28 | loss: 0.32955 | val_0_rmse: 0.72936 | val_1_rmse: 0.82677 |  0:00:10s
epoch 29 | loss: 0.33191 | val_0_rmse: 0.75316 | val_1_rmse: 0.8447  |  0:00:10s
epoch 30 | loss: 0.32319 | val_0_rmse: 0.71786 | val_1_rmse: 0.81005 |  0:00:10s
epoch 31 | loss: 0.33177 | val_0_rmse: 0.70956 | val_1_rmse: 0.78598 |  0:00:11s
epoch 32 | loss: 0.34216 | val_0_rmse: 0.72886 | val_1_rmse: 0.80608 |  0:00:11s
epoch 33 | loss: 0.32923 | val_0_rmse: 0.71681 | val_1_rmse: 0.8014  |  0:00:12s
epoch 34 | loss: 0.33117 | val_0_rmse: 0.74699 | val_1_rmse: 0.83082 |  0:00:12s
epoch 35 | loss: 0.31851 | val_0_rmse: 0.74062 | val_1_rmse: 0.82405 |  0:00:12s
epoch 36 | loss: 0.32098 | val_0_rmse: 0.72865 | val_1_rmse: 0.8193  |  0:00:13s
epoch 37 | loss: 0.31137 | val_0_rmse: 0.73394 | val_1_rmse: 0.82319 |  0:00:13s
epoch 38 | loss: 0.31113 | val_0_rmse: 0.70887 | val_1_rmse: 0.79511 |  0:00:13s
epoch 39 | loss: 0.30803 | val_0_rmse: 0.72827 | val_1_rmse: 0.81449 |  0:00:14s
epoch 40 | loss: 0.31649 | val_0_rmse: 0.70585 | val_1_rmse: 0.79719 |  0:00:14s
epoch 41 | loss: 0.3074  | val_0_rmse: 0.70456 | val_1_rmse: 0.79532 |  0:00:14s
epoch 42 | loss: 0.30813 | val_0_rmse: 0.73472 | val_1_rmse: 0.82202 |  0:00:15s
epoch 43 | loss: 0.30605 | val_0_rmse: 0.69511 | val_1_rmse: 0.79146 |  0:00:15s
epoch 44 | loss: 0.30085 | val_0_rmse: 0.68358 | val_1_rmse: 0.7807  |  0:00:15s
epoch 45 | loss: 0.29768 | val_0_rmse: 0.71198 | val_1_rmse: 0.80282 |  0:00:16s
epoch 46 | loss: 0.28883 | val_0_rmse: 0.68813 | val_1_rmse: 0.78854 |  0:00:16s
epoch 47 | loss: 0.29098 | val_0_rmse: 0.6899  | val_1_rmse: 0.79058 |  0:00:16s
epoch 48 | loss: 0.28358 | val_0_rmse: 0.69443 | val_1_rmse: 0.78857 |  0:00:17s
epoch 49 | loss: 0.28519 | val_0_rmse: 0.67715 | val_1_rmse: 0.773   |  0:00:17s
epoch 50 | loss: 0.28408 | val_0_rmse: 0.68225 | val_1_rmse: 0.77882 |  0:00:18s
epoch 51 | loss: 0.28967 | val_0_rmse: 0.71431 | val_1_rmse: 0.80022 |  0:00:18s
epoch 52 | loss: 0.28525 | val_0_rmse: 0.68262 | val_1_rmse: 0.775   |  0:00:18s
epoch 53 | loss: 0.27822 | val_0_rmse: 0.68704 | val_1_rmse: 0.77462 |  0:00:19s
epoch 54 | loss: 0.27798 | val_0_rmse: 0.66498 | val_1_rmse: 0.75243 |  0:00:19s
epoch 55 | loss: 0.27118 | val_0_rmse: 0.68065 | val_1_rmse: 0.76817 |  0:00:19s
epoch 56 | loss: 0.27309 | val_0_rmse: 0.6957  | val_1_rmse: 0.7816  |  0:00:20s
epoch 57 | loss: 0.27915 | val_0_rmse: 0.6621  | val_1_rmse: 0.75575 |  0:00:20s
epoch 58 | loss: 0.27206 | val_0_rmse: 0.68035 | val_1_rmse: 0.76449 |  0:00:20s
epoch 59 | loss: 0.26493 | val_0_rmse: 0.65504 | val_1_rmse: 0.74817 |  0:00:21s
epoch 60 | loss: 0.27085 | val_0_rmse: 0.67235 | val_1_rmse: 0.75844 |  0:00:21s
epoch 61 | loss: 0.26955 | val_0_rmse: 0.62645 | val_1_rmse: 0.73329 |  0:00:21s
epoch 62 | loss: 0.27012 | val_0_rmse: 0.62406 | val_1_rmse: 0.73095 |  0:00:22s
epoch 63 | loss: 0.27697 | val_0_rmse: 0.62624 | val_1_rmse: 0.73796 |  0:00:22s
epoch 64 | loss: 0.26739 | val_0_rmse: 0.63922 | val_1_rmse: 0.75033 |  0:00:22s
epoch 65 | loss: 0.26402 | val_0_rmse: 0.6477  | val_1_rmse: 0.75529 |  0:00:23s
epoch 66 | loss: 0.26293 | val_0_rmse: 0.67082 | val_1_rmse: 0.77391 |  0:00:23s
epoch 67 | loss: 0.26279 | val_0_rmse: 0.62306 | val_1_rmse: 0.73847 |  0:00:24s
epoch 68 | loss: 0.25934 | val_0_rmse: 0.6082  | val_1_rmse: 0.72853 |  0:00:24s
epoch 69 | loss: 0.26509 | val_0_rmse: 0.59448 | val_1_rmse: 0.7225  |  0:00:24s
epoch 70 | loss: 0.26307 | val_0_rmse: 0.6179  | val_1_rmse: 0.73814 |  0:00:25s
epoch 71 | loss: 0.26009 | val_0_rmse: 0.59741 | val_1_rmse: 0.72607 |  0:00:25s
epoch 72 | loss: 0.25597 | val_0_rmse: 0.61905 | val_1_rmse: 0.7392  |  0:00:25s
epoch 73 | loss: 0.26554 | val_0_rmse: 0.58997 | val_1_rmse: 0.71543 |  0:00:26s
epoch 74 | loss: 0.26719 | val_0_rmse: 0.58515 | val_1_rmse: 0.71128 |  0:00:26s
epoch 75 | loss: 0.25888 | val_0_rmse: 0.6335  | val_1_rmse: 0.74828 |  0:00:26s
epoch 76 | loss: 0.25919 | val_0_rmse: 0.58919 | val_1_rmse: 0.72274 |  0:00:27s
epoch 77 | loss: 0.26643 | val_0_rmse: 0.6178  | val_1_rmse: 0.73468 |  0:00:27s
epoch 78 | loss: 0.25496 | val_0_rmse: 0.60036 | val_1_rmse: 0.71558 |  0:00:27s
epoch 79 | loss: 0.26143 | val_0_rmse: 0.59419 | val_1_rmse: 0.71511 |  0:00:28s
epoch 80 | loss: 0.25897 | val_0_rmse: 0.59308 | val_1_rmse: 0.71648 |  0:00:28s
epoch 81 | loss: 0.24886 | val_0_rmse: 0.58995 | val_1_rmse: 0.7094  |  0:00:29s
epoch 82 | loss: 0.26374 | val_0_rmse: 0.56688 | val_1_rmse: 0.69268 |  0:00:29s
epoch 83 | loss: 0.25892 | val_0_rmse: 0.57618 | val_1_rmse: 0.70443 |  0:00:29s
epoch 84 | loss: 0.25399 | val_0_rmse: 0.56604 | val_1_rmse: 0.70208 |  0:00:30s
epoch 85 | loss: 0.25981 | val_0_rmse: 0.5814  | val_1_rmse: 0.71717 |  0:00:30s
epoch 86 | loss: 0.25373 | val_0_rmse: 0.59986 | val_1_rmse: 0.73292 |  0:00:30s
epoch 87 | loss: 0.24806 | val_0_rmse: 0.55941 | val_1_rmse: 0.70081 |  0:00:31s
epoch 88 | loss: 0.24512 | val_0_rmse: 0.59974 | val_1_rmse: 0.73033 |  0:00:31s
epoch 89 | loss: 0.25624 | val_0_rmse: 0.54569 | val_1_rmse: 0.68775 |  0:00:31s
epoch 90 | loss: 0.26664 | val_0_rmse: 0.54657 | val_1_rmse: 0.68578 |  0:00:32s
epoch 91 | loss: 0.24959 | val_0_rmse: 0.55599 | val_1_rmse: 0.69524 |  0:00:32s
epoch 92 | loss: 0.25301 | val_0_rmse: 0.54184 | val_1_rmse: 0.69225 |  0:00:32s
epoch 93 | loss: 0.25059 | val_0_rmse: 0.5533  | val_1_rmse: 0.71048 |  0:00:33s
epoch 94 | loss: 0.25061 | val_0_rmse: 0.56884 | val_1_rmse: 0.71857 |  0:00:33s
epoch 95 | loss: 0.23929 | val_0_rmse: 0.5653  | val_1_rmse: 0.72247 |  0:00:33s
epoch 96 | loss: 0.24313 | val_0_rmse: 0.54942 | val_1_rmse: 0.71084 |  0:00:34s
epoch 97 | loss: 0.24158 | val_0_rmse: 0.5252  | val_1_rmse: 0.68989 |  0:00:34s
epoch 98 | loss: 0.23926 | val_0_rmse: 0.53057 | val_1_rmse: 0.69094 |  0:00:34s
epoch 99 | loss: 0.24489 | val_0_rmse: 0.53475 | val_1_rmse: 0.69943 |  0:00:35s
epoch 100| loss: 0.243   | val_0_rmse: 0.54133 | val_1_rmse: 0.71162 |  0:00:35s
epoch 101| loss: 0.2424  | val_0_rmse: 0.51363 | val_1_rmse: 0.68715 |  0:00:36s
epoch 102| loss: 0.23779 | val_0_rmse: 0.51323 | val_1_rmse: 0.68282 |  0:00:36s
epoch 103| loss: 0.2339  | val_0_rmse: 0.50265 | val_1_rmse: 0.6816  |  0:00:36s
epoch 104| loss: 0.23746 | val_0_rmse: 0.51468 | val_1_rmse: 0.68768 |  0:00:37s
epoch 105| loss: 0.24084 | val_0_rmse: 0.5262  | val_1_rmse: 0.69663 |  0:00:37s
epoch 106| loss: 0.22922 | val_0_rmse: 0.50487 | val_1_rmse: 0.68573 |  0:00:37s
epoch 107| loss: 0.23962 | val_0_rmse: 0.50888 | val_1_rmse: 0.69115 |  0:00:38s
epoch 108| loss: 0.23002 | val_0_rmse: 0.51274 | val_1_rmse: 0.6944  |  0:00:38s
epoch 109| loss: 0.23569 | val_0_rmse: 0.51054 | val_1_rmse: 0.68976 |  0:00:38s
epoch 110| loss: 0.23611 | val_0_rmse: 0.4964  | val_1_rmse: 0.68336 |  0:00:39s
epoch 111| loss: 0.23296 | val_0_rmse: 0.5066  | val_1_rmse: 0.69314 |  0:00:39s
epoch 112| loss: 0.23495 | val_0_rmse: 0.50117 | val_1_rmse: 0.68172 |  0:00:39s
epoch 113| loss: 0.23937 | val_0_rmse: 0.48865 | val_1_rmse: 0.67118 |  0:00:40s
epoch 114| loss: 0.23928 | val_0_rmse: 0.49645 | val_1_rmse: 0.67827 |  0:00:40s
epoch 115| loss: 0.23161 | val_0_rmse: 0.49053 | val_1_rmse: 0.67576 |  0:00:40s
epoch 116| loss: 0.23977 | val_0_rmse: 0.4905  | val_1_rmse: 0.67982 |  0:00:41s
epoch 117| loss: 0.23144 | val_0_rmse: 0.5164  | val_1_rmse: 0.69071 |  0:00:41s
epoch 118| loss: 0.23101 | val_0_rmse: 0.47973 | val_1_rmse: 0.67574 |  0:00:42s
epoch 119| loss: 0.23438 | val_0_rmse: 0.5083  | val_1_rmse: 0.6929  |  0:00:42s
epoch 120| loss: 0.24953 | val_0_rmse: 0.47993 | val_1_rmse: 0.66685 |  0:00:42s
epoch 121| loss: 0.23211 | val_0_rmse: 0.49227 | val_1_rmse: 0.68945 |  0:00:43s
epoch 122| loss: 0.22639 | val_0_rmse: 0.4778  | val_1_rmse: 0.68278 |  0:00:43s
epoch 123| loss: 0.23191 | val_0_rmse: 0.47598 | val_1_rmse: 0.67921 |  0:00:43s
epoch 124| loss: 0.227   | val_0_rmse: 0.46929 | val_1_rmse: 0.67336 |  0:00:44s
epoch 125| loss: 0.22551 | val_0_rmse: 0.46588 | val_1_rmse: 0.67288 |  0:00:44s
epoch 126| loss: 0.22506 | val_0_rmse: 0.47464 | val_1_rmse: 0.66057 |  0:00:44s
epoch 127| loss: 0.22896 | val_0_rmse: 0.47199 | val_1_rmse: 0.66123 |  0:00:45s
epoch 128| loss: 0.22441 | val_0_rmse: 0.50148 | val_1_rmse: 0.68231 |  0:00:45s
epoch 129| loss: 0.22237 | val_0_rmse: 0.49091 | val_1_rmse: 0.67807 |  0:00:45s
epoch 130| loss: 0.22749 | val_0_rmse: 0.47169 | val_1_rmse: 0.68085 |  0:00:46s
epoch 131| loss: 0.23113 | val_0_rmse: 0.45782 | val_1_rmse: 0.66975 |  0:00:46s
epoch 132| loss: 0.22702 | val_0_rmse: 0.45393 | val_1_rmse: 0.67385 |  0:00:47s
epoch 133| loss: 0.22394 | val_0_rmse: 0.45965 | val_1_rmse: 0.6706  |  0:00:47s
epoch 134| loss: 0.22519 | val_0_rmse: 0.45679 | val_1_rmse: 0.68175 |  0:00:47s
epoch 135| loss: 0.23307 | val_0_rmse: 0.45506 | val_1_rmse: 0.67795 |  0:00:48s
epoch 136| loss: 0.22423 | val_0_rmse: 0.471   | val_1_rmse: 0.68525 |  0:00:48s
epoch 137| loss: 0.22249 | val_0_rmse: 0.47135 | val_1_rmse: 0.70054 |  0:00:48s
epoch 138| loss: 0.22068 | val_0_rmse: 0.45455 | val_1_rmse: 0.67885 |  0:00:49s
epoch 139| loss: 0.22252 | val_0_rmse: 0.45758 | val_1_rmse: 0.68215 |  0:00:49s
epoch 140| loss: 0.2155  | val_0_rmse: 0.44978 | val_1_rmse: 0.6758  |  0:00:49s
epoch 141| loss: 0.21435 | val_0_rmse: 0.49973 | val_1_rmse: 0.70887 |  0:00:50s
epoch 142| loss: 0.22382 | val_0_rmse: 0.44633 | val_1_rmse: 0.67448 |  0:00:50s
epoch 143| loss: 0.21997 | val_0_rmse: 0.44707 | val_1_rmse: 0.67793 |  0:00:50s
epoch 144| loss: 0.21321 | val_0_rmse: 0.44532 | val_1_rmse: 0.68279 |  0:00:51s
epoch 145| loss: 0.223   | val_0_rmse: 0.44494 | val_1_rmse: 0.67965 |  0:00:51s
epoch 146| loss: 0.22404 | val_0_rmse: 0.44466 | val_1_rmse: 0.67553 |  0:00:51s
epoch 147| loss: 0.21913 | val_0_rmse: 0.44265 | val_1_rmse: 0.6681  |  0:00:52s
epoch 148| loss: 0.213   | val_0_rmse: 0.45221 | val_1_rmse: 0.67848 |  0:00:52s
epoch 149| loss: 0.2207  | val_0_rmse: 0.43934 | val_1_rmse: 0.68016 |  0:00:53s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.66057
Best weights from best epoch are automatically used!
ended training at: 08:08:25
Feature importance:
Mean squared error is of 3139957344.67172
Mean absolute error:38686.58679495614
MAPE:0.2553717272849473
R2 score:0.6244414089127364
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:09:11
epoch 0  | loss: 0.53184 | val_0_rmse: 0.6533  | val_1_rmse: 0.65229 |  0:00:43s
epoch 1  | loss: 0.24866 | val_0_rmse: 0.53641 | val_1_rmse: 0.53722 |  0:01:23s
epoch 2  | loss: 0.22318 | val_0_rmse: 0.46872 | val_1_rmse: 0.47391 |  0:02:03s
epoch 3  | loss: 0.21475 | val_0_rmse: 0.50079 | val_1_rmse: 0.50393 |  0:02:43s
epoch 4  | loss: 0.20643 | val_0_rmse: 0.42871 | val_1_rmse: 0.44341 |  0:03:23s
epoch 5  | loss: 0.20042 | val_0_rmse: 0.44533 | val_1_rmse: 0.46286 |  0:04:02s
epoch 6  | loss: 0.19919 | val_0_rmse: 0.46507 | val_1_rmse: 0.47982 |  0:04:41s
epoch 7  | loss: 0.19748 | val_0_rmse: 0.43661 | val_1_rmse: 0.45768 |  0:05:21s
epoch 8  | loss: 0.19257 | val_0_rmse: 0.46321 | val_1_rmse: 0.47774 |  0:06:00s
epoch 9  | loss: 0.19288 | val_0_rmse: 0.46819 | val_1_rmse: 0.45859 |  0:06:39s
epoch 10 | loss: 0.19368 | val_0_rmse: 0.62345 | val_1_rmse: 0.6026  |  0:07:18s
epoch 11 | loss: 0.18866 | val_0_rmse: 0.49507 | val_1_rmse: 0.46556 |  0:07:58s
epoch 12 | loss: 0.18803 | val_0_rmse: 0.43751 | val_1_rmse: 0.45357 |  0:08:37s
epoch 13 | loss: 0.18717 | val_0_rmse: 0.45775 | val_1_rmse: 0.45469 |  0:09:17s
epoch 14 | loss: 0.18182 | val_0_rmse: 0.50432 | val_1_rmse: 0.48364 |  0:09:56s
epoch 15 | loss: 0.1837  | val_0_rmse: 0.42756 | val_1_rmse: 0.45122 |  0:10:35s
epoch 16 | loss: 0.18331 | val_0_rmse: 0.44901 | val_1_rmse: 0.47494 |  0:11:14s
epoch 17 | loss: 0.17886 | val_0_rmse: 0.43327 | val_1_rmse: 0.4577  |  0:11:54s
epoch 18 | loss: 0.1818  | val_0_rmse: 0.44411 | val_1_rmse: 0.46433 |  0:12:33s
epoch 19 | loss: 0.17926 | val_0_rmse: 0.4264  | val_1_rmse: 0.45271 |  0:13:12s
epoch 20 | loss: 0.18131 | val_0_rmse: 0.44759 | val_1_rmse: 0.47671 |  0:13:52s
epoch 21 | loss: 0.17792 | val_0_rmse: 0.43969 | val_1_rmse: 0.4673  |  0:14:31s
epoch 22 | loss: 0.17622 | val_0_rmse: 0.42365 | val_1_rmse: 0.45145 |  0:15:10s
epoch 23 | loss: 0.17681 | val_0_rmse: 0.58675 | val_1_rmse: 0.52448 |  0:15:50s
epoch 24 | loss: 0.18817 | val_0_rmse: 0.42568 | val_1_rmse: 0.45188 |  0:16:29s
epoch 25 | loss: 0.17944 | val_0_rmse: 0.42822 | val_1_rmse: 0.45795 |  0:17:08s
epoch 26 | loss: 0.18072 | val_0_rmse: 0.42207 | val_1_rmse: 0.44979 |  0:17:47s
epoch 27 | loss: 0.17683 | val_0_rmse: 0.42552 | val_1_rmse: 0.45143 |  0:18:28s
epoch 28 | loss: 0.17514 | val_0_rmse: 0.42454 | val_1_rmse: 0.45555 |  0:19:07s
epoch 29 | loss: 0.17512 | val_0_rmse: 0.4211  | val_1_rmse: 0.4513  |  0:19:46s
epoch 30 | loss: 0.17527 | val_0_rmse: 0.42033 | val_1_rmse: 0.45427 |  0:20:26s
epoch 31 | loss: 0.17372 | val_0_rmse: 0.42677 | val_1_rmse: 0.45336 |  0:21:05s
epoch 32 | loss: 0.17232 | val_0_rmse: 0.41959 | val_1_rmse: 0.44815 |  0:21:44s
epoch 33 | loss: 0.17123 | val_0_rmse: 0.45265 | val_1_rmse: 0.48314 |  0:22:24s
epoch 34 | loss: 0.17387 | val_0_rmse: 0.94574 | val_1_rmse: 0.78248 |  0:23:03s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.44341
Best weights from best epoch are automatically used!
ended training at: 08:32:40
Feature importance:
Mean squared error is of 8678024152.034445
Mean absolute error:60877.35974464751
MAPE:0.2688918204649756
R2 score:0.8036358060013683
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:32:50
epoch 0  | loss: 0.60729 | val_0_rmse: 0.64369 | val_1_rmse: 0.6482  |  0:00:40s
epoch 1  | loss: 0.27669 | val_0_rmse: 0.6263  | val_1_rmse: 0.62928 |  0:01:20s
epoch 2  | loss: 0.26977 | val_0_rmse: 0.49293 | val_1_rmse: 0.4987  |  0:02:00s
epoch 3  | loss: 0.22507 | val_0_rmse: 0.44967 | val_1_rmse: 0.45836 |  0:02:39s
epoch 4  | loss: 0.21303 | val_0_rmse: 0.44781 | val_1_rmse: 0.46185 |  0:03:19s
epoch 5  | loss: 0.20552 | val_0_rmse: 0.44553 | val_1_rmse: 0.46377 |  0:03:59s
epoch 6  | loss: 0.20107 | val_0_rmse: 0.45788 | val_1_rmse: 0.47394 |  0:04:39s
epoch 7  | loss: 0.20133 | val_0_rmse: 0.47565 | val_1_rmse: 0.49304 |  0:05:19s
epoch 8  | loss: 0.24089 | val_0_rmse: 0.48915 | val_1_rmse: 0.50549 |  0:05:58s
epoch 9  | loss: 0.20958 | val_0_rmse: 0.53417 | val_1_rmse: 0.7451  |  0:06:38s
epoch 10 | loss: 0.20477 | val_0_rmse: 0.46476 | val_1_rmse: 0.49806 |  0:07:18s
epoch 11 | loss: 0.21279 | val_0_rmse: 0.4973  | val_1_rmse: 0.5263  |  0:07:58s
epoch 12 | loss: 0.20376 | val_0_rmse: 0.45394 | val_1_rmse: 0.48608 |  0:08:38s
epoch 13 | loss: 0.19543 | val_0_rmse: 0.59388 | val_1_rmse: 1.11329 |  0:09:18s
epoch 14 | loss: 0.20449 | val_0_rmse: 0.4556  | val_1_rmse: 0.47953 |  0:10:03s
epoch 15 | loss: 0.2064  | val_0_rmse: 0.45381 | val_1_rmse: 0.4755  |  0:10:42s
epoch 16 | loss: 0.20766 | val_0_rmse: 0.45926 | val_1_rmse: 0.47941 |  0:11:22s
epoch 17 | loss: 0.20336 | val_0_rmse: 0.44896 | val_1_rmse: 0.46996 |  0:12:01s
epoch 18 | loss: 0.19392 | val_0_rmse: 0.52091 | val_1_rmse: 0.5427  |  0:12:40s
epoch 19 | loss: 0.1914  | val_0_rmse: 0.48046 | val_1_rmse: 0.50299 |  0:13:20s
epoch 20 | loss: 0.1938  | val_0_rmse: 0.49472 | val_1_rmse: 0.51328 |  0:13:59s
epoch 21 | loss: 0.19289 | val_0_rmse: 0.45964 | val_1_rmse: 0.48127 |  0:14:39s
epoch 22 | loss: 0.19803 | val_0_rmse: 0.48348 | val_1_rmse: 0.50068 |  0:15:18s
epoch 23 | loss: 0.19689 | val_0_rmse: 0.46306 | val_1_rmse: 0.47881 |  0:15:58s
epoch 24 | loss: 0.19048 | val_0_rmse: 0.43491 | val_1_rmse: 0.45701 |  0:16:37s
epoch 25 | loss: 0.18647 | val_0_rmse: 0.43787 | val_1_rmse: 0.46107 |  0:17:16s
epoch 26 | loss: 0.1851  | val_0_rmse: 0.44321 | val_1_rmse: 0.46766 |  0:17:56s
epoch 27 | loss: 0.18731 | val_0_rmse: 0.474   | val_1_rmse: 0.45627 |  0:18:35s
epoch 28 | loss: 0.1863  | val_0_rmse: 0.7105  | val_1_rmse: 0.51966 |  0:19:14s
epoch 29 | loss: 0.1862  | val_0_rmse: 0.62415 | val_1_rmse: 0.4562  |  0:19:54s
epoch 30 | loss: 0.18568 | val_0_rmse: 0.53292 | val_1_rmse: 0.46279 |  0:20:33s
epoch 31 | loss: 0.19137 | val_0_rmse: 0.43825 | val_1_rmse: 0.46128 |  0:21:13s
epoch 32 | loss: 0.18527 | val_0_rmse: 0.47278 | val_1_rmse: 0.48445 |  0:21:52s
epoch 33 | loss: 0.18283 | val_0_rmse: 0.6273  | val_1_rmse: 0.47411 |  0:22:31s
epoch 34 | loss: 0.18113 | val_0_rmse: 0.51711 | val_1_rmse: 0.52774 |  0:23:11s
epoch 35 | loss: 0.18579 | val_0_rmse: 0.43255 | val_1_rmse: 0.45517 |  0:23:50s
epoch 36 | loss: 0.17985 | val_0_rmse: 0.43033 | val_1_rmse: 0.45547 |  0:24:30s
epoch 37 | loss: 0.17904 | val_0_rmse: 0.43876 | val_1_rmse: 0.46503 |  0:25:10s
epoch 38 | loss: 0.17714 | val_0_rmse: 0.44659 | val_1_rmse: 0.472   |  0:25:50s
epoch 39 | loss: 0.1771  | val_0_rmse: 0.43164 | val_1_rmse: 0.45649 |  0:26:29s
epoch 40 | loss: 0.17609 | val_0_rmse: 0.42754 | val_1_rmse: 0.45197 |  0:27:09s
epoch 41 | loss: 0.17443 | val_0_rmse: 0.42814 | val_1_rmse: 0.45388 |  0:27:48s
epoch 42 | loss: 0.17411 | val_0_rmse: 0.43468 | val_1_rmse: 0.46166 |  0:28:27s
epoch 43 | loss: 0.17457 | val_0_rmse: 0.42568 | val_1_rmse: 0.45293 |  0:29:07s
epoch 44 | loss: 0.17388 | val_0_rmse: 0.42993 | val_1_rmse: 0.45494 |  0:29:46s
epoch 45 | loss: 0.17354 | val_0_rmse: 0.42645 | val_1_rmse: 0.4526  |  0:30:25s
epoch 46 | loss: 0.17343 | val_0_rmse: 0.4224  | val_1_rmse: 0.45223 |  0:31:05s
epoch 47 | loss: 0.17073 | val_0_rmse: 0.46729 | val_1_rmse: 0.49502 |  0:31:44s
epoch 48 | loss: 0.17064 | val_0_rmse: 0.48964 | val_1_rmse: 0.50795 |  0:32:23s
epoch 49 | loss: 0.16998 | val_0_rmse: 0.46096 | val_1_rmse: 0.47894 |  0:33:03s
epoch 50 | loss: 0.17225 | val_0_rmse: 0.60104 | val_1_rmse: 0.62034 |  0:33:43s
epoch 51 | loss: 0.17498 | val_0_rmse: 0.44367 | val_1_rmse: 0.46697 |  0:34:22s
epoch 52 | loss: 0.17015 | val_0_rmse: 0.43242 | val_1_rmse: 0.46059 |  0:35:01s
epoch 53 | loss: 0.1686  | val_0_rmse: 0.43359 | val_1_rmse: 0.46198 |  0:35:41s
epoch 54 | loss: 0.16885 | val_0_rmse: 0.42777 | val_1_rmse: 0.45967 |  0:36:20s
epoch 55 | loss: 0.16647 | val_0_rmse: 0.42164 | val_1_rmse: 0.45201 |  0:37:00s
epoch 56 | loss: 0.1664  | val_0_rmse: 0.42125 | val_1_rmse: 0.45722 |  0:37:39s
epoch 57 | loss: 0.16769 | val_0_rmse: 0.43049 | val_1_rmse: 0.46112 |  0:38:18s
epoch 58 | loss: 0.16457 | val_0_rmse: 0.42223 | val_1_rmse: 0.45629 |  0:38:58s
epoch 59 | loss: 0.16523 | val_0_rmse: 0.44806 | val_1_rmse: 0.47959 |  0:39:37s
epoch 60 | loss: 0.16456 | val_0_rmse: 0.41997 | val_1_rmse: 0.45369 |  0:40:17s
epoch 61 | loss: 0.16347 | val_0_rmse: 0.42137 | val_1_rmse: 0.45418 |  0:40:57s
epoch 62 | loss: 0.16422 | val_0_rmse: 0.41934 | val_1_rmse: 0.45372 |  0:41:36s
epoch 63 | loss: 0.16494 | val_0_rmse: 0.42622 | val_1_rmse: 0.462   |  0:42:15s
epoch 64 | loss: 0.16376 | val_0_rmse: 0.47613 | val_1_rmse: 0.50795 |  0:42:55s
epoch 65 | loss: 0.16454 | val_0_rmse: 0.56686 | val_1_rmse: 0.59188 |  0:43:34s
epoch 66 | loss: 0.16277 | val_0_rmse: 0.73807 | val_1_rmse: 0.75458 |  0:44:14s
epoch 67 | loss: 0.16337 | val_0_rmse: 0.46527 | val_1_rmse: 0.48725 |  0:44:53s
epoch 68 | loss: 0.16238 | val_0_rmse: 0.46853 | val_1_rmse: 0.50094 |  0:45:33s
epoch 69 | loss: 0.16285 | val_0_rmse: 0.45083 | val_1_rmse: 0.48804 |  0:46:12s
epoch 70 | loss: 0.16252 | val_0_rmse: 0.42803 | val_1_rmse: 0.45991 |  0:46:52s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.45197
Best weights from best epoch are automatically used!
ended training at: 09:20:05
Feature importance:
Mean squared error is of 8844237142.592613
Mean absolute error:61215.63434015228
MAPE:0.2721191561657966
R2 score:0.7988972988483202
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:20:14
epoch 0  | loss: 0.60687 | val_0_rmse: 0.62515 | val_1_rmse: 0.63031 |  0:00:40s
epoch 1  | loss: 0.27348 | val_0_rmse: 0.543   | val_1_rmse: 0.54705 |  0:01:20s
epoch 2  | loss: 0.23124 | val_0_rmse: 0.51103 | val_1_rmse: 0.51436 |  0:01:59s
epoch 3  | loss: 0.21766 | val_0_rmse: 0.49145 | val_1_rmse: 0.49449 |  0:02:39s
epoch 4  | loss: 0.21085 | val_0_rmse: 0.46477 | val_1_rmse: 0.47869 |  0:03:19s
epoch 5  | loss: 0.20901 | val_0_rmse: 0.45145 | val_1_rmse: 0.46328 |  0:03:59s
epoch 6  | loss: 0.20329 | val_0_rmse: 0.45472 | val_1_rmse: 0.47514 |  0:04:39s
epoch 7  | loss: 0.2041  | val_0_rmse: 0.44709 | val_1_rmse: 0.462   |  0:05:18s
epoch 8  | loss: 0.20343 | val_0_rmse: 0.45457 | val_1_rmse: 0.46798 |  0:05:58s
epoch 9  | loss: 0.19874 | val_0_rmse: 0.46759 | val_1_rmse: 0.47501 |  0:06:38s
epoch 10 | loss: 0.19516 | val_0_rmse: 0.52296 | val_1_rmse: 0.45434 |  0:07:18s
epoch 11 | loss: 0.20149 | val_0_rmse: 0.4635  | val_1_rmse: 0.48021 |  0:07:59s
epoch 12 | loss: 0.2099  | val_0_rmse: 0.46979 | val_1_rmse: 0.49708 |  0:08:39s
epoch 13 | loss: 0.19723 | val_0_rmse: 0.47304 | val_1_rmse: 0.50552 |  0:09:18s
epoch 14 | loss: 0.19347 | val_0_rmse: 0.43867 | val_1_rmse: 0.45597 |  0:09:58s
epoch 15 | loss: 0.19067 | val_0_rmse: 0.45044 | val_1_rmse: 0.47191 |  0:10:37s
epoch 16 | loss: 0.18742 | val_0_rmse: 0.45483 | val_1_rmse: 0.50758 |  0:11:16s
epoch 17 | loss: 0.18891 | val_0_rmse: 0.68836 | val_1_rmse: 0.68152 |  0:11:56s
epoch 18 | loss: 0.18592 | val_0_rmse: 0.46511 | val_1_rmse: 0.46299 |  0:12:35s
epoch 19 | loss: 0.18485 | val_0_rmse: 0.4351  | val_1_rmse: 0.45639 |  0:13:14s
epoch 20 | loss: 0.18227 | val_0_rmse: 0.43281 | val_1_rmse: 0.45512 |  0:13:53s
epoch 21 | loss: 0.18262 | val_0_rmse: 0.79807 | val_1_rmse: 0.80081 |  0:14:33s
epoch 22 | loss: 0.18588 | val_0_rmse: 0.5389  | val_1_rmse: 0.45372 |  0:15:12s
epoch 23 | loss: 0.1884  | val_0_rmse: 0.46503 | val_1_rmse: 0.47584 |  0:15:51s
epoch 24 | loss: 0.18632 | val_0_rmse: 0.55014 | val_1_rmse: 0.4633  |  0:16:31s
epoch 25 | loss: 0.17929 | val_0_rmse: 0.81924 | val_1_rmse: 0.45448 |  0:17:10s
epoch 26 | loss: 0.17717 | val_0_rmse: 0.43324 | val_1_rmse: 0.45033 |  0:17:49s
epoch 27 | loss: 0.17595 | val_0_rmse: 0.42832 | val_1_rmse: 0.45219 |  0:18:28s
epoch 28 | loss: 0.17561 | val_0_rmse: 0.42534 | val_1_rmse: 0.44649 |  0:19:08s
epoch 29 | loss: 0.17573 | val_0_rmse: 0.43032 | val_1_rmse: 0.45151 |  0:19:47s
epoch 30 | loss: 0.17449 | val_0_rmse: 0.42822 | val_1_rmse: 0.45463 |  0:20:27s
epoch 31 | loss: 0.17415 | val_0_rmse: 0.43044 | val_1_rmse: 0.45379 |  0:21:06s
epoch 32 | loss: 0.17291 | val_0_rmse: 0.42225 | val_1_rmse: 0.45103 |  0:21:45s
epoch 33 | loss: 0.17396 | val_0_rmse: 2.25972 | val_1_rmse: 1.96906 |  0:22:25s
epoch 34 | loss: 0.17816 | val_0_rmse: 0.55831 | val_1_rmse: 0.52225 |  0:23:05s
epoch 35 | loss: 0.17612 | val_0_rmse: 0.43384 | val_1_rmse: 0.45629 |  0:23:45s
epoch 36 | loss: 0.17367 | val_0_rmse: 0.42712 | val_1_rmse: 0.45154 |  0:24:24s
epoch 37 | loss: 0.17167 | val_0_rmse: 0.42235 | val_1_rmse: 0.44795 |  0:25:03s
epoch 38 | loss: 0.17173 | val_0_rmse: 0.43085 | val_1_rmse: 0.4623  |  0:25:43s
epoch 39 | loss: 0.17118 | val_0_rmse: 0.42241 | val_1_rmse: 0.44736 |  0:26:22s
epoch 40 | loss: 0.16996 | val_0_rmse: 0.42462 | val_1_rmse: 0.44983 |  0:27:01s
epoch 41 | loss: 0.16971 | val_0_rmse: 0.42555 | val_1_rmse: 0.45326 |  0:27:41s
epoch 42 | loss: 0.16915 | val_0_rmse: 0.42622 | val_1_rmse: 0.45789 |  0:28:20s
epoch 43 | loss: 0.16943 | val_0_rmse: 0.4232  | val_1_rmse: 0.44546 |  0:29:00s
epoch 44 | loss: 0.16849 | val_0_rmse: 0.41959 | val_1_rmse: 0.44699 |  0:29:39s
epoch 45 | loss: 0.16727 | val_0_rmse: 0.42205 | val_1_rmse: 0.4521  |  0:30:18s
epoch 46 | loss: 0.167   | val_0_rmse: 0.4164  | val_1_rmse: 0.44569 |  0:30:58s
epoch 47 | loss: 0.16689 | val_0_rmse: 0.4222  | val_1_rmse: 0.45205 |  0:31:37s
epoch 48 | loss: 0.16596 | val_0_rmse: 0.42105 | val_1_rmse: 0.4456  |  0:32:17s
epoch 49 | loss: 0.16862 | val_0_rmse: 0.42756 | val_1_rmse: 0.4575  |  0:32:56s
epoch 50 | loss: 0.16712 | val_0_rmse: 0.42859 | val_1_rmse: 0.45776 |  0:33:35s
epoch 51 | loss: 0.16713 | val_0_rmse: 0.42856 | val_1_rmse: 0.45366 |  0:34:15s
epoch 52 | loss: 0.16577 | val_0_rmse: 0.42693 | val_1_rmse: 0.45354 |  0:34:54s
epoch 53 | loss: 0.16483 | val_0_rmse: 0.42549 | val_1_rmse: 0.44776 |  0:35:34s
epoch 54 | loss: 0.16465 | val_0_rmse: 0.42986 | val_1_rmse: 0.4559  |  0:36:13s
epoch 55 | loss: 0.16605 | val_0_rmse: 0.42005 | val_1_rmse: 0.46799 |  0:36:52s
epoch 56 | loss: 0.16797 | val_0_rmse: 0.48356 | val_1_rmse: 0.47506 |  0:37:32s
epoch 57 | loss: 0.16589 | val_0_rmse: 0.80719 | val_1_rmse: 0.76554 |  0:38:12s
epoch 58 | loss: 0.1644  | val_0_rmse: 0.42558 | val_1_rmse: 0.46068 |  0:38:51s
epoch 59 | loss: 0.16611 | val_0_rmse: 0.42069 | val_1_rmse: 0.45413 |  0:39:31s
epoch 60 | loss: 0.16583 | val_0_rmse: 0.42469 | val_1_rmse: 0.45612 |  0:40:10s
epoch 61 | loss: 0.16362 | val_0_rmse: 0.431   | val_1_rmse: 0.46437 |  0:40:49s
epoch 62 | loss: 0.16349 | val_0_rmse: 0.4221  | val_1_rmse: 0.45756 |  0:41:29s
epoch 63 | loss: 0.16342 | val_0_rmse: 0.42303 | val_1_rmse: 0.45911 |  0:42:08s
epoch 64 | loss: 0.16389 | val_0_rmse: 0.43473 | val_1_rmse: 0.47001 |  0:42:47s
epoch 65 | loss: 0.16292 | val_0_rmse: 0.42001 | val_1_rmse: 0.4499  |  0:43:27s
epoch 66 | loss: 0.1614  | val_0_rmse: 0.46335 | val_1_rmse: 0.50438 |  0:44:06s
epoch 67 | loss: 0.16352 | val_0_rmse: 0.42959 | val_1_rmse: 0.46866 |  0:44:46s
epoch 68 | loss: 0.16161 | val_0_rmse: 0.42059 | val_1_rmse: 0.4584  |  0:45:25s
epoch 69 | loss: 0.16186 | val_0_rmse: 0.42972 | val_1_rmse: 0.46306 |  0:46:05s
epoch 70 | loss: 0.16283 | val_0_rmse: 0.43934 | val_1_rmse: 0.45574 |  0:46:44s
epoch 71 | loss: 0.16174 | val_0_rmse: 0.43609 | val_1_rmse: 0.46149 |  0:47:24s
epoch 72 | loss: 0.16034 | val_0_rmse: 0.44378 | val_1_rmse: 0.46497 |  0:48:03s
epoch 73 | loss: 0.16049 | val_0_rmse: 0.43197 | val_1_rmse: 0.45837 |  0:48:42s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.44546
Best weights from best epoch are automatically used!
ended training at: 10:09:19
Feature importance:
Mean squared error is of 8723159449.033287
Mean absolute error:60854.27596271456
MAPE:0.2766888549668832
R2 score:0.8025029132462148
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:09:29
epoch 0  | loss: 0.52508 | val_0_rmse: 0.65992 | val_1_rmse: 0.66354 |  0:00:39s
epoch 1  | loss: 0.24472 | val_0_rmse: 0.55813 | val_1_rmse: 0.56062 |  0:01:19s
epoch 2  | loss: 0.22242 | val_0_rmse: 0.49076 | val_1_rmse: 0.49283 |  0:01:59s
epoch 3  | loss: 0.21313 | val_0_rmse: 0.54618 | val_1_rmse: 0.55137 |  0:02:38s
epoch 4  | loss: 0.20306 | val_0_rmse: 0.43578 | val_1_rmse: 0.44651 |  0:03:18s
epoch 5  | loss: 0.19958 | val_0_rmse: 0.44733 | val_1_rmse: 0.45403 |  0:03:58s
epoch 6  | loss: 0.19583 | val_0_rmse: 0.44768 | val_1_rmse: 0.45943 |  0:04:38s
epoch 7  | loss: 0.19465 | val_0_rmse: 0.46271 | val_1_rmse: 0.48051 |  0:05:18s
epoch 8  | loss: 0.19298 | val_0_rmse: 0.46152 | val_1_rmse: 0.47436 |  0:05:57s
epoch 9  | loss: 0.18823 | val_0_rmse: 0.44354 | val_1_rmse: 0.4568  |  0:06:36s
epoch 10 | loss: 0.18965 | val_0_rmse: 0.44577 | val_1_rmse: 0.45923 |  0:07:16s
epoch 11 | loss: 0.18558 | val_0_rmse: 0.4705  | val_1_rmse: 0.47203 |  0:07:55s
epoch 12 | loss: 0.18431 | val_0_rmse: 0.44563 | val_1_rmse: 0.46298 |  0:08:34s
epoch 13 | loss: 0.18369 | val_0_rmse: 0.45604 | val_1_rmse: 0.47455 |  0:09:13s
epoch 14 | loss: 0.18127 | val_0_rmse: 0.42903 | val_1_rmse: 0.44903 |  0:09:53s
epoch 15 | loss: 0.18289 | val_0_rmse: 0.79096 | val_1_rmse: 0.62461 |  0:10:32s
epoch 16 | loss: 0.18389 | val_0_rmse: 0.42771 | val_1_rmse: 0.53008 |  0:11:11s
epoch 17 | loss: 0.17964 | val_0_rmse: 0.42984 | val_1_rmse: 0.56256 |  0:11:51s
epoch 18 | loss: 0.17812 | val_0_rmse: 0.43165 | val_1_rmse: 0.54957 |  0:12:30s
epoch 19 | loss: 0.17767 | val_0_rmse: 0.42997 | val_1_rmse: 0.51898 |  0:13:09s
epoch 20 | loss: 0.17683 | val_0_rmse: 0.44484 | val_1_rmse: 0.46722 |  0:13:49s
epoch 21 | loss: 0.17659 | val_0_rmse: 0.43155 | val_1_rmse: 0.45221 |  0:14:28s
epoch 22 | loss: 0.17569 | val_0_rmse: 0.43574 | val_1_rmse: 0.49109 |  0:15:07s
epoch 23 | loss: 0.17535 | val_0_rmse: 0.4355  | val_1_rmse: 0.45555 |  0:15:46s
epoch 24 | loss: 0.17527 | val_0_rmse: 0.42429 | val_1_rmse: 0.51664 |  0:16:26s
epoch 25 | loss: 0.17449 | val_0_rmse: 0.44583 | val_1_rmse: 0.59489 |  0:17:05s
epoch 26 | loss: 0.17411 | val_0_rmse: 0.76294 | val_1_rmse: 0.87279 |  0:17:44s
epoch 27 | loss: 0.17298 | val_0_rmse: 0.42144 | val_1_rmse: 0.53156 |  0:18:23s
epoch 28 | loss: 0.17276 | val_0_rmse: 0.42263 | val_1_rmse: 0.57804 |  0:19:03s
epoch 29 | loss: 0.17187 | val_0_rmse: 0.43144 | val_1_rmse: 0.51548 |  0:19:43s
epoch 30 | loss: 0.17223 | val_0_rmse: 0.42241 | val_1_rmse: 0.52666 |  0:20:22s
epoch 31 | loss: 0.17199 | val_0_rmse: 0.43307 | val_1_rmse: 0.57801 |  0:21:02s
epoch 32 | loss: 0.17255 | val_0_rmse: 0.42141 | val_1_rmse: 0.47389 |  0:21:41s
epoch 33 | loss: 0.17167 | val_0_rmse: 0.43928 | val_1_rmse: 0.53018 |  0:22:20s
epoch 34 | loss: 0.17207 | val_0_rmse: 0.92482 | val_1_rmse: 0.91081 |  0:22:59s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.44651
Best weights from best epoch are automatically used!
ended training at: 10:32:52
Feature importance:
Mean squared error is of 8865072926.894522
Mean absolute error:62452.94039596384
MAPE:0.292546833994584
R2 score:0.7975908286058861
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:33:01
epoch 0  | loss: 0.58046 | val_0_rmse: 0.62004 | val_1_rmse: 0.61999 |  0:00:40s
epoch 1  | loss: 0.27269 | val_0_rmse: 0.58387 | val_1_rmse: 0.58503 |  0:01:20s
epoch 2  | loss: 0.23494 | val_0_rmse: 0.46677 | val_1_rmse: 0.47068 |  0:02:00s
epoch 3  | loss: 0.218   | val_0_rmse: 0.44487 | val_1_rmse: 0.4534  |  0:02:39s
epoch 4  | loss: 0.21022 | val_0_rmse: 0.45571 | val_1_rmse: 0.46535 |  0:03:19s
epoch 5  | loss: 0.20623 | val_0_rmse: 0.45635 | val_1_rmse: 0.4685  |  0:03:59s
epoch 6  | loss: 0.20159 | val_0_rmse: 0.44476 | val_1_rmse: 0.46028 |  0:04:39s
epoch 7  | loss: 0.19921 | val_0_rmse: 0.45986 | val_1_rmse: 0.47858 |  0:05:19s
epoch 8  | loss: 0.1958  | val_0_rmse: 0.46474 | val_1_rmse: 0.48217 |  0:05:59s
epoch 9  | loss: 0.19369 | val_0_rmse: 0.47382 | val_1_rmse: 0.49227 |  0:06:39s
epoch 10 | loss: 0.20389 | val_0_rmse: 0.50123 | val_1_rmse: 0.51024 |  0:07:18s
epoch 11 | loss: 0.20332 | val_0_rmse: 0.45504 | val_1_rmse: 0.46874 |  0:07:58s
epoch 12 | loss: 0.19564 | val_0_rmse: 0.45677 | val_1_rmse: 0.47183 |  0:08:38s
epoch 13 | loss: 0.19041 | val_0_rmse: 0.45144 | val_1_rmse: 0.46285 |  0:09:18s
epoch 14 | loss: 0.18881 | val_0_rmse: 0.77727 | val_1_rmse: 0.77579 |  0:09:58s
epoch 15 | loss: 0.18631 | val_0_rmse: 0.7324  | val_1_rmse: 0.73981 |  0:10:38s
epoch 16 | loss: 0.18537 | val_0_rmse: 0.45672 | val_1_rmse: 0.478   |  0:11:20s
epoch 17 | loss: 0.18323 | val_0_rmse: 0.43616 | val_1_rmse: 0.45892 |  0:11:59s
epoch 18 | loss: 0.17967 | val_0_rmse: 0.50279 | val_1_rmse: 0.52451 |  0:12:39s
epoch 19 | loss: 0.17815 | val_0_rmse: 0.4495  | val_1_rmse: 0.47488 |  0:13:18s
epoch 20 | loss: 0.17833 | val_0_rmse: 0.42777 | val_1_rmse: 0.45045 |  0:13:58s
epoch 21 | loss: 0.18555 | val_0_rmse: 0.43475 | val_1_rmse: 0.45586 |  0:14:37s
epoch 22 | loss: 0.17874 | val_0_rmse: 0.42857 | val_1_rmse: 0.45602 |  0:15:17s
epoch 23 | loss: 0.17571 | val_0_rmse: 0.72522 | val_1_rmse: 0.73323 |  0:15:57s
epoch 24 | loss: 0.1759  | val_0_rmse: 0.43149 | val_1_rmse: 0.45469 |  0:16:36s
epoch 25 | loss: 0.1744  | val_0_rmse: 0.4282  | val_1_rmse: 0.45246 |  0:17:16s
epoch 26 | loss: 0.17343 | val_0_rmse: 0.42306 | val_1_rmse: 0.44454 |  0:17:55s
epoch 27 | loss: 0.1745  | val_0_rmse: 0.44114 | val_1_rmse: 0.46694 |  0:18:35s
epoch 28 | loss: 0.17384 | val_0_rmse: 0.47435 | val_1_rmse: 0.4535  |  0:19:14s
epoch 29 | loss: 0.17186 | val_0_rmse: 0.43095 | val_1_rmse: 0.45746 |  0:19:54s
epoch 30 | loss: 0.17172 | val_0_rmse: 0.43598 | val_1_rmse: 0.45877 |  0:20:33s
epoch 31 | loss: 0.17132 | val_0_rmse: 0.42859 | val_1_rmse: 0.45259 |  0:21:13s
epoch 32 | loss: 0.17044 | val_0_rmse: 0.42579 | val_1_rmse: 0.4512  |  0:21:52s
epoch 33 | loss: 0.16989 | val_0_rmse: 0.46093 | val_1_rmse: 0.48228 |  0:22:32s
epoch 34 | loss: 0.16964 | val_0_rmse: 0.41865 | val_1_rmse: 0.44588 |  0:23:12s
epoch 35 | loss: 0.16927 | val_0_rmse: 0.46462 | val_1_rmse: 0.48556 |  0:23:51s
epoch 36 | loss: 0.16874 | val_0_rmse: 0.42787 | val_1_rmse: 0.46772 |  0:24:31s
epoch 37 | loss: 0.16805 | val_0_rmse: 0.44133 | val_1_rmse: 0.46641 |  0:25:10s
epoch 38 | loss: 0.16894 | val_0_rmse: 0.41962 | val_1_rmse: 0.44716 |  0:25:50s
epoch 39 | loss: 0.16762 | val_0_rmse: 0.4214  | val_1_rmse: 0.45197 |  0:26:30s
epoch 40 | loss: 0.167   | val_0_rmse: 0.4539  | val_1_rmse: 0.47656 |  0:27:09s
epoch 41 | loss: 0.16822 | val_0_rmse: 0.41464 | val_1_rmse: 0.44444 |  0:27:49s
epoch 42 | loss: 0.16597 | val_0_rmse: 0.41853 | val_1_rmse: 0.45001 |  0:28:28s
epoch 43 | loss: 0.16677 | val_0_rmse: 0.48462 | val_1_rmse: 0.45471 |  0:29:08s
epoch 44 | loss: 0.16566 | val_0_rmse: 0.62263 | val_1_rmse: 0.44632 |  0:29:47s
epoch 45 | loss: 0.16675 | val_0_rmse: 0.51735 | val_1_rmse: 0.45404 |  0:30:27s
epoch 46 | loss: 0.16569 | val_0_rmse: 0.80459 | val_1_rmse: 0.44834 |  0:31:06s
epoch 47 | loss: 0.16469 | val_0_rmse: 0.45413 | val_1_rmse: 0.44952 |  0:31:46s
epoch 48 | loss: 0.16618 | val_0_rmse: 0.50346 | val_1_rmse: 0.46671 |  0:32:25s
epoch 49 | loss: 0.1643  | val_0_rmse: 0.71484 | val_1_rmse: 0.45244 |  0:33:05s
epoch 50 | loss: 0.16603 | val_0_rmse: 0.66858 | val_1_rmse: 0.44735 |  0:33:44s
epoch 51 | loss: 0.17249 | val_0_rmse: 0.72778 | val_1_rmse: 0.66776 |  0:34:24s
epoch 52 | loss: 0.2442  | val_0_rmse: 0.45825 | val_1_rmse: 0.47753 |  0:35:03s
epoch 53 | loss: 0.18941 | val_0_rmse: 0.45055 | val_1_rmse: 0.47341 |  0:35:43s
epoch 54 | loss: 0.18477 | val_0_rmse: 0.44702 | val_1_rmse: 0.4748  |  0:36:23s
epoch 55 | loss: 0.17788 | val_0_rmse: 0.44305 | val_1_rmse: 0.47062 |  0:37:02s
epoch 56 | loss: 0.17702 | val_0_rmse: 0.45811 | val_1_rmse: 0.48424 |  0:37:42s
epoch 57 | loss: 0.17619 | val_0_rmse: 0.44388 | val_1_rmse: 0.47409 |  0:38:22s
epoch 58 | loss: 0.17363 | val_0_rmse: 0.44171 | val_1_rmse: 0.4694  |  0:39:02s
epoch 59 | loss: 0.17188 | val_0_rmse: 0.42816 | val_1_rmse: 0.45689 |  0:39:41s
epoch 60 | loss: 0.16994 | val_0_rmse: 0.42802 | val_1_rmse: 0.46084 |  0:40:21s
epoch 61 | loss: 0.16921 | val_0_rmse: 0.42691 | val_1_rmse: 0.46007 |  0:41:00s
epoch 62 | loss: 0.17488 | val_0_rmse: 0.5511  | val_1_rmse: 0.57666 |  0:41:40s
epoch 63 | loss: 0.17387 | val_0_rmse: 0.43944 | val_1_rmse: 0.47865 |  0:42:20s
epoch 64 | loss: 0.17999 | val_0_rmse: 0.65575 | val_1_rmse: 0.67921 |  0:43:00s
epoch 65 | loss: 0.17579 | val_0_rmse: 0.42919 | val_1_rmse: 0.46172 |  0:43:39s
epoch 66 | loss: 0.1717  | val_0_rmse: 0.44566 | val_1_rmse: 0.47556 |  0:44:19s
epoch 67 | loss: 0.1702  | val_0_rmse: 0.42738 | val_1_rmse: 0.4621  |  0:44:58s
epoch 68 | loss: 0.16956 | val_0_rmse: 2.11451 | val_1_rmse: 0.45698 |  0:45:38s
epoch 69 | loss: 0.16979 | val_0_rmse: 0.43538 | val_1_rmse: 0.47444 |  0:46:17s
epoch 70 | loss: 0.16732 | val_0_rmse: 0.42778 | val_1_rmse: 0.46733 |  0:46:57s
epoch 71 | loss: 0.16552 | val_0_rmse: 0.48521 | val_1_rmse: 0.52725 |  0:47:36s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.44444
Best weights from best epoch are automatically used!
ended training at: 11:21:00
Feature importance:
Mean squared error is of 8726269983.805904
Mean absolute error:60714.41660076826
MAPE:0.27828682327636317
R2 score:0.800711094857386
------------------------------------------------------------------
