TabNet Logs:

Saving copy of script...
In this script all datasets are increased in size up to the size of the biggest dataset by sampling random rows and modifying them with a 1% noiseThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:24:53
epoch 0  | loss: 0.75423 | val_0_rmse: 0.81503 | val_1_rmse: 0.82364 |  0:00:10s
epoch 1  | loss: 0.6111  | val_0_rmse: 0.76011 | val_1_rmse: 0.7637  |  0:00:17s
epoch 2  | loss: 0.57146 | val_0_rmse: 0.74273 | val_1_rmse: 0.7477  |  0:00:24s
epoch 3  | loss: 0.55488 | val_0_rmse: 0.7477  | val_1_rmse: 0.75234 |  0:00:32s
epoch 4  | loss: 0.55184 | val_0_rmse: 0.73602 | val_1_rmse: 0.74176 |  0:00:39s
epoch 5  | loss: 0.55134 | val_0_rmse: 0.7266  | val_1_rmse: 0.7278  |  0:00:47s
epoch 6  | loss: 0.5438  | val_0_rmse: 0.74027 | val_1_rmse: 0.74275 |  0:00:54s
epoch 7  | loss: 0.541   | val_0_rmse: 0.73019 | val_1_rmse: 0.73186 |  0:01:01s
epoch 8  | loss: 0.54397 | val_0_rmse: 0.72378 | val_1_rmse: 0.7278  |  0:01:08s
epoch 9  | loss: 0.5344  | val_0_rmse: 0.72522 | val_1_rmse: 0.72986 |  0:01:16s
epoch 10 | loss: 0.53852 | val_0_rmse: 0.74759 | val_1_rmse: 0.75239 |  0:01:23s
epoch 11 | loss: 0.53781 | val_0_rmse: 0.72789 | val_1_rmse: 0.73077 |  0:01:30s
epoch 12 | loss: 0.53776 | val_0_rmse: 0.74473 | val_1_rmse: 0.74592 |  0:01:37s
epoch 13 | loss: 0.54494 | val_0_rmse: 0.73686 | val_1_rmse: 0.74276 |  0:01:45s
epoch 14 | loss: 0.55002 | val_0_rmse: 0.72586 | val_1_rmse: 0.73025 |  0:01:52s
epoch 15 | loss: 0.53789 | val_0_rmse: 0.72193 | val_1_rmse: 0.72474 |  0:01:59s
epoch 16 | loss: 0.53151 | val_0_rmse: 0.71548 | val_1_rmse: 0.7198  |  0:02:06s
epoch 17 | loss: 0.52821 | val_0_rmse: 0.71668 | val_1_rmse: 0.72289 |  0:02:13s
epoch 18 | loss: 0.5311  | val_0_rmse: 0.71333 | val_1_rmse: 0.71783 |  0:02:20s
epoch 19 | loss: 0.53137 | val_0_rmse: 0.71697 | val_1_rmse: 0.72131 |  0:02:28s
epoch 20 | loss: 0.53164 | val_0_rmse: 0.73331 | val_1_rmse: 0.73443 |  0:02:35s
epoch 21 | loss: 0.52657 | val_0_rmse: 0.72249 | val_1_rmse: 0.72711 |  0:02:42s
epoch 22 | loss: 0.52489 | val_0_rmse: 0.71469 | val_1_rmse: 0.71872 |  0:02:50s
epoch 23 | loss: 0.52526 | val_0_rmse: 0.72663 | val_1_rmse: 0.73196 |  0:02:57s
epoch 24 | loss: 0.52905 | val_0_rmse: 0.70727 | val_1_rmse: 0.71314 |  0:03:04s
epoch 25 | loss: 0.51978 | val_0_rmse: 0.71829 | val_1_rmse: 0.72096 |  0:03:11s
epoch 26 | loss: 0.53018 | val_0_rmse: 0.70738 | val_1_rmse: 0.71169 |  0:03:18s
epoch 27 | loss: 0.51932 | val_0_rmse: 0.72598 | val_1_rmse: 0.7305  |  0:03:26s
epoch 28 | loss: 0.51864 | val_0_rmse: 0.71628 | val_1_rmse: 0.72474 |  0:03:33s
epoch 29 | loss: 0.52223 | val_0_rmse: 0.71219 | val_1_rmse: 0.71886 |  0:03:40s
epoch 30 | loss: 0.51867 | val_0_rmse: 0.70728 | val_1_rmse: 0.71332 |  0:03:47s
epoch 31 | loss: 0.51655 | val_0_rmse: 0.71632 | val_1_rmse: 0.72069 |  0:03:54s
epoch 32 | loss: 0.5193  | val_0_rmse: 0.71185 | val_1_rmse: 0.71777 |  0:04:02s
epoch 33 | loss: 0.51922 | val_0_rmse: 0.71863 | val_1_rmse: 0.72217 |  0:04:09s
epoch 34 | loss: 0.52201 | val_0_rmse: 0.70579 | val_1_rmse: 0.71123 |  0:04:16s
epoch 35 | loss: 0.51234 | val_0_rmse: 0.70501 | val_1_rmse: 0.70999 |  0:04:24s
epoch 36 | loss: 0.51206 | val_0_rmse: 0.71226 | val_1_rmse: 0.71921 |  0:04:31s
epoch 37 | loss: 0.51565 | val_0_rmse: 0.69848 | val_1_rmse: 0.70464 |  0:04:38s
epoch 38 | loss: 0.51376 | val_0_rmse: 0.6981  | val_1_rmse: 0.70565 |  0:04:45s
epoch 39 | loss: 0.51381 | val_0_rmse: 0.70996 | val_1_rmse: 0.7181  |  0:04:53s
epoch 40 | loss: 0.51066 | val_0_rmse: 0.69599 | val_1_rmse: 0.70072 |  0:05:00s
epoch 41 | loss: 0.51146 | val_0_rmse: 0.69878 | val_1_rmse: 0.7063  |  0:05:07s
epoch 42 | loss: 0.51657 | val_0_rmse: 0.70894 | val_1_rmse: 0.71393 |  0:05:14s
epoch 43 | loss: 0.50922 | val_0_rmse: 0.70408 | val_1_rmse: 0.71225 |  0:05:22s
epoch 44 | loss: 0.50727 | val_0_rmse: 0.70224 | val_1_rmse: 0.70966 |  0:05:29s
epoch 45 | loss: 0.51425 | val_0_rmse: 0.70625 | val_1_rmse: 0.71344 |  0:05:36s
epoch 46 | loss: 0.50964 | val_0_rmse: 0.70415 | val_1_rmse: 0.71323 |  0:05:44s
epoch 47 | loss: 0.51363 | val_0_rmse: 0.7241  | val_1_rmse: 0.73053 |  0:05:51s
epoch 48 | loss: 0.53002 | val_0_rmse: 0.71251 | val_1_rmse: 0.71917 |  0:05:58s
epoch 49 | loss: 0.51546 | val_0_rmse: 0.70619 | val_1_rmse: 0.71344 |  0:06:06s
epoch 50 | loss: 0.51369 | val_0_rmse: 0.70448 | val_1_rmse: 0.71326 |  0:06:13s
epoch 51 | loss: 0.51507 | val_0_rmse: 0.7075  | val_1_rmse: 0.71096 |  0:06:20s
epoch 52 | loss: 0.51103 | val_0_rmse: 0.69118 | val_1_rmse: 0.69926 |  0:06:28s
epoch 53 | loss: 0.50661 | val_0_rmse: 0.7082  | val_1_rmse: 0.71584 |  0:06:35s
epoch 54 | loss: 0.51037 | val_0_rmse: 0.69188 | val_1_rmse: 0.70066 |  0:06:43s
epoch 55 | loss: 0.50791 | val_0_rmse: 0.69707 | val_1_rmse: 0.70567 |  0:06:50s
epoch 56 | loss: 0.50408 | val_0_rmse: 0.69849 | val_1_rmse: 0.70513 |  0:06:57s
epoch 57 | loss: 0.50412 | val_0_rmse: 0.69846 | val_1_rmse: 0.70888 |  0:07:04s
epoch 58 | loss: 0.50598 | val_0_rmse: 0.6899  | val_1_rmse: 0.69979 |  0:07:12s
epoch 59 | loss: 0.50567 | val_0_rmse: 0.70256 | val_1_rmse: 0.71007 |  0:07:19s
epoch 60 | loss: 0.50466 | val_0_rmse: 0.71408 | val_1_rmse: 0.72451 |  0:07:26s
epoch 61 | loss: 0.50066 | val_0_rmse: 0.69669 | val_1_rmse: 0.70589 |  0:07:34s
epoch 62 | loss: 0.50106 | val_0_rmse: 0.71148 | val_1_rmse: 0.72102 |  0:07:41s
epoch 63 | loss: 0.50242 | val_0_rmse: 0.69977 | val_1_rmse: 0.71031 |  0:07:48s
epoch 64 | loss: 0.50152 | val_0_rmse: 0.68935 | val_1_rmse: 0.70108 |  0:07:56s
epoch 65 | loss: 0.49959 | val_0_rmse: 0.6911  | val_1_rmse: 0.70103 |  0:08:03s
epoch 66 | loss: 0.49469 | val_0_rmse: 0.68034 | val_1_rmse: 0.69281 |  0:08:11s
epoch 67 | loss: 0.49628 | val_0_rmse: 0.68095 | val_1_rmse: 0.69514 |  0:08:18s
epoch 68 | loss: 0.49421 | val_0_rmse: 0.67933 | val_1_rmse: 0.69158 |  0:08:25s
epoch 69 | loss: 0.48998 | val_0_rmse: 0.68446 | val_1_rmse: 0.69654 |  0:08:32s
epoch 70 | loss: 0.49113 | val_0_rmse: 0.66579 | val_1_rmse: 0.67947 |  0:08:38s
epoch 71 | loss: 0.48601 | val_0_rmse: 0.66416 | val_1_rmse: 0.67548 |  0:08:45s
epoch 72 | loss: 0.47999 | val_0_rmse: 0.68352 | val_1_rmse: 0.69502 |  0:08:51s
epoch 73 | loss: 0.4836  | val_0_rmse: 0.66446 | val_1_rmse: 0.67588 |  0:08:57s
epoch 74 | loss: 0.47218 | val_0_rmse: 0.66043 | val_1_rmse: 0.67269 |  0:09:04s
epoch 75 | loss: 0.4773  | val_0_rmse: 0.67344 | val_1_rmse: 0.68939 |  0:09:10s
epoch 76 | loss: 0.47433 | val_0_rmse: 0.69175 | val_1_rmse: 0.70311 |  0:09:16s
epoch 77 | loss: 0.47224 | val_0_rmse: 0.69876 | val_1_rmse: 0.7143  |  0:09:23s
epoch 78 | loss: 0.46693 | val_0_rmse: 0.67583 | val_1_rmse: 0.69313 |  0:09:29s
epoch 79 | loss: 0.4719  | val_0_rmse: 0.67526 | val_1_rmse: 0.69364 |  0:09:35s
epoch 80 | loss: 0.46595 | val_0_rmse: 0.66765 | val_1_rmse: 0.68236 |  0:09:42s
epoch 81 | loss: 0.46682 | val_0_rmse: 0.6654  | val_1_rmse: 0.68018 |  0:09:48s
epoch 82 | loss: 0.46366 | val_0_rmse: 0.69811 | val_1_rmse: 0.70764 |  0:09:54s
epoch 83 | loss: 0.46216 | val_0_rmse: 0.67152 | val_1_rmse: 0.68396 |  0:10:01s
epoch 84 | loss: 0.46031 | val_0_rmse: 0.69963 | val_1_rmse: 0.70812 |  0:10:07s
epoch 85 | loss: 0.46127 | val_0_rmse: 0.66211 | val_1_rmse: 0.68096 |  0:10:13s
epoch 86 | loss: 0.457   | val_0_rmse: 0.67459 | val_1_rmse: 0.69305 |  0:10:20s
epoch 87 | loss: 0.4553  | val_0_rmse: 0.66964 | val_1_rmse: 0.68405 |  0:10:26s
epoch 88 | loss: 0.45588 | val_0_rmse: 0.64602 | val_1_rmse: 0.66396 |  0:10:33s
epoch 89 | loss: 0.45382 | val_0_rmse: 0.64222 | val_1_rmse: 0.65769 |  0:10:39s
epoch 90 | loss: 0.44796 | val_0_rmse: 0.68231 | val_1_rmse: 0.69884 |  0:10:45s
epoch 91 | loss: 0.45203 | val_0_rmse: 0.66846 | val_1_rmse: 0.6783  |  0:10:51s
epoch 92 | loss: 0.44723 | val_0_rmse: 0.67648 | val_1_rmse: 0.6942  |  0:10:58s
epoch 93 | loss: 0.43783 | val_0_rmse: 0.63958 | val_1_rmse: 0.65852 |  0:11:04s
epoch 94 | loss: 0.44127 | val_0_rmse: 0.61516 | val_1_rmse: 0.63478 |  0:11:10s
epoch 95 | loss: 0.43137 | val_0_rmse: 0.61495 | val_1_rmse: 0.62872 |  0:11:17s
epoch 96 | loss: 0.42402 | val_0_rmse: 0.63614 | val_1_rmse: 0.65229 |  0:11:23s
epoch 97 | loss: 0.42581 | val_0_rmse: 0.63857 | val_1_rmse: 0.65541 |  0:11:29s
epoch 98 | loss: 0.42498 | val_0_rmse: 0.6147  | val_1_rmse: 0.63248 |  0:11:36s
epoch 99 | loss: 0.41946 | val_0_rmse: 0.61815 | val_1_rmse: 0.63533 |  0:11:42s
epoch 100| loss: 0.42035 | val_0_rmse: 0.60082 | val_1_rmse: 0.61703 |  0:11:48s
epoch 101| loss: 0.41443 | val_0_rmse: 0.63928 | val_1_rmse: 0.65292 |  0:11:55s
epoch 102| loss: 0.41218 | val_0_rmse: 0.5981  | val_1_rmse: 0.61418 |  0:12:01s
epoch 103| loss: 0.41186 | val_0_rmse: 0.60413 | val_1_rmse: 0.61984 |  0:12:08s
epoch 104| loss: 0.41178 | val_0_rmse: 0.61131 | val_1_rmse: 0.62708 |  0:12:14s
epoch 105| loss: 0.4118  | val_0_rmse: 0.61657 | val_1_rmse: 0.62724 |  0:12:20s
epoch 106| loss: 0.39953 | val_0_rmse: 0.6153  | val_1_rmse: 0.63161 |  0:12:26s
epoch 107| loss: 0.4092  | val_0_rmse: 0.60297 | val_1_rmse: 0.61638 |  0:12:33s
epoch 108| loss: 0.40245 | val_0_rmse: 0.60681 | val_1_rmse: 0.62272 |  0:12:39s
epoch 109| loss: 0.40118 | val_0_rmse: 0.59881 | val_1_rmse: 0.6157  |  0:12:45s
epoch 110| loss: 0.40113 | val_0_rmse: 0.6167  | val_1_rmse: 0.63868 |  0:12:52s
epoch 111| loss: 0.40025 | val_0_rmse: 0.59376 | val_1_rmse: 0.60612 |  0:12:58s
epoch 112| loss: 0.39651 | val_0_rmse: 0.62889 | val_1_rmse: 0.64579 |  0:13:04s
epoch 113| loss: 0.39441 | val_0_rmse: 0.65394 | val_1_rmse: 0.66826 |  0:13:11s
epoch 114| loss: 0.39571 | val_0_rmse: 0.62911 | val_1_rmse: 0.6441  |  0:13:17s
epoch 115| loss: 0.39243 | val_0_rmse: 0.61123 | val_1_rmse: 0.62711 |  0:13:23s
epoch 116| loss: 0.39507 | val_0_rmse: 0.60696 | val_1_rmse: 0.62587 |  0:13:30s
epoch 117| loss: 0.39045 | val_0_rmse: 0.61542 | val_1_rmse: 0.63345 |  0:13:36s
epoch 118| loss: 0.39246 | val_0_rmse: 0.60634 | val_1_rmse: 0.61868 |  0:13:42s
epoch 119| loss: 0.39192 | val_0_rmse: 0.61445 | val_1_rmse: 0.62943 |  0:13:48s
epoch 120| loss: 0.38893 | val_0_rmse: 0.63664 | val_1_rmse: 0.65569 |  0:13:55s
epoch 121| loss: 0.38908 | val_0_rmse: 0.66349 | val_1_rmse: 0.67816 |  0:14:01s
epoch 122| loss: 0.39071 | val_0_rmse: 0.60139 | val_1_rmse: 0.62014 |  0:14:08s
epoch 123| loss: 0.38242 | val_0_rmse: 0.59014 | val_1_rmse: 0.60981 |  0:14:14s
epoch 124| loss: 0.38601 | val_0_rmse: 0.58353 | val_1_rmse: 0.60277 |  0:14:20s
epoch 125| loss: 0.38916 | val_0_rmse: 0.59128 | val_1_rmse: 0.61098 |  0:14:26s
epoch 126| loss: 0.37738 | val_0_rmse: 0.63913 | val_1_rmse: 0.65652 |  0:14:33s
epoch 127| loss: 0.37857 | val_0_rmse: 0.58348 | val_1_rmse: 0.60084 |  0:14:39s
epoch 128| loss: 0.38117 | val_0_rmse: 0.65241 | val_1_rmse: 0.6688  |  0:14:45s
epoch 129| loss: 0.38548 | val_0_rmse: 0.59721 | val_1_rmse: 0.61235 |  0:14:51s
epoch 130| loss: 0.37656 | val_0_rmse: 0.58509 | val_1_rmse: 0.60122 |  0:14:58s
epoch 131| loss: 0.37643 | val_0_rmse: 0.59122 | val_1_rmse: 0.60935 |  0:15:04s
epoch 132| loss: 0.37903 | val_0_rmse: 0.59571 | val_1_rmse: 0.61355 |  0:15:10s
epoch 133| loss: 0.37911 | val_0_rmse: 0.5854  | val_1_rmse: 0.60046 |  0:15:17s
epoch 134| loss: 0.37974 | val_0_rmse: 0.58983 | val_1_rmse: 0.60869 |  0:15:23s
epoch 135| loss: 0.37686 | val_0_rmse: 0.56476 | val_1_rmse: 0.58299 |  0:15:29s
epoch 136| loss: 0.3755  | val_0_rmse: 0.595   | val_1_rmse: 0.61462 |  0:15:35s
epoch 137| loss: 0.38228 | val_0_rmse: 0.59579 | val_1_rmse: 0.6164  |  0:15:42s
epoch 138| loss: 0.37521 | val_0_rmse: 0.60825 | val_1_rmse: 0.62635 |  0:15:49s
epoch 139| loss: 0.37816 | val_0_rmse: 0.60279 | val_1_rmse: 0.61763 |  0:15:55s
epoch 140| loss: 0.37939 | val_0_rmse: 0.61291 | val_1_rmse: 0.63047 |  0:16:01s
epoch 141| loss: 0.37408 | val_0_rmse: 0.57697 | val_1_rmse: 0.597   |  0:16:08s
epoch 142| loss: 0.38616 | val_0_rmse: 0.58144 | val_1_rmse: 0.5975  |  0:16:14s
epoch 143| loss: 0.37613 | val_0_rmse: 0.59538 | val_1_rmse: 0.61713 |  0:16:20s
epoch 144| loss: 0.37544 | val_0_rmse: 0.62287 | val_1_rmse: 0.63708 |  0:16:27s
epoch 145| loss: 0.37517 | val_0_rmse: 0.6068  | val_1_rmse: 0.62592 |  0:16:33s
epoch 146| loss: 0.37712 | val_0_rmse: 0.5695  | val_1_rmse: 0.5886  |  0:16:39s
epoch 147| loss: 0.37116 | val_0_rmse: 0.58157 | val_1_rmse: 0.60358 |  0:16:45s
epoch 148| loss: 0.37103 | val_0_rmse: 0.59978 | val_1_rmse: 0.61926 |  0:16:52s
epoch 149| loss: 0.37049 | val_0_rmse: 0.59158 | val_1_rmse: 0.60997 |  0:16:58s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.58299
Best weights from best epoch are automatically used!
ended training at: 05:41:54
Feature importance:
[('Area', 0.38570173322610146), ('Baths', 0.1717494254850921), ('Beds', 0.0), ('Latitude', 0.12912374219763037), ('Longitude', 0.0), ('Month', 0.011667406742118806), ('Year', 0.30175769234905725)]
Mean squared error is of 7704425585.7757635
Mean absolute error:62304.73317979952
MAPE:0.19945702335529383
R2 score:0.657446535746274
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:41:55
epoch 0  | loss: 0.57618 | val_0_rmse: 0.69794 | val_1_rmse: 0.70147 |  0:00:06s
epoch 1  | loss: 0.49178 | val_0_rmse: 0.68976 | val_1_rmse: 0.69267 |  0:00:12s
epoch 2  | loss: 0.46533 | val_0_rmse: 0.67037 | val_1_rmse: 0.67018 |  0:00:18s
epoch 3  | loss: 0.44255 | val_0_rmse: 0.65416 | val_1_rmse: 0.65694 |  0:00:25s
epoch 4  | loss: 0.42772 | val_0_rmse: 0.67014 | val_1_rmse: 0.67304 |  0:00:31s
epoch 5  | loss: 0.40216 | val_0_rmse: 0.68084 | val_1_rmse: 0.68601 |  0:00:37s
epoch 6  | loss: 0.3985  | val_0_rmse: 0.67954 | val_1_rmse: 0.68394 |  0:00:44s
epoch 7  | loss: 0.3915  | val_0_rmse: 0.70072 | val_1_rmse: 0.70538 |  0:00:50s
epoch 8  | loss: 0.37995 | val_0_rmse: 0.65796 | val_1_rmse: 0.66024 |  0:00:56s
epoch 9  | loss: 0.37852 | val_0_rmse: 0.65111 | val_1_rmse: 0.65289 |  0:01:02s
epoch 10 | loss: 0.36877 | val_0_rmse: 0.68719 | val_1_rmse: 0.68829 |  0:01:09s
epoch 11 | loss: 0.38067 | val_0_rmse: 0.6748  | val_1_rmse: 0.68018 |  0:01:15s
epoch 12 | loss: 0.3706  | val_0_rmse: 0.7314  | val_1_rmse: 0.73304 |  0:01:21s
epoch 13 | loss: 0.35935 | val_0_rmse: 0.77231 | val_1_rmse: 0.77297 |  0:01:28s
epoch 14 | loss: 0.35464 | val_0_rmse: 0.73997 | val_1_rmse: 0.74032 |  0:01:34s
epoch 15 | loss: 0.3524  | val_0_rmse: 0.72379 | val_1_rmse: 0.72532 |  0:01:40s
epoch 16 | loss: 0.34937 | val_0_rmse: 0.76953 | val_1_rmse: 0.76759 |  0:01:47s
epoch 17 | loss: 0.34748 | val_0_rmse: 0.73537 | val_1_rmse: 0.73566 |  0:01:53s
epoch 18 | loss: 0.36013 | val_0_rmse: 0.70132 | val_1_rmse: 0.70473 |  0:01:59s
epoch 19 | loss: 0.35994 | val_0_rmse: 0.73642 | val_1_rmse: 0.73972 |  0:02:05s
epoch 20 | loss: 0.35124 | val_0_rmse: 0.73206 | val_1_rmse: 0.73624 |  0:02:12s
epoch 21 | loss: 0.35083 | val_0_rmse: 0.72547 | val_1_rmse: 0.72921 |  0:02:18s
epoch 22 | loss: 0.3459  | val_0_rmse: 0.739   | val_1_rmse: 0.74264 |  0:02:24s
epoch 23 | loss: 0.34446 | val_0_rmse: 0.73075 | val_1_rmse: 0.73526 |  0:02:31s
epoch 24 | loss: 0.33951 | val_0_rmse: 0.75243 | val_1_rmse: 0.75579 |  0:02:37s
epoch 25 | loss: 0.34164 | val_0_rmse: 0.7819  | val_1_rmse: 0.78544 |  0:02:43s
epoch 26 | loss: 0.34576 | val_0_rmse: 0.76724 | val_1_rmse: 0.77106 |  0:02:50s
epoch 27 | loss: 0.35944 | val_0_rmse: 0.74076 | val_1_rmse: 0.74506 |  0:02:56s
epoch 28 | loss: 0.35224 | val_0_rmse: 0.76285 | val_1_rmse: 0.76769 |  0:03:02s
epoch 29 | loss: 0.3426  | val_0_rmse: 0.83127 | val_1_rmse: 0.83675 |  0:03:09s
epoch 30 | loss: 0.34198 | val_0_rmse: 0.72187 | val_1_rmse: 0.7256  |  0:03:15s
epoch 31 | loss: 0.33424 | val_0_rmse: 0.76146 | val_1_rmse: 0.7669  |  0:03:21s
epoch 32 | loss: 0.33329 | val_0_rmse: 0.78307 | val_1_rmse: 0.78754 |  0:03:28s
epoch 33 | loss: 0.33508 | val_0_rmse: 0.82217 | val_1_rmse: 0.82671 |  0:03:34s
epoch 34 | loss: 0.33189 | val_0_rmse: 0.81015 | val_1_rmse: 0.8149  |  0:03:41s
epoch 35 | loss: 0.33938 | val_0_rmse: 0.80828 | val_1_rmse: 0.81411 |  0:03:47s
epoch 36 | loss: 0.33348 | val_0_rmse: 0.80498 | val_1_rmse: 0.81014 |  0:03:53s
epoch 37 | loss: 0.33338 | val_0_rmse: 0.73956 | val_1_rmse: 0.744   |  0:04:00s
epoch 38 | loss: 0.33726 | val_0_rmse: 0.75747 | val_1_rmse: 0.75773 |  0:04:06s
epoch 39 | loss: 0.33652 | val_0_rmse: 0.76173 | val_1_rmse: 0.7656  |  0:04:12s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.65289
Best weights from best epoch are automatically used!
ended training at: 05:46:10
Feature importance:
[('Area', 0.2182192066542199), ('Baths', 0.325587355829818), ('Beds', 0.06820977462458905), ('Latitude', 0.3708456266331707), ('Longitude', 0.0), ('Month', 0.010479016456565857), ('Year', 0.0066590198016364425)]
Mean squared error is of 2812904859.474263
Mean absolute error:38861.423486617634
MAPE:0.3873116454272003
R2 score:0.5688363360735665
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:18
epoch 0  | loss: 0.45829 | val_0_rmse: 0.59242 | val_1_rmse: 0.59976 |  0:00:06s
epoch 1  | loss: 0.33205 | val_0_rmse: 0.5715  | val_1_rmse: 0.57668 |  0:00:12s
epoch 2  | loss: 0.31474 | val_0_rmse: 0.5573  | val_1_rmse: 0.56384 |  0:00:18s
epoch 3  | loss: 0.30886 | val_0_rmse: 0.58971 | val_1_rmse: 0.59418 |  0:00:24s
epoch 4  | loss: 0.30035 | val_0_rmse: 0.60072 | val_1_rmse: 0.60649 |  0:00:31s
epoch 5  | loss: 0.29282 | val_0_rmse: 0.55534 | val_1_rmse: 0.56072 |  0:00:37s
epoch 6  | loss: 0.29055 | val_0_rmse: 0.56659 | val_1_rmse: 0.57278 |  0:00:43s
epoch 7  | loss: 0.29255 | val_0_rmse: 0.52799 | val_1_rmse: 0.53574 |  0:00:50s
epoch 8  | loss: 0.28826 | val_0_rmse: 0.61013 | val_1_rmse: 0.61367 |  0:00:56s
epoch 9  | loss: 0.28435 | val_0_rmse: 0.59908 | val_1_rmse: 0.6038  |  0:01:02s
epoch 10 | loss: 0.28241 | val_0_rmse: 0.55923 | val_1_rmse: 0.56437 |  0:01:09s
epoch 11 | loss: 0.28078 | val_0_rmse: 0.51685 | val_1_rmse: 0.5235  |  0:01:15s
epoch 12 | loss: 0.27734 | val_0_rmse: 0.55717 | val_1_rmse: 0.56292 |  0:01:21s
epoch 13 | loss: 0.27639 | val_0_rmse: 0.53854 | val_1_rmse: 0.54527 |  0:01:28s
epoch 14 | loss: 0.27737 | val_0_rmse: 0.60239 | val_1_rmse: 0.60684 |  0:01:34s
epoch 15 | loss: 0.27363 | val_0_rmse: 0.54115 | val_1_rmse: 0.54623 |  0:01:40s
epoch 16 | loss: 0.27544 | val_0_rmse: 0.5668  | val_1_rmse: 0.57638 |  0:01:47s
epoch 17 | loss: 0.27317 | val_0_rmse: 0.54432 | val_1_rmse: 0.54611 |  0:01:53s
epoch 18 | loss: 0.27249 | val_0_rmse: 0.5328  | val_1_rmse: 0.53838 |  0:01:59s
epoch 19 | loss: 0.27471 | val_0_rmse: 0.56713 | val_1_rmse: 0.5702  |  0:02:06s
epoch 20 | loss: 0.27397 | val_0_rmse: 0.55243 | val_1_rmse: 0.55998 |  0:02:12s
epoch 21 | loss: 0.27128 | val_0_rmse: 0.57382 | val_1_rmse: 0.58176 |  0:02:18s
epoch 22 | loss: 0.27183 | val_0_rmse: 0.64436 | val_1_rmse: 0.64574 |  0:02:24s
epoch 23 | loss: 0.27309 | val_0_rmse: 0.58486 | val_1_rmse: 0.59152 |  0:02:31s
epoch 24 | loss: 0.27168 | val_0_rmse: 0.57262 | val_1_rmse: 0.57808 |  0:02:37s
epoch 25 | loss: 0.27051 | val_0_rmse: 0.57957 | val_1_rmse: 0.59024 |  0:02:44s
epoch 26 | loss: 0.26785 | val_0_rmse: 0.53262 | val_1_rmse: 0.53803 |  0:02:50s
epoch 27 | loss: 0.2682  | val_0_rmse: 0.5473  | val_1_rmse: 0.55266 |  0:02:56s
epoch 28 | loss: 0.26905 | val_0_rmse: 0.61227 | val_1_rmse: 0.61816 |  0:03:02s
epoch 29 | loss: 0.26858 | val_0_rmse: 0.51691 | val_1_rmse: 0.52513 |  0:03:09s
epoch 30 | loss: 0.27055 | val_0_rmse: 0.52092 | val_1_rmse: 0.52863 |  0:03:15s
epoch 31 | loss: 0.26799 | val_0_rmse: 0.59056 | val_1_rmse: 0.59624 |  0:03:21s
epoch 32 | loss: 0.26745 | val_0_rmse: 0.52787 | val_1_rmse: 0.53546 |  0:03:28s
epoch 33 | loss: 0.2647  | val_0_rmse: 0.58993 | val_1_rmse: 0.59856 |  0:03:35s
epoch 34 | loss: 0.26424 | val_0_rmse: 0.7612  | val_1_rmse: 0.76621 |  0:03:41s
epoch 35 | loss: 0.26431 | val_0_rmse: 0.56246 | val_1_rmse: 0.57289 |  0:03:47s
epoch 36 | loss: 0.26504 | val_0_rmse: 0.62224 | val_1_rmse: 0.62814 |  0:03:53s
epoch 37 | loss: 0.26335 | val_0_rmse: 0.57804 | val_1_rmse: 0.58534 |  0:04:00s
epoch 38 | loss: 0.2643  | val_0_rmse: 0.55086 | val_1_rmse: 0.55952 |  0:04:06s
epoch 39 | loss: 0.26116 | val_0_rmse: 0.53589 | val_1_rmse: 0.54352 |  0:04:13s
epoch 40 | loss: 0.26446 | val_0_rmse: 0.56053 | val_1_rmse: 0.56977 |  0:04:19s
epoch 41 | loss: 0.26718 | val_0_rmse: 0.51539 | val_1_rmse: 0.52272 |  0:04:25s
epoch 42 | loss: 0.26375 | val_0_rmse: 0.53633 | val_1_rmse: 0.54542 |  0:04:32s
epoch 43 | loss: 0.26833 | val_0_rmse: 0.55967 | val_1_rmse: 0.56453 |  0:04:38s
epoch 44 | loss: 0.26615 | val_0_rmse: 0.55658 | val_1_rmse: 0.56521 |  0:04:44s
epoch 45 | loss: 0.26653 | val_0_rmse: 0.52212 | val_1_rmse: 0.53085 |  0:04:51s
epoch 46 | loss: 0.26094 | val_0_rmse: 0.60483 | val_1_rmse: 0.61307 |  0:04:57s
epoch 47 | loss: 0.26261 | val_0_rmse: 0.51254 | val_1_rmse: 0.52174 |  0:05:03s
epoch 48 | loss: 0.26171 | val_0_rmse: 0.74258 | val_1_rmse: 0.75131 |  0:05:10s
epoch 49 | loss: 0.26118 | val_0_rmse: 0.60272 | val_1_rmse: 0.61162 |  0:05:16s
epoch 50 | loss: 0.26064 | val_0_rmse: 0.63147 | val_1_rmse: 0.64002 |  0:05:23s
epoch 51 | loss: 0.26277 | val_0_rmse: 0.57452 | val_1_rmse: 0.58125 |  0:05:29s
epoch 52 | loss: 0.26516 | val_0_rmse: 0.58491 | val_1_rmse: 0.59    |  0:05:35s
epoch 53 | loss: 0.26199 | val_0_rmse: 0.5541  | val_1_rmse: 0.5641  |  0:05:42s
epoch 54 | loss: 0.26327 | val_0_rmse: 0.58421 | val_1_rmse: 0.59261 |  0:05:48s
epoch 55 | loss: 0.2598  | val_0_rmse: 0.56376 | val_1_rmse: 0.57546 |  0:05:54s
epoch 56 | loss: 0.25876 | val_0_rmse: 0.52633 | val_1_rmse: 0.53169 |  0:06:01s
epoch 57 | loss: 0.26124 | val_0_rmse: 0.51951 | val_1_rmse: 0.52872 |  0:06:07s
epoch 58 | loss: 0.25724 | val_0_rmse: 0.51358 | val_1_rmse: 0.52406 |  0:06:14s
epoch 59 | loss: 0.25706 | val_0_rmse: 0.54816 | val_1_rmse: 0.56143 |  0:06:20s
epoch 60 | loss: 0.25987 | val_0_rmse: 0.58926 | val_1_rmse: 0.60128 |  0:06:26s
epoch 61 | loss: 0.26082 | val_0_rmse: 0.58655 | val_1_rmse: 0.59252 |  0:06:33s
epoch 62 | loss: 0.26106 | val_0_rmse: 0.56993 | val_1_rmse: 0.5764  |  0:06:39s
epoch 63 | loss: 0.25942 | val_0_rmse: 0.55973 | val_1_rmse: 0.56639 |  0:06:46s
epoch 64 | loss: 0.25792 | val_0_rmse: 0.53654 | val_1_rmse: 0.54596 |  0:06:52s
epoch 65 | loss: 0.26147 | val_0_rmse: 0.55114 | val_1_rmse: 0.55844 |  0:06:58s
epoch 66 | loss: 0.26286 | val_0_rmse: 0.5557  | val_1_rmse: 0.56242 |  0:07:05s
epoch 67 | loss: 0.25825 | val_0_rmse: 0.60061 | val_1_rmse: 0.60994 |  0:07:11s
epoch 68 | loss: 0.25818 | val_0_rmse: 0.53366 | val_1_rmse: 0.54423 |  0:07:17s
epoch 69 | loss: 0.25626 | val_0_rmse: 0.51075 | val_1_rmse: 0.52267 |  0:07:24s
epoch 70 | loss: 0.26147 | val_0_rmse: 0.53702 | val_1_rmse: 0.54498 |  0:07:30s
epoch 71 | loss: 0.25622 | val_0_rmse: 0.51601 | val_1_rmse: 0.52652 |  0:07:36s
epoch 72 | loss: 0.25455 | val_0_rmse: 0.55717 | val_1_rmse: 0.57098 |  0:07:43s
epoch 73 | loss: 0.25577 | val_0_rmse: 0.54953 | val_1_rmse: 0.5588  |  0:07:49s
epoch 74 | loss: 0.25635 | val_0_rmse: 0.51415 | val_1_rmse: 0.52276 |  0:07:55s
epoch 75 | loss: 0.26    | val_0_rmse: 0.58359 | val_1_rmse: 0.59183 |  0:08:02s
epoch 76 | loss: 0.25451 | val_0_rmse: 0.55518 | val_1_rmse: 0.56791 |  0:08:08s
epoch 77 | loss: 0.25579 | val_0_rmse: 0.50352 | val_1_rmse: 0.517   |  0:08:14s
epoch 78 | loss: 0.25597 | val_0_rmse: 0.58338 | val_1_rmse: 0.59166 |  0:08:21s
epoch 79 | loss: 0.25617 | val_0_rmse: 0.55706 | val_1_rmse: 0.56842 |  0:08:27s
epoch 80 | loss: 0.256   | val_0_rmse: 0.61334 | val_1_rmse: 0.62133 |  0:08:34s
epoch 81 | loss: 0.2576  | val_0_rmse: 0.50458 | val_1_rmse: 0.51721 |  0:08:40s
epoch 82 | loss: 0.25657 | val_0_rmse: 0.57011 | val_1_rmse: 0.57836 |  0:08:46s
epoch 83 | loss: 0.25496 | val_0_rmse: 0.55219 | val_1_rmse: 0.56367 |  0:08:53s
epoch 84 | loss: 0.25765 | val_0_rmse: 0.57803 | val_1_rmse: 0.59103 |  0:08:59s
epoch 85 | loss: 0.25369 | val_0_rmse: 0.57267 | val_1_rmse: 0.58144 |  0:09:05s
epoch 86 | loss: 0.2558  | val_0_rmse: 0.50757 | val_1_rmse: 0.51999 |  0:09:12s
epoch 87 | loss: 0.25303 | val_0_rmse: 0.56117 | val_1_rmse: 0.57531 |  0:09:18s
epoch 88 | loss: 0.25991 | val_0_rmse: 0.53539 | val_1_rmse: 0.54353 |  0:09:25s
epoch 89 | loss: 0.26079 | val_0_rmse: 0.55318 | val_1_rmse: 0.5605  |  0:09:31s
epoch 90 | loss: 0.25569 | val_0_rmse: 0.51576 | val_1_rmse: 0.52571 |  0:09:37s
epoch 91 | loss: 0.25468 | val_0_rmse: 0.53703 | val_1_rmse: 0.55022 |  0:09:44s
epoch 92 | loss: 0.26326 | val_0_rmse: 0.57968 | val_1_rmse: 0.58557 |  0:09:50s
epoch 93 | loss: 0.25934 | val_0_rmse: 0.512   | val_1_rmse: 0.52479 |  0:09:56s
epoch 94 | loss: 0.25779 | val_0_rmse: 0.57896 | val_1_rmse: 0.58712 |  0:10:02s
epoch 95 | loss: 0.25557 | val_0_rmse: 0.60321 | val_1_rmse: 0.61563 |  0:10:09s
epoch 96 | loss: 0.25531 | val_0_rmse: 0.55518 | val_1_rmse: 0.56474 |  0:10:15s
epoch 97 | loss: 0.27826 | val_0_rmse: 0.59181 | val_1_rmse: 0.59927 |  0:10:21s
epoch 98 | loss: 0.26445 | val_0_rmse: 0.61085 | val_1_rmse: 0.61938 |  0:10:28s
epoch 99 | loss: 0.26338 | val_0_rmse: 0.60469 | val_1_rmse: 0.61306 |  0:10:34s
epoch 100| loss: 0.26067 | val_0_rmse: 0.54759 | val_1_rmse: 0.55722 |  0:10:40s
epoch 101| loss: 0.26312 | val_0_rmse: 0.54832 | val_1_rmse: 0.55576 |  0:10:47s
epoch 102| loss: 0.26184 | val_0_rmse: 0.59502 | val_1_rmse: 0.59977 |  0:10:53s
epoch 103| loss: 0.26217 | val_0_rmse: 0.55786 | val_1_rmse: 0.56717 |  0:10:59s
epoch 104| loss: 0.26103 | val_0_rmse: 0.56063 | val_1_rmse: 0.57328 |  0:11:06s
epoch 105| loss: 0.25947 | val_0_rmse: 0.58533 | val_1_rmse: 0.59503 |  0:11:12s
epoch 106| loss: 0.2633  | val_0_rmse: 0.58914 | val_1_rmse: 0.5957  |  0:11:19s
epoch 107| loss: 0.26182 | val_0_rmse: 0.68192 | val_1_rmse: 0.68722 |  0:11:25s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.517
Best weights from best epoch are automatically used!
ended training at: 06:03:45
Feature importance:
[('Area', 0.24218275404008963), ('Baths', 0.14826064642235923), ('Beds', 0.10259344890057678), ('Latitude', 0.24705942430025352), ('Longitude', 0.18963084929971494), ('Month', 0.07027287703700592), ('Year', 0.0)]
Mean squared error is of 1051541158.1280103
Mean absolute error:22014.27717012407
MAPE:0.2755378260882099
R2 score:0.736238813534738
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:07:30
epoch 0  | loss: 0.64217 | val_0_rmse: 0.75559 | val_1_rmse: 0.75431 |  0:00:06s
epoch 1  | loss: 0.53393 | val_0_rmse: 0.67908 | val_1_rmse: 0.67874 |  0:00:12s
epoch 2  | loss: 0.44385 | val_0_rmse: 0.64465 | val_1_rmse: 0.64323 |  0:00:18s
epoch 3  | loss: 0.38915 | val_0_rmse: 0.63807 | val_1_rmse: 0.63921 |  0:00:25s
epoch 4  | loss: 0.35079 | val_0_rmse: 0.60041 | val_1_rmse: 0.59849 |  0:00:31s
epoch 5  | loss: 0.33038 | val_0_rmse: 0.6     | val_1_rmse: 0.60291 |  0:00:38s
epoch 6  | loss: 0.32347 | val_0_rmse: 0.55394 | val_1_rmse: 0.55841 |  0:00:44s
epoch 7  | loss: 0.32378 | val_0_rmse: 0.56861 | val_1_rmse: 0.57638 |  0:00:50s
epoch 8  | loss: 0.32112 | val_0_rmse: 0.56444 | val_1_rmse: 0.5661  |  0:00:56s
epoch 9  | loss: 0.31175 | val_0_rmse: 0.58813 | val_1_rmse: 0.58624 |  0:01:03s
epoch 10 | loss: 0.3182  | val_0_rmse: 0.54512 | val_1_rmse: 0.54605 |  0:01:09s
epoch 11 | loss: 0.30345 | val_0_rmse: 0.53328 | val_1_rmse: 0.53665 |  0:01:15s
epoch 12 | loss: 0.30604 | val_0_rmse: 0.54962 | val_1_rmse: 0.55504 |  0:01:22s
epoch 13 | loss: 0.30467 | val_0_rmse: 0.55614 | val_1_rmse: 0.55765 |  0:01:28s
epoch 14 | loss: 0.30185 | val_0_rmse: 0.57159 | val_1_rmse: 0.56714 |  0:01:34s
epoch 15 | loss: 0.29775 | val_0_rmse: 0.57354 | val_1_rmse: 0.5719  |  0:01:41s
epoch 16 | loss: 0.29333 | val_0_rmse: 0.54341 | val_1_rmse: 0.54547 |  0:01:47s
epoch 17 | loss: 0.29901 | val_0_rmse: 0.60661 | val_1_rmse: 0.61022 |  0:01:53s
epoch 18 | loss: 0.29357 | val_0_rmse: 0.52319 | val_1_rmse: 0.52568 |  0:02:00s
epoch 19 | loss: 0.29143 | val_0_rmse: 0.58124 | val_1_rmse: 0.58106 |  0:02:06s
epoch 20 | loss: 0.29114 | val_0_rmse: 0.5363  | val_1_rmse: 0.53531 |  0:02:12s
epoch 21 | loss: 0.28777 | val_0_rmse: 0.55258 | val_1_rmse: 0.55794 |  0:02:19s
epoch 22 | loss: 0.29464 | val_0_rmse: 0.53691 | val_1_rmse: 0.54139 |  0:02:25s
epoch 23 | loss: 0.29014 | val_0_rmse: 0.5446  | val_1_rmse: 0.54823 |  0:02:32s
epoch 24 | loss: 0.28736 | val_0_rmse: 0.53381 | val_1_rmse: 0.53863 |  0:02:38s
epoch 25 | loss: 0.28571 | val_0_rmse: 0.51567 | val_1_rmse: 0.51978 |  0:02:44s
epoch 26 | loss: 0.29147 | val_0_rmse: 0.54537 | val_1_rmse: 0.54508 |  0:02:51s
epoch 27 | loss: 0.28712 | val_0_rmse: 0.51806 | val_1_rmse: 0.51868 |  0:02:57s
epoch 28 | loss: 0.28687 | val_0_rmse: 0.52822 | val_1_rmse: 0.53228 |  0:03:03s
epoch 29 | loss: 0.28745 | val_0_rmse: 0.51712 | val_1_rmse: 0.51958 |  0:03:10s
epoch 30 | loss: 0.28295 | val_0_rmse: 0.53175 | val_1_rmse: 0.53479 |  0:03:16s
epoch 31 | loss: 0.28311 | val_0_rmse: 0.55513 | val_1_rmse: 0.55759 |  0:03:23s
epoch 32 | loss: 0.28147 | val_0_rmse: 0.52722 | val_1_rmse: 0.53092 |  0:03:29s
epoch 33 | loss: 0.27969 | val_0_rmse: 0.5315  | val_1_rmse: 0.53553 |  0:03:36s
epoch 34 | loss: 0.28292 | val_0_rmse: 0.55525 | val_1_rmse: 0.55732 |  0:03:42s
epoch 35 | loss: 0.28018 | val_0_rmse: 0.51328 | val_1_rmse: 0.51806 |  0:03:48s
epoch 36 | loss: 0.27797 | val_0_rmse: 0.53704 | val_1_rmse: 0.53933 |  0:03:55s
epoch 37 | loss: 0.28002 | val_0_rmse: 0.52911 | val_1_rmse: 0.5312  |  0:04:01s
epoch 38 | loss: 0.27499 | val_0_rmse: 0.52587 | val_1_rmse: 0.52702 |  0:04:07s
epoch 39 | loss: 0.28104 | val_0_rmse: 0.54423 | val_1_rmse: 0.54353 |  0:04:14s
epoch 40 | loss: 0.27913 | val_0_rmse: 0.52035 | val_1_rmse: 0.52118 |  0:04:20s
epoch 41 | loss: 0.27939 | val_0_rmse: 0.52876 | val_1_rmse: 0.53322 |  0:04:26s
epoch 42 | loss: 0.27874 | val_0_rmse: 0.5452  | val_1_rmse: 0.54911 |  0:04:33s
epoch 43 | loss: 0.27626 | val_0_rmse: 0.53876 | val_1_rmse: 0.53658 |  0:04:39s
epoch 44 | loss: 0.27738 | val_0_rmse: 0.54979 | val_1_rmse: 0.55258 |  0:04:45s
epoch 45 | loss: 0.27919 | val_0_rmse: 0.51724 | val_1_rmse: 0.51966 |  0:04:52s
epoch 46 | loss: 0.27894 | val_0_rmse: 0.51177 | val_1_rmse: 0.51711 |  0:04:58s
epoch 47 | loss: 0.27959 | val_0_rmse: 0.5186  | val_1_rmse: 0.52215 |  0:05:05s
epoch 48 | loss: 0.27542 | val_0_rmse: 0.50998 | val_1_rmse: 0.5121  |  0:05:11s
epoch 49 | loss: 0.27563 | val_0_rmse: 0.51282 | val_1_rmse: 0.51473 |  0:05:17s
epoch 50 | loss: 0.27794 | val_0_rmse: 0.51514 | val_1_rmse: 0.52117 |  0:05:24s
epoch 51 | loss: 0.27248 | val_0_rmse: 0.52014 | val_1_rmse: 0.52156 |  0:05:30s
epoch 52 | loss: 0.27736 | val_0_rmse: 0.51467 | val_1_rmse: 0.51922 |  0:05:37s
epoch 53 | loss: 0.27488 | val_0_rmse: 0.51534 | val_1_rmse: 0.52055 |  0:05:43s
epoch 54 | loss: 0.27082 | val_0_rmse: 0.51791 | val_1_rmse: 0.5246  |  0:05:49s
epoch 55 | loss: 0.27821 | val_0_rmse: 0.54323 | val_1_rmse: 0.54163 |  0:05:56s
epoch 56 | loss: 0.27294 | val_0_rmse: 0.53312 | val_1_rmse: 0.54011 |  0:06:02s
epoch 57 | loss: 0.27608 | val_0_rmse: 0.5116  | val_1_rmse: 0.51433 |  0:06:08s
epoch 58 | loss: 0.27581 | val_0_rmse: 0.52363 | val_1_rmse: 0.52597 |  0:06:15s
epoch 59 | loss: 0.272   | val_0_rmse: 0.53923 | val_1_rmse: 0.54036 |  0:06:21s
epoch 60 | loss: 0.27067 | val_0_rmse: 0.52007 | val_1_rmse: 0.52506 |  0:06:28s
epoch 61 | loss: 0.27171 | val_0_rmse: 0.54698 | val_1_rmse: 0.54752 |  0:06:34s
epoch 62 | loss: 0.27528 | val_0_rmse: 0.52299 | val_1_rmse: 0.52769 |  0:06:40s
epoch 63 | loss: 0.28184 | val_0_rmse: 0.51597 | val_1_rmse: 0.52032 |  0:06:47s
epoch 64 | loss: 0.27233 | val_0_rmse: 0.57171 | val_1_rmse: 0.57602 |  0:06:53s
epoch 65 | loss: 0.27296 | val_0_rmse: 0.52717 | val_1_rmse: 0.52687 |  0:06:59s
epoch 66 | loss: 0.27211 | val_0_rmse: 0.5457  | val_1_rmse: 0.55666 |  0:07:06s
epoch 67 | loss: 0.27425 | val_0_rmse: 0.54619 | val_1_rmse: 0.54867 |  0:07:12s
epoch 68 | loss: 0.27319 | val_0_rmse: 0.52402 | val_1_rmse: 0.52514 |  0:07:18s
epoch 69 | loss: 0.27237 | val_0_rmse: 0.50498 | val_1_rmse: 0.50909 |  0:07:25s
epoch 70 | loss: 0.26739 | val_0_rmse: 0.55389 | val_1_rmse: 0.55891 |  0:07:31s
epoch 71 | loss: 0.26991 | val_0_rmse: 0.5226  | val_1_rmse: 0.52772 |  0:07:38s
epoch 72 | loss: 0.27281 | val_0_rmse: 0.5046  | val_1_rmse: 0.51003 |  0:07:44s
epoch 73 | loss: 0.26789 | val_0_rmse: 0.5029  | val_1_rmse: 0.50675 |  0:07:50s
epoch 74 | loss: 0.26935 | val_0_rmse: 0.50874 | val_1_rmse: 0.51093 |  0:07:57s
epoch 75 | loss: 0.27073 | val_0_rmse: 0.51128 | val_1_rmse: 0.51816 |  0:08:03s
epoch 76 | loss: 0.26928 | val_0_rmse: 0.53434 | val_1_rmse: 0.53795 |  0:08:09s
epoch 77 | loss: 0.27078 | val_0_rmse: 0.51851 | val_1_rmse: 0.52018 |  0:08:16s
epoch 78 | loss: 0.26949 | val_0_rmse: 0.52596 | val_1_rmse: 0.53035 |  0:08:22s
epoch 79 | loss: 0.26966 | val_0_rmse: 0.53854 | val_1_rmse: 0.5408  |  0:08:29s
epoch 80 | loss: 0.26643 | val_0_rmse: 0.53652 | val_1_rmse: 0.54127 |  0:08:35s
epoch 81 | loss: 0.26796 | val_0_rmse: 0.52888 | val_1_rmse: 0.53051 |  0:08:41s
epoch 82 | loss: 0.26676 | val_0_rmse: 0.50378 | val_1_rmse: 0.50816 |  0:08:48s
epoch 83 | loss: 0.2635  | val_0_rmse: 0.57004 | val_1_rmse: 0.56921 |  0:08:54s
epoch 84 | loss: 0.26917 | val_0_rmse: 0.51017 | val_1_rmse: 0.51245 |  0:09:01s
epoch 85 | loss: 0.26479 | val_0_rmse: 0.53343 | val_1_rmse: 0.53769 |  0:09:07s
epoch 86 | loss: 0.26351 | val_0_rmse: 0.5083  | val_1_rmse: 0.51221 |  0:09:13s
epoch 87 | loss: 0.26759 | val_0_rmse: 0.51446 | val_1_rmse: 0.521   |  0:09:20s
epoch 88 | loss: 0.28707 | val_0_rmse: 0.53851 | val_1_rmse: 0.54465 |  0:09:26s
epoch 89 | loss: 0.27018 | val_0_rmse: 0.52139 | val_1_rmse: 0.52276 |  0:09:33s
epoch 90 | loss: 0.26996 | val_0_rmse: 0.50004 | val_1_rmse: 0.50684 |  0:09:39s
epoch 91 | loss: 0.26917 | val_0_rmse: 0.5048  | val_1_rmse: 0.51014 |  0:09:45s
epoch 92 | loss: 0.26777 | val_0_rmse: 0.51116 | val_1_rmse: 0.51607 |  0:09:52s
epoch 93 | loss: 0.26668 | val_0_rmse: 0.5161  | val_1_rmse: 0.52124 |  0:09:58s
epoch 94 | loss: 0.26774 | val_0_rmse: 0.69021 | val_1_rmse: 0.69404 |  0:10:04s
epoch 95 | loss: 0.26842 | val_0_rmse: 0.52628 | val_1_rmse: 0.52748 |  0:10:11s
epoch 96 | loss: 0.27061 | val_0_rmse: 0.51374 | val_1_rmse: 0.51753 |  0:10:17s
epoch 97 | loss: 0.26492 | val_0_rmse: 0.53183 | val_1_rmse: 0.53534 |  0:10:24s
epoch 98 | loss: 0.26642 | val_0_rmse: 0.52574 | val_1_rmse: 0.53583 |  0:10:30s
epoch 99 | loss: 0.2667  | val_0_rmse: 0.50796 | val_1_rmse: 0.51284 |  0:10:36s
epoch 100| loss: 0.26506 | val_0_rmse: 0.53685 | val_1_rmse: 0.54659 |  0:10:43s
epoch 101| loss: 0.26748 | val_0_rmse: 0.51043 | val_1_rmse: 0.51501 |  0:10:49s
epoch 102| loss: 0.26288 | val_0_rmse: 0.5015  | val_1_rmse: 0.50935 |  0:10:55s
epoch 103| loss: 0.26232 | val_0_rmse: 0.50673 | val_1_rmse: 0.51311 |  0:11:02s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.50675
Best weights from best epoch are automatically used!
ended training at: 06:18:34
Feature importance:
[('Area', 0.0), ('Baths', 0.3192073741852378), ('Beds', 0.18829255786444377), ('Latitude', 0.07533535409731756), ('Longitude', 0.1291674316815571), ('Month', 0.0), ('Year', 0.28799728217144377)]
Mean squared error is of 14447381725.900362
Mean absolute error:84107.31893680822
MAPE:0.35267462067233485
R2 score:0.7503122129029804
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:24:55
epoch 0  | loss: 0.66833 | val_0_rmse: 0.75558 | val_1_rmse: 0.75627 |  0:00:04s
epoch 1  | loss: 0.5111  | val_0_rmse: 0.67604 | val_1_rmse: 0.67938 |  0:00:09s
epoch 2  | loss: 0.37667 | val_0_rmse: 0.56567 | val_1_rmse: 0.56575 |  0:00:14s
epoch 3  | loss: 0.33386 | val_0_rmse: 0.56377 | val_1_rmse: 0.5622  |  0:00:19s
epoch 4  | loss: 0.33266 | val_0_rmse: 0.58629 | val_1_rmse: 0.58587 |  0:00:24s
epoch 5  | loss: 0.33548 | val_0_rmse: 0.56474 | val_1_rmse: 0.56195 |  0:00:29s
epoch 6  | loss: 0.32495 | val_0_rmse: 0.56519 | val_1_rmse: 0.564   |  0:00:35s
epoch 7  | loss: 0.32284 | val_0_rmse: 0.57107 | val_1_rmse: 0.5704  |  0:00:40s
epoch 8  | loss: 0.32364 | val_0_rmse: 0.59867 | val_1_rmse: 0.60053 |  0:00:45s
epoch 9  | loss: 0.32236 | val_0_rmse: 0.59822 | val_1_rmse: 0.60127 |  0:00:50s
epoch 10 | loss: 0.32107 | val_0_rmse: 0.55294 | val_1_rmse: 0.55459 |  0:00:55s
epoch 11 | loss: 0.31439 | val_0_rmse: 0.5714  | val_1_rmse: 0.56985 |  0:01:00s
epoch 12 | loss: 0.31957 | val_0_rmse: 0.62574 | val_1_rmse: 0.62838 |  0:01:05s
epoch 13 | loss: 0.31939 | val_0_rmse: 0.57634 | val_1_rmse: 0.57703 |  0:01:10s
epoch 14 | loss: 0.32061 | val_0_rmse: 0.60641 | val_1_rmse: 0.60649 |  0:01:15s
epoch 15 | loss: 0.34116 | val_0_rmse: 0.58668 | val_1_rmse: 0.58654 |  0:01:20s
epoch 16 | loss: 0.3563  | val_0_rmse: 0.60681 | val_1_rmse: 0.60937 |  0:01:25s
epoch 17 | loss: 0.32811 | val_0_rmse: 0.54861 | val_1_rmse: 0.54646 |  0:01:30s
epoch 18 | loss: 0.31077 | val_0_rmse: 0.5983  | val_1_rmse: 0.59855 |  0:01:35s
epoch 19 | loss: 0.31807 | val_0_rmse: 0.60477 | val_1_rmse: 0.60426 |  0:01:40s
epoch 20 | loss: 0.31112 | val_0_rmse: 0.53805 | val_1_rmse: 0.53505 |  0:01:45s
epoch 21 | loss: 0.32442 | val_0_rmse: 0.56369 | val_1_rmse: 0.56288 |  0:01:50s
epoch 22 | loss: 0.30947 | val_0_rmse: 0.53861 | val_1_rmse: 0.53684 |  0:01:55s
epoch 23 | loss: 0.30352 | val_0_rmse: 0.53584 | val_1_rmse: 0.53324 |  0:02:01s
epoch 24 | loss: 0.30989 | val_0_rmse: 0.59263 | val_1_rmse: 0.59017 |  0:02:06s
epoch 25 | loss: 0.30368 | val_0_rmse: 0.58778 | val_1_rmse: 0.58663 |  0:02:11s
epoch 26 | loss: 0.30611 | val_0_rmse: 0.58285 | val_1_rmse: 0.58229 |  0:02:16s
epoch 27 | loss: 0.3057  | val_0_rmse: 0.54192 | val_1_rmse: 0.54064 |  0:02:21s
epoch 28 | loss: 0.3106  | val_0_rmse: 0.53849 | val_1_rmse: 0.53623 |  0:02:26s
epoch 29 | loss: 0.3099  | val_0_rmse: 0.5322  | val_1_rmse: 0.52935 |  0:02:31s
epoch 30 | loss: 0.31349 | val_0_rmse: 0.56162 | val_1_rmse: 0.55735 |  0:02:36s
epoch 31 | loss: 0.30304 | val_0_rmse: 0.53928 | val_1_rmse: 0.53554 |  0:02:41s
epoch 32 | loss: 0.30533 | val_0_rmse: 0.53938 | val_1_rmse: 0.53829 |  0:02:46s
epoch 33 | loss: 0.30461 | val_0_rmse: 0.55312 | val_1_rmse: 0.55427 |  0:02:52s
epoch 34 | loss: 0.31281 | val_0_rmse: 0.66    | val_1_rmse: 0.65965 |  0:02:57s
epoch 35 | loss: 0.30175 | val_0_rmse: 0.56105 | val_1_rmse: 0.55904 |  0:03:02s
epoch 36 | loss: 0.31221 | val_0_rmse: 0.52699 | val_1_rmse: 0.52333 |  0:03:07s
epoch 37 | loss: 0.29837 | val_0_rmse: 0.54343 | val_1_rmse: 0.54076 |  0:03:12s
epoch 38 | loss: 0.31842 | val_0_rmse: 0.53634 | val_1_rmse: 0.53277 |  0:03:17s
epoch 39 | loss: 0.31093 | val_0_rmse: 0.54747 | val_1_rmse: 0.54542 |  0:03:22s
epoch 40 | loss: 0.29964 | val_0_rmse: 0.55768 | val_1_rmse: 0.55638 |  0:03:27s
epoch 41 | loss: 0.29953 | val_0_rmse: 0.65434 | val_1_rmse: 0.65648 |  0:03:33s
epoch 42 | loss: 0.31342 | val_0_rmse: 0.61804 | val_1_rmse: 0.61523 |  0:03:38s
epoch 43 | loss: 0.30699 | val_0_rmse: 0.55882 | val_1_rmse: 0.55754 |  0:03:43s
epoch 44 | loss: 0.29759 | val_0_rmse: 0.54712 | val_1_rmse: 0.54491 |  0:03:48s
epoch 45 | loss: 0.303   | val_0_rmse: 0.55341 | val_1_rmse: 0.5508  |  0:03:53s
epoch 46 | loss: 0.29691 | val_0_rmse: 0.53079 | val_1_rmse: 0.52956 |  0:03:58s
epoch 47 | loss: 0.29454 | val_0_rmse: 0.55574 | val_1_rmse: 0.55484 |  0:04:04s
epoch 48 | loss: 0.30391 | val_0_rmse: 0.5424  | val_1_rmse: 0.5397  |  0:04:09s
epoch 49 | loss: 0.29964 | val_0_rmse: 0.54598 | val_1_rmse: 0.54457 |  0:04:14s
epoch 50 | loss: 0.29958 | val_0_rmse: 0.55775 | val_1_rmse: 0.55671 |  0:04:19s
epoch 51 | loss: 0.30434 | val_0_rmse: 0.54321 | val_1_rmse: 0.54138 |  0:04:24s
epoch 52 | loss: 0.29643 | val_0_rmse: 0.6356  | val_1_rmse: 0.63675 |  0:04:29s
epoch 53 | loss: 0.29595 | val_0_rmse: 0.53943 | val_1_rmse: 0.53852 |  0:04:34s
epoch 54 | loss: 0.28935 | val_0_rmse: 0.58841 | val_1_rmse: 0.59092 |  0:04:39s
epoch 55 | loss: 0.29858 | val_0_rmse: 0.5294  | val_1_rmse: 0.52887 |  0:04:44s
epoch 56 | loss: 0.29421 | val_0_rmse: 0.61369 | val_1_rmse: 0.61263 |  0:04:49s
epoch 57 | loss: 0.3064  | val_0_rmse: 0.54742 | val_1_rmse: 0.54539 |  0:04:55s
epoch 58 | loss: 0.30409 | val_0_rmse: 0.52466 | val_1_rmse: 0.5217  |  0:05:00s
epoch 59 | loss: 0.29716 | val_0_rmse: 0.53916 | val_1_rmse: 0.53876 |  0:05:05s
epoch 60 | loss: 0.29546 | val_0_rmse: 0.5418  | val_1_rmse: 0.53852 |  0:05:10s
epoch 61 | loss: 0.29485 | val_0_rmse: 0.51547 | val_1_rmse: 0.51609 |  0:05:15s
epoch 62 | loss: 0.29578 | val_0_rmse: 0.55696 | val_1_rmse: 0.55673 |  0:05:20s
epoch 63 | loss: 0.31714 | val_0_rmse: 0.54447 | val_1_rmse: 0.54328 |  0:05:25s
epoch 64 | loss: 0.29666 | val_0_rmse: 0.55157 | val_1_rmse: 0.55249 |  0:05:30s
epoch 65 | loss: 0.28771 | val_0_rmse: 0.51863 | val_1_rmse: 0.51784 |  0:05:36s
epoch 66 | loss: 0.28704 | val_0_rmse: 0.51294 | val_1_rmse: 0.51271 |  0:05:41s
epoch 67 | loss: 0.28842 | val_0_rmse: 0.56143 | val_1_rmse: 0.56369 |  0:05:46s
epoch 68 | loss: 0.29974 | val_0_rmse: 0.54418 | val_1_rmse: 0.54407 |  0:05:51s
epoch 69 | loss: 0.29737 | val_0_rmse: 0.57729 | val_1_rmse: 0.57674 |  0:05:56s
epoch 70 | loss: 0.30754 | val_0_rmse: 0.56375 | val_1_rmse: 0.56449 |  0:06:01s
epoch 71 | loss: 0.29891 | val_0_rmse: 0.5467  | val_1_rmse: 0.54515 |  0:06:07s
epoch 72 | loss: 0.29399 | val_0_rmse: 0.56667 | val_1_rmse: 0.56634 |  0:06:12s
epoch 73 | loss: 0.30748 | val_0_rmse: 0.5797  | val_1_rmse: 0.57966 |  0:06:17s
epoch 74 | loss: 0.28876 | val_0_rmse: 0.54361 | val_1_rmse: 0.54473 |  0:06:22s
epoch 75 | loss: 0.2847  | val_0_rmse: 0.60494 | val_1_rmse: 0.60601 |  0:06:27s
epoch 76 | loss: 0.29246 | val_0_rmse: 0.54912 | val_1_rmse: 0.54772 |  0:06:32s
epoch 77 | loss: 0.28452 | val_0_rmse: 0.58272 | val_1_rmse: 0.58194 |  0:06:38s
epoch 78 | loss: 0.28622 | val_0_rmse: 0.55354 | val_1_rmse: 0.55282 |  0:06:43s
epoch 79 | loss: 0.28954 | val_0_rmse: 0.57433 | val_1_rmse: 0.57406 |  0:06:48s
epoch 80 | loss: 0.28304 | val_0_rmse: 0.60255 | val_1_rmse: 0.60303 |  0:06:53s
epoch 81 | loss: 0.29232 | val_0_rmse: 0.52052 | val_1_rmse: 0.51905 |  0:06:58s
epoch 82 | loss: 0.28458 | val_0_rmse: 0.58981 | val_1_rmse: 0.58961 |  0:07:03s
epoch 83 | loss: 0.29001 | val_0_rmse: 0.6891  | val_1_rmse: 0.69297 |  0:07:08s
epoch 84 | loss: 0.2975  | val_0_rmse: 0.56244 | val_1_rmse: 0.56361 |  0:07:14s
epoch 85 | loss: 0.31039 | val_0_rmse: 0.5448  | val_1_rmse: 0.54492 |  0:07:19s
epoch 86 | loss: 0.2883  | val_0_rmse: 0.51698 | val_1_rmse: 0.51421 |  0:07:24s
epoch 87 | loss: 0.28328 | val_0_rmse: 0.51609 | val_1_rmse: 0.51469 |  0:07:29s
epoch 88 | loss: 0.28644 | val_0_rmse: 0.5362  | val_1_rmse: 0.5373  |  0:07:34s
epoch 89 | loss: 0.29744 | val_0_rmse: 0.56222 | val_1_rmse: 0.56346 |  0:07:39s
epoch 90 | loss: 0.29111 | val_0_rmse: 0.56042 | val_1_rmse: 0.55823 |  0:07:44s
epoch 91 | loss: 0.28469 | val_0_rmse: 0.53602 | val_1_rmse: 0.53538 |  0:07:49s
epoch 92 | loss: 0.28899 | val_0_rmse: 0.54371 | val_1_rmse: 0.54339 |  0:07:54s
epoch 93 | loss: 0.30012 | val_0_rmse: 0.53476 | val_1_rmse: 0.5312  |  0:07:59s
epoch 94 | loss: 0.28646 | val_0_rmse: 0.53648 | val_1_rmse: 0.53475 |  0:08:05s
epoch 95 | loss: 0.28576 | val_0_rmse: 0.52697 | val_1_rmse: 0.52699 |  0:08:10s
epoch 96 | loss: 0.2861  | val_0_rmse: 0.56351 | val_1_rmse: 0.56313 |  0:08:15s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.51271
Best weights from best epoch are automatically used!
ended training at: 06:33:12
Feature importance:
[('Area', 0.5271106400740432), ('Baths', 0.10643675179497976), ('Beds', 0.06921302786117733), ('Latitude', 0.24455083796372387), ('Longitude', 0.052688742306075884), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 8106151345.918231
Mean absolute error:65677.43538475009
MAPE:0.17895231314555884
R2 score:0.7363190918597688
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:39:33
epoch 0  | loss: 0.75136 | val_0_rmse: 0.8293  | val_1_rmse: 0.83601 |  0:00:04s
epoch 1  | loss: 0.6824  | val_0_rmse: 0.81946 | val_1_rmse: 0.82845 |  0:00:09s
epoch 2  | loss: 0.65927 | val_0_rmse: 0.7973  | val_1_rmse: 0.80468 |  0:00:14s
epoch 3  | loss: 0.63743 | val_0_rmse: 0.77655 | val_1_rmse: 0.78401 |  0:00:19s
epoch 4  | loss: 0.62683 | val_0_rmse: 0.78046 | val_1_rmse: 0.78812 |  0:00:24s
epoch 5  | loss: 0.61656 | val_0_rmse: 0.77732 | val_1_rmse: 0.78444 |  0:00:29s
epoch 6  | loss: 0.61384 | val_0_rmse: 0.78583 | val_1_rmse: 0.79457 |  0:00:34s
epoch 7  | loss: 0.60556 | val_0_rmse: 0.77169 | val_1_rmse: 0.77928 |  0:00:39s
epoch 8  | loss: 0.59162 | val_0_rmse: 0.75108 | val_1_rmse: 0.75843 |  0:00:44s
epoch 9  | loss: 0.59159 | val_0_rmse: 0.7762  | val_1_rmse: 0.78691 |  0:00:49s
epoch 10 | loss: 0.58251 | val_0_rmse: 0.75258 | val_1_rmse: 0.75957 |  0:00:54s
epoch 11 | loss: 0.5704  | val_0_rmse: 0.74083 | val_1_rmse: 0.75127 |  0:00:59s
epoch 12 | loss: 0.57565 | val_0_rmse: 0.77045 | val_1_rmse: 0.77773 |  0:01:04s
epoch 13 | loss: 0.59055 | val_0_rmse: 0.74725 | val_1_rmse: 0.75637 |  0:01:09s
epoch 14 | loss: 0.57773 | val_0_rmse: 0.75295 | val_1_rmse: 0.76142 |  0:01:14s
epoch 15 | loss: 0.58671 | val_0_rmse: 0.73806 | val_1_rmse: 0.74531 |  0:01:20s
epoch 16 | loss: 0.57912 | val_0_rmse: 0.77493 | val_1_rmse: 0.78233 |  0:01:25s
epoch 17 | loss: 0.56742 | val_0_rmse: 0.73081 | val_1_rmse: 0.73911 |  0:01:30s
epoch 18 | loss: 0.56078 | val_0_rmse: 0.73806 | val_1_rmse: 0.74481 |  0:01:35s
epoch 19 | loss: 0.56385 | val_0_rmse: 0.7271  | val_1_rmse: 0.73735 |  0:01:40s
epoch 20 | loss: 0.5554  | val_0_rmse: 0.7444  | val_1_rmse: 0.75276 |  0:01:45s
epoch 21 | loss: 0.56339 | val_0_rmse: 0.7391  | val_1_rmse: 0.74592 |  0:01:50s
epoch 22 | loss: 0.56836 | val_0_rmse: 0.76725 | val_1_rmse: 0.77602 |  0:01:55s
epoch 23 | loss: 0.56691 | val_0_rmse: 0.728   | val_1_rmse: 0.73537 |  0:02:00s
epoch 24 | loss: 0.54559 | val_0_rmse: 0.72525 | val_1_rmse: 0.73203 |  0:02:05s
epoch 25 | loss: 0.54671 | val_0_rmse: 0.73929 | val_1_rmse: 0.74719 |  0:02:10s
epoch 26 | loss: 0.53815 | val_0_rmse: 0.73642 | val_1_rmse: 0.74457 |  0:02:15s
epoch 27 | loss: 0.54319 | val_0_rmse: 0.71721 | val_1_rmse: 0.72446 |  0:02:20s
epoch 28 | loss: 0.54222 | val_0_rmse: 0.73284 | val_1_rmse: 0.74073 |  0:02:26s
epoch 29 | loss: 0.54045 | val_0_rmse: 0.71581 | val_1_rmse: 0.7252  |  0:02:30s
epoch 30 | loss: 0.5371  | val_0_rmse: 0.74216 | val_1_rmse: 0.74848 |  0:02:36s
epoch 31 | loss: 0.53835 | val_0_rmse: 0.71933 | val_1_rmse: 0.72705 |  0:02:41s
epoch 32 | loss: 0.53507 | val_0_rmse: 0.71209 | val_1_rmse: 0.72058 |  0:02:46s
epoch 33 | loss: 0.53458 | val_0_rmse: 0.71949 | val_1_rmse: 0.72821 |  0:02:51s
epoch 34 | loss: 0.52961 | val_0_rmse: 0.72241 | val_1_rmse: 0.73223 |  0:02:56s
epoch 35 | loss: 0.53993 | val_0_rmse: 0.71502 | val_1_rmse: 0.72449 |  0:03:01s
epoch 36 | loss: 0.53525 | val_0_rmse: 0.71849 | val_1_rmse: 0.72675 |  0:03:06s
epoch 37 | loss: 0.53924 | val_0_rmse: 0.73126 | val_1_rmse: 0.73878 |  0:03:11s
epoch 38 | loss: 0.53497 | val_0_rmse: 0.71656 | val_1_rmse: 0.72376 |  0:03:16s
epoch 39 | loss: 0.53406 | val_0_rmse: 0.70516 | val_1_rmse: 0.71294 |  0:03:21s
epoch 40 | loss: 0.52622 | val_0_rmse: 0.71051 | val_1_rmse: 0.71861 |  0:03:27s
epoch 41 | loss: 0.53128 | val_0_rmse: 0.7438  | val_1_rmse: 0.75212 |  0:03:32s
epoch 42 | loss: 0.53215 | val_0_rmse: 0.7104  | val_1_rmse: 0.71992 |  0:03:37s
epoch 43 | loss: 0.53331 | val_0_rmse: 0.74285 | val_1_rmse: 0.74799 |  0:03:42s
epoch 44 | loss: 0.53476 | val_0_rmse: 0.71037 | val_1_rmse: 0.71679 |  0:03:47s
epoch 45 | loss: 0.53415 | val_0_rmse: 0.71911 | val_1_rmse: 0.72848 |  0:03:52s
epoch 46 | loss: 0.53457 | val_0_rmse: 0.72293 | val_1_rmse: 0.73337 |  0:03:57s
epoch 47 | loss: 0.53214 | val_0_rmse: 0.70842 | val_1_rmse: 0.7165  |  0:04:02s
epoch 48 | loss: 0.52218 | val_0_rmse: 0.71236 | val_1_rmse: 0.72258 |  0:04:07s
epoch 49 | loss: 0.52275 | val_0_rmse: 0.70854 | val_1_rmse: 0.71673 |  0:04:12s
epoch 50 | loss: 0.52186 | val_0_rmse: 0.70705 | val_1_rmse: 0.71701 |  0:04:17s
epoch 51 | loss: 0.52387 | val_0_rmse: 0.71109 | val_1_rmse: 0.71838 |  0:04:22s
epoch 52 | loss: 0.51845 | val_0_rmse: 0.70569 | val_1_rmse: 0.71527 |  0:04:27s
epoch 53 | loss: 0.5223  | val_0_rmse: 0.71873 | val_1_rmse: 0.72626 |  0:04:32s
epoch 54 | loss: 0.51859 | val_0_rmse: 0.70322 | val_1_rmse: 0.71375 |  0:04:37s
epoch 55 | loss: 0.52548 | val_0_rmse: 0.70252 | val_1_rmse: 0.71014 |  0:04:42s
epoch 56 | loss: 0.52838 | val_0_rmse: 0.70785 | val_1_rmse: 0.71671 |  0:04:48s
epoch 57 | loss: 0.52291 | val_0_rmse: 0.73912 | val_1_rmse: 0.74959 |  0:04:53s
epoch 58 | loss: 0.53385 | val_0_rmse: 0.70651 | val_1_rmse: 0.71762 |  0:04:58s
epoch 59 | loss: 0.52126 | val_0_rmse: 0.70923 | val_1_rmse: 0.72012 |  0:05:03s
epoch 60 | loss: 0.51583 | val_0_rmse: 0.70308 | val_1_rmse: 0.71032 |  0:05:08s
epoch 61 | loss: 0.51291 | val_0_rmse: 0.70499 | val_1_rmse: 0.71403 |  0:05:13s
epoch 62 | loss: 0.51806 | val_0_rmse: 0.70541 | val_1_rmse: 0.71203 |  0:05:18s
epoch 63 | loss: 0.51444 | val_0_rmse: 0.74048 | val_1_rmse: 0.75206 |  0:05:23s
epoch 64 | loss: 0.53579 | val_0_rmse: 0.70971 | val_1_rmse: 0.71985 |  0:05:28s
epoch 65 | loss: 0.53316 | val_0_rmse: 0.75088 | val_1_rmse: 0.7602  |  0:05:34s
epoch 66 | loss: 0.53676 | val_0_rmse: 0.73624 | val_1_rmse: 0.74348 |  0:05:39s
epoch 67 | loss: 0.52242 | val_0_rmse: 0.70759 | val_1_rmse: 0.71686 |  0:05:44s
epoch 68 | loss: 0.51352 | val_0_rmse: 0.6997  | val_1_rmse: 0.70897 |  0:05:49s
epoch 69 | loss: 0.5129  | val_0_rmse: 0.75799 | val_1_rmse: 0.76864 |  0:05:54s
epoch 70 | loss: 0.5106  | val_0_rmse: 0.71471 | val_1_rmse: 0.72451 |  0:05:59s
epoch 71 | loss: 0.50862 | val_0_rmse: 0.73786 | val_1_rmse: 0.7492  |  0:06:04s
epoch 72 | loss: 0.51871 | val_0_rmse: 0.71729 | val_1_rmse: 0.72788 |  0:06:10s
epoch 73 | loss: 0.51883 | val_0_rmse: 0.69907 | val_1_rmse: 0.70624 |  0:06:15s
epoch 74 | loss: 0.51852 | val_0_rmse: 0.69976 | val_1_rmse: 0.7071  |  0:06:20s
epoch 75 | loss: 0.51061 | val_0_rmse: 0.70883 | val_1_rmse: 0.71816 |  0:06:25s
epoch 76 | loss: 0.50995 | val_0_rmse: 0.71614 | val_1_rmse: 0.72625 |  0:06:30s
epoch 77 | loss: 0.51378 | val_0_rmse: 0.74809 | val_1_rmse: 0.75942 |  0:06:35s
epoch 78 | loss: 0.53234 | val_0_rmse: 0.71096 | val_1_rmse: 0.72416 |  0:06:40s
epoch 79 | loss: 0.51284 | val_0_rmse: 0.71199 | val_1_rmse: 0.72071 |  0:06:45s
epoch 80 | loss: 0.50488 | val_0_rmse: 0.71165 | val_1_rmse: 0.72311 |  0:06:50s
epoch 81 | loss: 0.50849 | val_0_rmse: 0.69921 | val_1_rmse: 0.70809 |  0:06:55s
epoch 82 | loss: 0.50295 | val_0_rmse: 0.72586 | val_1_rmse: 0.73613 |  0:07:00s
epoch 83 | loss: 0.50903 | val_0_rmse: 0.71496 | val_1_rmse: 0.72205 |  0:07:05s
epoch 84 | loss: 0.50843 | val_0_rmse: 0.73109 | val_1_rmse: 0.74168 |  0:07:11s
epoch 85 | loss: 0.50877 | val_0_rmse: 0.70479 | val_1_rmse: 0.71253 |  0:07:16s
epoch 86 | loss: 0.50537 | val_0_rmse: 0.73049 | val_1_rmse: 0.73764 |  0:07:21s
epoch 87 | loss: 0.50636 | val_0_rmse: 0.74596 | val_1_rmse: 0.75211 |  0:07:26s
epoch 88 | loss: 0.50583 | val_0_rmse: 0.70115 | val_1_rmse: 0.70999 |  0:07:31s
epoch 89 | loss: 0.50306 | val_0_rmse: 0.69665 | val_1_rmse: 0.7054  |  0:07:36s
epoch 90 | loss: 0.50001 | val_0_rmse: 0.73672 | val_1_rmse: 0.74348 |  0:07:42s
epoch 91 | loss: 0.50988 | val_0_rmse: 0.71933 | val_1_rmse: 0.72498 |  0:07:47s
epoch 92 | loss: 0.51549 | val_0_rmse: 0.72045 | val_1_rmse: 0.73002 |  0:07:52s
epoch 93 | loss: 0.51348 | val_0_rmse: 0.70818 | val_1_rmse: 0.71434 |  0:07:57s
epoch 94 | loss: 0.50205 | val_0_rmse: 0.766   | val_1_rmse: 0.77199 |  0:08:02s
epoch 95 | loss: 0.47743 | val_0_rmse: 0.74148 | val_1_rmse: 0.75015 |  0:08:07s
epoch 96 | loss: 0.46246 | val_0_rmse: 0.65114 | val_1_rmse: 0.65767 |  0:08:12s
epoch 97 | loss: 0.46395 | val_0_rmse: 0.74671 | val_1_rmse: 0.75598 |  0:08:17s
epoch 98 | loss: 0.45642 | val_0_rmse: 0.76843 | val_1_rmse: 0.77185 |  0:08:23s
epoch 99 | loss: 0.46051 | val_0_rmse: 0.65958 | val_1_rmse: 0.66409 |  0:08:28s
epoch 100| loss: 0.45172 | val_0_rmse: 0.72278 | val_1_rmse: 0.73103 |  0:08:33s
epoch 101| loss: 0.44812 | val_0_rmse: 0.65686 | val_1_rmse: 0.66486 |  0:08:38s
epoch 102| loss: 0.4449  | val_0_rmse: 0.65021 | val_1_rmse: 0.6561  |  0:08:43s
epoch 103| loss: 0.43639 | val_0_rmse: 0.70971 | val_1_rmse: 0.71922 |  0:08:48s
epoch 104| loss: 0.44536 | val_0_rmse: 0.67524 | val_1_rmse: 0.68326 |  0:08:53s
epoch 105| loss: 0.44653 | val_0_rmse: 0.75606 | val_1_rmse: 0.76603 |  0:08:58s
epoch 106| loss: 0.44724 | val_0_rmse: 0.66013 | val_1_rmse: 0.66811 |  0:09:03s
epoch 107| loss: 0.4441  | val_0_rmse: 0.75392 | val_1_rmse: 0.76502 |  0:09:08s
epoch 108| loss: 0.43989 | val_0_rmse: 0.646   | val_1_rmse: 0.65115 |  0:09:14s
epoch 109| loss: 0.43817 | val_0_rmse: 0.76554 | val_1_rmse: 0.77668 |  0:09:19s
epoch 110| loss: 0.43659 | val_0_rmse: 0.7254  | val_1_rmse: 0.73325 |  0:09:24s
epoch 111| loss: 0.44179 | val_0_rmse: 0.76153 | val_1_rmse: 0.77092 |  0:09:29s
epoch 112| loss: 0.46218 | val_0_rmse: 0.7278  | val_1_rmse: 0.7363  |  0:09:34s
epoch 113| loss: 0.45526 | val_0_rmse: 0.68589 | val_1_rmse: 0.69002 |  0:09:39s
epoch 114| loss: 0.44845 | val_0_rmse: 0.63958 | val_1_rmse: 0.64677 |  0:09:44s
epoch 115| loss: 0.44655 | val_0_rmse: 0.62737 | val_1_rmse: 0.63503 |  0:09:49s
epoch 116| loss: 0.44322 | val_0_rmse: 0.64893 | val_1_rmse: 0.65568 |  0:09:54s
epoch 117| loss: 0.44136 | val_0_rmse: 0.64995 | val_1_rmse: 0.65516 |  0:10:00s
epoch 118| loss: 0.44036 | val_0_rmse: 0.63924 | val_1_rmse: 0.64484 |  0:10:05s
epoch 119| loss: 0.43994 | val_0_rmse: 0.70392 | val_1_rmse: 0.70903 |  0:10:10s
epoch 120| loss: 0.43239 | val_0_rmse: 0.65072 | val_1_rmse: 0.65632 |  0:10:15s
epoch 121| loss: 0.43368 | val_0_rmse: 0.65633 | val_1_rmse: 0.66435 |  0:10:20s
epoch 122| loss: 0.43271 | val_0_rmse: 0.65196 | val_1_rmse: 0.65972 |  0:10:25s
epoch 123| loss: 0.43029 | val_0_rmse: 0.63544 | val_1_rmse: 0.63959 |  0:10:30s
epoch 124| loss: 0.42553 | val_0_rmse: 0.65665 | val_1_rmse: 0.66218 |  0:10:35s
epoch 125| loss: 0.4325  | val_0_rmse: 0.71923 | val_1_rmse: 0.72829 |  0:10:40s
epoch 126| loss: 0.42805 | val_0_rmse: 0.67835 | val_1_rmse: 0.68517 |  0:10:45s
epoch 127| loss: 0.42547 | val_0_rmse: 0.72047 | val_1_rmse: 0.72777 |  0:10:50s
epoch 128| loss: 0.43679 | val_0_rmse: 0.66487 | val_1_rmse: 0.67438 |  0:10:55s
epoch 129| loss: 0.42694 | val_0_rmse: 0.81649 | val_1_rmse: 0.82643 |  0:11:00s
epoch 130| loss: 0.43172 | val_0_rmse: 0.67307 | val_1_rmse: 0.68104 |  0:11:05s
epoch 131| loss: 0.43031 | val_0_rmse: 0.64979 | val_1_rmse: 0.65665 |  0:11:11s
epoch 132| loss: 0.4341  | val_0_rmse: 0.73396 | val_1_rmse: 0.73962 |  0:11:16s
epoch 133| loss: 0.42906 | val_0_rmse: 0.64407 | val_1_rmse: 0.65003 |  0:11:21s
epoch 134| loss: 0.42858 | val_0_rmse: 0.63995 | val_1_rmse: 0.64357 |  0:11:26s
epoch 135| loss: 0.42902 | val_0_rmse: 0.64184 | val_1_rmse: 0.64659 |  0:11:31s
epoch 136| loss: 0.42601 | val_0_rmse: 0.64126 | val_1_rmse: 0.6437  |  0:11:36s
epoch 137| loss: 0.42576 | val_0_rmse: 0.64853 | val_1_rmse: 0.65611 |  0:11:41s
epoch 138| loss: 0.42327 | val_0_rmse: 0.64257 | val_1_rmse: 0.6454  |  0:11:46s
epoch 139| loss: 0.42363 | val_0_rmse: 0.62433 | val_1_rmse: 0.62922 |  0:11:51s
epoch 140| loss: 0.4199  | val_0_rmse: 0.62371 | val_1_rmse: 0.62735 |  0:11:56s
epoch 141| loss: 0.42019 | val_0_rmse: 0.65463 | val_1_rmse: 0.66096 |  0:12:02s
epoch 142| loss: 0.42119 | val_0_rmse: 0.74903 | val_1_rmse: 0.75161 |  0:12:07s
epoch 143| loss: 0.42153 | val_0_rmse: 0.6834  | val_1_rmse: 0.6893  |  0:12:12s
epoch 144| loss: 0.42543 | val_0_rmse: 0.64114 | val_1_rmse: 0.6436  |  0:12:17s
epoch 145| loss: 0.42166 | val_0_rmse: 0.65174 | val_1_rmse: 0.65638 |  0:12:22s
epoch 146| loss: 0.41593 | val_0_rmse: 0.64385 | val_1_rmse: 0.65209 |  0:12:27s
epoch 147| loss: 0.41974 | val_0_rmse: 0.6539  | val_1_rmse: 0.66031 |  0:12:32s
epoch 148| loss: 0.41878 | val_0_rmse: 0.63643 | val_1_rmse: 0.64112 |  0:12:37s
epoch 149| loss: 0.42329 | val_0_rmse: 0.71426 | val_1_rmse: 0.7183  |  0:12:42s
Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_val_1_rmse = 0.62735
Best weights from best epoch are automatically used!
ended training at: 06:52:17
Feature importance:
[('Area', 0.14614906091773436), ('Baths', 0.001194841804082626), ('Beds', 0.2127946853506783), ('Latitude', 0.43221099938283863), ('Longitude', 0.20477452962924966), ('Month', 0.0), ('Year', 0.0028758829154164325)]
Mean squared error is of 32898530737.557255
Mean absolute error:133276.48649824248
MAPE:0.23326466250462286
R2 score:0.6072090948433855
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:58:53
epoch 0  | loss: 0.46239 | val_0_rmse: 0.63436 | val_1_rmse: 0.63529 |  0:00:04s
epoch 1  | loss: 0.40036 | val_0_rmse: 0.63625 | val_1_rmse: 0.64176 |  0:00:09s
epoch 2  | loss: 0.39687 | val_0_rmse: 0.61783 | val_1_rmse: 0.6183  |  0:00:14s
epoch 3  | loss: 0.37691 | val_0_rmse: 0.61218 | val_1_rmse: 0.61582 |  0:00:19s
epoch 4  | loss: 0.37418 | val_0_rmse: 0.58641 | val_1_rmse: 0.59031 |  0:00:24s
epoch 5  | loss: 0.36243 | val_0_rmse: 0.58471 | val_1_rmse: 0.58737 |  0:00:29s
epoch 6  | loss: 0.35636 | val_0_rmse: 0.58747 | val_1_rmse: 0.59113 |  0:00:34s
epoch 7  | loss: 0.35166 | val_0_rmse: 0.58003 | val_1_rmse: 0.58176 |  0:00:39s
epoch 8  | loss: 0.34931 | val_0_rmse: 0.57424 | val_1_rmse: 0.57763 |  0:00:44s
epoch 9  | loss: 0.33884 | val_0_rmse: 0.56736 | val_1_rmse: 0.57139 |  0:00:49s
epoch 10 | loss: 0.32243 | val_0_rmse: 0.55662 | val_1_rmse: 0.55906 |  0:00:54s
epoch 11 | loss: 0.32094 | val_0_rmse: 0.54756 | val_1_rmse: 0.54954 |  0:00:59s
epoch 12 | loss: 0.30245 | val_0_rmse: 0.61632 | val_1_rmse: 0.62258 |  0:01:04s
epoch 13 | loss: 0.28971 | val_0_rmse: 0.5155  | val_1_rmse: 0.51781 |  0:01:09s
epoch 14 | loss: 0.2809  | val_0_rmse: 0.69216 | val_1_rmse: 0.69562 |  0:01:14s
epoch 15 | loss: 0.26426 | val_0_rmse: 0.48714 | val_1_rmse: 0.4921  |  0:01:19s
epoch 16 | loss: 0.25395 | val_0_rmse: 0.58967 | val_1_rmse: 0.59188 |  0:01:25s
epoch 17 | loss: 0.24684 | val_0_rmse: 0.47561 | val_1_rmse: 0.47381 |  0:01:30s
epoch 18 | loss: 0.24572 | val_0_rmse: 0.48234 | val_1_rmse: 0.48362 |  0:01:35s
epoch 19 | loss: 0.24508 | val_0_rmse: 0.47192 | val_1_rmse: 0.47716 |  0:01:40s
epoch 20 | loss: 0.23063 | val_0_rmse: 0.44632 | val_1_rmse: 0.45109 |  0:01:45s
epoch 21 | loss: 0.22593 | val_0_rmse: 0.45918 | val_1_rmse: 0.46182 |  0:01:50s
epoch 22 | loss: 0.21719 | val_0_rmse: 0.44005 | val_1_rmse: 0.44703 |  0:01:55s
epoch 23 | loss: 0.22006 | val_0_rmse: 0.52554 | val_1_rmse: 0.52776 |  0:02:00s
epoch 24 | loss: 0.22062 | val_0_rmse: 0.62793 | val_1_rmse: 0.63256 |  0:02:06s
epoch 25 | loss: 0.21466 | val_0_rmse: 0.56732 | val_1_rmse: 0.5624  |  0:02:11s
epoch 26 | loss: 0.2128  | val_0_rmse: 0.70205 | val_1_rmse: 0.70623 |  0:02:16s
epoch 27 | loss: 0.208   | val_0_rmse: 0.75035 | val_1_rmse: 0.75489 |  0:02:21s
epoch 28 | loss: 0.21261 | val_0_rmse: 0.64648 | val_1_rmse: 0.65309 |  0:02:26s
epoch 29 | loss: 0.20251 | val_0_rmse: 0.69334 | val_1_rmse: 0.69377 |  0:02:31s
epoch 30 | loss: 0.20478 | val_0_rmse: 0.65459 | val_1_rmse: 0.65844 |  0:02:36s
epoch 31 | loss: 0.20278 | val_0_rmse: 0.52711 | val_1_rmse: 0.53583 |  0:02:41s
epoch 32 | loss: 0.19649 | val_0_rmse: 0.46393 | val_1_rmse: 0.47107 |  0:02:47s
epoch 33 | loss: 0.19623 | val_0_rmse: 0.43    | val_1_rmse: 0.43522 |  0:02:52s
epoch 34 | loss: 0.19353 | val_0_rmse: 0.53626 | val_1_rmse: 0.53642 |  0:02:57s
epoch 35 | loss: 0.19043 | val_0_rmse: 0.41697 | val_1_rmse: 0.42066 |  0:03:02s
epoch 36 | loss: 0.18968 | val_0_rmse: 0.48086 | val_1_rmse: 0.4793  |  0:03:07s
epoch 37 | loss: 0.19431 | val_0_rmse: 0.47295 | val_1_rmse: 0.47387 |  0:03:12s
epoch 38 | loss: 0.20043 | val_0_rmse: 0.58331 | val_1_rmse: 0.57851 |  0:03:17s
epoch 39 | loss: 0.18825 | val_0_rmse: 0.43252 | val_1_rmse: 0.44166 |  0:03:23s
epoch 40 | loss: 0.18542 | val_0_rmse: 0.46205 | val_1_rmse: 0.47059 |  0:03:28s
epoch 41 | loss: 0.17973 | val_0_rmse: 0.39861 | val_1_rmse: 0.40481 |  0:03:33s
epoch 42 | loss: 0.19067 | val_0_rmse: 0.40907 | val_1_rmse: 0.41715 |  0:03:38s
epoch 43 | loss: 0.18409 | val_0_rmse: 0.43208 | val_1_rmse: 0.43772 |  0:03:43s
epoch 44 | loss: 0.17534 | val_0_rmse: 0.41228 | val_1_rmse: 0.41755 |  0:03:49s
epoch 45 | loss: 0.17351 | val_0_rmse: 0.38483 | val_1_rmse: 0.39336 |  0:03:54s
epoch 46 | loss: 0.18105 | val_0_rmse: 0.65174 | val_1_rmse: 0.64752 |  0:03:59s
epoch 47 | loss: 0.17796 | val_0_rmse: 0.59237 | val_1_rmse: 0.59183 |  0:04:04s
epoch 48 | loss: 0.18161 | val_0_rmse: 0.41164 | val_1_rmse: 0.42024 |  0:04:09s
epoch 49 | loss: 0.17766 | val_0_rmse: 0.40873 | val_1_rmse: 0.41256 |  0:04:14s
epoch 50 | loss: 0.18162 | val_0_rmse: 0.392   | val_1_rmse: 0.39745 |  0:04:19s
epoch 51 | loss: 0.18047 | val_0_rmse: 0.39746 | val_1_rmse: 0.40628 |  0:04:24s
epoch 52 | loss: 0.17793 | val_0_rmse: 0.39409 | val_1_rmse: 0.40179 |  0:04:29s
epoch 53 | loss: 0.18518 | val_0_rmse: 0.39506 | val_1_rmse: 0.40546 |  0:04:34s
epoch 54 | loss: 0.18938 | val_0_rmse: 0.85437 | val_1_rmse: 0.85542 |  0:04:39s
epoch 55 | loss: 0.18841 | val_0_rmse: 0.48571 | val_1_rmse: 0.49111 |  0:04:45s
epoch 56 | loss: 0.17991 | val_0_rmse: 0.52312 | val_1_rmse: 0.52702 |  0:04:50s
epoch 57 | loss: 0.17585 | val_0_rmse: 0.66982 | val_1_rmse: 0.6656  |  0:04:55s
epoch 58 | loss: 0.17372 | val_0_rmse: 0.38278 | val_1_rmse: 0.38865 |  0:05:00s
epoch 59 | loss: 0.16521 | val_0_rmse: 0.43648 | val_1_rmse: 0.43993 |  0:05:05s
epoch 60 | loss: 0.16468 | val_0_rmse: 0.76489 | val_1_rmse: 0.76178 |  0:05:10s
epoch 61 | loss: 0.16133 | val_0_rmse: 0.56396 | val_1_rmse: 0.56614 |  0:05:15s
epoch 62 | loss: 0.16882 | val_0_rmse: 0.37331 | val_1_rmse: 0.37638 |  0:05:20s
epoch 63 | loss: 0.1619  | val_0_rmse: 0.37362 | val_1_rmse: 0.37906 |  0:05:25s
epoch 64 | loss: 0.17499 | val_0_rmse: 0.40885 | val_1_rmse: 0.41446 |  0:05:31s
epoch 65 | loss: 0.16987 | val_0_rmse: 0.54144 | val_1_rmse: 0.53869 |  0:05:36s
epoch 66 | loss: 0.16397 | val_0_rmse: 0.36619 | val_1_rmse: 0.37171 |  0:05:41s
epoch 67 | loss: 0.16734 | val_0_rmse: 0.37181 | val_1_rmse: 0.37767 |  0:05:46s
epoch 68 | loss: 0.16129 | val_0_rmse: 0.43494 | val_1_rmse: 0.44467 |  0:05:51s
epoch 69 | loss: 0.16304 | val_0_rmse: 0.63272 | val_1_rmse: 0.6378  |  0:05:56s
epoch 70 | loss: 0.16588 | val_0_rmse: 0.62496 | val_1_rmse: 0.62655 |  0:06:01s
epoch 71 | loss: 0.1587  | val_0_rmse: 0.57699 | val_1_rmse: 0.57403 |  0:06:06s
epoch 72 | loss: 0.15741 | val_0_rmse: 0.48532 | val_1_rmse: 0.48917 |  0:06:11s
epoch 73 | loss: 0.1602  | val_0_rmse: 0.55626 | val_1_rmse: 0.55711 |  0:06:16s
epoch 74 | loss: 0.16811 | val_0_rmse: 0.37658 | val_1_rmse: 0.38326 |  0:06:21s
epoch 75 | loss: 0.15541 | val_0_rmse: 0.35243 | val_1_rmse: 0.3609  |  0:06:27s
epoch 76 | loss: 0.16146 | val_0_rmse: 0.54154 | val_1_rmse: 0.54222 |  0:06:32s
epoch 77 | loss: 0.15756 | val_0_rmse: 0.4526  | val_1_rmse: 0.45449 |  0:06:37s
epoch 78 | loss: 0.15943 | val_0_rmse: 0.36701 | val_1_rmse: 0.37117 |  0:06:42s
epoch 79 | loss: 0.16714 | val_0_rmse: 0.56845 | val_1_rmse: 0.56313 |  0:06:47s
epoch 80 | loss: 0.15986 | val_0_rmse: 0.63241 | val_1_rmse: 0.63686 |  0:06:52s
epoch 81 | loss: 0.15299 | val_0_rmse: 0.40534 | val_1_rmse: 0.41319 |  0:06:57s
epoch 82 | loss: 0.15397 | val_0_rmse: 0.35328 | val_1_rmse: 0.36219 |  0:07:02s
epoch 83 | loss: 0.15095 | val_0_rmse: 0.37881 | val_1_rmse: 0.3852  |  0:07:08s
epoch 84 | loss: 0.15582 | val_0_rmse: 0.49703 | val_1_rmse: 0.50157 |  0:07:13s
epoch 85 | loss: 0.1752  | val_0_rmse: 0.46284 | val_1_rmse: 0.46436 |  0:07:18s
epoch 86 | loss: 0.16346 | val_0_rmse: 0.36219 | val_1_rmse: 0.37276 |  0:07:23s
epoch 87 | loss: 0.15557 | val_0_rmse: 0.36894 | val_1_rmse: 0.37571 |  0:07:28s
epoch 88 | loss: 0.16045 | val_0_rmse: 0.50961 | val_1_rmse: 0.51151 |  0:07:33s
epoch 89 | loss: 0.15367 | val_0_rmse: 0.57163 | val_1_rmse: 0.57341 |  0:07:38s
epoch 90 | loss: 0.15994 | val_0_rmse: 0.35985 | val_1_rmse: 0.36428 |  0:07:43s
epoch 91 | loss: 0.15817 | val_0_rmse: 0.40024 | val_1_rmse: 0.40424 |  0:07:48s
epoch 92 | loss: 0.15161 | val_0_rmse: 0.34357 | val_1_rmse: 0.35156 |  0:07:53s
epoch 93 | loss: 0.15613 | val_0_rmse: 0.36417 | val_1_rmse: 0.36821 |  0:07:58s
epoch 94 | loss: 0.15062 | val_0_rmse: 0.45612 | val_1_rmse: 0.45743 |  0:08:03s
epoch 95 | loss: 0.15052 | val_0_rmse: 0.47543 | val_1_rmse: 0.47767 |  0:08:08s
epoch 96 | loss: 0.15114 | val_0_rmse: 0.40961 | val_1_rmse: 0.41534 |  0:08:13s
epoch 97 | loss: 0.15378 | val_0_rmse: 0.4441  | val_1_rmse: 0.4503  |  0:08:18s
epoch 98 | loss: 0.1774  | val_0_rmse: 0.46789 | val_1_rmse: 0.47048 |  0:08:23s
epoch 99 | loss: 0.1669  | val_0_rmse: 0.35529 | val_1_rmse: 0.36448 |  0:08:29s
epoch 100| loss: 0.15595 | val_0_rmse: 0.66906 | val_1_rmse: 0.67685 |  0:08:34s
epoch 101| loss: 0.15778 | val_0_rmse: 0.37026 | val_1_rmse: 0.38112 |  0:08:39s
epoch 102| loss: 0.15653 | val_0_rmse: 0.38908 | val_1_rmse: 0.39818 |  0:08:44s
epoch 103| loss: 0.15055 | val_0_rmse: 0.34053 | val_1_rmse: 0.34671 |  0:08:49s
epoch 104| loss: 0.14953 | val_0_rmse: 0.396   | val_1_rmse: 0.40187 |  0:08:54s
epoch 105| loss: 0.1467  | val_0_rmse: 0.38144 | val_1_rmse: 0.38791 |  0:08:59s
epoch 106| loss: 0.14731 | val_0_rmse: 0.34726 | val_1_rmse: 0.35417 |  0:09:04s
epoch 107| loss: 0.16511 | val_0_rmse: 0.56718 | val_1_rmse: 0.56854 |  0:09:09s
epoch 108| loss: 0.15273 | val_0_rmse: 0.41862 | val_1_rmse: 0.42389 |  0:09:14s
epoch 109| loss: 0.15539 | val_0_rmse: 0.36595 | val_1_rmse: 0.37048 |  0:09:19s
epoch 110| loss: 0.1487  | val_0_rmse: 0.39833 | val_1_rmse: 0.40283 |  0:09:25s
epoch 111| loss: 0.14498 | val_0_rmse: 0.46795 | val_1_rmse: 0.46838 |  0:09:30s
epoch 112| loss: 0.14527 | val_0_rmse: 0.35166 | val_1_rmse: 0.36014 |  0:09:35s
epoch 113| loss: 0.14563 | val_0_rmse: 0.41031 | val_1_rmse: 0.41061 |  0:09:40s
epoch 114| loss: 0.14511 | val_0_rmse: 0.49922 | val_1_rmse: 0.50228 |  0:09:45s
epoch 115| loss: 0.15841 | val_0_rmse: 0.46559 | val_1_rmse: 0.4707  |  0:09:50s
epoch 116| loss: 0.15233 | val_0_rmse: 0.35563 | val_1_rmse: 0.36289 |  0:09:55s
epoch 117| loss: 0.15095 | val_0_rmse: 0.39541 | val_1_rmse: 0.40263 |  0:10:00s
epoch 118| loss: 0.14987 | val_0_rmse: 0.34387 | val_1_rmse: 0.35221 |  0:10:06s
epoch 119| loss: 0.14541 | val_0_rmse: 0.34974 | val_1_rmse: 0.35575 |  0:10:11s
epoch 120| loss: 0.14914 | val_0_rmse: 0.34725 | val_1_rmse: 0.35254 |  0:10:16s
epoch 121| loss: 0.14335 | val_0_rmse: 0.48429 | val_1_rmse: 0.48137 |  0:10:21s
epoch 122| loss: 0.15101 | val_0_rmse: 0.35082 | val_1_rmse: 0.35654 |  0:10:26s
epoch 123| loss: 0.13837 | val_0_rmse: 0.34221 | val_1_rmse: 0.34959 |  0:10:31s
epoch 124| loss: 0.14862 | val_0_rmse: 0.49374 | val_1_rmse: 0.49812 |  0:10:36s
epoch 125| loss: 0.14578 | val_0_rmse: 0.37012 | val_1_rmse: 0.37962 |  0:10:41s
epoch 126| loss: 0.14651 | val_0_rmse: 0.50932 | val_1_rmse: 0.51154 |  0:10:46s
epoch 127| loss: 0.14447 | val_0_rmse: 0.86758 | val_1_rmse: 0.86916 |  0:10:51s
epoch 128| loss: 0.15878 | val_0_rmse: 0.43484 | val_1_rmse: 0.44348 |  0:10:57s
epoch 129| loss: 0.14764 | val_0_rmse: 0.44182 | val_1_rmse: 0.44543 |  0:11:02s
epoch 130| loss: 0.1524  | val_0_rmse: 0.38152 | val_1_rmse: 0.38563 |  0:11:07s
epoch 131| loss: 0.15708 | val_0_rmse: 0.39636 | val_1_rmse: 0.39999 |  0:11:12s
epoch 132| loss: 0.1502  | val_0_rmse: 0.37431 | val_1_rmse: 0.3796  |  0:11:17s
epoch 133| loss: 0.14236 | val_0_rmse: 0.44509 | val_1_rmse: 0.44715 |  0:11:22s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.34671
Best weights from best epoch are automatically used!
ended training at: 07:10:18
Feature importance:
[('Area', 0.27984196985646065), ('Baths', 0.18931379185568184), ('Beds', 0.07725374908874318), ('Latitude', 0.06574968461369421), ('Longitude', 0.19686074632749517), ('Month', 0.168890454256969), ('Year', 0.022089604000955974)]
Mean squared error is of 882813371.7211177
Mean absolute error:20659.0965742323
MAPE:0.1908083850748147
R2 score:0.876909467463735
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:16:46
epoch 0  | loss: 0.57324 | val_0_rmse: 0.69095 | val_1_rmse: 0.68923 |  0:00:05s
epoch 1  | loss: 0.48619 | val_0_rmse: 0.68577 | val_1_rmse: 0.67946 |  0:00:09s
epoch 2  | loss: 0.47584 | val_0_rmse: 0.68379 | val_1_rmse: 0.67838 |  0:00:15s
epoch 3  | loss: 0.47569 | val_0_rmse: 0.68546 | val_1_rmse: 0.68313 |  0:00:19s
epoch 4  | loss: 0.47463 | val_0_rmse: 0.68533 | val_1_rmse: 0.68333 |  0:00:25s
epoch 5  | loss: 0.47712 | val_0_rmse: 0.68703 | val_1_rmse: 0.68335 |  0:00:29s
epoch 6  | loss: 0.47319 | val_0_rmse: 0.68203 | val_1_rmse: 0.67766 |  0:00:35s
epoch 7  | loss: 0.46907 | val_0_rmse: 0.67865 | val_1_rmse: 0.67617 |  0:00:40s
epoch 8  | loss: 0.47325 | val_0_rmse: 0.69054 | val_1_rmse: 0.69037 |  0:00:45s
epoch 9  | loss: 0.47955 | val_0_rmse: 0.68627 | val_1_rmse: 0.68423 |  0:00:50s
epoch 10 | loss: 0.47694 | val_0_rmse: 0.69287 | val_1_rmse: 0.68926 |  0:00:55s
epoch 11 | loss: 0.47897 | val_0_rmse: 0.68345 | val_1_rmse: 0.67942 |  0:01:00s
epoch 12 | loss: 0.47104 | val_0_rmse: 0.68721 | val_1_rmse: 0.685   |  0:01:05s
epoch 13 | loss: 0.47405 | val_0_rmse: 0.68312 | val_1_rmse: 0.67926 |  0:01:10s
epoch 14 | loss: 0.47154 | val_0_rmse: 0.67753 | val_1_rmse: 0.67446 |  0:01:15s
epoch 15 | loss: 0.46548 | val_0_rmse: 0.6792  | val_1_rmse: 0.67722 |  0:01:20s
epoch 16 | loss: 0.46739 | val_0_rmse: 0.6792  | val_1_rmse: 0.67549 |  0:01:26s
epoch 17 | loss: 0.4638  | val_0_rmse: 0.67673 | val_1_rmse: 0.6741  |  0:01:31s
epoch 18 | loss: 0.46106 | val_0_rmse: 0.68052 | val_1_rmse: 0.67929 |  0:01:36s
epoch 19 | loss: 0.46406 | val_0_rmse: 0.67754 | val_1_rmse: 0.6763  |  0:01:41s
epoch 20 | loss: 0.46004 | val_0_rmse: 0.67745 | val_1_rmse: 0.67719 |  0:01:46s
epoch 21 | loss: 0.46552 | val_0_rmse: 0.67986 | val_1_rmse: 0.68126 |  0:01:51s
epoch 22 | loss: 0.46145 | val_0_rmse: 0.68155 | val_1_rmse: 0.67715 |  0:01:56s
epoch 23 | loss: 0.46527 | val_0_rmse: 0.68164 | val_1_rmse: 0.67967 |  0:02:01s
epoch 24 | loss: 0.46315 | val_0_rmse: 0.67848 | val_1_rmse: 0.67621 |  0:02:06s
epoch 25 | loss: 0.46919 | val_0_rmse: 0.68205 | val_1_rmse: 0.67881 |  0:02:11s
epoch 26 | loss: 0.46646 | val_0_rmse: 0.67495 | val_1_rmse: 0.67281 |  0:02:17s
epoch 27 | loss: 0.46263 | val_0_rmse: 0.67328 | val_1_rmse: 0.67015 |  0:02:22s
epoch 28 | loss: 0.46272 | val_0_rmse: 0.6812  | val_1_rmse: 0.67662 |  0:02:27s
epoch 29 | loss: 0.4648  | val_0_rmse: 0.6751  | val_1_rmse: 0.67216 |  0:02:32s
epoch 30 | loss: 0.46186 | val_0_rmse: 0.69227 | val_1_rmse: 0.69186 |  0:02:37s
epoch 31 | loss: 0.4686  | val_0_rmse: 0.67902 | val_1_rmse: 0.67622 |  0:02:42s
epoch 32 | loss: 0.47192 | val_0_rmse: 0.68174 | val_1_rmse: 0.67982 |  0:02:47s
epoch 33 | loss: 0.47017 | val_0_rmse: 0.68036 | val_1_rmse: 0.67814 |  0:02:52s
epoch 34 | loss: 0.46764 | val_0_rmse: 0.68135 | val_1_rmse: 0.67876 |  0:02:57s
epoch 35 | loss: 0.46654 | val_0_rmse: 0.6839  | val_1_rmse: 0.68238 |  0:03:02s
epoch 36 | loss: 0.47529 | val_0_rmse: 0.6856  | val_1_rmse: 0.68547 |  0:03:07s
epoch 37 | loss: 0.47179 | val_0_rmse: 0.69226 | val_1_rmse: 0.68931 |  0:03:13s
epoch 38 | loss: 0.48135 | val_0_rmse: 0.69071 | val_1_rmse: 0.6868  |  0:03:18s
epoch 39 | loss: 0.47939 | val_0_rmse: 0.69126 | val_1_rmse: 0.6898  |  0:03:23s
epoch 40 | loss: 0.47596 | val_0_rmse: 0.69383 | val_1_rmse: 0.69168 |  0:03:28s
epoch 41 | loss: 0.47517 | val_0_rmse: 0.68183 | val_1_rmse: 0.67911 |  0:03:33s
epoch 42 | loss: 0.47263 | val_0_rmse: 0.68236 | val_1_rmse: 0.67953 |  0:03:38s
epoch 43 | loss: 0.46955 | val_0_rmse: 0.68329 | val_1_rmse: 0.68004 |  0:03:44s
epoch 44 | loss: 0.46715 | val_0_rmse: 0.68014 | val_1_rmse: 0.67666 |  0:03:49s
epoch 45 | loss: 0.46311 | val_0_rmse: 0.67338 | val_1_rmse: 0.66983 |  0:03:54s
epoch 46 | loss: 0.46196 | val_0_rmse: 0.67938 | val_1_rmse: 0.67511 |  0:03:59s
epoch 47 | loss: 0.45622 | val_0_rmse: 0.69524 | val_1_rmse: 0.68642 |  0:04:04s
epoch 48 | loss: 0.46224 | val_0_rmse: 0.68154 | val_1_rmse: 0.67771 |  0:04:09s
epoch 49 | loss: 0.44562 | val_0_rmse: 0.65585 | val_1_rmse: 0.65428 |  0:04:14s
epoch 50 | loss: 0.46498 | val_0_rmse: 0.79765 | val_1_rmse: 0.79064 |  0:04:19s
epoch 51 | loss: 0.4927  | val_0_rmse: 0.68958 | val_1_rmse: 0.6887  |  0:04:25s
epoch 52 | loss: 0.47996 | val_0_rmse: 0.69026 | val_1_rmse: 0.68756 |  0:04:30s
epoch 53 | loss: 0.47289 | val_0_rmse: 0.68577 | val_1_rmse: 0.68349 |  0:04:35s
epoch 54 | loss: 0.47068 | val_0_rmse: 0.6815  | val_1_rmse: 0.67972 |  0:04:40s
epoch 55 | loss: 0.46873 | val_0_rmse: 0.67844 | val_1_rmse: 0.67659 |  0:04:45s
epoch 56 | loss: 0.46859 | val_0_rmse: 0.68365 | val_1_rmse: 0.68003 |  0:04:50s
epoch 57 | loss: 0.46759 | val_0_rmse: 0.68797 | val_1_rmse: 0.68477 |  0:04:55s
epoch 58 | loss: 0.4699  | val_0_rmse: 0.6817  | val_1_rmse: 0.67886 |  0:05:00s
epoch 59 | loss: 0.46969 | val_0_rmse: 0.68242 | val_1_rmse: 0.6809  |  0:05:05s
epoch 60 | loss: 0.4667  | val_0_rmse: 0.67993 | val_1_rmse: 0.67782 |  0:05:11s
epoch 61 | loss: 0.46902 | val_0_rmse: 0.67834 | val_1_rmse: 0.67519 |  0:05:16s
epoch 62 | loss: 0.46596 | val_0_rmse: 0.67656 | val_1_rmse: 0.67472 |  0:05:21s
epoch 63 | loss: 0.46395 | val_0_rmse: 0.68361 | val_1_rmse: 0.68036 |  0:05:26s
epoch 64 | loss: 0.46485 | val_0_rmse: 0.69551 | val_1_rmse: 0.696   |  0:05:31s
epoch 65 | loss: 0.46788 | val_0_rmse: 0.68017 | val_1_rmse: 0.67945 |  0:05:36s
epoch 66 | loss: 0.46144 | val_0_rmse: 0.68525 | val_1_rmse: 0.68468 |  0:05:41s
epoch 67 | loss: 0.46134 | val_0_rmse: 0.67638 | val_1_rmse: 0.6754  |  0:05:46s
epoch 68 | loss: 0.46526 | val_0_rmse: 0.68231 | val_1_rmse: 0.68001 |  0:05:51s
epoch 69 | loss: 0.46535 | val_0_rmse: 0.68366 | val_1_rmse: 0.68009 |  0:05:56s
epoch 70 | loss: 0.4661  | val_0_rmse: 0.67605 | val_1_rmse: 0.67529 |  0:06:02s
epoch 71 | loss: 0.46486 | val_0_rmse: 0.67829 | val_1_rmse: 0.67535 |  0:06:07s
epoch 72 | loss: 0.46384 | val_0_rmse: 0.67449 | val_1_rmse: 0.67272 |  0:06:12s
epoch 73 | loss: 0.46354 | val_0_rmse: 0.70865 | val_1_rmse: 0.71048 |  0:06:17s
epoch 74 | loss: 0.46337 | val_0_rmse: 0.67766 | val_1_rmse: 0.67587 |  0:06:22s
epoch 75 | loss: 0.46429 | val_0_rmse: 0.68613 | val_1_rmse: 0.68286 |  0:06:27s
epoch 76 | loss: 0.46567 | val_0_rmse: 0.68316 | val_1_rmse: 0.68317 |  0:06:32s
epoch 77 | loss: 0.46551 | val_0_rmse: 0.67528 | val_1_rmse: 0.67391 |  0:06:37s
epoch 78 | loss: 0.46277 | val_0_rmse: 0.68399 | val_1_rmse: 0.68075 |  0:06:43s
epoch 79 | loss: 0.46261 | val_0_rmse: 0.67617 | val_1_rmse: 0.67439 |  0:06:48s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.65428
Best weights from best epoch are automatically used!
ended training at: 07:23:35
Feature importance:
[('Area', 0.581562061658616), ('Baths', 0.25200473498574955), ('Beds', 0.031199548673335874), ('Latitude', 0.041443923429719316), ('Longitude', 0.0937897312525793), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 3647442395.279638
Mean absolute error:42238.8158061042
MAPE:0.3732090346995439
R2 score:0.5653315195418892
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:32
epoch 0  | loss: 0.40237 | val_0_rmse: 0.57378 | val_1_rmse: 0.57978 |  0:00:04s
epoch 1  | loss: 0.33308 | val_0_rmse: 0.57175 | val_1_rmse: 0.57725 |  0:00:09s
epoch 2  | loss: 0.32907 | val_0_rmse: 0.56501 | val_1_rmse: 0.57153 |  0:00:14s
epoch 3  | loss: 0.32516 | val_0_rmse: 0.56706 | val_1_rmse: 0.57321 |  0:00:19s
epoch 4  | loss: 0.32358 | val_0_rmse: 0.56629 | val_1_rmse: 0.57085 |  0:00:24s
epoch 5  | loss: 0.32375 | val_0_rmse: 0.56278 | val_1_rmse: 0.56911 |  0:00:30s
epoch 6  | loss: 0.3228  | val_0_rmse: 0.56283 | val_1_rmse: 0.56902 |  0:00:35s
epoch 7  | loss: 0.32112 | val_0_rmse: 0.56255 | val_1_rmse: 0.56864 |  0:00:40s
epoch 8  | loss: 0.31941 | val_0_rmse: 0.56217 | val_1_rmse: 0.56774 |  0:00:45s
epoch 9  | loss: 0.32122 | val_0_rmse: 0.56404 | val_1_rmse: 0.57083 |  0:00:50s
epoch 10 | loss: 0.32369 | val_0_rmse: 0.58757 | val_1_rmse: 0.59274 |  0:00:55s
epoch 11 | loss: 0.32744 | val_0_rmse: 0.56363 | val_1_rmse: 0.56861 |  0:01:00s
epoch 12 | loss: 0.32138 | val_0_rmse: 0.57481 | val_1_rmse: 0.58101 |  0:01:05s
epoch 13 | loss: 0.31561 | val_0_rmse: 0.55749 | val_1_rmse: 0.56491 |  0:01:10s
epoch 14 | loss: 0.31646 | val_0_rmse: 0.56983 | val_1_rmse: 0.57581 |  0:01:15s
epoch 15 | loss: 0.31753 | val_0_rmse: 0.55313 | val_1_rmse: 0.5595  |  0:01:20s
epoch 16 | loss: 0.3092  | val_0_rmse: 0.56274 | val_1_rmse: 0.56913 |  0:01:26s
epoch 17 | loss: 0.30207 | val_0_rmse: 0.54349 | val_1_rmse: 0.54887 |  0:01:31s
epoch 18 | loss: 0.30878 | val_0_rmse: 0.57764 | val_1_rmse: 0.58336 |  0:01:36s
epoch 19 | loss: 0.30011 | val_0_rmse: 0.58509 | val_1_rmse: 0.59091 |  0:01:41s
epoch 20 | loss: 0.30227 | val_0_rmse: 0.57449 | val_1_rmse: 0.58126 |  0:01:46s
epoch 21 | loss: 0.29939 | val_0_rmse: 0.54286 | val_1_rmse: 0.54836 |  0:01:51s
epoch 22 | loss: 0.29717 | val_0_rmse: 0.6056  | val_1_rmse: 0.61352 |  0:01:56s
epoch 23 | loss: 0.29915 | val_0_rmse: 0.54867 | val_1_rmse: 0.55317 |  0:02:01s
epoch 24 | loss: 0.29915 | val_0_rmse: 0.60006 | val_1_rmse: 0.60474 |  0:02:06s
epoch 25 | loss: 0.29843 | val_0_rmse: 0.53985 | val_1_rmse: 0.54544 |  0:02:11s
epoch 26 | loss: 0.2963  | val_0_rmse: 0.58539 | val_1_rmse: 0.59099 |  0:02:16s
epoch 27 | loss: 0.30279 | val_0_rmse: 0.55298 | val_1_rmse: 0.55921 |  0:02:22s
epoch 28 | loss: 0.29507 | val_0_rmse: 0.54226 | val_1_rmse: 0.54756 |  0:02:27s
epoch 29 | loss: 0.2935  | val_0_rmse: 0.55253 | val_1_rmse: 0.55778 |  0:02:32s
epoch 30 | loss: 0.293   | val_0_rmse: 0.56892 | val_1_rmse: 0.57403 |  0:02:37s
epoch 31 | loss: 0.29386 | val_0_rmse: 0.54017 | val_1_rmse: 0.54495 |  0:02:42s
epoch 32 | loss: 0.29679 | val_0_rmse: 0.56391 | val_1_rmse: 0.57033 |  0:02:47s
epoch 33 | loss: 0.29313 | val_0_rmse: 0.55652 | val_1_rmse: 0.56249 |  0:02:52s
epoch 34 | loss: 0.29568 | val_0_rmse: 0.53479 | val_1_rmse: 0.5403  |  0:02:57s
epoch 35 | loss: 0.29381 | val_0_rmse: 0.57058 | val_1_rmse: 0.57757 |  0:03:02s
epoch 36 | loss: 0.29307 | val_0_rmse: 0.53372 | val_1_rmse: 0.53847 |  0:03:07s
epoch 37 | loss: 0.29023 | val_0_rmse: 0.59935 | val_1_rmse: 0.60401 |  0:03:12s
epoch 38 | loss: 0.29234 | val_0_rmse: 0.55501 | val_1_rmse: 0.56065 |  0:03:17s
epoch 39 | loss: 0.29427 | val_0_rmse: 0.61736 | val_1_rmse: 0.62131 |  0:03:23s
epoch 40 | loss: 0.29102 | val_0_rmse: 0.56854 | val_1_rmse: 0.5771  |  0:03:28s
epoch 41 | loss: 0.29585 | val_0_rmse: 0.53864 | val_1_rmse: 0.54297 |  0:03:33s
epoch 42 | loss: 0.29589 | val_0_rmse: 0.59906 | val_1_rmse: 0.60368 |  0:03:38s
epoch 43 | loss: 0.30514 | val_0_rmse: 0.99941 | val_1_rmse: 1.00384 |  0:03:43s
epoch 44 | loss: 0.29902 | val_0_rmse: 0.54506 | val_1_rmse: 0.54946 |  0:03:48s
epoch 45 | loss: 0.29708 | val_0_rmse: 0.53605 | val_1_rmse: 0.54223 |  0:03:53s
epoch 46 | loss: 0.29517 | val_0_rmse: 0.54355 | val_1_rmse: 0.54897 |  0:03:58s
epoch 47 | loss: 0.29157 | val_0_rmse: 0.58547 | val_1_rmse: 0.59221 |  0:04:03s
epoch 48 | loss: 0.29156 | val_0_rmse: 0.58198 | val_1_rmse: 0.58818 |  0:04:08s
epoch 49 | loss: 0.29333 | val_0_rmse: 0.59307 | val_1_rmse: 0.59987 |  0:04:13s
epoch 50 | loss: 0.29064 | val_0_rmse: 0.56939 | val_1_rmse: 0.57583 |  0:04:19s
epoch 51 | loss: 0.28986 | val_0_rmse: 0.60418 | val_1_rmse: 0.60946 |  0:04:24s
epoch 52 | loss: 0.29137 | val_0_rmse: 0.54216 | val_1_rmse: 0.54562 |  0:04:29s
epoch 53 | loss: 0.28955 | val_0_rmse: 0.57505 | val_1_rmse: 0.58035 |  0:04:34s
epoch 54 | loss: 0.29339 | val_0_rmse: 0.53725 | val_1_rmse: 0.54306 |  0:04:40s
epoch 55 | loss: 0.29191 | val_0_rmse: 0.6415  | val_1_rmse: 0.6444  |  0:04:45s
epoch 56 | loss: 0.2906  | val_0_rmse: 0.56674 | val_1_rmse: 0.57337 |  0:04:50s
epoch 57 | loss: 0.28835 | val_0_rmse: 0.5295  | val_1_rmse: 0.53535 |  0:04:55s
epoch 58 | loss: 0.28815 | val_0_rmse: 0.55949 | val_1_rmse: 0.56462 |  0:05:00s
epoch 59 | loss: 0.28875 | val_0_rmse: 0.54034 | val_1_rmse: 0.54696 |  0:05:05s
epoch 60 | loss: 0.28849 | val_0_rmse: 0.59812 | val_1_rmse: 0.60261 |  0:05:11s
epoch 61 | loss: 0.29476 | val_0_rmse: 0.58404 | val_1_rmse: 0.58909 |  0:05:16s
epoch 62 | loss: 0.28802 | val_0_rmse: 0.56176 | val_1_rmse: 0.56781 |  0:05:21s
epoch 63 | loss: 0.28862 | val_0_rmse: 0.5307  | val_1_rmse: 0.53755 |  0:05:26s
epoch 64 | loss: 0.28841 | val_0_rmse: 0.59505 | val_1_rmse: 0.59879 |  0:05:31s
epoch 65 | loss: 0.28771 | val_0_rmse: 0.53486 | val_1_rmse: 0.54139 |  0:05:37s
epoch 66 | loss: 0.29721 | val_0_rmse: 0.5604  | val_1_rmse: 0.56814 |  0:05:42s
epoch 67 | loss: 0.30686 | val_0_rmse: 0.5773  | val_1_rmse: 0.58256 |  0:05:47s
epoch 68 | loss: 0.29851 | val_0_rmse: 0.55363 | val_1_rmse: 0.56042 |  0:05:52s
epoch 69 | loss: 0.28891 | val_0_rmse: 0.60022 | val_1_rmse: 0.60552 |  0:05:57s
epoch 70 | loss: 0.30462 | val_0_rmse: 0.57109 | val_1_rmse: 0.57795 |  0:06:02s
epoch 71 | loss: 0.32061 | val_0_rmse: 0.56124 | val_1_rmse: 0.56893 |  0:06:07s
epoch 72 | loss: 0.32139 | val_0_rmse: 0.56677 | val_1_rmse: 0.57364 |  0:06:13s
epoch 73 | loss: 0.32058 | val_0_rmse: 0.57286 | val_1_rmse: 0.57911 |  0:06:18s
epoch 74 | loss: 0.32033 | val_0_rmse: 0.56079 | val_1_rmse: 0.56761 |  0:06:23s
epoch 75 | loss: 0.32129 | val_0_rmse: 0.55998 | val_1_rmse: 0.56723 |  0:06:28s
epoch 76 | loss: 0.32066 | val_0_rmse: 0.56232 | val_1_rmse: 0.56992 |  0:06:33s
epoch 77 | loss: 0.31814 | val_0_rmse: 0.5604  | val_1_rmse: 0.56731 |  0:06:38s
epoch 78 | loss: 0.32137 | val_0_rmse: 0.57289 | val_1_rmse: 0.57968 |  0:06:44s
epoch 79 | loss: 0.32908 | val_0_rmse: 0.56947 | val_1_rmse: 0.5774  |  0:06:49s
epoch 80 | loss: 0.32916 | val_0_rmse: 0.56606 | val_1_rmse: 0.573   |  0:06:54s
epoch 81 | loss: 0.32622 | val_0_rmse: 0.5718  | val_1_rmse: 0.57979 |  0:06:59s
epoch 82 | loss: 0.32653 | val_0_rmse: 0.56688 | val_1_rmse: 0.57358 |  0:07:04s
epoch 83 | loss: 0.32343 | val_0_rmse: 0.56157 | val_1_rmse: 0.56694 |  0:07:09s
epoch 84 | loss: 0.32165 | val_0_rmse: 0.56395 | val_1_rmse: 0.56897 |  0:07:14s
epoch 85 | loss: 0.32139 | val_0_rmse: 0.5629  | val_1_rmse: 0.56827 |  0:07:20s
epoch 86 | loss: 0.31922 | val_0_rmse: 0.56279 | val_1_rmse: 0.56752 |  0:07:25s
epoch 87 | loss: 0.32007 | val_0_rmse: 0.56137 | val_1_rmse: 0.56698 |  0:07:30s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.53535
Best weights from best epoch are automatically used!
ended training at: 07:33:04
Feature importance:
[('Area', 0.5883521297729771), ('Baths', 0.06522780075202629), ('Beds', 0.16711734677759396), ('Latitude', 0.0362848913014476), ('Longitude', 0.051714146622147045), ('Month', 0.06756233585830058), ('Year', 0.023741348915507426)]
Mean squared error is of 955010179.6564374
Mean absolute error:21676.842779965045
MAPE:0.36084945654519446
R2 score:0.7151397092178204
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:33:05
epoch 0  | loss: 0.34497 | val_0_rmse: 0.55162 | val_1_rmse: 0.55107 |  0:00:45s
epoch 1  | loss: 0.30542 | val_0_rmse: 0.55025 | val_1_rmse: 0.5491  |  0:01:31s
epoch 2  | loss: 0.30364 | val_0_rmse: 0.54699 | val_1_rmse: 0.54667 |  0:02:16s
epoch 3  | loss: 0.30187 | val_0_rmse: 0.54136 | val_1_rmse: 0.54023 |  0:03:01s
epoch 4  | loss: 0.29795 | val_0_rmse: 0.54499 | val_1_rmse: 0.54264 |  0:03:47s
epoch 5  | loss: 0.30071 | val_0_rmse: 0.54191 | val_1_rmse: 0.54091 |  0:04:32s
epoch 6  | loss: 0.30361 | val_0_rmse: 0.55164 | val_1_rmse: 0.54992 |  0:05:18s
epoch 7  | loss: 0.29746 | val_0_rmse: 0.5419  | val_1_rmse: 0.54117 |  0:06:03s
epoch 8  | loss: 0.29546 | val_0_rmse: 0.5468  | val_1_rmse: 0.54607 |  0:06:49s
epoch 9  | loss: 0.29396 | val_0_rmse: 0.57606 | val_1_rmse: 0.57684 |  0:07:34s
epoch 10 | loss: 0.29445 | val_0_rmse: 0.59751 | val_1_rmse: 0.5976  |  0:08:21s
epoch 11 | loss: 0.29436 | val_0_rmse: 0.53984 | val_1_rmse: 0.53944 |  0:09:05s
epoch 12 | loss: 0.29266 | val_0_rmse: 0.54084 | val_1_rmse: 0.54006 |  0:09:51s
epoch 13 | loss: 0.29312 | val_0_rmse: 0.54022 | val_1_rmse: 0.53913 |  0:10:37s
epoch 14 | loss: 0.30911 | val_0_rmse: 0.54414 | val_1_rmse: 0.54285 |  0:11:22s
epoch 15 | loss: 0.29383 | val_0_rmse: 0.554   | val_1_rmse: 0.55336 |  0:12:08s
epoch 16 | loss: 0.29257 | val_0_rmse: 0.54039 | val_1_rmse: 0.5392  |  0:12:53s
epoch 17 | loss: 0.29097 | val_0_rmse: 0.5372  | val_1_rmse: 0.5359  |  0:13:39s
epoch 18 | loss: 0.29185 | val_0_rmse: 0.54027 | val_1_rmse: 0.53929 |  0:14:25s
epoch 19 | loss: 0.29168 | val_0_rmse: 0.53651 | val_1_rmse: 0.53543 |  0:15:10s
epoch 20 | loss: 0.29036 | val_0_rmse: 0.54124 | val_1_rmse: 0.54016 |  0:15:56s
epoch 21 | loss: 0.28976 | val_0_rmse: 0.53543 | val_1_rmse: 0.53456 |  0:16:41s
epoch 22 | loss: 0.28941 | val_0_rmse: 0.53539 | val_1_rmse: 0.53429 |  0:17:26s
epoch 23 | loss: 0.28847 | val_0_rmse: 0.56022 | val_1_rmse: 0.55937 |  0:18:11s
epoch 24 | loss: 0.28848 | val_0_rmse: 0.5495  | val_1_rmse: 0.54873 |  0:18:57s
epoch 25 | loss: 0.29028 | val_0_rmse: 0.54941 | val_1_rmse: 0.54824 |  0:19:42s
epoch 26 | loss: 0.29058 | val_0_rmse: 0.54246 | val_1_rmse: 0.54146 |  0:20:27s
epoch 27 | loss: 0.29772 | val_0_rmse: 0.58768 | val_1_rmse: 0.58634 |  0:21:12s
epoch 28 | loss: 0.30898 | val_0_rmse: 0.54021 | val_1_rmse: 0.53897 |  0:21:58s
epoch 29 | loss: 0.29598 | val_0_rmse: 0.54483 | val_1_rmse: 0.54306 |  0:22:43s
epoch 30 | loss: 0.29149 | val_0_rmse: 0.5388  | val_1_rmse: 0.537   |  0:23:29s
epoch 31 | loss: 0.29176 | val_0_rmse: 0.54654 | val_1_rmse: 0.54501 |  0:24:14s
epoch 32 | loss: 0.28999 | val_0_rmse: 0.54006 | val_1_rmse: 0.53859 |  0:25:00s
epoch 33 | loss: 0.29112 | val_0_rmse: 0.5389  | val_1_rmse: 0.53836 |  0:25:45s
epoch 34 | loss: 0.29185 | val_0_rmse: 0.70103 | val_1_rmse: 0.70434 |  0:26:31s
epoch 35 | loss: 0.30994 | val_0_rmse: 0.55558 | val_1_rmse: 0.55463 |  0:27:17s
epoch 36 | loss: 0.29568 | val_0_rmse: 0.5412  | val_1_rmse: 0.54063 |  0:28:03s
epoch 37 | loss: 0.29236 | val_0_rmse: 0.53482 | val_1_rmse: 0.53346 |  0:28:48s
epoch 38 | loss: 0.29266 | val_0_rmse: 0.54162 | val_1_rmse: 0.54098 |  0:29:33s
epoch 39 | loss: 0.29288 | val_0_rmse: 0.53674 | val_1_rmse: 0.5354  |  0:30:19s
epoch 40 | loss: 0.28884 | val_0_rmse: 0.53348 | val_1_rmse: 0.53205 |  0:31:04s
epoch 41 | loss: 0.28821 | val_0_rmse: 0.5482  | val_1_rmse: 0.54761 |  0:31:49s
epoch 42 | loss: 0.28752 | val_0_rmse: 0.53809 | val_1_rmse: 0.53608 |  0:32:35s
epoch 43 | loss: 0.28697 | val_0_rmse: 0.60633 | val_1_rmse: 0.60731 |  0:33:20s
epoch 44 | loss: 0.28661 | val_0_rmse: 0.54089 | val_1_rmse: 0.53971 |  0:34:05s
epoch 45 | loss: 0.28676 | val_0_rmse: 0.53431 | val_1_rmse: 0.53344 |  0:34:51s
epoch 46 | loss: 0.29747 | val_0_rmse: 0.62151 | val_1_rmse: 0.62005 |  0:35:37s
epoch 47 | loss: 0.32396 | val_0_rmse: 0.54492 | val_1_rmse: 0.54297 |  0:36:22s
epoch 48 | loss: 0.30086 | val_0_rmse: 0.53899 | val_1_rmse: 0.53806 |  0:37:07s
epoch 49 | loss: 0.29286 | val_0_rmse: 0.53556 | val_1_rmse: 0.5342  |  0:37:52s
epoch 50 | loss: 0.29901 | val_0_rmse: 0.53401 | val_1_rmse: 0.53208 |  0:38:38s
epoch 51 | loss: 0.29182 | val_0_rmse: 0.56177 | val_1_rmse: 0.55944 |  0:39:24s
epoch 52 | loss: 0.29106 | val_0_rmse: 0.56616 | val_1_rmse: 0.56443 |  0:40:10s
epoch 53 | loss: 0.28954 | val_0_rmse: 0.52914 | val_1_rmse: 0.52739 |  0:40:55s
epoch 54 | loss: 0.28674 | val_0_rmse: 0.53141 | val_1_rmse: 0.53036 |  0:41:40s
epoch 55 | loss: 0.28407 | val_0_rmse: 0.53855 | val_1_rmse: 0.53769 |  0:42:25s
epoch 56 | loss: 0.28105 | val_0_rmse: 0.5428  | val_1_rmse: 0.54236 |  0:43:11s
epoch 57 | loss: 0.27987 | val_0_rmse: 0.60723 | val_1_rmse: 0.60581 |  0:43:56s
epoch 58 | loss: 0.27635 | val_0_rmse: 0.57387 | val_1_rmse: 0.57218 |  0:44:42s
epoch 59 | loss: 0.27604 | val_0_rmse: 0.55427 | val_1_rmse: 0.55255 |  0:45:27s
epoch 60 | loss: 0.2807  | val_0_rmse: 0.68581 | val_1_rmse: 0.68439 |  0:46:13s
epoch 61 | loss: 0.27485 | val_0_rmse: 0.66423 | val_1_rmse: 0.66245 |  0:46:59s
epoch 62 | loss: 0.27581 | val_0_rmse: 0.72669 | val_1_rmse: 0.72512 |  0:47:44s
epoch 63 | loss: 0.29039 | val_0_rmse: 0.64701 | val_1_rmse: 0.64728 |  0:48:29s
epoch 64 | loss: 0.30114 | val_0_rmse: 0.73649 | val_1_rmse: 0.7353  |  0:49:14s
epoch 65 | loss: 0.29028 | val_0_rmse: 0.5841  | val_1_rmse: 0.58283 |  0:50:00s
epoch 66 | loss: 0.29542 | val_0_rmse: 0.52257 | val_1_rmse: 0.52057 |  0:50:45s
epoch 67 | loss: 0.28042 | val_0_rmse: 0.58809 | val_1_rmse: 0.58671 |  0:51:30s
epoch 68 | loss: 0.29112 | val_0_rmse: 0.62881 | val_1_rmse: 0.62726 |  0:52:15s
epoch 69 | loss: 0.28593 | val_0_rmse: 0.55832 | val_1_rmse: 0.55832 |  0:53:01s
epoch 70 | loss: 0.2797  | val_0_rmse: 0.56127 | val_1_rmse: 0.55941 |  0:53:47s
epoch 71 | loss: 0.27402 | val_0_rmse: 0.73346 | val_1_rmse: 0.732   |  0:54:32s
epoch 72 | loss: 0.28498 | val_0_rmse: 0.56009 | val_1_rmse: 0.55824 |  0:55:17s
epoch 73 | loss: 0.28469 | val_0_rmse: 0.60154 | val_1_rmse: 0.60006 |  0:56:03s
epoch 74 | loss: 0.28116 | val_0_rmse: 0.75053 | val_1_rmse: 0.74959 |  0:56:49s
epoch 75 | loss: 0.2747  | val_0_rmse: 0.62021 | val_1_rmse: 0.61849 |  0:57:34s
epoch 76 | loss: 0.27967 | val_0_rmse: 0.76749 | val_1_rmse: 0.76621 |  0:58:19s
epoch 77 | loss: 0.27376 | val_0_rmse: 0.78721 | val_1_rmse: 0.78641 |  0:59:05s
epoch 78 | loss: 0.27129 | val_0_rmse: 0.6808  | val_1_rmse: 0.67978 |  0:59:50s
epoch 79 | loss: 0.26959 | val_0_rmse: 0.55834 | val_1_rmse: 0.55656 |  1:00:36s
epoch 80 | loss: 0.26948 | val_0_rmse: 0.52348 | val_1_rmse: 0.5218  |  1:01:21s
epoch 81 | loss: 0.2685  | val_0_rmse: 0.60913 | val_1_rmse: 0.60821 |  1:02:06s
epoch 82 | loss: 0.26867 | val_0_rmse: 0.54006 | val_1_rmse: 0.53797 |  1:02:52s
epoch 83 | loss: 0.26782 | val_0_rmse: 0.53654 | val_1_rmse: 0.53563 |  1:03:38s
epoch 84 | loss: 0.26797 | val_0_rmse: 0.55419 | val_1_rmse: 0.55207 |  1:04:23s
epoch 85 | loss: 0.26744 | val_0_rmse: 0.75768 | val_1_rmse: 0.75656 |  1:05:08s
epoch 86 | loss: 0.26721 | val_0_rmse: 0.75963 | val_1_rmse: 0.75944 |  1:05:53s
epoch 87 | loss: 0.26689 | val_0_rmse: 0.62904 | val_1_rmse: 0.62787 |  1:06:39s
epoch 88 | loss: 0.26617 | val_0_rmse: 0.5743  | val_1_rmse: 0.57361 |  1:07:24s
epoch 89 | loss: 0.26586 | val_0_rmse: 0.65236 | val_1_rmse: 0.65159 |  1:08:09s
epoch 90 | loss: 0.26628 | val_0_rmse: 0.52851 | val_1_rmse: 0.52758 |  1:08:54s
epoch 91 | loss: 0.26573 | val_0_rmse: 0.64719 | val_1_rmse: 0.64725 |  1:09:39s
epoch 92 | loss: 0.26707 | val_0_rmse: 0.75418 | val_1_rmse: 0.75309 |  1:10:24s
epoch 93 | loss: 0.26892 | val_0_rmse: 0.66541 | val_1_rmse: 0.66484 |  1:11:09s
epoch 94 | loss: 0.26572 | val_0_rmse: 0.69573 | val_1_rmse: 0.69487 |  1:11:55s
epoch 95 | loss: 0.26618 | val_0_rmse: 0.63129 | val_1_rmse: 0.63061 |  1:12:40s
epoch 96 | loss: 0.26454 | val_0_rmse: 0.73304 | val_1_rmse: 0.7323  |  1:13:25s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.52057
Best weights from best epoch are automatically used!
ended training at: 08:46:44
Feature importance:
[('Area', 0.08908559968167007), ('Baths', 0.2966400912528771), ('Beds', 0.0783550666330474), ('Latitude', 0.05909079757326763), ('Longitude', 0.23735570915163987), ('Month', 0.23947273570749794), ('Year', 0.0)]
Mean squared error is of 14878645279.6468
Mean absolute error:77549.35796720683
MAPE:0.41038615894251185
R2 score:0.7286566097614513
------------------------------------------------------------------
