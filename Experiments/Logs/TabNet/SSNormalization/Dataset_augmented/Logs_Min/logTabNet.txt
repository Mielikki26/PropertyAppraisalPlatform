TabNet Logs:

Saving copy of script...
In this script all datasets are decreased in size down to the size of the smallest dataset by sampling random rows and deleting them from the dataThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:36:49
epoch 0  | loss: 1.49801 | val_0_rmse: 1.43483 | val_1_rmse: 1.48256 |  0:00:03s
epoch 1  | loss: 0.86499 | val_0_rmse: 1.23137 | val_1_rmse: 1.28262 |  0:00:03s
epoch 2  | loss: 0.75549 | val_0_rmse: 0.96026 | val_1_rmse: 0.93127 |  0:00:03s
epoch 3  | loss: 0.68222 | val_0_rmse: 0.90599 | val_1_rmse: 0.89314 |  0:00:03s
epoch 4  | loss: 0.62178 | val_0_rmse: 0.88042 | val_1_rmse: 0.89888 |  0:00:03s
epoch 5  | loss: 0.5722  | val_0_rmse: 0.83302 | val_1_rmse: 0.83314 |  0:00:04s
epoch 6  | loss: 0.54319 | val_0_rmse: 0.81212 | val_1_rmse: 0.80548 |  0:00:04s
epoch 7  | loss: 0.50576 | val_0_rmse: 0.77583 | val_1_rmse: 0.75498 |  0:00:04s
epoch 8  | loss: 0.47242 | val_0_rmse: 0.78415 | val_1_rmse: 0.74934 |  0:00:05s
epoch 9  | loss: 0.48706 | val_0_rmse: 0.74128 | val_1_rmse: 0.71801 |  0:00:05s
epoch 10 | loss: 0.45961 | val_0_rmse: 0.69695 | val_1_rmse: 0.67258 |  0:00:05s
epoch 11 | loss: 0.44989 | val_0_rmse: 0.71207 | val_1_rmse: 0.71601 |  0:00:05s
epoch 12 | loss: 0.44753 | val_0_rmse: 0.70267 | val_1_rmse: 0.69178 |  0:00:05s
epoch 13 | loss: 0.42801 | val_0_rmse: 0.68558 | val_1_rmse: 0.6692  |  0:00:06s
epoch 14 | loss: 0.44442 | val_0_rmse: 0.67859 | val_1_rmse: 0.66932 |  0:00:06s
epoch 15 | loss: 0.42664 | val_0_rmse: 0.67187 | val_1_rmse: 0.65454 |  0:00:06s
epoch 16 | loss: 0.40965 | val_0_rmse: 0.66817 | val_1_rmse: 0.6308  |  0:00:06s
epoch 17 | loss: 0.42506 | val_0_rmse: 0.63118 | val_1_rmse: 0.59271 |  0:00:06s
epoch 18 | loss: 0.42697 | val_0_rmse: 0.62865 | val_1_rmse: 0.59612 |  0:00:07s
epoch 19 | loss: 0.41969 | val_0_rmse: 0.66586 | val_1_rmse: 0.64001 |  0:00:07s
epoch 20 | loss: 0.41052 | val_0_rmse: 0.65428 | val_1_rmse: 0.63839 |  0:00:07s
epoch 21 | loss: 0.3995  | val_0_rmse: 0.64262 | val_1_rmse: 0.62503 |  0:00:07s
epoch 22 | loss: 0.38753 | val_0_rmse: 0.63608 | val_1_rmse: 0.60781 |  0:00:07s
epoch 23 | loss: 0.38786 | val_0_rmse: 0.61074 | val_1_rmse: 0.59377 |  0:00:08s
epoch 24 | loss: 0.39926 | val_0_rmse: 0.62289 | val_1_rmse: 0.60015 |  0:00:08s
epoch 25 | loss: 0.38221 | val_0_rmse: 0.61264 | val_1_rmse: 0.59204 |  0:00:08s
epoch 26 | loss: 0.37298 | val_0_rmse: 0.59998 | val_1_rmse: 0.57947 |  0:00:08s
epoch 27 | loss: 0.36339 | val_0_rmse: 0.59459 | val_1_rmse: 0.56575 |  0:00:08s
epoch 28 | loss: 0.35644 | val_0_rmse: 0.60301 | val_1_rmse: 0.57911 |  0:00:09s
epoch 29 | loss: 0.36509 | val_0_rmse: 0.58683 | val_1_rmse: 0.58011 |  0:00:09s
epoch 30 | loss: 0.38696 | val_0_rmse: 0.58508 | val_1_rmse: 0.57396 |  0:00:09s
epoch 31 | loss: 0.36013 | val_0_rmse: 0.60817 | val_1_rmse: 0.58567 |  0:00:09s
epoch 32 | loss: 0.37968 | val_0_rmse: 0.58225 | val_1_rmse: 0.56796 |  0:00:09s
epoch 33 | loss: 0.37685 | val_0_rmse: 0.58385 | val_1_rmse: 0.56632 |  0:00:10s
epoch 34 | loss: 0.35546 | val_0_rmse: 0.59906 | val_1_rmse: 0.57387 |  0:00:10s
epoch 35 | loss: 0.36328 | val_0_rmse: 0.60294 | val_1_rmse: 0.57431 |  0:00:10s
epoch 36 | loss: 0.34394 | val_0_rmse: 0.59573 | val_1_rmse: 0.57226 |  0:00:10s
epoch 37 | loss: 0.35802 | val_0_rmse: 0.57991 | val_1_rmse: 0.55957 |  0:00:10s
epoch 38 | loss: 0.3463  | val_0_rmse: 0.58304 | val_1_rmse: 0.56697 |  0:00:11s
epoch 39 | loss: 0.35819 | val_0_rmse: 0.56807 | val_1_rmse: 0.54851 |  0:00:11s
epoch 40 | loss: 0.33912 | val_0_rmse: 0.58671 | val_1_rmse: 0.56237 |  0:00:11s
epoch 41 | loss: 0.35664 | val_0_rmse: 0.59043 | val_1_rmse: 0.56302 |  0:00:11s
epoch 42 | loss: 0.348   | val_0_rmse: 0.60621 | val_1_rmse: 0.58827 |  0:00:11s
epoch 43 | loss: 0.35329 | val_0_rmse: 0.60423 | val_1_rmse: 0.58095 |  0:00:12s
epoch 44 | loss: 0.35409 | val_0_rmse: 0.58699 | val_1_rmse: 0.571   |  0:00:12s
epoch 45 | loss: 0.36913 | val_0_rmse: 0.58836 | val_1_rmse: 0.55996 |  0:00:12s
epoch 46 | loss: 0.34006 | val_0_rmse: 0.56191 | val_1_rmse: 0.5255  |  0:00:12s
epoch 47 | loss: 0.33824 | val_0_rmse: 0.5547  | val_1_rmse: 0.52047 |  0:00:12s
epoch 48 | loss: 0.34395 | val_0_rmse: 0.55975 | val_1_rmse: 0.52933 |  0:00:12s
epoch 49 | loss: 0.336   | val_0_rmse: 0.56003 | val_1_rmse: 0.54228 |  0:00:13s
epoch 50 | loss: 0.33682 | val_0_rmse: 0.55351 | val_1_rmse: 0.53528 |  0:00:13s
epoch 51 | loss: 0.34943 | val_0_rmse: 0.56858 | val_1_rmse: 0.54726 |  0:00:13s
epoch 52 | loss: 0.32428 | val_0_rmse: 0.57547 | val_1_rmse: 0.5604  |  0:00:13s
epoch 53 | loss: 0.34306 | val_0_rmse: 0.56136 | val_1_rmse: 0.54197 |  0:00:13s
epoch 54 | loss: 0.33703 | val_0_rmse: 0.55596 | val_1_rmse: 0.54323 |  0:00:14s
epoch 55 | loss: 0.32934 | val_0_rmse: 0.56138 | val_1_rmse: 0.53863 |  0:00:14s
epoch 56 | loss: 0.35792 | val_0_rmse: 0.55574 | val_1_rmse: 0.52769 |  0:00:14s
epoch 57 | loss: 0.32544 | val_0_rmse: 0.55702 | val_1_rmse: 0.53173 |  0:00:14s
epoch 58 | loss: 0.33887 | val_0_rmse: 0.56502 | val_1_rmse: 0.54384 |  0:00:14s
epoch 59 | loss: 0.3322  | val_0_rmse: 0.55909 | val_1_rmse: 0.52568 |  0:00:14s
epoch 60 | loss: 0.33217 | val_0_rmse: 0.57684 | val_1_rmse: 0.54901 |  0:00:15s
epoch 61 | loss: 0.33223 | val_0_rmse: 0.54941 | val_1_rmse: 0.52623 |  0:00:15s
epoch 62 | loss: 0.34878 | val_0_rmse: 0.5654  | val_1_rmse: 0.5581  |  0:00:15s
epoch 63 | loss: 0.32576 | val_0_rmse: 0.57146 | val_1_rmse: 0.57176 |  0:00:15s
epoch 64 | loss: 0.32803 | val_0_rmse: 0.56497 | val_1_rmse: 0.55434 |  0:00:15s
epoch 65 | loss: 0.34515 | val_0_rmse: 0.54969 | val_1_rmse: 0.54205 |  0:00:16s
epoch 66 | loss: 0.31975 | val_0_rmse: 0.56901 | val_1_rmse: 0.557   |  0:00:16s
epoch 67 | loss: 0.33398 | val_0_rmse: 0.56633 | val_1_rmse: 0.54658 |  0:00:16s
epoch 68 | loss: 0.3169  | val_0_rmse: 0.55424 | val_1_rmse: 0.51982 |  0:00:16s
epoch 69 | loss: 0.33224 | val_0_rmse: 0.57647 | val_1_rmse: 0.54735 |  0:00:16s
epoch 70 | loss: 0.33242 | val_0_rmse: 0.55555 | val_1_rmse: 0.53611 |  0:00:16s
epoch 71 | loss: 0.33005 | val_0_rmse: 0.54667 | val_1_rmse: 0.52285 |  0:00:17s
epoch 72 | loss: 0.32745 | val_0_rmse: 0.54955 | val_1_rmse: 0.53432 |  0:00:17s
epoch 73 | loss: 0.31901 | val_0_rmse: 0.55508 | val_1_rmse: 0.55053 |  0:00:17s
epoch 74 | loss: 0.31908 | val_0_rmse: 0.54412 | val_1_rmse: 0.53107 |  0:00:17s
epoch 75 | loss: 0.31743 | val_0_rmse: 0.53404 | val_1_rmse: 0.51597 |  0:00:17s
epoch 76 | loss: 0.32055 | val_0_rmse: 0.53653 | val_1_rmse: 0.53085 |  0:00:17s
epoch 77 | loss: 0.31271 | val_0_rmse: 0.54047 | val_1_rmse: 0.53151 |  0:00:18s
epoch 78 | loss: 0.3098  | val_0_rmse: 0.54486 | val_1_rmse: 0.52978 |  0:00:18s
epoch 79 | loss: 0.31083 | val_0_rmse: 0.55495 | val_1_rmse: 0.54667 |  0:00:18s
epoch 80 | loss: 0.32014 | val_0_rmse: 0.54927 | val_1_rmse: 0.53999 |  0:00:18s
epoch 81 | loss: 0.30437 | val_0_rmse: 0.53505 | val_1_rmse: 0.51925 |  0:00:18s
epoch 82 | loss: 0.32129 | val_0_rmse: 0.5444  | val_1_rmse: 0.52584 |  0:00:19s
epoch 83 | loss: 0.33241 | val_0_rmse: 0.54248 | val_1_rmse: 0.53387 |  0:00:19s
epoch 84 | loss: 0.31208 | val_0_rmse: 0.54868 | val_1_rmse: 0.54647 |  0:00:19s
epoch 85 | loss: 0.3239  | val_0_rmse: 0.5335  | val_1_rmse: 0.52916 |  0:00:19s
epoch 86 | loss: 0.3013  | val_0_rmse: 0.5487  | val_1_rmse: 0.5535  |  0:00:19s
epoch 87 | loss: 0.31409 | val_0_rmse: 0.53194 | val_1_rmse: 0.54343 |  0:00:20s
epoch 88 | loss: 0.31663 | val_0_rmse: 0.52551 | val_1_rmse: 0.53992 |  0:00:20s
epoch 89 | loss: 0.29634 | val_0_rmse: 0.52763 | val_1_rmse: 0.53708 |  0:00:20s
epoch 90 | loss: 0.29215 | val_0_rmse: 0.53453 | val_1_rmse: 0.54358 |  0:00:20s
epoch 91 | loss: 0.30376 | val_0_rmse: 0.53079 | val_1_rmse: 0.54493 |  0:00:20s
epoch 92 | loss: 0.31068 | val_0_rmse: 0.55696 | val_1_rmse: 0.57483 |  0:00:20s
epoch 93 | loss: 0.30541 | val_0_rmse: 0.52505 | val_1_rmse: 0.53167 |  0:00:21s
epoch 94 | loss: 0.29523 | val_0_rmse: 0.52791 | val_1_rmse: 0.53113 |  0:00:21s
epoch 95 | loss: 0.30636 | val_0_rmse: 0.53688 | val_1_rmse: 0.53448 |  0:00:21s
epoch 96 | loss: 0.30907 | val_0_rmse: 0.53199 | val_1_rmse: 0.52575 |  0:00:21s
epoch 97 | loss: 0.32478 | val_0_rmse: 0.53164 | val_1_rmse: 0.51946 |  0:00:21s
epoch 98 | loss: 0.30724 | val_0_rmse: 0.57038 | val_1_rmse: 0.5623  |  0:00:21s
epoch 99 | loss: 0.33764 | val_0_rmse: 0.53267 | val_1_rmse: 0.54144 |  0:00:22s
epoch 100| loss: 0.31392 | val_0_rmse: 0.53397 | val_1_rmse: 0.54829 |  0:00:22s
epoch 101| loss: 0.29388 | val_0_rmse: 0.56785 | val_1_rmse: 0.57533 |  0:00:22s
epoch 102| loss: 0.33175 | val_0_rmse: 0.5291  | val_1_rmse: 0.52942 |  0:00:22s
epoch 103| loss: 0.3     | val_0_rmse: 0.53351 | val_1_rmse: 0.53152 |  0:00:22s
epoch 104| loss: 0.29506 | val_0_rmse: 0.55107 | val_1_rmse: 0.55544 |  0:00:23s
epoch 105| loss: 0.31286 | val_0_rmse: 0.52148 | val_1_rmse: 0.52733 |  0:00:23s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.51597
Best weights from best epoch are automatically used!
ended training at: 02:37:12
Feature importance:
[('Area', 0.20768207694427018), ('Baths', 0.06275405953828818), ('Beds', 0.03967203574431942), ('Latitude', 0.22414413710640327), ('Longitude', 0.3030038521473486), ('Month', 0.021799765543080413), ('Year', 0.14094407297628994)]
Mean squared error is of 10128489334.088144
Mean absolute error:71990.50769093407
MAPE:0.2370192095391489
R2 score:0.5555196545747854
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:37:13
epoch 0  | loss: 1.56161 | val_0_rmse: 1.41364 | val_1_rmse: 1.4196  |  0:00:00s
epoch 1  | loss: 0.88624 | val_0_rmse: 1.10258 | val_1_rmse: 1.09099 |  0:00:00s
epoch 2  | loss: 0.79833 | val_0_rmse: 0.97326 | val_1_rmse: 0.94494 |  0:00:00s
epoch 3  | loss: 0.69649 | val_0_rmse: 0.98233 | val_1_rmse: 0.94693 |  0:00:00s
epoch 4  | loss: 0.66568 | val_0_rmse: 0.9328  | val_1_rmse: 0.8631  |  0:00:00s
epoch 5  | loss: 0.60937 | val_0_rmse: 0.88869 | val_1_rmse: 0.85836 |  0:00:01s
epoch 6  | loss: 0.60265 | val_0_rmse: 0.82503 | val_1_rmse: 0.81042 |  0:00:01s
epoch 7  | loss: 0.57963 | val_0_rmse: 0.82599 | val_1_rmse: 0.82514 |  0:00:01s
epoch 8  | loss: 0.55296 | val_0_rmse: 0.83874 | val_1_rmse: 0.90432 |  0:00:01s
epoch 9  | loss: 0.53201 | val_0_rmse: 0.84215 | val_1_rmse: 0.89752 |  0:00:01s
epoch 10 | loss: 0.51383 | val_0_rmse: 0.73229 | val_1_rmse: 0.76892 |  0:00:02s
epoch 11 | loss: 0.49665 | val_0_rmse: 0.70616 | val_1_rmse: 0.69889 |  0:00:02s
epoch 12 | loss: 0.47617 | val_0_rmse: 0.69739 | val_1_rmse: 0.67796 |  0:00:02s
epoch 13 | loss: 0.45118 | val_0_rmse: 0.67164 | val_1_rmse: 0.67233 |  0:00:02s
epoch 14 | loss: 0.45048 | val_0_rmse: 0.65899 | val_1_rmse: 0.66341 |  0:00:02s
epoch 15 | loss: 0.42405 | val_0_rmse: 0.64203 | val_1_rmse: 0.65887 |  0:00:03s
epoch 16 | loss: 0.43073 | val_0_rmse: 0.63464 | val_1_rmse: 0.64938 |  0:00:03s
epoch 17 | loss: 0.40397 | val_0_rmse: 0.63865 | val_1_rmse: 0.6509  |  0:00:03s
epoch 18 | loss: 0.40527 | val_0_rmse: 0.63953 | val_1_rmse: 0.66337 |  0:00:03s
epoch 19 | loss: 0.40954 | val_0_rmse: 0.62531 | val_1_rmse: 0.64034 |  0:00:03s
epoch 20 | loss: 0.40554 | val_0_rmse: 0.6102  | val_1_rmse: 0.62773 |  0:00:04s
epoch 21 | loss: 0.38434 | val_0_rmse: 0.62066 | val_1_rmse: 0.65513 |  0:00:04s
epoch 22 | loss: 0.39034 | val_0_rmse: 0.65004 | val_1_rmse: 0.6656  |  0:00:04s
epoch 23 | loss: 0.40547 | val_0_rmse: 0.63267 | val_1_rmse: 0.68675 |  0:00:04s
epoch 24 | loss: 0.4041  | val_0_rmse: 0.6255  | val_1_rmse: 0.66205 |  0:00:04s
epoch 25 | loss: 0.40525 | val_0_rmse: 0.63571 | val_1_rmse: 0.65624 |  0:00:04s
epoch 26 | loss: 0.406   | val_0_rmse: 0.61653 | val_1_rmse: 0.62839 |  0:00:05s
epoch 27 | loss: 0.41322 | val_0_rmse: 0.61427 | val_1_rmse: 0.60834 |  0:00:05s
epoch 28 | loss: 0.37639 | val_0_rmse: 0.63876 | val_1_rmse: 0.68361 |  0:00:05s
epoch 29 | loss: 0.42256 | val_0_rmse: 0.6295  | val_1_rmse: 0.64981 |  0:00:05s
epoch 30 | loss: 0.38985 | val_0_rmse: 0.64002 | val_1_rmse: 0.64204 |  0:00:05s
epoch 31 | loss: 0.39491 | val_0_rmse: 0.62379 | val_1_rmse: 0.64411 |  0:00:06s
epoch 32 | loss: 0.38986 | val_0_rmse: 0.60277 | val_1_rmse: 0.62516 |  0:00:06s
epoch 33 | loss: 0.38295 | val_0_rmse: 0.60884 | val_1_rmse: 0.61317 |  0:00:06s
epoch 34 | loss: 0.3601  | val_0_rmse: 0.59406 | val_1_rmse: 0.59642 |  0:00:06s
epoch 35 | loss: 0.37002 | val_0_rmse: 0.6028  | val_1_rmse: 0.59783 |  0:00:06s
epoch 36 | loss: 0.38043 | val_0_rmse: 0.59684 | val_1_rmse: 0.59891 |  0:00:07s
epoch 37 | loss: 0.37002 | val_0_rmse: 0.59399 | val_1_rmse: 0.58462 |  0:00:07s
epoch 38 | loss: 0.3869  | val_0_rmse: 0.58909 | val_1_rmse: 0.56208 |  0:00:07s
epoch 39 | loss: 0.35272 | val_0_rmse: 0.59771 | val_1_rmse: 0.57343 |  0:00:07s
epoch 40 | loss: 0.35792 | val_0_rmse: 0.59651 | val_1_rmse: 0.57172 |  0:00:07s
epoch 41 | loss: 0.34874 | val_0_rmse: 0.57935 | val_1_rmse: 0.56669 |  0:00:08s
epoch 42 | loss: 0.34721 | val_0_rmse: 0.5888  | val_1_rmse: 0.58123 |  0:00:08s
epoch 43 | loss: 0.34676 | val_0_rmse: 0.58482 | val_1_rmse: 0.58468 |  0:00:08s
epoch 44 | loss: 0.34828 | val_0_rmse: 0.56434 | val_1_rmse: 0.57268 |  0:00:08s
epoch 45 | loss: 0.33462 | val_0_rmse: 0.56178 | val_1_rmse: 0.56012 |  0:00:08s
epoch 46 | loss: 0.33946 | val_0_rmse: 0.57668 | val_1_rmse: 0.56692 |  0:00:09s
epoch 47 | loss: 0.34616 | val_0_rmse: 0.57773 | val_1_rmse: 0.57898 |  0:00:09s
epoch 48 | loss: 0.34436 | val_0_rmse: 0.57064 | val_1_rmse: 0.58108 |  0:00:09s
epoch 49 | loss: 0.34588 | val_0_rmse: 0.5603  | val_1_rmse: 0.55488 |  0:00:09s
epoch 50 | loss: 0.33464 | val_0_rmse: 0.5982  | val_1_rmse: 0.56136 |  0:00:09s
epoch 51 | loss: 0.34621 | val_0_rmse: 0.59329 | val_1_rmse: 0.55112 |  0:00:10s
epoch 52 | loss: 0.37205 | val_0_rmse: 0.6206  | val_1_rmse: 0.61009 |  0:00:10s
epoch 53 | loss: 0.35714 | val_0_rmse: 0.57672 | val_1_rmse: 0.55683 |  0:00:10s
epoch 54 | loss: 0.35612 | val_0_rmse: 0.58891 | val_1_rmse: 0.56194 |  0:00:10s
epoch 55 | loss: 0.36835 | val_0_rmse: 0.58181 | val_1_rmse: 0.55705 |  0:00:10s
epoch 56 | loss: 0.3488  | val_0_rmse: 0.57682 | val_1_rmse: 0.54985 |  0:00:10s
epoch 57 | loss: 0.35859 | val_0_rmse: 0.56587 | val_1_rmse: 0.54822 |  0:00:11s
epoch 58 | loss: 0.35047 | val_0_rmse: 0.57319 | val_1_rmse: 0.56578 |  0:00:11s
epoch 59 | loss: 0.3438  | val_0_rmse: 0.56821 | val_1_rmse: 0.59196 |  0:00:11s
epoch 60 | loss: 0.34546 | val_0_rmse: 0.58812 | val_1_rmse: 0.62642 |  0:00:11s
epoch 61 | loss: 0.36163 | val_0_rmse: 0.5772  | val_1_rmse: 0.61071 |  0:00:11s
epoch 62 | loss: 0.3367  | val_0_rmse: 0.56233 | val_1_rmse: 0.58421 |  0:00:12s
epoch 63 | loss: 0.35078 | val_0_rmse: 0.57673 | val_1_rmse: 0.59445 |  0:00:12s
epoch 64 | loss: 0.34301 | val_0_rmse: 0.57395 | val_1_rmse: 0.57682 |  0:00:12s
epoch 65 | loss: 0.35229 | val_0_rmse: 0.60213 | val_1_rmse: 0.60239 |  0:00:12s
epoch 66 | loss: 0.36734 | val_0_rmse: 0.60184 | val_1_rmse: 0.60224 |  0:00:12s
epoch 67 | loss: 0.36869 | val_0_rmse: 0.60969 | val_1_rmse: 0.6081  |  0:00:13s
epoch 68 | loss: 0.36103 | val_0_rmse: 0.64534 | val_1_rmse: 0.62172 |  0:00:13s
epoch 69 | loss: 0.38308 | val_0_rmse: 0.63662 | val_1_rmse: 0.5949  |  0:00:13s
epoch 70 | loss: 0.38345 | val_0_rmse: 0.61821 | val_1_rmse: 0.58634 |  0:00:13s
epoch 71 | loss: 0.36956 | val_0_rmse: 0.61228 | val_1_rmse: 0.59516 |  0:00:13s
epoch 72 | loss: 0.36267 | val_0_rmse: 0.585   | val_1_rmse: 0.59686 |  0:00:14s
epoch 73 | loss: 0.33965 | val_0_rmse: 0.64578 | val_1_rmse: 0.65997 |  0:00:14s
epoch 74 | loss: 0.36554 | val_0_rmse: 0.59349 | val_1_rmse: 0.59989 |  0:00:14s
epoch 75 | loss: 0.35872 | val_0_rmse: 0.58085 | val_1_rmse: 0.58395 |  0:00:14s
epoch 76 | loss: 0.35328 | val_0_rmse: 0.59346 | val_1_rmse: 0.60682 |  0:00:14s
epoch 77 | loss: 0.3557  | val_0_rmse: 0.57358 | val_1_rmse: 0.58515 |  0:00:15s
epoch 78 | loss: 0.33409 | val_0_rmse: 0.58717 | val_1_rmse: 0.58615 |  0:00:15s
epoch 79 | loss: 0.35599 | val_0_rmse: 0.59079 | val_1_rmse: 0.60047 |  0:00:15s
epoch 80 | loss: 0.33479 | val_0_rmse: 0.58706 | val_1_rmse: 0.60443 |  0:00:15s
epoch 81 | loss: 0.34911 | val_0_rmse: 0.59271 | val_1_rmse: 0.59904 |  0:00:15s
epoch 82 | loss: 0.34874 | val_0_rmse: 0.60114 | val_1_rmse: 0.58575 |  0:00:16s
epoch 83 | loss: 0.35467 | val_0_rmse: 0.58937 | val_1_rmse: 0.57892 |  0:00:16s
epoch 84 | loss: 0.34768 | val_0_rmse: 0.58373 | val_1_rmse: 0.58711 |  0:00:16s
epoch 85 | loss: 0.36254 | val_0_rmse: 0.56973 | val_1_rmse: 0.57174 |  0:00:16s
epoch 86 | loss: 0.35445 | val_0_rmse: 0.57563 | val_1_rmse: 0.58721 |  0:00:16s
epoch 87 | loss: 0.35308 | val_0_rmse: 0.57073 | val_1_rmse: 0.60015 |  0:00:17s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.54822
Best weights from best epoch are automatically used!
ended training at: 02:37:30
Feature importance:
[('Area', 0.3534502462286085), ('Baths', 0.05679756597542842), ('Beds', 0.030175315320732476), ('Latitude', 0.28543426994945265), ('Longitude', 0.15907537072785885), ('Month', 0.0), ('Year', 0.11506723179791914)]
Mean squared error is of 8927185414.170824
Mean absolute error:69080.18648626373
MAPE:0.2506215343715163
R2 score:0.6350897350983693
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:37:30
epoch 0  | loss: 1.5421  | val_0_rmse: 1.22978 | val_1_rmse: 1.25107 |  0:00:00s
epoch 1  | loss: 1.02283 | val_0_rmse: 1.10685 | val_1_rmse: 1.09193 |  0:00:00s
epoch 2  | loss: 0.81434 | val_0_rmse: 0.92006 | val_1_rmse: 0.91612 |  0:00:00s
epoch 3  | loss: 0.73475 | val_0_rmse: 0.93661 | val_1_rmse: 0.91291 |  0:00:00s
epoch 4  | loss: 0.65529 | val_0_rmse: 0.99388 | val_1_rmse: 1.02143 |  0:00:01s
epoch 5  | loss: 0.59335 | val_0_rmse: 0.94416 | val_1_rmse: 0.99508 |  0:00:01s
epoch 6  | loss: 0.57872 | val_0_rmse: 0.91512 | val_1_rmse: 0.97467 |  0:00:01s
epoch 7  | loss: 0.55827 | val_0_rmse: 0.86222 | val_1_rmse: 0.94121 |  0:00:01s
epoch 8  | loss: 0.52859 | val_0_rmse: 0.80434 | val_1_rmse: 0.84202 |  0:00:01s
epoch 9  | loss: 0.5152  | val_0_rmse: 0.80597 | val_1_rmse: 0.81938 |  0:00:02s
epoch 10 | loss: 0.50737 | val_0_rmse: 0.81792 | val_1_rmse: 0.83923 |  0:00:02s
epoch 11 | loss: 0.49109 | val_0_rmse: 0.7111  | val_1_rmse: 0.74499 |  0:00:02s
epoch 12 | loss: 0.47857 | val_0_rmse: 0.69824 | val_1_rmse: 0.74    |  0:00:02s
epoch 13 | loss: 0.48978 | val_0_rmse: 0.68581 | val_1_rmse: 0.76142 |  0:00:02s
epoch 14 | loss: 0.47622 | val_0_rmse: 0.70228 | val_1_rmse: 0.7989  |  0:00:03s
epoch 15 | loss: 0.45064 | val_0_rmse: 0.6905  | val_1_rmse: 0.75518 |  0:00:03s
epoch 16 | loss: 0.46239 | val_0_rmse: 0.67155 | val_1_rmse: 0.72008 |  0:00:03s
epoch 17 | loss: 0.47959 | val_0_rmse: 0.66313 | val_1_rmse: 0.71858 |  0:00:03s
epoch 18 | loss: 0.45942 | val_0_rmse: 0.70345 | val_1_rmse: 0.76466 |  0:00:03s
epoch 19 | loss: 0.46028 | val_0_rmse: 0.65419 | val_1_rmse: 0.71388 |  0:00:04s
epoch 20 | loss: 0.46875 | val_0_rmse: 0.6613  | val_1_rmse: 0.70853 |  0:00:04s
epoch 21 | loss: 0.46195 | val_0_rmse: 0.6719  | val_1_rmse: 0.70901 |  0:00:04s
epoch 22 | loss: 0.44779 | val_0_rmse: 0.65477 | val_1_rmse: 0.68848 |  0:00:04s
epoch 23 | loss: 0.44616 | val_0_rmse: 0.65849 | val_1_rmse: 0.69649 |  0:00:04s
epoch 24 | loss: 0.42945 | val_0_rmse: 0.65495 | val_1_rmse: 0.69319 |  0:00:05s
epoch 25 | loss: 0.43503 | val_0_rmse: 0.64903 | val_1_rmse: 0.69516 |  0:00:05s
epoch 26 | loss: 0.43005 | val_0_rmse: 0.6488  | val_1_rmse: 0.70212 |  0:00:05s
epoch 27 | loss: 0.42637 | val_0_rmse: 0.671   | val_1_rmse: 0.70868 |  0:00:05s
epoch 28 | loss: 0.42362 | val_0_rmse: 0.65893 | val_1_rmse: 0.69809 |  0:00:05s
epoch 29 | loss: 0.4294  | val_0_rmse: 0.63774 | val_1_rmse: 0.67568 |  0:00:06s
epoch 30 | loss: 0.4346  | val_0_rmse: 0.63511 | val_1_rmse: 0.66818 |  0:00:06s
epoch 31 | loss: 0.42401 | val_0_rmse: 0.64632 | val_1_rmse: 0.67237 |  0:00:06s
epoch 32 | loss: 0.4138  | val_0_rmse: 0.62352 | val_1_rmse: 0.66092 |  0:00:06s
epoch 33 | loss: 0.4205  | val_0_rmse: 0.62568 | val_1_rmse: 0.66984 |  0:00:06s
epoch 34 | loss: 0.41964 | val_0_rmse: 0.63983 | val_1_rmse: 0.68439 |  0:00:07s
epoch 35 | loss: 0.41781 | val_0_rmse: 0.62368 | val_1_rmse: 0.6761  |  0:00:07s
epoch 36 | loss: 0.41052 | val_0_rmse: 0.63258 | val_1_rmse: 0.68228 |  0:00:07s
epoch 37 | loss: 0.41998 | val_0_rmse: 0.65117 | val_1_rmse: 0.70036 |  0:00:07s
epoch 38 | loss: 0.42546 | val_0_rmse: 0.64757 | val_1_rmse: 0.70943 |  0:00:07s
epoch 39 | loss: 0.43317 | val_0_rmse: 0.63466 | val_1_rmse: 0.68335 |  0:00:08s
epoch 40 | loss: 0.43946 | val_0_rmse: 0.64171 | val_1_rmse: 0.69755 |  0:00:08s
epoch 41 | loss: 0.42519 | val_0_rmse: 0.63027 | val_1_rmse: 0.68295 |  0:00:08s
epoch 42 | loss: 0.4203  | val_0_rmse: 0.63376 | val_1_rmse: 0.68449 |  0:00:08s
epoch 43 | loss: 0.42273 | val_0_rmse: 0.6415  | val_1_rmse: 0.69007 |  0:00:08s
epoch 44 | loss: 0.41325 | val_0_rmse: 0.62193 | val_1_rmse: 0.68037 |  0:00:09s
epoch 45 | loss: 0.41371 | val_0_rmse: 0.62485 | val_1_rmse: 0.69139 |  0:00:09s
epoch 46 | loss: 0.40516 | val_0_rmse: 0.63381 | val_1_rmse: 0.69327 |  0:00:09s
epoch 47 | loss: 0.41099 | val_0_rmse: 0.62607 | val_1_rmse: 0.68278 |  0:00:09s
epoch 48 | loss: 0.41295 | val_0_rmse: 0.61734 | val_1_rmse: 0.66861 |  0:00:09s
epoch 49 | loss: 0.3992  | val_0_rmse: 0.61961 | val_1_rmse: 0.65478 |  0:00:10s
epoch 50 | loss: 0.38925 | val_0_rmse: 0.62276 | val_1_rmse: 0.65047 |  0:00:10s
epoch 51 | loss: 0.39133 | val_0_rmse: 0.62829 | val_1_rmse: 0.64895 |  0:00:10s
epoch 52 | loss: 0.4028  | val_0_rmse: 0.62925 | val_1_rmse: 0.65366 |  0:00:10s
epoch 53 | loss: 0.38953 | val_0_rmse: 0.62401 | val_1_rmse: 0.65703 |  0:00:10s
epoch 54 | loss: 0.39518 | val_0_rmse: 0.61292 | val_1_rmse: 0.65543 |  0:00:11s
epoch 55 | loss: 0.37936 | val_0_rmse: 0.60788 | val_1_rmse: 0.6437  |  0:00:11s
epoch 56 | loss: 0.39343 | val_0_rmse: 0.60656 | val_1_rmse: 0.64805 |  0:00:11s
epoch 57 | loss: 0.39403 | val_0_rmse: 0.60292 | val_1_rmse: 0.64914 |  0:00:11s
epoch 58 | loss: 0.37805 | val_0_rmse: 0.6078  | val_1_rmse: 0.64933 |  0:00:11s
epoch 59 | loss: 0.38801 | val_0_rmse: 0.6166  | val_1_rmse: 0.67695 |  0:00:12s
epoch 60 | loss: 0.38155 | val_0_rmse: 0.59488 | val_1_rmse: 0.65615 |  0:00:12s
epoch 61 | loss: 0.37871 | val_0_rmse: 0.59703 | val_1_rmse: 0.65134 |  0:00:12s
epoch 62 | loss: 0.3772  | val_0_rmse: 0.60892 | val_1_rmse: 0.65743 |  0:00:12s
epoch 63 | loss: 0.37515 | val_0_rmse: 0.59596 | val_1_rmse: 0.64221 |  0:00:13s
epoch 64 | loss: 0.37047 | val_0_rmse: 0.6007  | val_1_rmse: 0.64859 |  0:00:13s
epoch 65 | loss: 0.38232 | val_0_rmse: 0.59908 | val_1_rmse: 0.65137 |  0:00:13s
epoch 66 | loss: 0.37123 | val_0_rmse: 0.60328 | val_1_rmse: 0.65668 |  0:00:13s
epoch 67 | loss: 0.37697 | val_0_rmse: 0.61082 | val_1_rmse: 0.65165 |  0:00:13s
epoch 68 | loss: 0.38317 | val_0_rmse: 0.63587 | val_1_rmse: 0.65651 |  0:00:14s
epoch 69 | loss: 0.39217 | val_0_rmse: 0.59775 | val_1_rmse: 0.62442 |  0:00:14s
epoch 70 | loss: 0.40125 | val_0_rmse: 0.58684 | val_1_rmse: 0.62032 |  0:00:14s
epoch 71 | loss: 0.36863 | val_0_rmse: 0.61063 | val_1_rmse: 0.64452 |  0:00:14s
epoch 72 | loss: 0.37231 | val_0_rmse: 0.59172 | val_1_rmse: 0.63435 |  0:00:14s
epoch 73 | loss: 0.3742  | val_0_rmse: 0.5864  | val_1_rmse: 0.63771 |  0:00:15s
epoch 74 | loss: 0.37867 | val_0_rmse: 0.59277 | val_1_rmse: 0.64195 |  0:00:15s
epoch 75 | loss: 0.36199 | val_0_rmse: 0.58663 | val_1_rmse: 0.63709 |  0:00:15s
epoch 76 | loss: 0.36518 | val_0_rmse: 0.5904  | val_1_rmse: 0.65926 |  0:00:15s
epoch 77 | loss: 0.36949 | val_0_rmse: 0.60108 | val_1_rmse: 0.66368 |  0:00:15s
epoch 78 | loss: 0.36276 | val_0_rmse: 0.5848  | val_1_rmse: 0.64355 |  0:00:16s
epoch 79 | loss: 0.3478  | val_0_rmse: 0.58192 | val_1_rmse: 0.63147 |  0:00:16s
epoch 80 | loss: 0.36401 | val_0_rmse: 0.57209 | val_1_rmse: 0.61905 |  0:00:16s
epoch 81 | loss: 0.35083 | val_0_rmse: 0.57991 | val_1_rmse: 0.62443 |  0:00:16s
epoch 82 | loss: 0.35777 | val_0_rmse: 0.58258 | val_1_rmse: 0.65574 |  0:00:16s
epoch 83 | loss: 0.38138 | val_0_rmse: 0.58216 | val_1_rmse: 0.66045 |  0:00:17s
epoch 84 | loss: 0.36357 | val_0_rmse: 0.56895 | val_1_rmse: 0.63466 |  0:00:17s
epoch 85 | loss: 0.37542 | val_0_rmse: 0.57574 | val_1_rmse: 0.63705 |  0:00:17s
epoch 86 | loss: 0.35618 | val_0_rmse: 0.61785 | val_1_rmse: 0.67125 |  0:00:17s
epoch 87 | loss: 0.3902  | val_0_rmse: 0.56705 | val_1_rmse: 0.62302 |  0:00:18s
epoch 88 | loss: 0.34478 | val_0_rmse: 0.58112 | val_1_rmse: 0.63588 |  0:00:18s
epoch 89 | loss: 0.35329 | val_0_rmse: 0.59204 | val_1_rmse: 0.64356 |  0:00:18s
epoch 90 | loss: 0.36247 | val_0_rmse: 0.62239 | val_1_rmse: 0.67356 |  0:00:18s
epoch 91 | loss: 0.36086 | val_0_rmse: 0.60443 | val_1_rmse: 0.67059 |  0:00:18s
epoch 92 | loss: 0.37199 | val_0_rmse: 0.57961 | val_1_rmse: 0.64813 |  0:00:19s
epoch 93 | loss: 0.35093 | val_0_rmse: 0.57949 | val_1_rmse: 0.64544 |  0:00:19s
epoch 94 | loss: 0.37043 | val_0_rmse: 0.57783 | val_1_rmse: 0.65833 |  0:00:19s
epoch 95 | loss: 0.36104 | val_0_rmse: 0.60115 | val_1_rmse: 0.68322 |  0:00:19s
epoch 96 | loss: 0.36736 | val_0_rmse: 0.58272 | val_1_rmse: 0.67294 |  0:00:19s
epoch 97 | loss: 0.35586 | val_0_rmse: 0.58753 | val_1_rmse: 0.67705 |  0:00:20s
epoch 98 | loss: 0.36354 | val_0_rmse: 0.57137 | val_1_rmse: 0.6684  |  0:00:20s
epoch 99 | loss: 0.34509 | val_0_rmse: 0.57146 | val_1_rmse: 0.67331 |  0:00:20s
epoch 100| loss: 0.34128 | val_0_rmse: 0.57038 | val_1_rmse: 0.66935 |  0:00:20s
epoch 101| loss: 0.34366 | val_0_rmse: 0.59066 | val_1_rmse: 0.67957 |  0:00:20s
epoch 102| loss: 0.34049 | val_0_rmse: 0.57547 | val_1_rmse: 0.65998 |  0:00:21s
epoch 103| loss: 0.34376 | val_0_rmse: 0.56439 | val_1_rmse: 0.64577 |  0:00:21s
epoch 104| loss: 0.32984 | val_0_rmse: 0.57202 | val_1_rmse: 0.64492 |  0:00:21s
epoch 105| loss: 0.33776 | val_0_rmse: 0.55747 | val_1_rmse: 0.61905 |  0:00:21s
epoch 106| loss: 0.33716 | val_0_rmse: 0.55212 | val_1_rmse: 0.61883 |  0:00:22s
epoch 107| loss: 0.32128 | val_0_rmse: 0.55115 | val_1_rmse: 0.63297 |  0:00:22s
epoch 108| loss: 0.31923 | val_0_rmse: 0.56201 | val_1_rmse: 0.63044 |  0:00:22s
epoch 109| loss: 0.34273 | val_0_rmse: 0.55002 | val_1_rmse: 0.61153 |  0:00:22s
epoch 110| loss: 0.32218 | val_0_rmse: 0.5487  | val_1_rmse: 0.60822 |  0:00:22s
epoch 111| loss: 0.32056 | val_0_rmse: 0.54536 | val_1_rmse: 0.60237 |  0:00:23s
epoch 112| loss: 0.3311  | val_0_rmse: 0.55131 | val_1_rmse: 0.60833 |  0:00:23s
epoch 113| loss: 0.32049 | val_0_rmse: 0.54622 | val_1_rmse: 0.61832 |  0:00:23s
epoch 114| loss: 0.32094 | val_0_rmse: 0.55748 | val_1_rmse: 0.62987 |  0:00:23s
epoch 115| loss: 0.32293 | val_0_rmse: 0.54808 | val_1_rmse: 0.609   |  0:00:23s
epoch 116| loss: 0.32203 | val_0_rmse: 0.54531 | val_1_rmse: 0.59977 |  0:00:24s
epoch 117| loss: 0.30935 | val_0_rmse: 0.54135 | val_1_rmse: 0.60334 |  0:00:24s
epoch 118| loss: 0.32131 | val_0_rmse: 0.54505 | val_1_rmse: 0.60669 |  0:00:24s
epoch 119| loss: 0.33042 | val_0_rmse: 0.54877 | val_1_rmse: 0.59865 |  0:00:24s
epoch 120| loss: 0.31839 | val_0_rmse: 0.56527 | val_1_rmse: 0.60431 |  0:00:25s
epoch 121| loss: 0.33167 | val_0_rmse: 0.53754 | val_1_rmse: 0.58743 |  0:00:25s
epoch 122| loss: 0.30522 | val_0_rmse: 0.54395 | val_1_rmse: 0.59848 |  0:00:25s
epoch 123| loss: 0.31917 | val_0_rmse: 0.53879 | val_1_rmse: 0.59447 |  0:00:25s
epoch 124| loss: 0.31741 | val_0_rmse: 0.54894 | val_1_rmse: 0.60181 |  0:00:25s
epoch 125| loss: 0.32325 | val_0_rmse: 0.53832 | val_1_rmse: 0.60297 |  0:00:26s
epoch 126| loss: 0.31387 | val_0_rmse: 0.54202 | val_1_rmse: 0.60963 |  0:00:26s
epoch 127| loss: 0.32113 | val_0_rmse: 0.54398 | val_1_rmse: 0.59306 |  0:00:26s
epoch 128| loss: 0.31782 | val_0_rmse: 0.55307 | val_1_rmse: 0.59962 |  0:00:26s
epoch 129| loss: 0.30527 | val_0_rmse: 0.53746 | val_1_rmse: 0.60773 |  0:00:26s
epoch 130| loss: 0.32454 | val_0_rmse: 0.54859 | val_1_rmse: 0.61636 |  0:00:27s
epoch 131| loss: 0.32819 | val_0_rmse: 0.54472 | val_1_rmse: 0.61353 |  0:00:27s
epoch 132| loss: 0.31623 | val_0_rmse: 0.53557 | val_1_rmse: 0.60179 |  0:00:27s
epoch 133| loss: 0.30811 | val_0_rmse: 0.55154 | val_1_rmse: 0.60261 |  0:00:27s
epoch 134| loss: 0.31912 | val_0_rmse: 0.5437  | val_1_rmse: 0.58977 |  0:00:27s
epoch 135| loss: 0.30905 | val_0_rmse: 0.53648 | val_1_rmse: 0.58335 |  0:00:28s
epoch 136| loss: 0.3258  | val_0_rmse: 0.53876 | val_1_rmse: 0.58456 |  0:00:28s
epoch 137| loss: 0.30936 | val_0_rmse: 0.53523 | val_1_rmse: 0.58947 |  0:00:28s
epoch 138| loss: 0.30363 | val_0_rmse: 0.53047 | val_1_rmse: 0.60073 |  0:00:28s
epoch 139| loss: 0.2936  | val_0_rmse: 0.52243 | val_1_rmse: 0.60426 |  0:00:29s
epoch 140| loss: 0.29652 | val_0_rmse: 0.54625 | val_1_rmse: 0.62951 |  0:00:29s
epoch 141| loss: 0.30462 | val_0_rmse: 0.56182 | val_1_rmse: 0.64929 |  0:00:29s
epoch 142| loss: 0.31379 | val_0_rmse: 0.52534 | val_1_rmse: 0.6187  |  0:00:29s
epoch 143| loss: 0.3227  | val_0_rmse: 0.52719 | val_1_rmse: 0.61828 |  0:00:29s
epoch 144| loss: 0.29251 | val_0_rmse: 0.53877 | val_1_rmse: 0.61103 |  0:00:30s
epoch 145| loss: 0.29694 | val_0_rmse: 0.52987 | val_1_rmse: 0.60238 |  0:00:30s
epoch 146| loss: 0.29874 | val_0_rmse: 0.52499 | val_1_rmse: 0.60212 |  0:00:30s
epoch 147| loss: 0.31465 | val_0_rmse: 0.53107 | val_1_rmse: 0.60641 |  0:00:30s
epoch 148| loss: 0.29268 | val_0_rmse: 0.54993 | val_1_rmse: 0.63236 |  0:00:31s
epoch 149| loss: 0.31045 | val_0_rmse: 0.53644 | val_1_rmse: 0.63283 |  0:00:31s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.58335
Best weights from best epoch are automatically used!
ended training at: 02:38:01
Feature importance:
[('Area', 0.33712081047117937), ('Baths', 0.04209271639778691), ('Beds', 0.05549005452509331), ('Latitude', 0.286851480783419), ('Longitude', 0.19583751493823956), ('Month', 0.014172515587621894), ('Year', 0.06843490729665995)]
Mean squared error is of 8078933810.49255
Mean absolute error:63714.41166586539
MAPE:0.2009743034343316
R2 score:0.6774204292804833
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:38:01
epoch 0  | loss: 1.64824 | val_0_rmse: 1.56479 | val_1_rmse: 1.61815 |  0:00:00s
epoch 1  | loss: 1.19656 | val_0_rmse: 1.13552 | val_1_rmse: 1.11472 |  0:00:00s
epoch 2  | loss: 0.79671 | val_0_rmse: 1.10637 | val_1_rmse: 1.07012 |  0:00:00s
epoch 3  | loss: 0.70191 | val_0_rmse: 1.03577 | val_1_rmse: 1.02952 |  0:00:00s
epoch 4  | loss: 0.66175 | val_0_rmse: 0.8948  | val_1_rmse: 0.89135 |  0:00:01s
epoch 5  | loss: 0.64468 | val_0_rmse: 0.88226 | val_1_rmse: 0.90362 |  0:00:01s
epoch 6  | loss: 0.61662 | val_0_rmse: 0.86642 | val_1_rmse: 0.85755 |  0:00:01s
epoch 7  | loss: 0.59077 | val_0_rmse: 0.82394 | val_1_rmse: 0.83306 |  0:00:01s
epoch 8  | loss: 0.57614 | val_0_rmse: 0.79386 | val_1_rmse: 0.80926 |  0:00:01s
epoch 9  | loss: 0.58214 | val_0_rmse: 0.7757  | val_1_rmse: 0.80115 |  0:00:02s
epoch 10 | loss: 0.55863 | val_0_rmse: 0.75591 | val_1_rmse: 0.79429 |  0:00:02s
epoch 11 | loss: 0.5326  | val_0_rmse: 0.74325 | val_1_rmse: 0.75933 |  0:00:02s
epoch 12 | loss: 0.51107 | val_0_rmse: 0.72263 | val_1_rmse: 0.73667 |  0:00:02s
epoch 13 | loss: 0.50232 | val_0_rmse: 0.76885 | val_1_rmse: 0.77568 |  0:00:03s
epoch 14 | loss: 0.51634 | val_0_rmse: 0.73559 | val_1_rmse: 0.7526  |  0:00:03s
epoch 15 | loss: 0.51099 | val_0_rmse: 0.71191 | val_1_rmse: 0.72039 |  0:00:03s
epoch 16 | loss: 0.47571 | val_0_rmse: 0.7501  | val_1_rmse: 0.77189 |  0:00:03s
epoch 17 | loss: 0.47847 | val_0_rmse: 0.73656 | val_1_rmse: 0.76111 |  0:00:03s
epoch 18 | loss: 0.46959 | val_0_rmse: 0.70142 | val_1_rmse: 0.73093 |  0:00:04s
epoch 19 | loss: 0.46964 | val_0_rmse: 0.73262 | val_1_rmse: 0.75735 |  0:00:04s
epoch 20 | loss: 0.46781 | val_0_rmse: 0.71231 | val_1_rmse: 0.72509 |  0:00:04s
epoch 21 | loss: 0.45531 | val_0_rmse: 0.688   | val_1_rmse: 0.70263 |  0:00:04s
epoch 22 | loss: 0.46642 | val_0_rmse: 0.67593 | val_1_rmse: 0.67956 |  0:00:05s
epoch 23 | loss: 0.44616 | val_0_rmse: 0.66737 | val_1_rmse: 0.67809 |  0:00:05s
epoch 24 | loss: 0.43515 | val_0_rmse: 0.65596 | val_1_rmse: 0.66635 |  0:00:05s
epoch 25 | loss: 0.41922 | val_0_rmse: 0.64937 | val_1_rmse: 0.65855 |  0:00:05s
epoch 26 | loss: 0.4213  | val_0_rmse: 0.63887 | val_1_rmse: 0.63976 |  0:00:05s
epoch 27 | loss: 0.42566 | val_0_rmse: 0.63562 | val_1_rmse: 0.63166 |  0:00:06s
epoch 28 | loss: 0.42565 | val_0_rmse: 0.6453  | val_1_rmse: 0.65172 |  0:00:06s
epoch 29 | loss: 0.4158  | val_0_rmse: 0.64911 | val_1_rmse: 0.69365 |  0:00:06s
epoch 30 | loss: 0.42032 | val_0_rmse: 0.65222 | val_1_rmse: 0.70535 |  0:00:06s
epoch 31 | loss: 0.40063 | val_0_rmse: 0.63802 | val_1_rmse: 0.69256 |  0:00:07s
epoch 32 | loss: 0.39817 | val_0_rmse: 0.62428 | val_1_rmse: 0.66052 |  0:00:07s
epoch 33 | loss: 0.406   | val_0_rmse: 0.62719 | val_1_rmse: 0.65748 |  0:00:07s
epoch 34 | loss: 0.40163 | val_0_rmse: 0.66446 | val_1_rmse: 0.69021 |  0:00:07s
epoch 35 | loss: 0.41645 | val_0_rmse: 0.6508  | val_1_rmse: 0.67538 |  0:00:07s
epoch 36 | loss: 0.39193 | val_0_rmse: 0.64727 | val_1_rmse: 0.66409 |  0:00:08s
epoch 37 | loss: 0.39063 | val_0_rmse: 0.63593 | val_1_rmse: 0.64919 |  0:00:08s
epoch 38 | loss: 0.38426 | val_0_rmse: 0.64057 | val_1_rmse: 0.67165 |  0:00:08s
epoch 39 | loss: 0.40933 | val_0_rmse: 0.64329 | val_1_rmse: 0.69365 |  0:00:08s
epoch 40 | loss: 0.37781 | val_0_rmse: 0.67188 | val_1_rmse: 0.7362  |  0:00:08s
epoch 41 | loss: 0.3962  | val_0_rmse: 0.65761 | val_1_rmse: 0.71676 |  0:00:09s
epoch 42 | loss: 0.39762 | val_0_rmse: 0.63097 | val_1_rmse: 0.69225 |  0:00:09s
epoch 43 | loss: 0.41502 | val_0_rmse: 0.63147 | val_1_rmse: 0.68793 |  0:00:09s
epoch 44 | loss: 0.38821 | val_0_rmse: 0.62047 | val_1_rmse: 0.67397 |  0:00:09s
epoch 45 | loss: 0.38422 | val_0_rmse: 0.59268 | val_1_rmse: 0.62623 |  0:00:10s
epoch 46 | loss: 0.36534 | val_0_rmse: 0.5938  | val_1_rmse: 0.62128 |  0:00:10s
epoch 47 | loss: 0.36022 | val_0_rmse: 0.59763 | val_1_rmse: 0.63612 |  0:00:10s
epoch 48 | loss: 0.38401 | val_0_rmse: 0.58883 | val_1_rmse: 0.61983 |  0:00:10s
epoch 49 | loss: 0.3891  | val_0_rmse: 0.59422 | val_1_rmse: 0.62959 |  0:00:10s
epoch 50 | loss: 0.36872 | val_0_rmse: 0.60796 | val_1_rmse: 0.64197 |  0:00:11s
epoch 51 | loss: 0.3831  | val_0_rmse: 0.61499 | val_1_rmse: 0.63117 |  0:00:11s
epoch 52 | loss: 0.38401 | val_0_rmse: 0.62323 | val_1_rmse: 0.61371 |  0:00:11s
epoch 53 | loss: 0.39336 | val_0_rmse: 0.63066 | val_1_rmse: 0.62543 |  0:00:11s
epoch 54 | loss: 0.4067  | val_0_rmse: 0.59653 | val_1_rmse: 0.59279 |  0:00:12s
epoch 55 | loss: 0.38098 | val_0_rmse: 0.60414 | val_1_rmse: 0.60232 |  0:00:12s
epoch 56 | loss: 0.38231 | val_0_rmse: 0.64014 | val_1_rmse: 0.64535 |  0:00:12s
epoch 57 | loss: 0.38399 | val_0_rmse: 0.61749 | val_1_rmse: 0.64856 |  0:00:12s
epoch 58 | loss: 0.38438 | val_0_rmse: 0.59395 | val_1_rmse: 0.6192  |  0:00:12s
epoch 59 | loss: 0.37603 | val_0_rmse: 0.61779 | val_1_rmse: 0.63425 |  0:00:13s
epoch 60 | loss: 0.38204 | val_0_rmse: 0.6281  | val_1_rmse: 0.64036 |  0:00:13s
epoch 61 | loss: 0.37338 | val_0_rmse: 0.6215  | val_1_rmse: 0.62961 |  0:00:13s
epoch 62 | loss: 0.38478 | val_0_rmse: 0.60313 | val_1_rmse: 0.60741 |  0:00:13s
epoch 63 | loss: 0.37368 | val_0_rmse: 0.59293 | val_1_rmse: 0.59392 |  0:00:13s
epoch 64 | loss: 0.36384 | val_0_rmse: 0.60196 | val_1_rmse: 0.60368 |  0:00:14s
epoch 65 | loss: 0.36726 | val_0_rmse: 0.5858  | val_1_rmse: 0.58151 |  0:00:14s
epoch 66 | loss: 0.35927 | val_0_rmse: 0.59412 | val_1_rmse: 0.59366 |  0:00:14s
epoch 67 | loss: 0.345   | val_0_rmse: 0.57535 | val_1_rmse: 0.57811 |  0:00:14s
epoch 68 | loss: 0.34565 | val_0_rmse: 0.56075 | val_1_rmse: 0.58035 |  0:00:15s
epoch 69 | loss: 0.34605 | val_0_rmse: 0.55697 | val_1_rmse: 0.56262 |  0:00:15s
epoch 70 | loss: 0.34821 | val_0_rmse: 0.57918 | val_1_rmse: 0.57812 |  0:00:15s
epoch 71 | loss: 0.35497 | val_0_rmse: 0.57097 | val_1_rmse: 0.58228 |  0:00:15s
epoch 72 | loss: 0.33405 | val_0_rmse: 0.5618  | val_1_rmse: 0.59006 |  0:00:15s
epoch 73 | loss: 0.33503 | val_0_rmse: 0.55274 | val_1_rmse: 0.58261 |  0:00:16s
epoch 74 | loss: 0.34364 | val_0_rmse: 0.56108 | val_1_rmse: 0.58797 |  0:00:16s
epoch 75 | loss: 0.34285 | val_0_rmse: 0.60264 | val_1_rmse: 0.60966 |  0:00:16s
epoch 76 | loss: 0.32775 | val_0_rmse: 0.56326 | val_1_rmse: 0.58651 |  0:00:16s
epoch 77 | loss: 0.32243 | val_0_rmse: 0.54539 | val_1_rmse: 0.56537 |  0:00:17s
epoch 78 | loss: 0.3402  | val_0_rmse: 0.54124 | val_1_rmse: 0.56757 |  0:00:17s
epoch 79 | loss: 0.31164 | val_0_rmse: 0.56017 | val_1_rmse: 0.59216 |  0:00:17s
epoch 80 | loss: 0.32393 | val_0_rmse: 0.55553 | val_1_rmse: 0.57543 |  0:00:17s
epoch 81 | loss: 0.31328 | val_0_rmse: 0.54649 | val_1_rmse: 0.56132 |  0:00:17s
epoch 82 | loss: 0.32228 | val_0_rmse: 0.54201 | val_1_rmse: 0.56246 |  0:00:18s
epoch 83 | loss: 0.31116 | val_0_rmse: 0.57297 | val_1_rmse: 0.61062 |  0:00:18s
epoch 84 | loss: 0.32427 | val_0_rmse: 0.55404 | val_1_rmse: 0.59937 |  0:00:18s
epoch 85 | loss: 0.3165  | val_0_rmse: 0.54708 | val_1_rmse: 0.58866 |  0:00:18s
epoch 86 | loss: 0.30312 | val_0_rmse: 0.54002 | val_1_rmse: 0.57688 |  0:00:19s
epoch 87 | loss: 0.31206 | val_0_rmse: 0.52258 | val_1_rmse: 0.55977 |  0:00:19s
epoch 88 | loss: 0.2851  | val_0_rmse: 0.52198 | val_1_rmse: 0.56347 |  0:00:19s
epoch 89 | loss: 0.31487 | val_0_rmse: 0.52873 | val_1_rmse: 0.5659  |  0:00:19s
epoch 90 | loss: 0.30735 | val_0_rmse: 0.52345 | val_1_rmse: 0.55178 |  0:00:19s
epoch 91 | loss: 0.29363 | val_0_rmse: 0.52743 | val_1_rmse: 0.54917 |  0:00:20s
epoch 92 | loss: 0.2912  | val_0_rmse: 0.53033 | val_1_rmse: 0.55421 |  0:00:20s
epoch 93 | loss: 0.29447 | val_0_rmse: 0.51954 | val_1_rmse: 0.55727 |  0:00:20s
epoch 94 | loss: 0.3109  | val_0_rmse: 0.52078 | val_1_rmse: 0.57546 |  0:00:20s
epoch 95 | loss: 0.30528 | val_0_rmse: 0.5297  | val_1_rmse: 0.58901 |  0:00:21s
epoch 96 | loss: 0.29813 | val_0_rmse: 0.51694 | val_1_rmse: 0.57145 |  0:00:21s
epoch 97 | loss: 0.29935 | val_0_rmse: 0.52016 | val_1_rmse: 0.55467 |  0:00:21s
epoch 98 | loss: 0.2974  | val_0_rmse: 0.52417 | val_1_rmse: 0.54467 |  0:00:21s
epoch 99 | loss: 0.31597 | val_0_rmse: 0.51637 | val_1_rmse: 0.54082 |  0:00:21s
epoch 100| loss: 0.29388 | val_0_rmse: 0.53113 | val_1_rmse: 0.55946 |  0:00:22s
epoch 101| loss: 0.30487 | val_0_rmse: 0.5274  | val_1_rmse: 0.56454 |  0:00:22s
epoch 102| loss: 0.30144 | val_0_rmse: 0.52657 | val_1_rmse: 0.56393 |  0:00:22s
epoch 103| loss: 0.28796 | val_0_rmse: 0.51943 | val_1_rmse: 0.55891 |  0:00:22s
epoch 104| loss: 0.28452 | val_0_rmse: 0.52056 | val_1_rmse: 0.55772 |  0:00:23s
epoch 105| loss: 0.2888  | val_0_rmse: 0.50255 | val_1_rmse: 0.55055 |  0:00:23s
epoch 106| loss: 0.29142 | val_0_rmse: 0.50305 | val_1_rmse: 0.55531 |  0:00:23s
epoch 107| loss: 0.27687 | val_0_rmse: 0.49651 | val_1_rmse: 0.55065 |  0:00:23s
epoch 108| loss: 0.28401 | val_0_rmse: 0.5082  | val_1_rmse: 0.55311 |  0:00:23s
epoch 109| loss: 0.28031 | val_0_rmse: 0.51529 | val_1_rmse: 0.56382 |  0:00:24s
epoch 110| loss: 0.28153 | val_0_rmse: 0.50811 | val_1_rmse: 0.55974 |  0:00:24s
epoch 111| loss: 0.27974 | val_0_rmse: 0.51341 | val_1_rmse: 0.58106 |  0:00:24s
epoch 112| loss: 0.2771  | val_0_rmse: 0.50906 | val_1_rmse: 0.592   |  0:00:24s
epoch 113| loss: 0.29267 | val_0_rmse: 0.49405 | val_1_rmse: 0.57612 |  0:00:25s
epoch 114| loss: 0.28066 | val_0_rmse: 0.49998 | val_1_rmse: 0.56306 |  0:00:25s
epoch 115| loss: 0.27438 | val_0_rmse: 0.49485 | val_1_rmse: 0.55655 |  0:00:25s
epoch 116| loss: 0.27621 | val_0_rmse: 0.49452 | val_1_rmse: 0.57035 |  0:00:25s
epoch 117| loss: 0.27322 | val_0_rmse: 0.48763 | val_1_rmse: 0.56736 |  0:00:25s
epoch 118| loss: 0.26619 | val_0_rmse: 0.4781  | val_1_rmse: 0.56547 |  0:00:26s
epoch 119| loss: 0.26602 | val_0_rmse: 0.48912 | val_1_rmse: 0.57596 |  0:00:26s
epoch 120| loss: 0.26752 | val_0_rmse: 0.49384 | val_1_rmse: 0.56948 |  0:00:26s
epoch 121| loss: 0.271   | val_0_rmse: 0.49047 | val_1_rmse: 0.56607 |  0:00:26s
epoch 122| loss: 0.29351 | val_0_rmse: 0.49017 | val_1_rmse: 0.56879 |  0:00:27s
epoch 123| loss: 0.29184 | val_0_rmse: 0.4914  | val_1_rmse: 0.57664 |  0:00:27s
epoch 124| loss: 0.25211 | val_0_rmse: 0.50137 | val_1_rmse: 0.57842 |  0:00:27s
epoch 125| loss: 0.26192 | val_0_rmse: 0.50077 | val_1_rmse: 0.57898 |  0:00:27s
epoch 126| loss: 0.26538 | val_0_rmse: 0.48852 | val_1_rmse: 0.57067 |  0:00:27s
epoch 127| loss: 0.26845 | val_0_rmse: 0.48511 | val_1_rmse: 0.57234 |  0:00:28s
epoch 128| loss: 0.26143 | val_0_rmse: 0.48889 | val_1_rmse: 0.55934 |  0:00:28s
epoch 129| loss: 0.28129 | val_0_rmse: 0.49759 | val_1_rmse: 0.56555 |  0:00:28s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.54082
Best weights from best epoch are automatically used!
ended training at: 02:38:30
Feature importance:
[('Area', 0.25007123451927604), ('Baths', 0.06789138213894531), ('Beds', 0.0648437636562154), ('Latitude', 0.32755916069617314), ('Longitude', 0.15233595460725158), ('Month', 0.01356188095316727), ('Year', 0.12373662342897128)]
Mean squared error is of 8003929208.428722
Mean absolute error:63641.557379464284
MAPE:0.21485607378233648
R2 score:0.655830422709561
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:38:30
epoch 0  | loss: 1.5521  | val_0_rmse: 1.27881 | val_1_rmse: 1.24619 |  0:00:00s
epoch 1  | loss: 0.9445  | val_0_rmse: 1.17835 | val_1_rmse: 1.28031 |  0:00:00s
epoch 2  | loss: 0.7562  | val_0_rmse: 0.97157 | val_1_rmse: 1.01967 |  0:00:00s
epoch 3  | loss: 0.66163 | val_0_rmse: 0.94642 | val_1_rmse: 0.95187 |  0:00:00s
epoch 4  | loss: 0.59777 | val_0_rmse: 0.88433 | val_1_rmse: 0.91857 |  0:00:01s
epoch 5  | loss: 0.58162 | val_0_rmse: 0.84137 | val_1_rmse: 0.92354 |  0:00:01s
epoch 6  | loss: 0.5792  | val_0_rmse: 0.85728 | val_1_rmse: 0.93672 |  0:00:01s
epoch 7  | loss: 0.55045 | val_0_rmse: 0.85635 | val_1_rmse: 0.90951 |  0:00:01s
epoch 8  | loss: 0.53187 | val_0_rmse: 0.79372 | val_1_rmse: 0.82043 |  0:00:02s
epoch 9  | loss: 0.50934 | val_0_rmse: 0.7726  | val_1_rmse: 0.80667 |  0:00:02s
epoch 10 | loss: 0.51827 | val_0_rmse: 0.76263 | val_1_rmse: 0.7927  |  0:00:02s
epoch 11 | loss: 0.48439 | val_0_rmse: 0.70233 | val_1_rmse: 0.77592 |  0:00:02s
epoch 12 | loss: 0.47128 | val_0_rmse: 0.71289 | val_1_rmse: 0.80297 |  0:00:03s
epoch 13 | loss: 0.49645 | val_0_rmse: 0.71587 | val_1_rmse: 0.79561 |  0:00:03s
epoch 14 | loss: 0.47076 | val_0_rmse: 0.70503 | val_1_rmse: 0.77087 |  0:00:03s
epoch 15 | loss: 0.4507  | val_0_rmse: 0.67517 | val_1_rmse: 0.74935 |  0:00:03s
epoch 16 | loss: 0.45051 | val_0_rmse: 0.69067 | val_1_rmse: 0.76489 |  0:00:03s
epoch 17 | loss: 0.45562 | val_0_rmse: 0.67399 | val_1_rmse: 0.72799 |  0:00:04s
epoch 18 | loss: 0.43849 | val_0_rmse: 0.68876 | val_1_rmse: 0.73806 |  0:00:04s
epoch 19 | loss: 0.44737 | val_0_rmse: 0.64381 | val_1_rmse: 0.70231 |  0:00:04s
epoch 20 | loss: 0.43422 | val_0_rmse: 0.64161 | val_1_rmse: 0.71214 |  0:00:04s
epoch 21 | loss: 0.43189 | val_0_rmse: 0.64069 | val_1_rmse: 0.70853 |  0:00:05s
epoch 22 | loss: 0.41197 | val_0_rmse: 0.63554 | val_1_rmse: 0.67741 |  0:00:05s
epoch 23 | loss: 0.40321 | val_0_rmse: 0.63509 | val_1_rmse: 0.68099 |  0:00:05s
epoch 24 | loss: 0.41342 | val_0_rmse: 0.61533 | val_1_rmse: 0.65923 |  0:00:05s
epoch 25 | loss: 0.41703 | val_0_rmse: 0.60216 | val_1_rmse: 0.66384 |  0:00:06s
epoch 26 | loss: 0.3825  | val_0_rmse: 0.60207 | val_1_rmse: 0.64477 |  0:00:06s
epoch 27 | loss: 0.37574 | val_0_rmse: 0.63742 | val_1_rmse: 0.65261 |  0:00:06s
epoch 28 | loss: 0.39632 | val_0_rmse: 0.61972 | val_1_rmse: 0.6523  |  0:00:06s
epoch 29 | loss: 0.37319 | val_0_rmse: 0.61627 | val_1_rmse: 0.66173 |  0:00:06s
epoch 30 | loss: 0.39459 | val_0_rmse: 0.62413 | val_1_rmse: 0.67253 |  0:00:07s
epoch 31 | loss: 0.38988 | val_0_rmse: 0.62564 | val_1_rmse: 0.67408 |  0:00:07s
epoch 32 | loss: 0.39059 | val_0_rmse: 0.61193 | val_1_rmse: 0.67387 |  0:00:07s
epoch 33 | loss: 0.40636 | val_0_rmse: 0.64321 | val_1_rmse: 0.71241 |  0:00:07s
epoch 34 | loss: 0.3958  | val_0_rmse: 0.64535 | val_1_rmse: 0.71324 |  0:00:08s
epoch 35 | loss: 0.39991 | val_0_rmse: 0.62211 | val_1_rmse: 0.70535 |  0:00:08s
epoch 36 | loss: 0.39052 | val_0_rmse: 0.65105 | val_1_rmse: 0.73847 |  0:00:08s
epoch 37 | loss: 0.37711 | val_0_rmse: 0.65668 | val_1_rmse: 0.71654 |  0:00:08s
epoch 38 | loss: 0.39119 | val_0_rmse: 0.59317 | val_1_rmse: 0.6484  |  0:00:08s
epoch 39 | loss: 0.40541 | val_0_rmse: 0.58929 | val_1_rmse: 0.64859 |  0:00:09s
epoch 40 | loss: 0.3928  | val_0_rmse: 0.60647 | val_1_rmse: 0.68124 |  0:00:09s
epoch 41 | loss: 0.39459 | val_0_rmse: 0.59362 | val_1_rmse: 0.67253 |  0:00:09s
epoch 42 | loss: 0.37114 | val_0_rmse: 0.59357 | val_1_rmse: 0.66353 |  0:00:09s
epoch 43 | loss: 0.36812 | val_0_rmse: 0.60971 | val_1_rmse: 0.66099 |  0:00:10s
epoch 44 | loss: 0.37485 | val_0_rmse: 0.60823 | val_1_rmse: 0.66107 |  0:00:10s
epoch 45 | loss: 0.37777 | val_0_rmse: 0.57046 | val_1_rmse: 0.62316 |  0:00:10s
epoch 46 | loss: 0.35962 | val_0_rmse: 0.58449 | val_1_rmse: 0.62517 |  0:00:10s
epoch 47 | loss: 0.35285 | val_0_rmse: 0.5661  | val_1_rmse: 0.59806 |  0:00:10s
epoch 48 | loss: 0.34405 | val_0_rmse: 0.56332 | val_1_rmse: 0.60237 |  0:00:11s
epoch 49 | loss: 0.35081 | val_0_rmse: 0.5956  | val_1_rmse: 0.63681 |  0:00:11s
epoch 50 | loss: 0.35996 | val_0_rmse: 0.56649 | val_1_rmse: 0.61169 |  0:00:11s
epoch 51 | loss: 0.3409  | val_0_rmse: 0.56817 | val_1_rmse: 0.61708 |  0:00:11s
epoch 52 | loss: 0.34567 | val_0_rmse: 0.57132 | val_1_rmse: 0.60736 |  0:00:12s
epoch 53 | loss: 0.33672 | val_0_rmse: 0.55344 | val_1_rmse: 0.59693 |  0:00:12s
epoch 54 | loss: 0.35125 | val_0_rmse: 0.55993 | val_1_rmse: 0.61471 |  0:00:12s
epoch 55 | loss: 0.32169 | val_0_rmse: 0.54503 | val_1_rmse: 0.58883 |  0:00:12s
epoch 56 | loss: 0.32191 | val_0_rmse: 0.55036 | val_1_rmse: 0.59636 |  0:00:13s
epoch 57 | loss: 0.3346  | val_0_rmse: 0.54964 | val_1_rmse: 0.61558 |  0:00:13s
epoch 58 | loss: 0.32129 | val_0_rmse: 0.54196 | val_1_rmse: 0.60876 |  0:00:13s
epoch 59 | loss: 0.33407 | val_0_rmse: 0.54141 | val_1_rmse: 0.60058 |  0:00:13s
epoch 60 | loss: 0.30447 | val_0_rmse: 0.53756 | val_1_rmse: 0.60234 |  0:00:13s
epoch 61 | loss: 0.31568 | val_0_rmse: 0.53868 | val_1_rmse: 0.59855 |  0:00:14s
epoch 62 | loss: 0.32721 | val_0_rmse: 0.54034 | val_1_rmse: 0.59689 |  0:00:14s
epoch 63 | loss: 0.31415 | val_0_rmse: 0.53954 | val_1_rmse: 0.59863 |  0:00:14s
epoch 64 | loss: 0.31062 | val_0_rmse: 0.54407 | val_1_rmse: 0.59673 |  0:00:14s
epoch 65 | loss: 0.31016 | val_0_rmse: 0.54743 | val_1_rmse: 0.60459 |  0:00:15s
epoch 66 | loss: 0.32189 | val_0_rmse: 0.56623 | val_1_rmse: 0.61585 |  0:00:15s
epoch 67 | loss: 0.32273 | val_0_rmse: 0.56894 | val_1_rmse: 0.62546 |  0:00:15s
epoch 68 | loss: 0.33559 | val_0_rmse: 0.54534 | val_1_rmse: 0.60194 |  0:00:15s
epoch 69 | loss: 0.3258  | val_0_rmse: 0.541   | val_1_rmse: 0.60993 |  0:00:15s
epoch 70 | loss: 0.31183 | val_0_rmse: 0.53329 | val_1_rmse: 0.61653 |  0:00:16s
epoch 71 | loss: 0.33631 | val_0_rmse: 0.53181 | val_1_rmse: 0.61079 |  0:00:16s
epoch 72 | loss: 0.32048 | val_0_rmse: 0.53691 | val_1_rmse: 0.59358 |  0:00:16s
epoch 73 | loss: 0.33369 | val_0_rmse: 0.53731 | val_1_rmse: 0.58674 |  0:00:16s
epoch 74 | loss: 0.31193 | val_0_rmse: 0.53293 | val_1_rmse: 0.59544 |  0:00:17s
epoch 75 | loss: 0.30837 | val_0_rmse: 0.5455  | val_1_rmse: 0.60336 |  0:00:17s
epoch 76 | loss: 0.31128 | val_0_rmse: 0.5438  | val_1_rmse: 0.58693 |  0:00:17s
epoch 77 | loss: 0.29747 | val_0_rmse: 0.54371 | val_1_rmse: 0.59951 |  0:00:17s
epoch 78 | loss: 0.30802 | val_0_rmse: 0.54729 | val_1_rmse: 0.63001 |  0:00:18s
epoch 79 | loss: 0.30414 | val_0_rmse: 0.52242 | val_1_rmse: 0.606   |  0:00:18s
epoch 80 | loss: 0.31097 | val_0_rmse: 0.52042 | val_1_rmse: 0.59009 |  0:00:18s
epoch 81 | loss: 0.30232 | val_0_rmse: 0.52126 | val_1_rmse: 0.58885 |  0:00:18s
epoch 82 | loss: 0.30525 | val_0_rmse: 0.52588 | val_1_rmse: 0.59877 |  0:00:18s
epoch 83 | loss: 0.31032 | val_0_rmse: 0.52193 | val_1_rmse: 0.60961 |  0:00:19s
epoch 84 | loss: 0.29986 | val_0_rmse: 0.53082 | val_1_rmse: 0.61282 |  0:00:19s
epoch 85 | loss: 0.31557 | val_0_rmse: 0.51739 | val_1_rmse: 0.60022 |  0:00:19s
epoch 86 | loss: 0.29917 | val_0_rmse: 0.52347 | val_1_rmse: 0.60985 |  0:00:19s
epoch 87 | loss: 0.28754 | val_0_rmse: 0.52427 | val_1_rmse: 0.60138 |  0:00:20s
epoch 88 | loss: 0.30637 | val_0_rmse: 0.51853 | val_1_rmse: 0.59289 |  0:00:20s
epoch 89 | loss: 0.30525 | val_0_rmse: 0.52656 | val_1_rmse: 0.60776 |  0:00:20s
epoch 90 | loss: 0.30012 | val_0_rmse: 0.51562 | val_1_rmse: 0.58977 |  0:00:20s
epoch 91 | loss: 0.28437 | val_0_rmse: 0.53929 | val_1_rmse: 0.607   |  0:00:20s
epoch 92 | loss: 0.305   | val_0_rmse: 0.52135 | val_1_rmse: 0.60272 |  0:00:21s
epoch 93 | loss: 0.29733 | val_0_rmse: 0.51471 | val_1_rmse: 0.59927 |  0:00:21s
epoch 94 | loss: 0.29119 | val_0_rmse: 0.53435 | val_1_rmse: 0.62127 |  0:00:21s
epoch 95 | loss: 0.29179 | val_0_rmse: 0.57958 | val_1_rmse: 0.65767 |  0:00:21s
epoch 96 | loss: 0.32809 | val_0_rmse: 0.53727 | val_1_rmse: 0.6046  |  0:00:22s
epoch 97 | loss: 0.30458 | val_0_rmse: 0.52723 | val_1_rmse: 0.59889 |  0:00:22s
epoch 98 | loss: 0.30791 | val_0_rmse: 0.52259 | val_1_rmse: 0.60211 |  0:00:22s
epoch 99 | loss: 0.30386 | val_0_rmse: 0.51502 | val_1_rmse: 0.59934 |  0:00:22s
epoch 100| loss: 0.29376 | val_0_rmse: 0.52059 | val_1_rmse: 0.59726 |  0:00:23s
epoch 101| loss: 0.29208 | val_0_rmse: 0.52775 | val_1_rmse: 0.60156 |  0:00:23s
epoch 102| loss: 0.29789 | val_0_rmse: 0.52589 | val_1_rmse: 0.61505 |  0:00:23s
epoch 103| loss: 0.30093 | val_0_rmse: 0.52057 | val_1_rmse: 0.61176 |  0:00:23s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.58674
Best weights from best epoch are automatically used!
ended training at: 02:38:54
Feature importance:
[('Area', 0.27665030188417317), ('Baths', 0.09452952030082751), ('Beds', 0.03010237337954301), ('Latitude', 0.2714865819057053), ('Longitude', 0.15520965587328633), ('Month', 0.021305286137984436), ('Year', 0.15071628051848027)]
Mean squared error is of 9158058118.675295
Mean absolute error:69323.04948695055
MAPE:0.259666765182503
R2 score:0.5816360059453785
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:38:54
epoch 0  | loss: 1.74562 | val_0_rmse: 1.09544 | val_1_rmse: 1.06994 |  0:00:00s
epoch 1  | loss: 0.76594 | val_0_rmse: 1.0872  | val_1_rmse: 1.1021  |  0:00:00s
epoch 2  | loss: 0.6598  | val_0_rmse: 1.15249 | val_1_rmse: 1.18485 |  0:00:00s
epoch 3  | loss: 0.59706 | val_0_rmse: 0.98439 | val_1_rmse: 0.99867 |  0:00:00s
epoch 4  | loss: 0.54297 | val_0_rmse: 0.93625 | val_1_rmse: 0.95565 |  0:00:01s
epoch 5  | loss: 0.53702 | val_0_rmse: 0.83553 | val_1_rmse: 0.8833  |  0:00:01s
epoch 6  | loss: 0.50209 | val_0_rmse: 0.83257 | val_1_rmse: 0.87464 |  0:00:01s
epoch 7  | loss: 0.51586 | val_0_rmse: 0.81584 | val_1_rmse: 0.84705 |  0:00:01s
epoch 8  | loss: 0.49353 | val_0_rmse: 0.79607 | val_1_rmse: 0.80278 |  0:00:02s
epoch 9  | loss: 0.49072 | val_0_rmse: 0.76968 | val_1_rmse: 0.7575  |  0:00:02s
epoch 10 | loss: 0.47988 | val_0_rmse: 0.76789 | val_1_rmse: 0.78136 |  0:00:02s
epoch 11 | loss: 0.49071 | val_0_rmse: 0.73917 | val_1_rmse: 0.75898 |  0:00:02s
epoch 12 | loss: 0.48696 | val_0_rmse: 0.71529 | val_1_rmse: 0.75933 |  0:00:03s
epoch 13 | loss: 0.47168 | val_0_rmse: 0.71527 | val_1_rmse: 0.74496 |  0:00:03s
epoch 14 | loss: 0.46744 | val_0_rmse: 0.70991 | val_1_rmse: 0.73826 |  0:00:03s
epoch 15 | loss: 0.4764  | val_0_rmse: 0.69756 | val_1_rmse: 0.734   |  0:00:03s
epoch 16 | loss: 0.48431 | val_0_rmse: 0.68902 | val_1_rmse: 0.7369  |  0:00:04s
epoch 17 | loss: 0.47241 | val_0_rmse: 0.67804 | val_1_rmse: 0.72464 |  0:00:04s
epoch 18 | loss: 0.47031 | val_0_rmse: 0.69259 | val_1_rmse: 0.74327 |  0:00:04s
epoch 19 | loss: 0.4604  | val_0_rmse: 0.6987  | val_1_rmse: 0.7527  |  0:00:04s
epoch 20 | loss: 0.47092 | val_0_rmse: 0.67597 | val_1_rmse: 0.72606 |  0:00:05s
epoch 21 | loss: 0.45918 | val_0_rmse: 0.67719 | val_1_rmse: 0.72342 |  0:00:05s
epoch 22 | loss: 0.46881 | val_0_rmse: 0.68254 | val_1_rmse: 0.7292  |  0:00:05s
epoch 23 | loss: 0.45775 | val_0_rmse: 0.66852 | val_1_rmse: 0.71103 |  0:00:05s
epoch 24 | loss: 0.45533 | val_0_rmse: 0.66835 | val_1_rmse: 0.70472 |  0:00:05s
epoch 25 | loss: 0.45822 | val_0_rmse: 0.67355 | val_1_rmse: 0.7138  |  0:00:06s
epoch 26 | loss: 0.44784 | val_0_rmse: 0.66785 | val_1_rmse: 0.71451 |  0:00:06s
epoch 27 | loss: 0.45126 | val_0_rmse: 0.66276 | val_1_rmse: 0.70549 |  0:00:06s
epoch 28 | loss: 0.45249 | val_0_rmse: 0.66165 | val_1_rmse: 0.70404 |  0:00:06s
epoch 29 | loss: 0.4616  | val_0_rmse: 0.67512 | val_1_rmse: 0.72276 |  0:00:07s
epoch 30 | loss: 0.46644 | val_0_rmse: 0.66103 | val_1_rmse: 0.70473 |  0:00:07s
epoch 31 | loss: 0.45313 | val_0_rmse: 0.67525 | val_1_rmse: 0.71195 |  0:00:07s
epoch 32 | loss: 0.45889 | val_0_rmse: 0.66584 | val_1_rmse: 0.71244 |  0:00:07s
epoch 33 | loss: 0.46976 | val_0_rmse: 0.6617  | val_1_rmse: 0.71084 |  0:00:07s
epoch 34 | loss: 0.45876 | val_0_rmse: 0.66415 | val_1_rmse: 0.70701 |  0:00:08s
epoch 35 | loss: 0.44825 | val_0_rmse: 0.66428 | val_1_rmse: 0.70979 |  0:00:08s
epoch 36 | loss: 0.44071 | val_0_rmse: 0.6641  | val_1_rmse: 0.7133  |  0:00:08s
epoch 37 | loss: 0.44211 | val_0_rmse: 0.66379 | val_1_rmse: 0.71021 |  0:00:08s
epoch 38 | loss: 0.44387 | val_0_rmse: 0.6604  | val_1_rmse: 0.69923 |  0:00:09s
epoch 39 | loss: 0.44254 | val_0_rmse: 0.65773 | val_1_rmse: 0.70468 |  0:00:09s
epoch 40 | loss: 0.44257 | val_0_rmse: 0.66545 | val_1_rmse: 0.71924 |  0:00:09s
epoch 41 | loss: 0.44433 | val_0_rmse: 0.65262 | val_1_rmse: 0.70735 |  0:00:09s
epoch 42 | loss: 0.44814 | val_0_rmse: 0.65376 | val_1_rmse: 0.70184 |  0:00:10s
epoch 43 | loss: 0.45008 | val_0_rmse: 0.66471 | val_1_rmse: 0.71564 |  0:00:10s
epoch 44 | loss: 0.43932 | val_0_rmse: 0.64685 | val_1_rmse: 0.69984 |  0:00:10s
epoch 45 | loss: 0.42665 | val_0_rmse: 0.64495 | val_1_rmse: 0.70276 |  0:00:10s
epoch 46 | loss: 0.42234 | val_0_rmse: 0.65746 | val_1_rmse: 0.70869 |  0:00:11s
epoch 47 | loss: 0.42174 | val_0_rmse: 0.64518 | val_1_rmse: 0.70029 |  0:00:11s
epoch 48 | loss: 0.431   | val_0_rmse: 0.65756 | val_1_rmse: 0.70989 |  0:00:11s
epoch 49 | loss: 0.43291 | val_0_rmse: 0.65746 | val_1_rmse: 0.71545 |  0:00:11s
epoch 50 | loss: 0.4398  | val_0_rmse: 0.66755 | val_1_rmse: 0.73168 |  0:00:11s
epoch 51 | loss: 0.44958 | val_0_rmse: 0.67879 | val_1_rmse: 0.73687 |  0:00:12s
epoch 52 | loss: 0.42452 | val_0_rmse: 0.71616 | val_1_rmse: 0.7678  |  0:00:12s
epoch 53 | loss: 0.42581 | val_0_rmse: 0.69378 | val_1_rmse: 0.74631 |  0:00:12s
epoch 54 | loss: 0.42027 | val_0_rmse: 0.65652 | val_1_rmse: 0.69892 |  0:00:12s
epoch 55 | loss: 0.41815 | val_0_rmse: 0.66357 | val_1_rmse: 0.68782 |  0:00:13s
epoch 56 | loss: 0.42022 | val_0_rmse: 0.82956 | val_1_rmse: 0.82303 |  0:00:13s
epoch 57 | loss: 0.42569 | val_0_rmse: 0.81503 | val_1_rmse: 0.83415 |  0:00:13s
epoch 58 | loss: 0.4183  | val_0_rmse: 0.63851 | val_1_rmse: 0.69347 |  0:00:13s
epoch 59 | loss: 0.41205 | val_0_rmse: 0.6721  | val_1_rmse: 0.728   |  0:00:14s
epoch 60 | loss: 0.42313 | val_0_rmse: 0.7995  | val_1_rmse: 0.82377 |  0:00:14s
epoch 61 | loss: 0.41626 | val_0_rmse: 0.74591 | val_1_rmse: 0.7736  |  0:00:14s
epoch 62 | loss: 0.40664 | val_0_rmse: 0.69829 | val_1_rmse: 0.74457 |  0:00:14s
epoch 63 | loss: 0.39623 | val_0_rmse: 0.70385 | val_1_rmse: 0.784   |  0:00:14s
epoch 64 | loss: 0.39675 | val_0_rmse: 0.74131 | val_1_rmse: 0.80912 |  0:00:15s
epoch 65 | loss: 0.40335 | val_0_rmse: 0.7216  | val_1_rmse: 0.78034 |  0:00:15s
epoch 66 | loss: 0.40021 | val_0_rmse: 0.66657 | val_1_rmse: 0.73096 |  0:00:15s
epoch 67 | loss: 0.36928 | val_0_rmse: 0.64088 | val_1_rmse: 0.74107 |  0:00:15s
epoch 68 | loss: 0.39577 | val_0_rmse: 0.61946 | val_1_rmse: 0.71733 |  0:00:16s
epoch 69 | loss: 0.38417 | val_0_rmse: 0.6259  | val_1_rmse: 0.71624 |  0:00:16s
epoch 70 | loss: 0.40499 | val_0_rmse: 0.6257  | val_1_rmse: 0.72191 |  0:00:16s
epoch 71 | loss: 0.41444 | val_0_rmse: 0.63529 | val_1_rmse: 0.72046 |  0:00:16s
epoch 72 | loss: 0.4056  | val_0_rmse: 0.64005 | val_1_rmse: 0.70206 |  0:00:16s
epoch 73 | loss: 0.41116 | val_0_rmse: 0.69231 | val_1_rmse: 0.73827 |  0:00:17s
epoch 74 | loss: 0.4167  | val_0_rmse: 0.66889 | val_1_rmse: 0.72092 |  0:00:17s
epoch 75 | loss: 0.41073 | val_0_rmse: 0.66777 | val_1_rmse: 0.72611 |  0:00:17s
epoch 76 | loss: 0.40062 | val_0_rmse: 0.6452  | val_1_rmse: 0.71535 |  0:00:17s
epoch 77 | loss: 0.42472 | val_0_rmse: 0.65328 | val_1_rmse: 0.72928 |  0:00:18s
epoch 78 | loss: 0.40977 | val_0_rmse: 0.66995 | val_1_rmse: 0.7608  |  0:00:18s
epoch 79 | loss: 0.39192 | val_0_rmse: 0.65304 | val_1_rmse: 0.7056  |  0:00:18s
epoch 80 | loss: 0.39008 | val_0_rmse: 0.64163 | val_1_rmse: 0.6761  |  0:00:18s
epoch 81 | loss: 0.39433 | val_0_rmse: 0.64817 | val_1_rmse: 0.67663 |  0:00:19s
epoch 82 | loss: 0.40091 | val_0_rmse: 0.62031 | val_1_rmse: 0.68275 |  0:00:19s
epoch 83 | loss: 0.38857 | val_0_rmse: 0.64895 | val_1_rmse: 0.7111  |  0:00:19s
epoch 84 | loss: 0.42136 | val_0_rmse: 0.62822 | val_1_rmse: 0.70169 |  0:00:19s
epoch 85 | loss: 0.38824 | val_0_rmse: 0.62962 | val_1_rmse: 0.69602 |  0:00:20s
epoch 86 | loss: 0.39932 | val_0_rmse: 0.62453 | val_1_rmse: 0.69411 |  0:00:20s
epoch 87 | loss: 0.38309 | val_0_rmse: 0.62592 | val_1_rmse: 0.7011  |  0:00:20s
epoch 88 | loss: 0.38401 | val_0_rmse: 0.64727 | val_1_rmse: 0.70757 |  0:00:20s
epoch 89 | loss: 0.37623 | val_0_rmse: 0.63887 | val_1_rmse: 0.71062 |  0:00:20s
epoch 90 | loss: 0.37957 | val_0_rmse: 0.62397 | val_1_rmse: 0.69512 |  0:00:21s
epoch 91 | loss: 0.39416 | val_0_rmse: 0.63902 | val_1_rmse: 0.70755 |  0:00:21s
epoch 92 | loss: 0.36496 | val_0_rmse: 0.66343 | val_1_rmse: 0.73729 |  0:00:21s
epoch 93 | loss: 0.39141 | val_0_rmse: 0.65269 | val_1_rmse: 0.73688 |  0:00:21s
epoch 94 | loss: 0.36686 | val_0_rmse: 0.648   | val_1_rmse: 0.74299 |  0:00:22s
epoch 95 | loss: 0.36844 | val_0_rmse: 0.65317 | val_1_rmse: 0.74252 |  0:00:22s
epoch 96 | loss: 0.3661  | val_0_rmse: 0.62348 | val_1_rmse: 0.70878 |  0:00:22s
epoch 97 | loss: 0.37292 | val_0_rmse: 0.59649 | val_1_rmse: 0.68075 |  0:00:22s
epoch 98 | loss: 0.35559 | val_0_rmse: 0.58742 | val_1_rmse: 0.66635 |  0:00:22s
epoch 99 | loss: 0.36723 | val_0_rmse: 0.59302 | val_1_rmse: 0.67296 |  0:00:23s
epoch 100| loss: 0.36928 | val_0_rmse: 0.61257 | val_1_rmse: 0.6968  |  0:00:23s
epoch 101| loss: 0.37704 | val_0_rmse: 0.6015  | val_1_rmse: 0.68735 |  0:00:23s
epoch 102| loss: 0.39622 | val_0_rmse: 0.62769 | val_1_rmse: 0.716   |  0:00:23s
epoch 103| loss: 0.37523 | val_0_rmse: 0.63293 | val_1_rmse: 0.70845 |  0:00:24s
epoch 104| loss: 0.37262 | val_0_rmse: 0.63288 | val_1_rmse: 0.7097  |  0:00:24s
epoch 105| loss: 0.37482 | val_0_rmse: 0.60238 | val_1_rmse: 0.67467 |  0:00:24s
epoch 106| loss: 0.36852 | val_0_rmse: 0.65255 | val_1_rmse: 0.71526 |  0:00:24s
epoch 107| loss: 0.37227 | val_0_rmse: 0.63677 | val_1_rmse: 0.70255 |  0:00:25s
epoch 108| loss: 0.37303 | val_0_rmse: 0.59839 | val_1_rmse: 0.66284 |  0:00:25s
epoch 109| loss: 0.36121 | val_0_rmse: 0.63759 | val_1_rmse: 0.70454 |  0:00:25s
epoch 110| loss: 0.37747 | val_0_rmse: 0.5873  | val_1_rmse: 0.65117 |  0:00:25s
epoch 111| loss: 0.35562 | val_0_rmse: 0.64109 | val_1_rmse: 0.69129 |  0:00:25s
epoch 112| loss: 0.36833 | val_0_rmse: 0.59201 | val_1_rmse: 0.65247 |  0:00:26s
epoch 113| loss: 0.35071 | val_0_rmse: 0.58314 | val_1_rmse: 0.64986 |  0:00:26s
epoch 114| loss: 0.35395 | val_0_rmse: 0.61295 | val_1_rmse: 0.67902 |  0:00:26s
epoch 115| loss: 0.34068 | val_0_rmse: 0.59627 | val_1_rmse: 0.66614 |  0:00:26s
epoch 116| loss: 0.3535  | val_0_rmse: 0.58108 | val_1_rmse: 0.6554  |  0:00:27s
epoch 117| loss: 0.34966 | val_0_rmse: 0.58026 | val_1_rmse: 0.65638 |  0:00:27s
epoch 118| loss: 0.35141 | val_0_rmse: 0.5983  | val_1_rmse: 0.66863 |  0:00:27s
epoch 119| loss: 0.33821 | val_0_rmse: 0.59711 | val_1_rmse: 0.65659 |  0:00:27s
epoch 120| loss: 0.34124 | val_0_rmse: 0.58289 | val_1_rmse: 0.64151 |  0:00:28s
epoch 121| loss: 0.35039 | val_0_rmse: 0.58025 | val_1_rmse: 0.64065 |  0:00:28s
epoch 122| loss: 0.3467  | val_0_rmse: 0.57377 | val_1_rmse: 0.63778 |  0:00:28s
epoch 123| loss: 0.3452  | val_0_rmse: 0.60063 | val_1_rmse: 0.66651 |  0:00:28s
epoch 124| loss: 0.34068 | val_0_rmse: 0.59837 | val_1_rmse: 0.65961 |  0:00:29s
epoch 125| loss: 0.33689 | val_0_rmse: 0.5827  | val_1_rmse: 0.6431  |  0:00:29s
epoch 126| loss: 0.34204 | val_0_rmse: 0.60054 | val_1_rmse: 0.65609 |  0:00:29s
epoch 127| loss: 0.35093 | val_0_rmse: 0.64881 | val_1_rmse: 0.7034  |  0:00:29s
epoch 128| loss: 0.34371 | val_0_rmse: 0.57761 | val_1_rmse: 0.64615 |  0:00:29s
epoch 129| loss: 0.34479 | val_0_rmse: 0.57654 | val_1_rmse: 0.64457 |  0:00:30s
epoch 130| loss: 0.34515 | val_0_rmse: 0.5817  | val_1_rmse: 0.6456  |  0:00:30s
epoch 131| loss: 0.35021 | val_0_rmse: 0.63485 | val_1_rmse: 0.69397 |  0:00:30s
epoch 132| loss: 0.34059 | val_0_rmse: 0.62239 | val_1_rmse: 0.68634 |  0:00:30s
epoch 133| loss: 0.33829 | val_0_rmse: 0.57236 | val_1_rmse: 0.64508 |  0:00:31s
epoch 134| loss: 0.34662 | val_0_rmse: 0.57126 | val_1_rmse: 0.64434 |  0:00:31s
epoch 135| loss: 0.34481 | val_0_rmse: 0.5711  | val_1_rmse: 0.64495 |  0:00:31s
epoch 136| loss: 0.34394 | val_0_rmse: 0.58132 | val_1_rmse: 0.65099 |  0:00:31s
epoch 137| loss: 0.35095 | val_0_rmse: 0.62116 | val_1_rmse: 0.68241 |  0:00:32s
epoch 138| loss: 0.35393 | val_0_rmse: 0.62834 | val_1_rmse: 0.68785 |  0:00:32s
epoch 139| loss: 0.35338 | val_0_rmse: 0.6662  | val_1_rmse: 0.71595 |  0:00:32s
epoch 140| loss: 0.35878 | val_0_rmse: 0.67841 | val_1_rmse: 0.73103 |  0:00:32s
epoch 141| loss: 0.3471  | val_0_rmse: 0.67788 | val_1_rmse: 0.73781 |  0:00:32s
epoch 142| loss: 0.35871 | val_0_rmse: 0.61685 | val_1_rmse: 0.69087 |  0:00:33s
epoch 143| loss: 0.34411 | val_0_rmse: 0.61241 | val_1_rmse: 0.68417 |  0:00:33s
epoch 144| loss: 0.34667 | val_0_rmse: 0.6318  | val_1_rmse: 0.68989 |  0:00:33s
epoch 145| loss: 0.34474 | val_0_rmse: 0.61766 | val_1_rmse: 0.67358 |  0:00:33s
epoch 146| loss: 0.33793 | val_0_rmse: 0.6018  | val_1_rmse: 0.66173 |  0:00:34s
epoch 147| loss: 0.34134 | val_0_rmse: 0.59398 | val_1_rmse: 0.6506  |  0:00:34s
epoch 148| loss: 0.34648 | val_0_rmse: 0.59548 | val_1_rmse: 0.6521  |  0:00:34s
epoch 149| loss: 0.35059 | val_0_rmse: 0.6115  | val_1_rmse: 0.67646 |  0:00:34s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.63778
Best weights from best epoch are automatically used!
ended training at: 02:39:29
Feature importance:
[('Area', 0.33903436508769036), ('Baths', 0.09363605421358626), ('Beds', 0.020547569385148796), ('Latitude', 0.2518052095097243), ('Longitude', 0.2701752830587257), ('Month', 0.010301577410881084), ('Year', 0.014499941334243562)]
Mean squared error is of 2610826140.0781446
Mean absolute error:37016.58949560439
MAPE:0.3472270129623298
R2 score:0.6260292610351603
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:39:30
epoch 0  | loss: 1.80402 | val_0_rmse: 2.23624 | val_1_rmse: 2.14606 |  0:00:00s
epoch 1  | loss: 1.03299 | val_0_rmse: 1.00803 | val_1_rmse: 1.01703 |  0:00:00s
epoch 2  | loss: 0.67984 | val_0_rmse: 0.96626 | val_1_rmse: 1.00029 |  0:00:00s
epoch 3  | loss: 0.61752 | val_0_rmse: 0.86399 | val_1_rmse: 0.93067 |  0:00:00s
epoch 4  | loss: 0.58368 | val_0_rmse: 0.80952 | val_1_rmse: 0.89884 |  0:00:01s
epoch 5  | loss: 0.54055 | val_0_rmse: 0.77953 | val_1_rmse: 0.83    |  0:00:01s
epoch 6  | loss: 0.52161 | val_0_rmse: 0.81022 | val_1_rmse: 0.84212 |  0:00:01s
epoch 7  | loss: 0.52692 | val_0_rmse: 0.83122 | val_1_rmse: 0.83772 |  0:00:01s
epoch 8  | loss: 0.50513 | val_0_rmse: 0.79003 | val_1_rmse: 0.82307 |  0:00:02s
epoch 9  | loss: 0.51203 | val_0_rmse: 0.77965 | val_1_rmse: 0.81187 |  0:00:02s
epoch 10 | loss: 0.49284 | val_0_rmse: 0.77935 | val_1_rmse: 0.81373 |  0:00:02s
epoch 11 | loss: 0.49401 | val_0_rmse: 0.75122 | val_1_rmse: 0.77656 |  0:00:02s
epoch 12 | loss: 0.48153 | val_0_rmse: 0.74527 | val_1_rmse: 0.76437 |  0:00:03s
epoch 13 | loss: 0.46569 | val_0_rmse: 0.73632 | val_1_rmse: 0.75459 |  0:00:03s
epoch 14 | loss: 0.46383 | val_0_rmse: 0.71395 | val_1_rmse: 0.73531 |  0:00:03s
epoch 15 | loss: 0.46793 | val_0_rmse: 0.70534 | val_1_rmse: 0.73355 |  0:00:03s
epoch 16 | loss: 0.44436 | val_0_rmse: 0.70151 | val_1_rmse: 0.728   |  0:00:04s
epoch 17 | loss: 0.45163 | val_0_rmse: 0.68417 | val_1_rmse: 0.72021 |  0:00:04s
epoch 18 | loss: 0.46319 | val_0_rmse: 0.67201 | val_1_rmse: 0.71682 |  0:00:04s
epoch 19 | loss: 0.45193 | val_0_rmse: 0.66912 | val_1_rmse: 0.71704 |  0:00:04s
epoch 20 | loss: 0.45611 | val_0_rmse: 0.66837 | val_1_rmse: 0.72412 |  0:00:05s
epoch 21 | loss: 0.44702 | val_0_rmse: 0.66182 | val_1_rmse: 0.71139 |  0:00:05s
epoch 22 | loss: 0.44827 | val_0_rmse: 0.66059 | val_1_rmse: 0.71561 |  0:00:05s
epoch 23 | loss: 0.44734 | val_0_rmse: 0.66765 | val_1_rmse: 0.73159 |  0:00:05s
epoch 24 | loss: 0.43757 | val_0_rmse: 0.67257 | val_1_rmse: 0.73695 |  0:00:06s
epoch 25 | loss: 0.45394 | val_0_rmse: 0.6632  | val_1_rmse: 0.73948 |  0:00:06s
epoch 26 | loss: 0.4533  | val_0_rmse: 0.66655 | val_1_rmse: 0.73883 |  0:00:06s
epoch 27 | loss: 0.4513  | val_0_rmse: 0.66661 | val_1_rmse: 0.73538 |  0:00:06s
epoch 28 | loss: 0.44949 | val_0_rmse: 0.67014 | val_1_rmse: 0.72855 |  0:00:06s
epoch 29 | loss: 0.44927 | val_0_rmse: 0.67037 | val_1_rmse: 0.73119 |  0:00:07s
epoch 30 | loss: 0.45974 | val_0_rmse: 0.66172 | val_1_rmse: 0.71191 |  0:00:07s
epoch 31 | loss: 0.44082 | val_0_rmse: 0.66518 | val_1_rmse: 0.71804 |  0:00:07s
epoch 32 | loss: 0.44867 | val_0_rmse: 0.67422 | val_1_rmse: 0.73096 |  0:00:07s
epoch 33 | loss: 0.46494 | val_0_rmse: 0.68779 | val_1_rmse: 0.7483  |  0:00:08s
epoch 34 | loss: 0.44263 | val_0_rmse: 0.68954 | val_1_rmse: 0.75267 |  0:00:08s
epoch 35 | loss: 0.44487 | val_0_rmse: 0.67604 | val_1_rmse: 0.73439 |  0:00:08s
epoch 36 | loss: 0.43352 | val_0_rmse: 0.65408 | val_1_rmse: 0.7223  |  0:00:08s
epoch 37 | loss: 0.44905 | val_0_rmse: 0.64922 | val_1_rmse: 0.72216 |  0:00:09s
epoch 38 | loss: 0.43223 | val_0_rmse: 0.65338 | val_1_rmse: 0.7251  |  0:00:09s
epoch 39 | loss: 0.44301 | val_0_rmse: 0.65984 | val_1_rmse: 0.72723 |  0:00:09s
epoch 40 | loss: 0.43223 | val_0_rmse: 0.65663 | val_1_rmse: 0.72067 |  0:00:09s
epoch 41 | loss: 0.43482 | val_0_rmse: 0.65418 | val_1_rmse: 0.70822 |  0:00:10s
epoch 42 | loss: 0.43507 | val_0_rmse: 0.64711 | val_1_rmse: 0.69966 |  0:00:10s
epoch 43 | loss: 0.44089 | val_0_rmse: 0.64852 | val_1_rmse: 0.69827 |  0:00:10s
epoch 44 | loss: 0.43022 | val_0_rmse: 0.64631 | val_1_rmse: 0.69714 |  0:00:10s
epoch 45 | loss: 0.43045 | val_0_rmse: 0.6484  | val_1_rmse: 0.70102 |  0:00:10s
epoch 46 | loss: 0.42338 | val_0_rmse: 0.64941 | val_1_rmse: 0.70197 |  0:00:11s
epoch 47 | loss: 0.42957 | val_0_rmse: 0.65482 | val_1_rmse: 0.70838 |  0:00:11s
epoch 48 | loss: 0.41834 | val_0_rmse: 0.66255 | val_1_rmse: 0.71796 |  0:00:11s
epoch 49 | loss: 0.42194 | val_0_rmse: 0.6447  | val_1_rmse: 0.70208 |  0:00:11s
epoch 50 | loss: 0.42354 | val_0_rmse: 0.64098 | val_1_rmse: 0.70407 |  0:00:12s
epoch 51 | loss: 0.44099 | val_0_rmse: 0.64024 | val_1_rmse: 0.71033 |  0:00:12s
epoch 52 | loss: 0.41678 | val_0_rmse: 0.64403 | val_1_rmse: 0.71331 |  0:00:12s
epoch 53 | loss: 0.42693 | val_0_rmse: 0.64229 | val_1_rmse: 0.72212 |  0:00:12s
epoch 54 | loss: 0.42206 | val_0_rmse: 0.64124 | val_1_rmse: 0.72661 |  0:00:13s
epoch 55 | loss: 0.41946 | val_0_rmse: 0.6426  | val_1_rmse: 0.72367 |  0:00:13s
epoch 56 | loss: 0.41245 | val_0_rmse: 0.64354 | val_1_rmse: 0.71416 |  0:00:13s
epoch 57 | loss: 0.44565 | val_0_rmse: 0.63879 | val_1_rmse: 0.69671 |  0:00:13s
epoch 58 | loss: 0.41699 | val_0_rmse: 0.6472  | val_1_rmse: 0.70677 |  0:00:13s
epoch 59 | loss: 0.42075 | val_0_rmse: 0.67276 | val_1_rmse: 0.7273  |  0:00:14s
epoch 60 | loss: 0.43095 | val_0_rmse: 0.6583  | val_1_rmse: 0.71008 |  0:00:14s
epoch 61 | loss: 0.42981 | val_0_rmse: 0.6321  | val_1_rmse: 0.68681 |  0:00:14s
epoch 62 | loss: 0.428   | val_0_rmse: 0.64212 | val_1_rmse: 0.70098 |  0:00:14s
epoch 63 | loss: 0.42944 | val_0_rmse: 0.66825 | val_1_rmse: 0.72676 |  0:00:15s
epoch 64 | loss: 0.42346 | val_0_rmse: 0.6508  | val_1_rmse: 0.70513 |  0:00:15s
epoch 65 | loss: 0.41745 | val_0_rmse: 0.64963 | val_1_rmse: 0.70408 |  0:00:15s
epoch 66 | loss: 0.41348 | val_0_rmse: 0.64096 | val_1_rmse: 0.69732 |  0:00:15s
epoch 67 | loss: 0.41229 | val_0_rmse: 0.62784 | val_1_rmse: 0.70077 |  0:00:16s
epoch 68 | loss: 0.40731 | val_0_rmse: 0.63311 | val_1_rmse: 0.70487 |  0:00:16s
epoch 69 | loss: 0.41502 | val_0_rmse: 0.64135 | val_1_rmse: 0.7091  |  0:00:16s
epoch 70 | loss: 0.41321 | val_0_rmse: 0.63524 | val_1_rmse: 0.70544 |  0:00:16s
epoch 71 | loss: 0.40701 | val_0_rmse: 0.63077 | val_1_rmse: 0.68787 |  0:00:17s
epoch 72 | loss: 0.40677 | val_0_rmse: 0.62957 | val_1_rmse: 0.68062 |  0:00:17s
epoch 73 | loss: 0.40377 | val_0_rmse: 0.63052 | val_1_rmse: 0.68241 |  0:00:17s
epoch 74 | loss: 0.40915 | val_0_rmse: 0.62268 | val_1_rmse: 0.67922 |  0:00:17s
epoch 75 | loss: 0.40724 | val_0_rmse: 0.62104 | val_1_rmse: 0.68306 |  0:00:17s
epoch 76 | loss: 0.38895 | val_0_rmse: 0.6195  | val_1_rmse: 0.69656 |  0:00:18s
epoch 77 | loss: 0.39708 | val_0_rmse: 0.62291 | val_1_rmse: 0.70129 |  0:00:18s
epoch 78 | loss: 0.41996 | val_0_rmse: 0.6301  | val_1_rmse: 0.68176 |  0:00:18s
epoch 79 | loss: 0.40053 | val_0_rmse: 0.64471 | val_1_rmse: 0.67921 |  0:00:18s
epoch 80 | loss: 0.41777 | val_0_rmse: 0.63606 | val_1_rmse: 0.69259 |  0:00:19s
epoch 81 | loss: 0.40432 | val_0_rmse: 0.62632 | val_1_rmse: 0.69297 |  0:00:19s
epoch 82 | loss: 0.40635 | val_0_rmse: 0.62354 | val_1_rmse: 0.68804 |  0:00:19s
epoch 83 | loss: 0.39757 | val_0_rmse: 0.63879 | val_1_rmse: 0.69926 |  0:00:19s
epoch 84 | loss: 0.39663 | val_0_rmse: 0.64164 | val_1_rmse: 0.70628 |  0:00:20s
epoch 85 | loss: 0.39332 | val_0_rmse: 0.61656 | val_1_rmse: 0.68226 |  0:00:20s
epoch 86 | loss: 0.38436 | val_0_rmse: 0.62026 | val_1_rmse: 0.69087 |  0:00:20s
epoch 87 | loss: 0.39809 | val_0_rmse: 0.67159 | val_1_rmse: 0.74395 |  0:00:20s
epoch 88 | loss: 0.40676 | val_0_rmse: 0.64755 | val_1_rmse: 0.7194  |  0:00:21s
epoch 89 | loss: 0.39422 | val_0_rmse: 0.63219 | val_1_rmse: 0.70456 |  0:00:21s
epoch 90 | loss: 0.39623 | val_0_rmse: 0.6219  | val_1_rmse: 0.69564 |  0:00:21s
epoch 91 | loss: 0.39435 | val_0_rmse: 0.61771 | val_1_rmse: 0.69361 |  0:00:21s
epoch 92 | loss: 0.37457 | val_0_rmse: 0.62069 | val_1_rmse: 0.70431 |  0:00:22s
epoch 93 | loss: 0.36265 | val_0_rmse: 0.62444 | val_1_rmse: 0.71138 |  0:00:22s
epoch 94 | loss: 0.38045 | val_0_rmse: 0.63241 | val_1_rmse: 0.71927 |  0:00:22s
epoch 95 | loss: 0.39945 | val_0_rmse: 0.63561 | val_1_rmse: 0.72043 |  0:00:22s
epoch 96 | loss: 0.38833 | val_0_rmse: 0.63752 | val_1_rmse: 0.7096  |  0:00:23s
epoch 97 | loss: 0.39241 | val_0_rmse: 0.62727 | val_1_rmse: 0.68415 |  0:00:23s
epoch 98 | loss: 0.3936  | val_0_rmse: 0.62901 | val_1_rmse: 0.68396 |  0:00:23s
epoch 99 | loss: 0.38523 | val_0_rmse: 0.67582 | val_1_rmse: 0.7481  |  0:00:23s
epoch 100| loss: 0.3883  | val_0_rmse: 0.64604 | val_1_rmse: 0.7227  |  0:00:23s
epoch 101| loss: 0.39145 | val_0_rmse: 0.62793 | val_1_rmse: 0.68151 |  0:00:24s
epoch 102| loss: 0.37615 | val_0_rmse: 0.64365 | val_1_rmse: 0.69606 |  0:00:24s
epoch 103| loss: 0.37503 | val_0_rmse: 0.61052 | val_1_rmse: 0.64446 |  0:00:24s
epoch 104| loss: 0.38964 | val_0_rmse: 0.59389 | val_1_rmse: 0.63328 |  0:00:24s
epoch 105| loss: 0.35794 | val_0_rmse: 0.60227 | val_1_rmse: 0.67461 |  0:00:25s
epoch 106| loss: 0.36906 | val_0_rmse: 0.60399 | val_1_rmse: 0.66453 |  0:00:25s
epoch 107| loss: 0.36927 | val_0_rmse: 0.60079 | val_1_rmse: 0.66964 |  0:00:25s
epoch 108| loss: 0.36557 | val_0_rmse: 0.62186 | val_1_rmse: 0.70493 |  0:00:25s
epoch 109| loss: 0.36215 | val_0_rmse: 0.5967  | val_1_rmse: 0.66406 |  0:00:26s
epoch 110| loss: 0.36576 | val_0_rmse: 0.64298 | val_1_rmse: 0.71658 |  0:00:26s
epoch 111| loss: 0.37323 | val_0_rmse: 0.7156  | val_1_rmse: 0.79895 |  0:00:26s
epoch 112| loss: 0.39832 | val_0_rmse: 0.60517 | val_1_rmse: 0.67814 |  0:00:26s
epoch 113| loss: 0.3635  | val_0_rmse: 0.61837 | val_1_rmse: 0.68812 |  0:00:27s
epoch 114| loss: 0.38049 | val_0_rmse: 0.6155  | val_1_rmse: 0.68892 |  0:00:27s
epoch 115| loss: 0.38208 | val_0_rmse: 0.58713 | val_1_rmse: 0.66144 |  0:00:27s
epoch 116| loss: 0.35946 | val_0_rmse: 0.6268  | val_1_rmse: 0.6853  |  0:00:27s
epoch 117| loss: 0.37184 | val_0_rmse: 0.6013  | val_1_rmse: 0.64554 |  0:00:27s
epoch 118| loss: 0.35518 | val_0_rmse: 0.62804 | val_1_rmse: 0.65722 |  0:00:28s
epoch 119| loss: 0.36474 | val_0_rmse: 0.73287 | val_1_rmse: 0.74843 |  0:00:28s
epoch 120| loss: 0.3623  | val_0_rmse: 0.74047 | val_1_rmse: 0.74867 |  0:00:28s
epoch 121| loss: 0.36764 | val_0_rmse: 0.65489 | val_1_rmse: 0.66258 |  0:00:28s
epoch 122| loss: 0.35793 | val_0_rmse: 0.63953 | val_1_rmse: 0.66076 |  0:00:29s
epoch 123| loss: 0.34741 | val_0_rmse: 0.60707 | val_1_rmse: 0.65523 |  0:00:29s
epoch 124| loss: 0.37017 | val_0_rmse: 0.58653 | val_1_rmse: 0.64774 |  0:00:29s
epoch 125| loss: 0.35942 | val_0_rmse: 0.58597 | val_1_rmse: 0.64612 |  0:00:29s
epoch 126| loss: 0.36017 | val_0_rmse: 0.57669 | val_1_rmse: 0.62053 |  0:00:30s
epoch 127| loss: 0.34665 | val_0_rmse: 0.57926 | val_1_rmse: 0.61904 |  0:00:30s
epoch 128| loss: 0.35589 | val_0_rmse: 0.57547 | val_1_rmse: 0.62593 |  0:00:30s
epoch 129| loss: 0.35758 | val_0_rmse: 0.57235 | val_1_rmse: 0.62756 |  0:00:30s
epoch 130| loss: 0.35826 | val_0_rmse: 0.58407 | val_1_rmse: 0.64796 |  0:00:31s
epoch 131| loss: 0.36053 | val_0_rmse: 0.58577 | val_1_rmse: 0.64908 |  0:00:31s
epoch 132| loss: 0.34687 | val_0_rmse: 0.61134 | val_1_rmse: 0.66365 |  0:00:31s
epoch 133| loss: 0.36715 | val_0_rmse: 0.60622 | val_1_rmse: 0.6536  |  0:00:31s
epoch 134| loss: 0.35614 | val_0_rmse: 0.58261 | val_1_rmse: 0.64315 |  0:00:32s
epoch 135| loss: 0.38115 | val_0_rmse: 0.64177 | val_1_rmse: 0.71027 |  0:00:32s
epoch 136| loss: 0.37519 | val_0_rmse: 0.6355  | val_1_rmse: 0.70935 |  0:00:32s
epoch 137| loss: 0.3739  | val_0_rmse: 0.57658 | val_1_rmse: 0.62952 |  0:00:32s
epoch 138| loss: 0.35435 | val_0_rmse: 0.58171 | val_1_rmse: 0.6212  |  0:00:33s
epoch 139| loss: 0.35814 | val_0_rmse: 0.58226 | val_1_rmse: 0.62272 |  0:00:33s
epoch 140| loss: 0.35425 | val_0_rmse: 0.58882 | val_1_rmse: 0.63485 |  0:00:33s
epoch 141| loss: 0.36069 | val_0_rmse: 0.62475 | val_1_rmse: 0.66841 |  0:00:33s
epoch 142| loss: 0.36583 | val_0_rmse: 0.64264 | val_1_rmse: 0.65931 |  0:00:34s
epoch 143| loss: 0.35485 | val_0_rmse: 0.70818 | val_1_rmse: 0.72481 |  0:00:34s
epoch 144| loss: 0.3523  | val_0_rmse: 0.70982 | val_1_rmse: 0.7332  |  0:00:34s
epoch 145| loss: 0.37033 | val_0_rmse: 0.65656 | val_1_rmse: 0.67609 |  0:00:34s
epoch 146| loss: 0.34008 | val_0_rmse: 0.65509 | val_1_rmse: 0.6878  |  0:00:34s
epoch 147| loss: 0.3701  | val_0_rmse: 0.59657 | val_1_rmse: 0.66112 |  0:00:35s
epoch 148| loss: 0.35189 | val_0_rmse: 0.60762 | val_1_rmse: 0.6692  |  0:00:35s
epoch 149| loss: 0.35135 | val_0_rmse: 0.67918 | val_1_rmse: 0.72365 |  0:00:35s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.61904
Best weights from best epoch are automatically used!
ended training at: 02:40:06
Feature importance:
[('Area', 0.21985279940192892), ('Baths', 0.24478579127275327), ('Beds', 0.016549501380180423), ('Latitude', 0.26248556596710454), ('Longitude', 0.2484845481053758), ('Month', 0.0034027789144903038), ('Year', 0.004439014958166744)]
Mean squared error is of 3218480742.8604674
Mean absolute error:41580.98365274725
MAPE:0.39851105262910097
R2 score:0.5470867748036099
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:40:06
epoch 0  | loss: 1.80961 | val_0_rmse: 1.11848 | val_1_rmse: 1.04727 |  0:00:00s
epoch 1  | loss: 0.8385  | val_0_rmse: 0.94317 | val_1_rmse: 0.91253 |  0:00:00s
epoch 2  | loss: 0.70147 | val_0_rmse: 0.88774 | val_1_rmse: 0.9071  |  0:00:00s
epoch 3  | loss: 0.60833 | val_0_rmse: 0.92999 | val_1_rmse: 0.99808 |  0:00:00s
epoch 4  | loss: 0.59313 | val_0_rmse: 0.84877 | val_1_rmse: 0.93608 |  0:00:01s
epoch 5  | loss: 0.58738 | val_0_rmse: 0.79742 | val_1_rmse: 0.87683 |  0:00:01s
epoch 6  | loss: 0.57452 | val_0_rmse: 0.80091 | val_1_rmse: 0.86609 |  0:00:01s
epoch 7  | loss: 0.5723  | val_0_rmse: 0.787   | val_1_rmse: 0.92975 |  0:00:01s
epoch 8  | loss: 0.5543  | val_0_rmse: 0.7702  | val_1_rmse: 0.85878 |  0:00:02s
epoch 9  | loss: 0.53229 | val_0_rmse: 0.77788 | val_1_rmse: 0.83464 |  0:00:02s
epoch 10 | loss: 0.51603 | val_0_rmse: 0.75111 | val_1_rmse: 0.84309 |  0:00:02s
epoch 11 | loss: 0.5308  | val_0_rmse: 0.72306 | val_1_rmse: 0.816   |  0:00:02s
epoch 12 | loss: 0.52629 | val_0_rmse: 0.71747 | val_1_rmse: 0.78779 |  0:00:03s
epoch 13 | loss: 0.50547 | val_0_rmse: 0.72435 | val_1_rmse: 0.77724 |  0:00:03s
epoch 14 | loss: 0.50457 | val_0_rmse: 0.70839 | val_1_rmse: 0.77072 |  0:00:03s
epoch 15 | loss: 0.50254 | val_0_rmse: 0.70169 | val_1_rmse: 0.78005 |  0:00:03s
epoch 16 | loss: 0.50093 | val_0_rmse: 0.69221 | val_1_rmse: 0.74204 |  0:00:04s
epoch 17 | loss: 0.49176 | val_0_rmse: 0.71338 | val_1_rmse: 0.74757 |  0:00:04s
epoch 18 | loss: 0.50124 | val_0_rmse: 0.69305 | val_1_rmse: 0.73319 |  0:00:04s
epoch 19 | loss: 0.47141 | val_0_rmse: 0.68341 | val_1_rmse: 0.74138 |  0:00:04s
epoch 20 | loss: 0.48807 | val_0_rmse: 0.68055 | val_1_rmse: 0.74046 |  0:00:05s
epoch 21 | loss: 0.46882 | val_0_rmse: 0.68129 | val_1_rmse: 0.72996 |  0:00:05s
epoch 22 | loss: 0.47311 | val_0_rmse: 0.68041 | val_1_rmse: 0.72473 |  0:00:05s
epoch 23 | loss: 0.47621 | val_0_rmse: 0.67548 | val_1_rmse: 0.7197  |  0:00:05s
epoch 24 | loss: 0.47136 | val_0_rmse: 0.66805 | val_1_rmse: 0.71969 |  0:00:06s
epoch 25 | loss: 0.46108 | val_0_rmse: 0.66625 | val_1_rmse: 0.72297 |  0:00:06s
epoch 26 | loss: 0.4603  | val_0_rmse: 0.66766 | val_1_rmse: 0.72441 |  0:00:06s
epoch 27 | loss: 0.45419 | val_0_rmse: 0.66657 | val_1_rmse: 0.72265 |  0:00:06s
epoch 28 | loss: 0.45768 | val_0_rmse: 0.66834 | val_1_rmse: 0.72023 |  0:00:07s
epoch 29 | loss: 0.45962 | val_0_rmse: 0.66639 | val_1_rmse: 0.72379 |  0:00:07s
epoch 30 | loss: 0.4428  | val_0_rmse: 0.66158 | val_1_rmse: 0.73144 |  0:00:07s
epoch 31 | loss: 0.44046 | val_0_rmse: 0.65701 | val_1_rmse: 0.73132 |  0:00:07s
epoch 32 | loss: 0.44353 | val_0_rmse: 0.6614  | val_1_rmse: 0.73009 |  0:00:08s
epoch 33 | loss: 0.44569 | val_0_rmse: 0.67752 | val_1_rmse: 0.73272 |  0:00:08s
epoch 34 | loss: 0.45784 | val_0_rmse: 0.66449 | val_1_rmse: 0.73088 |  0:00:08s
epoch 35 | loss: 0.45197 | val_0_rmse: 0.65994 | val_1_rmse: 0.72955 |  0:00:08s
epoch 36 | loss: 0.44602 | val_0_rmse: 0.6753  | val_1_rmse: 0.75601 |  0:00:08s
epoch 37 | loss: 0.45978 | val_0_rmse: 0.66788 | val_1_rmse: 0.739   |  0:00:09s
epoch 38 | loss: 0.44191 | val_0_rmse: 0.67776 | val_1_rmse: 0.73312 |  0:00:09s
epoch 39 | loss: 0.45925 | val_0_rmse: 0.67307 | val_1_rmse: 0.72985 |  0:00:09s
epoch 40 | loss: 0.46223 | val_0_rmse: 0.68365 | val_1_rmse: 0.75504 |  0:00:09s
epoch 41 | loss: 0.4671  | val_0_rmse: 0.69504 | val_1_rmse: 0.7649  |  0:00:10s
epoch 42 | loss: 0.46126 | val_0_rmse: 0.6833  | val_1_rmse: 0.75315 |  0:00:10s
epoch 43 | loss: 0.4537  | val_0_rmse: 0.67198 | val_1_rmse: 0.7426  |  0:00:10s
epoch 44 | loss: 0.45356 | val_0_rmse: 0.66926 | val_1_rmse: 0.73203 |  0:00:10s
epoch 45 | loss: 0.47082 | val_0_rmse: 0.65974 | val_1_rmse: 0.71231 |  0:00:11s
epoch 46 | loss: 0.44993 | val_0_rmse: 0.65676 | val_1_rmse: 0.71431 |  0:00:11s
epoch 47 | loss: 0.44524 | val_0_rmse: 0.66247 | val_1_rmse: 0.72225 |  0:00:11s
epoch 48 | loss: 0.45113 | val_0_rmse: 0.65725 | val_1_rmse: 0.72138 |  0:00:11s
epoch 49 | loss: 0.43694 | val_0_rmse: 0.65327 | val_1_rmse: 0.71863 |  0:00:11s
epoch 50 | loss: 0.4383  | val_0_rmse: 0.65168 | val_1_rmse: 0.72353 |  0:00:12s
epoch 51 | loss: 0.44061 | val_0_rmse: 0.65095 | val_1_rmse: 0.72722 |  0:00:12s
epoch 52 | loss: 0.42934 | val_0_rmse: 0.65303 | val_1_rmse: 0.72342 |  0:00:12s
epoch 53 | loss: 0.43591 | val_0_rmse: 0.6509  | val_1_rmse: 0.72325 |  0:00:12s
epoch 54 | loss: 0.43214 | val_0_rmse: 0.65351 | val_1_rmse: 0.71849 |  0:00:13s
epoch 55 | loss: 0.43156 | val_0_rmse: 0.64548 | val_1_rmse: 0.7161  |  0:00:13s
epoch 56 | loss: 0.42289 | val_0_rmse: 0.65538 | val_1_rmse: 0.73011 |  0:00:13s
epoch 57 | loss: 0.41405 | val_0_rmse: 0.67229 | val_1_rmse: 0.74813 |  0:00:13s
epoch 58 | loss: 0.43556 | val_0_rmse: 0.67507 | val_1_rmse: 0.74688 |  0:00:14s
epoch 59 | loss: 0.42123 | val_0_rmse: 0.64363 | val_1_rmse: 0.72189 |  0:00:14s
epoch 60 | loss: 0.40323 | val_0_rmse: 0.63311 | val_1_rmse: 0.72843 |  0:00:14s
epoch 61 | loss: 0.42586 | val_0_rmse: 0.64108 | val_1_rmse: 0.7295  |  0:00:14s
epoch 62 | loss: 0.4169  | val_0_rmse: 0.657   | val_1_rmse: 0.73391 |  0:00:15s
epoch 63 | loss: 0.44961 | val_0_rmse: 0.64778 | val_1_rmse: 0.71642 |  0:00:15s
epoch 64 | loss: 0.41296 | val_0_rmse: 0.6343  | val_1_rmse: 0.70365 |  0:00:15s
epoch 65 | loss: 0.41917 | val_0_rmse: 0.65557 | val_1_rmse: 0.71676 |  0:00:15s
epoch 66 | loss: 0.43528 | val_0_rmse: 0.66404 | val_1_rmse: 0.71518 |  0:00:16s
epoch 67 | loss: 0.44046 | val_0_rmse: 0.64644 | val_1_rmse: 0.69874 |  0:00:16s
epoch 68 | loss: 0.41617 | val_0_rmse: 0.63585 | val_1_rmse: 0.69163 |  0:00:16s
epoch 69 | loss: 0.40821 | val_0_rmse: 0.63779 | val_1_rmse: 0.70272 |  0:00:16s
epoch 70 | loss: 0.40272 | val_0_rmse: 0.6303  | val_1_rmse: 0.7164  |  0:00:17s
epoch 71 | loss: 0.40604 | val_0_rmse: 0.62518 | val_1_rmse: 0.72238 |  0:00:17s
epoch 72 | loss: 0.39028 | val_0_rmse: 0.61367 | val_1_rmse: 0.69188 |  0:00:17s
epoch 73 | loss: 0.40495 | val_0_rmse: 0.61517 | val_1_rmse: 0.68724 |  0:00:17s
epoch 74 | loss: 0.39915 | val_0_rmse: 0.691   | val_1_rmse: 0.752   |  0:00:18s
epoch 75 | loss: 0.4078  | val_0_rmse: 0.72315 | val_1_rmse: 0.781   |  0:00:18s
epoch 76 | loss: 0.40267 | val_0_rmse: 0.70653 | val_1_rmse: 0.77332 |  0:00:18s
epoch 77 | loss: 0.396   | val_0_rmse: 0.66521 | val_1_rmse: 0.73922 |  0:00:18s
epoch 78 | loss: 0.39612 | val_0_rmse: 0.64335 | val_1_rmse: 0.71222 |  0:00:18s
epoch 79 | loss: 0.39753 | val_0_rmse: 0.64847 | val_1_rmse: 0.7071  |  0:00:19s
epoch 80 | loss: 0.39259 | val_0_rmse: 0.63862 | val_1_rmse: 0.70385 |  0:00:19s
epoch 81 | loss: 0.39888 | val_0_rmse: 0.64176 | val_1_rmse: 0.71803 |  0:00:19s
epoch 82 | loss: 0.39508 | val_0_rmse: 0.71289 | val_1_rmse: 0.77945 |  0:00:19s
epoch 83 | loss: 0.40803 | val_0_rmse: 0.65445 | val_1_rmse: 0.75409 |  0:00:20s
epoch 84 | loss: 0.39082 | val_0_rmse: 0.64819 | val_1_rmse: 0.75152 |  0:00:20s
epoch 85 | loss: 0.41735 | val_0_rmse: 0.67059 | val_1_rmse: 0.76474 |  0:00:20s
epoch 86 | loss: 0.43572 | val_0_rmse: 0.65367 | val_1_rmse: 0.72827 |  0:00:20s
epoch 87 | loss: 0.41436 | val_0_rmse: 0.64332 | val_1_rmse: 0.70743 |  0:00:21s
epoch 88 | loss: 0.41819 | val_0_rmse: 0.69783 | val_1_rmse: 0.7506  |  0:00:21s
epoch 89 | loss: 0.42459 | val_0_rmse: 0.66034 | val_1_rmse: 0.73457 |  0:00:21s
epoch 90 | loss: 0.40772 | val_0_rmse: 0.64406 | val_1_rmse: 0.71729 |  0:00:21s
epoch 91 | loss: 0.43185 | val_0_rmse: 0.63862 | val_1_rmse: 0.69344 |  0:00:22s
epoch 92 | loss: 0.41707 | val_0_rmse: 0.65438 | val_1_rmse: 0.71813 |  0:00:22s
epoch 93 | loss: 0.403   | val_0_rmse: 0.6383  | val_1_rmse: 0.72388 |  0:00:22s
epoch 94 | loss: 0.40791 | val_0_rmse: 0.65215 | val_1_rmse: 0.73699 |  0:00:22s
epoch 95 | loss: 0.40201 | val_0_rmse: 0.62981 | val_1_rmse: 0.71108 |  0:00:22s
epoch 96 | loss: 0.39627 | val_0_rmse: 0.62412 | val_1_rmse: 0.71005 |  0:00:23s
epoch 97 | loss: 0.39159 | val_0_rmse: 0.62147 | val_1_rmse: 0.71838 |  0:00:23s
epoch 98 | loss: 0.38877 | val_0_rmse: 0.61477 | val_1_rmse: 0.70084 |  0:00:23s
epoch 99 | loss: 0.3944  | val_0_rmse: 0.65806 | val_1_rmse: 0.73616 |  0:00:23s
epoch 100| loss: 0.39934 | val_0_rmse: 0.69582 | val_1_rmse: 0.76966 |  0:00:24s
epoch 101| loss: 0.39054 | val_0_rmse: 0.6328  | val_1_rmse: 0.71159 |  0:00:24s
epoch 102| loss: 0.38889 | val_0_rmse: 0.61852 | val_1_rmse: 0.71119 |  0:00:24s
epoch 103| loss: 0.3933  | val_0_rmse: 0.63913 | val_1_rmse: 0.72348 |  0:00:24s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.68724
Best weights from best epoch are automatically used!
ended training at: 02:40:31
Feature importance:
[('Area', 0.3655818115532762), ('Baths', 0.13191347986775512), ('Beds', 0.058142980911651966), ('Latitude', 0.27669342758393317), ('Longitude', 0.13041646985393798), ('Month', 0.02955468885094565), ('Year', 0.007697141378499943)]
Mean squared error is of 3070228803.1418133
Mean absolute error:40276.765420810436
MAPE:0.373745193935663
R2 score:0.5589144194094451
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:40:31
epoch 0  | loss: 1.77482 | val_0_rmse: 1.6924  | val_1_rmse: 1.92585 |  0:00:00s
epoch 1  | loss: 0.72954 | val_0_rmse: 1.22519 | val_1_rmse: 1.23362 |  0:00:00s
epoch 2  | loss: 0.64278 | val_0_rmse: 0.95929 | val_1_rmse: 0.98372 |  0:00:00s
epoch 3  | loss: 0.58442 | val_0_rmse: 0.99754 | val_1_rmse: 1.01212 |  0:00:00s
epoch 4  | loss: 0.58599 | val_0_rmse: 0.8849  | val_1_rmse: 0.89885 |  0:00:01s
epoch 5  | loss: 0.57026 | val_0_rmse: 0.80698 | val_1_rmse: 0.83599 |  0:00:01s
epoch 6  | loss: 0.53752 | val_0_rmse: 0.78746 | val_1_rmse: 0.8415  |  0:00:01s
epoch 7  | loss: 0.52501 | val_0_rmse: 0.75397 | val_1_rmse: 0.79437 |  0:00:01s
epoch 8  | loss: 0.50481 | val_0_rmse: 0.76697 | val_1_rmse: 0.78632 |  0:00:02s
epoch 9  | loss: 0.50072 | val_0_rmse: 0.76323 | val_1_rmse: 0.79194 |  0:00:02s
epoch 10 | loss: 0.50146 | val_0_rmse: 0.73125 | val_1_rmse: 0.75788 |  0:00:02s
epoch 11 | loss: 0.50329 | val_0_rmse: 0.72704 | val_1_rmse: 0.75849 |  0:00:02s
epoch 12 | loss: 0.48713 | val_0_rmse: 0.72483 | val_1_rmse: 0.76263 |  0:00:03s
epoch 13 | loss: 0.48919 | val_0_rmse: 0.70101 | val_1_rmse: 0.72316 |  0:00:03s
epoch 14 | loss: 0.48249 | val_0_rmse: 0.6979  | val_1_rmse: 0.71645 |  0:00:03s
epoch 15 | loss: 0.48421 | val_0_rmse: 0.69662 | val_1_rmse: 0.72538 |  0:00:03s
epoch 16 | loss: 0.48954 | val_0_rmse: 0.69663 | val_1_rmse: 0.72067 |  0:00:04s
epoch 17 | loss: 0.47823 | val_0_rmse: 0.69568 | val_1_rmse: 0.70815 |  0:00:04s
epoch 18 | loss: 0.48978 | val_0_rmse: 0.6862  | val_1_rmse: 0.70704 |  0:00:04s
epoch 19 | loss: 0.4743  | val_0_rmse: 0.6911  | val_1_rmse: 0.72294 |  0:00:04s
epoch 20 | loss: 0.4872  | val_0_rmse: 0.68199 | val_1_rmse: 0.70695 |  0:00:05s
epoch 21 | loss: 0.4777  | val_0_rmse: 0.68298 | val_1_rmse: 0.70054 |  0:00:05s
epoch 22 | loss: 0.47899 | val_0_rmse: 0.67804 | val_1_rmse: 0.71476 |  0:00:05s
epoch 23 | loss: 0.46785 | val_0_rmse: 0.68603 | val_1_rmse: 0.72123 |  0:00:05s
epoch 24 | loss: 0.47206 | val_0_rmse: 0.69754 | val_1_rmse: 0.72062 |  0:00:06s
epoch 25 | loss: 0.47654 | val_0_rmse: 0.68612 | val_1_rmse: 0.71264 |  0:00:06s
epoch 26 | loss: 0.4733  | val_0_rmse: 0.67976 | val_1_rmse: 0.7161  |  0:00:06s
epoch 27 | loss: 0.45752 | val_0_rmse: 0.67548 | val_1_rmse: 0.7065  |  0:00:06s
epoch 28 | loss: 0.44944 | val_0_rmse: 0.66885 | val_1_rmse: 0.70777 |  0:00:07s
epoch 29 | loss: 0.45068 | val_0_rmse: 0.66593 | val_1_rmse: 0.70936 |  0:00:07s
epoch 30 | loss: 0.45511 | val_0_rmse: 0.67224 | val_1_rmse: 0.71421 |  0:00:07s
epoch 31 | loss: 0.45461 | val_0_rmse: 0.67048 | val_1_rmse: 0.72046 |  0:00:07s
epoch 32 | loss: 0.45657 | val_0_rmse: 0.66296 | val_1_rmse: 0.71465 |  0:00:07s
epoch 33 | loss: 0.4521  | val_0_rmse: 0.66508 | val_1_rmse: 0.69881 |  0:00:08s
epoch 34 | loss: 0.45085 | val_0_rmse: 0.66586 | val_1_rmse: 0.69965 |  0:00:08s
epoch 35 | loss: 0.43813 | val_0_rmse: 0.6655  | val_1_rmse: 0.71769 |  0:00:08s
epoch 36 | loss: 0.46592 | val_0_rmse: 0.68251 | val_1_rmse: 0.71111 |  0:00:08s
epoch 37 | loss: 0.47904 | val_0_rmse: 0.68352 | val_1_rmse: 0.728   |  0:00:09s
epoch 38 | loss: 0.4607  | val_0_rmse: 0.67919 | val_1_rmse: 0.75359 |  0:00:09s
epoch 39 | loss: 0.46421 | val_0_rmse: 0.66841 | val_1_rmse: 0.71407 |  0:00:09s
epoch 40 | loss: 0.45542 | val_0_rmse: 0.68478 | val_1_rmse: 0.70107 |  0:00:09s
epoch 41 | loss: 0.47936 | val_0_rmse: 0.68421 | val_1_rmse: 0.72178 |  0:00:10s
epoch 42 | loss: 0.47721 | val_0_rmse: 0.69026 | val_1_rmse: 0.71838 |  0:00:10s
epoch 43 | loss: 0.47045 | val_0_rmse: 0.68908 | val_1_rmse: 0.70626 |  0:00:10s
epoch 44 | loss: 0.47339 | val_0_rmse: 0.67871 | val_1_rmse: 0.72114 |  0:00:10s
epoch 45 | loss: 0.45874 | val_0_rmse: 0.6795  | val_1_rmse: 0.74342 |  0:00:11s
epoch 46 | loss: 0.46563 | val_0_rmse: 0.68234 | val_1_rmse: 0.7413  |  0:00:11s
epoch 47 | loss: 0.45847 | val_0_rmse: 0.68481 | val_1_rmse: 0.72813 |  0:00:11s
epoch 48 | loss: 0.4598  | val_0_rmse: 0.70821 | val_1_rmse: 0.74113 |  0:00:11s
epoch 49 | loss: 0.46571 | val_0_rmse: 0.70142 | val_1_rmse: 0.73645 |  0:00:12s
epoch 50 | loss: 0.46701 | val_0_rmse: 0.68277 | val_1_rmse: 0.73446 |  0:00:12s
epoch 51 | loss: 0.45714 | val_0_rmse: 0.67641 | val_1_rmse: 0.72139 |  0:00:12s
epoch 52 | loss: 0.4584  | val_0_rmse: 0.6831  | val_1_rmse: 0.73063 |  0:00:12s
epoch 53 | loss: 0.45217 | val_0_rmse: 0.68241 | val_1_rmse: 0.72536 |  0:00:12s
epoch 54 | loss: 0.46375 | val_0_rmse: 0.67677 | val_1_rmse: 0.70779 |  0:00:13s
epoch 55 | loss: 0.45939 | val_0_rmse: 0.67355 | val_1_rmse: 0.71216 |  0:00:13s
epoch 56 | loss: 0.45345 | val_0_rmse: 0.67042 | val_1_rmse: 0.71765 |  0:00:13s
epoch 57 | loss: 0.45066 | val_0_rmse: 0.67066 | val_1_rmse: 0.71334 |  0:00:13s
epoch 58 | loss: 0.46425 | val_0_rmse: 0.66804 | val_1_rmse: 0.69822 |  0:00:14s
epoch 59 | loss: 0.46205 | val_0_rmse: 0.6698  | val_1_rmse: 0.70624 |  0:00:14s
epoch 60 | loss: 0.45555 | val_0_rmse: 0.67152 | val_1_rmse: 0.69718 |  0:00:14s
epoch 61 | loss: 0.45412 | val_0_rmse: 0.66936 | val_1_rmse: 0.69183 |  0:00:14s
epoch 62 | loss: 0.45172 | val_0_rmse: 0.66136 | val_1_rmse: 0.68831 |  0:00:15s
epoch 63 | loss: 0.44248 | val_0_rmse: 0.66657 | val_1_rmse: 0.70604 |  0:00:15s
epoch 64 | loss: 0.45281 | val_0_rmse: 0.67291 | val_1_rmse: 0.7032  |  0:00:15s
epoch 65 | loss: 0.45122 | val_0_rmse: 0.66086 | val_1_rmse: 0.69996 |  0:00:15s
epoch 66 | loss: 0.44863 | val_0_rmse: 0.66764 | val_1_rmse: 0.70256 |  0:00:16s
epoch 67 | loss: 0.44488 | val_0_rmse: 0.66782 | val_1_rmse: 0.71219 |  0:00:16s
epoch 68 | loss: 0.44666 | val_0_rmse: 0.66923 | val_1_rmse: 0.72418 |  0:00:16s
epoch 69 | loss: 0.44646 | val_0_rmse: 0.66339 | val_1_rmse: 0.70611 |  0:00:16s
epoch 70 | loss: 0.44501 | val_0_rmse: 0.66461 | val_1_rmse: 0.71219 |  0:00:17s
epoch 71 | loss: 0.4486  | val_0_rmse: 0.66087 | val_1_rmse: 0.70882 |  0:00:17s
epoch 72 | loss: 0.44362 | val_0_rmse: 0.66081 | val_1_rmse: 0.69749 |  0:00:17s
epoch 73 | loss: 0.45033 | val_0_rmse: 0.66913 | val_1_rmse: 0.72784 |  0:00:17s
epoch 74 | loss: 0.45416 | val_0_rmse: 0.6701  | val_1_rmse: 0.7226  |  0:00:17s
epoch 75 | loss: 0.44434 | val_0_rmse: 0.65518 | val_1_rmse: 0.69392 |  0:00:18s
epoch 76 | loss: 0.44909 | val_0_rmse: 0.65868 | val_1_rmse: 0.71733 |  0:00:18s
epoch 77 | loss: 0.44392 | val_0_rmse: 0.67057 | val_1_rmse: 0.73671 |  0:00:18s
epoch 78 | loss: 0.44291 | val_0_rmse: 0.65836 | val_1_rmse: 0.71143 |  0:00:18s
epoch 79 | loss: 0.43725 | val_0_rmse: 0.65273 | val_1_rmse: 0.69359 |  0:00:19s
epoch 80 | loss: 0.4326  | val_0_rmse: 0.67953 | val_1_rmse: 0.7291  |  0:00:19s
epoch 81 | loss: 0.4283  | val_0_rmse: 0.67305 | val_1_rmse: 0.73097 |  0:00:19s
epoch 82 | loss: 0.4159  | val_0_rmse: 0.64    | val_1_rmse: 0.679   |  0:00:19s
epoch 83 | loss: 0.41667 | val_0_rmse: 0.64146 | val_1_rmse: 0.6835  |  0:00:20s
epoch 84 | loss: 0.4121  | val_0_rmse: 0.65288 | val_1_rmse: 0.70573 |  0:00:20s
epoch 85 | loss: 0.41592 | val_0_rmse: 0.63393 | val_1_rmse: 0.67799 |  0:00:20s
epoch 86 | loss: 0.41747 | val_0_rmse: 0.64293 | val_1_rmse: 0.68925 |  0:00:20s
epoch 87 | loss: 0.42644 | val_0_rmse: 0.67288 | val_1_rmse: 0.73236 |  0:00:21s
epoch 88 | loss: 0.41119 | val_0_rmse: 0.63976 | val_1_rmse: 0.68516 |  0:00:21s
epoch 89 | loss: 0.42594 | val_0_rmse: 0.63612 | val_1_rmse: 0.64536 |  0:00:21s
epoch 90 | loss: 0.41846 | val_0_rmse: 0.65763 | val_1_rmse: 0.6755  |  0:00:21s
epoch 91 | loss: 0.42154 | val_0_rmse: 0.64572 | val_1_rmse: 0.67483 |  0:00:22s
epoch 92 | loss: 0.41636 | val_0_rmse: 0.62966 | val_1_rmse: 0.64891 |  0:00:22s
epoch 93 | loss: 0.40825 | val_0_rmse: 0.64146 | val_1_rmse: 0.67418 |  0:00:22s
epoch 94 | loss: 0.42388 | val_0_rmse: 0.63619 | val_1_rmse: 0.68137 |  0:00:22s
epoch 95 | loss: 0.42314 | val_0_rmse: 0.63502 | val_1_rmse: 0.6792  |  0:00:23s
epoch 96 | loss: 0.42348 | val_0_rmse: 0.65644 | val_1_rmse: 0.70768 |  0:00:23s
epoch 97 | loss: 0.40913 | val_0_rmse: 0.66171 | val_1_rmse: 0.71026 |  0:00:23s
epoch 98 | loss: 0.40998 | val_0_rmse: 0.71124 | val_1_rmse: 0.76313 |  0:00:23s
epoch 99 | loss: 0.41112 | val_0_rmse: 0.71196 | val_1_rmse: 0.75805 |  0:00:24s
epoch 100| loss: 0.40462 | val_0_rmse: 0.64075 | val_1_rmse: 0.66195 |  0:00:24s
epoch 101| loss: 0.43097 | val_0_rmse: 0.68444 | val_1_rmse: 0.72607 |  0:00:24s
epoch 102| loss: 0.41226 | val_0_rmse: 0.65424 | val_1_rmse: 0.70844 |  0:00:24s
epoch 103| loss: 0.41618 | val_0_rmse: 0.63703 | val_1_rmse: 0.68348 |  0:00:24s
epoch 104| loss: 0.42598 | val_0_rmse: 0.64029 | val_1_rmse: 0.70806 |  0:00:25s
epoch 105| loss: 0.40451 | val_0_rmse: 0.64067 | val_1_rmse: 0.70843 |  0:00:25s
epoch 106| loss: 0.42341 | val_0_rmse: 0.63975 | val_1_rmse: 0.68494 |  0:00:25s
epoch 107| loss: 0.41854 | val_0_rmse: 0.65595 | val_1_rmse: 0.71134 |  0:00:25s
epoch 108| loss: 0.4105  | val_0_rmse: 0.64456 | val_1_rmse: 0.69742 |  0:00:26s
epoch 109| loss: 0.4071  | val_0_rmse: 0.62843 | val_1_rmse: 0.65577 |  0:00:26s
epoch 110| loss: 0.41217 | val_0_rmse: 0.62246 | val_1_rmse: 0.67186 |  0:00:26s
epoch 111| loss: 0.3991  | val_0_rmse: 0.62269 | val_1_rmse: 0.67935 |  0:00:26s
epoch 112| loss: 0.38577 | val_0_rmse: 0.62441 | val_1_rmse: 0.65984 |  0:00:27s
epoch 113| loss: 0.40231 | val_0_rmse: 0.63573 | val_1_rmse: 0.65431 |  0:00:27s
epoch 114| loss: 0.40196 | val_0_rmse: 0.61507 | val_1_rmse: 0.6454  |  0:00:27s
epoch 115| loss: 0.38585 | val_0_rmse: 0.61166 | val_1_rmse: 0.65646 |  0:00:27s
epoch 116| loss: 0.3946  | val_0_rmse: 0.61809 | val_1_rmse: 0.65684 |  0:00:28s
epoch 117| loss: 0.39052 | val_0_rmse: 0.64031 | val_1_rmse: 0.6793  |  0:00:28s
epoch 118| loss: 0.41026 | val_0_rmse: 0.62726 | val_1_rmse: 0.65416 |  0:00:28s
epoch 119| loss: 0.4075  | val_0_rmse: 0.61396 | val_1_rmse: 0.63787 |  0:00:28s
epoch 120| loss: 0.41768 | val_0_rmse: 0.61241 | val_1_rmse: 0.64394 |  0:00:29s
epoch 121| loss: 0.38592 | val_0_rmse: 0.61888 | val_1_rmse: 0.66927 |  0:00:29s
epoch 122| loss: 0.40377 | val_0_rmse: 0.62857 | val_1_rmse: 0.65585 |  0:00:29s
epoch 123| loss: 0.40074 | val_0_rmse: 0.62914 | val_1_rmse: 0.65678 |  0:00:29s
epoch 124| loss: 0.39157 | val_0_rmse: 0.63091 | val_1_rmse: 0.66247 |  0:00:30s
epoch 125| loss: 0.40594 | val_0_rmse: 0.63998 | val_1_rmse: 0.66258 |  0:00:30s
epoch 126| loss: 0.40806 | val_0_rmse: 0.64663 | val_1_rmse: 0.66906 |  0:00:30s
epoch 127| loss: 0.41801 | val_0_rmse: 0.62822 | val_1_rmse: 0.67934 |  0:00:30s
epoch 128| loss: 0.4164  | val_0_rmse: 0.62241 | val_1_rmse: 0.67288 |  0:00:30s
epoch 129| loss: 0.38792 | val_0_rmse: 0.62525 | val_1_rmse: 0.66172 |  0:00:31s
epoch 130| loss: 0.39606 | val_0_rmse: 0.63491 | val_1_rmse: 0.68043 |  0:00:31s
epoch 131| loss: 0.40475 | val_0_rmse: 0.6308  | val_1_rmse: 0.68797 |  0:00:31s
epoch 132| loss: 0.39618 | val_0_rmse: 0.6186  | val_1_rmse: 0.66796 |  0:00:31s
epoch 133| loss: 0.3868  | val_0_rmse: 0.61294 | val_1_rmse: 0.64582 |  0:00:32s
epoch 134| loss: 0.39779 | val_0_rmse: 0.61508 | val_1_rmse: 0.64561 |  0:00:32s
epoch 135| loss: 0.39287 | val_0_rmse: 0.61609 | val_1_rmse: 0.66654 |  0:00:32s
epoch 136| loss: 0.39074 | val_0_rmse: 0.63411 | val_1_rmse: 0.69395 |  0:00:32s
epoch 137| loss: 0.4102  | val_0_rmse: 0.63384 | val_1_rmse: 0.69212 |  0:00:33s
epoch 138| loss: 0.37564 | val_0_rmse: 0.64923 | val_1_rmse: 0.71321 |  0:00:33s
epoch 139| loss: 0.37764 | val_0_rmse: 0.6721  | val_1_rmse: 0.73401 |  0:00:33s
epoch 140| loss: 0.37647 | val_0_rmse: 0.64554 | val_1_rmse: 0.69068 |  0:00:33s
epoch 141| loss: 0.37641 | val_0_rmse: 0.60481 | val_1_rmse: 0.63545 |  0:00:34s
epoch 142| loss: 0.36267 | val_0_rmse: 0.60353 | val_1_rmse: 0.63938 |  0:00:34s
epoch 143| loss: 0.37677 | val_0_rmse: 0.60616 | val_1_rmse: 0.62446 |  0:00:34s
epoch 144| loss: 0.36844 | val_0_rmse: 0.61989 | val_1_rmse: 0.62903 |  0:00:34s
epoch 145| loss: 0.37447 | val_0_rmse: 0.61939 | val_1_rmse: 0.63364 |  0:00:35s
epoch 146| loss: 0.36184 | val_0_rmse: 0.59688 | val_1_rmse: 0.62234 |  0:00:35s
epoch 147| loss: 0.35501 | val_0_rmse: 0.60671 | val_1_rmse: 0.62997 |  0:00:35s
epoch 148| loss: 0.35611 | val_0_rmse: 0.60256 | val_1_rmse: 0.65374 |  0:00:35s
epoch 149| loss: 0.36452 | val_0_rmse: 0.61543 | val_1_rmse: 0.6767  |  0:00:36s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.62234
Best weights from best epoch are automatically used!
ended training at: 02:41:07
Feature importance:
[('Area', 0.26902799034619085), ('Baths', 0.11044762718552392), ('Beds', 0.024614555276778454), ('Latitude', 0.2580948168243276), ('Longitude', 0.2739380063921578), ('Month', 0.015905246910359123), ('Year', 0.04797175706466219)]
Mean squared error is of 3015134725.0262246
Mean absolute error:39858.141521447804
MAPE:0.41218782465949233
R2 score:0.5619670956232672
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:41:07
epoch 0  | loss: 1.79391 | val_0_rmse: 1.53503 | val_1_rmse: 1.58049 |  0:00:00s
epoch 1  | loss: 0.92033 | val_0_rmse: 1.00677 | val_1_rmse: 1.05943 |  0:00:00s
epoch 2  | loss: 0.66732 | val_0_rmse: 0.91283 | val_1_rmse: 1.00354 |  0:00:00s
epoch 3  | loss: 0.65369 | val_0_rmse: 1.04673 | val_1_rmse: 1.13409 |  0:00:00s
epoch 4  | loss: 0.61772 | val_0_rmse: 0.85048 | val_1_rmse: 0.94224 |  0:00:01s
epoch 5  | loss: 0.59083 | val_0_rmse: 0.77727 | val_1_rmse: 0.87592 |  0:00:01s
epoch 6  | loss: 0.57635 | val_0_rmse: 0.78334 | val_1_rmse: 0.88396 |  0:00:01s
epoch 7  | loss: 0.56021 | val_0_rmse: 0.7751  | val_1_rmse: 0.88624 |  0:00:01s
epoch 8  | loss: 0.53608 | val_0_rmse: 0.74181 | val_1_rmse: 0.85399 |  0:00:02s
epoch 9  | loss: 0.52254 | val_0_rmse: 0.72887 | val_1_rmse: 0.85487 |  0:00:02s
epoch 10 | loss: 0.53802 | val_0_rmse: 0.72119 | val_1_rmse: 0.84948 |  0:00:02s
epoch 11 | loss: 0.51506 | val_0_rmse: 0.72141 | val_1_rmse: 0.84734 |  0:00:02s
epoch 12 | loss: 0.50548 | val_0_rmse: 0.72484 | val_1_rmse: 0.86253 |  0:00:03s
epoch 13 | loss: 0.50656 | val_0_rmse: 0.73161 | val_1_rmse: 0.87071 |  0:00:03s
epoch 14 | loss: 0.51687 | val_0_rmse: 0.72038 | val_1_rmse: 0.8529  |  0:00:03s
epoch 15 | loss: 0.5029  | val_0_rmse: 0.72887 | val_1_rmse: 0.83411 |  0:00:03s
epoch 16 | loss: 0.49636 | val_0_rmse: 0.71443 | val_1_rmse: 0.81733 |  0:00:04s
epoch 17 | loss: 0.49308 | val_0_rmse: 0.70456 | val_1_rmse: 0.81102 |  0:00:04s
epoch 18 | loss: 0.50285 | val_0_rmse: 0.70134 | val_1_rmse: 0.81115 |  0:00:04s
epoch 19 | loss: 0.49571 | val_0_rmse: 0.693   | val_1_rmse: 0.81149 |  0:00:04s
epoch 20 | loss: 0.49638 | val_0_rmse: 0.69693 | val_1_rmse: 0.81004 |  0:00:05s
epoch 21 | loss: 0.4942  | val_0_rmse: 0.7113  | val_1_rmse: 0.81305 |  0:00:05s
epoch 22 | loss: 0.48908 | val_0_rmse: 0.7017  | val_1_rmse: 0.81454 |  0:00:05s
epoch 23 | loss: 0.48231 | val_0_rmse: 0.68849 | val_1_rmse: 0.81443 |  0:00:05s
epoch 24 | loss: 0.48363 | val_0_rmse: 0.6932  | val_1_rmse: 0.8167  |  0:00:06s
epoch 25 | loss: 0.47829 | val_0_rmse: 0.69011 | val_1_rmse: 0.80592 |  0:00:06s
epoch 26 | loss: 0.47193 | val_0_rmse: 0.68477 | val_1_rmse: 0.8092  |  0:00:06s
epoch 27 | loss: 0.4936  | val_0_rmse: 0.68303 | val_1_rmse: 0.81035 |  0:00:06s
epoch 28 | loss: 0.46752 | val_0_rmse: 0.6955  | val_1_rmse: 0.8142  |  0:00:07s
epoch 29 | loss: 0.47671 | val_0_rmse: 0.68594 | val_1_rmse: 0.80127 |  0:00:07s
epoch 30 | loss: 0.46183 | val_0_rmse: 0.67797 | val_1_rmse: 0.80518 |  0:00:07s
epoch 31 | loss: 0.46534 | val_0_rmse: 0.67516 | val_1_rmse: 0.81229 |  0:00:07s
epoch 32 | loss: 0.47335 | val_0_rmse: 0.67307 | val_1_rmse: 0.81473 |  0:00:08s
epoch 33 | loss: 0.46625 | val_0_rmse: 0.67396 | val_1_rmse: 0.79836 |  0:00:08s
epoch 34 | loss: 0.46046 | val_0_rmse: 0.67691 | val_1_rmse: 0.8021  |  0:00:08s
epoch 35 | loss: 0.47959 | val_0_rmse: 0.66308 | val_1_rmse: 0.78786 |  0:00:08s
epoch 36 | loss: 0.4622  | val_0_rmse: 0.68232 | val_1_rmse: 0.79005 |  0:00:09s
epoch 37 | loss: 0.45993 | val_0_rmse: 0.66797 | val_1_rmse: 0.77471 |  0:00:09s
epoch 38 | loss: 0.45626 | val_0_rmse: 0.66563 | val_1_rmse: 0.78646 |  0:00:09s
epoch 39 | loss: 0.45177 | val_0_rmse: 0.67183 | val_1_rmse: 0.81423 |  0:00:09s
epoch 40 | loss: 0.46233 | val_0_rmse: 0.71578 | val_1_rmse: 0.83291 |  0:00:10s
epoch 41 | loss: 0.47773 | val_0_rmse: 0.69445 | val_1_rmse: 0.80395 |  0:00:10s
epoch 42 | loss: 0.46387 | val_0_rmse: 0.69467 | val_1_rmse: 0.79468 |  0:00:10s
epoch 43 | loss: 0.47884 | val_0_rmse: 0.70812 | val_1_rmse: 0.80963 |  0:00:10s
epoch 44 | loss: 0.4717  | val_0_rmse: 0.685   | val_1_rmse: 0.79273 |  0:00:11s
epoch 45 | loss: 0.4484  | val_0_rmse: 0.68019 | val_1_rmse: 0.7886  |  0:00:11s
epoch 46 | loss: 0.47144 | val_0_rmse: 0.68387 | val_1_rmse: 0.78602 |  0:00:11s
epoch 47 | loss: 0.44433 | val_0_rmse: 0.6898  | val_1_rmse: 0.7903  |  0:00:11s
epoch 48 | loss: 0.46491 | val_0_rmse: 0.68734 | val_1_rmse: 0.79129 |  0:00:12s
epoch 49 | loss: 0.45641 | val_0_rmse: 0.69721 | val_1_rmse: 0.81698 |  0:00:12s
epoch 50 | loss: 0.44895 | val_0_rmse: 0.68526 | val_1_rmse: 0.79815 |  0:00:12s
epoch 51 | loss: 0.44523 | val_0_rmse: 0.7685  | val_1_rmse: 0.88508 |  0:00:12s
epoch 52 | loss: 0.44506 | val_0_rmse: 0.80954 | val_1_rmse: 0.88338 |  0:00:13s
epoch 53 | loss: 0.45919 | val_0_rmse: 0.73027 | val_1_rmse: 0.82719 |  0:00:13s
epoch 54 | loss: 0.44588 | val_0_rmse: 0.71545 | val_1_rmse: 0.80245 |  0:00:13s
epoch 55 | loss: 0.44738 | val_0_rmse: 0.66854 | val_1_rmse: 0.75899 |  0:00:13s
epoch 56 | loss: 0.45384 | val_0_rmse: 0.66007 | val_1_rmse: 0.76881 |  0:00:14s
epoch 57 | loss: 0.44667 | val_0_rmse: 0.66348 | val_1_rmse: 0.7756  |  0:00:14s
epoch 58 | loss: 0.45458 | val_0_rmse: 0.73791 | val_1_rmse: 0.83801 |  0:00:14s
epoch 59 | loss: 0.44693 | val_0_rmse: 0.7641  | val_1_rmse: 0.85644 |  0:00:14s
epoch 60 | loss: 0.45351 | val_0_rmse: 0.71666 | val_1_rmse: 0.80887 |  0:00:15s
epoch 61 | loss: 0.44044 | val_0_rmse: 0.66518 | val_1_rmse: 0.76523 |  0:00:15s
epoch 62 | loss: 0.434   | val_0_rmse: 0.67068 | val_1_rmse: 0.77655 |  0:00:15s
epoch 63 | loss: 0.4364  | val_0_rmse: 0.67149 | val_1_rmse: 0.7926  |  0:00:15s
epoch 64 | loss: 0.43903 | val_0_rmse: 0.65656 | val_1_rmse: 0.76562 |  0:00:16s
epoch 65 | loss: 0.44065 | val_0_rmse: 0.64733 | val_1_rmse: 0.73266 |  0:00:16s
epoch 66 | loss: 0.42779 | val_0_rmse: 0.65842 | val_1_rmse: 0.74229 |  0:00:16s
epoch 67 | loss: 0.43513 | val_0_rmse: 0.64575 | val_1_rmse: 0.74003 |  0:00:16s
epoch 68 | loss: 0.43365 | val_0_rmse: 0.64797 | val_1_rmse: 0.74848 |  0:00:16s
epoch 69 | loss: 0.42324 | val_0_rmse: 0.65906 | val_1_rmse: 0.76325 |  0:00:17s
epoch 70 | loss: 0.42421 | val_0_rmse: 0.65968 | val_1_rmse: 0.77136 |  0:00:17s
epoch 71 | loss: 0.42017 | val_0_rmse: 0.65046 | val_1_rmse: 0.76206 |  0:00:17s
epoch 72 | loss: 0.41796 | val_0_rmse: 0.64196 | val_1_rmse: 0.74334 |  0:00:17s
epoch 73 | loss: 0.43483 | val_0_rmse: 0.64474 | val_1_rmse: 0.75009 |  0:00:18s
epoch 74 | loss: 0.43897 | val_0_rmse: 0.63464 | val_1_rmse: 0.74209 |  0:00:18s
epoch 75 | loss: 0.42704 | val_0_rmse: 0.6442  | val_1_rmse: 0.74363 |  0:00:18s
epoch 76 | loss: 0.43392 | val_0_rmse: 0.63886 | val_1_rmse: 0.75603 |  0:00:18s
epoch 77 | loss: 0.41952 | val_0_rmse: 0.6414  | val_1_rmse: 0.76019 |  0:00:19s
epoch 78 | loss: 0.42758 | val_0_rmse: 0.65781 | val_1_rmse: 0.76931 |  0:00:19s
epoch 79 | loss: 0.42898 | val_0_rmse: 0.64316 | val_1_rmse: 0.74549 |  0:00:19s
epoch 80 | loss: 0.42044 | val_0_rmse: 0.65326 | val_1_rmse: 0.73944 |  0:00:19s
epoch 81 | loss: 0.41909 | val_0_rmse: 0.71465 | val_1_rmse: 0.77356 |  0:00:20s
epoch 82 | loss: 0.42299 | val_0_rmse: 0.68631 | val_1_rmse: 0.7456  |  0:00:20s
epoch 83 | loss: 0.42082 | val_0_rmse: 0.66076 | val_1_rmse: 0.7265  |  0:00:20s
epoch 84 | loss: 0.41044 | val_0_rmse: 0.64608 | val_1_rmse: 0.72192 |  0:00:20s
epoch 85 | loss: 0.41797 | val_0_rmse: 0.63894 | val_1_rmse: 0.71677 |  0:00:21s
epoch 86 | loss: 0.41335 | val_0_rmse: 0.67109 | val_1_rmse: 0.74779 |  0:00:21s
epoch 87 | loss: 0.41867 | val_0_rmse: 0.69521 | val_1_rmse: 0.7673  |  0:00:21s
epoch 88 | loss: 0.41312 | val_0_rmse: 0.63674 | val_1_rmse: 0.70321 |  0:00:21s
epoch 89 | loss: 0.42656 | val_0_rmse: 0.63375 | val_1_rmse: 0.70609 |  0:00:22s
epoch 90 | loss: 0.41679 | val_0_rmse: 0.65939 | val_1_rmse: 0.72873 |  0:00:22s
epoch 91 | loss: 0.40618 | val_0_rmse: 0.66224 | val_1_rmse: 0.73233 |  0:00:22s
epoch 92 | loss: 0.41013 | val_0_rmse: 0.6929  | val_1_rmse: 0.77265 |  0:00:22s
epoch 93 | loss: 0.41165 | val_0_rmse: 0.64573 | val_1_rmse: 0.7384  |  0:00:23s
epoch 94 | loss: 0.41442 | val_0_rmse: 0.63335 | val_1_rmse: 0.72832 |  0:00:23s
epoch 95 | loss: 0.41638 | val_0_rmse: 0.62911 | val_1_rmse: 0.72706 |  0:00:23s
epoch 96 | loss: 0.43194 | val_0_rmse: 0.62757 | val_1_rmse: 0.72481 |  0:00:23s
epoch 97 | loss: 0.40833 | val_0_rmse: 0.62096 | val_1_rmse: 0.71674 |  0:00:24s
epoch 98 | loss: 0.40173 | val_0_rmse: 0.61897 | val_1_rmse: 0.71192 |  0:00:24s
epoch 99 | loss: 0.39646 | val_0_rmse: 0.62137 | val_1_rmse: 0.70706 |  0:00:24s
epoch 100| loss: 0.39155 | val_0_rmse: 0.62599 | val_1_rmse: 0.70634 |  0:00:24s
epoch 101| loss: 0.39478 | val_0_rmse: 0.63719 | val_1_rmse: 0.71229 |  0:00:24s
epoch 102| loss: 0.39776 | val_0_rmse: 0.62516 | val_1_rmse: 0.70496 |  0:00:25s
epoch 103| loss: 0.40673 | val_0_rmse: 0.62921 | val_1_rmse: 0.71357 |  0:00:25s
epoch 104| loss: 0.40936 | val_0_rmse: 0.63222 | val_1_rmse: 0.72124 |  0:00:25s
epoch 105| loss: 0.39624 | val_0_rmse: 0.62857 | val_1_rmse: 0.73364 |  0:00:25s
epoch 106| loss: 0.41443 | val_0_rmse: 0.63303 | val_1_rmse: 0.74359 |  0:00:26s
epoch 107| loss: 0.40056 | val_0_rmse: 0.62783 | val_1_rmse: 0.74284 |  0:00:26s
epoch 108| loss: 0.4057  | val_0_rmse: 0.62078 | val_1_rmse: 0.73087 |  0:00:26s
epoch 109| loss: 0.40352 | val_0_rmse: 0.63804 | val_1_rmse: 0.7384  |  0:00:26s
epoch 110| loss: 0.403   | val_0_rmse: 0.63819 | val_1_rmse: 0.7382  |  0:00:27s
epoch 111| loss: 0.39836 | val_0_rmse: 0.63096 | val_1_rmse: 0.73083 |  0:00:27s
epoch 112| loss: 0.40069 | val_0_rmse: 0.62766 | val_1_rmse: 0.71734 |  0:00:27s
epoch 113| loss: 0.40914 | val_0_rmse: 0.62263 | val_1_rmse: 0.72211 |  0:00:27s
epoch 114| loss: 0.40363 | val_0_rmse: 0.62342 | val_1_rmse: 0.72977 |  0:00:28s
epoch 115| loss: 0.3999  | val_0_rmse: 0.63022 | val_1_rmse: 0.72121 |  0:00:28s
epoch 116| loss: 0.39092 | val_0_rmse: 0.63036 | val_1_rmse: 0.7124  |  0:00:28s
epoch 117| loss: 0.39633 | val_0_rmse: 0.62428 | val_1_rmse: 0.71629 |  0:00:28s
epoch 118| loss: 0.39386 | val_0_rmse: 0.6209  | val_1_rmse: 0.72174 |  0:00:29s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.70321
Best weights from best epoch are automatically used!
ended training at: 02:41:36
Feature importance:
[('Area', 0.3066490424586809), ('Baths', 0.11270566148709112), ('Beds', 0.05493544312192173), ('Latitude', 0.37817370055181315), ('Longitude', 0.06997520933108545), ('Month', 0.05187950642117653), ('Year', 0.02568143662823115)]
Mean squared error is of 2784165097.8272347
Mean absolute error:39812.335946085164
MAPE:0.3814808624792538
R2 score:0.573335255672474
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:41:36
epoch 0  | loss: 1.96875 | val_0_rmse: 1.06939 | val_1_rmse: 1.02693 |  0:00:00s
epoch 1  | loss: 0.87656 | val_0_rmse: 1.10829 | val_1_rmse: 1.08965 |  0:00:00s
epoch 2  | loss: 0.63775 | val_0_rmse: 1.03924 | val_1_rmse: 1.02661 |  0:00:00s
epoch 3  | loss: 0.53837 | val_0_rmse: 0.89552 | val_1_rmse: 0.91638 |  0:00:00s
epoch 4  | loss: 0.49755 | val_0_rmse: 0.79198 | val_1_rmse: 0.80382 |  0:00:01s
epoch 5  | loss: 0.47177 | val_0_rmse: 0.83866 | val_1_rmse: 0.7964  |  0:00:01s
epoch 6  | loss: 0.44525 | val_0_rmse: 0.87242 | val_1_rmse: 0.87443 |  0:00:01s
epoch 7  | loss: 0.42    | val_0_rmse: 0.80943 | val_1_rmse: 0.83691 |  0:00:01s
epoch 8  | loss: 0.41973 | val_0_rmse: 0.82442 | val_1_rmse: 0.82928 |  0:00:02s
epoch 9  | loss: 0.39924 | val_0_rmse: 0.78777 | val_1_rmse: 0.7809  |  0:00:02s
epoch 10 | loss: 0.39413 | val_0_rmse: 0.77363 | val_1_rmse: 0.75466 |  0:00:02s
epoch 11 | loss: 0.397   | val_0_rmse: 0.71316 | val_1_rmse: 0.71592 |  0:00:02s
epoch 12 | loss: 0.39713 | val_0_rmse: 0.68523 | val_1_rmse: 0.67004 |  0:00:03s
epoch 13 | loss: 0.38771 | val_0_rmse: 0.68243 | val_1_rmse: 0.68149 |  0:00:03s
epoch 14 | loss: 0.37322 | val_0_rmse: 0.66347 | val_1_rmse: 0.69125 |  0:00:03s
epoch 15 | loss: 0.36439 | val_0_rmse: 0.63754 | val_1_rmse: 0.63727 |  0:00:03s
epoch 16 | loss: 0.37465 | val_0_rmse: 0.63845 | val_1_rmse: 0.62169 |  0:00:04s
epoch 17 | loss: 0.37202 | val_0_rmse: 0.63162 | val_1_rmse: 0.6197  |  0:00:04s
epoch 18 | loss: 0.36888 | val_0_rmse: 0.61361 | val_1_rmse: 0.63757 |  0:00:04s
epoch 19 | loss: 0.35379 | val_0_rmse: 0.60171 | val_1_rmse: 0.60416 |  0:00:04s
epoch 20 | loss: 0.34946 | val_0_rmse: 0.59909 | val_1_rmse: 0.59728 |  0:00:05s
epoch 21 | loss: 0.35447 | val_0_rmse: 0.59398 | val_1_rmse: 0.61669 |  0:00:05s
epoch 22 | loss: 0.34225 | val_0_rmse: 0.59442 | val_1_rmse: 0.62193 |  0:00:05s
epoch 23 | loss: 0.35436 | val_0_rmse: 0.58496 | val_1_rmse: 0.60302 |  0:00:05s
epoch 24 | loss: 0.34032 | val_0_rmse: 0.57367 | val_1_rmse: 0.60454 |  0:00:06s
epoch 25 | loss: 0.34359 | val_0_rmse: 0.57693 | val_1_rmse: 0.60389 |  0:00:06s
epoch 26 | loss: 0.33843 | val_0_rmse: 0.59684 | val_1_rmse: 0.58416 |  0:00:06s
epoch 27 | loss: 0.35175 | val_0_rmse: 0.57278 | val_1_rmse: 0.5894  |  0:00:07s
epoch 28 | loss: 0.3413  | val_0_rmse: 0.57942 | val_1_rmse: 0.61664 |  0:00:07s
epoch 29 | loss: 0.33901 | val_0_rmse: 0.59022 | val_1_rmse: 0.59248 |  0:00:07s
epoch 30 | loss: 0.33536 | val_0_rmse: 0.59118 | val_1_rmse: 0.59568 |  0:00:07s
epoch 31 | loss: 0.33858 | val_0_rmse: 0.56086 | val_1_rmse: 0.59254 |  0:00:07s
epoch 32 | loss: 0.32624 | val_0_rmse: 0.56519 | val_1_rmse: 0.58556 |  0:00:08s
epoch 33 | loss: 0.32806 | val_0_rmse: 0.56834 | val_1_rmse: 0.59393 |  0:00:08s
epoch 34 | loss: 0.3333  | val_0_rmse: 0.56995 | val_1_rmse: 0.62006 |  0:00:08s
epoch 35 | loss: 0.33568 | val_0_rmse: 0.5577  | val_1_rmse: 0.588   |  0:00:08s
epoch 36 | loss: 0.32518 | val_0_rmse: 0.5612  | val_1_rmse: 0.59477 |  0:00:09s
epoch 37 | loss: 0.32519 | val_0_rmse: 0.55708 | val_1_rmse: 0.63601 |  0:00:09s
epoch 38 | loss: 0.32669 | val_0_rmse: 0.5532  | val_1_rmse: 0.62483 |  0:00:09s
epoch 39 | loss: 0.31304 | val_0_rmse: 0.55253 | val_1_rmse: 0.58169 |  0:00:09s
epoch 40 | loss: 0.31949 | val_0_rmse: 0.55288 | val_1_rmse: 0.59366 |  0:00:10s
epoch 41 | loss: 0.31438 | val_0_rmse: 0.55765 | val_1_rmse: 0.60936 |  0:00:10s
epoch 42 | loss: 0.32149 | val_0_rmse: 0.55285 | val_1_rmse: 0.599   |  0:00:10s
epoch 43 | loss: 0.32404 | val_0_rmse: 0.55274 | val_1_rmse: 0.60433 |  0:00:10s
epoch 44 | loss: 0.31038 | val_0_rmse: 0.55574 | val_1_rmse: 0.60153 |  0:00:11s
epoch 45 | loss: 0.31913 | val_0_rmse: 0.55954 | val_1_rmse: 0.61558 |  0:00:11s
epoch 46 | loss: 0.32719 | val_0_rmse: 0.55967 | val_1_rmse: 0.61675 |  0:00:11s
epoch 47 | loss: 0.32141 | val_0_rmse: 0.56419 | val_1_rmse: 0.58216 |  0:00:11s
epoch 48 | loss: 0.32303 | val_0_rmse: 0.55437 | val_1_rmse: 0.57389 |  0:00:12s
epoch 49 | loss: 0.31672 | val_0_rmse: 0.56024 | val_1_rmse: 0.58716 |  0:00:12s
epoch 50 | loss: 0.32695 | val_0_rmse: 0.57444 | val_1_rmse: 0.57762 |  0:00:12s
epoch 51 | loss: 0.32769 | val_0_rmse: 0.55534 | val_1_rmse: 0.59206 |  0:00:12s
epoch 52 | loss: 0.3327  | val_0_rmse: 0.57369 | val_1_rmse: 0.65522 |  0:00:13s
epoch 53 | loss: 0.33602 | val_0_rmse: 0.55312 | val_1_rmse: 0.56485 |  0:00:13s
epoch 54 | loss: 0.32143 | val_0_rmse: 0.58164 | val_1_rmse: 0.56863 |  0:00:13s
epoch 55 | loss: 0.32927 | val_0_rmse: 0.54707 | val_1_rmse: 0.58314 |  0:00:13s
epoch 56 | loss: 0.31876 | val_0_rmse: 0.5639  | val_1_rmse: 0.62618 |  0:00:14s
epoch 57 | loss: 0.31465 | val_0_rmse: 0.56046 | val_1_rmse: 0.58935 |  0:00:14s
epoch 58 | loss: 0.33102 | val_0_rmse: 0.55418 | val_1_rmse: 0.59958 |  0:00:14s
epoch 59 | loss: 0.31217 | val_0_rmse: 0.5625  | val_1_rmse: 0.65203 |  0:00:14s
epoch 60 | loss: 0.32298 | val_0_rmse: 0.5486  | val_1_rmse: 0.61113 |  0:00:15s
epoch 61 | loss: 0.31456 | val_0_rmse: 0.55502 | val_1_rmse: 0.5789  |  0:00:15s
epoch 62 | loss: 0.31499 | val_0_rmse: 0.55188 | val_1_rmse: 0.61316 |  0:00:15s
epoch 63 | loss: 0.30292 | val_0_rmse: 0.55559 | val_1_rmse: 0.61499 |  0:00:15s
epoch 64 | loss: 0.3133  | val_0_rmse: 0.55012 | val_1_rmse: 0.60708 |  0:00:15s
epoch 65 | loss: 0.30227 | val_0_rmse: 0.54519 | val_1_rmse: 0.60165 |  0:00:16s
epoch 66 | loss: 0.31627 | val_0_rmse: 0.54528 | val_1_rmse: 0.58522 |  0:00:16s
epoch 67 | loss: 0.32209 | val_0_rmse: 0.53938 | val_1_rmse: 0.58998 |  0:00:16s
epoch 68 | loss: 0.29425 | val_0_rmse: 0.54765 | val_1_rmse: 0.61258 |  0:00:16s
epoch 69 | loss: 0.30451 | val_0_rmse: 0.55079 | val_1_rmse: 0.60847 |  0:00:17s
epoch 70 | loss: 0.31295 | val_0_rmse: 0.55085 | val_1_rmse: 0.59852 |  0:00:17s
epoch 71 | loss: 0.29381 | val_0_rmse: 0.54606 | val_1_rmse: 0.59286 |  0:00:17s
epoch 72 | loss: 0.31758 | val_0_rmse: 0.54002 | val_1_rmse: 0.57825 |  0:00:17s
epoch 73 | loss: 0.30535 | val_0_rmse: 0.54172 | val_1_rmse: 0.57627 |  0:00:18s
epoch 74 | loss: 0.29925 | val_0_rmse: 0.54356 | val_1_rmse: 0.58094 |  0:00:18s
epoch 75 | loss: 0.29895 | val_0_rmse: 0.54649 | val_1_rmse: 0.59362 |  0:00:18s
epoch 76 | loss: 0.29958 | val_0_rmse: 0.54219 | val_1_rmse: 0.58932 |  0:00:18s
epoch 77 | loss: 0.30257 | val_0_rmse: 0.5349  | val_1_rmse: 0.58557 |  0:00:19s
epoch 78 | loss: 0.29169 | val_0_rmse: 0.53025 | val_1_rmse: 0.5966  |  0:00:19s
epoch 79 | loss: 0.29279 | val_0_rmse: 0.53565 | val_1_rmse: 0.6075  |  0:00:19s
epoch 80 | loss: 0.30518 | val_0_rmse: 0.52947 | val_1_rmse: 0.6004  |  0:00:19s
epoch 81 | loss: 0.30023 | val_0_rmse: 0.53068 | val_1_rmse: 0.58401 |  0:00:20s
epoch 82 | loss: 0.2929  | val_0_rmse: 0.52811 | val_1_rmse: 0.585   |  0:00:20s
epoch 83 | loss: 0.29139 | val_0_rmse: 0.53192 | val_1_rmse: 0.59254 |  0:00:20s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.56485
Best weights from best epoch are automatically used!
ended training at: 02:41:57
Feature importance:
[('Area', 0.30940023262889027), ('Baths', 0.14388608011967036), ('Beds', 0.13262665099314666), ('Latitude', 0.17087864261915167), ('Longitude', 0.1296216829621228), ('Month', 0.035885619663310955), ('Year', 0.07770109101370731)]
Mean squared error is of 1483056745.2957866
Mean absolute error:26009.908937335167
MAPE:0.3156475314893625
R2 score:0.6581907809339853
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:41:58
epoch 0  | loss: 2.13763 | val_0_rmse: 1.03155 | val_1_rmse: 0.94976 |  0:00:00s
epoch 1  | loss: 0.84651 | val_0_rmse: 1.20837 | val_1_rmse: 1.18282 |  0:00:00s
epoch 2  | loss: 0.64714 | val_0_rmse: 0.92134 | val_1_rmse: 0.85287 |  0:00:00s
epoch 3  | loss: 0.56447 | val_0_rmse: 0.88117 | val_1_rmse: 0.92528 |  0:00:00s
epoch 4  | loss: 0.51168 | val_0_rmse: 0.86173 | val_1_rmse: 0.91421 |  0:00:01s
epoch 5  | loss: 0.50104 | val_0_rmse: 0.76063 | val_1_rmse: 0.79152 |  0:00:01s
epoch 6  | loss: 0.45686 | val_0_rmse: 0.69518 | val_1_rmse: 0.72498 |  0:00:01s
epoch 7  | loss: 0.44903 | val_0_rmse: 0.68727 | val_1_rmse: 0.72373 |  0:00:01s
epoch 8  | loss: 0.431   | val_0_rmse: 0.67109 | val_1_rmse: 0.75061 |  0:00:02s
epoch 9  | loss: 0.42677 | val_0_rmse: 0.67719 | val_1_rmse: 0.76249 |  0:00:02s
epoch 10 | loss: 0.41496 | val_0_rmse: 0.67261 | val_1_rmse: 0.76524 |  0:00:02s
epoch 11 | loss: 0.3991  | val_0_rmse: 0.6533  | val_1_rmse: 0.71638 |  0:00:02s
epoch 12 | loss: 0.3923  | val_0_rmse: 0.63915 | val_1_rmse: 0.69297 |  0:00:03s
epoch 13 | loss: 0.37775 | val_0_rmse: 0.62371 | val_1_rmse: 0.69881 |  0:00:03s
epoch 14 | loss: 0.39216 | val_0_rmse: 0.61856 | val_1_rmse: 0.69041 |  0:00:03s
epoch 15 | loss: 0.38172 | val_0_rmse: 0.63172 | val_1_rmse: 0.69628 |  0:00:04s
epoch 16 | loss: 0.38773 | val_0_rmse: 0.63495 | val_1_rmse: 0.69338 |  0:00:04s
epoch 17 | loss: 0.38978 | val_0_rmse: 0.62811 | val_1_rmse: 0.6857  |  0:00:04s
epoch 18 | loss: 0.38207 | val_0_rmse: 0.61256 | val_1_rmse: 0.66609 |  0:00:04s
epoch 19 | loss: 0.37971 | val_0_rmse: 0.62761 | val_1_rmse: 0.67588 |  0:00:05s
epoch 20 | loss: 0.39198 | val_0_rmse: 0.6166  | val_1_rmse: 0.66104 |  0:00:05s
epoch 21 | loss: 0.37213 | val_0_rmse: 0.60523 | val_1_rmse: 0.65559 |  0:00:05s
epoch 22 | loss: 0.37223 | val_0_rmse: 0.60245 | val_1_rmse: 0.65727 |  0:00:05s
epoch 23 | loss: 0.36125 | val_0_rmse: 0.62597 | val_1_rmse: 0.67692 |  0:00:06s
epoch 24 | loss: 0.36329 | val_0_rmse: 0.60293 | val_1_rmse: 0.63589 |  0:00:06s
epoch 25 | loss: 0.3418  | val_0_rmse: 0.59498 | val_1_rmse: 0.60903 |  0:00:06s
epoch 26 | loss: 0.35162 | val_0_rmse: 0.59435 | val_1_rmse: 0.62413 |  0:00:06s
epoch 27 | loss: 0.34658 | val_0_rmse: 0.58932 | val_1_rmse: 0.62557 |  0:00:07s
epoch 28 | loss: 0.34879 | val_0_rmse: 0.58168 | val_1_rmse: 0.622   |  0:00:07s
epoch 29 | loss: 0.37221 | val_0_rmse: 0.62068 | val_1_rmse: 0.66621 |  0:00:07s
epoch 30 | loss: 0.37698 | val_0_rmse: 0.62188 | val_1_rmse: 0.66323 |  0:00:07s
epoch 31 | loss: 0.37664 | val_0_rmse: 0.62575 | val_1_rmse: 0.64574 |  0:00:07s
epoch 32 | loss: 0.37745 | val_0_rmse: 0.62194 | val_1_rmse: 0.6423  |  0:00:08s
epoch 33 | loss: 0.35814 | val_0_rmse: 0.62879 | val_1_rmse: 0.65325 |  0:00:08s
epoch 34 | loss: 0.36205 | val_0_rmse: 0.63514 | val_1_rmse: 0.64583 |  0:00:08s
epoch 35 | loss: 0.35883 | val_0_rmse: 0.60052 | val_1_rmse: 0.62211 |  0:00:08s
epoch 36 | loss: 0.35091 | val_0_rmse: 0.59638 | val_1_rmse: 0.63545 |  0:00:09s
epoch 37 | loss: 0.334   | val_0_rmse: 0.62895 | val_1_rmse: 0.65352 |  0:00:09s
epoch 38 | loss: 0.36208 | val_0_rmse: 0.58322 | val_1_rmse: 0.62576 |  0:00:09s
epoch 39 | loss: 0.33288 | val_0_rmse: 0.59027 | val_1_rmse: 0.63297 |  0:00:09s
epoch 40 | loss: 0.35705 | val_0_rmse: 0.58484 | val_1_rmse: 0.61844 |  0:00:10s
epoch 41 | loss: 0.32947 | val_0_rmse: 0.61773 | val_1_rmse: 0.63801 |  0:00:10s
epoch 42 | loss: 0.34638 | val_0_rmse: 0.57643 | val_1_rmse: 0.61456 |  0:00:10s
epoch 43 | loss: 0.32557 | val_0_rmse: 0.5622  | val_1_rmse: 0.62243 |  0:00:10s
epoch 44 | loss: 0.33482 | val_0_rmse: 0.56457 | val_1_rmse: 0.61359 |  0:00:11s
epoch 45 | loss: 0.3188  | val_0_rmse: 0.55106 | val_1_rmse: 0.59222 |  0:00:11s
epoch 46 | loss: 0.31067 | val_0_rmse: 0.54968 | val_1_rmse: 0.58527 |  0:00:11s
epoch 47 | loss: 0.3209  | val_0_rmse: 0.55274 | val_1_rmse: 0.58668 |  0:00:11s
epoch 48 | loss: 0.31271 | val_0_rmse: 0.54749 | val_1_rmse: 0.59059 |  0:00:12s
epoch 49 | loss: 0.31658 | val_0_rmse: 0.54949 | val_1_rmse: 0.5913  |  0:00:12s
epoch 50 | loss: 0.30125 | val_0_rmse: 0.53418 | val_1_rmse: 0.59258 |  0:00:12s
epoch 51 | loss: 0.29452 | val_0_rmse: 0.53178 | val_1_rmse: 0.59571 |  0:00:12s
epoch 52 | loss: 0.30064 | val_0_rmse: 0.53795 | val_1_rmse: 0.58854 |  0:00:13s
epoch 53 | loss: 0.2981  | val_0_rmse: 0.53442 | val_1_rmse: 0.57911 |  0:00:13s
epoch 54 | loss: 0.30061 | val_0_rmse: 0.53256 | val_1_rmse: 0.58658 |  0:00:13s
epoch 55 | loss: 0.31537 | val_0_rmse: 0.5325  | val_1_rmse: 0.5921  |  0:00:13s
epoch 56 | loss: 0.29466 | val_0_rmse: 0.53268 | val_1_rmse: 0.58187 |  0:00:14s
epoch 57 | loss: 0.30207 | val_0_rmse: 0.53615 | val_1_rmse: 0.58306 |  0:00:14s
epoch 58 | loss: 0.29733 | val_0_rmse: 0.54551 | val_1_rmse: 0.58424 |  0:00:14s
epoch 59 | loss: 0.31452 | val_0_rmse: 0.53234 | val_1_rmse: 0.58229 |  0:00:14s
epoch 60 | loss: 0.2889  | val_0_rmse: 0.54478 | val_1_rmse: 0.59709 |  0:00:15s
epoch 61 | loss: 0.31163 | val_0_rmse: 0.53804 | val_1_rmse: 0.59271 |  0:00:15s
epoch 62 | loss: 0.30479 | val_0_rmse: 0.54922 | val_1_rmse: 0.58626 |  0:00:15s
epoch 63 | loss: 0.30855 | val_0_rmse: 0.54682 | val_1_rmse: 0.57313 |  0:00:15s
epoch 64 | loss: 0.31653 | val_0_rmse: 0.54558 | val_1_rmse: 0.57643 |  0:00:16s
epoch 65 | loss: 0.30418 | val_0_rmse: 0.56044 | val_1_rmse: 0.61041 |  0:00:16s
epoch 66 | loss: 0.30553 | val_0_rmse: 0.55521 | val_1_rmse: 0.61056 |  0:00:16s
epoch 67 | loss: 0.32063 | val_0_rmse: 0.55102 | val_1_rmse: 0.60862 |  0:00:16s
epoch 68 | loss: 0.30624 | val_0_rmse: 0.54948 | val_1_rmse: 0.60397 |  0:00:17s
epoch 69 | loss: 0.29571 | val_0_rmse: 0.53702 | val_1_rmse: 0.58442 |  0:00:17s
epoch 70 | loss: 0.3005  | val_0_rmse: 0.55288 | val_1_rmse: 0.58596 |  0:00:17s
epoch 71 | loss: 0.3124  | val_0_rmse: 0.53646 | val_1_rmse: 0.58479 |  0:00:17s
epoch 72 | loss: 0.30187 | val_0_rmse: 0.54326 | val_1_rmse: 0.59009 |  0:00:18s
epoch 73 | loss: 0.31215 | val_0_rmse: 0.5392  | val_1_rmse: 0.5954  |  0:00:18s
epoch 74 | loss: 0.3087  | val_0_rmse: 0.54631 | val_1_rmse: 0.60857 |  0:00:18s
epoch 75 | loss: 0.30173 | val_0_rmse: 0.54583 | val_1_rmse: 0.59999 |  0:00:18s
epoch 76 | loss: 0.28998 | val_0_rmse: 0.53962 | val_1_rmse: 0.59627 |  0:00:18s
epoch 77 | loss: 0.31054 | val_0_rmse: 0.53101 | val_1_rmse: 0.59244 |  0:00:19s
epoch 78 | loss: 0.28984 | val_0_rmse: 0.54001 | val_1_rmse: 0.58773 |  0:00:19s
epoch 79 | loss: 0.29647 | val_0_rmse: 0.5332  | val_1_rmse: 0.5757  |  0:00:19s
epoch 80 | loss: 0.30487 | val_0_rmse: 0.53282 | val_1_rmse: 0.58922 |  0:00:19s
epoch 81 | loss: 0.29011 | val_0_rmse: 0.54122 | val_1_rmse: 0.5916  |  0:00:20s
epoch 82 | loss: 0.28695 | val_0_rmse: 0.53898 | val_1_rmse: 0.58647 |  0:00:20s
epoch 83 | loss: 0.30254 | val_0_rmse: 0.55941 | val_1_rmse: 0.59595 |  0:00:20s
epoch 84 | loss: 0.31129 | val_0_rmse: 0.56121 | val_1_rmse: 0.60034 |  0:00:20s
epoch 85 | loss: 0.30599 | val_0_rmse: 0.54145 | val_1_rmse: 0.57938 |  0:00:21s
epoch 86 | loss: 0.31194 | val_0_rmse: 0.53883 | val_1_rmse: 0.58929 |  0:00:21s
epoch 87 | loss: 0.29371 | val_0_rmse: 0.55754 | val_1_rmse: 0.60097 |  0:00:21s
epoch 88 | loss: 0.31177 | val_0_rmse: 0.54547 | val_1_rmse: 0.60188 |  0:00:21s
epoch 89 | loss: 0.31264 | val_0_rmse: 0.55116 | val_1_rmse: 0.60471 |  0:00:22s
epoch 90 | loss: 0.30522 | val_0_rmse: 0.56116 | val_1_rmse: 0.62366 |  0:00:22s
epoch 91 | loss: 0.31076 | val_0_rmse: 0.5429  | val_1_rmse: 0.61069 |  0:00:22s
epoch 92 | loss: 0.30244 | val_0_rmse: 0.53924 | val_1_rmse: 0.60261 |  0:00:22s
epoch 93 | loss: 0.30052 | val_0_rmse: 0.58341 | val_1_rmse: 0.62613 |  0:00:23s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.57313
Best weights from best epoch are automatically used!
ended training at: 02:42:21
Feature importance:
[('Area', 0.26607436846865656), ('Baths', 0.1695733180622781), ('Beds', 0.042361400497513704), ('Latitude', 0.13512943262559332), ('Longitude', 0.2263312468722176), ('Month', 0.08483898261862106), ('Year', 0.07569125085511967)]
Mean squared error is of 1476094246.3802154
Mean absolute error:26469.187667815935
MAPE:0.3203796769111976
R2 score:0.5887488606665046
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:42:21
epoch 0  | loss: 1.88886 | val_0_rmse: 1.18053 | val_1_rmse: 1.20093 |  0:00:00s
epoch 1  | loss: 0.85918 | val_0_rmse: 1.23148 | val_1_rmse: 1.29838 |  0:00:00s
epoch 2  | loss: 0.65137 | val_0_rmse: 1.05234 | val_1_rmse: 1.18518 |  0:00:00s
epoch 3  | loss: 0.59768 | val_0_rmse: 1.02253 | val_1_rmse: 1.10183 |  0:00:01s
epoch 4  | loss: 0.51952 | val_0_rmse: 0.90966 | val_1_rmse: 0.90358 |  0:00:01s
epoch 5  | loss: 0.47857 | val_0_rmse: 0.78461 | val_1_rmse: 0.79668 |  0:00:01s
epoch 6  | loss: 0.4577  | val_0_rmse: 0.73861 | val_1_rmse: 0.75112 |  0:00:01s
epoch 7  | loss: 0.44918 | val_0_rmse: 0.67528 | val_1_rmse: 0.68782 |  0:00:02s
epoch 8  | loss: 0.42543 | val_0_rmse: 0.64644 | val_1_rmse: 0.62995 |  0:00:02s
epoch 9  | loss: 0.41791 | val_0_rmse: 0.66665 | val_1_rmse: 0.64903 |  0:00:02s
epoch 10 | loss: 0.39991 | val_0_rmse: 0.64829 | val_1_rmse: 0.6184  |  0:00:02s
epoch 11 | loss: 0.38076 | val_0_rmse: 0.63261 | val_1_rmse: 0.6183  |  0:00:03s
epoch 12 | loss: 0.38281 | val_0_rmse: 0.63374 | val_1_rmse: 0.63526 |  0:00:03s
epoch 13 | loss: 0.37799 | val_0_rmse: 0.62628 | val_1_rmse: 0.64259 |  0:00:03s
epoch 14 | loss: 0.37839 | val_0_rmse: 0.6302  | val_1_rmse: 0.63481 |  0:00:03s
epoch 15 | loss: 0.37713 | val_0_rmse: 0.6358  | val_1_rmse: 0.62533 |  0:00:04s
epoch 16 | loss: 0.37721 | val_0_rmse: 0.6317  | val_1_rmse: 0.62024 |  0:00:04s
epoch 17 | loss: 0.37306 | val_0_rmse: 0.5998  | val_1_rmse: 0.59974 |  0:00:04s
epoch 18 | loss: 0.3546  | val_0_rmse: 0.61021 | val_1_rmse: 0.60359 |  0:00:04s
epoch 19 | loss: 0.37315 | val_0_rmse: 0.60508 | val_1_rmse: 0.59947 |  0:00:05s
epoch 20 | loss: 0.36104 | val_0_rmse: 0.62353 | val_1_rmse: 0.60774 |  0:00:05s
epoch 21 | loss: 0.36353 | val_0_rmse: 0.58941 | val_1_rmse: 0.57655 |  0:00:05s
epoch 22 | loss: 0.37016 | val_0_rmse: 0.57785 | val_1_rmse: 0.5738  |  0:00:05s
epoch 23 | loss: 0.35165 | val_0_rmse: 0.58267 | val_1_rmse: 0.58785 |  0:00:06s
epoch 24 | loss: 0.35798 | val_0_rmse: 0.60303 | val_1_rmse: 0.60313 |  0:00:06s
epoch 25 | loss: 0.34754 | val_0_rmse: 0.60148 | val_1_rmse: 0.59722 |  0:00:06s
epoch 26 | loss: 0.34124 | val_0_rmse: 0.57568 | val_1_rmse: 0.58397 |  0:00:06s
epoch 27 | loss: 0.33258 | val_0_rmse: 0.57064 | val_1_rmse: 0.58289 |  0:00:07s
epoch 28 | loss: 0.33202 | val_0_rmse: 0.57598 | val_1_rmse: 0.58419 |  0:00:07s
epoch 29 | loss: 0.33878 | val_0_rmse: 0.55932 | val_1_rmse: 0.56686 |  0:00:07s
epoch 30 | loss: 0.31806 | val_0_rmse: 0.55924 | val_1_rmse: 0.57571 |  0:00:07s
epoch 31 | loss: 0.32841 | val_0_rmse: 0.57174 | val_1_rmse: 0.57863 |  0:00:08s
epoch 32 | loss: 0.31835 | val_0_rmse: 0.56615 | val_1_rmse: 0.58131 |  0:00:08s
epoch 33 | loss: 0.32264 | val_0_rmse: 0.55115 | val_1_rmse: 0.58276 |  0:00:08s
epoch 34 | loss: 0.31546 | val_0_rmse: 0.55461 | val_1_rmse: 0.5744  |  0:00:08s
epoch 35 | loss: 0.31932 | val_0_rmse: 0.56795 | val_1_rmse: 0.57996 |  0:00:08s
epoch 36 | loss: 0.31211 | val_0_rmse: 0.54468 | val_1_rmse: 0.57411 |  0:00:09s
epoch 37 | loss: 0.31828 | val_0_rmse: 0.54265 | val_1_rmse: 0.57651 |  0:00:09s
epoch 38 | loss: 0.3145  | val_0_rmse: 0.55537 | val_1_rmse: 0.57895 |  0:00:09s
epoch 39 | loss: 0.30652 | val_0_rmse: 0.5408  | val_1_rmse: 0.56115 |  0:00:09s
epoch 40 | loss: 0.30055 | val_0_rmse: 0.53672 | val_1_rmse: 0.56526 |  0:00:10s
epoch 41 | loss: 0.30769 | val_0_rmse: 0.53498 | val_1_rmse: 0.56426 |  0:00:10s
epoch 42 | loss: 0.30541 | val_0_rmse: 0.53923 | val_1_rmse: 0.576   |  0:00:10s
epoch 43 | loss: 0.30971 | val_0_rmse: 0.53542 | val_1_rmse: 0.57478 |  0:00:10s
epoch 44 | loss: 0.3112  | val_0_rmse: 0.5361  | val_1_rmse: 0.55751 |  0:00:11s
epoch 45 | loss: 0.31986 | val_0_rmse: 0.54442 | val_1_rmse: 0.55287 |  0:00:11s
epoch 46 | loss: 0.31106 | val_0_rmse: 0.53932 | val_1_rmse: 0.5554  |  0:00:11s
epoch 47 | loss: 0.30323 | val_0_rmse: 0.5376  | val_1_rmse: 0.5668  |  0:00:11s
epoch 48 | loss: 0.3069  | val_0_rmse: 0.53871 | val_1_rmse: 0.5743  |  0:00:12s
epoch 49 | loss: 0.30307 | val_0_rmse: 0.54199 | val_1_rmse: 0.56749 |  0:00:12s
epoch 50 | loss: 0.30488 | val_0_rmse: 0.54046 | val_1_rmse: 0.55888 |  0:00:12s
epoch 51 | loss: 0.30996 | val_0_rmse: 0.55375 | val_1_rmse: 0.58749 |  0:00:12s
epoch 52 | loss: 0.30517 | val_0_rmse: 0.53346 | val_1_rmse: 0.58223 |  0:00:13s
epoch 53 | loss: 0.30319 | val_0_rmse: 0.54396 | val_1_rmse: 0.59473 |  0:00:13s
epoch 54 | loss: 0.32017 | val_0_rmse: 0.53516 | val_1_rmse: 0.58231 |  0:00:13s
epoch 55 | loss: 0.29926 | val_0_rmse: 0.53429 | val_1_rmse: 0.55274 |  0:00:13s
epoch 56 | loss: 0.3043  | val_0_rmse: 0.54559 | val_1_rmse: 0.55611 |  0:00:14s
epoch 57 | loss: 0.30583 | val_0_rmse: 0.54488 | val_1_rmse: 0.56314 |  0:00:14s
epoch 58 | loss: 0.31027 | val_0_rmse: 0.54286 | val_1_rmse: 0.56423 |  0:00:14s
epoch 59 | loss: 0.3015  | val_0_rmse: 0.5437  | val_1_rmse: 0.57107 |  0:00:14s
epoch 60 | loss: 0.3031  | val_0_rmse: 0.53641 | val_1_rmse: 0.55894 |  0:00:15s
epoch 61 | loss: 0.29323 | val_0_rmse: 0.539   | val_1_rmse: 0.56632 |  0:00:15s
epoch 62 | loss: 0.29454 | val_0_rmse: 0.549   | val_1_rmse: 0.59136 |  0:00:15s
epoch 63 | loss: 0.30944 | val_0_rmse: 0.54445 | val_1_rmse: 0.59009 |  0:00:15s
epoch 64 | loss: 0.3017  | val_0_rmse: 0.55268 | val_1_rmse: 0.569   |  0:00:16s
epoch 65 | loss: 0.3147  | val_0_rmse: 0.54955 | val_1_rmse: 0.55154 |  0:00:16s
epoch 66 | loss: 0.31468 | val_0_rmse: 0.5458  | val_1_rmse: 0.55078 |  0:00:16s
epoch 67 | loss: 0.31945 | val_0_rmse: 0.55369 | val_1_rmse: 0.55397 |  0:00:16s
epoch 68 | loss: 0.33253 | val_0_rmse: 0.55502 | val_1_rmse: 0.55739 |  0:00:17s
epoch 69 | loss: 0.32357 | val_0_rmse: 0.54447 | val_1_rmse: 0.55495 |  0:00:17s
epoch 70 | loss: 0.31145 | val_0_rmse: 0.55648 | val_1_rmse: 0.56901 |  0:00:17s
epoch 71 | loss: 0.3102  | val_0_rmse: 0.56303 | val_1_rmse: 0.57494 |  0:00:17s
epoch 72 | loss: 0.29833 | val_0_rmse: 0.55182 | val_1_rmse: 0.57395 |  0:00:18s
epoch 73 | loss: 0.31485 | val_0_rmse: 0.55666 | val_1_rmse: 0.57641 |  0:00:18s
epoch 74 | loss: 0.31995 | val_0_rmse: 0.60115 | val_1_rmse: 0.60559 |  0:00:18s
epoch 75 | loss: 0.32973 | val_0_rmse: 0.58568 | val_1_rmse: 0.59005 |  0:00:18s
epoch 76 | loss: 0.34144 | val_0_rmse: 0.56984 | val_1_rmse: 0.57072 |  0:00:19s
epoch 77 | loss: 0.32321 | val_0_rmse: 0.5709  | val_1_rmse: 0.56284 |  0:00:19s
epoch 78 | loss: 0.32846 | val_0_rmse: 0.56443 | val_1_rmse: 0.55611 |  0:00:19s
epoch 79 | loss: 0.33534 | val_0_rmse: 0.56503 | val_1_rmse: 0.56306 |  0:00:19s
epoch 80 | loss: 0.33104 | val_0_rmse: 0.56891 | val_1_rmse: 0.58249 |  0:00:19s
epoch 81 | loss: 0.3267  | val_0_rmse: 0.59155 | val_1_rmse: 0.60421 |  0:00:20s
epoch 82 | loss: 0.34277 | val_0_rmse: 0.56972 | val_1_rmse: 0.57093 |  0:00:20s
epoch 83 | loss: 0.34321 | val_0_rmse: 0.59429 | val_1_rmse: 0.59025 |  0:00:20s
epoch 84 | loss: 0.35682 | val_0_rmse: 0.59702 | val_1_rmse: 0.58792 |  0:00:20s
epoch 85 | loss: 0.35595 | val_0_rmse: 0.59399 | val_1_rmse: 0.58073 |  0:00:21s
epoch 86 | loss: 0.34977 | val_0_rmse: 0.60162 | val_1_rmse: 0.58769 |  0:00:21s
epoch 87 | loss: 0.34702 | val_0_rmse: 0.58555 | val_1_rmse: 0.56962 |  0:00:21s
epoch 88 | loss: 0.34019 | val_0_rmse: 0.57934 | val_1_rmse: 0.56864 |  0:00:21s
epoch 89 | loss: 0.34452 | val_0_rmse: 0.56459 | val_1_rmse: 0.55021 |  0:00:22s
epoch 90 | loss: 0.33785 | val_0_rmse: 0.56348 | val_1_rmse: 0.55188 |  0:00:22s
epoch 91 | loss: 0.32226 | val_0_rmse: 0.55853 | val_1_rmse: 0.56134 |  0:00:22s
epoch 92 | loss: 0.3176  | val_0_rmse: 0.5664  | val_1_rmse: 0.58062 |  0:00:22s
epoch 93 | loss: 0.32114 | val_0_rmse: 0.56403 | val_1_rmse: 0.57437 |  0:00:23s
epoch 94 | loss: 0.317   | val_0_rmse: 0.55437 | val_1_rmse: 0.5565  |  0:00:23s
epoch 95 | loss: 0.3175  | val_0_rmse: 0.54879 | val_1_rmse: 0.54842 |  0:00:23s
epoch 96 | loss: 0.31493 | val_0_rmse: 0.54413 | val_1_rmse: 0.54386 |  0:00:23s
epoch 97 | loss: 0.30857 | val_0_rmse: 0.54149 | val_1_rmse: 0.54579 |  0:00:24s
epoch 98 | loss: 0.30948 | val_0_rmse: 0.54163 | val_1_rmse: 0.55458 |  0:00:24s
epoch 99 | loss: 0.31574 | val_0_rmse: 0.53889 | val_1_rmse: 0.55446 |  0:00:24s
epoch 100| loss: 0.32564 | val_0_rmse: 0.53915 | val_1_rmse: 0.54702 |  0:00:24s
epoch 101| loss: 0.30348 | val_0_rmse: 0.54059 | val_1_rmse: 0.54694 |  0:00:25s
epoch 102| loss: 0.30476 | val_0_rmse: 0.54201 | val_1_rmse: 0.54689 |  0:00:25s
epoch 103| loss: 0.29517 | val_0_rmse: 0.53543 | val_1_rmse: 0.54616 |  0:00:25s
epoch 104| loss: 0.30834 | val_0_rmse: 0.5312  | val_1_rmse: 0.54901 |  0:00:25s
epoch 105| loss: 0.30619 | val_0_rmse: 0.53933 | val_1_rmse: 0.55506 |  0:00:26s
epoch 106| loss: 0.30495 | val_0_rmse: 0.53172 | val_1_rmse: 0.55089 |  0:00:26s
epoch 107| loss: 0.30371 | val_0_rmse: 0.53161 | val_1_rmse: 0.55704 |  0:00:26s
epoch 108| loss: 0.28998 | val_0_rmse: 0.53166 | val_1_rmse: 0.5509  |  0:00:26s
epoch 109| loss: 0.28463 | val_0_rmse: 0.52822 | val_1_rmse: 0.54721 |  0:00:27s
epoch 110| loss: 0.28317 | val_0_rmse: 0.53281 | val_1_rmse: 0.55824 |  0:00:27s
epoch 111| loss: 0.2855  | val_0_rmse: 0.53227 | val_1_rmse: 0.56571 |  0:00:27s
epoch 112| loss: 0.29803 | val_0_rmse: 0.53237 | val_1_rmse: 0.55166 |  0:00:27s
epoch 113| loss: 0.29058 | val_0_rmse: 0.52508 | val_1_rmse: 0.54577 |  0:00:28s
epoch 114| loss: 0.28075 | val_0_rmse: 0.53086 | val_1_rmse: 0.55728 |  0:00:28s
epoch 115| loss: 0.29566 | val_0_rmse: 0.5251  | val_1_rmse: 0.54766 |  0:00:28s
epoch 116| loss: 0.28116 | val_0_rmse: 0.52352 | val_1_rmse: 0.54346 |  0:00:28s
epoch 117| loss: 0.28472 | val_0_rmse: 0.51755 | val_1_rmse: 0.54417 |  0:00:29s
epoch 118| loss: 0.27918 | val_0_rmse: 0.51869 | val_1_rmse: 0.55185 |  0:00:29s
epoch 119| loss: 0.27861 | val_0_rmse: 0.51603 | val_1_rmse: 0.55392 |  0:00:29s
epoch 120| loss: 0.27428 | val_0_rmse: 0.51711 | val_1_rmse: 0.55511 |  0:00:29s
epoch 121| loss: 0.27503 | val_0_rmse: 0.5188  | val_1_rmse: 0.55193 |  0:00:30s
epoch 122| loss: 0.28074 | val_0_rmse: 0.51903 | val_1_rmse: 0.55145 |  0:00:30s
epoch 123| loss: 0.27762 | val_0_rmse: 0.51617 | val_1_rmse: 0.54741 |  0:00:30s
epoch 124| loss: 0.27823 | val_0_rmse: 0.52025 | val_1_rmse: 0.55255 |  0:00:30s
epoch 125| loss: 0.27496 | val_0_rmse: 0.52001 | val_1_rmse: 0.54962 |  0:00:30s
epoch 126| loss: 0.27813 | val_0_rmse: 0.51491 | val_1_rmse: 0.54742 |  0:00:31s
epoch 127| loss: 0.27712 | val_0_rmse: 0.50926 | val_1_rmse: 0.54332 |  0:00:31s
epoch 128| loss: 0.26858 | val_0_rmse: 0.51096 | val_1_rmse: 0.54997 |  0:00:31s
epoch 129| loss: 0.27758 | val_0_rmse: 0.50755 | val_1_rmse: 0.55655 |  0:00:31s
epoch 130| loss: 0.27464 | val_0_rmse: 0.51537 | val_1_rmse: 0.57019 |  0:00:32s
epoch 131| loss: 0.2953  | val_0_rmse: 0.50476 | val_1_rmse: 0.56022 |  0:00:32s
epoch 132| loss: 0.27163 | val_0_rmse: 0.50585 | val_1_rmse: 0.55737 |  0:00:32s
epoch 133| loss: 0.26767 | val_0_rmse: 0.51008 | val_1_rmse: 0.54021 |  0:00:32s
epoch 134| loss: 0.27171 | val_0_rmse: 0.50941 | val_1_rmse: 0.5401  |  0:00:33s
epoch 135| loss: 0.27904 | val_0_rmse: 0.50549 | val_1_rmse: 0.55031 |  0:00:33s
epoch 136| loss: 0.27364 | val_0_rmse: 0.51099 | val_1_rmse: 0.56339 |  0:00:33s
epoch 137| loss: 0.27    | val_0_rmse: 0.50843 | val_1_rmse: 0.55564 |  0:00:33s
epoch 138| loss: 0.26556 | val_0_rmse: 0.5011  | val_1_rmse: 0.54334 |  0:00:34s
epoch 139| loss: 0.27241 | val_0_rmse: 0.50066 | val_1_rmse: 0.54579 |  0:00:34s
epoch 140| loss: 0.27411 | val_0_rmse: 0.49955 | val_1_rmse: 0.54272 |  0:00:34s
epoch 141| loss: 0.27006 | val_0_rmse: 0.50496 | val_1_rmse: 0.54334 |  0:00:34s
epoch 142| loss: 0.28174 | val_0_rmse: 0.51428 | val_1_rmse: 0.55823 |  0:00:35s
epoch 143| loss: 0.27302 | val_0_rmse: 0.54035 | val_1_rmse: 0.59372 |  0:00:35s
epoch 144| loss: 0.30446 | val_0_rmse: 0.52304 | val_1_rmse: 0.56829 |  0:00:35s
epoch 145| loss: 0.28042 | val_0_rmse: 0.52254 | val_1_rmse: 0.56121 |  0:00:35s
epoch 146| loss: 0.2929  | val_0_rmse: 0.52898 | val_1_rmse: 0.5657  |  0:00:36s
epoch 147| loss: 0.29136 | val_0_rmse: 0.52651 | val_1_rmse: 0.55127 |  0:00:36s
epoch 148| loss: 0.28608 | val_0_rmse: 0.51789 | val_1_rmse: 0.55555 |  0:00:36s
epoch 149| loss: 0.27819 | val_0_rmse: 0.51411 | val_1_rmse: 0.56176 |  0:00:36s
Stop training because you reached max_epochs = 150 with best_epoch = 134 and best_val_1_rmse = 0.5401
Best weights from best epoch are automatically used!
ended training at: 02:42:58
Feature importance:
[('Area', 0.30145697526032605), ('Baths', 0.05546187171578087), ('Beds', 0.10540565181973792), ('Latitude', 0.08209961853952047), ('Longitude', 0.30586197791925684), ('Month', 0.10109818270808134), ('Year', 0.048615722037296465)]
Mean squared error is of 1233161042.1717055
Mean absolute error:25131.5806575
MAPE:0.34951013807392795
R2 score:0.6539750425501696
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:42:58
epoch 0  | loss: 1.99406 | val_0_rmse: 1.33185 | val_1_rmse: 1.33595 |  0:00:00s
epoch 1  | loss: 0.82176 | val_0_rmse: 1.13227 | val_1_rmse: 1.06723 |  0:00:00s
epoch 2  | loss: 0.62765 | val_0_rmse: 1.16069 | val_1_rmse: 1.11101 |  0:00:00s
epoch 3  | loss: 0.58287 | val_0_rmse: 1.04705 | val_1_rmse: 0.99677 |  0:00:00s
epoch 4  | loss: 0.53444 | val_0_rmse: 1.02369 | val_1_rmse: 0.93575 |  0:00:01s
epoch 5  | loss: 0.47945 | val_0_rmse: 0.88487 | val_1_rmse: 0.82589 |  0:00:01s
epoch 6  | loss: 0.45746 | val_0_rmse: 0.78959 | val_1_rmse: 0.77342 |  0:00:01s
epoch 7  | loss: 0.43168 | val_0_rmse: 0.75212 | val_1_rmse: 0.74064 |  0:00:02s
epoch 8  | loss: 0.42147 | val_0_rmse: 0.73038 | val_1_rmse: 0.71441 |  0:00:02s
epoch 9  | loss: 0.4205  | val_0_rmse: 0.68108 | val_1_rmse: 0.65738 |  0:00:02s
epoch 10 | loss: 0.41821 | val_0_rmse: 0.67496 | val_1_rmse: 0.66785 |  0:00:02s
epoch 11 | loss: 0.4042  | val_0_rmse: 0.6509  | val_1_rmse: 0.6575  |  0:00:03s
epoch 12 | loss: 0.40163 | val_0_rmse: 0.65308 | val_1_rmse: 0.64928 |  0:00:03s
epoch 13 | loss: 0.39093 | val_0_rmse: 0.62448 | val_1_rmse: 0.59935 |  0:00:03s
epoch 14 | loss: 0.37435 | val_0_rmse: 0.62795 | val_1_rmse: 0.61922 |  0:00:03s
epoch 15 | loss: 0.37641 | val_0_rmse: 0.65013 | val_1_rmse: 0.66256 |  0:00:04s
epoch 16 | loss: 0.36732 | val_0_rmse: 0.6038  | val_1_rmse: 0.59626 |  0:00:04s
epoch 17 | loss: 0.38171 | val_0_rmse: 0.597   | val_1_rmse: 0.5841  |  0:00:04s
epoch 18 | loss: 0.37259 | val_0_rmse: 0.62149 | val_1_rmse: 0.64266 |  0:00:04s
epoch 19 | loss: 0.36961 | val_0_rmse: 0.61434 | val_1_rmse: 0.63336 |  0:00:04s
epoch 20 | loss: 0.35594 | val_0_rmse: 0.61085 | val_1_rmse: 0.64123 |  0:00:05s
epoch 21 | loss: 0.3627  | val_0_rmse: 0.60746 | val_1_rmse: 0.63566 |  0:00:05s
epoch 22 | loss: 0.34842 | val_0_rmse: 0.59374 | val_1_rmse: 0.61833 |  0:00:05s
epoch 23 | loss: 0.34903 | val_0_rmse: 0.58821 | val_1_rmse: 0.60901 |  0:00:05s
epoch 24 | loss: 0.3555  | val_0_rmse: 0.5858  | val_1_rmse: 0.61602 |  0:00:06s
epoch 25 | loss: 0.33339 | val_0_rmse: 0.58186 | val_1_rmse: 0.60611 |  0:00:06s
epoch 26 | loss: 0.33966 | val_0_rmse: 0.57758 | val_1_rmse: 0.57896 |  0:00:06s
epoch 27 | loss: 0.33655 | val_0_rmse: 0.58439 | val_1_rmse: 0.57492 |  0:00:06s
epoch 28 | loss: 0.34445 | val_0_rmse: 0.56969 | val_1_rmse: 0.57998 |  0:00:07s
epoch 29 | loss: 0.33166 | val_0_rmse: 0.57071 | val_1_rmse: 0.59489 |  0:00:07s
epoch 30 | loss: 0.34362 | val_0_rmse: 0.58043 | val_1_rmse: 0.5953  |  0:00:07s
epoch 31 | loss: 0.35198 | val_0_rmse: 0.57682 | val_1_rmse: 0.60215 |  0:00:07s
epoch 32 | loss: 0.36562 | val_0_rmse: 0.59821 | val_1_rmse: 0.6047  |  0:00:08s
epoch 33 | loss: 0.36746 | val_0_rmse: 0.61129 | val_1_rmse: 0.61373 |  0:00:08s
epoch 34 | loss: 0.3818  | val_0_rmse: 0.61684 | val_1_rmse: 0.61094 |  0:00:08s
epoch 35 | loss: 0.36798 | val_0_rmse: 0.60039 | val_1_rmse: 0.59338 |  0:00:08s
epoch 36 | loss: 0.36227 | val_0_rmse: 0.60357 | val_1_rmse: 0.59808 |  0:00:09s
epoch 37 | loss: 0.37688 | val_0_rmse: 0.60651 | val_1_rmse: 0.60681 |  0:00:09s
epoch 38 | loss: 0.37602 | val_0_rmse: 0.60385 | val_1_rmse: 0.63845 |  0:00:09s
epoch 39 | loss: 0.3682  | val_0_rmse: 0.59181 | val_1_rmse: 0.6089  |  0:00:09s
epoch 40 | loss: 0.35958 | val_0_rmse: 0.59507 | val_1_rmse: 0.59726 |  0:00:10s
epoch 41 | loss: 0.35739 | val_0_rmse: 0.62331 | val_1_rmse: 0.64203 |  0:00:10s
epoch 42 | loss: 0.35334 | val_0_rmse: 0.6123  | val_1_rmse: 0.64544 |  0:00:10s
epoch 43 | loss: 0.35073 | val_0_rmse: 0.60916 | val_1_rmse: 0.66439 |  0:00:10s
epoch 44 | loss: 0.34032 | val_0_rmse: 0.64504 | val_1_rmse: 0.72955 |  0:00:11s
epoch 45 | loss: 0.3383  | val_0_rmse: 0.64805 | val_1_rmse: 0.73017 |  0:00:11s
epoch 46 | loss: 0.32845 | val_0_rmse: 0.60982 | val_1_rmse: 0.66762 |  0:00:11s
epoch 47 | loss: 0.33483 | val_0_rmse: 0.59382 | val_1_rmse: 0.64625 |  0:00:11s
epoch 48 | loss: 0.32816 | val_0_rmse: 0.58731 | val_1_rmse: 0.64111 |  0:00:12s
epoch 49 | loss: 0.33235 | val_0_rmse: 0.59834 | val_1_rmse: 0.6299  |  0:00:12s
epoch 50 | loss: 0.33574 | val_0_rmse: 0.60479 | val_1_rmse: 0.61497 |  0:00:12s
epoch 51 | loss: 0.33479 | val_0_rmse: 0.5742  | val_1_rmse: 0.58734 |  0:00:12s
epoch 52 | loss: 0.33912 | val_0_rmse: 0.56294 | val_1_rmse: 0.5866  |  0:00:13s
epoch 53 | loss: 0.3111  | val_0_rmse: 0.55741 | val_1_rmse: 0.59425 |  0:00:13s
epoch 54 | loss: 0.32455 | val_0_rmse: 0.55232 | val_1_rmse: 0.59214 |  0:00:13s
epoch 55 | loss: 0.30239 | val_0_rmse: 0.5515  | val_1_rmse: 0.5839  |  0:00:13s
epoch 56 | loss: 0.29993 | val_0_rmse: 0.55056 | val_1_rmse: 0.57105 |  0:00:13s
epoch 57 | loss: 0.30306 | val_0_rmse: 0.54184 | val_1_rmse: 0.57521 |  0:00:14s
epoch 58 | loss: 0.31733 | val_0_rmse: 0.5622  | val_1_rmse: 0.61857 |  0:00:14s
epoch 59 | loss: 0.30949 | val_0_rmse: 0.57404 | val_1_rmse: 0.64175 |  0:00:14s
epoch 60 | loss: 0.2965  | val_0_rmse: 0.60332 | val_1_rmse: 0.67951 |  0:00:14s
epoch 61 | loss: 0.31908 | val_0_rmse: 0.59091 | val_1_rmse: 0.65782 |  0:00:15s
epoch 62 | loss: 0.30326 | val_0_rmse: 0.53604 | val_1_rmse: 0.57712 |  0:00:15s
epoch 63 | loss: 0.318   | val_0_rmse: 0.54875 | val_1_rmse: 0.58552 |  0:00:15s
epoch 64 | loss: 0.30743 | val_0_rmse: 0.54193 | val_1_rmse: 0.59341 |  0:00:15s
epoch 65 | loss: 0.30975 | val_0_rmse: 0.56921 | val_1_rmse: 0.6359  |  0:00:16s
epoch 66 | loss: 0.31288 | val_0_rmse: 0.59461 | val_1_rmse: 0.6599  |  0:00:16s
epoch 67 | loss: 0.32919 | val_0_rmse: 0.5407  | val_1_rmse: 0.58544 |  0:00:16s
epoch 68 | loss: 0.29841 | val_0_rmse: 0.57122 | val_1_rmse: 0.58782 |  0:00:16s
epoch 69 | loss: 0.31248 | val_0_rmse: 0.54825 | val_1_rmse: 0.56788 |  0:00:17s
epoch 70 | loss: 0.30625 | val_0_rmse: 0.53063 | val_1_rmse: 0.56494 |  0:00:17s
epoch 71 | loss: 0.3077  | val_0_rmse: 0.55255 | val_1_rmse: 0.59222 |  0:00:17s
epoch 72 | loss: 0.2985  | val_0_rmse: 0.53831 | val_1_rmse: 0.57872 |  0:00:17s
epoch 73 | loss: 0.29234 | val_0_rmse: 0.53319 | val_1_rmse: 0.56987 |  0:00:18s
epoch 74 | loss: 0.30023 | val_0_rmse: 0.53331 | val_1_rmse: 0.56347 |  0:00:18s
epoch 75 | loss: 0.30317 | val_0_rmse: 0.55356 | val_1_rmse: 0.61334 |  0:00:18s
epoch 76 | loss: 0.29854 | val_0_rmse: 0.60392 | val_1_rmse: 0.6887  |  0:00:18s
epoch 77 | loss: 0.29819 | val_0_rmse: 0.58768 | val_1_rmse: 0.66479 |  0:00:19s
epoch 78 | loss: 0.32399 | val_0_rmse: 0.56254 | val_1_rmse: 0.63103 |  0:00:19s
epoch 79 | loss: 0.29658 | val_0_rmse: 0.55665 | val_1_rmse: 0.61202 |  0:00:19s
epoch 80 | loss: 0.32169 | val_0_rmse: 0.53675 | val_1_rmse: 0.58037 |  0:00:19s
epoch 81 | loss: 0.30623 | val_0_rmse: 0.52531 | val_1_rmse: 0.57021 |  0:00:19s
epoch 82 | loss: 0.2846  | val_0_rmse: 0.56227 | val_1_rmse: 0.6214  |  0:00:20s
epoch 83 | loss: 0.29632 | val_0_rmse: 0.53916 | val_1_rmse: 0.59924 |  0:00:20s
epoch 84 | loss: 0.29664 | val_0_rmse: 0.51491 | val_1_rmse: 0.5637  |  0:00:20s
epoch 85 | loss: 0.28176 | val_0_rmse: 0.5309  | val_1_rmse: 0.59323 |  0:00:20s
epoch 86 | loss: 0.2809  | val_0_rmse: 0.56115 | val_1_rmse: 0.64824 |  0:00:21s
epoch 87 | loss: 0.27423 | val_0_rmse: 0.61153 | val_1_rmse: 0.71113 |  0:00:21s
epoch 88 | loss: 0.28001 | val_0_rmse: 0.57842 | val_1_rmse: 0.67762 |  0:00:21s
epoch 89 | loss: 0.28735 | val_0_rmse: 0.51423 | val_1_rmse: 0.5929  |  0:00:22s
epoch 90 | loss: 0.28051 | val_0_rmse: 0.52293 | val_1_rmse: 0.57421 |  0:00:22s
epoch 91 | loss: 0.2844  | val_0_rmse: 0.51197 | val_1_rmse: 0.57512 |  0:00:22s
epoch 92 | loss: 0.27506 | val_0_rmse: 0.54881 | val_1_rmse: 0.63112 |  0:00:22s
epoch 93 | loss: 0.2815  | val_0_rmse: 0.52063 | val_1_rmse: 0.60541 |  0:00:22s
epoch 94 | loss: 0.27856 | val_0_rmse: 0.51617 | val_1_rmse: 0.56659 |  0:00:23s
epoch 95 | loss: 0.27904 | val_0_rmse: 0.51082 | val_1_rmse: 0.56411 |  0:00:23s
epoch 96 | loss: 0.27691 | val_0_rmse: 0.5048  | val_1_rmse: 0.56814 |  0:00:23s
epoch 97 | loss: 0.27659 | val_0_rmse: 0.50133 | val_1_rmse: 0.56395 |  0:00:23s
epoch 98 | loss: 0.26627 | val_0_rmse: 0.51601 | val_1_rmse: 0.56019 |  0:00:24s
epoch 99 | loss: 0.27173 | val_0_rmse: 0.51208 | val_1_rmse: 0.55034 |  0:00:24s
epoch 100| loss: 0.27488 | val_0_rmse: 0.51379 | val_1_rmse: 0.54338 |  0:00:24s
epoch 101| loss: 0.27786 | val_0_rmse: 0.50882 | val_1_rmse: 0.55106 |  0:00:24s
epoch 102| loss: 0.27708 | val_0_rmse: 0.51197 | val_1_rmse: 0.55281 |  0:00:25s
epoch 103| loss: 0.26491 | val_0_rmse: 0.52929 | val_1_rmse: 0.56154 |  0:00:25s
epoch 104| loss: 0.28868 | val_0_rmse: 0.53234 | val_1_rmse: 0.5748  |  0:00:25s
epoch 105| loss: 0.27404 | val_0_rmse: 0.51615 | val_1_rmse: 0.58753 |  0:00:25s
epoch 106| loss: 0.28005 | val_0_rmse: 0.5099  | val_1_rmse: 0.58399 |  0:00:26s
epoch 107| loss: 0.27557 | val_0_rmse: 0.50711 | val_1_rmse: 0.56972 |  0:00:26s
epoch 108| loss: 0.27223 | val_0_rmse: 0.51178 | val_1_rmse: 0.58755 |  0:00:26s
epoch 109| loss: 0.28227 | val_0_rmse: 0.53284 | val_1_rmse: 0.63068 |  0:00:26s
epoch 110| loss: 0.28434 | val_0_rmse: 0.53378 | val_1_rmse: 0.62962 |  0:00:27s
epoch 111| loss: 0.27209 | val_0_rmse: 0.50399 | val_1_rmse: 0.57944 |  0:00:27s
epoch 112| loss: 0.27036 | val_0_rmse: 0.54304 | val_1_rmse: 0.62792 |  0:00:27s
epoch 113| loss: 0.27922 | val_0_rmse: 0.55897 | val_1_rmse: 0.65259 |  0:00:27s
epoch 114| loss: 0.27829 | val_0_rmse: 0.50161 | val_1_rmse: 0.55965 |  0:00:28s
epoch 115| loss: 0.2731  | val_0_rmse: 0.5237  | val_1_rmse: 0.56969 |  0:00:28s
epoch 116| loss: 0.29225 | val_0_rmse: 0.5341  | val_1_rmse: 0.61261 |  0:00:28s
epoch 117| loss: 0.26593 | val_0_rmse: 0.55393 | val_1_rmse: 0.6452  |  0:00:28s
epoch 118| loss: 0.26878 | val_0_rmse: 0.56289 | val_1_rmse: 0.65124 |  0:00:29s
epoch 119| loss: 0.27533 | val_0_rmse: 0.53979 | val_1_rmse: 0.61078 |  0:00:29s
epoch 120| loss: 0.26853 | val_0_rmse: 0.57298 | val_1_rmse: 0.66006 |  0:00:29s
epoch 121| loss: 0.25925 | val_0_rmse: 0.55367 | val_1_rmse: 0.64782 |  0:00:29s
epoch 122| loss: 0.27639 | val_0_rmse: 0.50335 | val_1_rmse: 0.58473 |  0:00:30s
epoch 123| loss: 0.27637 | val_0_rmse: 0.50501 | val_1_rmse: 0.55647 |  0:00:30s
epoch 124| loss: 0.27499 | val_0_rmse: 0.51334 | val_1_rmse: 0.57244 |  0:00:30s
epoch 125| loss: 0.25999 | val_0_rmse: 0.50617 | val_1_rmse: 0.59005 |  0:00:30s
epoch 126| loss: 0.26198 | val_0_rmse: 0.50527 | val_1_rmse: 0.59636 |  0:00:31s
epoch 127| loss: 0.26722 | val_0_rmse: 0.48989 | val_1_rmse: 0.57911 |  0:00:31s
epoch 128| loss: 0.25482 | val_0_rmse: 0.50906 | val_1_rmse: 0.61286 |  0:00:31s
epoch 129| loss: 0.27021 | val_0_rmse: 0.52497 | val_1_rmse: 0.63708 |  0:00:31s
epoch 130| loss: 0.2568  | val_0_rmse: 0.50269 | val_1_rmse: 0.60089 |  0:00:31s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.54338
Best weights from best epoch are automatically used!
ended training at: 02:43:30
Feature importance:
[('Area', 0.30045089776354134), ('Baths', 0.2253663453031139), ('Beds', 0.03651952862406396), ('Latitude', 0.20882130359712348), ('Longitude', 0.16943306389460347), ('Month', 0.03205312913971514), ('Year', 0.027355731677838713)]
Mean squared error is of 1035776884.7970606
Mean absolute error:21562.988734546703
MAPE:0.26807452563631773
R2 score:0.7142904544329405
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:43:30
epoch 0  | loss: 1.91851 | val_0_rmse: 1.03843 | val_1_rmse: 0.97714 |  0:00:00s
epoch 1  | loss: 0.7741  | val_0_rmse: 1.09312 | val_1_rmse: 1.09632 |  0:00:00s
epoch 2  | loss: 0.61973 | val_0_rmse: 0.87879 | val_1_rmse: 0.83594 |  0:00:00s
epoch 3  | loss: 0.54217 | val_0_rmse: 0.87383 | val_1_rmse: 0.8313  |  0:00:01s
epoch 4  | loss: 0.49164 | val_0_rmse: 0.77726 | val_1_rmse: 0.75165 |  0:00:01s
epoch 5  | loss: 0.48178 | val_0_rmse: 0.75532 | val_1_rmse: 0.73538 |  0:00:01s
epoch 6  | loss: 0.45577 | val_0_rmse: 0.77111 | val_1_rmse: 0.7359  |  0:00:01s
epoch 7  | loss: 0.44537 | val_0_rmse: 0.69182 | val_1_rmse: 0.63967 |  0:00:02s
epoch 8  | loss: 0.44122 | val_0_rmse: 0.71283 | val_1_rmse: 0.67929 |  0:00:02s
epoch 9  | loss: 0.42846 | val_0_rmse: 0.74486 | val_1_rmse: 0.72489 |  0:00:02s
epoch 10 | loss: 0.41551 | val_0_rmse: 0.65534 | val_1_rmse: 0.64055 |  0:00:02s
epoch 11 | loss: 0.39591 | val_0_rmse: 0.63708 | val_1_rmse: 0.62524 |  0:00:03s
epoch 12 | loss: 0.39637 | val_0_rmse: 0.63178 | val_1_rmse: 0.62028 |  0:00:03s
epoch 13 | loss: 0.39543 | val_0_rmse: 0.64609 | val_1_rmse: 0.59954 |  0:00:03s
epoch 14 | loss: 0.39283 | val_0_rmse: 0.63712 | val_1_rmse: 0.59039 |  0:00:03s
epoch 15 | loss: 0.38492 | val_0_rmse: 0.61249 | val_1_rmse: 0.58364 |  0:00:04s
epoch 16 | loss: 0.38467 | val_0_rmse: 0.60649 | val_1_rmse: 0.57599 |  0:00:04s
epoch 17 | loss: 0.37174 | val_0_rmse: 0.60094 | val_1_rmse: 0.56896 |  0:00:04s
epoch 18 | loss: 0.36844 | val_0_rmse: 0.60351 | val_1_rmse: 0.58508 |  0:00:04s
epoch 19 | loss: 0.36656 | val_0_rmse: 0.60836 | val_1_rmse: 0.60141 |  0:00:05s
epoch 20 | loss: 0.36941 | val_0_rmse: 0.60592 | val_1_rmse: 0.60586 |  0:00:05s
epoch 21 | loss: 0.36848 | val_0_rmse: 0.5876  | val_1_rmse: 0.6048  |  0:00:05s
epoch 22 | loss: 0.35004 | val_0_rmse: 0.58286 | val_1_rmse: 0.6052  |  0:00:05s
epoch 23 | loss: 0.36321 | val_0_rmse: 0.57915 | val_1_rmse: 0.60135 |  0:00:06s
epoch 24 | loss: 0.34403 | val_0_rmse: 0.57816 | val_1_rmse: 0.60087 |  0:00:06s
epoch 25 | loss: 0.34836 | val_0_rmse: 0.57952 | val_1_rmse: 0.61668 |  0:00:06s
epoch 26 | loss: 0.33935 | val_0_rmse: 0.57967 | val_1_rmse: 0.61161 |  0:00:06s
epoch 27 | loss: 0.34622 | val_0_rmse: 0.5701  | val_1_rmse: 0.59039 |  0:00:07s
epoch 28 | loss: 0.33313 | val_0_rmse: 0.57352 | val_1_rmse: 0.58213 |  0:00:07s
epoch 29 | loss: 0.34188 | val_0_rmse: 0.57375 | val_1_rmse: 0.57308 |  0:00:07s
epoch 30 | loss: 0.33894 | val_0_rmse: 0.57281 | val_1_rmse: 0.56641 |  0:00:07s
epoch 31 | loss: 0.32889 | val_0_rmse: 0.57519 | val_1_rmse: 0.56329 |  0:00:08s
epoch 32 | loss: 0.32174 | val_0_rmse: 0.5771  | val_1_rmse: 0.57097 |  0:00:08s
epoch 33 | loss: 0.33674 | val_0_rmse: 0.56331 | val_1_rmse: 0.5491  |  0:00:08s
epoch 34 | loss: 0.32322 | val_0_rmse: 0.564   | val_1_rmse: 0.54197 |  0:00:08s
epoch 35 | loss: 0.32788 | val_0_rmse: 0.59512 | val_1_rmse: 0.59002 |  0:00:09s
epoch 36 | loss: 0.34686 | val_0_rmse: 0.58095 | val_1_rmse: 0.57736 |  0:00:09s
epoch 37 | loss: 0.3359  | val_0_rmse: 0.5777  | val_1_rmse: 0.56626 |  0:00:09s
epoch 38 | loss: 0.35459 | val_0_rmse: 0.58132 | val_1_rmse: 0.56463 |  0:00:09s
epoch 39 | loss: 0.34427 | val_0_rmse: 0.5905  | val_1_rmse: 0.58546 |  0:00:09s
epoch 40 | loss: 0.33795 | val_0_rmse: 0.56753 | val_1_rmse: 0.56873 |  0:00:10s
epoch 41 | loss: 0.33156 | val_0_rmse: 0.57551 | val_1_rmse: 0.56247 |  0:00:10s
epoch 42 | loss: 0.33113 | val_0_rmse: 0.60325 | val_1_rmse: 0.58359 |  0:00:10s
epoch 43 | loss: 0.35267 | val_0_rmse: 0.58457 | val_1_rmse: 0.55979 |  0:00:10s
epoch 44 | loss: 0.35149 | val_0_rmse: 0.5747  | val_1_rmse: 0.56019 |  0:00:11s
epoch 45 | loss: 0.33465 | val_0_rmse: 0.56452 | val_1_rmse: 0.55762 |  0:00:11s
epoch 46 | loss: 0.33192 | val_0_rmse: 0.57304 | val_1_rmse: 0.58044 |  0:00:11s
epoch 47 | loss: 0.34539 | val_0_rmse: 0.56995 | val_1_rmse: 0.57157 |  0:00:11s
epoch 48 | loss: 0.33109 | val_0_rmse: 0.57201 | val_1_rmse: 0.5819  |  0:00:12s
epoch 49 | loss: 0.33036 | val_0_rmse: 0.55682 | val_1_rmse: 0.56199 |  0:00:12s
epoch 50 | loss: 0.31805 | val_0_rmse: 0.55773 | val_1_rmse: 0.56412 |  0:00:12s
epoch 51 | loss: 0.31993 | val_0_rmse: 0.54885 | val_1_rmse: 0.55355 |  0:00:12s
epoch 52 | loss: 0.30617 | val_0_rmse: 0.54633 | val_1_rmse: 0.54678 |  0:00:13s
epoch 53 | loss: 0.30779 | val_0_rmse: 0.55007 | val_1_rmse: 0.53638 |  0:00:13s
epoch 54 | loss: 0.31359 | val_0_rmse: 0.5461  | val_1_rmse: 0.5321  |  0:00:13s
epoch 55 | loss: 0.31666 | val_0_rmse: 0.54071 | val_1_rmse: 0.52598 |  0:00:13s
epoch 56 | loss: 0.29799 | val_0_rmse: 0.53769 | val_1_rmse: 0.52471 |  0:00:14s
epoch 57 | loss: 0.28949 | val_0_rmse: 0.53388 | val_1_rmse: 0.53393 |  0:00:14s
epoch 58 | loss: 0.30164 | val_0_rmse: 0.53249 | val_1_rmse: 0.545   |  0:00:14s
epoch 59 | loss: 0.28869 | val_0_rmse: 0.52988 | val_1_rmse: 0.54449 |  0:00:14s
epoch 60 | loss: 0.29147 | val_0_rmse: 0.52652 | val_1_rmse: 0.54161 |  0:00:15s
epoch 61 | loss: 0.30313 | val_0_rmse: 0.52365 | val_1_rmse: 0.54306 |  0:00:15s
epoch 62 | loss: 0.28598 | val_0_rmse: 0.52499 | val_1_rmse: 0.5425  |  0:00:15s
epoch 63 | loss: 0.28444 | val_0_rmse: 0.5199  | val_1_rmse: 0.53649 |  0:00:15s
epoch 64 | loss: 0.28738 | val_0_rmse: 0.51831 | val_1_rmse: 0.5362  |  0:00:16s
epoch 65 | loss: 0.27838 | val_0_rmse: 0.51962 | val_1_rmse: 0.53081 |  0:00:16s
epoch 66 | loss: 0.2867  | val_0_rmse: 0.52352 | val_1_rmse: 0.53345 |  0:00:16s
epoch 67 | loss: 0.30305 | val_0_rmse: 0.56018 | val_1_rmse: 0.5746  |  0:00:16s
epoch 68 | loss: 0.29512 | val_0_rmse: 0.53699 | val_1_rmse: 0.55717 |  0:00:17s
epoch 69 | loss: 0.28949 | val_0_rmse: 0.53013 | val_1_rmse: 0.55548 |  0:00:17s
epoch 70 | loss: 0.29683 | val_0_rmse: 0.53135 | val_1_rmse: 0.55524 |  0:00:17s
epoch 71 | loss: 0.28654 | val_0_rmse: 0.52284 | val_1_rmse: 0.54315 |  0:00:17s
epoch 72 | loss: 0.28735 | val_0_rmse: 0.52491 | val_1_rmse: 0.54092 |  0:00:18s
epoch 73 | loss: 0.28963 | val_0_rmse: 0.51892 | val_1_rmse: 0.5314  |  0:00:18s
epoch 74 | loss: 0.27755 | val_0_rmse: 0.51615 | val_1_rmse: 0.5382  |  0:00:18s
epoch 75 | loss: 0.2771  | val_0_rmse: 0.51216 | val_1_rmse: 0.54093 |  0:00:18s
epoch 76 | loss: 0.27194 | val_0_rmse: 0.50964 | val_1_rmse: 0.54585 |  0:00:19s
epoch 77 | loss: 0.28823 | val_0_rmse: 0.52141 | val_1_rmse: 0.55217 |  0:00:19s
epoch 78 | loss: 0.27433 | val_0_rmse: 0.55656 | val_1_rmse: 0.58195 |  0:00:19s
epoch 79 | loss: 0.29557 | val_0_rmse: 0.51982 | val_1_rmse: 0.56135 |  0:00:19s
epoch 80 | loss: 0.2932  | val_0_rmse: 0.53102 | val_1_rmse: 0.56251 |  0:00:20s
epoch 81 | loss: 0.2886  | val_0_rmse: 0.51928 | val_1_rmse: 0.54848 |  0:00:20s
epoch 82 | loss: 0.26931 | val_0_rmse: 0.52095 | val_1_rmse: 0.53227 |  0:00:20s
epoch 83 | loss: 0.28109 | val_0_rmse: 0.5388  | val_1_rmse: 0.53975 |  0:00:20s
epoch 84 | loss: 0.29397 | val_0_rmse: 0.54278 | val_1_rmse: 0.54534 |  0:00:21s
epoch 85 | loss: 0.294   | val_0_rmse: 0.54445 | val_1_rmse: 0.55402 |  0:00:21s
epoch 86 | loss: 0.29154 | val_0_rmse: 0.53275 | val_1_rmse: 0.56111 |  0:00:21s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.52471
Best weights from best epoch are automatically used!
ended training at: 02:43:52
Feature importance:
[('Area', 0.36802697856954847), ('Baths', 0.12603812829685732), ('Beds', 0.07609334436529029), ('Latitude', 0.033931511353139744), ('Longitude', 0.20337525635983464), ('Month', 0.11118246816436825), ('Year', 0.08135231289096127)]
Mean squared error is of 1422706906.2987146
Mean absolute error:25924.840202032963
MAPE:0.3186319582956552
R2 score:0.6425534020146451
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:43:52
epoch 0  | loss: 1.37846 | val_0_rmse: 1.28626 | val_1_rmse: 1.37151 |  0:00:00s
epoch 1  | loss: 0.72233 | val_0_rmse: 0.96297 | val_1_rmse: 0.94668 |  0:00:00s
epoch 2  | loss: 0.52548 | val_0_rmse: 0.91723 | val_1_rmse: 0.87299 |  0:00:00s
epoch 3  | loss: 0.44583 | val_0_rmse: 0.95137 | val_1_rmse: 0.92148 |  0:00:01s
epoch 4  | loss: 0.40818 | val_0_rmse: 0.79292 | val_1_rmse: 0.73296 |  0:00:01s
epoch 5  | loss: 0.37999 | val_0_rmse: 0.73216 | val_1_rmse: 0.71518 |  0:00:01s
epoch 6  | loss: 0.36474 | val_0_rmse: 0.75053 | val_1_rmse: 0.71743 |  0:00:01s
epoch 7  | loss: 0.34176 | val_0_rmse: 0.6942  | val_1_rmse: 0.66702 |  0:00:02s
epoch 8  | loss: 0.32342 | val_0_rmse: 0.64411 | val_1_rmse: 0.62327 |  0:00:02s
epoch 9  | loss: 0.30775 | val_0_rmse: 0.61299 | val_1_rmse: 0.60973 |  0:00:02s
epoch 10 | loss: 0.30714 | val_0_rmse: 0.58626 | val_1_rmse: 0.5869  |  0:00:02s
epoch 11 | loss: 0.30486 | val_0_rmse: 0.57464 | val_1_rmse: 0.55715 |  0:00:03s
epoch 12 | loss: 0.28637 | val_0_rmse: 0.57738 | val_1_rmse: 0.54371 |  0:00:03s
epoch 13 | loss: 0.30097 | val_0_rmse: 0.56085 | val_1_rmse: 0.5315  |  0:00:03s
epoch 14 | loss: 0.28484 | val_0_rmse: 0.54949 | val_1_rmse: 0.55359 |  0:00:03s
epoch 15 | loss: 0.28092 | val_0_rmse: 0.56404 | val_1_rmse: 0.56949 |  0:00:04s
epoch 16 | loss: 0.27287 | val_0_rmse: 0.51738 | val_1_rmse: 0.52698 |  0:00:04s
epoch 17 | loss: 0.27084 | val_0_rmse: 0.50465 | val_1_rmse: 0.52551 |  0:00:04s
epoch 18 | loss: 0.27213 | val_0_rmse: 0.51739 | val_1_rmse: 0.5317  |  0:00:04s
epoch 19 | loss: 0.2591  | val_0_rmse: 0.51803 | val_1_rmse: 0.53849 |  0:00:05s
epoch 20 | loss: 0.25568 | val_0_rmse: 0.51602 | val_1_rmse: 0.54835 |  0:00:05s
epoch 21 | loss: 0.25412 | val_0_rmse: 0.50888 | val_1_rmse: 0.53135 |  0:00:05s
epoch 22 | loss: 0.24508 | val_0_rmse: 0.49625 | val_1_rmse: 0.51398 |  0:00:05s
epoch 23 | loss: 0.24831 | val_0_rmse: 0.48185 | val_1_rmse: 0.48798 |  0:00:06s
epoch 24 | loss: 0.24689 | val_0_rmse: 0.47893 | val_1_rmse: 0.48759 |  0:00:06s
epoch 25 | loss: 0.24027 | val_0_rmse: 0.47753 | val_1_rmse: 0.48725 |  0:00:06s
epoch 26 | loss: 0.23629 | val_0_rmse: 0.48218 | val_1_rmse: 0.49023 |  0:00:06s
epoch 27 | loss: 0.24314 | val_0_rmse: 0.46679 | val_1_rmse: 0.4794  |  0:00:07s
epoch 28 | loss: 0.2294  | val_0_rmse: 0.47718 | val_1_rmse: 0.50482 |  0:00:07s
epoch 29 | loss: 0.23643 | val_0_rmse: 0.46249 | val_1_rmse: 0.49468 |  0:00:07s
epoch 30 | loss: 0.22668 | val_0_rmse: 0.44996 | val_1_rmse: 0.465   |  0:00:07s
epoch 31 | loss: 0.21243 | val_0_rmse: 0.45503 | val_1_rmse: 0.46723 |  0:00:08s
epoch 32 | loss: 0.2205  | val_0_rmse: 0.45094 | val_1_rmse: 0.46377 |  0:00:08s
epoch 33 | loss: 0.21067 | val_0_rmse: 0.45658 | val_1_rmse: 0.46211 |  0:00:08s
epoch 34 | loss: 0.21446 | val_0_rmse: 0.452   | val_1_rmse: 0.46566 |  0:00:08s
epoch 35 | loss: 0.21869 | val_0_rmse: 0.44622 | val_1_rmse: 0.45662 |  0:00:09s
epoch 36 | loss: 0.21843 | val_0_rmse: 0.44436 | val_1_rmse: 0.44659 |  0:00:09s
epoch 37 | loss: 0.2158  | val_0_rmse: 0.44215 | val_1_rmse: 0.45123 |  0:00:09s
epoch 38 | loss: 0.20968 | val_0_rmse: 0.44666 | val_1_rmse: 0.46658 |  0:00:09s
epoch 39 | loss: 0.21493 | val_0_rmse: 0.43155 | val_1_rmse: 0.45332 |  0:00:10s
epoch 40 | loss: 0.20561 | val_0_rmse: 0.45918 | val_1_rmse: 0.48393 |  0:00:10s
epoch 41 | loss: 0.22726 | val_0_rmse: 0.43543 | val_1_rmse: 0.44984 |  0:00:10s
epoch 42 | loss: 0.20592 | val_0_rmse: 0.4538  | val_1_rmse: 0.47266 |  0:00:10s
epoch 43 | loss: 0.204   | val_0_rmse: 0.43372 | val_1_rmse: 0.45671 |  0:00:11s
epoch 44 | loss: 0.21752 | val_0_rmse: 0.45954 | val_1_rmse: 0.45693 |  0:00:11s
epoch 45 | loss: 0.2159  | val_0_rmse: 0.438   | val_1_rmse: 0.43484 |  0:00:11s
epoch 46 | loss: 0.22163 | val_0_rmse: 0.4278  | val_1_rmse: 0.45123 |  0:00:11s
epoch 47 | loss: 0.20481 | val_0_rmse: 0.44689 | val_1_rmse: 0.45341 |  0:00:12s
epoch 48 | loss: 0.20648 | val_0_rmse: 0.42775 | val_1_rmse: 0.43839 |  0:00:12s
epoch 49 | loss: 0.203   | val_0_rmse: 0.4346  | val_1_rmse: 0.45239 |  0:00:12s
epoch 50 | loss: 0.20958 | val_0_rmse: 0.43139 | val_1_rmse: 0.44256 |  0:00:12s
epoch 51 | loss: 0.20174 | val_0_rmse: 0.43532 | val_1_rmse: 0.44904 |  0:00:13s
epoch 52 | loss: 0.19221 | val_0_rmse: 0.43453 | val_1_rmse: 0.45447 |  0:00:13s
epoch 53 | loss: 0.19603 | val_0_rmse: 0.43055 | val_1_rmse: 0.44064 |  0:00:13s
epoch 54 | loss: 0.19566 | val_0_rmse: 0.4176  | val_1_rmse: 0.44646 |  0:00:13s
epoch 55 | loss: 0.19406 | val_0_rmse: 0.40851 | val_1_rmse: 0.44743 |  0:00:14s
epoch 56 | loss: 0.19185 | val_0_rmse: 0.41449 | val_1_rmse: 0.44869 |  0:00:14s
epoch 57 | loss: 0.19279 | val_0_rmse: 0.41391 | val_1_rmse: 0.46275 |  0:00:14s
epoch 58 | loss: 0.19022 | val_0_rmse: 0.4009  | val_1_rmse: 0.43109 |  0:00:14s
epoch 59 | loss: 0.18656 | val_0_rmse: 0.41369 | val_1_rmse: 0.43093 |  0:00:15s
epoch 60 | loss: 0.18764 | val_0_rmse: 0.40724 | val_1_rmse: 0.45398 |  0:00:15s
epoch 61 | loss: 0.18373 | val_0_rmse: 0.39728 | val_1_rmse: 0.44013 |  0:00:15s
epoch 62 | loss: 0.17722 | val_0_rmse: 0.39942 | val_1_rmse: 0.43906 |  0:00:15s
epoch 63 | loss: 0.18095 | val_0_rmse: 0.40739 | val_1_rmse: 0.46581 |  0:00:16s
epoch 64 | loss: 0.18523 | val_0_rmse: 0.40337 | val_1_rmse: 0.45318 |  0:00:16s
epoch 65 | loss: 0.17915 | val_0_rmse: 0.395   | val_1_rmse: 0.4331  |  0:00:16s
epoch 66 | loss: 0.18512 | val_0_rmse: 0.39223 | val_1_rmse: 0.42555 |  0:00:16s
epoch 67 | loss: 0.17293 | val_0_rmse: 0.39536 | val_1_rmse: 0.42642 |  0:00:17s
epoch 68 | loss: 0.18236 | val_0_rmse: 0.39342 | val_1_rmse: 0.4337  |  0:00:17s
epoch 69 | loss: 0.18841 | val_0_rmse: 0.39675 | val_1_rmse: 0.44468 |  0:00:17s
epoch 70 | loss: 0.19369 | val_0_rmse: 0.42164 | val_1_rmse: 0.45904 |  0:00:17s
epoch 71 | loss: 0.19087 | val_0_rmse: 0.42948 | val_1_rmse: 0.45513 |  0:00:18s
epoch 72 | loss: 0.17097 | val_0_rmse: 0.41004 | val_1_rmse: 0.46547 |  0:00:18s
epoch 73 | loss: 0.18649 | val_0_rmse: 0.40802 | val_1_rmse: 0.44562 |  0:00:18s
epoch 74 | loss: 0.17494 | val_0_rmse: 0.38585 | val_1_rmse: 0.4312  |  0:00:18s
epoch 75 | loss: 0.16229 | val_0_rmse: 0.38915 | val_1_rmse: 0.45807 |  0:00:19s
epoch 76 | loss: 0.17441 | val_0_rmse: 0.38876 | val_1_rmse: 0.438   |  0:00:19s
epoch 77 | loss: 0.17744 | val_0_rmse: 0.4021  | val_1_rmse: 0.445   |  0:00:19s
epoch 78 | loss: 0.17352 | val_0_rmse: 0.40153 | val_1_rmse: 0.4482  |  0:00:19s
epoch 79 | loss: 0.17743 | val_0_rmse: 0.40455 | val_1_rmse: 0.44334 |  0:00:20s
epoch 80 | loss: 0.17685 | val_0_rmse: 0.40303 | val_1_rmse: 0.44879 |  0:00:20s
epoch 81 | loss: 0.17503 | val_0_rmse: 0.38963 | val_1_rmse: 0.47509 |  0:00:20s
epoch 82 | loss: 0.16508 | val_0_rmse: 0.38167 | val_1_rmse: 0.4502  |  0:00:20s
epoch 83 | loss: 0.17196 | val_0_rmse: 0.39255 | val_1_rmse: 0.44211 |  0:00:21s
epoch 84 | loss: 0.1794  | val_0_rmse: 0.39392 | val_1_rmse: 0.45561 |  0:00:21s
epoch 85 | loss: 0.18173 | val_0_rmse: 0.39003 | val_1_rmse: 0.45828 |  0:00:21s
epoch 86 | loss: 0.18078 | val_0_rmse: 0.38538 | val_1_rmse: 0.46066 |  0:00:21s
epoch 87 | loss: 0.17684 | val_0_rmse: 0.38902 | val_1_rmse: 0.47207 |  0:00:21s
epoch 88 | loss: 0.17392 | val_0_rmse: 0.39638 | val_1_rmse: 0.47001 |  0:00:22s
epoch 89 | loss: 0.1631  | val_0_rmse: 0.41021 | val_1_rmse: 0.47101 |  0:00:22s
epoch 90 | loss: 0.16554 | val_0_rmse: 0.39493 | val_1_rmse: 0.44874 |  0:00:22s
epoch 91 | loss: 0.16398 | val_0_rmse: 0.37718 | val_1_rmse: 0.43866 |  0:00:22s
epoch 92 | loss: 0.16673 | val_0_rmse: 0.38518 | val_1_rmse: 0.45028 |  0:00:23s
epoch 93 | loss: 0.17107 | val_0_rmse: 0.37289 | val_1_rmse: 0.44291 |  0:00:23s
epoch 94 | loss: 0.16813 | val_0_rmse: 0.38119 | val_1_rmse: 0.44564 |  0:00:23s
epoch 95 | loss: 0.16992 | val_0_rmse: 0.38694 | val_1_rmse: 0.4579  |  0:00:23s
epoch 96 | loss: 0.16038 | val_0_rmse: 0.37826 | val_1_rmse: 0.42925 |  0:00:24s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.42555
Best weights from best epoch are automatically used!
ended training at: 02:44:16
Feature importance:
[('Area', 0.07741358267966109), ('Baths', 0.10253109774428798), ('Beds', 0.07261552299860233), ('Latitude', 0.25389251477775543), ('Longitude', 0.17125581060437972), ('Month', 0.020006468153445065), ('Year', 0.3022850030418684)]
Mean squared error is of 17299930552.083275
Mean absolute error:90295.22568285327
MAPE:0.3274983717618247
R2 score:0.7123645593977175
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:44:17
epoch 0  | loss: 1.35019 | val_0_rmse: 1.33532 | val_1_rmse: 1.30419 |  0:00:00s
epoch 1  | loss: 0.69757 | val_0_rmse: 0.88274 | val_1_rmse: 0.89748 |  0:00:00s
epoch 2  | loss: 0.56791 | val_0_rmse: 0.89377 | val_1_rmse: 0.86928 |  0:00:00s
epoch 3  | loss: 0.47935 | val_0_rmse: 0.82268 | val_1_rmse: 0.82609 |  0:00:00s
epoch 4  | loss: 0.45589 | val_0_rmse: 0.76471 | val_1_rmse: 0.74307 |  0:00:01s
epoch 5  | loss: 0.42776 | val_0_rmse: 0.73922 | val_1_rmse: 0.74581 |  0:00:01s
epoch 6  | loss: 0.39848 | val_0_rmse: 0.74876 | val_1_rmse: 0.77231 |  0:00:01s
epoch 7  | loss: 0.37223 | val_0_rmse: 0.7348  | val_1_rmse: 0.76831 |  0:00:01s
epoch 8  | loss: 0.34389 | val_0_rmse: 0.68526 | val_1_rmse: 0.71415 |  0:00:02s
epoch 9  | loss: 0.34335 | val_0_rmse: 0.67391 | val_1_rmse: 0.70256 |  0:00:02s
epoch 10 | loss: 0.34445 | val_0_rmse: 0.65287 | val_1_rmse: 0.67853 |  0:00:02s
epoch 11 | loss: 0.32423 | val_0_rmse: 0.63803 | val_1_rmse: 0.64216 |  0:00:02s
epoch 12 | loss: 0.3273  | val_0_rmse: 0.62193 | val_1_rmse: 0.63479 |  0:00:03s
epoch 13 | loss: 0.31124 | val_0_rmse: 0.61656 | val_1_rmse: 0.6224  |  0:00:03s
epoch 14 | loss: 0.32059 | val_0_rmse: 0.57625 | val_1_rmse: 0.55898 |  0:00:03s
epoch 15 | loss: 0.30492 | val_0_rmse: 0.55306 | val_1_rmse: 0.53662 |  0:00:03s
epoch 16 | loss: 0.30577 | val_0_rmse: 0.55459 | val_1_rmse: 0.52701 |  0:00:04s
epoch 17 | loss: 0.29863 | val_0_rmse: 0.55348 | val_1_rmse: 0.53734 |  0:00:04s
epoch 18 | loss: 0.29393 | val_0_rmse: 0.54627 | val_1_rmse: 0.53746 |  0:00:04s
epoch 19 | loss: 0.27251 | val_0_rmse: 0.52981 | val_1_rmse: 0.51265 |  0:00:04s
epoch 20 | loss: 0.26661 | val_0_rmse: 0.51464 | val_1_rmse: 0.51067 |  0:00:05s
epoch 21 | loss: 0.25404 | val_0_rmse: 0.5144  | val_1_rmse: 0.51598 |  0:00:05s
epoch 22 | loss: 0.25509 | val_0_rmse: 0.50136 | val_1_rmse: 0.49948 |  0:00:05s
epoch 23 | loss: 0.2548  | val_0_rmse: 0.50314 | val_1_rmse: 0.50023 |  0:00:05s
epoch 24 | loss: 0.24615 | val_0_rmse: 0.50734 | val_1_rmse: 0.50022 |  0:00:06s
epoch 25 | loss: 0.23934 | val_0_rmse: 0.48101 | val_1_rmse: 0.49248 |  0:00:06s
epoch 26 | loss: 0.2398  | val_0_rmse: 0.46821 | val_1_rmse: 0.4852  |  0:00:06s
epoch 27 | loss: 0.24113 | val_0_rmse: 0.46802 | val_1_rmse: 0.4972  |  0:00:07s
epoch 28 | loss: 0.22066 | val_0_rmse: 0.47573 | val_1_rmse: 0.50167 |  0:00:07s
epoch 29 | loss: 0.23615 | val_0_rmse: 0.46475 | val_1_rmse: 0.48096 |  0:00:07s
epoch 30 | loss: 0.23313 | val_0_rmse: 0.47505 | val_1_rmse: 0.47938 |  0:00:07s
epoch 31 | loss: 0.23224 | val_0_rmse: 0.47332 | val_1_rmse: 0.49727 |  0:00:08s
epoch 32 | loss: 0.25739 | val_0_rmse: 0.47287 | val_1_rmse: 0.48494 |  0:00:08s
epoch 33 | loss: 0.24722 | val_0_rmse: 0.4684  | val_1_rmse: 0.47424 |  0:00:08s
epoch 34 | loss: 0.22509 | val_0_rmse: 0.47729 | val_1_rmse: 0.49643 |  0:00:08s
epoch 35 | loss: 0.22408 | val_0_rmse: 0.45658 | val_1_rmse: 0.47598 |  0:00:09s
epoch 36 | loss: 0.22944 | val_0_rmse: 0.46393 | val_1_rmse: 0.48104 |  0:00:09s
epoch 37 | loss: 0.21524 | val_0_rmse: 0.49282 | val_1_rmse: 0.5052  |  0:00:09s
epoch 38 | loss: 0.22806 | val_0_rmse: 0.45061 | val_1_rmse: 0.45478 |  0:00:09s
epoch 39 | loss: 0.21773 | val_0_rmse: 0.44235 | val_1_rmse: 0.44405 |  0:00:10s
epoch 40 | loss: 0.21205 | val_0_rmse: 0.462   | val_1_rmse: 0.47348 |  0:00:10s
epoch 41 | loss: 0.21647 | val_0_rmse: 0.42945 | val_1_rmse: 0.44965 |  0:00:10s
epoch 42 | loss: 0.21624 | val_0_rmse: 0.42832 | val_1_rmse: 0.45172 |  0:00:10s
epoch 43 | loss: 0.20296 | val_0_rmse: 0.46395 | val_1_rmse: 0.4741  |  0:00:10s
epoch 44 | loss: 0.20719 | val_0_rmse: 0.43077 | val_1_rmse: 0.44904 |  0:00:11s
epoch 45 | loss: 0.20933 | val_0_rmse: 0.42961 | val_1_rmse: 0.44994 |  0:00:11s
epoch 46 | loss: 0.20218 | val_0_rmse: 0.4384  | val_1_rmse: 0.46487 |  0:00:11s
epoch 47 | loss: 0.19956 | val_0_rmse: 0.4264  | val_1_rmse: 0.44511 |  0:00:11s
epoch 48 | loss: 0.2175  | val_0_rmse: 0.42904 | val_1_rmse: 0.43164 |  0:00:12s
epoch 49 | loss: 0.21475 | val_0_rmse: 0.4396  | val_1_rmse: 0.46169 |  0:00:12s
epoch 50 | loss: 0.18818 | val_0_rmse: 0.43878 | val_1_rmse: 0.4687  |  0:00:12s
epoch 51 | loss: 0.21089 | val_0_rmse: 0.4234  | val_1_rmse: 0.45667 |  0:00:12s
epoch 52 | loss: 0.20736 | val_0_rmse: 0.4382  | val_1_rmse: 0.46232 |  0:00:13s
epoch 53 | loss: 0.2077  | val_0_rmse: 0.43504 | val_1_rmse: 0.46419 |  0:00:13s
epoch 54 | loss: 0.20962 | val_0_rmse: 0.4392  | val_1_rmse: 0.47586 |  0:00:13s
epoch 55 | loss: 0.20369 | val_0_rmse: 0.42914 | val_1_rmse: 0.45528 |  0:00:13s
epoch 56 | loss: 0.1895  | val_0_rmse: 0.42185 | val_1_rmse: 0.44738 |  0:00:14s
epoch 57 | loss: 0.19746 | val_0_rmse: 0.42241 | val_1_rmse: 0.45315 |  0:00:14s
epoch 58 | loss: 0.19969 | val_0_rmse: 0.43565 | val_1_rmse: 0.46273 |  0:00:14s
epoch 59 | loss: 0.20097 | val_0_rmse: 0.42322 | val_1_rmse: 0.43795 |  0:00:14s
epoch 60 | loss: 0.19592 | val_0_rmse: 0.43045 | val_1_rmse: 0.43131 |  0:00:15s
epoch 61 | loss: 0.19588 | val_0_rmse: 0.43121 | val_1_rmse: 0.43624 |  0:00:15s
epoch 62 | loss: 0.19035 | val_0_rmse: 0.42589 | val_1_rmse: 0.44453 |  0:00:15s
epoch 63 | loss: 0.20986 | val_0_rmse: 0.43212 | val_1_rmse: 0.46058 |  0:00:15s
epoch 64 | loss: 0.19968 | val_0_rmse: 0.43781 | val_1_rmse: 0.46797 |  0:00:16s
epoch 65 | loss: 0.19537 | val_0_rmse: 0.42507 | val_1_rmse: 0.45247 |  0:00:16s
epoch 66 | loss: 0.20041 | val_0_rmse: 0.42492 | val_1_rmse: 0.46791 |  0:00:16s
epoch 67 | loss: 0.19809 | val_0_rmse: 0.42748 | val_1_rmse: 0.47855 |  0:00:16s
epoch 68 | loss: 0.19791 | val_0_rmse: 0.41241 | val_1_rmse: 0.4483  |  0:00:17s
epoch 69 | loss: 0.18237 | val_0_rmse: 0.41427 | val_1_rmse: 0.43967 |  0:00:17s
epoch 70 | loss: 0.19386 | val_0_rmse: 0.40875 | val_1_rmse: 0.42792 |  0:00:17s
epoch 71 | loss: 0.18403 | val_0_rmse: 0.40516 | val_1_rmse: 0.44404 |  0:00:17s
epoch 72 | loss: 0.18679 | val_0_rmse: 0.40886 | val_1_rmse: 0.45543 |  0:00:18s
epoch 73 | loss: 0.18117 | val_0_rmse: 0.40869 | val_1_rmse: 0.43222 |  0:00:18s
epoch 74 | loss: 0.17327 | val_0_rmse: 0.4162  | val_1_rmse: 0.41717 |  0:00:18s
epoch 75 | loss: 0.19174 | val_0_rmse: 0.40579 | val_1_rmse: 0.42729 |  0:00:18s
epoch 76 | loss: 0.18635 | val_0_rmse: 0.40546 | val_1_rmse: 0.4634  |  0:00:19s
epoch 77 | loss: 0.17863 | val_0_rmse: 0.41122 | val_1_rmse: 0.48347 |  0:00:19s
epoch 78 | loss: 0.17886 | val_0_rmse: 0.3943  | val_1_rmse: 0.46258 |  0:00:19s
epoch 79 | loss: 0.18181 | val_0_rmse: 0.39272 | val_1_rmse: 0.4265  |  0:00:19s
epoch 80 | loss: 0.17529 | val_0_rmse: 0.4096  | val_1_rmse: 0.42246 |  0:00:20s
epoch 81 | loss: 0.18397 | val_0_rmse: 0.40652 | val_1_rmse: 0.42479 |  0:00:20s
epoch 82 | loss: 0.18004 | val_0_rmse: 0.41169 | val_1_rmse: 0.44529 |  0:00:20s
epoch 83 | loss: 0.18495 | val_0_rmse: 0.40404 | val_1_rmse: 0.45555 |  0:00:20s
epoch 84 | loss: 0.17771 | val_0_rmse: 0.41295 | val_1_rmse: 0.4626  |  0:00:21s
epoch 85 | loss: 0.18564 | val_0_rmse: 0.3946  | val_1_rmse: 0.43659 |  0:00:21s
epoch 86 | loss: 0.18262 | val_0_rmse: 0.39622 | val_1_rmse: 0.44227 |  0:00:21s
epoch 87 | loss: 0.17056 | val_0_rmse: 0.40311 | val_1_rmse: 0.45238 |  0:00:21s
epoch 88 | loss: 0.17654 | val_0_rmse: 0.39788 | val_1_rmse: 0.44412 |  0:00:22s
epoch 89 | loss: 0.1709  | val_0_rmse: 0.39572 | val_1_rmse: 0.43273 |  0:00:22s
epoch 90 | loss: 0.19201 | val_0_rmse: 0.39615 | val_1_rmse: 0.42646 |  0:00:22s
epoch 91 | loss: 0.16256 | val_0_rmse: 0.403   | val_1_rmse: 0.43512 |  0:00:22s
epoch 92 | loss: 0.17681 | val_0_rmse: 0.4016  | val_1_rmse: 0.44123 |  0:00:22s
epoch 93 | loss: 0.16993 | val_0_rmse: 0.39664 | val_1_rmse: 0.4424  |  0:00:23s
epoch 94 | loss: 0.166   | val_0_rmse: 0.39792 | val_1_rmse: 0.45988 |  0:00:23s
epoch 95 | loss: 0.17866 | val_0_rmse: 0.38371 | val_1_rmse: 0.46171 |  0:00:23s
epoch 96 | loss: 0.18687 | val_0_rmse: 0.39705 | val_1_rmse: 0.46633 |  0:00:23s
epoch 97 | loss: 0.18541 | val_0_rmse: 0.38148 | val_1_rmse: 0.44637 |  0:00:24s
epoch 98 | loss: 0.16881 | val_0_rmse: 0.38738 | val_1_rmse: 0.44012 |  0:00:24s
epoch 99 | loss: 0.16375 | val_0_rmse: 0.39109 | val_1_rmse: 0.45177 |  0:00:24s
epoch 100| loss: 0.16494 | val_0_rmse: 0.38799 | val_1_rmse: 0.44978 |  0:00:24s
epoch 101| loss: 0.17605 | val_0_rmse: 0.39077 | val_1_rmse: 0.45294 |  0:00:25s
epoch 102| loss: 0.165   | val_0_rmse: 0.37446 | val_1_rmse: 0.44159 |  0:00:25s
epoch 103| loss: 0.15873 | val_0_rmse: 0.39645 | val_1_rmse: 0.45698 |  0:00:25s
epoch 104| loss: 0.17513 | val_0_rmse: 0.37995 | val_1_rmse: 0.43931 |  0:00:25s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.41717
Best weights from best epoch are automatically used!
ended training at: 02:44:43
Feature importance:
[('Area', 0.060430605011622215), ('Baths', 0.13255249595045265), ('Beds', 0.0864013090392865), ('Latitude', 0.1624596822384884), ('Longitude', 0.26882567582932215), ('Month', 0.049386767373905016), ('Year', 0.2399434645569231)]
Mean squared error is of 13850539696.07388
Mean absolute error:84875.78281864697
MAPE:0.30262686228365754
R2 score:0.7590841104332957
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:44:43
epoch 0  | loss: 1.45436 | val_0_rmse: 1.5363  | val_1_rmse: 1.48143 |  0:00:00s
epoch 1  | loss: 0.69999 | val_0_rmse: 1.06698 | val_1_rmse: 1.1477  |  0:00:00s
epoch 2  | loss: 0.52234 | val_0_rmse: 0.96001 | val_1_rmse: 1.0592  |  0:00:00s
epoch 3  | loss: 0.44728 | val_0_rmse: 0.81745 | val_1_rmse: 0.86583 |  0:00:01s
epoch 4  | loss: 0.40972 | val_0_rmse: 0.75942 | val_1_rmse: 0.82455 |  0:00:01s
epoch 5  | loss: 0.40531 | val_0_rmse: 0.74744 | val_1_rmse: 0.76389 |  0:00:01s
epoch 6  | loss: 0.37814 | val_0_rmse: 0.69899 | val_1_rmse: 0.7074  |  0:00:01s
epoch 7  | loss: 0.34076 | val_0_rmse: 0.68465 | val_1_rmse: 0.71036 |  0:00:02s
epoch 8  | loss: 0.34821 | val_0_rmse: 0.66901 | val_1_rmse: 0.74211 |  0:00:02s
epoch 9  | loss: 0.3396  | val_0_rmse: 0.62718 | val_1_rmse: 0.68107 |  0:00:02s
epoch 10 | loss: 0.30987 | val_0_rmse: 0.60438 | val_1_rmse: 0.66997 |  0:00:02s
epoch 11 | loss: 0.31555 | val_0_rmse: 0.59643 | val_1_rmse: 0.65246 |  0:00:03s
epoch 12 | loss: 0.32453 | val_0_rmse: 0.5869  | val_1_rmse: 0.64634 |  0:00:03s
epoch 13 | loss: 0.2984  | val_0_rmse: 0.57867 | val_1_rmse: 0.64994 |  0:00:03s
epoch 14 | loss: 0.30385 | val_0_rmse: 0.54402 | val_1_rmse: 0.62126 |  0:00:03s
epoch 15 | loss: 0.28044 | val_0_rmse: 0.55653 | val_1_rmse: 0.6232  |  0:00:04s
epoch 16 | loss: 0.29028 | val_0_rmse: 0.5513  | val_1_rmse: 0.6123  |  0:00:04s
epoch 17 | loss: 0.27576 | val_0_rmse: 0.5336  | val_1_rmse: 0.59928 |  0:00:04s
epoch 18 | loss: 0.26757 | val_0_rmse: 0.53965 | val_1_rmse: 0.61477 |  0:00:04s
epoch 19 | loss: 0.26037 | val_0_rmse: 0.55004 | val_1_rmse: 0.62542 |  0:00:05s
epoch 20 | loss: 0.25743 | val_0_rmse: 0.53744 | val_1_rmse: 0.61634 |  0:00:05s
epoch 21 | loss: 0.26856 | val_0_rmse: 0.49817 | val_1_rmse: 0.56891 |  0:00:05s
epoch 22 | loss: 0.24259 | val_0_rmse: 0.48447 | val_1_rmse: 0.57777 |  0:00:06s
epoch 23 | loss: 0.25029 | val_0_rmse: 0.50971 | val_1_rmse: 0.62517 |  0:00:06s
epoch 24 | loss: 0.24691 | val_0_rmse: 0.48158 | val_1_rmse: 0.56846 |  0:00:06s
epoch 25 | loss: 0.25762 | val_0_rmse: 0.48235 | val_1_rmse: 0.56181 |  0:00:06s
epoch 26 | loss: 0.25138 | val_0_rmse: 0.47556 | val_1_rmse: 0.56966 |  0:00:07s
epoch 27 | loss: 0.23472 | val_0_rmse: 0.50673 | val_1_rmse: 0.58451 |  0:00:07s
epoch 28 | loss: 0.26154 | val_0_rmse: 0.47148 | val_1_rmse: 0.55103 |  0:00:07s
epoch 29 | loss: 0.23268 | val_0_rmse: 0.4762  | val_1_rmse: 0.55447 |  0:00:07s
epoch 30 | loss: 0.24305 | val_0_rmse: 0.48044 | val_1_rmse: 0.56869 |  0:00:07s
epoch 31 | loss: 0.23281 | val_0_rmse: 0.47496 | val_1_rmse: 0.58795 |  0:00:08s
epoch 32 | loss: 0.22349 | val_0_rmse: 0.46648 | val_1_rmse: 0.59885 |  0:00:08s
epoch 33 | loss: 0.22767 | val_0_rmse: 0.45782 | val_1_rmse: 0.58218 |  0:00:08s
epoch 34 | loss: 0.23374 | val_0_rmse: 0.45616 | val_1_rmse: 0.59029 |  0:00:08s
epoch 35 | loss: 0.21161 | val_0_rmse: 0.45868 | val_1_rmse: 0.60524 |  0:00:09s
epoch 36 | loss: 0.2232  | val_0_rmse: 0.46215 | val_1_rmse: 0.59607 |  0:00:09s
epoch 37 | loss: 0.22468 | val_0_rmse: 0.45827 | val_1_rmse: 0.58227 |  0:00:09s
epoch 38 | loss: 0.22657 | val_0_rmse: 0.45404 | val_1_rmse: 0.56109 |  0:00:09s
epoch 39 | loss: 0.21682 | val_0_rmse: 0.46023 | val_1_rmse: 0.55865 |  0:00:10s
epoch 40 | loss: 0.22658 | val_0_rmse: 0.4674  | val_1_rmse: 0.55407 |  0:00:10s
epoch 41 | loss: 0.22271 | val_0_rmse: 0.46172 | val_1_rmse: 0.55751 |  0:00:10s
epoch 42 | loss: 0.23058 | val_0_rmse: 0.46151 | val_1_rmse: 0.56624 |  0:00:10s
epoch 43 | loss: 0.21173 | val_0_rmse: 0.46321 | val_1_rmse: 0.57718 |  0:00:11s
epoch 44 | loss: 0.22798 | val_0_rmse: 0.46736 | val_1_rmse: 0.5974  |  0:00:11s
epoch 45 | loss: 0.2224  | val_0_rmse: 0.46108 | val_1_rmse: 0.58914 |  0:00:11s
epoch 46 | loss: 0.22352 | val_0_rmse: 0.44769 | val_1_rmse: 0.56012 |  0:00:11s
epoch 47 | loss: 0.20957 | val_0_rmse: 0.45744 | val_1_rmse: 0.57584 |  0:00:12s
epoch 48 | loss: 0.22919 | val_0_rmse: 0.45697 | val_1_rmse: 0.58628 |  0:00:12s
epoch 49 | loss: 0.23124 | val_0_rmse: 0.45536 | val_1_rmse: 0.57874 |  0:00:12s
epoch 50 | loss: 0.21767 | val_0_rmse: 0.46622 | val_1_rmse: 0.59594 |  0:00:12s
epoch 51 | loss: 0.20357 | val_0_rmse: 0.45016 | val_1_rmse: 0.57616 |  0:00:13s
epoch 52 | loss: 0.21866 | val_0_rmse: 0.44171 | val_1_rmse: 0.56105 |  0:00:13s
epoch 53 | loss: 0.1955  | val_0_rmse: 0.45319 | val_1_rmse: 0.57045 |  0:00:13s
epoch 54 | loss: 0.21317 | val_0_rmse: 0.43555 | val_1_rmse: 0.55054 |  0:00:13s
epoch 55 | loss: 0.20411 | val_0_rmse: 0.42716 | val_1_rmse: 0.54574 |  0:00:14s
epoch 56 | loss: 0.20227 | val_0_rmse: 0.42541 | val_1_rmse: 0.54693 |  0:00:14s
epoch 57 | loss: 0.19656 | val_0_rmse: 0.43153 | val_1_rmse: 0.554   |  0:00:14s
epoch 58 | loss: 0.19641 | val_0_rmse: 0.43535 | val_1_rmse: 0.57033 |  0:00:14s
epoch 59 | loss: 0.19371 | val_0_rmse: 0.42288 | val_1_rmse: 0.566   |  0:00:15s
epoch 60 | loss: 0.19219 | val_0_rmse: 0.41577 | val_1_rmse: 0.55494 |  0:00:15s
epoch 61 | loss: 0.18036 | val_0_rmse: 0.41569 | val_1_rmse: 0.55794 |  0:00:15s
epoch 62 | loss: 0.19889 | val_0_rmse: 0.41263 | val_1_rmse: 0.54594 |  0:00:15s
epoch 63 | loss: 0.18511 | val_0_rmse: 0.41715 | val_1_rmse: 0.54926 |  0:00:16s
epoch 64 | loss: 0.19611 | val_0_rmse: 0.42682 | val_1_rmse: 0.55352 |  0:00:16s
epoch 65 | loss: 0.19277 | val_0_rmse: 0.42302 | val_1_rmse: 0.55207 |  0:00:16s
epoch 66 | loss: 0.18857 | val_0_rmse: 0.40812 | val_1_rmse: 0.54657 |  0:00:16s
epoch 67 | loss: 0.18787 | val_0_rmse: 0.40102 | val_1_rmse: 0.54473 |  0:00:17s
epoch 68 | loss: 0.17818 | val_0_rmse: 0.40157 | val_1_rmse: 0.54642 |  0:00:17s
epoch 69 | loss: 0.17592 | val_0_rmse: 0.40929 | val_1_rmse: 0.56726 |  0:00:17s
epoch 70 | loss: 0.17968 | val_0_rmse: 0.4029  | val_1_rmse: 0.55463 |  0:00:17s
epoch 71 | loss: 0.17925 | val_0_rmse: 0.41017 | val_1_rmse: 0.55327 |  0:00:18s
epoch 72 | loss: 0.18605 | val_0_rmse: 0.40583 | val_1_rmse: 0.55404 |  0:00:18s
epoch 73 | loss: 0.18853 | val_0_rmse: 0.41509 | val_1_rmse: 0.55987 |  0:00:18s
epoch 74 | loss: 0.18247 | val_0_rmse: 0.40739 | val_1_rmse: 0.54367 |  0:00:18s
epoch 75 | loss: 0.176   | val_0_rmse: 0.40564 | val_1_rmse: 0.54431 |  0:00:19s
epoch 76 | loss: 0.18696 | val_0_rmse: 0.41197 | val_1_rmse: 0.56089 |  0:00:19s
epoch 77 | loss: 0.19533 | val_0_rmse: 0.39318 | val_1_rmse: 0.53884 |  0:00:19s
epoch 78 | loss: 0.18952 | val_0_rmse: 0.4218  | val_1_rmse: 0.54898 |  0:00:19s
epoch 79 | loss: 0.18916 | val_0_rmse: 0.40586 | val_1_rmse: 0.53193 |  0:00:20s
epoch 80 | loss: 0.19679 | val_0_rmse: 0.40425 | val_1_rmse: 0.53706 |  0:00:20s
epoch 81 | loss: 0.19243 | val_0_rmse: 0.40215 | val_1_rmse: 0.545   |  0:00:20s
epoch 82 | loss: 0.18134 | val_0_rmse: 0.4083  | val_1_rmse: 0.55265 |  0:00:20s
epoch 83 | loss: 0.18538 | val_0_rmse: 0.41857 | val_1_rmse: 0.5628  |  0:00:21s
epoch 84 | loss: 0.19967 | val_0_rmse: 0.42033 | val_1_rmse: 0.59082 |  0:00:21s
epoch 85 | loss: 0.19    | val_0_rmse: 0.41189 | val_1_rmse: 0.57817 |  0:00:21s
epoch 86 | loss: 0.18342 | val_0_rmse: 0.41132 | val_1_rmse: 0.58453 |  0:00:21s
epoch 87 | loss: 0.18764 | val_0_rmse: 0.44251 | val_1_rmse: 0.62018 |  0:00:22s
epoch 88 | loss: 0.19113 | val_0_rmse: 0.41075 | val_1_rmse: 0.59284 |  0:00:22s
epoch 89 | loss: 0.17104 | val_0_rmse: 0.40236 | val_1_rmse: 0.57434 |  0:00:22s
epoch 90 | loss: 0.18376 | val_0_rmse: 0.39504 | val_1_rmse: 0.56591 |  0:00:22s
epoch 91 | loss: 0.1755  | val_0_rmse: 0.39642 | val_1_rmse: 0.59656 |  0:00:23s
epoch 92 | loss: 0.18828 | val_0_rmse: 0.40298 | val_1_rmse: 0.62087 |  0:00:23s
epoch 93 | loss: 0.17832 | val_0_rmse: 0.39665 | val_1_rmse: 0.61142 |  0:00:23s
epoch 94 | loss: 0.17074 | val_0_rmse: 0.40902 | val_1_rmse: 0.5883  |  0:00:23s
epoch 95 | loss: 0.17702 | val_0_rmse: 0.39402 | val_1_rmse: 0.56009 |  0:00:24s
epoch 96 | loss: 0.17556 | val_0_rmse: 0.39042 | val_1_rmse: 0.5601  |  0:00:24s
epoch 97 | loss: 0.17661 | val_0_rmse: 0.40816 | val_1_rmse: 0.56076 |  0:00:24s
epoch 98 | loss: 0.18214 | val_0_rmse: 0.41509 | val_1_rmse: 0.55026 |  0:00:24s
epoch 99 | loss: 0.18484 | val_0_rmse: 0.40281 | val_1_rmse: 0.54091 |  0:00:25s
epoch 100| loss: 0.1813  | val_0_rmse: 0.42165 | val_1_rmse: 0.58276 |  0:00:25s
epoch 101| loss: 0.18705 | val_0_rmse: 0.41296 | val_1_rmse: 0.57914 |  0:00:25s
epoch 102| loss: 0.18064 | val_0_rmse: 0.40997 | val_1_rmse: 0.57735 |  0:00:25s
epoch 103| loss: 0.18545 | val_0_rmse: 0.40644 | val_1_rmse: 0.58152 |  0:00:26s
epoch 104| loss: 0.18221 | val_0_rmse: 0.40647 | val_1_rmse: 0.59362 |  0:00:26s
epoch 105| loss: 0.1765  | val_0_rmse: 0.44668 | val_1_rmse: 0.65021 |  0:00:26s
epoch 106| loss: 0.17089 | val_0_rmse: 0.41568 | val_1_rmse: 0.61755 |  0:00:26s
epoch 107| loss: 0.16716 | val_0_rmse: 0.39478 | val_1_rmse: 0.58846 |  0:00:26s
epoch 108| loss: 0.16982 | val_0_rmse: 0.42065 | val_1_rmse: 0.60798 |  0:00:27s
epoch 109| loss: 0.16532 | val_0_rmse: 0.39144 | val_1_rmse: 0.58995 |  0:00:27s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.53193
Best weights from best epoch are automatically used!
ended training at: 02:45:11
Feature importance:
[('Area', 0.11141032877276638), ('Baths', 0.1530981065139232), ('Beds', 0.12313502891197871), ('Latitude', 0.14085061887220693), ('Longitude', 0.2533539878549699), ('Month', 0.03049459338537058), ('Year', 0.18765733568878432)]
Mean squared error is of 13508766629.732895
Mean absolute error:86130.78902889766
MAPE:0.3825134255940536
R2 score:0.7617508700513071
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:45:11
epoch 0  | loss: 1.43439 | val_0_rmse: 1.19945 | val_1_rmse: 1.16159 |  0:00:00s
epoch 1  | loss: 0.67058 | val_0_rmse: 1.20753 | val_1_rmse: 1.20911 |  0:00:00s
epoch 2  | loss: 0.52068 | val_0_rmse: 1.47899 | val_1_rmse: 1.52436 |  0:00:00s
epoch 3  | loss: 0.43809 | val_0_rmse: 1.35562 | val_1_rmse: 1.34463 |  0:00:00s
epoch 4  | loss: 0.40116 | val_0_rmse: 0.87436 | val_1_rmse: 0.92576 |  0:00:01s
epoch 5  | loss: 0.3691  | val_0_rmse: 0.77673 | val_1_rmse: 0.8492  |  0:00:01s
epoch 6  | loss: 0.34349 | val_0_rmse: 0.66784 | val_1_rmse: 0.71275 |  0:00:01s
epoch 7  | loss: 0.32551 | val_0_rmse: 0.63873 | val_1_rmse: 0.66322 |  0:00:02s
epoch 8  | loss: 0.33089 | val_0_rmse: 0.6084  | val_1_rmse: 0.65628 |  0:00:02s
epoch 9  | loss: 0.31701 | val_0_rmse: 0.62263 | val_1_rmse: 0.68462 |  0:00:02s
epoch 10 | loss: 0.30158 | val_0_rmse: 0.56106 | val_1_rmse: 0.61618 |  0:00:02s
epoch 11 | loss: 0.2846  | val_0_rmse: 0.55039 | val_1_rmse: 0.62329 |  0:00:03s
epoch 12 | loss: 0.2812  | val_0_rmse: 0.57645 | val_1_rmse: 0.65988 |  0:00:03s
epoch 13 | loss: 0.28215 | val_0_rmse: 0.54656 | val_1_rmse: 0.61294 |  0:00:03s
epoch 14 | loss: 0.2704  | val_0_rmse: 0.52466 | val_1_rmse: 0.5634  |  0:00:03s
epoch 15 | loss: 0.28657 | val_0_rmse: 0.5054  | val_1_rmse: 0.55516 |  0:00:04s
epoch 16 | loss: 0.27347 | val_0_rmse: 0.51902 | val_1_rmse: 0.5945  |  0:00:04s
epoch 17 | loss: 0.27484 | val_0_rmse: 0.51747 | val_1_rmse: 0.57213 |  0:00:04s
epoch 18 | loss: 0.26316 | val_0_rmse: 0.51335 | val_1_rmse: 0.55878 |  0:00:04s
epoch 19 | loss: 0.27627 | val_0_rmse: 0.50033 | val_1_rmse: 0.55089 |  0:00:05s
epoch 20 | loss: 0.2556  | val_0_rmse: 0.50852 | val_1_rmse: 0.56758 |  0:00:05s
epoch 21 | loss: 0.2548  | val_0_rmse: 0.4954  | val_1_rmse: 0.52247 |  0:00:05s
epoch 22 | loss: 0.25361 | val_0_rmse: 0.49988 | val_1_rmse: 0.51203 |  0:00:05s
epoch 23 | loss: 0.24766 | val_0_rmse: 0.47167 | val_1_rmse: 0.52412 |  0:00:06s
epoch 24 | loss: 0.24243 | val_0_rmse: 0.47708 | val_1_rmse: 0.5068  |  0:00:06s
epoch 25 | loss: 0.24142 | val_0_rmse: 0.47409 | val_1_rmse: 0.50802 |  0:00:06s
epoch 26 | loss: 0.24194 | val_0_rmse: 0.4712  | val_1_rmse: 0.52171 |  0:00:06s
epoch 27 | loss: 0.24326 | val_0_rmse: 0.46446 | val_1_rmse: 0.50127 |  0:00:07s
epoch 28 | loss: 0.23118 | val_0_rmse: 0.48028 | val_1_rmse: 0.50877 |  0:00:07s
epoch 29 | loss: 0.23128 | val_0_rmse: 0.46076 | val_1_rmse: 0.51861 |  0:00:07s
epoch 30 | loss: 0.23751 | val_0_rmse: 0.46436 | val_1_rmse: 0.52075 |  0:00:07s
epoch 31 | loss: 0.2241  | val_0_rmse: 0.47106 | val_1_rmse: 0.50578 |  0:00:08s
epoch 32 | loss: 0.22688 | val_0_rmse: 0.44547 | val_1_rmse: 0.51836 |  0:00:08s
epoch 33 | loss: 0.21526 | val_0_rmse: 0.45463 | val_1_rmse: 0.54949 |  0:00:08s
epoch 34 | loss: 0.23124 | val_0_rmse: 0.44352 | val_1_rmse: 0.50406 |  0:00:08s
epoch 35 | loss: 0.2165  | val_0_rmse: 0.43805 | val_1_rmse: 0.49704 |  0:00:09s
epoch 36 | loss: 0.21501 | val_0_rmse: 0.43615 | val_1_rmse: 0.50744 |  0:00:09s
epoch 37 | loss: 0.2109  | val_0_rmse: 0.4416  | val_1_rmse: 0.50422 |  0:00:09s
epoch 38 | loss: 0.21355 | val_0_rmse: 0.44032 | val_1_rmse: 0.51079 |  0:00:09s
epoch 39 | loss: 0.20029 | val_0_rmse: 0.44602 | val_1_rmse: 0.54532 |  0:00:10s
epoch 40 | loss: 0.20104 | val_0_rmse: 0.4258  | val_1_rmse: 0.49639 |  0:00:10s
epoch 41 | loss: 0.22003 | val_0_rmse: 0.42865 | val_1_rmse: 0.49898 |  0:00:10s
epoch 42 | loss: 0.21782 | val_0_rmse: 0.44249 | val_1_rmse: 0.53661 |  0:00:10s
epoch 43 | loss: 0.20661 | val_0_rmse: 0.4633  | val_1_rmse: 0.51345 |  0:00:11s
epoch 44 | loss: 0.22424 | val_0_rmse: 0.46492 | val_1_rmse: 0.51284 |  0:00:11s
epoch 45 | loss: 0.20996 | val_0_rmse: 0.46096 | val_1_rmse: 0.53079 |  0:00:11s
epoch 46 | loss: 0.21514 | val_0_rmse: 0.46094 | val_1_rmse: 0.48483 |  0:00:11s
epoch 47 | loss: 0.20858 | val_0_rmse: 0.46249 | val_1_rmse: 0.48584 |  0:00:12s
epoch 48 | loss: 0.21575 | val_0_rmse: 0.44269 | val_1_rmse: 0.49653 |  0:00:12s
epoch 49 | loss: 0.20879 | val_0_rmse: 0.44814 | val_1_rmse: 0.52086 |  0:00:12s
epoch 50 | loss: 0.21535 | val_0_rmse: 0.44571 | val_1_rmse: 0.52297 |  0:00:12s
epoch 51 | loss: 0.20407 | val_0_rmse: 0.43743 | val_1_rmse: 0.51923 |  0:00:12s
epoch 52 | loss: 0.21223 | val_0_rmse: 0.43627 | val_1_rmse: 0.50838 |  0:00:13s
epoch 53 | loss: 0.20816 | val_0_rmse: 0.44921 | val_1_rmse: 0.50896 |  0:00:13s
epoch 54 | loss: 0.20878 | val_0_rmse: 0.45984 | val_1_rmse: 0.5115  |  0:00:13s
epoch 55 | loss: 0.21525 | val_0_rmse: 0.4619  | val_1_rmse: 0.53482 |  0:00:13s
epoch 56 | loss: 0.21255 | val_0_rmse: 0.44422 | val_1_rmse: 0.54711 |  0:00:14s
epoch 57 | loss: 0.2045  | val_0_rmse: 0.46276 | val_1_rmse: 0.51381 |  0:00:14s
epoch 58 | loss: 0.21545 | val_0_rmse: 0.47255 | val_1_rmse: 0.49771 |  0:00:14s
epoch 59 | loss: 0.19756 | val_0_rmse: 0.43755 | val_1_rmse: 0.47637 |  0:00:15s
epoch 60 | loss: 0.2079  | val_0_rmse: 0.45108 | val_1_rmse: 0.48317 |  0:00:15s
epoch 61 | loss: 0.20588 | val_0_rmse: 0.46    | val_1_rmse: 0.49776 |  0:00:15s
epoch 62 | loss: 0.20553 | val_0_rmse: 0.4204  | val_1_rmse: 0.49585 |  0:00:15s
epoch 63 | loss: 0.18605 | val_0_rmse: 0.42629 | val_1_rmse: 0.50974 |  0:00:15s
epoch 64 | loss: 0.19573 | val_0_rmse: 0.42645 | val_1_rmse: 0.49145 |  0:00:16s
epoch 65 | loss: 0.19111 | val_0_rmse: 0.43007 | val_1_rmse: 0.49533 |  0:00:16s
epoch 66 | loss: 0.19074 | val_0_rmse: 0.42409 | val_1_rmse: 0.47506 |  0:00:16s
epoch 67 | loss: 0.19171 | val_0_rmse: 0.41242 | val_1_rmse: 0.46643 |  0:00:16s
epoch 68 | loss: 0.18787 | val_0_rmse: 0.40668 | val_1_rmse: 0.51171 |  0:00:17s
epoch 69 | loss: 0.18705 | val_0_rmse: 0.40704 | val_1_rmse: 0.49824 |  0:00:17s
epoch 70 | loss: 0.18974 | val_0_rmse: 0.40557 | val_1_rmse: 0.49748 |  0:00:17s
epoch 71 | loss: 0.18753 | val_0_rmse: 0.40269 | val_1_rmse: 0.47957 |  0:00:17s
epoch 72 | loss: 0.18655 | val_0_rmse: 0.40241 | val_1_rmse: 0.45639 |  0:00:18s
epoch 73 | loss: 0.17786 | val_0_rmse: 0.40238 | val_1_rmse: 0.4799  |  0:00:18s
epoch 74 | loss: 0.18719 | val_0_rmse: 0.40991 | val_1_rmse: 0.48737 |  0:00:18s
epoch 75 | loss: 0.1724  | val_0_rmse: 0.39648 | val_1_rmse: 0.48564 |  0:00:18s
epoch 76 | loss: 0.17176 | val_0_rmse: 0.39657 | val_1_rmse: 0.47428 |  0:00:19s
epoch 77 | loss: 0.17193 | val_0_rmse: 0.40247 | val_1_rmse: 0.47083 |  0:00:19s
epoch 78 | loss: 0.17934 | val_0_rmse: 0.39879 | val_1_rmse: 0.47882 |  0:00:19s
epoch 79 | loss: 0.19144 | val_0_rmse: 0.41252 | val_1_rmse: 0.47888 |  0:00:19s
epoch 80 | loss: 0.18461 | val_0_rmse: 0.4279  | val_1_rmse: 0.47076 |  0:00:20s
epoch 81 | loss: 0.19028 | val_0_rmse: 0.39555 | val_1_rmse: 0.4837  |  0:00:20s
epoch 82 | loss: 0.19579 | val_0_rmse: 0.40404 | val_1_rmse: 0.47604 |  0:00:20s
epoch 83 | loss: 0.18066 | val_0_rmse: 0.43411 | val_1_rmse: 0.4888  |  0:00:20s
epoch 84 | loss: 0.18717 | val_0_rmse: 0.40432 | val_1_rmse: 0.49731 |  0:00:21s
epoch 85 | loss: 0.19588 | val_0_rmse: 0.39779 | val_1_rmse: 0.48399 |  0:00:21s
epoch 86 | loss: 0.18589 | val_0_rmse: 0.40841 | val_1_rmse: 0.47727 |  0:00:21s
epoch 87 | loss: 0.18198 | val_0_rmse: 0.39726 | val_1_rmse: 0.48245 |  0:00:21s
epoch 88 | loss: 0.18133 | val_0_rmse: 0.39355 | val_1_rmse: 0.47953 |  0:00:22s
epoch 89 | loss: 0.17862 | val_0_rmse: 0.41511 | val_1_rmse: 0.47507 |  0:00:22s
epoch 90 | loss: 0.18664 | val_0_rmse: 0.39317 | val_1_rmse: 0.48629 |  0:00:22s
epoch 91 | loss: 0.17801 | val_0_rmse: 0.39441 | val_1_rmse: 0.50817 |  0:00:22s
epoch 92 | loss: 0.1821  | val_0_rmse: 0.39219 | val_1_rmse: 0.46868 |  0:00:23s
epoch 93 | loss: 0.18952 | val_0_rmse: 0.3893  | val_1_rmse: 0.47645 |  0:00:23s
epoch 94 | loss: 0.18095 | val_0_rmse: 0.39332 | val_1_rmse: 0.49649 |  0:00:23s
epoch 95 | loss: 0.17191 | val_0_rmse: 0.38108 | val_1_rmse: 0.45704 |  0:00:23s
epoch 96 | loss: 0.17785 | val_0_rmse: 0.37448 | val_1_rmse: 0.45102 |  0:00:23s
epoch 97 | loss: 0.16295 | val_0_rmse: 0.38789 | val_1_rmse: 0.48832 |  0:00:24s
epoch 98 | loss: 0.1693  | val_0_rmse: 0.37077 | val_1_rmse: 0.47963 |  0:00:24s
epoch 99 | loss: 0.16102 | val_0_rmse: 0.39444 | val_1_rmse: 0.49467 |  0:00:24s
epoch 100| loss: 0.16779 | val_0_rmse: 0.37016 | val_1_rmse: 0.47961 |  0:00:24s
epoch 101| loss: 0.14987 | val_0_rmse: 0.37386 | val_1_rmse: 0.48862 |  0:00:25s
epoch 102| loss: 0.15582 | val_0_rmse: 0.37133 | val_1_rmse: 0.46004 |  0:00:25s
epoch 103| loss: 0.15112 | val_0_rmse: 0.37698 | val_1_rmse: 0.47248 |  0:00:25s
epoch 104| loss: 0.15887 | val_0_rmse: 0.38405 | val_1_rmse: 0.48813 |  0:00:25s
epoch 105| loss: 0.16338 | val_0_rmse: 0.37412 | val_1_rmse: 0.47162 |  0:00:26s
epoch 106| loss: 0.15183 | val_0_rmse: 0.37434 | val_1_rmse: 0.47497 |  0:00:26s
epoch 107| loss: 0.18603 | val_0_rmse: 0.37147 | val_1_rmse: 0.48678 |  0:00:26s
epoch 108| loss: 0.15625 | val_0_rmse: 0.36114 | val_1_rmse: 0.45999 |  0:00:26s
epoch 109| loss: 0.15553 | val_0_rmse: 0.3688  | val_1_rmse: 0.46311 |  0:00:27s
epoch 110| loss: 0.15341 | val_0_rmse: 0.36585 | val_1_rmse: 0.46995 |  0:00:27s
epoch 111| loss: 0.1541  | val_0_rmse: 0.3587  | val_1_rmse: 0.46578 |  0:00:27s
epoch 112| loss: 0.15899 | val_0_rmse: 0.35977 | val_1_rmse: 0.47751 |  0:00:27s
epoch 113| loss: 0.15397 | val_0_rmse: 0.35898 | val_1_rmse: 0.45103 |  0:00:28s
epoch 114| loss: 0.15683 | val_0_rmse: 0.37901 | val_1_rmse: 0.45245 |  0:00:28s
epoch 115| loss: 0.15375 | val_0_rmse: 0.35653 | val_1_rmse: 0.47066 |  0:00:28s
epoch 116| loss: 0.15937 | val_0_rmse: 0.36192 | val_1_rmse: 0.47919 |  0:00:28s
epoch 117| loss: 0.14776 | val_0_rmse: 0.38609 | val_1_rmse: 0.4611  |  0:00:29s
epoch 118| loss: 0.16374 | val_0_rmse: 0.38032 | val_1_rmse: 0.46853 |  0:00:29s
epoch 119| loss: 0.15507 | val_0_rmse: 0.36364 | val_1_rmse: 0.4706  |  0:00:29s
epoch 120| loss: 0.15335 | val_0_rmse: 0.355   | val_1_rmse: 0.47631 |  0:00:29s
epoch 121| loss: 0.14772 | val_0_rmse: 0.35276 | val_1_rmse: 0.47326 |  0:00:30s
epoch 122| loss: 0.13699 | val_0_rmse: 0.36274 | val_1_rmse: 0.47084 |  0:00:30s
epoch 123| loss: 0.14845 | val_0_rmse: 0.36217 | val_1_rmse: 0.47705 |  0:00:30s
epoch 124| loss: 0.15304 | val_0_rmse: 0.38312 | val_1_rmse: 0.48145 |  0:00:30s
epoch 125| loss: 0.15565 | val_0_rmse: 0.38327 | val_1_rmse: 0.47907 |  0:00:31s
epoch 126| loss: 0.16483 | val_0_rmse: 0.38243 | val_1_rmse: 0.49738 |  0:00:31s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.45102
Best weights from best epoch are automatically used!
ended training at: 02:45:42
Feature importance:
[('Area', 0.05454485898929464), ('Baths', 0.06514941670833546), ('Beds', 0.10240675741347473), ('Latitude', 0.1986499175059158), ('Longitude', 0.2221793252105), ('Month', 0.014707960836652165), ('Year', 0.3423617633358272)]
Mean squared error is of 16152227816.61878
Mean absolute error:92246.30664965662
MAPE:0.34399343468560534
R2 score:0.734507765722662
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:45:42
epoch 0  | loss: 1.4473  | val_0_rmse: 1.4475  | val_1_rmse: 1.47606 |  0:00:00s
epoch 1  | loss: 0.92076 | val_0_rmse: 1.17462 | val_1_rmse: 1.14471 |  0:00:00s
epoch 2  | loss: 0.64142 | val_0_rmse: 0.88574 | val_1_rmse: 0.84413 |  0:00:00s
epoch 3  | loss: 0.55336 | val_0_rmse: 0.91028 | val_1_rmse: 0.89906 |  0:00:01s
epoch 4  | loss: 0.53485 | val_0_rmse: 0.76343 | val_1_rmse: 0.72728 |  0:00:01s
epoch 5  | loss: 0.4711  | val_0_rmse: 0.81881 | val_1_rmse: 0.81209 |  0:00:01s
epoch 6  | loss: 0.4446  | val_0_rmse: 0.76566 | val_1_rmse: 0.78034 |  0:00:01s
epoch 7  | loss: 0.41947 | val_0_rmse: 0.72253 | val_1_rmse: 0.74856 |  0:00:02s
epoch 8  | loss: 0.39168 | val_0_rmse: 0.68174 | val_1_rmse: 0.70136 |  0:00:02s
epoch 9  | loss: 0.37808 | val_0_rmse: 0.64562 | val_1_rmse: 0.68751 |  0:00:02s
epoch 10 | loss: 0.38453 | val_0_rmse: 0.62655 | val_1_rmse: 0.69697 |  0:00:02s
epoch 11 | loss: 0.35413 | val_0_rmse: 0.64308 | val_1_rmse: 0.71342 |  0:00:02s
epoch 12 | loss: 0.34169 | val_0_rmse: 0.64581 | val_1_rmse: 0.71506 |  0:00:03s
epoch 13 | loss: 0.33821 | val_0_rmse: 0.60326 | val_1_rmse: 0.65586 |  0:00:03s
epoch 14 | loss: 0.34443 | val_0_rmse: 0.58075 | val_1_rmse: 0.63002 |  0:00:03s
epoch 15 | loss: 0.33523 | val_0_rmse: 0.57195 | val_1_rmse: 0.62702 |  0:00:04s
epoch 16 | loss: 0.32432 | val_0_rmse: 0.57898 | val_1_rmse: 0.6245  |  0:00:04s
epoch 17 | loss: 0.31126 | val_0_rmse: 0.58677 | val_1_rmse: 0.6372  |  0:00:04s
epoch 18 | loss: 0.30573 | val_0_rmse: 0.55847 | val_1_rmse: 0.62466 |  0:00:04s
epoch 19 | loss: 0.31037 | val_0_rmse: 0.55615 | val_1_rmse: 0.63192 |  0:00:05s
epoch 20 | loss: 0.30603 | val_0_rmse: 0.54821 | val_1_rmse: 0.6194  |  0:00:05s
epoch 21 | loss: 0.29249 | val_0_rmse: 0.53544 | val_1_rmse: 0.61156 |  0:00:05s
epoch 22 | loss: 0.30041 | val_0_rmse: 0.54395 | val_1_rmse: 0.62739 |  0:00:05s
epoch 23 | loss: 0.28114 | val_0_rmse: 0.54888 | val_1_rmse: 0.6207  |  0:00:06s
epoch 24 | loss: 0.27527 | val_0_rmse: 0.5329  | val_1_rmse: 0.60265 |  0:00:06s
epoch 25 | loss: 0.27275 | val_0_rmse: 0.51332 | val_1_rmse: 0.59858 |  0:00:06s
epoch 26 | loss: 0.27019 | val_0_rmse: 0.51175 | val_1_rmse: 0.61154 |  0:00:06s
epoch 27 | loss: 0.26961 | val_0_rmse: 0.50206 | val_1_rmse: 0.59222 |  0:00:07s
epoch 28 | loss: 0.26743 | val_0_rmse: 0.50374 | val_1_rmse: 0.60002 |  0:00:07s
epoch 29 | loss: 0.25342 | val_0_rmse: 0.49327 | val_1_rmse: 0.58281 |  0:00:07s
epoch 30 | loss: 0.25553 | val_0_rmse: 0.4818  | val_1_rmse: 0.56212 |  0:00:07s
epoch 31 | loss: 0.25657 | val_0_rmse: 0.48546 | val_1_rmse: 0.55186 |  0:00:08s
epoch 32 | loss: 0.26396 | val_0_rmse: 0.47759 | val_1_rmse: 0.54699 |  0:00:08s
epoch 33 | loss: 0.25402 | val_0_rmse: 0.47403 | val_1_rmse: 0.58769 |  0:00:08s
epoch 34 | loss: 0.24816 | val_0_rmse: 0.47992 | val_1_rmse: 0.59044 |  0:00:08s
epoch 35 | loss: 0.25046 | val_0_rmse: 0.47603 | val_1_rmse: 0.54389 |  0:00:09s
epoch 36 | loss: 0.23896 | val_0_rmse: 0.48337 | val_1_rmse: 0.54062 |  0:00:09s
epoch 37 | loss: 0.24496 | val_0_rmse: 0.48185 | val_1_rmse: 0.52195 |  0:00:09s
epoch 38 | loss: 0.23536 | val_0_rmse: 0.46771 | val_1_rmse: 0.52266 |  0:00:09s
epoch 39 | loss: 0.23359 | val_0_rmse: 0.47342 | val_1_rmse: 0.54072 |  0:00:10s
epoch 40 | loss: 0.23405 | val_0_rmse: 0.4647  | val_1_rmse: 0.53104 |  0:00:10s
epoch 41 | loss: 0.23685 | val_0_rmse: 0.47091 | val_1_rmse: 0.53734 |  0:00:10s
epoch 42 | loss: 0.23415 | val_0_rmse: 0.46046 | val_1_rmse: 0.54236 |  0:00:10s
epoch 43 | loss: 0.23326 | val_0_rmse: 0.45427 | val_1_rmse: 0.54632 |  0:00:11s
epoch 44 | loss: 0.2199  | val_0_rmse: 0.47686 | val_1_rmse: 0.58279 |  0:00:11s
epoch 45 | loss: 0.23499 | val_0_rmse: 0.46908 | val_1_rmse: 0.57391 |  0:00:11s
epoch 46 | loss: 0.23339 | val_0_rmse: 0.457   | val_1_rmse: 0.60016 |  0:00:11s
epoch 47 | loss: 0.21888 | val_0_rmse: 0.45391 | val_1_rmse: 0.57472 |  0:00:12s
epoch 48 | loss: 0.22862 | val_0_rmse: 0.44924 | val_1_rmse: 0.52903 |  0:00:12s
epoch 49 | loss: 0.21522 | val_0_rmse: 0.44787 | val_1_rmse: 0.53381 |  0:00:12s
epoch 50 | loss: 0.21901 | val_0_rmse: 0.47994 | val_1_rmse: 0.57021 |  0:00:12s
epoch 51 | loss: 0.21866 | val_0_rmse: 0.43957 | val_1_rmse: 0.52799 |  0:00:13s
epoch 52 | loss: 0.22793 | val_0_rmse: 0.43413 | val_1_rmse: 0.51284 |  0:00:13s
epoch 53 | loss: 0.19993 | val_0_rmse: 0.4806  | val_1_rmse: 0.5236  |  0:00:13s
epoch 54 | loss: 0.2278  | val_0_rmse: 0.44395 | val_1_rmse: 0.50708 |  0:00:13s
epoch 55 | loss: 0.2147  | val_0_rmse: 0.44603 | val_1_rmse: 0.49665 |  0:00:14s
epoch 56 | loss: 0.20427 | val_0_rmse: 0.43991 | val_1_rmse: 0.5145  |  0:00:14s
epoch 57 | loss: 0.21992 | val_0_rmse: 0.4357  | val_1_rmse: 0.49914 |  0:00:14s
epoch 58 | loss: 0.21285 | val_0_rmse: 0.43523 | val_1_rmse: 0.48606 |  0:00:14s
epoch 59 | loss: 0.21149 | val_0_rmse: 0.42738 | val_1_rmse: 0.48213 |  0:00:15s
epoch 60 | loss: 0.20119 | val_0_rmse: 0.43633 | val_1_rmse: 0.49091 |  0:00:15s
epoch 61 | loss: 0.19944 | val_0_rmse: 0.42898 | val_1_rmse: 0.48835 |  0:00:15s
epoch 62 | loss: 0.19945 | val_0_rmse: 0.41985 | val_1_rmse: 0.48495 |  0:00:15s
epoch 63 | loss: 0.20148 | val_0_rmse: 0.43897 | val_1_rmse: 0.50075 |  0:00:16s
epoch 64 | loss: 0.19601 | val_0_rmse: 0.41456 | val_1_rmse: 0.48667 |  0:00:16s
epoch 65 | loss: 0.18878 | val_0_rmse: 0.41059 | val_1_rmse: 0.49665 |  0:00:16s
epoch 66 | loss: 0.19007 | val_0_rmse: 0.45035 | val_1_rmse: 0.54693 |  0:00:16s
epoch 67 | loss: 0.20032 | val_0_rmse: 0.41216 | val_1_rmse: 0.50244 |  0:00:17s
epoch 68 | loss: 0.19996 | val_0_rmse: 0.40932 | val_1_rmse: 0.49204 |  0:00:17s
epoch 69 | loss: 0.19998 | val_0_rmse: 0.4413  | val_1_rmse: 0.54256 |  0:00:17s
epoch 70 | loss: 0.20204 | val_0_rmse: 0.42861 | val_1_rmse: 0.50834 |  0:00:17s
epoch 71 | loss: 0.20962 | val_0_rmse: 0.42429 | val_1_rmse: 0.50245 |  0:00:18s
epoch 72 | loss: 0.19746 | val_0_rmse: 0.44405 | val_1_rmse: 0.53659 |  0:00:18s
epoch 73 | loss: 0.20757 | val_0_rmse: 0.43091 | val_1_rmse: 0.50972 |  0:00:18s
epoch 74 | loss: 0.18442 | val_0_rmse: 0.4198  | val_1_rmse: 0.49612 |  0:00:18s
epoch 75 | loss: 0.20054 | val_0_rmse: 0.46356 | val_1_rmse: 0.53102 |  0:00:19s
epoch 76 | loss: 0.20701 | val_0_rmse: 0.47294 | val_1_rmse: 0.54376 |  0:00:19s
epoch 77 | loss: 0.21157 | val_0_rmse: 0.43552 | val_1_rmse: 0.51066 |  0:00:19s
epoch 78 | loss: 0.19864 | val_0_rmse: 0.42854 | val_1_rmse: 0.49563 |  0:00:19s
epoch 79 | loss: 0.19221 | val_0_rmse: 0.40679 | val_1_rmse: 0.47032 |  0:00:20s
epoch 80 | loss: 0.18548 | val_0_rmse: 0.40761 | val_1_rmse: 0.46447 |  0:00:20s
epoch 81 | loss: 0.17928 | val_0_rmse: 0.41041 | val_1_rmse: 0.47935 |  0:00:20s
epoch 82 | loss: 0.18593 | val_0_rmse: 0.41683 | val_1_rmse: 0.4938  |  0:00:20s
epoch 83 | loss: 0.17813 | val_0_rmse: 0.40199 | val_1_rmse: 0.49725 |  0:00:21s
epoch 84 | loss: 0.17709 | val_0_rmse: 0.39594 | val_1_rmse: 0.47959 |  0:00:21s
epoch 85 | loss: 0.18283 | val_0_rmse: 0.40535 | val_1_rmse: 0.48655 |  0:00:21s
epoch 86 | loss: 0.18539 | val_0_rmse: 0.39503 | val_1_rmse: 0.49699 |  0:00:21s
epoch 87 | loss: 0.17814 | val_0_rmse: 0.39852 | val_1_rmse: 0.49292 |  0:00:22s
epoch 88 | loss: 0.17358 | val_0_rmse: 0.39113 | val_1_rmse: 0.46727 |  0:00:22s
epoch 89 | loss: 0.17294 | val_0_rmse: 0.3987  | val_1_rmse: 0.47348 |  0:00:22s
epoch 90 | loss: 0.1748  | val_0_rmse: 0.40287 | val_1_rmse: 0.4783  |  0:00:22s
epoch 91 | loss: 0.18651 | val_0_rmse: 0.40547 | val_1_rmse: 0.48179 |  0:00:23s
epoch 92 | loss: 0.1822  | val_0_rmse: 0.3781  | val_1_rmse: 0.46681 |  0:00:23s
epoch 93 | loss: 0.1718  | val_0_rmse: 0.38941 | val_1_rmse: 0.48127 |  0:00:23s
epoch 94 | loss: 0.17453 | val_0_rmse: 0.38841 | val_1_rmse: 0.48531 |  0:00:23s
epoch 95 | loss: 0.16249 | val_0_rmse: 0.38127 | val_1_rmse: 0.49355 |  0:00:24s
epoch 96 | loss: 0.16579 | val_0_rmse: 0.38026 | val_1_rmse: 0.49658 |  0:00:24s
epoch 97 | loss: 0.16461 | val_0_rmse: 0.37479 | val_1_rmse: 0.48775 |  0:00:24s
epoch 98 | loss: 0.16405 | val_0_rmse: 0.387   | val_1_rmse: 0.48981 |  0:00:24s
epoch 99 | loss: 0.1811  | val_0_rmse: 0.3857  | val_1_rmse: 0.48556 |  0:00:25s
epoch 100| loss: 0.1818  | val_0_rmse: 0.40129 | val_1_rmse: 0.49546 |  0:00:25s
epoch 101| loss: 0.17714 | val_0_rmse: 0.39054 | val_1_rmse: 0.48547 |  0:00:25s
epoch 102| loss: 0.16623 | val_0_rmse: 0.39631 | val_1_rmse: 0.48228 |  0:00:25s
epoch 103| loss: 0.16517 | val_0_rmse: 0.38667 | val_1_rmse: 0.46438 |  0:00:25s
epoch 104| loss: 0.16633 | val_0_rmse: 0.40307 | val_1_rmse: 0.49355 |  0:00:26s
epoch 105| loss: 0.17672 | val_0_rmse: 0.3965  | val_1_rmse: 0.50026 |  0:00:26s
epoch 106| loss: 0.16729 | val_0_rmse: 0.38749 | val_1_rmse: 0.47601 |  0:00:26s
epoch 107| loss: 0.17525 | val_0_rmse: 0.39148 | val_1_rmse: 0.48227 |  0:00:26s
epoch 108| loss: 0.16566 | val_0_rmse: 0.39281 | val_1_rmse: 0.49815 |  0:00:27s
epoch 109| loss: 0.1696  | val_0_rmse: 0.38854 | val_1_rmse: 0.50473 |  0:00:27s
epoch 110| loss: 0.16899 | val_0_rmse: 0.37854 | val_1_rmse: 0.49847 |  0:00:27s
epoch 111| loss: 0.16041 | val_0_rmse: 0.378   | val_1_rmse: 0.49068 |  0:00:27s
epoch 112| loss: 0.15945 | val_0_rmse: 0.37432 | val_1_rmse: 0.49848 |  0:00:28s
epoch 113| loss: 0.16583 | val_0_rmse: 0.36216 | val_1_rmse: 0.49209 |  0:00:28s
epoch 114| loss: 0.16055 | val_0_rmse: 0.3928  | val_1_rmse: 0.4994  |  0:00:28s
epoch 115| loss: 0.16849 | val_0_rmse: 0.36461 | val_1_rmse: 0.48044 |  0:00:28s
epoch 116| loss: 0.1592  | val_0_rmse: 0.38211 | val_1_rmse: 0.4901  |  0:00:29s
epoch 117| loss: 0.168   | val_0_rmse: 0.37219 | val_1_rmse: 0.47097 |  0:00:29s
epoch 118| loss: 0.15915 | val_0_rmse: 0.38857 | val_1_rmse: 0.49627 |  0:00:29s
epoch 119| loss: 0.16083 | val_0_rmse: 0.38175 | val_1_rmse: 0.50638 |  0:00:29s
epoch 120| loss: 0.17034 | val_0_rmse: 0.39143 | val_1_rmse: 0.50488 |  0:00:30s
epoch 121| loss: 0.16171 | val_0_rmse: 0.37961 | val_1_rmse: 0.47189 |  0:00:30s
epoch 122| loss: 0.17114 | val_0_rmse: 0.38031 | val_1_rmse: 0.46344 |  0:00:30s
epoch 123| loss: 0.1639  | val_0_rmse: 0.40395 | val_1_rmse: 0.51095 |  0:00:30s
epoch 124| loss: 0.17642 | val_0_rmse: 0.36811 | val_1_rmse: 0.47016 |  0:00:31s
epoch 125| loss: 0.15328 | val_0_rmse: 0.37907 | val_1_rmse: 0.47539 |  0:00:31s
epoch 126| loss: 0.15624 | val_0_rmse: 0.38942 | val_1_rmse: 0.48902 |  0:00:31s
epoch 127| loss: 0.15997 | val_0_rmse: 0.38062 | val_1_rmse: 0.49489 |  0:00:31s
epoch 128| loss: 0.16637 | val_0_rmse: 0.36731 | val_1_rmse: 0.48268 |  0:00:32s
epoch 129| loss: 0.1612  | val_0_rmse: 0.36768 | val_1_rmse: 0.47731 |  0:00:32s
epoch 130| loss: 0.15283 | val_0_rmse: 0.364   | val_1_rmse: 0.48284 |  0:00:32s
epoch 131| loss: 0.15609 | val_0_rmse: 0.37301 | val_1_rmse: 0.49238 |  0:00:33s
epoch 132| loss: 0.15648 | val_0_rmse: 0.36038 | val_1_rmse: 0.47685 |  0:00:33s
epoch 133| loss: 0.16045 | val_0_rmse: 0.36407 | val_1_rmse: 0.46456 |  0:00:33s
epoch 134| loss: 0.14985 | val_0_rmse: 0.37796 | val_1_rmse: 0.46967 |  0:00:33s
epoch 135| loss: 0.15414 | val_0_rmse: 0.37521 | val_1_rmse: 0.48692 |  0:00:33s
epoch 136| loss: 0.15408 | val_0_rmse: 0.36398 | val_1_rmse: 0.48788 |  0:00:34s
epoch 137| loss: 0.15865 | val_0_rmse: 0.37141 | val_1_rmse: 0.50381 |  0:00:34s
epoch 138| loss: 0.15002 | val_0_rmse: 0.36209 | val_1_rmse: 0.49157 |  0:00:34s
epoch 139| loss: 0.14965 | val_0_rmse: 0.3619  | val_1_rmse: 0.48596 |  0:00:34s
epoch 140| loss: 0.15192 | val_0_rmse: 0.35584 | val_1_rmse: 0.493   |  0:00:35s
epoch 141| loss: 0.14967 | val_0_rmse: 0.34713 | val_1_rmse: 0.5148  |  0:00:35s
epoch 142| loss: 0.14725 | val_0_rmse: 0.3558  | val_1_rmse: 0.50939 |  0:00:35s
epoch 143| loss: 0.1522  | val_0_rmse: 0.34319 | val_1_rmse: 0.4763  |  0:00:35s
epoch 144| loss: 0.14118 | val_0_rmse: 0.35464 | val_1_rmse: 0.46561 |  0:00:36s
epoch 145| loss: 0.14914 | val_0_rmse: 0.35965 | val_1_rmse: 0.46844 |  0:00:36s
epoch 146| loss: 0.15224 | val_0_rmse: 0.34667 | val_1_rmse: 0.4769  |  0:00:36s
epoch 147| loss: 0.13909 | val_0_rmse: 0.3592  | val_1_rmse: 0.49579 |  0:00:36s
epoch 148| loss: 0.16151 | val_0_rmse: 0.35326 | val_1_rmse: 0.47513 |  0:00:37s
epoch 149| loss: 0.15755 | val_0_rmse: 0.36426 | val_1_rmse: 0.47947 |  0:00:37s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.46344
Best weights from best epoch are automatically used!
ended training at: 02:46:20
Feature importance:
[('Area', 0.05207806197092933), ('Baths', 0.1432959391834336), ('Beds', 0.05874017133560254), ('Latitude', 0.26328697934610895), ('Longitude', 0.2490083579010544), ('Month', 2.357194229388863e-06), ('Year', 0.23358813306864176)]
Mean squared error is of 17014001284.808111
Mean absolute error:93589.70871277472
MAPE:0.35227990216400384
R2 score:0.7358875032452146
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:46:20
epoch 0  | loss: 1.58587 | val_0_rmse: 1.16288 | val_1_rmse: 1.19266 |  0:00:00s
epoch 1  | loss: 0.70126 | val_0_rmse: 0.96529 | val_1_rmse: 0.90245 |  0:00:00s
epoch 2  | loss: 0.58114 | val_0_rmse: 0.85599 | val_1_rmse: 0.88338 |  0:00:00s
epoch 3  | loss: 0.50453 | val_0_rmse: 0.8416  | val_1_rmse: 0.81437 |  0:00:01s
epoch 4  | loss: 0.4599  | val_0_rmse: 0.78151 | val_1_rmse: 0.7536  |  0:00:01s
epoch 5  | loss: 0.43238 | val_0_rmse: 0.76782 | val_1_rmse: 0.75553 |  0:00:01s
epoch 6  | loss: 0.39539 | val_0_rmse: 0.70708 | val_1_rmse: 0.72255 |  0:00:01s
epoch 7  | loss: 0.38284 | val_0_rmse: 0.66014 | val_1_rmse: 0.6767  |  0:00:02s
epoch 8  | loss: 0.36946 | val_0_rmse: 0.68154 | val_1_rmse: 0.73709 |  0:00:02s
epoch 9  | loss: 0.35895 | val_0_rmse: 0.68153 | val_1_rmse: 0.76404 |  0:00:02s
epoch 10 | loss: 0.35899 | val_0_rmse: 0.66258 | val_1_rmse: 0.69616 |  0:00:02s
epoch 11 | loss: 0.35166 | val_0_rmse: 0.61607 | val_1_rmse: 0.65746 |  0:00:03s
epoch 12 | loss: 0.3409  | val_0_rmse: 0.60246 | val_1_rmse: 0.65275 |  0:00:03s
epoch 13 | loss: 0.34334 | val_0_rmse: 0.60911 | val_1_rmse: 0.65765 |  0:00:03s
epoch 14 | loss: 0.33022 | val_0_rmse: 0.60734 | val_1_rmse: 0.65502 |  0:00:03s
epoch 15 | loss: 0.32801 | val_0_rmse: 0.60268 | val_1_rmse: 0.6574  |  0:00:04s
epoch 16 | loss: 0.35449 | val_0_rmse: 0.56195 | val_1_rmse: 0.62812 |  0:00:04s
epoch 17 | loss: 0.32653 | val_0_rmse: 0.57591 | val_1_rmse: 0.6409  |  0:00:04s
epoch 18 | loss: 0.31142 | val_0_rmse: 0.56634 | val_1_rmse: 0.62704 |  0:00:04s
epoch 19 | loss: 0.29566 | val_0_rmse: 0.59317 | val_1_rmse: 0.65008 |  0:00:05s
epoch 20 | loss: 0.30323 | val_0_rmse: 0.5846  | val_1_rmse: 0.64658 |  0:00:05s
epoch 21 | loss: 0.28228 | val_0_rmse: 0.5764  | val_1_rmse: 0.62657 |  0:00:05s
epoch 22 | loss: 0.28859 | val_0_rmse: 0.5494  | val_1_rmse: 0.60596 |  0:00:05s
epoch 23 | loss: 0.28543 | val_0_rmse: 0.54673 | val_1_rmse: 0.61062 |  0:00:06s
epoch 24 | loss: 0.2843  | val_0_rmse: 0.55182 | val_1_rmse: 0.61318 |  0:00:06s
epoch 25 | loss: 0.27646 | val_0_rmse: 0.54937 | val_1_rmse: 0.60925 |  0:00:06s
epoch 26 | loss: 0.26259 | val_0_rmse: 0.55056 | val_1_rmse: 0.59331 |  0:00:06s
epoch 27 | loss: 0.27247 | val_0_rmse: 0.56125 | val_1_rmse: 0.59641 |  0:00:07s
epoch 28 | loss: 0.2759  | val_0_rmse: 0.52713 | val_1_rmse: 0.57096 |  0:00:07s
epoch 29 | loss: 0.27532 | val_0_rmse: 0.52528 | val_1_rmse: 0.57524 |  0:00:07s
epoch 30 | loss: 0.28648 | val_0_rmse: 0.51331 | val_1_rmse: 0.58178 |  0:00:07s
epoch 31 | loss: 0.26622 | val_0_rmse: 0.51214 | val_1_rmse: 0.57782 |  0:00:08s
epoch 32 | loss: 0.26339 | val_0_rmse: 0.50773 | val_1_rmse: 0.5831  |  0:00:08s
epoch 33 | loss: 0.26654 | val_0_rmse: 0.50939 | val_1_rmse: 0.57992 |  0:00:08s
epoch 34 | loss: 0.26813 | val_0_rmse: 0.49672 | val_1_rmse: 0.55973 |  0:00:08s
epoch 35 | loss: 0.27196 | val_0_rmse: 0.49079 | val_1_rmse: 0.56728 |  0:00:09s
epoch 36 | loss: 0.24631 | val_0_rmse: 0.49598 | val_1_rmse: 0.57113 |  0:00:09s
epoch 37 | loss: 0.27259 | val_0_rmse: 0.50387 | val_1_rmse: 0.56195 |  0:00:09s
epoch 38 | loss: 0.2579  | val_0_rmse: 0.49266 | val_1_rmse: 0.55753 |  0:00:09s
epoch 39 | loss: 0.25107 | val_0_rmse: 0.48895 | val_1_rmse: 0.56931 |  0:00:10s
epoch 40 | loss: 0.26795 | val_0_rmse: 0.49698 | val_1_rmse: 0.56015 |  0:00:10s
epoch 41 | loss: 0.24524 | val_0_rmse: 0.49697 | val_1_rmse: 0.56279 |  0:00:10s
epoch 42 | loss: 0.25646 | val_0_rmse: 0.49164 | val_1_rmse: 0.57042 |  0:00:10s
epoch 43 | loss: 0.26604 | val_0_rmse: 0.49982 | val_1_rmse: 0.55579 |  0:00:11s
epoch 44 | loss: 0.25125 | val_0_rmse: 0.53979 | val_1_rmse: 0.60184 |  0:00:11s
epoch 45 | loss: 0.28046 | val_0_rmse: 0.50079 | val_1_rmse: 0.57098 |  0:00:11s
epoch 46 | loss: 0.2694  | val_0_rmse: 0.50137 | val_1_rmse: 0.56329 |  0:00:11s
epoch 47 | loss: 0.26245 | val_0_rmse: 0.48554 | val_1_rmse: 0.55163 |  0:00:12s
epoch 48 | loss: 0.25958 | val_0_rmse: 0.48616 | val_1_rmse: 0.55499 |  0:00:12s
epoch 49 | loss: 0.25089 | val_0_rmse: 0.49768 | val_1_rmse: 0.56434 |  0:00:12s
epoch 50 | loss: 0.25248 | val_0_rmse: 0.49644 | val_1_rmse: 0.57234 |  0:00:12s
epoch 51 | loss: 0.25259 | val_0_rmse: 0.48028 | val_1_rmse: 0.56342 |  0:00:13s
epoch 52 | loss: 0.24656 | val_0_rmse: 0.4848  | val_1_rmse: 0.54831 |  0:00:13s
epoch 53 | loss: 0.25598 | val_0_rmse: 0.4948  | val_1_rmse: 0.5531  |  0:00:13s
epoch 54 | loss: 0.25852 | val_0_rmse: 0.52607 | val_1_rmse: 0.5648  |  0:00:13s
epoch 55 | loss: 0.25723 | val_0_rmse: 0.54645 | val_1_rmse: 0.58821 |  0:00:14s
epoch 56 | loss: 0.26051 | val_0_rmse: 0.49842 | val_1_rmse: 0.55067 |  0:00:14s
epoch 57 | loss: 0.2591  | val_0_rmse: 0.49363 | val_1_rmse: 0.55686 |  0:00:14s
epoch 58 | loss: 0.26244 | val_0_rmse: 0.49551 | val_1_rmse: 0.55785 |  0:00:14s
epoch 59 | loss: 0.25481 | val_0_rmse: 0.50102 | val_1_rmse: 0.56209 |  0:00:15s
epoch 60 | loss: 0.25199 | val_0_rmse: 0.50671 | val_1_rmse: 0.57798 |  0:00:15s
epoch 61 | loss: 0.26009 | val_0_rmse: 0.49145 | val_1_rmse: 0.56594 |  0:00:15s
epoch 62 | loss: 0.25538 | val_0_rmse: 0.49539 | val_1_rmse: 0.5657  |  0:00:15s
epoch 63 | loss: 0.24787 | val_0_rmse: 0.4847  | val_1_rmse: 0.54454 |  0:00:16s
epoch 64 | loss: 0.24547 | val_0_rmse: 0.49369 | val_1_rmse: 0.55379 |  0:00:16s
epoch 65 | loss: 0.25446 | val_0_rmse: 0.49776 | val_1_rmse: 0.57824 |  0:00:16s
epoch 66 | loss: 0.24528 | val_0_rmse: 0.47549 | val_1_rmse: 0.54933 |  0:00:16s
epoch 67 | loss: 0.24564 | val_0_rmse: 0.47348 | val_1_rmse: 0.54021 |  0:00:17s
epoch 68 | loss: 0.24705 | val_0_rmse: 0.47935 | val_1_rmse: 0.56237 |  0:00:17s
epoch 69 | loss: 0.24642 | val_0_rmse: 0.49561 | val_1_rmse: 0.57254 |  0:00:17s
epoch 70 | loss: 0.25638 | val_0_rmse: 0.50854 | val_1_rmse: 0.60109 |  0:00:17s
epoch 71 | loss: 0.25017 | val_0_rmse: 0.49247 | val_1_rmse: 0.57358 |  0:00:18s
epoch 72 | loss: 0.26347 | val_0_rmse: 0.49009 | val_1_rmse: 0.56364 |  0:00:18s
epoch 73 | loss: 0.2567  | val_0_rmse: 0.50095 | val_1_rmse: 0.57859 |  0:00:18s
epoch 74 | loss: 0.25598 | val_0_rmse: 0.50283 | val_1_rmse: 0.56603 |  0:00:18s
epoch 75 | loss: 0.26207 | val_0_rmse: 0.49804 | val_1_rmse: 0.56074 |  0:00:18s
epoch 76 | loss: 0.26075 | val_0_rmse: 0.49597 | val_1_rmse: 0.57227 |  0:00:19s
epoch 77 | loss: 0.24883 | val_0_rmse: 0.51374 | val_1_rmse: 0.59807 |  0:00:19s
epoch 78 | loss: 0.26286 | val_0_rmse: 0.49543 | val_1_rmse: 0.55091 |  0:00:19s
epoch 79 | loss: 0.24464 | val_0_rmse: 0.51702 | val_1_rmse: 0.57109 |  0:00:20s
epoch 80 | loss: 0.26284 | val_0_rmse: 0.48993 | val_1_rmse: 0.55945 |  0:00:20s
epoch 81 | loss: 0.25513 | val_0_rmse: 0.48162 | val_1_rmse: 0.55642 |  0:00:20s
epoch 82 | loss: 0.24751 | val_0_rmse: 0.49279 | val_1_rmse: 0.55194 |  0:00:20s
epoch 83 | loss: 0.24846 | val_0_rmse: 0.48836 | val_1_rmse: 0.54368 |  0:00:21s
epoch 84 | loss: 0.25062 | val_0_rmse: 0.48996 | val_1_rmse: 0.5634  |  0:00:21s
epoch 85 | loss: 0.24906 | val_0_rmse: 0.49367 | val_1_rmse: 0.55874 |  0:00:21s
epoch 86 | loss: 0.25254 | val_0_rmse: 0.49506 | val_1_rmse: 0.56191 |  0:00:21s
epoch 87 | loss: 0.24216 | val_0_rmse: 0.48352 | val_1_rmse: 0.55038 |  0:00:22s
epoch 88 | loss: 0.25193 | val_0_rmse: 0.48993 | val_1_rmse: 0.57205 |  0:00:22s
epoch 89 | loss: 0.27522 | val_0_rmse: 0.48727 | val_1_rmse: 0.58094 |  0:00:22s
epoch 90 | loss: 0.24529 | val_0_rmse: 0.48061 | val_1_rmse: 0.56101 |  0:00:22s
epoch 91 | loss: 0.25003 | val_0_rmse: 0.47762 | val_1_rmse: 0.56109 |  0:00:23s
epoch 92 | loss: 0.25844 | val_0_rmse: 0.48483 | val_1_rmse: 0.58832 |  0:00:23s
epoch 93 | loss: 0.25941 | val_0_rmse: 0.4976  | val_1_rmse: 0.60847 |  0:00:23s
epoch 94 | loss: 0.25094 | val_0_rmse: 0.4746  | val_1_rmse: 0.57237 |  0:00:23s
epoch 95 | loss: 0.23824 | val_0_rmse: 0.48195 | val_1_rmse: 0.55704 |  0:00:23s
epoch 96 | loss: 0.25403 | val_0_rmse: 0.4994  | val_1_rmse: 0.56694 |  0:00:24s
epoch 97 | loss: 0.24302 | val_0_rmse: 0.47662 | val_1_rmse: 0.56092 |  0:00:24s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.54021
Best weights from best epoch are automatically used!
ended training at: 02:46:44
Feature importance:
[('Area', 0.4224117734191252), ('Baths', 0.07607963921764829), ('Beds', 0.07417876309539309), ('Latitude', 0.3198565612019519), ('Longitude', 0.0318786725644724), ('Month', 0.029129494653411993), ('Year', 0.046465095847997115)]
Mean squared error is of 8544171117.560353
Mean absolute error:67936.02135576923
MAPE:0.1666648613393577
R2 score:0.7244426217757447
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:46:45
epoch 0  | loss: 1.51597 | val_0_rmse: 1.70176 | val_1_rmse: 1.65702 |  0:00:00s
epoch 1  | loss: 0.79278 | val_0_rmse: 0.96155 | val_1_rmse: 0.98272 |  0:00:00s
epoch 2  | loss: 0.60095 | val_0_rmse: 0.8739  | val_1_rmse: 0.916   |  0:00:00s
epoch 3  | loss: 0.54168 | val_0_rmse: 0.96893 | val_1_rmse: 1.04743 |  0:00:01s
epoch 4  | loss: 0.46865 | val_0_rmse: 0.81283 | val_1_rmse: 0.8395  |  0:00:01s
epoch 5  | loss: 0.42427 | val_0_rmse: 0.69574 | val_1_rmse: 0.70828 |  0:00:01s
epoch 6  | loss: 0.40575 | val_0_rmse: 0.70787 | val_1_rmse: 0.76236 |  0:00:01s
epoch 7  | loss: 0.38715 | val_0_rmse: 0.7251  | val_1_rmse: 0.76586 |  0:00:02s
epoch 8  | loss: 0.38207 | val_0_rmse: 0.66796 | val_1_rmse: 0.69904 |  0:00:02s
epoch 9  | loss: 0.36018 | val_0_rmse: 0.62144 | val_1_rmse: 0.65922 |  0:00:02s
epoch 10 | loss: 0.36738 | val_0_rmse: 0.59915 | val_1_rmse: 0.65211 |  0:00:02s
epoch 11 | loss: 0.35011 | val_0_rmse: 0.59486 | val_1_rmse: 0.65152 |  0:00:03s
epoch 12 | loss: 0.34186 | val_0_rmse: 0.58515 | val_1_rmse: 0.63522 |  0:00:03s
epoch 13 | loss: 0.33416 | val_0_rmse: 0.59718 | val_1_rmse: 0.64479 |  0:00:03s
epoch 14 | loss: 0.33961 | val_0_rmse: 0.60154 | val_1_rmse: 0.64813 |  0:00:03s
epoch 15 | loss: 0.34681 | val_0_rmse: 0.59309 | val_1_rmse: 0.63909 |  0:00:04s
epoch 16 | loss: 0.35051 | val_0_rmse: 0.59152 | val_1_rmse: 0.65161 |  0:00:04s
epoch 17 | loss: 0.3373  | val_0_rmse: 0.60704 | val_1_rmse: 0.67052 |  0:00:04s
epoch 18 | loss: 0.34065 | val_0_rmse: 0.58124 | val_1_rmse: 0.63644 |  0:00:04s
epoch 19 | loss: 0.32829 | val_0_rmse: 0.57892 | val_1_rmse: 0.62699 |  0:00:05s
epoch 20 | loss: 0.31653 | val_0_rmse: 0.59428 | val_1_rmse: 0.62864 |  0:00:05s
epoch 21 | loss: 0.32233 | val_0_rmse: 0.58516 | val_1_rmse: 0.63021 |  0:00:05s
epoch 22 | loss: 0.32381 | val_0_rmse: 0.55682 | val_1_rmse: 0.58433 |  0:00:05s
epoch 23 | loss: 0.31609 | val_0_rmse: 0.56112 | val_1_rmse: 0.60079 |  0:00:06s
epoch 24 | loss: 0.31757 | val_0_rmse: 0.54235 | val_1_rmse: 0.57917 |  0:00:06s
epoch 25 | loss: 0.30529 | val_0_rmse: 0.5493  | val_1_rmse: 0.58672 |  0:00:06s
epoch 26 | loss: 0.30481 | val_0_rmse: 0.55864 | val_1_rmse: 0.60029 |  0:00:06s
epoch 27 | loss: 0.31915 | val_0_rmse: 0.55219 | val_1_rmse: 0.59838 |  0:00:07s
epoch 28 | loss: 0.28775 | val_0_rmse: 0.54678 | val_1_rmse: 0.56883 |  0:00:07s
epoch 29 | loss: 0.31235 | val_0_rmse: 0.52807 | val_1_rmse: 0.54703 |  0:00:07s
epoch 30 | loss: 0.28447 | val_0_rmse: 0.5349  | val_1_rmse: 0.57398 |  0:00:07s
epoch 31 | loss: 0.29316 | val_0_rmse: 0.53105 | val_1_rmse: 0.5681  |  0:00:08s
epoch 32 | loss: 0.29184 | val_0_rmse: 0.53516 | val_1_rmse: 0.5831  |  0:00:08s
epoch 33 | loss: 0.30842 | val_0_rmse: 0.55148 | val_1_rmse: 0.60724 |  0:00:08s
epoch 34 | loss: 0.29959 | val_0_rmse: 0.52615 | val_1_rmse: 0.55797 |  0:00:08s
epoch 35 | loss: 0.29569 | val_0_rmse: 0.52826 | val_1_rmse: 0.54685 |  0:00:09s
epoch 36 | loss: 0.29076 | val_0_rmse: 0.56187 | val_1_rmse: 0.60206 |  0:00:09s
epoch 37 | loss: 0.28584 | val_0_rmse: 0.54698 | val_1_rmse: 0.58779 |  0:00:09s
epoch 38 | loss: 0.27729 | val_0_rmse: 0.52503 | val_1_rmse: 0.56002 |  0:00:09s
epoch 39 | loss: 0.28441 | val_0_rmse: 0.54527 | val_1_rmse: 0.58156 |  0:00:10s
epoch 40 | loss: 0.27897 | val_0_rmse: 0.55237 | val_1_rmse: 0.59115 |  0:00:10s
epoch 41 | loss: 0.27359 | val_0_rmse: 0.50895 | val_1_rmse: 0.54644 |  0:00:10s
epoch 42 | loss: 0.28314 | val_0_rmse: 0.51306 | val_1_rmse: 0.54423 |  0:00:10s
epoch 43 | loss: 0.26975 | val_0_rmse: 0.53666 | val_1_rmse: 0.55859 |  0:00:11s
epoch 44 | loss: 0.27147 | val_0_rmse: 0.51711 | val_1_rmse: 0.53724 |  0:00:11s
epoch 45 | loss: 0.26117 | val_0_rmse: 0.5078  | val_1_rmse: 0.53394 |  0:00:11s
epoch 46 | loss: 0.27376 | val_0_rmse: 0.51598 | val_1_rmse: 0.54443 |  0:00:11s
epoch 47 | loss: 0.26893 | val_0_rmse: 0.51595 | val_1_rmse: 0.54284 |  0:00:12s
epoch 48 | loss: 0.25495 | val_0_rmse: 0.49919 | val_1_rmse: 0.51951 |  0:00:12s
epoch 49 | loss: 0.2587  | val_0_rmse: 0.49892 | val_1_rmse: 0.51355 |  0:00:12s
epoch 50 | loss: 0.25974 | val_0_rmse: 0.50182 | val_1_rmse: 0.527   |  0:00:12s
epoch 51 | loss: 0.25367 | val_0_rmse: 0.49777 | val_1_rmse: 0.52985 |  0:00:13s
epoch 52 | loss: 0.25316 | val_0_rmse: 0.49447 | val_1_rmse: 0.5351  |  0:00:13s
epoch 53 | loss: 0.25581 | val_0_rmse: 0.49657 | val_1_rmse: 0.53112 |  0:00:13s
epoch 54 | loss: 0.2475  | val_0_rmse: 0.49138 | val_1_rmse: 0.52154 |  0:00:13s
epoch 55 | loss: 0.25207 | val_0_rmse: 0.48914 | val_1_rmse: 0.51763 |  0:00:13s
epoch 56 | loss: 0.25342 | val_0_rmse: 0.49119 | val_1_rmse: 0.52304 |  0:00:14s
epoch 57 | loss: 0.2521  | val_0_rmse: 0.48972 | val_1_rmse: 0.51922 |  0:00:14s
epoch 58 | loss: 0.25527 | val_0_rmse: 0.48629 | val_1_rmse: 0.51705 |  0:00:14s
epoch 59 | loss: 0.25703 | val_0_rmse: 0.49773 | val_1_rmse: 0.53218 |  0:00:14s
epoch 60 | loss: 0.26057 | val_0_rmse: 0.48196 | val_1_rmse: 0.51893 |  0:00:15s
epoch 61 | loss: 0.25482 | val_0_rmse: 0.48014 | val_1_rmse: 0.52471 |  0:00:15s
epoch 62 | loss: 0.25617 | val_0_rmse: 0.50208 | val_1_rmse: 0.54569 |  0:00:15s
epoch 63 | loss: 0.24189 | val_0_rmse: 0.48681 | val_1_rmse: 0.51771 |  0:00:15s
epoch 64 | loss: 0.24874 | val_0_rmse: 0.4805  | val_1_rmse: 0.51259 |  0:00:16s
epoch 65 | loss: 0.25013 | val_0_rmse: 0.48206 | val_1_rmse: 0.5137  |  0:00:16s
epoch 66 | loss: 0.24192 | val_0_rmse: 0.49227 | val_1_rmse: 0.51643 |  0:00:16s
epoch 67 | loss: 0.24648 | val_0_rmse: 0.49446 | val_1_rmse: 0.52003 |  0:00:17s
epoch 68 | loss: 0.25087 | val_0_rmse: 0.49076 | val_1_rmse: 0.51922 |  0:00:17s
epoch 69 | loss: 0.24356 | val_0_rmse: 0.48454 | val_1_rmse: 0.50961 |  0:00:17s
epoch 70 | loss: 0.24147 | val_0_rmse: 0.48563 | val_1_rmse: 0.50769 |  0:00:17s
epoch 71 | loss: 0.255   | val_0_rmse: 0.49193 | val_1_rmse: 0.5207  |  0:00:18s
epoch 72 | loss: 0.26449 | val_0_rmse: 0.48084 | val_1_rmse: 0.51301 |  0:00:18s
epoch 73 | loss: 0.24763 | val_0_rmse: 0.47982 | val_1_rmse: 0.51747 |  0:00:18s
epoch 74 | loss: 0.24459 | val_0_rmse: 0.52511 | val_1_rmse: 0.56705 |  0:00:18s
epoch 75 | loss: 0.25423 | val_0_rmse: 0.48417 | val_1_rmse: 0.52116 |  0:00:18s
epoch 76 | loss: 0.24471 | val_0_rmse: 0.48148 | val_1_rmse: 0.52987 |  0:00:19s
epoch 77 | loss: 0.24469 | val_0_rmse: 0.49303 | val_1_rmse: 0.53472 |  0:00:19s
epoch 78 | loss: 0.24403 | val_0_rmse: 0.4745  | val_1_rmse: 0.51477 |  0:00:19s
epoch 79 | loss: 0.24108 | val_0_rmse: 0.47268 | val_1_rmse: 0.50959 |  0:00:19s
epoch 80 | loss: 0.24474 | val_0_rmse: 0.48501 | val_1_rmse: 0.51912 |  0:00:20s
epoch 81 | loss: 0.2544  | val_0_rmse: 0.47286 | val_1_rmse: 0.50465 |  0:00:20s
epoch 82 | loss: 0.24259 | val_0_rmse: 0.47324 | val_1_rmse: 0.50697 |  0:00:20s
epoch 83 | loss: 0.24442 | val_0_rmse: 0.4882  | val_1_rmse: 0.51972 |  0:00:20s
epoch 84 | loss: 0.24962 | val_0_rmse: 0.47077 | val_1_rmse: 0.50891 |  0:00:21s
epoch 85 | loss: 0.24092 | val_0_rmse: 0.47261 | val_1_rmse: 0.50885 |  0:00:21s
epoch 86 | loss: 0.23329 | val_0_rmse: 0.471   | val_1_rmse: 0.5063  |  0:00:21s
epoch 87 | loss: 0.23566 | val_0_rmse: 0.48347 | val_1_rmse: 0.50956 |  0:00:21s
epoch 88 | loss: 0.24997 | val_0_rmse: 0.48213 | val_1_rmse: 0.50428 |  0:00:22s
epoch 89 | loss: 0.25088 | val_0_rmse: 0.46994 | val_1_rmse: 0.50729 |  0:00:22s
epoch 90 | loss: 0.23648 | val_0_rmse: 0.47296 | val_1_rmse: 0.51192 |  0:00:22s
epoch 91 | loss: 0.2473  | val_0_rmse: 0.48451 | val_1_rmse: 0.51876 |  0:00:22s
epoch 92 | loss: 0.24131 | val_0_rmse: 0.47672 | val_1_rmse: 0.51437 |  0:00:23s
epoch 93 | loss: 0.235   | val_0_rmse: 0.46212 | val_1_rmse: 0.50733 |  0:00:23s
epoch 94 | loss: 0.23486 | val_0_rmse: 0.46706 | val_1_rmse: 0.50737 |  0:00:23s
epoch 95 | loss: 0.23105 | val_0_rmse: 0.47715 | val_1_rmse: 0.5142  |  0:00:23s
epoch 96 | loss: 0.25519 | val_0_rmse: 0.47397 | val_1_rmse: 0.5129  |  0:00:24s
epoch 97 | loss: 0.24328 | val_0_rmse: 0.48556 | val_1_rmse: 0.52074 |  0:00:24s
epoch 98 | loss: 0.24317 | val_0_rmse: 0.49733 | val_1_rmse: 0.53107 |  0:00:24s
epoch 99 | loss: 0.24384 | val_0_rmse: 0.47716 | val_1_rmse: 0.5138  |  0:00:24s
epoch 100| loss: 0.23582 | val_0_rmse: 0.47696 | val_1_rmse: 0.51755 |  0:00:25s
epoch 101| loss: 0.24352 | val_0_rmse: 0.48739 | val_1_rmse: 0.53742 |  0:00:25s
epoch 102| loss: 0.25143 | val_0_rmse: 0.46895 | val_1_rmse: 0.51536 |  0:00:25s
epoch 103| loss: 0.2305  | val_0_rmse: 0.47219 | val_1_rmse: 0.51563 |  0:00:25s
epoch 104| loss: 0.23685 | val_0_rmse: 0.4689  | val_1_rmse: 0.51847 |  0:00:26s
epoch 105| loss: 0.23341 | val_0_rmse: 0.46923 | val_1_rmse: 0.52873 |  0:00:26s
epoch 106| loss: 0.23379 | val_0_rmse: 0.46371 | val_1_rmse: 0.52898 |  0:00:26s
epoch 107| loss: 0.22653 | val_0_rmse: 0.45867 | val_1_rmse: 0.53606 |  0:00:26s
epoch 108| loss: 0.22827 | val_0_rmse: 0.45935 | val_1_rmse: 0.54131 |  0:00:27s
epoch 109| loss: 0.22632 | val_0_rmse: 0.45461 | val_1_rmse: 0.52279 |  0:00:27s
epoch 110| loss: 0.22552 | val_0_rmse: 0.46516 | val_1_rmse: 0.51527 |  0:00:27s
epoch 111| loss: 0.22314 | val_0_rmse: 0.47241 | val_1_rmse: 0.53151 |  0:00:27s
epoch 112| loss: 0.22979 | val_0_rmse: 0.45984 | val_1_rmse: 0.52385 |  0:00:28s
epoch 113| loss: 0.2398  | val_0_rmse: 0.45905 | val_1_rmse: 0.50602 |  0:00:28s
epoch 114| loss: 0.22841 | val_0_rmse: 0.459   | val_1_rmse: 0.49971 |  0:00:28s
epoch 115| loss: 0.2244  | val_0_rmse: 0.47726 | val_1_rmse: 0.5275  |  0:00:28s
epoch 116| loss: 0.22549 | val_0_rmse: 0.44529 | val_1_rmse: 0.508   |  0:00:29s
epoch 117| loss: 0.20727 | val_0_rmse: 0.47081 | val_1_rmse: 0.52664 |  0:00:29s
epoch 118| loss: 0.21901 | val_0_rmse: 0.45176 | val_1_rmse: 0.5124  |  0:00:29s
epoch 119| loss: 0.21705 | val_0_rmse: 0.44595 | val_1_rmse: 0.50656 |  0:00:30s
epoch 120| loss: 0.22332 | val_0_rmse: 0.45061 | val_1_rmse: 0.50387 |  0:00:30s
epoch 121| loss: 0.21444 | val_0_rmse: 0.45336 | val_1_rmse: 0.51059 |  0:00:30s
epoch 122| loss: 0.21045 | val_0_rmse: 0.45446 | val_1_rmse: 0.51806 |  0:00:30s
epoch 123| loss: 0.2124  | val_0_rmse: 0.45553 | val_1_rmse: 0.52464 |  0:00:31s
epoch 124| loss: 0.21513 | val_0_rmse: 0.46015 | val_1_rmse: 0.5257  |  0:00:31s
epoch 125| loss: 0.21917 | val_0_rmse: 0.44766 | val_1_rmse: 0.51306 |  0:00:31s
epoch 126| loss: 0.21164 | val_0_rmse: 0.44623 | val_1_rmse: 0.51146 |  0:00:31s
epoch 127| loss: 0.22049 | val_0_rmse: 0.46375 | val_1_rmse: 0.52038 |  0:00:32s
epoch 128| loss: 0.22008 | val_0_rmse: 0.44019 | val_1_rmse: 0.49929 |  0:00:32s
epoch 129| loss: 0.22181 | val_0_rmse: 0.44506 | val_1_rmse: 0.50684 |  0:00:32s
epoch 130| loss: 0.21403 | val_0_rmse: 0.48821 | val_1_rmse: 0.53946 |  0:00:32s
epoch 131| loss: 0.22663 | val_0_rmse: 0.44418 | val_1_rmse: 0.51103 |  0:00:33s
epoch 132| loss: 0.21588 | val_0_rmse: 0.44657 | val_1_rmse: 0.5178  |  0:00:33s
epoch 133| loss: 0.21336 | val_0_rmse: 0.44151 | val_1_rmse: 0.50594 |  0:00:33s
epoch 134| loss: 0.20773 | val_0_rmse: 0.4418  | val_1_rmse: 0.50408 |  0:00:33s
epoch 135| loss: 0.20616 | val_0_rmse: 0.43592 | val_1_rmse: 0.5069  |  0:00:34s
epoch 136| loss: 0.20853 | val_0_rmse: 0.44402 | val_1_rmse: 0.52077 |  0:00:34s
epoch 137| loss: 0.20689 | val_0_rmse: 0.46563 | val_1_rmse: 0.53447 |  0:00:34s
epoch 138| loss: 0.21753 | val_0_rmse: 0.45345 | val_1_rmse: 0.50912 |  0:00:34s
epoch 139| loss: 0.21347 | val_0_rmse: 0.44305 | val_1_rmse: 0.50379 |  0:00:35s
epoch 140| loss: 0.21015 | val_0_rmse: 0.44747 | val_1_rmse: 0.51237 |  0:00:35s
epoch 141| loss: 0.2203  | val_0_rmse: 0.44772 | val_1_rmse: 0.521   |  0:00:35s
epoch 142| loss: 0.21903 | val_0_rmse: 0.44853 | val_1_rmse: 0.5256  |  0:00:35s
epoch 143| loss: 0.21215 | val_0_rmse: 0.43939 | val_1_rmse: 0.51827 |  0:00:36s
epoch 144| loss: 0.20762 | val_0_rmse: 0.43777 | val_1_rmse: 0.50771 |  0:00:36s
epoch 145| loss: 0.21727 | val_0_rmse: 0.44024 | val_1_rmse: 0.50853 |  0:00:36s
epoch 146| loss: 0.21009 | val_0_rmse: 0.44482 | val_1_rmse: 0.5168  |  0:00:36s
epoch 147| loss: 0.22373 | val_0_rmse: 0.45516 | val_1_rmse: 0.52485 |  0:00:37s
epoch 148| loss: 0.22289 | val_0_rmse: 0.45671 | val_1_rmse: 0.5245  |  0:00:37s
epoch 149| loss: 0.21743 | val_0_rmse: 0.45051 | val_1_rmse: 0.54356 |  0:00:37s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.49929
Best weights from best epoch are automatically used!
ended training at: 02:47:23
Feature importance:
[('Area', 0.3553485531266647), ('Baths', 0.059696536930524575), ('Beds', 0.13753699411734716), ('Latitude', 0.34996836764902123), ('Longitude', 0.050804003861578254), ('Month', 0.023162930400850552), ('Year', 0.02348261391401353)]
Mean squared error is of 8175331059.359025
Mean absolute error:65622.88360906593
MAPE:0.17067070472257753
R2 score:0.7482850735031062
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:47:23
epoch 0  | loss: 1.53679 | val_0_rmse: 1.44895 | val_1_rmse: 1.58523 |  0:00:00s
epoch 1  | loss: 0.74746 | val_0_rmse: 1.01971 | val_1_rmse: 1.09146 |  0:00:00s
epoch 2  | loss: 0.61551 | val_0_rmse: 1.15177 | val_1_rmse: 1.21079 |  0:00:00s
epoch 3  | loss: 0.59325 | val_0_rmse: 0.9178  | val_1_rmse: 0.96818 |  0:00:01s
epoch 4  | loss: 0.54014 | val_0_rmse: 0.81957 | val_1_rmse: 0.85299 |  0:00:01s
epoch 5  | loss: 0.50325 | val_0_rmse: 0.75737 | val_1_rmse: 0.76452 |  0:00:01s
epoch 6  | loss: 0.46612 | val_0_rmse: 0.73966 | val_1_rmse: 0.72178 |  0:00:01s
epoch 7  | loss: 0.45683 | val_0_rmse: 0.69961 | val_1_rmse: 0.67464 |  0:00:02s
epoch 8  | loss: 0.43    | val_0_rmse: 0.67764 | val_1_rmse: 0.65633 |  0:00:02s
epoch 9  | loss: 0.40697 | val_0_rmse: 0.66652 | val_1_rmse: 0.66232 |  0:00:02s
epoch 10 | loss: 0.38408 | val_0_rmse: 0.6596  | val_1_rmse: 0.67628 |  0:00:02s
epoch 11 | loss: 0.38053 | val_0_rmse: 0.64984 | val_1_rmse: 0.65083 |  0:00:03s
epoch 12 | loss: 0.36744 | val_0_rmse: 0.64668 | val_1_rmse: 0.64153 |  0:00:03s
epoch 13 | loss: 0.35909 | val_0_rmse: 0.63017 | val_1_rmse: 0.60304 |  0:00:03s
epoch 14 | loss: 0.35616 | val_0_rmse: 0.61978 | val_1_rmse: 0.60017 |  0:00:03s
epoch 15 | loss: 0.33764 | val_0_rmse: 0.63369 | val_1_rmse: 0.63479 |  0:00:04s
epoch 16 | loss: 0.34866 | val_0_rmse: 0.65961 | val_1_rmse: 0.67904 |  0:00:04s
epoch 17 | loss: 0.34764 | val_0_rmse: 0.63802 | val_1_rmse: 0.65146 |  0:00:04s
epoch 18 | loss: 0.34242 | val_0_rmse: 0.6067  | val_1_rmse: 0.60238 |  0:00:04s
epoch 19 | loss: 0.33559 | val_0_rmse: 0.58853 | val_1_rmse: 0.57495 |  0:00:05s
epoch 20 | loss: 0.32688 | val_0_rmse: 0.57368 | val_1_rmse: 0.5609  |  0:00:05s
epoch 21 | loss: 0.31549 | val_0_rmse: 0.57534 | val_1_rmse: 0.57577 |  0:00:05s
epoch 22 | loss: 0.332   | val_0_rmse: 0.57844 | val_1_rmse: 0.58595 |  0:00:05s
epoch 23 | loss: 0.32254 | val_0_rmse: 0.60149 | val_1_rmse: 0.61558 |  0:00:06s
epoch 24 | loss: 0.32106 | val_0_rmse: 0.59846 | val_1_rmse: 0.61607 |  0:00:06s
epoch 25 | loss: 0.33146 | val_0_rmse: 0.57472 | val_1_rmse: 0.58735 |  0:00:06s
epoch 26 | loss: 0.32516 | val_0_rmse: 0.56816 | val_1_rmse: 0.57714 |  0:00:06s
epoch 27 | loss: 0.32257 | val_0_rmse: 0.55447 | val_1_rmse: 0.57116 |  0:00:07s
epoch 28 | loss: 0.30515 | val_0_rmse: 0.55426 | val_1_rmse: 0.57131 |  0:00:07s
epoch 29 | loss: 0.30563 | val_0_rmse: 0.5499  | val_1_rmse: 0.55837 |  0:00:07s
epoch 30 | loss: 0.30021 | val_0_rmse: 0.5527  | val_1_rmse: 0.55254 |  0:00:07s
epoch 31 | loss: 0.30237 | val_0_rmse: 0.54332 | val_1_rmse: 0.52546 |  0:00:08s
epoch 32 | loss: 0.29037 | val_0_rmse: 0.54463 | val_1_rmse: 0.52181 |  0:00:08s
epoch 33 | loss: 0.28809 | val_0_rmse: 0.5717  | val_1_rmse: 0.56591 |  0:00:08s
epoch 34 | loss: 0.28954 | val_0_rmse: 0.59322 | val_1_rmse: 0.6035  |  0:00:09s
epoch 35 | loss: 0.28609 | val_0_rmse: 0.55132 | val_1_rmse: 0.55584 |  0:00:09s
epoch 36 | loss: 0.28825 | val_0_rmse: 0.53612 | val_1_rmse: 0.53595 |  0:00:09s
epoch 37 | loss: 0.27571 | val_0_rmse: 0.53719 | val_1_rmse: 0.53912 |  0:00:09s
epoch 38 | loss: 0.27539 | val_0_rmse: 0.52766 | val_1_rmse: 0.52865 |  0:00:10s
epoch 39 | loss: 0.27819 | val_0_rmse: 0.50813 | val_1_rmse: 0.51222 |  0:00:10s
epoch 40 | loss: 0.28067 | val_0_rmse: 0.50541 | val_1_rmse: 0.50952 |  0:00:10s
epoch 41 | loss: 0.2678  | val_0_rmse: 0.51475 | val_1_rmse: 0.51247 |  0:00:10s
epoch 42 | loss: 0.27056 | val_0_rmse: 0.51369 | val_1_rmse: 0.51599 |  0:00:11s
epoch 43 | loss: 0.26738 | val_0_rmse: 0.50763 | val_1_rmse: 0.51528 |  0:00:11s
epoch 44 | loss: 0.259   | val_0_rmse: 0.50134 | val_1_rmse: 0.50166 |  0:00:11s
epoch 45 | loss: 0.26948 | val_0_rmse: 0.5065  | val_1_rmse: 0.51995 |  0:00:11s
epoch 46 | loss: 0.26747 | val_0_rmse: 0.5     | val_1_rmse: 0.51985 |  0:00:12s
epoch 47 | loss: 0.26672 | val_0_rmse: 0.50462 | val_1_rmse: 0.53292 |  0:00:12s
epoch 48 | loss: 0.2642  | val_0_rmse: 0.49458 | val_1_rmse: 0.52163 |  0:00:12s
epoch 49 | loss: 0.26225 | val_0_rmse: 0.50027 | val_1_rmse: 0.52626 |  0:00:12s
epoch 50 | loss: 0.26961 | val_0_rmse: 0.49173 | val_1_rmse: 0.51884 |  0:00:13s
epoch 51 | loss: 0.26324 | val_0_rmse: 0.48956 | val_1_rmse: 0.50216 |  0:00:13s
epoch 52 | loss: 0.26432 | val_0_rmse: 0.49542 | val_1_rmse: 0.51106 |  0:00:13s
epoch 53 | loss: 0.25582 | val_0_rmse: 0.49313 | val_1_rmse: 0.51177 |  0:00:13s
epoch 54 | loss: 0.25572 | val_0_rmse: 0.49232 | val_1_rmse: 0.5137  |  0:00:14s
epoch 55 | loss: 0.25013 | val_0_rmse: 0.4943  | val_1_rmse: 0.52738 |  0:00:14s
epoch 56 | loss: 0.25493 | val_0_rmse: 0.50008 | val_1_rmse: 0.53621 |  0:00:14s
epoch 57 | loss: 0.25572 | val_0_rmse: 0.48744 | val_1_rmse: 0.51585 |  0:00:14s
epoch 58 | loss: 0.24493 | val_0_rmse: 0.48168 | val_1_rmse: 0.50641 |  0:00:15s
epoch 59 | loss: 0.2524  | val_0_rmse: 0.48481 | val_1_rmse: 0.51498 |  0:00:15s
epoch 60 | loss: 0.23875 | val_0_rmse: 0.48994 | val_1_rmse: 0.53311 |  0:00:15s
epoch 61 | loss: 0.25049 | val_0_rmse: 0.47984 | val_1_rmse: 0.52664 |  0:00:15s
epoch 62 | loss: 0.25002 | val_0_rmse: 0.47826 | val_1_rmse: 0.5156  |  0:00:16s
epoch 63 | loss: 0.24062 | val_0_rmse: 0.4758  | val_1_rmse: 0.50595 |  0:00:16s
epoch 64 | loss: 0.25057 | val_0_rmse: 0.48075 | val_1_rmse: 0.51401 |  0:00:16s
epoch 65 | loss: 0.24754 | val_0_rmse: 0.47764 | val_1_rmse: 0.5125  |  0:00:17s
epoch 66 | loss: 0.24054 | val_0_rmse: 0.47844 | val_1_rmse: 0.51569 |  0:00:17s
epoch 67 | loss: 0.23752 | val_0_rmse: 0.4723  | val_1_rmse: 0.50614 |  0:00:17s
epoch 68 | loss: 0.23913 | val_0_rmse: 0.47156 | val_1_rmse: 0.51549 |  0:00:17s
epoch 69 | loss: 0.2395  | val_0_rmse: 0.47115 | val_1_rmse: 0.52017 |  0:00:18s
epoch 70 | loss: 0.24189 | val_0_rmse: 0.47418 | val_1_rmse: 0.53672 |  0:00:18s
epoch 71 | loss: 0.24401 | val_0_rmse: 0.46916 | val_1_rmse: 0.52438 |  0:00:18s
epoch 72 | loss: 0.2331  | val_0_rmse: 0.46855 | val_1_rmse: 0.50186 |  0:00:18s
epoch 73 | loss: 0.24478 | val_0_rmse: 0.47131 | val_1_rmse: 0.50234 |  0:00:19s
epoch 74 | loss: 0.23654 | val_0_rmse: 0.46549 | val_1_rmse: 0.50021 |  0:00:19s
epoch 75 | loss: 0.22286 | val_0_rmse: 0.47074 | val_1_rmse: 0.51366 |  0:00:19s
epoch 76 | loss: 0.23292 | val_0_rmse: 0.46725 | val_1_rmse: 0.51833 |  0:00:19s
epoch 77 | loss: 0.22547 | val_0_rmse: 0.46096 | val_1_rmse: 0.51977 |  0:00:20s
epoch 78 | loss: 0.23515 | val_0_rmse: 0.46673 | val_1_rmse: 0.52441 |  0:00:20s
epoch 79 | loss: 0.23279 | val_0_rmse: 0.47296 | val_1_rmse: 0.51418 |  0:00:20s
epoch 80 | loss: 0.23991 | val_0_rmse: 0.47151 | val_1_rmse: 0.51293 |  0:00:20s
epoch 81 | loss: 0.23326 | val_0_rmse: 0.48132 | val_1_rmse: 0.53412 |  0:00:21s
epoch 82 | loss: 0.24409 | val_0_rmse: 0.47865 | val_1_rmse: 0.5387  |  0:00:21s
epoch 83 | loss: 0.23357 | val_0_rmse: 0.47452 | val_1_rmse: 0.53388 |  0:00:21s
epoch 84 | loss: 0.23932 | val_0_rmse: 0.46914 | val_1_rmse: 0.53277 |  0:00:21s
epoch 85 | loss: 0.23732 | val_0_rmse: 0.47425 | val_1_rmse: 0.53408 |  0:00:22s
epoch 86 | loss: 0.23706 | val_0_rmse: 0.49331 | val_1_rmse: 0.54039 |  0:00:22s
epoch 87 | loss: 0.24486 | val_0_rmse: 0.48245 | val_1_rmse: 0.53036 |  0:00:22s
epoch 88 | loss: 0.23912 | val_0_rmse: 0.46749 | val_1_rmse: 0.53013 |  0:00:22s
epoch 89 | loss: 0.22866 | val_0_rmse: 0.4655  | val_1_rmse: 0.53068 |  0:00:23s
epoch 90 | loss: 0.23553 | val_0_rmse: 0.47003 | val_1_rmse: 0.53171 |  0:00:23s
epoch 91 | loss: 0.23579 | val_0_rmse: 0.49016 | val_1_rmse: 0.55245 |  0:00:23s
epoch 92 | loss: 0.22619 | val_0_rmse: 0.476   | val_1_rmse: 0.53774 |  0:00:23s
epoch 93 | loss: 0.23371 | val_0_rmse: 0.45961 | val_1_rmse: 0.51891 |  0:00:24s
epoch 94 | loss: 0.22728 | val_0_rmse: 0.45689 | val_1_rmse: 0.52158 |  0:00:24s
epoch 95 | loss: 0.23022 | val_0_rmse: 0.4559  | val_1_rmse: 0.52917 |  0:00:24s
epoch 96 | loss: 0.22901 | val_0_rmse: 0.45961 | val_1_rmse: 0.52902 |  0:00:24s
epoch 97 | loss: 0.22381 | val_0_rmse: 0.45489 | val_1_rmse: 0.5249  |  0:00:25s
epoch 98 | loss: 0.22731 | val_0_rmse: 0.45184 | val_1_rmse: 0.52531 |  0:00:25s
epoch 99 | loss: 0.20908 | val_0_rmse: 0.45814 | val_1_rmse: 0.52844 |  0:00:25s
epoch 100| loss: 0.21937 | val_0_rmse: 0.44785 | val_1_rmse: 0.53604 |  0:00:25s
epoch 101| loss: 0.23027 | val_0_rmse: 0.45747 | val_1_rmse: 0.53637 |  0:00:26s
epoch 102| loss: 0.23183 | val_0_rmse: 0.48299 | val_1_rmse: 0.5514  |  0:00:26s
epoch 103| loss: 0.23244 | val_0_rmse: 0.46788 | val_1_rmse: 0.53783 |  0:00:26s
epoch 104| loss: 0.22662 | val_0_rmse: 0.47083 | val_1_rmse: 0.55463 |  0:00:26s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.50021
Best weights from best epoch are automatically used!
ended training at: 02:47:50
Feature importance:
[('Area', 0.3803774395443229), ('Baths', 0.1078500520960669), ('Beds', 0.11055377920068023), ('Latitude', 0.3442633700671332), ('Longitude', 0.015423461773028713), ('Month', 0.0017568760291362159), ('Year', 0.03977502128963185)]
Mean squared error is of 7905996336.386308
Mean absolute error:64309.733492032974
MAPE:0.16696875228597843
R2 score:0.742488059567975
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:47:50
epoch 0  | loss: 1.48161 | val_0_rmse: 1.20688 | val_1_rmse: 1.16326 |  0:00:00s
epoch 1  | loss: 0.6874  | val_0_rmse: 1.11783 | val_1_rmse: 1.25947 |  0:00:00s
epoch 2  | loss: 0.5484  | val_0_rmse: 0.88723 | val_1_rmse: 0.91583 |  0:00:00s
epoch 3  | loss: 0.47933 | val_0_rmse: 0.96266 | val_1_rmse: 0.9661  |  0:00:00s
epoch 4  | loss: 0.43976 | val_0_rmse: 0.796   | val_1_rmse: 0.83631 |  0:00:01s
epoch 5  | loss: 0.38622 | val_0_rmse: 0.78608 | val_1_rmse: 0.84582 |  0:00:01s
epoch 6  | loss: 0.38178 | val_0_rmse: 0.84182 | val_1_rmse: 0.9344  |  0:00:01s
epoch 7  | loss: 0.38124 | val_0_rmse: 0.74376 | val_1_rmse: 0.80414 |  0:00:01s
epoch 8  | loss: 0.37066 | val_0_rmse: 0.73917 | val_1_rmse: 0.80696 |  0:00:02s
epoch 9  | loss: 0.36827 | val_0_rmse: 0.72345 | val_1_rmse: 0.80036 |  0:00:02s
epoch 10 | loss: 0.3512  | val_0_rmse: 0.6706  | val_1_rmse: 0.76421 |  0:00:02s
epoch 11 | loss: 0.35226 | val_0_rmse: 0.63518 | val_1_rmse: 0.6962  |  0:00:03s
epoch 12 | loss: 0.35487 | val_0_rmse: 0.62503 | val_1_rmse: 0.6776  |  0:00:03s
epoch 13 | loss: 0.33287 | val_0_rmse: 0.6037  | val_1_rmse: 0.64691 |  0:00:03s
epoch 14 | loss: 0.33559 | val_0_rmse: 0.59118 | val_1_rmse: 0.62431 |  0:00:03s
epoch 15 | loss: 0.3305  | val_0_rmse: 0.5663  | val_1_rmse: 0.62778 |  0:00:04s
epoch 16 | loss: 0.31699 | val_0_rmse: 0.58617 | val_1_rmse: 0.66254 |  0:00:04s
epoch 17 | loss: 0.30751 | val_0_rmse: 0.55548 | val_1_rmse: 0.60919 |  0:00:04s
epoch 18 | loss: 0.3005  | val_0_rmse: 0.55302 | val_1_rmse: 0.61049 |  0:00:04s
epoch 19 | loss: 0.30312 | val_0_rmse: 0.60981 | val_1_rmse: 0.66674 |  0:00:05s
epoch 20 | loss: 0.29274 | val_0_rmse: 0.57345 | val_1_rmse: 0.63211 |  0:00:05s
epoch 21 | loss: 0.28763 | val_0_rmse: 0.53845 | val_1_rmse: 0.59117 |  0:00:05s
epoch 22 | loss: 0.29243 | val_0_rmse: 0.54965 | val_1_rmse: 0.60498 |  0:00:05s
epoch 23 | loss: 0.29256 | val_0_rmse: 0.56324 | val_1_rmse: 0.62442 |  0:00:06s
epoch 24 | loss: 0.28317 | val_0_rmse: 0.5369  | val_1_rmse: 0.60043 |  0:00:06s
epoch 25 | loss: 0.27951 | val_0_rmse: 0.52953 | val_1_rmse: 0.60613 |  0:00:06s
epoch 26 | loss: 0.28291 | val_0_rmse: 0.52625 | val_1_rmse: 0.61147 |  0:00:06s
epoch 27 | loss: 0.26809 | val_0_rmse: 0.52041 | val_1_rmse: 0.60768 |  0:00:07s
epoch 28 | loss: 0.26752 | val_0_rmse: 0.53313 | val_1_rmse: 0.61137 |  0:00:07s
epoch 29 | loss: 0.26342 | val_0_rmse: 0.5391  | val_1_rmse: 0.60275 |  0:00:07s
epoch 30 | loss: 0.27177 | val_0_rmse: 0.51963 | val_1_rmse: 0.58258 |  0:00:07s
epoch 31 | loss: 0.26165 | val_0_rmse: 0.49885 | val_1_rmse: 0.57715 |  0:00:08s
epoch 32 | loss: 0.2617  | val_0_rmse: 0.5041  | val_1_rmse: 0.58669 |  0:00:08s
epoch 33 | loss: 0.26904 | val_0_rmse: 0.51743 | val_1_rmse: 0.59533 |  0:00:08s
epoch 34 | loss: 0.26491 | val_0_rmse: 0.50672 | val_1_rmse: 0.58356 |  0:00:08s
epoch 35 | loss: 0.26608 | val_0_rmse: 0.51777 | val_1_rmse: 0.59348 |  0:00:09s
epoch 36 | loss: 0.27901 | val_0_rmse: 0.50234 | val_1_rmse: 0.58192 |  0:00:09s
epoch 37 | loss: 0.27305 | val_0_rmse: 0.50809 | val_1_rmse: 0.58412 |  0:00:09s
epoch 38 | loss: 0.27449 | val_0_rmse: 0.5089  | val_1_rmse: 0.57776 |  0:00:09s
epoch 39 | loss: 0.27629 | val_0_rmse: 0.49832 | val_1_rmse: 0.56805 |  0:00:10s
epoch 40 | loss: 0.26218 | val_0_rmse: 0.50625 | val_1_rmse: 0.5776  |  0:00:10s
epoch 41 | loss: 0.27468 | val_0_rmse: 0.4961  | val_1_rmse: 0.5708  |  0:00:10s
epoch 42 | loss: 0.2677  | val_0_rmse: 0.50081 | val_1_rmse: 0.57196 |  0:00:10s
epoch 43 | loss: 0.25754 | val_0_rmse: 0.49917 | val_1_rmse: 0.57144 |  0:00:11s
epoch 44 | loss: 0.25565 | val_0_rmse: 0.49852 | val_1_rmse: 0.57979 |  0:00:11s
epoch 45 | loss: 0.25833 | val_0_rmse: 0.49875 | val_1_rmse: 0.58271 |  0:00:11s
epoch 46 | loss: 0.25927 | val_0_rmse: 0.49206 | val_1_rmse: 0.5785  |  0:00:12s
epoch 47 | loss: 0.25427 | val_0_rmse: 0.4917  | val_1_rmse: 0.57435 |  0:00:12s
epoch 48 | loss: 0.25436 | val_0_rmse: 0.50851 | val_1_rmse: 0.58291 |  0:00:12s
epoch 49 | loss: 0.25427 | val_0_rmse: 0.48857 | val_1_rmse: 0.56415 |  0:00:12s
epoch 50 | loss: 0.26512 | val_0_rmse: 0.49297 | val_1_rmse: 0.56523 |  0:00:13s
epoch 51 | loss: 0.25561 | val_0_rmse: 0.51244 | val_1_rmse: 0.58113 |  0:00:13s
epoch 52 | loss: 0.26308 | val_0_rmse: 0.49015 | val_1_rmse: 0.56292 |  0:00:13s
epoch 53 | loss: 0.25866 | val_0_rmse: 0.48571 | val_1_rmse: 0.55905 |  0:00:13s
epoch 54 | loss: 0.24417 | val_0_rmse: 0.49445 | val_1_rmse: 0.5654  |  0:00:14s
epoch 55 | loss: 0.26395 | val_0_rmse: 0.48312 | val_1_rmse: 0.55617 |  0:00:14s
epoch 56 | loss: 0.25022 | val_0_rmse: 0.48403 | val_1_rmse: 0.56152 |  0:00:14s
epoch 57 | loss: 0.24904 | val_0_rmse: 0.49001 | val_1_rmse: 0.5742  |  0:00:14s
epoch 58 | loss: 0.25562 | val_0_rmse: 0.48133 | val_1_rmse: 0.56028 |  0:00:15s
epoch 59 | loss: 0.24125 | val_0_rmse: 0.50155 | val_1_rmse: 0.56026 |  0:00:15s
epoch 60 | loss: 0.25398 | val_0_rmse: 0.49016 | val_1_rmse: 0.55296 |  0:00:15s
epoch 61 | loss: 0.24724 | val_0_rmse: 0.50119 | val_1_rmse: 0.57013 |  0:00:15s
epoch 62 | loss: 0.2519  | val_0_rmse: 0.49787 | val_1_rmse: 0.57138 |  0:00:16s
epoch 63 | loss: 0.25332 | val_0_rmse: 0.49885 | val_1_rmse: 0.57159 |  0:00:16s
epoch 64 | loss: 0.25002 | val_0_rmse: 0.49119 | val_1_rmse: 0.56584 |  0:00:16s
epoch 65 | loss: 0.25347 | val_0_rmse: 0.49225 | val_1_rmse: 0.56681 |  0:00:16s
epoch 66 | loss: 0.25163 | val_0_rmse: 0.50646 | val_1_rmse: 0.57753 |  0:00:17s
epoch 67 | loss: 0.24624 | val_0_rmse: 0.4864  | val_1_rmse: 0.56136 |  0:00:17s
epoch 68 | loss: 0.25516 | val_0_rmse: 0.48579 | val_1_rmse: 0.5556  |  0:00:17s
epoch 69 | loss: 0.23681 | val_0_rmse: 0.5013  | val_1_rmse: 0.56292 |  0:00:17s
epoch 70 | loss: 0.24493 | val_0_rmse: 0.47592 | val_1_rmse: 0.55026 |  0:00:18s
epoch 71 | loss: 0.24309 | val_0_rmse: 0.47908 | val_1_rmse: 0.56684 |  0:00:18s
epoch 72 | loss: 0.24987 | val_0_rmse: 0.48133 | val_1_rmse: 0.55767 |  0:00:18s
epoch 73 | loss: 0.23565 | val_0_rmse: 0.48474 | val_1_rmse: 0.54565 |  0:00:18s
epoch 74 | loss: 0.23715 | val_0_rmse: 0.47392 | val_1_rmse: 0.53859 |  0:00:19s
epoch 75 | loss: 0.23875 | val_0_rmse: 0.46976 | val_1_rmse: 0.54031 |  0:00:19s
epoch 76 | loss: 0.24968 | val_0_rmse: 0.47608 | val_1_rmse: 0.54803 |  0:00:19s
epoch 77 | loss: 0.2385  | val_0_rmse: 0.48206 | val_1_rmse: 0.56383 |  0:00:19s
epoch 78 | loss: 0.23552 | val_0_rmse: 0.48251 | val_1_rmse: 0.56731 |  0:00:20s
epoch 79 | loss: 0.24063 | val_0_rmse: 0.47101 | val_1_rmse: 0.55961 |  0:00:20s
epoch 80 | loss: 0.24157 | val_0_rmse: 0.46601 | val_1_rmse: 0.55424 |  0:00:20s
epoch 81 | loss: 0.23872 | val_0_rmse: 0.46662 | val_1_rmse: 0.54958 |  0:00:20s
epoch 82 | loss: 0.23172 | val_0_rmse: 0.46378 | val_1_rmse: 0.54724 |  0:00:21s
epoch 83 | loss: 0.22215 | val_0_rmse: 0.46654 | val_1_rmse: 0.55452 |  0:00:21s
epoch 84 | loss: 0.23478 | val_0_rmse: 0.46167 | val_1_rmse: 0.55342 |  0:00:21s
epoch 85 | loss: 0.22681 | val_0_rmse: 0.46199 | val_1_rmse: 0.55001 |  0:00:21s
epoch 86 | loss: 0.23181 | val_0_rmse: 0.45987 | val_1_rmse: 0.54997 |  0:00:22s
epoch 87 | loss: 0.23784 | val_0_rmse: 0.45696 | val_1_rmse: 0.55516 |  0:00:22s
epoch 88 | loss: 0.22686 | val_0_rmse: 0.47117 | val_1_rmse: 0.56153 |  0:00:22s
epoch 89 | loss: 0.23128 | val_0_rmse: 0.46972 | val_1_rmse: 0.56246 |  0:00:22s
epoch 90 | loss: 0.24266 | val_0_rmse: 0.46518 | val_1_rmse: 0.56167 |  0:00:23s
epoch 91 | loss: 0.22474 | val_0_rmse: 0.4929  | val_1_rmse: 0.57539 |  0:00:23s
epoch 92 | loss: 0.23845 | val_0_rmse: 0.49966 | val_1_rmse: 0.58469 |  0:00:23s
epoch 93 | loss: 0.23666 | val_0_rmse: 0.52785 | val_1_rmse: 0.60129 |  0:00:23s
epoch 94 | loss: 0.25324 | val_0_rmse: 0.53379 | val_1_rmse: 0.59902 |  0:00:24s
epoch 95 | loss: 0.24944 | val_0_rmse: 0.49663 | val_1_rmse: 0.56713 |  0:00:24s
epoch 96 | loss: 0.24108 | val_0_rmse: 0.4867  | val_1_rmse: 0.55358 |  0:00:24s
epoch 97 | loss: 0.23988 | val_0_rmse: 0.47978 | val_1_rmse: 0.54394 |  0:00:24s
epoch 98 | loss: 0.23805 | val_0_rmse: 0.48032 | val_1_rmse: 0.54413 |  0:00:25s
epoch 99 | loss: 0.24524 | val_0_rmse: 0.49486 | val_1_rmse: 0.5636  |  0:00:25s
epoch 100| loss: 0.24332 | val_0_rmse: 0.48979 | val_1_rmse: 0.55897 |  0:00:25s
epoch 101| loss: 0.24198 | val_0_rmse: 0.4805  | val_1_rmse: 0.54948 |  0:00:25s
epoch 102| loss: 0.24807 | val_0_rmse: 0.48192 | val_1_rmse: 0.55485 |  0:00:26s
epoch 103| loss: 0.23539 | val_0_rmse: 0.48702 | val_1_rmse: 0.56806 |  0:00:26s
epoch 104| loss: 0.24814 | val_0_rmse: 0.478   | val_1_rmse: 0.56244 |  0:00:26s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.53859
Best weights from best epoch are automatically used!
ended training at: 02:48:16
Feature importance:
[('Area', 0.2571399966133787), ('Baths', 0.027529222156108844), ('Beds', 0.1189039149656045), ('Latitude', 0.4731296429269447), ('Longitude', 0.037719459218775335), ('Month', 0.025938571016821087), ('Year', 0.059639193102366844)]
Mean squared error is of 10000157892.9787
Mean absolute error:72008.99908873627
MAPE:0.1740121890929391
R2 score:0.7066662379727273
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:48:16
epoch 0  | loss: 1.61793 | val_0_rmse: 1.32826 | val_1_rmse: 1.23673 |  0:00:00s
epoch 1  | loss: 0.68037 | val_0_rmse: 0.87659 | val_1_rmse: 0.85315 |  0:00:00s
epoch 2  | loss: 0.51971 | val_0_rmse: 0.79554 | val_1_rmse: 0.79127 |  0:00:00s
epoch 3  | loss: 0.49522 | val_0_rmse: 0.73447 | val_1_rmse: 0.71856 |  0:00:01s
epoch 4  | loss: 0.43535 | val_0_rmse: 0.72175 | val_1_rmse: 0.71764 |  0:00:01s
epoch 5  | loss: 0.42776 | val_0_rmse: 0.70815 | val_1_rmse: 0.70059 |  0:00:01s
epoch 6  | loss: 0.39723 | val_0_rmse: 0.64065 | val_1_rmse: 0.67862 |  0:00:01s
epoch 7  | loss: 0.38159 | val_0_rmse: 0.63117 | val_1_rmse: 0.6773  |  0:00:02s
epoch 8  | loss: 0.37233 | val_0_rmse: 0.63928 | val_1_rmse: 0.67402 |  0:00:02s
epoch 9  | loss: 0.32953 | val_0_rmse: 0.64449 | val_1_rmse: 0.68505 |  0:00:02s
epoch 10 | loss: 0.32516 | val_0_rmse: 0.65163 | val_1_rmse: 0.70705 |  0:00:02s
epoch 11 | loss: 0.32001 | val_0_rmse: 0.63006 | val_1_rmse: 0.7096  |  0:00:03s
epoch 12 | loss: 0.31666 | val_0_rmse: 0.6209  | val_1_rmse: 0.7123  |  0:00:03s
epoch 13 | loss: 0.31286 | val_0_rmse: 0.63753 | val_1_rmse: 0.74691 |  0:00:03s
epoch 14 | loss: 0.30127 | val_0_rmse: 0.58494 | val_1_rmse: 0.6613  |  0:00:03s
epoch 15 | loss: 0.2964  | val_0_rmse: 0.58093 | val_1_rmse: 0.65581 |  0:00:04s
epoch 16 | loss: 0.30763 | val_0_rmse: 0.60208 | val_1_rmse: 0.66187 |  0:00:04s
epoch 17 | loss: 0.2885  | val_0_rmse: 0.5865  | val_1_rmse: 0.64812 |  0:00:04s
epoch 18 | loss: 0.29894 | val_0_rmse: 0.58342 | val_1_rmse: 0.64983 |  0:00:04s
epoch 19 | loss: 0.28426 | val_0_rmse: 0.5339  | val_1_rmse: 0.5789  |  0:00:05s
epoch 20 | loss: 0.27578 | val_0_rmse: 0.51538 | val_1_rmse: 0.54099 |  0:00:05s
epoch 21 | loss: 0.2787  | val_0_rmse: 0.5734  | val_1_rmse: 0.59695 |  0:00:05s
epoch 22 | loss: 0.28568 | val_0_rmse: 0.52683 | val_1_rmse: 0.55611 |  0:00:05s
epoch 23 | loss: 0.28108 | val_0_rmse: 0.52409 | val_1_rmse: 0.54917 |  0:00:06s
epoch 24 | loss: 0.28952 | val_0_rmse: 0.56591 | val_1_rmse: 0.59275 |  0:00:06s
epoch 25 | loss: 0.27942 | val_0_rmse: 0.51145 | val_1_rmse: 0.53357 |  0:00:06s
epoch 26 | loss: 0.28308 | val_0_rmse: 0.50581 | val_1_rmse: 0.52567 |  0:00:06s
epoch 27 | loss: 0.27    | val_0_rmse: 0.52786 | val_1_rmse: 0.55481 |  0:00:07s
epoch 28 | loss: 0.2784  | val_0_rmse: 0.49874 | val_1_rmse: 0.52    |  0:00:07s
epoch 29 | loss: 0.26615 | val_0_rmse: 0.50116 | val_1_rmse: 0.51771 |  0:00:07s
epoch 30 | loss: 0.26842 | val_0_rmse: 0.50232 | val_1_rmse: 0.52258 |  0:00:08s
epoch 31 | loss: 0.27897 | val_0_rmse: 0.49663 | val_1_rmse: 0.51287 |  0:00:08s
epoch 32 | loss: 0.25778 | val_0_rmse: 0.49509 | val_1_rmse: 0.50648 |  0:00:08s
epoch 33 | loss: 0.25869 | val_0_rmse: 0.49099 | val_1_rmse: 0.50677 |  0:00:08s
epoch 34 | loss: 0.25085 | val_0_rmse: 0.49316 | val_1_rmse: 0.51329 |  0:00:08s
epoch 35 | loss: 0.25567 | val_0_rmse: 0.49176 | val_1_rmse: 0.50671 |  0:00:09s
epoch 36 | loss: 0.26129 | val_0_rmse: 0.4949  | val_1_rmse: 0.51577 |  0:00:09s
epoch 37 | loss: 0.26085 | val_0_rmse: 0.4956  | val_1_rmse: 0.50609 |  0:00:09s
epoch 38 | loss: 0.26473 | val_0_rmse: 0.49809 | val_1_rmse: 0.51232 |  0:00:09s
epoch 39 | loss: 0.27168 | val_0_rmse: 0.49801 | val_1_rmse: 0.51465 |  0:00:10s
epoch 40 | loss: 0.27244 | val_0_rmse: 0.49823 | val_1_rmse: 0.51665 |  0:00:10s
epoch 41 | loss: 0.25949 | val_0_rmse: 0.5     | val_1_rmse: 0.51738 |  0:00:10s
epoch 42 | loss: 0.25766 | val_0_rmse: 0.50698 | val_1_rmse: 0.53007 |  0:00:10s
epoch 43 | loss: 0.2547  | val_0_rmse: 0.5068  | val_1_rmse: 0.52438 |  0:00:11s
epoch 44 | loss: 0.25927 | val_0_rmse: 0.50819 | val_1_rmse: 0.52432 |  0:00:11s
epoch 45 | loss: 0.25539 | val_0_rmse: 0.51392 | val_1_rmse: 0.53423 |  0:00:11s
epoch 46 | loss: 0.25451 | val_0_rmse: 0.51822 | val_1_rmse: 0.53297 |  0:00:11s
epoch 47 | loss: 0.26088 | val_0_rmse: 0.51458 | val_1_rmse: 0.5307  |  0:00:12s
epoch 48 | loss: 0.24685 | val_0_rmse: 0.49576 | val_1_rmse: 0.51021 |  0:00:12s
epoch 49 | loss: 0.27283 | val_0_rmse: 0.49412 | val_1_rmse: 0.5025  |  0:00:12s
epoch 50 | loss: 0.25468 | val_0_rmse: 0.49602 | val_1_rmse: 0.51169 |  0:00:12s
epoch 51 | loss: 0.25997 | val_0_rmse: 0.49463 | val_1_rmse: 0.5131  |  0:00:13s
epoch 52 | loss: 0.25829 | val_0_rmse: 0.49486 | val_1_rmse: 0.50788 |  0:00:13s
epoch 53 | loss: 0.26057 | val_0_rmse: 0.49209 | val_1_rmse: 0.50752 |  0:00:13s
epoch 54 | loss: 0.25446 | val_0_rmse: 0.50111 | val_1_rmse: 0.52323 |  0:00:13s
epoch 55 | loss: 0.26216 | val_0_rmse: 0.48493 | val_1_rmse: 0.49845 |  0:00:14s
epoch 56 | loss: 0.2601  | val_0_rmse: 0.48572 | val_1_rmse: 0.50717 |  0:00:14s
epoch 57 | loss: 0.25642 | val_0_rmse: 0.50053 | val_1_rmse: 0.54381 |  0:00:14s
epoch 58 | loss: 0.26055 | val_0_rmse: 0.48278 | val_1_rmse: 0.51546 |  0:00:14s
epoch 59 | loss: 0.25226 | val_0_rmse: 0.48217 | val_1_rmse: 0.50814 |  0:00:15s
epoch 60 | loss: 0.24565 | val_0_rmse: 0.49242 | val_1_rmse: 0.51808 |  0:00:15s
epoch 61 | loss: 0.25132 | val_0_rmse: 0.49259 | val_1_rmse: 0.51557 |  0:00:15s
epoch 62 | loss: 0.24339 | val_0_rmse: 0.48167 | val_1_rmse: 0.50354 |  0:00:15s
epoch 63 | loss: 0.24724 | val_0_rmse: 0.47943 | val_1_rmse: 0.50318 |  0:00:16s
epoch 64 | loss: 0.23702 | val_0_rmse: 0.47901 | val_1_rmse: 0.50555 |  0:00:16s
epoch 65 | loss: 0.2467  | val_0_rmse: 0.47852 | val_1_rmse: 0.49999 |  0:00:16s
epoch 66 | loss: 0.24343 | val_0_rmse: 0.48197 | val_1_rmse: 0.50843 |  0:00:16s
epoch 67 | loss: 0.24908 | val_0_rmse: 0.48803 | val_1_rmse: 0.52187 |  0:00:17s
epoch 68 | loss: 0.24839 | val_0_rmse: 0.48201 | val_1_rmse: 0.50736 |  0:00:17s
epoch 69 | loss: 0.25901 | val_0_rmse: 0.48518 | val_1_rmse: 0.50502 |  0:00:17s
epoch 70 | loss: 0.25543 | val_0_rmse: 0.48517 | val_1_rmse: 0.51007 |  0:00:17s
epoch 71 | loss: 0.24631 | val_0_rmse: 0.48788 | val_1_rmse: 0.50936 |  0:00:18s
epoch 72 | loss: 0.24016 | val_0_rmse: 0.48631 | val_1_rmse: 0.50497 |  0:00:18s
epoch 73 | loss: 0.23543 | val_0_rmse: 0.48449 | val_1_rmse: 0.50739 |  0:00:18s
epoch 74 | loss: 0.25295 | val_0_rmse: 0.48288 | val_1_rmse: 0.50871 |  0:00:18s
epoch 75 | loss: 0.2442  | val_0_rmse: 0.48037 | val_1_rmse: 0.50673 |  0:00:19s
epoch 76 | loss: 0.25211 | val_0_rmse: 0.47689 | val_1_rmse: 0.5109  |  0:00:19s
epoch 77 | loss: 0.25038 | val_0_rmse: 0.4855  | val_1_rmse: 0.5271  |  0:00:19s
epoch 78 | loss: 0.24543 | val_0_rmse: 0.47533 | val_1_rmse: 0.50838 |  0:00:19s
epoch 79 | loss: 0.25208 | val_0_rmse: 0.47598 | val_1_rmse: 0.51003 |  0:00:20s
epoch 80 | loss: 0.23162 | val_0_rmse: 0.48887 | val_1_rmse: 0.53654 |  0:00:20s
epoch 81 | loss: 0.24447 | val_0_rmse: 0.46689 | val_1_rmse: 0.50865 |  0:00:20s
epoch 82 | loss: 0.23712 | val_0_rmse: 0.47304 | val_1_rmse: 0.51438 |  0:00:20s
epoch 83 | loss: 0.24181 | val_0_rmse: 0.48034 | val_1_rmse: 0.53017 |  0:00:21s
epoch 84 | loss: 0.24709 | val_0_rmse: 0.46815 | val_1_rmse: 0.51171 |  0:00:21s
epoch 85 | loss: 0.2358  | val_0_rmse: 0.47735 | val_1_rmse: 0.50435 |  0:00:21s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.49845
Best weights from best epoch are automatically used!
ended training at: 02:48:38
Feature importance:
[('Area', 0.31795169497777853), ('Baths', 0.015577525468365007), ('Beds', 0.018917809945199964), ('Latitude', 0.3715754742936831), ('Longitude', 0.1272549971732811), ('Month', 0.06438166494103245), ('Year', 0.08434083320065984)]
Mean squared error is of 9327546618.39195
Mean absolute error:73244.5458054945
MAPE:0.20184875888113074
R2 score:0.727287134289755
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:48:38
epoch 0  | loss: 1.62507 | val_0_rmse: 1.38908 | val_1_rmse: 1.41691 |  0:00:00s
epoch 1  | loss: 0.87481 | val_0_rmse: 1.25337 | val_1_rmse: 1.32787 |  0:00:00s
epoch 2  | loss: 0.69714 | val_0_rmse: 1.01008 | val_1_rmse: 1.07153 |  0:00:00s
epoch 3  | loss: 0.60392 | val_0_rmse: 1.12257 | val_1_rmse: 1.22684 |  0:00:01s
epoch 4  | loss: 0.56951 | val_0_rmse: 1.11108 | val_1_rmse: 1.15261 |  0:00:01s
epoch 5  | loss: 0.51679 | val_0_rmse: 1.05982 | val_1_rmse: 1.06404 |  0:00:01s
epoch 6  | loss: 0.50212 | val_0_rmse: 1.08824 | val_1_rmse: 1.11125 |  0:00:01s
epoch 7  | loss: 0.45105 | val_0_rmse: 1.13515 | val_1_rmse: 1.1554  |  0:00:01s
epoch 8  | loss: 0.43877 | val_0_rmse: 1.06894 | val_1_rmse: 1.09324 |  0:00:02s
epoch 9  | loss: 0.45693 | val_0_rmse: 0.8924  | val_1_rmse: 0.91208 |  0:00:02s
epoch 10 | loss: 0.43459 | val_0_rmse: 0.78395 | val_1_rmse: 0.80451 |  0:00:02s
epoch 11 | loss: 0.42454 | val_0_rmse: 0.76934 | val_1_rmse: 0.80287 |  0:00:02s
epoch 12 | loss: 0.3951  | val_0_rmse: 0.76953 | val_1_rmse: 0.79345 |  0:00:03s
epoch 13 | loss: 0.39742 | val_0_rmse: 0.75568 | val_1_rmse: 0.76422 |  0:00:03s
epoch 14 | loss: 0.39444 | val_0_rmse: 0.6775  | val_1_rmse: 0.6861  |  0:00:03s
epoch 15 | loss: 0.3908  | val_0_rmse: 0.6514  | val_1_rmse: 0.66798 |  0:00:04s
epoch 16 | loss: 0.37536 | val_0_rmse: 0.65455 | val_1_rmse: 0.66571 |  0:00:04s
epoch 17 | loss: 0.37975 | val_0_rmse: 0.64727 | val_1_rmse: 0.65899 |  0:00:04s
epoch 18 | loss: 0.374   | val_0_rmse: 0.62727 | val_1_rmse: 0.65324 |  0:00:04s
epoch 19 | loss: 0.35062 | val_0_rmse: 0.62517 | val_1_rmse: 0.64875 |  0:00:05s
epoch 20 | loss: 0.34352 | val_0_rmse: 0.61042 | val_1_rmse: 0.62634 |  0:00:05s
epoch 21 | loss: 0.33771 | val_0_rmse: 0.59119 | val_1_rmse: 0.60919 |  0:00:05s
epoch 22 | loss: 0.32366 | val_0_rmse: 0.58372 | val_1_rmse: 0.60268 |  0:00:05s
epoch 23 | loss: 0.34113 | val_0_rmse: 0.56858 | val_1_rmse: 0.59165 |  0:00:06s
epoch 24 | loss: 0.34548 | val_0_rmse: 0.57227 | val_1_rmse: 0.59759 |  0:00:06s
epoch 25 | loss: 0.33321 | val_0_rmse: 0.56253 | val_1_rmse: 0.58864 |  0:00:06s
epoch 26 | loss: 0.31446 | val_0_rmse: 0.55572 | val_1_rmse: 0.58497 |  0:00:06s
epoch 27 | loss: 0.3349  | val_0_rmse: 0.56235 | val_1_rmse: 0.58974 |  0:00:07s
epoch 28 | loss: 0.32144 | val_0_rmse: 0.55957 | val_1_rmse: 0.58818 |  0:00:07s
epoch 29 | loss: 0.31968 | val_0_rmse: 0.551   | val_1_rmse: 0.5836  |  0:00:07s
epoch 30 | loss: 0.31467 | val_0_rmse: 0.55105 | val_1_rmse: 0.57398 |  0:00:07s
epoch 31 | loss: 0.31789 | val_0_rmse: 0.53621 | val_1_rmse: 0.55484 |  0:00:08s
epoch 32 | loss: 0.30752 | val_0_rmse: 0.54584 | val_1_rmse: 0.56643 |  0:00:08s
epoch 33 | loss: 0.28413 | val_0_rmse: 0.5453  | val_1_rmse: 0.55713 |  0:00:08s
epoch 34 | loss: 0.30028 | val_0_rmse: 0.53125 | val_1_rmse: 0.55978 |  0:00:08s
epoch 35 | loss: 0.28885 | val_0_rmse: 0.52826 | val_1_rmse: 0.56343 |  0:00:09s
epoch 36 | loss: 0.3005  | val_0_rmse: 0.53231 | val_1_rmse: 0.56583 |  0:00:09s
epoch 37 | loss: 0.29585 | val_0_rmse: 0.52079 | val_1_rmse: 0.55532 |  0:00:09s
epoch 38 | loss: 0.28554 | val_0_rmse: 0.5179  | val_1_rmse: 0.55557 |  0:00:09s
epoch 39 | loss: 0.28664 | val_0_rmse: 0.55056 | val_1_rmse: 0.58233 |  0:00:10s
epoch 40 | loss: 0.30105 | val_0_rmse: 0.5352  | val_1_rmse: 0.57392 |  0:00:10s
epoch 41 | loss: 0.28509 | val_0_rmse: 0.51856 | val_1_rmse: 0.55197 |  0:00:10s
epoch 42 | loss: 0.28142 | val_0_rmse: 0.53027 | val_1_rmse: 0.56929 |  0:00:10s
epoch 43 | loss: 0.28761 | val_0_rmse: 0.54776 | val_1_rmse: 0.59694 |  0:00:11s
epoch 44 | loss: 0.29396 | val_0_rmse: 0.52805 | val_1_rmse: 0.58379 |  0:00:11s
epoch 45 | loss: 0.29244 | val_0_rmse: 0.52519 | val_1_rmse: 0.57926 |  0:00:11s
epoch 46 | loss: 0.2796  | val_0_rmse: 0.53084 | val_1_rmse: 0.58151 |  0:00:11s
epoch 47 | loss: 0.2854  | val_0_rmse: 0.51819 | val_1_rmse: 0.57051 |  0:00:12s
epoch 48 | loss: 0.28475 | val_0_rmse: 0.51571 | val_1_rmse: 0.56574 |  0:00:12s
epoch 49 | loss: 0.29987 | val_0_rmse: 0.51497 | val_1_rmse: 0.56687 |  0:00:12s
epoch 50 | loss: 0.29594 | val_0_rmse: 0.50746 | val_1_rmse: 0.57638 |  0:00:12s
epoch 51 | loss: 0.28139 | val_0_rmse: 0.51282 | val_1_rmse: 0.57691 |  0:00:12s
epoch 52 | loss: 0.27767 | val_0_rmse: 0.50859 | val_1_rmse: 0.56905 |  0:00:13s
epoch 53 | loss: 0.26778 | val_0_rmse: 0.50321 | val_1_rmse: 0.56086 |  0:00:13s
epoch 54 | loss: 0.2784  | val_0_rmse: 0.50846 | val_1_rmse: 0.56638 |  0:00:13s
epoch 55 | loss: 0.27522 | val_0_rmse: 0.51363 | val_1_rmse: 0.57829 |  0:00:13s
epoch 56 | loss: 0.29561 | val_0_rmse: 0.50412 | val_1_rmse: 0.5654  |  0:00:14s
epoch 57 | loss: 0.27285 | val_0_rmse: 0.51301 | val_1_rmse: 0.56111 |  0:00:14s
epoch 58 | loss: 0.26977 | val_0_rmse: 0.52256 | val_1_rmse: 0.56785 |  0:00:14s
epoch 59 | loss: 0.26594 | val_0_rmse: 0.51539 | val_1_rmse: 0.57603 |  0:00:14s
epoch 60 | loss: 0.28552 | val_0_rmse: 0.51159 | val_1_rmse: 0.5871  |  0:00:15s
epoch 61 | loss: 0.26691 | val_0_rmse: 0.50363 | val_1_rmse: 0.58743 |  0:00:15s
epoch 62 | loss: 0.27594 | val_0_rmse: 0.50702 | val_1_rmse: 0.58785 |  0:00:15s
epoch 63 | loss: 0.27524 | val_0_rmse: 0.50334 | val_1_rmse: 0.59236 |  0:00:15s
epoch 64 | loss: 0.25984 | val_0_rmse: 0.49464 | val_1_rmse: 0.60097 |  0:00:16s
epoch 65 | loss: 0.25649 | val_0_rmse: 0.50071 | val_1_rmse: 0.61607 |  0:00:16s
epoch 66 | loss: 0.2602  | val_0_rmse: 0.49791 | val_1_rmse: 0.60191 |  0:00:16s
epoch 67 | loss: 0.25914 | val_0_rmse: 0.49637 | val_1_rmse: 0.58462 |  0:00:16s
epoch 68 | loss: 0.2604  | val_0_rmse: 0.48998 | val_1_rmse: 0.57762 |  0:00:17s
epoch 69 | loss: 0.26566 | val_0_rmse: 0.4907  | val_1_rmse: 0.57983 |  0:00:17s
epoch 70 | loss: 0.26033 | val_0_rmse: 0.49512 | val_1_rmse: 0.57601 |  0:00:17s
epoch 71 | loss: 0.25843 | val_0_rmse: 0.50117 | val_1_rmse: 0.57239 |  0:00:17s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.55197
Best weights from best epoch are automatically used!
ended training at: 02:48:56
Feature importance:
[('Area', 0.1290471503392932), ('Baths', 0.09920598998324258), ('Beds', 0.056435052181330665), ('Latitude', 0.2858147954096244), ('Longitude', 0.2357028655730573), ('Month', 0.0735792011527419), ('Year', 0.12021494536071)]
Mean squared error is of 25831386707.627335
Mean absolute error:120954.67135989011
MAPE:0.20976372792905879
R2 score:0.6880748812274688
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:48:57
epoch 0  | loss: 1.62608 | val_0_rmse: 1.32116 | val_1_rmse: 1.40186 |  0:00:00s
epoch 1  | loss: 0.95821 | val_0_rmse: 1.17546 | val_1_rmse: 1.30787 |  0:00:00s
epoch 2  | loss: 0.71798 | val_0_rmse: 0.99713 | val_1_rmse: 1.06558 |  0:00:00s
epoch 3  | loss: 0.59697 | val_0_rmse: 1.04511 | val_1_rmse: 1.11219 |  0:00:01s
epoch 4  | loss: 0.5337  | val_0_rmse: 1.1787  | val_1_rmse: 1.21536 |  0:00:01s
epoch 5  | loss: 0.48261 | val_0_rmse: 1.03114 | val_1_rmse: 1.05899 |  0:00:01s
epoch 6  | loss: 0.45449 | val_0_rmse: 0.9226  | val_1_rmse: 0.97207 |  0:00:01s
epoch 7  | loss: 0.42423 | val_0_rmse: 0.85994 | val_1_rmse: 0.92308 |  0:00:02s
epoch 8  | loss: 0.42394 | val_0_rmse: 0.75296 | val_1_rmse: 0.828   |  0:00:02s
epoch 9  | loss: 0.41506 | val_0_rmse: 0.77807 | val_1_rmse: 0.86185 |  0:00:02s
epoch 10 | loss: 0.40073 | val_0_rmse: 0.78462 | val_1_rmse: 0.85941 |  0:00:02s
epoch 11 | loss: 0.39598 | val_0_rmse: 0.66365 | val_1_rmse: 0.74866 |  0:00:03s
epoch 12 | loss: 0.38452 | val_0_rmse: 0.66908 | val_1_rmse: 0.76455 |  0:00:03s
epoch 13 | loss: 0.36109 | val_0_rmse: 0.65937 | val_1_rmse: 0.76138 |  0:00:03s
epoch 14 | loss: 0.37786 | val_0_rmse: 0.64507 | val_1_rmse: 0.75025 |  0:00:03s
epoch 15 | loss: 0.36569 | val_0_rmse: 0.64957 | val_1_rmse: 0.75569 |  0:00:04s
epoch 16 | loss: 0.3546  | val_0_rmse: 0.63915 | val_1_rmse: 0.77615 |  0:00:04s
epoch 17 | loss: 0.34827 | val_0_rmse: 0.63481 | val_1_rmse: 0.78443 |  0:00:04s
epoch 18 | loss: 0.33297 | val_0_rmse: 0.59738 | val_1_rmse: 0.73237 |  0:00:04s
epoch 19 | loss: 0.33688 | val_0_rmse: 0.58177 | val_1_rmse: 0.72127 |  0:00:05s
epoch 20 | loss: 0.33014 | val_0_rmse: 0.58445 | val_1_rmse: 0.71548 |  0:00:05s
epoch 21 | loss: 0.32496 | val_0_rmse: 0.56601 | val_1_rmse: 0.69405 |  0:00:05s
epoch 22 | loss: 0.32497 | val_0_rmse: 0.56752 | val_1_rmse: 0.70315 |  0:00:05s
epoch 23 | loss: 0.32546 | val_0_rmse: 0.57748 | val_1_rmse: 0.70876 |  0:00:06s
epoch 24 | loss: 0.32144 | val_0_rmse: 0.56485 | val_1_rmse: 0.69049 |  0:00:06s
epoch 25 | loss: 0.32449 | val_0_rmse: 0.56803 | val_1_rmse: 0.69705 |  0:00:06s
epoch 26 | loss: 0.32211 | val_0_rmse: 0.57037 | val_1_rmse: 0.70899 |  0:00:06s
epoch 27 | loss: 0.32804 | val_0_rmse: 0.57121 | val_1_rmse: 0.71468 |  0:00:07s
epoch 28 | loss: 0.31883 | val_0_rmse: 0.56587 | val_1_rmse: 0.70256 |  0:00:07s
epoch 29 | loss: 0.31183 | val_0_rmse: 0.57305 | val_1_rmse: 0.70374 |  0:00:07s
epoch 30 | loss: 0.31871 | val_0_rmse: 0.56911 | val_1_rmse: 0.70991 |  0:00:07s
epoch 31 | loss: 0.30432 | val_0_rmse: 0.55392 | val_1_rmse: 0.69088 |  0:00:08s
epoch 32 | loss: 0.31524 | val_0_rmse: 0.54983 | val_1_rmse: 0.68161 |  0:00:08s
epoch 33 | loss: 0.31319 | val_0_rmse: 0.55577 | val_1_rmse: 0.69116 |  0:00:08s
epoch 34 | loss: 0.30879 | val_0_rmse: 0.55365 | val_1_rmse: 0.69324 |  0:00:08s
epoch 35 | loss: 0.32904 | val_0_rmse: 0.57472 | val_1_rmse: 0.7257  |  0:00:09s
epoch 36 | loss: 0.34289 | val_0_rmse: 0.58465 | val_1_rmse: 0.71938 |  0:00:09s
epoch 37 | loss: 0.33995 | val_0_rmse: 0.58151 | val_1_rmse: 0.70214 |  0:00:09s
epoch 38 | loss: 0.32128 | val_0_rmse: 0.57554 | val_1_rmse: 0.68057 |  0:00:09s
epoch 39 | loss: 0.31584 | val_0_rmse: 0.59367 | val_1_rmse: 0.68863 |  0:00:10s
epoch 40 | loss: 0.31426 | val_0_rmse: 0.60242 | val_1_rmse: 0.70675 |  0:00:10s
epoch 41 | loss: 0.32098 | val_0_rmse: 0.56822 | val_1_rmse: 0.68961 |  0:00:10s
epoch 42 | loss: 0.30159 | val_0_rmse: 0.56767 | val_1_rmse: 0.68946 |  0:00:10s
epoch 43 | loss: 0.2918  | val_0_rmse: 0.56207 | val_1_rmse: 0.65771 |  0:00:11s
epoch 44 | loss: 0.30753 | val_0_rmse: 0.56647 | val_1_rmse: 0.66019 |  0:00:11s
epoch 45 | loss: 0.28213 | val_0_rmse: 0.56327 | val_1_rmse: 0.66753 |  0:00:11s
epoch 46 | loss: 0.28162 | val_0_rmse: 0.54289 | val_1_rmse: 0.64104 |  0:00:11s
epoch 47 | loss: 0.2889  | val_0_rmse: 0.53269 | val_1_rmse: 0.63671 |  0:00:12s
epoch 48 | loss: 0.27901 | val_0_rmse: 0.52917 | val_1_rmse: 0.64636 |  0:00:12s
epoch 49 | loss: 0.28803 | val_0_rmse: 0.52101 | val_1_rmse: 0.64315 |  0:00:12s
epoch 50 | loss: 0.28298 | val_0_rmse: 0.52162 | val_1_rmse: 0.65067 |  0:00:12s
epoch 51 | loss: 0.28499 | val_0_rmse: 0.5179  | val_1_rmse: 0.65495 |  0:00:13s
epoch 52 | loss: 0.30164 | val_0_rmse: 0.51848 | val_1_rmse: 0.65582 |  0:00:13s
epoch 53 | loss: 0.28556 | val_0_rmse: 0.51668 | val_1_rmse: 0.64437 |  0:00:13s
epoch 54 | loss: 0.30371 | val_0_rmse: 0.51698 | val_1_rmse: 0.63072 |  0:00:13s
epoch 55 | loss: 0.28342 | val_0_rmse: 0.5096  | val_1_rmse: 0.62758 |  0:00:14s
epoch 56 | loss: 0.283   | val_0_rmse: 0.51485 | val_1_rmse: 0.6352  |  0:00:14s
epoch 57 | loss: 0.28211 | val_0_rmse: 0.50954 | val_1_rmse: 0.64341 |  0:00:14s
epoch 58 | loss: 0.27027 | val_0_rmse: 0.50682 | val_1_rmse: 0.64055 |  0:00:14s
epoch 59 | loss: 0.282   | val_0_rmse: 0.49953 | val_1_rmse: 0.62635 |  0:00:15s
epoch 60 | loss: 0.27541 | val_0_rmse: 0.50576 | val_1_rmse: 0.6233  |  0:00:15s
epoch 61 | loss: 0.27399 | val_0_rmse: 0.49206 | val_1_rmse: 0.62017 |  0:00:15s
epoch 62 | loss: 0.28026 | val_0_rmse: 0.48903 | val_1_rmse: 0.62499 |  0:00:15s
epoch 63 | loss: 0.27348 | val_0_rmse: 0.49226 | val_1_rmse: 0.61892 |  0:00:16s
epoch 64 | loss: 0.25398 | val_0_rmse: 0.49036 | val_1_rmse: 0.60618 |  0:00:16s
epoch 65 | loss: 0.26739 | val_0_rmse: 0.49461 | val_1_rmse: 0.61475 |  0:00:16s
epoch 66 | loss: 0.2556  | val_0_rmse: 0.48405 | val_1_rmse: 0.61615 |  0:00:16s
epoch 67 | loss: 0.25761 | val_0_rmse: 0.48492 | val_1_rmse: 0.62092 |  0:00:17s
epoch 68 | loss: 0.25597 | val_0_rmse: 0.49157 | val_1_rmse: 0.62904 |  0:00:17s
epoch 69 | loss: 0.2571  | val_0_rmse: 0.47512 | val_1_rmse: 0.60635 |  0:00:17s
epoch 70 | loss: 0.25972 | val_0_rmse: 0.46861 | val_1_rmse: 0.60642 |  0:00:17s
epoch 71 | loss: 0.24634 | val_0_rmse: 0.48542 | val_1_rmse: 0.62658 |  0:00:18s
epoch 72 | loss: 0.26292 | val_0_rmse: 0.47888 | val_1_rmse: 0.6156  |  0:00:18s
epoch 73 | loss: 0.25106 | val_0_rmse: 0.4752  | val_1_rmse: 0.60748 |  0:00:18s
epoch 74 | loss: 0.24995 | val_0_rmse: 0.47824 | val_1_rmse: 0.61262 |  0:00:18s
epoch 75 | loss: 0.2665  | val_0_rmse: 0.48757 | val_1_rmse: 0.61524 |  0:00:19s
epoch 76 | loss: 0.25889 | val_0_rmse: 0.48746 | val_1_rmse: 0.6232  |  0:00:19s
epoch 77 | loss: 0.26106 | val_0_rmse: 0.49585 | val_1_rmse: 0.62753 |  0:00:19s
epoch 78 | loss: 0.24853 | val_0_rmse: 0.50025 | val_1_rmse: 0.63027 |  0:00:19s
epoch 79 | loss: 0.24887 | val_0_rmse: 0.4912  | val_1_rmse: 0.62651 |  0:00:20s
epoch 80 | loss: 0.24555 | val_0_rmse: 0.47328 | val_1_rmse: 0.61505 |  0:00:20s
epoch 81 | loss: 0.25785 | val_0_rmse: 0.47717 | val_1_rmse: 0.61475 |  0:00:20s
epoch 82 | loss: 0.24832 | val_0_rmse: 0.47696 | val_1_rmse: 0.62238 |  0:00:20s
epoch 83 | loss: 0.25299 | val_0_rmse: 0.47501 | val_1_rmse: 0.60483 |  0:00:21s
epoch 84 | loss: 0.24683 | val_0_rmse: 0.48335 | val_1_rmse: 0.61072 |  0:00:21s
epoch 85 | loss: 0.24694 | val_0_rmse: 0.48243 | val_1_rmse: 0.61849 |  0:00:21s
epoch 86 | loss: 0.2529  | val_0_rmse: 0.47579 | val_1_rmse: 0.60987 |  0:00:21s
epoch 87 | loss: 0.24065 | val_0_rmse: 0.47863 | val_1_rmse: 0.61149 |  0:00:22s
epoch 88 | loss: 0.24845 | val_0_rmse: 0.46762 | val_1_rmse: 0.60496 |  0:00:22s
epoch 89 | loss: 0.24551 | val_0_rmse: 0.4688  | val_1_rmse: 0.60241 |  0:00:22s
epoch 90 | loss: 0.25303 | val_0_rmse: 0.47112 | val_1_rmse: 0.59648 |  0:00:22s
epoch 91 | loss: 0.2475  | val_0_rmse: 0.47733 | val_1_rmse: 0.59545 |  0:00:23s
epoch 92 | loss: 0.23991 | val_0_rmse: 0.47729 | val_1_rmse: 0.60017 |  0:00:23s
epoch 93 | loss: 0.24016 | val_0_rmse: 0.47617 | val_1_rmse: 0.60791 |  0:00:23s
epoch 94 | loss: 0.24329 | val_0_rmse: 0.46552 | val_1_rmse: 0.60728 |  0:00:23s
epoch 95 | loss: 0.23862 | val_0_rmse: 0.45928 | val_1_rmse: 0.59892 |  0:00:24s
epoch 96 | loss: 0.25047 | val_0_rmse: 0.45701 | val_1_rmse: 0.59775 |  0:00:24s
epoch 97 | loss: 0.23088 | val_0_rmse: 0.4626  | val_1_rmse: 0.60676 |  0:00:24s
epoch 98 | loss: 0.24028 | val_0_rmse: 0.46512 | val_1_rmse: 0.62392 |  0:00:24s
epoch 99 | loss: 0.24646 | val_0_rmse: 0.47133 | val_1_rmse: 0.63242 |  0:00:25s
epoch 100| loss: 0.23775 | val_0_rmse: 0.4736  | val_1_rmse: 0.62259 |  0:00:25s
epoch 101| loss: 0.24505 | val_0_rmse: 0.4728  | val_1_rmse: 0.61591 |  0:00:25s
epoch 102| loss: 0.23534 | val_0_rmse: 0.47241 | val_1_rmse: 0.61779 |  0:00:25s
epoch 103| loss: 0.24274 | val_0_rmse: 0.47096 | val_1_rmse: 0.60972 |  0:00:26s
epoch 104| loss: 0.23891 | val_0_rmse: 0.47099 | val_1_rmse: 0.62012 |  0:00:26s
epoch 105| loss: 0.23471 | val_0_rmse: 0.47122 | val_1_rmse: 0.6279  |  0:00:26s
epoch 106| loss: 0.24558 | val_0_rmse: 0.47374 | val_1_rmse: 0.62848 |  0:00:26s
epoch 107| loss: 0.24111 | val_0_rmse: 0.46836 | val_1_rmse: 0.62541 |  0:00:26s
epoch 108| loss: 0.23887 | val_0_rmse: 0.46735 | val_1_rmse: 0.62876 |  0:00:27s
epoch 109| loss: 0.25032 | val_0_rmse: 0.46342 | val_1_rmse: 0.62926 |  0:00:27s
epoch 110| loss: 0.23861 | val_0_rmse: 0.47144 | val_1_rmse: 0.62685 |  0:00:27s
epoch 111| loss: 0.24841 | val_0_rmse: 0.48316 | val_1_rmse: 0.6299  |  0:00:28s
epoch 112| loss: 0.24863 | val_0_rmse: 0.4588  | val_1_rmse: 0.61048 |  0:00:28s
epoch 113| loss: 0.22862 | val_0_rmse: 0.47364 | val_1_rmse: 0.63122 |  0:00:28s
epoch 114| loss: 0.24088 | val_0_rmse: 0.47848 | val_1_rmse: 0.63683 |  0:00:28s
epoch 115| loss: 0.24376 | val_0_rmse: 0.46336 | val_1_rmse: 0.60684 |  0:00:29s
epoch 116| loss: 0.23853 | val_0_rmse: 0.46661 | val_1_rmse: 0.60934 |  0:00:29s
epoch 117| loss: 0.23611 | val_0_rmse: 0.48493 | val_1_rmse: 0.64566 |  0:00:29s
epoch 118| loss: 0.24364 | val_0_rmse: 0.45423 | val_1_rmse: 0.62137 |  0:00:29s
epoch 119| loss: 0.2399  | val_0_rmse: 0.4597  | val_1_rmse: 0.61646 |  0:00:29s
epoch 120| loss: 0.24119 | val_0_rmse: 0.5007  | val_1_rmse: 0.65323 |  0:00:30s
epoch 121| loss: 0.24502 | val_0_rmse: 0.46893 | val_1_rmse: 0.62706 |  0:00:30s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.59545
Best weights from best epoch are automatically used!
ended training at: 02:49:27
Feature importance:
[('Area', 0.25660323240186184), ('Baths', 0.017567832361950017), ('Beds', 0.11715740163819593), ('Latitude', 0.23648255604186785), ('Longitude', 0.30204784508860677), ('Month', 0.022240699058604343), ('Year', 0.04790043340891328)]
Mean squared error is of 23608327095.747265
Mean absolute error:112696.09373626375
MAPE:0.19662029652021515
R2 score:0.7082879401763009
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:49:27
epoch 0  | loss: 1.64132 | val_0_rmse: 1.13797 | val_1_rmse: 1.29076 |  0:00:00s
epoch 1  | loss: 1.08196 | val_0_rmse: 2.0653  | val_1_rmse: 2.02707 |  0:00:00s
epoch 2  | loss: 0.72932 | val_0_rmse: 1.03784 | val_1_rmse: 1.03704 |  0:00:00s
epoch 3  | loss: 0.62614 | val_0_rmse: 0.86906 | val_1_rmse: 0.80062 |  0:00:01s
epoch 4  | loss: 0.5479  | val_0_rmse: 0.92454 | val_1_rmse: 0.8804  |  0:00:01s
epoch 5  | loss: 0.53107 | val_0_rmse: 0.81077 | val_1_rmse: 0.83099 |  0:00:01s
epoch 6  | loss: 0.47084 | val_0_rmse: 0.80212 | val_1_rmse: 0.82646 |  0:00:01s
epoch 7  | loss: 0.45014 | val_0_rmse: 0.89603 | val_1_rmse: 0.90896 |  0:00:02s
epoch 8  | loss: 0.45053 | val_0_rmse: 0.76815 | val_1_rmse: 0.76722 |  0:00:02s
epoch 9  | loss: 0.43996 | val_0_rmse: 0.70315 | val_1_rmse: 0.71814 |  0:00:02s
epoch 10 | loss: 0.43555 | val_0_rmse: 0.72709 | val_1_rmse: 0.72006 |  0:00:02s
epoch 11 | loss: 0.42941 | val_0_rmse: 0.70364 | val_1_rmse: 0.72411 |  0:00:03s
epoch 12 | loss: 0.42208 | val_0_rmse: 0.69377 | val_1_rmse: 0.71717 |  0:00:03s
epoch 13 | loss: 0.4054  | val_0_rmse: 0.70049 | val_1_rmse: 0.69729 |  0:00:03s
epoch 14 | loss: 0.40595 | val_0_rmse: 0.65915 | val_1_rmse: 0.62177 |  0:00:03s
epoch 15 | loss: 0.40759 | val_0_rmse: 0.65722 | val_1_rmse: 0.59376 |  0:00:04s
epoch 16 | loss: 0.40621 | val_0_rmse: 0.63975 | val_1_rmse: 0.6155  |  0:00:04s
epoch 17 | loss: 0.39299 | val_0_rmse: 0.61296 | val_1_rmse: 0.60974 |  0:00:04s
epoch 18 | loss: 0.37734 | val_0_rmse: 0.60593 | val_1_rmse: 0.60277 |  0:00:04s
epoch 19 | loss: 0.37572 | val_0_rmse: 0.59426 | val_1_rmse: 0.58017 |  0:00:05s
epoch 20 | loss: 0.36324 | val_0_rmse: 0.59772 | val_1_rmse: 0.56668 |  0:00:05s
epoch 21 | loss: 0.36627 | val_0_rmse: 0.58266 | val_1_rmse: 0.55089 |  0:00:05s
epoch 22 | loss: 0.35007 | val_0_rmse: 0.58907 | val_1_rmse: 0.54456 |  0:00:05s
epoch 23 | loss: 0.33904 | val_0_rmse: 0.57571 | val_1_rmse: 0.54113 |  0:00:06s
epoch 24 | loss: 0.33283 | val_0_rmse: 0.56707 | val_1_rmse: 0.55136 |  0:00:06s
epoch 25 | loss: 0.34517 | val_0_rmse: 0.58023 | val_1_rmse: 0.54241 |  0:00:06s
epoch 26 | loss: 0.32619 | val_0_rmse: 0.58268 | val_1_rmse: 0.55054 |  0:00:06s
epoch 27 | loss: 0.32649 | val_0_rmse: 0.55494 | val_1_rmse: 0.52425 |  0:00:07s
epoch 28 | loss: 0.31241 | val_0_rmse: 0.55723 | val_1_rmse: 0.53034 |  0:00:07s
epoch 29 | loss: 0.31784 | val_0_rmse: 0.55351 | val_1_rmse: 0.52495 |  0:00:07s
epoch 30 | loss: 0.31105 | val_0_rmse: 0.54263 | val_1_rmse: 0.52982 |  0:00:07s
epoch 31 | loss: 0.31877 | val_0_rmse: 0.55402 | val_1_rmse: 0.54151 |  0:00:08s
epoch 32 | loss: 0.31766 | val_0_rmse: 0.54753 | val_1_rmse: 0.53295 |  0:00:08s
epoch 33 | loss: 0.30076 | val_0_rmse: 0.53591 | val_1_rmse: 0.52777 |  0:00:08s
epoch 34 | loss: 0.30244 | val_0_rmse: 0.53179 | val_1_rmse: 0.52358 |  0:00:08s
epoch 35 | loss: 0.29895 | val_0_rmse: 0.53077 | val_1_rmse: 0.52219 |  0:00:09s
epoch 36 | loss: 0.29722 | val_0_rmse: 0.52804 | val_1_rmse: 0.52626 |  0:00:09s
epoch 37 | loss: 0.29903 | val_0_rmse: 0.52621 | val_1_rmse: 0.53647 |  0:00:09s
epoch 38 | loss: 0.28703 | val_0_rmse: 0.51782 | val_1_rmse: 0.52908 |  0:00:09s
epoch 39 | loss: 0.27917 | val_0_rmse: 0.52649 | val_1_rmse: 0.52076 |  0:00:10s
epoch 40 | loss: 0.2959  | val_0_rmse: 0.51232 | val_1_rmse: 0.51815 |  0:00:10s
epoch 41 | loss: 0.29585 | val_0_rmse: 0.50965 | val_1_rmse: 0.53912 |  0:00:10s
epoch 42 | loss: 0.28267 | val_0_rmse: 0.51655 | val_1_rmse: 0.52241 |  0:00:10s
epoch 43 | loss: 0.29508 | val_0_rmse: 0.52467 | val_1_rmse: 0.52891 |  0:00:11s
epoch 44 | loss: 0.2832  | val_0_rmse: 0.5263  | val_1_rmse: 0.53126 |  0:00:11s
epoch 45 | loss: 0.29161 | val_0_rmse: 0.51423 | val_1_rmse: 0.52074 |  0:00:11s
epoch 46 | loss: 0.2794  | val_0_rmse: 0.50523 | val_1_rmse: 0.52637 |  0:00:11s
epoch 47 | loss: 0.27365 | val_0_rmse: 0.51374 | val_1_rmse: 0.53867 |  0:00:12s
epoch 48 | loss: 0.28417 | val_0_rmse: 0.50847 | val_1_rmse: 0.53565 |  0:00:12s
epoch 49 | loss: 0.2747  | val_0_rmse: 0.5025  | val_1_rmse: 0.52876 |  0:00:12s
epoch 50 | loss: 0.27327 | val_0_rmse: 0.50254 | val_1_rmse: 0.52541 |  0:00:12s
epoch 51 | loss: 0.27438 | val_0_rmse: 0.51036 | val_1_rmse: 0.53169 |  0:00:13s
epoch 52 | loss: 0.276   | val_0_rmse: 0.50632 | val_1_rmse: 0.52175 |  0:00:13s
epoch 53 | loss: 0.26736 | val_0_rmse: 0.50391 | val_1_rmse: 0.51582 |  0:00:13s
epoch 54 | loss: 0.27544 | val_0_rmse: 0.50167 | val_1_rmse: 0.51626 |  0:00:13s
epoch 55 | loss: 0.27558 | val_0_rmse: 0.51263 | val_1_rmse: 0.52053 |  0:00:14s
epoch 56 | loss: 0.27079 | val_0_rmse: 0.51443 | val_1_rmse: 0.51137 |  0:00:14s
epoch 57 | loss: 0.27393 | val_0_rmse: 0.51492 | val_1_rmse: 0.50931 |  0:00:14s
epoch 58 | loss: 0.2922  | val_0_rmse: 0.51022 | val_1_rmse: 0.50884 |  0:00:14s
epoch 59 | loss: 0.27376 | val_0_rmse: 0.52391 | val_1_rmse: 0.5478  |  0:00:15s
epoch 60 | loss: 0.29337 | val_0_rmse: 0.51585 | val_1_rmse: 0.52795 |  0:00:15s
epoch 61 | loss: 0.28058 | val_0_rmse: 0.51161 | val_1_rmse: 0.51572 |  0:00:15s
epoch 62 | loss: 0.26937 | val_0_rmse: 0.50801 | val_1_rmse: 0.53268 |  0:00:15s
epoch 63 | loss: 0.27831 | val_0_rmse: 0.49327 | val_1_rmse: 0.52044 |  0:00:16s
epoch 64 | loss: 0.2633  | val_0_rmse: 0.53482 | val_1_rmse: 0.5391  |  0:00:16s
epoch 65 | loss: 0.28272 | val_0_rmse: 0.49572 | val_1_rmse: 0.52457 |  0:00:16s
epoch 66 | loss: 0.26255 | val_0_rmse: 0.50069 | val_1_rmse: 0.54493 |  0:00:16s
epoch 67 | loss: 0.26845 | val_0_rmse: 0.50413 | val_1_rmse: 0.53668 |  0:00:17s
epoch 68 | loss: 0.26963 | val_0_rmse: 0.49981 | val_1_rmse: 0.52567 |  0:00:17s
epoch 69 | loss: 0.2488  | val_0_rmse: 0.49744 | val_1_rmse: 0.54305 |  0:00:17s
epoch 70 | loss: 0.27612 | val_0_rmse: 0.48337 | val_1_rmse: 0.53112 |  0:00:17s
epoch 71 | loss: 0.26582 | val_0_rmse: 0.49447 | val_1_rmse: 0.52561 |  0:00:18s
epoch 72 | loss: 0.27173 | val_0_rmse: 0.48763 | val_1_rmse: 0.52613 |  0:00:18s
epoch 73 | loss: 0.26001 | val_0_rmse: 0.49759 | val_1_rmse: 0.53872 |  0:00:18s
epoch 74 | loss: 0.26173 | val_0_rmse: 0.49699 | val_1_rmse: 0.53815 |  0:00:18s
epoch 75 | loss: 0.26511 | val_0_rmse: 0.49374 | val_1_rmse: 0.54229 |  0:00:19s
epoch 76 | loss: 0.26955 | val_0_rmse: 0.49754 | val_1_rmse: 0.538   |  0:00:19s
epoch 77 | loss: 0.26344 | val_0_rmse: 0.4977  | val_1_rmse: 0.5277  |  0:00:19s
epoch 78 | loss: 0.26277 | val_0_rmse: 0.49199 | val_1_rmse: 0.52217 |  0:00:19s
epoch 79 | loss: 0.26896 | val_0_rmse: 0.49236 | val_1_rmse: 0.5341  |  0:00:20s
epoch 80 | loss: 0.2652  | val_0_rmse: 0.49142 | val_1_rmse: 0.52036 |  0:00:20s
epoch 81 | loss: 0.25932 | val_0_rmse: 0.49285 | val_1_rmse: 0.52203 |  0:00:20s
epoch 82 | loss: 0.26511 | val_0_rmse: 0.49166 | val_1_rmse: 0.53355 |  0:00:20s
epoch 83 | loss: 0.2575  | val_0_rmse: 0.4824  | val_1_rmse: 0.52954 |  0:00:21s
epoch 84 | loss: 0.24742 | val_0_rmse: 0.49188 | val_1_rmse: 0.53023 |  0:00:21s
epoch 85 | loss: 0.25922 | val_0_rmse: 0.47911 | val_1_rmse: 0.53408 |  0:00:21s
epoch 86 | loss: 0.25507 | val_0_rmse: 0.49115 | val_1_rmse: 0.55343 |  0:00:21s
epoch 87 | loss: 0.25196 | val_0_rmse: 0.49756 | val_1_rmse: 0.51929 |  0:00:22s
epoch 88 | loss: 0.26657 | val_0_rmse: 0.48454 | val_1_rmse: 0.51953 |  0:00:22s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.50884
Best weights from best epoch are automatically used!
ended training at: 02:49:50
Feature importance:
[('Area', 0.17718241565653173), ('Baths', 0.017410910477791468), ('Beds', 0.08612387103295215), ('Latitude', 0.19712886999889315), ('Longitude', 0.34069444638917046), ('Month', 0.07882465341436906), ('Year', 0.10263483303029201)]
Mean squared error is of 25398599672.713985
Mean absolute error:117019.8356989011
MAPE:0.2120253968041501
R2 score:0.6662974614507964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:49:50
epoch 0  | loss: 1.72787 | val_0_rmse: 1.47841 | val_1_rmse: 1.4962  |  0:00:00s
epoch 1  | loss: 0.99131 | val_0_rmse: 1.49863 | val_1_rmse: 1.43637 |  0:00:00s
epoch 2  | loss: 0.75597 | val_0_rmse: 1.19475 | val_1_rmse: 1.19261 |  0:00:00s
epoch 3  | loss: 0.6725  | val_0_rmse: 1.28313 | val_1_rmse: 1.26836 |  0:00:01s
epoch 4  | loss: 0.61536 | val_0_rmse: 1.11693 | val_1_rmse: 1.18112 |  0:00:01s
epoch 5  | loss: 0.55632 | val_0_rmse: 0.95671 | val_1_rmse: 1.05222 |  0:00:01s
epoch 6  | loss: 0.50481 | val_0_rmse: 0.92237 | val_1_rmse: 1.05417 |  0:00:01s
epoch 7  | loss: 0.4528  | val_0_rmse: 0.96815 | val_1_rmse: 0.99558 |  0:00:02s
epoch 8  | loss: 0.45492 | val_0_rmse: 0.84395 | val_1_rmse: 0.83014 |  0:00:02s
epoch 9  | loss: 0.41704 | val_0_rmse: 0.73291 | val_1_rmse: 0.75614 |  0:00:02s
epoch 10 | loss: 0.41601 | val_0_rmse: 0.70883 | val_1_rmse: 0.74738 |  0:00:02s
epoch 11 | loss: 0.37724 | val_0_rmse: 0.73948 | val_1_rmse: 0.77013 |  0:00:03s
epoch 12 | loss: 0.37747 | val_0_rmse: 0.71444 | val_1_rmse: 0.74676 |  0:00:03s
epoch 13 | loss: 0.37236 | val_0_rmse: 0.65445 | val_1_rmse: 0.7036  |  0:00:03s
epoch 14 | loss: 0.36054 | val_0_rmse: 0.63196 | val_1_rmse: 0.67367 |  0:00:03s
epoch 15 | loss: 0.34711 | val_0_rmse: 0.6374  | val_1_rmse: 0.68007 |  0:00:04s
epoch 16 | loss: 0.3463  | val_0_rmse: 0.60157 | val_1_rmse: 0.63838 |  0:00:04s
epoch 17 | loss: 0.33615 | val_0_rmse: 0.59201 | val_1_rmse: 0.63902 |  0:00:04s
epoch 18 | loss: 0.32295 | val_0_rmse: 0.62055 | val_1_rmse: 0.66333 |  0:00:04s
epoch 19 | loss: 0.32807 | val_0_rmse: 0.6139  | val_1_rmse: 0.65434 |  0:00:05s
epoch 20 | loss: 0.33552 | val_0_rmse: 0.58248 | val_1_rmse: 0.63497 |  0:00:05s
epoch 21 | loss: 0.32058 | val_0_rmse: 0.56026 | val_1_rmse: 0.61276 |  0:00:05s
epoch 22 | loss: 0.318   | val_0_rmse: 0.56171 | val_1_rmse: 0.6118  |  0:00:05s
epoch 23 | loss: 0.31841 | val_0_rmse: 0.56161 | val_1_rmse: 0.60502 |  0:00:06s
epoch 24 | loss: 0.31299 | val_0_rmse: 0.56156 | val_1_rmse: 0.60366 |  0:00:06s
epoch 25 | loss: 0.31202 | val_0_rmse: 0.55093 | val_1_rmse: 0.59927 |  0:00:06s
epoch 26 | loss: 0.31474 | val_0_rmse: 0.5479  | val_1_rmse: 0.60125 |  0:00:07s
epoch 27 | loss: 0.32425 | val_0_rmse: 0.55164 | val_1_rmse: 0.60579 |  0:00:07s
epoch 28 | loss: 0.30578 | val_0_rmse: 0.55572 | val_1_rmse: 0.60933 |  0:00:07s
epoch 29 | loss: 0.31692 | val_0_rmse: 0.55011 | val_1_rmse: 0.59793 |  0:00:07s
epoch 30 | loss: 0.31753 | val_0_rmse: 0.55715 | val_1_rmse: 0.60038 |  0:00:08s
epoch 31 | loss: 0.31118 | val_0_rmse: 0.54668 | val_1_rmse: 0.59537 |  0:00:08s
epoch 32 | loss: 0.31166 | val_0_rmse: 0.53871 | val_1_rmse: 0.59135 |  0:00:08s
epoch 33 | loss: 0.31414 | val_0_rmse: 0.53611 | val_1_rmse: 0.59174 |  0:00:08s
epoch 34 | loss: 0.30364 | val_0_rmse: 0.53616 | val_1_rmse: 0.59489 |  0:00:09s
epoch 35 | loss: 0.30457 | val_0_rmse: 0.53676 | val_1_rmse: 0.59704 |  0:00:09s
epoch 36 | loss: 0.29153 | val_0_rmse: 0.53226 | val_1_rmse: 0.5903  |  0:00:09s
epoch 37 | loss: 0.30358 | val_0_rmse: 0.53253 | val_1_rmse: 0.59235 |  0:00:09s
epoch 38 | loss: 0.29499 | val_0_rmse: 0.53112 | val_1_rmse: 0.59684 |  0:00:10s
epoch 39 | loss: 0.3141  | val_0_rmse: 0.53865 | val_1_rmse: 0.6021  |  0:00:10s
epoch 40 | loss: 0.30917 | val_0_rmse: 0.53175 | val_1_rmse: 0.59255 |  0:00:10s
epoch 41 | loss: 0.29521 | val_0_rmse: 0.53391 | val_1_rmse: 0.59684 |  0:00:10s
epoch 42 | loss: 0.3004  | val_0_rmse: 0.53402 | val_1_rmse: 0.59064 |  0:00:10s
epoch 43 | loss: 0.30218 | val_0_rmse: 0.5359  | val_1_rmse: 0.58588 |  0:00:11s
epoch 44 | loss: 0.28806 | val_0_rmse: 0.52789 | val_1_rmse: 0.58695 |  0:00:11s
epoch 45 | loss: 0.29367 | val_0_rmse: 0.52667 | val_1_rmse: 0.59314 |  0:00:11s
epoch 46 | loss: 0.28764 | val_0_rmse: 0.52651 | val_1_rmse: 0.59088 |  0:00:11s
epoch 47 | loss: 0.29619 | val_0_rmse: 0.51993 | val_1_rmse: 0.59646 |  0:00:12s
epoch 48 | loss: 0.29609 | val_0_rmse: 0.5204  | val_1_rmse: 0.59125 |  0:00:12s
epoch 49 | loss: 0.30077 | val_0_rmse: 0.52741 | val_1_rmse: 0.59661 |  0:00:12s
epoch 50 | loss: 0.29425 | val_0_rmse: 0.53066 | val_1_rmse: 0.59861 |  0:00:12s
epoch 51 | loss: 0.29305 | val_0_rmse: 0.53186 | val_1_rmse: 0.60964 |  0:00:13s
epoch 52 | loss: 0.29064 | val_0_rmse: 0.51959 | val_1_rmse: 0.60822 |  0:00:13s
epoch 53 | loss: 0.28023 | val_0_rmse: 0.52651 | val_1_rmse: 0.61217 |  0:00:13s
epoch 54 | loss: 0.29246 | val_0_rmse: 0.5189  | val_1_rmse: 0.5877  |  0:00:13s
epoch 55 | loss: 0.28907 | val_0_rmse: 0.5203  | val_1_rmse: 0.58935 |  0:00:14s
epoch 56 | loss: 0.28863 | val_0_rmse: 0.52545 | val_1_rmse: 0.60338 |  0:00:14s
epoch 57 | loss: 0.29672 | val_0_rmse: 0.51886 | val_1_rmse: 0.59907 |  0:00:14s
epoch 58 | loss: 0.2884  | val_0_rmse: 0.5144  | val_1_rmse: 0.59501 |  0:00:14s
epoch 59 | loss: 0.27732 | val_0_rmse: 0.51187 | val_1_rmse: 0.6076  |  0:00:15s
epoch 60 | loss: 0.28446 | val_0_rmse: 0.50843 | val_1_rmse: 0.6059  |  0:00:15s
epoch 61 | loss: 0.28188 | val_0_rmse: 0.51232 | val_1_rmse: 0.59082 |  0:00:15s
epoch 62 | loss: 0.28637 | val_0_rmse: 0.51761 | val_1_rmse: 0.59007 |  0:00:15s
epoch 63 | loss: 0.28318 | val_0_rmse: 0.51475 | val_1_rmse: 0.5882  |  0:00:16s
epoch 64 | loss: 0.26976 | val_0_rmse: 0.52385 | val_1_rmse: 0.58824 |  0:00:16s
epoch 65 | loss: 0.28319 | val_0_rmse: 0.51002 | val_1_rmse: 0.58301 |  0:00:16s
epoch 66 | loss: 0.27733 | val_0_rmse: 0.50769 | val_1_rmse: 0.58396 |  0:00:16s
epoch 67 | loss: 0.27735 | val_0_rmse: 0.51142 | val_1_rmse: 0.59192 |  0:00:17s
epoch 68 | loss: 0.27522 | val_0_rmse: 0.50542 | val_1_rmse: 0.58922 |  0:00:17s
epoch 69 | loss: 0.27645 | val_0_rmse: 0.5018  | val_1_rmse: 0.59179 |  0:00:17s
epoch 70 | loss: 0.25826 | val_0_rmse: 0.49947 | val_1_rmse: 0.58659 |  0:00:17s
epoch 71 | loss: 0.25862 | val_0_rmse: 0.50051 | val_1_rmse: 0.58348 |  0:00:18s
epoch 72 | loss: 0.27067 | val_0_rmse: 0.50502 | val_1_rmse: 0.57984 |  0:00:18s
epoch 73 | loss: 0.27204 | val_0_rmse: 0.49516 | val_1_rmse: 0.57275 |  0:00:18s
epoch 74 | loss: 0.2685  | val_0_rmse: 0.49947 | val_1_rmse: 0.58301 |  0:00:18s
epoch 75 | loss: 0.27035 | val_0_rmse: 0.50691 | val_1_rmse: 0.57859 |  0:00:19s
epoch 76 | loss: 0.27358 | val_0_rmse: 0.50249 | val_1_rmse: 0.58117 |  0:00:19s
epoch 77 | loss: 0.26937 | val_0_rmse: 0.50334 | val_1_rmse: 0.57425 |  0:00:19s
epoch 78 | loss: 0.28068 | val_0_rmse: 0.49996 | val_1_rmse: 0.58032 |  0:00:19s
epoch 79 | loss: 0.27847 | val_0_rmse: 0.50481 | val_1_rmse: 0.58051 |  0:00:20s
epoch 80 | loss: 0.27322 | val_0_rmse: 0.51686 | val_1_rmse: 0.5715  |  0:00:20s
epoch 81 | loss: 0.27582 | val_0_rmse: 0.50972 | val_1_rmse: 0.56823 |  0:00:20s
epoch 82 | loss: 0.26765 | val_0_rmse: 0.51018 | val_1_rmse: 0.58911 |  0:00:20s
epoch 83 | loss: 0.27318 | val_0_rmse: 0.50691 | val_1_rmse: 0.5897  |  0:00:21s
epoch 84 | loss: 0.26658 | val_0_rmse: 0.50028 | val_1_rmse: 0.57478 |  0:00:21s
epoch 85 | loss: 0.27341 | val_0_rmse: 0.51226 | val_1_rmse: 0.58297 |  0:00:21s
epoch 86 | loss: 0.27023 | val_0_rmse: 0.49924 | val_1_rmse: 0.57995 |  0:00:21s
epoch 87 | loss: 0.27953 | val_0_rmse: 0.50364 | val_1_rmse: 0.59302 |  0:00:22s
epoch 88 | loss: 0.2694  | val_0_rmse: 0.50435 | val_1_rmse: 0.58616 |  0:00:22s
epoch 89 | loss: 0.27065 | val_0_rmse: 0.5011  | val_1_rmse: 0.5826  |  0:00:22s
epoch 90 | loss: 0.27191 | val_0_rmse: 0.5015  | val_1_rmse: 0.58338 |  0:00:22s
epoch 91 | loss: 0.28888 | val_0_rmse: 0.50071 | val_1_rmse: 0.57745 |  0:00:23s
epoch 92 | loss: 0.27784 | val_0_rmse: 0.50422 | val_1_rmse: 0.58398 |  0:00:23s
epoch 93 | loss: 0.27124 | val_0_rmse: 0.49899 | val_1_rmse: 0.583   |  0:00:23s
epoch 94 | loss: 0.27004 | val_0_rmse: 0.50489 | val_1_rmse: 0.57434 |  0:00:23s
epoch 95 | loss: 0.27891 | val_0_rmse: 0.50647 | val_1_rmse: 0.57882 |  0:00:24s
epoch 96 | loss: 0.28162 | val_0_rmse: 0.49752 | val_1_rmse: 0.58183 |  0:00:24s
epoch 97 | loss: 0.27787 | val_0_rmse: 0.49366 | val_1_rmse: 0.58235 |  0:00:24s
epoch 98 | loss: 0.26174 | val_0_rmse: 0.49395 | val_1_rmse: 0.58296 |  0:00:24s
epoch 99 | loss: 0.26994 | val_0_rmse: 0.48994 | val_1_rmse: 0.58259 |  0:00:24s
epoch 100| loss: 0.26096 | val_0_rmse: 0.50238 | val_1_rmse: 0.58628 |  0:00:25s
epoch 101| loss: 0.27518 | val_0_rmse: 0.49541 | val_1_rmse: 0.57977 |  0:00:25s
epoch 102| loss: 0.27886 | val_0_rmse: 0.50072 | val_1_rmse: 0.57848 |  0:00:25s
epoch 103| loss: 0.25739 | val_0_rmse: 0.52884 | val_1_rmse: 0.59096 |  0:00:25s
epoch 104| loss: 0.27894 | val_0_rmse: 0.49837 | val_1_rmse: 0.56986 |  0:00:26s
epoch 105| loss: 0.26369 | val_0_rmse: 0.49477 | val_1_rmse: 0.57356 |  0:00:26s
epoch 106| loss: 0.26337 | val_0_rmse: 0.51238 | val_1_rmse: 0.57765 |  0:00:26s
epoch 107| loss: 0.26956 | val_0_rmse: 0.49553 | val_1_rmse: 0.57198 |  0:00:26s
epoch 108| loss: 0.26223 | val_0_rmse: 0.50806 | val_1_rmse: 0.60096 |  0:00:27s
epoch 109| loss: 0.27684 | val_0_rmse: 0.51337 | val_1_rmse: 0.59641 |  0:00:27s
epoch 110| loss: 0.26704 | val_0_rmse: 0.49523 | val_1_rmse: 0.57537 |  0:00:27s
epoch 111| loss: 0.25972 | val_0_rmse: 0.49499 | val_1_rmse: 0.58255 |  0:00:27s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.56823
Best weights from best epoch are automatically used!
ended training at: 02:50:18
Feature importance:
[('Area', 0.2234759870917648), ('Baths', 0.029838690428788752), ('Beds', 0.10537278098205774), ('Latitude', 0.30523053607969086), ('Longitude', 0.24661613293088686), ('Month', 0.018072031597753634), ('Year', 0.07139384088905734)]
Mean squared error is of 23522274749.014038
Mean absolute error:111414.93963406593
MAPE:0.188965426433982
R2 score:0.7244842933784275
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:50:18
epoch 0  | loss: 1.79513 | val_0_rmse: 1.67089 | val_1_rmse: 1.79381 |  0:00:00s
epoch 1  | loss: 1.0482  | val_0_rmse: 1.16168 | val_1_rmse: 1.10951 |  0:00:00s
epoch 2  | loss: 0.86735 | val_0_rmse: 1.02163 | val_1_rmse: 0.98251 |  0:00:00s
epoch 3  | loss: 0.77272 | val_0_rmse: 1.0233  | val_1_rmse: 1.00466 |  0:00:01s
epoch 4  | loss: 0.71035 | val_0_rmse: 1.08863 | val_1_rmse: 1.05155 |  0:00:01s
epoch 5  | loss: 0.6558  | val_0_rmse: 0.96697 | val_1_rmse: 0.89252 |  0:00:01s
epoch 6  | loss: 0.59638 | val_0_rmse: 0.86895 | val_1_rmse: 0.81036 |  0:00:01s
epoch 7  | loss: 0.55289 | val_0_rmse: 0.96523 | val_1_rmse: 0.9622  |  0:00:02s
epoch 8  | loss: 0.51    | val_0_rmse: 0.95408 | val_1_rmse: 0.94143 |  0:00:02s
epoch 9  | loss: 0.49429 | val_0_rmse: 0.8435  | val_1_rmse: 0.81328 |  0:00:02s
epoch 10 | loss: 0.45629 | val_0_rmse: 0.83327 | val_1_rmse: 0.79988 |  0:00:02s
epoch 11 | loss: 0.41987 | val_0_rmse: 0.91296 | val_1_rmse: 0.86641 |  0:00:03s
epoch 12 | loss: 0.42979 | val_0_rmse: 0.74339 | val_1_rmse: 0.72567 |  0:00:03s
epoch 13 | loss: 0.40843 | val_0_rmse: 0.68073 | val_1_rmse: 0.68341 |  0:00:03s
epoch 14 | loss: 0.38782 | val_0_rmse: 0.71505 | val_1_rmse: 0.7313  |  0:00:03s
epoch 15 | loss: 0.39276 | val_0_rmse: 0.64456 | val_1_rmse: 0.65048 |  0:00:04s
epoch 16 | loss: 0.37864 | val_0_rmse: 0.6144  | val_1_rmse: 0.61855 |  0:00:04s
epoch 17 | loss: 0.3854  | val_0_rmse: 0.6605  | val_1_rmse: 0.64519 |  0:00:04s
epoch 18 | loss: 0.36461 | val_0_rmse: 0.70086 | val_1_rmse: 0.66139 |  0:00:04s
epoch 19 | loss: 0.36576 | val_0_rmse: 0.61461 | val_1_rmse: 0.614   |  0:00:05s
epoch 20 | loss: 0.33739 | val_0_rmse: 0.61421 | val_1_rmse: 0.60814 |  0:00:05s
epoch 21 | loss: 0.35043 | val_0_rmse: 0.67798 | val_1_rmse: 0.66874 |  0:00:05s
epoch 22 | loss: 0.35799 | val_0_rmse: 0.64259 | val_1_rmse: 0.63861 |  0:00:05s
epoch 23 | loss: 0.34767 | val_0_rmse: 0.60082 | val_1_rmse: 0.60684 |  0:00:06s
epoch 24 | loss: 0.34534 | val_0_rmse: 0.63009 | val_1_rmse: 0.64485 |  0:00:06s
epoch 25 | loss: 0.3775  | val_0_rmse: 0.63683 | val_1_rmse: 0.63093 |  0:00:06s
epoch 26 | loss: 0.3655  | val_0_rmse: 0.61572 | val_1_rmse: 0.5859  |  0:00:06s
epoch 27 | loss: 0.36607 | val_0_rmse: 0.61216 | val_1_rmse: 0.57133 |  0:00:07s
epoch 28 | loss: 0.35919 | val_0_rmse: 0.63526 | val_1_rmse: 0.58976 |  0:00:07s
epoch 29 | loss: 0.35736 | val_0_rmse: 0.59541 | val_1_rmse: 0.56465 |  0:00:07s
epoch 30 | loss: 0.3366  | val_0_rmse: 0.57671 | val_1_rmse: 0.5461  |  0:00:07s
epoch 31 | loss: 0.33109 | val_0_rmse: 0.59171 | val_1_rmse: 0.57453 |  0:00:08s
epoch 32 | loss: 0.34011 | val_0_rmse: 0.55703 | val_1_rmse: 0.57469 |  0:00:08s
epoch 33 | loss: 0.31937 | val_0_rmse: 0.55861 | val_1_rmse: 0.58585 |  0:00:08s
epoch 34 | loss: 0.3114  | val_0_rmse: 0.55683 | val_1_rmse: 0.58699 |  0:00:08s
epoch 35 | loss: 0.32282 | val_0_rmse: 0.54826 | val_1_rmse: 0.5759  |  0:00:09s
epoch 36 | loss: 0.30933 | val_0_rmse: 0.54297 | val_1_rmse: 0.566   |  0:00:09s
epoch 37 | loss: 0.32348 | val_0_rmse: 0.53667 | val_1_rmse: 0.55105 |  0:00:09s
epoch 38 | loss: 0.31197 | val_0_rmse: 0.54865 | val_1_rmse: 0.55717 |  0:00:09s
epoch 39 | loss: 0.31092 | val_0_rmse: 0.56451 | val_1_rmse: 0.56635 |  0:00:10s
epoch 40 | loss: 0.31697 | val_0_rmse: 0.56832 | val_1_rmse: 0.55963 |  0:00:10s
epoch 41 | loss: 0.32088 | val_0_rmse: 0.55745 | val_1_rmse: 0.54206 |  0:00:10s
epoch 42 | loss: 0.31905 | val_0_rmse: 0.57147 | val_1_rmse: 0.55518 |  0:00:10s
epoch 43 | loss: 0.33101 | val_0_rmse: 0.62462 | val_1_rmse: 0.59054 |  0:00:11s
epoch 44 | loss: 0.3241  | val_0_rmse: 0.64051 | val_1_rmse: 0.61135 |  0:00:11s
epoch 45 | loss: 0.32463 | val_0_rmse: 0.59249 | val_1_rmse: 0.59183 |  0:00:11s
epoch 46 | loss: 0.32962 | val_0_rmse: 0.57601 | val_1_rmse: 0.59454 |  0:00:11s
epoch 47 | loss: 0.32304 | val_0_rmse: 0.58308 | val_1_rmse: 0.58992 |  0:00:12s
epoch 48 | loss: 0.32499 | val_0_rmse: 0.58096 | val_1_rmse: 0.59841 |  0:00:12s
epoch 49 | loss: 0.3276  | val_0_rmse: 0.55666 | val_1_rmse: 0.58418 |  0:00:12s
epoch 50 | loss: 0.31284 | val_0_rmse: 0.54814 | val_1_rmse: 0.56144 |  0:00:12s
epoch 51 | loss: 0.31552 | val_0_rmse: 0.55412 | val_1_rmse: 0.57389 |  0:00:13s
epoch 52 | loss: 0.31611 | val_0_rmse: 0.55758 | val_1_rmse: 0.56424 |  0:00:13s
epoch 53 | loss: 0.30538 | val_0_rmse: 0.54455 | val_1_rmse: 0.55964 |  0:00:13s
epoch 54 | loss: 0.29663 | val_0_rmse: 0.55589 | val_1_rmse: 0.58143 |  0:00:13s
epoch 55 | loss: 0.30184 | val_0_rmse: 0.52904 | val_1_rmse: 0.56978 |  0:00:14s
epoch 56 | loss: 0.31239 | val_0_rmse: 0.54672 | val_1_rmse: 0.58413 |  0:00:14s
epoch 57 | loss: 0.29544 | val_0_rmse: 0.58206 | val_1_rmse: 0.60982 |  0:00:14s
epoch 58 | loss: 0.29511 | val_0_rmse: 0.523   | val_1_rmse: 0.55429 |  0:00:14s
epoch 59 | loss: 0.29422 | val_0_rmse: 0.52567 | val_1_rmse: 0.56478 |  0:00:15s
epoch 60 | loss: 0.3029  | val_0_rmse: 0.54835 | val_1_rmse: 0.58651 |  0:00:15s
epoch 61 | loss: 0.29447 | val_0_rmse: 0.53306 | val_1_rmse: 0.55744 |  0:00:15s
epoch 62 | loss: 0.28623 | val_0_rmse: 0.53497 | val_1_rmse: 0.54782 |  0:00:15s
epoch 63 | loss: 0.29307 | val_0_rmse: 0.54881 | val_1_rmse: 0.54895 |  0:00:16s
epoch 64 | loss: 0.30756 | val_0_rmse: 0.53822 | val_1_rmse: 0.54213 |  0:00:16s
epoch 65 | loss: 0.28581 | val_0_rmse: 0.52575 | val_1_rmse: 0.53935 |  0:00:16s
epoch 66 | loss: 0.2956  | val_0_rmse: 0.51902 | val_1_rmse: 0.5406  |  0:00:16s
epoch 67 | loss: 0.29758 | val_0_rmse: 0.53697 | val_1_rmse: 0.55085 |  0:00:17s
epoch 68 | loss: 0.30063 | val_0_rmse: 0.54424 | val_1_rmse: 0.55014 |  0:00:17s
epoch 69 | loss: 0.29931 | val_0_rmse: 0.53173 | val_1_rmse: 0.5412  |  0:00:17s
epoch 70 | loss: 0.29988 | val_0_rmse: 0.52088 | val_1_rmse: 0.54899 |  0:00:17s
epoch 71 | loss: 0.28751 | val_0_rmse: 0.52956 | val_1_rmse: 0.5732  |  0:00:18s
epoch 72 | loss: 0.2899  | val_0_rmse: 0.52692 | val_1_rmse: 0.56579 |  0:00:18s
epoch 73 | loss: 0.27937 | val_0_rmse: 0.51854 | val_1_rmse: 0.55077 |  0:00:18s
epoch 74 | loss: 0.2819  | val_0_rmse: 0.51242 | val_1_rmse: 0.54216 |  0:00:18s
epoch 75 | loss: 0.27652 | val_0_rmse: 0.51848 | val_1_rmse: 0.5504  |  0:00:19s
epoch 76 | loss: 0.27472 | val_0_rmse: 0.5162  | val_1_rmse: 0.54876 |  0:00:19s
epoch 77 | loss: 0.28045 | val_0_rmse: 0.51188 | val_1_rmse: 0.52664 |  0:00:19s
epoch 78 | loss: 0.28269 | val_0_rmse: 0.52025 | val_1_rmse: 0.5246  |  0:00:19s
epoch 79 | loss: 0.2881  | val_0_rmse: 0.51846 | val_1_rmse: 0.53016 |  0:00:20s
epoch 80 | loss: 0.28193 | val_0_rmse: 0.51849 | val_1_rmse: 0.54112 |  0:00:20s
epoch 81 | loss: 0.281   | val_0_rmse: 0.53063 | val_1_rmse: 0.5574  |  0:00:20s
epoch 82 | loss: 0.30583 | val_0_rmse: 0.51939 | val_1_rmse: 0.54637 |  0:00:20s
epoch 83 | loss: 0.28233 | val_0_rmse: 0.51579 | val_1_rmse: 0.54332 |  0:00:21s
epoch 84 | loss: 0.30078 | val_0_rmse: 0.51332 | val_1_rmse: 0.552   |  0:00:21s
epoch 85 | loss: 0.28811 | val_0_rmse: 0.52528 | val_1_rmse: 0.56673 |  0:00:21s
epoch 86 | loss: 0.28231 | val_0_rmse: 0.52193 | val_1_rmse: 0.55364 |  0:00:21s
epoch 87 | loss: 0.28121 | val_0_rmse: 0.51361 | val_1_rmse: 0.52927 |  0:00:22s
epoch 88 | loss: 0.28559 | val_0_rmse: 0.51174 | val_1_rmse: 0.5231  |  0:00:22s
epoch 89 | loss: 0.28877 | val_0_rmse: 0.51071 | val_1_rmse: 0.52658 |  0:00:22s
epoch 90 | loss: 0.27544 | val_0_rmse: 0.50781 | val_1_rmse: 0.53723 |  0:00:22s
epoch 91 | loss: 0.26708 | val_0_rmse: 0.507   | val_1_rmse: 0.54258 |  0:00:23s
epoch 92 | loss: 0.28145 | val_0_rmse: 0.50555 | val_1_rmse: 0.53477 |  0:00:23s
epoch 93 | loss: 0.27488 | val_0_rmse: 0.50288 | val_1_rmse: 0.53014 |  0:00:23s
epoch 94 | loss: 0.27552 | val_0_rmse: 0.50527 | val_1_rmse: 0.52947 |  0:00:23s
epoch 95 | loss: 0.27536 | val_0_rmse: 0.49957 | val_1_rmse: 0.52798 |  0:00:24s
epoch 96 | loss: 0.26775 | val_0_rmse: 0.50441 | val_1_rmse: 0.53563 |  0:00:24s
epoch 97 | loss: 0.27655 | val_0_rmse: 0.51242 | val_1_rmse: 0.5385  |  0:00:24s
epoch 98 | loss: 0.26949 | val_0_rmse: 0.51069 | val_1_rmse: 0.53663 |  0:00:24s
epoch 99 | loss: 0.27597 | val_0_rmse: 0.50383 | val_1_rmse: 0.5342  |  0:00:25s
epoch 100| loss: 0.27525 | val_0_rmse: 0.50051 | val_1_rmse: 0.54408 |  0:00:25s
epoch 101| loss: 0.2673  | val_0_rmse: 0.50442 | val_1_rmse: 0.5458  |  0:00:25s
epoch 102| loss: 0.27288 | val_0_rmse: 0.50233 | val_1_rmse: 0.52458 |  0:00:25s
epoch 103| loss: 0.27112 | val_0_rmse: 0.50615 | val_1_rmse: 0.52521 |  0:00:26s
epoch 104| loss: 0.27127 | val_0_rmse: 0.50487 | val_1_rmse: 0.53211 |  0:00:26s
epoch 105| loss: 0.26327 | val_0_rmse: 0.49378 | val_1_rmse: 0.52258 |  0:00:26s
epoch 106| loss: 0.25985 | val_0_rmse: 0.49935 | val_1_rmse: 0.52845 |  0:00:26s
epoch 107| loss: 0.27447 | val_0_rmse: 0.49158 | val_1_rmse: 0.52885 |  0:00:27s
epoch 108| loss: 0.26955 | val_0_rmse: 0.49801 | val_1_rmse: 0.53966 |  0:00:27s
epoch 109| loss: 0.26687 | val_0_rmse: 0.50339 | val_1_rmse: 0.53454 |  0:00:27s
epoch 110| loss: 0.26706 | val_0_rmse: 0.50455 | val_1_rmse: 0.54042 |  0:00:27s
epoch 111| loss: 0.26186 | val_0_rmse: 0.50634 | val_1_rmse: 0.5473  |  0:00:28s
epoch 112| loss: 0.25608 | val_0_rmse: 0.4939  | val_1_rmse: 0.52571 |  0:00:28s
epoch 113| loss: 0.25383 | val_0_rmse: 0.49758 | val_1_rmse: 0.52814 |  0:00:28s
epoch 114| loss: 0.27321 | val_0_rmse: 0.49796 | val_1_rmse: 0.52869 |  0:00:28s
epoch 115| loss: 0.26166 | val_0_rmse: 0.49996 | val_1_rmse: 0.52767 |  0:00:29s
epoch 116| loss: 0.26022 | val_0_rmse: 0.49608 | val_1_rmse: 0.52821 |  0:00:29s
epoch 117| loss: 0.25857 | val_0_rmse: 0.48921 | val_1_rmse: 0.52985 |  0:00:29s
epoch 118| loss: 0.25266 | val_0_rmse: 0.48721 | val_1_rmse: 0.53443 |  0:00:29s
epoch 119| loss: 0.25638 | val_0_rmse: 0.48434 | val_1_rmse: 0.54011 |  0:00:30s
epoch 120| loss: 0.24729 | val_0_rmse: 0.48708 | val_1_rmse: 0.54251 |  0:00:30s
epoch 121| loss: 0.25923 | val_0_rmse: 0.49407 | val_1_rmse: 0.53585 |  0:00:30s
epoch 122| loss: 0.26806 | val_0_rmse: 0.51377 | val_1_rmse: 0.54963 |  0:00:30s
epoch 123| loss: 0.26695 | val_0_rmse: 0.51423 | val_1_rmse: 0.54793 |  0:00:31s
epoch 124| loss: 0.2555  | val_0_rmse: 0.49648 | val_1_rmse: 0.53686 |  0:00:31s
epoch 125| loss: 0.27028 | val_0_rmse: 0.4899  | val_1_rmse: 0.53723 |  0:00:31s
epoch 126| loss: 0.25715 | val_0_rmse: 0.48889 | val_1_rmse: 0.53124 |  0:00:31s
epoch 127| loss: 0.2647  | val_0_rmse: 0.49065 | val_1_rmse: 0.51769 |  0:00:32s
epoch 128| loss: 0.26067 | val_0_rmse: 0.49382 | val_1_rmse: 0.52941 |  0:00:32s
epoch 129| loss: 0.26539 | val_0_rmse: 0.49402 | val_1_rmse: 0.54923 |  0:00:32s
epoch 130| loss: 0.26127 | val_0_rmse: 0.49228 | val_1_rmse: 0.5445  |  0:00:32s
epoch 131| loss: 0.26261 | val_0_rmse: 0.49527 | val_1_rmse: 0.54657 |  0:00:33s
epoch 132| loss: 0.2527  | val_0_rmse: 0.50042 | val_1_rmse: 0.55842 |  0:00:33s
epoch 133| loss: 0.25919 | val_0_rmse: 0.50692 | val_1_rmse: 0.56457 |  0:00:33s
epoch 134| loss: 0.2622  | val_0_rmse: 0.4939  | val_1_rmse: 0.54718 |  0:00:33s
epoch 135| loss: 0.2587  | val_0_rmse: 0.49242 | val_1_rmse: 0.54255 |  0:00:34s
epoch 136| loss: 0.26282 | val_0_rmse: 0.49643 | val_1_rmse: 0.54079 |  0:00:34s
epoch 137| loss: 0.25719 | val_0_rmse: 0.49524 | val_1_rmse: 0.53937 |  0:00:34s
epoch 138| loss: 0.26366 | val_0_rmse: 0.50027 | val_1_rmse: 0.55046 |  0:00:34s
epoch 139| loss: 0.25831 | val_0_rmse: 0.49221 | val_1_rmse: 0.54312 |  0:00:35s
epoch 140| loss: 0.25535 | val_0_rmse: 0.50106 | val_1_rmse: 0.54413 |  0:00:35s
epoch 141| loss: 0.2633  | val_0_rmse: 0.49154 | val_1_rmse: 0.53876 |  0:00:35s
epoch 142| loss: 0.26294 | val_0_rmse: 0.48462 | val_1_rmse: 0.53048 |  0:00:35s
epoch 143| loss: 0.25848 | val_0_rmse: 0.48817 | val_1_rmse: 0.52087 |  0:00:36s
epoch 144| loss: 0.25624 | val_0_rmse: 0.48841 | val_1_rmse: 0.52392 |  0:00:36s
epoch 145| loss: 0.2625  | val_0_rmse: 0.48164 | val_1_rmse: 0.52734 |  0:00:36s
epoch 146| loss: 0.24908 | val_0_rmse: 0.48315 | val_1_rmse: 0.52542 |  0:00:36s
epoch 147| loss: 0.24923 | val_0_rmse: 0.4888  | val_1_rmse: 0.53487 |  0:00:37s
epoch 148| loss: 0.2529  | val_0_rmse: 0.48016 | val_1_rmse: 0.53764 |  0:00:37s
epoch 149| loss: 0.24833 | val_0_rmse: 0.47588 | val_1_rmse: 0.53742 |  0:00:37s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.51769
Best weights from best epoch are automatically used!
ended training at: 02:50:55
Feature importance:
[('Area', 0.18234692024689472), ('Baths', 0.08020571790028809), ('Beds', 0.0626162414130123), ('Latitude', 0.2884704154078299), ('Longitude', 0.3473416408146454), ('Month', 2.621615795284408e-06), ('Year', 0.03901644260153431)]
Mean squared error is of 24780250385.724064
Mean absolute error:115498.44921153846
MAPE:0.19312152909409055
R2 score:0.6996940718898756
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:50:56
epoch 0  | loss: 1.50953 | val_0_rmse: 1.02697 | val_1_rmse: 1.14805 |  0:00:00s
epoch 1  | loss: 0.90823 | val_0_rmse: 1.06112 | val_1_rmse: 1.15767 |  0:00:00s
epoch 2  | loss: 0.60813 | val_0_rmse: 0.88497 | val_1_rmse: 1.03595 |  0:00:00s
epoch 3  | loss: 0.49979 | val_0_rmse: 0.92042 | val_1_rmse: 1.06464 |  0:00:01s
epoch 4  | loss: 0.46237 | val_0_rmse: 0.82653 | val_1_rmse: 0.91501 |  0:00:01s
epoch 5  | loss: 0.43244 | val_0_rmse: 0.76754 | val_1_rmse: 0.85758 |  0:00:01s
epoch 6  | loss: 0.43085 | val_0_rmse: 0.74571 | val_1_rmse: 0.86254 |  0:00:01s
epoch 7  | loss: 0.42443 | val_0_rmse: 0.73402 | val_1_rmse: 0.86069 |  0:00:02s
epoch 8  | loss: 0.40656 | val_0_rmse: 0.66451 | val_1_rmse: 0.81257 |  0:00:02s
epoch 9  | loss: 0.41722 | val_0_rmse: 0.67337 | val_1_rmse: 0.80673 |  0:00:02s
epoch 10 | loss: 0.4117  | val_0_rmse: 0.6935  | val_1_rmse: 0.8298  |  0:00:02s
epoch 11 | loss: 0.40529 | val_0_rmse: 0.65334 | val_1_rmse: 0.79451 |  0:00:03s
epoch 12 | loss: 0.40442 | val_0_rmse: 0.64032 | val_1_rmse: 0.77981 |  0:00:03s
epoch 13 | loss: 0.39552 | val_0_rmse: 0.64795 | val_1_rmse: 0.7868  |  0:00:03s
epoch 14 | loss: 0.40303 | val_0_rmse: 0.64022 | val_1_rmse: 0.78715 |  0:00:03s
epoch 15 | loss: 0.39697 | val_0_rmse: 0.63024 | val_1_rmse: 0.7772  |  0:00:04s
epoch 16 | loss: 0.39284 | val_0_rmse: 0.62215 | val_1_rmse: 0.75607 |  0:00:04s
epoch 17 | loss: 0.38653 | val_0_rmse: 0.62118 | val_1_rmse: 0.76415 |  0:00:04s
epoch 18 | loss: 0.38813 | val_0_rmse: 0.62253 | val_1_rmse: 0.77378 |  0:00:04s
epoch 19 | loss: 0.38876 | val_0_rmse: 0.61874 | val_1_rmse: 0.7705  |  0:00:05s
epoch 20 | loss: 0.38385 | val_0_rmse: 0.61379 | val_1_rmse: 0.76172 |  0:00:05s
epoch 21 | loss: 0.38554 | val_0_rmse: 0.61503 | val_1_rmse: 0.75712 |  0:00:05s
epoch 22 | loss: 0.39287 | val_0_rmse: 0.61019 | val_1_rmse: 0.75889 |  0:00:05s
epoch 23 | loss: 0.38252 | val_0_rmse: 0.62159 | val_1_rmse: 0.77941 |  0:00:06s
epoch 24 | loss: 0.39537 | val_0_rmse: 0.61229 | val_1_rmse: 0.77412 |  0:00:06s
epoch 25 | loss: 0.38332 | val_0_rmse: 0.61052 | val_1_rmse: 0.76411 |  0:00:06s
epoch 26 | loss: 0.3865  | val_0_rmse: 0.60848 | val_1_rmse: 0.76043 |  0:00:06s
epoch 27 | loss: 0.39022 | val_0_rmse: 0.61326 | val_1_rmse: 0.76447 |  0:00:07s
epoch 28 | loss: 0.37869 | val_0_rmse: 0.61247 | val_1_rmse: 0.75881 |  0:00:07s
epoch 29 | loss: 0.38457 | val_0_rmse: 0.60673 | val_1_rmse: 0.7499  |  0:00:07s
epoch 30 | loss: 0.37507 | val_0_rmse: 0.60832 | val_1_rmse: 0.74483 |  0:00:07s
epoch 31 | loss: 0.37414 | val_0_rmse: 0.60524 | val_1_rmse: 0.74259 |  0:00:08s
epoch 32 | loss: 0.38818 | val_0_rmse: 0.60619 | val_1_rmse: 0.73712 |  0:00:08s
epoch 33 | loss: 0.38779 | val_0_rmse: 0.61025 | val_1_rmse: 0.7298  |  0:00:08s
epoch 34 | loss: 0.38313 | val_0_rmse: 0.61075 | val_1_rmse: 0.72613 |  0:00:08s
epoch 35 | loss: 0.37485 | val_0_rmse: 0.61399 | val_1_rmse: 0.73824 |  0:00:09s
epoch 36 | loss: 0.37226 | val_0_rmse: 0.60463 | val_1_rmse: 0.74444 |  0:00:09s
epoch 37 | loss: 0.38344 | val_0_rmse: 0.60862 | val_1_rmse: 0.71567 |  0:00:09s
epoch 38 | loss: 0.38066 | val_0_rmse: 0.61538 | val_1_rmse: 0.71669 |  0:00:10s
epoch 39 | loss: 0.37073 | val_0_rmse: 0.60878 | val_1_rmse: 0.71478 |  0:00:10s
epoch 40 | loss: 0.37721 | val_0_rmse: 0.60843 | val_1_rmse: 0.71017 |  0:00:10s
epoch 41 | loss: 0.3635  | val_0_rmse: 0.60528 | val_1_rmse: 0.70868 |  0:00:10s
epoch 42 | loss: 0.36654 | val_0_rmse: 0.60176 | val_1_rmse: 0.71468 |  0:00:11s
epoch 43 | loss: 0.37442 | val_0_rmse: 0.6113  | val_1_rmse: 0.72849 |  0:00:11s
epoch 44 | loss: 0.3842  | val_0_rmse: 0.6015  | val_1_rmse: 0.71256 |  0:00:11s
epoch 45 | loss: 0.36031 | val_0_rmse: 0.60106 | val_1_rmse: 0.694   |  0:00:11s
epoch 46 | loss: 0.36724 | val_0_rmse: 0.60402 | val_1_rmse: 0.68993 |  0:00:12s
epoch 47 | loss: 0.37587 | val_0_rmse: 0.60288 | val_1_rmse: 0.69643 |  0:00:12s
epoch 48 | loss: 0.37675 | val_0_rmse: 0.60602 | val_1_rmse: 0.71257 |  0:00:12s
epoch 49 | loss: 0.37052 | val_0_rmse: 0.6118  | val_1_rmse: 0.71792 |  0:00:12s
epoch 50 | loss: 0.38938 | val_0_rmse: 0.60512 | val_1_rmse: 0.7287  |  0:00:13s
epoch 51 | loss: 0.36807 | val_0_rmse: 0.6074  | val_1_rmse: 0.74397 |  0:00:13s
epoch 52 | loss: 0.36768 | val_0_rmse: 0.61256 | val_1_rmse: 0.75057 |  0:00:13s
epoch 53 | loss: 0.35096 | val_0_rmse: 0.65359 | val_1_rmse: 0.77038 |  0:00:13s
epoch 54 | loss: 0.3515  | val_0_rmse: 0.67923 | val_1_rmse: 0.78626 |  0:00:14s
epoch 55 | loss: 0.37419 | val_0_rmse: 0.68625 | val_1_rmse: 0.79968 |  0:00:14s
epoch 56 | loss: 0.36505 | val_0_rmse: 0.64379 | val_1_rmse: 0.74988 |  0:00:14s
epoch 57 | loss: 0.36711 | val_0_rmse: 0.62418 | val_1_rmse: 0.73635 |  0:00:14s
epoch 58 | loss: 0.36305 | val_0_rmse: 0.61382 | val_1_rmse: 0.72703 |  0:00:15s
epoch 59 | loss: 0.35515 | val_0_rmse: 0.62163 | val_1_rmse: 0.71727 |  0:00:15s
epoch 60 | loss: 0.37826 | val_0_rmse: 0.60933 | val_1_rmse: 0.7202  |  0:00:15s
epoch 61 | loss: 0.36856 | val_0_rmse: 0.67634 | val_1_rmse: 0.78741 |  0:00:15s
epoch 62 | loss: 0.33985 | val_0_rmse: 0.69022 | val_1_rmse: 0.79657 |  0:00:16s
epoch 63 | loss: 0.34267 | val_0_rmse: 0.67372 | val_1_rmse: 0.76395 |  0:00:16s
epoch 64 | loss: 0.3454  | val_0_rmse: 0.60132 | val_1_rmse: 0.6978  |  0:00:16s
epoch 65 | loss: 0.33338 | val_0_rmse: 0.58225 | val_1_rmse: 0.69528 |  0:00:16s
epoch 66 | loss: 0.33199 | val_0_rmse: 0.58245 | val_1_rmse: 0.68166 |  0:00:17s
epoch 67 | loss: 0.3244  | val_0_rmse: 0.63331 | val_1_rmse: 0.72815 |  0:00:17s
epoch 68 | loss: 0.32824 | val_0_rmse: 0.65977 | val_1_rmse: 0.7552  |  0:00:17s
epoch 69 | loss: 0.34085 | val_0_rmse: 0.65694 | val_1_rmse: 0.75691 |  0:00:17s
epoch 70 | loss: 0.34092 | val_0_rmse: 0.59131 | val_1_rmse: 0.70019 |  0:00:18s
epoch 71 | loss: 0.33157 | val_0_rmse: 0.61077 | val_1_rmse: 0.72193 |  0:00:18s
epoch 72 | loss: 0.33485 | val_0_rmse: 0.6195  | val_1_rmse: 0.73009 |  0:00:18s
epoch 73 | loss: 0.33402 | val_0_rmse: 0.61321 | val_1_rmse: 0.70834 |  0:00:18s
epoch 74 | loss: 0.3685  | val_0_rmse: 0.58889 | val_1_rmse: 0.66667 |  0:00:19s
epoch 75 | loss: 0.35964 | val_0_rmse: 0.69815 | val_1_rmse: 0.87779 |  0:00:19s
epoch 76 | loss: 0.44231 | val_0_rmse: 0.67135 | val_1_rmse: 0.74105 |  0:00:19s
epoch 77 | loss: 0.4302  | val_0_rmse: 0.65627 | val_1_rmse: 0.71628 |  0:00:19s
epoch 78 | loss: 0.42912 | val_0_rmse: 0.64707 | val_1_rmse: 0.72012 |  0:00:20s
epoch 79 | loss: 0.43624 | val_0_rmse: 0.6761  | val_1_rmse: 0.74341 |  0:00:20s
epoch 80 | loss: 0.41035 | val_0_rmse: 0.66615 | val_1_rmse: 0.7346  |  0:00:20s
epoch 81 | loss: 0.40997 | val_0_rmse: 0.62959 | val_1_rmse: 0.7061  |  0:00:20s
epoch 82 | loss: 0.40241 | val_0_rmse: 0.62399 | val_1_rmse: 0.70391 |  0:00:21s
epoch 83 | loss: 0.40131 | val_0_rmse: 0.63333 | val_1_rmse: 0.71406 |  0:00:21s
epoch 84 | loss: 0.38698 | val_0_rmse: 0.61901 | val_1_rmse: 0.6988  |  0:00:21s
epoch 85 | loss: 0.38782 | val_0_rmse: 0.62326 | val_1_rmse: 0.7122  |  0:00:21s
epoch 86 | loss: 0.38678 | val_0_rmse: 0.63066 | val_1_rmse: 0.72829 |  0:00:22s
epoch 87 | loss: 0.39476 | val_0_rmse: 0.6257  | val_1_rmse: 0.72584 |  0:00:22s
epoch 88 | loss: 0.38163 | val_0_rmse: 0.61611 | val_1_rmse: 0.72154 |  0:00:22s
epoch 89 | loss: 0.3784  | val_0_rmse: 0.63553 | val_1_rmse: 0.74161 |  0:00:22s
epoch 90 | loss: 0.3756  | val_0_rmse: 0.64882 | val_1_rmse: 0.75337 |  0:00:23s
epoch 91 | loss: 0.37251 | val_0_rmse: 0.64814 | val_1_rmse: 0.75087 |  0:00:23s
epoch 92 | loss: 0.35045 | val_0_rmse: 0.65252 | val_1_rmse: 0.75091 |  0:00:23s
epoch 93 | loss: 0.34096 | val_0_rmse: 0.62305 | val_1_rmse: 0.72862 |  0:00:23s
epoch 94 | loss: 0.333   | val_0_rmse: 0.59554 | val_1_rmse: 0.70686 |  0:00:24s
epoch 95 | loss: 0.36203 | val_0_rmse: 0.59179 | val_1_rmse: 0.69431 |  0:00:24s
epoch 96 | loss: 0.33517 | val_0_rmse: 0.61099 | val_1_rmse: 0.70314 |  0:00:24s
epoch 97 | loss: 0.34429 | val_0_rmse: 0.60921 | val_1_rmse: 0.70593 |  0:00:24s
epoch 98 | loss: 0.34263 | val_0_rmse: 0.58886 | val_1_rmse: 0.7029  |  0:00:25s
epoch 99 | loss: 0.33391 | val_0_rmse: 0.57427 | val_1_rmse: 0.68838 |  0:00:25s
epoch 100| loss: 0.33298 | val_0_rmse: 0.57026 | val_1_rmse: 0.67806 |  0:00:25s
epoch 101| loss: 0.32338 | val_0_rmse: 0.57753 | val_1_rmse: 0.66681 |  0:00:25s
epoch 102| loss: 0.32122 | val_0_rmse: 0.59047 | val_1_rmse: 0.6781  |  0:00:26s
epoch 103| loss: 0.32825 | val_0_rmse: 0.59401 | val_1_rmse: 0.68619 |  0:00:26s
epoch 104| loss: 0.32253 | val_0_rmse: 0.58567 | val_1_rmse: 0.67896 |  0:00:26s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.66667
Best weights from best epoch are automatically used!
ended training at: 02:51:22
Feature importance:
[('Area', 0.3900763388342093), ('Baths', 0.17890306491081764), ('Beds', 0.0029046335770267586), ('Latitude', 0.2723981092717456), ('Longitude', 0.06396234789125067), ('Month', 0.038328805627514104), ('Year', 0.053426699887435876)]
Mean squared error is of 3475981515.5417275
Mean absolute error:41960.21421085166
MAPE:0.38473028115740504
R2 score:0.5718454254795933
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:51:23
epoch 0  | loss: 1.50877 | val_0_rmse: 1.04689 | val_1_rmse: 1.22208 |  0:00:00s
epoch 1  | loss: 0.93704 | val_0_rmse: 1.11935 | val_1_rmse: 1.16161 |  0:00:00s
epoch 2  | loss: 0.60359 | val_0_rmse: 1.06665 | val_1_rmse: 1.11866 |  0:00:00s
epoch 3  | loss: 0.56963 | val_0_rmse: 0.88042 | val_1_rmse: 0.89756 |  0:00:01s
epoch 4  | loss: 0.50851 | val_0_rmse: 0.79045 | val_1_rmse: 0.82811 |  0:00:01s
epoch 5  | loss: 0.50015 | val_0_rmse: 0.75819 | val_1_rmse: 0.80929 |  0:00:01s
epoch 6  | loss: 0.45407 | val_0_rmse: 0.7584  | val_1_rmse: 0.81333 |  0:00:01s
epoch 7  | loss: 0.45377 | val_0_rmse: 0.71637 | val_1_rmse: 0.75309 |  0:00:02s
epoch 8  | loss: 0.42773 | val_0_rmse: 0.69835 | val_1_rmse: 0.7233  |  0:00:02s
epoch 9  | loss: 0.43863 | val_0_rmse: 0.69151 | val_1_rmse: 0.71836 |  0:00:02s
epoch 10 | loss: 0.4371  | val_0_rmse: 0.69271 | val_1_rmse: 0.73113 |  0:00:02s
epoch 11 | loss: 0.42963 | val_0_rmse: 0.7126  | val_1_rmse: 0.74205 |  0:00:03s
epoch 12 | loss: 0.43806 | val_0_rmse: 0.69594 | val_1_rmse: 0.75625 |  0:00:03s
epoch 13 | loss: 0.43303 | val_0_rmse: 0.68103 | val_1_rmse: 0.76779 |  0:00:03s
epoch 14 | loss: 0.42304 | val_0_rmse: 0.67534 | val_1_rmse: 0.74317 |  0:00:03s
epoch 15 | loss: 0.41814 | val_0_rmse: 0.66441 | val_1_rmse: 0.71129 |  0:00:04s
epoch 16 | loss: 0.4084  | val_0_rmse: 0.65901 | val_1_rmse: 0.72466 |  0:00:04s
epoch 17 | loss: 0.40778 | val_0_rmse: 0.64563 | val_1_rmse: 0.71157 |  0:00:04s
epoch 18 | loss: 0.39612 | val_0_rmse: 0.63378 | val_1_rmse: 0.6943  |  0:00:04s
epoch 19 | loss: 0.40148 | val_0_rmse: 0.61597 | val_1_rmse: 0.69224 |  0:00:05s
epoch 20 | loss: 0.38566 | val_0_rmse: 0.6134  | val_1_rmse: 0.72128 |  0:00:05s
epoch 21 | loss: 0.39598 | val_0_rmse: 0.61023 | val_1_rmse: 0.70772 |  0:00:05s
epoch 22 | loss: 0.38875 | val_0_rmse: 0.61165 | val_1_rmse: 0.72055 |  0:00:05s
epoch 23 | loss: 0.39537 | val_0_rmse: 0.61643 | val_1_rmse: 0.72716 |  0:00:06s
epoch 24 | loss: 0.38981 | val_0_rmse: 0.61199 | val_1_rmse: 0.71579 |  0:00:06s
epoch 25 | loss: 0.38831 | val_0_rmse: 0.60924 | val_1_rmse: 0.71115 |  0:00:06s
epoch 26 | loss: 0.38002 | val_0_rmse: 0.61208 | val_1_rmse: 0.71304 |  0:00:06s
epoch 27 | loss: 0.39105 | val_0_rmse: 0.60448 | val_1_rmse: 0.70357 |  0:00:07s
epoch 28 | loss: 0.38898 | val_0_rmse: 0.61208 | val_1_rmse: 0.71022 |  0:00:07s
epoch 29 | loss: 0.37762 | val_0_rmse: 0.61    | val_1_rmse: 0.71794 |  0:00:07s
epoch 30 | loss: 0.39301 | val_0_rmse: 0.60621 | val_1_rmse: 0.70659 |  0:00:07s
epoch 31 | loss: 0.39765 | val_0_rmse: 0.61629 | val_1_rmse: 0.71168 |  0:00:07s
epoch 32 | loss: 0.3734  | val_0_rmse: 0.62357 | val_1_rmse: 0.72898 |  0:00:08s
epoch 33 | loss: 0.37635 | val_0_rmse: 0.6127  | val_1_rmse: 0.71196 |  0:00:08s
epoch 34 | loss: 0.3742  | val_0_rmse: 0.60646 | val_1_rmse: 0.69725 |  0:00:08s
epoch 35 | loss: 0.3799  | val_0_rmse: 0.6131  | val_1_rmse: 0.72807 |  0:00:08s
epoch 36 | loss: 0.37515 | val_0_rmse: 0.6347  | val_1_rmse: 0.75738 |  0:00:09s
epoch 37 | loss: 0.37709 | val_0_rmse: 0.63105 | val_1_rmse: 0.75129 |  0:00:09s
epoch 38 | loss: 0.37468 | val_0_rmse: 0.61786 | val_1_rmse: 0.71962 |  0:00:09s
epoch 39 | loss: 0.37063 | val_0_rmse: 0.61619 | val_1_rmse: 0.71735 |  0:00:09s
epoch 40 | loss: 0.36879 | val_0_rmse: 0.62465 | val_1_rmse: 0.74525 |  0:00:10s
epoch 41 | loss: 0.37542 | val_0_rmse: 0.61871 | val_1_rmse: 0.74212 |  0:00:10s
epoch 42 | loss: 0.38059 | val_0_rmse: 0.61916 | val_1_rmse: 0.70189 |  0:00:10s
epoch 43 | loss: 0.34817 | val_0_rmse: 0.62367 | val_1_rmse: 0.71066 |  0:00:10s
epoch 44 | loss: 0.37025 | val_0_rmse: 0.61692 | val_1_rmse: 0.73628 |  0:00:11s
epoch 45 | loss: 0.37543 | val_0_rmse: 0.61244 | val_1_rmse: 0.7089  |  0:00:11s
epoch 46 | loss: 0.3662  | val_0_rmse: 0.60859 | val_1_rmse: 0.68013 |  0:00:11s
epoch 47 | loss: 0.37571 | val_0_rmse: 0.60672 | val_1_rmse: 0.6833  |  0:00:11s
epoch 48 | loss: 0.37145 | val_0_rmse: 0.61979 | val_1_rmse: 0.7048  |  0:00:12s
epoch 49 | loss: 0.37718 | val_0_rmse: 0.5975  | val_1_rmse: 0.67724 |  0:00:12s
epoch 50 | loss: 0.3606  | val_0_rmse: 0.57979 | val_1_rmse: 0.66633 |  0:00:12s
epoch 51 | loss: 0.34459 | val_0_rmse: 0.59071 | val_1_rmse: 0.69278 |  0:00:12s
epoch 52 | loss: 0.34429 | val_0_rmse: 0.59078 | val_1_rmse: 0.70559 |  0:00:13s
epoch 53 | loss: 0.35349 | val_0_rmse: 0.5866  | val_1_rmse: 0.69822 |  0:00:13s
epoch 54 | loss: 0.34839 | val_0_rmse: 0.61202 | val_1_rmse: 0.72913 |  0:00:13s
epoch 55 | loss: 0.35393 | val_0_rmse: 0.68069 | val_1_rmse: 0.81664 |  0:00:13s
epoch 56 | loss: 0.35254 | val_0_rmse: 0.59898 | val_1_rmse: 0.73645 |  0:00:14s
epoch 57 | loss: 0.33761 | val_0_rmse: 0.58037 | val_1_rmse: 0.70712 |  0:00:14s
epoch 58 | loss: 0.33706 | val_0_rmse: 0.59334 | val_1_rmse: 0.72991 |  0:00:14s
epoch 59 | loss: 0.33648 | val_0_rmse: 0.60302 | val_1_rmse: 0.74541 |  0:00:14s
epoch 60 | loss: 0.33185 | val_0_rmse: 0.58616 | val_1_rmse: 0.7177  |  0:00:15s
epoch 61 | loss: 0.3355  | val_0_rmse: 0.57032 | val_1_rmse: 0.69128 |  0:00:15s
epoch 62 | loss: 0.3412  | val_0_rmse: 0.57459 | val_1_rmse: 0.6955  |  0:00:15s
epoch 63 | loss: 0.33867 | val_0_rmse: 0.59652 | val_1_rmse: 0.72401 |  0:00:15s
epoch 64 | loss: 0.32095 | val_0_rmse: 0.59345 | val_1_rmse: 0.74599 |  0:00:16s
epoch 65 | loss: 0.3463  | val_0_rmse: 0.58691 | val_1_rmse: 0.71059 |  0:00:16s
epoch 66 | loss: 0.32669 | val_0_rmse: 0.58149 | val_1_rmse: 0.69845 |  0:00:16s
epoch 67 | loss: 0.33369 | val_0_rmse: 0.56097 | val_1_rmse: 0.69374 |  0:00:16s
epoch 68 | loss: 0.33733 | val_0_rmse: 0.56837 | val_1_rmse: 0.69013 |  0:00:17s
epoch 69 | loss: 0.33266 | val_0_rmse: 0.56412 | val_1_rmse: 0.67202 |  0:00:17s
epoch 70 | loss: 0.33603 | val_0_rmse: 0.56494 | val_1_rmse: 0.66977 |  0:00:17s
epoch 71 | loss: 0.33006 | val_0_rmse: 0.61014 | val_1_rmse: 0.73174 |  0:00:17s
epoch 72 | loss: 0.36453 | val_0_rmse: 0.59851 | val_1_rmse: 0.70711 |  0:00:18s
epoch 73 | loss: 0.35082 | val_0_rmse: 0.59029 | val_1_rmse: 0.7034  |  0:00:18s
epoch 74 | loss: 0.34347 | val_0_rmse: 0.57082 | val_1_rmse: 0.69817 |  0:00:18s
epoch 75 | loss: 0.323   | val_0_rmse: 0.56887 | val_1_rmse: 0.69341 |  0:00:18s
epoch 76 | loss: 0.33063 | val_0_rmse: 0.57262 | val_1_rmse: 0.71008 |  0:00:19s
epoch 77 | loss: 0.33218 | val_0_rmse: 0.57772 | val_1_rmse: 0.72886 |  0:00:19s
epoch 78 | loss: 0.32317 | val_0_rmse: 0.57846 | val_1_rmse: 0.74328 |  0:00:19s
epoch 79 | loss: 0.33074 | val_0_rmse: 0.56813 | val_1_rmse: 0.7142  |  0:00:19s
epoch 80 | loss: 0.32271 | val_0_rmse: 0.59378 | val_1_rmse: 0.70298 |  0:00:20s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.66633
Best weights from best epoch are automatically used!
ended training at: 02:51:43
Feature importance:
[('Area', 0.32257005962648766), ('Baths', 0.1913171973344303), ('Beds', 0.012556384360849182), ('Latitude', 0.19464491921520138), ('Longitude', 0.17434704801928766), ('Month', 0.07745230293861907), ('Year', 0.02711208850512471)]
Mean squared error is of 3033043374.1979737
Mean absolute error:40122.34206567222
MAPE:0.37217409161694204
R2 score:0.5981668837775472
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:51:43
epoch 0  | loss: 1.65403 | val_0_rmse: 1.25434 | val_1_rmse: 1.15618 |  0:00:00s
epoch 1  | loss: 0.77526 | val_0_rmse: 1.09678 | val_1_rmse: 0.99753 |  0:00:00s
epoch 2  | loss: 0.62186 | val_0_rmse: 0.94234 | val_1_rmse: 0.85033 |  0:00:00s
epoch 3  | loss: 0.53476 | val_0_rmse: 0.88645 | val_1_rmse: 0.75816 |  0:00:01s
epoch 4  | loss: 0.51312 | val_0_rmse: 0.81015 | val_1_rmse: 0.68852 |  0:00:01s
epoch 5  | loss: 0.51831 | val_0_rmse: 0.84308 | val_1_rmse: 0.7267  |  0:00:01s
epoch 6  | loss: 0.5187  | val_0_rmse: 0.81387 | val_1_rmse: 0.72117 |  0:00:01s
epoch 7  | loss: 0.51525 | val_0_rmse: 0.7222  | val_1_rmse: 0.65271 |  0:00:01s
epoch 8  | loss: 0.52557 | val_0_rmse: 0.71107 | val_1_rmse: 0.65008 |  0:00:02s
epoch 9  | loss: 0.49912 | val_0_rmse: 0.7343  | val_1_rmse: 0.66737 |  0:00:02s
epoch 10 | loss: 0.50355 | val_0_rmse: 0.70909 | val_1_rmse: 0.64861 |  0:00:02s
epoch 11 | loss: 0.48528 | val_0_rmse: 0.71021 | val_1_rmse: 0.65564 |  0:00:03s
epoch 12 | loss: 0.48494 | val_0_rmse: 0.71068 | val_1_rmse: 0.64383 |  0:00:03s
epoch 13 | loss: 0.48222 | val_0_rmse: 0.70286 | val_1_rmse: 0.62836 |  0:00:03s
epoch 14 | loss: 0.46643 | val_0_rmse: 0.6975  | val_1_rmse: 0.62526 |  0:00:03s
epoch 15 | loss: 0.47741 | val_0_rmse: 0.70415 | val_1_rmse: 0.64405 |  0:00:04s
epoch 16 | loss: 0.48738 | val_0_rmse: 0.70394 | val_1_rmse: 0.65944 |  0:00:04s
epoch 17 | loss: 0.48403 | val_0_rmse: 0.6958  | val_1_rmse: 0.64546 |  0:00:04s
epoch 18 | loss: 0.47276 | val_0_rmse: 0.68273 | val_1_rmse: 0.6276  |  0:00:04s
epoch 19 | loss: 0.48144 | val_0_rmse: 0.68593 | val_1_rmse: 0.63106 |  0:00:05s
epoch 20 | loss: 0.46174 | val_0_rmse: 0.69195 | val_1_rmse: 0.64271 |  0:00:05s
epoch 21 | loss: 0.4593  | val_0_rmse: 0.67842 | val_1_rmse: 0.6354  |  0:00:05s
epoch 22 | loss: 0.46052 | val_0_rmse: 0.66471 | val_1_rmse: 0.6275  |  0:00:05s
epoch 23 | loss: 0.44808 | val_0_rmse: 0.66097 | val_1_rmse: 0.62908 |  0:00:06s
epoch 24 | loss: 0.43907 | val_0_rmse: 0.66491 | val_1_rmse: 0.63218 |  0:00:06s
epoch 25 | loss: 0.44973 | val_0_rmse: 0.66063 | val_1_rmse: 0.62771 |  0:00:06s
epoch 26 | loss: 0.44561 | val_0_rmse: 0.66126 | val_1_rmse: 0.61767 |  0:00:06s
epoch 27 | loss: 0.43799 | val_0_rmse: 0.65761 | val_1_rmse: 0.6208  |  0:00:06s
epoch 28 | loss: 0.43756 | val_0_rmse: 0.65467 | val_1_rmse: 0.65004 |  0:00:07s
epoch 29 | loss: 0.43052 | val_0_rmse: 0.65454 | val_1_rmse: 0.63132 |  0:00:07s
epoch 30 | loss: 0.4393  | val_0_rmse: 0.64711 | val_1_rmse: 0.61546 |  0:00:07s
epoch 31 | loss: 0.43174 | val_0_rmse: 0.64722 | val_1_rmse: 0.62378 |  0:00:08s
epoch 32 | loss: 0.41942 | val_0_rmse: 0.6441  | val_1_rmse: 0.6277  |  0:00:08s
epoch 33 | loss: 0.42249 | val_0_rmse: 0.64883 | val_1_rmse: 0.60124 |  0:00:08s
epoch 34 | loss: 0.4145  | val_0_rmse: 0.64904 | val_1_rmse: 0.6009  |  0:00:08s
epoch 35 | loss: 0.40296 | val_0_rmse: 0.67711 | val_1_rmse: 0.62346 |  0:00:09s
epoch 36 | loss: 0.40171 | val_0_rmse: 0.9233  | val_1_rmse: 0.86966 |  0:00:09s
epoch 37 | loss: 0.40788 | val_0_rmse: 0.83343 | val_1_rmse: 0.77772 |  0:00:09s
epoch 38 | loss: 0.40733 | val_0_rmse: 0.6848  | val_1_rmse: 0.64272 |  0:00:09s
epoch 39 | loss: 0.40564 | val_0_rmse: 0.74019 | val_1_rmse: 0.7093  |  0:00:10s
epoch 40 | loss: 0.38867 | val_0_rmse: 0.86187 | val_1_rmse: 0.83075 |  0:00:10s
epoch 41 | loss: 0.39405 | val_0_rmse: 0.70485 | val_1_rmse: 0.66284 |  0:00:10s
epoch 42 | loss: 0.38661 | val_0_rmse: 0.66071 | val_1_rmse: 0.60371 |  0:00:10s
epoch 43 | loss: 0.40061 | val_0_rmse: 0.80108 | val_1_rmse: 0.74602 |  0:00:11s
epoch 44 | loss: 0.38083 | val_0_rmse: 0.95458 | val_1_rmse: 0.92554 |  0:00:11s
epoch 45 | loss: 0.40031 | val_0_rmse: 0.8187  | val_1_rmse: 0.78929 |  0:00:11s
epoch 46 | loss: 0.37935 | val_0_rmse: 0.66307 | val_1_rmse: 0.64234 |  0:00:11s
epoch 47 | loss: 0.39724 | val_0_rmse: 0.70721 | val_1_rmse: 0.67231 |  0:00:12s
epoch 48 | loss: 0.38367 | val_0_rmse: 0.71272 | val_1_rmse: 0.67553 |  0:00:12s
epoch 49 | loss: 0.37958 | val_0_rmse: 0.68946 | val_1_rmse: 0.66655 |  0:00:12s
epoch 50 | loss: 0.37647 | val_0_rmse: 0.62524 | val_1_rmse: 0.61212 |  0:00:12s
epoch 51 | loss: 0.35567 | val_0_rmse: 0.60986 | val_1_rmse: 0.59155 |  0:00:13s
epoch 52 | loss: 0.3771  | val_0_rmse: 0.60526 | val_1_rmse: 0.59773 |  0:00:13s
epoch 53 | loss: 0.38276 | val_0_rmse: 0.59177 | val_1_rmse: 0.56531 |  0:00:13s
epoch 54 | loss: 0.36651 | val_0_rmse: 0.58992 | val_1_rmse: 0.55725 |  0:00:13s
epoch 55 | loss: 0.36705 | val_0_rmse: 0.59653 | val_1_rmse: 0.55471 |  0:00:14s
epoch 56 | loss: 0.36475 | val_0_rmse: 0.60863 | val_1_rmse: 0.56543 |  0:00:14s
epoch 57 | loss: 0.36011 | val_0_rmse: 0.69175 | val_1_rmse: 0.6368  |  0:00:14s
epoch 58 | loss: 0.36355 | val_0_rmse: 0.72327 | val_1_rmse: 0.66109 |  0:00:14s
epoch 59 | loss: 0.35409 | val_0_rmse: 0.63153 | val_1_rmse: 0.5921  |  0:00:15s
epoch 60 | loss: 0.36329 | val_0_rmse: 0.60189 | val_1_rmse: 0.57761 |  0:00:15s
epoch 61 | loss: 0.35116 | val_0_rmse: 0.58741 | val_1_rmse: 0.55066 |  0:00:15s
epoch 62 | loss: 0.34761 | val_0_rmse: 0.58201 | val_1_rmse: 0.55471 |  0:00:15s
epoch 63 | loss: 0.34617 | val_0_rmse: 0.58327 | val_1_rmse: 0.58236 |  0:00:16s
epoch 64 | loss: 0.35235 | val_0_rmse: 0.63222 | val_1_rmse: 0.62525 |  0:00:16s
epoch 65 | loss: 0.34319 | val_0_rmse: 0.64728 | val_1_rmse: 0.62081 |  0:00:16s
epoch 66 | loss: 0.33416 | val_0_rmse: 0.64702 | val_1_rmse: 0.60481 |  0:00:16s
epoch 67 | loss: 0.34304 | val_0_rmse: 0.64385 | val_1_rmse: 0.60778 |  0:00:17s
epoch 68 | loss: 0.33601 | val_0_rmse: 0.66165 | val_1_rmse: 0.62828 |  0:00:17s
epoch 69 | loss: 0.34514 | val_0_rmse: 0.69666 | val_1_rmse: 0.65146 |  0:00:17s
epoch 70 | loss: 0.33782 | val_0_rmse: 0.6994  | val_1_rmse: 0.65606 |  0:00:17s
epoch 71 | loss: 0.34218 | val_0_rmse: 0.70135 | val_1_rmse: 0.65389 |  0:00:18s
epoch 72 | loss: 0.34537 | val_0_rmse: 0.74041 | val_1_rmse: 0.68359 |  0:00:18s
epoch 73 | loss: 0.33636 | val_0_rmse: 0.72919 | val_1_rmse: 0.67142 |  0:00:18s
epoch 74 | loss: 0.33288 | val_0_rmse: 0.71527 | val_1_rmse: 0.66115 |  0:00:18s
epoch 75 | loss: 0.32849 | val_0_rmse: 0.70252 | val_1_rmse: 0.647   |  0:00:19s
epoch 76 | loss: 0.32605 | val_0_rmse: 0.66252 | val_1_rmse: 0.61491 |  0:00:19s
epoch 77 | loss: 0.32543 | val_0_rmse: 0.55934 | val_1_rmse: 0.54763 |  0:00:19s
epoch 78 | loss: 0.32188 | val_0_rmse: 0.68994 | val_1_rmse: 0.636   |  0:00:19s
epoch 79 | loss: 0.31666 | val_0_rmse: 0.72822 | val_1_rmse: 0.67297 |  0:00:20s
epoch 80 | loss: 0.31885 | val_0_rmse: 0.75682 | val_1_rmse: 0.70379 |  0:00:20s
epoch 81 | loss: 0.32429 | val_0_rmse: 0.73132 | val_1_rmse: 0.6921  |  0:00:20s
epoch 82 | loss: 0.32973 | val_0_rmse: 0.72442 | val_1_rmse: 0.68622 |  0:00:20s
epoch 83 | loss: 0.32555 | val_0_rmse: 0.71325 | val_1_rmse: 0.67199 |  0:00:21s
epoch 84 | loss: 0.32267 | val_0_rmse: 0.71648 | val_1_rmse: 0.67023 |  0:00:21s
epoch 85 | loss: 0.31473 | val_0_rmse: 0.66759 | val_1_rmse: 0.63754 |  0:00:21s
epoch 86 | loss: 0.3207  | val_0_rmse: 0.66796 | val_1_rmse: 0.63327 |  0:00:21s
epoch 87 | loss: 0.31949 | val_0_rmse: 0.62358 | val_1_rmse: 0.59117 |  0:00:22s
epoch 88 | loss: 0.33322 | val_0_rmse: 0.59034 | val_1_rmse: 0.5924  |  0:00:22s
epoch 89 | loss: 0.32588 | val_0_rmse: 0.59015 | val_1_rmse: 0.59003 |  0:00:22s
epoch 90 | loss: 0.32511 | val_0_rmse: 0.55004 | val_1_rmse: 0.57665 |  0:00:22s
epoch 91 | loss: 0.31699 | val_0_rmse: 0.5481  | val_1_rmse: 0.57403 |  0:00:22s
epoch 92 | loss: 0.31647 | val_0_rmse: 0.54344 | val_1_rmse: 0.5708  |  0:00:23s
epoch 93 | loss: 0.32086 | val_0_rmse: 0.54625 | val_1_rmse: 0.56897 |  0:00:23s
epoch 94 | loss: 0.31233 | val_0_rmse: 0.57417 | val_1_rmse: 0.58263 |  0:00:23s
epoch 95 | loss: 0.29948 | val_0_rmse: 0.56938 | val_1_rmse: 0.56542 |  0:00:24s
epoch 96 | loss: 0.31683 | val_0_rmse: 0.59317 | val_1_rmse: 0.57885 |  0:00:24s
epoch 97 | loss: 0.32911 | val_0_rmse: 0.63677 | val_1_rmse: 0.61241 |  0:00:24s
epoch 98 | loss: 0.30728 | val_0_rmse: 0.66535 | val_1_rmse: 0.63547 |  0:00:24s
epoch 99 | loss: 0.30412 | val_0_rmse: 0.65544 | val_1_rmse: 0.63907 |  0:00:25s
epoch 100| loss: 0.31277 | val_0_rmse: 0.6762  | val_1_rmse: 0.6507  |  0:00:25s
epoch 101| loss: 0.30577 | val_0_rmse: 0.69589 | val_1_rmse: 0.6631  |  0:00:25s
epoch 102| loss: 0.31124 | val_0_rmse: 0.61349 | val_1_rmse: 0.60177 |  0:00:25s
epoch 103| loss: 0.31053 | val_0_rmse: 0.57079 | val_1_rmse: 0.56609 |  0:00:26s
epoch 104| loss: 0.30105 | val_0_rmse: 0.59103 | val_1_rmse: 0.57976 |  0:00:26s
epoch 105| loss: 0.32313 | val_0_rmse: 0.64802 | val_1_rmse: 0.62941 |  0:00:26s
epoch 106| loss: 0.30321 | val_0_rmse: 0.63715 | val_1_rmse: 0.61931 |  0:00:26s
epoch 107| loss: 0.30466 | val_0_rmse: 0.6179  | val_1_rmse: 0.61184 |  0:00:27s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.54763
Best weights from best epoch are automatically used!
ended training at: 02:52:10
Feature importance:
[('Area', 0.27577512708977925), ('Baths', 0.19980421493364375), ('Beds', 0.016538719436934565), ('Latitude', 0.43686763678645396), ('Longitude', 0.027448458766134304), ('Month', 0.021319690385211008), ('Year', 0.02224615260184315)]
Mean squared error is of 2378316979.109378
Mean absolute error:35843.730977403844
MAPE:0.3341164193805323
R2 score:0.6959200932911487
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:52:10
epoch 0  | loss: 1.57598 | val_0_rmse: 1.16393 | val_1_rmse: 1.19989 |  0:00:00s
epoch 1  | loss: 0.88861 | val_0_rmse: 1.04721 | val_1_rmse: 0.98616 |  0:00:00s
epoch 2  | loss: 0.64166 | val_0_rmse: 1.8915  | val_1_rmse: 1.00501 |  0:00:00s
epoch 3  | loss: 0.58282 | val_0_rmse: 0.80154 | val_1_rmse: 0.80275 |  0:00:01s
epoch 4  | loss: 0.54477 | val_0_rmse: 0.80415 | val_1_rmse: 0.81409 |  0:00:01s
epoch 5  | loss: 0.50761 | val_0_rmse: 0.84722 | val_1_rmse: 0.7937  |  0:00:01s
epoch 6  | loss: 0.48686 | val_0_rmse: 0.75936 | val_1_rmse: 0.73702 |  0:00:01s
epoch 7  | loss: 0.48867 | val_0_rmse: 0.6971  | val_1_rmse: 0.68005 |  0:00:02s
epoch 8  | loss: 0.4686  | val_0_rmse: 0.69305 | val_1_rmse: 0.67972 |  0:00:02s
epoch 9  | loss: 0.45258 | val_0_rmse: 0.69888 | val_1_rmse: 0.68098 |  0:00:02s
epoch 10 | loss: 0.44455 | val_0_rmse: 0.69941 | val_1_rmse: 0.67572 |  0:00:02s
epoch 11 | loss: 0.46641 | val_0_rmse: 0.68624 | val_1_rmse: 0.6578  |  0:00:03s
epoch 12 | loss: 0.44485 | val_0_rmse: 0.67843 | val_1_rmse: 0.65546 |  0:00:03s
epoch 13 | loss: 0.44943 | val_0_rmse: 0.65869 | val_1_rmse: 0.65324 |  0:00:03s
epoch 14 | loss: 0.44835 | val_0_rmse: 0.66724 | val_1_rmse: 0.66026 |  0:00:03s
epoch 15 | loss: 0.44973 | val_0_rmse: 0.68942 | val_1_rmse: 0.67819 |  0:00:04s
epoch 16 | loss: 0.44849 | val_0_rmse: 0.68305 | val_1_rmse: 0.67006 |  0:00:04s
epoch 17 | loss: 0.44893 | val_0_rmse: 0.67033 | val_1_rmse: 0.65741 |  0:00:04s
epoch 18 | loss: 0.45518 | val_0_rmse: 0.66585 | val_1_rmse: 0.65797 |  0:00:04s
epoch 19 | loss: 0.44654 | val_0_rmse: 0.66419 | val_1_rmse: 0.66894 |  0:00:05s
epoch 20 | loss: 0.44811 | val_0_rmse: 0.6626  | val_1_rmse: 0.67003 |  0:00:05s
epoch 21 | loss: 0.4394  | val_0_rmse: 0.65853 | val_1_rmse: 0.66452 |  0:00:05s
epoch 22 | loss: 0.43909 | val_0_rmse: 0.65832 | val_1_rmse: 0.65758 |  0:00:05s
epoch 23 | loss: 0.43065 | val_0_rmse: 0.65572 | val_1_rmse: 0.65302 |  0:00:06s
epoch 24 | loss: 0.4224  | val_0_rmse: 0.65696 | val_1_rmse: 0.65629 |  0:00:06s
epoch 25 | loss: 0.428   | val_0_rmse: 0.65385 | val_1_rmse: 0.6586  |  0:00:06s
epoch 26 | loss: 0.41691 | val_0_rmse: 0.64686 | val_1_rmse: 0.6525  |  0:00:06s
epoch 27 | loss: 0.41976 | val_0_rmse: 0.64851 | val_1_rmse: 0.64391 |  0:00:07s
epoch 28 | loss: 0.41717 | val_0_rmse: 0.65066 | val_1_rmse: 0.63579 |  0:00:07s
epoch 29 | loss: 0.42767 | val_0_rmse: 0.6479  | val_1_rmse: 0.6417  |  0:00:07s
epoch 30 | loss: 0.42144 | val_0_rmse: 0.63985 | val_1_rmse: 0.64163 |  0:00:07s
epoch 31 | loss: 0.40915 | val_0_rmse: 0.64342 | val_1_rmse: 0.64584 |  0:00:08s
epoch 32 | loss: 0.4216  | val_0_rmse: 0.64915 | val_1_rmse: 0.64384 |  0:00:08s
epoch 33 | loss: 0.41877 | val_0_rmse: 0.64401 | val_1_rmse: 0.63994 |  0:00:08s
epoch 34 | loss: 0.41946 | val_0_rmse: 0.64464 | val_1_rmse: 0.63883 |  0:00:08s
epoch 35 | loss: 0.41664 | val_0_rmse: 0.64956 | val_1_rmse: 0.63267 |  0:00:09s
epoch 36 | loss: 0.42551 | val_0_rmse: 0.64853 | val_1_rmse: 0.62985 |  0:00:09s
epoch 37 | loss: 0.42172 | val_0_rmse: 0.64814 | val_1_rmse: 0.64687 |  0:00:09s
epoch 38 | loss: 0.42299 | val_0_rmse: 0.64377 | val_1_rmse: 0.6496  |  0:00:09s
epoch 39 | loss: 0.41485 | val_0_rmse: 0.64161 | val_1_rmse: 0.65278 |  0:00:10s
epoch 40 | loss: 0.42031 | val_0_rmse: 0.65124 | val_1_rmse: 0.66353 |  0:00:10s
epoch 41 | loss: 0.41013 | val_0_rmse: 0.64594 | val_1_rmse: 0.65603 |  0:00:10s
epoch 42 | loss: 0.41123 | val_0_rmse: 0.64351 | val_1_rmse: 0.6504  |  0:00:10s
epoch 43 | loss: 0.42443 | val_0_rmse: 0.65292 | val_1_rmse: 0.65778 |  0:00:11s
epoch 44 | loss: 0.4134  | val_0_rmse: 0.6606  | val_1_rmse: 0.66522 |  0:00:11s
epoch 45 | loss: 0.42161 | val_0_rmse: 0.64107 | val_1_rmse: 0.65925 |  0:00:11s
epoch 46 | loss: 0.43152 | val_0_rmse: 0.65561 | val_1_rmse: 0.67388 |  0:00:11s
epoch 47 | loss: 0.44035 | val_0_rmse: 0.65854 | val_1_rmse: 0.69293 |  0:00:12s
epoch 48 | loss: 0.43545 | val_0_rmse: 0.65545 | val_1_rmse: 0.68424 |  0:00:12s
epoch 49 | loss: 0.43066 | val_0_rmse: 0.6609  | val_1_rmse: 0.69796 |  0:00:12s
epoch 50 | loss: 0.42637 | val_0_rmse: 0.64024 | val_1_rmse: 0.65553 |  0:00:13s
epoch 51 | loss: 0.41087 | val_0_rmse: 0.63264 | val_1_rmse: 0.65263 |  0:00:13s
epoch 52 | loss: 0.40671 | val_0_rmse: 0.62762 | val_1_rmse: 0.64828 |  0:00:13s
epoch 53 | loss: 0.40889 | val_0_rmse: 0.62484 | val_1_rmse: 0.64311 |  0:00:13s
epoch 54 | loss: 0.40347 | val_0_rmse: 0.62606 | val_1_rmse: 0.64658 |  0:00:14s
epoch 55 | loss: 0.40726 | val_0_rmse: 0.63004 | val_1_rmse: 0.64963 |  0:00:14s
epoch 56 | loss: 0.41766 | val_0_rmse: 0.62885 | val_1_rmse: 0.64554 |  0:00:14s
epoch 57 | loss: 0.41615 | val_0_rmse: 0.62935 | val_1_rmse: 0.64246 |  0:00:14s
epoch 58 | loss: 0.41662 | val_0_rmse: 0.63166 | val_1_rmse: 0.646   |  0:00:15s
epoch 59 | loss: 0.41372 | val_0_rmse: 0.62972 | val_1_rmse: 0.64032 |  0:00:15s
epoch 60 | loss: 0.40862 | val_0_rmse: 0.62946 | val_1_rmse: 0.63242 |  0:00:15s
epoch 61 | loss: 0.40325 | val_0_rmse: 0.63645 | val_1_rmse: 0.63684 |  0:00:15s
epoch 62 | loss: 0.40765 | val_0_rmse: 0.6365  | val_1_rmse: 0.63858 |  0:00:16s
epoch 63 | loss: 0.40553 | val_0_rmse: 0.63216 | val_1_rmse: 0.63278 |  0:00:16s
epoch 64 | loss: 0.39337 | val_0_rmse: 0.62746 | val_1_rmse: 0.62937 |  0:00:16s
epoch 65 | loss: 0.40361 | val_0_rmse: 0.6226  | val_1_rmse: 0.62883 |  0:00:16s
epoch 66 | loss: 0.40223 | val_0_rmse: 0.62494 | val_1_rmse: 0.63834 |  0:00:17s
epoch 67 | loss: 0.39964 | val_0_rmse: 0.62804 | val_1_rmse: 0.64276 |  0:00:17s
epoch 68 | loss: 0.4002  | val_0_rmse: 0.63351 | val_1_rmse: 0.64807 |  0:00:17s
epoch 69 | loss: 0.40133 | val_0_rmse: 0.62942 | val_1_rmse: 0.64664 |  0:00:17s
epoch 70 | loss: 0.40335 | val_0_rmse: 0.6298  | val_1_rmse: 0.646   |  0:00:17s
epoch 71 | loss: 0.39274 | val_0_rmse: 0.63929 | val_1_rmse: 0.65461 |  0:00:18s
epoch 72 | loss: 0.40366 | val_0_rmse: 0.64126 | val_1_rmse: 0.66358 |  0:00:18s
epoch 73 | loss: 0.40418 | val_0_rmse: 0.63771 | val_1_rmse: 0.66937 |  0:00:18s
epoch 74 | loss: 0.4047  | val_0_rmse: 0.63452 | val_1_rmse: 0.67629 |  0:00:18s
epoch 75 | loss: 0.41584 | val_0_rmse: 0.63815 | val_1_rmse: 0.67855 |  0:00:19s
epoch 76 | loss: 0.40624 | val_0_rmse: 0.63952 | val_1_rmse: 0.66484 |  0:00:19s
epoch 77 | loss: 0.40658 | val_0_rmse: 0.6244  | val_1_rmse: 0.65659 |  0:00:19s
epoch 78 | loss: 0.40089 | val_0_rmse: 0.62561 | val_1_rmse: 0.65322 |  0:00:19s
epoch 79 | loss: 0.39673 | val_0_rmse: 0.6289  | val_1_rmse: 0.65448 |  0:00:20s
epoch 80 | loss: 0.40879 | val_0_rmse: 0.62778 | val_1_rmse: 0.64133 |  0:00:20s
epoch 81 | loss: 0.39816 | val_0_rmse: 0.62743 | val_1_rmse: 0.63898 |  0:00:20s
epoch 82 | loss: 0.40878 | val_0_rmse: 0.63309 | val_1_rmse: 0.64942 |  0:00:20s
epoch 83 | loss: 0.40049 | val_0_rmse: 0.63157 | val_1_rmse: 0.65802 |  0:00:21s
epoch 84 | loss: 0.39912 | val_0_rmse: 0.62032 | val_1_rmse: 0.65145 |  0:00:21s
epoch 85 | loss: 0.39958 | val_0_rmse: 0.62375 | val_1_rmse: 0.65796 |  0:00:21s
epoch 86 | loss: 0.39669 | val_0_rmse: 0.62007 | val_1_rmse: 0.66488 |  0:00:21s
epoch 87 | loss: 0.3953  | val_0_rmse: 0.61827 | val_1_rmse: 0.66994 |  0:00:22s
epoch 88 | loss: 0.38165 | val_0_rmse: 0.62743 | val_1_rmse: 0.68309 |  0:00:22s
epoch 89 | loss: 0.38872 | val_0_rmse: 0.62262 | val_1_rmse: 0.67552 |  0:00:22s
epoch 90 | loss: 0.38429 | val_0_rmse: 0.61277 | val_1_rmse: 0.66259 |  0:00:22s
epoch 91 | loss: 0.37917 | val_0_rmse: 0.61389 | val_1_rmse: 0.64885 |  0:00:23s
epoch 92 | loss: 0.40274 | val_0_rmse: 0.61485 | val_1_rmse: 0.64782 |  0:00:23s
epoch 93 | loss: 0.40327 | val_0_rmse: 0.61313 | val_1_rmse: 0.65272 |  0:00:23s
epoch 94 | loss: 0.38518 | val_0_rmse: 0.61787 | val_1_rmse: 0.66286 |  0:00:23s
epoch 95 | loss: 0.39348 | val_0_rmse: 0.61222 | val_1_rmse: 0.65513 |  0:00:24s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.62883
Best weights from best epoch are automatically used!
ended training at: 02:52:35
Feature importance:
[('Area', 0.37132612165752593), ('Baths', 0.12658752279583846), ('Beds', 0.04662677416105656), ('Latitude', 0.1053218659435936), ('Longitude', 0.2134259661825924), ('Month', 0.006422117769467699), ('Year', 0.1302896314899253)]
Mean squared error is of 3407472855.6691422
Mean absolute error:40707.85914677198
MAPE:0.3956882264748173
R2 score:0.5428772368028799
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:52:35
epoch 0  | loss: 1.56744 | val_0_rmse: 1.19803 | val_1_rmse: 1.13246 |  0:00:00s
epoch 1  | loss: 0.79785 | val_0_rmse: 1.13093 | val_1_rmse: 1.11862 |  0:00:00s
epoch 2  | loss: 0.60939 | val_0_rmse: 0.79933 | val_1_rmse: 0.79073 |  0:00:00s
epoch 3  | loss: 0.55792 | val_0_rmse: 0.81677 | val_1_rmse: 0.82448 |  0:00:01s
epoch 4  | loss: 0.53802 | val_0_rmse: 0.89145 | val_1_rmse: 0.88312 |  0:00:01s
epoch 5  | loss: 0.51204 | val_0_rmse: 0.73809 | val_1_rmse: 0.78047 |  0:00:01s
epoch 6  | loss: 0.49435 | val_0_rmse: 0.72204 | val_1_rmse: 0.74615 |  0:00:01s
epoch 7  | loss: 0.49704 | val_0_rmse: 0.75857 | val_1_rmse: 0.8132  |  0:00:02s
epoch 8  | loss: 0.47538 | val_0_rmse: 0.73813 | val_1_rmse: 0.80877 |  0:00:02s
epoch 9  | loss: 0.47464 | val_0_rmse: 0.73944 | val_1_rmse: 0.76592 |  0:00:02s
epoch 10 | loss: 0.466   | val_0_rmse: 0.71867 | val_1_rmse: 0.76834 |  0:00:02s
epoch 11 | loss: 0.46835 | val_0_rmse: 0.71805 | val_1_rmse: 0.76221 |  0:00:03s
epoch 12 | loss: 0.46652 | val_0_rmse: 0.70426 | val_1_rmse: 0.7487  |  0:00:03s
epoch 13 | loss: 0.45243 | val_0_rmse: 0.69203 | val_1_rmse: 0.73664 |  0:00:03s
epoch 14 | loss: 0.45065 | val_0_rmse: 0.67171 | val_1_rmse: 0.69787 |  0:00:03s
epoch 15 | loss: 0.45233 | val_0_rmse: 0.66352 | val_1_rmse: 0.67235 |  0:00:04s
epoch 16 | loss: 0.4551  | val_0_rmse: 0.66037 | val_1_rmse: 0.66911 |  0:00:04s
epoch 17 | loss: 0.44757 | val_0_rmse: 0.66231 | val_1_rmse: 0.6587  |  0:00:04s
epoch 18 | loss: 0.4447  | val_0_rmse: 0.67062 | val_1_rmse: 0.65569 |  0:00:04s
epoch 19 | loss: 0.45952 | val_0_rmse: 0.66735 | val_1_rmse: 0.65838 |  0:00:05s
epoch 20 | loss: 0.45027 | val_0_rmse: 0.66352 | val_1_rmse: 0.66249 |  0:00:05s
epoch 21 | loss: 0.43536 | val_0_rmse: 0.67545 | val_1_rmse: 0.65999 |  0:00:05s
epoch 22 | loss: 0.44198 | val_0_rmse: 0.67282 | val_1_rmse: 0.6594  |  0:00:05s
epoch 23 | loss: 0.43607 | val_0_rmse: 0.65673 | val_1_rmse: 0.66543 |  0:00:06s
epoch 24 | loss: 0.43799 | val_0_rmse: 0.6667  | val_1_rmse: 0.67825 |  0:00:06s
epoch 25 | loss: 0.45643 | val_0_rmse: 0.67528 | val_1_rmse: 0.67732 |  0:00:06s
epoch 26 | loss: 0.43692 | val_0_rmse: 0.66883 | val_1_rmse: 0.67205 |  0:00:06s
epoch 27 | loss: 0.44036 | val_0_rmse: 0.67357 | val_1_rmse: 0.66093 |  0:00:07s
epoch 28 | loss: 0.4357  | val_0_rmse: 0.66514 | val_1_rmse: 0.6608  |  0:00:07s
epoch 29 | loss: 0.44828 | val_0_rmse: 0.65802 | val_1_rmse: 0.65866 |  0:00:07s
epoch 30 | loss: 0.44261 | val_0_rmse: 0.66284 | val_1_rmse: 0.64728 |  0:00:07s
epoch 31 | loss: 0.43713 | val_0_rmse: 0.65052 | val_1_rmse: 0.64351 |  0:00:08s
epoch 32 | loss: 0.45169 | val_0_rmse: 0.6481  | val_1_rmse: 0.64993 |  0:00:08s
epoch 33 | loss: 0.43022 | val_0_rmse: 0.64521 | val_1_rmse: 0.63445 |  0:00:08s
epoch 34 | loss: 0.44097 | val_0_rmse: 0.64298 | val_1_rmse: 0.6357  |  0:00:08s
epoch 35 | loss: 0.42975 | val_0_rmse: 0.64244 | val_1_rmse: 0.63715 |  0:00:09s
epoch 36 | loss: 0.42671 | val_0_rmse: 0.6415  | val_1_rmse: 0.63206 |  0:00:09s
epoch 37 | loss: 0.42828 | val_0_rmse: 0.63529 | val_1_rmse: 0.6413  |  0:00:09s
epoch 38 | loss: 0.42179 | val_0_rmse: 0.64026 | val_1_rmse: 0.64803 |  0:00:09s
epoch 39 | loss: 0.42667 | val_0_rmse: 0.64565 | val_1_rmse: 0.64082 |  0:00:10s
epoch 40 | loss: 0.41643 | val_0_rmse: 0.64401 | val_1_rmse: 0.64029 |  0:00:10s
epoch 41 | loss: 0.42359 | val_0_rmse: 0.64475 | val_1_rmse: 0.65209 |  0:00:10s
epoch 42 | loss: 0.43065 | val_0_rmse: 0.64113 | val_1_rmse: 0.63906 |  0:00:10s
epoch 43 | loss: 0.41945 | val_0_rmse: 0.63819 | val_1_rmse: 0.62949 |  0:00:10s
epoch 44 | loss: 0.40848 | val_0_rmse: 0.6331  | val_1_rmse: 0.63144 |  0:00:11s
epoch 45 | loss: 0.41939 | val_0_rmse: 0.63905 | val_1_rmse: 0.63706 |  0:00:11s
epoch 46 | loss: 0.41216 | val_0_rmse: 0.64393 | val_1_rmse: 0.64069 |  0:00:11s
epoch 47 | loss: 0.41561 | val_0_rmse: 0.64382 | val_1_rmse: 0.65537 |  0:00:11s
epoch 48 | loss: 0.42046 | val_0_rmse: 0.63616 | val_1_rmse: 0.65442 |  0:00:12s
epoch 49 | loss: 0.4168  | val_0_rmse: 0.63316 | val_1_rmse: 0.64689 |  0:00:12s
epoch 50 | loss: 0.41752 | val_0_rmse: 0.63929 | val_1_rmse: 0.64357 |  0:00:12s
epoch 51 | loss: 0.41576 | val_0_rmse: 0.63083 | val_1_rmse: 0.64269 |  0:00:12s
epoch 52 | loss: 0.40585 | val_0_rmse: 0.63301 | val_1_rmse: 0.6594  |  0:00:13s
epoch 53 | loss: 0.41597 | val_0_rmse: 0.64227 | val_1_rmse: 0.65823 |  0:00:13s
epoch 54 | loss: 0.40436 | val_0_rmse: 0.66272 | val_1_rmse: 0.64966 |  0:00:13s
epoch 55 | loss: 0.42133 | val_0_rmse: 0.66658 | val_1_rmse: 0.64733 |  0:00:13s
epoch 56 | loss: 0.43099 | val_0_rmse: 0.65568 | val_1_rmse: 0.64715 |  0:00:14s
epoch 57 | loss: 0.41748 | val_0_rmse: 0.67801 | val_1_rmse: 0.65814 |  0:00:14s
epoch 58 | loss: 0.41213 | val_0_rmse: 0.66702 | val_1_rmse: 0.64773 |  0:00:14s
epoch 59 | loss: 0.40429 | val_0_rmse: 0.66393 | val_1_rmse: 0.64531 |  0:00:14s
epoch 60 | loss: 0.39917 | val_0_rmse: 0.66589 | val_1_rmse: 0.65969 |  0:00:15s
epoch 61 | loss: 0.41049 | val_0_rmse: 0.66776 | val_1_rmse: 0.66349 |  0:00:15s
epoch 62 | loss: 0.40413 | val_0_rmse: 0.63635 | val_1_rmse: 0.64106 |  0:00:15s
epoch 63 | loss: 0.40381 | val_0_rmse: 0.62157 | val_1_rmse: 0.64373 |  0:00:15s
epoch 64 | loss: 0.39754 | val_0_rmse: 0.63155 | val_1_rmse: 0.64088 |  0:00:16s
epoch 65 | loss: 0.40363 | val_0_rmse: 0.63591 | val_1_rmse: 0.63521 |  0:00:16s
epoch 66 | loss: 0.39679 | val_0_rmse: 0.61869 | val_1_rmse: 0.64121 |  0:00:16s
epoch 67 | loss: 0.39347 | val_0_rmse: 0.61763 | val_1_rmse: 0.65133 |  0:00:16s
epoch 68 | loss: 0.40432 | val_0_rmse: 0.61624 | val_1_rmse: 0.63978 |  0:00:17s
epoch 69 | loss: 0.38996 | val_0_rmse: 0.62632 | val_1_rmse: 0.64016 |  0:00:17s
epoch 70 | loss: 0.39326 | val_0_rmse: 0.61837 | val_1_rmse: 0.63781 |  0:00:17s
epoch 71 | loss: 0.40096 | val_0_rmse: 0.61357 | val_1_rmse: 0.63997 |  0:00:17s
epoch 72 | loss: 0.39712 | val_0_rmse: 0.61566 | val_1_rmse: 0.6386  |  0:00:18s
epoch 73 | loss: 0.38561 | val_0_rmse: 0.62547 | val_1_rmse: 0.64973 |  0:00:18s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.62949
Best weights from best epoch are automatically used!
ended training at: 02:52:53
Feature importance:
[('Area', 0.3245251488547025), ('Baths', 0.17862310779954937), ('Beds', 0.057869267069870625), ('Latitude', 0.17182558471619286), ('Longitude', 0.08926692538417318), ('Month', 0.06874629267349869), ('Year', 0.10914367350201275)]
Mean squared error is of 3220479084.046977
Mean absolute error:41576.92330583791
MAPE:0.3829328456909354
R2 score:0.5566122949340566
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:52:53
epoch 0  | loss: 1.6636  | val_0_rmse: 1.34119 | val_1_rmse: 1.10083 |  0:00:00s
epoch 1  | loss: 0.81689 | val_0_rmse: 1.77493 | val_1_rmse: 1.41251 |  0:00:00s
epoch 2  | loss: 0.64031 | val_0_rmse: 1.65816 | val_1_rmse: 1.5347  |  0:00:00s
epoch 3  | loss: 0.57735 | val_0_rmse: 1.82875 | val_1_rmse: 1.74057 |  0:00:00s
epoch 4  | loss: 0.53427 | val_0_rmse: 1.38005 | val_1_rmse: 1.51491 |  0:00:01s
epoch 5  | loss: 0.51298 | val_0_rmse: 1.1577  | val_1_rmse: 1.25453 |  0:00:01s
epoch 6  | loss: 0.48797 | val_0_rmse: 0.89276 | val_1_rmse: 0.93188 |  0:00:01s
epoch 7  | loss: 0.47766 | val_0_rmse: 0.7994  | val_1_rmse: 0.83619 |  0:00:01s
epoch 8  | loss: 0.46361 | val_0_rmse: 0.77625 | val_1_rmse: 0.84158 |  0:00:02s
epoch 9  | loss: 0.48768 | val_0_rmse: 0.78667 | val_1_rmse: 0.82312 |  0:00:02s
epoch 10 | loss: 0.46899 | val_0_rmse: 0.7784  | val_1_rmse: 0.80601 |  0:00:02s
epoch 11 | loss: 0.46317 | val_0_rmse: 0.72712 | val_1_rmse: 0.73789 |  0:00:03s
epoch 12 | loss: 0.45731 | val_0_rmse: 0.73506 | val_1_rmse: 0.73568 |  0:00:03s
epoch 13 | loss: 0.46917 | val_0_rmse: 0.70148 | val_1_rmse: 0.69518 |  0:00:03s
epoch 14 | loss: 0.45455 | val_0_rmse: 0.67241 | val_1_rmse: 0.67594 |  0:00:03s
epoch 15 | loss: 0.44433 | val_0_rmse: 0.67453 | val_1_rmse: 0.68188 |  0:00:04s
epoch 16 | loss: 0.44492 | val_0_rmse: 0.65399 | val_1_rmse: 0.66549 |  0:00:04s
epoch 17 | loss: 0.45064 | val_0_rmse: 0.64646 | val_1_rmse: 0.64819 |  0:00:04s
epoch 18 | loss: 0.43906 | val_0_rmse: 0.65469 | val_1_rmse: 0.65827 |  0:00:04s
epoch 19 | loss: 0.42274 | val_0_rmse: 0.65222 | val_1_rmse: 0.6392  |  0:00:05s
epoch 20 | loss: 0.43163 | val_0_rmse: 0.64815 | val_1_rmse: 0.60984 |  0:00:05s
epoch 21 | loss: 0.43757 | val_0_rmse: 0.65136 | val_1_rmse: 0.60956 |  0:00:05s
epoch 22 | loss: 0.42246 | val_0_rmse: 0.65093 | val_1_rmse: 0.61686 |  0:00:05s
epoch 23 | loss: 0.42364 | val_0_rmse: 0.65194 | val_1_rmse: 0.63169 |  0:00:06s
epoch 24 | loss: 0.42632 | val_0_rmse: 0.64981 | val_1_rmse: 0.63812 |  0:00:06s
epoch 25 | loss: 0.42698 | val_0_rmse: 0.63551 | val_1_rmse: 0.62332 |  0:00:06s
epoch 26 | loss: 0.41676 | val_0_rmse: 0.63736 | val_1_rmse: 0.62745 |  0:00:06s
epoch 27 | loss: 0.40935 | val_0_rmse: 0.62743 | val_1_rmse: 0.62878 |  0:00:07s
epoch 28 | loss: 0.41858 | val_0_rmse: 0.64332 | val_1_rmse: 0.63488 |  0:00:07s
epoch 29 | loss: 0.42422 | val_0_rmse: 0.66729 | val_1_rmse: 0.63166 |  0:00:07s
epoch 30 | loss: 0.40922 | val_0_rmse: 0.6767  | val_1_rmse: 0.64184 |  0:00:07s
epoch 31 | loss: 0.43013 | val_0_rmse: 0.64651 | val_1_rmse: 0.62318 |  0:00:08s
epoch 32 | loss: 0.42687 | val_0_rmse: 0.63793 | val_1_rmse: 0.62773 |  0:00:08s
epoch 33 | loss: 0.42282 | val_0_rmse: 0.63581 | val_1_rmse: 0.63736 |  0:00:08s
epoch 34 | loss: 0.39795 | val_0_rmse: 0.64713 | val_1_rmse: 0.65335 |  0:00:08s
epoch 35 | loss: 0.41601 | val_0_rmse: 0.62741 | val_1_rmse: 0.62921 |  0:00:09s
epoch 36 | loss: 0.40511 | val_0_rmse: 0.63482 | val_1_rmse: 0.64392 |  0:00:09s
epoch 37 | loss: 0.40709 | val_0_rmse: 0.63334 | val_1_rmse: 0.64328 |  0:00:09s
epoch 38 | loss: 0.4107  | val_0_rmse: 0.63108 | val_1_rmse: 0.64418 |  0:00:09s
epoch 39 | loss: 0.40219 | val_0_rmse: 0.64956 | val_1_rmse: 0.64252 |  0:00:10s
epoch 40 | loss: 0.39456 | val_0_rmse: 0.63207 | val_1_rmse: 0.64089 |  0:00:10s
epoch 41 | loss: 0.38681 | val_0_rmse: 0.6143  | val_1_rmse: 0.63796 |  0:00:10s
epoch 42 | loss: 0.38191 | val_0_rmse: 0.6169  | val_1_rmse: 0.63701 |  0:00:10s
epoch 43 | loss: 0.38137 | val_0_rmse: 0.62815 | val_1_rmse: 0.64719 |  0:00:11s
epoch 44 | loss: 0.40055 | val_0_rmse: 0.6431  | val_1_rmse: 0.65023 |  0:00:11s
epoch 45 | loss: 0.3878  | val_0_rmse: 0.64278 | val_1_rmse: 0.65244 |  0:00:11s
epoch 46 | loss: 0.4147  | val_0_rmse: 0.64597 | val_1_rmse: 0.65802 |  0:00:11s
epoch 47 | loss: 0.41279 | val_0_rmse: 0.65022 | val_1_rmse: 0.67587 |  0:00:11s
epoch 48 | loss: 0.39407 | val_0_rmse: 0.67745 | val_1_rmse: 0.69193 |  0:00:12s
epoch 49 | loss: 0.42162 | val_0_rmse: 0.65627 | val_1_rmse: 0.67004 |  0:00:12s
epoch 50 | loss: 0.42319 | val_0_rmse: 0.63786 | val_1_rmse: 0.65917 |  0:00:12s
epoch 51 | loss: 0.41418 | val_0_rmse: 0.63484 | val_1_rmse: 0.66167 |  0:00:13s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.60956
Best weights from best epoch are automatically used!
ended training at: 02:53:06
Feature importance:
[('Area', 0.3838836809907663), ('Baths', 0.13157105271121497), ('Beds', 0.12202487180079616), ('Latitude', 0.20982081729694718), ('Longitude', 0.00020154883792538098), ('Month', 0.061291279353241856), ('Year', 0.09120674900910818)]
Mean squared error is of 3900539101.7030168
Mean absolute error:42535.473674175824
MAPE:0.3252597117874156
R2 score:0.5581170251304425
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:53:07
epoch 0  | loss: 1.84807 | val_0_rmse: 1.45216 | val_1_rmse: 1.56513 |  0:00:00s
epoch 1  | loss: 0.96812 | val_0_rmse: 1.05435 | val_1_rmse: 1.11029 |  0:00:00s
epoch 2  | loss: 0.72686 | val_0_rmse: 1.05211 | val_1_rmse: 1.04501 |  0:00:00s
epoch 3  | loss: 0.67621 | val_0_rmse: 1.02504 | val_1_rmse: 1.03096 |  0:00:00s
epoch 4  | loss: 0.57682 | val_0_rmse: 1.04423 | val_1_rmse: 1.05273 |  0:00:01s
epoch 5  | loss: 0.58359 | val_0_rmse: 0.80701 | val_1_rmse: 0.8684  |  0:00:01s
epoch 6  | loss: 0.55022 | val_0_rmse: 0.84714 | val_1_rmse: 0.90549 |  0:00:01s
epoch 7  | loss: 0.55261 | val_0_rmse: 0.85057 | val_1_rmse: 0.8945  |  0:00:01s
epoch 8  | loss: 0.51604 | val_0_rmse: 0.84527 | val_1_rmse: 0.8819  |  0:00:02s
epoch 9  | loss: 0.49659 | val_0_rmse: 0.75405 | val_1_rmse: 0.79715 |  0:00:02s
epoch 10 | loss: 0.48266 | val_0_rmse: 0.72409 | val_1_rmse: 0.77044 |  0:00:02s
epoch 11 | loss: 0.48897 | val_0_rmse: 0.74149 | val_1_rmse: 0.7863  |  0:00:02s
epoch 12 | loss: 0.47712 | val_0_rmse: 0.73689 | val_1_rmse: 0.79308 |  0:00:03s
epoch 13 | loss: 0.4742  | val_0_rmse: 0.717   | val_1_rmse: 0.78918 |  0:00:03s
epoch 14 | loss: 0.47196 | val_0_rmse: 0.70199 | val_1_rmse: 0.76886 |  0:00:03s
epoch 15 | loss: 0.47148 | val_0_rmse: 0.68748 | val_1_rmse: 0.75486 |  0:00:03s
epoch 16 | loss: 0.45125 | val_0_rmse: 0.6844  | val_1_rmse: 0.7631  |  0:00:04s
epoch 17 | loss: 0.47854 | val_0_rmse: 0.69853 | val_1_rmse: 0.78242 |  0:00:04s
epoch 18 | loss: 0.46119 | val_0_rmse: 0.67965 | val_1_rmse: 0.76566 |  0:00:04s
epoch 19 | loss: 0.46368 | val_0_rmse: 0.66019 | val_1_rmse: 0.74115 |  0:00:04s
epoch 20 | loss: 0.43793 | val_0_rmse: 0.66823 | val_1_rmse: 0.73801 |  0:00:05s
epoch 21 | loss: 0.45251 | val_0_rmse: 0.67415 | val_1_rmse: 0.74505 |  0:00:05s
epoch 22 | loss: 0.44674 | val_0_rmse: 0.68411 | val_1_rmse: 0.76234 |  0:00:05s
epoch 23 | loss: 0.46736 | val_0_rmse: 0.66061 | val_1_rmse: 0.73841 |  0:00:05s
epoch 24 | loss: 0.43646 | val_0_rmse: 0.68757 | val_1_rmse: 0.76302 |  0:00:06s
epoch 25 | loss: 0.45843 | val_0_rmse: 0.66404 | val_1_rmse: 0.74085 |  0:00:06s
epoch 26 | loss: 0.44491 | val_0_rmse: 0.68179 | val_1_rmse: 0.7408  |  0:00:06s
epoch 27 | loss: 0.44913 | val_0_rmse: 0.6734  | val_1_rmse: 0.72888 |  0:00:06s
epoch 28 | loss: 0.43561 | val_0_rmse: 0.66519 | val_1_rmse: 0.72441 |  0:00:07s
epoch 29 | loss: 0.44634 | val_0_rmse: 0.67039 | val_1_rmse: 0.74015 |  0:00:07s
epoch 30 | loss: 0.43217 | val_0_rmse: 0.65688 | val_1_rmse: 0.74669 |  0:00:07s
epoch 31 | loss: 0.42129 | val_0_rmse: 0.64692 | val_1_rmse: 0.74285 |  0:00:07s
epoch 32 | loss: 0.41953 | val_0_rmse: 0.6479  | val_1_rmse: 0.73785 |  0:00:08s
epoch 33 | loss: 0.44252 | val_0_rmse: 0.64958 | val_1_rmse: 0.73581 |  0:00:08s
epoch 34 | loss: 0.42461 | val_0_rmse: 0.65093 | val_1_rmse: 0.72915 |  0:00:08s
epoch 35 | loss: 0.42408 | val_0_rmse: 0.64354 | val_1_rmse: 0.72351 |  0:00:08s
epoch 36 | loss: 0.41934 | val_0_rmse: 0.63916 | val_1_rmse: 0.73378 |  0:00:09s
epoch 37 | loss: 0.42226 | val_0_rmse: 0.64011 | val_1_rmse: 0.72826 |  0:00:09s
epoch 38 | loss: 0.42921 | val_0_rmse: 0.63007 | val_1_rmse: 0.71743 |  0:00:09s
epoch 39 | loss: 0.4186  | val_0_rmse: 0.624   | val_1_rmse: 0.72617 |  0:00:09s
epoch 40 | loss: 0.40374 | val_0_rmse: 0.62304 | val_1_rmse: 0.73687 |  0:00:10s
epoch 41 | loss: 0.40161 | val_0_rmse: 0.62461 | val_1_rmse: 0.73977 |  0:00:10s
epoch 42 | loss: 0.41254 | val_0_rmse: 0.61976 | val_1_rmse: 0.73943 |  0:00:10s
epoch 43 | loss: 0.39467 | val_0_rmse: 0.60938 | val_1_rmse: 0.73671 |  0:00:11s
epoch 44 | loss: 0.40247 | val_0_rmse: 0.61603 | val_1_rmse: 0.74106 |  0:00:11s
epoch 45 | loss: 0.39343 | val_0_rmse: 0.62107 | val_1_rmse: 0.75278 |  0:00:11s
epoch 46 | loss: 0.40769 | val_0_rmse: 0.61903 | val_1_rmse: 0.74278 |  0:00:11s
epoch 47 | loss: 0.39126 | val_0_rmse: 0.64683 | val_1_rmse: 0.74197 |  0:00:12s
epoch 48 | loss: 0.37603 | val_0_rmse: 0.63553 | val_1_rmse: 0.73266 |  0:00:12s
epoch 49 | loss: 0.38911 | val_0_rmse: 0.64148 | val_1_rmse: 0.73439 |  0:00:12s
epoch 50 | loss: 0.39563 | val_0_rmse: 0.65526 | val_1_rmse: 0.71676 |  0:00:12s
epoch 51 | loss: 0.42253 | val_0_rmse: 0.65432 | val_1_rmse: 0.71028 |  0:00:13s
epoch 52 | loss: 0.41048 | val_0_rmse: 0.65823 | val_1_rmse: 0.71459 |  0:00:13s
epoch 53 | loss: 0.40526 | val_0_rmse: 0.64606 | val_1_rmse: 0.69783 |  0:00:13s
epoch 54 | loss: 0.41282 | val_0_rmse: 0.63602 | val_1_rmse: 0.68827 |  0:00:13s
epoch 55 | loss: 0.40531 | val_0_rmse: 0.64827 | val_1_rmse: 0.712   |  0:00:14s
epoch 56 | loss: 0.40997 | val_0_rmse: 0.63559 | val_1_rmse: 0.70375 |  0:00:14s
epoch 57 | loss: 0.39551 | val_0_rmse: 0.63039 | val_1_rmse: 0.69442 |  0:00:14s
epoch 58 | loss: 0.40066 | val_0_rmse: 0.65786 | val_1_rmse: 0.6977  |  0:00:14s
epoch 59 | loss: 0.39971 | val_0_rmse: 0.65494 | val_1_rmse: 0.69697 |  0:00:15s
epoch 60 | loss: 0.41795 | val_0_rmse: 0.67247 | val_1_rmse: 0.70615 |  0:00:15s
epoch 61 | loss: 0.40709 | val_0_rmse: 0.63146 | val_1_rmse: 0.70037 |  0:00:15s
epoch 62 | loss: 0.37739 | val_0_rmse: 0.62314 | val_1_rmse: 0.70363 |  0:00:15s
epoch 63 | loss: 0.38686 | val_0_rmse: 0.6152  | val_1_rmse: 0.69087 |  0:00:16s
epoch 64 | loss: 0.36975 | val_0_rmse: 0.60774 | val_1_rmse: 0.68304 |  0:00:16s
epoch 65 | loss: 0.37541 | val_0_rmse: 0.60879 | val_1_rmse: 0.68792 |  0:00:16s
epoch 66 | loss: 0.35541 | val_0_rmse: 0.60738 | val_1_rmse: 0.67908 |  0:00:16s
epoch 67 | loss: 0.37181 | val_0_rmse: 0.60752 | val_1_rmse: 0.67102 |  0:00:17s
epoch 68 | loss: 0.37221 | val_0_rmse: 0.61328 | val_1_rmse: 0.6752  |  0:00:17s
epoch 69 | loss: 0.37386 | val_0_rmse: 0.63507 | val_1_rmse: 0.67473 |  0:00:17s
epoch 70 | loss: 0.37313 | val_0_rmse: 0.63834 | val_1_rmse: 0.72157 |  0:00:17s
epoch 71 | loss: 0.39303 | val_0_rmse: 0.64587 | val_1_rmse: 0.69927 |  0:00:17s
epoch 72 | loss: 0.36855 | val_0_rmse: 0.65501 | val_1_rmse: 0.6703  |  0:00:18s
epoch 73 | loss: 0.36664 | val_0_rmse: 0.63256 | val_1_rmse: 0.67555 |  0:00:18s
epoch 74 | loss: 0.3564  | val_0_rmse: 0.62741 | val_1_rmse: 0.67326 |  0:00:18s
epoch 75 | loss: 0.36223 | val_0_rmse: 0.62545 | val_1_rmse: 0.67246 |  0:00:19s
epoch 76 | loss: 0.36566 | val_0_rmse: 0.62419 | val_1_rmse: 0.67673 |  0:00:19s
epoch 77 | loss: 0.35055 | val_0_rmse: 0.61125 | val_1_rmse: 0.67377 |  0:00:19s
epoch 78 | loss: 0.35525 | val_0_rmse: 0.61689 | val_1_rmse: 0.67684 |  0:00:19s
epoch 79 | loss: 0.34612 | val_0_rmse: 0.61363 | val_1_rmse: 0.6666  |  0:00:19s
epoch 80 | loss: 0.35223 | val_0_rmse: 0.61804 | val_1_rmse: 0.71645 |  0:00:20s
epoch 81 | loss: 0.35089 | val_0_rmse: 0.60238 | val_1_rmse: 0.69842 |  0:00:20s
epoch 82 | loss: 0.36409 | val_0_rmse: 0.6139  | val_1_rmse: 0.71033 |  0:00:20s
epoch 83 | loss: 0.36018 | val_0_rmse: 0.61572 | val_1_rmse: 0.70585 |  0:00:21s
epoch 84 | loss: 0.36483 | val_0_rmse: 0.6385  | val_1_rmse: 0.71138 |  0:00:21s
epoch 85 | loss: 0.37351 | val_0_rmse: 0.60689 | val_1_rmse: 0.70851 |  0:00:21s
epoch 86 | loss: 0.37442 | val_0_rmse: 0.60977 | val_1_rmse: 0.71943 |  0:00:21s
epoch 87 | loss: 0.36726 | val_0_rmse: 0.6233  | val_1_rmse: 0.71767 |  0:00:21s
epoch 88 | loss: 0.37344 | val_0_rmse: 0.59853 | val_1_rmse: 0.70396 |  0:00:22s
epoch 89 | loss: 0.34974 | val_0_rmse: 0.5872  | val_1_rmse: 0.68869 |  0:00:22s
epoch 90 | loss: 0.34666 | val_0_rmse: 0.62865 | val_1_rmse: 0.69456 |  0:00:22s
epoch 91 | loss: 0.35028 | val_0_rmse: 0.6063  | val_1_rmse: 0.69762 |  0:00:22s
epoch 92 | loss: 0.35471 | val_0_rmse: 0.59082 | val_1_rmse: 0.68301 |  0:00:23s
epoch 93 | loss: 0.34734 | val_0_rmse: 0.59314 | val_1_rmse: 0.66277 |  0:00:23s
epoch 94 | loss: 0.35249 | val_0_rmse: 0.59726 | val_1_rmse: 0.66267 |  0:00:23s
epoch 95 | loss: 0.35552 | val_0_rmse: 0.60937 | val_1_rmse: 0.6763  |  0:00:23s
epoch 96 | loss: 0.35906 | val_0_rmse: 0.59774 | val_1_rmse: 0.66866 |  0:00:24s
epoch 97 | loss: 0.34456 | val_0_rmse: 0.61032 | val_1_rmse: 0.66673 |  0:00:24s
epoch 98 | loss: 0.36346 | val_0_rmse: 0.61745 | val_1_rmse: 0.68505 |  0:00:24s
epoch 99 | loss: 0.35106 | val_0_rmse: 0.62698 | val_1_rmse: 0.69972 |  0:00:24s
epoch 100| loss: 0.3557  | val_0_rmse: 0.62404 | val_1_rmse: 0.69299 |  0:00:25s
epoch 101| loss: 0.35451 | val_0_rmse: 0.59274 | val_1_rmse: 0.68054 |  0:00:25s
epoch 102| loss: 0.33472 | val_0_rmse: 0.59197 | val_1_rmse: 0.68973 |  0:00:25s
epoch 103| loss: 0.34381 | val_0_rmse: 0.60347 | val_1_rmse: 0.68396 |  0:00:25s
epoch 104| loss: 0.33212 | val_0_rmse: 0.61811 | val_1_rmse: 0.69277 |  0:00:26s
epoch 105| loss: 0.35283 | val_0_rmse: 0.60467 | val_1_rmse: 0.68921 |  0:00:26s
epoch 106| loss: 0.34639 | val_0_rmse: 0.62142 | val_1_rmse: 0.70083 |  0:00:26s
epoch 107| loss: 0.3385  | val_0_rmse: 0.62751 | val_1_rmse: 0.69692 |  0:00:26s
epoch 108| loss: 0.33552 | val_0_rmse: 0.62586 | val_1_rmse: 0.71852 |  0:00:27s
epoch 109| loss: 0.35394 | val_0_rmse: 0.61202 | val_1_rmse: 0.76681 |  0:00:27s
epoch 110| loss: 0.33926 | val_0_rmse: 0.60176 | val_1_rmse: 0.78007 |  0:00:27s
epoch 111| loss: 0.34435 | val_0_rmse: 0.59364 | val_1_rmse: 0.69936 |  0:00:27s
epoch 112| loss: 0.34592 | val_0_rmse: 0.60606 | val_1_rmse: 0.67771 |  0:00:28s
epoch 113| loss: 0.34524 | val_0_rmse: 0.63611 | val_1_rmse: 0.69318 |  0:00:28s
epoch 114| loss: 0.35767 | val_0_rmse: 0.60712 | val_1_rmse: 0.67652 |  0:00:28s
epoch 115| loss: 0.34915 | val_0_rmse: 0.60266 | val_1_rmse: 0.67331 |  0:00:28s
epoch 116| loss: 0.34391 | val_0_rmse: 0.6057  | val_1_rmse: 0.6783  |  0:00:29s
epoch 117| loss: 0.34396 | val_0_rmse: 0.60534 | val_1_rmse: 0.68411 |  0:00:29s
epoch 118| loss: 0.35398 | val_0_rmse: 0.60249 | val_1_rmse: 0.6856  |  0:00:29s
epoch 119| loss: 0.34806 | val_0_rmse: 0.60826 | val_1_rmse: 0.68401 |  0:00:29s
epoch 120| loss: 0.34517 | val_0_rmse: 0.64157 | val_1_rmse: 0.69273 |  0:00:30s
epoch 121| loss: 0.34689 | val_0_rmse: 0.60951 | val_1_rmse: 0.6725  |  0:00:30s
epoch 122| loss: 0.32967 | val_0_rmse: 0.59796 | val_1_rmse: 0.66852 |  0:00:30s
epoch 123| loss: 0.33181 | val_0_rmse: 0.59419 | val_1_rmse: 0.676   |  0:00:30s
epoch 124| loss: 0.34619 | val_0_rmse: 0.5938  | val_1_rmse: 0.67763 |  0:00:31s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.66267
Best weights from best epoch are automatically used!
ended training at: 02:53:38
Feature importance:
[('Area', 0.24738434324121994), ('Baths', 0.0956519823647124), ('Beds', 0.04191647791427813), ('Latitude', 0.3025366937919938), ('Longitude', 0.22030913743053068), ('Month', 0.0033182253244818487), ('Year', 0.08888313993278314)]
Mean squared error is of 2615928971.147631
Mean absolute error:36460.127924381864
MAPE:0.3278500248479899
R2 score:0.628529933267592
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:53:38
epoch 0  | loss: 1.83425 | val_0_rmse: 0.94397 | val_1_rmse: 0.89457 |  0:00:00s
epoch 1  | loss: 0.88669 | val_0_rmse: 1.05031 | val_1_rmse: 1.05733 |  0:00:00s
epoch 2  | loss: 0.61166 | val_0_rmse: 1.05434 | val_1_rmse: 1.21513 |  0:00:00s
epoch 3  | loss: 0.61247 | val_0_rmse: 0.8912  | val_1_rmse: 0.85155 |  0:00:00s
epoch 4  | loss: 0.58575 | val_0_rmse: 0.77681 | val_1_rmse: 0.71309 |  0:00:01s
epoch 5  | loss: 0.53343 | val_0_rmse: 0.78102 | val_1_rmse: 0.75862 |  0:00:01s
epoch 6  | loss: 0.50923 | val_0_rmse: 0.80283 | val_1_rmse: 0.78366 |  0:00:01s
epoch 7  | loss: 0.50779 | val_0_rmse: 0.81078 | val_1_rmse: 0.73425 |  0:00:02s
epoch 8  | loss: 0.49182 | val_0_rmse: 0.74826 | val_1_rmse: 0.67366 |  0:00:02s
epoch 9  | loss: 0.4809  | val_0_rmse: 0.71856 | val_1_rmse: 0.67421 |  0:00:02s
epoch 10 | loss: 0.47141 | val_0_rmse: 0.70132 | val_1_rmse: 0.65364 |  0:00:02s
epoch 11 | loss: 0.48285 | val_0_rmse: 0.71485 | val_1_rmse: 0.68085 |  0:00:03s
epoch 12 | loss: 0.46027 | val_0_rmse: 0.76864 | val_1_rmse: 0.75617 |  0:00:03s
epoch 13 | loss: 0.48711 | val_0_rmse: 0.70667 | val_1_rmse: 0.67237 |  0:00:03s
epoch 14 | loss: 0.46772 | val_0_rmse: 0.7152  | val_1_rmse: 0.66106 |  0:00:03s
epoch 15 | loss: 0.47636 | val_0_rmse: 0.70378 | val_1_rmse: 0.66436 |  0:00:03s
epoch 16 | loss: 0.43895 | val_0_rmse: 0.68577 | val_1_rmse: 0.63536 |  0:00:04s
epoch 17 | loss: 0.46206 | val_0_rmse: 0.70436 | val_1_rmse: 0.63152 |  0:00:04s
epoch 18 | loss: 0.46204 | val_0_rmse: 0.71023 | val_1_rmse: 0.61888 |  0:00:04s
epoch 19 | loss: 0.45738 | val_0_rmse: 0.71211 | val_1_rmse: 0.63753 |  0:00:05s
epoch 20 | loss: 0.45523 | val_0_rmse: 0.69096 | val_1_rmse: 0.63338 |  0:00:05s
epoch 21 | loss: 0.45285 | val_0_rmse: 0.66447 | val_1_rmse: 0.61426 |  0:00:05s
epoch 22 | loss: 0.43925 | val_0_rmse: 0.70792 | val_1_rmse: 0.63871 |  0:00:05s
epoch 23 | loss: 0.41557 | val_0_rmse: 0.7158  | val_1_rmse: 0.65689 |  0:00:05s
epoch 24 | loss: 0.43662 | val_0_rmse: 0.6632  | val_1_rmse: 0.58695 |  0:00:06s
epoch 25 | loss: 0.41881 | val_0_rmse: 0.71621 | val_1_rmse: 0.64767 |  0:00:06s
epoch 26 | loss: 0.41771 | val_0_rmse: 0.71722 | val_1_rmse: 0.64401 |  0:00:06s
epoch 27 | loss: 0.40283 | val_0_rmse: 0.66174 | val_1_rmse: 0.5864  |  0:00:07s
epoch 28 | loss: 0.39842 | val_0_rmse: 0.64162 | val_1_rmse: 0.55523 |  0:00:07s
epoch 29 | loss: 0.43135 | val_0_rmse: 0.6551  | val_1_rmse: 0.5621  |  0:00:07s
epoch 30 | loss: 0.3943  | val_0_rmse: 0.67172 | val_1_rmse: 0.59158 |  0:00:07s
epoch 31 | loss: 0.40829 | val_0_rmse: 0.69039 | val_1_rmse: 0.63651 |  0:00:08s
epoch 32 | loss: 0.41397 | val_0_rmse: 0.65862 | val_1_rmse: 0.59165 |  0:00:08s
epoch 33 | loss: 0.39787 | val_0_rmse: 0.6436  | val_1_rmse: 0.58635 |  0:00:08s
epoch 34 | loss: 0.40616 | val_0_rmse: 0.62619 | val_1_rmse: 0.57162 |  0:00:08s
epoch 35 | loss: 0.40036 | val_0_rmse: 0.65175 | val_1_rmse: 0.58636 |  0:00:09s
epoch 36 | loss: 0.38258 | val_0_rmse: 0.64893 | val_1_rmse: 0.57345 |  0:00:09s
epoch 37 | loss: 0.38555 | val_0_rmse: 0.66257 | val_1_rmse: 0.59964 |  0:00:09s
epoch 38 | loss: 0.39512 | val_0_rmse: 0.67601 | val_1_rmse: 0.62924 |  0:00:09s
epoch 39 | loss: 0.40271 | val_0_rmse: 0.64556 | val_1_rmse: 0.59535 |  0:00:10s
epoch 40 | loss: 0.38801 | val_0_rmse: 0.6579  | val_1_rmse: 0.5995  |  0:00:10s
epoch 41 | loss: 0.39517 | val_0_rmse: 0.62739 | val_1_rmse: 0.57417 |  0:00:10s
epoch 42 | loss: 0.38553 | val_0_rmse: 0.63146 | val_1_rmse: 0.58726 |  0:00:10s
epoch 43 | loss: 0.39142 | val_0_rmse: 0.6373  | val_1_rmse: 0.59893 |  0:00:11s
epoch 44 | loss: 0.38719 | val_0_rmse: 0.63733 | val_1_rmse: 0.59432 |  0:00:11s
epoch 45 | loss: 0.39402 | val_0_rmse: 0.6491  | val_1_rmse: 0.61672 |  0:00:11s
epoch 46 | loss: 0.39512 | val_0_rmse: 0.66866 | val_1_rmse: 0.64945 |  0:00:11s
epoch 47 | loss: 0.39456 | val_0_rmse: 0.63773 | val_1_rmse: 0.59823 |  0:00:12s
epoch 48 | loss: 0.40885 | val_0_rmse: 0.65046 | val_1_rmse: 0.6107  |  0:00:12s
epoch 49 | loss: 0.39853 | val_0_rmse: 0.65531 | val_1_rmse: 0.60991 |  0:00:12s
epoch 50 | loss: 0.40143 | val_0_rmse: 0.65085 | val_1_rmse: 0.57401 |  0:00:12s
epoch 51 | loss: 0.41143 | val_0_rmse: 0.63297 | val_1_rmse: 0.56523 |  0:00:13s
epoch 52 | loss: 0.38268 | val_0_rmse: 0.62982 | val_1_rmse: 0.55577 |  0:00:13s
epoch 53 | loss: 0.38736 | val_0_rmse: 0.63613 | val_1_rmse: 0.56106 |  0:00:13s
epoch 54 | loss: 0.38136 | val_0_rmse: 0.64521 | val_1_rmse: 0.56739 |  0:00:13s
epoch 55 | loss: 0.37819 | val_0_rmse: 0.63992 | val_1_rmse: 0.5472  |  0:00:14s
epoch 56 | loss: 0.38669 | val_0_rmse: 0.63188 | val_1_rmse: 0.53699 |  0:00:14s
epoch 57 | loss: 0.37045 | val_0_rmse: 0.62899 | val_1_rmse: 0.53584 |  0:00:14s
epoch 58 | loss: 0.38604 | val_0_rmse: 0.65352 | val_1_rmse: 0.55806 |  0:00:14s
epoch 59 | loss: 0.38468 | val_0_rmse: 0.7927  | val_1_rmse: 0.72543 |  0:00:15s
epoch 60 | loss: 0.39361 | val_0_rmse: 0.75274 | val_1_rmse: 0.69006 |  0:00:15s
epoch 61 | loss: 0.38188 | val_0_rmse: 0.67363 | val_1_rmse: 0.60141 |  0:00:15s
epoch 62 | loss: 0.37474 | val_0_rmse: 0.6685  | val_1_rmse: 0.59027 |  0:00:15s
epoch 63 | loss: 0.36375 | val_0_rmse: 0.65029 | val_1_rmse: 0.56007 |  0:00:16s
epoch 64 | loss: 0.38218 | val_0_rmse: 0.62518 | val_1_rmse: 0.52957 |  0:00:16s
epoch 65 | loss: 0.36729 | val_0_rmse: 0.64376 | val_1_rmse: 0.56079 |  0:00:16s
epoch 66 | loss: 0.37035 | val_0_rmse: 0.65617 | val_1_rmse: 0.5829  |  0:00:16s
epoch 67 | loss: 0.35942 | val_0_rmse: 0.63281 | val_1_rmse: 0.55499 |  0:00:17s
epoch 68 | loss: 0.39018 | val_0_rmse: 0.65019 | val_1_rmse: 0.5784  |  0:00:17s
epoch 69 | loss: 0.39152 | val_0_rmse: 0.64834 | val_1_rmse: 0.57596 |  0:00:17s
epoch 70 | loss: 0.36973 | val_0_rmse: 0.65334 | val_1_rmse: 0.58722 |  0:00:17s
epoch 71 | loss: 0.38937 | val_0_rmse: 0.64114 | val_1_rmse: 0.5761  |  0:00:18s
epoch 72 | loss: 0.38666 | val_0_rmse: 0.63012 | val_1_rmse: 0.56913 |  0:00:18s
epoch 73 | loss: 0.37521 | val_0_rmse: 0.63428 | val_1_rmse: 0.57409 |  0:00:18s
epoch 74 | loss: 0.38172 | val_0_rmse: 0.63198 | val_1_rmse: 0.57075 |  0:00:18s
epoch 75 | loss: 0.36863 | val_0_rmse: 0.62602 | val_1_rmse: 0.55174 |  0:00:19s
epoch 76 | loss: 0.38806 | val_0_rmse: 0.62624 | val_1_rmse: 0.55622 |  0:00:19s
epoch 77 | loss: 0.37583 | val_0_rmse: 0.62288 | val_1_rmse: 0.55187 |  0:00:19s
epoch 78 | loss: 0.38101 | val_0_rmse: 0.62493 | val_1_rmse: 0.55016 |  0:00:19s
epoch 79 | loss: 0.38256 | val_0_rmse: 0.64728 | val_1_rmse: 0.56823 |  0:00:19s
epoch 80 | loss: 0.38889 | val_0_rmse: 0.654   | val_1_rmse: 0.57974 |  0:00:20s
epoch 81 | loss: 0.3737  | val_0_rmse: 0.63178 | val_1_rmse: 0.5573  |  0:00:20s
epoch 82 | loss: 0.37205 | val_0_rmse: 0.61612 | val_1_rmse: 0.53934 |  0:00:20s
epoch 83 | loss: 0.37477 | val_0_rmse: 0.6498  | val_1_rmse: 0.57294 |  0:00:21s
epoch 84 | loss: 0.35823 | val_0_rmse: 0.66371 | val_1_rmse: 0.5969  |  0:00:21s
epoch 85 | loss: 0.36988 | val_0_rmse: 0.61666 | val_1_rmse: 0.55116 |  0:00:21s
epoch 86 | loss: 0.37238 | val_0_rmse: 0.61627 | val_1_rmse: 0.55617 |  0:00:21s
epoch 87 | loss: 0.37315 | val_0_rmse: 0.62646 | val_1_rmse: 0.56898 |  0:00:22s
epoch 88 | loss: 0.36778 | val_0_rmse: 0.62659 | val_1_rmse: 0.55203 |  0:00:22s
epoch 89 | loss: 0.3756  | val_0_rmse: 0.62161 | val_1_rmse: 0.55178 |  0:00:22s
epoch 90 | loss: 0.36045 | val_0_rmse: 0.64466 | val_1_rmse: 0.58345 |  0:00:22s
epoch 91 | loss: 0.35511 | val_0_rmse: 0.66263 | val_1_rmse: 0.59821 |  0:00:22s
epoch 92 | loss: 0.36676 | val_0_rmse: 0.66248 | val_1_rmse: 0.59623 |  0:00:23s
epoch 93 | loss: 0.35697 | val_0_rmse: 0.61495 | val_1_rmse: 0.54819 |  0:00:23s
epoch 94 | loss: 0.37363 | val_0_rmse: 0.60692 | val_1_rmse: 0.55332 |  0:00:23s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.52957
Best weights from best epoch are automatically used!
ended training at: 02:54:02
Feature importance:
[('Area', 0.27755915652227203), ('Baths', 0.1025341493324093), ('Beds', 0.001153310713325368), ('Latitude', 0.28631999599652075), ('Longitude', 0.2334598718245864), ('Month', 0.09103443660927396), ('Year', 0.007939079001612165)]
Mean squared error is of 4040956773.813538
Mean absolute error:40295.18552925824
MAPE:0.33081824732808507
R2 score:0.46909915018243287
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:54:02
epoch 0  | loss: 1.74966 | val_0_rmse: 1.04233 | val_1_rmse: 1.33371 |  0:00:00s
epoch 1  | loss: 0.81814 | val_0_rmse: 1.36675 | val_1_rmse: 1.71383 |  0:00:00s
epoch 2  | loss: 0.72537 | val_0_rmse: 0.91102 | val_1_rmse: 1.00346 |  0:00:00s
epoch 3  | loss: 0.62384 | val_0_rmse: 0.82377 | val_1_rmse: 0.88333 |  0:00:01s
epoch 4  | loss: 0.53214 | val_0_rmse: 0.88364 | val_1_rmse: 0.96094 |  0:00:01s
epoch 5  | loss: 0.51233 | val_0_rmse: 0.78349 | val_1_rmse: 0.87701 |  0:00:01s
epoch 6  | loss: 0.48764 | val_0_rmse: 0.71084 | val_1_rmse: 1.98777 |  0:00:01s
epoch 7  | loss: 0.4628  | val_0_rmse: 0.75604 | val_1_rmse: 0.76457 |  0:00:02s
epoch 8  | loss: 0.46791 | val_0_rmse: 0.7307  | val_1_rmse: 0.86727 |  0:00:02s
epoch 9  | loss: 0.42753 | val_0_rmse: 0.73331 | val_1_rmse: 0.8021  |  0:00:02s
epoch 10 | loss: 0.46961 | val_0_rmse: 0.72338 | val_1_rmse: 0.85439 |  0:00:02s
epoch 11 | loss: 0.45768 | val_0_rmse: 0.69326 | val_1_rmse: 0.85719 |  0:00:03s
epoch 12 | loss: 0.44589 | val_0_rmse: 0.72256 | val_1_rmse: 0.84909 |  0:00:03s
epoch 13 | loss: 0.46791 | val_0_rmse: 0.81921 | val_1_rmse: 0.88347 |  0:00:03s
epoch 14 | loss: 0.45954 | val_0_rmse: 0.82805 | val_1_rmse: 0.85479 |  0:00:03s
epoch 15 | loss: 0.48504 | val_0_rmse: 0.83634 | val_1_rmse: 0.86849 |  0:00:04s
epoch 16 | loss: 0.48967 | val_0_rmse: 0.81089 | val_1_rmse: 0.88169 |  0:00:04s
epoch 17 | loss: 0.45557 | val_0_rmse: 0.70142 | val_1_rmse: 0.78296 |  0:00:04s
epoch 18 | loss: 0.44109 | val_0_rmse: 0.65038 | val_1_rmse: 0.73365 |  0:00:04s
epoch 19 | loss: 0.42303 | val_0_rmse: 0.66092 | val_1_rmse: 0.72606 |  0:00:05s
epoch 20 | loss: 0.42923 | val_0_rmse: 0.69227 | val_1_rmse: 0.74213 |  0:00:05s
epoch 21 | loss: 0.41771 | val_0_rmse: 0.64351 | val_1_rmse: 0.72467 |  0:00:05s
epoch 22 | loss: 0.40234 | val_0_rmse: 0.63735 | val_1_rmse: 0.72436 |  0:00:05s
epoch 23 | loss: 0.42166 | val_0_rmse: 0.67089 | val_1_rmse: 0.76138 |  0:00:06s
epoch 24 | loss: 0.41806 | val_0_rmse: 0.69026 | val_1_rmse: 0.77556 |  0:00:06s
epoch 25 | loss: 0.40005 | val_0_rmse: 0.66991 | val_1_rmse: 0.75231 |  0:00:06s
epoch 26 | loss: 0.42988 | val_0_rmse: 0.66214 | val_1_rmse: 0.73545 |  0:00:06s
epoch 27 | loss: 0.41498 | val_0_rmse: 0.66344 | val_1_rmse: 0.727   |  0:00:07s
epoch 28 | loss: 0.43021 | val_0_rmse: 0.65764 | val_1_rmse: 0.72264 |  0:00:07s
epoch 29 | loss: 0.43272 | val_0_rmse: 0.64761 | val_1_rmse: 0.70006 |  0:00:07s
epoch 30 | loss: 0.41975 | val_0_rmse: 0.67197 | val_1_rmse: 0.74363 |  0:00:07s
epoch 31 | loss: 0.43554 | val_0_rmse: 0.63556 | val_1_rmse: 0.72302 |  0:00:08s
epoch 32 | loss: 0.40433 | val_0_rmse: 0.63431 | val_1_rmse: 0.71937 |  0:00:08s
epoch 33 | loss: 0.39082 | val_0_rmse: 0.62966 | val_1_rmse: 0.69838 |  0:00:08s
epoch 34 | loss: 0.40745 | val_0_rmse: 0.63418 | val_1_rmse: 0.69975 |  0:00:08s
epoch 35 | loss: 0.41695 | val_0_rmse: 0.64538 | val_1_rmse: 0.70038 |  0:00:09s
epoch 36 | loss: 0.39862 | val_0_rmse: 0.63755 | val_1_rmse: 0.68465 |  0:00:09s
epoch 37 | loss: 0.38557 | val_0_rmse: 0.63591 | val_1_rmse: 0.6943  |  0:00:09s
epoch 38 | loss: 0.39765 | val_0_rmse: 0.61808 | val_1_rmse: 0.69802 |  0:00:09s
epoch 39 | loss: 0.41087 | val_0_rmse: 0.60776 | val_1_rmse: 0.70296 |  0:00:10s
epoch 40 | loss: 0.38086 | val_0_rmse: 0.62269 | val_1_rmse: 0.70692 |  0:00:10s
epoch 41 | loss: 0.377   | val_0_rmse: 0.6248  | val_1_rmse: 0.70573 |  0:00:10s
epoch 42 | loss: 0.37333 | val_0_rmse: 0.61616 | val_1_rmse: 0.70317 |  0:00:11s
epoch 43 | loss: 0.38318 | val_0_rmse: 0.64735 | val_1_rmse: 0.73406 |  0:00:11s
epoch 44 | loss: 0.37251 | val_0_rmse: 0.6532  | val_1_rmse: 0.72835 |  0:00:11s
epoch 45 | loss: 0.37623 | val_0_rmse: 0.62215 | val_1_rmse: 0.70065 |  0:00:11s
epoch 46 | loss: 0.37264 | val_0_rmse: 0.62548 | val_1_rmse: 0.71249 |  0:00:11s
epoch 47 | loss: 0.37031 | val_0_rmse: 0.6249  | val_1_rmse: 0.69888 |  0:00:12s
epoch 48 | loss: 0.36006 | val_0_rmse: 0.62384 | val_1_rmse: 0.70082 |  0:00:12s
epoch 49 | loss: 0.36364 | val_0_rmse: 0.64446 | val_1_rmse: 0.73313 |  0:00:12s
epoch 50 | loss: 0.36307 | val_0_rmse: 0.64041 | val_1_rmse: 0.73637 |  0:00:12s
epoch 51 | loss: 0.35546 | val_0_rmse: 0.63102 | val_1_rmse: 0.72414 |  0:00:13s
epoch 52 | loss: 0.35305 | val_0_rmse: 0.64919 | val_1_rmse: 0.73471 |  0:00:13s
epoch 53 | loss: 0.3515  | val_0_rmse: 0.65674 | val_1_rmse: 0.7475  |  0:00:13s
epoch 54 | loss: 0.35858 | val_0_rmse: 0.62114 | val_1_rmse: 0.72418 |  0:00:13s
epoch 55 | loss: 0.35332 | val_0_rmse: 0.63518 | val_1_rmse: 0.7346  |  0:00:14s
epoch 56 | loss: 0.33806 | val_0_rmse: 0.66325 | val_1_rmse: 0.74788 |  0:00:14s
epoch 57 | loss: 0.34342 | val_0_rmse: 0.63104 | val_1_rmse: 0.71862 |  0:00:14s
epoch 58 | loss: 0.33657 | val_0_rmse: 0.60698 | val_1_rmse: 0.7093  |  0:00:14s
epoch 59 | loss: 0.35595 | val_0_rmse: 0.63434 | val_1_rmse: 0.75026 |  0:00:15s
epoch 60 | loss: 0.34285 | val_0_rmse: 0.67405 | val_1_rmse: 0.79982 |  0:00:15s
epoch 61 | loss: 0.33785 | val_0_rmse: 0.63671 | val_1_rmse: 0.76137 |  0:00:15s
epoch 62 | loss: 0.34812 | val_0_rmse: 0.60073 | val_1_rmse: 0.73758 |  0:00:16s
epoch 63 | loss: 0.34267 | val_0_rmse: 0.58975 | val_1_rmse: 0.70301 |  0:00:16s
epoch 64 | loss: 0.32842 | val_0_rmse: 0.59211 | val_1_rmse: 0.69601 |  0:00:16s
epoch 65 | loss: 0.329   | val_0_rmse: 0.59899 | val_1_rmse: 0.70309 |  0:00:16s
epoch 66 | loss: 0.33348 | val_0_rmse: 0.60762 | val_1_rmse: 0.70811 |  0:00:17s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.68465
Best weights from best epoch are automatically used!
ended training at: 02:54:19
Feature importance:
[('Area', 0.27644645756676467), ('Baths', 0.16819947315071232), ('Beds', 0.03443265109961113), ('Latitude', 0.2813930367340523), ('Longitude', 0.07701656847670411), ('Month', 0.08641403707999641), ('Year', 0.07609777589215903)]
Mean squared error is of 3393317683.9419255
Mean absolute error:39978.715903193675
MAPE:0.4160445983964903
R2 score:0.5374894717740129
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:54:19
epoch 0  | loss: 1.64423 | val_0_rmse: 1.28356 | val_1_rmse: 1.39475 |  0:00:00s
epoch 1  | loss: 0.79798 | val_0_rmse: 1.06559 | val_1_rmse: 1.09294 |  0:00:00s
epoch 2  | loss: 0.63148 | val_0_rmse: 1.09502 | val_1_rmse: 1.26904 |  0:00:00s
epoch 3  | loss: 0.66222 | val_0_rmse: 1.01004 | val_1_rmse: 1.02435 |  0:00:00s
epoch 4  | loss: 0.57808 | val_0_rmse: 0.7774  | val_1_rmse: 0.8146  |  0:00:01s
epoch 5  | loss: 0.55363 | val_0_rmse: 0.80461 | val_1_rmse: 0.91156 |  0:00:01s
epoch 6  | loss: 0.54278 | val_0_rmse: 0.83949 | val_1_rmse: 0.92285 |  0:00:01s
epoch 7  | loss: 0.51672 | val_0_rmse: 0.7326  | val_1_rmse: 0.8032  |  0:00:02s
epoch 8  | loss: 0.49849 | val_0_rmse: 0.69959 | val_1_rmse: 0.78887 |  0:00:02s
epoch 9  | loss: 0.50735 | val_0_rmse: 0.69719 | val_1_rmse: 0.79118 |  0:00:02s
epoch 10 | loss: 0.47402 | val_0_rmse: 0.71004 | val_1_rmse: 0.80199 |  0:00:02s
epoch 11 | loss: 0.46368 | val_0_rmse: 0.69735 | val_1_rmse: 0.78863 |  0:00:03s
epoch 12 | loss: 0.46351 | val_0_rmse: 0.67278 | val_1_rmse: 0.759   |  0:00:03s
epoch 13 | loss: 0.47064 | val_0_rmse: 0.6666  | val_1_rmse: 0.7497  |  0:00:03s
epoch 14 | loss: 0.46229 | val_0_rmse: 0.66831 | val_1_rmse: 0.75884 |  0:00:03s
epoch 15 | loss: 0.45901 | val_0_rmse: 0.68072 | val_1_rmse: 0.75939 |  0:00:04s
epoch 16 | loss: 0.46299 | val_0_rmse: 0.68209 | val_1_rmse: 0.76308 |  0:00:04s
epoch 17 | loss: 0.46928 | val_0_rmse: 0.66491 | val_1_rmse: 0.75674 |  0:00:04s
epoch 18 | loss: 0.44648 | val_0_rmse: 0.67693 | val_1_rmse: 0.74485 |  0:00:04s
epoch 19 | loss: 0.45102 | val_0_rmse: 0.6771  | val_1_rmse: 0.73821 |  0:00:05s
epoch 20 | loss: 0.44527 | val_0_rmse: 0.67958 | val_1_rmse: 0.7343  |  0:00:05s
epoch 21 | loss: 0.45184 | val_0_rmse: 0.68485 | val_1_rmse: 0.7427  |  0:00:05s
epoch 22 | loss: 0.44436 | val_0_rmse: 0.66075 | val_1_rmse: 0.72611 |  0:00:05s
epoch 23 | loss: 0.44863 | val_0_rmse: 0.65306 | val_1_rmse: 0.72909 |  0:00:06s
epoch 24 | loss: 0.4193  | val_0_rmse: 0.64726 | val_1_rmse: 0.72178 |  0:00:06s
epoch 25 | loss: 0.42267 | val_0_rmse: 0.65287 | val_1_rmse: 0.72136 |  0:00:06s
epoch 26 | loss: 0.43631 | val_0_rmse: 0.64057 | val_1_rmse: 0.70245 |  0:00:06s
epoch 27 | loss: 0.41698 | val_0_rmse: 0.66089 | val_1_rmse: 0.72116 |  0:00:07s
epoch 28 | loss: 0.44335 | val_0_rmse: 0.64362 | val_1_rmse: 0.70638 |  0:00:07s
epoch 29 | loss: 0.42328 | val_0_rmse: 0.63929 | val_1_rmse: 0.71305 |  0:00:07s
epoch 30 | loss: 0.42355 | val_0_rmse: 0.6414  | val_1_rmse: 0.71716 |  0:00:07s
epoch 31 | loss: 0.42725 | val_0_rmse: 0.64325 | val_1_rmse: 0.72133 |  0:00:08s
epoch 32 | loss: 0.43484 | val_0_rmse: 0.64098 | val_1_rmse: 0.71214 |  0:00:08s
epoch 33 | loss: 0.40748 | val_0_rmse: 0.69831 | val_1_rmse: 0.80897 |  0:00:08s
epoch 34 | loss: 0.43405 | val_0_rmse: 0.68453 | val_1_rmse: 0.76965 |  0:00:08s
epoch 35 | loss: 0.43072 | val_0_rmse: 0.64914 | val_1_rmse: 0.73255 |  0:00:09s
epoch 36 | loss: 0.41818 | val_0_rmse: 0.64607 | val_1_rmse: 0.71518 |  0:00:09s
epoch 37 | loss: 0.40154 | val_0_rmse: 0.65185 | val_1_rmse: 0.68881 |  0:00:09s
epoch 38 | loss: 0.42418 | val_0_rmse: 0.65031 | val_1_rmse: 0.7001  |  0:00:09s
epoch 39 | loss: 0.41503 | val_0_rmse: 0.63374 | val_1_rmse: 0.68404 |  0:00:10s
epoch 40 | loss: 0.39712 | val_0_rmse: 0.62354 | val_1_rmse: 0.68563 |  0:00:10s
epoch 41 | loss: 0.41013 | val_0_rmse: 0.62157 | val_1_rmse: 0.69404 |  0:00:10s
epoch 42 | loss: 0.40197 | val_0_rmse: 0.62615 | val_1_rmse: 0.69731 |  0:00:10s
epoch 43 | loss: 0.39751 | val_0_rmse: 0.62693 | val_1_rmse: 0.70543 |  0:00:11s
epoch 44 | loss: 0.39374 | val_0_rmse: 0.62478 | val_1_rmse: 0.70268 |  0:00:11s
epoch 45 | loss: 0.40207 | val_0_rmse: 0.63119 | val_1_rmse: 0.71828 |  0:00:11s
epoch 46 | loss: 0.3963  | val_0_rmse: 0.62241 | val_1_rmse: 0.69907 |  0:00:11s
epoch 47 | loss: 0.38548 | val_0_rmse: 0.62222 | val_1_rmse: 0.70385 |  0:00:12s
epoch 48 | loss: 0.39474 | val_0_rmse: 0.63754 | val_1_rmse: 0.72322 |  0:00:12s
epoch 49 | loss: 0.40363 | val_0_rmse: 0.64312 | val_1_rmse: 0.72835 |  0:00:12s
epoch 50 | loss: 0.40739 | val_0_rmse: 0.63705 | val_1_rmse: 0.70154 |  0:00:12s
epoch 51 | loss: 0.40565 | val_0_rmse: 0.62851 | val_1_rmse: 0.69024 |  0:00:13s
epoch 52 | loss: 0.38344 | val_0_rmse: 0.63114 | val_1_rmse: 0.69573 |  0:00:13s
epoch 53 | loss: 0.40881 | val_0_rmse: 0.62143 | val_1_rmse: 0.70807 |  0:00:13s
epoch 54 | loss: 0.38505 | val_0_rmse: 0.62723 | val_1_rmse: 0.71953 |  0:00:13s
epoch 55 | loss: 0.39502 | val_0_rmse: 0.63635 | val_1_rmse: 0.72377 |  0:00:14s
epoch 56 | loss: 0.37758 | val_0_rmse: 0.63386 | val_1_rmse: 0.71565 |  0:00:14s
epoch 57 | loss: 0.39003 | val_0_rmse: 0.64081 | val_1_rmse: 0.69534 |  0:00:14s
epoch 58 | loss: 0.40069 | val_0_rmse: 0.63062 | val_1_rmse: 0.67705 |  0:00:14s
epoch 59 | loss: 0.39347 | val_0_rmse: 0.64456 | val_1_rmse: 0.68626 |  0:00:15s
epoch 60 | loss: 0.41316 | val_0_rmse: 0.65621 | val_1_rmse: 0.69544 |  0:00:15s
epoch 61 | loss: 0.40955 | val_0_rmse: 0.65512 | val_1_rmse: 0.70072 |  0:00:15s
epoch 62 | loss: 0.40599 | val_0_rmse: 0.64382 | val_1_rmse: 0.67951 |  0:00:15s
epoch 63 | loss: 0.41093 | val_0_rmse: 0.6278  | val_1_rmse: 0.67365 |  0:00:16s
epoch 64 | loss: 0.40721 | val_0_rmse: 0.63178 | val_1_rmse: 0.68227 |  0:00:16s
epoch 65 | loss: 0.39217 | val_0_rmse: 0.6546  | val_1_rmse: 0.70511 |  0:00:16s
epoch 66 | loss: 0.39742 | val_0_rmse: 0.64428 | val_1_rmse: 0.72    |  0:00:16s
epoch 67 | loss: 0.39694 | val_0_rmse: 0.68589 | val_1_rmse: 0.75855 |  0:00:17s
epoch 68 | loss: 0.46006 | val_0_rmse: 0.69902 | val_1_rmse: 0.77341 |  0:00:17s
epoch 69 | loss: 0.47595 | val_0_rmse: 0.69877 | val_1_rmse: 0.76051 |  0:00:17s
epoch 70 | loss: 0.45371 | val_0_rmse: 0.67133 | val_1_rmse: 0.74072 |  0:00:17s
epoch 71 | loss: 0.43139 | val_0_rmse: 0.65666 | val_1_rmse: 0.70945 |  0:00:18s
epoch 72 | loss: 0.44309 | val_0_rmse: 0.65424 | val_1_rmse: 0.71764 |  0:00:18s
epoch 73 | loss: 0.41729 | val_0_rmse: 0.65453 | val_1_rmse: 0.7267  |  0:00:18s
epoch 74 | loss: 0.43055 | val_0_rmse: 0.65808 | val_1_rmse: 0.73282 |  0:00:18s
epoch 75 | loss: 0.42889 | val_0_rmse: 0.65266 | val_1_rmse: 0.71488 |  0:00:19s
epoch 76 | loss: 0.41752 | val_0_rmse: 0.66263 | val_1_rmse: 0.70944 |  0:00:19s
epoch 77 | loss: 0.43252 | val_0_rmse: 0.68313 | val_1_rmse: 0.72625 |  0:00:19s
epoch 78 | loss: 0.41512 | val_0_rmse: 0.66138 | val_1_rmse: 0.72874 |  0:00:19s
epoch 79 | loss: 0.42813 | val_0_rmse: 0.63236 | val_1_rmse: 0.70731 |  0:00:20s
epoch 80 | loss: 0.40268 | val_0_rmse: 0.6332  | val_1_rmse: 0.70013 |  0:00:20s
epoch 81 | loss: 0.40603 | val_0_rmse: 0.64426 | val_1_rmse: 0.71714 |  0:00:20s
epoch 82 | loss: 0.40862 | val_0_rmse: 0.6471  | val_1_rmse: 0.7217  |  0:00:20s
epoch 83 | loss: 0.40441 | val_0_rmse: 0.64618 | val_1_rmse: 0.72231 |  0:00:21s
epoch 84 | loss: 0.4122  | val_0_rmse: 0.652   | val_1_rmse: 0.72176 |  0:00:21s
epoch 85 | loss: 0.41188 | val_0_rmse: 0.66091 | val_1_rmse: 0.73012 |  0:00:21s
epoch 86 | loss: 0.41264 | val_0_rmse: 0.67445 | val_1_rmse: 0.74543 |  0:00:21s
epoch 87 | loss: 0.40127 | val_0_rmse: 0.69131 | val_1_rmse: 0.76714 |  0:00:22s
epoch 88 | loss: 0.4041  | val_0_rmse: 0.67663 | val_1_rmse: 0.76672 |  0:00:22s
epoch 89 | loss: 0.40361 | val_0_rmse: 0.63034 | val_1_rmse: 0.72624 |  0:00:22s
epoch 90 | loss: 0.3913  | val_0_rmse: 0.62206 | val_1_rmse: 0.71976 |  0:00:22s
epoch 91 | loss: 0.39738 | val_0_rmse: 0.62907 | val_1_rmse: 0.72972 |  0:00:23s
epoch 92 | loss: 0.39962 | val_0_rmse: 0.63728 | val_1_rmse: 0.72622 |  0:00:23s
epoch 93 | loss: 0.38706 | val_0_rmse: 0.63312 | val_1_rmse: 0.71586 |  0:00:23s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.67365
Best weights from best epoch are automatically used!
ended training at: 02:54:43
Feature importance:
[('Area', 0.30583459919913086), ('Baths', 0.2941164479148429), ('Beds', 0.030026751281352758), ('Latitude', 0.18616616691041543), ('Longitude', 0.03654170197159578), ('Month', 0.10010473142057007), ('Year', 0.04720960130209222)]
Mean squared error is of 3416300938.790087
Mean absolute error:41394.69432802198
MAPE:0.34222118925293976
R2 score:0.6380758621363325
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:54:43
epoch 0  | loss: 1.76741 | val_0_rmse: 1.81028 | val_1_rmse: 1.70683 |  0:00:00s
epoch 1  | loss: 0.66301 | val_0_rmse: 1.53676 | val_1_rmse: 1.46117 |  0:00:00s
epoch 2  | loss: 0.47347 | val_0_rmse: 1.1087  | val_1_rmse: 1.06577 |  0:00:00s
epoch 3  | loss: 0.41655 | val_0_rmse: 1.09654 | val_1_rmse: 1.09588 |  0:00:01s
epoch 4  | loss: 0.42232 | val_0_rmse: 0.87259 | val_1_rmse: 0.87118 |  0:00:01s
epoch 5  | loss: 0.39487 | val_0_rmse: 0.84885 | val_1_rmse: 0.8393  |  0:00:01s
epoch 6  | loss: 0.36769 | val_0_rmse: 0.78855 | val_1_rmse: 0.78995 |  0:00:01s
epoch 7  | loss: 0.37402 | val_0_rmse: 0.65323 | val_1_rmse: 0.66927 |  0:00:02s
epoch 8  | loss: 0.3643  | val_0_rmse: 0.6289  | val_1_rmse: 0.6521  |  0:00:02s
epoch 9  | loss: 0.36061 | val_0_rmse: 0.63699 | val_1_rmse: 0.67245 |  0:00:02s
epoch 10 | loss: 0.34901 | val_0_rmse: 0.64688 | val_1_rmse: 0.67602 |  0:00:02s
epoch 11 | loss: 0.34384 | val_0_rmse: 0.64438 | val_1_rmse: 0.6895  |  0:00:03s
epoch 12 | loss: 0.34555 | val_0_rmse: 0.61766 | val_1_rmse: 0.65952 |  0:00:03s
epoch 13 | loss: 0.34243 | val_0_rmse: 0.61364 | val_1_rmse: 0.65267 |  0:00:03s
epoch 14 | loss: 0.35264 | val_0_rmse: 0.59462 | val_1_rmse: 0.64001 |  0:00:03s
epoch 15 | loss: 0.3431  | val_0_rmse: 0.58309 | val_1_rmse: 0.63376 |  0:00:04s
epoch 16 | loss: 0.33462 | val_0_rmse: 0.58518 | val_1_rmse: 0.63002 |  0:00:04s
epoch 17 | loss: 0.33861 | val_0_rmse: 0.58641 | val_1_rmse: 0.62337 |  0:00:04s
epoch 18 | loss: 0.32886 | val_0_rmse: 0.57733 | val_1_rmse: 0.60441 |  0:00:04s
epoch 19 | loss: 0.33427 | val_0_rmse: 0.57837 | val_1_rmse: 0.6075  |  0:00:05s
epoch 20 | loss: 0.33409 | val_0_rmse: 0.57918 | val_1_rmse: 0.61131 |  0:00:05s
epoch 21 | loss: 0.33909 | val_0_rmse: 0.58211 | val_1_rmse: 0.61381 |  0:00:05s
epoch 22 | loss: 0.32885 | val_0_rmse: 0.57433 | val_1_rmse: 0.61207 |  0:00:05s
epoch 23 | loss: 0.33039 | val_0_rmse: 0.56859 | val_1_rmse: 0.60571 |  0:00:06s
epoch 24 | loss: 0.33006 | val_0_rmse: 0.56809 | val_1_rmse: 0.60662 |  0:00:06s
epoch 25 | loss: 0.33157 | val_0_rmse: 0.56746 | val_1_rmse: 0.61342 |  0:00:06s
epoch 26 | loss: 0.32684 | val_0_rmse: 0.56119 | val_1_rmse: 0.61024 |  0:00:06s
epoch 27 | loss: 0.32993 | val_0_rmse: 0.55978 | val_1_rmse: 0.6055  |  0:00:07s
epoch 28 | loss: 0.32176 | val_0_rmse: 0.55914 | val_1_rmse: 0.60717 |  0:00:07s
epoch 29 | loss: 0.3321  | val_0_rmse: 0.57115 | val_1_rmse: 0.61417 |  0:00:07s
epoch 30 | loss: 0.32207 | val_0_rmse: 0.56498 | val_1_rmse: 0.61029 |  0:00:07s
epoch 31 | loss: 0.32535 | val_0_rmse: 0.55345 | val_1_rmse: 0.60854 |  0:00:08s
epoch 32 | loss: 0.31705 | val_0_rmse: 0.55529 | val_1_rmse: 0.61701 |  0:00:08s
epoch 33 | loss: 0.32453 | val_0_rmse: 0.57013 | val_1_rmse: 0.61862 |  0:00:08s
epoch 34 | loss: 0.31898 | val_0_rmse: 0.56798 | val_1_rmse: 0.61246 |  0:00:08s
epoch 35 | loss: 0.33552 | val_0_rmse: 0.56334 | val_1_rmse: 0.61503 |  0:00:09s
epoch 36 | loss: 0.32729 | val_0_rmse: 0.56283 | val_1_rmse: 0.61224 |  0:00:09s
epoch 37 | loss: 0.3297  | val_0_rmse: 0.55744 | val_1_rmse: 0.5988  |  0:00:09s
epoch 38 | loss: 0.32717 | val_0_rmse: 0.55795 | val_1_rmse: 0.60634 |  0:00:09s
epoch 39 | loss: 0.31375 | val_0_rmse: 0.55902 | val_1_rmse: 0.60565 |  0:00:10s
epoch 40 | loss: 0.32339 | val_0_rmse: 0.56174 | val_1_rmse: 0.60571 |  0:00:10s
epoch 41 | loss: 0.31378 | val_0_rmse: 0.55903 | val_1_rmse: 0.60344 |  0:00:10s
epoch 42 | loss: 0.31924 | val_0_rmse: 0.55415 | val_1_rmse: 0.5983  |  0:00:10s
epoch 43 | loss: 0.3268  | val_0_rmse: 0.55312 | val_1_rmse: 0.59955 |  0:00:11s
epoch 44 | loss: 0.30839 | val_0_rmse: 0.55444 | val_1_rmse: 0.59871 |  0:00:11s
epoch 45 | loss: 0.31666 | val_0_rmse: 0.54902 | val_1_rmse: 0.59388 |  0:00:11s
epoch 46 | loss: 0.31252 | val_0_rmse: 0.54806 | val_1_rmse: 0.6007  |  0:00:11s
epoch 47 | loss: 0.32266 | val_0_rmse: 0.55444 | val_1_rmse: 0.60883 |  0:00:12s
epoch 48 | loss: 0.30836 | val_0_rmse: 0.55272 | val_1_rmse: 0.60774 |  0:00:12s
epoch 49 | loss: 0.31536 | val_0_rmse: 0.54421 | val_1_rmse: 0.60105 |  0:00:12s
epoch 50 | loss: 0.3073  | val_0_rmse: 0.5438  | val_1_rmse: 0.60172 |  0:00:12s
epoch 51 | loss: 0.30727 | val_0_rmse: 0.5502  | val_1_rmse: 0.61242 |  0:00:13s
epoch 52 | loss: 0.30397 | val_0_rmse: 0.54956 | val_1_rmse: 0.60669 |  0:00:13s
epoch 53 | loss: 0.30696 | val_0_rmse: 0.5465  | val_1_rmse: 0.61213 |  0:00:13s
epoch 54 | loss: 0.31028 | val_0_rmse: 0.54538 | val_1_rmse: 0.61534 |  0:00:13s
epoch 55 | loss: 0.30593 | val_0_rmse: 0.54428 | val_1_rmse: 0.61798 |  0:00:14s
epoch 56 | loss: 0.30196 | val_0_rmse: 0.53963 | val_1_rmse: 0.62121 |  0:00:14s
epoch 57 | loss: 0.29958 | val_0_rmse: 0.53943 | val_1_rmse: 0.62311 |  0:00:14s
epoch 58 | loss: 0.30706 | val_0_rmse: 0.53817 | val_1_rmse: 0.61763 |  0:00:14s
epoch 59 | loss: 0.30103 | val_0_rmse: 0.5396  | val_1_rmse: 0.61426 |  0:00:15s
epoch 60 | loss: 0.30471 | val_0_rmse: 0.53793 | val_1_rmse: 0.61818 |  0:00:15s
epoch 61 | loss: 0.30207 | val_0_rmse: 0.5386  | val_1_rmse: 0.62235 |  0:00:15s
epoch 62 | loss: 0.30432 | val_0_rmse: 0.53952 | val_1_rmse: 0.62175 |  0:00:15s
epoch 63 | loss: 0.30809 | val_0_rmse: 0.55411 | val_1_rmse: 0.62265 |  0:00:16s
epoch 64 | loss: 0.31482 | val_0_rmse: 0.55362 | val_1_rmse: 0.62149 |  0:00:16s
epoch 65 | loss: 0.30504 | val_0_rmse: 0.54316 | val_1_rmse: 0.61656 |  0:00:16s
epoch 66 | loss: 0.30943 | val_0_rmse: 0.54112 | val_1_rmse: 0.61633 |  0:00:16s
epoch 67 | loss: 0.3087  | val_0_rmse: 0.54631 | val_1_rmse: 0.61564 |  0:00:16s
epoch 68 | loss: 0.30562 | val_0_rmse: 0.54985 | val_1_rmse: 0.62108 |  0:00:17s
epoch 69 | loss: 0.30864 | val_0_rmse: 0.55223 | val_1_rmse: 0.61812 |  0:00:17s
epoch 70 | loss: 0.3079  | val_0_rmse: 0.55433 | val_1_rmse: 0.62706 |  0:00:17s
epoch 71 | loss: 0.31038 | val_0_rmse: 0.55403 | val_1_rmse: 0.62917 |  0:00:17s
epoch 72 | loss: 0.30612 | val_0_rmse: 0.54197 | val_1_rmse: 0.61848 |  0:00:18s
epoch 73 | loss: 0.30473 | val_0_rmse: 0.54487 | val_1_rmse: 0.61415 |  0:00:18s
epoch 74 | loss: 0.30542 | val_0_rmse: 0.54267 | val_1_rmse: 0.61165 |  0:00:18s
epoch 75 | loss: 0.30166 | val_0_rmse: 0.55073 | val_1_rmse: 0.61352 |  0:00:18s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.59388
Best weights from best epoch are automatically used!
ended training at: 02:55:02
Feature importance:
[('Area', 0.44078217390647056), ('Baths', 0.023731849453127573), ('Beds', 0.20643185410164788), ('Latitude', 0.028146303317509055), ('Longitude', 0.11290642732907351), ('Month', 0.04239997606528947), ('Year', 0.145601415826882)]
Mean squared error is of 1045962446.6295494
Mean absolute error:22565.227803700203
MAPE:0.377884454061938
R2 score:0.6603470487002998
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:55:03
epoch 0  | loss: 1.70673 | val_0_rmse: 1.52095 | val_1_rmse: 1.60516 |  0:00:00s
epoch 1  | loss: 0.72738 | val_0_rmse: 1.4268  | val_1_rmse: 1.39968 |  0:00:00s
epoch 2  | loss: 0.5786  | val_0_rmse: 1.30816 | val_1_rmse: 1.39744 |  0:00:00s
epoch 3  | loss: 0.43317 | val_0_rmse: 1.10319 | val_1_rmse: 1.17093 |  0:00:01s
epoch 4  | loss: 0.42334 | val_0_rmse: 0.96941 | val_1_rmse: 1.02255 |  0:00:01s
epoch 5  | loss: 0.41018 | val_0_rmse: 0.74225 | val_1_rmse: 0.75565 |  0:00:01s
epoch 6  | loss: 0.38696 | val_0_rmse: 0.66606 | val_1_rmse: 0.6629  |  0:00:01s
epoch 7  | loss: 0.38234 | val_0_rmse: 0.68778 | val_1_rmse: 0.69943 |  0:00:02s
epoch 8  | loss: 0.3851  | val_0_rmse: 0.67752 | val_1_rmse: 0.68959 |  0:00:02s
epoch 9  | loss: 0.3884  | val_0_rmse: 0.6212  | val_1_rmse: 0.61135 |  0:00:02s
epoch 10 | loss: 0.37286 | val_0_rmse: 0.60883 | val_1_rmse: 0.58668 |  0:00:02s
epoch 11 | loss: 0.37008 | val_0_rmse: 0.61071 | val_1_rmse: 0.58984 |  0:00:03s
epoch 12 | loss: 0.37904 | val_0_rmse: 0.62674 | val_1_rmse: 0.60876 |  0:00:03s
epoch 13 | loss: 0.38226 | val_0_rmse: 0.61002 | val_1_rmse: 0.59973 |  0:00:03s
epoch 14 | loss: 0.36441 | val_0_rmse: 0.59987 | val_1_rmse: 0.57814 |  0:00:03s
epoch 15 | loss: 0.37366 | val_0_rmse: 0.59992 | val_1_rmse: 0.57961 |  0:00:04s
epoch 16 | loss: 0.36664 | val_0_rmse: 0.59259 | val_1_rmse: 0.56946 |  0:00:04s
epoch 17 | loss: 0.36108 | val_0_rmse: 0.59378 | val_1_rmse: 0.55998 |  0:00:04s
epoch 18 | loss: 0.35993 | val_0_rmse: 0.59718 | val_1_rmse: 0.56362 |  0:00:04s
epoch 19 | loss: 0.35859 | val_0_rmse: 0.59833 | val_1_rmse: 0.56729 |  0:00:05s
epoch 20 | loss: 0.3555  | val_0_rmse: 0.59789 | val_1_rmse: 0.56115 |  0:00:05s
epoch 21 | loss: 0.35611 | val_0_rmse: 0.59212 | val_1_rmse: 0.55161 |  0:00:05s
epoch 22 | loss: 0.36131 | val_0_rmse: 0.59094 | val_1_rmse: 0.5563  |  0:00:05s
epoch 23 | loss: 0.34834 | val_0_rmse: 0.60845 | val_1_rmse: 0.58506 |  0:00:06s
epoch 24 | loss: 0.35424 | val_0_rmse: 0.58747 | val_1_rmse: 0.55215 |  0:00:06s
epoch 25 | loss: 0.35398 | val_0_rmse: 0.59427 | val_1_rmse: 0.56164 |  0:00:06s
epoch 26 | loss: 0.35833 | val_0_rmse: 0.59138 | val_1_rmse: 0.56527 |  0:00:06s
epoch 27 | loss: 0.36035 | val_0_rmse: 0.60253 | val_1_rmse: 0.57743 |  0:00:07s
epoch 28 | loss: 0.354   | val_0_rmse: 0.6183  | val_1_rmse: 0.57598 |  0:00:07s
epoch 29 | loss: 0.35729 | val_0_rmse: 0.59687 | val_1_rmse: 0.55465 |  0:00:07s
epoch 30 | loss: 0.35498 | val_0_rmse: 0.58844 | val_1_rmse: 0.55451 |  0:00:07s
epoch 31 | loss: 0.35015 | val_0_rmse: 0.59922 | val_1_rmse: 0.56311 |  0:00:08s
epoch 32 | loss: 0.35774 | val_0_rmse: 0.59373 | val_1_rmse: 0.55822 |  0:00:08s
epoch 33 | loss: 0.33545 | val_0_rmse: 0.58408 | val_1_rmse: 0.55121 |  0:00:08s
epoch 34 | loss: 0.34386 | val_0_rmse: 0.5796  | val_1_rmse: 0.5478  |  0:00:08s
epoch 35 | loss: 0.34201 | val_0_rmse: 0.57764 | val_1_rmse: 0.55423 |  0:00:09s
epoch 36 | loss: 0.34698 | val_0_rmse: 0.57649 | val_1_rmse: 0.56115 |  0:00:09s
epoch 37 | loss: 0.34615 | val_0_rmse: 0.57545 | val_1_rmse: 0.55869 |  0:00:09s
epoch 38 | loss: 0.33834 | val_0_rmse: 0.5736  | val_1_rmse: 0.55726 |  0:00:09s
epoch 39 | loss: 0.34191 | val_0_rmse: 0.57921 | val_1_rmse: 0.55661 |  0:00:10s
epoch 40 | loss: 0.33999 | val_0_rmse: 0.57989 | val_1_rmse: 0.55664 |  0:00:10s
epoch 41 | loss: 0.3495  | val_0_rmse: 0.57844 | val_1_rmse: 0.5533  |  0:00:10s
epoch 42 | loss: 0.34118 | val_0_rmse: 0.581   | val_1_rmse: 0.55835 |  0:00:10s
epoch 43 | loss: 0.34603 | val_0_rmse: 0.57505 | val_1_rmse: 0.55467 |  0:00:11s
epoch 44 | loss: 0.35098 | val_0_rmse: 0.57834 | val_1_rmse: 0.55291 |  0:00:11s
epoch 45 | loss: 0.34807 | val_0_rmse: 0.57362 | val_1_rmse: 0.55366 |  0:00:11s
epoch 46 | loss: 0.35516 | val_0_rmse: 0.57611 | val_1_rmse: 0.56244 |  0:00:11s
epoch 47 | loss: 0.34537 | val_0_rmse: 0.5732  | val_1_rmse: 0.55288 |  0:00:12s
epoch 48 | loss: 0.34328 | val_0_rmse: 0.57183 | val_1_rmse: 0.55004 |  0:00:12s
epoch 49 | loss: 0.34612 | val_0_rmse: 0.57371 | val_1_rmse: 0.56531 |  0:00:12s
epoch 50 | loss: 0.33541 | val_0_rmse: 0.57183 | val_1_rmse: 0.55599 |  0:00:12s
epoch 51 | loss: 0.33062 | val_0_rmse: 0.57193 | val_1_rmse: 0.55658 |  0:00:13s
epoch 52 | loss: 0.33063 | val_0_rmse: 0.56862 | val_1_rmse: 0.55904 |  0:00:13s
epoch 53 | loss: 0.33374 | val_0_rmse: 0.57197 | val_1_rmse: 0.56564 |  0:00:13s
epoch 54 | loss: 0.33491 | val_0_rmse: 0.57872 | val_1_rmse: 0.56425 |  0:00:13s
epoch 55 | loss: 0.33886 | val_0_rmse: 0.58736 | val_1_rmse: 0.55593 |  0:00:13s
epoch 56 | loss: 0.34043 | val_0_rmse: 0.58225 | val_1_rmse: 0.54738 |  0:00:14s
epoch 57 | loss: 0.35499 | val_0_rmse: 0.59005 | val_1_rmse: 0.55348 |  0:00:14s
epoch 58 | loss: 0.34725 | val_0_rmse: 0.58213 | val_1_rmse: 0.55056 |  0:00:14s
epoch 59 | loss: 0.33974 | val_0_rmse: 0.58253 | val_1_rmse: 0.55548 |  0:00:15s
epoch 60 | loss: 0.33353 | val_0_rmse: 0.57953 | val_1_rmse: 0.55814 |  0:00:15s
epoch 61 | loss: 0.33917 | val_0_rmse: 0.57741 | val_1_rmse: 0.56056 |  0:00:15s
epoch 62 | loss: 0.33421 | val_0_rmse: 0.5765  | val_1_rmse: 0.55399 |  0:00:15s
epoch 63 | loss: 0.33014 | val_0_rmse: 0.57037 | val_1_rmse: 0.54715 |  0:00:16s
epoch 64 | loss: 0.34618 | val_0_rmse: 0.56707 | val_1_rmse: 0.55003 |  0:00:16s
epoch 65 | loss: 0.33614 | val_0_rmse: 0.57019 | val_1_rmse: 0.5613  |  0:00:16s
epoch 66 | loss: 0.34296 | val_0_rmse: 0.57431 | val_1_rmse: 0.56092 |  0:00:16s
epoch 67 | loss: 0.34906 | val_0_rmse: 0.58127 | val_1_rmse: 0.55668 |  0:00:16s
epoch 68 | loss: 0.35781 | val_0_rmse: 0.57989 | val_1_rmse: 0.55306 |  0:00:17s
epoch 69 | loss: 0.34005 | val_0_rmse: 0.57021 | val_1_rmse: 0.55324 |  0:00:17s
epoch 70 | loss: 0.3486  | val_0_rmse: 0.57145 | val_1_rmse: 0.54793 |  0:00:17s
epoch 71 | loss: 0.34143 | val_0_rmse: 0.59155 | val_1_rmse: 0.55391 |  0:00:17s
epoch 72 | loss: 0.3448  | val_0_rmse: 0.57351 | val_1_rmse: 0.55418 |  0:00:18s
epoch 73 | loss: 0.33565 | val_0_rmse: 0.57194 | val_1_rmse: 0.54842 |  0:00:18s
epoch 74 | loss: 0.33551 | val_0_rmse: 0.5812  | val_1_rmse: 0.55415 |  0:00:18s
epoch 75 | loss: 0.33382 | val_0_rmse: 0.57024 | val_1_rmse: 0.55988 |  0:00:18s
epoch 76 | loss: 0.32901 | val_0_rmse: 0.57276 | val_1_rmse: 0.57064 |  0:00:19s
epoch 77 | loss: 0.33018 | val_0_rmse: 0.56391 | val_1_rmse: 0.5574  |  0:00:19s
epoch 78 | loss: 0.33193 | val_0_rmse: 0.56847 | val_1_rmse: 0.5633  |  0:00:19s
epoch 79 | loss: 0.3414  | val_0_rmse: 0.56623 | val_1_rmse: 0.56141 |  0:00:20s
epoch 80 | loss: 0.33141 | val_0_rmse: 0.56537 | val_1_rmse: 0.55752 |  0:00:20s
epoch 81 | loss: 0.33775 | val_0_rmse: 0.56609 | val_1_rmse: 0.5491  |  0:00:20s
epoch 82 | loss: 0.32828 | val_0_rmse: 0.56542 | val_1_rmse: 0.54454 |  0:00:20s
epoch 83 | loss: 0.32437 | val_0_rmse: 0.56312 | val_1_rmse: 0.54518 |  0:00:20s
epoch 84 | loss: 0.32747 | val_0_rmse: 0.5627  | val_1_rmse: 0.54476 |  0:00:21s
epoch 85 | loss: 0.31979 | val_0_rmse: 0.56067 | val_1_rmse: 0.54853 |  0:00:21s
epoch 86 | loss: 0.32771 | val_0_rmse: 0.55778 | val_1_rmse: 0.55738 |  0:00:21s
epoch 87 | loss: 0.32466 | val_0_rmse: 0.56287 | val_1_rmse: 0.56523 |  0:00:21s
epoch 88 | loss: 0.31356 | val_0_rmse: 0.56092 | val_1_rmse: 0.56588 |  0:00:22s
epoch 89 | loss: 0.32383 | val_0_rmse: 0.56103 | val_1_rmse: 0.55989 |  0:00:22s
epoch 90 | loss: 0.31682 | val_0_rmse: 0.55709 | val_1_rmse: 0.55011 |  0:00:22s
epoch 91 | loss: 0.32517 | val_0_rmse: 0.55803 | val_1_rmse: 0.54826 |  0:00:22s
epoch 92 | loss: 0.3215  | val_0_rmse: 0.56026 | val_1_rmse: 0.55604 |  0:00:23s
epoch 93 | loss: 0.3229  | val_0_rmse: 0.55653 | val_1_rmse: 0.56462 |  0:00:23s
epoch 94 | loss: 0.31776 | val_0_rmse: 0.55514 | val_1_rmse: 0.56433 |  0:00:23s
epoch 95 | loss: 0.32161 | val_0_rmse: 0.56422 | val_1_rmse: 0.55921 |  0:00:23s
epoch 96 | loss: 0.32372 | val_0_rmse: 0.56802 | val_1_rmse: 0.55854 |  0:00:24s
epoch 97 | loss: 0.32234 | val_0_rmse: 0.57641 | val_1_rmse: 0.56229 |  0:00:24s
epoch 98 | loss: 0.33312 | val_0_rmse: 0.55672 | val_1_rmse: 0.55456 |  0:00:24s
epoch 99 | loss: 0.31677 | val_0_rmse: 0.56932 | val_1_rmse: 0.55697 |  0:00:24s
epoch 100| loss: 0.33145 | val_0_rmse: 0.57878 | val_1_rmse: 0.55998 |  0:00:25s
epoch 101| loss: 0.35248 | val_0_rmse: 0.57758 | val_1_rmse: 0.54819 |  0:00:25s
epoch 102| loss: 0.34147 | val_0_rmse: 0.58485 | val_1_rmse: 0.55048 |  0:00:25s
epoch 103| loss: 0.35228 | val_0_rmse: 0.58379 | val_1_rmse: 0.55234 |  0:00:25s
epoch 104| loss: 0.35233 | val_0_rmse: 0.58424 | val_1_rmse: 0.55862 |  0:00:26s
epoch 105| loss: 0.34456 | val_0_rmse: 0.58163 | val_1_rmse: 0.55721 |  0:00:26s
epoch 106| loss: 0.34594 | val_0_rmse: 0.58055 | val_1_rmse: 0.55563 |  0:00:26s
epoch 107| loss: 0.34325 | val_0_rmse: 0.57651 | val_1_rmse: 0.5598  |  0:00:26s
epoch 108| loss: 0.34252 | val_0_rmse: 0.57457 | val_1_rmse: 0.55231 |  0:00:26s
epoch 109| loss: 0.33863 | val_0_rmse: 0.57462 | val_1_rmse: 0.5515  |  0:00:27s
epoch 110| loss: 0.32944 | val_0_rmse: 0.57724 | val_1_rmse: 0.56048 |  0:00:27s
epoch 111| loss: 0.34209 | val_0_rmse: 0.57401 | val_1_rmse: 0.55804 |  0:00:27s
epoch 112| loss: 0.33422 | val_0_rmse: 0.57392 | val_1_rmse: 0.55701 |  0:00:27s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.54454
Best weights from best epoch are automatically used!
ended training at: 02:55:31
Feature importance:
[('Area', 0.4460863920242508), ('Baths', 5.902048669002179e-05), ('Beds', 0.21218319388631024), ('Latitude', 0.015492174146789713), ('Longitude', 0.2677570597736191), ('Month', 0.012549436086973335), ('Year', 0.04587272359536681)]
Mean squared error is of 1014867349.2297798
Mean absolute error:22271.41863066621
MAPE:0.36847304490330657
R2 score:0.688263712703417
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:55:31
epoch 0  | loss: 1.73553 | val_0_rmse: 1.25075 | val_1_rmse: 1.09264 |  0:00:00s
epoch 1  | loss: 0.73146 | val_0_rmse: 0.92878 | val_1_rmse: 0.84973 |  0:00:00s
epoch 2  | loss: 0.55245 | val_0_rmse: 1.02768 | val_1_rmse: 0.89833 |  0:00:00s
epoch 3  | loss: 0.46719 | val_0_rmse: 1.07313 | val_1_rmse: 0.97659 |  0:00:01s
epoch 4  | loss: 0.4239  | val_0_rmse: 0.85432 | val_1_rmse: 0.80734 |  0:00:01s
epoch 5  | loss: 0.4076  | val_0_rmse: 0.788   | val_1_rmse: 0.71563 |  0:00:01s
epoch 6  | loss: 0.40775 | val_0_rmse: 0.76083 | val_1_rmse: 0.69821 |  0:00:01s
epoch 7  | loss: 0.39945 | val_0_rmse: 0.70809 | val_1_rmse: 0.67039 |  0:00:02s
epoch 8  | loss: 0.37856 | val_0_rmse: 0.67404 | val_1_rmse: 0.64902 |  0:00:02s
epoch 9  | loss: 0.37432 | val_0_rmse: 0.65294 | val_1_rmse: 0.62609 |  0:00:02s
epoch 10 | loss: 0.37584 | val_0_rmse: 0.63491 | val_1_rmse: 0.60744 |  0:00:02s
epoch 11 | loss: 0.36327 | val_0_rmse: 0.66385 | val_1_rmse: 0.62022 |  0:00:03s
epoch 12 | loss: 0.37463 | val_0_rmse: 0.67501 | val_1_rmse: 0.6319  |  0:00:03s
epoch 13 | loss: 0.36976 | val_0_rmse: 0.6639  | val_1_rmse: 0.6115  |  0:00:03s
epoch 14 | loss: 0.35848 | val_0_rmse: 0.6201  | val_1_rmse: 0.56646 |  0:00:03s
epoch 15 | loss: 0.35092 | val_0_rmse: 0.61193 | val_1_rmse: 0.55943 |  0:00:04s
epoch 16 | loss: 0.35034 | val_0_rmse: 0.61344 | val_1_rmse: 0.55435 |  0:00:04s
epoch 17 | loss: 0.35325 | val_0_rmse: 0.59926 | val_1_rmse: 0.54385 |  0:00:04s
epoch 18 | loss: 0.34903 | val_0_rmse: 0.59295 | val_1_rmse: 0.53001 |  0:00:04s
epoch 19 | loss: 0.33648 | val_0_rmse: 0.5859  | val_1_rmse: 0.52707 |  0:00:05s
epoch 20 | loss: 0.33493 | val_0_rmse: 0.59093 | val_1_rmse: 0.54431 |  0:00:05s
epoch 21 | loss: 0.34808 | val_0_rmse: 0.57918 | val_1_rmse: 0.54392 |  0:00:05s
epoch 22 | loss: 0.34712 | val_0_rmse: 0.57952 | val_1_rmse: 0.55059 |  0:00:05s
epoch 23 | loss: 0.34011 | val_0_rmse: 0.58373 | val_1_rmse: 0.55909 |  0:00:06s
epoch 24 | loss: 0.33374 | val_0_rmse: 0.59707 | val_1_rmse: 0.57972 |  0:00:06s
epoch 25 | loss: 0.34131 | val_0_rmse: 0.60099 | val_1_rmse: 0.58376 |  0:00:06s
epoch 26 | loss: 0.34294 | val_0_rmse: 0.57822 | val_1_rmse: 0.55141 |  0:00:06s
epoch 27 | loss: 0.3464  | val_0_rmse: 0.57573 | val_1_rmse: 0.54262 |  0:00:07s
epoch 28 | loss: 0.34306 | val_0_rmse: 0.57906 | val_1_rmse: 0.55547 |  0:00:07s
epoch 29 | loss: 0.33491 | val_0_rmse: 0.5848  | val_1_rmse: 0.56591 |  0:00:07s
epoch 30 | loss: 0.33694 | val_0_rmse: 0.57113 | val_1_rmse: 0.551   |  0:00:07s
epoch 31 | loss: 0.33106 | val_0_rmse: 0.57239 | val_1_rmse: 0.53913 |  0:00:08s
epoch 32 | loss: 0.33697 | val_0_rmse: 0.57016 | val_1_rmse: 0.53891 |  0:00:08s
epoch 33 | loss: 0.32618 | val_0_rmse: 0.56831 | val_1_rmse: 0.54553 |  0:00:08s
epoch 34 | loss: 0.32264 | val_0_rmse: 0.56054 | val_1_rmse: 0.53685 |  0:00:08s
epoch 35 | loss: 0.32879 | val_0_rmse: 0.56036 | val_1_rmse: 0.53219 |  0:00:09s
epoch 36 | loss: 0.31928 | val_0_rmse: 0.5604  | val_1_rmse: 0.53886 |  0:00:09s
epoch 37 | loss: 0.32309 | val_0_rmse: 0.55601 | val_1_rmse: 0.53611 |  0:00:09s
epoch 38 | loss: 0.32716 | val_0_rmse: 0.55668 | val_1_rmse: 0.53431 |  0:00:09s
epoch 39 | loss: 0.32996 | val_0_rmse: 0.5572  | val_1_rmse: 0.54792 |  0:00:10s
epoch 40 | loss: 0.32755 | val_0_rmse: 0.56743 | val_1_rmse: 0.5529  |  0:00:10s
epoch 41 | loss: 0.31883 | val_0_rmse: 0.55991 | val_1_rmse: 0.54099 |  0:00:10s
epoch 42 | loss: 0.31724 | val_0_rmse: 0.55843 | val_1_rmse: 0.53506 |  0:00:10s
epoch 43 | loss: 0.31682 | val_0_rmse: 0.55272 | val_1_rmse: 0.54037 |  0:00:11s
epoch 44 | loss: 0.31582 | val_0_rmse: 0.55301 | val_1_rmse: 0.52975 |  0:00:11s
epoch 45 | loss: 0.30952 | val_0_rmse: 0.56491 | val_1_rmse: 0.53137 |  0:00:11s
epoch 46 | loss: 0.31514 | val_0_rmse: 0.54899 | val_1_rmse: 0.52836 |  0:00:11s
epoch 47 | loss: 0.31234 | val_0_rmse: 0.54874 | val_1_rmse: 0.53697 |  0:00:12s
epoch 48 | loss: 0.31604 | val_0_rmse: 0.55632 | val_1_rmse: 0.53151 |  0:00:12s
epoch 49 | loss: 0.32394 | val_0_rmse: 0.55751 | val_1_rmse: 0.5443  |  0:00:12s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.52707
Best weights from best epoch are automatically used!
ended training at: 02:55:44
Feature importance:
[('Area', 0.3410250617415769), ('Baths', 0.14134279436574082), ('Beds', 0.11779573934236816), ('Latitude', 0.03007631761740792), ('Longitude', 0.07906878153608081), ('Month', 0.04487210345930668), ('Year', 0.24581920193751872)]
Mean squared error is of 1149789681.470539
Mean absolute error:23179.418414266398
MAPE:0.3933130492887464
R2 score:0.6632622847206555
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:55:44
epoch 0  | loss: 1.80521 | val_0_rmse: 2.08787 | val_1_rmse: 1.8639  |  0:00:00s
epoch 1  | loss: 0.77922 | val_0_rmse: 1.0128  | val_1_rmse: 0.93597 |  0:00:00s
epoch 2  | loss: 0.52283 | val_0_rmse: 1.15973 | val_1_rmse: 1.10421 |  0:00:00s
epoch 3  | loss: 0.4467  | val_0_rmse: 0.93881 | val_1_rmse: 0.90465 |  0:00:00s
epoch 4  | loss: 0.41232 | val_0_rmse: 0.87629 | val_1_rmse: 0.87522 |  0:00:01s
epoch 5  | loss: 0.4399  | val_0_rmse: 0.82615 | val_1_rmse: 0.83149 |  0:00:01s
epoch 6  | loss: 0.40257 | val_0_rmse: 0.80164 | val_1_rmse: 0.81014 |  0:00:01s
epoch 7  | loss: 0.37939 | val_0_rmse: 0.77397 | val_1_rmse: 0.80672 |  0:00:01s
epoch 8  | loss: 0.38474 | val_0_rmse: 0.73169 | val_1_rmse: 0.7642  |  0:00:02s
epoch 9  | loss: 0.37139 | val_0_rmse: 0.69284 | val_1_rmse: 0.73422 |  0:00:02s
epoch 10 | loss: 0.35588 | val_0_rmse: 0.67844 | val_1_rmse: 0.71102 |  0:00:02s
epoch 11 | loss: 0.36942 | val_0_rmse: 0.64769 | val_1_rmse: 0.67009 |  0:00:02s
epoch 12 | loss: 0.35401 | val_0_rmse: 0.62802 | val_1_rmse: 0.64706 |  0:00:03s
epoch 13 | loss: 0.34505 | val_0_rmse: 0.61463 | val_1_rmse: 0.63123 |  0:00:03s
epoch 14 | loss: 0.34461 | val_0_rmse: 0.6156  | val_1_rmse: 0.64313 |  0:00:03s
epoch 15 | loss: 0.35746 | val_0_rmse: 0.60131 | val_1_rmse: 0.61341 |  0:00:04s
epoch 16 | loss: 0.36115 | val_0_rmse: 0.60413 | val_1_rmse: 0.60941 |  0:00:04s
epoch 17 | loss: 0.35236 | val_0_rmse: 0.60326 | val_1_rmse: 0.61291 |  0:00:04s
epoch 18 | loss: 0.35665 | val_0_rmse: 0.58587 | val_1_rmse: 0.60049 |  0:00:04s
epoch 19 | loss: 0.35456 | val_0_rmse: 0.58042 | val_1_rmse: 0.59828 |  0:00:05s
epoch 20 | loss: 0.35293 | val_0_rmse: 0.57193 | val_1_rmse: 0.5924  |  0:00:05s
epoch 21 | loss: 0.34751 | val_0_rmse: 0.57258 | val_1_rmse: 0.59002 |  0:00:05s
epoch 22 | loss: 0.34991 | val_0_rmse: 0.57254 | val_1_rmse: 0.58921 |  0:00:05s
epoch 23 | loss: 0.33744 | val_0_rmse: 0.57157 | val_1_rmse: 0.57202 |  0:00:06s
epoch 24 | loss: 0.34308 | val_0_rmse: 0.57251 | val_1_rmse: 0.57055 |  0:00:06s
epoch 25 | loss: 0.34103 | val_0_rmse: 0.57184 | val_1_rmse: 0.57359 |  0:00:06s
epoch 26 | loss: 0.3383  | val_0_rmse: 0.57513 | val_1_rmse: 0.5726  |  0:00:06s
epoch 27 | loss: 0.34583 | val_0_rmse: 0.57309 | val_1_rmse: 0.57755 |  0:00:07s
epoch 28 | loss: 0.33857 | val_0_rmse: 0.5715  | val_1_rmse: 0.57931 |  0:00:07s
epoch 29 | loss: 0.33208 | val_0_rmse: 0.57409 | val_1_rmse: 0.59322 |  0:00:07s
epoch 30 | loss: 0.33713 | val_0_rmse: 0.57012 | val_1_rmse: 0.58162 |  0:00:07s
epoch 31 | loss: 0.33318 | val_0_rmse: 0.57407 | val_1_rmse: 0.57093 |  0:00:08s
epoch 32 | loss: 0.33324 | val_0_rmse: 0.5753  | val_1_rmse: 0.57681 |  0:00:08s
epoch 33 | loss: 0.33203 | val_0_rmse: 0.56492 | val_1_rmse: 0.57902 |  0:00:08s
epoch 34 | loss: 0.33016 | val_0_rmse: 0.56512 | val_1_rmse: 0.59225 |  0:00:08s
epoch 35 | loss: 0.32591 | val_0_rmse: 0.56432 | val_1_rmse: 0.59081 |  0:00:09s
epoch 36 | loss: 0.33319 | val_0_rmse: 0.56769 | val_1_rmse: 0.57418 |  0:00:09s
epoch 37 | loss: 0.34354 | val_0_rmse: 0.56395 | val_1_rmse: 0.57729 |  0:00:09s
epoch 38 | loss: 0.33367 | val_0_rmse: 0.56854 | val_1_rmse: 0.59517 |  0:00:09s
epoch 39 | loss: 0.32992 | val_0_rmse: 0.56211 | val_1_rmse: 0.56919 |  0:00:10s
epoch 40 | loss: 0.33378 | val_0_rmse: 0.56077 | val_1_rmse: 0.57494 |  0:00:10s
epoch 41 | loss: 0.31943 | val_0_rmse: 0.57155 | val_1_rmse: 0.60471 |  0:00:10s
epoch 42 | loss: 0.33955 | val_0_rmse: 0.55682 | val_1_rmse: 0.57842 |  0:00:10s
epoch 43 | loss: 0.31891 | val_0_rmse: 0.56987 | val_1_rmse: 0.56755 |  0:00:11s
epoch 44 | loss: 0.33089 | val_0_rmse: 0.55887 | val_1_rmse: 0.5722  |  0:00:11s
epoch 45 | loss: 0.31651 | val_0_rmse: 0.55599 | val_1_rmse: 0.57874 |  0:00:11s
epoch 46 | loss: 0.32305 | val_0_rmse: 0.55161 | val_1_rmse: 0.5645  |  0:00:11s
epoch 47 | loss: 0.32482 | val_0_rmse: 0.56606 | val_1_rmse: 0.56106 |  0:00:12s
epoch 48 | loss: 0.32656 | val_0_rmse: 0.56188 | val_1_rmse: 0.58233 |  0:00:12s
epoch 49 | loss: 0.32051 | val_0_rmse: 0.56641 | val_1_rmse: 0.59656 |  0:00:12s
epoch 50 | loss: 0.32961 | val_0_rmse: 0.55045 | val_1_rmse: 0.55868 |  0:00:12s
epoch 51 | loss: 0.31427 | val_0_rmse: 0.55605 | val_1_rmse: 0.55555 |  0:00:13s
epoch 52 | loss: 0.31686 | val_0_rmse: 0.55164 | val_1_rmse: 0.56113 |  0:00:13s
epoch 53 | loss: 0.33503 | val_0_rmse: 0.54761 | val_1_rmse: 0.57507 |  0:00:13s
epoch 54 | loss: 0.3117  | val_0_rmse: 0.54945 | val_1_rmse: 0.58123 |  0:00:13s
epoch 55 | loss: 0.31119 | val_0_rmse: 0.55094 | val_1_rmse: 0.60288 |  0:00:14s
epoch 56 | loss: 0.32007 | val_0_rmse: 0.54688 | val_1_rmse: 0.598   |  0:00:14s
epoch 57 | loss: 0.31242 | val_0_rmse: 0.54907 | val_1_rmse: 0.57603 |  0:00:14s
epoch 58 | loss: 0.32087 | val_0_rmse: 0.55248 | val_1_rmse: 0.57356 |  0:00:14s
epoch 59 | loss: 0.32089 | val_0_rmse: 0.55168 | val_1_rmse: 0.59739 |  0:00:15s
epoch 60 | loss: 0.30659 | val_0_rmse: 0.5509  | val_1_rmse: 0.60621 |  0:00:15s
epoch 61 | loss: 0.31046 | val_0_rmse: 0.54342 | val_1_rmse: 0.57167 |  0:00:15s
epoch 62 | loss: 0.31203 | val_0_rmse: 0.54292 | val_1_rmse: 0.5629  |  0:00:15s
epoch 63 | loss: 0.30242 | val_0_rmse: 0.54936 | val_1_rmse: 0.58199 |  0:00:16s
epoch 64 | loss: 0.32794 | val_0_rmse: 0.54123 | val_1_rmse: 0.57775 |  0:00:16s
epoch 65 | loss: 0.301   | val_0_rmse: 0.55171 | val_1_rmse: 0.58899 |  0:00:16s
epoch 66 | loss: 0.31787 | val_0_rmse: 0.55965 | val_1_rmse: 0.60142 |  0:00:16s
epoch 67 | loss: 0.31494 | val_0_rmse: 0.56019 | val_1_rmse: 0.60866 |  0:00:17s
epoch 68 | loss: 0.31488 | val_0_rmse: 0.56197 | val_1_rmse: 0.61535 |  0:00:17s
epoch 69 | loss: 0.32354 | val_0_rmse: 0.56298 | val_1_rmse: 0.60268 |  0:00:17s
epoch 70 | loss: 0.31459 | val_0_rmse: 0.56014 | val_1_rmse: 0.57563 |  0:00:17s
epoch 71 | loss: 0.32317 | val_0_rmse: 0.5604  | val_1_rmse: 0.57065 |  0:00:18s
epoch 72 | loss: 0.31716 | val_0_rmse: 0.55883 | val_1_rmse: 0.59113 |  0:00:18s
epoch 73 | loss: 0.31134 | val_0_rmse: 0.55922 | val_1_rmse: 0.58796 |  0:00:18s
epoch 74 | loss: 0.31566 | val_0_rmse: 0.55098 | val_1_rmse: 0.57682 |  0:00:18s
epoch 75 | loss: 0.3149  | val_0_rmse: 0.54987 | val_1_rmse: 0.57958 |  0:00:19s
epoch 76 | loss: 0.31101 | val_0_rmse: 0.55314 | val_1_rmse: 0.58945 |  0:00:19s
epoch 77 | loss: 0.32123 | val_0_rmse: 0.55562 | val_1_rmse: 0.59812 |  0:00:19s
epoch 78 | loss: 0.31028 | val_0_rmse: 0.54547 | val_1_rmse: 0.58158 |  0:00:19s
epoch 79 | loss: 0.30724 | val_0_rmse: 0.54838 | val_1_rmse: 0.57384 |  0:00:20s
epoch 80 | loss: 0.32254 | val_0_rmse: 0.54672 | val_1_rmse: 0.57652 |  0:00:20s
epoch 81 | loss: 0.31159 | val_0_rmse: 0.5458  | val_1_rmse: 0.59999 |  0:00:20s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.55555
Best weights from best epoch are automatically used!
ended training at: 02:56:04
Feature importance:
[('Area', 0.30457211358949343), ('Baths', 0.17167229273994444), ('Beds', 0.1359572473906378), ('Latitude', 0.08068021140220846), ('Longitude', 0.08341528198229903), ('Month', 0.016698890977321308), ('Year', 0.2070039619180955)]
Mean squared error is of 967013496.7436392
Mean absolute error:22431.605618775753
MAPE:0.37211147403977857
R2 score:0.6932603429656711
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:56:04
epoch 0  | loss: 1.73246 | val_0_rmse: 1.43856 | val_1_rmse: 1.48515 |  0:00:00s
epoch 1  | loss: 0.6579  | val_0_rmse: 1.5046  | val_1_rmse: 1.40708 |  0:00:00s
epoch 2  | loss: 0.4918  | val_0_rmse: 0.96979 | val_1_rmse: 0.89752 |  0:00:00s
epoch 3  | loss: 0.43854 | val_0_rmse: 0.90984 | val_1_rmse: 0.87629 |  0:00:01s
epoch 4  | loss: 0.39998 | val_0_rmse: 0.80162 | val_1_rmse: 0.78553 |  0:00:01s
epoch 5  | loss: 0.39612 | val_0_rmse: 0.67669 | val_1_rmse: 0.68555 |  0:00:01s
epoch 6  | loss: 0.38371 | val_0_rmse: 0.65714 | val_1_rmse: 0.66483 |  0:00:01s
epoch 7  | loss: 0.3751  | val_0_rmse: 0.64873 | val_1_rmse: 0.65767 |  0:00:02s
epoch 8  | loss: 0.37976 | val_0_rmse: 0.62593 | val_1_rmse: 0.6237  |  0:00:02s
epoch 9  | loss: 0.36435 | val_0_rmse: 0.61164 | val_1_rmse: 0.60931 |  0:00:02s
epoch 10 | loss: 0.35395 | val_0_rmse: 0.61516 | val_1_rmse: 0.61287 |  0:00:03s
epoch 11 | loss: 0.36303 | val_0_rmse: 0.61781 | val_1_rmse: 0.62463 |  0:00:03s
epoch 12 | loss: 0.36398 | val_0_rmse: 0.60183 | val_1_rmse: 0.6195  |  0:00:03s
epoch 13 | loss: 0.3555  | val_0_rmse: 0.59156 | val_1_rmse: 0.59079 |  0:00:03s
epoch 14 | loss: 0.35342 | val_0_rmse: 0.58855 | val_1_rmse: 0.59279 |  0:00:04s
epoch 15 | loss: 0.35431 | val_0_rmse: 0.59508 | val_1_rmse: 0.60932 |  0:00:04s
epoch 16 | loss: 0.35265 | val_0_rmse: 0.6101  | val_1_rmse: 0.62673 |  0:00:04s
epoch 17 | loss: 0.3538  | val_0_rmse: 0.59364 | val_1_rmse: 0.61679 |  0:00:04s
epoch 18 | loss: 0.34363 | val_0_rmse: 0.58829 | val_1_rmse: 0.62222 |  0:00:05s
epoch 19 | loss: 0.34352 | val_0_rmse: 0.59831 | val_1_rmse: 0.63509 |  0:00:05s
epoch 20 | loss: 0.33521 | val_0_rmse: 0.5956  | val_1_rmse: 0.61822 |  0:00:05s
epoch 21 | loss: 0.35718 | val_0_rmse: 0.59432 | val_1_rmse: 0.59442 |  0:00:05s
epoch 22 | loss: 0.36236 | val_0_rmse: 0.58497 | val_1_rmse: 0.60167 |  0:00:06s
epoch 23 | loss: 0.35511 | val_0_rmse: 0.59684 | val_1_rmse: 0.5686  |  0:00:06s
epoch 24 | loss: 0.36575 | val_0_rmse: 0.60027 | val_1_rmse: 0.58088 |  0:00:06s
epoch 25 | loss: 0.36443 | val_0_rmse: 0.60038 | val_1_rmse: 0.60073 |  0:00:06s
epoch 26 | loss: 0.36027 | val_0_rmse: 0.58803 | val_1_rmse: 0.61136 |  0:00:07s
epoch 27 | loss: 0.35217 | val_0_rmse: 0.5954  | val_1_rmse: 0.60365 |  0:00:07s
epoch 28 | loss: 0.35994 | val_0_rmse: 0.59318 | val_1_rmse: 0.61499 |  0:00:07s
epoch 29 | loss: 0.35987 | val_0_rmse: 0.58185 | val_1_rmse: 0.60786 |  0:00:07s
epoch 30 | loss: 0.35159 | val_0_rmse: 0.58826 | val_1_rmse: 0.59091 |  0:00:07s
epoch 31 | loss: 0.34608 | val_0_rmse: 0.58382 | val_1_rmse: 0.57603 |  0:00:08s
epoch 32 | loss: 0.34617 | val_0_rmse: 0.58188 | val_1_rmse: 0.59014 |  0:00:08s
epoch 33 | loss: 0.34597 | val_0_rmse: 0.5724  | val_1_rmse: 0.58631 |  0:00:08s
epoch 34 | loss: 0.3397  | val_0_rmse: 0.57337 | val_1_rmse: 0.58059 |  0:00:08s
epoch 35 | loss: 0.33617 | val_0_rmse: 0.57305 | val_1_rmse: 0.58101 |  0:00:09s
epoch 36 | loss: 0.34136 | val_0_rmse: 0.57466 | val_1_rmse: 0.58459 |  0:00:09s
epoch 37 | loss: 0.33469 | val_0_rmse: 0.57651 | val_1_rmse: 0.57661 |  0:00:09s
epoch 38 | loss: 0.33417 | val_0_rmse: 0.578   | val_1_rmse: 0.58906 |  0:00:09s
epoch 39 | loss: 0.33048 | val_0_rmse: 0.58033 | val_1_rmse: 0.60195 |  0:00:10s
epoch 40 | loss: 0.33103 | val_0_rmse: 0.56927 | val_1_rmse: 0.58646 |  0:00:10s
epoch 41 | loss: 0.32216 | val_0_rmse: 0.56894 | val_1_rmse: 0.57246 |  0:00:10s
epoch 42 | loss: 0.33549 | val_0_rmse: 0.57015 | val_1_rmse: 0.57467 |  0:00:10s
epoch 43 | loss: 0.32813 | val_0_rmse: 0.57594 | val_1_rmse: 0.59072 |  0:00:11s
epoch 44 | loss: 0.34562 | val_0_rmse: 0.56854 | val_1_rmse: 0.57754 |  0:00:11s
epoch 45 | loss: 0.32787 | val_0_rmse: 0.56954 | val_1_rmse: 0.56383 |  0:00:11s
epoch 46 | loss: 0.34366 | val_0_rmse: 0.57302 | val_1_rmse: 0.57522 |  0:00:11s
epoch 47 | loss: 0.33524 | val_0_rmse: 0.57197 | val_1_rmse: 0.58352 |  0:00:12s
epoch 48 | loss: 0.32625 | val_0_rmse: 0.56722 | val_1_rmse: 0.57608 |  0:00:12s
epoch 49 | loss: 0.32778 | val_0_rmse: 0.56523 | val_1_rmse: 0.57715 |  0:00:12s
epoch 50 | loss: 0.33156 | val_0_rmse: 0.5648  | val_1_rmse: 0.58029 |  0:00:12s
epoch 51 | loss: 0.32766 | val_0_rmse: 0.565   | val_1_rmse: 0.56783 |  0:00:13s
epoch 52 | loss: 0.32321 | val_0_rmse: 0.56638 | val_1_rmse: 0.56883 |  0:00:13s
epoch 53 | loss: 0.32431 | val_0_rmse: 0.5624  | val_1_rmse: 0.58502 |  0:00:13s
epoch 54 | loss: 0.32782 | val_0_rmse: 0.56815 | val_1_rmse: 0.58918 |  0:00:13s
epoch 55 | loss: 0.33067 | val_0_rmse: 0.56952 | val_1_rmse: 0.59636 |  0:00:14s
epoch 56 | loss: 0.32744 | val_0_rmse: 0.56219 | val_1_rmse: 0.6091  |  0:00:14s
epoch 57 | loss: 0.32609 | val_0_rmse: 0.56567 | val_1_rmse: 0.59739 |  0:00:14s
epoch 58 | loss: 0.3213  | val_0_rmse: 0.57238 | val_1_rmse: 0.59012 |  0:00:14s
epoch 59 | loss: 0.3299  | val_0_rmse: 0.55935 | val_1_rmse: 0.58993 |  0:00:15s
epoch 60 | loss: 0.32796 | val_0_rmse: 0.56117 | val_1_rmse: 0.59938 |  0:00:15s
epoch 61 | loss: 0.33134 | val_0_rmse: 0.56291 | val_1_rmse: 0.58826 |  0:00:15s
epoch 62 | loss: 0.32334 | val_0_rmse: 0.55698 | val_1_rmse: 0.58064 |  0:00:15s
epoch 63 | loss: 0.32686 | val_0_rmse: 0.56504 | val_1_rmse: 0.60315 |  0:00:16s
epoch 64 | loss: 0.32528 | val_0_rmse: 0.56209 | val_1_rmse: 0.60432 |  0:00:16s
epoch 65 | loss: 0.32778 | val_0_rmse: 0.5569  | val_1_rmse: 0.58685 |  0:00:16s
epoch 66 | loss: 0.31503 | val_0_rmse: 0.55532 | val_1_rmse: 0.59473 |  0:00:16s
epoch 67 | loss: 0.33431 | val_0_rmse: 0.55694 | val_1_rmse: 0.59176 |  0:00:17s
epoch 68 | loss: 0.31498 | val_0_rmse: 0.56729 | val_1_rmse: 0.58686 |  0:00:17s
epoch 69 | loss: 0.32971 | val_0_rmse: 0.56335 | val_1_rmse: 0.59106 |  0:00:17s
epoch 70 | loss: 0.31785 | val_0_rmse: 0.5632  | val_1_rmse: 0.6067  |  0:00:17s
epoch 71 | loss: 0.33142 | val_0_rmse: 0.55734 | val_1_rmse: 0.59012 |  0:00:18s
epoch 72 | loss: 0.32796 | val_0_rmse: 0.56731 | val_1_rmse: 0.58404 |  0:00:18s
epoch 73 | loss: 0.33123 | val_0_rmse: 0.56076 | val_1_rmse: 0.58842 |  0:00:18s
epoch 74 | loss: 0.3197  | val_0_rmse: 0.55754 | val_1_rmse: 0.60287 |  0:00:18s
epoch 75 | loss: 0.32321 | val_0_rmse: 0.55795 | val_1_rmse: 0.61387 |  0:00:19s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.56383
Best weights from best epoch are automatically used!
ended training at: 02:56:24
Feature importance:
[('Area', 0.3770641476271946), ('Baths', 0.028734698971321367), ('Beds', 0.3813014169419673), ('Latitude', 0.07861681508748304), ('Longitude', 0.013712171572089593), ('Month', 0.07364683778643406), ('Year', 0.04692391201351004)]
Mean squared error is of 1060138504.9183991
Mean absolute error:22030.547072244164
MAPE:0.3552576037433195
R2 score:0.6904706157191587
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:56:24
epoch 0  | loss: 0.69534 | val_0_rmse: 0.71318 | val_1_rmse: 0.7185  |  0:00:01s
epoch 1  | loss: 0.41679 | val_0_rmse: 0.64793 | val_1_rmse: 0.64923 |  0:00:03s
epoch 2  | loss: 0.36668 | val_0_rmse: 0.64287 | val_1_rmse: 0.64662 |  0:00:05s
epoch 3  | loss: 0.34784 | val_0_rmse: 0.57694 | val_1_rmse: 0.58681 |  0:00:07s
epoch 4  | loss: 0.33505 | val_0_rmse: 0.57301 | val_1_rmse: 0.57352 |  0:00:09s
epoch 5  | loss: 0.33107 | val_0_rmse: 0.5881  | val_1_rmse: 0.59652 |  0:00:11s
epoch 6  | loss: 0.32966 | val_0_rmse: 0.56249 | val_1_rmse: 0.56281 |  0:00:12s
epoch 7  | loss: 0.33492 | val_0_rmse: 0.57375 | val_1_rmse: 0.57403 |  0:00:14s
epoch 8  | loss: 0.32959 | val_0_rmse: 0.5911  | val_1_rmse: 0.59003 |  0:00:16s
epoch 9  | loss: 0.32575 | val_0_rmse: 0.59025 | val_1_rmse: 0.59354 |  0:00:18s
epoch 10 | loss: 0.31938 | val_0_rmse: 0.58773 | val_1_rmse: 0.58326 |  0:00:20s
epoch 11 | loss: 0.32016 | val_0_rmse: 0.5564  | val_1_rmse: 0.55412 |  0:00:21s
epoch 12 | loss: 0.32144 | val_0_rmse: 0.55066 | val_1_rmse: 0.54596 |  0:00:23s
epoch 13 | loss: 0.31811 | val_0_rmse: 0.5493  | val_1_rmse: 0.54659 |  0:00:25s
epoch 14 | loss: 0.32171 | val_0_rmse: 0.56441 | val_1_rmse: 0.56229 |  0:00:27s
epoch 15 | loss: 0.31517 | val_0_rmse: 0.54888 | val_1_rmse: 0.55379 |  0:00:29s
epoch 16 | loss: 0.3163  | val_0_rmse: 0.55147 | val_1_rmse: 0.55483 |  0:00:31s
epoch 17 | loss: 0.31302 | val_0_rmse: 0.56031 | val_1_rmse: 0.55879 |  0:00:33s
epoch 18 | loss: 0.31834 | val_0_rmse: 0.5639  | val_1_rmse: 0.56051 |  0:00:34s
epoch 19 | loss: 0.3185  | val_0_rmse: 0.54177 | val_1_rmse: 0.53987 |  0:00:36s
epoch 20 | loss: 0.31042 | val_0_rmse: 0.58156 | val_1_rmse: 0.58884 |  0:00:38s
epoch 21 | loss: 0.31116 | val_0_rmse: 0.56284 | val_1_rmse: 0.56259 |  0:00:40s
epoch 22 | loss: 0.31772 | val_0_rmse: 0.54842 | val_1_rmse: 0.54854 |  0:00:42s
epoch 23 | loss: 0.31181 | val_0_rmse: 0.54519 | val_1_rmse: 0.54477 |  0:00:44s
epoch 24 | loss: 0.30326 | val_0_rmse: 0.53688 | val_1_rmse: 0.53957 |  0:00:45s
epoch 25 | loss: 0.29855 | val_0_rmse: 0.54022 | val_1_rmse: 0.54361 |  0:00:47s
epoch 26 | loss: 0.30118 | val_0_rmse: 0.55337 | val_1_rmse: 0.5544  |  0:00:49s
epoch 27 | loss: 0.30223 | val_0_rmse: 0.54309 | val_1_rmse: 0.54398 |  0:00:51s
epoch 28 | loss: 0.30319 | val_0_rmse: 0.54822 | val_1_rmse: 0.54646 |  0:00:53s
epoch 29 | loss: 0.3016  | val_0_rmse: 0.53619 | val_1_rmse: 0.53405 |  0:00:55s
epoch 30 | loss: 0.29924 | val_0_rmse: 0.53405 | val_1_rmse: 0.53362 |  0:00:56s
epoch 31 | loss: 0.29873 | val_0_rmse: 0.54224 | val_1_rmse: 0.53655 |  0:00:58s
epoch 32 | loss: 0.30275 | val_0_rmse: 0.53241 | val_1_rmse: 0.5335  |  0:01:00s
epoch 33 | loss: 0.29772 | val_0_rmse: 0.5392  | val_1_rmse: 0.5372  |  0:01:02s
epoch 34 | loss: 0.29653 | val_0_rmse: 0.53548 | val_1_rmse: 0.54321 |  0:01:04s
epoch 35 | loss: 0.29497 | val_0_rmse: 0.53649 | val_1_rmse: 0.54079 |  0:01:06s
epoch 36 | loss: 0.29529 | val_0_rmse: 0.54359 | val_1_rmse: 0.54736 |  0:01:07s
epoch 37 | loss: 0.29871 | val_0_rmse: 0.53658 | val_1_rmse: 0.53561 |  0:01:09s
epoch 38 | loss: 0.30038 | val_0_rmse: 0.55138 | val_1_rmse: 0.55619 |  0:01:11s
epoch 39 | loss: 0.2996  | val_0_rmse: 0.5519  | val_1_rmse: 0.55707 |  0:01:13s
epoch 40 | loss: 0.29109 | val_0_rmse: 0.53173 | val_1_rmse: 0.53752 |  0:01:15s
epoch 41 | loss: 0.2927  | val_0_rmse: 0.53166 | val_1_rmse: 0.54275 |  0:01:17s
epoch 42 | loss: 0.29053 | val_0_rmse: 0.55076 | val_1_rmse: 0.56172 |  0:01:18s
epoch 43 | loss: 0.29285 | val_0_rmse: 0.54055 | val_1_rmse: 0.54639 |  0:01:20s
epoch 44 | loss: 0.29949 | val_0_rmse: 0.53989 | val_1_rmse: 0.54999 |  0:01:22s
epoch 45 | loss: 0.29714 | val_0_rmse: 0.64372 | val_1_rmse: 0.6616  |  0:01:24s
epoch 46 | loss: 0.29576 | val_0_rmse: 0.52931 | val_1_rmse: 0.53955 |  0:01:26s
epoch 47 | loss: 0.29866 | val_0_rmse: 0.60818 | val_1_rmse: 0.62357 |  0:01:28s
epoch 48 | loss: 0.29189 | val_0_rmse: 0.53864 | val_1_rmse: 0.55136 |  0:01:29s
epoch 49 | loss: 0.29172 | val_0_rmse: 0.53875 | val_1_rmse: 0.54854 |  0:01:31s
epoch 50 | loss: 0.3018  | val_0_rmse: 0.551   | val_1_rmse: 0.5607  |  0:01:33s
epoch 51 | loss: 0.29404 | val_0_rmse: 0.53192 | val_1_rmse: 0.54132 |  0:01:35s
epoch 52 | loss: 0.29369 | val_0_rmse: 0.5587  | val_1_rmse: 0.5735  |  0:01:37s
epoch 53 | loss: 0.30179 | val_0_rmse: 0.53408 | val_1_rmse: 0.54376 |  0:01:39s
epoch 54 | loss: 0.29484 | val_0_rmse: 0.5384  | val_1_rmse: 0.54292 |  0:01:40s
epoch 55 | loss: 0.29118 | val_0_rmse: 0.5323  | val_1_rmse: 0.5404  |  0:01:42s
epoch 56 | loss: 0.29606 | val_0_rmse: 0.53298 | val_1_rmse: 0.54065 |  0:01:44s
epoch 57 | loss: 0.29638 | val_0_rmse: 0.52403 | val_1_rmse: 0.53378 |  0:01:46s
epoch 58 | loss: 0.28931 | val_0_rmse: 0.53591 | val_1_rmse: 0.54323 |  0:01:48s
epoch 59 | loss: 0.29397 | val_0_rmse: 0.5357  | val_1_rmse: 0.54293 |  0:01:50s
epoch 60 | loss: 0.29078 | val_0_rmse: 0.5438  | val_1_rmse: 0.55007 |  0:01:51s
epoch 61 | loss: 0.28782 | val_0_rmse: 0.52393 | val_1_rmse: 0.5376  |  0:01:53s
epoch 62 | loss: 0.28401 | val_0_rmse: 0.52676 | val_1_rmse: 0.54145 |  0:01:55s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.5335
Best weights from best epoch are automatically used!
ended training at: 02:58:20
Feature importance:
[('Area', 0.07832249044107871), ('Baths', 0.13187848007452402), ('Beds', 0.0455700596149627), ('Latitude', 0.11393279377219027), ('Longitude', 0.34121794896456076), ('Month', 0.029440445355141176), ('Year', 0.2596377817775423)]
Mean squared error is of 16652825337.40222
Mean absolute error:83907.76726085226
MAPE:0.4191931214873116
R2 score:0.6915755244186136
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:58:21
epoch 0  | loss: 0.71879 | val_0_rmse: 0.7464  | val_1_rmse: 0.7556  |  0:00:01s
epoch 1  | loss: 0.41138 | val_0_rmse: 0.72051 | val_1_rmse: 0.73501 |  0:00:03s
epoch 2  | loss: 0.37595 | val_0_rmse: 0.61641 | val_1_rmse: 0.63405 |  0:00:05s
epoch 3  | loss: 0.36083 | val_0_rmse: 0.5764  | val_1_rmse: 0.60242 |  0:00:07s
epoch 4  | loss: 0.3486  | val_0_rmse: 0.58868 | val_1_rmse: 0.61488 |  0:00:09s
epoch 5  | loss: 0.35271 | val_0_rmse: 0.57225 | val_1_rmse: 0.60042 |  0:00:10s
epoch 6  | loss: 0.33787 | val_0_rmse: 0.57565 | val_1_rmse: 0.60782 |  0:00:12s
epoch 7  | loss: 0.32534 | val_0_rmse: 0.66971 | val_1_rmse: 0.68184 |  0:00:14s
epoch 8  | loss: 0.32995 | val_0_rmse: 0.56298 | val_1_rmse: 0.58684 |  0:00:16s
epoch 9  | loss: 0.31811 | val_0_rmse: 0.55778 | val_1_rmse: 0.58154 |  0:00:18s
epoch 10 | loss: 0.31112 | val_0_rmse: 0.56479 | val_1_rmse: 0.59526 |  0:00:20s
epoch 11 | loss: 0.32164 | val_0_rmse: 0.61981 | val_1_rmse: 0.64327 |  0:00:22s
epoch 12 | loss: 0.3079  | val_0_rmse: 0.57573 | val_1_rmse: 0.58984 |  0:00:23s
epoch 13 | loss: 0.3014  | val_0_rmse: 0.53789 | val_1_rmse: 0.56855 |  0:00:25s
epoch 14 | loss: 0.29997 | val_0_rmse: 0.54612 | val_1_rmse: 0.57621 |  0:00:27s
epoch 15 | loss: 0.30356 | val_0_rmse: 0.53955 | val_1_rmse: 0.55927 |  0:00:29s
epoch 16 | loss: 0.29979 | val_0_rmse: 0.53656 | val_1_rmse: 0.56682 |  0:00:31s
epoch 17 | loss: 0.29869 | val_0_rmse: 0.53277 | val_1_rmse: 0.5609  |  0:00:32s
epoch 18 | loss: 0.2951  | val_0_rmse: 0.53185 | val_1_rmse: 0.55324 |  0:00:34s
epoch 19 | loss: 0.29262 | val_0_rmse: 0.53903 | val_1_rmse: 0.56886 |  0:00:36s
epoch 20 | loss: 0.2916  | val_0_rmse: 0.53011 | val_1_rmse: 0.5616  |  0:00:38s
epoch 21 | loss: 0.29803 | val_0_rmse: 0.55607 | val_1_rmse: 0.57949 |  0:00:40s
epoch 22 | loss: 0.30083 | val_0_rmse: 0.54079 | val_1_rmse: 0.56335 |  0:00:42s
epoch 23 | loss: 0.30245 | val_0_rmse: 0.56899 | val_1_rmse: 0.59624 |  0:00:43s
epoch 24 | loss: 0.29338 | val_0_rmse: 0.5426  | val_1_rmse: 0.5712  |  0:00:45s
epoch 25 | loss: 0.29725 | val_0_rmse: 0.55313 | val_1_rmse: 0.5788  |  0:00:47s
epoch 26 | loss: 0.29649 | val_0_rmse: 0.53703 | val_1_rmse: 0.55724 |  0:00:49s
epoch 27 | loss: 0.29435 | val_0_rmse: 0.54404 | val_1_rmse: 0.56968 |  0:00:51s
epoch 28 | loss: 0.29485 | val_0_rmse: 0.53427 | val_1_rmse: 0.55927 |  0:00:53s
epoch 29 | loss: 0.28975 | val_0_rmse: 0.54955 | val_1_rmse: 0.57512 |  0:00:54s
epoch 30 | loss: 0.29515 | val_0_rmse: 0.53097 | val_1_rmse: 0.55902 |  0:00:56s
epoch 31 | loss: 0.29027 | val_0_rmse: 0.54065 | val_1_rmse: 0.56194 |  0:00:58s
epoch 32 | loss: 0.2934  | val_0_rmse: 0.53065 | val_1_rmse: 0.55593 |  0:01:00s
epoch 33 | loss: 0.29272 | val_0_rmse: 0.53137 | val_1_rmse: 0.5601  |  0:01:02s
epoch 34 | loss: 0.2888  | val_0_rmse: 0.53778 | val_1_rmse: 0.56785 |  0:01:03s
epoch 35 | loss: 0.29034 | val_0_rmse: 0.53057 | val_1_rmse: 0.54987 |  0:01:05s
epoch 36 | loss: 0.28695 | val_0_rmse: 0.52782 | val_1_rmse: 0.55225 |  0:01:07s
epoch 37 | loss: 0.28356 | val_0_rmse: 0.52312 | val_1_rmse: 0.5487  |  0:01:09s
epoch 38 | loss: 0.28335 | val_0_rmse: 0.52201 | val_1_rmse: 0.54522 |  0:01:11s
epoch 39 | loss: 0.29097 | val_0_rmse: 0.53094 | val_1_rmse: 0.56128 |  0:01:12s
epoch 40 | loss: 0.29039 | val_0_rmse: 0.5367  | val_1_rmse: 0.56157 |  0:01:14s
epoch 41 | loss: 0.28672 | val_0_rmse: 0.52966 | val_1_rmse: 0.55646 |  0:01:16s
epoch 42 | loss: 0.28499 | val_0_rmse: 0.52045 | val_1_rmse: 0.54726 |  0:01:18s
epoch 43 | loss: 0.28753 | val_0_rmse: 0.52672 | val_1_rmse: 0.55149 |  0:01:20s
epoch 44 | loss: 0.28546 | val_0_rmse: 0.53007 | val_1_rmse: 0.56052 |  0:01:21s
epoch 45 | loss: 0.28007 | val_0_rmse: 0.52731 | val_1_rmse: 0.55054 |  0:01:23s
epoch 46 | loss: 0.28283 | val_0_rmse: 0.52338 | val_1_rmse: 0.5495  |  0:01:25s
epoch 47 | loss: 0.28499 | val_0_rmse: 0.5248  | val_1_rmse: 0.54844 |  0:01:27s
epoch 48 | loss: 0.28581 | val_0_rmse: 0.52252 | val_1_rmse: 0.54847 |  0:01:29s
epoch 49 | loss: 0.28281 | val_0_rmse: 0.53277 | val_1_rmse: 0.56496 |  0:01:30s
epoch 50 | loss: 0.277   | val_0_rmse: 0.52738 | val_1_rmse: 0.55122 |  0:01:32s
epoch 51 | loss: 0.28869 | val_0_rmse: 0.54094 | val_1_rmse: 0.56379 |  0:01:34s
epoch 52 | loss: 0.28461 | val_0_rmse: 0.52807 | val_1_rmse: 0.54992 |  0:01:36s
epoch 53 | loss: 0.28278 | val_0_rmse: 0.54478 | val_1_rmse: 0.56792 |  0:01:38s
epoch 54 | loss: 0.28461 | val_0_rmse: 0.53768 | val_1_rmse: 0.55808 |  0:01:40s
epoch 55 | loss: 0.28714 | val_0_rmse: 0.53182 | val_1_rmse: 0.55663 |  0:01:41s
epoch 56 | loss: 0.29316 | val_0_rmse: 0.52787 | val_1_rmse: 0.55626 |  0:01:43s
epoch 57 | loss: 0.28142 | val_0_rmse: 0.54608 | val_1_rmse: 0.56361 |  0:01:45s
epoch 58 | loss: 0.28884 | val_0_rmse: 0.58892 | val_1_rmse: 0.62046 |  0:01:47s
epoch 59 | loss: 0.29068 | val_0_rmse: 0.54154 | val_1_rmse: 0.56889 |  0:01:49s
epoch 60 | loss: 0.29038 | val_0_rmse: 0.53148 | val_1_rmse: 0.55274 |  0:01:50s
epoch 61 | loss: 0.2838  | val_0_rmse: 0.53546 | val_1_rmse: 0.56404 |  0:01:52s
epoch 62 | loss: 0.28109 | val_0_rmse: 0.54736 | val_1_rmse: 0.56971 |  0:01:54s
epoch 63 | loss: 0.28414 | val_0_rmse: 0.53057 | val_1_rmse: 0.56173 |  0:01:56s
epoch 64 | loss: 0.28512 | val_0_rmse: 0.53287 | val_1_rmse: 0.56203 |  0:01:58s
epoch 65 | loss: 0.28044 | val_0_rmse: 0.52185 | val_1_rmse: 0.54832 |  0:02:00s
epoch 66 | loss: 0.28124 | val_0_rmse: 0.52355 | val_1_rmse: 0.55497 |  0:02:01s
epoch 67 | loss: 0.28256 | val_0_rmse: 0.52343 | val_1_rmse: 0.5534  |  0:02:03s
epoch 68 | loss: 0.28095 | val_0_rmse: 0.55187 | val_1_rmse: 0.5685  |  0:02:05s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.54522
Best weights from best epoch are automatically used!
ended training at: 03:00:27
Feature importance:
[('Area', 0.21867348961792352), ('Baths', 0.1368204892272741), ('Beds', 0.04467863562975211), ('Latitude', 0.159504075433039), ('Longitude', 0.297017524190958), ('Month', 0.01959664970473187), ('Year', 0.12370913619632137)]
Mean squared error is of 16487535433.915148
Mean absolute error:83354.96832324297
MAPE:0.4464275773939734
R2 score:0.703611288693198
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:00:27
epoch 0  | loss: 0.68341 | val_0_rmse: 0.78783 | val_1_rmse: 0.78038 |  0:00:01s
epoch 1  | loss: 0.42794 | val_0_rmse: 0.72253 | val_1_rmse: 0.7183  |  0:00:03s
epoch 2  | loss: 0.39382 | val_0_rmse: 0.61995 | val_1_rmse: 0.59979 |  0:00:05s
epoch 3  | loss: 0.37464 | val_0_rmse: 0.59164 | val_1_rmse: 0.58021 |  0:00:07s
epoch 4  | loss: 0.36586 | val_0_rmse: 0.58649 | val_1_rmse: 0.57606 |  0:00:09s
epoch 5  | loss: 0.35302 | val_0_rmse: 0.57817 | val_1_rmse: 0.56835 |  0:00:10s
epoch 6  | loss: 0.34317 | val_0_rmse: 0.59885 | val_1_rmse: 0.59621 |  0:00:12s
epoch 7  | loss: 0.33443 | val_0_rmse: 0.56454 | val_1_rmse: 0.55614 |  0:00:14s
epoch 8  | loss: 0.32254 | val_0_rmse: 0.56665 | val_1_rmse: 0.55849 |  0:00:16s
epoch 9  | loss: 0.32832 | val_0_rmse: 0.66673 | val_1_rmse: 0.65387 |  0:00:18s
epoch 10 | loss: 0.32957 | val_0_rmse: 0.60351 | val_1_rmse: 0.60118 |  0:00:20s
epoch 11 | loss: 0.32101 | val_0_rmse: 0.55277 | val_1_rmse: 0.54666 |  0:00:21s
epoch 12 | loss: 0.32237 | val_0_rmse: 0.56257 | val_1_rmse: 0.5537  |  0:00:23s
epoch 13 | loss: 0.31521 | val_0_rmse: 0.55591 | val_1_rmse: 0.54784 |  0:00:25s
epoch 14 | loss: 0.32433 | val_0_rmse: 0.74419 | val_1_rmse: 0.73976 |  0:00:27s
epoch 15 | loss: 0.31438 | val_0_rmse: 0.55309 | val_1_rmse: 0.54594 |  0:00:29s
epoch 16 | loss: 0.31567 | val_0_rmse: 0.66776 | val_1_rmse: 0.66131 |  0:00:31s
epoch 17 | loss: 0.30906 | val_0_rmse: 0.59317 | val_1_rmse: 0.57598 |  0:00:32s
epoch 18 | loss: 0.31769 | val_0_rmse: 0.55538 | val_1_rmse: 0.54971 |  0:00:34s
epoch 19 | loss: 0.31583 | val_0_rmse: 0.55921 | val_1_rmse: 0.55683 |  0:00:36s
epoch 20 | loss: 0.30843 | val_0_rmse: 0.54656 | val_1_rmse: 0.54298 |  0:00:38s
epoch 21 | loss: 0.30667 | val_0_rmse: 0.58077 | val_1_rmse: 0.57763 |  0:00:40s
epoch 22 | loss: 0.30533 | val_0_rmse: 0.62001 | val_1_rmse: 0.61697 |  0:00:42s
epoch 23 | loss: 0.31766 | val_0_rmse: 0.56064 | val_1_rmse: 0.55872 |  0:00:43s
epoch 24 | loss: 0.31337 | val_0_rmse: 0.5462  | val_1_rmse: 0.54185 |  0:00:45s
epoch 25 | loss: 0.30455 | val_0_rmse: 0.55018 | val_1_rmse: 0.54738 |  0:00:47s
epoch 26 | loss: 0.30621 | val_0_rmse: 0.55762 | val_1_rmse: 0.55034 |  0:00:49s
epoch 27 | loss: 0.31016 | val_0_rmse: 0.74938 | val_1_rmse: 0.75655 |  0:00:51s
epoch 28 | loss: 0.30887 | val_0_rmse: 0.56147 | val_1_rmse: 0.55707 |  0:00:53s
epoch 29 | loss: 0.30871 | val_0_rmse: 0.76834 | val_1_rmse: 0.76348 |  0:00:54s
epoch 30 | loss: 0.30697 | val_0_rmse: 0.59915 | val_1_rmse: 0.59967 |  0:00:56s
epoch 31 | loss: 0.30177 | val_0_rmse: 0.56545 | val_1_rmse: 0.56574 |  0:00:58s
epoch 32 | loss: 0.30896 | val_0_rmse: 0.56253 | val_1_rmse: 0.55656 |  0:01:00s
epoch 33 | loss: 0.30341 | val_0_rmse: 0.5446  | val_1_rmse: 0.54143 |  0:01:02s
epoch 34 | loss: 0.3083  | val_0_rmse: 0.68977 | val_1_rmse: 0.68778 |  0:01:04s
epoch 35 | loss: 0.30378 | val_0_rmse: 0.54488 | val_1_rmse: 0.54055 |  0:01:06s
epoch 36 | loss: 0.30492 | val_0_rmse: 0.57676 | val_1_rmse: 0.57388 |  0:01:08s
epoch 37 | loss: 0.30172 | val_0_rmse: 0.55944 | val_1_rmse: 0.55365 |  0:01:09s
epoch 38 | loss: 0.30533 | val_0_rmse: 0.571   | val_1_rmse: 0.57336 |  0:01:11s
epoch 39 | loss: 0.30576 | val_0_rmse: 0.55916 | val_1_rmse: 0.55173 |  0:01:13s
epoch 40 | loss: 0.30509 | val_0_rmse: 0.55584 | val_1_rmse: 0.55388 |  0:01:15s
epoch 41 | loss: 0.30268 | val_0_rmse: 0.60625 | val_1_rmse: 0.60012 |  0:01:17s
epoch 42 | loss: 0.29892 | val_0_rmse: 0.56427 | val_1_rmse: 0.55923 |  0:01:19s
epoch 43 | loss: 0.30269 | val_0_rmse: 0.61565 | val_1_rmse: 0.61865 |  0:01:20s
epoch 44 | loss: 0.29936 | val_0_rmse: 0.55093 | val_1_rmse: 0.54562 |  0:01:22s
epoch 45 | loss: 0.29789 | val_0_rmse: 0.60639 | val_1_rmse: 0.6133  |  0:01:24s
epoch 46 | loss: 0.295   | val_0_rmse: 0.55348 | val_1_rmse: 0.55602 |  0:01:26s
epoch 47 | loss: 0.29395 | val_0_rmse: 0.5529  | val_1_rmse: 0.55352 |  0:01:28s
epoch 48 | loss: 0.29963 | val_0_rmse: 0.55432 | val_1_rmse: 0.55218 |  0:01:30s
epoch 49 | loss: 0.29906 | val_0_rmse: 0.68428 | val_1_rmse: 0.69623 |  0:01:31s
epoch 50 | loss: 0.2946  | val_0_rmse: 2.3059  | val_1_rmse: 2.3346  |  0:01:33s
epoch 51 | loss: 0.30412 | val_0_rmse: 0.78574 | val_1_rmse: 0.78299 |  0:01:35s
epoch 52 | loss: 0.29454 | val_0_rmse: 0.53855 | val_1_rmse: 0.54011 |  0:01:37s
epoch 53 | loss: 0.3     | val_0_rmse: 0.5523  | val_1_rmse: 0.54207 |  0:01:39s
epoch 54 | loss: 0.29791 | val_0_rmse: 0.57807 | val_1_rmse: 0.57778 |  0:01:41s
epoch 55 | loss: 0.30114 | val_0_rmse: 0.57492 | val_1_rmse: 0.57483 |  0:01:42s
epoch 56 | loss: 0.30494 | val_0_rmse: 0.53608 | val_1_rmse: 0.5344  |  0:01:44s
epoch 57 | loss: 0.29936 | val_0_rmse: 0.53928 | val_1_rmse: 0.53952 |  0:01:46s
epoch 58 | loss: 0.29903 | val_0_rmse: 0.55699 | val_1_rmse: 0.55596 |  0:01:48s
epoch 59 | loss: 0.30192 | val_0_rmse: 0.55048 | val_1_rmse: 0.55438 |  0:01:50s
epoch 60 | loss: 0.30539 | val_0_rmse: 0.54019 | val_1_rmse: 0.53938 |  0:01:52s
epoch 61 | loss: 0.29778 | val_0_rmse: 0.53871 | val_1_rmse: 0.54479 |  0:01:53s
epoch 62 | loss: 0.29503 | val_0_rmse: 0.69272 | val_1_rmse: 0.70529 |  0:01:55s
epoch 63 | loss: 0.28866 | val_0_rmse: 0.61541 | val_1_rmse: 0.63208 |  0:01:57s
epoch 64 | loss: 0.29646 | val_0_rmse: 0.56439 | val_1_rmse: 0.56055 |  0:01:59s
epoch 65 | loss: 0.30086 | val_0_rmse: 0.62652 | val_1_rmse: 0.63631 |  0:02:01s
epoch 66 | loss: 0.29601 | val_0_rmse: 0.6165  | val_1_rmse: 0.62547 |  0:02:02s
epoch 67 | loss: 0.29137 | val_0_rmse: 0.56384 | val_1_rmse: 0.57181 |  0:02:04s
epoch 68 | loss: 0.29623 | val_0_rmse: 0.82958 | val_1_rmse: 0.82408 |  0:02:06s
epoch 69 | loss: 0.29452 | val_0_rmse: 0.53685 | val_1_rmse: 0.53699 |  0:02:08s
epoch 70 | loss: 0.29968 | val_0_rmse: 0.60436 | val_1_rmse: 0.61491 |  0:02:10s
epoch 71 | loss: 0.30071 | val_0_rmse: 0.55079 | val_1_rmse: 0.55196 |  0:02:12s
epoch 72 | loss: 0.29493 | val_0_rmse: 0.541   | val_1_rmse: 0.55306 |  0:02:13s
epoch 73 | loss: 0.28455 | val_0_rmse: 0.53187 | val_1_rmse: 0.54542 |  0:02:15s
epoch 74 | loss: 0.29016 | val_0_rmse: 0.55729 | val_1_rmse: 0.55929 |  0:02:17s
epoch 75 | loss: 0.28947 | val_0_rmse: 0.64777 | val_1_rmse: 0.66253 |  0:02:19s
epoch 76 | loss: 0.28875 | val_0_rmse: 0.53993 | val_1_rmse: 0.5488  |  0:02:21s
epoch 77 | loss: 0.29037 | val_0_rmse: 0.53692 | val_1_rmse: 0.54105 |  0:02:23s
epoch 78 | loss: 0.28925 | val_0_rmse: 0.58303 | val_1_rmse: 0.59478 |  0:02:24s
epoch 79 | loss: 0.28702 | val_0_rmse: 0.81827 | val_1_rmse: 0.81379 |  0:02:26s
epoch 80 | loss: 0.29475 | val_0_rmse: 0.5379  | val_1_rmse: 0.5456  |  0:02:28s
epoch 81 | loss: 0.29601 | val_0_rmse: 0.54558 | val_1_rmse: 0.55669 |  0:02:30s
epoch 82 | loss: 0.28958 | val_0_rmse: 0.52902 | val_1_rmse: 0.53733 |  0:02:32s
epoch 83 | loss: 0.28842 | val_0_rmse: 0.53594 | val_1_rmse: 0.53772 |  0:02:33s
epoch 84 | loss: 0.29393 | val_0_rmse: 0.53389 | val_1_rmse: 0.53722 |  0:02:35s
epoch 85 | loss: 0.29438 | val_0_rmse: 0.5362  | val_1_rmse: 0.54023 |  0:02:37s
epoch 86 | loss: 0.2966  | val_0_rmse: 0.54081 | val_1_rmse: 0.5428  |  0:02:39s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.5344
Best weights from best epoch are automatically used!
ended training at: 03:03:07
Feature importance:
[('Area', 0.27836864598752464), ('Baths', 0.013398061326755601), ('Beds', 0.003283007180556855), ('Latitude', 0.11011013733674573), ('Longitude', 0.3025572903477418), ('Month', 0.031690728626185584), ('Year', 0.2605921291944898)]
Mean squared error is of 15982450999.10193
Mean absolute error:81703.95672845872
MAPE:0.44949964130246617
R2 score:0.6956734450652047
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:03:07
epoch 0  | loss: 0.65897 | val_0_rmse: 0.83195 | val_1_rmse: 0.82628 |  0:00:01s
epoch 1  | loss: 0.45221 | val_0_rmse: 0.73203 | val_1_rmse: 0.72331 |  0:00:03s
epoch 2  | loss: 0.39254 | val_0_rmse: 0.67973 | val_1_rmse: 0.68297 |  0:00:05s
epoch 3  | loss: 0.36844 | val_0_rmse: 0.59124 | val_1_rmse: 0.58353 |  0:00:07s
epoch 4  | loss: 0.35655 | val_0_rmse: 0.70258 | val_1_rmse: 0.6977  |  0:00:09s
epoch 5  | loss: 0.34283 | val_0_rmse: 0.58353 | val_1_rmse: 0.58041 |  0:00:11s
epoch 6  | loss: 0.33858 | val_0_rmse: 0.6318  | val_1_rmse: 0.63425 |  0:00:12s
epoch 7  | loss: 0.33676 | val_0_rmse: 0.56441 | val_1_rmse: 0.5666  |  0:00:14s
epoch 8  | loss: 0.33073 | val_0_rmse: 0.57599 | val_1_rmse: 0.57607 |  0:00:16s
epoch 9  | loss: 0.32842 | val_0_rmse: 0.58763 | val_1_rmse: 0.59242 |  0:00:18s
epoch 10 | loss: 0.32523 | val_0_rmse: 0.56029 | val_1_rmse: 0.5562  |  0:00:20s
epoch 11 | loss: 0.3216  | val_0_rmse: 0.57589 | val_1_rmse: 0.57262 |  0:00:22s
epoch 12 | loss: 0.32806 | val_0_rmse: 0.57563 | val_1_rmse: 0.57304 |  0:00:23s
epoch 13 | loss: 0.32745 | val_0_rmse: 0.55775 | val_1_rmse: 0.55397 |  0:00:25s
epoch 14 | loss: 0.31695 | val_0_rmse: 0.58229 | val_1_rmse: 0.58582 |  0:00:27s
epoch 15 | loss: 0.32994 | val_0_rmse: 0.59481 | val_1_rmse: 0.5918  |  0:00:29s
epoch 16 | loss: 0.3301  | val_0_rmse: 0.56107 | val_1_rmse: 0.55817 |  0:00:31s
epoch 17 | loss: 0.32071 | val_0_rmse: 0.55581 | val_1_rmse: 0.55022 |  0:00:33s
epoch 18 | loss: 0.32432 | val_0_rmse: 0.95158 | val_1_rmse: 0.95933 |  0:00:34s
epoch 19 | loss: 0.32152 | val_0_rmse: 0.57324 | val_1_rmse: 0.56328 |  0:00:36s
epoch 20 | loss: 0.32606 | val_0_rmse: 0.56181 | val_1_rmse: 0.555   |  0:00:38s
epoch 21 | loss: 0.32092 | val_0_rmse: 0.59726 | val_1_rmse: 0.59409 |  0:00:40s
epoch 22 | loss: 0.3204  | val_0_rmse: 0.55401 | val_1_rmse: 0.55543 |  0:00:42s
epoch 23 | loss: 0.31275 | val_0_rmse: 0.56651 | val_1_rmse: 0.56403 |  0:00:44s
epoch 24 | loss: 0.32196 | val_0_rmse: 0.5649  | val_1_rmse: 0.56038 |  0:00:46s
epoch 25 | loss: 0.32191 | val_0_rmse: 0.56058 | val_1_rmse: 0.56332 |  0:00:48s
epoch 26 | loss: 0.31439 | val_0_rmse: 0.55899 | val_1_rmse: 0.5632  |  0:00:49s
epoch 27 | loss: 0.31664 | val_0_rmse: 0.55411 | val_1_rmse: 0.55529 |  0:00:51s
epoch 28 | loss: 0.31709 | val_0_rmse: 0.55995 | val_1_rmse: 0.56    |  0:00:53s
epoch 29 | loss: 0.31977 | val_0_rmse: 0.5611  | val_1_rmse: 0.55874 |  0:00:55s
epoch 30 | loss: 0.31933 | val_0_rmse: 0.55842 | val_1_rmse: 0.5531  |  0:00:57s
epoch 31 | loss: 0.32247 | val_0_rmse: 0.57015 | val_1_rmse: 0.56829 |  0:00:59s
epoch 32 | loss: 0.32058 | val_0_rmse: 0.56704 | val_1_rmse: 0.55963 |  0:01:00s
epoch 33 | loss: 0.31499 | val_0_rmse: 0.551   | val_1_rmse: 0.5466  |  0:01:02s
epoch 34 | loss: 0.30888 | val_0_rmse: 0.6113  | val_1_rmse: 0.60353 |  0:01:04s
epoch 35 | loss: 0.30867 | val_0_rmse: 0.54472 | val_1_rmse: 0.54507 |  0:01:06s
epoch 36 | loss: 0.30568 | val_0_rmse: 0.54823 | val_1_rmse: 0.54787 |  0:01:08s
epoch 37 | loss: 0.30344 | val_0_rmse: 0.54359 | val_1_rmse: 0.54592 |  0:01:09s
epoch 38 | loss: 0.30501 | val_0_rmse: 0.54658 | val_1_rmse: 0.54495 |  0:01:11s
epoch 39 | loss: 0.30886 | val_0_rmse: 0.55392 | val_1_rmse: 0.55823 |  0:01:13s
epoch 40 | loss: 0.3124  | val_0_rmse: 0.63557 | val_1_rmse: 0.62868 |  0:01:15s
epoch 41 | loss: 0.3093  | val_0_rmse: 0.5438  | val_1_rmse: 0.54405 |  0:01:17s
epoch 42 | loss: 0.30068 | val_0_rmse: 0.56675 | val_1_rmse: 0.57154 |  0:01:19s
epoch 43 | loss: 0.30419 | val_0_rmse: 0.54806 | val_1_rmse: 0.55001 |  0:01:20s
epoch 44 | loss: 0.30737 | val_0_rmse: 0.70609 | val_1_rmse: 0.69602 |  0:01:22s
epoch 45 | loss: 0.32    | val_0_rmse: 0.56836 | val_1_rmse: 0.57491 |  0:01:24s
epoch 46 | loss: 0.31327 | val_0_rmse: 0.58121 | val_1_rmse: 0.5849  |  0:01:26s
epoch 47 | loss: 0.32546 | val_0_rmse: 0.56496 | val_1_rmse: 0.5598  |  0:01:28s
epoch 48 | loss: 0.32699 | val_0_rmse: 0.57305 | val_1_rmse: 0.57258 |  0:01:29s
epoch 49 | loss: 0.32182 | val_0_rmse: 0.59813 | val_1_rmse: 0.59475 |  0:01:31s
epoch 50 | loss: 0.31297 | val_0_rmse: 0.55099 | val_1_rmse: 0.55397 |  0:01:33s
epoch 51 | loss: 0.31044 | val_0_rmse: 0.54888 | val_1_rmse: 0.55023 |  0:01:35s
epoch 52 | loss: 0.30817 | val_0_rmse: 0.54915 | val_1_rmse: 0.55438 |  0:01:37s
epoch 53 | loss: 0.30608 | val_0_rmse: 0.54779 | val_1_rmse: 0.55213 |  0:01:39s
epoch 54 | loss: 0.30901 | val_0_rmse: 0.55271 | val_1_rmse: 0.55763 |  0:01:41s
epoch 55 | loss: 0.30832 | val_0_rmse: 0.55681 | val_1_rmse: 0.55502 |  0:01:43s
epoch 56 | loss: 0.30159 | val_0_rmse: 0.54568 | val_1_rmse: 0.54684 |  0:01:44s
epoch 57 | loss: 0.30652 | val_0_rmse: 0.54297 | val_1_rmse: 0.54592 |  0:01:46s
epoch 58 | loss: 0.30335 | val_0_rmse: 0.54633 | val_1_rmse: 0.54989 |  0:01:48s
epoch 59 | loss: 0.3009  | val_0_rmse: 0.54425 | val_1_rmse: 0.54574 |  0:01:50s
epoch 60 | loss: 0.30038 | val_0_rmse: 0.5536  | val_1_rmse: 0.55445 |  0:01:52s
epoch 61 | loss: 0.30759 | val_0_rmse: 0.54671 | val_1_rmse: 0.54778 |  0:01:54s
epoch 62 | loss: 0.30232 | val_0_rmse: 0.55748 | val_1_rmse: 0.55286 |  0:01:55s
epoch 63 | loss: 0.30576 | val_0_rmse: 0.56307 | val_1_rmse: 0.5643  |  0:01:57s
epoch 64 | loss: 0.30819 | val_0_rmse: 0.54684 | val_1_rmse: 0.54368 |  0:01:59s
epoch 65 | loss: 0.30702 | val_0_rmse: 0.64814 | val_1_rmse: 0.64153 |  0:02:01s
epoch 66 | loss: 0.30603 | val_0_rmse: 0.55004 | val_1_rmse: 0.54793 |  0:02:03s
epoch 67 | loss: 0.30183 | val_0_rmse: 0.54341 | val_1_rmse: 0.54679 |  0:02:04s
epoch 68 | loss: 0.29986 | val_0_rmse: 0.54017 | val_1_rmse: 0.54303 |  0:02:06s
epoch 69 | loss: 0.29934 | val_0_rmse: 0.54871 | val_1_rmse: 0.55237 |  0:02:08s
epoch 70 | loss: 0.29949 | val_0_rmse: 0.54235 | val_1_rmse: 0.54396 |  0:02:10s
epoch 71 | loss: 0.31419 | val_0_rmse: 0.61454 | val_1_rmse: 0.62413 |  0:02:12s
epoch 72 | loss: 0.31762 | val_0_rmse: 0.55361 | val_1_rmse: 0.55043 |  0:02:14s
epoch 73 | loss: 0.3162  | val_0_rmse: 0.58673 | val_1_rmse: 0.57767 |  0:02:15s
epoch 74 | loss: 0.30877 | val_0_rmse: 0.58137 | val_1_rmse: 0.58706 |  0:02:17s
epoch 75 | loss: 0.30289 | val_0_rmse: 0.55246 | val_1_rmse: 0.55379 |  0:02:19s
epoch 76 | loss: 0.30712 | val_0_rmse: 0.55411 | val_1_rmse: 0.56018 |  0:02:21s
epoch 77 | loss: 0.30231 | val_0_rmse: 0.54466 | val_1_rmse: 0.55423 |  0:02:23s
epoch 78 | loss: 0.30138 | val_0_rmse: 0.71166 | val_1_rmse: 0.6997  |  0:02:24s
epoch 79 | loss: 0.30563 | val_0_rmse: 0.54356 | val_1_rmse: 0.54355 |  0:02:26s
epoch 80 | loss: 0.30112 | val_0_rmse: 0.56818 | val_1_rmse: 0.56509 |  0:02:28s
epoch 81 | loss: 0.30464 | val_0_rmse: 0.54442 | val_1_rmse: 0.553   |  0:02:30s
epoch 82 | loss: 0.30175 | val_0_rmse: 0.54832 | val_1_rmse: 0.5521  |  0:02:32s
epoch 83 | loss: 0.30173 | val_0_rmse: 0.54881 | val_1_rmse: 0.5564  |  0:02:34s
epoch 84 | loss: 0.30772 | val_0_rmse: 0.54492 | val_1_rmse: 0.55026 |  0:02:35s
epoch 85 | loss: 0.303   | val_0_rmse: 0.54055 | val_1_rmse: 0.54623 |  0:02:37s
epoch 86 | loss: 0.29885 | val_0_rmse: 0.53749 | val_1_rmse: 0.53597 |  0:02:39s
epoch 87 | loss: 0.29835 | val_0_rmse: 0.60518 | val_1_rmse: 0.59612 |  0:02:41s
epoch 88 | loss: 0.29633 | val_0_rmse: 0.54191 | val_1_rmse: 0.54168 |  0:02:43s
epoch 89 | loss: 0.29655 | val_0_rmse: 0.55131 | val_1_rmse: 0.55686 |  0:02:45s
epoch 90 | loss: 0.29797 | val_0_rmse: 0.53727 | val_1_rmse: 0.54018 |  0:02:46s
epoch 91 | loss: 0.30227 | val_0_rmse: 0.54758 | val_1_rmse: 0.55011 |  0:02:48s
epoch 92 | loss: 0.30672 | val_0_rmse: 0.5434  | val_1_rmse: 0.54651 |  0:02:50s
epoch 93 | loss: 0.30327 | val_0_rmse: 0.5408  | val_1_rmse: 0.54423 |  0:02:52s
epoch 94 | loss: 0.29713 | val_0_rmse: 0.53603 | val_1_rmse: 0.54114 |  0:02:54s
epoch 95 | loss: 0.29664 | val_0_rmse: 0.54564 | val_1_rmse: 0.55251 |  0:02:55s
epoch 96 | loss: 0.29496 | val_0_rmse: 0.53474 | val_1_rmse: 0.53984 |  0:02:57s
epoch 97 | loss: 0.29602 | val_0_rmse: 0.53696 | val_1_rmse: 0.54592 |  0:02:59s
epoch 98 | loss: 0.29693 | val_0_rmse: 0.7124  | val_1_rmse: 0.70514 |  0:03:01s
epoch 99 | loss: 0.30319 | val_0_rmse: 0.59708 | val_1_rmse: 0.60632 |  0:03:03s
epoch 100| loss: 0.29873 | val_0_rmse: 0.53595 | val_1_rmse: 0.54024 |  0:03:05s
epoch 101| loss: 0.29429 | val_0_rmse: 0.55098 | val_1_rmse: 0.55092 |  0:03:07s
epoch 102| loss: 0.29947 | val_0_rmse: 0.54017 | val_1_rmse: 0.54729 |  0:03:08s
epoch 103| loss: 0.29621 | val_0_rmse: 0.53296 | val_1_rmse: 0.53716 |  0:03:10s
epoch 104| loss: 0.29299 | val_0_rmse: 0.53495 | val_1_rmse: 0.54103 |  0:03:12s
epoch 105| loss: 0.29612 | val_0_rmse: 0.53914 | val_1_rmse: 0.54141 |  0:03:14s
epoch 106| loss: 0.29295 | val_0_rmse: 0.53346 | val_1_rmse: 0.54287 |  0:03:16s
epoch 107| loss: 0.29203 | val_0_rmse: 0.53333 | val_1_rmse: 0.53981 |  0:03:18s
epoch 108| loss: 0.29339 | val_0_rmse: 0.55717 | val_1_rmse: 0.55528 |  0:03:19s
epoch 109| loss: 0.2931  | val_0_rmse: 0.52865 | val_1_rmse: 0.53501 |  0:03:21s
epoch 110| loss: 0.2893  | val_0_rmse: 0.53076 | val_1_rmse: 0.53747 |  0:03:23s
epoch 111| loss: 0.29434 | val_0_rmse: 0.55533 | val_1_rmse: 0.55686 |  0:03:25s
epoch 112| loss: 0.30002 | val_0_rmse: 0.55661 | val_1_rmse: 0.56509 |  0:03:27s
epoch 113| loss: 0.29311 | val_0_rmse: 0.56535 | val_1_rmse: 0.56354 |  0:03:28s
epoch 114| loss: 0.29292 | val_0_rmse: 0.55526 | val_1_rmse: 0.56129 |  0:03:30s
epoch 115| loss: 0.29277 | val_0_rmse: 0.53988 | val_1_rmse: 0.54459 |  0:03:32s
epoch 116| loss: 0.292   | val_0_rmse: 0.53213 | val_1_rmse: 0.53674 |  0:03:34s
epoch 117| loss: 0.29192 | val_0_rmse: 0.53645 | val_1_rmse: 0.53833 |  0:03:36s
epoch 118| loss: 0.29025 | val_0_rmse: 0.5329  | val_1_rmse: 0.54683 |  0:03:37s
epoch 119| loss: 0.2873  | val_0_rmse: 0.53794 | val_1_rmse: 0.54417 |  0:03:39s
epoch 120| loss: 0.29337 | val_0_rmse: 0.54052 | val_1_rmse: 0.549   |  0:03:41s
epoch 121| loss: 0.29132 | val_0_rmse: 0.53948 | val_1_rmse: 0.54018 |  0:03:43s
epoch 122| loss: 0.29564 | val_0_rmse: 0.53833 | val_1_rmse: 0.54332 |  0:03:45s
epoch 123| loss: 0.29619 | val_0_rmse: 0.54269 | val_1_rmse: 0.54972 |  0:03:46s
epoch 124| loss: 0.2888  | val_0_rmse: 0.56251 | val_1_rmse: 0.56205 |  0:03:48s
epoch 125| loss: 0.28828 | val_0_rmse: 0.52549 | val_1_rmse: 0.5338  |  0:03:50s
epoch 126| loss: 0.288   | val_0_rmse: 0.52671 | val_1_rmse: 0.53801 |  0:03:52s
epoch 127| loss: 0.28895 | val_0_rmse: 0.53444 | val_1_rmse: 0.54056 |  0:03:54s
epoch 128| loss: 0.28882 | val_0_rmse: 0.52947 | val_1_rmse: 0.5381  |  0:03:56s
epoch 129| loss: 0.28686 | val_0_rmse: 0.5261  | val_1_rmse: 0.53476 |  0:03:57s
epoch 130| loss: 0.29025 | val_0_rmse: 0.52718 | val_1_rmse: 0.53491 |  0:03:59s
epoch 131| loss: 0.28605 | val_0_rmse: 0.52765 | val_1_rmse: 0.54074 |  0:04:01s
epoch 132| loss: 0.28872 | val_0_rmse: 0.52701 | val_1_rmse: 0.53864 |  0:04:03s
epoch 133| loss: 0.28954 | val_0_rmse: 0.65353 | val_1_rmse: 0.64623 |  0:04:05s
epoch 134| loss: 0.29007 | val_0_rmse: 0.54496 | val_1_rmse: 0.5495  |  0:04:06s
epoch 135| loss: 0.29275 | val_0_rmse: 0.53601 | val_1_rmse: 0.54443 |  0:04:08s
epoch 136| loss: 0.28857 | val_0_rmse: 0.53184 | val_1_rmse: 0.53802 |  0:04:10s
epoch 137| loss: 0.28671 | val_0_rmse: 0.53164 | val_1_rmse: 0.53506 |  0:04:12s
epoch 138| loss: 0.28599 | val_0_rmse: 0.54227 | val_1_rmse: 0.54811 |  0:04:14s
epoch 139| loss: 0.28934 | val_0_rmse: 0.54456 | val_1_rmse: 0.54954 |  0:04:15s
epoch 140| loss: 0.28982 | val_0_rmse: 0.5315  | val_1_rmse: 0.54325 |  0:04:17s
epoch 141| loss: 0.28685 | val_0_rmse: 0.52852 | val_1_rmse: 0.54437 |  0:04:19s
epoch 142| loss: 0.29611 | val_0_rmse: 0.53273 | val_1_rmse: 0.54119 |  0:04:21s
epoch 143| loss: 0.30003 | val_0_rmse: 0.53886 | val_1_rmse: 0.54529 |  0:04:23s
epoch 144| loss: 0.28792 | val_0_rmse: 0.55958 | val_1_rmse: 0.5628  |  0:04:24s
epoch 145| loss: 0.28329 | val_0_rmse: 0.53185 | val_1_rmse: 0.53782 |  0:04:26s
epoch 146| loss: 0.28439 | val_0_rmse: 0.52789 | val_1_rmse: 0.5368  |  0:04:28s
epoch 147| loss: 0.28424 | val_0_rmse: 0.5396  | val_1_rmse: 0.54734 |  0:04:30s
epoch 148| loss: 0.30399 | val_0_rmse: 0.5686  | val_1_rmse: 0.58681 |  0:04:32s
epoch 149| loss: 0.30379 | val_0_rmse: 0.54932 | val_1_rmse: 0.55565 |  0:04:34s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.5338
Best weights from best epoch are automatically used!
ended training at: 03:07:42
Feature importance:
[('Area', 0.18067276316839231), ('Baths', 0.03279850016061678), ('Beds', 0.019122458446730085), ('Latitude', 0.09140751331490049), ('Longitude', 0.5715522181813426), ('Month', 0.04885757924113269), ('Year', 0.05558896748688499)]
Mean squared error is of 14852822287.021395
Mean absolute error:78812.80119991576
MAPE:0.4015965331484365
R2 score:0.7205913071627291
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:07:42
epoch 0  | loss: 0.71724 | val_0_rmse: 0.76221 | val_1_rmse: 0.76466 |  0:00:01s
epoch 1  | loss: 0.41458 | val_0_rmse: 0.64022 | val_1_rmse: 0.63932 |  0:00:03s
epoch 2  | loss: 0.38381 | val_0_rmse: 0.60638 | val_1_rmse: 0.60524 |  0:00:05s
epoch 3  | loss: 0.3686  | val_0_rmse: 0.6415  | val_1_rmse: 0.63881 |  0:00:07s
epoch 4  | loss: 0.35213 | val_0_rmse: 0.57599 | val_1_rmse: 0.58202 |  0:00:09s
epoch 5  | loss: 0.34789 | val_0_rmse: 0.60948 | val_1_rmse: 0.61963 |  0:00:11s
epoch 6  | loss: 0.33933 | val_0_rmse: 0.5711  | val_1_rmse: 0.57708 |  0:00:12s
epoch 7  | loss: 0.34009 | val_0_rmse: 0.62328 | val_1_rmse: 0.6216  |  0:00:14s
epoch 8  | loss: 0.33978 | val_0_rmse: 0.57007 | val_1_rmse: 0.57462 |  0:00:16s
epoch 9  | loss: 0.32992 | val_0_rmse: 0.5649  | val_1_rmse: 0.56565 |  0:00:18s
epoch 10 | loss: 0.32984 | val_0_rmse: 0.55463 | val_1_rmse: 0.55756 |  0:00:20s
epoch 11 | loss: 0.32537 | val_0_rmse: 0.61151 | val_1_rmse: 0.5996  |  0:00:22s
epoch 12 | loss: 0.31992 | val_0_rmse: 0.67535 | val_1_rmse: 0.6822  |  0:00:23s
epoch 13 | loss: 0.32246 | val_0_rmse: 0.55592 | val_1_rmse: 0.55705 |  0:00:25s
epoch 14 | loss: 0.31242 | val_0_rmse: 0.60789 | val_1_rmse: 0.60002 |  0:00:27s
epoch 15 | loss: 0.31605 | val_0_rmse: 0.56672 | val_1_rmse: 0.55614 |  0:00:29s
epoch 16 | loss: 0.3153  | val_0_rmse: 0.55052 | val_1_rmse: 0.55796 |  0:00:31s
epoch 17 | loss: 0.31922 | val_0_rmse: 0.55124 | val_1_rmse: 0.55043 |  0:00:33s
epoch 18 | loss: 0.31081 | val_0_rmse: 0.54525 | val_1_rmse: 0.54326 |  0:00:34s
epoch 19 | loss: 0.31568 | val_0_rmse: 0.54259 | val_1_rmse: 0.54633 |  0:00:36s
epoch 20 | loss: 0.31347 | val_0_rmse: 0.67145 | val_1_rmse: 0.67119 |  0:00:38s
epoch 21 | loss: 0.30802 | val_0_rmse: 0.53748 | val_1_rmse: 0.53833 |  0:00:40s
epoch 22 | loss: 0.29983 | val_0_rmse: 0.53918 | val_1_rmse: 0.54623 |  0:00:42s
epoch 23 | loss: 0.3021  | val_0_rmse: 0.53738 | val_1_rmse: 0.53888 |  0:00:44s
epoch 24 | loss: 0.30213 | val_0_rmse: 0.86634 | val_1_rmse: 0.84015 |  0:00:45s
epoch 25 | loss: 0.30387 | val_0_rmse: 0.62278 | val_1_rmse: 0.62228 |  0:00:47s
epoch 26 | loss: 0.29878 | val_0_rmse: 0.53913 | val_1_rmse: 0.53871 |  0:00:49s
epoch 27 | loss: 0.30381 | val_0_rmse: 0.55575 | val_1_rmse: 0.5601  |  0:00:51s
epoch 28 | loss: 0.30425 | val_0_rmse: 0.56319 | val_1_rmse: 0.56349 |  0:00:53s
epoch 29 | loss: 0.30304 | val_0_rmse: 0.58893 | val_1_rmse: 0.58536 |  0:00:55s
epoch 30 | loss: 0.29792 | val_0_rmse: 0.53649 | val_1_rmse: 0.53594 |  0:00:57s
epoch 31 | loss: 0.30194 | val_0_rmse: 0.55657 | val_1_rmse: 0.56286 |  0:00:58s
epoch 32 | loss: 0.30406 | val_0_rmse: 0.59088 | val_1_rmse: 0.59413 |  0:01:00s
epoch 33 | loss: 0.29745 | val_0_rmse: 0.53883 | val_1_rmse: 0.53371 |  0:01:02s
epoch 34 | loss: 0.2976  | val_0_rmse: 0.56407 | val_1_rmse: 0.55903 |  0:01:04s
epoch 35 | loss: 0.2998  | val_0_rmse: 0.65323 | val_1_rmse: 0.65913 |  0:01:06s
epoch 36 | loss: 0.29778 | val_0_rmse: 0.58279 | val_1_rmse: 0.58326 |  0:01:07s
epoch 37 | loss: 0.2961  | val_0_rmse: 0.61561 | val_1_rmse: 0.61941 |  0:01:09s
epoch 38 | loss: 0.29639 | val_0_rmse: 0.86166 | val_1_rmse: 0.84406 |  0:01:11s
epoch 39 | loss: 0.29892 | val_0_rmse: 0.54909 | val_1_rmse: 0.55066 |  0:01:13s
epoch 40 | loss: 0.29464 | val_0_rmse: 0.55104 | val_1_rmse: 0.55934 |  0:01:15s
epoch 41 | loss: 0.29554 | val_0_rmse: 0.53514 | val_1_rmse: 0.5415  |  0:01:17s
epoch 42 | loss: 0.29523 | val_0_rmse: 0.5385  | val_1_rmse: 0.54188 |  0:01:18s
epoch 43 | loss: 0.29414 | val_0_rmse: 0.53067 | val_1_rmse: 0.53162 |  0:01:20s
epoch 44 | loss: 0.29349 | val_0_rmse: 0.54158 | val_1_rmse: 0.54594 |  0:01:22s
epoch 45 | loss: 0.29447 | val_0_rmse: 0.53269 | val_1_rmse: 0.5306  |  0:01:24s
epoch 46 | loss: 0.2934  | val_0_rmse: 0.53699 | val_1_rmse: 0.53855 |  0:01:26s
epoch 47 | loss: 0.29052 | val_0_rmse: 0.53974 | val_1_rmse: 0.54822 |  0:01:28s
epoch 48 | loss: 0.2911  | val_0_rmse: 0.53372 | val_1_rmse: 0.53643 |  0:01:29s
epoch 49 | loss: 0.29106 | val_0_rmse: 0.53904 | val_1_rmse: 0.54799 |  0:01:31s
epoch 50 | loss: 0.29097 | val_0_rmse: 0.55373 | val_1_rmse: 0.55442 |  0:01:33s
epoch 51 | loss: 0.28854 | val_0_rmse: 0.59166 | val_1_rmse: 0.59066 |  0:01:35s
epoch 52 | loss: 0.29108 | val_0_rmse: 0.53442 | val_1_rmse: 0.53762 |  0:01:37s
epoch 53 | loss: 0.28808 | val_0_rmse: 0.53374 | val_1_rmse: 0.53428 |  0:01:38s
epoch 54 | loss: 0.28942 | val_0_rmse: 0.57962 | val_1_rmse: 0.58751 |  0:01:40s
epoch 55 | loss: 0.28849 | val_0_rmse: 0.56739 | val_1_rmse: 0.57123 |  0:01:42s
epoch 56 | loss: 0.28652 | val_0_rmse: 0.56935 | val_1_rmse: 0.57865 |  0:01:44s
epoch 57 | loss: 0.29072 | val_0_rmse: 0.53056 | val_1_rmse: 0.52869 |  0:01:46s
epoch 58 | loss: 0.29172 | val_0_rmse: 0.62493 | val_1_rmse: 0.62907 |  0:01:48s
epoch 59 | loss: 0.2897  | val_0_rmse: 0.58251 | val_1_rmse: 0.59358 |  0:01:49s
epoch 60 | loss: 0.28805 | val_0_rmse: 0.60728 | val_1_rmse: 0.61396 |  0:01:51s
epoch 61 | loss: 0.28691 | val_0_rmse: 0.56926 | val_1_rmse: 0.58039 |  0:01:53s
epoch 62 | loss: 0.2907  | val_0_rmse: 2.57443 | val_1_rmse: 2.48256 |  0:01:55s
epoch 63 | loss: 0.29616 | val_0_rmse: 0.8657  | val_1_rmse: 0.83371 |  0:01:57s
epoch 64 | loss: 0.29332 | val_0_rmse: 0.54124 | val_1_rmse: 0.54043 |  0:01:58s
epoch 65 | loss: 0.29175 | val_0_rmse: 0.53356 | val_1_rmse: 0.5353  |  0:02:00s
epoch 66 | loss: 0.29423 | val_0_rmse: 0.66139 | val_1_rmse: 0.66514 |  0:02:02s
epoch 67 | loss: 0.28989 | val_0_rmse: 0.53156 | val_1_rmse: 0.5388  |  0:02:04s
epoch 68 | loss: 0.28619 | val_0_rmse: 0.60378 | val_1_rmse: 0.61484 |  0:02:06s
epoch 69 | loss: 0.28693 | val_0_rmse: 0.61973 | val_1_rmse: 0.62406 |  0:02:08s
epoch 70 | loss: 0.28883 | val_0_rmse: 0.54414 | val_1_rmse: 0.55245 |  0:02:10s
epoch 71 | loss: 0.28527 | val_0_rmse: 0.603   | val_1_rmse: 0.6119  |  0:02:11s
epoch 72 | loss: 0.29246 | val_0_rmse: 0.52401 | val_1_rmse: 0.52621 |  0:02:13s
epoch 73 | loss: 0.29345 | val_0_rmse: 0.53805 | val_1_rmse: 0.53738 |  0:02:15s
epoch 74 | loss: 0.2852  | val_0_rmse: 0.52828 | val_1_rmse: 0.5362  |  0:02:17s
epoch 75 | loss: 0.28678 | val_0_rmse: 0.51976 | val_1_rmse: 0.52351 |  0:02:19s
epoch 76 | loss: 0.28427 | val_0_rmse: 0.54961 | val_1_rmse: 0.55799 |  0:02:20s
epoch 77 | loss: 0.28927 | val_0_rmse: 0.56208 | val_1_rmse: 0.57437 |  0:02:22s
epoch 78 | loss: 0.28348 | val_0_rmse: 0.5398  | val_1_rmse: 0.54839 |  0:02:24s
epoch 79 | loss: 0.28136 | val_0_rmse: 0.5222  | val_1_rmse: 0.52446 |  0:02:26s
epoch 80 | loss: 0.28383 | val_0_rmse: 0.53281 | val_1_rmse: 0.53176 |  0:02:28s
epoch 81 | loss: 0.28438 | val_0_rmse: 0.52904 | val_1_rmse: 0.53678 |  0:02:30s
epoch 82 | loss: 0.28669 | val_0_rmse: 0.53257 | val_1_rmse: 0.54664 |  0:02:31s
epoch 83 | loss: 0.28638 | val_0_rmse: 0.5258  | val_1_rmse: 0.53118 |  0:02:33s
epoch 84 | loss: 0.28065 | val_0_rmse: 0.54    | val_1_rmse: 0.5499  |  0:02:35s
epoch 85 | loss: 0.27863 | val_0_rmse: 0.53702 | val_1_rmse: 0.55238 |  0:02:37s
epoch 86 | loss: 0.28357 | val_0_rmse: 0.57255 | val_1_rmse: 0.57981 |  0:02:39s
epoch 87 | loss: 0.28575 | val_0_rmse: 0.55195 | val_1_rmse: 0.56692 |  0:02:41s
epoch 88 | loss: 0.28    | val_0_rmse: 0.54819 | val_1_rmse: 0.56407 |  0:02:42s
epoch 89 | loss: 0.28227 | val_0_rmse: 0.53961 | val_1_rmse: 0.5497  |  0:02:44s
epoch 90 | loss: 0.28156 | val_0_rmse: 0.54444 | val_1_rmse: 0.55833 |  0:02:46s
epoch 91 | loss: 0.28144 | val_0_rmse: 0.62576 | val_1_rmse: 0.63666 |  0:02:48s
epoch 92 | loss: 0.27656 | val_0_rmse: 0.52119 | val_1_rmse: 0.52631 |  0:02:50s
epoch 93 | loss: 0.2824  | val_0_rmse: 0.54665 | val_1_rmse: 0.5493  |  0:02:51s
epoch 94 | loss: 0.27875 | val_0_rmse: 0.63132 | val_1_rmse: 0.6425  |  0:02:53s
epoch 95 | loss: 0.27776 | val_0_rmse: 0.65281 | val_1_rmse: 0.66628 |  0:02:55s
epoch 96 | loss: 0.27531 | val_0_rmse: 0.62995 | val_1_rmse: 0.64762 |  0:02:57s
epoch 97 | loss: 0.27961 | val_0_rmse: 0.67599 | val_1_rmse: 0.68176 |  0:02:59s
epoch 98 | loss: 0.28184 | val_0_rmse: 0.83676 | val_1_rmse: 0.81075 |  0:03:00s
epoch 99 | loss: 0.27819 | val_0_rmse: 0.52584 | val_1_rmse: 0.53655 |  0:03:02s
epoch 100| loss: 0.28785 | val_0_rmse: 0.56204 | val_1_rmse: 0.57277 |  0:03:04s
epoch 101| loss: 0.2822  | val_0_rmse: 0.52705 | val_1_rmse: 0.54571 |  0:03:06s
epoch 102| loss: 0.28152 | val_0_rmse: 0.53471 | val_1_rmse: 0.54441 |  0:03:08s
epoch 103| loss: 0.2802  | val_0_rmse: 0.56003 | val_1_rmse: 0.57272 |  0:03:10s
epoch 104| loss: 0.2744  | val_0_rmse: 0.58851 | val_1_rmse: 0.60136 |  0:03:11s
epoch 105| loss: 0.27468 | val_0_rmse: 0.58638 | val_1_rmse: 0.60357 |  0:03:13s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.52351
Best weights from best epoch are automatically used!
ended training at: 03:10:57
Feature importance:
[('Area', 0.18687929999613878), ('Baths', 0.05014712147994006), ('Beds', 0.008261748153702623), ('Latitude', 0.19539213439655875), ('Longitude', 0.3662794161191616), ('Month', 0.02627289446941242), ('Year', 0.16676738538508576)]
Mean squared error is of 16162978344.73761
Mean absolute error:81743.65204868866
MAPE:0.4097772251979282
R2 score:0.7090363149686513
------------------------------------------------------------------
