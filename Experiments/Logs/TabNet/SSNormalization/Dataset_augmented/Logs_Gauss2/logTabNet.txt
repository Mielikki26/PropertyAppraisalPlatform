TabNet Logs:

Saving copy of script...
In this script all datasets are increased in size up to the size of the biggest dataset by sampling random rows and modifying them with a noise depending on the standard deviation of the value in questionThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:09:27
epoch 0  | loss: 0.57816 | val_0_rmse: 0.64768 | val_1_rmse: 0.6536  |  0:00:08s
epoch 1  | loss: 0.41425 | val_0_rmse: 0.61141 | val_1_rmse: 0.61427 |  0:00:15s
epoch 2  | loss: 0.39596 | val_0_rmse: 0.60208 | val_1_rmse: 0.60685 |  0:00:21s
epoch 3  | loss: 0.36483 | val_0_rmse: 0.58    | val_1_rmse: 0.5863  |  0:00:27s
epoch 4  | loss: 0.35852 | val_0_rmse: 0.58572 | val_1_rmse: 0.58995 |  0:00:33s
epoch 5  | loss: 0.35802 | val_0_rmse: 0.57402 | val_1_rmse: 0.58108 |  0:00:40s
epoch 6  | loss: 0.34178 | val_0_rmse: 0.57371 | val_1_rmse: 0.57912 |  0:00:46s
epoch 7  | loss: 0.33894 | val_0_rmse: 0.56946 | val_1_rmse: 0.57584 |  0:00:52s
epoch 8  | loss: 0.34577 | val_0_rmse: 0.57818 | val_1_rmse: 0.58534 |  0:00:58s
epoch 9  | loss: 0.33831 | val_0_rmse: 0.58979 | val_1_rmse: 0.59118 |  0:01:05s
epoch 10 | loss: 0.34131 | val_0_rmse: 0.55586 | val_1_rmse: 0.56068 |  0:01:11s
epoch 11 | loss: 0.32463 | val_0_rmse: 0.56352 | val_1_rmse: 0.57261 |  0:01:17s
epoch 12 | loss: 0.32774 | val_0_rmse: 0.56871 | val_1_rmse: 0.57576 |  0:01:23s
epoch 13 | loss: 0.3267  | val_0_rmse: 0.55565 | val_1_rmse: 0.56193 |  0:01:29s
epoch 14 | loss: 0.32373 | val_0_rmse: 0.56898 | val_1_rmse: 0.57566 |  0:01:36s
epoch 15 | loss: 0.31638 | val_0_rmse: 0.5579  | val_1_rmse: 0.56529 |  0:01:42s
epoch 16 | loss: 0.32187 | val_0_rmse: 0.57016 | val_1_rmse: 0.57846 |  0:01:48s
epoch 17 | loss: 0.32595 | val_0_rmse: 0.54834 | val_1_rmse: 0.55571 |  0:01:54s
epoch 18 | loss: 0.31259 | val_0_rmse: 0.53251 | val_1_rmse: 0.54003 |  0:02:01s
epoch 19 | loss: 0.31271 | val_0_rmse: 0.54353 | val_1_rmse: 0.55294 |  0:02:07s
epoch 20 | loss: 0.3167  | val_0_rmse: 0.53    | val_1_rmse: 0.53712 |  0:02:13s
epoch 21 | loss: 0.31179 | val_0_rmse: 0.53483 | val_1_rmse: 0.54104 |  0:02:20s
epoch 22 | loss: 0.31905 | val_0_rmse: 0.58937 | val_1_rmse: 0.59323 |  0:02:26s
epoch 23 | loss: 0.32876 | val_0_rmse: 0.56812 | val_1_rmse: 0.57463 |  0:02:32s
epoch 24 | loss: 0.3337  | val_0_rmse: 0.5505  | val_1_rmse: 0.55719 |  0:02:39s
epoch 25 | loss: 0.32283 | val_0_rmse: 0.54739 | val_1_rmse: 0.55395 |  0:02:45s
epoch 26 | loss: 0.31694 | val_0_rmse: 0.54995 | val_1_rmse: 0.55714 |  0:02:51s
epoch 27 | loss: 0.31109 | val_0_rmse: 0.54127 | val_1_rmse: 0.54703 |  0:02:57s
epoch 28 | loss: 0.31829 | val_0_rmse: 0.55613 | val_1_rmse: 0.56383 |  0:03:03s
epoch 29 | loss: 0.32109 | val_0_rmse: 0.54554 | val_1_rmse: 0.55673 |  0:03:10s
epoch 30 | loss: 0.31366 | val_0_rmse: 0.55113 | val_1_rmse: 0.55681 |  0:03:16s
epoch 31 | loss: 0.32135 | val_0_rmse: 0.54814 | val_1_rmse: 0.55459 |  0:03:22s
epoch 32 | loss: 0.3078  | val_0_rmse: 0.52273 | val_1_rmse: 0.53293 |  0:03:28s
epoch 33 | loss: 0.30199 | val_0_rmse: 0.54661 | val_1_rmse: 0.55674 |  0:03:35s
epoch 34 | loss: 0.30359 | val_0_rmse: 0.54244 | val_1_rmse: 0.55106 |  0:03:41s
epoch 35 | loss: 0.31035 | val_0_rmse: 0.52773 | val_1_rmse: 0.5363  |  0:03:47s
epoch 36 | loss: 0.29858 | val_0_rmse: 0.53702 | val_1_rmse: 0.54414 |  0:03:54s
epoch 37 | loss: 0.2993  | val_0_rmse: 0.52657 | val_1_rmse: 0.53572 |  0:04:00s
epoch 38 | loss: 0.29628 | val_0_rmse: 0.55184 | val_1_rmse: 0.55877 |  0:04:06s
epoch 39 | loss: 0.30152 | val_0_rmse: 0.56007 | val_1_rmse: 0.56413 |  0:04:13s
epoch 40 | loss: 0.31741 | val_0_rmse: 0.57391 | val_1_rmse: 0.5825  |  0:04:19s
epoch 41 | loss: 0.31081 | val_0_rmse: 0.53152 | val_1_rmse: 0.53948 |  0:04:25s
epoch 42 | loss: 0.32019 | val_0_rmse: 0.61235 | val_1_rmse: 0.61433 |  0:04:32s
epoch 43 | loss: 0.32583 | val_0_rmse: 0.55196 | val_1_rmse: 0.56155 |  0:04:38s
epoch 44 | loss: 0.32986 | val_0_rmse: 0.57775 | val_1_rmse: 0.58581 |  0:04:44s
epoch 45 | loss: 0.32397 | val_0_rmse: 0.60649 | val_1_rmse: 0.61472 |  0:04:50s
epoch 46 | loss: 0.33191 | val_0_rmse: 0.62621 | val_1_rmse: 0.6338  |  0:04:57s
epoch 47 | loss: 0.32299 | val_0_rmse: 0.54275 | val_1_rmse: 0.55044 |  0:05:03s
epoch 48 | loss: 0.30334 | val_0_rmse: 0.53155 | val_1_rmse: 0.53862 |  0:05:09s
epoch 49 | loss: 0.29625 | val_0_rmse: 0.52154 | val_1_rmse: 0.52781 |  0:05:16s
epoch 50 | loss: 0.29577 | val_0_rmse: 0.52476 | val_1_rmse: 0.53378 |  0:05:22s
epoch 51 | loss: 0.29119 | val_0_rmse: 0.52694 | val_1_rmse: 0.53647 |  0:05:28s
epoch 52 | loss: 0.28949 | val_0_rmse: 0.50363 | val_1_rmse: 0.51158 |  0:05:35s
epoch 53 | loss: 0.28808 | val_0_rmse: 0.50931 | val_1_rmse: 0.51835 |  0:05:41s
epoch 54 | loss: 0.28359 | val_0_rmse: 0.53571 | val_1_rmse: 0.54305 |  0:05:47s
epoch 55 | loss: 0.28961 | val_0_rmse: 0.52319 | val_1_rmse: 0.53002 |  0:05:54s
epoch 56 | loss: 0.28549 | val_0_rmse: 0.5411  | val_1_rmse: 0.54734 |  0:06:00s
epoch 57 | loss: 0.28237 | val_0_rmse: 0.49726 | val_1_rmse: 0.50466 |  0:06:06s
epoch 58 | loss: 0.27287 | val_0_rmse: 0.49752 | val_1_rmse: 0.50586 |  0:06:12s
epoch 59 | loss: 0.27863 | val_0_rmse: 0.49918 | val_1_rmse: 0.50849 |  0:06:20s
epoch 60 | loss: 0.27767 | val_0_rmse: 0.49328 | val_1_rmse: 0.50087 |  0:06:26s
epoch 61 | loss: 0.28001 | val_0_rmse: 0.49567 | val_1_rmse: 0.50235 |  0:06:32s
epoch 62 | loss: 0.27445 | val_0_rmse: 0.49994 | val_1_rmse: 0.5081  |  0:06:39s
epoch 63 | loss: 0.2838  | val_0_rmse: 0.5191  | val_1_rmse: 0.5294  |  0:06:45s
epoch 64 | loss: 0.28622 | val_0_rmse: 0.56133 | val_1_rmse: 0.56726 |  0:06:51s
epoch 65 | loss: 0.29857 | val_0_rmse: 0.52441 | val_1_rmse: 0.53253 |  0:06:57s
epoch 66 | loss: 0.28157 | val_0_rmse: 0.49573 | val_1_rmse: 0.5037  |  0:07:04s
epoch 67 | loss: 0.27792 | val_0_rmse: 0.50136 | val_1_rmse: 0.50986 |  0:07:10s
epoch 68 | loss: 0.27592 | val_0_rmse: 0.50986 | val_1_rmse: 0.51678 |  0:07:16s
epoch 69 | loss: 0.28589 | val_0_rmse: 0.53331 | val_1_rmse: 0.5383  |  0:07:23s
epoch 70 | loss: 0.2819  | val_0_rmse: 0.51563 | val_1_rmse: 0.52022 |  0:07:29s
epoch 71 | loss: 0.28002 | val_0_rmse: 0.5113  | val_1_rmse: 0.5169  |  0:07:35s
epoch 72 | loss: 0.27274 | val_0_rmse: 0.52058 | val_1_rmse: 0.5223  |  0:07:42s
epoch 73 | loss: 0.28419 | val_0_rmse: 0.52033 | val_1_rmse: 0.52679 |  0:07:48s
epoch 74 | loss: 0.27679 | val_0_rmse: 0.4917  | val_1_rmse: 0.5012  |  0:07:54s
epoch 75 | loss: 0.27896 | val_0_rmse: 0.50334 | val_1_rmse: 0.51161 |  0:08:01s
epoch 76 | loss: 0.27111 | val_0_rmse: 0.49367 | val_1_rmse: 0.50302 |  0:08:07s
epoch 77 | loss: 0.27214 | val_0_rmse: 0.51901 | val_1_rmse: 0.5281  |  0:08:13s
epoch 78 | loss: 0.27456 | val_0_rmse: 0.50875 | val_1_rmse: 0.51794 |  0:08:20s
epoch 79 | loss: 0.26578 | val_0_rmse: 0.50096 | val_1_rmse: 0.51096 |  0:08:26s
epoch 80 | loss: 0.27323 | val_0_rmse: 0.53028 | val_1_rmse: 0.53664 |  0:08:32s
epoch 81 | loss: 0.27636 | val_0_rmse: 0.53169 | val_1_rmse: 0.53837 |  0:08:39s
epoch 82 | loss: 0.27569 | val_0_rmse: 0.50515 | val_1_rmse: 0.51324 |  0:08:45s
epoch 83 | loss: 0.2695  | val_0_rmse: 0.49187 | val_1_rmse: 0.50288 |  0:08:51s
epoch 84 | loss: 0.26353 | val_0_rmse: 0.51225 | val_1_rmse: 0.52258 |  0:08:58s
epoch 85 | loss: 0.26671 | val_0_rmse: 0.50505 | val_1_rmse: 0.51385 |  0:09:04s
epoch 86 | loss: 0.28385 | val_0_rmse: 0.53824 | val_1_rmse: 0.54396 |  0:09:10s
epoch 87 | loss: 0.28135 | val_0_rmse: 0.50935 | val_1_rmse: 0.51595 |  0:09:16s
epoch 88 | loss: 0.27561 | val_0_rmse: 0.49348 | val_1_rmse: 0.50178 |  0:09:23s
epoch 89 | loss: 0.2664  | val_0_rmse: 0.505   | val_1_rmse: 0.51014 |  0:09:29s
epoch 90 | loss: 0.26883 | val_0_rmse: 0.49028 | val_1_rmse: 0.49755 |  0:09:35s
epoch 91 | loss: 0.27045 | val_0_rmse: 0.50507 | val_1_rmse: 0.51265 |  0:09:42s
epoch 92 | loss: 0.26601 | val_0_rmse: 0.49735 | val_1_rmse: 0.50863 |  0:09:48s
epoch 93 | loss: 0.2641  | val_0_rmse: 0.48834 | val_1_rmse: 0.49786 |  0:09:54s
epoch 94 | loss: 0.25855 | val_0_rmse: 0.48109 | val_1_rmse: 0.49173 |  0:10:01s
epoch 95 | loss: 0.26724 | val_0_rmse: 0.49552 | val_1_rmse: 0.5064  |  0:10:07s
epoch 96 | loss: 0.27053 | val_0_rmse: 0.51183 | val_1_rmse: 0.51956 |  0:10:13s
epoch 97 | loss: 0.27183 | val_0_rmse: 0.49236 | val_1_rmse: 0.50184 |  0:10:19s
epoch 98 | loss: 0.2627  | val_0_rmse: 0.50155 | val_1_rmse: 0.51192 |  0:10:26s
epoch 99 | loss: 0.26051 | val_0_rmse: 0.49066 | val_1_rmse: 0.50093 |  0:10:32s
epoch 100| loss: 0.26263 | val_0_rmse: 0.49462 | val_1_rmse: 0.5055  |  0:10:38s
epoch 101| loss: 0.26842 | val_0_rmse: 0.50082 | val_1_rmse: 0.50903 |  0:10:45s
epoch 102| loss: 0.26463 | val_0_rmse: 0.50391 | val_1_rmse: 0.51205 |  0:10:51s
epoch 103| loss: 0.25827 | val_0_rmse: 0.49023 | val_1_rmse: 0.50025 |  0:10:57s
epoch 104| loss: 0.25634 | val_0_rmse: 0.50824 | val_1_rmse: 0.51661 |  0:11:04s
epoch 105| loss: 0.25561 | val_0_rmse: 0.49981 | val_1_rmse: 0.5105  |  0:11:10s
epoch 106| loss: 0.2529  | val_0_rmse: 0.47079 | val_1_rmse: 0.48278 |  0:11:16s
epoch 107| loss: 0.24944 | val_0_rmse: 0.48212 | val_1_rmse: 0.49448 |  0:11:23s
epoch 108| loss: 0.2551  | val_0_rmse: 0.48662 | val_1_rmse: 0.49732 |  0:11:29s
epoch 109| loss: 0.25479 | val_0_rmse: 0.507   | val_1_rmse: 0.51594 |  0:11:36s
epoch 110| loss: 0.25232 | val_0_rmse: 0.47646 | val_1_rmse: 0.48727 |  0:11:42s
epoch 111| loss: 0.24926 | val_0_rmse: 0.51932 | val_1_rmse: 0.52804 |  0:11:48s
epoch 112| loss: 0.27444 | val_0_rmse: 0.60521 | val_1_rmse: 0.61472 |  0:11:55s
epoch 113| loss: 0.25559 | val_0_rmse: 0.50885 | val_1_rmse: 0.52068 |  0:12:01s
epoch 114| loss: 0.2591  | val_0_rmse: 0.47115 | val_1_rmse: 0.48422 |  0:12:07s
epoch 115| loss: 0.25334 | val_0_rmse: 0.47467 | val_1_rmse: 0.48571 |  0:12:14s
epoch 116| loss: 0.25051 | val_0_rmse: 0.48722 | val_1_rmse: 0.50179 |  0:12:20s
epoch 117| loss: 0.25377 | val_0_rmse: 0.49809 | val_1_rmse: 0.51092 |  0:12:26s
epoch 118| loss: 0.25359 | val_0_rmse: 0.49866 | val_1_rmse: 0.50934 |  0:12:33s
epoch 119| loss: 0.25048 | val_0_rmse: 0.47414 | val_1_rmse: 0.48603 |  0:12:39s
epoch 120| loss: 0.31071 | val_0_rmse: 0.60018 | val_1_rmse: 0.60489 |  0:12:45s
epoch 121| loss: 0.33021 | val_0_rmse: 0.55738 | val_1_rmse: 0.56516 |  0:12:52s
epoch 122| loss: 0.31278 | val_0_rmse: 0.52466 | val_1_rmse: 0.53429 |  0:12:58s
epoch 123| loss: 0.31756 | val_0_rmse: 0.55231 | val_1_rmse: 0.5589  |  0:13:04s
epoch 124| loss: 0.31482 | val_0_rmse: 0.55098 | val_1_rmse: 0.56182 |  0:13:11s
epoch 125| loss: 0.31207 | val_0_rmse: 0.55431 | val_1_rmse: 0.56122 |  0:13:17s
epoch 126| loss: 0.30871 | val_0_rmse: 0.56325 | val_1_rmse: 0.57302 |  0:13:23s
epoch 127| loss: 0.29557 | val_0_rmse: 0.51158 | val_1_rmse: 0.5215  |  0:13:30s
epoch 128| loss: 0.2878  | val_0_rmse: 0.51866 | val_1_rmse: 0.52654 |  0:13:36s
epoch 129| loss: 0.27663 | val_0_rmse: 0.50735 | val_1_rmse: 0.51501 |  0:13:42s
epoch 130| loss: 0.27518 | val_0_rmse: 0.49274 | val_1_rmse: 0.50152 |  0:13:49s
epoch 131| loss: 0.27088 | val_0_rmse: 0.49684 | val_1_rmse: 0.50608 |  0:13:55s
epoch 132| loss: 0.28216 | val_0_rmse: 0.54483 | val_1_rmse: 0.55347 |  0:14:01s
epoch 133| loss: 0.26744 | val_0_rmse: 0.49246 | val_1_rmse: 0.50294 |  0:14:08s
epoch 134| loss: 0.26301 | val_0_rmse: 0.50359 | val_1_rmse: 0.51111 |  0:14:14s
epoch 135| loss: 0.26364 | val_0_rmse: 0.49355 | val_1_rmse: 0.50192 |  0:14:21s
epoch 136| loss: 0.26143 | val_0_rmse: 0.49612 | val_1_rmse: 0.50565 |  0:14:27s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.48278
Best weights from best epoch are automatically used!
ended training at: 21:23:57
Feature importance:
[('Area', 0.2988426836068764), ('Baths', 0.03959101456712035), ('Beds', 0.16219392170193586), ('Latitude', 0.2828569452046619), ('Longitude', 0.21651543491940548), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 5258961624.691596
Mean absolute error:50328.58801374053
MAPE:0.16583551863917487
R2 score:0.7690693525064011
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:23:58
epoch 0  | loss: 0.6059  | val_0_rmse: 0.64933 | val_1_rmse: 0.65782 |  0:00:06s
epoch 1  | loss: 0.39865 | val_0_rmse: 0.63122 | val_1_rmse: 0.64044 |  0:00:12s
epoch 2  | loss: 0.37297 | val_0_rmse: 0.61537 | val_1_rmse: 0.62669 |  0:00:19s
epoch 3  | loss: 0.35201 | val_0_rmse: 0.57141 | val_1_rmse: 0.57974 |  0:00:25s
epoch 4  | loss: 0.3473  | val_0_rmse: 0.58454 | val_1_rmse: 0.59455 |  0:00:31s
epoch 5  | loss: 0.34341 | val_0_rmse: 0.60268 | val_1_rmse: 0.613   |  0:00:38s
epoch 6  | loss: 0.33516 | val_0_rmse: 0.57206 | val_1_rmse: 0.58384 |  0:00:44s
epoch 7  | loss: 0.32736 | val_0_rmse: 0.5611  | val_1_rmse: 0.57348 |  0:00:51s
epoch 8  | loss: 0.32303 | val_0_rmse: 0.55438 | val_1_rmse: 0.56788 |  0:00:57s
epoch 9  | loss: 0.31598 | val_0_rmse: 0.54673 | val_1_rmse: 0.55845 |  0:01:03s
epoch 10 | loss: 0.31436 | val_0_rmse: 0.54587 | val_1_rmse: 0.55568 |  0:01:10s
epoch 11 | loss: 0.31623 | val_0_rmse: 0.55312 | val_1_rmse: 0.56721 |  0:01:16s
epoch 12 | loss: 0.31983 | val_0_rmse: 0.54757 | val_1_rmse: 0.55965 |  0:01:22s
epoch 13 | loss: 0.3228  | val_0_rmse: 0.57019 | val_1_rmse: 0.5827  |  0:01:29s
epoch 14 | loss: 0.32267 | val_0_rmse: 0.55408 | val_1_rmse: 0.56421 |  0:01:35s
epoch 15 | loss: 0.3183  | val_0_rmse: 0.53835 | val_1_rmse: 0.55113 |  0:01:42s
epoch 16 | loss: 0.31657 | val_0_rmse: 0.57824 | val_1_rmse: 0.58931 |  0:01:48s
epoch 17 | loss: 0.32849 | val_0_rmse: 0.57374 | val_1_rmse: 0.58157 |  0:01:54s
epoch 18 | loss: 0.31353 | val_0_rmse: 0.53624 | val_1_rmse: 0.5472  |  0:02:01s
epoch 19 | loss: 0.30728 | val_0_rmse: 0.54537 | val_1_rmse: 0.55455 |  0:02:08s
epoch 20 | loss: 0.32426 | val_0_rmse: 0.56209 | val_1_rmse: 0.5745  |  0:02:14s
epoch 21 | loss: 0.32949 | val_0_rmse: 0.55227 | val_1_rmse: 0.56336 |  0:02:21s
epoch 22 | loss: 0.31401 | val_0_rmse: 0.64323 | val_1_rmse: 0.65488 |  0:02:27s
epoch 23 | loss: 0.31493 | val_0_rmse: 0.62116 | val_1_rmse: 0.62501 |  0:02:34s
epoch 24 | loss: 0.30609 | val_0_rmse: 0.56882 | val_1_rmse: 0.57819 |  0:02:40s
epoch 25 | loss: 0.30772 | val_0_rmse: 0.56771 | val_1_rmse: 0.57836 |  0:02:47s
epoch 26 | loss: 0.2998  | val_0_rmse: 0.5551  | val_1_rmse: 0.56433 |  0:02:53s
epoch 27 | loss: 0.30032 | val_0_rmse: 0.55123 | val_1_rmse: 0.56486 |  0:02:59s
epoch 28 | loss: 0.29791 | val_0_rmse: 0.53849 | val_1_rmse: 0.55085 |  0:03:06s
epoch 29 | loss: 0.30267 | val_0_rmse: 0.51846 | val_1_rmse: 0.5278  |  0:03:12s
epoch 30 | loss: 0.29186 | val_0_rmse: 0.55169 | val_1_rmse: 0.56096 |  0:03:19s
epoch 31 | loss: 0.29347 | val_0_rmse: 0.52656 | val_1_rmse: 0.53816 |  0:03:25s
epoch 32 | loss: 0.28724 | val_0_rmse: 0.51888 | val_1_rmse: 0.53147 |  0:03:31s
epoch 33 | loss: 0.29393 | val_0_rmse: 0.52509 | val_1_rmse: 0.53858 |  0:03:38s
epoch 34 | loss: 0.28953 | val_0_rmse: 0.53691 | val_1_rmse: 0.55098 |  0:03:44s
epoch 35 | loss: 0.28699 | val_0_rmse: 0.51725 | val_1_rmse: 0.52928 |  0:03:50s
epoch 36 | loss: 0.29464 | val_0_rmse: 0.53425 | val_1_rmse: 0.54462 |  0:03:57s
epoch 37 | loss: 0.29641 | val_0_rmse: 0.54649 | val_1_rmse: 0.55375 |  0:04:03s
epoch 38 | loss: 0.29797 | val_0_rmse: 0.55427 | val_1_rmse: 0.56732 |  0:04:10s
epoch 39 | loss: 0.30953 | val_0_rmse: 0.53599 | val_1_rmse: 0.54409 |  0:04:16s
epoch 40 | loss: 0.32966 | val_0_rmse: 0.5582  | val_1_rmse: 0.56787 |  0:04:22s
epoch 41 | loss: 0.3474  | val_0_rmse: 0.57116 | val_1_rmse: 0.57533 |  0:04:29s
epoch 42 | loss: 0.33572 | val_0_rmse: 0.56242 | val_1_rmse: 0.57497 |  0:04:35s
epoch 43 | loss: 0.3353  | val_0_rmse: 0.56097 | val_1_rmse: 0.57148 |  0:04:42s
epoch 44 | loss: 0.32894 | val_0_rmse: 0.54035 | val_1_rmse: 0.54577 |  0:04:48s
epoch 45 | loss: 0.31858 | val_0_rmse: 0.56312 | val_1_rmse: 0.57213 |  0:04:54s
epoch 46 | loss: 0.31198 | val_0_rmse: 0.60305 | val_1_rmse: 0.60844 |  0:05:01s
epoch 47 | loss: 0.32137 | val_0_rmse: 0.57113 | val_1_rmse: 0.57763 |  0:05:07s
epoch 48 | loss: 0.33157 | val_0_rmse: 0.59436 | val_1_rmse: 0.60029 |  0:05:13s
epoch 49 | loss: 0.31688 | val_0_rmse: 0.54059 | val_1_rmse: 0.5451  |  0:05:20s
epoch 50 | loss: 0.31711 | val_0_rmse: 0.55795 | val_1_rmse: 0.56406 |  0:05:26s
epoch 51 | loss: 0.31635 | val_0_rmse: 0.56009 | val_1_rmse: 0.56749 |  0:05:32s
epoch 52 | loss: 0.31417 | val_0_rmse: 0.54948 | val_1_rmse: 0.55145 |  0:05:39s
epoch 53 | loss: 0.3079  | val_0_rmse: 0.60857 | val_1_rmse: 0.61351 |  0:05:45s
epoch 54 | loss: 0.31039 | val_0_rmse: 0.53954 | val_1_rmse: 0.5461  |  0:05:52s
epoch 55 | loss: 0.30383 | val_0_rmse: 0.59649 | val_1_rmse: 0.60149 |  0:05:58s
epoch 56 | loss: 0.30554 | val_0_rmse: 0.56592 | val_1_rmse: 0.57099 |  0:06:04s
epoch 57 | loss: 0.30593 | val_0_rmse: 0.52551 | val_1_rmse: 0.532   |  0:06:11s
epoch 58 | loss: 0.31301 | val_0_rmse: 0.58281 | val_1_rmse: 0.58882 |  0:06:17s
epoch 59 | loss: 0.30748 | val_0_rmse: 0.53241 | val_1_rmse: 0.54149 |  0:06:24s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.5278
Best weights from best epoch are automatically used!
ended training at: 21:30:24
Feature importance:
[('Area', 0.4027145647972587), ('Baths', 0.012142277979131987), ('Beds', 0.0), ('Latitude', 0.25171539174832014), ('Longitude', 0.2985961450533747), ('Month', 0.002091922928496646), ('Year', 0.03273969749341789)]
Mean squared error is of 6395134851.284045
Mean absolute error:54829.82180981098
MAPE:0.18716811984312284
R2 score:0.7177425122714729
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:30:25
epoch 0  | loss: 0.60649 | val_0_rmse: 0.71672 | val_1_rmse: 0.71007 |  0:00:06s
epoch 1  | loss: 0.49967 | val_0_rmse: 0.67909 | val_1_rmse: 0.67459 |  0:00:12s
epoch 2  | loss: 0.45075 | val_0_rmse: 0.64178 | val_1_rmse: 0.6411  |  0:00:19s
epoch 3  | loss: 0.42148 | val_0_rmse: 0.62858 | val_1_rmse: 0.62813 |  0:00:26s
epoch 4  | loss: 0.40565 | val_0_rmse: 0.66358 | val_1_rmse: 0.66156 |  0:00:32s
epoch 5  | loss: 0.41054 | val_0_rmse: 0.62144 | val_1_rmse: 0.62487 |  0:00:38s
epoch 6  | loss: 0.39764 | val_0_rmse: 0.66069 | val_1_rmse: 0.66375 |  0:00:45s
epoch 7  | loss: 0.38764 | val_0_rmse: 0.70012 | val_1_rmse: 0.70493 |  0:00:51s
epoch 8  | loss: 0.37303 | val_0_rmse: 0.67802 | val_1_rmse: 0.67529 |  0:00:58s
epoch 9  | loss: 0.36069 | val_0_rmse: 0.64676 | val_1_rmse: 0.6479  |  0:01:04s
epoch 10 | loss: 0.36011 | val_0_rmse: 0.6248  | val_1_rmse: 0.62599 |  0:01:10s
epoch 11 | loss: 0.35702 | val_0_rmse: 0.62749 | val_1_rmse: 0.62833 |  0:01:17s
epoch 12 | loss: 0.35157 | val_0_rmse: 0.67518 | val_1_rmse: 0.67251 |  0:01:23s
epoch 13 | loss: 0.3594  | val_0_rmse: 0.62923 | val_1_rmse: 0.62933 |  0:01:30s
epoch 14 | loss: 0.34783 | val_0_rmse: 0.57872 | val_1_rmse: 0.58262 |  0:01:36s
epoch 15 | loss: 0.34753 | val_0_rmse: 0.58272 | val_1_rmse: 0.5819  |  0:01:42s
epoch 16 | loss: 0.34264 | val_0_rmse: 0.63197 | val_1_rmse: 0.63226 |  0:01:49s
epoch 17 | loss: 0.34183 | val_0_rmse: 0.58932 | val_1_rmse: 0.59501 |  0:01:55s
epoch 18 | loss: 0.3395  | val_0_rmse: 0.61684 | val_1_rmse: 0.61673 |  0:02:02s
epoch 19 | loss: 0.34466 | val_0_rmse: 0.59189 | val_1_rmse: 0.59418 |  0:02:08s
epoch 20 | loss: 0.34206 | val_0_rmse: 0.63479 | val_1_rmse: 0.63652 |  0:02:15s
epoch 21 | loss: 0.34507 | val_0_rmse: 0.60716 | val_1_rmse: 0.60597 |  0:02:21s
epoch 22 | loss: 0.34008 | val_0_rmse: 0.62043 | val_1_rmse: 0.62004 |  0:02:27s
epoch 23 | loss: 0.33487 | val_0_rmse: 0.58523 | val_1_rmse: 0.5881  |  0:02:34s
epoch 24 | loss: 0.33432 | val_0_rmse: 0.58729 | val_1_rmse: 0.59069 |  0:02:40s
epoch 25 | loss: 0.33439 | val_0_rmse: 0.57552 | val_1_rmse: 0.57821 |  0:02:47s
epoch 26 | loss: 0.3444  | val_0_rmse: 0.59702 | val_1_rmse: 0.59706 |  0:02:53s
epoch 27 | loss: 0.33906 | val_0_rmse: 0.56002 | val_1_rmse: 0.5638  |  0:03:00s
epoch 28 | loss: 0.33466 | val_0_rmse: 0.56408 | val_1_rmse: 0.56781 |  0:03:06s
epoch 29 | loss: 0.33608 | val_0_rmse: 0.80824 | val_1_rmse: 0.81163 |  0:03:13s
epoch 30 | loss: 0.33271 | val_0_rmse: 0.60952 | val_1_rmse: 0.61343 |  0:03:19s
epoch 31 | loss: 0.33178 | val_0_rmse: 0.58697 | val_1_rmse: 0.58849 |  0:03:26s
epoch 32 | loss: 0.32835 | val_0_rmse: 0.61834 | val_1_rmse: 0.61997 |  0:03:32s
epoch 33 | loss: 0.33128 | val_0_rmse: 0.65513 | val_1_rmse: 0.65873 |  0:03:38s
epoch 34 | loss: 0.33131 | val_0_rmse: 0.59129 | val_1_rmse: 0.59294 |  0:03:45s
epoch 35 | loss: 0.33284 | val_0_rmse: 0.59184 | val_1_rmse: 0.59288 |  0:03:51s
epoch 36 | loss: 0.33114 | val_0_rmse: 0.64366 | val_1_rmse: 0.6445  |  0:03:58s
epoch 37 | loss: 0.32828 | val_0_rmse: 0.59422 | val_1_rmse: 0.59356 |  0:04:04s
epoch 38 | loss: 0.33013 | val_0_rmse: 0.62724 | val_1_rmse: 0.63208 |  0:04:11s
epoch 39 | loss: 0.32829 | val_0_rmse: 0.61677 | val_1_rmse: 0.61606 |  0:04:17s
epoch 40 | loss: 0.32936 | val_0_rmse: 0.82648 | val_1_rmse: 0.82752 |  0:04:23s
epoch 41 | loss: 0.32918 | val_0_rmse: 0.64853 | val_1_rmse: 0.65021 |  0:04:30s
epoch 42 | loss: 0.32531 | val_0_rmse: 0.64863 | val_1_rmse: 0.64681 |  0:04:36s
epoch 43 | loss: 0.3251  | val_0_rmse: 0.60807 | val_1_rmse: 0.60959 |  0:04:43s
epoch 44 | loss: 0.3213  | val_0_rmse: 0.5611  | val_1_rmse: 0.56298 |  0:04:49s
epoch 45 | loss: 0.32552 | val_0_rmse: 0.64402 | val_1_rmse: 0.64369 |  0:04:56s
epoch 46 | loss: 0.32539 | val_0_rmse: 0.62172 | val_1_rmse: 0.62081 |  0:05:02s
epoch 47 | loss: 0.32386 | val_0_rmse: 0.55893 | val_1_rmse: 0.56243 |  0:05:08s
epoch 48 | loss: 0.31962 | val_0_rmse: 0.6929  | val_1_rmse: 0.69623 |  0:05:15s
epoch 49 | loss: 0.324   | val_0_rmse: 0.67621 | val_1_rmse: 0.6808  |  0:05:21s
epoch 50 | loss: 0.32085 | val_0_rmse: 0.59124 | val_1_rmse: 0.59381 |  0:05:28s
epoch 51 | loss: 0.3267  | val_0_rmse: 0.55523 | val_1_rmse: 0.5579  |  0:05:34s
epoch 52 | loss: 0.3235  | val_0_rmse: 0.68027 | val_1_rmse: 0.68366 |  0:05:41s
epoch 53 | loss: 0.32133 | val_0_rmse: 0.67477 | val_1_rmse: 0.67743 |  0:05:47s
epoch 54 | loss: 0.32367 | val_0_rmse: 0.68419 | val_1_rmse: 0.6847  |  0:05:54s
epoch 55 | loss: 0.32032 | val_0_rmse: 0.98493 | val_1_rmse: 0.98229 |  0:06:00s
epoch 56 | loss: 0.34615 | val_0_rmse: 0.59129 | val_1_rmse: 0.59351 |  0:06:06s
epoch 57 | loss: 0.32257 | val_0_rmse: 0.56441 | val_1_rmse: 0.56975 |  0:06:13s
epoch 58 | loss: 0.32376 | val_0_rmse: 0.57529 | val_1_rmse: 0.58014 |  0:06:19s
epoch 59 | loss: 0.31702 | val_0_rmse: 0.60074 | val_1_rmse: 0.60308 |  0:06:26s
epoch 60 | loss: 0.319   | val_0_rmse: 0.66135 | val_1_rmse: 0.66416 |  0:06:32s
epoch 61 | loss: 0.31887 | val_0_rmse: 0.58613 | val_1_rmse: 0.58841 |  0:06:39s
epoch 62 | loss: 0.31659 | val_0_rmse: 0.57607 | val_1_rmse: 0.58112 |  0:06:45s
epoch 63 | loss: 0.31871 | val_0_rmse: 0.60297 | val_1_rmse: 0.605   |  0:06:52s
epoch 64 | loss: 0.31417 | val_0_rmse: 0.59856 | val_1_rmse: 0.60359 |  0:06:58s
epoch 65 | loss: 0.31167 | val_0_rmse: 0.8109  | val_1_rmse: 0.81578 |  0:07:05s
epoch 66 | loss: 0.31164 | val_0_rmse: 0.58127 | val_1_rmse: 0.58506 |  0:07:11s
epoch 67 | loss: 0.31269 | val_0_rmse: 0.61443 | val_1_rmse: 0.61558 |  0:07:18s
epoch 68 | loss: 0.31234 | val_0_rmse: 0.63577 | val_1_rmse: 0.63945 |  0:07:24s
epoch 69 | loss: 0.31123 | val_0_rmse: 0.69875 | val_1_rmse: 0.6986  |  0:07:31s
epoch 70 | loss: 0.31486 | val_0_rmse: 0.59193 | val_1_rmse: 0.59372 |  0:07:37s
epoch 71 | loss: 0.30998 | val_0_rmse: 0.76933 | val_1_rmse: 0.76858 |  0:07:44s
epoch 72 | loss: 0.30783 | val_0_rmse: 0.61671 | val_1_rmse: 0.61839 |  0:07:50s
epoch 73 | loss: 0.31053 | val_0_rmse: 0.54445 | val_1_rmse: 0.55197 |  0:07:57s
epoch 74 | loss: 0.31748 | val_0_rmse: 0.58657 | val_1_rmse: 0.58805 |  0:08:03s
epoch 75 | loss: 0.31474 | val_0_rmse: 0.60238 | val_1_rmse: 0.6045  |  0:08:10s
epoch 76 | loss: 0.31462 | val_0_rmse: 0.88368 | val_1_rmse: 0.88749 |  0:08:16s
epoch 77 | loss: 0.31403 | val_0_rmse: 0.58573 | val_1_rmse: 0.58966 |  0:08:23s
epoch 78 | loss: 0.32353 | val_0_rmse: 0.74417 | val_1_rmse: 0.7432  |  0:08:29s
epoch 79 | loss: 0.31646 | val_0_rmse: 0.62682 | val_1_rmse: 0.62727 |  0:08:36s
epoch 80 | loss: 0.30765 | val_0_rmse: 0.65718 | val_1_rmse: 0.65559 |  0:08:42s
epoch 81 | loss: 0.31263 | val_0_rmse: 0.60151 | val_1_rmse: 0.60399 |  0:08:49s
epoch 82 | loss: 0.31004 | val_0_rmse: 0.58695 | val_1_rmse: 0.59239 |  0:08:55s
epoch 83 | loss: 0.33182 | val_0_rmse: 0.70065 | val_1_rmse: 0.70323 |  0:09:01s
epoch 84 | loss: 0.33003 | val_0_rmse: 0.58702 | val_1_rmse: 0.58769 |  0:09:08s
epoch 85 | loss: 0.31842 | val_0_rmse: 0.56681 | val_1_rmse: 0.56994 |  0:09:14s
epoch 86 | loss: 0.31196 | val_0_rmse: 0.67191 | val_1_rmse: 0.67422 |  0:09:21s
epoch 87 | loss: 0.30979 | val_0_rmse: 0.5589  | val_1_rmse: 0.56344 |  0:09:27s
epoch 88 | loss: 0.31116 | val_0_rmse: 0.69764 | val_1_rmse: 0.69578 |  0:09:34s
epoch 89 | loss: 0.30785 | val_0_rmse: 0.62278 | val_1_rmse: 0.62486 |  0:09:41s
epoch 90 | loss: 0.30785 | val_0_rmse: 0.6575  | val_1_rmse: 0.65731 |  0:09:47s
epoch 91 | loss: 0.30874 | val_0_rmse: 0.61631 | val_1_rmse: 0.61762 |  0:09:54s
epoch 92 | loss: 0.30905 | val_0_rmse: 0.56253 | val_1_rmse: 0.56611 |  0:10:00s
epoch 93 | loss: 0.30781 | val_0_rmse: 0.64244 | val_1_rmse: 0.64428 |  0:10:07s
epoch 94 | loss: 0.31096 | val_0_rmse: 0.60154 | val_1_rmse: 0.60158 |  0:10:13s
epoch 95 | loss: 0.3184  | val_0_rmse: 0.65676 | val_1_rmse: 0.66067 |  0:10:19s
epoch 96 | loss: 0.31096 | val_0_rmse: 0.60146 | val_1_rmse: 0.60158 |  0:10:26s
epoch 97 | loss: 0.30831 | val_0_rmse: 0.66583 | val_1_rmse: 0.66895 |  0:10:32s
epoch 98 | loss: 0.30763 | val_0_rmse: 0.82707 | val_1_rmse: 0.82928 |  0:10:39s
epoch 99 | loss: 0.31056 | val_0_rmse: 0.59291 | val_1_rmse: 0.5941  |  0:10:45s
epoch 100| loss: 0.30661 | val_0_rmse: 0.58779 | val_1_rmse: 0.59175 |  0:10:52s
epoch 101| loss: 0.30631 | val_0_rmse: 0.59148 | val_1_rmse: 0.59401 |  0:10:58s
epoch 102| loss: 0.30567 | val_0_rmse: 0.59    | val_1_rmse: 0.5943  |  0:11:05s
epoch 103| loss: 0.30859 | val_0_rmse: 0.62074 | val_1_rmse: 0.6204  |  0:11:11s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.55197
Best weights from best epoch are automatically used!
ended training at: 21:41:39
Feature importance:
[('Area', 0.2870083679918979), ('Baths', 0.06497683786842726), ('Beds', 0.04570782690245581), ('Latitude', 0.15696718308801327), ('Longitude', 0.3454642196731745), ('Month', 0.05077931214226172), ('Year', 0.04909625233376954)]
Mean squared error is of 1943107558.3374784
Mean absolute error:31293.951759373384
MAPE:0.2959975051239342
R2 score:0.7031656891291829
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:41:40
epoch 0  | loss: 0.60385 | val_0_rmse: 0.73395 | val_1_rmse: 0.7323  |  0:00:06s
epoch 1  | loss: 0.52115 | val_0_rmse: 0.71657 | val_1_rmse: 0.71898 |  0:00:13s
epoch 2  | loss: 0.50021 | val_0_rmse: 0.68823 | val_1_rmse: 0.68731 |  0:00:19s
epoch 3  | loss: 0.45478 | val_0_rmse: 0.755   | val_1_rmse: 0.75422 |  0:00:26s
epoch 4  | loss: 0.44413 | val_0_rmse: 0.77118 | val_1_rmse: 0.77353 |  0:00:32s
epoch 5  | loss: 0.42412 | val_0_rmse: 0.664   | val_1_rmse: 0.66265 |  0:00:39s
epoch 6  | loss: 0.39242 | val_0_rmse: 0.63082 | val_1_rmse: 0.63237 |  0:00:45s
epoch 7  | loss: 0.39066 | val_0_rmse: 0.72029 | val_1_rmse: 0.71766 |  0:00:52s
epoch 8  | loss: 0.39029 | val_0_rmse: 0.68396 | val_1_rmse: 0.68634 |  0:00:58s
epoch 9  | loss: 0.37138 | val_0_rmse: 0.74567 | val_1_rmse: 0.75083 |  0:01:05s
epoch 10 | loss: 0.36174 | val_0_rmse: 0.62004 | val_1_rmse: 0.62079 |  0:01:11s
epoch 11 | loss: 0.35146 | val_0_rmse: 0.70087 | val_1_rmse: 0.70494 |  0:01:17s
epoch 12 | loss: 0.35221 | val_0_rmse: 0.69478 | val_1_rmse: 0.69933 |  0:01:24s
epoch 13 | loss: 0.34398 | val_0_rmse: 0.6546  | val_1_rmse: 0.65828 |  0:01:30s
epoch 14 | loss: 0.34003 | val_0_rmse: 0.6343  | val_1_rmse: 0.63235 |  0:01:36s
epoch 15 | loss: 0.34529 | val_0_rmse: 0.61372 | val_1_rmse: 0.61159 |  0:01:43s
epoch 16 | loss: 0.34064 | val_0_rmse: 0.64153 | val_1_rmse: 0.64475 |  0:01:49s
epoch 17 | loss: 0.33806 | val_0_rmse: 0.69684 | val_1_rmse: 0.70182 |  0:01:55s
epoch 18 | loss: 0.33571 | val_0_rmse: 0.6344  | val_1_rmse: 0.63239 |  0:02:02s
epoch 19 | loss: 0.33378 | val_0_rmse: 0.68287 | val_1_rmse: 0.68871 |  0:02:08s
epoch 20 | loss: 0.33342 | val_0_rmse: 0.69334 | val_1_rmse: 0.69856 |  0:02:14s
epoch 21 | loss: 0.33126 | val_0_rmse: 0.62221 | val_1_rmse: 0.62415 |  0:02:21s
epoch 22 | loss: 0.32864 | val_0_rmse: 0.60469 | val_1_rmse: 0.60805 |  0:02:27s
epoch 23 | loss: 0.33075 | val_0_rmse: 0.74367 | val_1_rmse: 0.75007 |  0:02:34s
epoch 24 | loss: 0.33211 | val_0_rmse: 0.67077 | val_1_rmse: 0.67593 |  0:02:40s
epoch 25 | loss: 0.33344 | val_0_rmse: 0.98713 | val_1_rmse: 0.99368 |  0:02:46s
epoch 26 | loss: 0.3264  | val_0_rmse: 0.63136 | val_1_rmse: 0.63377 |  0:02:53s
epoch 27 | loss: 0.32779 | val_0_rmse: 0.65198 | val_1_rmse: 0.65654 |  0:02:59s
epoch 28 | loss: 0.32903 | val_0_rmse: 0.64586 | val_1_rmse: 0.64327 |  0:03:06s
epoch 29 | loss: 0.33188 | val_0_rmse: 0.59553 | val_1_rmse: 0.59657 |  0:03:12s
epoch 30 | loss: 0.33504 | val_0_rmse: 0.68145 | val_1_rmse: 0.68436 |  0:03:18s
epoch 31 | loss: 0.32825 | val_0_rmse: 0.5965  | val_1_rmse: 0.59713 |  0:03:25s
epoch 32 | loss: 0.33099 | val_0_rmse: 0.79058 | val_1_rmse: 0.79657 |  0:03:31s
epoch 33 | loss: 0.32545 | val_0_rmse: 0.70929 | val_1_rmse: 0.70901 |  0:03:38s
epoch 34 | loss: 0.32223 | val_0_rmse: 0.58433 | val_1_rmse: 0.58748 |  0:03:44s
epoch 35 | loss: 0.32045 | val_0_rmse: 0.62745 | val_1_rmse: 0.63356 |  0:03:50s
epoch 36 | loss: 0.32048 | val_0_rmse: 0.57285 | val_1_rmse: 0.57116 |  0:03:57s
epoch 37 | loss: 0.32297 | val_0_rmse: 0.66056 | val_1_rmse: 0.66412 |  0:04:03s
epoch 38 | loss: 0.32177 | val_0_rmse: 0.85754 | val_1_rmse: 0.85217 |  0:04:10s
epoch 39 | loss: 0.3208  | val_0_rmse: 2.29487 | val_1_rmse: 2.29353 |  0:04:16s
epoch 40 | loss: 0.32465 | val_0_rmse: 0.5736  | val_1_rmse: 0.57588 |  0:04:23s
epoch 41 | loss: 0.31674 | val_0_rmse: 0.64142 | val_1_rmse: 0.64072 |  0:04:29s
epoch 42 | loss: 0.31876 | val_0_rmse: 0.55384 | val_1_rmse: 0.55618 |  0:04:36s
epoch 43 | loss: 0.31736 | val_0_rmse: 1.21377 | val_1_rmse: 1.20548 |  0:04:42s
epoch 44 | loss: 0.31753 | val_0_rmse: 0.61297 | val_1_rmse: 0.61116 |  0:04:49s
epoch 45 | loss: 0.32407 | val_0_rmse: 0.62105 | val_1_rmse: 0.62381 |  0:04:55s
epoch 46 | loss: 0.32049 | val_0_rmse: 0.63955 | val_1_rmse: 0.64467 |  0:05:01s
epoch 47 | loss: 0.32243 | val_0_rmse: 0.564   | val_1_rmse: 0.56536 |  0:05:08s
epoch 48 | loss: 0.31926 | val_0_rmse: 0.62296 | val_1_rmse: 0.62153 |  0:05:14s
epoch 49 | loss: 0.31986 | val_0_rmse: 0.67776 | val_1_rmse: 0.68162 |  0:05:21s
epoch 50 | loss: 0.31535 | val_0_rmse: 0.56884 | val_1_rmse: 0.56919 |  0:05:27s
epoch 51 | loss: 0.32183 | val_0_rmse: 0.66889 | val_1_rmse: 0.67109 |  0:05:33s
epoch 52 | loss: 0.31501 | val_0_rmse: 0.63267 | val_1_rmse: 0.63456 |  0:05:40s
epoch 53 | loss: 0.3163  | val_0_rmse: 0.71616 | val_1_rmse: 0.7211  |  0:05:46s
epoch 54 | loss: 0.31685 | val_0_rmse: 0.62229 | val_1_rmse: 0.62198 |  0:05:53s
epoch 55 | loss: 0.31753 | val_0_rmse: 0.71553 | val_1_rmse: 0.72144 |  0:05:59s
epoch 56 | loss: 0.31785 | val_0_rmse: 0.66579 | val_1_rmse: 0.67044 |  0:06:05s
epoch 57 | loss: 0.31681 | val_0_rmse: 0.5767  | val_1_rmse: 0.57884 |  0:06:12s
epoch 58 | loss: 0.31416 | val_0_rmse: 0.71183 | val_1_rmse: 0.70823 |  0:06:18s
epoch 59 | loss: 0.32238 | val_0_rmse: 0.91018 | val_1_rmse: 0.90629 |  0:06:25s
epoch 60 | loss: 0.3175  | val_0_rmse: 0.6135  | val_1_rmse: 0.61667 |  0:06:31s
epoch 61 | loss: 0.31613 | val_0_rmse: 1.21171 | val_1_rmse: 1.20845 |  0:06:37s
epoch 62 | loss: 0.32418 | val_0_rmse: 0.62727 | val_1_rmse: 0.62974 |  0:06:44s
epoch 63 | loss: 0.31452 | val_0_rmse: 0.70376 | val_1_rmse: 0.7088  |  0:06:50s
epoch 64 | loss: 0.31635 | val_0_rmse: 0.72537 | val_1_rmse: 0.72769 |  0:06:57s
epoch 65 | loss: 0.32376 | val_0_rmse: 0.76474 | val_1_rmse: 0.76865 |  0:07:03s
epoch 66 | loss: 0.31818 | val_0_rmse: 0.68992 | val_1_rmse: 0.69201 |  0:07:10s
epoch 67 | loss: 0.31267 | val_0_rmse: 0.56828 | val_1_rmse: 0.56828 |  0:07:16s
epoch 68 | loss: 0.31467 | val_0_rmse: 0.67226 | val_1_rmse: 0.67617 |  0:07:22s
epoch 69 | loss: 0.31343 | val_0_rmse: 0.71534 | val_1_rmse: 0.7104  |  0:07:29s
epoch 70 | loss: 0.31304 | val_0_rmse: 0.62655 | val_1_rmse: 0.62915 |  0:07:35s
epoch 71 | loss: 0.31628 | val_0_rmse: 0.65357 | val_1_rmse: 0.6567  |  0:07:42s
epoch 72 | loss: 0.31369 | val_0_rmse: 0.59171 | val_1_rmse: 0.59385 |  0:07:48s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.55618
Best weights from best epoch are automatically used!
ended training at: 21:49:31
Feature importance:
[('Area', 0.4360970246776699), ('Baths', 0.22721645154305356), ('Beds', 0.0), ('Latitude', 0.23201165541406552), ('Longitude', 0.0), ('Month', 0.10467486836521103), ('Year', 0.0)]
Mean squared error is of 2072511528.381651
Mean absolute error:32441.815031606067
MAPE:0.31841789769655193
R2 score:0.6850088031543935
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:09
epoch 0  | loss: 0.43718 | val_0_rmse: 0.57223 | val_1_rmse: 0.57398 |  0:00:06s
epoch 1  | loss: 0.32515 | val_0_rmse: 0.56984 | val_1_rmse: 0.57458 |  0:00:12s
epoch 2  | loss: 0.31036 | val_0_rmse: 0.55675 | val_1_rmse: 0.56124 |  0:00:19s
epoch 3  | loss: 0.30988 | val_0_rmse: 0.53341 | val_1_rmse: 0.53703 |  0:00:25s
epoch 4  | loss: 0.28938 | val_0_rmse: 0.52474 | val_1_rmse: 0.52982 |  0:00:32s
epoch 5  | loss: 0.28935 | val_0_rmse: 0.5446  | val_1_rmse: 0.55055 |  0:00:38s
epoch 6  | loss: 0.28911 | val_0_rmse: 0.53041 | val_1_rmse: 0.53479 |  0:00:44s
epoch 7  | loss: 0.28236 | val_0_rmse: 0.53614 | val_1_rmse: 0.5404  |  0:00:51s
epoch 8  | loss: 0.27966 | val_0_rmse: 0.58149 | val_1_rmse: 0.5846  |  0:00:57s
epoch 9  | loss: 0.27494 | val_0_rmse: 0.55562 | val_1_rmse: 0.55983 |  0:01:03s
epoch 10 | loss: 0.27994 | val_0_rmse: 0.51409 | val_1_rmse: 0.51346 |  0:01:10s
epoch 11 | loss: 0.27393 | val_0_rmse: 0.54348 | val_1_rmse: 0.54374 |  0:01:16s
epoch 12 | loss: 0.26874 | val_0_rmse: 0.50525 | val_1_rmse: 0.50729 |  0:01:23s
epoch 13 | loss: 0.27008 | val_0_rmse: 0.53979 | val_1_rmse: 0.53866 |  0:01:30s
epoch 14 | loss: 0.27831 | val_0_rmse: 0.54379 | val_1_rmse: 0.54463 |  0:01:36s
epoch 15 | loss: 0.27322 | val_0_rmse: 0.55187 | val_1_rmse: 0.55224 |  0:01:42s
epoch 16 | loss: 0.27037 | val_0_rmse: 0.49993 | val_1_rmse: 0.50024 |  0:01:49s
epoch 17 | loss: 0.26009 | val_0_rmse: 0.51248 | val_1_rmse: 0.51246 |  0:01:55s
epoch 18 | loss: 0.25593 | val_0_rmse: 0.58503 | val_1_rmse: 0.58381 |  0:02:02s
epoch 19 | loss: 0.25076 | val_0_rmse: 0.60983 | val_1_rmse: 0.61021 |  0:02:08s
epoch 20 | loss: 0.24968 | val_0_rmse: 0.52732 | val_1_rmse: 0.52769 |  0:02:14s
epoch 21 | loss: 0.24548 | val_0_rmse: 0.48685 | val_1_rmse: 0.48851 |  0:02:21s
epoch 22 | loss: 0.24572 | val_0_rmse: 0.52923 | val_1_rmse: 0.53144 |  0:02:27s
epoch 23 | loss: 0.24411 | val_0_rmse: 0.52737 | val_1_rmse: 0.52759 |  0:02:34s
epoch 24 | loss: 0.24554 | val_0_rmse: 0.49212 | val_1_rmse: 0.49422 |  0:02:40s
epoch 25 | loss: 0.24145 | val_0_rmse: 0.50294 | val_1_rmse: 0.50557 |  0:02:46s
epoch 26 | loss: 0.2417  | val_0_rmse: 0.56252 | val_1_rmse: 0.56391 |  0:02:53s
epoch 27 | loss: 0.24153 | val_0_rmse: 0.51586 | val_1_rmse: 0.51921 |  0:02:59s
epoch 28 | loss: 0.24202 | val_0_rmse: 0.50135 | val_1_rmse: 0.50368 |  0:03:06s
epoch 29 | loss: 0.23775 | val_0_rmse: 0.49003 | val_1_rmse: 0.49194 |  0:03:12s
epoch 30 | loss: 0.23852 | val_0_rmse: 0.47873 | val_1_rmse: 0.48129 |  0:03:19s
epoch 31 | loss: 0.23912 | val_0_rmse: 0.55786 | val_1_rmse: 0.55765 |  0:03:25s
epoch 32 | loss: 0.23941 | val_0_rmse: 0.57726 | val_1_rmse: 0.57926 |  0:03:31s
epoch 33 | loss: 0.23758 | val_0_rmse: 0.4966  | val_1_rmse: 0.49837 |  0:03:38s
epoch 34 | loss: 0.23689 | val_0_rmse: 0.54251 | val_1_rmse: 0.54532 |  0:03:45s
epoch 35 | loss: 0.23387 | val_0_rmse: 0.48907 | val_1_rmse: 0.49164 |  0:03:51s
epoch 36 | loss: 0.23811 | val_0_rmse: 0.51058 | val_1_rmse: 0.51589 |  0:03:58s
epoch 37 | loss: 0.23838 | val_0_rmse: 0.54691 | val_1_rmse: 0.54763 |  0:04:04s
epoch 38 | loss: 0.25037 | val_0_rmse: 0.51847 | val_1_rmse: 0.52513 |  0:04:10s
epoch 39 | loss: 0.25538 | val_0_rmse: 0.51749 | val_1_rmse: 0.52128 |  0:04:17s
epoch 40 | loss: 0.24294 | val_0_rmse: 0.52831 | val_1_rmse: 0.53045 |  0:04:23s
epoch 41 | loss: 0.24074 | val_0_rmse: 0.48802 | val_1_rmse: 0.4922  |  0:04:30s
epoch 42 | loss: 0.23826 | val_0_rmse: 0.48863 | val_1_rmse: 0.49072 |  0:04:36s
epoch 43 | loss: 0.23757 | val_0_rmse: 0.4985  | val_1_rmse: 0.5026  |  0:04:43s
epoch 44 | loss: 0.24279 | val_0_rmse: 0.52713 | val_1_rmse: 0.53024 |  0:04:49s
epoch 45 | loss: 0.23636 | val_0_rmse: 0.47704 | val_1_rmse: 0.47876 |  0:04:55s
epoch 46 | loss: 0.23676 | val_0_rmse: 0.52297 | val_1_rmse: 0.52448 |  0:05:02s
epoch 47 | loss: 0.23421 | val_0_rmse: 0.48825 | val_1_rmse: 0.49045 |  0:05:08s
epoch 48 | loss: 0.23831 | val_0_rmse: 0.54073 | val_1_rmse: 0.54216 |  0:05:15s
epoch 49 | loss: 0.23204 | val_0_rmse: 0.47756 | val_1_rmse: 0.48216 |  0:05:21s
epoch 50 | loss: 0.2344  | val_0_rmse: 0.50487 | val_1_rmse: 0.50536 |  0:05:28s
epoch 51 | loss: 0.23179 | val_0_rmse: 0.48064 | val_1_rmse: 0.4832  |  0:05:34s
epoch 52 | loss: 0.23453 | val_0_rmse: 0.49136 | val_1_rmse: 0.49381 |  0:05:41s
epoch 53 | loss: 0.22942 | val_0_rmse: 0.469   | val_1_rmse: 0.47324 |  0:05:47s
epoch 54 | loss: 0.22989 | val_0_rmse: 0.55694 | val_1_rmse: 0.56024 |  0:05:53s
epoch 55 | loss: 0.23013 | val_0_rmse: 0.53207 | val_1_rmse: 0.53802 |  0:06:00s
epoch 56 | loss: 0.23081 | val_0_rmse: 0.49286 | val_1_rmse: 0.49605 |  0:06:06s
epoch 57 | loss: 0.23179 | val_0_rmse: 0.48809 | val_1_rmse: 0.49208 |  0:06:13s
epoch 58 | loss: 0.23016 | val_0_rmse: 0.53288 | val_1_rmse: 0.5361  |  0:06:19s
epoch 59 | loss: 0.23189 | val_0_rmse: 0.5136  | val_1_rmse: 0.51818 |  0:06:25s
epoch 60 | loss: 0.22743 | val_0_rmse: 0.58277 | val_1_rmse: 0.58552 |  0:06:32s
epoch 61 | loss: 0.22877 | val_0_rmse: 0.48616 | val_1_rmse: 0.49067 |  0:06:38s
epoch 62 | loss: 0.22941 | val_0_rmse: 0.46856 | val_1_rmse: 0.47379 |  0:06:45s
epoch 63 | loss: 0.22663 | val_0_rmse: 0.49892 | val_1_rmse: 0.50257 |  0:06:51s
epoch 64 | loss: 0.22788 | val_0_rmse: 0.5353  | val_1_rmse: 0.54064 |  0:06:58s
epoch 65 | loss: 0.22946 | val_0_rmse: 0.52734 | val_1_rmse: 0.53221 |  0:07:04s
epoch 66 | loss: 0.22823 | val_0_rmse: 0.54055 | val_1_rmse: 0.54446 |  0:07:10s
epoch 67 | loss: 0.22561 | val_0_rmse: 0.54626 | val_1_rmse: 0.54829 |  0:07:17s
epoch 68 | loss: 0.22989 | val_0_rmse: 0.50105 | val_1_rmse: 0.50643 |  0:07:23s
epoch 69 | loss: 0.22733 | val_0_rmse: 0.47554 | val_1_rmse: 0.48135 |  0:07:30s
epoch 70 | loss: 0.23039 | val_0_rmse: 0.49296 | val_1_rmse: 0.49661 |  0:07:36s
epoch 71 | loss: 0.23156 | val_0_rmse: 0.47619 | val_1_rmse: 0.47941 |  0:07:43s
epoch 72 | loss: 0.23029 | val_0_rmse: 0.47715 | val_1_rmse: 0.48209 |  0:07:49s
epoch 73 | loss: 0.23613 | val_0_rmse: 0.51397 | val_1_rmse: 0.51872 |  0:07:56s
epoch 74 | loss: 0.23144 | val_0_rmse: 0.53313 | val_1_rmse: 0.53712 |  0:08:02s
epoch 75 | loss: 0.22728 | val_0_rmse: 0.58866 | val_1_rmse: 0.59216 |  0:08:08s
epoch 76 | loss: 0.22615 | val_0_rmse: 0.52179 | val_1_rmse: 0.528   |  0:08:15s
epoch 77 | loss: 0.23235 | val_0_rmse: 0.49297 | val_1_rmse: 0.49955 |  0:08:21s
epoch 78 | loss: 0.23051 | val_0_rmse: 0.51379 | val_1_rmse: 0.51938 |  0:08:28s
epoch 79 | loss: 0.22487 | val_0_rmse: 0.49735 | val_1_rmse: 0.50203 |  0:08:34s
epoch 80 | loss: 0.22589 | val_0_rmse: 0.52624 | val_1_rmse: 0.53221 |  0:08:41s
epoch 81 | loss: 0.2247  | val_0_rmse: 0.54879 | val_1_rmse: 0.55518 |  0:08:47s
epoch 82 | loss: 0.22579 | val_0_rmse: 0.46763 | val_1_rmse: 0.47277 |  0:08:54s
epoch 83 | loss: 0.22463 | val_0_rmse: 0.54301 | val_1_rmse: 0.5492  |  0:09:00s
epoch 84 | loss: 0.22284 | val_0_rmse: 0.53407 | val_1_rmse: 0.53973 |  0:09:07s
epoch 85 | loss: 0.22441 | val_0_rmse: 0.56953 | val_1_rmse: 0.5737  |  0:09:13s
epoch 86 | loss: 0.22157 | val_0_rmse: 0.46111 | val_1_rmse: 0.46697 |  0:09:19s
epoch 87 | loss: 0.22257 | val_0_rmse: 0.49918 | val_1_rmse: 0.50639 |  0:09:26s
epoch 88 | loss: 0.22324 | val_0_rmse: 0.47156 | val_1_rmse: 0.47916 |  0:09:32s
epoch 89 | loss: 0.22106 | val_0_rmse: 0.49186 | val_1_rmse: 0.49685 |  0:09:39s
epoch 90 | loss: 0.22226 | val_0_rmse: 0.48931 | val_1_rmse: 0.4991  |  0:09:45s
epoch 91 | loss: 0.22034 | val_0_rmse: 0.51525 | val_1_rmse: 0.5211  |  0:09:52s
epoch 92 | loss: 0.22169 | val_0_rmse: 0.51495 | val_1_rmse: 0.52283 |  0:09:58s
epoch 93 | loss: 0.22329 | val_0_rmse: 0.52259 | val_1_rmse: 0.52958 |  0:10:04s
epoch 94 | loss: 0.22238 | val_0_rmse: 0.47948 | val_1_rmse: 0.48884 |  0:10:11s
epoch 95 | loss: 0.2207  | val_0_rmse: 0.527   | val_1_rmse: 0.53478 |  0:10:18s
epoch 96 | loss: 0.2209  | val_0_rmse: 0.52399 | val_1_rmse: 0.53275 |  0:10:24s
epoch 97 | loss: 0.21996 | val_0_rmse: 0.55069 | val_1_rmse: 0.55989 |  0:10:31s
epoch 98 | loss: 0.22055 | val_0_rmse: 0.54172 | val_1_rmse: 0.54758 |  0:10:37s
epoch 99 | loss: 0.22049 | val_0_rmse: 0.57936 | val_1_rmse: 0.58559 |  0:10:44s
epoch 100| loss: 0.22577 | val_0_rmse: 0.5155  | val_1_rmse: 0.52412 |  0:10:50s
epoch 101| loss: 0.21829 | val_0_rmse: 0.51465 | val_1_rmse: 0.52082 |  0:10:57s
epoch 102| loss: 0.22146 | val_0_rmse: 0.48141 | val_1_rmse: 0.48805 |  0:11:03s
epoch 103| loss: 0.21788 | val_0_rmse: 0.50857 | val_1_rmse: 0.51589 |  0:11:10s
epoch 104| loss: 0.21558 | val_0_rmse: 0.58204 | val_1_rmse: 0.58513 |  0:11:16s
epoch 105| loss: 0.21804 | val_0_rmse: 0.46218 | val_1_rmse: 0.47168 |  0:11:23s
epoch 106| loss: 0.21951 | val_0_rmse: 0.57466 | val_1_rmse: 0.58127 |  0:11:29s
epoch 107| loss: 0.22111 | val_0_rmse: 0.51822 | val_1_rmse: 0.52625 |  0:11:36s
epoch 108| loss: 0.21932 | val_0_rmse: 0.51968 | val_1_rmse: 0.52834 |  0:11:42s
epoch 109| loss: 0.22335 | val_0_rmse: 0.49896 | val_1_rmse: 0.50646 |  0:11:49s
epoch 110| loss: 0.21843 | val_0_rmse: 0.53984 | val_1_rmse: 0.54768 |  0:11:55s
epoch 111| loss: 0.21807 | val_0_rmse: 0.54753 | val_1_rmse: 0.55676 |  0:12:02s
epoch 112| loss: 0.2196  | val_0_rmse: 0.46889 | val_1_rmse: 0.47689 |  0:12:08s
epoch 113| loss: 0.22059 | val_0_rmse: 0.71488 | val_1_rmse: 0.71727 |  0:12:15s
epoch 114| loss: 0.23221 | val_0_rmse: 0.48864 | val_1_rmse: 0.49712 |  0:12:21s
epoch 115| loss: 0.21948 | val_0_rmse: 0.60746 | val_1_rmse: 0.61314 |  0:12:28s
epoch 116| loss: 0.22181 | val_0_rmse: 0.52185 | val_1_rmse: 0.52838 |  0:12:35s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.46697
Best weights from best epoch are automatically used!
ended training at: 22:09:46
Feature importance:
[('Area', 0.38023486635557946), ('Baths', 0.0), ('Beds', 0.06437289604619326), ('Latitude', 0.15074921489244272), ('Longitude', 0.24353281614465355), ('Month', 0.10237688743723303), ('Year', 0.05873331912389799)]
Mean squared error is of 928793271.9560981
Mean absolute error:20666.541474399946
MAPE:0.2521845078728732
R2 score:0.7693102856113516
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:09:47
epoch 0  | loss: 0.45059 | val_0_rmse: 0.60627 | val_1_rmse: 0.61232 |  0:00:06s
epoch 1  | loss: 0.3272  | val_0_rmse: 0.55202 | val_1_rmse: 0.55654 |  0:00:12s
epoch 2  | loss: 0.30899 | val_0_rmse: 0.55069 | val_1_rmse: 0.55864 |  0:00:19s
epoch 3  | loss: 0.30584 | val_0_rmse: 0.54171 | val_1_rmse: 0.54974 |  0:00:25s
epoch 4  | loss: 0.29797 | val_0_rmse: 0.5518  | val_1_rmse: 0.56193 |  0:00:32s
epoch 5  | loss: 0.29171 | val_0_rmse: 0.54728 | val_1_rmse: 0.55863 |  0:00:38s
epoch 6  | loss: 0.28968 | val_0_rmse: 0.54851 | val_1_rmse: 0.55911 |  0:00:45s
epoch 7  | loss: 0.28621 | val_0_rmse: 0.55324 | val_1_rmse: 0.55923 |  0:00:52s
epoch 8  | loss: 0.28494 | val_0_rmse: 0.55757 | val_1_rmse: 0.5633  |  0:00:58s
epoch 9  | loss: 0.28301 | val_0_rmse: 0.51924 | val_1_rmse: 0.53048 |  0:01:05s
epoch 10 | loss: 0.27988 | val_0_rmse: 0.51293 | val_1_rmse: 0.52247 |  0:01:11s
epoch 11 | loss: 0.28006 | val_0_rmse: 0.51783 | val_1_rmse: 0.52798 |  0:01:18s
epoch 12 | loss: 0.27163 | val_0_rmse: 0.53941 | val_1_rmse: 0.54431 |  0:01:24s
epoch 13 | loss: 0.27036 | val_0_rmse: 0.51988 | val_1_rmse: 0.52998 |  0:01:31s
epoch 14 | loss: 0.27019 | val_0_rmse: 0.54791 | val_1_rmse: 0.56011 |  0:01:38s
epoch 15 | loss: 0.26755 | val_0_rmse: 0.51194 | val_1_rmse: 0.52323 |  0:01:44s
epoch 16 | loss: 0.26294 | val_0_rmse: 0.5024  | val_1_rmse: 0.51437 |  0:01:51s
epoch 17 | loss: 0.25913 | val_0_rmse: 0.50201 | val_1_rmse: 0.51555 |  0:01:57s
epoch 18 | loss: 0.2593  | val_0_rmse: 0.51879 | val_1_rmse: 0.53314 |  0:02:04s
epoch 19 | loss: 0.25204 | val_0_rmse: 0.51869 | val_1_rmse: 0.53008 |  0:02:10s
epoch 20 | loss: 0.26148 | val_0_rmse: 0.53529 | val_1_rmse: 0.5459  |  0:02:17s
epoch 21 | loss: 0.26594 | val_0_rmse: 0.4994  | val_1_rmse: 0.51198 |  0:02:23s
epoch 22 | loss: 0.25504 | val_0_rmse: 0.49523 | val_1_rmse: 0.50806 |  0:02:30s
epoch 23 | loss: 0.25522 | val_0_rmse: 0.4956  | val_1_rmse: 0.51084 |  0:02:37s
epoch 24 | loss: 0.2469  | val_0_rmse: 0.48744 | val_1_rmse: 0.50265 |  0:02:43s
epoch 25 | loss: 0.24418 | val_0_rmse: 0.61135 | val_1_rmse: 0.61437 |  0:02:50s
epoch 26 | loss: 0.24438 | val_0_rmse: 0.49129 | val_1_rmse: 0.50727 |  0:02:56s
epoch 27 | loss: 0.24364 | val_0_rmse: 0.545   | val_1_rmse: 0.55253 |  0:03:03s
epoch 28 | loss: 0.24484 | val_0_rmse: 0.49077 | val_1_rmse: 0.50546 |  0:03:09s
epoch 29 | loss: 0.24309 | val_0_rmse: 0.53623 | val_1_rmse: 0.54818 |  0:03:15s
epoch 30 | loss: 0.24043 | val_0_rmse: 0.50861 | val_1_rmse: 0.52155 |  0:03:22s
epoch 31 | loss: 0.24129 | val_0_rmse: 0.50373 | val_1_rmse: 0.51759 |  0:03:28s
epoch 32 | loss: 0.24122 | val_0_rmse: 0.53451 | val_1_rmse: 0.54384 |  0:03:35s
epoch 33 | loss: 0.23903 | val_0_rmse: 0.49427 | val_1_rmse: 0.50736 |  0:03:41s
epoch 34 | loss: 0.23787 | val_0_rmse: 0.48938 | val_1_rmse: 0.50373 |  0:03:47s
epoch 35 | loss: 0.23933 | val_0_rmse: 0.49716 | val_1_rmse: 0.51173 |  0:03:54s
epoch 36 | loss: 0.24141 | val_0_rmse: 0.47877 | val_1_rmse: 0.49711 |  0:04:00s
epoch 37 | loss: 0.23744 | val_0_rmse: 0.50823 | val_1_rmse: 0.5188  |  0:04:06s
epoch 38 | loss: 0.24133 | val_0_rmse: 0.52698 | val_1_rmse: 0.5383  |  0:04:13s
epoch 39 | loss: 0.23983 | val_0_rmse: 0.50366 | val_1_rmse: 0.51857 |  0:04:19s
epoch 40 | loss: 0.23914 | val_0_rmse: 0.48606 | val_1_rmse: 0.50348 |  0:04:25s
epoch 41 | loss: 0.23669 | val_0_rmse: 0.47889 | val_1_rmse: 0.49546 |  0:04:32s
epoch 42 | loss: 0.23692 | val_0_rmse: 0.55355 | val_1_rmse: 0.56709 |  0:04:38s
epoch 43 | loss: 0.23736 | val_0_rmse: 0.52147 | val_1_rmse: 0.53572 |  0:04:45s
epoch 44 | loss: 0.23482 | val_0_rmse: 0.57241 | val_1_rmse: 0.57777 |  0:04:51s
epoch 45 | loss: 0.23398 | val_0_rmse: 0.49652 | val_1_rmse: 0.51022 |  0:04:57s
epoch 46 | loss: 0.23333 | val_0_rmse: 0.47653 | val_1_rmse: 0.49397 |  0:05:04s
epoch 47 | loss: 0.2399  | val_0_rmse: 0.50654 | val_1_rmse: 0.52008 |  0:05:10s
epoch 48 | loss: 0.23465 | val_0_rmse: 0.48313 | val_1_rmse: 0.49909 |  0:05:17s
epoch 49 | loss: 0.23522 | val_0_rmse: 0.52918 | val_1_rmse: 0.54024 |  0:05:23s
epoch 50 | loss: 0.23868 | val_0_rmse: 0.48509 | val_1_rmse: 0.50235 |  0:05:29s
epoch 51 | loss: 0.23739 | val_0_rmse: 0.49045 | val_1_rmse: 0.50896 |  0:05:36s
epoch 52 | loss: 0.23496 | val_0_rmse: 0.50593 | val_1_rmse: 0.52264 |  0:05:42s
epoch 53 | loss: 0.23297 | val_0_rmse: 0.48935 | val_1_rmse: 0.50452 |  0:05:48s
epoch 54 | loss: 0.23203 | val_0_rmse: 0.49856 | val_1_rmse: 0.50967 |  0:05:55s
epoch 55 | loss: 0.23647 | val_0_rmse: 0.48089 | val_1_rmse: 0.49874 |  0:06:01s
epoch 56 | loss: 0.23345 | val_0_rmse: 0.48234 | val_1_rmse: 0.49849 |  0:06:08s
epoch 57 | loss: 0.23804 | val_0_rmse: 0.5025  | val_1_rmse: 0.51361 |  0:06:14s
epoch 58 | loss: 0.23176 | val_0_rmse: 0.4921  | val_1_rmse: 0.50875 |  0:06:21s
epoch 59 | loss: 0.24066 | val_0_rmse: 0.59455 | val_1_rmse: 0.60048 |  0:06:27s
epoch 60 | loss: 0.24248 | val_0_rmse: 0.48902 | val_1_rmse: 0.50304 |  0:06:34s
epoch 61 | loss: 0.23443 | val_0_rmse: 0.54745 | val_1_rmse: 0.55458 |  0:06:40s
epoch 62 | loss: 0.23312 | val_0_rmse: 0.49445 | val_1_rmse: 0.51351 |  0:06:46s
epoch 63 | loss: 0.23471 | val_0_rmse: 0.50042 | val_1_rmse: 0.51497 |  0:06:53s
epoch 64 | loss: 0.23074 | val_0_rmse: 0.47091 | val_1_rmse: 0.48972 |  0:06:59s
epoch 65 | loss: 0.2323  | val_0_rmse: 0.59758 | val_1_rmse: 0.60112 |  0:07:06s
epoch 66 | loss: 0.23419 | val_0_rmse: 0.51509 | val_1_rmse: 0.5275  |  0:07:12s
epoch 67 | loss: 0.23083 | val_0_rmse: 0.46892 | val_1_rmse: 0.48782 |  0:07:19s
epoch 68 | loss: 0.23169 | val_0_rmse: 0.50381 | val_1_rmse: 0.51719 |  0:07:25s
epoch 69 | loss: 0.23308 | val_0_rmse: 0.50192 | val_1_rmse: 0.51651 |  0:07:31s
epoch 70 | loss: 0.23132 | val_0_rmse: 0.52686 | val_1_rmse: 0.53925 |  0:07:38s
epoch 71 | loss: 0.23263 | val_0_rmse: 0.48847 | val_1_rmse: 0.5046  |  0:07:44s
epoch 72 | loss: 0.23738 | val_0_rmse: 0.56182 | val_1_rmse: 0.56969 |  0:07:51s
epoch 73 | loss: 0.23354 | val_0_rmse: 0.4954  | val_1_rmse: 0.51347 |  0:07:57s
epoch 74 | loss: 0.23724 | val_0_rmse: 0.48267 | val_1_rmse: 0.50114 |  0:08:04s
epoch 75 | loss: 0.23492 | val_0_rmse: 0.60071 | val_1_rmse: 0.60703 |  0:08:10s
epoch 76 | loss: 0.23343 | val_0_rmse: 0.47976 | val_1_rmse: 0.49927 |  0:08:17s
epoch 77 | loss: 0.23165 | val_0_rmse: 0.50215 | val_1_rmse: 0.51619 |  0:08:23s
epoch 78 | loss: 0.23643 | val_0_rmse: 0.46815 | val_1_rmse: 0.488   |  0:08:29s
epoch 79 | loss: 0.2324  | val_0_rmse: 0.49637 | val_1_rmse: 0.51225 |  0:08:36s
epoch 80 | loss: 0.23065 | val_0_rmse: 0.51039 | val_1_rmse: 0.52171 |  0:08:42s
epoch 81 | loss: 0.22906 | val_0_rmse: 0.5221  | val_1_rmse: 0.53277 |  0:08:49s
epoch 82 | loss: 0.23087 | val_0_rmse: 0.52144 | val_1_rmse: 0.53754 |  0:08:55s
epoch 83 | loss: 0.22861 | val_0_rmse: 0.47336 | val_1_rmse: 0.49334 |  0:09:02s
epoch 84 | loss: 0.22882 | val_0_rmse: 0.50367 | val_1_rmse: 0.51815 |  0:09:08s
epoch 85 | loss: 0.2262  | val_0_rmse: 0.54535 | val_1_rmse: 0.55423 |  0:09:14s
epoch 86 | loss: 0.23023 | val_0_rmse: 0.48529 | val_1_rmse: 0.50481 |  0:09:21s
epoch 87 | loss: 0.23037 | val_0_rmse: 0.5131  | val_1_rmse: 0.52413 |  0:09:27s
epoch 88 | loss: 0.23235 | val_0_rmse: 0.53414 | val_1_rmse: 0.543   |  0:09:34s
epoch 89 | loss: 0.2324  | val_0_rmse: 0.48188 | val_1_rmse: 0.50182 |  0:09:40s
epoch 90 | loss: 0.23029 | val_0_rmse: 0.48752 | val_1_rmse: 0.50455 |  0:09:47s
epoch 91 | loss: 0.23489 | val_0_rmse: 0.4963  | val_1_rmse: 0.51075 |  0:09:53s
epoch 92 | loss: 0.23254 | val_0_rmse: 0.59826 | val_1_rmse: 0.60367 |  0:09:59s
epoch 93 | loss: 0.23266 | val_0_rmse: 0.5013  | val_1_rmse: 0.51187 |  0:10:06s
epoch 94 | loss: 0.2305  | val_0_rmse: 0.4696  | val_1_rmse: 0.48775 |  0:10:12s
epoch 95 | loss: 0.22807 | val_0_rmse: 0.50548 | val_1_rmse: 0.51681 |  0:10:19s
epoch 96 | loss: 0.22901 | val_0_rmse: 0.51361 | val_1_rmse: 0.52524 |  0:10:25s
epoch 97 | loss: 0.22557 | val_0_rmse: 0.50002 | val_1_rmse: 0.51685 |  0:10:32s
epoch 98 | loss: 0.23167 | val_0_rmse: 0.4947  | val_1_rmse: 0.5105  |  0:10:38s
epoch 99 | loss: 0.23049 | val_0_rmse: 0.48683 | val_1_rmse: 0.50526 |  0:10:45s
epoch 100| loss: 0.23112 | val_0_rmse: 0.52757 | val_1_rmse: 0.5391  |  0:10:51s
epoch 101| loss: 0.23078 | val_0_rmse: 0.54256 | val_1_rmse: 0.55159 |  0:10:58s
epoch 102| loss: 0.22893 | val_0_rmse: 0.5136  | val_1_rmse: 0.52491 |  0:11:04s
epoch 103| loss: 0.22678 | val_0_rmse: 0.56044 | val_1_rmse: 0.56761 |  0:11:11s
epoch 104| loss: 0.22768 | val_0_rmse: 0.5953  | val_1_rmse: 0.60041 |  0:11:17s
epoch 105| loss: 0.22497 | val_0_rmse: 0.49144 | val_1_rmse: 0.51302 |  0:11:23s
epoch 106| loss: 0.22753 | val_0_rmse: 0.57671 | val_1_rmse: 0.58594 |  0:11:30s
epoch 107| loss: 0.22517 | val_0_rmse: 0.52886 | val_1_rmse: 0.5399  |  0:11:36s
epoch 108| loss: 0.22814 | val_0_rmse: 0.58778 | val_1_rmse: 0.5937  |  0:11:43s
epoch 109| loss: 0.22429 | val_0_rmse: 0.48058 | val_1_rmse: 0.4964  |  0:11:49s
epoch 110| loss: 0.22785 | val_0_rmse: 0.47918 | val_1_rmse: 0.49659 |  0:11:56s
epoch 111| loss: 0.22785 | val_0_rmse: 0.48463 | val_1_rmse: 0.50234 |  0:12:02s
epoch 112| loss: 0.2272  | val_0_rmse: 0.50341 | val_1_rmse: 0.51576 |  0:12:08s
epoch 113| loss: 0.22626 | val_0_rmse: 0.48658 | val_1_rmse: 0.50367 |  0:12:15s
epoch 114| loss: 0.22367 | val_0_rmse: 0.50364 | val_1_rmse: 0.5199  |  0:12:21s
epoch 115| loss: 0.22481 | val_0_rmse: 0.56899 | val_1_rmse: 0.57375 |  0:12:28s
epoch 116| loss: 0.22573 | val_0_rmse: 0.46999 | val_1_rmse: 0.49254 |  0:12:34s
epoch 117| loss: 0.22317 | val_0_rmse: 0.57201 | val_1_rmse: 0.57741 |  0:12:41s
epoch 118| loss: 0.22496 | val_0_rmse: 0.46227 | val_1_rmse: 0.48243 |  0:12:47s
epoch 119| loss: 0.22272 | val_0_rmse: 0.62825 | val_1_rmse: 0.63171 |  0:12:53s
epoch 120| loss: 0.22553 | val_0_rmse: 0.52861 | val_1_rmse: 0.53916 |  0:13:00s
epoch 121| loss: 0.23026 | val_0_rmse: 0.53187 | val_1_rmse: 0.53947 |  0:13:06s
epoch 122| loss: 0.23063 | val_0_rmse: 0.49022 | val_1_rmse: 0.51068 |  0:13:13s
epoch 123| loss: 0.22732 | val_0_rmse: 0.50332 | val_1_rmse: 0.51692 |  0:13:19s
epoch 124| loss: 0.22784 | val_0_rmse: 0.52453 | val_1_rmse: 0.53591 |  0:13:26s
epoch 125| loss: 0.22602 | val_0_rmse: 0.5217  | val_1_rmse: 0.53305 |  0:13:32s
epoch 126| loss: 0.22725 | val_0_rmse: 0.59725 | val_1_rmse: 0.59978 |  0:13:38s
epoch 127| loss: 0.22697 | val_0_rmse: 0.51754 | val_1_rmse: 0.52967 |  0:13:45s
epoch 128| loss: 0.22611 | val_0_rmse: 0.52386 | val_1_rmse: 0.53614 |  0:13:51s
epoch 129| loss: 0.22574 | val_0_rmse: 0.55031 | val_1_rmse: 0.55876 |  0:13:58s
epoch 130| loss: 0.22679 | val_0_rmse: 0.51731 | val_1_rmse: 0.53259 |  0:14:04s
epoch 131| loss: 0.23881 | val_0_rmse: 0.48599 | val_1_rmse: 0.49899 |  0:14:10s
epoch 132| loss: 0.23409 | val_0_rmse: 0.47628 | val_1_rmse: 0.49547 |  0:14:17s
epoch 133| loss: 0.22704 | val_0_rmse: 0.47993 | val_1_rmse: 0.50021 |  0:14:23s
epoch 134| loss: 0.22919 | val_0_rmse: 0.49645 | val_1_rmse: 0.51043 |  0:14:30s
epoch 135| loss: 0.23371 | val_0_rmse: 0.61084 | val_1_rmse: 0.61714 |  0:14:36s
epoch 136| loss: 0.22627 | val_0_rmse: 0.48896 | val_1_rmse: 0.50189 |  0:14:42s
epoch 137| loss: 0.22314 | val_0_rmse: 0.48271 | val_1_rmse: 0.49764 |  0:14:49s
epoch 138| loss: 0.22346 | val_0_rmse: 0.4617  | val_1_rmse: 0.48155 |  0:14:55s
epoch 139| loss: 0.22345 | val_0_rmse: 0.4687  | val_1_rmse: 0.48834 |  0:15:02s
epoch 140| loss: 0.22344 | val_0_rmse: 0.48285 | val_1_rmse: 0.50024 |  0:15:08s
epoch 141| loss: 0.22152 | val_0_rmse: 0.56327 | val_1_rmse: 0.57157 |  0:15:15s
epoch 142| loss: 0.22136 | val_0_rmse: 0.50573 | val_1_rmse: 0.51917 |  0:15:21s
epoch 143| loss: 0.22096 | val_0_rmse: 0.46079 | val_1_rmse: 0.48293 |  0:15:27s
epoch 144| loss: 0.22183 | val_0_rmse: 0.46322 | val_1_rmse: 0.48436 |  0:15:34s
epoch 145| loss: 0.22281 | val_0_rmse: 0.52547 | val_1_rmse: 0.53823 |  0:15:40s
epoch 146| loss: 0.22176 | val_0_rmse: 0.49559 | val_1_rmse: 0.5119  |  0:15:47s
epoch 147| loss: 0.22796 | val_0_rmse: 0.59349 | val_1_rmse: 0.59855 |  0:15:53s
epoch 148| loss: 0.22225 | val_0_rmse: 0.46374 | val_1_rmse: 0.48269 |  0:16:00s
epoch 149| loss: 0.22231 | val_0_rmse: 0.62364 | val_1_rmse: 0.62598 |  0:16:06s
Stop training because you reached max_epochs = 150 with best_epoch = 138 and best_val_1_rmse = 0.48155
Best weights from best epoch are automatically used!
ended training at: 22:25:55
Feature importance:
[('Area', 0.4090368307339934), ('Baths', 0.061201514192696406), ('Beds', 0.13125917265914683), ('Latitude', 0.0), ('Longitude', 0.3328161868282562), ('Month', 0.0), ('Year', 0.06568629558590722)]
Mean squared error is of 895026754.7021497
Mean absolute error:20079.60851701016
MAPE:0.24097142570378144
R2 score:0.7812674403944916
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:30:35
epoch 0  | loss: 0.41491 | val_0_rmse: 0.53703 | val_1_rmse: 0.54278 |  0:00:06s
epoch 1  | loss: 0.27313 | val_0_rmse: 0.48664 | val_1_rmse: 0.49071 |  0:00:12s
epoch 2  | loss: 0.24246 | val_0_rmse: 0.47273 | val_1_rmse: 0.48118 |  0:00:19s
epoch 3  | loss: 0.23336 | val_0_rmse: 0.47126 | val_1_rmse: 0.47627 |  0:00:25s
epoch 4  | loss: 0.22197 | val_0_rmse: 0.449   | val_1_rmse: 0.45989 |  0:00:32s
epoch 5  | loss: 0.21827 | val_0_rmse: 0.46497 | val_1_rmse: 0.47502 |  0:00:38s
epoch 6  | loss: 0.21576 | val_0_rmse: 0.44723 | val_1_rmse: 0.45635 |  0:00:45s
epoch 7  | loss: 0.21201 | val_0_rmse: 0.45877 | val_1_rmse: 0.46745 |  0:00:51s
epoch 8  | loss: 0.20942 | val_0_rmse: 0.44161 | val_1_rmse: 0.44933 |  0:00:58s
epoch 9  | loss: 0.20936 | val_0_rmse: 0.44259 | val_1_rmse: 0.45014 |  0:01:04s
epoch 10 | loss: 0.20552 | val_0_rmse: 0.44004 | val_1_rmse: 0.44859 |  0:01:10s
epoch 11 | loss: 0.20284 | val_0_rmse: 0.44548 | val_1_rmse: 0.45184 |  0:01:17s
epoch 12 | loss: 0.20334 | val_0_rmse: 0.43508 | val_1_rmse: 0.44414 |  0:01:23s
epoch 13 | loss: 0.20423 | val_0_rmse: 0.43038 | val_1_rmse: 0.44021 |  0:01:30s
epoch 14 | loss: 0.20664 | val_0_rmse: 0.43214 | val_1_rmse: 0.43948 |  0:01:36s
epoch 15 | loss: 0.20263 | val_0_rmse: 0.45134 | val_1_rmse: 0.45607 |  0:01:42s
epoch 16 | loss: 0.20282 | val_0_rmse: 0.43289 | val_1_rmse: 0.44221 |  0:01:49s
epoch 17 | loss: 0.20242 | val_0_rmse: 0.4318  | val_1_rmse: 0.44139 |  0:01:55s
epoch 18 | loss: 0.19613 | val_0_rmse: 0.428   | val_1_rmse: 0.43713 |  0:02:02s
epoch 19 | loss: 0.20494 | val_0_rmse: 0.43364 | val_1_rmse: 0.44001 |  0:02:08s
epoch 20 | loss: 0.19875 | val_0_rmse: 0.42907 | val_1_rmse: 0.43698 |  0:02:15s
epoch 21 | loss: 0.19519 | val_0_rmse: 0.42871 | val_1_rmse: 0.44058 |  0:02:21s
epoch 22 | loss: 0.19749 | val_0_rmse: 0.42634 | val_1_rmse: 0.43751 |  0:02:28s
epoch 23 | loss: 0.19622 | val_0_rmse: 0.41908 | val_1_rmse: 0.429   |  0:02:34s
epoch 24 | loss: 0.19432 | val_0_rmse: 0.4191  | val_1_rmse: 0.4304  |  0:02:40s
epoch 25 | loss: 0.20121 | val_0_rmse: 0.44926 | val_1_rmse: 0.45774 |  0:02:47s
epoch 26 | loss: 0.19513 | val_0_rmse: 0.42912 | val_1_rmse: 0.44222 |  0:02:53s
epoch 27 | loss: 0.19239 | val_0_rmse: 0.42298 | val_1_rmse: 0.4328  |  0:03:00s
epoch 28 | loss: 0.19651 | val_0_rmse: 0.42582 | val_1_rmse: 0.43594 |  0:03:06s
epoch 29 | loss: 0.19208 | val_0_rmse: 0.42515 | val_1_rmse: 0.43513 |  0:03:12s
epoch 30 | loss: 0.19169 | val_0_rmse: 0.43103 | val_1_rmse: 0.44183 |  0:03:19s
epoch 31 | loss: 0.19704 | val_0_rmse: 0.45524 | val_1_rmse: 0.46309 |  0:03:25s
epoch 32 | loss: 0.19274 | val_0_rmse: 0.43625 | val_1_rmse: 0.44856 |  0:03:31s
epoch 33 | loss: 0.19099 | val_0_rmse: 0.41435 | val_1_rmse: 0.42488 |  0:03:38s
epoch 34 | loss: 0.19058 | val_0_rmse: 0.4137  | val_1_rmse: 0.42499 |  0:03:44s
epoch 35 | loss: 0.19203 | val_0_rmse: 0.44515 | val_1_rmse: 0.45139 |  0:03:50s
epoch 36 | loss: 0.18865 | val_0_rmse: 0.4241  | val_1_rmse: 0.4322  |  0:03:57s
epoch 37 | loss: 0.19486 | val_0_rmse: 0.44409 | val_1_rmse: 0.45379 |  0:04:03s
epoch 38 | loss: 0.19183 | val_0_rmse: 0.43107 | val_1_rmse: 0.44232 |  0:04:10s
epoch 39 | loss: 0.19078 | val_0_rmse: 0.42328 | val_1_rmse: 0.43247 |  0:04:16s
epoch 40 | loss: 0.19167 | val_0_rmse: 0.41986 | val_1_rmse: 0.43098 |  0:04:23s
epoch 41 | loss: 0.18614 | val_0_rmse: 0.41308 | val_1_rmse: 0.42662 |  0:04:29s
epoch 42 | loss: 0.18768 | val_0_rmse: 0.41656 | val_1_rmse: 0.42635 |  0:04:35s
epoch 43 | loss: 0.18419 | val_0_rmse: 0.41872 | val_1_rmse: 0.43114 |  0:04:42s
epoch 44 | loss: 0.18738 | val_0_rmse: 0.45258 | val_1_rmse: 0.46062 |  0:04:48s
epoch 45 | loss: 0.18884 | val_0_rmse: 0.42475 | val_1_rmse: 0.43577 |  0:04:55s
epoch 46 | loss: 0.18641 | val_0_rmse: 0.41575 | val_1_rmse: 0.42751 |  0:05:01s
epoch 47 | loss: 0.18495 | val_0_rmse: 0.45473 | val_1_rmse: 0.46346 |  0:05:07s
epoch 48 | loss: 0.18499 | val_0_rmse: 0.41775 | val_1_rmse: 0.42991 |  0:05:14s
epoch 49 | loss: 0.18944 | val_0_rmse: 0.42052 | val_1_rmse: 0.43279 |  0:05:20s
epoch 50 | loss: 0.19032 | val_0_rmse: 0.42461 | val_1_rmse: 0.43512 |  0:05:27s
epoch 51 | loss: 0.18628 | val_0_rmse: 0.42824 | val_1_rmse: 0.4404  |  0:05:33s
epoch 52 | loss: 0.18548 | val_0_rmse: 0.42774 | val_1_rmse: 0.43985 |  0:05:40s
epoch 53 | loss: 0.18721 | val_0_rmse: 0.41206 | val_1_rmse: 0.42484 |  0:05:46s
epoch 54 | loss: 0.18624 | val_0_rmse: 0.42855 | val_1_rmse: 0.43867 |  0:05:52s
epoch 55 | loss: 0.18267 | val_0_rmse: 0.41262 | val_1_rmse: 0.42649 |  0:05:59s
epoch 56 | loss: 0.18367 | val_0_rmse: 0.41674 | val_1_rmse: 0.43143 |  0:06:05s
epoch 57 | loss: 0.18395 | val_0_rmse: 0.41259 | val_1_rmse: 0.42467 |  0:06:12s
epoch 58 | loss: 0.18289 | val_0_rmse: 0.43211 | val_1_rmse: 0.4415  |  0:06:18s
epoch 59 | loss: 0.18253 | val_0_rmse: 0.41009 | val_1_rmse: 0.42378 |  0:06:25s
epoch 60 | loss: 0.1833  | val_0_rmse: 0.41091 | val_1_rmse: 0.42488 |  0:06:31s
epoch 61 | loss: 0.18387 | val_0_rmse: 0.421   | val_1_rmse: 0.43466 |  0:06:37s
epoch 62 | loss: 0.18342 | val_0_rmse: 0.42522 | val_1_rmse: 0.43752 |  0:06:44s
epoch 63 | loss: 0.18128 | val_0_rmse: 0.40947 | val_1_rmse: 0.42299 |  0:06:50s
epoch 64 | loss: 0.1794  | val_0_rmse: 0.40734 | val_1_rmse: 0.42173 |  0:06:57s
epoch 65 | loss: 0.18024 | val_0_rmse: 0.41409 | val_1_rmse: 0.42729 |  0:07:03s
epoch 66 | loss: 0.18098 | val_0_rmse: 0.41074 | val_1_rmse: 0.42548 |  0:07:09s
epoch 67 | loss: 0.1811  | val_0_rmse: 0.43777 | val_1_rmse: 0.45333 |  0:07:16s
epoch 68 | loss: 0.18132 | val_0_rmse: 0.40447 | val_1_rmse: 0.4185  |  0:07:22s
epoch 69 | loss: 0.17805 | val_0_rmse: 0.40198 | val_1_rmse: 0.41794 |  0:07:29s
epoch 70 | loss: 0.17832 | val_0_rmse: 0.41978 | val_1_rmse: 0.43191 |  0:07:35s
epoch 71 | loss: 0.1818  | val_0_rmse: 0.42508 | val_1_rmse: 0.43628 |  0:07:42s
epoch 72 | loss: 0.1795  | val_0_rmse: 0.41778 | val_1_rmse: 0.42616 |  0:07:48s
epoch 73 | loss: 0.18292 | val_0_rmse: 0.41326 | val_1_rmse: 0.42679 |  0:07:55s
epoch 74 | loss: 0.18457 | val_0_rmse: 0.41118 | val_1_rmse: 0.42434 |  0:08:01s
epoch 75 | loss: 0.18163 | val_0_rmse: 0.44144 | val_1_rmse: 0.45591 |  0:08:08s
epoch 76 | loss: 0.18338 | val_0_rmse: 0.42542 | val_1_rmse: 0.43968 |  0:08:14s
epoch 77 | loss: 0.18307 | val_0_rmse: 0.40859 | val_1_rmse: 0.42464 |  0:08:21s
epoch 78 | loss: 0.18069 | val_0_rmse: 0.40496 | val_1_rmse: 0.41797 |  0:08:27s
epoch 79 | loss: 0.179   | val_0_rmse: 0.41696 | val_1_rmse: 0.42863 |  0:08:34s
epoch 80 | loss: 0.18218 | val_0_rmse: 0.40418 | val_1_rmse: 0.42067 |  0:08:40s
epoch 81 | loss: 0.18062 | val_0_rmse: 0.40273 | val_1_rmse: 0.4188  |  0:08:47s
epoch 82 | loss: 0.18089 | val_0_rmse: 0.43515 | val_1_rmse: 0.44576 |  0:08:53s
epoch 83 | loss: 0.17985 | val_0_rmse: 0.42374 | val_1_rmse: 0.43598 |  0:09:00s
epoch 84 | loss: 0.18266 | val_0_rmse: 0.41658 | val_1_rmse: 0.42879 |  0:09:06s
epoch 85 | loss: 0.17726 | val_0_rmse: 0.40395 | val_1_rmse: 0.41896 |  0:09:12s
epoch 86 | loss: 0.18044 | val_0_rmse: 0.40948 | val_1_rmse: 0.42432 |  0:09:19s
epoch 87 | loss: 0.17817 | val_0_rmse: 0.40429 | val_1_rmse: 0.41993 |  0:09:25s
epoch 88 | loss: 0.17614 | val_0_rmse: 0.40449 | val_1_rmse: 0.41746 |  0:09:32s
epoch 89 | loss: 0.1769  | val_0_rmse: 0.395   | val_1_rmse: 0.4119  |  0:09:38s
epoch 90 | loss: 0.17424 | val_0_rmse: 0.40011 | val_1_rmse: 0.41428 |  0:09:45s
epoch 91 | loss: 0.17444 | val_0_rmse: 0.40132 | val_1_rmse: 0.41787 |  0:09:51s
epoch 92 | loss: 0.17622 | val_0_rmse: 0.42768 | val_1_rmse: 0.44205 |  0:09:58s
epoch 93 | loss: 0.17597 | val_0_rmse: 0.40147 | val_1_rmse: 0.41877 |  0:10:04s
epoch 94 | loss: 0.17894 | val_0_rmse: 0.4145  | val_1_rmse: 0.42736 |  0:10:11s
epoch 95 | loss: 0.18206 | val_0_rmse: 0.40975 | val_1_rmse: 0.42633 |  0:10:17s
epoch 96 | loss: 0.17839 | val_0_rmse: 0.41256 | val_1_rmse: 0.42922 |  0:10:24s
epoch 97 | loss: 0.17418 | val_0_rmse: 0.39865 | val_1_rmse: 0.41498 |  0:10:30s
epoch 98 | loss: 0.1822  | val_0_rmse: 0.41087 | val_1_rmse: 0.4236  |  0:10:37s
epoch 99 | loss: 0.18443 | val_0_rmse: 0.4125  | val_1_rmse: 0.42425 |  0:10:43s
epoch 100| loss: 0.17992 | val_0_rmse: 0.43347 | val_1_rmse: 0.44556 |  0:10:50s
epoch 101| loss: 0.17784 | val_0_rmse: 0.41465 | val_1_rmse: 0.42862 |  0:10:56s
epoch 102| loss: 0.18088 | val_0_rmse: 0.43648 | val_1_rmse: 0.45234 |  0:11:03s
epoch 103| loss: 0.18177 | val_0_rmse: 0.40818 | val_1_rmse: 0.42408 |  0:11:09s
epoch 104| loss: 0.17613 | val_0_rmse: 0.40023 | val_1_rmse: 0.41701 |  0:11:16s
epoch 105| loss: 0.17633 | val_0_rmse: 0.39768 | val_1_rmse: 0.41464 |  0:11:22s
epoch 106| loss: 0.17534 | val_0_rmse: 0.39889 | val_1_rmse: 0.41563 |  0:11:29s
epoch 107| loss: 0.17427 | val_0_rmse: 0.39877 | val_1_rmse: 0.41496 |  0:11:35s
epoch 108| loss: 0.17397 | val_0_rmse: 0.42539 | val_1_rmse: 0.44105 |  0:11:42s
epoch 109| loss: 0.17932 | val_0_rmse: 0.4044  | val_1_rmse: 0.41948 |  0:11:48s
epoch 110| loss: 0.1728  | val_0_rmse: 0.40545 | val_1_rmse: 0.42125 |  0:11:55s
epoch 111| loss: 0.17363 | val_0_rmse: 0.40165 | val_1_rmse: 0.41867 |  0:12:01s
epoch 112| loss: 0.17443 | val_0_rmse: 0.41944 | val_1_rmse: 0.43381 |  0:12:08s
epoch 113| loss: 0.17662 | val_0_rmse: 0.40066 | val_1_rmse: 0.41754 |  0:12:14s
epoch 114| loss: 0.17403 | val_0_rmse: 0.41965 | val_1_rmse: 0.43714 |  0:12:21s
epoch 115| loss: 0.17349 | val_0_rmse: 0.39323 | val_1_rmse: 0.41152 |  0:12:27s
epoch 116| loss: 0.17107 | val_0_rmse: 0.41123 | val_1_rmse: 0.43211 |  0:12:33s
epoch 117| loss: 0.17541 | val_0_rmse: 0.40496 | val_1_rmse: 0.42198 |  0:12:40s
epoch 118| loss: 0.1734  | val_0_rmse: 0.3995  | val_1_rmse: 0.41542 |  0:12:46s
epoch 119| loss: 0.17225 | val_0_rmse: 0.41244 | val_1_rmse: 0.43199 |  0:12:53s
epoch 120| loss: 0.17187 | val_0_rmse: 0.39672 | val_1_rmse: 0.41455 |  0:12:59s
epoch 121| loss: 0.17088 | val_0_rmse: 0.39628 | val_1_rmse: 0.4139  |  0:13:06s
epoch 122| loss: 0.17163 | val_0_rmse: 0.40394 | val_1_rmse: 0.42253 |  0:13:13s
epoch 123| loss: 0.17187 | val_0_rmse: 0.43984 | val_1_rmse: 0.4553  |  0:13:19s
epoch 124| loss: 0.1741  | val_0_rmse: 0.44556 | val_1_rmse: 0.46433 |  0:13:26s
epoch 125| loss: 0.17143 | val_0_rmse: 0.40861 | val_1_rmse: 0.42571 |  0:13:32s
epoch 126| loss: 0.17362 | val_0_rmse: 0.40758 | val_1_rmse: 0.42438 |  0:13:39s
epoch 127| loss: 0.17545 | val_0_rmse: 0.39743 | val_1_rmse: 0.41526 |  0:13:45s
epoch 128| loss: 0.17117 | val_0_rmse: 0.40507 | val_1_rmse: 0.42153 |  0:13:52s
epoch 129| loss: 0.16938 | val_0_rmse: 0.40453 | val_1_rmse: 0.42286 |  0:13:58s
epoch 130| loss: 0.17181 | val_0_rmse: 0.41246 | val_1_rmse: 0.431   |  0:14:05s
epoch 131| loss: 0.17266 | val_0_rmse: 0.39906 | val_1_rmse: 0.41782 |  0:14:11s
epoch 132| loss: 0.17156 | val_0_rmse: 0.40043 | val_1_rmse: 0.4199  |  0:14:18s
epoch 133| loss: 0.17056 | val_0_rmse: 0.40004 | val_1_rmse: 0.41876 |  0:14:24s
epoch 134| loss: 0.17018 | val_0_rmse: 0.39179 | val_1_rmse: 0.41185 |  0:14:31s
epoch 135| loss: 0.16939 | val_0_rmse: 0.38783 | val_1_rmse: 0.40824 |  0:14:37s
epoch 136| loss: 0.18563 | val_0_rmse: 0.44372 | val_1_rmse: 0.4543  |  0:14:44s
epoch 137| loss: 0.21019 | val_0_rmse: 0.44652 | val_1_rmse: 0.45436 |  0:14:50s
epoch 138| loss: 0.18627 | val_0_rmse: 0.41169 | val_1_rmse: 0.42412 |  0:14:57s
epoch 139| loss: 0.18097 | val_0_rmse: 0.40851 | val_1_rmse: 0.42178 |  0:15:03s
epoch 140| loss: 0.17816 | val_0_rmse: 0.39994 | val_1_rmse: 0.41485 |  0:15:10s
epoch 141| loss: 0.17554 | val_0_rmse: 0.40565 | val_1_rmse: 0.42173 |  0:15:16s
epoch 142| loss: 0.17355 | val_0_rmse: 0.42614 | val_1_rmse: 0.44534 |  0:15:23s
epoch 143| loss: 0.1757  | val_0_rmse: 0.39536 | val_1_rmse: 0.41351 |  0:15:30s
epoch 144| loss: 0.17399 | val_0_rmse: 0.39878 | val_1_rmse: 0.41664 |  0:15:36s
epoch 145| loss: 0.17376 | val_0_rmse: 0.39886 | val_1_rmse: 0.41763 |  0:15:43s
epoch 146| loss: 0.17289 | val_0_rmse: 0.41561 | val_1_rmse: 0.43263 |  0:15:49s
epoch 147| loss: 0.17492 | val_0_rmse: 0.40089 | val_1_rmse: 0.41864 |  0:15:56s
epoch 148| loss: 0.17058 | val_0_rmse: 0.39316 | val_1_rmse: 0.41321 |  0:16:02s
epoch 149| loss: 0.16913 | val_0_rmse: 0.40466 | val_1_rmse: 0.42126 |  0:16:09s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.40824
Best weights from best epoch are automatically used!
ended training at: 22:46:46
Feature importance:
[('Area', 0.0), ('Baths', 0.1356017664590985), ('Beds', 0.19962434509232452), ('Latitude', 0.14914721828642408), ('Longitude', 0.19690935350078057), ('Month', 0.0), ('Year', 0.3187173166613723)]
Mean squared error is of 8997626760.997513
Mean absolute error:65583.76956693847
MAPE:0.2672208542820869
R2 score:0.8455071166870497
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:46:47
epoch 0  | loss: 0.40623 | val_0_rmse: 0.56989 | val_1_rmse: 0.57496 |  0:00:06s
epoch 1  | loss: 0.28205 | val_0_rmse: 0.49193 | val_1_rmse: 0.49371 |  0:00:12s
epoch 2  | loss: 0.24975 | val_0_rmse: 0.46358 | val_1_rmse: 0.4625  |  0:00:19s
epoch 3  | loss: 0.23822 | val_0_rmse: 0.46301 | val_1_rmse: 0.46029 |  0:00:26s
epoch 4  | loss: 0.22466 | val_0_rmse: 0.45763 | val_1_rmse: 0.45726 |  0:00:32s
epoch 5  | loss: 0.22603 | val_0_rmse: 0.45711 | val_1_rmse: 0.45624 |  0:00:39s
epoch 6  | loss: 0.21977 | val_0_rmse: 0.44919 | val_1_rmse: 0.44938 |  0:00:45s
epoch 7  | loss: 0.21585 | val_0_rmse: 0.43711 | val_1_rmse: 0.43358 |  0:00:52s
epoch 8  | loss: 0.21077 | val_0_rmse: 0.43895 | val_1_rmse: 0.44158 |  0:00:59s
epoch 9  | loss: 0.20958 | val_0_rmse: 0.44167 | val_1_rmse: 0.44316 |  0:01:05s
epoch 10 | loss: 0.2093  | val_0_rmse: 0.43408 | val_1_rmse: 0.43773 |  0:01:12s
epoch 11 | loss: 0.20919 | val_0_rmse: 0.43883 | val_1_rmse: 0.44142 |  0:01:18s
epoch 12 | loss: 0.2044  | val_0_rmse: 0.43279 | val_1_rmse: 0.43484 |  0:01:25s
epoch 13 | loss: 0.20472 | val_0_rmse: 0.44292 | val_1_rmse: 0.43993 |  0:01:31s
epoch 14 | loss: 0.20568 | val_0_rmse: 0.43472 | val_1_rmse: 0.4389  |  0:01:38s
epoch 15 | loss: 0.19972 | val_0_rmse: 0.46373 | val_1_rmse: 0.46075 |  0:01:44s
epoch 16 | loss: 0.20321 | val_0_rmse: 0.42826 | val_1_rmse: 0.42879 |  0:01:51s
epoch 17 | loss: 0.20122 | val_0_rmse: 0.41947 | val_1_rmse: 0.42105 |  0:01:57s
epoch 18 | loss: 0.19649 | val_0_rmse: 0.44203 | val_1_rmse: 0.44487 |  0:02:04s
epoch 19 | loss: 0.19583 | val_0_rmse: 0.42535 | val_1_rmse: 0.42773 |  0:02:10s
epoch 20 | loss: 0.19993 | val_0_rmse: 0.42542 | val_1_rmse: 0.42943 |  0:02:17s
epoch 21 | loss: 0.19508 | val_0_rmse: 0.43911 | val_1_rmse: 0.44325 |  0:02:23s
epoch 22 | loss: 0.19691 | val_0_rmse: 0.42328 | val_1_rmse: 0.42649 |  0:02:30s
epoch 23 | loss: 0.19669 | val_0_rmse: 0.42232 | val_1_rmse: 0.42214 |  0:02:36s
epoch 24 | loss: 0.19387 | val_0_rmse: 0.41714 | val_1_rmse: 0.41636 |  0:02:43s
epoch 25 | loss: 0.19335 | val_0_rmse: 0.42914 | val_1_rmse: 0.43526 |  0:02:49s
epoch 26 | loss: 0.19208 | val_0_rmse: 0.41691 | val_1_rmse: 0.41808 |  0:02:56s
epoch 27 | loss: 0.19265 | val_0_rmse: 0.42804 | val_1_rmse: 0.42996 |  0:03:02s
epoch 28 | loss: 0.19046 | val_0_rmse: 0.43562 | val_1_rmse: 0.4386  |  0:03:09s
epoch 29 | loss: 0.19137 | val_0_rmse: 0.4203  | val_1_rmse: 0.42052 |  0:03:15s
epoch 30 | loss: 0.19369 | val_0_rmse: 0.41101 | val_1_rmse: 0.41119 |  0:03:22s
epoch 31 | loss: 0.19084 | val_0_rmse: 0.41597 | val_1_rmse: 0.41679 |  0:03:29s
epoch 32 | loss: 0.18741 | val_0_rmse: 0.41    | val_1_rmse: 0.4122  |  0:03:35s
epoch 33 | loss: 0.19182 | val_0_rmse: 0.43203 | val_1_rmse: 0.43433 |  0:03:41s
epoch 34 | loss: 0.18894 | val_0_rmse: 0.47186 | val_1_rmse: 0.47524 |  0:03:47s
epoch 35 | loss: 0.19125 | val_0_rmse: 0.41934 | val_1_rmse: 0.4221  |  0:03:54s
epoch 36 | loss: 0.18905 | val_0_rmse: 0.44438 | val_1_rmse: 0.44641 |  0:04:00s
epoch 37 | loss: 0.1928  | val_0_rmse: 0.42581 | val_1_rmse: 0.43008 |  0:04:06s
epoch 38 | loss: 0.18938 | val_0_rmse: 0.47566 | val_1_rmse: 0.47608 |  0:04:13s
epoch 39 | loss: 0.18737 | val_0_rmse: 0.41522 | val_1_rmse: 0.41863 |  0:04:19s
epoch 40 | loss: 0.18433 | val_0_rmse: 0.41678 | val_1_rmse: 0.42038 |  0:04:25s
epoch 41 | loss: 0.18596 | val_0_rmse: 0.41196 | val_1_rmse: 0.41435 |  0:04:32s
epoch 42 | loss: 0.18552 | val_0_rmse: 0.41225 | val_1_rmse: 0.41531 |  0:04:38s
epoch 43 | loss: 0.18383 | val_0_rmse: 0.40702 | val_1_rmse: 0.40978 |  0:04:44s
epoch 44 | loss: 0.1842  | val_0_rmse: 0.4113  | val_1_rmse: 0.41547 |  0:04:51s
epoch 45 | loss: 0.18842 | val_0_rmse: 0.44392 | val_1_rmse: 0.45028 |  0:04:57s
epoch 46 | loss: 0.18656 | val_0_rmse: 0.40688 | val_1_rmse: 0.40866 |  0:05:04s
epoch 47 | loss: 0.18533 | val_0_rmse: 0.4176  | val_1_rmse: 0.42135 |  0:05:10s
epoch 48 | loss: 0.18367 | val_0_rmse: 0.41773 | val_1_rmse: 0.42072 |  0:05:17s
epoch 49 | loss: 0.18488 | val_0_rmse: 0.41019 | val_1_rmse: 0.41399 |  0:05:23s
epoch 50 | loss: 0.19052 | val_0_rmse: 0.43388 | val_1_rmse: 0.43538 |  0:05:29s
epoch 51 | loss: 0.19179 | val_0_rmse: 0.41672 | val_1_rmse: 0.41888 |  0:05:36s
epoch 52 | loss: 0.19036 | val_0_rmse: 0.41016 | val_1_rmse: 0.4133  |  0:05:42s
epoch 53 | loss: 0.18833 | val_0_rmse: 0.42755 | val_1_rmse: 0.42975 |  0:05:49s
epoch 54 | loss: 0.18703 | val_0_rmse: 0.41315 | val_1_rmse: 0.41757 |  0:05:55s
epoch 55 | loss: 0.18448 | val_0_rmse: 0.40534 | val_1_rmse: 0.40988 |  0:06:01s
epoch 56 | loss: 0.18615 | val_0_rmse: 0.41334 | val_1_rmse: 0.4181  |  0:06:08s
epoch 57 | loss: 0.18475 | val_0_rmse: 0.42269 | val_1_rmse: 0.42643 |  0:06:14s
epoch 58 | loss: 0.18602 | val_0_rmse: 0.43219 | val_1_rmse: 0.44078 |  0:06:21s
epoch 59 | loss: 0.18621 | val_0_rmse: 0.45255 | val_1_rmse: 0.45418 |  0:06:27s
epoch 60 | loss: 0.18794 | val_0_rmse: 0.40368 | val_1_rmse: 0.40765 |  0:06:33s
epoch 61 | loss: 0.18538 | val_0_rmse: 0.41395 | val_1_rmse: 0.41889 |  0:06:40s
epoch 62 | loss: 0.18499 | val_0_rmse: 0.41614 | val_1_rmse: 0.41824 |  0:06:46s
epoch 63 | loss: 0.18452 | val_0_rmse: 0.40698 | val_1_rmse: 0.41155 |  0:06:53s
epoch 64 | loss: 0.18103 | val_0_rmse: 0.40586 | val_1_rmse: 0.41011 |  0:06:59s
epoch 65 | loss: 0.18509 | val_0_rmse: 0.42244 | val_1_rmse: 0.42359 |  0:07:06s
epoch 66 | loss: 0.1857  | val_0_rmse: 0.41344 | val_1_rmse: 0.41576 |  0:07:12s
epoch 67 | loss: 0.18379 | val_0_rmse: 0.40918 | val_1_rmse: 0.41136 |  0:07:18s
epoch 68 | loss: 0.18246 | val_0_rmse: 0.41123 | val_1_rmse: 0.41698 |  0:07:25s
epoch 69 | loss: 0.19412 | val_0_rmse: 0.42847 | val_1_rmse: 0.43127 |  0:07:31s
epoch 70 | loss: 0.1876  | val_0_rmse: 0.43098 | val_1_rmse: 0.43538 |  0:07:38s
epoch 71 | loss: 0.18071 | val_0_rmse: 0.40478 | val_1_rmse: 0.40992 |  0:07:44s
epoch 72 | loss: 0.18288 | val_0_rmse: 0.43069 | val_1_rmse: 0.43198 |  0:07:50s
epoch 73 | loss: 0.18183 | val_0_rmse: 0.40862 | val_1_rmse: 0.41349 |  0:07:57s
epoch 74 | loss: 0.18249 | val_0_rmse: 0.4485  | val_1_rmse: 0.45143 |  0:08:03s
epoch 75 | loss: 0.19332 | val_0_rmse: 0.41615 | val_1_rmse: 0.42148 |  0:08:10s
epoch 76 | loss: 0.18831 | val_0_rmse: 0.43124 | val_1_rmse: 0.43231 |  0:08:16s
epoch 77 | loss: 0.19434 | val_0_rmse: 0.45412 | val_1_rmse: 0.45134 |  0:08:23s
epoch 78 | loss: 0.19386 | val_0_rmse: 0.42059 | val_1_rmse: 0.42532 |  0:08:29s
epoch 79 | loss: 0.18594 | val_0_rmse: 0.42521 | val_1_rmse: 0.42612 |  0:08:35s
epoch 80 | loss: 0.18398 | val_0_rmse: 0.41265 | val_1_rmse: 0.41868 |  0:08:42s
epoch 81 | loss: 0.18211 | val_0_rmse: 0.41017 | val_1_rmse: 0.41514 |  0:08:48s
epoch 82 | loss: 0.18624 | val_0_rmse: 0.41476 | val_1_rmse: 0.42124 |  0:08:55s
epoch 83 | loss: 0.18001 | val_0_rmse: 0.44401 | val_1_rmse: 0.44397 |  0:09:01s
epoch 84 | loss: 0.18286 | val_0_rmse: 0.41203 | val_1_rmse: 0.41458 |  0:09:07s
epoch 85 | loss: 0.1825  | val_0_rmse: 0.42613 | val_1_rmse: 0.43247 |  0:09:14s
epoch 86 | loss: 0.17904 | val_0_rmse: 0.40268 | val_1_rmse: 0.4045  |  0:09:20s
epoch 87 | loss: 0.18157 | val_0_rmse: 0.4225  | val_1_rmse: 0.4259  |  0:09:27s
epoch 88 | loss: 0.17979 | val_0_rmse: 0.41796 | val_1_rmse: 0.4255  |  0:09:33s
epoch 89 | loss: 0.19149 | val_0_rmse: 0.42502 | val_1_rmse: 0.42685 |  0:09:39s
epoch 90 | loss: 0.19017 | val_0_rmse: 0.41471 | val_1_rmse: 0.42148 |  0:09:46s
epoch 91 | loss: 0.18644 | val_0_rmse: 0.41244 | val_1_rmse: 0.41662 |  0:09:52s
epoch 92 | loss: 0.18638 | val_0_rmse: 0.40891 | val_1_rmse: 0.41367 |  0:09:59s
epoch 93 | loss: 0.18367 | val_0_rmse: 0.40603 | val_1_rmse: 0.41324 |  0:10:05s
epoch 94 | loss: 0.18055 | val_0_rmse: 0.40961 | val_1_rmse: 0.41541 |  0:10:11s
epoch 95 | loss: 0.18506 | val_0_rmse: 0.42991 | val_1_rmse: 0.43675 |  0:10:18s
epoch 96 | loss: 0.1819  | val_0_rmse: 0.40572 | val_1_rmse: 0.40963 |  0:10:24s
epoch 97 | loss: 0.18288 | val_0_rmse: 0.41946 | val_1_rmse: 0.42343 |  0:10:31s
epoch 98 | loss: 0.19055 | val_0_rmse: 0.45946 | val_1_rmse: 0.45699 |  0:10:37s
epoch 99 | loss: 0.18901 | val_0_rmse: 0.43725 | val_1_rmse: 0.43686 |  0:10:43s
epoch 100| loss: 0.18903 | val_0_rmse: 0.41017 | val_1_rmse: 0.41373 |  0:10:50s
epoch 101| loss: 0.18266 | val_0_rmse: 0.40429 | val_1_rmse: 0.40761 |  0:10:56s
epoch 102| loss: 0.18177 | val_0_rmse: 0.40329 | val_1_rmse: 0.40655 |  0:11:03s
epoch 103| loss: 0.18369 | val_0_rmse: 0.40268 | val_1_rmse: 0.40546 |  0:11:09s
epoch 104| loss: 0.18013 | val_0_rmse: 0.40969 | val_1_rmse: 0.4139  |  0:11:15s
epoch 105| loss: 0.18177 | val_0_rmse: 0.40056 | val_1_rmse: 0.40352 |  0:11:22s
epoch 106| loss: 0.18017 | val_0_rmse: 0.41269 | val_1_rmse: 0.42005 |  0:11:28s
epoch 107| loss: 0.18109 | val_0_rmse: 0.41215 | val_1_rmse: 0.41718 |  0:11:35s
epoch 108| loss: 0.18009 | val_0_rmse: 0.40473 | val_1_rmse: 0.41214 |  0:11:41s
epoch 109| loss: 0.18121 | val_0_rmse: 0.4208  | val_1_rmse: 0.4282  |  0:11:47s
epoch 110| loss: 0.17716 | val_0_rmse: 0.40279 | val_1_rmse: 0.41103 |  0:11:54s
epoch 111| loss: 0.17878 | val_0_rmse: 0.40403 | val_1_rmse: 0.41171 |  0:12:00s
epoch 112| loss: 0.1776  | val_0_rmse: 0.40247 | val_1_rmse: 0.4101  |  0:12:07s
epoch 113| loss: 0.1754  | val_0_rmse: 0.40507 | val_1_rmse: 0.41183 |  0:12:13s
epoch 114| loss: 0.18406 | val_0_rmse: 0.41597 | val_1_rmse: 0.42063 |  0:12:19s
epoch 115| loss: 0.18104 | val_0_rmse: 0.43199 | val_1_rmse: 0.4326  |  0:12:26s
epoch 116| loss: 0.18661 | val_0_rmse: 0.43215 | val_1_rmse: 0.43446 |  0:12:32s
epoch 117| loss: 0.18573 | val_0_rmse: 0.40768 | val_1_rmse: 0.41025 |  0:12:38s
epoch 118| loss: 0.18167 | val_0_rmse: 0.41311 | val_1_rmse: 0.41299 |  0:12:45s
epoch 119| loss: 0.18017 | val_0_rmse: 0.4037  | val_1_rmse: 0.40704 |  0:12:51s
epoch 120| loss: 0.19024 | val_0_rmse: 0.42247 | val_1_rmse: 0.42328 |  0:12:58s
epoch 121| loss: 0.18669 | val_0_rmse: 0.40646 | val_1_rmse: 0.41171 |  0:13:04s
epoch 122| loss: 0.18763 | val_0_rmse: 0.42422 | val_1_rmse: 0.43071 |  0:13:10s
epoch 123| loss: 0.17848 | val_0_rmse: 0.41203 | val_1_rmse: 0.41733 |  0:13:17s
epoch 124| loss: 0.18358 | val_0_rmse: 0.40723 | val_1_rmse: 0.41197 |  0:13:23s
epoch 125| loss: 0.18285 | val_0_rmse: 0.41071 | val_1_rmse: 0.41231 |  0:13:29s
epoch 126| loss: 0.18304 | val_0_rmse: 0.40383 | val_1_rmse: 0.40959 |  0:13:36s
epoch 127| loss: 0.1751  | val_0_rmse: 0.40165 | val_1_rmse: 0.40348 |  0:13:42s
epoch 128| loss: 0.17543 | val_0_rmse: 0.40199 | val_1_rmse: 0.40571 |  0:13:49s
epoch 129| loss: 0.17632 | val_0_rmse: 0.40478 | val_1_rmse: 0.40967 |  0:13:55s
epoch 130| loss: 0.18512 | val_0_rmse: 0.42153 | val_1_rmse: 0.42183 |  0:14:01s
epoch 131| loss: 0.19566 | val_0_rmse: 0.45986 | val_1_rmse: 0.46362 |  0:14:08s
epoch 132| loss: 0.19842 | val_0_rmse: 0.4225  | val_1_rmse: 0.42546 |  0:14:14s
epoch 133| loss: 0.20336 | val_0_rmse: 0.42243 | val_1_rmse: 0.42559 |  0:14:21s
epoch 134| loss: 0.19219 | val_0_rmse: 0.45486 | val_1_rmse: 0.45722 |  0:14:27s
epoch 135| loss: 0.20212 | val_0_rmse: 0.44567 | val_1_rmse: 0.4462  |  0:14:34s
epoch 136| loss: 0.19732 | val_0_rmse: 0.41211 | val_1_rmse: 0.41242 |  0:14:40s
epoch 137| loss: 0.19023 | val_0_rmse: 0.41527 | val_1_rmse: 0.41566 |  0:14:46s
epoch 138| loss: 0.19253 | val_0_rmse: 0.42524 | val_1_rmse: 0.42659 |  0:14:53s
epoch 139| loss: 0.18945 | val_0_rmse: 0.40592 | val_1_rmse: 0.40801 |  0:14:59s
epoch 140| loss: 0.18424 | val_0_rmse: 0.4507  | val_1_rmse: 0.45276 |  0:15:05s
epoch 141| loss: 0.18444 | val_0_rmse: 0.41685 | val_1_rmse: 0.41849 |  0:15:12s
epoch 142| loss: 0.18322 | val_0_rmse: 0.42896 | val_1_rmse: 0.42939 |  0:15:18s
epoch 143| loss: 0.18054 | val_0_rmse: 0.41144 | val_1_rmse: 0.41712 |  0:15:25s
epoch 144| loss: 0.18392 | val_0_rmse: 0.41726 | val_1_rmse: 0.42302 |  0:15:31s
epoch 145| loss: 0.17848 | val_0_rmse: 0.40175 | val_1_rmse: 0.4064  |  0:15:37s
epoch 146| loss: 0.18466 | val_0_rmse: 0.42733 | val_1_rmse: 0.42945 |  0:15:44s
epoch 147| loss: 0.18443 | val_0_rmse: 0.44781 | val_1_rmse: 0.45257 |  0:15:50s
epoch 148| loss: 0.1793  | val_0_rmse: 0.39834 | val_1_rmse: 0.40212 |  0:15:57s
epoch 149| loss: 0.17727 | val_0_rmse: 0.40301 | val_1_rmse: 0.40726 |  0:16:03s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.40212
Best weights from best epoch are automatically used!
ended training at: 23:02:53
Feature importance:
[('Area', 0.0), ('Baths', 0.1538467279185506), ('Beds', 0.191933956030304), ('Latitude', 0.19237086176499377), ('Longitude', 0.24078321344319284), ('Month', 0.0), ('Year', 0.22106524084295875)]
Mean squared error is of 9312326323.03312
Mean absolute error:67496.402222927
MAPE:0.2930179092201708
R2 score:0.8424229828423239
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:12:34
epoch 0  | loss: 0.38395 | val_0_rmse: 0.54328 | val_1_rmse: 0.54535 |  0:00:06s
epoch 1  | loss: 0.28156 | val_0_rmse: 0.51674 | val_1_rmse: 0.51506 |  0:00:12s
epoch 2  | loss: 0.26706 | val_0_rmse: 0.50045 | val_1_rmse: 0.49921 |  0:00:19s
epoch 3  | loss: 0.2659  | val_0_rmse: 0.49912 | val_1_rmse: 0.49704 |  0:00:25s
epoch 4  | loss: 0.25289 | val_0_rmse: 0.492   | val_1_rmse: 0.49062 |  0:00:32s
epoch 5  | loss: 0.2485  | val_0_rmse: 0.48809 | val_1_rmse: 0.48703 |  0:00:38s
epoch 6  | loss: 0.24278 | val_0_rmse: 0.47131 | val_1_rmse: 0.4683  |  0:00:45s
epoch 7  | loss: 0.23151 | val_0_rmse: 0.4663  | val_1_rmse: 0.46315 |  0:00:51s
epoch 8  | loss: 0.22574 | val_0_rmse: 0.45576 | val_1_rmse: 0.45366 |  0:00:58s
epoch 9  | loss: 0.22212 | val_0_rmse: 0.46464 | val_1_rmse: 0.4644  |  0:01:04s
epoch 10 | loss: 0.21886 | val_0_rmse: 0.44708 | val_1_rmse: 0.44671 |  0:01:11s
epoch 11 | loss: 0.21549 | val_0_rmse: 0.44843 | val_1_rmse: 0.44749 |  0:01:17s
epoch 12 | loss: 0.21664 | val_0_rmse: 0.43641 | val_1_rmse: 0.43472 |  0:01:24s
epoch 13 | loss: 0.2139  | val_0_rmse: 0.44557 | val_1_rmse: 0.44394 |  0:01:30s
epoch 14 | loss: 0.21269 | val_0_rmse: 0.44038 | val_1_rmse: 0.43926 |  0:01:37s
epoch 15 | loss: 0.21016 | val_0_rmse: 0.44529 | val_1_rmse: 0.44357 |  0:01:43s
epoch 16 | loss: 0.20899 | val_0_rmse: 0.46086 | val_1_rmse: 0.46    |  0:01:50s
epoch 17 | loss: 0.21377 | val_0_rmse: 0.45353 | val_1_rmse: 0.45341 |  0:01:56s
epoch 18 | loss: 0.20863 | val_0_rmse: 0.43675 | val_1_rmse: 0.43587 |  0:02:03s
epoch 19 | loss: 0.20998 | val_0_rmse: 0.44615 | val_1_rmse: 0.4472  |  0:02:09s
epoch 20 | loss: 0.20367 | val_0_rmse: 0.42919 | val_1_rmse: 0.42868 |  0:02:16s
epoch 21 | loss: 0.20375 | val_0_rmse: 0.43288 | val_1_rmse: 0.43147 |  0:02:22s
epoch 22 | loss: 0.20043 | val_0_rmse: 0.43336 | val_1_rmse: 0.4324  |  0:02:29s
epoch 23 | loss: 0.20306 | val_0_rmse: 0.45513 | val_1_rmse: 0.45569 |  0:02:35s
epoch 24 | loss: 0.20462 | val_0_rmse: 0.44278 | val_1_rmse: 0.44287 |  0:02:42s
epoch 25 | loss: 0.20803 | val_0_rmse: 0.42573 | val_1_rmse: 0.42461 |  0:02:48s
epoch 26 | loss: 0.20381 | val_0_rmse: 0.43517 | val_1_rmse: 0.43385 |  0:02:55s
epoch 27 | loss: 0.19983 | val_0_rmse: 0.43822 | val_1_rmse: 0.4398  |  0:03:01s
epoch 28 | loss: 0.19902 | val_0_rmse: 0.45602 | val_1_rmse: 0.45605 |  0:03:08s
epoch 29 | loss: 0.20154 | val_0_rmse: 0.42297 | val_1_rmse: 0.42273 |  0:03:14s
epoch 30 | loss: 0.19547 | val_0_rmse: 0.4248  | val_1_rmse: 0.42324 |  0:03:21s
epoch 31 | loss: 0.1975  | val_0_rmse: 0.42702 | val_1_rmse: 0.42577 |  0:03:27s
epoch 32 | loss: 0.19755 | val_0_rmse: 0.42517 | val_1_rmse: 0.42443 |  0:03:34s
epoch 33 | loss: 0.22122 | val_0_rmse: 0.45129 | val_1_rmse: 0.45046 |  0:03:41s
epoch 34 | loss: 0.20548 | val_0_rmse: 0.43187 | val_1_rmse: 0.4331  |  0:03:47s
epoch 35 | loss: 0.20325 | val_0_rmse: 0.45127 | val_1_rmse: 0.45018 |  0:03:54s
epoch 36 | loss: 0.19879 | val_0_rmse: 0.43918 | val_1_rmse: 0.43835 |  0:04:00s
epoch 37 | loss: 0.1949  | val_0_rmse: 0.45072 | val_1_rmse: 0.44998 |  0:04:07s
epoch 38 | loss: 0.19409 | val_0_rmse: 0.42915 | val_1_rmse: 0.43047 |  0:04:13s
epoch 39 | loss: 0.19036 | val_0_rmse: 0.42016 | val_1_rmse: 0.42007 |  0:04:20s
epoch 40 | loss: 0.19457 | val_0_rmse: 0.42415 | val_1_rmse: 0.42329 |  0:04:26s
epoch 41 | loss: 0.1906  | val_0_rmse: 0.42226 | val_1_rmse: 0.42291 |  0:04:34s
epoch 42 | loss: 0.19085 | val_0_rmse: 0.41832 | val_1_rmse: 0.41824 |  0:04:40s
epoch 43 | loss: 0.19468 | val_0_rmse: 0.42767 | val_1_rmse: 0.42805 |  0:04:47s
epoch 44 | loss: 0.19028 | val_0_rmse: 0.43424 | val_1_rmse: 0.43617 |  0:04:53s
epoch 45 | loss: 0.18916 | val_0_rmse: 0.42738 | val_1_rmse: 0.42951 |  0:05:00s
epoch 46 | loss: 0.18862 | val_0_rmse: 0.41537 | val_1_rmse: 0.41848 |  0:05:06s
epoch 47 | loss: 0.19196 | val_0_rmse: 0.44304 | val_1_rmse: 0.44401 |  0:05:13s
epoch 48 | loss: 0.19186 | val_0_rmse: 0.42178 | val_1_rmse: 0.42195 |  0:05:19s
epoch 49 | loss: 0.18684 | val_0_rmse: 0.42104 | val_1_rmse: 0.42424 |  0:05:26s
epoch 50 | loss: 0.18786 | val_0_rmse: 0.41174 | val_1_rmse: 0.41243 |  0:05:32s
epoch 51 | loss: 0.18505 | val_0_rmse: 0.41323 | val_1_rmse: 0.4145  |  0:05:37s
epoch 52 | loss: 0.18285 | val_0_rmse: 0.40653 | val_1_rmse: 0.40944 |  0:05:43s
epoch 53 | loss: 0.18519 | val_0_rmse: 0.41299 | val_1_rmse: 0.4164  |  0:05:48s
epoch 54 | loss: 0.18392 | val_0_rmse: 0.42008 | val_1_rmse: 0.42334 |  0:05:53s
epoch 55 | loss: 0.18568 | val_0_rmse: 0.40335 | val_1_rmse: 0.40597 |  0:05:58s
epoch 56 | loss: 0.18512 | val_0_rmse: 0.40236 | val_1_rmse: 0.40565 |  0:06:03s
epoch 57 | loss: 0.18428 | val_0_rmse: 0.43621 | val_1_rmse: 0.43878 |  0:06:08s
epoch 58 | loss: 0.20468 | val_0_rmse: 0.43979 | val_1_rmse: 0.43977 |  0:06:13s
epoch 59 | loss: 0.20148 | val_0_rmse: 0.42352 | val_1_rmse: 0.42386 |  0:06:19s
epoch 60 | loss: 0.19013 | val_0_rmse: 0.41046 | val_1_rmse: 0.41244 |  0:06:24s
epoch 61 | loss: 0.19008 | val_0_rmse: 0.4181  | val_1_rmse: 0.41892 |  0:06:29s
epoch 62 | loss: 0.19077 | val_0_rmse: 0.42751 | val_1_rmse: 0.43018 |  0:06:34s
epoch 63 | loss: 0.18887 | val_0_rmse: 0.41434 | val_1_rmse: 0.4169  |  0:06:39s
epoch 64 | loss: 0.1874  | val_0_rmse: 0.41887 | val_1_rmse: 0.41974 |  0:06:44s
epoch 65 | loss: 0.18953 | val_0_rmse: 0.41563 | val_1_rmse: 0.4166  |  0:06:49s
epoch 66 | loss: 0.182   | val_0_rmse: 0.40881 | val_1_rmse: 0.41313 |  0:06:55s
epoch 67 | loss: 0.18959 | val_0_rmse: 0.45778 | val_1_rmse: 0.45703 |  0:07:00s
epoch 68 | loss: 0.18733 | val_0_rmse: 0.41221 | val_1_rmse: 0.41387 |  0:07:05s
epoch 69 | loss: 0.17987 | val_0_rmse: 0.40432 | val_1_rmse: 0.40877 |  0:07:10s
epoch 70 | loss: 0.1802  | val_0_rmse: 0.4146  | val_1_rmse: 0.41831 |  0:07:15s
epoch 71 | loss: 0.18463 | val_0_rmse: 0.4196  | val_1_rmse: 0.42217 |  0:07:21s
epoch 72 | loss: 0.18182 | val_0_rmse: 0.41266 | val_1_rmse: 0.41624 |  0:07:26s
epoch 73 | loss: 0.18065 | val_0_rmse: 0.41238 | val_1_rmse: 0.41507 |  0:07:31s
epoch 74 | loss: 0.17878 | val_0_rmse: 0.4044  | val_1_rmse: 0.40784 |  0:07:36s
epoch 75 | loss: 0.17919 | val_0_rmse: 0.42716 | val_1_rmse: 0.43021 |  0:07:41s
epoch 76 | loss: 0.17732 | val_0_rmse: 0.40523 | val_1_rmse: 0.41009 |  0:07:47s
epoch 77 | loss: 0.17417 | val_0_rmse: 0.39445 | val_1_rmse: 0.3985  |  0:07:52s
epoch 78 | loss: 0.17803 | val_0_rmse: 0.39655 | val_1_rmse: 0.40171 |  0:07:57s
epoch 79 | loss: 0.17243 | val_0_rmse: 0.40095 | val_1_rmse: 0.40473 |  0:08:02s
epoch 80 | loss: 0.18377 | val_0_rmse: 0.4204  | val_1_rmse: 0.4251  |  0:08:07s
epoch 81 | loss: 0.18212 | val_0_rmse: 0.40823 | val_1_rmse: 0.41233 |  0:08:12s
epoch 82 | loss: 0.18356 | val_0_rmse: 0.40683 | val_1_rmse: 0.41093 |  0:08:17s
epoch 83 | loss: 0.17978 | val_0_rmse: 0.40172 | val_1_rmse: 0.40661 |  0:08:22s
epoch 84 | loss: 0.18043 | val_0_rmse: 0.42013 | val_1_rmse: 0.42434 |  0:08:27s
epoch 85 | loss: 0.18758 | val_0_rmse: 0.40938 | val_1_rmse: 0.41255 |  0:08:32s
epoch 86 | loss: 0.18087 | val_0_rmse: 0.46092 | val_1_rmse: 0.46334 |  0:08:37s
epoch 87 | loss: 0.17953 | val_0_rmse: 0.39766 | val_1_rmse: 0.40196 |  0:08:42s
epoch 88 | loss: 0.17834 | val_0_rmse: 0.41061 | val_1_rmse: 0.41533 |  0:08:47s
epoch 89 | loss: 0.1764  | val_0_rmse: 0.41368 | val_1_rmse: 0.41727 |  0:08:53s
epoch 90 | loss: 0.18474 | val_0_rmse: 0.43817 | val_1_rmse: 0.43854 |  0:08:58s
epoch 91 | loss: 0.19347 | val_0_rmse: 0.43802 | val_1_rmse: 0.44022 |  0:09:03s
epoch 92 | loss: 0.20633 | val_0_rmse: 0.43549 | val_1_rmse: 0.4366  |  0:09:08s
epoch 93 | loss: 0.20242 | val_0_rmse: 0.47612 | val_1_rmse: 0.47762 |  0:09:13s
epoch 94 | loss: 0.19736 | val_0_rmse: 0.42224 | val_1_rmse: 0.4256  |  0:09:18s
epoch 95 | loss: 0.1886  | val_0_rmse: 0.4496  | val_1_rmse: 0.45122 |  0:09:23s
epoch 96 | loss: 0.19538 | val_0_rmse: 0.41902 | val_1_rmse: 0.42436 |  0:09:28s
epoch 97 | loss: 0.19688 | val_0_rmse: 0.42934 | val_1_rmse: 0.43227 |  0:09:33s
epoch 98 | loss: 0.19811 | val_0_rmse: 0.42135 | val_1_rmse: 0.42545 |  0:09:38s
epoch 99 | loss: 0.19314 | val_0_rmse: 0.43298 | val_1_rmse: 0.4336  |  0:09:43s
epoch 100| loss: 0.19281 | val_0_rmse: 0.45159 | val_1_rmse: 0.45394 |  0:09:48s
epoch 101| loss: 0.18826 | val_0_rmse: 0.40926 | val_1_rmse: 0.41284 |  0:09:53s
epoch 102| loss: 0.18287 | val_0_rmse: 0.42385 | val_1_rmse: 0.42696 |  0:09:58s
epoch 103| loss: 0.18435 | val_0_rmse: 0.41166 | val_1_rmse: 0.41475 |  0:10:04s
epoch 104| loss: 0.18311 | val_0_rmse: 0.40521 | val_1_rmse: 0.41051 |  0:10:09s
epoch 105| loss: 0.18225 | val_0_rmse: 0.41358 | val_1_rmse: 0.41667 |  0:10:14s
epoch 106| loss: 0.18178 | val_0_rmse: 0.40269 | val_1_rmse: 0.40798 |  0:10:19s
epoch 107| loss: 0.17983 | val_0_rmse: 0.41214 | val_1_rmse: 0.41636 |  0:10:24s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.3985
Best weights from best epoch are automatically used!
ended training at: 23:23:00
Feature importance:
[('Area', 0.2791217552610183), ('Baths', 0.0), ('Beds', 0.10554084955289626), ('Latitude', 0.3576829995514622), ('Longitude', 0.20305600049067807), ('Month', 0.0), ('Year', 0.05459839514394513)]
Mean squared error is of 4961260392.634703
Mean absolute error:49942.06654481697
MAPE:0.13285706543693682
R2 score:0.8394111874779027
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:23:01
epoch 0  | loss: 0.38111 | val_0_rmse: 0.53199 | val_1_rmse: 0.52784 |  0:00:05s
epoch 1  | loss: 0.27834 | val_0_rmse: 0.52673 | val_1_rmse: 0.52191 |  0:00:10s
epoch 2  | loss: 0.27168 | val_0_rmse: 0.51943 | val_1_rmse: 0.51458 |  0:00:15s
epoch 3  | loss: 0.2633  | val_0_rmse: 0.49336 | val_1_rmse: 0.48939 |  0:00:20s
epoch 4  | loss: 0.26651 | val_0_rmse: 0.52308 | val_1_rmse: 0.51813 |  0:00:25s
epoch 5  | loss: 0.2624  | val_0_rmse: 0.49274 | val_1_rmse: 0.48974 |  0:00:30s
epoch 6  | loss: 0.25395 | val_0_rmse: 0.49735 | val_1_rmse: 0.4946  |  0:00:35s
epoch 7  | loss: 0.25112 | val_0_rmse: 0.48701 | val_1_rmse: 0.48288 |  0:00:40s
epoch 8  | loss: 0.24852 | val_0_rmse: 0.48639 | val_1_rmse: 0.48253 |  0:00:46s
epoch 9  | loss: 0.23866 | val_0_rmse: 0.48787 | val_1_rmse: 0.48507 |  0:00:51s
epoch 10 | loss: 0.24412 | val_0_rmse: 0.47246 | val_1_rmse: 0.46872 |  0:00:56s
epoch 11 | loss: 0.23219 | val_0_rmse: 0.4568  | val_1_rmse: 0.45447 |  0:01:01s
epoch 12 | loss: 0.23375 | val_0_rmse: 0.48692 | val_1_rmse: 0.4827  |  0:01:06s
epoch 13 | loss: 0.22853 | val_0_rmse: 0.45928 | val_1_rmse: 0.45689 |  0:01:11s
epoch 14 | loss: 0.22734 | val_0_rmse: 0.4529  | val_1_rmse: 0.44877 |  0:01:16s
epoch 15 | loss: 0.21916 | val_0_rmse: 0.46066 | val_1_rmse: 0.45812 |  0:01:21s
epoch 16 | loss: 0.22146 | val_0_rmse: 0.44227 | val_1_rmse: 0.4421  |  0:01:26s
epoch 17 | loss: 0.21579 | val_0_rmse: 0.45808 | val_1_rmse: 0.45718 |  0:01:31s
epoch 18 | loss: 0.21638 | val_0_rmse: 0.4584  | val_1_rmse: 0.45772 |  0:01:36s
epoch 19 | loss: 0.21954 | val_0_rmse: 0.44245 | val_1_rmse: 0.44184 |  0:01:41s
epoch 20 | loss: 0.21388 | val_0_rmse: 0.44013 | val_1_rmse: 0.43738 |  0:01:46s
epoch 21 | loss: 0.21185 | val_0_rmse: 0.44296 | val_1_rmse: 0.44113 |  0:01:52s
epoch 22 | loss: 0.2183  | val_0_rmse: 0.46067 | val_1_rmse: 0.45984 |  0:01:57s
epoch 23 | loss: 0.2128  | val_0_rmse: 0.44274 | val_1_rmse: 0.44441 |  0:02:02s
epoch 24 | loss: 0.21109 | val_0_rmse: 0.43591 | val_1_rmse: 0.43627 |  0:02:07s
epoch 25 | loss: 0.21133 | val_0_rmse: 0.43802 | val_1_rmse: 0.43818 |  0:02:12s
epoch 26 | loss: 0.21011 | val_0_rmse: 0.43299 | val_1_rmse: 0.4321  |  0:02:17s
epoch 27 | loss: 0.20723 | val_0_rmse: 0.43385 | val_1_rmse: 0.43115 |  0:02:23s
epoch 28 | loss: 0.20698 | val_0_rmse: 0.43579 | val_1_rmse: 0.43551 |  0:02:28s
epoch 29 | loss: 0.20572 | val_0_rmse: 0.44386 | val_1_rmse: 0.44453 |  0:02:33s
epoch 30 | loss: 0.20933 | val_0_rmse: 0.46419 | val_1_rmse: 0.46276 |  0:02:38s
epoch 31 | loss: 0.21007 | val_0_rmse: 0.44693 | val_1_rmse: 0.44683 |  0:02:43s
epoch 32 | loss: 0.20968 | val_0_rmse: 0.44245 | val_1_rmse: 0.44007 |  0:02:48s
epoch 33 | loss: 0.20732 | val_0_rmse: 0.4498  | val_1_rmse: 0.45323 |  0:02:53s
epoch 34 | loss: 0.20543 | val_0_rmse: 0.43293 | val_1_rmse: 0.43225 |  0:02:58s
epoch 35 | loss: 0.20156 | val_0_rmse: 0.43654 | val_1_rmse: 0.43568 |  0:03:03s
epoch 36 | loss: 0.20708 | val_0_rmse: 0.43914 | val_1_rmse: 0.43995 |  0:03:08s
epoch 37 | loss: 0.21011 | val_0_rmse: 0.44172 | val_1_rmse: 0.43976 |  0:03:13s
epoch 38 | loss: 0.20541 | val_0_rmse: 0.429   | val_1_rmse: 0.42754 |  0:03:18s
epoch 39 | loss: 0.20439 | val_0_rmse: 0.43258 | val_1_rmse: 0.43307 |  0:03:23s
epoch 40 | loss: 0.20303 | val_0_rmse: 0.4333  | val_1_rmse: 0.43458 |  0:03:28s
epoch 41 | loss: 0.20548 | val_0_rmse: 0.42842 | val_1_rmse: 0.42699 |  0:03:34s
epoch 42 | loss: 0.20222 | val_0_rmse: 0.435   | val_1_rmse: 0.43701 |  0:03:39s
epoch 43 | loss: 0.20662 | val_0_rmse: 0.43483 | val_1_rmse: 0.43468 |  0:03:44s
epoch 44 | loss: 0.20382 | val_0_rmse: 0.43298 | val_1_rmse: 0.43275 |  0:03:49s
epoch 45 | loss: 0.19801 | val_0_rmse: 0.44622 | val_1_rmse: 0.44348 |  0:03:54s
epoch 46 | loss: 0.2016  | val_0_rmse: 0.42773 | val_1_rmse: 0.42839 |  0:03:59s
epoch 47 | loss: 0.20039 | val_0_rmse: 0.42992 | val_1_rmse: 0.42728 |  0:04:04s
epoch 48 | loss: 0.19814 | val_0_rmse: 0.42988 | val_1_rmse: 0.42906 |  0:04:09s
epoch 49 | loss: 0.20108 | val_0_rmse: 0.43211 | val_1_rmse: 0.42892 |  0:04:14s
epoch 50 | loss: 0.19756 | val_0_rmse: 0.44305 | val_1_rmse: 0.44492 |  0:04:19s
epoch 51 | loss: 0.19763 | val_0_rmse: 0.4379  | val_1_rmse: 0.43823 |  0:04:24s
epoch 52 | loss: 0.19569 | val_0_rmse: 0.43231 | val_1_rmse: 0.4321  |  0:04:30s
epoch 53 | loss: 0.19653 | val_0_rmse: 0.423   | val_1_rmse: 0.4236  |  0:04:35s
epoch 54 | loss: 0.1958  | val_0_rmse: 0.41573 | val_1_rmse: 0.4174  |  0:04:40s
epoch 55 | loss: 0.1949  | val_0_rmse: 0.42325 | val_1_rmse: 0.42442 |  0:04:45s
epoch 56 | loss: 0.19415 | val_0_rmse: 0.41795 | val_1_rmse: 0.41924 |  0:04:50s
epoch 57 | loss: 0.19891 | val_0_rmse: 0.42504 | val_1_rmse: 0.42795 |  0:04:55s
epoch 58 | loss: 0.1943  | val_0_rmse: 0.42277 | val_1_rmse: 0.42343 |  0:05:00s
epoch 59 | loss: 0.19518 | val_0_rmse: 0.4148  | val_1_rmse: 0.4195  |  0:05:05s
epoch 60 | loss: 0.19292 | val_0_rmse: 0.42285 | val_1_rmse: 0.42541 |  0:05:10s
epoch 61 | loss: 0.1958  | val_0_rmse: 0.43507 | val_1_rmse: 0.43671 |  0:05:15s
epoch 62 | loss: 0.19292 | val_0_rmse: 0.41896 | val_1_rmse: 0.42046 |  0:05:21s
epoch 63 | loss: 0.19445 | val_0_rmse: 0.4171  | val_1_rmse: 0.41967 |  0:05:26s
epoch 64 | loss: 0.19254 | val_0_rmse: 0.41318 | val_1_rmse: 0.41543 |  0:05:31s
epoch 65 | loss: 0.19569 | val_0_rmse: 0.4189  | val_1_rmse: 0.42363 |  0:05:36s
epoch 66 | loss: 0.19086 | val_0_rmse: 0.43178 | val_1_rmse: 0.43077 |  0:05:41s
epoch 67 | loss: 0.19164 | val_0_rmse: 0.41981 | val_1_rmse: 0.42242 |  0:05:46s
epoch 68 | loss: 0.19222 | val_0_rmse: 0.41627 | val_1_rmse: 0.41969 |  0:05:51s
epoch 69 | loss: 0.19631 | val_0_rmse: 0.42458 | val_1_rmse: 0.43084 |  0:05:56s
epoch 70 | loss: 0.19031 | val_0_rmse: 0.41169 | val_1_rmse: 0.41556 |  0:06:02s
epoch 71 | loss: 0.19305 | val_0_rmse: 0.41964 | val_1_rmse: 0.42087 |  0:06:07s
epoch 72 | loss: 0.1908  | val_0_rmse: 0.42468 | val_1_rmse: 0.42755 |  0:06:12s
epoch 73 | loss: 0.18704 | val_0_rmse: 0.41789 | val_1_rmse: 0.41844 |  0:06:17s
epoch 74 | loss: 0.18802 | val_0_rmse: 0.40938 | val_1_rmse: 0.41314 |  0:06:22s
epoch 75 | loss: 0.18797 | val_0_rmse: 0.42162 | val_1_rmse: 0.42574 |  0:06:27s
epoch 76 | loss: 0.18763 | val_0_rmse: 0.41166 | val_1_rmse: 0.41374 |  0:06:32s
epoch 77 | loss: 0.18801 | val_0_rmse: 0.4154  | val_1_rmse: 0.41924 |  0:06:37s
epoch 78 | loss: 0.18944 | val_0_rmse: 0.41349 | val_1_rmse: 0.41614 |  0:06:42s
epoch 79 | loss: 0.18554 | val_0_rmse: 0.41584 | val_1_rmse: 0.41969 |  0:06:47s
epoch 80 | loss: 0.18486 | val_0_rmse: 0.40812 | val_1_rmse: 0.41286 |  0:06:53s
epoch 81 | loss: 0.18415 | val_0_rmse: 0.40854 | val_1_rmse: 0.41059 |  0:06:58s
epoch 82 | loss: 0.18543 | val_0_rmse: 0.41118 | val_1_rmse: 0.41572 |  0:07:03s
epoch 83 | loss: 0.18767 | val_0_rmse: 0.40974 | val_1_rmse: 0.41213 |  0:07:08s
epoch 84 | loss: 0.18514 | val_0_rmse: 0.40338 | val_1_rmse: 0.40648 |  0:07:13s
epoch 85 | loss: 0.1847  | val_0_rmse: 0.40212 | val_1_rmse: 0.40677 |  0:07:18s
epoch 86 | loss: 0.18149 | val_0_rmse: 0.40501 | val_1_rmse: 0.40861 |  0:07:23s
epoch 87 | loss: 0.18242 | val_0_rmse: 0.40377 | val_1_rmse: 0.4077  |  0:07:28s
epoch 88 | loss: 0.18691 | val_0_rmse: 0.41271 | val_1_rmse: 0.41593 |  0:07:33s
epoch 89 | loss: 0.18437 | val_0_rmse: 0.41729 | val_1_rmse: 0.41923 |  0:07:38s
epoch 90 | loss: 0.18308 | val_0_rmse: 0.3996  | val_1_rmse: 0.40392 |  0:07:43s
epoch 91 | loss: 0.18165 | val_0_rmse: 0.40285 | val_1_rmse: 0.40748 |  0:07:48s
epoch 92 | loss: 0.19322 | val_0_rmse: 0.42659 | val_1_rmse: 0.42934 |  0:07:53s
epoch 93 | loss: 0.18928 | val_0_rmse: 0.40365 | val_1_rmse: 0.40762 |  0:07:58s
epoch 94 | loss: 0.18458 | val_0_rmse: 0.40323 | val_1_rmse: 0.40959 |  0:08:03s
epoch 95 | loss: 0.18256 | val_0_rmse: 0.4013  | val_1_rmse: 0.40592 |  0:08:09s
epoch 96 | loss: 0.18237 | val_0_rmse: 0.39617 | val_1_rmse: 0.39877 |  0:08:14s
epoch 97 | loss: 0.17973 | val_0_rmse: 0.40019 | val_1_rmse: 0.40632 |  0:08:19s
epoch 98 | loss: 0.18202 | val_0_rmse: 0.41847 | val_1_rmse: 0.41764 |  0:08:24s
epoch 99 | loss: 0.1785  | val_0_rmse: 0.40436 | val_1_rmse: 0.40687 |  0:08:29s
epoch 100| loss: 0.18116 | val_0_rmse: 0.39552 | val_1_rmse: 0.39868 |  0:08:34s
epoch 101| loss: 0.17837 | val_0_rmse: 0.39559 | val_1_rmse: 0.39882 |  0:08:39s
epoch 102| loss: 0.18262 | val_0_rmse: 0.40294 | val_1_rmse: 0.40815 |  0:08:45s
epoch 103| loss: 0.17968 | val_0_rmse: 0.40304 | val_1_rmse: 0.40449 |  0:08:50s
epoch 104| loss: 0.17825 | val_0_rmse: 0.41342 | val_1_rmse: 0.41967 |  0:08:55s
epoch 105| loss: 0.17772 | val_0_rmse: 0.41203 | val_1_rmse: 0.41649 |  0:09:00s
epoch 106| loss: 0.17905 | val_0_rmse: 0.41031 | val_1_rmse: 0.41709 |  0:09:05s
epoch 107| loss: 0.17743 | val_0_rmse: 0.40274 | val_1_rmse: 0.40745 |  0:09:10s
epoch 108| loss: 0.1772  | val_0_rmse: 0.39916 | val_1_rmse: 0.40073 |  0:09:15s
epoch 109| loss: 0.17684 | val_0_rmse: 0.39455 | val_1_rmse: 0.3996  |  0:09:20s
epoch 110| loss: 0.1764  | val_0_rmse: 0.38804 | val_1_rmse: 0.39259 |  0:09:25s
epoch 111| loss: 0.17596 | val_0_rmse: 0.38675 | val_1_rmse: 0.39302 |  0:09:31s
epoch 112| loss: 0.17698 | val_0_rmse: 0.3967  | val_1_rmse: 0.40044 |  0:09:36s
epoch 113| loss: 0.17694 | val_0_rmse: 0.39438 | val_1_rmse: 0.40148 |  0:09:41s
epoch 114| loss: 0.1781  | val_0_rmse: 0.39113 | val_1_rmse: 0.39813 |  0:09:46s
epoch 115| loss: 0.17128 | val_0_rmse: 0.38214 | val_1_rmse: 0.38959 |  0:09:51s
epoch 116| loss: 0.17934 | val_0_rmse: 0.40149 | val_1_rmse: 0.40512 |  0:09:56s
epoch 117| loss: 0.18187 | val_0_rmse: 0.40042 | val_1_rmse: 0.40603 |  0:10:01s
epoch 118| loss: 0.17645 | val_0_rmse: 0.38823 | val_1_rmse: 0.39435 |  0:10:06s
epoch 119| loss: 0.17579 | val_0_rmse: 0.38922 | val_1_rmse: 0.39422 |  0:10:11s
epoch 120| loss: 0.17582 | val_0_rmse: 0.39542 | val_1_rmse: 0.40073 |  0:10:16s
epoch 121| loss: 0.173   | val_0_rmse: 0.41438 | val_1_rmse: 0.41886 |  0:10:21s
epoch 122| loss: 0.18254 | val_0_rmse: 0.41598 | val_1_rmse: 0.41898 |  0:10:26s
epoch 123| loss: 0.17802 | val_0_rmse: 0.39648 | val_1_rmse: 0.4002  |  0:10:32s
epoch 124| loss: 0.17845 | val_0_rmse: 0.40909 | val_1_rmse: 0.41178 |  0:10:37s
epoch 125| loss: 0.17677 | val_0_rmse: 0.39459 | val_1_rmse: 0.40228 |  0:10:42s
epoch 126| loss: 0.17234 | val_0_rmse: 0.3983  | val_1_rmse: 0.40578 |  0:10:47s
epoch 127| loss: 0.17239 | val_0_rmse: 0.3808  | val_1_rmse: 0.3864  |  0:10:52s
epoch 128| loss: 0.1708  | val_0_rmse: 0.38233 | val_1_rmse: 0.3885  |  0:10:57s
epoch 129| loss: 0.16974 | val_0_rmse: 0.38293 | val_1_rmse: 0.3869  |  0:11:02s
epoch 130| loss: 0.17233 | val_0_rmse: 0.40124 | val_1_rmse: 0.40598 |  0:11:07s
epoch 131| loss: 0.17112 | val_0_rmse: 0.38761 | val_1_rmse: 0.39245 |  0:11:13s
epoch 132| loss: 0.16985 | val_0_rmse: 0.38477 | val_1_rmse: 0.39103 |  0:11:18s
epoch 133| loss: 0.17069 | val_0_rmse: 0.39362 | val_1_rmse: 0.40017 |  0:11:23s
epoch 134| loss: 0.16867 | val_0_rmse: 0.38907 | val_1_rmse: 0.39517 |  0:11:28s
epoch 135| loss: 0.16918 | val_0_rmse: 0.41363 | val_1_rmse: 0.41316 |  0:11:33s
epoch 136| loss: 0.17122 | val_0_rmse: 0.40181 | val_1_rmse: 0.40699 |  0:11:39s
epoch 137| loss: 0.17302 | val_0_rmse: 0.39569 | val_1_rmse: 0.40033 |  0:11:44s
epoch 138| loss: 0.1745  | val_0_rmse: 0.40924 | val_1_rmse: 0.41713 |  0:11:49s
epoch 139| loss: 0.16826 | val_0_rmse: 0.38091 | val_1_rmse: 0.38668 |  0:11:54s
epoch 140| loss: 0.16856 | val_0_rmse: 0.38475 | val_1_rmse: 0.38912 |  0:11:59s
epoch 141| loss: 0.17074 | val_0_rmse: 0.3818  | val_1_rmse: 0.3885  |  0:12:04s
epoch 142| loss: 0.16822 | val_0_rmse: 0.38088 | val_1_rmse: 0.38879 |  0:12:09s
epoch 143| loss: 0.16711 | val_0_rmse: 0.38274 | val_1_rmse: 0.39318 |  0:12:14s
epoch 144| loss: 0.16991 | val_0_rmse: 0.37864 | val_1_rmse: 0.38713 |  0:12:20s
epoch 145| loss: 0.1653  | val_0_rmse: 0.39496 | val_1_rmse: 0.4008  |  0:12:25s
epoch 146| loss: 0.17188 | val_0_rmse: 0.38822 | val_1_rmse: 0.39422 |  0:12:30s
epoch 147| loss: 0.16906 | val_0_rmse: 0.39327 | val_1_rmse: 0.39777 |  0:12:35s
epoch 148| loss: 0.16729 | val_0_rmse: 0.395   | val_1_rmse: 0.39805 |  0:12:40s
epoch 149| loss: 0.16871 | val_0_rmse: 0.38322 | val_1_rmse: 0.38924 |  0:12:45s
Stop training because you reached max_epochs = 150 with best_epoch = 127 and best_val_1_rmse = 0.3864
Best weights from best epoch are automatically used!
ended training at: 23:35:48
Feature importance:
[('Area', 0.4299451191922273), ('Baths', 0.0), ('Beds', 0.011336304547432048), ('Latitude', 0.37513476906329213), ('Longitude', 0.0), ('Month', 0.17760914580764403), ('Year', 0.005974661389404511)]
Mean squared error is of 4876758780.143885
Mean absolute error:50479.001883204364
MAPE:0.13906689102680647
R2 score:0.8425642574095479
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:43:46
epoch 0  | loss: 0.4453  | val_0_rmse: 0.56096 | val_1_rmse: 0.56798 |  0:00:04s
epoch 1  | loss: 0.30774 | val_0_rmse: 0.53657 | val_1_rmse: 0.53991 |  0:00:09s
epoch 2  | loss: 0.28968 | val_0_rmse: 0.52283 | val_1_rmse: 0.52835 |  0:00:14s
epoch 3  | loss: 0.28115 | val_0_rmse: 0.50624 | val_1_rmse: 0.51248 |  0:00:19s
epoch 4  | loss: 0.27228 | val_0_rmse: 0.51476 | val_1_rmse: 0.52084 |  0:00:24s
epoch 5  | loss: 0.26631 | val_0_rmse: 0.49516 | val_1_rmse: 0.50194 |  0:00:29s
epoch 6  | loss: 0.26441 | val_0_rmse: 0.49145 | val_1_rmse: 0.49825 |  0:00:34s
epoch 7  | loss: 0.26014 | val_0_rmse: 0.49642 | val_1_rmse: 0.50295 |  0:00:39s
epoch 8  | loss: 0.26243 | val_0_rmse: 0.48952 | val_1_rmse: 0.49812 |  0:00:44s
epoch 9  | loss: 0.25452 | val_0_rmse: 0.49413 | val_1_rmse: 0.50098 |  0:00:49s
epoch 10 | loss: 0.25094 | val_0_rmse: 0.4994  | val_1_rmse: 0.50782 |  0:00:54s
epoch 11 | loss: 0.24866 | val_0_rmse: 0.50256 | val_1_rmse: 0.5117  |  0:00:59s
epoch 12 | loss: 0.24649 | val_0_rmse: 0.50362 | val_1_rmse: 0.51067 |  0:01:04s
epoch 13 | loss: 0.24677 | val_0_rmse: 0.47891 | val_1_rmse: 0.48944 |  0:01:09s
epoch 14 | loss: 0.23968 | val_0_rmse: 0.46736 | val_1_rmse: 0.47736 |  0:01:14s
epoch 15 | loss: 0.23436 | val_0_rmse: 0.47628 | val_1_rmse: 0.48309 |  0:01:19s
epoch 16 | loss: 0.23866 | val_0_rmse: 0.46063 | val_1_rmse: 0.47034 |  0:01:24s
epoch 17 | loss: 0.23493 | val_0_rmse: 0.47315 | val_1_rmse: 0.48052 |  0:01:29s
epoch 18 | loss: 0.22961 | val_0_rmse: 0.46267 | val_1_rmse: 0.47371 |  0:01:34s
epoch 19 | loss: 0.23338 | val_0_rmse: 0.46917 | val_1_rmse: 0.47787 |  0:01:39s
epoch 20 | loss: 0.22912 | val_0_rmse: 0.45073 | val_1_rmse: 0.46066 |  0:01:44s
epoch 21 | loss: 0.22799 | val_0_rmse: 0.45026 | val_1_rmse: 0.46149 |  0:01:49s
epoch 22 | loss: 0.23181 | val_0_rmse: 0.47158 | val_1_rmse: 0.48168 |  0:01:54s
epoch 23 | loss: 0.22685 | val_0_rmse: 0.4521  | val_1_rmse: 0.4603  |  0:01:59s
epoch 24 | loss: 0.21993 | val_0_rmse: 0.45615 | val_1_rmse: 0.46629 |  0:02:04s
epoch 25 | loss: 0.2312  | val_0_rmse: 0.43804 | val_1_rmse: 0.44977 |  0:02:09s
epoch 26 | loss: 0.21858 | val_0_rmse: 0.44105 | val_1_rmse: 0.45187 |  0:02:14s
epoch 27 | loss: 0.21947 | val_0_rmse: 0.44541 | val_1_rmse: 0.45106 |  0:02:19s
epoch 28 | loss: 0.21412 | val_0_rmse: 0.43846 | val_1_rmse: 0.4497  |  0:02:24s
epoch 29 | loss: 0.21666 | val_0_rmse: 0.45733 | val_1_rmse: 0.4653  |  0:02:29s
epoch 30 | loss: 0.23017 | val_0_rmse: 0.44412 | val_1_rmse: 0.45225 |  0:02:34s
epoch 31 | loss: 0.21812 | val_0_rmse: 0.45207 | val_1_rmse: 0.46099 |  0:02:39s
epoch 32 | loss: 0.21329 | val_0_rmse: 0.43557 | val_1_rmse: 0.44539 |  0:02:45s
epoch 33 | loss: 0.21257 | val_0_rmse: 0.45615 | val_1_rmse: 0.46662 |  0:02:50s
epoch 34 | loss: 0.21214 | val_0_rmse: 0.44347 | val_1_rmse: 0.45495 |  0:02:55s
epoch 35 | loss: 0.21215 | val_0_rmse: 0.44193 | val_1_rmse: 0.45411 |  0:03:00s
epoch 36 | loss: 0.21443 | val_0_rmse: 0.43409 | val_1_rmse: 0.44553 |  0:03:05s
epoch 37 | loss: 0.20814 | val_0_rmse: 0.44476 | val_1_rmse: 0.45462 |  0:03:10s
epoch 38 | loss: 0.2062  | val_0_rmse: 0.44637 | val_1_rmse: 0.45462 |  0:03:15s
epoch 39 | loss: 0.20927 | val_0_rmse: 0.45033 | val_1_rmse: 0.46274 |  0:03:20s
epoch 40 | loss: 0.20554 | val_0_rmse: 0.42054 | val_1_rmse: 0.43006 |  0:03:25s
epoch 41 | loss: 0.20448 | val_0_rmse: 0.43846 | val_1_rmse: 0.44945 |  0:03:31s
epoch 42 | loss: 0.20939 | val_0_rmse: 0.43081 | val_1_rmse: 0.43984 |  0:03:36s
epoch 43 | loss: 0.20203 | val_0_rmse: 0.41235 | val_1_rmse: 0.42409 |  0:03:41s
epoch 44 | loss: 0.19878 | val_0_rmse: 0.42536 | val_1_rmse: 0.43814 |  0:03:46s
epoch 45 | loss: 0.19598 | val_0_rmse: 0.41493 | val_1_rmse: 0.42616 |  0:03:51s
epoch 46 | loss: 0.19647 | val_0_rmse: 0.41895 | val_1_rmse: 0.4319  |  0:03:56s
epoch 47 | loss: 0.19657 | val_0_rmse: 0.41909 | val_1_rmse: 0.4338  |  0:04:01s
epoch 48 | loss: 0.19348 | val_0_rmse: 0.41036 | val_1_rmse: 0.4241  |  0:04:06s
epoch 49 | loss: 0.19171 | val_0_rmse: 0.41996 | val_1_rmse: 0.43164 |  0:04:11s
epoch 50 | loss: 0.19574 | val_0_rmse: 0.42413 | val_1_rmse: 0.43786 |  0:04:16s
epoch 51 | loss: 0.19357 | val_0_rmse: 0.40441 | val_1_rmse: 0.41511 |  0:04:21s
epoch 52 | loss: 0.18825 | val_0_rmse: 0.40199 | val_1_rmse: 0.41472 |  0:04:26s
epoch 53 | loss: 0.18816 | val_0_rmse: 0.40675 | val_1_rmse: 0.41801 |  0:04:32s
epoch 54 | loss: 0.18698 | val_0_rmse: 0.40572 | val_1_rmse: 0.41981 |  0:04:37s
epoch 55 | loss: 0.19622 | val_0_rmse: 0.41077 | val_1_rmse: 0.42396 |  0:04:42s
epoch 56 | loss: 0.19468 | val_0_rmse: 0.40828 | val_1_rmse: 0.42265 |  0:04:47s
epoch 57 | loss: 0.19054 | val_0_rmse: 0.396   | val_1_rmse: 0.40999 |  0:04:52s
epoch 58 | loss: 0.18861 | val_0_rmse: 0.41239 | val_1_rmse: 0.42456 |  0:04:57s
epoch 59 | loss: 0.18821 | val_0_rmse: 0.42735 | val_1_rmse: 0.44067 |  0:05:03s
epoch 60 | loss: 0.18655 | val_0_rmse: 0.4041  | val_1_rmse: 0.41924 |  0:05:08s
epoch 61 | loss: 0.18425 | val_0_rmse: 0.39918 | val_1_rmse: 0.41226 |  0:05:13s
epoch 62 | loss: 0.18436 | val_0_rmse: 0.38476 | val_1_rmse: 0.39759 |  0:05:18s
epoch 63 | loss: 0.18442 | val_0_rmse: 0.3966  | val_1_rmse: 0.40981 |  0:05:23s
epoch 64 | loss: 0.18412 | val_0_rmse: 0.40811 | val_1_rmse: 0.42206 |  0:05:28s
epoch 65 | loss: 0.18416 | val_0_rmse: 0.40157 | val_1_rmse: 0.41328 |  0:05:33s
epoch 66 | loss: 0.18998 | val_0_rmse: 0.40049 | val_1_rmse: 0.41264 |  0:05:38s
epoch 67 | loss: 0.18494 | val_0_rmse: 0.38999 | val_1_rmse: 0.40409 |  0:05:43s
epoch 68 | loss: 0.18208 | val_0_rmse: 0.40083 | val_1_rmse: 0.41258 |  0:05:49s
epoch 69 | loss: 0.18179 | val_0_rmse: 0.38796 | val_1_rmse: 0.40031 |  0:05:54s
epoch 70 | loss: 0.17869 | val_0_rmse: 0.40127 | val_1_rmse: 0.41363 |  0:05:59s
epoch 71 | loss: 0.17782 | val_0_rmse: 0.39457 | val_1_rmse: 0.40799 |  0:06:04s
epoch 72 | loss: 0.18068 | val_0_rmse: 0.40203 | val_1_rmse: 0.41557 |  0:06:09s
epoch 73 | loss: 0.18001 | val_0_rmse: 0.38752 | val_1_rmse: 0.40011 |  0:06:14s
epoch 74 | loss: 0.18025 | val_0_rmse: 0.39032 | val_1_rmse: 0.40349 |  0:06:19s
epoch 75 | loss: 0.18333 | val_0_rmse: 0.40029 | val_1_rmse: 0.41465 |  0:06:24s
epoch 76 | loss: 0.1835  | val_0_rmse: 0.40808 | val_1_rmse: 0.42178 |  0:06:29s
epoch 77 | loss: 0.17999 | val_0_rmse: 0.38871 | val_1_rmse: 0.40189 |  0:06:34s
epoch 78 | loss: 0.18279 | val_0_rmse: 0.38941 | val_1_rmse: 0.40451 |  0:06:39s
epoch 79 | loss: 0.17685 | val_0_rmse: 0.38052 | val_1_rmse: 0.39246 |  0:06:45s
epoch 80 | loss: 0.17832 | val_0_rmse: 0.38495 | val_1_rmse: 0.39686 |  0:06:50s
epoch 81 | loss: 0.17728 | val_0_rmse: 0.40452 | val_1_rmse: 0.41654 |  0:06:55s
epoch 82 | loss: 0.19189 | val_0_rmse: 0.39942 | val_1_rmse: 0.41273 |  0:07:00s
epoch 83 | loss: 0.17629 | val_0_rmse: 0.37204 | val_1_rmse: 0.38507 |  0:07:05s
epoch 84 | loss: 0.18157 | val_0_rmse: 0.39373 | val_1_rmse: 0.4031  |  0:07:10s
epoch 85 | loss: 0.17928 | val_0_rmse: 0.39204 | val_1_rmse: 0.40293 |  0:07:16s
epoch 86 | loss: 0.1794  | val_0_rmse: 0.41027 | val_1_rmse: 0.42367 |  0:07:21s
epoch 87 | loss: 0.17492 | val_0_rmse: 0.381   | val_1_rmse: 0.39178 |  0:07:26s
epoch 88 | loss: 0.17812 | val_0_rmse: 0.38898 | val_1_rmse: 0.40318 |  0:07:31s
epoch 89 | loss: 0.17291 | val_0_rmse: 0.40316 | val_1_rmse: 0.41525 |  0:07:36s
epoch 90 | loss: 0.19153 | val_0_rmse: 0.40024 | val_1_rmse: 0.41309 |  0:07:41s
epoch 91 | loss: 0.17599 | val_0_rmse: 0.37955 | val_1_rmse: 0.39604 |  0:07:46s
epoch 92 | loss: 0.1724  | val_0_rmse: 0.36854 | val_1_rmse: 0.38572 |  0:07:51s
epoch 93 | loss: 0.17228 | val_0_rmse: 0.38154 | val_1_rmse: 0.39705 |  0:07:56s
epoch 94 | loss: 0.17289 | val_0_rmse: 0.37197 | val_1_rmse: 0.38601 |  0:08:01s
epoch 95 | loss: 0.17152 | val_0_rmse: 0.37186 | val_1_rmse: 0.38512 |  0:08:06s
epoch 96 | loss: 0.17268 | val_0_rmse: 0.37072 | val_1_rmse: 0.38548 |  0:08:11s
epoch 97 | loss: 0.17049 | val_0_rmse: 0.37678 | val_1_rmse: 0.38922 |  0:08:16s
epoch 98 | loss: 0.17738 | val_0_rmse: 0.39444 | val_1_rmse: 0.40949 |  0:08:21s
epoch 99 | loss: 0.17843 | val_0_rmse: 0.38957 | val_1_rmse: 0.40418 |  0:08:26s
epoch 100| loss: 0.17355 | val_0_rmse: 0.36933 | val_1_rmse: 0.38418 |  0:08:32s
epoch 101| loss: 0.17034 | val_0_rmse: 0.3681  | val_1_rmse: 0.38555 |  0:08:37s
epoch 102| loss: 0.16567 | val_0_rmse: 0.37321 | val_1_rmse: 0.38857 |  0:08:42s
epoch 103| loss: 0.17231 | val_0_rmse: 0.36684 | val_1_rmse: 0.38071 |  0:08:47s
epoch 104| loss: 0.17585 | val_0_rmse: 0.38012 | val_1_rmse: 0.39518 |  0:08:52s
epoch 105| loss: 0.16821 | val_0_rmse: 0.37139 | val_1_rmse: 0.38516 |  0:08:57s
epoch 106| loss: 0.1695  | val_0_rmse: 0.37492 | val_1_rmse: 0.39077 |  0:09:02s
epoch 107| loss: 0.17062 | val_0_rmse: 0.36488 | val_1_rmse: 0.37817 |  0:09:07s
epoch 108| loss: 0.16887 | val_0_rmse: 0.36604 | val_1_rmse: 0.38079 |  0:09:12s
epoch 109| loss: 0.16512 | val_0_rmse: 0.37215 | val_1_rmse: 0.38734 |  0:09:18s
epoch 110| loss: 0.17825 | val_0_rmse: 0.35943 | val_1_rmse: 0.37416 |  0:09:23s
epoch 111| loss: 0.17108 | val_0_rmse: 0.37978 | val_1_rmse: 0.39335 |  0:09:28s
epoch 112| loss: 0.17207 | val_0_rmse: 0.37276 | val_1_rmse: 0.38603 |  0:09:33s
epoch 113| loss: 0.17088 | val_0_rmse: 0.37418 | val_1_rmse: 0.39038 |  0:09:38s
epoch 114| loss: 0.17369 | val_0_rmse: 0.37204 | val_1_rmse: 0.38658 |  0:09:43s
epoch 115| loss: 0.17391 | val_0_rmse: 0.37135 | val_1_rmse: 0.38876 |  0:09:48s
epoch 116| loss: 0.16844 | val_0_rmse: 0.36467 | val_1_rmse: 0.37988 |  0:09:53s
epoch 117| loss: 0.16364 | val_0_rmse: 0.35791 | val_1_rmse: 0.37295 |  0:09:58s
epoch 118| loss: 0.16638 | val_0_rmse: 0.35775 | val_1_rmse: 0.37125 |  0:10:03s
epoch 119| loss: 0.16575 | val_0_rmse: 0.37546 | val_1_rmse: 0.38804 |  0:10:09s
epoch 120| loss: 0.16632 | val_0_rmse: 0.36149 | val_1_rmse: 0.37787 |  0:10:14s
epoch 121| loss: 0.16794 | val_0_rmse: 0.37362 | val_1_rmse: 0.38904 |  0:10:19s
epoch 122| loss: 0.1735  | val_0_rmse: 0.38668 | val_1_rmse: 0.4046  |  0:10:24s
epoch 123| loss: 0.18572 | val_0_rmse: 0.37575 | val_1_rmse: 0.39058 |  0:10:29s
epoch 124| loss: 0.16657 | val_0_rmse: 0.37264 | val_1_rmse: 0.38579 |  0:10:34s
epoch 125| loss: 0.18047 | val_0_rmse: 0.39201 | val_1_rmse: 0.40498 |  0:10:40s
epoch 126| loss: 0.17835 | val_0_rmse: 0.38218 | val_1_rmse: 0.39471 |  0:10:45s
epoch 127| loss: 0.17641 | val_0_rmse: 0.37447 | val_1_rmse: 0.39065 |  0:10:50s
epoch 128| loss: 0.18112 | val_0_rmse: 0.38424 | val_1_rmse: 0.39752 |  0:10:55s
epoch 129| loss: 0.17876 | val_0_rmse: 0.38716 | val_1_rmse: 0.3991  |  0:11:00s
epoch 130| loss: 0.17756 | val_0_rmse: 0.36125 | val_1_rmse: 0.37478 |  0:11:05s
epoch 131| loss: 0.1672  | val_0_rmse: 0.3735  | val_1_rmse: 0.38623 |  0:11:10s
epoch 132| loss: 0.17248 | val_0_rmse: 0.38365 | val_1_rmse: 0.40055 |  0:11:15s
epoch 133| loss: 0.16872 | val_0_rmse: 0.36977 | val_1_rmse: 0.38278 |  0:11:20s
epoch 134| loss: 0.17669 | val_0_rmse: 0.37024 | val_1_rmse: 0.3838  |  0:11:25s
epoch 135| loss: 0.17302 | val_0_rmse: 0.371   | val_1_rmse: 0.38349 |  0:11:30s
epoch 136| loss: 0.16681 | val_0_rmse: 0.38446 | val_1_rmse: 0.39744 |  0:11:36s
epoch 137| loss: 0.17067 | val_0_rmse: 0.3772  | val_1_rmse: 0.38978 |  0:11:41s
epoch 138| loss: 0.17151 | val_0_rmse: 0.38874 | val_1_rmse: 0.40157 |  0:11:46s
epoch 139| loss: 0.16852 | val_0_rmse: 0.36995 | val_1_rmse: 0.38328 |  0:11:51s
epoch 140| loss: 0.167   | val_0_rmse: 0.3635  | val_1_rmse: 0.37836 |  0:11:56s
epoch 141| loss: 0.16922 | val_0_rmse: 0.36289 | val_1_rmse: 0.37698 |  0:12:01s
epoch 142| loss: 0.16386 | val_0_rmse: 0.35547 | val_1_rmse: 0.36643 |  0:12:06s
epoch 143| loss: 0.16588 | val_0_rmse: 0.35641 | val_1_rmse: 0.36942 |  0:12:12s
epoch 144| loss: 0.16222 | val_0_rmse: 0.35366 | val_1_rmse: 0.37191 |  0:12:17s
epoch 145| loss: 0.16968 | val_0_rmse: 0.36739 | val_1_rmse: 0.37901 |  0:12:22s
epoch 146| loss: 0.18875 | val_0_rmse: 0.41082 | val_1_rmse: 0.42116 |  0:12:27s
epoch 147| loss: 0.18269 | val_0_rmse: 0.37617 | val_1_rmse: 0.3905  |  0:12:32s
epoch 148| loss: 0.18803 | val_0_rmse: 0.39944 | val_1_rmse: 0.40897 |  0:12:37s
epoch 149| loss: 0.21561 | val_0_rmse: 0.45986 | val_1_rmse: 0.46732 |  0:12:42s
Stop training because you reached max_epochs = 150 with best_epoch = 142 and best_val_1_rmse = 0.36643
Best weights from best epoch are automatically used!
ended training at: 23:56:30
Feature importance:
[('Area', 0.3039749750149795), ('Baths', 0.03315099602524234), ('Beds', 0.18002702288105), ('Latitude', 0.22049255134859286), ('Longitude', 0.16274343363928426), ('Month', 0.017661065102643394), ('Year', 0.08194995598820766)]
Mean squared error is of 11313601204.188105
Mean absolute error:76850.3861370921
MAPE:0.13403658091402432
R2 score:0.860384244793431
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:56:31
epoch 0  | loss: 0.41719 | val_0_rmse: 0.53393 | val_1_rmse: 0.53575 |  0:00:05s
epoch 1  | loss: 0.29384 | val_0_rmse: 0.53489 | val_1_rmse: 0.53905 |  0:00:10s
epoch 2  | loss: 0.28192 | val_0_rmse: 0.51198 | val_1_rmse: 0.51525 |  0:00:15s
epoch 3  | loss: 0.27568 | val_0_rmse: 0.50457 | val_1_rmse: 0.50739 |  0:00:20s
epoch 4  | loss: 0.26993 | val_0_rmse: 0.49683 | val_1_rmse: 0.49953 |  0:00:25s
epoch 5  | loss: 0.26821 | val_0_rmse: 0.48898 | val_1_rmse: 0.49104 |  0:00:31s
epoch 6  | loss: 0.26106 | val_0_rmse: 0.49286 | val_1_rmse: 0.49381 |  0:00:36s
epoch 7  | loss: 0.25394 | val_0_rmse: 0.4894  | val_1_rmse: 0.49295 |  0:00:41s
epoch 8  | loss: 0.24828 | val_0_rmse: 0.48341 | val_1_rmse: 0.48656 |  0:00:46s
epoch 9  | loss: 0.25078 | val_0_rmse: 0.50554 | val_1_rmse: 0.50783 |  0:00:51s
epoch 10 | loss: 0.24933 | val_0_rmse: 0.47586 | val_1_rmse: 0.47924 |  0:00:56s
epoch 11 | loss: 0.25064 | val_0_rmse: 0.51026 | val_1_rmse: 0.51504 |  0:01:01s
epoch 12 | loss: 0.24729 | val_0_rmse: 0.48071 | val_1_rmse: 0.48558 |  0:01:06s
epoch 13 | loss: 0.24403 | val_0_rmse: 0.47629 | val_1_rmse: 0.48354 |  0:01:11s
epoch 14 | loss: 0.23611 | val_0_rmse: 0.46364 | val_1_rmse: 0.46961 |  0:01:16s
epoch 15 | loss: 0.24101 | val_0_rmse: 0.4536  | val_1_rmse: 0.45986 |  0:01:22s
epoch 16 | loss: 0.22606 | val_0_rmse: 0.46144 | val_1_rmse: 0.46674 |  0:01:27s
epoch 17 | loss: 0.22894 | val_0_rmse: 0.46354 | val_1_rmse: 0.4703  |  0:01:32s
epoch 18 | loss: 0.22812 | val_0_rmse: 0.45016 | val_1_rmse: 0.4578  |  0:01:37s
epoch 19 | loss: 0.23036 | val_0_rmse: 0.45287 | val_1_rmse: 0.46106 |  0:01:42s
epoch 20 | loss: 0.22223 | val_0_rmse: 0.45427 | val_1_rmse: 0.46202 |  0:01:47s
epoch 21 | loss: 0.22283 | val_0_rmse: 0.44502 | val_1_rmse: 0.45457 |  0:01:52s
epoch 22 | loss: 0.21961 | val_0_rmse: 0.45365 | val_1_rmse: 0.46174 |  0:01:57s
epoch 23 | loss: 0.22348 | val_0_rmse: 0.435   | val_1_rmse: 0.44332 |  0:02:02s
epoch 24 | loss: 0.21451 | val_0_rmse: 0.43986 | val_1_rmse: 0.45297 |  0:02:08s
epoch 25 | loss: 0.21258 | val_0_rmse: 0.43108 | val_1_rmse: 0.43784 |  0:02:13s
epoch 26 | loss: 0.2122  | val_0_rmse: 0.43256 | val_1_rmse: 0.44265 |  0:02:18s
epoch 27 | loss: 0.20812 | val_0_rmse: 0.43516 | val_1_rmse: 0.44266 |  0:02:23s
epoch 28 | loss: 0.21144 | val_0_rmse: 0.43733 | val_1_rmse: 0.44648 |  0:02:28s
epoch 29 | loss: 0.2073  | val_0_rmse: 0.42969 | val_1_rmse: 0.43613 |  0:02:33s
epoch 30 | loss: 0.20728 | val_0_rmse: 0.41638 | val_1_rmse: 0.42734 |  0:02:38s
epoch 31 | loss: 0.20672 | val_0_rmse: 0.42346 | val_1_rmse: 0.43121 |  0:02:43s
epoch 32 | loss: 0.20409 | val_0_rmse: 0.42643 | val_1_rmse: 0.43359 |  0:02:48s
epoch 33 | loss: 0.20348 | val_0_rmse: 0.44197 | val_1_rmse: 0.45046 |  0:02:54s
epoch 34 | loss: 0.20185 | val_0_rmse: 0.43208 | val_1_rmse: 0.44262 |  0:02:59s
epoch 35 | loss: 0.21178 | val_0_rmse: 0.41464 | val_1_rmse: 0.42504 |  0:03:04s
epoch 36 | loss: 0.19698 | val_0_rmse: 0.41718 | val_1_rmse: 0.42656 |  0:03:09s
epoch 37 | loss: 0.19846 | val_0_rmse: 0.42407 | val_1_rmse: 0.43427 |  0:03:14s
epoch 38 | loss: 0.20188 | val_0_rmse: 0.40391 | val_1_rmse: 0.41371 |  0:03:19s
epoch 39 | loss: 0.19322 | val_0_rmse: 0.42383 | val_1_rmse: 0.43509 |  0:03:25s
epoch 40 | loss: 0.19576 | val_0_rmse: 0.42913 | val_1_rmse: 0.43873 |  0:03:30s
epoch 41 | loss: 0.19446 | val_0_rmse: 0.41845 | val_1_rmse: 0.42652 |  0:03:35s
epoch 42 | loss: 0.19407 | val_0_rmse: 0.40887 | val_1_rmse: 0.41823 |  0:03:40s
epoch 43 | loss: 0.18786 | val_0_rmse: 0.40446 | val_1_rmse: 0.41605 |  0:03:44s
epoch 44 | loss: 0.18777 | val_0_rmse: 0.4002  | val_1_rmse: 0.40949 |  0:03:50s
epoch 45 | loss: 0.18571 | val_0_rmse: 0.39962 | val_1_rmse: 0.41023 |  0:03:55s
epoch 46 | loss: 0.18814 | val_0_rmse: 0.39707 | val_1_rmse: 0.41042 |  0:04:00s
epoch 47 | loss: 0.18789 | val_0_rmse: 0.41556 | val_1_rmse: 0.4242  |  0:04:05s
epoch 48 | loss: 0.18675 | val_0_rmse: 0.39204 | val_1_rmse: 0.40137 |  0:04:10s
epoch 49 | loss: 0.18204 | val_0_rmse: 0.39907 | val_1_rmse: 0.4112  |  0:04:15s
epoch 50 | loss: 0.18596 | val_0_rmse: 0.42059 | val_1_rmse: 0.43246 |  0:04:20s
epoch 51 | loss: 0.18094 | val_0_rmse: 0.43722 | val_1_rmse: 0.44581 |  0:04:26s
epoch 52 | loss: 0.18671 | val_0_rmse: 0.40799 | val_1_rmse: 0.41976 |  0:04:31s
epoch 53 | loss: 0.17852 | val_0_rmse: 0.38314 | val_1_rmse: 0.39373 |  0:04:36s
epoch 54 | loss: 0.18069 | val_0_rmse: 0.3808  | val_1_rmse: 0.39381 |  0:04:41s
epoch 55 | loss: 0.18    | val_0_rmse: 0.38851 | val_1_rmse: 0.40071 |  0:04:46s
epoch 56 | loss: 0.1782  | val_0_rmse: 0.38826 | val_1_rmse: 0.40124 |  0:04:52s
epoch 57 | loss: 0.17682 | val_0_rmse: 0.40414 | val_1_rmse: 0.41599 |  0:04:57s
epoch 58 | loss: 0.18127 | val_0_rmse: 0.37927 | val_1_rmse: 0.39274 |  0:05:02s
epoch 59 | loss: 0.179   | val_0_rmse: 0.37724 | val_1_rmse: 0.38783 |  0:05:07s
epoch 60 | loss: 0.17681 | val_0_rmse: 0.40386 | val_1_rmse: 0.41736 |  0:05:12s
epoch 61 | loss: 0.18245 | val_0_rmse: 0.39826 | val_1_rmse: 0.41124 |  0:05:17s
epoch 62 | loss: 0.1779  | val_0_rmse: 0.38573 | val_1_rmse: 0.39842 |  0:05:22s
epoch 63 | loss: 0.17257 | val_0_rmse: 0.38292 | val_1_rmse: 0.3959  |  0:05:28s
epoch 64 | loss: 0.18041 | val_0_rmse: 0.3874  | val_1_rmse: 0.39891 |  0:05:33s
epoch 65 | loss: 0.17446 | val_0_rmse: 0.37537 | val_1_rmse: 0.38808 |  0:05:38s
epoch 66 | loss: 0.16844 | val_0_rmse: 0.37371 | val_1_rmse: 0.38603 |  0:05:43s
epoch 67 | loss: 0.16979 | val_0_rmse: 0.39254 | val_1_rmse: 0.40429 |  0:05:48s
epoch 68 | loss: 0.18316 | val_0_rmse: 0.38854 | val_1_rmse: 0.40057 |  0:05:53s
epoch 69 | loss: 0.17655 | val_0_rmse: 0.37974 | val_1_rmse: 0.39312 |  0:05:59s
epoch 70 | loss: 0.23309 | val_0_rmse: 0.54311 | val_1_rmse: 0.55297 |  0:06:04s
epoch 71 | loss: 0.25138 | val_0_rmse: 0.46629 | val_1_rmse: 0.47226 |  0:06:09s
epoch 72 | loss: 0.21896 | val_0_rmse: 0.44796 | val_1_rmse: 0.45799 |  0:06:14s
epoch 73 | loss: 0.20748 | val_0_rmse: 0.41463 | val_1_rmse: 0.4263  |  0:06:19s
epoch 74 | loss: 0.19997 | val_0_rmse: 0.42145 | val_1_rmse: 0.43196 |  0:06:24s
epoch 75 | loss: 0.19258 | val_0_rmse: 0.40602 | val_1_rmse: 0.41596 |  0:06:29s
epoch 76 | loss: 0.19148 | val_0_rmse: 0.40233 | val_1_rmse: 0.41376 |  0:06:34s
epoch 77 | loss: 0.18626 | val_0_rmse: 0.39385 | val_1_rmse: 0.40531 |  0:06:39s
epoch 78 | loss: 0.18155 | val_0_rmse: 0.38493 | val_1_rmse: 0.39709 |  0:06:44s
epoch 79 | loss: 0.181   | val_0_rmse: 0.38449 | val_1_rmse: 0.39501 |  0:06:49s
epoch 80 | loss: 0.17961 | val_0_rmse: 0.38337 | val_1_rmse: 0.39272 |  0:06:54s
epoch 81 | loss: 0.17552 | val_0_rmse: 0.38829 | val_1_rmse: 0.39999 |  0:07:00s
epoch 82 | loss: 0.17633 | val_0_rmse: 0.38128 | val_1_rmse: 0.39339 |  0:07:05s
epoch 83 | loss: 0.17363 | val_0_rmse: 0.38112 | val_1_rmse: 0.39337 |  0:07:10s
epoch 84 | loss: 0.17337 | val_0_rmse: 0.37749 | val_1_rmse: 0.39065 |  0:07:15s
epoch 85 | loss: 0.17367 | val_0_rmse: 0.39265 | val_1_rmse: 0.40735 |  0:07:21s
epoch 86 | loss: 0.17417 | val_0_rmse: 0.38063 | val_1_rmse: 0.39216 |  0:07:26s
epoch 87 | loss: 0.17296 | val_0_rmse: 0.38454 | val_1_rmse: 0.39732 |  0:07:31s
epoch 88 | loss: 0.17322 | val_0_rmse: 0.37474 | val_1_rmse: 0.38731 |  0:07:36s
epoch 89 | loss: 0.16817 | val_0_rmse: 0.38261 | val_1_rmse: 0.39808 |  0:07:41s
epoch 90 | loss: 0.16816 | val_0_rmse: 0.37083 | val_1_rmse: 0.38411 |  0:07:46s
epoch 91 | loss: 0.17039 | val_0_rmse: 0.36305 | val_1_rmse: 0.37454 |  0:07:51s
epoch 92 | loss: 0.16769 | val_0_rmse: 0.36403 | val_1_rmse: 0.37995 |  0:07:56s
epoch 93 | loss: 0.16643 | val_0_rmse: 0.37656 | val_1_rmse: 0.39009 |  0:08:01s
epoch 94 | loss: 0.16929 | val_0_rmse: 0.36819 | val_1_rmse: 0.38173 |  0:08:06s
epoch 95 | loss: 0.17591 | val_0_rmse: 0.38651 | val_1_rmse: 0.39699 |  0:08:11s
epoch 96 | loss: 0.17335 | val_0_rmse: 0.36661 | val_1_rmse: 0.37991 |  0:08:17s
epoch 97 | loss: 0.16698 | val_0_rmse: 0.37371 | val_1_rmse: 0.38423 |  0:08:22s
epoch 98 | loss: 0.16729 | val_0_rmse: 0.36586 | val_1_rmse: 0.37843 |  0:08:27s
epoch 99 | loss: 0.1648  | val_0_rmse: 0.3634  | val_1_rmse: 0.37708 |  0:08:32s
epoch 100| loss: 0.16635 | val_0_rmse: 0.37252 | val_1_rmse: 0.38643 |  0:08:37s
epoch 101| loss: 0.17394 | val_0_rmse: 0.37319 | val_1_rmse: 0.38533 |  0:08:43s
epoch 102| loss: 0.16924 | val_0_rmse: 0.35399 | val_1_rmse: 0.36982 |  0:08:48s
epoch 103| loss: 0.1626  | val_0_rmse: 0.36477 | val_1_rmse: 0.37904 |  0:08:53s
epoch 104| loss: 0.1626  | val_0_rmse: 0.36719 | val_1_rmse: 0.37892 |  0:08:58s
epoch 105| loss: 0.17148 | val_0_rmse: 0.3889  | val_1_rmse: 0.398   |  0:09:03s
epoch 106| loss: 0.17871 | val_0_rmse: 0.37639 | val_1_rmse: 0.39168 |  0:09:08s
epoch 107| loss: 0.1657  | val_0_rmse: 0.37166 | val_1_rmse: 0.38582 |  0:09:13s
epoch 108| loss: 0.16789 | val_0_rmse: 0.36062 | val_1_rmse: 0.37175 |  0:09:18s
epoch 109| loss: 0.16334 | val_0_rmse: 0.35715 | val_1_rmse: 0.3701  |  0:09:24s
epoch 110| loss: 0.17176 | val_0_rmse: 0.38103 | val_1_rmse: 0.39424 |  0:09:29s
epoch 111| loss: 0.17303 | val_0_rmse: 0.37903 | val_1_rmse: 0.39309 |  0:09:34s
epoch 112| loss: 0.16459 | val_0_rmse: 0.37581 | val_1_rmse: 0.38923 |  0:09:39s
epoch 113| loss: 0.16594 | val_0_rmse: 0.3573  | val_1_rmse: 0.36932 |  0:09:44s
epoch 114| loss: 0.15752 | val_0_rmse: 0.36435 | val_1_rmse: 0.37849 |  0:09:49s
epoch 115| loss: 0.16158 | val_0_rmse: 0.35642 | val_1_rmse: 0.36875 |  0:09:54s
epoch 116| loss: 0.17387 | val_0_rmse: 0.43911 | val_1_rmse: 0.44521 |  0:09:59s
epoch 117| loss: 0.19017 | val_0_rmse: 0.37498 | val_1_rmse: 0.38806 |  0:10:05s
epoch 118| loss: 0.16957 | val_0_rmse: 0.35983 | val_1_rmse: 0.37679 |  0:10:10s
epoch 119| loss: 0.16654 | val_0_rmse: 0.37077 | val_1_rmse: 0.38357 |  0:10:15s
epoch 120| loss: 0.1655  | val_0_rmse: 0.37951 | val_1_rmse: 0.39281 |  0:10:20s
epoch 121| loss: 0.15999 | val_0_rmse: 0.35369 | val_1_rmse: 0.37095 |  0:10:25s
epoch 122| loss: 0.15946 | val_0_rmse: 0.34874 | val_1_rmse: 0.36433 |  0:10:30s
epoch 123| loss: 0.16046 | val_0_rmse: 0.36686 | val_1_rmse: 0.37852 |  0:10:35s
epoch 124| loss: 0.1607  | val_0_rmse: 0.35384 | val_1_rmse: 0.37018 |  0:10:41s
epoch 125| loss: 0.1606  | val_0_rmse: 0.35618 | val_1_rmse: 0.37214 |  0:10:46s
epoch 126| loss: 0.16061 | val_0_rmse: 0.3438  | val_1_rmse: 0.36034 |  0:10:51s
epoch 127| loss: 0.15548 | val_0_rmse: 0.34669 | val_1_rmse: 0.36278 |  0:10:56s
epoch 128| loss: 0.15554 | val_0_rmse: 0.35291 | val_1_rmse: 0.36962 |  0:11:01s
epoch 129| loss: 0.15934 | val_0_rmse: 0.34468 | val_1_rmse: 0.35931 |  0:11:07s
epoch 130| loss: 0.15604 | val_0_rmse: 0.37809 | val_1_rmse: 0.39153 |  0:11:12s
epoch 131| loss: 0.16041 | val_0_rmse: 0.35595 | val_1_rmse: 0.37258 |  0:11:17s
epoch 132| loss: 0.16161 | val_0_rmse: 0.39457 | val_1_rmse: 0.40627 |  0:11:22s
epoch 133| loss: 0.15967 | val_0_rmse: 0.36308 | val_1_rmse: 0.37934 |  0:11:27s
epoch 134| loss: 0.15907 | val_0_rmse: 0.35988 | val_1_rmse: 0.37629 |  0:11:32s
epoch 135| loss: 0.15724 | val_0_rmse: 0.36136 | val_1_rmse: 0.37626 |  0:11:37s
epoch 136| loss: 0.15713 | val_0_rmse: 0.34819 | val_1_rmse: 0.36265 |  0:11:42s
epoch 137| loss: 0.1522  | val_0_rmse: 0.3596  | val_1_rmse: 0.37323 |  0:11:47s
epoch 138| loss: 0.15261 | val_0_rmse: 0.34789 | val_1_rmse: 0.36536 |  0:11:52s
epoch 139| loss: 0.15286 | val_0_rmse: 0.34187 | val_1_rmse: 0.3582  |  0:11:57s
epoch 140| loss: 0.15214 | val_0_rmse: 0.37028 | val_1_rmse: 0.38704 |  0:12:02s
epoch 141| loss: 0.15263 | val_0_rmse: 0.3438  | val_1_rmse: 0.36048 |  0:12:07s
epoch 142| loss: 0.1489  | val_0_rmse: 0.34956 | val_1_rmse: 0.36595 |  0:12:12s
epoch 143| loss: 0.15659 | val_0_rmse: 0.3387  | val_1_rmse: 0.3547  |  0:12:18s
epoch 144| loss: 0.14966 | val_0_rmse: 0.3357  | val_1_rmse: 0.35047 |  0:12:23s
epoch 145| loss: 0.15455 | val_0_rmse: 0.35011 | val_1_rmse: 0.36276 |  0:12:28s
epoch 146| loss: 0.15464 | val_0_rmse: 0.33277 | val_1_rmse: 0.3502  |  0:12:33s
epoch 147| loss: 0.15361 | val_0_rmse: 0.34677 | val_1_rmse: 0.36286 |  0:12:38s
epoch 148| loss: 0.15311 | val_0_rmse: 0.35104 | val_1_rmse: 0.3657  |  0:12:43s
epoch 149| loss: 0.15084 | val_0_rmse: 0.35051 | val_1_rmse: 0.36647 |  0:12:48s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.3502
Best weights from best epoch are automatically used!
ended training at: 00:09:21
Feature importance:
[('Area', 0.3966978971618682), ('Baths', 0.0), ('Beds', 0.0350685590219173), ('Latitude', 0.22958660434766084), ('Longitude', 0.22419955874671144), ('Month', 0.08375030913821106), ('Year', 0.030697071583631125)]
Mean squared error is of 9773357340.683533
Mean absolute error:71380.93069321127
MAPE:0.12933205790171282
R2 score:0.8788413440491111
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:17:39
epoch 0  | loss: 0.55441 | val_0_rmse: 0.67099 | val_1_rmse: 0.67112 |  0:00:04s
epoch 1  | loss: 0.45274 | val_0_rmse: 0.6643  | val_1_rmse: 0.66637 |  0:00:09s
epoch 2  | loss: 0.42385 | val_0_rmse: 0.94793 | val_1_rmse: 0.95607 |  0:00:14s
epoch 3  | loss: 0.39956 | val_0_rmse: 0.67281 | val_1_rmse: 0.6741  |  0:00:19s
epoch 4  | loss: 0.37916 | val_0_rmse: 0.69154 | val_1_rmse: 0.69652 |  0:00:24s
epoch 5  | loss: 0.34008 | val_0_rmse: 0.58045 | val_1_rmse: 0.58076 |  0:00:29s
epoch 6  | loss: 0.32449 | val_0_rmse: 0.53886 | val_1_rmse: 0.53849 |  0:00:34s
epoch 7  | loss: 0.3036  | val_0_rmse: 0.82579 | val_1_rmse: 0.83638 |  0:00:38s
epoch 8  | loss: 0.30035 | val_0_rmse: 0.5267  | val_1_rmse: 0.52616 |  0:00:43s
epoch 9  | loss: 0.30064 | val_0_rmse: 0.58347 | val_1_rmse: 0.58439 |  0:00:48s
epoch 10 | loss: 0.28843 | val_0_rmse: 0.63875 | val_1_rmse: 0.64439 |  0:00:53s
epoch 11 | loss: 0.28367 | val_0_rmse: 0.52531 | val_1_rmse: 0.52111 |  0:00:59s
epoch 12 | loss: 0.28581 | val_0_rmse: 0.61096 | val_1_rmse: 0.61597 |  0:01:04s
epoch 13 | loss: 0.27415 | val_0_rmse: 0.51535 | val_1_rmse: 0.51187 |  0:01:09s
epoch 14 | loss: 0.2696  | val_0_rmse: 0.59286 | val_1_rmse: 0.59485 |  0:01:14s
epoch 15 | loss: 0.2665  | val_0_rmse: 0.55814 | val_1_rmse: 0.56154 |  0:01:20s
epoch 16 | loss: 0.26909 | val_0_rmse: 0.52067 | val_1_rmse: 0.51894 |  0:01:25s
epoch 17 | loss: 0.26266 | val_0_rmse: 0.50807 | val_1_rmse: 0.5053  |  0:01:31s
epoch 18 | loss: 0.26492 | val_0_rmse: 0.53324 | val_1_rmse: 0.53477 |  0:01:37s
epoch 19 | loss: 0.26149 | val_0_rmse: 0.69408 | val_1_rmse: 0.70294 |  0:01:44s
epoch 20 | loss: 0.27339 | val_0_rmse: 0.55352 | val_1_rmse: 0.55536 |  0:01:49s
epoch 21 | loss: 0.26779 | val_0_rmse: 0.71806 | val_1_rmse: 0.71756 |  0:01:54s
epoch 22 | loss: 0.25778 | val_0_rmse: 0.60754 | val_1_rmse: 0.61385 |  0:02:00s
epoch 23 | loss: 0.25662 | val_0_rmse: 0.67914 | val_1_rmse: 0.67726 |  0:02:05s
epoch 24 | loss: 0.25203 | val_0_rmse: 0.53267 | val_1_rmse: 0.5294  |  0:02:11s
epoch 25 | loss: 0.24391 | val_0_rmse: 0.63671 | val_1_rmse: 0.64527 |  0:02:16s
epoch 26 | loss: 0.24127 | val_0_rmse: 0.70421 | val_1_rmse: 0.71322 |  0:02:21s
epoch 27 | loss: 0.23816 | val_0_rmse: 0.49934 | val_1_rmse: 0.50357 |  0:02:26s
epoch 28 | loss: 0.24296 | val_0_rmse: 0.52193 | val_1_rmse: 0.52101 |  0:02:31s
epoch 29 | loss: 0.24965 | val_0_rmse: 0.55517 | val_1_rmse: 0.55246 |  0:02:36s
epoch 30 | loss: 0.24944 | val_0_rmse: 0.47878 | val_1_rmse: 0.47972 |  0:02:41s
epoch 31 | loss: 0.24034 | val_0_rmse: 0.66841 | val_1_rmse: 0.66319 |  0:02:47s
epoch 32 | loss: 0.23461 | val_0_rmse: 0.69109 | val_1_rmse: 0.68944 |  0:02:52s
epoch 33 | loss: 0.23427 | val_0_rmse: 0.47906 | val_1_rmse: 0.48036 |  0:02:57s
epoch 34 | loss: 0.23167 | val_0_rmse: 0.76402 | val_1_rmse: 0.77557 |  0:03:02s
epoch 35 | loss: 0.24337 | val_0_rmse: 0.49573 | val_1_rmse: 0.49671 |  0:03:07s
epoch 36 | loss: 0.32465 | val_0_rmse: 0.60112 | val_1_rmse: 0.60044 |  0:03:12s
epoch 37 | loss: 0.29645 | val_0_rmse: 0.72077 | val_1_rmse: 0.73247 |  0:03:17s
epoch 38 | loss: 0.26391 | val_0_rmse: 0.56767 | val_1_rmse: 0.56834 |  0:03:22s
epoch 39 | loss: 0.25713 | val_0_rmse: 0.60271 | val_1_rmse: 0.60241 |  0:03:27s
epoch 40 | loss: 0.24231 | val_0_rmse: 0.65176 | val_1_rmse: 0.64925 |  0:03:33s
epoch 41 | loss: 0.24687 | val_0_rmse: 0.59378 | val_1_rmse: 0.5998  |  0:03:38s
epoch 42 | loss: 0.24258 | val_0_rmse: 0.66878 | val_1_rmse: 0.67626 |  0:03:43s
epoch 43 | loss: 0.2306  | val_0_rmse: 0.60249 | val_1_rmse: 0.60161 |  0:03:48s
epoch 44 | loss: 0.2284  | val_0_rmse: 0.47311 | val_1_rmse: 0.47407 |  0:03:53s
epoch 45 | loss: 0.22973 | val_0_rmse: 0.46883 | val_1_rmse: 0.46934 |  0:03:59s
epoch 46 | loss: 0.22658 | val_0_rmse: 0.49273 | val_1_rmse: 0.4963  |  0:04:04s
epoch 47 | loss: 0.21932 | val_0_rmse: 0.49244 | val_1_rmse: 0.49527 |  0:04:09s
epoch 48 | loss: 0.22336 | val_0_rmse: 0.46627 | val_1_rmse: 0.47299 |  0:04:15s
epoch 49 | loss: 0.21795 | val_0_rmse: 0.49264 | val_1_rmse: 0.496   |  0:04:21s
epoch 50 | loss: 0.21531 | val_0_rmse: 0.43876 | val_1_rmse: 0.44244 |  0:04:26s
epoch 51 | loss: 0.20899 | val_0_rmse: 0.50898 | val_1_rmse: 0.51521 |  0:04:31s
epoch 52 | loss: 0.22154 | val_0_rmse: 0.45377 | val_1_rmse: 0.46092 |  0:04:36s
epoch 53 | loss: 0.20822 | val_0_rmse: 0.43684 | val_1_rmse: 0.44019 |  0:04:41s
epoch 54 | loss: 0.21392 | val_0_rmse: 0.58198 | val_1_rmse: 0.58742 |  0:04:46s
epoch 55 | loss: 0.20776 | val_0_rmse: 0.42154 | val_1_rmse: 0.42556 |  0:04:52s
epoch 56 | loss: 0.20817 | val_0_rmse: 0.56756 | val_1_rmse: 0.56658 |  0:04:57s
epoch 57 | loss: 0.208   | val_0_rmse: 0.43368 | val_1_rmse: 0.43789 |  0:05:03s
epoch 58 | loss: 0.20212 | val_0_rmse: 0.67598 | val_1_rmse: 0.67438 |  0:05:09s
epoch 59 | loss: 0.20498 | val_0_rmse: 0.54249 | val_1_rmse: 0.54591 |  0:05:15s
epoch 60 | loss: 0.20849 | val_0_rmse: 0.62293 | val_1_rmse: 0.62596 |  0:05:20s
epoch 61 | loss: 0.204   | val_0_rmse: 0.59887 | val_1_rmse: 0.60628 |  0:05:26s
epoch 62 | loss: 0.20276 | val_0_rmse: 0.62863 | val_1_rmse: 0.63304 |  0:05:31s
epoch 63 | loss: 0.21144 | val_0_rmse: 0.49697 | val_1_rmse: 0.50287 |  0:05:36s
epoch 64 | loss: 0.19691 | val_0_rmse: 0.40966 | val_1_rmse: 0.41517 |  0:05:41s
epoch 65 | loss: 0.1998  | val_0_rmse: 0.43783 | val_1_rmse: 0.44364 |  0:05:47s
epoch 66 | loss: 0.22616 | val_0_rmse: 0.64571 | val_1_rmse: 0.65293 |  0:05:52s
epoch 67 | loss: 0.20928 | val_0_rmse: 0.71256 | val_1_rmse: 0.7262  |  0:05:58s
epoch 68 | loss: 0.229   | val_0_rmse: 0.70582 | val_1_rmse: 0.71432 |  0:06:04s
epoch 69 | loss: 0.24339 | val_0_rmse: 0.47586 | val_1_rmse: 0.48202 |  0:06:09s
epoch 70 | loss: 0.21207 | val_0_rmse: 0.45213 | val_1_rmse: 0.45123 |  0:06:15s
epoch 71 | loss: 0.20567 | val_0_rmse: 0.44337 | val_1_rmse: 0.44715 |  0:06:21s
epoch 72 | loss: 0.20322 | val_0_rmse: 0.81049 | val_1_rmse: 0.81298 |  0:06:27s
epoch 73 | loss: 0.19505 | val_0_rmse: 0.49453 | val_1_rmse: 0.49504 |  0:06:32s
epoch 74 | loss: 0.19694 | val_0_rmse: 0.56337 | val_1_rmse: 0.56819 |  0:06:38s
epoch 75 | loss: 0.18915 | val_0_rmse: 0.4272  | val_1_rmse: 0.42987 |  0:06:44s
epoch 76 | loss: 0.20165 | val_0_rmse: 0.56697 | val_1_rmse: 0.56814 |  0:06:49s
epoch 77 | loss: 0.18903 | val_0_rmse: 0.45421 | val_1_rmse: 0.45459 |  0:06:55s
epoch 78 | loss: 0.19437 | val_0_rmse: 0.38565 | val_1_rmse: 0.39216 |  0:07:00s
epoch 79 | loss: 0.18893 | val_0_rmse: 0.40285 | val_1_rmse: 0.40654 |  0:07:06s
epoch 80 | loss: 0.19061 | val_0_rmse: 0.58537 | val_1_rmse: 0.59121 |  0:07:12s
epoch 81 | loss: 0.19055 | val_0_rmse: 0.46649 | val_1_rmse: 0.46883 |  0:07:17s
epoch 82 | loss: 0.19269 | val_0_rmse: 0.659   | val_1_rmse: 0.6517  |  0:07:22s
epoch 83 | loss: 0.22592 | val_0_rmse: 0.59477 | val_1_rmse: 0.60417 |  0:07:27s
epoch 84 | loss: 0.20626 | val_0_rmse: 0.45758 | val_1_rmse: 0.45817 |  0:07:32s
epoch 85 | loss: 0.18744 | val_0_rmse: 0.42175 | val_1_rmse: 0.42529 |  0:07:37s
epoch 86 | loss: 0.18764 | val_0_rmse: 0.4038  | val_1_rmse: 0.40807 |  0:07:43s
epoch 87 | loss: 0.18739 | val_0_rmse: 0.39163 | val_1_rmse: 0.39502 |  0:07:48s
epoch 88 | loss: 0.18164 | val_0_rmse: 0.5983  | val_1_rmse: 0.60505 |  0:07:53s
epoch 89 | loss: 0.18062 | val_0_rmse: 0.53941 | val_1_rmse: 0.54657 |  0:07:58s
epoch 90 | loss: 0.17986 | val_0_rmse: 0.67325 | val_1_rmse: 0.67071 |  0:08:03s
epoch 91 | loss: 0.19483 | val_0_rmse: 0.54954 | val_1_rmse: 0.54869 |  0:08:08s
epoch 92 | loss: 0.17825 | val_0_rmse: 0.6684  | val_1_rmse: 0.68053 |  0:08:13s
epoch 93 | loss: 0.21032 | val_0_rmse: 0.60506 | val_1_rmse: 0.59834 |  0:08:18s
epoch 94 | loss: 0.23593 | val_0_rmse: 0.71536 | val_1_rmse: 0.71253 |  0:08:23s
epoch 95 | loss: 0.23992 | val_0_rmse: 0.48055 | val_1_rmse: 0.48266 |  0:08:28s
epoch 96 | loss: 0.20415 | val_0_rmse: 0.50474 | val_1_rmse: 0.50511 |  0:08:33s
epoch 97 | loss: 0.19204 | val_0_rmse: 0.61568 | val_1_rmse: 0.6133  |  0:08:39s
epoch 98 | loss: 0.19919 | val_0_rmse: 0.45763 | val_1_rmse: 0.45938 |  0:08:44s
epoch 99 | loss: 0.191   | val_0_rmse: 0.59757 | val_1_rmse: 0.60609 |  0:08:49s
epoch 100| loss: 0.18735 | val_0_rmse: 0.80724 | val_1_rmse: 0.81923 |  0:08:54s
epoch 101| loss: 0.19523 | val_0_rmse: 0.4076  | val_1_rmse: 0.41277 |  0:08:59s
epoch 102| loss: 0.1904  | val_0_rmse: 0.46156 | val_1_rmse: 0.46629 |  0:09:04s
epoch 103| loss: 0.1783  | val_0_rmse: 0.38238 | val_1_rmse: 0.38691 |  0:09:10s
epoch 104| loss: 0.20846 | val_0_rmse: 0.8034  | val_1_rmse: 0.81318 |  0:09:15s
epoch 105| loss: 0.20276 | val_0_rmse: 0.50813 | val_1_rmse: 0.50482 |  0:09:20s
epoch 106| loss: 0.18902 | val_0_rmse: 0.38992 | val_1_rmse: 0.39358 |  0:09:25s
epoch 107| loss: 0.19788 | val_0_rmse: 0.79314 | val_1_rmse: 0.79428 |  0:09:30s
epoch 108| loss: 0.20127 | val_0_rmse: 0.61507 | val_1_rmse: 0.62318 |  0:09:35s
epoch 109| loss: 0.21215 | val_0_rmse: 0.55025 | val_1_rmse: 0.55734 |  0:09:40s
epoch 110| loss: 0.20168 | val_0_rmse: 0.58946 | val_1_rmse: 0.5978  |  0:09:45s
epoch 111| loss: 0.20214 | val_0_rmse: 0.47613 | val_1_rmse: 0.47504 |  0:09:50s
epoch 112| loss: 0.20089 | val_0_rmse: 0.54754 | val_1_rmse: 0.54713 |  0:09:55s
epoch 113| loss: 0.19232 | val_0_rmse: 0.40547 | val_1_rmse: 0.40916 |  0:10:00s
epoch 114| loss: 0.1905  | val_0_rmse: 0.78863 | val_1_rmse: 0.79219 |  0:10:05s
epoch 115| loss: 0.20905 | val_0_rmse: 0.79483 | val_1_rmse: 0.79677 |  0:10:11s
epoch 116| loss: 0.19788 | val_0_rmse: 0.41825 | val_1_rmse: 0.42439 |  0:10:16s
epoch 117| loss: 0.1864  | val_0_rmse: 0.68075 | val_1_rmse: 0.69456 |  0:10:21s
epoch 118| loss: 0.17991 | val_0_rmse: 0.3803  | val_1_rmse: 0.38655 |  0:10:26s
epoch 119| loss: 0.17591 | val_0_rmse: 0.52799 | val_1_rmse: 0.5245  |  0:10:31s
epoch 120| loss: 0.17343 | val_0_rmse: 0.47343 | val_1_rmse: 0.47207 |  0:10:36s
epoch 121| loss: 0.17135 | val_0_rmse: 0.39666 | val_1_rmse: 0.40218 |  0:10:41s
epoch 122| loss: 0.17031 | val_0_rmse: 0.49498 | val_1_rmse: 0.4918  |  0:10:46s
epoch 123| loss: 0.17317 | val_0_rmse: 0.45145 | val_1_rmse: 0.44871 |  0:10:51s
epoch 124| loss: 0.17996 | val_0_rmse: 0.38438 | val_1_rmse: 0.3912  |  0:10:56s
epoch 125| loss: 0.17525 | val_0_rmse: 0.39746 | val_1_rmse: 0.40394 |  0:11:01s
epoch 126| loss: 0.16877 | val_0_rmse: 0.4015  | val_1_rmse: 0.40486 |  0:11:07s
epoch 127| loss: 0.16565 | val_0_rmse: 0.52137 | val_1_rmse: 0.52751 |  0:11:12s
epoch 128| loss: 0.16734 | val_0_rmse: 0.55418 | val_1_rmse: 0.55343 |  0:11:17s
epoch 129| loss: 0.17837 | val_0_rmse: 0.47206 | val_1_rmse: 0.47125 |  0:11:22s
epoch 130| loss: 0.1654  | val_0_rmse: 0.68927 | val_1_rmse: 0.7003  |  0:11:27s
epoch 131| loss: 0.16423 | val_0_rmse: 0.58369 | val_1_rmse: 0.57955 |  0:11:32s
epoch 132| loss: 0.16931 | val_0_rmse: 0.36723 | val_1_rmse: 0.37194 |  0:11:37s
epoch 133| loss: 0.16167 | val_0_rmse: 0.41978 | val_1_rmse: 0.42321 |  0:11:43s
epoch 134| loss: 0.16158 | val_0_rmse: 0.4329  | val_1_rmse: 0.43355 |  0:11:48s
epoch 135| loss: 0.15876 | val_0_rmse: 0.56446 | val_1_rmse: 0.55984 |  0:11:53s
epoch 136| loss: 0.15627 | val_0_rmse: 0.39406 | val_1_rmse: 0.39995 |  0:11:58s
epoch 137| loss: 0.16149 | val_0_rmse: 0.45503 | val_1_rmse: 0.46048 |  0:12:03s
epoch 138| loss: 0.15767 | val_0_rmse: 0.68008 | val_1_rmse: 0.67687 |  0:12:09s
epoch 139| loss: 0.15658 | val_0_rmse: 0.62123 | val_1_rmse: 0.61724 |  0:12:14s
epoch 140| loss: 0.15448 | val_0_rmse: 0.63309 | val_1_rmse: 0.63183 |  0:12:20s
epoch 141| loss: 0.1597  | val_0_rmse: 0.38297 | val_1_rmse: 0.38775 |  0:12:25s
epoch 142| loss: 0.16101 | val_0_rmse: 0.57256 | val_1_rmse: 0.58007 |  0:12:30s
epoch 143| loss: 0.16158 | val_0_rmse: 0.49858 | val_1_rmse: 0.49423 |  0:12:35s
epoch 144| loss: 0.17785 | val_0_rmse: 0.62235 | val_1_rmse: 0.61988 |  0:12:40s
epoch 145| loss: 0.15727 | val_0_rmse: 0.36785 | val_1_rmse: 0.37745 |  0:12:45s
epoch 146| loss: 0.16376 | val_0_rmse: 0.37969 | val_1_rmse: 0.38545 |  0:12:51s
epoch 147| loss: 0.15583 | val_0_rmse: 0.39162 | val_1_rmse: 0.39866 |  0:12:56s
epoch 148| loss: 0.15574 | val_0_rmse: 0.33763 | val_1_rmse: 0.347   |  0:13:01s
epoch 149| loss: 0.15303 | val_0_rmse: 0.40705 | val_1_rmse: 0.41289 |  0:13:06s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.347
Best weights from best epoch are automatically used!
ended training at: 00:30:47
Feature importance:
[('Area', 0.29865163214067525), ('Baths', 0.08146729337837216), ('Beds', 0.031052289190955863), ('Latitude', 0.41875760251898997), ('Longitude', 0.11498815620684355), ('Month', 0.0550830265641632), ('Year', 0.0)]
Mean squared error is of 848489976.9564213
Mean absolute error:20218.43811814308
MAPE:0.17225036533068744
R2 score:0.8829446270559179
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:30:48
epoch 0  | loss: 0.5459  | val_0_rmse: 0.66774 | val_1_rmse: 0.67094 |  0:00:05s
epoch 1  | loss: 0.44869 | val_0_rmse: 0.64574 | val_1_rmse: 0.65314 |  0:00:10s
epoch 2  | loss: 0.37998 | val_0_rmse: 0.59372 | val_1_rmse: 0.60053 |  0:00:15s
epoch 3  | loss: 0.35163 | val_0_rmse: 0.57979 | val_1_rmse: 0.58541 |  0:00:20s
epoch 4  | loss: 0.33779 | val_0_rmse: 0.56428 | val_1_rmse: 0.57092 |  0:00:26s
epoch 5  | loss: 0.33308 | val_0_rmse: 0.56317 | val_1_rmse: 0.5689  |  0:00:31s
epoch 6  | loss: 0.32221 | val_0_rmse: 0.58075 | val_1_rmse: 0.58764 |  0:00:36s
epoch 7  | loss: 0.30898 | val_0_rmse: 0.56007 | val_1_rmse: 0.56415 |  0:00:41s
epoch 8  | loss: 0.30961 | val_0_rmse: 0.54689 | val_1_rmse: 0.55279 |  0:00:46s
epoch 9  | loss: 0.29786 | val_0_rmse: 0.53919 | val_1_rmse: 0.54239 |  0:00:51s
epoch 10 | loss: 0.29122 | val_0_rmse: 0.5552  | val_1_rmse: 0.56038 |  0:00:57s
epoch 11 | loss: 0.29118 | val_0_rmse: 0.53138 | val_1_rmse: 0.53602 |  0:01:02s
epoch 12 | loss: 0.28523 | val_0_rmse: 0.56685 | val_1_rmse: 0.57298 |  0:01:07s
epoch 13 | loss: 0.28844 | val_0_rmse: 0.63889 | val_1_rmse: 0.64127 |  0:01:12s
epoch 14 | loss: 0.28187 | val_0_rmse: 0.52524 | val_1_rmse: 0.5324  |  0:01:17s
epoch 15 | loss: 0.27387 | val_0_rmse: 0.73313 | val_1_rmse: 0.73836 |  0:01:23s
epoch 16 | loss: 0.26793 | val_0_rmse: 0.60949 | val_1_rmse: 0.61119 |  0:01:28s
epoch 17 | loss: 0.2709  | val_0_rmse: 0.5087  | val_1_rmse: 0.51614 |  0:01:33s
epoch 18 | loss: 0.27425 | val_0_rmse: 0.58429 | val_1_rmse: 0.59061 |  0:01:38s
epoch 19 | loss: 0.27524 | val_0_rmse: 0.71402 | val_1_rmse: 0.71906 |  0:01:43s
epoch 20 | loss: 0.26829 | val_0_rmse: 0.81414 | val_1_rmse: 0.82315 |  0:01:48s
epoch 21 | loss: 0.26065 | val_0_rmse: 0.52575 | val_1_rmse: 0.52849 |  0:01:53s
epoch 22 | loss: 0.26157 | val_0_rmse: 0.49721 | val_1_rmse: 0.50241 |  0:01:58s
epoch 23 | loss: 0.26006 | val_0_rmse: 0.51036 | val_1_rmse: 0.51147 |  0:02:03s
epoch 24 | loss: 0.2871  | val_0_rmse: 0.77141 | val_1_rmse: 0.78157 |  0:02:08s
epoch 25 | loss: 0.28378 | val_0_rmse: 0.54197 | val_1_rmse: 0.54759 |  0:02:13s
epoch 26 | loss: 0.273   | val_0_rmse: 0.57011 | val_1_rmse: 0.57571 |  0:02:19s
epoch 27 | loss: 0.27851 | val_0_rmse: 0.50973 | val_1_rmse: 0.51416 |  0:02:24s
epoch 28 | loss: 0.30755 | val_0_rmse: 0.67801 | val_1_rmse: 0.68328 |  0:02:29s
epoch 29 | loss: 0.28742 | val_0_rmse: 0.60767 | val_1_rmse: 0.6156  |  0:02:34s
epoch 30 | loss: 0.27317 | val_0_rmse: 0.68889 | val_1_rmse: 0.69704 |  0:02:39s
epoch 31 | loss: 0.2702  | val_0_rmse: 0.54831 | val_1_rmse: 0.55421 |  0:02:44s
epoch 32 | loss: 0.26158 | val_0_rmse: 0.55826 | val_1_rmse: 0.56051 |  0:02:49s
epoch 33 | loss: 0.25712 | val_0_rmse: 0.53954 | val_1_rmse: 0.54473 |  0:02:55s
epoch 34 | loss: 0.2561  | val_0_rmse: 0.55784 | val_1_rmse: 0.55927 |  0:03:00s
epoch 35 | loss: 0.25271 | val_0_rmse: 0.60165 | val_1_rmse: 0.60747 |  0:03:05s
epoch 36 | loss: 0.25615 | val_0_rmse: 0.79065 | val_1_rmse: 0.78836 |  0:03:10s
epoch 37 | loss: 0.25453 | val_0_rmse: 0.51731 | val_1_rmse: 0.52328 |  0:03:15s
epoch 38 | loss: 0.25093 | val_0_rmse: 0.59121 | val_1_rmse: 0.59233 |  0:03:20s
epoch 39 | loss: 0.2516  | val_0_rmse: 0.53278 | val_1_rmse: 0.54213 |  0:03:25s
epoch 40 | loss: 0.25087 | val_0_rmse: 0.50484 | val_1_rmse: 0.50813 |  0:03:30s
epoch 41 | loss: 0.24383 | val_0_rmse: 0.57076 | val_1_rmse: 0.57448 |  0:03:35s
epoch 42 | loss: 0.23937 | val_0_rmse: 0.72467 | val_1_rmse: 0.72796 |  0:03:40s
epoch 43 | loss: 0.24181 | val_0_rmse: 0.49323 | val_1_rmse: 0.49649 |  0:03:45s
epoch 44 | loss: 0.24752 | val_0_rmse: 1.0188  | val_1_rmse: 1.01951 |  0:03:51s
epoch 45 | loss: 0.27542 | val_0_rmse: 0.53521 | val_1_rmse: 0.54117 |  0:03:56s
epoch 46 | loss: 0.24442 | val_0_rmse: 0.57142 | val_1_rmse: 0.57516 |  0:04:01s
epoch 47 | loss: 0.24909 | val_0_rmse: 0.48888 | val_1_rmse: 0.4933  |  0:04:06s
epoch 48 | loss: 0.24487 | val_0_rmse: 0.54004 | val_1_rmse: 0.54508 |  0:04:11s
epoch 49 | loss: 0.24291 | val_0_rmse: 0.72926 | val_1_rmse: 0.73238 |  0:04:16s
epoch 50 | loss: 0.24076 | val_0_rmse: 0.50123 | val_1_rmse: 0.50465 |  0:04:21s
epoch 51 | loss: 0.25445 | val_0_rmse: 0.56363 | val_1_rmse: 0.56739 |  0:04:26s
epoch 52 | loss: 0.23924 | val_0_rmse: 0.63913 | val_1_rmse: 0.649   |  0:04:31s
epoch 53 | loss: 0.23491 | val_0_rmse: 0.60615 | val_1_rmse: 0.61143 |  0:04:36s
epoch 54 | loss: 0.23941 | val_0_rmse: 0.62218 | val_1_rmse: 0.62897 |  0:04:41s
epoch 55 | loss: 0.2407  | val_0_rmse: 0.60982 | val_1_rmse: 0.61427 |  0:04:46s
epoch 56 | loss: 0.24132 | val_0_rmse: 0.73644 | val_1_rmse: 0.74189 |  0:04:51s
epoch 57 | loss: 0.24991 | val_0_rmse: 0.46539 | val_1_rmse: 0.46863 |  0:04:56s
epoch 58 | loss: 0.23347 | val_0_rmse: 0.62085 | val_1_rmse: 0.6276  |  0:05:02s
epoch 59 | loss: 0.23574 | val_0_rmse: 0.60289 | val_1_rmse: 0.60498 |  0:05:07s
epoch 60 | loss: 0.22919 | val_0_rmse: 0.69039 | val_1_rmse: 0.69328 |  0:05:12s
epoch 61 | loss: 0.25796 | val_0_rmse: 0.78706 | val_1_rmse: 0.79619 |  0:05:17s
epoch 62 | loss: 0.29101 | val_0_rmse: 0.59827 | val_1_rmse: 0.60361 |  0:05:22s
epoch 63 | loss: 0.27164 | val_0_rmse: 0.78648 | val_1_rmse: 0.79686 |  0:05:27s
epoch 64 | loss: 0.27513 | val_0_rmse: 0.52049 | val_1_rmse: 0.52432 |  0:05:33s
epoch 65 | loss: 0.24901 | val_0_rmse: 0.64947 | val_1_rmse: 0.64829 |  0:05:38s
epoch 66 | loss: 0.2487  | val_0_rmse: 0.50976 | val_1_rmse: 0.51964 |  0:05:43s
epoch 67 | loss: 0.2352  | val_0_rmse: 0.68309 | val_1_rmse: 0.69037 |  0:05:48s
epoch 68 | loss: 0.23852 | val_0_rmse: 0.50822 | val_1_rmse: 0.51122 |  0:05:53s
epoch 69 | loss: 0.23738 | val_0_rmse: 0.60239 | val_1_rmse: 0.60522 |  0:05:58s
epoch 70 | loss: 0.2377  | val_0_rmse: 0.68999 | val_1_rmse: 0.69249 |  0:06:03s
epoch 71 | loss: 0.2393  | val_0_rmse: 0.47718 | val_1_rmse: 0.48637 |  0:06:08s
epoch 72 | loss: 0.22496 | val_0_rmse: 0.58302 | val_1_rmse: 0.59449 |  0:06:13s
epoch 73 | loss: 0.2327  | val_0_rmse: 0.73025 | val_1_rmse: 0.73395 |  0:06:19s
epoch 74 | loss: 0.24102 | val_0_rmse: 0.51638 | val_1_rmse: 0.52579 |  0:06:24s
epoch 75 | loss: 0.25438 | val_0_rmse: 0.5071  | val_1_rmse: 0.51351 |  0:06:29s
epoch 76 | loss: 0.23159 | val_0_rmse: 0.4768  | val_1_rmse: 0.48136 |  0:06:34s
epoch 77 | loss: 0.2338  | val_0_rmse: 0.79455 | val_1_rmse: 0.79568 |  0:06:39s
epoch 78 | loss: 0.23993 | val_0_rmse: 0.49727 | val_1_rmse: 0.50672 |  0:06:44s
epoch 79 | loss: 0.22775 | val_0_rmse: 0.72836 | val_1_rmse: 0.73215 |  0:06:49s
epoch 80 | loss: 0.25765 | val_0_rmse: 0.6429  | val_1_rmse: 0.64577 |  0:06:54s
epoch 81 | loss: 0.243   | val_0_rmse: 0.53357 | val_1_rmse: 0.54117 |  0:07:00s
epoch 82 | loss: 0.23123 | val_0_rmse: 0.48374 | val_1_rmse: 0.48975 |  0:07:05s
epoch 83 | loss: 0.22857 | val_0_rmse: 0.53357 | val_1_rmse: 0.54062 |  0:07:10s
epoch 84 | loss: 0.2305  | val_0_rmse: 0.50148 | val_1_rmse: 0.50611 |  0:07:15s
epoch 85 | loss: 0.22374 | val_0_rmse: 0.63425 | val_1_rmse: 0.6385  |  0:07:20s
epoch 86 | loss: 0.22574 | val_0_rmse: 0.88007 | val_1_rmse: 0.87964 |  0:07:25s
epoch 87 | loss: 0.22963 | val_0_rmse: 0.82644 | val_1_rmse: 0.82258 |  0:07:30s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.46863
Best weights from best epoch are automatically used!
ended training at: 00:38:20
Feature importance:
[('Area', 0.3317219025854861), ('Baths', 0.2195960836609927), ('Beds', 0.046673121884696606), ('Latitude', 0.29255585178596566), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.10945304008285887)]
Mean squared error is of 1577245031.1292732
Mean absolute error:28054.55680802013
MAPE:0.24341665814584013
R2 score:0.7846953738766386
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:46:33
epoch 0  | loss: 0.53132 | val_0_rmse: 0.65868 | val_1_rmse: 0.65322 |  0:00:04s
epoch 1  | loss: 0.40776 | val_0_rmse: 0.70665 | val_1_rmse: 0.69599 |  0:00:10s
epoch 2  | loss: 0.38618 | val_0_rmse: 0.64277 | val_1_rmse: 0.64055 |  0:00:14s
epoch 3  | loss: 0.36784 | val_0_rmse: 0.69841 | val_1_rmse: 0.70013 |  0:00:19s
epoch 4  | loss: 0.3478  | val_0_rmse: 0.61746 | val_1_rmse: 0.62251 |  0:00:24s
epoch 5  | loss: 0.33394 | val_0_rmse: 0.67143 | val_1_rmse: 0.66735 |  0:00:29s
epoch 6  | loss: 0.33797 | val_0_rmse: 0.63292 | val_1_rmse: 0.631   |  0:00:34s
epoch 7  | loss: 0.33909 | val_0_rmse: 0.68597 | val_1_rmse: 0.68202 |  0:00:39s
epoch 8  | loss: 0.34859 | val_0_rmse: 0.57939 | val_1_rmse: 0.57427 |  0:00:44s
epoch 9  | loss: 0.33301 | val_0_rmse: 0.57748 | val_1_rmse: 0.57202 |  0:00:49s
epoch 10 | loss: 0.33255 | val_0_rmse: 0.69081 | val_1_rmse: 0.68447 |  0:00:54s
epoch 11 | loss: 0.33094 | val_0_rmse: 0.62128 | val_1_rmse: 0.61875 |  0:00:59s
epoch 12 | loss: 0.33023 | val_0_rmse: 0.62144 | val_1_rmse: 0.61913 |  0:01:04s
epoch 13 | loss: 0.32196 | val_0_rmse: 0.60641 | val_1_rmse: 0.60601 |  0:01:09s
epoch 14 | loss: 0.31894 | val_0_rmse: 0.55248 | val_1_rmse: 0.54874 |  0:01:15s
epoch 15 | loss: 0.32434 | val_0_rmse: 0.64295 | val_1_rmse: 0.63852 |  0:01:19s
epoch 16 | loss: 0.33076 | val_0_rmse: 0.64647 | val_1_rmse: 0.64469 |  0:01:25s
epoch 17 | loss: 0.32954 | val_0_rmse: 0.59398 | val_1_rmse: 0.59203 |  0:01:30s
epoch 18 | loss: 0.31793 | val_0_rmse: 0.61701 | val_1_rmse: 0.61263 |  0:01:35s
epoch 19 | loss: 0.31402 | val_0_rmse: 0.54609 | val_1_rmse: 0.54524 |  0:01:40s
epoch 20 | loss: 0.32821 | val_0_rmse: 0.56099 | val_1_rmse: 0.55601 |  0:01:45s
epoch 21 | loss: 0.31407 | val_0_rmse: 0.59565 | val_1_rmse: 0.59244 |  0:01:50s
epoch 22 | loss: 0.31237 | val_0_rmse: 0.58672 | val_1_rmse: 0.58211 |  0:01:55s
epoch 23 | loss: 0.3155  | val_0_rmse: 0.5852  | val_1_rmse: 0.58185 |  0:02:00s
epoch 24 | loss: 0.30866 | val_0_rmse: 0.54933 | val_1_rmse: 0.54267 |  0:02:05s
epoch 25 | loss: 0.30411 | val_0_rmse: 0.62182 | val_1_rmse: 0.62108 |  0:02:10s
epoch 26 | loss: 0.30648 | val_0_rmse: 0.57829 | val_1_rmse: 0.57282 |  0:02:15s
epoch 27 | loss: 0.30259 | val_0_rmse: 0.5993  | val_1_rmse: 0.5949  |  0:02:20s
epoch 28 | loss: 0.29948 | val_0_rmse: 0.60165 | val_1_rmse: 0.59639 |  0:02:25s
epoch 29 | loss: 0.29116 | val_0_rmse: 0.56587 | val_1_rmse: 0.56039 |  0:02:30s
epoch 30 | loss: 0.29403 | val_0_rmse: 0.56994 | val_1_rmse: 0.56809 |  0:02:36s
epoch 31 | loss: 0.29128 | val_0_rmse: 0.54925 | val_1_rmse: 0.54695 |  0:02:41s
epoch 32 | loss: 0.28984 | val_0_rmse: 0.56265 | val_1_rmse: 0.55979 |  0:02:46s
epoch 33 | loss: 0.29084 | val_0_rmse: 0.56183 | val_1_rmse: 0.55835 |  0:02:51s
epoch 34 | loss: 0.2938  | val_0_rmse: 0.56089 | val_1_rmse: 0.55888 |  0:02:56s
epoch 35 | loss: 0.29624 | val_0_rmse: 0.56708 | val_1_rmse: 0.56255 |  0:03:01s
epoch 36 | loss: 0.30892 | val_0_rmse: 0.597   | val_1_rmse: 0.59509 |  0:03:06s
epoch 37 | loss: 0.28702 | val_0_rmse: 0.57486 | val_1_rmse: 0.57488 |  0:03:11s
epoch 38 | loss: 0.28874 | val_0_rmse: 0.54872 | val_1_rmse: 0.54436 |  0:03:16s
epoch 39 | loss: 0.29369 | val_0_rmse: 0.54856 | val_1_rmse: 0.54736 |  0:03:21s
epoch 40 | loss: 0.28919 | val_0_rmse: 0.57613 | val_1_rmse: 0.57266 |  0:03:27s
epoch 41 | loss: 0.27971 | val_0_rmse: 0.56536 | val_1_rmse: 0.56573 |  0:03:32s
epoch 42 | loss: 0.28418 | val_0_rmse: 0.54496 | val_1_rmse: 0.54379 |  0:03:37s
epoch 43 | loss: 0.28505 | val_0_rmse: 0.56846 | val_1_rmse: 0.56708 |  0:03:42s
epoch 44 | loss: 0.28847 | val_0_rmse: 0.58659 | val_1_rmse: 0.58381 |  0:03:47s
epoch 45 | loss: 0.2957  | val_0_rmse: 0.55698 | val_1_rmse: 0.55624 |  0:03:52s
epoch 46 | loss: 0.28404 | val_0_rmse: 0.56342 | val_1_rmse: 0.5632  |  0:03:57s
epoch 47 | loss: 0.27792 | val_0_rmse: 0.56078 | val_1_rmse: 0.56053 |  0:04:03s
epoch 48 | loss: 0.27522 | val_0_rmse: 0.57875 | val_1_rmse: 0.57828 |  0:04:08s
epoch 49 | loss: 0.28452 | val_0_rmse: 0.7506  | val_1_rmse: 0.7459  |  0:04:13s
epoch 50 | loss: 0.28359 | val_0_rmse: 0.61519 | val_1_rmse: 0.61326 |  0:04:18s
epoch 51 | loss: 0.2742  | val_0_rmse: 0.55139 | val_1_rmse: 0.55137 |  0:04:23s
epoch 52 | loss: 0.27773 | val_0_rmse: 0.54891 | val_1_rmse: 0.54776 |  0:04:28s
epoch 53 | loss: 0.27557 | val_0_rmse: 0.59168 | val_1_rmse: 0.58972 |  0:04:34s
epoch 54 | loss: 0.27021 | val_0_rmse: 0.59455 | val_1_rmse: 0.59494 |  0:04:38s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.54267
Best weights from best epoch are automatically used!
ended training at: 00:51:14
Feature importance:
[('Area', 0.4063759924170303), ('Baths', 0.21804840616226742), ('Beds', 0.0), ('Latitude', 0.24402331838700617), ('Longitude', 0.12465621553311855), ('Month', 0.006896067500577547), ('Year', 0.0)]
Mean squared error is of 2624062735.0971437
Mean absolute error:34938.493886616925
MAPE:0.31797467964022785
R2 score:0.6882026533748964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:51:15
epoch 0  | loss: 0.54211 | val_0_rmse: 0.67643 | val_1_rmse: 0.6815  |  0:00:05s
epoch 1  | loss: 0.41724 | val_0_rmse: 0.64235 | val_1_rmse: 0.64725 |  0:00:10s
epoch 2  | loss: 0.37092 | val_0_rmse: 0.62448 | val_1_rmse: 0.63264 |  0:00:15s
epoch 3  | loss: 0.33921 | val_0_rmse: 0.59703 | val_1_rmse: 0.60591 |  0:00:20s
epoch 4  | loss: 0.32217 | val_0_rmse: 0.57313 | val_1_rmse: 0.57841 |  0:00:25s
epoch 5  | loss: 0.34499 | val_0_rmse: 0.62549 | val_1_rmse: 0.63447 |  0:00:30s
epoch 6  | loss: 0.32824 | val_0_rmse: 0.57863 | val_1_rmse: 0.58496 |  0:00:35s
epoch 7  | loss: 0.31231 | val_0_rmse: 0.67074 | val_1_rmse: 0.68044 |  0:00:40s
epoch 8  | loss: 0.32991 | val_0_rmse: 0.60482 | val_1_rmse: 0.61417 |  0:00:45s
epoch 9  | loss: 0.31366 | val_0_rmse: 0.55491 | val_1_rmse: 0.55965 |  0:00:51s
epoch 10 | loss: 0.31218 | val_0_rmse: 0.57696 | val_1_rmse: 0.5821  |  0:00:56s
epoch 11 | loss: 0.30643 | val_0_rmse: 0.61605 | val_1_rmse: 0.62316 |  0:01:01s
epoch 12 | loss: 0.29802 | val_0_rmse: 0.57862 | val_1_rmse: 0.58709 |  0:01:06s
epoch 13 | loss: 0.30141 | val_0_rmse: 0.60681 | val_1_rmse: 0.61496 |  0:01:11s
epoch 14 | loss: 0.30128 | val_0_rmse: 0.56811 | val_1_rmse: 0.57433 |  0:01:16s
epoch 15 | loss: 0.29543 | val_0_rmse: 0.60786 | val_1_rmse: 0.61844 |  0:01:21s
epoch 16 | loss: 0.2957  | val_0_rmse: 0.64339 | val_1_rmse: 0.65444 |  0:01:26s
epoch 17 | loss: 0.32394 | val_0_rmse: 0.62047 | val_1_rmse: 0.62669 |  0:01:31s
epoch 18 | loss: 0.32185 | val_0_rmse: 0.64538 | val_1_rmse: 0.65512 |  0:01:36s
epoch 19 | loss: 0.31845 | val_0_rmse: 0.67544 | val_1_rmse: 0.68167 |  0:01:41s
epoch 20 | loss: 0.31252 | val_0_rmse: 0.56932 | val_1_rmse: 0.57853 |  0:01:46s
epoch 21 | loss: 0.30212 | val_0_rmse: 0.59597 | val_1_rmse: 0.60599 |  0:01:51s
epoch 22 | loss: 0.30161 | val_0_rmse: 0.73768 | val_1_rmse: 0.73504 |  0:01:56s
epoch 23 | loss: 0.30274 | val_0_rmse: 0.57831 | val_1_rmse: 0.58541 |  0:02:01s
epoch 24 | loss: 0.28957 | val_0_rmse: 0.56795 | val_1_rmse: 0.57485 |  0:02:06s
epoch 25 | loss: 0.2839  | val_0_rmse: 0.55428 | val_1_rmse: 0.56177 |  0:02:11s
epoch 26 | loss: 0.28209 | val_0_rmse: 0.5541  | val_1_rmse: 0.5571  |  0:02:17s
epoch 27 | loss: 0.28178 | val_0_rmse: 0.51975 | val_1_rmse: 0.52541 |  0:02:22s
epoch 28 | loss: 0.26863 | val_0_rmse: 0.57014 | val_1_rmse: 0.5811  |  0:02:27s
epoch 29 | loss: 0.26504 | val_0_rmse: 0.59461 | val_1_rmse: 0.60385 |  0:02:32s
epoch 30 | loss: 0.28338 | val_0_rmse: 0.60673 | val_1_rmse: 0.61792 |  0:02:37s
epoch 31 | loss: 0.27082 | val_0_rmse: 0.56589 | val_1_rmse: 0.57764 |  0:02:42s
epoch 32 | loss: 0.26291 | val_0_rmse: 0.54102 | val_1_rmse: 0.55    |  0:02:47s
epoch 33 | loss: 0.25812 | val_0_rmse: 0.54531 | val_1_rmse: 0.55438 |  0:02:53s
epoch 34 | loss: 0.25532 | val_0_rmse: 0.56721 | val_1_rmse: 0.57505 |  0:02:58s
epoch 35 | loss: 0.25646 | val_0_rmse: 0.55211 | val_1_rmse: 0.56248 |  0:03:03s
epoch 36 | loss: 0.25513 | val_0_rmse: 0.58278 | val_1_rmse: 0.59258 |  0:03:08s
epoch 37 | loss: 0.25794 | val_0_rmse: 0.52958 | val_1_rmse: 0.53726 |  0:03:13s
epoch 38 | loss: 0.25831 | val_0_rmse: 0.57938 | val_1_rmse: 0.59246 |  0:03:19s
epoch 39 | loss: 0.25648 | val_0_rmse: 0.53903 | val_1_rmse: 0.55027 |  0:03:24s
epoch 40 | loss: 0.25508 | val_0_rmse: 0.54034 | val_1_rmse: 0.55029 |  0:03:29s
epoch 41 | loss: 0.24628 | val_0_rmse: 0.56975 | val_1_rmse: 0.57823 |  0:03:34s
epoch 42 | loss: 0.24237 | val_0_rmse: 0.53233 | val_1_rmse: 0.54468 |  0:03:39s
epoch 43 | loss: 0.24513 | val_0_rmse: 0.56451 | val_1_rmse: 0.57736 |  0:03:44s
epoch 44 | loss: 0.24679 | val_0_rmse: 0.66061 | val_1_rmse: 0.66849 |  0:03:49s
epoch 45 | loss: 0.24827 | val_0_rmse: 0.56662 | val_1_rmse: 0.58028 |  0:03:54s
epoch 46 | loss: 0.25089 | val_0_rmse: 0.52514 | val_1_rmse: 0.53432 |  0:03:59s
epoch 47 | loss: 0.24572 | val_0_rmse: 0.53913 | val_1_rmse: 0.55361 |  0:04:04s
epoch 48 | loss: 0.24604 | val_0_rmse: 0.54895 | val_1_rmse: 0.56063 |  0:04:10s
epoch 49 | loss: 0.24275 | val_0_rmse: 0.52993 | val_1_rmse: 0.54033 |  0:04:15s
epoch 50 | loss: 0.23806 | val_0_rmse: 0.54223 | val_1_rmse: 0.55448 |  0:04:20s
epoch 51 | loss: 0.23445 | val_0_rmse: 0.56781 | val_1_rmse: 0.57728 |  0:04:25s
epoch 52 | loss: 0.22967 | val_0_rmse: 0.50186 | val_1_rmse: 0.51453 |  0:04:30s
epoch 53 | loss: 0.23218 | val_0_rmse: 0.55049 | val_1_rmse: 0.56324 |  0:04:35s
epoch 54 | loss: 0.23244 | val_0_rmse: 0.57993 | val_1_rmse: 0.59467 |  0:04:41s
epoch 55 | loss: 0.22988 | val_0_rmse: 0.58556 | val_1_rmse: 0.59522 |  0:04:46s
epoch 56 | loss: 0.22891 | val_0_rmse: 0.53406 | val_1_rmse: 0.54583 |  0:04:51s
epoch 57 | loss: 0.231   | val_0_rmse: 0.54383 | val_1_rmse: 0.55435 |  0:04:56s
epoch 58 | loss: 0.22879 | val_0_rmse: 0.5292  | val_1_rmse: 0.54033 |  0:05:01s
epoch 59 | loss: 0.22388 | val_0_rmse: 0.57892 | val_1_rmse: 0.58927 |  0:05:06s
epoch 60 | loss: 0.22537 | val_0_rmse: 0.51988 | val_1_rmse: 0.53037 |  0:05:11s
epoch 61 | loss: 0.22376 | val_0_rmse: 0.52921 | val_1_rmse: 0.54253 |  0:05:16s
epoch 62 | loss: 0.22361 | val_0_rmse: 0.59136 | val_1_rmse: 0.60421 |  0:05:21s
epoch 63 | loss: 0.22564 | val_0_rmse: 0.54007 | val_1_rmse: 0.55058 |  0:05:26s
epoch 64 | loss: 0.22062 | val_0_rmse: 0.54773 | val_1_rmse: 0.55833 |  0:05:31s
epoch 65 | loss: 0.22035 | val_0_rmse: 0.5631  | val_1_rmse: 0.57301 |  0:05:36s
epoch 66 | loss: 0.22837 | val_0_rmse: 0.589   | val_1_rmse: 0.59969 |  0:05:41s
epoch 67 | loss: 0.22397 | val_0_rmse: 0.52914 | val_1_rmse: 0.53861 |  0:05:46s
epoch 68 | loss: 0.22109 | val_0_rmse: 0.556   | val_1_rmse: 0.56798 |  0:05:51s
epoch 69 | loss: 0.21892 | val_0_rmse: 0.49311 | val_1_rmse: 0.50082 |  0:05:57s
epoch 70 | loss: 0.22528 | val_0_rmse: 0.57164 | val_1_rmse: 0.58349 |  0:06:02s
epoch 71 | loss: 0.24188 | val_0_rmse: 0.56852 | val_1_rmse: 0.57728 |  0:06:07s
epoch 72 | loss: 0.2267  | val_0_rmse: 0.51227 | val_1_rmse: 0.52411 |  0:06:12s
epoch 73 | loss: 0.21997 | val_0_rmse: 0.54644 | val_1_rmse: 0.55597 |  0:06:17s
epoch 74 | loss: 0.21823 | val_0_rmse: 0.52106 | val_1_rmse: 0.52973 |  0:06:22s
epoch 75 | loss: 0.22085 | val_0_rmse: 0.5966  | val_1_rmse: 0.60769 |  0:06:27s
epoch 76 | loss: 0.2164  | val_0_rmse: 0.51019 | val_1_rmse: 0.51603 |  0:06:32s
epoch 77 | loss: 0.22418 | val_0_rmse: 0.45178 | val_1_rmse: 0.45878 |  0:06:37s
epoch 78 | loss: 0.21679 | val_0_rmse: 0.50814 | val_1_rmse: 0.51585 |  0:06:42s
epoch 79 | loss: 0.2226  | val_0_rmse: 0.53759 | val_1_rmse: 0.5453  |  0:06:47s
epoch 80 | loss: 0.22303 | val_0_rmse: 0.56241 | val_1_rmse: 0.57347 |  0:06:52s
epoch 81 | loss: 0.21544 | val_0_rmse: 0.56518 | val_1_rmse: 0.5749  |  0:06:58s
epoch 82 | loss: 0.21255 | val_0_rmse: 0.54665 | val_1_rmse: 0.5604  |  0:07:03s
epoch 83 | loss: 0.21257 | val_0_rmse: 0.53301 | val_1_rmse: 0.54533 |  0:07:08s
epoch 84 | loss: 0.20999 | val_0_rmse: 0.56093 | val_1_rmse: 0.57301 |  0:07:13s
epoch 85 | loss: 0.21338 | val_0_rmse: 0.56108 | val_1_rmse: 0.57198 |  0:07:18s
epoch 86 | loss: 0.21507 | val_0_rmse: 0.5046  | val_1_rmse: 0.51055 |  0:07:23s
epoch 87 | loss: 0.21391 | val_0_rmse: 0.54878 | val_1_rmse: 0.56016 |  0:07:29s
epoch 88 | loss: 0.21013 | val_0_rmse: 0.52576 | val_1_rmse: 0.53728 |  0:07:34s
epoch 89 | loss: 0.20735 | val_0_rmse: 0.52061 | val_1_rmse: 0.53243 |  0:07:39s
epoch 90 | loss: 0.20969 | val_0_rmse: 0.55306 | val_1_rmse: 0.56397 |  0:07:44s
epoch 91 | loss: 0.20681 | val_0_rmse: 0.5457  | val_1_rmse: 0.55669 |  0:07:49s
epoch 92 | loss: 0.20295 | val_0_rmse: 0.5578  | val_1_rmse: 0.56952 |  0:07:55s
epoch 93 | loss: 0.20875 | val_0_rmse: 0.54928 | val_1_rmse: 0.5603  |  0:08:00s
epoch 94 | loss: 0.21112 | val_0_rmse: 0.51516 | val_1_rmse: 0.52699 |  0:08:05s
epoch 95 | loss: 0.20713 | val_0_rmse: 0.56079 | val_1_rmse: 0.57135 |  0:08:10s
epoch 96 | loss: 0.21031 | val_0_rmse: 0.60857 | val_1_rmse: 0.6182  |  0:08:15s
epoch 97 | loss: 0.20646 | val_0_rmse: 0.58836 | val_1_rmse: 0.59801 |  0:08:20s
epoch 98 | loss: 0.23132 | val_0_rmse: 0.56245 | val_1_rmse: 0.57232 |  0:08:25s
epoch 99 | loss: 0.22364 | val_0_rmse: 0.5357  | val_1_rmse: 0.54439 |  0:08:30s
epoch 100| loss: 0.27137 | val_0_rmse: 0.55421 | val_1_rmse: 0.55818 |  0:08:36s
epoch 101| loss: 0.26149 | val_0_rmse: 0.5163  | val_1_rmse: 0.52236 |  0:08:41s
epoch 102| loss: 0.26354 | val_0_rmse: 0.64142 | val_1_rmse: 0.64961 |  0:08:46s
epoch 103| loss: 0.24116 | val_0_rmse: 0.59254 | val_1_rmse: 0.59933 |  0:08:51s
epoch 104| loss: 0.24204 | val_0_rmse: 0.53259 | val_1_rmse: 0.53833 |  0:08:56s
epoch 105| loss: 0.22963 | val_0_rmse: 0.52994 | val_1_rmse: 0.53635 |  0:09:01s
epoch 106| loss: 0.23077 | val_0_rmse: 0.60753 | val_1_rmse: 0.61109 |  0:09:06s
epoch 107| loss: 0.23738 | val_0_rmse: 0.58332 | val_1_rmse: 0.59351 |  0:09:12s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.45878
Best weights from best epoch are automatically used!
ended training at: 01:00:28
Feature importance:
[('Area', 0.2660562965076225), ('Baths', 0.038759570823185624), ('Beds', 0.0), ('Latitude', 0.2581767114778901), ('Longitude', 0.43226642643047575), ('Month', 0.0033573604996964203), ('Year', 0.0013836342611295426)]
Mean squared error is of 1771024582.9641974
Mean absolute error:28035.939837541962
MAPE:0.25619265513428285
R2 score:0.7884256757326621
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:02:51
epoch 0  | loss: 0.40412 | val_0_rmse: 0.5751  | val_1_rmse: 0.57553 |  0:00:04s
epoch 1  | loss: 0.33259 | val_0_rmse: 0.5678  | val_1_rmse: 0.56775 |  0:00:09s
epoch 2  | loss: 0.32793 | val_0_rmse: 0.5691  | val_1_rmse: 0.56986 |  0:00:14s
epoch 3  | loss: 0.32586 | val_0_rmse: 0.57684 | val_1_rmse: 0.57588 |  0:00:19s
epoch 4  | loss: 0.32602 | val_0_rmse: 0.56461 | val_1_rmse: 0.56503 |  0:00:25s
epoch 5  | loss: 0.32347 | val_0_rmse: 0.57337 | val_1_rmse: 0.57153 |  0:00:30s
epoch 6  | loss: 0.32312 | val_0_rmse: 0.57317 | val_1_rmse: 0.57478 |  0:00:35s
epoch 7  | loss: 0.32193 | val_0_rmse: 0.56659 | val_1_rmse: 0.56719 |  0:00:40s
epoch 8  | loss: 0.31895 | val_0_rmse: 0.56191 | val_1_rmse: 0.5622  |  0:00:45s
epoch 9  | loss: 0.32103 | val_0_rmse: 0.56358 | val_1_rmse: 0.5641  |  0:00:50s
epoch 10 | loss: 0.31899 | val_0_rmse: 0.57517 | val_1_rmse: 0.57565 |  0:00:55s
epoch 11 | loss: 0.3155  | val_0_rmse: 0.59584 | val_1_rmse: 0.59656 |  0:01:00s
epoch 12 | loss: 0.31766 | val_0_rmse: 0.56252 | val_1_rmse: 0.56455 |  0:01:05s
epoch 13 | loss: 0.3063  | val_0_rmse: 0.58344 | val_1_rmse: 0.58435 |  0:01:10s
epoch 14 | loss: 0.29858 | val_0_rmse: 0.60075 | val_1_rmse: 0.60219 |  0:01:15s
epoch 15 | loss: 0.29667 | val_0_rmse: 0.59642 | val_1_rmse: 0.59803 |  0:01:20s
epoch 16 | loss: 0.30103 | val_0_rmse: 0.92167 | val_1_rmse: 0.92349 |  0:01:25s
epoch 17 | loss: 0.29803 | val_0_rmse: 0.61544 | val_1_rmse: 0.6171  |  0:01:30s
epoch 18 | loss: 0.30155 | val_0_rmse: 0.58015 | val_1_rmse: 0.58141 |  0:01:35s
epoch 19 | loss: 0.29661 | val_0_rmse: 0.53758 | val_1_rmse: 0.53779 |  0:01:41s
epoch 20 | loss: 0.29476 | val_0_rmse: 0.58429 | val_1_rmse: 0.58585 |  0:01:46s
epoch 21 | loss: 0.29276 | val_0_rmse: 0.55976 | val_1_rmse: 0.56005 |  0:01:51s
epoch 22 | loss: 0.29477 | val_0_rmse: 0.54406 | val_1_rmse: 0.54479 |  0:01:56s
epoch 23 | loss: 0.29322 | val_0_rmse: 0.54386 | val_1_rmse: 0.54464 |  0:02:01s
epoch 24 | loss: 0.29325 | val_0_rmse: 0.56669 | val_1_rmse: 0.56841 |  0:02:06s
epoch 25 | loss: 0.29534 | val_0_rmse: 0.58219 | val_1_rmse: 0.58344 |  0:02:11s
epoch 26 | loss: 0.29559 | val_0_rmse: 0.65254 | val_1_rmse: 0.65277 |  0:02:16s
epoch 27 | loss: 0.29166 | val_0_rmse: 0.60854 | val_1_rmse: 0.60856 |  0:02:21s
epoch 28 | loss: 0.29393 | val_0_rmse: 0.58691 | val_1_rmse: 0.58842 |  0:02:26s
epoch 29 | loss: 0.28989 | val_0_rmse: 0.56476 | val_1_rmse: 0.56638 |  0:02:31s
epoch 30 | loss: 0.2911  | val_0_rmse: 0.54957 | val_1_rmse: 0.54848 |  0:02:36s
epoch 31 | loss: 0.29017 | val_0_rmse: 0.53753 | val_1_rmse: 0.53879 |  0:02:41s
epoch 32 | loss: 0.2893  | val_0_rmse: 0.57427 | val_1_rmse: 0.57592 |  0:02:46s
epoch 33 | loss: 0.28972 | val_0_rmse: 0.72199 | val_1_rmse: 0.72316 |  0:02:52s
epoch 34 | loss: 0.28682 | val_0_rmse: 0.5521  | val_1_rmse: 0.55279 |  0:02:57s
epoch 35 | loss: 0.2894  | val_0_rmse: 0.67564 | val_1_rmse: 0.67627 |  0:03:02s
epoch 36 | loss: 0.28744 | val_0_rmse: 0.53862 | val_1_rmse: 0.53886 |  0:03:07s
epoch 37 | loss: 0.28585 | val_0_rmse: 0.52885 | val_1_rmse: 0.53058 |  0:03:12s
epoch 38 | loss: 0.28425 | val_0_rmse: 0.6583  | val_1_rmse: 0.66013 |  0:03:17s
epoch 39 | loss: 0.28255 | val_0_rmse: 0.52825 | val_1_rmse: 0.52955 |  0:03:22s
epoch 40 | loss: 0.28408 | val_0_rmse: 0.55465 | val_1_rmse: 0.55484 |  0:03:27s
epoch 41 | loss: 0.28267 | val_0_rmse: 0.53251 | val_1_rmse: 0.53295 |  0:03:33s
epoch 42 | loss: 0.28334 | val_0_rmse: 0.56099 | val_1_rmse: 0.56094 |  0:03:38s
epoch 43 | loss: 0.28621 | val_0_rmse: 0.54164 | val_1_rmse: 0.54251 |  0:03:43s
epoch 44 | loss: 0.28361 | val_0_rmse: 0.57982 | val_1_rmse: 0.58105 |  0:03:48s
epoch 45 | loss: 0.28392 | val_0_rmse: 0.53193 | val_1_rmse: 0.53385 |  0:03:53s
epoch 46 | loss: 0.28284 | val_0_rmse: 0.54343 | val_1_rmse: 0.54399 |  0:03:58s
epoch 47 | loss: 0.28126 | val_0_rmse: 0.53671 | val_1_rmse: 0.53744 |  0:04:03s
epoch 48 | loss: 0.28159 | val_0_rmse: 0.55883 | val_1_rmse: 0.5596  |  0:04:08s
epoch 49 | loss: 0.28206 | val_0_rmse: 0.53048 | val_1_rmse: 0.53117 |  0:04:13s
epoch 50 | loss: 0.28551 | val_0_rmse: 0.65319 | val_1_rmse: 0.65524 |  0:04:18s
epoch 51 | loss: 0.28299 | val_0_rmse: 0.52413 | val_1_rmse: 0.52612 |  0:04:23s
epoch 52 | loss: 0.28095 | val_0_rmse: 0.52506 | val_1_rmse: 0.52522 |  0:04:28s
epoch 53 | loss: 0.28036 | val_0_rmse: 0.54107 | val_1_rmse: 0.54279 |  0:04:33s
epoch 54 | loss: 0.28505 | val_0_rmse: 0.53141 | val_1_rmse: 0.53228 |  0:04:38s
epoch 55 | loss: 0.2816  | val_0_rmse: 0.55356 | val_1_rmse: 0.55442 |  0:04:43s
epoch 56 | loss: 0.28371 | val_0_rmse: 0.58266 | val_1_rmse: 0.58334 |  0:04:48s
epoch 57 | loss: 0.2801  | val_0_rmse: 0.53257 | val_1_rmse: 0.53352 |  0:04:53s
epoch 58 | loss: 0.28741 | val_0_rmse: 0.65126 | val_1_rmse: 0.651   |  0:04:58s
epoch 59 | loss: 0.28444 | val_0_rmse: 0.536   | val_1_rmse: 0.5382  |  0:05:04s
epoch 60 | loss: 0.28893 | val_0_rmse: 0.55846 | val_1_rmse: 0.55849 |  0:05:09s
epoch 61 | loss: 0.28402 | val_0_rmse: 0.5767  | val_1_rmse: 0.5785  |  0:05:14s
epoch 62 | loss: 0.28279 | val_0_rmse: 0.61227 | val_1_rmse: 0.6163  |  0:05:19s
epoch 63 | loss: 0.28404 | val_0_rmse: 0.72449 | val_1_rmse: 0.72761 |  0:05:24s
epoch 64 | loss: 0.28386 | val_0_rmse: 0.57061 | val_1_rmse: 0.57253 |  0:05:29s
epoch 65 | loss: 0.28556 | val_0_rmse: 0.66994 | val_1_rmse: 0.67373 |  0:05:34s
epoch 66 | loss: 0.28053 | val_0_rmse: 0.52424 | val_1_rmse: 0.52503 |  0:05:40s
epoch 67 | loss: 0.27749 | val_0_rmse: 0.61707 | val_1_rmse: 0.62037 |  0:05:45s
epoch 68 | loss: 0.27934 | val_0_rmse: 0.5437  | val_1_rmse: 0.54467 |  0:05:50s
epoch 69 | loss: 0.27768 | val_0_rmse: 0.70787 | val_1_rmse: 0.71221 |  0:05:55s
epoch 70 | loss: 0.27881 | val_0_rmse: 0.64191 | val_1_rmse: 0.64447 |  0:06:00s
epoch 71 | loss: 0.27884 | val_0_rmse: 0.53069 | val_1_rmse: 0.53391 |  0:06:05s
epoch 72 | loss: 0.27716 | val_0_rmse: 0.66415 | val_1_rmse: 0.6689  |  0:06:11s
epoch 73 | loss: 0.27796 | val_0_rmse: 0.52192 | val_1_rmse: 0.52523 |  0:06:16s
epoch 74 | loss: 0.27408 | val_0_rmse: 0.6204  | val_1_rmse: 0.62434 |  0:06:21s
epoch 75 | loss: 0.27504 | val_0_rmse: 0.73591 | val_1_rmse: 0.73784 |  0:06:26s
epoch 76 | loss: 0.27406 | val_0_rmse: 0.55456 | val_1_rmse: 0.55641 |  0:06:31s
epoch 77 | loss: 0.27476 | val_0_rmse: 0.62258 | val_1_rmse: 0.62495 |  0:06:36s
epoch 78 | loss: 0.2774  | val_0_rmse: 0.9379  | val_1_rmse: 0.94004 |  0:06:41s
epoch 79 | loss: 0.27818 | val_0_rmse: 0.67715 | val_1_rmse: 0.68217 |  0:06:46s
epoch 80 | loss: 0.27558 | val_0_rmse: 0.56871 | val_1_rmse: 0.57193 |  0:06:51s
epoch 81 | loss: 0.27381 | val_0_rmse: 0.71565 | val_1_rmse: 0.72003 |  0:06:57s
epoch 82 | loss: 0.2725  | val_0_rmse: 0.64224 | val_1_rmse: 0.64534 |  0:07:02s
epoch 83 | loss: 0.27479 | val_0_rmse: 0.55142 | val_1_rmse: 0.55455 |  0:07:07s
epoch 84 | loss: 0.27231 | val_0_rmse: 0.54593 | val_1_rmse: 0.5484  |  0:07:12s
epoch 85 | loss: 0.27511 | val_0_rmse: 0.66687 | val_1_rmse: 0.66997 |  0:07:17s
epoch 86 | loss: 0.27152 | val_0_rmse: 0.52172 | val_1_rmse: 0.5251  |  0:07:22s
epoch 87 | loss: 0.27149 | val_0_rmse: 0.58965 | val_1_rmse: 0.59218 |  0:07:27s
epoch 88 | loss: 0.27123 | val_0_rmse: 0.65662 | val_1_rmse: 0.66002 |  0:07:32s
epoch 89 | loss: 0.27314 | val_0_rmse: 0.51503 | val_1_rmse: 0.51952 |  0:07:37s
epoch 90 | loss: 0.27031 | val_0_rmse: 0.54374 | val_1_rmse: 0.54622 |  0:07:42s
epoch 91 | loss: 0.26989 | val_0_rmse: 0.53773 | val_1_rmse: 0.53976 |  0:07:48s
epoch 92 | loss: 0.2694  | val_0_rmse: 0.5215  | val_1_rmse: 0.52504 |  0:07:53s
epoch 93 | loss: 0.27066 | val_0_rmse: 0.51207 | val_1_rmse: 0.51368 |  0:07:58s
epoch 94 | loss: 0.27283 | val_0_rmse: 0.54096 | val_1_rmse: 0.54376 |  0:08:03s
epoch 95 | loss: 0.26871 | val_0_rmse: 0.56454 | val_1_rmse: 0.56504 |  0:08:08s
epoch 96 | loss: 0.26669 | val_0_rmse: 0.60926 | val_1_rmse: 0.61255 |  0:08:13s
epoch 97 | loss: 0.26313 | val_0_rmse: 0.54861 | val_1_rmse: 0.55031 |  0:08:18s
epoch 98 | loss: 0.26557 | val_0_rmse: 0.56934 | val_1_rmse: 0.57178 |  0:08:23s
epoch 99 | loss: 0.27063 | val_0_rmse: 0.54075 | val_1_rmse: 0.54249 |  0:08:28s
epoch 100| loss: 0.2622  | val_0_rmse: 0.51219 | val_1_rmse: 0.51402 |  0:08:33s
epoch 101| loss: 0.27313 | val_0_rmse: 0.56106 | val_1_rmse: 0.55938 |  0:08:38s
epoch 102| loss: 0.27225 | val_0_rmse: 0.6654  | val_1_rmse: 0.66766 |  0:08:44s
epoch 103| loss: 0.28324 | val_0_rmse: 0.59692 | val_1_rmse: 0.59839 |  0:08:49s
epoch 104| loss: 0.28048 | val_0_rmse: 0.54005 | val_1_rmse: 0.54176 |  0:08:54s
epoch 105| loss: 0.27723 | val_0_rmse: 0.58181 | val_1_rmse: 0.58452 |  0:08:59s
epoch 106| loss: 0.27455 | val_0_rmse: 0.5185  | val_1_rmse: 0.52301 |  0:09:04s
epoch 107| loss: 0.27422 | val_0_rmse: 0.51304 | val_1_rmse: 0.51616 |  0:09:09s
epoch 108| loss: 0.27289 | val_0_rmse: 0.51622 | val_1_rmse: 0.5208  |  0:09:14s
epoch 109| loss: 0.27301 | val_0_rmse: 0.53208 | val_1_rmse: 0.53581 |  0:09:19s
epoch 110| loss: 0.2688  | val_0_rmse: 0.53126 | val_1_rmse: 0.53514 |  0:09:24s
epoch 111| loss: 0.26818 | val_0_rmse: 0.57842 | val_1_rmse: 0.58195 |  0:09:29s
epoch 112| loss: 0.2715  | val_0_rmse: 0.61542 | val_1_rmse: 0.61985 |  0:09:35s
epoch 113| loss: 0.26973 | val_0_rmse: 0.54763 | val_1_rmse: 0.54948 |  0:09:40s
epoch 114| loss: 0.26977 | val_0_rmse: 0.52338 | val_1_rmse: 0.52628 |  0:09:45s
epoch 115| loss: 0.26781 | val_0_rmse: 0.55454 | val_1_rmse: 0.55844 |  0:09:50s
epoch 116| loss: 0.271   | val_0_rmse: 0.52835 | val_1_rmse: 0.53253 |  0:09:55s
epoch 117| loss: 0.26732 | val_0_rmse: 0.51866 | val_1_rmse: 0.5223  |  0:10:00s
epoch 118| loss: 0.26633 | val_0_rmse: 0.52421 | val_1_rmse: 0.52645 |  0:10:05s
epoch 119| loss: 0.26709 | val_0_rmse: 0.65298 | val_1_rmse: 0.65717 |  0:10:10s
epoch 120| loss: 0.26836 | val_0_rmse: 0.68956 | val_1_rmse: 0.69392 |  0:10:15s
epoch 121| loss: 0.26942 | val_0_rmse: 0.50889 | val_1_rmse: 0.51204 |  0:10:21s
epoch 122| loss: 0.26858 | val_0_rmse: 0.61777 | val_1_rmse: 0.62233 |  0:10:26s
epoch 123| loss: 0.26901 | val_0_rmse: 0.62501 | val_1_rmse: 0.62745 |  0:10:31s
epoch 124| loss: 0.26598 | val_0_rmse: 0.51862 | val_1_rmse: 0.52314 |  0:10:36s
epoch 125| loss: 0.26384 | val_0_rmse: 0.53813 | val_1_rmse: 0.54021 |  0:10:41s
epoch 126| loss: 0.26382 | val_0_rmse: 0.51556 | val_1_rmse: 0.51936 |  0:10:46s
epoch 127| loss: 0.26497 | val_0_rmse: 0.51777 | val_1_rmse: 0.5122  |  0:10:51s
epoch 128| loss: 0.26195 | val_0_rmse: 0.59058 | val_1_rmse: 0.58526 |  0:10:56s
epoch 129| loss: 0.26111 | val_0_rmse: 0.57855 | val_1_rmse: 0.58203 |  0:11:01s
epoch 130| loss: 0.26244 | val_0_rmse: 0.60575 | val_1_rmse: 0.59913 |  0:11:06s
epoch 131| loss: 0.26014 | val_0_rmse: 0.61486 | val_1_rmse: 0.61898 |  0:11:11s
epoch 132| loss: 0.25991 | val_0_rmse: 0.55656 | val_1_rmse: 0.56011 |  0:11:16s
epoch 133| loss: 0.25713 | val_0_rmse: 0.57003 | val_1_rmse: 0.57338 |  0:11:21s
epoch 134| loss: 0.25523 | val_0_rmse: 0.49645 | val_1_rmse: 0.50025 |  0:11:26s
epoch 135| loss: 0.2564  | val_0_rmse: 0.7281  | val_1_rmse: 0.70897 |  0:11:31s
epoch 136| loss: 0.25612 | val_0_rmse: 0.53752 | val_1_rmse: 0.50489 |  0:11:36s
epoch 137| loss: 0.25469 | val_0_rmse: 0.6141  | val_1_rmse: 0.60909 |  0:11:41s
epoch 138| loss: 0.25724 | val_0_rmse: 0.61888 | val_1_rmse: 0.59947 |  0:11:47s
epoch 139| loss: 0.25613 | val_0_rmse: 0.60428 | val_1_rmse: 0.5604  |  0:11:52s
epoch 140| loss: 0.26786 | val_0_rmse: 0.54725 | val_1_rmse: 0.54964 |  0:11:57s
epoch 141| loss: 0.25744 | val_0_rmse: 0.60156 | val_1_rmse: 0.60871 |  0:12:02s
epoch 142| loss: 0.25377 | val_0_rmse: 0.51053 | val_1_rmse: 0.51065 |  0:12:07s
epoch 143| loss: 0.25735 | val_0_rmse: 0.55812 | val_1_rmse: 0.55792 |  0:12:12s
epoch 144| loss: 0.25245 | val_0_rmse: 0.55123 | val_1_rmse: 0.55041 |  0:12:17s
epoch 145| loss: 0.25554 | val_0_rmse: 0.53744 | val_1_rmse: 0.53951 |  0:12:22s
epoch 146| loss: 0.25411 | val_0_rmse: 0.55591 | val_1_rmse: 0.56035 |  0:12:27s
epoch 147| loss: 0.25281 | val_0_rmse: 0.59925 | val_1_rmse: 0.60172 |  0:12:32s
epoch 148| loss: 0.25816 | val_0_rmse: 0.53175 | val_1_rmse: 0.52101 |  0:12:37s
epoch 149| loss: 0.25341 | val_0_rmse: 0.57377 | val_1_rmse: 0.5777  |  0:12:43s
Stop training because you reached max_epochs = 150 with best_epoch = 134 and best_val_1_rmse = 0.50025
Best weights from best epoch are automatically used!
ended training at: 01:15:35
Feature importance:
[('Area', 0.4838545322774837), ('Baths', 0.0), ('Beds', 0.241324492699712), ('Latitude', 0.1865027864867139), ('Longitude', 0.024873529346348455), ('Month', 0.0), ('Year', 0.06344465918974194)]
Mean squared error is of 836881549.783368
Mean absolute error:19830.65966454577
MAPE:0.3138915285951034
R2 score:0.7496310479223998
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:15:36
epoch 0  | loss: 0.41604 | val_0_rmse: 0.59941 | val_1_rmse: 0.59547 |  0:00:05s
epoch 1  | loss: 0.34321 | val_0_rmse: 0.57065 | val_1_rmse: 0.5684  |  0:00:10s
epoch 2  | loss: 0.33574 | val_0_rmse: 0.57207 | val_1_rmse: 0.56964 |  0:00:15s
epoch 3  | loss: 0.33288 | val_0_rmse: 0.56632 | val_1_rmse: 0.56403 |  0:00:20s
epoch 4  | loss: 0.32953 | val_0_rmse: 0.57141 | val_1_rmse: 0.56856 |  0:00:25s
epoch 5  | loss: 0.33053 | val_0_rmse: 0.5761  | val_1_rmse: 0.57459 |  0:00:30s
epoch 6  | loss: 0.32895 | val_0_rmse: 0.56603 | val_1_rmse: 0.56235 |  0:00:35s
epoch 7  | loss: 0.32683 | val_0_rmse: 0.56296 | val_1_rmse: 0.56006 |  0:00:41s
epoch 8  | loss: 0.32063 | val_0_rmse: 0.56105 | val_1_rmse: 0.55966 |  0:00:46s
epoch 9  | loss: 0.32278 | val_0_rmse: 0.56139 | val_1_rmse: 0.5609  |  0:00:51s
epoch 10 | loss: 0.32078 | val_0_rmse: 0.56112 | val_1_rmse: 0.559   |  0:00:56s
epoch 11 | loss: 0.32028 | val_0_rmse: 0.56964 | val_1_rmse: 0.56846 |  0:01:01s
epoch 12 | loss: 0.32219 | val_0_rmse: 0.56091 | val_1_rmse: 0.55967 |  0:01:06s
epoch 13 | loss: 0.31872 | val_0_rmse: 0.5607  | val_1_rmse: 0.55923 |  0:01:11s
epoch 14 | loss: 0.31655 | val_0_rmse: 0.56054 | val_1_rmse: 0.56002 |  0:01:16s
epoch 15 | loss: 0.31685 | val_0_rmse: 0.55636 | val_1_rmse: 0.55499 |  0:01:21s
epoch 16 | loss: 0.31559 | val_0_rmse: 0.57032 | val_1_rmse: 0.56982 |  0:01:26s
epoch 17 | loss: 0.31723 | val_0_rmse: 0.5638  | val_1_rmse: 0.56107 |  0:01:31s
epoch 18 | loss: 0.31795 | val_0_rmse: 0.55938 | val_1_rmse: 0.55703 |  0:01:36s
epoch 19 | loss: 0.31426 | val_0_rmse: 0.56748 | val_1_rmse: 0.56426 |  0:01:42s
epoch 20 | loss: 0.31478 | val_0_rmse: 0.55757 | val_1_rmse: 0.55549 |  0:01:47s
epoch 21 | loss: 0.3135  | val_0_rmse: 0.62804 | val_1_rmse: 0.62409 |  0:01:52s
epoch 22 | loss: 0.30987 | val_0_rmse: 0.58032 | val_1_rmse: 0.58231 |  0:01:57s
epoch 23 | loss: 0.30517 | val_0_rmse: 0.55536 | val_1_rmse: 0.55378 |  0:02:02s
epoch 24 | loss: 0.30247 | val_0_rmse: 0.59654 | val_1_rmse: 0.597   |  0:02:07s
epoch 25 | loss: 0.29346 | val_0_rmse: 0.53632 | val_1_rmse: 0.53634 |  0:02:13s
epoch 26 | loss: 0.28935 | val_0_rmse: 0.57054 | val_1_rmse: 0.57211 |  0:02:18s
epoch 27 | loss: 0.28695 | val_0_rmse: 0.59132 | val_1_rmse: 0.58995 |  0:02:23s
epoch 28 | loss: 0.28325 | val_0_rmse: 0.59138 | val_1_rmse: 0.5887  |  0:02:28s
epoch 29 | loss: 0.28687 | val_0_rmse: 0.61992 | val_1_rmse: 0.62575 |  0:02:33s
epoch 30 | loss: 0.28419 | val_0_rmse: 0.58971 | val_1_rmse: 0.58922 |  0:02:38s
epoch 31 | loss: 0.28197 | val_0_rmse: 0.598   | val_1_rmse: 0.59539 |  0:02:43s
epoch 32 | loss: 0.28048 | val_0_rmse: 0.64961 | val_1_rmse: 0.64651 |  0:02:48s
epoch 33 | loss: 0.28305 | val_0_rmse: 0.95383 | val_1_rmse: 0.94965 |  0:02:53s
epoch 34 | loss: 0.28694 | val_0_rmse: 0.58    | val_1_rmse: 0.58036 |  0:02:58s
epoch 35 | loss: 0.28724 | val_0_rmse: 0.59943 | val_1_rmse: 0.5999  |  0:03:03s
epoch 36 | loss: 0.2854  | val_0_rmse: 0.55114 | val_1_rmse: 0.54933 |  0:03:08s
epoch 37 | loss: 0.28279 | val_0_rmse: 0.60291 | val_1_rmse: 0.60259 |  0:03:13s
epoch 38 | loss: 0.27977 | val_0_rmse: 0.60709 | val_1_rmse: 0.61439 |  0:03:19s
epoch 39 | loss: 0.28044 | val_0_rmse: 0.5886  | val_1_rmse: 0.58871 |  0:03:24s
epoch 40 | loss: 0.27785 | val_0_rmse: 0.52391 | val_1_rmse: 0.52424 |  0:03:29s
epoch 41 | loss: 0.27829 | val_0_rmse: 0.5629  | val_1_rmse: 0.56219 |  0:03:34s
epoch 42 | loss: 0.27849 | val_0_rmse: 0.51907 | val_1_rmse: 0.52111 |  0:03:40s
epoch 43 | loss: 0.27428 | val_0_rmse: 0.56761 | val_1_rmse: 0.5671  |  0:03:45s
epoch 44 | loss: 0.27413 | val_0_rmse: 0.59706 | val_1_rmse: 0.59524 |  0:03:50s
epoch 45 | loss: 0.27339 | val_0_rmse: 0.59489 | val_1_rmse: 0.59509 |  0:03:55s
epoch 46 | loss: 0.27166 | val_0_rmse: 0.56984 | val_1_rmse: 0.56978 |  0:04:00s
epoch 47 | loss: 0.27121 | val_0_rmse: 0.58978 | val_1_rmse: 0.58881 |  0:04:05s
epoch 48 | loss: 0.26732 | val_0_rmse: 0.65041 | val_1_rmse: 0.64743 |  0:04:10s
epoch 49 | loss: 0.2605  | val_0_rmse: 0.6201  | val_1_rmse: 0.62004 |  0:04:16s
epoch 50 | loss: 0.26415 | val_0_rmse: 0.64799 | val_1_rmse: 0.64697 |  0:04:21s
epoch 51 | loss: 0.26132 | val_0_rmse: 0.58833 | val_1_rmse: 0.59073 |  0:04:26s
epoch 52 | loss: 0.25561 | val_0_rmse: 0.6016  | val_1_rmse: 0.60414 |  0:04:31s
epoch 53 | loss: 0.25821 | val_0_rmse: 0.64222 | val_1_rmse: 0.64224 |  0:04:36s
epoch 54 | loss: 0.25535 | val_0_rmse: 0.60728 | val_1_rmse: 0.61322 |  0:04:41s
epoch 55 | loss: 0.25504 | val_0_rmse: 0.61198 | val_1_rmse: 0.61235 |  0:04:46s
epoch 56 | loss: 0.25288 | val_0_rmse: 0.59392 | val_1_rmse: 0.59324 |  0:04:51s
epoch 57 | loss: 0.25457 | val_0_rmse: 0.61645 | val_1_rmse: 0.62048 |  0:04:56s
epoch 58 | loss: 0.2532  | val_0_rmse: 0.5887  | val_1_rmse: 0.58917 |  0:05:01s
epoch 59 | loss: 0.25331 | val_0_rmse: 0.61953 | val_1_rmse: 0.61973 |  0:05:06s
epoch 60 | loss: 0.25447 | val_0_rmse: 0.61326 | val_1_rmse: 0.61407 |  0:05:11s
epoch 61 | loss: 0.25082 | val_0_rmse: 0.57863 | val_1_rmse: 0.58306 |  0:05:17s
epoch 62 | loss: 0.24953 | val_0_rmse: 0.64824 | val_1_rmse: 0.64656 |  0:05:22s
epoch 63 | loss: 0.25174 | val_0_rmse: 0.57271 | val_1_rmse: 0.57383 |  0:05:27s
epoch 64 | loss: 0.25276 | val_0_rmse: 0.58769 | val_1_rmse: 0.58868 |  0:05:32s
epoch 65 | loss: 0.24579 | val_0_rmse: 0.63686 | val_1_rmse: 0.63626 |  0:05:37s
epoch 66 | loss: 0.2495  | val_0_rmse: 0.57182 | val_1_rmse: 0.57384 |  0:05:42s
epoch 67 | loss: 0.2475  | val_0_rmse: 0.59933 | val_1_rmse: 0.60268 |  0:05:47s
epoch 68 | loss: 0.24431 | val_0_rmse: 0.53761 | val_1_rmse: 0.5369  |  0:05:52s
epoch 69 | loss: 0.24352 | val_0_rmse: 0.58497 | val_1_rmse: 0.58704 |  0:05:58s
epoch 70 | loss: 0.24236 | val_0_rmse: 0.63581 | val_1_rmse: 0.6347  |  0:06:03s
epoch 71 | loss: 0.23997 | val_0_rmse: 0.5404  | val_1_rmse: 0.54083 |  0:06:08s
epoch 72 | loss: 0.24064 | val_0_rmse: 0.62874 | val_1_rmse: 0.62954 |  0:06:13s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.52111
Best weights from best epoch are automatically used!
ended training at: 01:21:52
Feature importance:
[('Area', 0.577294314554922), ('Baths', 0.023543379921722157), ('Beds', 0.14120823601340174), ('Latitude', 0.06658853917006254), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.1913655303398916)]
Mean squared error is of 874151665.1320878
Mean absolute error:20514.332010798887
MAPE:0.33858403192145864
R2 score:0.7360920993101949
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:21:53
epoch 0  | loss: 0.33253 | val_0_rmse: 0.55286 | val_1_rmse: 0.55363 |  0:00:45s
epoch 1  | loss: 0.30345 | val_0_rmse: 0.54413 | val_1_rmse: 0.54478 |  0:01:30s
epoch 2  | loss: 0.30222 | val_0_rmse: 0.54376 | val_1_rmse: 0.54484 |  0:02:15s
epoch 3  | loss: 0.29841 | val_0_rmse: 0.53901 | val_1_rmse: 0.54053 |  0:03:01s
epoch 4  | loss: 0.29693 | val_0_rmse: 0.53831 | val_1_rmse: 0.53957 |  0:03:46s
epoch 5  | loss: 0.29947 | val_0_rmse: 0.54083 | val_1_rmse: 0.54196 |  0:04:31s
epoch 6  | loss: 0.29637 | val_0_rmse: 0.55346 | val_1_rmse: 0.55574 |  0:05:17s
epoch 7  | loss: 0.29503 | val_0_rmse: 0.54095 | val_1_rmse: 0.5421  |  0:06:02s
epoch 8  | loss: 0.29996 | val_0_rmse: 0.65864 | val_1_rmse: 0.66031 |  0:06:47s
epoch 9  | loss: 0.30333 | val_0_rmse: 0.5554  | val_1_rmse: 0.55726 |  0:07:32s
epoch 10 | loss: 0.30612 | val_0_rmse: 0.68948 | val_1_rmse: 0.69185 |  0:08:18s
epoch 11 | loss: 0.29711 | val_0_rmse: 0.72785 | val_1_rmse: 0.72956 |  0:09:03s
epoch 12 | loss: 0.29624 | val_0_rmse: 0.53445 | val_1_rmse: 0.53632 |  0:09:48s
epoch 13 | loss: 0.29201 | val_0_rmse: 0.61336 | val_1_rmse: 0.6151  |  0:10:34s
epoch 14 | loss: 0.285   | val_0_rmse: 0.73541 | val_1_rmse: 0.73706 |  0:11:20s
epoch 15 | loss: 0.32367 | val_0_rmse: 0.55531 | val_1_rmse: 0.55555 |  0:12:05s
epoch 16 | loss: 0.31659 | val_0_rmse: 0.56625 | val_1_rmse: 0.56838 |  0:12:50s
epoch 17 | loss: 0.30518 | val_0_rmse: 0.54039 | val_1_rmse: 0.54155 |  0:13:36s
epoch 18 | loss: 0.29716 | val_0_rmse: 0.54486 | val_1_rmse: 0.54701 |  0:14:22s
epoch 19 | loss: 0.29541 | val_0_rmse: 0.54133 | val_1_rmse: 0.54278 |  0:15:07s
epoch 20 | loss: 0.29375 | val_0_rmse: 0.53608 | val_1_rmse: 0.53728 |  0:15:53s
epoch 21 | loss: 0.29112 | val_0_rmse: 0.53504 | val_1_rmse: 0.53624 |  0:16:39s
epoch 22 | loss: 0.28843 | val_0_rmse: 0.53783 | val_1_rmse: 0.53857 |  0:17:24s
epoch 23 | loss: 0.28682 | val_0_rmse: 0.59861 | val_1_rmse: 0.60059 |  0:18:09s
epoch 24 | loss: 0.27925 | val_0_rmse: 0.62497 | val_1_rmse: 0.62741 |  0:18:55s
epoch 25 | loss: 0.27621 | val_0_rmse: 0.64702 | val_1_rmse: 0.64883 |  0:19:40s
epoch 26 | loss: 0.27482 | val_0_rmse: 0.65155 | val_1_rmse: 0.65359 |  0:20:25s
epoch 27 | loss: 0.27224 | val_0_rmse: 0.68809 | val_1_rmse: 0.69011 |  0:21:11s
epoch 28 | loss: 0.27259 | val_0_rmse: 0.64661 | val_1_rmse: 0.6485  |  0:21:56s
epoch 29 | loss: 0.28657 | val_0_rmse: 0.54128 | val_1_rmse: 0.5429  |  0:22:41s
epoch 30 | loss: 0.29368 | val_0_rmse: 0.54165 | val_1_rmse: 0.54339 |  0:23:27s
epoch 31 | loss: 0.29567 | val_0_rmse: 0.54585 | val_1_rmse: 0.54683 |  0:24:12s
epoch 32 | loss: 0.29418 | val_0_rmse: 0.53712 | val_1_rmse: 0.53838 |  0:24:58s
epoch 33 | loss: 0.29335 | val_0_rmse: 0.53567 | val_1_rmse: 0.53743 |  0:25:44s
epoch 34 | loss: 0.29259 | val_0_rmse: 0.53436 | val_1_rmse: 0.53549 |  0:26:29s
epoch 35 | loss: 0.29474 | val_0_rmse: 0.69132 | val_1_rmse: 0.69125 |  0:27:14s
epoch 36 | loss: 0.30026 | val_0_rmse: 0.55207 | val_1_rmse: 0.55418 |  0:28:00s
epoch 37 | loss: 0.30065 | val_0_rmse: 0.54562 | val_1_rmse: 0.54731 |  0:28:45s
epoch 38 | loss: 0.29794 | val_0_rmse: 0.53668 | val_1_rmse: 0.53843 |  0:29:30s
epoch 39 | loss: 0.29351 | val_0_rmse: 0.53744 | val_1_rmse: 0.53866 |  0:30:15s
epoch 40 | loss: 0.29262 | val_0_rmse: 0.59072 | val_1_rmse: 0.59308 |  0:31:00s
epoch 41 | loss: 0.27545 | val_0_rmse: 0.63236 | val_1_rmse: 0.6348  |  0:31:45s
epoch 42 | loss: 0.27096 | val_0_rmse: 0.63701 | val_1_rmse: 0.63899 |  0:32:30s
epoch 43 | loss: 0.26934 | val_0_rmse: 0.65485 | val_1_rmse: 0.65703 |  0:33:15s
epoch 44 | loss: 0.27372 | val_0_rmse: 0.69543 | val_1_rmse: 0.6988  |  0:34:01s
epoch 45 | loss: 0.26837 | val_0_rmse: 0.66211 | val_1_rmse: 0.66436 |  0:34:46s
epoch 46 | loss: 0.26798 | val_0_rmse: 0.66172 | val_1_rmse: 0.66363 |  0:35:31s
epoch 47 | loss: 0.26499 | val_0_rmse: 0.66348 | val_1_rmse: 0.66641 |  0:36:16s
epoch 48 | loss: 0.264   | val_0_rmse: 0.68642 | val_1_rmse: 0.69067 |  0:37:03s
epoch 49 | loss: 0.26219 | val_0_rmse: 0.66556 | val_1_rmse: 0.67457 |  0:37:48s
epoch 50 | loss: 0.26467 | val_0_rmse: 0.68222 | val_1_rmse: 0.68714 |  0:38:33s
epoch 51 | loss: 0.26359 | val_0_rmse: 0.66114 | val_1_rmse: 0.66423 |  0:39:18s
epoch 52 | loss: 0.27265 | val_0_rmse: 0.66888 | val_1_rmse: 0.67262 |  0:40:04s
epoch 53 | loss: 0.25723 | val_0_rmse: 0.69071 | val_1_rmse: 0.69457 |  0:40:50s
epoch 54 | loss: 0.25539 | val_0_rmse: 0.67617 | val_1_rmse: 0.67972 |  0:41:35s
epoch 55 | loss: 0.25428 | val_0_rmse: 0.68033 | val_1_rmse: 0.68453 |  0:42:20s
epoch 56 | loss: 0.25544 | val_0_rmse: 0.65981 | val_1_rmse: 0.66348 |  0:43:06s
epoch 57 | loss: 0.25301 | val_0_rmse: 0.64736 | val_1_rmse: 0.65031 |  0:43:51s
epoch 58 | loss: 0.25159 | val_0_rmse: 0.81232 | val_1_rmse: 0.81575 |  0:44:36s
epoch 59 | loss: 0.25124 | val_0_rmse: 0.66062 | val_1_rmse: 0.6683  |  0:45:21s
epoch 60 | loss: 0.255   | val_0_rmse: 0.69415 | val_1_rmse: 0.69786 |  0:46:07s
epoch 61 | loss: 0.24949 | val_0_rmse: 0.67658 | val_1_rmse: 0.68022 |  0:46:52s
epoch 62 | loss: 0.24736 | val_0_rmse: 0.6816  | val_1_rmse: 0.68525 |  0:47:37s
epoch 63 | loss: 0.25712 | val_0_rmse: 0.63349 | val_1_rmse: 0.63717 |  0:48:22s
epoch 64 | loss: 0.25093 | val_0_rmse: 0.70178 | val_1_rmse: 0.7052  |  0:49:08s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.53549
Best weights from best epoch are automatically used!
ended training at: 02:11:14
Feature importance:
[('Area', 0.3959778403745406), ('Baths', 0.08576293522554893), ('Beds', 0.0), ('Latitude', 0.2198684398386231), ('Longitude', 0.29839078456128737), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 15360777216.764645
Mean absolute error:79051.1175060795
MAPE:0.39551413636383154
R2 score:0.7133712525976821
------------------------------------------------------------------
