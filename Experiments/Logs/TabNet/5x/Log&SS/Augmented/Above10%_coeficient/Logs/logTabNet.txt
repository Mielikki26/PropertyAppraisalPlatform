TabNet Logs:

Saving copy of script...
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:40:35
epoch 0  | loss: 1.52555 | val_0_rmse: 0.99687 | val_1_rmse: 1.00194 |  0:00:03s
epoch 1  | loss: 0.96709 | val_0_rmse: 0.91063 | val_1_rmse: 0.91228 |  0:00:05s
epoch 2  | loss: 0.83521 | val_0_rmse: 0.89812 | val_1_rmse: 0.90309 |  0:00:07s
epoch 3  | loss: 0.79738 | val_0_rmse: 0.88209 | val_1_rmse: 0.89086 |  0:00:09s
epoch 4  | loss: 0.79486 | val_0_rmse: 0.88904 | val_1_rmse: 0.89859 |  0:00:11s
epoch 5  | loss: 0.79828 | val_0_rmse: 0.89442 | val_1_rmse: 0.90283 |  0:00:12s
epoch 6  | loss: 0.81519 | val_0_rmse: 0.88593 | val_1_rmse: 0.89703 |  0:00:14s
epoch 7  | loss: 0.81072 | val_0_rmse: 0.89941 | val_1_rmse: 0.90819 |  0:00:16s
epoch 8  | loss: 0.80574 | val_0_rmse: 0.89267 | val_1_rmse: 0.89832 |  0:00:18s
epoch 9  | loss: 0.79206 | val_0_rmse: 0.88291 | val_1_rmse: 0.88996 |  0:00:19s
epoch 10 | loss: 0.77154 | val_0_rmse: 0.87897 | val_1_rmse: 0.88856 |  0:00:21s
epoch 11 | loss: 0.77752 | val_0_rmse: 0.88224 | val_1_rmse: 0.88922 |  0:00:23s
epoch 12 | loss: 0.76735 | val_0_rmse: 0.87867 | val_1_rmse: 0.89008 |  0:00:25s
epoch 13 | loss: 0.7674  | val_0_rmse: 0.8769  | val_1_rmse: 0.88729 |  0:00:27s
epoch 14 | loss: 0.75622 | val_0_rmse: 0.86918 | val_1_rmse: 0.88551 |  0:00:28s
epoch 15 | loss: 0.74482 | val_0_rmse: 0.86699 | val_1_rmse: 0.88272 |  0:00:30s
epoch 16 | loss: 0.73849 | val_0_rmse: 0.86899 | val_1_rmse: 0.88855 |  0:00:32s
epoch 17 | loss: 0.73374 | val_0_rmse: 0.85912 | val_1_rmse: 0.87605 |  0:00:34s
epoch 18 | loss: 0.72604 | val_0_rmse: 0.8519  | val_1_rmse: 0.87341 |  0:00:35s
epoch 19 | loss: 0.72222 | val_0_rmse: 0.84141 | val_1_rmse: 0.86435 |  0:00:37s
epoch 20 | loss: 0.71374 | val_0_rmse: 0.83509 | val_1_rmse: 0.86724 |  0:00:39s
epoch 21 | loss: 0.70266 | val_0_rmse: 0.8429  | val_1_rmse: 0.869   |  0:00:41s
epoch 22 | loss: 0.69938 | val_0_rmse: 0.82495 | val_1_rmse: 0.8482  |  0:00:43s
epoch 23 | loss: 0.68621 | val_0_rmse: 0.82005 | val_1_rmse: 0.84559 |  0:00:44s
epoch 24 | loss: 0.67871 | val_0_rmse: 0.81491 | val_1_rmse: 0.84152 |  0:00:46s
epoch 25 | loss: 0.67391 | val_0_rmse: 0.80572 | val_1_rmse: 0.83355 |  0:00:48s
epoch 26 | loss: 0.68922 | val_0_rmse: 0.81128 | val_1_rmse: 0.83852 |  0:00:50s
epoch 27 | loss: 0.67557 | val_0_rmse: 0.80544 | val_1_rmse: 0.836   |  0:00:52s
epoch 28 | loss: 0.66698 | val_0_rmse: 0.79506 | val_1_rmse: 0.82553 |  0:00:53s
epoch 29 | loss: 0.6538  | val_0_rmse: 0.80463 | val_1_rmse: 0.83812 |  0:00:55s
epoch 30 | loss: 0.66145 | val_0_rmse: 0.79747 | val_1_rmse: 0.82497 |  0:00:57s
epoch 31 | loss: 0.64593 | val_0_rmse: 0.79784 | val_1_rmse: 0.82651 |  0:00:59s
epoch 32 | loss: 0.64388 | val_0_rmse: 0.78639 | val_1_rmse: 0.82226 |  0:01:01s
epoch 33 | loss: 0.65239 | val_0_rmse: 0.81009 | val_1_rmse: 0.84216 |  0:01:02s
epoch 34 | loss: 0.64985 | val_0_rmse: 0.79005 | val_1_rmse: 0.82928 |  0:01:04s
epoch 35 | loss: 0.63492 | val_0_rmse: 0.77932 | val_1_rmse: 0.82148 |  0:01:06s
epoch 36 | loss: 0.62887 | val_0_rmse: 0.78593 | val_1_rmse: 0.83469 |  0:01:08s
epoch 37 | loss: 0.62986 | val_0_rmse: 0.77586 | val_1_rmse: 0.82646 |  0:01:09s
epoch 38 | loss: 0.618   | val_0_rmse: 0.77284 | val_1_rmse: 0.82884 |  0:01:11s
epoch 39 | loss: 0.61811 | val_0_rmse: 0.77    | val_1_rmse: 0.81696 |  0:01:13s
epoch 40 | loss: 0.61481 | val_0_rmse: 0.76478 | val_1_rmse: 0.81501 |  0:01:15s
epoch 41 | loss: 0.60134 | val_0_rmse: 0.77797 | val_1_rmse: 0.81245 |  0:01:17s
epoch 42 | loss: 0.60415 | val_0_rmse: 0.7681  | val_1_rmse: 0.81704 |  0:01:18s
epoch 43 | loss: 0.61441 | val_0_rmse: 0.81754 | val_1_rmse: 0.86183 |  0:01:20s
epoch 44 | loss: 0.60076 | val_0_rmse: 0.75912 | val_1_rmse: 0.80565 |  0:01:22s
epoch 45 | loss: 0.58829 | val_0_rmse: 0.75819 | val_1_rmse: 0.81199 |  0:01:24s
epoch 46 | loss: 0.5914  | val_0_rmse: 0.75041 | val_1_rmse: 0.81312 |  0:01:26s
epoch 47 | loss: 0.60758 | val_0_rmse: 0.75842 | val_1_rmse: 0.80529 |  0:01:27s
epoch 48 | loss: 0.59041 | val_0_rmse: 0.75243 | val_1_rmse: 0.81557 |  0:01:29s
epoch 49 | loss: 0.57868 | val_0_rmse: 0.738   | val_1_rmse: 0.79716 |  0:01:31s
epoch 50 | loss: 0.57654 | val_0_rmse: 0.74335 | val_1_rmse: 0.80667 |  0:01:33s
epoch 51 | loss: 0.57038 | val_0_rmse: 0.75512 | val_1_rmse: 0.82129 |  0:01:35s
epoch 52 | loss: 0.56472 | val_0_rmse: 0.75105 | val_1_rmse: 0.81309 |  0:01:36s
epoch 53 | loss: 0.56812 | val_0_rmse: 0.7295  | val_1_rmse: 0.7951  |  0:01:38s
epoch 54 | loss: 0.55615 | val_0_rmse: 0.74419 | val_1_rmse: 0.82162 |  0:01:40s
epoch 55 | loss: 0.55987 | val_0_rmse: 0.7376  | val_1_rmse: 0.79868 |  0:01:42s
epoch 56 | loss: 0.55345 | val_0_rmse: 0.72249 | val_1_rmse: 0.79057 |  0:01:44s
epoch 57 | loss: 0.54391 | val_0_rmse: 0.71652 | val_1_rmse: 0.79093 |  0:01:45s
epoch 58 | loss: 0.53955 | val_0_rmse: 0.72506 | val_1_rmse: 0.80388 |  0:01:47s
epoch 59 | loss: 0.54658 | val_0_rmse: 0.71671 | val_1_rmse: 0.78996 |  0:01:49s
epoch 60 | loss: 0.53919 | val_0_rmse: 0.72523 | val_1_rmse: 0.7982  |  0:01:51s
epoch 61 | loss: 0.53582 | val_0_rmse: 0.71121 | val_1_rmse: 0.80327 |  0:01:52s
epoch 62 | loss: 0.53155 | val_0_rmse: 0.70589 | val_1_rmse: 0.7973  |  0:01:54s
epoch 63 | loss: 0.52917 | val_0_rmse: 0.7013  | val_1_rmse: 0.80801 |  0:01:56s
epoch 64 | loss: 0.52413 | val_0_rmse: 0.70003 | val_1_rmse: 0.79918 |  0:01:58s
epoch 65 | loss: 0.51767 | val_0_rmse: 0.7022  | val_1_rmse: 0.8063  |  0:02:00s
epoch 66 | loss: 0.51451 | val_0_rmse: 0.70016 | val_1_rmse: 0.80831 |  0:02:01s
epoch 67 | loss: 0.51206 | val_0_rmse: 0.69605 | val_1_rmse: 0.81067 |  0:02:03s
epoch 68 | loss: 0.50622 | val_0_rmse: 0.69538 | val_1_rmse: 0.79061 |  0:02:05s
epoch 69 | loss: 0.51327 | val_0_rmse: 0.69242 | val_1_rmse: 0.7926  |  0:02:07s
epoch 70 | loss: 0.50952 | val_0_rmse: 0.69853 | val_1_rmse: 0.78429 |  0:02:09s
epoch 71 | loss: 0.49817 | val_0_rmse: 0.68815 | val_1_rmse: 0.80104 |  0:02:10s
epoch 72 | loss: 0.50221 | val_0_rmse: 0.6995  | val_1_rmse: 0.79963 |  0:02:12s
epoch 73 | loss: 0.49571 | val_0_rmse: 0.75981 | val_1_rmse: 0.83972 |  0:02:14s
epoch 74 | loss: 0.49384 | val_0_rmse: 0.68971 | val_1_rmse: 0.80539 |  0:02:16s
epoch 75 | loss: 0.48688 | val_0_rmse: 0.68041 | val_1_rmse: 0.80173 |  0:02:17s
epoch 76 | loss: 0.49462 | val_0_rmse: 0.69597 | val_1_rmse: 0.808   |  0:02:19s
epoch 77 | loss: 0.49503 | val_0_rmse: 0.70258 | val_1_rmse: 0.8399  |  0:02:21s
epoch 78 | loss: 0.48096 | val_0_rmse: 0.69891 | val_1_rmse: 0.82122 |  0:02:23s
epoch 79 | loss: 0.46981 | val_0_rmse: 0.6753  | val_1_rmse: 0.82486 |  0:02:25s
epoch 80 | loss: 0.47527 | val_0_rmse: 0.65874 | val_1_rmse: 0.7848  |  0:02:26s
epoch 81 | loss: 0.47942 | val_0_rmse: 0.66432 | val_1_rmse: 0.78978 |  0:02:28s
epoch 82 | loss: 0.47182 | val_0_rmse: 0.65285 | val_1_rmse: 0.79577 |  0:02:30s
epoch 83 | loss: 0.45668 | val_0_rmse: 0.65237 | val_1_rmse: 0.8189  |  0:02:32s
epoch 84 | loss: 0.45326 | val_0_rmse: 0.64744 | val_1_rmse: 0.77823 |  0:02:34s
epoch 85 | loss: 0.45125 | val_0_rmse: 0.65828 | val_1_rmse: 0.80282 |  0:02:35s
epoch 86 | loss: 0.45287 | val_0_rmse: 0.72681 | val_1_rmse: 0.89178 |  0:02:37s
epoch 87 | loss: 0.45181 | val_0_rmse: 0.65367 | val_1_rmse: 0.79836 |  0:02:39s
epoch 88 | loss: 0.43907 | val_0_rmse: 0.65733 | val_1_rmse: 0.79846 |  0:02:41s
epoch 89 | loss: 0.42888 | val_0_rmse: 0.65463 | val_1_rmse: 0.7985  |  0:02:42s
epoch 90 | loss: 0.42417 | val_0_rmse: 0.6306  | val_1_rmse: 0.78756 |  0:02:44s
epoch 91 | loss: 0.43627 | val_0_rmse: 0.65187 | val_1_rmse: 0.80739 |  0:02:46s
epoch 92 | loss: 0.43497 | val_0_rmse: 0.6569  | val_1_rmse: 0.78525 |  0:02:48s
epoch 93 | loss: 0.43412 | val_0_rmse: 0.63035 | val_1_rmse: 0.80459 |  0:02:50s
epoch 94 | loss: 0.42889 | val_0_rmse: 0.63761 | val_1_rmse: 0.80121 |  0:02:51s
epoch 95 | loss: 0.42419 | val_0_rmse: 0.62628 | val_1_rmse: 0.8013  |  0:02:53s
epoch 96 | loss: 0.4225  | val_0_rmse: 0.65915 | val_1_rmse: 0.79979 |  0:02:55s
epoch 97 | loss: 0.42681 | val_0_rmse: 0.62222 | val_1_rmse: 0.78636 |  0:02:57s
epoch 98 | loss: 0.4149  | val_0_rmse: 0.6325  | val_1_rmse: 0.79632 |  0:02:58s
epoch 99 | loss: 0.41216 | val_0_rmse: 0.63366 | val_1_rmse: 0.81105 |  0:03:00s
epoch 100| loss: 0.41874 | val_0_rmse: 0.62035 | val_1_rmse: 0.79243 |  0:03:02s
epoch 101| loss: 0.40902 | val_0_rmse: 0.644   | val_1_rmse: 0.77803 |  0:03:04s
epoch 102| loss: 0.41819 | val_0_rmse: 0.63909 | val_1_rmse: 0.81708 |  0:03:06s
epoch 103| loss: 0.41403 | val_0_rmse: 0.60481 | val_1_rmse: 0.80611 |  0:03:07s
epoch 104| loss: 0.40227 | val_0_rmse: 0.62027 | val_1_rmse: 0.8191  |  0:03:09s
epoch 105| loss: 0.3994  | val_0_rmse: 0.62753 | val_1_rmse: 0.80936 |  0:03:11s
epoch 106| loss: 0.38823 | val_0_rmse: 0.60546 | val_1_rmse: 0.80746 |  0:03:13s
epoch 107| loss: 0.39364 | val_0_rmse: 0.60081 | val_1_rmse: 0.81382 |  0:03:14s
epoch 108| loss: 0.39722 | val_0_rmse: 0.60561 | val_1_rmse: 0.8216  |  0:03:16s
epoch 109| loss: 0.40057 | val_0_rmse: 0.6054  | val_1_rmse: 0.79333 |  0:03:18s
epoch 110| loss: 0.38761 | val_0_rmse: 0.58466 | val_1_rmse: 0.81387 |  0:03:20s
epoch 111| loss: 0.38406 | val_0_rmse: 0.5934  | val_1_rmse: 0.81482 |  0:03:22s
epoch 112| loss: 0.38385 | val_0_rmse: 0.59391 | val_1_rmse: 0.824   |  0:03:23s
epoch 113| loss: 0.37754 | val_0_rmse: 0.58795 | val_1_rmse: 0.81713 |  0:03:25s
epoch 114| loss: 0.38599 | val_0_rmse: 0.57991 | val_1_rmse: 0.80532 |  0:03:27s
epoch 115| loss: 0.37982 | val_0_rmse: 0.59345 | val_1_rmse: 0.83464 |  0:03:29s
epoch 116| loss: 0.38118 | val_0_rmse: 0.58849 | val_1_rmse: 0.83758 |  0:03:30s
epoch 117| loss: 0.37546 | val_0_rmse: 0.58658 | val_1_rmse: 0.80833 |  0:03:32s
epoch 118| loss: 0.37812 | val_0_rmse: 0.57743 | val_1_rmse: 0.80871 |  0:03:34s
epoch 119| loss: 0.37422 | val_0_rmse: 0.60171 | val_1_rmse: 0.80762 |  0:03:36s
epoch 120| loss: 0.38293 | val_0_rmse: 0.5854  | val_1_rmse: 0.81496 |  0:03:38s
epoch 121| loss: 0.36682 | val_0_rmse: 0.56802 | val_1_rmse: 0.80743 |  0:03:39s
epoch 122| loss: 0.38151 | val_0_rmse: 0.5919  | val_1_rmse: 0.81446 |  0:03:41s
epoch 123| loss: 0.37487 | val_0_rmse: 0.58732 | val_1_rmse: 0.78703 |  0:03:43s
epoch 124| loss: 0.38028 | val_0_rmse: 0.63837 | val_1_rmse: 0.83633 |  0:03:45s
epoch 125| loss: 0.38677 | val_0_rmse: 0.57312 | val_1_rmse: 0.804   |  0:03:46s
epoch 126| loss: 0.38158 | val_0_rmse: 0.58949 | val_1_rmse: 0.81933 |  0:03:48s
epoch 127| loss: 0.36951 | val_0_rmse: 0.57301 | val_1_rmse: 0.82025 |  0:03:50s
epoch 128| loss: 0.36758 | val_0_rmse: 0.56923 | val_1_rmse: 0.81079 |  0:03:52s
epoch 129| loss: 0.37199 | val_0_rmse: 0.5806  | val_1_rmse: 0.82286 |  0:03:54s
epoch 130| loss: 0.36112 | val_0_rmse: 0.56519 | val_1_rmse: 0.81394 |  0:03:55s
epoch 131| loss: 0.3647  | val_0_rmse: 0.56996 | val_1_rmse: 0.80836 |  0:03:57s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.77803
Best weights from best epoch are automatically used!
ended training at: 04:44:34
Feature importance:
Mean squared error is of 0.044189159373703844
Mean absolute error:0.13675339875511427
MAPE:0.1489746477411136
R2 score:0.3934125364093284
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:44:35
epoch 0  | loss: 1.5134  | val_0_rmse: 1.01047 | val_1_rmse: 0.97883 |  0:00:01s
epoch 1  | loss: 1.02975 | val_0_rmse: 1.01036 | val_1_rmse: 0.9787  |  0:00:03s
epoch 2  | loss: 1.02501 | val_0_rmse: 1.01021 | val_1_rmse: 0.97858 |  0:00:05s
epoch 3  | loss: 1.02322 | val_0_rmse: 1.01007 | val_1_rmse: 0.97846 |  0:00:07s
epoch 4  | loss: 1.02319 | val_0_rmse: 1.01008 | val_1_rmse: 0.97835 |  0:00:09s
epoch 5  | loss: 1.02146 | val_0_rmse: 1.0084  | val_1_rmse: 0.977   |  0:00:10s
epoch 6  | loss: 1.01915 | val_0_rmse: 1.00623 | val_1_rmse: 0.97488 |  0:00:12s
epoch 7  | loss: 1.00619 | val_0_rmse: 0.97132 | val_1_rmse: 0.94161 |  0:00:14s
epoch 8  | loss: 0.94534 | val_0_rmse: 0.95598 | val_1_rmse: 0.93331 |  0:00:16s
epoch 9  | loss: 0.85893 | val_0_rmse: 0.91768 | val_1_rmse: 0.89766 |  0:00:18s
epoch 10 | loss: 0.79796 | val_0_rmse: 0.89167 | val_1_rmse: 0.86913 |  0:00:19s
epoch 11 | loss: 0.7677  | val_0_rmse: 0.88758 | val_1_rmse: 0.87266 |  0:00:21s
epoch 12 | loss: 0.72571 | val_0_rmse: 0.89871 | val_1_rmse: 0.89405 |  0:00:23s
epoch 13 | loss: 0.69391 | val_0_rmse: 0.85059 | val_1_rmse: 0.82963 |  0:00:25s
epoch 14 | loss: 0.66341 | val_0_rmse: 0.82803 | val_1_rmse: 0.81077 |  0:00:27s
epoch 15 | loss: 0.64552 | val_0_rmse: 0.86439 | val_1_rmse: 0.83975 |  0:00:28s
epoch 16 | loss: 0.62589 | val_0_rmse: 0.85898 | val_1_rmse: 0.83865 |  0:00:30s
epoch 17 | loss: 0.61062 | val_0_rmse: 0.86917 | val_1_rmse: 0.86949 |  0:00:32s
epoch 18 | loss: 0.60198 | val_0_rmse: 0.81075 | val_1_rmse: 0.8009  |  0:00:34s
epoch 19 | loss: 0.59799 | val_0_rmse: 0.87354 | val_1_rmse: 0.88835 |  0:00:36s
epoch 20 | loss: 0.57927 | val_0_rmse: 0.79127 | val_1_rmse: 0.79771 |  0:00:37s
epoch 21 | loss: 0.56186 | val_0_rmse: 0.77497 | val_1_rmse: 0.78529 |  0:00:39s
epoch 22 | loss: 0.56572 | val_0_rmse: 0.78613 | val_1_rmse: 0.78378 |  0:00:41s
epoch 23 | loss: 0.55542 | val_0_rmse: 0.76535 | val_1_rmse: 0.77588 |  0:00:43s
epoch 24 | loss: 0.55893 | val_0_rmse: 0.73784 | val_1_rmse: 0.75724 |  0:00:45s
epoch 25 | loss: 0.54807 | val_0_rmse: 0.7885  | val_1_rmse: 0.82895 |  0:00:46s
epoch 26 | loss: 0.5369  | val_0_rmse: 0.72708 | val_1_rmse: 0.75989 |  0:00:48s
epoch 27 | loss: 0.53559 | val_0_rmse: 0.73763 | val_1_rmse: 0.76322 |  0:00:50s
epoch 28 | loss: 0.53    | val_0_rmse: 0.75077 | val_1_rmse: 0.76335 |  0:00:52s
epoch 29 | loss: 0.5218  | val_0_rmse: 0.71627 | val_1_rmse: 0.73825 |  0:00:54s
epoch 30 | loss: 0.51838 | val_0_rmse: 0.72568 | val_1_rmse: 0.78791 |  0:00:55s
epoch 31 | loss: 0.52303 | val_0_rmse: 0.70982 | val_1_rmse: 0.74529 |  0:00:57s
epoch 32 | loss: 0.52133 | val_0_rmse: 0.72156 | val_1_rmse: 0.74547 |  0:00:59s
epoch 33 | loss: 0.51241 | val_0_rmse: 0.72397 | val_1_rmse: 0.76505 |  0:01:01s
epoch 34 | loss: 0.50104 | val_0_rmse: 0.77758 | val_1_rmse: 0.88477 |  0:01:03s
epoch 35 | loss: 0.51451 | val_0_rmse: 0.73239 | val_1_rmse: 0.76361 |  0:01:04s
epoch 36 | loss: 0.51582 | val_0_rmse: 0.77838 | val_1_rmse: 0.79675 |  0:01:06s
epoch 37 | loss: 0.50349 | val_0_rmse: 0.68248 | val_1_rmse: 0.75503 |  0:01:08s
epoch 38 | loss: 0.49615 | val_0_rmse: 0.72026 | val_1_rmse: 0.80024 |  0:01:10s
epoch 39 | loss: 0.49123 | val_0_rmse: 0.68418 | val_1_rmse: 0.75333 |  0:01:12s
epoch 40 | loss: 0.47948 | val_0_rmse: 0.65898 | val_1_rmse: 0.74655 |  0:01:13s
epoch 41 | loss: 0.47372 | val_0_rmse: 0.66628 | val_1_rmse: 0.74986 |  0:01:15s
epoch 42 | loss: 0.47526 | val_0_rmse: 0.7309  | val_1_rmse: 0.77853 |  0:01:17s
epoch 43 | loss: 0.47178 | val_0_rmse: 0.6643  | val_1_rmse: 0.74708 |  0:01:19s
epoch 44 | loss: 0.46128 | val_0_rmse: 0.65108 | val_1_rmse: 0.75781 |  0:01:21s
epoch 45 | loss: 0.46499 | val_0_rmse: 0.71136 | val_1_rmse: 0.86764 |  0:01:22s
epoch 46 | loss: 0.45531 | val_0_rmse: 0.65896 | val_1_rmse: 0.75754 |  0:01:24s
epoch 47 | loss: 0.45391 | val_0_rmse: 0.66891 | val_1_rmse: 0.78539 |  0:01:26s
epoch 48 | loss: 0.44663 | val_0_rmse: 0.63295 | val_1_rmse: 0.76261 |  0:01:28s
epoch 49 | loss: 0.44463 | val_0_rmse: 0.66321 | val_1_rmse: 0.75342 |  0:01:30s
epoch 50 | loss: 0.44464 | val_0_rmse: 0.63618 | val_1_rmse: 0.77104 |  0:01:31s
epoch 51 | loss: 0.44466 | val_0_rmse: 0.63617 | val_1_rmse: 0.76612 |  0:01:33s
epoch 52 | loss: 0.43976 | val_0_rmse: 0.64903 | val_1_rmse: 0.76515 |  0:01:35s
epoch 53 | loss: 0.44118 | val_0_rmse: 0.64485 | val_1_rmse: 0.75362 |  0:01:37s
epoch 54 | loss: 0.42824 | val_0_rmse: 0.62622 | val_1_rmse: 0.77579 |  0:01:38s
epoch 55 | loss: 0.4326  | val_0_rmse: 0.63127 | val_1_rmse: 0.77402 |  0:01:40s
epoch 56 | loss: 0.429   | val_0_rmse: 0.65904 | val_1_rmse: 0.75656 |  0:01:42s
epoch 57 | loss: 0.42824 | val_0_rmse: 0.62768 | val_1_rmse: 0.77103 |  0:01:44s
epoch 58 | loss: 0.41885 | val_0_rmse: 0.71023 | val_1_rmse: 0.88142 |  0:01:46s
epoch 59 | loss: 0.42491 | val_0_rmse: 0.6427  | val_1_rmse: 0.76045 |  0:01:48s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.73825
Best weights from best epoch are automatically used!
ended training at: 04:46:24
Feature importance:
Mean squared error is of 0.0426763618319652
Mean absolute error:0.13247544584403775
MAPE:0.14065795392041078
R2 score:0.37576746127871585
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:46:24
epoch 0  | loss: 1.50309 | val_0_rmse: 1.00748 | val_1_rmse: 0.96712 |  0:00:01s
epoch 1  | loss: 1.01769 | val_0_rmse: 0.98941 | val_1_rmse: 0.95229 |  0:00:03s
epoch 2  | loss: 0.97486 | val_0_rmse: 1.01547 | val_1_rmse: 0.97667 |  0:00:05s
epoch 3  | loss: 0.84036 | val_0_rmse: 0.89414 | val_1_rmse: 0.86892 |  0:00:07s
epoch 4  | loss: 0.77286 | val_0_rmse: 0.87041 | val_1_rmse: 0.84187 |  0:00:09s
epoch 5  | loss: 0.78546 | val_0_rmse: 0.87424 | val_1_rmse: 0.85328 |  0:00:11s
epoch 6  | loss: 0.76026 | val_0_rmse: 0.85824 | val_1_rmse: 0.83372 |  0:00:13s
epoch 7  | loss: 0.74337 | val_0_rmse: 0.87626 | val_1_rmse: 0.85557 |  0:00:14s
epoch 8  | loss: 0.74397 | val_0_rmse: 0.86121 | val_1_rmse: 0.84606 |  0:00:16s
epoch 9  | loss: 0.7389  | val_0_rmse: 0.85092 | val_1_rmse: 0.82949 |  0:00:18s
epoch 10 | loss: 0.73044 | val_0_rmse: 0.84872 | val_1_rmse: 0.83214 |  0:00:20s
epoch 11 | loss: 0.72718 | val_0_rmse: 0.8492  | val_1_rmse: 0.82699 |  0:00:22s
epoch 12 | loss: 0.72298 | val_0_rmse: 0.8527  | val_1_rmse: 0.83329 |  0:00:23s
epoch 13 | loss: 0.71912 | val_0_rmse: 0.84378 | val_1_rmse: 0.82607 |  0:00:25s
epoch 14 | loss: 0.71296 | val_0_rmse: 0.84341 | val_1_rmse: 0.82713 |  0:00:27s
epoch 15 | loss: 0.71362 | val_0_rmse: 0.84226 | val_1_rmse: 0.82444 |  0:00:29s
epoch 16 | loss: 0.71089 | val_0_rmse: 0.84887 | val_1_rmse: 0.83324 |  0:00:31s
epoch 17 | loss: 0.71663 | val_0_rmse: 0.83371 | val_1_rmse: 0.82183 |  0:00:32s
epoch 18 | loss: 0.70499 | val_0_rmse: 0.83355 | val_1_rmse: 0.81853 |  0:00:34s
epoch 19 | loss: 0.694   | val_0_rmse: 0.84143 | val_1_rmse: 0.83199 |  0:00:36s
epoch 20 | loss: 0.7017  | val_0_rmse: 0.82375 | val_1_rmse: 0.81026 |  0:00:38s
epoch 21 | loss: 0.68597 | val_0_rmse: 0.82079 | val_1_rmse: 0.81198 |  0:00:40s
epoch 22 | loss: 0.67601 | val_0_rmse: 0.80927 | val_1_rmse: 0.80224 |  0:00:41s
epoch 23 | loss: 0.66566 | val_0_rmse: 0.81326 | val_1_rmse: 0.80412 |  0:00:43s
epoch 24 | loss: 0.66057 | val_0_rmse: 0.80266 | val_1_rmse: 0.80688 |  0:00:45s
epoch 25 | loss: 0.66149 | val_0_rmse: 0.80468 | val_1_rmse: 0.80325 |  0:00:47s
epoch 26 | loss: 0.65246 | val_0_rmse: 0.79048 | val_1_rmse: 0.79113 |  0:00:49s
epoch 27 | loss: 0.64522 | val_0_rmse: 0.7949  | val_1_rmse: 0.79337 |  0:00:50s
epoch 28 | loss: 0.6483  | val_0_rmse: 0.8033  | val_1_rmse: 0.79864 |  0:00:52s
epoch 29 | loss: 0.63596 | val_0_rmse: 0.79854 | val_1_rmse: 0.80515 |  0:00:54s
epoch 30 | loss: 0.65019 | val_0_rmse: 0.79812 | val_1_rmse: 0.79779 |  0:00:56s
epoch 31 | loss: 0.64547 | val_0_rmse: 0.79034 | val_1_rmse: 0.79237 |  0:00:58s
epoch 32 | loss: 0.6437  | val_0_rmse: 0.78581 | val_1_rmse: 0.79415 |  0:00:59s
epoch 33 | loss: 0.63949 | val_0_rmse: 0.78675 | val_1_rmse: 0.79217 |  0:01:01s
epoch 34 | loss: 0.63227 | val_0_rmse: 0.78308 | val_1_rmse: 0.79213 |  0:01:03s
epoch 35 | loss: 0.62945 | val_0_rmse: 0.78387 | val_1_rmse: 0.79007 |  0:01:05s
epoch 36 | loss: 0.62328 | val_0_rmse: 0.77968 | val_1_rmse: 0.78994 |  0:01:07s
epoch 37 | loss: 0.6353  | val_0_rmse: 0.79988 | val_1_rmse: 0.80938 |  0:01:08s
epoch 38 | loss: 0.65    | val_0_rmse: 0.79283 | val_1_rmse: 0.79386 |  0:01:10s
epoch 39 | loss: 0.64931 | val_0_rmse: 0.8047  | val_1_rmse: 0.81645 |  0:01:12s
epoch 40 | loss: 0.65772 | val_0_rmse: 0.78963 | val_1_rmse: 0.79717 |  0:01:14s
epoch 41 | loss: 0.63423 | val_0_rmse: 0.78748 | val_1_rmse: 0.793   |  0:01:16s
epoch 42 | loss: 0.63815 | val_0_rmse: 0.79684 | val_1_rmse: 0.80286 |  0:01:17s
epoch 43 | loss: 0.62827 | val_0_rmse: 0.78925 | val_1_rmse: 0.78863 |  0:01:19s
epoch 44 | loss: 0.6358  | val_0_rmse: 0.79386 | val_1_rmse: 0.80608 |  0:01:21s
epoch 45 | loss: 0.64435 | val_0_rmse: 0.80601 | val_1_rmse: 0.80328 |  0:01:23s
epoch 46 | loss: 0.64464 | val_0_rmse: 0.79381 | val_1_rmse: 0.79741 |  0:01:25s
epoch 47 | loss: 0.64449 | val_0_rmse: 0.78717 | val_1_rmse: 0.79107 |  0:01:27s
epoch 48 | loss: 0.63599 | val_0_rmse: 0.78727 | val_1_rmse: 0.79436 |  0:01:28s
epoch 49 | loss: 0.63078 | val_0_rmse: 0.79279 | val_1_rmse: 0.8006  |  0:01:30s
epoch 50 | loss: 0.62846 | val_0_rmse: 0.78193 | val_1_rmse: 0.78565 |  0:01:32s
epoch 51 | loss: 0.62959 | val_0_rmse: 0.78427 | val_1_rmse: 0.79543 |  0:01:34s
epoch 52 | loss: 0.6215  | val_0_rmse: 0.78405 | val_1_rmse: 0.79646 |  0:01:36s
epoch 53 | loss: 0.62123 | val_0_rmse: 0.77699 | val_1_rmse: 0.79069 |  0:01:37s
epoch 54 | loss: 0.62308 | val_0_rmse: 0.78098 | val_1_rmse: 0.78678 |  0:01:39s
epoch 55 | loss: 0.62077 | val_0_rmse: 0.77566 | val_1_rmse: 0.7883  |  0:01:41s
epoch 56 | loss: 0.61775 | val_0_rmse: 0.77993 | val_1_rmse: 0.79516 |  0:01:43s
epoch 57 | loss: 0.61436 | val_0_rmse: 0.77447 | val_1_rmse: 0.79292 |  0:01:44s
epoch 58 | loss: 0.61154 | val_0_rmse: 0.76943 | val_1_rmse: 0.78577 |  0:01:46s
epoch 59 | loss: 0.61336 | val_0_rmse: 0.76921 | val_1_rmse: 0.78679 |  0:01:48s
epoch 60 | loss: 0.60864 | val_0_rmse: 0.77106 | val_1_rmse: 0.7949  |  0:01:50s
epoch 61 | loss: 0.60615 | val_0_rmse: 0.76454 | val_1_rmse: 0.78233 |  0:01:52s
epoch 62 | loss: 0.60169 | val_0_rmse: 0.77209 | val_1_rmse: 0.79368 |  0:01:54s
epoch 63 | loss: 0.60612 | val_0_rmse: 0.76185 | val_1_rmse: 0.78712 |  0:01:55s
epoch 64 | loss: 0.61658 | val_0_rmse: 0.76262 | val_1_rmse: 0.78826 |  0:01:57s
epoch 65 | loss: 0.59857 | val_0_rmse: 0.76015 | val_1_rmse: 0.79035 |  0:01:59s
epoch 66 | loss: 0.5973  | val_0_rmse: 0.76245 | val_1_rmse: 0.78023 |  0:02:01s
epoch 67 | loss: 0.60513 | val_0_rmse: 0.75867 | val_1_rmse: 0.78669 |  0:02:03s
epoch 68 | loss: 0.58982 | val_0_rmse: 0.75772 | val_1_rmse: 0.78518 |  0:02:04s
epoch 69 | loss: 0.58742 | val_0_rmse: 0.75618 | val_1_rmse: 0.77999 |  0:02:06s
epoch 70 | loss: 0.58162 | val_0_rmse: 0.74889 | val_1_rmse: 0.77988 |  0:02:08s
epoch 71 | loss: 0.58023 | val_0_rmse: 0.76424 | val_1_rmse: 0.79684 |  0:02:10s
epoch 72 | loss: 0.5878  | val_0_rmse: 0.75369 | val_1_rmse: 0.78384 |  0:02:12s
epoch 73 | loss: 0.58305 | val_0_rmse: 0.75536 | val_1_rmse: 0.7933  |  0:02:13s
epoch 74 | loss: 0.57768 | val_0_rmse: 0.74439 | val_1_rmse: 0.78593 |  0:02:15s
epoch 75 | loss: 0.57953 | val_0_rmse: 0.74127 | val_1_rmse: 0.78469 |  0:02:17s
epoch 76 | loss: 0.57328 | val_0_rmse: 0.7423  | val_1_rmse: 0.7798  |  0:02:19s
epoch 77 | loss: 0.5667  | val_0_rmse: 0.74684 | val_1_rmse: 0.78423 |  0:02:21s
epoch 78 | loss: 0.56691 | val_0_rmse: 0.74748 | val_1_rmse: 0.78982 |  0:02:22s
epoch 79 | loss: 0.56224 | val_0_rmse: 0.74575 | val_1_rmse: 0.79317 |  0:02:24s
epoch 80 | loss: 0.56398 | val_0_rmse: 0.74047 | val_1_rmse: 0.78043 |  0:02:26s
epoch 81 | loss: 0.57346 | val_0_rmse: 0.74394 | val_1_rmse: 0.78278 |  0:02:28s
epoch 82 | loss: 0.56136 | val_0_rmse: 0.72755 | val_1_rmse: 0.78622 |  0:02:30s
epoch 83 | loss: 0.55679 | val_0_rmse: 0.73252 | val_1_rmse: 0.78131 |  0:02:31s
epoch 84 | loss: 0.55066 | val_0_rmse: 0.72857 | val_1_rmse: 0.78677 |  0:02:33s
epoch 85 | loss: 0.55435 | val_0_rmse: 0.73701 | val_1_rmse: 0.8041  |  0:02:35s
epoch 86 | loss: 0.56979 | val_0_rmse: 0.74262 | val_1_rmse: 0.79213 |  0:02:37s
epoch 87 | loss: 0.54998 | val_0_rmse: 0.72902 | val_1_rmse: 0.77418 |  0:02:39s
epoch 88 | loss: 0.55396 | val_0_rmse: 0.73217 | val_1_rmse: 0.78797 |  0:02:40s
epoch 89 | loss: 0.55182 | val_0_rmse: 0.73855 | val_1_rmse: 0.80497 |  0:02:42s
epoch 90 | loss: 0.54532 | val_0_rmse: 0.72556 | val_1_rmse: 0.79907 |  0:02:44s
epoch 91 | loss: 0.54062 | val_0_rmse: 0.73106 | val_1_rmse: 0.80553 |  0:02:46s
epoch 92 | loss: 0.53927 | val_0_rmse: 0.72665 | val_1_rmse: 0.78928 |  0:02:48s
epoch 93 | loss: 0.54399 | val_0_rmse: 0.72621 | val_1_rmse: 0.78992 |  0:02:49s
epoch 94 | loss: 0.53188 | val_0_rmse: 0.71743 | val_1_rmse: 0.78846 |  0:02:51s
epoch 95 | loss: 0.61547 | val_0_rmse: 0.84608 | val_1_rmse: 0.94727 |  0:02:53s
epoch 96 | loss: 0.60784 | val_0_rmse: 0.77724 | val_1_rmse: 0.82904 |  0:02:55s
epoch 97 | loss: 0.6005  | val_0_rmse: 0.75373 | val_1_rmse: 0.79184 |  0:02:57s
epoch 98 | loss: 0.57619 | val_0_rmse: 0.77323 | val_1_rmse: 0.81678 |  0:02:58s
epoch 99 | loss: 0.5922  | val_0_rmse: 0.7602  | val_1_rmse: 0.80787 |  0:03:00s
epoch 100| loss: 0.57404 | val_0_rmse: 0.74881 | val_1_rmse: 0.79325 |  0:03:02s
epoch 101| loss: 0.56067 | val_0_rmse: 0.72573 | val_1_rmse: 0.78128 |  0:03:04s
epoch 102| loss: 0.5575  | val_0_rmse: 0.73615 | val_1_rmse: 0.78965 |  0:03:06s
epoch 103| loss: 0.55608 | val_0_rmse: 0.72595 | val_1_rmse: 0.8016  |  0:03:07s
epoch 104| loss: 0.54288 | val_0_rmse: 0.72603 | val_1_rmse: 0.80309 |  0:03:09s
epoch 105| loss: 0.54315 | val_0_rmse: 0.73193 | val_1_rmse: 0.79751 |  0:03:11s
epoch 106| loss: 0.55189 | val_0_rmse: 0.72218 | val_1_rmse: 0.77119 |  0:03:13s
epoch 107| loss: 0.5453  | val_0_rmse: 0.73245 | val_1_rmse: 0.80099 |  0:03:15s
epoch 108| loss: 0.53051 | val_0_rmse: 0.71648 | val_1_rmse: 0.78624 |  0:03:16s
epoch 109| loss: 0.54536 | val_0_rmse: 0.72922 | val_1_rmse: 0.78313 |  0:03:18s
epoch 110| loss: 0.55228 | val_0_rmse: 0.74808 | val_1_rmse: 0.79855 |  0:03:20s
epoch 111| loss: 0.54178 | val_0_rmse: 0.72723 | val_1_rmse: 0.80566 |  0:03:22s
epoch 112| loss: 0.53307 | val_0_rmse: 0.71033 | val_1_rmse: 0.76743 |  0:03:24s
epoch 113| loss: 0.52291 | val_0_rmse: 0.72451 | val_1_rmse: 0.76785 |  0:03:25s
epoch 114| loss: 0.54896 | val_0_rmse: 0.72508 | val_1_rmse: 0.77708 |  0:03:27s
epoch 115| loss: 0.55009 | val_0_rmse: 0.72733 | val_1_rmse: 0.7902  |  0:03:29s
epoch 116| loss: 0.53935 | val_0_rmse: 0.71748 | val_1_rmse: 0.78671 |  0:03:31s
epoch 117| loss: 0.5481  | val_0_rmse: 0.71588 | val_1_rmse: 0.78179 |  0:03:33s
epoch 118| loss: 0.53351 | val_0_rmse: 0.72549 | val_1_rmse: 0.80715 |  0:03:34s
epoch 119| loss: 0.52867 | val_0_rmse: 0.71785 | val_1_rmse: 0.79314 |  0:03:36s
epoch 120| loss: 0.52817 | val_0_rmse: 0.70867 | val_1_rmse: 0.78989 |  0:03:38s
epoch 121| loss: 0.52672 | val_0_rmse: 0.71338 | val_1_rmse: 0.79143 |  0:03:40s
epoch 122| loss: 0.52271 | val_0_rmse: 0.7036  | val_1_rmse: 0.77194 |  0:03:42s
epoch 123| loss: 0.52387 | val_0_rmse: 0.70951 | val_1_rmse: 0.77104 |  0:03:43s
epoch 124| loss: 0.51943 | val_0_rmse: 0.70691 | val_1_rmse: 0.78401 |  0:03:45s
epoch 125| loss: 0.51836 | val_0_rmse: 0.70878 | val_1_rmse: 0.77774 |  0:03:47s
epoch 126| loss: 0.51664 | val_0_rmse: 0.69802 | val_1_rmse: 0.78582 |  0:03:49s
epoch 127| loss: 0.5083  | val_0_rmse: 0.69606 | val_1_rmse: 0.77627 |  0:03:51s
epoch 128| loss: 0.50868 | val_0_rmse: 0.69381 | val_1_rmse: 0.78403 |  0:03:52s
epoch 129| loss: 0.51058 | val_0_rmse: 0.69491 | val_1_rmse: 0.77122 |  0:03:54s
epoch 130| loss: 0.51076 | val_0_rmse: 0.69667 | val_1_rmse: 0.78545 |  0:03:56s
epoch 131| loss: 0.51101 | val_0_rmse: 0.70097 | val_1_rmse: 0.78041 |  0:03:58s
epoch 132| loss: 0.50384 | val_0_rmse: 0.69432 | val_1_rmse: 0.78152 |  0:03:59s
epoch 133| loss: 0.50111 | val_0_rmse: 0.69027 | val_1_rmse: 0.77111 |  0:04:01s
epoch 134| loss: 0.49507 | val_0_rmse: 0.69524 | val_1_rmse: 0.7873  |  0:04:03s
epoch 135| loss: 0.50057 | val_0_rmse: 0.69126 | val_1_rmse: 0.7733  |  0:04:05s
epoch 136| loss: 0.49836 | val_0_rmse: 0.68488 | val_1_rmse: 0.77199 |  0:04:07s
epoch 137| loss: 0.49773 | val_0_rmse: 0.68526 | val_1_rmse: 0.76764 |  0:04:08s
epoch 138| loss: 0.50332 | val_0_rmse: 0.68887 | val_1_rmse: 0.77961 |  0:04:10s
epoch 139| loss: 0.49462 | val_0_rmse: 0.67903 | val_1_rmse: 0.76724 |  0:04:12s
epoch 140| loss: 0.49497 | val_0_rmse: 0.67328 | val_1_rmse: 0.77812 |  0:04:14s
epoch 141| loss: 0.50294 | val_0_rmse: 0.69021 | val_1_rmse: 0.77334 |  0:04:16s
epoch 142| loss: 0.5004  | val_0_rmse: 0.68032 | val_1_rmse: 0.777   |  0:04:17s
epoch 143| loss: 0.49919 | val_0_rmse: 0.6774  | val_1_rmse: 0.77988 |  0:04:19s
epoch 144| loss: 0.49177 | val_0_rmse: 0.68057 | val_1_rmse: 0.7788  |  0:04:21s
epoch 145| loss: 0.48727 | val_0_rmse: 0.72565 | val_1_rmse: 0.81261 |  0:04:23s
epoch 146| loss: 0.4839  | val_0_rmse: 0.67289 | val_1_rmse: 0.77755 |  0:04:25s
epoch 147| loss: 0.47769 | val_0_rmse: 0.67014 | val_1_rmse: 0.77739 |  0:04:26s
epoch 148| loss: 0.46784 | val_0_rmse: 0.66302 | val_1_rmse: 0.7722  |  0:04:28s
epoch 149| loss: 0.46289 | val_0_rmse: 0.66965 | val_1_rmse: 0.77931 |  0:04:30s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.76724
Best weights from best epoch are automatically used!
ended training at: 04:50:56
Feature importance:
Mean squared error is of 0.04191944062714321
Mean absolute error:0.13652150542677802
MAPE:0.1458035860138812
R2 score:0.38328841607729425
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:50:56
epoch 0  | loss: 1.48344 | val_0_rmse: 1.00555 | val_1_rmse: 0.99079 |  0:00:01s
epoch 1  | loss: 0.97026 | val_0_rmse: 0.9371  | val_1_rmse: 0.93567 |  0:00:03s
epoch 2  | loss: 0.85146 | val_0_rmse: 0.90768 | val_1_rmse: 0.91011 |  0:00:05s
epoch 3  | loss: 0.81961 | val_0_rmse: 0.88677 | val_1_rmse: 0.88089 |  0:00:07s
epoch 4  | loss: 0.80145 | val_0_rmse: 0.89555 | val_1_rmse: 0.89301 |  0:00:09s
epoch 5  | loss: 0.79699 | val_0_rmse: 0.88649 | val_1_rmse: 0.88245 |  0:00:10s
epoch 6  | loss: 0.78747 | val_0_rmse: 0.89167 | val_1_rmse: 0.88892 |  0:00:12s
epoch 7  | loss: 0.78581 | val_0_rmse: 0.88438 | val_1_rmse: 0.879   |  0:00:14s
epoch 8  | loss: 0.77917 | val_0_rmse: 0.88576 | val_1_rmse: 0.88582 |  0:00:16s
epoch 9  | loss: 0.77913 | val_0_rmse: 0.88195 | val_1_rmse: 0.87999 |  0:00:18s
epoch 10 | loss: 0.77291 | val_0_rmse: 0.87836 | val_1_rmse: 0.87636 |  0:00:20s
epoch 11 | loss: 0.76695 | val_0_rmse: 0.87817 | val_1_rmse: 0.87863 |  0:00:21s
epoch 12 | loss: 0.76024 | val_0_rmse: 0.87064 | val_1_rmse: 0.87097 |  0:00:23s
epoch 13 | loss: 0.75462 | val_0_rmse: 0.86494 | val_1_rmse: 0.86258 |  0:00:25s
epoch 14 | loss: 0.75499 | val_0_rmse: 0.87229 | val_1_rmse: 0.87514 |  0:00:27s
epoch 15 | loss: 0.7434  | val_0_rmse: 0.86301 | val_1_rmse: 0.8649  |  0:00:29s
epoch 16 | loss: 0.74745 | val_0_rmse: 0.86856 | val_1_rmse: 0.87176 |  0:00:30s
epoch 17 | loss: 0.74315 | val_0_rmse: 0.85653 | val_1_rmse: 0.86082 |  0:00:32s
epoch 18 | loss: 0.73366 | val_0_rmse: 0.85406 | val_1_rmse: 0.85684 |  0:00:34s
epoch 19 | loss: 0.73441 | val_0_rmse: 0.85921 | val_1_rmse: 0.86696 |  0:00:36s
epoch 20 | loss: 0.72775 | val_0_rmse: 0.85042 | val_1_rmse: 0.85251 |  0:00:38s
epoch 21 | loss: 0.72458 | val_0_rmse: 0.85757 | val_1_rmse: 0.85861 |  0:00:40s
epoch 22 | loss: 0.72255 | val_0_rmse: 0.84109 | val_1_rmse: 0.84263 |  0:00:41s
epoch 23 | loss: 0.70975 | val_0_rmse: 0.83496 | val_1_rmse: 0.8446  |  0:00:43s
epoch 24 | loss: 0.70236 | val_0_rmse: 0.83699 | val_1_rmse: 0.84427 |  0:00:45s
epoch 25 | loss: 0.69716 | val_0_rmse: 0.82696 | val_1_rmse: 0.8323  |  0:00:47s
epoch 26 | loss: 0.68806 | val_0_rmse: 0.82821 | val_1_rmse: 0.84166 |  0:00:49s
epoch 27 | loss: 0.71134 | val_0_rmse: 0.83224 | val_1_rmse: 0.84223 |  0:00:50s
epoch 28 | loss: 0.68272 | val_0_rmse: 0.8215  | val_1_rmse: 0.83731 |  0:00:52s
epoch 29 | loss: 0.68877 | val_0_rmse: 0.82145 | val_1_rmse: 0.83848 |  0:00:54s
epoch 30 | loss: 0.66936 | val_0_rmse: 0.80187 | val_1_rmse: 0.8194  |  0:00:56s
epoch 31 | loss: 0.65579 | val_0_rmse: 0.79187 | val_1_rmse: 0.80695 |  0:00:58s
epoch 32 | loss: 0.66222 | val_0_rmse: 0.79699 | val_1_rmse: 0.80661 |  0:00:59s
epoch 33 | loss: 0.64906 | val_0_rmse: 0.79067 | val_1_rmse: 0.80499 |  0:01:01s
epoch 34 | loss: 0.65175 | val_0_rmse: 0.78191 | val_1_rmse: 0.7988  |  0:01:03s
epoch 35 | loss: 0.63971 | val_0_rmse: 0.78446 | val_1_rmse: 0.80398 |  0:01:05s
epoch 36 | loss: 0.63109 | val_0_rmse: 0.78653 | val_1_rmse: 0.80313 |  0:01:07s
epoch 37 | loss: 0.65473 | val_0_rmse: 0.78275 | val_1_rmse: 0.80259 |  0:01:09s
epoch 38 | loss: 0.6267  | val_0_rmse: 0.77016 | val_1_rmse: 0.79127 |  0:01:10s
epoch 39 | loss: 0.62626 | val_0_rmse: 0.77585 | val_1_rmse: 0.79789 |  0:01:12s
epoch 40 | loss: 0.61709 | val_0_rmse: 0.76705 | val_1_rmse: 0.80024 |  0:01:14s
epoch 41 | loss: 0.60973 | val_0_rmse: 0.76428 | val_1_rmse: 0.79567 |  0:01:16s
epoch 42 | loss: 0.60437 | val_0_rmse: 0.78335 | val_1_rmse: 0.80166 |  0:01:18s
epoch 43 | loss: 0.60693 | val_0_rmse: 0.75753 | val_1_rmse: 0.78016 |  0:01:20s
epoch 44 | loss: 0.59295 | val_0_rmse: 0.78852 | val_1_rmse: 0.80903 |  0:01:21s
epoch 45 | loss: 0.60056 | val_0_rmse: 0.76377 | val_1_rmse: 0.78215 |  0:01:23s
epoch 46 | loss: 0.58588 | val_0_rmse: 0.73829 | val_1_rmse: 0.77965 |  0:01:25s
epoch 47 | loss: 0.58454 | val_0_rmse: 0.74147 | val_1_rmse: 0.77492 |  0:01:27s
epoch 48 | loss: 0.57415 | val_0_rmse: 0.73516 | val_1_rmse: 0.77473 |  0:01:29s
epoch 49 | loss: 0.57236 | val_0_rmse: 0.73886 | val_1_rmse: 0.77846 |  0:01:30s
epoch 50 | loss: 0.58022 | val_0_rmse: 0.7506  | val_1_rmse: 0.79255 |  0:01:32s
epoch 51 | loss: 0.56605 | val_0_rmse: 0.74777 | val_1_rmse: 0.79197 |  0:01:34s
epoch 52 | loss: 0.56336 | val_0_rmse: 0.73467 | val_1_rmse: 0.77135 |  0:01:36s
epoch 53 | loss: 0.56332 | val_0_rmse: 0.73999 | val_1_rmse: 0.78204 |  0:01:38s
epoch 54 | loss: 0.55943 | val_0_rmse: 0.72845 | val_1_rmse: 0.77294 |  0:01:40s
epoch 55 | loss: 0.55832 | val_0_rmse: 0.7263  | val_1_rmse: 0.77392 |  0:01:41s
epoch 56 | loss: 0.55396 | val_0_rmse: 0.71709 | val_1_rmse: 0.76846 |  0:01:43s
epoch 57 | loss: 0.54385 | val_0_rmse: 0.7187  | val_1_rmse: 0.76151 |  0:01:45s
epoch 58 | loss: 0.54093 | val_0_rmse: 0.74809 | val_1_rmse: 0.81273 |  0:01:47s
epoch 59 | loss: 0.542   | val_0_rmse: 0.71603 | val_1_rmse: 0.75656 |  0:01:49s
epoch 60 | loss: 0.53424 | val_0_rmse: 0.70579 | val_1_rmse: 0.7589  |  0:01:50s
epoch 61 | loss: 0.53638 | val_0_rmse: 0.71802 | val_1_rmse: 0.77793 |  0:01:52s
epoch 62 | loss: 0.53253 | val_0_rmse: 0.71532 | val_1_rmse: 0.77424 |  0:01:54s
epoch 63 | loss: 0.52046 | val_0_rmse: 0.69492 | val_1_rmse: 0.75442 |  0:01:56s
epoch 64 | loss: 0.51767 | val_0_rmse: 0.71809 | val_1_rmse: 0.77614 |  0:01:58s
epoch 65 | loss: 0.52767 | val_0_rmse: 0.75817 | val_1_rmse: 0.80721 |  0:01:59s
epoch 66 | loss: 0.51563 | val_0_rmse: 0.69618 | val_1_rmse: 0.75347 |  0:02:01s
epoch 67 | loss: 0.51574 | val_0_rmse: 0.70008 | val_1_rmse: 0.77737 |  0:02:03s
epoch 68 | loss: 0.50777 | val_0_rmse: 0.69795 | val_1_rmse: 0.77521 |  0:02:05s
epoch 69 | loss: 0.5132  | val_0_rmse: 0.72698 | val_1_rmse: 0.78195 |  0:02:07s
epoch 70 | loss: 0.50957 | val_0_rmse: 0.69189 | val_1_rmse: 0.76495 |  0:02:09s
epoch 71 | loss: 0.49374 | val_0_rmse: 0.68499 | val_1_rmse: 0.75535 |  0:02:10s
epoch 72 | loss: 0.49758 | val_0_rmse: 0.69927 | val_1_rmse: 0.77245 |  0:02:12s
epoch 73 | loss: 0.4869  | val_0_rmse: 0.70888 | val_1_rmse: 0.78258 |  0:02:14s
epoch 74 | loss: 0.48857 | val_0_rmse: 0.69809 | val_1_rmse: 0.8023  |  0:02:16s
epoch 75 | loss: 0.49036 | val_0_rmse: 0.70023 | val_1_rmse: 0.82082 |  0:02:18s
epoch 76 | loss: 0.49351 | val_0_rmse: 0.69475 | val_1_rmse: 0.77015 |  0:02:19s
epoch 77 | loss: 0.50013 | val_0_rmse: 0.68542 | val_1_rmse: 0.7713  |  0:02:21s
epoch 78 | loss: 0.48599 | val_0_rmse: 0.66147 | val_1_rmse: 0.75839 |  0:02:23s
epoch 79 | loss: 0.481   | val_0_rmse: 0.66246 | val_1_rmse: 0.74662 |  0:02:25s
epoch 80 | loss: 0.47832 | val_0_rmse: 0.66227 | val_1_rmse: 0.74398 |  0:02:27s
epoch 81 | loss: 0.46659 | val_0_rmse: 0.71586 | val_1_rmse: 0.84118 |  0:02:29s
epoch 82 | loss: 0.46822 | val_0_rmse: 0.69215 | val_1_rmse: 0.77529 |  0:02:30s
epoch 83 | loss: 0.4639  | val_0_rmse: 0.65791 | val_1_rmse: 0.75826 |  0:02:32s
epoch 84 | loss: 0.45706 | val_0_rmse: 0.65031 | val_1_rmse: 0.76731 |  0:02:34s
epoch 85 | loss: 0.45465 | val_0_rmse: 0.65223 | val_1_rmse: 0.76828 |  0:02:36s
epoch 86 | loss: 0.46343 | val_0_rmse: 0.65038 | val_1_rmse: 0.76255 |  0:02:38s
epoch 87 | loss: 0.45536 | val_0_rmse: 0.64224 | val_1_rmse: 0.76838 |  0:02:39s
epoch 88 | loss: 0.46145 | val_0_rmse: 0.73825 | val_1_rmse: 0.82876 |  0:02:41s
epoch 89 | loss: 0.45097 | val_0_rmse: 1.07244 | val_1_rmse: 1.18401 |  0:02:43s
epoch 90 | loss: 0.45195 | val_0_rmse: 0.65254 | val_1_rmse: 0.7707  |  0:02:45s
epoch 91 | loss: 0.4571  | val_0_rmse: 0.7608  | val_1_rmse: 0.83928 |  0:02:47s
epoch 92 | loss: 0.4524  | val_0_rmse: 0.65253 | val_1_rmse: 0.75194 |  0:02:49s
epoch 93 | loss: 0.44918 | val_0_rmse: 0.63483 | val_1_rmse: 0.7708  |  0:02:50s
epoch 94 | loss: 0.44604 | val_0_rmse: 0.68244 | val_1_rmse: 0.81963 |  0:02:52s
epoch 95 | loss: 0.44215 | val_0_rmse: 0.6299  | val_1_rmse: 0.7645  |  0:02:54s
epoch 96 | loss: 0.4326  | val_0_rmse: 0.62565 | val_1_rmse: 0.76773 |  0:02:56s
epoch 97 | loss: 0.4307  | val_0_rmse: 0.64602 | val_1_rmse: 0.77745 |  0:02:58s
epoch 98 | loss: 0.42763 | val_0_rmse: 0.63479 | val_1_rmse: 0.76196 |  0:02:59s
epoch 99 | loss: 0.43219 | val_0_rmse: 0.632   | val_1_rmse: 0.77033 |  0:03:01s
epoch 100| loss: 0.42846 | val_0_rmse: 0.61328 | val_1_rmse: 0.755   |  0:03:03s
epoch 101| loss: 0.42371 | val_0_rmse: 0.61635 | val_1_rmse: 0.78146 |  0:03:05s
epoch 102| loss: 0.42136 | val_0_rmse: 0.61868 | val_1_rmse: 0.7681  |  0:03:07s
epoch 103| loss: 0.40845 | val_0_rmse: 0.6452  | val_1_rmse: 0.78382 |  0:03:09s
epoch 104| loss: 0.40865 | val_0_rmse: 0.6163  | val_1_rmse: 0.76785 |  0:03:10s
epoch 105| loss: 0.41094 | val_0_rmse: 0.65141 | val_1_rmse: 0.78634 |  0:03:12s
epoch 106| loss: 0.40195 | val_0_rmse: 0.60516 | val_1_rmse: 0.78652 |  0:03:14s
epoch 107| loss: 0.39947 | val_0_rmse: 0.60412 | val_1_rmse: 0.79827 |  0:03:16s
epoch 108| loss: 0.40911 | val_0_rmse: 0.61278 | val_1_rmse: 0.77531 |  0:03:18s
epoch 109| loss: 0.40747 | val_0_rmse: 0.68562 | val_1_rmse: 0.86477 |  0:03:19s
epoch 110| loss: 0.41128 | val_0_rmse: 0.66878 | val_1_rmse: 0.80327 |  0:03:21s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.74398
Best weights from best epoch are automatically used!
ended training at: 04:54:19
Feature importance:
Mean squared error is of 0.04474703706957794
Mean absolute error:0.13222301526594205
MAPE:0.13916484238294158
R2 score:0.3506504644633306
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:54:20
epoch 0  | loss: 1.4459  | val_0_rmse: 0.99184 | val_1_rmse: 1.00578 |  0:00:01s
epoch 1  | loss: 0.99332 | val_0_rmse: 0.99207 | val_1_rmse: 1.006   |  0:00:03s
epoch 2  | loss: 0.98145 | val_0_rmse: 0.96713 | val_1_rmse: 0.97798 |  0:00:05s
epoch 3  | loss: 0.91167 | val_0_rmse: 0.91699 | val_1_rmse: 0.92237 |  0:00:07s
epoch 4  | loss: 0.84328 | val_0_rmse: 0.88117 | val_1_rmse: 0.88772 |  0:00:09s
epoch 5  | loss: 0.8233  | val_0_rmse: 0.89217 | val_1_rmse: 0.89132 |  0:00:11s
epoch 6  | loss: 0.80244 | val_0_rmse: 0.88071 | val_1_rmse: 0.88474 |  0:00:12s
epoch 7  | loss: 0.79541 | val_0_rmse: 0.88139 | val_1_rmse: 0.88212 |  0:00:14s
epoch 8  | loss: 0.78416 | val_0_rmse: 0.87887 | val_1_rmse: 0.88088 |  0:00:16s
epoch 9  | loss: 0.7846  | val_0_rmse: 0.89089 | val_1_rmse: 0.89311 |  0:00:18s
epoch 10 | loss: 0.77894 | val_0_rmse: 0.87543 | val_1_rmse: 0.87929 |  0:00:20s
epoch 11 | loss: 0.77343 | val_0_rmse: 0.88099 | val_1_rmse: 0.88048 |  0:00:21s
epoch 12 | loss: 0.77516 | val_0_rmse: 0.8747  | val_1_rmse: 0.87688 |  0:00:23s
epoch 13 | loss: 0.76961 | val_0_rmse: 0.87274 | val_1_rmse: 0.87839 |  0:00:25s
epoch 14 | loss: 0.76731 | val_0_rmse: 0.87167 | val_1_rmse: 0.87481 |  0:00:27s
epoch 15 | loss: 0.75724 | val_0_rmse: 0.88119 | val_1_rmse: 0.88459 |  0:00:29s
epoch 16 | loss: 0.7684  | val_0_rmse: 0.87111 | val_1_rmse: 0.87759 |  0:00:31s
epoch 17 | loss: 0.75825 | val_0_rmse: 0.86884 | val_1_rmse: 0.87754 |  0:00:32s
epoch 18 | loss: 0.75752 | val_0_rmse: 0.86679 | val_1_rmse: 0.87671 |  0:00:34s
epoch 19 | loss: 0.75672 | val_0_rmse: 0.86613 | val_1_rmse: 0.87643 |  0:00:36s
epoch 20 | loss: 0.75371 | val_0_rmse: 0.86853 | val_1_rmse: 0.88003 |  0:00:38s
epoch 21 | loss: 0.75426 | val_0_rmse: 0.86417 | val_1_rmse: 0.87339 |  0:00:40s
epoch 22 | loss: 0.75144 | val_0_rmse: 0.86346 | val_1_rmse: 0.86982 |  0:00:41s
epoch 23 | loss: 0.741   | val_0_rmse: 0.85621 | val_1_rmse: 0.86579 |  0:00:43s
epoch 24 | loss: 0.74319 | val_0_rmse: 0.86395 | val_1_rmse: 0.87312 |  0:00:45s
epoch 25 | loss: 0.74144 | val_0_rmse: 0.85975 | val_1_rmse: 0.87367 |  0:00:47s
epoch 26 | loss: 0.7302  | val_0_rmse: 0.8494  | val_1_rmse: 0.86184 |  0:00:49s
epoch 27 | loss: 0.72043 | val_0_rmse: 0.8409  | val_1_rmse: 0.85636 |  0:00:51s
epoch 28 | loss: 0.71096 | val_0_rmse: 0.8318  | val_1_rmse: 0.85171 |  0:00:52s
epoch 29 | loss: 0.70137 | val_0_rmse: 0.82255 | val_1_rmse: 0.8452  |  0:00:54s
epoch 30 | loss: 0.68418 | val_0_rmse: 0.80967 | val_1_rmse: 0.83528 |  0:00:56s
epoch 31 | loss: 0.67486 | val_0_rmse: 0.83223 | val_1_rmse: 0.85286 |  0:00:58s
epoch 32 | loss: 0.66918 | val_0_rmse: 0.81025 | val_1_rmse: 0.83668 |  0:01:00s
epoch 33 | loss: 0.65298 | val_0_rmse: 0.79768 | val_1_rmse: 0.82921 |  0:01:01s
epoch 34 | loss: 0.64923 | val_0_rmse: 0.79281 | val_1_rmse: 0.81625 |  0:01:03s
epoch 35 | loss: 0.63729 | val_0_rmse: 0.78237 | val_1_rmse: 0.80812 |  0:01:05s
epoch 36 | loss: 0.63081 | val_0_rmse: 0.78777 | val_1_rmse: 0.82918 |  0:01:07s
epoch 37 | loss: 0.62404 | val_0_rmse: 0.81345 | val_1_rmse: 0.85059 |  0:01:09s
epoch 38 | loss: 0.61456 | val_0_rmse: 0.78924 | val_1_rmse: 0.8211  |  0:01:11s
epoch 39 | loss: 0.60912 | val_0_rmse: 0.76236 | val_1_rmse: 0.79792 |  0:01:12s
epoch 40 | loss: 0.5979  | val_0_rmse: 0.74924 | val_1_rmse: 0.78221 |  0:01:14s
epoch 41 | loss: 0.59311 | val_0_rmse: 0.74871 | val_1_rmse: 0.79131 |  0:01:16s
epoch 42 | loss: 0.58152 | val_0_rmse: 0.75523 | val_1_rmse: 0.79064 |  0:01:18s
epoch 43 | loss: 0.58709 | val_0_rmse: 0.75579 | val_1_rmse: 0.79491 |  0:01:20s
epoch 44 | loss: 0.58184 | val_0_rmse: 0.74172 | val_1_rmse: 0.78024 |  0:01:21s
epoch 45 | loss: 0.56485 | val_0_rmse: 0.74233 | val_1_rmse: 0.78871 |  0:01:23s
epoch 46 | loss: 0.56561 | val_0_rmse: 0.78013 | val_1_rmse: 0.84577 |  0:01:25s
epoch 47 | loss: 0.55872 | val_0_rmse: 0.75301 | val_1_rmse: 0.78672 |  0:01:27s
epoch 48 | loss: 0.54898 | val_0_rmse: 0.72807 | val_1_rmse: 0.78875 |  0:01:29s
epoch 49 | loss: 0.54672 | val_0_rmse: 0.72062 | val_1_rmse: 0.78388 |  0:01:31s
epoch 50 | loss: 0.54288 | val_0_rmse: 0.73352 | val_1_rmse: 0.78765 |  0:01:32s
epoch 51 | loss: 0.54787 | val_0_rmse: 0.72245 | val_1_rmse: 0.78434 |  0:01:34s
epoch 52 | loss: 0.53655 | val_0_rmse: 0.74684 | val_1_rmse: 0.79516 |  0:01:36s
epoch 53 | loss: 0.53419 | val_0_rmse: 0.70633 | val_1_rmse: 0.78022 |  0:01:38s
epoch 54 | loss: 0.5323  | val_0_rmse: 0.75126 | val_1_rmse: 0.81648 |  0:01:40s
epoch 55 | loss: 0.52841 | val_0_rmse: 0.71123 | val_1_rmse: 0.78636 |  0:01:41s
epoch 56 | loss: 0.52081 | val_0_rmse: 0.71416 | val_1_rmse: 0.79925 |  0:01:43s
epoch 57 | loss: 0.50683 | val_0_rmse: 0.72125 | val_1_rmse: 0.78313 |  0:01:45s
epoch 58 | loss: 0.53898 | val_0_rmse: 1.00575 | val_1_rmse: 1.07286 |  0:01:47s
epoch 59 | loss: 0.52157 | val_0_rmse: 0.73961 | val_1_rmse: 0.80662 |  0:01:49s
epoch 60 | loss: 0.50233 | val_0_rmse: 0.68286 | val_1_rmse: 0.77231 |  0:01:51s
epoch 61 | loss: 0.49248 | val_0_rmse: 0.67635 | val_1_rmse: 0.76649 |  0:01:52s
epoch 62 | loss: 0.48544 | val_0_rmse: 0.677   | val_1_rmse: 0.77037 |  0:01:54s
epoch 63 | loss: 0.49195 | val_0_rmse: 0.69268 | val_1_rmse: 0.77784 |  0:01:56s
epoch 64 | loss: 0.48735 | val_0_rmse: 0.67736 | val_1_rmse: 0.79    |  0:01:58s
epoch 65 | loss: 0.48697 | val_0_rmse: 0.67412 | val_1_rmse: 0.77907 |  0:02:00s
epoch 66 | loss: 0.5155  | val_0_rmse: 0.68296 | val_1_rmse: 0.79139 |  0:02:01s
epoch 67 | loss: 0.49815 | val_0_rmse: 0.67752 | val_1_rmse: 0.76133 |  0:02:03s
epoch 68 | loss: 0.49083 | val_0_rmse: 0.69756 | val_1_rmse: 0.81157 |  0:02:05s
epoch 69 | loss: 0.47849 | val_0_rmse: 0.72509 | val_1_rmse: 0.81814 |  0:02:07s
epoch 70 | loss: 0.48997 | val_0_rmse: 0.69577 | val_1_rmse: 0.81024 |  0:02:09s
epoch 71 | loss: 0.46858 | val_0_rmse: 0.68046 | val_1_rmse: 0.79309 |  0:02:11s
epoch 72 | loss: 0.46508 | val_0_rmse: 0.71134 | val_1_rmse: 0.81113 |  0:02:12s
epoch 73 | loss: 0.46741 | val_0_rmse: 0.79879 | val_1_rmse: 0.9299  |  0:02:14s
epoch 74 | loss: 0.46627 | val_0_rmse: 0.6589  | val_1_rmse: 0.77547 |  0:02:16s
epoch 75 | loss: 0.46772 | val_0_rmse: 0.65844 | val_1_rmse: 0.78428 |  0:02:18s
epoch 76 | loss: 0.46282 | val_0_rmse: 0.65367 | val_1_rmse: 0.7715  |  0:02:20s
epoch 77 | loss: 0.45344 | val_0_rmse: 0.69048 | val_1_rmse: 0.83297 |  0:02:21s
epoch 78 | loss: 0.45274 | val_0_rmse: 0.65549 | val_1_rmse: 0.78146 |  0:02:23s
epoch 79 | loss: 0.4454  | val_0_rmse: 0.64706 | val_1_rmse: 0.78866 |  0:02:25s
epoch 80 | loss: 0.4402  | val_0_rmse: 0.6454  | val_1_rmse: 0.7785  |  0:02:27s
epoch 81 | loss: 0.43921 | val_0_rmse: 0.65217 | val_1_rmse: 0.774   |  0:02:29s
epoch 82 | loss: 0.44699 | val_0_rmse: 0.64962 | val_1_rmse: 0.77403 |  0:02:31s
epoch 83 | loss: 0.43899 | val_0_rmse: 0.63331 | val_1_rmse: 0.77575 |  0:02:32s
epoch 84 | loss: 0.44183 | val_0_rmse: 0.65182 | val_1_rmse: 0.78816 |  0:02:34s
epoch 85 | loss: 0.44406 | val_0_rmse: 0.65389 | val_1_rmse: 0.78119 |  0:02:36s
epoch 86 | loss: 0.44894 | val_0_rmse: 0.63966 | val_1_rmse: 0.77509 |  0:02:38s
epoch 87 | loss: 0.4367  | val_0_rmse: 0.63096 | val_1_rmse: 0.79155 |  0:02:40s
epoch 88 | loss: 0.42885 | val_0_rmse: 0.66102 | val_1_rmse: 0.80167 |  0:02:41s
epoch 89 | loss: 0.43325 | val_0_rmse: 0.66228 | val_1_rmse: 0.82573 |  0:02:43s
epoch 90 | loss: 0.43429 | val_0_rmse: 0.68184 | val_1_rmse: 0.79167 |  0:02:45s
epoch 91 | loss: 0.42954 | val_0_rmse: 0.65079 | val_1_rmse: 0.78795 |  0:02:47s
epoch 92 | loss: 0.42346 | val_0_rmse: 0.62482 | val_1_rmse: 0.77355 |  0:02:49s
epoch 93 | loss: 0.41357 | val_0_rmse: 0.6255  | val_1_rmse: 0.7777  |  0:02:51s
epoch 94 | loss: 0.41877 | val_0_rmse: 0.63565 | val_1_rmse: 0.78925 |  0:02:52s
epoch 95 | loss: 0.41668 | val_0_rmse: 0.67628 | val_1_rmse: 0.80729 |  0:02:54s
epoch 96 | loss: 0.42435 | val_0_rmse: 0.62347 | val_1_rmse: 0.78341 |  0:02:56s
epoch 97 | loss: 0.41679 | val_0_rmse: 0.61563 | val_1_rmse: 0.78791 |  0:02:58s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.76133
Best weights from best epoch are automatically used!
ended training at: 04:57:19
Feature importance:
Mean squared error is of 0.045606358081753076
Mean absolute error:0.13748726370148198
MAPE:0.14347974756380139
R2 score:0.37734064961840996
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:58:23
epoch 0  | loss: 1.2108  | val_0_rmse: 0.99958 | val_1_rmse: 0.986   |  0:00:07s
epoch 1  | loss: 1.00471 | val_0_rmse: 1.00101 | val_1_rmse: 0.98688 |  0:00:13s
epoch 2  | loss: 1.00143 | val_0_rmse: 0.99549 | val_1_rmse: 0.98145 |  0:00:20s
epoch 3  | loss: 0.97218 | val_0_rmse: 0.95954 | val_1_rmse: 0.94602 |  0:00:27s
epoch 4  | loss: 0.91277 | val_0_rmse: 0.95495 | val_1_rmse: 0.93959 |  0:00:34s
epoch 5  | loss: 0.87645 | val_0_rmse: 0.93781 | val_1_rmse: 0.92088 |  0:00:41s
epoch 6  | loss: 0.85691 | val_0_rmse: 0.93602 | val_1_rmse: 0.92018 |  0:00:48s
epoch 7  | loss: 0.83898 | val_0_rmse: 0.92678 | val_1_rmse: 0.91702 |  0:00:55s
epoch 8  | loss: 0.82174 | val_0_rmse: 0.91654 | val_1_rmse: 0.90907 |  0:01:02s
epoch 9  | loss: 0.81313 | val_0_rmse: 0.90189 | val_1_rmse: 0.89588 |  0:01:09s
epoch 10 | loss: 0.80069 | val_0_rmse: 0.89469 | val_1_rmse: 0.89875 |  0:01:16s
epoch 11 | loss: 0.79221 | val_0_rmse: 0.89102 | val_1_rmse: 0.89615 |  0:01:22s
epoch 12 | loss: 0.77981 | val_0_rmse: 0.8777  | val_1_rmse: 0.88726 |  0:01:29s
epoch 13 | loss: 0.77023 | val_0_rmse: 0.86589 | val_1_rmse: 0.88498 |  0:01:36s
epoch 14 | loss: 0.75914 | val_0_rmse: 0.86079 | val_1_rmse: 0.88142 |  0:01:43s
epoch 15 | loss: 0.75501 | val_0_rmse: 0.86774 | val_1_rmse: 0.89963 |  0:01:50s
epoch 16 | loss: 0.75063 | val_0_rmse: 0.8687  | val_1_rmse: 0.89015 |  0:01:57s
epoch 17 | loss: 0.73892 | val_0_rmse: 0.8788  | val_1_rmse: 0.90902 |  0:02:04s
epoch 18 | loss: 0.73317 | val_0_rmse: 0.87126 | val_1_rmse: 0.88791 |  0:02:11s
epoch 19 | loss: 0.72309 | val_0_rmse: 0.84767 | val_1_rmse: 0.89553 |  0:02:18s
epoch 20 | loss: 0.7174  | val_0_rmse: 0.84147 | val_1_rmse: 0.88112 |  0:02:25s
epoch 21 | loss: 0.71208 | val_0_rmse: 0.8453  | val_1_rmse: 0.89328 |  0:02:32s
epoch 22 | loss: 0.72265 | val_0_rmse: 0.84857 | val_1_rmse: 0.89128 |  0:02:39s
epoch 23 | loss: 0.71571 | val_0_rmse: 0.85249 | val_1_rmse: 0.89738 |  0:02:45s
epoch 24 | loss: 0.70297 | val_0_rmse: 0.84872 | val_1_rmse: 0.89137 |  0:02:52s
epoch 25 | loss: 0.69226 | val_0_rmse: 0.85457 | val_1_rmse: 0.9026  |  0:02:59s
epoch 26 | loss: 0.69089 | val_0_rmse: 0.86907 | val_1_rmse: 0.89575 |  0:03:06s
epoch 27 | loss: 0.68055 | val_0_rmse: 0.86092 | val_1_rmse: 0.89964 |  0:03:13s
epoch 28 | loss: 0.67528 | val_0_rmse: 0.83436 | val_1_rmse: 0.90375 |  0:03:20s
epoch 29 | loss: 0.67112 | val_0_rmse: 0.82654 | val_1_rmse: 0.88334 |  0:03:27s
epoch 30 | loss: 0.67305 | val_0_rmse: 0.82787 | val_1_rmse: 0.88363 |  0:03:34s
epoch 31 | loss: 0.66627 | val_0_rmse: 0.82243 | val_1_rmse: 0.88212 |  0:03:41s
epoch 32 | loss: 0.65913 | val_0_rmse: 0.83281 | val_1_rmse: 0.89637 |  0:03:48s
epoch 33 | loss: 0.65273 | val_0_rmse: 0.80815 | val_1_rmse: 0.89943 |  0:03:55s
epoch 34 | loss: 0.64743 | val_0_rmse: 0.82201 | val_1_rmse: 0.8826  |  0:04:01s
epoch 35 | loss: 0.65047 | val_0_rmse: 0.85965 | val_1_rmse: 0.94318 |  0:04:08s
epoch 36 | loss: 0.64212 | val_0_rmse: 0.8305  | val_1_rmse: 1.01884 |  0:04:16s
epoch 37 | loss: 0.63967 | val_0_rmse: 0.81717 | val_1_rmse: 1.51643 |  0:04:23s
epoch 38 | loss: 0.63499 | val_0_rmse: 0.80669 | val_1_rmse: 0.97447 |  0:04:29s
epoch 39 | loss: 0.63288 | val_0_rmse: 0.83801 | val_1_rmse: 1.11035 |  0:04:36s
epoch 40 | loss: 0.62672 | val_0_rmse: 0.81321 | val_1_rmse: 1.23025 |  0:04:43s
epoch 41 | loss: 0.62659 | val_0_rmse: 0.80177 | val_1_rmse: 0.89527 |  0:04:50s
epoch 42 | loss: 0.62014 | val_0_rmse: 0.80216 | val_1_rmse: 0.91558 |  0:04:57s
epoch 43 | loss: 0.61665 | val_0_rmse: 0.78359 | val_1_rmse: 0.90223 |  0:05:04s
epoch 44 | loss: 0.6202  | val_0_rmse: 0.81932 | val_1_rmse: 0.92077 |  0:05:11s
epoch 45 | loss: 0.61298 | val_0_rmse: 0.793   | val_1_rmse: 1.33093 |  0:05:18s
epoch 46 | loss: 0.61352 | val_0_rmse: 0.7849  | val_1_rmse: 1.60165 |  0:05:25s
epoch 47 | loss: 0.60885 | val_0_rmse: 0.78782 | val_1_rmse: 0.93887 |  0:05:32s
epoch 48 | loss: 0.60723 | val_0_rmse: 0.79265 | val_1_rmse: 0.89578 |  0:05:38s
epoch 49 | loss: 0.60794 | val_0_rmse: 0.79168 | val_1_rmse: 0.90425 |  0:05:45s
epoch 50 | loss: 0.60149 | val_0_rmse: 0.77418 | val_1_rmse: 0.88721 |  0:05:52s

Early stopping occured at epoch 50 with best_epoch = 20 and best_val_1_rmse = 0.88112
Best weights from best epoch are automatically used!
ended training at: 05:04:20
Feature importance:
Mean squared error is of 0.06526938805634987
Mean absolute error:0.18570537003020404
MAPE:0.19710000629192173
R2 score:0.17832463142206678
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:04:22
epoch 0  | loss: 1.17467 | val_0_rmse: 0.99506 | val_1_rmse: 1.01484 |  0:00:06s
epoch 1  | loss: 0.98879 | val_0_rmse: 0.96721 | val_1_rmse: 0.98642 |  0:00:13s
epoch 2  | loss: 0.94618 | val_0_rmse: 0.94591 | val_1_rmse: 0.9653  |  0:00:20s
epoch 3  | loss: 0.88895 | val_0_rmse: 0.94127 | val_1_rmse: 0.95862 |  0:00:27s
epoch 4  | loss: 0.84578 | val_0_rmse: 0.91995 | val_1_rmse: 0.93743 |  0:00:34s
epoch 5  | loss: 0.81425 | val_0_rmse: 0.91591 | val_1_rmse: 0.93494 |  0:00:41s
epoch 6  | loss: 0.79773 | val_0_rmse: 0.907   | val_1_rmse: 0.9314  |  0:00:48s
epoch 7  | loss: 0.77872 | val_0_rmse: 0.89242 | val_1_rmse: 0.91826 |  0:00:55s
epoch 8  | loss: 0.76675 | val_0_rmse: 0.88424 | val_1_rmse: 0.91508 |  0:01:02s
epoch 9  | loss: 0.7494  | val_0_rmse: 0.87272 | val_1_rmse: 0.91212 |  0:01:09s
epoch 10 | loss: 0.74483 | val_0_rmse: 0.86546 | val_1_rmse: 0.9146  |  0:01:16s
epoch 11 | loss: 0.73362 | val_0_rmse: 0.84931 | val_1_rmse: 0.89921 |  0:01:23s
epoch 12 | loss: 0.72849 | val_0_rmse: 0.83864 | val_1_rmse: 0.90384 |  0:01:30s
epoch 13 | loss: 0.71915 | val_0_rmse: 0.83822 | val_1_rmse: 0.89902 |  0:01:37s
epoch 14 | loss: 0.71177 | val_0_rmse: 0.84404 | val_1_rmse: 0.91607 |  0:01:44s
epoch 15 | loss: 0.70897 | val_0_rmse: 0.8352  | val_1_rmse: 0.90599 |  0:01:51s
epoch 16 | loss: 0.70104 | val_0_rmse: 0.82339 | val_1_rmse: 0.90418 |  0:01:58s
epoch 17 | loss: 0.69496 | val_0_rmse: 0.82166 | val_1_rmse: 0.90712 |  0:02:04s
epoch 18 | loss: 0.68881 | val_0_rmse: 0.82228 | val_1_rmse: 0.92106 |  0:02:11s
epoch 19 | loss: 0.68018 | val_0_rmse: 0.83454 | val_1_rmse: 0.92334 |  0:02:18s
epoch 20 | loss: 0.67857 | val_0_rmse: 0.82157 | val_1_rmse: 0.91084 |  0:02:25s
epoch 21 | loss: 0.67328 | val_0_rmse: 0.81505 | val_1_rmse: 0.91136 |  0:02:32s
epoch 22 | loss: 0.6664  | val_0_rmse: 0.81652 | val_1_rmse: 0.91814 |  0:02:39s
epoch 23 | loss: 0.66097 | val_0_rmse: 0.81509 | val_1_rmse: 0.91599 |  0:02:46s
epoch 24 | loss: 0.65862 | val_0_rmse: 0.811   | val_1_rmse: 0.91721 |  0:02:53s
epoch 25 | loss: 0.65333 | val_0_rmse: 0.80383 | val_1_rmse: 0.89866 |  0:03:00s
epoch 26 | loss: 0.64573 | val_0_rmse: 0.79891 | val_1_rmse: 0.91459 |  0:03:07s
epoch 27 | loss: 0.64424 | val_0_rmse: 0.80697 | val_1_rmse: 0.90571 |  0:03:14s
epoch 28 | loss: 0.64303 | val_0_rmse: 0.81798 | val_1_rmse: 0.90874 |  0:03:21s
epoch 29 | loss: 0.63252 | val_0_rmse: 0.79238 | val_1_rmse: 0.90438 |  0:03:28s
epoch 30 | loss: 0.62846 | val_0_rmse: 0.80881 | val_1_rmse: 0.91077 |  0:03:35s
epoch 31 | loss: 0.62663 | val_0_rmse: 0.8319  | val_1_rmse: 0.92228 |  0:03:42s
epoch 32 | loss: 0.62091 | val_0_rmse: 0.79319 | val_1_rmse: 0.95852 |  0:03:49s
epoch 33 | loss: 0.61921 | val_0_rmse: 0.78858 | val_1_rmse: 0.93841 |  0:03:57s
epoch 34 | loss: 0.61347 | val_0_rmse: 0.79167 | val_1_rmse: 0.91305 |  0:04:04s
epoch 35 | loss: 0.61096 | val_0_rmse: 0.79406 | val_1_rmse: 0.91573 |  0:04:11s
epoch 36 | loss: 0.60607 | val_0_rmse: 0.78224 | val_1_rmse: 0.91884 |  0:04:18s
epoch 37 | loss: 0.60412 | val_0_rmse: 0.79466 | val_1_rmse: 0.94664 |  0:04:25s
epoch 38 | loss: 0.60135 | val_0_rmse: 0.78459 | val_1_rmse: 0.9062  |  0:04:32s
epoch 39 | loss: 0.59581 | val_0_rmse: 0.78225 | val_1_rmse: 0.93745 |  0:04:39s
epoch 40 | loss: 0.5904  | val_0_rmse: 0.79072 | val_1_rmse: 0.91517 |  0:04:46s
epoch 41 | loss: 0.59271 | val_0_rmse: 0.77948 | val_1_rmse: 0.92431 |  0:04:53s
epoch 42 | loss: 0.58847 | val_0_rmse: 0.78926 | val_1_rmse: 0.93955 |  0:05:00s
epoch 43 | loss: 0.58855 | val_0_rmse: 0.77909 | val_1_rmse: 0.91091 |  0:05:07s
epoch 44 | loss: 0.58552 | val_0_rmse: 0.7817  | val_1_rmse: 0.93042 |  0:05:14s
epoch 45 | loss: 0.58121 | val_0_rmse: 0.77077 | val_1_rmse: 0.9392  |  0:05:21s
epoch 46 | loss: 0.58044 | val_0_rmse: 0.77303 | val_1_rmse: 0.94115 |  0:05:27s
epoch 47 | loss: 0.57461 | val_0_rmse: 0.77193 | val_1_rmse: 0.94106 |  0:05:34s
epoch 48 | loss: 0.57284 | val_0_rmse: 0.75894 | val_1_rmse: 0.91895 |  0:05:41s
epoch 49 | loss: 0.56884 | val_0_rmse: 0.75526 | val_1_rmse: 0.91945 |  0:05:48s
epoch 50 | loss: 0.56997 | val_0_rmse: 0.7576  | val_1_rmse: 0.91332 |  0:05:55s
epoch 51 | loss: 0.56941 | val_0_rmse: 0.7575  | val_1_rmse: 0.91611 |  0:06:02s
epoch 52 | loss: 0.56211 | val_0_rmse: 0.75593 | val_1_rmse: 0.93197 |  0:06:09s
epoch 53 | loss: 0.55701 | val_0_rmse: 0.76261 | val_1_rmse: 0.94427 |  0:06:16s
epoch 54 | loss: 0.56093 | val_0_rmse: 0.76415 | val_1_rmse: 0.92898 |  0:06:23s
epoch 55 | loss: 0.55966 | val_0_rmse: 0.7547  | val_1_rmse: 0.9245  |  0:06:30s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.89866
Best weights from best epoch are automatically used!
ended training at: 05:10:56
Feature importance:
Mean squared error is of 0.06070807932640753
Mean absolute error:0.18443652008258551
MAPE:0.20246075228870067
R2 score:0.18539286761927543
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:10:58
epoch 0  | loss: 1.12696 | val_0_rmse: 0.99577 | val_1_rmse: 1.01574 |  0:00:06s
epoch 1  | loss: 0.99215 | val_0_rmse: 0.9949  | val_1_rmse: 1.01535 |  0:00:13s
epoch 2  | loss: 0.98971 | val_0_rmse: 0.99235 | val_1_rmse: 1.01299 |  0:00:20s
epoch 3  | loss: 0.97449 | val_0_rmse: 0.96248 | val_1_rmse: 0.98457 |  0:00:27s
epoch 4  | loss: 0.90285 | val_0_rmse: 0.92906 | val_1_rmse: 0.95053 |  0:00:34s
epoch 5  | loss: 0.85278 | val_0_rmse: 0.91518 | val_1_rmse: 0.93461 |  0:00:41s
epoch 6  | loss: 0.82272 | val_0_rmse: 0.90918 | val_1_rmse: 0.93244 |  0:00:48s
epoch 7  | loss: 0.80261 | val_0_rmse: 0.90104 | val_1_rmse: 0.92596 |  0:00:55s
epoch 8  | loss: 0.78465 | val_0_rmse: 0.88807 | val_1_rmse: 0.9175  |  0:01:02s
epoch 9  | loss: 0.77158 | val_0_rmse: 0.87879 | val_1_rmse: 0.91816 |  0:01:09s
epoch 10 | loss: 0.76379 | val_0_rmse: 0.86966 | val_1_rmse: 0.91541 |  0:01:16s
epoch 11 | loss: 0.75645 | val_0_rmse: 0.86178 | val_1_rmse: 0.9075  |  0:01:23s
epoch 12 | loss: 0.74733 | val_0_rmse: 0.85644 | val_1_rmse: 0.90847 |  0:01:30s
epoch 13 | loss: 0.74233 | val_0_rmse: 0.84616 | val_1_rmse: 0.90512 |  0:01:37s
epoch 14 | loss: 0.73722 | val_0_rmse: 0.84534 | val_1_rmse: 0.90108 |  0:01:44s
epoch 15 | loss: 0.72904 | val_0_rmse: 0.84144 | val_1_rmse: 0.90565 |  0:01:51s
epoch 16 | loss: 0.71992 | val_0_rmse: 0.83652 | val_1_rmse: 0.90998 |  0:01:58s
epoch 17 | loss: 0.71298 | val_0_rmse: 0.83241 | val_1_rmse: 0.91054 |  0:02:05s
epoch 18 | loss: 0.70665 | val_0_rmse: 0.83459 | val_1_rmse: 0.90584 |  0:02:12s
epoch 19 | loss: 0.69613 | val_0_rmse: 0.82871 | val_1_rmse: 0.91522 |  0:02:19s
epoch 20 | loss: 0.68976 | val_0_rmse: 0.82315 | val_1_rmse: 0.90327 |  0:02:26s
epoch 21 | loss: 0.68282 | val_0_rmse: 0.8214  | val_1_rmse: 0.92147 |  0:02:33s
epoch 22 | loss: 0.67633 | val_0_rmse: 0.81542 | val_1_rmse: 0.9053  |  0:02:40s
epoch 23 | loss: 0.67147 | val_0_rmse: 0.81424 | val_1_rmse: 0.90874 |  0:02:47s
epoch 24 | loss: 0.66151 | val_0_rmse: 0.80955 | val_1_rmse: 0.91248 |  0:02:54s
epoch 25 | loss: 0.6547  | val_0_rmse: 1.00684 | val_1_rmse: 1.10213 |  0:03:01s
epoch 26 | loss: 0.65294 | val_0_rmse: 0.81168 | val_1_rmse: 0.90927 |  0:03:08s
epoch 27 | loss: 0.65202 | val_0_rmse: 0.8245  | val_1_rmse: 0.91531 |  0:03:15s
epoch 28 | loss: 0.64314 | val_0_rmse: 0.86755 | val_1_rmse: 0.91997 |  0:03:22s
epoch 29 | loss: 0.63968 | val_0_rmse: 0.7909  | val_1_rmse: 0.90304 |  0:03:29s
epoch 30 | loss: 0.63223 | val_0_rmse: 0.80589 | val_1_rmse: 0.93737 |  0:03:36s
epoch 31 | loss: 0.6373  | val_0_rmse: 1.04475 | val_1_rmse: 1.27503 |  0:03:43s
epoch 32 | loss: 0.62141 | val_0_rmse: 0.78638 | val_1_rmse: 0.91835 |  0:03:49s
epoch 33 | loss: 0.62007 | val_0_rmse: 0.82833 | val_1_rmse: 0.91647 |  0:03:57s
epoch 34 | loss: 0.61874 | val_0_rmse: 0.78548 | val_1_rmse: 0.90621 |  0:04:04s
epoch 35 | loss: 0.6133  | val_0_rmse: 0.87158 | val_1_rmse: 0.96167 |  0:04:10s
epoch 36 | loss: 0.64773 | val_0_rmse: 0.96805 | val_1_rmse: 0.92635 |  0:04:17s
epoch 37 | loss: 0.61647 | val_0_rmse: 0.80373 | val_1_rmse: 0.91995 |  0:04:24s
epoch 38 | loss: 0.60432 | val_0_rmse: 0.78586 | val_1_rmse: 0.91897 |  0:04:31s
epoch 39 | loss: 0.5962  | val_0_rmse: 0.77617 | val_1_rmse: 0.92664 |  0:04:38s
epoch 40 | loss: 0.59271 | val_0_rmse: 0.80345 | val_1_rmse: 0.94431 |  0:04:45s
epoch 41 | loss: 0.59301 | val_0_rmse: 0.7738  | val_1_rmse: 0.92186 |  0:04:52s
epoch 42 | loss: 0.58782 | val_0_rmse: 0.77317 | val_1_rmse: 0.91469 |  0:04:59s
epoch 43 | loss: 0.58033 | val_0_rmse: 0.86742 | val_1_rmse: 0.91914 |  0:05:06s
epoch 44 | loss: 0.57701 | val_0_rmse: 1.00596 | val_1_rmse: 0.93226 |  0:05:13s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.90108
Best weights from best epoch are automatically used!
ended training at: 05:16:15
Feature importance:
Mean squared error is of 0.06442823963466165
Mean absolute error:0.18448222893090807
MAPE:0.1984716091514218
R2 score:0.1865679351662607
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:16:17
epoch 0  | loss: 1.15258 | val_0_rmse: 0.98147 | val_1_rmse: 0.98089 |  0:00:06s
epoch 1  | loss: 0.96321 | val_0_rmse: 0.97026 | val_1_rmse: 0.96825 |  0:00:13s
epoch 2  | loss: 0.9445  | val_0_rmse: 0.96524 | val_1_rmse: 0.96579 |  0:00:20s
epoch 3  | loss: 0.93812 | val_0_rmse: 0.9598  | val_1_rmse: 0.95926 |  0:00:27s
epoch 4  | loss: 0.92231 | val_0_rmse: 0.95233 | val_1_rmse: 0.95419 |  0:00:34s
epoch 5  | loss: 0.89213 | val_0_rmse: 0.94447 | val_1_rmse: 0.94194 |  0:00:41s
epoch 6  | loss: 0.85677 | val_0_rmse: 0.92267 | val_1_rmse: 0.92254 |  0:00:48s
epoch 7  | loss: 0.83464 | val_0_rmse: 0.91349 | val_1_rmse: 0.91603 |  0:00:55s
epoch 8  | loss: 0.81937 | val_0_rmse: 0.90743 | val_1_rmse: 0.91175 |  0:01:02s
epoch 9  | loss: 0.8053  | val_0_rmse: 0.89414 | val_1_rmse: 0.90924 |  0:01:09s
epoch 10 | loss: 0.79446 | val_0_rmse: 0.88221 | val_1_rmse: 0.90106 |  0:01:16s
epoch 11 | loss: 0.78488 | val_0_rmse: 0.87851 | val_1_rmse: 0.90441 |  0:01:23s
epoch 12 | loss: 0.7709  | val_0_rmse: 0.87709 | val_1_rmse: 0.9093  |  0:01:30s
epoch 13 | loss: 0.76656 | val_0_rmse: 0.86439 | val_1_rmse: 0.90183 |  0:01:37s
epoch 14 | loss: 0.75766 | val_0_rmse: 0.85642 | val_1_rmse: 0.903   |  0:01:44s
epoch 15 | loss: 0.74369 | val_0_rmse: 0.86173 | val_1_rmse: 0.92266 |  0:01:51s
epoch 16 | loss: 0.73878 | val_0_rmse: 0.84745 | val_1_rmse: 0.96897 |  0:01:58s
epoch 17 | loss: 0.72836 | val_0_rmse: 0.84084 | val_1_rmse: 0.88988 |  0:02:05s
epoch 18 | loss: 0.71924 | val_0_rmse: 0.86429 | val_1_rmse: 0.91462 |  0:02:12s
epoch 19 | loss: 0.71155 | val_0_rmse: 0.84893 | val_1_rmse: 0.90874 |  0:02:19s
epoch 20 | loss: 0.70364 | val_0_rmse: 0.87619 | val_1_rmse: 1.42102 |  0:02:26s
epoch 21 | loss: 0.69377 | val_0_rmse: 0.83336 | val_1_rmse: 0.90022 |  0:02:33s
epoch 22 | loss: 0.68928 | val_0_rmse: 0.84628 | val_1_rmse: 0.90643 |  0:02:40s
epoch 23 | loss: 0.68329 | val_0_rmse: 0.8364  | val_1_rmse: 0.98245 |  0:02:47s
epoch 24 | loss: 0.67713 | val_0_rmse: 0.9024  | val_1_rmse: 1.32028 |  0:02:54s
epoch 25 | loss: 0.67177 | val_0_rmse: 0.93961 | val_1_rmse: 1.14252 |  0:03:01s
epoch 26 | loss: 0.66288 | val_0_rmse: 0.81703 | val_1_rmse: 0.99053 |  0:03:08s
epoch 27 | loss: 0.65958 | val_0_rmse: 0.81563 | val_1_rmse: 1.03876 |  0:03:15s
epoch 28 | loss: 0.65707 | val_0_rmse: 0.8339  | val_1_rmse: 0.96931 |  0:03:21s
epoch 29 | loss: 0.6507  | val_0_rmse: 0.8035  | val_1_rmse: 0.90981 |  0:03:28s
epoch 30 | loss: 0.64475 | val_0_rmse: 0.82407 | val_1_rmse: 1.09037 |  0:03:35s
epoch 31 | loss: 0.64063 | val_0_rmse: 0.91808 | val_1_rmse: 0.99327 |  0:03:42s
epoch 32 | loss: 0.63474 | val_0_rmse: 0.81537 | val_1_rmse: 1.24155 |  0:03:49s
epoch 33 | loss: 0.62875 | val_0_rmse: 0.80187 | val_1_rmse: 0.91966 |  0:03:56s
epoch 34 | loss: 0.62692 | val_0_rmse: 0.89932 | val_1_rmse: 1.11517 |  0:04:03s
epoch 35 | loss: 0.62043 | val_0_rmse: 0.83957 | val_1_rmse: 1.15439 |  0:04:10s
epoch 36 | loss: 0.61946 | val_0_rmse: 0.80954 | val_1_rmse: 1.21341 |  0:04:17s
epoch 37 | loss: 0.61135 | val_0_rmse: 0.78497 | val_1_rmse: 0.91189 |  0:04:24s
epoch 38 | loss: 0.60667 | val_0_rmse: 0.79315 | val_1_rmse: 0.95855 |  0:04:31s
epoch 39 | loss: 0.60707 | val_0_rmse: 0.79383 | val_1_rmse: 1.00022 |  0:04:38s
epoch 40 | loss: 0.59703 | val_0_rmse: 0.80055 | val_1_rmse: 1.23454 |  0:04:45s
epoch 41 | loss: 0.59726 | val_0_rmse: 1.03834 | val_1_rmse: 1.23563 |  0:04:52s
epoch 42 | loss: 0.58845 | val_0_rmse: 0.78843 | val_1_rmse: 1.15529 |  0:04:59s
epoch 43 | loss: 0.59054 | val_0_rmse: 0.81154 | val_1_rmse: 1.13762 |  0:05:06s
epoch 44 | loss: 0.58569 | val_0_rmse: 1.09857 | val_1_rmse: 2.80708 |  0:05:13s
epoch 45 | loss: 0.5819  | val_0_rmse: 0.79399 | val_1_rmse: 1.03385 |  0:05:20s
epoch 46 | loss: 0.5797  | val_0_rmse: 0.97037 | val_1_rmse: 2.24223 |  0:05:27s
epoch 47 | loss: 0.57558 | val_0_rmse: 0.86045 | val_1_rmse: 1.67902 |  0:05:34s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.88988
Best weights from best epoch are automatically used!
ended training at: 05:21:55
Feature importance:
Mean squared error is of 0.0617130127013167
Mean absolute error:0.18426938644349974
MAPE:0.20007896989150167
R2 score:0.19584513421786642
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:21:56
epoch 0  | loss: 1.17436 | val_0_rmse: 1.00002 | val_1_rmse: 0.99486 |  0:00:06s
epoch 1  | loss: 0.99254 | val_0_rmse: 0.96188 | val_1_rmse: 0.96048 |  0:00:13s
epoch 2  | loss: 0.9466  | val_0_rmse: 0.95106 | val_1_rmse: 0.94722 |  0:00:20s
epoch 3  | loss: 0.90174 | val_0_rmse: 0.93878 | val_1_rmse: 0.93488 |  0:00:27s
epoch 4  | loss: 0.85564 | val_0_rmse: 0.93583 | val_1_rmse: 0.93225 |  0:00:34s
epoch 5  | loss: 0.82951 | val_0_rmse: 0.92868 | val_1_rmse: 0.92477 |  0:00:41s
epoch 6  | loss: 0.81318 | val_0_rmse: 0.91366 | val_1_rmse: 0.90904 |  0:00:48s
epoch 7  | loss: 0.79753 | val_0_rmse: 0.92063 | val_1_rmse: 0.91665 |  0:00:55s
epoch 8  | loss: 0.78851 | val_0_rmse: 0.89663 | val_1_rmse: 0.89733 |  0:01:02s
epoch 9  | loss: 0.78165 | val_0_rmse: 0.89039 | val_1_rmse: 0.89393 |  0:01:09s
epoch 10 | loss: 0.77335 | val_0_rmse: 0.87242 | val_1_rmse: 0.88596 |  0:01:15s
epoch 11 | loss: 0.76168 | val_0_rmse: 0.86443 | val_1_rmse: 0.8849  |  0:01:22s
epoch 12 | loss: 0.75051 | val_0_rmse: 0.85887 | val_1_rmse: 0.88647 |  0:01:29s
epoch 13 | loss: 0.73991 | val_0_rmse: 0.84989 | val_1_rmse: 0.87985 |  0:01:36s
epoch 14 | loss: 0.737   | val_0_rmse: 0.84073 | val_1_rmse: 0.87881 |  0:01:43s
epoch 15 | loss: 0.72263 | val_0_rmse: 0.84774 | val_1_rmse: 0.95774 |  0:01:50s
epoch 16 | loss: 0.7189  | val_0_rmse: 0.83106 | val_1_rmse: 0.87472 |  0:01:57s
epoch 17 | loss: 0.70508 | val_0_rmse: 0.83373 | val_1_rmse: 0.87547 |  0:02:04s
epoch 18 | loss: 0.69836 | val_0_rmse: 0.82997 | val_1_rmse: 0.87391 |  0:02:11s
epoch 19 | loss: 0.68934 | val_0_rmse: 0.83085 | val_1_rmse: 0.89394 |  0:02:18s
epoch 20 | loss: 0.68481 | val_0_rmse: 0.82103 | val_1_rmse: 0.88    |  0:02:25s
epoch 21 | loss: 0.68026 | val_0_rmse: 0.83811 | val_1_rmse: 0.87963 |  0:02:32s
epoch 22 | loss: 0.67089 | val_0_rmse: 0.87951 | val_1_rmse: 0.88289 |  0:02:39s
epoch 23 | loss: 0.66669 | val_0_rmse: 0.81951 | val_1_rmse: 0.88243 |  0:02:46s
epoch 24 | loss: 0.65892 | val_0_rmse: 0.81466 | val_1_rmse: 0.88976 |  0:02:53s
epoch 25 | loss: 0.65425 | val_0_rmse: 0.815   | val_1_rmse: 0.88048 |  0:02:59s
epoch 26 | loss: 0.6512  | val_0_rmse: 0.80252 | val_1_rmse: 0.88255 |  0:03:06s
epoch 27 | loss: 0.64455 | val_0_rmse: 0.79603 | val_1_rmse: 0.88151 |  0:03:13s
epoch 28 | loss: 0.64098 | val_0_rmse: 0.83127 | val_1_rmse: 0.92723 |  0:03:20s
epoch 29 | loss: 0.63828 | val_0_rmse: 0.83556 | val_1_rmse: 0.91705 |  0:03:27s
epoch 30 | loss: 0.64523 | val_0_rmse: 0.80678 | val_1_rmse: 0.90103 |  0:03:34s
epoch 31 | loss: 0.64109 | val_0_rmse: 0.80135 | val_1_rmse: 0.89549 |  0:03:41s
epoch 32 | loss: 0.63437 | val_0_rmse: 0.79688 | val_1_rmse: 0.8845  |  0:03:48s
epoch 33 | loss: 0.62688 | val_0_rmse: 0.80491 | val_1_rmse: 0.89324 |  0:03:55s
epoch 34 | loss: 0.62953 | val_0_rmse: 0.79579 | val_1_rmse: 0.88834 |  0:04:02s
epoch 35 | loss: 0.62346 | val_0_rmse: 0.79151 | val_1_rmse: 0.8949  |  0:04:09s
epoch 36 | loss: 0.6187  | val_0_rmse: 0.79944 | val_1_rmse: 0.88899 |  0:04:16s
epoch 37 | loss: 0.61301 | val_0_rmse: 0.80162 | val_1_rmse: 0.9088  |  0:04:23s
epoch 38 | loss: 0.60888 | val_0_rmse: 0.77876 | val_1_rmse: 0.90187 |  0:04:30s
epoch 39 | loss: 0.60609 | val_0_rmse: 0.77399 | val_1_rmse: 0.88573 |  0:04:37s
epoch 40 | loss: 0.60469 | val_0_rmse: 0.77961 | val_1_rmse: 0.88228 |  0:04:44s
epoch 41 | loss: 0.60083 | val_0_rmse: 0.77559 | val_1_rmse: 0.88179 |  0:04:51s
epoch 42 | loss: 0.59446 | val_0_rmse: 0.77371 | val_1_rmse: 0.89342 |  0:04:59s
epoch 43 | loss: 0.59029 | val_0_rmse: 0.77326 | val_1_rmse: 0.89334 |  0:05:06s
epoch 44 | loss: 0.59169 | val_0_rmse: 0.77388 | val_1_rmse: 0.89625 |  0:05:13s
epoch 45 | loss: 0.5874  | val_0_rmse: 0.77215 | val_1_rmse: 0.90923 |  0:05:20s
epoch 46 | loss: 0.58409 | val_0_rmse: 0.76408 | val_1_rmse: 0.88378 |  0:05:27s
epoch 47 | loss: 0.58182 | val_0_rmse: 0.76317 | val_1_rmse: 0.89894 |  0:05:34s
epoch 48 | loss: 0.58157 | val_0_rmse: 0.76479 | val_1_rmse: 0.88147 |  0:05:41s

Early stopping occured at epoch 48 with best_epoch = 18 and best_val_1_rmse = 0.87391
Best weights from best epoch are automatically used!
ended training at: 05:27:41
Feature importance:
Mean squared error is of 0.06027583942918015
Mean absolute error:0.18273551774285965
MAPE:0.20029542021585156
R2 score:0.2062886686661668
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:27:44
epoch 0  | loss: 2.66954 | val_0_rmse: 0.99028 | val_1_rmse: 1.01567 |  0:00:01s
epoch 1  | loss: 0.96041 | val_0_rmse: 0.97794 | val_1_rmse: 1.00135 |  0:00:02s
epoch 2  | loss: 0.93446 | val_0_rmse: 0.95842 | val_1_rmse: 0.97674 |  0:00:03s
epoch 3  | loss: 0.88584 | val_0_rmse: 0.94049 | val_1_rmse: 0.95753 |  0:00:04s
epoch 4  | loss: 0.86818 | val_0_rmse: 0.94172 | val_1_rmse: 0.96056 |  0:00:05s
epoch 5  | loss: 0.83856 | val_0_rmse: 0.92811 | val_1_rmse: 0.93702 |  0:00:06s
epoch 6  | loss: 0.81407 | val_0_rmse: 0.91763 | val_1_rmse: 0.92428 |  0:00:07s
epoch 7  | loss: 0.79413 | val_0_rmse: 0.92172 | val_1_rmse: 0.91831 |  0:00:08s
epoch 8  | loss: 0.77526 | val_0_rmse: 0.90615 | val_1_rmse: 0.90413 |  0:00:09s
epoch 9  | loss: 0.75205 | val_0_rmse: 0.8855  | val_1_rmse: 0.88424 |  0:00:10s
epoch 10 | loss: 0.7393  | val_0_rmse: 0.90042 | val_1_rmse: 0.89441 |  0:00:11s
epoch 11 | loss: 0.73456 | val_0_rmse: 0.88576 | val_1_rmse: 0.88843 |  0:00:12s
epoch 12 | loss: 0.72219 | val_0_rmse: 0.88506 | val_1_rmse: 0.89221 |  0:00:13s
epoch 13 | loss: 0.71233 | val_0_rmse: 0.89006 | val_1_rmse: 0.89567 |  0:00:14s
epoch 14 | loss: 0.70236 | val_0_rmse: 0.87931 | val_1_rmse: 0.88557 |  0:00:15s
epoch 15 | loss: 0.69175 | val_0_rmse: 0.87658 | val_1_rmse: 0.88076 |  0:00:16s
epoch 16 | loss: 0.68419 | val_0_rmse: 0.86283 | val_1_rmse: 0.86778 |  0:00:17s
epoch 17 | loss: 0.6784  | val_0_rmse: 0.86381 | val_1_rmse: 0.86949 |  0:00:18s
epoch 18 | loss: 0.66835 | val_0_rmse: 0.85502 | val_1_rmse: 0.86196 |  0:00:19s
epoch 19 | loss: 0.66701 | val_0_rmse: 0.85624 | val_1_rmse: 0.8655  |  0:00:20s
epoch 20 | loss: 0.66578 | val_0_rmse: 0.84898 | val_1_rmse: 0.86081 |  0:00:21s
epoch 21 | loss: 0.65701 | val_0_rmse: 0.86178 | val_1_rmse: 0.86888 |  0:00:22s
epoch 22 | loss: 0.66032 | val_0_rmse: 0.84353 | val_1_rmse: 0.85369 |  0:00:23s
epoch 23 | loss: 0.66332 | val_0_rmse: 0.84352 | val_1_rmse: 0.85242 |  0:00:24s
epoch 24 | loss: 0.65976 | val_0_rmse: 0.84219 | val_1_rmse: 0.84872 |  0:00:25s
epoch 25 | loss: 0.64712 | val_0_rmse: 0.83172 | val_1_rmse: 0.85161 |  0:00:26s
epoch 26 | loss: 0.63846 | val_0_rmse: 0.82692 | val_1_rmse: 0.84024 |  0:00:27s
epoch 27 | loss: 0.63696 | val_0_rmse: 0.8145  | val_1_rmse: 0.83005 |  0:00:28s
epoch 28 | loss: 0.63147 | val_0_rmse: 0.82143 | val_1_rmse: 0.83654 |  0:00:29s
epoch 29 | loss: 0.6318  | val_0_rmse: 0.80994 | val_1_rmse: 0.82825 |  0:00:30s
epoch 30 | loss: 0.62796 | val_0_rmse: 0.81032 | val_1_rmse: 0.8294  |  0:00:31s
epoch 31 | loss: 0.63074 | val_0_rmse: 0.8046  | val_1_rmse: 0.83437 |  0:00:32s
epoch 32 | loss: 0.62159 | val_0_rmse: 0.80405 | val_1_rmse: 0.83177 |  0:00:33s
epoch 33 | loss: 0.61836 | val_0_rmse: 0.79413 | val_1_rmse: 0.82949 |  0:00:34s
epoch 34 | loss: 0.62198 | val_0_rmse: 0.80531 | val_1_rmse: 0.83737 |  0:00:35s
epoch 35 | loss: 0.62285 | val_0_rmse: 0.80647 | val_1_rmse: 0.84224 |  0:00:36s
epoch 36 | loss: 0.62105 | val_0_rmse: 0.79008 | val_1_rmse: 0.83129 |  0:00:37s
epoch 37 | loss: 0.61009 | val_0_rmse: 0.79485 | val_1_rmse: 0.83406 |  0:00:38s
epoch 38 | loss: 0.60704 | val_0_rmse: 0.7814  | val_1_rmse: 0.83198 |  0:00:39s
epoch 39 | loss: 0.59986 | val_0_rmse: 0.77715 | val_1_rmse: 0.8293  |  0:00:40s
epoch 40 | loss: 0.60798 | val_0_rmse: 0.78036 | val_1_rmse: 0.82904 |  0:00:41s
epoch 41 | loss: 0.60485 | val_0_rmse: 0.77252 | val_1_rmse: 0.82263 |  0:00:42s
epoch 42 | loss: 0.5964  | val_0_rmse: 0.7707  | val_1_rmse: 0.82229 |  0:00:43s
epoch 43 | loss: 0.59556 | val_0_rmse: 0.76832 | val_1_rmse: 0.82433 |  0:00:44s
epoch 44 | loss: 0.60227 | val_0_rmse: 0.77213 | val_1_rmse: 0.83254 |  0:00:45s
epoch 45 | loss: 0.59127 | val_0_rmse: 0.7695  | val_1_rmse: 0.83225 |  0:00:46s
epoch 46 | loss: 0.58398 | val_0_rmse: 0.76388 | val_1_rmse: 0.82812 |  0:00:47s
epoch 47 | loss: 0.58809 | val_0_rmse: 0.75798 | val_1_rmse: 0.82125 |  0:00:48s
epoch 48 | loss: 0.58302 | val_0_rmse: 0.78789 | val_1_rmse: 0.83613 |  0:00:49s
epoch 49 | loss: 0.58164 | val_0_rmse: 0.74809 | val_1_rmse: 0.81434 |  0:00:50s
epoch 50 | loss: 0.57711 | val_0_rmse: 0.74075 | val_1_rmse: 0.81755 |  0:00:51s
epoch 51 | loss: 0.5715  | val_0_rmse: 0.74338 | val_1_rmse: 0.81379 |  0:00:52s
epoch 52 | loss: 0.57214 | val_0_rmse: 0.74244 | val_1_rmse: 0.82533 |  0:00:53s
epoch 53 | loss: 0.57169 | val_0_rmse: 0.73632 | val_1_rmse: 0.82292 |  0:00:54s
epoch 54 | loss: 0.55828 | val_0_rmse: 0.73593 | val_1_rmse: 0.82649 |  0:00:55s
epoch 55 | loss: 0.5662  | val_0_rmse: 0.73152 | val_1_rmse: 0.81762 |  0:00:56s
epoch 56 | loss: 0.57036 | val_0_rmse: 0.73146 | val_1_rmse: 0.82508 |  0:00:57s
epoch 57 | loss: 0.55705 | val_0_rmse: 0.72686 | val_1_rmse: 0.83329 |  0:00:58s
epoch 58 | loss: 0.56215 | val_0_rmse: 0.73908 | val_1_rmse: 0.84411 |  0:00:59s
epoch 59 | loss: 0.55857 | val_0_rmse: 0.72311 | val_1_rmse: 0.83578 |  0:01:00s
epoch 60 | loss: 0.55349 | val_0_rmse: 0.72465 | val_1_rmse: 0.831   |  0:01:01s
epoch 61 | loss: 0.549   | val_0_rmse: 0.72021 | val_1_rmse: 0.82172 |  0:01:02s
epoch 62 | loss: 0.54893 | val_0_rmse: 0.71301 | val_1_rmse: 0.82396 |  0:01:03s
epoch 63 | loss: 0.54923 | val_0_rmse: 0.74719 | val_1_rmse: 0.86429 |  0:01:04s
epoch 64 | loss: 0.54622 | val_0_rmse: 0.7143  | val_1_rmse: 0.8291  |  0:01:05s
epoch 65 | loss: 0.53956 | val_0_rmse: 0.72557 | val_1_rmse: 0.82675 |  0:01:06s
epoch 66 | loss: 0.53729 | val_0_rmse: 0.71662 | val_1_rmse: 0.83063 |  0:01:07s
epoch 67 | loss: 0.54159 | val_0_rmse: 0.70556 | val_1_rmse: 0.82778 |  0:01:08s
epoch 68 | loss: 0.53619 | val_0_rmse: 0.70862 | val_1_rmse: 0.82707 |  0:01:09s
epoch 69 | loss: 0.52675 | val_0_rmse: 0.70448 | val_1_rmse: 0.82903 |  0:01:10s
epoch 70 | loss: 0.53078 | val_0_rmse: 0.70844 | val_1_rmse: 0.83736 |  0:01:11s
epoch 71 | loss: 0.53276 | val_0_rmse: 0.70597 | val_1_rmse: 0.83261 |  0:01:12s
epoch 72 | loss: 0.52762 | val_0_rmse: 0.70347 | val_1_rmse: 0.84821 |  0:01:13s
epoch 73 | loss: 0.52844 | val_0_rmse: 0.70275 | val_1_rmse: 0.83726 |  0:01:14s
epoch 74 | loss: 0.52275 | val_0_rmse: 0.69377 | val_1_rmse: 0.84106 |  0:01:15s
epoch 75 | loss: 0.52339 | val_0_rmse: 0.69744 | val_1_rmse: 0.83139 |  0:01:16s
epoch 76 | loss: 0.52362 | val_0_rmse: 0.70383 | val_1_rmse: 0.82699 |  0:01:17s
epoch 77 | loss: 0.51626 | val_0_rmse: 0.69986 | val_1_rmse: 0.85916 |  0:01:18s
epoch 78 | loss: 0.51942 | val_0_rmse: 0.6937  | val_1_rmse: 0.85074 |  0:01:19s
epoch 79 | loss: 0.5161  | val_0_rmse: 0.68957 | val_1_rmse: 0.83191 |  0:01:20s
epoch 80 | loss: 0.51204 | val_0_rmse: 0.68838 | val_1_rmse: 0.84973 |  0:01:21s
epoch 81 | loss: 0.51386 | val_0_rmse: 0.68814 | val_1_rmse: 0.85502 |  0:01:22s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.81379
Best weights from best epoch are automatically used!
ended training at: 05:29:07
Feature importance:
Mean squared error is of 0.09647745452611792
Mean absolute error:0.18481367366435814
MAPE:0.1966436050969836
R2 score:0.24147865864665385
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:29:08
epoch 0  | loss: 2.61795 | val_0_rmse: 0.9886  | val_1_rmse: 1.01275 |  0:00:00s
epoch 1  | loss: 1.01655 | val_0_rmse: 0.98343 | val_1_rmse: 1.00659 |  0:00:01s
epoch 2  | loss: 0.95583 | val_0_rmse: 0.96399 | val_1_rmse: 0.98818 |  0:00:03s
epoch 3  | loss: 0.89914 | val_0_rmse: 0.94612 | val_1_rmse: 0.96561 |  0:00:04s
epoch 4  | loss: 0.87024 | val_0_rmse: 0.94565 | val_1_rmse: 0.96494 |  0:00:05s
epoch 5  | loss: 0.83013 | val_0_rmse: 0.93251 | val_1_rmse: 0.94775 |  0:00:06s
epoch 6  | loss: 0.80663 | val_0_rmse: 0.91053 | val_1_rmse: 0.92631 |  0:00:07s
epoch 7  | loss: 0.79094 | val_0_rmse: 0.90204 | val_1_rmse: 0.92225 |  0:00:08s
epoch 8  | loss: 0.78839 | val_0_rmse: 0.88882 | val_1_rmse: 0.91291 |  0:00:09s
epoch 9  | loss: 0.78146 | val_0_rmse: 0.91835 | val_1_rmse: 0.94312 |  0:00:10s
epoch 10 | loss: 0.79272 | val_0_rmse: 0.9148  | val_1_rmse: 0.94092 |  0:00:11s
epoch 11 | loss: 0.77839 | val_0_rmse: 0.94187 | val_1_rmse: 0.9625  |  0:00:12s
epoch 12 | loss: 0.76776 | val_0_rmse: 0.87796 | val_1_rmse: 0.90403 |  0:00:13s
epoch 13 | loss: 0.75387 | val_0_rmse: 0.89018 | val_1_rmse: 0.91764 |  0:00:14s
epoch 14 | loss: 0.74217 | val_0_rmse: 0.88266 | val_1_rmse: 0.90817 |  0:00:15s
epoch 15 | loss: 0.7368  | val_0_rmse: 0.86844 | val_1_rmse: 0.89356 |  0:00:16s
epoch 16 | loss: 0.73435 | val_0_rmse: 0.86749 | val_1_rmse: 0.88919 |  0:00:17s
epoch 17 | loss: 0.72259 | val_0_rmse: 0.85808 | val_1_rmse: 0.88145 |  0:00:18s
epoch 18 | loss: 0.71297 | val_0_rmse: 0.85253 | val_1_rmse: 0.8752  |  0:00:19s
epoch 19 | loss: 0.70933 | val_0_rmse: 0.86438 | val_1_rmse: 0.88873 |  0:00:20s
epoch 20 | loss: 0.70968 | val_0_rmse: 0.84821 | val_1_rmse: 0.871   |  0:00:21s
epoch 21 | loss: 0.70836 | val_0_rmse: 0.84793 | val_1_rmse: 0.87243 |  0:00:22s
epoch 22 | loss: 0.70772 | val_0_rmse: 0.8582  | val_1_rmse: 0.88203 |  0:00:23s
epoch 23 | loss: 0.704   | val_0_rmse: 0.8486  | val_1_rmse: 0.8794  |  0:00:24s
epoch 24 | loss: 0.69845 | val_0_rmse: 0.84423 | val_1_rmse: 0.87476 |  0:00:25s
epoch 25 | loss: 0.70414 | val_0_rmse: 0.84745 | val_1_rmse: 0.87569 |  0:00:26s
epoch 26 | loss: 0.71768 | val_0_rmse: 0.86315 | val_1_rmse: 0.88558 |  0:00:27s
epoch 27 | loss: 0.75023 | val_0_rmse: 0.87966 | val_1_rmse: 0.8956  |  0:00:28s
epoch 28 | loss: 0.76843 | val_0_rmse: 0.87069 | val_1_rmse: 0.89501 |  0:00:29s
epoch 29 | loss: 0.75305 | val_0_rmse: 0.86938 | val_1_rmse: 0.90048 |  0:00:30s
epoch 30 | loss: 0.74607 | val_0_rmse: 0.87861 | val_1_rmse: 0.90551 |  0:00:31s
epoch 31 | loss: 0.74839 | val_0_rmse: 0.86932 | val_1_rmse: 0.89553 |  0:00:32s
epoch 32 | loss: 0.7327  | val_0_rmse: 0.8488  | val_1_rmse: 0.87671 |  0:00:33s
epoch 33 | loss: 0.71962 | val_0_rmse: 0.83782 | val_1_rmse: 0.86991 |  0:00:34s
epoch 34 | loss: 0.70952 | val_0_rmse: 0.83486 | val_1_rmse: 0.86775 |  0:00:35s
epoch 35 | loss: 0.70115 | val_0_rmse: 0.8438  | val_1_rmse: 0.88189 |  0:00:36s
epoch 36 | loss: 0.70325 | val_0_rmse: 0.838   | val_1_rmse: 0.87473 |  0:00:37s
epoch 37 | loss: 0.69113 | val_0_rmse: 0.82543 | val_1_rmse: 0.86214 |  0:00:38s
epoch 38 | loss: 0.6918  | val_0_rmse: 0.82351 | val_1_rmse: 0.8624  |  0:00:39s
epoch 39 | loss: 0.68863 | val_0_rmse: 0.82063 | val_1_rmse: 0.85993 |  0:00:40s
epoch 40 | loss: 0.69197 | val_0_rmse: 0.81657 | val_1_rmse: 0.85542 |  0:00:41s
epoch 41 | loss: 0.68055 | val_0_rmse: 0.81875 | val_1_rmse: 0.85995 |  0:00:42s
epoch 42 | loss: 0.67209 | val_0_rmse: 0.80802 | val_1_rmse: 0.85306 |  0:00:43s
epoch 43 | loss: 0.65889 | val_0_rmse: 0.81808 | val_1_rmse: 0.85817 |  0:00:44s
epoch 44 | loss: 0.65099 | val_0_rmse: 0.82146 | val_1_rmse: 0.86552 |  0:00:45s
epoch 45 | loss: 0.65045 | val_0_rmse: 0.80276 | val_1_rmse: 0.85506 |  0:00:46s
epoch 46 | loss: 0.64337 | val_0_rmse: 0.79876 | val_1_rmse: 0.85466 |  0:00:47s
epoch 47 | loss: 0.64469 | val_0_rmse: 0.96949 | val_1_rmse: 1.01984 |  0:00:48s
epoch 48 | loss: 0.6574  | val_0_rmse: 0.80297 | val_1_rmse: 0.8573  |  0:00:49s
epoch 49 | loss: 0.65081 | val_0_rmse: 0.80424 | val_1_rmse: 0.85485 |  0:00:50s
epoch 50 | loss: 0.64045 | val_0_rmse: 0.80425 | val_1_rmse: 0.86151 |  0:00:51s
epoch 51 | loss: 0.64589 | val_0_rmse: 0.8237  | val_1_rmse: 0.87828 |  0:00:52s
epoch 52 | loss: 0.6599  | val_0_rmse: 0.88813 | val_1_rmse: 0.94329 |  0:00:53s
epoch 53 | loss: 0.65481 | val_0_rmse: 0.7953  | val_1_rmse: 0.8554  |  0:00:54s
epoch 54 | loss: 0.64865 | val_0_rmse: 0.79331 | val_1_rmse: 0.85736 |  0:00:55s
epoch 55 | loss: 0.63887 | val_0_rmse: 0.83094 | val_1_rmse: 0.8823  |  0:00:56s
epoch 56 | loss: 0.63859 | val_0_rmse: 0.79342 | val_1_rmse: 0.86731 |  0:00:57s
epoch 57 | loss: 0.63147 | val_0_rmse: 0.78907 | val_1_rmse: 0.86549 |  0:00:58s
epoch 58 | loss: 0.64436 | val_0_rmse: 0.8024  | val_1_rmse: 0.87626 |  0:00:59s
epoch 59 | loss: 0.63865 | val_0_rmse: 0.83807 | val_1_rmse: 0.9053  |  0:01:00s
epoch 60 | loss: 0.64283 | val_0_rmse: 0.78482 | val_1_rmse: 0.85713 |  0:01:01s
epoch 61 | loss: 0.63252 | val_0_rmse: 0.79236 | val_1_rmse: 0.86223 |  0:01:02s
epoch 62 | loss: 0.63829 | val_0_rmse: 0.78544 | val_1_rmse: 0.86274 |  0:01:03s
epoch 63 | loss: 0.63119 | val_0_rmse: 0.78687 | val_1_rmse: 0.86236 |  0:01:04s
epoch 64 | loss: 0.63396 | val_0_rmse: 0.78263 | val_1_rmse: 0.85509 |  0:01:05s
epoch 65 | loss: 0.6302  | val_0_rmse: 0.79014 | val_1_rmse: 0.86607 |  0:01:06s
epoch 66 | loss: 0.6241  | val_0_rmse: 0.82005 | val_1_rmse: 0.88774 |  0:01:07s
epoch 67 | loss: 0.62244 | val_0_rmse: 0.7828  | val_1_rmse: 0.87032 |  0:01:08s
epoch 68 | loss: 0.61046 | val_0_rmse: 0.7912  | val_1_rmse: 0.87682 |  0:01:09s
epoch 69 | loss: 0.61316 | val_0_rmse: 0.76875 | val_1_rmse: 0.86223 |  0:01:10s
epoch 70 | loss: 0.60773 | val_0_rmse: 0.76882 | val_1_rmse: 0.86117 |  0:01:11s
epoch 71 | loss: 0.60776 | val_0_rmse: 0.76947 | val_1_rmse: 0.86695 |  0:01:12s
epoch 72 | loss: 0.6099  | val_0_rmse: 0.76546 | val_1_rmse: 0.87356 |  0:01:13s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.85306
Best weights from best epoch are automatically used!
ended training at: 05:30:22
Feature importance:
Mean squared error is of 0.09473142840884863
Mean absolute error:0.18692471359096266
MAPE:0.20077180563628305
R2 score:0.21362989878665484
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:30:23
epoch 0  | loss: 2.58164 | val_0_rmse: 1.00263 | val_1_rmse: 0.98923 |  0:00:00s
epoch 1  | loss: 1.0263  | val_0_rmse: 0.99821 | val_1_rmse: 0.98662 |  0:00:02s
epoch 2  | loss: 1.0066  | val_0_rmse: 1.00521 | val_1_rmse: 0.99388 |  0:00:03s
epoch 3  | loss: 0.99325 | val_0_rmse: 1.00746 | val_1_rmse: 0.99699 |  0:00:04s
epoch 4  | loss: 0.97987 | val_0_rmse: 0.99781 | val_1_rmse: 0.99203 |  0:00:05s
epoch 5  | loss: 0.96486 | val_0_rmse: 0.98732 | val_1_rmse: 0.98286 |  0:00:06s
epoch 6  | loss: 0.94107 | val_0_rmse: 0.97469 | val_1_rmse: 0.9718  |  0:00:07s
epoch 7  | loss: 0.91767 | val_0_rmse: 0.9775  | val_1_rmse: 0.97926 |  0:00:08s
epoch 8  | loss: 0.87921 | val_0_rmse: 0.97341 | val_1_rmse: 0.98031 |  0:00:09s
epoch 9  | loss: 0.84562 | val_0_rmse: 0.91641 | val_1_rmse: 0.92719 |  0:00:10s
epoch 10 | loss: 0.80888 | val_0_rmse: 0.90549 | val_1_rmse: 0.91738 |  0:00:11s
epoch 11 | loss: 0.78458 | val_0_rmse: 0.89028 | val_1_rmse: 0.90698 |  0:00:12s
epoch 12 | loss: 0.76601 | val_0_rmse: 0.89459 | val_1_rmse: 0.91461 |  0:00:13s
epoch 13 | loss: 0.7494  | val_0_rmse: 0.88518 | val_1_rmse: 0.89935 |  0:00:14s
epoch 14 | loss: 0.73947 | val_0_rmse: 0.86051 | val_1_rmse: 0.86959 |  0:00:15s
epoch 15 | loss: 0.73577 | val_0_rmse: 0.86281 | val_1_rmse: 0.87473 |  0:00:16s
epoch 16 | loss: 0.72131 | val_0_rmse: 0.86477 | val_1_rmse: 0.87825 |  0:00:17s
epoch 17 | loss: 0.71257 | val_0_rmse: 0.86359 | val_1_rmse: 0.87426 |  0:00:18s
epoch 18 | loss: 0.70771 | val_0_rmse: 0.86048 | val_1_rmse: 0.86839 |  0:00:19s
epoch 19 | loss: 0.70306 | val_0_rmse: 0.85746 | val_1_rmse: 0.86676 |  0:00:20s
epoch 20 | loss: 0.69536 | val_0_rmse: 0.86457 | val_1_rmse: 0.87574 |  0:00:21s
epoch 21 | loss: 0.68955 | val_0_rmse: 0.85469 | val_1_rmse: 0.86844 |  0:00:22s
epoch 22 | loss: 0.68213 | val_0_rmse: 0.84769 | val_1_rmse: 0.85841 |  0:00:23s
epoch 23 | loss: 0.68065 | val_0_rmse: 0.84284 | val_1_rmse: 0.85959 |  0:00:24s
epoch 24 | loss: 0.67579 | val_0_rmse: 0.84116 | val_1_rmse: 0.85976 |  0:00:25s
epoch 25 | loss: 0.66805 | val_0_rmse: 0.83301 | val_1_rmse: 0.85205 |  0:00:26s
epoch 26 | loss: 0.66421 | val_0_rmse: 0.87516 | val_1_rmse: 0.90602 |  0:00:27s
epoch 27 | loss: 0.66279 | val_0_rmse: 0.82977 | val_1_rmse: 0.85877 |  0:00:28s
epoch 28 | loss: 0.66002 | val_0_rmse: 0.84217 | val_1_rmse: 0.87322 |  0:00:29s
epoch 29 | loss: 0.65118 | val_0_rmse: 0.83695 | val_1_rmse: 0.86629 |  0:00:30s
epoch 30 | loss: 0.65778 | val_0_rmse: 0.82541 | val_1_rmse: 0.85634 |  0:00:31s
epoch 31 | loss: 0.6537  | val_0_rmse: 0.83066 | val_1_rmse: 0.85417 |  0:00:32s
epoch 32 | loss: 0.64306 | val_0_rmse: 0.81547 | val_1_rmse: 0.8452  |  0:00:33s
epoch 33 | loss: 0.6395  | val_0_rmse: 0.81062 | val_1_rmse: 0.84288 |  0:00:34s
epoch 34 | loss: 0.6426  | val_0_rmse: 0.80812 | val_1_rmse: 0.84041 |  0:00:35s
epoch 35 | loss: 0.64689 | val_0_rmse: 0.81614 | val_1_rmse: 0.84423 |  0:00:36s
epoch 36 | loss: 0.63601 | val_0_rmse: 0.79912 | val_1_rmse: 0.83393 |  0:00:37s
epoch 37 | loss: 0.63041 | val_0_rmse: 0.80396 | val_1_rmse: 0.84105 |  0:00:38s
epoch 38 | loss: 0.63023 | val_0_rmse: 0.80042 | val_1_rmse: 0.84169 |  0:00:39s
epoch 39 | loss: 0.6253  | val_0_rmse: 0.83901 | val_1_rmse: 0.87683 |  0:00:40s
epoch 40 | loss: 0.66136 | val_0_rmse: 0.81469 | val_1_rmse: 0.84358 |  0:00:41s
epoch 41 | loss: 0.65615 | val_0_rmse: 0.81289 | val_1_rmse: 0.83401 |  0:00:42s
epoch 42 | loss: 0.64058 | val_0_rmse: 0.81448 | val_1_rmse: 0.8354  |  0:00:43s
epoch 43 | loss: 0.62997 | val_0_rmse: 0.82109 | val_1_rmse: 0.84407 |  0:00:44s
epoch 44 | loss: 0.62754 | val_0_rmse: 0.79056 | val_1_rmse: 0.82357 |  0:00:45s
epoch 45 | loss: 0.61949 | val_0_rmse: 0.80222 | val_1_rmse: 0.84238 |  0:00:46s
epoch 46 | loss: 0.62894 | val_0_rmse: 0.79716 | val_1_rmse: 0.8485  |  0:00:47s
epoch 47 | loss: 0.60982 | val_0_rmse: 0.77228 | val_1_rmse: 0.82765 |  0:00:48s
epoch 48 | loss: 0.60964 | val_0_rmse: 0.81076 | val_1_rmse: 0.84518 |  0:00:49s
epoch 49 | loss: 0.60488 | val_0_rmse: 0.77985 | val_1_rmse: 0.83791 |  0:00:50s
epoch 50 | loss: 0.59616 | val_0_rmse: 0.76077 | val_1_rmse: 0.83316 |  0:00:51s
epoch 51 | loss: 0.59371 | val_0_rmse: 0.75375 | val_1_rmse: 0.82145 |  0:00:52s
epoch 52 | loss: 0.59139 | val_0_rmse: 0.76045 | val_1_rmse: 0.8379  |  0:00:53s
epoch 53 | loss: 0.58666 | val_0_rmse: 0.75974 | val_1_rmse: 0.82981 |  0:00:54s
epoch 54 | loss: 0.583   | val_0_rmse: 0.74184 | val_1_rmse: 0.82469 |  0:00:55s
epoch 55 | loss: 0.57201 | val_0_rmse: 0.73932 | val_1_rmse: 0.82811 |  0:00:56s
epoch 56 | loss: 0.57001 | val_0_rmse: 0.73456 | val_1_rmse: 0.82695 |  0:00:57s
epoch 57 | loss: 0.5634  | val_0_rmse: 0.73657 | val_1_rmse: 0.82227 |  0:00:58s
epoch 58 | loss: 0.56732 | val_0_rmse: 0.74574 | val_1_rmse: 0.84122 |  0:00:59s
epoch 59 | loss: 0.58132 | val_0_rmse: 0.75768 | val_1_rmse: 0.83973 |  0:01:00s
epoch 60 | loss: 0.57404 | val_0_rmse: 0.73822 | val_1_rmse: 0.83102 |  0:01:01s
epoch 61 | loss: 0.57367 | val_0_rmse: 0.73414 | val_1_rmse: 0.82655 |  0:01:02s
epoch 62 | loss: 0.5606  | val_0_rmse: 0.76218 | val_1_rmse: 0.87003 |  0:01:03s
epoch 63 | loss: 0.57206 | val_0_rmse: 0.73533 | val_1_rmse: 0.8311  |  0:01:04s
epoch 64 | loss: 0.56    | val_0_rmse: 0.72604 | val_1_rmse: 0.8353  |  0:01:05s
epoch 65 | loss: 0.54581 | val_0_rmse: 0.71761 | val_1_rmse: 0.8295  |  0:01:06s
epoch 66 | loss: 0.54796 | val_0_rmse: 0.75014 | val_1_rmse: 0.88733 |  0:01:07s
epoch 67 | loss: 0.55409 | val_0_rmse: 0.72353 | val_1_rmse: 0.84294 |  0:01:08s
epoch 68 | loss: 0.55037 | val_0_rmse: 0.71516 | val_1_rmse: 0.82968 |  0:01:09s
epoch 69 | loss: 0.54402 | val_0_rmse: 0.72028 | val_1_rmse: 0.82576 |  0:01:10s
epoch 70 | loss: 0.53971 | val_0_rmse: 0.71648 | val_1_rmse: 0.8476  |  0:01:11s
epoch 71 | loss: 0.53892 | val_0_rmse: 0.73821 | val_1_rmse: 0.8955  |  0:01:12s
epoch 72 | loss: 0.53488 | val_0_rmse: 0.7176  | val_1_rmse: 0.82293 |  0:01:13s
epoch 73 | loss: 0.54102 | val_0_rmse: 0.78719 | val_1_rmse: 0.94884 |  0:01:14s
epoch 74 | loss: 0.53767 | val_0_rmse: 0.70878 | val_1_rmse: 0.8668  |  0:01:15s
epoch 75 | loss: 0.53109 | val_0_rmse: 0.70551 | val_1_rmse: 0.8722  |  0:01:16s
epoch 76 | loss: 0.52866 | val_0_rmse: 0.73206 | val_1_rmse: 0.85786 |  0:01:17s
epoch 77 | loss: 0.53116 | val_0_rmse: 0.72582 | val_1_rmse: 0.8402  |  0:01:18s
epoch 78 | loss: 0.53106 | val_0_rmse: 0.70119 | val_1_rmse: 0.86598 |  0:01:19s
epoch 79 | loss: 0.51558 | val_0_rmse: 0.70308 | val_1_rmse: 0.86831 |  0:01:20s
epoch 80 | loss: 0.51731 | val_0_rmse: 0.69473 | val_1_rmse: 0.85628 |  0:01:21s
epoch 81 | loss: 0.52211 | val_0_rmse: 0.69932 | val_1_rmse: 0.85846 |  0:01:22s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.82145
Best weights from best epoch are automatically used!
ended training at: 05:31:46
Feature importance:
Mean squared error is of 0.06862423625865144
Mean absolute error:0.18398268899858655
MAPE:0.19721337035121828
R2 score:0.29179463709875575
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:31:46
epoch 0  | loss: 2.24504 | val_0_rmse: 0.99548 | val_1_rmse: 1.01959 |  0:00:00s
epoch 1  | loss: 1.02163 | val_0_rmse: 0.99588 | val_1_rmse: 1.01965 |  0:00:02s
epoch 2  | loss: 0.96    | val_0_rmse: 0.98328 | val_1_rmse: 1.00823 |  0:00:03s
epoch 3  | loss: 0.93007 | val_0_rmse: 0.9918  | val_1_rmse: 1.01656 |  0:00:04s
epoch 4  | loss: 0.88742 | val_0_rmse: 0.92529 | val_1_rmse: 0.94941 |  0:00:05s
epoch 5  | loss: 0.87498 | val_0_rmse: 0.93054 | val_1_rmse: 0.95155 |  0:00:06s
epoch 6  | loss: 0.85152 | val_0_rmse: 0.92505 | val_1_rmse: 0.95137 |  0:00:07s
epoch 7  | loss: 0.83454 | val_0_rmse: 0.90002 | val_1_rmse: 0.92834 |  0:00:08s
epoch 8  | loss: 0.80825 | val_0_rmse: 0.90074 | val_1_rmse: 0.93412 |  0:00:09s
epoch 9  | loss: 0.79405 | val_0_rmse: 0.90164 | val_1_rmse: 0.92897 |  0:00:10s
epoch 10 | loss: 0.78013 | val_0_rmse: 0.89907 | val_1_rmse: 0.92921 |  0:00:11s
epoch 11 | loss: 0.76344 | val_0_rmse: 0.90615 | val_1_rmse: 0.93977 |  0:00:12s
epoch 12 | loss: 0.75347 | val_0_rmse: 0.8864  | val_1_rmse: 0.91772 |  0:00:13s
epoch 13 | loss: 0.73982 | val_0_rmse: 0.88348 | val_1_rmse: 0.91483 |  0:00:14s
epoch 14 | loss: 0.73019 | val_0_rmse: 0.86657 | val_1_rmse: 0.89919 |  0:00:15s
epoch 15 | loss: 0.72328 | val_0_rmse: 0.86552 | val_1_rmse: 0.89597 |  0:00:16s
epoch 16 | loss: 0.70618 | val_0_rmse: 0.86945 | val_1_rmse: 0.90371 |  0:00:17s
epoch 17 | loss: 0.69376 | val_0_rmse: 0.85651 | val_1_rmse: 0.89034 |  0:00:18s
epoch 18 | loss: 0.69133 | val_0_rmse: 0.85687 | val_1_rmse: 0.88892 |  0:00:19s
epoch 19 | loss: 0.68627 | val_0_rmse: 0.85309 | val_1_rmse: 0.88744 |  0:00:20s
epoch 20 | loss: 0.67713 | val_0_rmse: 0.86705 | val_1_rmse: 0.90651 |  0:00:21s
epoch 21 | loss: 0.67926 | val_0_rmse: 0.85186 | val_1_rmse: 0.89127 |  0:00:22s
epoch 22 | loss: 0.66124 | val_0_rmse: 0.84349 | val_1_rmse: 0.88264 |  0:00:23s
epoch 23 | loss: 0.66631 | val_0_rmse: 0.84993 | val_1_rmse: 0.88616 |  0:00:24s
epoch 24 | loss: 0.65753 | val_0_rmse: 0.84377 | val_1_rmse: 0.88588 |  0:00:25s
epoch 25 | loss: 0.65164 | val_0_rmse: 0.83687 | val_1_rmse: 0.87557 |  0:00:26s
epoch 26 | loss: 0.6556  | val_0_rmse: 0.84196 | val_1_rmse: 0.88638 |  0:00:27s
epoch 27 | loss: 0.65009 | val_0_rmse: 0.83803 | val_1_rmse: 0.88005 |  0:00:28s
epoch 28 | loss: 0.64286 | val_0_rmse: 0.82867 | val_1_rmse: 0.87229 |  0:00:29s
epoch 29 | loss: 0.64621 | val_0_rmse: 0.82514 | val_1_rmse: 0.87656 |  0:00:30s
epoch 30 | loss: 0.64687 | val_0_rmse: 0.81902 | val_1_rmse: 0.87531 |  0:00:31s
epoch 31 | loss: 0.644   | val_0_rmse: 0.8261  | val_1_rmse: 0.87981 |  0:00:32s
epoch 32 | loss: 0.6393  | val_0_rmse: 0.80916 | val_1_rmse: 0.86987 |  0:00:33s
epoch 33 | loss: 0.6294  | val_0_rmse: 0.80981 | val_1_rmse: 0.86246 |  0:00:34s
epoch 34 | loss: 0.63091 | val_0_rmse: 0.81029 | val_1_rmse: 0.87039 |  0:00:35s
epoch 35 | loss: 0.62802 | val_0_rmse: 0.81928 | val_1_rmse: 0.88063 |  0:00:36s
epoch 36 | loss: 0.63267 | val_0_rmse: 0.7987  | val_1_rmse: 0.86297 |  0:00:37s
epoch 37 | loss: 0.62335 | val_0_rmse: 0.80042 | val_1_rmse: 0.87395 |  0:00:38s
epoch 38 | loss: 0.61819 | val_0_rmse: 0.79044 | val_1_rmse: 0.85683 |  0:00:39s
epoch 39 | loss: 0.62495 | val_0_rmse: 0.79134 | val_1_rmse: 0.86212 |  0:00:40s
epoch 40 | loss: 0.62081 | val_0_rmse: 0.79649 | val_1_rmse: 0.86896 |  0:00:41s
epoch 41 | loss: 0.63497 | val_0_rmse: 0.7927  | val_1_rmse: 0.87013 |  0:00:42s
epoch 42 | loss: 0.61882 | val_0_rmse: 0.7862  | val_1_rmse: 0.86112 |  0:00:43s
epoch 43 | loss: 0.61078 | val_0_rmse: 0.77721 | val_1_rmse: 0.84722 |  0:00:44s
epoch 44 | loss: 0.60849 | val_0_rmse: 0.78112 | val_1_rmse: 0.86357 |  0:00:45s
epoch 45 | loss: 0.60264 | val_0_rmse: 0.76426 | val_1_rmse: 0.85412 |  0:00:46s
epoch 46 | loss: 0.61036 | val_0_rmse: 0.77091 | val_1_rmse: 0.84949 |  0:00:47s
epoch 47 | loss: 0.60631 | val_0_rmse: 0.76431 | val_1_rmse: 0.8534  |  0:00:48s
epoch 48 | loss: 0.58982 | val_0_rmse: 0.76557 | val_1_rmse: 0.85461 |  0:00:49s
epoch 49 | loss: 0.59471 | val_0_rmse: 0.75506 | val_1_rmse: 0.84414 |  0:00:50s
epoch 50 | loss: 0.59249 | val_0_rmse: 0.74947 | val_1_rmse: 0.85412 |  0:00:51s
epoch 51 | loss: 0.58008 | val_0_rmse: 0.74919 | val_1_rmse: 0.85206 |  0:00:52s
epoch 52 | loss: 0.59289 | val_0_rmse: 0.74502 | val_1_rmse: 0.84901 |  0:00:53s
epoch 53 | loss: 0.59348 | val_0_rmse: 0.75413 | val_1_rmse: 0.85417 |  0:00:54s
epoch 54 | loss: 0.58462 | val_0_rmse: 0.74591 | val_1_rmse: 0.85436 |  0:00:55s
epoch 55 | loss: 0.58102 | val_0_rmse: 0.74783 | val_1_rmse: 0.85498 |  0:00:56s
epoch 56 | loss: 0.57882 | val_0_rmse: 0.73799 | val_1_rmse: 0.85064 |  0:00:57s
epoch 57 | loss: 0.57366 | val_0_rmse: 0.73116 | val_1_rmse: 0.86102 |  0:00:58s
epoch 58 | loss: 0.57097 | val_0_rmse: 0.73551 | val_1_rmse: 0.85662 |  0:00:59s
epoch 59 | loss: 0.57964 | val_0_rmse: 0.7495  | val_1_rmse: 0.86869 |  0:01:00s
epoch 60 | loss: 0.56567 | val_0_rmse: 0.72485 | val_1_rmse: 0.85992 |  0:01:01s
epoch 61 | loss: 0.56654 | val_0_rmse: 0.73057 | val_1_rmse: 0.86176 |  0:01:02s
epoch 62 | loss: 0.56775 | val_0_rmse: 0.72697 | val_1_rmse: 0.86153 |  0:01:03s
epoch 63 | loss: 0.56242 | val_0_rmse: 0.7358  | val_1_rmse: 0.86461 |  0:01:04s
epoch 64 | loss: 0.56059 | val_0_rmse: 0.72167 | val_1_rmse: 0.86996 |  0:01:05s
epoch 65 | loss: 0.55417 | val_0_rmse: 0.72172 | val_1_rmse: 0.86628 |  0:01:06s
epoch 66 | loss: 0.56078 | val_0_rmse: 0.72008 | val_1_rmse: 0.85628 |  0:01:07s
epoch 67 | loss: 0.55645 | val_0_rmse: 0.72276 | val_1_rmse: 0.86939 |  0:01:08s
epoch 68 | loss: 0.55411 | val_0_rmse: 0.71948 | val_1_rmse: 0.86943 |  0:01:09s
epoch 69 | loss: 0.54942 | val_0_rmse: 0.71236 | val_1_rmse: 0.87001 |  0:01:10s
epoch 70 | loss: 0.55265 | val_0_rmse: 0.7354  | val_1_rmse: 0.88529 |  0:01:11s
epoch 71 | loss: 0.5644  | val_0_rmse: 0.72872 | val_1_rmse: 0.86018 |  0:01:12s
epoch 72 | loss: 0.55501 | val_0_rmse: 0.72087 | val_1_rmse: 0.85825 |  0:01:13s
epoch 73 | loss: 0.5477  | val_0_rmse: 0.71354 | val_1_rmse: 0.86515 |  0:01:14s
epoch 74 | loss: 0.53876 | val_0_rmse: 0.71536 | val_1_rmse: 0.86183 |  0:01:15s
epoch 75 | loss: 0.54153 | val_0_rmse: 0.71054 | val_1_rmse: 0.86041 |  0:01:16s
epoch 76 | loss: 0.5492  | val_0_rmse: 0.72185 | val_1_rmse: 0.87709 |  0:01:17s
epoch 77 | loss: 0.54141 | val_0_rmse: 0.71752 | val_1_rmse: 0.8908  |  0:01:18s
epoch 78 | loss: 0.54959 | val_0_rmse: 0.72066 | val_1_rmse: 0.87555 |  0:01:19s
epoch 79 | loss: 0.54657 | val_0_rmse: 0.71881 | val_1_rmse: 0.87892 |  0:01:20s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.84414
Best weights from best epoch are automatically used!
ended training at: 05:33:07
Feature importance:
Mean squared error is of 0.06806213173684295
Mean absolute error:0.18324500636696087
MAPE:0.19448271099616507
R2 score:0.26402250383442094
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:33:08
epoch 0  | loss: 2.44186 | val_0_rmse: 0.99742 | val_1_rmse: 1.04169 |  0:00:00s
epoch 1  | loss: 1.01522 | val_0_rmse: 0.99716 | val_1_rmse: 1.04201 |  0:00:02s
epoch 2  | loss: 0.97963 | val_0_rmse: 0.98999 | val_1_rmse: 1.03355 |  0:00:02s
epoch 3  | loss: 0.96381 | val_0_rmse: 0.9763  | val_1_rmse: 1.0182  |  0:00:04s
epoch 4  | loss: 0.94027 | val_0_rmse: 0.96815 | val_1_rmse: 1.00789 |  0:00:05s
epoch 5  | loss: 0.90507 | val_0_rmse: 0.94724 | val_1_rmse: 0.98526 |  0:00:06s
epoch 6  | loss: 0.86395 | val_0_rmse: 0.93407 | val_1_rmse: 0.96691 |  0:00:07s
epoch 7  | loss: 0.83514 | val_0_rmse: 0.93736 | val_1_rmse: 0.97051 |  0:00:08s
epoch 8  | loss: 0.8319  | val_0_rmse: 0.93386 | val_1_rmse: 0.97125 |  0:00:09s
epoch 9  | loss: 0.82206 | val_0_rmse: 0.9315  | val_1_rmse: 0.96189 |  0:00:10s
epoch 10 | loss: 0.8235  | val_0_rmse: 0.93384 | val_1_rmse: 0.97113 |  0:00:11s
epoch 11 | loss: 0.81471 | val_0_rmse: 0.93898 | val_1_rmse: 0.97195 |  0:00:12s
epoch 12 | loss: 0.79254 | val_0_rmse: 0.89937 | val_1_rmse: 0.92902 |  0:00:13s
epoch 13 | loss: 0.78141 | val_0_rmse: 0.89068 | val_1_rmse: 0.91828 |  0:00:14s
epoch 14 | loss: 0.76637 | val_0_rmse: 0.89015 | val_1_rmse: 0.92209 |  0:00:15s
epoch 15 | loss: 0.74637 | val_0_rmse: 0.89353 | val_1_rmse: 0.92571 |  0:00:16s
epoch 16 | loss: 0.73014 | val_0_rmse: 0.87667 | val_1_rmse: 0.91244 |  0:00:17s
epoch 17 | loss: 0.72196 | val_0_rmse: 0.8643  | val_1_rmse: 0.89363 |  0:00:18s
epoch 18 | loss: 0.72134 | val_0_rmse: 0.87391 | val_1_rmse: 0.90449 |  0:00:19s
epoch 19 | loss: 0.7124  | val_0_rmse: 0.87262 | val_1_rmse: 0.90669 |  0:00:20s
epoch 20 | loss: 0.71407 | val_0_rmse: 0.90171 | val_1_rmse: 0.93166 |  0:00:21s
epoch 21 | loss: 0.73734 | val_0_rmse: 0.86765 | val_1_rmse: 0.9016  |  0:00:22s
epoch 22 | loss: 0.71951 | val_0_rmse: 0.86555 | val_1_rmse: 0.89678 |  0:00:23s
epoch 23 | loss: 0.729   | val_0_rmse: 0.88448 | val_1_rmse: 0.92034 |  0:00:24s
epoch 24 | loss: 0.74753 | val_0_rmse: 0.8751  | val_1_rmse: 0.91279 |  0:00:25s
epoch 25 | loss: 0.74092 | val_0_rmse: 0.85721 | val_1_rmse: 0.89696 |  0:00:26s
epoch 26 | loss: 0.7185  | val_0_rmse: 0.8571  | val_1_rmse: 0.89803 |  0:00:27s
epoch 27 | loss: 0.71012 | val_0_rmse: 0.84893 | val_1_rmse: 0.8861  |  0:00:28s
epoch 28 | loss: 0.69831 | val_0_rmse: 0.84634 | val_1_rmse: 0.8873  |  0:00:29s
epoch 29 | loss: 0.69615 | val_0_rmse: 0.8364  | val_1_rmse: 0.87528 |  0:00:30s
epoch 30 | loss: 0.69741 | val_0_rmse: 0.84874 | val_1_rmse: 0.88052 |  0:00:31s
epoch 31 | loss: 0.70114 | val_0_rmse: 0.83826 | val_1_rmse: 0.87774 |  0:00:32s
epoch 32 | loss: 0.69259 | val_0_rmse: 0.82667 | val_1_rmse: 0.86919 |  0:00:33s
epoch 33 | loss: 0.6797  | val_0_rmse: 0.82904 | val_1_rmse: 0.87184 |  0:00:34s
epoch 34 | loss: 0.67092 | val_0_rmse: 0.82175 | val_1_rmse: 0.86253 |  0:00:35s
epoch 35 | loss: 0.66699 | val_0_rmse: 0.82099 | val_1_rmse: 0.86758 |  0:00:36s
epoch 36 | loss: 0.66856 | val_0_rmse: 0.83025 | val_1_rmse: 0.87179 |  0:00:37s
epoch 37 | loss: 0.66019 | val_0_rmse: 0.82204 | val_1_rmse: 0.87035 |  0:00:38s
epoch 38 | loss: 0.65404 | val_0_rmse: 0.81557 | val_1_rmse: 0.87006 |  0:00:39s
epoch 39 | loss: 0.65477 | val_0_rmse: 0.81416 | val_1_rmse: 0.87482 |  0:00:40s
epoch 40 | loss: 0.65206 | val_0_rmse: 0.81058 | val_1_rmse: 0.86395 |  0:00:41s
epoch 41 | loss: 0.64754 | val_0_rmse: 0.80385 | val_1_rmse: 0.86288 |  0:00:42s
epoch 42 | loss: 0.64583 | val_0_rmse: 0.7971  | val_1_rmse: 0.86497 |  0:00:43s
epoch 43 | loss: 0.63595 | val_0_rmse: 0.79388 | val_1_rmse: 0.85402 |  0:00:44s
epoch 44 | loss: 0.63187 | val_0_rmse: 0.78924 | val_1_rmse: 0.84745 |  0:00:45s
epoch 45 | loss: 0.62825 | val_0_rmse: 0.78484 | val_1_rmse: 0.8508  |  0:00:46s
epoch 46 | loss: 0.62817 | val_0_rmse: 0.78335 | val_1_rmse: 0.85549 |  0:00:47s
epoch 47 | loss: 0.62952 | val_0_rmse: 0.81265 | val_1_rmse: 0.87106 |  0:00:48s
epoch 48 | loss: 0.62749 | val_0_rmse: 0.78947 | val_1_rmse: 0.85441 |  0:00:49s
epoch 49 | loss: 0.62578 | val_0_rmse: 0.77538 | val_1_rmse: 0.85159 |  0:00:50s
epoch 50 | loss: 0.62433 | val_0_rmse: 0.77357 | val_1_rmse: 0.85619 |  0:00:51s
epoch 51 | loss: 0.61768 | val_0_rmse: 0.77318 | val_1_rmse: 0.8528  |  0:00:52s
epoch 52 | loss: 0.62294 | val_0_rmse: 0.77733 | val_1_rmse: 0.85209 |  0:00:53s
epoch 53 | loss: 0.61664 | val_0_rmse: 0.76857 | val_1_rmse: 0.84912 |  0:00:54s
epoch 54 | loss: 0.60881 | val_0_rmse: 0.76314 | val_1_rmse: 0.85821 |  0:00:55s
epoch 55 | loss: 0.61336 | val_0_rmse: 0.76882 | val_1_rmse: 0.85378 |  0:00:56s
epoch 56 | loss: 0.61019 | val_0_rmse: 0.76974 | val_1_rmse: 0.85946 |  0:00:57s
epoch 57 | loss: 0.59797 | val_0_rmse: 0.75738 | val_1_rmse: 0.85035 |  0:00:58s
epoch 58 | loss: 0.59825 | val_0_rmse: 0.76408 | val_1_rmse: 0.85758 |  0:00:59s
epoch 59 | loss: 0.59242 | val_0_rmse: 0.75508 | val_1_rmse: 0.85728 |  0:01:00s
epoch 60 | loss: 0.59026 | val_0_rmse: 0.79427 | val_1_rmse: 0.87853 |  0:01:01s
epoch 61 | loss: 0.59758 | val_0_rmse: 0.76246 | val_1_rmse: 0.8734  |  0:01:02s
epoch 62 | loss: 0.58579 | val_0_rmse: 0.76551 | val_1_rmse: 0.86503 |  0:01:03s
epoch 63 | loss: 0.59756 | val_0_rmse: 0.7507  | val_1_rmse: 0.86187 |  0:01:04s
epoch 64 | loss: 0.58511 | val_0_rmse: 0.7463  | val_1_rmse: 0.85193 |  0:01:05s
epoch 65 | loss: 0.58115 | val_0_rmse: 0.75407 | val_1_rmse: 0.85233 |  0:01:06s
epoch 66 | loss: 0.60475 | val_0_rmse: 0.76239 | val_1_rmse: 0.85588 |  0:01:07s
epoch 67 | loss: 0.59513 | val_0_rmse: 0.75083 | val_1_rmse: 0.8671  |  0:01:08s
epoch 68 | loss: 0.59164 | val_0_rmse: 0.74801 | val_1_rmse: 0.86839 |  0:01:09s
epoch 69 | loss: 0.58433 | val_0_rmse: 0.7504  | val_1_rmse: 0.86806 |  0:01:10s
epoch 70 | loss: 0.58457 | val_0_rmse: 0.75284 | val_1_rmse: 0.85665 |  0:01:11s
epoch 71 | loss: 0.57562 | val_0_rmse: 0.76825 | val_1_rmse: 0.85454 |  0:01:12s
epoch 72 | loss: 0.58245 | val_0_rmse: 0.84688 | val_1_rmse: 0.8722  |  0:01:13s
epoch 73 | loss: 0.57232 | val_0_rmse: 0.78176 | val_1_rmse: 0.85487 |  0:01:14s
epoch 74 | loss: 0.5758  | val_0_rmse: 0.74835 | val_1_rmse: 0.86651 |  0:01:15s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.84745
Best weights from best epoch are automatically used!
ended training at: 05:34:24
Feature importance:
Mean squared error is of 0.05560432558905055
Mean absolute error:0.17790267883091712
MAPE:0.19668726022499144
R2 score:0.306549449003505
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:34:26
epoch 0  | loss: 1.49144 | val_0_rmse: 0.92959 | val_1_rmse: 0.90639 |  0:00:00s
epoch 1  | loss: 0.65407 | val_0_rmse: 0.76583 | val_1_rmse: 0.77588 |  0:00:01s
epoch 2  | loss: 0.49404 | val_0_rmse: 0.73218 | val_1_rmse: 0.73731 |  0:00:02s
epoch 3  | loss: 0.45269 | val_0_rmse: 0.67722 | val_1_rmse: 0.68494 |  0:00:03s
epoch 4  | loss: 0.45319 | val_0_rmse: 0.6664  | val_1_rmse: 0.67427 |  0:00:04s
epoch 5  | loss: 0.44559 | val_0_rmse: 0.66466 | val_1_rmse: 0.6574  |  0:00:05s
epoch 6  | loss: 0.43671 | val_0_rmse: 0.66166 | val_1_rmse: 0.66071 |  0:00:06s
epoch 7  | loss: 0.43374 | val_0_rmse: 0.65863 | val_1_rmse: 0.66052 |  0:00:07s
epoch 8  | loss: 0.42697 | val_0_rmse: 0.65917 | val_1_rmse: 0.66381 |  0:00:08s
epoch 9  | loss: 0.40183 | val_0_rmse: 0.62404 | val_1_rmse: 0.62366 |  0:00:09s
epoch 10 | loss: 0.35748 | val_0_rmse: 0.59026 | val_1_rmse: 0.59993 |  0:00:10s
epoch 11 | loss: 0.3541  | val_0_rmse: 0.58676 | val_1_rmse: 0.59649 |  0:00:11s
epoch 12 | loss: 0.34144 | val_0_rmse: 0.60154 | val_1_rmse: 0.61546 |  0:00:12s
epoch 13 | loss: 0.30706 | val_0_rmse: 0.58089 | val_1_rmse: 0.59012 |  0:00:13s
epoch 14 | loss: 0.28551 | val_0_rmse: 0.565   | val_1_rmse: 0.58177 |  0:00:13s
epoch 15 | loss: 0.26534 | val_0_rmse: 0.55895 | val_1_rmse: 0.57854 |  0:00:14s
epoch 16 | loss: 0.25996 | val_0_rmse: 0.53086 | val_1_rmse: 0.54365 |  0:00:15s
epoch 17 | loss: 0.23759 | val_0_rmse: 0.53861 | val_1_rmse: 0.56291 |  0:00:16s
epoch 18 | loss: 0.23104 | val_0_rmse: 0.53691 | val_1_rmse: 0.56268 |  0:00:17s
epoch 19 | loss: 0.22969 | val_0_rmse: 0.5057  | val_1_rmse: 0.52572 |  0:00:18s
epoch 20 | loss: 0.23594 | val_0_rmse: 0.48724 | val_1_rmse: 0.50752 |  0:00:19s
epoch 21 | loss: 0.22431 | val_0_rmse: 0.47068 | val_1_rmse: 0.49521 |  0:00:20s
epoch 22 | loss: 0.21401 | val_0_rmse: 0.47481 | val_1_rmse: 0.49224 |  0:00:21s
epoch 23 | loss: 0.22164 | val_0_rmse: 0.50287 | val_1_rmse: 0.51517 |  0:00:22s
epoch 24 | loss: 0.21333 | val_0_rmse: 0.48388 | val_1_rmse: 0.50593 |  0:00:23s
epoch 25 | loss: 0.21032 | val_0_rmse: 0.46874 | val_1_rmse: 0.48602 |  0:00:24s
epoch 26 | loss: 0.20416 | val_0_rmse: 0.46063 | val_1_rmse: 0.48359 |  0:00:25s
epoch 27 | loss: 0.20994 | val_0_rmse: 0.44554 | val_1_rmse: 0.46852 |  0:00:26s
epoch 28 | loss: 0.19745 | val_0_rmse: 0.43397 | val_1_rmse: 0.45471 |  0:00:27s
epoch 29 | loss: 0.19164 | val_0_rmse: 0.42636 | val_1_rmse: 0.44558 |  0:00:27s
epoch 30 | loss: 0.18628 | val_0_rmse: 0.45398 | val_1_rmse: 0.46537 |  0:00:28s
epoch 31 | loss: 0.18726 | val_0_rmse: 0.41998 | val_1_rmse: 0.43474 |  0:00:29s
epoch 32 | loss: 0.18717 | val_0_rmse: 0.41583 | val_1_rmse: 0.43359 |  0:00:30s
epoch 33 | loss: 0.18431 | val_0_rmse: 0.43807 | val_1_rmse: 0.4579  |  0:00:31s
epoch 34 | loss: 0.18071 | val_0_rmse: 0.4106  | val_1_rmse: 0.42881 |  0:00:32s
epoch 35 | loss: 0.18022 | val_0_rmse: 0.41362 | val_1_rmse: 0.43343 |  0:00:33s
epoch 36 | loss: 0.18074 | val_0_rmse: 0.44375 | val_1_rmse: 0.45301 |  0:00:34s
epoch 37 | loss: 0.18229 | val_0_rmse: 0.40175 | val_1_rmse: 0.42037 |  0:00:35s
epoch 38 | loss: 0.16892 | val_0_rmse: 0.38531 | val_1_rmse: 0.40409 |  0:00:36s
epoch 39 | loss: 0.16769 | val_0_rmse: 0.38235 | val_1_rmse: 0.39869 |  0:00:37s
epoch 40 | loss: 0.16753 | val_0_rmse: 0.38795 | val_1_rmse: 0.40731 |  0:00:38s
epoch 41 | loss: 0.18316 | val_0_rmse: 0.44132 | val_1_rmse: 0.45602 |  0:00:39s
epoch 42 | loss: 0.19111 | val_0_rmse: 0.40167 | val_1_rmse: 0.41365 |  0:00:39s
epoch 43 | loss: 0.16941 | val_0_rmse: 0.39875 | val_1_rmse: 0.41576 |  0:00:40s
epoch 44 | loss: 0.16625 | val_0_rmse: 0.39019 | val_1_rmse: 0.40089 |  0:00:41s
epoch 45 | loss: 0.17411 | val_0_rmse: 0.37614 | val_1_rmse: 0.39087 |  0:00:42s
epoch 46 | loss: 0.17054 | val_0_rmse: 0.41675 | val_1_rmse: 0.42487 |  0:00:43s
epoch 47 | loss: 0.17663 | val_0_rmse: 0.39563 | val_1_rmse: 0.41236 |  0:00:44s
epoch 48 | loss: 0.17756 | val_0_rmse: 0.39299 | val_1_rmse: 0.41432 |  0:00:45s
epoch 49 | loss: 0.17151 | val_0_rmse: 0.38054 | val_1_rmse: 0.39989 |  0:00:46s
epoch 50 | loss: 0.16887 | val_0_rmse: 0.41046 | val_1_rmse: 0.42533 |  0:00:47s
epoch 51 | loss: 0.16291 | val_0_rmse: 0.39496 | val_1_rmse: 0.41836 |  0:00:48s
epoch 52 | loss: 0.16713 | val_0_rmse: 0.40056 | val_1_rmse: 0.427   |  0:00:49s
epoch 53 | loss: 0.20694 | val_0_rmse: 0.46222 | val_1_rmse: 0.4752  |  0:00:50s
epoch 54 | loss: 0.20542 | val_0_rmse: 0.45431 | val_1_rmse: 0.46149 |  0:00:51s
epoch 55 | loss: 0.18656 | val_0_rmse: 0.46357 | val_1_rmse: 0.47469 |  0:00:52s
epoch 56 | loss: 0.17634 | val_0_rmse: 0.39775 | val_1_rmse: 0.41082 |  0:00:52s
epoch 57 | loss: 0.16875 | val_0_rmse: 0.40156 | val_1_rmse: 0.42226 |  0:00:53s
epoch 58 | loss: 0.17118 | val_0_rmse: 0.37672 | val_1_rmse: 0.39488 |  0:00:54s
epoch 59 | loss: 0.16263 | val_0_rmse: 0.38097 | val_1_rmse: 0.39769 |  0:00:55s
epoch 60 | loss: 0.1757  | val_0_rmse: 0.38907 | val_1_rmse: 0.40362 |  0:00:56s
epoch 61 | loss: 0.17216 | val_0_rmse: 0.40393 | val_1_rmse: 0.41691 |  0:00:57s
epoch 62 | loss: 0.1788  | val_0_rmse: 0.40959 | val_1_rmse: 0.42317 |  0:00:58s
epoch 63 | loss: 0.1683  | val_0_rmse: 0.38795 | val_1_rmse: 0.40211 |  0:00:59s
epoch 64 | loss: 0.17227 | val_0_rmse: 0.39035 | val_1_rmse: 0.41059 |  0:01:00s
epoch 65 | loss: 0.16317 | val_0_rmse: 0.38117 | val_1_rmse: 0.39727 |  0:01:01s
epoch 66 | loss: 0.16073 | val_0_rmse: 0.39911 | val_1_rmse: 0.42011 |  0:01:02s
epoch 67 | loss: 0.17612 | val_0_rmse: 0.41512 | val_1_rmse: 0.42909 |  0:01:03s
epoch 68 | loss: 0.18164 | val_0_rmse: 0.39792 | val_1_rmse: 0.40868 |  0:01:04s
epoch 69 | loss: 0.18003 | val_0_rmse: 0.4183  | val_1_rmse: 0.42698 |  0:01:04s
epoch 70 | loss: 0.20768 | val_0_rmse: 0.47793 | val_1_rmse: 0.48695 |  0:01:05s
epoch 71 | loss: 0.21666 | val_0_rmse: 0.42798 | val_1_rmse: 0.43831 |  0:01:06s
epoch 72 | loss: 0.1971  | val_0_rmse: 0.41192 | val_1_rmse: 0.42761 |  0:01:07s
epoch 73 | loss: 0.19344 | val_0_rmse: 0.42066 | val_1_rmse: 0.42604 |  0:01:08s
epoch 74 | loss: 0.18001 | val_0_rmse: 0.40817 | val_1_rmse: 0.41916 |  0:01:09s
epoch 75 | loss: 0.18461 | val_0_rmse: 0.41954 | val_1_rmse: 0.4335  |  0:01:10s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.39087
Best weights from best epoch are automatically used!
ended training at: 05:35:37
Feature importance:
Mean squared error is of 0.07914180585431338
Mean absolute error:0.1777607723915164
MAPE:0.21336725555180935
R2 score:0.8433627210966504
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:35:37
epoch 0  | loss: 1.37105 | val_0_rmse: 0.90626 | val_1_rmse: 0.87824 |  0:00:00s
epoch 1  | loss: 0.61295 | val_0_rmse: 0.8031  | val_1_rmse: 0.76516 |  0:00:01s
epoch 2  | loss: 0.45745 | val_0_rmse: 0.73068 | val_1_rmse: 0.71639 |  0:00:02s
epoch 3  | loss: 0.37433 | val_0_rmse: 0.65936 | val_1_rmse: 0.64391 |  0:00:03s
epoch 4  | loss: 0.34734 | val_0_rmse: 0.63323 | val_1_rmse: 0.61607 |  0:00:04s
epoch 5  | loss: 0.32142 | val_0_rmse: 0.61272 | val_1_rmse: 0.59412 |  0:00:05s
epoch 6  | loss: 0.30625 | val_0_rmse: 0.62086 | val_1_rmse: 0.6053  |  0:00:06s
epoch 7  | loss: 0.28342 | val_0_rmse: 0.63128 | val_1_rmse: 0.61208 |  0:00:07s
epoch 8  | loss: 0.26189 | val_0_rmse: 0.59904 | val_1_rmse: 0.58292 |  0:00:08s
epoch 9  | loss: 0.253   | val_0_rmse: 0.59623 | val_1_rmse: 0.57437 |  0:00:09s
epoch 10 | loss: 0.23016 | val_0_rmse: 0.56199 | val_1_rmse: 0.54068 |  0:00:10s
epoch 11 | loss: 0.21964 | val_0_rmse: 0.60654 | val_1_rmse: 0.58251 |  0:00:11s
epoch 12 | loss: 0.20728 | val_0_rmse: 0.57036 | val_1_rmse: 0.54749 |  0:00:12s
epoch 13 | loss: 0.20895 | val_0_rmse: 0.53869 | val_1_rmse: 0.51868 |  0:00:13s
epoch 14 | loss: 0.20204 | val_0_rmse: 0.53056 | val_1_rmse: 0.51107 |  0:00:14s
epoch 15 | loss: 0.20087 | val_0_rmse: 0.55972 | val_1_rmse: 0.5443  |  0:00:14s
epoch 16 | loss: 0.19969 | val_0_rmse: 0.55965 | val_1_rmse: 0.54062 |  0:00:15s
epoch 17 | loss: 0.19041 | val_0_rmse: 0.49085 | val_1_rmse: 0.47644 |  0:00:16s
epoch 18 | loss: 0.18597 | val_0_rmse: 0.46799 | val_1_rmse: 0.45717 |  0:00:17s
epoch 19 | loss: 0.18158 | val_0_rmse: 0.47636 | val_1_rmse: 0.46492 |  0:00:18s
epoch 20 | loss: 0.17741 | val_0_rmse: 0.46627 | val_1_rmse: 0.45434 |  0:00:19s
epoch 21 | loss: 0.17934 | val_0_rmse: 0.54165 | val_1_rmse: 0.52035 |  0:00:20s
epoch 22 | loss: 0.18128 | val_0_rmse: 0.45267 | val_1_rmse: 0.44605 |  0:00:21s
epoch 23 | loss: 0.17411 | val_0_rmse: 0.45221 | val_1_rmse: 0.44604 |  0:00:22s
epoch 24 | loss: 0.17871 | val_0_rmse: 0.47847 | val_1_rmse: 0.46895 |  0:00:23s
epoch 25 | loss: 0.19702 | val_0_rmse: 0.41361 | val_1_rmse: 0.4116  |  0:00:24s
epoch 26 | loss: 0.1926  | val_0_rmse: 0.4442  | val_1_rmse: 0.43753 |  0:00:25s
epoch 27 | loss: 0.17846 | val_0_rmse: 0.44764 | val_1_rmse: 0.43937 |  0:00:26s
epoch 28 | loss: 0.17596 | val_0_rmse: 0.42991 | val_1_rmse: 0.4286  |  0:00:27s
epoch 29 | loss: 0.17091 | val_0_rmse: 0.40765 | val_1_rmse: 0.40474 |  0:00:27s
epoch 30 | loss: 0.16898 | val_0_rmse: 0.4199  | val_1_rmse: 0.41512 |  0:00:28s
epoch 31 | loss: 0.1721  | val_0_rmse: 0.41087 | val_1_rmse: 0.4132  |  0:00:29s
epoch 32 | loss: 0.1735  | val_0_rmse: 0.41466 | val_1_rmse: 0.40997 |  0:00:30s
epoch 33 | loss: 0.16861 | val_0_rmse: 0.40523 | val_1_rmse: 0.40393 |  0:00:31s
epoch 34 | loss: 0.16484 | val_0_rmse: 0.39417 | val_1_rmse: 0.39548 |  0:00:32s
epoch 35 | loss: 0.16306 | val_0_rmse: 0.39738 | val_1_rmse: 0.39447 |  0:00:33s
epoch 36 | loss: 0.15905 | val_0_rmse: 0.38559 | val_1_rmse: 0.38818 |  0:00:34s
epoch 37 | loss: 0.16202 | val_0_rmse: 0.38308 | val_1_rmse: 0.38964 |  0:00:35s
epoch 38 | loss: 0.15528 | val_0_rmse: 0.37308 | val_1_rmse: 0.37818 |  0:00:36s
epoch 39 | loss: 0.16124 | val_0_rmse: 0.37251 | val_1_rmse: 0.38122 |  0:00:37s
epoch 40 | loss: 0.16144 | val_0_rmse: 0.39604 | val_1_rmse: 0.40098 |  0:00:38s
epoch 41 | loss: 0.15929 | val_0_rmse: 0.37578 | val_1_rmse: 0.37989 |  0:00:39s
epoch 42 | loss: 0.15641 | val_0_rmse: 0.37234 | val_1_rmse: 0.3765  |  0:00:40s
epoch 43 | loss: 0.15426 | val_0_rmse: 0.37384 | val_1_rmse: 0.37973 |  0:00:40s
epoch 44 | loss: 0.15628 | val_0_rmse: 0.36872 | val_1_rmse: 0.37624 |  0:00:41s
epoch 45 | loss: 0.1523  | val_0_rmse: 0.37863 | val_1_rmse: 0.38398 |  0:00:42s
epoch 46 | loss: 0.15422 | val_0_rmse: 0.36509 | val_1_rmse: 0.37681 |  0:00:43s
epoch 47 | loss: 0.15806 | val_0_rmse: 0.36936 | val_1_rmse: 0.37905 |  0:00:44s
epoch 48 | loss: 0.15055 | val_0_rmse: 0.36417 | val_1_rmse: 0.37103 |  0:00:45s
epoch 49 | loss: 0.15796 | val_0_rmse: 0.38342 | val_1_rmse: 0.39629 |  0:00:46s
epoch 50 | loss: 0.1578  | val_0_rmse: 0.39591 | val_1_rmse: 0.39744 |  0:00:47s
epoch 51 | loss: 0.15376 | val_0_rmse: 0.37253 | val_1_rmse: 0.38497 |  0:00:48s
epoch 52 | loss: 0.1508  | val_0_rmse: 0.36085 | val_1_rmse: 0.37398 |  0:00:49s
epoch 53 | loss: 0.14718 | val_0_rmse: 0.36225 | val_1_rmse: 0.37019 |  0:00:50s
epoch 54 | loss: 0.15664 | val_0_rmse: 0.43288 | val_1_rmse: 0.43043 |  0:00:51s
epoch 55 | loss: 0.1625  | val_0_rmse: 0.37104 | val_1_rmse: 0.37925 |  0:00:52s
epoch 56 | loss: 0.15171 | val_0_rmse: 0.36913 | val_1_rmse: 0.37793 |  0:00:52s
epoch 57 | loss: 0.14653 | val_0_rmse: 0.35618 | val_1_rmse: 0.36767 |  0:00:53s
epoch 58 | loss: 0.14896 | val_0_rmse: 0.36426 | val_1_rmse: 0.37689 |  0:00:54s
epoch 59 | loss: 0.15335 | val_0_rmse: 0.38675 | val_1_rmse: 0.39373 |  0:00:55s
epoch 60 | loss: 0.15703 | val_0_rmse: 0.36378 | val_1_rmse: 0.37115 |  0:00:56s
epoch 61 | loss: 0.15116 | val_0_rmse: 0.36136 | val_1_rmse: 0.37489 |  0:00:57s
epoch 62 | loss: 0.1549  | val_0_rmse: 0.36729 | val_1_rmse: 0.37329 |  0:00:58s
epoch 63 | loss: 0.14672 | val_0_rmse: 0.35043 | val_1_rmse: 0.35939 |  0:00:59s
epoch 64 | loss: 0.14854 | val_0_rmse: 0.3743  | val_1_rmse: 0.38549 |  0:01:00s
epoch 65 | loss: 0.15626 | val_0_rmse: 0.37608 | val_1_rmse: 0.39012 |  0:01:01s
epoch 66 | loss: 0.15658 | val_0_rmse: 0.37096 | val_1_rmse: 0.38048 |  0:01:02s
epoch 67 | loss: 0.15432 | val_0_rmse: 0.38815 | val_1_rmse: 0.39146 |  0:01:03s
epoch 68 | loss: 0.15422 | val_0_rmse: 0.38253 | val_1_rmse: 0.38281 |  0:01:04s
epoch 69 | loss: 0.14603 | val_0_rmse: 0.35889 | val_1_rmse: 0.37373 |  0:01:05s
epoch 70 | loss: 0.14527 | val_0_rmse: 0.35343 | val_1_rmse: 0.36398 |  0:01:05s
epoch 71 | loss: 0.14412 | val_0_rmse: 0.34986 | val_1_rmse: 0.36148 |  0:01:06s
epoch 72 | loss: 0.13872 | val_0_rmse: 0.35552 | val_1_rmse: 0.36719 |  0:01:07s
epoch 73 | loss: 0.13861 | val_0_rmse: 0.35375 | val_1_rmse: 0.36596 |  0:01:08s
epoch 74 | loss: 0.14271 | val_0_rmse: 0.35454 | val_1_rmse: 0.36303 |  0:01:09s
epoch 75 | loss: 0.1404  | val_0_rmse: 0.34626 | val_1_rmse: 0.35772 |  0:01:10s
epoch 76 | loss: 0.14937 | val_0_rmse: 0.36992 | val_1_rmse: 0.37944 |  0:01:11s
epoch 77 | loss: 0.1473  | val_0_rmse: 0.36496 | val_1_rmse: 0.37438 |  0:01:12s
epoch 78 | loss: 0.15286 | val_0_rmse: 0.3724  | val_1_rmse: 0.38411 |  0:01:13s
epoch 79 | loss: 0.14408 | val_0_rmse: 0.35472 | val_1_rmse: 0.3688  |  0:01:14s
epoch 80 | loss: 0.13906 | val_0_rmse: 0.35035 | val_1_rmse: 0.35649 |  0:01:15s
epoch 81 | loss: 0.13882 | val_0_rmse: 0.35959 | val_1_rmse: 0.36741 |  0:01:16s
epoch 82 | loss: 0.14117 | val_0_rmse: 0.35033 | val_1_rmse: 0.36264 |  0:01:17s
epoch 83 | loss: 0.15223 | val_0_rmse: 0.39299 | val_1_rmse: 0.39803 |  0:01:17s
epoch 84 | loss: 0.16134 | val_0_rmse: 0.38041 | val_1_rmse: 0.38745 |  0:01:18s
epoch 85 | loss: 0.15334 | val_0_rmse: 0.36228 | val_1_rmse: 0.3746  |  0:01:19s
epoch 86 | loss: 0.14672 | val_0_rmse: 0.3737  | val_1_rmse: 0.38175 |  0:01:20s
epoch 87 | loss: 0.14863 | val_0_rmse: 0.34549 | val_1_rmse: 0.3545  |  0:01:21s
epoch 88 | loss: 0.14357 | val_0_rmse: 0.36021 | val_1_rmse: 0.36448 |  0:01:22s
epoch 89 | loss: 0.14079 | val_0_rmse: 0.34397 | val_1_rmse: 0.35529 |  0:01:23s
epoch 90 | loss: 0.14283 | val_0_rmse: 0.3741  | val_1_rmse: 0.38028 |  0:01:24s
epoch 91 | loss: 0.13851 | val_0_rmse: 0.35965 | val_1_rmse: 0.36262 |  0:01:25s
epoch 92 | loss: 0.13965 | val_0_rmse: 0.35697 | val_1_rmse: 0.36722 |  0:01:26s
epoch 93 | loss: 0.1387  | val_0_rmse: 0.34161 | val_1_rmse: 0.35337 |  0:01:27s
epoch 94 | loss: 0.13221 | val_0_rmse: 0.33704 | val_1_rmse: 0.3469  |  0:01:28s
epoch 95 | loss: 0.13629 | val_0_rmse: 0.34314 | val_1_rmse: 0.35251 |  0:01:29s
epoch 96 | loss: 0.13965 | val_0_rmse: 0.3445  | val_1_rmse: 0.35097 |  0:01:30s
epoch 97 | loss: 0.13316 | val_0_rmse: 0.34724 | val_1_rmse: 0.35145 |  0:01:30s
epoch 98 | loss: 0.13616 | val_0_rmse: 0.353   | val_1_rmse: 0.3675  |  0:01:31s
epoch 99 | loss: 0.13838 | val_0_rmse: 0.35423 | val_1_rmse: 0.36154 |  0:01:32s
epoch 100| loss: 0.14203 | val_0_rmse: 0.34889 | val_1_rmse: 0.35824 |  0:01:33s
epoch 101| loss: 0.13723 | val_0_rmse: 0.3397  | val_1_rmse: 0.34983 |  0:01:34s
epoch 102| loss: 0.13265 | val_0_rmse: 0.33509 | val_1_rmse: 0.35197 |  0:01:35s
epoch 103| loss: 0.12974 | val_0_rmse: 0.348   | val_1_rmse: 0.36635 |  0:01:36s
epoch 104| loss: 0.12983 | val_0_rmse: 0.33943 | val_1_rmse: 0.34776 |  0:01:37s
epoch 105| loss: 0.12905 | val_0_rmse: 0.33056 | val_1_rmse: 0.34632 |  0:01:38s
epoch 106| loss: 0.12645 | val_0_rmse: 0.33813 | val_1_rmse: 0.35039 |  0:01:39s
epoch 107| loss: 0.12483 | val_0_rmse: 0.33666 | val_1_rmse: 0.35213 |  0:01:40s
epoch 108| loss: 0.1275  | val_0_rmse: 0.33016 | val_1_rmse: 0.34237 |  0:01:41s
epoch 109| loss: 0.12611 | val_0_rmse: 0.33127 | val_1_rmse: 0.34468 |  0:01:42s
epoch 110| loss: 0.12759 | val_0_rmse: 0.328   | val_1_rmse: 0.34638 |  0:01:43s
epoch 111| loss: 0.13148 | val_0_rmse: 0.34603 | val_1_rmse: 0.35891 |  0:01:43s
epoch 112| loss: 0.13131 | val_0_rmse: 0.33496 | val_1_rmse: 0.34494 |  0:01:44s
epoch 113| loss: 0.12764 | val_0_rmse: 0.32678 | val_1_rmse: 0.34506 |  0:01:45s
epoch 114| loss: 0.12568 | val_0_rmse: 0.33245 | val_1_rmse: 0.34284 |  0:01:46s
epoch 115| loss: 0.12362 | val_0_rmse: 0.32705 | val_1_rmse: 0.34169 |  0:01:47s
epoch 116| loss: 0.12542 | val_0_rmse: 0.37969 | val_1_rmse: 0.38984 |  0:01:48s
epoch 117| loss: 0.13    | val_0_rmse: 0.35462 | val_1_rmse: 0.36959 |  0:01:49s
epoch 118| loss: 0.13137 | val_0_rmse: 0.3293  | val_1_rmse: 0.34764 |  0:01:50s
epoch 119| loss: 0.12384 | val_0_rmse: 0.34181 | val_1_rmse: 0.35415 |  0:01:51s
epoch 120| loss: 0.12174 | val_0_rmse: 0.32246 | val_1_rmse: 0.34053 |  0:01:52s
epoch 121| loss: 0.12287 | val_0_rmse: 0.32229 | val_1_rmse: 0.34375 |  0:01:53s
epoch 122| loss: 0.1211  | val_0_rmse: 0.32632 | val_1_rmse: 0.33942 |  0:01:54s
epoch 123| loss: 0.12712 | val_0_rmse: 0.32303 | val_1_rmse: 0.33821 |  0:01:55s
epoch 124| loss: 0.12153 | val_0_rmse: 0.32708 | val_1_rmse: 0.34093 |  0:01:56s
epoch 125| loss: 0.12463 | val_0_rmse: 0.32602 | val_1_rmse: 0.34887 |  0:01:56s
epoch 126| loss: 0.12102 | val_0_rmse: 0.32323 | val_1_rmse: 0.34177 |  0:01:57s
epoch 127| loss: 0.12226 | val_0_rmse: 0.32878 | val_1_rmse: 0.34334 |  0:01:58s
epoch 128| loss: 0.1228  | val_0_rmse: 0.33471 | val_1_rmse: 0.35247 |  0:01:59s
epoch 129| loss: 0.12456 | val_0_rmse: 0.32436 | val_1_rmse: 0.33855 |  0:02:00s
epoch 130| loss: 0.12344 | val_0_rmse: 0.32309 | val_1_rmse: 0.33828 |  0:02:01s
epoch 131| loss: 0.12086 | val_0_rmse: 0.3324  | val_1_rmse: 0.35078 |  0:02:02s
epoch 132| loss: 0.12127 | val_0_rmse: 0.32423 | val_1_rmse: 0.34027 |  0:02:03s
epoch 133| loss: 0.12611 | val_0_rmse: 0.35019 | val_1_rmse: 0.36555 |  0:02:04s
epoch 134| loss: 0.13269 | val_0_rmse: 0.35778 | val_1_rmse: 0.36712 |  0:02:05s
epoch 135| loss: 0.13037 | val_0_rmse: 0.341   | val_1_rmse: 0.34962 |  0:02:06s
epoch 136| loss: 0.12477 | val_0_rmse: 0.33732 | val_1_rmse: 0.35105 |  0:02:07s
epoch 137| loss: 0.1238  | val_0_rmse: 0.33675 | val_1_rmse: 0.35164 |  0:02:08s
epoch 138| loss: 0.12942 | val_0_rmse: 0.34785 | val_1_rmse: 0.36242 |  0:02:08s
epoch 139| loss: 0.14321 | val_0_rmse: 0.35223 | val_1_rmse: 0.36832 |  0:02:09s
epoch 140| loss: 0.13301 | val_0_rmse: 0.34257 | val_1_rmse: 0.35884 |  0:02:10s
epoch 141| loss: 0.13187 | val_0_rmse: 0.37129 | val_1_rmse: 0.38125 |  0:02:11s
epoch 142| loss: 0.13064 | val_0_rmse: 0.36877 | val_1_rmse: 0.38355 |  0:02:12s
epoch 143| loss: 0.12693 | val_0_rmse: 0.32839 | val_1_rmse: 0.3452  |  0:02:13s
epoch 144| loss: 0.12333 | val_0_rmse: 0.36447 | val_1_rmse: 0.37996 |  0:02:14s
epoch 145| loss: 0.12899 | val_0_rmse: 0.32582 | val_1_rmse: 0.34606 |  0:02:15s
epoch 146| loss: 0.11971 | val_0_rmse: 0.32405 | val_1_rmse: 0.34283 |  0:02:16s
epoch 147| loss: 0.12156 | val_0_rmse: 0.32765 | val_1_rmse: 0.34507 |  0:02:17s
epoch 148| loss: 0.11745 | val_0_rmse: 0.31945 | val_1_rmse: 0.34009 |  0:02:18s
epoch 149| loss: 0.11745 | val_0_rmse: 0.33722 | val_1_rmse: 0.35996 |  0:02:19s
Stop training because you reached max_epochs = 150 with best_epoch = 123 and best_val_1_rmse = 0.33821
Best weights from best epoch are automatically used!
ended training at: 05:37:57
Feature importance:
Mean squared error is of 0.05504253564940708
Mean absolute error:0.15570170963760607
MAPE:0.1905547895360599
R2 score:0.8775449527025113
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:37:57
epoch 0  | loss: 1.32589 | val_0_rmse: 0.90841 | val_1_rmse: 0.9108  |  0:00:00s
epoch 1  | loss: 0.68524 | val_0_rmse: 0.85308 | val_1_rmse: 0.86624 |  0:00:01s
epoch 2  | loss: 0.52952 | val_0_rmse: 0.80922 | val_1_rmse: 0.81623 |  0:00:02s
epoch 3  | loss: 0.42827 | val_0_rmse: 0.72885 | val_1_rmse: 0.738   |  0:00:03s
epoch 4  | loss: 0.37026 | val_0_rmse: 0.66653 | val_1_rmse: 0.68198 |  0:00:04s
epoch 5  | loss: 0.33927 | val_0_rmse: 0.74771 | val_1_rmse: 0.75977 |  0:00:05s
epoch 6  | loss: 0.31966 | val_0_rmse: 0.78609 | val_1_rmse: 0.80407 |  0:00:06s
epoch 7  | loss: 0.30756 | val_0_rmse: 0.76179 | val_1_rmse: 0.7821  |  0:00:07s
epoch 8  | loss: 0.29057 | val_0_rmse: 0.67966 | val_1_rmse: 0.69063 |  0:00:08s
epoch 9  | loss: 0.28285 | val_0_rmse: 0.72093 | val_1_rmse: 0.73441 |  0:00:09s
epoch 10 | loss: 0.27717 | val_0_rmse: 0.67136 | val_1_rmse: 0.68737 |  0:00:10s
epoch 11 | loss: 0.26684 | val_0_rmse: 0.59032 | val_1_rmse: 0.60716 |  0:00:11s
epoch 12 | loss: 0.25291 | val_0_rmse: 0.58275 | val_1_rmse: 0.60259 |  0:00:11s
epoch 13 | loss: 0.2484  | val_0_rmse: 0.61619 | val_1_rmse: 0.63632 |  0:00:12s
epoch 14 | loss: 0.2298  | val_0_rmse: 0.60974 | val_1_rmse: 0.62828 |  0:00:13s
epoch 15 | loss: 0.22332 | val_0_rmse: 0.56363 | val_1_rmse: 0.57598 |  0:00:14s
epoch 16 | loss: 0.21792 | val_0_rmse: 0.53649 | val_1_rmse: 0.54682 |  0:00:15s
epoch 17 | loss: 0.21527 | val_0_rmse: 0.55024 | val_1_rmse: 0.56477 |  0:00:16s
epoch 18 | loss: 0.2204  | val_0_rmse: 0.49652 | val_1_rmse: 0.50716 |  0:00:17s
epoch 19 | loss: 0.21299 | val_0_rmse: 0.5363  | val_1_rmse: 0.55084 |  0:00:18s
epoch 20 | loss: 0.20418 | val_0_rmse: 0.50375 | val_1_rmse: 0.5167  |  0:00:19s
epoch 21 | loss: 0.19056 | val_0_rmse: 0.48833 | val_1_rmse: 0.50278 |  0:00:20s
epoch 22 | loss: 0.19304 | val_0_rmse: 0.49812 | val_1_rmse: 0.51271 |  0:00:21s
epoch 23 | loss: 0.18791 | val_0_rmse: 0.43689 | val_1_rmse: 0.45556 |  0:00:22s
epoch 24 | loss: 0.1883  | val_0_rmse: 0.44143 | val_1_rmse: 0.4597  |  0:00:23s
epoch 25 | loss: 0.17499 | val_0_rmse: 0.44525 | val_1_rmse: 0.46897 |  0:00:24s
epoch 26 | loss: 0.18075 | val_0_rmse: 0.46251 | val_1_rmse: 0.48251 |  0:00:24s
epoch 27 | loss: 0.1762  | val_0_rmse: 0.4128  | val_1_rmse: 0.42882 |  0:00:25s
epoch 28 | loss: 0.17577 | val_0_rmse: 0.41194 | val_1_rmse: 0.43703 |  0:00:26s
epoch 29 | loss: 0.17313 | val_0_rmse: 0.41547 | val_1_rmse: 0.43598 |  0:00:27s
epoch 30 | loss: 0.17053 | val_0_rmse: 0.40635 | val_1_rmse: 0.42811 |  0:00:28s
epoch 31 | loss: 0.18132 | val_0_rmse: 0.40758 | val_1_rmse: 0.42762 |  0:00:29s
epoch 32 | loss: 0.17874 | val_0_rmse: 0.41051 | val_1_rmse: 0.43003 |  0:00:30s
epoch 33 | loss: 0.17242 | val_0_rmse: 0.41202 | val_1_rmse: 0.4282  |  0:00:31s
epoch 34 | loss: 0.17101 | val_0_rmse: 0.39825 | val_1_rmse: 0.426   |  0:00:32s
epoch 35 | loss: 0.16478 | val_0_rmse: 0.41118 | val_1_rmse: 0.43236 |  0:00:33s
epoch 36 | loss: 0.16497 | val_0_rmse: 0.3907  | val_1_rmse: 0.40927 |  0:00:34s
epoch 37 | loss: 0.16644 | val_0_rmse: 0.38988 | val_1_rmse: 0.4131  |  0:00:35s
epoch 38 | loss: 0.16674 | val_0_rmse: 0.39871 | val_1_rmse: 0.42282 |  0:00:36s
epoch 39 | loss: 0.16278 | val_0_rmse: 0.38154 | val_1_rmse: 0.40607 |  0:00:37s
epoch 40 | loss: 0.15848 | val_0_rmse: 0.40865 | val_1_rmse: 0.43007 |  0:00:37s
epoch 41 | loss: 0.16593 | val_0_rmse: 0.37083 | val_1_rmse: 0.39703 |  0:00:38s
epoch 42 | loss: 0.1549  | val_0_rmse: 0.38078 | val_1_rmse: 0.40199 |  0:00:39s
epoch 43 | loss: 0.15292 | val_0_rmse: 0.36947 | val_1_rmse: 0.39372 |  0:00:40s
epoch 44 | loss: 0.15517 | val_0_rmse: 0.36837 | val_1_rmse: 0.39059 |  0:00:41s
epoch 45 | loss: 0.15053 | val_0_rmse: 0.37321 | val_1_rmse: 0.39874 |  0:00:42s
epoch 46 | loss: 0.1536  | val_0_rmse: 0.39493 | val_1_rmse: 0.41893 |  0:00:43s
epoch 47 | loss: 0.15637 | val_0_rmse: 0.37705 | val_1_rmse: 0.40806 |  0:00:44s
epoch 48 | loss: 0.1477  | val_0_rmse: 0.37563 | val_1_rmse: 0.39991 |  0:00:45s
epoch 49 | loss: 0.14841 | val_0_rmse: 0.36795 | val_1_rmse: 0.39528 |  0:00:46s
epoch 50 | loss: 0.15672 | val_0_rmse: 0.36953 | val_1_rmse: 0.38977 |  0:00:47s
epoch 51 | loss: 0.15333 | val_0_rmse: 0.36098 | val_1_rmse: 0.38721 |  0:00:48s
epoch 52 | loss: 0.14674 | val_0_rmse: 0.35768 | val_1_rmse: 0.38216 |  0:00:49s
epoch 53 | loss: 0.1481  | val_0_rmse: 0.36323 | val_1_rmse: 0.38846 |  0:00:50s
epoch 54 | loss: 0.14636 | val_0_rmse: 0.37775 | val_1_rmse: 0.39942 |  0:00:50s
epoch 55 | loss: 0.15942 | val_0_rmse: 0.39444 | val_1_rmse: 0.41551 |  0:00:51s
epoch 56 | loss: 0.15496 | val_0_rmse: 0.38191 | val_1_rmse: 0.40315 |  0:00:52s
epoch 57 | loss: 0.15416 | val_0_rmse: 0.37577 | val_1_rmse: 0.39601 |  0:00:53s
epoch 58 | loss: 0.14678 | val_0_rmse: 0.38556 | val_1_rmse: 0.40538 |  0:00:54s
epoch 59 | loss: 0.15098 | val_0_rmse: 0.36388 | val_1_rmse: 0.38872 |  0:00:55s
epoch 60 | loss: 0.15265 | val_0_rmse: 0.36089 | val_1_rmse: 0.39022 |  0:00:56s
epoch 61 | loss: 0.14312 | val_0_rmse: 0.34921 | val_1_rmse: 0.37486 |  0:00:57s
epoch 62 | loss: 0.14587 | val_0_rmse: 0.36443 | val_1_rmse: 0.38819 |  0:00:58s
epoch 63 | loss: 0.14262 | val_0_rmse: 0.35202 | val_1_rmse: 0.38307 |  0:00:59s
epoch 64 | loss: 0.13874 | val_0_rmse: 0.35182 | val_1_rmse: 0.38109 |  0:01:00s
epoch 65 | loss: 0.14195 | val_0_rmse: 0.34928 | val_1_rmse: 0.37452 |  0:01:01s
epoch 66 | loss: 0.14454 | val_0_rmse: 0.36115 | val_1_rmse: 0.38803 |  0:01:02s
epoch 67 | loss: 0.14379 | val_0_rmse: 0.36532 | val_1_rmse: 0.39328 |  0:01:02s
epoch 68 | loss: 0.14414 | val_0_rmse: 0.34411 | val_1_rmse: 0.36613 |  0:01:03s
epoch 69 | loss: 0.13742 | val_0_rmse: 0.35188 | val_1_rmse: 0.38005 |  0:01:04s
epoch 70 | loss: 0.13741 | val_0_rmse: 0.35332 | val_1_rmse: 0.37994 |  0:01:05s
epoch 71 | loss: 0.13558 | val_0_rmse: 0.3567  | val_1_rmse: 0.37968 |  0:01:06s
epoch 72 | loss: 0.13499 | val_0_rmse: 0.34361 | val_1_rmse: 0.37399 |  0:01:07s
epoch 73 | loss: 0.13994 | val_0_rmse: 0.37568 | val_1_rmse: 0.39798 |  0:01:08s
epoch 74 | loss: 0.14518 | val_0_rmse: 0.35538 | val_1_rmse: 0.38566 |  0:01:09s
epoch 75 | loss: 0.14196 | val_0_rmse: 0.36261 | val_1_rmse: 0.38061 |  0:01:10s
epoch 76 | loss: 0.14779 | val_0_rmse: 0.35412 | val_1_rmse: 0.37685 |  0:01:11s
epoch 77 | loss: 0.13809 | val_0_rmse: 0.36565 | val_1_rmse: 0.38679 |  0:01:12s
epoch 78 | loss: 0.13737 | val_0_rmse: 0.34298 | val_1_rmse: 0.37114 |  0:01:13s
epoch 79 | loss: 0.13352 | val_0_rmse: 0.34909 | val_1_rmse: 0.37001 |  0:01:13s
epoch 80 | loss: 0.13378 | val_0_rmse: 0.33731 | val_1_rmse: 0.36768 |  0:01:14s
epoch 81 | loss: 0.13139 | val_0_rmse: 0.34408 | val_1_rmse: 0.37235 |  0:01:15s
epoch 82 | loss: 0.13396 | val_0_rmse: 0.35477 | val_1_rmse: 0.37614 |  0:01:16s
epoch 83 | loss: 0.14049 | val_0_rmse: 0.34222 | val_1_rmse: 0.3675  |  0:01:17s
epoch 84 | loss: 0.13315 | val_0_rmse: 0.38261 | val_1_rmse: 0.40291 |  0:01:18s
epoch 85 | loss: 0.14    | val_0_rmse: 0.34539 | val_1_rmse: 0.37242 |  0:01:19s
epoch 86 | loss: 0.13704 | val_0_rmse: 0.33967 | val_1_rmse: 0.3648  |  0:01:20s
epoch 87 | loss: 0.13243 | val_0_rmse: 0.35717 | val_1_rmse: 0.38748 |  0:01:21s
epoch 88 | loss: 0.13099 | val_0_rmse: 0.34676 | val_1_rmse: 0.37736 |  0:01:22s
epoch 89 | loss: 0.13356 | val_0_rmse: 0.33701 | val_1_rmse: 0.36457 |  0:01:23s
epoch 90 | loss: 0.13465 | val_0_rmse: 0.35598 | val_1_rmse: 0.3899  |  0:01:24s
epoch 91 | loss: 0.13248 | val_0_rmse: 0.35601 | val_1_rmse: 0.37927 |  0:01:25s
epoch 92 | loss: 0.13264 | val_0_rmse: 0.34869 | val_1_rmse: 0.37634 |  0:01:26s
epoch 93 | loss: 0.13849 | val_0_rmse: 0.34509 | val_1_rmse: 0.37032 |  0:01:26s
epoch 94 | loss: 0.14567 | val_0_rmse: 0.33505 | val_1_rmse: 0.36285 |  0:01:27s
epoch 95 | loss: 0.14385 | val_0_rmse: 0.35167 | val_1_rmse: 0.37814 |  0:01:28s
epoch 96 | loss: 0.13707 | val_0_rmse: 0.34603 | val_1_rmse: 0.37307 |  0:01:29s
epoch 97 | loss: 0.14107 | val_0_rmse: 0.37163 | val_1_rmse: 0.38856 |  0:01:30s
epoch 98 | loss: 0.13542 | val_0_rmse: 0.375   | val_1_rmse: 0.39295 |  0:01:31s
epoch 99 | loss: 0.13854 | val_0_rmse: 0.33659 | val_1_rmse: 0.36391 |  0:01:32s
epoch 100| loss: 0.13074 | val_0_rmse: 0.34072 | val_1_rmse: 0.36945 |  0:01:33s
epoch 101| loss: 0.13256 | val_0_rmse: 0.33917 | val_1_rmse: 0.36656 |  0:01:34s
epoch 102| loss: 0.13081 | val_0_rmse: 0.34433 | val_1_rmse: 0.37767 |  0:01:35s
epoch 103| loss: 0.12903 | val_0_rmse: 0.32748 | val_1_rmse: 0.35698 |  0:01:36s
epoch 104| loss: 0.1266  | val_0_rmse: 0.32556 | val_1_rmse: 0.35736 |  0:01:37s
epoch 105| loss: 0.1249  | val_0_rmse: 0.32909 | val_1_rmse: 0.36194 |  0:01:38s
epoch 106| loss: 0.12604 | val_0_rmse: 0.3275  | val_1_rmse: 0.35512 |  0:01:38s
epoch 107| loss: 0.12288 | val_0_rmse: 0.33364 | val_1_rmse: 0.36473 |  0:01:39s
epoch 108| loss: 0.12393 | val_0_rmse: 0.33277 | val_1_rmse: 0.36396 |  0:01:40s
epoch 109| loss: 0.12284 | val_0_rmse: 0.34466 | val_1_rmse: 0.37538 |  0:01:41s
epoch 110| loss: 0.12659 | val_0_rmse: 0.36048 | val_1_rmse: 0.38393 |  0:01:42s
epoch 111| loss: 0.12793 | val_0_rmse: 0.33494 | val_1_rmse: 0.36232 |  0:01:43s
epoch 112| loss: 0.12542 | val_0_rmse: 0.34475 | val_1_rmse: 0.37171 |  0:01:44s
epoch 113| loss: 0.12838 | val_0_rmse: 0.3359  | val_1_rmse: 0.36608 |  0:01:45s
epoch 114| loss: 0.1252  | val_0_rmse: 0.3391  | val_1_rmse: 0.37015 |  0:01:46s
epoch 115| loss: 0.12562 | val_0_rmse: 0.33506 | val_1_rmse: 0.37063 |  0:01:47s
epoch 116| loss: 0.12651 | val_0_rmse: 0.33973 | val_1_rmse: 0.37129 |  0:01:48s
epoch 117| loss: 0.12245 | val_0_rmse: 0.34264 | val_1_rmse: 0.37118 |  0:01:49s
epoch 118| loss: 0.12784 | val_0_rmse: 0.33962 | val_1_rmse: 0.37415 |  0:01:50s
epoch 119| loss: 0.12733 | val_0_rmse: 0.36972 | val_1_rmse: 0.39531 |  0:01:50s
epoch 120| loss: 0.13612 | val_0_rmse: 0.3388  | val_1_rmse: 0.37307 |  0:01:51s
epoch 121| loss: 0.13214 | val_0_rmse: 0.34111 | val_1_rmse: 0.37783 |  0:01:52s
epoch 122| loss: 0.12723 | val_0_rmse: 0.34245 | val_1_rmse: 0.37607 |  0:01:53s
epoch 123| loss: 0.12581 | val_0_rmse: 0.32928 | val_1_rmse: 0.36464 |  0:01:54s
epoch 124| loss: 0.12527 | val_0_rmse: 0.33142 | val_1_rmse: 0.36502 |  0:01:55s
epoch 125| loss: 0.12584 | val_0_rmse: 0.32861 | val_1_rmse: 0.36725 |  0:01:56s
epoch 126| loss: 0.12416 | val_0_rmse: 0.3331  | val_1_rmse: 0.36676 |  0:01:57s
epoch 127| loss: 0.12539 | val_0_rmse: 0.32791 | val_1_rmse: 0.36416 |  0:01:58s
epoch 128| loss: 0.12277 | val_0_rmse: 0.33097 | val_1_rmse: 0.36619 |  0:01:59s
epoch 129| loss: 0.12265 | val_0_rmse: 0.33968 | val_1_rmse: 0.37383 |  0:02:00s
epoch 130| loss: 0.12447 | val_0_rmse: 0.33603 | val_1_rmse: 0.37762 |  0:02:01s
epoch 131| loss: 0.12019 | val_0_rmse: 0.31979 | val_1_rmse: 0.35955 |  0:02:02s
epoch 132| loss: 0.11969 | val_0_rmse: 0.34086 | val_1_rmse: 0.38258 |  0:02:03s
epoch 133| loss: 0.13388 | val_0_rmse: 0.34966 | val_1_rmse: 0.384   |  0:02:03s
epoch 134| loss: 0.12938 | val_0_rmse: 0.33084 | val_1_rmse: 0.36758 |  0:02:04s
epoch 135| loss: 0.12774 | val_0_rmse: 0.33703 | val_1_rmse: 0.37651 |  0:02:05s
epoch 136| loss: 0.12385 | val_0_rmse: 0.32789 | val_1_rmse: 0.36227 |  0:02:06s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.35512
Best weights from best epoch are automatically used!
ended training at: 05:40:04
Feature importance:
Mean squared error is of 0.06682510161792095
Mean absolute error:0.1598309631219658
MAPE:0.1827504219042269
R2 score:0.862101638278733
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:40:05
epoch 0  | loss: 1.33364 | val_0_rmse: 0.88359 | val_1_rmse: 0.8831  |  0:00:01s
epoch 1  | loss: 0.69635 | val_0_rmse: 0.8236  | val_1_rmse: 0.82099 |  0:00:01s
epoch 2  | loss: 0.59385 | val_0_rmse: 0.78303 | val_1_rmse: 0.76272 |  0:00:02s
epoch 3  | loss: 0.47385 | val_0_rmse: 0.76636 | val_1_rmse: 0.76204 |  0:00:03s
epoch 4  | loss: 0.41541 | val_0_rmse: 0.73039 | val_1_rmse: 0.71901 |  0:00:04s
epoch 5  | loss: 0.35278 | val_0_rmse: 0.65613 | val_1_rmse: 0.64871 |  0:00:05s
epoch 6  | loss: 0.32799 | val_0_rmse: 0.67098 | val_1_rmse: 0.66049 |  0:00:06s
epoch 7  | loss: 0.36804 | val_0_rmse: 0.7547  | val_1_rmse: 0.74185 |  0:00:07s
epoch 8  | loss: 0.33077 | val_0_rmse: 0.67304 | val_1_rmse: 0.65839 |  0:00:08s
epoch 9  | loss: 0.3399  | val_0_rmse: 0.69794 | val_1_rmse: 0.69081 |  0:00:09s
epoch 10 | loss: 0.34277 | val_0_rmse: 0.62252 | val_1_rmse: 0.61124 |  0:00:10s
epoch 11 | loss: 0.33072 | val_0_rmse: 0.62146 | val_1_rmse: 0.6294  |  0:00:11s
epoch 12 | loss: 0.29171 | val_0_rmse: 0.57482 | val_1_rmse: 0.58246 |  0:00:12s
epoch 13 | loss: 0.26691 | val_0_rmse: 0.57651 | val_1_rmse: 0.58031 |  0:00:13s
epoch 14 | loss: 0.2443  | val_0_rmse: 0.58544 | val_1_rmse: 0.59362 |  0:00:13s
epoch 15 | loss: 0.24103 | val_0_rmse: 0.54498 | val_1_rmse: 0.55077 |  0:00:14s
epoch 16 | loss: 0.22964 | val_0_rmse: 0.55275 | val_1_rmse: 0.55931 |  0:00:15s
epoch 17 | loss: 0.23568 | val_0_rmse: 0.64323 | val_1_rmse: 0.66703 |  0:00:16s
epoch 18 | loss: 0.23778 | val_0_rmse: 0.5828  | val_1_rmse: 0.59801 |  0:00:17s
epoch 19 | loss: 0.22885 | val_0_rmse: 0.57463 | val_1_rmse: 0.58998 |  0:00:18s
epoch 20 | loss: 0.22205 | val_0_rmse: 0.60327 | val_1_rmse: 0.62489 |  0:00:19s
epoch 21 | loss: 0.209   | val_0_rmse: 0.53513 | val_1_rmse: 0.55426 |  0:00:20s
epoch 22 | loss: 0.20288 | val_0_rmse: 0.55943 | val_1_rmse: 0.57825 |  0:00:21s
epoch 23 | loss: 0.20193 | val_0_rmse: 0.51099 | val_1_rmse: 0.52132 |  0:00:22s
epoch 24 | loss: 0.21164 | val_0_rmse: 0.47525 | val_1_rmse: 0.47903 |  0:00:23s
epoch 25 | loss: 0.21479 | val_0_rmse: 0.47695 | val_1_rmse: 0.47427 |  0:00:24s
epoch 26 | loss: 0.206   | val_0_rmse: 0.45406 | val_1_rmse: 0.45396 |  0:00:25s
epoch 27 | loss: 0.20049 | val_0_rmse: 0.44075 | val_1_rmse: 0.43891 |  0:00:25s
epoch 28 | loss: 0.19722 | val_0_rmse: 0.45927 | val_1_rmse: 0.45767 |  0:00:26s
epoch 29 | loss: 0.21749 | val_0_rmse: 0.48316 | val_1_rmse: 0.48514 |  0:00:27s
epoch 30 | loss: 0.21544 | val_0_rmse: 0.48762 | val_1_rmse: 0.49145 |  0:00:28s
epoch 31 | loss: 0.20633 | val_0_rmse: 0.4513  | val_1_rmse: 0.44845 |  0:00:29s
epoch 32 | loss: 0.20445 | val_0_rmse: 0.45048 | val_1_rmse: 0.45011 |  0:00:30s
epoch 33 | loss: 0.20121 | val_0_rmse: 0.42119 | val_1_rmse: 0.41843 |  0:00:31s
epoch 34 | loss: 0.19221 | val_0_rmse: 0.42258 | val_1_rmse: 0.42491 |  0:00:32s
epoch 35 | loss: 0.18869 | val_0_rmse: 0.43235 | val_1_rmse: 0.42657 |  0:00:33s
epoch 36 | loss: 0.19071 | val_0_rmse: 0.42153 | val_1_rmse: 0.41745 |  0:00:34s
epoch 37 | loss: 0.18827 | val_0_rmse: 0.42329 | val_1_rmse: 0.41849 |  0:00:35s
epoch 38 | loss: 0.18698 | val_0_rmse: 0.41375 | val_1_rmse: 0.41621 |  0:00:36s
epoch 39 | loss: 0.18307 | val_0_rmse: 0.4083  | val_1_rmse: 0.40735 |  0:00:37s
epoch 40 | loss: 0.17974 | val_0_rmse: 0.43549 | val_1_rmse: 0.43356 |  0:00:37s
epoch 41 | loss: 0.18881 | val_0_rmse: 0.45439 | val_1_rmse: 0.45244 |  0:00:38s
epoch 42 | loss: 0.18737 | val_0_rmse: 0.42218 | val_1_rmse: 0.42246 |  0:00:39s
epoch 43 | loss: 0.1756  | val_0_rmse: 0.40082 | val_1_rmse: 0.40609 |  0:00:40s
epoch 44 | loss: 0.17136 | val_0_rmse: 0.41685 | val_1_rmse: 0.42204 |  0:00:41s
epoch 45 | loss: 0.16687 | val_0_rmse: 0.39353 | val_1_rmse: 0.39898 |  0:00:42s
epoch 46 | loss: 0.16621 | val_0_rmse: 0.39671 | val_1_rmse: 0.4046  |  0:00:43s
epoch 47 | loss: 0.16821 | val_0_rmse: 0.38788 | val_1_rmse: 0.39662 |  0:00:44s
epoch 48 | loss: 0.16693 | val_0_rmse: 0.38203 | val_1_rmse: 0.39087 |  0:00:45s
epoch 49 | loss: 0.16153 | val_0_rmse: 0.38902 | val_1_rmse: 0.39581 |  0:00:46s
epoch 50 | loss: 0.15638 | val_0_rmse: 0.37361 | val_1_rmse: 0.38464 |  0:00:47s
epoch 51 | loss: 0.15977 | val_0_rmse: 0.37671 | val_1_rmse: 0.38749 |  0:00:48s
epoch 52 | loss: 0.16128 | val_0_rmse: 0.37973 | val_1_rmse: 0.39141 |  0:00:49s
epoch 53 | loss: 0.15836 | val_0_rmse: 0.37537 | val_1_rmse: 0.38951 |  0:00:49s
epoch 54 | loss: 0.16141 | val_0_rmse: 0.37017 | val_1_rmse: 0.38173 |  0:00:50s
epoch 55 | loss: 0.16112 | val_0_rmse: 0.38299 | val_1_rmse: 0.39332 |  0:00:51s
epoch 56 | loss: 0.15537 | val_0_rmse: 0.37114 | val_1_rmse: 0.38463 |  0:00:52s
epoch 57 | loss: 0.1601  | val_0_rmse: 0.38047 | val_1_rmse: 0.39152 |  0:00:53s
epoch 58 | loss: 0.1572  | val_0_rmse: 0.37754 | val_1_rmse: 0.38697 |  0:00:54s
epoch 59 | loss: 0.15708 | val_0_rmse: 0.36111 | val_1_rmse: 0.37557 |  0:00:55s
epoch 60 | loss: 0.15003 | val_0_rmse: 0.36255 | val_1_rmse: 0.37591 |  0:00:56s
epoch 61 | loss: 0.15211 | val_0_rmse: 0.39551 | val_1_rmse: 0.40026 |  0:00:57s
epoch 62 | loss: 0.15751 | val_0_rmse: 0.38734 | val_1_rmse: 0.39817 |  0:00:58s
epoch 63 | loss: 0.15131 | val_0_rmse: 0.36718 | val_1_rmse: 0.3776  |  0:00:59s
epoch 64 | loss: 0.1514  | val_0_rmse: 0.36337 | val_1_rmse: 0.37564 |  0:01:00s
epoch 65 | loss: 0.14951 | val_0_rmse: 0.36313 | val_1_rmse: 0.37437 |  0:01:01s
epoch 66 | loss: 0.14772 | val_0_rmse: 0.35796 | val_1_rmse: 0.37094 |  0:01:01s
epoch 67 | loss: 0.14576 | val_0_rmse: 0.356   | val_1_rmse: 0.37127 |  0:01:02s
epoch 68 | loss: 0.14801 | val_0_rmse: 0.35604 | val_1_rmse: 0.37041 |  0:01:03s
epoch 69 | loss: 0.14186 | val_0_rmse: 0.35915 | val_1_rmse: 0.37335 |  0:01:04s
epoch 70 | loss: 0.14205 | val_0_rmse: 0.34813 | val_1_rmse: 0.36348 |  0:01:05s
epoch 71 | loss: 0.13975 | val_0_rmse: 0.34696 | val_1_rmse: 0.36453 |  0:01:06s
epoch 72 | loss: 0.1381  | val_0_rmse: 0.35208 | val_1_rmse: 0.36385 |  0:01:07s
epoch 73 | loss: 0.14142 | val_0_rmse: 0.35277 | val_1_rmse: 0.36933 |  0:01:08s
epoch 74 | loss: 0.14518 | val_0_rmse: 0.34759 | val_1_rmse: 0.36166 |  0:01:09s
epoch 75 | loss: 0.14818 | val_0_rmse: 0.36328 | val_1_rmse: 0.37918 |  0:01:10s
epoch 76 | loss: 0.14264 | val_0_rmse: 0.36104 | val_1_rmse: 0.37899 |  0:01:11s
epoch 77 | loss: 0.15427 | val_0_rmse: 0.35349 | val_1_rmse: 0.36849 |  0:01:12s
epoch 78 | loss: 0.14387 | val_0_rmse: 0.35929 | val_1_rmse: 0.36981 |  0:01:13s
epoch 79 | loss: 0.14108 | val_0_rmse: 0.34193 | val_1_rmse: 0.35764 |  0:01:14s
epoch 80 | loss: 0.14251 | val_0_rmse: 0.35188 | val_1_rmse: 0.36644 |  0:01:14s
epoch 81 | loss: 0.14389 | val_0_rmse: 0.38159 | val_1_rmse: 0.39697 |  0:01:15s
epoch 82 | loss: 0.15458 | val_0_rmse: 0.39733 | val_1_rmse: 0.40877 |  0:01:16s
epoch 83 | loss: 0.16327 | val_0_rmse: 0.37801 | val_1_rmse: 0.38799 |  0:01:17s
epoch 84 | loss: 0.15086 | val_0_rmse: 0.40692 | val_1_rmse: 0.41307 |  0:01:18s
epoch 85 | loss: 0.1489  | val_0_rmse: 0.36139 | val_1_rmse: 0.37434 |  0:01:19s
epoch 86 | loss: 0.14633 | val_0_rmse: 0.34849 | val_1_rmse: 0.36184 |  0:01:20s
epoch 87 | loss: 0.14208 | val_0_rmse: 0.36464 | val_1_rmse: 0.37658 |  0:01:21s
epoch 88 | loss: 0.14389 | val_0_rmse: 0.34708 | val_1_rmse: 0.36145 |  0:01:22s
epoch 89 | loss: 0.13998 | val_0_rmse: 0.35557 | val_1_rmse: 0.37114 |  0:01:23s
epoch 90 | loss: 0.14133 | val_0_rmse: 0.3706  | val_1_rmse: 0.38398 |  0:01:24s
epoch 91 | loss: 0.13683 | val_0_rmse: 0.35081 | val_1_rmse: 0.36609 |  0:01:25s
epoch 92 | loss: 0.13626 | val_0_rmse: 0.34251 | val_1_rmse: 0.35736 |  0:01:26s
epoch 93 | loss: 0.13551 | val_0_rmse: 0.3408  | val_1_rmse: 0.35724 |  0:01:26s
epoch 94 | loss: 0.13735 | val_0_rmse: 0.35422 | val_1_rmse: 0.36789 |  0:01:27s
epoch 95 | loss: 0.13776 | val_0_rmse: 0.34052 | val_1_rmse: 0.35757 |  0:01:28s
epoch 96 | loss: 0.1373  | val_0_rmse: 0.33731 | val_1_rmse: 0.35239 |  0:01:29s
epoch 97 | loss: 0.13205 | val_0_rmse: 0.3357  | val_1_rmse: 0.35624 |  0:01:30s
epoch 98 | loss: 0.13764 | val_0_rmse: 0.34736 | val_1_rmse: 0.36532 |  0:01:31s
epoch 99 | loss: 0.13307 | val_0_rmse: 0.34466 | val_1_rmse: 0.36198 |  0:01:32s
epoch 100| loss: 0.13503 | val_0_rmse: 0.34519 | val_1_rmse: 0.36345 |  0:01:33s
epoch 101| loss: 0.13495 | val_0_rmse: 0.36705 | val_1_rmse: 0.38512 |  0:01:34s
epoch 102| loss: 0.13464 | val_0_rmse: 0.36878 | val_1_rmse: 0.38649 |  0:01:35s
epoch 103| loss: 0.13301 | val_0_rmse: 0.33257 | val_1_rmse: 0.35067 |  0:01:36s
epoch 104| loss: 0.13038 | val_0_rmse: 0.34526 | val_1_rmse: 0.3663  |  0:01:37s
epoch 105| loss: 0.13153 | val_0_rmse: 0.3386  | val_1_rmse: 0.35799 |  0:01:38s
epoch 106| loss: 0.12711 | val_0_rmse: 0.37741 | val_1_rmse: 0.39653 |  0:01:38s
epoch 107| loss: 0.13099 | val_0_rmse: 0.3545  | val_1_rmse: 0.37686 |  0:01:39s
epoch 108| loss: 0.13451 | val_0_rmse: 0.34539 | val_1_rmse: 0.36906 |  0:01:40s
epoch 109| loss: 0.12857 | val_0_rmse: 0.35064 | val_1_rmse: 0.37113 |  0:01:41s
epoch 110| loss: 0.13034 | val_0_rmse: 0.35551 | val_1_rmse: 0.3783  |  0:01:42s
epoch 111| loss: 0.13505 | val_0_rmse: 0.33577 | val_1_rmse: 0.3608  |  0:01:43s
epoch 112| loss: 0.13113 | val_0_rmse: 0.33607 | val_1_rmse: 0.35604 |  0:01:44s
epoch 113| loss: 0.13082 | val_0_rmse: 0.33405 | val_1_rmse: 0.35734 |  0:01:45s
epoch 114| loss: 0.12562 | val_0_rmse: 0.34904 | val_1_rmse: 0.37143 |  0:01:46s
epoch 115| loss: 0.1331  | val_0_rmse: 0.33899 | val_1_rmse: 0.35803 |  0:01:47s
epoch 116| loss: 0.1315  | val_0_rmse: 0.34094 | val_1_rmse: 0.35928 |  0:01:48s
epoch 117| loss: 0.1281  | val_0_rmse: 0.32742 | val_1_rmse: 0.35123 |  0:01:49s
epoch 118| loss: 0.13141 | val_0_rmse: 0.33625 | val_1_rmse: 0.36146 |  0:01:50s
epoch 119| loss: 0.12834 | val_0_rmse: 0.33374 | val_1_rmse: 0.35933 |  0:01:51s
epoch 120| loss: 0.12711 | val_0_rmse: 0.325   | val_1_rmse: 0.3496  |  0:01:51s
epoch 121| loss: 0.12118 | val_0_rmse: 0.32619 | val_1_rmse: 0.35425 |  0:01:52s
epoch 122| loss: 0.12498 | val_0_rmse: 0.32657 | val_1_rmse: 0.34928 |  0:01:53s
epoch 123| loss: 0.12854 | val_0_rmse: 0.33153 | val_1_rmse: 0.35794 |  0:01:54s
epoch 124| loss: 0.1315  | val_0_rmse: 0.35944 | val_1_rmse: 0.37707 |  0:01:55s
epoch 125| loss: 0.13438 | val_0_rmse: 0.34585 | val_1_rmse: 0.36547 |  0:01:56s
epoch 126| loss: 0.13743 | val_0_rmse: 0.33649 | val_1_rmse: 0.35781 |  0:01:57s
epoch 127| loss: 0.12895 | val_0_rmse: 0.37021 | val_1_rmse: 0.39292 |  0:01:58s
epoch 128| loss: 0.12795 | val_0_rmse: 0.33066 | val_1_rmse: 0.3544  |  0:01:59s
epoch 129| loss: 0.12635 | val_0_rmse: 0.33893 | val_1_rmse: 0.35922 |  0:02:00s
epoch 130| loss: 0.12837 | val_0_rmse: 0.34495 | val_1_rmse: 0.36893 |  0:02:01s
epoch 131| loss: 0.13295 | val_0_rmse: 0.32708 | val_1_rmse: 0.35256 |  0:02:02s
epoch 132| loss: 0.12432 | val_0_rmse: 0.35117 | val_1_rmse: 0.37497 |  0:02:02s
epoch 133| loss: 0.12741 | val_0_rmse: 0.32877 | val_1_rmse: 0.35285 |  0:02:03s
epoch 134| loss: 0.12059 | val_0_rmse: 0.32749 | val_1_rmse: 0.35498 |  0:02:04s
epoch 135| loss: 0.12776 | val_0_rmse: 0.33028 | val_1_rmse: 0.35249 |  0:02:05s
epoch 136| loss: 0.12406 | val_0_rmse: 0.33366 | val_1_rmse: 0.36276 |  0:02:06s
epoch 137| loss: 0.12537 | val_0_rmse: 0.32812 | val_1_rmse: 0.35582 |  0:02:07s
epoch 138| loss: 0.12513 | val_0_rmse: 0.33316 | val_1_rmse: 0.35956 |  0:02:08s
epoch 139| loss: 0.12777 | val_0_rmse: 0.32847 | val_1_rmse: 0.35796 |  0:02:09s
epoch 140| loss: 0.12099 | val_0_rmse: 0.32231 | val_1_rmse: 0.35416 |  0:02:10s
epoch 141| loss: 0.11677 | val_0_rmse: 0.31665 | val_1_rmse: 0.34332 |  0:02:11s
epoch 142| loss: 0.12195 | val_0_rmse: 0.3183  | val_1_rmse: 0.34935 |  0:02:12s
epoch 143| loss: 0.12224 | val_0_rmse: 0.33676 | val_1_rmse: 0.36869 |  0:02:13s
epoch 144| loss: 0.12754 | val_0_rmse: 0.31797 | val_1_rmse: 0.34485 |  0:02:14s
epoch 145| loss: 0.12184 | val_0_rmse: 0.32444 | val_1_rmse: 0.35668 |  0:02:14s
epoch 146| loss: 0.12133 | val_0_rmse: 0.32088 | val_1_rmse: 0.35239 |  0:02:15s
epoch 147| loss: 0.12241 | val_0_rmse: 0.32801 | val_1_rmse: 0.36123 |  0:02:16s
epoch 148| loss: 0.11612 | val_0_rmse: 0.33841 | val_1_rmse: 0.36587 |  0:02:17s
epoch 149| loss: 0.12205 | val_0_rmse: 0.34124 | val_1_rmse: 0.37273 |  0:02:18s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.34332
Best weights from best epoch are automatically used!
ended training at: 05:42:24
Feature importance:
Mean squared error is of 0.05940406562510932
Mean absolute error:0.1566899664123278
MAPE:0.18995367378618355
R2 score:0.8769051155028437
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:42:25
epoch 0  | loss: 1.35669 | val_0_rmse: 0.83211 | val_1_rmse: 0.85245 |  0:00:00s
epoch 1  | loss: 0.67196 | val_0_rmse: 0.81219 | val_1_rmse: 0.82377 |  0:00:01s
epoch 2  | loss: 0.4898  | val_0_rmse: 0.76305 | val_1_rmse: 0.77343 |  0:00:02s
epoch 3  | loss: 0.42514 | val_0_rmse: 0.72426 | val_1_rmse: 0.73712 |  0:00:03s
epoch 4  | loss: 0.35129 | val_0_rmse: 0.69996 | val_1_rmse: 0.72013 |  0:00:04s
epoch 5  | loss: 0.31802 | val_0_rmse: 0.6504  | val_1_rmse: 0.66934 |  0:00:05s
epoch 6  | loss: 0.2913  | val_0_rmse: 0.63895 | val_1_rmse: 0.65949 |  0:00:06s
epoch 7  | loss: 0.25331 | val_0_rmse: 0.63914 | val_1_rmse: 0.65585 |  0:00:07s
epoch 8  | loss: 0.24455 | val_0_rmse: 0.60976 | val_1_rmse: 0.62242 |  0:00:08s
epoch 9  | loss: 0.24743 | val_0_rmse: 0.6039  | val_1_rmse: 0.61824 |  0:00:09s
epoch 10 | loss: 0.23292 | val_0_rmse: 0.60159 | val_1_rmse: 0.61105 |  0:00:10s
epoch 11 | loss: 0.23249 | val_0_rmse: 0.59158 | val_1_rmse: 0.60851 |  0:00:11s
epoch 12 | loss: 0.21393 | val_0_rmse: 0.61065 | val_1_rmse: 0.6277  |  0:00:12s
epoch 13 | loss: 0.21176 | val_0_rmse: 0.57294 | val_1_rmse: 0.58775 |  0:00:12s
epoch 14 | loss: 0.19484 | val_0_rmse: 0.56101 | val_1_rmse: 0.57082 |  0:00:13s
epoch 15 | loss: 0.19116 | val_0_rmse: 0.53094 | val_1_rmse: 0.53869 |  0:00:14s
epoch 16 | loss: 0.18269 | val_0_rmse: 0.55357 | val_1_rmse: 0.56024 |  0:00:15s
epoch 17 | loss: 0.18926 | val_0_rmse: 0.54064 | val_1_rmse: 0.54363 |  0:00:16s
epoch 18 | loss: 0.18023 | val_0_rmse: 0.52784 | val_1_rmse: 0.53435 |  0:00:17s
epoch 19 | loss: 0.18087 | val_0_rmse: 0.49428 | val_1_rmse: 0.48877 |  0:00:18s
epoch 20 | loss: 0.17842 | val_0_rmse: 0.49222 | val_1_rmse: 0.48387 |  0:00:19s
epoch 21 | loss: 0.17746 | val_0_rmse: 0.49211 | val_1_rmse: 0.48619 |  0:00:20s
epoch 22 | loss: 0.18258 | val_0_rmse: 0.47509 | val_1_rmse: 0.47333 |  0:00:21s
epoch 23 | loss: 0.17702 | val_0_rmse: 0.48937 | val_1_rmse: 0.48299 |  0:00:22s
epoch 24 | loss: 0.17486 | val_0_rmse: 0.47413 | val_1_rmse: 0.4636  |  0:00:23s
epoch 25 | loss: 0.1772  | val_0_rmse: 0.44433 | val_1_rmse: 0.43156 |  0:00:24s
epoch 26 | loss: 0.17184 | val_0_rmse: 0.45062 | val_1_rmse: 0.44803 |  0:00:25s
epoch 27 | loss: 0.17489 | val_0_rmse: 0.41675 | val_1_rmse: 0.40668 |  0:00:26s
epoch 28 | loss: 0.17103 | val_0_rmse: 0.41995 | val_1_rmse: 0.40899 |  0:00:26s
epoch 29 | loss: 0.16444 | val_0_rmse: 0.43199 | val_1_rmse: 0.41964 |  0:00:27s
epoch 30 | loss: 0.16896 | val_0_rmse: 0.44199 | val_1_rmse: 0.43841 |  0:00:28s
epoch 31 | loss: 0.17113 | val_0_rmse: 0.41103 | val_1_rmse: 0.40942 |  0:00:29s
epoch 32 | loss: 0.1665  | val_0_rmse: 0.41017 | val_1_rmse: 0.41023 |  0:00:30s
epoch 33 | loss: 0.16654 | val_0_rmse: 0.40833 | val_1_rmse: 0.41401 |  0:00:31s
epoch 34 | loss: 0.16581 | val_0_rmse: 0.41144 | val_1_rmse: 0.40429 |  0:00:32s
epoch 35 | loss: 0.17178 | val_0_rmse: 0.41243 | val_1_rmse: 0.40971 |  0:00:33s
epoch 36 | loss: 0.16635 | val_0_rmse: 0.40795 | val_1_rmse: 0.40578 |  0:00:34s
epoch 37 | loss: 0.16236 | val_0_rmse: 0.40099 | val_1_rmse: 0.39984 |  0:00:35s
epoch 38 | loss: 0.16461 | val_0_rmse: 0.38324 | val_1_rmse: 0.38031 |  0:00:36s
epoch 39 | loss: 0.16069 | val_0_rmse: 0.38555 | val_1_rmse: 0.38274 |  0:00:37s
epoch 40 | loss: 0.15869 | val_0_rmse: 0.37422 | val_1_rmse: 0.36921 |  0:00:38s
epoch 41 | loss: 0.16121 | val_0_rmse: 0.39954 | val_1_rmse: 0.40288 |  0:00:39s
epoch 42 | loss: 0.16541 | val_0_rmse: 0.37393 | val_1_rmse: 0.37234 |  0:00:39s
epoch 43 | loss: 0.1611  | val_0_rmse: 0.37217 | val_1_rmse: 0.36651 |  0:00:40s
epoch 44 | loss: 0.15136 | val_0_rmse: 0.37327 | val_1_rmse: 0.37602 |  0:00:41s
epoch 45 | loss: 0.14845 | val_0_rmse: 0.38025 | val_1_rmse: 0.37803 |  0:00:42s
epoch 46 | loss: 0.15335 | val_0_rmse: 0.36152 | val_1_rmse: 0.35598 |  0:00:43s
epoch 47 | loss: 0.15881 | val_0_rmse: 0.36561 | val_1_rmse: 0.36536 |  0:00:44s
epoch 48 | loss: 0.15776 | val_0_rmse: 0.37548 | val_1_rmse: 0.37888 |  0:00:45s
epoch 49 | loss: 0.15491 | val_0_rmse: 0.36614 | val_1_rmse: 0.3686  |  0:00:46s
epoch 50 | loss: 0.14784 | val_0_rmse: 0.37251 | val_1_rmse: 0.37272 |  0:00:47s
epoch 51 | loss: 0.14803 | val_0_rmse: 0.36546 | val_1_rmse: 0.36289 |  0:00:48s
epoch 52 | loss: 0.14606 | val_0_rmse: 0.37005 | val_1_rmse: 0.36746 |  0:00:49s
epoch 53 | loss: 0.15248 | val_0_rmse: 0.36141 | val_1_rmse: 0.36183 |  0:00:50s
epoch 54 | loss: 0.15059 | val_0_rmse: 0.3597  | val_1_rmse: 0.35449 |  0:00:51s
epoch 55 | loss: 0.15263 | val_0_rmse: 0.37641 | val_1_rmse: 0.37819 |  0:00:52s
epoch 56 | loss: 0.15119 | val_0_rmse: 0.41032 | val_1_rmse: 0.40577 |  0:00:52s
epoch 57 | loss: 0.1515  | val_0_rmse: 0.38352 | val_1_rmse: 0.38411 |  0:00:53s
epoch 58 | loss: 0.14605 | val_0_rmse: 0.35136 | val_1_rmse: 0.35255 |  0:00:54s
epoch 59 | loss: 0.14221 | val_0_rmse: 0.34899 | val_1_rmse: 0.35106 |  0:00:55s
epoch 60 | loss: 0.14558 | val_0_rmse: 0.35592 | val_1_rmse: 0.36056 |  0:00:56s
epoch 61 | loss: 0.14854 | val_0_rmse: 0.38931 | val_1_rmse: 0.38652 |  0:00:57s
epoch 62 | loss: 0.14712 | val_0_rmse: 0.36016 | val_1_rmse: 0.36109 |  0:00:58s
epoch 63 | loss: 0.14486 | val_0_rmse: 0.35791 | val_1_rmse: 0.36408 |  0:00:59s
epoch 64 | loss: 0.14451 | val_0_rmse: 0.35129 | val_1_rmse: 0.34966 |  0:01:00s
epoch 65 | loss: 0.1367  | val_0_rmse: 0.36838 | val_1_rmse: 0.37323 |  0:01:01s
epoch 66 | loss: 0.14045 | val_0_rmse: 0.35032 | val_1_rmse: 0.34936 |  0:01:02s
epoch 67 | loss: 0.14416 | val_0_rmse: 0.3555  | val_1_rmse: 0.36084 |  0:01:03s
epoch 68 | loss: 0.14173 | val_0_rmse: 0.37762 | val_1_rmse: 0.3798  |  0:01:04s
epoch 69 | loss: 0.14027 | val_0_rmse: 0.36275 | val_1_rmse: 0.36519 |  0:01:04s
epoch 70 | loss: 0.13851 | val_0_rmse: 0.34999 | val_1_rmse: 0.3525  |  0:01:05s
epoch 71 | loss: 0.13737 | val_0_rmse: 0.33481 | val_1_rmse: 0.33826 |  0:01:06s
epoch 72 | loss: 0.13521 | val_0_rmse: 0.36524 | val_1_rmse: 0.37306 |  0:01:07s
epoch 73 | loss: 0.13746 | val_0_rmse: 0.349   | val_1_rmse: 0.35253 |  0:01:08s
epoch 74 | loss: 0.13266 | val_0_rmse: 0.36733 | val_1_rmse: 0.36998 |  0:01:09s
epoch 75 | loss: 0.15578 | val_0_rmse: 0.37503 | val_1_rmse: 0.37129 |  0:01:10s
epoch 76 | loss: 0.15384 | val_0_rmse: 0.36022 | val_1_rmse: 0.35746 |  0:01:11s
epoch 77 | loss: 0.15145 | val_0_rmse: 0.3744  | val_1_rmse: 0.37628 |  0:01:12s
epoch 78 | loss: 0.14282 | val_0_rmse: 0.34659 | val_1_rmse: 0.34741 |  0:01:13s
epoch 79 | loss: 0.13913 | val_0_rmse: 0.34336 | val_1_rmse: 0.34467 |  0:01:14s
epoch 80 | loss: 0.14177 | val_0_rmse: 0.35181 | val_1_rmse: 0.35104 |  0:01:15s
epoch 81 | loss: 0.13712 | val_0_rmse: 0.3633  | val_1_rmse: 0.36537 |  0:01:16s
epoch 82 | loss: 0.13657 | val_0_rmse: 0.34419 | val_1_rmse: 0.34756 |  0:01:16s
epoch 83 | loss: 0.13404 | val_0_rmse: 0.3463  | val_1_rmse: 0.34911 |  0:01:17s
epoch 84 | loss: 0.13851 | val_0_rmse: 0.3397  | val_1_rmse: 0.34685 |  0:01:18s
epoch 85 | loss: 0.1375  | val_0_rmse: 0.35232 | val_1_rmse: 0.35354 |  0:01:19s
epoch 86 | loss: 0.13562 | val_0_rmse: 0.3366  | val_1_rmse: 0.34405 |  0:01:20s
epoch 87 | loss: 0.13382 | val_0_rmse: 0.33431 | val_1_rmse: 0.34525 |  0:01:21s
epoch 88 | loss: 0.13655 | val_0_rmse: 0.3775  | val_1_rmse: 0.38532 |  0:01:22s
epoch 89 | loss: 0.13499 | val_0_rmse: 0.33894 | val_1_rmse: 0.33856 |  0:01:23s
epoch 90 | loss: 0.13345 | val_0_rmse: 0.33944 | val_1_rmse: 0.34853 |  0:01:24s
epoch 91 | loss: 0.13452 | val_0_rmse: 0.3555  | val_1_rmse: 0.36067 |  0:01:25s
epoch 92 | loss: 0.13305 | val_0_rmse: 0.34783 | val_1_rmse: 0.35377 |  0:01:26s
epoch 93 | loss: 0.12966 | val_0_rmse: 0.36476 | val_1_rmse: 0.37023 |  0:01:27s
epoch 94 | loss: 0.13131 | val_0_rmse: 0.39216 | val_1_rmse: 0.39789 |  0:01:28s
epoch 95 | loss: 0.14741 | val_0_rmse: 0.34338 | val_1_rmse: 0.34829 |  0:01:28s
epoch 96 | loss: 0.13876 | val_0_rmse: 0.35539 | val_1_rmse: 0.36623 |  0:01:29s
epoch 97 | loss: 0.13652 | val_0_rmse: 0.33303 | val_1_rmse: 0.34245 |  0:01:30s
epoch 98 | loss: 0.13305 | val_0_rmse: 0.33767 | val_1_rmse: 0.35008 |  0:01:31s
epoch 99 | loss: 0.12768 | val_0_rmse: 0.33444 | val_1_rmse: 0.33806 |  0:01:32s
epoch 100| loss: 0.13376 | val_0_rmse: 0.34681 | val_1_rmse: 0.35591 |  0:01:33s
epoch 101| loss: 0.13347 | val_0_rmse: 0.37021 | val_1_rmse: 0.38478 |  0:01:34s
epoch 102| loss: 0.12803 | val_0_rmse: 0.33105 | val_1_rmse: 0.3411  |  0:01:35s
epoch 103| loss: 0.13098 | val_0_rmse: 0.34366 | val_1_rmse: 0.34996 |  0:01:36s
epoch 104| loss: 0.12646 | val_0_rmse: 0.33049 | val_1_rmse: 0.33811 |  0:01:37s
epoch 105| loss: 0.12474 | val_0_rmse: 0.32676 | val_1_rmse: 0.33905 |  0:01:38s
epoch 106| loss: 0.12345 | val_0_rmse: 0.32918 | val_1_rmse: 0.33833 |  0:01:39s
epoch 107| loss: 0.12469 | val_0_rmse: 0.32189 | val_1_rmse: 0.33286 |  0:01:40s
epoch 108| loss: 0.1246  | val_0_rmse: 0.32381 | val_1_rmse: 0.33554 |  0:01:41s
epoch 109| loss: 0.12042 | val_0_rmse: 0.31848 | val_1_rmse: 0.33692 |  0:01:41s
epoch 110| loss: 0.11914 | val_0_rmse: 0.33191 | val_1_rmse: 0.34564 |  0:01:42s
epoch 111| loss: 0.12766 | val_0_rmse: 0.3291  | val_1_rmse: 0.34748 |  0:01:43s
epoch 112| loss: 0.12752 | val_0_rmse: 0.33355 | val_1_rmse: 0.33846 |  0:01:44s
epoch 113| loss: 0.13155 | val_0_rmse: 0.35952 | val_1_rmse: 0.36996 |  0:01:45s
epoch 114| loss: 0.13099 | val_0_rmse: 0.33262 | val_1_rmse: 0.34567 |  0:01:46s
epoch 115| loss: 0.12569 | val_0_rmse: 0.35145 | val_1_rmse: 0.3634  |  0:01:47s
epoch 116| loss: 0.13073 | val_0_rmse: 0.33082 | val_1_rmse: 0.34566 |  0:01:48s
epoch 117| loss: 0.12283 | val_0_rmse: 0.32657 | val_1_rmse: 0.34201 |  0:01:49s
epoch 118| loss: 0.12644 | val_0_rmse: 0.32903 | val_1_rmse: 0.33306 |  0:01:50s
epoch 119| loss: 0.12349 | val_0_rmse: 0.33126 | val_1_rmse: 0.3449  |  0:01:51s
epoch 120| loss: 0.12015 | val_0_rmse: 0.32972 | val_1_rmse: 0.33786 |  0:01:52s
epoch 121| loss: 0.12508 | val_0_rmse: 0.33219 | val_1_rmse: 0.3453  |  0:01:53s
epoch 122| loss: 0.12558 | val_0_rmse: 0.33456 | val_1_rmse: 0.34414 |  0:01:53s
epoch 123| loss: 0.12763 | val_0_rmse: 0.32592 | val_1_rmse: 0.34291 |  0:01:54s
epoch 124| loss: 0.12685 | val_0_rmse: 0.33426 | val_1_rmse: 0.34651 |  0:01:55s
epoch 125| loss: 0.12242 | val_0_rmse: 0.32783 | val_1_rmse: 0.34242 |  0:01:56s
epoch 126| loss: 0.12422 | val_0_rmse: 0.33843 | val_1_rmse: 0.35275 |  0:01:57s
epoch 127| loss: 0.12869 | val_0_rmse: 0.32969 | val_1_rmse: 0.33997 |  0:01:58s
epoch 128| loss: 0.13013 | val_0_rmse: 0.35434 | val_1_rmse: 0.36503 |  0:01:59s
epoch 129| loss: 0.1326  | val_0_rmse: 0.35238 | val_1_rmse: 0.36265 |  0:02:00s
epoch 130| loss: 0.12889 | val_0_rmse: 0.3299  | val_1_rmse: 0.34602 |  0:02:01s
epoch 131| loss: 0.12575 | val_0_rmse: 0.32601 | val_1_rmse: 0.33989 |  0:02:02s
epoch 132| loss: 0.12416 | val_0_rmse: 0.33202 | val_1_rmse: 0.34391 |  0:02:03s
epoch 133| loss: 0.11876 | val_0_rmse: 0.31756 | val_1_rmse: 0.32963 |  0:02:04s
epoch 134| loss: 0.12095 | val_0_rmse: 0.31978 | val_1_rmse: 0.3394  |  0:02:05s
epoch 135| loss: 0.12404 | val_0_rmse: 0.33179 | val_1_rmse: 0.34339 |  0:02:05s
epoch 136| loss: 0.1263  | val_0_rmse: 0.33865 | val_1_rmse: 0.35351 |  0:02:06s
epoch 137| loss: 0.11983 | val_0_rmse: 0.3149  | val_1_rmse: 0.3318  |  0:02:07s
epoch 138| loss: 0.12499 | val_0_rmse: 0.34978 | val_1_rmse: 0.36661 |  0:02:08s
epoch 139| loss: 0.12319 | val_0_rmse: 0.33843 | val_1_rmse: 0.34819 |  0:02:09s
epoch 140| loss: 0.12551 | val_0_rmse: 0.32705 | val_1_rmse: 0.34076 |  0:02:10s
epoch 141| loss: 0.12861 | val_0_rmse: 0.34583 | val_1_rmse: 0.35736 |  0:02:11s
epoch 142| loss: 0.1327  | val_0_rmse: 0.34208 | val_1_rmse: 0.35064 |  0:02:12s
epoch 143| loss: 0.12535 | val_0_rmse: 0.33126 | val_1_rmse: 0.34793 |  0:02:13s
epoch 144| loss: 0.12203 | val_0_rmse: 0.34148 | val_1_rmse: 0.35648 |  0:02:14s
epoch 145| loss: 0.1389  | val_0_rmse: 0.35655 | val_1_rmse: 0.36542 |  0:02:15s
epoch 146| loss: 0.12952 | val_0_rmse: 0.33102 | val_1_rmse: 0.34278 |  0:02:16s
epoch 147| loss: 0.12452 | val_0_rmse: 0.32288 | val_1_rmse: 0.34022 |  0:02:17s
epoch 148| loss: 0.12579 | val_0_rmse: 0.35725 | val_1_rmse: 0.3714  |  0:02:18s
epoch 149| loss: 0.12177 | val_0_rmse: 0.33048 | val_1_rmse: 0.35139 |  0:02:18s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.32963
Best weights from best epoch are automatically used!
ended training at: 05:44:44
Feature importance:
Mean squared error is of 0.06377051538630532
Mean absolute error:0.15854308524642202
MAPE:0.18407734910925855
R2 score:0.8725995893796575
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:44:45
epoch 0  | loss: 1.57518 | val_0_rmse: 1.00215 | val_1_rmse: 1.01684 |  0:00:00s
epoch 1  | loss: 1.0324  | val_0_rmse: 0.94813 | val_1_rmse: 0.93755 |  0:00:00s
epoch 2  | loss: 0.93976 | val_0_rmse: 0.94414 | val_1_rmse: 0.92381 |  0:00:00s
epoch 3  | loss: 0.89141 | val_0_rmse: 0.9272  | val_1_rmse: 0.91131 |  0:00:01s
epoch 4  | loss: 0.87082 | val_0_rmse: 0.9385  | val_1_rmse: 0.94202 |  0:00:01s
epoch 5  | loss: 0.85663 | val_0_rmse: 0.92391 | val_1_rmse: 0.91506 |  0:00:01s
epoch 6  | loss: 0.83359 | val_0_rmse: 0.90404 | val_1_rmse: 0.90829 |  0:00:02s
epoch 7  | loss: 0.82726 | val_0_rmse: 0.9107  | val_1_rmse: 0.90295 |  0:00:02s
epoch 8  | loss: 0.82142 | val_0_rmse: 0.89215 | val_1_rmse: 0.88849 |  0:00:02s
epoch 9  | loss: 0.78719 | val_0_rmse: 0.90146 | val_1_rmse: 0.88521 |  0:00:03s
epoch 10 | loss: 0.77853 | val_0_rmse: 0.89969 | val_1_rmse: 0.87892 |  0:00:03s
epoch 11 | loss: 0.72999 | val_0_rmse: 0.86503 | val_1_rmse: 0.84362 |  0:00:03s
epoch 12 | loss: 0.70092 | val_0_rmse: 0.86926 | val_1_rmse: 0.80037 |  0:00:03s
epoch 13 | loss: 0.69131 | val_0_rmse: 0.85205 | val_1_rmse: 0.7774  |  0:00:04s
epoch 14 | loss: 0.67927 | val_0_rmse: 0.8408  | val_1_rmse: 0.77253 |  0:00:04s
epoch 15 | loss: 0.66677 | val_0_rmse: 0.82049 | val_1_rmse: 0.76199 |  0:00:04s
epoch 16 | loss: 0.64571 | val_0_rmse: 0.80703 | val_1_rmse: 0.74514 |  0:00:05s
epoch 17 | loss: 0.64177 | val_0_rmse: 0.80506 | val_1_rmse: 0.752   |  0:00:05s
epoch 18 | loss: 0.64527 | val_0_rmse: 0.80987 | val_1_rmse: 0.7763  |  0:00:05s
epoch 19 | loss: 0.63162 | val_0_rmse: 0.80277 | val_1_rmse: 0.75936 |  0:00:06s
epoch 20 | loss: 0.63533 | val_0_rmse: 0.79838 | val_1_rmse: 0.75209 |  0:00:06s
epoch 21 | loss: 0.62961 | val_0_rmse: 0.8046  | val_1_rmse: 0.77092 |  0:00:06s
epoch 22 | loss: 0.62415 | val_0_rmse: 0.79649 | val_1_rmse: 0.75293 |  0:00:07s
epoch 23 | loss: 0.63699 | val_0_rmse: 0.80304 | val_1_rmse: 0.76393 |  0:00:07s
epoch 24 | loss: 0.63862 | val_0_rmse: 0.8024  | val_1_rmse: 0.76613 |  0:00:07s
epoch 25 | loss: 0.60472 | val_0_rmse: 0.79475 | val_1_rmse: 0.74893 |  0:00:08s
epoch 26 | loss: 0.61968 | val_0_rmse: 0.79867 | val_1_rmse: 0.76154 |  0:00:08s
epoch 27 | loss: 0.6132  | val_0_rmse: 0.79738 | val_1_rmse: 0.75524 |  0:00:08s
epoch 28 | loss: 0.60039 | val_0_rmse: 0.79588 | val_1_rmse: 0.75261 |  0:00:08s
epoch 29 | loss: 0.59812 | val_0_rmse: 0.79827 | val_1_rmse: 0.76646 |  0:00:09s
epoch 30 | loss: 0.60182 | val_0_rmse: 0.79425 | val_1_rmse: 0.75104 |  0:00:09s
epoch 31 | loss: 0.59412 | val_0_rmse: 0.80148 | val_1_rmse: 0.75465 |  0:00:09s
epoch 32 | loss: 0.60203 | val_0_rmse: 0.80117 | val_1_rmse: 0.76027 |  0:00:10s
epoch 33 | loss: 0.60804 | val_0_rmse: 0.79954 | val_1_rmse: 0.75992 |  0:00:10s
epoch 34 | loss: 0.60109 | val_0_rmse: 0.79361 | val_1_rmse: 0.7421  |  0:00:10s
epoch 35 | loss: 0.59059 | val_0_rmse: 0.79127 | val_1_rmse: 0.74698 |  0:00:11s
epoch 36 | loss: 0.59238 | val_0_rmse: 0.78943 | val_1_rmse: 0.7384  |  0:00:11s
epoch 37 | loss: 0.58386 | val_0_rmse: 0.79483 | val_1_rmse: 0.74249 |  0:00:11s
epoch 38 | loss: 0.57538 | val_0_rmse: 0.79085 | val_1_rmse: 0.73834 |  0:00:11s
epoch 39 | loss: 0.58374 | val_0_rmse: 0.79823 | val_1_rmse: 0.74227 |  0:00:12s
epoch 40 | loss: 0.58461 | val_0_rmse: 0.79    | val_1_rmse: 0.73533 |  0:00:12s
epoch 41 | loss: 0.58004 | val_0_rmse: 0.78251 | val_1_rmse: 0.74107 |  0:00:12s
epoch 42 | loss: 0.57768 | val_0_rmse: 0.78377 | val_1_rmse: 0.74409 |  0:00:13s
epoch 43 | loss: 0.5717  | val_0_rmse: 0.78145 | val_1_rmse: 0.74583 |  0:00:13s
epoch 44 | loss: 0.56606 | val_0_rmse: 0.77893 | val_1_rmse: 0.73733 |  0:00:13s
epoch 45 | loss: 0.56375 | val_0_rmse: 0.77924 | val_1_rmse: 0.74751 |  0:00:14s
epoch 46 | loss: 0.57253 | val_0_rmse: 0.77871 | val_1_rmse: 0.74181 |  0:00:14s
epoch 47 | loss: 0.56755 | val_0_rmse: 0.77487 | val_1_rmse: 0.74068 |  0:00:14s
epoch 48 | loss: 0.56526 | val_0_rmse: 0.78125 | val_1_rmse: 0.73718 |  0:00:14s
epoch 49 | loss: 0.57007 | val_0_rmse: 0.7743  | val_1_rmse: 0.73353 |  0:00:15s
epoch 50 | loss: 0.5628  | val_0_rmse: 0.77234 | val_1_rmse: 0.73894 |  0:00:15s
epoch 51 | loss: 0.55832 | val_0_rmse: 0.77346 | val_1_rmse: 0.74527 |  0:00:15s
epoch 52 | loss: 0.55712 | val_0_rmse: 0.76718 | val_1_rmse: 0.73997 |  0:00:16s
epoch 53 | loss: 0.55075 | val_0_rmse: 0.76529 | val_1_rmse: 0.72982 |  0:00:16s
epoch 54 | loss: 0.54866 | val_0_rmse: 0.76706 | val_1_rmse: 0.72902 |  0:00:16s
epoch 55 | loss: 0.54876 | val_0_rmse: 0.76691 | val_1_rmse: 0.74251 |  0:00:17s
epoch 56 | loss: 0.55135 | val_0_rmse: 0.77245 | val_1_rmse: 0.73978 |  0:00:17s
epoch 57 | loss: 0.54566 | val_0_rmse: 0.76583 | val_1_rmse: 0.73461 |  0:00:17s
epoch 58 | loss: 0.55116 | val_0_rmse: 0.76173 | val_1_rmse: 0.74806 |  0:00:18s
epoch 59 | loss: 0.54916 | val_0_rmse: 0.7605  | val_1_rmse: 0.73507 |  0:00:18s
epoch 60 | loss: 0.54858 | val_0_rmse: 0.7567  | val_1_rmse: 0.73605 |  0:00:18s
epoch 61 | loss: 0.54368 | val_0_rmse: 0.76392 | val_1_rmse: 0.73869 |  0:00:19s
epoch 62 | loss: 0.54377 | val_0_rmse: 0.75856 | val_1_rmse: 0.73345 |  0:00:19s
epoch 63 | loss: 0.5368  | val_0_rmse: 0.75905 | val_1_rmse: 0.73832 |  0:00:19s
epoch 64 | loss: 0.54528 | val_0_rmse: 0.75998 | val_1_rmse: 0.74039 |  0:00:19s
epoch 65 | loss: 0.54333 | val_0_rmse: 0.75817 | val_1_rmse: 0.73813 |  0:00:20s
epoch 66 | loss: 0.55275 | val_0_rmse: 0.7533  | val_1_rmse: 0.73745 |  0:00:20s
epoch 67 | loss: 0.53843 | val_0_rmse: 0.75677 | val_1_rmse: 0.7448  |  0:00:20s
epoch 68 | loss: 0.53727 | val_0_rmse: 0.74948 | val_1_rmse: 0.72961 |  0:00:21s
epoch 69 | loss: 0.53386 | val_0_rmse: 0.75496 | val_1_rmse: 0.72967 |  0:00:21s
epoch 70 | loss: 0.53959 | val_0_rmse: 0.75454 | val_1_rmse: 0.73308 |  0:00:21s
epoch 71 | loss: 0.54276 | val_0_rmse: 0.74599 | val_1_rmse: 0.72487 |  0:00:21s
epoch 72 | loss: 0.5392  | val_0_rmse: 0.74688 | val_1_rmse: 0.72554 |  0:00:22s
epoch 73 | loss: 0.53319 | val_0_rmse: 0.74031 | val_1_rmse: 0.73254 |  0:00:22s
epoch 74 | loss: 0.53229 | val_0_rmse: 0.73955 | val_1_rmse: 0.72905 |  0:00:22s
epoch 75 | loss: 0.53088 | val_0_rmse: 0.73856 | val_1_rmse: 0.73381 |  0:00:23s
epoch 76 | loss: 0.52906 | val_0_rmse: 0.74837 | val_1_rmse: 0.73606 |  0:00:23s
epoch 77 | loss: 0.52642 | val_0_rmse: 0.73392 | val_1_rmse: 0.7303  |  0:00:23s
epoch 78 | loss: 0.52661 | val_0_rmse: 0.73182 | val_1_rmse: 0.72297 |  0:00:24s
epoch 79 | loss: 0.52411 | val_0_rmse: 0.72868 | val_1_rmse: 0.72744 |  0:00:24s
epoch 80 | loss: 0.51448 | val_0_rmse: 0.72825 | val_1_rmse: 0.73471 |  0:00:24s
epoch 81 | loss: 0.51588 | val_0_rmse: 0.72867 | val_1_rmse: 0.73971 |  0:00:25s
epoch 82 | loss: 0.51619 | val_0_rmse: 0.7255  | val_1_rmse: 0.73401 |  0:00:25s
epoch 83 | loss: 0.51639 | val_0_rmse: 0.72358 | val_1_rmse: 0.74457 |  0:00:25s
epoch 84 | loss: 0.52008 | val_0_rmse: 0.72721 | val_1_rmse: 0.74564 |  0:00:25s
epoch 85 | loss: 0.5159  | val_0_rmse: 0.73047 | val_1_rmse: 0.75262 |  0:00:26s
epoch 86 | loss: 0.52208 | val_0_rmse: 0.72237 | val_1_rmse: 0.74179 |  0:00:26s
epoch 87 | loss: 0.51963 | val_0_rmse: 0.72494 | val_1_rmse: 0.73026 |  0:00:26s
epoch 88 | loss: 0.51452 | val_0_rmse: 0.72316 | val_1_rmse: 0.73431 |  0:00:27s
epoch 89 | loss: 0.51621 | val_0_rmse: 0.72381 | val_1_rmse: 0.7252  |  0:00:27s
epoch 90 | loss: 0.5182  | val_0_rmse: 0.71544 | val_1_rmse: 0.72894 |  0:00:27s
epoch 91 | loss: 0.51111 | val_0_rmse: 0.70794 | val_1_rmse: 0.73552 |  0:00:28s
epoch 92 | loss: 0.50932 | val_0_rmse: 0.70734 | val_1_rmse: 0.7394  |  0:00:28s
epoch 93 | loss: 0.49648 | val_0_rmse: 0.71804 | val_1_rmse: 0.75782 |  0:00:28s
epoch 94 | loss: 0.49657 | val_0_rmse: 0.70707 | val_1_rmse: 0.74506 |  0:00:29s
epoch 95 | loss: 0.48861 | val_0_rmse: 0.70745 | val_1_rmse: 0.74157 |  0:00:29s
epoch 96 | loss: 0.49821 | val_0_rmse: 0.71013 | val_1_rmse: 0.73571 |  0:00:29s
epoch 97 | loss: 0.50816 | val_0_rmse: 0.70194 | val_1_rmse: 0.74819 |  0:00:29s
epoch 98 | loss: 0.49518 | val_0_rmse: 0.70175 | val_1_rmse: 0.74605 |  0:00:30s
epoch 99 | loss: 0.49046 | val_0_rmse: 0.69897 | val_1_rmse: 0.74712 |  0:00:30s
epoch 100| loss: 0.4986  | val_0_rmse: 0.69735 | val_1_rmse: 0.7363  |  0:00:30s
epoch 101| loss: 0.4906  | val_0_rmse: 0.69413 | val_1_rmse: 0.74153 |  0:00:31s
epoch 102| loss: 0.47635 | val_0_rmse: 0.6892  | val_1_rmse: 0.76944 |  0:00:31s
epoch 103| loss: 0.48655 | val_0_rmse: 0.68756 | val_1_rmse: 0.73792 |  0:00:31s
epoch 104| loss: 0.47917 | val_0_rmse: 0.68863 | val_1_rmse: 0.74944 |  0:00:32s
epoch 105| loss: 0.48609 | val_0_rmse: 0.68578 | val_1_rmse: 0.77744 |  0:00:32s
epoch 106| loss: 0.47284 | val_0_rmse: 0.68038 | val_1_rmse: 0.76594 |  0:00:32s
epoch 107| loss: 0.47612 | val_0_rmse: 0.6823  | val_1_rmse: 0.72844 |  0:00:32s
epoch 108| loss: 0.4655  | val_0_rmse: 0.6851  | val_1_rmse: 0.73355 |  0:00:33s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.72297
Best weights from best epoch are automatically used!
ended training at: 05:45:18
Feature importance:
Mean squared error is of 0.035168777571894516
Mean absolute error:0.1437241219491445
MAPE:0.15510504054886867
R2 score:0.4299456326339304
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:18
epoch 0  | loss: 1.57893 | val_0_rmse: 0.9955  | val_1_rmse: 1.03792 |  0:00:00s
epoch 1  | loss: 1.14536 | val_0_rmse: 0.98648 | val_1_rmse: 1.02706 |  0:00:00s
epoch 2  | loss: 0.96893 | val_0_rmse: 0.97342 | val_1_rmse: 1.00932 |  0:00:00s
epoch 3  | loss: 0.92591 | val_0_rmse: 0.95978 | val_1_rmse: 0.98567 |  0:00:01s
epoch 4  | loss: 0.87226 | val_0_rmse: 0.93982 | val_1_rmse: 0.95204 |  0:00:01s
epoch 5  | loss: 0.81578 | val_0_rmse: 0.88818 | val_1_rmse: 0.88576 |  0:00:01s
epoch 6  | loss: 0.80227 | val_0_rmse: 0.8954  | val_1_rmse: 0.89876 |  0:00:02s
epoch 7  | loss: 0.78054 | val_0_rmse: 0.92619 | val_1_rmse: 0.91489 |  0:00:02s
epoch 8  | loss: 0.74208 | val_0_rmse: 0.88545 | val_1_rmse: 0.9054  |  0:00:02s
epoch 9  | loss: 0.71264 | val_0_rmse: 0.90235 | val_1_rmse: 0.92803 |  0:00:03s
epoch 10 | loss: 0.68744 | val_0_rmse: 0.83215 | val_1_rmse: 0.85888 |  0:00:03s
epoch 11 | loss: 0.67293 | val_0_rmse: 0.818   | val_1_rmse: 0.84427 |  0:00:03s
epoch 12 | loss: 0.65903 | val_0_rmse: 0.83093 | val_1_rmse: 0.85168 |  0:00:04s
epoch 13 | loss: 0.66079 | val_0_rmse: 0.81648 | val_1_rmse: 0.84256 |  0:00:04s
epoch 14 | loss: 0.66044 | val_0_rmse: 0.80644 | val_1_rmse: 0.82966 |  0:00:04s
epoch 15 | loss: 0.64241 | val_0_rmse: 0.8019  | val_1_rmse: 0.82197 |  0:00:05s
epoch 16 | loss: 0.64442 | val_0_rmse: 0.7983  | val_1_rmse: 0.81631 |  0:00:05s
epoch 17 | loss: 0.64183 | val_0_rmse: 0.80193 | val_1_rmse: 0.81576 |  0:00:05s
epoch 18 | loss: 0.63819 | val_0_rmse: 0.79567 | val_1_rmse: 0.8129  |  0:00:05s
epoch 19 | loss: 0.64159 | val_0_rmse: 0.79885 | val_1_rmse: 0.81909 |  0:00:06s
epoch 20 | loss: 0.64395 | val_0_rmse: 0.79222 | val_1_rmse: 0.81105 |  0:00:06s
epoch 21 | loss: 0.63808 | val_0_rmse: 0.79368 | val_1_rmse: 0.8161  |  0:00:06s
epoch 22 | loss: 0.63959 | val_0_rmse: 0.80302 | val_1_rmse: 0.81836 |  0:00:07s
epoch 23 | loss: 0.64001 | val_0_rmse: 0.79472 | val_1_rmse: 0.81704 |  0:00:07s
epoch 24 | loss: 0.63166 | val_0_rmse: 0.80396 | val_1_rmse: 0.8181  |  0:00:07s
epoch 25 | loss: 0.63481 | val_0_rmse: 0.79275 | val_1_rmse: 0.81088 |  0:00:08s
epoch 26 | loss: 0.62435 | val_0_rmse: 0.7892  | val_1_rmse: 0.80767 |  0:00:08s
epoch 27 | loss: 0.62329 | val_0_rmse: 0.78938 | val_1_rmse: 0.80623 |  0:00:08s
epoch 28 | loss: 0.61331 | val_0_rmse: 0.78405 | val_1_rmse: 0.79998 |  0:00:08s
epoch 29 | loss: 0.60617 | val_0_rmse: 0.78385 | val_1_rmse: 0.80215 |  0:00:09s
epoch 30 | loss: 0.59982 | val_0_rmse: 0.78186 | val_1_rmse: 0.79664 |  0:00:09s
epoch 31 | loss: 0.59796 | val_0_rmse: 0.77678 | val_1_rmse: 0.79109 |  0:00:09s
epoch 32 | loss: 0.57907 | val_0_rmse: 0.78496 | val_1_rmse: 0.80204 |  0:00:10s
epoch 33 | loss: 0.59315 | val_0_rmse: 0.78774 | val_1_rmse: 0.80781 |  0:00:10s
epoch 34 | loss: 0.58641 | val_0_rmse: 0.78326 | val_1_rmse: 0.80496 |  0:00:10s
epoch 35 | loss: 0.58367 | val_0_rmse: 0.77342 | val_1_rmse: 0.79537 |  0:00:11s
epoch 36 | loss: 0.57798 | val_0_rmse: 0.77395 | val_1_rmse: 0.79208 |  0:00:11s
epoch 37 | loss: 0.58381 | val_0_rmse: 0.77527 | val_1_rmse: 0.7939  |  0:00:11s
epoch 38 | loss: 0.57652 | val_0_rmse: 0.76665 | val_1_rmse: 0.78697 |  0:00:11s
epoch 39 | loss: 0.57398 | val_0_rmse: 0.76675 | val_1_rmse: 0.79068 |  0:00:12s
epoch 40 | loss: 0.58539 | val_0_rmse: 0.7895  | val_1_rmse: 0.81155 |  0:00:12s
epoch 41 | loss: 0.56804 | val_0_rmse: 0.78225 | val_1_rmse: 0.80475 |  0:00:12s
epoch 42 | loss: 0.57037 | val_0_rmse: 0.78526 | val_1_rmse: 0.80665 |  0:00:13s
epoch 43 | loss: 0.55953 | val_0_rmse: 0.77251 | val_1_rmse: 0.78812 |  0:00:13s
epoch 44 | loss: 0.5638  | val_0_rmse: 0.77401 | val_1_rmse: 0.78851 |  0:00:13s
epoch 45 | loss: 0.56467 | val_0_rmse: 0.76936 | val_1_rmse: 0.78014 |  0:00:14s
epoch 46 | loss: 0.56942 | val_0_rmse: 0.76705 | val_1_rmse: 0.78121 |  0:00:14s
epoch 47 | loss: 0.56068 | val_0_rmse: 0.76873 | val_1_rmse: 0.78175 |  0:00:14s
epoch 48 | loss: 0.56366 | val_0_rmse: 0.76793 | val_1_rmse: 0.78448 |  0:00:15s
epoch 49 | loss: 0.55631 | val_0_rmse: 0.76616 | val_1_rmse: 0.78577 |  0:00:15s
epoch 50 | loss: 0.55952 | val_0_rmse: 0.76208 | val_1_rmse: 0.78259 |  0:00:15s
epoch 51 | loss: 0.55528 | val_0_rmse: 0.76306 | val_1_rmse: 0.78032 |  0:00:16s
epoch 52 | loss: 0.55233 | val_0_rmse: 0.75736 | val_1_rmse: 0.77953 |  0:00:16s
epoch 53 | loss: 0.54916 | val_0_rmse: 0.75654 | val_1_rmse: 0.77609 |  0:00:16s
epoch 54 | loss: 0.55024 | val_0_rmse: 0.75471 | val_1_rmse: 0.7826  |  0:00:16s
epoch 55 | loss: 0.54272 | val_0_rmse: 0.75256 | val_1_rmse: 0.78531 |  0:00:17s
epoch 56 | loss: 0.53882 | val_0_rmse: 0.75878 | val_1_rmse: 0.78451 |  0:00:17s
epoch 57 | loss: 0.55532 | val_0_rmse: 0.7546  | val_1_rmse: 0.78532 |  0:00:17s
epoch 58 | loss: 0.54166 | val_0_rmse: 0.75195 | val_1_rmse: 0.78646 |  0:00:18s
epoch 59 | loss: 0.53656 | val_0_rmse: 0.75436 | val_1_rmse: 0.78316 |  0:00:18s
epoch 60 | loss: 0.54333 | val_0_rmse: 0.7547  | val_1_rmse: 0.78349 |  0:00:18s
epoch 61 | loss: 0.54798 | val_0_rmse: 0.75385 | val_1_rmse: 0.78788 |  0:00:19s
epoch 62 | loss: 0.54732 | val_0_rmse: 0.74696 | val_1_rmse: 0.78049 |  0:00:19s
epoch 63 | loss: 0.54123 | val_0_rmse: 0.74251 | val_1_rmse: 0.77745 |  0:00:19s
epoch 64 | loss: 0.55026 | val_0_rmse: 0.74286 | val_1_rmse: 0.77837 |  0:00:19s
epoch 65 | loss: 0.53554 | val_0_rmse: 0.74988 | val_1_rmse: 0.78531 |  0:00:20s
epoch 66 | loss: 0.53267 | val_0_rmse: 0.74705 | val_1_rmse: 0.78122 |  0:00:20s
epoch 67 | loss: 0.54054 | val_0_rmse: 0.74285 | val_1_rmse: 0.77628 |  0:00:20s
epoch 68 | loss: 0.52785 | val_0_rmse: 0.73694 | val_1_rmse: 0.77199 |  0:00:21s
epoch 69 | loss: 0.52986 | val_0_rmse: 0.73376 | val_1_rmse: 0.77468 |  0:00:21s
epoch 70 | loss: 0.52827 | val_0_rmse: 0.72938 | val_1_rmse: 0.77139 |  0:00:21s
epoch 71 | loss: 0.52382 | val_0_rmse: 0.7316  | val_1_rmse: 0.76812 |  0:00:22s
epoch 72 | loss: 0.53181 | val_0_rmse: 0.73554 | val_1_rmse: 0.77205 |  0:00:22s
epoch 73 | loss: 0.54106 | val_0_rmse: 0.73092 | val_1_rmse: 0.77027 |  0:00:22s
epoch 74 | loss: 0.53267 | val_0_rmse: 0.73221 | val_1_rmse: 0.77548 |  0:00:22s
epoch 75 | loss: 0.53855 | val_0_rmse: 0.7244  | val_1_rmse: 0.77263 |  0:00:23s
epoch 76 | loss: 0.52894 | val_0_rmse: 0.73532 | val_1_rmse: 0.77678 |  0:00:23s
epoch 77 | loss: 0.53225 | val_0_rmse: 0.7312  | val_1_rmse: 0.77885 |  0:00:23s
epoch 78 | loss: 0.53208 | val_0_rmse: 0.72516 | val_1_rmse: 0.77814 |  0:00:24s
epoch 79 | loss: 0.52542 | val_0_rmse: 0.72578 | val_1_rmse: 0.78104 |  0:00:24s
epoch 80 | loss: 0.51506 | val_0_rmse: 0.72151 | val_1_rmse: 0.78217 |  0:00:24s
epoch 81 | loss: 0.52051 | val_0_rmse: 0.72037 | val_1_rmse: 0.77799 |  0:00:25s
epoch 82 | loss: 0.5176  | val_0_rmse: 0.71942 | val_1_rmse: 0.77599 |  0:00:25s
epoch 83 | loss: 0.51613 | val_0_rmse: 0.71631 | val_1_rmse: 0.7801  |  0:00:25s
epoch 84 | loss: 0.49951 | val_0_rmse: 0.71392 | val_1_rmse: 0.7675  |  0:00:26s
epoch 85 | loss: 0.5059  | val_0_rmse: 0.71357 | val_1_rmse: 0.7752  |  0:00:26s
epoch 86 | loss: 0.50523 | val_0_rmse: 0.71487 | val_1_rmse: 0.78063 |  0:00:26s
epoch 87 | loss: 0.50521 | val_0_rmse: 0.70652 | val_1_rmse: 0.77432 |  0:00:26s
epoch 88 | loss: 0.49531 | val_0_rmse: 0.70515 | val_1_rmse: 0.7735  |  0:00:27s
epoch 89 | loss: 0.49775 | val_0_rmse: 0.70808 | val_1_rmse: 0.7801  |  0:00:27s
epoch 90 | loss: 0.49496 | val_0_rmse: 0.70578 | val_1_rmse: 0.77386 |  0:00:27s
epoch 91 | loss: 0.4961  | val_0_rmse: 0.70083 | val_1_rmse: 0.77509 |  0:00:28s
epoch 92 | loss: 0.48823 | val_0_rmse: 0.70008 | val_1_rmse: 0.7833  |  0:00:28s
epoch 93 | loss: 0.49265 | val_0_rmse: 0.70234 | val_1_rmse: 0.78576 |  0:00:28s
epoch 94 | loss: 0.49424 | val_0_rmse: 0.6964  | val_1_rmse: 0.77748 |  0:00:29s
epoch 95 | loss: 0.49668 | val_0_rmse: 0.6922  | val_1_rmse: 0.78106 |  0:00:29s
epoch 96 | loss: 0.49056 | val_0_rmse: 0.69495 | val_1_rmse: 0.78234 |  0:00:29s
epoch 97 | loss: 0.48905 | val_0_rmse: 0.7009  | val_1_rmse: 0.78094 |  0:00:29s
epoch 98 | loss: 0.49288 | val_0_rmse: 0.69515 | val_1_rmse: 0.77578 |  0:00:30s
epoch 99 | loss: 0.49955 | val_0_rmse: 0.70522 | val_1_rmse: 0.79179 |  0:00:30s
epoch 100| loss: 0.50335 | val_0_rmse: 0.69292 | val_1_rmse: 0.78651 |  0:00:30s
epoch 101| loss: 0.4874  | val_0_rmse: 0.70542 | val_1_rmse: 0.80531 |  0:00:31s
epoch 102| loss: 0.49717 | val_0_rmse: 0.69814 | val_1_rmse: 0.79134 |  0:00:31s
epoch 103| loss: 0.50109 | val_0_rmse: 0.69819 | val_1_rmse: 0.79643 |  0:00:31s
epoch 104| loss: 0.4982  | val_0_rmse: 0.69773 | val_1_rmse: 0.79299 |  0:00:32s
epoch 105| loss: 0.49038 | val_0_rmse: 0.68943 | val_1_rmse: 0.78433 |  0:00:32s
epoch 106| loss: 0.49471 | val_0_rmse: 0.68527 | val_1_rmse: 0.79288 |  0:00:32s
epoch 107| loss: 0.48119 | val_0_rmse: 0.6895  | val_1_rmse: 0.78352 |  0:00:33s
epoch 108| loss: 0.48418 | val_0_rmse: 0.68871 | val_1_rmse: 0.78852 |  0:00:33s
epoch 109| loss: 0.49177 | val_0_rmse: 0.691   | val_1_rmse: 0.7977  |  0:00:33s
epoch 110| loss: 0.48721 | val_0_rmse: 0.68643 | val_1_rmse: 0.78996 |  0:00:33s
epoch 111| loss: 0.48926 | val_0_rmse: 0.67871 | val_1_rmse: 0.7918  |  0:00:34s
epoch 112| loss: 0.47584 | val_0_rmse: 0.67599 | val_1_rmse: 0.7936  |  0:00:34s
epoch 113| loss: 0.47571 | val_0_rmse: 0.67299 | val_1_rmse: 0.7946  |  0:00:34s
epoch 114| loss: 0.46594 | val_0_rmse: 0.68052 | val_1_rmse: 0.79722 |  0:00:35s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.7675
Best weights from best epoch are automatically used!
ended training at: 05:45:54
Feature importance:
Mean squared error is of 0.037394587868439126
Mean absolute error:0.14277403043266595
MAPE:0.14496111442255113
R2 score:0.38954723705354755
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:45:54
epoch 0  | loss: 1.75307 | val_0_rmse: 0.98305 | val_1_rmse: 1.00447 |  0:00:00s
epoch 1  | loss: 1.17239 | val_0_rmse: 0.95877 | val_1_rmse: 0.98319 |  0:00:00s
epoch 2  | loss: 1.0347  | val_0_rmse: 0.97164 | val_1_rmse: 0.99166 |  0:00:00s
epoch 3  | loss: 0.88761 | val_0_rmse: 0.87053 | val_1_rmse: 0.8832  |  0:00:01s
epoch 4  | loss: 0.74072 | val_0_rmse: 0.84265 | val_1_rmse: 0.84921 |  0:00:01s
epoch 5  | loss: 0.65508 | val_0_rmse: 0.84813 | val_1_rmse: 0.83845 |  0:00:01s
epoch 6  | loss: 0.65671 | val_0_rmse: 0.8049  | val_1_rmse: 0.81151 |  0:00:02s
epoch 7  | loss: 0.64919 | val_0_rmse: 0.81452 | val_1_rmse: 0.81586 |  0:00:02s
epoch 8  | loss: 0.63481 | val_0_rmse: 0.80913 | val_1_rmse: 0.81489 |  0:00:02s
epoch 9  | loss: 0.63532 | val_0_rmse: 0.79717 | val_1_rmse: 0.79984 |  0:00:03s
epoch 10 | loss: 0.63972 | val_0_rmse: 0.79374 | val_1_rmse: 0.79122 |  0:00:03s
epoch 11 | loss: 0.63641 | val_0_rmse: 0.79527 | val_1_rmse: 0.79143 |  0:00:03s
epoch 12 | loss: 0.63453 | val_0_rmse: 0.78876 | val_1_rmse: 0.79052 |  0:00:03s
epoch 13 | loss: 0.64304 | val_0_rmse: 0.7898  | val_1_rmse: 0.79254 |  0:00:04s
epoch 14 | loss: 0.62161 | val_0_rmse: 0.79025 | val_1_rmse: 0.79033 |  0:00:04s
epoch 15 | loss: 0.61757 | val_0_rmse: 0.78599 | val_1_rmse: 0.78681 |  0:00:04s
epoch 16 | loss: 0.6234  | val_0_rmse: 0.78496 | val_1_rmse: 0.78614 |  0:00:05s
epoch 17 | loss: 0.62066 | val_0_rmse: 0.78316 | val_1_rmse: 0.7866  |  0:00:05s
epoch 18 | loss: 0.61164 | val_0_rmse: 0.78173 | val_1_rmse: 0.78697 |  0:00:05s
epoch 19 | loss: 0.6022  | val_0_rmse: 0.78415 | val_1_rmse: 0.78464 |  0:00:06s
epoch 20 | loss: 0.60497 | val_0_rmse: 0.78058 | val_1_rmse: 0.78593 |  0:00:06s
epoch 21 | loss: 0.59323 | val_0_rmse: 0.78378 | val_1_rmse: 0.78931 |  0:00:06s
epoch 22 | loss: 0.59949 | val_0_rmse: 0.78899 | val_1_rmse: 0.78522 |  0:00:06s
epoch 23 | loss: 0.59401 | val_0_rmse: 0.78171 | val_1_rmse: 0.78062 |  0:00:07s
epoch 24 | loss: 0.59138 | val_0_rmse: 0.78298 | val_1_rmse: 0.7791  |  0:00:07s
epoch 25 | loss: 0.58865 | val_0_rmse: 0.77675 | val_1_rmse: 0.77894 |  0:00:07s
epoch 26 | loss: 0.59495 | val_0_rmse: 0.7767  | val_1_rmse: 0.78074 |  0:00:08s
epoch 27 | loss: 0.59167 | val_0_rmse: 0.77386 | val_1_rmse: 0.77511 |  0:00:08s
epoch 28 | loss: 0.58765 | val_0_rmse: 0.7744  | val_1_rmse: 0.77646 |  0:00:08s
epoch 29 | loss: 0.58637 | val_0_rmse: 0.77675 | val_1_rmse: 0.77609 |  0:00:09s
epoch 30 | loss: 0.58449 | val_0_rmse: 0.77436 | val_1_rmse: 0.77911 |  0:00:09s
epoch 31 | loss: 0.57725 | val_0_rmse: 0.76905 | val_1_rmse: 0.77311 |  0:00:09s
epoch 32 | loss: 0.58447 | val_0_rmse: 0.77385 | val_1_rmse: 0.7805  |  0:00:10s
epoch 33 | loss: 0.59046 | val_0_rmse: 0.77336 | val_1_rmse: 0.78081 |  0:00:10s
epoch 34 | loss: 0.58134 | val_0_rmse: 0.77173 | val_1_rmse: 0.77722 |  0:00:10s
epoch 35 | loss: 0.57642 | val_0_rmse: 0.77726 | val_1_rmse: 0.77507 |  0:00:11s
epoch 36 | loss: 0.58273 | val_0_rmse: 0.78187 | val_1_rmse: 0.78861 |  0:00:11s
epoch 37 | loss: 0.58341 | val_0_rmse: 0.78246 | val_1_rmse: 0.78949 |  0:00:11s
epoch 38 | loss: 0.59632 | val_0_rmse: 0.77619 | val_1_rmse: 0.79154 |  0:00:11s
epoch 39 | loss: 0.58237 | val_0_rmse: 0.76947 | val_1_rmse: 0.77545 |  0:00:12s
epoch 40 | loss: 0.58233 | val_0_rmse: 0.76708 | val_1_rmse: 0.77608 |  0:00:12s
epoch 41 | loss: 0.57435 | val_0_rmse: 0.7654  | val_1_rmse: 0.773   |  0:00:12s
epoch 42 | loss: 0.57289 | val_0_rmse: 0.76234 | val_1_rmse: 0.76904 |  0:00:13s
epoch 43 | loss: 0.57129 | val_0_rmse: 0.76529 | val_1_rmse: 0.76679 |  0:00:13s
epoch 44 | loss: 0.57326 | val_0_rmse: 0.76345 | val_1_rmse: 0.77556 |  0:00:13s
epoch 45 | loss: 0.5728  | val_0_rmse: 0.76526 | val_1_rmse: 0.77002 |  0:00:14s
epoch 46 | loss: 0.57818 | val_0_rmse: 0.77537 | val_1_rmse: 0.76732 |  0:00:14s
epoch 47 | loss: 0.56858 | val_0_rmse: 0.77142 | val_1_rmse: 0.76726 |  0:00:14s
epoch 48 | loss: 0.56673 | val_0_rmse: 0.78371 | val_1_rmse: 0.77985 |  0:00:14s
epoch 49 | loss: 0.57875 | val_0_rmse: 0.78488 | val_1_rmse: 0.77495 |  0:00:15s
epoch 50 | loss: 0.58214 | val_0_rmse: 0.77026 | val_1_rmse: 0.7682  |  0:00:15s
epoch 51 | loss: 0.58324 | val_0_rmse: 0.78148 | val_1_rmse: 0.77006 |  0:00:15s
epoch 52 | loss: 0.59654 | val_0_rmse: 0.77457 | val_1_rmse: 0.76859 |  0:00:16s
epoch 53 | loss: 0.59602 | val_0_rmse: 0.77522 | val_1_rmse: 0.77355 |  0:00:16s
epoch 54 | loss: 0.59198 | val_0_rmse: 0.78455 | val_1_rmse: 0.786   |  0:00:16s
epoch 55 | loss: 0.58681 | val_0_rmse: 0.77235 | val_1_rmse: 0.77458 |  0:00:17s
epoch 56 | loss: 0.59003 | val_0_rmse: 0.79607 | val_1_rmse: 0.79087 |  0:00:17s
epoch 57 | loss: 0.57832 | val_0_rmse: 0.76159 | val_1_rmse: 0.76602 |  0:00:17s
epoch 58 | loss: 0.57876 | val_0_rmse: 0.75417 | val_1_rmse: 0.76198 |  0:00:17s
epoch 59 | loss: 0.57997 | val_0_rmse: 0.7583  | val_1_rmse: 0.76757 |  0:00:18s
epoch 60 | loss: 0.56896 | val_0_rmse: 0.75832 | val_1_rmse: 0.77461 |  0:00:18s
epoch 61 | loss: 0.57039 | val_0_rmse: 0.75405 | val_1_rmse: 0.75882 |  0:00:18s
epoch 62 | loss: 0.57175 | val_0_rmse: 0.76561 | val_1_rmse: 0.76965 |  0:00:19s
epoch 63 | loss: 0.58166 | val_0_rmse: 0.76802 | val_1_rmse: 0.76932 |  0:00:19s
epoch 64 | loss: 0.58089 | val_0_rmse: 0.77158 | val_1_rmse: 0.76766 |  0:00:19s
epoch 65 | loss: 0.57832 | val_0_rmse: 0.7566  | val_1_rmse: 0.76119 |  0:00:20s
epoch 66 | loss: 0.56893 | val_0_rmse: 0.75931 | val_1_rmse: 0.76881 |  0:00:20s
epoch 67 | loss: 0.56735 | val_0_rmse: 0.75421 | val_1_rmse: 0.77224 |  0:00:20s
epoch 68 | loss: 0.57111 | val_0_rmse: 0.7551  | val_1_rmse: 0.77097 |  0:00:21s
epoch 69 | loss: 0.56108 | val_0_rmse: 0.75615 | val_1_rmse: 0.76292 |  0:00:21s
epoch 70 | loss: 0.55809 | val_0_rmse: 0.74718 | val_1_rmse: 0.75637 |  0:00:21s
epoch 71 | loss: 0.55178 | val_0_rmse: 0.74948 | val_1_rmse: 0.75055 |  0:00:22s
epoch 72 | loss: 0.55423 | val_0_rmse: 0.74553 | val_1_rmse: 0.75061 |  0:00:22s
epoch 73 | loss: 0.54367 | val_0_rmse: 0.74335 | val_1_rmse: 0.75341 |  0:00:22s
epoch 74 | loss: 0.54376 | val_0_rmse: 0.74151 | val_1_rmse: 0.75415 |  0:00:22s
epoch 75 | loss: 0.54329 | val_0_rmse: 0.74092 | val_1_rmse: 0.75075 |  0:00:23s
epoch 76 | loss: 0.55358 | val_0_rmse: 0.73832 | val_1_rmse: 0.75367 |  0:00:23s
epoch 77 | loss: 0.55337 | val_0_rmse: 0.742   | val_1_rmse: 0.75641 |  0:00:23s
epoch 78 | loss: 0.54806 | val_0_rmse: 0.73548 | val_1_rmse: 0.75395 |  0:00:24s
epoch 79 | loss: 0.55978 | val_0_rmse: 0.77451 | val_1_rmse: 0.80371 |  0:00:24s
epoch 80 | loss: 0.58421 | val_0_rmse: 0.75911 | val_1_rmse: 0.7629  |  0:00:24s
epoch 81 | loss: 0.58424 | val_0_rmse: 0.76024 | val_1_rmse: 0.77418 |  0:00:25s
epoch 82 | loss: 0.57854 | val_0_rmse: 0.75849 | val_1_rmse: 0.76593 |  0:00:25s
epoch 83 | loss: 0.57458 | val_0_rmse: 0.76584 | val_1_rmse: 0.7845  |  0:00:25s
epoch 84 | loss: 0.57574 | val_0_rmse: 0.77514 | val_1_rmse: 0.80011 |  0:00:25s
epoch 85 | loss: 0.58288 | val_0_rmse: 0.76271 | val_1_rmse: 0.78778 |  0:00:26s
epoch 86 | loss: 0.57724 | val_0_rmse: 0.76175 | val_1_rmse: 0.7831  |  0:00:26s
epoch 87 | loss: 0.56675 | val_0_rmse: 0.75967 | val_1_rmse: 0.77799 |  0:00:26s
epoch 88 | loss: 0.57203 | val_0_rmse: 0.75752 | val_1_rmse: 0.77276 |  0:00:27s
epoch 89 | loss: 0.57515 | val_0_rmse: 0.76119 | val_1_rmse: 0.79945 |  0:00:27s
epoch 90 | loss: 0.57988 | val_0_rmse: 0.75319 | val_1_rmse: 0.76144 |  0:00:27s
epoch 91 | loss: 0.57317 | val_0_rmse: 0.75776 | val_1_rmse: 0.77271 |  0:00:28s
epoch 92 | loss: 0.56778 | val_0_rmse: 0.75351 | val_1_rmse: 0.75649 |  0:00:28s
epoch 93 | loss: 0.57154 | val_0_rmse: 0.74835 | val_1_rmse: 0.76577 |  0:00:28s
epoch 94 | loss: 0.56508 | val_0_rmse: 0.75391 | val_1_rmse: 0.75864 |  0:00:28s
epoch 95 | loss: 0.55629 | val_0_rmse: 0.74365 | val_1_rmse: 0.76053 |  0:00:29s
epoch 96 | loss: 0.55602 | val_0_rmse: 0.76701 | val_1_rmse: 0.77535 |  0:00:29s
epoch 97 | loss: 0.57184 | val_0_rmse: 0.75067 | val_1_rmse: 0.76372 |  0:00:29s
epoch 98 | loss: 0.57197 | val_0_rmse: 0.74582 | val_1_rmse: 0.75879 |  0:00:30s
epoch 99 | loss: 0.561   | val_0_rmse: 0.74395 | val_1_rmse: 0.76141 |  0:00:30s
epoch 100| loss: 0.5599  | val_0_rmse: 0.74082 | val_1_rmse: 0.7623  |  0:00:30s
epoch 101| loss: 0.55106 | val_0_rmse: 0.74243 | val_1_rmse: 0.76307 |  0:00:31s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.75055
Best weights from best epoch are automatically used!
ended training at: 05:46:25
Feature importance:
Mean squared error is of 0.03826526826492584
Mean absolute error:0.14661425873785422
MAPE:0.15170535636499854
R2 score:0.390245908612768
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:26
epoch 0  | loss: 1.68761 | val_0_rmse: 0.99733 | val_1_rmse: 0.99606 |  0:00:00s
epoch 1  | loss: 1.0296  | val_0_rmse: 0.92819 | val_1_rmse: 0.92709 |  0:00:00s
epoch 2  | loss: 0.78987 | val_0_rmse: 0.85908 | val_1_rmse: 0.86872 |  0:00:01s
epoch 3  | loss: 0.7373  | val_0_rmse: 0.83238 | val_1_rmse: 0.79864 |  0:00:01s
epoch 4  | loss: 0.67431 | val_0_rmse: 0.84941 | val_1_rmse: 0.83345 |  0:00:01s
epoch 5  | loss: 0.6736  | val_0_rmse: 0.8197  | val_1_rmse: 0.80549 |  0:00:01s
epoch 6  | loss: 0.6686  | val_0_rmse: 0.81863 | val_1_rmse: 0.80577 |  0:00:02s
epoch 7  | loss: 0.66037 | val_0_rmse: 0.81374 | val_1_rmse: 0.80149 |  0:00:02s
epoch 8  | loss: 0.65914 | val_0_rmse: 0.81619 | val_1_rmse: 0.79805 |  0:00:02s
epoch 9  | loss: 0.65904 | val_0_rmse: 0.82019 | val_1_rmse: 0.81138 |  0:00:03s
epoch 10 | loss: 0.65441 | val_0_rmse: 0.81179 | val_1_rmse: 0.80359 |  0:00:03s
epoch 11 | loss: 0.65705 | val_0_rmse: 0.81548 | val_1_rmse: 0.81369 |  0:00:03s
epoch 12 | loss: 0.66028 | val_0_rmse: 0.81289 | val_1_rmse: 0.80846 |  0:00:04s
epoch 13 | loss: 0.65721 | val_0_rmse: 0.81326 | val_1_rmse: 0.80147 |  0:00:04s
epoch 14 | loss: 0.6573  | val_0_rmse: 0.80565 | val_1_rmse: 0.79453 |  0:00:04s
epoch 15 | loss: 0.64828 | val_0_rmse: 0.80738 | val_1_rmse: 0.79604 |  0:00:04s
epoch 16 | loss: 0.65184 | val_0_rmse: 0.81081 | val_1_rmse: 0.79334 |  0:00:05s
epoch 17 | loss: 0.64304 | val_0_rmse: 0.8105  | val_1_rmse: 0.79681 |  0:00:05s
epoch 18 | loss: 0.64613 | val_0_rmse: 0.80742 | val_1_rmse: 0.79597 |  0:00:05s
epoch 19 | loss: 0.64701 | val_0_rmse: 0.80501 | val_1_rmse: 0.79981 |  0:00:06s
epoch 20 | loss: 0.6535  | val_0_rmse: 0.8063  | val_1_rmse: 0.78723 |  0:00:06s
epoch 21 | loss: 0.64896 | val_0_rmse: 0.81123 | val_1_rmse: 0.79415 |  0:00:06s
epoch 22 | loss: 0.64433 | val_0_rmse: 0.80264 | val_1_rmse: 0.78762 |  0:00:07s
epoch 23 | loss: 0.64205 | val_0_rmse: 0.80901 | val_1_rmse: 0.79438 |  0:00:07s
epoch 24 | loss: 0.65143 | val_0_rmse: 0.80708 | val_1_rmse: 0.78481 |  0:00:07s
epoch 25 | loss: 0.65388 | val_0_rmse: 0.81663 | val_1_rmse: 0.80787 |  0:00:08s
epoch 26 | loss: 0.64426 | val_0_rmse: 0.80446 | val_1_rmse: 0.7861  |  0:00:08s
epoch 27 | loss: 0.64444 | val_0_rmse: 0.8101  | val_1_rmse: 0.7886  |  0:00:08s
epoch 28 | loss: 0.63957 | val_0_rmse: 0.801   | val_1_rmse: 0.78339 |  0:00:08s
epoch 29 | loss: 0.64005 | val_0_rmse: 0.80173 | val_1_rmse: 0.79197 |  0:00:09s
epoch 30 | loss: 0.62016 | val_0_rmse: 0.80343 | val_1_rmse: 0.78615 |  0:00:09s
epoch 31 | loss: 0.63492 | val_0_rmse: 0.80155 | val_1_rmse: 0.78779 |  0:00:09s
epoch 32 | loss: 0.62923 | val_0_rmse: 0.80587 | val_1_rmse: 0.79155 |  0:00:10s
epoch 33 | loss: 0.62551 | val_0_rmse: 0.80401 | val_1_rmse: 0.7937  |  0:00:10s
epoch 34 | loss: 0.62907 | val_0_rmse: 0.80582 | val_1_rmse: 0.79763 |  0:00:10s
epoch 35 | loss: 0.62748 | val_0_rmse: 0.81907 | val_1_rmse: 0.816   |  0:00:11s
epoch 36 | loss: 0.62093 | val_0_rmse: 0.81134 | val_1_rmse: 0.79718 |  0:00:11s
epoch 37 | loss: 0.6222  | val_0_rmse: 0.80865 | val_1_rmse: 0.7995  |  0:00:11s
epoch 38 | loss: 0.62062 | val_0_rmse: 0.81708 | val_1_rmse: 0.80907 |  0:00:12s
epoch 39 | loss: 0.613   | val_0_rmse: 0.80914 | val_1_rmse: 0.80235 |  0:00:12s
epoch 40 | loss: 0.61861 | val_0_rmse: 0.81484 | val_1_rmse: 0.80755 |  0:00:12s
epoch 41 | loss: 0.60752 | val_0_rmse: 0.80915 | val_1_rmse: 0.80313 |  0:00:12s
epoch 42 | loss: 0.61924 | val_0_rmse: 0.80253 | val_1_rmse: 0.79574 |  0:00:13s
epoch 43 | loss: 0.612   | val_0_rmse: 0.80436 | val_1_rmse: 0.80128 |  0:00:13s
epoch 44 | loss: 0.61788 | val_0_rmse: 0.79808 | val_1_rmse: 0.79266 |  0:00:13s
epoch 45 | loss: 0.60731 | val_0_rmse: 0.80543 | val_1_rmse: 0.79546 |  0:00:14s
epoch 46 | loss: 0.61131 | val_0_rmse: 0.798   | val_1_rmse: 0.78166 |  0:00:14s
epoch 47 | loss: 0.61874 | val_0_rmse: 0.81015 | val_1_rmse: 0.80543 |  0:00:14s
epoch 48 | loss: 0.62788 | val_0_rmse: 0.81194 | val_1_rmse: 0.79889 |  0:00:15s
epoch 49 | loss: 0.61747 | val_0_rmse: 0.81491 | val_1_rmse: 0.80481 |  0:00:15s
epoch 50 | loss: 0.61271 | val_0_rmse: 0.80948 | val_1_rmse: 0.81449 |  0:00:15s
epoch 51 | loss: 0.62152 | val_0_rmse: 0.79795 | val_1_rmse: 0.79654 |  0:00:15s
epoch 52 | loss: 0.60932 | val_0_rmse: 0.81416 | val_1_rmse: 0.81296 |  0:00:16s
epoch 53 | loss: 0.61172 | val_0_rmse: 0.80423 | val_1_rmse: 0.79958 |  0:00:16s
epoch 54 | loss: 0.60739 | val_0_rmse: 0.80972 | val_1_rmse: 0.81027 |  0:00:16s
epoch 55 | loss: 0.60137 | val_0_rmse: 0.80848 | val_1_rmse: 0.80103 |  0:00:17s
epoch 56 | loss: 0.60929 | val_0_rmse: 0.80972 | val_1_rmse: 0.80152 |  0:00:17s
epoch 57 | loss: 0.61155 | val_0_rmse: 0.80244 | val_1_rmse: 0.79641 |  0:00:17s
epoch 58 | loss: 0.61384 | val_0_rmse: 0.80158 | val_1_rmse: 0.78482 |  0:00:18s
epoch 59 | loss: 0.61796 | val_0_rmse: 0.80753 | val_1_rmse: 0.79691 |  0:00:18s
epoch 60 | loss: 0.60804 | val_0_rmse: 0.79411 | val_1_rmse: 0.78354 |  0:00:18s
epoch 61 | loss: 0.62069 | val_0_rmse: 0.79683 | val_1_rmse: 0.79237 |  0:00:18s
epoch 62 | loss: 0.62452 | val_0_rmse: 0.79442 | val_1_rmse: 0.79166 |  0:00:19s
epoch 63 | loss: 0.60853 | val_0_rmse: 0.79444 | val_1_rmse: 0.78233 |  0:00:19s
epoch 64 | loss: 0.61579 | val_0_rmse: 0.78918 | val_1_rmse: 0.78311 |  0:00:19s
epoch 65 | loss: 0.61144 | val_0_rmse: 0.78576 | val_1_rmse: 0.78047 |  0:00:20s
epoch 66 | loss: 0.61507 | val_0_rmse: 0.78709 | val_1_rmse: 0.77392 |  0:00:20s
epoch 67 | loss: 0.62246 | val_0_rmse: 0.78548 | val_1_rmse: 0.7743  |  0:00:20s
epoch 68 | loss: 0.61872 | val_0_rmse: 0.784   | val_1_rmse: 0.77859 |  0:00:21s
epoch 69 | loss: 0.61073 | val_0_rmse: 0.78482 | val_1_rmse: 0.77299 |  0:00:21s
epoch 70 | loss: 0.6116  | val_0_rmse: 0.78376 | val_1_rmse: 0.77893 |  0:00:21s
epoch 71 | loss: 0.61428 | val_0_rmse: 0.78232 | val_1_rmse: 0.77519 |  0:00:21s
epoch 72 | loss: 0.61238 | val_0_rmse: 0.78043 | val_1_rmse: 0.77233 |  0:00:22s
epoch 73 | loss: 0.61728 | val_0_rmse: 0.78453 | val_1_rmse: 0.77416 |  0:00:22s
epoch 74 | loss: 0.6107  | val_0_rmse: 0.77774 | val_1_rmse: 0.77379 |  0:00:22s
epoch 75 | loss: 0.61044 | val_0_rmse: 0.7788  | val_1_rmse: 0.7693  |  0:00:23s
epoch 76 | loss: 0.61655 | val_0_rmse: 0.77962 | val_1_rmse: 0.77343 |  0:00:23s
epoch 77 | loss: 0.60994 | val_0_rmse: 0.78132 | val_1_rmse: 0.77722 |  0:00:23s
epoch 78 | loss: 0.61288 | val_0_rmse: 0.78389 | val_1_rmse: 0.78215 |  0:00:24s
epoch 79 | loss: 0.61194 | val_0_rmse: 0.77564 | val_1_rmse: 0.77306 |  0:00:24s
epoch 80 | loss: 0.60871 | val_0_rmse: 0.77419 | val_1_rmse: 0.7704  |  0:00:24s
epoch 81 | loss: 0.60571 | val_0_rmse: 0.77479 | val_1_rmse: 0.7673  |  0:00:25s
epoch 82 | loss: 0.60756 | val_0_rmse: 0.77571 | val_1_rmse: 0.77465 |  0:00:25s
epoch 83 | loss: 0.61102 | val_0_rmse: 0.77263 | val_1_rmse: 0.76872 |  0:00:25s
epoch 84 | loss: 0.61065 | val_0_rmse: 0.77427 | val_1_rmse: 0.77323 |  0:00:26s
epoch 85 | loss: 0.60184 | val_0_rmse: 0.77248 | val_1_rmse: 0.76895 |  0:00:26s
epoch 86 | loss: 0.60442 | val_0_rmse: 0.77669 | val_1_rmse: 0.77569 |  0:00:26s
epoch 87 | loss: 0.60445 | val_0_rmse: 0.7713  | val_1_rmse: 0.76879 |  0:00:26s
epoch 88 | loss: 0.60085 | val_0_rmse: 0.77265 | val_1_rmse: 0.76993 |  0:00:27s
epoch 89 | loss: 0.60317 | val_0_rmse: 0.77053 | val_1_rmse: 0.77005 |  0:00:27s
epoch 90 | loss: 0.60394 | val_0_rmse: 0.77003 | val_1_rmse: 0.76402 |  0:00:27s
epoch 91 | loss: 0.60152 | val_0_rmse: 0.78232 | val_1_rmse: 0.77759 |  0:00:28s
epoch 92 | loss: 0.60181 | val_0_rmse: 0.77292 | val_1_rmse: 0.76986 |  0:00:28s
epoch 93 | loss: 0.60222 | val_0_rmse: 0.77258 | val_1_rmse: 0.77015 |  0:00:28s
epoch 94 | loss: 0.60103 | val_0_rmse: 0.77036 | val_1_rmse: 0.76576 |  0:00:29s
epoch 95 | loss: 0.60174 | val_0_rmse: 0.77122 | val_1_rmse: 0.76434 |  0:00:29s
epoch 96 | loss: 0.59952 | val_0_rmse: 0.77552 | val_1_rmse: 0.77317 |  0:00:29s
epoch 97 | loss: 0.6044  | val_0_rmse: 0.77044 | val_1_rmse: 0.77122 |  0:00:29s
epoch 98 | loss: 0.60422 | val_0_rmse: 0.77143 | val_1_rmse: 0.77118 |  0:00:30s
epoch 99 | loss: 0.6003  | val_0_rmse: 0.76953 | val_1_rmse: 0.76776 |  0:00:30s
epoch 100| loss: 0.60071 | val_0_rmse: 0.77081 | val_1_rmse: 0.7708  |  0:00:30s
epoch 101| loss: 0.59823 | val_0_rmse: 0.77031 | val_1_rmse: 0.76586 |  0:00:31s
epoch 102| loss: 0.59684 | val_0_rmse: 0.7684  | val_1_rmse: 0.76744 |  0:00:31s
epoch 103| loss: 0.59733 | val_0_rmse: 0.76938 | val_1_rmse: 0.77085 |  0:00:31s
epoch 104| loss: 0.59828 | val_0_rmse: 0.76754 | val_1_rmse: 0.76914 |  0:00:32s
epoch 105| loss: 0.59506 | val_0_rmse: 0.76828 | val_1_rmse: 0.76841 |  0:00:32s
epoch 106| loss: 0.59842 | val_0_rmse: 0.77383 | val_1_rmse: 0.76958 |  0:00:32s
epoch 107| loss: 0.60248 | val_0_rmse: 0.76982 | val_1_rmse: 0.76553 |  0:00:32s
epoch 108| loss: 0.60266 | val_0_rmse: 0.77061 | val_1_rmse: 0.77082 |  0:00:33s
epoch 109| loss: 0.59921 | val_0_rmse: 0.77082 | val_1_rmse: 0.76898 |  0:00:33s
epoch 110| loss: 0.59892 | val_0_rmse: 0.76943 | val_1_rmse: 0.77192 |  0:00:33s
epoch 111| loss: 0.59948 | val_0_rmse: 0.76957 | val_1_rmse: 0.76759 |  0:00:34s
epoch 112| loss: 0.59469 | val_0_rmse: 0.76824 | val_1_rmse: 0.76717 |  0:00:34s
epoch 113| loss: 0.59719 | val_0_rmse: 0.76805 | val_1_rmse: 0.76661 |  0:00:34s
epoch 114| loss: 0.59462 | val_0_rmse: 0.76885 | val_1_rmse: 0.76612 |  0:00:35s
epoch 115| loss: 0.59913 | val_0_rmse: 0.76801 | val_1_rmse: 0.76943 |  0:00:35s
epoch 116| loss: 0.59748 | val_0_rmse: 0.77049 | val_1_rmse: 0.76852 |  0:00:35s
epoch 117| loss: 0.6035  | val_0_rmse: 0.77036 | val_1_rmse: 0.76728 |  0:00:36s
epoch 118| loss: 0.60378 | val_0_rmse: 0.76999 | val_1_rmse: 0.77108 |  0:00:36s
epoch 119| loss: 0.60083 | val_0_rmse: 0.7708  | val_1_rmse: 0.77355 |  0:00:36s
epoch 120| loss: 0.59978 | val_0_rmse: 0.76915 | val_1_rmse: 0.77053 |  0:00:36s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.76402
Best weights from best epoch are automatically used!
ended training at: 05:47:03
Feature importance:
Mean squared error is of 0.03591822267036368
Mean absolute error:0.14374401315561672
MAPE:0.14524612110563503
R2 score:0.4417423170248934
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:47:03
epoch 0  | loss: 1.90156 | val_0_rmse: 0.99906 | val_1_rmse: 0.97382 |  0:00:00s
epoch 1  | loss: 1.10483 | val_0_rmse: 0.99437 | val_1_rmse: 0.96798 |  0:00:00s
epoch 2  | loss: 0.92751 | val_0_rmse: 0.92879 | val_1_rmse: 0.91268 |  0:00:00s
epoch 3  | loss: 0.88219 | val_0_rmse: 0.95916 | val_1_rmse: 0.94293 |  0:00:01s
epoch 4  | loss: 0.86094 | val_0_rmse: 0.94612 | val_1_rmse: 0.93122 |  0:00:01s
epoch 5  | loss: 0.84933 | val_0_rmse: 0.95448 | val_1_rmse: 0.94355 |  0:00:01s
epoch 6  | loss: 0.84065 | val_0_rmse: 0.96854 | val_1_rmse: 0.95794 |  0:00:02s
epoch 7  | loss: 0.85023 | val_0_rmse: 0.92447 | val_1_rmse: 0.91221 |  0:00:02s
epoch 8  | loss: 0.85605 | val_0_rmse: 0.91575 | val_1_rmse: 0.89293 |  0:00:02s
epoch 9  | loss: 0.85575 | val_0_rmse: 0.90948 | val_1_rmse: 0.88544 |  0:00:03s
epoch 10 | loss: 0.82142 | val_0_rmse: 0.8682  | val_1_rmse: 0.84345 |  0:00:03s
epoch 11 | loss: 0.75926 | val_0_rmse: 0.85392 | val_1_rmse: 0.82144 |  0:00:03s
epoch 12 | loss: 0.71698 | val_0_rmse: 0.83616 | val_1_rmse: 0.80033 |  0:00:03s
epoch 13 | loss: 0.70154 | val_0_rmse: 0.83428 | val_1_rmse: 0.80416 |  0:00:04s
epoch 14 | loss: 0.70664 | val_0_rmse: 0.82663 | val_1_rmse: 0.79814 |  0:00:04s
epoch 15 | loss: 0.69191 | val_0_rmse: 0.82029 | val_1_rmse: 0.80385 |  0:00:04s
epoch 16 | loss: 0.67326 | val_0_rmse: 0.80746 | val_1_rmse: 0.78104 |  0:00:05s
epoch 17 | loss: 0.66938 | val_0_rmse: 0.80801 | val_1_rmse: 0.78599 |  0:00:05s
epoch 18 | loss: 0.65918 | val_0_rmse: 0.79809 | val_1_rmse: 0.77387 |  0:00:05s
epoch 19 | loss: 0.64417 | val_0_rmse: 0.8037  | val_1_rmse: 0.76738 |  0:00:06s
epoch 20 | loss: 0.64196 | val_0_rmse: 0.80252 | val_1_rmse: 0.78489 |  0:00:06s
epoch 21 | loss: 0.62838 | val_0_rmse: 0.79285 | val_1_rmse: 0.77323 |  0:00:06s
epoch 22 | loss: 0.63239 | val_0_rmse: 0.79749 | val_1_rmse: 0.77509 |  0:00:07s
epoch 23 | loss: 0.61306 | val_0_rmse: 0.79543 | val_1_rmse: 0.76836 |  0:00:07s
epoch 24 | loss: 0.60896 | val_0_rmse: 0.79074 | val_1_rmse: 0.76042 |  0:00:07s
epoch 25 | loss: 0.60528 | val_0_rmse: 0.79133 | val_1_rmse: 0.76866 |  0:00:07s
epoch 26 | loss: 0.61032 | val_0_rmse: 0.79171 | val_1_rmse: 0.76574 |  0:00:08s
epoch 27 | loss: 0.59858 | val_0_rmse: 0.79141 | val_1_rmse: 0.7716  |  0:00:08s
epoch 28 | loss: 0.59164 | val_0_rmse: 0.79705 | val_1_rmse: 0.77633 |  0:00:08s
epoch 29 | loss: 0.59473 | val_0_rmse: 0.7908  | val_1_rmse: 0.76829 |  0:00:09s
epoch 30 | loss: 0.59726 | val_0_rmse: 0.78499 | val_1_rmse: 0.76893 |  0:00:09s
epoch 31 | loss: 0.59105 | val_0_rmse: 0.78332 | val_1_rmse: 0.76556 |  0:00:09s
epoch 32 | loss: 0.59153 | val_0_rmse: 0.7866  | val_1_rmse: 0.76776 |  0:00:10s
epoch 33 | loss: 0.59699 | val_0_rmse: 0.78709 | val_1_rmse: 0.77766 |  0:00:10s
epoch 34 | loss: 0.58348 | val_0_rmse: 0.79281 | val_1_rmse: 0.77658 |  0:00:10s
epoch 35 | loss: 0.59122 | val_0_rmse: 0.77927 | val_1_rmse: 0.76224 |  0:00:11s
epoch 36 | loss: 0.58606 | val_0_rmse: 0.79451 | val_1_rmse: 0.77736 |  0:00:11s
epoch 37 | loss: 0.58663 | val_0_rmse: 0.78288 | val_1_rmse: 0.7742  |  0:00:11s
epoch 38 | loss: 0.592   | val_0_rmse: 0.7849  | val_1_rmse: 0.77114 |  0:00:11s
epoch 39 | loss: 0.58562 | val_0_rmse: 0.81174 | val_1_rmse: 0.80716 |  0:00:12s
epoch 40 | loss: 0.57809 | val_0_rmse: 0.8033  | val_1_rmse: 0.78485 |  0:00:12s
epoch 41 | loss: 0.6002  | val_0_rmse: 0.78526 | val_1_rmse: 0.77417 |  0:00:12s
epoch 42 | loss: 0.57716 | val_0_rmse: 0.77794 | val_1_rmse: 0.76502 |  0:00:13s
epoch 43 | loss: 0.58265 | val_0_rmse: 0.77669 | val_1_rmse: 0.76582 |  0:00:13s
epoch 44 | loss: 0.57509 | val_0_rmse: 0.77587 | val_1_rmse: 0.76379 |  0:00:13s
epoch 45 | loss: 0.58128 | val_0_rmse: 0.77505 | val_1_rmse: 0.76249 |  0:00:14s
epoch 46 | loss: 0.57661 | val_0_rmse: 0.7705  | val_1_rmse: 0.76179 |  0:00:14s
epoch 47 | loss: 0.57462 | val_0_rmse: 0.77382 | val_1_rmse: 0.75206 |  0:00:14s
epoch 48 | loss: 0.57222 | val_0_rmse: 0.78607 | val_1_rmse: 0.7835  |  0:00:14s
epoch 49 | loss: 0.5713  | val_0_rmse: 0.77277 | val_1_rmse: 0.749   |  0:00:15s
epoch 50 | loss: 0.56577 | val_0_rmse: 0.77803 | val_1_rmse: 0.77947 |  0:00:15s
epoch 51 | loss: 0.5651  | val_0_rmse: 0.77297 | val_1_rmse: 0.75645 |  0:00:15s
epoch 52 | loss: 0.56031 | val_0_rmse: 0.76573 | val_1_rmse: 0.76016 |  0:00:16s
epoch 53 | loss: 0.565   | val_0_rmse: 0.771   | val_1_rmse: 0.75212 |  0:00:16s
epoch 54 | loss: 0.57272 | val_0_rmse: 0.77235 | val_1_rmse: 0.76243 |  0:00:16s
epoch 55 | loss: 0.5811  | val_0_rmse: 0.77158 | val_1_rmse: 0.75282 |  0:00:17s
epoch 56 | loss: 0.5763  | val_0_rmse: 0.76745 | val_1_rmse: 0.74905 |  0:00:17s
epoch 57 | loss: 0.56143 | val_0_rmse: 0.76223 | val_1_rmse: 0.74989 |  0:00:17s
epoch 58 | loss: 0.56489 | val_0_rmse: 0.75745 | val_1_rmse: 0.74886 |  0:00:18s
epoch 59 | loss: 0.55821 | val_0_rmse: 0.76134 | val_1_rmse: 0.75756 |  0:00:18s
epoch 60 | loss: 0.56736 | val_0_rmse: 0.75322 | val_1_rmse: 0.73843 |  0:00:18s
epoch 61 | loss: 0.5599  | val_0_rmse: 0.75415 | val_1_rmse: 0.74217 |  0:00:18s
epoch 62 | loss: 0.55391 | val_0_rmse: 0.75718 | val_1_rmse: 0.74083 |  0:00:19s
epoch 63 | loss: 0.55188 | val_0_rmse: 0.7509  | val_1_rmse: 0.74675 |  0:00:19s
epoch 64 | loss: 0.55125 | val_0_rmse: 0.75075 | val_1_rmse: 0.74392 |  0:00:19s
epoch 65 | loss: 0.55197 | val_0_rmse: 0.76141 | val_1_rmse: 0.7718  |  0:00:20s
epoch 66 | loss: 0.55203 | val_0_rmse: 0.75611 | val_1_rmse: 0.75921 |  0:00:20s
epoch 67 | loss: 0.55625 | val_0_rmse: 0.75459 | val_1_rmse: 0.76457 |  0:00:20s
epoch 68 | loss: 0.5588  | val_0_rmse: 0.75243 | val_1_rmse: 0.74242 |  0:00:21s
epoch 69 | loss: 0.56006 | val_0_rmse: 0.75689 | val_1_rmse: 0.7485  |  0:00:21s
epoch 70 | loss: 0.55053 | val_0_rmse: 0.7582  | val_1_rmse: 0.76069 |  0:00:21s
epoch 71 | loss: 0.55116 | val_0_rmse: 0.75859 | val_1_rmse: 0.76019 |  0:00:22s
epoch 72 | loss: 0.55094 | val_0_rmse: 0.74491 | val_1_rmse: 0.75004 |  0:00:22s
epoch 73 | loss: 0.54695 | val_0_rmse: 0.74655 | val_1_rmse: 0.75837 |  0:00:22s
epoch 74 | loss: 0.5502  | val_0_rmse: 0.74065 | val_1_rmse: 0.74641 |  0:00:22s
epoch 75 | loss: 0.53835 | val_0_rmse: 0.7372  | val_1_rmse: 0.752   |  0:00:23s
epoch 76 | loss: 0.5393  | val_0_rmse: 0.74303 | val_1_rmse: 0.76418 |  0:00:23s
epoch 77 | loss: 0.53172 | val_0_rmse: 0.7316  | val_1_rmse: 0.74042 |  0:00:23s
epoch 78 | loss: 0.54087 | val_0_rmse: 0.73307 | val_1_rmse: 0.75014 |  0:00:24s
epoch 79 | loss: 0.53361 | val_0_rmse: 0.73365 | val_1_rmse: 0.76231 |  0:00:24s
epoch 80 | loss: 0.51756 | val_0_rmse: 0.72618 | val_1_rmse: 0.74834 |  0:00:24s
epoch 81 | loss: 0.52153 | val_0_rmse: 0.72367 | val_1_rmse: 0.76491 |  0:00:25s
epoch 82 | loss: 0.5217  | val_0_rmse: 0.72044 | val_1_rmse: 0.7411  |  0:00:25s
epoch 83 | loss: 0.51486 | val_0_rmse: 0.71965 | val_1_rmse: 0.74698 |  0:00:25s
epoch 84 | loss: 0.51495 | val_0_rmse: 0.71853 | val_1_rmse: 0.75758 |  0:00:25s
epoch 85 | loss: 0.51817 | val_0_rmse: 0.7146  | val_1_rmse: 0.7548  |  0:00:26s
epoch 86 | loss: 0.51123 | val_0_rmse: 0.71399 | val_1_rmse: 0.74442 |  0:00:26s
epoch 87 | loss: 0.51386 | val_0_rmse: 0.71879 | val_1_rmse: 0.74512 |  0:00:26s
epoch 88 | loss: 0.50536 | val_0_rmse: 0.72383 | val_1_rmse: 0.75862 |  0:00:27s
epoch 89 | loss: 0.51361 | val_0_rmse: 0.71067 | val_1_rmse: 0.74335 |  0:00:27s
epoch 90 | loss: 0.51995 | val_0_rmse: 0.71983 | val_1_rmse: 0.75786 |  0:00:27s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.73843
Best weights from best epoch are automatically used!
ended training at: 05:47:31
Feature importance:
Mean squared error is of 0.04055104194283912
Mean absolute error:0.14781813608689992
MAPE:0.15510677951759946
R2 score:0.38422788851845524
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:47:32
epoch 0  | loss: 2.339   | val_0_rmse: 0.99435 | val_1_rmse: 1.03876 |  0:00:00s
epoch 1  | loss: 1.29104 | val_0_rmse: 0.99191 | val_1_rmse: 1.03869 |  0:00:01s
epoch 2  | loss: 1.09116 | val_0_rmse: 0.99184 | val_1_rmse: 1.03872 |  0:00:01s
epoch 3  | loss: 0.99735 | val_0_rmse: 0.99132 | val_1_rmse: 1.03792 |  0:00:02s
epoch 4  | loss: 0.98987 | val_0_rmse: 0.99072 | val_1_rmse: 1.0374  |  0:00:03s
epoch 5  | loss: 0.99112 | val_0_rmse: 0.99104 | val_1_rmse: 1.03774 |  0:00:03s
epoch 6  | loss: 0.98643 | val_0_rmse: 0.99091 | val_1_rmse: 1.03751 |  0:00:04s
epoch 7  | loss: 0.98552 | val_0_rmse: 0.99092 | val_1_rmse: 1.03757 |  0:00:05s
epoch 8  | loss: 0.98527 | val_0_rmse: 0.99103 | val_1_rmse: 1.03784 |  0:00:05s
epoch 9  | loss: 0.98381 | val_0_rmse: 0.99024 | val_1_rmse: 1.03679 |  0:00:06s
epoch 10 | loss: 0.9844  | val_0_rmse: 0.99037 | val_1_rmse: 1.0369  |  0:00:06s
epoch 11 | loss: 0.98292 | val_0_rmse: 0.99    | val_1_rmse: 1.03669 |  0:00:07s
epoch 12 | loss: 0.98301 | val_0_rmse: 0.98958 | val_1_rmse: 1.03659 |  0:00:08s
epoch 13 | loss: 0.98052 | val_0_rmse: 0.9887  | val_1_rmse: 1.03548 |  0:00:08s
epoch 14 | loss: 0.98246 | val_0_rmse: 0.98972 | val_1_rmse: 1.03746 |  0:00:09s
epoch 15 | loss: 0.98337 | val_0_rmse: 0.98977 | val_1_rmse: 1.03646 |  0:00:09s
epoch 16 | loss: 0.98038 | val_0_rmse: 0.98888 | val_1_rmse: 1.0372  |  0:00:10s
epoch 17 | loss: 0.97864 | val_0_rmse: 0.98811 | val_1_rmse: 1.03699 |  0:00:11s
epoch 18 | loss: 0.98077 | val_0_rmse: 0.98839 | val_1_rmse: 1.03657 |  0:00:11s
epoch 19 | loss: 0.97973 | val_0_rmse: 0.9907  | val_1_rmse: 1.03743 |  0:00:12s
epoch 20 | loss: 0.97904 | val_0_rmse: 0.99124 | val_1_rmse: 1.03827 |  0:00:12s
epoch 21 | loss: 0.97967 | val_0_rmse: 0.99013 | val_1_rmse: 1.03692 |  0:00:13s
epoch 22 | loss: 0.97748 | val_0_rmse: 0.99074 | val_1_rmse: 1.03723 |  0:00:14s
epoch 23 | loss: 0.97739 | val_0_rmse: 0.99066 | val_1_rmse: 1.03703 |  0:00:14s
epoch 24 | loss: 0.97894 | val_0_rmse: 0.99054 | val_1_rmse: 1.03803 |  0:00:15s
epoch 25 | loss: 0.97755 | val_0_rmse: 0.9883  | val_1_rmse: 1.0372  |  0:00:16s
epoch 26 | loss: 0.97742 | val_0_rmse: 0.98665 | val_1_rmse: 1.03923 |  0:00:16s
epoch 27 | loss: 0.97619 | val_0_rmse: 0.98657 | val_1_rmse: 1.03978 |  0:00:17s
epoch 28 | loss: 0.97818 | val_0_rmse: 0.98858 | val_1_rmse: 1.0412  |  0:00:18s
epoch 29 | loss: 0.97619 | val_0_rmse: 0.98896 | val_1_rmse: 1.04064 |  0:00:18s
epoch 30 | loss: 0.97806 | val_0_rmse: 0.98995 | val_1_rmse: 1.03839 |  0:00:19s
epoch 31 | loss: 0.97566 | val_0_rmse: 0.98755 | val_1_rmse: 1.03821 |  0:00:20s
epoch 32 | loss: 0.97631 | val_0_rmse: 0.98838 | val_1_rmse: 1.04198 |  0:00:20s
epoch 33 | loss: 0.98008 | val_0_rmse: 0.98804 | val_1_rmse: 1.03944 |  0:00:21s
epoch 34 | loss: 0.97832 | val_0_rmse: 0.98786 | val_1_rmse: 1.037   |  0:00:21s
epoch 35 | loss: 0.97868 | val_0_rmse: 0.98891 | val_1_rmse: 1.03738 |  0:00:22s
epoch 36 | loss: 0.97875 | val_0_rmse: 0.98947 | val_1_rmse: 1.03791 |  0:00:23s
epoch 37 | loss: 0.977   | val_0_rmse: 0.98705 | val_1_rmse: 1.0357  |  0:00:23s
epoch 38 | loss: 0.9787  | val_0_rmse: 0.98832 | val_1_rmse: 1.0386  |  0:00:24s
epoch 39 | loss: 0.97944 | val_0_rmse: 0.98725 | val_1_rmse: 1.03732 |  0:00:24s
epoch 40 | loss: 0.98171 | val_0_rmse: 0.99051 | val_1_rmse: 1.0384  |  0:00:25s
epoch 41 | loss: 0.97844 | val_0_rmse: 0.98752 | val_1_rmse: 1.03553 |  0:00:26s
epoch 42 | loss: 0.97974 | val_0_rmse: 0.98842 | val_1_rmse: 1.0373  |  0:00:26s
epoch 43 | loss: 0.9779  | val_0_rmse: 0.9871  | val_1_rmse: 1.03706 |  0:00:27s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 1.03548
Best weights from best epoch are automatically used!
ended training at: 05:48:00
Feature importance:
Mean squared error is of 0.09522617015499868
Mean absolute error:0.19667686273639473
MAPE:0.2077163524029635
R2 score:0.00559424836284983
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:48:00
epoch 0  | loss: 2.28592 | val_0_rmse: 1.00222 | val_1_rmse: 1.06927 |  0:00:00s
epoch 1  | loss: 1.19837 | val_0_rmse: 0.99012 | val_1_rmse: 1.05815 |  0:00:01s
epoch 2  | loss: 1.01293 | val_0_rmse: 0.99035 | val_1_rmse: 1.05886 |  0:00:01s
epoch 3  | loss: 0.99673 | val_0_rmse: 0.99044 | val_1_rmse: 1.05858 |  0:00:02s
epoch 4  | loss: 0.98702 | val_0_rmse: 0.99055 | val_1_rmse: 1.05901 |  0:00:03s
epoch 5  | loss: 0.98341 | val_0_rmse: 0.99012 | val_1_rmse: 1.05883 |  0:00:03s
epoch 6  | loss: 0.97834 | val_0_rmse: 0.9879  | val_1_rmse: 1.0577  |  0:00:04s
epoch 7  | loss: 0.97219 | val_0_rmse: 0.95507 | val_1_rmse: 1.01817 |  0:00:05s
epoch 8  | loss: 0.96455 | val_0_rmse: 0.97555 | val_1_rmse: 1.04145 |  0:00:05s
epoch 9  | loss: 0.93726 | val_0_rmse: 0.999   | val_1_rmse: 1.05051 |  0:00:06s
epoch 10 | loss: 0.90671 | val_0_rmse: 0.94847 | val_1_rmse: 1.00397 |  0:00:06s
epoch 11 | loss: 0.88376 | val_0_rmse: 0.92173 | val_1_rmse: 0.98261 |  0:00:07s
epoch 12 | loss: 0.87359 | val_0_rmse: 0.91562 | val_1_rmse: 0.97683 |  0:00:08s
epoch 13 | loss: 0.86684 | val_0_rmse: 0.92002 | val_1_rmse: 0.98298 |  0:00:08s
epoch 14 | loss: 0.86449 | val_0_rmse: 0.93195 | val_1_rmse: 1.00197 |  0:00:09s
epoch 15 | loss: 0.86401 | val_0_rmse: 0.92143 | val_1_rmse: 0.98907 |  0:00:09s
epoch 16 | loss: 0.872   | val_0_rmse: 0.91449 | val_1_rmse: 0.97373 |  0:00:10s
epoch 17 | loss: 0.86729 | val_0_rmse: 0.91246 | val_1_rmse: 0.97446 |  0:00:11s
epoch 18 | loss: 0.86983 | val_0_rmse: 0.92111 | val_1_rmse: 0.97645 |  0:00:11s
epoch 19 | loss: 0.85937 | val_0_rmse: 0.90971 | val_1_rmse: 0.97317 |  0:00:12s
epoch 20 | loss: 0.8502  | val_0_rmse: 0.90493 | val_1_rmse: 0.96018 |  0:00:12s
epoch 21 | loss: 0.83898 | val_0_rmse: 0.90362 | val_1_rmse: 0.96701 |  0:00:13s
epoch 22 | loss: 0.84335 | val_0_rmse: 0.9007  | val_1_rmse: 0.95958 |  0:00:14s
epoch 23 | loss: 0.84399 | val_0_rmse: 0.90601 | val_1_rmse: 0.95317 |  0:00:14s
epoch 24 | loss: 0.84124 | val_0_rmse: 0.90553 | val_1_rmse: 0.96272 |  0:00:15s
epoch 25 | loss: 0.84786 | val_0_rmse: 0.9027  | val_1_rmse: 0.95629 |  0:00:15s
epoch 26 | loss: 0.8469  | val_0_rmse: 0.9104  | val_1_rmse: 0.97511 |  0:00:16s
epoch 27 | loss: 0.84178 | val_0_rmse: 0.91808 | val_1_rmse: 0.95897 |  0:00:17s
epoch 28 | loss: 0.84733 | val_0_rmse: 0.90765 | val_1_rmse: 0.95388 |  0:00:17s
epoch 29 | loss: 0.83458 | val_0_rmse: 0.8985  | val_1_rmse: 0.95105 |  0:00:18s
epoch 30 | loss: 0.83229 | val_0_rmse: 0.89612 | val_1_rmse: 0.94852 |  0:00:19s
epoch 31 | loss: 0.85703 | val_0_rmse: 0.90258 | val_1_rmse: 0.95158 |  0:00:19s
epoch 32 | loss: 0.83873 | val_0_rmse: 0.90449 | val_1_rmse: 0.94919 |  0:00:20s
epoch 33 | loss: 0.81924 | val_0_rmse: 0.90872 | val_1_rmse: 0.95172 |  0:00:20s
epoch 34 | loss: 0.84865 | val_0_rmse: 0.897   | val_1_rmse: 0.95725 |  0:00:21s
epoch 35 | loss: 0.84587 | val_0_rmse: 0.90207 | val_1_rmse: 0.96258 |  0:00:22s
epoch 36 | loss: 0.84208 | val_0_rmse: 0.90584 | val_1_rmse: 0.95423 |  0:00:22s
epoch 37 | loss: 0.82974 | val_0_rmse: 0.92526 | val_1_rmse: 0.9637  |  0:00:23s
epoch 38 | loss: 0.82242 | val_0_rmse: 0.90904 | val_1_rmse: 0.97159 |  0:00:23s
epoch 39 | loss: 0.84366 | val_0_rmse: 0.90322 | val_1_rmse: 0.96556 |  0:00:24s
epoch 40 | loss: 0.83075 | val_0_rmse: 0.90789 | val_1_rmse: 0.9528  |  0:00:25s
epoch 41 | loss: 0.8214  | val_0_rmse: 0.89442 | val_1_rmse: 0.95249 |  0:00:25s
epoch 42 | loss: 0.81921 | val_0_rmse: 0.89377 | val_1_rmse: 0.94984 |  0:00:26s
epoch 43 | loss: 0.82666 | val_0_rmse: 0.89435 | val_1_rmse: 0.94286 |  0:00:27s
epoch 44 | loss: 0.82212 | val_0_rmse: 0.91075 | val_1_rmse: 0.95465 |  0:00:27s
epoch 45 | loss: 0.81713 | val_0_rmse: 0.89558 | val_1_rmse: 0.94664 |  0:00:28s
epoch 46 | loss: 0.81161 | val_0_rmse: 0.90161 | val_1_rmse: 0.94846 |  0:00:28s
epoch 47 | loss: 0.81467 | val_0_rmse: 0.89289 | val_1_rmse: 0.94751 |  0:00:29s
epoch 48 | loss: 0.80847 | val_0_rmse: 0.89209 | val_1_rmse: 0.9518  |  0:00:30s
epoch 49 | loss: 0.82917 | val_0_rmse: 0.89943 | val_1_rmse: 0.96221 |  0:00:30s
epoch 50 | loss: 0.82989 | val_0_rmse: 0.8958  | val_1_rmse: 0.94157 |  0:00:31s
epoch 51 | loss: 0.81474 | val_0_rmse: 0.89263 | val_1_rmse: 0.94575 |  0:00:31s
epoch 52 | loss: 0.82365 | val_0_rmse: 0.89442 | val_1_rmse: 0.95505 |  0:00:32s
epoch 53 | loss: 0.80212 | val_0_rmse: 0.90067 | val_1_rmse: 0.97404 |  0:00:33s
epoch 54 | loss: 0.81952 | val_0_rmse: 0.90878 | val_1_rmse: 0.99261 |  0:00:33s
epoch 55 | loss: 0.83756 | val_0_rmse: 0.91276 | val_1_rmse: 0.9931  |  0:00:34s
epoch 56 | loss: 0.81886 | val_0_rmse: 0.91468 | val_1_rmse: 1.00498 |  0:00:35s
epoch 57 | loss: 0.8171  | val_0_rmse: 0.9204  | val_1_rmse: 1.00708 |  0:00:35s
epoch 58 | loss: 0.80985 | val_0_rmse: 0.92375 | val_1_rmse: 1.01973 |  0:00:36s
epoch 59 | loss: 0.81057 | val_0_rmse: 0.93114 | val_1_rmse: 1.03895 |  0:00:36s
epoch 60 | loss: 0.81322 | val_0_rmse: 0.93304 | val_1_rmse: 1.04534 |  0:00:37s
epoch 61 | loss: 0.81726 | val_0_rmse: 0.92489 | val_1_rmse: 1.02157 |  0:00:38s
epoch 62 | loss: 0.82202 | val_0_rmse: 0.91572 | val_1_rmse: 1.0113  |  0:00:38s
epoch 63 | loss: 0.80819 | val_0_rmse: 0.90909 | val_1_rmse: 0.99014 |  0:00:39s
epoch 64 | loss: 0.81856 | val_0_rmse: 0.89618 | val_1_rmse: 0.97002 |  0:00:39s
epoch 65 | loss: 0.80387 | val_0_rmse: 0.89471 | val_1_rmse: 0.96666 |  0:00:40s
epoch 66 | loss: 0.8065  | val_0_rmse: 0.89536 | val_1_rmse: 0.96964 |  0:00:41s
epoch 67 | loss: 0.80427 | val_0_rmse: 0.89643 | val_1_rmse: 0.97501 |  0:00:41s
epoch 68 | loss: 0.80528 | val_0_rmse: 0.89254 | val_1_rmse: 0.96022 |  0:00:42s
epoch 69 | loss: 0.8057  | val_0_rmse: 0.90277 | val_1_rmse: 0.95618 |  0:00:42s
epoch 70 | loss: 0.80582 | val_0_rmse: 0.88966 | val_1_rmse: 0.95283 |  0:00:43s
epoch 71 | loss: 0.80129 | val_0_rmse: 0.89256 | val_1_rmse: 0.94812 |  0:00:44s
epoch 72 | loss: 0.80657 | val_0_rmse: 0.89206 | val_1_rmse: 0.94553 |  0:00:44s
epoch 73 | loss: 0.79845 | val_0_rmse: 0.89004 | val_1_rmse: 0.94838 |  0:00:45s
epoch 74 | loss: 0.81309 | val_0_rmse: 0.88938 | val_1_rmse: 0.94385 |  0:00:45s
epoch 75 | loss: 0.81087 | val_0_rmse: 0.88883 | val_1_rmse: 0.94121 |  0:00:46s
epoch 76 | loss: 0.79807 | val_0_rmse: 0.88748 | val_1_rmse: 0.94396 |  0:00:47s
epoch 77 | loss: 0.8178  | val_0_rmse: 0.88756 | val_1_rmse: 0.94214 |  0:00:47s
epoch 78 | loss: 0.81182 | val_0_rmse: 0.89086 | val_1_rmse: 0.94097 |  0:00:48s
epoch 79 | loss: 0.81269 | val_0_rmse: 0.88974 | val_1_rmse: 0.94962 |  0:00:49s
epoch 80 | loss: 0.81616 | val_0_rmse: 0.89092 | val_1_rmse: 0.94458 |  0:00:49s
epoch 81 | loss: 0.81078 | val_0_rmse: 0.89014 | val_1_rmse: 0.94441 |  0:00:50s
epoch 82 | loss: 0.79326 | val_0_rmse: 0.88794 | val_1_rmse: 0.94252 |  0:00:50s
epoch 83 | loss: 0.81012 | val_0_rmse: 0.88754 | val_1_rmse: 0.94255 |  0:00:51s
epoch 84 | loss: 0.80049 | val_0_rmse: 0.89527 | val_1_rmse: 0.96458 |  0:00:52s
epoch 85 | loss: 0.81479 | val_0_rmse: 0.88648 | val_1_rmse: 0.94592 |  0:00:52s
epoch 86 | loss: 0.81995 | val_0_rmse: 0.89468 | val_1_rmse: 0.95913 |  0:00:53s
epoch 87 | loss: 0.81313 | val_0_rmse: 0.89024 | val_1_rmse: 0.94148 |  0:00:53s
epoch 88 | loss: 0.80885 | val_0_rmse: 0.88763 | val_1_rmse: 0.94249 |  0:00:54s
epoch 89 | loss: 0.81659 | val_0_rmse: 0.88574 | val_1_rmse: 0.94643 |  0:00:55s
epoch 90 | loss: 0.79958 | val_0_rmse: 0.88625 | val_1_rmse: 0.94369 |  0:00:55s
epoch 91 | loss: 0.80512 | val_0_rmse: 0.88566 | val_1_rmse: 0.94381 |  0:00:56s
epoch 92 | loss: 0.80306 | val_0_rmse: 0.89024 | val_1_rmse: 0.95605 |  0:00:56s
epoch 93 | loss: 0.81776 | val_0_rmse: 0.88858 | val_1_rmse: 0.95827 |  0:00:57s
epoch 94 | loss: 0.82664 | val_0_rmse: 0.9014  | val_1_rmse: 0.95075 |  0:00:58s
epoch 95 | loss: 0.80934 | val_0_rmse: 0.88726 | val_1_rmse: 0.94515 |  0:00:58s
epoch 96 | loss: 0.81464 | val_0_rmse: 0.88728 | val_1_rmse: 0.94491 |  0:00:59s
epoch 97 | loss: 0.80977 | val_0_rmse: 0.88703 | val_1_rmse: 0.9456  |  0:00:59s
epoch 98 | loss: 0.79659 | val_0_rmse: 0.88453 | val_1_rmse: 0.94727 |  0:01:00s
epoch 99 | loss: 0.80511 | val_0_rmse: 0.88435 | val_1_rmse: 0.94626 |  0:01:01s
epoch 100| loss: 0.79945 | val_0_rmse: 0.8848  | val_1_rmse: 0.94961 |  0:01:01s
epoch 101| loss: 0.80417 | val_0_rmse: 0.88343 | val_1_rmse: 0.94682 |  0:01:02s
epoch 102| loss: 0.7826  | val_0_rmse: 0.88224 | val_1_rmse: 0.94511 |  0:01:03s
epoch 103| loss: 0.80309 | val_0_rmse: 0.88266 | val_1_rmse: 0.95241 |  0:01:03s
epoch 104| loss: 0.79425 | val_0_rmse: 0.88229 | val_1_rmse: 0.95476 |  0:01:04s
epoch 105| loss: 0.79967 | val_0_rmse: 0.87951 | val_1_rmse: 0.94397 |  0:01:04s
epoch 106| loss: 0.80273 | val_0_rmse: 0.87907 | val_1_rmse: 0.94874 |  0:01:05s
epoch 107| loss: 0.79841 | val_0_rmse: 0.88177 | val_1_rmse: 0.94141 |  0:01:06s
epoch 108| loss: 0.78838 | val_0_rmse: 0.88167 | val_1_rmse: 0.95424 |  0:01:06s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.94097
Best weights from best epoch are automatically used!
ended training at: 05:49:07
Feature importance:
Mean squared error is of 0.07058896101880266
Mean absolute error:0.18014228462211235
MAPE:0.18299254112126237
R2 score:0.16765291671128169
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:49:08
epoch 0  | loss: 2.62061 | val_0_rmse: 0.99965 | val_1_rmse: 1.01943 |  0:00:00s
epoch 1  | loss: 1.27961 | val_0_rmse: 0.99915 | val_1_rmse: 1.01687 |  0:00:01s
epoch 2  | loss: 1.07352 | val_0_rmse: 1.00182 | val_1_rmse: 1.01925 |  0:00:01s
epoch 3  | loss: 1.02181 | val_0_rmse: 1.00277 | val_1_rmse: 1.02025 |  0:00:02s
epoch 4  | loss: 1.00727 | val_0_rmse: 1.00145 | val_1_rmse: 1.01943 |  0:00:03s
epoch 5  | loss: 1.00767 | val_0_rmse: 1.00222 | val_1_rmse: 1.01986 |  0:00:03s
epoch 6  | loss: 1.00716 | val_0_rmse: 1.00285 | val_1_rmse: 1.02031 |  0:00:04s
epoch 7  | loss: 1.00631 | val_0_rmse: 1.00323 | val_1_rmse: 1.02067 |  0:00:04s
epoch 8  | loss: 1.00547 | val_0_rmse: 1.00202 | val_1_rmse: 1.0198  |  0:00:05s
epoch 9  | loss: 1.0065  | val_0_rmse: 1.00182 | val_1_rmse: 1.01957 |  0:00:06s
epoch 10 | loss: 1.00635 | val_0_rmse: 1.00167 | val_1_rmse: 1.01943 |  0:00:06s
epoch 11 | loss: 1.00268 | val_0_rmse: 0.99721 | val_1_rmse: 1.0153  |  0:00:07s
epoch 12 | loss: 1.00251 | val_0_rmse: 0.98988 | val_1_rmse: 1.0086  |  0:00:08s
epoch 13 | loss: 0.99838 | val_0_rmse: 0.9906  | val_1_rmse: 1.00884 |  0:00:08s
epoch 14 | loss: 0.99195 | val_0_rmse: 0.97603 | val_1_rmse: 0.99472 |  0:00:09s
epoch 15 | loss: 0.98147 | val_0_rmse: 0.96071 | val_1_rmse: 0.97976 |  0:00:09s
epoch 16 | loss: 0.95468 | val_0_rmse: 0.95037 | val_1_rmse: 0.96521 |  0:00:10s
epoch 17 | loss: 0.9258  | val_0_rmse: 0.94268 | val_1_rmse: 0.95917 |  0:00:11s
epoch 18 | loss: 0.90573 | val_0_rmse: 0.95732 | val_1_rmse: 0.97302 |  0:00:11s
epoch 19 | loss: 0.87638 | val_0_rmse: 0.91071 | val_1_rmse: 0.92756 |  0:00:12s
epoch 20 | loss: 0.83222 | val_0_rmse: 0.8873  | val_1_rmse: 0.9023  |  0:00:12s
epoch 21 | loss: 0.79765 | val_0_rmse: 0.90432 | val_1_rmse: 0.92031 |  0:00:13s
epoch 22 | loss: 0.7729  | val_0_rmse: 0.90454 | val_1_rmse: 0.91199 |  0:00:14s
epoch 23 | loss: 0.74795 | val_0_rmse: 0.85654 | val_1_rmse: 0.86555 |  0:00:14s
epoch 24 | loss: 0.71465 | val_0_rmse: 0.84676 | val_1_rmse: 0.8398  |  0:00:15s
epoch 25 | loss: 0.67095 | val_0_rmse: 0.83501 | val_1_rmse: 0.83586 |  0:00:16s
epoch 26 | loss: 0.66711 | val_0_rmse: 0.85742 | val_1_rmse: 0.85442 |  0:00:16s
epoch 27 | loss: 0.66534 | val_0_rmse: 0.85182 | val_1_rmse: 0.85191 |  0:00:17s
epoch 28 | loss: 0.69531 | val_0_rmse: 0.82185 | val_1_rmse: 0.81458 |  0:00:17s
epoch 29 | loss: 0.63942 | val_0_rmse: 0.86437 | val_1_rmse: 0.87228 |  0:00:18s
epoch 30 | loss: 0.60672 | val_0_rmse: 0.81086 | val_1_rmse: 0.78798 |  0:00:19s
epoch 31 | loss: 0.59145 | val_0_rmse: 0.82597 | val_1_rmse: 0.83391 |  0:00:19s
epoch 32 | loss: 0.59174 | val_0_rmse: 0.78979 | val_1_rmse: 0.79528 |  0:00:20s
epoch 33 | loss: 0.56798 | val_0_rmse: 0.80476 | val_1_rmse: 0.80908 |  0:00:21s
epoch 34 | loss: 0.55396 | val_0_rmse: 0.79583 | val_1_rmse: 0.81212 |  0:00:21s
epoch 35 | loss: 0.56478 | val_0_rmse: 0.78232 | val_1_rmse: 0.79402 |  0:00:22s
epoch 36 | loss: 0.53645 | val_0_rmse: 0.78607 | val_1_rmse: 0.7952  |  0:00:22s
epoch 37 | loss: 0.5364  | val_0_rmse: 0.78625 | val_1_rmse: 0.79499 |  0:00:23s
epoch 38 | loss: 0.53192 | val_0_rmse: 0.79045 | val_1_rmse: 0.81775 |  0:00:24s
epoch 39 | loss: 0.52121 | val_0_rmse: 0.77757 | val_1_rmse: 0.81208 |  0:00:24s
epoch 40 | loss: 0.51914 | val_0_rmse: 0.77338 | val_1_rmse: 0.79889 |  0:00:25s
epoch 41 | loss: 0.50888 | val_0_rmse: 0.77984 | val_1_rmse: 0.81519 |  0:00:25s
epoch 42 | loss: 0.52305 | val_0_rmse: 0.78207 | val_1_rmse: 0.82492 |  0:00:26s
epoch 43 | loss: 0.51705 | val_0_rmse: 0.76633 | val_1_rmse: 0.81116 |  0:00:27s
epoch 44 | loss: 0.5091  | val_0_rmse: 0.77215 | val_1_rmse: 0.82419 |  0:00:27s
epoch 45 | loss: 0.49985 | val_0_rmse: 0.75958 | val_1_rmse: 0.80297 |  0:00:28s
epoch 46 | loss: 0.48646 | val_0_rmse: 0.76961 | val_1_rmse: 0.80385 |  0:00:28s
epoch 47 | loss: 0.49112 | val_0_rmse: 0.75933 | val_1_rmse: 0.80457 |  0:00:29s
epoch 48 | loss: 0.47999 | val_0_rmse: 0.75527 | val_1_rmse: 0.80005 |  0:00:30s
epoch 49 | loss: 0.48883 | val_0_rmse: 0.74994 | val_1_rmse: 0.79246 |  0:00:30s
epoch 50 | loss: 0.47852 | val_0_rmse: 0.79257 | val_1_rmse: 0.84729 |  0:00:31s
epoch 51 | loss: 0.48042 | val_0_rmse: 0.7589  | val_1_rmse: 0.81156 |  0:00:32s
epoch 52 | loss: 0.47172 | val_0_rmse: 0.74526 | val_1_rmse: 0.82172 |  0:00:32s
epoch 53 | loss: 0.46744 | val_0_rmse: 0.74388 | val_1_rmse: 0.81491 |  0:00:33s
epoch 54 | loss: 0.45698 | val_0_rmse: 0.73104 | val_1_rmse: 0.79408 |  0:00:33s
epoch 55 | loss: 0.43867 | val_0_rmse: 0.72755 | val_1_rmse: 0.81335 |  0:00:34s
epoch 56 | loss: 0.45305 | val_0_rmse: 0.71744 | val_1_rmse: 0.79308 |  0:00:35s
epoch 57 | loss: 0.44398 | val_0_rmse: 0.7218  | val_1_rmse: 0.80698 |  0:00:35s
epoch 58 | loss: 0.44943 | val_0_rmse: 0.71384 | val_1_rmse: 0.8058  |  0:00:36s
epoch 59 | loss: 0.43851 | val_0_rmse: 0.71032 | val_1_rmse: 0.81411 |  0:00:36s
epoch 60 | loss: 0.42623 | val_0_rmse: 0.72731 | val_1_rmse: 0.8389  |  0:00:37s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.78798
Best weights from best epoch are automatically used!
ended training at: 05:49:46
Feature importance:
Mean squared error is of 0.05904045890419826
Mean absolute error:0.17009141974710404
MAPE:0.1733289288341997
R2 score:0.307462315098201
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:49:46
epoch 0  | loss: 2.25425 | val_0_rmse: 1.02661 | val_1_rmse: 0.99931 |  0:00:00s
epoch 1  | loss: 1.26201 | val_0_rmse: 1.01355 | val_1_rmse: 0.98841 |  0:00:01s
epoch 2  | loss: 1.07136 | val_0_rmse: 1.01339 | val_1_rmse: 0.9889  |  0:00:01s
epoch 3  | loss: 1.03841 | val_0_rmse: 1.01424 | val_1_rmse: 0.98926 |  0:00:02s
epoch 4  | loss: 1.03021 | val_0_rmse: 1.01421 | val_1_rmse: 0.98929 |  0:00:03s
epoch 5  | loss: 1.0338  | val_0_rmse: 1.01455 | val_1_rmse: 0.98922 |  0:00:03s
epoch 6  | loss: 1.03213 | val_0_rmse: 1.01345 | val_1_rmse: 0.98811 |  0:00:04s
epoch 7  | loss: 1.03016 | val_0_rmse: 1.01366 | val_1_rmse: 0.98828 |  0:00:04s
epoch 8  | loss: 1.0313  | val_0_rmse: 1.01302 | val_1_rmse: 0.98842 |  0:00:05s
epoch 9  | loss: 1.02957 | val_0_rmse: 1.01326 | val_1_rmse: 0.98879 |  0:00:06s
epoch 10 | loss: 1.03021 | val_0_rmse: 1.01371 | val_1_rmse: 0.98873 |  0:00:06s
epoch 11 | loss: 1.0273  | val_0_rmse: 1.01347 | val_1_rmse: 0.98835 |  0:00:07s
epoch 12 | loss: 1.02693 | val_0_rmse: 1.0133  | val_1_rmse: 0.9884  |  0:00:07s
epoch 13 | loss: 1.02675 | val_0_rmse: 1.0132  | val_1_rmse: 0.98866 |  0:00:08s
epoch 14 | loss: 1.02601 | val_0_rmse: 1.01391 | val_1_rmse: 0.98945 |  0:00:09s
epoch 15 | loss: 1.02519 | val_0_rmse: 1.01305 | val_1_rmse: 0.98901 |  0:00:09s
epoch 16 | loss: 1.02186 | val_0_rmse: 1.01123 | val_1_rmse: 0.9883  |  0:00:10s
epoch 17 | loss: 1.02166 | val_0_rmse: 1.00786 | val_1_rmse: 0.9869  |  0:00:11s
epoch 18 | loss: 1.01628 | val_0_rmse: 0.99881 | val_1_rmse: 0.98526 |  0:00:11s
epoch 19 | loss: 1.01184 | val_0_rmse: 0.99345 | val_1_rmse: 0.97108 |  0:00:12s
epoch 20 | loss: 1.00974 | val_0_rmse: 0.99634 | val_1_rmse: 0.97574 |  0:00:12s
epoch 21 | loss: 1.00134 | val_0_rmse: 0.99022 | val_1_rmse: 0.96792 |  0:00:13s
epoch 22 | loss: 0.98962 | val_0_rmse: 0.98892 | val_1_rmse: 0.96931 |  0:00:14s
epoch 23 | loss: 0.98182 | val_0_rmse: 0.98519 | val_1_rmse: 0.96103 |  0:00:14s
epoch 24 | loss: 0.97296 | val_0_rmse: 0.98845 | val_1_rmse: 0.96526 |  0:00:15s
epoch 25 | loss: 0.96725 | val_0_rmse: 0.98425 | val_1_rmse: 0.95926 |  0:00:16s
epoch 26 | loss: 0.96422 | val_0_rmse: 0.98325 | val_1_rmse: 0.95998 |  0:00:16s
epoch 27 | loss: 0.95364 | val_0_rmse: 0.97938 | val_1_rmse: 0.95705 |  0:00:17s
epoch 28 | loss: 0.94952 | val_0_rmse: 0.97999 | val_1_rmse: 0.96221 |  0:00:17s
epoch 29 | loss: 0.93103 | val_0_rmse: 0.97426 | val_1_rmse: 0.95614 |  0:00:18s
epoch 30 | loss: 0.91987 | val_0_rmse: 0.97428 | val_1_rmse: 0.96041 |  0:00:19s
epoch 31 | loss: 0.90786 | val_0_rmse: 0.94226 | val_1_rmse: 0.93802 |  0:00:19s
epoch 32 | loss: 0.88277 | val_0_rmse: 0.93767 | val_1_rmse: 0.93185 |  0:00:20s
epoch 33 | loss: 0.84983 | val_0_rmse: 0.91842 | val_1_rmse: 0.91356 |  0:00:20s
epoch 34 | loss: 0.80725 | val_0_rmse: 0.89206 | val_1_rmse: 0.88783 |  0:00:21s
epoch 35 | loss: 0.76515 | val_0_rmse: 0.85674 | val_1_rmse: 0.85532 |  0:00:22s
epoch 36 | loss: 0.73461 | val_0_rmse: 0.85868 | val_1_rmse: 0.84729 |  0:00:22s
epoch 37 | loss: 0.74099 | val_0_rmse: 0.84241 | val_1_rmse: 0.83732 |  0:00:23s
epoch 38 | loss: 0.71549 | val_0_rmse: 0.85546 | val_1_rmse: 0.84669 |  0:00:24s
epoch 39 | loss: 0.68529 | val_0_rmse: 0.82981 | val_1_rmse: 0.83773 |  0:00:24s
epoch 40 | loss: 0.68638 | val_0_rmse: 0.83902 | val_1_rmse: 0.84111 |  0:00:25s
epoch 41 | loss: 0.6906  | val_0_rmse: 0.81844 | val_1_rmse: 0.82052 |  0:00:25s
epoch 42 | loss: 0.63414 | val_0_rmse: 0.81927 | val_1_rmse: 0.83718 |  0:00:26s
epoch 43 | loss: 0.64166 | val_0_rmse: 0.80988 | val_1_rmse: 0.80898 |  0:00:27s
epoch 44 | loss: 0.62818 | val_0_rmse: 0.79963 | val_1_rmse: 0.80607 |  0:00:27s
epoch 45 | loss: 0.59078 | val_0_rmse: 0.80867 | val_1_rmse: 0.81941 |  0:00:28s
epoch 46 | loss: 0.58907 | val_0_rmse: 0.81818 | val_1_rmse: 0.82272 |  0:00:29s
epoch 47 | loss: 0.57551 | val_0_rmse: 0.79483 | val_1_rmse: 0.79414 |  0:00:29s
epoch 48 | loss: 0.55439 | val_0_rmse: 0.82583 | val_1_rmse: 0.81893 |  0:00:30s
epoch 49 | loss: 0.56815 | val_0_rmse: 0.79827 | val_1_rmse: 0.78888 |  0:00:30s
epoch 50 | loss: 0.55212 | val_0_rmse: 0.80617 | val_1_rmse: 0.78685 |  0:00:31s
epoch 51 | loss: 0.54375 | val_0_rmse: 0.79127 | val_1_rmse: 0.7884  |  0:00:32s
epoch 52 | loss: 0.55183 | val_0_rmse: 0.79312 | val_1_rmse: 0.79964 |  0:00:32s
epoch 53 | loss: 0.55042 | val_0_rmse: 0.79063 | val_1_rmse: 0.79599 |  0:00:33s
epoch 54 | loss: 0.53999 | val_0_rmse: 0.8204  | val_1_rmse: 0.82858 |  0:00:33s
epoch 55 | loss: 0.54844 | val_0_rmse: 0.80737 | val_1_rmse: 0.80291 |  0:00:34s
epoch 56 | loss: 0.5276  | val_0_rmse: 0.8083  | val_1_rmse: 0.8153  |  0:00:35s
epoch 57 | loss: 0.54899 | val_0_rmse: 0.80425 | val_1_rmse: 0.80663 |  0:00:35s
epoch 58 | loss: 0.51039 | val_0_rmse: 0.80892 | val_1_rmse: 0.7989  |  0:00:36s
epoch 59 | loss: 0.52603 | val_0_rmse: 0.83401 | val_1_rmse: 0.78885 |  0:00:36s
epoch 60 | loss: 0.51702 | val_0_rmse: 0.82164 | val_1_rmse: 0.79583 |  0:00:37s
epoch 61 | loss: 0.51106 | val_0_rmse: 0.8387  | val_1_rmse: 0.82107 |  0:00:38s
epoch 62 | loss: 0.50709 | val_0_rmse: 0.80894 | val_1_rmse: 0.80535 |  0:00:38s
epoch 63 | loss: 0.50157 | val_0_rmse: 0.78735 | val_1_rmse: 0.80069 |  0:00:39s
epoch 64 | loss: 0.49    | val_0_rmse: 0.74666 | val_1_rmse: 0.7987  |  0:00:40s
epoch 65 | loss: 0.48824 | val_0_rmse: 0.76176 | val_1_rmse: 0.82359 |  0:00:40s
epoch 66 | loss: 0.48726 | val_0_rmse: 0.73312 | val_1_rmse: 0.79045 |  0:00:41s
epoch 67 | loss: 0.47423 | val_0_rmse: 0.73264 | val_1_rmse: 0.80628 |  0:00:42s
epoch 68 | loss: 0.46881 | val_0_rmse: 0.72536 | val_1_rmse: 0.79644 |  0:00:42s
epoch 69 | loss: 0.47729 | val_0_rmse: 0.73246 | val_1_rmse: 0.8042  |  0:00:43s
epoch 70 | loss: 0.46896 | val_0_rmse: 0.71697 | val_1_rmse: 0.79997 |  0:00:43s
epoch 71 | loss: 0.46214 | val_0_rmse: 0.73023 | val_1_rmse: 0.80611 |  0:00:44s
epoch 72 | loss: 0.45687 | val_0_rmse: 0.71268 | val_1_rmse: 0.7928  |  0:00:45s
epoch 73 | loss: 0.46503 | val_0_rmse: 0.71544 | val_1_rmse: 0.80438 |  0:00:45s
epoch 74 | loss: 0.45749 | val_0_rmse: 0.69504 | val_1_rmse: 0.8054  |  0:00:46s
epoch 75 | loss: 0.44228 | val_0_rmse: 0.7092  | val_1_rmse: 0.81897 |  0:00:46s
epoch 76 | loss: 0.44443 | val_0_rmse: 0.69611 | val_1_rmse: 0.8023  |  0:00:47s
epoch 77 | loss: 0.44286 | val_0_rmse: 0.68323 | val_1_rmse: 0.79775 |  0:00:48s
epoch 78 | loss: 0.44127 | val_0_rmse: 0.68394 | val_1_rmse: 0.82191 |  0:00:48s
epoch 79 | loss: 0.44551 | val_0_rmse: 0.68058 | val_1_rmse: 0.80087 |  0:00:49s
epoch 80 | loss: 0.44033 | val_0_rmse: 0.68911 | val_1_rmse: 0.81015 |  0:00:49s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.78685
Best weights from best epoch are automatically used!
ended training at: 05:50:36
Feature importance:
Mean squared error is of 0.0542581907123268
Mean absolute error:0.16621906756579893
MAPE:0.17138010406504559
R2 score:0.28278401832222266
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:50:37
epoch 0  | loss: 2.0569  | val_0_rmse: 1.00764 | val_1_rmse: 0.93755 |  0:00:00s
epoch 1  | loss: 1.24704 | val_0_rmse: 1.00176 | val_1_rmse: 0.93695 |  0:00:01s
epoch 2  | loss: 1.06365 | val_0_rmse: 1.00207 | val_1_rmse: 0.93629 |  0:00:01s
epoch 3  | loss: 1.02172 | val_0_rmse: 1.00078 | val_1_rmse: 0.93593 |  0:00:02s
epoch 4  | loss: 1.01124 | val_0_rmse: 1.00054 | val_1_rmse: 0.93518 |  0:00:03s
epoch 5  | loss: 1.00586 | val_0_rmse: 1.00245 | val_1_rmse: 0.93573 |  0:00:03s
epoch 6  | loss: 1.00604 | val_0_rmse: 1.00102 | val_1_rmse: 0.93521 |  0:00:04s
epoch 7  | loss: 1.00547 | val_0_rmse: 1.00086 | val_1_rmse: 0.93549 |  0:00:05s
epoch 8  | loss: 1.00259 | val_0_rmse: 1.00083 | val_1_rmse: 0.93531 |  0:00:05s
epoch 9  | loss: 1.00243 | val_0_rmse: 1.00097 | val_1_rmse: 0.93534 |  0:00:06s
epoch 10 | loss: 1.00252 | val_0_rmse: 1.00092 | val_1_rmse: 0.93514 |  0:00:06s
epoch 11 | loss: 1.00507 | val_0_rmse: 1.00083 | val_1_rmse: 0.93517 |  0:00:07s
epoch 12 | loss: 1.00329 | val_0_rmse: 1.00084 | val_1_rmse: 0.93543 |  0:00:08s
epoch 13 | loss: 1.00201 | val_0_rmse: 1.00056 | val_1_rmse: 0.93399 |  0:00:08s
epoch 14 | loss: 1.00336 | val_0_rmse: 1.00061 | val_1_rmse: 0.93495 |  0:00:09s
epoch 15 | loss: 1.00487 | val_0_rmse: 1.00022 | val_1_rmse: 0.93353 |  0:00:09s
epoch 16 | loss: 1.00486 | val_0_rmse: 1.0006  | val_1_rmse: 0.9341  |  0:00:10s
epoch 17 | loss: 1.00327 | val_0_rmse: 1.00145 | val_1_rmse: 0.93745 |  0:00:11s
epoch 18 | loss: 1.00446 | val_0_rmse: 1.00105 | val_1_rmse: 0.93591 |  0:00:11s
epoch 19 | loss: 1.0029  | val_0_rmse: 1.00083 | val_1_rmse: 0.93545 |  0:00:12s
epoch 20 | loss: 1.00227 | val_0_rmse: 1.00082 | val_1_rmse: 0.93542 |  0:00:13s
epoch 21 | loss: 1.00189 | val_0_rmse: 1.0009  | val_1_rmse: 0.93594 |  0:00:13s
epoch 22 | loss: 0.99928 | val_0_rmse: 1.0014  | val_1_rmse: 0.93752 |  0:00:14s
epoch 23 | loss: 0.99841 | val_0_rmse: 1.00165 | val_1_rmse: 0.93857 |  0:00:14s
epoch 24 | loss: 1.00026 | val_0_rmse: 1.00198 | val_1_rmse: 0.93869 |  0:00:15s
epoch 25 | loss: 1.00268 | val_0_rmse: 1.00133 | val_1_rmse: 0.93803 |  0:00:16s
epoch 26 | loss: 0.99715 | val_0_rmse: 1.00109 | val_1_rmse: 0.93806 |  0:00:16s
epoch 27 | loss: 0.99595 | val_0_rmse: 1.00008 | val_1_rmse: 0.93706 |  0:00:17s
epoch 28 | loss: 0.99447 | val_0_rmse: 1.0097  | val_1_rmse: 0.95259 |  0:00:17s
epoch 29 | loss: 0.98868 | val_0_rmse: 0.99723 | val_1_rmse: 0.93657 |  0:00:18s
epoch 30 | loss: 0.98702 | val_0_rmse: 0.98982 | val_1_rmse: 0.9327  |  0:00:19s
epoch 31 | loss: 0.97824 | val_0_rmse: 0.98983 | val_1_rmse: 0.93473 |  0:00:19s
epoch 32 | loss: 0.96917 | val_0_rmse: 1.01705 | val_1_rmse: 0.97112 |  0:00:20s
epoch 33 | loss: 0.95819 | val_0_rmse: 0.98011 | val_1_rmse: 0.9298  |  0:00:21s
epoch 34 | loss: 0.9497  | val_0_rmse: 0.9755  | val_1_rmse: 0.92514 |  0:00:21s
epoch 35 | loss: 0.92591 | val_0_rmse: 0.9694  | val_1_rmse: 0.92615 |  0:00:22s
epoch 36 | loss: 0.90166 | val_0_rmse: 0.94869 | val_1_rmse: 0.9031  |  0:00:22s
epoch 37 | loss: 0.88252 | val_0_rmse: 0.91854 | val_1_rmse: 0.88178 |  0:00:23s
epoch 38 | loss: 0.85274 | val_0_rmse: 0.90533 | val_1_rmse: 0.87849 |  0:00:24s
epoch 39 | loss: 0.8057  | val_0_rmse: 1.06726 | val_1_rmse: 1.0674  |  0:00:24s
epoch 40 | loss: 0.77869 | val_0_rmse: 0.85075 | val_1_rmse: 0.83581 |  0:00:25s
epoch 41 | loss: 0.74908 | val_0_rmse: 0.83476 | val_1_rmse: 0.85549 |  0:00:25s
epoch 42 | loss: 0.70393 | val_0_rmse: 0.82102 | val_1_rmse: 0.84046 |  0:00:26s
epoch 43 | loss: 0.69672 | val_0_rmse: 0.95859 | val_1_rmse: 0.95569 |  0:00:27s
epoch 44 | loss: 0.67941 | val_0_rmse: 0.83097 | val_1_rmse: 0.84002 |  0:00:27s
epoch 45 | loss: 0.65864 | val_0_rmse: 0.82137 | val_1_rmse: 0.82782 |  0:00:28s
epoch 46 | loss: 0.62697 | val_0_rmse: 0.80531 | val_1_rmse: 0.81362 |  0:00:29s
epoch 47 | loss: 0.62269 | val_0_rmse: 0.81501 | val_1_rmse: 0.83508 |  0:00:29s
epoch 48 | loss: 0.5992  | val_0_rmse: 0.7923  | val_1_rmse: 0.8114  |  0:00:30s
epoch 49 | loss: 0.614   | val_0_rmse: 0.81004 | val_1_rmse: 0.83507 |  0:00:30s
epoch 50 | loss: 0.60355 | val_0_rmse: 0.78643 | val_1_rmse: 0.80049 |  0:00:31s
epoch 51 | loss: 0.59023 | val_0_rmse: 0.79612 | val_1_rmse: 0.82868 |  0:00:32s
epoch 52 | loss: 0.58367 | val_0_rmse: 0.78842 | val_1_rmse: 0.82051 |  0:00:32s
epoch 53 | loss: 0.57331 | val_0_rmse: 0.76922 | val_1_rmse: 0.7946  |  0:00:33s
epoch 54 | loss: 0.55775 | val_0_rmse: 0.78266 | val_1_rmse: 0.81581 |  0:00:33s
epoch 55 | loss: 0.53958 | val_0_rmse: 0.76031 | val_1_rmse: 0.79524 |  0:00:34s
epoch 56 | loss: 0.54417 | val_0_rmse: 0.75671 | val_1_rmse: 0.80121 |  0:00:35s
epoch 57 | loss: 0.53956 | val_0_rmse: 0.76853 | val_1_rmse: 0.80363 |  0:00:35s
epoch 58 | loss: 0.53153 | val_0_rmse: 0.74684 | val_1_rmse: 0.78934 |  0:00:36s
epoch 59 | loss: 0.52253 | val_0_rmse: 0.74506 | val_1_rmse: 0.78521 |  0:00:37s
epoch 60 | loss: 0.52331 | val_0_rmse: 0.74822 | val_1_rmse: 0.79833 |  0:00:37s
epoch 61 | loss: 0.51377 | val_0_rmse: 0.74931 | val_1_rmse: 0.80551 |  0:00:38s
epoch 62 | loss: 0.5119  | val_0_rmse: 0.74417 | val_1_rmse: 0.79536 |  0:00:38s
epoch 63 | loss: 0.50287 | val_0_rmse: 0.75494 | val_1_rmse: 0.81417 |  0:00:39s
epoch 64 | loss: 0.49446 | val_0_rmse: 0.7307  | val_1_rmse: 0.78277 |  0:00:40s
epoch 65 | loss: 0.49932 | val_0_rmse: 0.72695 | val_1_rmse: 0.79749 |  0:00:40s
epoch 66 | loss: 0.49279 | val_0_rmse: 0.7211  | val_1_rmse: 0.79066 |  0:00:41s
epoch 67 | loss: 0.49288 | val_0_rmse: 0.71791 | val_1_rmse: 0.79278 |  0:00:41s
epoch 68 | loss: 0.48086 | val_0_rmse: 0.71327 | val_1_rmse: 0.79622 |  0:00:42s
epoch 69 | loss: 0.46482 | val_0_rmse: 0.71749 | val_1_rmse: 0.81239 |  0:00:43s
epoch 70 | loss: 0.48431 | val_0_rmse: 0.70983 | val_1_rmse: 0.79797 |  0:00:43s
epoch 71 | loss: 0.47006 | val_0_rmse: 0.70022 | val_1_rmse: 0.79639 |  0:00:44s
epoch 72 | loss: 0.47263 | val_0_rmse: 0.70571 | val_1_rmse: 0.8055  |  0:00:45s
epoch 73 | loss: 0.47623 | val_0_rmse: 0.70141 | val_1_rmse: 0.79824 |  0:00:45s
epoch 74 | loss: 0.46337 | val_0_rmse: 0.71581 | val_1_rmse: 0.84188 |  0:00:46s
epoch 75 | loss: 0.45598 | val_0_rmse: 0.67974 | val_1_rmse: 0.80102 |  0:00:46s
epoch 76 | loss: 0.4638  | val_0_rmse: 0.70078 | val_1_rmse: 0.81122 |  0:00:47s
epoch 77 | loss: 0.46082 | val_0_rmse: 0.69409 | val_1_rmse: 0.8005  |  0:00:48s
epoch 78 | loss: 0.45079 | val_0_rmse: 0.68138 | val_1_rmse: 0.81184 |  0:00:48s
epoch 79 | loss: 0.43871 | val_0_rmse: 0.66475 | val_1_rmse: 0.82352 |  0:00:49s
epoch 80 | loss: 0.44055 | val_0_rmse: 0.66006 | val_1_rmse: 0.80488 |  0:00:50s
epoch 81 | loss: 0.43533 | val_0_rmse: 0.6533  | val_1_rmse: 0.82472 |  0:00:50s
epoch 82 | loss: 0.42753 | val_0_rmse: 0.64612 | val_1_rmse: 0.81177 |  0:00:51s
epoch 83 | loss: 0.42829 | val_0_rmse: 0.64462 | val_1_rmse: 0.82163 |  0:00:51s
epoch 84 | loss: 0.42906 | val_0_rmse: 0.64707 | val_1_rmse: 0.82386 |  0:00:52s
epoch 85 | loss: 0.42547 | val_0_rmse: 0.64252 | val_1_rmse: 0.81789 |  0:00:53s
epoch 86 | loss: 0.42427 | val_0_rmse: 0.64826 | val_1_rmse: 0.8482  |  0:00:53s
epoch 87 | loss: 0.41471 | val_0_rmse: 0.6295  | val_1_rmse: 0.83885 |  0:00:54s
epoch 88 | loss: 0.40257 | val_0_rmse: 0.62795 | val_1_rmse: 0.84176 |  0:00:54s
epoch 89 | loss: 0.40031 | val_0_rmse: 0.61966 | val_1_rmse: 0.82793 |  0:00:55s
epoch 90 | loss: 0.40224 | val_0_rmse: 0.61895 | val_1_rmse: 0.8232  |  0:00:56s
epoch 91 | loss: 0.4138  | val_0_rmse: 0.67451 | val_1_rmse: 0.86959 |  0:00:56s
epoch 92 | loss: 0.41341 | val_0_rmse: 0.66719 | val_1_rmse: 0.87126 |  0:00:57s
epoch 93 | loss: 0.40409 | val_0_rmse: 0.61805 | val_1_rmse: 0.84067 |  0:00:57s
epoch 94 | loss: 0.39341 | val_0_rmse: 0.60663 | val_1_rmse: 0.84767 |  0:00:58s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.78277
Best weights from best epoch are automatically used!
ended training at: 05:51:36
Feature importance:
Mean squared error is of 0.05028507449146398
Mean absolute error:0.1600549620524014
MAPE:0.1611519928472972
R2 score:0.5617120624971659
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:36
epoch 0  | loss: 3.02696 | val_0_rmse: 1.04409 | val_1_rmse: 1.03453 |  0:00:00s
epoch 1  | loss: 2.13863 | val_0_rmse: 1.02955 | val_1_rmse: 1.01088 |  0:00:00s
epoch 2  | loss: 1.6593  | val_0_rmse: 1.01278 | val_1_rmse: 0.98465 |  0:00:00s
epoch 3  | loss: 1.61271 | val_0_rmse: 1.00798 | val_1_rmse: 0.97341 |  0:00:00s
epoch 4  | loss: 1.4169  | val_0_rmse: 1.00843 | val_1_rmse: 0.97108 |  0:00:00s
epoch 5  | loss: 1.22291 | val_0_rmse: 1.00883 | val_1_rmse: 0.97065 |  0:00:00s
epoch 6  | loss: 1.18144 | val_0_rmse: 1.00912 | val_1_rmse: 0.97102 |  0:00:00s
epoch 7  | loss: 1.12291 | val_0_rmse: 1.00915 | val_1_rmse: 0.97132 |  0:00:00s
epoch 8  | loss: 1.17375 | val_0_rmse: 1.00863 | val_1_rmse: 0.97005 |  0:00:00s
epoch 9  | loss: 1.10556 | val_0_rmse: 1.00853 | val_1_rmse: 0.96967 |  0:00:00s
epoch 10 | loss: 1.08356 | val_0_rmse: 1.00811 | val_1_rmse: 0.96946 |  0:00:00s
epoch 11 | loss: 1.12278 | val_0_rmse: 1.00801 | val_1_rmse: 0.96977 |  0:00:01s
epoch 12 | loss: 1.07099 | val_0_rmse: 1.00781 | val_1_rmse: 0.97012 |  0:00:01s
epoch 13 | loss: 1.06498 | val_0_rmse: 1.0077  | val_1_rmse: 0.97032 |  0:00:01s
epoch 14 | loss: 1.02109 | val_0_rmse: 1.00781 | val_1_rmse: 0.97022 |  0:00:01s
epoch 15 | loss: 1.04419 | val_0_rmse: 1.00818 | val_1_rmse: 0.97    |  0:00:01s
epoch 16 | loss: 1.01654 | val_0_rmse: 1.00804 | val_1_rmse: 0.97019 |  0:00:01s
epoch 17 | loss: 1.02565 | val_0_rmse: 1.00781 | val_1_rmse: 0.97058 |  0:00:01s
epoch 18 | loss: 1.0073  | val_0_rmse: 1.00774 | val_1_rmse: 0.97078 |  0:00:01s
epoch 19 | loss: 1.01393 | val_0_rmse: 1.00767 | val_1_rmse: 0.97114 |  0:00:01s
epoch 20 | loss: 1.0004  | val_0_rmse: 1.00767 | val_1_rmse: 0.9711  |  0:00:01s
epoch 21 | loss: 1.01806 | val_0_rmse: 1.00748 | val_1_rmse: 0.97122 |  0:00:01s
epoch 22 | loss: 1.0009  | val_0_rmse: 1.00718 | val_1_rmse: 0.97167 |  0:00:01s
epoch 23 | loss: 0.99897 | val_0_rmse: 1.0071  | val_1_rmse: 0.972   |  0:00:02s
epoch 24 | loss: 1.00501 | val_0_rmse: 1.00679 | val_1_rmse: 0.97125 |  0:00:02s
epoch 25 | loss: 1.01166 | val_0_rmse: 1.00694 | val_1_rmse: 0.97094 |  0:00:02s
epoch 26 | loss: 0.99061 | val_0_rmse: 1.0068  | val_1_rmse: 0.97041 |  0:00:02s
epoch 27 | loss: 1.00056 | val_0_rmse: 1.00617 | val_1_rmse: 0.97065 |  0:00:02s
epoch 28 | loss: 1.00718 | val_0_rmse: 1.0056  | val_1_rmse: 0.97078 |  0:00:02s
epoch 29 | loss: 0.98255 | val_0_rmse: 1.00529 | val_1_rmse: 0.97139 |  0:00:02s
epoch 30 | loss: 1.0007  | val_0_rmse: 1.00493 | val_1_rmse: 0.97166 |  0:00:02s
epoch 31 | loss: 0.9856  | val_0_rmse: 1.00495 | val_1_rmse: 0.97254 |  0:00:02s
epoch 32 | loss: 1.00618 | val_0_rmse: 1.00478 | val_1_rmse: 0.97353 |  0:00:02s
epoch 33 | loss: 1.01458 | val_0_rmse: 1.00507 | val_1_rmse: 0.97301 |  0:00:02s
epoch 34 | loss: 0.99207 | val_0_rmse: 1.00528 | val_1_rmse: 0.9728  |  0:00:02s
epoch 35 | loss: 1.02009 | val_0_rmse: 1.00547 | val_1_rmse: 0.97247 |  0:00:03s
epoch 36 | loss: 1.00835 | val_0_rmse: 1.00573 | val_1_rmse: 0.97201 |  0:00:03s
epoch 37 | loss: 0.99917 | val_0_rmse: 1.006   | val_1_rmse: 0.97142 |  0:00:03s
epoch 38 | loss: 0.99807 | val_0_rmse: 1.00616 | val_1_rmse: 0.97126 |  0:00:03s
epoch 39 | loss: 0.99531 | val_0_rmse: 1.00626 | val_1_rmse: 0.97109 |  0:00:03s
epoch 40 | loss: 0.99804 | val_0_rmse: 1.00619 | val_1_rmse: 0.97114 |  0:00:03s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.96946
Best weights from best epoch are automatically used!
ended training at: 05:51:40
Feature importance:
Mean squared error is of 0.0888207571239988
Mean absolute error:0.20115042240233494
MAPE:0.23459809381744642
R2 score:-0.00018573781357766883
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:40
epoch 0  | loss: 3.74839 | val_0_rmse: 1.02795 | val_1_rmse: 0.86564 |  0:00:00s
epoch 1  | loss: 2.59874 | val_0_rmse: 1.03131 | val_1_rmse: 0.87152 |  0:00:00s
epoch 2  | loss: 1.79751 | val_0_rmse: 1.03937 | val_1_rmse: 0.8809  |  0:00:00s
epoch 3  | loss: 1.64916 | val_0_rmse: 1.04292 | val_1_rmse: 0.88566 |  0:00:00s
epoch 4  | loss: 1.78289 | val_0_rmse: 1.03498 | val_1_rmse: 0.87571 |  0:00:00s
epoch 5  | loss: 1.54018 | val_0_rmse: 1.02943 | val_1_rmse: 0.86711 |  0:00:00s
epoch 6  | loss: 1.2923  | val_0_rmse: 1.02755 | val_1_rmse: 0.86493 |  0:00:00s
epoch 7  | loss: 1.31359 | val_0_rmse: 1.02752 | val_1_rmse: 0.86468 |  0:00:00s
epoch 8  | loss: 1.19295 | val_0_rmse: 1.02886 | val_1_rmse: 0.86691 |  0:00:00s
epoch 9  | loss: 1.15369 | val_0_rmse: 1.02926 | val_1_rmse: 0.86762 |  0:00:01s
epoch 10 | loss: 1.11556 | val_0_rmse: 1.02918 | val_1_rmse: 0.86732 |  0:00:01s
epoch 11 | loss: 1.17789 | val_0_rmse: 1.02941 | val_1_rmse: 0.86722 |  0:00:01s
epoch 12 | loss: 1.10236 | val_0_rmse: 1.02974 | val_1_rmse: 0.86697 |  0:00:01s
epoch 13 | loss: 1.083   | val_0_rmse: 1.02912 | val_1_rmse: 0.86633 |  0:00:01s
epoch 14 | loss: 1.08497 | val_0_rmse: 1.02848 | val_1_rmse: 0.86602 |  0:00:01s
epoch 15 | loss: 1.0634  | val_0_rmse: 1.02849 | val_1_rmse: 0.86699 |  0:00:01s
epoch 16 | loss: 1.0329  | val_0_rmse: 1.02851 | val_1_rmse: 0.86739 |  0:00:01s
epoch 17 | loss: 1.05246 | val_0_rmse: 1.02858 | val_1_rmse: 0.86743 |  0:00:01s
epoch 18 | loss: 1.06139 | val_0_rmse: 1.02833 | val_1_rmse: 0.86729 |  0:00:01s
epoch 19 | loss: 1.07107 | val_0_rmse: 1.02829 | val_1_rmse: 0.86749 |  0:00:01s
epoch 20 | loss: 1.0496  | val_0_rmse: 1.0286  | val_1_rmse: 0.86816 |  0:00:01s
epoch 21 | loss: 1.06717 | val_0_rmse: 1.02892 | val_1_rmse: 0.86832 |  0:00:01s
epoch 22 | loss: 1.05907 | val_0_rmse: 1.0287  | val_1_rmse: 0.8678  |  0:00:02s
epoch 23 | loss: 1.03315 | val_0_rmse: 1.02877 | val_1_rmse: 0.86796 |  0:00:02s
epoch 24 | loss: 1.05466 | val_0_rmse: 1.02823 | val_1_rmse: 0.86774 |  0:00:02s
epoch 25 | loss: 1.05567 | val_0_rmse: 1.02836 | val_1_rmse: 0.8672  |  0:00:02s
epoch 26 | loss: 1.05421 | val_0_rmse: 1.02863 | val_1_rmse: 0.867   |  0:00:02s
epoch 27 | loss: 1.03477 | val_0_rmse: 1.02897 | val_1_rmse: 0.86778 |  0:00:02s
epoch 28 | loss: 1.0392  | val_0_rmse: 1.02946 | val_1_rmse: 0.86877 |  0:00:02s
epoch 29 | loss: 1.04423 | val_0_rmse: 1.02964 | val_1_rmse: 0.86917 |  0:00:02s
epoch 30 | loss: 1.0503  | val_0_rmse: 1.02946 | val_1_rmse: 0.86911 |  0:00:02s
epoch 31 | loss: 1.03655 | val_0_rmse: 1.02902 | val_1_rmse: 0.86886 |  0:00:02s
epoch 32 | loss: 1.03096 | val_0_rmse: 1.02855 | val_1_rmse: 0.86877 |  0:00:02s
epoch 33 | loss: 1.02217 | val_0_rmse: 1.02783 | val_1_rmse: 0.86871 |  0:00:02s
epoch 34 | loss: 1.0257  | val_0_rmse: 1.02711 | val_1_rmse: 0.8689  |  0:00:03s
epoch 35 | loss: 1.03225 | val_0_rmse: 1.02671 | val_1_rmse: 0.86915 |  0:00:03s
epoch 36 | loss: 1.01881 | val_0_rmse: 1.02693 | val_1_rmse: 0.87048 |  0:00:03s
epoch 37 | loss: 1.01996 | val_0_rmse: 1.02822 | val_1_rmse: 0.87279 |  0:00:03s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.86468
Best weights from best epoch are automatically used!
ended training at: 05:51:43
Feature importance:
Mean squared error is of 0.0841326106788626
Mean absolute error:0.21058248431610466
MAPE:0.24692723967410932
R2 score:0.003546792602936333
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:44
epoch 0  | loss: 3.67001 | val_0_rmse: 1.06409 | val_1_rmse: 0.99635 |  0:00:00s
epoch 1  | loss: 2.53996 | val_0_rmse: 1.08504 | val_1_rmse: 1.01589 |  0:00:00s
epoch 2  | loss: 2.4916  | val_0_rmse: 1.04371 | val_1_rmse: 0.9766  |  0:00:00s
epoch 3  | loss: 2.00625 | val_0_rmse: 1.01709 | val_1_rmse: 0.94522 |  0:00:00s
epoch 4  | loss: 1.32944 | val_0_rmse: 1.01615 | val_1_rmse: 0.9422  |  0:00:00s
epoch 5  | loss: 1.42464 | val_0_rmse: 1.01676 | val_1_rmse: 0.9441  |  0:00:00s
epoch 6  | loss: 1.32006 | val_0_rmse: 1.01553 | val_1_rmse: 0.94175 |  0:00:00s
epoch 7  | loss: 1.18401 | val_0_rmse: 1.01487 | val_1_rmse: 0.94234 |  0:00:00s
epoch 8  | loss: 1.20883 | val_0_rmse: 1.01505 | val_1_rmse: 0.9422  |  0:00:00s
epoch 9  | loss: 1.12265 | val_0_rmse: 1.01533 | val_1_rmse: 0.9423  |  0:00:00s
epoch 10 | loss: 1.16006 | val_0_rmse: 1.01593 | val_1_rmse: 0.94179 |  0:00:00s
epoch 11 | loss: 1.13226 | val_0_rmse: 1.01925 | val_1_rmse: 0.94418 |  0:00:01s
epoch 12 | loss: 1.04165 | val_0_rmse: 1.02112 | val_1_rmse: 0.9463  |  0:00:01s
epoch 13 | loss: 1.03681 | val_0_rmse: 1.02105 | val_1_rmse: 0.9458  |  0:00:01s
epoch 14 | loss: 1.07573 | val_0_rmse: 1.02043 | val_1_rmse: 0.94507 |  0:00:01s
epoch 15 | loss: 1.05642 | val_0_rmse: 1.01909 | val_1_rmse: 0.94351 |  0:00:01s
epoch 16 | loss: 1.06264 | val_0_rmse: 1.01747 | val_1_rmse: 0.94274 |  0:00:01s
epoch 17 | loss: 1.02868 | val_0_rmse: 1.01615 | val_1_rmse: 0.94349 |  0:00:01s
epoch 18 | loss: 1.05163 | val_0_rmse: 1.01494 | val_1_rmse: 0.94466 |  0:00:01s
epoch 19 | loss: 1.01513 | val_0_rmse: 1.01487 | val_1_rmse: 0.94634 |  0:00:01s
epoch 20 | loss: 1.01548 | val_0_rmse: 1.01516 | val_1_rmse: 0.94767 |  0:00:01s
epoch 21 | loss: 1.01596 | val_0_rmse: 1.01568 | val_1_rmse: 0.94869 |  0:00:01s
epoch 22 | loss: 1.02309 | val_0_rmse: 1.01501 | val_1_rmse: 0.94751 |  0:00:01s
epoch 23 | loss: 1.02219 | val_0_rmse: 1.01399 | val_1_rmse: 0.94552 |  0:00:02s
epoch 24 | loss: 1.02241 | val_0_rmse: 1.01331 | val_1_rmse: 0.9446  |  0:00:02s
epoch 25 | loss: 1.03365 | val_0_rmse: 1.01256 | val_1_rmse: 0.94431 |  0:00:02s
epoch 26 | loss: 1.01012 | val_0_rmse: 1.01203 | val_1_rmse: 0.94334 |  0:00:02s
epoch 27 | loss: 0.99966 | val_0_rmse: 1.01156 | val_1_rmse: 0.94303 |  0:00:02s
epoch 28 | loss: 1.00583 | val_0_rmse: 1.01323 | val_1_rmse: 0.94614 |  0:00:02s
epoch 29 | loss: 0.99755 | val_0_rmse: 1.01463 | val_1_rmse: 0.95001 |  0:00:02s
epoch 30 | loss: 0.99654 | val_0_rmse: 1.01543 | val_1_rmse: 0.95092 |  0:00:02s
epoch 31 | loss: 1.0121  | val_0_rmse: 1.01541 | val_1_rmse: 0.95062 |  0:00:02s
epoch 32 | loss: 1.01801 | val_0_rmse: 1.01497 | val_1_rmse: 0.9501  |  0:00:02s
epoch 33 | loss: 1.00207 | val_0_rmse: 1.01489 | val_1_rmse: 0.94918 |  0:00:02s
epoch 34 | loss: 0.99093 | val_0_rmse: 1.0147  | val_1_rmse: 0.94916 |  0:00:02s
epoch 35 | loss: 0.96814 | val_0_rmse: 1.01275 | val_1_rmse: 0.94967 |  0:00:03s
epoch 36 | loss: 0.97338 | val_0_rmse: 1.00779 | val_1_rmse: 0.94868 |  0:00:03s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.94175
Best weights from best epoch are automatically used!
ended training at: 05:51:47
Feature importance:
Mean squared error is of 0.08644246360088355
Mean absolute error:0.20624429449591603
MAPE:0.21767028074808556
R2 score:-0.008999863433804212
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:47
epoch 0  | loss: 3.05756 | val_0_rmse: 1.01151 | val_1_rmse: 0.98284 |  0:00:00s
epoch 1  | loss: 1.92099 | val_0_rmse: 1.0114  | val_1_rmse: 0.98385 |  0:00:00s
epoch 2  | loss: 1.7376  | val_0_rmse: 1.00992 | val_1_rmse: 0.98067 |  0:00:00s
epoch 3  | loss: 1.67633 | val_0_rmse: 1.00686 | val_1_rmse: 0.98016 |  0:00:00s
epoch 4  | loss: 1.60306 | val_0_rmse: 1.00758 | val_1_rmse: 0.97899 |  0:00:00s
epoch 5  | loss: 1.37419 | val_0_rmse: 1.00961 | val_1_rmse: 0.97913 |  0:00:00s
epoch 6  | loss: 1.31024 | val_0_rmse: 1.00836 | val_1_rmse: 0.97932 |  0:00:00s
epoch 7  | loss: 1.2138  | val_0_rmse: 1.00688 | val_1_rmse: 0.98083 |  0:00:00s
epoch 8  | loss: 1.23765 | val_0_rmse: 1.00732 | val_1_rmse: 0.98273 |  0:00:00s
epoch 9  | loss: 1.22273 | val_0_rmse: 1.00756 | val_1_rmse: 0.98297 |  0:00:00s
epoch 10 | loss: 1.10943 | val_0_rmse: 1.0072  | val_1_rmse: 0.98234 |  0:00:00s
epoch 11 | loss: 1.10868 | val_0_rmse: 1.00698 | val_1_rmse: 0.98127 |  0:00:01s
epoch 12 | loss: 1.1295  | val_0_rmse: 1.0068  | val_1_rmse: 0.98094 |  0:00:01s
epoch 13 | loss: 1.1047  | val_0_rmse: 1.00666 | val_1_rmse: 0.98016 |  0:00:01s
epoch 14 | loss: 1.06802 | val_0_rmse: 1.00661 | val_1_rmse: 0.97956 |  0:00:01s
epoch 15 | loss: 1.05877 | val_0_rmse: 1.00669 | val_1_rmse: 0.97901 |  0:00:01s
epoch 16 | loss: 1.05023 | val_0_rmse: 1.00665 | val_1_rmse: 0.97915 |  0:00:01s
epoch 17 | loss: 1.05493 | val_0_rmse: 1.00671 | val_1_rmse: 0.97931 |  0:00:01s
epoch 18 | loss: 1.02335 | val_0_rmse: 1.00677 | val_1_rmse: 0.97943 |  0:00:01s
epoch 19 | loss: 1.02338 | val_0_rmse: 1.00681 | val_1_rmse: 0.97951 |  0:00:01s
epoch 20 | loss: 1.01712 | val_0_rmse: 1.00679 | val_1_rmse: 0.97991 |  0:00:01s
epoch 21 | loss: 1.0127  | val_0_rmse: 1.00686 | val_1_rmse: 0.98076 |  0:00:01s
epoch 22 | loss: 1.00745 | val_0_rmse: 1.00688 | val_1_rmse: 0.98079 |  0:00:01s
epoch 23 | loss: 1.00414 | val_0_rmse: 1.00678 | val_1_rmse: 0.98051 |  0:00:02s
epoch 24 | loss: 1.01043 | val_0_rmse: 1.00689 | val_1_rmse: 0.98041 |  0:00:02s
epoch 25 | loss: 1.01174 | val_0_rmse: 1.0071  | val_1_rmse: 0.9808  |  0:00:02s
epoch 26 | loss: 1.0048  | val_0_rmse: 1.00756 | val_1_rmse: 0.98181 |  0:00:02s
epoch 27 | loss: 1.00578 | val_0_rmse: 1.0081  | val_1_rmse: 0.983   |  0:00:02s
epoch 28 | loss: 1.00393 | val_0_rmse: 1.00827 | val_1_rmse: 0.98307 |  0:00:02s
epoch 29 | loss: 0.99417 | val_0_rmse: 1.00815 | val_1_rmse: 0.98253 |  0:00:02s
epoch 30 | loss: 0.99186 | val_0_rmse: 1.00746 | val_1_rmse: 0.98085 |  0:00:02s
epoch 31 | loss: 0.98937 | val_0_rmse: 1.00699 | val_1_rmse: 0.97985 |  0:00:02s
epoch 32 | loss: 0.98367 | val_0_rmse: 1.00659 | val_1_rmse: 0.97943 |  0:00:02s
epoch 33 | loss: 0.95852 | val_0_rmse: 1.00612 | val_1_rmse: 0.97935 |  0:00:02s
epoch 34 | loss: 0.96127 | val_0_rmse: 1.00574 | val_1_rmse: 0.97954 |  0:00:02s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.97899
Best weights from best epoch are automatically used!
ended training at: 05:51:50
Feature importance:
Mean squared error is of 0.08444618810476431
Mean absolute error:0.20808160792675232
MAPE:0.2417583661704636
R2 score:-0.00016251073755246814
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:51
epoch 0  | loss: 3.62304 | val_0_rmse: 1.03198 | val_1_rmse: 0.99114 |  0:00:00s
epoch 1  | loss: 2.41095 | val_0_rmse: 1.03439 | val_1_rmse: 0.98518 |  0:00:00s
epoch 2  | loss: 2.15491 | val_0_rmse: 1.02792 | val_1_rmse: 0.97507 |  0:00:00s
epoch 3  | loss: 1.62806 | val_0_rmse: 1.02763 | val_1_rmse: 0.97269 |  0:00:00s
epoch 4  | loss: 1.36514 | val_0_rmse: 1.0272  | val_1_rmse: 0.97137 |  0:00:00s
epoch 5  | loss: 1.31672 | val_0_rmse: 1.02616 | val_1_rmse: 0.97129 |  0:00:00s
epoch 6  | loss: 1.25052 | val_0_rmse: 1.02723 | val_1_rmse: 0.97419 |  0:00:00s
epoch 7  | loss: 1.33244 | val_0_rmse: 1.02695 | val_1_rmse: 0.97207 |  0:00:00s
epoch 8  | loss: 1.26626 | val_0_rmse: 1.02819 | val_1_rmse: 0.972   |  0:00:00s
epoch 9  | loss: 1.17505 | val_0_rmse: 1.02835 | val_1_rmse: 0.97236 |  0:00:00s
epoch 10 | loss: 1.15464 | val_0_rmse: 1.02836 | val_1_rmse: 0.97266 |  0:00:00s
epoch 11 | loss: 1.14899 | val_0_rmse: 1.02779 | val_1_rmse: 0.97357 |  0:00:01s
epoch 12 | loss: 1.15025 | val_0_rmse: 1.02814 | val_1_rmse: 0.9741  |  0:00:01s
epoch 13 | loss: 1.11873 | val_0_rmse: 1.02822 | val_1_rmse: 0.97378 |  0:00:01s
epoch 14 | loss: 1.07622 | val_0_rmse: 1.02791 | val_1_rmse: 0.97389 |  0:00:01s
epoch 15 | loss: 1.05139 | val_0_rmse: 1.02799 | val_1_rmse: 0.97431 |  0:00:01s
epoch 16 | loss: 1.07454 | val_0_rmse: 1.02811 | val_1_rmse: 0.97406 |  0:00:01s
epoch 17 | loss: 1.07422 | val_0_rmse: 1.02797 | val_1_rmse: 0.97378 |  0:00:01s
epoch 18 | loss: 1.05239 | val_0_rmse: 1.02766 | val_1_rmse: 0.97335 |  0:00:01s
epoch 19 | loss: 1.04989 | val_0_rmse: 1.02758 | val_1_rmse: 0.97317 |  0:00:01s
epoch 20 | loss: 1.05827 | val_0_rmse: 1.02746 | val_1_rmse: 0.97282 |  0:00:01s
epoch 21 | loss: 1.09929 | val_0_rmse: 1.02739 | val_1_rmse: 0.9729  |  0:00:01s
epoch 22 | loss: 1.06972 | val_0_rmse: 1.02709 | val_1_rmse: 0.97336 |  0:00:01s
epoch 23 | loss: 1.05651 | val_0_rmse: 1.02677 | val_1_rmse: 0.97309 |  0:00:02s
epoch 24 | loss: 1.05245 | val_0_rmse: 1.02658 | val_1_rmse: 0.97274 |  0:00:02s
epoch 25 | loss: 1.06299 | val_0_rmse: 1.02637 | val_1_rmse: 0.97259 |  0:00:02s
epoch 26 | loss: 1.0618  | val_0_rmse: 1.02599 | val_1_rmse: 0.97332 |  0:00:02s
epoch 27 | loss: 1.05054 | val_0_rmse: 1.02568 | val_1_rmse: 0.97388 |  0:00:02s
epoch 28 | loss: 1.04945 | val_0_rmse: 1.02437 | val_1_rmse: 0.97492 |  0:00:02s
epoch 29 | loss: 1.04921 | val_0_rmse: 1.02273 | val_1_rmse: 0.97764 |  0:00:02s
epoch 30 | loss: 1.04891 | val_0_rmse: 1.02152 | val_1_rmse: 0.97956 |  0:00:02s
epoch 31 | loss: 1.0385  | val_0_rmse: 1.01985 | val_1_rmse: 0.98154 |  0:00:02s
epoch 32 | loss: 1.04118 | val_0_rmse: 1.01842 | val_1_rmse: 0.9809  |  0:00:02s
epoch 33 | loss: 1.03197 | val_0_rmse: 1.01816 | val_1_rmse: 0.9798  |  0:00:02s
epoch 34 | loss: 1.02459 | val_0_rmse: 1.01827 | val_1_rmse: 0.9825  |  0:00:02s
epoch 35 | loss: 1.01038 | val_0_rmse: 1.01825 | val_1_rmse: 0.99361 |  0:00:03s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 0.97129
Best weights from best epoch are automatically used!
ended training at: 05:51:54
Feature importance:
Mean squared error is of 0.08339802823965178
Mean absolute error:0.1832547824272568
MAPE:0.1988281399885453
R2 score:-0.004526391099807681
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:51:55
epoch 0  | loss: 2.04828 | val_0_rmse: 0.99496 | val_1_rmse: 1.04683 |  0:00:00s
epoch 1  | loss: 1.17238 | val_0_rmse: 0.97646 | val_1_rmse: 1.01116 |  0:00:00s
epoch 2  | loss: 1.04004 | val_0_rmse: 0.9747  | val_1_rmse: 1.01052 |  0:00:00s
epoch 3  | loss: 0.97244 | val_0_rmse: 0.97393 | val_1_rmse: 1.00898 |  0:00:01s
epoch 4  | loss: 0.95236 | val_0_rmse: 0.97985 | val_1_rmse: 1.01463 |  0:00:01s
epoch 5  | loss: 0.94826 | val_0_rmse: 0.98255 | val_1_rmse: 1.01623 |  0:00:01s
epoch 6  | loss: 0.94357 | val_0_rmse: 0.9754  | val_1_rmse: 1.01026 |  0:00:02s
epoch 7  | loss: 0.94533 | val_0_rmse: 0.9674  | val_1_rmse: 1.00046 |  0:00:02s
epoch 8  | loss: 0.94946 | val_0_rmse: 0.96669 | val_1_rmse: 1.00096 |  0:00:02s
epoch 9  | loss: 0.93855 | val_0_rmse: 0.96445 | val_1_rmse: 0.99869 |  0:00:03s
epoch 10 | loss: 0.93032 | val_0_rmse: 0.93717 | val_1_rmse: 0.96824 |  0:00:03s
epoch 11 | loss: 0.92061 | val_0_rmse: 0.9032  | val_1_rmse: 0.94177 |  0:00:03s
epoch 12 | loss: 0.88727 | val_0_rmse: 0.8926  | val_1_rmse: 0.93216 |  0:00:03s
epoch 13 | loss: 0.85344 | val_0_rmse: 0.8729  | val_1_rmse: 0.89879 |  0:00:04s
epoch 14 | loss: 0.82954 | val_0_rmse: 0.87181 | val_1_rmse: 0.89204 |  0:00:04s
epoch 15 | loss: 0.82839 | val_0_rmse: 0.87482 | val_1_rmse: 0.89367 |  0:00:04s
epoch 16 | loss: 0.79228 | val_0_rmse: 0.88391 | val_1_rmse: 0.90411 |  0:00:05s
epoch 17 | loss: 0.77941 | val_0_rmse: 0.8743  | val_1_rmse: 0.88984 |  0:00:05s
epoch 18 | loss: 0.75031 | val_0_rmse: 0.87662 | val_1_rmse: 0.89066 |  0:00:05s
epoch 19 | loss: 0.72338 | val_0_rmse: 0.85932 | val_1_rmse: 0.87268 |  0:00:05s
epoch 20 | loss: 0.69834 | val_0_rmse: 0.88828 | val_1_rmse: 0.89854 |  0:00:06s
epoch 21 | loss: 0.68216 | val_0_rmse: 0.87098 | val_1_rmse: 0.88492 |  0:00:06s
epoch 22 | loss: 0.67339 | val_0_rmse: 0.85468 | val_1_rmse: 0.85478 |  0:00:06s
epoch 23 | loss: 0.66758 | val_0_rmse: 0.82579 | val_1_rmse: 0.82717 |  0:00:07s
epoch 24 | loss: 0.6627  | val_0_rmse: 0.82445 | val_1_rmse: 0.83066 |  0:00:07s
epoch 25 | loss: 0.64485 | val_0_rmse: 0.82466 | val_1_rmse: 0.83825 |  0:00:07s
epoch 26 | loss: 0.65058 | val_0_rmse: 0.81247 | val_1_rmse: 0.82657 |  0:00:07s
epoch 27 | loss: 0.64194 | val_0_rmse: 0.80343 | val_1_rmse: 0.81896 |  0:00:08s
epoch 28 | loss: 0.62483 | val_0_rmse: 0.82038 | val_1_rmse: 0.83623 |  0:00:08s
epoch 29 | loss: 0.61455 | val_0_rmse: 0.80878 | val_1_rmse: 0.82419 |  0:00:08s
epoch 30 | loss: 0.61057 | val_0_rmse: 0.83184 | val_1_rmse: 0.84087 |  0:00:09s
epoch 31 | loss: 0.60807 | val_0_rmse: 0.79671 | val_1_rmse: 0.80736 |  0:00:09s
epoch 32 | loss: 0.59266 | val_0_rmse: 0.79732 | val_1_rmse: 0.80169 |  0:00:09s
epoch 33 | loss: 0.58609 | val_0_rmse: 0.79405 | val_1_rmse: 0.79237 |  0:00:09s
epoch 34 | loss: 0.57152 | val_0_rmse: 0.78894 | val_1_rmse: 0.78825 |  0:00:10s
epoch 35 | loss: 0.56442 | val_0_rmse: 0.79029 | val_1_rmse: 0.78677 |  0:00:10s
epoch 36 | loss: 0.56    | val_0_rmse: 0.79658 | val_1_rmse: 0.79373 |  0:00:10s
epoch 37 | loss: 0.55715 | val_0_rmse: 0.78722 | val_1_rmse: 0.79846 |  0:00:11s
epoch 38 | loss: 0.54689 | val_0_rmse: 0.78026 | val_1_rmse: 0.79132 |  0:00:11s
epoch 39 | loss: 0.54951 | val_0_rmse: 0.78131 | val_1_rmse: 0.80002 |  0:00:11s
epoch 40 | loss: 0.54702 | val_0_rmse: 0.77756 | val_1_rmse: 0.79392 |  0:00:11s
epoch 41 | loss: 0.54087 | val_0_rmse: 0.77258 | val_1_rmse: 0.77641 |  0:00:12s
epoch 42 | loss: 0.54095 | val_0_rmse: 0.78106 | val_1_rmse: 0.77988 |  0:00:12s
epoch 43 | loss: 0.53509 | val_0_rmse: 0.76708 | val_1_rmse: 0.77809 |  0:00:12s
epoch 44 | loss: 0.53215 | val_0_rmse: 0.7781  | val_1_rmse: 0.78233 |  0:00:13s
epoch 45 | loss: 0.53204 | val_0_rmse: 0.77258 | val_1_rmse: 0.77938 |  0:00:13s
epoch 46 | loss: 0.53251 | val_0_rmse: 0.76842 | val_1_rmse: 0.78381 |  0:00:13s
epoch 47 | loss: 0.52801 | val_0_rmse: 0.75932 | val_1_rmse: 0.77545 |  0:00:13s
epoch 48 | loss: 0.51597 | val_0_rmse: 0.76454 | val_1_rmse: 0.7852  |  0:00:14s
epoch 49 | loss: 0.51668 | val_0_rmse: 0.76118 | val_1_rmse: 0.77621 |  0:00:14s
epoch 50 | loss: 0.51293 | val_0_rmse: 0.76777 | val_1_rmse: 0.77891 |  0:00:14s
epoch 51 | loss: 0.50743 | val_0_rmse: 0.75455 | val_1_rmse: 0.76965 |  0:00:15s
epoch 52 | loss: 0.51446 | val_0_rmse: 0.75613 | val_1_rmse: 0.76961 |  0:00:15s
epoch 53 | loss: 0.511   | val_0_rmse: 0.7579  | val_1_rmse: 0.77461 |  0:00:15s
epoch 54 | loss: 0.50206 | val_0_rmse: 0.75268 | val_1_rmse: 0.77801 |  0:00:15s
epoch 55 | loss: 0.498   | val_0_rmse: 0.75809 | val_1_rmse: 0.77686 |  0:00:16s
epoch 56 | loss: 0.50629 | val_0_rmse: 0.75145 | val_1_rmse: 0.7719  |  0:00:16s
epoch 57 | loss: 0.50697 | val_0_rmse: 0.7554  | val_1_rmse: 0.77551 |  0:00:16s
epoch 58 | loss: 0.50183 | val_0_rmse: 0.75309 | val_1_rmse: 0.77982 |  0:00:16s
epoch 59 | loss: 0.49531 | val_0_rmse: 0.75803 | val_1_rmse: 0.78593 |  0:00:17s
epoch 60 | loss: 0.50046 | val_0_rmse: 0.75256 | val_1_rmse: 0.7725  |  0:00:17s
epoch 61 | loss: 0.49267 | val_0_rmse: 0.74312 | val_1_rmse: 0.76896 |  0:00:17s
epoch 62 | loss: 0.48343 | val_0_rmse: 0.74302 | val_1_rmse: 0.77988 |  0:00:18s
epoch 63 | loss: 0.47938 | val_0_rmse: 0.73904 | val_1_rmse: 0.77137 |  0:00:18s
epoch 64 | loss: 0.48499 | val_0_rmse: 0.73667 | val_1_rmse: 0.76797 |  0:00:18s
epoch 65 | loss: 0.47757 | val_0_rmse: 0.73635 | val_1_rmse: 0.77083 |  0:00:18s
epoch 66 | loss: 0.47184 | val_0_rmse: 0.73786 | val_1_rmse: 0.76914 |  0:00:19s
epoch 67 | loss: 0.48409 | val_0_rmse: 0.75712 | val_1_rmse: 0.77747 |  0:00:19s
epoch 68 | loss: 0.48471 | val_0_rmse: 0.72769 | val_1_rmse: 0.7576  |  0:00:19s
epoch 69 | loss: 0.47627 | val_0_rmse: 0.73097 | val_1_rmse: 0.76026 |  0:00:20s
epoch 70 | loss: 0.47729 | val_0_rmse: 0.73931 | val_1_rmse: 0.77325 |  0:00:20s
epoch 71 | loss: 0.47373 | val_0_rmse: 0.73149 | val_1_rmse: 0.77544 |  0:00:20s
epoch 72 | loss: 0.47322 | val_0_rmse: 0.72759 | val_1_rmse: 0.77034 |  0:00:21s
epoch 73 | loss: 0.47668 | val_0_rmse: 0.72931 | val_1_rmse: 0.77347 |  0:00:21s
epoch 74 | loss: 0.47984 | val_0_rmse: 0.74236 | val_1_rmse: 0.80467 |  0:00:21s
epoch 75 | loss: 0.49342 | val_0_rmse: 0.73798 | val_1_rmse: 0.78565 |  0:00:21s
epoch 76 | loss: 0.47411 | val_0_rmse: 0.75742 | val_1_rmse: 0.80033 |  0:00:22s
epoch 77 | loss: 0.48116 | val_0_rmse: 0.72184 | val_1_rmse: 0.78091 |  0:00:22s
epoch 78 | loss: 0.46739 | val_0_rmse: 0.71392 | val_1_rmse: 0.78677 |  0:00:22s
epoch 79 | loss: 0.46518 | val_0_rmse: 0.71681 | val_1_rmse: 0.79215 |  0:00:23s
epoch 80 | loss: 0.46114 | val_0_rmse: 0.71534 | val_1_rmse: 0.78854 |  0:00:23s
epoch 81 | loss: 0.46283 | val_0_rmse: 0.72357 | val_1_rmse: 0.78452 |  0:00:23s
epoch 82 | loss: 0.45673 | val_0_rmse: 0.72954 | val_1_rmse: 0.7899  |  0:00:23s
epoch 83 | loss: 0.45325 | val_0_rmse: 0.70065 | val_1_rmse: 0.78978 |  0:00:24s
epoch 84 | loss: 0.45906 | val_0_rmse: 0.6967  | val_1_rmse: 0.78028 |  0:00:24s
epoch 85 | loss: 0.45447 | val_0_rmse: 0.71401 | val_1_rmse: 0.7939  |  0:00:24s
epoch 86 | loss: 0.46466 | val_0_rmse: 0.71092 | val_1_rmse: 0.80085 |  0:00:25s
epoch 87 | loss: 0.46501 | val_0_rmse: 0.69521 | val_1_rmse: 0.79402 |  0:00:25s
epoch 88 | loss: 0.45134 | val_0_rmse: 0.71244 | val_1_rmse: 0.8019  |  0:00:25s
epoch 89 | loss: 0.45759 | val_0_rmse: 0.68872 | val_1_rmse: 0.78985 |  0:00:25s
epoch 90 | loss: 0.45544 | val_0_rmse: 0.67894 | val_1_rmse: 0.77933 |  0:00:26s
epoch 91 | loss: 0.43949 | val_0_rmse: 0.68955 | val_1_rmse: 0.7786  |  0:00:26s
epoch 92 | loss: 0.44147 | val_0_rmse: 0.68369 | val_1_rmse: 0.78571 |  0:00:26s
epoch 93 | loss: 0.42898 | val_0_rmse: 0.67173 | val_1_rmse: 0.78074 |  0:00:27s
epoch 94 | loss: 0.43133 | val_0_rmse: 0.68542 | val_1_rmse: 0.79011 |  0:00:27s
epoch 95 | loss: 0.43998 | val_0_rmse: 0.67131 | val_1_rmse: 0.7819  |  0:00:27s
epoch 96 | loss: 0.4251  | val_0_rmse: 0.66062 | val_1_rmse: 0.7818  |  0:00:27s
epoch 97 | loss: 0.43338 | val_0_rmse: 0.68144 | val_1_rmse: 0.79428 |  0:00:28s
epoch 98 | loss: 0.43113 | val_0_rmse: 0.67126 | val_1_rmse: 0.7837  |  0:00:28s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.7576
Best weights from best epoch are automatically used!
ended training at: 05:52:23
Feature importance:
Mean squared error is of 0.06655019108834204
Mean absolute error:0.174588643492699
MAPE:0.19044149541791097
R2 score:0.3706865288556318
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:24
epoch 0  | loss: 1.9142  | val_0_rmse: 1.02846 | val_1_rmse: 1.00334 |  0:00:00s
epoch 1  | loss: 1.32284 | val_0_rmse: 0.9941  | val_1_rmse: 0.98062 |  0:00:00s
epoch 2  | loss: 1.06858 | val_0_rmse: 0.98363 | val_1_rmse: 0.96835 |  0:00:00s
epoch 3  | loss: 1.00478 | val_0_rmse: 0.98075 | val_1_rmse: 0.96327 |  0:00:01s
epoch 4  | loss: 0.98245 | val_0_rmse: 0.95838 | val_1_rmse: 0.93968 |  0:00:01s
epoch 5  | loss: 0.95948 | val_0_rmse: 0.94583 | val_1_rmse: 0.92246 |  0:00:01s
epoch 6  | loss: 0.91115 | val_0_rmse: 0.93417 | val_1_rmse: 0.91936 |  0:00:02s
epoch 7  | loss: 0.86091 | val_0_rmse: 0.96201 | val_1_rmse: 0.92452 |  0:00:02s
epoch 8  | loss: 0.8219  | val_0_rmse: 0.92393 | val_1_rmse: 0.89293 |  0:00:02s
epoch 9  | loss: 0.80063 | val_0_rmse: 0.91128 | val_1_rmse: 0.88477 |  0:00:02s
epoch 10 | loss: 0.77426 | val_0_rmse: 0.89172 | val_1_rmse: 0.86142 |  0:00:03s
epoch 11 | loss: 0.76792 | val_0_rmse: 0.87013 | val_1_rmse: 0.84784 |  0:00:03s
epoch 12 | loss: 0.74449 | val_0_rmse: 0.87051 | val_1_rmse: 0.86593 |  0:00:03s
epoch 13 | loss: 0.76145 | val_0_rmse: 0.87447 | val_1_rmse: 0.86649 |  0:00:04s
epoch 14 | loss: 0.7381  | val_0_rmse: 0.85992 | val_1_rmse: 0.86558 |  0:00:04s
epoch 15 | loss: 0.71921 | val_0_rmse: 0.88137 | val_1_rmse: 0.87074 |  0:00:04s
epoch 16 | loss: 0.70605 | val_0_rmse: 0.90136 | val_1_rmse: 0.88498 |  0:00:04s
epoch 17 | loss: 0.69933 | val_0_rmse: 0.86515 | val_1_rmse: 0.8516  |  0:00:05s
epoch 18 | loss: 0.69216 | val_0_rmse: 0.88081 | val_1_rmse: 0.87559 |  0:00:05s
epoch 19 | loss: 0.69677 | val_0_rmse: 0.87296 | val_1_rmse: 0.85527 |  0:00:05s
epoch 20 | loss: 0.70806 | val_0_rmse: 0.84084 | val_1_rmse: 0.8368  |  0:00:06s
epoch 21 | loss: 0.69152 | val_0_rmse: 0.83882 | val_1_rmse: 0.82038 |  0:00:06s
epoch 22 | loss: 0.68317 | val_0_rmse: 0.83753 | val_1_rmse: 0.83245 |  0:00:06s
epoch 23 | loss: 0.6798  | val_0_rmse: 0.83411 | val_1_rmse: 0.8286  |  0:00:07s
epoch 24 | loss: 0.66748 | val_0_rmse: 0.84027 | val_1_rmse: 0.83273 |  0:00:07s
epoch 25 | loss: 0.67026 | val_0_rmse: 0.83092 | val_1_rmse: 0.82906 |  0:00:07s
epoch 26 | loss: 0.6639  | val_0_rmse: 0.82697 | val_1_rmse: 0.82559 |  0:00:07s
epoch 27 | loss: 0.66859 | val_0_rmse: 0.82229 | val_1_rmse: 0.81485 |  0:00:08s
epoch 28 | loss: 0.66516 | val_0_rmse: 0.83388 | val_1_rmse: 0.82076 |  0:00:08s
epoch 29 | loss: 0.66368 | val_0_rmse: 0.84636 | val_1_rmse: 0.83398 |  0:00:08s
epoch 30 | loss: 0.66996 | val_0_rmse: 0.83991 | val_1_rmse: 0.8399  |  0:00:09s
epoch 31 | loss: 0.67864 | val_0_rmse: 0.83294 | val_1_rmse: 0.82766 |  0:00:09s
epoch 32 | loss: 0.67212 | val_0_rmse: 0.82378 | val_1_rmse: 0.81558 |  0:00:09s
epoch 33 | loss: 0.67088 | val_0_rmse: 0.82463 | val_1_rmse: 0.82095 |  0:00:09s
epoch 34 | loss: 0.67369 | val_0_rmse: 0.82361 | val_1_rmse: 0.82289 |  0:00:10s
epoch 35 | loss: 0.68052 | val_0_rmse: 0.8161  | val_1_rmse: 0.81227 |  0:00:10s
epoch 36 | loss: 0.66744 | val_0_rmse: 0.82352 | val_1_rmse: 0.80947 |  0:00:10s
epoch 37 | loss: 0.66182 | val_0_rmse: 0.82229 | val_1_rmse: 0.81465 |  0:00:11s
epoch 38 | loss: 0.66172 | val_0_rmse: 0.82204 | val_1_rmse: 0.82573 |  0:00:11s
epoch 39 | loss: 0.65383 | val_0_rmse: 0.81098 | val_1_rmse: 0.81103 |  0:00:11s
epoch 40 | loss: 0.64457 | val_0_rmse: 0.8156  | val_1_rmse: 0.81437 |  0:00:11s
epoch 41 | loss: 0.6429  | val_0_rmse: 0.83062 | val_1_rmse: 0.81602 |  0:00:12s
epoch 42 | loss: 0.65315 | val_0_rmse: 0.83345 | val_1_rmse: 0.81905 |  0:00:12s
epoch 43 | loss: 0.68005 | val_0_rmse: 0.82703 | val_1_rmse: 0.8184  |  0:00:12s
epoch 44 | loss: 0.65731 | val_0_rmse: 0.83888 | val_1_rmse: 0.82716 |  0:00:13s
epoch 45 | loss: 0.662   | val_0_rmse: 0.8261  | val_1_rmse: 0.81872 |  0:00:13s
epoch 46 | loss: 0.65107 | val_0_rmse: 0.82277 | val_1_rmse: 0.81774 |  0:00:13s
epoch 47 | loss: 0.65021 | val_0_rmse: 0.82067 | val_1_rmse: 0.81986 |  0:00:13s
epoch 48 | loss: 0.65343 | val_0_rmse: 0.81783 | val_1_rmse: 0.8154  |  0:00:14s
epoch 49 | loss: 0.64976 | val_0_rmse: 0.81149 | val_1_rmse: 0.8026  |  0:00:14s
epoch 50 | loss: 0.65316 | val_0_rmse: 0.81408 | val_1_rmse: 0.79877 |  0:00:14s
epoch 51 | loss: 0.66748 | val_0_rmse: 0.81779 | val_1_rmse: 0.80306 |  0:00:15s
epoch 52 | loss: 0.66549 | val_0_rmse: 0.81548 | val_1_rmse: 0.8017  |  0:00:15s
epoch 53 | loss: 0.65262 | val_0_rmse: 0.81903 | val_1_rmse: 0.80892 |  0:00:15s
epoch 54 | loss: 0.64485 | val_0_rmse: 0.81747 | val_1_rmse: 0.81126 |  0:00:15s
epoch 55 | loss: 0.65821 | val_0_rmse: 0.8201  | val_1_rmse: 0.81209 |  0:00:16s
epoch 56 | loss: 0.66335 | val_0_rmse: 0.81197 | val_1_rmse: 0.80553 |  0:00:16s
epoch 57 | loss: 0.63595 | val_0_rmse: 0.83283 | val_1_rmse: 0.81686 |  0:00:16s
epoch 58 | loss: 0.6559  | val_0_rmse: 0.81334 | val_1_rmse: 0.8019  |  0:00:16s
epoch 59 | loss: 0.63973 | val_0_rmse: 0.81319 | val_1_rmse: 0.80468 |  0:00:17s
epoch 60 | loss: 0.6484  | val_0_rmse: 0.80306 | val_1_rmse: 0.79987 |  0:00:17s
epoch 61 | loss: 0.64287 | val_0_rmse: 0.81743 | val_1_rmse: 0.81776 |  0:00:17s
epoch 62 | loss: 0.63443 | val_0_rmse: 0.80422 | val_1_rmse: 0.79553 |  0:00:18s
epoch 63 | loss: 0.62899 | val_0_rmse: 0.8053  | val_1_rmse: 0.79228 |  0:00:18s
epoch 64 | loss: 0.63009 | val_0_rmse: 0.80392 | val_1_rmse: 0.80911 |  0:00:18s
epoch 65 | loss: 0.62957 | val_0_rmse: 0.80653 | val_1_rmse: 0.81945 |  0:00:18s
epoch 66 | loss: 0.6288  | val_0_rmse: 0.80396 | val_1_rmse: 0.81159 |  0:00:19s
epoch 67 | loss: 0.62517 | val_0_rmse: 0.82159 | val_1_rmse: 0.83193 |  0:00:19s
epoch 68 | loss: 0.63516 | val_0_rmse: 0.81004 | val_1_rmse: 0.80323 |  0:00:19s
epoch 69 | loss: 0.62845 | val_0_rmse: 0.80537 | val_1_rmse: 0.79664 |  0:00:20s
epoch 70 | loss: 0.63091 | val_0_rmse: 0.80478 | val_1_rmse: 0.79671 |  0:00:20s
epoch 71 | loss: 0.63136 | val_0_rmse: 0.80815 | val_1_rmse: 0.81511 |  0:00:20s
epoch 72 | loss: 0.61451 | val_0_rmse: 0.80201 | val_1_rmse: 0.80327 |  0:00:20s
epoch 73 | loss: 0.63226 | val_0_rmse: 0.796   | val_1_rmse: 0.79929 |  0:00:21s
epoch 74 | loss: 0.61728 | val_0_rmse: 0.80069 | val_1_rmse: 0.81231 |  0:00:21s
epoch 75 | loss: 0.61672 | val_0_rmse: 0.81066 | val_1_rmse: 0.80408 |  0:00:21s
epoch 76 | loss: 0.61285 | val_0_rmse: 0.7966  | val_1_rmse: 0.79538 |  0:00:22s
epoch 77 | loss: 0.60763 | val_0_rmse: 0.79807 | val_1_rmse: 0.78828 |  0:00:22s
epoch 78 | loss: 0.61406 | val_0_rmse: 0.78893 | val_1_rmse: 0.77977 |  0:00:22s
epoch 79 | loss: 0.61213 | val_0_rmse: 0.79388 | val_1_rmse: 0.78289 |  0:00:22s
epoch 80 | loss: 0.61279 | val_0_rmse: 0.8059  | val_1_rmse: 0.79798 |  0:00:23s
epoch 81 | loss: 0.6207  | val_0_rmse: 0.79469 | val_1_rmse: 0.79152 |  0:00:23s
epoch 82 | loss: 0.62079 | val_0_rmse: 0.79338 | val_1_rmse: 0.78622 |  0:00:23s
epoch 83 | loss: 0.60842 | val_0_rmse: 0.8129  | val_1_rmse: 0.80626 |  0:00:24s
epoch 84 | loss: 0.63941 | val_0_rmse: 0.80419 | val_1_rmse: 0.80155 |  0:00:24s
epoch 85 | loss: 0.62606 | val_0_rmse: 0.80366 | val_1_rmse: 0.79619 |  0:00:24s
epoch 86 | loss: 0.62335 | val_0_rmse: 0.79798 | val_1_rmse: 0.79427 |  0:00:24s
epoch 87 | loss: 0.61723 | val_0_rmse: 0.79533 | val_1_rmse: 0.7932  |  0:00:25s
epoch 88 | loss: 0.61264 | val_0_rmse: 0.78776 | val_1_rmse: 0.78972 |  0:00:25s
epoch 89 | loss: 0.6141  | val_0_rmse: 0.78394 | val_1_rmse: 0.79511 |  0:00:25s
epoch 90 | loss: 0.61157 | val_0_rmse: 0.7838  | val_1_rmse: 0.78783 |  0:00:26s
epoch 91 | loss: 0.61741 | val_0_rmse: 0.78497 | val_1_rmse: 0.78963 |  0:00:26s
epoch 92 | loss: 0.60051 | val_0_rmse: 0.78745 | val_1_rmse: 0.79024 |  0:00:26s
epoch 93 | loss: 0.6101  | val_0_rmse: 0.7741  | val_1_rmse: 0.78062 |  0:00:27s
epoch 94 | loss: 0.6147  | val_0_rmse: 0.80296 | val_1_rmse: 0.79358 |  0:00:27s
epoch 95 | loss: 0.62346 | val_0_rmse: 0.79826 | val_1_rmse: 0.79445 |  0:00:27s
epoch 96 | loss: 0.62802 | val_0_rmse: 0.78965 | val_1_rmse: 0.79794 |  0:00:27s
epoch 97 | loss: 0.61805 | val_0_rmse: 0.78881 | val_1_rmse: 0.79623 |  0:00:28s
epoch 98 | loss: 0.60544 | val_0_rmse: 0.78897 | val_1_rmse: 0.79938 |  0:00:28s
epoch 99 | loss: 0.60586 | val_0_rmse: 0.77923 | val_1_rmse: 0.79912 |  0:00:28s
epoch 100| loss: 0.6056  | val_0_rmse: 0.7678  | val_1_rmse: 0.78946 |  0:00:29s
epoch 101| loss: 0.59434 | val_0_rmse: 0.7757  | val_1_rmse: 0.79622 |  0:00:29s
epoch 102| loss: 0.59562 | val_0_rmse: 0.77323 | val_1_rmse: 0.79185 |  0:00:29s
epoch 103| loss: 0.59867 | val_0_rmse: 0.77495 | val_1_rmse: 0.7919  |  0:00:29s
epoch 104| loss: 0.59312 | val_0_rmse: 0.77229 | val_1_rmse: 0.78488 |  0:00:30s
epoch 105| loss: 0.59694 | val_0_rmse: 0.7682  | val_1_rmse: 0.78264 |  0:00:30s
epoch 106| loss: 0.59164 | val_0_rmse: 0.7723  | val_1_rmse: 0.78161 |  0:00:30s
epoch 107| loss: 0.60302 | val_0_rmse: 0.77301 | val_1_rmse: 0.77797 |  0:00:31s
epoch 108| loss: 0.58833 | val_0_rmse: 0.76374 | val_1_rmse: 0.77927 |  0:00:31s
epoch 109| loss: 0.59046 | val_0_rmse: 0.76636 | val_1_rmse: 0.78109 |  0:00:31s
epoch 110| loss: 0.59694 | val_0_rmse: 0.77222 | val_1_rmse: 0.77131 |  0:00:31s
epoch 111| loss: 0.59683 | val_0_rmse: 0.76354 | val_1_rmse: 0.77363 |  0:00:32s
epoch 112| loss: 0.58545 | val_0_rmse: 0.77626 | val_1_rmse: 0.79029 |  0:00:32s
epoch 113| loss: 0.60851 | val_0_rmse: 0.78365 | val_1_rmse: 0.79601 |  0:00:32s
epoch 114| loss: 0.60892 | val_0_rmse: 0.78113 | val_1_rmse: 0.80486 |  0:00:33s
epoch 115| loss: 0.59548 | val_0_rmse: 0.768   | val_1_rmse: 0.78048 |  0:00:33s
epoch 116| loss: 0.59543 | val_0_rmse: 0.76458 | val_1_rmse: 0.7785  |  0:00:33s
epoch 117| loss: 0.59624 | val_0_rmse: 0.7759  | val_1_rmse: 0.79067 |  0:00:33s
epoch 118| loss: 0.5921  | val_0_rmse: 0.7615  | val_1_rmse: 0.77879 |  0:00:34s
epoch 119| loss: 0.58915 | val_0_rmse: 0.76497 | val_1_rmse: 0.78168 |  0:00:34s
epoch 120| loss: 0.59569 | val_0_rmse: 0.77607 | val_1_rmse: 0.80954 |  0:00:34s
epoch 121| loss: 0.60726 | val_0_rmse: 0.76522 | val_1_rmse: 0.79864 |  0:00:35s
epoch 122| loss: 0.58833 | val_0_rmse: 0.76647 | val_1_rmse: 0.79292 |  0:00:35s
epoch 123| loss: 0.58579 | val_0_rmse: 0.76402 | val_1_rmse: 0.79481 |  0:00:35s
epoch 124| loss: 0.60808 | val_0_rmse: 0.76033 | val_1_rmse: 0.78077 |  0:00:35s
epoch 125| loss: 0.63689 | val_0_rmse: 0.78734 | val_1_rmse: 0.79571 |  0:00:36s
epoch 126| loss: 0.63044 | val_0_rmse: 0.79397 | val_1_rmse: 0.80453 |  0:00:36s
epoch 127| loss: 0.63371 | val_0_rmse: 0.7887  | val_1_rmse: 0.78704 |  0:00:36s
epoch 128| loss: 0.63933 | val_0_rmse: 0.79845 | val_1_rmse: 0.79912 |  0:00:36s
epoch 129| loss: 0.63223 | val_0_rmse: 0.79651 | val_1_rmse: 0.79962 |  0:00:37s
epoch 130| loss: 0.61596 | val_0_rmse: 0.78867 | val_1_rmse: 0.80953 |  0:00:37s
epoch 131| loss: 0.61862 | val_0_rmse: 0.77748 | val_1_rmse: 0.80654 |  0:00:37s
epoch 132| loss: 0.61375 | val_0_rmse: 0.77643 | val_1_rmse: 0.80628 |  0:00:38s
epoch 133| loss: 0.61472 | val_0_rmse: 0.79282 | val_1_rmse: 0.81637 |  0:00:38s
epoch 134| loss: 0.61789 | val_0_rmse: 0.77584 | val_1_rmse: 0.79696 |  0:00:38s
epoch 135| loss: 0.60857 | val_0_rmse: 0.77932 | val_1_rmse: 0.79702 |  0:00:38s
epoch 136| loss: 0.61258 | val_0_rmse: 0.77109 | val_1_rmse: 0.78448 |  0:00:39s
epoch 137| loss: 0.59844 | val_0_rmse: 0.76562 | val_1_rmse: 0.78741 |  0:00:39s
epoch 138| loss: 0.59339 | val_0_rmse: 0.75763 | val_1_rmse: 0.7875  |  0:00:39s
epoch 139| loss: 0.57818 | val_0_rmse: 0.76932 | val_1_rmse: 0.79417 |  0:00:40s
epoch 140| loss: 0.58619 | val_0_rmse: 0.75826 | val_1_rmse: 0.77879 |  0:00:40s

Early stopping occured at epoch 140 with best_epoch = 110 and best_val_1_rmse = 0.77131
Best weights from best epoch are automatically used!
ended training at: 05:53:04
Feature importance:
Mean squared error is of 0.054370221263455684
Mean absolute error:0.17192391210073085
MAPE:0.20182848191535305
R2 score:0.3493969468831435
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:53:05
epoch 0  | loss: 1.93247 | val_0_rmse: 0.99164 | val_1_rmse: 1.02746 |  0:00:00s
epoch 1  | loss: 1.24942 | val_0_rmse: 0.98759 | val_1_rmse: 1.01869 |  0:00:00s
epoch 2  | loss: 1.02266 | val_0_rmse: 0.98966 | val_1_rmse: 1.02253 |  0:00:01s
epoch 3  | loss: 0.99709 | val_0_rmse: 0.98783 | val_1_rmse: 1.0205  |  0:00:01s
epoch 4  | loss: 0.98144 | val_0_rmse: 0.98747 | val_1_rmse: 1.01819 |  0:00:01s
epoch 5  | loss: 0.97253 | val_0_rmse: 0.98633 | val_1_rmse: 1.01742 |  0:00:01s
epoch 6  | loss: 0.95769 | val_0_rmse: 0.98047 | val_1_rmse: 1.01364 |  0:00:02s
epoch 7  | loss: 0.96807 | val_0_rmse: 0.96833 | val_1_rmse: 1.00363 |  0:00:02s
epoch 8  | loss: 0.94454 | val_0_rmse: 0.95881 | val_1_rmse: 0.99584 |  0:00:02s
epoch 9  | loss: 0.92869 | val_0_rmse: 0.95016 | val_1_rmse: 0.99236 |  0:00:03s
epoch 10 | loss: 0.91945 | val_0_rmse: 0.94398 | val_1_rmse: 0.98969 |  0:00:03s
epoch 11 | loss: 0.90226 | val_0_rmse: 0.93783 | val_1_rmse: 0.98066 |  0:00:03s
epoch 12 | loss: 0.88359 | val_0_rmse: 0.92938 | val_1_rmse: 0.97876 |  0:00:03s
epoch 13 | loss: 0.86043 | val_0_rmse: 0.93392 | val_1_rmse: 0.98941 |  0:00:04s
epoch 14 | loss: 0.82895 | val_0_rmse: 0.9239  | val_1_rmse: 0.97686 |  0:00:04s
epoch 15 | loss: 0.80304 | val_0_rmse: 0.904   | val_1_rmse: 0.94967 |  0:00:04s
epoch 16 | loss: 0.76693 | val_0_rmse: 0.88351 | val_1_rmse: 0.92401 |  0:00:05s
epoch 17 | loss: 0.74991 | val_0_rmse: 0.87468 | val_1_rmse: 0.9158  |  0:00:05s
epoch 18 | loss: 0.71498 | val_0_rmse: 0.89639 | val_1_rmse: 0.94429 |  0:00:05s
epoch 19 | loss: 0.68883 | val_0_rmse: 0.89902 | val_1_rmse: 0.94034 |  0:00:05s
epoch 20 | loss: 0.687   | val_0_rmse: 0.89078 | val_1_rmse: 0.92993 |  0:00:06s
epoch 21 | loss: 0.65718 | val_0_rmse: 0.90158 | val_1_rmse: 0.94354 |  0:00:06s
epoch 22 | loss: 0.6362  | val_0_rmse: 0.8973  | val_1_rmse: 0.94287 |  0:00:06s
epoch 23 | loss: 0.63587 | val_0_rmse: 0.84233 | val_1_rmse: 0.88083 |  0:00:07s
epoch 24 | loss: 0.63224 | val_0_rmse: 0.85087 | val_1_rmse: 0.88899 |  0:00:07s
epoch 25 | loss: 0.62032 | val_0_rmse: 0.85302 | val_1_rmse: 0.89523 |  0:00:07s
epoch 26 | loss: 0.61135 | val_0_rmse: 0.83098 | val_1_rmse: 0.8679  |  0:00:07s
epoch 27 | loss: 0.59316 | val_0_rmse: 0.80408 | val_1_rmse: 0.84411 |  0:00:08s
epoch 28 | loss: 0.59555 | val_0_rmse: 0.81424 | val_1_rmse: 0.85295 |  0:00:08s
epoch 29 | loss: 0.60021 | val_0_rmse: 0.80086 | val_1_rmse: 0.83848 |  0:00:08s
epoch 30 | loss: 0.59363 | val_0_rmse: 0.81116 | val_1_rmse: 0.85284 |  0:00:09s
epoch 31 | loss: 0.58235 | val_0_rmse: 0.8062  | val_1_rmse: 0.85243 |  0:00:09s
epoch 32 | loss: 0.57776 | val_0_rmse: 0.80221 | val_1_rmse: 0.8501  |  0:00:09s
epoch 33 | loss: 0.57722 | val_0_rmse: 0.79732 | val_1_rmse: 0.84691 |  0:00:09s
epoch 34 | loss: 0.54745 | val_0_rmse: 0.79404 | val_1_rmse: 0.84585 |  0:00:10s
epoch 35 | loss: 0.55686 | val_0_rmse: 0.80734 | val_1_rmse: 0.86198 |  0:00:10s
epoch 36 | loss: 0.55359 | val_0_rmse: 0.82848 | val_1_rmse: 0.88031 |  0:00:10s
epoch 37 | loss: 0.55484 | val_0_rmse: 0.80702 | val_1_rmse: 0.8636  |  0:00:11s
epoch 38 | loss: 0.53936 | val_0_rmse: 0.79242 | val_1_rmse: 0.85346 |  0:00:11s
epoch 39 | loss: 0.54303 | val_0_rmse: 0.78176 | val_1_rmse: 0.83977 |  0:00:11s
epoch 40 | loss: 0.53675 | val_0_rmse: 0.78461 | val_1_rmse: 0.84714 |  0:00:11s
epoch 41 | loss: 0.5394  | val_0_rmse: 0.78653 | val_1_rmse: 0.85175 |  0:00:12s
epoch 42 | loss: 0.53852 | val_0_rmse: 0.79082 | val_1_rmse: 0.85722 |  0:00:12s
epoch 43 | loss: 0.53444 | val_0_rmse: 0.78604 | val_1_rmse: 0.84903 |  0:00:12s
epoch 44 | loss: 0.53769 | val_0_rmse: 0.79722 | val_1_rmse: 0.86077 |  0:00:13s
epoch 45 | loss: 0.53769 | val_0_rmse: 0.7835  | val_1_rmse: 0.84738 |  0:00:13s
epoch 46 | loss: 0.52995 | val_0_rmse: 0.78278 | val_1_rmse: 0.85561 |  0:00:13s
epoch 47 | loss: 0.53128 | val_0_rmse: 0.78298 | val_1_rmse: 0.85601 |  0:00:13s
epoch 48 | loss: 0.51295 | val_0_rmse: 0.77213 | val_1_rmse: 0.84947 |  0:00:14s
epoch 49 | loss: 0.52461 | val_0_rmse: 0.77256 | val_1_rmse: 0.84889 |  0:00:14s
epoch 50 | loss: 0.52092 | val_0_rmse: 0.78303 | val_1_rmse: 0.85719 |  0:00:14s
epoch 51 | loss: 0.53351 | val_0_rmse: 0.7752  | val_1_rmse: 0.83918 |  0:00:15s
epoch 52 | loss: 0.53038 | val_0_rmse: 0.76764 | val_1_rmse: 0.83027 |  0:00:15s
epoch 53 | loss: 0.52776 | val_0_rmse: 0.76373 | val_1_rmse: 0.82613 |  0:00:15s
epoch 54 | loss: 0.53523 | val_0_rmse: 0.76674 | val_1_rmse: 0.83492 |  0:00:16s
epoch 55 | loss: 0.52931 | val_0_rmse: 0.77196 | val_1_rmse: 0.8509  |  0:00:16s
epoch 56 | loss: 0.53012 | val_0_rmse: 0.79776 | val_1_rmse: 0.87573 |  0:00:16s
epoch 57 | loss: 0.53383 | val_0_rmse: 0.78716 | val_1_rmse: 0.87057 |  0:00:16s
epoch 58 | loss: 0.52422 | val_0_rmse: 0.77378 | val_1_rmse: 0.85612 |  0:00:17s
epoch 59 | loss: 0.51312 | val_0_rmse: 0.77113 | val_1_rmse: 0.85334 |  0:00:17s
epoch 60 | loss: 0.50939 | val_0_rmse: 0.76048 | val_1_rmse: 0.8445  |  0:00:17s
epoch 61 | loss: 0.5081  | val_0_rmse: 0.75684 | val_1_rmse: 0.84531 |  0:00:18s
epoch 62 | loss: 0.50013 | val_0_rmse: 0.76721 | val_1_rmse: 0.86997 |  0:00:18s
epoch 63 | loss: 0.50064 | val_0_rmse: 0.78937 | val_1_rmse: 0.89823 |  0:00:18s
epoch 64 | loss: 0.48898 | val_0_rmse: 0.76706 | val_1_rmse: 0.87243 |  0:00:18s
epoch 65 | loss: 0.4938  | val_0_rmse: 0.755   | val_1_rmse: 0.85479 |  0:00:19s
epoch 66 | loss: 0.48817 | val_0_rmse: 0.74816 | val_1_rmse: 0.84456 |  0:00:19s
epoch 67 | loss: 0.50095 | val_0_rmse: 0.74739 | val_1_rmse: 0.84538 |  0:00:19s
epoch 68 | loss: 0.48588 | val_0_rmse: 0.75237 | val_1_rmse: 0.85594 |  0:00:20s
epoch 69 | loss: 0.48688 | val_0_rmse: 0.73641 | val_1_rmse: 0.83663 |  0:00:20s
epoch 70 | loss: 0.47855 | val_0_rmse: 0.74189 | val_1_rmse: 0.84751 |  0:00:20s
epoch 71 | loss: 0.46957 | val_0_rmse: 0.74078 | val_1_rmse: 0.86177 |  0:00:20s
epoch 72 | loss: 0.47442 | val_0_rmse: 0.72665 | val_1_rmse: 0.84242 |  0:00:21s
epoch 73 | loss: 0.47089 | val_0_rmse: 0.72613 | val_1_rmse: 0.82904 |  0:00:21s
epoch 74 | loss: 0.48557 | val_0_rmse: 0.72532 | val_1_rmse: 0.83146 |  0:00:21s
epoch 75 | loss: 0.47723 | val_0_rmse: 0.72746 | val_1_rmse: 0.83917 |  0:00:22s
epoch 76 | loss: 0.48128 | val_0_rmse: 0.72301 | val_1_rmse: 0.8359  |  0:00:22s
epoch 77 | loss: 0.46949 | val_0_rmse: 0.72186 | val_1_rmse: 0.8392  |  0:00:22s
epoch 78 | loss: 0.46604 | val_0_rmse: 0.72591 | val_1_rmse: 0.86074 |  0:00:22s
epoch 79 | loss: 0.46419 | val_0_rmse: 0.73032 | val_1_rmse: 0.86615 |  0:00:23s
epoch 80 | loss: 0.45868 | val_0_rmse: 0.73247 | val_1_rmse: 0.87311 |  0:00:23s
epoch 81 | loss: 0.45572 | val_0_rmse: 0.71746 | val_1_rmse: 0.85623 |  0:00:23s
epoch 82 | loss: 0.45885 | val_0_rmse: 0.71543 | val_1_rmse: 0.83776 |  0:00:24s
epoch 83 | loss: 0.4621  | val_0_rmse: 0.71012 | val_1_rmse: 0.84155 |  0:00:24s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.82613
Best weights from best epoch are automatically used!
ended training at: 05:53:29
Feature importance:
Mean squared error is of 0.06258231139181437
Mean absolute error:0.17995781891658888
MAPE:0.20050064811048388
R2 score:0.32131623286681543
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:53:29
epoch 0  | loss: 1.98642 | val_0_rmse: 1.02529 | val_1_rmse: 1.08867 |  0:00:00s
epoch 1  | loss: 1.2087  | val_0_rmse: 0.98485 | val_1_rmse: 1.05036 |  0:00:00s
epoch 2  | loss: 0.99922 | val_0_rmse: 0.98353 | val_1_rmse: 1.04926 |  0:00:00s
epoch 3  | loss: 0.9798  | val_0_rmse: 0.98212 | val_1_rmse: 1.04708 |  0:00:01s
epoch 4  | loss: 0.96631 | val_0_rmse: 0.96527 | val_1_rmse: 1.019   |  0:00:01s
epoch 5  | loss: 0.92893 | val_0_rmse: 0.95183 | val_1_rmse: 0.99489 |  0:00:01s
epoch 6  | loss: 0.90041 | val_0_rmse: 1.00566 | val_1_rmse: 1.07968 |  0:00:02s
epoch 7  | loss: 0.79588 | val_0_rmse: 1.08183 | val_1_rmse: 1.19605 |  0:00:02s
epoch 8  | loss: 0.77282 | val_0_rmse: 0.945   | val_1_rmse: 1.04802 |  0:00:02s
epoch 9  | loss: 0.73457 | val_0_rmse: 0.87784 | val_1_rmse: 0.94479 |  0:00:02s
epoch 10 | loss: 0.72264 | val_0_rmse: 0.91598 | val_1_rmse: 0.97963 |  0:00:03s
epoch 11 | loss: 0.71538 | val_0_rmse: 0.86948 | val_1_rmse: 0.92916 |  0:00:03s
epoch 12 | loss: 0.71898 | val_0_rmse: 0.85191 | val_1_rmse: 0.91824 |  0:00:03s
epoch 13 | loss: 0.70245 | val_0_rmse: 0.87576 | val_1_rmse: 0.94566 |  0:00:04s
epoch 14 | loss: 0.70753 | val_0_rmse: 0.84921 | val_1_rmse: 0.91006 |  0:00:04s
epoch 15 | loss: 0.70913 | val_0_rmse: 0.85668 | val_1_rmse: 0.91669 |  0:00:04s
epoch 16 | loss: 0.69589 | val_0_rmse: 0.84742 | val_1_rmse: 0.91086 |  0:00:04s
epoch 17 | loss: 0.69683 | val_0_rmse: 0.85022 | val_1_rmse: 0.91815 |  0:00:05s
epoch 18 | loss: 0.70079 | val_0_rmse: 0.84566 | val_1_rmse: 0.90741 |  0:00:05s
epoch 19 | loss: 0.67914 | val_0_rmse: 0.85174 | val_1_rmse: 0.90598 |  0:00:05s
epoch 20 | loss: 0.68596 | val_0_rmse: 0.86465 | val_1_rmse: 0.91571 |  0:00:06s
epoch 21 | loss: 0.70114 | val_0_rmse: 0.84345 | val_1_rmse: 0.90558 |  0:00:06s
epoch 22 | loss: 0.68943 | val_0_rmse: 0.84786 | val_1_rmse: 0.91039 |  0:00:06s
epoch 23 | loss: 0.68008 | val_0_rmse: 0.84286 | val_1_rmse: 0.90637 |  0:00:06s
epoch 24 | loss: 0.67188 | val_0_rmse: 0.82916 | val_1_rmse: 0.90099 |  0:00:07s
epoch 25 | loss: 0.67539 | val_0_rmse: 0.81611 | val_1_rmse: 0.89668 |  0:00:07s
epoch 26 | loss: 0.67998 | val_0_rmse: 0.81939 | val_1_rmse: 0.89299 |  0:00:07s
epoch 27 | loss: 0.66564 | val_0_rmse: 0.82977 | val_1_rmse: 0.90836 |  0:00:08s
epoch 28 | loss: 0.67504 | val_0_rmse: 0.82364 | val_1_rmse: 0.90352 |  0:00:08s
epoch 29 | loss: 0.67359 | val_0_rmse: 0.82551 | val_1_rmse: 0.90683 |  0:00:08s
epoch 30 | loss: 0.6672  | val_0_rmse: 0.82971 | val_1_rmse: 0.91642 |  0:00:09s
epoch 31 | loss: 0.64887 | val_0_rmse: 0.81458 | val_1_rmse: 0.89566 |  0:00:09s
epoch 32 | loss: 0.6658  | val_0_rmse: 0.81825 | val_1_rmse: 0.90077 |  0:00:09s
epoch 33 | loss: 0.64413 | val_0_rmse: 0.80702 | val_1_rmse: 0.88026 |  0:00:09s
epoch 34 | loss: 0.65235 | val_0_rmse: 0.80743 | val_1_rmse: 0.88668 |  0:00:10s
epoch 35 | loss: 0.63608 | val_0_rmse: 0.81271 | val_1_rmse: 0.89012 |  0:00:10s
epoch 36 | loss: 0.63322 | val_0_rmse: 0.81656 | val_1_rmse: 0.90333 |  0:00:10s
epoch 37 | loss: 0.62909 | val_0_rmse: 0.81136 | val_1_rmse: 0.9101  |  0:00:11s
epoch 38 | loss: 0.62867 | val_0_rmse: 0.80561 | val_1_rmse: 0.88976 |  0:00:11s
epoch 39 | loss: 0.61847 | val_0_rmse: 0.81127 | val_1_rmse: 0.90171 |  0:00:11s
epoch 40 | loss: 0.62359 | val_0_rmse: 0.79968 | val_1_rmse: 0.87783 |  0:00:11s
epoch 41 | loss: 0.61179 | val_0_rmse: 0.80874 | val_1_rmse: 0.88666 |  0:00:12s
epoch 42 | loss: 0.61073 | val_0_rmse: 0.81012 | val_1_rmse: 0.88151 |  0:00:12s
epoch 43 | loss: 0.61037 | val_0_rmse: 0.83652 | val_1_rmse: 0.91848 |  0:00:12s
epoch 44 | loss: 0.60412 | val_0_rmse: 0.79926 | val_1_rmse: 0.876   |  0:00:13s
epoch 45 | loss: 0.60508 | val_0_rmse: 0.79312 | val_1_rmse: 0.86834 |  0:00:13s
epoch 46 | loss: 0.59486 | val_0_rmse: 0.79698 | val_1_rmse: 0.87866 |  0:00:13s
epoch 47 | loss: 0.60673 | val_0_rmse: 0.77895 | val_1_rmse: 0.8398  |  0:00:13s
epoch 48 | loss: 0.59327 | val_0_rmse: 0.7842  | val_1_rmse: 0.85248 |  0:00:14s
epoch 49 | loss: 0.59488 | val_0_rmse: 0.78477 | val_1_rmse: 0.84888 |  0:00:14s
epoch 50 | loss: 0.58597 | val_0_rmse: 0.78687 | val_1_rmse: 0.85438 |  0:00:14s
epoch 51 | loss: 0.59429 | val_0_rmse: 0.79408 | val_1_rmse: 0.86645 |  0:00:15s
epoch 52 | loss: 0.5977  | val_0_rmse: 0.80555 | val_1_rmse: 0.88376 |  0:00:15s
epoch 53 | loss: 0.60031 | val_0_rmse: 0.8049  | val_1_rmse: 0.88156 |  0:00:15s
epoch 54 | loss: 0.58901 | val_0_rmse: 0.79118 | val_1_rmse: 0.87706 |  0:00:15s
epoch 55 | loss: 0.58761 | val_0_rmse: 0.79044 | val_1_rmse: 0.87639 |  0:00:16s
epoch 56 | loss: 0.58877 | val_0_rmse: 0.79299 | val_1_rmse: 0.88094 |  0:00:16s
epoch 57 | loss: 0.58666 | val_0_rmse: 0.7885  | val_1_rmse: 0.87851 |  0:00:16s
epoch 58 | loss: 0.60484 | val_0_rmse: 0.78102 | val_1_rmse: 0.86376 |  0:00:17s
epoch 59 | loss: 0.58658 | val_0_rmse: 0.78454 | val_1_rmse: 0.86851 |  0:00:17s
epoch 60 | loss: 0.57931 | val_0_rmse: 0.77118 | val_1_rmse: 0.85611 |  0:00:17s
epoch 61 | loss: 0.58113 | val_0_rmse: 0.77976 | val_1_rmse: 0.86984 |  0:00:17s
epoch 62 | loss: 0.57999 | val_0_rmse: 0.77579 | val_1_rmse: 0.86837 |  0:00:18s
epoch 63 | loss: 0.56788 | val_0_rmse: 0.77111 | val_1_rmse: 0.8605  |  0:00:18s
epoch 64 | loss: 0.56734 | val_0_rmse: 0.7741  | val_1_rmse: 0.86428 |  0:00:18s
epoch 65 | loss: 0.55762 | val_0_rmse: 0.76956 | val_1_rmse: 0.86239 |  0:00:19s
epoch 66 | loss: 0.5534  | val_0_rmse: 0.76965 | val_1_rmse: 0.85674 |  0:00:19s
epoch 67 | loss: 0.56288 | val_0_rmse: 0.76916 | val_1_rmse: 0.85457 |  0:00:19s
epoch 68 | loss: 0.55341 | val_0_rmse: 0.76285 | val_1_rmse: 0.8517  |  0:00:19s
epoch 69 | loss: 0.56042 | val_0_rmse: 0.76173 | val_1_rmse: 0.85443 |  0:00:20s
epoch 70 | loss: 0.56827 | val_0_rmse: 0.76289 | val_1_rmse: 0.85433 |  0:00:20s
epoch 71 | loss: 0.56478 | val_0_rmse: 0.76814 | val_1_rmse: 0.86405 |  0:00:20s
epoch 72 | loss: 0.56356 | val_0_rmse: 0.76392 | val_1_rmse: 0.86257 |  0:00:20s
epoch 73 | loss: 0.56022 | val_0_rmse: 0.78596 | val_1_rmse: 0.88298 |  0:00:21s
epoch 74 | loss: 0.56704 | val_0_rmse: 0.76735 | val_1_rmse: 0.86661 |  0:00:21s
epoch 75 | loss: 0.55266 | val_0_rmse: 0.75579 | val_1_rmse: 0.85292 |  0:00:21s
epoch 76 | loss: 0.55024 | val_0_rmse: 0.75624 | val_1_rmse: 0.85187 |  0:00:22s
epoch 77 | loss: 0.55395 | val_0_rmse: 0.75629 | val_1_rmse: 0.85127 |  0:00:22s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.8398
Best weights from best epoch are automatically used!
ended training at: 05:53:52
Feature importance:
Mean squared error is of 0.05941393430395281
Mean absolute error:0.17199221589828592
MAPE:0.1879520547347296
R2 score:0.3200800322503504
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:53:52
epoch 0  | loss: 2.26771 | val_0_rmse: 1.01287 | val_1_rmse: 0.94824 |  0:00:00s
epoch 1  | loss: 1.18555 | val_0_rmse: 1.01026 | val_1_rmse: 0.94187 |  0:00:00s
epoch 2  | loss: 1.02303 | val_0_rmse: 0.98972 | val_1_rmse: 0.92202 |  0:00:00s
epoch 3  | loss: 0.99503 | val_0_rmse: 0.96902 | val_1_rmse: 0.90998 |  0:00:01s
epoch 4  | loss: 0.96255 | val_0_rmse: 1.01329 | val_1_rmse: 0.96969 |  0:00:01s
epoch 5  | loss: 0.92737 | val_0_rmse: 0.92948 | val_1_rmse: 0.86703 |  0:00:01s
epoch 6  | loss: 0.90785 | val_0_rmse: 0.9886  | val_1_rmse: 0.93289 |  0:00:02s
epoch 7  | loss: 0.8863  | val_0_rmse: 0.90979 | val_1_rmse: 0.85792 |  0:00:02s
epoch 8  | loss: 0.84274 | val_0_rmse: 0.86759 | val_1_rmse: 0.81204 |  0:00:02s
epoch 9  | loss: 0.82724 | val_0_rmse: 0.86634 | val_1_rmse: 0.8107  |  0:00:02s
epoch 10 | loss: 0.81637 | val_0_rmse: 0.85418 | val_1_rmse: 0.79481 |  0:00:03s
epoch 11 | loss: 0.8099  | val_0_rmse: 0.85035 | val_1_rmse: 0.79707 |  0:00:03s
epoch 12 | loss: 0.80208 | val_0_rmse: 0.85037 | val_1_rmse: 0.79947 |  0:00:03s
epoch 13 | loss: 0.79163 | val_0_rmse: 0.84783 | val_1_rmse: 0.79783 |  0:00:04s
epoch 14 | loss: 0.79333 | val_0_rmse: 0.84011 | val_1_rmse: 0.78898 |  0:00:04s
epoch 15 | loss: 0.77399 | val_0_rmse: 0.83547 | val_1_rmse: 0.78354 |  0:00:04s
epoch 16 | loss: 0.77125 | val_0_rmse: 0.84007 | val_1_rmse: 0.7798  |  0:00:05s
epoch 17 | loss: 0.76763 | val_0_rmse: 0.83231 | val_1_rmse: 0.7723  |  0:00:05s
epoch 18 | loss: 0.75864 | val_0_rmse: 0.82777 | val_1_rmse: 0.76603 |  0:00:05s
epoch 19 | loss: 0.7539  | val_0_rmse: 0.82915 | val_1_rmse: 0.76175 |  0:00:05s
epoch 20 | loss: 0.7394  | val_0_rmse: 0.82486 | val_1_rmse: 0.76263 |  0:00:06s
epoch 21 | loss: 0.74435 | val_0_rmse: 0.82156 | val_1_rmse: 0.75608 |  0:00:06s
epoch 22 | loss: 0.73899 | val_0_rmse: 0.8185  | val_1_rmse: 0.75461 |  0:00:06s
epoch 23 | loss: 0.73992 | val_0_rmse: 0.81677 | val_1_rmse: 0.75346 |  0:00:07s
epoch 24 | loss: 0.73394 | val_0_rmse: 0.83267 | val_1_rmse: 0.77255 |  0:00:07s
epoch 25 | loss: 0.73311 | val_0_rmse: 0.81555 | val_1_rmse: 0.75473 |  0:00:07s
epoch 26 | loss: 0.71743 | val_0_rmse: 0.81482 | val_1_rmse: 0.7546  |  0:00:07s
epoch 27 | loss: 0.71267 | val_0_rmse: 0.8137  | val_1_rmse: 0.75203 |  0:00:08s
epoch 28 | loss: 0.70952 | val_0_rmse: 0.80895 | val_1_rmse: 0.75982 |  0:00:08s
epoch 29 | loss: 0.71485 | val_0_rmse: 0.8004  | val_1_rmse: 0.74926 |  0:00:08s
epoch 30 | loss: 0.70987 | val_0_rmse: 0.80459 | val_1_rmse: 0.75152 |  0:00:09s
epoch 31 | loss: 0.70757 | val_0_rmse: 0.80304 | val_1_rmse: 0.75826 |  0:00:09s
epoch 32 | loss: 0.69248 | val_0_rmse: 0.80505 | val_1_rmse: 0.75671 |  0:00:09s
epoch 33 | loss: 0.69446 | val_0_rmse: 0.80354 | val_1_rmse: 0.76105 |  0:00:09s
epoch 34 | loss: 0.68872 | val_0_rmse: 0.80618 | val_1_rmse: 0.7565  |  0:00:10s
epoch 35 | loss: 0.68872 | val_0_rmse: 0.80505 | val_1_rmse: 0.75647 |  0:00:10s
epoch 36 | loss: 0.67867 | val_0_rmse: 0.81052 | val_1_rmse: 0.7711  |  0:00:10s
epoch 37 | loss: 0.67527 | val_0_rmse: 0.79986 | val_1_rmse: 0.76051 |  0:00:11s
epoch 38 | loss: 0.68137 | val_0_rmse: 0.80406 | val_1_rmse: 0.76539 |  0:00:11s
epoch 39 | loss: 0.67905 | val_0_rmse: 0.80674 | val_1_rmse: 0.7665  |  0:00:11s
epoch 40 | loss: 0.67502 | val_0_rmse: 0.81366 | val_1_rmse: 0.77287 |  0:00:11s
epoch 41 | loss: 0.68355 | val_0_rmse: 0.81327 | val_1_rmse: 0.77444 |  0:00:12s
epoch 42 | loss: 0.67297 | val_0_rmse: 0.81066 | val_1_rmse: 0.76197 |  0:00:12s
epoch 43 | loss: 0.6696  | val_0_rmse: 0.80351 | val_1_rmse: 0.75533 |  0:00:12s
epoch 44 | loss: 0.66336 | val_0_rmse: 0.81496 | val_1_rmse: 0.77343 |  0:00:13s
epoch 45 | loss: 0.66912 | val_0_rmse: 0.80353 | val_1_rmse: 0.763   |  0:00:13s
epoch 46 | loss: 0.6601  | val_0_rmse: 0.79846 | val_1_rmse: 0.76087 |  0:00:13s
epoch 47 | loss: 0.65326 | val_0_rmse: 0.806   | val_1_rmse: 0.77242 |  0:00:13s
epoch 48 | loss: 0.64434 | val_0_rmse: 0.79531 | val_1_rmse: 0.75555 |  0:00:14s
epoch 49 | loss: 0.63862 | val_0_rmse: 0.78968 | val_1_rmse: 0.74777 |  0:00:14s
epoch 50 | loss: 0.62142 | val_0_rmse: 0.79514 | val_1_rmse: 0.75636 |  0:00:14s
epoch 51 | loss: 0.60201 | val_0_rmse: 0.7979  | val_1_rmse: 0.75143 |  0:00:15s
epoch 52 | loss: 0.61273 | val_0_rmse: 0.79961 | val_1_rmse: 0.7549  |  0:00:15s
epoch 53 | loss: 0.61411 | val_0_rmse: 0.79533 | val_1_rmse: 0.75769 |  0:00:15s
epoch 54 | loss: 0.61804 | val_0_rmse: 0.79913 | val_1_rmse: 0.75888 |  0:00:15s
epoch 55 | loss: 0.62034 | val_0_rmse: 0.80035 | val_1_rmse: 0.76202 |  0:00:16s
epoch 56 | loss: 0.61173 | val_0_rmse: 0.79341 | val_1_rmse: 0.76237 |  0:00:16s
epoch 57 | loss: 0.61034 | val_0_rmse: 0.78513 | val_1_rmse: 0.75197 |  0:00:16s
epoch 58 | loss: 0.60812 | val_0_rmse: 0.78629 | val_1_rmse: 0.75263 |  0:00:17s
epoch 59 | loss: 0.59327 | val_0_rmse: 0.77914 | val_1_rmse: 0.75307 |  0:00:17s
epoch 60 | loss: 0.59951 | val_0_rmse: 0.77939 | val_1_rmse: 0.75019 |  0:00:17s
epoch 61 | loss: 0.59752 | val_0_rmse: 0.78215 | val_1_rmse: 0.75182 |  0:00:17s
epoch 62 | loss: 0.59541 | val_0_rmse: 0.78598 | val_1_rmse: 0.75456 |  0:00:18s
epoch 63 | loss: 0.58327 | val_0_rmse: 0.79268 | val_1_rmse: 0.76208 |  0:00:18s
epoch 64 | loss: 0.58823 | val_0_rmse: 0.77947 | val_1_rmse: 0.75205 |  0:00:18s
epoch 65 | loss: 0.58953 | val_0_rmse: 0.78384 | val_1_rmse: 0.74945 |  0:00:19s
epoch 66 | loss: 0.58366 | val_0_rmse: 0.77769 | val_1_rmse: 0.75198 |  0:00:19s
epoch 67 | loss: 0.58227 | val_0_rmse: 0.7773  | val_1_rmse: 0.75572 |  0:00:19s
epoch 68 | loss: 0.57198 | val_0_rmse: 0.77821 | val_1_rmse: 0.7537  |  0:00:19s
epoch 69 | loss: 0.57838 | val_0_rmse: 0.77644 | val_1_rmse: 0.75098 |  0:00:20s
epoch 70 | loss: 0.57988 | val_0_rmse: 0.77872 | val_1_rmse: 0.75587 |  0:00:20s
epoch 71 | loss: 0.57196 | val_0_rmse: 0.78521 | val_1_rmse: 0.75909 |  0:00:20s
epoch 72 | loss: 0.55886 | val_0_rmse: 0.78786 | val_1_rmse: 0.75818 |  0:00:20s
epoch 73 | loss: 0.55474 | val_0_rmse: 0.7878  | val_1_rmse: 0.76723 |  0:00:21s
epoch 74 | loss: 0.56053 | val_0_rmse: 0.773   | val_1_rmse: 0.75664 |  0:00:21s
epoch 75 | loss: 0.55674 | val_0_rmse: 0.76753 | val_1_rmse: 0.75841 |  0:00:21s
epoch 76 | loss: 0.55466 | val_0_rmse: 0.77004 | val_1_rmse: 0.76265 |  0:00:22s
epoch 77 | loss: 0.55781 | val_0_rmse: 0.75549 | val_1_rmse: 0.76451 |  0:00:22s
epoch 78 | loss: 0.56618 | val_0_rmse: 0.76357 | val_1_rmse: 0.77357 |  0:00:22s
epoch 79 | loss: 0.56228 | val_0_rmse: 0.7734  | val_1_rmse: 0.76481 |  0:00:22s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.74777
Best weights from best epoch are automatically used!
ended training at: 05:54:15
Feature importance:
Mean squared error is of 0.05733988574057731
Mean absolute error:0.1705016123106644
MAPE:0.19208817843751594
R2 score:0.3434684532750626
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:58:50
epoch 0  | loss: 1.02626 | val_0_rmse: 0.96677 | val_1_rmse: 0.98459 |  0:00:21s
epoch 1  | loss: 0.76004 | val_0_rmse: 0.84905 | val_1_rmse: 0.85296 |  0:00:43s
epoch 2  | loss: 0.63746 | val_0_rmse: 0.80069 | val_1_rmse: 0.8117  |  0:01:04s
epoch 3  | loss: 0.58487 | val_0_rmse: 0.75747 | val_1_rmse: 0.77086 |  0:01:25s
epoch 4  | loss: 0.55952 | val_0_rmse: 0.75134 | val_1_rmse: 0.77232 |  0:01:47s
epoch 5  | loss: 0.54427 | val_0_rmse: 0.71807 | val_1_rmse: 0.7491  |  0:02:08s
epoch 6  | loss: 0.5256  | val_0_rmse: 0.7303  | val_1_rmse: 0.77373 |  0:02:30s
epoch 7  | loss: 0.54984 | val_0_rmse: 0.7159  | val_1_rmse: 0.75126 |  0:02:51s
epoch 8  | loss: 0.51371 | val_0_rmse: 0.69741 | val_1_rmse: 0.7461  |  0:03:13s
epoch 9  | loss: 0.49883 | val_0_rmse: 0.69494 | val_1_rmse: 0.75035 |  0:03:34s
epoch 10 | loss: 0.48582 | val_0_rmse: 0.70139 | val_1_rmse: 0.74986 |  0:03:56s
epoch 11 | loss: 0.47804 | val_0_rmse: 0.69718 | val_1_rmse: 0.76113 |  0:04:18s
epoch 12 | loss: 0.47117 | val_0_rmse: 0.71472 | val_1_rmse: 0.78412 |  0:04:39s
epoch 13 | loss: 0.47513 | val_0_rmse: 0.68637 | val_1_rmse: 0.75695 |  0:05:01s
epoch 14 | loss: 0.46669 | val_0_rmse: 0.69675 | val_1_rmse: 0.75668 |  0:05:22s
epoch 15 | loss: 0.45534 | val_0_rmse: 0.68199 | val_1_rmse: 0.76418 |  0:05:43s
epoch 16 | loss: 0.45076 | val_0_rmse: 0.70883 | val_1_rmse: 0.77493 |  0:06:05s
epoch 17 | loss: 0.44952 | val_0_rmse: 0.81789 | val_1_rmse: 0.86607 |  0:06:26s
epoch 18 | loss: 0.44589 | val_0_rmse: 0.68415 | val_1_rmse: 0.75449 |  0:06:47s
epoch 19 | loss: 0.44072 | val_0_rmse: 0.67605 | val_1_rmse: 0.75197 |  0:07:09s
epoch 20 | loss: 0.43398 | val_0_rmse: 0.67505 | val_1_rmse: 0.74129 |  0:07:30s
epoch 21 | loss: 0.43461 | val_0_rmse: 0.6632  | val_1_rmse: 0.74145 |  0:07:52s
epoch 22 | loss: 0.42725 | val_0_rmse: 0.66127 | val_1_rmse: 0.73964 |  0:08:13s
epoch 23 | loss: 0.42759 | val_0_rmse: 0.75068 | val_1_rmse: 0.81312 |  0:08:34s
epoch 24 | loss: 0.42053 | val_0_rmse: 0.6692  | val_1_rmse: 0.75854 |  0:08:56s
epoch 25 | loss: 0.41728 | val_0_rmse: 0.77773 | val_1_rmse: 0.83911 |  0:09:17s
epoch 26 | loss: 0.41399 | val_0_rmse: 0.66537 | val_1_rmse: 0.75256 |  0:09:38s
epoch 27 | loss: 0.412   | val_0_rmse: 0.67313 | val_1_rmse: 0.75762 |  0:10:00s
epoch 28 | loss: 0.41169 | val_0_rmse: 0.81014 | val_1_rmse: 0.85167 |  0:10:21s
epoch 29 | loss: 0.40873 | val_0_rmse: 0.6993  | val_1_rmse: 0.77539 |  0:10:42s
epoch 30 | loss: 0.40588 | val_0_rmse: 0.84045 | val_1_rmse: 0.88966 |  0:11:04s
epoch 31 | loss: 0.40299 | val_0_rmse: 0.7752  | val_1_rmse: 0.86936 |  0:11:25s
epoch 32 | loss: 0.40095 | val_0_rmse: 0.71209 | val_1_rmse: 0.78975 |  0:11:46s
epoch 33 | loss: 0.40175 | val_0_rmse: 0.6888  | val_1_rmse: 0.77086 |  0:12:08s
epoch 34 | loss: 0.39822 | val_0_rmse: 0.74627 | val_1_rmse: 0.84108 |  0:12:29s
epoch 35 | loss: 0.39695 | val_0_rmse: 0.73214 | val_1_rmse: 0.79123 |  0:12:50s
epoch 36 | loss: 0.41833 | val_0_rmse: 0.67048 | val_1_rmse: 0.77322 |  0:13:12s
epoch 37 | loss: 0.39927 | val_0_rmse: 0.70007 | val_1_rmse: 0.80437 |  0:13:33s
epoch 38 | loss: 0.39328 | val_0_rmse: 0.68608 | val_1_rmse: 0.78988 |  0:13:54s
epoch 39 | loss: 0.38846 | val_0_rmse: 0.69444 | val_1_rmse: 0.77124 |  0:14:16s
epoch 40 | loss: 0.38591 | val_0_rmse: 0.73603 | val_1_rmse: 0.77752 |  0:14:37s
epoch 41 | loss: 0.38565 | val_0_rmse: 0.71427 | val_1_rmse: 0.75675 |  0:14:58s
epoch 42 | loss: 0.38209 | val_0_rmse: 0.65024 | val_1_rmse: 0.76725 |  0:15:20s
epoch 43 | loss: 0.38015 | val_0_rmse: 0.66582 | val_1_rmse: 0.7697  |  0:15:41s
epoch 44 | loss: 0.38168 | val_0_rmse: 0.67992 | val_1_rmse: 0.77668 |  0:16:03s
epoch 45 | loss: 0.37722 | val_0_rmse: 0.66723 | val_1_rmse: 0.76245 |  0:16:24s
epoch 46 | loss: 0.3772  | val_0_rmse: 0.67966 | val_1_rmse: 0.79194 |  0:16:45s
epoch 47 | loss: 0.37597 | val_0_rmse: 0.69008 | val_1_rmse: 0.80018 |  0:17:07s
epoch 48 | loss: 0.37374 | val_0_rmse: 1.04166 | val_1_rmse: 1.2963  |  0:17:28s
epoch 49 | loss: 0.39698 | val_0_rmse: 0.66901 | val_1_rmse: 0.78023 |  0:17:49s
epoch 50 | loss: 0.37914 | val_0_rmse: 0.66359 | val_1_rmse: 0.7971  |  0:18:11s
epoch 51 | loss: 0.37422 | val_0_rmse: 0.64015 | val_1_rmse: 0.76272 |  0:18:32s
epoch 52 | loss: 0.37164 | val_0_rmse: 0.64599 | val_1_rmse: 0.78637 |  0:18:54s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.73964
Best weights from best epoch are automatically used!
ended training at: 06:17:57
Feature importance:
Mean squared error is of 0.06609313781962746
Mean absolute error:0.1748682326229638
MAPE:0.19242610510567662
R2 score:0.4430465146499365
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:18:01
epoch 0  | loss: 0.98002 | val_0_rmse: 0.88398 | val_1_rmse: 0.86941 |  0:00:21s
epoch 1  | loss: 0.69481 | val_0_rmse: 0.87565 | val_1_rmse: 0.86326 |  0:00:43s
epoch 2  | loss: 0.62903 | val_0_rmse: 0.78973 | val_1_rmse: 0.77614 |  0:01:04s
epoch 3  | loss: 0.58909 | val_0_rmse: 0.80219 | val_1_rmse: 0.79667 |  0:01:25s
epoch 4  | loss: 0.57101 | val_0_rmse: 0.75246 | val_1_rmse: 0.75166 |  0:01:47s
epoch 5  | loss: 0.55056 | val_0_rmse: 0.7239  | val_1_rmse: 0.72889 |  0:02:08s
epoch 6  | loss: 0.54357 | val_0_rmse: 0.7298  | val_1_rmse: 0.73626 |  0:02:29s
epoch 7  | loss: 0.53027 | val_0_rmse: 0.70455 | val_1_rmse: 0.72292 |  0:02:51s
epoch 8  | loss: 0.51321 | val_0_rmse: 0.69835 | val_1_rmse: 0.72439 |  0:03:12s
epoch 9  | loss: 0.5001  | val_0_rmse: 0.76916 | val_1_rmse: 0.80339 |  0:03:34s
epoch 10 | loss: 0.49292 | val_0_rmse: 0.70583 | val_1_rmse: 0.73631 |  0:03:55s
epoch 11 | loss: 0.4834  | val_0_rmse: 0.76465 | val_1_rmse: 0.77973 |  0:04:16s
epoch 12 | loss: 0.47913 | val_0_rmse: 0.69088 | val_1_rmse: 0.72652 |  0:04:38s
epoch 13 | loss: 0.47086 | val_0_rmse: 0.70769 | val_1_rmse: 0.76033 |  0:04:59s
epoch 14 | loss: 0.4606  | val_0_rmse: 0.68527 | val_1_rmse: 0.73738 |  0:05:21s
epoch 15 | loss: 0.45428 | val_0_rmse: 0.68871 | val_1_rmse: 0.79617 |  0:05:42s
epoch 16 | loss: 0.45299 | val_0_rmse: 0.69255 | val_1_rmse: 0.75958 |  0:06:04s
epoch 17 | loss: 0.44675 | val_0_rmse: 0.7053  | val_1_rmse: 1.28159 |  0:06:25s
epoch 18 | loss: 0.44323 | val_0_rmse: 0.67547 | val_1_rmse: 0.74183 |  0:06:46s
epoch 19 | loss: 0.43638 | val_0_rmse: 0.67365 | val_1_rmse: 0.81797 |  0:07:08s
epoch 20 | loss: 0.43373 | val_0_rmse: 0.68862 | val_1_rmse: 1.12834 |  0:07:29s
epoch 21 | loss: 0.43108 | val_0_rmse: 0.69929 | val_1_rmse: 1.68393 |  0:07:51s
epoch 22 | loss: 0.42748 | val_0_rmse: 0.6894  | val_1_rmse: 0.92817 |  0:08:12s
epoch 23 | loss: 0.42274 | val_0_rmse: 0.69976 | val_1_rmse: 0.80114 |  0:08:34s
epoch 24 | loss: 0.42162 | val_0_rmse: 0.66749 | val_1_rmse: 0.74017 |  0:08:55s
epoch 25 | loss: 0.42394 | val_0_rmse: 0.78465 | val_1_rmse: 1.13734 |  0:09:16s
epoch 26 | loss: 0.4157  | val_0_rmse: 0.68655 | val_1_rmse: 0.73976 |  0:09:38s
epoch 27 | loss: 0.414   | val_0_rmse: 0.7366  | val_1_rmse: 0.76907 |  0:09:59s
epoch 28 | loss: 0.41166 | val_0_rmse: 0.66394 | val_1_rmse: 0.73022 |  0:10:21s
epoch 29 | loss: 0.40776 | val_0_rmse: 0.66716 | val_1_rmse: 0.75342 |  0:10:42s
epoch 30 | loss: 0.40379 | val_0_rmse: 0.67537 | val_1_rmse: 0.75428 |  0:11:03s
epoch 31 | loss: 0.40292 | val_0_rmse: 0.67076 | val_1_rmse: 0.75473 |  0:11:25s
epoch 32 | loss: 0.40797 | val_0_rmse: 0.70916 | val_1_rmse: 0.79121 |  0:11:46s
epoch 33 | loss: 0.39856 | val_0_rmse: 0.65038 | val_1_rmse: 0.73318 |  0:12:08s
epoch 34 | loss: 0.3965  | val_0_rmse: 0.67846 | val_1_rmse: 0.73705 |  0:12:29s
epoch 35 | loss: 0.39315 | val_0_rmse: 0.6917  | val_1_rmse: 0.76623 |  0:12:50s
epoch 36 | loss: 0.39518 | val_0_rmse: 0.64875 | val_1_rmse: 0.74146 |  0:13:12s
epoch 37 | loss: 0.38722 | val_0_rmse: 0.66702 | val_1_rmse: 0.7445  |  0:13:33s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.72292
Best weights from best epoch are automatically used!
ended training at: 06:31:48
Feature importance:
Mean squared error is of 0.06905713965182983
Mean absolute error:0.17972258301729496
MAPE:0.19634079920483452
R2 score:0.4571130959910775
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:31:52
epoch 0  | loss: 0.99976 | val_0_rmse: 0.89111 | val_1_rmse: 0.88163 |  0:00:21s
epoch 1  | loss: 0.71603 | val_0_rmse: 0.8309  | val_1_rmse: 0.82407 |  0:00:42s
epoch 2  | loss: 0.63584 | val_0_rmse: 0.82937 | val_1_rmse: 0.82408 |  0:01:04s
epoch 3  | loss: 0.59421 | val_0_rmse: 0.76804 | val_1_rmse: 0.77079 |  0:01:25s
epoch 4  | loss: 0.56579 | val_0_rmse: 0.74076 | val_1_rmse: 0.75194 |  0:01:47s
epoch 5  | loss: 0.54525 | val_0_rmse: 0.71549 | val_1_rmse: 0.73643 |  0:02:09s
epoch 6  | loss: 0.53517 | val_0_rmse: 0.73866 | val_1_rmse: 0.76445 |  0:02:30s
epoch 7  | loss: 0.52052 | val_0_rmse: 0.7048  | val_1_rmse: 0.74564 |  0:02:51s
epoch 8  | loss: 0.50909 | val_0_rmse: 0.69867 | val_1_rmse: 0.74479 |  0:03:13s
epoch 9  | loss: 0.50287 | val_0_rmse: 0.69545 | val_1_rmse: 0.74353 |  0:03:34s
epoch 10 | loss: 0.49182 | val_0_rmse: 1.47569 | val_1_rmse: 1.3517  |  0:03:56s
epoch 11 | loss: 0.48972 | val_0_rmse: 0.74769 | val_1_rmse: 0.83076 |  0:04:17s
epoch 12 | loss: 0.47946 | val_0_rmse: 0.71373 | val_1_rmse: 0.75438 |  0:04:38s
epoch 13 | loss: 0.47597 | val_0_rmse: 0.69611 | val_1_rmse: 0.74019 |  0:05:00s
epoch 14 | loss: 0.48328 | val_0_rmse: 0.81205 | val_1_rmse: 0.84196 |  0:05:21s
epoch 15 | loss: 0.46781 | val_0_rmse: 0.7293  | val_1_rmse: 0.77204 |  0:05:42s
epoch 16 | loss: 0.4573  | val_0_rmse: 0.91985 | val_1_rmse: 0.92807 |  0:06:03s
epoch 17 | loss: 0.45139 | val_0_rmse: 0.6754  | val_1_rmse: 0.73853 |  0:06:25s
epoch 18 | loss: 0.4446  | val_0_rmse: 0.67396 | val_1_rmse: 0.7341  |  0:06:46s
epoch 19 | loss: 0.44206 | val_0_rmse: 0.6642  | val_1_rmse: 0.73612 |  0:07:07s
epoch 20 | loss: 0.43563 | val_0_rmse: 0.66912 | val_1_rmse: 0.73789 |  0:07:29s
epoch 21 | loss: 0.43461 | val_0_rmse: 0.66885 | val_1_rmse: 0.73869 |  0:07:50s
epoch 22 | loss: 0.42902 | val_0_rmse: 0.67622 | val_1_rmse: 0.75865 |  0:08:11s
epoch 23 | loss: 0.42484 | val_0_rmse: 0.82443 | val_1_rmse: 0.87906 |  0:08:33s
epoch 24 | loss: 0.4223  | val_0_rmse: 0.65797 | val_1_rmse: 0.73342 |  0:08:54s
epoch 25 | loss: 0.42041 | val_0_rmse: 0.67157 | val_1_rmse: 0.74998 |  0:09:15s
epoch 26 | loss: 0.41798 | val_0_rmse: 0.6648  | val_1_rmse: 0.75769 |  0:09:37s
epoch 27 | loss: 0.41576 | val_0_rmse: 0.70203 | val_1_rmse: 0.77196 |  0:09:58s
epoch 28 | loss: 0.41066 | val_0_rmse: 0.66181 | val_1_rmse: 0.74161 |  0:10:20s
epoch 29 | loss: 0.41109 | val_0_rmse: 0.65634 | val_1_rmse: 0.75035 |  0:10:41s
epoch 30 | loss: 0.40568 | val_0_rmse: 0.66949 | val_1_rmse: 0.75331 |  0:11:02s
epoch 31 | loss: 0.40546 | val_0_rmse: 0.68488 | val_1_rmse: 0.75785 |  0:11:24s
epoch 32 | loss: 0.40057 | val_0_rmse: 0.66463 | val_1_rmse: 0.75933 |  0:11:45s
epoch 33 | loss: 0.39926 | val_0_rmse: 0.69218 | val_1_rmse: 0.77291 |  0:12:07s
epoch 34 | loss: 0.40999 | val_0_rmse: 1.02077 | val_1_rmse: 0.76825 |  0:12:28s
epoch 35 | loss: 0.42332 | val_0_rmse: 0.69956 | val_1_rmse: 0.75152 |  0:12:49s
epoch 36 | loss: 0.40378 | val_0_rmse: 0.66907 | val_1_rmse: 0.75499 |  0:13:11s
epoch 37 | loss: 0.39424 | val_0_rmse: 0.79532 | val_1_rmse: 0.77065 |  0:13:32s
epoch 38 | loss: 0.39135 | val_0_rmse: 0.78338 | val_1_rmse: 0.7644  |  0:13:54s
epoch 39 | loss: 0.38982 | val_0_rmse: 0.78241 | val_1_rmse: 0.74412 |  0:14:15s
epoch 40 | loss: 0.38866 | val_0_rmse: 0.87942 | val_1_rmse: 0.84378 |  0:14:37s
epoch 41 | loss: 0.38272 | val_0_rmse: 0.78241 | val_1_rmse: 0.74565 |  0:14:58s
epoch 42 | loss: 0.38177 | val_0_rmse: 0.71312 | val_1_rmse: 0.77449 |  0:15:20s
epoch 43 | loss: 0.38297 | val_0_rmse: 0.73336 | val_1_rmse: 0.79193 |  0:15:41s
epoch 44 | loss: 0.38057 | val_0_rmse: 0.76009 | val_1_rmse: 0.79772 |  0:16:03s
epoch 45 | loss: 0.37866 | val_0_rmse: 0.69519 | val_1_rmse: 0.77284 |  0:16:24s
epoch 46 | loss: 0.39033 | val_0_rmse: 0.71735 | val_1_rmse: 0.80551 |  0:16:46s
epoch 47 | loss: 0.41495 | val_0_rmse: 0.72369 | val_1_rmse: 0.75464 |  0:17:07s
epoch 48 | loss: 0.38738 | val_0_rmse: 0.66721 | val_1_rmse: 0.76069 |  0:17:29s
epoch 49 | loss: 0.38169 | val_0_rmse: 0.65565 | val_1_rmse: 0.7508  |  0:17:50s
epoch 50 | loss: 0.37193 | val_0_rmse: 0.91115 | val_1_rmse: 0.96044 |  0:18:12s
epoch 51 | loss: 0.37354 | val_0_rmse: 0.63757 | val_1_rmse: 0.75957 |  0:18:33s
epoch 52 | loss: 0.37149 | val_0_rmse: 0.6465  | val_1_rmse: 0.77304 |  0:18:54s
epoch 53 | loss: 0.36781 | val_0_rmse: 0.6388  | val_1_rmse: 0.7538  |  0:19:16s
epoch 54 | loss: 0.36443 | val_0_rmse: 0.64175 | val_1_rmse: 0.75905 |  0:19:37s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.73342
Best weights from best epoch are automatically used!
ended training at: 06:51:43
Feature importance:
Mean squared error is of 0.06628621319794727
Mean absolute error:0.17783584546217004
MAPE:0.19617888760988647
R2 score:0.4601677929876302
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:51:47
epoch 0  | loss: 0.96149 | val_0_rmse: 0.86987 | val_1_rmse: 0.86502 |  0:00:21s
epoch 1  | loss: 0.71503 | val_0_rmse: 0.83965 | val_1_rmse: 0.83557 |  0:00:42s
epoch 2  | loss: 0.66981 | val_0_rmse: 0.81932 | val_1_rmse: 0.8146  |  0:01:03s
epoch 3  | loss: 0.62173 | val_0_rmse: 0.79226 | val_1_rmse: 0.79412 |  0:01:25s
epoch 4  | loss: 0.5994  | val_0_rmse: 0.77407 | val_1_rmse: 0.77167 |  0:01:46s
epoch 5  | loss: 0.58134 | val_0_rmse: 0.77205 | val_1_rmse: 0.76144 |  0:02:07s
epoch 6  | loss: 0.56449 | val_0_rmse: 0.73343 | val_1_rmse: 0.76063 |  0:02:29s
epoch 7  | loss: 0.55161 | val_0_rmse: 0.72777 | val_1_rmse: 0.74992 |  0:02:50s
epoch 8  | loss: 0.53951 | val_0_rmse: 1.04003 | val_1_rmse: 0.79038 |  0:03:11s
epoch 9  | loss: 0.531   | val_0_rmse: 0.71752 | val_1_rmse: 0.7517  |  0:03:33s
epoch 10 | loss: 0.51854 | val_0_rmse: 0.73519 | val_1_rmse: 0.76497 |  0:03:54s
epoch 11 | loss: 0.51206 | val_0_rmse: 0.71605 | val_1_rmse: 0.75741 |  0:04:15s
epoch 12 | loss: 0.49642 | val_0_rmse: 0.70501 | val_1_rmse: 0.74713 |  0:04:37s
epoch 13 | loss: 0.48669 | val_0_rmse: 0.70987 | val_1_rmse: 0.76578 |  0:04:58s
epoch 14 | loss: 0.48286 | val_0_rmse: 0.70696 | val_1_rmse: 0.76317 |  0:05:19s
epoch 15 | loss: 0.47327 | val_0_rmse: 0.70235 | val_1_rmse: 0.75885 |  0:05:41s
epoch 16 | loss: 0.46623 | val_0_rmse: 0.71797 | val_1_rmse: 0.7756  |  0:06:02s
epoch 17 | loss: 0.45801 | val_0_rmse: 0.69418 | val_1_rmse: 0.76687 |  0:06:24s
epoch 18 | loss: 0.45567 | val_0_rmse: 0.68669 | val_1_rmse: 0.7555  |  0:06:45s
epoch 19 | loss: 0.44842 | val_0_rmse: 0.67775 | val_1_rmse: 0.74447 |  0:07:06s
epoch 20 | loss: 0.44798 | val_0_rmse: 0.68362 | val_1_rmse: 0.75278 |  0:07:28s
epoch 21 | loss: 0.44434 | val_0_rmse: 0.68657 | val_1_rmse: 0.7658  |  0:07:49s
epoch 22 | loss: 0.4392  | val_0_rmse: 0.67706 | val_1_rmse: 0.74898 |  0:08:10s
epoch 23 | loss: 0.43435 | val_0_rmse: 0.693   | val_1_rmse: 0.74388 |  0:08:32s
epoch 24 | loss: 0.43297 | val_0_rmse: 0.70766 | val_1_rmse: 0.77344 |  0:08:53s
epoch 25 | loss: 0.42677 | val_0_rmse: 0.67446 | val_1_rmse: 0.74823 |  0:09:14s
epoch 26 | loss: 0.42377 | val_0_rmse: 0.69109 | val_1_rmse: 0.76127 |  0:09:36s
epoch 27 | loss: 0.41801 | val_0_rmse: 0.67133 | val_1_rmse: 0.75969 |  0:09:57s
epoch 28 | loss: 0.41498 | val_0_rmse: 0.6612  | val_1_rmse: 0.7508  |  0:10:18s
epoch 29 | loss: 0.42012 | val_0_rmse: 1.05563 | val_1_rmse: 1.83691 |  0:10:39s
epoch 30 | loss: 0.41113 | val_0_rmse: 0.85133 | val_1_rmse: 1.0373  |  0:11:01s
epoch 31 | loss: 0.4082  | val_0_rmse: 0.65585 | val_1_rmse: 0.74482 |  0:11:22s
epoch 32 | loss: 0.40583 | val_0_rmse: 1.1011  | val_1_rmse: 2.37231 |  0:11:44s
epoch 33 | loss: 0.40529 | val_0_rmse: 0.68376 | val_1_rmse: 0.77241 |  0:12:06s
epoch 34 | loss: 0.40184 | val_0_rmse: 0.66204 | val_1_rmse: 0.75955 |  0:12:27s
epoch 35 | loss: 0.39887 | val_0_rmse: 0.65328 | val_1_rmse: 0.7425  |  0:12:48s
epoch 36 | loss: 0.39615 | val_0_rmse: 0.97572 | val_1_rmse: 1.93108 |  0:13:10s
epoch 37 | loss: 0.39548 | val_0_rmse: 0.65131 | val_1_rmse: 1.01241 |  0:13:31s
epoch 38 | loss: 0.38963 | val_0_rmse: 0.66319 | val_1_rmse: 0.84499 |  0:13:52s
epoch 39 | loss: 0.38924 | val_0_rmse: 0.9519  | val_1_rmse: 2.79685 |  0:14:14s
epoch 40 | loss: 0.38552 | val_0_rmse: 0.65388 | val_1_rmse: 0.758   |  0:14:35s
epoch 41 | loss: 0.3875  | val_0_rmse: 1.00856 | val_1_rmse: 1.90633 |  0:14:56s
epoch 42 | loss: 0.383   | val_0_rmse: 0.63702 | val_1_rmse: 0.75239 |  0:15:18s
epoch 43 | loss: 0.38426 | val_0_rmse: 1.69016 | val_1_rmse: 1.73003 |  0:15:39s
epoch 44 | loss: 0.42997 | val_0_rmse: 0.67236 | val_1_rmse: 0.76612 |  0:16:00s
epoch 45 | loss: 0.39062 | val_0_rmse: 0.65835 | val_1_rmse: 0.76112 |  0:16:22s
epoch 46 | loss: 0.38363 | val_0_rmse: 0.65511 | val_1_rmse: 0.75974 |  0:16:43s
epoch 47 | loss: 0.37876 | val_0_rmse: 0.65198 | val_1_rmse: 0.77215 |  0:17:05s
epoch 48 | loss: 0.37639 | val_0_rmse: 1.77212 | val_1_rmse: 4.41406 |  0:17:26s
epoch 49 | loss: 0.37376 | val_0_rmse: 0.6395  | val_1_rmse: 0.7758  |  0:17:47s
epoch 50 | loss: 0.3699  | val_0_rmse: 0.64392 | val_1_rmse: 0.77046 |  0:18:09s
epoch 51 | loss: 0.36686 | val_0_rmse: 0.65888 | val_1_rmse: 1.16459 |  0:18:30s
epoch 52 | loss: 0.36674 | val_0_rmse: 0.63674 | val_1_rmse: 0.76111 |  0:18:51s
epoch 53 | loss: 0.36731 | val_0_rmse: 0.63764 | val_1_rmse: 0.75957 |  0:19:13s
epoch 54 | loss: 0.36312 | val_0_rmse: 0.62632 | val_1_rmse: 0.76123 |  0:19:34s
epoch 55 | loss: 0.36402 | val_0_rmse: 0.63456 | val_1_rmse: 0.75889 |  0:19:55s
epoch 56 | loss: 0.36382 | val_0_rmse: 0.63475 | val_1_rmse: 0.75756 |  0:20:17s
epoch 57 | loss: 0.3608  | val_0_rmse: 0.64072 | val_1_rmse: 0.75684 |  0:20:38s
epoch 58 | loss: 0.35756 | val_0_rmse: 0.62655 | val_1_rmse: 0.7591  |  0:20:59s
epoch 59 | loss: 0.35835 | val_0_rmse: 0.62748 | val_1_rmse: 0.76183 |  0:21:21s
epoch 60 | loss: 0.35677 | val_0_rmse: 0.64713 | val_1_rmse: 0.77644 |  0:21:42s
epoch 61 | loss: 0.35314 | val_0_rmse: 0.63543 | val_1_rmse: 0.76516 |  0:22:03s
epoch 62 | loss: 0.35124 | val_0_rmse: 0.66602 | val_1_rmse: 0.77712 |  0:22:25s
epoch 63 | loss: 0.35138 | val_0_rmse: 0.65195 | val_1_rmse: 0.7667  |  0:22:46s
epoch 64 | loss: 0.35143 | val_0_rmse: 0.7383  | val_1_rmse: 0.94207 |  0:23:08s
epoch 65 | loss: 0.34945 | val_0_rmse: 0.69577 | val_1_rmse: 0.83881 |  0:23:29s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.7425
Best weights from best epoch are automatically used!
ended training at: 07:15:30
Feature importance:
Mean squared error is of 0.07580821407840362
Mean absolute error:0.17732178794703238
MAPE:0.19349367127965103
R2 score:0.4238919666272287
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:15:34
epoch 0  | loss: 1.00851 | val_0_rmse: 0.95616 | val_1_rmse: 0.93746 |  0:00:21s
epoch 1  | loss: 0.72926 | val_0_rmse: 0.85152 | val_1_rmse: 0.83597 |  0:00:42s
epoch 2  | loss: 0.62664 | val_0_rmse: 0.80454 | val_1_rmse: 0.79771 |  0:01:03s
epoch 3  | loss: 0.5797  | val_0_rmse: 0.78881 | val_1_rmse: 0.78579 |  0:01:25s
epoch 4  | loss: 0.55871 | val_0_rmse: 0.74017 | val_1_rmse: 0.74692 |  0:01:46s
epoch 5  | loss: 0.54867 | val_0_rmse: 0.74943 | val_1_rmse: 0.76875 |  0:02:08s
epoch 6  | loss: 0.53387 | val_0_rmse: 0.70529 | val_1_rmse: 0.72889 |  0:02:29s
epoch 7  | loss: 0.52128 | val_0_rmse: 0.71138 | val_1_rmse: 0.7436  |  0:02:50s
epoch 8  | loss: 0.51039 | val_0_rmse: 0.7069  | val_1_rmse: 0.74037 |  0:03:12s
epoch 9  | loss: 0.49932 | val_0_rmse: 0.72078 | val_1_rmse: 0.75421 |  0:03:33s
epoch 10 | loss: 0.4893  | val_0_rmse: 0.69246 | val_1_rmse: 0.74134 |  0:03:55s
epoch 11 | loss: 0.48215 | val_0_rmse: 0.73882 | val_1_rmse: 0.78708 |  0:04:16s
epoch 12 | loss: 0.49536 | val_0_rmse: 0.71968 | val_1_rmse: 0.75262 |  0:04:38s
epoch 13 | loss: 0.4969  | val_0_rmse: 0.88541 | val_1_rmse: 0.88738 |  0:04:59s
epoch 14 | loss: 0.50404 | val_0_rmse: 0.71355 | val_1_rmse: 0.75667 |  0:05:20s
epoch 15 | loss: 0.48201 | val_0_rmse: 0.70724 | val_1_rmse: 0.76721 |  0:05:41s
epoch 16 | loss: 0.46738 | val_0_rmse: 0.70493 | val_1_rmse: 0.74871 |  0:06:03s
epoch 17 | loss: 0.45989 | val_0_rmse: 0.85152 | val_1_rmse: 0.76563 |  0:06:24s
epoch 18 | loss: 0.45075 | val_0_rmse: 0.72623 | val_1_rmse: 0.76817 |  0:06:46s
epoch 19 | loss: 0.44396 | val_0_rmse: 0.71342 | val_1_rmse: 0.75214 |  0:07:07s
epoch 20 | loss: 0.44008 | val_0_rmse: 0.73627 | val_1_rmse: 0.7487  |  0:07:28s
epoch 21 | loss: 0.43676 | val_0_rmse: 0.76849 | val_1_rmse: 0.77753 |  0:07:49s
epoch 22 | loss: 0.43113 | val_0_rmse: 0.74391 | val_1_rmse: 0.74763 |  0:08:11s
epoch 23 | loss: 0.42514 | val_0_rmse: 0.74042 | val_1_rmse: 0.75144 |  0:08:32s
epoch 24 | loss: 0.42388 | val_0_rmse: 0.71662 | val_1_rmse: 0.76968 |  0:08:53s
epoch 25 | loss: 0.42249 | val_0_rmse: 0.7837  | val_1_rmse: 0.74367 |  0:09:15s
epoch 26 | loss: 0.41791 | val_0_rmse: 0.73852 | val_1_rmse: 0.75234 |  0:09:36s
epoch 27 | loss: 0.41445 | val_0_rmse: 0.7029  | val_1_rmse: 0.77312 |  0:09:57s
epoch 28 | loss: 0.4148  | val_0_rmse: 0.74426 | val_1_rmse: 0.74473 |  0:10:19s
epoch 29 | loss: 0.40951 | val_0_rmse: 0.77845 | val_1_rmse: 0.75477 |  0:10:40s
epoch 30 | loss: 0.40472 | val_0_rmse: 0.71499 | val_1_rmse: 0.80126 |  0:11:02s
epoch 31 | loss: 0.40394 | val_0_rmse: 0.69695 | val_1_rmse: 0.74907 |  0:11:23s
epoch 32 | loss: 0.40232 | val_0_rmse: 0.67194 | val_1_rmse: 0.78048 |  0:11:45s
epoch 33 | loss: 0.40097 | val_0_rmse: 0.6604  | val_1_rmse: 0.74273 |  0:12:06s
epoch 34 | loss: 0.39826 | val_0_rmse: 0.87302 | val_1_rmse: 0.92033 |  0:12:27s
epoch 35 | loss: 0.39654 | val_0_rmse: 0.72778 | val_1_rmse: 0.74407 |  0:12:49s
epoch 36 | loss: 0.39245 | val_0_rmse: 0.65873 | val_1_rmse: 0.76173 |  0:13:10s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.72889
Best weights from best epoch are automatically used!
ended training at: 07:28:57
Feature importance:
Mean squared error is of 0.06556412633818486
Mean absolute error:0.17949190565866732
MAPE:0.19916618234744218
R2 score:0.4558321462638609
------------------------------------------------------------------
