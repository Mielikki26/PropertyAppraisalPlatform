TabNet Logs:

Saving copy of script...
In this script the datasets were geopy augmented.This is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:26:09
epoch 0  | loss: 1.23943 | val_0_rmse: 1.00276 | val_1_rmse: 1.01221 |  0:00:05s
epoch 1  | loss: 0.90524 | val_0_rmse: 0.92997 | val_1_rmse: 0.92805 |  0:00:07s
epoch 2  | loss: 0.8029  | val_0_rmse: 0.8831  | val_1_rmse: 0.88625 |  0:00:09s
epoch 3  | loss: 0.67177 | val_0_rmse: 0.85461 | val_1_rmse: 0.85779 |  0:00:11s
epoch 4  | loss: 0.56121 | val_0_rmse: 0.82204 | val_1_rmse: 0.82261 |  0:00:13s
epoch 5  | loss: 0.49621 | val_0_rmse: 0.81176 | val_1_rmse: 0.81597 |  0:00:15s
epoch 6  | loss: 0.42795 | val_0_rmse: 0.74672 | val_1_rmse: 0.74848 |  0:00:17s
epoch 7  | loss: 0.38679 | val_0_rmse: 0.73726 | val_1_rmse: 0.73435 |  0:00:19s
epoch 8  | loss: 0.36855 | val_0_rmse: 0.70687 | val_1_rmse: 0.70614 |  0:00:21s
epoch 9  | loss: 0.34914 | val_0_rmse: 0.67788 | val_1_rmse: 0.67753 |  0:00:23s
epoch 10 | loss: 0.33278 | val_0_rmse: 0.65906 | val_1_rmse: 0.65903 |  0:00:25s
epoch 11 | loss: 0.35299 | val_0_rmse: 0.71589 | val_1_rmse: 0.7229  |  0:00:27s
epoch 12 | loss: 0.36024 | val_0_rmse: 0.66406 | val_1_rmse: 0.67987 |  0:00:28s
epoch 13 | loss: 0.34083 | val_0_rmse: 0.62837 | val_1_rmse: 0.64655 |  0:00:30s
epoch 14 | loss: 0.31888 | val_0_rmse: 0.64203 | val_1_rmse: 0.6598  |  0:00:32s
epoch 15 | loss: 0.32583 | val_0_rmse: 0.62205 | val_1_rmse: 0.64214 |  0:00:34s
epoch 16 | loss: 0.32696 | val_0_rmse: 0.60182 | val_1_rmse: 0.6311  |  0:00:36s
epoch 17 | loss: 0.32573 | val_0_rmse: 0.60033 | val_1_rmse: 0.62006 |  0:00:38s
epoch 18 | loss: 0.30893 | val_0_rmse: 0.56445 | val_1_rmse: 0.58975 |  0:00:40s
epoch 19 | loss: 0.30297 | val_0_rmse: 0.55571 | val_1_rmse: 0.5826  |  0:00:42s
epoch 20 | loss: 0.29488 | val_0_rmse: 0.54822 | val_1_rmse: 0.5697  |  0:00:44s
epoch 21 | loss: 0.29584 | val_0_rmse: 0.54931 | val_1_rmse: 0.57661 |  0:00:46s
epoch 22 | loss: 0.29069 | val_0_rmse: 0.53409 | val_1_rmse: 0.56664 |  0:00:48s
epoch 23 | loss: 0.29515 | val_0_rmse: 0.54405 | val_1_rmse: 0.56591 |  0:00:49s
epoch 24 | loss: 0.31984 | val_0_rmse: 0.59439 | val_1_rmse: 0.61756 |  0:00:51s
epoch 25 | loss: 0.3293  | val_0_rmse: 0.58651 | val_1_rmse: 0.61064 |  0:00:53s
epoch 26 | loss: 0.31385 | val_0_rmse: 0.52557 | val_1_rmse: 0.55661 |  0:00:55s
epoch 27 | loss: 0.28989 | val_0_rmse: 0.51952 | val_1_rmse: 0.55414 |  0:00:57s
epoch 28 | loss: 0.2897  | val_0_rmse: 0.51596 | val_1_rmse: 0.54989 |  0:00:59s
epoch 29 | loss: 0.28492 | val_0_rmse: 0.51561 | val_1_rmse: 0.54927 |  0:01:01s
epoch 30 | loss: 0.27865 | val_0_rmse: 0.50568 | val_1_rmse: 0.54345 |  0:01:03s
epoch 31 | loss: 0.27948 | val_0_rmse: 0.50492 | val_1_rmse: 0.54244 |  0:01:05s
epoch 32 | loss: 0.27094 | val_0_rmse: 0.5004  | val_1_rmse: 0.53543 |  0:01:07s
epoch 33 | loss: 0.27147 | val_0_rmse: 0.49835 | val_1_rmse: 0.53306 |  0:01:09s
epoch 34 | loss: 0.26748 | val_0_rmse: 0.49969 | val_1_rmse: 0.53688 |  0:01:10s
epoch 35 | loss: 0.26482 | val_0_rmse: 0.49263 | val_1_rmse: 0.53567 |  0:01:12s
epoch 36 | loss: 0.26624 | val_0_rmse: 0.50065 | val_1_rmse: 0.5404  |  0:01:14s
epoch 37 | loss: 0.26707 | val_0_rmse: 0.49081 | val_1_rmse: 0.53091 |  0:01:16s
epoch 38 | loss: 0.25932 | val_0_rmse: 0.48675 | val_1_rmse: 0.5296  |  0:01:18s
epoch 39 | loss: 0.2608  | val_0_rmse: 0.48837 | val_1_rmse: 0.53311 |  0:01:20s
epoch 40 | loss: 0.25681 | val_0_rmse: 0.48388 | val_1_rmse: 0.53103 |  0:01:22s
epoch 41 | loss: 0.26021 | val_0_rmse: 0.4865  | val_1_rmse: 0.5308  |  0:01:24s
epoch 42 | loss: 0.25327 | val_0_rmse: 0.48993 | val_1_rmse: 0.53533 |  0:01:26s
epoch 43 | loss: 0.2587  | val_0_rmse: 0.48158 | val_1_rmse: 0.52774 |  0:01:28s
epoch 44 | loss: 0.25093 | val_0_rmse: 0.48189 | val_1_rmse: 0.52641 |  0:01:29s
epoch 45 | loss: 0.2504  | val_0_rmse: 0.47717 | val_1_rmse: 0.52124 |  0:01:31s
epoch 46 | loss: 0.25327 | val_0_rmse: 0.48637 | val_1_rmse: 0.53641 |  0:01:33s
epoch 47 | loss: 0.25259 | val_0_rmse: 0.47957 | val_1_rmse: 0.53024 |  0:01:35s
epoch 48 | loss: 0.24941 | val_0_rmse: 0.48945 | val_1_rmse: 0.54036 |  0:01:37s
epoch 49 | loss: 0.25888 | val_0_rmse: 0.48331 | val_1_rmse: 0.5285  |  0:01:39s
epoch 50 | loss: 0.25059 | val_0_rmse: 0.48248 | val_1_rmse: 0.53281 |  0:01:41s
epoch 51 | loss: 0.25018 | val_0_rmse: 0.48261 | val_1_rmse: 0.53232 |  0:01:43s
epoch 52 | loss: 0.2498  | val_0_rmse: 0.47462 | val_1_rmse: 0.52552 |  0:01:45s
epoch 53 | loss: 0.24901 | val_0_rmse: 0.47286 | val_1_rmse: 0.52781 |  0:01:47s
epoch 54 | loss: 0.24359 | val_0_rmse: 0.47936 | val_1_rmse: 0.53095 |  0:01:48s
epoch 55 | loss: 0.24439 | val_0_rmse: 0.47642 | val_1_rmse: 0.53208 |  0:01:50s
epoch 56 | loss: 0.24482 | val_0_rmse: 0.46864 | val_1_rmse: 0.5235  |  0:01:52s
epoch 57 | loss: 0.24668 | val_0_rmse: 0.47531 | val_1_rmse: 0.52533 |  0:01:54s
epoch 58 | loss: 0.24069 | val_0_rmse: 0.4683  | val_1_rmse: 0.52257 |  0:01:56s
epoch 59 | loss: 0.23905 | val_0_rmse: 0.47101 | val_1_rmse: 0.52556 |  0:01:58s
epoch 60 | loss: 0.2414  | val_0_rmse: 0.46379 | val_1_rmse: 0.52072 |  0:02:00s
epoch 61 | loss: 0.2354  | val_0_rmse: 0.46605 | val_1_rmse: 0.52117 |  0:02:02s
epoch 62 | loss: 0.23663 | val_0_rmse: 0.46672 | val_1_rmse: 0.52049 |  0:02:04s
epoch 63 | loss: 0.23493 | val_0_rmse: 0.47358 | val_1_rmse: 0.53912 |  0:02:06s
epoch 64 | loss: 0.23427 | val_0_rmse: 0.46156 | val_1_rmse: 0.52582 |  0:02:07s
epoch 65 | loss: 0.23129 | val_0_rmse: 0.46339 | val_1_rmse: 0.52092 |  0:02:09s
epoch 66 | loss: 0.23605 | val_0_rmse: 0.46266 | val_1_rmse: 0.52393 |  0:02:11s
epoch 67 | loss: 0.22944 | val_0_rmse: 0.45965 | val_1_rmse: 0.52439 |  0:02:13s
epoch 68 | loss: 0.23391 | val_0_rmse: 0.46754 | val_1_rmse: 0.52976 |  0:02:15s
epoch 69 | loss: 0.23068 | val_0_rmse: 0.46099 | val_1_rmse: 0.52606 |  0:02:17s
epoch 70 | loss: 0.23465 | val_0_rmse: 0.45451 | val_1_rmse: 0.52047 |  0:02:19s
epoch 71 | loss: 0.22954 | val_0_rmse: 0.4548  | val_1_rmse: 0.52968 |  0:02:21s
epoch 72 | loss: 0.22488 | val_0_rmse: 0.46159 | val_1_rmse: 0.53373 |  0:02:23s
epoch 73 | loss: 0.23126 | val_0_rmse: 0.46261 | val_1_rmse: 0.52387 |  0:02:25s
epoch 74 | loss: 0.23161 | val_0_rmse: 0.4601  | val_1_rmse: 0.53122 |  0:02:26s
epoch 75 | loss: 0.22708 | val_0_rmse: 0.45159 | val_1_rmse: 0.52242 |  0:02:28s
epoch 76 | loss: 0.24336 | val_0_rmse: 0.50151 | val_1_rmse: 0.56514 |  0:02:30s
epoch 77 | loss: 0.24937 | val_0_rmse: 0.47626 | val_1_rmse: 0.54598 |  0:02:32s
epoch 78 | loss: 0.2375  | val_0_rmse: 0.46851 | val_1_rmse: 0.53661 |  0:02:34s
epoch 79 | loss: 0.24023 | val_0_rmse: 0.47718 | val_1_rmse: 0.54525 |  0:02:36s
epoch 80 | loss: 0.24304 | val_0_rmse: 0.4712  | val_1_rmse: 0.53576 |  0:02:38s
epoch 81 | loss: 0.23475 | val_0_rmse: 0.45922 | val_1_rmse: 0.53004 |  0:02:40s
epoch 82 | loss: 0.23062 | val_0_rmse: 0.45661 | val_1_rmse: 0.53037 |  0:02:42s
epoch 83 | loss: 0.22965 | val_0_rmse: 0.46068 | val_1_rmse: 0.53672 |  0:02:43s
epoch 84 | loss: 0.22655 | val_0_rmse: 0.45841 | val_1_rmse: 0.54014 |  0:02:45s
epoch 85 | loss: 0.22426 | val_0_rmse: 0.45692 | val_1_rmse: 0.53263 |  0:02:47s
epoch 86 | loss: 0.22265 | val_0_rmse: 0.46949 | val_1_rmse: 0.55047 |  0:02:49s
epoch 87 | loss: 0.22392 | val_0_rmse: 0.45153 | val_1_rmse: 0.52919 |  0:02:51s
epoch 88 | loss: 0.21884 | val_0_rmse: 0.44895 | val_1_rmse: 0.5282  |  0:02:53s
epoch 89 | loss: 0.21931 | val_0_rmse: 0.44565 | val_1_rmse: 0.53361 |  0:02:55s
epoch 90 | loss: 0.21992 | val_0_rmse: 0.44826 | val_1_rmse: 0.53063 |  0:02:57s
epoch 91 | loss: 0.21751 | val_0_rmse: 0.44659 | val_1_rmse: 0.52937 |  0:02:59s
epoch 92 | loss: 0.21858 | val_0_rmse: 0.45122 | val_1_rmse: 0.53284 |  0:03:01s
epoch 93 | loss: 0.22176 | val_0_rmse: 0.44345 | val_1_rmse: 0.53055 |  0:03:02s
epoch 94 | loss: 0.21521 | val_0_rmse: 0.44311 | val_1_rmse: 0.52831 |  0:03:04s
epoch 95 | loss: 0.21313 | val_0_rmse: 0.44647 | val_1_rmse: 0.53082 |  0:03:06s
epoch 96 | loss: 0.21869 | val_0_rmse: 0.4477  | val_1_rmse: 0.53575 |  0:03:08s
epoch 97 | loss: 0.21904 | val_0_rmse: 0.44316 | val_1_rmse: 0.53185 |  0:03:10s
epoch 98 | loss: 0.22099 | val_0_rmse: 0.45408 | val_1_rmse: 0.53691 |  0:03:12s
epoch 99 | loss: 0.21481 | val_0_rmse: 0.43442 | val_1_rmse: 0.52242 |  0:03:14s
epoch 100| loss: 0.21471 | val_0_rmse: 0.43412 | val_1_rmse: 0.53077 |  0:03:16s

Early stopping occured at epoch 100 with best_epoch = 70 and best_val_1_rmse = 0.52047
Best weights from best epoch are automatically used!
ended training at: 15:29:26
Feature importance:
Mean squared error is of 5200758937.531908
Mean absolute error:48639.32933492939
MAPE:0.14823547702128625
R2 score:0.7565125463605851
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:29:27
epoch 0  | loss: 1.23487 | val_0_rmse: 0.9362  | val_1_rmse: 0.95835 |  0:00:01s
epoch 1  | loss: 0.7968  | val_0_rmse: 0.87647 | val_1_rmse: 0.89595 |  0:00:03s
epoch 2  | loss: 0.67939 | val_0_rmse: 0.82647 | val_1_rmse: 0.84624 |  0:00:05s
epoch 3  | loss: 0.54235 | val_0_rmse: 0.78933 | val_1_rmse: 0.81555 |  0:00:07s
epoch 4  | loss: 0.46137 | val_0_rmse: 0.77351 | val_1_rmse: 0.79681 |  0:00:09s
epoch 5  | loss: 0.42393 | val_0_rmse: 0.78709 | val_1_rmse: 0.81043 |  0:00:11s
epoch 6  | loss: 0.42169 | val_0_rmse: 0.76169 | val_1_rmse: 0.78257 |  0:00:13s
epoch 7  | loss: 0.40674 | val_0_rmse: 0.72811 | val_1_rmse: 0.74875 |  0:00:15s
epoch 8  | loss: 0.36094 | val_0_rmse: 0.69883 | val_1_rmse: 0.71845 |  0:00:17s
epoch 9  | loss: 0.34017 | val_0_rmse: 0.70076 | val_1_rmse: 0.72455 |  0:00:19s
epoch 10 | loss: 0.37432 | val_0_rmse: 0.6655  | val_1_rmse: 0.68865 |  0:00:21s
epoch 11 | loss: 0.3311  | val_0_rmse: 0.64846 | val_1_rmse: 0.67414 |  0:00:23s
epoch 12 | loss: 0.31767 | val_0_rmse: 0.61899 | val_1_rmse: 0.6421  |  0:00:25s
epoch 13 | loss: 0.30712 | val_0_rmse: 0.61224 | val_1_rmse: 0.63602 |  0:00:26s
epoch 14 | loss: 0.30696 | val_0_rmse: 0.59443 | val_1_rmse: 0.62199 |  0:00:28s
epoch 15 | loss: 0.31001 | val_0_rmse: 0.60594 | val_1_rmse: 0.62703 |  0:00:30s
epoch 16 | loss: 0.31297 | val_0_rmse: 0.61208 | val_1_rmse: 0.63505 |  0:00:32s
epoch 17 | loss: 0.32181 | val_0_rmse: 0.60789 | val_1_rmse: 0.63474 |  0:00:34s
epoch 18 | loss: 0.31695 | val_0_rmse: 0.57312 | val_1_rmse: 0.60551 |  0:00:36s
epoch 19 | loss: 0.30572 | val_0_rmse: 0.56259 | val_1_rmse: 0.59437 |  0:00:38s
epoch 20 | loss: 0.29744 | val_0_rmse: 0.54312 | val_1_rmse: 0.57483 |  0:00:40s
epoch 21 | loss: 0.29314 | val_0_rmse: 0.53127 | val_1_rmse: 0.56546 |  0:00:42s
epoch 22 | loss: 0.28351 | val_0_rmse: 0.52271 | val_1_rmse: 0.56081 |  0:00:44s
epoch 23 | loss: 0.28401 | val_0_rmse: 0.52087 | val_1_rmse: 0.55569 |  0:00:46s
epoch 24 | loss: 0.27599 | val_0_rmse: 0.51627 | val_1_rmse: 0.55395 |  0:00:48s
epoch 25 | loss: 0.28026 | val_0_rmse: 0.52449 | val_1_rmse: 0.56377 |  0:00:50s
epoch 26 | loss: 0.2847  | val_0_rmse: 0.51618 | val_1_rmse: 0.55563 |  0:00:52s
epoch 27 | loss: 0.28122 | val_0_rmse: 0.52163 | val_1_rmse: 0.56033 |  0:00:53s
epoch 28 | loss: 0.27627 | val_0_rmse: 0.50571 | val_1_rmse: 0.54957 |  0:00:55s
epoch 29 | loss: 0.26944 | val_0_rmse: 0.49947 | val_1_rmse: 0.54524 |  0:00:57s
epoch 30 | loss: 0.27131 | val_0_rmse: 0.50264 | val_1_rmse: 0.54309 |  0:00:59s
epoch 31 | loss: 0.2706  | val_0_rmse: 0.50708 | val_1_rmse: 0.55022 |  0:01:01s
epoch 32 | loss: 0.26808 | val_0_rmse: 0.50084 | val_1_rmse: 0.54641 |  0:01:03s
epoch 33 | loss: 0.26556 | val_0_rmse: 0.50195 | val_1_rmse: 0.54738 |  0:01:05s
epoch 34 | loss: 0.26354 | val_0_rmse: 0.49862 | val_1_rmse: 0.54381 |  0:01:07s
epoch 35 | loss: 0.25872 | val_0_rmse: 0.49201 | val_1_rmse: 0.53698 |  0:01:09s
epoch 36 | loss: 0.25848 | val_0_rmse: 0.48931 | val_1_rmse: 0.53742 |  0:01:11s
epoch 37 | loss: 0.25618 | val_0_rmse: 0.49028 | val_1_rmse: 0.54073 |  0:01:13s
epoch 38 | loss: 0.25489 | val_0_rmse: 0.49813 | val_1_rmse: 0.54588 |  0:01:15s
epoch 39 | loss: 0.25867 | val_0_rmse: 0.48566 | val_1_rmse: 0.53945 |  0:01:17s
epoch 40 | loss: 0.25247 | val_0_rmse: 0.49294 | val_1_rmse: 0.5452  |  0:01:18s
epoch 41 | loss: 0.25119 | val_0_rmse: 0.49247 | val_1_rmse: 0.54332 |  0:01:20s
epoch 42 | loss: 0.25295 | val_0_rmse: 0.48969 | val_1_rmse: 0.54496 |  0:01:22s
epoch 43 | loss: 0.25228 | val_0_rmse: 0.48533 | val_1_rmse: 0.53795 |  0:01:24s
epoch 44 | loss: 0.26021 | val_0_rmse: 0.48918 | val_1_rmse: 0.54468 |  0:01:26s
epoch 45 | loss: 0.24713 | val_0_rmse: 0.48233 | val_1_rmse: 0.53942 |  0:01:28s
epoch 46 | loss: 0.24988 | val_0_rmse: 0.47963 | val_1_rmse: 0.53082 |  0:01:30s
epoch 47 | loss: 0.24618 | val_0_rmse: 0.47558 | val_1_rmse: 0.53215 |  0:01:32s
epoch 48 | loss: 0.25265 | val_0_rmse: 0.4827  | val_1_rmse: 0.5396  |  0:01:34s
epoch 49 | loss: 0.24973 | val_0_rmse: 0.48472 | val_1_rmse: 0.54541 |  0:01:36s
epoch 50 | loss: 0.23988 | val_0_rmse: 0.47908 | val_1_rmse: 0.53243 |  0:01:38s
epoch 51 | loss: 0.24062 | val_0_rmse: 0.47845 | val_1_rmse: 0.53724 |  0:01:40s
epoch 52 | loss: 0.23946 | val_0_rmse: 0.4716  | val_1_rmse: 0.5344  |  0:01:42s
epoch 53 | loss: 0.236   | val_0_rmse: 0.46478 | val_1_rmse: 0.52957 |  0:01:43s
epoch 54 | loss: 0.23466 | val_0_rmse: 0.4722  | val_1_rmse: 0.53416 |  0:01:45s
epoch 55 | loss: 0.23549 | val_0_rmse: 0.46804 | val_1_rmse: 0.53317 |  0:01:47s
epoch 56 | loss: 0.24329 | val_0_rmse: 0.48027 | val_1_rmse: 0.54666 |  0:01:49s
epoch 57 | loss: 0.23699 | val_0_rmse: 0.47729 | val_1_rmse: 0.55329 |  0:01:51s
epoch 58 | loss: 0.23146 | val_0_rmse: 0.47834 | val_1_rmse: 0.55712 |  0:01:53s
epoch 59 | loss: 0.23153 | val_0_rmse: 0.47632 | val_1_rmse: 0.55353 |  0:01:55s
epoch 60 | loss: 0.22673 | val_0_rmse: 0.46376 | val_1_rmse: 0.53308 |  0:01:57s
epoch 61 | loss: 0.23366 | val_0_rmse: 0.48368 | val_1_rmse: 0.58918 |  0:01:59s
epoch 62 | loss: 0.22887 | val_0_rmse: 0.47066 | val_1_rmse: 0.55169 |  0:02:01s
epoch 63 | loss: 0.22805 | val_0_rmse: 0.47284 | val_1_rmse: 0.55791 |  0:02:03s
epoch 64 | loss: 0.22927 | val_0_rmse: 0.48126 | val_1_rmse: 0.61336 |  0:02:05s
epoch 65 | loss: 0.22715 | val_0_rmse: 0.47548 | val_1_rmse: 0.55922 |  0:02:07s
epoch 66 | loss: 0.22964 | val_0_rmse: 0.4573  | val_1_rmse: 0.54231 |  0:02:09s
epoch 67 | loss: 0.23025 | val_0_rmse: 0.47715 | val_1_rmse: 0.54757 |  0:02:10s
epoch 68 | loss: 0.24113 | val_0_rmse: 0.46209 | val_1_rmse: 0.53253 |  0:02:12s
epoch 69 | loss: 0.22962 | val_0_rmse: 0.55804 | val_1_rmse: 0.69958 |  0:02:14s
epoch 70 | loss: 0.23026 | val_0_rmse: 0.53032 | val_1_rmse: 0.65551 |  0:02:16s
epoch 71 | loss: 0.22654 | val_0_rmse: 0.46481 | val_1_rmse: 0.54076 |  0:02:18s
epoch 72 | loss: 0.23794 | val_0_rmse: 0.48119 | val_1_rmse: 0.54765 |  0:02:20s
epoch 73 | loss: 0.23214 | val_0_rmse: 0.47229 | val_1_rmse: 0.55995 |  0:02:22s
epoch 74 | loss: 0.22743 | val_0_rmse: 0.48016 | val_1_rmse: 0.54338 |  0:02:24s
epoch 75 | loss: 0.22617 | val_0_rmse: 0.46048 | val_1_rmse: 0.54618 |  0:02:26s
epoch 76 | loss: 0.2245  | val_0_rmse: 0.46795 | val_1_rmse: 0.54342 |  0:02:28s
epoch 77 | loss: 0.22777 | val_0_rmse: 0.51097 | val_1_rmse: 0.57193 |  0:02:30s
epoch 78 | loss: 0.22106 | val_0_rmse: 0.45539 | val_1_rmse: 0.53722 |  0:02:32s
epoch 79 | loss: 0.21828 | val_0_rmse: 0.45704 | val_1_rmse: 0.53228 |  0:02:34s
epoch 80 | loss: 0.2211  | val_0_rmse: 0.45947 | val_1_rmse: 0.52767 |  0:02:36s
epoch 81 | loss: 0.21868 | val_0_rmse: 0.47005 | val_1_rmse: 0.55201 |  0:02:37s
epoch 82 | loss: 0.22129 | val_0_rmse: 0.45948 | val_1_rmse: 0.53244 |  0:02:39s
epoch 83 | loss: 0.22302 | val_0_rmse: 0.44658 | val_1_rmse: 0.52889 |  0:02:41s
epoch 84 | loss: 0.21721 | val_0_rmse: 0.44532 | val_1_rmse: 0.52218 |  0:02:43s
epoch 85 | loss: 0.21698 | val_0_rmse: 0.45207 | val_1_rmse: 0.53341 |  0:02:45s
epoch 86 | loss: 0.21285 | val_0_rmse: 0.44559 | val_1_rmse: 0.5404  |  0:02:47s
epoch 87 | loss: 0.21232 | val_0_rmse: 0.44139 | val_1_rmse: 0.525   |  0:02:49s
epoch 88 | loss: 0.20821 | val_0_rmse: 0.44352 | val_1_rmse: 0.53267 |  0:02:51s
epoch 89 | loss: 0.21124 | val_0_rmse: 0.4365  | val_1_rmse: 0.53156 |  0:02:53s
epoch 90 | loss: 0.20651 | val_0_rmse: 0.44174 | val_1_rmse: 0.53201 |  0:02:55s
epoch 91 | loss: 0.20718 | val_0_rmse: 0.43376 | val_1_rmse: 0.52825 |  0:02:57s
epoch 92 | loss: 0.20414 | val_0_rmse: 0.4369  | val_1_rmse: 0.5313  |  0:02:59s
epoch 93 | loss: 0.20431 | val_0_rmse: 0.43598 | val_1_rmse: 0.53406 |  0:03:01s
epoch 94 | loss: 0.20239 | val_0_rmse: 0.4293  | val_1_rmse: 0.52683 |  0:03:03s
epoch 95 | loss: 0.20146 | val_0_rmse: 0.42601 | val_1_rmse: 0.53422 |  0:03:05s
epoch 96 | loss: 0.19992 | val_0_rmse: 0.44786 | val_1_rmse: 0.52628 |  0:03:06s
epoch 97 | loss: 0.20185 | val_0_rmse: 0.43526 | val_1_rmse: 0.52626 |  0:03:08s
epoch 98 | loss: 0.20089 | val_0_rmse: 0.42253 | val_1_rmse: 0.52634 |  0:03:10s
epoch 99 | loss: 0.20438 | val_0_rmse: 0.42545 | val_1_rmse: 0.54413 |  0:03:12s
epoch 100| loss: 0.2055  | val_0_rmse: 0.42818 | val_1_rmse: 0.53585 |  0:03:14s
epoch 101| loss: 0.20226 | val_0_rmse: 0.42948 | val_1_rmse: 0.54439 |  0:03:16s
epoch 102| loss: 0.20057 | val_0_rmse: 0.42724 | val_1_rmse: 0.53868 |  0:03:18s
epoch 103| loss: 0.2002  | val_0_rmse: 0.42811 | val_1_rmse: 0.52935 |  0:03:20s
epoch 104| loss: 0.20515 | val_0_rmse: 0.44287 | val_1_rmse: 0.58358 |  0:03:22s
epoch 105| loss: 0.20128 | val_0_rmse: 0.42684 | val_1_rmse: 0.52701 |  0:03:24s
epoch 106| loss: 0.20034 | val_0_rmse: 0.42398 | val_1_rmse: 0.53131 |  0:03:26s
epoch 107| loss: 0.19869 | val_0_rmse: 0.4261  | val_1_rmse: 0.54231 |  0:03:28s
epoch 108| loss: 0.19452 | val_0_rmse: 0.42001 | val_1_rmse: 0.53301 |  0:03:30s
epoch 109| loss: 0.19318 | val_0_rmse: 0.41314 | val_1_rmse: 0.5308  |  0:03:31s
epoch 110| loss: 0.1913  | val_0_rmse: 0.42204 | val_1_rmse: 0.53595 |  0:03:33s
epoch 111| loss: 0.19436 | val_0_rmse: 0.41764 | val_1_rmse: 0.53043 |  0:03:35s
epoch 112| loss: 0.18809 | val_0_rmse: 0.42143 | val_1_rmse: 0.54353 |  0:03:37s
epoch 113| loss: 0.19262 | val_0_rmse: 0.42369 | val_1_rmse: 0.54535 |  0:03:39s
epoch 114| loss: 0.19298 | val_0_rmse: 0.41637 | val_1_rmse: 0.53631 |  0:03:41s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.52218
Best weights from best epoch are automatically used!
ended training at: 15:33:10
Feature importance:
Mean squared error is of 5515031349.611661
Mean absolute error:49511.01721118731
MAPE:0.15740658334620414
R2 score:0.7590100999115539
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:33:10
epoch 0  | loss: 1.23933 | val_0_rmse: 0.99864 | val_1_rmse: 1.00022 |  0:00:01s
epoch 1  | loss: 0.94063 | val_0_rmse: 0.89032 | val_1_rmse: 0.89706 |  0:00:03s
epoch 2  | loss: 0.71852 | val_0_rmse: 0.88585 | val_1_rmse: 0.89812 |  0:00:05s
epoch 3  | loss: 0.5368  | val_0_rmse: 0.80199 | val_1_rmse: 0.81559 |  0:00:07s
epoch 4  | loss: 0.42558 | val_0_rmse: 0.75943 | val_1_rmse: 0.77694 |  0:00:09s
epoch 5  | loss: 0.37286 | val_0_rmse: 0.75302 | val_1_rmse: 0.77066 |  0:00:11s
epoch 6  | loss: 0.33498 | val_0_rmse: 0.71384 | val_1_rmse: 0.73232 |  0:00:13s
epoch 7  | loss: 0.31399 | val_0_rmse: 0.68933 | val_1_rmse: 0.70924 |  0:00:15s
epoch 8  | loss: 0.30322 | val_0_rmse: 0.67261 | val_1_rmse: 0.69308 |  0:00:17s
epoch 9  | loss: 0.28599 | val_0_rmse: 0.65771 | val_1_rmse: 0.67993 |  0:00:19s
epoch 10 | loss: 0.28701 | val_0_rmse: 0.64904 | val_1_rmse: 0.67271 |  0:00:21s
epoch 11 | loss: 0.27203 | val_0_rmse: 0.64398 | val_1_rmse: 0.66861 |  0:00:23s
epoch 12 | loss: 0.27169 | val_0_rmse: 0.63255 | val_1_rmse: 0.65902 |  0:00:25s
epoch 13 | loss: 0.27304 | val_0_rmse: 0.59537 | val_1_rmse: 0.62054 |  0:00:27s
epoch 14 | loss: 0.27574 | val_0_rmse: 0.61521 | val_1_rmse: 0.64149 |  0:00:29s
epoch 15 | loss: 0.2602  | val_0_rmse: 0.57538 | val_1_rmse: 0.60232 |  0:00:31s
epoch 16 | loss: 0.25726 | val_0_rmse: 0.56766 | val_1_rmse: 0.59904 |  0:00:33s
epoch 17 | loss: 0.25316 | val_0_rmse: 0.55979 | val_1_rmse: 0.59285 |  0:00:35s
epoch 18 | loss: 0.24742 | val_0_rmse: 0.53529 | val_1_rmse: 0.56831 |  0:00:37s
epoch 19 | loss: 0.24728 | val_0_rmse: 0.53419 | val_1_rmse: 0.56981 |  0:00:38s
epoch 20 | loss: 0.24524 | val_0_rmse: 0.52719 | val_1_rmse: 0.56357 |  0:00:40s
epoch 21 | loss: 0.23711 | val_0_rmse: 0.50093 | val_1_rmse: 0.54008 |  0:00:42s
epoch 22 | loss: 0.23525 | val_0_rmse: 0.49459 | val_1_rmse: 0.53745 |  0:00:44s
epoch 23 | loss: 0.24443 | val_0_rmse: 0.49889 | val_1_rmse: 0.53859 |  0:00:46s
epoch 24 | loss: 0.24181 | val_0_rmse: 0.48342 | val_1_rmse: 0.52623 |  0:00:48s
epoch 25 | loss: 0.23735 | val_0_rmse: 0.47203 | val_1_rmse: 0.51831 |  0:00:50s
epoch 26 | loss: 0.23901 | val_0_rmse: 0.47342 | val_1_rmse: 0.5153  |  0:00:52s
epoch 27 | loss: 0.22701 | val_0_rmse: 0.46195 | val_1_rmse: 0.51125 |  0:00:54s
epoch 28 | loss: 0.2305  | val_0_rmse: 0.45652 | val_1_rmse: 0.50949 |  0:00:56s
epoch 29 | loss: 0.22612 | val_0_rmse: 0.4531  | val_1_rmse: 0.50917 |  0:00:58s
epoch 30 | loss: 0.23317 | val_0_rmse: 0.45744 | val_1_rmse: 0.50822 |  0:01:00s
epoch 31 | loss: 0.22716 | val_0_rmse: 0.45351 | val_1_rmse: 0.50997 |  0:01:02s
epoch 32 | loss: 0.23105 | val_0_rmse: 0.45944 | val_1_rmse: 0.51684 |  0:01:04s
epoch 33 | loss: 0.22378 | val_0_rmse: 0.44514 | val_1_rmse: 0.50377 |  0:01:06s
epoch 34 | loss: 0.2247  | val_0_rmse: 0.45152 | val_1_rmse: 0.50724 |  0:01:08s
epoch 35 | loss: 0.23341 | val_0_rmse: 0.44594 | val_1_rmse: 0.50239 |  0:01:10s
epoch 36 | loss: 0.23321 | val_0_rmse: 0.45417 | val_1_rmse: 0.50636 |  0:01:12s
epoch 37 | loss: 0.22222 | val_0_rmse: 0.44252 | val_1_rmse: 0.50365 |  0:01:14s
epoch 38 | loss: 0.22178 | val_0_rmse: 0.44535 | val_1_rmse: 0.51025 |  0:01:15s
epoch 39 | loss: 0.22168 | val_0_rmse: 0.44512 | val_1_rmse: 0.50432 |  0:01:17s
epoch 40 | loss: 0.22907 | val_0_rmse: 0.44597 | val_1_rmse: 0.5104  |  0:01:19s
epoch 41 | loss: 0.22289 | val_0_rmse: 0.44101 | val_1_rmse: 0.51409 |  0:01:21s
epoch 42 | loss: 0.22095 | val_0_rmse: 0.43892 | val_1_rmse: 0.50801 |  0:01:23s
epoch 43 | loss: 0.21556 | val_0_rmse: 0.44113 | val_1_rmse: 0.50734 |  0:01:25s
epoch 44 | loss: 0.21805 | val_0_rmse: 0.43773 | val_1_rmse: 0.50389 |  0:01:27s
epoch 45 | loss: 0.21633 | val_0_rmse: 0.44193 | val_1_rmse: 0.51216 |  0:01:29s
epoch 46 | loss: 0.21344 | val_0_rmse: 0.43411 | val_1_rmse: 0.50277 |  0:01:31s
epoch 47 | loss: 0.21281 | val_0_rmse: 0.44071 | val_1_rmse: 0.50619 |  0:01:33s
epoch 48 | loss: 0.21262 | val_0_rmse: 0.43245 | val_1_rmse: 0.50547 |  0:01:35s
epoch 49 | loss: 0.21135 | val_0_rmse: 0.43384 | val_1_rmse: 0.50436 |  0:01:37s
epoch 50 | loss: 0.21208 | val_0_rmse: 0.4445  | val_1_rmse: 0.51495 |  0:01:38s
epoch 51 | loss: 0.20709 | val_0_rmse: 0.4417  | val_1_rmse: 0.51398 |  0:01:40s
epoch 52 | loss: 0.22001 | val_0_rmse: 0.45542 | val_1_rmse: 0.53116 |  0:01:42s
epoch 53 | loss: 0.21367 | val_0_rmse: 0.43461 | val_1_rmse: 0.50593 |  0:01:44s
epoch 54 | loss: 0.21068 | val_0_rmse: 0.49818 | val_1_rmse: 0.5617  |  0:01:46s
epoch 55 | loss: 0.20825 | val_0_rmse: 0.43039 | val_1_rmse: 0.51085 |  0:01:48s
epoch 56 | loss: 0.2095  | val_0_rmse: 0.43657 | val_1_rmse: 0.51507 |  0:01:50s
epoch 57 | loss: 0.20658 | val_0_rmse: 0.43038 | val_1_rmse: 0.5094  |  0:01:52s
epoch 58 | loss: 0.21399 | val_0_rmse: 0.447   | val_1_rmse: 0.52334 |  0:01:54s
epoch 59 | loss: 0.21114 | val_0_rmse: 0.43095 | val_1_rmse: 0.5171  |  0:01:56s
epoch 60 | loss: 0.20693 | val_0_rmse: 0.42558 | val_1_rmse: 0.50393 |  0:01:58s
epoch 61 | loss: 0.20439 | val_0_rmse: 0.42509 | val_1_rmse: 0.51094 |  0:01:59s
epoch 62 | loss: 0.2012  | val_0_rmse: 0.42029 | val_1_rmse: 0.50924 |  0:02:01s
epoch 63 | loss: 0.19962 | val_0_rmse: 0.42462 | val_1_rmse: 0.5113  |  0:02:03s
epoch 64 | loss: 0.20141 | val_0_rmse: 0.41708 | val_1_rmse: 0.50498 |  0:02:05s
epoch 65 | loss: 0.20326 | val_0_rmse: 0.42478 | val_1_rmse: 0.51273 |  0:02:07s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.50239
Best weights from best epoch are automatically used!
ended training at: 15:35:19
Feature importance:
Mean squared error is of 5748115512.364848
Mean absolute error:50754.35184715347
MAPE:0.16825713795608954
R2 score:0.7472479540355699
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:35:20
epoch 0  | loss: 1.20063 | val_0_rmse: 1.00257 | val_1_rmse: 1.00297 |  0:00:01s
epoch 1  | loss: 0.87268 | val_0_rmse: 0.93778 | val_1_rmse: 0.9456  |  0:00:03s
epoch 2  | loss: 0.65529 | val_0_rmse: 0.93666 | val_1_rmse: 0.94594 |  0:00:05s
epoch 3  | loss: 0.53024 | val_0_rmse: 0.84052 | val_1_rmse: 0.85084 |  0:00:07s
epoch 4  | loss: 0.45769 | val_0_rmse: 0.78768 | val_1_rmse: 0.79955 |  0:00:09s
epoch 5  | loss: 0.39596 | val_0_rmse: 0.78479 | val_1_rmse: 0.79634 |  0:00:11s
epoch 6  | loss: 0.35049 | val_0_rmse: 0.7544  | val_1_rmse: 0.76342 |  0:00:13s
epoch 7  | loss: 0.31921 | val_0_rmse: 0.71616 | val_1_rmse: 0.72817 |  0:00:15s
epoch 8  | loss: 0.31123 | val_0_rmse: 0.69519 | val_1_rmse: 0.70661 |  0:00:17s
epoch 9  | loss: 0.3019  | val_0_rmse: 0.69661 | val_1_rmse: 0.71177 |  0:00:19s
epoch 10 | loss: 0.29324 | val_0_rmse: 0.68517 | val_1_rmse: 0.70812 |  0:00:21s
epoch 11 | loss: 0.29175 | val_0_rmse: 0.65451 | val_1_rmse: 0.67431 |  0:00:23s
epoch 12 | loss: 0.29045 | val_0_rmse: 0.62363 | val_1_rmse: 0.63284 |  0:00:25s
epoch 13 | loss: 0.2929  | val_0_rmse: 0.62088 | val_1_rmse: 0.63424 |  0:00:27s
epoch 14 | loss: 0.27341 | val_0_rmse: 0.59354 | val_1_rmse: 0.61002 |  0:00:29s
epoch 15 | loss: 0.27391 | val_0_rmse: 0.58752 | val_1_rmse: 0.61118 |  0:00:31s
epoch 16 | loss: 0.26619 | val_0_rmse: 0.56353 | val_1_rmse: 0.58417 |  0:00:32s
epoch 17 | loss: 0.25574 | val_0_rmse: 0.55402 | val_1_rmse: 0.57201 |  0:00:34s
epoch 18 | loss: 0.25549 | val_0_rmse: 0.56495 | val_1_rmse: 0.59478 |  0:00:36s
epoch 19 | loss: 0.25244 | val_0_rmse: 0.55256 | val_1_rmse: 0.5754  |  0:00:38s
epoch 20 | loss: 0.25626 | val_0_rmse: 0.52542 | val_1_rmse: 0.55135 |  0:00:40s
epoch 21 | loss: 0.24912 | val_0_rmse: 0.51752 | val_1_rmse: 0.54373 |  0:00:42s
epoch 22 | loss: 0.24616 | val_0_rmse: 0.50632 | val_1_rmse: 0.53972 |  0:00:44s
epoch 23 | loss: 0.24537 | val_0_rmse: 0.50371 | val_1_rmse: 0.53676 |  0:00:46s
epoch 24 | loss: 0.23796 | val_0_rmse: 0.4871  | val_1_rmse: 0.52782 |  0:00:48s
epoch 25 | loss: 0.23848 | val_0_rmse: 0.47583 | val_1_rmse: 0.5173  |  0:00:50s
epoch 26 | loss: 0.23968 | val_0_rmse: 0.47338 | val_1_rmse: 0.50974 |  0:00:52s
epoch 27 | loss: 0.23662 | val_0_rmse: 0.47105 | val_1_rmse: 0.50833 |  0:00:54s
epoch 28 | loss: 0.23926 | val_0_rmse: 0.46978 | val_1_rmse: 0.5076  |  0:00:56s
epoch 29 | loss: 0.2358  | val_0_rmse: 0.4975  | val_1_rmse: 0.54389 |  0:00:58s
epoch 30 | loss: 0.23409 | val_0_rmse: 0.46624 | val_1_rmse: 0.50818 |  0:01:00s
epoch 31 | loss: 0.23242 | val_0_rmse: 0.45814 | val_1_rmse: 0.50532 |  0:01:02s
epoch 32 | loss: 0.22805 | val_0_rmse: 0.47975 | val_1_rmse: 0.51961 |  0:01:04s
epoch 33 | loss: 0.22699 | val_0_rmse: 0.44935 | val_1_rmse: 0.49535 |  0:01:05s
epoch 34 | loss: 0.22721 | val_0_rmse: 0.46544 | val_1_rmse: 0.51837 |  0:01:07s
epoch 35 | loss: 0.2276  | val_0_rmse: 0.45442 | val_1_rmse: 0.5047  |  0:01:09s
epoch 36 | loss: 0.23308 | val_0_rmse: 0.45203 | val_1_rmse: 0.50583 |  0:01:11s
epoch 37 | loss: 0.22646 | val_0_rmse: 0.44619 | val_1_rmse: 0.50815 |  0:01:13s
epoch 38 | loss: 0.22748 | val_0_rmse: 0.44027 | val_1_rmse: 0.49794 |  0:01:15s
epoch 39 | loss: 0.22222 | val_0_rmse: 0.45577 | val_1_rmse: 0.52333 |  0:01:17s
epoch 40 | loss: 0.22188 | val_0_rmse: 0.45532 | val_1_rmse: 0.51175 |  0:01:19s
epoch 41 | loss: 0.21933 | val_0_rmse: 0.43986 | val_1_rmse: 0.50674 |  0:01:21s
epoch 42 | loss: 0.21659 | val_0_rmse: 0.43738 | val_1_rmse: 0.50422 |  0:01:23s
epoch 43 | loss: 0.21934 | val_0_rmse: 0.48442 | val_1_rmse: 0.53709 |  0:01:25s
epoch 44 | loss: 0.22223 | val_0_rmse: 0.44192 | val_1_rmse: 0.50822 |  0:01:27s
epoch 45 | loss: 0.21754 | val_0_rmse: 0.43877 | val_1_rmse: 0.50637 |  0:01:29s
epoch 46 | loss: 0.21892 | val_0_rmse: 0.44402 | val_1_rmse: 0.51485 |  0:01:31s
epoch 47 | loss: 0.21972 | val_0_rmse: 0.43671 | val_1_rmse: 0.50851 |  0:01:33s
epoch 48 | loss: 0.21216 | val_0_rmse: 0.46711 | val_1_rmse: 0.5255  |  0:01:35s
epoch 49 | loss: 0.21532 | val_0_rmse: 0.43678 | val_1_rmse: 0.50861 |  0:01:36s
epoch 50 | loss: 0.21676 | val_0_rmse: 0.47911 | val_1_rmse: 0.54581 |  0:01:38s
epoch 51 | loss: 0.21663 | val_0_rmse: 0.43775 | val_1_rmse: 0.50642 |  0:01:40s
epoch 52 | loss: 0.20989 | val_0_rmse: 0.44333 | val_1_rmse: 0.52311 |  0:01:42s
epoch 53 | loss: 0.21832 | val_0_rmse: 0.44376 | val_1_rmse: 0.51342 |  0:01:44s
epoch 54 | loss: 0.21485 | val_0_rmse: 0.4387  | val_1_rmse: 0.52127 |  0:01:46s
epoch 55 | loss: 0.21055 | val_0_rmse: 0.44096 | val_1_rmse: 0.51249 |  0:01:48s
epoch 56 | loss: 0.2076  | val_0_rmse: 0.43084 | val_1_rmse: 0.51184 |  0:01:50s
epoch 57 | loss: 0.20611 | val_0_rmse: 0.42775 | val_1_rmse: 0.5039  |  0:01:52s
epoch 58 | loss: 0.20721 | val_0_rmse: 0.43986 | val_1_rmse: 0.52198 |  0:01:54s
epoch 59 | loss: 0.20195 | val_0_rmse: 0.4304  | val_1_rmse: 0.50999 |  0:01:56s
epoch 60 | loss: 0.20175 | val_0_rmse: 0.42385 | val_1_rmse: 0.5082  |  0:01:58s
epoch 61 | loss: 0.20201 | val_0_rmse: 0.45975 | val_1_rmse: 0.54275 |  0:02:00s
epoch 62 | loss: 0.2048  | val_0_rmse: 0.43463 | val_1_rmse: 0.50882 |  0:02:02s
epoch 63 | loss: 0.21427 | val_0_rmse: 0.50621 | val_1_rmse: 0.58984 |  0:02:04s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.49535
Best weights from best epoch are automatically used!
ended training at: 15:37:24
Feature importance:
Mean squared error is of 5484442727.493039
Mean absolute error:49019.99219196357
MAPE:0.15676753150619485
R2 score:0.7515008830501639
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:37:25
epoch 0  | loss: 1.19599 | val_0_rmse: 1.00394 | val_1_rmse: 1.00578 |  0:00:01s
epoch 1  | loss: 0.92709 | val_0_rmse: 0.94018 | val_1_rmse: 0.93667 |  0:00:03s
epoch 2  | loss: 0.77366 | val_0_rmse: 0.89696 | val_1_rmse: 0.8996  |  0:00:05s
epoch 3  | loss: 0.58318 | val_0_rmse: 0.87735 | val_1_rmse: 0.87784 |  0:00:07s
epoch 4  | loss: 0.46752 | val_0_rmse: 0.80528 | val_1_rmse: 0.80513 |  0:00:09s
epoch 5  | loss: 0.40285 | val_0_rmse: 0.79574 | val_1_rmse: 0.79212 |  0:00:11s
epoch 6  | loss: 0.36175 | val_0_rmse: 0.75155 | val_1_rmse: 0.74533 |  0:00:13s
epoch 7  | loss: 0.34478 | val_0_rmse: 0.79831 | val_1_rmse: 0.79202 |  0:00:15s
epoch 8  | loss: 0.31562 | val_0_rmse: 0.72011 | val_1_rmse: 0.71323 |  0:00:17s
epoch 9  | loss: 0.30605 | val_0_rmse: 0.6967  | val_1_rmse: 0.68824 |  0:00:19s
epoch 10 | loss: 0.29098 | val_0_rmse: 0.67603 | val_1_rmse: 0.66722 |  0:00:21s
epoch 11 | loss: 0.31033 | val_0_rmse: 0.68866 | val_1_rmse: 0.6791  |  0:00:23s
epoch 12 | loss: 0.30434 | val_0_rmse: 0.65241 | val_1_rmse: 0.64644 |  0:00:25s
epoch 13 | loss: 0.29673 | val_0_rmse: 0.63359 | val_1_rmse: 0.62532 |  0:00:27s
epoch 14 | loss: 0.28035 | val_0_rmse: 0.61711 | val_1_rmse: 0.60735 |  0:00:29s
epoch 15 | loss: 0.27778 | val_0_rmse: 0.60003 | val_1_rmse: 0.59114 |  0:00:31s
epoch 16 | loss: 0.26783 | val_0_rmse: 0.58807 | val_1_rmse: 0.58045 |  0:00:33s
epoch 17 | loss: 0.26358 | val_0_rmse: 0.57031 | val_1_rmse: 0.56584 |  0:00:35s
epoch 18 | loss: 0.2764  | val_0_rmse: 0.55041 | val_1_rmse: 0.54902 |  0:00:36s
epoch 19 | loss: 0.26095 | val_0_rmse: 0.53305 | val_1_rmse: 0.53326 |  0:00:38s
epoch 20 | loss: 0.26367 | val_0_rmse: 0.54083 | val_1_rmse: 0.54965 |  0:00:40s
epoch 21 | loss: 0.2517  | val_0_rmse: 0.52184 | val_1_rmse: 0.53034 |  0:00:42s
epoch 22 | loss: 0.25487 | val_0_rmse: 0.50563 | val_1_rmse: 0.52115 |  0:00:44s
epoch 23 | loss: 0.25687 | val_0_rmse: 0.49569 | val_1_rmse: 0.50927 |  0:00:46s
epoch 24 | loss: 0.2532  | val_0_rmse: 0.48801 | val_1_rmse: 0.50457 |  0:00:48s
epoch 25 | loss: 0.24731 | val_0_rmse: 0.48237 | val_1_rmse: 0.50125 |  0:00:50s
epoch 26 | loss: 0.24266 | val_0_rmse: 0.47341 | val_1_rmse: 0.4991  |  0:00:52s
epoch 27 | loss: 0.24326 | val_0_rmse: 0.47519 | val_1_rmse: 0.50095 |  0:00:54s
epoch 28 | loss: 0.24414 | val_0_rmse: 0.4779  | val_1_rmse: 0.5004  |  0:00:56s
epoch 29 | loss: 0.24185 | val_0_rmse: 0.47124 | val_1_rmse: 0.50147 |  0:00:58s
epoch 30 | loss: 0.2427  | val_0_rmse: 0.47364 | val_1_rmse: 0.50684 |  0:01:00s
epoch 31 | loss: 0.25338 | val_0_rmse: 0.47515 | val_1_rmse: 0.50613 |  0:01:02s
epoch 32 | loss: 0.24078 | val_0_rmse: 0.46317 | val_1_rmse: 0.50009 |  0:01:04s
epoch 33 | loss: 0.24288 | val_0_rmse: 0.47167 | val_1_rmse: 0.51807 |  0:01:06s
epoch 34 | loss: 0.24115 | val_0_rmse: 0.46641 | val_1_rmse: 0.50809 |  0:01:08s
epoch 35 | loss: 0.23461 | val_0_rmse: 0.46658 | val_1_rmse: 0.50778 |  0:01:09s
epoch 36 | loss: 0.23785 | val_0_rmse: 0.45778 | val_1_rmse: 0.50419 |  0:01:11s
epoch 37 | loss: 0.23414 | val_0_rmse: 0.4523  | val_1_rmse: 0.49452 |  0:01:13s
epoch 38 | loss: 0.22934 | val_0_rmse: 0.45279 | val_1_rmse: 0.50072 |  0:01:15s
epoch 39 | loss: 0.22905 | val_0_rmse: 0.45491 | val_1_rmse: 0.5037  |  0:01:17s
epoch 40 | loss: 0.22941 | val_0_rmse: 0.46051 | val_1_rmse: 0.51687 |  0:01:19s
epoch 41 | loss: 0.22941 | val_0_rmse: 0.45001 | val_1_rmse: 0.50647 |  0:01:21s
epoch 42 | loss: 0.22755 | val_0_rmse: 0.45079 | val_1_rmse: 0.50033 |  0:01:23s
epoch 43 | loss: 0.22884 | val_0_rmse: 0.44966 | val_1_rmse: 0.50597 |  0:01:25s
epoch 44 | loss: 0.22306 | val_0_rmse: 0.44809 | val_1_rmse: 0.49525 |  0:01:27s
epoch 45 | loss: 0.22116 | val_0_rmse: 0.45365 | val_1_rmse: 0.50809 |  0:01:29s
epoch 46 | loss: 0.22449 | val_0_rmse: 0.45537 | val_1_rmse: 0.49912 |  0:01:31s
epoch 47 | loss: 0.22183 | val_0_rmse: 0.4467  | val_1_rmse: 0.50085 |  0:01:33s
epoch 48 | loss: 0.21866 | val_0_rmse: 0.44856 | val_1_rmse: 0.50108 |  0:01:35s
epoch 49 | loss: 0.2241  | val_0_rmse: 0.45227 | val_1_rmse: 0.51772 |  0:01:37s
epoch 50 | loss: 0.23412 | val_0_rmse: 0.45806 | val_1_rmse: 0.5221  |  0:01:39s
epoch 51 | loss: 0.22044 | val_0_rmse: 0.45253 | val_1_rmse: 0.50104 |  0:01:40s
epoch 52 | loss: 0.22028 | val_0_rmse: 0.45601 | val_1_rmse: 0.51922 |  0:01:42s
epoch 53 | loss: 0.21551 | val_0_rmse: 0.44476 | val_1_rmse: 0.49361 |  0:01:44s
epoch 54 | loss: 0.21609 | val_0_rmse: 0.43291 | val_1_rmse: 0.49524 |  0:01:46s
epoch 55 | loss: 0.21738 | val_0_rmse: 0.44242 | val_1_rmse: 0.50097 |  0:01:48s
epoch 56 | loss: 0.21567 | val_0_rmse: 0.43535 | val_1_rmse: 0.50737 |  0:01:50s
epoch 57 | loss: 0.21021 | val_0_rmse: 0.43177 | val_1_rmse: 0.49921 |  0:01:52s
epoch 58 | loss: 0.20698 | val_0_rmse: 0.42677 | val_1_rmse: 0.5001  |  0:01:54s
epoch 59 | loss: 0.20794 | val_0_rmse: 0.43787 | val_1_rmse: 0.50831 |  0:01:56s
epoch 60 | loss: 0.21276 | val_0_rmse: 0.43299 | val_1_rmse: 0.50842 |  0:01:58s
epoch 61 | loss: 0.2049  | val_0_rmse: 0.42494 | val_1_rmse: 0.49949 |  0:02:00s
epoch 62 | loss: 0.2036  | val_0_rmse: 0.42961 | val_1_rmse: 0.49647 |  0:02:02s
epoch 63 | loss: 0.20185 | val_0_rmse: 0.43134 | val_1_rmse: 0.5092  |  0:02:04s
epoch 64 | loss: 0.20896 | val_0_rmse: 0.44036 | val_1_rmse: 0.51293 |  0:02:06s
epoch 65 | loss: 0.20896 | val_0_rmse: 0.4282  | val_1_rmse: 0.5036  |  0:02:08s
epoch 66 | loss: 0.20581 | val_0_rmse: 0.42848 | val_1_rmse: 0.49993 |  0:02:09s
epoch 67 | loss: 0.20424 | val_0_rmse: 0.4285  | val_1_rmse: 0.50533 |  0:02:11s
epoch 68 | loss: 0.20287 | val_0_rmse: 0.43505 | val_1_rmse: 0.5197  |  0:02:13s
epoch 69 | loss: 0.20267 | val_0_rmse: 0.42665 | val_1_rmse: 0.50815 |  0:02:15s
epoch 70 | loss: 0.20614 | val_0_rmse: 0.43458 | val_1_rmse: 0.51873 |  0:02:17s
epoch 71 | loss: 0.20393 | val_0_rmse: 0.43738 | val_1_rmse: 0.51062 |  0:02:19s
epoch 72 | loss: 0.20486 | val_0_rmse: 0.43216 | val_1_rmse: 0.53359 |  0:02:21s
epoch 73 | loss: 0.19804 | val_0_rmse: 0.42234 | val_1_rmse: 0.50855 |  0:02:23s
epoch 74 | loss: 0.19634 | val_0_rmse: 0.41749 | val_1_rmse: 0.51176 |  0:02:25s
epoch 75 | loss: 0.19769 | val_0_rmse: 0.42567 | val_1_rmse: 0.52022 |  0:02:27s
epoch 76 | loss: 0.19605 | val_0_rmse: 0.41927 | val_1_rmse: 0.50506 |  0:02:29s
epoch 77 | loss: 0.19564 | val_0_rmse: 0.41489 | val_1_rmse: 0.49769 |  0:02:31s
epoch 78 | loss: 0.19151 | val_0_rmse: 0.41005 | val_1_rmse: 0.49523 |  0:02:33s
epoch 79 | loss: 0.19309 | val_0_rmse: 0.43406 | val_1_rmse: 0.52299 |  0:02:35s
epoch 80 | loss: 0.19723 | val_0_rmse: 0.42329 | val_1_rmse: 0.51121 |  0:02:37s
epoch 81 | loss: 0.19731 | val_0_rmse: 0.42402 | val_1_rmse: 0.51843 |  0:02:38s
epoch 82 | loss: 0.19522 | val_0_rmse: 0.42112 | val_1_rmse: 0.50798 |  0:02:40s
epoch 83 | loss: 0.19502 | val_0_rmse: 0.44644 | val_1_rmse: 0.54403 |  0:02:42s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.49361
Best weights from best epoch are automatically used!
ended training at: 15:40:09
Feature importance:
Mean squared error is of 5831665694.318675
Mean absolute error:50528.12847249838
MAPE:0.15687280029407774
R2 score:0.7309817316726677
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:42:25
epoch 0  | loss: 0.91487 | val_0_rmse: 0.86979 | val_1_rmse: 0.85383 |  0:00:11s
epoch 1  | loss: 0.47923 | val_0_rmse: 0.7471  | val_1_rmse: 0.73201 |  0:00:22s
epoch 2  | loss: 0.36436 | val_0_rmse: 0.71257 | val_1_rmse: 0.70052 |  0:00:33s
epoch 3  | loss: 0.33673 | val_0_rmse: 0.67852 | val_1_rmse: 0.66882 |  0:00:45s
epoch 4  | loss: 0.31155 | val_0_rmse: 0.61906 | val_1_rmse: 0.61088 |  0:00:56s
epoch 5  | loss: 0.30217 | val_0_rmse: 0.58956 | val_1_rmse: 0.58525 |  0:01:08s
epoch 6  | loss: 0.29496 | val_0_rmse: 0.56517 | val_1_rmse: 0.5681  |  0:01:21s
epoch 7  | loss: 0.28779 | val_0_rmse: 0.54265 | val_1_rmse: 0.55148 |  0:01:33s
epoch 8  | loss: 0.28476 | val_0_rmse: 0.51851 | val_1_rmse: 0.53587 |  0:01:44s
epoch 9  | loss: 0.28112 | val_0_rmse: 0.50586 | val_1_rmse: 0.53423 |  0:01:56s
epoch 10 | loss: 0.27728 | val_0_rmse: 0.50457 | val_1_rmse: 0.53792 |  0:02:07s
epoch 11 | loss: 0.27316 | val_0_rmse: 0.51541 | val_1_rmse: 0.54753 |  0:02:19s
epoch 12 | loss: 0.26701 | val_0_rmse: 0.51315 | val_1_rmse: 0.55211 |  0:02:30s
epoch 13 | loss: 0.26247 | val_0_rmse: 0.50969 | val_1_rmse: 0.54884 |  0:02:42s
epoch 14 | loss: 0.25909 | val_0_rmse: 0.53038 | val_1_rmse: 0.5821  |  0:02:53s
epoch 15 | loss: 0.2595  | val_0_rmse: 0.52929 | val_1_rmse: 0.57889 |  0:03:05s
epoch 16 | loss: 0.25701 | val_0_rmse: 0.53603 | val_1_rmse: 0.58338 |  0:03:16s
epoch 17 | loss: 0.25131 | val_0_rmse: 0.52168 | val_1_rmse: 0.56882 |  0:03:28s
epoch 18 | loss: 0.2496  | val_0_rmse: 0.51399 | val_1_rmse: 0.56686 |  0:03:40s
epoch 19 | loss: 0.24902 | val_0_rmse: 0.50677 | val_1_rmse: 0.5575  |  0:03:51s
epoch 20 | loss: 0.24656 | val_0_rmse: 0.50883 | val_1_rmse: 0.56197 |  0:04:03s
epoch 21 | loss: 0.24498 | val_0_rmse: 0.51572 | val_1_rmse: 0.56736 |  0:04:14s
epoch 22 | loss: 0.24269 | val_0_rmse: 0.50979 | val_1_rmse: 0.56184 |  0:04:26s
epoch 23 | loss: 0.24504 | val_0_rmse: 0.51767 | val_1_rmse: 0.57608 |  0:04:37s
epoch 24 | loss: 0.23911 | val_0_rmse: 0.50945 | val_1_rmse: 0.56089 |  0:04:49s
epoch 25 | loss: 0.2361  | val_0_rmse: 0.51559 | val_1_rmse: 0.57524 |  0:05:00s
epoch 26 | loss: 0.23969 | val_0_rmse: 0.80311 | val_1_rmse: 1.3218  |  0:05:12s
epoch 27 | loss: 0.23634 | val_0_rmse: 0.86593 | val_1_rmse: 1.49281 |  0:05:24s
epoch 28 | loss: 0.23406 | val_0_rmse: 0.50824 | val_1_rmse: 0.57521 |  0:05:35s
epoch 29 | loss: 0.23044 | val_0_rmse: 0.49208 | val_1_rmse: 0.55551 |  0:05:47s
epoch 30 | loss: 0.22996 | val_0_rmse: 0.49265 | val_1_rmse: 0.55473 |  0:05:58s
epoch 31 | loss: 0.22813 | val_0_rmse: 0.49207 | val_1_rmse: 0.55996 |  0:06:10s
epoch 32 | loss: 0.22846 | val_0_rmse: 0.48716 | val_1_rmse: 0.55452 |  0:06:21s
epoch 33 | loss: 0.22493 | val_0_rmse: 0.49184 | val_1_rmse: 0.56691 |  0:06:33s
epoch 34 | loss: 0.22483 | val_0_rmse: 0.49385 | val_1_rmse: 0.55626 |  0:06:44s
epoch 35 | loss: 0.22467 | val_0_rmse: 0.47909 | val_1_rmse: 0.54518 |  0:06:56s
epoch 36 | loss: 0.22435 | val_0_rmse: 0.50811 | val_1_rmse: 0.56612 |  0:07:08s
epoch 37 | loss: 0.21831 | val_0_rmse: 0.48447 | val_1_rmse: 0.5526  |  0:07:19s
epoch 38 | loss: 0.21909 | val_0_rmse: 0.48708 | val_1_rmse: 0.5627  |  0:07:31s
epoch 39 | loss: 0.21753 | val_0_rmse: 0.47409 | val_1_rmse: 0.54952 |  0:07:42s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.53423
Best weights from best epoch are automatically used!
ended training at: 15:50:15
Feature importance:
Mean squared error is of 1853619971.6430392
Mean absolute error:30378.151165091767
MAPE:0.25780625197225365
R2 score:0.7275299353233582
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:50:17
epoch 0  | loss: 0.80523 | val_0_rmse: 0.75122 | val_1_rmse: 0.75199 |  0:00:11s
epoch 1  | loss: 0.39865 | val_0_rmse: 0.72838 | val_1_rmse: 0.73072 |  0:00:22s
epoch 2  | loss: 0.33656 | val_0_rmse: 0.72963 | val_1_rmse: 0.73245 |  0:00:34s
epoch 3  | loss: 0.31338 | val_0_rmse: 0.65779 | val_1_rmse: 0.66327 |  0:00:45s
epoch 4  | loss: 0.29594 | val_0_rmse: 0.61523 | val_1_rmse: 0.62514 |  0:00:56s
epoch 5  | loss: 0.28704 | val_0_rmse: 0.58735 | val_1_rmse: 0.6013  |  0:01:08s
epoch 6  | loss: 0.28192 | val_0_rmse: 0.55309 | val_1_rmse: 0.57383 |  0:01:19s
epoch 7  | loss: 0.27606 | val_0_rmse: 0.56258 | val_1_rmse: 0.58169 |  0:01:31s
epoch 8  | loss: 0.26963 | val_0_rmse: 0.50441 | val_1_rmse: 0.54258 |  0:01:42s
epoch 9  | loss: 0.26576 | val_0_rmse: 0.50165 | val_1_rmse: 0.54532 |  0:01:53s
epoch 10 | loss: 0.25869 | val_0_rmse: 0.52314 | val_1_rmse: 0.56139 |  0:02:05s
epoch 11 | loss: 0.2592  | val_0_rmse: 0.50431 | val_1_rmse: 0.55084 |  0:02:16s
epoch 12 | loss: 0.25891 | val_0_rmse: 0.5057  | val_1_rmse: 0.56317 |  0:02:28s
epoch 13 | loss: 0.25446 | val_0_rmse: 0.4986  | val_1_rmse: 0.55506 |  0:02:39s
epoch 14 | loss: 0.25449 | val_0_rmse: 0.50737 | val_1_rmse: 0.56372 |  0:02:51s
epoch 15 | loss: 0.24922 | val_0_rmse: 0.51197 | val_1_rmse: 0.56584 |  0:03:02s
epoch 16 | loss: 0.24518 | val_0_rmse: 0.51224 | val_1_rmse: 0.5672  |  0:03:13s
epoch 17 | loss: 0.24315 | val_0_rmse: 0.49606 | val_1_rmse: 0.55488 |  0:03:25s
epoch 18 | loss: 0.24217 | val_0_rmse: 0.81337 | val_1_rmse: 0.5693  |  0:03:36s
epoch 19 | loss: 0.24219 | val_0_rmse: 0.5268  | val_1_rmse: 0.5778  |  0:03:48s
epoch 20 | loss: 0.23781 | val_0_rmse: 0.54732 | val_1_rmse: 0.60465 |  0:03:59s
epoch 21 | loss: 0.23614 | val_0_rmse: 0.51102 | val_1_rmse: 0.57633 |  0:04:10s
epoch 22 | loss: 0.23445 | val_0_rmse: 0.51931 | val_1_rmse: 0.58398 |  0:04:22s
epoch 23 | loss: 0.23018 | val_0_rmse: 0.48782 | val_1_rmse: 0.55718 |  0:04:33s
epoch 24 | loss: 0.22812 | val_0_rmse: 0.51568 | val_1_rmse: 0.58353 |  0:04:44s
epoch 25 | loss: 0.22628 | val_0_rmse: 0.50597 | val_1_rmse: 0.57841 |  0:04:56s
epoch 26 | loss: 0.22685 | val_0_rmse: 0.4797  | val_1_rmse: 0.55872 |  0:05:07s
epoch 27 | loss: 0.22464 | val_0_rmse: 0.48439 | val_1_rmse: 0.56397 |  0:05:19s
epoch 28 | loss: 0.22158 | val_0_rmse: 0.48702 | val_1_rmse: 0.56984 |  0:05:30s
epoch 29 | loss: 0.21929 | val_0_rmse: 0.4819  | val_1_rmse: 0.56446 |  0:05:41s
epoch 30 | loss: 0.21841 | val_0_rmse: 0.48973 | val_1_rmse: 0.57167 |  0:05:53s
epoch 31 | loss: 0.21687 | val_0_rmse: 0.50726 | val_1_rmse: 0.58636 |  0:06:04s
epoch 32 | loss: 0.21519 | val_0_rmse: 0.50115 | val_1_rmse: 0.58998 |  0:06:15s
epoch 33 | loss: 0.21515 | val_0_rmse: 0.47836 | val_1_rmse: 0.56774 |  0:06:27s
epoch 34 | loss: 0.21072 | val_0_rmse: 0.47665 | val_1_rmse: 0.56809 |  0:06:38s
epoch 35 | loss: 0.21285 | val_0_rmse: 0.50908 | val_1_rmse: 0.58943 |  0:06:50s
epoch 36 | loss: 0.21112 | val_0_rmse: 0.47076 | val_1_rmse: 0.56086 |  0:07:01s
epoch 37 | loss: 0.20851 | val_0_rmse: 0.46981 | val_1_rmse: 0.56306 |  0:07:12s
epoch 38 | loss: 0.20583 | val_0_rmse: 0.46585 | val_1_rmse: 0.56221 |  0:07:24s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.54258
Best weights from best epoch are automatically used!
ended training at: 15:57:48
Feature importance:
Mean squared error is of 1801924339.5312781
Mean absolute error:30269.5461145529
MAPE:0.273358286935292
R2 score:0.7327695907567374
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:57:50
epoch 0  | loss: 0.75983 | val_0_rmse: 0.78097 | val_1_rmse: 0.77655 |  0:00:11s
epoch 1  | loss: 0.38063 | val_0_rmse: 0.70856 | val_1_rmse: 0.70532 |  0:00:22s
epoch 2  | loss: 0.32681 | val_0_rmse: 0.67101 | val_1_rmse: 0.66932 |  0:00:33s
epoch 3  | loss: 0.30149 | val_0_rmse: 0.64158 | val_1_rmse: 0.64231 |  0:00:45s
epoch 4  | loss: 0.28792 | val_0_rmse: 0.60394 | val_1_rmse: 0.606   |  0:00:56s
epoch 5  | loss: 0.28162 | val_0_rmse: 0.58161 | val_1_rmse: 0.58895 |  0:01:07s
epoch 6  | loss: 0.275   | val_0_rmse: 0.55474 | val_1_rmse: 0.56911 |  0:01:19s
epoch 7  | loss: 0.26839 | val_0_rmse: 0.51689 | val_1_rmse: 0.53946 |  0:01:30s
epoch 8  | loss: 0.26468 | val_0_rmse: 0.50448 | val_1_rmse: 0.5327  |  0:01:41s
epoch 9  | loss: 0.26205 | val_0_rmse: 0.50405 | val_1_rmse: 0.53782 |  0:01:53s
epoch 10 | loss: 0.25663 | val_0_rmse: 0.48892 | val_1_rmse: 0.53335 |  0:02:04s
epoch 11 | loss: 0.25311 | val_0_rmse: 0.49741 | val_1_rmse: 0.5402  |  0:02:15s
epoch 12 | loss: 0.25134 | val_0_rmse: 0.49565 | val_1_rmse: 0.54173 |  0:02:26s
epoch 13 | loss: 0.24757 | val_0_rmse: 0.50646 | val_1_rmse: 0.54374 |  0:02:38s
epoch 14 | loss: 0.24639 | val_0_rmse: 0.52904 | val_1_rmse: 0.56779 |  0:02:49s
epoch 15 | loss: 0.24344 | val_0_rmse: 0.54852 | val_1_rmse: 0.58941 |  0:03:00s
epoch 16 | loss: 0.24281 | val_0_rmse: 0.51585 | val_1_rmse: 0.56663 |  0:03:12s
epoch 17 | loss: 0.24267 | val_0_rmse: 0.52228 | val_1_rmse: 0.5725  |  0:03:23s
epoch 18 | loss: 0.23828 | val_0_rmse: 0.55921 | val_1_rmse: 0.59428 |  0:03:34s
epoch 19 | loss: 0.23668 | val_0_rmse: 0.50634 | val_1_rmse: 0.55772 |  0:03:45s
epoch 20 | loss: 0.23385 | val_0_rmse: 0.71553 | val_1_rmse: 0.62773 |  0:03:57s
epoch 21 | loss: 0.23151 | val_0_rmse: 0.53348 | val_1_rmse: 0.60704 |  0:04:08s
epoch 22 | loss: 0.23117 | val_0_rmse: 0.50951 | val_1_rmse: 0.60504 |  0:04:19s
epoch 23 | loss: 0.22663 | val_0_rmse: 0.51989 | val_1_rmse: 0.6148  |  0:04:31s
epoch 24 | loss: 0.22889 | val_0_rmse: 0.4939  | val_1_rmse: 0.59035 |  0:04:42s
epoch 25 | loss: 0.2257  | val_0_rmse: 0.48562 | val_1_rmse: 0.5658  |  0:04:53s
epoch 26 | loss: 0.23166 | val_0_rmse: 0.52301 | val_1_rmse: 0.60868 |  0:05:05s
epoch 27 | loss: 0.22576 | val_0_rmse: 0.50668 | val_1_rmse: 0.57529 |  0:05:16s
epoch 28 | loss: 0.22291 | val_0_rmse: 0.53448 | val_1_rmse: 0.78045 |  0:05:27s
epoch 29 | loss: 0.22103 | val_0_rmse: 0.49898 | val_1_rmse: 0.63153 |  0:05:39s
epoch 30 | loss: 0.21835 | val_0_rmse: 0.47849 | val_1_rmse: 0.56172 |  0:05:50s
epoch 31 | loss: 0.2173  | val_0_rmse: 0.47681 | val_1_rmse: 0.55307 |  0:06:01s
epoch 32 | loss: 0.21524 | val_0_rmse: 0.48627 | val_1_rmse: 0.57984 |  0:06:13s
epoch 33 | loss: 0.21456 | val_0_rmse: 0.50581 | val_1_rmse: 0.71114 |  0:06:24s
epoch 34 | loss: 0.21195 | val_0_rmse: 0.4714  | val_1_rmse: 0.88038 |  0:06:35s
epoch 35 | loss: 0.20813 | val_0_rmse: 0.47863 | val_1_rmse: 0.93431 |  0:06:47s
epoch 36 | loss: 0.20938 | val_0_rmse: 0.49015 | val_1_rmse: 0.97927 |  0:06:58s
epoch 37 | loss: 0.20823 | val_0_rmse: 0.46985 | val_1_rmse: 1.05627 |  0:07:10s
epoch 38 | loss: 0.20557 | val_0_rmse: 1.26158 | val_1_rmse: 1.18332 |  0:07:21s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.5327
Best weights from best epoch are automatically used!
ended training at: 16:05:18
Feature importance:
Mean squared error is of 1744985826.7249725
Mean absolute error:29762.85424054681
MAPE:0.27665773704105806
R2 score:0.7431420045588715
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:05:21
epoch 0  | loss: 0.86544 | val_0_rmse: 0.81816 | val_1_rmse: 0.8307  |  0:00:11s
epoch 1  | loss: 0.48462 | val_0_rmse: 0.72225 | val_1_rmse: 0.73904 |  0:00:22s
epoch 2  | loss: 0.38587 | val_0_rmse: 0.6915  | val_1_rmse: 0.70792 |  0:00:34s
epoch 3  | loss: 0.33421 | val_0_rmse: 0.64094 | val_1_rmse: 0.65614 |  0:00:45s
epoch 4  | loss: 0.32452 | val_0_rmse: 0.62927 | val_1_rmse: 0.64614 |  0:00:56s
epoch 5  | loss: 0.31157 | val_0_rmse: 0.60191 | val_1_rmse: 0.61928 |  0:01:08s
epoch 6  | loss: 0.29708 | val_0_rmse: 0.56463 | val_1_rmse: 0.5849  |  0:01:19s
epoch 7  | loss: 0.29404 | val_0_rmse: 0.55467 | val_1_rmse: 0.58035 |  0:01:30s
epoch 8  | loss: 0.28542 | val_0_rmse: 0.51819 | val_1_rmse: 0.54849 |  0:01:42s
epoch 9  | loss: 0.27739 | val_0_rmse: 0.5342  | val_1_rmse: 0.56642 |  0:01:53s
epoch 10 | loss: 0.28276 | val_0_rmse: 0.52269 | val_1_rmse: 0.5598  |  0:02:04s
epoch 11 | loss: 0.27633 | val_0_rmse: 0.52566 | val_1_rmse: 0.57057 |  0:02:16s
epoch 12 | loss: 0.28893 | val_0_rmse: 0.53361 | val_1_rmse: 0.57374 |  0:02:27s
epoch 13 | loss: 0.2825  | val_0_rmse: 0.54487 | val_1_rmse: 0.59005 |  0:02:39s
epoch 14 | loss: 0.27174 | val_0_rmse: 0.55397 | val_1_rmse: 0.59577 |  0:02:50s
epoch 15 | loss: 0.29371 | val_0_rmse: 0.56355 | val_1_rmse: 0.59446 |  0:03:01s
epoch 16 | loss: 0.27871 | val_0_rmse: 0.53742 | val_1_rmse: 0.57803 |  0:03:13s
epoch 17 | loss: 0.27501 | val_0_rmse: 0.54672 | val_1_rmse: 0.58807 |  0:03:24s
epoch 18 | loss: 0.28484 | val_0_rmse: 0.55722 | val_1_rmse: 0.60411 |  0:03:36s
epoch 19 | loss: 0.27403 | val_0_rmse: 0.52832 | val_1_rmse: 0.57484 |  0:03:47s
epoch 20 | loss: 0.30403 | val_0_rmse: 0.55521 | val_1_rmse: 0.58916 |  0:03:58s
epoch 21 | loss: 0.28271 | val_0_rmse: 0.5836  | val_1_rmse: 0.61798 |  0:04:10s
epoch 22 | loss: 0.27394 | val_0_rmse: 0.527   | val_1_rmse: 0.57107 |  0:04:21s
epoch 23 | loss: 0.2646  | val_0_rmse: 0.52483 | val_1_rmse: 0.57044 |  0:04:33s
epoch 24 | loss: 0.26295 | val_0_rmse: 0.5501  | val_1_rmse: 0.59388 |  0:04:44s
epoch 25 | loss: 0.25626 | val_0_rmse: 0.52572 | val_1_rmse: 0.57622 |  0:04:55s
epoch 26 | loss: 0.25423 | val_0_rmse: 0.52269 | val_1_rmse: 0.57439 |  0:05:07s
epoch 27 | loss: 0.25655 | val_0_rmse: 0.52554 | val_1_rmse: 0.57241 |  0:05:18s
epoch 28 | loss: 0.25471 | val_0_rmse: 0.52114 | val_1_rmse: 0.57557 |  0:05:29s
epoch 29 | loss: 0.24935 | val_0_rmse: 0.53093 | val_1_rmse: 0.58168 |  0:05:41s
epoch 30 | loss: 0.24843 | val_0_rmse: 0.51683 | val_1_rmse: 0.57293 |  0:05:52s
epoch 31 | loss: 0.24661 | val_0_rmse: 0.54337 | val_1_rmse: 0.59706 |  0:06:04s
epoch 32 | loss: 0.24509 | val_0_rmse: 0.54617 | val_1_rmse: 0.59445 |  0:06:15s
epoch 33 | loss: 0.2487  | val_0_rmse: 0.52154 | val_1_rmse: 0.57874 |  0:06:26s
epoch 34 | loss: 0.25262 | val_0_rmse: 0.53089 | val_1_rmse: 0.58291 |  0:06:38s
epoch 35 | loss: 0.24779 | val_0_rmse: 0.53104 | val_1_rmse: 0.58887 |  0:06:49s
epoch 36 | loss: 0.24786 | val_0_rmse: 0.51382 | val_1_rmse: 0.57286 |  0:07:01s
epoch 37 | loss: 0.25135 | val_0_rmse: 0.53385 | val_1_rmse: 0.5864  |  0:07:12s
epoch 38 | loss: 0.24455 | val_0_rmse: 0.53965 | val_1_rmse: 0.5712  |  0:07:23s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.54849
Best weights from best epoch are automatically used!
ended training at: 16:12:51
Feature importance:
Mean squared error is of 1911019245.335181
Mean absolute error:31141.162781647046
MAPE:0.28134485332769865
R2 score:0.7208308390722775
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:12:53
epoch 0  | loss: 0.81865 | val_0_rmse: 0.8128  | val_1_rmse: 0.81629 |  0:00:11s
epoch 1  | loss: 0.40942 | val_0_rmse: 0.71067 | val_1_rmse: 0.71218 |  0:00:22s
epoch 2  | loss: 0.33784 | val_0_rmse: 0.68971 | val_1_rmse: 0.69134 |  0:00:34s
epoch 3  | loss: 0.30769 | val_0_rmse: 0.64135 | val_1_rmse: 0.64617 |  0:00:45s
epoch 4  | loss: 0.29117 | val_0_rmse: 0.60069 | val_1_rmse: 0.60899 |  0:00:56s
epoch 5  | loss: 0.28509 | val_0_rmse: 0.58177 | val_1_rmse: 0.59396 |  0:01:08s
epoch 6  | loss: 0.27461 | val_0_rmse: 0.55175 | val_1_rmse: 0.57266 |  0:01:19s
epoch 7  | loss: 0.27044 | val_0_rmse: 0.53285 | val_1_rmse: 0.56294 |  0:01:30s
epoch 8  | loss: 0.26774 | val_0_rmse: 0.5014  | val_1_rmse: 0.53815 |  0:01:42s
epoch 9  | loss: 0.26383 | val_0_rmse: 0.54381 | val_1_rmse: 0.57974 |  0:01:53s
epoch 10 | loss: 0.25906 | val_0_rmse: 0.50576 | val_1_rmse: 0.55252 |  0:02:05s
epoch 11 | loss: 0.2551  | val_0_rmse: 0.49057 | val_1_rmse: 0.54222 |  0:02:16s
epoch 12 | loss: 0.25394 | val_0_rmse: 0.53995 | val_1_rmse: 0.59098 |  0:02:27s
epoch 13 | loss: 0.25204 | val_0_rmse: 0.5537  | val_1_rmse: 0.59782 |  0:02:39s
epoch 14 | loss: 0.24982 | val_0_rmse: 0.5013  | val_1_rmse: 0.55617 |  0:02:50s
epoch 15 | loss: 0.24589 | val_0_rmse: 0.51589 | val_1_rmse: 0.56946 |  0:03:01s
epoch 16 | loss: 0.24584 | val_0_rmse: 0.5001  | val_1_rmse: 0.5593  |  0:03:13s
epoch 17 | loss: 0.24285 | val_0_rmse: 0.50995 | val_1_rmse: 0.56883 |  0:03:24s
epoch 18 | loss: 0.24035 | val_0_rmse: 0.68493 | val_1_rmse: 0.5791  |  0:03:36s
epoch 19 | loss: 0.23955 | val_0_rmse: 0.6535  | val_1_rmse: 0.57057 |  0:03:47s
epoch 20 | loss: 0.23632 | val_0_rmse: 0.49515 | val_1_rmse: 0.56525 |  0:03:58s
epoch 21 | loss: 0.23638 | val_0_rmse: 0.49994 | val_1_rmse: 0.56719 |  0:04:10s
epoch 22 | loss: 0.23371 | val_0_rmse: 0.5069  | val_1_rmse: 0.60137 |  0:04:21s
epoch 23 | loss: 0.23146 | val_0_rmse: 0.5264  | val_1_rmse: 0.59614 |  0:04:33s
epoch 24 | loss: 0.22995 | val_0_rmse: 0.62003 | val_1_rmse: 0.58998 |  0:04:44s
epoch 25 | loss: 0.22775 | val_0_rmse: 0.48783 | val_1_rmse: 0.56275 |  0:04:55s
epoch 26 | loss: 0.22495 | val_0_rmse: 0.62667 | val_1_rmse: 0.61037 |  0:05:07s
epoch 27 | loss: 0.2248  | val_0_rmse: 0.48416 | val_1_rmse: 0.55568 |  0:05:18s
epoch 28 | loss: 0.22407 | val_0_rmse: 0.61208 | val_1_rmse: 0.65398 |  0:05:29s
epoch 29 | loss: 0.22122 | val_0_rmse: 0.48836 | val_1_rmse: 0.57059 |  0:05:41s
epoch 30 | loss: 0.21871 | val_0_rmse: 0.49528 | val_1_rmse: 0.58007 |  0:05:52s
epoch 31 | loss: 0.21735 | val_0_rmse: 0.49161 | val_1_rmse: 0.61812 |  0:06:04s
epoch 32 | loss: 0.21616 | val_0_rmse: 0.50109 | val_1_rmse: 0.59195 |  0:06:15s
epoch 33 | loss: 0.2156  | val_0_rmse: 0.5039  | val_1_rmse: 0.59151 |  0:06:26s
epoch 34 | loss: 0.21667 | val_0_rmse: 0.50197 | val_1_rmse: 0.58428 |  0:06:38s
epoch 35 | loss: 0.21419 | val_0_rmse: 0.49636 | val_1_rmse: 0.58778 |  0:06:50s
epoch 36 | loss: 0.2122  | val_0_rmse: 0.49289 | val_1_rmse: 0.5802  |  0:07:01s
epoch 37 | loss: 0.21093 | val_0_rmse: 0.50123 | val_1_rmse: 0.59367 |  0:07:13s
epoch 38 | loss: 0.20964 | val_0_rmse: 0.48935 | val_1_rmse: 0.59537 |  0:07:24s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.53815
Best weights from best epoch are automatically used!
ended training at: 16:20:25
Feature importance:
Mean squared error is of 1807782808.7716546
Mean absolute error:30247.071999829757
MAPE:0.26865485509538045
R2 score:0.7343774775823944
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:20:32
epoch 0  | loss: 1.55865 | val_0_rmse: 0.96149 | val_1_rmse: 0.95059 |  0:00:01s
epoch 1  | loss: 0.584   | val_0_rmse: 0.94378 | val_1_rmse: 0.93589 |  0:00:03s
epoch 2  | loss: 0.32965 | val_0_rmse: 0.64704 | val_1_rmse: 0.64949 |  0:00:05s
epoch 3  | loss: 0.26752 | val_0_rmse: 0.63682 | val_1_rmse: 0.63836 |  0:00:06s
epoch 4  | loss: 0.23795 | val_0_rmse: 0.64989 | val_1_rmse: 0.65184 |  0:00:08s
epoch 5  | loss: 0.21678 | val_0_rmse: 0.58286 | val_1_rmse: 0.58826 |  0:00:10s
epoch 6  | loss: 0.20738 | val_0_rmse: 0.58772 | val_1_rmse: 0.59061 |  0:00:11s
epoch 7  | loss: 0.21153 | val_0_rmse: 0.55469 | val_1_rmse: 0.55931 |  0:00:13s
epoch 8  | loss: 0.20299 | val_0_rmse: 0.52913 | val_1_rmse: 0.5307  |  0:00:15s
epoch 9  | loss: 0.19594 | val_0_rmse: 0.55246 | val_1_rmse: 0.55351 |  0:00:16s
epoch 10 | loss: 0.19441 | val_0_rmse: 0.52872 | val_1_rmse: 0.52603 |  0:00:18s
epoch 11 | loss: 0.19421 | val_0_rmse: 0.51482 | val_1_rmse: 0.51598 |  0:00:20s
epoch 12 | loss: 0.188   | val_0_rmse: 0.50968 | val_1_rmse: 0.51172 |  0:00:21s
epoch 13 | loss: 0.18812 | val_0_rmse: 0.51867 | val_1_rmse: 0.52263 |  0:00:23s
epoch 14 | loss: 0.19322 | val_0_rmse: 0.53217 | val_1_rmse: 0.52898 |  0:00:24s
epoch 15 | loss: 0.18799 | val_0_rmse: 0.57812 | val_1_rmse: 0.57681 |  0:00:26s
epoch 16 | loss: 0.18483 | val_0_rmse: 0.48069 | val_1_rmse: 0.4806  |  0:00:28s
epoch 17 | loss: 0.1796  | val_0_rmse: 0.46226 | val_1_rmse: 0.4632  |  0:00:29s
epoch 18 | loss: 0.17968 | val_0_rmse: 0.44377 | val_1_rmse: 0.44705 |  0:00:31s
epoch 19 | loss: 0.17685 | val_0_rmse: 0.43906 | val_1_rmse: 0.44082 |  0:00:33s
epoch 20 | loss: 0.17725 | val_0_rmse: 0.46003 | val_1_rmse: 0.4585  |  0:00:34s
epoch 21 | loss: 0.18469 | val_0_rmse: 0.4416  | val_1_rmse: 0.45123 |  0:00:36s
epoch 22 | loss: 0.18863 | val_0_rmse: 0.42973 | val_1_rmse: 0.43912 |  0:00:38s
epoch 23 | loss: 0.18463 | val_0_rmse: 0.45867 | val_1_rmse: 0.46121 |  0:00:39s
epoch 24 | loss: 0.18563 | val_0_rmse: 0.42253 | val_1_rmse: 0.43396 |  0:00:41s
epoch 25 | loss: 0.18395 | val_0_rmse: 0.424   | val_1_rmse: 0.43893 |  0:00:43s
epoch 26 | loss: 0.18672 | val_0_rmse: 0.42642 | val_1_rmse: 0.43836 |  0:00:44s
epoch 27 | loss: 0.17879 | val_0_rmse: 0.41285 | val_1_rmse: 0.42673 |  0:00:46s
epoch 28 | loss: 0.17878 | val_0_rmse: 0.41874 | val_1_rmse: 0.43259 |  0:00:47s
epoch 29 | loss: 0.17627 | val_0_rmse: 0.40055 | val_1_rmse: 0.42198 |  0:00:49s
epoch 30 | loss: 0.18261 | val_0_rmse: 0.4074  | val_1_rmse: 0.42886 |  0:00:51s
epoch 31 | loss: 0.18047 | val_0_rmse: 0.43633 | val_1_rmse: 0.45506 |  0:00:52s
epoch 32 | loss: 0.18719 | val_0_rmse: 0.42315 | val_1_rmse: 0.43979 |  0:00:54s
epoch 33 | loss: 0.18837 | val_0_rmse: 0.4123  | val_1_rmse: 0.42995 |  0:00:56s
epoch 34 | loss: 0.18504 | val_0_rmse: 0.4282  | val_1_rmse: 0.44035 |  0:00:57s
epoch 35 | loss: 0.17704 | val_0_rmse: 0.39389 | val_1_rmse: 0.41325 |  0:00:59s
epoch 36 | loss: 0.16943 | val_0_rmse: 0.39281 | val_1_rmse: 0.41141 |  0:01:01s
epoch 37 | loss: 0.16631 | val_0_rmse: 0.38892 | val_1_rmse: 0.41069 |  0:01:02s
epoch 38 | loss: 0.1664  | val_0_rmse: 0.38914 | val_1_rmse: 0.40792 |  0:01:04s
epoch 39 | loss: 0.16607 | val_0_rmse: 0.38796 | val_1_rmse: 0.40992 |  0:01:06s
epoch 40 | loss: 0.17527 | val_0_rmse: 0.42223 | val_1_rmse: 0.43812 |  0:01:07s
epoch 41 | loss: 0.16856 | val_0_rmse: 0.39852 | val_1_rmse: 0.42177 |  0:01:09s
epoch 42 | loss: 0.17077 | val_0_rmse: 0.39091 | val_1_rmse: 0.41297 |  0:01:10s
epoch 43 | loss: 0.16736 | val_0_rmse: 0.38508 | val_1_rmse: 0.4093  |  0:01:12s
epoch 44 | loss: 0.15907 | val_0_rmse: 0.38219 | val_1_rmse: 0.40967 |  0:01:14s
epoch 45 | loss: 0.16434 | val_0_rmse: 0.38927 | val_1_rmse: 0.41689 |  0:01:15s
epoch 46 | loss: 0.16109 | val_0_rmse: 0.38339 | val_1_rmse: 0.41199 |  0:01:17s
epoch 47 | loss: 0.15752 | val_0_rmse: 0.39555 | val_1_rmse: 0.41614 |  0:01:19s
epoch 48 | loss: 0.16062 | val_0_rmse: 0.38122 | val_1_rmse: 0.40804 |  0:01:20s
epoch 49 | loss: 0.1584  | val_0_rmse: 0.39463 | val_1_rmse: 0.42105 |  0:01:22s
epoch 50 | loss: 0.16049 | val_0_rmse: 0.38275 | val_1_rmse: 0.40897 |  0:01:24s
epoch 51 | loss: 0.15524 | val_0_rmse: 0.37765 | val_1_rmse: 0.40911 |  0:01:25s
epoch 52 | loss: 0.15303 | val_0_rmse: 0.37402 | val_1_rmse: 0.40206 |  0:01:27s
epoch 53 | loss: 0.15319 | val_0_rmse: 0.37526 | val_1_rmse: 0.40763 |  0:01:29s
epoch 54 | loss: 0.15169 | val_0_rmse: 0.3718  | val_1_rmse: 0.40169 |  0:01:30s
epoch 55 | loss: 0.15317 | val_0_rmse: 0.37162 | val_1_rmse: 0.40311 |  0:01:32s
epoch 56 | loss: 0.1505  | val_0_rmse: 0.3815  | val_1_rmse: 0.41387 |  0:01:33s
epoch 57 | loss: 0.15376 | val_0_rmse: 0.37858 | val_1_rmse: 0.41185 |  0:01:35s
epoch 58 | loss: 0.14974 | val_0_rmse: 0.3699  | val_1_rmse: 0.40455 |  0:01:37s
epoch 59 | loss: 0.14955 | val_0_rmse: 0.37891 | val_1_rmse: 0.41351 |  0:01:38s
epoch 60 | loss: 0.14826 | val_0_rmse: 0.36801 | val_1_rmse: 0.40423 |  0:01:40s
epoch 61 | loss: 0.14627 | val_0_rmse: 0.36771 | val_1_rmse: 0.40555 |  0:01:42s
epoch 62 | loss: 0.14975 | val_0_rmse: 0.38245 | val_1_rmse: 0.41379 |  0:01:43s
epoch 63 | loss: 0.14817 | val_0_rmse: 0.37098 | val_1_rmse: 0.40648 |  0:01:45s
epoch 64 | loss: 0.14931 | val_0_rmse: 0.40094 | val_1_rmse: 0.43369 |  0:01:47s
epoch 65 | loss: 0.17381 | val_0_rmse: 0.41456 | val_1_rmse: 0.45455 |  0:01:48s
epoch 66 | loss: 0.17034 | val_0_rmse: 0.40068 | val_1_rmse: 0.42754 |  0:01:50s
epoch 67 | loss: 0.16355 | val_0_rmse: 0.38472 | val_1_rmse: 0.41349 |  0:01:52s
epoch 68 | loss: 0.17425 | val_0_rmse: 0.40878 | val_1_rmse: 0.43769 |  0:01:53s
epoch 69 | loss: 0.17218 | val_0_rmse: 0.40638 | val_1_rmse: 0.43901 |  0:01:55s
epoch 70 | loss: 0.16754 | val_0_rmse: 0.39624 | val_1_rmse: 0.41834 |  0:01:56s
epoch 71 | loss: 0.17933 | val_0_rmse: 0.4132  | val_1_rmse: 0.44107 |  0:01:58s
epoch 72 | loss: 0.19307 | val_0_rmse: 0.45387 | val_1_rmse: 0.47743 |  0:02:00s
epoch 73 | loss: 0.19342 | val_0_rmse: 0.4223  | val_1_rmse: 0.45149 |  0:02:01s
epoch 74 | loss: 0.18583 | val_0_rmse: 0.4345  | val_1_rmse: 0.45416 |  0:02:03s
epoch 75 | loss: 0.20418 | val_0_rmse: 0.44972 | val_1_rmse: 0.47255 |  0:02:05s
epoch 76 | loss: 0.20957 | val_0_rmse: 0.49833 | val_1_rmse: 0.49997 |  0:02:06s
epoch 77 | loss: 0.19853 | val_0_rmse: 0.45444 | val_1_rmse: 0.46612 |  0:02:08s
epoch 78 | loss: 0.19136 | val_0_rmse: 0.46234 | val_1_rmse: 0.51971 |  0:02:10s
epoch 79 | loss: 0.18123 | val_0_rmse: 0.40409 | val_1_rmse: 0.4205  |  0:02:11s
epoch 80 | loss: 0.17005 | val_0_rmse: 0.40004 | val_1_rmse: 0.41686 |  0:02:13s
epoch 81 | loss: 0.16851 | val_0_rmse: 0.40938 | val_1_rmse: 0.42677 |  0:02:15s
epoch 82 | loss: 0.18461 | val_0_rmse: 0.45438 | val_1_rmse: 0.47473 |  0:02:16s
epoch 83 | loss: 0.17999 | val_0_rmse: 0.40185 | val_1_rmse: 0.42364 |  0:02:18s
epoch 84 | loss: 0.16841 | val_0_rmse: 0.39876 | val_1_rmse: 0.41945 |  0:02:19s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.40169
Best weights from best epoch are automatically used!
ended training at: 16:22:52
Feature importance:
Mean squared error is of 960844351.3792542
Mean absolute error:20105.563584822663
MAPE:0.21559256702791374
R2 score:0.7837185608943121
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:22:53
epoch 0  | loss: 1.47577 | val_0_rmse: 0.96607 | val_1_rmse: 0.96758 |  0:00:01s
epoch 1  | loss: 0.62387 | val_0_rmse: 0.75279 | val_1_rmse: 0.74762 |  0:00:03s
epoch 2  | loss: 0.40855 | val_0_rmse: 0.70055 | val_1_rmse: 0.6882  |  0:00:04s
epoch 3  | loss: 0.33569 | val_0_rmse: 0.6316  | val_1_rmse: 0.61836 |  0:00:06s
epoch 4  | loss: 0.3035  | val_0_rmse: 0.59053 | val_1_rmse: 0.57925 |  0:00:08s
epoch 5  | loss: 0.28296 | val_0_rmse: 0.57919 | val_1_rmse: 0.57079 |  0:00:09s
epoch 6  | loss: 0.25605 | val_0_rmse: 0.53771 | val_1_rmse: 0.534   |  0:00:11s
epoch 7  | loss: 0.22473 | val_0_rmse: 0.52766 | val_1_rmse: 0.52399 |  0:00:13s
epoch 8  | loss: 0.22539 | val_0_rmse: 0.50511 | val_1_rmse: 0.50478 |  0:00:14s
epoch 9  | loss: 0.20635 | val_0_rmse: 0.50312 | val_1_rmse: 0.50205 |  0:00:16s
epoch 10 | loss: 0.20142 | val_0_rmse: 0.48545 | val_1_rmse: 0.48119 |  0:00:18s
epoch 11 | loss: 0.19569 | val_0_rmse: 0.49242 | val_1_rmse: 0.49207 |  0:00:19s
epoch 12 | loss: 0.1891  | val_0_rmse: 0.4764  | val_1_rmse: 0.47631 |  0:00:21s
epoch 13 | loss: 0.1867  | val_0_rmse: 0.46006 | val_1_rmse: 0.46255 |  0:00:23s
epoch 14 | loss: 0.17786 | val_0_rmse: 0.46133 | val_1_rmse: 0.46078 |  0:00:24s
epoch 15 | loss: 0.17438 | val_0_rmse: 0.45764 | val_1_rmse: 0.45546 |  0:00:26s
epoch 16 | loss: 0.17495 | val_0_rmse: 0.45077 | val_1_rmse: 0.44929 |  0:00:28s
epoch 17 | loss: 0.17085 | val_0_rmse: 0.44285 | val_1_rmse: 0.44036 |  0:00:29s
epoch 18 | loss: 0.17489 | val_0_rmse: 0.43099 | val_1_rmse: 0.43269 |  0:00:31s
epoch 19 | loss: 0.18172 | val_0_rmse: 0.43965 | val_1_rmse: 0.44237 |  0:00:33s
epoch 20 | loss: 0.17155 | val_0_rmse: 0.43406 | val_1_rmse: 0.44082 |  0:00:34s
epoch 21 | loss: 0.16981 | val_0_rmse: 0.4278  | val_1_rmse: 0.43018 |  0:00:36s
epoch 22 | loss: 0.16578 | val_0_rmse: 0.41395 | val_1_rmse: 0.42322 |  0:00:37s
epoch 23 | loss: 0.1646  | val_0_rmse: 0.42194 | val_1_rmse: 0.43243 |  0:00:39s
epoch 24 | loss: 0.17235 | val_0_rmse: 0.41082 | val_1_rmse: 0.42396 |  0:00:41s
epoch 25 | loss: 0.16831 | val_0_rmse: 0.43132 | val_1_rmse: 0.43904 |  0:00:42s
epoch 26 | loss: 0.17768 | val_0_rmse: 0.40912 | val_1_rmse: 0.42411 |  0:00:44s
epoch 27 | loss: 0.16935 | val_0_rmse: 0.39846 | val_1_rmse: 0.41618 |  0:00:46s
epoch 28 | loss: 0.16861 | val_0_rmse: 0.39708 | val_1_rmse: 0.41385 |  0:00:47s
epoch 29 | loss: 0.16167 | val_0_rmse: 0.39341 | val_1_rmse: 0.41337 |  0:00:49s
epoch 30 | loss: 0.16051 | val_0_rmse: 0.39344 | val_1_rmse: 0.41259 |  0:00:51s
epoch 31 | loss: 0.15936 | val_0_rmse: 0.38589 | val_1_rmse: 0.4079  |  0:00:52s
epoch 32 | loss: 0.15747 | val_0_rmse: 0.38574 | val_1_rmse: 0.41314 |  0:00:54s
epoch 33 | loss: 0.15575 | val_0_rmse: 0.37841 | val_1_rmse: 0.4053  |  0:00:56s
epoch 34 | loss: 0.15439 | val_0_rmse: 0.38476 | val_1_rmse: 0.41181 |  0:00:57s
epoch 35 | loss: 0.1531  | val_0_rmse: 0.38381 | val_1_rmse: 0.41475 |  0:00:59s
epoch 36 | loss: 0.15208 | val_0_rmse: 0.38035 | val_1_rmse: 0.41244 |  0:01:00s
epoch 37 | loss: 0.15362 | val_0_rmse: 0.37417 | val_1_rmse: 0.40517 |  0:01:02s
epoch 38 | loss: 0.15069 | val_0_rmse: 0.36869 | val_1_rmse: 0.40489 |  0:01:04s
epoch 39 | loss: 0.14861 | val_0_rmse: 0.37006 | val_1_rmse: 0.40711 |  0:01:05s
epoch 40 | loss: 0.14585 | val_0_rmse: 0.36331 | val_1_rmse: 0.40384 |  0:01:07s
epoch 41 | loss: 0.1466  | val_0_rmse: 0.3697  | val_1_rmse: 0.40991 |  0:01:09s
epoch 42 | loss: 0.14868 | val_0_rmse: 0.37467 | val_1_rmse: 0.41756 |  0:01:10s
epoch 43 | loss: 0.15077 | val_0_rmse: 0.3843  | val_1_rmse: 0.42755 |  0:01:12s
epoch 44 | loss: 0.15224 | val_0_rmse: 0.38507 | val_1_rmse: 0.42161 |  0:01:14s
epoch 45 | loss: 0.15933 | val_0_rmse: 0.39505 | val_1_rmse: 0.43162 |  0:01:15s
epoch 46 | loss: 0.16816 | val_0_rmse: 0.3979  | val_1_rmse: 0.4335  |  0:01:17s
epoch 47 | loss: 0.16035 | val_0_rmse: 0.42131 | val_1_rmse: 0.45807 |  0:01:19s
epoch 48 | loss: 0.16349 | val_0_rmse: 0.39937 | val_1_rmse: 0.43651 |  0:01:20s
epoch 49 | loss: 0.16184 | val_0_rmse: 0.3957  | val_1_rmse: 0.43097 |  0:01:22s
epoch 50 | loss: 0.16583 | val_0_rmse: 0.4027  | val_1_rmse: 0.44116 |  0:01:24s
epoch 51 | loss: 0.16356 | val_0_rmse: 0.38678 | val_1_rmse: 0.42558 |  0:01:25s
epoch 52 | loss: 0.15703 | val_0_rmse: 0.39395 | val_1_rmse: 0.42649 |  0:01:27s
epoch 53 | loss: 0.15958 | val_0_rmse: 0.39295 | val_1_rmse: 0.42436 |  0:01:28s
epoch 54 | loss: 0.15584 | val_0_rmse: 0.38805 | val_1_rmse: 0.41725 |  0:01:30s
epoch 55 | loss: 0.16017 | val_0_rmse: 0.39116 | val_1_rmse: 0.4286  |  0:01:32s
epoch 56 | loss: 0.15474 | val_0_rmse: 0.37622 | val_1_rmse: 0.41235 |  0:01:33s
epoch 57 | loss: 0.15109 | val_0_rmse: 0.3769  | val_1_rmse: 0.41377 |  0:01:35s
epoch 58 | loss: 0.14802 | val_0_rmse: 0.36897 | val_1_rmse: 0.40669 |  0:01:37s
epoch 59 | loss: 0.14723 | val_0_rmse: 0.36688 | val_1_rmse: 0.40555 |  0:01:38s
epoch 60 | loss: 0.14446 | val_0_rmse: 0.37152 | val_1_rmse: 0.41035 |  0:01:40s
epoch 61 | loss: 0.14645 | val_0_rmse: 0.40912 | val_1_rmse: 0.45028 |  0:01:42s
epoch 62 | loss: 0.15335 | val_0_rmse: 0.40452 | val_1_rmse: 0.46404 |  0:01:43s
epoch 63 | loss: 0.16183 | val_0_rmse: 0.38283 | val_1_rmse: 0.42572 |  0:01:45s
epoch 64 | loss: 0.1534  | val_0_rmse: 0.37797 | val_1_rmse: 0.41789 |  0:01:47s
epoch 65 | loss: 0.15124 | val_0_rmse: 0.37923 | val_1_rmse: 0.41962 |  0:01:48s
epoch 66 | loss: 0.15172 | val_0_rmse: 0.37449 | val_1_rmse: 0.42179 |  0:01:50s
epoch 67 | loss: 0.14496 | val_0_rmse: 0.37534 | val_1_rmse: 0.41912 |  0:01:51s
epoch 68 | loss: 0.14422 | val_0_rmse: 0.37576 | val_1_rmse: 0.42197 |  0:01:53s
epoch 69 | loss: 0.146   | val_0_rmse: 0.38524 | val_1_rmse: 0.43342 |  0:01:55s
epoch 70 | loss: 0.14919 | val_0_rmse: 0.3804  | val_1_rmse: 0.42465 |  0:01:56s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.40384
Best weights from best epoch are automatically used!
ended training at: 16:24:51
Feature importance:
Mean squared error is of 923513399.7679749
Mean absolute error:20030.514401471955
MAPE:0.21367832921121752
R2 score:0.7988372736364237
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:24:51
epoch 0  | loss: 1.69642 | val_0_rmse: 0.98178 | val_1_rmse: 0.97753 |  0:00:01s
epoch 1  | loss: 0.57749 | val_0_rmse: 0.84893 | val_1_rmse: 0.84836 |  0:00:03s
epoch 2  | loss: 0.40019 | val_0_rmse: 0.64933 | val_1_rmse: 0.65093 |  0:00:04s
epoch 3  | loss: 0.31887 | val_0_rmse: 0.70135 | val_1_rmse: 0.70699 |  0:00:06s
epoch 4  | loss: 0.28919 | val_0_rmse: 0.58773 | val_1_rmse: 0.59589 |  0:00:08s
epoch 5  | loss: 0.27108 | val_0_rmse: 0.56704 | val_1_rmse: 0.57145 |  0:00:09s
epoch 6  | loss: 0.24987 | val_0_rmse: 0.57956 | val_1_rmse: 0.5875  |  0:00:11s
epoch 7  | loss: 0.24007 | val_0_rmse: 0.54024 | val_1_rmse: 0.54598 |  0:00:13s
epoch 8  | loss: 0.2317  | val_0_rmse: 0.52989 | val_1_rmse: 0.54047 |  0:00:14s
epoch 9  | loss: 0.21328 | val_0_rmse: 0.52436 | val_1_rmse: 0.53503 |  0:00:16s
epoch 10 | loss: 0.20219 | val_0_rmse: 0.51256 | val_1_rmse: 0.52057 |  0:00:18s
epoch 11 | loss: 0.21082 | val_0_rmse: 0.49975 | val_1_rmse: 0.5049  |  0:00:19s
epoch 12 | loss: 0.2039  | val_0_rmse: 0.48182 | val_1_rmse: 0.4882  |  0:00:21s
epoch 13 | loss: 0.19582 | val_0_rmse: 0.48066 | val_1_rmse: 0.48964 |  0:00:23s
epoch 14 | loss: 0.19002 | val_0_rmse: 0.47127 | val_1_rmse: 0.47826 |  0:00:24s
epoch 15 | loss: 0.18871 | val_0_rmse: 0.46259 | val_1_rmse: 0.46882 |  0:00:26s
epoch 16 | loss: 0.18255 | val_0_rmse: 0.46877 | val_1_rmse: 0.47763 |  0:00:27s
epoch 17 | loss: 0.18242 | val_0_rmse: 0.44641 | val_1_rmse: 0.45694 |  0:00:29s
epoch 18 | loss: 0.18178 | val_0_rmse: 0.44341 | val_1_rmse: 0.45464 |  0:00:31s
epoch 19 | loss: 0.18378 | val_0_rmse: 0.44458 | val_1_rmse: 0.45335 |  0:00:32s
epoch 20 | loss: 0.18044 | val_0_rmse: 0.45134 | val_1_rmse: 0.46066 |  0:00:34s
epoch 21 | loss: 0.17876 | val_0_rmse: 0.42229 | val_1_rmse: 0.43451 |  0:00:36s
epoch 22 | loss: 0.17731 | val_0_rmse: 0.423   | val_1_rmse: 0.43808 |  0:00:37s
epoch 23 | loss: 0.17644 | val_0_rmse: 0.43688 | val_1_rmse: 0.45263 |  0:00:39s
epoch 24 | loss: 0.17084 | val_0_rmse: 0.42304 | val_1_rmse: 0.43837 |  0:00:40s
epoch 25 | loss: 0.17137 | val_0_rmse: 0.41656 | val_1_rmse: 0.43376 |  0:00:42s
epoch 26 | loss: 0.16922 | val_0_rmse: 0.42054 | val_1_rmse: 0.44016 |  0:00:44s
epoch 27 | loss: 0.16869 | val_0_rmse: 0.40858 | val_1_rmse: 0.42807 |  0:00:45s
epoch 28 | loss: 0.16523 | val_0_rmse: 0.40418 | val_1_rmse: 0.42448 |  0:00:47s
epoch 29 | loss: 0.1642  | val_0_rmse: 0.40121 | val_1_rmse: 0.4223  |  0:00:49s
epoch 30 | loss: 0.16019 | val_0_rmse: 0.39302 | val_1_rmse: 0.4191  |  0:00:50s
epoch 31 | loss: 0.16182 | val_0_rmse: 0.4042  | val_1_rmse: 0.42701 |  0:00:52s
epoch 32 | loss: 0.1631  | val_0_rmse: 0.39215 | val_1_rmse: 0.41719 |  0:00:54s
epoch 33 | loss: 0.16519 | val_0_rmse: 0.38815 | val_1_rmse: 0.41332 |  0:00:55s
epoch 34 | loss: 0.16347 | val_0_rmse: 0.38446 | val_1_rmse: 0.40905 |  0:00:57s
epoch 35 | loss: 0.16299 | val_0_rmse: 0.38907 | val_1_rmse: 0.41387 |  0:00:58s
epoch 36 | loss: 0.16179 | val_0_rmse: 0.38885 | val_1_rmse: 0.4151  |  0:01:00s
epoch 37 | loss: 0.16169 | val_0_rmse: 0.38925 | val_1_rmse: 0.41379 |  0:01:02s
epoch 38 | loss: 0.15695 | val_0_rmse: 0.39051 | val_1_rmse: 0.41837 |  0:01:03s
epoch 39 | loss: 0.15827 | val_0_rmse: 0.40913 | val_1_rmse: 0.44172 |  0:01:05s
epoch 40 | loss: 0.15599 | val_0_rmse: 0.39495 | val_1_rmse: 0.42971 |  0:01:07s
epoch 41 | loss: 0.15918 | val_0_rmse: 0.39835 | val_1_rmse: 0.4409  |  0:01:08s
epoch 42 | loss: 0.15635 | val_0_rmse: 0.39293 | val_1_rmse: 0.43079 |  0:01:10s
epoch 43 | loss: 0.15725 | val_0_rmse: 0.40643 | val_1_rmse: 0.44797 |  0:01:12s
epoch 44 | loss: 0.15723 | val_0_rmse: 0.39551 | val_1_rmse: 0.44025 |  0:01:13s
epoch 45 | loss: 0.15552 | val_0_rmse: 0.39764 | val_1_rmse: 0.4432  |  0:01:15s
epoch 46 | loss: 0.15765 | val_0_rmse: 0.39492 | val_1_rmse: 0.4386  |  0:01:16s
epoch 47 | loss: 0.15739 | val_0_rmse: 0.38268 | val_1_rmse: 0.42263 |  0:01:18s
epoch 48 | loss: 0.15398 | val_0_rmse: 0.38362 | val_1_rmse: 0.41684 |  0:01:20s
epoch 49 | loss: 0.15456 | val_0_rmse: 0.38202 | val_1_rmse: 0.4222  |  0:01:21s
epoch 50 | loss: 0.15726 | val_0_rmse: 0.38253 | val_1_rmse: 0.42124 |  0:01:23s
epoch 51 | loss: 0.15939 | val_0_rmse: 0.39181 | val_1_rmse: 0.42636 |  0:01:25s
epoch 52 | loss: 0.15812 | val_0_rmse: 0.38363 | val_1_rmse: 0.42014 |  0:01:26s
epoch 53 | loss: 0.15417 | val_0_rmse: 0.37776 | val_1_rmse: 0.41717 |  0:01:28s
epoch 54 | loss: 0.1535  | val_0_rmse: 0.38161 | val_1_rmse: 0.4138  |  0:01:29s
epoch 55 | loss: 0.15018 | val_0_rmse: 0.38445 | val_1_rmse: 0.42596 |  0:01:31s
epoch 56 | loss: 0.15237 | val_0_rmse: 0.38055 | val_1_rmse: 0.42327 |  0:01:33s
epoch 57 | loss: 0.15537 | val_0_rmse: 0.37975 | val_1_rmse: 0.42683 |  0:01:34s
epoch 58 | loss: 0.15105 | val_0_rmse: 0.38652 | val_1_rmse: 0.42965 |  0:01:36s
epoch 59 | loss: 0.14737 | val_0_rmse: 0.37193 | val_1_rmse: 0.42038 |  0:01:38s
epoch 60 | loss: 0.14617 | val_0_rmse: 0.37504 | val_1_rmse: 0.42176 |  0:01:39s
epoch 61 | loss: 0.14549 | val_0_rmse: 0.37067 | val_1_rmse: 0.42048 |  0:01:41s
epoch 62 | loss: 0.15057 | val_0_rmse: 0.37356 | val_1_rmse: 0.42339 |  0:01:43s
epoch 63 | loss: 0.14653 | val_0_rmse: 0.3884  | val_1_rmse: 0.43201 |  0:01:44s
epoch 64 | loss: 0.14573 | val_0_rmse: 0.36408 | val_1_rmse: 0.41723 |  0:01:46s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.40905
Best weights from best epoch are automatically used!
ended training at: 16:26:38
Feature importance:
Mean squared error is of 916236108.3918415
Mean absolute error:20051.27915560665
MAPE:0.21163291070736334
R2 score:0.7995403022616739
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:26:39
epoch 0  | loss: 1.46545 | val_0_rmse: 0.97616 | val_1_rmse: 0.95749 |  0:00:01s
epoch 1  | loss: 0.59094 | val_0_rmse: 0.86815 | val_1_rmse: 0.85951 |  0:00:03s
epoch 2  | loss: 0.36397 | val_0_rmse: 0.60722 | val_1_rmse: 0.59234 |  0:00:04s
epoch 3  | loss: 0.29213 | val_0_rmse: 0.5523  | val_1_rmse: 0.53751 |  0:00:06s
epoch 4  | loss: 0.26063 | val_0_rmse: 0.53865 | val_1_rmse: 0.52254 |  0:00:08s
epoch 5  | loss: 0.24369 | val_0_rmse: 0.58463 | val_1_rmse: 0.57153 |  0:00:09s
epoch 6  | loss: 0.23697 | val_0_rmse: 0.5534  | val_1_rmse: 0.54437 |  0:00:11s
epoch 7  | loss: 0.22293 | val_0_rmse: 0.5485  | val_1_rmse: 0.53601 |  0:00:13s
epoch 8  | loss: 0.21116 | val_0_rmse: 0.52546 | val_1_rmse: 0.5144  |  0:00:14s
epoch 9  | loss: 0.20521 | val_0_rmse: 0.52143 | val_1_rmse: 0.50725 |  0:00:16s
epoch 10 | loss: 0.20061 | val_0_rmse: 0.53387 | val_1_rmse: 0.52157 |  0:00:18s
epoch 11 | loss: 0.19446 | val_0_rmse: 0.51775 | val_1_rmse: 0.50794 |  0:00:19s
epoch 12 | loss: 0.18905 | val_0_rmse: 0.52008 | val_1_rmse: 0.51286 |  0:00:21s
epoch 13 | loss: 0.1944  | val_0_rmse: 0.50245 | val_1_rmse: 0.4925  |  0:00:23s
epoch 14 | loss: 0.1923  | val_0_rmse: 0.53908 | val_1_rmse: 0.52989 |  0:00:24s
epoch 15 | loss: 0.20189 | val_0_rmse: 0.47717 | val_1_rmse: 0.46546 |  0:00:26s
epoch 16 | loss: 0.19421 | val_0_rmse: 0.46255 | val_1_rmse: 0.45015 |  0:00:27s
epoch 17 | loss: 0.18737 | val_0_rmse: 0.45451 | val_1_rmse: 0.44812 |  0:00:29s
epoch 18 | loss: 0.1899  | val_0_rmse: 0.45764 | val_1_rmse: 0.45015 |  0:00:31s
epoch 19 | loss: 0.18778 | val_0_rmse: 0.4477  | val_1_rmse: 0.44097 |  0:00:32s
epoch 20 | loss: 0.18522 | val_0_rmse: 0.44671 | val_1_rmse: 0.4398  |  0:00:34s
epoch 21 | loss: 0.192   | val_0_rmse: 0.46507 | val_1_rmse: 0.4606  |  0:00:36s
epoch 22 | loss: 0.18708 | val_0_rmse: 0.47475 | val_1_rmse: 0.46915 |  0:00:37s
epoch 23 | loss: 0.18426 | val_0_rmse: 0.42728 | val_1_rmse: 0.4211  |  0:00:39s
epoch 24 | loss: 0.188   | val_0_rmse: 0.42959 | val_1_rmse: 0.42334 |  0:00:41s
epoch 25 | loss: 0.18445 | val_0_rmse: 0.42589 | val_1_rmse: 0.42586 |  0:00:42s
epoch 26 | loss: 0.18863 | val_0_rmse: 0.41863 | val_1_rmse: 0.4217  |  0:00:44s
epoch 27 | loss: 0.18187 | val_0_rmse: 0.42181 | val_1_rmse: 0.418   |  0:00:46s
epoch 28 | loss: 0.17959 | val_0_rmse: 0.42028 | val_1_rmse: 0.41646 |  0:00:47s
epoch 29 | loss: 0.183   | val_0_rmse: 0.42355 | val_1_rmse: 0.4257  |  0:00:49s
epoch 30 | loss: 0.18176 | val_0_rmse: 0.42038 | val_1_rmse: 0.4199  |  0:00:50s
epoch 31 | loss: 0.18443 | val_0_rmse: 0.42578 | val_1_rmse: 0.4274  |  0:00:52s
epoch 32 | loss: 0.18212 | val_0_rmse: 0.41421 | val_1_rmse: 0.4121  |  0:00:54s
epoch 33 | loss: 0.18192 | val_0_rmse: 0.41823 | val_1_rmse: 0.41543 |  0:00:55s
epoch 34 | loss: 0.17822 | val_0_rmse: 0.40893 | val_1_rmse: 0.4072  |  0:00:57s
epoch 35 | loss: 0.1788  | val_0_rmse: 0.41531 | val_1_rmse: 0.41284 |  0:00:59s
epoch 36 | loss: 0.18235 | val_0_rmse: 0.427   | val_1_rmse: 0.43068 |  0:01:00s
epoch 37 | loss: 0.18249 | val_0_rmse: 0.42058 | val_1_rmse: 0.42602 |  0:01:02s
epoch 38 | loss: 0.18397 | val_0_rmse: 0.42285 | val_1_rmse: 0.42423 |  0:01:04s
epoch 39 | loss: 0.18103 | val_0_rmse: 0.40965 | val_1_rmse: 0.41234 |  0:01:05s
epoch 40 | loss: 0.17831 | val_0_rmse: 0.41053 | val_1_rmse: 0.41481 |  0:01:07s
epoch 41 | loss: 0.17584 | val_0_rmse: 0.40657 | val_1_rmse: 0.40768 |  0:01:09s
epoch 42 | loss: 0.17724 | val_0_rmse: 0.40857 | val_1_rmse: 0.40532 |  0:01:10s
epoch 43 | loss: 0.17398 | val_0_rmse: 0.40553 | val_1_rmse: 0.40427 |  0:01:12s
epoch 44 | loss: 0.17925 | val_0_rmse: 0.41092 | val_1_rmse: 0.4108  |  0:01:13s
epoch 45 | loss: 0.17873 | val_0_rmse: 0.40952 | val_1_rmse: 0.40913 |  0:01:15s
epoch 46 | loss: 0.17603 | val_0_rmse: 0.40471 | val_1_rmse: 0.40561 |  0:01:17s
epoch 47 | loss: 0.17283 | val_0_rmse: 0.40603 | val_1_rmse: 0.40728 |  0:01:18s
epoch 48 | loss: 0.1705  | val_0_rmse: 0.40878 | val_1_rmse: 0.40711 |  0:01:20s
epoch 49 | loss: 0.17684 | val_0_rmse: 0.47842 | val_1_rmse: 0.42271 |  0:01:22s
epoch 50 | loss: 0.17582 | val_0_rmse: 0.40926 | val_1_rmse: 0.40462 |  0:01:23s
epoch 51 | loss: 0.17131 | val_0_rmse: 0.40289 | val_1_rmse: 0.40207 |  0:01:25s
epoch 52 | loss: 0.16868 | val_0_rmse: 0.40602 | val_1_rmse: 0.40272 |  0:01:27s
epoch 53 | loss: 0.17062 | val_0_rmse: 0.40778 | val_1_rmse: 0.40276 |  0:01:28s
epoch 54 | loss: 0.17594 | val_0_rmse: 0.41207 | val_1_rmse: 0.40923 |  0:01:30s
epoch 55 | loss: 0.17307 | val_0_rmse: 0.40143 | val_1_rmse: 0.40046 |  0:01:31s
epoch 56 | loss: 0.16963 | val_0_rmse: 0.42143 | val_1_rmse: 0.41593 |  0:01:33s
epoch 57 | loss: 0.17082 | val_0_rmse: 0.40662 | val_1_rmse: 0.40368 |  0:01:35s
epoch 58 | loss: 0.17175 | val_0_rmse: 0.40031 | val_1_rmse: 0.40055 |  0:01:36s
epoch 59 | loss: 0.17101 | val_0_rmse: 0.40891 | val_1_rmse: 0.40872 |  0:01:38s
epoch 60 | loss: 0.17168 | val_0_rmse: 0.40711 | val_1_rmse: 0.4039  |  0:01:40s
epoch 61 | loss: 0.17002 | val_0_rmse: 0.40382 | val_1_rmse: 0.40117 |  0:01:41s
epoch 62 | loss: 0.16862 | val_0_rmse: 0.40789 | val_1_rmse: 0.40586 |  0:01:43s
epoch 63 | loss: 0.17453 | val_0_rmse: 0.4007  | val_1_rmse: 0.39882 |  0:01:45s
epoch 64 | loss: 0.16865 | val_0_rmse: 0.40992 | val_1_rmse: 0.41035 |  0:01:46s
epoch 65 | loss: 0.1694  | val_0_rmse: 0.41006 | val_1_rmse: 0.41049 |  0:01:48s
epoch 66 | loss: 0.17044 | val_0_rmse: 0.41104 | val_1_rmse: 0.41258 |  0:01:49s
epoch 67 | loss: 0.16948 | val_0_rmse: 0.41502 | val_1_rmse: 0.4485  |  0:01:51s
epoch 68 | loss: 0.17087 | val_0_rmse: 0.41044 | val_1_rmse: 0.4219  |  0:01:53s
epoch 69 | loss: 0.17084 | val_0_rmse: 0.41741 | val_1_rmse: 0.42261 |  0:01:54s
epoch 70 | loss: 0.17368 | val_0_rmse: 0.41376 | val_1_rmse: 0.42067 |  0:01:56s
epoch 71 | loss: 0.1674  | val_0_rmse: 0.40465 | val_1_rmse: 0.40917 |  0:01:58s
epoch 72 | loss: 0.16847 | val_0_rmse: 0.40923 | val_1_rmse: 0.41643 |  0:01:59s
epoch 73 | loss: 0.17318 | val_0_rmse: 0.40931 | val_1_rmse: 0.41027 |  0:02:01s
epoch 74 | loss: 0.17461 | val_0_rmse: 0.40603 | val_1_rmse: 0.40942 |  0:02:03s
epoch 75 | loss: 0.17973 | val_0_rmse: 0.41938 | val_1_rmse: 0.41721 |  0:02:04s
epoch 76 | loss: 0.17799 | val_0_rmse: 0.42328 | val_1_rmse: 0.42211 |  0:02:06s
epoch 77 | loss: 0.17824 | val_0_rmse: 0.43649 | val_1_rmse: 0.44209 |  0:02:08s
epoch 78 | loss: 0.17954 | val_0_rmse: 0.41266 | val_1_rmse: 0.41881 |  0:02:09s
epoch 79 | loss: 0.17678 | val_0_rmse: 0.44996 | val_1_rmse: 0.43404 |  0:02:11s
epoch 80 | loss: 0.19292 | val_0_rmse: 0.42697 | val_1_rmse: 0.42228 |  0:02:12s
epoch 81 | loss: 0.18817 | val_0_rmse: 0.42123 | val_1_rmse: 0.41744 |  0:02:14s
epoch 82 | loss: 0.17902 | val_0_rmse: 0.41609 | val_1_rmse: 0.411   |  0:02:16s
epoch 83 | loss: 0.17592 | val_0_rmse: 0.40983 | val_1_rmse: 0.40703 |  0:02:17s
epoch 84 | loss: 0.17906 | val_0_rmse: 0.42453 | val_1_rmse: 0.41812 |  0:02:19s
epoch 85 | loss: 0.17496 | val_0_rmse: 0.40418 | val_1_rmse: 0.40275 |  0:02:21s
epoch 86 | loss: 0.17198 | val_0_rmse: 0.41246 | val_1_rmse: 0.40658 |  0:02:22s
epoch 87 | loss: 0.1775  | val_0_rmse: 0.41622 | val_1_rmse: 0.40907 |  0:02:24s
epoch 88 | loss: 0.17542 | val_0_rmse: 0.41278 | val_1_rmse: 0.40898 |  0:02:26s
epoch 89 | loss: 0.17149 | val_0_rmse: 0.41111 | val_1_rmse: 0.40777 |  0:02:27s
epoch 90 | loss: 0.17126 | val_0_rmse: 0.40681 | val_1_rmse: 0.40858 |  0:02:29s
epoch 91 | loss: 0.1696  | val_0_rmse: 0.4015  | val_1_rmse: 0.4044  |  0:02:31s
epoch 92 | loss: 0.17329 | val_0_rmse: 0.40521 | val_1_rmse: 0.40912 |  0:02:32s
epoch 93 | loss: 0.16827 | val_0_rmse: 0.40783 | val_1_rmse: 0.41103 |  0:02:34s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.39882
Best weights from best epoch are automatically used!
ended training at: 16:29:14
Feature importance:
Mean squared error is of 856944118.7184722
Mean absolute error:19687.155573483073
MAPE:0.21631925015010856
R2 score:0.8087703035760834
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:29:14
epoch 0  | loss: 1.57009 | val_0_rmse: 0.97527 | val_1_rmse: 0.97389 |  0:00:01s
epoch 1  | loss: 0.48287 | val_0_rmse: 0.73997 | val_1_rmse: 0.75548 |  0:00:03s
epoch 2  | loss: 0.28642 | val_0_rmse: 0.63817 | val_1_rmse: 0.64967 |  0:00:04s
epoch 3  | loss: 0.25455 | val_0_rmse: 0.66599 | val_1_rmse: 0.67866 |  0:00:06s
epoch 4  | loss: 0.24315 | val_0_rmse: 0.68481 | val_1_rmse: 0.70601 |  0:00:08s
epoch 5  | loss: 0.24015 | val_0_rmse: 0.54761 | val_1_rmse: 0.56111 |  0:00:09s
epoch 6  | loss: 0.23723 | val_0_rmse: 0.54274 | val_1_rmse: 0.55375 |  0:00:11s
epoch 7  | loss: 0.2383  | val_0_rmse: 0.54654 | val_1_rmse: 0.55601 |  0:00:13s
epoch 8  | loss: 0.22921 | val_0_rmse: 0.53061 | val_1_rmse: 0.55081 |  0:00:14s
epoch 9  | loss: 0.22035 | val_0_rmse: 0.53418 | val_1_rmse: 0.54166 |  0:00:16s
epoch 10 | loss: 0.2184  | val_0_rmse: 0.48677 | val_1_rmse: 0.4946  |  0:00:18s
epoch 11 | loss: 0.21154 | val_0_rmse: 0.48917 | val_1_rmse: 0.49635 |  0:00:19s
epoch 12 | loss: 0.20435 | val_0_rmse: 0.48153 | val_1_rmse: 0.49011 |  0:00:21s
epoch 13 | loss: 0.19575 | val_0_rmse: 0.47842 | val_1_rmse: 0.48495 |  0:00:22s
epoch 14 | loss: 0.19344 | val_0_rmse: 0.46548 | val_1_rmse: 0.46903 |  0:00:24s
epoch 15 | loss: 0.19572 | val_0_rmse: 0.48078 | val_1_rmse: 0.49291 |  0:00:26s
epoch 16 | loss: 0.19452 | val_0_rmse: 0.50533 | val_1_rmse: 0.51106 |  0:00:27s
epoch 17 | loss: 0.21003 | val_0_rmse: 0.46776 | val_1_rmse: 0.46857 |  0:00:29s
epoch 18 | loss: 0.20216 | val_0_rmse: 0.45449 | val_1_rmse: 0.46438 |  0:00:31s
epoch 19 | loss: 0.19619 | val_0_rmse: 0.44538 | val_1_rmse: 0.45025 |  0:00:32s
epoch 20 | loss: 0.19243 | val_0_rmse: 0.45988 | val_1_rmse: 0.46676 |  0:00:34s
epoch 21 | loss: 0.19016 | val_0_rmse: 0.43965 | val_1_rmse: 0.44639 |  0:00:36s
epoch 22 | loss: 0.18488 | val_0_rmse: 0.439   | val_1_rmse: 0.44222 |  0:00:37s
epoch 23 | loss: 0.18322 | val_0_rmse: 0.4439  | val_1_rmse: 0.44598 |  0:00:39s
epoch 24 | loss: 0.18943 | val_0_rmse: 0.44792 | val_1_rmse: 0.45524 |  0:00:41s
epoch 25 | loss: 0.18923 | val_0_rmse: 0.43152 | val_1_rmse: 0.4355  |  0:00:42s
epoch 26 | loss: 0.1884  | val_0_rmse: 0.42106 | val_1_rmse: 0.42704 |  0:00:44s
epoch 27 | loss: 0.18031 | val_0_rmse: 0.41621 | val_1_rmse: 0.41951 |  0:00:45s
epoch 28 | loss: 0.18618 | val_0_rmse: 0.43133 | val_1_rmse: 0.43822 |  0:00:47s
epoch 29 | loss: 0.18987 | val_0_rmse: 0.41765 | val_1_rmse: 0.42063 |  0:00:49s
epoch 30 | loss: 0.18558 | val_0_rmse: 0.41364 | val_1_rmse: 0.41775 |  0:00:50s
epoch 31 | loss: 0.17987 | val_0_rmse: 0.41237 | val_1_rmse: 0.41226 |  0:00:52s
epoch 32 | loss: 0.17838 | val_0_rmse: 0.43161 | val_1_rmse: 0.43505 |  0:00:54s
epoch 33 | loss: 0.17603 | val_0_rmse: 0.40829 | val_1_rmse: 0.41443 |  0:00:55s
epoch 34 | loss: 0.17885 | val_0_rmse: 0.42534 | val_1_rmse: 0.43414 |  0:00:57s
epoch 35 | loss: 0.18106 | val_0_rmse: 0.42502 | val_1_rmse: 0.42866 |  0:00:59s
epoch 36 | loss: 0.17734 | val_0_rmse: 0.45162 | val_1_rmse: 0.4585  |  0:01:00s
epoch 37 | loss: 0.17779 | val_0_rmse: 0.40529 | val_1_rmse: 0.41382 |  0:01:02s
epoch 38 | loss: 0.17475 | val_0_rmse: 0.40912 | val_1_rmse: 0.41753 |  0:01:03s
epoch 39 | loss: 0.16957 | val_0_rmse: 0.40188 | val_1_rmse: 0.41017 |  0:01:05s
epoch 40 | loss: 0.16855 | val_0_rmse: 0.40622 | val_1_rmse: 0.41624 |  0:01:07s
epoch 41 | loss: 0.17285 | val_0_rmse: 0.40997 | val_1_rmse: 0.41995 |  0:01:08s
epoch 42 | loss: 0.17326 | val_0_rmse: 0.39706 | val_1_rmse: 0.40406 |  0:01:10s
epoch 43 | loss: 0.17504 | val_0_rmse: 0.41206 | val_1_rmse: 0.41361 |  0:01:12s
epoch 44 | loss: 0.17361 | val_0_rmse: 0.41197 | val_1_rmse: 0.4182  |  0:01:13s
epoch 45 | loss: 0.16952 | val_0_rmse: 0.4011  | val_1_rmse: 0.40622 |  0:01:15s
epoch 46 | loss: 0.17828 | val_0_rmse: 0.41151 | val_1_rmse: 0.41908 |  0:01:17s
epoch 47 | loss: 0.17598 | val_0_rmse: 0.40806 | val_1_rmse: 0.41723 |  0:01:18s
epoch 48 | loss: 0.19065 | val_0_rmse: 0.4257  | val_1_rmse: 0.43482 |  0:01:20s
epoch 49 | loss: 0.18746 | val_0_rmse: 0.41435 | val_1_rmse: 0.41683 |  0:01:22s
epoch 50 | loss: 0.17918 | val_0_rmse: 0.43825 | val_1_rmse: 0.43945 |  0:01:23s
epoch 51 | loss: 0.17357 | val_0_rmse: 0.40012 | val_1_rmse: 0.40789 |  0:01:25s
epoch 52 | loss: 0.16912 | val_0_rmse: 0.40059 | val_1_rmse: 0.40965 |  0:01:27s
epoch 53 | loss: 0.16991 | val_0_rmse: 0.4022  | val_1_rmse: 0.41144 |  0:01:28s
epoch 54 | loss: 0.16876 | val_0_rmse: 0.39548 | val_1_rmse: 0.4024  |  0:01:30s
epoch 55 | loss: 0.1671  | val_0_rmse: 0.39733 | val_1_rmse: 0.40395 |  0:01:31s
epoch 56 | loss: 0.16819 | val_0_rmse: 0.39783 | val_1_rmse: 0.40212 |  0:01:33s
epoch 57 | loss: 0.16818 | val_0_rmse: 0.41815 | val_1_rmse: 0.42401 |  0:01:35s
epoch 58 | loss: 0.16754 | val_0_rmse: 0.39947 | val_1_rmse: 0.40763 |  0:01:36s
epoch 59 | loss: 0.16878 | val_0_rmse: 0.40728 | val_1_rmse: 0.41579 |  0:01:38s
epoch 60 | loss: 0.17207 | val_0_rmse: 0.40036 | val_1_rmse: 0.40979 |  0:01:40s
epoch 61 | loss: 0.17179 | val_0_rmse: 0.40758 | val_1_rmse: 0.41812 |  0:01:41s
epoch 62 | loss: 0.16851 | val_0_rmse: 0.3978  | val_1_rmse: 0.40814 |  0:01:43s
epoch 63 | loss: 0.17129 | val_0_rmse: 0.41327 | val_1_rmse: 0.42345 |  0:01:44s
epoch 64 | loss: 0.1719  | val_0_rmse: 0.42358 | val_1_rmse: 0.42788 |  0:01:46s
epoch 65 | loss: 0.17215 | val_0_rmse: 0.4041  | val_1_rmse: 0.41157 |  0:01:48s
epoch 66 | loss: 0.16769 | val_0_rmse: 0.40781 | val_1_rmse: 0.41708 |  0:01:49s
epoch 67 | loss: 0.16652 | val_0_rmse: 0.39456 | val_1_rmse: 0.40344 |  0:01:51s
epoch 68 | loss: 0.16434 | val_0_rmse: 0.39646 | val_1_rmse: 0.40527 |  0:01:53s
epoch 69 | loss: 0.1643  | val_0_rmse: 0.40366 | val_1_rmse: 0.41519 |  0:01:54s
epoch 70 | loss: 0.16372 | val_0_rmse: 0.40108 | val_1_rmse: 0.41101 |  0:01:56s
epoch 71 | loss: 0.16587 | val_0_rmse: 0.41089 | val_1_rmse: 0.41247 |  0:01:58s
epoch 72 | loss: 0.16593 | val_0_rmse: 0.41345 | val_1_rmse: 0.41725 |  0:01:59s
epoch 73 | loss: 0.17003 | val_0_rmse: 0.41283 | val_1_rmse: 0.4195  |  0:02:01s
epoch 74 | loss: 0.16549 | val_0_rmse: 0.40101 | val_1_rmse: 0.40708 |  0:02:02s
epoch 75 | loss: 0.18777 | val_0_rmse: 0.43488 | val_1_rmse: 0.4423  |  0:02:04s
epoch 76 | loss: 0.19338 | val_0_rmse: 0.43367 | val_1_rmse: 0.46224 |  0:02:06s
epoch 77 | loss: 0.18985 | val_0_rmse: 0.41843 | val_1_rmse: 0.41982 |  0:02:07s
epoch 78 | loss: 0.20423 | val_0_rmse: 0.44643 | val_1_rmse: 0.44956 |  0:02:09s
epoch 79 | loss: 0.1888  | val_0_rmse: 0.44691 | val_1_rmse: 0.45337 |  0:02:11s
epoch 80 | loss: 0.1831  | val_0_rmse: 0.41628 | val_1_rmse: 0.42807 |  0:02:12s
epoch 81 | loss: 0.19186 | val_0_rmse: 0.46062 | val_1_rmse: 0.46569 |  0:02:14s
epoch 82 | loss: 0.20489 | val_0_rmse: 0.44858 | val_1_rmse: 0.4532  |  0:02:16s
epoch 83 | loss: 0.1991  | val_0_rmse: 0.42349 | val_1_rmse: 0.43406 |  0:02:17s
epoch 84 | loss: 0.18214 | val_0_rmse: 0.41184 | val_1_rmse: 0.43034 |  0:02:19s
epoch 85 | loss: 0.17893 | val_0_rmse: 0.40882 | val_1_rmse: 0.42108 |  0:02:21s
epoch 86 | loss: 0.17831 | val_0_rmse: 0.40995 | val_1_rmse: 0.42208 |  0:02:22s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.40212
Best weights from best epoch are automatically used!
ended training at: 16:31:38
Feature importance:
Mean squared error is of 892862620.4401337
Mean absolute error:19554.377845056053
MAPE:0.21117496034199926
R2 score:0.7966521673705641
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:32:03
epoch 0  | loss: 0.59849 | val_0_rmse: 0.71268 | val_1_rmse: 0.71321 |  0:00:03s
epoch 1  | loss: 0.36205 | val_0_rmse: 0.68644 | val_1_rmse: 0.68945 |  0:00:06s
epoch 2  | loss: 0.32008 | val_0_rmse: 0.60295 | val_1_rmse: 0.59896 |  0:00:10s
epoch 3  | loss: 0.32255 | val_0_rmse: 0.59458 | val_1_rmse: 0.59545 |  0:00:13s
epoch 4  | loss: 0.29115 | val_0_rmse: 0.56975 | val_1_rmse: 0.57228 |  0:00:17s
epoch 5  | loss: 0.28639 | val_0_rmse: 0.5409  | val_1_rmse: 0.53743 |  0:00:20s
epoch 6  | loss: 0.28167 | val_0_rmse: 0.50842 | val_1_rmse: 0.50806 |  0:00:23s
epoch 7  | loss: 0.2638  | val_0_rmse: 0.50911 | val_1_rmse: 0.51001 |  0:00:27s
epoch 8  | loss: 0.26152 | val_0_rmse: 0.5008  | val_1_rmse: 0.50199 |  0:00:30s
epoch 9  | loss: 0.25856 | val_0_rmse: 0.49639 | val_1_rmse: 0.49356 |  0:00:34s
epoch 10 | loss: 0.25766 | val_0_rmse: 0.50017 | val_1_rmse: 0.49665 |  0:00:37s
epoch 11 | loss: 0.25671 | val_0_rmse: 0.55136 | val_1_rmse: 0.5501  |  0:00:40s
epoch 12 | loss: 0.2731  | val_0_rmse: 0.51269 | val_1_rmse: 0.51696 |  0:00:44s
epoch 13 | loss: 0.2733  | val_0_rmse: 0.50132 | val_1_rmse: 0.50792 |  0:00:47s
epoch 14 | loss: 0.27039 | val_0_rmse: 0.50482 | val_1_rmse: 0.50463 |  0:00:51s
epoch 15 | loss: 0.27272 | val_0_rmse: 0.53355 | val_1_rmse: 0.53655 |  0:00:54s
epoch 16 | loss: 0.26145 | val_0_rmse: 0.48994 | val_1_rmse: 0.49087 |  0:00:58s
epoch 17 | loss: 0.2589  | val_0_rmse: 0.49214 | val_1_rmse: 0.49125 |  0:01:01s
epoch 18 | loss: 0.24983 | val_0_rmse: 0.48964 | val_1_rmse: 0.48958 |  0:01:04s
epoch 19 | loss: 0.26072 | val_0_rmse: 0.53513 | val_1_rmse: 0.53371 |  0:01:08s
epoch 20 | loss: 0.27257 | val_0_rmse: 0.51433 | val_1_rmse: 0.51566 |  0:01:11s
epoch 21 | loss: 0.26734 | val_0_rmse: 0.54421 | val_1_rmse: 0.54163 |  0:01:15s
epoch 22 | loss: 0.27119 | val_0_rmse: 0.52274 | val_1_rmse: 0.51834 |  0:01:18s
epoch 23 | loss: 0.29175 | val_0_rmse: 0.53116 | val_1_rmse: 0.53179 |  0:01:21s
epoch 24 | loss: 0.29289 | val_0_rmse: 0.5388  | val_1_rmse: 0.53998 |  0:01:25s
epoch 25 | loss: 0.25688 | val_0_rmse: 0.49316 | val_1_rmse: 0.49263 |  0:01:28s
epoch 26 | loss: 0.28518 | val_0_rmse: 0.54511 | val_1_rmse: 0.54502 |  0:01:32s
epoch 27 | loss: 0.29312 | val_0_rmse: 0.52571 | val_1_rmse: 0.52305 |  0:01:35s
epoch 28 | loss: 0.26164 | val_0_rmse: 0.48132 | val_1_rmse: 0.47801 |  0:01:38s
epoch 29 | loss: 0.24436 | val_0_rmse: 0.48774 | val_1_rmse: 0.48492 |  0:01:42s
epoch 30 | loss: 0.24024 | val_0_rmse: 0.47343 | val_1_rmse: 0.47166 |  0:01:45s
epoch 31 | loss: 0.23692 | val_0_rmse: 0.46708 | val_1_rmse: 0.46543 |  0:01:49s
epoch 32 | loss: 0.23495 | val_0_rmse: 0.46615 | val_1_rmse: 0.46483 |  0:01:52s
epoch 33 | loss: 0.23615 | val_0_rmse: 0.47897 | val_1_rmse: 0.47804 |  0:01:56s
epoch 34 | loss: 0.23294 | val_0_rmse: 0.48345 | val_1_rmse: 0.48231 |  0:01:59s
epoch 35 | loss: 0.23102 | val_0_rmse: 0.46742 | val_1_rmse: 0.46572 |  0:02:02s
epoch 36 | loss: 0.23033 | val_0_rmse: 0.46223 | val_1_rmse: 0.46059 |  0:02:06s
epoch 37 | loss: 0.23282 | val_0_rmse: 0.46693 | val_1_rmse: 0.46463 |  0:02:09s
epoch 38 | loss: 0.2272  | val_0_rmse: 0.46055 | val_1_rmse: 0.45772 |  0:02:13s
epoch 39 | loss: 0.22696 | val_0_rmse: 0.47186 | val_1_rmse: 0.47008 |  0:02:16s
epoch 40 | loss: 0.22775 | val_0_rmse: 0.46186 | val_1_rmse: 0.45968 |  0:02:19s
epoch 41 | loss: 0.22673 | val_0_rmse: 0.46803 | val_1_rmse: 0.46587 |  0:02:23s
epoch 42 | loss: 0.22694 | val_0_rmse: 0.47132 | val_1_rmse: 0.4712  |  0:02:26s
epoch 43 | loss: 0.22863 | val_0_rmse: 0.46362 | val_1_rmse: 0.46155 |  0:02:30s
epoch 44 | loss: 0.22619 | val_0_rmse: 0.45973 | val_1_rmse: 0.4585  |  0:02:33s
epoch 45 | loss: 0.22449 | val_0_rmse: 0.4562  | val_1_rmse: 0.4561  |  0:02:37s
epoch 46 | loss: 0.22198 | val_0_rmse: 0.46732 | val_1_rmse: 0.466   |  0:02:40s
epoch 47 | loss: 0.22405 | val_0_rmse: 0.45976 | val_1_rmse: 0.45869 |  0:02:44s
epoch 48 | loss: 0.22099 | val_0_rmse: 0.46073 | val_1_rmse: 0.46299 |  0:02:47s
epoch 49 | loss: 0.22058 | val_0_rmse: 0.45648 | val_1_rmse: 0.45693 |  0:02:50s
epoch 50 | loss: 0.22055 | val_0_rmse: 0.45197 | val_1_rmse: 0.45123 |  0:02:54s
epoch 51 | loss: 0.22106 | val_0_rmse: 0.47128 | val_1_rmse: 0.47013 |  0:02:57s
epoch 52 | loss: 0.22062 | val_0_rmse: 0.46816 | val_1_rmse: 0.46911 |  0:03:01s
epoch 53 | loss: 0.21717 | val_0_rmse: 0.45239 | val_1_rmse: 0.45272 |  0:03:04s
epoch 54 | loss: 0.22133 | val_0_rmse: 0.47344 | val_1_rmse: 0.47252 |  0:03:07s
epoch 55 | loss: 0.21725 | val_0_rmse: 0.45285 | val_1_rmse: 0.45354 |  0:03:11s
epoch 56 | loss: 0.21951 | val_0_rmse: 0.45304 | val_1_rmse: 0.45329 |  0:03:14s
epoch 57 | loss: 0.2172  | val_0_rmse: 0.45137 | val_1_rmse: 0.45105 |  0:03:18s
epoch 58 | loss: 0.21552 | val_0_rmse: 0.45328 | val_1_rmse: 0.45473 |  0:03:21s
epoch 59 | loss: 0.21805 | val_0_rmse: 0.45783 | val_1_rmse: 0.45866 |  0:03:24s
epoch 60 | loss: 0.21752 | val_0_rmse: 0.45789 | val_1_rmse: 0.45763 |  0:03:28s
epoch 61 | loss: 0.21409 | val_0_rmse: 0.44811 | val_1_rmse: 0.44905 |  0:03:31s
epoch 62 | loss: 0.213   | val_0_rmse: 0.45929 | val_1_rmse: 0.45997 |  0:03:35s
epoch 63 | loss: 0.21353 | val_0_rmse: 0.45045 | val_1_rmse: 0.45143 |  0:03:38s
epoch 64 | loss: 0.21612 | val_0_rmse: 0.45053 | val_1_rmse: 0.45028 |  0:03:42s
epoch 65 | loss: 0.21569 | val_0_rmse: 0.45357 | val_1_rmse: 0.45352 |  0:03:45s
epoch 66 | loss: 0.21649 | val_0_rmse: 0.4591  | val_1_rmse: 0.45942 |  0:03:48s
epoch 67 | loss: 0.21281 | val_0_rmse: 0.4513  | val_1_rmse: 0.45026 |  0:03:52s
epoch 68 | loss: 0.21322 | val_0_rmse: 0.44931 | val_1_rmse: 0.44844 |  0:03:55s
epoch 69 | loss: 0.21025 | val_0_rmse: 0.45249 | val_1_rmse: 0.45318 |  0:03:59s
epoch 70 | loss: 0.21382 | val_0_rmse: 0.46011 | val_1_rmse: 0.46125 |  0:04:02s
epoch 71 | loss: 0.20969 | val_0_rmse: 0.44968 | val_1_rmse: 0.45051 |  0:04:05s
epoch 72 | loss: 0.21466 | val_0_rmse: 0.46707 | val_1_rmse: 0.46782 |  0:04:09s
epoch 73 | loss: 0.21122 | val_0_rmse: 0.45516 | val_1_rmse: 0.45672 |  0:04:12s
epoch 74 | loss: 0.21263 | val_0_rmse: 0.44624 | val_1_rmse: 0.44699 |  0:04:16s
epoch 75 | loss: 0.20934 | val_0_rmse: 0.45444 | val_1_rmse: 0.4561  |  0:04:19s
epoch 76 | loss: 0.21127 | val_0_rmse: 0.45419 | val_1_rmse: 0.45662 |  0:04:22s
epoch 77 | loss: 0.21053 | val_0_rmse: 0.47283 | val_1_rmse: 0.47661 |  0:04:26s
epoch 78 | loss: 0.20866 | val_0_rmse: 0.44698 | val_1_rmse: 0.44918 |  0:04:29s
epoch 79 | loss: 0.21441 | val_0_rmse: 0.45632 | val_1_rmse: 0.45714 |  0:04:33s
epoch 80 | loss: 0.21255 | val_0_rmse: 0.45158 | val_1_rmse: 0.4533  |  0:04:36s
epoch 81 | loss: 0.21322 | val_0_rmse: 0.44374 | val_1_rmse: 0.44597 |  0:04:40s
epoch 82 | loss: 0.2083  | val_0_rmse: 0.44602 | val_1_rmse: 0.4496  |  0:04:43s
epoch 83 | loss: 0.21091 | val_0_rmse: 0.46302 | val_1_rmse: 0.46471 |  0:04:46s
epoch 84 | loss: 0.20964 | val_0_rmse: 0.44443 | val_1_rmse: 0.4451  |  0:04:50s
epoch 85 | loss: 0.20791 | val_0_rmse: 0.44378 | val_1_rmse: 0.4454  |  0:04:53s
epoch 86 | loss: 0.20598 | val_0_rmse: 0.43686 | val_1_rmse: 0.43966 |  0:04:57s
epoch 87 | loss: 0.20481 | val_0_rmse: 0.46124 | val_1_rmse: 0.46313 |  0:05:01s
epoch 88 | loss: 0.2042  | val_0_rmse: 0.43816 | val_1_rmse: 0.44009 |  0:05:04s
epoch 89 | loss: 0.20397 | val_0_rmse: 0.43952 | val_1_rmse: 0.44153 |  0:05:07s
epoch 90 | loss: 0.20695 | val_0_rmse: 0.44359 | val_1_rmse: 0.44428 |  0:05:11s
epoch 91 | loss: 0.20336 | val_0_rmse: 0.43811 | val_1_rmse: 0.44049 |  0:05:14s
epoch 92 | loss: 0.20527 | val_0_rmse: 0.47104 | val_1_rmse: 0.47314 |  0:05:18s
epoch 93 | loss: 0.2036  | val_0_rmse: 0.43926 | val_1_rmse: 0.44188 |  0:05:21s
epoch 94 | loss: 0.20387 | val_0_rmse: 0.4536  | val_1_rmse: 0.45487 |  0:05:24s
epoch 95 | loss: 0.20293 | val_0_rmse: 0.43805 | val_1_rmse: 0.44034 |  0:05:28s
epoch 96 | loss: 0.20526 | val_0_rmse: 0.45705 | val_1_rmse: 0.46123 |  0:05:31s
epoch 97 | loss: 0.20509 | val_0_rmse: 0.44673 | val_1_rmse: 0.44718 |  0:05:35s
epoch 98 | loss: 0.20068 | val_0_rmse: 0.44499 | val_1_rmse: 0.4462  |  0:05:38s
epoch 99 | loss: 0.20061 | val_0_rmse: 0.43604 | val_1_rmse: 0.44031 |  0:05:41s
epoch 100| loss: 0.19962 | val_0_rmse: 0.43832 | val_1_rmse: 0.44401 |  0:05:45s
epoch 101| loss: 0.20153 | val_0_rmse: 0.45193 | val_1_rmse: 0.45633 |  0:05:48s
epoch 102| loss: 0.21153 | val_0_rmse: 0.45657 | val_1_rmse: 0.45969 |  0:05:52s
epoch 103| loss: 0.2169  | val_0_rmse: 0.51789 | val_1_rmse: 0.52152 |  0:05:55s
epoch 104| loss: 0.21158 | val_0_rmse: 0.44115 | val_1_rmse: 0.44259 |  0:05:58s
epoch 105| loss: 0.20867 | val_0_rmse: 0.44914 | val_1_rmse: 0.45176 |  0:06:02s
epoch 106| loss: 0.2048  | val_0_rmse: 0.4353  | val_1_rmse: 0.44109 |  0:06:05s
epoch 107| loss: 0.20529 | val_0_rmse: 0.43566 | val_1_rmse: 0.44104 |  0:06:09s
epoch 108| loss: 0.20666 | val_0_rmse: 0.44903 | val_1_rmse: 0.45355 |  0:06:12s
epoch 109| loss: 0.20451 | val_0_rmse: 0.43632 | val_1_rmse: 0.44022 |  0:06:15s
epoch 110| loss: 0.20269 | val_0_rmse: 0.44235 | val_1_rmse: 0.44652 |  0:06:19s
epoch 111| loss: 0.20341 | val_0_rmse: 0.45535 | val_1_rmse: 0.45873 |  0:06:22s
epoch 112| loss: 0.20372 | val_0_rmse: 0.43873 | val_1_rmse: 0.44389 |  0:06:26s
epoch 113| loss: 0.20619 | val_0_rmse: 0.43989 | val_1_rmse: 0.44348 |  0:06:29s
epoch 114| loss: 0.2039  | val_0_rmse: 0.43953 | val_1_rmse: 0.44427 |  0:06:32s
epoch 115| loss: 0.2047  | val_0_rmse: 0.45822 | val_1_rmse: 0.46314 |  0:06:36s
epoch 116| loss: 0.20412 | val_0_rmse: 0.4447  | val_1_rmse: 0.44737 |  0:06:39s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.43966
Best weights from best epoch are automatically used!
ended training at: 16:38:44
Feature importance:
Mean squared error is of 9956899870.858599
Mean absolute error:68212.24655400329
MAPE:0.2625964198647353
R2 score:0.8297338176030985
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:38:45
epoch 0  | loss: 0.75482 | val_0_rmse: 0.72902 | val_1_rmse: 0.72946 |  0:00:03s
epoch 1  | loss: 0.41287 | val_0_rmse: 0.64554 | val_1_rmse: 0.64218 |  0:00:06s
epoch 2  | loss: 0.33682 | val_0_rmse: 0.63291 | val_1_rmse: 0.62902 |  0:00:10s
epoch 3  | loss: 0.30753 | val_0_rmse: 0.5783  | val_1_rmse: 0.57457 |  0:00:13s
epoch 4  | loss: 0.2809  | val_0_rmse: 0.5536  | val_1_rmse: 0.55368 |  0:00:17s
epoch 5  | loss: 0.27408 | val_0_rmse: 0.53247 | val_1_rmse: 0.53307 |  0:00:20s
epoch 6  | loss: 0.27247 | val_0_rmse: 0.52922 | val_1_rmse: 0.53013 |  0:00:24s
epoch 7  | loss: 0.26406 | val_0_rmse: 0.50756 | val_1_rmse: 0.51011 |  0:00:27s
epoch 8  | loss: 0.25453 | val_0_rmse: 0.49802 | val_1_rmse: 0.49904 |  0:00:30s
epoch 9  | loss: 0.24521 | val_0_rmse: 0.49339 | val_1_rmse: 0.49332 |  0:00:34s
epoch 10 | loss: 0.23995 | val_0_rmse: 0.4868  | val_1_rmse: 0.49033 |  0:00:37s
epoch 11 | loss: 0.23776 | val_0_rmse: 0.47307 | val_1_rmse: 0.4779  |  0:00:41s
epoch 12 | loss: 0.23399 | val_0_rmse: 0.47489 | val_1_rmse: 0.47673 |  0:00:44s
epoch 13 | loss: 0.22859 | val_0_rmse: 0.4688  | val_1_rmse: 0.4699  |  0:00:48s
epoch 14 | loss: 0.22957 | val_0_rmse: 0.45779 | val_1_rmse: 0.46133 |  0:00:51s
epoch 15 | loss: 0.22641 | val_0_rmse: 0.45713 | val_1_rmse: 0.46097 |  0:00:55s
epoch 16 | loss: 0.22451 | val_0_rmse: 0.44862 | val_1_rmse: 0.45512 |  0:00:58s
epoch 17 | loss: 0.2233  | val_0_rmse: 0.45445 | val_1_rmse: 0.46068 |  0:01:02s
epoch 18 | loss: 0.22512 | val_0_rmse: 0.44857 | val_1_rmse: 0.45403 |  0:01:05s
epoch 19 | loss: 0.21845 | val_0_rmse: 0.44929 | val_1_rmse: 0.45254 |  0:01:08s
epoch 20 | loss: 0.21619 | val_0_rmse: 0.4427  | val_1_rmse: 0.44912 |  0:01:12s
epoch 21 | loss: 0.21358 | val_0_rmse: 0.43979 | val_1_rmse: 0.44806 |  0:01:15s
epoch 22 | loss: 0.21037 | val_0_rmse: 0.43919 | val_1_rmse: 0.44586 |  0:01:19s
epoch 23 | loss: 0.21258 | val_0_rmse: 0.43882 | val_1_rmse: 0.44679 |  0:01:22s
epoch 24 | loss: 0.20942 | val_0_rmse: 0.43646 | val_1_rmse: 0.44335 |  0:01:25s
epoch 25 | loss: 0.2061  | val_0_rmse: 0.43894 | val_1_rmse: 0.44862 |  0:01:29s
epoch 26 | loss: 0.2079  | val_0_rmse: 0.43901 | val_1_rmse: 0.45009 |  0:01:32s
epoch 27 | loss: 0.20669 | val_0_rmse: 0.4331  | val_1_rmse: 0.44427 |  0:01:36s
epoch 28 | loss: 0.20357 | val_0_rmse: 0.43534 | val_1_rmse: 0.44591 |  0:01:39s
epoch 29 | loss: 0.20435 | val_0_rmse: 0.43102 | val_1_rmse: 0.44248 |  0:01:43s
epoch 30 | loss: 0.20912 | val_0_rmse: 0.4344  | val_1_rmse: 0.44409 |  0:01:46s
epoch 31 | loss: 0.2055  | val_0_rmse: 0.43169 | val_1_rmse: 0.44196 |  0:01:50s
epoch 32 | loss: 0.20428 | val_0_rmse: 0.44591 | val_1_rmse: 0.45595 |  0:01:53s
epoch 33 | loss: 0.20536 | val_0_rmse: 0.43169 | val_1_rmse: 0.43922 |  0:01:56s
epoch 34 | loss: 0.20224 | val_0_rmse: 0.46397 | val_1_rmse: 0.47184 |  0:02:00s
epoch 35 | loss: 0.20243 | val_0_rmse: 0.42853 | val_1_rmse: 0.43785 |  0:02:03s
epoch 36 | loss: 0.20319 | val_0_rmse: 0.44801 | val_1_rmse: 0.46098 |  0:02:07s
epoch 37 | loss: 0.20068 | val_0_rmse: 0.44042 | val_1_rmse: 0.45276 |  0:02:10s
epoch 38 | loss: 0.1985  | val_0_rmse: 0.4282  | val_1_rmse: 0.4388  |  0:02:14s
epoch 39 | loss: 0.19367 | val_0_rmse: 0.42219 | val_1_rmse: 0.43413 |  0:02:17s
epoch 40 | loss: 0.19745 | val_0_rmse: 0.42723 | val_1_rmse: 0.43991 |  0:02:21s
epoch 41 | loss: 0.19516 | val_0_rmse: 0.43227 | val_1_rmse: 0.44243 |  0:02:24s
epoch 42 | loss: 0.19709 | val_0_rmse: 0.43184 | val_1_rmse: 0.44141 |  0:02:27s
epoch 43 | loss: 0.1976  | val_0_rmse: 0.42656 | val_1_rmse: 0.44018 |  0:02:31s
epoch 44 | loss: 0.19527 | val_0_rmse: 0.42157 | val_1_rmse: 0.43665 |  0:02:34s
epoch 45 | loss: 0.193   | val_0_rmse: 0.41982 | val_1_rmse: 0.43348 |  0:02:38s
epoch 46 | loss: 0.19428 | val_0_rmse: 0.43638 | val_1_rmse: 0.44997 |  0:02:41s
epoch 47 | loss: 0.19612 | val_0_rmse: 0.42906 | val_1_rmse: 0.44195 |  0:02:45s
epoch 48 | loss: 0.19432 | val_0_rmse: 0.42112 | val_1_rmse: 0.43384 |  0:02:48s
epoch 49 | loss: 0.19504 | val_0_rmse: 0.41955 | val_1_rmse: 0.43408 |  0:02:51s
epoch 50 | loss: 0.19036 | val_0_rmse: 0.42398 | val_1_rmse: 0.43783 |  0:02:55s
epoch 51 | loss: 0.1891  | val_0_rmse: 0.42603 | val_1_rmse: 0.43977 |  0:02:58s
epoch 52 | loss: 0.19035 | val_0_rmse: 0.41513 | val_1_rmse: 0.43393 |  0:03:02s
epoch 53 | loss: 0.19559 | val_0_rmse: 0.42614 | val_1_rmse: 0.44108 |  0:03:05s
epoch 54 | loss: 0.18791 | val_0_rmse: 0.43076 | val_1_rmse: 0.44715 |  0:03:09s
epoch 55 | loss: 0.18834 | val_0_rmse: 0.41324 | val_1_rmse: 0.42896 |  0:03:12s
epoch 56 | loss: 0.18848 | val_0_rmse: 0.42037 | val_1_rmse: 0.43775 |  0:03:15s
epoch 57 | loss: 0.18943 | val_0_rmse: 0.42891 | val_1_rmse: 0.44674 |  0:03:19s
epoch 58 | loss: 0.18983 | val_0_rmse: 0.42381 | val_1_rmse: 0.43624 |  0:03:22s
epoch 59 | loss: 0.18772 | val_0_rmse: 0.44015 | val_1_rmse: 0.4562  |  0:03:26s
epoch 60 | loss: 0.18804 | val_0_rmse: 0.43639 | val_1_rmse: 0.4508  |  0:03:29s
epoch 61 | loss: 0.18621 | val_0_rmse: 0.41134 | val_1_rmse: 0.42758 |  0:03:33s
epoch 62 | loss: 0.18533 | val_0_rmse: 0.43357 | val_1_rmse: 0.45212 |  0:03:36s
epoch 63 | loss: 0.19104 | val_0_rmse: 0.41798 | val_1_rmse: 0.43418 |  0:03:40s
epoch 64 | loss: 0.18655 | val_0_rmse: 0.41299 | val_1_rmse: 0.43152 |  0:03:43s
epoch 65 | loss: 0.18572 | val_0_rmse: 0.41727 | val_1_rmse: 0.43282 |  0:03:46s
epoch 66 | loss: 0.18442 | val_0_rmse: 0.42725 | val_1_rmse: 0.44587 |  0:03:50s
epoch 67 | loss: 0.18263 | val_0_rmse: 0.41581 | val_1_rmse: 0.43213 |  0:03:53s
epoch 68 | loss: 0.18613 | val_0_rmse: 0.41443 | val_1_rmse: 0.43327 |  0:03:57s
epoch 69 | loss: 0.18672 | val_0_rmse: 0.42522 | val_1_rmse: 0.44511 |  0:04:00s
epoch 70 | loss: 0.18627 | val_0_rmse: 0.41281 | val_1_rmse: 0.43041 |  0:04:04s
epoch 71 | loss: 0.18525 | val_0_rmse: 0.41699 | val_1_rmse: 0.43215 |  0:04:07s
epoch 72 | loss: 0.18318 | val_0_rmse: 0.408   | val_1_rmse: 0.42727 |  0:04:10s
epoch 73 | loss: 0.1853  | val_0_rmse: 0.41885 | val_1_rmse: 0.4383  |  0:04:14s
epoch 74 | loss: 0.1846  | val_0_rmse: 0.43903 | val_1_rmse: 0.45752 |  0:04:17s
epoch 75 | loss: 0.18222 | val_0_rmse: 0.43094 | val_1_rmse: 0.45103 |  0:04:21s
epoch 76 | loss: 0.18375 | val_0_rmse: 0.40876 | val_1_rmse: 0.42942 |  0:04:24s
epoch 77 | loss: 0.18458 | val_0_rmse: 0.4109  | val_1_rmse: 0.43107 |  0:04:28s
epoch 78 | loss: 0.18622 | val_0_rmse: 0.42986 | val_1_rmse: 0.45137 |  0:04:31s
epoch 79 | loss: 0.1862  | val_0_rmse: 0.41211 | val_1_rmse: 0.43067 |  0:04:34s
epoch 80 | loss: 0.18209 | val_0_rmse: 0.40816 | val_1_rmse: 0.4281  |  0:04:38s
epoch 81 | loss: 0.18479 | val_0_rmse: 0.41223 | val_1_rmse: 0.43094 |  0:04:41s
epoch 82 | loss: 0.18174 | val_0_rmse: 0.41068 | val_1_rmse: 0.43272 |  0:04:45s
epoch 83 | loss: 0.18252 | val_0_rmse: 0.40784 | val_1_rmse: 0.42832 |  0:04:48s
epoch 84 | loss: 0.18175 | val_0_rmse: 0.40733 | val_1_rmse: 0.43101 |  0:04:52s
epoch 85 | loss: 0.17996 | val_0_rmse: 0.46248 | val_1_rmse: 0.48144 |  0:04:55s
epoch 86 | loss: 0.18219 | val_0_rmse: 0.41084 | val_1_rmse: 0.43186 |  0:04:59s
epoch 87 | loss: 0.18616 | val_0_rmse: 0.42315 | val_1_rmse: 0.44193 |  0:05:02s
epoch 88 | loss: 0.18442 | val_0_rmse: 0.44814 | val_1_rmse: 0.46718 |  0:05:05s
epoch 89 | loss: 0.18092 | val_0_rmse: 0.41084 | val_1_rmse: 0.43162 |  0:05:09s
epoch 90 | loss: 0.17957 | val_0_rmse: 0.4109  | val_1_rmse: 0.43578 |  0:05:12s
epoch 91 | loss: 0.17936 | val_0_rmse: 0.41197 | val_1_rmse: 0.43569 |  0:05:16s
epoch 92 | loss: 0.17826 | val_0_rmse: 0.41048 | val_1_rmse: 0.43224 |  0:05:19s
epoch 93 | loss: 0.17788 | val_0_rmse: 0.41425 | val_1_rmse: 0.43999 |  0:05:23s
epoch 94 | loss: 0.17985 | val_0_rmse: 0.41229 | val_1_rmse: 0.43909 |  0:05:26s
epoch 95 | loss: 0.17745 | val_0_rmse: 0.40317 | val_1_rmse: 0.42632 |  0:05:29s
epoch 96 | loss: 0.17669 | val_0_rmse: 0.40951 | val_1_rmse: 0.43479 |  0:05:33s
epoch 97 | loss: 0.17998 | val_0_rmse: 0.40611 | val_1_rmse: 0.42653 |  0:05:36s
epoch 98 | loss: 0.17966 | val_0_rmse: 0.4109  | val_1_rmse: 0.43258 |  0:05:40s
epoch 99 | loss: 0.17814 | val_0_rmse: 0.40372 | val_1_rmse: 0.42891 |  0:05:43s
epoch 100| loss: 0.17865 | val_0_rmse: 0.42178 | val_1_rmse: 0.44541 |  0:05:47s
epoch 101| loss: 0.17622 | val_0_rmse: 0.40329 | val_1_rmse: 0.42625 |  0:05:50s
epoch 102| loss: 0.18043 | val_0_rmse: 0.40937 | val_1_rmse: 0.4284  |  0:05:53s
epoch 103| loss: 0.18012 | val_0_rmse: 0.40793 | val_1_rmse: 0.43056 |  0:05:57s
epoch 104| loss: 0.1786  | val_0_rmse: 0.40352 | val_1_rmse: 0.42944 |  0:06:00s
epoch 105| loss: 0.17839 | val_0_rmse: 0.40469 | val_1_rmse: 0.43039 |  0:06:04s
epoch 106| loss: 0.17473 | val_0_rmse: 0.4069  | val_1_rmse: 0.43336 |  0:06:07s
epoch 107| loss: 0.17583 | val_0_rmse: 0.4055  | val_1_rmse: 0.43204 |  0:06:11s
epoch 108| loss: 0.17878 | val_0_rmse: 0.41254 | val_1_rmse: 0.43522 |  0:06:14s
epoch 109| loss: 0.17807 | val_0_rmse: 0.40175 | val_1_rmse: 0.4254  |  0:06:17s
epoch 110| loss: 0.17734 | val_0_rmse: 0.39907 | val_1_rmse: 0.42556 |  0:06:21s
epoch 111| loss: 0.17456 | val_0_rmse: 0.40518 | val_1_rmse: 0.43186 |  0:06:24s
epoch 112| loss: 0.17729 | val_0_rmse: 0.4271  | val_1_rmse: 0.45039 |  0:06:28s
epoch 113| loss: 0.17634 | val_0_rmse: 0.41415 | val_1_rmse: 0.43847 |  0:06:31s
epoch 114| loss: 0.17689 | val_0_rmse: 0.39731 | val_1_rmse: 0.42255 |  0:06:35s
epoch 115| loss: 0.17633 | val_0_rmse: 0.39687 | val_1_rmse: 0.42041 |  0:06:38s
epoch 116| loss: 0.17327 | val_0_rmse: 0.40514 | val_1_rmse: 0.42959 |  0:06:41s
epoch 117| loss: 0.17363 | val_0_rmse: 0.40268 | val_1_rmse: 0.42693 |  0:06:45s
epoch 118| loss: 0.17494 | val_0_rmse: 0.40354 | val_1_rmse: 0.42915 |  0:06:48s
epoch 119| loss: 0.17826 | val_0_rmse: 0.41032 | val_1_rmse: 0.43229 |  0:06:52s
epoch 120| loss: 0.17672 | val_0_rmse: 0.42127 | val_1_rmse: 0.44708 |  0:06:55s
epoch 121| loss: 0.17671 | val_0_rmse: 0.40843 | val_1_rmse: 0.43314 |  0:06:59s
epoch 122| loss: 0.17186 | val_0_rmse: 0.39939 | val_1_rmse: 0.42727 |  0:07:02s
epoch 123| loss: 0.17677 | val_0_rmse: 0.41142 | val_1_rmse: 0.44089 |  0:07:06s
epoch 124| loss: 0.17497 | val_0_rmse: 0.40748 | val_1_rmse: 0.4308  |  0:07:09s
epoch 125| loss: 0.17905 | val_0_rmse: 0.42503 | val_1_rmse: 0.44738 |  0:07:12s
epoch 126| loss: 0.17966 | val_0_rmse: 0.44449 | val_1_rmse: 0.46571 |  0:07:16s
epoch 127| loss: 0.18292 | val_0_rmse: 0.41383 | val_1_rmse: 0.43538 |  0:07:19s
epoch 128| loss: 0.18489 | val_0_rmse: 0.40393 | val_1_rmse: 0.42734 |  0:07:23s
epoch 129| loss: 0.17812 | val_0_rmse: 0.39707 | val_1_rmse: 0.42129 |  0:07:26s
epoch 130| loss: 0.17483 | val_0_rmse: 0.41065 | val_1_rmse: 0.43252 |  0:07:30s
epoch 131| loss: 0.17696 | val_0_rmse: 0.42462 | val_1_rmse: 0.44543 |  0:07:33s
epoch 132| loss: 0.1767  | val_0_rmse: 0.39918 | val_1_rmse: 0.4227  |  0:07:36s
epoch 133| loss: 0.17043 | val_0_rmse: 0.39772 | val_1_rmse: 0.42424 |  0:07:40s
epoch 134| loss: 0.17008 | val_0_rmse: 0.39443 | val_1_rmse: 0.4228  |  0:07:43s
epoch 135| loss: 0.17096 | val_0_rmse: 0.40012 | val_1_rmse: 0.42806 |  0:07:47s
epoch 136| loss: 0.17147 | val_0_rmse: 0.40055 | val_1_rmse: 0.42838 |  0:07:50s
epoch 137| loss: 0.17225 | val_0_rmse: 0.39992 | val_1_rmse: 0.42534 |  0:07:54s
epoch 138| loss: 0.1717  | val_0_rmse: 0.4126  | val_1_rmse: 0.43606 |  0:07:57s
epoch 139| loss: 0.17247 | val_0_rmse: 0.39398 | val_1_rmse: 0.41955 |  0:08:00s
epoch 140| loss: 0.16919 | val_0_rmse: 0.4037  | val_1_rmse: 0.43329 |  0:08:04s
epoch 141| loss: 0.17    | val_0_rmse: 0.3962  | val_1_rmse: 0.42496 |  0:08:07s
epoch 142| loss: 0.16988 | val_0_rmse: 0.41805 | val_1_rmse: 0.44308 |  0:08:11s
epoch 143| loss: 0.17046 | val_0_rmse: 0.41658 | val_1_rmse: 0.44144 |  0:08:14s
epoch 144| loss: 0.17577 | val_0_rmse: 0.40101 | val_1_rmse: 0.42587 |  0:08:18s
epoch 145| loss: 0.17041 | val_0_rmse: 0.39006 | val_1_rmse: 0.42096 |  0:08:21s
epoch 146| loss: 0.17032 | val_0_rmse: 0.39313 | val_1_rmse: 0.42273 |  0:08:24s
epoch 147| loss: 0.16957 | val_0_rmse: 0.39542 | val_1_rmse: 0.42131 |  0:08:28s
epoch 148| loss: 0.1713  | val_0_rmse: 0.39192 | val_1_rmse: 0.42098 |  0:08:31s
epoch 149| loss: 0.17212 | val_0_rmse: 0.40847 | val_1_rmse: 0.43889 |  0:08:35s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.41955
Best weights from best epoch are automatically used!
ended training at: 16:47:21
Feature importance:
Mean squared error is of 9377121844.789553
Mean absolute error:64956.36246558571
MAPE:0.25075526771406903
R2 score:0.8369575334525192
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:47:22
epoch 0  | loss: 0.57908 | val_0_rmse: 0.76553 | val_1_rmse: 0.75687 |  0:00:03s
epoch 1  | loss: 0.38659 | val_0_rmse: 0.71794 | val_1_rmse: 0.70743 |  0:00:06s
epoch 2  | loss: 0.33454 | val_0_rmse: 0.67123 | val_1_rmse: 0.66472 |  0:00:10s
epoch 3  | loss: 0.31369 | val_0_rmse: 0.59588 | val_1_rmse: 0.58742 |  0:00:13s
epoch 4  | loss: 0.28522 | val_0_rmse: 0.55461 | val_1_rmse: 0.54631 |  0:00:17s
epoch 5  | loss: 0.26345 | val_0_rmse: 0.54665 | val_1_rmse: 0.53903 |  0:00:20s
epoch 6  | loss: 0.26451 | val_0_rmse: 0.51694 | val_1_rmse: 0.50767 |  0:00:24s
epoch 7  | loss: 0.25949 | val_0_rmse: 0.51686 | val_1_rmse: 0.50768 |  0:00:27s
epoch 8  | loss: 0.25671 | val_0_rmse: 0.49832 | val_1_rmse: 0.49286 |  0:00:30s
epoch 9  | loss: 0.2525  | val_0_rmse: 0.50542 | val_1_rmse: 0.49535 |  0:00:34s
epoch 10 | loss: 0.25207 | val_0_rmse: 0.48203 | val_1_rmse: 0.474   |  0:00:37s
epoch 11 | loss: 0.24965 | val_0_rmse: 0.48078 | val_1_rmse: 0.47017 |  0:00:41s
epoch 12 | loss: 0.24607 | val_0_rmse: 0.47698 | val_1_rmse: 0.46751 |  0:00:44s
epoch 13 | loss: 0.26021 | val_0_rmse: 0.48291 | val_1_rmse: 0.47432 |  0:00:47s
epoch 14 | loss: 0.25052 | val_0_rmse: 0.49122 | val_1_rmse: 0.48212 |  0:00:51s
epoch 15 | loss: 0.24281 | val_0_rmse: 0.47802 | val_1_rmse: 0.46725 |  0:00:55s
epoch 16 | loss: 0.23771 | val_0_rmse: 0.48242 | val_1_rmse: 0.47456 |  0:00:58s
epoch 17 | loss: 0.23597 | val_0_rmse: 0.48662 | val_1_rmse: 0.48033 |  0:01:02s
epoch 18 | loss: 0.23248 | val_0_rmse: 0.46627 | val_1_rmse: 0.45959 |  0:01:06s
epoch 19 | loss: 0.23368 | val_0_rmse: 0.48018 | val_1_rmse: 0.47238 |  0:01:10s
epoch 20 | loss: 0.23159 | val_0_rmse: 0.46366 | val_1_rmse: 0.45281 |  0:01:13s
epoch 21 | loss: 0.23129 | val_0_rmse: 0.50447 | val_1_rmse: 0.50306 |  0:01:17s
epoch 22 | loss: 0.26337 | val_0_rmse: 0.48341 | val_1_rmse: 0.47661 |  0:01:20s
epoch 23 | loss: 0.27706 | val_0_rmse: 0.48929 | val_1_rmse: 0.47935 |  0:01:24s
epoch 24 | loss: 0.26476 | val_0_rmse: 0.49656 | val_1_rmse: 0.4861  |  0:01:27s
epoch 25 | loss: 0.24557 | val_0_rmse: 0.47524 | val_1_rmse: 0.462   |  0:01:31s
epoch 26 | loss: 0.23515 | val_0_rmse: 0.47735 | val_1_rmse: 0.46786 |  0:01:34s
epoch 27 | loss: 0.23003 | val_0_rmse: 0.45999 | val_1_rmse: 0.44933 |  0:01:37s
epoch 28 | loss: 0.22346 | val_0_rmse: 0.46156 | val_1_rmse: 0.45255 |  0:01:41s
epoch 29 | loss: 0.2273  | val_0_rmse: 0.45239 | val_1_rmse: 0.44533 |  0:01:44s
epoch 30 | loss: 0.23908 | val_0_rmse: 0.46808 | val_1_rmse: 0.45724 |  0:01:48s
epoch 31 | loss: 0.23097 | val_0_rmse: 0.45434 | val_1_rmse: 0.44495 |  0:01:51s
epoch 32 | loss: 0.22736 | val_0_rmse: 0.50447 | val_1_rmse: 0.49811 |  0:01:55s
epoch 33 | loss: 0.25765 | val_0_rmse: 0.49767 | val_1_rmse: 0.48924 |  0:01:58s
epoch 34 | loss: 0.25437 | val_0_rmse: 0.49971 | val_1_rmse: 0.48772 |  0:02:01s
epoch 35 | loss: 0.25197 | val_0_rmse: 0.5052  | val_1_rmse: 0.48153 |  0:02:05s
epoch 36 | loss: 0.31142 | val_0_rmse: 0.52751 | val_1_rmse: 0.51835 |  0:02:08s
epoch 37 | loss: 0.27058 | val_0_rmse: 0.50612 | val_1_rmse: 0.49251 |  0:02:12s
epoch 38 | loss: 0.2577  | val_0_rmse: 0.49527 | val_1_rmse: 0.48504 |  0:02:15s
epoch 39 | loss: 0.24437 | val_0_rmse: 0.47581 | val_1_rmse: 0.46598 |  0:02:19s
epoch 40 | loss: 0.24175 | val_0_rmse: 0.47885 | val_1_rmse: 0.4689  |  0:02:22s
epoch 41 | loss: 0.23745 | val_0_rmse: 0.46763 | val_1_rmse: 0.4617  |  0:02:25s
epoch 42 | loss: 0.23477 | val_0_rmse: 0.47138 | val_1_rmse: 0.46531 |  0:02:29s
epoch 43 | loss: 0.23108 | val_0_rmse: 0.464   | val_1_rmse: 0.4577  |  0:02:32s
epoch 44 | loss: 0.22964 | val_0_rmse: 0.46218 | val_1_rmse: 0.45523 |  0:02:36s
epoch 45 | loss: 0.23    | val_0_rmse: 0.45909 | val_1_rmse: 0.45473 |  0:02:39s
epoch 46 | loss: 0.22714 | val_0_rmse: 0.45608 | val_1_rmse: 0.44904 |  0:02:43s
epoch 47 | loss: 0.22003 | val_0_rmse: 0.46299 | val_1_rmse: 0.4542  |  0:02:46s
epoch 48 | loss: 0.2187  | val_0_rmse: 0.45202 | val_1_rmse: 0.44656 |  0:02:50s
epoch 49 | loss: 0.22033 | val_0_rmse: 0.45481 | val_1_rmse: 0.44568 |  0:02:53s
epoch 50 | loss: 0.21856 | val_0_rmse: 0.44504 | val_1_rmse: 0.43947 |  0:02:56s
epoch 51 | loss: 0.21605 | val_0_rmse: 0.44992 | val_1_rmse: 0.44497 |  0:03:00s
epoch 52 | loss: 0.21187 | val_0_rmse: 0.45075 | val_1_rmse: 0.44886 |  0:03:03s
epoch 53 | loss: 0.21233 | val_0_rmse: 0.44368 | val_1_rmse: 0.43803 |  0:03:07s
epoch 54 | loss: 0.21663 | val_0_rmse: 0.44724 | val_1_rmse: 0.43897 |  0:03:10s
epoch 55 | loss: 0.213   | val_0_rmse: 0.44634 | val_1_rmse: 0.44134 |  0:03:14s
epoch 56 | loss: 0.21327 | val_0_rmse: 0.44191 | val_1_rmse: 0.44047 |  0:03:17s
epoch 57 | loss: 0.21015 | val_0_rmse: 0.44608 | val_1_rmse: 0.44152 |  0:03:20s
epoch 58 | loss: 0.20871 | val_0_rmse: 0.43818 | val_1_rmse: 0.43444 |  0:03:24s
epoch 59 | loss: 0.20614 | val_0_rmse: 0.43338 | val_1_rmse: 0.42925 |  0:03:27s
epoch 60 | loss: 0.20406 | val_0_rmse: 0.43109 | val_1_rmse: 0.42909 |  0:03:31s
epoch 61 | loss: 0.203   | val_0_rmse: 0.43379 | val_1_rmse: 0.43047 |  0:03:34s
epoch 62 | loss: 0.20328 | val_0_rmse: 0.43806 | val_1_rmse: 0.43768 |  0:03:37s
epoch 63 | loss: 0.20314 | val_0_rmse: 0.43378 | val_1_rmse: 0.4317  |  0:03:41s
epoch 64 | loss: 0.20218 | val_0_rmse: 0.43781 | val_1_rmse: 0.43568 |  0:03:44s
epoch 65 | loss: 0.20047 | val_0_rmse: 0.43197 | val_1_rmse: 0.42932 |  0:03:48s
epoch 66 | loss: 0.20199 | val_0_rmse: 0.44311 | val_1_rmse: 0.44125 |  0:03:51s
epoch 67 | loss: 0.205   | val_0_rmse: 0.43607 | val_1_rmse: 0.43358 |  0:03:55s
epoch 68 | loss: 0.20407 | val_0_rmse: 0.4386  | val_1_rmse: 0.43385 |  0:03:58s
epoch 69 | loss: 0.19963 | val_0_rmse: 0.42598 | val_1_rmse: 0.42776 |  0:04:01s
epoch 70 | loss: 0.19944 | val_0_rmse: 0.43943 | val_1_rmse: 0.43765 |  0:04:05s
epoch 71 | loss: 0.20065 | val_0_rmse: 0.4384  | val_1_rmse: 0.43677 |  0:04:08s
epoch 72 | loss: 0.20123 | val_0_rmse: 0.4405  | val_1_rmse: 0.43539 |  0:04:12s
epoch 73 | loss: 0.19894 | val_0_rmse: 0.43048 | val_1_rmse: 0.42827 |  0:04:15s
epoch 74 | loss: 0.1963  | val_0_rmse: 0.42947 | val_1_rmse: 0.42726 |  0:04:19s
epoch 75 | loss: 0.19525 | val_0_rmse: 0.51528 | val_1_rmse: 0.51553 |  0:04:22s
epoch 76 | loss: 0.20064 | val_0_rmse: 0.42734 | val_1_rmse: 0.42689 |  0:04:25s
epoch 77 | loss: 0.19756 | val_0_rmse: 0.42501 | val_1_rmse: 0.42423 |  0:04:29s
epoch 78 | loss: 0.19592 | val_0_rmse: 0.4356  | val_1_rmse: 0.43207 |  0:04:32s
epoch 79 | loss: 0.19834 | val_0_rmse: 0.42615 | val_1_rmse: 0.42528 |  0:04:36s
epoch 80 | loss: 0.19362 | val_0_rmse: 0.42909 | val_1_rmse: 0.43283 |  0:04:39s
epoch 81 | loss: 0.19615 | val_0_rmse: 0.42543 | val_1_rmse: 0.4277  |  0:04:43s
epoch 82 | loss: 0.19446 | val_0_rmse: 0.42032 | val_1_rmse: 0.42052 |  0:04:46s
epoch 83 | loss: 0.19186 | val_0_rmse: 0.42507 | val_1_rmse: 0.42775 |  0:04:49s
epoch 84 | loss: 0.19277 | val_0_rmse: 0.42456 | val_1_rmse: 0.42688 |  0:04:53s
epoch 85 | loss: 0.19275 | val_0_rmse: 0.42188 | val_1_rmse: 0.42615 |  0:04:56s
epoch 86 | loss: 0.1911  | val_0_rmse: 0.41828 | val_1_rmse: 0.42108 |  0:05:00s
epoch 87 | loss: 0.19584 | val_0_rmse: 0.42655 | val_1_rmse: 0.42728 |  0:05:03s
epoch 88 | loss: 0.19283 | val_0_rmse: 0.42531 | val_1_rmse: 0.42823 |  0:05:07s
epoch 89 | loss: 0.19315 | val_0_rmse: 0.42137 | val_1_rmse: 0.42512 |  0:05:10s
epoch 90 | loss: 0.19162 | val_0_rmse: 0.42026 | val_1_rmse: 0.42149 |  0:05:13s
epoch 91 | loss: 0.19229 | val_0_rmse: 0.42051 | val_1_rmse: 0.42408 |  0:05:17s
epoch 92 | loss: 0.18974 | val_0_rmse: 0.42084 | val_1_rmse: 0.42738 |  0:05:20s
epoch 93 | loss: 0.19184 | val_0_rmse: 0.42023 | val_1_rmse: 0.42186 |  0:05:24s
epoch 94 | loss: 0.19089 | val_0_rmse: 0.44314 | val_1_rmse: 0.44284 |  0:05:27s
epoch 95 | loss: 0.18892 | val_0_rmse: 0.42501 | val_1_rmse: 0.43052 |  0:05:31s
epoch 96 | loss: 0.19084 | val_0_rmse: 0.46897 | val_1_rmse: 0.46739 |  0:05:34s
epoch 97 | loss: 0.18833 | val_0_rmse: 0.42604 | val_1_rmse: 0.42778 |  0:05:38s
epoch 98 | loss: 0.18932 | val_0_rmse: 0.41897 | val_1_rmse: 0.42341 |  0:05:41s
epoch 99 | loss: 0.18772 | val_0_rmse: 0.44065 | val_1_rmse: 0.44868 |  0:05:45s
epoch 100| loss: 0.19178 | val_0_rmse: 0.43193 | val_1_rmse: 0.43062 |  0:05:48s
epoch 101| loss: 0.19051 | val_0_rmse: 0.4167  | val_1_rmse: 0.42031 |  0:05:51s
epoch 102| loss: 0.19069 | val_0_rmse: 0.42846 | val_1_rmse: 0.4313  |  0:05:55s
epoch 103| loss: 0.18957 | val_0_rmse: 0.42153 | val_1_rmse: 0.42313 |  0:05:58s
epoch 104| loss: 0.19257 | val_0_rmse: 0.42533 | val_1_rmse: 0.42301 |  0:06:02s
epoch 105| loss: 0.19121 | val_0_rmse: 0.41614 | val_1_rmse: 0.41855 |  0:06:05s
epoch 106| loss: 0.18819 | val_0_rmse: 0.42723 | val_1_rmse: 0.4289  |  0:06:08s
epoch 107| loss: 0.19272 | val_0_rmse: 0.41854 | val_1_rmse: 0.42092 |  0:06:12s
epoch 108| loss: 0.18967 | val_0_rmse: 0.4137  | val_1_rmse: 0.41465 |  0:06:15s
epoch 109| loss: 0.18633 | val_0_rmse: 0.42514 | val_1_rmse: 0.42792 |  0:06:19s
epoch 110| loss: 0.18684 | val_0_rmse: 0.45592 | val_1_rmse: 0.45744 |  0:06:22s
epoch 111| loss: 0.18789 | val_0_rmse: 0.43113 | val_1_rmse: 0.43722 |  0:06:26s
epoch 112| loss: 0.19032 | val_0_rmse: 0.44368 | val_1_rmse: 0.44502 |  0:06:29s
epoch 113| loss: 0.19    | val_0_rmse: 0.42561 | val_1_rmse: 0.43151 |  0:06:32s
epoch 114| loss: 0.18739 | val_0_rmse: 0.41991 | val_1_rmse: 0.42233 |  0:06:36s
epoch 115| loss: 0.18852 | val_0_rmse: 0.43359 | val_1_rmse: 0.43372 |  0:06:39s
epoch 116| loss: 0.18643 | val_0_rmse: 0.42165 | val_1_rmse: 0.42487 |  0:06:43s
epoch 117| loss: 0.18757 | val_0_rmse: 0.41256 | val_1_rmse: 0.41696 |  0:06:46s
epoch 118| loss: 0.18688 | val_0_rmse: 0.42186 | val_1_rmse: 0.42587 |  0:06:50s
epoch 119| loss: 0.18605 | val_0_rmse: 0.40978 | val_1_rmse: 0.4158  |  0:06:53s
epoch 120| loss: 0.18433 | val_0_rmse: 0.43989 | val_1_rmse: 0.44046 |  0:06:56s
epoch 121| loss: 0.18755 | val_0_rmse: 0.41628 | val_1_rmse: 0.41846 |  0:07:00s
epoch 122| loss: 0.18176 | val_0_rmse: 0.41432 | val_1_rmse: 0.41988 |  0:07:03s
epoch 123| loss: 0.18365 | val_0_rmse: 0.4218  | val_1_rmse: 0.42851 |  0:07:07s
epoch 124| loss: 0.18498 | val_0_rmse: 0.41087 | val_1_rmse: 0.41781 |  0:07:10s
epoch 125| loss: 0.18156 | val_0_rmse: 0.41467 | val_1_rmse: 0.4193  |  0:07:13s
epoch 126| loss: 0.18313 | val_0_rmse: 0.41696 | val_1_rmse: 0.42001 |  0:07:17s
epoch 127| loss: 0.18364 | val_0_rmse: 0.41594 | val_1_rmse: 0.42008 |  0:07:20s
epoch 128| loss: 0.18315 | val_0_rmse: 0.41328 | val_1_rmse: 0.41682 |  0:07:24s
epoch 129| loss: 0.18227 | val_0_rmse: 0.41459 | val_1_rmse: 0.41877 |  0:07:27s
epoch 130| loss: 0.18012 | val_0_rmse: 0.40609 | val_1_rmse: 0.41311 |  0:07:31s
epoch 131| loss: 0.18513 | val_0_rmse: 0.41349 | val_1_rmse: 0.42053 |  0:07:34s
epoch 132| loss: 0.18246 | val_0_rmse: 0.40326 | val_1_rmse: 0.41293 |  0:07:37s
epoch 133| loss: 0.1811  | val_0_rmse: 0.41495 | val_1_rmse: 0.41868 |  0:07:41s
epoch 134| loss: 0.18552 | val_0_rmse: 0.42911 | val_1_rmse: 0.43734 |  0:07:44s
epoch 135| loss: 0.1864  | val_0_rmse: 0.41526 | val_1_rmse: 0.42106 |  0:07:48s
epoch 136| loss: 0.18368 | val_0_rmse: 0.44145 | val_1_rmse: 0.44972 |  0:07:51s
epoch 137| loss: 0.18363 | val_0_rmse: 0.41089 | val_1_rmse: 0.41673 |  0:07:55s
epoch 138| loss: 0.18226 | val_0_rmse: 0.40859 | val_1_rmse: 0.41621 |  0:07:58s
epoch 139| loss: 0.18078 | val_0_rmse: 0.40786 | val_1_rmse: 0.41493 |  0:08:02s
epoch 140| loss: 0.18187 | val_0_rmse: 0.41312 | val_1_rmse: 0.42138 |  0:08:05s
epoch 141| loss: 0.1832  | val_0_rmse: 0.41221 | val_1_rmse: 0.41895 |  0:08:08s
epoch 142| loss: 0.18299 | val_0_rmse: 0.40422 | val_1_rmse: 0.41328 |  0:08:12s
epoch 143| loss: 0.18103 | val_0_rmse: 0.41951 | val_1_rmse: 0.42718 |  0:08:15s
epoch 144| loss: 0.19309 | val_0_rmse: 0.43083 | val_1_rmse: 0.43347 |  0:08:19s
epoch 145| loss: 0.18247 | val_0_rmse: 0.41033 | val_1_rmse: 0.41773 |  0:08:22s
epoch 146| loss: 0.18109 | val_0_rmse: 0.40451 | val_1_rmse: 0.41412 |  0:08:26s
epoch 147| loss: 0.1779  | val_0_rmse: 0.40633 | val_1_rmse: 0.41669 |  0:08:29s
epoch 148| loss: 0.18327 | val_0_rmse: 0.44014 | val_1_rmse: 0.44152 |  0:08:32s
epoch 149| loss: 0.18077 | val_0_rmse: 0.4229  | val_1_rmse: 0.42946 |  0:08:36s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.41293
Best weights from best epoch are automatically used!
ended training at: 16:56:00
Feature importance:
Mean squared error is of 9969908834.341095
Mean absolute error:67287.42543215724
MAPE:0.2543473737185506
R2 score:0.825781219908512
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:56:01
epoch 0  | loss: 0.58454 | val_0_rmse: 0.65494 | val_1_rmse: 0.65609 |  0:00:03s
epoch 1  | loss: 0.32809 | val_0_rmse: 0.61571 | val_1_rmse: 0.61873 |  0:00:06s
epoch 2  | loss: 0.29672 | val_0_rmse: 0.58695 | val_1_rmse: 0.58721 |  0:00:10s
epoch 3  | loss: 0.28624 | val_0_rmse: 0.57577 | val_1_rmse: 0.57798 |  0:00:13s
epoch 4  | loss: 0.27122 | val_0_rmse: 0.57    | val_1_rmse: 0.5751  |  0:00:17s
epoch 5  | loss: 0.27524 | val_0_rmse: 0.61968 | val_1_rmse: 0.61305 |  0:00:20s
epoch 6  | loss: 0.27089 | val_0_rmse: 0.60775 | val_1_rmse: 0.59254 |  0:00:24s
epoch 7  | loss: 0.26756 | val_0_rmse: 0.54263 | val_1_rmse: 0.53761 |  0:00:27s
epoch 8  | loss: 0.26952 | val_0_rmse: 0.6518  | val_1_rmse: 0.63291 |  0:00:30s
epoch 9  | loss: 0.27842 | val_0_rmse: 0.72425 | val_1_rmse: 0.71445 |  0:00:34s
epoch 10 | loss: 0.28601 | val_0_rmse: 0.50758 | val_1_rmse: 0.50964 |  0:00:37s
epoch 11 | loss: 0.26258 | val_0_rmse: 0.49526 | val_1_rmse: 0.49955 |  0:00:41s
epoch 12 | loss: 0.2521  | val_0_rmse: 0.47644 | val_1_rmse: 0.48123 |  0:00:44s
epoch 13 | loss: 0.2451  | val_0_rmse: 0.47329 | val_1_rmse: 0.47798 |  0:00:47s
epoch 14 | loss: 0.24042 | val_0_rmse: 0.48102 | val_1_rmse: 0.48402 |  0:00:51s
epoch 15 | loss: 0.23574 | val_0_rmse: 0.47472 | val_1_rmse: 0.47655 |  0:00:54s
epoch 16 | loss: 0.2311  | val_0_rmse: 0.47683 | val_1_rmse: 0.47744 |  0:00:58s
epoch 17 | loss: 0.22995 | val_0_rmse: 0.47693 | val_1_rmse: 0.48098 |  0:01:01s
epoch 18 | loss: 0.23217 | val_0_rmse: 0.46793 | val_1_rmse: 0.46773 |  0:01:05s
epoch 19 | loss: 0.22378 | val_0_rmse: 0.47915 | val_1_rmse: 0.48562 |  0:01:08s
epoch 20 | loss: 0.22616 | val_0_rmse: 0.46034 | val_1_rmse: 0.46379 |  0:01:11s
epoch 21 | loss: 0.22288 | val_0_rmse: 0.45554 | val_1_rmse: 0.46152 |  0:01:15s
epoch 22 | loss: 0.22058 | val_0_rmse: 0.46364 | val_1_rmse: 0.46731 |  0:01:18s
epoch 23 | loss: 0.21787 | val_0_rmse: 0.45693 | val_1_rmse: 0.46042 |  0:01:22s
epoch 24 | loss: 0.22269 | val_0_rmse: 0.45965 | val_1_rmse: 0.46736 |  0:01:25s
epoch 25 | loss: 0.21882 | val_0_rmse: 0.45409 | val_1_rmse: 0.45968 |  0:01:28s
epoch 26 | loss: 0.2174  | val_0_rmse: 0.44608 | val_1_rmse: 0.45035 |  0:01:32s
epoch 27 | loss: 0.22185 | val_0_rmse: 0.45811 | val_1_rmse: 0.46286 |  0:01:35s
epoch 28 | loss: 0.21455 | val_0_rmse: 0.44468 | val_1_rmse: 0.44971 |  0:01:39s
epoch 29 | loss: 0.21101 | val_0_rmse: 0.44914 | val_1_rmse: 0.45584 |  0:01:42s
epoch 30 | loss: 0.21268 | val_0_rmse: 0.44865 | val_1_rmse: 0.45297 |  0:01:46s
epoch 31 | loss: 0.2143  | val_0_rmse: 0.46747 | val_1_rmse: 0.47404 |  0:01:49s
epoch 32 | loss: 0.21199 | val_0_rmse: 0.44689 | val_1_rmse: 0.44938 |  0:01:52s
epoch 33 | loss: 0.21561 | val_0_rmse: 0.44244 | val_1_rmse: 0.44704 |  0:01:56s
epoch 34 | loss: 0.21073 | val_0_rmse: 0.4486  | val_1_rmse: 0.45103 |  0:01:59s
epoch 35 | loss: 0.21446 | val_0_rmse: 0.45334 | val_1_rmse: 0.45914 |  0:02:03s
epoch 36 | loss: 0.21024 | val_0_rmse: 0.43854 | val_1_rmse: 0.44344 |  0:02:06s
epoch 37 | loss: 0.2057  | val_0_rmse: 0.43526 | val_1_rmse: 0.44119 |  0:02:10s
epoch 38 | loss: 0.20276 | val_0_rmse: 0.44264 | val_1_rmse: 0.45289 |  0:02:13s
epoch 39 | loss: 0.20301 | val_0_rmse: 0.43964 | val_1_rmse: 0.44749 |  0:02:16s
epoch 40 | loss: 0.20439 | val_0_rmse: 0.4402  | val_1_rmse: 0.44759 |  0:02:20s
epoch 41 | loss: 0.20477 | val_0_rmse: 0.46506 | val_1_rmse: 0.47005 |  0:02:23s
epoch 42 | loss: 0.20457 | val_0_rmse: 0.43599 | val_1_rmse: 0.44408 |  0:02:27s
epoch 43 | loss: 0.2019  | val_0_rmse: 0.44882 | val_1_rmse: 0.45725 |  0:02:30s
epoch 44 | loss: 0.19922 | val_0_rmse: 0.43712 | val_1_rmse: 0.45046 |  0:02:34s
epoch 45 | loss: 0.20307 | val_0_rmse: 0.45242 | val_1_rmse: 0.45871 |  0:02:37s
epoch 46 | loss: 0.20153 | val_0_rmse: 0.42853 | val_1_rmse: 0.43718 |  0:02:40s
epoch 47 | loss: 0.20237 | val_0_rmse: 0.42806 | val_1_rmse: 0.43796 |  0:02:44s
epoch 48 | loss: 0.19651 | val_0_rmse: 0.42723 | val_1_rmse: 0.43406 |  0:02:47s
epoch 49 | loss: 0.20079 | val_0_rmse: 0.43045 | val_1_rmse: 0.43921 |  0:02:51s
epoch 50 | loss: 0.19856 | val_0_rmse: 0.42998 | val_1_rmse: 0.43985 |  0:02:54s
epoch 51 | loss: 0.2028  | val_0_rmse: 0.43506 | val_1_rmse: 0.44332 |  0:02:57s
epoch 52 | loss: 0.19752 | val_0_rmse: 0.44724 | val_1_rmse: 0.45918 |  0:03:01s
epoch 53 | loss: 0.19653 | val_0_rmse: 0.42624 | val_1_rmse: 0.43561 |  0:03:04s
epoch 54 | loss: 0.19816 | val_0_rmse: 0.43417 | val_1_rmse: 0.44626 |  0:03:08s
epoch 55 | loss: 0.19809 | val_0_rmse: 0.42617 | val_1_rmse: 0.43644 |  0:03:11s
epoch 56 | loss: 0.19065 | val_0_rmse: 0.42285 | val_1_rmse: 0.43439 |  0:03:15s
epoch 57 | loss: 0.19306 | val_0_rmse: 0.43223 | val_1_rmse: 0.44346 |  0:03:18s
epoch 58 | loss: 0.19373 | val_0_rmse: 0.42513 | val_1_rmse: 0.43568 |  0:03:21s
epoch 59 | loss: 0.19588 | val_0_rmse: 0.44304 | val_1_rmse: 0.45239 |  0:03:25s
epoch 60 | loss: 0.19076 | val_0_rmse: 0.42925 | val_1_rmse: 0.4403  |  0:03:28s
epoch 61 | loss: 0.19051 | val_0_rmse: 0.41957 | val_1_rmse: 0.42895 |  0:03:32s
epoch 62 | loss: 0.19391 | val_0_rmse: 0.42699 | val_1_rmse: 0.43657 |  0:03:35s
epoch 63 | loss: 0.19081 | val_0_rmse: 0.42471 | val_1_rmse: 0.43634 |  0:03:39s
epoch 64 | loss: 0.20359 | val_0_rmse: 0.4609  | val_1_rmse: 0.46474 |  0:03:42s
epoch 65 | loss: 0.20265 | val_0_rmse: 0.44809 | val_1_rmse: 0.45902 |  0:03:45s
epoch 66 | loss: 0.20032 | val_0_rmse: 0.44684 | val_1_rmse: 0.45408 |  0:03:49s
epoch 67 | loss: 0.19405 | val_0_rmse: 0.41949 | val_1_rmse: 0.43092 |  0:03:52s
epoch 68 | loss: 0.19221 | val_0_rmse: 0.41949 | val_1_rmse: 0.4316  |  0:03:56s
epoch 69 | loss: 0.19159 | val_0_rmse: 0.44448 | val_1_rmse: 0.45606 |  0:03:59s
epoch 70 | loss: 0.1924  | val_0_rmse: 0.43043 | val_1_rmse: 0.44093 |  0:04:02s
epoch 71 | loss: 0.18763 | val_0_rmse: 0.42133 | val_1_rmse: 0.43423 |  0:04:06s
epoch 72 | loss: 0.18704 | val_0_rmse: 0.41595 | val_1_rmse: 0.42787 |  0:04:09s
epoch 73 | loss: 0.18688 | val_0_rmse: 0.42056 | val_1_rmse: 0.4334  |  0:04:13s
epoch 74 | loss: 0.18647 | val_0_rmse: 0.42347 | val_1_rmse: 0.43397 |  0:04:16s
epoch 75 | loss: 0.18913 | val_0_rmse: 0.42394 | val_1_rmse: 0.4346  |  0:04:20s
epoch 76 | loss: 0.19584 | val_0_rmse: 0.45329 | val_1_rmse: 0.46639 |  0:04:23s
epoch 77 | loss: 0.18876 | val_0_rmse: 0.42879 | val_1_rmse: 0.44034 |  0:04:26s
epoch 78 | loss: 0.1911  | val_0_rmse: 0.42755 | val_1_rmse: 0.43959 |  0:04:30s
epoch 79 | loss: 0.186   | val_0_rmse: 0.41244 | val_1_rmse: 0.42451 |  0:04:33s
epoch 80 | loss: 0.18693 | val_0_rmse: 0.41455 | val_1_rmse: 0.42728 |  0:04:37s
epoch 81 | loss: 0.18508 | val_0_rmse: 0.41301 | val_1_rmse: 0.42713 |  0:04:40s
epoch 82 | loss: 0.18426 | val_0_rmse: 0.41369 | val_1_rmse: 0.4278  |  0:04:43s
epoch 83 | loss: 0.19381 | val_0_rmse: 0.46727 | val_1_rmse: 0.47981 |  0:04:47s
epoch 84 | loss: 0.22658 | val_0_rmse: 0.46052 | val_1_rmse: 0.46849 |  0:04:50s
epoch 85 | loss: 0.20911 | val_0_rmse: 0.43685 | val_1_rmse: 0.44514 |  0:04:54s
epoch 86 | loss: 0.21334 | val_0_rmse: 0.45301 | val_1_rmse: 0.45939 |  0:04:57s
epoch 87 | loss: 0.21733 | val_0_rmse: 0.4781  | val_1_rmse: 0.48909 |  0:05:01s
epoch 88 | loss: 0.21596 | val_0_rmse: 0.44232 | val_1_rmse: 0.45153 |  0:05:04s
epoch 89 | loss: 0.20986 | val_0_rmse: 0.45115 | val_1_rmse: 0.45977 |  0:05:07s
epoch 90 | loss: 0.20646 | val_0_rmse: 0.45085 | val_1_rmse: 0.46017 |  0:05:11s
epoch 91 | loss: 0.20486 | val_0_rmse: 0.44211 | val_1_rmse: 0.44816 |  0:05:14s
epoch 92 | loss: 0.20357 | val_0_rmse: 0.43065 | val_1_rmse: 0.43899 |  0:05:17s
epoch 93 | loss: 0.19842 | val_0_rmse: 0.45935 | val_1_rmse: 0.46513 |  0:05:21s
epoch 94 | loss: 0.19765 | val_0_rmse: 0.44155 | val_1_rmse: 0.44921 |  0:05:24s
epoch 95 | loss: 0.19871 | val_0_rmse: 0.43764 | val_1_rmse: 0.446   |  0:05:28s
epoch 96 | loss: 0.1953  | val_0_rmse: 0.42642 | val_1_rmse: 0.4372  |  0:05:31s
epoch 97 | loss: 0.19952 | val_0_rmse: 0.42915 | val_1_rmse: 0.43996 |  0:05:35s
epoch 98 | loss: 0.19423 | val_0_rmse: 0.42797 | val_1_rmse: 0.44034 |  0:05:38s
epoch 99 | loss: 0.19654 | val_0_rmse: 0.43069 | val_1_rmse: 0.44425 |  0:05:41s
epoch 100| loss: 0.19506 | val_0_rmse: 0.44114 | val_1_rmse: 0.45072 |  0:05:45s
epoch 101| loss: 0.19363 | val_0_rmse: 0.41906 | val_1_rmse: 0.43114 |  0:05:48s
epoch 102| loss: 0.19447 | val_0_rmse: 0.42374 | val_1_rmse: 0.43893 |  0:05:52s
epoch 103| loss: 0.19672 | val_0_rmse: 0.43541 | val_1_rmse: 0.44815 |  0:05:55s
epoch 104| loss: 0.20065 | val_0_rmse: 0.42739 | val_1_rmse: 0.4395  |  0:05:58s
epoch 105| loss: 0.19664 | val_0_rmse: 0.42524 | val_1_rmse: 0.43825 |  0:06:02s
epoch 106| loss: 0.19703 | val_0_rmse: 0.43265 | val_1_rmse: 0.44373 |  0:06:05s
epoch 107| loss: 0.19434 | val_0_rmse: 0.42842 | val_1_rmse: 0.44405 |  0:06:09s
epoch 108| loss: 0.19445 | val_0_rmse: 0.47336 | val_1_rmse: 0.48908 |  0:06:12s
epoch 109| loss: 0.19132 | val_0_rmse: 0.42605 | val_1_rmse: 0.4389  |  0:06:15s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.42451
Best weights from best epoch are automatically used!
ended training at: 17:02:18
Feature importance:
Mean squared error is of 9553090374.297346
Mean absolute error:66697.90731587571
MAPE:0.25841997465798167
R2 score:0.8309583559996494
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:02:18
epoch 0  | loss: 0.6237  | val_0_rmse: 0.76489 | val_1_rmse: 0.77402 |  0:00:03s
epoch 1  | loss: 0.33801 | val_0_rmse: 0.61651 | val_1_rmse: 0.62426 |  0:00:06s
epoch 2  | loss: 0.28942 | val_0_rmse: 0.60808 | val_1_rmse: 0.61555 |  0:00:10s
epoch 3  | loss: 0.27337 | val_0_rmse: 0.58048 | val_1_rmse: 0.58771 |  0:00:13s
epoch 4  | loss: 0.28362 | val_0_rmse: 0.53038 | val_1_rmse: 0.53526 |  0:00:17s
epoch 5  | loss: 0.27673 | val_0_rmse: 0.53863 | val_1_rmse: 0.54343 |  0:00:20s
epoch 6  | loss: 0.28614 | val_0_rmse: 0.53881 | val_1_rmse: 0.54934 |  0:00:24s
epoch 7  | loss: 0.26445 | val_0_rmse: 0.49703 | val_1_rmse: 0.49995 |  0:00:27s
epoch 8  | loss: 0.2789  | val_0_rmse: 0.51506 | val_1_rmse: 0.5201  |  0:00:31s
epoch 9  | loss: 0.27243 | val_0_rmse: 0.4956  | val_1_rmse: 0.49774 |  0:00:34s
epoch 10 | loss: 0.26269 | val_0_rmse: 0.50477 | val_1_rmse: 0.50474 |  0:00:37s
epoch 11 | loss: 0.25491 | val_0_rmse: 0.49759 | val_1_rmse: 0.49917 |  0:00:41s
epoch 12 | loss: 0.25634 | val_0_rmse: 0.47968 | val_1_rmse: 0.47736 |  0:00:44s
epoch 13 | loss: 0.24755 | val_0_rmse: 0.47493 | val_1_rmse: 0.47845 |  0:00:48s
epoch 14 | loss: 0.27493 | val_0_rmse: 0.51955 | val_1_rmse: 0.51965 |  0:00:51s
epoch 15 | loss: 0.25315 | val_0_rmse: 0.48033 | val_1_rmse: 0.48363 |  0:00:55s
epoch 16 | loss: 0.27429 | val_0_rmse: 0.50071 | val_1_rmse: 0.50369 |  0:00:58s
epoch 17 | loss: 0.28466 | val_0_rmse: 0.5099  | val_1_rmse: 0.51257 |  0:01:01s
epoch 18 | loss: 0.25759 | val_0_rmse: 0.48604 | val_1_rmse: 0.48779 |  0:01:05s
epoch 19 | loss: 0.28285 | val_0_rmse: 0.5585  | val_1_rmse: 0.5657  |  0:01:08s
epoch 20 | loss: 0.27399 | val_0_rmse: 0.50857 | val_1_rmse: 0.5142  |  0:01:12s
epoch 21 | loss: 0.26636 | val_0_rmse: 0.4985  | val_1_rmse: 0.49981 |  0:01:15s
epoch 22 | loss: 0.24531 | val_0_rmse: 0.47478 | val_1_rmse: 0.47801 |  0:01:19s
epoch 23 | loss: 0.23831 | val_0_rmse: 0.47426 | val_1_rmse: 0.47863 |  0:01:22s
epoch 24 | loss: 0.23002 | val_0_rmse: 0.47036 | val_1_rmse: 0.47464 |  0:01:25s
epoch 25 | loss: 0.23051 | val_0_rmse: 0.45589 | val_1_rmse: 0.45957 |  0:01:29s
epoch 26 | loss: 0.22414 | val_0_rmse: 0.45959 | val_1_rmse: 0.46131 |  0:01:32s
epoch 27 | loss: 0.24591 | val_0_rmse: 0.57157 | val_1_rmse: 0.57547 |  0:01:36s
epoch 28 | loss: 0.28019 | val_0_rmse: 0.48828 | val_1_rmse: 0.49313 |  0:01:39s
epoch 29 | loss: 0.24532 | val_0_rmse: 0.49158 | val_1_rmse: 0.49716 |  0:01:43s
epoch 30 | loss: 0.2396  | val_0_rmse: 0.50379 | val_1_rmse: 0.50965 |  0:01:46s
epoch 31 | loss: 0.24226 | val_0_rmse: 0.47473 | val_1_rmse: 0.47676 |  0:01:49s
epoch 32 | loss: 0.23702 | val_0_rmse: 0.48072 | val_1_rmse: 0.48693 |  0:01:53s
epoch 33 | loss: 0.22248 | val_0_rmse: 0.46627 | val_1_rmse: 0.46983 |  0:01:56s
epoch 34 | loss: 0.21348 | val_0_rmse: 0.46063 | val_1_rmse: 0.46422 |  0:02:00s
epoch 35 | loss: 0.22155 | val_0_rmse: 0.53525 | val_1_rmse: 0.53436 |  0:02:03s
epoch 36 | loss: 0.2287  | val_0_rmse: 0.49311 | val_1_rmse: 0.49905 |  0:02:07s
epoch 37 | loss: 0.25831 | val_0_rmse: 0.4703  | val_1_rmse: 0.47085 |  0:02:10s
epoch 38 | loss: 0.23016 | val_0_rmse: 0.46925 | val_1_rmse: 0.47456 |  0:02:14s
epoch 39 | loss: 0.22269 | val_0_rmse: 0.46846 | val_1_rmse: 0.47149 |  0:02:17s
epoch 40 | loss: 0.2178  | val_0_rmse: 0.44791 | val_1_rmse: 0.45046 |  0:02:20s
epoch 41 | loss: 0.22235 | val_0_rmse: 0.44544 | val_1_rmse: 0.44769 |  0:02:24s
epoch 42 | loss: 0.21511 | val_0_rmse: 0.44463 | val_1_rmse: 0.44653 |  0:02:27s
epoch 43 | loss: 0.21078 | val_0_rmse: 0.44544 | val_1_rmse: 0.45027 |  0:02:31s
epoch 44 | loss: 0.20837 | val_0_rmse: 0.43436 | val_1_rmse: 0.43765 |  0:02:34s
epoch 45 | loss: 0.2093  | val_0_rmse: 0.43886 | val_1_rmse: 0.44368 |  0:02:38s
epoch 46 | loss: 0.21028 | val_0_rmse: 0.43975 | val_1_rmse: 0.44472 |  0:02:41s
epoch 47 | loss: 0.20613 | val_0_rmse: 0.44754 | val_1_rmse: 0.45196 |  0:02:44s
epoch 48 | loss: 0.20521 | val_0_rmse: 0.43451 | val_1_rmse: 0.43846 |  0:02:48s
epoch 49 | loss: 0.2029  | val_0_rmse: 0.43359 | val_1_rmse: 0.43764 |  0:02:51s
epoch 50 | loss: 0.20822 | val_0_rmse: 0.43683 | val_1_rmse: 0.44318 |  0:02:55s
epoch 51 | loss: 0.20543 | val_0_rmse: 0.4371  | val_1_rmse: 0.4393  |  0:02:58s
epoch 52 | loss: 0.20714 | val_0_rmse: 0.4768  | val_1_rmse: 0.45405 |  0:03:02s
epoch 53 | loss: 0.20704 | val_0_rmse: 0.44413 | val_1_rmse: 0.44836 |  0:03:05s
epoch 54 | loss: 0.20503 | val_0_rmse: 0.43916 | val_1_rmse: 0.44285 |  0:03:08s
epoch 55 | loss: 0.20502 | val_0_rmse: 0.44019 | val_1_rmse: 0.44145 |  0:03:12s
epoch 56 | loss: 0.20212 | val_0_rmse: 0.44209 | val_1_rmse: 0.44592 |  0:03:15s
epoch 57 | loss: 0.2027  | val_0_rmse: 0.43094 | val_1_rmse: 0.43683 |  0:03:19s
epoch 58 | loss: 0.19913 | val_0_rmse: 0.43235 | val_1_rmse: 0.43988 |  0:03:22s
epoch 59 | loss: 0.2034  | val_0_rmse: 0.43751 | val_1_rmse: 0.44399 |  0:03:26s
epoch 60 | loss: 0.20613 | val_0_rmse: 0.44312 | val_1_rmse: 0.44829 |  0:03:29s
epoch 61 | loss: 0.19876 | val_0_rmse: 0.43273 | val_1_rmse: 0.43736 |  0:03:33s
epoch 62 | loss: 0.1968  | val_0_rmse: 0.4335  | val_1_rmse: 0.43767 |  0:03:36s
epoch 63 | loss: 0.19734 | val_0_rmse: 0.42824 | val_1_rmse: 0.43258 |  0:03:39s
epoch 64 | loss: 0.20033 | val_0_rmse: 0.44017 | val_1_rmse: 0.44457 |  0:03:43s
epoch 65 | loss: 0.21177 | val_0_rmse: 0.47239 | val_1_rmse: 0.47293 |  0:03:46s
epoch 66 | loss: 0.20933 | val_0_rmse: 0.44725 | val_1_rmse: 0.44953 |  0:03:50s
epoch 67 | loss: 0.2158  | val_0_rmse: 0.44318 | val_1_rmse: 0.44548 |  0:03:53s
epoch 68 | loss: 0.20543 | val_0_rmse: 0.44683 | val_1_rmse: 0.45084 |  0:03:57s
epoch 69 | loss: 0.20141 | val_0_rmse: 0.45731 | val_1_rmse: 0.45885 |  0:04:00s
epoch 70 | loss: 0.22546 | val_0_rmse: 0.49265 | val_1_rmse: 0.51967 |  0:04:03s
epoch 71 | loss: 0.23237 | val_0_rmse: 0.45827 | val_1_rmse: 0.46263 |  0:04:07s
epoch 72 | loss: 0.21373 | val_0_rmse: 0.47578 | val_1_rmse: 0.47917 |  0:04:10s
epoch 73 | loss: 0.2058  | val_0_rmse: 0.43549 | val_1_rmse: 0.43886 |  0:04:14s
epoch 74 | loss: 0.19993 | val_0_rmse: 0.42942 | val_1_rmse: 0.43172 |  0:04:17s
epoch 75 | loss: 0.2004  | val_0_rmse: 0.43055 | val_1_rmse: 0.43458 |  0:04:21s
epoch 76 | loss: 0.19846 | val_0_rmse: 0.42729 | val_1_rmse: 0.43188 |  0:04:24s
epoch 77 | loss: 0.19817 | val_0_rmse: 0.42566 | val_1_rmse: 0.43294 |  0:04:28s
epoch 78 | loss: 0.19704 | val_0_rmse: 0.42884 | val_1_rmse: 0.43575 |  0:04:31s
epoch 79 | loss: 0.19336 | val_0_rmse: 0.4307  | val_1_rmse: 0.43629 |  0:04:34s
epoch 80 | loss: 0.19373 | val_0_rmse: 0.41851 | val_1_rmse: 0.42614 |  0:04:38s
epoch 81 | loss: 0.19256 | val_0_rmse: 0.42351 | val_1_rmse: 0.43297 |  0:04:41s
epoch 82 | loss: 0.19096 | val_0_rmse: 0.41535 | val_1_rmse: 0.42388 |  0:04:45s
epoch 83 | loss: 0.19041 | val_0_rmse: 0.41424 | val_1_rmse: 0.42101 |  0:04:48s
epoch 84 | loss: 0.19019 | val_0_rmse: 0.41493 | val_1_rmse: 0.42739 |  0:04:52s
epoch 85 | loss: 0.18951 | val_0_rmse: 0.42056 | val_1_rmse: 0.42599 |  0:04:55s
epoch 86 | loss: 0.19155 | val_0_rmse: 0.42256 | val_1_rmse: 0.42965 |  0:04:58s
epoch 87 | loss: 0.1877  | val_0_rmse: 0.41301 | val_1_rmse: 0.42332 |  0:05:02s
epoch 88 | loss: 0.18914 | val_0_rmse: 0.4141  | val_1_rmse: 0.42527 |  0:05:05s
epoch 89 | loss: 0.18901 | val_0_rmse: 0.42039 | val_1_rmse: 0.43168 |  0:05:09s
epoch 90 | loss: 0.1849  | val_0_rmse: 0.41797 | val_1_rmse: 0.42514 |  0:05:12s
epoch 91 | loss: 0.1869  | val_0_rmse: 0.4162  | val_1_rmse: 0.42759 |  0:05:16s
epoch 92 | loss: 0.18505 | val_0_rmse: 0.41361 | val_1_rmse: 0.42523 |  0:05:19s
epoch 93 | loss: 0.18398 | val_0_rmse: 0.41414 | val_1_rmse: 0.42627 |  0:05:23s
epoch 94 | loss: 0.18491 | val_0_rmse: 0.41314 | val_1_rmse: 0.42504 |  0:05:26s
epoch 95 | loss: 0.18729 | val_0_rmse: 0.4129  | val_1_rmse: 0.42424 |  0:05:30s
epoch 96 | loss: 0.18598 | val_0_rmse: 0.42181 | val_1_rmse: 0.43038 |  0:05:33s
epoch 97 | loss: 0.18299 | val_0_rmse: 0.40858 | val_1_rmse: 0.42266 |  0:05:37s
epoch 98 | loss: 0.18441 | val_0_rmse: 0.41749 | val_1_rmse: 0.43136 |  0:05:40s
epoch 99 | loss: 0.18505 | val_0_rmse: 0.41773 | val_1_rmse: 0.4257  |  0:05:43s
epoch 100| loss: 0.18452 | val_0_rmse: 0.40632 | val_1_rmse: 0.41684 |  0:05:47s
epoch 101| loss: 0.18502 | val_0_rmse: 0.40863 | val_1_rmse: 0.42192 |  0:05:50s
epoch 102| loss: 0.18154 | val_0_rmse: 0.40838 | val_1_rmse: 0.42481 |  0:05:54s
epoch 103| loss: 0.18227 | val_0_rmse: 0.40424 | val_1_rmse: 0.44624 |  0:05:57s
epoch 104| loss: 0.18117 | val_0_rmse: 0.40259 | val_1_rmse: 0.44025 |  0:06:01s
epoch 105| loss: 0.18345 | val_0_rmse: 0.40967 | val_1_rmse: 0.44162 |  0:06:04s
epoch 106| loss: 0.18257 | val_0_rmse: 0.40361 | val_1_rmse: 0.43616 |  0:06:08s
epoch 107| loss: 0.18027 | val_0_rmse: 0.40866 | val_1_rmse: 0.43011 |  0:06:11s
epoch 108| loss: 0.1811  | val_0_rmse: 0.40946 | val_1_rmse: 0.4245  |  0:06:14s
epoch 109| loss: 0.18118 | val_0_rmse: 0.43683 | val_1_rmse: 0.63037 |  0:06:18s
epoch 110| loss: 0.1808  | val_0_rmse: 0.40512 | val_1_rmse: 0.42017 |  0:06:21s
epoch 111| loss: 0.18292 | val_0_rmse: 0.40454 | val_1_rmse: 0.42299 |  0:06:25s
epoch 112| loss: 0.1806  | val_0_rmse: 0.4141  | val_1_rmse: 0.42989 |  0:06:28s
epoch 113| loss: 0.1805  | val_0_rmse: 0.40298 | val_1_rmse: 0.42409 |  0:06:32s
epoch 114| loss: 0.18059 | val_0_rmse: 0.40831 | val_1_rmse: 0.56341 |  0:06:35s
epoch 115| loss: 0.17945 | val_0_rmse: 0.43086 | val_1_rmse: 0.60213 |  0:06:38s
epoch 116| loss: 0.17731 | val_0_rmse: 0.42356 | val_1_rmse: 0.57365 |  0:06:42s
epoch 117| loss: 0.18053 | val_0_rmse: 0.40111 | val_1_rmse: 0.4527  |  0:06:45s
epoch 118| loss: 0.17859 | val_0_rmse: 0.41211 | val_1_rmse: 0.52559 |  0:06:49s
epoch 119| loss: 0.18002 | val_0_rmse: 0.40217 | val_1_rmse: 0.49835 |  0:06:52s
epoch 120| loss: 0.17617 | val_0_rmse: 0.39896 | val_1_rmse: 0.45884 |  0:06:56s
epoch 121| loss: 0.17981 | val_0_rmse: 0.39792 | val_1_rmse: 0.4489  |  0:06:59s
epoch 122| loss: 0.17745 | val_0_rmse: 0.40257 | val_1_rmse: 0.47737 |  0:07:03s
epoch 123| loss: 0.17687 | val_0_rmse: 0.40701 | val_1_rmse: 0.44612 |  0:07:06s
epoch 124| loss: 0.17887 | val_0_rmse: 0.39758 | val_1_rmse: 0.44836 |  0:07:09s
epoch 125| loss: 0.17496 | val_0_rmse: 0.40105 | val_1_rmse: 0.42471 |  0:07:13s
epoch 126| loss: 0.17998 | val_0_rmse: 0.40701 | val_1_rmse: 0.43507 |  0:07:16s
epoch 127| loss: 0.18052 | val_0_rmse: 0.40594 | val_1_rmse: 0.45091 |  0:07:20s
epoch 128| loss: 0.17877 | val_0_rmse: 0.39944 | val_1_rmse: 0.41754 |  0:07:23s
epoch 129| loss: 0.17616 | val_0_rmse: 0.39816 | val_1_rmse: 0.41709 |  0:07:27s
epoch 130| loss: 0.17668 | val_0_rmse: 0.41335 | val_1_rmse: 0.46532 |  0:07:30s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.41684
Best weights from best epoch are automatically used!
ended training at: 17:09:50
Feature importance:
Mean squared error is of 9937294589.22981
Mean absolute error:67208.8350838228
MAPE:0.2565392011184001
R2 score:0.829784182440175
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:09:53
epoch 0  | loss: 1.24496 | val_0_rmse: 0.9211  | val_1_rmse: 0.93019 |  0:00:01s
epoch 1  | loss: 0.70946 | val_0_rmse: 0.70178 | val_1_rmse: 0.7144  |  0:00:02s
epoch 2  | loss: 0.41033 | val_0_rmse: 0.67401 | val_1_rmse: 0.69448 |  0:00:03s
epoch 3  | loss: 0.33911 | val_0_rmse: 0.62229 | val_1_rmse: 0.63018 |  0:00:04s
epoch 4  | loss: 0.30546 | val_0_rmse: 0.64432 | val_1_rmse: 0.64963 |  0:00:05s
epoch 5  | loss: 0.28593 | val_0_rmse: 0.59524 | val_1_rmse: 0.59459 |  0:00:06s
epoch 6  | loss: 0.26681 | val_0_rmse: 0.57832 | val_1_rmse: 0.58414 |  0:00:07s
epoch 7  | loss: 0.24777 | val_0_rmse: 0.56356 | val_1_rmse: 0.56934 |  0:00:09s
epoch 8  | loss: 0.24886 | val_0_rmse: 0.55895 | val_1_rmse: 0.56187 |  0:00:10s
epoch 9  | loss: 0.24576 | val_0_rmse: 0.55247 | val_1_rmse: 0.55053 |  0:00:11s
epoch 10 | loss: 0.23597 | val_0_rmse: 0.55123 | val_1_rmse: 0.54454 |  0:00:12s
epoch 11 | loss: 0.22423 | val_0_rmse: 0.53433 | val_1_rmse: 0.53109 |  0:00:13s
epoch 12 | loss: 0.21928 | val_0_rmse: 0.54017 | val_1_rmse: 0.53665 |  0:00:14s
epoch 13 | loss: 0.22051 | val_0_rmse: 0.532   | val_1_rmse: 0.52871 |  0:00:15s
epoch 14 | loss: 0.22016 | val_0_rmse: 0.55279 | val_1_rmse: 0.54955 |  0:00:17s
epoch 15 | loss: 0.22959 | val_0_rmse: 0.52696 | val_1_rmse: 0.52451 |  0:00:18s
epoch 16 | loss: 0.21775 | val_0_rmse: 0.50395 | val_1_rmse: 0.50741 |  0:00:19s
epoch 17 | loss: 0.21065 | val_0_rmse: 0.49722 | val_1_rmse: 0.50168 |  0:00:20s
epoch 18 | loss: 0.21216 | val_0_rmse: 0.48794 | val_1_rmse: 0.4913  |  0:00:21s
epoch 19 | loss: 0.20775 | val_0_rmse: 0.48331 | val_1_rmse: 0.48668 |  0:00:22s
epoch 20 | loss: 0.20746 | val_0_rmse: 0.47361 | val_1_rmse: 0.47594 |  0:00:23s
epoch 21 | loss: 0.20486 | val_0_rmse: 0.46996 | val_1_rmse: 0.47145 |  0:00:25s
epoch 22 | loss: 0.20968 | val_0_rmse: 0.47536 | val_1_rmse: 0.47828 |  0:00:26s
epoch 23 | loss: 0.21092 | val_0_rmse: 0.47765 | val_1_rmse: 0.47785 |  0:00:27s
epoch 24 | loss: 0.21017 | val_0_rmse: 0.46905 | val_1_rmse: 0.4684  |  0:00:28s
epoch 25 | loss: 0.20738 | val_0_rmse: 0.46262 | val_1_rmse: 0.46117 |  0:00:29s
epoch 26 | loss: 0.20546 | val_0_rmse: 0.45622 | val_1_rmse: 0.45627 |  0:00:30s
epoch 27 | loss: 0.2026  | val_0_rmse: 0.45523 | val_1_rmse: 0.45815 |  0:00:31s
epoch 28 | loss: 0.2021  | val_0_rmse: 0.44574 | val_1_rmse: 0.44939 |  0:00:33s
epoch 29 | loss: 0.20252 | val_0_rmse: 0.44798 | val_1_rmse: 0.45078 |  0:00:34s
epoch 30 | loss: 0.20074 | val_0_rmse: 0.43799 | val_1_rmse: 0.44097 |  0:00:35s
epoch 31 | loss: 0.20309 | val_0_rmse: 0.44469 | val_1_rmse: 0.44617 |  0:00:36s
epoch 32 | loss: 0.20409 | val_0_rmse: 0.46644 | val_1_rmse: 0.47097 |  0:00:37s
epoch 33 | loss: 0.20557 | val_0_rmse: 0.43946 | val_1_rmse: 0.44745 |  0:00:38s
epoch 34 | loss: 0.20175 | val_0_rmse: 0.43606 | val_1_rmse: 0.44102 |  0:00:39s
epoch 35 | loss: 0.19925 | val_0_rmse: 0.44522 | val_1_rmse: 0.4471  |  0:00:40s
epoch 36 | loss: 0.19923 | val_0_rmse: 0.43789 | val_1_rmse: 0.4428  |  0:00:42s
epoch 37 | loss: 0.20266 | val_0_rmse: 0.4681  | val_1_rmse: 0.47936 |  0:00:43s
epoch 38 | loss: 0.2009  | val_0_rmse: 0.43476 | val_1_rmse: 0.44112 |  0:00:44s
epoch 39 | loss: 0.19988 | val_0_rmse: 0.43487 | val_1_rmse: 0.44067 |  0:00:45s
epoch 40 | loss: 0.19949 | val_0_rmse: 0.44579 | val_1_rmse: 0.45525 |  0:00:46s
epoch 41 | loss: 0.20267 | val_0_rmse: 0.4347  | val_1_rmse: 0.44169 |  0:00:47s
epoch 42 | loss: 0.19747 | val_0_rmse: 0.43753 | val_1_rmse: 0.44332 |  0:00:48s
epoch 43 | loss: 0.2002  | val_0_rmse: 0.42868 | val_1_rmse: 0.43567 |  0:00:50s
epoch 44 | loss: 0.19579 | val_0_rmse: 0.42894 | val_1_rmse: 0.4352  |  0:00:51s
epoch 45 | loss: 0.1972  | val_0_rmse: 0.42906 | val_1_rmse: 0.43592 |  0:00:52s
epoch 46 | loss: 0.19509 | val_0_rmse: 0.42445 | val_1_rmse: 0.43349 |  0:00:53s
epoch 47 | loss: 0.19461 | val_0_rmse: 0.42585 | val_1_rmse: 0.4336  |  0:00:54s
epoch 48 | loss: 0.19312 | val_0_rmse: 0.42598 | val_1_rmse: 0.43438 |  0:00:55s
epoch 49 | loss: 0.19299 | val_0_rmse: 0.42328 | val_1_rmse: 0.43287 |  0:00:56s
epoch 50 | loss: 0.19358 | val_0_rmse: 0.4278  | val_1_rmse: 0.44007 |  0:00:58s
epoch 51 | loss: 0.19625 | val_0_rmse: 0.43816 | val_1_rmse: 0.45094 |  0:00:59s
epoch 52 | loss: 0.19851 | val_0_rmse: 0.42758 | val_1_rmse: 0.43813 |  0:01:00s
epoch 53 | loss: 0.19519 | val_0_rmse: 0.42839 | val_1_rmse: 0.44017 |  0:01:01s
epoch 54 | loss: 0.19594 | val_0_rmse: 0.43041 | val_1_rmse: 0.43757 |  0:01:02s
epoch 55 | loss: 0.19165 | val_0_rmse: 0.42313 | val_1_rmse: 0.43342 |  0:01:03s
epoch 56 | loss: 0.18906 | val_0_rmse: 0.42622 | val_1_rmse: 0.43764 |  0:01:04s
epoch 57 | loss: 0.1932  | val_0_rmse: 0.42323 | val_1_rmse: 0.43472 |  0:01:05s
epoch 58 | loss: 0.19224 | val_0_rmse: 0.42184 | val_1_rmse: 0.43545 |  0:01:07s
epoch 59 | loss: 0.18919 | val_0_rmse: 0.42393 | val_1_rmse: 0.44009 |  0:01:08s
epoch 60 | loss: 0.19004 | val_0_rmse: 0.42071 | val_1_rmse: 0.43344 |  0:01:09s
epoch 61 | loss: 0.1887  | val_0_rmse: 0.4231  | val_1_rmse: 0.4354  |  0:01:10s
epoch 62 | loss: 0.19477 | val_0_rmse: 0.42787 | val_1_rmse: 0.43915 |  0:01:11s
epoch 63 | loss: 0.18936 | val_0_rmse: 0.43628 | val_1_rmse: 0.45591 |  0:01:12s
epoch 64 | loss: 0.19758 | val_0_rmse: 0.42601 | val_1_rmse: 0.44171 |  0:01:13s
epoch 65 | loss: 0.19404 | val_0_rmse: 0.42167 | val_1_rmse: 0.43352 |  0:01:15s
epoch 66 | loss: 0.19052 | val_0_rmse: 0.43109 | val_1_rmse: 0.44389 |  0:01:16s
epoch 67 | loss: 0.19264 | val_0_rmse: 0.42462 | val_1_rmse: 0.43759 |  0:01:17s
epoch 68 | loss: 0.1903  | val_0_rmse: 0.4233  | val_1_rmse: 0.43707 |  0:01:18s
epoch 69 | loss: 0.18852 | val_0_rmse: 0.42206 | val_1_rmse: 0.4358  |  0:01:19s
epoch 70 | loss: 0.18806 | val_0_rmse: 0.41944 | val_1_rmse: 0.43692 |  0:01:20s
epoch 71 | loss: 0.18384 | val_0_rmse: 0.41612 | val_1_rmse: 0.43365 |  0:01:21s
epoch 72 | loss: 0.18737 | val_0_rmse: 0.42908 | val_1_rmse: 0.44138 |  0:01:22s
epoch 73 | loss: 0.19284 | val_0_rmse: 0.42302 | val_1_rmse: 0.44001 |  0:01:24s
epoch 74 | loss: 0.19372 | val_0_rmse: 0.42868 | val_1_rmse: 0.4453  |  0:01:25s
epoch 75 | loss: 0.19391 | val_0_rmse: 0.43424 | val_1_rmse: 0.45042 |  0:01:26s
epoch 76 | loss: 0.19231 | val_0_rmse: 0.41726 | val_1_rmse: 0.43715 |  0:01:27s
epoch 77 | loss: 0.18518 | val_0_rmse: 0.41721 | val_1_rmse: 0.43537 |  0:01:28s
epoch 78 | loss: 0.18311 | val_0_rmse: 0.42751 | val_1_rmse: 0.44789 |  0:01:29s
epoch 79 | loss: 0.18658 | val_0_rmse: 0.4163  | val_1_rmse: 0.43697 |  0:01:30s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.43287
Best weights from best epoch are automatically used!
ended training at: 17:11:24
Feature importance:
Mean squared error is of 6489969562.921213
Mean absolute error:56107.13745129931
MAPE:0.14632590904833068
R2 score:0.785291555792553
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:11:25
epoch 0  | loss: 1.30089 | val_0_rmse: 0.96972 | val_1_rmse: 0.97969 |  0:00:01s
epoch 1  | loss: 0.55901 | val_0_rmse: 0.69569 | val_1_rmse: 0.69711 |  0:00:02s
epoch 2  | loss: 0.32515 | val_0_rmse: 0.59875 | val_1_rmse: 0.58838 |  0:00:03s
epoch 3  | loss: 0.29224 | val_0_rmse: 0.57694 | val_1_rmse: 0.56275 |  0:00:04s
epoch 4  | loss: 0.29068 | val_0_rmse: 0.52299 | val_1_rmse: 0.51394 |  0:00:05s
epoch 5  | loss: 0.27859 | val_0_rmse: 0.55    | val_1_rmse: 0.54642 |  0:00:06s
epoch 6  | loss: 0.26091 | val_0_rmse: 0.50504 | val_1_rmse: 0.49516 |  0:00:08s
epoch 7  | loss: 0.24743 | val_0_rmse: 0.50413 | val_1_rmse: 0.49566 |  0:00:09s
epoch 8  | loss: 0.2793  | val_0_rmse: 0.52329 | val_1_rmse: 0.51098 |  0:00:10s
epoch 9  | loss: 0.24816 | val_0_rmse: 0.51182 | val_1_rmse: 0.50439 |  0:00:11s
epoch 10 | loss: 0.24452 | val_0_rmse: 0.50762 | val_1_rmse: 0.49879 |  0:00:12s
epoch 11 | loss: 0.24599 | val_0_rmse: 0.52491 | val_1_rmse: 0.51638 |  0:00:13s
epoch 12 | loss: 0.23511 | val_0_rmse: 0.52965 | val_1_rmse: 0.51811 |  0:00:14s
epoch 13 | loss: 0.23389 | val_0_rmse: 0.50935 | val_1_rmse: 0.50586 |  0:00:15s
epoch 14 | loss: 0.24112 | val_0_rmse: 0.51634 | val_1_rmse: 0.51306 |  0:00:17s
epoch 15 | loss: 0.2314  | val_0_rmse: 0.51776 | val_1_rmse: 0.51244 |  0:00:18s
epoch 16 | loss: 0.22657 | val_0_rmse: 0.5124  | val_1_rmse: 0.50333 |  0:00:19s
epoch 17 | loss: 0.22262 | val_0_rmse: 0.50848 | val_1_rmse: 0.50215 |  0:00:20s
epoch 18 | loss: 0.22315 | val_0_rmse: 0.5032  | val_1_rmse: 0.49489 |  0:00:21s
epoch 19 | loss: 0.21683 | val_0_rmse: 0.49499 | val_1_rmse: 0.48859 |  0:00:22s
epoch 20 | loss: 0.22243 | val_0_rmse: 0.49446 | val_1_rmse: 0.48746 |  0:00:23s
epoch 21 | loss: 0.21861 | val_0_rmse: 0.4819  | val_1_rmse: 0.4726  |  0:00:24s
epoch 22 | loss: 0.21566 | val_0_rmse: 0.47939 | val_1_rmse: 0.47133 |  0:00:26s
epoch 23 | loss: 0.21997 | val_0_rmse: 0.48192 | val_1_rmse: 0.47291 |  0:00:27s
epoch 24 | loss: 0.22305 | val_0_rmse: 0.49944 | val_1_rmse: 0.49011 |  0:00:28s
epoch 25 | loss: 0.22524 | val_0_rmse: 0.48583 | val_1_rmse: 0.47934 |  0:00:29s
epoch 26 | loss: 0.22672 | val_0_rmse: 0.48272 | val_1_rmse: 0.47683 |  0:00:30s
epoch 27 | loss: 0.21865 | val_0_rmse: 0.45995 | val_1_rmse: 0.45349 |  0:00:31s
epoch 28 | loss: 0.21602 | val_0_rmse: 0.45645 | val_1_rmse: 0.453   |  0:00:32s
epoch 29 | loss: 0.21672 | val_0_rmse: 0.45779 | val_1_rmse: 0.45368 |  0:00:34s
epoch 30 | loss: 0.21344 | val_0_rmse: 0.45973 | val_1_rmse: 0.45282 |  0:00:35s
epoch 31 | loss: 0.22089 | val_0_rmse: 0.45252 | val_1_rmse: 0.4488  |  0:00:36s
epoch 32 | loss: 0.2154  | val_0_rmse: 0.45467 | val_1_rmse: 0.45204 |  0:00:37s
epoch 33 | loss: 0.21293 | val_0_rmse: 0.45409 | val_1_rmse: 0.45078 |  0:00:38s
epoch 34 | loss: 0.21076 | val_0_rmse: 0.44471 | val_1_rmse: 0.44222 |  0:00:39s
epoch 35 | loss: 0.21099 | val_0_rmse: 0.44787 | val_1_rmse: 0.4474  |  0:00:41s
epoch 36 | loss: 0.21291 | val_0_rmse: 0.44648 | val_1_rmse: 0.44376 |  0:00:42s
epoch 37 | loss: 0.2106  | val_0_rmse: 0.44411 | val_1_rmse: 0.4419  |  0:00:43s
epoch 38 | loss: 0.21301 | val_0_rmse: 0.4514  | val_1_rmse: 0.44841 |  0:00:44s
epoch 39 | loss: 0.21608 | val_0_rmse: 0.45281 | val_1_rmse: 0.45067 |  0:00:45s
epoch 40 | loss: 0.21446 | val_0_rmse: 0.46866 | val_1_rmse: 0.46427 |  0:00:46s
epoch 41 | loss: 0.21144 | val_0_rmse: 0.46931 | val_1_rmse: 0.46873 |  0:00:47s
epoch 42 | loss: 0.20903 | val_0_rmse: 0.44925 | val_1_rmse: 0.44801 |  0:00:48s
epoch 43 | loss: 0.20956 | val_0_rmse: 0.44991 | val_1_rmse: 0.44903 |  0:00:50s
epoch 44 | loss: 0.20673 | val_0_rmse: 0.44368 | val_1_rmse: 0.44339 |  0:00:51s
epoch 45 | loss: 0.20973 | val_0_rmse: 0.44208 | val_1_rmse: 0.44275 |  0:00:52s
epoch 46 | loss: 0.20689 | val_0_rmse: 0.44553 | val_1_rmse: 0.44772 |  0:00:53s
epoch 47 | loss: 0.20726 | val_0_rmse: 0.44001 | val_1_rmse: 0.44124 |  0:00:54s
epoch 48 | loss: 0.20529 | val_0_rmse: 0.44087 | val_1_rmse: 0.44287 |  0:00:55s
epoch 49 | loss: 0.20785 | val_0_rmse: 0.43995 | val_1_rmse: 0.44211 |  0:00:56s
epoch 50 | loss: 0.20519 | val_0_rmse: 0.43947 | val_1_rmse: 0.44038 |  0:00:57s
epoch 51 | loss: 0.20719 | val_0_rmse: 0.44444 | val_1_rmse: 0.44224 |  0:00:59s
epoch 52 | loss: 0.20894 | val_0_rmse: 0.44622 | val_1_rmse: 0.44828 |  0:01:00s
epoch 53 | loss: 0.21111 | val_0_rmse: 0.44309 | val_1_rmse: 0.44375 |  0:01:01s
epoch 54 | loss: 0.20391 | val_0_rmse: 0.43762 | val_1_rmse: 0.43912 |  0:01:02s
epoch 55 | loss: 0.20311 | val_0_rmse: 0.442   | val_1_rmse: 0.44948 |  0:01:03s
epoch 56 | loss: 0.20651 | val_0_rmse: 0.43659 | val_1_rmse: 0.43925 |  0:01:04s
epoch 57 | loss: 0.20182 | val_0_rmse: 0.43647 | val_1_rmse: 0.43976 |  0:01:05s
epoch 58 | loss: 0.19982 | val_0_rmse: 0.43937 | val_1_rmse: 0.44406 |  0:01:07s
epoch 59 | loss: 0.207   | val_0_rmse: 0.45791 | val_1_rmse: 0.45919 |  0:01:08s
epoch 60 | loss: 0.20592 | val_0_rmse: 0.43942 | val_1_rmse: 0.44166 |  0:01:09s
epoch 61 | loss: 0.20225 | val_0_rmse: 0.43803 | val_1_rmse: 0.43904 |  0:01:10s
epoch 62 | loss: 0.20332 | val_0_rmse: 0.43569 | val_1_rmse: 0.43951 |  0:01:11s
epoch 63 | loss: 0.20599 | val_0_rmse: 0.43385 | val_1_rmse: 0.43914 |  0:01:12s
epoch 64 | loss: 0.2057  | val_0_rmse: 0.44314 | val_1_rmse: 0.44531 |  0:01:13s
epoch 65 | loss: 0.20649 | val_0_rmse: 0.46752 | val_1_rmse: 0.47155 |  0:01:15s
epoch 66 | loss: 0.20447 | val_0_rmse: 0.46938 | val_1_rmse: 0.47241 |  0:01:16s
epoch 67 | loss: 0.20229 | val_0_rmse: 0.43034 | val_1_rmse: 0.43572 |  0:01:17s
epoch 68 | loss: 0.20031 | val_0_rmse: 0.43583 | val_1_rmse: 0.44076 |  0:01:18s
epoch 69 | loss: 0.20059 | val_0_rmse: 0.43434 | val_1_rmse: 0.43672 |  0:01:19s
epoch 70 | loss: 0.20854 | val_0_rmse: 0.44992 | val_1_rmse: 0.4518  |  0:01:20s
epoch 71 | loss: 0.24623 | val_0_rmse: 0.48818 | val_1_rmse: 0.48922 |  0:01:21s
epoch 72 | loss: 0.24277 | val_0_rmse: 0.53095 | val_1_rmse: 0.52734 |  0:01:23s
epoch 73 | loss: 0.2422  | val_0_rmse: 0.51237 | val_1_rmse: 0.51259 |  0:01:24s
epoch 74 | loss: 0.24925 | val_0_rmse: 0.48833 | val_1_rmse: 0.48159 |  0:01:25s
epoch 75 | loss: 0.2884  | val_0_rmse: 0.5095  | val_1_rmse: 0.50472 |  0:01:26s
epoch 76 | loss: 0.3381  | val_0_rmse: 0.55161 | val_1_rmse: 0.55187 |  0:01:27s
epoch 77 | loss: 0.25759 | val_0_rmse: 0.54511 | val_1_rmse: 0.545   |  0:01:28s
epoch 78 | loss: 0.25036 | val_0_rmse: 0.49672 | val_1_rmse: 0.49405 |  0:01:29s
epoch 79 | loss: 0.24335 | val_0_rmse: 0.53015 | val_1_rmse: 0.52868 |  0:01:30s
epoch 80 | loss: 0.23847 | val_0_rmse: 0.48365 | val_1_rmse: 0.47981 |  0:01:32s
epoch 81 | loss: 0.22779 | val_0_rmse: 0.47252 | val_1_rmse: 0.46868 |  0:01:33s
epoch 82 | loss: 0.22883 | val_0_rmse: 0.47565 | val_1_rmse: 0.47591 |  0:01:34s
epoch 83 | loss: 0.23167 | val_0_rmse: 0.46034 | val_1_rmse: 0.45756 |  0:01:35s
epoch 84 | loss: 0.23129 | val_0_rmse: 0.46703 | val_1_rmse: 0.46203 |  0:01:36s
epoch 85 | loss: 0.22616 | val_0_rmse: 0.45913 | val_1_rmse: 0.45383 |  0:01:37s
epoch 86 | loss: 0.22314 | val_0_rmse: 0.46243 | val_1_rmse: 0.45986 |  0:01:38s
epoch 87 | loss: 0.22411 | val_0_rmse: 0.46001 | val_1_rmse: 0.45545 |  0:01:39s
epoch 88 | loss: 0.22577 | val_0_rmse: 0.45643 | val_1_rmse: 0.45351 |  0:01:41s
epoch 89 | loss: 0.23376 | val_0_rmse: 0.48781 | val_1_rmse: 0.48252 |  0:01:42s
epoch 90 | loss: 0.23268 | val_0_rmse: 0.45732 | val_1_rmse: 0.4531  |  0:01:43s
epoch 91 | loss: 0.22164 | val_0_rmse: 0.45628 | val_1_rmse: 0.4525  |  0:01:44s
epoch 92 | loss: 0.22194 | val_0_rmse: 0.4522  | val_1_rmse: 0.44976 |  0:01:45s
epoch 93 | loss: 0.22138 | val_0_rmse: 0.45705 | val_1_rmse: 0.45457 |  0:01:46s
epoch 94 | loss: 0.22084 | val_0_rmse: 0.45278 | val_1_rmse: 0.45155 |  0:01:47s
epoch 95 | loss: 0.21716 | val_0_rmse: 0.45611 | val_1_rmse: 0.4536  |  0:01:49s
epoch 96 | loss: 0.2191  | val_0_rmse: 0.44845 | val_1_rmse: 0.44425 |  0:01:50s
epoch 97 | loss: 0.21282 | val_0_rmse: 0.44962 | val_1_rmse: 0.44625 |  0:01:51s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.43572
Best weights from best epoch are automatically used!
ended training at: 17:13:17
Feature importance:
Mean squared error is of 6503986322.945346
Mean absolute error:55962.77493617224
MAPE:0.14677370801035186
R2 score:0.7870327337768225
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:13:17
epoch 0  | loss: 1.26146 | val_0_rmse: 0.99604 | val_1_rmse: 0.99943 |  0:00:01s
epoch 1  | loss: 0.88964 | val_0_rmse: 0.81439 | val_1_rmse: 0.82042 |  0:00:02s
epoch 2  | loss: 0.61417 | val_0_rmse: 0.76734 | val_1_rmse: 0.79972 |  0:00:03s
epoch 3  | loss: 0.48399 | val_0_rmse: 0.84013 | val_1_rmse: 0.86696 |  0:00:04s
epoch 4  | loss: 0.42544 | val_0_rmse: 0.72203 | val_1_rmse: 0.75407 |  0:00:05s
epoch 5  | loss: 0.37966 | val_0_rmse: 0.67615 | val_1_rmse: 0.7063  |  0:00:06s
epoch 6  | loss: 0.33327 | val_0_rmse: 0.64866 | val_1_rmse: 0.67534 |  0:00:08s
epoch 7  | loss: 0.34647 | val_0_rmse: 0.69551 | val_1_rmse: 0.71961 |  0:00:09s
epoch 8  | loss: 0.31344 | val_0_rmse: 0.61158 | val_1_rmse: 0.63963 |  0:00:10s
epoch 9  | loss: 0.29007 | val_0_rmse: 0.64726 | val_1_rmse: 0.67506 |  0:00:11s
epoch 10 | loss: 0.27379 | val_0_rmse: 0.58287 | val_1_rmse: 0.60953 |  0:00:12s
epoch 11 | loss: 0.25945 | val_0_rmse: 0.58175 | val_1_rmse: 0.61329 |  0:00:13s
epoch 12 | loss: 0.25367 | val_0_rmse: 0.5615  | val_1_rmse: 0.58968 |  0:00:14s
epoch 13 | loss: 0.25374 | val_0_rmse: 0.58592 | val_1_rmse: 0.61361 |  0:00:15s
epoch 14 | loss: 0.23903 | val_0_rmse: 0.56038 | val_1_rmse: 0.58285 |  0:00:17s
epoch 15 | loss: 0.24563 | val_0_rmse: 0.55605 | val_1_rmse: 0.58336 |  0:00:18s
epoch 16 | loss: 0.23453 | val_0_rmse: 0.54183 | val_1_rmse: 0.57092 |  0:00:19s
epoch 17 | loss: 0.2364  | val_0_rmse: 0.52266 | val_1_rmse: 0.55417 |  0:00:20s
epoch 18 | loss: 0.23176 | val_0_rmse: 0.50976 | val_1_rmse: 0.53988 |  0:00:21s
epoch 19 | loss: 0.22772 | val_0_rmse: 0.51194 | val_1_rmse: 0.54217 |  0:00:22s
epoch 20 | loss: 0.22695 | val_0_rmse: 0.51504 | val_1_rmse: 0.54828 |  0:00:23s
epoch 21 | loss: 0.22465 | val_0_rmse: 0.50043 | val_1_rmse: 0.53254 |  0:00:24s
epoch 22 | loss: 0.22053 | val_0_rmse: 0.49841 | val_1_rmse: 0.52442 |  0:00:26s
epoch 23 | loss: 0.21778 | val_0_rmse: 0.48706 | val_1_rmse: 0.51934 |  0:00:27s
epoch 24 | loss: 0.21539 | val_0_rmse: 0.4898  | val_1_rmse: 0.51849 |  0:00:28s
epoch 25 | loss: 0.21266 | val_0_rmse: 0.48411 | val_1_rmse: 0.51297 |  0:00:29s
epoch 26 | loss: 0.21405 | val_0_rmse: 0.47647 | val_1_rmse: 0.50378 |  0:00:30s
epoch 27 | loss: 0.21491 | val_0_rmse: 0.47283 | val_1_rmse: 0.499   |  0:00:31s
epoch 28 | loss: 0.21334 | val_0_rmse: 0.46856 | val_1_rmse: 0.49343 |  0:00:32s
epoch 29 | loss: 0.21206 | val_0_rmse: 0.46268 | val_1_rmse: 0.48956 |  0:00:34s
epoch 30 | loss: 0.21182 | val_0_rmse: 0.46314 | val_1_rmse: 0.49373 |  0:00:35s
epoch 31 | loss: 0.21879 | val_0_rmse: 0.45893 | val_1_rmse: 0.49242 |  0:00:36s
epoch 32 | loss: 0.21612 | val_0_rmse: 0.4542  | val_1_rmse: 0.48915 |  0:00:37s
epoch 33 | loss: 0.20971 | val_0_rmse: 0.46887 | val_1_rmse: 0.49651 |  0:00:38s
epoch 34 | loss: 0.21054 | val_0_rmse: 0.47293 | val_1_rmse: 0.50417 |  0:00:39s
epoch 35 | loss: 0.21102 | val_0_rmse: 0.46615 | val_1_rmse: 0.49959 |  0:00:40s
epoch 36 | loss: 0.20537 | val_0_rmse: 0.4491  | val_1_rmse: 0.48426 |  0:00:42s
epoch 37 | loss: 0.20545 | val_0_rmse: 0.45668 | val_1_rmse: 0.49231 |  0:00:43s
epoch 38 | loss: 0.20307 | val_0_rmse: 0.44508 | val_1_rmse: 0.47911 |  0:00:44s
epoch 39 | loss: 0.20187 | val_0_rmse: 0.43674 | val_1_rmse: 0.4694  |  0:00:45s
epoch 40 | loss: 0.20184 | val_0_rmse: 0.44139 | val_1_rmse: 0.47224 |  0:00:46s
epoch 41 | loss: 0.20188 | val_0_rmse: 0.4415  | val_1_rmse: 0.46955 |  0:00:47s
epoch 42 | loss: 0.19885 | val_0_rmse: 0.43384 | val_1_rmse: 0.46545 |  0:00:48s
epoch 43 | loss: 0.19724 | val_0_rmse: 0.44109 | val_1_rmse: 0.47779 |  0:00:49s
epoch 44 | loss: 0.19813 | val_0_rmse: 0.44357 | val_1_rmse: 0.47296 |  0:00:51s
epoch 45 | loss: 0.20252 | val_0_rmse: 0.43795 | val_1_rmse: 0.4701  |  0:00:52s
epoch 46 | loss: 0.20883 | val_0_rmse: 0.4562  | val_1_rmse: 0.48521 |  0:00:53s
epoch 47 | loss: 0.20959 | val_0_rmse: 0.46527 | val_1_rmse: 0.49047 |  0:00:54s
epoch 48 | loss: 0.21052 | val_0_rmse: 0.4364  | val_1_rmse: 0.46322 |  0:00:55s
epoch 49 | loss: 0.20455 | val_0_rmse: 0.43436 | val_1_rmse: 0.46408 |  0:00:56s
epoch 50 | loss: 0.20202 | val_0_rmse: 0.43386 | val_1_rmse: 0.46495 |  0:00:57s
epoch 51 | loss: 0.19752 | val_0_rmse: 0.42547 | val_1_rmse: 0.4606  |  0:00:59s
epoch 52 | loss: 0.19717 | val_0_rmse: 0.43298 | val_1_rmse: 0.46873 |  0:01:00s
epoch 53 | loss: 0.19742 | val_0_rmse: 0.44653 | val_1_rmse: 0.47869 |  0:01:01s
epoch 54 | loss: 0.19515 | val_0_rmse: 0.42678 | val_1_rmse: 0.46031 |  0:01:02s
epoch 55 | loss: 0.19538 | val_0_rmse: 0.42662 | val_1_rmse: 0.45867 |  0:01:03s
epoch 56 | loss: 0.19468 | val_0_rmse: 0.43007 | val_1_rmse: 0.46158 |  0:01:04s
epoch 57 | loss: 0.19548 | val_0_rmse: 0.43514 | val_1_rmse: 0.47024 |  0:01:05s
epoch 58 | loss: 0.1917  | val_0_rmse: 0.4214  | val_1_rmse: 0.45597 |  0:01:06s
epoch 59 | loss: 0.19145 | val_0_rmse: 0.42077 | val_1_rmse: 0.46117 |  0:01:08s
epoch 60 | loss: 0.19512 | val_0_rmse: 0.43375 | val_1_rmse: 0.46666 |  0:01:09s
epoch 61 | loss: 0.19256 | val_0_rmse: 0.42835 | val_1_rmse: 0.46779 |  0:01:10s
epoch 62 | loss: 0.19084 | val_0_rmse: 0.42853 | val_1_rmse: 0.46625 |  0:01:11s
epoch 63 | loss: 0.18965 | val_0_rmse: 0.42358 | val_1_rmse: 0.45872 |  0:01:12s
epoch 64 | loss: 0.19164 | val_0_rmse: 0.41793 | val_1_rmse: 0.45938 |  0:01:13s
epoch 65 | loss: 0.19    | val_0_rmse: 0.51354 | val_1_rmse: 0.53894 |  0:01:14s
epoch 66 | loss: 0.19093 | val_0_rmse: 0.42571 | val_1_rmse: 0.46218 |  0:01:16s
epoch 67 | loss: 0.19062 | val_0_rmse: 0.41927 | val_1_rmse: 0.45797 |  0:01:17s
epoch 68 | loss: 0.18923 | val_0_rmse: 0.42735 | val_1_rmse: 0.46926 |  0:01:18s
epoch 69 | loss: 0.18908 | val_0_rmse: 0.42559 | val_1_rmse: 0.46491 |  0:01:19s
epoch 70 | loss: 0.19758 | val_0_rmse: 0.43222 | val_1_rmse: 0.46759 |  0:01:20s
epoch 71 | loss: 0.19508 | val_0_rmse: 0.43592 | val_1_rmse: 0.47298 |  0:01:21s
epoch 72 | loss: 0.20185 | val_0_rmse: 0.46756 | val_1_rmse: 0.49191 |  0:01:22s
epoch 73 | loss: 0.21093 | val_0_rmse: 0.44047 | val_1_rmse: 0.47351 |  0:01:23s
epoch 74 | loss: 0.2026  | val_0_rmse: 0.43904 | val_1_rmse: 0.47585 |  0:01:25s
epoch 75 | loss: 0.21606 | val_0_rmse: 0.54152 | val_1_rmse: 0.56812 |  0:01:26s
epoch 76 | loss: 0.23245 | val_0_rmse: 0.56251 | val_1_rmse: 0.58949 |  0:01:27s
epoch 77 | loss: 0.21782 | val_0_rmse: 0.47111 | val_1_rmse: 0.49124 |  0:01:28s
epoch 78 | loss: 0.21481 | val_0_rmse: 0.44848 | val_1_rmse: 0.47603 |  0:01:29s
epoch 79 | loss: 0.20657 | val_0_rmse: 0.44546 | val_1_rmse: 0.47795 |  0:01:30s
epoch 80 | loss: 0.20154 | val_0_rmse: 0.43145 | val_1_rmse: 0.46108 |  0:01:31s
epoch 81 | loss: 0.19756 | val_0_rmse: 0.44052 | val_1_rmse: 0.47075 |  0:01:33s
epoch 82 | loss: 0.19985 | val_0_rmse: 0.42956 | val_1_rmse: 0.46112 |  0:01:34s
epoch 83 | loss: 0.19532 | val_0_rmse: 0.43156 | val_1_rmse: 0.46429 |  0:01:35s
epoch 84 | loss: 0.19727 | val_0_rmse: 0.4255  | val_1_rmse: 0.4585  |  0:01:36s
epoch 85 | loss: 0.19343 | val_0_rmse: 0.44044 | val_1_rmse: 0.47511 |  0:01:37s
epoch 86 | loss: 0.19876 | val_0_rmse: 0.44034 | val_1_rmse: 0.47136 |  0:01:38s
epoch 87 | loss: 0.20253 | val_0_rmse: 0.43692 | val_1_rmse: 0.46986 |  0:01:39s
epoch 88 | loss: 0.19713 | val_0_rmse: 0.42429 | val_1_rmse: 0.45939 |  0:01:40s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.45597
Best weights from best epoch are automatically used!
ended training at: 17:14:59
Feature importance:
Mean squared error is of 6187779938.8737335
Mean absolute error:56078.27016530728
MAPE:0.14786030516847312
R2 score:0.80034700401371
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:14:59
epoch 0  | loss: 1.3134  | val_0_rmse: 0.84364 | val_1_rmse: 0.85215 |  0:00:01s
epoch 1  | loss: 0.61225 | val_0_rmse: 0.84736 | val_1_rmse: 0.85949 |  0:00:02s
epoch 2  | loss: 0.5418  | val_0_rmse: 0.83565 | val_1_rmse: 0.84393 |  0:00:03s
epoch 3  | loss: 0.45782 | val_0_rmse: 0.98303 | val_1_rmse: 0.99126 |  0:00:04s
epoch 4  | loss: 0.35647 | val_0_rmse: 0.7843  | val_1_rmse: 0.79086 |  0:00:05s
epoch 5  | loss: 0.31942 | val_0_rmse: 0.65813 | val_1_rmse: 0.6705  |  0:00:06s
epoch 6  | loss: 0.29883 | val_0_rmse: 0.62741 | val_1_rmse: 0.64277 |  0:00:07s
epoch 7  | loss: 0.28093 | val_0_rmse: 0.62824 | val_1_rmse: 0.64493 |  0:00:09s
epoch 8  | loss: 0.27487 | val_0_rmse: 0.60638 | val_1_rmse: 0.62077 |  0:00:10s
epoch 9  | loss: 0.26513 | val_0_rmse: 0.58919 | val_1_rmse: 0.60399 |  0:00:11s
epoch 10 | loss: 0.25821 | val_0_rmse: 0.59193 | val_1_rmse: 0.60897 |  0:00:12s
epoch 11 | loss: 0.253   | val_0_rmse: 0.59785 | val_1_rmse: 0.61527 |  0:00:13s
epoch 12 | loss: 0.23945 | val_0_rmse: 0.54885 | val_1_rmse: 0.56691 |  0:00:14s
epoch 13 | loss: 0.23735 | val_0_rmse: 0.57971 | val_1_rmse: 0.59595 |  0:00:15s
epoch 14 | loss: 0.24361 | val_0_rmse: 0.53263 | val_1_rmse: 0.54989 |  0:00:16s
epoch 15 | loss: 0.25788 | val_0_rmse: 0.52602 | val_1_rmse: 0.54726 |  0:00:18s
epoch 16 | loss: 0.22604 | val_0_rmse: 0.54891 | val_1_rmse: 0.57051 |  0:00:19s
epoch 17 | loss: 0.22316 | val_0_rmse: 0.51693 | val_1_rmse: 0.53848 |  0:00:20s
epoch 18 | loss: 0.22211 | val_0_rmse: 0.51355 | val_1_rmse: 0.53444 |  0:00:21s
epoch 19 | loss: 0.21706 | val_0_rmse: 0.50342 | val_1_rmse: 0.52449 |  0:00:22s
epoch 20 | loss: 0.22499 | val_0_rmse: 0.52553 | val_1_rmse: 0.54664 |  0:00:23s
epoch 21 | loss: 0.21373 | val_0_rmse: 0.49081 | val_1_rmse: 0.51147 |  0:00:24s
epoch 22 | loss: 0.21104 | val_0_rmse: 0.4904  | val_1_rmse: 0.50953 |  0:00:26s
epoch 23 | loss: 0.21143 | val_0_rmse: 0.48875 | val_1_rmse: 0.51082 |  0:00:27s
epoch 24 | loss: 0.20772 | val_0_rmse: 0.47829 | val_1_rmse: 0.50135 |  0:00:28s
epoch 25 | loss: 0.21055 | val_0_rmse: 0.46954 | val_1_rmse: 0.48964 |  0:00:29s
epoch 26 | loss: 0.20894 | val_0_rmse: 0.46497 | val_1_rmse: 0.48671 |  0:00:30s
epoch 27 | loss: 0.2111  | val_0_rmse: 0.47059 | val_1_rmse: 0.49228 |  0:00:31s
epoch 28 | loss: 0.20976 | val_0_rmse: 0.46158 | val_1_rmse: 0.48214 |  0:00:32s
epoch 29 | loss: 0.21003 | val_0_rmse: 0.45982 | val_1_rmse: 0.48211 |  0:00:34s
epoch 30 | loss: 0.20743 | val_0_rmse: 0.4543  | val_1_rmse: 0.47767 |  0:00:35s
epoch 31 | loss: 0.20987 | val_0_rmse: 0.45773 | val_1_rmse: 0.47802 |  0:00:36s
epoch 32 | loss: 0.20802 | val_0_rmse: 0.44594 | val_1_rmse: 0.46935 |  0:00:37s
epoch 33 | loss: 0.20764 | val_0_rmse: 0.44958 | val_1_rmse: 0.47303 |  0:00:38s
epoch 34 | loss: 0.20717 | val_0_rmse: 0.44443 | val_1_rmse: 0.46445 |  0:00:39s
epoch 35 | loss: 0.20764 | val_0_rmse: 0.44508 | val_1_rmse: 0.46689 |  0:00:40s
epoch 36 | loss: 0.20449 | val_0_rmse: 0.44245 | val_1_rmse: 0.46689 |  0:00:42s
epoch 37 | loss: 0.20397 | val_0_rmse: 0.44653 | val_1_rmse: 0.47224 |  0:00:43s
epoch 38 | loss: 0.20537 | val_0_rmse: 0.44341 | val_1_rmse: 0.4645  |  0:00:44s
epoch 39 | loss: 0.19962 | val_0_rmse: 0.44037 | val_1_rmse: 0.46472 |  0:00:45s
epoch 40 | loss: 0.20616 | val_0_rmse: 0.4495  | val_1_rmse: 0.47417 |  0:00:46s
epoch 41 | loss: 0.20544 | val_0_rmse: 0.44453 | val_1_rmse: 0.47125 |  0:00:47s
epoch 42 | loss: 0.20505 | val_0_rmse: 0.44144 | val_1_rmse: 0.47038 |  0:00:48s
epoch 43 | loss: 0.20902 | val_0_rmse: 0.43752 | val_1_rmse: 0.46494 |  0:00:49s
epoch 44 | loss: 0.20606 | val_0_rmse: 0.43868 | val_1_rmse: 0.46563 |  0:00:50s
epoch 45 | loss: 0.2071  | val_0_rmse: 0.44374 | val_1_rmse: 0.47373 |  0:00:52s
epoch 46 | loss: 0.20768 | val_0_rmse: 0.44707 | val_1_rmse: 0.47498 |  0:00:53s
epoch 47 | loss: 0.21347 | val_0_rmse: 0.44541 | val_1_rmse: 0.47207 |  0:00:54s
epoch 48 | loss: 0.21524 | val_0_rmse: 0.49452 | val_1_rmse: 0.5145  |  0:00:55s
epoch 49 | loss: 0.21787 | val_0_rmse: 0.45251 | val_1_rmse: 0.47496 |  0:00:56s
epoch 50 | loss: 0.21349 | val_0_rmse: 0.45696 | val_1_rmse: 0.48121 |  0:00:57s
epoch 51 | loss: 0.22654 | val_0_rmse: 0.45849 | val_1_rmse: 0.48275 |  0:00:58s
epoch 52 | loss: 0.21417 | val_0_rmse: 0.45438 | val_1_rmse: 0.4782  |  0:01:00s
epoch 53 | loss: 0.20557 | val_0_rmse: 0.43741 | val_1_rmse: 0.46667 |  0:01:01s
epoch 54 | loss: 0.20249 | val_0_rmse: 0.436   | val_1_rmse: 0.46307 |  0:01:02s
epoch 55 | loss: 0.2021  | val_0_rmse: 0.43724 | val_1_rmse: 0.46575 |  0:01:03s
epoch 56 | loss: 0.19565 | val_0_rmse: 0.43086 | val_1_rmse: 0.46213 |  0:01:04s
epoch 57 | loss: 0.19605 | val_0_rmse: 0.43294 | val_1_rmse: 0.46503 |  0:01:05s
epoch 58 | loss: 0.20512 | val_0_rmse: 0.43391 | val_1_rmse: 0.46384 |  0:01:06s
epoch 59 | loss: 0.19841 | val_0_rmse: 0.42898 | val_1_rmse: 0.46097 |  0:01:08s
epoch 60 | loss: 0.19594 | val_0_rmse: 0.42931 | val_1_rmse: 0.46106 |  0:01:09s
epoch 61 | loss: 0.19767 | val_0_rmse: 0.42774 | val_1_rmse: 0.46149 |  0:01:10s
epoch 62 | loss: 0.19453 | val_0_rmse: 0.42527 | val_1_rmse: 0.459   |  0:01:11s
epoch 63 | loss: 0.1908  | val_0_rmse: 0.42845 | val_1_rmse: 0.46229 |  0:01:12s
epoch 64 | loss: 0.18959 | val_0_rmse: 0.42582 | val_1_rmse: 0.46184 |  0:01:13s
epoch 65 | loss: 0.19668 | val_0_rmse: 0.43594 | val_1_rmse: 0.46707 |  0:01:14s
epoch 66 | loss: 0.19084 | val_0_rmse: 0.42418 | val_1_rmse: 0.45913 |  0:01:15s
epoch 67 | loss: 0.18993 | val_0_rmse: 0.42663 | val_1_rmse: 0.45995 |  0:01:16s
epoch 68 | loss: 0.18991 | val_0_rmse: 0.42807 | val_1_rmse: 0.46446 |  0:01:18s
epoch 69 | loss: 0.19061 | val_0_rmse: 0.42308 | val_1_rmse: 0.45895 |  0:01:19s
epoch 70 | loss: 0.1868  | val_0_rmse: 0.42492 | val_1_rmse: 0.46104 |  0:01:20s
epoch 71 | loss: 0.18891 | val_0_rmse: 0.42269 | val_1_rmse: 0.45835 |  0:01:21s
epoch 72 | loss: 0.18517 | val_0_rmse: 0.41758 | val_1_rmse: 0.45453 |  0:01:22s
epoch 73 | loss: 0.18679 | val_0_rmse: 0.42194 | val_1_rmse: 0.46299 |  0:01:23s
epoch 74 | loss: 0.18905 | val_0_rmse: 0.42173 | val_1_rmse: 0.46038 |  0:01:24s
epoch 75 | loss: 0.18457 | val_0_rmse: 0.41759 | val_1_rmse: 0.45586 |  0:01:26s
epoch 76 | loss: 0.18582 | val_0_rmse: 0.41992 | val_1_rmse: 0.45988 |  0:01:27s
epoch 77 | loss: 0.18828 | val_0_rmse: 0.41675 | val_1_rmse: 0.45896 |  0:01:28s
epoch 78 | loss: 0.18618 | val_0_rmse: 0.41578 | val_1_rmse: 0.45704 |  0:01:29s
epoch 79 | loss: 0.18288 | val_0_rmse: 0.4215  | val_1_rmse: 0.4611  |  0:01:30s
epoch 80 | loss: 0.18618 | val_0_rmse: 0.42013 | val_1_rmse: 0.46223 |  0:01:31s
epoch 81 | loss: 0.1853  | val_0_rmse: 0.42392 | val_1_rmse: 0.46461 |  0:01:32s
epoch 82 | loss: 0.18533 | val_0_rmse: 0.42378 | val_1_rmse: 0.46706 |  0:01:34s
epoch 83 | loss: 0.185   | val_0_rmse: 0.42513 | val_1_rmse: 0.46571 |  0:01:35s
epoch 84 | loss: 0.18652 | val_0_rmse: 0.41432 | val_1_rmse: 0.45525 |  0:01:36s
epoch 85 | loss: 0.18275 | val_0_rmse: 0.41645 | val_1_rmse: 0.45891 |  0:01:37s
epoch 86 | loss: 0.18115 | val_0_rmse: 0.41438 | val_1_rmse: 0.45761 |  0:01:38s
epoch 87 | loss: 0.17934 | val_0_rmse: 0.41115 | val_1_rmse: 0.45707 |  0:01:39s
epoch 88 | loss: 0.17931 | val_0_rmse: 0.41245 | val_1_rmse: 0.45501 |  0:01:40s
epoch 89 | loss: 0.18036 | val_0_rmse: 0.42683 | val_1_rmse: 0.46708 |  0:01:42s
epoch 90 | loss: 0.1816  | val_0_rmse: 0.41358 | val_1_rmse: 0.45877 |  0:01:43s
epoch 91 | loss: 0.18384 | val_0_rmse: 0.41188 | val_1_rmse: 0.45867 |  0:01:44s
epoch 92 | loss: 0.17671 | val_0_rmse: 0.40971 | val_1_rmse: 0.45902 |  0:01:45s
epoch 93 | loss: 0.17989 | val_0_rmse: 0.41975 | val_1_rmse: 0.46428 |  0:01:46s
epoch 94 | loss: 0.17801 | val_0_rmse: 0.42088 | val_1_rmse: 0.47192 |  0:01:47s
epoch 95 | loss: 0.18294 | val_0_rmse: 0.40781 | val_1_rmse: 0.45721 |  0:01:48s
epoch 96 | loss: 0.18405 | val_0_rmse: 0.41956 | val_1_rmse: 0.46431 |  0:01:49s
epoch 97 | loss: 0.17816 | val_0_rmse: 0.40513 | val_1_rmse: 0.45489 |  0:01:50s
epoch 98 | loss: 0.17346 | val_0_rmse: 0.40715 | val_1_rmse: 0.45697 |  0:01:52s
epoch 99 | loss: 0.17871 | val_0_rmse: 0.40663 | val_1_rmse: 0.46019 |  0:01:53s
epoch 100| loss: 0.17589 | val_0_rmse: 0.41141 | val_1_rmse: 0.46628 |  0:01:54s
epoch 101| loss: 0.17586 | val_0_rmse: 0.40483 | val_1_rmse: 0.46206 |  0:01:55s
epoch 102| loss: 0.17825 | val_0_rmse: 0.41014 | val_1_rmse: 0.45991 |  0:01:56s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.45453
Best weights from best epoch are automatically used!
ended training at: 17:16:56
Feature importance:
Mean squared error is of 5972768030.130217
Mean absolute error:54053.340333254295
MAPE:0.13898154641359015
R2 score:0.8021594337306095
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:16:57
epoch 0  | loss: 1.24583 | val_0_rmse: 0.88452 | val_1_rmse: 0.87196 |  0:00:01s
epoch 1  | loss: 0.5259  | val_0_rmse: 0.76213 | val_1_rmse: 0.76788 |  0:00:02s
epoch 2  | loss: 0.37554 | val_0_rmse: 0.69397 | val_1_rmse: 0.69779 |  0:00:03s
epoch 3  | loss: 0.32666 | val_0_rmse: 0.52966 | val_1_rmse: 0.5249  |  0:00:04s
epoch 4  | loss: 0.31643 | val_0_rmse: 0.56661 | val_1_rmse: 0.5618  |  0:00:05s
epoch 5  | loss: 0.30001 | val_0_rmse: 0.57187 | val_1_rmse: 0.56868 |  0:00:06s
epoch 6  | loss: 0.26804 | val_0_rmse: 0.61987 | val_1_rmse: 0.62517 |  0:00:07s
epoch 7  | loss: 0.2655  | val_0_rmse: 0.51789 | val_1_rmse: 0.51513 |  0:00:09s
epoch 8  | loss: 0.24444 | val_0_rmse: 0.50648 | val_1_rmse: 0.49588 |  0:00:10s
epoch 9  | loss: 0.24355 | val_0_rmse: 0.51709 | val_1_rmse: 0.51043 |  0:00:11s
epoch 10 | loss: 0.2404  | val_0_rmse: 0.51436 | val_1_rmse: 0.51118 |  0:00:12s
epoch 11 | loss: 0.24373 | val_0_rmse: 0.52822 | val_1_rmse: 0.52218 |  0:00:13s
epoch 12 | loss: 0.24262 | val_0_rmse: 0.52966 | val_1_rmse: 0.52267 |  0:00:14s
epoch 13 | loss: 0.25166 | val_0_rmse: 0.52184 | val_1_rmse: 0.51663 |  0:00:15s
epoch 14 | loss: 0.2342  | val_0_rmse: 0.51187 | val_1_rmse: 0.51    |  0:00:16s
epoch 15 | loss: 0.2428  | val_0_rmse: 0.5306  | val_1_rmse: 0.52509 |  0:00:18s
epoch 16 | loss: 0.23866 | val_0_rmse: 0.51014 | val_1_rmse: 0.50878 |  0:00:19s
epoch 17 | loss: 0.23305 | val_0_rmse: 0.49284 | val_1_rmse: 0.49165 |  0:00:20s
epoch 18 | loss: 0.22673 | val_0_rmse: 0.49638 | val_1_rmse: 0.49337 |  0:00:21s
epoch 19 | loss: 0.22503 | val_0_rmse: 0.48573 | val_1_rmse: 0.4821  |  0:00:22s
epoch 20 | loss: 0.22861 | val_0_rmse: 0.48577 | val_1_rmse: 0.4833  |  0:00:23s
epoch 21 | loss: 0.23969 | val_0_rmse: 0.52212 | val_1_rmse: 0.52282 |  0:00:24s
epoch 22 | loss: 0.2411  | val_0_rmse: 0.49054 | val_1_rmse: 0.48826 |  0:00:26s
epoch 23 | loss: 0.23653 | val_0_rmse: 0.47937 | val_1_rmse: 0.47335 |  0:00:27s
epoch 24 | loss: 0.23413 | val_0_rmse: 0.49232 | val_1_rmse: 0.48445 |  0:00:28s
epoch 25 | loss: 0.23478 | val_0_rmse: 0.49688 | val_1_rmse: 0.49284 |  0:00:29s
epoch 26 | loss: 0.26265 | val_0_rmse: 0.502   | val_1_rmse: 0.49891 |  0:00:30s
epoch 27 | loss: 0.24431 | val_0_rmse: 0.49113 | val_1_rmse: 0.48682 |  0:00:31s
epoch 28 | loss: 0.2421  | val_0_rmse: 0.53293 | val_1_rmse: 0.53309 |  0:00:32s
epoch 29 | loss: 0.24077 | val_0_rmse: 0.47631 | val_1_rmse: 0.47298 |  0:00:34s
epoch 30 | loss: 0.232   | val_0_rmse: 0.47473 | val_1_rmse: 0.47328 |  0:00:35s
epoch 31 | loss: 0.22864 | val_0_rmse: 0.48212 | val_1_rmse: 0.47829 |  0:00:36s
epoch 32 | loss: 0.22618 | val_0_rmse: 0.47766 | val_1_rmse: 0.47492 |  0:00:37s
epoch 33 | loss: 0.22846 | val_0_rmse: 0.46284 | val_1_rmse: 0.4613  |  0:00:38s
epoch 34 | loss: 0.22191 | val_0_rmse: 0.46776 | val_1_rmse: 0.46373 |  0:00:39s
epoch 35 | loss: 0.23433 | val_0_rmse: 0.49024 | val_1_rmse: 0.49232 |  0:00:40s
epoch 36 | loss: 0.22574 | val_0_rmse: 0.46277 | val_1_rmse: 0.46004 |  0:00:41s
epoch 37 | loss: 0.22596 | val_0_rmse: 0.46405 | val_1_rmse: 0.46112 |  0:00:43s
epoch 38 | loss: 0.22886 | val_0_rmse: 0.4621  | val_1_rmse: 0.46159 |  0:00:44s
epoch 39 | loss: 0.22943 | val_0_rmse: 0.46829 | val_1_rmse: 0.46443 |  0:00:45s
epoch 40 | loss: 0.22685 | val_0_rmse: 0.46863 | val_1_rmse: 0.46504 |  0:00:46s
epoch 41 | loss: 0.22446 | val_0_rmse: 0.46412 | val_1_rmse: 0.46085 |  0:00:47s
epoch 42 | loss: 0.22958 | val_0_rmse: 0.46531 | val_1_rmse: 0.46282 |  0:00:48s
epoch 43 | loss: 0.223   | val_0_rmse: 0.45948 | val_1_rmse: 0.45572 |  0:00:49s
epoch 44 | loss: 0.22278 | val_0_rmse: 0.45749 | val_1_rmse: 0.4583  |  0:00:50s
epoch 45 | loss: 0.21903 | val_0_rmse: 0.46025 | val_1_rmse: 0.45997 |  0:00:52s
epoch 46 | loss: 0.22321 | val_0_rmse: 0.45799 | val_1_rmse: 0.45944 |  0:00:53s
epoch 47 | loss: 0.22126 | val_0_rmse: 0.4593  | val_1_rmse: 0.46056 |  0:00:54s
epoch 48 | loss: 0.2232  | val_0_rmse: 0.45098 | val_1_rmse: 0.45183 |  0:00:55s
epoch 49 | loss: 0.21614 | val_0_rmse: 0.44655 | val_1_rmse: 0.44593 |  0:00:56s
epoch 50 | loss: 0.21338 | val_0_rmse: 0.44835 | val_1_rmse: 0.4471  |  0:00:57s
epoch 51 | loss: 0.21366 | val_0_rmse: 0.45226 | val_1_rmse: 0.45343 |  0:00:58s
epoch 52 | loss: 0.21967 | val_0_rmse: 0.45688 | val_1_rmse: 0.45574 |  0:01:00s
epoch 53 | loss: 0.21438 | val_0_rmse: 0.45182 | val_1_rmse: 0.45249 |  0:01:01s
epoch 54 | loss: 0.21565 | val_0_rmse: 0.45383 | val_1_rmse: 0.45343 |  0:01:02s
epoch 55 | loss: 0.21487 | val_0_rmse: 0.45086 | val_1_rmse: 0.44772 |  0:01:03s
epoch 56 | loss: 0.21302 | val_0_rmse: 0.44563 | val_1_rmse: 0.4477  |  0:01:04s
epoch 57 | loss: 0.21241 | val_0_rmse: 0.44936 | val_1_rmse: 0.44821 |  0:01:05s
epoch 58 | loss: 0.20908 | val_0_rmse: 0.44651 | val_1_rmse: 0.44413 |  0:01:06s
epoch 59 | loss: 0.20646 | val_0_rmse: 0.44136 | val_1_rmse: 0.44392 |  0:01:08s
epoch 60 | loss: 0.20814 | val_0_rmse: 0.44336 | val_1_rmse: 0.44503 |  0:01:09s
epoch 61 | loss: 0.20587 | val_0_rmse: 0.45148 | val_1_rmse: 0.45257 |  0:01:10s
epoch 62 | loss: 0.20886 | val_0_rmse: 0.44555 | val_1_rmse: 0.44779 |  0:01:11s
epoch 63 | loss: 0.20376 | val_0_rmse: 0.44159 | val_1_rmse: 0.44333 |  0:01:12s
epoch 64 | loss: 0.20267 | val_0_rmse: 0.4517  | val_1_rmse: 0.45584 |  0:01:13s
epoch 65 | loss: 0.20519 | val_0_rmse: 0.48913 | val_1_rmse: 0.48097 |  0:01:14s
epoch 66 | loss: 0.20564 | val_0_rmse: 0.44274 | val_1_rmse: 0.44352 |  0:01:15s
epoch 67 | loss: 0.20448 | val_0_rmse: 0.45541 | val_1_rmse: 0.45442 |  0:01:16s
epoch 68 | loss: 0.20623 | val_0_rmse: 0.44135 | val_1_rmse: 0.44119 |  0:01:18s
epoch 69 | loss: 0.20304 | val_0_rmse: 0.44667 | val_1_rmse: 0.4463  |  0:01:19s
epoch 70 | loss: 0.20253 | val_0_rmse: 0.44303 | val_1_rmse: 0.44435 |  0:01:20s
epoch 71 | loss: 0.2027  | val_0_rmse: 0.44018 | val_1_rmse: 0.44185 |  0:01:21s
epoch 72 | loss: 0.2093  | val_0_rmse: 0.4457  | val_1_rmse: 0.44783 |  0:01:22s
epoch 73 | loss: 0.20846 | val_0_rmse: 0.46412 | val_1_rmse: 0.4653  |  0:01:23s
epoch 74 | loss: 0.20708 | val_0_rmse: 0.46895 | val_1_rmse: 0.47022 |  0:01:24s
epoch 75 | loss: 0.20797 | val_0_rmse: 0.4488  | val_1_rmse: 0.45159 |  0:01:26s
epoch 76 | loss: 0.2067  | val_0_rmse: 0.45733 | val_1_rmse: 0.46298 |  0:01:27s
epoch 77 | loss: 0.20231 | val_0_rmse: 0.43761 | val_1_rmse: 0.44085 |  0:01:28s
epoch 78 | loss: 0.20195 | val_0_rmse: 0.44648 | val_1_rmse: 0.44573 |  0:01:29s
epoch 79 | loss: 0.20096 | val_0_rmse: 0.44206 | val_1_rmse: 0.44553 |  0:01:30s
epoch 80 | loss: 0.20414 | val_0_rmse: 0.44012 | val_1_rmse: 0.44113 |  0:01:31s
epoch 81 | loss: 0.20058 | val_0_rmse: 0.43653 | val_1_rmse: 0.43822 |  0:01:32s
epoch 82 | loss: 0.20037 | val_0_rmse: 0.43225 | val_1_rmse: 0.4348  |  0:01:34s
epoch 83 | loss: 0.19811 | val_0_rmse: 0.4321  | val_1_rmse: 0.43385 |  0:01:35s
epoch 84 | loss: 0.19694 | val_0_rmse: 0.43689 | val_1_rmse: 0.44081 |  0:01:36s
epoch 85 | loss: 0.19815 | val_0_rmse: 0.43223 | val_1_rmse: 0.4353  |  0:01:37s
epoch 86 | loss: 0.19528 | val_0_rmse: 0.44079 | val_1_rmse: 0.44223 |  0:01:38s
epoch 87 | loss: 0.1982  | val_0_rmse: 0.43168 | val_1_rmse: 0.43497 |  0:01:39s
epoch 88 | loss: 0.20033 | val_0_rmse: 0.43072 | val_1_rmse: 0.43397 |  0:01:40s
epoch 89 | loss: 0.19723 | val_0_rmse: 0.43407 | val_1_rmse: 0.43722 |  0:01:41s
epoch 90 | loss: 0.19461 | val_0_rmse: 0.43055 | val_1_rmse: 0.43504 |  0:01:43s
epoch 91 | loss: 0.19805 | val_0_rmse: 0.43166 | val_1_rmse: 0.43411 |  0:01:44s
epoch 92 | loss: 0.19364 | val_0_rmse: 0.43282 | val_1_rmse: 0.43789 |  0:01:45s
epoch 93 | loss: 0.19471 | val_0_rmse: 0.433   | val_1_rmse: 0.43788 |  0:01:46s
epoch 94 | loss: 0.19508 | val_0_rmse: 0.44316 | val_1_rmse: 0.44471 |  0:01:47s
epoch 95 | loss: 0.19829 | val_0_rmse: 0.43006 | val_1_rmse: 0.43683 |  0:01:48s
epoch 96 | loss: 0.19767 | val_0_rmse: 0.43119 | val_1_rmse: 0.43668 |  0:01:49s
epoch 97 | loss: 0.19504 | val_0_rmse: 0.43166 | val_1_rmse: 0.4376  |  0:01:51s
epoch 98 | loss: 0.19418 | val_0_rmse: 0.43214 | val_1_rmse: 0.43869 |  0:01:52s
epoch 99 | loss: 0.19516 | val_0_rmse: 0.43538 | val_1_rmse: 0.44291 |  0:01:53s
epoch 100| loss: 0.19978 | val_0_rmse: 0.43064 | val_1_rmse: 0.43609 |  0:01:54s
epoch 101| loss: 0.19898 | val_0_rmse: 0.42935 | val_1_rmse: 0.43656 |  0:01:55s
epoch 102| loss: 0.19509 | val_0_rmse: 0.44116 | val_1_rmse: 0.44912 |  0:01:56s
epoch 103| loss: 0.19651 | val_0_rmse: 0.43192 | val_1_rmse: 0.43638 |  0:01:57s
epoch 104| loss: 0.19216 | val_0_rmse: 0.43161 | val_1_rmse: 0.439   |  0:01:58s
epoch 105| loss: 0.20193 | val_0_rmse: 0.43585 | val_1_rmse: 0.43963 |  0:02:00s
epoch 106| loss: 0.19597 | val_0_rmse: 0.42793 | val_1_rmse: 0.43587 |  0:02:01s
epoch 107| loss: 0.20066 | val_0_rmse: 0.4387  | val_1_rmse: 0.4457  |  0:02:02s
epoch 108| loss: 0.20932 | val_0_rmse: 0.45066 | val_1_rmse: 0.45781 |  0:02:03s
epoch 109| loss: 0.211   | val_0_rmse: 0.44386 | val_1_rmse: 0.45189 |  0:02:04s
epoch 110| loss: 0.20685 | val_0_rmse: 0.45027 | val_1_rmse: 0.45441 |  0:02:05s
epoch 111| loss: 0.20091 | val_0_rmse: 0.45092 | val_1_rmse: 0.45575 |  0:02:06s
epoch 112| loss: 0.20785 | val_0_rmse: 0.45235 | val_1_rmse: 0.45475 |  0:02:07s
epoch 113| loss: 0.21739 | val_0_rmse: 0.45834 | val_1_rmse: 0.46039 |  0:02:09s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.43385
Best weights from best epoch are automatically used!
ended training at: 17:19:06
Feature importance:
Mean squared error is of 6413265187.885148
Mean absolute error:56104.48909079413
MAPE:0.14471536099249316
R2 score:0.7939156341423578
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:19:08
epoch 0  | loss: 2.0667  | val_0_rmse: 0.99354 | val_1_rmse: 1.03664 |  0:00:00s
epoch 1  | loss: 1.04012 | val_0_rmse: 0.98711 | val_1_rmse: 1.0307  |  0:00:01s
epoch 2  | loss: 0.91931 | val_0_rmse: 0.94154 | val_1_rmse: 0.97764 |  0:00:01s
epoch 3  | loss: 0.85434 | val_0_rmse: 0.89767 | val_1_rmse: 0.91987 |  0:00:02s
epoch 4  | loss: 0.76446 | val_0_rmse: 0.88342 | val_1_rmse: 0.893   |  0:00:03s
epoch 5  | loss: 0.71916 | val_0_rmse: 0.8619  | val_1_rmse: 0.86617 |  0:00:03s
epoch 6  | loss: 0.68709 | val_0_rmse: 0.87176 | val_1_rmse: 0.87204 |  0:00:04s
epoch 7  | loss: 0.64929 | val_0_rmse: 0.87642 | val_1_rmse: 0.87502 |  0:00:04s
epoch 8  | loss: 0.57411 | val_0_rmse: 0.88412 | val_1_rmse: 0.88796 |  0:00:05s
epoch 9  | loss: 0.53524 | val_0_rmse: 0.89128 | val_1_rmse: 0.8937  |  0:00:06s
epoch 10 | loss: 0.52503 | val_0_rmse: 0.8788  | val_1_rmse: 0.88079 |  0:00:06s
epoch 11 | loss: 0.50144 | val_0_rmse: 0.82859 | val_1_rmse: 0.8342  |  0:00:07s
epoch 12 | loss: 0.46749 | val_0_rmse: 0.79378 | val_1_rmse: 0.80852 |  0:00:08s
epoch 13 | loss: 0.44234 | val_0_rmse: 0.78448 | val_1_rmse: 0.8013  |  0:00:08s
epoch 14 | loss: 0.42255 | val_0_rmse: 0.78277 | val_1_rmse: 0.79417 |  0:00:09s
epoch 15 | loss: 0.41098 | val_0_rmse: 0.78469 | val_1_rmse: 0.80342 |  0:00:09s
epoch 16 | loss: 0.40265 | val_0_rmse: 0.76531 | val_1_rmse: 0.77235 |  0:00:10s
epoch 17 | loss: 0.38767 | val_0_rmse: 0.72291 | val_1_rmse: 0.73792 |  0:00:11s
epoch 18 | loss: 0.3782  | val_0_rmse: 0.71314 | val_1_rmse: 0.71815 |  0:00:11s
epoch 19 | loss: 0.36059 | val_0_rmse: 0.76036 | val_1_rmse: 0.75735 |  0:00:12s
epoch 20 | loss: 0.34095 | val_0_rmse: 0.77215 | val_1_rmse: 0.76854 |  0:00:13s
epoch 21 | loss: 0.33245 | val_0_rmse: 0.68403 | val_1_rmse: 0.68905 |  0:00:13s
epoch 22 | loss: 0.3251  | val_0_rmse: 0.65048 | val_1_rmse: 0.66387 |  0:00:14s
epoch 23 | loss: 0.32935 | val_0_rmse: 0.66574 | val_1_rmse: 0.67475 |  0:00:14s
epoch 24 | loss: 0.31759 | val_0_rmse: 0.6766  | val_1_rmse: 0.6865  |  0:00:15s
epoch 25 | loss: 0.30544 | val_0_rmse: 0.69476 | val_1_rmse: 0.7004  |  0:00:16s
epoch 26 | loss: 0.29227 | val_0_rmse: 0.6509  | val_1_rmse: 0.66265 |  0:00:16s
epoch 27 | loss: 0.2825  | val_0_rmse: 0.63584 | val_1_rmse: 0.64896 |  0:00:17s
epoch 28 | loss: 0.29005 | val_0_rmse: 0.61521 | val_1_rmse: 0.63035 |  0:00:18s
epoch 29 | loss: 0.27971 | val_0_rmse: 0.61571 | val_1_rmse: 0.62833 |  0:00:18s
epoch 30 | loss: 0.27764 | val_0_rmse: 0.62693 | val_1_rmse: 0.63973 |  0:00:19s
epoch 31 | loss: 0.27998 | val_0_rmse: 0.64424 | val_1_rmse: 0.65454 |  0:00:19s
epoch 32 | loss: 0.29068 | val_0_rmse: 0.67764 | val_1_rmse: 0.68203 |  0:00:20s
epoch 33 | loss: 0.29024 | val_0_rmse: 0.65398 | val_1_rmse: 0.66176 |  0:00:21s
epoch 34 | loss: 0.28071 | val_0_rmse: 0.62719 | val_1_rmse: 0.64072 |  0:00:21s
epoch 35 | loss: 0.26875 | val_0_rmse: 0.61826 | val_1_rmse: 0.63618 |  0:00:22s
epoch 36 | loss: 0.25649 | val_0_rmse: 0.60606 | val_1_rmse: 0.61541 |  0:00:22s
epoch 37 | loss: 0.24733 | val_0_rmse: 0.60549 | val_1_rmse: 0.61382 |  0:00:23s
epoch 38 | loss: 0.25497 | val_0_rmse: 0.61353 | val_1_rmse: 0.62282 |  0:00:24s
epoch 39 | loss: 0.25088 | val_0_rmse: 0.61355 | val_1_rmse: 0.62056 |  0:00:24s
epoch 40 | loss: 0.23894 | val_0_rmse: 0.59986 | val_1_rmse: 0.61243 |  0:00:25s
epoch 41 | loss: 0.2336  | val_0_rmse: 0.58888 | val_1_rmse: 0.60692 |  0:00:26s
epoch 42 | loss: 0.23704 | val_0_rmse: 0.57047 | val_1_rmse: 0.59129 |  0:00:26s
epoch 43 | loss: 0.23307 | val_0_rmse: 0.58275 | val_1_rmse: 0.60426 |  0:00:27s
epoch 44 | loss: 0.23003 | val_0_rmse: 0.57702 | val_1_rmse: 0.5991  |  0:00:27s
epoch 45 | loss: 0.23224 | val_0_rmse: 0.57877 | val_1_rmse: 0.60154 |  0:00:28s
epoch 46 | loss: 0.22233 | val_0_rmse: 0.567   | val_1_rmse: 0.58796 |  0:00:29s
epoch 47 | loss: 0.22116 | val_0_rmse: 0.56426 | val_1_rmse: 0.58327 |  0:00:29s
epoch 48 | loss: 0.22107 | val_0_rmse: 0.56759 | val_1_rmse: 0.59137 |  0:00:30s
epoch 49 | loss: 0.21984 | val_0_rmse: 0.54344 | val_1_rmse: 0.56948 |  0:00:31s
epoch 50 | loss: 0.21366 | val_0_rmse: 0.54952 | val_1_rmse: 0.5746  |  0:00:31s
epoch 51 | loss: 0.20879 | val_0_rmse: 0.54645 | val_1_rmse: 0.57332 |  0:00:32s
epoch 52 | loss: 0.21012 | val_0_rmse: 0.53489 | val_1_rmse: 0.56971 |  0:00:32s
epoch 53 | loss: 0.20854 | val_0_rmse: 0.52665 | val_1_rmse: 0.55766 |  0:00:33s
epoch 54 | loss: 0.20637 | val_0_rmse: 0.523   | val_1_rmse: 0.55249 |  0:00:34s
epoch 55 | loss: 0.20638 | val_0_rmse: 0.54395 | val_1_rmse: 0.57665 |  0:00:34s
epoch 56 | loss: 0.20806 | val_0_rmse: 0.52588 | val_1_rmse: 0.56058 |  0:00:35s
epoch 57 | loss: 0.20424 | val_0_rmse: 0.52069 | val_1_rmse: 0.55495 |  0:00:35s
epoch 58 | loss: 0.20117 | val_0_rmse: 0.51677 | val_1_rmse: 0.54964 |  0:00:36s
epoch 59 | loss: 0.19774 | val_0_rmse: 0.49788 | val_1_rmse: 0.52844 |  0:00:37s
epoch 60 | loss: 0.19256 | val_0_rmse: 0.51716 | val_1_rmse: 0.55142 |  0:00:37s
epoch 61 | loss: 0.1918  | val_0_rmse: 0.49602 | val_1_rmse: 0.53018 |  0:00:38s
epoch 62 | loss: 0.19543 | val_0_rmse: 0.48767 | val_1_rmse: 0.52568 |  0:00:39s
epoch 63 | loss: 0.19583 | val_0_rmse: 0.49054 | val_1_rmse: 0.52597 |  0:00:39s
epoch 64 | loss: 0.18769 | val_0_rmse: 0.48041 | val_1_rmse: 0.51949 |  0:00:40s
epoch 65 | loss: 0.19255 | val_0_rmse: 0.47053 | val_1_rmse: 0.51256 |  0:00:40s
epoch 66 | loss: 0.18846 | val_0_rmse: 0.48664 | val_1_rmse: 0.53218 |  0:00:41s
epoch 67 | loss: 0.19159 | val_0_rmse: 0.47058 | val_1_rmse: 0.51587 |  0:00:42s
epoch 68 | loss: 0.18726 | val_0_rmse: 0.46957 | val_1_rmse: 0.51433 |  0:00:42s
epoch 69 | loss: 0.18585 | val_0_rmse: 0.47284 | val_1_rmse: 0.51311 |  0:00:43s
epoch 70 | loss: 0.18712 | val_0_rmse: 0.46688 | val_1_rmse: 0.51069 |  0:00:44s
epoch 71 | loss: 0.18964 | val_0_rmse: 0.46387 | val_1_rmse: 0.5116  |  0:00:44s
epoch 72 | loss: 0.18984 | val_0_rmse: 0.45787 | val_1_rmse: 0.50241 |  0:00:45s
epoch 73 | loss: 0.19772 | val_0_rmse: 0.47617 | val_1_rmse: 0.52118 |  0:00:45s
epoch 74 | loss: 0.19061 | val_0_rmse: 0.45169 | val_1_rmse: 0.50565 |  0:00:46s
epoch 75 | loss: 0.18435 | val_0_rmse: 0.45166 | val_1_rmse: 0.50508 |  0:00:47s
epoch 76 | loss: 0.18118 | val_0_rmse: 0.43765 | val_1_rmse: 0.49774 |  0:00:47s
epoch 77 | loss: 0.17945 | val_0_rmse: 0.43378 | val_1_rmse: 0.49404 |  0:00:48s
epoch 78 | loss: 0.17722 | val_0_rmse: 0.43615 | val_1_rmse: 0.49752 |  0:00:49s
epoch 79 | loss: 0.17751 | val_0_rmse: 0.42411 | val_1_rmse: 0.48985 |  0:00:49s
epoch 80 | loss: 0.17479 | val_0_rmse: 0.42005 | val_1_rmse: 0.48576 |  0:00:50s
epoch 81 | loss: 0.17532 | val_0_rmse: 0.42118 | val_1_rmse: 0.48648 |  0:00:50s
epoch 82 | loss: 0.17677 | val_0_rmse: 0.42103 | val_1_rmse: 0.48791 |  0:00:51s
epoch 83 | loss: 0.17567 | val_0_rmse: 0.41089 | val_1_rmse: 0.48221 |  0:00:52s
epoch 84 | loss: 0.1784  | val_0_rmse: 0.42572 | val_1_rmse: 0.49581 |  0:00:52s
epoch 85 | loss: 0.1787  | val_0_rmse: 0.41455 | val_1_rmse: 0.4926  |  0:00:53s
epoch 86 | loss: 0.17348 | val_0_rmse: 0.42862 | val_1_rmse: 0.49965 |  0:00:53s
epoch 87 | loss: 0.17235 | val_0_rmse: 0.39969 | val_1_rmse: 0.4805  |  0:00:54s
epoch 88 | loss: 0.17361 | val_0_rmse: 0.40821 | val_1_rmse: 0.48989 |  0:00:55s
epoch 89 | loss: 0.17185 | val_0_rmse: 0.40509 | val_1_rmse: 0.49173 |  0:00:55s
epoch 90 | loss: 0.17019 | val_0_rmse: 0.39699 | val_1_rmse: 0.48186 |  0:00:56s
epoch 91 | loss: 0.17055 | val_0_rmse: 0.39842 | val_1_rmse: 0.48177 |  0:00:57s
epoch 92 | loss: 0.17172 | val_0_rmse: 0.42881 | val_1_rmse: 0.50684 |  0:00:57s
epoch 93 | loss: 0.17399 | val_0_rmse: 0.40149 | val_1_rmse: 0.48511 |  0:00:58s
epoch 94 | loss: 0.17074 | val_0_rmse: 0.38666 | val_1_rmse: 0.47843 |  0:00:59s
epoch 95 | loss: 0.16816 | val_0_rmse: 0.39248 | val_1_rmse: 0.4883  |  0:00:59s
epoch 96 | loss: 0.16724 | val_0_rmse: 0.3962  | val_1_rmse: 0.49551 |  0:01:00s
epoch 97 | loss: 0.16592 | val_0_rmse: 0.4017  | val_1_rmse: 0.50611 |  0:01:00s
epoch 98 | loss: 0.16999 | val_0_rmse: 0.38234 | val_1_rmse: 0.48389 |  0:01:01s
epoch 99 | loss: 0.16865 | val_0_rmse: 0.41196 | val_1_rmse: 0.50298 |  0:01:02s
epoch 100| loss: 0.17144 | val_0_rmse: 0.38492 | val_1_rmse: 0.48604 |  0:01:02s
epoch 101| loss: 0.16545 | val_0_rmse: 0.38305 | val_1_rmse: 0.48331 |  0:01:03s
epoch 102| loss: 0.16596 | val_0_rmse: 0.38284 | val_1_rmse: 0.49034 |  0:01:03s
epoch 103| loss: 0.16439 | val_0_rmse: 0.38864 | val_1_rmse: 0.48792 |  0:01:04s
epoch 104| loss: 0.15856 | val_0_rmse: 0.38361 | val_1_rmse: 0.49234 |  0:01:05s
epoch 105| loss: 0.16403 | val_0_rmse: 0.38585 | val_1_rmse: 0.49507 |  0:01:05s
epoch 106| loss: 0.16174 | val_0_rmse: 0.37789 | val_1_rmse: 0.48751 |  0:01:06s
epoch 107| loss: 0.16492 | val_0_rmse: 0.40365 | val_1_rmse: 0.50188 |  0:01:06s
epoch 108| loss: 0.16416 | val_0_rmse: 0.38182 | val_1_rmse: 0.4965  |  0:01:07s
epoch 109| loss: 0.16876 | val_0_rmse: 0.40087 | val_1_rmse: 0.50506 |  0:01:08s
epoch 110| loss: 0.16598 | val_0_rmse: 0.37469 | val_1_rmse: 0.49179 |  0:01:08s
epoch 111| loss: 0.16638 | val_0_rmse: 0.38006 | val_1_rmse: 0.49381 |  0:01:09s
epoch 112| loss: 0.15679 | val_0_rmse: 0.36738 | val_1_rmse: 0.48545 |  0:01:10s
epoch 113| loss: 0.154   | val_0_rmse: 0.36462 | val_1_rmse: 0.4908  |  0:01:10s
epoch 114| loss: 0.16078 | val_0_rmse: 0.36938 | val_1_rmse: 0.48621 |  0:01:11s
epoch 115| loss: 0.15369 | val_0_rmse: 0.36112 | val_1_rmse: 0.48498 |  0:01:12s
epoch 116| loss: 0.15279 | val_0_rmse: 0.35983 | val_1_rmse: 0.47983 |  0:01:12s
epoch 117| loss: 0.15838 | val_0_rmse: 0.39678 | val_1_rmse: 0.50914 |  0:01:13s
epoch 118| loss: 0.16859 | val_0_rmse: 0.39102 | val_1_rmse: 0.49941 |  0:01:13s
epoch 119| loss: 0.1633  | val_0_rmse: 0.37921 | val_1_rmse: 0.4913  |  0:01:14s
epoch 120| loss: 0.16262 | val_0_rmse: 0.37181 | val_1_rmse: 0.48794 |  0:01:15s
epoch 121| loss: 0.15817 | val_0_rmse: 0.36805 | val_1_rmse: 0.48179 |  0:01:15s
epoch 122| loss: 0.15594 | val_0_rmse: 0.36827 | val_1_rmse: 0.48693 |  0:01:16s
epoch 123| loss: 0.15661 | val_0_rmse: 0.3665  | val_1_rmse: 0.48671 |  0:01:16s
epoch 124| loss: 0.15693 | val_0_rmse: 0.37628 | val_1_rmse: 0.49389 |  0:01:17s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.47843
Best weights from best epoch are automatically used!
ended training at: 17:20:25
Feature importance:
Mean squared error is of 20828211231.365513
Mean absolute error:102495.88689458465
MAPE:0.17115612522274606
R2 score:0.7399318378398055
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:20:26
epoch 0  | loss: 2.01693 | val_0_rmse: 0.99997 | val_1_rmse: 1.00426 |  0:00:00s
epoch 1  | loss: 1.16956 | val_0_rmse: 1.00139 | val_1_rmse: 1.00471 |  0:00:01s
epoch 2  | loss: 1.008   | val_0_rmse: 0.99967 | val_1_rmse: 1.00128 |  0:00:01s
epoch 3  | loss: 0.99338 | val_0_rmse: 0.99856 | val_1_rmse: 0.99972 |  0:00:02s
epoch 4  | loss: 0.96827 | val_0_rmse: 0.97635 | val_1_rmse: 0.98402 |  0:00:03s
epoch 5  | loss: 0.86533 | val_0_rmse: 1.14914 | val_1_rmse: 1.18075 |  0:00:03s
epoch 6  | loss: 0.72702 | val_0_rmse: 0.92507 | val_1_rmse: 0.95231 |  0:00:04s
epoch 7  | loss: 0.69094 | val_0_rmse: 0.89492 | val_1_rmse: 0.91148 |  0:00:05s
epoch 8  | loss: 0.62129 | val_0_rmse: 0.85637 | val_1_rmse: 0.86827 |  0:00:05s
epoch 9  | loss: 0.58214 | val_0_rmse: 0.87482 | val_1_rmse: 0.88445 |  0:00:06s
epoch 10 | loss: 0.5342  | val_0_rmse: 0.87608 | val_1_rmse: 0.88198 |  0:00:06s
epoch 11 | loss: 0.46841 | val_0_rmse: 0.92097 | val_1_rmse: 0.92627 |  0:00:07s
epoch 12 | loss: 0.4333  | val_0_rmse: 0.91785 | val_1_rmse: 0.91649 |  0:00:08s
epoch 13 | loss: 0.39609 | val_0_rmse: 0.90367 | val_1_rmse: 0.904   |  0:00:08s
epoch 14 | loss: 0.37333 | val_0_rmse: 0.86871 | val_1_rmse: 0.87334 |  0:00:09s
epoch 15 | loss: 0.35072 | val_0_rmse: 0.84802 | val_1_rmse: 0.85444 |  0:00:10s
epoch 16 | loss: 0.33274 | val_0_rmse: 0.84604 | val_1_rmse: 0.85771 |  0:00:10s
epoch 17 | loss: 0.31971 | val_0_rmse: 0.7952  | val_1_rmse: 0.80739 |  0:00:11s
epoch 18 | loss: 0.30155 | val_0_rmse: 0.80554 | val_1_rmse: 0.81304 |  0:00:11s
epoch 19 | loss: 0.30094 | val_0_rmse: 0.79992 | val_1_rmse: 0.80167 |  0:00:12s
epoch 20 | loss: 0.31658 | val_0_rmse: 0.75427 | val_1_rmse: 0.75437 |  0:00:13s
epoch 21 | loss: 0.30885 | val_0_rmse: 0.71966 | val_1_rmse: 0.73487 |  0:00:13s
epoch 22 | loss: 0.30551 | val_0_rmse: 0.69378 | val_1_rmse: 0.71306 |  0:00:14s
epoch 23 | loss: 0.28789 | val_0_rmse: 0.67804 | val_1_rmse: 0.69235 |  0:00:14s
epoch 24 | loss: 0.28022 | val_0_rmse: 0.68806 | val_1_rmse: 0.69793 |  0:00:15s
epoch 25 | loss: 0.27464 | val_0_rmse: 0.66595 | val_1_rmse: 0.67758 |  0:00:16s
epoch 26 | loss: 0.26489 | val_0_rmse: 0.67438 | val_1_rmse: 0.68499 |  0:00:16s
epoch 27 | loss: 0.26529 | val_0_rmse: 0.64464 | val_1_rmse: 0.65852 |  0:00:17s
epoch 28 | loss: 0.2589  | val_0_rmse: 0.64303 | val_1_rmse: 0.64955 |  0:00:18s
epoch 29 | loss: 0.25152 | val_0_rmse: 0.64462 | val_1_rmse: 0.65493 |  0:00:18s
epoch 30 | loss: 0.24303 | val_0_rmse: 0.63865 | val_1_rmse: 0.65506 |  0:00:19s
epoch 31 | loss: 0.24756 | val_0_rmse: 0.63916 | val_1_rmse: 0.65953 |  0:00:20s
epoch 32 | loss: 0.23576 | val_0_rmse: 0.64222 | val_1_rmse: 0.66267 |  0:00:20s
epoch 33 | loss: 0.23071 | val_0_rmse: 0.63929 | val_1_rmse: 0.65918 |  0:00:21s
epoch 34 | loss: 0.23311 | val_0_rmse: 0.62671 | val_1_rmse: 0.65453 |  0:00:21s
epoch 35 | loss: 0.23556 | val_0_rmse: 0.60725 | val_1_rmse: 0.63461 |  0:00:22s
epoch 36 | loss: 0.23592 | val_0_rmse: 0.6141  | val_1_rmse: 0.63581 |  0:00:23s
epoch 37 | loss: 0.23932 | val_0_rmse: 0.61331 | val_1_rmse: 0.63718 |  0:00:23s
epoch 38 | loss: 0.22725 | val_0_rmse: 0.61194 | val_1_rmse: 0.64031 |  0:00:24s
epoch 39 | loss: 0.22117 | val_0_rmse: 0.60808 | val_1_rmse: 0.64077 |  0:00:24s
epoch 40 | loss: 0.21917 | val_0_rmse: 0.60776 | val_1_rmse: 0.63072 |  0:00:25s
epoch 41 | loss: 0.22379 | val_0_rmse: 0.59033 | val_1_rmse: 0.61544 |  0:00:26s
epoch 42 | loss: 0.21198 | val_0_rmse: 0.58576 | val_1_rmse: 0.61408 |  0:00:26s
epoch 43 | loss: 0.20869 | val_0_rmse: 0.57919 | val_1_rmse: 0.60843 |  0:00:27s
epoch 44 | loss: 0.20822 | val_0_rmse: 0.58253 | val_1_rmse: 0.61034 |  0:00:28s
epoch 45 | loss: 0.20692 | val_0_rmse: 0.58267 | val_1_rmse: 0.61082 |  0:00:28s
epoch 46 | loss: 0.20634 | val_0_rmse: 0.57299 | val_1_rmse: 0.5946  |  0:00:29s
epoch 47 | loss: 0.20681 | val_0_rmse: 0.56695 | val_1_rmse: 0.59383 |  0:00:29s
epoch 48 | loss: 0.20114 | val_0_rmse: 0.57399 | val_1_rmse: 0.60321 |  0:00:30s
epoch 49 | loss: 0.20155 | val_0_rmse: 0.56819 | val_1_rmse: 0.59422 |  0:00:31s
epoch 50 | loss: 0.19974 | val_0_rmse: 0.5623  | val_1_rmse: 0.59256 |  0:00:31s
epoch 51 | loss: 0.20324 | val_0_rmse: 0.56508 | val_1_rmse: 0.59327 |  0:00:32s
epoch 52 | loss: 0.19662 | val_0_rmse: 0.55128 | val_1_rmse: 0.58155 |  0:00:32s
epoch 53 | loss: 0.19676 | val_0_rmse: 0.54945 | val_1_rmse: 0.58206 |  0:00:33s
epoch 54 | loss: 0.19637 | val_0_rmse: 0.54514 | val_1_rmse: 0.57844 |  0:00:34s
epoch 55 | loss: 0.19659 | val_0_rmse: 0.53733 | val_1_rmse: 0.57234 |  0:00:34s
epoch 56 | loss: 0.19736 | val_0_rmse: 0.53774 | val_1_rmse: 0.56652 |  0:00:35s
epoch 57 | loss: 0.1984  | val_0_rmse: 0.52564 | val_1_rmse: 0.56574 |  0:00:36s
epoch 58 | loss: 0.19481 | val_0_rmse: 0.52968 | val_1_rmse: 0.56753 |  0:00:36s
epoch 59 | loss: 0.19122 | val_0_rmse: 0.51893 | val_1_rmse: 0.55649 |  0:00:37s
epoch 60 | loss: 0.19493 | val_0_rmse: 0.51606 | val_1_rmse: 0.55648 |  0:00:38s
epoch 61 | loss: 0.18904 | val_0_rmse: 0.50906 | val_1_rmse: 0.55372 |  0:00:38s
epoch 62 | loss: 0.1955  | val_0_rmse: 0.51453 | val_1_rmse: 0.55009 |  0:00:39s
epoch 63 | loss: 0.19613 | val_0_rmse: 0.515   | val_1_rmse: 0.54617 |  0:00:39s
epoch 64 | loss: 0.19146 | val_0_rmse: 0.51652 | val_1_rmse: 0.56627 |  0:00:40s
epoch 65 | loss: 0.2045  | val_0_rmse: 0.4943  | val_1_rmse: 0.53594 |  0:00:41s
epoch 66 | loss: 0.20035 | val_0_rmse: 0.5077  | val_1_rmse: 0.5474  |  0:00:41s
epoch 67 | loss: 0.1929  | val_0_rmse: 0.49387 | val_1_rmse: 0.54712 |  0:00:42s
epoch 68 | loss: 0.18542 | val_0_rmse: 0.48201 | val_1_rmse: 0.53078 |  0:00:42s
epoch 69 | loss: 0.18477 | val_0_rmse: 0.47553 | val_1_rmse: 0.52103 |  0:00:43s
epoch 70 | loss: 0.18202 | val_0_rmse: 0.47033 | val_1_rmse: 0.52309 |  0:00:44s
epoch 71 | loss: 0.18203 | val_0_rmse: 0.46035 | val_1_rmse: 0.51688 |  0:00:44s
epoch 72 | loss: 0.18691 | val_0_rmse: 0.4554  | val_1_rmse: 0.51203 |  0:00:45s
epoch 73 | loss: 0.1798  | val_0_rmse: 0.45727 | val_1_rmse: 0.52542 |  0:00:46s
epoch 74 | loss: 0.17793 | val_0_rmse: 0.44729 | val_1_rmse: 0.50925 |  0:00:46s
epoch 75 | loss: 0.17969 | val_0_rmse: 0.4436  | val_1_rmse: 0.50906 |  0:00:47s
epoch 76 | loss: 0.1749  | val_0_rmse: 0.44212 | val_1_rmse: 0.51227 |  0:00:47s
epoch 77 | loss: 0.17851 | val_0_rmse: 0.43893 | val_1_rmse: 0.51109 |  0:00:48s
epoch 78 | loss: 0.17009 | val_0_rmse: 0.43428 | val_1_rmse: 0.5061  |  0:00:49s
epoch 79 | loss: 0.17601 | val_0_rmse: 0.43019 | val_1_rmse: 0.51057 |  0:00:49s
epoch 80 | loss: 0.17383 | val_0_rmse: 0.41871 | val_1_rmse: 0.50023 |  0:00:50s
epoch 81 | loss: 0.1731  | val_0_rmse: 0.42219 | val_1_rmse: 0.50079 |  0:00:50s
epoch 82 | loss: 0.17484 | val_0_rmse: 0.42089 | val_1_rmse: 0.50624 |  0:00:51s
epoch 83 | loss: 0.17629 | val_0_rmse: 0.41248 | val_1_rmse: 0.49941 |  0:00:52s
epoch 84 | loss: 0.17385 | val_0_rmse: 0.41166 | val_1_rmse: 0.50489 |  0:00:52s
epoch 85 | loss: 0.17211 | val_0_rmse: 0.41739 | val_1_rmse: 0.50932 |  0:00:53s
epoch 86 | loss: 0.1707  | val_0_rmse: 0.4073  | val_1_rmse: 0.49964 |  0:00:54s
epoch 87 | loss: 0.17178 | val_0_rmse: 0.42467 | val_1_rmse: 0.52251 |  0:00:54s
epoch 88 | loss: 0.17304 | val_0_rmse: 0.39868 | val_1_rmse: 0.49585 |  0:00:55s
epoch 89 | loss: 0.16928 | val_0_rmse: 0.40162 | val_1_rmse: 0.50079 |  0:00:55s
epoch 90 | loss: 0.16802 | val_0_rmse: 0.39096 | val_1_rmse: 0.49338 |  0:00:56s
epoch 91 | loss: 0.1617  | val_0_rmse: 0.38336 | val_1_rmse: 0.4932  |  0:00:57s
epoch 92 | loss: 0.163   | val_0_rmse: 0.39037 | val_1_rmse: 0.49785 |  0:00:57s
epoch 93 | loss: 0.1615  | val_0_rmse: 0.38393 | val_1_rmse: 0.49336 |  0:00:58s
epoch 94 | loss: 0.15753 | val_0_rmse: 0.38135 | val_1_rmse: 0.49869 |  0:00:59s
epoch 95 | loss: 0.16143 | val_0_rmse: 0.38107 | val_1_rmse: 0.49488 |  0:00:59s
epoch 96 | loss: 0.16336 | val_0_rmse: 0.38066 | val_1_rmse: 0.49193 |  0:01:00s
epoch 97 | loss: 0.16233 | val_0_rmse: 0.37553 | val_1_rmse: 0.4949  |  0:01:00s
epoch 98 | loss: 0.16086 | val_0_rmse: 0.3726  | val_1_rmse: 0.49252 |  0:01:01s
epoch 99 | loss: 0.15568 | val_0_rmse: 0.37002 | val_1_rmse: 0.49173 |  0:01:02s
epoch 100| loss: 0.1553  | val_0_rmse: 0.37087 | val_1_rmse: 0.49139 |  0:01:02s
epoch 101| loss: 0.15325 | val_0_rmse: 0.36361 | val_1_rmse: 0.48965 |  0:01:03s
epoch 102| loss: 0.15248 | val_0_rmse: 0.36507 | val_1_rmse: 0.49164 |  0:01:03s
epoch 103| loss: 0.15029 | val_0_rmse: 0.36176 | val_1_rmse: 0.49844 |  0:01:04s
epoch 104| loss: 0.14906 | val_0_rmse: 0.35829 | val_1_rmse: 0.49525 |  0:01:05s
epoch 105| loss: 0.15532 | val_0_rmse: 0.36182 | val_1_rmse: 0.49375 |  0:01:05s
epoch 106| loss: 0.1493  | val_0_rmse: 0.36169 | val_1_rmse: 0.4959  |  0:01:06s
epoch 107| loss: 0.15153 | val_0_rmse: 0.35717 | val_1_rmse: 0.49292 |  0:01:07s
epoch 108| loss: 0.14954 | val_0_rmse: 0.3516  | val_1_rmse: 0.4928  |  0:01:07s
epoch 109| loss: 0.14736 | val_0_rmse: 0.36164 | val_1_rmse: 0.5065  |  0:01:08s
epoch 110| loss: 0.14978 | val_0_rmse: 0.36116 | val_1_rmse: 0.50395 |  0:01:08s
epoch 111| loss: 0.15334 | val_0_rmse: 0.35792 | val_1_rmse: 0.49531 |  0:01:09s
epoch 112| loss: 0.14856 | val_0_rmse: 0.34686 | val_1_rmse: 0.49738 |  0:01:10s
epoch 113| loss: 0.1454  | val_0_rmse: 0.34973 | val_1_rmse: 0.4967  |  0:01:10s
epoch 114| loss: 0.14276 | val_0_rmse: 0.34592 | val_1_rmse: 0.49546 |  0:01:11s
epoch 115| loss: 0.14569 | val_0_rmse: 0.34733 | val_1_rmse: 0.4953  |  0:01:12s
epoch 116| loss: 0.14931 | val_0_rmse: 0.35125 | val_1_rmse: 0.5097  |  0:01:12s
epoch 117| loss: 0.15724 | val_0_rmse: 0.34582 | val_1_rmse: 0.49564 |  0:01:13s
epoch 118| loss: 0.14844 | val_0_rmse: 0.34769 | val_1_rmse: 0.49745 |  0:01:13s
epoch 119| loss: 0.14681 | val_0_rmse: 0.34326 | val_1_rmse: 0.49886 |  0:01:14s
epoch 120| loss: 0.14622 | val_0_rmse: 0.34408 | val_1_rmse: 0.48732 |  0:01:15s
epoch 121| loss: 0.14032 | val_0_rmse: 0.3444  | val_1_rmse: 0.50097 |  0:01:15s
epoch 122| loss: 0.14181 | val_0_rmse: 0.33993 | val_1_rmse: 0.49583 |  0:01:16s
epoch 123| loss: 0.14221 | val_0_rmse: 0.34529 | val_1_rmse: 0.49656 |  0:01:16s
epoch 124| loss: 0.14627 | val_0_rmse: 0.3455  | val_1_rmse: 0.51382 |  0:01:17s
epoch 125| loss: 0.14486 | val_0_rmse: 0.34136 | val_1_rmse: 0.5002  |  0:01:18s
epoch 126| loss: 0.1385  | val_0_rmse: 0.33555 | val_1_rmse: 0.50109 |  0:01:18s
epoch 127| loss: 0.13957 | val_0_rmse: 0.3331  | val_1_rmse: 0.49615 |  0:01:19s
epoch 128| loss: 0.13588 | val_0_rmse: 0.33088 | val_1_rmse: 0.49609 |  0:01:20s
epoch 129| loss: 0.13714 | val_0_rmse: 0.33683 | val_1_rmse: 0.49844 |  0:01:20s
epoch 130| loss: 0.14183 | val_0_rmse: 0.34421 | val_1_rmse: 0.51503 |  0:01:21s
epoch 131| loss: 0.13967 | val_0_rmse: 0.3384  | val_1_rmse: 0.49764 |  0:01:21s
epoch 132| loss: 0.13662 | val_0_rmse: 0.3305  | val_1_rmse: 0.49981 |  0:01:22s
epoch 133| loss: 0.13874 | val_0_rmse: 0.33271 | val_1_rmse: 0.4993  |  0:01:23s
epoch 134| loss: 0.14308 | val_0_rmse: 0.33315 | val_1_rmse: 0.49764 |  0:01:23s
epoch 135| loss: 0.13797 | val_0_rmse: 0.33212 | val_1_rmse: 0.50166 |  0:01:24s
epoch 136| loss: 0.13417 | val_0_rmse: 0.3282  | val_1_rmse: 0.50347 |  0:01:25s
epoch 137| loss: 0.13354 | val_0_rmse: 0.32351 | val_1_rmse: 0.49666 |  0:01:25s
epoch 138| loss: 0.12827 | val_0_rmse: 0.33165 | val_1_rmse: 0.50046 |  0:01:26s
epoch 139| loss: 0.13133 | val_0_rmse: 0.33056 | val_1_rmse: 0.49916 |  0:01:26s
epoch 140| loss: 0.12758 | val_0_rmse: 0.33316 | val_1_rmse: 0.50558 |  0:01:27s
epoch 141| loss: 0.13438 | val_0_rmse: 0.32683 | val_1_rmse: 0.50965 |  0:01:28s
epoch 142| loss: 0.12912 | val_0_rmse: 0.32749 | val_1_rmse: 0.50272 |  0:01:28s
epoch 143| loss: 0.13344 | val_0_rmse: 0.3187  | val_1_rmse: 0.49758 |  0:01:29s
epoch 144| loss: 0.13348 | val_0_rmse: 0.32472 | val_1_rmse: 0.50364 |  0:01:29s
epoch 145| loss: 0.13511 | val_0_rmse: 0.32785 | val_1_rmse: 0.49782 |  0:01:30s
epoch 146| loss: 0.13708 | val_0_rmse: 0.33076 | val_1_rmse: 0.51612 |  0:01:31s
epoch 147| loss: 0.13122 | val_0_rmse: 0.32663 | val_1_rmse: 0.49553 |  0:01:31s
epoch 148| loss: 0.12735 | val_0_rmse: 0.31938 | val_1_rmse: 0.50016 |  0:01:32s
epoch 149| loss: 0.12978 | val_0_rmse: 0.32321 | val_1_rmse: 0.50412 |  0:01:33s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.48732
Best weights from best epoch are automatically used!
ended training at: 17:21:59
Feature importance:
Mean squared error is of 21918885940.872864
Mean absolute error:103684.5768121714
MAPE:0.17270748094771088
R2 score:0.725592760253644
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:22:00
epoch 0  | loss: 1.96218 | val_0_rmse: 1.00036 | val_1_rmse: 0.99854 |  0:00:00s
epoch 1  | loss: 1.09707 | val_0_rmse: 0.99961 | val_1_rmse: 0.99461 |  0:00:01s
epoch 2  | loss: 0.98186 | val_0_rmse: 0.92442 | val_1_rmse: 0.91479 |  0:00:01s
epoch 3  | loss: 0.8228  | val_0_rmse: 0.85643 | val_1_rmse: 0.82817 |  0:00:02s
epoch 4  | loss: 0.69797 | val_0_rmse: 0.82228 | val_1_rmse: 0.81013 |  0:00:03s
epoch 5  | loss: 0.61142 | val_0_rmse: 0.77813 | val_1_rmse: 0.76735 |  0:00:03s
epoch 6  | loss: 0.5246  | val_0_rmse: 0.74992 | val_1_rmse: 0.75052 |  0:00:04s
epoch 7  | loss: 0.47585 | val_0_rmse: 0.73499 | val_1_rmse: 0.72896 |  0:00:05s
epoch 8  | loss: 0.44195 | val_0_rmse: 0.73108 | val_1_rmse: 0.7118  |  0:00:05s
epoch 9  | loss: 0.41248 | val_0_rmse: 0.71767 | val_1_rmse: 0.71067 |  0:00:06s
epoch 10 | loss: 0.40046 | val_0_rmse: 0.70337 | val_1_rmse: 0.69528 |  0:00:06s
epoch 11 | loss: 0.37542 | val_0_rmse: 0.68867 | val_1_rmse: 0.68183 |  0:00:07s
epoch 12 | loss: 0.36969 | val_0_rmse: 0.68734 | val_1_rmse: 0.68732 |  0:00:08s
epoch 13 | loss: 0.35762 | val_0_rmse: 0.68007 | val_1_rmse: 0.67393 |  0:00:08s
epoch 14 | loss: 0.343   | val_0_rmse: 0.66506 | val_1_rmse: 0.66601 |  0:00:09s
epoch 15 | loss: 0.33145 | val_0_rmse: 0.66132 | val_1_rmse: 0.65538 |  0:00:10s
epoch 16 | loss: 0.32223 | val_0_rmse: 0.65715 | val_1_rmse: 0.65396 |  0:00:10s
epoch 17 | loss: 0.33214 | val_0_rmse: 0.67583 | val_1_rmse: 0.67541 |  0:00:11s
epoch 18 | loss: 0.32323 | val_0_rmse: 0.67218 | val_1_rmse: 0.67427 |  0:00:11s
epoch 19 | loss: 0.31628 | val_0_rmse: 0.6575  | val_1_rmse: 0.65699 |  0:00:12s
epoch 20 | loss: 0.30419 | val_0_rmse: 0.63908 | val_1_rmse: 0.64635 |  0:00:13s
epoch 21 | loss: 0.30575 | val_0_rmse: 0.6408  | val_1_rmse: 0.64978 |  0:00:13s
epoch 22 | loss: 0.3128  | val_0_rmse: 0.62955 | val_1_rmse: 0.63987 |  0:00:14s
epoch 23 | loss: 0.30153 | val_0_rmse: 0.64884 | val_1_rmse: 0.64411 |  0:00:15s
epoch 24 | loss: 0.30394 | val_0_rmse: 0.66313 | val_1_rmse: 0.65628 |  0:00:15s
epoch 25 | loss: 0.29997 | val_0_rmse: 0.64109 | val_1_rmse: 0.63608 |  0:00:16s
epoch 26 | loss: 0.29559 | val_0_rmse: 0.61031 | val_1_rmse: 0.61553 |  0:00:16s
epoch 27 | loss: 0.29223 | val_0_rmse: 0.59255 | val_1_rmse: 0.60279 |  0:00:17s
epoch 28 | loss: 0.28876 | val_0_rmse: 0.58879 | val_1_rmse: 0.59979 |  0:00:18s
epoch 29 | loss: 0.28037 | val_0_rmse: 0.58728 | val_1_rmse: 0.59068 |  0:00:18s
epoch 30 | loss: 0.28019 | val_0_rmse: 0.58622 | val_1_rmse: 0.59089 |  0:00:19s
epoch 31 | loss: 0.28221 | val_0_rmse: 0.58132 | val_1_rmse: 0.58618 |  0:00:20s
epoch 32 | loss: 0.28213 | val_0_rmse: 0.60544 | val_1_rmse: 0.6183  |  0:00:20s
epoch 33 | loss: 0.29805 | val_0_rmse: 0.5877  | val_1_rmse: 0.59172 |  0:00:21s
epoch 34 | loss: 0.29857 | val_0_rmse: 0.63677 | val_1_rmse: 0.6359  |  0:00:21s
epoch 35 | loss: 0.2802  | val_0_rmse: 0.5993  | val_1_rmse: 0.60121 |  0:00:22s
epoch 36 | loss: 0.27602 | val_0_rmse: 0.57213 | val_1_rmse: 0.58177 |  0:00:23s
epoch 37 | loss: 0.27562 | val_0_rmse: 0.56714 | val_1_rmse: 0.57747 |  0:00:23s
epoch 38 | loss: 0.26123 | val_0_rmse: 0.58762 | val_1_rmse: 0.59336 |  0:00:24s
epoch 39 | loss: 0.26358 | val_0_rmse: 0.57424 | val_1_rmse: 0.58364 |  0:00:25s
epoch 40 | loss: 0.25287 | val_0_rmse: 0.56945 | val_1_rmse: 0.58733 |  0:00:25s
epoch 41 | loss: 0.24874 | val_0_rmse: 0.5679  | val_1_rmse: 0.57389 |  0:00:26s
epoch 42 | loss: 0.24566 | val_0_rmse: 0.56867 | val_1_rmse: 0.57634 |  0:00:26s
epoch 43 | loss: 0.24336 | val_0_rmse: 0.54842 | val_1_rmse: 0.56269 |  0:00:27s
epoch 44 | loss: 0.23522 | val_0_rmse: 0.55048 | val_1_rmse: 0.56711 |  0:00:28s
epoch 45 | loss: 0.24642 | val_0_rmse: 0.55972 | val_1_rmse: 0.57008 |  0:00:28s
epoch 46 | loss: 0.24486 | val_0_rmse: 0.55279 | val_1_rmse: 0.56664 |  0:00:29s
epoch 47 | loss: 0.24659 | val_0_rmse: 0.53938 | val_1_rmse: 0.55364 |  0:00:30s
epoch 48 | loss: 0.24121 | val_0_rmse: 0.53888 | val_1_rmse: 0.5513  |  0:00:30s
epoch 49 | loss: 0.23955 | val_0_rmse: 0.52986 | val_1_rmse: 0.54457 |  0:00:31s
epoch 50 | loss: 0.23784 | val_0_rmse: 0.52091 | val_1_rmse: 0.54175 |  0:00:31s
epoch 51 | loss: 0.24333 | val_0_rmse: 0.56913 | val_1_rmse: 0.59685 |  0:00:32s
epoch 52 | loss: 0.24523 | val_0_rmse: 0.53401 | val_1_rmse: 0.5635  |  0:00:33s
epoch 53 | loss: 0.23849 | val_0_rmse: 0.53124 | val_1_rmse: 0.55628 |  0:00:33s
epoch 54 | loss: 0.24016 | val_0_rmse: 0.52581 | val_1_rmse: 0.54623 |  0:00:34s
epoch 55 | loss: 0.24502 | val_0_rmse: 0.52151 | val_1_rmse: 0.5472  |  0:00:34s
epoch 56 | loss: 0.23241 | val_0_rmse: 0.51646 | val_1_rmse: 0.54503 |  0:00:35s
epoch 57 | loss: 0.22973 | val_0_rmse: 0.51446 | val_1_rmse: 0.53595 |  0:00:36s
epoch 58 | loss: 0.22229 | val_0_rmse: 0.51338 | val_1_rmse: 0.54651 |  0:00:36s
epoch 59 | loss: 0.22908 | val_0_rmse: 0.51126 | val_1_rmse: 0.53701 |  0:00:37s
epoch 60 | loss: 0.21885 | val_0_rmse: 0.51056 | val_1_rmse: 0.54111 |  0:00:38s
epoch 61 | loss: 0.22644 | val_0_rmse: 0.51078 | val_1_rmse: 0.54535 |  0:00:38s
epoch 62 | loss: 0.2201  | val_0_rmse: 0.50786 | val_1_rmse: 0.54245 |  0:00:39s
epoch 63 | loss: 0.22569 | val_0_rmse: 0.50969 | val_1_rmse: 0.5359  |  0:00:40s
epoch 64 | loss: 0.23814 | val_0_rmse: 0.52936 | val_1_rmse: 0.57005 |  0:00:40s
epoch 65 | loss: 0.24384 | val_0_rmse: 0.53003 | val_1_rmse: 0.57197 |  0:00:41s
epoch 66 | loss: 0.23975 | val_0_rmse: 0.51104 | val_1_rmse: 0.54961 |  0:00:41s
epoch 67 | loss: 0.23377 | val_0_rmse: 0.49341 | val_1_rmse: 0.53012 |  0:00:42s
epoch 68 | loss: 0.22194 | val_0_rmse: 0.48699 | val_1_rmse: 0.52676 |  0:00:43s
epoch 69 | loss: 0.2216  | val_0_rmse: 0.4825  | val_1_rmse: 0.52243 |  0:00:43s
epoch 70 | loss: 0.21917 | val_0_rmse: 0.47632 | val_1_rmse: 0.52397 |  0:00:44s
epoch 71 | loss: 0.21714 | val_0_rmse: 0.47678 | val_1_rmse: 0.52388 |  0:00:44s
epoch 72 | loss: 0.2153  | val_0_rmse: 0.46518 | val_1_rmse: 0.51415 |  0:00:45s
epoch 73 | loss: 0.21067 | val_0_rmse: 0.46842 | val_1_rmse: 0.50948 |  0:00:46s
epoch 74 | loss: 0.22076 | val_0_rmse: 0.45832 | val_1_rmse: 0.5041  |  0:00:46s
epoch 75 | loss: 0.21861 | val_0_rmse: 0.47504 | val_1_rmse: 0.52042 |  0:00:47s
epoch 76 | loss: 0.2168  | val_0_rmse: 0.4593  | val_1_rmse: 0.50603 |  0:00:48s
epoch 77 | loss: 0.20903 | val_0_rmse: 0.46856 | val_1_rmse: 0.51312 |  0:00:48s
epoch 78 | loss: 0.20607 | val_0_rmse: 0.44998 | val_1_rmse: 0.50535 |  0:00:49s
epoch 79 | loss: 0.20314 | val_0_rmse: 0.45125 | val_1_rmse: 0.51204 |  0:00:50s
epoch 80 | loss: 0.20512 | val_0_rmse: 0.4443  | val_1_rmse: 0.49925 |  0:00:50s
epoch 81 | loss: 0.20556 | val_0_rmse: 0.43816 | val_1_rmse: 0.49409 |  0:00:51s
epoch 82 | loss: 0.20035 | val_0_rmse: 0.44635 | val_1_rmse: 0.50452 |  0:00:52s
epoch 83 | loss: 0.21288 | val_0_rmse: 0.438   | val_1_rmse: 0.50081 |  0:00:52s
epoch 84 | loss: 0.20127 | val_0_rmse: 0.43532 | val_1_rmse: 0.49267 |  0:00:53s
epoch 85 | loss: 0.19569 | val_0_rmse: 0.42602 | val_1_rmse: 0.49349 |  0:00:53s
epoch 86 | loss: 0.20412 | val_0_rmse: 0.42785 | val_1_rmse: 0.49231 |  0:00:54s
epoch 87 | loss: 0.19865 | val_0_rmse: 0.43006 | val_1_rmse: 0.50343 |  0:00:55s
epoch 88 | loss: 0.19505 | val_0_rmse: 0.42081 | val_1_rmse: 0.48975 |  0:00:55s
epoch 89 | loss: 0.19557 | val_0_rmse: 0.42004 | val_1_rmse: 0.49325 |  0:00:56s
epoch 90 | loss: 0.19097 | val_0_rmse: 0.4215  | val_1_rmse: 0.48958 |  0:00:57s
epoch 91 | loss: 0.19969 | val_0_rmse: 0.42554 | val_1_rmse: 0.49373 |  0:00:57s
epoch 92 | loss: 0.19853 | val_0_rmse: 0.4173  | val_1_rmse: 0.48817 |  0:00:58s
epoch 93 | loss: 0.19503 | val_0_rmse: 0.41987 | val_1_rmse: 0.50091 |  0:00:58s
epoch 94 | loss: 0.18573 | val_0_rmse: 0.40973 | val_1_rmse: 0.49722 |  0:00:59s
epoch 95 | loss: 0.1844  | val_0_rmse: 0.41541 | val_1_rmse: 0.5012  |  0:01:00s
epoch 96 | loss: 0.1847  | val_0_rmse: 0.42234 | val_1_rmse: 0.50763 |  0:01:00s
epoch 97 | loss: 0.18759 | val_0_rmse: 0.40708 | val_1_rmse: 0.48833 |  0:01:01s
epoch 98 | loss: 0.1869  | val_0_rmse: 0.41391 | val_1_rmse: 0.49102 |  0:01:01s
epoch 99 | loss: 0.1925  | val_0_rmse: 0.41423 | val_1_rmse: 0.49967 |  0:01:02s
epoch 100| loss: 0.18496 | val_0_rmse: 0.39949 | val_1_rmse: 0.48482 |  0:01:03s
epoch 101| loss: 0.18439 | val_0_rmse: 0.39833 | val_1_rmse: 0.48334 |  0:01:03s
epoch 102| loss: 0.18401 | val_0_rmse: 0.39609 | val_1_rmse: 0.48305 |  0:01:04s
epoch 103| loss: 0.18242 | val_0_rmse: 0.39415 | val_1_rmse: 0.48084 |  0:01:05s
epoch 104| loss: 0.17933 | val_0_rmse: 0.39036 | val_1_rmse: 0.48407 |  0:01:05s
epoch 105| loss: 0.17786 | val_0_rmse: 0.39511 | val_1_rmse: 0.48172 |  0:01:06s
epoch 106| loss: 0.17412 | val_0_rmse: 0.39002 | val_1_rmse: 0.48013 |  0:01:07s
epoch 107| loss: 0.17512 | val_0_rmse: 0.38685 | val_1_rmse: 0.49198 |  0:01:07s
epoch 108| loss: 0.17756 | val_0_rmse: 0.38854 | val_1_rmse: 0.49047 |  0:01:08s
epoch 109| loss: 0.17453 | val_0_rmse: 0.40653 | val_1_rmse: 0.49617 |  0:01:08s
epoch 110| loss: 0.19456 | val_0_rmse: 0.43456 | val_1_rmse: 0.5057  |  0:01:09s
epoch 111| loss: 0.21257 | val_0_rmse: 0.43807 | val_1_rmse: 0.5062  |  0:01:10s
epoch 112| loss: 0.20426 | val_0_rmse: 0.43086 | val_1_rmse: 0.5111  |  0:01:10s
epoch 113| loss: 0.19755 | val_0_rmse: 0.41223 | val_1_rmse: 0.49049 |  0:01:11s
epoch 114| loss: 0.19673 | val_0_rmse: 0.4054  | val_1_rmse: 0.49422 |  0:01:11s
epoch 115| loss: 0.18819 | val_0_rmse: 0.40542 | val_1_rmse: 0.49383 |  0:01:12s
epoch 116| loss: 0.18549 | val_0_rmse: 0.40007 | val_1_rmse: 0.49236 |  0:01:13s
epoch 117| loss: 0.18846 | val_0_rmse: 0.40262 | val_1_rmse: 0.49383 |  0:01:13s
epoch 118| loss: 0.18608 | val_0_rmse: 0.39898 | val_1_rmse: 0.48536 |  0:01:14s
epoch 119| loss: 0.18142 | val_0_rmse: 0.39907 | val_1_rmse: 0.48447 |  0:01:15s
epoch 120| loss: 0.17895 | val_0_rmse: 0.39183 | val_1_rmse: 0.48999 |  0:01:15s
epoch 121| loss: 0.18029 | val_0_rmse: 0.4054  | val_1_rmse: 0.49705 |  0:01:16s
epoch 122| loss: 0.17798 | val_0_rmse: 0.38677 | val_1_rmse: 0.4817  |  0:01:16s
epoch 123| loss: 0.17121 | val_0_rmse: 0.38949 | val_1_rmse: 0.48164 |  0:01:17s
epoch 124| loss: 0.1793  | val_0_rmse: 0.38636 | val_1_rmse: 0.48906 |  0:01:18s
epoch 125| loss: 0.17301 | val_0_rmse: 0.38332 | val_1_rmse: 0.48454 |  0:01:18s
epoch 126| loss: 0.17278 | val_0_rmse: 0.38334 | val_1_rmse: 0.48525 |  0:01:19s
epoch 127| loss: 0.1737  | val_0_rmse: 0.3859  | val_1_rmse: 0.49736 |  0:01:19s
epoch 128| loss: 0.17319 | val_0_rmse: 0.39356 | val_1_rmse: 0.50607 |  0:01:20s
epoch 129| loss: 0.17913 | val_0_rmse: 0.38351 | val_1_rmse: 0.48384 |  0:01:21s
epoch 130| loss: 0.16879 | val_0_rmse: 0.37978 | val_1_rmse: 0.48833 |  0:01:21s
epoch 131| loss: 0.16806 | val_0_rmse: 0.37596 | val_1_rmse: 0.49173 |  0:01:22s
epoch 132| loss: 0.1632  | val_0_rmse: 0.37872 | val_1_rmse: 0.50231 |  0:01:23s
epoch 133| loss: 0.16697 | val_0_rmse: 0.37551 | val_1_rmse: 0.48558 |  0:01:23s
epoch 134| loss: 0.1666  | val_0_rmse: 0.3703  | val_1_rmse: 0.48338 |  0:01:24s
epoch 135| loss: 0.16533 | val_0_rmse: 0.38142 | val_1_rmse: 0.49097 |  0:01:24s
epoch 136| loss: 0.16865 | val_0_rmse: 0.36946 | val_1_rmse: 0.48313 |  0:01:25s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.48013
Best weights from best epoch are automatically used!
ended training at: 17:23:26
Feature importance:
Mean squared error is of 20675595506.119232
Mean absolute error:100769.35657209516
MAPE:0.16956983844180096
R2 score:0.7456265693932964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:23:26
epoch 0  | loss: 1.91725 | val_0_rmse: 0.99272 | val_1_rmse: 1.01358 |  0:00:00s
epoch 1  | loss: 1.11482 | val_0_rmse: 0.99536 | val_1_rmse: 1.01514 |  0:00:01s
epoch 2  | loss: 1.00348 | val_0_rmse: 0.99407 | val_1_rmse: 1.01381 |  0:00:01s
epoch 3  | loss: 0.97448 | val_0_rmse: 0.99995 | val_1_rmse: 1.02008 |  0:00:02s
epoch 4  | loss: 0.9489  | val_0_rmse: 0.99323 | val_1_rmse: 1.01416 |  0:00:03s
epoch 5  | loss: 0.91555 | val_0_rmse: 0.97606 | val_1_rmse: 0.99921 |  0:00:03s
epoch 6  | loss: 0.84391 | val_0_rmse: 0.85698 | val_1_rmse: 0.88108 |  0:00:04s
epoch 7  | loss: 0.71601 | val_0_rmse: 0.82294 | val_1_rmse: 0.84434 |  0:00:04s
epoch 8  | loss: 0.59184 | val_0_rmse: 0.84099 | val_1_rmse: 0.8614  |  0:00:05s
epoch 9  | loss: 0.50977 | val_0_rmse: 0.88772 | val_1_rmse: 0.90953 |  0:00:06s
epoch 10 | loss: 0.43343 | val_0_rmse: 1.0106  | val_1_rmse: 1.02708 |  0:00:06s
epoch 11 | loss: 0.39214 | val_0_rmse: 0.84425 | val_1_rmse: 0.86521 |  0:00:07s
epoch 12 | loss: 0.33798 | val_0_rmse: 0.84876 | val_1_rmse: 0.86546 |  0:00:08s
epoch 13 | loss: 0.31541 | val_0_rmse: 0.79579 | val_1_rmse: 0.81282 |  0:00:08s
epoch 14 | loss: 0.29654 | val_0_rmse: 0.78432 | val_1_rmse: 0.80473 |  0:00:09s
epoch 15 | loss: 0.2815  | val_0_rmse: 0.79545 | val_1_rmse: 0.81685 |  0:00:09s
epoch 16 | loss: 0.2652  | val_0_rmse: 0.78758 | val_1_rmse: 0.80649 |  0:00:10s
epoch 17 | loss: 0.25782 | val_0_rmse: 0.75811 | val_1_rmse: 0.776   |  0:00:11s
epoch 18 | loss: 0.24655 | val_0_rmse: 0.75233 | val_1_rmse: 0.77208 |  0:00:11s
epoch 19 | loss: 0.24618 | val_0_rmse: 0.76216 | val_1_rmse: 0.78139 |  0:00:12s
epoch 20 | loss: 0.23805 | val_0_rmse: 0.75661 | val_1_rmse: 0.77862 |  0:00:13s
epoch 21 | loss: 0.23885 | val_0_rmse: 0.74951 | val_1_rmse: 0.77245 |  0:00:13s
epoch 22 | loss: 0.2274  | val_0_rmse: 0.7406  | val_1_rmse: 0.76548 |  0:00:14s
epoch 23 | loss: 0.22242 | val_0_rmse: 0.74577 | val_1_rmse: 0.76923 |  0:00:14s
epoch 24 | loss: 0.22712 | val_0_rmse: 0.737   | val_1_rmse: 0.75984 |  0:00:15s
epoch 25 | loss: 0.22399 | val_0_rmse: 0.74854 | val_1_rmse: 0.77163 |  0:00:16s
epoch 26 | loss: 0.22666 | val_0_rmse: 0.71941 | val_1_rmse: 0.74336 |  0:00:16s
epoch 27 | loss: 0.21672 | val_0_rmse: 0.70527 | val_1_rmse: 0.72919 |  0:00:17s
epoch 28 | loss: 0.21289 | val_0_rmse: 0.71319 | val_1_rmse: 0.73702 |  0:00:17s
epoch 29 | loss: 0.21468 | val_0_rmse: 0.70625 | val_1_rmse: 0.72844 |  0:00:18s
epoch 30 | loss: 0.20842 | val_0_rmse: 0.71567 | val_1_rmse: 0.73476 |  0:00:19s
epoch 31 | loss: 0.20425 | val_0_rmse: 0.69572 | val_1_rmse: 0.71834 |  0:00:19s
epoch 32 | loss: 0.20431 | val_0_rmse: 0.69526 | val_1_rmse: 0.71812 |  0:00:20s
epoch 33 | loss: 0.20024 | val_0_rmse: 0.70661 | val_1_rmse: 0.72808 |  0:00:21s
epoch 34 | loss: 0.19987 | val_0_rmse: 0.67922 | val_1_rmse: 0.70652 |  0:00:21s
epoch 35 | loss: 0.21522 | val_0_rmse: 0.69719 | val_1_rmse: 0.72586 |  0:00:22s
epoch 36 | loss: 0.20693 | val_0_rmse: 0.67121 | val_1_rmse: 0.70111 |  0:00:23s
epoch 37 | loss: 0.20406 | val_0_rmse: 0.67872 | val_1_rmse: 0.70661 |  0:00:23s
epoch 38 | loss: 0.20557 | val_0_rmse: 0.67152 | val_1_rmse: 0.69671 |  0:00:24s
epoch 39 | loss: 0.19822 | val_0_rmse: 0.6347  | val_1_rmse: 0.66304 |  0:00:24s
epoch 40 | loss: 0.19946 | val_0_rmse: 0.64884 | val_1_rmse: 0.67274 |  0:00:25s
epoch 41 | loss: 0.19977 | val_0_rmse: 0.62782 | val_1_rmse: 0.65353 |  0:00:26s
epoch 42 | loss: 0.19737 | val_0_rmse: 0.63987 | val_1_rmse: 0.66421 |  0:00:26s
epoch 43 | loss: 0.19464 | val_0_rmse: 0.62624 | val_1_rmse: 0.65214 |  0:00:27s
epoch 44 | loss: 0.19855 | val_0_rmse: 0.61191 | val_1_rmse: 0.63838 |  0:00:27s
epoch 45 | loss: 0.18587 | val_0_rmse: 0.63218 | val_1_rmse: 0.65686 |  0:00:28s
epoch 46 | loss: 0.19493 | val_0_rmse: 0.61141 | val_1_rmse: 0.64042 |  0:00:29s
epoch 47 | loss: 0.19363 | val_0_rmse: 0.60097 | val_1_rmse: 0.63354 |  0:00:29s
epoch 48 | loss: 0.18607 | val_0_rmse: 0.59853 | val_1_rmse: 0.62771 |  0:00:30s
epoch 49 | loss: 0.18928 | val_0_rmse: 0.59697 | val_1_rmse: 0.62738 |  0:00:31s
epoch 50 | loss: 0.18668 | val_0_rmse: 0.58072 | val_1_rmse: 0.60932 |  0:00:31s
epoch 51 | loss: 0.1896  | val_0_rmse: 0.57112 | val_1_rmse: 0.60116 |  0:00:32s
epoch 52 | loss: 0.17681 | val_0_rmse: 0.57616 | val_1_rmse: 0.60437 |  0:00:32s
epoch 53 | loss: 0.18223 | val_0_rmse: 0.55423 | val_1_rmse: 0.58634 |  0:00:33s
epoch 54 | loss: 0.18031 | val_0_rmse: 0.55303 | val_1_rmse: 0.58792 |  0:00:34s
epoch 55 | loss: 0.17754 | val_0_rmse: 0.5457  | val_1_rmse: 0.57861 |  0:00:34s
epoch 56 | loss: 0.17294 | val_0_rmse: 0.53597 | val_1_rmse: 0.57005 |  0:00:35s
epoch 57 | loss: 0.17585 | val_0_rmse: 0.53654 | val_1_rmse: 0.57164 |  0:00:35s
epoch 58 | loss: 0.16779 | val_0_rmse: 0.5175  | val_1_rmse: 0.55444 |  0:00:36s
epoch 59 | loss: 0.16979 | val_0_rmse: 0.51274 | val_1_rmse: 0.54841 |  0:00:37s
epoch 60 | loss: 0.17244 | val_0_rmse: 0.51849 | val_1_rmse: 0.55637 |  0:00:37s
epoch 61 | loss: 0.17999 | val_0_rmse: 0.50904 | val_1_rmse: 0.54453 |  0:00:38s
epoch 62 | loss: 0.17235 | val_0_rmse: 0.50339 | val_1_rmse: 0.54211 |  0:00:39s
epoch 63 | loss: 0.16863 | val_0_rmse: 0.49552 | val_1_rmse: 0.53476 |  0:00:39s
epoch 64 | loss: 0.16591 | val_0_rmse: 0.49621 | val_1_rmse: 0.53572 |  0:00:40s
epoch 65 | loss: 0.16533 | val_0_rmse: 0.48825 | val_1_rmse: 0.53102 |  0:00:41s
epoch 66 | loss: 0.16293 | val_0_rmse: 0.47378 | val_1_rmse: 0.52249 |  0:00:41s
epoch 67 | loss: 0.16632 | val_0_rmse: 0.47336 | val_1_rmse: 0.52086 |  0:00:42s
epoch 68 | loss: 0.16183 | val_0_rmse: 0.47123 | val_1_rmse: 0.51922 |  0:00:42s
epoch 69 | loss: 0.16257 | val_0_rmse: 0.45481 | val_1_rmse: 0.50716 |  0:00:43s
epoch 70 | loss: 0.16199 | val_0_rmse: 0.44054 | val_1_rmse: 0.49425 |  0:00:44s
epoch 71 | loss: 0.16187 | val_0_rmse: 0.45051 | val_1_rmse: 0.50484 |  0:00:44s
epoch 72 | loss: 0.15949 | val_0_rmse: 0.43366 | val_1_rmse: 0.4905  |  0:00:45s
epoch 73 | loss: 0.16084 | val_0_rmse: 0.44392 | val_1_rmse: 0.49775 |  0:00:46s
epoch 74 | loss: 0.15514 | val_0_rmse: 0.44968 | val_1_rmse: 0.50458 |  0:00:46s
epoch 75 | loss: 0.16456 | val_0_rmse: 0.43185 | val_1_rmse: 0.48704 |  0:00:47s
epoch 76 | loss: 0.16303 | val_0_rmse: 0.42486 | val_1_rmse: 0.48406 |  0:00:47s
epoch 77 | loss: 0.16106 | val_0_rmse: 0.41473 | val_1_rmse: 0.47879 |  0:00:48s
epoch 78 | loss: 0.15552 | val_0_rmse: 0.41567 | val_1_rmse: 0.47966 |  0:00:49s
epoch 79 | loss: 0.15446 | val_0_rmse: 0.40376 | val_1_rmse: 0.47106 |  0:00:49s
epoch 80 | loss: 0.15153 | val_0_rmse: 0.40832 | val_1_rmse: 0.4778  |  0:00:50s
epoch 81 | loss: 0.15394 | val_0_rmse: 0.39468 | val_1_rmse: 0.47131 |  0:00:50s
epoch 82 | loss: 0.14999 | val_0_rmse: 0.39358 | val_1_rmse: 0.46922 |  0:00:51s
epoch 83 | loss: 0.14997 | val_0_rmse: 0.40217 | val_1_rmse: 0.48027 |  0:00:52s
epoch 84 | loss: 0.15291 | val_0_rmse: 0.38942 | val_1_rmse: 0.47651 |  0:00:52s
epoch 85 | loss: 0.14719 | val_0_rmse: 0.39309 | val_1_rmse: 0.48032 |  0:00:53s
epoch 86 | loss: 0.14892 | val_0_rmse: 0.36838 | val_1_rmse: 0.46137 |  0:00:54s
epoch 87 | loss: 0.15047 | val_0_rmse: 0.37277 | val_1_rmse: 0.46184 |  0:00:54s
epoch 88 | loss: 0.14366 | val_0_rmse: 0.37016 | val_1_rmse: 0.46432 |  0:00:55s
epoch 89 | loss: 0.14407 | val_0_rmse: 0.38317 | val_1_rmse: 0.47418 |  0:00:55s
epoch 90 | loss: 0.14872 | val_0_rmse: 0.3666  | val_1_rmse: 0.46489 |  0:00:56s
epoch 91 | loss: 0.14889 | val_0_rmse: 0.3629  | val_1_rmse: 0.46186 |  0:00:57s
epoch 92 | loss: 0.14256 | val_0_rmse: 0.36461 | val_1_rmse: 0.46481 |  0:00:57s
epoch 93 | loss: 0.14472 | val_0_rmse: 0.35747 | val_1_rmse: 0.46593 |  0:00:58s
epoch 94 | loss: 0.14195 | val_0_rmse: 0.3548  | val_1_rmse: 0.46633 |  0:00:59s
epoch 95 | loss: 0.14537 | val_0_rmse: 0.35772 | val_1_rmse: 0.47277 |  0:00:59s
epoch 96 | loss: 0.1428  | val_0_rmse: 0.34804 | val_1_rmse: 0.46281 |  0:01:00s
epoch 97 | loss: 0.1445  | val_0_rmse: 0.34948 | val_1_rmse: 0.46145 |  0:01:00s
epoch 98 | loss: 0.1461  | val_0_rmse: 0.36126 | val_1_rmse: 0.47622 |  0:01:01s
epoch 99 | loss: 0.1457  | val_0_rmse: 0.34829 | val_1_rmse: 0.4669  |  0:01:02s
epoch 100| loss: 0.13873 | val_0_rmse: 0.35338 | val_1_rmse: 0.47481 |  0:01:02s
epoch 101| loss: 0.14343 | val_0_rmse: 0.34743 | val_1_rmse: 0.46763 |  0:01:03s
epoch 102| loss: 0.1405  | val_0_rmse: 0.3593  | val_1_rmse: 0.4821  |  0:01:03s
epoch 103| loss: 0.1368  | val_0_rmse: 0.33392 | val_1_rmse: 0.46785 |  0:01:04s
epoch 104| loss: 0.13858 | val_0_rmse: 0.33369 | val_1_rmse: 0.47136 |  0:01:05s
epoch 105| loss: 0.13311 | val_0_rmse: 0.3425  | val_1_rmse: 0.47865 |  0:01:05s
epoch 106| loss: 0.13846 | val_0_rmse: 0.34212 | val_1_rmse: 0.48251 |  0:01:06s
epoch 107| loss: 0.14261 | val_0_rmse: 0.34527 | val_1_rmse: 0.48872 |  0:01:07s
epoch 108| loss: 0.13554 | val_0_rmse: 0.33924 | val_1_rmse: 0.49324 |  0:01:07s
epoch 109| loss: 0.13502 | val_0_rmse: 0.32999 | val_1_rmse: 0.49421 |  0:01:08s
epoch 110| loss: 0.13681 | val_0_rmse: 0.33343 | val_1_rmse: 0.48309 |  0:01:08s
epoch 111| loss: 0.13076 | val_0_rmse: 0.32636 | val_1_rmse: 0.48755 |  0:01:09s
epoch 112| loss: 0.12968 | val_0_rmse: 0.33056 | val_1_rmse: 0.49876 |  0:01:10s
epoch 113| loss: 0.13176 | val_0_rmse: 0.33726 | val_1_rmse: 0.49801 |  0:01:10s
epoch 114| loss: 0.12531 | val_0_rmse: 0.33654 | val_1_rmse: 0.49216 |  0:01:11s
epoch 115| loss: 0.13053 | val_0_rmse: 0.32187 | val_1_rmse: 0.49012 |  0:01:12s
epoch 116| loss: 0.12783 | val_0_rmse: 0.31776 | val_1_rmse: 0.48808 |  0:01:12s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.46137
Best weights from best epoch are automatically used!
ended training at: 17:24:39
Feature importance:
Mean squared error is of 21050676323.077847
Mean absolute error:104154.8278629732
MAPE:0.17254496278900308
R2 score:0.7453021014495225
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:24:39
epoch 0  | loss: 2.22865 | val_0_rmse: 1.00384 | val_1_rmse: 0.98792 |  0:00:00s
epoch 1  | loss: 1.25764 | val_0_rmse: 0.95887 | val_1_rmse: 0.94619 |  0:00:01s
epoch 2  | loss: 1.00369 | val_0_rmse: 0.84049 | val_1_rmse: 0.86059 |  0:00:01s
epoch 3  | loss: 0.82749 | val_0_rmse: 0.88378 | val_1_rmse: 0.92674 |  0:00:02s
epoch 4  | loss: 0.75285 | val_0_rmse: 0.82301 | val_1_rmse: 0.84017 |  0:00:03s
epoch 5  | loss: 0.7303  | val_0_rmse: 0.82347 | val_1_rmse: 0.83977 |  0:00:03s
epoch 6  | loss: 0.70314 | val_0_rmse: 0.83003 | val_1_rmse: 0.85722 |  0:00:04s
epoch 7  | loss: 0.67588 | val_0_rmse: 0.83254 | val_1_rmse: 0.85515 |  0:00:04s
epoch 8  | loss: 0.64534 | val_0_rmse: 0.83782 | val_1_rmse: 0.84563 |  0:00:05s
epoch 9  | loss: 0.62976 | val_0_rmse: 0.84618 | val_1_rmse: 0.86263 |  0:00:06s
epoch 10 | loss: 0.56613 | val_0_rmse: 0.84523 | val_1_rmse: 0.8522  |  0:00:06s
epoch 11 | loss: 0.53603 | val_0_rmse: 0.83904 | val_1_rmse: 0.85189 |  0:00:07s
epoch 12 | loss: 0.48037 | val_0_rmse: 0.84212 | val_1_rmse: 0.85509 |  0:00:08s
epoch 13 | loss: 0.4662  | val_0_rmse: 0.83242 | val_1_rmse: 0.84008 |  0:00:08s
epoch 14 | loss: 0.43792 | val_0_rmse: 0.88566 | val_1_rmse: 0.88829 |  0:00:09s
epoch 15 | loss: 0.44132 | val_0_rmse: 0.81242 | val_1_rmse: 0.82427 |  0:00:09s
epoch 16 | loss: 0.43144 | val_0_rmse: 0.79968 | val_1_rmse: 0.80399 |  0:00:10s
epoch 17 | loss: 0.41946 | val_0_rmse: 0.80163 | val_1_rmse: 0.79614 |  0:00:11s
epoch 18 | loss: 0.40239 | val_0_rmse: 0.79873 | val_1_rmse: 0.81775 |  0:00:11s
epoch 19 | loss: 0.39079 | val_0_rmse: 0.77937 | val_1_rmse: 0.79976 |  0:00:12s
epoch 20 | loss: 0.38071 | val_0_rmse: 0.79553 | val_1_rmse: 0.81691 |  0:00:13s
epoch 21 | loss: 0.37607 | val_0_rmse: 0.7892  | val_1_rmse: 0.82038 |  0:00:13s
epoch 22 | loss: 0.36329 | val_0_rmse: 0.77559 | val_1_rmse: 0.80266 |  0:00:14s
epoch 23 | loss: 0.34465 | val_0_rmse: 0.80483 | val_1_rmse: 0.82833 |  0:00:14s
epoch 24 | loss: 0.3254  | val_0_rmse: 0.76571 | val_1_rmse: 0.79032 |  0:00:15s
epoch 25 | loss: 0.35489 | val_0_rmse: 0.76706 | val_1_rmse: 0.78087 |  0:00:16s
epoch 26 | loss: 0.34311 | val_0_rmse: 0.80192 | val_1_rmse: 0.81601 |  0:00:16s
epoch 27 | loss: 0.33048 | val_0_rmse: 0.75701 | val_1_rmse: 0.79267 |  0:00:17s
epoch 28 | loss: 0.34401 | val_0_rmse: 0.77183 | val_1_rmse: 0.80407 |  0:00:18s
epoch 29 | loss: 0.32972 | val_0_rmse: 0.74357 | val_1_rmse: 0.78509 |  0:00:18s
epoch 30 | loss: 0.30862 | val_0_rmse: 0.73876 | val_1_rmse: 0.77796 |  0:00:19s
epoch 31 | loss: 0.2971  | val_0_rmse: 0.72961 | val_1_rmse: 0.76621 |  0:00:19s
epoch 32 | loss: 0.29004 | val_0_rmse: 0.72462 | val_1_rmse: 0.75954 |  0:00:20s
epoch 33 | loss: 0.30066 | val_0_rmse: 0.70727 | val_1_rmse: 0.73451 |  0:00:21s
epoch 34 | loss: 0.28363 | val_0_rmse: 0.6788  | val_1_rmse: 0.71488 |  0:00:21s
epoch 35 | loss: 0.28612 | val_0_rmse: 0.69451 | val_1_rmse: 0.72989 |  0:00:22s
epoch 36 | loss: 0.2772  | val_0_rmse: 0.68907 | val_1_rmse: 0.71142 |  0:00:23s
epoch 37 | loss: 0.27687 | val_0_rmse: 0.69971 | val_1_rmse: 0.727   |  0:00:23s
epoch 38 | loss: 0.27594 | val_0_rmse: 0.65642 | val_1_rmse: 0.69121 |  0:00:24s
epoch 39 | loss: 0.26757 | val_0_rmse: 0.65816 | val_1_rmse: 0.69343 |  0:00:24s
epoch 40 | loss: 0.26251 | val_0_rmse: 0.65049 | val_1_rmse: 0.67359 |  0:00:25s
epoch 41 | loss: 0.26522 | val_0_rmse: 0.64286 | val_1_rmse: 0.67217 |  0:00:26s
epoch 42 | loss: 0.26995 | val_0_rmse: 0.64063 | val_1_rmse: 0.67883 |  0:00:26s
epoch 43 | loss: 0.25018 | val_0_rmse: 0.61468 | val_1_rmse: 0.65378 |  0:00:27s
epoch 44 | loss: 0.25262 | val_0_rmse: 0.61789 | val_1_rmse: 0.65318 |  0:00:28s
epoch 45 | loss: 0.262   | val_0_rmse: 0.61748 | val_1_rmse: 0.65881 |  0:00:28s
epoch 46 | loss: 0.25786 | val_0_rmse: 0.60536 | val_1_rmse: 0.64416 |  0:00:29s
epoch 47 | loss: 0.25657 | val_0_rmse: 0.60663 | val_1_rmse: 0.65176 |  0:00:29s
epoch 48 | loss: 0.25085 | val_0_rmse: 0.59904 | val_1_rmse: 0.6362  |  0:00:30s
epoch 49 | loss: 0.25258 | val_0_rmse: 0.58914 | val_1_rmse: 0.633   |  0:00:31s
epoch 50 | loss: 0.24504 | val_0_rmse: 0.56804 | val_1_rmse: 0.60899 |  0:00:31s
epoch 51 | loss: 0.24961 | val_0_rmse: 0.5684  | val_1_rmse: 0.61159 |  0:00:32s
epoch 52 | loss: 0.24117 | val_0_rmse: 0.56434 | val_1_rmse: 0.60074 |  0:00:32s
epoch 53 | loss: 0.23507 | val_0_rmse: 0.56537 | val_1_rmse: 0.6051  |  0:00:33s
epoch 54 | loss: 0.23777 | val_0_rmse: 0.5785  | val_1_rmse: 0.62069 |  0:00:34s
epoch 55 | loss: 0.2387  | val_0_rmse: 0.55745 | val_1_rmse: 0.59777 |  0:00:34s
epoch 56 | loss: 0.23194 | val_0_rmse: 0.54769 | val_1_rmse: 0.58499 |  0:00:35s
epoch 57 | loss: 0.2257  | val_0_rmse: 0.5429  | val_1_rmse: 0.59171 |  0:00:36s
epoch 58 | loss: 0.22274 | val_0_rmse: 0.53447 | val_1_rmse: 0.57694 |  0:00:36s
epoch 59 | loss: 0.22304 | val_0_rmse: 0.53069 | val_1_rmse: 0.57417 |  0:00:37s
epoch 60 | loss: 0.21776 | val_0_rmse: 0.52821 | val_1_rmse: 0.57561 |  0:00:38s
epoch 61 | loss: 0.21994 | val_0_rmse: 0.52974 | val_1_rmse: 0.57674 |  0:00:38s
epoch 62 | loss: 0.21987 | val_0_rmse: 0.51711 | val_1_rmse: 0.56012 |  0:00:39s
epoch 63 | loss: 0.21384 | val_0_rmse: 0.50827 | val_1_rmse: 0.55143 |  0:00:39s
epoch 64 | loss: 0.218   | val_0_rmse: 0.52119 | val_1_rmse: 0.56664 |  0:00:40s
epoch 65 | loss: 0.22502 | val_0_rmse: 0.52139 | val_1_rmse: 0.57062 |  0:00:41s
epoch 66 | loss: 0.22894 | val_0_rmse: 0.50287 | val_1_rmse: 0.54832 |  0:00:41s
epoch 67 | loss: 0.23402 | val_0_rmse: 0.50444 | val_1_rmse: 0.54511 |  0:00:42s
epoch 68 | loss: 0.2253  | val_0_rmse: 0.49327 | val_1_rmse: 0.54856 |  0:00:42s
epoch 69 | loss: 0.22454 | val_0_rmse: 0.50135 | val_1_rmse: 0.54612 |  0:00:43s
epoch 70 | loss: 0.21885 | val_0_rmse: 0.49162 | val_1_rmse: 0.54269 |  0:00:44s
epoch 71 | loss: 0.21808 | val_0_rmse: 0.49109 | val_1_rmse: 0.5437  |  0:00:44s
epoch 72 | loss: 0.22138 | val_0_rmse: 0.49306 | val_1_rmse: 0.55213 |  0:00:45s
epoch 73 | loss: 0.21707 | val_0_rmse: 0.48058 | val_1_rmse: 0.52477 |  0:00:46s
epoch 74 | loss: 0.2132  | val_0_rmse: 0.46997 | val_1_rmse: 0.52327 |  0:00:46s
epoch 75 | loss: 0.21504 | val_0_rmse: 0.46672 | val_1_rmse: 0.51252 |  0:00:47s
epoch 76 | loss: 0.21353 | val_0_rmse: 0.46427 | val_1_rmse: 0.5207  |  0:00:47s
epoch 77 | loss: 0.20811 | val_0_rmse: 0.46932 | val_1_rmse: 0.53178 |  0:00:48s
epoch 78 | loss: 0.21136 | val_0_rmse: 0.46648 | val_1_rmse: 0.51537 |  0:00:49s
epoch 79 | loss: 0.21178 | val_0_rmse: 0.45574 | val_1_rmse: 0.51593 |  0:00:49s
epoch 80 | loss: 0.20084 | val_0_rmse: 0.45876 | val_1_rmse: 0.51436 |  0:00:50s
epoch 81 | loss: 0.20452 | val_0_rmse: 0.46386 | val_1_rmse: 0.51428 |  0:00:50s
epoch 82 | loss: 0.21262 | val_0_rmse: 0.45061 | val_1_rmse: 0.50508 |  0:00:51s
epoch 83 | loss: 0.20761 | val_0_rmse: 0.46031 | val_1_rmse: 0.51306 |  0:00:52s
epoch 84 | loss: 0.21044 | val_0_rmse: 0.44714 | val_1_rmse: 0.50671 |  0:00:52s
epoch 85 | loss: 0.20937 | val_0_rmse: 0.44901 | val_1_rmse: 0.5042  |  0:00:53s
epoch 86 | loss: 0.21428 | val_0_rmse: 0.44877 | val_1_rmse: 0.51169 |  0:00:54s
epoch 87 | loss: 0.20038 | val_0_rmse: 0.44003 | val_1_rmse: 0.49633 |  0:00:54s
epoch 88 | loss: 0.19959 | val_0_rmse: 0.43593 | val_1_rmse: 0.4991  |  0:00:55s
epoch 89 | loss: 0.19421 | val_0_rmse: 0.43277 | val_1_rmse: 0.49365 |  0:00:56s
epoch 90 | loss: 0.19943 | val_0_rmse: 0.43296 | val_1_rmse: 0.48972 |  0:00:56s
epoch 91 | loss: 0.19894 | val_0_rmse: 0.43328 | val_1_rmse: 0.49877 |  0:00:57s
epoch 92 | loss: 0.19761 | val_0_rmse: 0.42977 | val_1_rmse: 0.49116 |  0:00:57s
epoch 93 | loss: 0.19861 | val_0_rmse: 0.42427 | val_1_rmse: 0.49326 |  0:00:58s
epoch 94 | loss: 0.20012 | val_0_rmse: 0.44014 | val_1_rmse: 0.50583 |  0:00:59s
epoch 95 | loss: 0.20055 | val_0_rmse: 0.42501 | val_1_rmse: 0.49916 |  0:00:59s
epoch 96 | loss: 0.19389 | val_0_rmse: 0.44043 | val_1_rmse: 0.50288 |  0:01:00s
epoch 97 | loss: 0.20168 | val_0_rmse: 0.41881 | val_1_rmse: 0.49196 |  0:01:00s
epoch 98 | loss: 0.19114 | val_0_rmse: 0.43477 | val_1_rmse: 0.51527 |  0:01:01s
epoch 99 | loss: 0.19478 | val_0_rmse: 0.423   | val_1_rmse: 0.49087 |  0:01:02s
epoch 100| loss: 0.19316 | val_0_rmse: 0.41494 | val_1_rmse: 0.49329 |  0:01:02s
epoch 101| loss: 0.19025 | val_0_rmse: 0.41393 | val_1_rmse: 0.49062 |  0:01:03s
epoch 102| loss: 0.18683 | val_0_rmse: 0.41397 | val_1_rmse: 0.48306 |  0:01:03s
epoch 103| loss: 0.19119 | val_0_rmse: 0.41679 | val_1_rmse: 0.49758 |  0:01:04s
epoch 104| loss: 0.18881 | val_0_rmse: 0.40765 | val_1_rmse: 0.48851 |  0:01:05s
epoch 105| loss: 0.18157 | val_0_rmse: 0.40886 | val_1_rmse: 0.49155 |  0:01:05s
epoch 106| loss: 0.18148 | val_0_rmse: 0.40639 | val_1_rmse: 0.48915 |  0:01:06s
epoch 107| loss: 0.18642 | val_0_rmse: 0.41689 | val_1_rmse: 0.49907 |  0:01:07s
epoch 108| loss: 0.18465 | val_0_rmse: 0.40843 | val_1_rmse: 0.49598 |  0:01:07s
epoch 109| loss: 0.18551 | val_0_rmse: 0.40315 | val_1_rmse: 0.49572 |  0:01:08s
epoch 110| loss: 0.17968 | val_0_rmse: 0.40575 | val_1_rmse: 0.49353 |  0:01:08s
epoch 111| loss: 0.18116 | val_0_rmse: 0.40092 | val_1_rmse: 0.50183 |  0:01:09s
epoch 112| loss: 0.17525 | val_0_rmse: 0.39915 | val_1_rmse: 0.49586 |  0:01:10s
epoch 113| loss: 0.18531 | val_0_rmse: 0.39664 | val_1_rmse: 0.49157 |  0:01:10s
epoch 114| loss: 0.17798 | val_0_rmse: 0.39769 | val_1_rmse: 0.49371 |  0:01:11s
epoch 115| loss: 0.17809 | val_0_rmse: 0.39792 | val_1_rmse: 0.49202 |  0:01:11s
epoch 116| loss: 0.1739  | val_0_rmse: 0.39705 | val_1_rmse: 0.49512 |  0:01:12s
epoch 117| loss: 0.17691 | val_0_rmse: 0.39622 | val_1_rmse: 0.49089 |  0:01:13s
epoch 118| loss: 0.17273 | val_0_rmse: 0.39464 | val_1_rmse: 0.50152 |  0:01:13s
epoch 119| loss: 0.1779  | val_0_rmse: 0.40423 | val_1_rmse: 0.50048 |  0:01:14s
epoch 120| loss: 0.17479 | val_0_rmse: 0.3931  | val_1_rmse: 0.50005 |  0:01:15s
epoch 121| loss: 0.17459 | val_0_rmse: 0.38982 | val_1_rmse: 0.49882 |  0:01:15s
epoch 122| loss: 0.1764  | val_0_rmse: 0.39119 | val_1_rmse: 0.49613 |  0:01:16s
epoch 123| loss: 0.16716 | val_0_rmse: 0.3886  | val_1_rmse: 0.50909 |  0:01:16s
epoch 124| loss: 0.17724 | val_0_rmse: 0.38494 | val_1_rmse: 0.49565 |  0:01:17s
epoch 125| loss: 0.17012 | val_0_rmse: 0.38367 | val_1_rmse: 0.4951  |  0:01:18s
epoch 126| loss: 0.16464 | val_0_rmse: 0.38424 | val_1_rmse: 0.49566 |  0:01:18s
epoch 127| loss: 0.16843 | val_0_rmse: 0.38171 | val_1_rmse: 0.49737 |  0:01:19s
epoch 128| loss: 0.16732 | val_0_rmse: 0.38217 | val_1_rmse: 0.4943  |  0:01:20s
epoch 129| loss: 0.16644 | val_0_rmse: 0.38385 | val_1_rmse: 0.49515 |  0:01:20s
epoch 130| loss: 0.17165 | val_0_rmse: 0.38691 | val_1_rmse: 0.50632 |  0:01:21s
epoch 131| loss: 0.16185 | val_0_rmse: 0.37872 | val_1_rmse: 0.49674 |  0:01:21s
epoch 132| loss: 0.16424 | val_0_rmse: 0.37527 | val_1_rmse: 0.50394 |  0:01:22s

Early stopping occured at epoch 132 with best_epoch = 102 and best_val_1_rmse = 0.48306
Best weights from best epoch are automatically used!
ended training at: 17:26:02
Feature importance:
Mean squared error is of 20692640983.824413
Mean absolute error:103747.04809108832
MAPE:0.1819687749132315
R2 score:0.736846906203056
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:26:03
epoch 0  | loss: 3.35978 | val_0_rmse: 1.15696 | val_1_rmse: 1.1804  |  0:00:00s
epoch 1  | loss: 2.82662 | val_0_rmse: 1.02759 | val_1_rmse: 1.04786 |  0:00:00s
epoch 2  | loss: 1.72793 | val_0_rmse: 0.99755 | val_1_rmse: 1.01672 |  0:00:00s
epoch 3  | loss: 1.57258 | val_0_rmse: 1.00285 | val_1_rmse: 1.02076 |  0:00:00s
epoch 4  | loss: 1.19151 | val_0_rmse: 1.00496 | val_1_rmse: 1.02305 |  0:00:00s
epoch 5  | loss: 1.11491 | val_0_rmse: 1.00697 | val_1_rmse: 1.02622 |  0:00:00s
epoch 6  | loss: 1.09247 | val_0_rmse: 1.00789 | val_1_rmse: 1.02621 |  0:00:01s
epoch 7  | loss: 1.07648 | val_0_rmse: 1.01077 | val_1_rmse: 1.02876 |  0:00:01s
epoch 8  | loss: 1.04322 | val_0_rmse: 1.01048 | val_1_rmse: 1.02826 |  0:00:01s
epoch 9  | loss: 1.03767 | val_0_rmse: 1.00667 | val_1_rmse: 1.02316 |  0:00:01s
epoch 10 | loss: 0.98053 | val_0_rmse: 0.99514 | val_1_rmse: 1.01145 |  0:00:01s
epoch 11 | loss: 0.95624 | val_0_rmse: 0.98045 | val_1_rmse: 0.99716 |  0:00:01s
epoch 12 | loss: 0.91869 | val_0_rmse: 0.97207 | val_1_rmse: 0.99159 |  0:00:02s
epoch 13 | loss: 0.86308 | val_0_rmse: 0.96837 | val_1_rmse: 1.0023  |  0:00:02s
epoch 14 | loss: 0.83798 | val_0_rmse: 0.85657 | val_1_rmse: 0.90465 |  0:00:02s
epoch 15 | loss: 0.77226 | val_0_rmse: 0.80976 | val_1_rmse: 0.86563 |  0:00:02s
epoch 16 | loss: 0.71013 | val_0_rmse: 0.91579 | val_1_rmse: 0.97492 |  0:00:02s
epoch 17 | loss: 0.64656 | val_0_rmse: 0.91023 | val_1_rmse: 0.97775 |  0:00:02s
epoch 18 | loss: 0.61578 | val_0_rmse: 0.96095 | val_1_rmse: 1.02202 |  0:00:03s
epoch 19 | loss: 0.59109 | val_0_rmse: 1.02058 | val_1_rmse: 1.07591 |  0:00:03s
epoch 20 | loss: 0.54882 | val_0_rmse: 0.86922 | val_1_rmse: 0.92601 |  0:00:03s
epoch 21 | loss: 0.55289 | val_0_rmse: 0.84689 | val_1_rmse: 0.91279 |  0:00:03s
epoch 22 | loss: 0.47941 | val_0_rmse: 0.95848 | val_1_rmse: 1.04039 |  0:00:03s
epoch 23 | loss: 0.48101 | val_0_rmse: 0.88734 | val_1_rmse: 0.95044 |  0:00:03s
epoch 24 | loss: 0.45136 | val_0_rmse: 0.79457 | val_1_rmse: 0.85494 |  0:00:03s
epoch 25 | loss: 0.46797 | val_0_rmse: 0.84974 | val_1_rmse: 0.90078 |  0:00:04s
epoch 26 | loss: 0.42656 | val_0_rmse: 0.96155 | val_1_rmse: 1.01208 |  0:00:04s
epoch 27 | loss: 0.41355 | val_0_rmse: 0.79799 | val_1_rmse: 0.86118 |  0:00:04s
epoch 28 | loss: 0.38219 | val_0_rmse: 0.7197  | val_1_rmse: 0.78159 |  0:00:04s
epoch 29 | loss: 0.38461 | val_0_rmse: 0.71941 | val_1_rmse: 0.78565 |  0:00:04s
epoch 30 | loss: 0.36373 | val_0_rmse: 0.73185 | val_1_rmse: 0.79833 |  0:00:04s
epoch 31 | loss: 0.35221 | val_0_rmse: 0.72578 | val_1_rmse: 0.79076 |  0:00:05s
epoch 32 | loss: 0.33648 | val_0_rmse: 0.71964 | val_1_rmse: 0.78775 |  0:00:05s
epoch 33 | loss: 0.33443 | val_0_rmse: 0.70797 | val_1_rmse: 0.77547 |  0:00:05s
epoch 34 | loss: 0.32297 | val_0_rmse: 0.7129  | val_1_rmse: 0.78097 |  0:00:05s
epoch 35 | loss: 0.32893 | val_0_rmse: 0.70464 | val_1_rmse: 0.7772  |  0:00:05s
epoch 36 | loss: 0.32622 | val_0_rmse: 0.69303 | val_1_rmse: 0.76271 |  0:00:05s
epoch 37 | loss: 0.303   | val_0_rmse: 0.68861 | val_1_rmse: 0.75207 |  0:00:06s
epoch 38 | loss: 0.30472 | val_0_rmse: 0.70137 | val_1_rmse: 0.77082 |  0:00:06s
epoch 39 | loss: 0.30224 | val_0_rmse: 0.72715 | val_1_rmse: 0.80216 |  0:00:06s
epoch 40 | loss: 0.30474 | val_0_rmse: 0.71881 | val_1_rmse: 0.78699 |  0:00:06s
epoch 41 | loss: 0.29712 | val_0_rmse: 0.7197  | val_1_rmse: 0.78496 |  0:00:06s
epoch 42 | loss: 0.29416 | val_0_rmse: 0.70116 | val_1_rmse: 0.77894 |  0:00:07s
epoch 43 | loss: 0.28177 | val_0_rmse: 0.69497 | val_1_rmse: 0.77544 |  0:00:07s
epoch 44 | loss: 0.27522 | val_0_rmse: 0.67833 | val_1_rmse: 0.73945 |  0:00:07s
epoch 45 | loss: 0.27521 | val_0_rmse: 0.68594 | val_1_rmse: 0.74692 |  0:00:07s
epoch 46 | loss: 0.28543 | val_0_rmse: 0.69794 | val_1_rmse: 0.77535 |  0:00:07s
epoch 47 | loss: 0.27894 | val_0_rmse: 0.70087 | val_1_rmse: 0.78097 |  0:00:07s
epoch 48 | loss: 0.28029 | val_0_rmse: 0.68236 | val_1_rmse: 0.74568 |  0:00:07s
epoch 49 | loss: 0.26651 | val_0_rmse: 0.68352 | val_1_rmse: 0.74883 |  0:00:08s
epoch 50 | loss: 0.26165 | val_0_rmse: 0.67566 | val_1_rmse: 0.74741 |  0:00:08s
epoch 51 | loss: 0.25762 | val_0_rmse: 0.68201 | val_1_rmse: 0.7503  |  0:00:08s
epoch 52 | loss: 0.25346 | val_0_rmse: 0.69484 | val_1_rmse: 0.75565 |  0:00:08s
epoch 53 | loss: 0.25924 | val_0_rmse: 0.69658 | val_1_rmse: 0.75861 |  0:00:08s
epoch 54 | loss: 0.24425 | val_0_rmse: 0.68661 | val_1_rmse: 0.75532 |  0:00:08s
epoch 55 | loss: 0.23688 | val_0_rmse: 0.68212 | val_1_rmse: 0.76233 |  0:00:09s
epoch 56 | loss: 0.23596 | val_0_rmse: 0.68255 | val_1_rmse: 0.76147 |  0:00:09s
epoch 57 | loss: 0.2291  | val_0_rmse: 0.6767  | val_1_rmse: 0.75016 |  0:00:09s
epoch 58 | loss: 0.23318 | val_0_rmse: 0.67193 | val_1_rmse: 0.74522 |  0:00:09s
epoch 59 | loss: 0.23409 | val_0_rmse: 0.67891 | val_1_rmse: 0.74464 |  0:00:09s
epoch 60 | loss: 0.24269 | val_0_rmse: 0.68372 | val_1_rmse: 0.73607 |  0:00:09s
epoch 61 | loss: 0.23676 | val_0_rmse: 0.69626 | val_1_rmse: 0.73961 |  0:00:10s
epoch 62 | loss: 0.2399  | val_0_rmse: 0.69267 | val_1_rmse: 0.74161 |  0:00:10s
epoch 63 | loss: 0.23763 | val_0_rmse: 0.66771 | val_1_rmse: 0.7291  |  0:00:10s
epoch 64 | loss: 0.23247 | val_0_rmse: 0.65421 | val_1_rmse: 0.72089 |  0:00:10s
epoch 65 | loss: 0.2388  | val_0_rmse: 0.66272 | val_1_rmse: 0.73383 |  0:00:10s
epoch 66 | loss: 0.22692 | val_0_rmse: 0.66954 | val_1_rmse: 0.73934 |  0:00:10s
epoch 67 | loss: 0.22745 | val_0_rmse: 0.66242 | val_1_rmse: 0.72664 |  0:00:10s
epoch 68 | loss: 0.21924 | val_0_rmse: 0.66389 | val_1_rmse: 0.72858 |  0:00:11s
epoch 69 | loss: 0.21965 | val_0_rmse: 0.66301 | val_1_rmse: 0.72576 |  0:00:11s
epoch 70 | loss: 0.22272 | val_0_rmse: 0.64669 | val_1_rmse: 0.71212 |  0:00:11s
epoch 71 | loss: 0.221   | val_0_rmse: 0.63444 | val_1_rmse: 0.70623 |  0:00:11s
epoch 72 | loss: 0.21209 | val_0_rmse: 0.63607 | val_1_rmse: 0.71031 |  0:00:11s
epoch 73 | loss: 0.20053 | val_0_rmse: 0.63968 | val_1_rmse: 0.71357 |  0:00:11s
epoch 74 | loss: 0.20881 | val_0_rmse: 0.64626 | val_1_rmse: 0.71727 |  0:00:12s
epoch 75 | loss: 0.20625 | val_0_rmse: 0.64698 | val_1_rmse: 0.72212 |  0:00:12s
epoch 76 | loss: 0.20435 | val_0_rmse: 0.64908 | val_1_rmse: 0.72431 |  0:00:12s
epoch 77 | loss: 0.20359 | val_0_rmse: 0.65003 | val_1_rmse: 0.72309 |  0:00:12s
epoch 78 | loss: 0.19573 | val_0_rmse: 0.65702 | val_1_rmse: 0.73319 |  0:00:12s
epoch 79 | loss: 0.2131  | val_0_rmse: 0.66555 | val_1_rmse: 0.73975 |  0:00:12s
epoch 80 | loss: 0.19016 | val_0_rmse: 0.67015 | val_1_rmse: 0.7415  |  0:00:13s
epoch 81 | loss: 0.19267 | val_0_rmse: 0.64275 | val_1_rmse: 0.72138 |  0:00:13s
epoch 82 | loss: 0.18991 | val_0_rmse: 0.63839 | val_1_rmse: 0.72436 |  0:00:13s
epoch 83 | loss: 0.18911 | val_0_rmse: 0.66444 | val_1_rmse: 0.74686 |  0:00:13s
epoch 84 | loss: 0.18653 | val_0_rmse: 0.6606  | val_1_rmse: 0.73257 |  0:00:13s
epoch 85 | loss: 0.18734 | val_0_rmse: 0.65037 | val_1_rmse: 0.71944 |  0:00:13s
epoch 86 | loss: 0.17687 | val_0_rmse: 0.6391  | val_1_rmse: 0.71544 |  0:00:13s
epoch 87 | loss: 0.17046 | val_0_rmse: 0.64966 | val_1_rmse: 0.7221  |  0:00:14s
epoch 88 | loss: 0.19892 | val_0_rmse: 0.64365 | val_1_rmse: 0.71209 |  0:00:14s
epoch 89 | loss: 0.17979 | val_0_rmse: 0.62737 | val_1_rmse: 0.70085 |  0:00:14s
epoch 90 | loss: 0.17496 | val_0_rmse: 0.62835 | val_1_rmse: 0.71128 |  0:00:14s
epoch 91 | loss: 0.17046 | val_0_rmse: 0.62941 | val_1_rmse: 0.71357 |  0:00:14s
epoch 92 | loss: 0.17661 | val_0_rmse: 0.62636 | val_1_rmse: 0.7026  |  0:00:14s
epoch 93 | loss: 0.17896 | val_0_rmse: 0.64008 | val_1_rmse: 0.71136 |  0:00:15s
epoch 94 | loss: 0.17027 | val_0_rmse: 0.64597 | val_1_rmse: 0.7186  |  0:00:15s
epoch 95 | loss: 0.17543 | val_0_rmse: 0.63968 | val_1_rmse: 0.71503 |  0:00:15s
epoch 96 | loss: 0.18051 | val_0_rmse: 0.63421 | val_1_rmse: 0.71192 |  0:00:15s
epoch 97 | loss: 0.17436 | val_0_rmse: 0.63132 | val_1_rmse: 0.71348 |  0:00:15s
epoch 98 | loss: 0.17098 | val_0_rmse: 0.62309 | val_1_rmse: 0.70402 |  0:00:15s
epoch 99 | loss: 0.16805 | val_0_rmse: 0.61642 | val_1_rmse: 0.70067 |  0:00:16s
epoch 100| loss: 0.16932 | val_0_rmse: 0.62447 | val_1_rmse: 0.70901 |  0:00:16s
epoch 101| loss: 0.17069 | val_0_rmse: 0.62236 | val_1_rmse: 0.70724 |  0:00:16s
epoch 102| loss: 0.17507 | val_0_rmse: 0.62019 | val_1_rmse: 0.70946 |  0:00:16s
epoch 103| loss: 0.17762 | val_0_rmse: 0.62696 | val_1_rmse: 0.71569 |  0:00:16s
epoch 104| loss: 0.15731 | val_0_rmse: 0.62115 | val_1_rmse: 0.70688 |  0:00:16s
epoch 105| loss: 0.15185 | val_0_rmse: 0.61956 | val_1_rmse: 0.70326 |  0:00:17s
epoch 106| loss: 0.16235 | val_0_rmse: 0.61511 | val_1_rmse: 0.70165 |  0:00:17s
epoch 107| loss: 0.16107 | val_0_rmse: 0.62029 | val_1_rmse: 0.70726 |  0:00:17s
epoch 108| loss: 0.16811 | val_0_rmse: 0.6409  | val_1_rmse: 0.72714 |  0:00:17s
epoch 109| loss: 0.16036 | val_0_rmse: 0.64974 | val_1_rmse: 0.7339  |  0:00:17s
epoch 110| loss: 0.15636 | val_0_rmse: 0.62383 | val_1_rmse: 0.71637 |  0:00:17s
epoch 111| loss: 0.1467  | val_0_rmse: 0.61313 | val_1_rmse: 0.71378 |  0:00:17s
epoch 112| loss: 0.16533 | val_0_rmse: 0.61578 | val_1_rmse: 0.71027 |  0:00:18s
epoch 113| loss: 0.15748 | val_0_rmse: 0.62186 | val_1_rmse: 0.71329 |  0:00:18s
epoch 114| loss: 0.15482 | val_0_rmse: 0.62582 | val_1_rmse: 0.72114 |  0:00:18s
epoch 115| loss: 0.14368 | val_0_rmse: 0.61388 | val_1_rmse: 0.71619 |  0:00:18s
epoch 116| loss: 0.14234 | val_0_rmse: 0.61435 | val_1_rmse: 0.7164  |  0:00:18s
epoch 117| loss: 0.15322 | val_0_rmse: 0.61474 | val_1_rmse: 0.71152 |  0:00:18s
epoch 118| loss: 0.14733 | val_0_rmse: 0.60792 | val_1_rmse: 0.7114  |  0:00:19s
epoch 119| loss: 0.14683 | val_0_rmse: 0.60494 | val_1_rmse: 0.71655 |  0:00:19s
epoch 120| loss: 0.14846 | val_0_rmse: 0.60201 | val_1_rmse: 0.70764 |  0:00:19s
epoch 121| loss: 0.14322 | val_0_rmse: 0.60112 | val_1_rmse: 0.70743 |  0:00:19s
epoch 122| loss: 0.14205 | val_0_rmse: 0.59148 | val_1_rmse: 0.70865 |  0:00:19s
epoch 123| loss: 0.15    | val_0_rmse: 0.58405 | val_1_rmse: 0.70642 |  0:00:19s
epoch 124| loss: 0.15186 | val_0_rmse: 0.58749 | val_1_rmse: 0.70262 |  0:00:20s
epoch 125| loss: 0.14561 | val_0_rmse: 0.59319 | val_1_rmse: 0.7031  |  0:00:20s
epoch 126| loss: 0.14894 | val_0_rmse: 0.59287 | val_1_rmse: 0.70499 |  0:00:20s
epoch 127| loss: 0.14272 | val_0_rmse: 0.58798 | val_1_rmse: 0.69888 |  0:00:20s
epoch 128| loss: 0.14367 | val_0_rmse: 0.58046 | val_1_rmse: 0.69465 |  0:00:20s
epoch 129| loss: 0.14898 | val_0_rmse: 0.58348 | val_1_rmse: 0.69407 |  0:00:20s
epoch 130| loss: 0.14213 | val_0_rmse: 0.59948 | val_1_rmse: 0.70456 |  0:00:20s
epoch 131| loss: 0.14527 | val_0_rmse: 0.61464 | val_1_rmse: 0.71956 |  0:00:21s
epoch 132| loss: 0.14491 | val_0_rmse: 0.59852 | val_1_rmse: 0.71015 |  0:00:21s
epoch 133| loss: 0.14243 | val_0_rmse: 0.5854  | val_1_rmse: 0.71234 |  0:00:21s
epoch 134| loss: 0.14995 | val_0_rmse: 0.59927 | val_1_rmse: 0.72185 |  0:00:21s
epoch 135| loss: 0.14802 | val_0_rmse: 0.59926 | val_1_rmse: 0.71647 |  0:00:21s
epoch 136| loss: 0.1447  | val_0_rmse: 0.57617 | val_1_rmse: 0.71359 |  0:00:21s
epoch 137| loss: 0.14564 | val_0_rmse: 0.57512 | val_1_rmse: 0.71957 |  0:00:22s
epoch 138| loss: 0.13372 | val_0_rmse: 0.58806 | val_1_rmse: 0.7121  |  0:00:22s
epoch 139| loss: 0.13544 | val_0_rmse: 0.59764 | val_1_rmse: 0.71111 |  0:00:22s
epoch 140| loss: 0.13875 | val_0_rmse: 0.57703 | val_1_rmse: 0.71015 |  0:00:22s
epoch 141| loss: 0.13803 | val_0_rmse: 0.56573 | val_1_rmse: 0.71807 |  0:00:22s
epoch 142| loss: 0.13334 | val_0_rmse: 0.58003 | val_1_rmse: 0.70734 |  0:00:22s
epoch 143| loss: 0.13269 | val_0_rmse: 0.59288 | val_1_rmse: 0.70625 |  0:00:23s
epoch 144| loss: 0.1452  | val_0_rmse: 0.5573  | val_1_rmse: 0.69887 |  0:00:23s
epoch 145| loss: 0.13356 | val_0_rmse: 0.55402 | val_1_rmse: 0.71399 |  0:00:23s
epoch 146| loss: 0.12409 | val_0_rmse: 0.55773 | val_1_rmse: 0.70412 |  0:00:23s
epoch 147| loss: 0.13468 | val_0_rmse: 0.56689 | val_1_rmse: 0.70197 |  0:00:23s
epoch 148| loss: 0.13002 | val_0_rmse: 0.548   | val_1_rmse: 0.70171 |  0:00:23s
epoch 149| loss: 0.13373 | val_0_rmse: 0.53722 | val_1_rmse: 0.70899 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.69407
Best weights from best epoch are automatically used!
ended training at: 17:26:27
Feature importance:
Mean squared error is of 2791113692.30017
Mean absolute error:37946.74654839853
MAPE:0.34077110626634555
R2 score:0.5708364824627871
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:26:27
epoch 0  | loss: 4.77836 | val_0_rmse: 1.05507 | val_1_rmse: 1.05579 |  0:00:00s
epoch 1  | loss: 2.68002 | val_0_rmse: 1.01925 | val_1_rmse: 1.01509 |  0:00:00s
epoch 2  | loss: 2.18641 | val_0_rmse: 1.01986 | val_1_rmse: 1.00962 |  0:00:00s
epoch 3  | loss: 1.49821 | val_0_rmse: 1.02408 | val_1_rmse: 1.01235 |  0:00:00s
epoch 4  | loss: 1.26886 | val_0_rmse: 1.02033 | val_1_rmse: 1.01075 |  0:00:00s
epoch 5  | loss: 1.11417 | val_0_rmse: 1.00705 | val_1_rmse: 1.00311 |  0:00:00s
epoch 6  | loss: 1.07769 | val_0_rmse: 1.00185 | val_1_rmse: 0.99904 |  0:00:01s
epoch 7  | loss: 1.00403 | val_0_rmse: 1.00444 | val_1_rmse: 1.00794 |  0:00:01s
epoch 8  | loss: 0.98616 | val_0_rmse: 1.00182 | val_1_rmse: 1.00196 |  0:00:01s
epoch 9  | loss: 0.94174 | val_0_rmse: 0.999   | val_1_rmse: 0.99443 |  0:00:01s
epoch 10 | loss: 0.94028 | val_0_rmse: 0.98465 | val_1_rmse: 0.97745 |  0:00:01s
epoch 11 | loss: 0.93628 | val_0_rmse: 0.97955 | val_1_rmse: 0.96514 |  0:00:02s
epoch 12 | loss: 0.89817 | val_0_rmse: 0.96757 | val_1_rmse: 0.95474 |  0:00:02s
epoch 13 | loss: 0.85933 | val_0_rmse: 0.94711 | val_1_rmse: 0.93363 |  0:00:02s
epoch 14 | loss: 0.82419 | val_0_rmse: 0.92286 | val_1_rmse: 0.92277 |  0:00:02s
epoch 15 | loss: 0.79124 | val_0_rmse: 0.92819 | val_1_rmse: 0.93587 |  0:00:02s
epoch 16 | loss: 0.73339 | val_0_rmse: 0.94691 | val_1_rmse: 0.95711 |  0:00:02s
epoch 17 | loss: 0.70071 | val_0_rmse: 0.90302 | val_1_rmse: 0.92174 |  0:00:03s
epoch 18 | loss: 0.66817 | val_0_rmse: 0.88424 | val_1_rmse: 0.89238 |  0:00:03s
epoch 19 | loss: 0.6326  | val_0_rmse: 0.88376 | val_1_rmse: 0.91178 |  0:00:03s
epoch 20 | loss: 0.57148 | val_0_rmse: 0.88611 | val_1_rmse: 0.9062  |  0:00:03s
epoch 21 | loss: 0.55667 | val_0_rmse: 0.86387 | val_1_rmse: 0.87091 |  0:00:03s
epoch 22 | loss: 0.53666 | val_0_rmse: 0.82285 | val_1_rmse: 0.82924 |  0:00:03s
epoch 23 | loss: 0.54039 | val_0_rmse: 0.78762 | val_1_rmse: 0.77049 |  0:00:04s
epoch 24 | loss: 0.51635 | val_0_rmse: 0.75637 | val_1_rmse: 0.74763 |  0:00:04s
epoch 25 | loss: 0.47949 | val_0_rmse: 0.73803 | val_1_rmse: 0.7372  |  0:00:04s
epoch 26 | loss: 0.47453 | val_0_rmse: 0.73532 | val_1_rmse: 0.73425 |  0:00:04s
epoch 27 | loss: 0.46528 | val_0_rmse: 0.7277  | val_1_rmse: 0.74289 |  0:00:04s
epoch 28 | loss: 0.42464 | val_0_rmse: 0.72501 | val_1_rmse: 0.73992 |  0:00:04s
epoch 29 | loss: 0.42507 | val_0_rmse: 0.72694 | val_1_rmse: 0.74334 |  0:00:05s
epoch 30 | loss: 0.41305 | val_0_rmse: 0.73293 | val_1_rmse: 0.74003 |  0:00:05s
epoch 31 | loss: 0.39194 | val_0_rmse: 0.75124 | val_1_rmse: 0.75821 |  0:00:05s
epoch 32 | loss: 0.40242 | val_0_rmse: 0.7531  | val_1_rmse: 0.76184 |  0:00:05s
epoch 33 | loss: 0.37821 | val_0_rmse: 0.7669  | val_1_rmse: 0.77778 |  0:00:05s
epoch 34 | loss: 0.40819 | val_0_rmse: 0.78052 | val_1_rmse: 0.79785 |  0:00:05s
epoch 35 | loss: 0.39129 | val_0_rmse: 0.75527 | val_1_rmse: 0.75407 |  0:00:05s
epoch 36 | loss: 0.36576 | val_0_rmse: 0.75355 | val_1_rmse: 0.73844 |  0:00:06s
epoch 37 | loss: 0.3541  | val_0_rmse: 0.75419 | val_1_rmse: 0.74242 |  0:00:06s
epoch 38 | loss: 0.36043 | val_0_rmse: 0.76681 | val_1_rmse: 0.75891 |  0:00:06s
epoch 39 | loss: 0.34538 | val_0_rmse: 0.75701 | val_1_rmse: 0.75278 |  0:00:06s
epoch 40 | loss: 0.35398 | val_0_rmse: 0.72702 | val_1_rmse: 0.7286  |  0:00:06s
epoch 41 | loss: 0.36438 | val_0_rmse: 0.71559 | val_1_rmse: 0.71668 |  0:00:06s
epoch 42 | loss: 0.34342 | val_0_rmse: 0.72115 | val_1_rmse: 0.72285 |  0:00:07s
epoch 43 | loss: 0.32263 | val_0_rmse: 0.71685 | val_1_rmse: 0.73026 |  0:00:07s
epoch 44 | loss: 0.33785 | val_0_rmse: 0.70834 | val_1_rmse: 0.7324  |  0:00:07s
epoch 45 | loss: 0.33901 | val_0_rmse: 0.69666 | val_1_rmse: 0.70964 |  0:00:07s
epoch 46 | loss: 0.36096 | val_0_rmse: 0.68821 | val_1_rmse: 0.70341 |  0:00:07s
epoch 47 | loss: 0.33928 | val_0_rmse: 0.68672 | val_1_rmse: 0.71368 |  0:00:07s
epoch 48 | loss: 0.32441 | val_0_rmse: 0.69579 | val_1_rmse: 0.7213  |  0:00:08s
epoch 49 | loss: 0.34978 | val_0_rmse: 0.69748 | val_1_rmse: 0.71631 |  0:00:08s
epoch 50 | loss: 0.3291  | val_0_rmse: 0.68403 | val_1_rmse: 0.69548 |  0:00:08s
epoch 51 | loss: 0.32269 | val_0_rmse: 0.69964 | val_1_rmse: 0.71219 |  0:00:08s
epoch 52 | loss: 0.31563 | val_0_rmse: 0.69963 | val_1_rmse: 0.70684 |  0:00:08s
epoch 53 | loss: 0.33827 | val_0_rmse: 0.69408 | val_1_rmse: 0.69897 |  0:00:08s
epoch 54 | loss: 0.30917 | val_0_rmse: 0.68586 | val_1_rmse: 0.69006 |  0:00:09s
epoch 55 | loss: 0.31401 | val_0_rmse: 0.68461 | val_1_rmse: 0.68647 |  0:00:09s
epoch 56 | loss: 0.29583 | val_0_rmse: 0.69378 | val_1_rmse: 0.69651 |  0:00:09s
epoch 57 | loss: 0.30253 | val_0_rmse: 0.68388 | val_1_rmse: 0.6903  |  0:00:09s
epoch 58 | loss: 0.29145 | val_0_rmse: 0.67397 | val_1_rmse: 0.68723 |  0:00:09s
epoch 59 | loss: 0.30439 | val_0_rmse: 0.66858 | val_1_rmse: 0.6847  |  0:00:09s
epoch 60 | loss: 0.28733 | val_0_rmse: 0.68579 | val_1_rmse: 0.701   |  0:00:09s
epoch 61 | loss: 0.28635 | val_0_rmse: 0.66873 | val_1_rmse: 0.68156 |  0:00:10s
epoch 62 | loss: 0.29399 | val_0_rmse: 0.65613 | val_1_rmse: 0.66954 |  0:00:10s
epoch 63 | loss: 0.2793  | val_0_rmse: 0.65928 | val_1_rmse: 0.66973 |  0:00:10s
epoch 64 | loss: 0.2864  | val_0_rmse: 0.68469 | val_1_rmse: 0.69368 |  0:00:10s
epoch 65 | loss: 0.27212 | val_0_rmse: 0.68152 | val_1_rmse: 0.6902  |  0:00:10s
epoch 66 | loss: 0.27407 | val_0_rmse: 0.66724 | val_1_rmse: 0.6776  |  0:00:10s
epoch 67 | loss: 0.27062 | val_0_rmse: 0.66167 | val_1_rmse: 0.67559 |  0:00:11s
epoch 68 | loss: 0.27586 | val_0_rmse: 0.66167 | val_1_rmse: 0.68026 |  0:00:11s
epoch 69 | loss: 0.26682 | val_0_rmse: 0.66844 | val_1_rmse: 0.69346 |  0:00:11s
epoch 70 | loss: 0.27768 | val_0_rmse: 0.66361 | val_1_rmse: 0.6863  |  0:00:11s
epoch 71 | loss: 0.27961 | val_0_rmse: 0.65929 | val_1_rmse: 0.67745 |  0:00:11s
epoch 72 | loss: 0.26384 | val_0_rmse: 0.65995 | val_1_rmse: 0.67493 |  0:00:11s
epoch 73 | loss: 0.26335 | val_0_rmse: 0.65603 | val_1_rmse: 0.67312 |  0:00:12s
epoch 74 | loss: 0.2609  | val_0_rmse: 0.65317 | val_1_rmse: 0.67266 |  0:00:12s
epoch 75 | loss: 0.25938 | val_0_rmse: 0.6504  | val_1_rmse: 0.67235 |  0:00:12s
epoch 76 | loss: 0.26523 | val_0_rmse: 0.64626 | val_1_rmse: 0.66714 |  0:00:12s
epoch 77 | loss: 0.26487 | val_0_rmse: 0.64637 | val_1_rmse: 0.66532 |  0:00:12s
epoch 78 | loss: 0.25391 | val_0_rmse: 0.66683 | val_1_rmse: 0.67988 |  0:00:12s
epoch 79 | loss: 0.25682 | val_0_rmse: 0.67731 | val_1_rmse: 0.68819 |  0:00:13s
epoch 80 | loss: 0.26532 | val_0_rmse: 0.66355 | val_1_rmse: 0.67731 |  0:00:13s
epoch 81 | loss: 0.25635 | val_0_rmse: 0.65866 | val_1_rmse: 0.67242 |  0:00:13s
epoch 82 | loss: 0.25121 | val_0_rmse: 0.65926 | val_1_rmse: 0.67712 |  0:00:13s
epoch 83 | loss: 0.25844 | val_0_rmse: 0.66019 | val_1_rmse: 0.67376 |  0:00:13s
epoch 84 | loss: 0.23869 | val_0_rmse: 0.66099 | val_1_rmse: 0.66887 |  0:00:13s
epoch 85 | loss: 0.24983 | val_0_rmse: 0.65952 | val_1_rmse: 0.66509 |  0:00:13s
epoch 86 | loss: 0.25403 | val_0_rmse: 0.65876 | val_1_rmse: 0.66326 |  0:00:14s
epoch 87 | loss: 0.24174 | val_0_rmse: 0.66425 | val_1_rmse: 0.66921 |  0:00:14s
epoch 88 | loss: 0.24089 | val_0_rmse: 0.66011 | val_1_rmse: 0.6675  |  0:00:14s
epoch 89 | loss: 0.23689 | val_0_rmse: 0.65803 | val_1_rmse: 0.66475 |  0:00:14s
epoch 90 | loss: 0.23658 | val_0_rmse: 0.66057 | val_1_rmse: 0.67078 |  0:00:14s
epoch 91 | loss: 0.2387  | val_0_rmse: 0.66911 | val_1_rmse: 0.68065 |  0:00:14s
epoch 92 | loss: 0.23963 | val_0_rmse: 0.67079 | val_1_rmse: 0.69194 |  0:00:15s
epoch 93 | loss: 0.24004 | val_0_rmse: 0.66176 | val_1_rmse: 0.68671 |  0:00:15s
epoch 94 | loss: 0.2389  | val_0_rmse: 0.65335 | val_1_rmse: 0.67726 |  0:00:15s
epoch 95 | loss: 0.23517 | val_0_rmse: 0.66129 | val_1_rmse: 0.6842  |  0:00:15s
epoch 96 | loss: 0.24041 | val_0_rmse: 0.66617 | val_1_rmse: 0.69421 |  0:00:15s
epoch 97 | loss: 0.25182 | val_0_rmse: 0.65609 | val_1_rmse: 0.68769 |  0:00:15s
epoch 98 | loss: 0.27089 | val_0_rmse: 0.64285 | val_1_rmse: 0.67858 |  0:00:16s
epoch 99 | loss: 0.24744 | val_0_rmse: 0.63404 | val_1_rmse: 0.67292 |  0:00:16s
epoch 100| loss: 0.22902 | val_0_rmse: 0.63641 | val_1_rmse: 0.67571 |  0:00:16s
epoch 101| loss: 0.25038 | val_0_rmse: 0.63905 | val_1_rmse: 0.67332 |  0:00:16s
epoch 102| loss: 0.23582 | val_0_rmse: 0.64388 | val_1_rmse: 0.67641 |  0:00:16s
epoch 103| loss: 0.2362  | val_0_rmse: 0.65646 | val_1_rmse: 0.69242 |  0:00:16s
epoch 104| loss: 0.22934 | val_0_rmse: 0.65001 | val_1_rmse: 0.68091 |  0:00:16s
epoch 105| loss: 0.23642 | val_0_rmse: 0.65517 | val_1_rmse: 0.6889  |  0:00:17s
epoch 106| loss: 0.24387 | val_0_rmse: 0.66032 | val_1_rmse: 0.70088 |  0:00:17s
epoch 107| loss: 0.2283  | val_0_rmse: 0.65496 | val_1_rmse: 0.6885  |  0:00:17s
epoch 108| loss: 0.23214 | val_0_rmse: 0.65263 | val_1_rmse: 0.68245 |  0:00:17s
epoch 109| loss: 0.22273 | val_0_rmse: 0.66749 | val_1_rmse: 0.69649 |  0:00:17s
epoch 110| loss: 0.23108 | val_0_rmse: 0.66231 | val_1_rmse: 0.68904 |  0:00:17s
epoch 111| loss: 0.22605 | val_0_rmse: 0.64668 | val_1_rmse: 0.6783  |  0:00:18s
epoch 112| loss: 0.22954 | val_0_rmse: 0.65247 | val_1_rmse: 0.67558 |  0:00:18s
epoch 113| loss: 0.22529 | val_0_rmse: 0.65425 | val_1_rmse: 0.67548 |  0:00:18s
epoch 114| loss: 0.24403 | val_0_rmse: 0.63459 | val_1_rmse: 0.66525 |  0:00:18s
epoch 115| loss: 0.22147 | val_0_rmse: 0.62325 | val_1_rmse: 0.6567  |  0:00:18s
epoch 116| loss: 0.22133 | val_0_rmse: 0.61412 | val_1_rmse: 0.65114 |  0:00:18s
epoch 117| loss: 0.22565 | val_0_rmse: 0.61462 | val_1_rmse: 0.64759 |  0:00:19s
epoch 118| loss: 0.23824 | val_0_rmse: 0.61253 | val_1_rmse: 0.64733 |  0:00:19s
epoch 119| loss: 0.22998 | val_0_rmse: 0.60854 | val_1_rmse: 0.64849 |  0:00:19s
epoch 120| loss: 0.21783 | val_0_rmse: 0.61166 | val_1_rmse: 0.65799 |  0:00:19s
epoch 121| loss: 0.21551 | val_0_rmse: 0.62334 | val_1_rmse: 0.67048 |  0:00:19s
epoch 122| loss: 0.22259 | val_0_rmse: 0.62617 | val_1_rmse: 0.67132 |  0:00:19s
epoch 123| loss: 0.22111 | val_0_rmse: 0.61686 | val_1_rmse: 0.6511  |  0:00:20s
epoch 124| loss: 0.23436 | val_0_rmse: 0.6109  | val_1_rmse: 0.64279 |  0:00:20s
epoch 125| loss: 0.21597 | val_0_rmse: 0.60999 | val_1_rmse: 0.64119 |  0:00:20s
epoch 126| loss: 0.22213 | val_0_rmse: 0.60425 | val_1_rmse: 0.63934 |  0:00:20s
epoch 127| loss: 0.23057 | val_0_rmse: 0.60128 | val_1_rmse: 0.64099 |  0:00:20s
epoch 128| loss: 0.22695 | val_0_rmse: 0.59969 | val_1_rmse: 0.63951 |  0:00:20s
epoch 129| loss: 0.21369 | val_0_rmse: 0.60696 | val_1_rmse: 0.64584 |  0:00:20s
epoch 130| loss: 0.22649 | val_0_rmse: 0.60472 | val_1_rmse: 0.64857 |  0:00:21s
epoch 131| loss: 0.22271 | val_0_rmse: 0.6074  | val_1_rmse: 0.65388 |  0:00:21s
epoch 132| loss: 0.21233 | val_0_rmse: 0.60305 | val_1_rmse: 0.64392 |  0:00:21s
epoch 133| loss: 0.23085 | val_0_rmse: 0.60408 | val_1_rmse: 0.64288 |  0:00:21s
epoch 134| loss: 0.21454 | val_0_rmse: 0.60482 | val_1_rmse: 0.64966 |  0:00:21s
epoch 135| loss: 0.23914 | val_0_rmse: 0.60676 | val_1_rmse: 0.64819 |  0:00:21s
epoch 136| loss: 0.23656 | val_0_rmse: 0.61368 | val_1_rmse: 0.64553 |  0:00:22s
epoch 137| loss: 0.23457 | val_0_rmse: 0.62896 | val_1_rmse: 0.65474 |  0:00:22s
epoch 138| loss: 0.22667 | val_0_rmse: 0.61255 | val_1_rmse: 0.64424 |  0:00:22s
epoch 139| loss: 0.22421 | val_0_rmse: 0.60268 | val_1_rmse: 0.64799 |  0:00:22s
epoch 140| loss: 0.24231 | val_0_rmse: 0.60758 | val_1_rmse: 0.65252 |  0:00:22s
epoch 141| loss: 0.23663 | val_0_rmse: 0.61882 | val_1_rmse: 0.66744 |  0:00:23s
epoch 142| loss: 0.23684 | val_0_rmse: 0.61314 | val_1_rmse: 0.65488 |  0:00:23s
epoch 143| loss: 0.23239 | val_0_rmse: 0.60341 | val_1_rmse: 0.64281 |  0:00:23s
epoch 144| loss: 0.22381 | val_0_rmse: 0.60026 | val_1_rmse: 0.64539 |  0:00:23s
epoch 145| loss: 0.22701 | val_0_rmse: 0.59602 | val_1_rmse: 0.64624 |  0:00:23s
epoch 146| loss: 0.22961 | val_0_rmse: 0.59311 | val_1_rmse: 0.65236 |  0:00:23s
epoch 147| loss: 0.21854 | val_0_rmse: 0.59806 | val_1_rmse: 0.66496 |  0:00:23s
epoch 148| loss: 0.2187  | val_0_rmse: 0.60461 | val_1_rmse: 0.67484 |  0:00:24s
epoch 149| loss: 0.22281 | val_0_rmse: 0.59376 | val_1_rmse: 0.66784 |  0:00:24s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.63934
Best weights from best epoch are automatically used!
ended training at: 17:26:52
Feature importance:
Mean squared error is of 2366637995.938729
Mean absolute error:35676.30234141157
MAPE:0.3184231704890031
R2 score:0.632408838683061
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:26:52
epoch 0  | loss: 4.32751 | val_0_rmse: 1.10992 | val_1_rmse: 1.10495 |  0:00:00s
epoch 1  | loss: 2.52745 | val_0_rmse: 1.01607 | val_1_rmse: 1.04051 |  0:00:00s
epoch 2  | loss: 1.77131 | val_0_rmse: 0.99185 | val_1_rmse: 1.02281 |  0:00:00s
epoch 3  | loss: 1.54397 | val_0_rmse: 0.99145 | val_1_rmse: 1.02567 |  0:00:00s
epoch 4  | loss: 1.21154 | val_0_rmse: 0.99343 | val_1_rmse: 1.02966 |  0:00:00s
epoch 5  | loss: 1.18734 | val_0_rmse: 0.99368 | val_1_rmse: 1.0288  |  0:00:00s
epoch 6  | loss: 1.10375 | val_0_rmse: 0.99245 | val_1_rmse: 1.02634 |  0:00:01s
epoch 7  | loss: 1.07834 | val_0_rmse: 0.99453 | val_1_rmse: 1.02875 |  0:00:01s
epoch 8  | loss: 1.00274 | val_0_rmse: 0.99382 | val_1_rmse: 1.02759 |  0:00:01s
epoch 9  | loss: 0.98901 | val_0_rmse: 0.99183 | val_1_rmse: 1.02534 |  0:00:01s
epoch 10 | loss: 0.96045 | val_0_rmse: 0.98957 | val_1_rmse: 1.02256 |  0:00:01s
epoch 11 | loss: 0.97241 | val_0_rmse: 0.98099 | val_1_rmse: 1.01502 |  0:00:01s
epoch 12 | loss: 0.92398 | val_0_rmse: 0.96666 | val_1_rmse: 1.00202 |  0:00:02s
epoch 13 | loss: 0.89438 | val_0_rmse: 0.95158 | val_1_rmse: 0.98542 |  0:00:02s
epoch 14 | loss: 0.88221 | val_0_rmse: 0.94332 | val_1_rmse: 0.97885 |  0:00:02s
epoch 15 | loss: 0.84204 | val_0_rmse: 0.91169 | val_1_rmse: 0.95486 |  0:00:02s
epoch 16 | loss: 0.80311 | val_0_rmse: 0.87039 | val_1_rmse: 0.9112  |  0:00:02s
epoch 17 | loss: 0.74708 | val_0_rmse: 0.82009 | val_1_rmse: 0.85474 |  0:00:02s
epoch 18 | loss: 0.6898  | val_0_rmse: 0.78849 | val_1_rmse: 0.81557 |  0:00:03s
epoch 19 | loss: 0.65044 | val_0_rmse: 0.79669 | val_1_rmse: 0.79168 |  0:00:03s
epoch 20 | loss: 0.60091 | val_0_rmse: 0.80194 | val_1_rmse: 0.75516 |  0:00:03s
epoch 21 | loss: 0.59352 | val_0_rmse: 0.79772 | val_1_rmse: 0.73094 |  0:00:03s
epoch 22 | loss: 0.53582 | val_0_rmse: 0.8068  | val_1_rmse: 0.75059 |  0:00:03s
epoch 23 | loss: 0.51558 | val_0_rmse: 0.79107 | val_1_rmse: 0.72293 |  0:00:03s
epoch 24 | loss: 0.48961 | val_0_rmse: 0.80715 | val_1_rmse: 0.73874 |  0:00:04s
epoch 25 | loss: 0.46041 | val_0_rmse: 0.76491 | val_1_rmse: 0.70499 |  0:00:04s
epoch 26 | loss: 0.43763 | val_0_rmse: 0.75127 | val_1_rmse: 0.6882  |  0:00:04s
epoch 27 | loss: 0.4183  | val_0_rmse: 0.84432 | val_1_rmse: 0.74928 |  0:00:04s
epoch 28 | loss: 0.41499 | val_0_rmse: 0.85653 | val_1_rmse: 0.76401 |  0:00:04s
epoch 29 | loss: 0.41269 | val_0_rmse: 0.749   | val_1_rmse: 0.67021 |  0:00:04s
epoch 30 | loss: 0.38031 | val_0_rmse: 0.71229 | val_1_rmse: 0.65324 |  0:00:05s
epoch 31 | loss: 0.37646 | val_0_rmse: 0.73904 | val_1_rmse: 0.67319 |  0:00:05s
epoch 32 | loss: 0.36129 | val_0_rmse: 0.76215 | val_1_rmse: 0.69319 |  0:00:05s
epoch 33 | loss: 0.3418  | val_0_rmse: 0.73567 | val_1_rmse: 0.67752 |  0:00:05s
epoch 34 | loss: 0.33596 | val_0_rmse: 0.74031 | val_1_rmse: 0.68468 |  0:00:05s
epoch 35 | loss: 0.3234  | val_0_rmse: 0.75361 | val_1_rmse: 0.69713 |  0:00:05s
epoch 36 | loss: 0.32206 | val_0_rmse: 0.72313 | val_1_rmse: 0.67154 |  0:00:05s
epoch 37 | loss: 0.29919 | val_0_rmse: 0.69811 | val_1_rmse: 0.65589 |  0:00:06s
epoch 38 | loss: 0.31622 | val_0_rmse: 0.69553 | val_1_rmse: 0.65664 |  0:00:06s
epoch 39 | loss: 0.3062  | val_0_rmse: 0.72163 | val_1_rmse: 0.68395 |  0:00:06s
epoch 40 | loss: 0.29541 | val_0_rmse: 0.69499 | val_1_rmse: 0.66403 |  0:00:06s
epoch 41 | loss: 0.29204 | val_0_rmse: 0.66622 | val_1_rmse: 0.63798 |  0:00:06s
epoch 42 | loss: 0.28584 | val_0_rmse: 0.67771 | val_1_rmse: 0.63551 |  0:00:06s
epoch 43 | loss: 0.27949 | val_0_rmse: 0.72748 | val_1_rmse: 0.68083 |  0:00:07s
epoch 44 | loss: 0.28632 | val_0_rmse: 0.70125 | val_1_rmse: 0.6643  |  0:00:07s
epoch 45 | loss: 0.28646 | val_0_rmse: 0.66662 | val_1_rmse: 0.64696 |  0:00:07s
epoch 46 | loss: 0.29049 | val_0_rmse: 0.67531 | val_1_rmse: 0.65599 |  0:00:07s
epoch 47 | loss: 0.29451 | val_0_rmse: 0.76631 | val_1_rmse: 0.74555 |  0:00:07s
epoch 48 | loss: 0.28026 | val_0_rmse: 0.74303 | val_1_rmse: 0.71832 |  0:00:07s
epoch 49 | loss: 0.25685 | val_0_rmse: 0.6749  | val_1_rmse: 0.64817 |  0:00:08s
epoch 50 | loss: 0.26745 | val_0_rmse: 0.67315 | val_1_rmse: 0.64668 |  0:00:08s
epoch 51 | loss: 0.24884 | val_0_rmse: 0.70165 | val_1_rmse: 0.6714  |  0:00:08s
epoch 52 | loss: 0.26113 | val_0_rmse: 0.68423 | val_1_rmse: 0.65373 |  0:00:08s
epoch 53 | loss: 0.24283 | val_0_rmse: 0.66432 | val_1_rmse: 0.63443 |  0:00:08s
epoch 54 | loss: 0.24802 | val_0_rmse: 0.66566 | val_1_rmse: 0.64011 |  0:00:08s
epoch 55 | loss: 0.24417 | val_0_rmse: 0.66923 | val_1_rmse: 0.65351 |  0:00:08s
epoch 56 | loss: 0.23355 | val_0_rmse: 0.67285 | val_1_rmse: 0.65954 |  0:00:09s
epoch 57 | loss: 0.23319 | val_0_rmse: 0.66249 | val_1_rmse: 0.64984 |  0:00:09s
epoch 58 | loss: 0.24395 | val_0_rmse: 0.66296 | val_1_rmse: 0.65615 |  0:00:09s
epoch 59 | loss: 0.2447  | val_0_rmse: 0.66821 | val_1_rmse: 0.66068 |  0:00:09s
epoch 60 | loss: 0.22708 | val_0_rmse: 0.66499 | val_1_rmse: 0.65981 |  0:00:09s
epoch 61 | loss: 0.23522 | val_0_rmse: 0.6557  | val_1_rmse: 0.64976 |  0:00:09s
epoch 62 | loss: 0.23074 | val_0_rmse: 0.66078 | val_1_rmse: 0.65141 |  0:00:10s
epoch 63 | loss: 0.22473 | val_0_rmse: 0.66401 | val_1_rmse: 0.65883 |  0:00:10s
epoch 64 | loss: 0.21813 | val_0_rmse: 0.66144 | val_1_rmse: 0.66083 |  0:00:10s
epoch 65 | loss: 0.22539 | val_0_rmse: 0.65792 | val_1_rmse: 0.65203 |  0:00:10s
epoch 66 | loss: 0.2159  | val_0_rmse: 0.65406 | val_1_rmse: 0.64706 |  0:00:10s
epoch 67 | loss: 0.22271 | val_0_rmse: 0.65086 | val_1_rmse: 0.64169 |  0:00:10s
epoch 68 | loss: 0.21488 | val_0_rmse: 0.65465 | val_1_rmse: 0.63789 |  0:00:11s
epoch 69 | loss: 0.22006 | val_0_rmse: 0.66511 | val_1_rmse: 0.64119 |  0:00:11s
epoch 70 | loss: 0.21808 | val_0_rmse: 0.66743 | val_1_rmse: 0.6429  |  0:00:11s
epoch 71 | loss: 0.22252 | val_0_rmse: 0.66372 | val_1_rmse: 0.63667 |  0:00:11s
epoch 72 | loss: 0.21273 | val_0_rmse: 0.66318 | val_1_rmse: 0.63285 |  0:00:11s
epoch 73 | loss: 0.20621 | val_0_rmse: 0.66397 | val_1_rmse: 0.63301 |  0:00:11s
epoch 74 | loss: 0.20993 | val_0_rmse: 0.6682  | val_1_rmse: 0.64238 |  0:00:11s
epoch 75 | loss: 0.21363 | val_0_rmse: 0.66406 | val_1_rmse: 0.64602 |  0:00:12s
epoch 76 | loss: 0.20948 | val_0_rmse: 0.65908 | val_1_rmse: 0.63557 |  0:00:12s
epoch 77 | loss: 0.19804 | val_0_rmse: 0.6616  | val_1_rmse: 0.63562 |  0:00:12s
epoch 78 | loss: 0.1997  | val_0_rmse: 0.66407 | val_1_rmse: 0.63044 |  0:00:12s
epoch 79 | loss: 0.19529 | val_0_rmse: 0.65826 | val_1_rmse: 0.6316  |  0:00:12s
epoch 80 | loss: 0.19269 | val_0_rmse: 0.65436 | val_1_rmse: 0.62938 |  0:00:12s
epoch 81 | loss: 0.20108 | val_0_rmse: 0.65171 | val_1_rmse: 0.62511 |  0:00:13s
epoch 82 | loss: 0.1898  | val_0_rmse: 0.65093 | val_1_rmse: 0.6305  |  0:00:13s
epoch 83 | loss: 0.19829 | val_0_rmse: 0.64548 | val_1_rmse: 0.63607 |  0:00:13s
epoch 84 | loss: 0.18717 | val_0_rmse: 0.64114 | val_1_rmse: 0.63565 |  0:00:13s
epoch 85 | loss: 0.18851 | val_0_rmse: 0.64146 | val_1_rmse: 0.63337 |  0:00:13s
epoch 86 | loss: 0.18145 | val_0_rmse: 0.64149 | val_1_rmse: 0.63498 |  0:00:13s
epoch 87 | loss: 0.1906  | val_0_rmse: 0.63783 | val_1_rmse: 0.63467 |  0:00:14s
epoch 88 | loss: 0.19101 | val_0_rmse: 0.63682 | val_1_rmse: 0.63778 |  0:00:14s
epoch 89 | loss: 0.20166 | val_0_rmse: 0.63468 | val_1_rmse: 0.64019 |  0:00:14s
epoch 90 | loss: 0.1967  | val_0_rmse: 0.63973 | val_1_rmse: 0.65292 |  0:00:14s
epoch 91 | loss: 0.20198 | val_0_rmse: 0.6379  | val_1_rmse: 0.65799 |  0:00:14s
epoch 92 | loss: 0.19435 | val_0_rmse: 0.63261 | val_1_rmse: 0.65483 |  0:00:14s
epoch 93 | loss: 0.18595 | val_0_rmse: 0.63326 | val_1_rmse: 0.65034 |  0:00:14s
epoch 94 | loss: 0.18318 | val_0_rmse: 0.63602 | val_1_rmse: 0.64416 |  0:00:15s
epoch 95 | loss: 0.18294 | val_0_rmse: 0.63817 | val_1_rmse: 0.64527 |  0:00:15s
epoch 96 | loss: 0.18293 | val_0_rmse: 0.64208 | val_1_rmse: 0.64722 |  0:00:15s
epoch 97 | loss: 0.17929 | val_0_rmse: 0.6411  | val_1_rmse: 0.64971 |  0:00:15s
epoch 98 | loss: 0.17843 | val_0_rmse: 0.64209 | val_1_rmse: 0.64478 |  0:00:15s
epoch 99 | loss: 0.18644 | val_0_rmse: 0.6599  | val_1_rmse: 0.64773 |  0:00:15s
epoch 100| loss: 0.18562 | val_0_rmse: 0.65046 | val_1_rmse: 0.64482 |  0:00:16s
epoch 101| loss: 0.18261 | val_0_rmse: 0.643   | val_1_rmse: 0.64672 |  0:00:16s
epoch 102| loss: 0.18397 | val_0_rmse: 0.64436 | val_1_rmse: 0.64966 |  0:00:16s
epoch 103| loss: 0.18991 | val_0_rmse: 0.63845 | val_1_rmse: 0.63846 |  0:00:16s
epoch 104| loss: 0.17616 | val_0_rmse: 0.63784 | val_1_rmse: 0.63919 |  0:00:16s
epoch 105| loss: 0.1695  | val_0_rmse: 0.63133 | val_1_rmse: 0.6356  |  0:00:16s
epoch 106| loss: 0.17658 | val_0_rmse: 0.63833 | val_1_rmse: 0.6328  |  0:00:16s
epoch 107| loss: 0.17205 | val_0_rmse: 0.65317 | val_1_rmse: 0.63876 |  0:00:17s
epoch 108| loss: 0.16842 | val_0_rmse: 0.65184 | val_1_rmse: 0.63791 |  0:00:17s
epoch 109| loss: 0.1652  | val_0_rmse: 0.63604 | val_1_rmse: 0.63088 |  0:00:17s
epoch 110| loss: 0.16769 | val_0_rmse: 0.62722 | val_1_rmse: 0.62584 |  0:00:17s
epoch 111| loss: 0.17935 | val_0_rmse: 0.61661 | val_1_rmse: 0.62528 |  0:00:17s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.62511
Best weights from best epoch are automatically used!
ended training at: 17:27:10
Feature importance:
Mean squared error is of 3080766576.9270325
Mean absolute error:40555.683391935934
MAPE:0.33981117632081254
R2 score:0.5926901854176975
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:27:10
epoch 0  | loss: 4.66577 | val_0_rmse: 1.04091 | val_1_rmse: 0.98033 |  0:00:00s
epoch 1  | loss: 2.55013 | val_0_rmse: 1.01351 | val_1_rmse: 0.97004 |  0:00:00s
epoch 2  | loss: 1.49248 | val_0_rmse: 1.0036  | val_1_rmse: 0.99039 |  0:00:00s
epoch 3  | loss: 1.24284 | val_0_rmse: 0.99383 | val_1_rmse: 0.98955 |  0:00:00s
epoch 4  | loss: 1.1635  | val_0_rmse: 0.98479 | val_1_rmse: 0.97602 |  0:00:00s
epoch 5  | loss: 1.14505 | val_0_rmse: 0.99324 | val_1_rmse: 0.97845 |  0:00:00s
epoch 6  | loss: 1.05618 | val_0_rmse: 0.99807 | val_1_rmse: 0.97574 |  0:00:01s
epoch 7  | loss: 1.03129 | val_0_rmse: 0.99751 | val_1_rmse: 0.96766 |  0:00:01s
epoch 8  | loss: 0.97763 | val_0_rmse: 0.99463 | val_1_rmse: 0.95876 |  0:00:01s
epoch 9  | loss: 0.95155 | val_0_rmse: 0.97074 | val_1_rmse: 0.94101 |  0:00:01s
epoch 10 | loss: 0.89542 | val_0_rmse: 0.95419 | val_1_rmse: 0.92362 |  0:00:01s
epoch 11 | loss: 0.86339 | val_0_rmse: 0.97352 | val_1_rmse: 0.92464 |  0:00:01s
epoch 12 | loss: 0.79318 | val_0_rmse: 0.95054 | val_1_rmse: 0.91168 |  0:00:02s
epoch 13 | loss: 0.7493  | val_0_rmse: 0.92092 | val_1_rmse: 0.89896 |  0:00:02s
epoch 14 | loss: 0.67577 | val_0_rmse: 0.89589 | val_1_rmse: 0.87823 |  0:00:02s
epoch 15 | loss: 0.64683 | val_0_rmse: 0.8572  | val_1_rmse: 0.83924 |  0:00:02s
epoch 16 | loss: 0.64674 | val_0_rmse: 0.80672 | val_1_rmse: 0.79818 |  0:00:02s
epoch 17 | loss: 0.6017  | val_0_rmse: 0.81311 | val_1_rmse: 0.77978 |  0:00:02s
epoch 18 | loss: 0.57457 | val_0_rmse: 0.78421 | val_1_rmse: 0.76291 |  0:00:03s
epoch 19 | loss: 0.54514 | val_0_rmse: 0.77025 | val_1_rmse: 0.75456 |  0:00:03s
epoch 20 | loss: 0.51446 | val_0_rmse: 0.82629 | val_1_rmse: 0.79548 |  0:00:03s
epoch 21 | loss: 0.48794 | val_0_rmse: 0.80655 | val_1_rmse: 0.78582 |  0:00:03s
epoch 22 | loss: 0.47035 | val_0_rmse: 0.79587 | val_1_rmse: 0.76361 |  0:00:03s
epoch 23 | loss: 0.42859 | val_0_rmse: 0.8612  | val_1_rmse: 0.81852 |  0:00:03s
epoch 24 | loss: 0.45039 | val_0_rmse: 0.84875 | val_1_rmse: 0.81004 |  0:00:04s
epoch 25 | loss: 0.4157  | val_0_rmse: 0.80811 | val_1_rmse: 0.77656 |  0:00:04s
epoch 26 | loss: 0.40578 | val_0_rmse: 0.80293 | val_1_rmse: 0.77488 |  0:00:04s
epoch 27 | loss: 0.39772 | val_0_rmse: 0.77521 | val_1_rmse: 0.74915 |  0:00:04s
epoch 28 | loss: 0.37204 | val_0_rmse: 0.73753 | val_1_rmse: 0.72072 |  0:00:04s
epoch 29 | loss: 0.35133 | val_0_rmse: 0.74005 | val_1_rmse: 0.72149 |  0:00:04s
epoch 30 | loss: 0.35587 | val_0_rmse: 0.75212 | val_1_rmse: 0.72961 |  0:00:04s
epoch 31 | loss: 0.32291 | val_0_rmse: 0.70116 | val_1_rmse: 0.68856 |  0:00:05s
epoch 32 | loss: 0.3355  | val_0_rmse: 0.68352 | val_1_rmse: 0.67134 |  0:00:05s
epoch 33 | loss: 0.30296 | val_0_rmse: 0.69628 | val_1_rmse: 0.66833 |  0:00:05s
epoch 34 | loss: 0.31506 | val_0_rmse: 0.69787 | val_1_rmse: 0.66285 |  0:00:05s
epoch 35 | loss: 0.30258 | val_0_rmse: 0.69527 | val_1_rmse: 0.66559 |  0:00:05s
epoch 36 | loss: 0.30525 | val_0_rmse: 0.71477 | val_1_rmse: 0.68683 |  0:00:05s
epoch 37 | loss: 0.28756 | val_0_rmse: 0.73584 | val_1_rmse: 0.70798 |  0:00:06s
epoch 38 | loss: 0.29018 | val_0_rmse: 0.71041 | val_1_rmse: 0.69006 |  0:00:06s
epoch 39 | loss: 0.29355 | val_0_rmse: 0.69335 | val_1_rmse: 0.67532 |  0:00:06s
epoch 40 | loss: 0.28077 | val_0_rmse: 0.72078 | val_1_rmse: 0.69559 |  0:00:06s
epoch 41 | loss: 0.27112 | val_0_rmse: 0.71821 | val_1_rmse: 0.69414 |  0:00:06s
epoch 42 | loss: 0.27053 | val_0_rmse: 0.6982  | val_1_rmse: 0.67914 |  0:00:06s
epoch 43 | loss: 0.26301 | val_0_rmse: 0.69245 | val_1_rmse: 0.67743 |  0:00:07s
epoch 44 | loss: 0.2548  | val_0_rmse: 0.68677 | val_1_rmse: 0.67499 |  0:00:07s
epoch 45 | loss: 0.25265 | val_0_rmse: 0.68871 | val_1_rmse: 0.67983 |  0:00:07s
epoch 46 | loss: 0.24245 | val_0_rmse: 0.69951 | val_1_rmse: 0.68836 |  0:00:07s
epoch 47 | loss: 0.2329  | val_0_rmse: 0.6985  | val_1_rmse: 0.68602 |  0:00:07s
epoch 48 | loss: 0.22993 | val_0_rmse: 0.69326 | val_1_rmse: 0.67864 |  0:00:07s
epoch 49 | loss: 0.23086 | val_0_rmse: 0.70581 | val_1_rmse: 0.68643 |  0:00:08s
epoch 50 | loss: 0.23293 | val_0_rmse: 0.69967 | val_1_rmse: 0.67945 |  0:00:08s
epoch 51 | loss: 0.23267 | val_0_rmse: 0.6861  | val_1_rmse: 0.66985 |  0:00:08s
epoch 52 | loss: 0.22687 | val_0_rmse: 0.69135 | val_1_rmse: 0.67173 |  0:00:08s
epoch 53 | loss: 0.2173  | val_0_rmse: 0.72663 | val_1_rmse: 0.7004  |  0:00:08s
epoch 54 | loss: 0.21834 | val_0_rmse: 0.70816 | val_1_rmse: 0.68302 |  0:00:08s
epoch 55 | loss: 0.21476 | val_0_rmse: 0.68411 | val_1_rmse: 0.66413 |  0:00:08s
epoch 56 | loss: 0.22235 | val_0_rmse: 0.69177 | val_1_rmse: 0.67294 |  0:00:09s
epoch 57 | loss: 0.21254 | val_0_rmse: 0.71772 | val_1_rmse: 0.69536 |  0:00:09s
epoch 58 | loss: 0.21465 | val_0_rmse: 0.70276 | val_1_rmse: 0.68313 |  0:00:09s
epoch 59 | loss: 0.21619 | val_0_rmse: 0.68379 | val_1_rmse: 0.66901 |  0:00:09s
epoch 60 | loss: 0.22574 | val_0_rmse: 0.68602 | val_1_rmse: 0.67178 |  0:00:09s
epoch 61 | loss: 0.2181  | val_0_rmse: 0.68848 | val_1_rmse: 0.67517 |  0:00:09s
epoch 62 | loss: 0.21777 | val_0_rmse: 0.68989 | val_1_rmse: 0.67546 |  0:00:10s
epoch 63 | loss: 0.20334 | val_0_rmse: 0.68678 | val_1_rmse: 0.66887 |  0:00:10s
epoch 64 | loss: 0.21383 | val_0_rmse: 0.68858 | val_1_rmse: 0.66822 |  0:00:10s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.66285
Best weights from best epoch are automatically used!
ended training at: 17:27:21
Feature importance:
Mean squared error is of 3542062496.541907
Mean absolute error:44375.51700807823
MAPE:0.45906687495094317
R2 score:0.5080473909745656
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:27:21
epoch 0  | loss: 3.82914 | val_0_rmse: 1.07542 | val_1_rmse: 1.05881 |  0:00:00s
epoch 1  | loss: 2.22341 | val_0_rmse: 1.01302 | val_1_rmse: 0.99513 |  0:00:00s
epoch 2  | loss: 1.89124 | val_0_rmse: 1.01699 | val_1_rmse: 0.99887 |  0:00:00s
epoch 3  | loss: 1.37776 | val_0_rmse: 1.01028 | val_1_rmse: 0.99333 |  0:00:00s
epoch 4  | loss: 1.28533 | val_0_rmse: 1.01937 | val_1_rmse: 1.00164 |  0:00:00s
epoch 5  | loss: 1.14163 | val_0_rmse: 1.0106  | val_1_rmse: 0.99149 |  0:00:00s
epoch 6  | loss: 1.06542 | val_0_rmse: 1.00757 | val_1_rmse: 0.98867 |  0:00:01s
epoch 7  | loss: 1.1313  | val_0_rmse: 1.00953 | val_1_rmse: 0.99126 |  0:00:01s
epoch 8  | loss: 1.04789 | val_0_rmse: 1.0082  | val_1_rmse: 0.99086 |  0:00:01s
epoch 9  | loss: 1.00903 | val_0_rmse: 1.0043  | val_1_rmse: 0.98766 |  0:00:01s
epoch 10 | loss: 0.96013 | val_0_rmse: 0.98405 | val_1_rmse: 0.97165 |  0:00:01s
epoch 11 | loss: 0.9168  | val_0_rmse: 0.94083 | val_1_rmse: 0.9259  |  0:00:01s
epoch 12 | loss: 0.91835 | val_0_rmse: 0.90571 | val_1_rmse: 0.89524 |  0:00:02s
epoch 13 | loss: 0.88564 | val_0_rmse: 0.91207 | val_1_rmse: 0.89855 |  0:00:02s
epoch 14 | loss: 0.86248 | val_0_rmse: 0.94575 | val_1_rmse: 0.93407 |  0:00:02s
epoch 15 | loss: 0.85267 | val_0_rmse: 0.90645 | val_1_rmse: 0.90105 |  0:00:02s
epoch 16 | loss: 0.77261 | val_0_rmse: 0.91535 | val_1_rmse: 0.88978 |  0:00:02s
epoch 17 | loss: 0.72575 | val_0_rmse: 0.93756 | val_1_rmse: 0.90868 |  0:00:02s
epoch 18 | loss: 0.67349 | val_0_rmse: 0.93162 | val_1_rmse: 0.91077 |  0:00:03s
epoch 19 | loss: 0.66704 | val_0_rmse: 0.85458 | val_1_rmse: 0.81698 |  0:00:03s
epoch 20 | loss: 0.64619 | val_0_rmse: 0.85998 | val_1_rmse: 0.80569 |  0:00:03s
epoch 21 | loss: 0.62072 | val_0_rmse: 0.88723 | val_1_rmse: 0.80576 |  0:00:03s
epoch 22 | loss: 0.60899 | val_0_rmse: 0.85676 | val_1_rmse: 0.79461 |  0:00:03s
epoch 23 | loss: 0.58472 | val_0_rmse: 0.82572 | val_1_rmse: 0.77777 |  0:00:03s
epoch 24 | loss: 0.56214 | val_0_rmse: 0.82047 | val_1_rmse: 0.77041 |  0:00:04s
epoch 25 | loss: 0.55206 | val_0_rmse: 0.82549 | val_1_rmse: 0.77339 |  0:00:04s
epoch 26 | loss: 0.55765 | val_0_rmse: 0.80885 | val_1_rmse: 0.74803 |  0:00:04s
epoch 27 | loss: 0.53206 | val_0_rmse: 0.80047 | val_1_rmse: 0.74722 |  0:00:04s
epoch 28 | loss: 0.53907 | val_0_rmse: 0.77886 | val_1_rmse: 0.73784 |  0:00:04s
epoch 29 | loss: 0.51942 | val_0_rmse: 0.7651  | val_1_rmse: 0.72867 |  0:00:04s
epoch 30 | loss: 0.51641 | val_0_rmse: 0.75563 | val_1_rmse: 0.73367 |  0:00:05s
epoch 31 | loss: 0.50145 | val_0_rmse: 0.76135 | val_1_rmse: 0.73621 |  0:00:05s
epoch 32 | loss: 0.50964 | val_0_rmse: 0.77466 | val_1_rmse: 0.7361  |  0:00:05s
epoch 33 | loss: 0.45007 | val_0_rmse: 0.7726  | val_1_rmse: 0.73371 |  0:00:05s
epoch 34 | loss: 0.44657 | val_0_rmse: 0.74863 | val_1_rmse: 0.70957 |  0:00:05s
epoch 35 | loss: 0.46412 | val_0_rmse: 0.71722 | val_1_rmse: 0.67972 |  0:00:05s
epoch 36 | loss: 0.43823 | val_0_rmse: 0.70832 | val_1_rmse: 0.6671  |  0:00:05s
epoch 37 | loss: 0.43168 | val_0_rmse: 0.71855 | val_1_rmse: 0.67265 |  0:00:06s
epoch 38 | loss: 0.41766 | val_0_rmse: 0.73098 | val_1_rmse: 0.69035 |  0:00:06s
epoch 39 | loss: 0.40152 | val_0_rmse: 0.69668 | val_1_rmse: 0.66442 |  0:00:06s
epoch 40 | loss: 0.41699 | val_0_rmse: 0.70648 | val_1_rmse: 0.68895 |  0:00:06s
epoch 41 | loss: 0.40363 | val_0_rmse: 0.74164 | val_1_rmse: 0.72117 |  0:00:06s
epoch 42 | loss: 0.44438 | val_0_rmse: 0.71326 | val_1_rmse: 0.69047 |  0:00:07s
epoch 43 | loss: 0.37926 | val_0_rmse: 0.70207 | val_1_rmse: 0.68646 |  0:00:07s
epoch 44 | loss: 0.37573 | val_0_rmse: 0.70148 | val_1_rmse: 0.68712 |  0:00:07s
epoch 45 | loss: 0.37272 | val_0_rmse: 0.71487 | val_1_rmse: 0.69709 |  0:00:07s
epoch 46 | loss: 0.38005 | val_0_rmse: 0.70332 | val_1_rmse: 0.69037 |  0:00:07s
epoch 47 | loss: 0.36407 | val_0_rmse: 0.69729 | val_1_rmse: 0.68821 |  0:00:07s
epoch 48 | loss: 0.35245 | val_0_rmse: 0.6998  | val_1_rmse: 0.68287 |  0:00:08s
epoch 49 | loss: 0.34691 | val_0_rmse: 0.70085 | val_1_rmse: 0.6877  |  0:00:08s
epoch 50 | loss: 0.33092 | val_0_rmse: 0.69701 | val_1_rmse: 0.6939  |  0:00:08s
epoch 51 | loss: 0.34178 | val_0_rmse: 0.69254 | val_1_rmse: 0.68939 |  0:00:08s
epoch 52 | loss: 0.34862 | val_0_rmse: 0.67665 | val_1_rmse: 0.6684  |  0:00:08s
epoch 53 | loss: 0.34634 | val_0_rmse: 0.67603 | val_1_rmse: 0.66348 |  0:00:08s
epoch 54 | loss: 0.34308 | val_0_rmse: 0.69133 | val_1_rmse: 0.67269 |  0:00:09s
epoch 55 | loss: 0.31641 | val_0_rmse: 0.6836  | val_1_rmse: 0.65889 |  0:00:09s
epoch 56 | loss: 0.32172 | val_0_rmse: 0.68686 | val_1_rmse: 0.66371 |  0:00:09s
epoch 57 | loss: 0.30322 | val_0_rmse: 0.70105 | val_1_rmse: 0.67564 |  0:00:09s
epoch 58 | loss: 0.31927 | val_0_rmse: 0.69532 | val_1_rmse: 0.66971 |  0:00:09s
epoch 59 | loss: 0.31869 | val_0_rmse: 0.68398 | val_1_rmse: 0.65883 |  0:00:09s
epoch 60 | loss: 0.31459 | val_0_rmse: 0.67807 | val_1_rmse: 0.65594 |  0:00:09s
epoch 61 | loss: 0.29625 | val_0_rmse: 0.6852  | val_1_rmse: 0.66252 |  0:00:10s
epoch 62 | loss: 0.30209 | val_0_rmse: 0.69473 | val_1_rmse: 0.67566 |  0:00:10s
epoch 63 | loss: 0.2917  | val_0_rmse: 0.67787 | val_1_rmse: 0.66508 |  0:00:10s
epoch 64 | loss: 0.29511 | val_0_rmse: 0.6688  | val_1_rmse: 0.65829 |  0:00:10s
epoch 65 | loss: 0.2911  | val_0_rmse: 0.67406 | val_1_rmse: 0.66018 |  0:00:10s
epoch 66 | loss: 0.28374 | val_0_rmse: 0.66943 | val_1_rmse: 0.65552 |  0:00:10s
epoch 67 | loss: 0.2833  | val_0_rmse: 0.6632  | val_1_rmse: 0.65015 |  0:00:11s
epoch 68 | loss: 0.29082 | val_0_rmse: 0.66498 | val_1_rmse: 0.64575 |  0:00:11s
epoch 69 | loss: 0.28582 | val_0_rmse: 0.67629 | val_1_rmse: 0.65398 |  0:00:11s
epoch 70 | loss: 0.2922  | val_0_rmse: 0.67958 | val_1_rmse: 0.65852 |  0:00:11s
epoch 71 | loss: 0.289   | val_0_rmse: 0.6778  | val_1_rmse: 0.65592 |  0:00:11s
epoch 72 | loss: 0.28431 | val_0_rmse: 0.6765  | val_1_rmse: 0.65613 |  0:00:11s
epoch 73 | loss: 0.2901  | val_0_rmse: 0.68132 | val_1_rmse: 0.65802 |  0:00:12s
epoch 74 | loss: 0.29338 | val_0_rmse: 0.68162 | val_1_rmse: 0.66232 |  0:00:12s
epoch 75 | loss: 0.28546 | val_0_rmse: 0.66125 | val_1_rmse: 0.64882 |  0:00:12s
epoch 76 | loss: 0.29397 | val_0_rmse: 0.66623 | val_1_rmse: 0.65008 |  0:00:12s
epoch 77 | loss: 0.27523 | val_0_rmse: 0.6852  | val_1_rmse: 0.66578 |  0:00:12s
epoch 78 | loss: 0.28483 | val_0_rmse: 0.67066 | val_1_rmse: 0.64971 |  0:00:12s
epoch 79 | loss: 0.29    | val_0_rmse: 0.67351 | val_1_rmse: 0.65088 |  0:00:12s
epoch 80 | loss: 0.28823 | val_0_rmse: 0.68699 | val_1_rmse: 0.66464 |  0:00:13s
epoch 81 | loss: 0.30218 | val_0_rmse: 0.68295 | val_1_rmse: 0.66562 |  0:00:13s
epoch 82 | loss: 0.29105 | val_0_rmse: 0.66476 | val_1_rmse: 0.65156 |  0:00:13s
epoch 83 | loss: 0.28154 | val_0_rmse: 0.66101 | val_1_rmse: 0.64534 |  0:00:13s
epoch 84 | loss: 0.27303 | val_0_rmse: 0.67121 | val_1_rmse: 0.6575  |  0:00:13s
epoch 85 | loss: 0.28597 | val_0_rmse: 0.65704 | val_1_rmse: 0.64323 |  0:00:13s
epoch 86 | loss: 0.27054 | val_0_rmse: 0.64772 | val_1_rmse: 0.62928 |  0:00:14s
epoch 87 | loss: 0.27211 | val_0_rmse: 0.65007 | val_1_rmse: 0.63352 |  0:00:14s
epoch 88 | loss: 0.26228 | val_0_rmse: 0.66319 | val_1_rmse: 0.64569 |  0:00:14s
epoch 89 | loss: 0.25997 | val_0_rmse: 0.65811 | val_1_rmse: 0.63806 |  0:00:14s
epoch 90 | loss: 0.2604  | val_0_rmse: 0.64866 | val_1_rmse: 0.62961 |  0:00:14s
epoch 91 | loss: 0.26106 | val_0_rmse: 0.64231 | val_1_rmse: 0.62301 |  0:00:14s
epoch 92 | loss: 0.25015 | val_0_rmse: 0.63596 | val_1_rmse: 0.62582 |  0:00:15s
epoch 93 | loss: 0.26365 | val_0_rmse: 0.63843 | val_1_rmse: 0.62949 |  0:00:15s
epoch 94 | loss: 0.25592 | val_0_rmse: 0.64174 | val_1_rmse: 0.63119 |  0:00:15s
epoch 95 | loss: 0.2679  | val_0_rmse: 0.64229 | val_1_rmse: 0.63152 |  0:00:15s
epoch 96 | loss: 0.26322 | val_0_rmse: 0.64459 | val_1_rmse: 0.63707 |  0:00:15s
epoch 97 | loss: 0.25422 | val_0_rmse: 0.64646 | val_1_rmse: 0.64022 |  0:00:15s
epoch 98 | loss: 0.24939 | val_0_rmse: 0.64154 | val_1_rmse: 0.63575 |  0:00:15s
epoch 99 | loss: 0.25573 | val_0_rmse: 0.63964 | val_1_rmse: 0.63388 |  0:00:16s
epoch 100| loss: 0.25316 | val_0_rmse: 0.64361 | val_1_rmse: 0.6391  |  0:00:16s
epoch 101| loss: 0.25901 | val_0_rmse: 0.65284 | val_1_rmse: 0.64712 |  0:00:16s
epoch 102| loss: 0.25842 | val_0_rmse: 0.64609 | val_1_rmse: 0.63841 |  0:00:16s
epoch 103| loss: 0.25112 | val_0_rmse: 0.64326 | val_1_rmse: 0.63582 |  0:00:16s
epoch 104| loss: 0.25532 | val_0_rmse: 0.6452  | val_1_rmse: 0.63589 |  0:00:16s
epoch 105| loss: 0.25084 | val_0_rmse: 0.64739 | val_1_rmse: 0.63437 |  0:00:17s
epoch 106| loss: 0.25286 | val_0_rmse: 0.6474  | val_1_rmse: 0.63512 |  0:00:17s
epoch 107| loss: 0.25166 | val_0_rmse: 0.64075 | val_1_rmse: 0.63017 |  0:00:17s
epoch 108| loss: 0.24326 | val_0_rmse: 0.63391 | val_1_rmse: 0.62371 |  0:00:17s
epoch 109| loss: 0.24014 | val_0_rmse: 0.63076 | val_1_rmse: 0.62164 |  0:00:17s
epoch 110| loss: 0.24427 | val_0_rmse: 0.63145 | val_1_rmse: 0.6231  |  0:00:17s
epoch 111| loss: 0.24182 | val_0_rmse: 0.63587 | val_1_rmse: 0.62764 |  0:00:18s
epoch 112| loss: 0.23884 | val_0_rmse: 0.63194 | val_1_rmse: 0.62974 |  0:00:18s
epoch 113| loss: 0.23831 | val_0_rmse: 0.62749 | val_1_rmse: 0.62794 |  0:00:18s
epoch 114| loss: 0.23995 | val_0_rmse: 0.62533 | val_1_rmse: 0.62784 |  0:00:18s
epoch 115| loss: 0.23929 | val_0_rmse: 0.62016 | val_1_rmse: 0.62278 |  0:00:18s
epoch 116| loss: 0.23972 | val_0_rmse: 0.62847 | val_1_rmse: 0.6297  |  0:00:18s
epoch 117| loss: 0.23943 | val_0_rmse: 0.6373  | val_1_rmse: 0.63943 |  0:00:18s
epoch 118| loss: 0.23676 | val_0_rmse: 0.63912 | val_1_rmse: 0.6408  |  0:00:19s
epoch 119| loss: 0.23686 | val_0_rmse: 0.62215 | val_1_rmse: 0.62584 |  0:00:19s
epoch 120| loss: 0.24057 | val_0_rmse: 0.62457 | val_1_rmse: 0.62973 |  0:00:19s
epoch 121| loss: 0.2368  | val_0_rmse: 0.63682 | val_1_rmse: 0.64081 |  0:00:19s
epoch 122| loss: 0.22549 | val_0_rmse: 0.64412 | val_1_rmse: 0.64808 |  0:00:19s
epoch 123| loss: 0.22336 | val_0_rmse: 0.6285  | val_1_rmse: 0.62997 |  0:00:19s
epoch 124| loss: 0.23285 | val_0_rmse: 0.62223 | val_1_rmse: 0.6203  |  0:00:20s
epoch 125| loss: 0.22961 | val_0_rmse: 0.63437 | val_1_rmse: 0.63188 |  0:00:20s
epoch 126| loss: 0.2311  | val_0_rmse: 0.63657 | val_1_rmse: 0.63611 |  0:00:20s
epoch 127| loss: 0.22645 | val_0_rmse: 0.62422 | val_1_rmse: 0.62624 |  0:00:20s
epoch 128| loss: 0.22599 | val_0_rmse: 0.61879 | val_1_rmse: 0.622   |  0:00:20s
epoch 129| loss: 0.22497 | val_0_rmse: 0.61747 | val_1_rmse: 0.62312 |  0:00:20s
epoch 130| loss: 0.22641 | val_0_rmse: 0.61296 | val_1_rmse: 0.62366 |  0:00:21s
epoch 131| loss: 0.22778 | val_0_rmse: 0.60555 | val_1_rmse: 0.61679 |  0:00:21s
epoch 132| loss: 0.22342 | val_0_rmse: 0.60314 | val_1_rmse: 0.61433 |  0:00:21s
epoch 133| loss: 0.23518 | val_0_rmse: 0.5974  | val_1_rmse: 0.6107  |  0:00:21s
epoch 134| loss: 0.22996 | val_0_rmse: 0.5953  | val_1_rmse: 0.60692 |  0:00:21s
epoch 135| loss: 0.2323  | val_0_rmse: 0.59364 | val_1_rmse: 0.60502 |  0:00:21s
epoch 136| loss: 0.22498 | val_0_rmse: 0.59107 | val_1_rmse: 0.6036  |  0:00:22s
epoch 137| loss: 0.21896 | val_0_rmse: 0.59275 | val_1_rmse: 0.60775 |  0:00:22s
epoch 138| loss: 0.21872 | val_0_rmse: 0.59538 | val_1_rmse: 0.61492 |  0:00:22s
epoch 139| loss: 0.21847 | val_0_rmse: 0.59502 | val_1_rmse: 0.61474 |  0:00:22s
epoch 140| loss: 0.22021 | val_0_rmse: 0.59425 | val_1_rmse: 0.61598 |  0:00:22s
epoch 141| loss: 0.21866 | val_0_rmse: 0.59806 | val_1_rmse: 0.62001 |  0:00:22s
epoch 142| loss: 0.21222 | val_0_rmse: 0.60636 | val_1_rmse: 0.62389 |  0:00:22s
epoch 143| loss: 0.21875 | val_0_rmse: 0.60983 | val_1_rmse: 0.62186 |  0:00:23s
epoch 144| loss: 0.2175  | val_0_rmse: 0.60471 | val_1_rmse: 0.61143 |  0:00:23s
epoch 145| loss: 0.21211 | val_0_rmse: 0.60282 | val_1_rmse: 0.60671 |  0:00:23s
epoch 146| loss: 0.21182 | val_0_rmse: 0.59826 | val_1_rmse: 0.60574 |  0:00:23s
epoch 147| loss: 0.21249 | val_0_rmse: 0.601   | val_1_rmse: 0.61314 |  0:00:23s
epoch 148| loss: 0.21907 | val_0_rmse: 0.59693 | val_1_rmse: 0.61455 |  0:00:23s
epoch 149| loss: 0.21075 | val_0_rmse: 0.5836  | val_1_rmse: 0.59848 |  0:00:24s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.59848
Best weights from best epoch are automatically used!
ended training at: 17:27:45
Feature importance:
Mean squared error is of 2407380205.907679
Mean absolute error:34771.63204464286
MAPE:0.3127473824686189
R2 score:0.643416227476236
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:27:46
epoch 0  | loss: 2.98134 | val_0_rmse: 1.00383 | val_1_rmse: 0.98457 |  0:00:00s
epoch 1  | loss: 1.55156 | val_0_rmse: 1.73994 | val_1_rmse: 0.98039 |  0:00:00s
epoch 2  | loss: 1.05304 | val_0_rmse: 1.00199 | val_1_rmse: 0.98599 |  0:00:01s
epoch 3  | loss: 0.88249 | val_0_rmse: 0.98446 | val_1_rmse: 0.9634  |  0:00:01s
epoch 4  | loss: 0.78477 | val_0_rmse: 0.90922 | val_1_rmse: 0.87931 |  0:00:01s
epoch 5  | loss: 0.74103 | val_0_rmse: 0.8172  | val_1_rmse: 0.78695 |  0:00:02s
epoch 6  | loss: 0.70628 | val_0_rmse: 0.81717 | val_1_rmse: 0.79985 |  0:00:02s
epoch 7  | loss: 0.67574 | val_0_rmse: 0.81541 | val_1_rmse: 0.7855  |  0:00:02s
epoch 8  | loss: 0.63768 | val_0_rmse: 0.85249 | val_1_rmse: 0.81561 |  0:00:03s
epoch 9  | loss: 0.59677 | val_0_rmse: 0.90396 | val_1_rmse: 0.87488 |  0:00:03s
epoch 10 | loss: 0.57762 | val_0_rmse: 0.86694 | val_1_rmse: 0.83798 |  0:00:03s
epoch 11 | loss: 0.55519 | val_0_rmse: 0.88057 | val_1_rmse: 0.84483 |  0:00:04s
epoch 12 | loss: 0.55108 | val_0_rmse: 0.85316 | val_1_rmse: 0.82559 |  0:00:04s
epoch 13 | loss: 0.55914 | val_0_rmse: 0.81111 | val_1_rmse: 0.78628 |  0:00:05s
epoch 14 | loss: 0.52592 | val_0_rmse: 0.81087 | val_1_rmse: 0.78342 |  0:00:05s
epoch 15 | loss: 0.51005 | val_0_rmse: 0.80863 | val_1_rmse: 0.77621 |  0:00:05s
epoch 16 | loss: 0.50552 | val_0_rmse: 0.79976 | val_1_rmse: 0.7704  |  0:00:06s
epoch 17 | loss: 0.50216 | val_0_rmse: 0.77224 | val_1_rmse: 0.7624  |  0:00:06s
epoch 18 | loss: 0.48821 | val_0_rmse: 0.77354 | val_1_rmse: 0.75865 |  0:00:06s
epoch 19 | loss: 0.49844 | val_0_rmse: 0.76035 | val_1_rmse: 0.74175 |  0:00:07s
epoch 20 | loss: 0.49657 | val_0_rmse: 0.79725 | val_1_rmse: 0.7758  |  0:00:07s
epoch 21 | loss: 0.48425 | val_0_rmse: 0.80593 | val_1_rmse: 0.78575 |  0:00:07s
epoch 22 | loss: 0.48284 | val_0_rmse: 0.79747 | val_1_rmse: 0.78    |  0:00:08s
epoch 23 | loss: 0.47173 | val_0_rmse: 0.77848 | val_1_rmse: 0.75633 |  0:00:08s
epoch 24 | loss: 0.46645 | val_0_rmse: 0.79073 | val_1_rmse: 0.7684  |  0:00:08s
epoch 25 | loss: 0.47182 | val_0_rmse: 0.76793 | val_1_rmse: 0.75032 |  0:00:09s
epoch 26 | loss: 0.46691 | val_0_rmse: 0.79605 | val_1_rmse: 0.77914 |  0:00:09s
epoch 27 | loss: 0.44976 | val_0_rmse: 0.77491 | val_1_rmse: 0.76789 |  0:00:09s
epoch 28 | loss: 0.45956 | val_0_rmse: 0.78777 | val_1_rmse: 0.77748 |  0:00:10s
epoch 29 | loss: 0.45658 | val_0_rmse: 0.78639 | val_1_rmse: 0.77811 |  0:00:10s
epoch 30 | loss: 0.45012 | val_0_rmse: 0.77038 | val_1_rmse: 0.76515 |  0:00:10s
epoch 31 | loss: 0.45354 | val_0_rmse: 0.77952 | val_1_rmse: 0.77674 |  0:00:11s
epoch 32 | loss: 0.44289 | val_0_rmse: 0.748   | val_1_rmse: 0.74452 |  0:00:11s
epoch 33 | loss: 0.45431 | val_0_rmse: 0.77217 | val_1_rmse: 0.76418 |  0:00:11s
epoch 34 | loss: 0.4381  | val_0_rmse: 0.74873 | val_1_rmse: 0.73552 |  0:00:12s
epoch 35 | loss: 0.42895 | val_0_rmse: 0.77766 | val_1_rmse: 0.76076 |  0:00:12s
epoch 36 | loss: 0.43943 | val_0_rmse: 0.74859 | val_1_rmse: 0.732   |  0:00:12s
epoch 37 | loss: 0.43434 | val_0_rmse: 0.76484 | val_1_rmse: 0.74379 |  0:00:13s
epoch 38 | loss: 0.42528 | val_0_rmse: 0.76396 | val_1_rmse: 0.72805 |  0:00:13s
epoch 39 | loss: 0.41916 | val_0_rmse: 0.79552 | val_1_rmse: 0.73503 |  0:00:14s
epoch 40 | loss: 0.41473 | val_0_rmse: 0.8263  | val_1_rmse: 0.73752 |  0:00:14s
epoch 41 | loss: 0.40975 | val_0_rmse: 0.7592  | val_1_rmse: 0.73414 |  0:00:14s
epoch 42 | loss: 0.41187 | val_0_rmse: 0.77726 | val_1_rmse: 0.75199 |  0:00:15s
epoch 43 | loss: 0.41502 | val_0_rmse: 0.74738 | val_1_rmse: 0.72579 |  0:00:15s
epoch 44 | loss: 0.4163  | val_0_rmse: 0.76181 | val_1_rmse: 0.74923 |  0:00:15s
epoch 45 | loss: 0.40988 | val_0_rmse: 0.74103 | val_1_rmse: 0.72506 |  0:00:16s
epoch 46 | loss: 0.40651 | val_0_rmse: 0.75595 | val_1_rmse: 0.74775 |  0:00:16s
epoch 47 | loss: 0.40955 | val_0_rmse: 0.74123 | val_1_rmse: 0.72932 |  0:00:16s
epoch 48 | loss: 0.40469 | val_0_rmse: 0.75803 | val_1_rmse: 0.74765 |  0:00:17s
epoch 49 | loss: 0.40331 | val_0_rmse: 0.73247 | val_1_rmse: 0.72279 |  0:00:17s
epoch 50 | loss: 0.39882 | val_0_rmse: 0.73321 | val_1_rmse: 0.72295 |  0:00:17s
epoch 51 | loss: 0.39137 | val_0_rmse: 0.72818 | val_1_rmse: 0.71758 |  0:00:18s
epoch 52 | loss: 0.39719 | val_0_rmse: 0.72745 | val_1_rmse: 0.71726 |  0:00:18s
epoch 53 | loss: 0.39408 | val_0_rmse: 0.73283 | val_1_rmse: 0.72347 |  0:00:19s
epoch 54 | loss: 0.39857 | val_0_rmse: 0.72828 | val_1_rmse: 0.71558 |  0:00:19s
epoch 55 | loss: 0.39425 | val_0_rmse: 0.72682 | val_1_rmse: 0.72282 |  0:00:19s
epoch 56 | loss: 0.39411 | val_0_rmse: 0.74211 | val_1_rmse: 0.7384  |  0:00:20s
epoch 57 | loss: 0.38783 | val_0_rmse: 0.71991 | val_1_rmse: 0.71591 |  0:00:20s
epoch 58 | loss: 0.3935  | val_0_rmse: 0.73109 | val_1_rmse: 0.72704 |  0:00:20s
epoch 59 | loss: 0.38614 | val_0_rmse: 0.71316 | val_1_rmse: 0.70234 |  0:00:21s
epoch 60 | loss: 0.38907 | val_0_rmse: 0.72297 | val_1_rmse: 0.71405 |  0:00:21s
epoch 61 | loss: 0.38995 | val_0_rmse: 0.70674 | val_1_rmse: 0.70155 |  0:00:21s
epoch 62 | loss: 0.38747 | val_0_rmse: 0.72323 | val_1_rmse: 0.72335 |  0:00:22s
epoch 63 | loss: 0.38911 | val_0_rmse: 0.71594 | val_1_rmse: 0.71278 |  0:00:22s
epoch 64 | loss: 0.38069 | val_0_rmse: 0.7128  | val_1_rmse: 0.70845 |  0:00:22s
epoch 65 | loss: 0.38954 | val_0_rmse: 0.73557 | val_1_rmse: 0.71995 |  0:00:23s
epoch 66 | loss: 0.38759 | val_0_rmse: 0.71604 | val_1_rmse: 0.70528 |  0:00:23s
epoch 67 | loss: 0.38115 | val_0_rmse: 0.70999 | val_1_rmse: 0.69718 |  0:00:23s
epoch 68 | loss: 0.38165 | val_0_rmse: 0.70337 | val_1_rmse: 0.69743 |  0:00:24s
epoch 69 | loss: 0.39356 | val_0_rmse: 0.70911 | val_1_rmse: 0.70612 |  0:00:24s
epoch 70 | loss: 0.38471 | val_0_rmse: 0.70578 | val_1_rmse: 0.70389 |  0:00:24s
epoch 71 | loss: 0.38793 | val_0_rmse: 0.70126 | val_1_rmse: 0.69929 |  0:00:25s
epoch 72 | loss: 0.39126 | val_0_rmse: 0.68958 | val_1_rmse: 0.68172 |  0:00:25s
epoch 73 | loss: 0.38068 | val_0_rmse: 0.70904 | val_1_rmse: 0.69063 |  0:00:25s
epoch 74 | loss: 0.38241 | val_0_rmse: 0.70497 | val_1_rmse: 0.68362 |  0:00:26s
epoch 75 | loss: 0.38689 | val_0_rmse: 0.7006  | val_1_rmse: 0.68372 |  0:00:26s
epoch 76 | loss: 0.38317 | val_0_rmse: 0.68968 | val_1_rmse: 0.67264 |  0:00:26s
epoch 77 | loss: 0.37975 | val_0_rmse: 0.68745 | val_1_rmse: 0.67552 |  0:00:27s
epoch 78 | loss: 0.38404 | val_0_rmse: 0.69108 | val_1_rmse: 0.68523 |  0:00:27s
epoch 79 | loss: 0.37514 | val_0_rmse: 0.67261 | val_1_rmse: 0.66997 |  0:00:27s
epoch 80 | loss: 0.38228 | val_0_rmse: 0.66906 | val_1_rmse: 0.66788 |  0:00:28s
epoch 81 | loss: 0.37852 | val_0_rmse: 0.65958 | val_1_rmse: 0.66166 |  0:00:28s
epoch 82 | loss: 0.37148 | val_0_rmse: 0.65934 | val_1_rmse: 0.66647 |  0:00:29s
epoch 83 | loss: 0.37124 | val_0_rmse: 0.65968 | val_1_rmse: 0.6616  |  0:00:29s
epoch 84 | loss: 0.36952 | val_0_rmse: 0.65058 | val_1_rmse: 0.65591 |  0:00:29s
epoch 85 | loss: 0.36489 | val_0_rmse: 0.64755 | val_1_rmse: 0.65822 |  0:00:30s
epoch 86 | loss: 0.36732 | val_0_rmse: 0.64544 | val_1_rmse: 0.65399 |  0:00:30s
epoch 87 | loss: 0.36343 | val_0_rmse: 0.64919 | val_1_rmse: 0.65509 |  0:00:30s
epoch 88 | loss: 0.36193 | val_0_rmse: 0.65278 | val_1_rmse: 0.65044 |  0:00:31s
epoch 89 | loss: 0.3631  | val_0_rmse: 0.64509 | val_1_rmse: 0.65424 |  0:00:31s
epoch 90 | loss: 0.367   | val_0_rmse: 0.65194 | val_1_rmse: 0.66512 |  0:00:31s
epoch 91 | loss: 0.36296 | val_0_rmse: 0.64652 | val_1_rmse: 0.65958 |  0:00:32s
epoch 92 | loss: 0.35675 | val_0_rmse: 0.64096 | val_1_rmse: 0.6525  |  0:00:32s
epoch 93 | loss: 0.36277 | val_0_rmse: 0.63503 | val_1_rmse: 0.6462  |  0:00:32s
epoch 94 | loss: 0.36057 | val_0_rmse: 0.63268 | val_1_rmse: 0.65114 |  0:00:33s
epoch 95 | loss: 0.35953 | val_0_rmse: 0.62833 | val_1_rmse: 0.65688 |  0:00:33s
epoch 96 | loss: 0.35955 | val_0_rmse: 0.62955 | val_1_rmse: 0.64829 |  0:00:33s
epoch 97 | loss: 0.36081 | val_0_rmse: 0.63212 | val_1_rmse: 0.65215 |  0:00:34s
epoch 98 | loss: 0.35545 | val_0_rmse: 0.62726 | val_1_rmse: 0.66606 |  0:00:34s
epoch 99 | loss: 0.36047 | val_0_rmse: 0.62914 | val_1_rmse: 0.65421 |  0:00:34s
epoch 100| loss: 0.36565 | val_0_rmse: 0.61877 | val_1_rmse: 0.6538  |  0:00:35s
epoch 101| loss: 0.3584  | val_0_rmse: 0.62426 | val_1_rmse: 0.65435 |  0:00:35s
epoch 102| loss: 0.36549 | val_0_rmse: 0.61461 | val_1_rmse: 0.6544  |  0:00:35s
epoch 103| loss: 0.36232 | val_0_rmse: 0.61023 | val_1_rmse: 0.64953 |  0:00:36s
epoch 104| loss: 0.35498 | val_0_rmse: 0.60799 | val_1_rmse: 0.64927 |  0:00:36s
epoch 105| loss: 0.35429 | val_0_rmse: 0.60638 | val_1_rmse: 0.65248 |  0:00:37s
epoch 106| loss: 0.35191 | val_0_rmse: 0.59778 | val_1_rmse: 0.64882 |  0:00:37s
epoch 107| loss: 0.34912 | val_0_rmse: 0.60231 | val_1_rmse: 0.65077 |  0:00:37s
epoch 108| loss: 0.34287 | val_0_rmse: 0.59657 | val_1_rmse: 0.65224 |  0:00:38s
epoch 109| loss: 0.34448 | val_0_rmse: 0.59385 | val_1_rmse: 0.6416  |  0:00:38s
epoch 110| loss: 0.34676 | val_0_rmse: 0.61337 | val_1_rmse: 0.70851 |  0:00:38s
epoch 111| loss: 0.34519 | val_0_rmse: 0.59482 | val_1_rmse: 0.64299 |  0:00:39s
epoch 112| loss: 0.35175 | val_0_rmse: 0.61281 | val_1_rmse: 0.63039 |  0:00:39s
epoch 113| loss: 0.35263 | val_0_rmse: 0.60525 | val_1_rmse: 0.62892 |  0:00:39s
epoch 114| loss: 0.35784 | val_0_rmse: 0.60198 | val_1_rmse: 0.63254 |  0:00:40s
epoch 115| loss: 0.34683 | val_0_rmse: 0.60056 | val_1_rmse: 0.62989 |  0:00:40s
epoch 116| loss: 0.34881 | val_0_rmse: 0.59874 | val_1_rmse: 0.62957 |  0:00:40s
epoch 117| loss: 0.34686 | val_0_rmse: 0.59501 | val_1_rmse: 0.6254  |  0:00:41s
epoch 118| loss: 0.35306 | val_0_rmse: 0.59306 | val_1_rmse: 0.62578 |  0:00:41s
epoch 119| loss: 0.34881 | val_0_rmse: 0.59038 | val_1_rmse: 0.62497 |  0:00:42s
epoch 120| loss: 0.34269 | val_0_rmse: 0.5914  | val_1_rmse: 0.63318 |  0:00:42s
epoch 121| loss: 0.3465  | val_0_rmse: 0.59006 | val_1_rmse: 0.63801 |  0:00:42s
epoch 122| loss: 0.3418  | val_0_rmse: 0.58481 | val_1_rmse: 0.63261 |  0:00:43s
epoch 123| loss: 0.34619 | val_0_rmse: 0.58752 | val_1_rmse: 0.63052 |  0:00:43s
epoch 124| loss: 0.35079 | val_0_rmse: 0.58442 | val_1_rmse: 0.62956 |  0:00:43s
epoch 125| loss: 0.34903 | val_0_rmse: 0.59156 | val_1_rmse: 0.63876 |  0:00:44s
epoch 126| loss: 0.34161 | val_0_rmse: 0.58189 | val_1_rmse: 0.63325 |  0:00:44s
epoch 127| loss: 0.34339 | val_0_rmse: 0.57888 | val_1_rmse: 0.63452 |  0:00:44s
epoch 128| loss: 0.34614 | val_0_rmse: 0.57449 | val_1_rmse: 0.62977 |  0:00:45s
epoch 129| loss: 0.34394 | val_0_rmse: 0.57616 | val_1_rmse: 0.6254  |  0:00:45s
epoch 130| loss: 0.33852 | val_0_rmse: 0.57242 | val_1_rmse: 0.62271 |  0:00:45s
epoch 131| loss: 0.34633 | val_0_rmse: 0.58799 | val_1_rmse: 0.63987 |  0:00:46s
epoch 132| loss: 0.34335 | val_0_rmse: 0.57611 | val_1_rmse: 0.62409 |  0:00:46s
epoch 133| loss: 0.34533 | val_0_rmse: 0.58864 | val_1_rmse: 0.63536 |  0:00:46s
epoch 134| loss: 0.34791 | val_0_rmse: 0.59878 | val_1_rmse: 0.63482 |  0:00:47s
epoch 135| loss: 0.34962 | val_0_rmse: 0.59472 | val_1_rmse: 0.6425  |  0:00:47s
epoch 136| loss: 0.34724 | val_0_rmse: 0.58389 | val_1_rmse: 0.62623 |  0:00:47s
epoch 137| loss: 0.34552 | val_0_rmse: 0.5747  | val_1_rmse: 0.63676 |  0:00:48s
epoch 138| loss: 0.33162 | val_0_rmse: 0.57109 | val_1_rmse: 0.62495 |  0:00:48s
epoch 139| loss: 0.33686 | val_0_rmse: 0.56742 | val_1_rmse: 0.62374 |  0:00:48s
epoch 140| loss: 0.3367  | val_0_rmse: 0.57504 | val_1_rmse: 0.62898 |  0:00:49s
epoch 141| loss: 0.33941 | val_0_rmse: 0.57883 | val_1_rmse: 0.62796 |  0:00:49s
epoch 142| loss: 0.33931 | val_0_rmse: 0.56935 | val_1_rmse: 0.61951 |  0:00:49s
epoch 143| loss: 0.32683 | val_0_rmse: 0.57773 | val_1_rmse: 0.62171 |  0:00:50s
epoch 144| loss: 0.34086 | val_0_rmse: 0.56884 | val_1_rmse: 0.62522 |  0:00:50s
epoch 145| loss: 0.33555 | val_0_rmse: 0.5721  | val_1_rmse: 0.62455 |  0:00:50s
epoch 146| loss: 0.33157 | val_0_rmse: 0.56717 | val_1_rmse: 0.62053 |  0:00:51s
epoch 147| loss: 0.33096 | val_0_rmse: 0.56489 | val_1_rmse: 0.6167  |  0:00:51s
epoch 148| loss: 0.32426 | val_0_rmse: 0.56339 | val_1_rmse: 0.61867 |  0:00:51s
epoch 149| loss: 0.32449 | val_0_rmse: 0.55927 | val_1_rmse: 0.61931 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.6167
Best weights from best epoch are automatically used!
ended training at: 17:28:39
Feature importance:
Mean squared error is of 2900001258.951097
Mean absolute error:34962.729733738335
MAPE:0.28546616130549796
R2 score:0.6541600333045525
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:28:39
epoch 0  | loss: 2.97291 | val_0_rmse: 0.99465 | val_1_rmse: 1.05797 |  0:00:00s
epoch 1  | loss: 1.51927 | val_0_rmse: 0.95231 | val_1_rmse: 1.00819 |  0:00:00s
epoch 2  | loss: 1.03605 | val_0_rmse: 0.92687 | val_1_rmse: 0.98128 |  0:00:01s
epoch 3  | loss: 0.81511 | val_0_rmse: 0.82452 | val_1_rmse: 0.87919 |  0:00:01s
epoch 4  | loss: 0.71743 | val_0_rmse: 0.82359 | val_1_rmse: 0.86958 |  0:00:01s
epoch 5  | loss: 0.64711 | val_0_rmse: 0.8092  | val_1_rmse: 0.84514 |  0:00:02s
epoch 6  | loss: 0.61757 | val_0_rmse: 0.77903 | val_1_rmse: 0.81963 |  0:00:02s
epoch 7  | loss: 0.57342 | val_0_rmse: 0.79216 | val_1_rmse: 0.82711 |  0:00:02s
epoch 8  | loss: 0.52721 | val_0_rmse: 0.78121 | val_1_rmse: 0.81794 |  0:00:03s
epoch 9  | loss: 0.51498 | val_0_rmse: 0.828   | val_1_rmse: 0.86381 |  0:00:03s
epoch 10 | loss: 0.50571 | val_0_rmse: 0.82868 | val_1_rmse: 0.86176 |  0:00:04s
epoch 11 | loss: 0.49269 | val_0_rmse: 0.83389 | val_1_rmse: 0.86333 |  0:00:04s
epoch 12 | loss: 0.48197 | val_0_rmse: 0.84763 | val_1_rmse: 0.87587 |  0:00:04s
epoch 13 | loss: 0.47538 | val_0_rmse: 0.84984 | val_1_rmse: 0.88041 |  0:00:05s
epoch 14 | loss: 0.4766  | val_0_rmse: 0.81351 | val_1_rmse: 0.84995 |  0:00:05s
epoch 15 | loss: 0.4678  | val_0_rmse: 0.78496 | val_1_rmse: 0.82017 |  0:00:05s
epoch 16 | loss: 0.46605 | val_0_rmse: 0.79979 | val_1_rmse: 0.83631 |  0:00:06s
epoch 17 | loss: 0.46112 | val_0_rmse: 0.79452 | val_1_rmse: 0.8336  |  0:00:06s
epoch 18 | loss: 0.45266 | val_0_rmse: 0.78999 | val_1_rmse: 0.82494 |  0:00:06s
epoch 19 | loss: 0.45168 | val_0_rmse: 0.78798 | val_1_rmse: 0.82386 |  0:00:07s
epoch 20 | loss: 0.44894 | val_0_rmse: 0.79209 | val_1_rmse: 0.8297  |  0:00:07s
epoch 21 | loss: 0.443   | val_0_rmse: 0.79038 | val_1_rmse: 0.82572 |  0:00:07s
epoch 22 | loss: 0.44314 | val_0_rmse: 0.78724 | val_1_rmse: 0.82617 |  0:00:08s
epoch 23 | loss: 0.43559 | val_0_rmse: 0.79285 | val_1_rmse: 0.83208 |  0:00:08s
epoch 24 | loss: 0.4261  | val_0_rmse: 0.80552 | val_1_rmse: 0.84568 |  0:00:08s
epoch 25 | loss: 0.42724 | val_0_rmse: 0.78587 | val_1_rmse: 0.82247 |  0:00:09s
epoch 26 | loss: 0.43054 | val_0_rmse: 0.78781 | val_1_rmse: 0.82433 |  0:00:09s
epoch 27 | loss: 0.42137 | val_0_rmse: 0.78482 | val_1_rmse: 0.82004 |  0:00:09s
epoch 28 | loss: 0.42203 | val_0_rmse: 0.80297 | val_1_rmse: 0.84132 |  0:00:10s
epoch 29 | loss: 0.42015 | val_0_rmse: 0.78591 | val_1_rmse: 0.82686 |  0:00:10s
epoch 30 | loss: 0.41749 | val_0_rmse: 0.7935  | val_1_rmse: 0.83566 |  0:00:10s
epoch 31 | loss: 0.41272 | val_0_rmse: 0.78286 | val_1_rmse: 0.82943 |  0:00:11s
epoch 32 | loss: 0.40771 | val_0_rmse: 0.78046 | val_1_rmse: 0.82421 |  0:00:11s
epoch 33 | loss: 0.40175 | val_0_rmse: 0.76887 | val_1_rmse: 0.81571 |  0:00:11s
epoch 34 | loss: 0.40249 | val_0_rmse: 0.78455 | val_1_rmse: 0.82971 |  0:00:12s
epoch 35 | loss: 0.39903 | val_0_rmse: 0.76548 | val_1_rmse: 0.81052 |  0:00:12s
epoch 36 | loss: 0.39519 | val_0_rmse: 0.75586 | val_1_rmse: 0.80145 |  0:00:12s
epoch 37 | loss: 0.40384 | val_0_rmse: 0.77124 | val_1_rmse: 0.81679 |  0:00:13s
epoch 38 | loss: 0.40345 | val_0_rmse: 0.75959 | val_1_rmse: 0.80287 |  0:00:13s
epoch 39 | loss: 0.3928  | val_0_rmse: 0.76613 | val_1_rmse: 0.80899 |  0:00:13s
epoch 40 | loss: 0.39441 | val_0_rmse: 0.76468 | val_1_rmse: 0.80842 |  0:00:14s
epoch 41 | loss: 0.39219 | val_0_rmse: 0.76659 | val_1_rmse: 0.8053  |  0:00:14s
epoch 42 | loss: 0.3928  | val_0_rmse: 0.768   | val_1_rmse: 0.81015 |  0:00:14s
epoch 43 | loss: 0.38722 | val_0_rmse: 0.76373 | val_1_rmse: 0.80637 |  0:00:15s
epoch 44 | loss: 0.38894 | val_0_rmse: 0.76101 | val_1_rmse: 0.80604 |  0:00:15s
epoch 45 | loss: 0.39094 | val_0_rmse: 0.76632 | val_1_rmse: 0.8037  |  0:00:16s
epoch 46 | loss: 0.38911 | val_0_rmse: 0.74219 | val_1_rmse: 0.78118 |  0:00:16s
epoch 47 | loss: 0.38656 | val_0_rmse: 0.73772 | val_1_rmse: 0.78154 |  0:00:16s
epoch 48 | loss: 0.39988 | val_0_rmse: 0.73471 | val_1_rmse: 0.77443 |  0:00:17s
epoch 49 | loss: 0.39044 | val_0_rmse: 0.72673 | val_1_rmse: 0.76703 |  0:00:17s
epoch 50 | loss: 0.39298 | val_0_rmse: 0.73541 | val_1_rmse: 0.77853 |  0:00:17s
epoch 51 | loss: 0.39303 | val_0_rmse: 0.73925 | val_1_rmse: 0.78244 |  0:00:18s
epoch 52 | loss: 0.39341 | val_0_rmse: 0.71913 | val_1_rmse: 0.76389 |  0:00:18s
epoch 53 | loss: 0.40058 | val_0_rmse: 0.73171 | val_1_rmse: 0.78089 |  0:00:18s
epoch 54 | loss: 0.40715 | val_0_rmse: 0.72869 | val_1_rmse: 0.77433 |  0:00:19s
epoch 55 | loss: 0.40869 | val_0_rmse: 0.72604 | val_1_rmse: 0.77044 |  0:00:19s
epoch 56 | loss: 0.40494 | val_0_rmse: 0.72376 | val_1_rmse: 0.76977 |  0:00:19s
epoch 57 | loss: 0.4082  | val_0_rmse: 0.72576 | val_1_rmse: 0.77865 |  0:00:20s
epoch 58 | loss: 0.41597 | val_0_rmse: 0.7207  | val_1_rmse: 0.77172 |  0:00:20s
epoch 59 | loss: 0.41756 | val_0_rmse: 0.73516 | val_1_rmse: 0.78512 |  0:00:21s
epoch 60 | loss: 0.40491 | val_0_rmse: 0.72506 | val_1_rmse: 0.77397 |  0:00:21s
epoch 61 | loss: 0.39409 | val_0_rmse: 0.71924 | val_1_rmse: 0.76723 |  0:00:21s
epoch 62 | loss: 0.40331 | val_0_rmse: 0.71135 | val_1_rmse: 0.76464 |  0:00:22s
epoch 63 | loss: 0.39752 | val_0_rmse: 0.70882 | val_1_rmse: 0.7601  |  0:00:22s
epoch 64 | loss: 0.40247 | val_0_rmse: 0.70816 | val_1_rmse: 0.76194 |  0:00:22s
epoch 65 | loss: 0.40903 | val_0_rmse: 0.71693 | val_1_rmse: 0.77358 |  0:00:23s
epoch 66 | loss: 0.4083  | val_0_rmse: 0.71113 | val_1_rmse: 0.76598 |  0:00:23s
epoch 67 | loss: 0.41397 | val_0_rmse: 0.70468 | val_1_rmse: 0.75652 |  0:00:23s
epoch 68 | loss: 0.42561 | val_0_rmse: 0.71024 | val_1_rmse: 0.7614  |  0:00:24s
epoch 69 | loss: 0.41673 | val_0_rmse: 0.70686 | val_1_rmse: 0.75621 |  0:00:24s
epoch 70 | loss: 0.41487 | val_0_rmse: 0.69751 | val_1_rmse: 0.74283 |  0:00:24s
epoch 71 | loss: 0.40651 | val_0_rmse: 0.68942 | val_1_rmse: 0.74239 |  0:00:25s
epoch 72 | loss: 0.40235 | val_0_rmse: 0.70462 | val_1_rmse: 0.76508 |  0:00:25s
epoch 73 | loss: 0.40339 | val_0_rmse: 0.68654 | val_1_rmse: 0.75772 |  0:00:25s
epoch 74 | loss: 0.40629 | val_0_rmse: 0.69317 | val_1_rmse: 0.76289 |  0:00:26s
epoch 75 | loss: 0.39057 | val_0_rmse: 0.67441 | val_1_rmse: 0.74067 |  0:00:26s
epoch 76 | loss: 0.39304 | val_0_rmse: 0.68877 | val_1_rmse: 0.7533  |  0:00:26s
epoch 77 | loss: 0.39325 | val_0_rmse: 0.67734 | val_1_rmse: 0.73887 |  0:00:27s
epoch 78 | loss: 0.38676 | val_0_rmse: 0.6829  | val_1_rmse: 0.75365 |  0:00:27s
epoch 79 | loss: 0.38833 | val_0_rmse: 0.6735  | val_1_rmse: 0.73968 |  0:00:27s
epoch 80 | loss: 0.39835 | val_0_rmse: 0.67192 | val_1_rmse: 0.73144 |  0:00:28s
epoch 81 | loss: 0.39283 | val_0_rmse: 0.65925 | val_1_rmse: 0.72299 |  0:00:28s
epoch 82 | loss: 0.38373 | val_0_rmse: 0.67055 | val_1_rmse: 0.73745 |  0:00:28s
epoch 83 | loss: 0.38279 | val_0_rmse: 0.66352 | val_1_rmse: 0.73213 |  0:00:29s
epoch 84 | loss: 0.38265 | val_0_rmse: 0.67148 | val_1_rmse: 0.7417  |  0:00:29s
epoch 85 | loss: 0.37861 | val_0_rmse: 0.6638  | val_1_rmse: 0.73719 |  0:00:29s
epoch 86 | loss: 0.38023 | val_0_rmse: 0.66739 | val_1_rmse: 0.73466 |  0:00:30s
epoch 87 | loss: 0.37478 | val_0_rmse: 0.65287 | val_1_rmse: 0.72434 |  0:00:30s
epoch 88 | loss: 0.37416 | val_0_rmse: 0.65365 | val_1_rmse: 0.73004 |  0:00:30s
epoch 89 | loss: 0.37776 | val_0_rmse: 0.64343 | val_1_rmse: 0.72519 |  0:00:31s
epoch 90 | loss: 0.37556 | val_0_rmse: 0.64626 | val_1_rmse: 0.73391 |  0:00:31s
epoch 91 | loss: 0.37961 | val_0_rmse: 0.64273 | val_1_rmse: 0.73035 |  0:00:32s
epoch 92 | loss: 0.36446 | val_0_rmse: 0.63812 | val_1_rmse: 0.7207  |  0:00:32s
epoch 93 | loss: 0.3651  | val_0_rmse: 0.63963 | val_1_rmse: 0.72128 |  0:00:32s
epoch 94 | loss: 0.365   | val_0_rmse: 0.63218 | val_1_rmse: 0.71278 |  0:00:33s
epoch 95 | loss: 0.36287 | val_0_rmse: 0.63119 | val_1_rmse: 0.71966 |  0:00:33s
epoch 96 | loss: 0.36819 | val_0_rmse: 0.68604 | val_1_rmse: 0.73069 |  0:00:33s
epoch 97 | loss: 0.36637 | val_0_rmse: 0.66187 | val_1_rmse: 0.71768 |  0:00:34s
epoch 98 | loss: 0.36458 | val_0_rmse: 0.64183 | val_1_rmse: 0.71406 |  0:00:34s
epoch 99 | loss: 0.37137 | val_0_rmse: 0.6273  | val_1_rmse: 0.71069 |  0:00:34s
epoch 100| loss: 0.36651 | val_0_rmse: 0.62451 | val_1_rmse: 0.70753 |  0:00:35s
epoch 101| loss: 0.36214 | val_0_rmse: 0.62263 | val_1_rmse: 0.71047 |  0:00:35s
epoch 102| loss: 0.36143 | val_0_rmse: 0.61802 | val_1_rmse: 0.70867 |  0:00:35s
epoch 103| loss: 0.36224 | val_0_rmse: 0.66284 | val_1_rmse: 0.74238 |  0:00:36s
epoch 104| loss: 0.36783 | val_0_rmse: 0.65701 | val_1_rmse: 0.73408 |  0:00:36s
epoch 105| loss: 0.36207 | val_0_rmse: 0.61924 | val_1_rmse: 0.70705 |  0:00:36s
epoch 106| loss: 0.36587 | val_0_rmse: 0.61335 | val_1_rmse: 0.69956 |  0:00:37s
epoch 107| loss: 0.36047 | val_0_rmse: 0.60556 | val_1_rmse: 0.69632 |  0:00:37s
epoch 108| loss: 0.35769 | val_0_rmse: 0.60966 | val_1_rmse: 0.70122 |  0:00:38s
epoch 109| loss: 0.36429 | val_0_rmse: 0.61017 | val_1_rmse: 0.70317 |  0:00:38s
epoch 110| loss: 0.36403 | val_0_rmse: 0.60968 | val_1_rmse: 0.70061 |  0:00:38s
epoch 111| loss: 0.36064 | val_0_rmse: 0.60842 | val_1_rmse: 0.69697 |  0:00:39s
epoch 112| loss: 0.35932 | val_0_rmse: 0.60841 | val_1_rmse: 0.70894 |  0:00:39s
epoch 113| loss: 0.35787 | val_0_rmse: 0.60291 | val_1_rmse: 0.70329 |  0:00:39s
epoch 114| loss: 0.35811 | val_0_rmse: 0.59745 | val_1_rmse: 0.69057 |  0:00:40s
epoch 115| loss: 0.36316 | val_0_rmse: 0.60531 | val_1_rmse: 0.69472 |  0:00:40s
epoch 116| loss: 0.35727 | val_0_rmse: 0.60086 | val_1_rmse: 0.68817 |  0:00:40s
epoch 117| loss: 0.36189 | val_0_rmse: 0.59893 | val_1_rmse: 0.69164 |  0:00:41s
epoch 118| loss: 0.35677 | val_0_rmse: 0.59281 | val_1_rmse: 0.69238 |  0:00:41s
epoch 119| loss: 0.35416 | val_0_rmse: 0.59089 | val_1_rmse: 0.69643 |  0:00:41s
epoch 120| loss: 0.35003 | val_0_rmse: 0.59089 | val_1_rmse: 0.7016  |  0:00:42s
epoch 121| loss: 0.35463 | val_0_rmse: 0.59062 | val_1_rmse: 0.70626 |  0:00:42s
epoch 122| loss: 0.3652  | val_0_rmse: 0.59388 | val_1_rmse: 0.70753 |  0:00:42s
epoch 123| loss: 0.35376 | val_0_rmse: 0.59418 | val_1_rmse: 0.70424 |  0:00:43s
epoch 124| loss: 0.35928 | val_0_rmse: 0.59394 | val_1_rmse: 0.70225 |  0:00:43s
epoch 125| loss: 0.35839 | val_0_rmse: 0.59593 | val_1_rmse: 0.70384 |  0:00:43s
epoch 126| loss: 0.35397 | val_0_rmse: 0.59609 | val_1_rmse: 0.71125 |  0:00:44s
epoch 127| loss: 0.37419 | val_0_rmse: 0.62444 | val_1_rmse: 0.72638 |  0:00:44s
epoch 128| loss: 0.38093 | val_0_rmse: 0.61382 | val_1_rmse: 0.71551 |  0:00:44s
epoch 129| loss: 0.37258 | val_0_rmse: 0.60472 | val_1_rmse: 0.71209 |  0:00:45s
epoch 130| loss: 0.37237 | val_0_rmse: 0.60054 | val_1_rmse: 0.70781 |  0:00:45s
epoch 131| loss: 0.35997 | val_0_rmse: 0.60217 | val_1_rmse: 0.70561 |  0:00:45s
epoch 132| loss: 0.36104 | val_0_rmse: 0.59687 | val_1_rmse: 0.70029 |  0:00:46s
epoch 133| loss: 0.35833 | val_0_rmse: 0.59215 | val_1_rmse: 0.69258 |  0:00:46s
epoch 134| loss: 0.35933 | val_0_rmse: 0.59035 | val_1_rmse: 0.69176 |  0:00:46s
epoch 135| loss: 0.35411 | val_0_rmse: 0.5886  | val_1_rmse: 0.69311 |  0:00:47s
epoch 136| loss: 0.3533  | val_0_rmse: 0.58755 | val_1_rmse: 0.69768 |  0:00:47s
epoch 137| loss: 0.3571  | val_0_rmse: 0.5895  | val_1_rmse: 0.69561 |  0:00:47s
epoch 138| loss: 0.35329 | val_0_rmse: 0.58516 | val_1_rmse: 0.69425 |  0:00:48s
epoch 139| loss: 0.35602 | val_0_rmse: 0.58942 | val_1_rmse: 0.6974  |  0:00:48s
epoch 140| loss: 0.35966 | val_0_rmse: 0.59277 | val_1_rmse: 0.69867 |  0:00:49s
epoch 141| loss: 0.3581  | val_0_rmse: 0.59208 | val_1_rmse: 0.69869 |  0:00:49s
epoch 142| loss: 0.35709 | val_0_rmse: 0.58628 | val_1_rmse: 0.69239 |  0:00:49s
epoch 143| loss: 0.35025 | val_0_rmse: 0.58238 | val_1_rmse: 0.69309 |  0:00:50s
epoch 144| loss: 0.35098 | val_0_rmse: 0.58036 | val_1_rmse: 0.69369 |  0:00:50s
epoch 145| loss: 0.34717 | val_0_rmse: 0.57899 | val_1_rmse: 0.69411 |  0:00:50s
epoch 146| loss: 0.34785 | val_0_rmse: 0.58092 | val_1_rmse: 0.69151 |  0:00:51s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.68817
Best weights from best epoch are automatically used!
ended training at: 17:29:30
Feature importance:
Mean squared error is of 3095635958.930726
Mean absolute error:36558.18663327674
MAPE:0.28835242486802287
R2 score:0.6410241463230266
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:29:31
epoch 0  | loss: 3.09049 | val_0_rmse: 1.02005 | val_1_rmse: 1.00354 |  0:00:00s
epoch 1  | loss: 1.49588 | val_0_rmse: 0.9488  | val_1_rmse: 0.92731 |  0:00:00s
epoch 2  | loss: 1.01179 | val_0_rmse: 0.85782 | val_1_rmse: 0.86374 |  0:00:01s
epoch 3  | loss: 0.83176 | val_0_rmse: 0.83622 | val_1_rmse: 0.85341 |  0:00:01s
epoch 4  | loss: 0.79153 | val_0_rmse: 0.87382 | val_1_rmse: 0.90172 |  0:00:01s
epoch 5  | loss: 0.7009  | val_0_rmse: 0.88682 | val_1_rmse: 0.90279 |  0:00:02s
epoch 6  | loss: 0.64828 | val_0_rmse: 0.82561 | val_1_rmse: 0.83371 |  0:00:02s
epoch 7  | loss: 0.59165 | val_0_rmse: 0.90089 | val_1_rmse: 0.91795 |  0:00:02s
epoch 8  | loss: 0.55114 | val_0_rmse: 0.89931 | val_1_rmse: 0.90577 |  0:00:03s
epoch 9  | loss: 0.51574 | val_0_rmse: 0.86469 | val_1_rmse: 0.87618 |  0:00:03s
epoch 10 | loss: 0.4988  | val_0_rmse: 0.85843 | val_1_rmse: 0.86603 |  0:00:03s
epoch 11 | loss: 0.4877  | val_0_rmse: 0.85545 | val_1_rmse: 0.86786 |  0:00:04s
epoch 12 | loss: 0.47484 | val_0_rmse: 0.8291  | val_1_rmse: 0.84294 |  0:00:04s
epoch 13 | loss: 0.46686 | val_0_rmse: 0.84075 | val_1_rmse: 0.84994 |  0:00:05s
epoch 14 | loss: 0.45935 | val_0_rmse: 0.79153 | val_1_rmse: 0.8051  |  0:00:05s
epoch 15 | loss: 0.43776 | val_0_rmse: 0.79108 | val_1_rmse: 0.80872 |  0:00:05s
epoch 16 | loss: 0.44457 | val_0_rmse: 0.76792 | val_1_rmse: 0.78018 |  0:00:06s
epoch 17 | loss: 0.44867 | val_0_rmse: 0.76901 | val_1_rmse: 0.78551 |  0:00:06s
epoch 18 | loss: 0.43859 | val_0_rmse: 0.77974 | val_1_rmse: 0.7931  |  0:00:06s
epoch 19 | loss: 0.43467 | val_0_rmse: 0.79342 | val_1_rmse: 0.8047  |  0:00:07s
epoch 20 | loss: 0.43087 | val_0_rmse: 0.7721  | val_1_rmse: 0.7821  |  0:00:07s
epoch 21 | loss: 0.43731 | val_0_rmse: 0.81467 | val_1_rmse: 0.83707 |  0:00:07s
epoch 22 | loss: 0.4268  | val_0_rmse: 0.76613 | val_1_rmse: 0.78618 |  0:00:08s
epoch 23 | loss: 0.41742 | val_0_rmse: 0.75907 | val_1_rmse: 0.78031 |  0:00:08s
epoch 24 | loss: 0.41491 | val_0_rmse: 0.74948 | val_1_rmse: 0.77833 |  0:00:08s
epoch 25 | loss: 0.41981 | val_0_rmse: 0.77413 | val_1_rmse: 0.79294 |  0:00:09s
epoch 26 | loss: 0.41024 | val_0_rmse: 0.76972 | val_1_rmse: 0.78648 |  0:00:09s
epoch 27 | loss: 0.41278 | val_0_rmse: 0.7787  | val_1_rmse: 0.78737 |  0:00:09s
epoch 28 | loss: 0.40703 | val_0_rmse: 0.74614 | val_1_rmse: 0.75741 |  0:00:10s
epoch 29 | loss: 0.40939 | val_0_rmse: 0.75368 | val_1_rmse: 0.75826 |  0:00:10s
epoch 30 | loss: 0.39961 | val_0_rmse: 0.74694 | val_1_rmse: 0.75336 |  0:00:10s
epoch 31 | loss: 0.40241 | val_0_rmse: 0.75498 | val_1_rmse: 0.76737 |  0:00:11s
epoch 32 | loss: 0.39023 | val_0_rmse: 0.7643  | val_1_rmse: 0.77097 |  0:00:11s
epoch 33 | loss: 0.39392 | val_0_rmse: 0.76569 | val_1_rmse: 0.7788  |  0:00:11s
epoch 34 | loss: 0.38988 | val_0_rmse: 0.75021 | val_1_rmse: 0.7608  |  0:00:12s
epoch 35 | loss: 0.39247 | val_0_rmse: 0.75242 | val_1_rmse: 0.7733  |  0:00:12s
epoch 36 | loss: 0.38297 | val_0_rmse: 0.7508  | val_1_rmse: 0.76935 |  0:00:12s
epoch 37 | loss: 0.38767 | val_0_rmse: 0.75688 | val_1_rmse: 0.77244 |  0:00:13s
epoch 38 | loss: 0.37553 | val_0_rmse: 0.74068 | val_1_rmse: 0.76011 |  0:00:13s
epoch 39 | loss: 0.38217 | val_0_rmse: 0.74088 | val_1_rmse: 0.75548 |  0:00:13s
epoch 40 | loss: 0.37547 | val_0_rmse: 0.73283 | val_1_rmse: 0.75116 |  0:00:14s
epoch 41 | loss: 0.37761 | val_0_rmse: 0.73985 | val_1_rmse: 0.75282 |  0:00:14s
epoch 42 | loss: 0.3833  | val_0_rmse: 0.73165 | val_1_rmse: 0.75254 |  0:00:15s
epoch 43 | loss: 0.3807  | val_0_rmse: 0.7245  | val_1_rmse: 0.74275 |  0:00:15s
epoch 44 | loss: 0.37295 | val_0_rmse: 0.73916 | val_1_rmse: 0.74981 |  0:00:15s
epoch 45 | loss: 0.37473 | val_0_rmse: 0.73554 | val_1_rmse: 0.75485 |  0:00:16s
epoch 46 | loss: 0.36951 | val_0_rmse: 0.73245 | val_1_rmse: 0.75104 |  0:00:16s
epoch 47 | loss: 0.37094 | val_0_rmse: 0.72447 | val_1_rmse: 0.74851 |  0:00:16s
epoch 48 | loss: 0.36557 | val_0_rmse: 0.72884 | val_1_rmse: 0.75174 |  0:00:17s
epoch 49 | loss: 0.36281 | val_0_rmse: 0.73194 | val_1_rmse: 0.75418 |  0:00:17s
epoch 50 | loss: 0.36629 | val_0_rmse: 0.71245 | val_1_rmse: 0.73893 |  0:00:17s
epoch 51 | loss: 0.36951 | val_0_rmse: 0.72212 | val_1_rmse: 0.7436  |  0:00:18s
epoch 52 | loss: 0.37111 | val_0_rmse: 0.69806 | val_1_rmse: 0.72711 |  0:00:18s
epoch 53 | loss: 0.36873 | val_0_rmse: 0.7208  | val_1_rmse: 0.75252 |  0:00:18s
epoch 54 | loss: 0.37368 | val_0_rmse: 0.70664 | val_1_rmse: 0.7354  |  0:00:19s
epoch 55 | loss: 0.36108 | val_0_rmse: 0.70847 | val_1_rmse: 0.73971 |  0:00:19s
epoch 56 | loss: 0.36825 | val_0_rmse: 0.69851 | val_1_rmse: 0.72974 |  0:00:20s
epoch 57 | loss: 0.36274 | val_0_rmse: 0.71575 | val_1_rmse: 0.75128 |  0:00:20s
epoch 58 | loss: 0.36396 | val_0_rmse: 0.69846 | val_1_rmse: 0.73581 |  0:00:20s
epoch 59 | loss: 0.36129 | val_0_rmse: 0.69874 | val_1_rmse: 0.73247 |  0:00:21s
epoch 60 | loss: 0.36085 | val_0_rmse: 0.69772 | val_1_rmse: 0.73295 |  0:00:21s
epoch 61 | loss: 0.35877 | val_0_rmse: 0.69539 | val_1_rmse: 0.7315  |  0:00:21s
epoch 62 | loss: 0.36394 | val_0_rmse: 0.69215 | val_1_rmse: 0.7302  |  0:00:22s
epoch 63 | loss: 0.36203 | val_0_rmse: 0.68655 | val_1_rmse: 0.72499 |  0:00:22s
epoch 64 | loss: 0.35102 | val_0_rmse: 0.68363 | val_1_rmse: 0.7234  |  0:00:22s
epoch 65 | loss: 0.35667 | val_0_rmse: 0.68039 | val_1_rmse: 0.72225 |  0:00:23s
epoch 66 | loss: 0.34956 | val_0_rmse: 0.68511 | val_1_rmse: 0.72317 |  0:00:23s
epoch 67 | loss: 0.35633 | val_0_rmse: 0.68698 | val_1_rmse: 0.71791 |  0:00:23s
epoch 68 | loss: 0.34771 | val_0_rmse: 0.69053 | val_1_rmse: 0.72442 |  0:00:24s
epoch 69 | loss: 0.35055 | val_0_rmse: 0.6688  | val_1_rmse: 0.70884 |  0:00:24s
epoch 70 | loss: 0.34787 | val_0_rmse: 0.67702 | val_1_rmse: 0.70909 |  0:00:24s
epoch 71 | loss: 0.35935 | val_0_rmse: 0.70109 | val_1_rmse: 0.71515 |  0:00:25s
epoch 72 | loss: 0.37567 | val_0_rmse: 0.70068 | val_1_rmse: 0.72133 |  0:00:25s
epoch 73 | loss: 0.3716  | val_0_rmse: 0.68761 | val_1_rmse: 0.71402 |  0:00:25s
epoch 74 | loss: 0.36786 | val_0_rmse: 0.667   | val_1_rmse: 0.69788 |  0:00:26s
epoch 75 | loss: 0.36232 | val_0_rmse: 0.66607 | val_1_rmse: 0.69833 |  0:00:26s
epoch 76 | loss: 0.36107 | val_0_rmse: 0.66298 | val_1_rmse: 0.70096 |  0:00:26s
epoch 77 | loss: 0.37122 | val_0_rmse: 0.65813 | val_1_rmse: 0.69463 |  0:00:27s
epoch 78 | loss: 0.36175 | val_0_rmse: 0.65404 | val_1_rmse: 0.69475 |  0:00:27s
epoch 79 | loss: 0.35599 | val_0_rmse: 0.6573  | val_1_rmse: 0.70011 |  0:00:27s
epoch 80 | loss: 0.35821 | val_0_rmse: 0.65577 | val_1_rmse: 0.70809 |  0:00:28s
epoch 81 | loss: 0.35995 | val_0_rmse: 0.66737 | val_1_rmse: 0.69832 |  0:00:28s
epoch 82 | loss: 0.36082 | val_0_rmse: 0.65886 | val_1_rmse: 0.69995 |  0:00:28s
epoch 83 | loss: 0.36111 | val_0_rmse: 0.65981 | val_1_rmse: 0.6953  |  0:00:29s
epoch 84 | loss: 0.35453 | val_0_rmse: 0.6512  | val_1_rmse: 0.69018 |  0:00:29s
epoch 85 | loss: 0.35404 | val_0_rmse: 0.64844 | val_1_rmse: 0.69956 |  0:00:30s
epoch 86 | loss: 0.34992 | val_0_rmse: 0.64189 | val_1_rmse: 0.69541 |  0:00:30s
epoch 87 | loss: 0.35187 | val_0_rmse: 0.64174 | val_1_rmse: 0.69742 |  0:00:30s
epoch 88 | loss: 0.3461  | val_0_rmse: 0.63555 | val_1_rmse: 0.69657 |  0:00:31s
epoch 89 | loss: 0.34789 | val_0_rmse: 0.63895 | val_1_rmse: 0.69565 |  0:00:31s
epoch 90 | loss: 0.34915 | val_0_rmse: 0.63908 | val_1_rmse: 0.69651 |  0:00:31s
epoch 91 | loss: 0.34599 | val_0_rmse: 0.63066 | val_1_rmse: 0.69042 |  0:00:32s
epoch 92 | loss: 0.338   | val_0_rmse: 0.63395 | val_1_rmse: 0.69832 |  0:00:32s
epoch 93 | loss: 0.34069 | val_0_rmse: 0.62905 | val_1_rmse: 0.69947 |  0:00:32s
epoch 94 | loss: 0.33716 | val_0_rmse: 0.62547 | val_1_rmse: 0.70312 |  0:00:33s
epoch 95 | loss: 0.34099 | val_0_rmse: 0.61207 | val_1_rmse: 0.69094 |  0:00:33s
epoch 96 | loss: 0.34284 | val_0_rmse: 0.61417 | val_1_rmse: 0.68727 |  0:00:33s
epoch 97 | loss: 0.3362  | val_0_rmse: 0.59789 | val_1_rmse: 0.69223 |  0:00:34s
epoch 98 | loss: 0.333   | val_0_rmse: 0.5974  | val_1_rmse: 0.6845  |  0:00:34s
epoch 99 | loss: 0.33251 | val_0_rmse: 0.60133 | val_1_rmse: 0.6866  |  0:00:34s
epoch 100| loss: 0.33453 | val_0_rmse: 0.59921 | val_1_rmse: 0.69853 |  0:00:35s
epoch 101| loss: 0.33298 | val_0_rmse: 0.58954 | val_1_rmse: 0.68439 |  0:00:35s
epoch 102| loss: 0.33876 | val_0_rmse: 0.59325 | val_1_rmse: 0.68114 |  0:00:35s
epoch 103| loss: 0.34149 | val_0_rmse: 0.59744 | val_1_rmse: 0.68466 |  0:00:36s
epoch 104| loss: 0.32852 | val_0_rmse: 0.59826 | val_1_rmse: 0.68644 |  0:00:36s
epoch 105| loss: 0.33507 | val_0_rmse: 0.60455 | val_1_rmse: 0.69934 |  0:00:36s
epoch 106| loss: 0.35101 | val_0_rmse: 0.64225 | val_1_rmse: 0.71787 |  0:00:37s
epoch 107| loss: 0.38168 | val_0_rmse: 0.63726 | val_1_rmse: 0.70473 |  0:00:37s
epoch 108| loss: 0.37742 | val_0_rmse: 0.62809 | val_1_rmse: 0.70306 |  0:00:37s
epoch 109| loss: 0.36636 | val_0_rmse: 0.61702 | val_1_rmse: 0.6942  |  0:00:38s
epoch 110| loss: 0.37286 | val_0_rmse: 0.61642 | val_1_rmse: 0.69369 |  0:00:38s
epoch 111| loss: 0.36355 | val_0_rmse: 0.60867 | val_1_rmse: 0.68562 |  0:00:39s
epoch 112| loss: 0.35745 | val_0_rmse: 0.60494 | val_1_rmse: 0.6912  |  0:00:39s
epoch 113| loss: 0.35805 | val_0_rmse: 0.60099 | val_1_rmse: 0.69311 |  0:00:39s
epoch 114| loss: 0.34326 | val_0_rmse: 0.59687 | val_1_rmse: 0.68637 |  0:00:40s
epoch 115| loss: 0.34894 | val_0_rmse: 0.58935 | val_1_rmse: 0.69541 |  0:00:40s
epoch 116| loss: 0.34063 | val_0_rmse: 0.58265 | val_1_rmse: 0.69344 |  0:00:40s
epoch 117| loss: 0.34264 | val_0_rmse: 0.58275 | val_1_rmse: 0.6861  |  0:00:41s
epoch 118| loss: 0.33408 | val_0_rmse: 0.57775 | val_1_rmse: 0.6839  |  0:00:41s
epoch 119| loss: 0.33524 | val_0_rmse: 0.57871 | val_1_rmse: 0.68251 |  0:00:41s
epoch 120| loss: 0.33786 | val_0_rmse: 0.58208 | val_1_rmse: 0.68243 |  0:00:42s
epoch 121| loss: 0.33638 | val_0_rmse: 0.57309 | val_1_rmse: 0.69333 |  0:00:42s
epoch 122| loss: 0.33268 | val_0_rmse: 0.57686 | val_1_rmse: 0.69169 |  0:00:42s
epoch 123| loss: 0.33536 | val_0_rmse: 0.57749 | val_1_rmse: 0.68949 |  0:00:43s
epoch 124| loss: 0.32355 | val_0_rmse: 0.57183 | val_1_rmse: 0.68928 |  0:00:43s
epoch 125| loss: 0.32565 | val_0_rmse: 0.57223 | val_1_rmse: 0.67656 |  0:00:43s
epoch 126| loss: 0.32827 | val_0_rmse: 0.56835 | val_1_rmse: 0.67861 |  0:00:44s
epoch 127| loss: 0.3205  | val_0_rmse: 0.56009 | val_1_rmse: 0.68098 |  0:00:44s
epoch 128| loss: 0.32068 | val_0_rmse: 0.55748 | val_1_rmse: 0.6774  |  0:00:44s
epoch 129| loss: 0.3203  | val_0_rmse: 0.55311 | val_1_rmse: 0.67596 |  0:00:45s
epoch 130| loss: 0.3149  | val_0_rmse: 0.56191 | val_1_rmse: 0.66831 |  0:00:45s
epoch 131| loss: 0.31482 | val_0_rmse: 0.54862 | val_1_rmse: 0.67238 |  0:00:46s
epoch 132| loss: 0.31287 | val_0_rmse: 0.55294 | val_1_rmse: 0.68203 |  0:00:46s
epoch 133| loss: 0.31256 | val_0_rmse: 0.54943 | val_1_rmse: 0.69072 |  0:00:46s
epoch 134| loss: 0.3161  | val_0_rmse: 0.556   | val_1_rmse: 0.67267 |  0:00:47s
epoch 135| loss: 0.3125  | val_0_rmse: 0.55061 | val_1_rmse: 0.6671  |  0:00:47s
epoch 136| loss: 0.31663 | val_0_rmse: 0.54845 | val_1_rmse: 0.67587 |  0:00:47s
epoch 137| loss: 0.31694 | val_0_rmse: 0.54982 | val_1_rmse: 0.67653 |  0:00:48s
epoch 138| loss: 0.31827 | val_0_rmse: 0.54605 | val_1_rmse: 0.67835 |  0:00:48s
epoch 139| loss: 0.31896 | val_0_rmse: 0.54314 | val_1_rmse: 0.6707  |  0:00:48s
epoch 140| loss: 0.31737 | val_0_rmse: 0.53862 | val_1_rmse: 0.67789 |  0:00:49s
epoch 141| loss: 0.30878 | val_0_rmse: 0.54418 | val_1_rmse: 0.66823 |  0:00:49s
epoch 142| loss: 0.3096  | val_0_rmse: 0.5348  | val_1_rmse: 0.67708 |  0:00:49s
epoch 143| loss: 0.30336 | val_0_rmse: 0.53593 | val_1_rmse: 0.6748  |  0:00:50s
epoch 144| loss: 0.30692 | val_0_rmse: 0.53068 | val_1_rmse: 0.67325 |  0:00:50s
epoch 145| loss: 0.30258 | val_0_rmse: 0.5354  | val_1_rmse: 0.66515 |  0:00:50s
epoch 146| loss: 0.29768 | val_0_rmse: 0.54234 | val_1_rmse: 0.67434 |  0:00:51s
epoch 147| loss: 0.3     | val_0_rmse: 0.53444 | val_1_rmse: 0.67313 |  0:00:51s
epoch 148| loss: 0.30768 | val_0_rmse: 0.53331 | val_1_rmse: 0.67962 |  0:00:51s
epoch 149| loss: 0.30223 | val_0_rmse: 0.53622 | val_1_rmse: 0.66665 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.66515
Best weights from best epoch are automatically used!
ended training at: 17:30:23
Feature importance:
Mean squared error is of 2986068629.3525653
Mean absolute error:36270.767431823006
MAPE:0.2831871599757588
R2 score:0.6598890971655087
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:30:24
epoch 0  | loss: 3.81337 | val_0_rmse: 1.03326 | val_1_rmse: 1.25711 |  0:00:00s
epoch 1  | loss: 1.73501 | val_0_rmse: 1.00095 | val_1_rmse: 1.03418 |  0:00:00s
epoch 2  | loss: 1.13303 | val_0_rmse: 1.02389 | val_1_rmse: 1.05829 |  0:00:01s
epoch 3  | loss: 1.00714 | val_0_rmse: 0.99762 | val_1_rmse: 1.0288  |  0:00:01s
epoch 4  | loss: 0.96297 | val_0_rmse: 0.98335 | val_1_rmse: 1.01147 |  0:00:01s
epoch 5  | loss: 0.92851 | val_0_rmse: 0.96985 | val_1_rmse: 1.00156 |  0:00:02s
epoch 6  | loss: 0.88398 | val_0_rmse: 0.95957 | val_1_rmse: 0.99246 |  0:00:02s
epoch 7  | loss: 0.81728 | val_0_rmse: 0.93293 | val_1_rmse: 0.95924 |  0:00:02s
epoch 8  | loss: 0.7171  | val_0_rmse: 0.85273 | val_1_rmse: 0.89188 |  0:00:03s
epoch 9  | loss: 0.66036 | val_0_rmse: 0.88009 | val_1_rmse: 0.90748 |  0:00:03s
epoch 10 | loss: 0.62215 | val_0_rmse: 1.08748 | val_1_rmse: 1.10274 |  0:00:03s
epoch 11 | loss: 0.59174 | val_0_rmse: 1.12575 | val_1_rmse: 1.14387 |  0:00:04s
epoch 12 | loss: 0.58848 | val_0_rmse: 1.06253 | val_1_rmse: 1.08959 |  0:00:04s
epoch 13 | loss: 0.56884 | val_0_rmse: 1.15649 | val_1_rmse: 1.17716 |  0:00:05s
epoch 14 | loss: 0.54841 | val_0_rmse: 0.93031 | val_1_rmse: 0.96372 |  0:00:05s
epoch 15 | loss: 0.52975 | val_0_rmse: 0.9225  | val_1_rmse: 0.95675 |  0:00:05s
epoch 16 | loss: 0.49446 | val_0_rmse: 0.92507 | val_1_rmse: 0.9627  |  0:00:06s
epoch 17 | loss: 0.47964 | val_0_rmse: 0.8248  | val_1_rmse: 0.86669 |  0:00:06s
epoch 18 | loss: 0.47157 | val_0_rmse: 0.81604 | val_1_rmse: 0.85348 |  0:00:06s
epoch 19 | loss: 0.4584  | val_0_rmse: 0.81297 | val_1_rmse: 0.85183 |  0:00:07s
epoch 20 | loss: 0.44332 | val_0_rmse: 0.82786 | val_1_rmse: 0.86757 |  0:00:07s
epoch 21 | loss: 0.44323 | val_0_rmse: 0.82829 | val_1_rmse: 0.86903 |  0:00:07s
epoch 22 | loss: 0.43174 | val_0_rmse: 0.81837 | val_1_rmse: 0.85731 |  0:00:08s
epoch 23 | loss: 0.41636 | val_0_rmse: 0.85626 | val_1_rmse: 0.88888 |  0:00:08s
epoch 24 | loss: 0.41225 | val_0_rmse: 0.80432 | val_1_rmse: 0.84239 |  0:00:08s
epoch 25 | loss: 0.41061 | val_0_rmse: 0.82971 | val_1_rmse: 0.86363 |  0:00:09s
epoch 26 | loss: 0.4073  | val_0_rmse: 0.78809 | val_1_rmse: 0.82456 |  0:00:09s
epoch 27 | loss: 0.4072  | val_0_rmse: 0.80159 | val_1_rmse: 0.84042 |  0:00:09s
epoch 28 | loss: 0.39643 | val_0_rmse: 0.76941 | val_1_rmse: 0.8093  |  0:00:10s
epoch 29 | loss: 0.39693 | val_0_rmse: 0.79275 | val_1_rmse: 0.82763 |  0:00:10s
epoch 30 | loss: 0.39611 | val_0_rmse: 0.76664 | val_1_rmse: 0.8104  |  0:00:10s
epoch 31 | loss: 0.39559 | val_0_rmse: 0.81146 | val_1_rmse: 0.84908 |  0:00:11s
epoch 32 | loss: 0.38921 | val_0_rmse: 0.76716 | val_1_rmse: 0.81029 |  0:00:11s
epoch 33 | loss: 0.38945 | val_0_rmse: 0.77496 | val_1_rmse: 0.81338 |  0:00:11s
epoch 34 | loss: 0.39646 | val_0_rmse: 0.77584 | val_1_rmse: 0.81258 |  0:00:12s
epoch 35 | loss: 0.38441 | val_0_rmse: 0.76364 | val_1_rmse: 0.80546 |  0:00:12s
epoch 36 | loss: 0.38352 | val_0_rmse: 0.76269 | val_1_rmse: 0.80107 |  0:00:12s
epoch 37 | loss: 0.38486 | val_0_rmse: 0.7431  | val_1_rmse: 0.7861  |  0:00:13s
epoch 38 | loss: 0.37643 | val_0_rmse: 0.73444 | val_1_rmse: 0.77773 |  0:00:13s
epoch 39 | loss: 0.38192 | val_0_rmse: 0.736   | val_1_rmse: 0.77896 |  0:00:13s
epoch 40 | loss: 0.38116 | val_0_rmse: 0.73443 | val_1_rmse: 0.78018 |  0:00:14s
epoch 41 | loss: 0.37813 | val_0_rmse: 0.73248 | val_1_rmse: 0.77675 |  0:00:14s
epoch 42 | loss: 0.37467 | val_0_rmse: 0.7254  | val_1_rmse: 0.77281 |  0:00:15s
epoch 43 | loss: 0.36388 | val_0_rmse: 0.741   | val_1_rmse: 0.7887  |  0:00:15s
epoch 44 | loss: 0.37568 | val_0_rmse: 0.72407 | val_1_rmse: 0.77121 |  0:00:15s
epoch 45 | loss: 0.3707  | val_0_rmse: 0.73151 | val_1_rmse: 0.77911 |  0:00:16s
epoch 46 | loss: 0.36526 | val_0_rmse: 0.73524 | val_1_rmse: 0.7843  |  0:00:16s
epoch 47 | loss: 0.37228 | val_0_rmse: 0.73198 | val_1_rmse: 0.7788  |  0:00:16s
epoch 48 | loss: 0.3706  | val_0_rmse: 0.73435 | val_1_rmse: 0.77905 |  0:00:17s
epoch 49 | loss: 0.36203 | val_0_rmse: 0.7272  | val_1_rmse: 0.77599 |  0:00:17s
epoch 50 | loss: 0.36809 | val_0_rmse: 0.73844 | val_1_rmse: 0.78853 |  0:00:17s
epoch 51 | loss: 0.37614 | val_0_rmse: 0.71682 | val_1_rmse: 0.76918 |  0:00:18s
epoch 52 | loss: 0.36488 | val_0_rmse: 0.72138 | val_1_rmse: 0.77179 |  0:00:18s
epoch 53 | loss: 0.36385 | val_0_rmse: 0.70959 | val_1_rmse: 0.75929 |  0:00:18s
epoch 54 | loss: 0.35817 | val_0_rmse: 0.71526 | val_1_rmse: 0.76401 |  0:00:19s
epoch 55 | loss: 0.3585  | val_0_rmse: 0.71303 | val_1_rmse: 0.76212 |  0:00:19s
epoch 56 | loss: 0.36046 | val_0_rmse: 0.70461 | val_1_rmse: 0.75473 |  0:00:19s
epoch 57 | loss: 0.35656 | val_0_rmse: 0.69381 | val_1_rmse: 0.74407 |  0:00:20s
epoch 58 | loss: 0.35207 | val_0_rmse: 0.69853 | val_1_rmse: 0.75247 |  0:00:20s
epoch 59 | loss: 0.3597  | val_0_rmse: 0.69636 | val_1_rmse: 0.75175 |  0:00:20s
epoch 60 | loss: 0.35438 | val_0_rmse: 0.68722 | val_1_rmse: 0.74086 |  0:00:21s
epoch 61 | loss: 0.35247 | val_0_rmse: 0.6829  | val_1_rmse: 0.73436 |  0:00:21s
epoch 62 | loss: 0.34916 | val_0_rmse: 0.68223 | val_1_rmse: 0.73536 |  0:00:21s
epoch 63 | loss: 0.35996 | val_0_rmse: 0.69761 | val_1_rmse: 0.75226 |  0:00:22s
epoch 64 | loss: 0.35364 | val_0_rmse: 0.68366 | val_1_rmse: 0.73745 |  0:00:22s
epoch 65 | loss: 0.34233 | val_0_rmse: 0.68028 | val_1_rmse: 0.73609 |  0:00:23s
epoch 66 | loss: 0.34915 | val_0_rmse: 0.67643 | val_1_rmse: 0.73338 |  0:00:23s
epoch 67 | loss: 0.34792 | val_0_rmse: 0.67758 | val_1_rmse: 0.73472 |  0:00:23s
epoch 68 | loss: 0.34747 | val_0_rmse: 0.67313 | val_1_rmse: 0.72953 |  0:00:24s
epoch 69 | loss: 0.34901 | val_0_rmse: 0.67843 | val_1_rmse: 0.73562 |  0:00:24s
epoch 70 | loss: 0.34512 | val_0_rmse: 0.66644 | val_1_rmse: 0.72199 |  0:00:24s
epoch 71 | loss: 0.34352 | val_0_rmse: 0.66818 | val_1_rmse: 0.72389 |  0:00:25s
epoch 72 | loss: 0.34597 | val_0_rmse: 0.66137 | val_1_rmse: 0.71966 |  0:00:25s
epoch 73 | loss: 0.34701 | val_0_rmse: 0.66087 | val_1_rmse: 0.7228  |  0:00:25s
epoch 74 | loss: 0.34358 | val_0_rmse: 0.65259 | val_1_rmse: 0.71346 |  0:00:26s
epoch 75 | loss: 0.34134 | val_0_rmse: 0.64853 | val_1_rmse: 0.71003 |  0:00:26s
epoch 76 | loss: 0.3359  | val_0_rmse: 0.65091 | val_1_rmse: 0.71258 |  0:00:26s
epoch 77 | loss: 0.3394  | val_0_rmse: 0.67261 | val_1_rmse: 0.73353 |  0:00:27s
epoch 78 | loss: 0.33684 | val_0_rmse: 0.67028 | val_1_rmse: 0.73302 |  0:00:27s
epoch 79 | loss: 0.33811 | val_0_rmse: 0.65819 | val_1_rmse: 0.7265  |  0:00:27s
epoch 80 | loss: 0.34066 | val_0_rmse: 0.653   | val_1_rmse: 0.72173 |  0:00:28s
epoch 81 | loss: 0.33846 | val_0_rmse: 0.64488 | val_1_rmse: 0.71773 |  0:00:28s
epoch 82 | loss: 0.33707 | val_0_rmse: 0.64987 | val_1_rmse: 0.72886 |  0:00:28s
epoch 83 | loss: 0.3428  | val_0_rmse: 0.6414  | val_1_rmse: 0.71842 |  0:00:29s
epoch 84 | loss: 0.33466 | val_0_rmse: 0.64583 | val_1_rmse: 0.72336 |  0:00:29s
epoch 85 | loss: 0.33261 | val_0_rmse: 0.63626 | val_1_rmse: 0.71626 |  0:00:29s
epoch 86 | loss: 0.33261 | val_0_rmse: 0.63763 | val_1_rmse: 0.71847 |  0:00:30s
epoch 87 | loss: 0.33    | val_0_rmse: 0.63295 | val_1_rmse: 0.7175  |  0:00:30s
epoch 88 | loss: 0.32703 | val_0_rmse: 0.62485 | val_1_rmse: 0.70945 |  0:00:31s
epoch 89 | loss: 0.32659 | val_0_rmse: 0.6199  | val_1_rmse: 0.6961  |  0:00:31s
epoch 90 | loss: 0.32464 | val_0_rmse: 0.61201 | val_1_rmse: 0.69492 |  0:00:31s
epoch 91 | loss: 0.3268  | val_0_rmse: 0.62202 | val_1_rmse: 0.72245 |  0:00:32s
epoch 92 | loss: 0.32671 | val_0_rmse: 0.6132  | val_1_rmse: 0.70733 |  0:00:32s
epoch 93 | loss: 0.32624 | val_0_rmse: 0.60932 | val_1_rmse: 0.69699 |  0:00:32s
epoch 94 | loss: 0.32189 | val_0_rmse: 0.61405 | val_1_rmse: 0.70547 |  0:00:33s
epoch 95 | loss: 0.31851 | val_0_rmse: 0.60379 | val_1_rmse: 0.70184 |  0:00:33s
epoch 96 | loss: 0.32463 | val_0_rmse: 0.59988 | val_1_rmse: 0.70172 |  0:00:33s
epoch 97 | loss: 0.32733 | val_0_rmse: 0.60047 | val_1_rmse: 0.69373 |  0:00:34s
epoch 98 | loss: 0.32466 | val_0_rmse: 0.60756 | val_1_rmse: 0.70688 |  0:00:34s
epoch 99 | loss: 0.32484 | val_0_rmse: 0.60163 | val_1_rmse: 0.69851 |  0:00:34s
epoch 100| loss: 0.31558 | val_0_rmse: 0.60183 | val_1_rmse: 0.69391 |  0:00:35s
epoch 101| loss: 0.32113 | val_0_rmse: 0.58928 | val_1_rmse: 0.6903  |  0:00:35s
epoch 102| loss: 0.31601 | val_0_rmse: 0.58533 | val_1_rmse: 0.69237 |  0:00:35s
epoch 103| loss: 0.31663 | val_0_rmse: 0.589   | val_1_rmse: 0.69087 |  0:00:36s
epoch 104| loss: 0.31653 | val_0_rmse: 0.58992 | val_1_rmse: 0.69233 |  0:00:36s
epoch 105| loss: 0.3165  | val_0_rmse: 0.58474 | val_1_rmse: 0.69071 |  0:00:37s
epoch 106| loss: 0.31354 | val_0_rmse: 0.58582 | val_1_rmse: 0.69251 |  0:00:37s
epoch 107| loss: 0.31449 | val_0_rmse: 0.57712 | val_1_rmse: 0.68482 |  0:00:37s
epoch 108| loss: 0.30324 | val_0_rmse: 0.57318 | val_1_rmse: 0.67797 |  0:00:38s
epoch 109| loss: 0.30492 | val_0_rmse: 0.57398 | val_1_rmse: 0.67917 |  0:00:38s
epoch 110| loss: 0.30685 | val_0_rmse: 0.57621 | val_1_rmse: 0.67987 |  0:00:38s
epoch 111| loss: 0.30528 | val_0_rmse: 0.57748 | val_1_rmse: 0.69262 |  0:00:39s
epoch 112| loss: 0.30333 | val_0_rmse: 0.56497 | val_1_rmse: 0.68958 |  0:00:39s
epoch 113| loss: 0.29901 | val_0_rmse: 0.57363 | val_1_rmse: 0.69942 |  0:00:39s
epoch 114| loss: 0.30537 | val_0_rmse: 0.57483 | val_1_rmse: 0.69263 |  0:00:40s
epoch 115| loss: 0.30971 | val_0_rmse: 0.56755 | val_1_rmse: 0.68771 |  0:00:40s
epoch 116| loss: 0.30613 | val_0_rmse: 0.55675 | val_1_rmse: 0.68346 |  0:00:40s
epoch 117| loss: 0.30749 | val_0_rmse: 0.56928 | val_1_rmse: 0.70659 |  0:00:41s
epoch 118| loss: 0.30638 | val_0_rmse: 0.55645 | val_1_rmse: 0.6921  |  0:00:41s
epoch 119| loss: 0.30645 | val_0_rmse: 0.56235 | val_1_rmse: 0.68734 |  0:00:41s
epoch 120| loss: 0.30613 | val_0_rmse: 0.58001 | val_1_rmse: 0.69334 |  0:00:42s
epoch 121| loss: 0.30272 | val_0_rmse: 0.56617 | val_1_rmse: 0.69316 |  0:00:42s
epoch 122| loss: 0.30223 | val_0_rmse: 0.5471  | val_1_rmse: 0.67671 |  0:00:42s
epoch 123| loss: 0.29994 | val_0_rmse: 0.54486 | val_1_rmse: 0.67838 |  0:00:43s
epoch 124| loss: 0.29629 | val_0_rmse: 0.55609 | val_1_rmse: 0.68691 |  0:00:43s
epoch 125| loss: 0.29558 | val_0_rmse: 0.5533  | val_1_rmse: 0.68729 |  0:00:43s
epoch 126| loss: 0.29509 | val_0_rmse: 0.54615 | val_1_rmse: 0.68817 |  0:00:44s
epoch 127| loss: 0.29513 | val_0_rmse: 0.54166 | val_1_rmse: 0.68259 |  0:00:44s
epoch 128| loss: 0.29367 | val_0_rmse: 0.53749 | val_1_rmse: 0.68234 |  0:00:44s
epoch 129| loss: 0.29188 | val_0_rmse: 0.53668 | val_1_rmse: 0.67915 |  0:00:45s
epoch 130| loss: 0.28645 | val_0_rmse: 0.53026 | val_1_rmse: 0.68237 |  0:00:45s
epoch 131| loss: 0.28942 | val_0_rmse: 0.52925 | val_1_rmse: 0.67838 |  0:00:45s
epoch 132| loss: 0.2935  | val_0_rmse: 0.52947 | val_1_rmse: 0.69053 |  0:00:46s
epoch 133| loss: 0.29403 | val_0_rmse: 0.52868 | val_1_rmse: 0.69122 |  0:00:46s
epoch 134| loss: 0.28562 | val_0_rmse: 0.53097 | val_1_rmse: 0.69164 |  0:00:46s
epoch 135| loss: 0.3     | val_0_rmse: 0.53427 | val_1_rmse: 0.6976  |  0:00:47s
epoch 136| loss: 0.29735 | val_0_rmse: 0.53006 | val_1_rmse: 0.68933 |  0:00:47s
epoch 137| loss: 0.29024 | val_0_rmse: 0.5213  | val_1_rmse: 0.68566 |  0:00:47s
epoch 138| loss: 0.28829 | val_0_rmse: 0.5194  | val_1_rmse: 0.68866 |  0:00:48s
epoch 139| loss: 0.28569 | val_0_rmse: 0.51325 | val_1_rmse: 0.68858 |  0:00:48s
epoch 140| loss: 0.28203 | val_0_rmse: 0.52383 | val_1_rmse: 0.69492 |  0:00:49s
epoch 141| loss: 0.27437 | val_0_rmse: 0.50997 | val_1_rmse: 0.68691 |  0:00:49s
epoch 142| loss: 0.27252 | val_0_rmse: 0.5139  | val_1_rmse: 0.69008 |  0:00:49s
epoch 143| loss: 0.27468 | val_0_rmse: 0.51452 | val_1_rmse: 0.70095 |  0:00:50s
epoch 144| loss: 0.27389 | val_0_rmse: 0.51597 | val_1_rmse: 0.69855 |  0:00:50s
epoch 145| loss: 0.27803 | val_0_rmse: 0.52239 | val_1_rmse: 0.70967 |  0:00:50s
epoch 146| loss: 0.27685 | val_0_rmse: 0.52859 | val_1_rmse: 0.72852 |  0:00:51s
epoch 147| loss: 0.27584 | val_0_rmse: 0.52052 | val_1_rmse: 0.71187 |  0:00:51s
epoch 148| loss: 0.27235 | val_0_rmse: 0.50181 | val_1_rmse: 0.68546 |  0:00:51s
epoch 149| loss: 0.27399 | val_0_rmse: 0.50689 | val_1_rmse: 0.69975 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.67671
Best weights from best epoch are automatically used!
ended training at: 17:31:16
Feature importance:
Mean squared error is of 2476461947.2329626
Mean absolute error:33521.97121386885
MAPE:0.2817716541941856
R2 score:0.6610911051936115
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:31:16
epoch 0  | loss: 3.51966 | val_0_rmse: 1.03389 | val_1_rmse: 1.02339 |  0:00:00s
epoch 1  | loss: 1.43686 | val_0_rmse: 0.99792 | val_1_rmse: 0.99663 |  0:00:00s
epoch 2  | loss: 1.28856 | val_0_rmse: 1.06242 | val_1_rmse: 1.06673 |  0:00:01s
epoch 3  | loss: 1.05418 | val_0_rmse: 0.98804 | val_1_rmse: 0.99132 |  0:00:01s
epoch 4  | loss: 0.98636 | val_0_rmse: 0.93848 | val_1_rmse: 0.94095 |  0:00:01s
epoch 5  | loss: 0.88195 | val_0_rmse: 0.87161 | val_1_rmse: 0.87966 |  0:00:02s
epoch 6  | loss: 0.79263 | val_0_rmse: 0.85989 | val_1_rmse: 0.86816 |  0:00:02s
epoch 7  | loss: 0.71023 | val_0_rmse: 0.83225 | val_1_rmse: 0.83508 |  0:00:02s
epoch 8  | loss: 0.66614 | val_0_rmse: 0.80459 | val_1_rmse: 0.81433 |  0:00:03s
epoch 9  | loss: 0.64393 | val_0_rmse: 0.7945  | val_1_rmse: 0.79593 |  0:00:03s
epoch 10 | loss: 0.60125 | val_0_rmse: 0.79381 | val_1_rmse: 0.81271 |  0:00:03s
epoch 11 | loss: 0.58434 | val_0_rmse: 0.77061 | val_1_rmse: 0.78925 |  0:00:04s
epoch 12 | loss: 0.54254 | val_0_rmse: 0.7822  | val_1_rmse: 0.80552 |  0:00:04s
epoch 13 | loss: 0.52759 | val_0_rmse: 0.78341 | val_1_rmse: 0.80661 |  0:00:04s
epoch 14 | loss: 0.51502 | val_0_rmse: 0.81666 | val_1_rmse: 0.84742 |  0:00:05s
epoch 15 | loss: 0.52578 | val_0_rmse: 0.79724 | val_1_rmse: 0.82702 |  0:00:05s
epoch 16 | loss: 0.52033 | val_0_rmse: 0.7887  | val_1_rmse: 0.82317 |  0:00:05s
epoch 17 | loss: 0.50481 | val_0_rmse: 0.79142 | val_1_rmse: 0.82479 |  0:00:06s
epoch 18 | loss: 0.47826 | val_0_rmse: 0.74677 | val_1_rmse: 0.78397 |  0:00:06s
epoch 19 | loss: 0.48117 | val_0_rmse: 0.73376 | val_1_rmse: 0.77319 |  0:00:06s
epoch 20 | loss: 0.47664 | val_0_rmse: 0.73538 | val_1_rmse: 0.7804  |  0:00:07s
epoch 21 | loss: 0.45906 | val_0_rmse: 0.73422 | val_1_rmse: 0.77729 |  0:00:07s
epoch 22 | loss: 0.44943 | val_0_rmse: 0.73184 | val_1_rmse: 0.76968 |  0:00:07s
epoch 23 | loss: 0.44719 | val_0_rmse: 0.73541 | val_1_rmse: 0.77907 |  0:00:08s
epoch 24 | loss: 0.43552 | val_0_rmse: 0.73047 | val_1_rmse: 0.7727  |  0:00:08s
epoch 25 | loss: 0.42537 | val_0_rmse: 0.73011 | val_1_rmse: 0.77085 |  0:00:09s
epoch 26 | loss: 0.41923 | val_0_rmse: 0.72199 | val_1_rmse: 0.76617 |  0:00:09s
epoch 27 | loss: 0.41601 | val_0_rmse: 0.73273 | val_1_rmse: 0.76736 |  0:00:09s
epoch 28 | loss: 0.41655 | val_0_rmse: 0.72503 | val_1_rmse: 0.76025 |  0:00:10s
epoch 29 | loss: 0.40171 | val_0_rmse: 0.7201  | val_1_rmse: 0.76028 |  0:00:10s
epoch 30 | loss: 0.41677 | val_0_rmse: 0.72424 | val_1_rmse: 0.7616  |  0:00:10s
epoch 31 | loss: 0.40398 | val_0_rmse: 0.72486 | val_1_rmse: 0.76202 |  0:00:11s
epoch 32 | loss: 0.40534 | val_0_rmse: 0.71987 | val_1_rmse: 0.76352 |  0:00:11s
epoch 33 | loss: 0.3985  | val_0_rmse: 0.72325 | val_1_rmse: 0.76221 |  0:00:11s
epoch 34 | loss: 0.3977  | val_0_rmse: 0.71075 | val_1_rmse: 0.75455 |  0:00:12s
epoch 35 | loss: 0.38929 | val_0_rmse: 0.70661 | val_1_rmse: 0.74879 |  0:00:12s
epoch 36 | loss: 0.39636 | val_0_rmse: 0.71069 | val_1_rmse: 0.75447 |  0:00:12s
epoch 37 | loss: 0.39406 | val_0_rmse: 0.70575 | val_1_rmse: 0.75369 |  0:00:13s
epoch 38 | loss: 0.39142 | val_0_rmse: 0.70211 | val_1_rmse: 0.74665 |  0:00:13s
epoch 39 | loss: 0.37673 | val_0_rmse: 0.70619 | val_1_rmse: 0.75939 |  0:00:13s
epoch 40 | loss: 0.38316 | val_0_rmse: 0.70382 | val_1_rmse: 0.75583 |  0:00:14s
epoch 41 | loss: 0.38234 | val_0_rmse: 0.69611 | val_1_rmse: 0.75148 |  0:00:14s
epoch 42 | loss: 0.37992 | val_0_rmse: 0.69954 | val_1_rmse: 0.75617 |  0:00:15s
epoch 43 | loss: 0.37855 | val_0_rmse: 0.70368 | val_1_rmse: 0.75753 |  0:00:15s
epoch 44 | loss: 0.37575 | val_0_rmse: 0.7075  | val_1_rmse: 0.75754 |  0:00:15s
epoch 45 | loss: 0.37058 | val_0_rmse: 0.69712 | val_1_rmse: 0.74991 |  0:00:16s
epoch 46 | loss: 0.37621 | val_0_rmse: 0.69695 | val_1_rmse: 0.74816 |  0:00:16s
epoch 47 | loss: 0.36914 | val_0_rmse: 0.69274 | val_1_rmse: 0.7437  |  0:00:16s
epoch 48 | loss: 0.37207 | val_0_rmse: 0.69426 | val_1_rmse: 0.74937 |  0:00:17s
epoch 49 | loss: 0.37028 | val_0_rmse: 0.69342 | val_1_rmse: 0.74189 |  0:00:17s
epoch 50 | loss: 0.36377 | val_0_rmse: 0.69549 | val_1_rmse: 0.73807 |  0:00:17s
epoch 51 | loss: 0.38448 | val_0_rmse: 0.70623 | val_1_rmse: 0.74719 |  0:00:18s
epoch 52 | loss: 0.38688 | val_0_rmse: 0.7068  | val_1_rmse: 0.74366 |  0:00:18s
epoch 53 | loss: 0.38558 | val_0_rmse: 0.706   | val_1_rmse: 0.7411  |  0:00:18s
epoch 54 | loss: 0.37444 | val_0_rmse: 0.6949  | val_1_rmse: 0.73472 |  0:00:19s
epoch 55 | loss: 0.37572 | val_0_rmse: 0.68902 | val_1_rmse: 0.73419 |  0:00:19s
epoch 56 | loss: 0.37466 | val_0_rmse: 0.68062 | val_1_rmse: 0.73097 |  0:00:19s
epoch 57 | loss: 0.38118 | val_0_rmse: 0.68196 | val_1_rmse: 0.73339 |  0:00:20s
epoch 58 | loss: 0.36991 | val_0_rmse: 0.67783 | val_1_rmse: 0.72917 |  0:00:20s
epoch 59 | loss: 0.36962 | val_0_rmse: 0.68211 | val_1_rmse: 0.73204 |  0:00:20s
epoch 60 | loss: 0.36989 | val_0_rmse: 0.69345 | val_1_rmse: 0.74332 |  0:00:21s
epoch 61 | loss: 0.36943 | val_0_rmse: 0.6883  | val_1_rmse: 0.73756 |  0:00:21s
epoch 62 | loss: 0.35757 | val_0_rmse: 0.67542 | val_1_rmse: 0.7316  |  0:00:22s
epoch 63 | loss: 0.37014 | val_0_rmse: 0.67739 | val_1_rmse: 0.72799 |  0:00:22s
epoch 64 | loss: 0.35993 | val_0_rmse: 0.6669  | val_1_rmse: 0.72272 |  0:00:22s
epoch 65 | loss: 0.36192 | val_0_rmse: 0.66673 | val_1_rmse: 0.71983 |  0:00:23s
epoch 66 | loss: 0.35778 | val_0_rmse: 0.66809 | val_1_rmse: 0.72046 |  0:00:23s
epoch 67 | loss: 0.35998 | val_0_rmse: 0.666   | val_1_rmse: 0.72245 |  0:00:23s
epoch 68 | loss: 0.36434 | val_0_rmse: 0.67205 | val_1_rmse: 0.72365 |  0:00:24s
epoch 69 | loss: 0.36101 | val_0_rmse: 0.66891 | val_1_rmse: 0.71928 |  0:00:24s
epoch 70 | loss: 0.35816 | val_0_rmse: 0.68005 | val_1_rmse: 0.72943 |  0:00:24s
epoch 71 | loss: 0.3528  | val_0_rmse: 0.67202 | val_1_rmse: 0.73003 |  0:00:25s
epoch 72 | loss: 0.362   | val_0_rmse: 0.6709  | val_1_rmse: 0.72913 |  0:00:25s
epoch 73 | loss: 0.35418 | val_0_rmse: 0.65914 | val_1_rmse: 0.72383 |  0:00:25s
epoch 74 | loss: 0.35071 | val_0_rmse: 0.65629 | val_1_rmse: 0.72146 |  0:00:26s
epoch 75 | loss: 0.35124 | val_0_rmse: 0.64841 | val_1_rmse: 0.7111  |  0:00:26s
epoch 76 | loss: 0.3561  | val_0_rmse: 0.65056 | val_1_rmse: 0.70986 |  0:00:26s
epoch 77 | loss: 0.34727 | val_0_rmse: 0.64781 | val_1_rmse: 0.70277 |  0:00:27s
epoch 78 | loss: 0.34798 | val_0_rmse: 0.65476 | val_1_rmse: 0.70803 |  0:00:27s
epoch 79 | loss: 0.34822 | val_0_rmse: 0.64577 | val_1_rmse: 0.70476 |  0:00:27s
epoch 80 | loss: 0.35472 | val_0_rmse: 0.64319 | val_1_rmse: 0.70568 |  0:00:28s
epoch 81 | loss: 0.35028 | val_0_rmse: 0.64152 | val_1_rmse: 0.70749 |  0:00:28s
epoch 82 | loss: 0.34443 | val_0_rmse: 0.63573 | val_1_rmse: 0.69571 |  0:00:28s
epoch 83 | loss: 0.3434  | val_0_rmse: 0.6442  | val_1_rmse: 0.70935 |  0:00:29s
epoch 84 | loss: 0.34829 | val_0_rmse: 0.63866 | val_1_rmse: 0.70157 |  0:00:29s
epoch 85 | loss: 0.34275 | val_0_rmse: 0.63601 | val_1_rmse: 0.70068 |  0:00:29s
epoch 86 | loss: 0.34741 | val_0_rmse: 0.64064 | val_1_rmse: 0.71178 |  0:00:30s
epoch 87 | loss: 0.34289 | val_0_rmse: 0.63721 | val_1_rmse: 0.70223 |  0:00:30s
epoch 88 | loss: 0.33889 | val_0_rmse: 0.63421 | val_1_rmse: 0.70536 |  0:00:31s
epoch 89 | loss: 0.34251 | val_0_rmse: 0.635   | val_1_rmse: 0.70078 |  0:00:31s
epoch 90 | loss: 0.33833 | val_0_rmse: 0.63211 | val_1_rmse: 0.69943 |  0:00:31s
epoch 91 | loss: 0.33873 | val_0_rmse: 0.63748 | val_1_rmse: 0.70505 |  0:00:32s
epoch 92 | loss: 0.3477  | val_0_rmse: 0.62792 | val_1_rmse: 0.69135 |  0:00:32s
epoch 93 | loss: 0.33628 | val_0_rmse: 0.62889 | val_1_rmse: 0.69656 |  0:00:32s
epoch 94 | loss: 0.34626 | val_0_rmse: 0.62477 | val_1_rmse: 0.68865 |  0:00:33s
epoch 95 | loss: 0.33524 | val_0_rmse: 0.62418 | val_1_rmse: 0.68562 |  0:00:33s
epoch 96 | loss: 0.33778 | val_0_rmse: 0.61561 | val_1_rmse: 0.68713 |  0:00:33s
epoch 97 | loss: 0.33453 | val_0_rmse: 0.62312 | val_1_rmse: 0.69216 |  0:00:34s
epoch 98 | loss: 0.33729 | val_0_rmse: 0.62382 | val_1_rmse: 0.70004 |  0:00:34s
epoch 99 | loss: 0.34088 | val_0_rmse: 0.61415 | val_1_rmse: 0.69644 |  0:00:34s
epoch 100| loss: 0.33495 | val_0_rmse: 0.61833 | val_1_rmse: 0.69124 |  0:00:35s
epoch 101| loss: 0.3381  | val_0_rmse: 0.6115  | val_1_rmse: 0.68034 |  0:00:35s
epoch 102| loss: 0.33607 | val_0_rmse: 0.60273 | val_1_rmse: 0.67045 |  0:00:36s
epoch 103| loss: 0.34197 | val_0_rmse: 0.60837 | val_1_rmse: 0.66956 |  0:00:36s
epoch 104| loss: 0.32815 | val_0_rmse: 0.6051  | val_1_rmse: 0.67254 |  0:00:36s
epoch 105| loss: 0.34091 | val_0_rmse: 0.61194 | val_1_rmse: 0.67788 |  0:00:37s
epoch 106| loss: 0.34738 | val_0_rmse: 0.61091 | val_1_rmse: 0.67936 |  0:00:37s
epoch 107| loss: 0.34458 | val_0_rmse: 0.60642 | val_1_rmse: 0.68407 |  0:00:37s
epoch 108| loss: 0.33189 | val_0_rmse: 0.60189 | val_1_rmse: 0.68448 |  0:00:38s
epoch 109| loss: 0.34243 | val_0_rmse: 0.59847 | val_1_rmse: 0.68351 |  0:00:38s
epoch 110| loss: 0.33008 | val_0_rmse: 0.59376 | val_1_rmse: 0.67661 |  0:00:38s
epoch 111| loss: 0.3328  | val_0_rmse: 0.59109 | val_1_rmse: 0.67588 |  0:00:39s
epoch 112| loss: 0.3323  | val_0_rmse: 0.59557 | val_1_rmse: 0.67657 |  0:00:39s
epoch 113| loss: 0.343   | val_0_rmse: 0.59912 | val_1_rmse: 0.67075 |  0:00:39s
epoch 114| loss: 0.33692 | val_0_rmse: 0.59082 | val_1_rmse: 0.67684 |  0:00:40s
epoch 115| loss: 0.32479 | val_0_rmse: 0.58005 | val_1_rmse: 0.67203 |  0:00:40s
epoch 116| loss: 0.33183 | val_0_rmse: 0.58383 | val_1_rmse: 0.67794 |  0:00:40s
epoch 117| loss: 0.32955 | val_0_rmse: 0.58367 | val_1_rmse: 0.68044 |  0:00:41s
epoch 118| loss: 0.33151 | val_0_rmse: 0.57945 | val_1_rmse: 0.67835 |  0:00:41s
epoch 119| loss: 0.3325  | val_0_rmse: 0.56908 | val_1_rmse: 0.67462 |  0:00:41s
epoch 120| loss: 0.32386 | val_0_rmse: 0.57498 | val_1_rmse: 0.68128 |  0:00:42s
epoch 121| loss: 0.32309 | val_0_rmse: 0.57008 | val_1_rmse: 0.68381 |  0:00:42s
epoch 122| loss: 0.32337 | val_0_rmse: 0.56375 | val_1_rmse: 0.67924 |  0:00:42s
epoch 123| loss: 0.32005 | val_0_rmse: 0.56682 | val_1_rmse: 0.68349 |  0:00:43s
epoch 124| loss: 0.32234 | val_0_rmse: 0.56736 | val_1_rmse: 0.67813 |  0:00:43s
epoch 125| loss: 0.32761 | val_0_rmse: 0.56845 | val_1_rmse: 0.67905 |  0:00:43s
epoch 126| loss: 0.3227  | val_0_rmse: 0.55374 | val_1_rmse: 0.67211 |  0:00:44s
epoch 127| loss: 0.31758 | val_0_rmse: 0.56641 | val_1_rmse: 0.67677 |  0:00:44s
epoch 128| loss: 0.31823 | val_0_rmse: 0.56013 | val_1_rmse: 0.68084 |  0:00:44s
epoch 129| loss: 0.3193  | val_0_rmse: 0.55743 | val_1_rmse: 0.68992 |  0:00:45s
epoch 130| loss: 0.31717 | val_0_rmse: 0.55829 | val_1_rmse: 0.68111 |  0:00:45s
epoch 131| loss: 0.31462 | val_0_rmse: 0.55192 | val_1_rmse: 0.68728 |  0:00:46s
epoch 132| loss: 0.31826 | val_0_rmse: 0.5566  | val_1_rmse: 0.67903 |  0:00:46s
epoch 133| loss: 0.31693 | val_0_rmse: 0.55695 | val_1_rmse: 0.67949 |  0:00:46s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.66956
Best weights from best epoch are automatically used!
ended training at: 17:32:03
Feature importance:
Mean squared error is of 3124377265.5061264
Mean absolute error:36973.58550183043
MAPE:0.33616550290323327
R2 score:0.6092867578041739
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:45:19
epoch 0  | loss: 0.49521 | val_0_rmse: 0.61389 | val_1_rmse: 0.61399 |  0:00:44s
epoch 1  | loss: 0.21546 | val_0_rmse: 0.49064 | val_1_rmse: 0.49086 |  0:01:28s
epoch 2  | loss: 0.185   | val_0_rmse: 0.42177 | val_1_rmse: 0.42665 |  0:02:12s
epoch 3  | loss: 0.17039 | val_0_rmse: 0.39302 | val_1_rmse: 0.40161 |  0:02:55s
epoch 4  | loss: 0.16303 | val_0_rmse: 0.4323  | val_1_rmse: 0.42315 |  0:03:39s
epoch 5  | loss: 0.15723 | val_0_rmse: 0.39538 | val_1_rmse: 0.41139 |  0:04:23s
epoch 6  | loss: 0.15299 | val_0_rmse: 0.45253 | val_1_rmse: 0.46573 |  0:05:07s
epoch 7  | loss: 0.15135 | val_0_rmse: 0.40645 | val_1_rmse: 0.42138 |  0:05:50s
epoch 8  | loss: 0.14974 | val_0_rmse: 0.40472 | val_1_rmse: 0.42472 |  0:06:34s
epoch 9  | loss: 0.1464  | val_0_rmse: 0.40006 | val_1_rmse: 0.41886 |  0:07:18s
epoch 10 | loss: 0.14641 | val_0_rmse: 0.44016 | val_1_rmse: 0.46084 |  0:08:03s
epoch 11 | loss: 0.14508 | val_0_rmse: 0.39902 | val_1_rmse: 0.4219  |  0:08:47s
epoch 12 | loss: 0.14414 | val_0_rmse: 0.40454 | val_1_rmse: 0.42597 |  0:09:31s
epoch 13 | loss: 0.14593 | val_0_rmse: 0.3926  | val_1_rmse: 0.41428 |  0:10:14s
epoch 14 | loss: 0.14206 | val_0_rmse: 1.15125 | val_1_rmse: 0.93637 |  0:10:58s
epoch 15 | loss: 0.14376 | val_0_rmse: 0.39639 | val_1_rmse: 0.40805 |  0:11:42s
epoch 16 | loss: 0.13813 | val_0_rmse: 0.39667 | val_1_rmse: 0.41669 |  0:12:25s
epoch 17 | loss: 0.13918 | val_0_rmse: 0.40876 | val_1_rmse: 0.42912 |  0:13:09s
epoch 18 | loss: 0.14248 | val_0_rmse: 0.40113 | val_1_rmse: 0.42405 |  0:13:53s
epoch 19 | loss: 0.13734 | val_0_rmse: 0.39168 | val_1_rmse: 0.4155  |  0:14:36s
epoch 20 | loss: 0.1335  | val_0_rmse: 0.3834  | val_1_rmse: 0.41071 |  0:15:20s
epoch 21 | loss: 0.13644 | val_0_rmse: 0.41052 | val_1_rmse: 0.43009 |  0:16:03s
epoch 22 | loss: 0.14406 | val_0_rmse: 0.43833 | val_1_rmse: 0.41375 |  0:16:47s
epoch 23 | loss: 0.13571 | val_0_rmse: 0.50762 | val_1_rmse: 0.51698 |  0:17:30s
epoch 24 | loss: 0.13712 | val_0_rmse: 0.39998 | val_1_rmse: 0.42502 |  0:18:14s
epoch 25 | loss: 0.13357 | val_0_rmse: 0.42912 | val_1_rmse: 0.46183 |  0:18:58s
epoch 26 | loss: 0.13178 | val_0_rmse: 0.45465 | val_1_rmse: 0.42986 |  0:19:44s
epoch 27 | loss: 0.12905 | val_0_rmse: 0.37223 | val_1_rmse: 0.40321 |  0:20:29s
epoch 28 | loss: 0.1262  | val_0_rmse: 0.83946 | val_1_rmse: 0.42138 |  0:21:13s
epoch 29 | loss: 0.12624 | val_0_rmse: 0.97762 | val_1_rmse: 0.42573 |  0:21:57s
epoch 30 | loss: 0.12485 | val_0_rmse: 0.51476 | val_1_rmse: 0.43539 |  0:22:41s
epoch 31 | loss: 0.12421 | val_0_rmse: 0.83105 | val_1_rmse: 0.4306  |  0:23:25s
epoch 32 | loss: 0.12452 | val_0_rmse: 0.42866 | val_1_rmse: 0.40305 |  0:24:11s
epoch 33 | loss: 0.12311 | val_0_rmse: 0.4455  | val_1_rmse: 0.41375 |  0:24:55s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.40161
Best weights from best epoch are automatically used!
ended training at: 18:10:42
Feature importance:
Mean squared error is of 7759460801.220695
Mean absolute error:55177.81868261436
MAPE:0.26590843597983504
R2 score:0.8227644566900729
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:10:56
epoch 0  | loss: 0.4806  | val_0_rmse: 0.61818 | val_1_rmse: 0.62038 |  0:00:44s
epoch 1  | loss: 0.21249 | val_0_rmse: 0.50077 | val_1_rmse: 0.50343 |  0:01:28s
epoch 2  | loss: 0.18231 | val_0_rmse: 0.41392 | val_1_rmse: 0.42173 |  0:02:12s
epoch 3  | loss: 0.17425 | val_0_rmse: 0.41346 | val_1_rmse: 0.42089 |  0:02:57s
epoch 4  | loss: 0.17306 | val_0_rmse: 0.39661 | val_1_rmse: 0.40794 |  0:03:40s
epoch 5  | loss: 0.16451 | val_0_rmse: 0.42085 | val_1_rmse: 0.43353 |  0:04:24s
epoch 6  | loss: 0.18527 | val_0_rmse: 0.41157 | val_1_rmse: 0.42439 |  0:05:08s
epoch 7  | loss: 0.17233 | val_0_rmse: 0.41619 | val_1_rmse: 0.43004 |  0:05:52s
epoch 8  | loss: 0.16541 | val_0_rmse: 0.4491  | val_1_rmse: 0.46426 |  0:06:36s
epoch 9  | loss: 0.16173 | val_0_rmse: 0.40424 | val_1_rmse: 0.421   |  0:07:20s
epoch 10 | loss: 0.16136 | val_0_rmse: 0.43731 | val_1_rmse: 0.45054 |  0:08:04s
epoch 11 | loss: 0.15537 | val_0_rmse: 0.4128  | val_1_rmse: 0.43093 |  0:08:47s
epoch 12 | loss: 0.14951 | val_0_rmse: 0.39365 | val_1_rmse: 0.41326 |  0:09:31s
epoch 13 | loss: 0.14567 | val_0_rmse: 0.42656 | val_1_rmse: 0.44709 |  0:10:15s
epoch 14 | loss: 0.14627 | val_0_rmse: 0.38196 | val_1_rmse: 0.40275 |  0:10:59s
epoch 15 | loss: 0.14226 | val_0_rmse: 0.49138 | val_1_rmse: 0.50592 |  0:11:43s
epoch 16 | loss: 0.13982 | val_0_rmse: 0.37213 | val_1_rmse: 0.39426 |  0:12:27s
epoch 17 | loss: 0.14143 | val_0_rmse: 0.40636 | val_1_rmse: 0.42512 |  0:13:11s
epoch 18 | loss: 0.14808 | val_0_rmse: 0.39378 | val_1_rmse: 0.41392 |  0:13:56s
epoch 19 | loss: 0.13965 | val_0_rmse: 0.39653 | val_1_rmse: 0.42036 |  0:14:42s
epoch 20 | loss: 0.14261 | val_0_rmse: 0.39296 | val_1_rmse: 0.41524 |  0:15:25s
epoch 21 | loss: 0.14623 | val_0_rmse: 0.39163 | val_1_rmse: 0.41479 |  0:16:09s
epoch 22 | loss: 0.15198 | val_0_rmse: 0.7755  | val_1_rmse: 0.62547 |  0:16:53s
epoch 23 | loss: 0.14231 | val_0_rmse: 0.41887 | val_1_rmse: 0.43669 |  0:17:37s
epoch 24 | loss: 0.13832 | val_0_rmse: 0.39417 | val_1_rmse: 0.41556 |  0:18:21s
epoch 25 | loss: 0.13802 | val_0_rmse: 0.40409 | val_1_rmse: 0.42615 |  0:19:05s
epoch 26 | loss: 0.15251 | val_0_rmse: 0.40724 | val_1_rmse: 0.42872 |  0:19:49s
epoch 27 | loss: 0.1473  | val_0_rmse: 0.77357 | val_1_rmse: 0.77552 |  0:20:33s
epoch 28 | loss: 0.15252 | val_0_rmse: 0.43892 | val_1_rmse: 0.45769 |  0:21:16s
epoch 29 | loss: 0.15068 | val_0_rmse: 0.49352 | val_1_rmse: 0.5027  |  0:22:00s
epoch 30 | loss: 0.15286 | val_0_rmse: 0.42317 | val_1_rmse: 0.44009 |  0:22:43s
epoch 31 | loss: 0.15177 | val_0_rmse: 0.40932 | val_1_rmse: 0.43098 |  0:23:27s
epoch 32 | loss: 0.14624 | val_0_rmse: 0.4952  | val_1_rmse: 0.49232 |  0:24:11s
epoch 33 | loss: 0.15697 | val_0_rmse: 0.41114 | val_1_rmse: 0.42259 |  0:24:55s
epoch 34 | loss: 0.15298 | val_0_rmse: 0.42969 | val_1_rmse: 0.45025 |  0:25:39s
epoch 35 | loss: 0.15359 | val_0_rmse: 0.40551 | val_1_rmse: 0.42985 |  0:26:23s
epoch 36 | loss: 0.14686 | val_0_rmse: 0.4275  | val_1_rmse: 0.45012 |  0:27:06s
epoch 37 | loss: 0.14114 | val_0_rmse: 0.4439  | val_1_rmse: 0.46525 |  0:27:50s
epoch 38 | loss: 0.13739 | val_0_rmse: 0.43952 | val_1_rmse: 0.45331 |  0:28:34s
epoch 39 | loss: 0.13667 | val_0_rmse: 0.39907 | val_1_rmse: 0.41663 |  0:29:18s
epoch 40 | loss: 0.13559 | val_0_rmse: 0.397   | val_1_rmse: 0.41554 |  0:30:02s
epoch 41 | loss: 0.13252 | val_0_rmse: 0.40844 | val_1_rmse: 0.43544 |  0:30:46s
epoch 42 | loss: 0.13617 | val_0_rmse: 0.3856  | val_1_rmse: 0.41154 |  0:31:29s
epoch 43 | loss: 0.13738 | val_0_rmse: 0.41857 | val_1_rmse: 0.4493  |  0:32:13s
epoch 44 | loss: 0.1332  | val_0_rmse: 0.44696 | val_1_rmse: 0.47904 |  0:32:57s
epoch 45 | loss: 0.13869 | val_0_rmse: 0.39902 | val_1_rmse: 0.42568 |  0:33:40s
epoch 46 | loss: 0.1325  | val_0_rmse: 0.39169 | val_1_rmse: 0.4195  |  0:34:24s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.39426
Best weights from best epoch are automatically used!
ended training at: 18:45:48
Feature importance:
Mean squared error is of 7064918487.240798
Mean absolute error:51835.89478485548
MAPE:0.26200855441618237
R2 score:0.8383674413494306
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:45:56
epoch 0  | loss: 0.43705 | val_0_rmse: 0.63185 | val_1_rmse: 0.63588 |  0:00:43s
epoch 1  | loss: 0.20258 | val_0_rmse: 0.50688 | val_1_rmse: 0.51002 |  0:01:26s
epoch 2  | loss: 0.17555 | val_0_rmse: 0.4254  | val_1_rmse: 0.432   |  0:02:10s
epoch 3  | loss: 0.17059 | val_0_rmse: 0.38341 | val_1_rmse: 0.39818 |  0:02:53s
epoch 4  | loss: 0.15905 | val_0_rmse: 0.38818 | val_1_rmse: 0.4052  |  0:03:36s
epoch 5  | loss: 0.15388 | val_0_rmse: 0.40969 | val_1_rmse: 0.43065 |  0:04:20s
epoch 6  | loss: 0.15041 | val_0_rmse: 0.52005 | val_1_rmse: 0.53399 |  0:05:03s
epoch 7  | loss: 0.14734 | val_0_rmse: 0.40866 | val_1_rmse: 0.42631 |  0:05:46s
epoch 8  | loss: 0.14391 | val_0_rmse: 0.40764 | val_1_rmse: 0.43205 |  0:06:29s
epoch 9  | loss: 0.14324 | val_0_rmse: 0.43065 | val_1_rmse: 0.45354 |  0:07:13s
epoch 10 | loss: 0.13991 | val_0_rmse: 0.43307 | val_1_rmse: 0.45682 |  0:07:56s
epoch 11 | loss: 0.13895 | val_0_rmse: 0.39477 | val_1_rmse: 0.41461 |  0:08:39s
epoch 12 | loss: 0.13697 | val_0_rmse: 0.39456 | val_1_rmse: 0.42124 |  0:09:25s
epoch 13 | loss: 0.13589 | val_0_rmse: 0.44313 | val_1_rmse: 0.46317 |  0:10:08s
epoch 14 | loss: 0.13446 | val_0_rmse: 0.38447 | val_1_rmse: 0.4125  |  0:10:52s
epoch 15 | loss: 0.13352 | val_0_rmse: 0.38258 | val_1_rmse: 0.4098  |  0:11:35s
epoch 16 | loss: 0.13202 | val_0_rmse: 0.43883 | val_1_rmse: 0.46653 |  0:12:19s
epoch 17 | loss: 0.13124 | val_0_rmse: 0.40532 | val_1_rmse: 0.41043 |  0:13:02s
epoch 18 | loss: 0.13053 | val_0_rmse: 0.46047 | val_1_rmse: 0.46092 |  0:13:46s
epoch 19 | loss: 0.12938 | val_0_rmse: 0.37575 | val_1_rmse: 0.40866 |  0:14:29s
epoch 20 | loss: 0.12773 | val_0_rmse: 0.3852  | val_1_rmse: 0.41753 |  0:15:13s
epoch 21 | loss: 0.12779 | val_0_rmse: 0.38822 | val_1_rmse: 0.41828 |  0:15:57s
epoch 22 | loss: 0.128   | val_0_rmse: 0.38543 | val_1_rmse: 0.41144 |  0:16:40s
epoch 23 | loss: 0.12739 | val_0_rmse: 0.36958 | val_1_rmse: 0.40658 |  0:17:24s
epoch 24 | loss: 0.12431 | val_0_rmse: 0.40293 | val_1_rmse: 0.42281 |  0:18:07s
epoch 25 | loss: 0.12464 | val_0_rmse: 0.74428 | val_1_rmse: 0.76371 |  0:18:51s
epoch 26 | loss: 0.12357 | val_0_rmse: 0.38228 | val_1_rmse: 0.41126 |  0:19:34s
epoch 27 | loss: 0.12375 | val_0_rmse: 0.39023 | val_1_rmse: 0.42723 |  0:20:17s
epoch 28 | loss: 0.12259 | val_0_rmse: 0.38492 | val_1_rmse: 0.42199 |  0:21:01s
epoch 29 | loss: 0.12337 | val_0_rmse: 0.41925 | val_1_rmse: 0.41476 |  0:21:44s
epoch 30 | loss: 0.12321 | val_0_rmse: 0.37042 | val_1_rmse: 0.4057  |  0:22:29s
epoch 31 | loss: 0.12087 | val_0_rmse: 0.36876 | val_1_rmse: 0.41911 |  0:23:13s
epoch 32 | loss: 0.12005 | val_0_rmse: 0.36902 | val_1_rmse: 0.40825 |  0:23:57s
epoch 33 | loss: 0.11955 | val_0_rmse: 0.3664  | val_1_rmse: 0.4054  |  0:24:41s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.39818
Best weights from best epoch are automatically used!
ended training at: 19:11:05
Feature importance:
Mean squared error is of 7290328716.8541155
Mean absolute error:54116.79621467776
MAPE:0.27461043384041994
R2 score:0.8337228866418274
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:11:18
epoch 0  | loss: 0.5185  | val_0_rmse: 0.58234 | val_1_rmse: 0.5798  |  0:00:43s
epoch 1  | loss: 0.22314 | val_0_rmse: 0.50984 | val_1_rmse: 0.50898 |  0:01:27s
epoch 2  | loss: 0.18525 | val_0_rmse: 0.4566  | val_1_rmse: 0.45857 |  0:02:13s
epoch 3  | loss: 0.17299 | val_0_rmse: 0.39956 | val_1_rmse: 0.40833 |  0:02:57s
epoch 4  | loss: 0.16326 | val_0_rmse: 0.38884 | val_1_rmse: 0.40525 |  0:03:41s
epoch 5  | loss: 0.15785 | val_0_rmse: 0.40681 | val_1_rmse: 0.42156 |  0:04:25s
epoch 6  | loss: 0.15354 | val_0_rmse: 0.43486 | val_1_rmse: 0.45161 |  0:05:09s
epoch 7  | loss: 0.15011 | val_0_rmse: 0.41778 | val_1_rmse: 0.43427 |  0:05:53s
epoch 8  | loss: 0.14657 | val_0_rmse: 0.41551 | val_1_rmse: 0.43359 |  0:06:37s
epoch 9  | loss: 0.14404 | val_0_rmse: 0.40098 | val_1_rmse: 0.42088 |  0:07:20s
epoch 10 | loss: 0.14318 | val_0_rmse: 0.39251 | val_1_rmse: 0.41456 |  0:08:04s
epoch 11 | loss: 0.14059 | val_0_rmse: 0.39333 | val_1_rmse: 0.41974 |  0:08:48s
epoch 12 | loss: 0.1409  | val_0_rmse: 0.48638 | val_1_rmse: 0.46758 |  0:09:31s
epoch 13 | loss: 0.1379  | val_0_rmse: 1.09506 | val_1_rmse: 1.00848 |  0:10:15s
epoch 14 | loss: 0.13735 | val_0_rmse: 0.42107 | val_1_rmse: 0.43742 |  0:10:59s
epoch 15 | loss: 0.13469 | val_0_rmse: 0.41979 | val_1_rmse: 0.44139 |  0:11:42s
epoch 16 | loss: 0.13512 | val_0_rmse: 0.47443 | val_1_rmse: 0.51348 |  0:12:26s
epoch 17 | loss: 0.13335 | val_0_rmse: 0.54625 | val_1_rmse: 0.55218 |  0:13:10s
epoch 18 | loss: 0.13206 | val_0_rmse: 0.37742 | val_1_rmse: 0.40974 |  0:13:54s
epoch 19 | loss: 0.13178 | val_0_rmse: 0.39874 | val_1_rmse: 0.42711 |  0:14:37s
epoch 20 | loss: 0.1303  | val_0_rmse: 0.39023 | val_1_rmse: 0.41657 |  0:15:21s
epoch 21 | loss: 0.13132 | val_0_rmse: 0.40917 | val_1_rmse: 0.43342 |  0:16:05s
epoch 22 | loss: 0.13083 | val_0_rmse: 0.42006 | val_1_rmse: 0.44488 |  0:16:48s
epoch 23 | loss: 0.12871 | val_0_rmse: 0.44902 | val_1_rmse: 0.46632 |  0:17:33s
epoch 24 | loss: 0.12942 | val_0_rmse: 0.38415 | val_1_rmse: 0.41449 |  0:18:17s
epoch 25 | loss: 0.12769 | val_0_rmse: 0.39319 | val_1_rmse: 0.42395 |  0:19:00s
epoch 26 | loss: 0.12822 | val_0_rmse: 0.37559 | val_1_rmse: 0.40801 |  0:19:44s
epoch 27 | loss: 0.12752 | val_0_rmse: 0.37656 | val_1_rmse: 0.41324 |  0:20:28s
epoch 28 | loss: 0.12513 | val_0_rmse: 0.37276 | val_1_rmse: 0.40917 |  0:21:11s
epoch 29 | loss: 0.1256  | val_0_rmse: 0.3934  | val_1_rmse: 0.42412 |  0:21:55s
epoch 30 | loss: 0.12533 | val_0_rmse: 0.37937 | val_1_rmse: 0.42499 |  0:22:39s
epoch 31 | loss: 0.1237  | val_0_rmse: 0.37601 | val_1_rmse: 0.4084  |  0:23:22s
epoch 32 | loss: 0.12295 | val_0_rmse: 0.3706  | val_1_rmse: 0.40686 |  0:24:06s
epoch 33 | loss: 0.12218 | val_0_rmse: 0.39801 | val_1_rmse: 0.42129 |  0:24:50s
epoch 34 | loss: 0.12327 | val_0_rmse: 0.38308 | val_1_rmse: 0.40676 |  0:25:33s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.40525
Best weights from best epoch are automatically used!
ended training at: 19:37:20
Feature importance:
Mean squared error is of 8399527160.138046
Mean absolute error:55405.0426063825
MAPE:0.2648677306540931
R2 score:0.8101872539463457
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:37:28
epoch 0  | loss: 0.44315 | val_0_rmse: 0.64869 | val_1_rmse: 0.64799 |  0:00:43s
epoch 1  | loss: 0.20812 | val_0_rmse: 0.52714 | val_1_rmse: 0.52921 |  0:01:26s
epoch 2  | loss: 0.18829 | val_0_rmse: 0.42654 | val_1_rmse: 0.4314  |  0:02:10s
epoch 3  | loss: 0.17502 | val_0_rmse: 0.4103  | val_1_rmse: 0.42053 |  0:02:53s
epoch 4  | loss: 0.16921 | val_0_rmse: 0.402   | val_1_rmse: 0.41583 |  0:03:37s
epoch 5  | loss: 0.16177 | val_0_rmse: 0.42647 | val_1_rmse: 0.44208 |  0:04:20s
epoch 6  | loss: 0.16371 | val_0_rmse: 0.40263 | val_1_rmse: 0.42158 |  0:05:04s
epoch 7  | loss: 0.15727 | val_0_rmse: 0.41645 | val_1_rmse: 0.43785 |  0:05:47s
epoch 8  | loss: 0.15506 | val_0_rmse: 0.42647 | val_1_rmse: 0.44451 |  0:06:32s
epoch 9  | loss: 0.15278 | val_0_rmse: 0.40316 | val_1_rmse: 0.42944 |  0:07:16s
epoch 10 | loss: 0.15275 | val_0_rmse: 0.44655 | val_1_rmse: 0.46888 |  0:08:00s
epoch 11 | loss: 0.15279 | val_0_rmse: 0.41805 | val_1_rmse: 0.44208 |  0:08:43s
epoch 12 | loss: 0.15081 | val_0_rmse: 0.61109 | val_1_rmse: 0.62595 |  0:09:27s
epoch 13 | loss: 0.14727 | val_0_rmse: 0.4113  | val_1_rmse: 0.44007 |  0:10:11s
epoch 14 | loss: 0.14662 | val_0_rmse: 0.39754 | val_1_rmse: 0.42475 |  0:10:54s
epoch 15 | loss: 0.14656 | val_0_rmse: 0.39098 | val_1_rmse: 0.418   |  0:11:38s
epoch 16 | loss: 0.1451  | val_0_rmse: 0.42442 | val_1_rmse: 0.45121 |  0:12:22s
epoch 17 | loss: 0.14599 | val_0_rmse: 0.42852 | val_1_rmse: 0.45905 |  0:13:05s
epoch 18 | loss: 0.15577 | val_0_rmse: 0.44403 | val_1_rmse: 0.45637 |  0:13:49s
epoch 19 | loss: 0.14947 | val_0_rmse: 0.39848 | val_1_rmse: 0.4218  |  0:14:33s
epoch 20 | loss: 0.14113 | val_0_rmse: 0.39052 | val_1_rmse: 0.41306 |  0:15:16s
epoch 21 | loss: 0.13921 | val_0_rmse: 0.55473 | val_1_rmse: 0.57716 |  0:16:00s
epoch 22 | loss: 0.13826 | val_0_rmse: 0.38911 | val_1_rmse: 0.41421 |  0:16:43s
epoch 23 | loss: 0.14298 | val_0_rmse: 0.41026 | val_1_rmse: 0.43374 |  0:17:27s
epoch 24 | loss: 0.14048 | val_0_rmse: 0.38855 | val_1_rmse: 0.41457 |  0:18:10s
epoch 25 | loss: 0.1385  | val_0_rmse: 0.40849 | val_1_rmse: 0.43255 |  0:18:54s
epoch 26 | loss: 0.15224 | val_0_rmse: 0.40673 | val_1_rmse: 0.43049 |  0:19:37s
epoch 27 | loss: 0.14676 | val_0_rmse: 0.49689 | val_1_rmse: 0.52006 |  0:20:21s
epoch 28 | loss: 0.15628 | val_0_rmse: 0.41054 | val_1_rmse: 0.43584 |  0:21:05s
epoch 29 | loss: 0.13995 | val_0_rmse: 0.40544 | val_1_rmse: 0.43183 |  0:21:49s
epoch 30 | loss: 0.1371  | val_0_rmse: 0.4069  | val_1_rmse: 0.43241 |  0:22:33s
epoch 31 | loss: 0.13389 | val_0_rmse: 0.4254  | val_1_rmse: 0.44673 |  0:23:17s
epoch 32 | loss: 0.13107 | val_0_rmse: 0.40218 | val_1_rmse: 0.40926 |  0:24:01s
epoch 33 | loss: 0.12859 | val_0_rmse: 0.38098 | val_1_rmse: 0.40694 |  0:24:44s
epoch 34 | loss: 0.12827 | val_0_rmse: 0.47216 | val_1_rmse: 0.42832 |  0:25:28s
epoch 35 | loss: 0.13037 | val_0_rmse: 0.43901 | val_1_rmse: 0.4345  |  0:26:12s
epoch 36 | loss: 0.13102 | val_0_rmse: 0.37856 | val_1_rmse: 0.4083  |  0:26:55s
epoch 37 | loss: 0.12767 | val_0_rmse: 0.43156 | val_1_rmse: 0.42157 |  0:27:39s
epoch 38 | loss: 0.12725 | val_0_rmse: 0.38141 | val_1_rmse: 0.41191 |  0:28:23s
epoch 39 | loss: 0.12624 | val_0_rmse: 0.42231 | val_1_rmse: 0.42647 |  0:29:06s
epoch 40 | loss: 0.12556 | val_0_rmse: 0.39582 | val_1_rmse: 0.42342 |  0:29:50s
epoch 41 | loss: 0.12826 | val_0_rmse: 0.41398 | val_1_rmse: 0.44098 |  0:30:34s
epoch 42 | loss: 0.12626 | val_0_rmse: 0.38404 | val_1_rmse: 0.41295 |  0:31:17s
epoch 43 | loss: 0.12629 | val_0_rmse: 0.46191 | val_1_rmse: 0.48299 |  0:32:01s
epoch 44 | loss: 0.12507 | val_0_rmse: 0.38977 | val_1_rmse: 0.42012 |  0:32:45s
epoch 45 | loss: 0.12274 | val_0_rmse: 0.39029 | val_1_rmse: 0.42012 |  0:33:28s
epoch 46 | loss: 0.12221 | val_0_rmse: 0.39215 | val_1_rmse: 0.42513 |  0:34:12s
epoch 47 | loss: 0.12182 | val_0_rmse: 0.37899 | val_1_rmse: 0.41114 |  0:34:56s
epoch 48 | loss: 0.12019 | val_0_rmse: 0.38711 | val_1_rmse: 0.42242 |  0:35:39s
epoch 49 | loss: 0.12117 | val_0_rmse: 0.36937 | val_1_rmse: 0.41019 |  0:36:23s
epoch 50 | loss: 0.11967 | val_0_rmse: 0.36397 | val_1_rmse: 0.40282 |  0:37:08s
epoch 51 | loss: 0.12008 | val_0_rmse: 0.37276 | val_1_rmse: 0.41433 |  0:37:51s
epoch 52 | loss: 0.11939 | val_0_rmse: 1.13717 | val_1_rmse: 0.41559 |  0:38:35s
epoch 53 | loss: 0.11877 | val_0_rmse: 2.06319 | val_1_rmse: 4.06766 |  0:39:18s
epoch 54 | loss: 0.12009 | val_0_rmse: 0.38296 | val_1_rmse: 0.42199 |  0:40:02s
epoch 55 | loss: 0.12561 | val_0_rmse: 0.38496 | val_1_rmse: 0.42107 |  0:40:45s
epoch 56 | loss: 0.12244 | val_0_rmse: 1.22829 | val_1_rmse: 2.37705 |  0:41:28s
epoch 57 | loss: 0.11884 | val_0_rmse: 1.27515 | val_1_rmse: 2.47744 |  0:42:12s
epoch 58 | loss: 0.11769 | val_0_rmse: 0.38083 | val_1_rmse: 0.4161  |  0:42:55s
epoch 59 | loss: 0.14071 | val_0_rmse: 0.40928 | val_1_rmse: 0.46294 |  0:43:39s
epoch 60 | loss: 0.12886 | val_0_rmse: 0.42845 | val_1_rmse: 0.65576 |  0:44:22s
epoch 61 | loss: 0.12365 | val_0_rmse: 0.41009 | val_1_rmse: 0.4146  |  0:45:06s
epoch 62 | loss: 0.12141 | val_0_rmse: 0.38431 | val_1_rmse: 0.42136 |  0:45:50s
epoch 63 | loss: 0.12011 | val_0_rmse: 0.37521 | val_1_rmse: 0.41418 |  0:46:33s
epoch 64 | loss: 0.12168 | val_0_rmse: 0.37143 | val_1_rmse: 0.41387 |  0:47:17s
epoch 65 | loss: 0.11908 | val_0_rmse: 0.39482 | val_1_rmse: 0.43525 |  0:48:00s
epoch 66 | loss: 0.12005 | val_0_rmse: 0.62167 | val_1_rmse: 0.65172 |  0:48:44s
epoch 67 | loss: 0.12374 | val_0_rmse: 0.42437 | val_1_rmse: 0.46131 |  0:49:27s
epoch 68 | loss: 0.11671 | val_0_rmse: 0.36142 | val_1_rmse: 0.40319 |  0:50:11s
epoch 69 | loss: 0.11543 | val_0_rmse: 0.37389 | val_1_rmse: 0.41767 |  0:50:54s
epoch 70 | loss: 0.11471 | val_0_rmse: 0.37047 | val_1_rmse: 0.41828 |  0:51:38s
epoch 71 | loss: 0.11444 | val_0_rmse: 0.38533 | val_1_rmse: 0.42431 |  0:52:22s
epoch 72 | loss: 0.11595 | val_0_rmse: 0.37025 | val_1_rmse: 0.41397 |  0:53:06s
epoch 73 | loss: 0.11538 | val_0_rmse: 0.36923 | val_1_rmse: 0.41355 |  0:53:50s
epoch 74 | loss: 0.12038 | val_0_rmse: 0.36889 | val_1_rmse: 0.40818 |  0:54:33s
epoch 75 | loss: 0.11838 | val_0_rmse: 0.36893 | val_1_rmse: 0.4098  |  0:55:17s
epoch 76 | loss: 0.11427 | val_0_rmse: 0.36727 | val_1_rmse: 0.41019 |  0:56:01s
epoch 77 | loss: 0.11351 | val_0_rmse: 0.35995 | val_1_rmse: 0.40472 |  0:56:45s
epoch 78 | loss: 0.1127  | val_0_rmse: 0.37259 | val_1_rmse: 0.41853 |  0:57:28s
epoch 79 | loss: 0.11273 | val_0_rmse: 0.38515 | val_1_rmse: 0.42657 |  0:58:12s
epoch 80 | loss: 0.11259 | val_0_rmse: 0.3722  | val_1_rmse: 0.41771 |  0:58:55s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.40282
Best weights from best epoch are automatically used!
ended training at: 20:36:51
Feature importance:
Mean squared error is of 7013782836.655064
Mean absolute error:51521.97386823527
MAPE:0.264023540977569
R2 score:0.8395980350145604
------------------------------------------------------------------
