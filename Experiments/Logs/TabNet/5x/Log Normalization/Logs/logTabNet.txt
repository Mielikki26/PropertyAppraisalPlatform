TabNet Logs:

Saving copy of script...
In this script all the datasets are used and the features are normalized using the logarithmic function
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:18:06
epoch 0  | loss: 48.48484| val_0_rmse: 6.3263  | val_1_rmse: 6.31001 |  0:00:05s
epoch 1  | loss: 0.77594 | val_0_rmse: 1.03701 | val_1_rmse: 1.02682 |  0:00:06s
epoch 2  | loss: 0.51189 | val_0_rmse: 1.68284 | val_1_rmse: 1.66944 |  0:00:08s
epoch 3  | loss: 0.23664 | val_0_rmse: 2.78086 | val_1_rmse: 2.75922 |  0:00:09s
epoch 4  | loss: 0.1681  | val_0_rmse: 1.08928 | val_1_rmse: 1.07133 |  0:00:11s
epoch 5  | loss: 0.1704  | val_0_rmse: 0.72655 | val_1_rmse: 0.71234 |  0:00:12s
epoch 6  | loss: 0.16272 | val_0_rmse: 0.65948 | val_1_rmse: 0.64839 |  0:00:14s
epoch 7  | loss: 0.18852 | val_0_rmse: 0.57679 | val_1_rmse: 0.56006 |  0:00:15s
epoch 8  | loss: 0.17423 | val_0_rmse: 0.45279 | val_1_rmse: 0.43975 |  0:00:17s
epoch 9  | loss: 0.174   | val_0_rmse: 0.51661 | val_1_rmse: 0.50335 |  0:00:18s
epoch 10 | loss: 0.16594 | val_0_rmse: 0.4795  | val_1_rmse: 0.46379 |  0:00:20s
epoch 11 | loss: 0.15785 | val_0_rmse: 0.45439 | val_1_rmse: 0.44044 |  0:00:21s
epoch 12 | loss: 0.16253 | val_0_rmse: 0.4775  | val_1_rmse: 0.46164 |  0:00:23s
epoch 13 | loss: 0.16151 | val_0_rmse: 0.43408 | val_1_rmse: 0.4146  |  0:00:24s
epoch 14 | loss: 0.15677 | val_0_rmse: 0.44444 | val_1_rmse: 0.43021 |  0:00:26s
epoch 15 | loss: 0.15827 | val_0_rmse: 0.45517 | val_1_rmse: 0.43351 |  0:00:27s
epoch 16 | loss: 0.15548 | val_0_rmse: 0.51738 | val_1_rmse: 0.49722 |  0:00:29s
epoch 17 | loss: 0.16022 | val_0_rmse: 0.45717 | val_1_rmse: 0.4347  |  0:00:30s
epoch 18 | loss: 0.15242 | val_0_rmse: 0.42695 | val_1_rmse: 0.40941 |  0:00:32s
epoch 19 | loss: 0.13528 | val_0_rmse: 0.40815 | val_1_rmse: 0.39177 |  0:00:33s
epoch 20 | loss: 0.14961 | val_0_rmse: 0.47796 | val_1_rmse: 0.46307 |  0:00:35s
epoch 21 | loss: 0.14759 | val_0_rmse: 0.3883  | val_1_rmse: 0.37025 |  0:00:36s
epoch 22 | loss: 0.12792 | val_0_rmse: 0.4421  | val_1_rmse: 0.42577 |  0:00:38s
epoch 23 | loss: 0.11966 | val_0_rmse: 0.45226 | val_1_rmse: 0.43381 |  0:00:39s
epoch 24 | loss: 0.11908 | val_0_rmse: 0.43149 | val_1_rmse: 0.41097 |  0:00:41s
epoch 25 | loss: 0.11537 | val_0_rmse: 0.40127 | val_1_rmse: 0.37989 |  0:00:42s
epoch 26 | loss: 0.13096 | val_0_rmse: 0.40495 | val_1_rmse: 0.38835 |  0:00:44s
epoch 27 | loss: 0.11458 | val_0_rmse: 0.3979  | val_1_rmse: 0.38126 |  0:00:45s
epoch 28 | loss: 0.10448 | val_0_rmse: 0.39156 | val_1_rmse: 0.37396 |  0:00:47s
epoch 29 | loss: 0.10222 | val_0_rmse: 0.3924  | val_1_rmse: 0.37508 |  0:00:48s
epoch 30 | loss: 0.10149 | val_0_rmse: 0.3759  | val_1_rmse: 0.35725 |  0:00:50s
epoch 31 | loss: 0.09746 | val_0_rmse: 0.38782 | val_1_rmse: 0.36772 |  0:00:51s
epoch 32 | loss: 0.10118 | val_0_rmse: 0.38033 | val_1_rmse: 0.36294 |  0:00:53s
epoch 33 | loss: 0.1319  | val_0_rmse: 0.40817 | val_1_rmse: 0.38854 |  0:00:54s
epoch 34 | loss: 0.12538 | val_0_rmse: 0.3812  | val_1_rmse: 0.3621  |  0:00:56s
epoch 35 | loss: 0.11213 | val_0_rmse: 0.40747 | val_1_rmse: 0.38798 |  0:00:57s
epoch 36 | loss: 0.09163 | val_0_rmse: 0.36851 | val_1_rmse: 0.34973 |  0:00:58s
epoch 37 | loss: 0.0946  | val_0_rmse: 0.43326 | val_1_rmse: 0.41528 |  0:01:00s
epoch 38 | loss: 0.0934  | val_0_rmse: 0.36638 | val_1_rmse: 0.34852 |  0:01:01s
epoch 39 | loss: 0.10532 | val_0_rmse: 0.36767 | val_1_rmse: 0.34952 |  0:01:03s
epoch 40 | loss: 0.13643 | val_0_rmse: 0.46657 | val_1_rmse: 0.4462  |  0:01:04s
epoch 41 | loss: 0.12906 | val_0_rmse: 0.35293 | val_1_rmse: 0.33197 |  0:01:06s
epoch 42 | loss: 0.10011 | val_0_rmse: 0.35053 | val_1_rmse: 0.33137 |  0:01:07s
epoch 43 | loss: 0.09342 | val_0_rmse: 0.38124 | val_1_rmse: 0.36132 |  0:01:09s
epoch 44 | loss: 0.08976 | val_0_rmse: 0.35465 | val_1_rmse: 0.33365 |  0:01:10s
epoch 45 | loss: 0.09251 | val_0_rmse: 0.33143 | val_1_rmse: 0.31109 |  0:01:12s
epoch 46 | loss: 0.11852 | val_0_rmse: 0.33587 | val_1_rmse: 0.31465 |  0:01:13s
epoch 47 | loss: 0.08746 | val_0_rmse: 0.32911 | val_1_rmse: 0.30876 |  0:01:15s
epoch 48 | loss: 0.09538 | val_0_rmse: 0.3181  | val_1_rmse: 0.29918 |  0:01:16s
epoch 49 | loss: 0.10015 | val_0_rmse: 0.32754 | val_1_rmse: 0.30747 |  0:01:18s
epoch 50 | loss: 0.08324 | val_0_rmse: 0.32764 | val_1_rmse: 0.30773 |  0:01:19s
epoch 51 | loss: 0.08339 | val_0_rmse: 0.31911 | val_1_rmse: 0.30039 |  0:01:21s
epoch 52 | loss: 0.09693 | val_0_rmse: 0.30226 | val_1_rmse: 0.28425 |  0:01:22s
epoch 53 | loss: 0.08311 | val_0_rmse: 0.30456 | val_1_rmse: 0.28822 |  0:01:24s
epoch 54 | loss: 0.0925  | val_0_rmse: 0.28342 | val_1_rmse: 0.26609 |  0:01:25s
epoch 55 | loss: 0.0752  | val_0_rmse: 0.30488 | val_1_rmse: 0.28832 |  0:01:27s
epoch 56 | loss: 0.07992 | val_0_rmse: 0.27491 | val_1_rmse: 0.25987 |  0:01:28s
epoch 57 | loss: 0.098   | val_0_rmse: 0.28519 | val_1_rmse: 0.26778 |  0:01:30s
epoch 58 | loss: 0.07751 | val_0_rmse: 0.29447 | val_1_rmse: 0.27915 |  0:01:31s
epoch 59 | loss: 0.07789 | val_0_rmse: 0.28105 | val_1_rmse: 0.26628 |  0:01:33s
epoch 60 | loss: 0.07668 | val_0_rmse: 0.27422 | val_1_rmse: 0.26311 |  0:01:34s
epoch 61 | loss: 0.09833 | val_0_rmse: 0.34026 | val_1_rmse: 0.32419 |  0:01:36s
epoch 62 | loss: 0.12637 | val_0_rmse: 0.45505 | val_1_rmse: 0.44313 |  0:01:37s
epoch 63 | loss: 0.16643 | val_0_rmse: 0.47285 | val_1_rmse: 0.46998 |  0:01:39s
epoch 64 | loss: 0.13094 | val_0_rmse: 0.26146 | val_1_rmse: 0.24759 |  0:01:40s
epoch 65 | loss: 0.07421 | val_0_rmse: 0.26057 | val_1_rmse: 0.24879 |  0:01:42s
epoch 66 | loss: 0.08016 | val_0_rmse: 0.28397 | val_1_rmse: 0.27487 |  0:01:43s
epoch 67 | loss: 0.07386 | val_0_rmse: 0.26034 | val_1_rmse: 0.25024 |  0:01:45s
epoch 68 | loss: 0.07681 | val_0_rmse: 0.26342 | val_1_rmse: 0.25063 |  0:01:46s
epoch 69 | loss: 0.13401 | val_0_rmse: 0.28553 | val_1_rmse: 0.27002 |  0:01:48s
epoch 70 | loss: 0.10905 | val_0_rmse: 0.34837 | val_1_rmse: 0.34497 |  0:01:49s
epoch 71 | loss: 0.11177 | val_0_rmse: 0.27169 | val_1_rmse: 0.25843 |  0:01:51s
epoch 72 | loss: 0.10519 | val_0_rmse: 0.261   | val_1_rmse: 0.24696 |  0:01:52s
epoch 73 | loss: 0.10669 | val_0_rmse: 0.37059 | val_1_rmse: 0.36666 |  0:01:54s
epoch 74 | loss: 0.11259 | val_0_rmse: 0.29884 | val_1_rmse: 0.28565 |  0:01:55s
epoch 75 | loss: 0.10498 | val_0_rmse: 0.2714  | val_1_rmse: 0.25613 |  0:01:57s
epoch 76 | loss: 0.10245 | val_0_rmse: 0.35375 | val_1_rmse: 0.34931 |  0:01:58s
epoch 77 | loss: 0.10985 | val_0_rmse: 0.26604 | val_1_rmse: 0.2516  |  0:02:00s
epoch 78 | loss: 0.10824 | val_0_rmse: 0.26512 | val_1_rmse: 0.25242 |  0:02:01s
epoch 79 | loss: 0.10916 | val_0_rmse: 0.33065 | val_1_rmse: 0.32646 |  0:02:03s
epoch 80 | loss: 0.10404 | val_0_rmse: 0.27474 | val_1_rmse: 0.26082 |  0:02:04s
epoch 81 | loss: 0.104   | val_0_rmse: 0.25342 | val_1_rmse: 0.23952 |  0:02:06s
epoch 82 | loss: 0.10594 | val_0_rmse: 0.30332 | val_1_rmse: 0.29644 |  0:02:07s
epoch 83 | loss: 0.08543 | val_0_rmse: 0.26334 | val_1_rmse: 0.24846 |  0:02:08s
epoch 84 | loss: 0.07975 | val_0_rmse: 0.26963 | val_1_rmse: 0.25431 |  0:02:10s
epoch 85 | loss: 0.07984 | val_0_rmse: 0.2643  | val_1_rmse: 0.25059 |  0:02:11s
epoch 86 | loss: 0.07922 | val_0_rmse: 0.26079 | val_1_rmse: 0.2469  |  0:02:13s
epoch 87 | loss: 0.07998 | val_0_rmse: 0.26671 | val_1_rmse: 0.25273 |  0:02:14s
epoch 88 | loss: 0.07617 | val_0_rmse: 0.28641 | val_1_rmse: 0.27117 |  0:02:16s
epoch 89 | loss: 0.07926 | val_0_rmse: 0.257   | val_1_rmse: 0.24676 |  0:02:17s
epoch 90 | loss: 0.07131 | val_0_rmse: 0.27889 | val_1_rmse: 0.27081 |  0:02:19s
epoch 91 | loss: 0.07697 | val_0_rmse: 0.25518 | val_1_rmse: 0.24059 |  0:02:20s
epoch 92 | loss: 0.09    | val_0_rmse: 0.39081 | val_1_rmse: 0.37819 |  0:02:22s
epoch 93 | loss: 0.09156 | val_0_rmse: 0.26157 | val_1_rmse: 0.25219 |  0:02:23s
epoch 94 | loss: 0.10472 | val_0_rmse: 0.36718 | val_1_rmse: 0.3615  |  0:02:25s
epoch 95 | loss: 0.07931 | val_0_rmse: 0.27184 | val_1_rmse: 0.25978 |  0:02:26s
epoch 96 | loss: 0.1105  | val_0_rmse: 0.29372 | val_1_rmse: 0.28614 |  0:02:28s
epoch 97 | loss: 0.08311 | val_0_rmse: 0.26215 | val_1_rmse: 0.2455  |  0:02:29s
epoch 98 | loss: 0.07867 | val_0_rmse: 0.26497 | val_1_rmse: 0.25119 |  0:02:31s
epoch 99 | loss: 0.07406 | val_0_rmse: 0.31393 | val_1_rmse: 0.30497 |  0:02:32s
epoch 100| loss: 0.14186 | val_0_rmse: 0.37194 | val_1_rmse: 0.36739 |  0:02:34s
epoch 101| loss: 0.09122 | val_0_rmse: 0.25846 | val_1_rmse: 0.24532 |  0:02:35s
epoch 102| loss: 0.12109 | val_0_rmse: 0.3241  | val_1_rmse: 0.31695 |  0:02:37s
epoch 103| loss: 0.08253 | val_0_rmse: 0.28997 | val_1_rmse: 0.28504 |  0:02:38s
epoch 104| loss: 0.08558 | val_0_rmse: 0.26022 | val_1_rmse: 0.24962 |  0:02:40s
epoch 105| loss: 0.07916 | val_0_rmse: 0.27496 | val_1_rmse: 0.26675 |  0:02:41s
epoch 106| loss: 0.07816 | val_0_rmse: 0.27432 | val_1_rmse: 0.26519 |  0:02:43s
epoch 107| loss: 0.07831 | val_0_rmse: 0.26177 | val_1_rmse: 0.25252 |  0:02:44s
epoch 108| loss: 0.07902 | val_0_rmse: 0.27197 | val_1_rmse: 0.26462 |  0:02:46s
epoch 109| loss: 0.08088 | val_0_rmse: 0.30204 | val_1_rmse: 0.29369 |  0:02:48s
epoch 110| loss: 0.0763  | val_0_rmse: 0.2586  | val_1_rmse: 0.24825 |  0:02:49s
epoch 111| loss: 0.08758 | val_0_rmse: 0.27873 | val_1_rmse: 0.26921 |  0:02:51s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.23952
Best weights from best epoch are automatically used!
ended training at: 16:20:58
Feature importance:
[('Area', 0.07831644311521292), ('Baths', 0.4073510833197163), ('Beds', 1.5180827632096364e-06), ('Latitude', 0.1382093874501156), ('Longitude', 0.13068408784345262), ('Month', 0.004510801345499513), ('Year', 0.2409266788432398)]
Mean squared error is of 7292814086.433648
Mean absolute error:59929.308060227646
MAPE:0.19431745123251315
R2 score:0.6740064712048419
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:20:59
epoch 0  | loss: 47.06024| val_0_rmse: 4.78143 | val_1_rmse: 4.77253 |  0:00:01s
epoch 1  | loss: 0.75455 | val_0_rmse: 1.46574 | val_1_rmse: 1.45672 |  0:00:03s
epoch 2  | loss: 0.23502 | val_0_rmse: 0.73462 | val_1_rmse: 0.72376 |  0:00:04s
epoch 3  | loss: 0.17039 | val_0_rmse: 0.97514 | val_1_rmse: 0.96331 |  0:00:06s
epoch 4  | loss: 0.1514  | val_0_rmse: 0.93193 | val_1_rmse: 0.92489 |  0:00:07s
epoch 5  | loss: 0.12515 | val_0_rmse: 1.02792 | val_1_rmse: 1.01946 |  0:00:08s
epoch 6  | loss: 0.13195 | val_0_rmse: 0.62713 | val_1_rmse: 0.61713 |  0:00:10s
epoch 7  | loss: 0.1333  | val_0_rmse: 0.48528 | val_1_rmse: 0.47562 |  0:00:12s
epoch 8  | loss: 0.12247 | val_0_rmse: 0.4296  | val_1_rmse: 0.41984 |  0:00:13s
epoch 9  | loss: 0.11838 | val_0_rmse: 0.56523 | val_1_rmse: 0.55559 |  0:00:15s
epoch 10 | loss: 0.11722 | val_0_rmse: 0.53417 | val_1_rmse: 0.52486 |  0:00:16s
epoch 11 | loss: 0.11478 | val_0_rmse: 0.42912 | val_1_rmse: 0.4213  |  0:00:18s
epoch 12 | loss: 0.11676 | val_0_rmse: 0.57163 | val_1_rmse: 0.56879 |  0:00:19s
epoch 13 | loss: 0.1135  | val_0_rmse: 0.48562 | val_1_rmse: 0.47935 |  0:00:21s
epoch 14 | loss: 0.10691 | val_0_rmse: 0.44942 | val_1_rmse: 0.44638 |  0:00:22s
epoch 15 | loss: 0.10937 | val_0_rmse: 0.3923  | val_1_rmse: 0.38616 |  0:00:23s
epoch 16 | loss: 0.16138 | val_0_rmse: 0.42606 | val_1_rmse: 0.42289 |  0:00:25s
epoch 17 | loss: 0.1332  | val_0_rmse: 0.4714  | val_1_rmse: 0.46969 |  0:00:26s
epoch 18 | loss: 0.10743 | val_0_rmse: 0.40516 | val_1_rmse: 0.40102 |  0:00:28s
epoch 19 | loss: 0.10319 | val_0_rmse: 0.38408 | val_1_rmse: 0.3785  |  0:00:29s
epoch 20 | loss: 0.09618 | val_0_rmse: 0.38591 | val_1_rmse: 0.38106 |  0:00:31s
epoch 21 | loss: 0.11102 | val_0_rmse: 0.46454 | val_1_rmse: 0.46516 |  0:00:32s
epoch 22 | loss: 0.19186 | val_0_rmse: 0.43826 | val_1_rmse: 0.43579 |  0:00:34s
epoch 23 | loss: 0.14627 | val_0_rmse: 0.46974 | val_1_rmse: 0.46074 |  0:00:35s
epoch 24 | loss: 0.17506 | val_0_rmse: 0.48732 | val_1_rmse: 0.47663 |  0:00:37s
epoch 25 | loss: 0.14584 | val_0_rmse: 0.43571 | val_1_rmse: 0.43541 |  0:00:38s
epoch 26 | loss: 0.1314  | val_0_rmse: 0.40816 | val_1_rmse: 0.40597 |  0:00:40s
epoch 27 | loss: 0.12859 | val_0_rmse: 0.43173 | val_1_rmse: 0.4223  |  0:00:41s
epoch 28 | loss: 0.12857 | val_0_rmse: 0.44672 | val_1_rmse: 0.4465  |  0:00:43s
epoch 29 | loss: 0.12834 | val_0_rmse: 0.4426  | val_1_rmse: 0.441   |  0:00:44s
epoch 30 | loss: 0.12437 | val_0_rmse: 0.43374 | val_1_rmse: 0.42357 |  0:00:46s
epoch 31 | loss: 0.12927 | val_0_rmse: 0.42658 | val_1_rmse: 0.42222 |  0:00:47s
epoch 32 | loss: 0.12859 | val_0_rmse: 0.40068 | val_1_rmse: 0.39536 |  0:00:49s
epoch 33 | loss: 0.1251  | val_0_rmse: 0.42852 | val_1_rmse: 0.41955 |  0:00:50s
epoch 34 | loss: 0.12294 | val_0_rmse: 0.38369 | val_1_rmse: 0.37881 |  0:00:52s
epoch 35 | loss: 0.12746 | val_0_rmse: 0.37613 | val_1_rmse: 0.37059 |  0:00:53s
epoch 36 | loss: 0.12433 | val_0_rmse: 0.4582  | val_1_rmse: 0.44859 |  0:00:55s
epoch 37 | loss: 0.13575 | val_0_rmse: 0.37598 | val_1_rmse: 0.36967 |  0:00:56s
epoch 38 | loss: 0.14378 | val_0_rmse: 0.36126 | val_1_rmse: 0.35222 |  0:00:58s
epoch 39 | loss: 0.12843 | val_0_rmse: 0.43783 | val_1_rmse: 0.4291  |  0:00:59s
epoch 40 | loss: 0.12568 | val_0_rmse: 0.35689 | val_1_rmse: 0.35009 |  0:01:01s
epoch 41 | loss: 0.13028 | val_0_rmse: 0.3457  | val_1_rmse: 0.33938 |  0:01:02s
epoch 42 | loss: 0.13339 | val_0_rmse: 0.40044 | val_1_rmse: 0.39189 |  0:01:04s
epoch 43 | loss: 0.17054 | val_0_rmse: 0.45224 | val_1_rmse: 0.44917 |  0:01:05s
epoch 44 | loss: 0.16901 | val_0_rmse: 0.35614 | val_1_rmse: 0.34968 |  0:01:07s
epoch 45 | loss: 0.12659 | val_0_rmse: 0.33885 | val_1_rmse: 0.33081 |  0:01:08s
epoch 46 | loss: 0.12149 | val_0_rmse: 0.40269 | val_1_rmse: 0.39365 |  0:01:10s
epoch 47 | loss: 0.12964 | val_0_rmse: 0.36069 | val_1_rmse: 0.35247 |  0:01:11s
epoch 48 | loss: 0.11212 | val_0_rmse: 0.3238  | val_1_rmse: 0.31626 |  0:01:13s
epoch 49 | loss: 0.10253 | val_0_rmse: 0.3334  | val_1_rmse: 0.3244  |  0:01:14s
epoch 50 | loss: 0.10652 | val_0_rmse: 0.34105 | val_1_rmse: 0.32983 |  0:01:16s
epoch 51 | loss: 0.10039 | val_0_rmse: 0.31742 | val_1_rmse: 0.30756 |  0:01:17s
epoch 52 | loss: 0.10142 | val_0_rmse: 0.31543 | val_1_rmse: 0.30871 |  0:01:19s
epoch 53 | loss: 0.12592 | val_0_rmse: 0.34267 | val_1_rmse: 0.33275 |  0:01:20s
epoch 54 | loss: 0.13196 | val_0_rmse: 0.32963 | val_1_rmse: 0.32359 |  0:01:22s
epoch 55 | loss: 0.11877 | val_0_rmse: 0.35657 | val_1_rmse: 0.3532  |  0:01:23s
epoch 56 | loss: 0.11845 | val_0_rmse: 0.33926 | val_1_rmse: 0.33326 |  0:01:25s
epoch 57 | loss: 0.11779 | val_0_rmse: 0.29797 | val_1_rmse: 0.29369 |  0:01:26s
epoch 58 | loss: 0.11661 | val_0_rmse: 0.36637 | val_1_rmse: 0.3628  |  0:01:28s
epoch 59 | loss: 0.11676 | val_0_rmse: 0.31324 | val_1_rmse: 0.30919 |  0:01:29s
epoch 60 | loss: 0.12118 | val_0_rmse: 0.29801 | val_1_rmse: 0.29394 |  0:01:31s
epoch 61 | loss: 0.11386 | val_0_rmse: 0.35064 | val_1_rmse: 0.34654 |  0:01:32s
epoch 62 | loss: 0.11381 | val_0_rmse: 0.30264 | val_1_rmse: 0.30104 |  0:01:34s
epoch 63 | loss: 0.11488 | val_0_rmse: 0.28572 | val_1_rmse: 0.28486 |  0:01:35s
epoch 64 | loss: 0.11347 | val_0_rmse: 0.34951 | val_1_rmse: 0.34667 |  0:01:37s
epoch 65 | loss: 0.11634 | val_0_rmse: 0.28575 | val_1_rmse: 0.28365 |  0:01:38s
epoch 66 | loss: 0.11233 | val_0_rmse: 0.29604 | val_1_rmse: 0.29444 |  0:01:40s
epoch 67 | loss: 0.10868 | val_0_rmse: 0.35294 | val_1_rmse: 0.35056 |  0:01:41s
epoch 68 | loss: 0.1106  | val_0_rmse: 0.28897 | val_1_rmse: 0.28713 |  0:01:43s
epoch 69 | loss: 0.11032 | val_0_rmse: 0.28643 | val_1_rmse: 0.28424 |  0:01:44s
epoch 70 | loss: 0.10984 | val_0_rmse: 0.3737  | val_1_rmse: 0.37273 |  0:01:46s
epoch 71 | loss: 0.11134 | val_0_rmse: 0.30785 | val_1_rmse: 0.30752 |  0:01:47s
epoch 72 | loss: 0.10602 | val_0_rmse: 0.27263 | val_1_rmse: 0.27129 |  0:01:49s
epoch 73 | loss: 0.11363 | val_0_rmse: 0.31699 | val_1_rmse: 0.31479 |  0:01:50s
epoch 74 | loss: 0.11053 | val_0_rmse: 0.29956 | val_1_rmse: 0.29937 |  0:01:52s
epoch 75 | loss: 0.10549 | val_0_rmse: 0.29456 | val_1_rmse: 0.29542 |  0:01:53s
epoch 76 | loss: 0.10684 | val_0_rmse: 0.35177 | val_1_rmse: 0.35198 |  0:01:55s
epoch 77 | loss: 0.11299 | val_0_rmse: 0.32581 | val_1_rmse: 0.3262  |  0:01:56s
epoch 78 | loss: 0.1092  | val_0_rmse: 0.27504 | val_1_rmse: 0.27197 |  0:01:58s
epoch 79 | loss: 0.10767 | val_0_rmse: 0.36445 | val_1_rmse: 0.3623  |  0:01:59s
epoch 80 | loss: 0.11074 | val_0_rmse: 0.28166 | val_1_rmse: 0.28044 |  0:02:01s
epoch 81 | loss: 0.10758 | val_0_rmse: 0.2872  | val_1_rmse: 0.28815 |  0:02:02s
epoch 82 | loss: 0.10516 | val_0_rmse: 0.34005 | val_1_rmse: 0.33783 |  0:02:04s
epoch 83 | loss: 0.11053 | val_0_rmse: 0.30495 | val_1_rmse: 0.304   |  0:02:05s
epoch 84 | loss: 0.10871 | val_0_rmse: 0.27318 | val_1_rmse: 0.2734  |  0:02:07s
epoch 85 | loss: 0.1056  | val_0_rmse: 0.32164 | val_1_rmse: 0.32231 |  0:02:08s
epoch 86 | loss: 0.1065  | val_0_rmse: 0.29703 | val_1_rmse: 0.29776 |  0:02:10s
epoch 87 | loss: 0.10732 | val_0_rmse: 0.27427 | val_1_rmse: 0.27541 |  0:02:11s
epoch 88 | loss: 0.10699 | val_0_rmse: 0.35893 | val_1_rmse: 0.36345 |  0:02:13s
epoch 89 | loss: 0.1098  | val_0_rmse: 0.28463 | val_1_rmse: 0.28437 |  0:02:14s
epoch 90 | loss: 0.10381 | val_0_rmse: 0.26716 | val_1_rmse: 0.26764 |  0:02:16s
epoch 91 | loss: 0.10437 | val_0_rmse: 0.32649 | val_1_rmse: 0.32818 |  0:02:17s
epoch 92 | loss: 0.10507 | val_0_rmse: 0.27754 | val_1_rmse: 0.27895 |  0:02:19s
epoch 93 | loss: 0.09933 | val_0_rmse: 0.27118 | val_1_rmse: 0.27157 |  0:02:20s
epoch 94 | loss: 0.10135 | val_0_rmse: 0.31928 | val_1_rmse: 0.32098 |  0:02:21s
epoch 95 | loss: 0.10233 | val_0_rmse: 0.30194 | val_1_rmse: 0.30224 |  0:02:23s
epoch 96 | loss: 0.11592 | val_0_rmse: 0.29472 | val_1_rmse: 0.29405 |  0:02:24s
epoch 97 | loss: 0.11078 | val_0_rmse: 0.33307 | val_1_rmse: 0.33014 |  0:02:26s
epoch 98 | loss: 0.10774 | val_0_rmse: 0.29443 | val_1_rmse: 0.29268 |  0:02:27s
epoch 99 | loss: 0.10429 | val_0_rmse: 0.26714 | val_1_rmse: 0.26734 |  0:02:29s
epoch 100| loss: 0.10232 | val_0_rmse: 0.34469 | val_1_rmse: 0.34329 |  0:02:30s
epoch 101| loss: 0.1047  | val_0_rmse: 0.28763 | val_1_rmse: 0.2868  |  0:02:32s
epoch 102| loss: 0.10182 | val_0_rmse: 0.29074 | val_1_rmse: 0.29098 |  0:02:33s
epoch 103| loss: 0.09935 | val_0_rmse: 0.31162 | val_1_rmse: 0.31089 |  0:02:35s
epoch 104| loss: 0.10498 | val_0_rmse: 0.28672 | val_1_rmse: 0.28549 |  0:02:36s
epoch 105| loss: 0.10174 | val_0_rmse: 0.27217 | val_1_rmse: 0.27131 |  0:02:38s
epoch 106| loss: 0.10337 | val_0_rmse: 0.35199 | val_1_rmse: 0.35495 |  0:02:40s
epoch 107| loss: 0.10179 | val_0_rmse: 0.28993 | val_1_rmse: 0.28949 |  0:02:41s
epoch 108| loss: 0.10144 | val_0_rmse: 0.27795 | val_1_rmse: 0.27806 |  0:02:43s
epoch 109| loss: 0.10208 | val_0_rmse: 0.31921 | val_1_rmse: 0.31814 |  0:02:44s
epoch 110| loss: 0.10259 | val_0_rmse: 0.28634 | val_1_rmse: 0.28395 |  0:02:46s
epoch 111| loss: 0.10069 | val_0_rmse: 0.27278 | val_1_rmse: 0.26948 |  0:02:47s
epoch 112| loss: 0.09972 | val_0_rmse: 0.32285 | val_1_rmse: 0.3217  |  0:02:49s
epoch 113| loss: 0.10118 | val_0_rmse: 0.30069 | val_1_rmse: 0.30036 |  0:02:50s
epoch 114| loss: 0.10121 | val_0_rmse: 0.26507 | val_1_rmse: 0.26727 |  0:02:52s
epoch 115| loss: 0.09792 | val_0_rmse: 0.31582 | val_1_rmse: 0.31673 |  0:02:53s
epoch 116| loss: 0.10109 | val_0_rmse: 0.26748 | val_1_rmse: 0.27071 |  0:02:55s
epoch 117| loss: 0.1031  | val_0_rmse: 0.27975 | val_1_rmse: 0.27963 |  0:02:56s
epoch 118| loss: 0.10339 | val_0_rmse: 0.32159 | val_1_rmse: 0.32526 |  0:02:58s
epoch 119| loss: 0.10242 | val_0_rmse: 0.29389 | val_1_rmse: 0.29443 |  0:02:59s
epoch 120| loss: 0.09768 | val_0_rmse: 0.27703 | val_1_rmse: 0.27816 |  0:03:01s
epoch 121| loss: 0.09706 | val_0_rmse: 0.34664 | val_1_rmse: 0.34956 |  0:03:02s
epoch 122| loss: 0.0998  | val_0_rmse: 0.29444 | val_1_rmse: 0.29657 |  0:03:04s
epoch 123| loss: 0.09861 | val_0_rmse: 0.28499 | val_1_rmse: 0.28312 |  0:03:05s
epoch 124| loss: 0.09614 | val_0_rmse: 0.33354 | val_1_rmse: 0.33741 |  0:03:07s
epoch 125| loss: 0.10136 | val_0_rmse: 0.28377 | val_1_rmse: 0.28709 |  0:03:08s
epoch 126| loss: 0.09768 | val_0_rmse: 0.27942 | val_1_rmse: 0.28316 |  0:03:10s
epoch 127| loss: 0.09851 | val_0_rmse: 0.32119 | val_1_rmse: 0.32024 |  0:03:11s
epoch 128| loss: 0.10269 | val_0_rmse: 0.30846 | val_1_rmse: 0.30816 |  0:03:13s
epoch 129| loss: 0.09744 | val_0_rmse: 0.26009 | val_1_rmse: 0.26189 |  0:03:14s
epoch 130| loss: 0.09562 | val_0_rmse: 0.32593 | val_1_rmse: 0.32669 |  0:03:16s
epoch 131| loss: 0.10295 | val_0_rmse: 0.27769 | val_1_rmse: 0.27616 |  0:03:17s
epoch 132| loss: 0.09883 | val_0_rmse: 0.28696 | val_1_rmse: 0.28768 |  0:03:19s
epoch 133| loss: 0.09589 | val_0_rmse: 0.36145 | val_1_rmse: 0.36387 |  0:03:20s
epoch 134| loss: 0.10378 | val_0_rmse: 0.27127 | val_1_rmse: 0.27093 |  0:03:22s
epoch 135| loss: 0.09798 | val_0_rmse: 0.26219 | val_1_rmse: 0.26221 |  0:03:23s
epoch 136| loss: 0.10251 | val_0_rmse: 0.33682 | val_1_rmse: 0.33666 |  0:03:25s
epoch 137| loss: 0.09987 | val_0_rmse: 0.30492 | val_1_rmse: 0.29569 |  0:03:26s
epoch 138| loss: 0.10516 | val_0_rmse: 0.27286 | val_1_rmse: 0.27115 |  0:03:28s
epoch 139| loss: 0.09788 | val_0_rmse: 0.30509 | val_1_rmse: 0.30277 |  0:03:29s
epoch 140| loss: 0.10006 | val_0_rmse: 0.28786 | val_1_rmse: 0.28919 |  0:03:31s
epoch 141| loss: 0.08134 | val_0_rmse: 0.26559 | val_1_rmse: 0.26592 |  0:03:32s
epoch 142| loss: 0.07514 | val_0_rmse: 0.27595 | val_1_rmse: 0.27733 |  0:03:34s
epoch 143| loss: 0.07764 | val_0_rmse: 0.27859 | val_1_rmse: 0.2779  |  0:03:35s
epoch 144| loss: 0.08415 | val_0_rmse: 0.29201 | val_1_rmse: 0.29747 |  0:03:37s
epoch 145| loss: 0.08515 | val_0_rmse: 0.28056 | val_1_rmse: 0.281   |  0:03:38s
epoch 146| loss: 0.08061 | val_0_rmse: 0.27328 | val_1_rmse: 0.27444 |  0:03:40s
epoch 147| loss: 0.08076 | val_0_rmse: 0.25671 | val_1_rmse: 0.25766 |  0:03:41s
epoch 148| loss: 0.07793 | val_0_rmse: 0.26793 | val_1_rmse: 0.26913 |  0:03:43s
epoch 149| loss: 0.0793  | val_0_rmse: 0.27254 | val_1_rmse: 0.27142 |  0:03:44s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.25766
Best weights from best epoch are automatically used!
ended training at: 16:24:44
Feature importance:
[('Area', 0.15311498289228984), ('Baths', 0.08419093554907325), ('Beds', 0.06687572721615476), ('Latitude', 0.27743401064489104), ('Longitude', 0.4046875452969829), ('Month', 0.013696798400608208), ('Year', 0.0)]
Mean squared error is of 7270742940.187838
Mean absolute error:58777.74938098523
MAPE:0.19071211408600688
R2 score:0.6767862679058144
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:24:44
epoch 0  | loss: 47.64103| val_0_rmse: 3.67308 | val_1_rmse: 3.67929 |  0:00:01s
epoch 1  | loss: 0.77044 | val_0_rmse: 0.94393 | val_1_rmse: 0.94767 |  0:00:02s
epoch 2  | loss: 0.25341 | val_0_rmse: 2.08693 | val_1_rmse: 2.08987 |  0:00:04s
epoch 3  | loss: 0.17111 | val_0_rmse: 1.52981 | val_1_rmse: 1.53276 |  0:00:05s
epoch 4  | loss: 0.17235 | val_0_rmse: 0.77262 | val_1_rmse: 0.76648 |  0:00:07s
epoch 5  | loss: 0.13891 | val_0_rmse: 0.79671 | val_1_rmse: 0.79632 |  0:00:08s
epoch 6  | loss: 0.13316 | val_0_rmse: 1.38081 | val_1_rmse: 1.39275 |  0:00:10s
epoch 7  | loss: 0.13147 | val_0_rmse: 1.54255 | val_1_rmse: 1.53723 |  0:00:11s
epoch 8  | loss: 0.11798 | val_0_rmse: 1.49296 | val_1_rmse: 1.48806 |  0:00:13s
epoch 9  | loss: 0.15847 | val_0_rmse: 1.19279 | val_1_rmse: 1.18512 |  0:00:14s
epoch 10 | loss: 0.14054 | val_0_rmse: 1.04076 | val_1_rmse: 1.04588 |  0:00:16s
epoch 11 | loss: 0.13847 | val_0_rmse: 1.25253 | val_1_rmse: 1.25269 |  0:00:17s
epoch 12 | loss: 0.13545 | val_0_rmse: 1.21941 | val_1_rmse: 1.2271  |  0:00:19s
epoch 13 | loss: 0.12915 | val_0_rmse: 0.78458 | val_1_rmse: 0.79291 |  0:00:20s
epoch 14 | loss: 0.13641 | val_0_rmse: 0.96216 | val_1_rmse: 0.9707  |  0:00:22s
epoch 15 | loss: 0.12438 | val_0_rmse: 0.82258 | val_1_rmse: 0.83019 |  0:00:23s
epoch 16 | loss: 0.12847 | val_0_rmse: 0.59454 | val_1_rmse: 0.59957 |  0:00:25s
epoch 17 | loss: 0.12677 | val_0_rmse: 0.8113  | val_1_rmse: 0.81426 |  0:00:26s
epoch 18 | loss: 0.12453 | val_0_rmse: 0.66728 | val_1_rmse: 0.66763 |  0:00:28s
epoch 19 | loss: 0.12076 | val_0_rmse: 0.45963 | val_1_rmse: 0.46121 |  0:00:29s
epoch 20 | loss: 0.1196  | val_0_rmse: 0.63738 | val_1_rmse: 0.63752 |  0:00:31s
epoch 21 | loss: 0.12221 | val_0_rmse: 0.57608 | val_1_rmse: 0.57631 |  0:00:32s
epoch 22 | loss: 0.12357 | val_0_rmse: 0.44164 | val_1_rmse: 0.44585 |  0:00:34s
epoch 23 | loss: 0.13694 | val_0_rmse: 0.44625 | val_1_rmse: 0.44635 |  0:00:35s
epoch 24 | loss: 0.12212 | val_0_rmse: 0.41949 | val_1_rmse: 0.42062 |  0:00:37s
epoch 25 | loss: 0.12621 | val_0_rmse: 0.46139 | val_1_rmse: 0.4691  |  0:00:38s
epoch 26 | loss: 0.10667 | val_0_rmse: 0.41626 | val_1_rmse: 0.42192 |  0:00:40s
epoch 27 | loss: 0.12865 | val_0_rmse: 0.3834  | val_1_rmse: 0.38666 |  0:00:41s
epoch 28 | loss: 0.11897 | val_0_rmse: 0.43201 | val_1_rmse: 0.43039 |  0:00:43s
epoch 29 | loss: 0.11804 | val_0_rmse: 0.38472 | val_1_rmse: 0.38756 |  0:00:44s
epoch 30 | loss: 0.11776 | val_0_rmse: 0.45173 | val_1_rmse: 0.45811 |  0:00:46s
epoch 31 | loss: 0.10593 | val_0_rmse: 0.37067 | val_1_rmse: 0.37386 |  0:00:47s
epoch 32 | loss: 0.11447 | val_0_rmse: 0.38881 | val_1_rmse: 0.3918  |  0:00:49s
epoch 33 | loss: 0.1271  | val_0_rmse: 0.46459 | val_1_rmse: 0.46978 |  0:00:50s
epoch 34 | loss: 0.09831 | val_0_rmse: 0.3779  | val_1_rmse: 0.38275 |  0:00:52s
epoch 35 | loss: 0.09391 | val_0_rmse: 0.3878  | val_1_rmse: 0.39325 |  0:00:53s
epoch 36 | loss: 0.10136 | val_0_rmse: 0.39599 | val_1_rmse: 0.40099 |  0:00:55s
epoch 37 | loss: 0.09815 | val_0_rmse: 0.3943  | val_1_rmse: 0.40068 |  0:00:56s
epoch 38 | loss: 0.10598 | val_0_rmse: 0.42882 | val_1_rmse: 0.43517 |  0:00:58s
epoch 39 | loss: 0.11601 | val_0_rmse: 0.38167 | val_1_rmse: 0.38428 |  0:00:59s
epoch 40 | loss: 0.1127  | val_0_rmse: 0.37666 | val_1_rmse: 0.38186 |  0:01:01s
epoch 41 | loss: 0.08973 | val_0_rmse: 0.3826  | val_1_rmse: 0.38647 |  0:01:02s
epoch 42 | loss: 0.08772 | val_0_rmse: 0.41563 | val_1_rmse: 0.41945 |  0:01:04s
epoch 43 | loss: 0.08791 | val_0_rmse: 0.38493 | val_1_rmse: 0.3909  |  0:01:05s
epoch 44 | loss: 0.08457 | val_0_rmse: 0.35374 | val_1_rmse: 0.3585  |  0:01:07s
epoch 45 | loss: 0.08805 | val_0_rmse: 0.32795 | val_1_rmse: 0.33222 |  0:01:08s
epoch 46 | loss: 0.11789 | val_0_rmse: 0.34073 | val_1_rmse: 0.34349 |  0:01:10s
epoch 47 | loss: 0.1113  | val_0_rmse: 0.36277 | val_1_rmse: 0.3675  |  0:01:11s
epoch 48 | loss: 0.10774 | val_0_rmse: 0.31474 | val_1_rmse: 0.31907 |  0:01:13s
epoch 49 | loss: 0.10672 | val_0_rmse: 0.34372 | val_1_rmse: 0.34602 |  0:01:14s
epoch 50 | loss: 0.10642 | val_0_rmse: 0.34339 | val_1_rmse: 0.3494  |  0:01:16s
epoch 51 | loss: 0.10463 | val_0_rmse: 0.32634 | val_1_rmse: 0.33438 |  0:01:17s
epoch 52 | loss: 0.10483 | val_0_rmse: 0.35207 | val_1_rmse: 0.35633 |  0:01:19s
epoch 53 | loss: 0.10631 | val_0_rmse: 0.31944 | val_1_rmse: 0.32807 |  0:01:20s
epoch 54 | loss: 0.10276 | val_0_rmse: 0.2839  | val_1_rmse: 0.2912  |  0:01:22s
epoch 55 | loss: 0.07972 | val_0_rmse: 0.2911  | val_1_rmse: 0.29761 |  0:01:23s
epoch 56 | loss: 0.07981 | val_0_rmse: 0.2745  | val_1_rmse: 0.28337 |  0:01:25s
epoch 57 | loss: 0.08068 | val_0_rmse: 0.27285 | val_1_rmse: 0.2806  |  0:01:26s
epoch 58 | loss: 0.07923 | val_0_rmse: 0.29853 | val_1_rmse: 0.30591 |  0:01:28s
epoch 59 | loss: 0.08586 | val_0_rmse: 0.31718 | val_1_rmse: 0.3209  |  0:01:29s
epoch 60 | loss: 0.08236 | val_0_rmse: 0.28756 | val_1_rmse: 0.2931  |  0:01:31s
epoch 61 | loss: 0.09202 | val_0_rmse: 0.28555 | val_1_rmse: 0.29437 |  0:01:32s
epoch 62 | loss: 0.09469 | val_0_rmse: 0.31919 | val_1_rmse: 0.32382 |  0:01:34s
epoch 63 | loss: 0.10669 | val_0_rmse: 0.30027 | val_1_rmse: 0.30916 |  0:01:35s
epoch 64 | loss: 0.09608 | val_0_rmse: 0.37836 | val_1_rmse: 0.38798 |  0:01:37s
epoch 65 | loss: 0.11144 | val_0_rmse: 0.2815  | val_1_rmse: 0.28916 |  0:01:38s
epoch 66 | loss: 0.07734 | val_0_rmse: 0.28781 | val_1_rmse: 0.29263 |  0:01:40s
epoch 67 | loss: 0.07611 | val_0_rmse: 0.28211 | val_1_rmse: 0.28983 |  0:01:41s
epoch 68 | loss: 0.10839 | val_0_rmse: 0.2864  | val_1_rmse: 0.29482 |  0:01:43s
epoch 69 | loss: 0.10329 | val_0_rmse: 0.26621 | val_1_rmse: 0.27428 |  0:01:44s
epoch 70 | loss: 0.10158 | val_0_rmse: 0.32113 | val_1_rmse: 0.32685 |  0:01:46s
epoch 71 | loss: 0.09975 | val_0_rmse: 0.26971 | val_1_rmse: 0.27887 |  0:01:47s
epoch 72 | loss: 0.09771 | val_0_rmse: 0.26198 | val_1_rmse: 0.27174 |  0:01:49s
epoch 73 | loss: 0.09861 | val_0_rmse: 0.31943 | val_1_rmse: 0.32452 |  0:01:50s
epoch 74 | loss: 0.09658 | val_0_rmse: 0.26133 | val_1_rmse: 0.27066 |  0:01:52s
epoch 75 | loss: 0.09502 | val_0_rmse: 0.24715 | val_1_rmse: 0.25712 |  0:01:53s
epoch 76 | loss: 0.09692 | val_0_rmse: 0.34513 | val_1_rmse: 0.34962 |  0:01:55s
epoch 77 | loss: 0.09822 | val_0_rmse: 0.24877 | val_1_rmse: 0.25832 |  0:01:56s
epoch 78 | loss: 0.0916  | val_0_rmse: 0.25999 | val_1_rmse: 0.26758 |  0:01:58s
epoch 79 | loss: 0.07549 | val_0_rmse: 0.25555 | val_1_rmse: 0.26512 |  0:01:59s
epoch 80 | loss: 0.07521 | val_0_rmse: 0.26219 | val_1_rmse: 0.27243 |  0:02:01s
epoch 81 | loss: 0.074   | val_0_rmse: 0.28133 | val_1_rmse: 0.28791 |  0:02:02s
epoch 82 | loss: 0.07989 | val_0_rmse: 0.30613 | val_1_rmse: 0.31202 |  0:02:04s
epoch 83 | loss: 0.10435 | val_0_rmse: 0.25181 | val_1_rmse: 0.26123 |  0:02:05s
epoch 84 | loss: 0.09057 | val_0_rmse: 0.24959 | val_1_rmse: 0.25745 |  0:02:07s
epoch 85 | loss: 0.08119 | val_0_rmse: 0.2676  | val_1_rmse: 0.27339 |  0:02:08s
epoch 86 | loss: 0.0744  | val_0_rmse: 0.27625 | val_1_rmse: 0.28587 |  0:02:10s
epoch 87 | loss: 0.10266 | val_0_rmse: 0.27941 | val_1_rmse: 0.28836 |  0:02:11s
epoch 88 | loss: 0.10013 | val_0_rmse: 0.34106 | val_1_rmse: 0.34561 |  0:02:13s
epoch 89 | loss: 0.09715 | val_0_rmse: 0.26601 | val_1_rmse: 0.2753  |  0:02:14s
epoch 90 | loss: 0.09773 | val_0_rmse: 0.24144 | val_1_rmse: 0.25249 |  0:02:16s
epoch 91 | loss: 0.09545 | val_0_rmse: 0.3496  | val_1_rmse: 0.35289 |  0:02:17s
epoch 92 | loss: 0.09731 | val_0_rmse: 0.25382 | val_1_rmse: 0.26385 |  0:02:19s
epoch 93 | loss: 0.09416 | val_0_rmse: 0.25213 | val_1_rmse: 0.26259 |  0:02:20s
epoch 94 | loss: 0.09105 | val_0_rmse: 0.3436  | val_1_rmse: 0.34734 |  0:02:22s
epoch 95 | loss: 0.09555 | val_0_rmse: 0.26877 | val_1_rmse: 0.2797  |  0:02:23s
epoch 96 | loss: 0.09161 | val_0_rmse: 0.24612 | val_1_rmse: 0.2559  |  0:02:25s
epoch 97 | loss: 0.09067 | val_0_rmse: 0.3296  | val_1_rmse: 0.3344  |  0:02:26s
epoch 98 | loss: 0.09894 | val_0_rmse: 0.26187 | val_1_rmse: 0.27058 |  0:02:28s
epoch 99 | loss: 0.09425 | val_0_rmse: 0.24715 | val_1_rmse: 0.25749 |  0:02:29s
epoch 100| loss: 0.09425 | val_0_rmse: 0.34482 | val_1_rmse: 0.34988 |  0:02:31s
epoch 101| loss: 0.09519 | val_0_rmse: 0.27194 | val_1_rmse: 0.28258 |  0:02:32s
epoch 102| loss: 0.09314 | val_0_rmse: 0.24734 | val_1_rmse: 0.2555  |  0:02:34s
epoch 103| loss: 0.0922  | val_0_rmse: 0.34144 | val_1_rmse: 0.34698 |  0:02:35s
epoch 104| loss: 0.09371 | val_0_rmse: 0.25466 | val_1_rmse: 0.26541 |  0:02:37s
epoch 105| loss: 0.09113 | val_0_rmse: 0.24484 | val_1_rmse: 0.25718 |  0:02:38s
epoch 106| loss: 0.09366 | val_0_rmse: 0.28804 | val_1_rmse: 0.29411 |  0:02:40s
epoch 107| loss: 0.0757  | val_0_rmse: 0.2598  | val_1_rmse: 0.26661 |  0:02:41s
epoch 108| loss: 0.07554 | val_0_rmse: 0.25522 | val_1_rmse: 0.26412 |  0:02:43s
epoch 109| loss: 0.06964 | val_0_rmse: 0.24128 | val_1_rmse: 0.25035 |  0:02:44s
epoch 110| loss: 0.06891 | val_0_rmse: 0.26075 | val_1_rmse: 0.27091 |  0:02:46s
epoch 111| loss: 0.09281 | val_0_rmse: 0.31552 | val_1_rmse: 0.32118 |  0:02:47s
epoch 112| loss: 0.09852 | val_0_rmse: 0.30145 | val_1_rmse: 0.31062 |  0:02:49s
epoch 113| loss: 0.07243 | val_0_rmse: 0.29545 | val_1_rmse: 0.30195 |  0:02:50s
epoch 114| loss: 0.07475 | val_0_rmse: 0.26478 | val_1_rmse: 0.27304 |  0:02:52s
epoch 115| loss: 0.07889 | val_0_rmse: 0.25792 | val_1_rmse: 0.26774 |  0:02:53s
epoch 116| loss: 0.098   | val_0_rmse: 0.29175 | val_1_rmse: 0.29832 |  0:02:55s
epoch 117| loss: 0.07648 | val_0_rmse: 0.29196 | val_1_rmse: 0.29675 |  0:02:56s
epoch 118| loss: 0.09909 | val_0_rmse: 0.27402 | val_1_rmse: 0.28428 |  0:02:58s
epoch 119| loss: 0.09492 | val_0_rmse: 0.24663 | val_1_rmse: 0.25562 |  0:02:59s
epoch 120| loss: 0.07864 | val_0_rmse: 0.24386 | val_1_rmse: 0.25207 |  0:03:01s
epoch 121| loss: 0.07499 | val_0_rmse: 0.29208 | val_1_rmse: 0.29846 |  0:03:02s
epoch 122| loss: 0.09244 | val_0_rmse: 0.24533 | val_1_rmse: 0.25548 |  0:03:04s
epoch 123| loss: 0.06788 | val_0_rmse: 0.2606  | val_1_rmse: 0.26663 |  0:03:05s
epoch 124| loss: 0.07443 | val_0_rmse: 0.31162 | val_1_rmse: 0.31946 |  0:03:07s
epoch 125| loss: 0.09464 | val_0_rmse: 0.29606 | val_1_rmse: 0.30384 |  0:03:08s
epoch 126| loss: 0.07367 | val_0_rmse: 0.25355 | val_1_rmse: 0.26556 |  0:03:10s
epoch 127| loss: 0.06841 | val_0_rmse: 0.25298 | val_1_rmse: 0.26161 |  0:03:11s
epoch 128| loss: 0.07342 | val_0_rmse: 0.37967 | val_1_rmse: 0.38176 |  0:03:13s
epoch 129| loss: 0.08053 | val_0_rmse: 0.25241 | val_1_rmse: 0.2623  |  0:03:14s
epoch 130| loss: 0.07444 | val_0_rmse: 0.24534 | val_1_rmse: 0.25581 |  0:03:16s
epoch 131| loss: 0.06732 | val_0_rmse: 0.25333 | val_1_rmse: 0.26242 |  0:03:17s
epoch 132| loss: 0.07874 | val_0_rmse: 0.28118 | val_1_rmse: 0.2909  |  0:03:19s
epoch 133| loss: 0.07905 | val_0_rmse: 0.28385 | val_1_rmse: 0.29775 |  0:03:20s
epoch 134| loss: 0.08088 | val_0_rmse: 0.27503 | val_1_rmse: 0.28033 |  0:03:22s
epoch 135| loss: 0.0884  | val_0_rmse: 0.29777 | val_1_rmse: 0.30192 |  0:03:23s
epoch 136| loss: 0.08203 | val_0_rmse: 0.2781  | val_1_rmse: 0.28581 |  0:03:25s
epoch 137| loss: 0.08114 | val_0_rmse: 0.27892 | val_1_rmse: 0.28508 |  0:03:26s
epoch 138| loss: 0.0804  | val_0_rmse: 0.26702 | val_1_rmse: 0.27348 |  0:03:28s
epoch 139| loss: 0.11888 | val_0_rmse: 0.26771 | val_1_rmse: 0.27607 |  0:03:29s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.25035
Best weights from best epoch are automatically used!
ended training at: 16:28:14
Feature importance:
[('Area', 0.1015806170202087), ('Baths', 0.17390813346198764), ('Beds', 0.004245550165852071), ('Latitude', 0.3017201982868711), ('Longitude', 0.06818734391311342), ('Month', 0.11226436354732161), ('Year', 0.2380937936046455)]
Mean squared error is of 7406248242.154609
Mean absolute error:58160.87250611204
MAPE:0.17574948610478913
R2 score:0.6674278883195247
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:28:14
epoch 0  | loss: 47.6099 | val_0_rmse: 26.26412| val_1_rmse: 26.41002|  0:00:01s
epoch 1  | loss: 0.55749 | val_0_rmse: 1.74426 | val_1_rmse: 1.73658 |  0:00:02s
epoch 2  | loss: 0.32325 | val_0_rmse: 0.68272 | val_1_rmse: 0.68173 |  0:00:04s
epoch 3  | loss: 0.22329 | val_0_rmse: 0.84    | val_1_rmse: 0.8386  |  0:00:05s
epoch 4  | loss: 0.18308 | val_0_rmse: 0.57465 | val_1_rmse: 0.57803 |  0:00:07s
epoch 5  | loss: 0.14813 | val_0_rmse: 0.49594 | val_1_rmse: 0.49787 |  0:00:08s
epoch 6  | loss: 0.18405 | val_0_rmse: 0.4997  | val_1_rmse: 0.50182 |  0:00:10s
epoch 7  | loss: 0.16908 | val_0_rmse: 0.50513 | val_1_rmse: 0.51093 |  0:00:11s
epoch 8  | loss: 0.17317 | val_0_rmse: 0.508   | val_1_rmse: 0.51252 |  0:00:13s
epoch 9  | loss: 0.16507 | val_0_rmse: 0.5435  | val_1_rmse: 0.54736 |  0:00:14s
epoch 10 | loss: 0.16012 | val_0_rmse: 0.49741 | val_1_rmse: 0.50768 |  0:00:16s
epoch 11 | loss: 0.15754 | val_0_rmse: 0.61336 | val_1_rmse: 0.61637 |  0:00:17s
epoch 12 | loss: 0.15607 | val_0_rmse: 0.56636 | val_1_rmse: 0.56994 |  0:00:19s
epoch 13 | loss: 0.15037 | val_0_rmse: 0.44053 | val_1_rmse: 0.44919 |  0:00:20s
epoch 14 | loss: 0.15021 | val_0_rmse: 0.54348 | val_1_rmse: 0.54699 |  0:00:22s
epoch 15 | loss: 0.14805 | val_0_rmse: 0.55386 | val_1_rmse: 0.55869 |  0:00:23s
epoch 16 | loss: 0.14704 | val_0_rmse: 0.45174 | val_1_rmse: 0.46288 |  0:00:25s
epoch 17 | loss: 0.15216 | val_0_rmse: 0.52602 | val_1_rmse: 0.52916 |  0:00:26s
epoch 18 | loss: 0.15615 | val_0_rmse: 0.49628 | val_1_rmse: 0.50442 |  0:00:28s
epoch 19 | loss: 0.13218 | val_0_rmse: 0.44525 | val_1_rmse: 0.44981 |  0:00:29s
epoch 20 | loss: 0.11878 | val_0_rmse: 0.46175 | val_1_rmse: 0.46792 |  0:00:31s
epoch 21 | loss: 0.12463 | val_0_rmse: 0.44406 | val_1_rmse: 0.44831 |  0:00:32s
epoch 22 | loss: 0.12465 | val_0_rmse: 0.41834 | val_1_rmse: 0.42249 |  0:00:34s
epoch 23 | loss: 0.1309  | val_0_rmse: 0.40307 | val_1_rmse: 0.41193 |  0:00:35s
epoch 24 | loss: 0.16329 | val_0_rmse: 0.5193  | val_1_rmse: 0.52809 |  0:00:37s
epoch 25 | loss: 0.16088 | val_0_rmse: 0.41151 | val_1_rmse: 0.42021 |  0:00:38s
epoch 26 | loss: 0.13149 | val_0_rmse: 0.40272 | val_1_rmse: 0.41161 |  0:00:40s
epoch 27 | loss: 0.12782 | val_0_rmse: 0.42217 | val_1_rmse: 0.42648 |  0:00:41s
epoch 28 | loss: 0.11661 | val_0_rmse: 0.44309 | val_1_rmse: 0.45219 |  0:00:43s
epoch 29 | loss: 0.11832 | val_0_rmse: 0.39108 | val_1_rmse: 0.39568 |  0:00:44s
epoch 30 | loss: 0.11709 | val_0_rmse: 0.37935 | val_1_rmse: 0.3858  |  0:00:46s
epoch 31 | loss: 0.11495 | val_0_rmse: 0.43031 | val_1_rmse: 0.43781 |  0:00:47s
epoch 32 | loss: 0.11609 | val_0_rmse: 0.39104 | val_1_rmse: 0.39592 |  0:00:49s
epoch 33 | loss: 0.11759 | val_0_rmse: 0.36932 | val_1_rmse: 0.37548 |  0:00:50s
epoch 34 | loss: 0.10804 | val_0_rmse: 0.46282 | val_1_rmse: 0.4721  |  0:00:52s
epoch 35 | loss: 0.11126 | val_0_rmse: 0.36526 | val_1_rmse: 0.37333 |  0:00:53s
epoch 36 | loss: 0.11172 | val_0_rmse: 0.3613  | val_1_rmse: 0.36822 |  0:00:55s
epoch 37 | loss: 0.10934 | val_0_rmse: 0.50269 | val_1_rmse: 0.51265 |  0:00:56s
epoch 38 | loss: 0.11351 | val_0_rmse: 0.39497 | val_1_rmse: 0.39894 |  0:00:58s
epoch 39 | loss: 0.10881 | val_0_rmse: 0.36428 | val_1_rmse: 0.37264 |  0:00:59s
epoch 40 | loss: 0.11128 | val_0_rmse: 0.43332 | val_1_rmse: 0.44082 |  0:01:01s
epoch 41 | loss: 0.11025 | val_0_rmse: 0.34846 | val_1_rmse: 0.35499 |  0:01:02s
epoch 42 | loss: 0.10548 | val_0_rmse: 0.34961 | val_1_rmse: 0.35586 |  0:01:04s
epoch 43 | loss: 0.10484 | val_0_rmse: 0.4476  | val_1_rmse: 0.45657 |  0:01:05s
epoch 44 | loss: 0.10667 | val_0_rmse: 0.34744 | val_1_rmse: 0.35194 |  0:01:07s
epoch 45 | loss: 0.10419 | val_0_rmse: 0.32008 | val_1_rmse: 0.3283  |  0:01:08s
epoch 46 | loss: 0.10292 | val_0_rmse: 0.38759 | val_1_rmse: 0.39619 |  0:01:10s
epoch 47 | loss: 0.10647 | val_0_rmse: 0.32132 | val_1_rmse: 0.3302  |  0:01:11s
epoch 48 | loss: 0.10918 | val_0_rmse: 0.32493 | val_1_rmse: 0.33113 |  0:01:13s
epoch 49 | loss: 0.10688 | val_0_rmse: 0.37981 | val_1_rmse: 0.38639 |  0:01:14s
epoch 50 | loss: 0.10712 | val_0_rmse: 0.3232  | val_1_rmse: 0.33208 |  0:01:16s
epoch 51 | loss: 0.10452 | val_0_rmse: 0.30416 | val_1_rmse: 0.3106  |  0:01:17s
epoch 52 | loss: 0.10007 | val_0_rmse: 0.372   | val_1_rmse: 0.38109 |  0:01:19s
epoch 53 | loss: 0.10189 | val_0_rmse: 0.32562 | val_1_rmse: 0.32964 |  0:01:20s
epoch 54 | loss: 0.10784 | val_0_rmse: 0.29818 | val_1_rmse: 0.30723 |  0:01:22s
epoch 55 | loss: 0.10652 | val_0_rmse: 0.40194 | val_1_rmse: 0.41383 |  0:01:23s
epoch 56 | loss: 0.10383 | val_0_rmse: 0.33427 | val_1_rmse: 0.34183 |  0:01:25s
epoch 57 | loss: 0.10248 | val_0_rmse: 0.31899 | val_1_rmse: 0.3241  |  0:01:26s
epoch 58 | loss: 0.09775 | val_0_rmse: 0.29657 | val_1_rmse: 0.30901 |  0:01:28s
epoch 59 | loss: 0.09742 | val_0_rmse: 0.29731 | val_1_rmse: 0.30422 |  0:01:29s
epoch 60 | loss: 0.09963 | val_0_rmse: 0.26817 | val_1_rmse: 0.27745 |  0:01:31s
epoch 61 | loss: 0.09636 | val_0_rmse: 0.29896 | val_1_rmse: 0.31184 |  0:01:32s
epoch 62 | loss: 0.09776 | val_0_rmse: 0.29001 | val_1_rmse: 0.29807 |  0:01:34s
epoch 63 | loss: 0.09896 | val_0_rmse: 0.26551 | val_1_rmse: 0.27775 |  0:01:35s
epoch 64 | loss: 0.10021 | val_0_rmse: 0.31215 | val_1_rmse: 0.32747 |  0:01:37s
epoch 65 | loss: 0.09975 | val_0_rmse: 0.29648 | val_1_rmse: 0.30855 |  0:01:38s
epoch 66 | loss: 0.09895 | val_0_rmse: 0.27356 | val_1_rmse: 0.28452 |  0:01:40s
epoch 67 | loss: 0.0955  | val_0_rmse: 0.30194 | val_1_rmse: 0.31548 |  0:01:41s
epoch 68 | loss: 0.09716 | val_0_rmse: 0.28383 | val_1_rmse: 0.29407 |  0:01:43s
epoch 69 | loss: 0.09822 | val_0_rmse: 0.27415 | val_1_rmse: 0.28915 |  0:01:44s
epoch 70 | loss: 0.09819 | val_0_rmse: 0.30152 | val_1_rmse: 0.31611 |  0:01:46s
epoch 71 | loss: 0.09623 | val_0_rmse: 0.29943 | val_1_rmse: 0.30958 |  0:01:47s
epoch 72 | loss: 0.10067 | val_0_rmse: 0.27232 | val_1_rmse: 0.2858  |  0:01:49s
epoch 73 | loss: 0.09803 | val_0_rmse: 0.29017 | val_1_rmse: 0.30604 |  0:01:50s
epoch 74 | loss: 0.0974  | val_0_rmse: 0.26816 | val_1_rmse: 0.27701 |  0:01:52s
epoch 75 | loss: 0.09449 | val_0_rmse: 0.25557 | val_1_rmse: 0.26941 |  0:01:53s
epoch 76 | loss: 0.09445 | val_0_rmse: 0.30596 | val_1_rmse: 0.31959 |  0:01:55s
epoch 77 | loss: 0.097   | val_0_rmse: 0.28339 | val_1_rmse: 0.29144 |  0:01:56s
epoch 78 | loss: 0.12136 | val_0_rmse: 0.31963 | val_1_rmse: 0.33545 |  0:01:58s
epoch 79 | loss: 0.0884  | val_0_rmse: 0.33709 | val_1_rmse: 0.34652 |  0:01:59s
epoch 80 | loss: 0.07471 | val_0_rmse: 0.26348 | val_1_rmse: 0.2792  |  0:02:01s
epoch 81 | loss: 0.07133 | val_0_rmse: 0.24615 | val_1_rmse: 0.26091 |  0:02:02s
epoch 82 | loss: 0.0703  | val_0_rmse: 0.25138 | val_1_rmse: 0.26783 |  0:02:04s
epoch 83 | loss: 0.07326 | val_0_rmse: 0.34821 | val_1_rmse: 0.35553 |  0:02:05s
epoch 84 | loss: 0.12123 | val_0_rmse: 0.30124 | val_1_rmse: 0.31535 |  0:02:07s
epoch 85 | loss: 0.124   | val_0_rmse: 0.30213 | val_1_rmse: 0.31726 |  0:02:08s
epoch 86 | loss: 0.07184 | val_0_rmse: 0.25452 | val_1_rmse: 0.26649 |  0:02:10s
epoch 87 | loss: 0.09639 | val_0_rmse: 0.29968 | val_1_rmse: 0.31051 |  0:02:11s
epoch 88 | loss: 0.10018 | val_0_rmse: 0.25301 | val_1_rmse: 0.27069 |  0:02:13s
epoch 89 | loss: 0.07245 | val_0_rmse: 0.25742 | val_1_rmse: 0.2713  |  0:02:14s
epoch 90 | loss: 0.07362 | val_0_rmse: 0.25066 | val_1_rmse: 0.2648  |  0:02:16s
epoch 91 | loss: 0.07348 | val_0_rmse: 0.28496 | val_1_rmse: 0.29774 |  0:02:17s
epoch 92 | loss: 0.10439 | val_0_rmse: 0.25739 | val_1_rmse: 0.27174 |  0:02:19s
epoch 93 | loss: 0.09643 | val_0_rmse: 0.33303 | val_1_rmse: 0.34692 |  0:02:20s
epoch 94 | loss: 0.10065 | val_0_rmse: 0.27504 | val_1_rmse: 0.28478 |  0:02:22s
epoch 95 | loss: 0.10244 | val_0_rmse: 0.31147 | val_1_rmse: 0.32359 |  0:02:23s
epoch 96 | loss: 0.09934 | val_0_rmse: 0.2929  | val_1_rmse: 0.3061  |  0:02:25s
epoch 97 | loss: 0.09382 | val_0_rmse: 0.25607 | val_1_rmse: 0.26932 |  0:02:26s
epoch 98 | loss: 0.0899  | val_0_rmse: 0.2984  | val_1_rmse: 0.31306 |  0:02:27s
epoch 99 | loss: 0.09128 | val_0_rmse: 0.27683 | val_1_rmse: 0.28731 |  0:02:29s
epoch 100| loss: 0.09661 | val_0_rmse: 0.27137 | val_1_rmse: 0.28373 |  0:02:30s
epoch 101| loss: 0.09735 | val_0_rmse: 0.30277 | val_1_rmse: 0.31941 |  0:02:32s
epoch 102| loss: 0.09391 | val_0_rmse: 0.31536 | val_1_rmse: 0.32672 |  0:02:33s
epoch 103| loss: 0.09364 | val_0_rmse: 0.25282 | val_1_rmse: 0.26617 |  0:02:35s
epoch 104| loss: 0.09265 | val_0_rmse: 0.26472 | val_1_rmse: 0.28024 |  0:02:36s
epoch 105| loss: 0.09281 | val_0_rmse: 0.25035 | val_1_rmse: 0.26431 |  0:02:38s
epoch 106| loss: 0.10196 | val_0_rmse: 0.28499 | val_1_rmse: 0.29608 |  0:02:39s
epoch 107| loss: 0.09613 | val_0_rmse: 0.38188 | val_1_rmse: 0.39287 |  0:02:41s
epoch 108| loss: 0.09781 | val_0_rmse: 0.30133 | val_1_rmse: 0.31123 |  0:02:42s
epoch 109| loss: 0.09485 | val_0_rmse: 0.29026 | val_1_rmse: 0.30078 |  0:02:44s
epoch 110| loss: 0.09398 | val_0_rmse: 0.26923 | val_1_rmse: 0.28553 |  0:02:45s
epoch 111| loss: 0.09885 | val_0_rmse: 0.29653 | val_1_rmse: 0.30414 |  0:02:47s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.26091
Best weights from best epoch are automatically used!
ended training at: 16:31:02
Feature importance:
[('Area', 0.036550535411531256), ('Baths', 0.42930043283262165), ('Beds', 0.20927404108750783), ('Latitude', 0.09502385615466358), ('Longitude', 0.0), ('Month', 0.050019469656041364), ('Year', 0.17983166485763433)]
Mean squared error is of 7613643470.204264
Mean absolute error:61463.06733195707
MAPE:0.19104191605021253
R2 score:0.651484423854972
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:31:02
epoch 0  | loss: 47.13498| val_0_rmse: 2.50091 | val_1_rmse: 2.53428 |  0:00:01s
epoch 1  | loss: 0.59434 | val_0_rmse: 0.63078 | val_1_rmse: 0.63165 |  0:00:02s
epoch 2  | loss: 0.2482  | val_0_rmse: 0.84353 | val_1_rmse: 0.83475 |  0:00:04s
epoch 3  | loss: 0.19117 | val_0_rmse: 0.48026 | val_1_rmse: 0.47979 |  0:00:05s
epoch 4  | loss: 0.15185 | val_0_rmse: 0.53832 | val_1_rmse: 0.5381  |  0:00:07s
epoch 5  | loss: 0.13987 | val_0_rmse: 0.4332  | val_1_rmse: 0.43041 |  0:00:08s
epoch 6  | loss: 0.12561 | val_0_rmse: 0.47673 | val_1_rmse: 0.47831 |  0:00:10s
epoch 7  | loss: 0.14745 | val_0_rmse: 0.47451 | val_1_rmse: 0.47585 |  0:00:11s
epoch 8  | loss: 0.15474 | val_0_rmse: 0.41536 | val_1_rmse: 0.41806 |  0:00:13s
epoch 9  | loss: 0.13357 | val_0_rmse: 0.40917 | val_1_rmse: 0.41051 |  0:00:14s
epoch 10 | loss: 0.1171  | val_0_rmse: 0.3996  | val_1_rmse: 0.40156 |  0:00:16s
epoch 11 | loss: 0.12749 | val_0_rmse: 0.4214  | val_1_rmse: 0.4244  |  0:00:17s
epoch 12 | loss: 0.12073 | val_0_rmse: 0.52034 | val_1_rmse: 0.52339 |  0:00:19s
epoch 13 | loss: 0.13116 | val_0_rmse: 0.39563 | val_1_rmse: 0.3952  |  0:00:20s
epoch 14 | loss: 0.12809 | val_0_rmse: 0.39428 | val_1_rmse: 0.39339 |  0:00:22s
epoch 15 | loss: 0.12259 | val_0_rmse: 0.4861  | val_1_rmse: 0.4854  |  0:00:23s
epoch 16 | loss: 0.12421 | val_0_rmse: 0.39641 | val_1_rmse: 0.39423 |  0:00:25s
epoch 17 | loss: 0.11961 | val_0_rmse: 0.40042 | val_1_rmse: 0.39671 |  0:00:26s
epoch 18 | loss: 0.12186 | val_0_rmse: 0.4329  | val_1_rmse: 0.43147 |  0:00:28s
epoch 19 | loss: 0.1211  | val_0_rmse: 0.42097 | val_1_rmse: 0.41765 |  0:00:29s
epoch 20 | loss: 0.1163  | val_0_rmse: 0.39962 | val_1_rmse: 0.39648 |  0:00:31s
epoch 21 | loss: 0.11497 | val_0_rmse: 0.43244 | val_1_rmse: 0.43202 |  0:00:32s
epoch 22 | loss: 0.11453 | val_0_rmse: 0.40568 | val_1_rmse: 0.40395 |  0:00:34s
epoch 23 | loss: 0.11309 | val_0_rmse: 0.40635 | val_1_rmse: 0.40409 |  0:00:35s
epoch 24 | loss: 0.11288 | val_0_rmse: 0.45143 | val_1_rmse: 0.45148 |  0:00:37s
epoch 25 | loss: 0.11343 | val_0_rmse: 0.40058 | val_1_rmse: 0.39918 |  0:00:38s
epoch 26 | loss: 0.11329 | val_0_rmse: 0.37887 | val_1_rmse: 0.37764 |  0:00:40s
epoch 27 | loss: 0.11099 | val_0_rmse: 0.46281 | val_1_rmse: 0.463   |  0:00:41s
epoch 28 | loss: 0.11272 | val_0_rmse: 0.39271 | val_1_rmse: 0.38975 |  0:00:43s
epoch 29 | loss: 0.11165 | val_0_rmse: 0.37918 | val_1_rmse: 0.37662 |  0:00:44s
epoch 30 | loss: 0.11568 | val_0_rmse: 0.44224 | val_1_rmse: 0.44141 |  0:00:46s
epoch 31 | loss: 0.11649 | val_0_rmse: 0.39391 | val_1_rmse: 0.38979 |  0:00:47s
epoch 32 | loss: 0.1087  | val_0_rmse: 0.37939 | val_1_rmse: 0.3755  |  0:00:49s
epoch 33 | loss: 0.10875 | val_0_rmse: 0.39313 | val_1_rmse: 0.39009 |  0:00:50s
epoch 34 | loss: 0.10853 | val_0_rmse: 0.40213 | val_1_rmse: 0.39873 |  0:00:52s
epoch 35 | loss: 0.1084  | val_0_rmse: 0.38314 | val_1_rmse: 0.38048 |  0:00:53s
epoch 36 | loss: 0.10653 | val_0_rmse: 0.37856 | val_1_rmse: 0.37592 |  0:00:55s
epoch 37 | loss: 0.10816 | val_0_rmse: 0.41982 | val_1_rmse: 0.41523 |  0:00:56s
epoch 38 | loss: 0.10853 | val_0_rmse: 0.37728 | val_1_rmse: 0.37475 |  0:00:57s
epoch 39 | loss: 0.10264 | val_0_rmse: 0.35448 | val_1_rmse: 0.35556 |  0:00:59s
epoch 40 | loss: 0.10746 | val_0_rmse: 0.3989  | val_1_rmse: 0.39607 |  0:01:00s
epoch 41 | loss: 0.10414 | val_0_rmse: 0.37768 | val_1_rmse: 0.37496 |  0:01:02s
epoch 42 | loss: 0.10067 | val_0_rmse: 0.34675 | val_1_rmse: 0.3456  |  0:01:03s
epoch 43 | loss: 0.10504 | val_0_rmse: 0.39158 | val_1_rmse: 0.38739 |  0:01:05s
epoch 44 | loss: 0.10845 | val_0_rmse: 0.35336 | val_1_rmse: 0.34854 |  0:01:06s
epoch 45 | loss: 0.10301 | val_0_rmse: 0.34039 | val_1_rmse: 0.33923 |  0:01:08s
epoch 46 | loss: 0.10061 | val_0_rmse: 0.37119 | val_1_rmse: 0.36653 |  0:01:09s
epoch 47 | loss: 0.1057  | val_0_rmse: 0.35958 | val_1_rmse: 0.35452 |  0:01:11s
epoch 48 | loss: 0.1014  | val_0_rmse: 0.32504 | val_1_rmse: 0.32309 |  0:01:12s
epoch 49 | loss: 0.10313 | val_0_rmse: 0.35239 | val_1_rmse: 0.34903 |  0:01:14s
epoch 50 | loss: 0.10247 | val_0_rmse: 0.33289 | val_1_rmse: 0.32852 |  0:01:15s
epoch 51 | loss: 0.09902 | val_0_rmse: 0.34524 | val_1_rmse: 0.3455  |  0:01:17s
epoch 52 | loss: 0.10126 | val_0_rmse: 0.30974 | val_1_rmse: 0.30554 |  0:01:18s
epoch 53 | loss: 0.10016 | val_0_rmse: 0.29836 | val_1_rmse: 0.29151 |  0:01:20s
epoch 54 | loss: 0.10472 | val_0_rmse: 0.31476 | val_1_rmse: 0.31276 |  0:01:21s
epoch 55 | loss: 0.10189 | val_0_rmse: 0.3248  | val_1_rmse: 0.31865 |  0:01:23s
epoch 56 | loss: 0.10195 | val_0_rmse: 0.28833 | val_1_rmse: 0.28319 |  0:01:24s
epoch 57 | loss: 0.09696 | val_0_rmse: 0.34043 | val_1_rmse: 0.34155 |  0:01:26s
epoch 58 | loss: 0.10462 | val_0_rmse: 0.28137 | val_1_rmse: 0.27767 |  0:01:27s
epoch 59 | loss: 0.09853 | val_0_rmse: 0.26967 | val_1_rmse: 0.26829 |  0:01:29s
epoch 60 | loss: 0.10188 | val_0_rmse: 0.35319 | val_1_rmse: 0.35672 |  0:01:30s
epoch 61 | loss: 0.1043  | val_0_rmse: 0.2811  | val_1_rmse: 0.27728 |  0:01:32s
epoch 62 | loss: 0.10079 | val_0_rmse: 0.27733 | val_1_rmse: 0.27528 |  0:01:33s
epoch 63 | loss: 0.09987 | val_0_rmse: 0.33721 | val_1_rmse: 0.33779 |  0:01:35s
epoch 64 | loss: 0.10101 | val_0_rmse: 0.28519 | val_1_rmse: 0.28466 |  0:01:36s
epoch 65 | loss: 0.10056 | val_0_rmse: 0.26694 | val_1_rmse: 0.2659  |  0:01:38s
epoch 66 | loss: 0.09625 | val_0_rmse: 0.35284 | val_1_rmse: 0.35448 |  0:01:39s
epoch 67 | loss: 0.09658 | val_0_rmse: 0.30711 | val_1_rmse: 0.30428 |  0:01:41s
epoch 68 | loss: 0.12031 | val_0_rmse: 0.28391 | val_1_rmse: 0.28607 |  0:01:42s
epoch 69 | loss: 0.10541 | val_0_rmse: 0.3513  | val_1_rmse: 0.35335 |  0:01:44s
epoch 70 | loss: 0.10408 | val_0_rmse: 0.27584 | val_1_rmse: 0.27419 |  0:01:45s
epoch 71 | loss: 0.1005  | val_0_rmse: 0.30493 | val_1_rmse: 0.3029  |  0:01:47s
epoch 72 | loss: 0.09843 | val_0_rmse: 0.32912 | val_1_rmse: 0.33052 |  0:01:48s
epoch 73 | loss: 0.09941 | val_0_rmse: 0.29683 | val_1_rmse: 0.29402 |  0:01:50s
epoch 74 | loss: 0.09968 | val_0_rmse: 0.26851 | val_1_rmse: 0.2687  |  0:01:51s
epoch 75 | loss: 0.09542 | val_0_rmse: 0.31078 | val_1_rmse: 0.31041 |  0:01:53s
epoch 76 | loss: 0.10293 | val_0_rmse: 0.30093 | val_1_rmse: 0.29985 |  0:01:54s
epoch 77 | loss: 0.11174 | val_0_rmse: 0.28847 | val_1_rmse: 0.28891 |  0:01:56s
epoch 78 | loss: 0.10791 | val_0_rmse: 0.3505  | val_1_rmse: 0.35253 |  0:01:57s
epoch 79 | loss: 0.10839 | val_0_rmse: 0.29927 | val_1_rmse: 0.30062 |  0:01:58s
epoch 80 | loss: 0.10514 | val_0_rmse: 0.27056 | val_1_rmse: 0.26916 |  0:02:00s
epoch 81 | loss: 0.10162 | val_0_rmse: 0.33029 | val_1_rmse: 0.32996 |  0:02:01s
epoch 82 | loss: 0.1031  | val_0_rmse: 0.27452 | val_1_rmse: 0.277   |  0:02:03s
epoch 83 | loss: 0.10224 | val_0_rmse: 0.27656 | val_1_rmse: 0.27443 |  0:02:04s
epoch 84 | loss: 0.10204 | val_0_rmse: 0.32644 | val_1_rmse: 0.32768 |  0:02:06s
epoch 85 | loss: 0.10752 | val_0_rmse: 0.29359 | val_1_rmse: 0.29238 |  0:02:07s
epoch 86 | loss: 0.10257 | val_0_rmse: 0.27247 | val_1_rmse: 0.2723  |  0:02:09s
epoch 87 | loss: 0.10119 | val_0_rmse: 0.303   | val_1_rmse: 0.30338 |  0:02:10s
epoch 88 | loss: 0.09876 | val_0_rmse: 0.29092 | val_1_rmse: 0.2913  |  0:02:12s
epoch 89 | loss: 0.09809 | val_0_rmse: 0.28686 | val_1_rmse: 0.28495 |  0:02:13s
epoch 90 | loss: 0.09948 | val_0_rmse: 0.33155 | val_1_rmse: 0.33187 |  0:02:15s
epoch 91 | loss: 0.10128 | val_0_rmse: 0.27997 | val_1_rmse: 0.27913 |  0:02:16s
epoch 92 | loss: 0.09759 | val_0_rmse: 0.25957 | val_1_rmse: 0.25729 |  0:02:18s
epoch 93 | loss: 0.12103 | val_0_rmse: 0.35842 | val_1_rmse: 0.35588 |  0:02:19s
epoch 94 | loss: 0.13321 | val_0_rmse: 0.2985  | val_1_rmse: 0.29326 |  0:02:21s
epoch 95 | loss: 0.11264 | val_0_rmse: 0.32298 | val_1_rmse: 0.32253 |  0:02:22s
epoch 96 | loss: 0.11578 | val_0_rmse: 0.35521 | val_1_rmse: 0.35472 |  0:02:24s
epoch 97 | loss: 0.10915 | val_0_rmse: 0.30009 | val_1_rmse: 0.29862 |  0:02:25s
epoch 98 | loss: 0.10523 | val_0_rmse: 0.29097 | val_1_rmse: 0.28982 |  0:02:27s
epoch 99 | loss: 0.10477 | val_0_rmse: 0.31851 | val_1_rmse: 0.31708 |  0:02:28s
epoch 100| loss: 0.10521 | val_0_rmse: 0.28476 | val_1_rmse: 0.28265 |  0:02:30s
epoch 101| loss: 0.10322 | val_0_rmse: 0.28202 | val_1_rmse: 0.2805  |  0:02:31s
epoch 102| loss: 0.10129 | val_0_rmse: 0.33443 | val_1_rmse: 0.33612 |  0:02:33s
epoch 103| loss: 0.10796 | val_0_rmse: 0.2932  | val_1_rmse: 0.29007 |  0:02:34s
epoch 104| loss: 0.1045  | val_0_rmse: 0.26979 | val_1_rmse: 0.26852 |  0:02:36s
epoch 105| loss: 0.10357 | val_0_rmse: 0.31467 | val_1_rmse: 0.31419 |  0:02:37s
epoch 106| loss: 0.10281 | val_0_rmse: 0.28097 | val_1_rmse: 0.27929 |  0:02:39s
epoch 107| loss: 0.10151 | val_0_rmse: 0.27572 | val_1_rmse: 0.27237 |  0:02:40s
epoch 108| loss: 0.09921 | val_0_rmse: 0.33483 | val_1_rmse: 0.33502 |  0:02:42s
epoch 109| loss: 0.10196 | val_0_rmse: 0.28548 | val_1_rmse: 0.28276 |  0:02:43s
epoch 110| loss: 0.09904 | val_0_rmse: 0.27249 | val_1_rmse: 0.26961 |  0:02:45s
epoch 111| loss: 0.10062 | val_0_rmse: 0.32828 | val_1_rmse: 0.32948 |  0:02:46s
epoch 112| loss: 0.10338 | val_0_rmse: 0.29183 | val_1_rmse: 0.28883 |  0:02:47s
epoch 113| loss: 0.10053 | val_0_rmse: 0.26749 | val_1_rmse: 0.26499 |  0:02:49s
epoch 114| loss: 0.10089 | val_0_rmse: 0.31907 | val_1_rmse: 0.31944 |  0:02:50s
epoch 115| loss: 0.10067 | val_0_rmse: 0.27328 | val_1_rmse: 0.27033 |  0:02:52s
epoch 116| loss: 0.099   | val_0_rmse: 0.26733 | val_1_rmse: 0.26455 |  0:02:53s
epoch 117| loss: 0.0997  | val_0_rmse: 0.32121 | val_1_rmse: 0.32116 |  0:02:55s
epoch 118| loss: 0.1042  | val_0_rmse: 0.29698 | val_1_rmse: 0.29529 |  0:02:56s
epoch 119| loss: 0.10082 | val_0_rmse: 0.26951 | val_1_rmse: 0.26856 |  0:02:58s
epoch 120| loss: 0.09637 | val_0_rmse: 0.34789 | val_1_rmse: 0.34662 |  0:02:59s
epoch 121| loss: 0.09077 | val_0_rmse: 0.27778 | val_1_rmse: 0.27577 |  0:03:01s
epoch 122| loss: 0.08041 | val_0_rmse: 0.26438 | val_1_rmse: 0.26232 |  0:03:02s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.25729
Best weights from best epoch are automatically used!
ended training at: 16:34:06
Feature importance:
[('Area', 0.20306631021603766), ('Baths', 0.08581988820841126), ('Beds', 0.31170467929820217), ('Latitude', 0.23317623206994967), ('Longitude', 0.1456692255933119), ('Month', 0.00964782998160092), ('Year', 0.010915834632486425)]
Mean squared error is of 7699359510.556068
Mean absolute error:61843.23819773576
MAPE:0.20323523057899065
R2 score:0.6532649816088338
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:36:34
epoch 0  | loss: 10.5967 | val_0_rmse: 0.71106 | val_1_rmse: 0.71639 |  0:00:05s
epoch 1  | loss: 0.24056 | val_0_rmse: 0.8058  | val_1_rmse: 0.81104 |  0:00:11s
epoch 2  | loss: 0.22696 | val_0_rmse: 0.66458 | val_1_rmse: 0.66836 |  0:00:17s
epoch 3  | loss: 0.22692 | val_0_rmse: 0.64511 | val_1_rmse: 0.64812 |  0:00:22s
epoch 4  | loss: 0.22599 | val_0_rmse: 0.5517  | val_1_rmse: 0.55638 |  0:00:28s
epoch 5  | loss: 0.26361 | val_0_rmse: 0.56505 | val_1_rmse: 0.56855 |  0:00:33s
epoch 6  | loss: 0.30836 | val_0_rmse: 0.5915  | val_1_rmse: 0.59479 |  0:00:39s
epoch 7  | loss: 0.29091 | val_0_rmse: 0.67362 | val_1_rmse: 0.67517 |  0:00:45s
epoch 8  | loss: 0.22846 | val_0_rmse: 0.53225 | val_1_rmse: 0.53479 |  0:00:50s
epoch 9  | loss: 0.24199 | val_0_rmse: 0.44946 | val_1_rmse: 0.45365 |  0:00:56s
epoch 10 | loss: 0.22039 | val_0_rmse: 0.48064 | val_1_rmse: 0.4836  |  0:01:02s
epoch 11 | loss: 0.19418 | val_0_rmse: 0.427   | val_1_rmse: 0.43114 |  0:01:07s
epoch 12 | loss: 0.22857 | val_0_rmse: 0.46978 | val_1_rmse: 0.46931 |  0:01:13s
epoch 13 | loss: 0.1986  | val_0_rmse: 0.41514 | val_1_rmse: 0.42036 |  0:01:18s
epoch 14 | loss: 0.19454 | val_0_rmse: 0.45227 | val_1_rmse: 0.45599 |  0:01:24s
epoch 15 | loss: 0.20079 | val_0_rmse: 0.42104 | val_1_rmse: 0.42597 |  0:01:30s
epoch 16 | loss: 0.19591 | val_0_rmse: 0.47367 | val_1_rmse: 0.47901 |  0:01:35s
epoch 17 | loss: 0.23208 | val_0_rmse: 0.41045 | val_1_rmse: 0.41471 |  0:01:41s
epoch 18 | loss: 0.18602 | val_0_rmse: 0.44221 | val_1_rmse: 0.44611 |  0:01:47s
epoch 19 | loss: 0.19516 | val_0_rmse: 0.41841 | val_1_rmse: 0.42221 |  0:01:52s
epoch 20 | loss: 0.17983 | val_0_rmse: 0.41436 | val_1_rmse: 0.41886 |  0:01:58s
epoch 21 | loss: 0.19294 | val_0_rmse: 0.41731 | val_1_rmse: 0.42106 |  0:02:04s
epoch 22 | loss: 0.19932 | val_0_rmse: 0.46476 | val_1_rmse: 0.4663  |  0:02:09s
epoch 23 | loss: 0.17688 | val_0_rmse: 0.56882 | val_1_rmse: 0.56974 |  0:02:15s
epoch 24 | loss: 0.21277 | val_0_rmse: 0.40662 | val_1_rmse: 0.41273 |  0:02:20s
epoch 25 | loss: 0.17818 | val_0_rmse: 0.42687 | val_1_rmse: 0.43272 |  0:02:26s
epoch 26 | loss: 0.18961 | val_0_rmse: 0.41473 | val_1_rmse: 0.41972 |  0:02:32s
epoch 27 | loss: 0.17383 | val_0_rmse: 0.42184 | val_1_rmse: 0.4273  |  0:02:37s
epoch 28 | loss: 0.21871 | val_0_rmse: 0.43943 | val_1_rmse: 0.44361 |  0:02:43s
epoch 29 | loss: 0.1703  | val_0_rmse: 0.40985 | val_1_rmse: 0.41495 |  0:02:49s
epoch 30 | loss: 0.17544 | val_0_rmse: 0.4443  | val_1_rmse: 0.44893 |  0:02:54s
epoch 31 | loss: 0.18864 | val_0_rmse: 0.39886 | val_1_rmse: 0.40357 |  0:03:00s
epoch 32 | loss: 0.18577 | val_0_rmse: 0.4012  | val_1_rmse: 0.40718 |  0:03:06s
epoch 33 | loss: 0.18326 | val_0_rmse: 0.4251  | val_1_rmse: 0.43045 |  0:03:11s
epoch 34 | loss: 0.18499 | val_0_rmse: 0.45259 | val_1_rmse: 0.45738 |  0:03:17s
epoch 35 | loss: 0.1825  | val_0_rmse: 0.42737 | val_1_rmse: 0.43283 |  0:03:23s
epoch 36 | loss: 0.1828  | val_0_rmse: 0.44614 | val_1_rmse: 0.45089 |  0:03:28s
epoch 37 | loss: 0.18343 | val_0_rmse: 0.41838 | val_1_rmse: 0.42294 |  0:03:34s
epoch 38 | loss: 0.17982 | val_0_rmse: 0.41057 | val_1_rmse: 0.41614 |  0:03:39s
epoch 39 | loss: 0.16974 | val_0_rmse: 0.40478 | val_1_rmse: 0.40996 |  0:03:45s
epoch 40 | loss: 0.18021 | val_0_rmse: 0.42944 | val_1_rmse: 0.43233 |  0:03:51s
epoch 41 | loss: 0.17518 | val_0_rmse: 0.42816 | val_1_rmse: 0.43248 |  0:03:56s
epoch 42 | loss: 0.18306 | val_0_rmse: 0.42366 | val_1_rmse: 0.42901 |  0:04:02s
epoch 43 | loss: 0.19194 | val_0_rmse: 0.4221  | val_1_rmse: 0.42671 |  0:04:08s
epoch 44 | loss: 0.1753  | val_0_rmse: 0.45195 | val_1_rmse: 0.45648 |  0:04:13s
epoch 45 | loss: 0.1721  | val_0_rmse: 0.41646 | val_1_rmse: 0.42189 |  0:04:19s
epoch 46 | loss: 0.18238 | val_0_rmse: 0.39318 | val_1_rmse: 0.39831 |  0:04:25s
epoch 47 | loss: 0.16484 | val_0_rmse: 0.42799 | val_1_rmse: 0.43323 |  0:04:30s
epoch 48 | loss: 0.18152 | val_0_rmse: 0.41333 | val_1_rmse: 0.42024 |  0:04:36s
epoch 49 | loss: 0.15944 | val_0_rmse: 0.42677 | val_1_rmse: 0.43275 |  0:04:42s
epoch 50 | loss: 0.17551 | val_0_rmse: 0.40529 | val_1_rmse: 0.41218 |  0:04:47s
epoch 51 | loss: 0.18328 | val_0_rmse: 0.39155 | val_1_rmse: 0.39865 |  0:04:53s
epoch 52 | loss: 0.21742 | val_0_rmse: 0.47595 | val_1_rmse: 0.47956 |  0:04:58s
epoch 53 | loss: 0.18254 | val_0_rmse: 0.40053 | val_1_rmse: 0.40732 |  0:05:04s
epoch 54 | loss: 0.1749  | val_0_rmse: 0.39242 | val_1_rmse: 0.39942 |  0:05:10s
epoch 55 | loss: 0.17258 | val_0_rmse: 0.39051 | val_1_rmse: 0.39661 |  0:05:15s
epoch 56 | loss: 0.15475 | val_0_rmse: 0.39233 | val_1_rmse: 0.39886 |  0:05:21s
epoch 57 | loss: 0.17465 | val_0_rmse: 0.40215 | val_1_rmse: 0.40711 |  0:05:27s
epoch 58 | loss: 0.166   | val_0_rmse: 0.39686 | val_1_rmse: 0.40348 |  0:05:32s
epoch 59 | loss: 0.155   | val_0_rmse: 0.38927 | val_1_rmse: 0.39653 |  0:05:38s
epoch 60 | loss: 0.17764 | val_0_rmse: 0.45147 | val_1_rmse: 0.456   |  0:05:44s
epoch 61 | loss: 0.15283 | val_0_rmse: 0.39482 | val_1_rmse: 0.40058 |  0:05:49s
epoch 62 | loss: 0.1581  | val_0_rmse: 0.38213 | val_1_rmse: 0.38902 |  0:05:55s
epoch 63 | loss: 0.16243 | val_0_rmse: 0.42721 | val_1_rmse: 0.43146 |  0:06:00s
epoch 64 | loss: 0.1641  | val_0_rmse: 0.38531 | val_1_rmse: 0.39232 |  0:06:06s
epoch 65 | loss: 0.19145 | val_0_rmse: 0.39281 | val_1_rmse: 0.39963 |  0:06:12s
epoch 66 | loss: 0.15864 | val_0_rmse: 0.44542 | val_1_rmse: 0.45084 |  0:06:17s
epoch 67 | loss: 0.17381 | val_0_rmse: 0.39618 | val_1_rmse: 0.40186 |  0:06:23s
epoch 68 | loss: 0.1674  | val_0_rmse: 0.43894 | val_1_rmse: 0.44426 |  0:06:29s
epoch 69 | loss: 0.21523 | val_0_rmse: 0.43816 | val_1_rmse: 0.44501 |  0:06:34s
epoch 70 | loss: 0.15853 | val_0_rmse: 0.38984 | val_1_rmse: 0.39615 |  0:06:40s
epoch 71 | loss: 0.17168 | val_0_rmse: 0.39279 | val_1_rmse: 0.39844 |  0:06:45s
epoch 72 | loss: 0.15947 | val_0_rmse: 0.38185 | val_1_rmse: 0.38804 |  0:06:51s
epoch 73 | loss: 0.15231 | val_0_rmse: 0.39921 | val_1_rmse: 0.40558 |  0:06:57s
epoch 74 | loss: 0.17264 | val_0_rmse: 0.39014 | val_1_rmse: 0.39672 |  0:07:02s
epoch 75 | loss: 0.16574 | val_0_rmse: 0.38561 | val_1_rmse: 0.39269 |  0:07:08s
epoch 76 | loss: 0.16457 | val_0_rmse: 0.40828 | val_1_rmse: 0.41395 |  0:07:13s
epoch 77 | loss: 0.16415 | val_0_rmse: 0.38703 | val_1_rmse: 0.39507 |  0:07:19s
epoch 78 | loss: 0.16203 | val_0_rmse: 0.41275 | val_1_rmse: 0.41853 |  0:07:25s
epoch 79 | loss: 0.15222 | val_0_rmse: 0.41339 | val_1_rmse: 0.41975 |  0:07:30s
epoch 80 | loss: 0.15972 | val_0_rmse: 0.3728  | val_1_rmse: 0.37825 |  0:07:36s
epoch 81 | loss: 0.14774 | val_0_rmse: 0.37109 | val_1_rmse: 0.37938 |  0:07:42s
epoch 82 | loss: 0.1513  | val_0_rmse: 0.41486 | val_1_rmse: 0.42022 |  0:07:47s
epoch 83 | loss: 0.1493  | val_0_rmse: 0.38046 | val_1_rmse: 0.38617 |  0:07:53s
epoch 84 | loss: 0.16048 | val_0_rmse: 0.41221 | val_1_rmse: 0.41794 |  0:07:59s
epoch 85 | loss: 0.15747 | val_0_rmse: 0.38053 | val_1_rmse: 0.38743 |  0:08:04s
epoch 86 | loss: 0.17516 | val_0_rmse: 0.40362 | val_1_rmse: 0.41063 |  0:08:10s
epoch 87 | loss: 0.18322 | val_0_rmse: 0.39345 | val_1_rmse: 0.40062 |  0:08:15s
epoch 88 | loss: 0.15761 | val_0_rmse: 0.40497 | val_1_rmse: 0.41192 |  0:08:21s
epoch 89 | loss: 0.14639 | val_0_rmse: 0.37598 | val_1_rmse: 0.38378 |  0:08:27s
epoch 90 | loss: 0.1547  | val_0_rmse: 0.48059 | val_1_rmse: 0.48515 |  0:08:32s
epoch 91 | loss: 0.16642 | val_0_rmse: 0.46897 | val_1_rmse: 0.47389 |  0:08:38s
epoch 92 | loss: 0.16956 | val_0_rmse: 0.44714 | val_1_rmse: 0.45492 |  0:08:44s
epoch 93 | loss: 0.15016 | val_0_rmse: 0.41204 | val_1_rmse: 0.41817 |  0:08:49s
epoch 94 | loss: 0.14636 | val_0_rmse: 0.40217 | val_1_rmse: 0.40766 |  0:08:55s
epoch 95 | loss: 0.14681 | val_0_rmse: 0.38978 | val_1_rmse: 0.39792 |  0:09:01s
epoch 96 | loss: 0.15203 | val_0_rmse: 0.38231 | val_1_rmse: 0.38945 |  0:09:06s
epoch 97 | loss: 0.14936 | val_0_rmse: 0.39718 | val_1_rmse: 0.40397 |  0:09:12s
epoch 98 | loss: 0.1486  | val_0_rmse: 0.38943 | val_1_rmse: 0.39566 |  0:09:17s
epoch 99 | loss: 0.16529 | val_0_rmse: 0.38342 | val_1_rmse: 0.39089 |  0:09:23s
epoch 100| loss: 0.14403 | val_0_rmse: 0.38725 | val_1_rmse: 0.39456 |  0:09:29s
epoch 101| loss: 0.14682 | val_0_rmse: 0.38788 | val_1_rmse: 0.39459 |  0:09:34s
epoch 102| loss: 0.16009 | val_0_rmse: 0.43717 | val_1_rmse: 0.44383 |  0:09:40s
epoch 103| loss: 0.15741 | val_0_rmse: 0.37664 | val_1_rmse: 0.38492 |  0:09:45s
epoch 104| loss: 0.14649 | val_0_rmse: 0.39111 | val_1_rmse: 0.39826 |  0:09:51s
epoch 105| loss: 0.14997 | val_0_rmse: 0.37904 | val_1_rmse: 0.38694 |  0:09:57s
epoch 106| loss: 0.16167 | val_0_rmse: 0.41072 | val_1_rmse: 0.41705 |  0:10:02s
epoch 107| loss: 0.15074 | val_0_rmse: 0.37275 | val_1_rmse: 0.37971 |  0:10:08s
epoch 108| loss: 0.14563 | val_0_rmse: 0.40916 | val_1_rmse: 0.41671 |  0:10:14s
epoch 109| loss: 0.14737 | val_0_rmse: 0.40537 | val_1_rmse: 0.41273 |  0:10:19s
epoch 110| loss: 0.14837 | val_0_rmse: 0.40063 | val_1_rmse: 0.40746 |  0:10:25s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.37825
Best weights from best epoch are automatically used!
ended training at: 16:47:01
Feature importance:
[('Area', 0.16687531461170102), ('Baths', 0.1414649650179995), ('Beds', 0.0), ('Latitude', 0.25098887425986105), ('Longitude', 0.22030057818485985), ('Month', 0.2203702679255786), ('Year', 0.0)]
Mean squared error is of 2462663285.8932652
Mean absolute error:35001.140643418556
MAPE:0.317147376482555
R2 score:0.6380772224354313
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:47:02
epoch 0  | loss: 10.31316| val_0_rmse: 0.56455 | val_1_rmse: 0.55539 |  0:00:05s
epoch 1  | loss: 0.26277 | val_0_rmse: 0.58061 | val_1_rmse: 0.57507 |  0:00:11s
epoch 2  | loss: 0.25545 | val_0_rmse: 0.56878 | val_1_rmse: 0.56289 |  0:00:16s
epoch 3  | loss: 0.2194  | val_0_rmse: 0.6284  | val_1_rmse: 0.62411 |  0:00:22s
epoch 4  | loss: 0.21451 | val_0_rmse: 0.58898 | val_1_rmse: 0.58267 |  0:00:28s
epoch 5  | loss: 0.19855 | val_0_rmse: 0.53312 | val_1_rmse: 0.52514 |  0:00:33s
epoch 6  | loss: 0.19464 | val_0_rmse: 0.6152  | val_1_rmse: 0.60964 |  0:00:39s
epoch 7  | loss: 0.20757 | val_0_rmse: 0.75248 | val_1_rmse: 0.74899 |  0:00:45s
epoch 8  | loss: 0.19863 | val_0_rmse: 0.5778  | val_1_rmse: 0.57181 |  0:00:50s
epoch 9  | loss: 0.17164 | val_0_rmse: 0.60389 | val_1_rmse: 0.59894 |  0:00:56s
epoch 10 | loss: 0.17744 | val_0_rmse: 0.54336 | val_1_rmse: 0.53836 |  0:01:02s
epoch 11 | loss: 0.19293 | val_0_rmse: 0.41064 | val_1_rmse: 0.40349 |  0:01:07s
epoch 12 | loss: 0.18485 | val_0_rmse: 0.39965 | val_1_rmse: 0.3938  |  0:01:13s
epoch 13 | loss: 0.18697 | val_0_rmse: 0.48019 | val_1_rmse: 0.47557 |  0:01:19s
epoch 14 | loss: 0.18798 | val_0_rmse: 0.42933 | val_1_rmse: 0.42262 |  0:01:24s
epoch 15 | loss: 0.18797 | val_0_rmse: 0.44212 | val_1_rmse: 0.4361  |  0:01:30s
epoch 16 | loss: 0.18622 | val_0_rmse: 0.40571 | val_1_rmse: 0.39933 |  0:01:36s
epoch 17 | loss: 0.18582 | val_0_rmse: 0.39296 | val_1_rmse: 0.38572 |  0:01:41s
epoch 18 | loss: 0.18372 | val_0_rmse: 0.41309 | val_1_rmse: 0.40778 |  0:01:47s
epoch 19 | loss: 0.18222 | val_0_rmse: 0.41215 | val_1_rmse: 0.40638 |  0:01:53s
epoch 20 | loss: 0.17997 | val_0_rmse: 0.42205 | val_1_rmse: 0.41396 |  0:01:58s
epoch 21 | loss: 0.17792 | val_0_rmse: 0.47296 | val_1_rmse: 0.46512 |  0:02:04s
epoch 22 | loss: 0.17094 | val_0_rmse: 0.39843 | val_1_rmse: 0.39202 |  0:02:10s
epoch 23 | loss: 0.16867 | val_0_rmse: 0.42374 | val_1_rmse: 0.4177  |  0:02:15s
epoch 24 | loss: 0.15839 | val_0_rmse: 0.47463 | val_1_rmse: 0.46979 |  0:02:21s
epoch 25 | loss: 0.17548 | val_0_rmse: 0.38311 | val_1_rmse: 0.37727 |  0:02:27s
epoch 26 | loss: 0.16125 | val_0_rmse: 0.41021 | val_1_rmse: 0.4055  |  0:02:32s
epoch 27 | loss: 0.15617 | val_0_rmse: 0.37824 | val_1_rmse: 0.37251 |  0:02:38s
epoch 28 | loss: 0.18096 | val_0_rmse: 0.39319 | val_1_rmse: 0.38645 |  0:02:44s
epoch 29 | loss: 0.18389 | val_0_rmse: 0.46322 | val_1_rmse: 0.45739 |  0:02:49s
epoch 30 | loss: 0.18078 | val_0_rmse: 0.40871 | val_1_rmse: 0.40154 |  0:02:55s
epoch 31 | loss: 0.17444 | val_0_rmse: 0.39753 | val_1_rmse: 0.39124 |  0:03:01s
epoch 32 | loss: 0.16372 | val_0_rmse: 0.4438  | val_1_rmse: 0.43835 |  0:03:06s
epoch 33 | loss: 0.17346 | val_0_rmse: 0.40673 | val_1_rmse: 0.40128 |  0:03:12s
epoch 34 | loss: 0.18442 | val_0_rmse: 0.47473 | val_1_rmse: 0.46979 |  0:03:17s
epoch 35 | loss: 0.18107 | val_0_rmse: 0.4324  | val_1_rmse: 0.42647 |  0:03:23s
epoch 36 | loss: 0.16414 | val_0_rmse: 0.51789 | val_1_rmse: 0.51305 |  0:03:29s
epoch 37 | loss: 0.1729  | val_0_rmse: 0.41988 | val_1_rmse: 0.41235 |  0:03:34s
epoch 38 | loss: 0.16256 | val_0_rmse: 0.42203 | val_1_rmse: 0.41665 |  0:03:40s
epoch 39 | loss: 0.16606 | val_0_rmse: 0.47505 | val_1_rmse: 0.46939 |  0:03:46s
epoch 40 | loss: 0.2309  | val_0_rmse: 0.39915 | val_1_rmse: 0.39256 |  0:03:51s
epoch 41 | loss: 0.18565 | val_0_rmse: 0.40046 | val_1_rmse: 0.39324 |  0:03:57s
epoch 42 | loss: 0.15486 | val_0_rmse: 0.40309 | val_1_rmse: 0.39734 |  0:04:02s
epoch 43 | loss: 0.16245 | val_0_rmse: 0.39138 | val_1_rmse: 0.38522 |  0:04:08s
epoch 44 | loss: 0.17062 | val_0_rmse: 0.51828 | val_1_rmse: 0.51375 |  0:04:14s
epoch 45 | loss: 0.17379 | val_0_rmse: 0.41177 | val_1_rmse: 0.40539 |  0:04:19s
epoch 46 | loss: 0.18537 | val_0_rmse: 0.39773 | val_1_rmse: 0.39102 |  0:04:25s
epoch 47 | loss: 0.15888 | val_0_rmse: 0.41237 | val_1_rmse: 0.40564 |  0:04:31s
epoch 48 | loss: 0.17473 | val_0_rmse: 0.42952 | val_1_rmse: 0.42478 |  0:04:36s
epoch 49 | loss: 0.16902 | val_0_rmse: 0.43571 | val_1_rmse: 0.43064 |  0:04:42s
epoch 50 | loss: 0.19089 | val_0_rmse: 0.45444 | val_1_rmse: 0.44983 |  0:04:48s
epoch 51 | loss: 0.22827 | val_0_rmse: 0.51867 | val_1_rmse: 0.51412 |  0:04:53s
epoch 52 | loss: 0.17865 | val_0_rmse: 0.52385 | val_1_rmse: 0.51816 |  0:04:59s
epoch 53 | loss: 0.17743 | val_0_rmse: 0.41567 | val_1_rmse: 0.41089 |  0:05:05s
epoch 54 | loss: 0.17765 | val_0_rmse: 0.40124 | val_1_rmse: 0.39515 |  0:05:10s
epoch 55 | loss: 0.15985 | val_0_rmse: 0.38573 | val_1_rmse: 0.38201 |  0:05:16s
epoch 56 | loss: 0.16373 | val_0_rmse: 0.39235 | val_1_rmse: 0.38689 |  0:05:22s
epoch 57 | loss: 0.16742 | val_0_rmse: 0.44434 | val_1_rmse: 0.44055 |  0:05:27s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.37251
Best weights from best epoch are automatically used!
ended training at: 16:52:32
Feature importance:
[('Area', 0.29175528901104303), ('Baths', 0.2358686197637317), ('Beds', 0.06929794855606726), ('Latitude', 0.1467485640613405), ('Longitude', 0.21077165371227016), ('Month', 0.04555787526665981), ('Year', 4.9628887581445265e-08)]
Mean squared error is of 2495082676.227135
Mean absolute error:35394.3137538834
MAPE:0.30830622473522656
R2 score:0.6318519466004142
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:52:33
epoch 0  | loss: 10.07337| val_0_rmse: 0.63524 | val_1_rmse: 0.63395 |  0:00:05s
epoch 1  | loss: 0.25394 | val_0_rmse: 0.54707 | val_1_rmse: 0.54435 |  0:00:11s
epoch 2  | loss: 0.23167 | val_0_rmse: 0.52556 | val_1_rmse: 0.52402 |  0:00:17s
epoch 3  | loss: 0.21388 | val_0_rmse: 0.54684 | val_1_rmse: 0.546   |  0:00:22s
epoch 4  | loss: 0.21593 | val_0_rmse: 0.58537 | val_1_rmse: 0.58486 |  0:00:28s
epoch 5  | loss: 0.22754 | val_0_rmse: 0.51752 | val_1_rmse: 0.51724 |  0:00:33s
epoch 6  | loss: 0.20477 | val_0_rmse: 0.64758 | val_1_rmse: 0.64783 |  0:00:39s
epoch 7  | loss: 0.195   | val_0_rmse: 0.62804 | val_1_rmse: 0.62957 |  0:00:45s
epoch 8  | loss: 0.2057  | val_0_rmse: 0.53447 | val_1_rmse: 0.53612 |  0:00:50s
epoch 9  | loss: 0.20089 | val_0_rmse: 0.65363 | val_1_rmse: 0.65381 |  0:00:56s
epoch 10 | loss: 0.19632 | val_0_rmse: 0.48153 | val_1_rmse: 0.47961 |  0:01:02s
epoch 11 | loss: 0.19233 | val_0_rmse: 0.4224  | val_1_rmse: 0.41977 |  0:01:07s
epoch 12 | loss: 0.1905  | val_0_rmse: 0.40004 | val_1_rmse: 0.3968  |  0:01:13s
epoch 13 | loss: 0.18853 | val_0_rmse: 0.47172 | val_1_rmse: 0.4704  |  0:01:19s
epoch 14 | loss: 0.18731 | val_0_rmse: 0.47614 | val_1_rmse: 0.47205 |  0:01:24s
epoch 15 | loss: 0.17519 | val_0_rmse: 0.39343 | val_1_rmse: 0.39058 |  0:01:30s
epoch 16 | loss: 0.16752 | val_0_rmse: 0.4031  | val_1_rmse: 0.40031 |  0:01:36s
epoch 17 | loss: 0.16811 | val_0_rmse: 0.40203 | val_1_rmse: 0.39924 |  0:01:41s
epoch 18 | loss: 0.16681 | val_0_rmse: 0.38759 | val_1_rmse: 0.38557 |  0:01:47s
epoch 19 | loss: 0.18181 | val_0_rmse: 0.41236 | val_1_rmse: 0.41066 |  0:01:53s
epoch 20 | loss: 0.16644 | val_0_rmse: 0.56705 | val_1_rmse: 0.56445 |  0:01:58s
epoch 21 | loss: 0.16059 | val_0_rmse: 0.38707 | val_1_rmse: 0.3847  |  0:02:04s
epoch 22 | loss: 0.18251 | val_0_rmse: 0.41065 | val_1_rmse: 0.40836 |  0:02:10s
epoch 23 | loss: 0.18245 | val_0_rmse: 0.41477 | val_1_rmse: 0.41083 |  0:02:15s
epoch 24 | loss: 0.17315 | val_0_rmse: 0.3817  | val_1_rmse: 0.37992 |  0:02:21s
epoch 25 | loss: 0.1625  | val_0_rmse: 0.5308  | val_1_rmse: 0.53232 |  0:02:27s
epoch 26 | loss: 0.17615 | val_0_rmse: 0.39847 | val_1_rmse: 0.39564 |  0:02:32s
epoch 27 | loss: 0.18983 | val_0_rmse: 0.421   | val_1_rmse: 0.42016 |  0:02:38s
epoch 28 | loss: 0.16041 | val_0_rmse: 0.44534 | val_1_rmse: 0.44441 |  0:02:44s
epoch 29 | loss: 0.17484 | val_0_rmse: 0.40961 | val_1_rmse: 0.4079  |  0:02:49s
epoch 30 | loss: 0.1828  | val_0_rmse: 0.58214 | val_1_rmse: 0.58321 |  0:02:55s
epoch 31 | loss: 0.17193 | val_0_rmse: 0.38941 | val_1_rmse: 0.38841 |  0:03:01s
epoch 32 | loss: 0.1685  | val_0_rmse: 0.46832 | val_1_rmse: 0.46473 |  0:03:06s
epoch 33 | loss: 0.20391 | val_0_rmse: 0.40364 | val_1_rmse: 0.40195 |  0:03:12s
epoch 34 | loss: 0.16382 | val_0_rmse: 0.39722 | val_1_rmse: 0.39413 |  0:03:18s
epoch 35 | loss: 0.15909 | val_0_rmse: 0.54366 | val_1_rmse: 0.54049 |  0:03:23s
epoch 36 | loss: 0.17021 | val_0_rmse: 0.43789 | val_1_rmse: 0.43576 |  0:03:29s
epoch 37 | loss: 0.16549 | val_0_rmse: 0.41352 | val_1_rmse: 0.41276 |  0:03:35s
epoch 38 | loss: 0.17603 | val_0_rmse: 0.43834 | val_1_rmse: 0.4379  |  0:03:40s
epoch 39 | loss: 0.15394 | val_0_rmse: 0.44261 | val_1_rmse: 0.44243 |  0:03:46s
epoch 40 | loss: 0.17938 | val_0_rmse: 0.54001 | val_1_rmse: 0.53738 |  0:03:52s
epoch 41 | loss: 0.15833 | val_0_rmse: 0.40406 | val_1_rmse: 0.39938 |  0:03:57s
epoch 42 | loss: 0.15716 | val_0_rmse: 0.50176 | val_1_rmse: 0.49916 |  0:04:03s
epoch 43 | loss: 0.15557 | val_0_rmse: 0.39847 | val_1_rmse: 0.39497 |  0:04:09s
epoch 44 | loss: 0.15912 | val_0_rmse: 0.41886 | val_1_rmse: 0.41651 |  0:04:14s
epoch 45 | loss: 0.1639  | val_0_rmse: 0.39832 | val_1_rmse: 0.39629 |  0:04:20s
epoch 46 | loss: 0.17565 | val_0_rmse: 0.41526 | val_1_rmse: 0.41377 |  0:04:26s
epoch 47 | loss: 0.17472 | val_0_rmse: 0.43667 | val_1_rmse: 0.43637 |  0:04:31s
epoch 48 | loss: 0.16998 | val_0_rmse: 0.41019 | val_1_rmse: 0.40734 |  0:04:37s
epoch 49 | loss: 0.15446 | val_0_rmse: 0.43165 | val_1_rmse: 0.43152 |  0:04:43s
epoch 50 | loss: 0.16594 | val_0_rmse: 0.44229 | val_1_rmse: 0.44224 |  0:04:48s
epoch 51 | loss: 0.14831 | val_0_rmse: 0.40299 | val_1_rmse: 0.40109 |  0:04:54s
epoch 52 | loss: 0.15435 | val_0_rmse: 0.42836 | val_1_rmse: 0.42874 |  0:05:00s
epoch 53 | loss: 0.16054 | val_0_rmse: 0.40351 | val_1_rmse: 0.40242 |  0:05:05s
epoch 54 | loss: 0.16219 | val_0_rmse: 0.40853 | val_1_rmse: 0.40641 |  0:05:11s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.37992
Best weights from best epoch are automatically used!
ended training at: 16:57:46
Feature importance:
[('Area', 0.12956253037750842), ('Baths', 0.2645434143258007), ('Beds', 0.018227849062828833), ('Latitude', 0.13441288598739057), ('Longitude', 0.24264227608927735), ('Month', 0.21061104415719417), ('Year', 0.0)]
Mean squared error is of 2732911021.6856723
Mean absolute error:35487.19407157681
MAPE:0.32902091158331354
R2 score:0.5984973881497787
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:57:46
epoch 0  | loss: 10.47081| val_0_rmse: 0.61516 | val_1_rmse: 0.61631 |  0:00:05s
epoch 1  | loss: 0.24745 | val_0_rmse: 0.76362 | val_1_rmse: 0.76562 |  0:00:11s
epoch 2  | loss: 0.23282 | val_0_rmse: 0.65198 | val_1_rmse: 0.65384 |  0:00:16s
epoch 3  | loss: 0.23374 | val_0_rmse: 0.56195 | val_1_rmse: 0.56353 |  0:00:22s
epoch 4  | loss: 0.22327 | val_0_rmse: 0.53979 | val_1_rmse: 0.54003 |  0:00:28s
epoch 5  | loss: 0.21242 | val_0_rmse: 0.55859 | val_1_rmse: 0.56105 |  0:00:33s
epoch 6  | loss: 0.23548 | val_0_rmse: 0.56714 | val_1_rmse: 0.57211 |  0:00:39s
epoch 7  | loss: 0.23307 | val_0_rmse: 0.48331 | val_1_rmse: 0.48316 |  0:00:45s
epoch 8  | loss: 0.22387 | val_0_rmse: 0.50052 | val_1_rmse: 0.50021 |  0:00:50s
epoch 9  | loss: 0.21337 | val_0_rmse: 0.63404 | val_1_rmse: 0.63456 |  0:00:56s
epoch 10 | loss: 0.20351 | val_0_rmse: 0.50872 | val_1_rmse: 0.50931 |  0:01:02s
epoch 11 | loss: 0.19989 | val_0_rmse: 0.43305 | val_1_rmse: 0.43458 |  0:01:07s
epoch 12 | loss: 0.19234 | val_0_rmse: 0.48618 | val_1_rmse: 0.48825 |  0:01:13s
epoch 13 | loss: 0.18908 | val_0_rmse: 0.45966 | val_1_rmse: 0.46149 |  0:01:19s
epoch 14 | loss: 0.18834 | val_0_rmse: 0.4041  | val_1_rmse: 0.40479 |  0:01:24s
epoch 15 | loss: 0.18605 | val_0_rmse: 0.45397 | val_1_rmse: 0.45395 |  0:01:30s
epoch 16 | loss: 0.18564 | val_0_rmse: 0.40329 | val_1_rmse: 0.40543 |  0:01:36s
epoch 17 | loss: 0.18362 | val_0_rmse: 0.44775 | val_1_rmse: 0.45067 |  0:01:41s
epoch 18 | loss: 0.18354 | val_0_rmse: 0.44283 | val_1_rmse: 0.44526 |  0:01:47s
epoch 19 | loss: 0.18116 | val_0_rmse: 0.39866 | val_1_rmse: 0.39999 |  0:01:53s
epoch 20 | loss: 0.1842  | val_0_rmse: 0.40549 | val_1_rmse: 0.40727 |  0:01:58s
epoch 21 | loss: 0.18664 | val_0_rmse: 0.41513 | val_1_rmse: 0.41623 |  0:02:04s
epoch 22 | loss: 0.17454 | val_0_rmse: 0.39104 | val_1_rmse: 0.39473 |  0:02:10s
epoch 23 | loss: 0.16071 | val_0_rmse: 0.38757 | val_1_rmse: 0.38988 |  0:02:15s
epoch 24 | loss: 0.16397 | val_0_rmse: 0.40034 | val_1_rmse: 0.4009  |  0:02:21s
epoch 25 | loss: 0.16346 | val_0_rmse: 0.40106 | val_1_rmse: 0.40225 |  0:02:27s
epoch 26 | loss: 0.15859 | val_0_rmse: 0.39084 | val_1_rmse: 0.39223 |  0:02:32s
epoch 27 | loss: 0.16425 | val_0_rmse: 0.38818 | val_1_rmse: 0.38979 |  0:02:38s
epoch 28 | loss: 0.17424 | val_0_rmse: 0.4117  | val_1_rmse: 0.41494 |  0:02:43s
epoch 29 | loss: 0.17145 | val_0_rmse: 0.42029 | val_1_rmse: 0.42215 |  0:02:49s
epoch 30 | loss: 0.16873 | val_0_rmse: 0.4354  | val_1_rmse: 0.43842 |  0:02:55s
epoch 31 | loss: 0.17408 | val_0_rmse: 0.43262 | val_1_rmse: 0.43501 |  0:03:01s
epoch 32 | loss: 0.17203 | val_0_rmse: 0.39543 | val_1_rmse: 0.39838 |  0:03:06s
epoch 33 | loss: 0.15793 | val_0_rmse: 0.40467 | val_1_rmse: 0.40755 |  0:03:12s
epoch 34 | loss: 0.16832 | val_0_rmse: 0.44418 | val_1_rmse: 0.44712 |  0:03:17s
epoch 35 | loss: 0.1736  | val_0_rmse: 0.44948 | val_1_rmse: 0.45204 |  0:03:23s
epoch 36 | loss: 0.16965 | val_0_rmse: 0.41627 | val_1_rmse: 0.4193  |  0:03:29s
epoch 37 | loss: 0.17124 | val_0_rmse: 0.37802 | val_1_rmse: 0.37962 |  0:03:34s
epoch 38 | loss: 0.1635  | val_0_rmse: 0.39868 | val_1_rmse: 0.40027 |  0:03:40s
epoch 39 | loss: 0.15435 | val_0_rmse: 0.40433 | val_1_rmse: 0.40545 |  0:03:46s
epoch 40 | loss: 0.16473 | val_0_rmse: 0.39381 | val_1_rmse: 0.39441 |  0:03:51s
epoch 41 | loss: 0.15376 | val_0_rmse: 0.38866 | val_1_rmse: 0.39101 |  0:03:57s
epoch 42 | loss: 0.16871 | val_0_rmse: 0.4091  | val_1_rmse: 0.40928 |  0:04:03s
epoch 43 | loss: 0.18956 | val_0_rmse: 0.39654 | val_1_rmse: 0.39909 |  0:04:08s
epoch 44 | loss: 0.16037 | val_0_rmse: 0.39279 | val_1_rmse: 0.39537 |  0:04:14s
epoch 45 | loss: 0.16105 | val_0_rmse: 0.375   | val_1_rmse: 0.37749 |  0:04:20s
epoch 46 | loss: 0.16602 | val_0_rmse: 0.40751 | val_1_rmse: 0.40833 |  0:04:26s
epoch 47 | loss: 0.1794  | val_0_rmse: 0.42594 | val_1_rmse: 0.42699 |  0:04:31s
epoch 48 | loss: 0.16652 | val_0_rmse: 0.38169 | val_1_rmse: 0.38355 |  0:04:37s
epoch 49 | loss: 0.18211 | val_0_rmse: 0.39712 | val_1_rmse: 0.39972 |  0:04:42s
epoch 50 | loss: 0.1503  | val_0_rmse: 0.40164 | val_1_rmse: 0.40302 |  0:04:48s
epoch 51 | loss: 0.16556 | val_0_rmse: 0.4379  | val_1_rmse: 0.43962 |  0:04:54s
epoch 52 | loss: 0.17855 | val_0_rmse: 0.41481 | val_1_rmse: 0.41685 |  0:04:59s
epoch 53 | loss: 0.14794 | val_0_rmse: 0.38546 | val_1_rmse: 0.38829 |  0:05:05s
epoch 54 | loss: 0.14741 | val_0_rmse: 0.42282 | val_1_rmse: 0.42447 |  0:05:11s
epoch 55 | loss: 0.16636 | val_0_rmse: 0.46148 | val_1_rmse: 0.46513 |  0:05:16s
epoch 56 | loss: 0.1478  | val_0_rmse: 0.37995 | val_1_rmse: 0.38316 |  0:05:22s
epoch 57 | loss: 0.14908 | val_0_rmse: 0.40624 | val_1_rmse: 0.40883 |  0:05:28s
epoch 58 | loss: 0.16122 | val_0_rmse: 0.41506 | val_1_rmse: 0.41702 |  0:05:33s
epoch 59 | loss: 0.16689 | val_0_rmse: 0.38796 | val_1_rmse: 0.38912 |  0:05:39s
epoch 60 | loss: 0.15544 | val_0_rmse: 0.39944 | val_1_rmse: 0.40173 |  0:05:45s
epoch 61 | loss: 0.14722 | val_0_rmse: 0.37534 | val_1_rmse: 0.37775 |  0:05:50s
epoch 62 | loss: 0.15499 | val_0_rmse: 0.41496 | val_1_rmse: 0.41659 |  0:05:56s
epoch 63 | loss: 0.15822 | val_0_rmse: 0.40364 | val_1_rmse: 0.4052  |  0:06:02s
epoch 64 | loss: 0.16459 | val_0_rmse: 0.46178 | val_1_rmse: 0.46407 |  0:06:07s
epoch 65 | loss: 0.16367 | val_0_rmse: 0.41028 | val_1_rmse: 0.41427 |  0:06:13s
epoch 66 | loss: 0.15057 | val_0_rmse: 0.41588 | val_1_rmse: 0.41759 |  0:06:19s
epoch 67 | loss: 0.16186 | val_0_rmse: 0.38804 | val_1_rmse: 0.39145 |  0:06:25s
epoch 68 | loss: 0.14919 | val_0_rmse: 0.37892 | val_1_rmse: 0.38169 |  0:06:30s
epoch 69 | loss: 0.1478  | val_0_rmse: 0.38361 | val_1_rmse: 0.38729 |  0:06:36s
epoch 70 | loss: 0.1541  | val_0_rmse: 0.40349 | val_1_rmse: 0.40433 |  0:06:42s
epoch 71 | loss: 0.15819 | val_0_rmse: 0.39369 | val_1_rmse: 0.39579 |  0:06:47s
epoch 72 | loss: 0.1518  | val_0_rmse: 0.56652 | val_1_rmse: 0.57028 |  0:06:53s
epoch 73 | loss: 0.15694 | val_0_rmse: 0.46604 | val_1_rmse: 0.4682  |  0:06:59s
epoch 74 | loss: 0.185   | val_0_rmse: 0.43855 | val_1_rmse: 0.44124 |  0:07:04s
epoch 75 | loss: 0.17924 | val_0_rmse: 0.4041  | val_1_rmse: 0.40767 |  0:07:10s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.37749
Best weights from best epoch are automatically used!
ended training at: 17:04:58
Feature importance:
[('Area', 0.09240704101417271), ('Baths', 0.449867430845799), ('Beds', 0.031333447497829854), ('Latitude', 0.0027118806256882064), ('Longitude', 0.08266920755672373), ('Month', 0.33567079422153323), ('Year', 0.005340198238253278)]
Mean squared error is of 2372060502.05208
Mean absolute error:34946.299876857454
MAPE:0.3186250834465428
R2 score:0.6485664060379956
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:04:59
epoch 0  | loss: 10.65929| val_0_rmse: 0.55958 | val_1_rmse: 0.55045 |  0:00:05s
epoch 1  | loss: 0.27465 | val_0_rmse: 0.59069 | val_1_rmse: 0.58749 |  0:00:11s
epoch 2  | loss: 0.24799 | val_0_rmse: 0.90982 | val_1_rmse: 0.90956 |  0:00:17s
epoch 3  | loss: 0.22923 | val_0_rmse: 0.75333 | val_1_rmse: 0.75383 |  0:00:22s
epoch 4  | loss: 0.23032 | val_0_rmse: 0.77356 | val_1_rmse: 0.77246 |  0:00:28s
epoch 5  | loss: 0.21827 | val_0_rmse: 1.05578 | val_1_rmse: 1.0573  |  0:00:34s
epoch 6  | loss: 0.20659 | val_0_rmse: 1.02268 | val_1_rmse: 1.02508 |  0:00:39s
epoch 7  | loss: 0.20166 | val_0_rmse: 0.80488 | val_1_rmse: 0.80473 |  0:00:45s
epoch 8  | loss: 0.19317 | val_0_rmse: 0.79877 | val_1_rmse: 0.79975 |  0:00:51s
epoch 9  | loss: 0.19197 | val_0_rmse: 0.68903 | val_1_rmse: 0.69019 |  0:00:56s
epoch 10 | loss: 0.16915 | val_0_rmse: 0.45396 | val_1_rmse: 0.45689 |  0:01:02s
epoch 11 | loss: 0.17769 | val_0_rmse: 0.41953 | val_1_rmse: 0.41972 |  0:01:08s
epoch 12 | loss: 0.16811 | val_0_rmse: 0.40679 | val_1_rmse: 0.40748 |  0:01:13s
epoch 13 | loss: 0.18337 | val_0_rmse: 0.43981 | val_1_rmse: 0.44295 |  0:01:19s
epoch 14 | loss: 0.1749  | val_0_rmse: 0.39569 | val_1_rmse: 0.39381 |  0:01:25s
epoch 15 | loss: 0.1883  | val_0_rmse: 0.41512 | val_1_rmse: 0.41354 |  0:01:30s
epoch 16 | loss: 0.21923 | val_0_rmse: 0.47861 | val_1_rmse: 0.47461 |  0:01:36s
epoch 17 | loss: 0.19356 | val_0_rmse: 0.42001 | val_1_rmse: 0.42179 |  0:01:42s
epoch 18 | loss: 0.19417 | val_0_rmse: 0.43355 | val_1_rmse: 0.43081 |  0:01:48s
epoch 19 | loss: 0.17528 | val_0_rmse: 0.41595 | val_1_rmse: 0.42014 |  0:01:53s
epoch 20 | loss: 0.17222 | val_0_rmse: 0.39332 | val_1_rmse: 0.39612 |  0:01:59s
epoch 21 | loss: 0.16415 | val_0_rmse: 0.38982 | val_1_rmse: 0.39253 |  0:02:05s
epoch 22 | loss: 0.16868 | val_0_rmse: 0.38178 | val_1_rmse: 0.38247 |  0:02:10s
epoch 23 | loss: 0.17778 | val_0_rmse: 0.43178 | val_1_rmse: 0.43602 |  0:02:16s
epoch 24 | loss: 0.18562 | val_0_rmse: 0.50274 | val_1_rmse: 0.50739 |  0:02:22s
epoch 25 | loss: 0.17783 | val_0_rmse: 0.38701 | val_1_rmse: 0.38832 |  0:02:27s
epoch 26 | loss: 0.18154 | val_0_rmse: 0.39883 | val_1_rmse: 0.39845 |  0:02:33s
epoch 27 | loss: 0.16144 | val_0_rmse: 0.38921 | val_1_rmse: 0.39119 |  0:02:39s
epoch 28 | loss: 0.16194 | val_0_rmse: 0.40403 | val_1_rmse: 0.40568 |  0:02:44s
epoch 29 | loss: 0.15989 | val_0_rmse: 0.39062 | val_1_rmse: 0.3929  |  0:02:50s
epoch 30 | loss: 0.18218 | val_0_rmse: 0.38328 | val_1_rmse: 0.38563 |  0:02:56s
epoch 31 | loss: 0.1906  | val_0_rmse: 0.3981  | val_1_rmse: 0.39841 |  0:03:01s
epoch 32 | loss: 0.17461 | val_0_rmse: 0.39797 | val_1_rmse: 0.39791 |  0:03:07s
epoch 33 | loss: 0.17336 | val_0_rmse: 0.42284 | val_1_rmse: 0.42228 |  0:03:13s
epoch 34 | loss: 0.1816  | val_0_rmse: 0.38726 | val_1_rmse: 0.38908 |  0:03:19s
epoch 35 | loss: 0.21685 | val_0_rmse: 0.39906 | val_1_rmse: 0.39801 |  0:03:24s
epoch 36 | loss: 0.17668 | val_0_rmse: 0.49517 | val_1_rmse: 0.49901 |  0:03:30s
epoch 37 | loss: 0.16576 | val_0_rmse: 0.40971 | val_1_rmse: 0.41321 |  0:03:36s
epoch 38 | loss: 0.16304 | val_0_rmse: 0.39652 | val_1_rmse: 0.39657 |  0:03:41s
epoch 39 | loss: 0.17004 | val_0_rmse: 0.4062  | val_1_rmse: 0.40546 |  0:03:47s
epoch 40 | loss: 0.17737 | val_0_rmse: 0.38702 | val_1_rmse: 0.38818 |  0:03:53s
epoch 41 | loss: 0.16922 | val_0_rmse: 0.40273 | val_1_rmse: 0.40298 |  0:03:58s
epoch 42 | loss: 0.17552 | val_0_rmse: 0.37914 | val_1_rmse: 0.38003 |  0:04:04s
epoch 43 | loss: 0.17647 | val_0_rmse: 0.40852 | val_1_rmse: 0.40759 |  0:04:10s
epoch 44 | loss: 0.17164 | val_0_rmse: 0.4114  | val_1_rmse: 0.41131 |  0:04:15s
epoch 45 | loss: 0.17377 | val_0_rmse: 0.38274 | val_1_rmse: 0.38297 |  0:04:21s
epoch 46 | loss: 0.17139 | val_0_rmse: 0.47559 | val_1_rmse: 0.47975 |  0:04:27s
epoch 47 | loss: 0.18186 | val_0_rmse: 0.41191 | val_1_rmse: 0.41149 |  0:04:32s
epoch 48 | loss: 0.16802 | val_0_rmse: 0.43478 | val_1_rmse: 0.43723 |  0:04:38s
epoch 49 | loss: 0.15818 | val_0_rmse: 0.39408 | val_1_rmse: 0.39678 |  0:04:44s
epoch 50 | loss: 0.15963 | val_0_rmse: 0.46141 | val_1_rmse: 0.45937 |  0:04:49s
epoch 51 | loss: 0.15652 | val_0_rmse: 0.3884  | val_1_rmse: 0.38881 |  0:04:55s
epoch 52 | loss: 0.15703 | val_0_rmse: 0.39069 | val_1_rmse: 0.39292 |  0:05:01s
epoch 53 | loss: 0.17937 | val_0_rmse: 0.38107 | val_1_rmse: 0.3823  |  0:05:06s
epoch 54 | loss: 0.15536 | val_0_rmse: 0.40677 | val_1_rmse: 0.4095  |  0:05:12s
epoch 55 | loss: 0.15632 | val_0_rmse: 0.37908 | val_1_rmse: 0.38271 |  0:05:18s
epoch 56 | loss: 0.15927 | val_0_rmse: 0.37898 | val_1_rmse: 0.38046 |  0:05:23s
epoch 57 | loss: 0.16615 | val_0_rmse: 0.4095  | val_1_rmse: 0.41314 |  0:05:29s
epoch 58 | loss: 0.21795 | val_0_rmse: 0.41965 | val_1_rmse: 0.41843 |  0:05:35s
epoch 59 | loss: 0.16494 | val_0_rmse: 0.40296 | val_1_rmse: 0.40484 |  0:05:40s
epoch 60 | loss: 0.15423 | val_0_rmse: 0.39912 | val_1_rmse: 0.40143 |  0:05:46s
epoch 61 | loss: 0.14924 | val_0_rmse: 0.37764 | val_1_rmse: 0.38033 |  0:05:51s
epoch 62 | loss: 0.15337 | val_0_rmse: 0.42473 | val_1_rmse: 0.42716 |  0:05:57s
epoch 63 | loss: 0.15425 | val_0_rmse: 0.37597 | val_1_rmse: 0.37776 |  0:06:03s
epoch 64 | loss: 0.15013 | val_0_rmse: 0.41228 | val_1_rmse: 0.41435 |  0:06:09s
epoch 65 | loss: 0.15179 | val_0_rmse: 0.3814  | val_1_rmse: 0.38221 |  0:06:14s
epoch 66 | loss: 0.16381 | val_0_rmse: 0.39319 | val_1_rmse: 0.39381 |  0:06:20s
epoch 67 | loss: 0.16169 | val_0_rmse: 0.62176 | val_1_rmse: 0.617   |  0:06:26s
epoch 68 | loss: 0.16363 | val_0_rmse: 0.45654 | val_1_rmse: 0.45958 |  0:06:31s
epoch 69 | loss: 0.18553 | val_0_rmse: 0.45784 | val_1_rmse: 0.4559  |  0:06:37s
epoch 70 | loss: 0.19354 | val_0_rmse: 0.44298 | val_1_rmse: 0.43882 |  0:06:43s
epoch 71 | loss: 0.18779 | val_0_rmse: 0.41045 | val_1_rmse: 0.41203 |  0:06:48s
epoch 72 | loss: 0.17127 | val_0_rmse: 0.41015 | val_1_rmse: 0.41102 |  0:06:54s
epoch 73 | loss: 0.17283 | val_0_rmse: 0.42756 | val_1_rmse: 0.429   |  0:07:00s
epoch 74 | loss: 0.1691  | val_0_rmse: 0.45021 | val_1_rmse: 0.44835 |  0:07:06s
epoch 75 | loss: 0.17118 | val_0_rmse: 0.40856 | val_1_rmse: 0.40648 |  0:07:11s
epoch 76 | loss: 0.17692 | val_0_rmse: 0.41379 | val_1_rmse: 0.41342 |  0:07:17s
epoch 77 | loss: 0.17778 | val_0_rmse: 0.42334 | val_1_rmse: 0.42658 |  0:07:23s
epoch 78 | loss: 0.1651  | val_0_rmse: 0.40418 | val_1_rmse: 0.4054  |  0:07:28s
epoch 79 | loss: 0.1612  | val_0_rmse: 0.41039 | val_1_rmse: 0.41257 |  0:07:34s
epoch 80 | loss: 0.1716  | val_0_rmse: 0.39346 | val_1_rmse: 0.39327 |  0:07:40s
epoch 81 | loss: 0.15441 | val_0_rmse: 0.41352 | val_1_rmse: 0.41374 |  0:07:45s
epoch 82 | loss: 0.16454 | val_0_rmse: 0.43053 | val_1_rmse: 0.43436 |  0:07:51s
epoch 83 | loss: 0.1672  | val_0_rmse: 0.38309 | val_1_rmse: 0.38323 |  0:07:57s
epoch 84 | loss: 0.1519  | val_0_rmse: 0.38651 | val_1_rmse: 0.38804 |  0:08:02s
epoch 85 | loss: 0.15615 | val_0_rmse: 0.38732 | val_1_rmse: 0.38871 |  0:08:08s
epoch 86 | loss: 0.17451 | val_0_rmse: 0.38766 | val_1_rmse: 0.38889 |  0:08:14s
epoch 87 | loss: 0.15303 | val_0_rmse: 0.384   | val_1_rmse: 0.3854  |  0:08:19s
epoch 88 | loss: 0.15517 | val_0_rmse: 0.40149 | val_1_rmse: 0.40228 |  0:08:25s
epoch 89 | loss: 0.14878 | val_0_rmse: 0.39925 | val_1_rmse: 0.39882 |  0:08:31s
epoch 90 | loss: 0.15175 | val_0_rmse: 0.37958 | val_1_rmse: 0.37959 |  0:08:36s
epoch 91 | loss: 0.15102 | val_0_rmse: 0.38528 | val_1_rmse: 0.38625 |  0:08:42s
epoch 92 | loss: 0.15469 | val_0_rmse: 0.43387 | val_1_rmse: 0.43114 |  0:08:48s
epoch 93 | loss: 0.15167 | val_0_rmse: 0.4533  | val_1_rmse: 0.45122 |  0:08:54s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.37776
Best weights from best epoch are automatically used!
ended training at: 17:13:54
Feature importance:
[('Area', 0.31511439209343584), ('Baths', 0.14530001700959344), ('Beds', 0.0), ('Latitude', 0.2157682766813599), ('Longitude', 0.32381731421561083), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2651663251.59347
Mean absolute error:36231.18082194753
MAPE:0.3112028830734448
R2 score:0.6152135835245255
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:14:05
epoch 0  | loss: 25.27047| val_0_rmse: 5.90226 | val_1_rmse: 5.89435 |  0:00:02s
epoch 1  | loss: 0.41666 | val_0_rmse: 1.82933 | val_1_rmse: 1.82871 |  0:00:04s
epoch 2  | loss: 0.23076 | val_0_rmse: 0.90089 | val_1_rmse: 0.90364 |  0:00:06s
epoch 3  | loss: 0.28985 | val_0_rmse: 1.46094 | val_1_rmse: 1.4575  |  0:00:08s
epoch 4  | loss: 0.18192 | val_0_rmse: 0.91395 | val_1_rmse: 0.91372 |  0:00:10s
epoch 5  | loss: 0.15317 | val_0_rmse: 0.63546 | val_1_rmse: 0.62998 |  0:00:12s
epoch 6  | loss: 0.18006 | val_0_rmse: 0.6645  | val_1_rmse: 0.66289 |  0:00:14s
epoch 7  | loss: 0.14737 | val_0_rmse: 0.59132 | val_1_rmse: 0.5872  |  0:00:16s
epoch 8  | loss: 0.16047 | val_0_rmse: 0.62512 | val_1_rmse: 0.61937 |  0:00:18s
epoch 9  | loss: 0.14144 | val_0_rmse: 0.59143 | val_1_rmse: 0.58604 |  0:00:20s
epoch 10 | loss: 0.15777 | val_0_rmse: 0.61557 | val_1_rmse: 0.6105  |  0:00:22s
epoch 11 | loss: 0.15664 | val_0_rmse: 0.61216 | val_1_rmse: 0.60773 |  0:00:24s
epoch 12 | loss: 0.15303 | val_0_rmse: 0.61728 | val_1_rmse: 0.61199 |  0:00:26s
epoch 13 | loss: 0.15161 | val_0_rmse: 0.60632 | val_1_rmse: 0.60047 |  0:00:28s
epoch 14 | loss: 0.14602 | val_0_rmse: 0.51005 | val_1_rmse: 0.50377 |  0:00:31s
epoch 15 | loss: 0.14615 | val_0_rmse: 0.55441 | val_1_rmse: 0.55064 |  0:00:33s
epoch 16 | loss: 0.14702 | val_0_rmse: 0.46419 | val_1_rmse: 0.45734 |  0:00:35s
epoch 17 | loss: 0.14378 | val_0_rmse: 0.52083 | val_1_rmse: 0.51829 |  0:00:37s
epoch 18 | loss: 0.14545 | val_0_rmse: 0.41764 | val_1_rmse: 0.41292 |  0:00:39s
epoch 19 | loss: 0.14261 | val_0_rmse: 0.54864 | val_1_rmse: 0.54611 |  0:00:41s
epoch 20 | loss: 0.13797 | val_0_rmse: 0.47421 | val_1_rmse: 0.47047 |  0:00:43s
epoch 21 | loss: 0.13187 | val_0_rmse: 0.48429 | val_1_rmse: 0.48043 |  0:00:45s
epoch 22 | loss: 0.15002 | val_0_rmse: 0.55267 | val_1_rmse: 0.54781 |  0:00:47s
epoch 23 | loss: 0.16073 | val_0_rmse: 0.47525 | val_1_rmse: 0.47039 |  0:00:49s
epoch 24 | loss: 0.19912 | val_0_rmse: 0.71913 | val_1_rmse: 0.71546 |  0:00:51s
epoch 25 | loss: 0.16795 | val_0_rmse: 0.51801 | val_1_rmse: 0.51313 |  0:00:53s
epoch 26 | loss: 0.11894 | val_0_rmse: 0.47613 | val_1_rmse: 0.46699 |  0:00:55s
epoch 27 | loss: 0.13219 | val_0_rmse: 0.43989 | val_1_rmse: 0.43293 |  0:00:57s
epoch 28 | loss: 0.14211 | val_0_rmse: 0.46638 | val_1_rmse: 0.46197 |  0:00:59s
epoch 29 | loss: 0.14177 | val_0_rmse: 0.3698  | val_1_rmse: 0.36493 |  0:01:01s
epoch 30 | loss: 0.14258 | val_0_rmse: 0.404   | val_1_rmse: 0.4005  |  0:01:03s
epoch 31 | loss: 0.14472 | val_0_rmse: 0.3621  | val_1_rmse: 0.35756 |  0:01:06s
epoch 32 | loss: 0.14057 | val_0_rmse: 0.38981 | val_1_rmse: 0.3869  |  0:01:08s
epoch 33 | loss: 0.14007 | val_0_rmse: 0.34489 | val_1_rmse: 0.33902 |  0:01:10s
epoch 34 | loss: 0.1387  | val_0_rmse: 0.394   | val_1_rmse: 0.38944 |  0:01:12s
epoch 35 | loss: 0.13985 | val_0_rmse: 0.34428 | val_1_rmse: 0.33896 |  0:01:14s
epoch 36 | loss: 0.13921 | val_0_rmse: 0.38107 | val_1_rmse: 0.37609 |  0:01:16s
epoch 37 | loss: 0.14112 | val_0_rmse: 0.34944 | val_1_rmse: 0.34217 |  0:01:18s
epoch 38 | loss: 0.13962 | val_0_rmse: 0.35286 | val_1_rmse: 0.34742 |  0:01:20s
epoch 39 | loss: 0.13987 | val_0_rmse: 0.33177 | val_1_rmse: 0.32733 |  0:01:22s
epoch 40 | loss: 0.1386  | val_0_rmse: 0.32719 | val_1_rmse: 0.32027 |  0:01:24s
epoch 41 | loss: 0.12398 | val_0_rmse: 0.33136 | val_1_rmse: 0.32496 |  0:01:26s
epoch 42 | loss: 0.11826 | val_0_rmse: 0.34847 | val_1_rmse: 0.34015 |  0:01:28s
epoch 43 | loss: 0.12324 | val_0_rmse: 0.33911 | val_1_rmse: 0.33253 |  0:01:30s
epoch 44 | loss: 0.1195  | val_0_rmse: 0.32866 | val_1_rmse: 0.32319 |  0:01:32s
epoch 45 | loss: 0.11875 | val_0_rmse: 0.32364 | val_1_rmse: 0.31744 |  0:01:34s
epoch 46 | loss: 0.11699 | val_0_rmse: 0.32873 | val_1_rmse: 0.32336 |  0:01:36s
epoch 47 | loss: 0.11486 | val_0_rmse: 0.34218 | val_1_rmse: 0.33654 |  0:01:39s
epoch 48 | loss: 0.11953 | val_0_rmse: 0.34027 | val_1_rmse: 0.33271 |  0:01:41s
epoch 49 | loss: 0.11979 | val_0_rmse: 0.322   | val_1_rmse: 0.31594 |  0:01:43s
epoch 50 | loss: 0.11793 | val_0_rmse: 0.32326 | val_1_rmse: 0.31717 |  0:01:45s
epoch 51 | loss: 0.11612 | val_0_rmse: 0.3274  | val_1_rmse: 0.31889 |  0:01:47s
epoch 52 | loss: 0.11867 | val_0_rmse: 0.33028 | val_1_rmse: 0.32231 |  0:01:49s
epoch 53 | loss: 0.11446 | val_0_rmse: 0.39337 | val_1_rmse: 0.38953 |  0:01:51s
epoch 54 | loss: 0.11571 | val_0_rmse: 0.32078 | val_1_rmse: 0.31408 |  0:01:53s
epoch 55 | loss: 0.119   | val_0_rmse: 0.32955 | val_1_rmse: 0.32405 |  0:01:55s
epoch 56 | loss: 0.12907 | val_0_rmse: 0.36897 | val_1_rmse: 0.36367 |  0:01:57s
epoch 57 | loss: 0.13915 | val_0_rmse: 0.33314 | val_1_rmse: 0.32756 |  0:01:59s
epoch 58 | loss: 0.17639 | val_0_rmse: 0.33842 | val_1_rmse: 0.32982 |  0:02:01s
epoch 59 | loss: 0.13853 | val_0_rmse: 0.32767 | val_1_rmse: 0.32303 |  0:02:03s
epoch 60 | loss: 0.14322 | val_0_rmse: 0.35806 | val_1_rmse: 0.34966 |  0:02:05s
epoch 61 | loss: 0.13836 | val_0_rmse: 0.34774 | val_1_rmse: 0.34253 |  0:02:07s
epoch 62 | loss: 0.12151 | val_0_rmse: 0.33294 | val_1_rmse: 0.32604 |  0:02:09s
epoch 63 | loss: 0.1302  | val_0_rmse: 0.35464 | val_1_rmse: 0.3512  |  0:02:11s
epoch 64 | loss: 0.17519 | val_0_rmse: 0.33315 | val_1_rmse: 0.32366 |  0:02:13s
epoch 65 | loss: 0.12094 | val_0_rmse: 0.38476 | val_1_rmse: 0.37836 |  0:02:16s
epoch 66 | loss: 0.12144 | val_0_rmse: 0.33583 | val_1_rmse: 0.32553 |  0:02:18s
epoch 67 | loss: 0.14569 | val_0_rmse: 0.40707 | val_1_rmse: 0.4009  |  0:02:20s
epoch 68 | loss: 0.22532 | val_0_rmse: 0.40551 | val_1_rmse: 0.39946 |  0:02:22s
epoch 69 | loss: 0.12582 | val_0_rmse: 0.34146 | val_1_rmse: 0.33378 |  0:02:24s
epoch 70 | loss: 0.11688 | val_0_rmse: 0.33162 | val_1_rmse: 0.32534 |  0:02:26s
epoch 71 | loss: 0.133   | val_0_rmse: 0.44779 | val_1_rmse: 0.44055 |  0:02:28s
epoch 72 | loss: 0.13736 | val_0_rmse: 0.32894 | val_1_rmse: 0.32066 |  0:02:30s
epoch 73 | loss: 0.13618 | val_0_rmse: 0.33777 | val_1_rmse: 0.33047 |  0:02:32s
epoch 74 | loss: 0.19841 | val_0_rmse: 0.37009 | val_1_rmse: 0.36156 |  0:02:34s
epoch 75 | loss: 0.19834 | val_0_rmse: 0.34316 | val_1_rmse: 0.33427 |  0:02:36s
epoch 76 | loss: 0.19037 | val_0_rmse: 0.48588 | val_1_rmse: 0.48038 |  0:02:38s
epoch 77 | loss: 0.19475 | val_0_rmse: 0.45759 | val_1_rmse: 0.4527  |  0:02:40s
epoch 78 | loss: 0.19493 | val_0_rmse: 0.38255 | val_1_rmse: 0.37619 |  0:02:42s
epoch 79 | loss: 0.19938 | val_0_rmse: 0.38396 | val_1_rmse: 0.3774  |  0:02:44s
epoch 80 | loss: 0.19077 | val_0_rmse: 0.3866  | val_1_rmse: 0.37998 |  0:02:46s
epoch 81 | loss: 0.14259 | val_0_rmse: 0.33193 | val_1_rmse: 0.32453 |  0:02:48s
epoch 82 | loss: 0.13911 | val_0_rmse: 0.33204 | val_1_rmse: 0.32472 |  0:02:51s
epoch 83 | loss: 0.13765 | val_0_rmse: 0.3291  | val_1_rmse: 0.32046 |  0:02:53s
epoch 84 | loss: 0.13918 | val_0_rmse: 0.35338 | val_1_rmse: 0.34596 |  0:02:55s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.31408
Best weights from best epoch are automatically used!
ended training at: 17:17:00
Feature importance:
[('Area', 0.26465711149480514), ('Baths', 0.2686343690857761), ('Beds', 0.13901964569207242), ('Latitude', 0.0916493783731183), ('Longitude', 0.20893373720060698), ('Month', 0.02710558324872255), ('Year', 1.7490489848840454e-07)]
Mean squared error is of 1111674057.5999243
Mean absolute error:22229.35144647046
MAPE:0.2516397547495963
R2 score:0.7184439106109218
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:17:01
epoch 0  | loss: 25.83103| val_0_rmse: 7.26209 | val_1_rmse: 7.23823 |  0:00:02s
epoch 1  | loss: 0.62039 | val_0_rmse: 1.1636  | val_1_rmse: 1.15687 |  0:00:04s
epoch 2  | loss: 0.34095 | val_0_rmse: 1.34004 | val_1_rmse: 1.33352 |  0:00:06s
epoch 3  | loss: 0.34511 | val_0_rmse: 1.021   | val_1_rmse: 1.01726 |  0:00:08s
epoch 4  | loss: 0.27743 | val_0_rmse: 0.75419 | val_1_rmse: 0.7495  |  0:00:10s
epoch 5  | loss: 0.181   | val_0_rmse: 0.88838 | val_1_rmse: 0.88357 |  0:00:12s
epoch 6  | loss: 0.15901 | val_0_rmse: 0.71687 | val_1_rmse: 0.71348 |  0:00:14s
epoch 7  | loss: 0.15912 | val_0_rmse: 0.56967 | val_1_rmse: 0.56756 |  0:00:16s
epoch 8  | loss: 0.14218 | val_0_rmse: 0.56899 | val_1_rmse: 0.56667 |  0:00:18s
epoch 9  | loss: 0.16869 | val_0_rmse: 0.54033 | val_1_rmse: 0.54097 |  0:00:20s
epoch 10 | loss: 0.1497  | val_0_rmse: 0.54337 | val_1_rmse: 0.54687 |  0:00:22s
epoch 11 | loss: 0.15161 | val_0_rmse: 0.50199 | val_1_rmse: 0.50129 |  0:00:24s
epoch 12 | loss: 0.13748 | val_0_rmse: 0.51848 | val_1_rmse: 0.51847 |  0:00:26s
epoch 13 | loss: 0.15436 | val_0_rmse: 0.51355 | val_1_rmse: 0.51768 |  0:00:28s
epoch 14 | loss: 0.13748 | val_0_rmse: 0.55494 | val_1_rmse: 0.55872 |  0:00:30s
epoch 15 | loss: 0.1513  | val_0_rmse: 0.51205 | val_1_rmse: 0.51444 |  0:00:32s
epoch 16 | loss: 0.14648 | val_0_rmse: 0.48084 | val_1_rmse: 0.48453 |  0:00:34s
epoch 17 | loss: 0.14706 | val_0_rmse: 0.46426 | val_1_rmse: 0.46718 |  0:00:37s
epoch 18 | loss: 0.15041 | val_0_rmse: 0.4374  | val_1_rmse: 0.44148 |  0:00:39s
epoch 19 | loss: 0.13319 | val_0_rmse: 0.47023 | val_1_rmse: 0.47136 |  0:00:41s
epoch 20 | loss: 0.13835 | val_0_rmse: 0.43242 | val_1_rmse: 0.43481 |  0:00:43s
epoch 21 | loss: 0.1214  | val_0_rmse: 0.45563 | val_1_rmse: 0.45908 |  0:00:45s
epoch 22 | loss: 0.12105 | val_0_rmse: 0.48897 | val_1_rmse: 0.49368 |  0:00:47s
epoch 23 | loss: 0.13593 | val_0_rmse: 0.48365 | val_1_rmse: 0.49013 |  0:00:49s
epoch 24 | loss: 0.12805 | val_0_rmse: 0.48206 | val_1_rmse: 0.48831 |  0:00:51s
epoch 25 | loss: 0.15008 | val_0_rmse: 0.48781 | val_1_rmse: 0.49695 |  0:00:53s
epoch 26 | loss: 0.17283 | val_0_rmse: 0.49844 | val_1_rmse: 0.50271 |  0:00:55s
epoch 27 | loss: 0.12342 | val_0_rmse: 0.48109 | val_1_rmse: 0.48407 |  0:00:57s
epoch 28 | loss: 0.12441 | val_0_rmse: 0.40419 | val_1_rmse: 0.41075 |  0:00:59s
epoch 29 | loss: 0.12183 | val_0_rmse: 0.49036 | val_1_rmse: 0.49251 |  0:01:01s
epoch 30 | loss: 0.13946 | val_0_rmse: 0.47186 | val_1_rmse: 0.47319 |  0:01:03s
epoch 31 | loss: 0.14091 | val_0_rmse: 0.38888 | val_1_rmse: 0.39295 |  0:01:05s
epoch 32 | loss: 0.13525 | val_0_rmse: 0.42432 | val_1_rmse: 0.42516 |  0:01:07s
epoch 33 | loss: 0.13554 | val_0_rmse: 0.36494 | val_1_rmse: 0.37094 |  0:01:09s
epoch 34 | loss: 0.13915 | val_0_rmse: 0.3532  | val_1_rmse: 0.35496 |  0:01:11s
epoch 35 | loss: 0.13055 | val_0_rmse: 0.32996 | val_1_rmse: 0.33355 |  0:01:14s
epoch 36 | loss: 0.11662 | val_0_rmse: 0.32088 | val_1_rmse: 0.32393 |  0:01:16s
epoch 37 | loss: 0.12653 | val_0_rmse: 0.33416 | val_1_rmse: 0.33631 |  0:01:18s
epoch 38 | loss: 0.14048 | val_0_rmse: 0.38924 | val_1_rmse: 0.38727 |  0:01:20s
epoch 39 | loss: 0.12994 | val_0_rmse: 0.346   | val_1_rmse: 0.3447  |  0:01:22s
epoch 40 | loss: 0.14655 | val_0_rmse: 0.41222 | val_1_rmse: 0.40919 |  0:01:24s
epoch 41 | loss: 0.17728 | val_0_rmse: 0.39133 | val_1_rmse: 0.3891  |  0:01:26s
epoch 42 | loss: 0.15111 | val_0_rmse: 0.38612 | val_1_rmse: 0.39239 |  0:01:28s
epoch 43 | loss: 0.12718 | val_0_rmse: 0.35834 | val_1_rmse: 0.36093 |  0:01:30s
epoch 44 | loss: 0.13119 | val_0_rmse: 0.40166 | val_1_rmse: 0.41013 |  0:01:32s
epoch 45 | loss: 0.13329 | val_0_rmse: 0.34949 | val_1_rmse: 0.35321 |  0:01:34s
epoch 46 | loss: 0.17184 | val_0_rmse: 0.32163 | val_1_rmse: 0.32614 |  0:01:36s
epoch 47 | loss: 0.1384  | val_0_rmse: 0.39789 | val_1_rmse: 0.39612 |  0:01:38s
epoch 48 | loss: 0.13607 | val_0_rmse: 0.36564 | val_1_rmse: 0.36482 |  0:01:40s
epoch 49 | loss: 0.13284 | val_0_rmse: 0.36107 | val_1_rmse: 0.3676  |  0:01:42s
epoch 50 | loss: 0.13248 | val_0_rmse: 0.32813 | val_1_rmse: 0.3271  |  0:01:44s
epoch 51 | loss: 0.135   | val_0_rmse: 0.32718 | val_1_rmse: 0.33061 |  0:01:46s
epoch 52 | loss: 0.134   | val_0_rmse: 0.3673  | val_1_rmse: 0.36658 |  0:01:48s
epoch 53 | loss: 0.13239 | val_0_rmse: 0.34399 | val_1_rmse: 0.3487  |  0:01:50s
epoch 54 | loss: 0.13346 | val_0_rmse: 0.33912 | val_1_rmse: 0.33761 |  0:01:53s
epoch 55 | loss: 0.13554 | val_0_rmse: 0.3516  | val_1_rmse: 0.35014 |  0:01:55s
epoch 56 | loss: 0.13361 | val_0_rmse: 0.33468 | val_1_rmse: 0.33469 |  0:01:57s
epoch 57 | loss: 0.1528  | val_0_rmse: 0.50131 | val_1_rmse: 0.50276 |  0:01:59s
epoch 58 | loss: 0.13876 | val_0_rmse: 0.34417 | val_1_rmse: 0.34475 |  0:02:01s
epoch 59 | loss: 0.12835 | val_0_rmse: 0.33316 | val_1_rmse: 0.33701 |  0:02:03s
epoch 60 | loss: 0.11875 | val_0_rmse: 0.37175 | val_1_rmse: 0.37765 |  0:02:05s
epoch 61 | loss: 0.11201 | val_0_rmse: 0.33464 | val_1_rmse: 0.34136 |  0:02:07s
epoch 62 | loss: 0.12434 | val_0_rmse: 0.33353 | val_1_rmse: 0.33643 |  0:02:09s
epoch 63 | loss: 0.11159 | val_0_rmse: 0.34871 | val_1_rmse: 0.34661 |  0:02:11s
epoch 64 | loss: 0.11093 | val_0_rmse: 0.3231  | val_1_rmse: 0.32414 |  0:02:13s
epoch 65 | loss: 0.12944 | val_0_rmse: 0.44801 | val_1_rmse: 0.44521 |  0:02:15s
epoch 66 | loss: 0.13941 | val_0_rmse: 0.36596 | val_1_rmse: 0.37186 |  0:02:17s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.32393
Best weights from best epoch are automatically used!
ended training at: 17:19:19
Feature importance:
[('Area', 0.0), ('Baths', 0.0653966974778012), ('Beds', 0.2606765715243666), ('Latitude', 0.06171282094305035), ('Longitude', 0.39066264890705554), ('Month', 0.09942693193633319), ('Year', 0.12212432921139314)]
Mean squared error is of 1091343271.3689756
Mean absolute error:22362.82829277948
MAPE:0.27048216045476536
R2 score:0.7240351924213442
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:19:20
epoch 0  | loss: 24.79796| val_0_rmse: 3.25078 | val_1_rmse: 3.24272 |  0:00:02s
epoch 1  | loss: 0.45782 | val_0_rmse: 0.98705 | val_1_rmse: 0.9913  |  0:00:04s
epoch 2  | loss: 0.27267 | val_0_rmse: 1.04037 | val_1_rmse: 1.04538 |  0:00:06s
epoch 3  | loss: 0.21295 | val_0_rmse: 1.48768 | val_1_rmse: 1.49071 |  0:00:08s
epoch 4  | loss: 0.19179 | val_0_rmse: 0.79068 | val_1_rmse: 0.8001  |  0:00:10s
epoch 5  | loss: 0.202   | val_0_rmse: 0.60386 | val_1_rmse: 0.60522 |  0:00:12s
epoch 6  | loss: 0.23112 | val_0_rmse: 0.58127 | val_1_rmse: 0.58416 |  0:00:14s
epoch 7  | loss: 0.2022  | val_0_rmse: 0.64422 | val_1_rmse: 0.64656 |  0:00:16s
epoch 8  | loss: 0.1778  | val_0_rmse: 0.61718 | val_1_rmse: 0.61305 |  0:00:18s
epoch 9  | loss: 0.17479 | val_0_rmse: 0.55113 | val_1_rmse: 0.55191 |  0:00:20s
epoch 10 | loss: 0.16508 | val_0_rmse: 0.5379  | val_1_rmse: 0.54012 |  0:00:22s
epoch 11 | loss: 0.18945 | val_0_rmse: 0.51419 | val_1_rmse: 0.51275 |  0:00:24s
epoch 12 | loss: 0.17817 | val_0_rmse: 0.52275 | val_1_rmse: 0.52343 |  0:00:26s
epoch 13 | loss: 0.18067 | val_0_rmse: 0.4739  | val_1_rmse: 0.47112 |  0:00:28s
epoch 14 | loss: 0.17922 | val_0_rmse: 0.53378 | val_1_rmse: 0.53463 |  0:00:31s
epoch 15 | loss: 0.23646 | val_0_rmse: 0.52078 | val_1_rmse: 0.52427 |  0:00:33s
epoch 16 | loss: 0.15233 | val_0_rmse: 0.50044 | val_1_rmse: 0.49953 |  0:00:35s
epoch 17 | loss: 0.15169 | val_0_rmse: 0.52018 | val_1_rmse: 0.52426 |  0:00:37s
epoch 18 | loss: 0.15003 | val_0_rmse: 0.51232 | val_1_rmse: 0.51295 |  0:00:39s
epoch 19 | loss: 0.15898 | val_0_rmse: 0.49167 | val_1_rmse: 0.49289 |  0:00:41s
epoch 20 | loss: 0.19375 | val_0_rmse: 0.55238 | val_1_rmse: 0.55562 |  0:00:43s
epoch 21 | loss: 0.14619 | val_0_rmse: 0.49532 | val_1_rmse: 0.4955  |  0:00:45s
epoch 22 | loss: 0.14515 | val_0_rmse: 0.56357 | val_1_rmse: 0.56695 |  0:00:47s
epoch 23 | loss: 0.14077 | val_0_rmse: 0.55157 | val_1_rmse: 0.55166 |  0:00:49s
epoch 24 | loss: 0.13938 | val_0_rmse: 0.60076 | val_1_rmse: 0.60462 |  0:00:51s
epoch 25 | loss: 0.12762 | val_0_rmse: 0.54971 | val_1_rmse: 0.54805 |  0:00:53s
epoch 26 | loss: 0.14187 | val_0_rmse: 0.49766 | val_1_rmse: 0.49732 |  0:00:55s
epoch 27 | loss: 0.13054 | val_0_rmse: 0.57999 | val_1_rmse: 0.58434 |  0:00:57s
epoch 28 | loss: 0.13354 | val_0_rmse: 0.46231 | val_1_rmse: 0.46091 |  0:00:59s
epoch 29 | loss: 0.15941 | val_0_rmse: 0.41007 | val_1_rmse: 0.41178 |  0:01:01s
epoch 30 | loss: 0.12859 | val_0_rmse: 0.40412 | val_1_rmse: 0.40744 |  0:01:03s
epoch 31 | loss: 0.12803 | val_0_rmse: 0.36799 | val_1_rmse: 0.3728  |  0:01:05s
epoch 32 | loss: 0.11566 | val_0_rmse: 0.37585 | val_1_rmse: 0.37989 |  0:01:08s
epoch 33 | loss: 0.11545 | val_0_rmse: 0.33531 | val_1_rmse: 0.33787 |  0:01:10s
epoch 34 | loss: 0.11229 | val_0_rmse: 0.3337  | val_1_rmse: 0.33517 |  0:01:12s
epoch 35 | loss: 0.12768 | val_0_rmse: 0.37179 | val_1_rmse: 0.37276 |  0:01:14s
epoch 36 | loss: 0.14241 | val_0_rmse: 0.32457 | val_1_rmse: 0.32702 |  0:01:16s
epoch 37 | loss: 0.11524 | val_0_rmse: 0.34243 | val_1_rmse: 0.3476  |  0:01:18s
epoch 38 | loss: 0.11709 | val_0_rmse: 0.32503 | val_1_rmse: 0.32625 |  0:01:20s
epoch 39 | loss: 0.12183 | val_0_rmse: 0.35883 | val_1_rmse: 0.36337 |  0:01:22s
epoch 40 | loss: 0.11354 | val_0_rmse: 0.33551 | val_1_rmse: 0.33595 |  0:01:24s
epoch 41 | loss: 0.11587 | val_0_rmse: 0.33235 | val_1_rmse: 0.33436 |  0:01:26s
epoch 42 | loss: 0.11202 | val_0_rmse: 0.33995 | val_1_rmse: 0.3448  |  0:01:28s
epoch 43 | loss: 0.11321 | val_0_rmse: 0.32354 | val_1_rmse: 0.32663 |  0:01:30s
epoch 44 | loss: 0.12283 | val_0_rmse: 0.31816 | val_1_rmse: 0.31907 |  0:01:32s
epoch 45 | loss: 0.1502  | val_0_rmse: 0.33274 | val_1_rmse: 0.33622 |  0:01:34s
epoch 46 | loss: 0.11405 | val_0_rmse: 0.35604 | val_1_rmse: 0.36018 |  0:01:36s
epoch 47 | loss: 0.14709 | val_0_rmse: 0.33365 | val_1_rmse: 0.33729 |  0:01:38s
epoch 48 | loss: 0.13403 | val_0_rmse: 0.31594 | val_1_rmse: 0.31741 |  0:01:40s
epoch 49 | loss: 0.12541 | val_0_rmse: 0.32175 | val_1_rmse: 0.32439 |  0:01:42s
epoch 50 | loss: 0.1095  | val_0_rmse: 0.31597 | val_1_rmse: 0.31818 |  0:01:44s
epoch 51 | loss: 0.10786 | val_0_rmse: 0.34319 | val_1_rmse: 0.34726 |  0:01:46s
epoch 52 | loss: 0.10931 | val_0_rmse: 0.3204  | val_1_rmse: 0.32401 |  0:01:49s
epoch 53 | loss: 0.13027 | val_0_rmse: 0.35312 | val_1_rmse: 0.35239 |  0:01:51s
epoch 54 | loss: 0.13084 | val_0_rmse: 0.36753 | val_1_rmse: 0.37199 |  0:01:53s
epoch 55 | loss: 0.13392 | val_0_rmse: 0.37416 | val_1_rmse: 0.37301 |  0:01:55s
epoch 56 | loss: 0.13178 | val_0_rmse: 0.38229 | val_1_rmse: 0.38658 |  0:01:57s
epoch 57 | loss: 0.12901 | val_0_rmse: 0.37692 | val_1_rmse: 0.37522 |  0:01:59s
epoch 58 | loss: 0.13091 | val_0_rmse: 0.37333 | val_1_rmse: 0.3782  |  0:02:01s
epoch 59 | loss: 0.13278 | val_0_rmse: 0.35941 | val_1_rmse: 0.36009 |  0:02:03s
epoch 60 | loss: 0.13201 | val_0_rmse: 0.39598 | val_1_rmse: 0.40066 |  0:02:05s
epoch 61 | loss: 0.13194 | val_0_rmse: 0.35115 | val_1_rmse: 0.35039 |  0:02:07s
epoch 62 | loss: 0.12952 | val_0_rmse: 0.37846 | val_1_rmse: 0.38329 |  0:02:09s
epoch 63 | loss: 0.12876 | val_0_rmse: 0.35167 | val_1_rmse: 0.35065 |  0:02:11s
epoch 64 | loss: 0.13137 | val_0_rmse: 0.38625 | val_1_rmse: 0.39187 |  0:02:13s
epoch 65 | loss: 0.13013 | val_0_rmse: 0.33585 | val_1_rmse: 0.33615 |  0:02:15s
epoch 66 | loss: 0.12876 | val_0_rmse: 0.39209 | val_1_rmse: 0.39807 |  0:02:17s
epoch 67 | loss: 0.13051 | val_0_rmse: 0.388   | val_1_rmse: 0.38672 |  0:02:19s
epoch 68 | loss: 0.11376 | val_0_rmse: 0.3377  | val_1_rmse: 0.33718 |  0:02:21s
epoch 69 | loss: 0.1083  | val_0_rmse: 0.34638 | val_1_rmse: 0.35228 |  0:02:23s
epoch 70 | loss: 0.10735 | val_0_rmse: 0.33658 | val_1_rmse: 0.34126 |  0:02:25s
epoch 71 | loss: 0.12151 | val_0_rmse: 0.32314 | val_1_rmse: 0.32802 |  0:02:28s
epoch 72 | loss: 0.12958 | val_0_rmse: 0.33056 | val_1_rmse: 0.33055 |  0:02:30s
epoch 73 | loss: 0.12703 | val_0_rmse: 0.33155 | val_1_rmse: 0.33568 |  0:02:32s
epoch 74 | loss: 0.12881 | val_0_rmse: 0.32866 | val_1_rmse: 0.33055 |  0:02:34s
epoch 75 | loss: 0.11805 | val_0_rmse: 0.34024 | val_1_rmse: 0.34746 |  0:02:36s
epoch 76 | loss: 0.1103  | val_0_rmse: 0.35561 | val_1_rmse: 0.3602  |  0:02:38s
epoch 77 | loss: 0.1364  | val_0_rmse: 0.31027 | val_1_rmse: 0.31241 |  0:02:40s
epoch 78 | loss: 0.13177 | val_0_rmse: 0.31809 | val_1_rmse: 0.32122 |  0:02:42s
epoch 79 | loss: 0.13322 | val_0_rmse: 0.32206 | val_1_rmse: 0.3235  |  0:02:44s
epoch 80 | loss: 0.11153 | val_0_rmse: 0.31236 | val_1_rmse: 0.31492 |  0:02:46s
epoch 81 | loss: 0.10391 | val_0_rmse: 0.31355 | val_1_rmse: 0.31627 |  0:02:48s
epoch 82 | loss: 0.10424 | val_0_rmse: 0.32448 | val_1_rmse: 0.32972 |  0:02:50s
epoch 83 | loss: 0.10151 | val_0_rmse: 0.31867 | val_1_rmse: 0.32208 |  0:02:52s
epoch 84 | loss: 0.1169  | val_0_rmse: 0.34509 | val_1_rmse: 0.35037 |  0:02:54s
epoch 85 | loss: 0.12896 | val_0_rmse: 0.32919 | val_1_rmse: 0.33103 |  0:02:56s
epoch 86 | loss: 0.11732 | val_0_rmse: 0.31457 | val_1_rmse: 0.31709 |  0:02:58s
epoch 87 | loss: 0.10661 | val_0_rmse: 0.3155  | val_1_rmse: 0.31772 |  0:03:00s
epoch 88 | loss: 0.10819 | val_0_rmse: 0.32964 | val_1_rmse: 0.33152 |  0:03:02s
epoch 89 | loss: 0.11983 | val_0_rmse: 0.3159  | val_1_rmse: 0.31967 |  0:03:04s
epoch 90 | loss: 0.11657 | val_0_rmse: 0.33189 | val_1_rmse: 0.33653 |  0:03:06s
epoch 91 | loss: 0.10871 | val_0_rmse: 0.32123 | val_1_rmse: 0.32464 |  0:03:08s
epoch 92 | loss: 0.12344 | val_0_rmse: 0.37421 | val_1_rmse: 0.37921 |  0:03:11s
epoch 93 | loss: 0.11709 | val_0_rmse: 0.31705 | val_1_rmse: 0.32043 |  0:03:13s
epoch 94 | loss: 0.10826 | val_0_rmse: 0.35773 | val_1_rmse: 0.36257 |  0:03:15s
epoch 95 | loss: 0.11851 | val_0_rmse: 0.3121  | val_1_rmse: 0.3149  |  0:03:17s
epoch 96 | loss: 0.12188 | val_0_rmse: 0.36807 | val_1_rmse: 0.37498 |  0:03:19s
epoch 97 | loss: 0.12812 | val_0_rmse: 0.32425 | val_1_rmse: 0.32498 |  0:03:21s
epoch 98 | loss: 0.12535 | val_0_rmse: 0.34635 | val_1_rmse: 0.35189 |  0:03:23s
epoch 99 | loss: 0.12674 | val_0_rmse: 0.33286 | val_1_rmse: 0.33368 |  0:03:25s
epoch 100| loss: 0.12528 | val_0_rmse: 0.34508 | val_1_rmse: 0.35004 |  0:03:27s
epoch 101| loss: 0.12546 | val_0_rmse: 0.32639 | val_1_rmse: 0.32888 |  0:03:29s
epoch 102| loss: 0.11827 | val_0_rmse: 0.3201  | val_1_rmse: 0.32168 |  0:03:31s
epoch 103| loss: 0.10687 | val_0_rmse: 0.30992 | val_1_rmse: 0.31372 |  0:03:33s
epoch 104| loss: 0.10648 | val_0_rmse: 0.31847 | val_1_rmse: 0.32208 |  0:03:35s
epoch 105| loss: 0.11826 | val_0_rmse: 0.31696 | val_1_rmse: 0.32167 |  0:03:37s
epoch 106| loss: 0.108   | val_0_rmse: 0.30885 | val_1_rmse: 0.31131 |  0:03:39s
epoch 107| loss: 0.10073 | val_0_rmse: 0.31757 | val_1_rmse: 0.32029 |  0:03:41s
epoch 108| loss: 0.1042  | val_0_rmse: 0.31068 | val_1_rmse: 0.31396 |  0:03:43s
epoch 109| loss: 0.11016 | val_0_rmse: 0.32423 | val_1_rmse: 0.32536 |  0:03:45s
epoch 110| loss: 0.13288 | val_0_rmse: 0.37483 | val_1_rmse: 0.37544 |  0:03:47s
epoch 111| loss: 0.1291  | val_0_rmse: 0.36441 | val_1_rmse: 0.37016 |  0:03:50s
epoch 112| loss: 0.12659 | val_0_rmse: 0.34686 | val_1_rmse: 0.34858 |  0:03:52s
epoch 113| loss: 0.12644 | val_0_rmse: 0.37867 | val_1_rmse: 0.38601 |  0:03:54s
epoch 114| loss: 0.12251 | val_0_rmse: 0.35268 | val_1_rmse: 0.35362 |  0:03:56s
epoch 115| loss: 0.11863 | val_0_rmse: 0.31645 | val_1_rmse: 0.31826 |  0:03:58s
epoch 116| loss: 0.12472 | val_0_rmse: 0.35665 | val_1_rmse: 0.36329 |  0:04:00s
epoch 117| loss: 0.12516 | val_0_rmse: 0.32714 | val_1_rmse: 0.32874 |  0:04:02s
epoch 118| loss: 0.12615 | val_0_rmse: 0.34089 | val_1_rmse: 0.34582 |  0:04:04s
epoch 119| loss: 0.12265 | val_0_rmse: 0.32219 | val_1_rmse: 0.32528 |  0:04:06s
epoch 120| loss: 0.12405 | val_0_rmse: 0.36455 | val_1_rmse: 0.37007 |  0:04:08s
epoch 121| loss: 0.12595 | val_0_rmse: 0.3194  | val_1_rmse: 0.32389 |  0:04:10s
epoch 122| loss: 0.10561 | val_0_rmse: 0.34065 | val_1_rmse: 0.34708 |  0:04:12s
epoch 123| loss: 0.10543 | val_0_rmse: 0.32655 | val_1_rmse: 0.33109 |  0:04:14s
epoch 124| loss: 0.10098 | val_0_rmse: 0.31474 | val_1_rmse: 0.31822 |  0:04:16s
epoch 125| loss: 0.09956 | val_0_rmse: 0.32795 | val_1_rmse: 0.33382 |  0:04:18s
epoch 126| loss: 0.11233 | val_0_rmse: 0.34202 | val_1_rmse: 0.34461 |  0:04:20s
epoch 127| loss: 0.12283 | val_0_rmse: 0.36231 | val_1_rmse: 0.36377 |  0:04:22s
epoch 128| loss: 0.11202 | val_0_rmse: 0.34081 | val_1_rmse: 0.34441 |  0:04:24s
epoch 129| loss: 0.10204 | val_0_rmse: 0.34013 | val_1_rmse: 0.34456 |  0:04:26s
epoch 130| loss: 0.10459 | val_0_rmse: 0.31888 | val_1_rmse: 0.32507 |  0:04:29s
epoch 131| loss: 0.10578 | val_0_rmse: 0.32298 | val_1_rmse: 0.32551 |  0:04:31s
epoch 132| loss: 0.10245 | val_0_rmse: 0.33133 | val_1_rmse: 0.33796 |  0:04:33s
epoch 133| loss: 0.14044 | val_0_rmse: 0.31441 | val_1_rmse: 0.31567 |  0:04:35s
epoch 134| loss: 0.10143 | val_0_rmse: 0.32494 | val_1_rmse: 0.32855 |  0:04:37s
epoch 135| loss: 0.10542 | val_0_rmse: 0.33854 | val_1_rmse: 0.34114 |  0:04:39s
epoch 136| loss: 0.10635 | val_0_rmse: 0.3342  | val_1_rmse: 0.3416  |  0:04:41s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.31131
Best weights from best epoch are automatically used!
ended training at: 17:24:01
Feature importance:
[('Area', 0.2995853512373257), ('Baths', 0.2350480320700992), ('Beds', 0.10176259309867759), ('Latitude', 0.0387863512044777), ('Longitude', 0.2591953553849466), ('Month', 0.044721925904738016), ('Year', 0.020900391099735197)]
Mean squared error is of 1117120518.8521872
Mean absolute error:22169.448857342435
MAPE:0.2508205166766937
R2 score:0.7207763741275217
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:24:02
epoch 0  | loss: 25.80503| val_0_rmse: 12.77651| val_1_rmse: 12.82479|  0:00:02s
epoch 1  | loss: 0.4403  | val_0_rmse: 1.18694 | val_1_rmse: 1.18713 |  0:00:04s
epoch 2  | loss: 0.25867 | val_0_rmse: 1.32105 | val_1_rmse: 1.32562 |  0:00:06s
epoch 3  | loss: 0.2114  | val_0_rmse: 1.06572 | val_1_rmse: 1.06818 |  0:00:08s
epoch 4  | loss: 0.21905 | val_0_rmse: 1.13783 | val_1_rmse: 1.14067 |  0:00:10s
epoch 5  | loss: 0.20501 | val_0_rmse: 1.04005 | val_1_rmse: 1.04142 |  0:00:12s
epoch 6  | loss: 0.2636  | val_0_rmse: 0.81413 | val_1_rmse: 0.82167 |  0:00:14s
epoch 7  | loss: 0.18025 | val_0_rmse: 0.52195 | val_1_rmse: 0.52725 |  0:00:16s
epoch 8  | loss: 0.1572  | val_0_rmse: 0.51756 | val_1_rmse: 0.52306 |  0:00:18s
epoch 9  | loss: 0.14601 | val_0_rmse: 0.65548 | val_1_rmse: 0.66064 |  0:00:20s
epoch 10 | loss: 0.20918 | val_0_rmse: 0.53508 | val_1_rmse: 0.5372  |  0:00:22s
epoch 11 | loss: 0.13825 | val_0_rmse: 0.48546 | val_1_rmse: 0.48803 |  0:00:24s
epoch 12 | loss: 0.13296 | val_0_rmse: 0.56517 | val_1_rmse: 0.56779 |  0:00:26s
epoch 13 | loss: 0.15891 | val_0_rmse: 0.65564 | val_1_rmse: 0.65881 |  0:00:28s
epoch 14 | loss: 0.15926 | val_0_rmse: 0.56496 | val_1_rmse: 0.56677 |  0:00:30s
epoch 15 | loss: 0.14908 | val_0_rmse: 0.59397 | val_1_rmse: 0.59298 |  0:00:32s
epoch 16 | loss: 0.15468 | val_0_rmse: 0.52719 | val_1_rmse: 0.52912 |  0:00:35s
epoch 17 | loss: 0.14853 | val_0_rmse: 0.49281 | val_1_rmse: 0.49526 |  0:00:37s
epoch 18 | loss: 0.13372 | val_0_rmse: 0.48014 | val_1_rmse: 0.47993 |  0:00:39s
epoch 19 | loss: 0.15853 | val_0_rmse: 0.51108 | val_1_rmse: 0.50986 |  0:00:41s
epoch 20 | loss: 0.1528  | val_0_rmse: 0.44869 | val_1_rmse: 0.45015 |  0:00:43s
epoch 21 | loss: 0.15165 | val_0_rmse: 0.51332 | val_1_rmse: 0.51254 |  0:00:45s
epoch 22 | loss: 0.15049 | val_0_rmse: 0.47615 | val_1_rmse: 0.47677 |  0:00:47s
epoch 23 | loss: 0.14622 | val_0_rmse: 0.54189 | val_1_rmse: 0.54306 |  0:00:49s
epoch 24 | loss: 0.14762 | val_0_rmse: 0.49806 | val_1_rmse: 0.49682 |  0:00:51s
epoch 25 | loss: 0.14775 | val_0_rmse: 0.54773 | val_1_rmse: 0.54849 |  0:00:53s
epoch 26 | loss: 0.13428 | val_0_rmse: 0.53576 | val_1_rmse: 0.5364  |  0:00:55s
epoch 27 | loss: 0.14677 | val_0_rmse: 0.46932 | val_1_rmse: 0.47006 |  0:00:57s
epoch 28 | loss: 0.13832 | val_0_rmse: 0.42598 | val_1_rmse: 0.42668 |  0:00:59s
epoch 29 | loss: 0.146   | val_0_rmse: 0.42451 | val_1_rmse: 0.4262  |  0:01:01s
epoch 30 | loss: 0.18558 | val_0_rmse: 0.40497 | val_1_rmse: 0.40356 |  0:01:04s
epoch 31 | loss: 0.15175 | val_0_rmse: 0.39963 | val_1_rmse: 0.39656 |  0:01:06s
epoch 32 | loss: 0.13743 | val_0_rmse: 0.33966 | val_1_rmse: 0.33502 |  0:01:08s
epoch 33 | loss: 0.13119 | val_0_rmse: 0.35447 | val_1_rmse: 0.35095 |  0:01:10s
epoch 34 | loss: 0.13183 | val_0_rmse: 0.40908 | val_1_rmse: 0.40348 |  0:01:12s
epoch 35 | loss: 0.14409 | val_0_rmse: 0.35625 | val_1_rmse: 0.35008 |  0:01:14s
epoch 36 | loss: 0.12469 | val_0_rmse: 0.34327 | val_1_rmse: 0.34114 |  0:01:16s
epoch 37 | loss: 0.12404 | val_0_rmse: 0.3581  | val_1_rmse: 0.35313 |  0:01:18s
epoch 38 | loss: 0.12769 | val_0_rmse: 0.33094 | val_1_rmse: 0.32545 |  0:01:20s
epoch 39 | loss: 0.11996 | val_0_rmse: 0.35749 | val_1_rmse: 0.35144 |  0:01:22s
epoch 40 | loss: 0.12992 | val_0_rmse: 0.32911 | val_1_rmse: 0.32655 |  0:01:24s
epoch 41 | loss: 0.13048 | val_0_rmse: 0.34092 | val_1_rmse: 0.33977 |  0:01:26s
epoch 42 | loss: 0.126   | val_0_rmse: 0.3453  | val_1_rmse: 0.33917 |  0:01:28s
epoch 43 | loss: 0.11862 | val_0_rmse: 0.33972 | val_1_rmse: 0.33651 |  0:01:30s
epoch 44 | loss: 0.11999 | val_0_rmse: 0.35792 | val_1_rmse: 0.35452 |  0:01:32s
epoch 45 | loss: 0.11751 | val_0_rmse: 0.31875 | val_1_rmse: 0.31448 |  0:01:34s
epoch 46 | loss: 0.13086 | val_0_rmse: 0.34017 | val_1_rmse: 0.33513 |  0:01:36s
epoch 47 | loss: 0.12207 | val_0_rmse: 0.36398 | val_1_rmse: 0.35874 |  0:01:38s
epoch 48 | loss: 0.12657 | val_0_rmse: 0.3188  | val_1_rmse: 0.31196 |  0:01:40s
epoch 49 | loss: 0.11562 | val_0_rmse: 0.33341 | val_1_rmse: 0.32734 |  0:01:42s
epoch 50 | loss: 0.13342 | val_0_rmse: 0.36395 | val_1_rmse: 0.35765 |  0:01:45s
epoch 51 | loss: 0.14075 | val_0_rmse: 0.35972 | val_1_rmse: 0.35323 |  0:01:47s
epoch 52 | loss: 0.13715 | val_0_rmse: 0.35358 | val_1_rmse: 0.34951 |  0:01:49s
epoch 53 | loss: 0.13704 | val_0_rmse: 0.33346 | val_1_rmse: 0.32674 |  0:01:51s
epoch 54 | loss: 0.13474 | val_0_rmse: 0.34446 | val_1_rmse: 0.34073 |  0:01:53s
epoch 55 | loss: 0.13543 | val_0_rmse: 0.34274 | val_1_rmse: 0.33817 |  0:01:55s
epoch 56 | loss: 0.13439 | val_0_rmse: 0.33366 | val_1_rmse: 0.33041 |  0:01:57s
epoch 57 | loss: 0.1338  | val_0_rmse: 0.37151 | val_1_rmse: 0.36576 |  0:01:59s
epoch 58 | loss: 0.1284  | val_0_rmse: 0.32336 | val_1_rmse: 0.31935 |  0:02:01s
epoch 59 | loss: 0.12936 | val_0_rmse: 0.33567 | val_1_rmse: 0.33218 |  0:02:03s
epoch 60 | loss: 0.12575 | val_0_rmse: 0.33451 | val_1_rmse: 0.32783 |  0:02:05s
epoch 61 | loss: 0.11141 | val_0_rmse: 0.32626 | val_1_rmse: 0.32266 |  0:02:07s
epoch 62 | loss: 0.13054 | val_0_rmse: 0.39921 | val_1_rmse: 0.3971  |  0:02:09s
epoch 63 | loss: 0.1919  | val_0_rmse: 0.31292 | val_1_rmse: 0.30847 |  0:02:11s
epoch 64 | loss: 0.11196 | val_0_rmse: 0.32414 | val_1_rmse: 0.32128 |  0:02:14s
epoch 65 | loss: 0.13915 | val_0_rmse: 0.31174 | val_1_rmse: 0.30736 |  0:02:16s
epoch 66 | loss: 0.13543 | val_0_rmse: 0.33443 | val_1_rmse: 0.33006 |  0:02:18s
epoch 67 | loss: 0.1319  | val_0_rmse: 0.32515 | val_1_rmse: 0.32047 |  0:02:20s
epoch 68 | loss: 0.13066 | val_0_rmse: 0.34204 | val_1_rmse: 0.33862 |  0:02:22s
epoch 69 | loss: 0.1174  | val_0_rmse: 0.31716 | val_1_rmse: 0.31186 |  0:02:24s
epoch 70 | loss: 0.10826 | val_0_rmse: 0.34447 | val_1_rmse: 0.33831 |  0:02:26s
epoch 71 | loss: 0.10801 | val_0_rmse: 0.33319 | val_1_rmse: 0.33237 |  0:02:28s
epoch 72 | loss: 0.10723 | val_0_rmse: 0.31792 | val_1_rmse: 0.3155  |  0:02:30s
epoch 73 | loss: 0.13563 | val_0_rmse: 0.35008 | val_1_rmse: 0.34498 |  0:02:32s
epoch 74 | loss: 0.12336 | val_0_rmse: 0.31876 | val_1_rmse: 0.31499 |  0:02:34s
epoch 75 | loss: 0.11213 | val_0_rmse: 0.34233 | val_1_rmse: 0.33941 |  0:02:36s
epoch 76 | loss: 0.11669 | val_0_rmse: 0.33022 | val_1_rmse: 0.32538 |  0:02:38s
epoch 77 | loss: 0.11028 | val_0_rmse: 0.32089 | val_1_rmse: 0.31728 |  0:02:40s
epoch 78 | loss: 0.13479 | val_0_rmse: 0.32043 | val_1_rmse: 0.31668 |  0:02:42s
epoch 79 | loss: 0.11737 | val_0_rmse: 0.3427  | val_1_rmse: 0.3395  |  0:02:44s
epoch 80 | loss: 0.11482 | val_0_rmse: 0.33158 | val_1_rmse: 0.32603 |  0:02:46s
epoch 81 | loss: 0.10629 | val_0_rmse: 0.31543 | val_1_rmse: 0.30942 |  0:02:48s
epoch 82 | loss: 0.11136 | val_0_rmse: 0.34117 | val_1_rmse: 0.33489 |  0:02:51s
epoch 83 | loss: 0.10966 | val_0_rmse: 0.32226 | val_1_rmse: 0.31829 |  0:02:53s
epoch 84 | loss: 0.12505 | val_0_rmse: 0.33049 | val_1_rmse: 0.32774 |  0:02:55s
epoch 85 | loss: 0.13824 | val_0_rmse: 0.32257 | val_1_rmse: 0.31722 |  0:02:57s
epoch 86 | loss: 0.12598 | val_0_rmse: 0.3228  | val_1_rmse: 0.32012 |  0:02:59s
epoch 87 | loss: 0.12388 | val_0_rmse: 0.3275  | val_1_rmse: 0.32306 |  0:03:01s
epoch 88 | loss: 0.10585 | val_0_rmse: 0.35591 | val_1_rmse: 0.35273 |  0:03:03s
epoch 89 | loss: 0.12309 | val_0_rmse: 0.31319 | val_1_rmse: 0.30796 |  0:03:05s
epoch 90 | loss: 0.10582 | val_0_rmse: 0.33412 | val_1_rmse: 0.332   |  0:03:07s
epoch 91 | loss: 0.1126  | val_0_rmse: 0.32119 | val_1_rmse: 0.31632 |  0:03:09s
epoch 92 | loss: 0.11677 | val_0_rmse: 0.31348 | val_1_rmse: 0.30957 |  0:03:11s
epoch 93 | loss: 0.1037  | val_0_rmse: 0.31352 | val_1_rmse: 0.30996 |  0:03:13s
epoch 94 | loss: 0.10516 | val_0_rmse: 0.32761 | val_1_rmse: 0.32409 |  0:03:15s
epoch 95 | loss: 0.10887 | val_0_rmse: 0.30797 | val_1_rmse: 0.30334 |  0:03:17s
epoch 96 | loss: 0.10682 | val_0_rmse: 0.31062 | val_1_rmse: 0.30513 |  0:03:19s
epoch 97 | loss: 0.11507 | val_0_rmse: 0.42402 | val_1_rmse: 0.42198 |  0:03:21s
epoch 98 | loss: 0.13303 | val_0_rmse: 0.33788 | val_1_rmse: 0.33503 |  0:03:23s
epoch 99 | loss: 0.12633 | val_0_rmse: 0.32765 | val_1_rmse: 0.32295 |  0:03:25s
epoch 100| loss: 0.12951 | val_0_rmse: 0.34877 | val_1_rmse: 0.34393 |  0:03:27s
epoch 101| loss: 0.12996 | val_0_rmse: 0.32301 | val_1_rmse: 0.3192  |  0:03:29s
epoch 102| loss: 0.12706 | val_0_rmse: 0.31178 | val_1_rmse: 0.30643 |  0:03:31s
epoch 103| loss: 0.12544 | val_0_rmse: 0.31137 | val_1_rmse: 0.30611 |  0:03:34s
epoch 104| loss: 0.15153 | val_0_rmse: 0.32345 | val_1_rmse: 0.31879 |  0:03:36s
epoch 105| loss: 0.14398 | val_0_rmse: 0.34658 | val_1_rmse: 0.34192 |  0:03:38s
epoch 106| loss: 0.11898 | val_0_rmse: 0.31303 | val_1_rmse: 0.30726 |  0:03:40s
epoch 107| loss: 0.10256 | val_0_rmse: 0.32311 | val_1_rmse: 0.32017 |  0:03:42s
epoch 108| loss: 0.10615 | val_0_rmse: 0.31276 | val_1_rmse: 0.30857 |  0:03:44s
epoch 109| loss: 0.10812 | val_0_rmse: 0.31428 | val_1_rmse: 0.31081 |  0:03:46s
epoch 110| loss: 0.11949 | val_0_rmse: 0.3329  | val_1_rmse: 0.32929 |  0:03:48s
epoch 111| loss: 0.1046  | val_0_rmse: 0.30777 | val_1_rmse: 0.30539 |  0:03:50s
epoch 112| loss: 0.10325 | val_0_rmse: 0.31784 | val_1_rmse: 0.31422 |  0:03:52s
epoch 113| loss: 0.10482 | val_0_rmse: 0.33261 | val_1_rmse: 0.32953 |  0:03:54s
epoch 114| loss: 0.10567 | val_0_rmse: 0.31246 | val_1_rmse: 0.30616 |  0:03:56s
epoch 115| loss: 0.10767 | val_0_rmse: 0.31124 | val_1_rmse: 0.30856 |  0:03:58s
epoch 116| loss: 0.10191 | val_0_rmse: 0.30845 | val_1_rmse: 0.30396 |  0:04:00s
epoch 117| loss: 0.10347 | val_0_rmse: 0.31138 | val_1_rmse: 0.30839 |  0:04:02s
epoch 118| loss: 0.10806 | val_0_rmse: 0.34768 | val_1_rmse: 0.34366 |  0:04:05s
epoch 119| loss: 0.1004  | val_0_rmse: 0.34425 | val_1_rmse: 0.3413  |  0:04:07s
epoch 120| loss: 0.13117 | val_0_rmse: 0.31928 | val_1_rmse: 0.31397 |  0:04:09s
epoch 121| loss: 0.1194  | val_0_rmse: 0.32768 | val_1_rmse: 0.32474 |  0:04:11s
epoch 122| loss: 0.10457 | val_0_rmse: 0.30858 | val_1_rmse: 0.30376 |  0:04:13s
epoch 123| loss: 0.10475 | val_0_rmse: 0.34751 | val_1_rmse: 0.34408 |  0:04:15s
epoch 124| loss: 0.12263 | val_0_rmse: 0.31191 | val_1_rmse: 0.30719 |  0:04:17s
epoch 125| loss: 0.09933 | val_0_rmse: 0.31778 | val_1_rmse: 0.31402 |  0:04:19s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.30334
Best weights from best epoch are automatically used!
ended training at: 17:28:22
Feature importance:
[('Area', 0.27941955296277127), ('Baths', 0.1647553819052914), ('Beds', 0.03653589821660441), ('Latitude', 0.13941350302182082), ('Longitude', 0.29512011483505396), ('Month', 0.08475371643461167), ('Year', 1.832623846473831e-06)]
Mean squared error is of 1119443824.5960078
Mean absolute error:22200.57175029544
MAPE:0.25077470618588293
R2 score:0.7258415066793726
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:28:22
epoch 0  | loss: 24.47229| val_0_rmse: 5.99522 | val_1_rmse: 5.99268 |  0:00:02s
epoch 1  | loss: 0.29844 | val_0_rmse: 1.10541 | val_1_rmse: 1.10169 |  0:00:04s
epoch 2  | loss: 0.24452 | val_0_rmse: 0.77093 | val_1_rmse: 0.76466 |  0:00:06s
epoch 3  | loss: 0.2368  | val_0_rmse: 0.66752 | val_1_rmse: 0.67286 |  0:00:08s
epoch 4  | loss: 0.20777 | val_0_rmse: 0.6712  | val_1_rmse: 0.66882 |  0:00:10s
epoch 5  | loss: 0.19941 | val_0_rmse: 0.76497 | val_1_rmse: 0.76178 |  0:00:12s
epoch 6  | loss: 0.17893 | val_0_rmse: 0.6048  | val_1_rmse: 0.60373 |  0:00:14s
epoch 7  | loss: 0.15651 | val_0_rmse: 0.68785 | val_1_rmse: 0.6894  |  0:00:16s
epoch 8  | loss: 0.15463 | val_0_rmse: 0.53058 | val_1_rmse: 0.53357 |  0:00:18s
epoch 9  | loss: 0.18009 | val_0_rmse: 0.4423  | val_1_rmse: 0.44369 |  0:00:20s
epoch 10 | loss: 0.21954 | val_0_rmse: 0.54537 | val_1_rmse: 0.5495  |  0:00:22s
epoch 11 | loss: 0.16386 | val_0_rmse: 0.52891 | val_1_rmse: 0.52758 |  0:00:24s
epoch 12 | loss: 0.14228 | val_0_rmse: 0.53714 | val_1_rmse: 0.53492 |  0:00:26s
epoch 13 | loss: 0.16707 | val_0_rmse: 0.62267 | val_1_rmse: 0.62454 |  0:00:28s
epoch 14 | loss: 0.14628 | val_0_rmse: 0.52355 | val_1_rmse: 0.52464 |  0:00:30s
epoch 15 | loss: 0.16189 | val_0_rmse: 0.54398 | val_1_rmse: 0.54273 |  0:00:32s
epoch 16 | loss: 0.16169 | val_0_rmse: 0.50745 | val_1_rmse: 0.50848 |  0:00:34s
epoch 17 | loss: 0.13195 | val_0_rmse: 0.53417 | val_1_rmse: 0.53324 |  0:00:37s
epoch 18 | loss: 0.19177 | val_0_rmse: 0.63544 | val_1_rmse: 0.63604 |  0:00:39s
epoch 19 | loss: 0.17337 | val_0_rmse: 0.47557 | val_1_rmse: 0.47845 |  0:00:41s
epoch 20 | loss: 0.14846 | val_0_rmse: 0.45215 | val_1_rmse: 0.45342 |  0:00:43s
epoch 21 | loss: 0.15899 | val_0_rmse: 0.54044 | val_1_rmse: 0.54147 |  0:00:45s
epoch 22 | loss: 0.13032 | val_0_rmse: 0.45434 | val_1_rmse: 0.45579 |  0:00:47s
epoch 23 | loss: 0.14243 | val_0_rmse: 0.54632 | val_1_rmse: 0.54794 |  0:00:49s
epoch 24 | loss: 0.14294 | val_0_rmse: 0.46234 | val_1_rmse: 0.46428 |  0:00:51s
epoch 25 | loss: 0.13043 | val_0_rmse: 0.50203 | val_1_rmse: 0.50273 |  0:00:53s
epoch 26 | loss: 0.12339 | val_0_rmse: 0.59434 | val_1_rmse: 0.59551 |  0:00:55s
epoch 27 | loss: 0.13517 | val_0_rmse: 0.46176 | val_1_rmse: 0.46217 |  0:00:57s
epoch 28 | loss: 0.12703 | val_0_rmse: 0.43951 | val_1_rmse: 0.44335 |  0:00:59s
epoch 29 | loss: 0.12153 | val_0_rmse: 0.47558 | val_1_rmse: 0.4782  |  0:01:01s
epoch 30 | loss: 0.13735 | val_0_rmse: 0.42425 | val_1_rmse: 0.42459 |  0:01:03s
epoch 31 | loss: 0.14384 | val_0_rmse: 0.47134 | val_1_rmse: 0.47534 |  0:01:05s
epoch 32 | loss: 0.12601 | val_0_rmse: 0.35422 | val_1_rmse: 0.35896 |  0:01:07s
epoch 33 | loss: 0.13184 | val_0_rmse: 0.40164 | val_1_rmse: 0.40629 |  0:01:09s
epoch 34 | loss: 0.11619 | val_0_rmse: 0.33815 | val_1_rmse: 0.34419 |  0:01:12s
epoch 35 | loss: 0.124   | val_0_rmse: 0.32093 | val_1_rmse: 0.32614 |  0:01:14s
epoch 36 | loss: 0.11174 | val_0_rmse: 0.36074 | val_1_rmse: 0.36409 |  0:01:16s
epoch 37 | loss: 0.11461 | val_0_rmse: 0.34675 | val_1_rmse: 0.35133 |  0:01:18s
epoch 38 | loss: 0.11224 | val_0_rmse: 0.31888 | val_1_rmse: 0.32323 |  0:01:20s
epoch 39 | loss: 0.1132  | val_0_rmse: 0.32298 | val_1_rmse: 0.32615 |  0:01:22s
epoch 40 | loss: 0.1211  | val_0_rmse: 0.35884 | val_1_rmse: 0.35968 |  0:01:24s
epoch 41 | loss: 0.12581 | val_0_rmse: 0.32776 | val_1_rmse: 0.33269 |  0:01:26s
epoch 42 | loss: 0.17458 | val_0_rmse: 0.34256 | val_1_rmse: 0.34561 |  0:01:28s
epoch 43 | loss: 0.11034 | val_0_rmse: 0.33064 | val_1_rmse: 0.33512 |  0:01:30s
epoch 44 | loss: 0.1074  | val_0_rmse: 0.33186 | val_1_rmse: 0.3382  |  0:01:32s
epoch 45 | loss: 0.11478 | val_0_rmse: 0.31757 | val_1_rmse: 0.32149 |  0:01:34s
epoch 46 | loss: 0.11818 | val_0_rmse: 0.36865 | val_1_rmse: 0.37185 |  0:01:36s
epoch 47 | loss: 0.13447 | val_0_rmse: 0.3644  | val_1_rmse: 0.36782 |  0:01:38s
epoch 48 | loss: 0.12437 | val_0_rmse: 0.34166 | val_1_rmse: 0.34853 |  0:01:40s
epoch 49 | loss: 0.11156 | val_0_rmse: 0.31286 | val_1_rmse: 0.31646 |  0:01:42s
epoch 50 | loss: 0.13071 | val_0_rmse: 0.36691 | val_1_rmse: 0.36947 |  0:01:44s
epoch 51 | loss: 0.11373 | val_0_rmse: 0.30783 | val_1_rmse: 0.31207 |  0:01:46s
epoch 52 | loss: 0.11834 | val_0_rmse: 0.35363 | val_1_rmse: 0.35862 |  0:01:49s
epoch 53 | loss: 0.13152 | val_0_rmse: 0.32787 | val_1_rmse: 0.33245 |  0:01:51s
epoch 54 | loss: 0.17081 | val_0_rmse: 0.44134 | val_1_rmse: 0.44629 |  0:01:53s
epoch 55 | loss: 0.13168 | val_0_rmse: 0.3368  | val_1_rmse: 0.34139 |  0:01:55s
epoch 56 | loss: 0.11537 | val_0_rmse: 0.31656 | val_1_rmse: 0.32328 |  0:01:57s
epoch 57 | loss: 0.11232 | val_0_rmse: 0.38851 | val_1_rmse: 0.39459 |  0:01:59s
epoch 58 | loss: 0.11696 | val_0_rmse: 0.33008 | val_1_rmse: 0.3334  |  0:02:01s
epoch 59 | loss: 0.13172 | val_0_rmse: 0.3278  | val_1_rmse: 0.33244 |  0:02:03s
epoch 60 | loss: 0.12154 | val_0_rmse: 0.32229 | val_1_rmse: 0.32517 |  0:02:05s
epoch 61 | loss: 0.12792 | val_0_rmse: 0.4629  | val_1_rmse: 0.46696 |  0:02:07s
epoch 62 | loss: 0.15109 | val_0_rmse: 0.56373 | val_1_rmse: 0.564   |  0:02:09s
epoch 63 | loss: 0.20817 | val_0_rmse: 0.333   | val_1_rmse: 0.33727 |  0:02:11s
epoch 64 | loss: 0.18304 | val_0_rmse: 0.47444 | val_1_rmse: 0.47743 |  0:02:13s
epoch 65 | loss: 0.18051 | val_0_rmse: 0.40378 | val_1_rmse: 0.40766 |  0:02:15s
epoch 66 | loss: 0.12861 | val_0_rmse: 0.38831 | val_1_rmse: 0.39215 |  0:02:17s
epoch 67 | loss: 0.13102 | val_0_rmse: 0.41375 | val_1_rmse: 0.41727 |  0:02:19s
epoch 68 | loss: 0.12734 | val_0_rmse: 0.4154  | val_1_rmse: 0.41935 |  0:02:21s
epoch 69 | loss: 0.12953 | val_0_rmse: 0.34808 | val_1_rmse: 0.35087 |  0:02:24s
epoch 70 | loss: 0.12845 | val_0_rmse: 0.3732  | val_1_rmse: 0.37724 |  0:02:26s
epoch 71 | loss: 0.12569 | val_0_rmse: 0.35967 | val_1_rmse: 0.36277 |  0:02:28s
epoch 72 | loss: 0.12679 | val_0_rmse: 0.36628 | val_1_rmse: 0.37237 |  0:02:30s
epoch 73 | loss: 0.1245  | val_0_rmse: 0.34563 | val_1_rmse: 0.34983 |  0:02:32s
epoch 74 | loss: 0.12299 | val_0_rmse: 0.42933 | val_1_rmse: 0.43296 |  0:02:34s
epoch 75 | loss: 0.12488 | val_0_rmse: 0.37711 | val_1_rmse: 0.37857 |  0:02:36s
epoch 76 | loss: 0.12814 | val_0_rmse: 0.43478 | val_1_rmse: 0.43849 |  0:02:38s
epoch 77 | loss: 0.12583 | val_0_rmse: 0.30849 | val_1_rmse: 0.31171 |  0:02:40s
epoch 78 | loss: 0.10551 | val_0_rmse: 0.33408 | val_1_rmse: 0.33845 |  0:02:42s
epoch 79 | loss: 0.10572 | val_0_rmse: 0.33429 | val_1_rmse: 0.33798 |  0:02:44s
epoch 80 | loss: 0.10617 | val_0_rmse: 0.36122 | val_1_rmse: 0.36845 |  0:02:46s
epoch 81 | loss: 0.10513 | val_0_rmse: 0.35659 | val_1_rmse: 0.36054 |  0:02:48s
epoch 82 | loss: 0.12574 | val_0_rmse: 0.32852 | val_1_rmse: 0.33239 |  0:02:50s
epoch 83 | loss: 0.10185 | val_0_rmse: 0.32199 | val_1_rmse: 0.32618 |  0:02:52s
epoch 84 | loss: 0.10163 | val_0_rmse: 0.33013 | val_1_rmse: 0.3326  |  0:02:54s
epoch 85 | loss: 0.10935 | val_0_rmse: 0.32681 | val_1_rmse: 0.33187 |  0:02:56s
epoch 86 | loss: 0.10284 | val_0_rmse: 0.4529  | val_1_rmse: 0.448   |  0:02:58s
epoch 87 | loss: 0.10926 | val_0_rmse: 0.37177 | val_1_rmse: 0.37848 |  0:03:00s
epoch 88 | loss: 0.12775 | val_0_rmse: 0.38337 | val_1_rmse: 0.38783 |  0:03:03s
epoch 89 | loss: 0.13021 | val_0_rmse: 0.32209 | val_1_rmse: 0.32758 |  0:03:05s
epoch 90 | loss: 0.13042 | val_0_rmse: 0.34137 | val_1_rmse: 0.34745 |  0:03:07s
epoch 91 | loss: 0.12574 | val_0_rmse: 0.33823 | val_1_rmse: 0.34158 |  0:03:09s
epoch 92 | loss: 0.12466 | val_0_rmse: 0.38081 | val_1_rmse: 0.38561 |  0:03:11s
epoch 93 | loss: 0.11356 | val_0_rmse: 0.31586 | val_1_rmse: 0.31978 |  0:03:13s
epoch 94 | loss: 0.10571 | val_0_rmse: 0.34891 | val_1_rmse: 0.35073 |  0:03:15s
epoch 95 | loss: 0.10635 | val_0_rmse: 0.33507 | val_1_rmse: 0.33746 |  0:03:17s
epoch 96 | loss: 0.12495 | val_0_rmse: 0.34293 | val_1_rmse: 0.34757 |  0:03:19s
epoch 97 | loss: 0.12931 | val_0_rmse: 0.36926 | val_1_rmse: 0.37271 |  0:03:21s
epoch 98 | loss: 0.12951 | val_0_rmse: 0.33823 | val_1_rmse: 0.34088 |  0:03:23s
epoch 99 | loss: 0.10939 | val_0_rmse: 0.34202 | val_1_rmse: 0.34617 |  0:03:25s
epoch 100| loss: 0.1051  | val_0_rmse: 0.32268 | val_1_rmse: 0.32429 |  0:03:27s
epoch 101| loss: 0.11235 | val_0_rmse: 0.32251 | val_1_rmse: 0.32777 |  0:03:29s
epoch 102| loss: 0.10328 | val_0_rmse: 0.3628  | val_1_rmse: 0.36785 |  0:03:31s
epoch 103| loss: 0.10903 | val_0_rmse: 0.3506  | val_1_rmse: 0.35224 |  0:03:33s
epoch 104| loss: 0.12361 | val_0_rmse: 0.33177 | val_1_rmse: 0.3336  |  0:03:35s
epoch 105| loss: 0.10203 | val_0_rmse: 0.34965 | val_1_rmse: 0.35443 |  0:03:37s
epoch 106| loss: 0.14336 | val_0_rmse: 0.34476 | val_1_rmse: 0.34739 |  0:03:39s
epoch 107| loss: 0.13171 | val_0_rmse: 0.35761 | val_1_rmse: 0.35921 |  0:03:42s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.31171
Best weights from best epoch are automatically used!
ended training at: 17:32:05
Feature importance:
[('Area', 0.258570674619015), ('Baths', 0.11815260418963601), ('Beds', 0.0), ('Latitude', 0.33467878399832257), ('Longitude', 0.07981206487408887), ('Month', 0.05644105891495531), ('Year', 0.15234481340398226)]
Mean squared error is of 1057904774.4726533
Mean absolute error:21726.653890391066
MAPE:0.25057016413671185
R2 score:0.7377485278972806
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:32:30
epoch 0  | loss: 21.60193| val_0_rmse: 6.54344 | val_1_rmse: 6.55733 |  0:00:03s
epoch 1  | loss: 0.38119 | val_0_rmse: 9.41155 | val_1_rmse: 9.42766 |  0:00:06s
epoch 2  | loss: 0.31621 | val_0_rmse: 6.84359 | val_1_rmse: 6.85487 |  0:00:09s
epoch 3  | loss: 0.27713 | val_0_rmse: 3.91143 | val_1_rmse: 3.93411 |  0:00:13s
epoch 4  | loss: 0.25872 | val_0_rmse: 3.20487 | val_1_rmse: 3.22414 |  0:00:16s
epoch 5  | loss: 0.24689 | val_0_rmse: 1.77736 | val_1_rmse: 1.79423 |  0:00:19s
epoch 6  | loss: 0.21596 | val_0_rmse: 1.3643  | val_1_rmse: 1.38157 |  0:00:23s
epoch 7  | loss: 0.22891 | val_0_rmse: 1.37085 | val_1_rmse: 1.39029 |  0:00:26s
epoch 8  | loss: 0.20069 | val_0_rmse: 1.0394  | val_1_rmse: 1.06051 |  0:00:29s
epoch 9  | loss: 0.18196 | val_0_rmse: 0.79433 | val_1_rmse: 0.81494 |  0:00:33s
epoch 10 | loss: 0.18224 | val_0_rmse: 0.82012 | val_1_rmse: 0.83825 |  0:00:36s
epoch 11 | loss: 0.1761  | val_0_rmse: 0.92895 | val_1_rmse: 0.94839 |  0:00:39s
epoch 12 | loss: 0.18345 | val_0_rmse: 0.98741 | val_1_rmse: 1.00751 |  0:00:42s
epoch 13 | loss: 0.19993 | val_0_rmse: 0.88459 | val_1_rmse: 0.90493 |  0:00:46s
epoch 14 | loss: 0.16157 | val_0_rmse: 0.80986 | val_1_rmse: 0.82802 |  0:00:49s
epoch 15 | loss: 0.18272 | val_0_rmse: 0.78622 | val_1_rmse: 0.80329 |  0:00:52s
epoch 16 | loss: 0.16357 | val_0_rmse: 0.79058 | val_1_rmse: 0.80694 |  0:00:56s
epoch 17 | loss: 0.16472 | val_0_rmse: 0.69753 | val_1_rmse: 0.71103 |  0:00:59s
epoch 18 | loss: 0.1654  | val_0_rmse: 0.68847 | val_1_rmse: 0.70279 |  0:01:02s
epoch 19 | loss: 0.17395 | val_0_rmse: 0.66608 | val_1_rmse: 0.67899 |  0:01:06s
epoch 20 | loss: 0.18669 | val_0_rmse: 0.76214 | val_1_rmse: 0.77711 |  0:01:09s
epoch 21 | loss: 0.17959 | val_0_rmse: 0.74687 | val_1_rmse: 0.7601  |  0:01:12s
epoch 22 | loss: 0.17502 | val_0_rmse: 0.67345 | val_1_rmse: 0.68444 |  0:01:15s
epoch 23 | loss: 0.17385 | val_0_rmse: 0.55818 | val_1_rmse: 0.56378 |  0:01:19s
epoch 24 | loss: 0.17314 | val_0_rmse: 0.49354 | val_1_rmse: 0.49681 |  0:01:22s
epoch 25 | loss: 0.17082 | val_0_rmse: 0.46221 | val_1_rmse: 0.46505 |  0:01:25s
epoch 26 | loss: 0.16859 | val_0_rmse: 0.47734 | val_1_rmse: 0.48271 |  0:01:29s
epoch 27 | loss: 0.15225 | val_0_rmse: 0.40698 | val_1_rmse: 0.4143  |  0:01:32s
epoch 28 | loss: 0.1612  | val_0_rmse: 0.37193 | val_1_rmse: 0.38003 |  0:01:35s
epoch 29 | loss: 0.14831 | val_0_rmse: 0.40026 | val_1_rmse: 0.40761 |  0:01:39s
epoch 30 | loss: 0.16229 | val_0_rmse: 0.38551 | val_1_rmse: 0.39298 |  0:01:42s
epoch 31 | loss: 0.17853 | val_0_rmse: 0.3678  | val_1_rmse: 0.3768  |  0:01:45s
epoch 32 | loss: 0.16275 | val_0_rmse: 0.3991  | val_1_rmse: 0.408   |  0:01:49s
epoch 33 | loss: 0.16483 | val_0_rmse: 0.3697  | val_1_rmse: 0.37873 |  0:01:52s
epoch 34 | loss: 0.16178 | val_0_rmse: 0.36975 | val_1_rmse: 0.38005 |  0:01:55s
epoch 35 | loss: 0.16192 | val_0_rmse: 0.39481 | val_1_rmse: 0.40426 |  0:01:58s
epoch 36 | loss: 0.1604  | val_0_rmse: 0.36571 | val_1_rmse: 0.37768 |  0:02:02s
epoch 37 | loss: 0.16079 | val_0_rmse: 0.38156 | val_1_rmse: 0.39175 |  0:02:05s
epoch 38 | loss: 0.16149 | val_0_rmse: 0.39812 | val_1_rmse: 0.40732 |  0:02:08s
epoch 39 | loss: 0.16118 | val_0_rmse: 0.36598 | val_1_rmse: 0.3766  |  0:02:12s
epoch 40 | loss: 0.16383 | val_0_rmse: 0.35963 | val_1_rmse: 0.37011 |  0:02:15s
epoch 41 | loss: 0.1607  | val_0_rmse: 0.39909 | val_1_rmse: 0.40915 |  0:02:18s
epoch 42 | loss: 0.16299 | val_0_rmse: 0.40987 | val_1_rmse: 0.41942 |  0:02:22s
epoch 43 | loss: 0.16566 | val_0_rmse: 0.36223 | val_1_rmse: 0.37358 |  0:02:25s
epoch 44 | loss: 0.1663  | val_0_rmse: 0.35896 | val_1_rmse: 0.36984 |  0:02:28s
epoch 45 | loss: 0.13529 | val_0_rmse: 0.36349 | val_1_rmse: 0.37333 |  0:02:31s
epoch 46 | loss: 0.15004 | val_0_rmse: 0.36658 | val_1_rmse: 0.37612 |  0:02:35s
epoch 47 | loss: 0.19349 | val_0_rmse: 0.36234 | val_1_rmse: 0.37411 |  0:02:38s
epoch 48 | loss: 0.22073 | val_0_rmse: 0.43577 | val_1_rmse: 0.44512 |  0:02:41s
epoch 49 | loss: 0.19923 | val_0_rmse: 0.38098 | val_1_rmse: 0.39356 |  0:02:45s
epoch 50 | loss: 0.19525 | val_0_rmse: 0.38602 | val_1_rmse: 0.3978  |  0:02:48s
epoch 51 | loss: 0.20677 | val_0_rmse: 0.46206 | val_1_rmse: 0.47158 |  0:02:51s
epoch 52 | loss: 0.21736 | val_0_rmse: 0.47453 | val_1_rmse: 0.48495 |  0:02:54s
epoch 53 | loss: 0.2057  | val_0_rmse: 0.38789 | val_1_rmse: 0.39828 |  0:02:58s
epoch 54 | loss: 0.19747 | val_0_rmse: 0.36967 | val_1_rmse: 0.38155 |  0:03:01s
epoch 55 | loss: 0.14376 | val_0_rmse: 0.36161 | val_1_rmse: 0.37287 |  0:03:04s
epoch 56 | loss: 0.14267 | val_0_rmse: 0.34893 | val_1_rmse: 0.36142 |  0:03:08s
epoch 57 | loss: 0.13358 | val_0_rmse: 0.36994 | val_1_rmse: 0.3805  |  0:03:11s
epoch 58 | loss: 0.13489 | val_0_rmse: 0.36012 | val_1_rmse: 0.37505 |  0:03:14s
epoch 59 | loss: 0.15795 | val_0_rmse: 0.43494 | val_1_rmse: 0.44479 |  0:03:18s
epoch 60 | loss: 0.14029 | val_0_rmse: 0.36304 | val_1_rmse: 0.379   |  0:03:21s
epoch 61 | loss: 0.13559 | val_0_rmse: 0.36788 | val_1_rmse: 0.37916 |  0:03:24s
epoch 62 | loss: 0.16618 | val_0_rmse: 0.34812 | val_1_rmse: 0.36285 |  0:03:27s
epoch 63 | loss: 0.13872 | val_0_rmse: 0.35824 | val_1_rmse: 0.37308 |  0:03:31s
epoch 64 | loss: 0.18712 | val_0_rmse: 0.53408 | val_1_rmse: 0.54009 |  0:03:34s
epoch 65 | loss: 0.14832 | val_0_rmse: 0.3456  | val_1_rmse: 0.35596 |  0:03:37s
epoch 66 | loss: 0.17722 | val_0_rmse: 0.35608 | val_1_rmse: 0.36994 |  0:03:41s
epoch 67 | loss: 0.15015 | val_0_rmse: 0.38738 | val_1_rmse: 0.39826 |  0:03:44s
epoch 68 | loss: 0.15261 | val_0_rmse: 0.36056 | val_1_rmse: 0.37166 |  0:03:47s
epoch 69 | loss: 0.15183 | val_0_rmse: 0.34503 | val_1_rmse: 0.35976 |  0:03:51s
epoch 70 | loss: 0.14947 | val_0_rmse: 0.36061 | val_1_rmse: 0.3752  |  0:03:54s
epoch 71 | loss: 0.14249 | val_0_rmse: 0.34636 | val_1_rmse: 0.3603  |  0:03:57s
epoch 72 | loss: 0.13072 | val_0_rmse: 0.35713 | val_1_rmse: 0.37042 |  0:04:00s
epoch 73 | loss: 0.13435 | val_0_rmse: 0.34208 | val_1_rmse: 0.35561 |  0:04:04s
epoch 74 | loss: 0.14104 | val_0_rmse: 0.37484 | val_1_rmse: 0.38626 |  0:04:07s
epoch 75 | loss: 0.16439 | val_0_rmse: 0.35388 | val_1_rmse: 0.36797 |  0:04:10s
epoch 76 | loss: 0.12996 | val_0_rmse: 0.33201 | val_1_rmse: 0.34667 |  0:04:14s
epoch 77 | loss: 0.13861 | val_0_rmse: 0.35522 | val_1_rmse: 0.36794 |  0:04:17s
epoch 78 | loss: 0.1387  | val_0_rmse: 0.3521  | val_1_rmse: 0.36743 |  0:04:20s
epoch 79 | loss: 0.12939 | val_0_rmse: 0.34099 | val_1_rmse: 0.35717 |  0:04:24s
epoch 80 | loss: 0.162   | val_0_rmse: 0.35484 | val_1_rmse: 0.37045 |  0:04:27s
epoch 81 | loss: 0.13138 | val_0_rmse: 0.35217 | val_1_rmse: 0.36419 |  0:04:30s
epoch 82 | loss: 0.14405 | val_0_rmse: 0.34704 | val_1_rmse: 0.35996 |  0:04:33s
epoch 83 | loss: 0.14028 | val_0_rmse: 0.35287 | val_1_rmse: 0.36614 |  0:04:37s
epoch 84 | loss: 0.14155 | val_0_rmse: 0.33598 | val_1_rmse: 0.3503  |  0:04:40s
epoch 85 | loss: 0.13537 | val_0_rmse: 0.35294 | val_1_rmse: 0.36847 |  0:04:43s
epoch 86 | loss: 0.146   | val_0_rmse: 0.33761 | val_1_rmse: 0.35178 |  0:04:46s
epoch 87 | loss: 0.19717 | val_0_rmse: 0.3392  | val_1_rmse: 0.35354 |  0:04:50s
epoch 88 | loss: 0.20235 | val_0_rmse: 0.33553 | val_1_rmse: 0.34877 |  0:04:53s
epoch 89 | loss: 0.14997 | val_0_rmse: 0.4569  | val_1_rmse: 0.46967 |  0:04:56s
epoch 90 | loss: 0.15921 | val_0_rmse: 0.34792 | val_1_rmse: 0.36497 |  0:05:00s
epoch 91 | loss: 0.14213 | val_0_rmse: 0.40514 | val_1_rmse: 0.41875 |  0:05:03s
epoch 92 | loss: 0.14708 | val_0_rmse: 0.33486 | val_1_rmse: 0.34888 |  0:05:06s
epoch 93 | loss: 0.17229 | val_0_rmse: 0.34434 | val_1_rmse: 0.35939 |  0:05:10s
epoch 94 | loss: 0.14754 | val_0_rmse: 0.34233 | val_1_rmse: 0.35827 |  0:05:13s
epoch 95 | loss: 0.14093 | val_0_rmse: 0.33224 | val_1_rmse: 0.34793 |  0:05:16s
epoch 96 | loss: 0.14209 | val_0_rmse: 0.39258 | val_1_rmse: 0.40615 |  0:05:19s
epoch 97 | loss: 0.15305 | val_0_rmse: 0.3509  | val_1_rmse: 0.36523 |  0:05:23s
epoch 98 | loss: 0.13098 | val_0_rmse: 0.34982 | val_1_rmse: 0.36585 |  0:05:26s
epoch 99 | loss: 0.13762 | val_0_rmse: 0.33773 | val_1_rmse: 0.3539  |  0:05:29s
epoch 100| loss: 0.1269  | val_0_rmse: 0.33379 | val_1_rmse: 0.3486  |  0:05:33s
epoch 101| loss: 0.12371 | val_0_rmse: 0.33076 | val_1_rmse: 0.34628 |  0:05:36s
epoch 102| loss: 0.12789 | val_0_rmse: 0.33651 | val_1_rmse: 0.35111 |  0:05:39s
epoch 103| loss: 0.12357 | val_0_rmse: 0.33194 | val_1_rmse: 0.34893 |  0:05:42s
epoch 104| loss: 0.14088 | val_0_rmse: 0.33481 | val_1_rmse: 0.34807 |  0:05:46s
epoch 105| loss: 0.12526 | val_0_rmse: 0.35647 | val_1_rmse: 0.37145 |  0:05:49s
epoch 106| loss: 0.14267 | val_0_rmse: 0.38085 | val_1_rmse: 0.3944  |  0:05:52s
epoch 107| loss: 0.13571 | val_0_rmse: 0.38087 | val_1_rmse: 0.39582 |  0:05:56s
epoch 108| loss: 0.12407 | val_0_rmse: 0.34734 | val_1_rmse: 0.36456 |  0:05:59s
epoch 109| loss: 0.12846 | val_0_rmse: 0.42169 | val_1_rmse: 0.43254 |  0:06:02s
epoch 110| loss: 0.13638 | val_0_rmse: 0.33694 | val_1_rmse: 0.35289 |  0:06:05s
epoch 111| loss: 0.13181 | val_0_rmse: 0.44774 | val_1_rmse: 0.45722 |  0:06:09s
epoch 112| loss: 0.15712 | val_0_rmse: 0.35012 | val_1_rmse: 0.36383 |  0:06:12s
epoch 113| loss: 0.1474  | val_0_rmse: 0.36114 | val_1_rmse: 0.37374 |  0:06:15s
epoch 114| loss: 0.15191 | val_0_rmse: 0.3367  | val_1_rmse: 0.34961 |  0:06:19s
epoch 115| loss: 0.13297 | val_0_rmse: 0.3412  | val_1_rmse: 0.35708 |  0:06:22s
epoch 116| loss: 0.15291 | val_0_rmse: 0.34339 | val_1_rmse: 0.35708 |  0:06:25s
epoch 117| loss: 0.13448 | val_0_rmse: 0.35798 | val_1_rmse: 0.37174 |  0:06:29s
epoch 118| loss: 0.13414 | val_0_rmse: 0.33433 | val_1_rmse: 0.34974 |  0:06:32s
epoch 119| loss: 0.12331 | val_0_rmse: 0.33093 | val_1_rmse: 0.3473  |  0:06:35s
epoch 120| loss: 0.17884 | val_0_rmse: 0.33725 | val_1_rmse: 0.35223 |  0:06:38s
epoch 121| loss: 0.14673 | val_0_rmse: 0.39047 | val_1_rmse: 0.40482 |  0:06:42s
epoch 122| loss: 0.16871 | val_0_rmse: 0.39052 | val_1_rmse: 0.404   |  0:06:45s
epoch 123| loss: 0.18374 | val_0_rmse: 0.33648 | val_1_rmse: 0.35178 |  0:06:48s
epoch 124| loss: 0.13182 | val_0_rmse: 0.33087 | val_1_rmse: 0.34658 |  0:06:52s
epoch 125| loss: 0.13216 | val_0_rmse: 0.36398 | val_1_rmse: 0.37772 |  0:06:55s
epoch 126| loss: 0.13607 | val_0_rmse: 0.33158 | val_1_rmse: 0.34586 |  0:06:58s
epoch 127| loss: 0.13904 | val_0_rmse: 0.3461  | val_1_rmse: 0.36061 |  0:07:02s
epoch 128| loss: 0.14061 | val_0_rmse: 0.37514 | val_1_rmse: 0.38671 |  0:07:05s
epoch 129| loss: 0.14113 | val_0_rmse: 0.32653 | val_1_rmse: 0.34288 |  0:07:08s
epoch 130| loss: 0.14173 | val_0_rmse: 0.34614 | val_1_rmse: 0.36109 |  0:07:11s
epoch 131| loss: 0.14218 | val_0_rmse: 0.36428 | val_1_rmse: 0.38147 |  0:07:15s
epoch 132| loss: 0.1445  | val_0_rmse: 0.37427 | val_1_rmse: 0.38984 |  0:07:18s
epoch 133| loss: 0.12417 | val_0_rmse: 0.3543  | val_1_rmse: 0.36934 |  0:07:21s
epoch 134| loss: 0.17552 | val_0_rmse: 0.33757 | val_1_rmse: 0.35625 |  0:07:25s
epoch 135| loss: 0.15453 | val_0_rmse: 0.32853 | val_1_rmse: 0.34478 |  0:07:28s
epoch 136| loss: 0.12526 | val_0_rmse: 0.33614 | val_1_rmse: 0.35111 |  0:07:31s
epoch 137| loss: 0.12446 | val_0_rmse: 0.33789 | val_1_rmse: 0.35275 |  0:07:34s
epoch 138| loss: 0.12954 | val_0_rmse: 0.39496 | val_1_rmse: 0.40952 |  0:07:38s
epoch 139| loss: 0.16343 | val_0_rmse: 0.3495  | val_1_rmse: 0.36404 |  0:07:41s
epoch 140| loss: 0.14665 | val_0_rmse: 0.35563 | val_1_rmse: 0.37041 |  0:07:44s
epoch 141| loss: 0.12365 | val_0_rmse: 0.33289 | val_1_rmse: 0.34956 |  0:07:48s
epoch 142| loss: 0.15234 | val_0_rmse: 0.32945 | val_1_rmse: 0.34684 |  0:07:51s
epoch 143| loss: 0.14532 | val_0_rmse: 0.32761 | val_1_rmse: 0.34406 |  0:07:54s
epoch 144| loss: 0.12197 | val_0_rmse: 0.34373 | val_1_rmse: 0.3589  |  0:07:57s
epoch 145| loss: 0.13195 | val_0_rmse: 0.3532  | val_1_rmse: 0.3678  |  0:08:01s
epoch 146| loss: 0.143   | val_0_rmse: 0.40375 | val_1_rmse: 0.4176  |  0:08:04s
epoch 147| loss: 0.15435 | val_0_rmse: 0.38493 | val_1_rmse: 0.398   |  0:08:07s
epoch 148| loss: 0.1273  | val_0_rmse: 0.3444  | val_1_rmse: 0.35855 |  0:08:11s
epoch 149| loss: 0.14847 | val_0_rmse: 0.347   | val_1_rmse: 0.36403 |  0:08:14s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.34288
Best weights from best epoch are automatically used!
ended training at: 17:40:46
Feature importance:
[('Area', 0.0), ('Baths', 0.262949949982249), ('Beds', 0.09972441370632917), ('Latitude', 0.2127777229093386), ('Longitude', 0.15457239868238076), ('Month', 1.3489475252589323e-05), ('Year', 0.26996202524444984)]
Mean squared error is of 12467512496.108393
Mean absolute error:76272.82533718726
MAPE:0.25830533464845945
R2 score:0.788080286326665
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:40:46
epoch 0  | loss: 21.91846| val_0_rmse: 7.30451 | val_1_rmse: 7.26429 |  0:00:03s
epoch 1  | loss: 0.46084 | val_0_rmse: 17.02601| val_1_rmse: 17.00306|  0:00:06s
epoch 2  | loss: 0.32868 | val_0_rmse: 7.21708 | val_1_rmse: 7.22971 |  0:00:09s
epoch 3  | loss: 0.31687 | val_0_rmse: 3.65449 | val_1_rmse: 3.6545  |  0:00:13s
epoch 4  | loss: 0.25721 | val_0_rmse: 3.10183 | val_1_rmse: 3.10021 |  0:00:16s
epoch 5  | loss: 0.25273 | val_0_rmse: 2.63106 | val_1_rmse: 2.62751 |  0:00:20s
epoch 6  | loss: 0.30177 | val_0_rmse: 2.20176 | val_1_rmse: 2.20021 |  0:00:23s
epoch 7  | loss: 0.23372 | val_0_rmse: 2.19102 | val_1_rmse: 2.1854  |  0:00:26s
epoch 8  | loss: 0.2312  | val_0_rmse: 1.37929 | val_1_rmse: 1.37263 |  0:00:30s
epoch 9  | loss: 0.23268 | val_0_rmse: 1.13152 | val_1_rmse: 1.12888 |  0:00:33s
epoch 10 | loss: 0.2404  | val_0_rmse: 0.96414 | val_1_rmse: 0.96224 |  0:00:36s
epoch 11 | loss: 0.22934 | val_0_rmse: 0.90603 | val_1_rmse: 0.90653 |  0:00:39s
epoch 12 | loss: 0.22593 | val_0_rmse: 1.06311 | val_1_rmse: 1.0639  |  0:00:43s
epoch 13 | loss: 0.22554 | val_0_rmse: 0.98263 | val_1_rmse: 0.98194 |  0:00:46s
epoch 14 | loss: 0.21694 | val_0_rmse: 0.98588 | val_1_rmse: 0.98648 |  0:00:49s
epoch 15 | loss: 0.21319 | val_0_rmse: 0.74597 | val_1_rmse: 0.74851 |  0:00:53s
epoch 16 | loss: 0.21212 | val_0_rmse: 0.70878 | val_1_rmse: 0.71113 |  0:00:56s
epoch 17 | loss: 0.20861 | val_0_rmse: 0.73075 | val_1_rmse: 0.73228 |  0:00:59s
epoch 18 | loss: 0.21055 | val_0_rmse: 0.79249 | val_1_rmse: 0.79308 |  0:01:03s
epoch 19 | loss: 0.21672 | val_0_rmse: 0.71838 | val_1_rmse: 0.72239 |  0:01:06s
epoch 20 | loss: 0.19205 | val_0_rmse: 0.65365 | val_1_rmse: 0.65804 |  0:01:09s
epoch 21 | loss: 0.18481 | val_0_rmse: 0.66613 | val_1_rmse: 0.66934 |  0:01:13s
epoch 22 | loss: 0.19981 | val_0_rmse: 0.58129 | val_1_rmse: 0.58584 |  0:01:16s
epoch 23 | loss: 0.19473 | val_0_rmse: 0.55233 | val_1_rmse: 0.55736 |  0:01:19s
epoch 24 | loss: 0.19038 | val_0_rmse: 0.52229 | val_1_rmse: 0.52639 |  0:01:22s
epoch 25 | loss: 0.19426 | val_0_rmse: 0.51853 | val_1_rmse: 0.52513 |  0:01:26s
epoch 26 | loss: 0.18448 | val_0_rmse: 0.46426 | val_1_rmse: 0.47053 |  0:01:29s
epoch 27 | loss: 0.16768 | val_0_rmse: 0.43223 | val_1_rmse: 0.43762 |  0:01:32s
epoch 28 | loss: 0.18899 | val_0_rmse: 0.40268 | val_1_rmse: 0.40598 |  0:01:36s
epoch 29 | loss: 0.15759 | val_0_rmse: 0.39464 | val_1_rmse: 0.39616 |  0:01:39s
epoch 30 | loss: 0.15155 | val_0_rmse: 0.36275 | val_1_rmse: 0.36637 |  0:01:42s
epoch 31 | loss: 0.1507  | val_0_rmse: 0.37525 | val_1_rmse: 0.37899 |  0:01:46s
epoch 32 | loss: 0.15831 | val_0_rmse: 0.41082 | val_1_rmse: 0.41643 |  0:01:49s
epoch 33 | loss: 0.17465 | val_0_rmse: 0.44921 | val_1_rmse: 0.45597 |  0:01:52s
epoch 34 | loss: 0.16828 | val_0_rmse: 0.41101 | val_1_rmse: 0.41531 |  0:01:56s
epoch 35 | loss: 0.16679 | val_0_rmse: 0.35678 | val_1_rmse: 0.36301 |  0:01:59s
epoch 36 | loss: 0.16473 | val_0_rmse: 0.36142 | val_1_rmse: 0.36881 |  0:02:02s
epoch 37 | loss: 0.16491 | val_0_rmse: 0.39986 | val_1_rmse: 0.40525 |  0:02:06s
epoch 38 | loss: 0.15805 | val_0_rmse: 0.39346 | val_1_rmse: 0.39766 |  0:02:09s
epoch 39 | loss: 0.16998 | val_0_rmse: 0.37933 | val_1_rmse: 0.38681 |  0:02:12s
epoch 40 | loss: 0.16338 | val_0_rmse: 0.38093 | val_1_rmse: 0.38667 |  0:02:16s
epoch 41 | loss: 0.15731 | val_0_rmse: 0.36433 | val_1_rmse: 0.37475 |  0:02:19s
epoch 42 | loss: 0.15509 | val_0_rmse: 0.34957 | val_1_rmse: 0.35869 |  0:02:22s
epoch 43 | loss: 0.15532 | val_0_rmse: 0.35248 | val_1_rmse: 0.36076 |  0:02:25s
epoch 44 | loss: 0.16226 | val_0_rmse: 0.35891 | val_1_rmse: 0.36622 |  0:02:29s
epoch 45 | loss: 0.15435 | val_0_rmse: 0.34481 | val_1_rmse: 0.35298 |  0:02:32s
epoch 46 | loss: 0.15437 | val_0_rmse: 0.35935 | val_1_rmse: 0.36817 |  0:02:35s
epoch 47 | loss: 0.15419 | val_0_rmse: 0.38196 | val_1_rmse: 0.39099 |  0:02:39s
epoch 48 | loss: 0.144   | val_0_rmse: 0.36509 | val_1_rmse: 0.37331 |  0:02:42s
epoch 49 | loss: 0.14559 | val_0_rmse: 0.34867 | val_1_rmse: 0.35609 |  0:02:45s
epoch 50 | loss: 0.14314 | val_0_rmse: 0.39578 | val_1_rmse: 0.40252 |  0:02:49s
epoch 51 | loss: 0.13823 | val_0_rmse: 0.3562  | val_1_rmse: 0.36356 |  0:02:52s
epoch 52 | loss: 0.14211 | val_0_rmse: 0.4537  | val_1_rmse: 0.45904 |  0:02:55s
epoch 53 | loss: 0.21449 | val_0_rmse: 0.44923 | val_1_rmse: 0.45605 |  0:02:58s
epoch 54 | loss: 0.20382 | val_0_rmse: 0.37068 | val_1_rmse: 0.37856 |  0:03:02s
epoch 55 | loss: 0.18746 | val_0_rmse: 0.35394 | val_1_rmse: 0.3631  |  0:03:05s
epoch 56 | loss: 0.13144 | val_0_rmse: 0.3412  | val_1_rmse: 0.3497  |  0:03:08s
epoch 57 | loss: 0.14284 | val_0_rmse: 0.39593 | val_1_rmse: 0.40356 |  0:03:12s
epoch 58 | loss: 0.15069 | val_0_rmse: 0.34722 | val_1_rmse: 0.35785 |  0:03:15s
epoch 59 | loss: 0.13181 | val_0_rmse: 0.37295 | val_1_rmse: 0.38185 |  0:03:18s
epoch 60 | loss: 0.12874 | val_0_rmse: 0.34194 | val_1_rmse: 0.35036 |  0:03:22s
epoch 61 | loss: 0.1866  | val_0_rmse: 0.48058 | val_1_rmse: 0.48677 |  0:03:25s
epoch 62 | loss: 0.15675 | val_0_rmse: 0.3572  | val_1_rmse: 0.36467 |  0:03:28s
epoch 63 | loss: 0.13586 | val_0_rmse: 0.34045 | val_1_rmse: 0.3508  |  0:03:32s
epoch 64 | loss: 0.13853 | val_0_rmse: 0.39489 | val_1_rmse: 0.40316 |  0:03:35s
epoch 65 | loss: 0.16543 | val_0_rmse: 0.3292  | val_1_rmse: 0.34035 |  0:03:38s
epoch 66 | loss: 0.14416 | val_0_rmse: 0.34022 | val_1_rmse: 0.3499  |  0:03:42s
epoch 67 | loss: 0.12835 | val_0_rmse: 0.35788 | val_1_rmse: 0.36781 |  0:03:45s
epoch 68 | loss: 0.13018 | val_0_rmse: 0.34034 | val_1_rmse: 0.34787 |  0:03:48s
epoch 69 | loss: 0.14876 | val_0_rmse: 0.33235 | val_1_rmse: 0.34351 |  0:03:51s
epoch 70 | loss: 0.1282  | val_0_rmse: 0.3428  | val_1_rmse: 0.35212 |  0:03:55s
epoch 71 | loss: 0.12669 | val_0_rmse: 0.3343  | val_1_rmse: 0.34407 |  0:03:58s
epoch 72 | loss: 0.16364 | val_0_rmse: 0.33497 | val_1_rmse: 0.34557 |  0:04:01s
epoch 73 | loss: 0.14865 | val_0_rmse: 0.39757 | val_1_rmse: 0.40468 |  0:04:05s
epoch 74 | loss: 0.1455  | val_0_rmse: 0.34949 | val_1_rmse: 0.3602  |  0:04:08s
epoch 75 | loss: 0.14035 | val_0_rmse: 0.33619 | val_1_rmse: 0.34822 |  0:04:11s
epoch 76 | loss: 0.13387 | val_0_rmse: 0.33369 | val_1_rmse: 0.34469 |  0:04:15s
epoch 77 | loss: 0.13014 | val_0_rmse: 0.34814 | val_1_rmse: 0.35849 |  0:04:18s
epoch 78 | loss: 0.12711 | val_0_rmse: 0.36695 | val_1_rmse: 0.37681 |  0:04:21s
epoch 79 | loss: 0.13663 | val_0_rmse: 0.33136 | val_1_rmse: 0.34183 |  0:04:24s
epoch 80 | loss: 0.12947 | val_0_rmse: 0.36953 | val_1_rmse: 0.37752 |  0:04:28s
epoch 81 | loss: 0.12775 | val_0_rmse: 0.333   | val_1_rmse: 0.3458  |  0:04:31s
epoch 82 | loss: 0.12473 | val_0_rmse: 0.32969 | val_1_rmse: 0.33947 |  0:04:34s
epoch 83 | loss: 0.13631 | val_0_rmse: 0.32532 | val_1_rmse: 0.3372  |  0:04:38s
epoch 84 | loss: 0.12237 | val_0_rmse: 0.3525  | val_1_rmse: 0.36304 |  0:04:41s
epoch 85 | loss: 0.12548 | val_0_rmse: 0.3773  | val_1_rmse: 0.38809 |  0:04:44s
epoch 86 | loss: 0.13992 | val_0_rmse: 0.33709 | val_1_rmse: 0.34712 |  0:04:48s
epoch 87 | loss: 0.13313 | val_0_rmse: 0.34275 | val_1_rmse: 0.35337 |  0:04:51s
epoch 88 | loss: 0.1212  | val_0_rmse: 0.32466 | val_1_rmse: 0.33822 |  0:04:54s
epoch 89 | loss: 0.13567 | val_0_rmse: 0.33577 | val_1_rmse: 0.34789 |  0:04:58s
epoch 90 | loss: 0.13184 | val_0_rmse: 0.33948 | val_1_rmse: 0.35182 |  0:05:01s
epoch 91 | loss: 0.12364 | val_0_rmse: 0.34244 | val_1_rmse: 0.3549  |  0:05:04s
epoch 92 | loss: 0.15682 | val_0_rmse: 0.51417 | val_1_rmse: 0.52212 |  0:05:07s
epoch 93 | loss: 0.13012 | val_0_rmse: 0.34915 | val_1_rmse: 0.35987 |  0:05:11s
epoch 94 | loss: 0.14211 | val_0_rmse: 0.33813 | val_1_rmse: 0.35169 |  0:05:14s
epoch 95 | loss: 0.12533 | val_0_rmse: 0.33519 | val_1_rmse: 0.34461 |  0:05:17s
epoch 96 | loss: 0.13675 | val_0_rmse: 0.35476 | val_1_rmse: 0.36815 |  0:05:21s
epoch 97 | loss: 0.12718 | val_0_rmse: 0.36271 | val_1_rmse: 0.37302 |  0:05:24s
epoch 98 | loss: 0.13081 | val_0_rmse: 0.3455  | val_1_rmse: 0.3581  |  0:05:27s
epoch 99 | loss: 0.11939 | val_0_rmse: 0.33412 | val_1_rmse: 0.34633 |  0:05:31s
epoch 100| loss: 0.12279 | val_0_rmse: 0.32405 | val_1_rmse: 0.33746 |  0:05:34s
epoch 101| loss: 0.12466 | val_0_rmse: 0.34331 | val_1_rmse: 0.35584 |  0:05:37s
epoch 102| loss: 0.13501 | val_0_rmse: 0.32116 | val_1_rmse: 0.3345  |  0:05:41s
epoch 103| loss: 0.12858 | val_0_rmse: 0.34225 | val_1_rmse: 0.35395 |  0:05:44s
epoch 104| loss: 0.13907 | val_0_rmse: 0.33111 | val_1_rmse: 0.34368 |  0:05:47s
epoch 105| loss: 0.12301 | val_0_rmse: 0.32727 | val_1_rmse: 0.33968 |  0:05:50s
epoch 106| loss: 0.11998 | val_0_rmse: 0.32281 | val_1_rmse: 0.33616 |  0:05:54s
epoch 107| loss: 0.12924 | val_0_rmse: 0.34862 | val_1_rmse: 0.36098 |  0:05:57s
epoch 108| loss: 0.1309  | val_0_rmse: 0.31762 | val_1_rmse: 0.33287 |  0:06:00s
epoch 109| loss: 0.12982 | val_0_rmse: 0.3574  | val_1_rmse: 0.37114 |  0:06:04s
epoch 110| loss: 0.1291  | val_0_rmse: 0.32209 | val_1_rmse: 0.33518 |  0:06:07s
epoch 111| loss: 0.12964 | val_0_rmse: 0.31935 | val_1_rmse: 0.33233 |  0:06:10s
epoch 112| loss: 0.12058 | val_0_rmse: 0.35606 | val_1_rmse: 0.36528 |  0:06:14s
epoch 113| loss: 0.12748 | val_0_rmse: 0.32279 | val_1_rmse: 0.3359  |  0:06:17s
epoch 114| loss: 0.12229 | val_0_rmse: 0.3587  | val_1_rmse: 0.37124 |  0:06:20s
epoch 115| loss: 0.12417 | val_0_rmse: 0.3215  | val_1_rmse: 0.33635 |  0:06:24s
epoch 116| loss: 0.11994 | val_0_rmse: 0.32615 | val_1_rmse: 0.34052 |  0:06:27s
epoch 117| loss: 0.11906 | val_0_rmse: 0.3359  | val_1_rmse: 0.34832 |  0:06:30s
epoch 118| loss: 0.14075 | val_0_rmse: 0.36423 | val_1_rmse: 0.37806 |  0:06:33s
epoch 119| loss: 0.12217 | val_0_rmse: 0.32128 | val_1_rmse: 0.33606 |  0:06:37s
epoch 120| loss: 0.12942 | val_0_rmse: 0.416   | val_1_rmse: 0.42968 |  0:06:40s
epoch 121| loss: 0.1669  | val_0_rmse: 0.32732 | val_1_rmse: 0.33908 |  0:06:43s
epoch 122| loss: 0.12554 | val_0_rmse: 0.31865 | val_1_rmse: 0.33328 |  0:06:47s
epoch 123| loss: 0.12762 | val_0_rmse: 0.38864 | val_1_rmse: 0.40116 |  0:06:50s
epoch 124| loss: 0.12577 | val_0_rmse: 0.43022 | val_1_rmse: 0.44265 |  0:06:53s
epoch 125| loss: 0.13484 | val_0_rmse: 0.33743 | val_1_rmse: 0.35036 |  0:06:57s
epoch 126| loss: 0.12852 | val_0_rmse: 0.32377 | val_1_rmse: 0.33691 |  0:07:00s
epoch 127| loss: 0.12146 | val_0_rmse: 0.32497 | val_1_rmse: 0.33733 |  0:07:03s
epoch 128| loss: 0.12382 | val_0_rmse: 0.32239 | val_1_rmse: 0.3366  |  0:07:07s
epoch 129| loss: 0.13435 | val_0_rmse: 0.3351  | val_1_rmse: 0.34858 |  0:07:10s
epoch 130| loss: 0.13222 | val_0_rmse: 0.31544 | val_1_rmse: 0.33194 |  0:07:13s
epoch 131| loss: 0.11612 | val_0_rmse: 0.33652 | val_1_rmse: 0.34897 |  0:07:16s
epoch 132| loss: 0.1312  | val_0_rmse: 0.37697 | val_1_rmse: 0.38983 |  0:07:20s
epoch 133| loss: 0.12909 | val_0_rmse: 0.31741 | val_1_rmse: 0.33159 |  0:07:23s
epoch 134| loss: 0.11818 | val_0_rmse: 0.31972 | val_1_rmse: 0.33618 |  0:07:26s
epoch 135| loss: 0.11433 | val_0_rmse: 0.33205 | val_1_rmse: 0.34744 |  0:07:30s
epoch 136| loss: 0.12009 | val_0_rmse: 0.3187  | val_1_rmse: 0.33445 |  0:07:33s
epoch 137| loss: 0.11849 | val_0_rmse: 0.32168 | val_1_rmse: 0.3387  |  0:07:36s
epoch 138| loss: 0.12195 | val_0_rmse: 0.35401 | val_1_rmse: 0.36819 |  0:07:40s
epoch 139| loss: 0.11981 | val_0_rmse: 0.34126 | val_1_rmse: 0.35464 |  0:07:43s
epoch 140| loss: 0.11649 | val_0_rmse: 0.31536 | val_1_rmse: 0.33191 |  0:07:46s
epoch 141| loss: 0.13665 | val_0_rmse: 0.31675 | val_1_rmse: 0.33301 |  0:07:49s
epoch 142| loss: 0.1288  | val_0_rmse: 0.32688 | val_1_rmse: 0.34261 |  0:07:53s
epoch 143| loss: 0.12228 | val_0_rmse: 0.31691 | val_1_rmse: 0.33423 |  0:07:56s
epoch 144| loss: 0.11991 | val_0_rmse: 0.31288 | val_1_rmse: 0.32898 |  0:07:59s
epoch 145| loss: 0.12558 | val_0_rmse: 0.32894 | val_1_rmse: 0.34418 |  0:08:03s
epoch 146| loss: 0.13327 | val_0_rmse: 0.35157 | val_1_rmse: 0.36617 |  0:08:06s
epoch 147| loss: 0.11728 | val_0_rmse: 0.33039 | val_1_rmse: 0.34198 |  0:08:09s
epoch 148| loss: 0.12899 | val_0_rmse: 0.41474 | val_1_rmse: 0.42838 |  0:08:13s
epoch 149| loss: 0.12699 | val_0_rmse: 0.37702 | val_1_rmse: 0.39335 |  0:08:16s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.32898
Best weights from best epoch are automatically used!
ended training at: 17:49:04
Feature importance:
[('Area', 0.0), ('Baths', 0.24839587232579952), ('Beds', 0.04402386371658696), ('Latitude', 0.18178148482265266), ('Longitude', 0.2349210785127872), ('Month', 0.0944201579803818), ('Year', 0.19645754264179188)]
Mean squared error is of 10588356474.842182
Mean absolute error:69731.42264063298
MAPE:0.2560184206865282
R2 score:0.8142334195196734
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:49:04
epoch 0  | loss: 21.11231| val_0_rmse: 6.51481 | val_1_rmse: 6.5265  |  0:00:03s
epoch 1  | loss: 0.39026 | val_0_rmse: 4.97209 | val_1_rmse: 4.98125 |  0:00:06s
epoch 2  | loss: 0.30193 | val_0_rmse: 4.87846 | val_1_rmse: 4.8819  |  0:00:09s
epoch 3  | loss: 0.27922 | val_0_rmse: 1.39605 | val_1_rmse: 1.40282 |  0:00:13s
epoch 4  | loss: 0.28826 | val_0_rmse: 1.37975 | val_1_rmse: 1.38702 |  0:00:16s
epoch 5  | loss: 0.28922 | val_0_rmse: 1.34279 | val_1_rmse: 1.34982 |  0:00:19s
epoch 6  | loss: 0.26316 | val_0_rmse: 0.86091 | val_1_rmse: 0.87095 |  0:00:23s
epoch 7  | loss: 0.27199 | val_0_rmse: 0.91237 | val_1_rmse: 0.92202 |  0:00:26s
epoch 8  | loss: 0.28885 | val_0_rmse: 1.08767 | val_1_rmse: 1.09251 |  0:00:29s
epoch 9  | loss: 0.25843 | val_0_rmse: 0.75284 | val_1_rmse: 0.7646  |  0:00:33s
epoch 10 | loss: 0.2471  | val_0_rmse: 0.93004 | val_1_rmse: 0.9375  |  0:00:36s
epoch 11 | loss: 0.25328 | val_0_rmse: 0.77861 | val_1_rmse: 0.78899 |  0:00:39s
epoch 12 | loss: 0.27043 | val_0_rmse: 0.83303 | val_1_rmse: 0.84069 |  0:00:43s
epoch 13 | loss: 0.26509 | val_0_rmse: 1.0048  | val_1_rmse: 1.01376 |  0:00:46s
epoch 14 | loss: 0.25808 | val_0_rmse: 1.09886 | val_1_rmse: 1.10876 |  0:00:49s
epoch 15 | loss: 0.25701 | val_0_rmse: 0.93244 | val_1_rmse: 0.94147 |  0:00:53s
epoch 16 | loss: 0.25778 | val_0_rmse: 0.67622 | val_1_rmse: 0.68695 |  0:00:56s
epoch 17 | loss: 0.25107 | val_0_rmse: 0.65514 | val_1_rmse: 0.66617 |  0:00:59s
epoch 18 | loss: 0.24926 | val_0_rmse: 0.6399  | val_1_rmse: 0.64994 |  0:01:02s
epoch 19 | loss: 0.22833 | val_0_rmse: 0.64398 | val_1_rmse: 0.65571 |  0:01:06s
epoch 20 | loss: 0.2168  | val_0_rmse: 0.70981 | val_1_rmse: 0.71953 |  0:01:09s
epoch 21 | loss: 0.20527 | val_0_rmse: 0.76077 | val_1_rmse: 0.77134 |  0:01:12s
epoch 22 | loss: 0.22098 | val_0_rmse: 0.65907 | val_1_rmse: 0.67018 |  0:01:16s
epoch 23 | loss: 0.20608 | val_0_rmse: 0.54017 | val_1_rmse: 0.55253 |  0:01:19s
epoch 24 | loss: 0.20112 | val_0_rmse: 0.49356 | val_1_rmse: 0.50789 |  0:01:22s
epoch 25 | loss: 0.18289 | val_0_rmse: 0.52205 | val_1_rmse: 0.53787 |  0:01:26s
epoch 26 | loss: 0.19854 | val_0_rmse: 0.42668 | val_1_rmse: 0.43885 |  0:01:29s
epoch 27 | loss: 0.19854 | val_0_rmse: 0.45292 | val_1_rmse: 0.46611 |  0:01:32s
epoch 28 | loss: 0.18843 | val_0_rmse: 0.4566  | val_1_rmse: 0.47005 |  0:01:36s
epoch 29 | loss: 0.18434 | val_0_rmse: 0.40841 | val_1_rmse: 0.42098 |  0:01:39s
epoch 30 | loss: 0.17996 | val_0_rmse: 0.38857 | val_1_rmse: 0.39682 |  0:01:42s
epoch 31 | loss: 0.17695 | val_0_rmse: 0.41458 | val_1_rmse: 0.4193  |  0:01:45s
epoch 32 | loss: 0.17624 | val_0_rmse: 0.39429 | val_1_rmse: 0.40116 |  0:01:49s
epoch 33 | loss: 0.17568 | val_0_rmse: 0.41629 | val_1_rmse: 0.4287  |  0:01:52s
epoch 34 | loss: 0.16033 | val_0_rmse: 0.37282 | val_1_rmse: 0.38446 |  0:01:55s
epoch 35 | loss: 0.17491 | val_0_rmse: 0.42624 | val_1_rmse: 0.44065 |  0:01:59s
epoch 36 | loss: 0.16715 | val_0_rmse: 0.36525 | val_1_rmse: 0.37329 |  0:02:02s
epoch 37 | loss: 0.16449 | val_0_rmse: 0.36263 | val_1_rmse: 0.37256 |  0:02:05s
epoch 38 | loss: 0.16172 | val_0_rmse: 0.3822  | val_1_rmse: 0.39204 |  0:02:09s
epoch 39 | loss: 0.14282 | val_0_rmse: 0.35684 | val_1_rmse: 0.36286 |  0:02:12s
epoch 40 | loss: 0.14274 | val_0_rmse: 0.37233 | val_1_rmse: 0.37914 |  0:02:15s
epoch 41 | loss: 0.15223 | val_0_rmse: 0.39556 | val_1_rmse: 0.4063  |  0:02:19s
epoch 42 | loss: 0.15799 | val_0_rmse: 0.35136 | val_1_rmse: 0.36223 |  0:02:22s
epoch 43 | loss: 0.14496 | val_0_rmse: 0.3512  | val_1_rmse: 0.35896 |  0:02:25s
epoch 44 | loss: 0.15507 | val_0_rmse: 0.36084 | val_1_rmse: 0.3687  |  0:02:29s
epoch 45 | loss: 0.15533 | val_0_rmse: 0.36323 | val_1_rmse: 0.37431 |  0:02:32s
epoch 46 | loss: 0.15683 | val_0_rmse: 0.3689  | val_1_rmse: 0.38095 |  0:02:35s
epoch 47 | loss: 0.15366 | val_0_rmse: 0.34795 | val_1_rmse: 0.35998 |  0:02:38s
epoch 48 | loss: 0.1526  | val_0_rmse: 0.36962 | val_1_rmse: 0.37385 |  0:02:42s
epoch 49 | loss: 0.15255 | val_0_rmse: 0.39435 | val_1_rmse: 0.3995  |  0:02:45s
epoch 50 | loss: 0.15394 | val_0_rmse: 0.35956 | val_1_rmse: 0.36858 |  0:02:48s
epoch 51 | loss: 0.16262 | val_0_rmse: 0.55079 | val_1_rmse: 0.5539  |  0:02:52s
epoch 52 | loss: 0.16136 | val_0_rmse: 0.3814  | val_1_rmse: 0.38772 |  0:02:55s
epoch 53 | loss: 0.15313 | val_0_rmse: 0.38234 | val_1_rmse: 0.38711 |  0:02:58s
epoch 54 | loss: 0.15512 | val_0_rmse: 0.35409 | val_1_rmse: 0.3658  |  0:03:02s
epoch 55 | loss: 0.15222 | val_0_rmse: 0.40468 | val_1_rmse: 0.41686 |  0:03:05s
epoch 56 | loss: 0.1528  | val_0_rmse: 0.34484 | val_1_rmse: 0.35602 |  0:03:08s
epoch 57 | loss: 0.14615 | val_0_rmse: 0.38364 | val_1_rmse: 0.38899 |  0:03:11s
epoch 58 | loss: 0.14305 | val_0_rmse: 0.3377  | val_1_rmse: 0.34644 |  0:03:15s
epoch 59 | loss: 0.14268 | val_0_rmse: 0.38353 | val_1_rmse: 0.39771 |  0:03:18s
epoch 60 | loss: 0.15666 | val_0_rmse: 0.34554 | val_1_rmse: 0.35787 |  0:03:21s
epoch 61 | loss: 0.14932 | val_0_rmse: 0.36833 | val_1_rmse: 0.37572 |  0:03:25s
epoch 62 | loss: 0.13689 | val_0_rmse: 0.34264 | val_1_rmse: 0.35487 |  0:03:28s
epoch 63 | loss: 0.12943 | val_0_rmse: 0.35893 | val_1_rmse: 0.36738 |  0:03:31s
epoch 64 | loss: 0.13195 | val_0_rmse: 0.35723 | val_1_rmse: 0.36524 |  0:03:34s
epoch 65 | loss: 0.13227 | val_0_rmse: 0.34257 | val_1_rmse: 0.35549 |  0:03:38s
epoch 66 | loss: 0.12514 | val_0_rmse: 0.34343 | val_1_rmse: 0.35424 |  0:03:41s
epoch 67 | loss: 0.14251 | val_0_rmse: 0.33428 | val_1_rmse: 0.34408 |  0:03:44s
epoch 68 | loss: 0.14757 | val_0_rmse: 0.34248 | val_1_rmse: 0.35376 |  0:03:48s
epoch 69 | loss: 0.15304 | val_0_rmse: 0.35056 | val_1_rmse: 0.36183 |  0:03:51s
epoch 70 | loss: 0.14203 | val_0_rmse: 0.35813 | val_1_rmse: 0.37041 |  0:03:54s
epoch 71 | loss: 0.13436 | val_0_rmse: 0.34019 | val_1_rmse: 0.34889 |  0:03:58s
epoch 72 | loss: 0.14474 | val_0_rmse: 0.33623 | val_1_rmse: 0.34652 |  0:04:01s
epoch 73 | loss: 0.12592 | val_0_rmse: 0.33501 | val_1_rmse: 0.34682 |  0:04:04s
epoch 74 | loss: 0.14229 | val_0_rmse: 0.34153 | val_1_rmse: 0.3511  |  0:04:07s
epoch 75 | loss: 0.14554 | val_0_rmse: 0.37706 | val_1_rmse: 0.38203 |  0:04:11s
epoch 76 | loss: 0.14519 | val_0_rmse: 0.33936 | val_1_rmse: 0.34835 |  0:04:14s
epoch 77 | loss: 0.14777 | val_0_rmse: 0.33593 | val_1_rmse: 0.34716 |  0:04:17s
epoch 78 | loss: 0.12946 | val_0_rmse: 0.36933 | val_1_rmse: 0.37945 |  0:04:21s
epoch 79 | loss: 0.14135 | val_0_rmse: 0.3443  | val_1_rmse: 0.35495 |  0:04:24s
epoch 80 | loss: 0.15027 | val_0_rmse: 0.42436 | val_1_rmse: 0.43579 |  0:04:27s
epoch 81 | loss: 0.19499 | val_0_rmse: 0.4572  | val_1_rmse: 0.46134 |  0:04:31s
epoch 82 | loss: 0.18239 | val_0_rmse: 0.35224 | val_1_rmse: 0.36056 |  0:04:34s
epoch 83 | loss: 0.14064 | val_0_rmse: 0.39503 | val_1_rmse: 0.40091 |  0:04:37s
epoch 84 | loss: 0.13304 | val_0_rmse: 0.37091 | val_1_rmse: 0.38484 |  0:04:41s
epoch 85 | loss: 0.15329 | val_0_rmse: 0.38752 | val_1_rmse: 0.40059 |  0:04:44s
epoch 86 | loss: 0.15812 | val_0_rmse: 0.36148 | val_1_rmse: 0.37536 |  0:04:47s
epoch 87 | loss: 0.14486 | val_0_rmse: 0.38148 | val_1_rmse: 0.39537 |  0:04:50s
epoch 88 | loss: 0.13167 | val_0_rmse: 0.33806 | val_1_rmse: 0.35011 |  0:04:54s
epoch 89 | loss: 0.1236  | val_0_rmse: 0.33028 | val_1_rmse: 0.34208 |  0:04:57s
epoch 90 | loss: 0.13654 | val_0_rmse: 0.33605 | val_1_rmse: 0.34899 |  0:05:00s
epoch 91 | loss: 0.1265  | val_0_rmse: 0.34243 | val_1_rmse: 0.35037 |  0:05:04s
epoch 92 | loss: 0.133   | val_0_rmse: 0.33657 | val_1_rmse: 0.34815 |  0:05:07s
epoch 93 | loss: 0.13409 | val_0_rmse: 0.34817 | val_1_rmse: 0.36134 |  0:05:10s
epoch 94 | loss: 0.14697 | val_0_rmse: 0.33207 | val_1_rmse: 0.3449  |  0:05:14s
epoch 95 | loss: 0.1424  | val_0_rmse: 0.35673 | val_1_rmse: 0.36974 |  0:05:17s
epoch 96 | loss: 0.13632 | val_0_rmse: 0.33393 | val_1_rmse: 0.34671 |  0:05:20s
epoch 97 | loss: 0.13042 | val_0_rmse: 0.33873 | val_1_rmse: 0.35318 |  0:05:23s
epoch 98 | loss: 0.14499 | val_0_rmse: 0.32868 | val_1_rmse: 0.33989 |  0:05:27s
epoch 99 | loss: 0.15299 | val_0_rmse: 0.32333 | val_1_rmse: 0.33514 |  0:05:30s
epoch 100| loss: 0.13057 | val_0_rmse: 0.32141 | val_1_rmse: 0.33288 |  0:05:33s
epoch 101| loss: 0.16064 | val_0_rmse: 0.38161 | val_1_rmse: 0.38929 |  0:05:37s
epoch 102| loss: 0.13077 | val_0_rmse: 0.32401 | val_1_rmse: 0.33589 |  0:05:40s
epoch 103| loss: 0.12184 | val_0_rmse: 0.32701 | val_1_rmse: 0.34155 |  0:05:43s
epoch 104| loss: 0.12999 | val_0_rmse: 0.34671 | val_1_rmse: 0.36095 |  0:05:47s
epoch 105| loss: 0.12349 | val_0_rmse: 0.32731 | val_1_rmse: 0.34062 |  0:05:50s
epoch 106| loss: 0.20222 | val_0_rmse: 0.40917 | val_1_rmse: 0.42306 |  0:05:53s
epoch 107| loss: 0.14179 | val_0_rmse: 0.46144 | val_1_rmse: 0.46653 |  0:05:56s
epoch 108| loss: 0.14383 | val_0_rmse: 0.32478 | val_1_rmse: 0.33644 |  0:06:00s
epoch 109| loss: 0.1388  | val_0_rmse: 0.33816 | val_1_rmse: 0.35189 |  0:06:03s
epoch 110| loss: 0.1342  | val_0_rmse: 0.32581 | val_1_rmse: 0.34065 |  0:06:06s
epoch 111| loss: 0.12372 | val_0_rmse: 0.33424 | val_1_rmse: 0.34715 |  0:06:10s
epoch 112| loss: 0.12712 | val_0_rmse: 0.32392 | val_1_rmse: 0.33565 |  0:06:13s
epoch 113| loss: 0.12075 | val_0_rmse: 0.3416  | val_1_rmse: 0.3522  |  0:06:16s
epoch 114| loss: 0.14073 | val_0_rmse: 0.33477 | val_1_rmse: 0.34716 |  0:06:20s
epoch 115| loss: 0.14099 | val_0_rmse: 0.32345 | val_1_rmse: 0.33712 |  0:06:23s
epoch 116| loss: 0.1409  | val_0_rmse: 0.40803 | val_1_rmse: 0.42136 |  0:06:26s
epoch 117| loss: 0.1381  | val_0_rmse: 0.33198 | val_1_rmse: 0.34698 |  0:06:29s
epoch 118| loss: 0.12725 | val_0_rmse: 0.32466 | val_1_rmse: 0.33741 |  0:06:33s
epoch 119| loss: 0.16402 | val_0_rmse: 0.46787 | val_1_rmse: 0.47985 |  0:06:36s
epoch 120| loss: 0.17551 | val_0_rmse: 0.32503 | val_1_rmse: 0.33847 |  0:06:39s
epoch 121| loss: 0.12019 | val_0_rmse: 0.33173 | val_1_rmse: 0.34142 |  0:06:43s
epoch 122| loss: 0.12104 | val_0_rmse: 0.32492 | val_1_rmse: 0.33895 |  0:06:46s
epoch 123| loss: 0.13711 | val_0_rmse: 0.32999 | val_1_rmse: 0.34573 |  0:06:49s
epoch 124| loss: 0.14255 | val_0_rmse: 0.31907 | val_1_rmse: 0.3322  |  0:06:53s
epoch 125| loss: 0.12948 | val_0_rmse: 0.3246  | val_1_rmse: 0.33787 |  0:06:56s
epoch 126| loss: 0.1359  | val_0_rmse: 0.33965 | val_1_rmse: 0.35072 |  0:06:59s
epoch 127| loss: 0.13626 | val_0_rmse: 0.32683 | val_1_rmse: 0.34044 |  0:07:03s
epoch 128| loss: 0.12464 | val_0_rmse: 0.32646 | val_1_rmse: 0.33665 |  0:07:06s
epoch 129| loss: 0.12802 | val_0_rmse: 0.40888 | val_1_rmse: 0.42358 |  0:07:09s
epoch 130| loss: 0.13402 | val_0_rmse: 0.32088 | val_1_rmse: 0.33208 |  0:07:13s
epoch 131| loss: 0.12058 | val_0_rmse: 0.34158 | val_1_rmse: 0.35134 |  0:07:16s
epoch 132| loss: 0.13851 | val_0_rmse: 0.40243 | val_1_rmse: 0.41734 |  0:07:19s
epoch 133| loss: 0.14106 | val_0_rmse: 0.3219  | val_1_rmse: 0.33593 |  0:07:22s
epoch 134| loss: 0.12165 | val_0_rmse: 0.31969 | val_1_rmse: 0.33215 |  0:07:26s
epoch 135| loss: 0.11879 | val_0_rmse: 0.32189 | val_1_rmse: 0.33669 |  0:07:29s
epoch 136| loss: 0.17945 | val_0_rmse: 0.38495 | val_1_rmse: 0.39984 |  0:07:32s
epoch 137| loss: 0.15757 | val_0_rmse: 0.31943 | val_1_rmse: 0.3311  |  0:07:36s
epoch 138| loss: 0.11728 | val_0_rmse: 0.32034 | val_1_rmse: 0.33472 |  0:07:39s
epoch 139| loss: 0.12043 | val_0_rmse: 0.32321 | val_1_rmse: 0.33914 |  0:07:42s
epoch 140| loss: 0.12342 | val_0_rmse: 0.32386 | val_1_rmse: 0.33747 |  0:07:46s
epoch 141| loss: 0.13662 | val_0_rmse: 0.33174 | val_1_rmse: 0.34545 |  0:07:49s
epoch 142| loss: 0.11507 | val_0_rmse: 0.32453 | val_1_rmse: 0.33793 |  0:07:52s
epoch 143| loss: 0.11758 | val_0_rmse: 0.32115 | val_1_rmse: 0.33719 |  0:07:55s
epoch 144| loss: 0.11849 | val_0_rmse: 0.31754 | val_1_rmse: 0.33492 |  0:07:59s
epoch 145| loss: 0.12021 | val_0_rmse: 0.32963 | val_1_rmse: 0.34496 |  0:08:02s
epoch 146| loss: 0.12679 | val_0_rmse: 0.42949 | val_1_rmse: 0.44362 |  0:08:05s
epoch 147| loss: 0.13071 | val_0_rmse: 0.34976 | val_1_rmse: 0.36674 |  0:08:09s
epoch 148| loss: 0.16556 | val_0_rmse: 0.43672 | val_1_rmse: 0.44319 |  0:08:12s
epoch 149| loss: 0.18902 | val_0_rmse: 0.37136 | val_1_rmse: 0.39007 |  0:08:15s
Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_val_1_rmse = 0.3311
Best weights from best epoch are automatically used!
ended training at: 17:57:21
Feature importance:
[('Area', 0.09951648282412151), ('Baths', 0.34558437285251636), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.30555712919449757), ('Month', 0.011905513337809523), ('Year', 0.23743650179105505)]
Mean squared error is of 11297468085.568897
Mean absolute error:73398.88404853905
MAPE:0.24783703363763696
R2 score:0.8042931798908665
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:57:21
epoch 0  | loss: 21.63117| val_0_rmse: 4.91237 | val_1_rmse: 4.89722 |  0:00:03s
epoch 1  | loss: 0.50093 | val_0_rmse: 8.70067 | val_1_rmse: 8.66408 |  0:00:06s
epoch 2  | loss: 0.33834 | val_0_rmse: 5.60356 | val_1_rmse: 5.58804 |  0:00:09s
epoch 3  | loss: 0.25892 | val_0_rmse: 4.48517 | val_1_rmse: 4.47272 |  0:00:13s
epoch 4  | loss: 0.23563 | val_0_rmse: 2.68048 | val_1_rmse: 2.66846 |  0:00:16s
epoch 5  | loss: 0.31172 | val_0_rmse: 3.54887 | val_1_rmse: 3.52292 |  0:00:19s
epoch 6  | loss: 0.30085 | val_0_rmse: 2.08859 | val_1_rmse: 2.0755  |  0:00:23s
epoch 7  | loss: 0.29089 | val_0_rmse: 1.67382 | val_1_rmse: 1.66357 |  0:00:26s
epoch 8  | loss: 0.25221 | val_0_rmse: 0.97569 | val_1_rmse: 0.96931 |  0:00:29s
epoch 9  | loss: 0.20118 | val_0_rmse: 1.20228 | val_1_rmse: 1.19544 |  0:00:33s
epoch 10 | loss: 0.20392 | val_0_rmse: 1.06323 | val_1_rmse: 1.05736 |  0:00:36s
epoch 11 | loss: 0.18583 | val_0_rmse: 0.9332  | val_1_rmse: 0.92791 |  0:00:39s
epoch 12 | loss: 0.17035 | val_0_rmse: 0.83991 | val_1_rmse: 0.83564 |  0:00:43s
epoch 13 | loss: 0.17035 | val_0_rmse: 0.98474 | val_1_rmse: 0.97835 |  0:00:46s
epoch 14 | loss: 0.18118 | val_0_rmse: 0.80945 | val_1_rmse: 0.8052  |  0:00:49s
epoch 15 | loss: 0.20043 | val_0_rmse: 0.85444 | val_1_rmse: 0.84943 |  0:00:52s
epoch 16 | loss: 0.1653  | val_0_rmse: 0.82299 | val_1_rmse: 0.81798 |  0:00:56s
epoch 17 | loss: 0.16559 | val_0_rmse: 0.78981 | val_1_rmse: 0.78519 |  0:00:59s
epoch 18 | loss: 0.18881 | val_0_rmse: 0.89811 | val_1_rmse: 0.89348 |  0:01:02s
epoch 19 | loss: 0.18554 | val_0_rmse: 0.83578 | val_1_rmse: 0.83304 |  0:01:06s
epoch 20 | loss: 0.18093 | val_0_rmse: 0.70465 | val_1_rmse: 0.70653 |  0:01:09s
epoch 21 | loss: 0.18082 | val_0_rmse: 0.67892 | val_1_rmse: 0.68058 |  0:01:12s
epoch 22 | loss: 0.17751 | val_0_rmse: 0.73585 | val_1_rmse: 0.73184 |  0:01:16s
epoch 23 | loss: 0.17723 | val_0_rmse: 0.71044 | val_1_rmse: 0.70526 |  0:01:19s
epoch 24 | loss: 0.17454 | val_0_rmse: 0.63291 | val_1_rmse: 0.62961 |  0:01:22s
epoch 25 | loss: 0.17409 | val_0_rmse: 0.47729 | val_1_rmse: 0.48174 |  0:01:26s
epoch 26 | loss: 0.17462 | val_0_rmse: 0.42623 | val_1_rmse: 0.43012 |  0:01:29s
epoch 27 | loss: 0.17212 | val_0_rmse: 0.40037 | val_1_rmse: 0.40422 |  0:01:32s
epoch 28 | loss: 0.16914 | val_0_rmse: 0.39106 | val_1_rmse: 0.39462 |  0:01:35s
epoch 29 | loss: 0.15537 | val_0_rmse: 0.36983 | val_1_rmse: 0.37295 |  0:01:39s
epoch 30 | loss: 0.15733 | val_0_rmse: 0.41136 | val_1_rmse: 0.41602 |  0:01:42s
epoch 31 | loss: 0.19117 | val_0_rmse: 0.36832 | val_1_rmse: 0.37123 |  0:01:45s
epoch 32 | loss: 0.19662 | val_0_rmse: 0.35896 | val_1_rmse: 0.36078 |  0:01:49s
epoch 33 | loss: 0.14577 | val_0_rmse: 0.36068 | val_1_rmse: 0.36069 |  0:01:52s
epoch 34 | loss: 0.14738 | val_0_rmse: 0.39293 | val_1_rmse: 0.39435 |  0:01:55s
epoch 35 | loss: 0.16861 | val_0_rmse: 0.44124 | val_1_rmse: 0.44519 |  0:01:59s
epoch 36 | loss: 0.22084 | val_0_rmse: 0.36068 | val_1_rmse: 0.36346 |  0:02:02s
epoch 37 | loss: 0.15525 | val_0_rmse: 0.38315 | val_1_rmse: 0.38543 |  0:02:05s
epoch 38 | loss: 0.14292 | val_0_rmse: 0.35746 | val_1_rmse: 0.36038 |  0:02:09s
epoch 39 | loss: 0.21893 | val_0_rmse: 0.4411  | val_1_rmse: 0.44229 |  0:02:12s
epoch 40 | loss: 0.22038 | val_0_rmse: 0.52423 | val_1_rmse: 0.52212 |  0:02:15s
epoch 41 | loss: 0.22047 | val_0_rmse: 0.52482 | val_1_rmse: 0.52911 |  0:02:18s
epoch 42 | loss: 0.18973 | val_0_rmse: 0.38191 | val_1_rmse: 0.38537 |  0:02:22s
epoch 43 | loss: 0.14038 | val_0_rmse: 0.34736 | val_1_rmse: 0.34969 |  0:02:25s
epoch 44 | loss: 0.1413  | val_0_rmse: 0.36143 | val_1_rmse: 0.36171 |  0:02:28s
epoch 45 | loss: 0.16929 | val_0_rmse: 0.3453  | val_1_rmse: 0.34643 |  0:02:32s
epoch 46 | loss: 0.15022 | val_0_rmse: 0.36242 | val_1_rmse: 0.36239 |  0:02:35s
epoch 47 | loss: 0.13627 | val_0_rmse: 0.36652 | val_1_rmse: 0.36781 |  0:02:38s
epoch 48 | loss: 0.15152 | val_0_rmse: 0.34563 | val_1_rmse: 0.34904 |  0:02:42s
epoch 49 | loss: 0.1439  | val_0_rmse: 0.35651 | val_1_rmse: 0.363   |  0:02:45s
epoch 50 | loss: 0.14706 | val_0_rmse: 0.35259 | val_1_rmse: 0.35483 |  0:02:48s
epoch 51 | loss: 0.15043 | val_0_rmse: 0.35513 | val_1_rmse: 0.35606 |  0:02:52s
epoch 52 | loss: 0.13345 | val_0_rmse: 0.39818 | val_1_rmse: 0.40257 |  0:02:55s
epoch 53 | loss: 0.14284 | val_0_rmse: 0.34431 | val_1_rmse: 0.34766 |  0:02:58s
epoch 54 | loss: 0.17874 | val_0_rmse: 0.41437 | val_1_rmse: 0.4131  |  0:03:01s
epoch 55 | loss: 0.18101 | val_0_rmse: 0.38124 | val_1_rmse: 0.38405 |  0:03:05s
epoch 56 | loss: 0.14283 | val_0_rmse: 0.34768 | val_1_rmse: 0.35175 |  0:03:08s
epoch 57 | loss: 0.14743 | val_0_rmse: 0.34669 | val_1_rmse: 0.34864 |  0:03:11s
epoch 58 | loss: 0.13636 | val_0_rmse: 0.35306 | val_1_rmse: 0.35404 |  0:03:15s
epoch 59 | loss: 0.14274 | val_0_rmse: 0.3786  | val_1_rmse: 0.38172 |  0:03:18s
epoch 60 | loss: 0.15334 | val_0_rmse: 0.36732 | val_1_rmse: 0.37017 |  0:03:21s
epoch 61 | loss: 0.16156 | val_0_rmse: 0.34277 | val_1_rmse: 0.34551 |  0:03:25s
epoch 62 | loss: 0.15611 | val_0_rmse: 0.38972 | val_1_rmse: 0.39157 |  0:03:28s
epoch 63 | loss: 0.15625 | val_0_rmse: 0.34225 | val_1_rmse: 0.34552 |  0:03:31s
epoch 64 | loss: 0.15714 | val_0_rmse: 0.37763 | val_1_rmse: 0.38085 |  0:03:35s
epoch 65 | loss: 0.15402 | val_0_rmse: 0.37802 | val_1_rmse: 0.38333 |  0:03:38s
epoch 66 | loss: 0.15211 | val_0_rmse: 0.36277 | val_1_rmse: 0.3674  |  0:03:41s
epoch 67 | loss: 0.15152 | val_0_rmse: 0.36018 | val_1_rmse: 0.36305 |  0:03:44s
epoch 68 | loss: 0.15356 | val_0_rmse: 0.3374  | val_1_rmse: 0.34012 |  0:03:48s
epoch 69 | loss: 0.15383 | val_0_rmse: 0.37266 | val_1_rmse: 0.37709 |  0:03:51s
epoch 70 | loss: 0.15213 | val_0_rmse: 0.39429 | val_1_rmse: 0.39838 |  0:03:54s
epoch 71 | loss: 0.15115 | val_0_rmse: 0.34075 | val_1_rmse: 0.34557 |  0:03:58s
epoch 72 | loss: 0.14949 | val_0_rmse: 0.35694 | val_1_rmse: 0.36068 |  0:04:01s
epoch 73 | loss: 0.15009 | val_0_rmse: 0.37536 | val_1_rmse: 0.37796 |  0:04:04s
epoch 74 | loss: 0.1519  | val_0_rmse: 0.34457 | val_1_rmse: 0.34791 |  0:04:08s
epoch 75 | loss: 0.14756 | val_0_rmse: 0.36247 | val_1_rmse: 0.36864 |  0:04:11s
epoch 76 | loss: 0.14903 | val_0_rmse: 0.35717 | val_1_rmse: 0.36193 |  0:04:14s
epoch 77 | loss: 0.1461  | val_0_rmse: 0.35672 | val_1_rmse: 0.36135 |  0:04:17s
epoch 78 | loss: 0.14594 | val_0_rmse: 0.33844 | val_1_rmse: 0.34329 |  0:04:21s
epoch 79 | loss: 0.15019 | val_0_rmse: 0.33737 | val_1_rmse: 0.3412  |  0:04:24s
epoch 80 | loss: 0.13438 | val_0_rmse: 0.4946  | val_1_rmse: 0.49584 |  0:04:27s
epoch 81 | loss: 0.15802 | val_0_rmse: 0.35191 | val_1_rmse: 0.35551 |  0:04:31s
epoch 82 | loss: 0.14554 | val_0_rmse: 0.36407 | val_1_rmse: 0.36528 |  0:04:34s
epoch 83 | loss: 0.14613 | val_0_rmse: 0.36931 | val_1_rmse: 0.37262 |  0:04:37s
epoch 84 | loss: 0.14628 | val_0_rmse: 0.36432 | val_1_rmse: 0.36888 |  0:04:40s
epoch 85 | loss: 0.13816 | val_0_rmse: 0.33902 | val_1_rmse: 0.34425 |  0:04:44s
epoch 86 | loss: 0.13131 | val_0_rmse: 0.36616 | val_1_rmse: 0.37159 |  0:04:47s
epoch 87 | loss: 0.13386 | val_0_rmse: 0.35586 | val_1_rmse: 0.3618  |  0:04:50s
epoch 88 | loss: 0.13112 | val_0_rmse: 0.34313 | val_1_rmse: 0.34958 |  0:04:54s
epoch 89 | loss: 0.15167 | val_0_rmse: 0.41719 | val_1_rmse: 0.42428 |  0:04:57s
epoch 90 | loss: 0.17317 | val_0_rmse: 0.36583 | val_1_rmse: 0.37265 |  0:05:00s
epoch 91 | loss: 0.13169 | val_0_rmse: 0.36448 | val_1_rmse: 0.36876 |  0:05:04s
epoch 92 | loss: 0.12965 | val_0_rmse: 0.3339  | val_1_rmse: 0.33797 |  0:05:07s
epoch 93 | loss: 0.13455 | val_0_rmse: 0.42222 | val_1_rmse: 0.42648 |  0:05:10s
epoch 94 | loss: 0.1421  | val_0_rmse: 0.34339 | val_1_rmse: 0.3486  |  0:05:13s
epoch 95 | loss: 0.13234 | val_0_rmse: 0.32962 | val_1_rmse: 0.33695 |  0:05:17s
epoch 96 | loss: 0.12539 | val_0_rmse: 0.34455 | val_1_rmse: 0.35078 |  0:05:20s
epoch 97 | loss: 0.12619 | val_0_rmse: 0.35539 | val_1_rmse: 0.36187 |  0:05:23s
epoch 98 | loss: 0.13154 | val_0_rmse: 0.37924 | val_1_rmse: 0.38355 |  0:05:27s
epoch 99 | loss: 0.1363  | val_0_rmse: 0.35617 | val_1_rmse: 0.35953 |  0:05:30s
epoch 100| loss: 0.12717 | val_0_rmse: 0.32823 | val_1_rmse: 0.33524 |  0:05:33s
epoch 101| loss: 0.13382 | val_0_rmse: 0.40585 | val_1_rmse: 0.41176 |  0:05:37s
epoch 102| loss: 0.1309  | val_0_rmse: 0.33437 | val_1_rmse: 0.34115 |  0:05:40s
epoch 103| loss: 0.1246  | val_0_rmse: 0.33692 | val_1_rmse: 0.34127 |  0:05:43s
epoch 104| loss: 0.1416  | val_0_rmse: 0.37126 | val_1_rmse: 0.37825 |  0:05:47s
epoch 105| loss: 0.14447 | val_0_rmse: 0.33507 | val_1_rmse: 0.34137 |  0:05:50s
epoch 106| loss: 0.13878 | val_0_rmse: 0.33333 | val_1_rmse: 0.34087 |  0:05:53s
epoch 107| loss: 0.1305  | val_0_rmse: 0.38906 | val_1_rmse: 0.39414 |  0:05:56s
epoch 108| loss: 0.14459 | val_0_rmse: 0.34706 | val_1_rmse: 0.35133 |  0:06:00s
epoch 109| loss: 0.13159 | val_0_rmse: 0.35495 | val_1_rmse: 0.36696 |  0:06:03s
epoch 110| loss: 0.14533 | val_0_rmse: 0.32959 | val_1_rmse: 0.33699 |  0:06:06s
epoch 111| loss: 0.12257 | val_0_rmse: 0.32688 | val_1_rmse: 0.33229 |  0:06:10s
epoch 112| loss: 0.14059 | val_0_rmse: 0.35254 | val_1_rmse: 0.35889 |  0:06:13s
epoch 113| loss: 0.13698 | val_0_rmse: 0.43238 | val_1_rmse: 0.43644 |  0:06:16s
epoch 114| loss: 0.13615 | val_0_rmse: 0.33589 | val_1_rmse: 0.34344 |  0:06:20s
epoch 115| loss: 0.12544 | val_0_rmse: 0.35136 | val_1_rmse: 0.36    |  0:06:23s
epoch 116| loss: 0.15104 | val_0_rmse: 0.37718 | val_1_rmse: 0.38262 |  0:06:26s
epoch 117| loss: 0.13949 | val_0_rmse: 0.33736 | val_1_rmse: 0.34511 |  0:06:29s
epoch 118| loss: 0.1293  | val_0_rmse: 0.33944 | val_1_rmse: 0.34694 |  0:06:33s
epoch 119| loss: 0.14273 | val_0_rmse: 0.34494 | val_1_rmse: 0.35323 |  0:06:36s
epoch 120| loss: 0.12188 | val_0_rmse: 0.32922 | val_1_rmse: 0.33668 |  0:06:39s
epoch 121| loss: 0.12487 | val_0_rmse: 0.3232  | val_1_rmse: 0.33098 |  0:06:43s
epoch 122| loss: 0.1186  | val_0_rmse: 0.3309  | val_1_rmse: 0.33846 |  0:06:46s
epoch 123| loss: 0.12807 | val_0_rmse: 0.34384 | val_1_rmse: 0.35246 |  0:06:49s
epoch 124| loss: 0.12145 | val_0_rmse: 0.3563  | val_1_rmse: 0.36626 |  0:06:53s
epoch 125| loss: 0.13239 | val_0_rmse: 0.38285 | val_1_rmse: 0.39155 |  0:06:56s
epoch 126| loss: 0.12321 | val_0_rmse: 0.34929 | val_1_rmse: 0.36038 |  0:06:59s
epoch 127| loss: 0.12937 | val_0_rmse: 0.32248 | val_1_rmse: 0.3334  |  0:07:03s
epoch 128| loss: 0.1319  | val_0_rmse: 0.36036 | val_1_rmse: 0.36898 |  0:07:06s
epoch 129| loss: 0.11932 | val_0_rmse: 0.32172 | val_1_rmse: 0.32706 |  0:07:09s
epoch 130| loss: 0.14627 | val_0_rmse: 0.31958 | val_1_rmse: 0.32758 |  0:07:12s
epoch 131| loss: 0.12196 | val_0_rmse: 0.33444 | val_1_rmse: 0.34351 |  0:07:16s
epoch 132| loss: 0.1273  | val_0_rmse: 0.3249  | val_1_rmse: 0.33414 |  0:07:19s
epoch 133| loss: 0.11631 | val_0_rmse: 0.32393 | val_1_rmse: 0.32974 |  0:07:22s
epoch 134| loss: 0.11753 | val_0_rmse: 0.36723 | val_1_rmse: 0.37622 |  0:07:26s
epoch 135| loss: 0.13784 | val_0_rmse: 0.33751 | val_1_rmse: 0.34511 |  0:07:29s
epoch 136| loss: 0.13981 | val_0_rmse: 0.44812 | val_1_rmse: 0.45282 |  0:07:32s
epoch 137| loss: 0.15311 | val_0_rmse: 0.32359 | val_1_rmse: 0.33478 |  0:07:36s
epoch 138| loss: 0.14825 | val_0_rmse: 0.35805 | val_1_rmse: 0.36576 |  0:07:39s
epoch 139| loss: 0.14013 | val_0_rmse: 0.34938 | val_1_rmse: 0.35925 |  0:07:42s
epoch 140| loss: 0.18053 | val_0_rmse: 0.37293 | val_1_rmse: 0.38266 |  0:07:46s
epoch 141| loss: 0.18225 | val_0_rmse: 0.41132 | val_1_rmse: 0.41855 |  0:07:49s
epoch 142| loss: 0.1617  | val_0_rmse: 0.31599 | val_1_rmse: 0.32866 |  0:07:52s
epoch 143| loss: 0.13313 | val_0_rmse: 0.33572 | val_1_rmse: 0.34764 |  0:07:55s
epoch 144| loss: 0.12352 | val_0_rmse: 0.33277 | val_1_rmse: 0.34561 |  0:07:59s
epoch 145| loss: 0.11584 | val_0_rmse: 0.32908 | val_1_rmse: 0.33814 |  0:08:02s
epoch 146| loss: 0.11716 | val_0_rmse: 0.35024 | val_1_rmse: 0.3604  |  0:08:05s
epoch 147| loss: 0.12771 | val_0_rmse: 0.31975 | val_1_rmse: 0.33116 |  0:08:09s
epoch 148| loss: 0.13899 | val_0_rmse: 0.34194 | val_1_rmse: 0.35047 |  0:08:12s
epoch 149| loss: 0.12661 | val_0_rmse: 0.32856 | val_1_rmse: 0.34078 |  0:08:15s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.32706
Best weights from best epoch are automatically used!
ended training at: 18:05:38
Feature importance:
[('Area', 0.4028547741491006), ('Baths', 0.025873209793564424), ('Beds', 0.01671362914448753), ('Latitude', 0.07622879468812994), ('Longitude', 0.1329693117755877), ('Month', 0.0), ('Year', 0.34536028044912975)]
Mean squared error is of 10407125800.58807
Mean absolute error:70294.5369391082
MAPE:0.26289270803101616
R2 score:0.819743911892387
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:05:38
epoch 0  | loss: 21.61501| val_0_rmse: 2.01883 | val_1_rmse: 2.02476 |  0:00:03s
epoch 1  | loss: 0.43622 | val_0_rmse: 6.81296 | val_1_rmse: 6.80441 |  0:00:06s
epoch 2  | loss: 0.30463 | val_0_rmse: 9.16281 | val_1_rmse: 9.13794 |  0:00:09s
epoch 3  | loss: 0.26165 | val_0_rmse: 6.17313 | val_1_rmse: 6.13762 |  0:00:13s
epoch 4  | loss: 0.21643 | val_0_rmse: 1.76818 | val_1_rmse: 1.76771 |  0:00:16s
epoch 5  | loss: 0.24293 | val_0_rmse: 1.17706 | val_1_rmse: 1.17025 |  0:00:19s
epoch 6  | loss: 0.2273  | val_0_rmse: 0.87485 | val_1_rmse: 0.85737 |  0:00:23s
epoch 7  | loss: 0.21259 | val_0_rmse: 0.96267 | val_1_rmse: 0.94132 |  0:00:26s
epoch 8  | loss: 0.22378 | val_0_rmse: 1.42035 | val_1_rmse: 1.40098 |  0:00:29s
epoch 9  | loss: 0.24672 | val_0_rmse: 1.26791 | val_1_rmse: 1.25437 |  0:00:33s
epoch 10 | loss: 0.21195 | val_0_rmse: 0.91229 | val_1_rmse: 0.89969 |  0:00:36s
epoch 11 | loss: 0.20155 | val_0_rmse: 0.80637 | val_1_rmse: 0.79553 |  0:00:39s
epoch 12 | loss: 0.2     | val_0_rmse: 0.84652 | val_1_rmse: 0.84004 |  0:00:42s
epoch 13 | loss: 0.20032 | val_0_rmse: 0.85998 | val_1_rmse: 0.84753 |  0:00:46s
epoch 14 | loss: 0.1935  | val_0_rmse: 0.9051  | val_1_rmse: 0.896   |  0:00:49s
epoch 15 | loss: 0.19258 | val_0_rmse: 0.91025 | val_1_rmse: 0.9001  |  0:00:52s
epoch 16 | loss: 0.17837 | val_0_rmse: 1.02014 | val_1_rmse: 1.01047 |  0:00:56s
epoch 17 | loss: 0.20364 | val_0_rmse: 0.81317 | val_1_rmse: 0.804   |  0:00:59s
epoch 18 | loss: 0.1828  | val_0_rmse: 0.71724 | val_1_rmse: 0.70871 |  0:01:02s
epoch 19 | loss: 0.16929 | val_0_rmse: 0.74982 | val_1_rmse: 0.74136 |  0:01:06s
epoch 20 | loss: 0.23443 | val_0_rmse: 0.77888 | val_1_rmse: 0.77114 |  0:01:09s
epoch 21 | loss: 0.18602 | val_0_rmse: 0.66161 | val_1_rmse: 0.65575 |  0:01:12s
epoch 22 | loss: 0.15192 | val_0_rmse: 0.61759 | val_1_rmse: 0.61261 |  0:01:15s
epoch 23 | loss: 0.15197 | val_0_rmse: 0.58404 | val_1_rmse: 0.57776 |  0:01:19s
epoch 24 | loss: 0.17755 | val_0_rmse: 0.60055 | val_1_rmse: 0.59624 |  0:01:22s
epoch 25 | loss: 0.18428 | val_0_rmse: 0.5741  | val_1_rmse: 0.57347 |  0:01:25s
epoch 26 | loss: 0.17089 | val_0_rmse: 0.48153 | val_1_rmse: 0.47955 |  0:01:29s
epoch 27 | loss: 0.15299 | val_0_rmse: 0.46762 | val_1_rmse: 0.46587 |  0:01:32s
epoch 28 | loss: 0.18457 | val_0_rmse: 0.44036 | val_1_rmse: 0.43916 |  0:01:35s
epoch 29 | loss: 0.16373 | val_0_rmse: 0.39177 | val_1_rmse: 0.39267 |  0:01:39s
epoch 30 | loss: 0.15998 | val_0_rmse: 0.37112 | val_1_rmse: 0.37346 |  0:01:42s
epoch 31 | loss: 0.1455  | val_0_rmse: 0.36519 | val_1_rmse: 0.36633 |  0:01:45s
epoch 32 | loss: 0.21727 | val_0_rmse: 0.38877 | val_1_rmse: 0.39017 |  0:01:49s
epoch 33 | loss: 0.15051 | val_0_rmse: 0.35381 | val_1_rmse: 0.35657 |  0:01:52s
epoch 34 | loss: 0.14273 | val_0_rmse: 0.43787 | val_1_rmse: 0.44553 |  0:01:55s
epoch 35 | loss: 0.14957 | val_0_rmse: 0.3608  | val_1_rmse: 0.36741 |  0:01:58s
epoch 36 | loss: 0.16044 | val_0_rmse: 0.43491 | val_1_rmse: 0.44133 |  0:02:02s
epoch 37 | loss: 0.1573  | val_0_rmse: 0.35471 | val_1_rmse: 0.3563  |  0:02:05s
epoch 38 | loss: 0.17144 | val_0_rmse: 0.35742 | val_1_rmse: 0.36169 |  0:02:08s
epoch 39 | loss: 0.18784 | val_0_rmse: 0.43615 | val_1_rmse: 0.43618 |  0:02:12s
epoch 40 | loss: 0.22462 | val_0_rmse: 0.3662  | val_1_rmse: 0.36857 |  0:02:15s
epoch 41 | loss: 0.20665 | val_0_rmse: 0.44392 | val_1_rmse: 0.45157 |  0:02:18s
epoch 42 | loss: 0.20648 | val_0_rmse: 0.46633 | val_1_rmse: 0.46558 |  0:02:22s
epoch 43 | loss: 0.20641 | val_0_rmse: 0.39218 | val_1_rmse: 0.40255 |  0:02:25s
epoch 44 | loss: 0.14769 | val_0_rmse: 0.42653 | val_1_rmse: 0.43045 |  0:02:28s
epoch 45 | loss: 0.14016 | val_0_rmse: 0.35017 | val_1_rmse: 0.35118 |  0:02:32s
epoch 46 | loss: 0.14746 | val_0_rmse: 0.36576 | val_1_rmse: 0.37078 |  0:02:35s
epoch 47 | loss: 0.14504 | val_0_rmse: 0.37958 | val_1_rmse: 0.38832 |  0:02:38s
epoch 48 | loss: 0.14211 | val_0_rmse: 0.36474 | val_1_rmse: 0.3641  |  0:02:41s
epoch 49 | loss: 0.13671 | val_0_rmse: 0.34918 | val_1_rmse: 0.35287 |  0:02:45s
epoch 50 | loss: 0.14048 | val_0_rmse: 0.34715 | val_1_rmse: 0.34979 |  0:02:48s
epoch 51 | loss: 0.13524 | val_0_rmse: 0.35034 | val_1_rmse: 0.35629 |  0:02:51s
epoch 52 | loss: 0.13351 | val_0_rmse: 0.35708 | val_1_rmse: 0.36319 |  0:02:55s
epoch 53 | loss: 0.14248 | val_0_rmse: 0.35958 | val_1_rmse: 0.36236 |  0:02:58s
epoch 54 | loss: 0.13419 | val_0_rmse: 0.34715 | val_1_rmse: 0.35523 |  0:03:01s
epoch 55 | loss: 0.14859 | val_0_rmse: 0.3981  | val_1_rmse: 0.4029  |  0:03:05s
epoch 56 | loss: 0.13504 | val_0_rmse: 0.33767 | val_1_rmse: 0.33989 |  0:03:08s
epoch 57 | loss: 0.12793 | val_0_rmse: 0.34099 | val_1_rmse: 0.34799 |  0:03:11s
epoch 58 | loss: 0.20557 | val_0_rmse: 0.36414 | val_1_rmse: 0.37204 |  0:03:15s
epoch 59 | loss: 0.2101  | val_0_rmse: 0.3467  | val_1_rmse: 0.35349 |  0:03:18s
epoch 60 | loss: 0.161   | val_0_rmse: 0.35476 | val_1_rmse: 0.35611 |  0:03:21s
epoch 61 | loss: 0.13225 | val_0_rmse: 0.36786 | val_1_rmse: 0.37342 |  0:03:24s
epoch 62 | loss: 0.13197 | val_0_rmse: 0.34718 | val_1_rmse: 0.35384 |  0:03:28s
epoch 63 | loss: 0.13293 | val_0_rmse: 0.37706 | val_1_rmse: 0.38533 |  0:03:31s
epoch 64 | loss: 0.13878 | val_0_rmse: 0.3403  | val_1_rmse: 0.3456  |  0:03:34s
epoch 65 | loss: 0.18875 | val_0_rmse: 0.3434  | val_1_rmse: 0.34774 |  0:03:38s
epoch 66 | loss: 0.12936 | val_0_rmse: 0.3338  | val_1_rmse: 0.33882 |  0:03:41s
epoch 67 | loss: 0.14129 | val_0_rmse: 0.34163 | val_1_rmse: 0.3445  |  0:03:44s
epoch 68 | loss: 0.16027 | val_0_rmse: 0.34832 | val_1_rmse: 0.35174 |  0:03:48s
epoch 69 | loss: 0.13449 | val_0_rmse: 0.34325 | val_1_rmse: 0.34732 |  0:03:51s
epoch 70 | loss: 0.16098 | val_0_rmse: 0.36837 | val_1_rmse: 0.36938 |  0:03:54s
epoch 71 | loss: 0.17073 | val_0_rmse: 0.34167 | val_1_rmse: 0.34729 |  0:03:57s
epoch 72 | loss: 0.12747 | val_0_rmse: 0.34703 | val_1_rmse: 0.34823 |  0:04:01s
epoch 73 | loss: 0.14713 | val_0_rmse: 0.36499 | val_1_rmse: 0.37574 |  0:04:04s
epoch 74 | loss: 0.17142 | val_0_rmse: 0.42767 | val_1_rmse: 0.43595 |  0:04:07s
epoch 75 | loss: 0.20012 | val_0_rmse: 0.43324 | val_1_rmse: 0.43349 |  0:04:11s
epoch 76 | loss: 0.17271 | val_0_rmse: 0.33459 | val_1_rmse: 0.33884 |  0:04:14s
epoch 77 | loss: 0.15109 | val_0_rmse: 0.36803 | val_1_rmse: 0.37772 |  0:04:17s
epoch 78 | loss: 0.13152 | val_0_rmse: 0.34008 | val_1_rmse: 0.34239 |  0:04:21s
epoch 79 | loss: 0.14997 | val_0_rmse: 0.34525 | val_1_rmse: 0.34993 |  0:04:24s
epoch 80 | loss: 0.12522 | val_0_rmse: 0.35229 | val_1_rmse: 0.35731 |  0:04:27s
epoch 81 | loss: 0.13627 | val_0_rmse: 0.35192 | val_1_rmse: 0.3601  |  0:04:31s
epoch 82 | loss: 0.1397  | val_0_rmse: 0.35955 | val_1_rmse: 0.36362 |  0:04:34s
epoch 83 | loss: 0.16036 | val_0_rmse: 0.42605 | val_1_rmse: 0.43289 |  0:04:37s
epoch 84 | loss: 0.19463 | val_0_rmse: 0.3505  | val_1_rmse: 0.35603 |  0:04:40s
epoch 85 | loss: 0.14137 | val_0_rmse: 0.34835 | val_1_rmse: 0.35433 |  0:04:44s
epoch 86 | loss: 0.14634 | val_0_rmse: 0.36148 | val_1_rmse: 0.36978 |  0:04:47s
epoch 87 | loss: 0.15208 | val_0_rmse: 0.34427 | val_1_rmse: 0.35132 |  0:04:50s
epoch 88 | loss: 0.14363 | val_0_rmse: 0.33304 | val_1_rmse: 0.33727 |  0:04:54s
epoch 89 | loss: 0.13405 | val_0_rmse: 0.34129 | val_1_rmse: 0.34806 |  0:04:57s
epoch 90 | loss: 0.1277  | val_0_rmse: 0.37412 | val_1_rmse: 0.3748  |  0:05:00s
epoch 91 | loss: 0.15489 | val_0_rmse: 0.5049  | val_1_rmse: 0.50155 |  0:05:04s
epoch 92 | loss: 0.15492 | val_0_rmse: 0.33135 | val_1_rmse: 0.3376  |  0:05:07s
epoch 93 | loss: 0.1673  | val_0_rmse: 0.33575 | val_1_rmse: 0.34192 |  0:05:10s
epoch 94 | loss: 0.14616 | val_0_rmse: 0.33532 | val_1_rmse: 0.33814 |  0:05:13s
epoch 95 | loss: 0.1324  | val_0_rmse: 0.33665 | val_1_rmse: 0.34066 |  0:05:17s
epoch 96 | loss: 0.13158 | val_0_rmse: 0.36547 | val_1_rmse: 0.36643 |  0:05:20s
epoch 97 | loss: 0.14609 | val_0_rmse: 0.37886 | val_1_rmse: 0.38072 |  0:05:23s
epoch 98 | loss: 0.14116 | val_0_rmse: 0.34107 | val_1_rmse: 0.34554 |  0:05:27s
epoch 99 | loss: 0.13089 | val_0_rmse: 0.33499 | val_1_rmse: 0.34286 |  0:05:30s
epoch 100| loss: 0.13324 | val_0_rmse: 0.34544 | val_1_rmse: 0.35048 |  0:05:33s
epoch 101| loss: 0.13106 | val_0_rmse: 0.33833 | val_1_rmse: 0.34391 |  0:05:37s
epoch 102| loss: 0.14433 | val_0_rmse: 0.34153 | val_1_rmse: 0.34589 |  0:05:40s
epoch 103| loss: 0.12993 | val_0_rmse: 0.3343  | val_1_rmse: 0.33878 |  0:05:43s
epoch 104| loss: 0.13329 | val_0_rmse: 0.3514  | val_1_rmse: 0.36091 |  0:05:47s
epoch 105| loss: 0.12561 | val_0_rmse: 0.33272 | val_1_rmse: 0.33747 |  0:05:50s
epoch 106| loss: 0.13699 | val_0_rmse: 0.33415 | val_1_rmse: 0.33933 |  0:05:53s
epoch 107| loss: 0.12936 | val_0_rmse: 0.34114 | val_1_rmse: 0.34694 |  0:05:56s
epoch 108| loss: 0.14087 | val_0_rmse: 0.38376 | val_1_rmse: 0.38427 |  0:06:00s
epoch 109| loss: 0.16053 | val_0_rmse: 0.38841 | val_1_rmse: 0.387   |  0:06:03s
epoch 110| loss: 0.17294 | val_0_rmse: 0.33631 | val_1_rmse: 0.33941 |  0:06:06s
epoch 111| loss: 0.13203 | val_0_rmse: 0.35268 | val_1_rmse: 0.35537 |  0:06:10s
epoch 112| loss: 0.15092 | val_0_rmse: 0.36955 | val_1_rmse: 0.37091 |  0:06:13s
epoch 113| loss: 0.13038 | val_0_rmse: 0.33695 | val_1_rmse: 0.34166 |  0:06:16s
epoch 114| loss: 0.14228 | val_0_rmse: 0.35348 | val_1_rmse: 0.35505 |  0:06:20s
epoch 115| loss: 0.14392 | val_0_rmse: 0.37137 | val_1_rmse: 0.37283 |  0:06:23s
epoch 116| loss: 0.14243 | val_0_rmse: 0.35174 | val_1_rmse: 0.35289 |  0:06:26s
epoch 117| loss: 0.14079 | val_0_rmse: 0.34149 | val_1_rmse: 0.34978 |  0:06:29s
epoch 118| loss: 0.15217 | val_0_rmse: 0.33286 | val_1_rmse: 0.33553 |  0:06:33s
epoch 119| loss: 0.12152 | val_0_rmse: 0.32668 | val_1_rmse: 0.33147 |  0:06:36s
epoch 120| loss: 0.18646 | val_0_rmse: 0.3545  | val_1_rmse: 0.36172 |  0:06:39s
epoch 121| loss: 0.16488 | val_0_rmse: 0.33993 | val_1_rmse: 0.34714 |  0:06:43s
epoch 122| loss: 0.14003 | val_0_rmse: 0.33753 | val_1_rmse: 0.34059 |  0:06:46s
epoch 123| loss: 0.12639 | val_0_rmse: 0.35549 | val_1_rmse: 0.36223 |  0:06:49s
epoch 124| loss: 0.12525 | val_0_rmse: 0.37395 | val_1_rmse: 0.3779  |  0:06:53s
epoch 125| loss: 0.14354 | val_0_rmse: 0.36646 | val_1_rmse: 0.37425 |  0:06:56s
epoch 126| loss: 0.14127 | val_0_rmse: 0.34407 | val_1_rmse: 0.34836 |  0:06:59s
epoch 127| loss: 0.1418  | val_0_rmse: 0.35305 | val_1_rmse: 0.35873 |  0:07:02s
epoch 128| loss: 0.15525 | val_0_rmse: 0.32926 | val_1_rmse: 0.337   |  0:07:06s
epoch 129| loss: 0.13072 | val_0_rmse: 0.33705 | val_1_rmse: 0.34345 |  0:07:09s
epoch 130| loss: 0.13669 | val_0_rmse: 0.34113 | val_1_rmse: 0.34766 |  0:07:12s
epoch 131| loss: 0.12489 | val_0_rmse: 0.33111 | val_1_rmse: 0.33686 |  0:07:16s
epoch 132| loss: 0.16309 | val_0_rmse: 0.35745 | val_1_rmse: 0.36502 |  0:07:19s
epoch 133| loss: 0.12469 | val_0_rmse: 0.34102 | val_1_rmse: 0.3463  |  0:07:22s
epoch 134| loss: 0.12575 | val_0_rmse: 0.36316 | val_1_rmse: 0.36853 |  0:07:26s
epoch 135| loss: 0.14632 | val_0_rmse: 0.33383 | val_1_rmse: 0.33919 |  0:07:29s
epoch 136| loss: 0.12175 | val_0_rmse: 0.32893 | val_1_rmse: 0.33525 |  0:07:32s
epoch 137| loss: 0.12633 | val_0_rmse: 0.32918 | val_1_rmse: 0.33304 |  0:07:35s
epoch 138| loss: 0.11913 | val_0_rmse: 0.35301 | val_1_rmse: 0.36099 |  0:07:39s
epoch 139| loss: 0.13732 | val_0_rmse: 0.33818 | val_1_rmse: 0.34318 |  0:07:42s
epoch 140| loss: 0.14212 | val_0_rmse: 0.35527 | val_1_rmse: 0.36072 |  0:07:45s
epoch 141| loss: 0.13571 | val_0_rmse: 0.3262  | val_1_rmse: 0.33358 |  0:07:49s
epoch 142| loss: 0.1357  | val_0_rmse: 0.33698 | val_1_rmse: 0.34288 |  0:07:52s
epoch 143| loss: 0.13865 | val_0_rmse: 0.36297 | val_1_rmse: 0.37087 |  0:07:55s
epoch 144| loss: 0.13797 | val_0_rmse: 0.33032 | val_1_rmse: 0.33944 |  0:07:58s
epoch 145| loss: 0.13813 | val_0_rmse: 0.33357 | val_1_rmse: 0.33891 |  0:08:02s
epoch 146| loss: 0.13609 | val_0_rmse: 0.36892 | val_1_rmse: 0.37158 |  0:08:05s
epoch 147| loss: 0.13547 | val_0_rmse: 0.34332 | val_1_rmse: 0.34729 |  0:08:08s
epoch 148| loss: 0.13984 | val_0_rmse: 0.33107 | val_1_rmse: 0.33749 |  0:08:12s
epoch 149| loss: 0.12628 | val_0_rmse: 0.3326  | val_1_rmse: 0.33815 |  0:08:15s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.33147
Best weights from best epoch are automatically used!
ended training at: 18:13:55
Feature importance:
[('Area', 0.2175939600298767), ('Baths', 0.3099182615814238), ('Beds', 0.03911085805281594), ('Latitude', 0.14866664959339543), ('Longitude', 0.17905082434048758), ('Month', 0.0), ('Year', 0.10565944640200058)]
Mean squared error is of 10731737174.367605
Mean absolute error:70541.86713571603
MAPE:0.25681506452570707
R2 score:0.8143710582715327
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:13:57
epoch 0  | loss: 73.13591| val_0_rmse: 16.45817| val_1_rmse: 16.45748|  0:00:00s
epoch 1  | loss: 3.39675 | val_0_rmse: 11.08661| val_1_rmse: 11.07041|  0:00:01s
epoch 2  | loss: 0.40756 | val_0_rmse: 0.77341 | val_1_rmse: 0.77924 |  0:00:02s
epoch 3  | loss: 0.2524  | val_0_rmse: 4.81126 | val_1_rmse: 4.81471 |  0:00:03s
epoch 4  | loss: 0.20318 | val_0_rmse: 3.86833 | val_1_rmse: 3.87186 |  0:00:04s
epoch 5  | loss: 0.20009 | val_0_rmse: 3.90869 | val_1_rmse: 3.91222 |  0:00:05s
epoch 6  | loss: 0.16304 | val_0_rmse: 4.12918 | val_1_rmse: 4.13267 |  0:00:06s
epoch 7  | loss: 0.14334 | val_0_rmse: 3.69236 | val_1_rmse: 3.6959  |  0:00:07s
epoch 8  | loss: 0.10903 | val_0_rmse: 3.44234 | val_1_rmse: 3.44592 |  0:00:08s
epoch 9  | loss: 0.10593 | val_0_rmse: 2.22763 | val_1_rmse: 2.23164 |  0:00:10s
epoch 10 | loss: 0.09262 | val_0_rmse: 2.14368 | val_1_rmse: 2.14695 |  0:00:10s
epoch 11 | loss: 0.13671 | val_0_rmse: 2.95875 | val_1_rmse: 2.98887 |  0:00:12s
epoch 12 | loss: 0.10371 | val_0_rmse: 2.08538 | val_1_rmse: 2.103   |  0:00:13s
epoch 13 | loss: 0.09127 | val_0_rmse: 1.27635 | val_1_rmse: 1.2974  |  0:00:14s
epoch 14 | loss: 0.0836  | val_0_rmse: 2.09427 | val_1_rmse: 2.12676 |  0:00:15s
epoch 15 | loss: 0.12322 | val_0_rmse: 2.5548  | val_1_rmse: 2.58873 |  0:00:16s
epoch 16 | loss: 0.1093  | val_0_rmse: 1.79001 | val_1_rmse: 1.81729 |  0:00:17s
epoch 17 | loss: 0.10408 | val_0_rmse: 1.314   | val_1_rmse: 1.33905 |  0:00:18s
epoch 18 | loss: 0.10268 | val_0_rmse: 1.24554 | val_1_rmse: 1.27004 |  0:00:19s
epoch 19 | loss: 0.09745 | val_0_rmse: 1.20582 | val_1_rmse: 1.23066 |  0:00:20s
epoch 20 | loss: 0.10137 | val_0_rmse: 1.19292 | val_1_rmse: 1.21792 |  0:00:21s
epoch 21 | loss: 0.09986 | val_0_rmse: 0.98698 | val_1_rmse: 1.01063 |  0:00:22s
epoch 22 | loss: 0.09823 | val_0_rmse: 0.58084 | val_1_rmse: 0.59802 |  0:00:23s
epoch 23 | loss: 0.10231 | val_0_rmse: 0.4973  | val_1_rmse: 0.51235 |  0:00:24s
epoch 24 | loss: 0.10514 | val_0_rmse: 0.51201 | val_1_rmse: 0.52381 |  0:00:25s
epoch 25 | loss: 0.10425 | val_0_rmse: 0.72376 | val_1_rmse: 0.73866 |  0:00:26s
epoch 26 | loss: 0.10124 | val_0_rmse: 0.72829 | val_1_rmse: 0.74176 |  0:00:26s
epoch 27 | loss: 0.10194 | val_0_rmse: 0.69163 | val_1_rmse: 0.70432 |  0:00:27s
epoch 28 | loss: 0.10076 | val_0_rmse: 0.48604 | val_1_rmse: 0.49659 |  0:00:28s
epoch 29 | loss: 0.09408 | val_0_rmse: 0.42598 | val_1_rmse: 0.43306 |  0:00:29s
epoch 30 | loss: 0.09725 | val_0_rmse: 0.51084 | val_1_rmse: 0.51679 |  0:00:30s
epoch 31 | loss: 0.09608 | val_0_rmse: 0.7156  | val_1_rmse: 0.72683 |  0:00:31s
epoch 32 | loss: 0.092   | val_0_rmse: 0.73443 | val_1_rmse: 0.745   |  0:00:32s
epoch 33 | loss: 0.0972  | val_0_rmse: 0.6677  | val_1_rmse: 0.67832 |  0:00:33s
epoch 34 | loss: 0.09453 | val_0_rmse: 0.50664 | val_1_rmse: 0.5158  |  0:00:34s
epoch 35 | loss: 0.09555 | val_0_rmse: 0.44943 | val_1_rmse: 0.45934 |  0:00:35s
epoch 36 | loss: 0.09274 | val_0_rmse: 0.48163 | val_1_rmse: 0.49331 |  0:00:36s
epoch 37 | loss: 0.08752 | val_0_rmse: 0.65566 | val_1_rmse: 0.66923 |  0:00:37s
epoch 38 | loss: 0.08848 | val_0_rmse: 0.70463 | val_1_rmse: 0.71633 |  0:00:38s
epoch 39 | loss: 0.09409 | val_0_rmse: 0.57232 | val_1_rmse: 0.5826  |  0:00:39s
epoch 40 | loss: 0.09059 | val_0_rmse: 0.39567 | val_1_rmse: 0.40416 |  0:00:40s
epoch 41 | loss: 0.09398 | val_0_rmse: 0.39698 | val_1_rmse: 0.40585 |  0:00:41s
epoch 42 | loss: 0.09501 | val_0_rmse: 0.43276 | val_1_rmse: 0.44238 |  0:00:42s
epoch 43 | loss: 0.09279 | val_0_rmse: 0.59197 | val_1_rmse: 0.60119 |  0:00:43s
epoch 44 | loss: 0.09625 | val_0_rmse: 0.36993 | val_1_rmse: 0.37802 |  0:00:44s
epoch 45 | loss: 0.09446 | val_0_rmse: 0.35196 | val_1_rmse: 0.35784 |  0:00:45s
epoch 46 | loss: 0.09276 | val_0_rmse: 0.39212 | val_1_rmse: 0.40152 |  0:00:46s
epoch 47 | loss: 0.08786 | val_0_rmse: 0.48234 | val_1_rmse: 0.49236 |  0:00:47s
epoch 48 | loss: 0.14542 | val_0_rmse: 0.50902 | val_1_rmse: 0.51927 |  0:00:48s
epoch 49 | loss: 0.09028 | val_0_rmse: 0.46283 | val_1_rmse: 0.47291 |  0:00:49s
epoch 50 | loss: 0.07312 | val_0_rmse: 0.43095 | val_1_rmse: 0.44125 |  0:00:50s
epoch 51 | loss: 0.07261 | val_0_rmse: 0.34964 | val_1_rmse: 0.35636 |  0:00:51s
epoch 52 | loss: 0.09348 | val_0_rmse: 0.34782 | val_1_rmse: 0.35791 |  0:00:52s
epoch 53 | loss: 0.08874 | val_0_rmse: 0.5127  | val_1_rmse: 0.52393 |  0:00:53s
epoch 54 | loss: 0.0921  | val_0_rmse: 0.48514 | val_1_rmse: 0.49605 |  0:00:54s
epoch 55 | loss: 0.08927 | val_0_rmse: 0.39251 | val_1_rmse: 0.40177 |  0:00:55s
epoch 56 | loss: 0.09006 | val_0_rmse: 0.29671 | val_1_rmse: 0.2996  |  0:00:56s
epoch 57 | loss: 0.06969 | val_0_rmse: 0.48204 | val_1_rmse: 0.49288 |  0:00:57s
epoch 58 | loss: 0.12264 | val_0_rmse: 0.42176 | val_1_rmse: 0.42622 |  0:00:58s
epoch 59 | loss: 0.10435 | val_0_rmse: 0.39073 | val_1_rmse: 0.40072 |  0:00:59s
epoch 60 | loss: 0.08584 | val_0_rmse: 0.3104  | val_1_rmse: 0.31581 |  0:01:00s
epoch 61 | loss: 0.1006  | val_0_rmse: 0.38147 | val_1_rmse: 0.38704 |  0:01:01s
epoch 62 | loss: 0.07821 | val_0_rmse: 0.29872 | val_1_rmse: 0.30215 |  0:01:02s
epoch 63 | loss: 0.07138 | val_0_rmse: 0.2958  | val_1_rmse: 0.30036 |  0:01:03s
epoch 64 | loss: 0.06714 | val_0_rmse: 0.27182 | val_1_rmse: 0.27261 |  0:01:04s
epoch 65 | loss: 0.13906 | val_0_rmse: 0.27518 | val_1_rmse: 0.28013 |  0:01:05s
epoch 66 | loss: 0.09283 | val_0_rmse: 0.27045 | val_1_rmse: 0.27222 |  0:01:06s
epoch 67 | loss: 0.08827 | val_0_rmse: 0.26451 | val_1_rmse: 0.26753 |  0:01:07s
epoch 68 | loss: 0.06786 | val_0_rmse: 0.29369 | val_1_rmse: 0.29815 |  0:01:08s
epoch 69 | loss: 0.07497 | val_0_rmse: 0.32572 | val_1_rmse: 0.33298 |  0:01:09s
epoch 70 | loss: 0.06784 | val_0_rmse: 0.30034 | val_1_rmse: 0.3028  |  0:01:10s
epoch 71 | loss: 0.07038 | val_0_rmse: 0.27165 | val_1_rmse: 0.27438 |  0:01:11s
epoch 72 | loss: 0.07075 | val_0_rmse: 0.27577 | val_1_rmse: 0.27994 |  0:01:12s
epoch 73 | loss: 0.07312 | val_0_rmse: 0.32975 | val_1_rmse: 0.33498 |  0:01:13s
epoch 74 | loss: 0.07299 | val_0_rmse: 0.29121 | val_1_rmse: 0.29464 |  0:01:14s
epoch 75 | loss: 0.07044 | val_0_rmse: 0.26264 | val_1_rmse: 0.26448 |  0:01:15s
epoch 76 | loss: 0.06368 | val_0_rmse: 0.28915 | val_1_rmse: 0.29391 |  0:01:16s
epoch 77 | loss: 0.06782 | val_0_rmse: 0.26728 | val_1_rmse: 0.26695 |  0:01:17s
epoch 78 | loss: 0.07311 | val_0_rmse: 0.29977 | val_1_rmse: 0.30623 |  0:01:17s
epoch 79 | loss: 0.05523 | val_0_rmse: 0.27098 | val_1_rmse: 0.27439 |  0:01:18s
epoch 80 | loss: 0.07603 | val_0_rmse: 0.34127 | val_1_rmse: 0.34591 |  0:01:19s
epoch 81 | loss: 0.08664 | val_0_rmse: 0.28978 | val_1_rmse: 0.29384 |  0:01:20s
epoch 82 | loss: 0.08357 | val_0_rmse: 0.24172 | val_1_rmse: 0.24048 |  0:01:21s
epoch 83 | loss: 0.0858  | val_0_rmse: 0.28169 | val_1_rmse: 0.27875 |  0:01:22s
epoch 84 | loss: 0.0863  | val_0_rmse: 0.25215 | val_1_rmse: 0.25076 |  0:01:23s
epoch 85 | loss: 0.08435 | val_0_rmse: 0.28828 | val_1_rmse: 0.29165 |  0:01:24s
epoch 86 | loss: 0.08574 | val_0_rmse: 0.33244 | val_1_rmse: 0.33771 |  0:01:25s
epoch 87 | loss: 0.08908 | val_0_rmse: 0.26157 | val_1_rmse: 0.26584 |  0:01:26s
epoch 88 | loss: 0.08516 | val_0_rmse: 0.26371 | val_1_rmse: 0.26083 |  0:01:27s
epoch 89 | loss: 0.0877  | val_0_rmse: 0.29771 | val_1_rmse: 0.2924  |  0:01:28s
epoch 90 | loss: 0.08713 | val_0_rmse: 0.25096 | val_1_rmse: 0.24847 |  0:01:29s
epoch 91 | loss: 0.08284 | val_0_rmse: 0.25559 | val_1_rmse: 0.25906 |  0:01:30s
epoch 92 | loss: 0.08581 | val_0_rmse: 0.28461 | val_1_rmse: 0.29008 |  0:01:31s
epoch 93 | loss: 0.08868 | val_0_rmse: 0.23809 | val_1_rmse: 0.24027 |  0:01:32s
epoch 94 | loss: 0.08704 | val_0_rmse: 0.23592 | val_1_rmse: 0.23406 |  0:01:33s
epoch 95 | loss: 0.08782 | val_0_rmse: 0.25475 | val_1_rmse: 0.25049 |  0:01:34s
epoch 96 | loss: 0.08498 | val_0_rmse: 0.23169 | val_1_rmse: 0.23045 |  0:01:35s
epoch 97 | loss: 0.08208 | val_0_rmse: 0.2748  | val_1_rmse: 0.27828 |  0:01:36s
epoch 98 | loss: 0.08403 | val_0_rmse: 0.31622 | val_1_rmse: 0.32119 |  0:01:37s
epoch 99 | loss: 0.08784 | val_0_rmse: 0.25919 | val_1_rmse: 0.26187 |  0:01:38s
epoch 100| loss: 0.08285 | val_0_rmse: 0.2527  | val_1_rmse: 0.25035 |  0:01:39s
epoch 101| loss: 0.08436 | val_0_rmse: 0.31002 | val_1_rmse: 0.30599 |  0:01:40s
epoch 102| loss: 0.08622 | val_0_rmse: 0.26961 | val_1_rmse: 0.26734 |  0:01:41s
epoch 103| loss: 0.0844  | val_0_rmse: 0.24563 | val_1_rmse: 0.24925 |  0:01:42s
epoch 104| loss: 0.08259 | val_0_rmse: 0.27817 | val_1_rmse: 0.28314 |  0:01:43s
epoch 105| loss: 0.08263 | val_0_rmse: 0.23705 | val_1_rmse: 0.23918 |  0:01:44s
epoch 106| loss: 0.08111 | val_0_rmse: 0.27102 | val_1_rmse: 0.26678 |  0:01:45s
epoch 107| loss: 0.08148 | val_0_rmse: 0.31657 | val_1_rmse: 0.31142 |  0:01:46s
epoch 108| loss: 0.08307 | val_0_rmse: 0.22989 | val_1_rmse: 0.22753 |  0:01:47s
epoch 109| loss: 0.08462 | val_0_rmse: 0.24536 | val_1_rmse: 0.24742 |  0:01:48s
epoch 110| loss: 0.08096 | val_0_rmse: 0.27449 | val_1_rmse: 0.27906 |  0:01:49s
epoch 111| loss: 0.08154 | val_0_rmse: 0.22514 | val_1_rmse: 0.2267  |  0:01:50s
epoch 112| loss: 0.08095 | val_0_rmse: 0.28297 | val_1_rmse: 0.27992 |  0:01:51s
epoch 113| loss: 0.08243 | val_0_rmse: 0.31626 | val_1_rmse: 0.31143 |  0:01:52s
epoch 114| loss: 0.08229 | val_0_rmse: 0.24192 | val_1_rmse: 0.23984 |  0:01:53s
epoch 115| loss: 0.08286 | val_0_rmse: 0.2528  | val_1_rmse: 0.25806 |  0:01:54s
epoch 116| loss: 0.07994 | val_0_rmse: 0.28455 | val_1_rmse: 0.28937 |  0:01:55s
epoch 117| loss: 0.0809  | val_0_rmse: 0.24089 | val_1_rmse: 0.2434  |  0:01:56s
epoch 118| loss: 0.08177 | val_0_rmse: 0.27136 | val_1_rmse: 0.2665  |  0:01:57s
epoch 119| loss: 0.08091 | val_0_rmse: 0.3013  | val_1_rmse: 0.29625 |  0:01:58s
epoch 120| loss: 0.07625 | val_0_rmse: 0.23639 | val_1_rmse: 0.23949 |  0:01:59s
epoch 121| loss: 0.06551 | val_0_rmse: 0.2515  | val_1_rmse: 0.24905 |  0:02:00s
epoch 122| loss: 0.05922 | val_0_rmse: 0.23765 | val_1_rmse: 0.23861 |  0:02:01s
epoch 123| loss: 0.05863 | val_0_rmse: 0.22407 | val_1_rmse: 0.2228  |  0:02:02s
epoch 124| loss: 0.05421 | val_0_rmse: 0.2234  | val_1_rmse: 0.22395 |  0:02:03s
epoch 125| loss: 0.06185 | val_0_rmse: 0.23931 | val_1_rmse: 0.23655 |  0:02:04s
epoch 126| loss: 0.06111 | val_0_rmse: 0.27987 | val_1_rmse: 0.27495 |  0:02:05s
epoch 127| loss: 0.09549 | val_0_rmse: 0.33202 | val_1_rmse: 0.32476 |  0:02:06s
epoch 128| loss: 0.08625 | val_0_rmse: 0.27751 | val_1_rmse: 0.27238 |  0:02:07s
epoch 129| loss: 0.08766 | val_0_rmse: 0.22988 | val_1_rmse: 0.22993 |  0:02:08s
epoch 130| loss: 0.08221 | val_0_rmse: 0.26432 | val_1_rmse: 0.26925 |  0:02:09s
epoch 131| loss: 0.08192 | val_0_rmse: 0.22767 | val_1_rmse: 0.22897 |  0:02:09s
epoch 132| loss: 0.08024 | val_0_rmse: 0.27283 | val_1_rmse: 0.2683  |  0:02:10s
epoch 133| loss: 0.08186 | val_0_rmse: 0.32278 | val_1_rmse: 0.31681 |  0:02:11s
epoch 134| loss: 0.08373 | val_0_rmse: 0.25827 | val_1_rmse: 0.25415 |  0:02:12s
epoch 135| loss: 0.08299 | val_0_rmse: 0.22976 | val_1_rmse: 0.23298 |  0:02:13s
epoch 136| loss: 0.08309 | val_0_rmse: 0.24601 | val_1_rmse: 0.25026 |  0:02:14s
epoch 137| loss: 0.08021 | val_0_rmse: 0.22505 | val_1_rmse: 0.22719 |  0:02:15s
epoch 138| loss: 0.05833 | val_0_rmse: 0.23332 | val_1_rmse: 0.23804 |  0:02:16s
epoch 139| loss: 0.05939 | val_0_rmse: 0.23054 | val_1_rmse: 0.22802 |  0:02:17s
epoch 140| loss: 0.05833 | val_0_rmse: 0.2202  | val_1_rmse: 0.21793 |  0:02:18s
epoch 141| loss: 0.05618 | val_0_rmse: 0.22017 | val_1_rmse: 0.22187 |  0:02:19s
epoch 142| loss: 0.05371 | val_0_rmse: 0.26488 | val_1_rmse: 0.26036 |  0:02:20s
epoch 143| loss: 0.05755 | val_0_rmse: 0.22044 | val_1_rmse: 0.21915 |  0:02:21s
epoch 144| loss: 0.05177 | val_0_rmse: 0.23305 | val_1_rmse: 0.23615 |  0:02:22s
epoch 145| loss: 0.05728 | val_0_rmse: 0.22776 | val_1_rmse: 0.22557 |  0:02:23s
epoch 146| loss: 0.0584  | val_0_rmse: 0.22392 | val_1_rmse: 0.22224 |  0:02:24s
epoch 147| loss: 0.06498 | val_0_rmse: 0.22128 | val_1_rmse: 0.21999 |  0:02:25s
epoch 148| loss: 0.06192 | val_0_rmse: 0.22402 | val_1_rmse: 0.22431 |  0:02:26s
epoch 149| loss: 0.05507 | val_0_rmse: 0.22967 | val_1_rmse: 0.22801 |  0:02:27s
Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_val_1_rmse = 0.21793
Best weights from best epoch are automatically used!
ended training at: 18:16:25
Feature importance:
[('Area', 0.2686444042146399), ('Baths', 0.09729667659747862), ('Beds', 0.0), ('Latitude', 0.3552401349417087), ('Longitude', 0.03261093504754829), ('Month', 0.24620784919862448), ('Year', 0.0)]
Mean squared error is of 8465549674.266445
Mean absolute error:65090.72376361978
MAPE:0.16294733204779183
R2 score:0.7272198126964788
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:16:25
epoch 0  | loss: 75.71151| val_0_rmse: 74.20746| val_1_rmse: 74.20271|  0:00:00s
epoch 1  | loss: 4.09066 | val_0_rmse: 11.58963| val_1_rmse: 11.66026|  0:00:01s
epoch 2  | loss: 0.32833 | val_0_rmse: 0.95906 | val_1_rmse: 0.97751 |  0:00:03s
epoch 3  | loss: 0.18462 | val_0_rmse: 7.43765 | val_1_rmse: 7.45856 |  0:00:03s
epoch 4  | loss: 0.13553 | val_0_rmse: 5.17113 | val_1_rmse: 5.18459 |  0:00:04s
epoch 5  | loss: 0.12758 | val_0_rmse: 4.97376 | val_1_rmse: 4.98761 |  0:00:05s
epoch 6  | loss: 0.11373 | val_0_rmse: 5.19986 | val_1_rmse: 5.21394 |  0:00:06s
epoch 7  | loss: 0.1389  | val_0_rmse: 4.38175 | val_1_rmse: 4.39596 |  0:00:07s
epoch 8  | loss: 0.15106 | val_0_rmse: 5.17192 | val_1_rmse: 5.18616 |  0:00:08s
epoch 9  | loss: 0.14226 | val_0_rmse: 4.80242 | val_1_rmse: 4.81607 |  0:00:09s
epoch 10 | loss: 0.16184 | val_0_rmse: 3.98405 | val_1_rmse: 3.99782 |  0:00:10s
epoch 11 | loss: 0.15461 | val_0_rmse: 4.45534 | val_1_rmse: 4.46905 |  0:00:11s
epoch 12 | loss: 0.13135 | val_0_rmse: 4.08459 | val_1_rmse: 4.09837 |  0:00:12s
epoch 13 | loss: 0.12554 | val_0_rmse: 3.82258 | val_1_rmse: 3.83639 |  0:00:13s
epoch 14 | loss: 0.12128 | val_0_rmse: 2.98627 | val_1_rmse: 3.00028 |  0:00:14s
epoch 15 | loss: 0.11818 | val_0_rmse: 2.24055 | val_1_rmse: 2.25482 |  0:00:15s
epoch 16 | loss: 0.11891 | val_0_rmse: 1.94732 | val_1_rmse: 1.96193 |  0:00:16s
epoch 17 | loss: 0.11616 | val_0_rmse: 1.97832 | val_1_rmse: 1.99298 |  0:00:17s
epoch 18 | loss: 0.11176 | val_0_rmse: 2.15665 | val_1_rmse: 2.17203 |  0:00:18s
epoch 19 | loss: 0.10773 | val_0_rmse: 2.00975 | val_1_rmse: 2.02519 |  0:00:19s
epoch 20 | loss: 0.1093  | val_0_rmse: 1.4836  | val_1_rmse: 1.49738 |  0:00:20s
epoch 21 | loss: 0.10845 | val_0_rmse: 0.93158 | val_1_rmse: 0.94721 |  0:00:21s
epoch 22 | loss: 0.111   | val_0_rmse: 0.78177 | val_1_rmse: 0.79799 |  0:00:22s
epoch 23 | loss: 0.10672 | val_0_rmse: 0.91276 | val_1_rmse: 0.92851 |  0:00:23s
epoch 24 | loss: 0.10117 | val_0_rmse: 1.09728 | val_1_rmse: 1.11145 |  0:00:24s
epoch 25 | loss: 0.10781 | val_0_rmse: 1.12684 | val_1_rmse: 1.14077 |  0:00:25s
epoch 26 | loss: 0.10232 | val_0_rmse: 0.97697 | val_1_rmse: 0.99441 |  0:00:26s
epoch 27 | loss: 0.09781 | val_0_rmse: 0.80254 | val_1_rmse: 0.81912 |  0:00:27s
epoch 28 | loss: 0.10019 | val_0_rmse: 0.68787 | val_1_rmse: 0.70345 |  0:00:28s
epoch 29 | loss: 0.10051 | val_0_rmse: 0.77064 | val_1_rmse: 0.78312 |  0:00:29s
epoch 30 | loss: 0.09809 | val_0_rmse: 0.99092 | val_1_rmse: 0.9986  |  0:00:30s
epoch 31 | loss: 0.09838 | val_0_rmse: 1.09836 | val_1_rmse: 1.10487 |  0:00:31s
epoch 32 | loss: 0.09658 | val_0_rmse: 0.85192 | val_1_rmse: 0.85584 |  0:00:32s
epoch 33 | loss: 0.09545 | val_0_rmse: 0.54051 | val_1_rmse: 0.54321 |  0:00:33s
epoch 34 | loss: 0.09592 | val_0_rmse: 0.51926 | val_1_rmse: 0.51812 |  0:00:34s
epoch 35 | loss: 0.0951  | val_0_rmse: 0.55732 | val_1_rmse: 0.55545 |  0:00:35s
epoch 36 | loss: 0.09395 | val_0_rmse: 0.666   | val_1_rmse: 0.66649 |  0:00:36s
epoch 37 | loss: 0.09431 | val_0_rmse: 0.70954 | val_1_rmse: 0.71403 |  0:00:37s
epoch 38 | loss: 0.09397 | val_0_rmse: 0.6077  | val_1_rmse: 0.61468 |  0:00:38s
epoch 39 | loss: 0.09019 | val_0_rmse: 0.41882 | val_1_rmse: 0.42166 |  0:00:39s
epoch 40 | loss: 0.09487 | val_0_rmse: 0.38558 | val_1_rmse: 0.38633 |  0:00:40s
epoch 41 | loss: 0.09564 | val_0_rmse: 0.46288 | val_1_rmse: 0.46437 |  0:00:41s
epoch 42 | loss: 0.08947 | val_0_rmse: 0.6179  | val_1_rmse: 0.62262 |  0:00:42s
epoch 43 | loss: 0.09388 | val_0_rmse: 0.62022 | val_1_rmse: 0.62609 |  0:00:43s
epoch 44 | loss: 0.09043 | val_0_rmse: 0.50983 | val_1_rmse: 0.51499 |  0:00:44s
epoch 45 | loss: 0.0915  | val_0_rmse: 0.34269 | val_1_rmse: 0.34793 |  0:00:45s
epoch 46 | loss: 0.09023 | val_0_rmse: 0.34673 | val_1_rmse: 0.3486  |  0:00:46s
epoch 47 | loss: 0.08939 | val_0_rmse: 0.39143 | val_1_rmse: 0.39335 |  0:00:47s
epoch 48 | loss: 0.08716 | val_0_rmse: 0.55143 | val_1_rmse: 0.54937 |  0:00:48s
epoch 49 | loss: 0.08935 | val_0_rmse: 0.54912 | val_1_rmse: 0.55283 |  0:00:49s
epoch 50 | loss: 0.08926 | val_0_rmse: 0.52611 | val_1_rmse: 0.52714 |  0:00:50s
epoch 51 | loss: 0.0858  | val_0_rmse: 0.33853 | val_1_rmse: 0.34062 |  0:00:51s
epoch 52 | loss: 0.09041 | val_0_rmse: 0.32546 | val_1_rmse: 0.32466 |  0:00:52s
epoch 53 | loss: 0.08971 | val_0_rmse: 0.34865 | val_1_rmse: 0.34868 |  0:00:53s
epoch 54 | loss: 0.08678 | val_0_rmse: 0.47854 | val_1_rmse: 0.4804  |  0:00:54s
epoch 55 | loss: 0.08574 | val_0_rmse: 0.51209 | val_1_rmse: 0.51278 |  0:00:55s
epoch 56 | loss: 0.08477 | val_0_rmse: 0.47962 | val_1_rmse: 0.47898 |  0:00:56s
epoch 57 | loss: 0.08362 | val_0_rmse: 0.30455 | val_1_rmse: 0.30143 |  0:00:57s
epoch 58 | loss: 0.08711 | val_0_rmse: 0.30105 | val_1_rmse: 0.30082 |  0:00:58s
epoch 59 | loss: 0.08645 | val_0_rmse: 0.29973 | val_1_rmse: 0.29623 |  0:00:59s
epoch 60 | loss: 0.0884  | val_0_rmse: 0.45505 | val_1_rmse: 0.44931 |  0:01:00s
epoch 61 | loss: 0.09066 | val_0_rmse: 0.45785 | val_1_rmse: 0.45926 |  0:01:01s
epoch 62 | loss: 0.08977 | val_0_rmse: 0.43237 | val_1_rmse: 0.43273 |  0:01:02s
epoch 63 | loss: 0.08656 | val_0_rmse: 0.28919 | val_1_rmse: 0.28767 |  0:01:03s
epoch 64 | loss: 0.08589 | val_0_rmse: 0.29503 | val_1_rmse: 0.29437 |  0:01:04s
epoch 65 | loss: 0.09094 | val_0_rmse: 0.29612 | val_1_rmse: 0.29122 |  0:01:04s
epoch 66 | loss: 0.08758 | val_0_rmse: 0.38883 | val_1_rmse: 0.38452 |  0:01:05s
epoch 67 | loss: 0.08395 | val_0_rmse: 0.45183 | val_1_rmse: 0.44764 |  0:01:06s
epoch 68 | loss: 0.0845  | val_0_rmse: 0.36954 | val_1_rmse: 0.36849 |  0:01:07s
epoch 69 | loss: 0.08274 | val_0_rmse: 0.26062 | val_1_rmse: 0.26115 |  0:01:08s
epoch 70 | loss: 0.08485 | val_0_rmse: 0.2665  | val_1_rmse: 0.26283 |  0:01:09s
epoch 71 | loss: 0.084   | val_0_rmse: 0.26528 | val_1_rmse: 0.25761 |  0:01:10s
epoch 72 | loss: 0.08427 | val_0_rmse: 0.3442  | val_1_rmse: 0.33829 |  0:01:11s
epoch 73 | loss: 0.08178 | val_0_rmse: 0.39059 | val_1_rmse: 0.38544 |  0:01:12s
epoch 74 | loss: 0.08631 | val_0_rmse: 0.34886 | val_1_rmse: 0.34442 |  0:01:13s
epoch 75 | loss: 0.08292 | val_0_rmse: 0.2554  | val_1_rmse: 0.25909 |  0:01:14s
epoch 76 | loss: 0.08405 | val_0_rmse: 0.2701  | val_1_rmse: 0.26994 |  0:01:15s
epoch 77 | loss: 0.08546 | val_0_rmse: 0.25011 | val_1_rmse: 0.24654 |  0:01:16s
epoch 78 | loss: 0.08135 | val_0_rmse: 0.31935 | val_1_rmse: 0.31721 |  0:01:17s
epoch 79 | loss: 0.07906 | val_0_rmse: 0.36032 | val_1_rmse: 0.35699 |  0:01:18s
epoch 80 | loss: 0.0814  | val_0_rmse: 0.30523 | val_1_rmse: 0.30448 |  0:01:19s
epoch 81 | loss: 0.08341 | val_0_rmse: 0.25504 | val_1_rmse: 0.2558  |  0:01:20s
epoch 82 | loss: 0.08215 | val_0_rmse: 0.28833 | val_1_rmse: 0.29015 |  0:01:21s
epoch 83 | loss: 0.08323 | val_0_rmse: 0.24509 | val_1_rmse: 0.24435 |  0:01:22s
epoch 84 | loss: 0.08248 | val_0_rmse: 0.29958 | val_1_rmse: 0.29768 |  0:01:23s
epoch 85 | loss: 0.08207 | val_0_rmse: 0.34101 | val_1_rmse: 0.3385  |  0:01:24s
epoch 86 | loss: 0.0802  | val_0_rmse: 0.26043 | val_1_rmse: 0.25777 |  0:01:25s
epoch 87 | loss: 0.08033 | val_0_rmse: 0.24107 | val_1_rmse: 0.24247 |  0:01:26s
epoch 88 | loss: 0.08066 | val_0_rmse: 0.28018 | val_1_rmse: 0.28214 |  0:01:27s
epoch 89 | loss: 0.08079 | val_0_rmse: 0.23395 | val_1_rmse: 0.23515 |  0:01:28s
epoch 90 | loss: 0.0765  | val_0_rmse: 0.27095 | val_1_rmse: 0.26809 |  0:01:29s
epoch 91 | loss: 0.07825 | val_0_rmse: 0.32831 | val_1_rmse: 0.32553 |  0:01:30s
epoch 92 | loss: 0.08051 | val_0_rmse: 0.2395  | val_1_rmse: 0.23889 |  0:01:31s
epoch 93 | loss: 0.07981 | val_0_rmse: 0.26166 | val_1_rmse: 0.26152 |  0:01:32s
epoch 94 | loss: 0.0833  | val_0_rmse: 0.28446 | val_1_rmse: 0.28689 |  0:01:33s
epoch 95 | loss: 0.0807  | val_0_rmse: 0.23813 | val_1_rmse: 0.23985 |  0:01:34s
epoch 96 | loss: 0.08017 | val_0_rmse: 0.25303 | val_1_rmse: 0.25178 |  0:01:35s
epoch 97 | loss: 0.0797  | val_0_rmse: 0.28099 | val_1_rmse: 0.27802 |  0:01:36s
epoch 98 | loss: 0.08671 | val_0_rmse: 0.25746 | val_1_rmse: 0.2579  |  0:01:37s
epoch 99 | loss: 0.08021 | val_0_rmse: 0.25467 | val_1_rmse: 0.2568  |  0:01:38s
epoch 100| loss: 0.08273 | val_0_rmse: 0.28982 | val_1_rmse: 0.29307 |  0:01:39s
epoch 101| loss: 0.08155 | val_0_rmse: 0.23446 | val_1_rmse: 0.23514 |  0:01:40s
epoch 102| loss: 0.08344 | val_0_rmse: 0.26218 | val_1_rmse: 0.26145 |  0:01:41s
epoch 103| loss: 0.07974 | val_0_rmse: 0.29745 | val_1_rmse: 0.29383 |  0:01:42s
epoch 104| loss: 0.08499 | val_0_rmse: 0.25445 | val_1_rmse: 0.25549 |  0:01:43s
epoch 105| loss: 0.08459 | val_0_rmse: 0.23829 | val_1_rmse: 0.24147 |  0:01:44s
epoch 106| loss: 0.08116 | val_0_rmse: 0.34063 | val_1_rmse: 0.3427  |  0:01:45s
epoch 107| loss: 0.08385 | val_0_rmse: 0.24807 | val_1_rmse: 0.2481  |  0:01:46s
epoch 108| loss: 0.08024 | val_0_rmse: 0.26995 | val_1_rmse: 0.26632 |  0:01:47s
epoch 109| loss: 0.08081 | val_0_rmse: 0.30027 | val_1_rmse: 0.29655 |  0:01:48s
epoch 110| loss: 0.07727 | val_0_rmse: 0.23732 | val_1_rmse: 0.23495 |  0:01:49s
epoch 111| loss: 0.07931 | val_0_rmse: 0.26782 | val_1_rmse: 0.26804 |  0:01:50s
epoch 112| loss: 0.08233 | val_0_rmse: 0.28339 | val_1_rmse: 0.28501 |  0:01:51s
epoch 113| loss: 0.08644 | val_0_rmse: 0.24858 | val_1_rmse: 0.24878 |  0:01:52s
epoch 114| loss: 0.0793  | val_0_rmse: 0.2484  | val_1_rmse: 0.24865 |  0:01:53s
epoch 115| loss: 0.08423 | val_0_rmse: 0.29783 | val_1_rmse: 0.29566 |  0:01:54s
epoch 116| loss: 0.08364 | val_0_rmse: 0.23341 | val_1_rmse: 0.23061 |  0:01:55s
epoch 117| loss: 0.07771 | val_0_rmse: 0.25662 | val_1_rmse: 0.25695 |  0:01:56s
epoch 118| loss: 0.07859 | val_0_rmse: 0.30984 | val_1_rmse: 0.31135 |  0:01:57s
epoch 119| loss: 0.07579 | val_0_rmse: 0.25759 | val_1_rmse: 0.25912 |  0:01:58s
epoch 120| loss: 0.07771 | val_0_rmse: 0.22961 | val_1_rmse: 0.22978 |  0:01:59s
epoch 121| loss: 0.07989 | val_0_rmse: 0.26072 | val_1_rmse: 0.25766 |  0:02:00s
epoch 122| loss: 0.07888 | val_0_rmse: 0.21909 | val_1_rmse: 0.21776 |  0:02:01s
epoch 123| loss: 0.07886 | val_0_rmse: 0.22969 | val_1_rmse: 0.22899 |  0:02:01s
epoch 124| loss: 0.0772  | val_0_rmse: 0.31909 | val_1_rmse: 0.32297 |  0:02:02s
epoch 125| loss: 0.08668 | val_0_rmse: 0.24147 | val_1_rmse: 0.24601 |  0:02:03s
epoch 126| loss: 0.08212 | val_0_rmse: 0.23907 | val_1_rmse: 0.24065 |  0:02:04s
epoch 127| loss: 0.07781 | val_0_rmse: 0.26218 | val_1_rmse: 0.25963 |  0:02:05s
epoch 128| loss: 0.07831 | val_0_rmse: 0.22871 | val_1_rmse: 0.22827 |  0:02:06s
epoch 129| loss: 0.07754 | val_0_rmse: 0.27588 | val_1_rmse: 0.27578 |  0:02:07s
epoch 130| loss: 0.07785 | val_0_rmse: 0.32327 | val_1_rmse: 0.3253  |  0:02:08s
epoch 131| loss: 0.07893 | val_0_rmse: 0.28166 | val_1_rmse: 0.28146 |  0:02:09s
epoch 132| loss: 0.07873 | val_0_rmse: 0.23564 | val_1_rmse: 0.23479 |  0:02:10s
epoch 133| loss: 0.08333 | val_0_rmse: 0.2999  | val_1_rmse: 0.29267 |  0:02:11s
epoch 134| loss: 0.08025 | val_0_rmse: 0.25372 | val_1_rmse: 0.24933 |  0:02:12s
epoch 135| loss: 0.07798 | val_0_rmse: 0.27829 | val_1_rmse: 0.27909 |  0:02:13s
epoch 136| loss: 0.07757 | val_0_rmse: 0.31556 | val_1_rmse: 0.31807 |  0:02:14s
epoch 137| loss: 0.07662 | val_0_rmse: 0.23788 | val_1_rmse: 0.23884 |  0:02:15s
epoch 138| loss: 0.0771  | val_0_rmse: 0.24974 | val_1_rmse: 0.24565 |  0:02:16s
epoch 139| loss: 0.07845 | val_0_rmse: 0.27737 | val_1_rmse: 0.27282 |  0:02:17s
epoch 140| loss: 0.081   | val_0_rmse: 0.24214 | val_1_rmse: 0.23822 |  0:02:18s
epoch 141| loss: 0.07824 | val_0_rmse: 0.26915 | val_1_rmse: 0.26999 |  0:02:19s
epoch 142| loss: 0.07684 | val_0_rmse: 0.29346 | val_1_rmse: 0.29504 |  0:02:20s
epoch 143| loss: 0.07877 | val_0_rmse: 0.261   | val_1_rmse: 0.26217 |  0:02:21s
epoch 144| loss: 0.07999 | val_0_rmse: 0.23978 | val_1_rmse: 0.2379  |  0:02:22s
epoch 145| loss: 0.07665 | val_0_rmse: 0.26669 | val_1_rmse: 0.26336 |  0:02:23s
epoch 146| loss: 0.07644 | val_0_rmse: 0.22421 | val_1_rmse: 0.22309 |  0:02:24s
epoch 147| loss: 0.07688 | val_0_rmse: 0.26006 | val_1_rmse: 0.26202 |  0:02:25s
epoch 148| loss: 0.07502 | val_0_rmse: 0.30292 | val_1_rmse: 0.30485 |  0:02:26s
epoch 149| loss: 0.0832  | val_0_rmse: 0.27335 | val_1_rmse: 0.27485 |  0:02:27s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.21776
Best weights from best epoch are automatically used!
ended training at: 18:18:53
Feature importance:
[('Area', 0.0), ('Baths', 0.13612505415722628), ('Beds', 0.08436760409439603), ('Latitude', 0.24198206670846398), ('Longitude', 0.18086187743761575), ('Month', 0.17484416697988886), ('Year', 0.1818192306224091)]
Mean squared error is of 8235966174.574988
Mean absolute error:66486.66519308317
MAPE:0.1793442256188223
R2 score:0.734115074424063
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:18:53
epoch 0  | loss: 72.88543| val_0_rmse: 12.75362| val_1_rmse: 12.75925|  0:00:00s
epoch 1  | loss: 3.55442 | val_0_rmse: 21.83873| val_1_rmse: 21.88314|  0:00:02s
epoch 2  | loss: 0.26009 | val_0_rmse: 25.21998| val_1_rmse: 25.18684|  0:00:02s
epoch 3  | loss: 0.20714 | val_0_rmse: 24.34937| val_1_rmse: 24.35335|  0:00:03s
epoch 4  | loss: 0.14285 | val_0_rmse: 5.38849 | val_1_rmse: 5.37488 |  0:00:04s
epoch 5  | loss: 0.1005  | val_0_rmse: 4.39216 | val_1_rmse: 4.40519 |  0:00:05s
epoch 6  | loss: 0.09746 | val_0_rmse: 4.03034 | val_1_rmse: 4.04343 |  0:00:06s
epoch 7  | loss: 0.12973 | val_0_rmse: 4.42544 | val_1_rmse: 4.43849 |  0:00:07s
epoch 8  | loss: 0.12487 | val_0_rmse: 4.20123 | val_1_rmse: 4.21459 |  0:00:08s
epoch 9  | loss: 0.11552 | val_0_rmse: 3.59854 | val_1_rmse: 3.61167 |  0:00:09s
epoch 10 | loss: 0.11168 | val_0_rmse: 2.71221 | val_1_rmse: 2.72562 |  0:00:10s
epoch 11 | loss: 0.10926 | val_0_rmse: 2.03633 | val_1_rmse: 2.04948 |  0:00:11s
epoch 12 | loss: 0.10722 | val_0_rmse: 2.21942 | val_1_rmse: 2.22828 |  0:00:12s
epoch 13 | loss: 0.10746 | val_0_rmse: 2.11266 | val_1_rmse: 2.1265  |  0:00:13s
epoch 14 | loss: 0.10556 | val_0_rmse: 2.41305 | val_1_rmse: 2.41882 |  0:00:14s
epoch 15 | loss: 0.1022  | val_0_rmse: 1.75238 | val_1_rmse: 1.75556 |  0:00:15s
epoch 16 | loss: 0.10362 | val_0_rmse: 2.03675 | val_1_rmse: 2.04282 |  0:00:16s
epoch 17 | loss: 0.10474 | val_0_rmse: 1.32508 | val_1_rmse: 1.331   |  0:00:17s
epoch 18 | loss: 0.10106 | val_0_rmse: 0.85873 | val_1_rmse: 0.86476 |  0:00:18s
epoch 19 | loss: 0.10302 | val_0_rmse: 0.98161 | val_1_rmse: 0.98855 |  0:00:19s
epoch 20 | loss: 0.10189 | val_0_rmse: 1.36934 | val_1_rmse: 1.37644 |  0:00:20s
epoch 21 | loss: 0.10241 | val_0_rmse: 1.29931 | val_1_rmse: 1.30982 |  0:00:21s
epoch 22 | loss: 0.09808 | val_0_rmse: 1.10821 | val_1_rmse: 1.11745 |  0:00:22s
epoch 23 | loss: 0.09823 | val_0_rmse: 0.58536 | val_1_rmse: 0.59379 |  0:00:23s
epoch 24 | loss: 0.09905 | val_0_rmse: 0.56307 | val_1_rmse: 0.57098 |  0:00:24s
epoch 25 | loss: 0.09585 | val_0_rmse: 0.89673 | val_1_rmse: 0.90532 |  0:00:25s
epoch 26 | loss: 0.09337 | val_0_rmse: 1.06218 | val_1_rmse: 1.06835 |  0:00:26s
epoch 27 | loss: 0.09728 | val_0_rmse: 1.28368 | val_1_rmse: 1.29252 |  0:00:27s
epoch 28 | loss: 0.09476 | val_0_rmse: 1.2181  | val_1_rmse: 1.22739 |  0:00:28s
epoch 29 | loss: 0.09054 | val_0_rmse: 0.75578 | val_1_rmse: 0.76439 |  0:00:29s
epoch 30 | loss: 0.08763 | val_0_rmse: 0.71171 | val_1_rmse: 0.71987 |  0:00:30s
epoch 31 | loss: 0.08914 | val_0_rmse: 0.73782 | val_1_rmse: 0.74677 |  0:00:31s
epoch 32 | loss: 0.08621 | val_0_rmse: 1.03754 | val_1_rmse: 1.04678 |  0:00:32s
epoch 33 | loss: 0.08474 | val_0_rmse: 1.15178 | val_1_rmse: 1.16331 |  0:00:33s
epoch 34 | loss: 0.08843 | val_0_rmse: 1.05624 | val_1_rmse: 1.06696 |  0:00:34s
epoch 35 | loss: 0.08588 | val_0_rmse: 0.68929 | val_1_rmse: 0.70041 |  0:00:35s
epoch 36 | loss: 0.0863  | val_0_rmse: 0.52399 | val_1_rmse: 0.53443 |  0:00:36s
epoch 37 | loss: 0.09192 | val_0_rmse: 0.5746  | val_1_rmse: 0.5874  |  0:00:37s
epoch 38 | loss: 0.0848  | val_0_rmse: 0.79125 | val_1_rmse: 0.79975 |  0:00:38s
epoch 39 | loss: 0.08379 | val_0_rmse: 0.80452 | val_1_rmse: 0.81121 |  0:00:39s
epoch 40 | loss: 0.08364 | val_0_rmse: 0.63978 | val_1_rmse: 0.64446 |  0:00:40s
epoch 41 | loss: 0.08206 | val_0_rmse: 0.42922 | val_1_rmse: 0.43365 |  0:00:41s
epoch 42 | loss: 0.08336 | val_0_rmse: 0.34997 | val_1_rmse: 0.34947 |  0:00:42s
epoch 43 | loss: 0.08783 | val_0_rmse: 0.47374 | val_1_rmse: 0.4809  |  0:00:43s
epoch 44 | loss: 0.09783 | val_0_rmse: 0.58866 | val_1_rmse: 0.59748 |  0:00:44s
epoch 45 | loss: 0.10649 | val_0_rmse: 0.37943 | val_1_rmse: 0.38294 |  0:00:45s
epoch 46 | loss: 0.07381 | val_0_rmse: 0.59946 | val_1_rmse: 0.60607 |  0:00:46s
epoch 47 | loss: 0.07051 | val_0_rmse: 0.34624 | val_1_rmse: 0.34533 |  0:00:47s
epoch 48 | loss: 0.09039 | val_0_rmse: 0.48012 | val_1_rmse: 0.48607 |  0:00:48s
epoch 49 | loss: 0.0863  | val_0_rmse: 0.55947 | val_1_rmse: 0.56414 |  0:00:49s
epoch 50 | loss: 0.08247 | val_0_rmse: 0.45196 | val_1_rmse: 0.45592 |  0:00:50s
epoch 51 | loss: 0.08339 | val_0_rmse: 0.32045 | val_1_rmse: 0.32018 |  0:00:51s
epoch 52 | loss: 0.08461 | val_0_rmse: 0.30595 | val_1_rmse: 0.30271 |  0:00:52s
epoch 53 | loss: 0.08782 | val_0_rmse: 0.35741 | val_1_rmse: 0.35953 |  0:00:53s
epoch 54 | loss: 0.08769 | val_0_rmse: 0.4843  | val_1_rmse: 0.49    |  0:00:54s
epoch 55 | loss: 0.08188 | val_0_rmse: 0.46716 | val_1_rmse: 0.47191 |  0:00:55s
epoch 56 | loss: 0.08097 | val_0_rmse: 0.39712 | val_1_rmse: 0.40016 |  0:00:56s
epoch 57 | loss: 0.0808  | val_0_rmse: 0.32468 | val_1_rmse: 0.32672 |  0:00:56s
epoch 58 | loss: 0.05525 | val_0_rmse: 0.36034 | val_1_rmse: 0.36152 |  0:00:58s
epoch 59 | loss: 0.05094 | val_0_rmse: 0.35879 | val_1_rmse: 0.36181 |  0:00:58s
epoch 60 | loss: 0.05781 | val_0_rmse: 0.30336 | val_1_rmse: 0.29901 |  0:00:59s
epoch 61 | loss: 0.06073 | val_0_rmse: 0.3635  | val_1_rmse: 0.36361 |  0:01:00s
epoch 62 | loss: 0.06297 | val_0_rmse: 0.44164 | val_1_rmse: 0.44407 |  0:01:01s
epoch 63 | loss: 0.05739 | val_0_rmse: 0.35478 | val_1_rmse: 0.35516 |  0:01:02s
epoch 64 | loss: 0.05947 | val_0_rmse: 0.37372 | val_1_rmse: 0.37519 |  0:01:03s
epoch 65 | loss: 0.06363 | val_0_rmse: 0.29318 | val_1_rmse: 0.29187 |  0:01:04s
epoch 66 | loss: 0.1452  | val_0_rmse: 0.27906 | val_1_rmse: 0.27381 |  0:01:05s
epoch 67 | loss: 0.06749 | val_0_rmse: 0.30887 | val_1_rmse: 0.30719 |  0:01:06s
epoch 68 | loss: 0.05635 | val_0_rmse: 0.30473 | val_1_rmse: 0.30285 |  0:01:07s
epoch 69 | loss: 0.05692 | val_0_rmse: 0.28139 | val_1_rmse: 0.27572 |  0:01:08s
epoch 70 | loss: 0.0703  | val_0_rmse: 0.34066 | val_1_rmse: 0.3408  |  0:01:09s
epoch 71 | loss: 0.05715 | val_0_rmse: 0.353   | val_1_rmse: 0.35209 |  0:01:10s
epoch 72 | loss: 0.0576  | val_0_rmse: 0.29731 | val_1_rmse: 0.29447 |  0:01:11s
epoch 73 | loss: 0.05904 | val_0_rmse: 0.29834 | val_1_rmse: 0.29476 |  0:01:12s
epoch 74 | loss: 0.0568  | val_0_rmse: 0.35416 | val_1_rmse: 0.35385 |  0:01:13s
epoch 75 | loss: 0.0565  | val_0_rmse: 0.40448 | val_1_rmse: 0.40676 |  0:01:14s
epoch 76 | loss: 0.05998 | val_0_rmse: 0.31166 | val_1_rmse: 0.30949 |  0:01:15s
epoch 77 | loss: 0.04996 | val_0_rmse: 0.35828 | val_1_rmse: 0.35972 |  0:01:16s
epoch 78 | loss: 0.05414 | val_0_rmse: 0.27503 | val_1_rmse: 0.27273 |  0:01:17s
epoch 79 | loss: 0.04852 | val_0_rmse: 0.26695 | val_1_rmse: 0.26414 |  0:01:18s
epoch 80 | loss: 0.05251 | val_0_rmse: 0.2722  | val_1_rmse: 0.26948 |  0:01:19s
epoch 81 | loss: 0.05499 | val_0_rmse: 0.25334 | val_1_rmse: 0.24898 |  0:01:20s
epoch 82 | loss: 0.05216 | val_0_rmse: 0.2491  | val_1_rmse: 0.24476 |  0:01:21s
epoch 83 | loss: 0.05712 | val_0_rmse: 0.24399 | val_1_rmse: 0.24486 |  0:01:22s
epoch 84 | loss: 0.05767 | val_0_rmse: 0.24057 | val_1_rmse: 0.23961 |  0:01:23s
epoch 85 | loss: 0.05649 | val_0_rmse: 0.24678 | val_1_rmse: 0.24505 |  0:01:24s
epoch 86 | loss: 0.05671 | val_0_rmse: 0.24446 | val_1_rmse: 0.24225 |  0:01:25s
epoch 87 | loss: 0.05461 | val_0_rmse: 0.26453 | val_1_rmse: 0.26657 |  0:01:26s
epoch 88 | loss: 0.05252 | val_0_rmse: 0.28377 | val_1_rmse: 0.28691 |  0:01:27s
epoch 89 | loss: 0.06551 | val_0_rmse: 0.26921 | val_1_rmse: 0.26969 |  0:01:28s
epoch 90 | loss: 0.0879  | val_0_rmse: 0.24625 | val_1_rmse: 0.24475 |  0:01:29s
epoch 91 | loss: 0.07681 | val_0_rmse: 0.2285  | val_1_rmse: 0.23023 |  0:01:30s
epoch 92 | loss: 0.05713 | val_0_rmse: 0.2112  | val_1_rmse: 0.21286 |  0:01:31s
epoch 93 | loss: 0.05582 | val_0_rmse: 0.21486 | val_1_rmse: 0.21498 |  0:01:32s
epoch 94 | loss: 0.05184 | val_0_rmse: 0.23122 | val_1_rmse: 0.23201 |  0:01:33s
epoch 95 | loss: 0.04992 | val_0_rmse: 0.21999 | val_1_rmse: 0.21991 |  0:01:34s
epoch 96 | loss: 0.05279 | val_0_rmse: 0.21694 | val_1_rmse: 0.21533 |  0:01:35s
epoch 97 | loss: 0.05263 | val_0_rmse: 0.21561 | val_1_rmse: 0.21631 |  0:01:36s
epoch 98 | loss: 0.04729 | val_0_rmse: 0.21804 | val_1_rmse: 0.21812 |  0:01:37s
epoch 99 | loss: 0.04645 | val_0_rmse: 0.2114  | val_1_rmse: 0.20968 |  0:01:38s
epoch 100| loss: 0.0493  | val_0_rmse: 0.23068 | val_1_rmse: 0.23149 |  0:01:39s
epoch 101| loss: 0.04857 | val_0_rmse: 0.20477 | val_1_rmse: 0.20323 |  0:01:40s
epoch 102| loss: 0.04764 | val_0_rmse: 0.21405 | val_1_rmse: 0.21344 |  0:01:41s
epoch 103| loss: 0.05792 | val_0_rmse: 0.20916 | val_1_rmse: 0.20664 |  0:01:42s
epoch 104| loss: 0.10142 | val_0_rmse: 0.28136 | val_1_rmse: 0.28063 |  0:01:43s
epoch 105| loss: 0.08863 | val_0_rmse: 0.28174 | val_1_rmse: 0.28013 |  0:01:44s
epoch 106| loss: 0.07614 | val_0_rmse: 0.25247 | val_1_rmse: 0.24879 |  0:01:45s
epoch 107| loss: 0.05606 | val_0_rmse: 0.25439 | val_1_rmse: 0.25006 |  0:01:46s
epoch 108| loss: 0.05762 | val_0_rmse: 0.22949 | val_1_rmse: 0.22676 |  0:01:46s
epoch 109| loss: 0.05564 | val_0_rmse: 0.20107 | val_1_rmse: 0.20074 |  0:01:48s
epoch 110| loss: 0.04653 | val_0_rmse: 0.2182  | val_1_rmse: 0.21777 |  0:01:49s
epoch 111| loss: 0.0788  | val_0_rmse: 0.21272 | val_1_rmse: 0.21142 |  0:01:49s
epoch 112| loss: 0.08233 | val_0_rmse: 0.23301 | val_1_rmse: 0.22816 |  0:01:50s
epoch 113| loss: 0.07502 | val_0_rmse: 0.28312 | val_1_rmse: 0.27957 |  0:01:51s
epoch 114| loss: 0.08292 | val_0_rmse: 0.23044 | val_1_rmse: 0.22621 |  0:01:52s
epoch 115| loss: 0.08019 | val_0_rmse: 0.23923 | val_1_rmse: 0.23946 |  0:01:53s
epoch 116| loss: 0.07875 | val_0_rmse: 0.26277 | val_1_rmse: 0.26368 |  0:01:54s
epoch 117| loss: 0.07955 | val_0_rmse: 0.20258 | val_1_rmse: 0.20083 |  0:01:55s
epoch 118| loss: 0.08016 | val_0_rmse: 0.24925 | val_1_rmse: 0.24616 |  0:01:56s
epoch 119| loss: 0.08092 | val_0_rmse: 0.30283 | val_1_rmse: 0.29862 |  0:01:57s
epoch 120| loss: 0.14471 | val_0_rmse: 0.36911 | val_1_rmse: 0.37118 |  0:01:58s
epoch 121| loss: 0.09712 | val_0_rmse: 0.24232 | val_1_rmse: 0.24384 |  0:01:59s
epoch 122| loss: 0.08259 | val_0_rmse: 0.28651 | val_1_rmse: 0.28169 |  0:02:00s
epoch 123| loss: 0.07865 | val_0_rmse: 0.30053 | val_1_rmse: 0.29543 |  0:02:01s
epoch 124| loss: 0.07923 | val_0_rmse: 0.25125 | val_1_rmse: 0.24683 |  0:02:02s
epoch 125| loss: 0.07981 | val_0_rmse: 0.21297 | val_1_rmse: 0.2122  |  0:02:03s
epoch 126| loss: 0.07778 | val_0_rmse: 0.25083 | val_1_rmse: 0.25181 |  0:02:04s
epoch 127| loss: 0.07985 | val_0_rmse: 0.20176 | val_1_rmse: 0.20002 |  0:02:05s
epoch 128| loss: 0.08657 | val_0_rmse: 0.201   | val_1_rmse: 0.19865 |  0:02:06s
epoch 129| loss: 0.11662 | val_0_rmse: 0.25609 | val_1_rmse: 0.2586  |  0:02:07s
epoch 130| loss: 0.0835  | val_0_rmse: 0.24351 | val_1_rmse: 0.24445 |  0:02:08s
epoch 131| loss: 0.07537 | val_0_rmse: 0.27739 | val_1_rmse: 0.27204 |  0:02:09s
epoch 132| loss: 0.10387 | val_0_rmse: 0.21897 | val_1_rmse: 0.21914 |  0:02:10s
epoch 133| loss: 0.11694 | val_0_rmse: 0.21232 | val_1_rmse: 0.21238 |  0:02:11s
epoch 134| loss: 0.05474 | val_0_rmse: 0.24253 | val_1_rmse: 0.23958 |  0:02:12s
epoch 135| loss: 0.05588 | val_0_rmse: 0.2233  | val_1_rmse: 0.21812 |  0:02:13s
epoch 136| loss: 0.05373 | val_0_rmse: 0.22532 | val_1_rmse: 0.22399 |  0:02:14s
epoch 137| loss: 0.05254 | val_0_rmse: 0.20099 | val_1_rmse: 0.20109 |  0:02:15s
epoch 138| loss: 0.05565 | val_0_rmse: 0.20428 | val_1_rmse: 0.20474 |  0:02:16s
epoch 139| loss: 0.08877 | val_0_rmse: 0.22927 | val_1_rmse: 0.23049 |  0:02:17s
epoch 140| loss: 0.07319 | val_0_rmse: 0.27651 | val_1_rmse: 0.27236 |  0:02:18s
epoch 141| loss: 0.07761 | val_0_rmse: 0.26632 | val_1_rmse: 0.26298 |  0:02:19s
epoch 142| loss: 0.07986 | val_0_rmse: 0.29546 | val_1_rmse: 0.29049 |  0:02:20s
epoch 143| loss: 0.13277 | val_0_rmse: 0.2422  | val_1_rmse: 0.24319 |  0:02:21s
epoch 144| loss: 0.12255 | val_0_rmse: 0.30766 | val_1_rmse: 0.30942 |  0:02:22s
epoch 145| loss: 0.1514  | val_0_rmse: 0.28652 | val_1_rmse: 0.28328 |  0:02:23s
epoch 146| loss: 0.05415 | val_0_rmse: 0.20802 | val_1_rmse: 0.20553 |  0:02:24s
epoch 147| loss: 0.04594 | val_0_rmse: 0.20584 | val_1_rmse: 0.20558 |  0:02:25s
epoch 148| loss: 0.04633 | val_0_rmse: 0.21007 | val_1_rmse: 0.20629 |  0:02:26s
epoch 149| loss: 0.05237 | val_0_rmse: 0.2056  | val_1_rmse: 0.20442 |  0:02:27s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.19865
Best weights from best epoch are automatically used!
ended training at: 18:21:21
Feature importance:
[('Area', 0.30143348387907765), ('Baths', 0.0), ('Beds', 0.006673790134274852), ('Latitude', 0.20253469254579623), ('Longitude', 0.2720946039931449), ('Month', 0.19822920072818973), ('Year', 0.019034228719516686)]
Mean squared error is of 7043207496.092032
Mean absolute error:60062.315056319196
MAPE:0.15492342533651574
R2 score:0.770029940627319
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:21:21
epoch 0  | loss: 73.63442| val_0_rmse: 13.68897| val_1_rmse: 13.70431|  0:00:01s
epoch 1  | loss: 3.51834 | val_0_rmse: 4.16055 | val_1_rmse: 4.17214 |  0:00:02s
epoch 2  | loss: 0.33868 | val_0_rmse: 8.85213 | val_1_rmse: 8.83936 |  0:00:02s
epoch 3  | loss: 0.22953 | val_0_rmse: 7.83124 | val_1_rmse: 7.82459 |  0:00:03s
epoch 4  | loss: 0.1904  | val_0_rmse: 6.92812 | val_1_rmse: 6.92001 |  0:00:04s
epoch 5  | loss: 0.15953 | val_0_rmse: 7.95731 | val_1_rmse: 7.94798 |  0:00:05s
epoch 6  | loss: 0.15205 | val_0_rmse: 5.41824 | val_1_rmse: 5.40685 |  0:00:06s
epoch 7  | loss: 0.09689 | val_0_rmse: 5.75789 | val_1_rmse: 5.74265 |  0:00:07s
epoch 8  | loss: 0.12231 | val_0_rmse: 5.06646 | val_1_rmse: 5.05655 |  0:00:08s
epoch 9  | loss: 0.10153 | val_0_rmse: 4.50618 | val_1_rmse: 4.4896  |  0:00:09s
epoch 10 | loss: 0.13108 | val_0_rmse: 4.96689 | val_1_rmse: 4.9532  |  0:00:10s
epoch 11 | loss: 0.09467 | val_0_rmse: 4.83719 | val_1_rmse: 4.79643 |  0:00:11s
epoch 12 | loss: 0.13879 | val_0_rmse: 4.5675  | val_1_rmse: 4.52147 |  0:00:12s
epoch 13 | loss: 0.11954 | val_0_rmse: 3.35481 | val_1_rmse: 3.3228  |  0:00:13s
epoch 14 | loss: 0.10853 | val_0_rmse: 2.88283 | val_1_rmse: 2.86866 |  0:00:14s
epoch 15 | loss: 0.11136 | val_0_rmse: 3.15386 | val_1_rmse: 3.14551 |  0:00:15s
epoch 16 | loss: 0.10441 | val_0_rmse: 2.95376 | val_1_rmse: 2.93161 |  0:00:16s
epoch 17 | loss: 0.10715 | val_0_rmse: 2.35636 | val_1_rmse: 2.33649 |  0:00:17s
epoch 18 | loss: 0.10263 | val_0_rmse: 2.08803 | val_1_rmse: 2.0691  |  0:00:18s
epoch 19 | loss: 0.09907 | val_0_rmse: 1.96457 | val_1_rmse: 1.95353 |  0:00:19s
epoch 20 | loss: 0.09212 | val_0_rmse: 2.92907 | val_1_rmse: 2.92311 |  0:00:20s
epoch 21 | loss: 0.1052  | val_0_rmse: 2.13719 | val_1_rmse: 2.13064 |  0:00:21s
epoch 22 | loss: 0.11132 | val_0_rmse: 1.94232 | val_1_rmse: 1.93627 |  0:00:22s
epoch 23 | loss: 0.09259 | val_0_rmse: 2.19365 | val_1_rmse: 2.18773 |  0:00:23s
epoch 24 | loss: 0.07503 | val_0_rmse: 1.77577 | val_1_rmse: 1.77079 |  0:00:24s
epoch 25 | loss: 0.08518 | val_0_rmse: 1.45018 | val_1_rmse: 1.44366 |  0:00:25s
epoch 26 | loss: 0.14621 | val_0_rmse: 1.4947  | val_1_rmse: 1.48741 |  0:00:26s
epoch 27 | loss: 0.08354 | val_0_rmse: 1.50616 | val_1_rmse: 1.49849 |  0:00:27s
epoch 28 | loss: 0.07681 | val_0_rmse: 1.27505 | val_1_rmse: 1.26795 |  0:00:28s
epoch 29 | loss: 0.09286 | val_0_rmse: 0.80569 | val_1_rmse: 0.79893 |  0:00:29s
epoch 30 | loss: 0.0707  | val_0_rmse: 1.07903 | val_1_rmse: 1.0723  |  0:00:30s
epoch 31 | loss: 0.06604 | val_0_rmse: 1.14889 | val_1_rmse: 1.1429  |  0:00:31s
epoch 32 | loss: 0.06567 | val_0_rmse: 1.10851 | val_1_rmse: 1.10246 |  0:00:32s
epoch 33 | loss: 0.0652  | val_0_rmse: 1.12662 | val_1_rmse: 1.11964 |  0:00:33s
epoch 34 | loss: 0.06757 | val_0_rmse: 1.13895 | val_1_rmse: 1.13132 |  0:00:34s
epoch 35 | loss: 0.07626 | val_0_rmse: 0.89684 | val_1_rmse: 0.88942 |  0:00:35s
epoch 36 | loss: 0.09464 | val_0_rmse: 0.91063 | val_1_rmse: 0.90313 |  0:00:36s
epoch 37 | loss: 0.09317 | val_0_rmse: 0.93575 | val_1_rmse: 0.92833 |  0:00:37s
epoch 38 | loss: 0.0947  | val_0_rmse: 0.63466 | val_1_rmse: 0.62772 |  0:00:38s
epoch 39 | loss: 0.0913  | val_0_rmse: 0.53472 | val_1_rmse: 0.52788 |  0:00:39s
epoch 40 | loss: 0.09053 | val_0_rmse: 0.59172 | val_1_rmse: 0.58446 |  0:00:40s
epoch 41 | loss: 0.09032 | val_0_rmse: 0.77327 | val_1_rmse: 0.76619 |  0:00:41s
epoch 42 | loss: 0.08786 | val_0_rmse: 0.75921 | val_1_rmse: 0.75237 |  0:00:42s
epoch 43 | loss: 0.08871 | val_0_rmse: 0.70195 | val_1_rmse: 0.6951  |  0:00:43s
epoch 44 | loss: 0.08797 | val_0_rmse: 0.40087 | val_1_rmse: 0.39544 |  0:00:44s
epoch 45 | loss: 0.08978 | val_0_rmse: 0.40486 | val_1_rmse: 0.39958 |  0:00:45s
epoch 46 | loss: 0.08975 | val_0_rmse: 0.42818 | val_1_rmse: 0.42259 |  0:00:46s
epoch 47 | loss: 0.08731 | val_0_rmse: 0.6475  | val_1_rmse: 0.64103 |  0:00:47s
epoch 48 | loss: 0.08595 | val_0_rmse: 0.66076 | val_1_rmse: 0.65441 |  0:00:48s
epoch 49 | loss: 0.08523 | val_0_rmse: 0.53374 | val_1_rmse: 0.52782 |  0:00:49s
epoch 50 | loss: 0.09568 | val_0_rmse: 0.46257 | val_1_rmse: 0.45701 |  0:00:50s
epoch 51 | loss: 0.06711 | val_0_rmse: 0.64584 | val_1_rmse: 0.63973 |  0:00:51s
epoch 52 | loss: 0.09022 | val_0_rmse: 0.67039 | val_1_rmse: 0.66455 |  0:00:52s
epoch 53 | loss: 0.10442 | val_0_rmse: 0.53391 | val_1_rmse: 0.52866 |  0:00:53s
epoch 54 | loss: 0.08192 | val_0_rmse: 0.43178 | val_1_rmse: 0.42708 |  0:00:54s
epoch 55 | loss: 0.09943 | val_0_rmse: 0.31032 | val_1_rmse: 0.30877 |  0:00:55s
epoch 56 | loss: 0.09488 | val_0_rmse: 0.31536 | val_1_rmse: 0.31295 |  0:00:55s
epoch 57 | loss: 0.09505 | val_0_rmse: 0.34528 | val_1_rmse: 0.3422  |  0:00:57s
epoch 58 | loss: 0.07936 | val_0_rmse: 0.48379 | val_1_rmse: 0.48001 |  0:00:57s
epoch 59 | loss: 0.05937 | val_0_rmse: 0.5249  | val_1_rmse: 0.52097 |  0:00:58s
epoch 60 | loss: 0.06335 | val_0_rmse: 0.44395 | val_1_rmse: 0.43968 |  0:00:59s
epoch 61 | loss: 0.05619 | val_0_rmse: 0.33108 | val_1_rmse: 0.32861 |  0:01:00s
epoch 62 | loss: 0.09354 | val_0_rmse: 0.31642 | val_1_rmse: 0.31911 |  0:01:01s
epoch 63 | loss: 0.14905 | val_0_rmse: 0.27955 | val_1_rmse: 0.27958 |  0:01:02s
epoch 64 | loss: 0.07484 | val_0_rmse: 0.35508 | val_1_rmse: 0.35286 |  0:01:03s
epoch 65 | loss: 0.0642  | val_0_rmse: 0.33594 | val_1_rmse: 0.33605 |  0:01:04s
epoch 66 | loss: 0.06116 | val_0_rmse: 0.2731  | val_1_rmse: 0.27476 |  0:01:05s
epoch 67 | loss: 0.05546 | val_0_rmse: 0.31506 | val_1_rmse: 0.3143  |  0:01:06s
epoch 68 | loss: 0.05426 | val_0_rmse: 0.30154 | val_1_rmse: 0.30125 |  0:01:07s
epoch 69 | loss: 0.05875 | val_0_rmse: 0.34234 | val_1_rmse: 0.34237 |  0:01:08s
epoch 70 | loss: 0.04986 | val_0_rmse: 0.30557 | val_1_rmse: 0.3049  |  0:01:09s
epoch 71 | loss: 0.05097 | val_0_rmse: 0.33913 | val_1_rmse: 0.33967 |  0:01:10s
epoch 72 | loss: 0.05002 | val_0_rmse: 0.26659 | val_1_rmse: 0.27041 |  0:01:11s
epoch 73 | loss: 0.05683 | val_0_rmse: 0.34168 | val_1_rmse: 0.34122 |  0:01:12s
epoch 74 | loss: 0.05744 | val_0_rmse: 0.26612 | val_1_rmse: 0.26936 |  0:01:13s
epoch 75 | loss: 0.05946 | val_0_rmse: 0.25886 | val_1_rmse: 0.26308 |  0:01:14s
epoch 76 | loss: 0.06312 | val_0_rmse: 0.3161  | val_1_rmse: 0.31966 |  0:01:15s
epoch 77 | loss: 0.05987 | val_0_rmse: 0.30596 | val_1_rmse: 0.31166 |  0:01:16s
epoch 78 | loss: 0.06143 | val_0_rmse: 0.28001 | val_1_rmse: 0.28577 |  0:01:17s
epoch 79 | loss: 0.05968 | val_0_rmse: 0.25411 | val_1_rmse: 0.26184 |  0:01:18s
epoch 80 | loss: 0.04979 | val_0_rmse: 0.26861 | val_1_rmse: 0.27665 |  0:01:19s
epoch 81 | loss: 0.05313 | val_0_rmse: 0.3112  | val_1_rmse: 0.31509 |  0:01:20s
epoch 82 | loss: 0.06119 | val_0_rmse: 0.24163 | val_1_rmse: 0.24809 |  0:01:21s
epoch 83 | loss: 0.0584  | val_0_rmse: 0.23509 | val_1_rmse: 0.24432 |  0:01:22s
epoch 84 | loss: 0.05808 | val_0_rmse: 0.28731 | val_1_rmse: 0.29718 |  0:01:23s
epoch 85 | loss: 0.07727 | val_0_rmse: 0.24015 | val_1_rmse: 0.25005 |  0:01:24s
epoch 86 | loss: 0.05382 | val_0_rmse: 0.23309 | val_1_rmse: 0.24171 |  0:01:25s
epoch 87 | loss: 0.05135 | val_0_rmse: 0.23031 | val_1_rmse: 0.24017 |  0:01:26s
epoch 88 | loss: 0.05127 | val_0_rmse: 0.27519 | val_1_rmse: 0.28091 |  0:01:27s
epoch 89 | loss: 0.05142 | val_0_rmse: 0.22286 | val_1_rmse: 0.23165 |  0:01:28s
epoch 90 | loss: 0.04792 | val_0_rmse: 0.22199 | val_1_rmse: 0.23062 |  0:01:29s
epoch 91 | loss: 0.05587 | val_0_rmse: 0.21696 | val_1_rmse: 0.22624 |  0:01:30s
epoch 92 | loss: 0.05891 | val_0_rmse: 0.22598 | val_1_rmse: 0.23569 |  0:01:31s
epoch 93 | loss: 0.04545 | val_0_rmse: 0.23745 | val_1_rmse: 0.24557 |  0:01:32s
epoch 94 | loss: 0.07257 | val_0_rmse: 0.26202 | val_1_rmse: 0.27124 |  0:01:33s
epoch 95 | loss: 0.08963 | val_0_rmse: 0.2915  | val_1_rmse: 0.30049 |  0:01:34s
epoch 96 | loss: 0.08197 | val_0_rmse: 0.24554 | val_1_rmse: 0.25529 |  0:01:35s
epoch 97 | loss: 0.08232 | val_0_rmse: 0.24173 | val_1_rmse: 0.24789 |  0:01:36s
epoch 98 | loss: 0.08379 | val_0_rmse: 0.2764  | val_1_rmse: 0.28096 |  0:01:37s
epoch 99 | loss: 0.08442 | val_0_rmse: 0.2409  | val_1_rmse: 0.24668 |  0:01:38s
epoch 100| loss: 0.08023 | val_0_rmse: 0.27147 | val_1_rmse: 0.27954 |  0:01:39s
epoch 101| loss: 0.07815 | val_0_rmse: 0.32381 | val_1_rmse: 0.33255 |  0:01:40s
epoch 102| loss: 0.0827  | val_0_rmse: 0.26373 | val_1_rmse: 0.27278 |  0:01:41s
epoch 103| loss: 0.07691 | val_0_rmse: 0.27698 | val_1_rmse: 0.28041 |  0:01:42s
epoch 104| loss: 0.08639 | val_0_rmse: 0.36138 | val_1_rmse: 0.36255 |  0:01:43s
epoch 105| loss: 0.08458 | val_0_rmse: 0.25401 | val_1_rmse: 0.25767 |  0:01:44s
epoch 106| loss: 0.07749 | val_0_rmse: 0.24901 | val_1_rmse: 0.25814 |  0:01:45s
epoch 107| loss: 0.08042 | val_0_rmse: 0.31528 | val_1_rmse: 0.32394 |  0:01:46s
epoch 108| loss: 0.07949 | val_0_rmse: 0.24541 | val_1_rmse: 0.25351 |  0:01:47s
epoch 109| loss: 0.08336 | val_0_rmse: 0.20475 | val_1_rmse: 0.21191 |  0:01:48s
epoch 110| loss: 0.07577 | val_0_rmse: 0.36106 | val_1_rmse: 0.36302 |  0:01:49s
epoch 111| loss: 0.06816 | val_0_rmse: 0.23416 | val_1_rmse: 0.24047 |  0:01:50s
epoch 112| loss: 0.05055 | val_0_rmse: 0.2018  | val_1_rmse: 0.21142 |  0:01:51s
epoch 113| loss: 0.04789 | val_0_rmse: 0.21628 | val_1_rmse: 0.22557 |  0:01:52s
epoch 114| loss: 0.09359 | val_0_rmse: 0.238   | val_1_rmse: 0.24817 |  0:01:53s
epoch 115| loss: 0.05821 | val_0_rmse: 0.23875 | val_1_rmse: 0.24617 |  0:01:54s
epoch 116| loss: 0.09566 | val_0_rmse: 0.25458 | val_1_rmse: 0.25957 |  0:01:55s
epoch 117| loss: 0.05766 | val_0_rmse: 0.2357  | val_1_rmse: 0.2444  |  0:01:56s
epoch 118| loss: 0.05949 | val_0_rmse: 0.24238 | val_1_rmse: 0.25164 |  0:01:57s
epoch 119| loss: 0.05611 | val_0_rmse: 0.21423 | val_1_rmse: 0.22309 |  0:01:58s
epoch 120| loss: 0.05539 | val_0_rmse: 0.22985 | val_1_rmse: 0.23734 |  0:01:59s
epoch 121| loss: 0.05476 | val_0_rmse: 0.22233 | val_1_rmse: 0.2317  |  0:02:00s
epoch 122| loss: 0.05656 | val_0_rmse: 0.24557 | val_1_rmse: 0.25394 |  0:02:00s
epoch 123| loss: 0.05934 | val_0_rmse: 0.2149  | val_1_rmse: 0.22264 |  0:02:01s
epoch 124| loss: 0.05488 | val_0_rmse: 0.22392 | val_1_rmse: 0.23175 |  0:02:02s
epoch 125| loss: 0.05311 | val_0_rmse: 0.22556 | val_1_rmse: 0.23596 |  0:02:03s
epoch 126| loss: 0.0496  | val_0_rmse: 0.19722 | val_1_rmse: 0.20789 |  0:02:04s
epoch 127| loss: 0.04828 | val_0_rmse: 0.19761 | val_1_rmse: 0.20883 |  0:02:05s
epoch 128| loss: 0.05392 | val_0_rmse: 0.22789 | val_1_rmse: 0.23927 |  0:02:06s
epoch 129| loss: 0.04966 | val_0_rmse: 0.19828 | val_1_rmse: 0.20849 |  0:02:07s
epoch 130| loss: 0.05731 | val_0_rmse: 0.23101 | val_1_rmse: 0.24181 |  0:02:08s
epoch 131| loss: 0.05407 | val_0_rmse: 0.25758 | val_1_rmse: 0.26744 |  0:02:09s
epoch 132| loss: 0.13127 | val_0_rmse: 0.28626 | val_1_rmse: 0.28784 |  0:02:10s
epoch 133| loss: 0.15088 | val_0_rmse: 0.29046 | val_1_rmse: 0.29945 |  0:02:11s
epoch 134| loss: 0.12947 | val_0_rmse: 0.31158 | val_1_rmse: 0.32067 |  0:02:12s
epoch 135| loss: 0.14617 | val_0_rmse: 0.33797 | val_1_rmse: 0.34142 |  0:02:13s
epoch 136| loss: 0.13541 | val_0_rmse: 0.42211 | val_1_rmse: 0.4285  |  0:02:14s
epoch 137| loss: 0.14778 | val_0_rmse: 0.25889 | val_1_rmse: 0.26366 |  0:02:15s
epoch 138| loss: 0.12663 | val_0_rmse: 0.2978  | val_1_rmse: 0.30228 |  0:02:16s
epoch 139| loss: 0.14417 | val_0_rmse: 0.3867  | val_1_rmse: 0.39561 |  0:02:17s
epoch 140| loss: 0.13577 | val_0_rmse: 0.34781 | val_1_rmse: 0.3499  |  0:02:18s
epoch 141| loss: 0.13588 | val_0_rmse: 0.24597 | val_1_rmse: 0.25614 |  0:02:19s
epoch 142| loss: 0.12412 | val_0_rmse: 0.31301 | val_1_rmse: 0.32224 |  0:02:20s
epoch 143| loss: 0.13947 | val_0_rmse: 0.38582 | val_1_rmse: 0.38674 |  0:02:21s
epoch 144| loss: 0.13727 | val_0_rmse: 0.33682 | val_1_rmse: 0.34474 |  0:02:22s
epoch 145| loss: 0.12968 | val_0_rmse: 0.23179 | val_1_rmse: 0.23877 |  0:02:23s
epoch 146| loss: 0.13181 | val_0_rmse: 0.25603 | val_1_rmse: 0.26048 |  0:02:24s
epoch 147| loss: 0.12529 | val_0_rmse: 0.41337 | val_1_rmse: 0.42086 |  0:02:25s
epoch 148| loss: 0.13847 | val_0_rmse: 0.311   | val_1_rmse: 0.31441 |  0:02:26s
epoch 149| loss: 0.13886 | val_0_rmse: 0.27439 | val_1_rmse: 0.28414 |  0:02:27s
Stop training because you reached max_epochs = 150 with best_epoch = 126 and best_val_1_rmse = 0.20789
Best weights from best epoch are automatically used!
ended training at: 18:23:49
Feature importance:
[('Area', 0.312320739646273), ('Baths', 0.10026181681835498), ('Beds', 0.041754827415653194), ('Latitude', 0.15360241983958534), ('Longitude', 0.22438489178239646), ('Month', 0.06840373011087607), ('Year', 0.09927157438686099)]
Mean squared error is of 7031070684.108371
Mean absolute error:59884.527981127336
MAPE:0.1535443760838714
R2 score:0.7757004103042663
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:23:49
epoch 0  | loss: 71.95096| val_0_rmse: 56.6829 | val_1_rmse: 56.59075|  0:00:00s
epoch 1  | loss: 3.4179  | val_0_rmse: 5.4278  | val_1_rmse: 5.55034 |  0:00:01s
epoch 2  | loss: 0.32821 | val_0_rmse: 9.84911 | val_1_rmse: 9.82423 |  0:00:02s
epoch 3  | loss: 0.19854 | val_0_rmse: 1.65623 | val_1_rmse: 1.64351 |  0:00:03s
epoch 4  | loss: 0.16054 | val_0_rmse: 1.74773 | val_1_rmse: 1.74318 |  0:00:04s
epoch 5  | loss: 0.16491 | val_0_rmse: 1.7864  | val_1_rmse: 1.78231 |  0:00:05s
epoch 6  | loss: 0.10701 | val_0_rmse: 1.32234 | val_1_rmse: 1.31762 |  0:00:06s
epoch 7  | loss: 0.10711 | val_0_rmse: 1.07277 | val_1_rmse: 1.07407 |  0:00:07s
epoch 8  | loss: 0.14125 | val_0_rmse: 1.03158 | val_1_rmse: 1.03679 |  0:00:08s
epoch 9  | loss: 0.10201 | val_0_rmse: 1.13269 | val_1_rmse: 1.13449 |  0:00:09s
epoch 10 | loss: 0.12905 | val_0_rmse: 0.80688 | val_1_rmse: 0.81134 |  0:00:10s
epoch 11 | loss: 0.11828 | val_0_rmse: 1.00852 | val_1_rmse: 1.00878 |  0:00:11s
epoch 12 | loss: 0.13038 | val_0_rmse: 0.87332 | val_1_rmse: 0.87215 |  0:00:12s
epoch 13 | loss: 0.1204  | val_0_rmse: 0.68301 | val_1_rmse: 0.68546 |  0:00:13s
epoch 14 | loss: 0.11896 | val_0_rmse: 0.75562 | val_1_rmse: 0.75707 |  0:00:14s
epoch 15 | loss: 0.11028 | val_0_rmse: 0.79083 | val_1_rmse: 0.79114 |  0:00:15s
epoch 16 | loss: 0.1128  | val_0_rmse: 1.0414  | val_1_rmse: 1.04267 |  0:00:16s
epoch 17 | loss: 0.10684 | val_0_rmse: 1.14434 | val_1_rmse: 1.14912 |  0:00:17s
epoch 18 | loss: 0.10601 | val_0_rmse: 0.94136 | val_1_rmse: 0.94736 |  0:00:18s
epoch 19 | loss: 0.0943  | val_0_rmse: 0.75115 | val_1_rmse: 0.75335 |  0:00:19s
epoch 20 | loss: 0.0901  | val_0_rmse: 0.58267 | val_1_rmse: 0.58127 |  0:00:20s
epoch 21 | loss: 0.08532 | val_0_rmse: 0.71755 | val_1_rmse: 0.71787 |  0:00:21s
epoch 22 | loss: 0.07967 | val_0_rmse: 0.7447  | val_1_rmse: 0.74446 |  0:00:22s
epoch 23 | loss: 0.07096 | val_0_rmse: 0.7051  | val_1_rmse: 0.70332 |  0:00:23s
epoch 24 | loss: 0.06577 | val_0_rmse: 0.77631 | val_1_rmse: 0.7751  |  0:00:24s
epoch 25 | loss: 0.06634 | val_0_rmse: 0.90227 | val_1_rmse: 0.9021  |  0:00:25s
epoch 26 | loss: 0.11232 | val_0_rmse: 0.58038 | val_1_rmse: 0.57828 |  0:00:26s
epoch 27 | loss: 0.07534 | val_0_rmse: 0.61186 | val_1_rmse: 0.6101  |  0:00:27s
epoch 28 | loss: 0.09566 | val_0_rmse: 0.9076  | val_1_rmse: 0.90643 |  0:00:28s
epoch 29 | loss: 0.10523 | val_0_rmse: 0.89623 | val_1_rmse: 0.89386 |  0:00:29s
epoch 30 | loss: 0.10441 | val_0_rmse: 0.81898 | val_1_rmse: 0.81644 |  0:00:30s
epoch 31 | loss: 0.09702 | val_0_rmse: 0.51457 | val_1_rmse: 0.51375 |  0:00:31s
epoch 32 | loss: 0.0933  | val_0_rmse: 0.43331 | val_1_rmse: 0.43273 |  0:00:32s
epoch 33 | loss: 0.09511 | val_0_rmse: 0.47935 | val_1_rmse: 0.48026 |  0:00:33s
epoch 34 | loss: 0.09429 | val_0_rmse: 0.69192 | val_1_rmse: 0.69127 |  0:00:34s
epoch 35 | loss: 0.09186 | val_0_rmse: 0.72078 | val_1_rmse: 0.71935 |  0:00:35s
epoch 36 | loss: 0.09075 | val_0_rmse: 0.64523 | val_1_rmse: 0.64457 |  0:00:36s
epoch 37 | loss: 0.09311 | val_0_rmse: 0.42254 | val_1_rmse: 0.42187 |  0:00:37s
epoch 38 | loss: 0.0918  | val_0_rmse: 0.38734 | val_1_rmse: 0.38794 |  0:00:38s
epoch 39 | loss: 0.09247 | val_0_rmse: 0.44309 | val_1_rmse: 0.44535 |  0:00:39s
epoch 40 | loss: 0.09118 | val_0_rmse: 0.66048 | val_1_rmse: 0.66025 |  0:00:40s
epoch 41 | loss: 0.08832 | val_0_rmse: 0.69812 | val_1_rmse: 0.69897 |  0:00:41s
epoch 42 | loss: 0.08938 | val_0_rmse: 0.55959 | val_1_rmse: 0.55995 |  0:00:42s
epoch 43 | loss: 0.08274 | val_0_rmse: 0.38037 | val_1_rmse: 0.37961 |  0:00:43s
epoch 44 | loss: 0.08581 | val_0_rmse: 0.34862 | val_1_rmse: 0.34919 |  0:00:44s
epoch 45 | loss: 0.09052 | val_0_rmse: 0.37827 | val_1_rmse: 0.37809 |  0:00:45s
epoch 46 | loss: 0.08532 | val_0_rmse: 0.51957 | val_1_rmse: 0.5193  |  0:00:46s
epoch 47 | loss: 0.08997 | val_0_rmse: 0.59021 | val_1_rmse: 0.59011 |  0:00:47s
epoch 48 | loss: 0.08908 | val_0_rmse: 0.49138 | val_1_rmse: 0.49021 |  0:00:48s
epoch 49 | loss: 0.08731 | val_0_rmse: 0.35663 | val_1_rmse: 0.35715 |  0:00:49s
epoch 50 | loss: 0.0811  | val_0_rmse: 0.32586 | val_1_rmse: 0.32703 |  0:00:50s
epoch 51 | loss: 0.08807 | val_0_rmse: 0.34588 | val_1_rmse: 0.3468  |  0:00:51s
epoch 52 | loss: 0.08419 | val_0_rmse: 0.51881 | val_1_rmse: 0.51843 |  0:00:52s
epoch 53 | loss: 0.087   | val_0_rmse: 0.5619  | val_1_rmse: 0.56133 |  0:00:52s
epoch 54 | loss: 0.08527 | val_0_rmse: 0.454   | val_1_rmse: 0.45332 |  0:00:53s
epoch 55 | loss: 0.08393 | val_0_rmse: 0.30146 | val_1_rmse: 0.3031  |  0:00:54s
epoch 56 | loss: 0.08475 | val_0_rmse: 0.29378 | val_1_rmse: 0.29528 |  0:00:55s
epoch 57 | loss: 0.08805 | val_0_rmse: 0.31249 | val_1_rmse: 0.31457 |  0:00:56s
epoch 58 | loss: 0.08233 | val_0_rmse: 0.45617 | val_1_rmse: 0.45599 |  0:00:57s
epoch 59 | loss: 0.08461 | val_0_rmse: 0.54031 | val_1_rmse: 0.53993 |  0:00:58s
epoch 60 | loss: 0.08557 | val_0_rmse: 0.40549 | val_1_rmse: 0.40522 |  0:00:59s
epoch 61 | loss: 0.07957 | val_0_rmse: 0.27811 | val_1_rmse: 0.28028 |  0:01:00s
epoch 62 | loss: 0.08211 | val_0_rmse: 0.27881 | val_1_rmse: 0.28309 |  0:01:01s
epoch 63 | loss: 0.08537 | val_0_rmse: 0.2721  | val_1_rmse: 0.2754  |  0:01:02s
epoch 64 | loss: 0.08597 | val_0_rmse: 0.38595 | val_1_rmse: 0.38538 |  0:01:03s
epoch 65 | loss: 0.07937 | val_0_rmse: 0.45798 | val_1_rmse: 0.45582 |  0:01:04s
epoch 66 | loss: 0.08112 | val_0_rmse: 0.3747  | val_1_rmse: 0.374   |  0:01:05s
epoch 67 | loss: 0.07847 | val_0_rmse: 0.26161 | val_1_rmse: 0.26566 |  0:01:06s
epoch 68 | loss: 0.07984 | val_0_rmse: 0.27408 | val_1_rmse: 0.27799 |  0:01:07s
epoch 69 | loss: 0.08166 | val_0_rmse: 0.25614 | val_1_rmse: 0.25821 |  0:01:08s
epoch 70 | loss: 0.07805 | val_0_rmse: 0.35278 | val_1_rmse: 0.35217 |  0:01:09s
epoch 71 | loss: 0.08058 | val_0_rmse: 0.39369 | val_1_rmse: 0.39407 |  0:01:10s
epoch 72 | loss: 0.08314 | val_0_rmse: 0.33604 | val_1_rmse: 0.33629 |  0:01:11s
epoch 73 | loss: 0.08092 | val_0_rmse: 0.25427 | val_1_rmse: 0.26013 |  0:01:12s
epoch 74 | loss: 0.07983 | val_0_rmse: 0.25698 | val_1_rmse: 0.26222 |  0:01:13s
epoch 75 | loss: 0.08184 | val_0_rmse: 0.25054 | val_1_rmse: 0.25678 |  0:01:14s
epoch 76 | loss: 0.0795  | val_0_rmse: 0.34106 | val_1_rmse: 0.34099 |  0:01:15s
epoch 77 | loss: 0.07921 | val_0_rmse: 0.40146 | val_1_rmse: 0.40202 |  0:01:16s
epoch 78 | loss: 0.08075 | val_0_rmse: 0.32679 | val_1_rmse: 0.32745 |  0:01:17s
epoch 79 | loss: 0.07759 | val_0_rmse: 0.23515 | val_1_rmse: 0.23996 |  0:01:18s
epoch 80 | loss: 0.07617 | val_0_rmse: 0.26671 | val_1_rmse: 0.27148 |  0:01:19s
epoch 81 | loss: 0.08102 | val_0_rmse: 0.24874 | val_1_rmse: 0.2548  |  0:01:20s
epoch 82 | loss: 0.07704 | val_0_rmse: 0.30215 | val_1_rmse: 0.30522 |  0:01:21s
epoch 83 | loss: 0.07702 | val_0_rmse: 0.32431 | val_1_rmse: 0.32585 |  0:01:22s
epoch 84 | loss: 0.07837 | val_0_rmse: 0.26044 | val_1_rmse: 0.2629  |  0:01:23s
epoch 85 | loss: 0.07861 | val_0_rmse: 0.23574 | val_1_rmse: 0.24027 |  0:01:24s
epoch 86 | loss: 0.07479 | val_0_rmse: 0.2712  | val_1_rmse: 0.27576 |  0:01:25s
epoch 87 | loss: 0.07846 | val_0_rmse: 0.23044 | val_1_rmse: 0.23611 |  0:01:26s
epoch 88 | loss: 0.07626 | val_0_rmse: 0.26857 | val_1_rmse: 0.27121 |  0:01:27s
epoch 89 | loss: 0.07821 | val_0_rmse: 0.30414 | val_1_rmse: 0.30586 |  0:01:28s
epoch 90 | loss: 0.08002 | val_0_rmse: 0.24727 | val_1_rmse: 0.25212 |  0:01:29s
epoch 91 | loss: 0.07821 | val_0_rmse: 0.25732 | val_1_rmse: 0.26111 |  0:01:30s
epoch 92 | loss: 0.07883 | val_0_rmse: 0.28982 | val_1_rmse: 0.29244 |  0:01:31s
epoch 93 | loss: 0.0825  | val_0_rmse: 0.23028 | val_1_rmse: 0.23055 |  0:01:32s
epoch 94 | loss: 0.07899 | val_0_rmse: 0.2792  | val_1_rmse: 0.28184 |  0:01:33s
epoch 95 | loss: 0.07681 | val_0_rmse: 0.28539 | val_1_rmse: 0.28961 |  0:01:34s
epoch 96 | loss: 0.08459 | val_0_rmse: 0.25322 | val_1_rmse: 0.25364 |  0:01:35s
epoch 97 | loss: 0.08695 | val_0_rmse: 0.2615  | val_1_rmse: 0.26197 |  0:01:36s
epoch 98 | loss: 0.08463 | val_0_rmse: 0.29225 | val_1_rmse: 0.29281 |  0:01:37s
epoch 99 | loss: 0.08619 | val_0_rmse: 0.27293 | val_1_rmse: 0.27693 |  0:01:38s
epoch 100| loss: 0.08389 | val_0_rmse: 0.26565 | val_1_rmse: 0.27077 |  0:01:39s
epoch 101| loss: 0.08562 | val_0_rmse: 0.31189 | val_1_rmse: 0.31384 |  0:01:40s
epoch 102| loss: 0.09665 | val_0_rmse: 0.28322 | val_1_rmse: 0.28285 |  0:01:41s
epoch 103| loss: 0.11111 | val_0_rmse: 0.3202  | val_1_rmse: 0.32034 |  0:01:42s
epoch 104| loss: 0.1     | val_0_rmse: 0.32084 | val_1_rmse: 0.32381 |  0:01:43s
epoch 105| loss: 0.09452 | val_0_rmse: 0.24251 | val_1_rmse: 0.24101 |  0:01:44s
epoch 106| loss: 0.06138 | val_0_rmse: 0.24356 | val_1_rmse: 0.24784 |  0:01:45s
epoch 107| loss: 0.06809 | val_0_rmse: 0.22932 | val_1_rmse: 0.23229 |  0:01:46s
epoch 108| loss: 0.05751 | val_0_rmse: 0.22388 | val_1_rmse: 0.22451 |  0:01:47s
epoch 109| loss: 0.05979 | val_0_rmse: 0.2319  | val_1_rmse: 0.23581 |  0:01:48s
epoch 110| loss: 0.07114 | val_0_rmse: 0.21992 | val_1_rmse: 0.22346 |  0:01:49s
epoch 111| loss: 0.05427 | val_0_rmse: 0.22374 | val_1_rmse: 0.22518 |  0:01:50s
epoch 112| loss: 0.0534  | val_0_rmse: 0.23914 | val_1_rmse: 0.23966 |  0:01:51s
epoch 113| loss: 0.0526  | val_0_rmse: 0.22081 | val_1_rmse: 0.22275 |  0:01:51s
epoch 114| loss: 0.05747 | val_0_rmse: 0.23344 | val_1_rmse: 0.23573 |  0:01:52s
epoch 115| loss: 0.05511 | val_0_rmse: 0.23565 | val_1_rmse: 0.23871 |  0:01:53s
epoch 116| loss: 0.06018 | val_0_rmse: 0.22092 | val_1_rmse: 0.22384 |  0:01:54s
epoch 117| loss: 0.05193 | val_0_rmse: 0.22425 | val_1_rmse: 0.22737 |  0:01:55s
epoch 118| loss: 0.05588 | val_0_rmse: 0.20842 | val_1_rmse: 0.21375 |  0:01:56s
epoch 119| loss: 0.06013 | val_0_rmse: 0.27102 | val_1_rmse: 0.27777 |  0:01:57s
epoch 120| loss: 0.07788 | val_0_rmse: 0.25657 | val_1_rmse: 0.25973 |  0:01:58s
epoch 121| loss: 0.07399 | val_0_rmse: 0.32126 | val_1_rmse: 0.3295  |  0:01:59s
epoch 122| loss: 0.08851 | val_0_rmse: 0.21006 | val_1_rmse: 0.2152  |  0:02:00s
epoch 123| loss: 0.08373 | val_0_rmse: 0.24655 | val_1_rmse: 0.2485  |  0:02:01s
epoch 124| loss: 0.08186 | val_0_rmse: 0.27674 | val_1_rmse: 0.27827 |  0:02:02s
epoch 125| loss: 0.08112 | val_0_rmse: 0.21818 | val_1_rmse: 0.22263 |  0:02:03s
epoch 126| loss: 0.07974 | val_0_rmse: 0.25424 | val_1_rmse: 0.26081 |  0:02:04s
epoch 127| loss: 0.07888 | val_0_rmse: 0.26957 | val_1_rmse: 0.27576 |  0:02:05s
epoch 128| loss: 0.07913 | val_0_rmse: 0.21156 | val_1_rmse: 0.21719 |  0:02:06s
epoch 129| loss: 0.07778 | val_0_rmse: 0.27547 | val_1_rmse: 0.27674 |  0:02:07s
epoch 130| loss: 0.07961 | val_0_rmse: 0.31613 | val_1_rmse: 0.31754 |  0:02:08s
epoch 131| loss: 0.07941 | val_0_rmse: 0.26131 | val_1_rmse: 0.26271 |  0:02:09s
epoch 132| loss: 0.07893 | val_0_rmse: 0.21378 | val_1_rmse: 0.22063 |  0:02:10s
epoch 133| loss: 0.07918 | val_0_rmse: 0.2552  | val_1_rmse: 0.26245 |  0:02:11s
epoch 134| loss: 0.07851 | val_0_rmse: 0.20567 | val_1_rmse: 0.21107 |  0:02:12s
epoch 135| loss: 0.07944 | val_0_rmse: 0.24016 | val_1_rmse: 0.24153 |  0:02:13s
epoch 136| loss: 0.07437 | val_0_rmse: 0.21962 | val_1_rmse: 0.22161 |  0:02:14s
epoch 137| loss: 0.05389 | val_0_rmse: 0.21337 | val_1_rmse: 0.21886 |  0:02:15s
epoch 138| loss: 0.06128 | val_0_rmse: 0.22716 | val_1_rmse: 0.23452 |  0:02:16s
epoch 139| loss: 0.05929 | val_0_rmse: 0.22902 | val_1_rmse: 0.2291  |  0:02:17s
epoch 140| loss: 0.05015 | val_0_rmse: 0.20677 | val_1_rmse: 0.20934 |  0:02:18s
epoch 141| loss: 0.04764 | val_0_rmse: 0.2105  | val_1_rmse: 0.21161 |  0:02:19s
epoch 142| loss: 0.05593 | val_0_rmse: 0.23002 | val_1_rmse: 0.23241 |  0:02:20s
epoch 143| loss: 0.06008 | val_0_rmse: 0.22061 | val_1_rmse: 0.22437 |  0:02:21s
epoch 144| loss: 0.05714 | val_0_rmse: 0.22473 | val_1_rmse: 0.2237  |  0:02:22s
epoch 145| loss: 0.05643 | val_0_rmse: 0.22205 | val_1_rmse: 0.22315 |  0:02:23s
epoch 146| loss: 0.05751 | val_0_rmse: 0.21805 | val_1_rmse: 0.2222  |  0:02:24s
epoch 147| loss: 0.05611 | val_0_rmse: 0.21116 | val_1_rmse: 0.21367 |  0:02:25s
epoch 148| loss: 0.05553 | val_0_rmse: 0.22292 | val_1_rmse: 0.22434 |  0:02:26s
epoch 149| loss: 0.05142 | val_0_rmse: 0.20561 | val_1_rmse: 0.20927 |  0:02:27s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.20927
Best weights from best epoch are automatically used!
ended training at: 18:26:17
Feature importance:
[('Area', 0.1651748946542954), ('Baths', 0.10926767002548264), ('Beds', 0.0), ('Latitude', 0.14309879777072274), ('Longitude', 0.24009654159087934), ('Month', 0.07808150810171528), ('Year', 0.26428058785690456)]
Mean squared error is of 7942059629.781894
Mean absolute error:61561.968369935836
MAPE:0.15558073827584162
R2 score:0.7418792248699786
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:26:17
epoch 0  | loss: 136.21355| val_0_rmse: 23.54544| val_1_rmse: 23.56139|  0:00:00s
epoch 1  | loss: 27.85573| val_0_rmse: 58.24371| val_1_rmse: 58.26955|  0:00:00s
epoch 2  | loss: 10.95199| val_0_rmse: 53.18847| val_1_rmse: 53.16075|  0:00:01s
epoch 3  | loss: 2.25654 | val_0_rmse: 33.99855| val_1_rmse: 34.15604|  0:00:01s
epoch 4  | loss: 1.25465 | val_0_rmse: 1.98789 | val_1_rmse: 2.10159 |  0:00:02s
epoch 5  | loss: 0.64315 | val_0_rmse: 6.65779 | val_1_rmse: 6.63483 |  0:00:02s
epoch 6  | loss: 0.50073 | val_0_rmse: 3.24068 | val_1_rmse: 3.24583 |  0:00:03s
epoch 7  | loss: 0.2885  | val_0_rmse: 2.16371 | val_1_rmse: 2.17234 |  0:00:03s
epoch 8  | loss: 0.35329 | val_0_rmse: 2.62594 | val_1_rmse: 2.62851 |  0:00:04s
epoch 9  | loss: 0.29689 | val_0_rmse: 0.88859 | val_1_rmse: 0.8702  |  0:00:04s
epoch 10 | loss: 0.33023 | val_0_rmse: 0.57903 | val_1_rmse: 0.57536 |  0:00:05s
epoch 11 | loss: 0.41073 | val_0_rmse: 0.50944 | val_1_rmse: 0.50258 |  0:00:05s
epoch 12 | loss: 0.30138 | val_0_rmse: 0.76403 | val_1_rmse: 0.74604 |  0:00:06s
epoch 13 | loss: 0.21754 | val_0_rmse: 0.83758 | val_1_rmse: 0.81787 |  0:00:06s
epoch 14 | loss: 0.17392 | val_0_rmse: 0.93493 | val_1_rmse: 0.91209 |  0:00:06s
epoch 15 | loss: 0.21527 | val_0_rmse: 0.77794 | val_1_rmse: 0.75827 |  0:00:07s
epoch 16 | loss: 0.19354 | val_0_rmse: 0.89872 | val_1_rmse: 0.87602 |  0:00:07s
epoch 17 | loss: 0.22453 | val_0_rmse: 0.6065  | val_1_rmse: 0.59552 |  0:00:08s
epoch 18 | loss: 0.19471 | val_0_rmse: 0.54399 | val_1_rmse: 0.54799 |  0:00:08s
epoch 19 | loss: 0.17361 | val_0_rmse: 0.54407 | val_1_rmse: 0.54387 |  0:00:09s
epoch 20 | loss: 0.18501 | val_0_rmse: 0.55887 | val_1_rmse: 0.5473  |  0:00:09s
epoch 21 | loss: 0.15964 | val_0_rmse: 0.49588 | val_1_rmse: 0.50343 |  0:00:10s
epoch 22 | loss: 0.16759 | val_0_rmse: 0.53664 | val_1_rmse: 0.52641 |  0:00:10s
epoch 23 | loss: 0.19391 | val_0_rmse: 0.71002 | val_1_rmse: 0.69654 |  0:00:11s
epoch 24 | loss: 0.15141 | val_0_rmse: 1.05529 | val_1_rmse: 1.03927 |  0:00:11s
epoch 25 | loss: 0.17952 | val_0_rmse: 1.09309 | val_1_rmse: 1.07851 |  0:00:12s
epoch 26 | loss: 0.18491 | val_0_rmse: 1.03972 | val_1_rmse: 1.03219 |  0:00:12s
epoch 27 | loss: 0.17118 | val_0_rmse: 1.07386 | val_1_rmse: 1.05762 |  0:00:13s
epoch 28 | loss: 0.18397 | val_0_rmse: 0.83334 | val_1_rmse: 0.80951 |  0:00:13s
epoch 29 | loss: 0.13368 | val_0_rmse: 0.59689 | val_1_rmse: 0.60212 |  0:00:14s
epoch 30 | loss: 0.13524 | val_0_rmse: 0.609   | val_1_rmse: 0.62207 |  0:00:14s
epoch 31 | loss: 0.13916 | val_0_rmse: 0.80601 | val_1_rmse: 0.82616 |  0:00:14s
epoch 32 | loss: 0.12708 | val_0_rmse: 0.66858 | val_1_rmse: 0.6882  |  0:00:15s
epoch 33 | loss: 0.14193 | val_0_rmse: 0.68439 | val_1_rmse: 0.70294 |  0:00:15s
epoch 34 | loss: 0.14576 | val_0_rmse: 0.60569 | val_1_rmse: 0.61915 |  0:00:16s
epoch 35 | loss: 0.16259 | val_0_rmse: 0.48936 | val_1_rmse: 0.49024 |  0:00:16s
epoch 36 | loss: 0.15217 | val_0_rmse: 0.46674 | val_1_rmse: 0.469   |  0:00:17s
epoch 37 | loss: 0.20676 | val_0_rmse: 0.51114 | val_1_rmse: 0.52446 |  0:00:17s
epoch 38 | loss: 0.16006 | val_0_rmse: 0.44462 | val_1_rmse: 0.46012 |  0:00:18s
epoch 39 | loss: 0.1446  | val_0_rmse: 0.54185 | val_1_rmse: 0.56826 |  0:00:18s
epoch 40 | loss: 0.16143 | val_0_rmse: 0.47506 | val_1_rmse: 0.48332 |  0:00:19s
epoch 41 | loss: 0.1531  | val_0_rmse: 0.45582 | val_1_rmse: 0.47464 |  0:00:19s
epoch 42 | loss: 0.12462 | val_0_rmse: 0.44219 | val_1_rmse: 0.46025 |  0:00:20s
epoch 43 | loss: 0.11905 | val_0_rmse: 0.43603 | val_1_rmse: 0.45354 |  0:00:20s
epoch 44 | loss: 0.12737 | val_0_rmse: 0.46737 | val_1_rmse: 0.48666 |  0:00:21s
epoch 45 | loss: 0.11383 | val_0_rmse: 0.42186 | val_1_rmse: 0.43488 |  0:00:21s
epoch 46 | loss: 0.12678 | val_0_rmse: 0.53346 | val_1_rmse: 0.55257 |  0:00:21s
epoch 47 | loss: 0.17118 | val_0_rmse: 0.48918 | val_1_rmse: 0.4971  |  0:00:22s
epoch 48 | loss: 0.16638 | val_0_rmse: 0.41566 | val_1_rmse: 0.42675 |  0:00:22s
epoch 49 | loss: 0.13522 | val_0_rmse: 0.43802 | val_1_rmse: 0.45683 |  0:00:23s
epoch 50 | loss: 0.13326 | val_0_rmse: 0.4197  | val_1_rmse: 0.43511 |  0:00:23s
epoch 51 | loss: 0.12366 | val_0_rmse: 0.5073  | val_1_rmse: 0.52891 |  0:00:24s
epoch 52 | loss: 0.12181 | val_0_rmse: 0.42511 | val_1_rmse: 0.44091 |  0:00:24s
epoch 53 | loss: 0.12895 | val_0_rmse: 0.49545 | val_1_rmse: 0.51644 |  0:00:25s
epoch 54 | loss: 0.14598 | val_0_rmse: 0.47098 | val_1_rmse: 0.49694 |  0:00:25s
epoch 55 | loss: 0.13622 | val_0_rmse: 0.45903 | val_1_rmse: 0.4846  |  0:00:26s
epoch 56 | loss: 0.14933 | val_0_rmse: 0.42211 | val_1_rmse: 0.44089 |  0:00:26s
epoch 57 | loss: 0.122   | val_0_rmse: 0.43234 | val_1_rmse: 0.44184 |  0:00:27s
epoch 58 | loss: 0.12265 | val_0_rmse: 0.39292 | val_1_rmse: 0.40061 |  0:00:27s
epoch 59 | loss: 0.11131 | val_0_rmse: 0.40227 | val_1_rmse: 0.41269 |  0:00:27s
epoch 60 | loss: 0.12862 | val_0_rmse: 0.452   | val_1_rmse: 0.46831 |  0:00:28s
epoch 61 | loss: 0.11501 | val_0_rmse: 0.39619 | val_1_rmse: 0.40697 |  0:00:28s
epoch 62 | loss: 0.13106 | val_0_rmse: 0.41853 | val_1_rmse: 0.43433 |  0:00:29s
epoch 63 | loss: 0.11088 | val_0_rmse: 0.41029 | val_1_rmse: 0.41963 |  0:00:29s
epoch 64 | loss: 0.11826 | val_0_rmse: 0.48424 | val_1_rmse: 0.49671 |  0:00:30s
epoch 65 | loss: 0.1155  | val_0_rmse: 0.41516 | val_1_rmse: 0.42142 |  0:00:30s
epoch 66 | loss: 0.1264  | val_0_rmse: 0.48538 | val_1_rmse: 0.49563 |  0:00:31s
epoch 67 | loss: 0.11746 | val_0_rmse: 0.43581 | val_1_rmse: 0.44472 |  0:00:31s
epoch 68 | loss: 0.11416 | val_0_rmse: 0.45544 | val_1_rmse: 0.46555 |  0:00:32s
epoch 69 | loss: 0.1143  | val_0_rmse: 0.4162  | val_1_rmse: 0.4266  |  0:00:32s
epoch 70 | loss: 0.11243 | val_0_rmse: 0.4672  | val_1_rmse: 0.48069 |  0:00:33s
epoch 71 | loss: 0.11544 | val_0_rmse: 0.44163 | val_1_rmse: 0.45347 |  0:00:33s
epoch 72 | loss: 0.11708 | val_0_rmse: 0.40805 | val_1_rmse: 0.42165 |  0:00:34s
epoch 73 | loss: 0.12765 | val_0_rmse: 0.53511 | val_1_rmse: 0.55061 |  0:00:34s
epoch 74 | loss: 0.22471 | val_0_rmse: 0.4476  | val_1_rmse: 0.44685 |  0:00:35s
epoch 75 | loss: 0.20003 | val_0_rmse: 0.38701 | val_1_rmse: 0.38949 |  0:00:35s
epoch 76 | loss: 0.22941 | val_0_rmse: 0.5823  | val_1_rmse: 0.59772 |  0:00:35s
epoch 77 | loss: 0.21234 | val_0_rmse: 0.51013 | val_1_rmse: 0.52516 |  0:00:36s
epoch 78 | loss: 0.21601 | val_0_rmse: 0.427   | val_1_rmse: 0.42755 |  0:00:36s
epoch 79 | loss: 0.19352 | val_0_rmse: 0.38124 | val_1_rmse: 0.38819 |  0:00:37s
epoch 80 | loss: 0.21211 | val_0_rmse: 0.58274 | val_1_rmse: 0.59731 |  0:00:37s
epoch 81 | loss: 0.20983 | val_0_rmse: 0.50128 | val_1_rmse: 0.51522 |  0:00:38s
epoch 82 | loss: 0.19167 | val_0_rmse: 0.41538 | val_1_rmse: 0.41376 |  0:00:38s
epoch 83 | loss: 0.1907  | val_0_rmse: 0.37986 | val_1_rmse: 0.38466 |  0:00:39s
epoch 84 | loss: 0.22142 | val_0_rmse: 0.58088 | val_1_rmse: 0.59641 |  0:00:39s
epoch 85 | loss: 0.21483 | val_0_rmse: 0.48575 | val_1_rmse: 0.50151 |  0:00:40s
epoch 86 | loss: 0.19533 | val_0_rmse: 0.41913 | val_1_rmse: 0.41652 |  0:00:40s
epoch 87 | loss: 0.18527 | val_0_rmse: 0.38755 | val_1_rmse: 0.39175 |  0:00:41s
epoch 88 | loss: 0.21822 | val_0_rmse: 0.56661 | val_1_rmse: 0.58101 |  0:00:41s
epoch 89 | loss: 0.21275 | val_0_rmse: 0.47757 | val_1_rmse: 0.4929  |  0:00:42s
epoch 90 | loss: 0.20503 | val_0_rmse: 0.41635 | val_1_rmse: 0.41517 |  0:00:42s
epoch 91 | loss: 0.1923  | val_0_rmse: 0.38708 | val_1_rmse: 0.39527 |  0:00:42s
epoch 92 | loss: 0.20873 | val_0_rmse: 0.5838  | val_1_rmse: 0.60164 |  0:00:43s
epoch 93 | loss: 0.19721 | val_0_rmse: 0.51186 | val_1_rmse: 0.5282  |  0:00:43s
epoch 94 | loss: 0.20685 | val_0_rmse: 0.41101 | val_1_rmse: 0.41072 |  0:00:44s
epoch 95 | loss: 0.18424 | val_0_rmse: 0.38446 | val_1_rmse: 0.38996 |  0:00:44s
epoch 96 | loss: 0.21161 | val_0_rmse: 0.58245 | val_1_rmse: 0.59924 |  0:00:45s
epoch 97 | loss: 0.21089 | val_0_rmse: 0.47227 | val_1_rmse: 0.48901 |  0:00:45s
epoch 98 | loss: 0.19249 | val_0_rmse: 0.41958 | val_1_rmse: 0.42005 |  0:00:46s
epoch 99 | loss: 0.18072 | val_0_rmse: 0.38568 | val_1_rmse: 0.3931  |  0:00:46s
epoch 100| loss: 0.21178 | val_0_rmse: 0.56962 | val_1_rmse: 0.58621 |  0:00:47s
epoch 101| loss: 0.19987 | val_0_rmse: 0.48707 | val_1_rmse: 0.50184 |  0:00:47s
epoch 102| loss: 0.19703 | val_0_rmse: 0.42284 | val_1_rmse: 0.42056 |  0:00:48s
epoch 103| loss: 0.18288 | val_0_rmse: 0.38379 | val_1_rmse: 0.3868  |  0:00:48s
epoch 104| loss: 0.22434 | val_0_rmse: 0.57439 | val_1_rmse: 0.58963 |  0:00:49s
epoch 105| loss: 0.20488 | val_0_rmse: 0.47227 | val_1_rmse: 0.48586 |  0:00:49s
epoch 106| loss: 0.20377 | val_0_rmse: 0.43074 | val_1_rmse: 0.42933 |  0:00:49s
epoch 107| loss: 0.19177 | val_0_rmse: 0.38358 | val_1_rmse: 0.3874  |  0:00:50s
epoch 108| loss: 0.20389 | val_0_rmse: 0.58126 | val_1_rmse: 0.59723 |  0:00:50s
epoch 109| loss: 0.19223 | val_0_rmse: 0.4906  | val_1_rmse: 0.50651 |  0:00:51s
epoch 110| loss: 0.18857 | val_0_rmse: 0.41164 | val_1_rmse: 0.41063 |  0:00:51s
epoch 111| loss: 0.18117 | val_0_rmse: 0.37962 | val_1_rmse: 0.38601 |  0:00:52s
epoch 112| loss: 0.21282 | val_0_rmse: 0.56273 | val_1_rmse: 0.58023 |  0:00:52s
epoch 113| loss: 0.19823 | val_0_rmse: 0.48858 | val_1_rmse: 0.50605 |  0:00:53s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.38466
Best weights from best epoch are automatically used!
ended training at: 18:27:10
Feature importance:
[('Area', 0.2748484896669767), ('Baths', 0.21971159279978272), ('Beds', 0.04320943049713462), ('Latitude', 0.3865832757085154), ('Longitude', 0.0018010847414073256), ('Month', 0.013432581891271111), ('Year', 0.0604135446949121)]
Mean squared error is of 60018297740.365715
Mean absolute error:184386.90313282073
MAPE:0.3210268426969015
R2 score:0.23794155505578507
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:27:11
epoch 0  | loss: 134.58514| val_0_rmse: 11.1263 | val_1_rmse: 10.90562|  0:00:00s
epoch 1  | loss: 26.34511| val_0_rmse: 19.97318| val_1_rmse: 20.37397|  0:00:00s
epoch 2  | loss: 7.49814 | val_0_rmse: 4.33839 | val_1_rmse: 4.33926 |  0:00:01s
epoch 3  | loss: 1.26798 | val_0_rmse: 1.34805 | val_1_rmse: 1.35242 |  0:00:01s
epoch 4  | loss: 0.66883 | val_0_rmse: 1.45019 | val_1_rmse: 1.45935 |  0:00:02s
epoch 5  | loss: 0.42336 | val_0_rmse: 0.87256 | val_1_rmse: 0.87941 |  0:00:02s
epoch 6  | loss: 0.36808 | val_0_rmse: 0.60258 | val_1_rmse: 0.60939 |  0:00:03s
epoch 7  | loss: 0.27672 | val_0_rmse: 1.1667  | val_1_rmse: 1.167   |  0:00:03s
epoch 8  | loss: 0.26391 | val_0_rmse: 1.86317 | val_1_rmse: 1.86075 |  0:00:04s
epoch 9  | loss: 0.27098 | val_0_rmse: 1.24158 | val_1_rmse: 1.24167 |  0:00:04s
epoch 10 | loss: 0.29073 | val_0_rmse: 1.6221  | val_1_rmse: 1.6277  |  0:00:05s
epoch 11 | loss: 0.23511 | val_0_rmse: 1.21275 | val_1_rmse: 1.22188 |  0:00:05s
epoch 12 | loss: 0.30993 | val_0_rmse: 0.74498 | val_1_rmse: 0.75642 |  0:00:06s
epoch 13 | loss: 0.21232 | val_0_rmse: 0.86268 | val_1_rmse: 0.86072 |  0:00:06s
epoch 14 | loss: 0.22298 | val_0_rmse: 0.73231 | val_1_rmse: 0.72675 |  0:00:07s
epoch 15 | loss: 0.1912  | val_0_rmse: 0.86828 | val_1_rmse: 0.87195 |  0:00:07s
epoch 16 | loss: 0.17509 | val_0_rmse: 0.74219 | val_1_rmse: 0.74699 |  0:00:07s
epoch 17 | loss: 0.17485 | val_0_rmse: 0.61888 | val_1_rmse: 0.61907 |  0:00:08s
epoch 18 | loss: 0.17126 | val_0_rmse: 0.8496  | val_1_rmse: 0.82411 |  0:00:08s
epoch 19 | loss: 0.15433 | val_0_rmse: 1.3319  | val_1_rmse: 1.31822 |  0:00:09s
epoch 20 | loss: 0.14683 | val_0_rmse: 1.51984 | val_1_rmse: 1.50437 |  0:00:09s
epoch 21 | loss: 0.12696 | val_0_rmse: 1.47495 | val_1_rmse: 1.47898 |  0:00:10s
epoch 22 | loss: 0.17546 | val_0_rmse: 1.37338 | val_1_rmse: 1.38111 |  0:00:10s
epoch 23 | loss: 0.17383 | val_0_rmse: 1.22991 | val_1_rmse: 1.24038 |  0:00:11s
epoch 24 | loss: 0.16872 | val_0_rmse: 1.32919 | val_1_rmse: 1.34248 |  0:00:11s
epoch 25 | loss: 0.15154 | val_0_rmse: 1.45246 | val_1_rmse: 1.4672  |  0:00:12s
epoch 26 | loss: 0.13575 | val_0_rmse: 1.29155 | val_1_rmse: 1.30411 |  0:00:12s
epoch 27 | loss: 0.15426 | val_0_rmse: 0.93464 | val_1_rmse: 0.94648 |  0:00:13s
epoch 28 | loss: 0.12189 | val_0_rmse: 1.07898 | val_1_rmse: 1.09024 |  0:00:13s
epoch 29 | loss: 0.15023 | val_0_rmse: 1.1378  | val_1_rmse: 1.14919 |  0:00:13s
epoch 30 | loss: 0.15837 | val_0_rmse: 1.02653 | val_1_rmse: 1.03903 |  0:00:14s
epoch 31 | loss: 0.17152 | val_0_rmse: 0.98244 | val_1_rmse: 0.99478 |  0:00:14s
epoch 32 | loss: 0.1203  | val_0_rmse: 0.74518 | val_1_rmse: 0.7579  |  0:00:15s
epoch 33 | loss: 0.1066  | val_0_rmse: 0.97495 | val_1_rmse: 0.9863  |  0:00:15s
epoch 34 | loss: 0.11006 | val_0_rmse: 0.83346 | val_1_rmse: 0.84424 |  0:00:16s
epoch 35 | loss: 0.11036 | val_0_rmse: 0.66896 | val_1_rmse: 0.68071 |  0:00:16s
epoch 36 | loss: 0.11025 | val_0_rmse: 0.79123 | val_1_rmse: 0.80363 |  0:00:17s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.60939
Best weights from best epoch are automatically used!
ended training at: 18:27:28
Feature importance:
[('Area', 0.0), ('Baths', 0.20867686536223165), ('Beds', 0.40308891596548235), ('Latitude', 0.031656903051981135), ('Longitude', 0.245882918210758), ('Month', 0.0), ('Year', 0.11069439740954687)]
Mean squared error is of 149472684683.19803
Mean absolute error:305564.3160912198
MAPE:0.6522109456536959
R2 score:-0.8398011366306455
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:27:28
epoch 0  | loss: 129.97763| val_0_rmse: 27.09017| val_1_rmse: 27.02493|  0:00:00s
epoch 1  | loss: 23.77252| val_0_rmse: 17.46477| val_1_rmse: 17.46698|  0:00:00s
epoch 2  | loss: 10.07384| val_0_rmse: 10.51236| val_1_rmse: 10.48644|  0:00:01s
epoch 3  | loss: 1.49964 | val_0_rmse: 17.35961| val_1_rmse: 17.29756|  0:00:01s
epoch 4  | loss: 0.93807 | val_0_rmse: 10.33395| val_1_rmse: 10.24807|  0:00:02s
epoch 5  | loss: 0.88189 | val_0_rmse: 8.7042  | val_1_rmse: 8.57919 |  0:00:02s
epoch 6  | loss: 0.49572 | val_0_rmse: 5.4825  | val_1_rmse: 5.57236 |  0:00:03s
epoch 7  | loss: 0.52464 | val_0_rmse: 2.4261  | val_1_rmse: 2.36437 |  0:00:03s
epoch 8  | loss: 0.37109 | val_0_rmse: 2.22088 | val_1_rmse: 2.22465 |  0:00:04s
epoch 9  | loss: 0.31788 | val_0_rmse: 1.34546 | val_1_rmse: 1.3466  |  0:00:04s
epoch 10 | loss: 0.26096 | val_0_rmse: 0.81671 | val_1_rmse: 0.81393 |  0:00:05s
epoch 11 | loss: 0.18776 | val_0_rmse: 0.55071 | val_1_rmse: 0.52803 |  0:00:05s
epoch 12 | loss: 0.18992 | val_0_rmse: 0.63033 | val_1_rmse: 0.61206 |  0:00:06s
epoch 13 | loss: 0.2054  | val_0_rmse: 1.1559  | val_1_rmse: 1.16839 |  0:00:06s
epoch 14 | loss: 0.22337 | val_0_rmse: 1.34027 | val_1_rmse: 1.35926 |  0:00:07s
epoch 15 | loss: 0.2746  | val_0_rmse: 1.92978 | val_1_rmse: 1.94895 |  0:00:07s
epoch 16 | loss: 0.26305 | val_0_rmse: 2.34518 | val_1_rmse: 2.3682  |  0:00:07s
epoch 17 | loss: 0.18158 | val_0_rmse: 2.41504 | val_1_rmse: 2.43556 |  0:00:08s
epoch 18 | loss: 0.12773 | val_0_rmse: 2.79103 | val_1_rmse: 2.8067  |  0:00:08s
epoch 19 | loss: 0.12978 | val_0_rmse: 2.68549 | val_1_rmse: 2.69907 |  0:00:09s
epoch 20 | loss: 0.11761 | val_0_rmse: 2.79421 | val_1_rmse: 2.80555 |  0:00:09s
epoch 21 | loss: 0.13699 | val_0_rmse: 2.82342 | val_1_rmse: 2.82979 |  0:00:10s
epoch 22 | loss: 0.13997 | val_0_rmse: 2.07883 | val_1_rmse: 2.08459 |  0:00:10s
epoch 23 | loss: 0.10424 | val_0_rmse: 2.03884 | val_1_rmse: 2.04372 |  0:00:11s
epoch 24 | loss: 0.0966  | val_0_rmse: 1.92452 | val_1_rmse: 1.92787 |  0:00:11s
epoch 25 | loss: 0.10807 | val_0_rmse: 2.14139 | val_1_rmse: 2.13841 |  0:00:12s
epoch 26 | loss: 0.09454 | val_0_rmse: 1.72907 | val_1_rmse: 1.72567 |  0:00:12s
epoch 27 | loss: 0.09175 | val_0_rmse: 1.8061  | val_1_rmse: 1.80156 |  0:00:13s
epoch 28 | loss: 0.10201 | val_0_rmse: 1.52884 | val_1_rmse: 1.52562 |  0:00:13s
epoch 29 | loss: 0.09723 | val_0_rmse: 1.53953 | val_1_rmse: 1.54053 |  0:00:14s
epoch 30 | loss: 0.09575 | val_0_rmse: 1.35569 | val_1_rmse: 1.35625 |  0:00:14s
epoch 31 | loss: 0.09091 | val_0_rmse: 1.49161 | val_1_rmse: 1.49151 |  0:00:14s
epoch 32 | loss: 0.08217 | val_0_rmse: 1.13995 | val_1_rmse: 1.14161 |  0:00:15s
epoch 33 | loss: 0.09957 | val_0_rmse: 1.01619 | val_1_rmse: 1.01948 |  0:00:15s
epoch 34 | loss: 0.11838 | val_0_rmse: 0.7637  | val_1_rmse: 0.76646 |  0:00:16s
epoch 35 | loss: 0.20517 | val_0_rmse: 0.89918 | val_1_rmse: 0.90505 |  0:00:16s
epoch 36 | loss: 0.1934  | val_0_rmse: 1.01731 | val_1_rmse: 1.02312 |  0:00:17s
epoch 37 | loss: 0.12843 | val_0_rmse: 0.93052 | val_1_rmse: 0.93764 |  0:00:17s
epoch 38 | loss: 0.12726 | val_0_rmse: 1.05966 | val_1_rmse: 1.06634 |  0:00:18s
epoch 39 | loss: 0.13118 | val_0_rmse: 1.01637 | val_1_rmse: 1.02175 |  0:00:18s
epoch 40 | loss: 0.10709 | val_0_rmse: 0.91132 | val_1_rmse: 0.91787 |  0:00:19s
epoch 41 | loss: 0.11961 | val_0_rmse: 0.93967 | val_1_rmse: 0.94693 |  0:00:19s

Early stopping occured at epoch 41 with best_epoch = 11 and best_val_1_rmse = 0.52803
Best weights from best epoch are automatically used!
ended training at: 18:27:48
Feature importance:
[('Area', 0.0), ('Baths', 0.03564019435544752), ('Beds', 0.2553610617912212), ('Latitude', 0.026471773633843232), ('Longitude', 0.3718214449419523), ('Month', 0.23527903657197283), ('Year', 0.07542648870556293)]
Mean squared error is of 197755727841.20944
Mean absolute error:316738.4342669558
MAPE:0.6312251214735619
R2 score:-1.4661534142446504
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:27:48
epoch 0  | loss: 136.79251| val_0_rmse: 5.91527 | val_1_rmse: 5.99698 |  0:00:00s
epoch 1  | loss: 26.84866| val_0_rmse: 53.15569| val_1_rmse: 53.54456|  0:00:00s
epoch 2  | loss: 11.16112| val_0_rmse: 30.74486| val_1_rmse: 30.71103|  0:00:01s
epoch 3  | loss: 1.70246 | val_0_rmse: 7.85249 | val_1_rmse: 7.81584 |  0:00:01s
epoch 4  | loss: 0.67573 | val_0_rmse: 2.70256 | val_1_rmse: 2.66123 |  0:00:02s
epoch 5  | loss: 0.4764  | val_0_rmse: 1.82693 | val_1_rmse: 1.94943 |  0:00:02s
epoch 6  | loss: 0.37551 | val_0_rmse: 2.75874 | val_1_rmse: 2.75029 |  0:00:03s
epoch 7  | loss: 0.36397 | val_0_rmse: 1.94588 | val_1_rmse: 1.95271 |  0:00:03s
epoch 8  | loss: 0.30953 | val_0_rmse: 1.6865  | val_1_rmse: 1.66134 |  0:00:04s
epoch 9  | loss: 0.31344 | val_0_rmse: 1.17053 | val_1_rmse: 1.13342 |  0:00:04s
epoch 10 | loss: 0.24728 | val_0_rmse: 0.6093  | val_1_rmse: 0.59922 |  0:00:05s
epoch 11 | loss: 0.2996  | val_0_rmse: 0.71722 | val_1_rmse: 0.70149 |  0:00:05s
epoch 12 | loss: 0.34191 | val_0_rmse: 0.84307 | val_1_rmse: 0.84348 |  0:00:06s
epoch 13 | loss: 0.32179 | val_0_rmse: 1.10052 | val_1_rmse: 1.07534 |  0:00:06s
epoch 14 | loss: 0.27326 | val_0_rmse: 1.00354 | val_1_rmse: 0.97676 |  0:00:07s
epoch 15 | loss: 0.30834 | val_0_rmse: 0.85313 | val_1_rmse: 0.82454 |  0:00:07s
epoch 16 | loss: 0.26997 | val_0_rmse: 0.78382 | val_1_rmse: 0.7641  |  0:00:07s
epoch 17 | loss: 0.21932 | val_0_rmse: 0.77713 | val_1_rmse: 0.75893 |  0:00:08s
epoch 18 | loss: 0.19218 | val_0_rmse: 0.75234 | val_1_rmse: 0.73888 |  0:00:08s
epoch 19 | loss: 0.17028 | val_0_rmse: 0.91995 | val_1_rmse: 0.90544 |  0:00:09s
epoch 20 | loss: 0.13919 | val_0_rmse: 0.79181 | val_1_rmse: 0.7895  |  0:00:09s
epoch 21 | loss: 0.14014 | val_0_rmse: 0.9281  | val_1_rmse: 0.93233 |  0:00:10s
epoch 22 | loss: 0.14256 | val_0_rmse: 0.89847 | val_1_rmse: 0.90916 |  0:00:10s
epoch 23 | loss: 0.15513 | val_0_rmse: 0.96347 | val_1_rmse: 0.97893 |  0:00:11s
epoch 24 | loss: 0.17297 | val_0_rmse: 1.14487 | val_1_rmse: 1.18955 |  0:00:11s
epoch 25 | loss: 0.21302 | val_0_rmse: 1.20724 | val_1_rmse: 1.24466 |  0:00:12s
epoch 26 | loss: 0.14652 | val_0_rmse: 1.53163 | val_1_rmse: 1.57576 |  0:00:12s
epoch 27 | loss: 0.13251 | val_0_rmse: 1.43618 | val_1_rmse: 1.44466 |  0:00:13s
epoch 28 | loss: 0.14879 | val_0_rmse: 1.44759 | val_1_rmse: 1.45982 |  0:00:13s
epoch 29 | loss: 0.12274 | val_0_rmse: 1.36218 | val_1_rmse: 1.36742 |  0:00:14s
epoch 30 | loss: 0.14599 | val_0_rmse: 1.27921 | val_1_rmse: 1.27697 |  0:00:14s
epoch 31 | loss: 0.17534 | val_0_rmse: 1.14434 | val_1_rmse: 1.12738 |  0:00:14s
epoch 32 | loss: 0.20712 | val_0_rmse: 0.99184 | val_1_rmse: 0.97849 |  0:00:15s
epoch 33 | loss: 0.15055 | val_0_rmse: 1.03979 | val_1_rmse: 1.02662 |  0:00:15s
epoch 34 | loss: 0.17697 | val_0_rmse: 1.33065 | val_1_rmse: 1.31312 |  0:00:16s
epoch 35 | loss: 0.13932 | val_0_rmse: 1.0631  | val_1_rmse: 1.06735 |  0:00:16s
epoch 36 | loss: 0.10606 | val_0_rmse: 1.16306 | val_1_rmse: 1.14094 |  0:00:17s
epoch 37 | loss: 0.08606 | val_0_rmse: 1.02972 | val_1_rmse: 1.02102 |  0:00:17s
epoch 38 | loss: 0.09151 | val_0_rmse: 0.84399 | val_1_rmse: 0.84453 |  0:00:18s
epoch 39 | loss: 0.09631 | val_0_rmse: 0.80853 | val_1_rmse: 0.80582 |  0:00:18s
epoch 40 | loss: 0.09002 | val_0_rmse: 0.74556 | val_1_rmse: 0.73853 |  0:00:19s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.59922
Best weights from best epoch are automatically used!
ended training at: 18:28:08
Feature importance:
[('Area', 0.017887978544995325), ('Baths', 0.2616445928407743), ('Beds', 0.0003307382173024543), ('Latitude', 0.26240729936394813), ('Longitude', 0.06270230163942075), ('Month', 0.14328340054965555), ('Year', 0.2517436888439035)]
Mean squared error is of 124974566540.89206
Mean absolute error:295062.93126636435
MAPE:0.6612830882095093
R2 score:-0.5615172748559831
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:28:08
epoch 0  | loss: 135.33133| val_0_rmse: 6.54809 | val_1_rmse: 6.51828 |  0:00:00s
epoch 1  | loss: 26.27387| val_0_rmse: 35.65109| val_1_rmse: 35.61095|  0:00:00s
epoch 2  | loss: 9.42862 | val_0_rmse: 5.69203 | val_1_rmse: 5.7607  |  0:00:01s
epoch 3  | loss: 1.24276 | val_0_rmse: 7.31419 | val_1_rmse: 7.31689 |  0:00:01s
epoch 4  | loss: 0.7589  | val_0_rmse: 0.73611 | val_1_rmse: 0.71526 |  0:00:02s
epoch 5  | loss: 0.35321 | val_0_rmse: 0.65837 | val_1_rmse: 0.63634 |  0:00:02s
epoch 6  | loss: 0.59069 | val_0_rmse: 1.40989 | val_1_rmse: 1.41372 |  0:00:03s
epoch 7  | loss: 0.47217 | val_0_rmse: 1.12265 | val_1_rmse: 1.09756 |  0:00:03s
epoch 8  | loss: 0.3209  | val_0_rmse: 0.96863 | val_1_rmse: 0.9539  |  0:00:04s
epoch 9  | loss: 0.2775  | val_0_rmse: 1.1439  | val_1_rmse: 1.12939 |  0:00:04s
epoch 10 | loss: 0.19441 | val_0_rmse: 1.7318  | val_1_rmse: 1.71159 |  0:00:05s
epoch 11 | loss: 0.14547 | val_0_rmse: 1.8743  | val_1_rmse: 1.85323 |  0:00:05s
epoch 12 | loss: 0.14523 | val_0_rmse: 1.39802 | val_1_rmse: 1.4026  |  0:00:06s
epoch 13 | loss: 0.14161 | val_0_rmse: 2.04795 | val_1_rmse: 2.06318 |  0:00:06s
epoch 14 | loss: 0.13044 | val_0_rmse: 1.50913 | val_1_rmse: 1.52515 |  0:00:07s
epoch 15 | loss: 0.12279 | val_0_rmse: 1.01968 | val_1_rmse: 1.02927 |  0:00:07s
epoch 16 | loss: 0.13969 | val_0_rmse: 1.20084 | val_1_rmse: 1.20765 |  0:00:07s
epoch 17 | loss: 0.15684 | val_0_rmse: 0.99699 | val_1_rmse: 1.00683 |  0:00:08s
epoch 18 | loss: 0.1135  | val_0_rmse: 0.76757 | val_1_rmse: 0.77461 |  0:00:08s
epoch 19 | loss: 0.11295 | val_0_rmse: 0.88295 | val_1_rmse: 0.88615 |  0:00:09s
epoch 20 | loss: 0.12975 | val_0_rmse: 0.53487 | val_1_rmse: 0.52535 |  0:00:09s
epoch 21 | loss: 0.14422 | val_0_rmse: 0.87055 | val_1_rmse: 0.8686  |  0:00:10s
epoch 22 | loss: 0.12688 | val_0_rmse: 1.19284 | val_1_rmse: 1.19396 |  0:00:10s
epoch 23 | loss: 0.13444 | val_0_rmse: 0.58592 | val_1_rmse: 0.58357 |  0:00:11s
epoch 24 | loss: 0.14452 | val_0_rmse: 0.53207 | val_1_rmse: 0.52124 |  0:00:11s
epoch 25 | loss: 0.12398 | val_0_rmse: 0.51858 | val_1_rmse: 0.50506 |  0:00:12s
epoch 26 | loss: 0.10505 | val_0_rmse: 0.4798  | val_1_rmse: 0.46983 |  0:00:12s
epoch 27 | loss: 0.11006 | val_0_rmse: 0.43963 | val_1_rmse: 0.42082 |  0:00:13s
epoch 28 | loss: 0.1046  | val_0_rmse: 0.42649 | val_1_rmse: 0.41203 |  0:00:13s
epoch 29 | loss: 0.10898 | val_0_rmse: 0.48127 | val_1_rmse: 0.45953 |  0:00:14s
epoch 30 | loss: 0.09072 | val_0_rmse: 0.52403 | val_1_rmse: 0.50148 |  0:00:14s
epoch 31 | loss: 0.10028 | val_0_rmse: 0.43833 | val_1_rmse: 0.41864 |  0:00:15s
epoch 32 | loss: 0.10346 | val_0_rmse: 0.41246 | val_1_rmse: 0.39501 |  0:00:15s
epoch 33 | loss: 0.16754 | val_0_rmse: 0.44649 | val_1_rmse: 0.44286 |  0:00:15s
epoch 34 | loss: 0.28615 | val_0_rmse: 0.45682 | val_1_rmse: 0.44267 |  0:00:16s
epoch 35 | loss: 0.1582  | val_0_rmse: 0.93274 | val_1_rmse: 0.91416 |  0:00:16s
epoch 36 | loss: 0.14105 | val_0_rmse: 0.50278 | val_1_rmse: 0.48399 |  0:00:17s
epoch 37 | loss: 0.12187 | val_0_rmse: 0.40611 | val_1_rmse: 0.39538 |  0:00:17s
epoch 38 | loss: 0.11254 | val_0_rmse: 0.47761 | val_1_rmse: 0.46501 |  0:00:18s
epoch 39 | loss: 0.08859 | val_0_rmse: 0.53018 | val_1_rmse: 0.51496 |  0:00:18s
epoch 40 | loss: 0.10104 | val_0_rmse: 0.53907 | val_1_rmse: 0.524   |  0:00:19s
epoch 41 | loss: 0.11055 | val_0_rmse: 0.46578 | val_1_rmse: 0.4529  |  0:00:19s
epoch 42 | loss: 0.11762 | val_0_rmse: 0.5286  | val_1_rmse: 0.51574 |  0:00:20s
epoch 43 | loss: 0.08718 | val_0_rmse: 0.59937 | val_1_rmse: 0.58931 |  0:00:20s
epoch 44 | loss: 0.09248 | val_0_rmse: 0.63274 | val_1_rmse: 0.63156 |  0:00:21s
epoch 45 | loss: 0.08851 | val_0_rmse: 0.65071 | val_1_rmse: 0.65903 |  0:00:21s
epoch 46 | loss: 0.08387 | val_0_rmse: 0.62155 | val_1_rmse: 0.62516 |  0:00:22s
epoch 47 | loss: 0.08192 | val_0_rmse: 0.6081  | val_1_rmse: 0.61645 |  0:00:22s
epoch 48 | loss: 0.08526 | val_0_rmse: 0.63775 | val_1_rmse: 0.64446 |  0:00:23s
epoch 49 | loss: 0.08359 | val_0_rmse: 0.6978  | val_1_rmse: 0.71075 |  0:00:23s
epoch 50 | loss: 0.0772  | val_0_rmse: 0.69865 | val_1_rmse: 0.70955 |  0:00:23s
epoch 51 | loss: 0.08823 | val_0_rmse: 0.76693 | val_1_rmse: 0.76471 |  0:00:24s
epoch 52 | loss: 0.09147 | val_0_rmse: 0.78271 | val_1_rmse: 0.77936 |  0:00:24s
epoch 53 | loss: 0.08875 | val_0_rmse: 0.70689 | val_1_rmse: 0.71524 |  0:00:25s
epoch 54 | loss: 0.07897 | val_0_rmse: 0.65052 | val_1_rmse: 0.65555 |  0:00:25s
epoch 55 | loss: 0.08114 | val_0_rmse: 0.61672 | val_1_rmse: 0.62693 |  0:00:26s
epoch 56 | loss: 0.07727 | val_0_rmse: 0.58282 | val_1_rmse: 0.57754 |  0:00:26s
epoch 57 | loss: 0.09069 | val_0_rmse: 0.5446  | val_1_rmse: 0.53964 |  0:00:27s
epoch 58 | loss: 0.07933 | val_0_rmse: 0.52722 | val_1_rmse: 0.51613 |  0:00:27s
epoch 59 | loss: 0.07219 | val_0_rmse: 0.59437 | val_1_rmse: 0.56758 |  0:00:28s
epoch 60 | loss: 0.07233 | val_0_rmse: 0.56368 | val_1_rmse: 0.53675 |  0:00:28s
epoch 61 | loss: 0.07137 | val_0_rmse: 0.58222 | val_1_rmse: 0.55995 |  0:00:29s
epoch 62 | loss: 0.0742  | val_0_rmse: 0.65085 | val_1_rmse: 0.61922 |  0:00:29s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.39501
Best weights from best epoch are automatically used!
ended training at: 18:28:37
Feature importance:
[('Area', 0.060316975239390454), ('Baths', 0.29242065408380663), ('Beds', 0.0), ('Latitude', 0.3125855474021653), ('Longitude', 0.15617813110191223), ('Month', 0.0), ('Year', 0.17849869217272538)]
Mean squared error is of 66659470848.416374
Mean absolute error:198876.2189980284
MAPE:0.35368656015500394
R2 score:0.17798645992095863
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:28:37
epoch 0  | loss: 142.85861| val_0_rmse: 8.33819 | val_1_rmse: 8.53032 |  0:00:00s
epoch 1  | loss: 107.41288| val_0_rmse: 7.45127 | val_1_rmse: 7.40365 |  0:00:00s
epoch 2  | loss: 77.94424| val_0_rmse: 4.13352 | val_1_rmse: 4.17059 |  0:00:00s
epoch 3  | loss: 47.36789| val_0_rmse: 2.4708  | val_1_rmse: 2.61091 |  0:00:00s
epoch 4  | loss: 19.26138| val_0_rmse: 6.02093 | val_1_rmse: 5.92627 |  0:00:00s
epoch 5  | loss: 4.16469 | val_0_rmse: 11.35087| val_1_rmse: 11.31358|  0:00:00s
epoch 6  | loss: 6.44169 | val_0_rmse: 9.40231 | val_1_rmse: 9.44253 |  0:00:00s
epoch 7  | loss: 7.27215 | val_0_rmse: 6.16629 | val_1_rmse: 6.2517  |  0:00:01s
epoch 8  | loss: 3.30748 | val_0_rmse: 2.89897 | val_1_rmse: 3.03655 |  0:00:01s
epoch 9  | loss: 1.16069 | val_0_rmse: 1.71388 | val_1_rmse: 1.75808 |  0:00:01s
epoch 10 | loss: 1.55669 | val_0_rmse: 1.7451  | val_1_rmse: 1.74785 |  0:00:01s
epoch 11 | loss: 0.85871 | val_0_rmse: 3.93847 | val_1_rmse: 4.0094  |  0:00:01s
epoch 12 | loss: 0.74253 | val_0_rmse: 7.68769 | val_1_rmse: 7.6485  |  0:00:01s
epoch 13 | loss: 0.61142 | val_0_rmse: 8.71506 | val_1_rmse: 8.68002 |  0:00:01s
epoch 14 | loss: 0.57302 | val_0_rmse: 8.49855 | val_1_rmse: 8.46189 |  0:00:02s
epoch 15 | loss: 0.53663 | val_0_rmse: 6.96897 | val_1_rmse: 6.93079 |  0:00:02s
epoch 16 | loss: 0.44337 | val_0_rmse: 6.82364 | val_1_rmse: 6.78086 |  0:00:02s
epoch 17 | loss: 0.92059 | val_0_rmse: 7.5923  | val_1_rmse: 7.5478  |  0:00:02s
epoch 18 | loss: 0.47633 | val_0_rmse: 6.11098 | val_1_rmse: 6.07089 |  0:00:02s
epoch 19 | loss: 0.67829 | val_0_rmse: 4.87227 | val_1_rmse: 4.81974 |  0:00:02s
epoch 20 | loss: 0.44038 | val_0_rmse: 5.19149 | val_1_rmse: 5.14233 |  0:00:02s
epoch 21 | loss: 0.43378 | val_0_rmse: 5.16676 | val_1_rmse: 5.11283 |  0:00:02s
epoch 22 | loss: 0.36988 | val_0_rmse: 3.50669 | val_1_rmse: 3.46629 |  0:00:03s
epoch 23 | loss: 0.34747 | val_0_rmse: 2.77965 | val_1_rmse: 2.72794 |  0:00:03s
epoch 24 | loss: 0.29838 | val_0_rmse: 4.53227 | val_1_rmse: 4.46683 |  0:00:03s
epoch 25 | loss: 0.28496 | val_0_rmse: 4.94928 | val_1_rmse: 4.89508 |  0:00:03s
epoch 26 | loss: 0.25752 | val_0_rmse: 3.5698  | val_1_rmse: 3.54701 |  0:00:03s
epoch 27 | loss: 0.25659 | val_0_rmse: 3.20046 | val_1_rmse: 3.15453 |  0:00:03s
epoch 28 | loss: 0.21745 | val_0_rmse: 3.81606 | val_1_rmse: 3.763   |  0:00:03s
epoch 29 | loss: 0.24684 | val_0_rmse: 3.8583  | val_1_rmse: 3.80505 |  0:00:04s
epoch 30 | loss: 0.21886 | val_0_rmse: 3.75221 | val_1_rmse: 3.68822 |  0:00:04s
epoch 31 | loss: 0.21563 | val_0_rmse: 3.59718 | val_1_rmse: 3.53202 |  0:00:04s
epoch 32 | loss: 0.21026 | val_0_rmse: 3.85198 | val_1_rmse: 3.7887  |  0:00:04s
epoch 33 | loss: 0.1992  | val_0_rmse: 4.08894 | val_1_rmse: 4.0256  |  0:00:04s
epoch 34 | loss: 0.20367 | val_0_rmse: 3.66975 | val_1_rmse: 3.60763 |  0:00:04s
epoch 35 | loss: 0.20801 | val_0_rmse: 3.74533 | val_1_rmse: 3.6845  |  0:00:04s
epoch 36 | loss: 0.20859 | val_0_rmse: 3.25256 | val_1_rmse: 3.19281 |  0:00:05s
epoch 37 | loss: 0.23018 | val_0_rmse: 3.33076 | val_1_rmse: 3.27268 |  0:00:05s
epoch 38 | loss: 0.204   | val_0_rmse: 2.949   | val_1_rmse: 2.89959 |  0:00:05s
epoch 39 | loss: 0.2024  | val_0_rmse: 2.59062 | val_1_rmse: 2.54513 |  0:00:05s
epoch 40 | loss: 0.19738 | val_0_rmse: 2.29823 | val_1_rmse: 2.24979 |  0:00:05s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 1.74785
Best weights from best epoch are automatically used!
ended training at: 18:28:43
Feature importance:
[('Area', 0.002274932348226329), ('Baths', 0.0), ('Beds', 0.34252096432941975), ('Latitude', 0.03427035984110527), ('Longitude', 0.03832480717451701), ('Month', 0.5826089363067317), ('Year', 0.0)]
Mean squared error is of 112721556816473.1
Mean absolute error:604479.8506511805
MAPE:6.891906282910075
R2 score:-15638.918974251574
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:28:44
epoch 0  | loss: 143.13507| val_0_rmse: 6.32965 | val_1_rmse: 6.41715 |  0:00:00s
epoch 1  | loss: 108.15259| val_0_rmse: 8.92643 | val_1_rmse: 9.00524 |  0:00:00s
epoch 2  | loss: 75.78578| val_0_rmse: 9.66986 | val_1_rmse: 9.55613 |  0:00:00s
epoch 3  | loss: 42.63747| val_0_rmse: 12.32382| val_1_rmse: 12.64215|  0:00:00s
epoch 4  | loss: 13.8451 | val_0_rmse: 14.72501| val_1_rmse: 14.92579|  0:00:00s
epoch 5  | loss: 3.16493 | val_0_rmse: 9.0328  | val_1_rmse: 8.99352 |  0:00:00s
epoch 6  | loss: 7.59414 | val_0_rmse: 4.09493 | val_1_rmse: 4.1214  |  0:00:00s
epoch 7  | loss: 6.36918 | val_0_rmse: 1.60866 | val_1_rmse: 1.67252 |  0:00:01s
epoch 8  | loss: 1.74995 | val_0_rmse: 1.78564 | val_1_rmse: 1.75133 |  0:00:01s
epoch 9  | loss: 1.2934  | val_0_rmse: 17.62327| val_1_rmse: 17.56783|  0:00:01s
epoch 10 | loss: 1.02932 | val_0_rmse: 19.64755| val_1_rmse: 19.75307|  0:00:01s
epoch 11 | loss: 0.81602 | val_0_rmse: 15.46569| val_1_rmse: 15.57886|  0:00:01s
epoch 12 | loss: 0.63321 | val_0_rmse: 8.08893 | val_1_rmse: 8.08228 |  0:00:01s
epoch 13 | loss: 0.4998  | val_0_rmse: 3.77618 | val_1_rmse: 3.68369 |  0:00:01s
epoch 14 | loss: 0.45475 | val_0_rmse: 1.35571 | val_1_rmse: 1.18201 |  0:00:02s
epoch 15 | loss: 0.43022 | val_0_rmse: 1.00672 | val_1_rmse: 0.89629 |  0:00:02s
epoch 16 | loss: 0.42139 | val_0_rmse: 1.30896 | val_1_rmse: 1.25911 |  0:00:02s
epoch 17 | loss: 0.44081 | val_0_rmse: 1.13034 | val_1_rmse: 1.16904 |  0:00:02s
epoch 18 | loss: 0.36181 | val_0_rmse: 1.39612 | val_1_rmse: 1.42472 |  0:00:02s
epoch 19 | loss: 0.3258  | val_0_rmse: 0.96494 | val_1_rmse: 0.95765 |  0:00:02s
epoch 20 | loss: 0.29469 | val_0_rmse: 0.77289 | val_1_rmse: 0.75352 |  0:00:02s
epoch 21 | loss: 0.29669 | val_0_rmse: 0.52037 | val_1_rmse: 0.51372 |  0:00:02s
epoch 22 | loss: 0.26579 | val_0_rmse: 0.88207 | val_1_rmse: 0.90532 |  0:00:03s
epoch 23 | loss: 0.25237 | val_0_rmse: 0.68928 | val_1_rmse: 0.71104 |  0:00:03s
epoch 24 | loss: 0.23866 | val_0_rmse: 0.81681 | val_1_rmse: 0.844   |  0:00:03s
epoch 25 | loss: 0.22457 | val_0_rmse: 0.66105 | val_1_rmse: 0.68522 |  0:00:03s
epoch 26 | loss: 0.2351  | val_0_rmse: 0.74087 | val_1_rmse: 0.76858 |  0:00:03s
epoch 27 | loss: 0.20533 | val_0_rmse: 0.79367 | val_1_rmse: 0.81881 |  0:00:03s
epoch 28 | loss: 0.2137  | val_0_rmse: 0.67971 | val_1_rmse: 0.69564 |  0:00:03s
epoch 29 | loss: 0.20661 | val_0_rmse: 0.76108 | val_1_rmse: 0.78094 |  0:00:04s
epoch 30 | loss: 0.20928 | val_0_rmse: 0.84    | val_1_rmse: 0.86731 |  0:00:04s
epoch 31 | loss: 0.20512 | val_0_rmse: 0.78867 | val_1_rmse: 0.81324 |  0:00:04s
epoch 32 | loss: 0.19893 | val_0_rmse: 0.91907 | val_1_rmse: 0.9476  |  0:00:04s
epoch 33 | loss: 0.19981 | val_0_rmse: 0.84499 | val_1_rmse: 0.87395 |  0:00:04s
epoch 34 | loss: 0.18872 | val_0_rmse: 0.90991 | val_1_rmse: 0.941   |  0:00:04s
epoch 35 | loss: 0.18899 | val_0_rmse: 1.02619 | val_1_rmse: 1.05995 |  0:00:04s
epoch 36 | loss: 0.19583 | val_0_rmse: 1.02939 | val_1_rmse: 1.06215 |  0:00:05s
epoch 37 | loss: 0.18633 | val_0_rmse: 1.14459 | val_1_rmse: 1.18121 |  0:00:05s
epoch 38 | loss: 0.18739 | val_0_rmse: 1.16819 | val_1_rmse: 1.20715 |  0:00:05s
epoch 39 | loss: 0.19699 | val_0_rmse: 0.76006 | val_1_rmse: 0.79161 |  0:00:05s
epoch 40 | loss: 0.19906 | val_0_rmse: 0.84433 | val_1_rmse: 0.88187 |  0:00:05s
epoch 41 | loss: 0.17459 | val_0_rmse: 0.97146 | val_1_rmse: 1.01785 |  0:00:05s
epoch 42 | loss: 0.20489 | val_0_rmse: 0.87392 | val_1_rmse: 0.91382 |  0:00:05s
epoch 43 | loss: 0.18233 | val_0_rmse: 0.92284 | val_1_rmse: 0.96063 |  0:00:05s
epoch 44 | loss: 0.18511 | val_0_rmse: 0.97379 | val_1_rmse: 1.0169  |  0:00:06s
epoch 45 | loss: 0.18723 | val_0_rmse: 1.07031 | val_1_rmse: 1.11823 |  0:00:06s
epoch 46 | loss: 0.21689 | val_0_rmse: 0.86774 | val_1_rmse: 0.91269 |  0:00:06s
epoch 47 | loss: 0.17985 | val_0_rmse: 0.88856 | val_1_rmse: 0.92787 |  0:00:06s
epoch 48 | loss: 0.20287 | val_0_rmse: 0.84014 | val_1_rmse: 0.87937 |  0:00:06s
epoch 49 | loss: 0.17797 | val_0_rmse: 0.91793 | val_1_rmse: 0.96024 |  0:00:06s
epoch 50 | loss: 0.18428 | val_0_rmse: 0.66177 | val_1_rmse: 0.69463 |  0:00:06s
epoch 51 | loss: 0.19549 | val_0_rmse: 0.81976 | val_1_rmse: 0.84952 |  0:00:07s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.51372
Best weights from best epoch are automatically used!
ended training at: 18:28:51
Feature importance:
[('Area', 0.2695664214916209), ('Baths', 0.3030514238920545), ('Beds', 0.22579732932428567), ('Latitude', 0.04385363753319874), ('Longitude', 0.0), ('Month', 0.042705132974708855), ('Year', 0.11502605478413136)]
Mean squared error is of 5307316737.368721
Mean absolute error:54219.95976943681
MAPE:0.49153412057687856
R2 score:0.18743564669633028
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:28:51
epoch 0  | loss: 141.74902| val_0_rmse: 22.43278| val_1_rmse: 22.59884|  0:00:00s
epoch 1  | loss: 103.38872| val_0_rmse: 2.49645 | val_1_rmse: 2.39605 |  0:00:00s
epoch 2  | loss: 71.58292| val_0_rmse: 6.14443 | val_1_rmse: 6.11934 |  0:00:00s
epoch 3  | loss: 39.03906| val_0_rmse: 6.64249 | val_1_rmse: 6.49802 |  0:00:00s
epoch 4  | loss: 13.25286| val_0_rmse: 10.73828| val_1_rmse: 10.7347 |  0:00:00s
epoch 5  | loss: 3.05119 | val_0_rmse: 9.74121 | val_1_rmse: 9.84064 |  0:00:00s
epoch 6  | loss: 8.18222 | val_0_rmse: 8.02674 | val_1_rmse: 8.08411 |  0:00:00s
epoch 7  | loss: 8.19584 | val_0_rmse: 6.62675 | val_1_rmse: 6.67526 |  0:00:01s
epoch 8  | loss: 3.04306 | val_0_rmse: 6.85738 | val_1_rmse: 6.85931 |  0:00:01s
epoch 9  | loss: 1.29703 | val_0_rmse: 7.68014 | val_1_rmse: 7.71453 |  0:00:01s
epoch 10 | loss: 1.26238 | val_0_rmse: 4.09692 | val_1_rmse: 4.20419 |  0:00:01s
epoch 11 | loss: 0.66038 | val_0_rmse: 2.82179 | val_1_rmse: 2.67381 |  0:00:01s
epoch 12 | loss: 0.61964 | val_0_rmse: 2.21642 | val_1_rmse: 2.15519 |  0:00:01s
epoch 13 | loss: 0.65536 | val_0_rmse: 1.70182 | val_1_rmse: 1.67272 |  0:00:01s
epoch 14 | loss: 0.51689 | val_0_rmse: 1.39371 | val_1_rmse: 1.50418 |  0:00:02s
epoch 15 | loss: 0.44601 | val_0_rmse: 1.50409 | val_1_rmse: 1.45864 |  0:00:02s
epoch 16 | loss: 0.39347 | val_0_rmse: 2.19399 | val_1_rmse: 2.17814 |  0:00:02s
epoch 17 | loss: 0.47605 | val_0_rmse: 3.82818 | val_1_rmse: 3.71225 |  0:00:02s
epoch 18 | loss: 0.42104 | val_0_rmse: 4.74312 | val_1_rmse: 4.66863 |  0:00:02s
epoch 19 | loss: 0.45099 | val_0_rmse: 4.78231 | val_1_rmse: 4.69291 |  0:00:02s
epoch 20 | loss: 0.35918 | val_0_rmse: 4.62384 | val_1_rmse: 4.49928 |  0:00:02s
epoch 21 | loss: 0.41941 | val_0_rmse: 4.22845 | val_1_rmse: 4.05112 |  0:00:02s
epoch 22 | loss: 0.36204 | val_0_rmse: 3.9128  | val_1_rmse: 3.72041 |  0:00:03s
epoch 23 | loss: 0.38111 | val_0_rmse: 3.56158 | val_1_rmse: 3.39175 |  0:00:03s
epoch 24 | loss: 0.36719 | val_0_rmse: 2.28401 | val_1_rmse: 2.143   |  0:00:03s
epoch 25 | loss: 0.38977 | val_0_rmse: 1.91874 | val_1_rmse: 1.79269 |  0:00:03s
epoch 26 | loss: 0.40075 | val_0_rmse: 1.35248 | val_1_rmse: 1.26998 |  0:00:03s
epoch 27 | loss: 0.29922 | val_0_rmse: 0.73465 | val_1_rmse: 0.70906 |  0:00:03s
epoch 28 | loss: 0.33669 | val_0_rmse: 0.82202 | val_1_rmse: 0.7861  |  0:00:03s
epoch 29 | loss: 0.32461 | val_0_rmse: 0.95128 | val_1_rmse: 0.92474 |  0:00:04s
epoch 30 | loss: 0.31876 | val_0_rmse: 1.15052 | val_1_rmse: 1.12139 |  0:00:04s
epoch 31 | loss: 0.35306 | val_0_rmse: 1.40116 | val_1_rmse: 1.35705 |  0:00:04s
epoch 32 | loss: 0.39784 | val_0_rmse: 1.15264 | val_1_rmse: 1.10731 |  0:00:04s
epoch 33 | loss: 0.30921 | val_0_rmse: 1.22482 | val_1_rmse: 1.16183 |  0:00:04s
epoch 34 | loss: 0.27299 | val_0_rmse: 1.36768 | val_1_rmse: 1.29312 |  0:00:04s
epoch 35 | loss: 0.27053 | val_0_rmse: 1.07503 | val_1_rmse: 1.00563 |  0:00:04s
epoch 36 | loss: 0.28257 | val_0_rmse: 1.18314 | val_1_rmse: 1.10904 |  0:00:05s
epoch 37 | loss: 0.26099 | val_0_rmse: 1.45784 | val_1_rmse: 1.38654 |  0:00:05s
epoch 38 | loss: 0.28299 | val_0_rmse: 1.05688 | val_1_rmse: 1.00785 |  0:00:05s
epoch 39 | loss: 0.27293 | val_0_rmse: 1.01585 | val_1_rmse: 0.97389 |  0:00:05s
epoch 40 | loss: 0.26142 | val_0_rmse: 1.17631 | val_1_rmse: 1.12939 |  0:00:05s
epoch 41 | loss: 0.29171 | val_0_rmse: 0.97996 | val_1_rmse: 0.94435 |  0:00:05s
epoch 42 | loss: 0.22559 | val_0_rmse: 0.94264 | val_1_rmse: 0.90966 |  0:00:05s
epoch 43 | loss: 0.21389 | val_0_rmse: 0.86277 | val_1_rmse: 0.83311 |  0:00:05s
epoch 44 | loss: 0.20977 | val_0_rmse: 0.75241 | val_1_rmse: 0.73841 |  0:00:06s
epoch 45 | loss: 0.21829 | val_0_rmse: 0.90027 | val_1_rmse: 0.87416 |  0:00:06s
epoch 46 | loss: 0.2375  | val_0_rmse: 0.80856 | val_1_rmse: 0.7868  |  0:00:06s
epoch 47 | loss: 0.21253 | val_0_rmse: 0.74296 | val_1_rmse: 0.72783 |  0:00:06s
epoch 48 | loss: 0.23294 | val_0_rmse: 0.78463 | val_1_rmse: 0.75511 |  0:00:06s
epoch 49 | loss: 0.21035 | val_0_rmse: 0.82223 | val_1_rmse: 0.79728 |  0:00:06s
epoch 50 | loss: 0.21067 | val_0_rmse: 0.78346 | val_1_rmse: 0.75291 |  0:00:06s
epoch 51 | loss: 0.18617 | val_0_rmse: 0.7438  | val_1_rmse: 0.71115 |  0:00:07s
epoch 52 | loss: 0.19088 | val_0_rmse: 0.98601 | val_1_rmse: 0.9464  |  0:00:07s
epoch 53 | loss: 0.21372 | val_0_rmse: 0.84121 | val_1_rmse: 0.8073  |  0:00:07s
epoch 54 | loss: 0.1983  | val_0_rmse: 0.89792 | val_1_rmse: 0.86111 |  0:00:07s
epoch 55 | loss: 0.23329 | val_0_rmse: 0.87225 | val_1_rmse: 0.84595 |  0:00:07s
epoch 56 | loss: 0.18602 | val_0_rmse: 1.03114 | val_1_rmse: 1.0134  |  0:00:07s
epoch 57 | loss: 0.19982 | val_0_rmse: 1.01722 | val_1_rmse: 1.00354 |  0:00:07s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.70906
Best weights from best epoch are automatically used!
ended training at: 18:28:59
Feature importance:
[('Area', 0.12980447802298833), ('Baths', 0.244655279259935), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.04740908297925189), ('Month', 0.5713329953032131), ('Year', 0.006798164434611659)]
Mean squared error is of 19790147698.005074
Mean absolute error:93670.79103324175
MAPE:0.7869153974951661
R2 score:-1.5522914988050283
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:28:59
epoch 0  | loss: 142.60723| val_0_rmse: 4.9825  | val_1_rmse: 5.06645 |  0:00:00s
epoch 1  | loss: 106.60368| val_0_rmse: 6.72941 | val_1_rmse: 6.66181 |  0:00:00s
epoch 2  | loss: 76.60801| val_0_rmse: 27.2998 | val_1_rmse: 27.7803 |  0:00:00s
epoch 3  | loss: 44.35908| val_0_rmse: 15.70938| val_1_rmse: 15.73485|  0:00:00s
epoch 4  | loss: 16.43124| val_0_rmse: 52.51625| val_1_rmse: 52.52483|  0:00:00s
epoch 5  | loss: 5.14931 | val_0_rmse: 57.57337| val_1_rmse: 57.45848|  0:00:00s
epoch 6  | loss: 8.22312 | val_0_rmse: 53.96638| val_1_rmse: 53.8611 |  0:00:00s
epoch 7  | loss: 9.59589 | val_0_rmse: 45.88379| val_1_rmse: 45.43802|  0:00:01s
epoch 8  | loss: 5.38812 | val_0_rmse: 35.41062| val_1_rmse: 35.22774|  0:00:01s
epoch 9  | loss: 1.7839  | val_0_rmse: 17.75661| val_1_rmse: 18.14571|  0:00:01s
epoch 10 | loss: 1.78869 | val_0_rmse: 19.3746 | val_1_rmse: 18.73183|  0:00:01s
epoch 11 | loss: 1.57447 | val_0_rmse: 14.81421| val_1_rmse: 14.0079 |  0:00:01s
epoch 12 | loss: 1.00382 | val_0_rmse: 14.16037| val_1_rmse: 14.17978|  0:00:01s
epoch 13 | loss: 1.06774 | val_0_rmse: 13.43326| val_1_rmse: 13.64242|  0:00:01s
epoch 14 | loss: 0.58964 | val_0_rmse: 14.52918| val_1_rmse: 14.55576|  0:00:02s
epoch 15 | loss: 0.74106 | val_0_rmse: 14.07479| val_1_rmse: 14.03365|  0:00:02s
epoch 16 | loss: 0.46502 | val_0_rmse: 13.58063| val_1_rmse: 13.52978|  0:00:02s
epoch 17 | loss: 0.58309 | val_0_rmse: 14.48327| val_1_rmse: 14.45017|  0:00:02s
epoch 18 | loss: 0.44208 | val_0_rmse: 13.95086| val_1_rmse: 13.93441|  0:00:02s
epoch 19 | loss: 0.54309 | val_0_rmse: 13.127  | val_1_rmse: 13.12127|  0:00:02s
epoch 20 | loss: 0.39796 | val_0_rmse: 13.12887| val_1_rmse: 13.13728|  0:00:02s
epoch 21 | loss: 0.43997 | val_0_rmse: 10.70032| val_1_rmse: 10.77474|  0:00:02s
epoch 22 | loss: 0.49936 | val_0_rmse: 8.95853 | val_1_rmse: 9.02819 |  0:00:03s
epoch 23 | loss: 0.37128 | val_0_rmse: 5.8221  | val_1_rmse: 5.89781 |  0:00:03s
epoch 24 | loss: 0.41047 | val_0_rmse: 3.60205 | val_1_rmse: 3.59648 |  0:00:03s
epoch 25 | loss: 0.39434 | val_0_rmse: 2.66055 | val_1_rmse: 2.6435  |  0:00:03s
epoch 26 | loss: 0.39104 | val_0_rmse: 2.37277 | val_1_rmse: 2.48655 |  0:00:03s
epoch 27 | loss: 0.31709 | val_0_rmse: 1.67388 | val_1_rmse: 1.76794 |  0:00:03s
epoch 28 | loss: 0.30582 | val_0_rmse: 1.75584 | val_1_rmse: 1.77549 |  0:00:03s
epoch 29 | loss: 0.36597 | val_0_rmse: 1.69466 | val_1_rmse: 1.69295 |  0:00:04s
epoch 30 | loss: 0.37719 | val_0_rmse: 1.24079 | val_1_rmse: 1.24637 |  0:00:04s
epoch 31 | loss: 0.28293 | val_0_rmse: 1.25624 | val_1_rmse: 1.25136 |  0:00:04s
epoch 32 | loss: 0.24181 | val_0_rmse: 1.31444 | val_1_rmse: 1.31653 |  0:00:04s
epoch 33 | loss: 0.24873 | val_0_rmse: 1.71711 | val_1_rmse: 1.71233 |  0:00:04s
epoch 34 | loss: 0.25063 | val_0_rmse: 1.60815 | val_1_rmse: 1.56506 |  0:00:04s
epoch 35 | loss: 0.25217 | val_0_rmse: 1.67432 | val_1_rmse: 1.66911 |  0:00:04s
epoch 36 | loss: 0.2257  | val_0_rmse: 1.40926 | val_1_rmse: 1.42236 |  0:00:05s
epoch 37 | loss: 0.2191  | val_0_rmse: 1.41166 | val_1_rmse: 1.43077 |  0:00:05s
epoch 38 | loss: 0.20767 | val_0_rmse: 1.08192 | val_1_rmse: 1.0867  |  0:00:05s
epoch 39 | loss: 0.21796 | val_0_rmse: 1.12699 | val_1_rmse: 1.12577 |  0:00:05s
epoch 40 | loss: 0.27176 | val_0_rmse: 0.96862 | val_1_rmse: 0.96893 |  0:00:05s
epoch 41 | loss: 0.23601 | val_0_rmse: 0.81369 | val_1_rmse: 0.81003 |  0:00:05s
epoch 42 | loss: 0.28161 | val_0_rmse: 0.88151 | val_1_rmse: 0.87845 |  0:00:05s
epoch 43 | loss: 0.21389 | val_0_rmse: 0.80978 | val_1_rmse: 0.77582 |  0:00:06s
epoch 44 | loss: 0.21244 | val_0_rmse: 0.7206  | val_1_rmse: 0.69931 |  0:00:06s
epoch 45 | loss: 0.24386 | val_0_rmse: 0.81298 | val_1_rmse: 0.77088 |  0:00:06s
epoch 46 | loss: 0.26065 | val_0_rmse: 0.85997 | val_1_rmse: 0.82831 |  0:00:06s
epoch 47 | loss: 0.23699 | val_0_rmse: 0.65301 | val_1_rmse: 0.67949 |  0:00:06s
epoch 48 | loss: 0.27098 | val_0_rmse: 0.59777 | val_1_rmse: 0.61128 |  0:00:06s
epoch 49 | loss: 0.20232 | val_0_rmse: 0.58387 | val_1_rmse: 0.60783 |  0:00:06s
epoch 50 | loss: 0.20051 | val_0_rmse: 0.57478 | val_1_rmse: 0.58143 |  0:00:06s
epoch 51 | loss: 0.19336 | val_0_rmse: 0.58945 | val_1_rmse: 0.59481 |  0:00:07s
epoch 52 | loss: 0.19946 | val_0_rmse: 0.65891 | val_1_rmse: 0.6912  |  0:00:07s
epoch 53 | loss: 0.19577 | val_0_rmse: 0.69049 | val_1_rmse: 0.71956 |  0:00:07s
epoch 54 | loss: 0.19894 | val_0_rmse: 0.6913  | val_1_rmse: 0.71666 |  0:00:07s
epoch 55 | loss: 0.20366 | val_0_rmse: 0.64999 | val_1_rmse: 0.64897 |  0:00:07s
epoch 56 | loss: 0.21399 | val_0_rmse: 0.73031 | val_1_rmse: 0.74475 |  0:00:07s
epoch 57 | loss: 0.21275 | val_0_rmse: 0.67252 | val_1_rmse: 0.66404 |  0:00:07s
epoch 58 | loss: 0.22587 | val_0_rmse: 0.69471 | val_1_rmse: 0.69704 |  0:00:08s
epoch 59 | loss: 0.20052 | val_0_rmse: 0.74397 | val_1_rmse: 0.7594  |  0:00:08s
epoch 60 | loss: 0.21424 | val_0_rmse: 0.6893  | val_1_rmse: 0.68866 |  0:00:08s
epoch 61 | loss: 0.1997  | val_0_rmse: 0.72392 | val_1_rmse: 0.73612 |  0:00:08s
epoch 62 | loss: 0.20077 | val_0_rmse: 0.72317 | val_1_rmse: 0.73586 |  0:00:08s
epoch 63 | loss: 0.19602 | val_0_rmse: 0.77726 | val_1_rmse: 0.79475 |  0:00:08s
epoch 64 | loss: 0.20635 | val_0_rmse: 0.71019 | val_1_rmse: 0.70434 |  0:00:08s
epoch 65 | loss: 0.20381 | val_0_rmse: 0.75503 | val_1_rmse: 0.76403 |  0:00:09s
epoch 66 | loss: 0.18689 | val_0_rmse: 0.73112 | val_1_rmse: 0.73642 |  0:00:09s
epoch 67 | loss: 0.1861  | val_0_rmse: 0.78313 | val_1_rmse: 0.81018 |  0:00:09s
epoch 68 | loss: 0.1975  | val_0_rmse: 0.7341  | val_1_rmse: 0.75814 |  0:00:09s
epoch 69 | loss: 0.19408 | val_0_rmse: 0.76678 | val_1_rmse: 0.80721 |  0:00:09s
epoch 70 | loss: 0.2004  | val_0_rmse: 0.71726 | val_1_rmse: 0.7355  |  0:00:09s
epoch 71 | loss: 0.19803 | val_0_rmse: 0.73922 | val_1_rmse: 0.78133 |  0:00:09s
epoch 72 | loss: 0.20057 | val_0_rmse: 0.71171 | val_1_rmse: 0.7283  |  0:00:10s
epoch 73 | loss: 0.19357 | val_0_rmse: 0.71734 | val_1_rmse: 0.74132 |  0:00:10s
epoch 74 | loss: 0.20652 | val_0_rmse: 0.70035 | val_1_rmse: 0.6751  |  0:00:10s
epoch 75 | loss: 0.22658 | val_0_rmse: 0.6751  | val_1_rmse: 0.67905 |  0:00:10s
epoch 76 | loss: 0.2034  | val_0_rmse: 0.70545 | val_1_rmse: 0.73155 |  0:00:10s
epoch 77 | loss: 0.20777 | val_0_rmse: 0.69489 | val_1_rmse: 0.68907 |  0:00:10s
epoch 78 | loss: 0.1989  | val_0_rmse: 0.72964 | val_1_rmse: 0.76129 |  0:00:10s
epoch 79 | loss: 0.22365 | val_0_rmse: 0.69705 | val_1_rmse: 0.71475 |  0:00:10s
epoch 80 | loss: 0.19536 | val_0_rmse: 0.7352  | val_1_rmse: 0.76642 |  0:00:11s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.58143
Best weights from best epoch are automatically used!
ended training at: 18:29:10
Feature importance:
[('Area', 0.1457795369264363), ('Baths', 0.18201365365792643), ('Beds', 0.003966939212786302), ('Latitude', 0.0013007566576271037), ('Longitude', 0.32426847825380484), ('Month', 0.3338334494867852), ('Year', 0.008837185804633755)]
Mean squared error is of 8266016482.920986
Mean absolute error:64780.23077918956
MAPE:0.5176875681229159
R2 score:-0.0948151510436741
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:29:10
epoch 0  | loss: 142.93156| val_0_rmse: 7.87563 | val_1_rmse: 8.01557 |  0:00:00s
epoch 1  | loss: 105.52277| val_0_rmse: 7.94972 | val_1_rmse: 8.17954 |  0:00:00s
epoch 2  | loss: 73.48918| val_0_rmse: 19.84092| val_1_rmse: 20.05077|  0:00:00s
epoch 3  | loss: 40.56618| val_0_rmse: 26.4092 | val_1_rmse: 26.59442|  0:00:00s
epoch 4  | loss: 13.80118| val_0_rmse: 33.71815| val_1_rmse: 34.20212|  0:00:00s
epoch 5  | loss: 2.97804 | val_0_rmse: 30.69599| val_1_rmse: 31.34459|  0:00:00s
epoch 6  | loss: 8.20625 | val_0_rmse: 25.49585| val_1_rmse: 26.1033 |  0:00:00s
epoch 7  | loss: 9.63499 | val_0_rmse: 15.33383| val_1_rmse: 15.79872|  0:00:01s
epoch 8  | loss: 4.73805 | val_0_rmse: 7.38362 | val_1_rmse: 7.69778 |  0:00:01s
epoch 9  | loss: 2.06844 | val_0_rmse: 4.08869 | val_1_rmse: 4.22961 |  0:00:01s
epoch 10 | loss: 2.4392  | val_0_rmse: 2.68584 | val_1_rmse: 2.82449 |  0:00:01s
epoch 11 | loss: 1.52109 | val_0_rmse: 3.37703 | val_1_rmse: 3.56329 |  0:00:01s
epoch 12 | loss: 1.21664 | val_0_rmse: 7.95533 | val_1_rmse: 8.01046 |  0:00:01s
epoch 13 | loss: 1.07665 | val_0_rmse: 8.98818 | val_1_rmse: 9.07952 |  0:00:01s
epoch 14 | loss: 0.74967 | val_0_rmse: 9.49839 | val_1_rmse: 9.54959 |  0:00:02s
epoch 15 | loss: 0.6396  | val_0_rmse: 10.32259| val_1_rmse: 10.36341|  0:00:02s
epoch 16 | loss: 0.55444 | val_0_rmse: 10.58028| val_1_rmse: 10.61997|  0:00:02s
epoch 17 | loss: 0.46317 | val_0_rmse: 10.30285| val_1_rmse: 10.33618|  0:00:02s
epoch 18 | loss: 0.45899 | val_0_rmse: 10.57775| val_1_rmse: 10.61583|  0:00:02s
epoch 19 | loss: 0.46732 | val_0_rmse: 9.43447 | val_1_rmse: 9.47788 |  0:00:02s
epoch 20 | loss: 0.46951 | val_0_rmse: 4.97159 | val_1_rmse: 5.03262 |  0:00:02s
epoch 21 | loss: 0.43125 | val_0_rmse: 4.7967  | val_1_rmse: 4.85032 |  0:00:02s
epoch 22 | loss: 0.45988 | val_0_rmse: 5.39036 | val_1_rmse: 5.45217 |  0:00:03s
epoch 23 | loss: 0.41533 | val_0_rmse: 5.2305  | val_1_rmse: 5.30822 |  0:00:03s
epoch 24 | loss: 0.48951 | val_0_rmse: 5.6224  | val_1_rmse: 5.71512 |  0:00:03s
epoch 25 | loss: 0.43635 | val_0_rmse: 6.26877 | val_1_rmse: 6.39435 |  0:00:03s
epoch 26 | loss: 0.49623 | val_0_rmse: 5.66503 | val_1_rmse: 5.78168 |  0:00:03s
epoch 27 | loss: 0.43865 | val_0_rmse: 4.14206 | val_1_rmse: 4.23786 |  0:00:03s
epoch 28 | loss: 0.44141 | val_0_rmse: 3.40769 | val_1_rmse: 3.49557 |  0:00:03s
epoch 29 | loss: 0.42949 | val_0_rmse: 3.63834 | val_1_rmse: 3.73166 |  0:00:04s
epoch 30 | loss: 0.39002 | val_0_rmse: 3.35618 | val_1_rmse: 3.44873 |  0:00:04s
epoch 31 | loss: 0.38704 | val_0_rmse: 2.51117 | val_1_rmse: 2.59932 |  0:00:04s
epoch 32 | loss: 0.37297 | val_0_rmse: 2.36061 | val_1_rmse: 2.45299 |  0:00:04s
epoch 33 | loss: 0.38688 | val_0_rmse: 2.88699 | val_1_rmse: 2.99125 |  0:00:04s
epoch 34 | loss: 0.36803 | val_0_rmse: 2.80004 | val_1_rmse: 2.89954 |  0:00:04s
epoch 35 | loss: 0.38259 | val_0_rmse: 2.0987  | val_1_rmse: 2.18442 |  0:00:04s
epoch 36 | loss: 0.35517 | val_0_rmse: 2.03976 | val_1_rmse: 2.13288 |  0:00:05s
epoch 37 | loss: 0.34121 | val_0_rmse: 2.38923 | val_1_rmse: 2.48071 |  0:00:05s
epoch 38 | loss: 0.31553 | val_0_rmse: 2.11267 | val_1_rmse: 2.19289 |  0:00:05s
epoch 39 | loss: 0.28824 | val_0_rmse: 1.66968 | val_1_rmse: 1.74298 |  0:00:05s
epoch 40 | loss: 0.334   | val_0_rmse: 1.97758 | val_1_rmse: 2.05269 |  0:00:05s
epoch 41 | loss: 0.24971 | val_0_rmse: 1.83438 | val_1_rmse: 1.89274 |  0:00:05s
epoch 42 | loss: 0.25402 | val_0_rmse: 1.42832 | val_1_rmse: 1.46228 |  0:00:05s
epoch 43 | loss: 0.26371 | val_0_rmse: 1.70154 | val_1_rmse: 1.74907 |  0:00:06s
epoch 44 | loss: 0.38587 | val_0_rmse: 1.63888 | val_1_rmse: 1.70499 |  0:00:06s
epoch 45 | loss: 0.35557 | val_0_rmse: 1.02049 | val_1_rmse: 1.07629 |  0:00:06s
epoch 46 | loss: 0.39497 | val_0_rmse: 0.85064 | val_1_rmse: 0.89393 |  0:00:06s
epoch 47 | loss: 0.45671 | val_0_rmse: 1.14342 | val_1_rmse: 1.20361 |  0:00:06s
epoch 48 | loss: 0.27071 | val_0_rmse: 1.32129 | val_1_rmse: 1.38369 |  0:00:06s
epoch 49 | loss: 0.27562 | val_0_rmse: 0.95652 | val_1_rmse: 1.01043 |  0:00:06s
epoch 50 | loss: 0.29296 | val_0_rmse: 0.90478 | val_1_rmse: 0.94761 |  0:00:07s
epoch 51 | loss: 0.25695 | val_0_rmse: 1.11827 | val_1_rmse: 1.1944  |  0:00:07s
epoch 52 | loss: 0.31239 | val_0_rmse: 0.86387 | val_1_rmse: 0.91349 |  0:00:07s
epoch 53 | loss: 0.23711 | val_0_rmse: 0.85879 | val_1_rmse: 0.9047  |  0:00:07s
epoch 54 | loss: 0.23436 | val_0_rmse: 0.76424 | val_1_rmse: 0.80116 |  0:00:07s
epoch 55 | loss: 0.23347 | val_0_rmse: 0.6945  | val_1_rmse: 0.70363 |  0:00:07s
epoch 56 | loss: 0.24123 | val_0_rmse: 0.64836 | val_1_rmse: 0.61537 |  0:00:07s
epoch 57 | loss: 0.24884 | val_0_rmse: 0.68209 | val_1_rmse: 0.64325 |  0:00:07s
epoch 58 | loss: 0.23244 | val_0_rmse: 0.7721  | val_1_rmse: 0.71922 |  0:00:08s
epoch 59 | loss: 0.22961 | val_0_rmse: 0.76544 | val_1_rmse: 0.71821 |  0:00:08s
epoch 60 | loss: 0.23012 | val_0_rmse: 0.77646 | val_1_rmse: 0.72867 |  0:00:08s
epoch 61 | loss: 0.22761 | val_0_rmse: 0.77041 | val_1_rmse: 0.72717 |  0:00:08s
epoch 62 | loss: 0.22499 | val_0_rmse: 0.73187 | val_1_rmse: 0.69709 |  0:00:08s
epoch 63 | loss: 0.22474 | val_0_rmse: 0.70401 | val_1_rmse: 0.67435 |  0:00:08s
epoch 64 | loss: 0.2184  | val_0_rmse: 0.73448 | val_1_rmse: 0.71324 |  0:00:08s
epoch 65 | loss: 0.21484 | val_0_rmse: 0.69792 | val_1_rmse: 0.69506 |  0:00:09s
epoch 66 | loss: 0.23081 | val_0_rmse: 0.75927 | val_1_rmse: 0.74396 |  0:00:09s
epoch 67 | loss: 0.2293  | val_0_rmse: 0.73785 | val_1_rmse: 0.7283  |  0:00:09s
epoch 68 | loss: 0.22495 | val_0_rmse: 0.82534 | val_1_rmse: 0.80376 |  0:00:09s
epoch 69 | loss: 0.21637 | val_0_rmse: 0.73121 | val_1_rmse: 0.72007 |  0:00:09s
epoch 70 | loss: 0.2519  | val_0_rmse: 0.79133 | val_1_rmse: 0.77134 |  0:00:09s
epoch 71 | loss: 0.21458 | val_0_rmse: 0.94752 | val_1_rmse: 0.91683 |  0:00:09s
epoch 72 | loss: 0.24419 | val_0_rmse: 0.76806 | val_1_rmse: 0.75497 |  0:00:09s
epoch 73 | loss: 0.24324 | val_0_rmse: 0.79689 | val_1_rmse: 0.77684 |  0:00:10s
epoch 74 | loss: 0.22211 | val_0_rmse: 1.03674 | val_1_rmse: 0.99314 |  0:00:10s
epoch 75 | loss: 0.29518 | val_0_rmse: 0.83097 | val_1_rmse: 0.79669 |  0:00:10s
epoch 76 | loss: 0.21108 | val_0_rmse: 0.92413 | val_1_rmse: 0.88716 |  0:00:10s
epoch 77 | loss: 0.21462 | val_0_rmse: 0.92724 | val_1_rmse: 0.89209 |  0:00:10s
epoch 78 | loss: 0.20736 | val_0_rmse: 0.93869 | val_1_rmse: 0.90492 |  0:00:10s
epoch 79 | loss: 0.20706 | val_0_rmse: 1.08723 | val_1_rmse: 1.05841 |  0:00:10s
epoch 80 | loss: 0.21631 | val_0_rmse: 0.8917  | val_1_rmse: 0.87241 |  0:00:11s
epoch 81 | loss: 0.21988 | val_0_rmse: 1.03453 | val_1_rmse: 1.00929 |  0:00:11s
epoch 82 | loss: 0.21476 | val_0_rmse: 0.86839 | val_1_rmse: 0.84482 |  0:00:11s
epoch 83 | loss: 0.23024 | val_0_rmse: 1.03458 | val_1_rmse: 1.00783 |  0:00:11s
epoch 84 | loss: 0.21401 | val_0_rmse: 0.81973 | val_1_rmse: 0.79492 |  0:00:11s
epoch 85 | loss: 0.20796 | val_0_rmse: 0.96836 | val_1_rmse: 0.94062 |  0:00:11s
epoch 86 | loss: 0.21501 | val_0_rmse: 0.93589 | val_1_rmse: 0.90803 |  0:00:11s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.61537
Best weights from best epoch are automatically used!
ended training at: 18:29:22
Feature importance:
[('Area', 0.003264528368989482), ('Baths', 0.005607218002265917), ('Beds', 0.3803193745539908), ('Latitude', 0.017681976847570075), ('Longitude', 0.27310405116334635), ('Month', 0.002630072643629521), ('Year', 0.31739277842020786)]
Mean squared error is of 8214901849.124749
Mean absolute error:64492.46122843406
MAPE:0.5036166576657364
R2 score:-0.21502543919344386
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:29:22
epoch 0  | loss: 126.69158| val_0_rmse: 5.19495 | val_1_rmse: 5.3375  |  0:00:00s
epoch 1  | loss: 59.48382| val_0_rmse: 7.68656 | val_1_rmse: 7.28589 |  0:00:00s
epoch 2  | loss: 8.65477 | val_0_rmse: 16.26415| val_1_rmse: 15.71997|  0:00:00s
epoch 3  | loss: 7.54903 | val_0_rmse: 8.75677 | val_1_rmse: 8.45692 |  0:00:01s
epoch 4  | loss: 2.02341 | val_0_rmse: 6.30721 | val_1_rmse: 6.64488 |  0:00:01s
epoch 5  | loss: 1.17952 | val_0_rmse: 22.2932 | val_1_rmse: 22.56499|  0:00:01s
epoch 6  | loss: 0.66212 | val_0_rmse: 22.25008| val_1_rmse: 21.8468 |  0:00:02s
epoch 7  | loss: 0.55093 | val_0_rmse: 14.70168| val_1_rmse: 14.44699|  0:00:02s
epoch 8  | loss: 0.45415 | val_0_rmse: 2.72124 | val_1_rmse: 2.77134 |  0:00:02s
epoch 9  | loss: 0.39118 | val_0_rmse: 1.40505 | val_1_rmse: 1.40784 |  0:00:03s
epoch 10 | loss: 0.34904 | val_0_rmse: 1.17122 | val_1_rmse: 1.11673 |  0:00:03s
epoch 11 | loss: 0.23726 | val_0_rmse: 1.73804 | val_1_rmse: 1.74886 |  0:00:03s
epoch 12 | loss: 0.26502 | val_0_rmse: 1.5906  | val_1_rmse: 1.57915 |  0:00:03s
epoch 13 | loss: 0.24691 | val_0_rmse: 1.81492 | val_1_rmse: 1.86909 |  0:00:04s
epoch 14 | loss: 0.25959 | val_0_rmse: 3.75871 | val_1_rmse: 3.84223 |  0:00:04s
epoch 15 | loss: 0.26731 | val_0_rmse: 3.46117 | val_1_rmse: 3.49015 |  0:00:04s
epoch 16 | loss: 0.24163 | val_0_rmse: 2.72912 | val_1_rmse: 2.74263 |  0:00:05s
epoch 17 | loss: 0.24596 | val_0_rmse: 1.59286 | val_1_rmse: 1.59322 |  0:00:05s
epoch 18 | loss: 0.24574 | val_0_rmse: 1.75491 | val_1_rmse: 1.75261 |  0:00:05s
epoch 19 | loss: 0.21241 | val_0_rmse: 1.37979 | val_1_rmse: 1.3801  |  0:00:06s
epoch 20 | loss: 0.24177 | val_0_rmse: 0.9647  | val_1_rmse: 0.9691  |  0:00:06s
epoch 21 | loss: 0.23968 | val_0_rmse: 1.22993 | val_1_rmse: 1.2376  |  0:00:06s
epoch 22 | loss: 0.21037 | val_0_rmse: 1.06938 | val_1_rmse: 1.08479 |  0:00:06s
epoch 23 | loss: 0.22938 | val_0_rmse: 0.71751 | val_1_rmse: 0.71012 |  0:00:07s
epoch 24 | loss: 0.21884 | val_0_rmse: 0.7997  | val_1_rmse: 0.79944 |  0:00:07s
epoch 25 | loss: 0.20339 | val_0_rmse: 0.82968 | val_1_rmse: 0.83909 |  0:00:07s
epoch 26 | loss: 0.21584 | val_0_rmse: 0.65335 | val_1_rmse: 0.64767 |  0:00:08s
epoch 27 | loss: 0.21062 | val_0_rmse: 0.78059 | val_1_rmse: 0.78813 |  0:00:08s
epoch 28 | loss: 0.20006 | val_0_rmse: 0.88759 | val_1_rmse: 0.8942  |  0:00:08s
epoch 29 | loss: 0.21035 | val_0_rmse: 0.96344 | val_1_rmse: 0.94093 |  0:00:09s
epoch 30 | loss: 0.21338 | val_0_rmse: 1.04913 | val_1_rmse: 1.02675 |  0:00:09s
epoch 31 | loss: 0.21563 | val_0_rmse: 1.18945 | val_1_rmse: 1.1514  |  0:00:09s
epoch 32 | loss: 0.20228 | val_0_rmse: 1.38028 | val_1_rmse: 1.34762 |  0:00:09s
epoch 33 | loss: 0.2133  | val_0_rmse: 1.34065 | val_1_rmse: 1.3167  |  0:00:10s
epoch 34 | loss: 0.20671 | val_0_rmse: 1.2589  | val_1_rmse: 1.23497 |  0:00:10s
epoch 35 | loss: 0.23135 | val_0_rmse: 1.35599 | val_1_rmse: 1.32971 |  0:00:10s
epoch 36 | loss: 0.20791 | val_0_rmse: 1.13386 | val_1_rmse: 1.11424 |  0:00:11s
epoch 37 | loss: 0.204   | val_0_rmse: 1.0911  | val_1_rmse: 1.05073 |  0:00:11s
epoch 38 | loss: 0.21156 | val_0_rmse: 1.3445  | val_1_rmse: 1.29491 |  0:00:11s
epoch 39 | loss: 0.21474 | val_0_rmse: 1.09093 | val_1_rmse: 1.04274 |  0:00:11s
epoch 40 | loss: 0.20244 | val_0_rmse: 1.11581 | val_1_rmse: 1.06587 |  0:00:12s
epoch 41 | loss: 0.20319 | val_0_rmse: 1.28003 | val_1_rmse: 1.22896 |  0:00:12s
epoch 42 | loss: 0.20606 | val_0_rmse: 1.18183 | val_1_rmse: 1.13209 |  0:00:12s
epoch 43 | loss: 0.1976  | val_0_rmse: 1.14563 | val_1_rmse: 1.09965 |  0:00:13s
epoch 44 | loss: 0.19533 | val_0_rmse: 1.40886 | val_1_rmse: 1.36639 |  0:00:13s
epoch 45 | loss: 0.20389 | val_0_rmse: 1.18655 | val_1_rmse: 1.14814 |  0:00:13s
epoch 46 | loss: 0.19768 | val_0_rmse: 1.12908 | val_1_rmse: 1.09291 |  0:00:14s
epoch 47 | loss: 0.19907 | val_0_rmse: 1.36852 | val_1_rmse: 1.32752 |  0:00:14s
epoch 48 | loss: 0.20327 | val_0_rmse: 1.20065 | val_1_rmse: 1.15824 |  0:00:14s
epoch 49 | loss: 0.19589 | val_0_rmse: 1.23123 | val_1_rmse: 1.18806 |  0:00:15s
epoch 50 | loss: 0.18852 | val_0_rmse: 1.36977 | val_1_rmse: 1.32695 |  0:00:15s
epoch 51 | loss: 0.19929 | val_0_rmse: 1.13553 | val_1_rmse: 1.09903 |  0:00:15s
epoch 52 | loss: 0.19806 | val_0_rmse: 1.09753 | val_1_rmse: 1.06649 |  0:00:15s
epoch 53 | loss: 0.19311 | val_0_rmse: 1.40598 | val_1_rmse: 1.37047 |  0:00:16s
epoch 54 | loss: 0.19889 | val_0_rmse: 1.17987 | val_1_rmse: 1.14582 |  0:00:16s
epoch 55 | loss: 0.19644 | val_0_rmse: 1.21781 | val_1_rmse: 1.18339 |  0:00:16s
epoch 56 | loss: 0.19835 | val_0_rmse: 1.44671 | val_1_rmse: 1.40926 |  0:00:17s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.64767
Best weights from best epoch are automatically used!
ended training at: 18:29:39
Feature importance:
[('Area', 0.037395244912827384), ('Baths', 0.3493395763431048), ('Beds', 0.0), ('Latitude', 0.0032199181047152517), ('Longitude', 0.1796805566356485), ('Month', 0.05279978905445482), ('Year', 0.37756491494924926)]
Mean squared error is of 12395658788.973902
Mean absolute error:86398.42763945776
MAPE:0.6944036326286277
R2 score:-0.5430130968303113
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:29:40
epoch 0  | loss: 125.77344| val_0_rmse: 11.61815| val_1_rmse: 11.53599|  0:00:00s
epoch 1  | loss: 55.23578| val_0_rmse: 45.79478| val_1_rmse: 46.19874|  0:00:00s
epoch 2  | loss: 7.95123 | val_0_rmse: 46.67157| val_1_rmse: 46.17983|  0:00:00s
epoch 3  | loss: 6.92539 | val_0_rmse: 19.88661| val_1_rmse: 19.66974|  0:00:01s
epoch 4  | loss: 1.45361 | val_0_rmse: 0.97519 | val_1_rmse: 0.97234 |  0:00:01s
epoch 5  | loss: 0.77618 | val_0_rmse: 1.37036 | val_1_rmse: 1.34961 |  0:00:01s
epoch 6  | loss: 0.50576 | val_0_rmse: 1.54329 | val_1_rmse: 1.52082 |  0:00:02s
epoch 7  | loss: 0.41581 | val_0_rmse: 2.45546 | val_1_rmse: 2.43507 |  0:00:02s
epoch 8  | loss: 0.34738 | val_0_rmse: 2.60319 | val_1_rmse: 2.61948 |  0:00:02s
epoch 9  | loss: 0.64282 | val_0_rmse: 1.87808 | val_1_rmse: 1.87709 |  0:00:03s
epoch 10 | loss: 0.64894 | val_0_rmse: 1.73294 | val_1_rmse: 1.7188  |  0:00:03s
epoch 11 | loss: 0.54684 | val_0_rmse: 0.9217  | val_1_rmse: 0.9015  |  0:00:03s
epoch 12 | loss: 0.50291 | val_0_rmse: 1.24262 | val_1_rmse: 1.22192 |  0:00:03s
epoch 13 | loss: 0.37349 | val_0_rmse: 0.72172 | val_1_rmse: 0.69473 |  0:00:04s
epoch 14 | loss: 0.29628 | val_0_rmse: 0.75374 | val_1_rmse: 0.72604 |  0:00:04s
epoch 15 | loss: 0.27357 | val_0_rmse: 0.49953 | val_1_rmse: 0.47954 |  0:00:04s
epoch 16 | loss: 0.32923 | val_0_rmse: 0.89107 | val_1_rmse: 0.86513 |  0:00:05s
epoch 17 | loss: 0.31825 | val_0_rmse: 0.5577  | val_1_rmse: 0.53568 |  0:00:05s
epoch 18 | loss: 0.26916 | val_0_rmse: 0.52068 | val_1_rmse: 0.50001 |  0:00:05s
epoch 19 | loss: 0.24321 | val_0_rmse: 0.57997 | val_1_rmse: 0.54741 |  0:00:06s
epoch 20 | loss: 0.23296 | val_0_rmse: 0.57623 | val_1_rmse: 0.54594 |  0:00:06s
epoch 21 | loss: 0.22179 | val_0_rmse: 0.4895  | val_1_rmse: 0.4712  |  0:00:06s
epoch 22 | loss: 0.22635 | val_0_rmse: 0.49627 | val_1_rmse: 0.48888 |  0:00:06s
epoch 23 | loss: 0.22805 | val_0_rmse: 0.57579 | val_1_rmse: 0.57375 |  0:00:07s
epoch 24 | loss: 0.21002 | val_0_rmse: 0.77037 | val_1_rmse: 0.75426 |  0:00:07s
epoch 25 | loss: 0.1989  | val_0_rmse: 0.74635 | val_1_rmse: 0.73837 |  0:00:07s
epoch 26 | loss: 0.20371 | val_0_rmse: 0.63927 | val_1_rmse: 0.62367 |  0:00:08s
epoch 27 | loss: 0.20491 | val_0_rmse: 0.67202 | val_1_rmse: 0.6609  |  0:00:08s
epoch 28 | loss: 0.20053 | val_0_rmse: 0.57877 | val_1_rmse: 0.56952 |  0:00:08s
epoch 29 | loss: 0.20643 | val_0_rmse: 0.61024 | val_1_rmse: 0.60465 |  0:00:09s
epoch 30 | loss: 0.20243 | val_0_rmse: 0.82241 | val_1_rmse: 0.82728 |  0:00:09s
epoch 31 | loss: 0.21659 | val_0_rmse: 0.81795 | val_1_rmse: 0.82434 |  0:00:09s
epoch 32 | loss: 0.23466 | val_0_rmse: 0.53874 | val_1_rmse: 0.53253 |  0:00:09s
epoch 33 | loss: 0.31132 | val_0_rmse: 0.87067 | val_1_rmse: 0.87727 |  0:00:10s
epoch 34 | loss: 0.25849 | val_0_rmse: 0.73812 | val_1_rmse: 0.74576 |  0:00:10s
epoch 35 | loss: 0.19521 | val_0_rmse: 0.88097 | val_1_rmse: 0.88724 |  0:00:10s
epoch 36 | loss: 0.22486 | val_0_rmse: 0.95501 | val_1_rmse: 0.97228 |  0:00:11s
epoch 37 | loss: 0.20076 | val_0_rmse: 0.93843 | val_1_rmse: 0.9531  |  0:00:11s
epoch 38 | loss: 0.19381 | val_0_rmse: 0.91605 | val_1_rmse: 0.92609 |  0:00:11s
epoch 39 | loss: 0.20648 | val_0_rmse: 0.87235 | val_1_rmse: 0.87588 |  0:00:12s
epoch 40 | loss: 0.28109 | val_0_rmse: 0.79703 | val_1_rmse: 0.79889 |  0:00:12s
epoch 41 | loss: 0.27979 | val_0_rmse: 1.13229 | val_1_rmse: 1.1461  |  0:00:12s
epoch 42 | loss: 0.27501 | val_0_rmse: 0.77588 | val_1_rmse: 0.78182 |  0:00:12s
epoch 43 | loss: 0.22566 | val_0_rmse: 1.01799 | val_1_rmse: 1.03079 |  0:00:13s
epoch 44 | loss: 0.20909 | val_0_rmse: 1.03254 | val_1_rmse: 1.04408 |  0:00:13s
epoch 45 | loss: 0.18529 | val_0_rmse: 0.99264 | val_1_rmse: 1.00255 |  0:00:13s
epoch 46 | loss: 0.21869 | val_0_rmse: 1.21495 | val_1_rmse: 1.22555 |  0:00:14s
epoch 47 | loss: 0.24774 | val_0_rmse: 0.97924 | val_1_rmse: 0.98639 |  0:00:14s
epoch 48 | loss: 0.20972 | val_0_rmse: 1.11255 | val_1_rmse: 1.11854 |  0:00:14s
epoch 49 | loss: 0.22883 | val_0_rmse: 1.06613 | val_1_rmse: 1.07297 |  0:00:15s
epoch 50 | loss: 0.18379 | val_0_rmse: 0.95348 | val_1_rmse: 0.95686 |  0:00:15s
epoch 51 | loss: 0.17811 | val_0_rmse: 0.91043 | val_1_rmse: 0.90823 |  0:00:15s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.4712
Best weights from best epoch are automatically used!
ended training at: 18:29:56
Feature importance:
[('Area', 1.7118484184749943e-05), ('Baths', 0.3845574943165277), ('Beds', 0.0), ('Latitude', 0.020577778741800466), ('Longitude', 0.32764424034378986), ('Month', 0.0006202115497282336), ('Year', 0.26658315656396897)]
Mean squared error is of 6501313112.3716345
Mean absolute error:55331.34411364601
MAPE:0.40112744300078657
R2 score:0.26932865878418866
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:29:56
epoch 0  | loss: 126.16195| val_0_rmse: 6.568   | val_1_rmse: 6.64913 |  0:00:00s
epoch 1  | loss: 57.44786| val_0_rmse: 21.09279| val_1_rmse: 21.26503|  0:00:00s
epoch 2  | loss: 9.07801 | val_0_rmse: 21.91696| val_1_rmse: 22.03723|  0:00:00s
epoch 3  | loss: 10.92287| val_0_rmse: 7.91826 | val_1_rmse: 7.91955 |  0:00:01s
epoch 4  | loss: 3.29371 | val_0_rmse: 5.83258 | val_1_rmse: 5.84219 |  0:00:01s
epoch 5  | loss: 1.25563 | val_0_rmse: 7.42428 | val_1_rmse: 7.43834 |  0:00:01s
epoch 6  | loss: 0.72607 | val_0_rmse: 5.72941 | val_1_rmse: 5.73694 |  0:00:02s
epoch 7  | loss: 0.49317 | val_0_rmse: 5.38014 | val_1_rmse: 5.38526 |  0:00:02s
epoch 8  | loss: 0.45923 | val_0_rmse: 6.09256 | val_1_rmse: 6.10041 |  0:00:02s
epoch 9  | loss: 0.388   | val_0_rmse: 3.43807 | val_1_rmse: 3.44104 |  0:00:03s
epoch 10 | loss: 0.32281 | val_0_rmse: 2.55693 | val_1_rmse: 2.56096 |  0:00:03s
epoch 11 | loss: 0.3253  | val_0_rmse: 1.66906 | val_1_rmse: 1.67548 |  0:00:03s
epoch 12 | loss: 0.27518 | val_0_rmse: 1.48919 | val_1_rmse: 1.48587 |  0:00:03s
epoch 13 | loss: 0.33167 | val_0_rmse: 1.71946 | val_1_rmse: 1.72194 |  0:00:04s
epoch 14 | loss: 0.31899 | val_0_rmse: 1.56509 | val_1_rmse: 1.56637 |  0:00:04s
epoch 15 | loss: 0.28755 | val_0_rmse: 1.11849 | val_1_rmse: 1.11766 |  0:00:04s
epoch 16 | loss: 0.28218 | val_0_rmse: 0.78073 | val_1_rmse: 0.78845 |  0:00:05s
epoch 17 | loss: 0.24261 | val_0_rmse: 0.95862 | val_1_rmse: 0.96484 |  0:00:05s
epoch 18 | loss: 0.20768 | val_0_rmse: 1.11235 | val_1_rmse: 1.11808 |  0:00:05s
epoch 19 | loss: 0.216   | val_0_rmse: 1.14264 | val_1_rmse: 1.14789 |  0:00:06s
epoch 20 | loss: 0.20657 | val_0_rmse: 1.71971 | val_1_rmse: 1.724   |  0:00:06s
epoch 21 | loss: 0.20469 | val_0_rmse: 1.532   | val_1_rmse: 1.52924 |  0:00:06s
epoch 22 | loss: 0.20035 | val_0_rmse: 1.2865  | val_1_rmse: 1.28325 |  0:00:07s
epoch 23 | loss: 0.20245 | val_0_rmse: 1.35405 | val_1_rmse: 1.35328 |  0:00:07s
epoch 24 | loss: 0.1917  | val_0_rmse: 1.07143 | val_1_rmse: 1.07905 |  0:00:07s
epoch 25 | loss: 0.21514 | val_0_rmse: 0.97083 | val_1_rmse: 0.97988 |  0:00:07s
epoch 26 | loss: 0.20326 | val_0_rmse: 1.13764 | val_1_rmse: 1.14036 |  0:00:08s
epoch 27 | loss: 0.19186 | val_0_rmse: 1.05198 | val_1_rmse: 1.05573 |  0:00:08s
epoch 28 | loss: 0.19551 | val_0_rmse: 1.04485 | val_1_rmse: 1.02843 |  0:00:08s
epoch 29 | loss: 0.19529 | val_0_rmse: 1.11807 | val_1_rmse: 1.11281 |  0:00:09s
epoch 30 | loss: 0.18717 | val_0_rmse: 1.10839 | val_1_rmse: 1.10538 |  0:00:09s
epoch 31 | loss: 0.19104 | val_0_rmse: 1.13418 | val_1_rmse: 1.09971 |  0:00:09s
epoch 32 | loss: 0.18095 | val_0_rmse: 1.21973 | val_1_rmse: 1.17901 |  0:00:10s
epoch 33 | loss: 0.18423 | val_0_rmse: 1.34459 | val_1_rmse: 1.33305 |  0:00:10s
epoch 34 | loss: 0.18029 | val_0_rmse: 1.40241 | val_1_rmse: 1.38887 |  0:00:10s
epoch 35 | loss: 0.1831  | val_0_rmse: 1.34512 | val_1_rmse: 1.33203 |  0:00:10s
epoch 36 | loss: 0.20266 | val_0_rmse: 1.24748 | val_1_rmse: 1.23355 |  0:00:11s
epoch 37 | loss: 0.19591 | val_0_rmse: 1.46286 | val_1_rmse: 1.47545 |  0:00:11s
epoch 38 | loss: 0.23707 | val_0_rmse: 1.35419 | val_1_rmse: 1.36457 |  0:00:11s
epoch 39 | loss: 0.24954 | val_0_rmse: 1.38309 | val_1_rmse: 1.3919  |  0:00:12s
epoch 40 | loss: 0.20651 | val_0_rmse: 1.6015  | val_1_rmse: 1.62591 |  0:00:12s
epoch 41 | loss: 0.21463 | val_0_rmse: 1.42457 | val_1_rmse: 1.4284  |  0:00:12s
epoch 42 | loss: 0.19817 | val_0_rmse: 1.51883 | val_1_rmse: 1.52664 |  0:00:13s
epoch 43 | loss: 0.19435 | val_0_rmse: 1.52342 | val_1_rmse: 1.53568 |  0:00:13s
epoch 44 | loss: 0.19789 | val_0_rmse: 1.50434 | val_1_rmse: 1.53269 |  0:00:13s
epoch 45 | loss: 0.28677 | val_0_rmse: 1.29287 | val_1_rmse: 1.31042 |  0:00:13s
epoch 46 | loss: 0.35    | val_0_rmse: 1.6461  | val_1_rmse: 1.66006 |  0:00:14s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.78845
Best weights from best epoch are automatically used!
ended training at: 18:30:10
Feature importance:
[('Area', 0.306104594428285), ('Baths', 0.0), ('Beds', 0.3106237380566974), ('Latitude', 0.383226349387795), ('Longitude', 0.0), ('Month', 0.0), ('Year', 4.5318127222597246e-05)]
Mean squared error is of 15950698294.48844
Mean absolute error:111832.21997113753
MAPE:1.1122251015816915
R2 score:-0.8692397736429105
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:30:10
epoch 0  | loss: 126.20354| val_0_rmse: 4.98507 | val_1_rmse: 5.12858 |  0:00:00s
epoch 1  | loss: 55.66494| val_0_rmse: 11.44482| val_1_rmse: 12.08692|  0:00:00s
epoch 2  | loss: 7.35103 | val_0_rmse: 11.29571| val_1_rmse: 11.44501|  0:00:00s
epoch 3  | loss: 6.48412 | val_0_rmse: 8.60701 | val_1_rmse: 8.21488 |  0:00:01s
epoch 4  | loss: 1.52004 | val_0_rmse: 4.13095 | val_1_rmse: 4.045   |  0:00:01s
epoch 5  | loss: 1.02362 | val_0_rmse: 4.80889 | val_1_rmse: 5.02807 |  0:00:01s
epoch 6  | loss: 0.93722 | val_0_rmse: 3.60421 | val_1_rmse: 3.55853 |  0:00:02s
epoch 7  | loss: 0.92341 | val_0_rmse: 3.20341 | val_1_rmse: 3.13535 |  0:00:02s
epoch 8  | loss: 0.52682 | val_0_rmse: 3.29985 | val_1_rmse: 3.20746 |  0:00:02s
epoch 9  | loss: 0.38259 | val_0_rmse: 1.09261 | val_1_rmse: 1.09224 |  0:00:03s
epoch 10 | loss: 0.32828 | val_0_rmse: 0.84233 | val_1_rmse: 0.83148 |  0:00:03s
epoch 11 | loss: 0.36023 | val_0_rmse: 0.94684 | val_1_rmse: 0.93533 |  0:00:03s
epoch 12 | loss: 0.32387 | val_0_rmse: 0.74969 | val_1_rmse: 0.73977 |  0:00:03s
epoch 13 | loss: 0.26747 | val_0_rmse: 0.71647 | val_1_rmse: 0.71132 |  0:00:04s
epoch 14 | loss: 0.25858 | val_0_rmse: 0.70397 | val_1_rmse: 0.70491 |  0:00:04s
epoch 15 | loss: 0.25799 | val_0_rmse: 0.59123 | val_1_rmse: 0.60707 |  0:00:04s
epoch 16 | loss: 0.25645 | val_0_rmse: 0.6049  | val_1_rmse: 0.60609 |  0:00:05s
epoch 17 | loss: 0.22926 | val_0_rmse: 0.87969 | val_1_rmse: 0.87198 |  0:00:05s
epoch 18 | loss: 0.26271 | val_0_rmse: 0.59915 | val_1_rmse: 0.60647 |  0:00:05s
epoch 19 | loss: 0.26824 | val_0_rmse: 0.71137 | val_1_rmse: 0.70803 |  0:00:06s
epoch 20 | loss: 0.21683 | val_0_rmse: 0.73113 | val_1_rmse: 0.73512 |  0:00:06s
epoch 21 | loss: 0.21419 | val_0_rmse: 0.78964 | val_1_rmse: 0.79367 |  0:00:06s
epoch 22 | loss: 0.22476 | val_0_rmse: 0.64273 | val_1_rmse: 0.64404 |  0:00:06s
epoch 23 | loss: 0.21638 | val_0_rmse: 0.67363 | val_1_rmse: 0.67215 |  0:00:07s
epoch 24 | loss: 0.23927 | val_0_rmse: 0.86248 | val_1_rmse: 0.85186 |  0:00:07s
epoch 25 | loss: 0.21788 | val_0_rmse: 0.8186  | val_1_rmse: 0.80418 |  0:00:07s
epoch 26 | loss: 0.19374 | val_0_rmse: 0.77065 | val_1_rmse: 0.75482 |  0:00:08s
epoch 27 | loss: 0.19049 | val_0_rmse: 0.7242  | val_1_rmse: 0.7112  |  0:00:08s
epoch 28 | loss: 0.18812 | val_0_rmse: 0.75304 | val_1_rmse: 0.73854 |  0:00:08s
epoch 29 | loss: 0.18323 | val_0_rmse: 0.93966 | val_1_rmse: 0.92369 |  0:00:09s
epoch 30 | loss: 0.1838  | val_0_rmse: 0.83514 | val_1_rmse: 0.82332 |  0:00:09s
epoch 31 | loss: 0.18642 | val_0_rmse: 0.75094 | val_1_rmse: 0.73812 |  0:00:09s
epoch 32 | loss: 0.18238 | val_0_rmse: 0.87855 | val_1_rmse: 0.86013 |  0:00:09s
epoch 33 | loss: 0.1991  | val_0_rmse: 0.93363 | val_1_rmse: 0.90747 |  0:00:10s
epoch 34 | loss: 0.22211 | val_0_rmse: 1.25957 | val_1_rmse: 1.23291 |  0:00:10s
epoch 35 | loss: 0.28489 | val_0_rmse: 1.01936 | val_1_rmse: 1.01179 |  0:00:10s
epoch 36 | loss: 0.23313 | val_0_rmse: 1.65134 | val_1_rmse: 1.63527 |  0:00:11s
epoch 37 | loss: 0.27177 | val_0_rmse: 1.00501 | val_1_rmse: 1.00061 |  0:00:11s
epoch 38 | loss: 0.28206 | val_0_rmse: 1.48833 | val_1_rmse: 1.47589 |  0:00:11s
epoch 39 | loss: 0.32971 | val_0_rmse: 0.73381 | val_1_rmse: 0.73684 |  0:00:12s
epoch 40 | loss: 0.28342 | val_0_rmse: 1.72071 | val_1_rmse: 1.71064 |  0:00:12s
epoch 41 | loss: 0.3358  | val_0_rmse: 1.04239 | val_1_rmse: 1.0427  |  0:00:12s
epoch 42 | loss: 0.25485 | val_0_rmse: 1.64153 | val_1_rmse: 1.63101 |  0:00:12s
epoch 43 | loss: 0.30693 | val_0_rmse: 0.96478 | val_1_rmse: 0.96761 |  0:00:13s
epoch 44 | loss: 0.25543 | val_0_rmse: 1.76134 | val_1_rmse: 1.7635  |  0:00:13s
epoch 45 | loss: 0.29391 | val_0_rmse: 0.8903  | val_1_rmse: 0.89383 |  0:00:13s
epoch 46 | loss: 0.25802 | val_0_rmse: 1.49019 | val_1_rmse: 1.48235 |  0:00:14s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.60609
Best weights from best epoch are automatically used!
ended training at: 18:30:24
Feature importance:
[('Area', 0.2794740823948194), ('Baths', 0.2917126642544963), ('Beds', 0.3080635473705174), ('Latitude', 0.06934168291937537), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.05140802306079153)]
Mean squared error is of 10837893629.784569
Mean absolute error:76366.93213444397
MAPE:0.6707313392140953
R2 score:-0.34192547387183003
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:30:24
epoch 0  | loss: 129.54639| val_0_rmse: 11.52929| val_1_rmse: 11.51574|  0:00:00s
epoch 1  | loss: 59.22769| val_0_rmse: 39.44241| val_1_rmse: 39.41153|  0:00:00s
epoch 2  | loss: 7.71327 | val_0_rmse: 62.26547| val_1_rmse: 62.2532 |  0:00:00s
epoch 3  | loss: 9.82221 | val_0_rmse: 39.09079| val_1_rmse: 39.08119|  0:00:01s
epoch 4  | loss: 2.20742 | val_0_rmse: 13.66775| val_1_rmse: 13.66108|  0:00:01s
epoch 5  | loss: 0.9837  | val_0_rmse: 11.88239| val_1_rmse: 11.87633|  0:00:01s
epoch 6  | loss: 1.09723 | val_0_rmse: 4.16365 | val_1_rmse: 4.15881 |  0:00:02s
epoch 7  | loss: 0.83769 | val_0_rmse: 2.60305 | val_1_rmse: 2.59757 |  0:00:02s
epoch 8  | loss: 0.80276 | val_0_rmse: 2.88376 | val_1_rmse: 2.87797 |  0:00:02s
epoch 9  | loss: 0.57602 | val_0_rmse: 0.78416 | val_1_rmse: 0.78614 |  0:00:03s
epoch 10 | loss: 0.65491 | val_0_rmse: 1.01032 | val_1_rmse: 1.00839 |  0:00:03s
epoch 11 | loss: 0.48455 | val_0_rmse: 0.69249 | val_1_rmse: 0.69841 |  0:00:03s
epoch 12 | loss: 0.39624 | val_0_rmse: 0.74088 | val_1_rmse: 0.74518 |  0:00:03s
epoch 13 | loss: 0.34282 | val_0_rmse: 0.86352 | val_1_rmse: 0.86525 |  0:00:04s
epoch 14 | loss: 0.31638 | val_0_rmse: 0.6147  | val_1_rmse: 0.62207 |  0:00:04s
epoch 15 | loss: 0.40926 | val_0_rmse: 0.76234 | val_1_rmse: 0.76528 |  0:00:04s
epoch 16 | loss: 0.3367  | val_0_rmse: 0.52797 | val_1_rmse: 0.54074 |  0:00:05s
epoch 17 | loss: 0.35506 | val_0_rmse: 0.74839 | val_1_rmse: 0.75169 |  0:00:05s
epoch 18 | loss: 0.26678 | val_0_rmse: 0.83776 | val_1_rmse: 0.84017 |  0:00:05s
epoch 19 | loss: 0.27719 | val_0_rmse: 0.53227 | val_1_rmse: 0.55797 |  0:00:06s
epoch 20 | loss: 0.2529  | val_0_rmse: 0.56986 | val_1_rmse: 0.58981 |  0:00:06s
epoch 21 | loss: 0.26613 | val_0_rmse: 0.74917 | val_1_rmse: 0.7624  |  0:00:06s
epoch 22 | loss: 0.27323 | val_0_rmse: 0.66773 | val_1_rmse: 0.67616 |  0:00:06s
epoch 23 | loss: 0.23652 | val_0_rmse: 0.58796 | val_1_rmse: 0.60466 |  0:00:07s
epoch 24 | loss: 0.23242 | val_0_rmse: 0.72145 | val_1_rmse: 0.73713 |  0:00:07s
epoch 25 | loss: 0.21176 | val_0_rmse: 0.69653 | val_1_rmse: 0.7145  |  0:00:07s
epoch 26 | loss: 0.19701 | val_0_rmse: 0.70616 | val_1_rmse: 0.7268  |  0:00:08s
epoch 27 | loss: 0.19214 | val_0_rmse: 0.66276 | val_1_rmse: 0.68591 |  0:00:08s
epoch 28 | loss: 0.19006 | val_0_rmse: 0.67351 | val_1_rmse: 0.69446 |  0:00:08s
epoch 29 | loss: 0.19349 | val_0_rmse: 0.62159 | val_1_rmse: 0.64044 |  0:00:09s
epoch 30 | loss: 0.18664 | val_0_rmse: 0.64887 | val_1_rmse: 0.66216 |  0:00:09s
epoch 31 | loss: 0.18909 | val_0_rmse: 0.65454 | val_1_rmse: 0.67326 |  0:00:09s
epoch 32 | loss: 0.21468 | val_0_rmse: 0.5275  | val_1_rmse: 0.5437  |  0:00:09s
epoch 33 | loss: 0.21086 | val_0_rmse: 0.48782 | val_1_rmse: 0.50783 |  0:00:10s
epoch 34 | loss: 0.22425 | val_0_rmse: 0.4631  | val_1_rmse: 0.485   |  0:00:10s
epoch 35 | loss: 0.19244 | val_0_rmse: 0.47536 | val_1_rmse: 0.49795 |  0:00:10s
epoch 36 | loss: 0.20908 | val_0_rmse: 0.53295 | val_1_rmse: 0.55715 |  0:00:11s
epoch 37 | loss: 0.18344 | val_0_rmse: 0.55049 | val_1_rmse: 0.57758 |  0:00:11s
epoch 38 | loss: 0.18214 | val_0_rmse: 0.6348  | val_1_rmse: 0.66162 |  0:00:11s
epoch 39 | loss: 0.18897 | val_0_rmse: 0.61926 | val_1_rmse: 0.63572 |  0:00:12s
epoch 40 | loss: 0.18872 | val_0_rmse: 0.78369 | val_1_rmse: 0.75693 |  0:00:12s
epoch 41 | loss: 0.17912 | val_0_rmse: 0.86317 | val_1_rmse: 0.81573 |  0:00:12s
epoch 42 | loss: 0.20613 | val_0_rmse: 0.97193 | val_1_rmse: 0.91898 |  0:00:12s
epoch 43 | loss: 0.206   | val_0_rmse: 0.99141 | val_1_rmse: 0.95583 |  0:00:13s
epoch 44 | loss: 0.27531 | val_0_rmse: 1.26543 | val_1_rmse: 1.28006 |  0:00:13s
epoch 45 | loss: 0.19037 | val_0_rmse: 1.28962 | val_1_rmse: 1.33546 |  0:00:13s
epoch 46 | loss: 0.17739 | val_0_rmse: 1.61953 | val_1_rmse: 1.65486 |  0:00:14s
epoch 47 | loss: 0.17755 | val_0_rmse: 2.13621 | val_1_rmse: 2.13117 |  0:00:14s
epoch 48 | loss: 0.17644 | val_0_rmse: 2.41767 | val_1_rmse: 2.39091 |  0:00:14s
epoch 49 | loss: 0.18208 | val_0_rmse: 2.31813 | val_1_rmse: 2.29235 |  0:00:15s
epoch 50 | loss: 0.16341 | val_0_rmse: 2.18432 | val_1_rmse: 2.16218 |  0:00:15s
epoch 51 | loss: 0.17073 | val_0_rmse: 2.28161 | val_1_rmse: 2.2679  |  0:00:15s
epoch 52 | loss: 0.17128 | val_0_rmse: 2.40744 | val_1_rmse: 2.39475 |  0:00:15s
epoch 53 | loss: 0.18834 | val_0_rmse: 2.45455 | val_1_rmse: 2.44333 |  0:00:16s
epoch 54 | loss: 0.17589 | val_0_rmse: 2.31547 | val_1_rmse: 2.3063  |  0:00:16s
epoch 55 | loss: 0.1673  | val_0_rmse: 2.56959 | val_1_rmse: 2.55537 |  0:00:16s
epoch 56 | loss: 0.19279 | val_0_rmse: 2.29518 | val_1_rmse: 2.28389 |  0:00:17s
epoch 57 | loss: 0.17223 | val_0_rmse: 2.04724 | val_1_rmse: 2.04428 |  0:00:17s
epoch 58 | loss: 0.17173 | val_0_rmse: 1.51208 | val_1_rmse: 1.51015 |  0:00:17s
epoch 59 | loss: 0.16932 | val_0_rmse: 1.36167 | val_1_rmse: 1.35834 |  0:00:18s
epoch 60 | loss: 0.17302 | val_0_rmse: 1.2043  | val_1_rmse: 1.20282 |  0:00:18s
epoch 61 | loss: 0.2045  | val_0_rmse: 1.25262 | val_1_rmse: 1.24564 |  0:00:18s
epoch 62 | loss: 0.19426 | val_0_rmse: 1.46707 | val_1_rmse: 1.45846 |  0:00:18s
epoch 63 | loss: 0.19997 | val_0_rmse: 1.56033 | val_1_rmse: 1.54626 |  0:00:19s
epoch 64 | loss: 0.25403 | val_0_rmse: 1.81925 | val_1_rmse: 1.80039 |  0:00:19s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.485
Best weights from best epoch are automatically used!
ended training at: 18:30:44
Feature importance:
[('Area', 0.0), ('Baths', 0.3126950698270606), ('Beds', 0.33300185388660314), ('Latitude', 2.584873882205639e-07), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.35430281779894807)]
Mean squared error is of 6618871612.209477
Mean absolute error:54826.94208419991
MAPE:0.36774348853750516
R2 score:0.19741892581481857
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:30:44
epoch 0  | loss: 13.04045| val_0_rmse: 18.0202 | val_1_rmse: 18.01618|  0:00:03s
epoch 1  | loss: 0.28864 | val_0_rmse: 6.15042 | val_1_rmse: 6.16411 |  0:00:07s
epoch 2  | loss: 0.23727 | val_0_rmse: 5.79312 | val_1_rmse: 5.80064 |  0:00:11s
epoch 3  | loss: 0.25158 | val_0_rmse: 1.71744 | val_1_rmse: 1.72199 |  0:00:15s
epoch 4  | loss: 0.21233 | val_0_rmse: 0.89287 | val_1_rmse: 0.90023 |  0:00:19s
epoch 5  | loss: 0.22362 | val_0_rmse: 0.54926 | val_1_rmse: 0.5538  |  0:00:23s
epoch 6  | loss: 0.20908 | val_0_rmse: 0.4483  | val_1_rmse: 0.44762 |  0:00:27s
epoch 7  | loss: 0.19301 | val_0_rmse: 0.42551 | val_1_rmse: 0.42142 |  0:00:31s
epoch 8  | loss: 0.17774 | val_0_rmse: 0.43034 | val_1_rmse: 0.42967 |  0:00:35s
epoch 9  | loss: 0.17058 | val_0_rmse: 0.44577 | val_1_rmse: 0.4417  |  0:00:39s
epoch 10 | loss: 0.18453 | val_0_rmse: 0.47373 | val_1_rmse: 0.47174 |  0:00:43s
epoch 11 | loss: 0.19412 | val_0_rmse: 0.51007 | val_1_rmse: 0.5015  |  0:00:47s
epoch 12 | loss: 0.16739 | val_0_rmse: 0.50969 | val_1_rmse: 0.50021 |  0:00:51s
epoch 13 | loss: 0.1669  | val_0_rmse: 0.40981 | val_1_rmse: 0.40303 |  0:00:54s
epoch 14 | loss: 0.17331 | val_0_rmse: 0.43638 | val_1_rmse: 0.42943 |  0:00:58s
epoch 15 | loss: 0.17368 | val_0_rmse: 0.39779 | val_1_rmse: 0.39695 |  0:01:02s
epoch 16 | loss: 0.23353 | val_0_rmse: 0.39764 | val_1_rmse: 0.39296 |  0:01:06s
epoch 17 | loss: 0.16944 | val_0_rmse: 0.39464 | val_1_rmse: 0.39072 |  0:01:10s
epoch 18 | loss: 0.17188 | val_0_rmse: 0.39804 | val_1_rmse: 0.39502 |  0:01:14s
epoch 19 | loss: 0.17326 | val_0_rmse: 0.44479 | val_1_rmse: 0.43685 |  0:01:18s
epoch 20 | loss: 0.16296 | val_0_rmse: 0.42883 | val_1_rmse: 0.42745 |  0:01:22s
epoch 21 | loss: 0.18395 | val_0_rmse: 0.40393 | val_1_rmse: 0.40292 |  0:01:26s
epoch 22 | loss: 0.16751 | val_0_rmse: 0.39603 | val_1_rmse: 0.38813 |  0:01:30s
epoch 23 | loss: 0.16662 | val_0_rmse: 0.43197 | val_1_rmse: 0.43235 |  0:01:34s
epoch 24 | loss: 0.1786  | val_0_rmse: 0.38641 | val_1_rmse: 0.38092 |  0:01:38s
epoch 25 | loss: 0.16031 | val_0_rmse: 0.48628 | val_1_rmse: 0.48895 |  0:01:42s
epoch 26 | loss: 0.22317 | val_0_rmse: 0.41495 | val_1_rmse: 0.41529 |  0:01:46s
epoch 27 | loss: 0.23326 | val_0_rmse: 0.418   | val_1_rmse: 0.41475 |  0:01:50s
epoch 28 | loss: 0.23015 | val_0_rmse: 0.45267 | val_1_rmse: 0.4428  |  0:01:54s
epoch 29 | loss: 0.20874 | val_0_rmse: 0.40141 | val_1_rmse: 0.39857 |  0:01:57s
epoch 30 | loss: 0.1724  | val_0_rmse: 0.40992 | val_1_rmse: 0.40948 |  0:02:01s
epoch 31 | loss: 0.17809 | val_0_rmse: 0.40501 | val_1_rmse: 0.39573 |  0:02:05s
epoch 32 | loss: 0.17347 | val_0_rmse: 0.41214 | val_1_rmse: 0.4058  |  0:02:09s
epoch 33 | loss: 0.16167 | val_0_rmse: 0.403   | val_1_rmse: 0.39625 |  0:02:13s
epoch 34 | loss: 0.16034 | val_0_rmse: 0.39014 | val_1_rmse: 0.38321 |  0:02:17s
epoch 35 | loss: 0.16507 | val_0_rmse: 0.39147 | val_1_rmse: 0.3892  |  0:02:21s
epoch 36 | loss: 0.17925 | val_0_rmse: 0.38579 | val_1_rmse: 0.38045 |  0:02:25s
epoch 37 | loss: 0.15887 | val_0_rmse: 0.40463 | val_1_rmse: 0.40111 |  0:02:29s
epoch 38 | loss: 0.15976 | val_0_rmse: 0.40993 | val_1_rmse: 0.40261 |  0:02:33s
epoch 39 | loss: 0.16165 | val_0_rmse: 0.42928 | val_1_rmse: 0.42343 |  0:02:37s
epoch 40 | loss: 0.16009 | val_0_rmse: 0.45116 | val_1_rmse: 0.45239 |  0:02:41s
epoch 41 | loss: 0.15836 | val_0_rmse: 0.39507 | val_1_rmse: 0.38834 |  0:02:45s
epoch 42 | loss: 0.15879 | val_0_rmse: 0.47005 | val_1_rmse: 0.46075 |  0:02:48s
epoch 43 | loss: 0.17904 | val_0_rmse: 0.44378 | val_1_rmse: 0.44702 |  0:02:52s
epoch 44 | loss: 0.18027 | val_0_rmse: 0.42401 | val_1_rmse: 0.41927 |  0:02:56s
epoch 45 | loss: 0.16891 | val_0_rmse: 0.42603 | val_1_rmse: 0.42206 |  0:03:00s
epoch 46 | loss: 0.15783 | val_0_rmse: 0.41152 | val_1_rmse: 0.41025 |  0:03:04s
epoch 47 | loss: 0.1662  | val_0_rmse: 0.43054 | val_1_rmse: 0.42401 |  0:03:08s
epoch 48 | loss: 0.16122 | val_0_rmse: 0.4116  | val_1_rmse: 0.40989 |  0:03:12s
epoch 49 | loss: 0.17812 | val_0_rmse: 0.60677 | val_1_rmse: 0.61464 |  0:03:16s
epoch 50 | loss: 0.25681 | val_0_rmse: 0.47949 | val_1_rmse: 0.48324 |  0:03:20s
epoch 51 | loss: 0.22207 | val_0_rmse: 0.45774 | val_1_rmse: 0.44777 |  0:03:24s
epoch 52 | loss: 0.19665 | val_0_rmse: 0.41221 | val_1_rmse: 0.40374 |  0:03:28s
epoch 53 | loss: 0.17486 | val_0_rmse: 0.38962 | val_1_rmse: 0.38921 |  0:03:32s
epoch 54 | loss: 0.17273 | val_0_rmse: 0.42976 | val_1_rmse: 0.42084 |  0:03:36s
epoch 55 | loss: 0.17199 | val_0_rmse: 0.40514 | val_1_rmse: 0.40532 |  0:03:40s
epoch 56 | loss: 0.17217 | val_0_rmse: 0.41981 | val_1_rmse: 0.41152 |  0:03:44s
epoch 57 | loss: 0.17262 | val_0_rmse: 0.5822  | val_1_rmse: 0.58382 |  0:03:47s
epoch 58 | loss: 0.15835 | val_0_rmse: 0.4772  | val_1_rmse: 0.46635 |  0:03:51s
epoch 59 | loss: 0.2176  | val_0_rmse: 0.42827 | val_1_rmse: 0.42838 |  0:03:55s
epoch 60 | loss: 0.20294 | val_0_rmse: 0.38588 | val_1_rmse: 0.38512 |  0:03:59s
epoch 61 | loss: 0.16876 | val_0_rmse: 0.39786 | val_1_rmse: 0.38933 |  0:04:03s
epoch 62 | loss: 0.16209 | val_0_rmse: 0.42001 | val_1_rmse: 0.414   |  0:04:07s
epoch 63 | loss: 0.16262 | val_0_rmse: 0.46611 | val_1_rmse: 0.4558  |  0:04:11s
epoch 64 | loss: 0.17047 | val_0_rmse: 0.39815 | val_1_rmse: 0.3974  |  0:04:15s
epoch 65 | loss: 0.15836 | val_0_rmse: 0.38868 | val_1_rmse: 0.38556 |  0:04:19s
epoch 66 | loss: 0.17031 | val_0_rmse: 0.39026 | val_1_rmse: 0.38625 |  0:04:23s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.38045
Best weights from best epoch are automatically used!
ended training at: 18:35:09
Feature importance:
[('Area', 0.2728735615168162), ('Baths', 0.1438905544800347), ('Beds', 0.12710542495961702), ('Latitude', 0.14964053072036584), ('Longitude', 0.17161177353104437), ('Month', 0.057949990588940674), ('Year', 0.07692816420318122)]
Mean squared error is of 1003586668.5141712
Mean absolute error:20931.079397732505
MAPE:0.30172611460912885
R2 score:0.6998426398205595
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:35:09
epoch 0  | loss: 12.42343| val_0_rmse: 0.6685  | val_1_rmse: 0.65982 |  0:00:03s
epoch 1  | loss: 0.32143 | val_0_rmse: 4.42614 | val_1_rmse: 4.41618 |  0:00:07s
epoch 2  | loss: 0.23848 | val_0_rmse: 2.39632 | val_1_rmse: 2.40005 |  0:00:11s
epoch 3  | loss: 0.26723 | val_0_rmse: 1.6315  | val_1_rmse: 1.6291  |  0:00:15s
epoch 4  | loss: 0.20503 | val_0_rmse: 0.97069 | val_1_rmse: 0.96876 |  0:00:19s
epoch 5  | loss: 0.19513 | val_0_rmse: 0.64717 | val_1_rmse: 0.64447 |  0:00:23s
epoch 6  | loss: 0.20318 | val_0_rmse: 0.62907 | val_1_rmse: 0.62569 |  0:00:27s
epoch 7  | loss: 0.18772 | val_0_rmse: 0.62881 | val_1_rmse: 0.62799 |  0:00:31s
epoch 8  | loss: 0.18764 | val_0_rmse: 0.61117 | val_1_rmse: 0.61012 |  0:00:35s
epoch 9  | loss: 0.18729 | val_0_rmse: 0.57397 | val_1_rmse: 0.57438 |  0:00:39s
epoch 10 | loss: 0.17809 | val_0_rmse: 0.48434 | val_1_rmse: 0.4821  |  0:00:43s
epoch 11 | loss: 0.17785 | val_0_rmse: 0.4424  | val_1_rmse: 0.43645 |  0:00:47s
epoch 12 | loss: 0.1752  | val_0_rmse: 0.43177 | val_1_rmse: 0.42903 |  0:00:51s
epoch 13 | loss: 0.20655 | val_0_rmse: 0.40837 | val_1_rmse: 0.40594 |  0:00:55s
epoch 14 | loss: 0.21124 | val_0_rmse: 0.44203 | val_1_rmse: 0.44205 |  0:00:59s
epoch 15 | loss: 0.20885 | val_0_rmse: 0.43629 | val_1_rmse: 0.43402 |  0:01:03s
epoch 16 | loss: 0.17739 | val_0_rmse: 0.45783 | val_1_rmse: 0.45543 |  0:01:07s
epoch 17 | loss: 0.20071 | val_0_rmse: 0.41167 | val_1_rmse: 0.41171 |  0:01:11s
epoch 18 | loss: 0.17609 | val_0_rmse: 0.39393 | val_1_rmse: 0.39406 |  0:01:14s
epoch 19 | loss: 0.19785 | val_0_rmse: 0.40783 | val_1_rmse: 0.4071  |  0:01:18s
epoch 20 | loss: 0.20503 | val_0_rmse: 0.39358 | val_1_rmse: 0.39353 |  0:01:22s
epoch 21 | loss: 0.17831 | val_0_rmse: 0.42496 | val_1_rmse: 0.4229  |  0:01:26s
epoch 22 | loss: 0.16904 | val_0_rmse: 0.40288 | val_1_rmse: 0.40054 |  0:01:30s
epoch 23 | loss: 0.17512 | val_0_rmse: 0.42013 | val_1_rmse: 0.42057 |  0:01:34s
epoch 24 | loss: 0.18682 | val_0_rmse: 0.43496 | val_1_rmse: 0.4328  |  0:01:38s
epoch 25 | loss: 0.17983 | val_0_rmse: 0.41034 | val_1_rmse: 0.40886 |  0:01:42s
epoch 26 | loss: 0.17326 | val_0_rmse: 0.39787 | val_1_rmse: 0.39784 |  0:01:46s
epoch 27 | loss: 0.17073 | val_0_rmse: 0.39956 | val_1_rmse: 0.39767 |  0:01:50s
epoch 28 | loss: 0.22256 | val_0_rmse: 0.40169 | val_1_rmse: 0.40049 |  0:01:54s
epoch 29 | loss: 0.16793 | val_0_rmse: 0.48168 | val_1_rmse: 0.48497 |  0:01:58s
epoch 30 | loss: 0.19866 | val_0_rmse: 0.42108 | val_1_rmse: 0.42358 |  0:02:02s
epoch 31 | loss: 0.16831 | val_0_rmse: 0.44116 | val_1_rmse: 0.44329 |  0:02:06s
epoch 32 | loss: 0.1792  | val_0_rmse: 0.39385 | val_1_rmse: 0.39427 |  0:02:10s
epoch 33 | loss: 0.1663  | val_0_rmse: 0.39797 | val_1_rmse: 0.39925 |  0:02:13s
epoch 34 | loss: 0.18364 | val_0_rmse: 0.44079 | val_1_rmse: 0.42467 |  0:02:17s
epoch 35 | loss: 0.17865 | val_0_rmse: 0.42387 | val_1_rmse: 0.42212 |  0:02:21s
epoch 36 | loss: 0.1793  | val_0_rmse: 0.40536 | val_1_rmse: 0.39541 |  0:02:25s
epoch 37 | loss: 0.17203 | val_0_rmse: 0.40609 | val_1_rmse: 0.39867 |  0:02:29s
epoch 38 | loss: 0.17995 | val_0_rmse: 0.45451 | val_1_rmse: 0.44582 |  0:02:33s
epoch 39 | loss: 0.16511 | val_0_rmse: 0.39028 | val_1_rmse: 0.3835  |  0:02:37s
epoch 40 | loss: 0.17422 | val_0_rmse: 0.39339 | val_1_rmse: 0.39314 |  0:02:41s
epoch 41 | loss: 0.18182 | val_0_rmse: 0.45718 | val_1_rmse: 0.44892 |  0:02:45s
epoch 42 | loss: 0.17776 | val_0_rmse: 0.3969  | val_1_rmse: 0.39647 |  0:02:49s
epoch 43 | loss: 0.1799  | val_0_rmse: 0.45011 | val_1_rmse: 0.45096 |  0:02:53s
epoch 44 | loss: 0.19329 | val_0_rmse: 0.38755 | val_1_rmse: 0.38808 |  0:02:57s
epoch 45 | loss: 0.17939 | val_0_rmse: 0.39274 | val_1_rmse: 0.39285 |  0:03:01s
epoch 46 | loss: 0.17631 | val_0_rmse: 0.409   | val_1_rmse: 0.40976 |  0:03:05s
epoch 47 | loss: 0.1714  | val_0_rmse: 0.41574 | val_1_rmse: 0.41128 |  0:03:09s
epoch 48 | loss: 0.17029 | val_0_rmse: 0.39379 | val_1_rmse: 0.39547 |  0:03:13s
epoch 49 | loss: 0.17268 | val_0_rmse: 0.40776 | val_1_rmse: 0.40926 |  0:03:17s
epoch 50 | loss: 0.16187 | val_0_rmse: 0.41246 | val_1_rmse: 0.41428 |  0:03:20s
epoch 51 | loss: 0.1996  | val_0_rmse: 0.45144 | val_1_rmse: 0.45155 |  0:03:24s
epoch 52 | loss: 0.19769 | val_0_rmse: 0.39792 | val_1_rmse: 0.39911 |  0:03:28s
epoch 53 | loss: 0.20622 | val_0_rmse: 0.47919 | val_1_rmse: 0.48154 |  0:03:32s
epoch 54 | loss: 0.1817  | val_0_rmse: 0.50977 | val_1_rmse: 0.50842 |  0:03:36s
epoch 55 | loss: 0.19125 | val_0_rmse: 0.45789 | val_1_rmse: 0.45611 |  0:03:40s
epoch 56 | loss: 0.1771  | val_0_rmse: 0.39192 | val_1_rmse: 0.39189 |  0:03:44s
epoch 57 | loss: 0.17615 | val_0_rmse: 0.41701 | val_1_rmse: 0.41549 |  0:03:48s
epoch 58 | loss: 0.17632 | val_0_rmse: 0.41674 | val_1_rmse: 0.41747 |  0:03:52s
epoch 59 | loss: 0.18002 | val_0_rmse: 0.38878 | val_1_rmse: 0.38917 |  0:03:56s
epoch 60 | loss: 0.17559 | val_0_rmse: 0.42624 | val_1_rmse: 0.42459 |  0:04:00s
epoch 61 | loss: 0.18896 | val_0_rmse: 0.43012 | val_1_rmse: 0.4311  |  0:04:04s
epoch 62 | loss: 0.18117 | val_0_rmse: 0.43926 | val_1_rmse: 0.43856 |  0:04:08s
epoch 63 | loss: 0.17755 | val_0_rmse: 0.40109 | val_1_rmse: 0.40137 |  0:04:12s
epoch 64 | loss: 0.15761 | val_0_rmse: 0.39742 | val_1_rmse: 0.39886 |  0:04:16s
epoch 65 | loss: 0.1893  | val_0_rmse: 0.41546 | val_1_rmse: 0.41701 |  0:04:19s
epoch 66 | loss: 0.18154 | val_0_rmse: 0.40976 | val_1_rmse: 0.41004 |  0:04:23s
epoch 67 | loss: 0.16072 | val_0_rmse: 0.40162 | val_1_rmse: 0.4034  |  0:04:27s
epoch 68 | loss: 0.16263 | val_0_rmse: 0.40836 | val_1_rmse: 0.40963 |  0:04:31s
epoch 69 | loss: 0.1636  | val_0_rmse: 0.3963  | val_1_rmse: 0.39659 |  0:04:35s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.3835
Best weights from best epoch are automatically used!
ended training at: 18:39:46
Feature importance:
[('Area', 0.0), ('Baths', 0.09408474556178094), ('Beds', 0.1006891942339547), ('Latitude', 0.1121033245735281), ('Longitude', 0.3271151704637269), ('Month', 0.09703369208430856), ('Year', 0.2689738730827008)]
Mean squared error is of 1116091952.701612
Mean absolute error:21556.798377983043
MAPE:0.29868627103114337
R2 score:0.6681850456177918
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:39:47
epoch 0  | loss: 12.80595| val_0_rmse: 7.63369 | val_1_rmse: 7.62004 |  0:00:03s
epoch 1  | loss: 0.28457 | val_0_rmse: 4.84246 | val_1_rmse: 4.83736 |  0:00:07s
epoch 2  | loss: 0.22113 | val_0_rmse: 2.57268 | val_1_rmse: 2.57081 |  0:00:11s
epoch 3  | loss: 0.24275 | val_0_rmse: 1.58414 | val_1_rmse: 1.58201 |  0:00:15s
epoch 4  | loss: 0.22322 | val_0_rmse: 0.70644 | val_1_rmse: 0.70396 |  0:00:19s
epoch 5  | loss: 0.20431 | val_0_rmse: 0.68185 | val_1_rmse: 0.68008 |  0:00:23s
epoch 6  | loss: 0.20376 | val_0_rmse: 0.67921 | val_1_rmse: 0.68249 |  0:00:27s
epoch 7  | loss: 0.19384 | val_0_rmse: 0.47286 | val_1_rmse: 0.48677 |  0:00:31s
epoch 8  | loss: 0.22598 | val_0_rmse: 0.463   | val_1_rmse: 0.47151 |  0:00:35s
epoch 9  | loss: 0.20111 | val_0_rmse: 0.44597 | val_1_rmse: 0.45379 |  0:00:39s
epoch 10 | loss: 0.1918  | val_0_rmse: 0.44613 | val_1_rmse: 0.45131 |  0:00:43s
epoch 11 | loss: 0.19455 | val_0_rmse: 0.44759 | val_1_rmse: 0.45222 |  0:00:47s
epoch 12 | loss: 0.20088 | val_0_rmse: 0.40588 | val_1_rmse: 0.41392 |  0:00:51s
epoch 13 | loss: 0.17445 | val_0_rmse: 0.40165 | val_1_rmse: 0.41125 |  0:00:55s
epoch 14 | loss: 0.17362 | val_0_rmse: 0.42707 | val_1_rmse: 0.43055 |  0:00:59s
epoch 15 | loss: 0.17323 | val_0_rmse: 0.44708 | val_1_rmse: 0.45299 |  0:01:03s
epoch 16 | loss: 0.17925 | val_0_rmse: 0.39285 | val_1_rmse: 0.39991 |  0:01:07s
epoch 17 | loss: 0.16778 | val_0_rmse: 0.40762 | val_1_rmse: 0.41724 |  0:01:11s
epoch 18 | loss: 0.17764 | val_0_rmse: 0.40397 | val_1_rmse: 0.41216 |  0:01:14s
epoch 19 | loss: 0.1898  | val_0_rmse: 0.40622 | val_1_rmse: 0.41219 |  0:01:18s
epoch 20 | loss: 0.18708 | val_0_rmse: 0.47723 | val_1_rmse: 0.48381 |  0:01:22s
epoch 21 | loss: 0.18872 | val_0_rmse: 0.42651 | val_1_rmse: 0.43211 |  0:01:26s
epoch 22 | loss: 0.1688  | val_0_rmse: 0.41745 | val_1_rmse: 0.42974 |  0:01:30s
epoch 23 | loss: 0.17531 | val_0_rmse: 0.41541 | val_1_rmse: 0.42148 |  0:01:34s
epoch 24 | loss: 0.19165 | val_0_rmse: 0.40944 | val_1_rmse: 0.42058 |  0:01:38s
epoch 25 | loss: 0.18439 | val_0_rmse: 0.49381 | val_1_rmse: 0.49882 |  0:01:42s
epoch 26 | loss: 0.18002 | val_0_rmse: 0.43756 | val_1_rmse: 0.44773 |  0:01:46s
epoch 27 | loss: 0.17968 | val_0_rmse: 0.40735 | val_1_rmse: 0.41358 |  0:01:50s
epoch 28 | loss: 0.18858 | val_0_rmse: 0.60292 | val_1_rmse: 0.60661 |  0:01:54s
epoch 29 | loss: 0.20794 | val_0_rmse: 0.42016 | val_1_rmse: 0.42387 |  0:01:58s
epoch 30 | loss: 0.16211 | val_0_rmse: 0.40369 | val_1_rmse: 0.41443 |  0:02:02s
epoch 31 | loss: 0.16735 | val_0_rmse: 0.55636 | val_1_rmse: 0.57324 |  0:02:06s
epoch 32 | loss: 0.17256 | val_0_rmse: 0.41352 | val_1_rmse: 0.41837 |  0:02:10s
epoch 33 | loss: 0.16699 | val_0_rmse: 0.41682 | val_1_rmse: 0.42106 |  0:02:14s
epoch 34 | loss: 0.17564 | val_0_rmse: 0.42499 | val_1_rmse: 0.42875 |  0:02:18s
epoch 35 | loss: 0.18678 | val_0_rmse: 0.40381 | val_1_rmse: 0.41043 |  0:02:21s
epoch 36 | loss: 0.18296 | val_0_rmse: 0.40146 | val_1_rmse: 0.40808 |  0:02:25s
epoch 37 | loss: 0.16403 | val_0_rmse: 0.46954 | val_1_rmse: 0.4772  |  0:02:29s
epoch 38 | loss: 0.18385 | val_0_rmse: 0.44173 | val_1_rmse: 0.44615 |  0:02:33s
epoch 39 | loss: 0.19202 | val_0_rmse: 0.44444 | val_1_rmse: 0.45391 |  0:02:37s
epoch 40 | loss: 0.2015  | val_0_rmse: 0.43349 | val_1_rmse: 0.4424  |  0:02:41s
epoch 41 | loss: 0.19313 | val_0_rmse: 0.40819 | val_1_rmse: 0.41681 |  0:02:45s
epoch 42 | loss: 0.17763 | val_0_rmse: 0.40604 | val_1_rmse: 0.41586 |  0:02:49s
epoch 43 | loss: 0.18456 | val_0_rmse: 0.46731 | val_1_rmse: 0.47473 |  0:02:53s
epoch 44 | loss: 0.18787 | val_0_rmse: 0.41667 | val_1_rmse: 0.42582 |  0:02:57s
epoch 45 | loss: 0.19638 | val_0_rmse: 0.4569  | val_1_rmse: 0.46792 |  0:03:01s
epoch 46 | loss: 0.17986 | val_0_rmse: 0.40433 | val_1_rmse: 0.41298 |  0:03:05s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.39991
Best weights from best epoch are automatically used!
ended training at: 18:42:53
Feature importance:
[('Area', 0.24532056107964323), ('Baths', 0.09581273142699472), ('Beds', 0.21027084334770557), ('Latitude', 0.035076989450878296), ('Longitude', 0.35076375663716447), ('Month', 0.014353035518270876), ('Year', 0.04840208253934285)]
Mean squared error is of 1085227455.89491
Mean absolute error:22094.699134893915
MAPE:0.33177696628354214
R2 score:0.6810628220593131
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:42:53
epoch 0  | loss: 12.08167| val_0_rmse: 4.38238 | val_1_rmse: 4.39111 |  0:00:03s
epoch 1  | loss: 0.31843 | val_0_rmse: 2.51424 | val_1_rmse: 2.52241 |  0:00:07s
epoch 2  | loss: 0.25653 | val_0_rmse: 2.17154 | val_1_rmse: 2.18095 |  0:00:11s
epoch 3  | loss: 0.22894 | val_0_rmse: 1.70487 | val_1_rmse: 1.71454 |  0:00:15s
epoch 4  | loss: 0.22819 | val_0_rmse: 1.47616 | val_1_rmse: 1.48671 |  0:00:19s
epoch 5  | loss: 0.2938  | val_0_rmse: 0.85295 | val_1_rmse: 0.86205 |  0:00:23s
epoch 6  | loss: 0.22623 | val_0_rmse: 0.75007 | val_1_rmse: 0.75697 |  0:00:27s
epoch 7  | loss: 0.23073 | val_0_rmse: 0.75152 | val_1_rmse: 0.75888 |  0:00:31s
epoch 8  | loss: 0.22037 | val_0_rmse: 0.83286 | val_1_rmse: 0.8381  |  0:00:35s
epoch 9  | loss: 0.20772 | val_0_rmse: 0.56649 | val_1_rmse: 0.56657 |  0:00:39s
epoch 10 | loss: 0.19661 | val_0_rmse: 0.4866  | val_1_rmse: 0.49001 |  0:00:43s
epoch 11 | loss: 0.19147 | val_0_rmse: 0.45357 | val_1_rmse: 0.45627 |  0:00:47s
epoch 12 | loss: 0.18131 | val_0_rmse: 0.41567 | val_1_rmse: 0.41684 |  0:00:51s
epoch 13 | loss: 0.16846 | val_0_rmse: 0.4201  | val_1_rmse: 0.42135 |  0:00:55s
epoch 14 | loss: 0.17374 | val_0_rmse: 0.43547 | val_1_rmse: 0.43868 |  0:00:59s
epoch 15 | loss: 0.17861 | val_0_rmse: 0.38991 | val_1_rmse: 0.38919 |  0:01:03s
epoch 16 | loss: 0.17789 | val_0_rmse: 0.438   | val_1_rmse: 0.43966 |  0:01:07s
epoch 17 | loss: 0.17378 | val_0_rmse: 0.44195 | val_1_rmse: 0.44437 |  0:01:10s
epoch 18 | loss: 0.16557 | val_0_rmse: 0.3949  | val_1_rmse: 0.39534 |  0:01:14s
epoch 19 | loss: 0.18102 | val_0_rmse: 0.45101 | val_1_rmse: 0.45155 |  0:01:18s
epoch 20 | loss: 0.17276 | val_0_rmse: 0.39205 | val_1_rmse: 0.39198 |  0:01:22s
epoch 21 | loss: 0.17264 | val_0_rmse: 0.38696 | val_1_rmse: 0.38844 |  0:01:26s
epoch 22 | loss: 0.17221 | val_0_rmse: 0.49117 | val_1_rmse: 0.49024 |  0:01:30s
epoch 23 | loss: 0.18346 | val_0_rmse: 0.51814 | val_1_rmse: 0.52384 |  0:01:34s
epoch 24 | loss: 0.17865 | val_0_rmse: 0.44059 | val_1_rmse: 0.44498 |  0:01:38s
epoch 25 | loss: 0.19053 | val_0_rmse: 0.40381 | val_1_rmse: 0.40735 |  0:01:42s
epoch 26 | loss: 0.18719 | val_0_rmse: 0.40993 | val_1_rmse: 0.41189 |  0:01:46s
epoch 27 | loss: 0.17058 | val_0_rmse: 0.41088 | val_1_rmse: 0.41307 |  0:01:50s
epoch 28 | loss: 0.17083 | val_0_rmse: 0.39982 | val_1_rmse: 0.40396 |  0:01:54s
epoch 29 | loss: 0.17234 | val_0_rmse: 0.40068 | val_1_rmse: 0.40498 |  0:01:58s
epoch 30 | loss: 0.17958 | val_0_rmse: 0.4087  | val_1_rmse: 0.41184 |  0:02:02s
epoch 31 | loss: 0.17872 | val_0_rmse: 0.40372 | val_1_rmse: 0.40576 |  0:02:06s
epoch 32 | loss: 0.16963 | val_0_rmse: 0.40416 | val_1_rmse: 0.40823 |  0:02:09s
epoch 33 | loss: 0.18883 | val_0_rmse: 0.46624 | val_1_rmse: 0.47018 |  0:02:13s
epoch 34 | loss: 0.18097 | val_0_rmse: 0.39321 | val_1_rmse: 0.39648 |  0:02:17s
epoch 35 | loss: 0.2129  | val_0_rmse: 0.41145 | val_1_rmse: 0.41413 |  0:02:21s
epoch 36 | loss: 0.23377 | val_0_rmse: 0.42691 | val_1_rmse: 0.4303  |  0:02:25s
epoch 37 | loss: 0.22327 | val_0_rmse: 0.39177 | val_1_rmse: 0.39589 |  0:02:29s
epoch 38 | loss: 0.19901 | val_0_rmse: 0.42661 | val_1_rmse: 0.43031 |  0:02:33s
epoch 39 | loss: 0.18776 | val_0_rmse: 0.40108 | val_1_rmse: 0.40575 |  0:02:37s
epoch 40 | loss: 0.16801 | val_0_rmse: 0.39915 | val_1_rmse: 0.4016  |  0:02:41s
epoch 41 | loss: 0.17921 | val_0_rmse: 0.39961 | val_1_rmse: 0.40347 |  0:02:45s
epoch 42 | loss: 0.16768 | val_0_rmse: 0.40149 | val_1_rmse: 0.40459 |  0:02:49s
epoch 43 | loss: 0.16823 | val_0_rmse: 0.4211  | val_1_rmse: 0.4233  |  0:02:53s
epoch 44 | loss: 0.22273 | val_0_rmse: 0.40378 | val_1_rmse: 0.4068  |  0:02:57s
epoch 45 | loss: 0.1708  | val_0_rmse: 0.3907  | val_1_rmse: 0.39472 |  0:03:01s
epoch 46 | loss: 0.19001 | val_0_rmse: 0.41626 | val_1_rmse: 0.41995 |  0:03:05s
epoch 47 | loss: 0.18159 | val_0_rmse: 0.40908 | val_1_rmse: 0.41194 |  0:03:09s
epoch 48 | loss: 0.18032 | val_0_rmse: 0.40648 | val_1_rmse: 0.41063 |  0:03:12s
epoch 49 | loss: 0.18912 | val_0_rmse: 0.40536 | val_1_rmse: 0.40979 |  0:03:16s
epoch 50 | loss: 0.18024 | val_0_rmse: 0.3972  | val_1_rmse: 0.39994 |  0:03:20s
epoch 51 | loss: 0.16526 | val_0_rmse: 0.39362 | val_1_rmse: 0.39664 |  0:03:24s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.38844
Best weights from best epoch are automatically used!
ended training at: 18:46:19
Feature importance:
[('Area', 0.09367636346050244), ('Baths', 0.3209343297015086), ('Beds', 0.08675233602865803), ('Latitude', 0.22649823053947976), ('Longitude', 0.2472524678131711), ('Month', 1.3515836501374507e-07), ('Year', 0.024886137298315056)]
Mean squared error is of 1042087016.6531123
Mean absolute error:21731.711197173307
MAPE:0.3292156093279561
R2 score:0.6907492601220606
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:46:20
epoch 0  | loss: 12.61544| val_0_rmse: 0.81991 | val_1_rmse: 0.82103 |  0:00:03s
epoch 1  | loss: 0.32367 | val_0_rmse: 2.32783 | val_1_rmse: 2.32234 |  0:00:07s
epoch 2  | loss: 0.24095 | val_0_rmse: 0.88883 | val_1_rmse: 0.88447 |  0:00:11s
epoch 3  | loss: 0.23018 | val_0_rmse: 0.87784 | val_1_rmse: 0.87985 |  0:00:15s
epoch 4  | loss: 0.21593 | val_0_rmse: 0.58435 | val_1_rmse: 0.58842 |  0:00:19s
epoch 5  | loss: 0.2065  | val_0_rmse: 0.78817 | val_1_rmse: 0.79135 |  0:00:23s
epoch 6  | loss: 0.21003 | val_0_rmse: 0.7755  | val_1_rmse: 0.77714 |  0:00:27s
epoch 7  | loss: 0.19129 | val_0_rmse: 0.58864 | val_1_rmse: 0.58997 |  0:00:31s
epoch 8  | loss: 0.18247 | val_0_rmse: 0.49226 | val_1_rmse: 0.49087 |  0:00:35s
epoch 9  | loss: 0.18556 | val_0_rmse: 0.48293 | val_1_rmse: 0.48193 |  0:00:39s
epoch 10 | loss: 0.19217 | val_0_rmse: 0.51442 | val_1_rmse: 0.52161 |  0:00:43s
epoch 11 | loss: 0.20392 | val_0_rmse: 0.55303 | val_1_rmse: 0.55559 |  0:00:47s
epoch 12 | loss: 0.23072 | val_0_rmse: 0.42995 | val_1_rmse: 0.4294  |  0:00:51s
epoch 13 | loss: 0.18213 | val_0_rmse: 0.41441 | val_1_rmse: 0.41726 |  0:00:54s
epoch 14 | loss: 0.1771  | val_0_rmse: 0.43615 | val_1_rmse: 0.43473 |  0:00:58s
epoch 15 | loss: 0.17765 | val_0_rmse: 0.41495 | val_1_rmse: 0.41609 |  0:01:02s
epoch 16 | loss: 0.18894 | val_0_rmse: 0.44012 | val_1_rmse: 0.43836 |  0:01:06s
epoch 17 | loss: 0.17854 | val_0_rmse: 0.4084  | val_1_rmse: 0.40999 |  0:01:10s
epoch 18 | loss: 0.1781  | val_0_rmse: 0.4125  | val_1_rmse: 0.41083 |  0:01:14s
epoch 19 | loss: 0.1817  | val_0_rmse: 0.43923 | val_1_rmse: 0.44284 |  0:01:18s
epoch 20 | loss: 0.22403 | val_0_rmse: 0.57699 | val_1_rmse: 0.57871 |  0:01:22s
epoch 21 | loss: 0.19532 | val_0_rmse: 0.40248 | val_1_rmse: 0.40462 |  0:01:26s
epoch 22 | loss: 0.17039 | val_0_rmse: 0.3961  | val_1_rmse: 0.3965  |  0:01:30s
epoch 23 | loss: 0.17133 | val_0_rmse: 0.40018 | val_1_rmse: 0.41458 |  0:01:34s
epoch 24 | loss: 0.18869 | val_0_rmse: 0.45309 | val_1_rmse: 0.45161 |  0:01:38s
epoch 25 | loss: 0.18977 | val_0_rmse: 0.42086 | val_1_rmse: 0.44359 |  0:01:42s
epoch 26 | loss: 0.18465 | val_0_rmse: 0.43647 | val_1_rmse: 0.43481 |  0:01:46s
epoch 27 | loss: 0.18266 | val_0_rmse: 0.52782 | val_1_rmse: 0.53215 |  0:01:50s
epoch 28 | loss: 0.16842 | val_0_rmse: 0.41237 | val_1_rmse: 0.41009 |  0:01:53s
epoch 29 | loss: 0.16327 | val_0_rmse: 0.43848 | val_1_rmse: 0.43626 |  0:01:57s
epoch 30 | loss: 0.16345 | val_0_rmse: 0.39179 | val_1_rmse: 0.39198 |  0:02:01s
epoch 31 | loss: 0.16096 | val_0_rmse: 0.38967 | val_1_rmse: 0.38649 |  0:02:05s
epoch 32 | loss: 0.16483 | val_0_rmse: 0.40789 | val_1_rmse: 0.4069  |  0:02:09s
epoch 33 | loss: 0.19089 | val_0_rmse: 0.45946 | val_1_rmse: 0.45626 |  0:02:13s
epoch 34 | loss: 0.17458 | val_0_rmse: 0.45702 | val_1_rmse: 0.45889 |  0:02:17s
epoch 35 | loss: 0.17504 | val_0_rmse: 0.42911 | val_1_rmse: 0.42681 |  0:02:21s
epoch 36 | loss: 0.19028 | val_0_rmse: 0.40057 | val_1_rmse: 0.39887 |  0:02:25s
epoch 37 | loss: 0.16562 | val_0_rmse: 0.38635 | val_1_rmse: 0.3881  |  0:02:29s
epoch 38 | loss: 0.16832 | val_0_rmse: 0.40401 | val_1_rmse: 0.40245 |  0:02:33s
epoch 39 | loss: 0.17631 | val_0_rmse: 0.38301 | val_1_rmse: 0.38192 |  0:02:37s
epoch 40 | loss: 0.16951 | val_0_rmse: 0.42846 | val_1_rmse: 0.42659 |  0:02:41s
epoch 41 | loss: 0.1727  | val_0_rmse: 0.42288 | val_1_rmse: 0.4233  |  0:02:45s
epoch 42 | loss: 0.17155 | val_0_rmse: 0.41875 | val_1_rmse: 0.41939 |  0:02:49s
epoch 43 | loss: 0.17218 | val_0_rmse: 0.43204 | val_1_rmse: 0.43207 |  0:02:53s
epoch 44 | loss: 0.1669  | val_0_rmse: 0.40295 | val_1_rmse: 0.40094 |  0:02:57s
epoch 45 | loss: 0.173   | val_0_rmse: 0.41687 | val_1_rmse: 0.41395 |  0:03:00s
epoch 46 | loss: 0.15392 | val_0_rmse: 0.38285 | val_1_rmse: 0.38152 |  0:03:04s
epoch 47 | loss: 0.16817 | val_0_rmse: 0.4078  | val_1_rmse: 0.409   |  0:03:08s
epoch 48 | loss: 0.17332 | val_0_rmse: 0.42778 | val_1_rmse: 0.42919 |  0:03:12s
epoch 49 | loss: 0.17241 | val_0_rmse: 0.39814 | val_1_rmse: 0.39524 |  0:03:16s
epoch 50 | loss: 0.16746 | val_0_rmse: 0.39385 | val_1_rmse: 0.39297 |  0:03:20s
epoch 51 | loss: 0.15423 | val_0_rmse: 0.40148 | val_1_rmse: 0.39892 |  0:03:24s
epoch 52 | loss: 0.18625 | val_0_rmse: 0.41227 | val_1_rmse: 0.41118 |  0:03:28s
epoch 53 | loss: 0.1528  | val_0_rmse: 0.38153 | val_1_rmse: 0.38188 |  0:03:32s
epoch 54 | loss: 0.15733 | val_0_rmse: 0.43539 | val_1_rmse: 0.43317 |  0:03:36s
epoch 55 | loss: 0.15617 | val_0_rmse: 0.46596 | val_1_rmse: 0.46867 |  0:03:40s
epoch 56 | loss: 0.16229 | val_0_rmse: 0.37909 | val_1_rmse: 0.37773 |  0:03:44s
epoch 57 | loss: 0.16829 | val_0_rmse: 0.37747 | val_1_rmse: 0.37833 |  0:03:48s
epoch 58 | loss: 0.15442 | val_0_rmse: 0.39323 | val_1_rmse: 0.39342 |  0:03:52s
epoch 59 | loss: 0.15348 | val_0_rmse: 0.3739  | val_1_rmse: 0.37393 |  0:03:56s
epoch 60 | loss: 0.17947 | val_0_rmse: 0.4656  | val_1_rmse: 0.47053 |  0:04:00s
epoch 61 | loss: 0.16571 | val_0_rmse: 0.3994  | val_1_rmse: 0.3992  |  0:04:03s
epoch 62 | loss: 0.17226 | val_0_rmse: 0.40866 | val_1_rmse: 0.40907 |  0:04:07s
epoch 63 | loss: 0.15323 | val_0_rmse: 0.38073 | val_1_rmse: 0.38247 |  0:04:11s
epoch 64 | loss: 0.16045 | val_0_rmse: 0.37229 | val_1_rmse: 0.3724  |  0:04:15s
epoch 65 | loss: 0.16335 | val_0_rmse: 0.38282 | val_1_rmse: 0.38112 |  0:04:19s
epoch 66 | loss: 0.16193 | val_0_rmse: 0.47794 | val_1_rmse: 0.47608 |  0:04:23s
epoch 67 | loss: 0.14876 | val_0_rmse: 0.37657 | val_1_rmse: 0.3748  |  0:04:27s
epoch 68 | loss: 0.1498  | val_0_rmse: 0.43206 | val_1_rmse: 0.43378 |  0:04:31s
epoch 69 | loss: 0.16892 | val_0_rmse: 0.42143 | val_1_rmse: 0.42261 |  0:04:35s
epoch 70 | loss: 0.17251 | val_0_rmse: 0.47597 | val_1_rmse: 0.47266 |  0:04:39s
epoch 71 | loss: 0.16947 | val_0_rmse: 0.72387 | val_1_rmse: 0.72835 |  0:04:43s
epoch 72 | loss: 0.15318 | val_0_rmse: 0.43838 | val_1_rmse: 0.44199 |  0:04:47s
epoch 73 | loss: 0.15335 | val_0_rmse: 0.38904 | val_1_rmse: 0.38987 |  0:04:51s
epoch 74 | loss: 0.15039 | val_0_rmse: 0.37973 | val_1_rmse: 0.37877 |  0:04:55s
epoch 75 | loss: 0.14893 | val_0_rmse: 0.39528 | val_1_rmse: 0.39629 |  0:04:59s
epoch 76 | loss: 0.14871 | val_0_rmse: 0.38313 | val_1_rmse: 0.38261 |  0:05:03s
epoch 77 | loss: 0.15981 | val_0_rmse: 0.37847 | val_1_rmse: 0.37745 |  0:05:07s
epoch 78 | loss: 0.14852 | val_0_rmse: 0.37532 | val_1_rmse: 0.37577 |  0:05:10s
epoch 79 | loss: 0.15587 | val_0_rmse: 0.37289 | val_1_rmse: 0.3725  |  0:05:14s
epoch 80 | loss: 0.15793 | val_0_rmse: 0.39331 | val_1_rmse: 0.39195 |  0:05:18s
epoch 81 | loss: 0.16875 | val_0_rmse: 0.44838 | val_1_rmse: 0.44972 |  0:05:22s
epoch 82 | loss: 0.15848 | val_0_rmse: 0.48135 | val_1_rmse: 0.47883 |  0:05:26s
epoch 83 | loss: 0.15652 | val_0_rmse: 0.41276 | val_1_rmse: 0.41157 |  0:05:30s
epoch 84 | loss: 0.14941 | val_0_rmse: 0.38612 | val_1_rmse: 0.3846  |  0:05:34s
epoch 85 | loss: 0.15027 | val_0_rmse: 0.37035 | val_1_rmse: 0.37037 |  0:05:38s
epoch 86 | loss: 0.15467 | val_0_rmse: 0.40604 | val_1_rmse: 0.40576 |  0:05:42s
epoch 87 | loss: 0.18052 | val_0_rmse: 0.4059  | val_1_rmse: 0.4078  |  0:05:46s
epoch 88 | loss: 0.1731  | val_0_rmse: 0.42086 | val_1_rmse: 0.41981 |  0:05:50s
epoch 89 | loss: 0.15612 | val_0_rmse: 0.37725 | val_1_rmse: 0.37809 |  0:05:54s
epoch 90 | loss: 0.16422 | val_0_rmse: 0.45189 | val_1_rmse: 0.45313 |  0:05:58s
epoch 91 | loss: 0.16168 | val_0_rmse: 0.37879 | val_1_rmse: 0.37838 |  0:06:02s
epoch 92 | loss: 0.14859 | val_0_rmse: 0.42981 | val_1_rmse: 0.42909 |  0:06:05s
epoch 93 | loss: 0.16469 | val_0_rmse: 0.40675 | val_1_rmse: 0.40742 |  0:06:09s
epoch 94 | loss: 0.15467 | val_0_rmse: 0.37    | val_1_rmse: 0.3694  |  0:06:13s
epoch 95 | loss: 0.16099 | val_0_rmse: 0.373   | val_1_rmse: 0.37416 |  0:06:17s
epoch 96 | loss: 0.15159 | val_0_rmse: 0.43067 | val_1_rmse: 0.42874 |  0:06:21s
epoch 97 | loss: 0.15941 | val_0_rmse: 0.67214 | val_1_rmse: 0.67909 |  0:06:25s
epoch 98 | loss: 0.14644 | val_0_rmse: 0.38932 | val_1_rmse: 0.38715 |  0:06:29s
epoch 99 | loss: 0.18473 | val_0_rmse: 0.37662 | val_1_rmse: 0.37563 |  0:06:33s
epoch 100| loss: 0.15206 | val_0_rmse: 0.39425 | val_1_rmse: 0.39322 |  0:06:37s
epoch 101| loss: 0.16057 | val_0_rmse: 0.41315 | val_1_rmse: 0.41115 |  0:06:41s
epoch 102| loss: 0.16569 | val_0_rmse: 0.36917 | val_1_rmse: 0.36995 |  0:06:45s
epoch 103| loss: 0.15203 | val_0_rmse: 0.42097 | val_1_rmse: 0.41999 |  0:06:49s
epoch 104| loss: 0.15582 | val_0_rmse: 0.38103 | val_1_rmse: 0.3821  |  0:06:53s
epoch 105| loss: 0.14997 | val_0_rmse: 0.60599 | val_1_rmse: 0.61228 |  0:06:57s
epoch 106| loss: 0.14641 | val_0_rmse: 0.40459 | val_1_rmse: 0.41299 |  0:07:01s
epoch 107| loss: 0.15078 | val_0_rmse: 0.41394 | val_1_rmse: 0.41711 |  0:07:05s
epoch 108| loss: 0.14831 | val_0_rmse: 0.40231 | val_1_rmse: 0.39992 |  0:07:08s
epoch 109| loss: 0.14693 | val_0_rmse: 0.36556 | val_1_rmse: 0.36484 |  0:07:12s
epoch 110| loss: 0.15753 | val_0_rmse: 0.38035 | val_1_rmse: 0.37855 |  0:07:16s
epoch 111| loss: 0.15332 | val_0_rmse: 0.45252 | val_1_rmse: 0.45283 |  0:07:20s
epoch 112| loss: 0.15642 | val_0_rmse: 0.48597 | val_1_rmse: 0.48325 |  0:07:24s
epoch 113| loss: 0.16016 | val_0_rmse: 0.42363 | val_1_rmse: 0.42514 |  0:07:28s
epoch 114| loss: 0.19684 | val_0_rmse: 0.37    | val_1_rmse: 0.36987 |  0:07:32s
epoch 115| loss: 0.14267 | val_0_rmse: 0.5366  | val_1_rmse: 0.53404 |  0:07:36s
epoch 116| loss: 0.15248 | val_0_rmse: 0.41914 | val_1_rmse: 0.41576 |  0:07:40s
epoch 117| loss: 0.14971 | val_0_rmse: 0.37799 | val_1_rmse: 0.37756 |  0:07:44s
epoch 118| loss: 0.14649 | val_0_rmse: 0.55652 | val_1_rmse: 0.55407 |  0:07:48s
epoch 119| loss: 0.1482  | val_0_rmse: 0.37545 | val_1_rmse: 0.37547 |  0:07:52s
epoch 120| loss: 0.14818 | val_0_rmse: 0.38077 | val_1_rmse: 0.37845 |  0:07:56s
epoch 121| loss: 0.14975 | val_0_rmse: 0.44113 | val_1_rmse: 0.44355 |  0:08:00s
epoch 122| loss: 0.15538 | val_0_rmse: 0.50791 | val_1_rmse: 0.5136  |  0:08:04s
epoch 123| loss: 0.14967 | val_0_rmse: 0.38926 | val_1_rmse: 0.38885 |  0:08:08s
epoch 124| loss: 0.14958 | val_0_rmse: 0.37098 | val_1_rmse: 0.37166 |  0:08:11s
epoch 125| loss: 0.16795 | val_0_rmse: 0.48999 | val_1_rmse: 0.4948  |  0:08:15s
epoch 126| loss: 0.15243 | val_0_rmse: 0.40057 | val_1_rmse: 0.40139 |  0:08:19s
epoch 127| loss: 0.15445 | val_0_rmse: 0.42267 | val_1_rmse: 0.42411 |  0:08:23s
epoch 128| loss: 0.15107 | val_0_rmse: 0.38394 | val_1_rmse: 0.38591 |  0:08:27s
epoch 129| loss: 0.1587  | val_0_rmse: 0.39172 | val_1_rmse: 0.39263 |  0:08:31s
epoch 130| loss: 0.15673 | val_0_rmse: 0.40847 | val_1_rmse: 0.40676 |  0:08:35s
epoch 131| loss: 0.14929 | val_0_rmse: 0.38755 | val_1_rmse: 0.38764 |  0:08:39s
epoch 132| loss: 0.14827 | val_0_rmse: 0.38034 | val_1_rmse: 0.38169 |  0:08:43s
epoch 133| loss: 0.14805 | val_0_rmse: 0.3893  | val_1_rmse: 0.3876  |  0:08:47s
epoch 134| loss: 0.15446 | val_0_rmse: 0.37551 | val_1_rmse: 0.37469 |  0:08:51s
epoch 135| loss: 0.15696 | val_0_rmse: 0.43974 | val_1_rmse: 0.44161 |  0:08:55s
epoch 136| loss: 0.145   | val_0_rmse: 0.37984 | val_1_rmse: 0.3784  |  0:08:59s
epoch 137| loss: 0.14517 | val_0_rmse: 0.3925  | val_1_rmse: 0.39305 |  0:09:03s
epoch 138| loss: 0.16525 | val_0_rmse: 0.41853 | val_1_rmse: 0.41804 |  0:09:07s
epoch 139| loss: 0.15334 | val_0_rmse: 0.36808 | val_1_rmse: 0.36735 |  0:09:11s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.36484
Best weights from best epoch are automatically used!
ended training at: 18:55:32
Feature importance:
[('Area', 0.12090703619474515), ('Baths', 0.15842342358278888), ('Beds', 0.18531579049862612), ('Latitude', 0.3288037245318433), ('Longitude', 0.08149642460919299), ('Month', 0.03793979217556575), ('Year', 0.08711380840723783)]
Mean squared error is of 956282168.7949135
Mean absolute error:20476.04257313728
MAPE:0.29457137541537826
R2 score:0.721323682238644
------------------------------------------------------------------
Normalization used is Log Transformation
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:16:45
epoch 0  | loss: 3.53832 | val_0_rmse: 0.86212 | val_1_rmse: 0.86768 |  0:00:18s
epoch 1  | loss: 0.29156 | val_0_rmse: 0.5315  | val_1_rmse: 0.53412 |  0:00:36s
epoch 2  | loss: 0.27439 | val_0_rmse: 0.54113 | val_1_rmse: 0.54439 |  0:00:54s
epoch 3  | loss: 0.25961 | val_0_rmse: 0.56626 | val_1_rmse: 0.56814 |  0:01:13s
epoch 4  | loss: 0.27292 | val_0_rmse: 0.46809 | val_1_rmse: 0.47039 |  0:01:31s
epoch 5  | loss: 0.24579 | val_0_rmse: 0.5259  | val_1_rmse: 0.52748 |  0:01:49s
epoch 6  | loss: 0.23379 | val_0_rmse: 0.51775 | val_1_rmse: 0.5208  |  0:02:08s
epoch 7  | loss: 0.25106 | val_0_rmse: 0.47108 | val_1_rmse: 0.47186 |  0:02:26s
epoch 8  | loss: 0.24604 | val_0_rmse: 0.50725 | val_1_rmse: 0.50916 |  0:02:44s
epoch 9  | loss: 0.23811 | val_0_rmse: 0.47789 | val_1_rmse: 0.48046 |  0:03:03s
epoch 10 | loss: 0.24331 | val_0_rmse: 0.4889  | val_1_rmse: 0.49174 |  0:03:21s
epoch 11 | loss: 0.24608 | val_0_rmse: 0.5308  | val_1_rmse: 0.53508 |  0:03:39s
epoch 12 | loss: 0.23244 | val_0_rmse: 0.50712 | val_1_rmse: 0.50951 |  0:03:58s
epoch 13 | loss: 0.23491 | val_0_rmse: 0.51532 | val_1_rmse: 0.51823 |  0:04:16s
epoch 14 | loss: 0.2327  | val_0_rmse: 0.60993 | val_1_rmse: 0.61298 |  0:04:34s
epoch 15 | loss: 0.23796 | val_0_rmse: 0.45892 | val_1_rmse: 0.46122 |  0:04:53s
epoch 16 | loss: 0.23995 | val_0_rmse: 0.49991 | val_1_rmse: 0.50148 |  0:05:11s
epoch 17 | loss: 0.21933 | val_0_rmse: 0.92003 | val_1_rmse: 0.9208  |  0:05:29s
epoch 18 | loss: 0.21648 | val_0_rmse: 0.52392 | val_1_rmse: 0.52543 |  0:05:47s
epoch 19 | loss: 0.21979 | val_0_rmse: 0.45309 | val_1_rmse: 0.45508 |  0:06:06s
epoch 20 | loss: 0.21376 | val_0_rmse: 0.53344 | val_1_rmse: 0.53679 |  0:06:24s
epoch 21 | loss: 0.21031 | val_0_rmse: 0.56601 | val_1_rmse: 0.56915 |  0:06:42s
epoch 22 | loss: 0.20687 | val_0_rmse: 0.68931 | val_1_rmse: 0.68927 |  0:07:01s
epoch 23 | loss: 0.2093  | val_0_rmse: 0.52084 | val_1_rmse: 0.52307 |  0:07:19s
epoch 24 | loss: 0.2354  | val_0_rmse: 0.86352 | val_1_rmse: 0.86468 |  0:07:38s
epoch 25 | loss: 0.21105 | val_0_rmse: 0.52206 | val_1_rmse: 0.52489 |  0:07:56s
epoch 26 | loss: 0.2012  | val_0_rmse: 0.71266 | val_1_rmse: 0.71404 |  0:08:14s
epoch 27 | loss: 0.21859 | val_0_rmse: 0.54924 | val_1_rmse: 0.55369 |  0:08:33s
epoch 28 | loss: 0.20781 | val_0_rmse: 0.52241 | val_1_rmse: 0.52634 |  0:08:51s
epoch 29 | loss: 0.19788 | val_0_rmse: 0.813   | val_1_rmse: 0.81203 |  0:09:09s
epoch 30 | loss: 0.19929 | val_0_rmse: 0.58627 | val_1_rmse: 0.58922 |  0:09:28s
epoch 31 | loss: 0.20816 | val_0_rmse: 1.25954 | val_1_rmse: 1.25536 |  0:09:46s
epoch 32 | loss: 0.21102 | val_0_rmse: 0.61702 | val_1_rmse: 0.61886 |  0:10:05s
epoch 33 | loss: 0.20117 | val_0_rmse: 0.49407 | val_1_rmse: 0.49739 |  0:10:23s
epoch 34 | loss: 0.20913 | val_0_rmse: 0.47936 | val_1_rmse: 0.48318 |  0:10:41s
epoch 35 | loss: 0.20871 | val_0_rmse: 0.5061  | val_1_rmse: 0.50879 |  0:11:00s
epoch 36 | loss: 0.20542 | val_0_rmse: 0.4782  | val_1_rmse: 0.4819  |  0:11:18s
epoch 37 | loss: 0.2032  | val_0_rmse: 0.77648 | val_1_rmse: 0.77823 |  0:11:36s
epoch 38 | loss: 0.19949 | val_0_rmse: 0.5675  | val_1_rmse: 0.5704  |  0:11:55s
epoch 39 | loss: 0.20403 | val_0_rmse: 0.50963 | val_1_rmse: 0.51249 |  0:12:13s
epoch 40 | loss: 0.2032  | val_0_rmse: 0.47733 | val_1_rmse: 0.48098 |  0:12:31s
epoch 41 | loss: 0.19547 | val_0_rmse: 0.477   | val_1_rmse: 0.47875 |  0:12:50s
epoch 42 | loss: 0.19496 | val_0_rmse: 0.47868 | val_1_rmse: 0.48147 |  0:13:08s
epoch 43 | loss: 0.20594 | val_0_rmse: 0.69726 | val_1_rmse: 0.7002  |  0:13:27s
epoch 44 | loss: 0.20564 | val_0_rmse: 0.84745 | val_1_rmse: 0.84833 |  0:13:45s
epoch 45 | loss: 0.20106 | val_0_rmse: 0.46449 | val_1_rmse: 0.46648 |  0:14:03s
epoch 46 | loss: 0.19758 | val_0_rmse: 0.46516 | val_1_rmse: 0.46826 |  0:14:22s
epoch 47 | loss: 0.19682 | val_0_rmse: 0.6216  | val_1_rmse: 0.62397 |  0:14:40s
epoch 48 | loss: 0.20342 | val_0_rmse: 0.47253 | val_1_rmse: 0.4754  |  0:14:58s
epoch 49 | loss: 0.21004 | val_0_rmse: 0.7038  | val_1_rmse: 0.70559 |  0:15:17s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.45508
Best weights from best epoch are automatically used!
ended training at: 19:32:07
Feature importance:
[('Area', 0.03450237292709106), ('Baths', 0.17986775821667533), ('Beds', 0.0), ('Latitude', 0.25994302885618925), ('Longitude', 0.4640117819221122), ('Month', 0.06167505807793216), ('Year', 0.0)]
Mean squared error is of 15204056562.957804
Mean absolute error:71053.75134857264
MAPE:0.3585457588244153
R2 score:0.6140412340954889
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:32:10
epoch 0  | loss: 3.55749 | val_0_rmse: 0.77394 | val_1_rmse: 0.77886 |  0:00:18s
epoch 1  | loss: 0.31172 | val_0_rmse: 0.5466  | val_1_rmse: 0.54832 |  0:00:36s
epoch 2  | loss: 0.3211  | val_0_rmse: 0.5374  | val_1_rmse: 0.53664 |  0:00:54s
epoch 3  | loss: 0.30158 | val_0_rmse: 0.53425 | val_1_rmse: 0.53313 |  0:01:13s
epoch 4  | loss: 0.27728 | val_0_rmse: 0.54766 | val_1_rmse: 0.54883 |  0:01:31s
epoch 5  | loss: 0.2617  | val_0_rmse: 0.48405 | val_1_rmse: 0.48588 |  0:01:50s
epoch 6  | loss: 0.25122 | val_0_rmse: 0.53403 | val_1_rmse: 0.53667 |  0:02:08s
epoch 7  | loss: 0.24664 | val_0_rmse: 0.55369 | val_1_rmse: 0.5559  |  0:02:26s
epoch 8  | loss: 0.2406  | val_0_rmse: 0.62264 | val_1_rmse: 0.62461 |  0:02:45s
epoch 9  | loss: 0.23192 | val_0_rmse: 0.55821 | val_1_rmse: 0.5592  |  0:03:03s
epoch 10 | loss: 0.24159 | val_0_rmse: 0.58802 | val_1_rmse: 0.58961 |  0:03:21s
epoch 11 | loss: 0.22737 | val_0_rmse: 0.56227 | val_1_rmse: 0.56399 |  0:03:40s
epoch 12 | loss: 0.23357 | val_0_rmse: 0.46804 | val_1_rmse: 0.46897 |  0:03:58s
epoch 13 | loss: 0.23204 | val_0_rmse: 0.61277 | val_1_rmse: 0.61441 |  0:04:16s
epoch 14 | loss: 0.23175 | val_0_rmse: 0.54181 | val_1_rmse: 0.54326 |  0:04:35s
epoch 15 | loss: 0.22261 | val_0_rmse: 0.54195 | val_1_rmse: 0.54392 |  0:04:53s
epoch 16 | loss: 0.22251 | val_0_rmse: 0.65848 | val_1_rmse: 0.66093 |  0:05:11s
epoch 17 | loss: 0.23762 | val_0_rmse: 0.49657 | val_1_rmse: 0.49749 |  0:05:30s
epoch 18 | loss: 0.23802 | val_0_rmse: 0.71875 | val_1_rmse: 0.72158 |  0:05:48s
epoch 19 | loss: 0.21995 | val_0_rmse: 0.44733 | val_1_rmse: 0.44995 |  0:06:06s
epoch 20 | loss: 0.21399 | val_0_rmse: 0.60992 | val_1_rmse: 0.61066 |  0:06:25s
epoch 21 | loss: 0.22155 | val_0_rmse: 0.50253 | val_1_rmse: 0.50591 |  0:06:43s
epoch 22 | loss: 0.22299 | val_0_rmse: 0.53062 | val_1_rmse: 0.53273 |  0:07:01s
epoch 23 | loss: 0.2134  | val_0_rmse: 0.46977 | val_1_rmse: 0.47255 |  0:07:20s
epoch 24 | loss: 0.22102 | val_0_rmse: 0.49553 | val_1_rmse: 0.49874 |  0:07:38s
epoch 25 | loss: 0.22643 | val_0_rmse: 0.46267 | val_1_rmse: 0.46488 |  0:07:56s
epoch 26 | loss: 0.2256  | val_0_rmse: 0.48941 | val_1_rmse: 0.49303 |  0:08:14s
epoch 27 | loss: 0.20963 | val_0_rmse: 0.63918 | val_1_rmse: 0.6423  |  0:08:33s
epoch 28 | loss: 0.22178 | val_0_rmse: 0.5361  | val_1_rmse: 0.53876 |  0:08:51s
epoch 29 | loss: 0.21508 | val_0_rmse: 0.47705 | val_1_rmse: 0.47917 |  0:09:09s
epoch 30 | loss: 0.23766 | val_0_rmse: 0.54413 | val_1_rmse: 0.54749 |  0:09:28s
epoch 31 | loss: 0.2372  | val_0_rmse: 0.57336 | val_1_rmse: 0.57477 |  0:09:46s
epoch 32 | loss: 0.21728 | val_0_rmse: 0.62068 | val_1_rmse: 0.62355 |  0:10:05s
epoch 33 | loss: 0.22395 | val_0_rmse: 0.73471 | val_1_rmse: 0.73861 |  0:10:23s
epoch 34 | loss: 0.22563 | val_0_rmse: 1.41977 | val_1_rmse: 1.43783 |  0:10:41s
epoch 35 | loss: 0.22812 | val_0_rmse: 0.50492 | val_1_rmse: 0.50706 |  0:11:00s
epoch 36 | loss: 0.2135  | val_0_rmse: 0.60437 | val_1_rmse: 0.60905 |  0:11:18s
epoch 37 | loss: 0.21412 | val_0_rmse: 0.48511 | val_1_rmse: 0.48842 |  0:11:36s
epoch 38 | loss: 0.20922 | val_0_rmse: 0.55942 | val_1_rmse: 0.56178 |  0:11:54s
epoch 39 | loss: 0.21081 | val_0_rmse: 0.70653 | val_1_rmse: 0.71324 |  0:12:13s
epoch 40 | loss: 0.21529 | val_0_rmse: 0.74336 | val_1_rmse: 0.74675 |  0:12:31s
epoch 41 | loss: 0.21418 | val_0_rmse: 0.60519 | val_1_rmse: 0.60648 |  0:12:49s
epoch 42 | loss: 0.20553 | val_0_rmse: 0.47132 | val_1_rmse: 0.47498 |  0:13:08s
epoch 43 | loss: 0.21093 | val_0_rmse: 0.73149 | val_1_rmse: 0.7341  |  0:13:26s
epoch 44 | loss: 0.22013 | val_0_rmse: 0.6346  | val_1_rmse: 0.63723 |  0:13:44s
epoch 45 | loss: 0.20651 | val_0_rmse: 0.50261 | val_1_rmse: 0.5044  |  0:14:02s
epoch 46 | loss: 0.20703 | val_0_rmse: 0.45454 | val_1_rmse: 0.45808 |  0:14:21s
epoch 47 | loss: 0.22162 | val_0_rmse: 0.53797 | val_1_rmse: 0.53897 |  0:14:39s
epoch 48 | loss: 0.20672 | val_0_rmse: 0.47159 | val_1_rmse: 0.47517 |  0:14:57s
epoch 49 | loss: 0.20731 | val_0_rmse: 0.64957 | val_1_rmse: 0.6528  |  0:15:16s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.44995
Best weights from best epoch are automatically used!
ended training at: 19:47:31
Feature importance:
[('Area', 0.116687757642842), ('Baths', 0.21367766913195943), ('Beds', 0.15795454185056143), ('Latitude', 0.17778701224218554), ('Longitude', 0.25065348824503864), ('Month', 0.08323953088741295), ('Year', 0.0)]
Mean squared error is of 13200908733.893303
Mean absolute error:68839.8493383304
MAPE:0.3850596319607862
R2 score:0.6607194531181761
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:47:33
epoch 0  | loss: 3.46215 | val_0_rmse: 0.59443 | val_1_rmse: 0.59736 |  0:00:18s
epoch 1  | loss: 0.28778 | val_0_rmse: 0.54467 | val_1_rmse: 0.54879 |  0:00:36s
epoch 2  | loss: 0.26794 | val_0_rmse: 0.52549 | val_1_rmse: 0.53043 |  0:00:54s
epoch 3  | loss: 0.26018 | val_0_rmse: 0.62764 | val_1_rmse: 0.63052 |  0:01:13s
epoch 4  | loss: 0.24063 | val_0_rmse: 0.58103 | val_1_rmse: 0.58265 |  0:01:31s
epoch 5  | loss: 0.23946 | val_0_rmse: 0.60227 | val_1_rmse: 0.60467 |  0:01:49s
epoch 6  | loss: 0.23655 | val_0_rmse: 0.57556 | val_1_rmse: 0.57747 |  0:02:07s
epoch 7  | loss: 0.23591 | val_0_rmse: 0.79179 | val_1_rmse: 0.79114 |  0:02:26s
epoch 8  | loss: 0.24039 | val_0_rmse: 0.59106 | val_1_rmse: 0.59485 |  0:02:44s
epoch 9  | loss: 0.23325 | val_0_rmse: 0.55545 | val_1_rmse: 0.55645 |  0:03:02s
epoch 10 | loss: 0.22224 | val_0_rmse: 0.56796 | val_1_rmse: 0.56949 |  0:03:21s
epoch 11 | loss: 0.30787 | val_0_rmse: 0.5938  | val_1_rmse: 0.59606 |  0:03:39s
epoch 12 | loss: 0.23618 | val_0_rmse: 0.55808 | val_1_rmse: 0.55983 |  0:03:57s
epoch 13 | loss: 0.23577 | val_0_rmse: 0.64589 | val_1_rmse: 0.6462  |  0:04:15s
epoch 14 | loss: 0.22936 | val_0_rmse: 0.62028 | val_1_rmse: 0.62116 |  0:04:34s
epoch 15 | loss: 0.22492 | val_0_rmse: 0.59599 | val_1_rmse: 0.59659 |  0:04:52s
epoch 16 | loss: 0.22886 | val_0_rmse: 0.64129 | val_1_rmse: 0.64179 |  0:05:10s
epoch 17 | loss: 0.219   | val_0_rmse: 0.63957 | val_1_rmse: 0.64147 |  0:05:28s
epoch 18 | loss: 0.22887 | val_0_rmse: 0.61637 | val_1_rmse: 0.61801 |  0:05:46s
epoch 19 | loss: 0.22226 | val_0_rmse: 0.75406 | val_1_rmse: 0.75362 |  0:06:05s
epoch 20 | loss: 0.21586 | val_0_rmse: 0.73135 | val_1_rmse: 0.73158 |  0:06:23s
epoch 21 | loss: 0.23943 | val_0_rmse: 0.66687 | val_1_rmse: 0.66705 |  0:06:41s
epoch 22 | loss: 0.22594 | val_0_rmse: 0.57642 | val_1_rmse: 0.57752 |  0:07:00s
epoch 23 | loss: 0.22005 | val_0_rmse: 0.59444 | val_1_rmse: 0.59484 |  0:07:18s
epoch 24 | loss: 0.22043 | val_0_rmse: 0.59279 | val_1_rmse: 0.59416 |  0:07:36s
epoch 25 | loss: 0.21648 | val_0_rmse: 0.61404 | val_1_rmse: 0.61413 |  0:07:54s
epoch 26 | loss: 0.22458 | val_0_rmse: 0.78865 | val_1_rmse: 0.78786 |  0:08:13s
epoch 27 | loss: 0.22405 | val_0_rmse: 0.62513 | val_1_rmse: 0.62564 |  0:08:31s
epoch 28 | loss: 0.21553 | val_0_rmse: 0.60572 | val_1_rmse: 0.60676 |  0:08:49s
epoch 29 | loss: 0.21282 | val_0_rmse: 0.80028 | val_1_rmse: 0.80036 |  0:09:08s
epoch 30 | loss: 0.22105 | val_0_rmse: 0.71031 | val_1_rmse: 0.70996 |  0:09:26s
epoch 31 | loss: 0.22157 | val_0_rmse: 0.93884 | val_1_rmse: 0.937   |  0:09:44s
epoch 32 | loss: 0.21664 | val_0_rmse: 0.65713 | val_1_rmse: 0.65804 |  0:10:03s

Early stopping occured at epoch 32 with best_epoch = 2 and best_val_1_rmse = 0.53043
Best weights from best epoch are automatically used!
ended training at: 19:57:41
Feature importance:
[('Area', 0.21861606097178948), ('Baths', 0.04837712750714554), ('Beds', 0.06595317157325423), ('Latitude', 0.3152154715406059), ('Longitude', 0.32539730748880374), ('Month', 0.003615718489759656), ('Year', 0.022825142428641438)]
Mean squared error is of 16216593723.6219
Mean absolute error:79017.71347130396
MAPE:0.5050925675526371
R2 score:0.5926507117728859
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:57:43
epoch 0  | loss: 3.49533 | val_0_rmse: 1.18171 | val_1_rmse: 1.1849  |  0:00:18s
epoch 1  | loss: 0.27238 | val_0_rmse: 0.64047 | val_1_rmse: 0.64267 |  0:00:36s
epoch 2  | loss: 0.26311 | val_0_rmse: 0.54562 | val_1_rmse: 0.54769 |  0:00:54s
epoch 3  | loss: 0.24216 | val_0_rmse: 0.52247 | val_1_rmse: 0.52442 |  0:01:13s
epoch 4  | loss: 0.2564  | val_0_rmse: 0.52138 | val_1_rmse: 0.52182 |  0:01:31s
epoch 5  | loss: 0.25411 | val_0_rmse: 0.47803 | val_1_rmse: 0.47873 |  0:01:49s
epoch 6  | loss: 0.25865 | val_0_rmse: 0.50407 | val_1_rmse: 0.50466 |  0:02:07s
epoch 7  | loss: 0.24876 | val_0_rmse: 0.51325 | val_1_rmse: 0.51474 |  0:02:26s
epoch 8  | loss: 0.24462 | val_0_rmse: 0.52164 | val_1_rmse: 0.52186 |  0:02:44s
epoch 9  | loss: 0.2404  | val_0_rmse: 0.49396 | val_1_rmse: 0.4951  |  0:03:02s
epoch 10 | loss: 0.23523 | val_0_rmse: 0.47102 | val_1_rmse: 0.4711  |  0:03:20s
epoch 11 | loss: 0.23786 | val_0_rmse: 0.53115 | val_1_rmse: 0.53086 |  0:03:38s
epoch 12 | loss: 0.23276 | val_0_rmse: 0.54021 | val_1_rmse: 0.53985 |  0:03:57s
epoch 13 | loss: 0.23297 | val_0_rmse: 0.48829 | val_1_rmse: 0.48745 |  0:04:15s
epoch 14 | loss: 0.26637 | val_0_rmse: 0.56015 | val_1_rmse: 0.55965 |  0:04:33s
epoch 15 | loss: 0.25558 | val_0_rmse: 0.47012 | val_1_rmse: 0.4708  |  0:04:51s
epoch 16 | loss: 0.26374 | val_0_rmse: 0.5408  | val_1_rmse: 0.54164 |  0:05:10s
epoch 17 | loss: 0.2457  | val_0_rmse: 0.50896 | val_1_rmse: 0.50859 |  0:05:28s
epoch 18 | loss: 0.23634 | val_0_rmse: 0.46643 | val_1_rmse: 0.46745 |  0:05:46s
epoch 19 | loss: 0.27215 | val_0_rmse: 0.68667 | val_1_rmse: 0.68672 |  0:06:05s
epoch 20 | loss: 0.28892 | val_0_rmse: 0.52413 | val_1_rmse: 0.52466 |  0:06:23s
epoch 21 | loss: 0.27397 | val_0_rmse: 0.52681 | val_1_rmse: 0.52721 |  0:06:41s
epoch 22 | loss: 0.26687 | val_0_rmse: 0.5143  | val_1_rmse: 0.51522 |  0:06:59s
epoch 23 | loss: 0.25543 | val_0_rmse: 0.47644 | val_1_rmse: 0.47617 |  0:07:18s
epoch 24 | loss: 0.23429 | val_0_rmse: 0.49121 | val_1_rmse: 0.49167 |  0:07:36s
epoch 25 | loss: 0.25896 | val_0_rmse: 0.50676 | val_1_rmse: 0.50814 |  0:07:54s
epoch 26 | loss: 0.22977 | val_0_rmse: 0.47788 | val_1_rmse: 0.47878 |  0:08:13s
epoch 27 | loss: 0.2272  | val_0_rmse: 0.47457 | val_1_rmse: 0.47567 |  0:08:31s
epoch 28 | loss: 0.22534 | val_0_rmse: 0.56667 | val_1_rmse: 0.5669  |  0:08:49s
epoch 29 | loss: 0.21933 | val_0_rmse: 0.45162 | val_1_rmse: 0.45306 |  0:09:07s
epoch 30 | loss: 0.21989 | val_0_rmse: 0.50695 | val_1_rmse: 0.50749 |  0:09:26s
epoch 31 | loss: 0.24294 | val_0_rmse: 0.50524 | val_1_rmse: 0.50505 |  0:09:44s
epoch 32 | loss: 0.24306 | val_0_rmse: 0.57157 | val_1_rmse: 0.57111 |  0:10:02s
epoch 33 | loss: 0.23537 | val_0_rmse: 0.47085 | val_1_rmse: 0.47066 |  0:10:21s
epoch 34 | loss: 0.2275  | val_0_rmse: 0.52323 | val_1_rmse: 0.52256 |  0:10:39s
epoch 35 | loss: 0.23541 | val_0_rmse: 0.48548 | val_1_rmse: 0.48618 |  0:10:57s
epoch 36 | loss: 0.22062 | val_0_rmse: 0.46249 | val_1_rmse: 0.46229 |  0:11:15s
epoch 37 | loss: 0.22292 | val_0_rmse: 0.47727 | val_1_rmse: 0.47746 |  0:11:34s
epoch 38 | loss: 0.21896 | val_0_rmse: 0.46862 | val_1_rmse: 0.46904 |  0:11:52s
epoch 39 | loss: 0.21752 | val_0_rmse: 0.50665 | val_1_rmse: 0.50669 |  0:12:10s
epoch 40 | loss: 0.21692 | val_0_rmse: 0.47963 | val_1_rmse: 0.48027 |  0:12:29s
epoch 41 | loss: 0.21816 | val_0_rmse: 0.50868 | val_1_rmse: 0.50703 |  0:12:47s
epoch 42 | loss: 0.22329 | val_0_rmse: 0.48159 | val_1_rmse: 0.48206 |  0:13:05s
epoch 43 | loss: 0.22239 | val_0_rmse: 0.47656 | val_1_rmse: 0.47691 |  0:13:24s
epoch 44 | loss: 0.21432 | val_0_rmse: 0.46974 | val_1_rmse: 0.4701  |  0:13:42s
epoch 45 | loss: 0.21563 | val_0_rmse: 0.46248 | val_1_rmse: 0.46295 |  0:14:00s
epoch 46 | loss: 0.21018 | val_0_rmse: 0.51527 | val_1_rmse: 0.5145  |  0:14:19s
epoch 47 | loss: 0.21216 | val_0_rmse: 0.46842 | val_1_rmse: 0.46782 |  0:14:37s
epoch 48 | loss: 0.21993 | val_0_rmse: 0.48057 | val_1_rmse: 0.48016 |  0:14:55s
epoch 49 | loss: 0.21384 | val_0_rmse: 0.52112 | val_1_rmse: 0.52141 |  0:15:14s
epoch 50 | loss: 0.21239 | val_0_rmse: 0.48242 | val_1_rmse: 0.48222 |  0:15:32s
epoch 51 | loss: 0.21183 | val_0_rmse: 0.53706 | val_1_rmse: 0.53893 |  0:15:50s
epoch 52 | loss: 0.20746 | val_0_rmse: 0.62469 | val_1_rmse: 0.62405 |  0:16:08s
epoch 53 | loss: 0.20852 | val_0_rmse: 0.50653 | val_1_rmse: 0.50789 |  0:16:27s
epoch 54 | loss: 0.2129  | val_0_rmse: 0.49253 | val_1_rmse: 0.49345 |  0:16:45s
epoch 55 | loss: 0.20922 | val_0_rmse: 0.56101 | val_1_rmse: 0.56218 |  0:17:03s
epoch 56 | loss: 0.21263 | val_0_rmse: 0.66664 | val_1_rmse: 0.66485 |  0:17:22s
epoch 57 | loss: 0.21201 | val_0_rmse: 0.4675  | val_1_rmse: 0.4684  |  0:17:40s
epoch 58 | loss: 0.20503 | val_0_rmse: 0.46072 | val_1_rmse: 0.46105 |  0:17:58s
epoch 59 | loss: 0.21195 | val_0_rmse: 0.4741  | val_1_rmse: 0.47401 |  0:18:17s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.45306
Best weights from best epoch are automatically used!
ended training at: 20:16:06
Feature importance:
[('Area', 0.05589851225763458), ('Baths', 0.11280537748513113), ('Beds', 0.0), ('Latitude', 0.12627420058541236), ('Longitude', 0.3651305593145951), ('Month', 0.17447674750152692), ('Year', 0.16541460285569995)]
Mean squared error is of 13083099071.302456
Mean absolute error:68672.36927533631
MAPE:0.40500233037393824
R2 score:0.6691910970417203
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:16:07
epoch 0  | loss: 3.46594 | val_0_rmse: 0.58393 | val_1_rmse: 0.58265 |  0:00:18s
epoch 1  | loss: 0.28566 | val_0_rmse: 0.53405 | val_1_rmse: 0.53262 |  0:00:36s
epoch 2  | loss: 0.2673  | val_0_rmse: 0.51406 | val_1_rmse: 0.51292 |  0:00:54s
epoch 3  | loss: 0.28409 | val_0_rmse: 0.54179 | val_1_rmse: 0.53899 |  0:01:13s
epoch 4  | loss: 0.26915 | val_0_rmse: 0.53463 | val_1_rmse: 0.53351 |  0:01:31s
epoch 5  | loss: 0.24512 | val_0_rmse: 0.51036 | val_1_rmse: 0.50744 |  0:01:49s
epoch 6  | loss: 0.26068 | val_0_rmse: 0.49408 | val_1_rmse: 0.4915  |  0:02:08s
epoch 7  | loss: 0.33749 | val_0_rmse: 0.55769 | val_1_rmse: 0.55659 |  0:02:26s
epoch 8  | loss: 0.26813 | val_0_rmse: 0.49697 | val_1_rmse: 0.49368 |  0:02:44s
epoch 9  | loss: 0.25241 | val_0_rmse: 0.5211  | val_1_rmse: 0.51797 |  0:03:02s
epoch 10 | loss: 0.25518 | val_0_rmse: 0.64382 | val_1_rmse: 0.64405 |  0:03:21s
epoch 11 | loss: 0.24661 | val_0_rmse: 0.47081 | val_1_rmse: 0.46909 |  0:03:39s
epoch 12 | loss: 0.23519 | val_0_rmse: 0.48821 | val_1_rmse: 0.48618 |  0:03:57s
epoch 13 | loss: 0.23086 | val_0_rmse: 0.49856 | val_1_rmse: 0.49533 |  0:04:15s
epoch 14 | loss: 0.22263 | val_0_rmse: 0.4767  | val_1_rmse: 0.47503 |  0:04:34s
epoch 15 | loss: 0.23429 | val_0_rmse: 0.46067 | val_1_rmse: 0.45881 |  0:04:52s
epoch 16 | loss: 0.22361 | val_0_rmse: 0.48311 | val_1_rmse: 0.48192 |  0:05:10s
epoch 17 | loss: 0.22756 | val_0_rmse: 0.51754 | val_1_rmse: 0.51626 |  0:05:29s
epoch 18 | loss: 0.24254 | val_0_rmse: 0.59626 | val_1_rmse: 0.5926  |  0:05:47s
epoch 19 | loss: 0.23647 | val_0_rmse: 0.46219 | val_1_rmse: 0.45946 |  0:06:05s
epoch 20 | loss: 0.21599 | val_0_rmse: 0.46261 | val_1_rmse: 0.46099 |  0:06:23s
epoch 21 | loss: 0.2234  | val_0_rmse: 0.49999 | val_1_rmse: 0.49833 |  0:06:42s
epoch 22 | loss: 0.2184  | val_0_rmse: 0.4921  | val_1_rmse: 0.48977 |  0:07:00s
epoch 23 | loss: 0.22072 | val_0_rmse: 0.4686  | val_1_rmse: 0.4666  |  0:07:18s
epoch 24 | loss: 0.21752 | val_0_rmse: 0.47435 | val_1_rmse: 0.47159 |  0:07:37s
epoch 25 | loss: 0.21666 | val_0_rmse: 0.51584 | val_1_rmse: 0.51487 |  0:07:55s
epoch 26 | loss: 0.2158  | val_0_rmse: 0.59244 | val_1_rmse: 0.59305 |  0:08:13s
epoch 27 | loss: 0.2277  | val_0_rmse: 0.47462 | val_1_rmse: 0.47408 |  0:08:32s
epoch 28 | loss: 0.23269 | val_0_rmse: 0.53605 | val_1_rmse: 0.53437 |  0:08:50s
epoch 29 | loss: 0.22928 | val_0_rmse: 0.60167 | val_1_rmse: 0.59942 |  0:09:08s
epoch 30 | loss: 0.22511 | val_0_rmse: 0.47201 | val_1_rmse: 0.4702  |  0:09:27s
epoch 31 | loss: 0.2242  | val_0_rmse: 0.50323 | val_1_rmse: 0.50223 |  0:09:45s
epoch 32 | loss: 0.22467 | val_0_rmse: 0.47935 | val_1_rmse: 0.47759 |  0:10:03s
epoch 33 | loss: 0.22149 | val_0_rmse: 0.46385 | val_1_rmse: 0.46163 |  0:10:22s
epoch 34 | loss: 0.21482 | val_0_rmse: 0.47213 | val_1_rmse: 0.4698  |  0:10:40s
epoch 35 | loss: 0.21966 | val_0_rmse: 0.50501 | val_1_rmse: 0.50159 |  0:10:58s
epoch 36 | loss: 0.29398 | val_0_rmse: 0.55356 | val_1_rmse: 0.55085 |  0:11:17s
epoch 37 | loss: 0.2566  | val_0_rmse: 0.49746 | val_1_rmse: 0.49606 |  0:11:35s
epoch 38 | loss: 0.23929 | val_0_rmse: 0.48424 | val_1_rmse: 0.48278 |  0:11:53s
epoch 39 | loss: 0.23014 | val_0_rmse: 0.53734 | val_1_rmse: 0.53677 |  0:12:11s
epoch 40 | loss: 0.22906 | val_0_rmse: 0.46907 | val_1_rmse: 0.46687 |  0:12:30s
epoch 41 | loss: 0.22062 | val_0_rmse: 0.47832 | val_1_rmse: 0.47614 |  0:12:48s
epoch 42 | loss: 0.22129 | val_0_rmse: 0.56813 | val_1_rmse: 0.56759 |  0:13:06s
epoch 43 | loss: 0.22498 | val_0_rmse: 0.49581 | val_1_rmse: 0.49488 |  0:13:25s
epoch 44 | loss: 0.22219 | val_0_rmse: 0.5372  | val_1_rmse: 0.53492 |  0:13:43s
epoch 45 | loss: 0.2423  | val_0_rmse: 0.55165 | val_1_rmse: 0.55084 |  0:14:01s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.45881
Best weights from best epoch are automatically used!
ended training at: 20:30:14
Feature importance:
[('Area', 0.0), ('Baths', 0.24101320027488052), ('Beds', 0.04106203935566407), ('Latitude', 0.2966264536091584), ('Longitude', 0.40033057730563726), ('Month', 0.020967729454659755), ('Year', 0.0)]
Mean squared error is of 13833203902.606289
Mean absolute error:70628.05058729138
MAPE:0.43017988473895064
R2 score:0.6523866560122105
------------------------------------------------------------------
