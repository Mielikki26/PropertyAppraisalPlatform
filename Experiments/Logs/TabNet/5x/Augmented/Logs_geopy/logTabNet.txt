TabNet Logs:

Saving copy of script...
In this script the datasets were geopy augmented.This is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:12:58
epoch 0  | loss: 1.23059 | val_0_rmse: 0.99238 | val_1_rmse: 0.99859 |  0:00:05s
epoch 1  | loss: 0.87476 | val_0_rmse: 0.9276  | val_1_rmse: 0.93832 |  0:00:07s
epoch 2  | loss: 0.61939 | val_0_rmse: 0.80802 | val_1_rmse: 0.82564 |  0:00:09s
epoch 3  | loss: 0.45283 | val_0_rmse: 0.83366 | val_1_rmse: 0.85279 |  0:00:11s
epoch 4  | loss: 0.37889 | val_0_rmse: 0.79014 | val_1_rmse: 0.80514 |  0:00:13s
epoch 5  | loss: 0.34858 | val_0_rmse: 0.80017 | val_1_rmse: 0.81776 |  0:00:15s
epoch 6  | loss: 0.32529 | val_0_rmse: 0.77108 | val_1_rmse: 0.78766 |  0:00:17s
epoch 7  | loss: 0.31147 | val_0_rmse: 0.75118 | val_1_rmse: 0.7695  |  0:00:19s
epoch 8  | loss: 0.28438 | val_0_rmse: 0.73517 | val_1_rmse: 0.75284 |  0:00:21s
epoch 9  | loss: 0.27229 | val_0_rmse: 0.72258 | val_1_rmse: 0.74156 |  0:00:23s
epoch 10 | loss: 0.27216 | val_0_rmse: 0.69715 | val_1_rmse: 0.71687 |  0:00:24s
epoch 11 | loss: 0.25883 | val_0_rmse: 0.68364 | val_1_rmse: 0.70075 |  0:00:26s
epoch 12 | loss: 0.25834 | val_0_rmse: 0.68534 | val_1_rmse: 0.70395 |  0:00:28s
epoch 13 | loss: 0.24524 | val_0_rmse: 0.64769 | val_1_rmse: 0.67033 |  0:00:30s
epoch 14 | loss: 0.24428 | val_0_rmse: 0.62853 | val_1_rmse: 0.65081 |  0:00:32s
epoch 15 | loss: 0.23841 | val_0_rmse: 0.613   | val_1_rmse: 0.6373  |  0:00:34s
epoch 16 | loss: 0.24294 | val_0_rmse: 0.58185 | val_1_rmse: 0.61059 |  0:00:36s
epoch 17 | loss: 0.24098 | val_0_rmse: 0.58188 | val_1_rmse: 0.60813 |  0:00:38s
epoch 18 | loss: 0.24167 | val_0_rmse: 0.54859 | val_1_rmse: 0.58185 |  0:00:40s
epoch 19 | loss: 0.23408 | val_0_rmse: 0.5258  | val_1_rmse: 0.5618  |  0:00:42s
epoch 20 | loss: 0.23333 | val_0_rmse: 0.51572 | val_1_rmse: 0.55428 |  0:00:44s
epoch 21 | loss: 0.22422 | val_0_rmse: 0.49373 | val_1_rmse: 0.53595 |  0:00:46s
epoch 22 | loss: 0.22625 | val_0_rmse: 0.50283 | val_1_rmse: 0.54492 |  0:00:48s
epoch 23 | loss: 0.22156 | val_0_rmse: 0.52778 | val_1_rmse: 0.5653  |  0:00:50s
epoch 24 | loss: 0.22052 | val_0_rmse: 0.48004 | val_1_rmse: 0.53135 |  0:00:52s
epoch 25 | loss: 0.21549 | val_0_rmse: 0.45878 | val_1_rmse: 0.51204 |  0:00:54s
epoch 26 | loss: 0.2163  | val_0_rmse: 0.45262 | val_1_rmse: 0.50725 |  0:00:55s
epoch 27 | loss: 0.21622 | val_0_rmse: 0.44663 | val_1_rmse: 0.50329 |  0:00:57s
epoch 28 | loss: 0.21242 | val_0_rmse: 0.44775 | val_1_rmse: 0.50977 |  0:00:59s
epoch 29 | loss: 0.2226  | val_0_rmse: 0.44118 | val_1_rmse: 0.50649 |  0:01:01s
epoch 30 | loss: 0.22126 | val_0_rmse: 0.4438  | val_1_rmse: 0.50659 |  0:01:03s
epoch 31 | loss: 0.21074 | val_0_rmse: 0.44992 | val_1_rmse: 0.50891 |  0:01:05s
epoch 32 | loss: 0.21256 | val_0_rmse: 0.43521 | val_1_rmse: 0.49995 |  0:01:07s
epoch 33 | loss: 0.21078 | val_0_rmse: 0.43087 | val_1_rmse: 0.50218 |  0:01:09s
epoch 34 | loss: 0.2056  | val_0_rmse: 0.43255 | val_1_rmse: 0.50121 |  0:01:11s
epoch 35 | loss: 0.20531 | val_0_rmse: 0.42932 | val_1_rmse: 0.50266 |  0:01:13s
epoch 36 | loss: 0.20588 | val_0_rmse: 0.42532 | val_1_rmse: 0.49585 |  0:01:15s
epoch 37 | loss: 0.21181 | val_0_rmse: 0.47604 | val_1_rmse: 0.54155 |  0:01:17s
epoch 38 | loss: 0.2062  | val_0_rmse: 0.42953 | val_1_rmse: 0.51069 |  0:01:19s
epoch 39 | loss: 0.20784 | val_0_rmse: 0.43888 | val_1_rmse: 0.51704 |  0:01:21s
epoch 40 | loss: 0.20208 | val_0_rmse: 0.44805 | val_1_rmse: 0.51755 |  0:01:23s
epoch 41 | loss: 0.2082  | val_0_rmse: 0.42739 | val_1_rmse: 0.5041  |  0:01:24s
epoch 42 | loss: 0.20391 | val_0_rmse: 0.46452 | val_1_rmse: 0.53883 |  0:01:26s
epoch 43 | loss: 0.19916 | val_0_rmse: 0.42308 | val_1_rmse: 0.50575 |  0:01:28s
epoch 44 | loss: 0.19809 | val_0_rmse: 0.41834 | val_1_rmse: 0.4981  |  0:01:30s
epoch 45 | loss: 0.19713 | val_0_rmse: 0.41963 | val_1_rmse: 0.50814 |  0:01:32s
epoch 46 | loss: 0.2076  | val_0_rmse: 0.43472 | val_1_rmse: 0.51847 |  0:01:34s
epoch 47 | loss: 0.20523 | val_0_rmse: 0.42717 | val_1_rmse: 0.5064  |  0:01:36s
epoch 48 | loss: 0.20301 | val_0_rmse: 0.42685 | val_1_rmse: 0.51118 |  0:01:38s
epoch 49 | loss: 0.20167 | val_0_rmse: 0.41791 | val_1_rmse: 0.50352 |  0:01:40s
epoch 50 | loss: 0.19515 | val_0_rmse: 0.42607 | val_1_rmse: 0.51508 |  0:01:42s
epoch 51 | loss: 0.19491 | val_0_rmse: 0.42884 | val_1_rmse: 0.5223  |  0:01:44s
epoch 52 | loss: 0.19293 | val_0_rmse: 0.42532 | val_1_rmse: 0.51816 |  0:01:46s
epoch 53 | loss: 0.19334 | val_0_rmse: 0.4176  | val_1_rmse: 0.51025 |  0:01:48s
epoch 54 | loss: 0.19511 | val_0_rmse: 0.44262 | val_1_rmse: 0.53103 |  0:01:50s
epoch 55 | loss: 0.19492 | val_0_rmse: 0.42459 | val_1_rmse: 0.51106 |  0:01:52s
epoch 56 | loss: 0.18826 | val_0_rmse: 0.41308 | val_1_rmse: 0.50621 |  0:01:54s
epoch 57 | loss: 0.18864 | val_0_rmse: 0.41091 | val_1_rmse: 0.50959 |  0:01:55s
epoch 58 | loss: 0.18257 | val_0_rmse: 0.40674 | val_1_rmse: 0.5094  |  0:01:57s
epoch 59 | loss: 0.18469 | val_0_rmse: 0.40181 | val_1_rmse: 0.50881 |  0:01:59s
epoch 60 | loss: 0.18935 | val_0_rmse: 0.40541 | val_1_rmse: 0.50809 |  0:02:01s
epoch 61 | loss: 0.18772 | val_0_rmse: 0.40208 | val_1_rmse: 0.50852 |  0:02:03s
epoch 62 | loss: 0.18351 | val_0_rmse: 0.40205 | val_1_rmse: 0.51022 |  0:02:05s
epoch 63 | loss: 0.18415 | val_0_rmse: 0.41576 | val_1_rmse: 0.51664 |  0:02:07s
epoch 64 | loss: 0.18715 | val_0_rmse: 0.40745 | val_1_rmse: 0.51452 |  0:02:09s
epoch 65 | loss: 0.17733 | val_0_rmse: 0.40231 | val_1_rmse: 0.51631 |  0:02:11s
epoch 66 | loss: 0.17945 | val_0_rmse: 0.39776 | val_1_rmse: 0.51994 |  0:02:13s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.49585
Best weights from best epoch are automatically used!
ended training at: 21:15:12
Feature importance:
Mean squared error is of 5532094559.1230755
Mean absolute error:49815.18685957409
MAPE:0.16317595054299047
R2 score:0.7598118018055418
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:15:13
epoch 0  | loss: 1.21622 | val_0_rmse: 0.90746 | val_1_rmse: 0.90322 |  0:00:01s
epoch 1  | loss: 0.84739 | val_0_rmse: 0.87487 | val_1_rmse: 0.86654 |  0:00:03s
epoch 2  | loss: 0.73164 | val_0_rmse: 0.88821 | val_1_rmse: 0.88561 |  0:00:05s
epoch 3  | loss: 0.60279 | val_0_rmse: 0.92321 | val_1_rmse: 0.92293 |  0:00:07s
epoch 4  | loss: 0.49461 | val_0_rmse: 0.89372 | val_1_rmse: 0.89545 |  0:00:09s
epoch 5  | loss: 0.4361  | val_0_rmse: 0.89714 | val_1_rmse: 0.89983 |  0:00:11s
epoch 6  | loss: 0.4065  | val_0_rmse: 0.86482 | val_1_rmse: 0.8669  |  0:00:13s
epoch 7  | loss: 0.39647 | val_0_rmse: 0.87099 | val_1_rmse: 0.87055 |  0:00:15s
epoch 8  | loss: 0.37947 | val_0_rmse: 0.8111  | val_1_rmse: 0.81387 |  0:00:17s
epoch 9  | loss: 0.35369 | val_0_rmse: 0.77428 | val_1_rmse: 0.78072 |  0:00:19s
epoch 10 | loss: 0.34958 | val_0_rmse: 0.79453 | val_1_rmse: 0.80425 |  0:00:21s
epoch 11 | loss: 0.34196 | val_0_rmse: 0.72588 | val_1_rmse: 0.73611 |  0:00:23s
epoch 12 | loss: 0.33113 | val_0_rmse: 0.72466 | val_1_rmse: 0.73738 |  0:00:25s
epoch 13 | loss: 0.32525 | val_0_rmse: 0.6872  | val_1_rmse: 0.69362 |  0:00:27s
epoch 14 | loss: 0.31684 | val_0_rmse: 0.68166 | val_1_rmse: 0.68865 |  0:00:29s
epoch 15 | loss: 0.31253 | val_0_rmse: 0.65744 | val_1_rmse: 0.67091 |  0:00:31s
epoch 16 | loss: 0.30573 | val_0_rmse: 0.6215  | val_1_rmse: 0.63224 |  0:00:32s
epoch 17 | loss: 0.30401 | val_0_rmse: 0.59781 | val_1_rmse: 0.61157 |  0:00:34s
epoch 18 | loss: 0.29259 | val_0_rmse: 0.60108 | val_1_rmse: 0.62349 |  0:00:36s
epoch 19 | loss: 0.28234 | val_0_rmse: 0.54911 | val_1_rmse: 0.57245 |  0:00:38s
epoch 20 | loss: 0.27137 | val_0_rmse: 0.5441  | val_1_rmse: 0.56651 |  0:00:40s
epoch 21 | loss: 0.26619 | val_0_rmse: 0.56697 | val_1_rmse: 0.59877 |  0:00:42s
epoch 22 | loss: 0.27264 | val_0_rmse: 0.51143 | val_1_rmse: 0.53763 |  0:00:44s
epoch 23 | loss: 0.26348 | val_0_rmse: 0.50183 | val_1_rmse: 0.52898 |  0:00:46s
epoch 24 | loss: 0.2613  | val_0_rmse: 0.49181 | val_1_rmse: 0.52301 |  0:00:48s
epoch 25 | loss: 0.25231 | val_0_rmse: 0.48596 | val_1_rmse: 0.51928 |  0:00:50s
epoch 26 | loss: 0.25158 | val_0_rmse: 0.49014 | val_1_rmse: 0.52602 |  0:00:52s
epoch 27 | loss: 0.24925 | val_0_rmse: 0.47825 | val_1_rmse: 0.51382 |  0:00:54s
epoch 28 | loss: 0.24302 | val_0_rmse: 0.47283 | val_1_rmse: 0.50986 |  0:00:56s
epoch 29 | loss: 0.24095 | val_0_rmse: 0.47287 | val_1_rmse: 0.51189 |  0:00:58s
epoch 30 | loss: 0.24228 | val_0_rmse: 0.47692 | val_1_rmse: 0.51118 |  0:01:00s
epoch 31 | loss: 0.24601 | val_0_rmse: 0.47507 | val_1_rmse: 0.51697 |  0:01:02s
epoch 32 | loss: 0.24382 | val_0_rmse: 0.46271 | val_1_rmse: 0.505   |  0:01:04s
epoch 33 | loss: 0.23962 | val_0_rmse: 0.47102 | val_1_rmse: 0.51137 |  0:01:06s
epoch 34 | loss: 0.23677 | val_0_rmse: 0.45744 | val_1_rmse: 0.5071  |  0:01:08s
epoch 35 | loss: 0.23753 | val_0_rmse: 0.47641 | val_1_rmse: 0.52245 |  0:01:09s
epoch 36 | loss: 0.23309 | val_0_rmse: 0.46738 | val_1_rmse: 0.51176 |  0:01:11s
epoch 37 | loss: 0.23221 | val_0_rmse: 0.46504 | val_1_rmse: 0.51435 |  0:01:13s
epoch 38 | loss: 0.2297  | val_0_rmse: 0.4612  | val_1_rmse: 0.511   |  0:01:15s
epoch 39 | loss: 0.22625 | val_0_rmse: 0.46346 | val_1_rmse: 0.51693 |  0:01:17s
epoch 40 | loss: 0.225   | val_0_rmse: 0.45625 | val_1_rmse: 0.51365 |  0:01:19s
epoch 41 | loss: 0.22383 | val_0_rmse: 0.45188 | val_1_rmse: 0.50834 |  0:01:21s
epoch 42 | loss: 0.22444 | val_0_rmse: 0.48157 | val_1_rmse: 0.52884 |  0:01:23s
epoch 43 | loss: 0.22972 | val_0_rmse: 0.45044 | val_1_rmse: 0.50824 |  0:01:25s
epoch 44 | loss: 0.2251  | val_0_rmse: 0.44485 | val_1_rmse: 0.50442 |  0:01:27s
epoch 45 | loss: 0.21592 | val_0_rmse: 0.45276 | val_1_rmse: 0.51301 |  0:01:29s
epoch 46 | loss: 0.22098 | val_0_rmse: 0.45272 | val_1_rmse: 0.50679 |  0:01:31s
epoch 47 | loss: 0.21908 | val_0_rmse: 0.44208 | val_1_rmse: 0.50678 |  0:01:33s
epoch 48 | loss: 0.21381 | val_0_rmse: 0.43919 | val_1_rmse: 0.5054  |  0:01:35s
epoch 49 | loss: 0.2139  | val_0_rmse: 0.44161 | val_1_rmse: 0.50556 |  0:01:37s
epoch 50 | loss: 0.21338 | val_0_rmse: 0.44425 | val_1_rmse: 0.51032 |  0:01:39s
epoch 51 | loss: 0.21491 | val_0_rmse: 0.44725 | val_1_rmse: 0.5136  |  0:01:40s
epoch 52 | loss: 0.20886 | val_0_rmse: 0.43432 | val_1_rmse: 0.50202 |  0:01:42s
epoch 53 | loss: 0.20594 | val_0_rmse: 0.43576 | val_1_rmse: 0.503   |  0:01:44s
epoch 54 | loss: 0.20918 | val_0_rmse: 0.43939 | val_1_rmse: 0.51062 |  0:01:46s
epoch 55 | loss: 0.20528 | val_0_rmse: 0.43033 | val_1_rmse: 0.50452 |  0:01:48s
epoch 56 | loss: 0.20271 | val_0_rmse: 0.4299  | val_1_rmse: 0.50347 |  0:01:50s
epoch 57 | loss: 0.20482 | val_0_rmse: 0.43731 | val_1_rmse: 0.5091  |  0:01:52s
epoch 58 | loss: 0.20355 | val_0_rmse: 0.45218 | val_1_rmse: 0.52194 |  0:01:54s
epoch 59 | loss: 0.21035 | val_0_rmse: 0.42861 | val_1_rmse: 0.50482 |  0:01:56s
epoch 60 | loss: 0.20197 | val_0_rmse: 0.43053 | val_1_rmse: 0.5095  |  0:01:58s
epoch 61 | loss: 0.20161 | val_0_rmse: 0.42588 | val_1_rmse: 0.50397 |  0:02:00s
epoch 62 | loss: 0.20011 | val_0_rmse: 0.44236 | val_1_rmse: 0.51633 |  0:02:02s
epoch 63 | loss: 0.20152 | val_0_rmse: 0.63317 | val_1_rmse: 0.51155 |  0:02:04s
epoch 64 | loss: 0.20074 | val_0_rmse: 0.42341 | val_1_rmse: 0.50234 |  0:02:06s
epoch 65 | loss: 0.19984 | val_0_rmse: 0.42662 | val_1_rmse: 0.50664 |  0:02:08s
epoch 66 | loss: 0.20015 | val_0_rmse: 0.42183 | val_1_rmse: 0.50495 |  0:02:09s
epoch 67 | loss: 0.19994 | val_0_rmse: 0.42481 | val_1_rmse: 0.50836 |  0:02:11s
epoch 68 | loss: 0.19824 | val_0_rmse: 0.42283 | val_1_rmse: 0.50479 |  0:02:13s
epoch 69 | loss: 0.20135 | val_0_rmse: 0.42177 | val_1_rmse: 0.50336 |  0:02:15s
epoch 70 | loss: 0.1967  | val_0_rmse: 0.42109 | val_1_rmse: 0.51269 |  0:02:17s
epoch 71 | loss: 0.19784 | val_0_rmse: 0.42973 | val_1_rmse: 0.51407 |  0:02:19s
epoch 72 | loss: 0.19661 | val_0_rmse: 0.4317  | val_1_rmse: 0.5298  |  0:02:21s
epoch 73 | loss: 0.1977  | val_0_rmse: 0.4248  | val_1_rmse: 0.52277 |  0:02:23s
epoch 74 | loss: 0.19743 | val_0_rmse: 0.43136 | val_1_rmse: 0.51978 |  0:02:25s
epoch 75 | loss: 0.19777 | val_0_rmse: 0.41798 | val_1_rmse: 0.50909 |  0:02:27s
epoch 76 | loss: 0.19261 | val_0_rmse: 0.41941 | val_1_rmse: 0.50972 |  0:02:29s
epoch 77 | loss: 0.19182 | val_0_rmse: 0.42412 | val_1_rmse: 0.51406 |  0:02:31s
epoch 78 | loss: 0.19109 | val_0_rmse: 0.418   | val_1_rmse: 0.51117 |  0:02:33s
epoch 79 | loss: 0.19872 | val_0_rmse: 0.41351 | val_1_rmse: 0.50734 |  0:02:35s
epoch 80 | loss: 0.1932  | val_0_rmse: 0.41686 | val_1_rmse: 0.51585 |  0:02:37s
epoch 81 | loss: 0.19552 | val_0_rmse: 0.41785 | val_1_rmse: 0.52241 |  0:02:39s
epoch 82 | loss: 0.19174 | val_0_rmse: 0.4109  | val_1_rmse: 0.5106  |  0:02:40s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.50202
Best weights from best epoch are automatically used!
ended training at: 21:17:55
Feature importance:
Mean squared error is of 5319957185.025056
Mean absolute error:48935.64600199338
MAPE:0.15904944623446599
R2 score:0.765785763173771
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:17:55
epoch 0  | loss: 1.25378 | val_0_rmse: 0.9969  | val_1_rmse: 0.98239 |  0:00:01s
epoch 1  | loss: 0.89733 | val_0_rmse: 1.06446 | val_1_rmse: 1.04481 |  0:00:03s
epoch 2  | loss: 0.63524 | val_0_rmse: 0.91921 | val_1_rmse: 0.9082  |  0:00:05s
epoch 3  | loss: 0.45646 | val_0_rmse: 0.86704 | val_1_rmse: 0.85853 |  0:00:07s
epoch 4  | loss: 0.40408 | val_0_rmse: 0.81157 | val_1_rmse: 0.80404 |  0:00:09s
epoch 5  | loss: 0.35716 | val_0_rmse: 0.78793 | val_1_rmse: 0.78378 |  0:00:11s
epoch 6  | loss: 0.33747 | val_0_rmse: 0.78179 | val_1_rmse: 0.78287 |  0:00:13s
epoch 7  | loss: 0.31887 | val_0_rmse: 0.75973 | val_1_rmse: 0.76118 |  0:00:15s
epoch 8  | loss: 0.29733 | val_0_rmse: 0.7533  | val_1_rmse: 0.75074 |  0:00:17s
epoch 9  | loss: 0.28932 | val_0_rmse: 0.72503 | val_1_rmse: 0.72348 |  0:00:19s
epoch 10 | loss: 0.28017 | val_0_rmse: 0.73269 | val_1_rmse: 0.73314 |  0:00:21s
epoch 11 | loss: 0.27597 | val_0_rmse: 0.70829 | val_1_rmse: 0.71162 |  0:00:23s
epoch 12 | loss: 0.26557 | val_0_rmse: 0.67151 | val_1_rmse: 0.67234 |  0:00:25s
epoch 13 | loss: 0.25642 | val_0_rmse: 0.63744 | val_1_rmse: 0.64577 |  0:00:27s
epoch 14 | loss: 0.25481 | val_0_rmse: 0.64549 | val_1_rmse: 0.65462 |  0:00:29s
epoch 15 | loss: 0.2529  | val_0_rmse: 0.63889 | val_1_rmse: 0.64742 |  0:00:31s
epoch 16 | loss: 0.25064 | val_0_rmse: 0.59804 | val_1_rmse: 0.61399 |  0:00:33s
epoch 17 | loss: 0.24063 | val_0_rmse: 0.58857 | val_1_rmse: 0.60367 |  0:00:34s
epoch 18 | loss: 0.2464  | val_0_rmse: 0.58626 | val_1_rmse: 0.59899 |  0:00:36s
epoch 19 | loss: 0.25117 | val_0_rmse: 0.64969 | val_1_rmse: 0.65828 |  0:00:38s
epoch 20 | loss: 0.24707 | val_0_rmse: 0.5501  | val_1_rmse: 0.57256 |  0:00:40s
epoch 21 | loss: 0.24419 | val_0_rmse: 0.52863 | val_1_rmse: 0.55572 |  0:00:42s
epoch 22 | loss: 0.24829 | val_0_rmse: 0.52525 | val_1_rmse: 0.55327 |  0:00:44s
epoch 23 | loss: 0.24127 | val_0_rmse: 0.49239 | val_1_rmse: 0.5244  |  0:00:46s
epoch 24 | loss: 0.23628 | val_0_rmse: 0.50828 | val_1_rmse: 0.54057 |  0:00:48s
epoch 25 | loss: 0.23235 | val_0_rmse: 0.49593 | val_1_rmse: 0.53075 |  0:00:50s
epoch 26 | loss: 0.22534 | val_0_rmse: 0.46381 | val_1_rmse: 0.50903 |  0:00:52s
epoch 27 | loss: 0.22427 | val_0_rmse: 0.46786 | val_1_rmse: 0.51144 |  0:00:54s
epoch 28 | loss: 0.22031 | val_0_rmse: 0.45291 | val_1_rmse: 0.49888 |  0:00:56s
epoch 29 | loss: 0.21703 | val_0_rmse: 0.44402 | val_1_rmse: 0.49605 |  0:00:58s
epoch 30 | loss: 0.21748 | val_0_rmse: 0.44328 | val_1_rmse: 0.49439 |  0:01:00s
epoch 31 | loss: 0.21299 | val_0_rmse: 0.43788 | val_1_rmse: 0.49707 |  0:01:02s
epoch 32 | loss: 0.21576 | val_0_rmse: 0.44803 | val_1_rmse: 0.50389 |  0:01:04s
epoch 33 | loss: 0.2157  | val_0_rmse: 0.43917 | val_1_rmse: 0.49775 |  0:01:06s
epoch 34 | loss: 0.21372 | val_0_rmse: 0.44027 | val_1_rmse: 0.50092 |  0:01:07s
epoch 35 | loss: 0.21003 | val_0_rmse: 0.43582 | val_1_rmse: 0.50064 |  0:01:09s
epoch 36 | loss: 0.20937 | val_0_rmse: 0.43444 | val_1_rmse: 0.49628 |  0:01:11s
epoch 37 | loss: 0.20372 | val_0_rmse: 0.43199 | val_1_rmse: 0.49833 |  0:01:13s
epoch 38 | loss: 0.20431 | val_0_rmse: 0.43349 | val_1_rmse: 0.50423 |  0:01:15s
epoch 39 | loss: 0.20555 | val_0_rmse: 0.42456 | val_1_rmse: 0.5031  |  0:01:17s
epoch 40 | loss: 0.20368 | val_0_rmse: 0.43406 | val_1_rmse: 0.52255 |  0:01:19s
epoch 41 | loss: 0.20086 | val_0_rmse: 0.44312 | val_1_rmse: 0.54425 |  0:01:21s
epoch 42 | loss: 0.20424 | val_0_rmse: 0.4251  | val_1_rmse: 0.50396 |  0:01:23s
epoch 43 | loss: 0.20285 | val_0_rmse: 0.42758 | val_1_rmse: 0.51114 |  0:01:25s
epoch 44 | loss: 0.20573 | val_0_rmse: 0.45311 | val_1_rmse: 0.52439 |  0:01:27s
epoch 45 | loss: 0.20128 | val_0_rmse: 0.42821 | val_1_rmse: 0.50261 |  0:01:29s
epoch 46 | loss: 0.19661 | val_0_rmse: 0.4288  | val_1_rmse: 0.5134  |  0:01:31s
epoch 47 | loss: 0.1981  | val_0_rmse: 0.42004 | val_1_rmse: 0.50507 |  0:01:33s
epoch 48 | loss: 0.19862 | val_0_rmse: 0.43181 | val_1_rmse: 0.53004 |  0:01:35s
epoch 49 | loss: 0.19593 | val_0_rmse: 0.43304 | val_1_rmse: 0.51443 |  0:01:37s
epoch 50 | loss: 0.19882 | val_0_rmse: 0.43058 | val_1_rmse: 0.52284 |  0:01:39s
epoch 51 | loss: 0.19464 | val_0_rmse: 0.42655 | val_1_rmse: 0.53802 |  0:01:40s
epoch 52 | loss: 0.19225 | val_0_rmse: 0.42157 | val_1_rmse: 0.5297  |  0:01:42s
epoch 53 | loss: 0.19246 | val_0_rmse: 0.43572 | val_1_rmse: 0.54547 |  0:01:44s
epoch 54 | loss: 0.19426 | val_0_rmse: 0.43247 | val_1_rmse: 0.54782 |  0:01:46s
epoch 55 | loss: 0.19272 | val_0_rmse: 0.43841 | val_1_rmse: 0.54338 |  0:01:48s
epoch 56 | loss: 0.18848 | val_0_rmse: 0.43906 | val_1_rmse: 0.53941 |  0:01:50s
epoch 57 | loss: 0.18785 | val_0_rmse: 0.42351 | val_1_rmse: 0.52894 |  0:01:52s
epoch 58 | loss: 0.18689 | val_0_rmse: 0.45379 | val_1_rmse: 0.55595 |  0:01:54s
epoch 59 | loss: 0.18871 | val_0_rmse: 0.44189 | val_1_rmse: 0.56514 |  0:01:56s
epoch 60 | loss: 0.18517 | val_0_rmse: 0.40364 | val_1_rmse: 0.53498 |  0:01:58s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.49439
Best weights from best epoch are automatically used!
ended training at: 21:19:54
Feature importance:
Mean squared error is of 5466365784.873924
Mean absolute error:49091.00712693759
MAPE:0.15163887298526882
R2 score:0.7658665655663941
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:19:54
epoch 0  | loss: 1.20802 | val_0_rmse: 0.95386 | val_1_rmse: 0.95036 |  0:00:01s
epoch 1  | loss: 0.80162 | val_0_rmse: 0.90185 | val_1_rmse: 0.90237 |  0:00:03s
epoch 2  | loss: 0.67946 | val_0_rmse: 0.98202 | val_1_rmse: 0.98027 |  0:00:05s
epoch 3  | loss: 0.52841 | val_0_rmse: 0.94584 | val_1_rmse: 0.94834 |  0:00:07s
epoch 4  | loss: 0.43594 | val_0_rmse: 0.94518 | val_1_rmse: 0.94226 |  0:00:09s
epoch 5  | loss: 0.40478 | val_0_rmse: 0.87246 | val_1_rmse: 0.87071 |  0:00:11s
epoch 6  | loss: 0.3844  | val_0_rmse: 0.8848  | val_1_rmse: 0.88511 |  0:00:13s
epoch 7  | loss: 0.36583 | val_0_rmse: 0.83442 | val_1_rmse: 0.83486 |  0:00:15s
epoch 8  | loss: 0.3563  | val_0_rmse: 0.75685 | val_1_rmse: 0.76654 |  0:00:17s
epoch 9  | loss: 0.35016 | val_0_rmse: 0.8123  | val_1_rmse: 0.82439 |  0:00:19s
epoch 10 | loss: 0.33987 | val_0_rmse: 0.76283 | val_1_rmse: 0.77539 |  0:00:21s
epoch 11 | loss: 0.33829 | val_0_rmse: 0.75403 | val_1_rmse: 0.7645  |  0:00:23s
epoch 12 | loss: 0.32758 | val_0_rmse: 0.72359 | val_1_rmse: 0.73293 |  0:00:25s
epoch 13 | loss: 0.31851 | val_0_rmse: 0.69918 | val_1_rmse: 0.7107  |  0:00:27s
epoch 14 | loss: 0.30628 | val_0_rmse: 0.67314 | val_1_rmse: 0.68401 |  0:00:29s
epoch 15 | loss: 0.29474 | val_0_rmse: 0.64193 | val_1_rmse: 0.65111 |  0:00:31s
epoch 16 | loss: 0.28582 | val_0_rmse: 0.60894 | val_1_rmse: 0.61708 |  0:00:33s
epoch 17 | loss: 0.2803  | val_0_rmse: 0.60899 | val_1_rmse: 0.61725 |  0:00:35s
epoch 18 | loss: 0.28488 | val_0_rmse: 0.60586 | val_1_rmse: 0.61503 |  0:00:37s
epoch 19 | loss: 0.27988 | val_0_rmse: 0.58043 | val_1_rmse: 0.58904 |  0:00:39s
epoch 20 | loss: 0.27189 | val_0_rmse: 0.56422 | val_1_rmse: 0.57755 |  0:00:40s
epoch 21 | loss: 0.26151 | val_0_rmse: 0.53974 | val_1_rmse: 0.5586  |  0:00:42s
epoch 22 | loss: 0.26046 | val_0_rmse: 0.51343 | val_1_rmse: 0.53223 |  0:00:44s
epoch 23 | loss: 0.25221 | val_0_rmse: 0.50151 | val_1_rmse: 0.52223 |  0:00:46s
epoch 24 | loss: 0.25118 | val_0_rmse: 0.49453 | val_1_rmse: 0.52228 |  0:00:48s
epoch 25 | loss: 0.2457  | val_0_rmse: 0.49233 | val_1_rmse: 0.51797 |  0:00:50s
epoch 26 | loss: 0.25188 | val_0_rmse: 0.49009 | val_1_rmse: 0.51246 |  0:00:52s
epoch 27 | loss: 0.25312 | val_0_rmse: 0.48944 | val_1_rmse: 0.52025 |  0:00:54s
epoch 28 | loss: 0.24993 | val_0_rmse: 0.48911 | val_1_rmse: 0.521   |  0:00:56s
epoch 29 | loss: 0.24719 | val_0_rmse: 0.47364 | val_1_rmse: 0.50502 |  0:00:58s
epoch 30 | loss: 0.24453 | val_0_rmse: 0.47842 | val_1_rmse: 0.51453 |  0:01:00s
epoch 31 | loss: 0.23783 | val_0_rmse: 0.4636  | val_1_rmse: 0.50245 |  0:01:02s
epoch 32 | loss: 0.23673 | val_0_rmse: 0.46129 | val_1_rmse: 0.49938 |  0:01:05s
epoch 33 | loss: 0.24014 | val_0_rmse: 0.45732 | val_1_rmse: 0.49307 |  0:01:07s
epoch 34 | loss: 0.23417 | val_0_rmse: 0.46651 | val_1_rmse: 0.50647 |  0:01:09s
epoch 35 | loss: 0.23558 | val_0_rmse: 0.45138 | val_1_rmse: 0.48993 |  0:01:11s
epoch 36 | loss: 0.22637 | val_0_rmse: 0.45478 | val_1_rmse: 0.48984 |  0:01:13s
epoch 37 | loss: 0.22558 | val_0_rmse: 0.44891 | val_1_rmse: 0.4904  |  0:01:15s
epoch 38 | loss: 0.22299 | val_0_rmse: 0.45357 | val_1_rmse: 0.49791 |  0:01:17s
epoch 39 | loss: 0.2262  | val_0_rmse: 0.45123 | val_1_rmse: 0.49571 |  0:01:19s
epoch 40 | loss: 0.22701 | val_0_rmse: 0.44936 | val_1_rmse: 0.49351 |  0:01:21s
epoch 41 | loss: 0.22152 | val_0_rmse: 0.45504 | val_1_rmse: 0.50215 |  0:01:23s
epoch 42 | loss: 0.22346 | val_0_rmse: 0.44788 | val_1_rmse: 0.49906 |  0:01:25s
epoch 43 | loss: 0.22687 | val_0_rmse: 0.45593 | val_1_rmse: 0.50182 |  0:01:27s
epoch 44 | loss: 0.22638 | val_0_rmse: 0.45055 | val_1_rmse: 0.49899 |  0:01:29s
epoch 45 | loss: 0.21901 | val_0_rmse: 0.44519 | val_1_rmse: 0.49925 |  0:01:31s
epoch 46 | loss: 0.21791 | val_0_rmse: 0.44487 | val_1_rmse: 0.49063 |  0:01:33s
epoch 47 | loss: 0.21704 | val_0_rmse: 0.4416  | val_1_rmse: 0.4963  |  0:01:35s
epoch 48 | loss: 0.21757 | val_0_rmse: 0.44611 | val_1_rmse: 0.49895 |  0:01:37s
epoch 49 | loss: 0.21377 | val_0_rmse: 0.44083 | val_1_rmse: 0.49333 |  0:01:39s
epoch 50 | loss: 0.21132 | val_0_rmse: 0.47904 | val_1_rmse: 0.53826 |  0:01:41s
epoch 51 | loss: 0.22971 | val_0_rmse: 0.45859 | val_1_rmse: 0.50638 |  0:01:43s
epoch 52 | loss: 0.23459 | val_0_rmse: 0.46197 | val_1_rmse: 0.51424 |  0:01:44s
epoch 53 | loss: 0.23213 | val_0_rmse: 0.45123 | val_1_rmse: 0.49961 |  0:01:46s
epoch 54 | loss: 0.22159 | val_0_rmse: 0.453   | val_1_rmse: 0.5093  |  0:01:48s
epoch 55 | loss: 0.21993 | val_0_rmse: 0.44496 | val_1_rmse: 0.50028 |  0:01:50s
epoch 56 | loss: 0.215   | val_0_rmse: 0.44194 | val_1_rmse: 0.5006  |  0:01:52s
epoch 57 | loss: 0.21638 | val_0_rmse: 0.44195 | val_1_rmse: 0.50413 |  0:01:54s
epoch 58 | loss: 0.20792 | val_0_rmse: 0.43282 | val_1_rmse: 0.49392 |  0:01:56s
epoch 59 | loss: 0.20428 | val_0_rmse: 0.43062 | val_1_rmse: 0.49522 |  0:01:58s
epoch 60 | loss: 0.20579 | val_0_rmse: 0.43039 | val_1_rmse: 0.48789 |  0:02:00s
epoch 61 | loss: 0.20372 | val_0_rmse: 0.43977 | val_1_rmse: 0.49933 |  0:02:02s
epoch 62 | loss: 0.20545 | val_0_rmse: 0.42344 | val_1_rmse: 0.49112 |  0:02:04s
epoch 63 | loss: 0.19932 | val_0_rmse: 0.42735 | val_1_rmse: 0.49922 |  0:02:06s
epoch 64 | loss: 0.20468 | val_0_rmse: 0.43139 | val_1_rmse: 0.50345 |  0:02:08s
epoch 65 | loss: 0.20669 | val_0_rmse: 0.42796 | val_1_rmse: 0.50298 |  0:02:10s
epoch 66 | loss: 0.20144 | val_0_rmse: 0.42027 | val_1_rmse: 0.4966  |  0:02:12s
epoch 67 | loss: 0.19545 | val_0_rmse: 0.419   | val_1_rmse: 0.49476 |  0:02:14s
epoch 68 | loss: 0.18927 | val_0_rmse: 0.41622 | val_1_rmse: 0.4973  |  0:02:16s
epoch 69 | loss: 0.1923  | val_0_rmse: 0.4175  | val_1_rmse: 0.4924  |  0:02:18s
epoch 70 | loss: 0.19165 | val_0_rmse: 0.41343 | val_1_rmse: 0.49079 |  0:02:19s
epoch 71 | loss: 0.18928 | val_0_rmse: 0.41773 | val_1_rmse: 0.49594 |  0:02:21s
epoch 72 | loss: 0.19232 | val_0_rmse: 0.41717 | val_1_rmse: 0.49329 |  0:02:23s
epoch 73 | loss: 0.18882 | val_0_rmse: 0.42348 | val_1_rmse: 0.50252 |  0:02:25s
epoch 74 | loss: 0.19153 | val_0_rmse: 0.42858 | val_1_rmse: 0.49559 |  0:02:27s
epoch 75 | loss: 0.1878  | val_0_rmse: 0.41458 | val_1_rmse: 0.4924  |  0:02:29s
epoch 76 | loss: 0.18682 | val_0_rmse: 0.40764 | val_1_rmse: 0.49994 |  0:02:31s
epoch 77 | loss: 0.18433 | val_0_rmse: 0.43195 | val_1_rmse: 0.51588 |  0:02:33s
epoch 78 | loss: 0.18599 | val_0_rmse: 0.40766 | val_1_rmse: 0.5035  |  0:02:35s
epoch 79 | loss: 0.18322 | val_0_rmse: 0.40705 | val_1_rmse: 0.49638 |  0:02:37s
epoch 80 | loss: 0.19225 | val_0_rmse: 0.41047 | val_1_rmse: 0.49679 |  0:02:39s
epoch 81 | loss: 0.19027 | val_0_rmse: 0.40555 | val_1_rmse: 0.50006 |  0:02:41s
epoch 82 | loss: 0.1845  | val_0_rmse: 0.4083  | val_1_rmse: 0.49751 |  0:02:43s
epoch 83 | loss: 0.18134 | val_0_rmse: 0.40282 | val_1_rmse: 0.49757 |  0:02:45s
epoch 84 | loss: 0.18442 | val_0_rmse: 0.4105  | val_1_rmse: 0.50434 |  0:02:47s
epoch 85 | loss: 0.1888  | val_0_rmse: 0.43264 | val_1_rmse: 0.53212 |  0:02:49s
epoch 86 | loss: 0.18406 | val_0_rmse: 0.40868 | val_1_rmse: 0.51075 |  0:02:51s
epoch 87 | loss: 0.18547 | val_0_rmse: 0.40171 | val_1_rmse: 0.50467 |  0:02:53s
epoch 88 | loss: 0.18114 | val_0_rmse: 0.40047 | val_1_rmse: 0.50371 |  0:02:54s
epoch 89 | loss: 0.17917 | val_0_rmse: 0.40894 | val_1_rmse: 0.50904 |  0:02:56s
epoch 90 | loss: 0.18123 | val_0_rmse: 0.39768 | val_1_rmse: 0.49858 |  0:02:58s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.48789
Best weights from best epoch are automatically used!
ended training at: 21:22:54
Feature importance:
Mean squared error is of 5565589008.341583
Mean absolute error:49833.86149916689
MAPE:0.15892138472659612
R2 score:0.7502841819685216
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:22:54
epoch 0  | loss: 1.19985 | val_0_rmse: 0.99386 | val_1_rmse: 0.9803  |  0:00:01s
epoch 1  | loss: 0.82103 | val_0_rmse: 0.87967 | val_1_rmse: 0.86875 |  0:00:03s
epoch 2  | loss: 0.58295 | val_0_rmse: 0.84144 | val_1_rmse: 0.83168 |  0:00:05s
epoch 3  | loss: 0.45127 | val_0_rmse: 0.84689 | val_1_rmse: 0.83567 |  0:00:07s
epoch 4  | loss: 0.4031  | val_0_rmse: 0.82928 | val_1_rmse: 0.81312 |  0:00:09s
epoch 5  | loss: 0.39242 | val_0_rmse: 0.74588 | val_1_rmse: 0.73272 |  0:00:11s
epoch 6  | loss: 0.37919 | val_0_rmse: 0.78557 | val_1_rmse: 0.76764 |  0:00:13s
epoch 7  | loss: 0.35805 | val_0_rmse: 0.77032 | val_1_rmse: 0.75375 |  0:00:15s
epoch 8  | loss: 0.35727 | val_0_rmse: 0.74383 | val_1_rmse: 0.7284  |  0:00:17s
epoch 9  | loss: 0.34175 | val_0_rmse: 0.73296 | val_1_rmse: 0.71671 |  0:00:19s
epoch 10 | loss: 0.33937 | val_0_rmse: 0.72031 | val_1_rmse: 0.70732 |  0:00:21s
epoch 11 | loss: 0.33867 | val_0_rmse: 0.71425 | val_1_rmse: 0.69914 |  0:00:23s
epoch 12 | loss: 0.32166 | val_0_rmse: 0.68974 | val_1_rmse: 0.67878 |  0:00:25s
epoch 13 | loss: 0.3183  | val_0_rmse: 0.68486 | val_1_rmse: 0.67248 |  0:00:27s
epoch 14 | loss: 0.30836 | val_0_rmse: 0.64279 | val_1_rmse: 0.62586 |  0:00:29s
epoch 15 | loss: 0.30933 | val_0_rmse: 0.63345 | val_1_rmse: 0.6215  |  0:00:31s
epoch 16 | loss: 0.30706 | val_0_rmse: 0.61189 | val_1_rmse: 0.6016  |  0:00:33s
epoch 17 | loss: 0.30034 | val_0_rmse: 0.63068 | val_1_rmse: 0.62343 |  0:00:35s
epoch 18 | loss: 0.32013 | val_0_rmse: 0.58533 | val_1_rmse: 0.58037 |  0:00:37s
epoch 19 | loss: 0.29386 | val_0_rmse: 0.56046 | val_1_rmse: 0.5598  |  0:00:38s
epoch 20 | loss: 0.29344 | val_0_rmse: 0.56033 | val_1_rmse: 0.56126 |  0:00:40s
epoch 21 | loss: 0.28989 | val_0_rmse: 0.54264 | val_1_rmse: 0.54316 |  0:00:42s
epoch 22 | loss: 0.2871  | val_0_rmse: 0.52508 | val_1_rmse: 0.53381 |  0:00:44s
epoch 23 | loss: 0.27306 | val_0_rmse: 0.53091 | val_1_rmse: 0.53656 |  0:00:46s
epoch 24 | loss: 0.26882 | val_0_rmse: 0.50073 | val_1_rmse: 0.50786 |  0:00:48s
epoch 25 | loss: 0.26495 | val_0_rmse: 0.50139 | val_1_rmse: 0.50942 |  0:00:50s
epoch 26 | loss: 0.25987 | val_0_rmse: 0.49675 | val_1_rmse: 0.50859 |  0:00:52s
epoch 27 | loss: 0.25878 | val_0_rmse: 0.48877 | val_1_rmse: 0.50218 |  0:00:54s
epoch 28 | loss: 0.26704 | val_0_rmse: 0.50382 | val_1_rmse: 0.51168 |  0:00:56s
epoch 29 | loss: 0.26284 | val_0_rmse: 0.48956 | val_1_rmse: 0.50213 |  0:00:58s
epoch 30 | loss: 0.25578 | val_0_rmse: 0.48986 | val_1_rmse: 0.50052 |  0:01:00s
epoch 31 | loss: 0.25413 | val_0_rmse: 0.4882  | val_1_rmse: 0.49731 |  0:01:02s
epoch 32 | loss: 0.25291 | val_0_rmse: 0.48768 | val_1_rmse: 0.50382 |  0:01:04s
epoch 33 | loss: 0.25063 | val_0_rmse: 0.49944 | val_1_rmse: 0.51404 |  0:01:06s
epoch 34 | loss: 0.2505  | val_0_rmse: 0.48461 | val_1_rmse: 0.49842 |  0:01:08s
epoch 35 | loss: 0.25123 | val_0_rmse: 0.47617 | val_1_rmse: 0.49184 |  0:01:10s
epoch 36 | loss: 0.24277 | val_0_rmse: 0.47545 | val_1_rmse: 0.49589 |  0:01:12s
epoch 37 | loss: 0.2444  | val_0_rmse: 0.48075 | val_1_rmse: 0.50099 |  0:01:14s
epoch 38 | loss: 0.24238 | val_0_rmse: 0.47324 | val_1_rmse: 0.49839 |  0:01:15s
epoch 39 | loss: 0.24117 | val_0_rmse: 0.47859 | val_1_rmse: 0.49715 |  0:01:17s
epoch 40 | loss: 0.24051 | val_0_rmse: 0.48469 | val_1_rmse: 0.49832 |  0:01:19s
epoch 41 | loss: 0.24263 | val_0_rmse: 0.47243 | val_1_rmse: 0.49377 |  0:01:21s
epoch 42 | loss: 0.23656 | val_0_rmse: 0.46701 | val_1_rmse: 0.49372 |  0:01:23s
epoch 43 | loss: 0.23585 | val_0_rmse: 0.47201 | val_1_rmse: 0.48725 |  0:01:25s
epoch 44 | loss: 0.2329  | val_0_rmse: 0.46797 | val_1_rmse: 0.48966 |  0:01:27s
epoch 45 | loss: 0.23164 | val_0_rmse: 0.46073 | val_1_rmse: 0.48382 |  0:01:29s
epoch 46 | loss: 0.23168 | val_0_rmse: 0.46283 | val_1_rmse: 0.48759 |  0:01:31s
epoch 47 | loss: 0.22911 | val_0_rmse: 0.46195 | val_1_rmse: 0.48534 |  0:01:33s
epoch 48 | loss: 0.22812 | val_0_rmse: 0.45718 | val_1_rmse: 0.48586 |  0:01:35s
epoch 49 | loss: 0.22446 | val_0_rmse: 0.46726 | val_1_rmse: 0.49654 |  0:01:37s
epoch 50 | loss: 0.24388 | val_0_rmse: 0.47151 | val_1_rmse: 0.50093 |  0:01:39s
epoch 51 | loss: 0.23649 | val_0_rmse: 0.46278 | val_1_rmse: 0.49003 |  0:01:41s
epoch 52 | loss: 0.22881 | val_0_rmse: 0.45407 | val_1_rmse: 0.47985 |  0:01:43s
epoch 53 | loss: 0.22609 | val_0_rmse: 0.45317 | val_1_rmse: 0.48308 |  0:01:45s
epoch 54 | loss: 0.22475 | val_0_rmse: 0.46232 | val_1_rmse: 0.49293 |  0:01:47s
epoch 55 | loss: 0.22462 | val_0_rmse: 0.4639  | val_1_rmse: 0.48924 |  0:01:49s
epoch 56 | loss: 0.22412 | val_0_rmse: 0.47451 | val_1_rmse: 0.50208 |  0:01:51s
epoch 57 | loss: 0.22201 | val_0_rmse: 0.45236 | val_1_rmse: 0.48678 |  0:01:52s
epoch 58 | loss: 0.22096 | val_0_rmse: 0.45295 | val_1_rmse: 0.48148 |  0:01:54s
epoch 59 | loss: 0.22001 | val_0_rmse: 0.4473  | val_1_rmse: 0.47938 |  0:01:56s
epoch 60 | loss: 0.21861 | val_0_rmse: 0.4474  | val_1_rmse: 0.47888 |  0:01:58s
epoch 61 | loss: 0.21713 | val_0_rmse: 0.44764 | val_1_rmse: 0.48276 |  0:02:00s
epoch 62 | loss: 0.21521 | val_0_rmse: 0.44476 | val_1_rmse: 0.47984 |  0:02:02s
epoch 63 | loss: 0.21234 | val_0_rmse: 0.43699 | val_1_rmse: 0.47807 |  0:02:04s
epoch 64 | loss: 0.20789 | val_0_rmse: 0.43747 | val_1_rmse: 0.48035 |  0:02:06s
epoch 65 | loss: 0.20789 | val_0_rmse: 0.4363  | val_1_rmse: 0.4792  |  0:02:08s
epoch 66 | loss: 0.20739 | val_0_rmse: 0.44014 | val_1_rmse: 0.48882 |  0:02:10s
epoch 67 | loss: 0.21018 | val_0_rmse: 0.44115 | val_1_rmse: 0.48625 |  0:02:12s
epoch 68 | loss: 0.2044  | val_0_rmse: 0.43621 | val_1_rmse: 0.48187 |  0:02:14s
epoch 69 | loss: 0.20778 | val_0_rmse: 0.43553 | val_1_rmse: 0.48485 |  0:02:16s
epoch 70 | loss: 0.20921 | val_0_rmse: 0.45284 | val_1_rmse: 0.49404 |  0:02:18s
epoch 71 | loss: 0.20793 | val_0_rmse: 0.43716 | val_1_rmse: 0.47921 |  0:02:20s
epoch 72 | loss: 0.20923 | val_0_rmse: 0.44427 | val_1_rmse: 0.48956 |  0:02:22s
epoch 73 | loss: 0.20198 | val_0_rmse: 0.43474 | val_1_rmse: 0.47864 |  0:02:24s
epoch 74 | loss: 0.19808 | val_0_rmse: 0.42628 | val_1_rmse: 0.48101 |  0:02:26s
epoch 75 | loss: 0.19883 | val_0_rmse: 0.42759 | val_1_rmse: 0.48317 |  0:02:28s
epoch 76 | loss: 0.19647 | val_0_rmse: 0.43089 | val_1_rmse: 0.4847  |  0:02:30s
epoch 77 | loss: 0.19931 | val_0_rmse: 0.43042 | val_1_rmse: 0.49402 |  0:02:31s
epoch 78 | loss: 0.20009 | val_0_rmse: 0.43262 | val_1_rmse: 0.48508 |  0:02:33s
epoch 79 | loss: 0.19877 | val_0_rmse: 0.42447 | val_1_rmse: 0.483   |  0:02:35s
epoch 80 | loss: 0.20076 | val_0_rmse: 0.43264 | val_1_rmse: 0.48849 |  0:02:37s
epoch 81 | loss: 0.19711 | val_0_rmse: 0.43757 | val_1_rmse: 0.49332 |  0:02:39s
epoch 82 | loss: 0.20272 | val_0_rmse: 0.44378 | val_1_rmse: 0.49433 |  0:02:41s
epoch 83 | loss: 0.23371 | val_0_rmse: 0.45988 | val_1_rmse: 0.50065 |  0:02:43s
epoch 84 | loss: 0.21979 | val_0_rmse: 0.44859 | val_1_rmse: 0.49428 |  0:02:45s
epoch 85 | loss: 0.21185 | val_0_rmse: 0.43757 | val_1_rmse: 0.48174 |  0:02:47s
epoch 86 | loss: 0.20066 | val_0_rmse: 0.43155 | val_1_rmse: 0.48796 |  0:02:49s
epoch 87 | loss: 0.2045  | val_0_rmse: 0.43169 | val_1_rmse: 0.48196 |  0:02:51s
epoch 88 | loss: 0.19953 | val_0_rmse: 0.42771 | val_1_rmse: 0.48442 |  0:02:53s
epoch 89 | loss: 0.2015  | val_0_rmse: 0.42885 | val_1_rmse: 0.49151 |  0:02:55s
epoch 90 | loss: 0.20032 | val_0_rmse: 0.42602 | val_1_rmse: 0.48419 |  0:02:57s
epoch 91 | loss: 0.20038 | val_0_rmse: 0.42602 | val_1_rmse: 0.48904 |  0:02:59s
epoch 92 | loss: 0.19301 | val_0_rmse: 0.42062 | val_1_rmse: 0.48503 |  0:03:01s
epoch 93 | loss: 0.19452 | val_0_rmse: 0.4318  | val_1_rmse: 0.49495 |  0:03:03s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.47807
Best weights from best epoch are automatically used!
ended training at: 21:25:58
Feature importance:
Mean squared error is of 5490379563.553677
Mean absolute error:50169.19368333507
MAPE:0.16234437802951482
R2 score:0.7537076519355481
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:26:02
epoch 0  | loss: 0.70361 | val_0_rmse: 0.81156 | val_1_rmse: 0.80124 |  0:00:11s
epoch 1  | loss: 0.3743  | val_0_rmse: 0.72781 | val_1_rmse: 0.72035 |  0:00:22s
epoch 2  | loss: 0.32561 | val_0_rmse: 0.65279 | val_1_rmse: 0.6483  |  0:00:34s
epoch 3  | loss: 0.30322 | val_0_rmse: 0.62505 | val_1_rmse: 0.62211 |  0:00:45s
epoch 4  | loss: 0.29395 | val_0_rmse: 0.59213 | val_1_rmse: 0.59084 |  0:00:56s
epoch 5  | loss: 0.28245 | val_0_rmse: 0.56495 | val_1_rmse: 0.56866 |  0:01:08s
epoch 6  | loss: 0.29416 | val_0_rmse: 0.53652 | val_1_rmse: 0.54462 |  0:01:19s
epoch 7  | loss: 0.28562 | val_0_rmse: 0.53879 | val_1_rmse: 0.54735 |  0:01:31s
epoch 8  | loss: 0.28087 | val_0_rmse: 0.51074 | val_1_rmse: 0.52676 |  0:01:42s
epoch 9  | loss: 0.27317 | val_0_rmse: 0.50369 | val_1_rmse: 0.52163 |  0:01:53s
epoch 10 | loss: 0.27123 | val_0_rmse: 0.50651 | val_1_rmse: 0.5365  |  0:02:05s
epoch 11 | loss: 0.28025 | val_0_rmse: 0.52103 | val_1_rmse: 0.56506 |  0:02:16s
epoch 12 | loss: 0.27308 | val_0_rmse: 0.54899 | val_1_rmse: 0.66564 |  0:02:27s
epoch 13 | loss: 0.27691 | val_0_rmse: 0.51234 | val_1_rmse: 0.60707 |  0:02:39s
epoch 14 | loss: 0.26731 | val_0_rmse: 0.522   | val_1_rmse: 0.61923 |  0:02:50s
epoch 15 | loss: 0.26571 | val_0_rmse: 0.505   | val_1_rmse: 0.55016 |  0:03:02s
epoch 16 | loss: 0.26682 | val_0_rmse: 0.57012 | val_1_rmse: 0.60219 |  0:03:13s
epoch 17 | loss: 0.26367 | val_0_rmse: 0.51665 | val_1_rmse: 0.54655 |  0:03:24s
epoch 18 | loss: 0.25733 | val_0_rmse: 0.495   | val_1_rmse: 0.52624 |  0:03:36s
epoch 19 | loss: 0.25278 | val_0_rmse: 0.53088 | val_1_rmse: 0.58007 |  0:03:47s
epoch 20 | loss: 0.24926 | val_0_rmse: 0.50328 | val_1_rmse: 0.55028 |  0:03:58s
epoch 21 | loss: 0.24596 | val_0_rmse: 0.5125  | val_1_rmse: 0.56512 |  0:04:10s
epoch 22 | loss: 0.24521 | val_0_rmse: 0.50215 | val_1_rmse: 0.56207 |  0:04:21s
epoch 23 | loss: 0.24504 | val_0_rmse: 0.54718 | val_1_rmse: 0.59453 |  0:04:32s
epoch 24 | loss: 0.24373 | val_0_rmse: 0.50083 | val_1_rmse: 0.56216 |  0:04:44s
epoch 25 | loss: 0.23954 | val_0_rmse: 0.49508 | val_1_rmse: 0.54814 |  0:04:55s
epoch 26 | loss: 0.23743 | val_0_rmse: 0.54588 | val_1_rmse: 0.6077  |  0:05:07s
epoch 27 | loss: 0.23588 | val_0_rmse: 0.5008  | val_1_rmse: 0.5643  |  0:05:18s
epoch 28 | loss: 0.23159 | val_0_rmse: 0.48474 | val_1_rmse: 0.54749 |  0:05:29s
epoch 29 | loss: 0.23393 | val_0_rmse: 0.48295 | val_1_rmse: 0.53618 |  0:05:41s
epoch 30 | loss: 0.23337 | val_0_rmse: 0.48873 | val_1_rmse: 0.53449 |  0:05:52s
epoch 31 | loss: 0.23005 | val_0_rmse: 0.47627 | val_1_rmse: 0.53288 |  0:06:04s
epoch 32 | loss: 0.23124 | val_0_rmse: 0.48021 | val_1_rmse: 0.52906 |  0:06:15s
epoch 33 | loss: 0.23231 | val_0_rmse: 0.49108 | val_1_rmse: 0.54354 |  0:06:26s
epoch 34 | loss: 0.22869 | val_0_rmse: 0.4715  | val_1_rmse: 0.52202 |  0:06:38s
epoch 35 | loss: 0.22851 | val_0_rmse: 0.47413 | val_1_rmse: 0.52752 |  0:06:49s
epoch 36 | loss: 0.23121 | val_0_rmse: 0.47591 | val_1_rmse: 0.52973 |  0:07:00s
epoch 37 | loss: 0.22733 | val_0_rmse: 0.46956 | val_1_rmse: 0.52389 |  0:07:12s
epoch 38 | loss: 0.2253  | val_0_rmse: 0.47083 | val_1_rmse: 0.52392 |  0:07:23s
epoch 39 | loss: 0.22262 | val_0_rmse: 0.47509 | val_1_rmse: 0.53241 |  0:07:35s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.52163
Best weights from best epoch are automatically used!
ended training at: 21:33:42
Feature importance:
Mean squared error is of 1823477361.1873555
Mean absolute error:30812.20741744645
MAPE:0.29703359923171857
R2 score:0.7281883336526197
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:33:45
epoch 0  | loss: 0.84138 | val_0_rmse: 0.80557 | val_1_rmse: 0.80574 |  0:00:11s
epoch 1  | loss: 0.37545 | val_0_rmse: 0.69899 | val_1_rmse: 0.70192 |  0:00:22s
epoch 2  | loss: 0.31272 | val_0_rmse: 0.65298 | val_1_rmse: 0.65805 |  0:00:34s
epoch 3  | loss: 0.28419 | val_0_rmse: 0.61371 | val_1_rmse: 0.61951 |  0:00:45s
epoch 4  | loss: 0.27144 | val_0_rmse: 0.57501 | val_1_rmse: 0.58412 |  0:00:57s
epoch 5  | loss: 0.26267 | val_0_rmse: 0.54649 | val_1_rmse: 0.55858 |  0:01:08s
epoch 6  | loss: 0.25814 | val_0_rmse: 0.51636 | val_1_rmse: 0.53334 |  0:01:19s
epoch 7  | loss: 0.25439 | val_0_rmse: 0.51691 | val_1_rmse: 0.53568 |  0:01:31s
epoch 8  | loss: 0.25118 | val_0_rmse: 0.48505 | val_1_rmse: 0.50944 |  0:01:42s
epoch 9  | loss: 0.24863 | val_0_rmse: 0.50128 | val_1_rmse: 0.52684 |  0:01:54s
epoch 10 | loss: 0.24463 | val_0_rmse: 0.49649 | val_1_rmse: 0.52504 |  0:02:05s
epoch 11 | loss: 0.24263 | val_0_rmse: 0.48784 | val_1_rmse: 0.51804 |  0:02:16s
epoch 12 | loss: 0.23969 | val_0_rmse: 0.47623 | val_1_rmse: 0.51276 |  0:02:28s
epoch 13 | loss: 0.23685 | val_0_rmse: 0.49064 | val_1_rmse: 0.51932 |  0:02:40s
epoch 14 | loss: 0.23805 | val_0_rmse: 0.48769 | val_1_rmse: 0.52324 |  0:02:52s
epoch 15 | loss: 0.23542 | val_0_rmse: 0.48233 | val_1_rmse: 0.51927 |  0:03:03s
epoch 16 | loss: 0.23044 | val_0_rmse: 0.48431 | val_1_rmse: 0.51683 |  0:03:15s
epoch 17 | loss: 0.23037 | val_0_rmse: 0.53123 | val_1_rmse: 0.66415 |  0:03:26s
epoch 18 | loss: 0.22687 | val_0_rmse: 0.48269 | val_1_rmse: 0.52092 |  0:03:38s
epoch 19 | loss: 0.22751 | val_0_rmse: 0.56287 | val_1_rmse: 0.60487 |  0:03:49s
epoch 20 | loss: 0.22471 | val_0_rmse: 0.51309 | val_1_rmse: 0.58337 |  0:04:01s
epoch 21 | loss: 0.2254  | val_0_rmse: 0.48921 | val_1_rmse: 0.53087 |  0:04:12s
epoch 22 | loss: 0.22468 | val_0_rmse: 0.57286 | val_1_rmse: 0.60486 |  0:04:24s
epoch 23 | loss: 0.22195 | val_0_rmse: 0.49776 | val_1_rmse: 0.55533 |  0:04:35s
epoch 24 | loss: 0.21845 | val_0_rmse: 0.49253 | val_1_rmse: 0.55264 |  0:04:47s
epoch 25 | loss: 0.21657 | val_0_rmse: 0.5018  | val_1_rmse: 0.58266 |  0:04:58s
epoch 26 | loss: 0.22264 | val_0_rmse: 0.47547 | val_1_rmse: 0.52689 |  0:05:10s
epoch 27 | loss: 0.21746 | val_0_rmse: 0.46572 | val_1_rmse: 0.52007 |  0:05:21s
epoch 28 | loss: 0.21464 | val_0_rmse: 0.47023 | val_1_rmse: 0.53517 |  0:05:33s
epoch 29 | loss: 0.21191 | val_0_rmse: 0.46979 | val_1_rmse: 0.52849 |  0:05:44s
epoch 30 | loss: 0.21144 | val_0_rmse: 0.4641  | val_1_rmse: 0.52852 |  0:05:56s
epoch 31 | loss: 0.2122  | val_0_rmse: 0.48774 | val_1_rmse: 0.54845 |  0:06:07s
epoch 32 | loss: 0.20951 | val_0_rmse: 0.46939 | val_1_rmse: 0.5419  |  0:06:19s
epoch 33 | loss: 0.20793 | val_0_rmse: 0.54329 | val_1_rmse: 0.5831  |  0:06:30s
epoch 34 | loss: 0.20663 | val_0_rmse: 0.47169 | val_1_rmse: 0.55104 |  0:06:42s
epoch 35 | loss: 0.20458 | val_0_rmse: 0.45479 | val_1_rmse: 0.52057 |  0:06:53s
epoch 36 | loss: 0.20569 | val_0_rmse: 0.50486 | val_1_rmse: 0.56975 |  0:07:05s
epoch 37 | loss: 0.20351 | val_0_rmse: 0.503   | val_1_rmse: 0.57692 |  0:07:16s
epoch 38 | loss: 0.20267 | val_0_rmse: 0.47538 | val_1_rmse: 0.54263 |  0:07:28s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.50944
Best weights from best epoch are automatically used!
ended training at: 21:41:19
Feature importance:
Mean squared error is of 1790424929.8529334
Mean absolute error:30203.025433062914
MAPE:0.2781718312821709
R2 score:0.734714509335481
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:41:22
epoch 0  | loss: 0.77025 | val_0_rmse: 0.82948 | val_1_rmse: 0.83378 |  0:00:11s
epoch 1  | loss: 0.39267 | val_0_rmse: 0.71702 | val_1_rmse: 0.72219 |  0:00:22s
epoch 2  | loss: 0.33129 | val_0_rmse: 0.67288 | val_1_rmse: 0.68112 |  0:00:34s
epoch 3  | loss: 0.32023 | val_0_rmse: 0.61755 | val_1_rmse: 0.62003 |  0:00:45s
epoch 4  | loss: 0.29371 | val_0_rmse: 0.58398 | val_1_rmse: 0.58744 |  0:00:56s
epoch 5  | loss: 0.27973 | val_0_rmse: 0.5573  | val_1_rmse: 0.56514 |  0:01:08s
epoch 6  | loss: 0.27239 | val_0_rmse: 0.53627 | val_1_rmse: 0.54992 |  0:01:19s
epoch 7  | loss: 0.26772 | val_0_rmse: 0.50392 | val_1_rmse: 0.52204 |  0:01:31s
epoch 8  | loss: 0.26097 | val_0_rmse: 0.49646 | val_1_rmse: 0.51729 |  0:01:42s
epoch 9  | loss: 0.26455 | val_0_rmse: 0.49457 | val_1_rmse: 0.51612 |  0:01:53s
epoch 10 | loss: 0.25201 | val_0_rmse: 0.48074 | val_1_rmse: 0.50857 |  0:02:05s
epoch 11 | loss: 0.24862 | val_0_rmse: 0.50633 | val_1_rmse: 0.54223 |  0:02:16s
epoch 12 | loss: 0.25058 | val_0_rmse: 0.63335 | val_1_rmse: 0.67179 |  0:02:28s
epoch 13 | loss: 0.25252 | val_0_rmse: 0.49921 | val_1_rmse: 0.52622 |  0:02:39s
epoch 14 | loss: 0.24403 | val_0_rmse: 0.49289 | val_1_rmse: 0.52563 |  0:02:51s
epoch 15 | loss: 0.24888 | val_0_rmse: 0.49423 | val_1_rmse: 0.52919 |  0:03:02s
epoch 16 | loss: 0.24496 | val_0_rmse: 0.56419 | val_1_rmse: 0.60243 |  0:03:14s
epoch 17 | loss: 0.25484 | val_0_rmse: 0.55012 | val_1_rmse: 0.58477 |  0:03:25s
epoch 18 | loss: 0.24723 | val_0_rmse: 0.53453 | val_1_rmse: 0.57421 |  0:03:37s
epoch 19 | loss: 0.24763 | val_0_rmse: 0.56498 | val_1_rmse: 0.61106 |  0:03:48s
epoch 20 | loss: 0.24395 | val_0_rmse: 0.49663 | val_1_rmse: 0.54207 |  0:04:00s
epoch 21 | loss: 0.23402 | val_0_rmse: 0.53386 | val_1_rmse: 0.58311 |  0:04:11s
epoch 22 | loss: 0.23408 | val_0_rmse: 0.51321 | val_1_rmse: 0.54729 |  0:04:22s
epoch 23 | loss: 0.23748 | val_0_rmse: 0.4876  | val_1_rmse: 0.52877 |  0:04:34s
epoch 24 | loss: 0.23412 | val_0_rmse: 0.48928 | val_1_rmse: 0.5331  |  0:04:45s
epoch 25 | loss: 0.23711 | val_0_rmse: 0.50896 | val_1_rmse: 0.55648 |  0:04:57s
epoch 26 | loss: 0.24234 | val_0_rmse: 0.4995  | val_1_rmse: 0.5359  |  0:05:08s
epoch 27 | loss: 0.24255 | val_0_rmse: 0.51851 | val_1_rmse: 0.54765 |  0:05:20s
epoch 28 | loss: 0.24398 | val_0_rmse: 0.49975 | val_1_rmse: 0.5394  |  0:05:31s
epoch 29 | loss: 0.24618 | val_0_rmse: 0.49501 | val_1_rmse: 0.53678 |  0:05:43s
epoch 30 | loss: 0.24053 | val_0_rmse: 0.49512 | val_1_rmse: 0.53888 |  0:05:54s
epoch 31 | loss: 0.23213 | val_0_rmse: 0.61598 | val_1_rmse: 0.66816 |  0:06:06s
epoch 32 | loss: 0.23129 | val_0_rmse: 0.53294 | val_1_rmse: 0.56618 |  0:06:17s
epoch 33 | loss: 0.24603 | val_0_rmse: 0.5384  | val_1_rmse: 0.5821  |  0:06:29s
epoch 34 | loss: 0.22731 | val_0_rmse: 0.51392 | val_1_rmse: 0.5697  |  0:06:40s
epoch 35 | loss: 0.22426 | val_0_rmse: 0.55061 | val_1_rmse: 0.60334 |  0:06:51s
epoch 36 | loss: 0.22336 | val_0_rmse: 0.49245 | val_1_rmse: 0.54929 |  0:07:03s
epoch 37 | loss: 0.24067 | val_0_rmse: 0.68124 | val_1_rmse: 0.73093 |  0:07:14s
epoch 38 | loss: 0.23849 | val_0_rmse: 0.51208 | val_1_rmse: 0.56386 |  0:07:26s
epoch 39 | loss: 0.23268 | val_0_rmse: 0.55012 | val_1_rmse: 0.601   |  0:07:37s
epoch 40 | loss: 0.23272 | val_0_rmse: 0.48391 | val_1_rmse: 0.53465 |  0:07:49s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.50857
Best weights from best epoch are automatically used!
ended training at: 21:49:16
Feature importance:
Mean squared error is of 1758033574.2662692
Mean absolute error:30077.043432495342
MAPE:0.275874539843276
R2 score:0.7432883475023329
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:49:19
epoch 0  | loss: 0.81796 | val_0_rmse: 0.78568 | val_1_rmse: 0.78316 |  0:00:11s
epoch 1  | loss: 0.36982 | val_0_rmse: 0.73509 | val_1_rmse: 0.73335 |  0:00:22s
epoch 2  | loss: 0.30942 | val_0_rmse: 0.69932 | val_1_rmse: 0.70122 |  0:00:34s
epoch 3  | loss: 0.29043 | val_0_rmse: 0.60934 | val_1_rmse: 0.61197 |  0:00:45s
epoch 4  | loss: 0.27705 | val_0_rmse: 0.59208 | val_1_rmse: 0.59698 |  0:00:57s
epoch 5  | loss: 0.26699 | val_0_rmse: 0.56631 | val_1_rmse: 0.57548 |  0:01:08s
epoch 6  | loss: 0.26411 | val_0_rmse: 0.55268 | val_1_rmse: 0.56415 |  0:01:20s
epoch 7  | loss: 0.25709 | val_0_rmse: 0.51131 | val_1_rmse: 0.52828 |  0:01:31s
epoch 8  | loss: 0.25455 | val_0_rmse: 0.48539 | val_1_rmse: 0.50837 |  0:01:42s
epoch 9  | loss: 0.25228 | val_0_rmse: 0.49109 | val_1_rmse: 0.51342 |  0:01:54s
epoch 10 | loss: 0.24615 | val_0_rmse: 0.48553 | val_1_rmse: 0.51234 |  0:02:05s
epoch 11 | loss: 0.24768 | val_0_rmse: 0.49539 | val_1_rmse: 0.52433 |  0:02:17s
epoch 12 | loss: 0.2435  | val_0_rmse: 0.49779 | val_1_rmse: 0.52635 |  0:02:29s
epoch 13 | loss: 0.2418  | val_0_rmse: 0.47792 | val_1_rmse: 0.50798 |  0:02:40s
epoch 14 | loss: 0.23911 | val_0_rmse: 0.50176 | val_1_rmse: 0.53182 |  0:02:52s
epoch 15 | loss: 0.23691 | val_0_rmse: 0.48886 | val_1_rmse: 0.52329 |  0:03:03s
epoch 16 | loss: 0.23694 | val_0_rmse: 0.48203 | val_1_rmse: 0.51645 |  0:03:14s
epoch 17 | loss: 0.23358 | val_0_rmse: 0.49502 | val_1_rmse: 0.53301 |  0:03:26s
epoch 18 | loss: 0.23243 | val_0_rmse: 0.5022  | val_1_rmse: 0.54072 |  0:03:37s
epoch 19 | loss: 0.23152 | val_0_rmse: 0.53899 | val_1_rmse: 0.57789 |  0:03:49s
epoch 20 | loss: 0.22875 | val_0_rmse: 0.48734 | val_1_rmse: 0.52942 |  0:04:00s
epoch 21 | loss: 0.22612 | val_0_rmse: 0.47922 | val_1_rmse: 0.52119 |  0:04:12s
epoch 22 | loss: 0.22863 | val_0_rmse: 0.50629 | val_1_rmse: 0.52816 |  0:04:23s
epoch 23 | loss: 0.2248  | val_0_rmse: 0.50016 | val_1_rmse: 0.55477 |  0:04:34s
epoch 24 | loss: 0.22238 | val_0_rmse: 0.49535 | val_1_rmse: 0.54124 |  0:04:46s
epoch 25 | loss: 0.22036 | val_0_rmse: 0.48245 | val_1_rmse: 0.53137 |  0:04:57s
epoch 26 | loss: 0.21904 | val_0_rmse: 0.48852 | val_1_rmse: 0.53337 |  0:05:09s
epoch 27 | loss: 0.21611 | val_0_rmse: 0.47022 | val_1_rmse: 0.51803 |  0:05:20s
epoch 28 | loss: 0.21421 | val_0_rmse: 0.46824 | val_1_rmse: 0.51858 |  0:05:32s
epoch 29 | loss: 0.21497 | val_0_rmse: 0.47688 | val_1_rmse: 0.53106 |  0:05:43s
epoch 30 | loss: 0.21259 | val_0_rmse: 0.46051 | val_1_rmse: 0.52154 |  0:05:54s
epoch 31 | loss: 0.21255 | val_0_rmse: 0.45745 | val_1_rmse: 0.5167  |  0:06:06s
epoch 32 | loss: 0.20997 | val_0_rmse: 0.45905 | val_1_rmse: 0.52047 |  0:06:17s
epoch 33 | loss: 0.21022 | val_0_rmse: 0.4546  | val_1_rmse: 0.51559 |  0:06:29s
epoch 34 | loss: 0.20702 | val_0_rmse: 0.46074 | val_1_rmse: 0.52503 |  0:06:40s
epoch 35 | loss: 0.20829 | val_0_rmse: 0.46655 | val_1_rmse: 0.52868 |  0:06:52s
epoch 36 | loss: 0.2064  | val_0_rmse: 0.4569  | val_1_rmse: 0.51989 |  0:07:03s
epoch 37 | loss: 0.2041  | val_0_rmse: 0.46106 | val_1_rmse: 0.52646 |  0:07:15s
epoch 38 | loss: 0.20602 | val_0_rmse: 0.45242 | val_1_rmse: 0.5208  |  0:07:26s
epoch 39 | loss: 0.20323 | val_0_rmse: 0.44926 | val_1_rmse: 0.52123 |  0:07:37s
epoch 40 | loss: 0.20036 | val_0_rmse: 0.44721 | val_1_rmse: 0.51746 |  0:07:49s
epoch 41 | loss: 0.20487 | val_0_rmse: 0.45511 | val_1_rmse: 0.52577 |  0:08:00s
epoch 42 | loss: 0.20251 | val_0_rmse: 0.44839 | val_1_rmse: 0.52371 |  0:08:12s
epoch 43 | loss: 0.19929 | val_0_rmse: 0.44229 | val_1_rmse: 0.51461 |  0:08:23s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.50798
Best weights from best epoch are automatically used!
ended training at: 21:57:48
Feature importance:
Mean squared error is of 1829240647.3859632
Mean absolute error:30544.224953431793
MAPE:0.2926370509853547
R2 score:0.7305578973787679
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:50
epoch 0  | loss: 0.74412 | val_0_rmse: 0.74692 | val_1_rmse: 0.74632 |  0:00:11s
epoch 1  | loss: 0.37007 | val_0_rmse: 0.72868 | val_1_rmse: 0.72654 |  0:00:22s
epoch 2  | loss: 0.32707 | val_0_rmse: 0.67571 | val_1_rmse: 0.67637 |  0:00:34s
epoch 3  | loss: 0.31134 | val_0_rmse: 0.6345  | val_1_rmse: 0.63617 |  0:00:45s
epoch 4  | loss: 0.29469 | val_0_rmse: 0.58431 | val_1_rmse: 0.58768 |  0:00:57s
epoch 5  | loss: 0.29687 | val_0_rmse: 0.57528 | val_1_rmse: 0.57862 |  0:01:08s
epoch 6  | loss: 0.27554 | val_0_rmse: 0.52991 | val_1_rmse: 0.5377  |  0:01:19s
epoch 7  | loss: 0.26798 | val_0_rmse: 0.52078 | val_1_rmse: 0.53274 |  0:01:31s
epoch 8  | loss: 0.26515 | val_0_rmse: 0.50084 | val_1_rmse: 0.51659 |  0:01:42s
epoch 9  | loss: 0.26342 | val_0_rmse: 0.53812 | val_1_rmse: 0.56455 |  0:01:54s
epoch 10 | loss: 0.2625  | val_0_rmse: 0.49581 | val_1_rmse: 0.53066 |  0:02:05s
epoch 11 | loss: 0.26233 | val_0_rmse: 0.53469 | val_1_rmse: 0.55859 |  0:02:17s
epoch 12 | loss: 0.26323 | val_0_rmse: 0.49142 | val_1_rmse: 0.51675 |  0:02:28s
epoch 13 | loss: 0.2516  | val_0_rmse: 0.57679 | val_1_rmse: 0.60186 |  0:02:40s
epoch 14 | loss: 0.26176 | val_0_rmse: 0.51179 | val_1_rmse: 0.54304 |  0:02:51s
epoch 15 | loss: 0.26517 | val_0_rmse: 0.53258 | val_1_rmse: 0.5564  |  0:03:02s
epoch 16 | loss: 0.25421 | val_0_rmse: 0.51881 | val_1_rmse: 0.54618 |  0:03:14s
epoch 17 | loss: 0.2471  | val_0_rmse: 0.50191 | val_1_rmse: 0.53033 |  0:03:25s
epoch 18 | loss: 0.24866 | val_0_rmse: 0.51258 | val_1_rmse: 0.53541 |  0:03:37s
epoch 19 | loss: 0.24055 | val_0_rmse: 0.4993  | val_1_rmse: 0.53154 |  0:03:48s
epoch 20 | loss: 0.23788 | val_0_rmse: 0.5057  | val_1_rmse: 0.52874 |  0:04:00s
epoch 21 | loss: 0.24845 | val_0_rmse: 0.51441 | val_1_rmse: 0.54808 |  0:04:11s
epoch 22 | loss: 0.26966 | val_0_rmse: 0.64371 | val_1_rmse: 0.66987 |  0:04:23s
epoch 23 | loss: 0.26174 | val_0_rmse: 0.52883 | val_1_rmse: 0.56467 |  0:04:34s
epoch 24 | loss: 0.25813 | val_0_rmse: 0.49968 | val_1_rmse: 0.5347  |  0:04:45s
epoch 25 | loss: 0.25593 | val_0_rmse: 0.51263 | val_1_rmse: 0.54974 |  0:04:57s
epoch 26 | loss: 0.24252 | val_0_rmse: 0.52279 | val_1_rmse: 0.5588  |  0:05:08s
epoch 27 | loss: 0.25068 | val_0_rmse: 0.50304 | val_1_rmse: 0.5372  |  0:05:20s
epoch 28 | loss: 0.24229 | val_0_rmse: 0.52549 | val_1_rmse: 0.57211 |  0:05:31s
epoch 29 | loss: 0.2355  | val_0_rmse: 0.51079 | val_1_rmse: 0.5526  |  0:05:43s
epoch 30 | loss: 0.23495 | val_0_rmse: 0.50459 | val_1_rmse: 0.54816 |  0:05:54s
epoch 31 | loss: 0.23199 | val_0_rmse: 0.52988 | val_1_rmse: 0.56228 |  0:06:06s
epoch 32 | loss: 0.23002 | val_0_rmse: 0.6815  | val_1_rmse: 0.56694 |  0:06:17s
epoch 33 | loss: 0.23503 | val_0_rmse: 0.97143 | val_1_rmse: 0.54049 |  0:06:29s
epoch 34 | loss: 0.22879 | val_0_rmse: 1.54734 | val_1_rmse: 0.7226  |  0:06:40s
epoch 35 | loss: 0.22418 | val_0_rmse: 1.26514 | val_1_rmse: 0.56149 |  0:06:51s
epoch 36 | loss: 0.22977 | val_0_rmse: 3.34937 | val_1_rmse: 0.60074 |  0:07:03s
epoch 37 | loss: 0.2408  | val_0_rmse: 0.72996 | val_1_rmse: 0.74981 |  0:07:14s
epoch 38 | loss: 0.24535 | val_0_rmse: 2.82914 | val_1_rmse: 0.56978 |  0:07:26s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.51659
Best weights from best epoch are automatically used!
ended training at: 22:05:22
Feature importance:
Mean squared error is of 1850675970.2424033
Mean absolute error:31198.613211469
MAPE:0.306203632184801
R2 score:0.7280592456685546
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:05:24
epoch 0  | loss: 1.62971 | val_0_rmse: 0.96962 | val_1_rmse: 0.95766 |  0:00:01s
epoch 1  | loss: 0.71747 | val_0_rmse: 0.78352 | val_1_rmse: 0.80618 |  0:00:03s
epoch 2  | loss: 0.39633 | val_0_rmse: 0.67469 | val_1_rmse: 0.70236 |  0:00:04s
epoch 3  | loss: 0.32337 | val_0_rmse: 0.61202 | val_1_rmse: 0.63614 |  0:00:06s
epoch 4  | loss: 0.29887 | val_0_rmse: 0.61202 | val_1_rmse: 0.62989 |  0:00:08s
epoch 5  | loss: 0.28259 | val_0_rmse: 0.57288 | val_1_rmse: 0.59776 |  0:00:09s
epoch 6  | loss: 0.26413 | val_0_rmse: 0.5811  | val_1_rmse: 0.59547 |  0:00:11s
epoch 7  | loss: 0.25121 | val_0_rmse: 0.54383 | val_1_rmse: 0.56719 |  0:00:13s
epoch 8  | loss: 0.24469 | val_0_rmse: 0.54467 | val_1_rmse: 0.56717 |  0:00:14s
epoch 9  | loss: 0.2387  | val_0_rmse: 0.52995 | val_1_rmse: 0.5551  |  0:00:16s
epoch 10 | loss: 0.23305 | val_0_rmse: 0.5399  | val_1_rmse: 0.55091 |  0:00:18s
epoch 11 | loss: 0.22774 | val_0_rmse: 0.50329 | val_1_rmse: 0.52698 |  0:00:19s
epoch 12 | loss: 0.22643 | val_0_rmse: 0.51098 | val_1_rmse: 0.53857 |  0:00:21s
epoch 13 | loss: 0.22331 | val_0_rmse: 0.50055 | val_1_rmse: 0.51973 |  0:00:23s
epoch 14 | loss: 0.21731 | val_0_rmse: 0.48852 | val_1_rmse: 0.51001 |  0:00:24s
epoch 15 | loss: 0.22142 | val_0_rmse: 0.50483 | val_1_rmse: 0.53177 |  0:00:26s
epoch 16 | loss: 0.20895 | val_0_rmse: 0.47582 | val_1_rmse: 0.4972  |  0:00:28s
epoch 17 | loss: 0.20516 | val_0_rmse: 0.48526 | val_1_rmse: 0.49706 |  0:00:29s
epoch 18 | loss: 0.20083 | val_0_rmse: 0.48012 | val_1_rmse: 0.49806 |  0:00:31s
epoch 19 | loss: 0.20123 | val_0_rmse: 0.45845 | val_1_rmse: 0.47949 |  0:00:33s
epoch 20 | loss: 0.19786 | val_0_rmse: 0.48279 | val_1_rmse: 0.49618 |  0:00:34s
epoch 21 | loss: 0.19841 | val_0_rmse: 0.4604  | val_1_rmse: 0.47483 |  0:00:36s
epoch 22 | loss: 0.20384 | val_0_rmse: 0.44538 | val_1_rmse: 0.47333 |  0:00:38s
epoch 23 | loss: 0.19627 | val_0_rmse: 0.44545 | val_1_rmse: 0.46849 |  0:00:39s
epoch 24 | loss: 0.19556 | val_0_rmse: 0.4407  | val_1_rmse: 0.46194 |  0:00:41s
epoch 25 | loss: 0.19237 | val_0_rmse: 0.45076 | val_1_rmse: 0.47575 |  0:00:43s
epoch 26 | loss: 0.19803 | val_0_rmse: 0.43102 | val_1_rmse: 0.45504 |  0:00:44s
epoch 27 | loss: 0.18963 | val_0_rmse: 0.42922 | val_1_rmse: 0.45645 |  0:00:46s
epoch 28 | loss: 0.19062 | val_0_rmse: 0.44214 | val_1_rmse: 0.46305 |  0:00:48s
epoch 29 | loss: 0.19189 | val_0_rmse: 0.42777 | val_1_rmse: 0.46013 |  0:00:49s
epoch 30 | loss: 0.18815 | val_0_rmse: 0.43473 | val_1_rmse: 0.46065 |  0:00:51s
epoch 31 | loss: 0.18976 | val_0_rmse: 0.42806 | val_1_rmse: 0.4551  |  0:00:53s
epoch 32 | loss: 0.18844 | val_0_rmse: 0.41901 | val_1_rmse: 0.44948 |  0:00:54s
epoch 33 | loss: 0.18588 | val_0_rmse: 0.41363 | val_1_rmse: 0.44712 |  0:00:56s
epoch 34 | loss: 0.1865  | val_0_rmse: 0.41642 | val_1_rmse: 0.45217 |  0:00:58s
epoch 35 | loss: 0.18617 | val_0_rmse: 0.41428 | val_1_rmse: 0.45001 |  0:00:59s
epoch 36 | loss: 0.18424 | val_0_rmse: 0.42778 | val_1_rmse: 0.46001 |  0:01:01s
epoch 37 | loss: 0.18355 | val_0_rmse: 0.40808 | val_1_rmse: 0.44731 |  0:01:03s
epoch 38 | loss: 0.183   | val_0_rmse: 0.42649 | val_1_rmse: 0.47405 |  0:01:04s
epoch 39 | loss: 0.18303 | val_0_rmse: 0.40959 | val_1_rmse: 0.45666 |  0:01:06s
epoch 40 | loss: 0.18508 | val_0_rmse: 0.40449 | val_1_rmse: 0.4497  |  0:01:08s
epoch 41 | loss: 0.18647 | val_0_rmse: 0.40599 | val_1_rmse: 0.44885 |  0:01:09s
epoch 42 | loss: 0.18284 | val_0_rmse: 0.41898 | val_1_rmse: 0.47173 |  0:01:11s
epoch 43 | loss: 0.17907 | val_0_rmse: 0.41835 | val_1_rmse: 0.4603  |  0:01:13s
epoch 44 | loss: 0.18492 | val_0_rmse: 0.40946 | val_1_rmse: 0.46121 |  0:01:14s
epoch 45 | loss: 0.17792 | val_0_rmse: 0.40456 | val_1_rmse: 0.45764 |  0:01:16s
epoch 46 | loss: 0.18077 | val_0_rmse: 0.41115 | val_1_rmse: 0.46114 |  0:01:18s
epoch 47 | loss: 0.17757 | val_0_rmse: 0.40419 | val_1_rmse: 0.45369 |  0:01:19s
epoch 48 | loss: 0.17419 | val_0_rmse: 0.40294 | val_1_rmse: 0.45666 |  0:01:21s
epoch 49 | loss: 0.17655 | val_0_rmse: 0.40797 | val_1_rmse: 0.46378 |  0:01:23s
epoch 50 | loss: 0.17243 | val_0_rmse: 0.43213 | val_1_rmse: 0.49207 |  0:01:24s
epoch 51 | loss: 0.17348 | val_0_rmse: 0.39861 | val_1_rmse: 0.4546  |  0:01:26s
epoch 52 | loss: 0.17313 | val_0_rmse: 0.39924 | val_1_rmse: 0.45376 |  0:01:28s
epoch 53 | loss: 0.1706  | val_0_rmse: 0.40053 | val_1_rmse: 0.45843 |  0:01:29s
epoch 54 | loss: 0.17335 | val_0_rmse: 0.3992  | val_1_rmse: 0.45635 |  0:01:31s
epoch 55 | loss: 0.17157 | val_0_rmse: 0.39822 | val_1_rmse: 0.4561  |  0:01:33s
epoch 56 | loss: 0.16914 | val_0_rmse: 0.40024 | val_1_rmse: 0.4659  |  0:01:34s
epoch 57 | loss: 0.17005 | val_0_rmse: 0.39723 | val_1_rmse: 0.45901 |  0:01:36s
epoch 58 | loss: 0.16642 | val_0_rmse: 0.39594 | val_1_rmse: 0.46231 |  0:01:38s
epoch 59 | loss: 0.16578 | val_0_rmse: 0.40679 | val_1_rmse: 0.46575 |  0:01:39s
epoch 60 | loss: 0.17119 | val_0_rmse: 0.43787 | val_1_rmse: 0.50923 |  0:01:41s
epoch 61 | loss: 0.16869 | val_0_rmse: 0.39215 | val_1_rmse: 0.4598  |  0:01:42s
epoch 62 | loss: 0.16433 | val_0_rmse: 0.39224 | val_1_rmse: 0.45541 |  0:01:44s
epoch 63 | loss: 0.1647  | val_0_rmse: 0.38917 | val_1_rmse: 0.45074 |  0:01:46s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.44712
Best weights from best epoch are automatically used!
ended training at: 22:07:11
Feature importance:
Mean squared error is of 841014467.5930638
Mean absolute error:19575.211066956967
MAPE:0.22202978864648343
R2 score:0.8157323592943047
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:07:12
epoch 0  | loss: 1.51189 | val_0_rmse: 0.96563 | val_1_rmse: 0.97537 |  0:00:01s
epoch 1  | loss: 0.71146 | val_0_rmse: 0.85777 | val_1_rmse: 0.86483 |  0:00:03s
epoch 2  | loss: 0.43582 | val_0_rmse: 0.74809 | val_1_rmse: 0.74438 |  0:00:04s
epoch 3  | loss: 0.34578 | val_0_rmse: 0.60406 | val_1_rmse: 0.6078  |  0:00:06s
epoch 4  | loss: 0.2867  | val_0_rmse: 0.59462 | val_1_rmse: 0.59658 |  0:00:08s
epoch 5  | loss: 0.26159 | val_0_rmse: 0.5861  | val_1_rmse: 0.58772 |  0:00:09s
epoch 6  | loss: 0.25249 | val_0_rmse: 0.57489 | val_1_rmse: 0.58859 |  0:00:11s
epoch 7  | loss: 0.23484 | val_0_rmse: 0.53966 | val_1_rmse: 0.55381 |  0:00:13s
epoch 8  | loss: 0.22718 | val_0_rmse: 0.53344 | val_1_rmse: 0.54104 |  0:00:14s
epoch 9  | loss: 0.22157 | val_0_rmse: 0.50738 | val_1_rmse: 0.51634 |  0:00:16s
epoch 10 | loss: 0.20978 | val_0_rmse: 0.51104 | val_1_rmse: 0.52242 |  0:00:18s
epoch 11 | loss: 0.20798 | val_0_rmse: 0.5015  | val_1_rmse: 0.50946 |  0:00:19s
epoch 12 | loss: 0.20569 | val_0_rmse: 0.49554 | val_1_rmse: 0.50441 |  0:00:21s
epoch 13 | loss: 0.20117 | val_0_rmse: 0.48018 | val_1_rmse: 0.48726 |  0:00:23s
epoch 14 | loss: 0.2007  | val_0_rmse: 0.47025 | val_1_rmse: 0.47879 |  0:00:24s
epoch 15 | loss: 0.19959 | val_0_rmse: 0.47084 | val_1_rmse: 0.48089 |  0:00:26s
epoch 16 | loss: 0.19387 | val_0_rmse: 0.47919 | val_1_rmse: 0.48919 |  0:00:28s
epoch 17 | loss: 0.19533 | val_0_rmse: 0.47136 | val_1_rmse: 0.48424 |  0:00:29s
epoch 18 | loss: 0.19021 | val_0_rmse: 0.44867 | val_1_rmse: 0.4601  |  0:00:31s
epoch 19 | loss: 0.19273 | val_0_rmse: 0.45341 | val_1_rmse: 0.46331 |  0:00:33s
epoch 20 | loss: 0.19085 | val_0_rmse: 0.44127 | val_1_rmse: 0.4511  |  0:00:34s
epoch 21 | loss: 0.19139 | val_0_rmse: 0.46901 | val_1_rmse: 0.4787  |  0:00:36s
epoch 22 | loss: 0.1878  | val_0_rmse: 0.44264 | val_1_rmse: 0.45571 |  0:00:38s
epoch 23 | loss: 0.18791 | val_0_rmse: 0.43853 | val_1_rmse: 0.44802 |  0:00:39s
epoch 24 | loss: 0.18754 | val_0_rmse: 0.43416 | val_1_rmse: 0.44873 |  0:00:41s
epoch 25 | loss: 0.18808 | val_0_rmse: 0.42827 | val_1_rmse: 0.4407  |  0:00:43s
epoch 26 | loss: 0.1828  | val_0_rmse: 0.45307 | val_1_rmse: 0.46969 |  0:00:44s
epoch 27 | loss: 0.18283 | val_0_rmse: 0.41901 | val_1_rmse: 0.43842 |  0:00:46s
epoch 28 | loss: 0.18052 | val_0_rmse: 0.41732 | val_1_rmse: 0.43935 |  0:00:47s
epoch 29 | loss: 0.18284 | val_0_rmse: 0.41282 | val_1_rmse: 0.44019 |  0:00:49s
epoch 30 | loss: 0.17945 | val_0_rmse: 0.40968 | val_1_rmse: 0.43714 |  0:00:51s
epoch 31 | loss: 0.17689 | val_0_rmse: 0.40577 | val_1_rmse: 0.4371  |  0:00:52s
epoch 32 | loss: 0.17779 | val_0_rmse: 0.4323  | val_1_rmse: 0.46181 |  0:00:54s
epoch 33 | loss: 0.1816  | val_0_rmse: 0.40736 | val_1_rmse: 0.43928 |  0:00:56s
epoch 34 | loss: 0.18387 | val_0_rmse: 0.40144 | val_1_rmse: 0.43769 |  0:00:57s
epoch 35 | loss: 0.17636 | val_0_rmse: 0.40257 | val_1_rmse: 0.43973 |  0:00:59s
epoch 36 | loss: 0.17523 | val_0_rmse: 0.39603 | val_1_rmse: 0.43775 |  0:01:01s
epoch 37 | loss: 0.17069 | val_0_rmse: 0.40416 | val_1_rmse: 0.44711 |  0:01:02s
epoch 38 | loss: 0.17216 | val_0_rmse: 0.39591 | val_1_rmse: 0.43608 |  0:01:04s
epoch 39 | loss: 0.17188 | val_0_rmse: 0.38973 | val_1_rmse: 0.43422 |  0:01:06s
epoch 40 | loss: 0.16934 | val_0_rmse: 0.39211 | val_1_rmse: 0.43688 |  0:01:07s
epoch 41 | loss: 0.16778 | val_0_rmse: 0.39178 | val_1_rmse: 0.43782 |  0:01:09s
epoch 42 | loss: 0.16989 | val_0_rmse: 0.39093 | val_1_rmse: 0.44467 |  0:01:11s
epoch 43 | loss: 0.17104 | val_0_rmse: 0.39056 | val_1_rmse: 0.44428 |  0:01:12s
epoch 44 | loss: 0.17006 | val_0_rmse: 0.3902  | val_1_rmse: 0.44102 |  0:01:14s
epoch 45 | loss: 0.16747 | val_0_rmse: 0.38734 | val_1_rmse: 0.44141 |  0:01:16s
epoch 46 | loss: 0.16549 | val_0_rmse: 0.38657 | val_1_rmse: 0.44183 |  0:01:17s
epoch 47 | loss: 0.16325 | val_0_rmse: 0.38342 | val_1_rmse: 0.45002 |  0:01:19s
epoch 48 | loss: 0.16429 | val_0_rmse: 0.51407 | val_1_rmse: 0.6002  |  0:01:21s
epoch 49 | loss: 0.16656 | val_0_rmse: 0.38381 | val_1_rmse: 0.60695 |  0:01:22s
epoch 50 | loss: 0.16439 | val_0_rmse: 0.39169 | val_1_rmse: 0.70685 |  0:01:24s
epoch 51 | loss: 0.16634 | val_0_rmse: 0.39755 | val_1_rmse: 0.61376 |  0:01:25s
epoch 52 | loss: 0.16569 | val_0_rmse: 0.38607 | val_1_rmse: 0.5335  |  0:01:27s
epoch 53 | loss: 0.16267 | val_0_rmse: 0.38534 | val_1_rmse: 0.53624 |  0:01:29s
epoch 54 | loss: 0.16158 | val_0_rmse: 0.38258 | val_1_rmse: 0.44143 |  0:01:30s
epoch 55 | loss: 0.16381 | val_0_rmse: 0.38738 | val_1_rmse: 0.45243 |  0:01:32s
epoch 56 | loss: 0.16238 | val_0_rmse: 0.40131 | val_1_rmse: 0.46567 |  0:01:34s
epoch 57 | loss: 0.16386 | val_0_rmse: 0.39344 | val_1_rmse: 0.44676 |  0:01:35s
epoch 58 | loss: 0.16031 | val_0_rmse: 0.37976 | val_1_rmse: 0.44062 |  0:01:37s
epoch 59 | loss: 0.16223 | val_0_rmse: 0.38535 | val_1_rmse: 0.4439  |  0:01:39s
epoch 60 | loss: 0.15948 | val_0_rmse: 0.38251 | val_1_rmse: 0.4427  |  0:01:40s
epoch 61 | loss: 0.16111 | val_0_rmse: 0.38032 | val_1_rmse: 0.44577 |  0:01:42s
epoch 62 | loss: 0.15742 | val_0_rmse: 0.38112 | val_1_rmse: 0.44607 |  0:01:44s
epoch 63 | loss: 0.15558 | val_0_rmse: 0.37498 | val_1_rmse: 0.43739 |  0:01:45s
epoch 64 | loss: 0.15517 | val_0_rmse: 0.38092 | val_1_rmse: 0.44732 |  0:01:47s
epoch 65 | loss: 0.15907 | val_0_rmse: 0.38125 | val_1_rmse: 0.44662 |  0:01:49s
epoch 66 | loss: 0.15479 | val_0_rmse: 0.37522 | val_1_rmse: 0.44154 |  0:01:50s
epoch 67 | loss: 0.15776 | val_0_rmse: 0.37873 | val_1_rmse: 0.44851 |  0:01:52s
epoch 68 | loss: 0.15498 | val_0_rmse: 0.37629 | val_1_rmse: 0.44605 |  0:01:54s
epoch 69 | loss: 0.15525 | val_0_rmse: 0.37157 | val_1_rmse: 0.44064 |  0:01:55s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.43422
Best weights from best epoch are automatically used!
ended training at: 22:09:08
Feature importance:
Mean squared error is of 910423702.0393442
Mean absolute error:20365.79872095406
MAPE:0.22930370288173915
R2 score:0.802701290352019
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:09:08
epoch 0  | loss: 1.54928 | val_0_rmse: 0.99346 | val_1_rmse: 1.01498 |  0:00:01s
epoch 1  | loss: 0.7687  | val_0_rmse: 0.75341 | val_1_rmse: 0.75823 |  0:00:03s
epoch 2  | loss: 0.40392 | val_0_rmse: 0.76857 | val_1_rmse: 0.77165 |  0:00:04s
epoch 3  | loss: 0.32375 | val_0_rmse: 0.66844 | val_1_rmse: 0.66758 |  0:00:06s
epoch 4  | loss: 0.30345 | val_0_rmse: 0.61407 | val_1_rmse: 0.62563 |  0:00:08s
epoch 5  | loss: 0.29223 | val_0_rmse: 0.60798 | val_1_rmse: 0.61716 |  0:00:09s
epoch 6  | loss: 0.27705 | val_0_rmse: 0.62569 | val_1_rmse: 0.62564 |  0:00:11s
epoch 7  | loss: 0.27572 | val_0_rmse: 0.5921  | val_1_rmse: 0.59923 |  0:00:13s
epoch 8  | loss: 0.26179 | val_0_rmse: 0.56874 | val_1_rmse: 0.57476 |  0:00:14s
epoch 9  | loss: 0.25234 | val_0_rmse: 0.55875 | val_1_rmse: 0.5619  |  0:00:16s
epoch 10 | loss: 0.24785 | val_0_rmse: 0.56316 | val_1_rmse: 0.57627 |  0:00:18s
epoch 11 | loss: 0.2588  | val_0_rmse: 0.54544 | val_1_rmse: 0.55603 |  0:00:19s
epoch 12 | loss: 0.26083 | val_0_rmse: 0.54493 | val_1_rmse: 0.56287 |  0:00:21s
epoch 13 | loss: 0.25309 | val_0_rmse: 0.51594 | val_1_rmse: 0.53568 |  0:00:23s
epoch 14 | loss: 0.24495 | val_0_rmse: 0.4966  | val_1_rmse: 0.51245 |  0:00:24s
epoch 15 | loss: 0.23794 | val_0_rmse: 0.51402 | val_1_rmse: 0.53008 |  0:00:26s
epoch 16 | loss: 0.22918 | val_0_rmse: 0.49296 | val_1_rmse: 0.5113  |  0:00:28s
epoch 17 | loss: 0.22186 | val_0_rmse: 0.48708 | val_1_rmse: 0.50713 |  0:00:29s
epoch 18 | loss: 0.23422 | val_0_rmse: 0.47055 | val_1_rmse: 0.49016 |  0:00:31s
epoch 19 | loss: 0.21864 | val_0_rmse: 0.46458 | val_1_rmse: 0.48792 |  0:00:33s
epoch 20 | loss: 0.21087 | val_0_rmse: 0.46475 | val_1_rmse: 0.48884 |  0:00:34s
epoch 21 | loss: 0.20954 | val_0_rmse: 0.47075 | val_1_rmse: 0.49539 |  0:00:36s
epoch 22 | loss: 0.20918 | val_0_rmse: 0.47558 | val_1_rmse: 0.49855 |  0:00:38s
epoch 23 | loss: 0.20473 | val_0_rmse: 0.44658 | val_1_rmse: 0.47024 |  0:00:39s
epoch 24 | loss: 0.20886 | val_0_rmse: 0.45459 | val_1_rmse: 0.48135 |  0:00:41s
epoch 25 | loss: 0.20243 | val_0_rmse: 0.4428  | val_1_rmse: 0.47413 |  0:00:43s
epoch 26 | loss: 0.20458 | val_0_rmse: 0.44979 | val_1_rmse: 0.48295 |  0:00:44s
epoch 27 | loss: 0.19901 | val_0_rmse: 0.43543 | val_1_rmse: 0.47067 |  0:00:46s
epoch 28 | loss: 0.20052 | val_0_rmse: 0.4464  | val_1_rmse: 0.48007 |  0:00:48s
epoch 29 | loss: 0.19918 | val_0_rmse: 0.44093 | val_1_rmse: 0.4763  |  0:00:49s
epoch 30 | loss: 0.19677 | val_0_rmse: 0.43106 | val_1_rmse: 0.46672 |  0:00:51s
epoch 31 | loss: 0.18825 | val_0_rmse: 0.43767 | val_1_rmse: 0.46867 |  0:00:53s
epoch 32 | loss: 0.19065 | val_0_rmse: 0.42556 | val_1_rmse: 0.46095 |  0:00:54s
epoch 33 | loss: 0.19234 | val_0_rmse: 0.42604 | val_1_rmse: 0.4634  |  0:00:56s
epoch 34 | loss: 0.19094 | val_0_rmse: 0.42243 | val_1_rmse: 0.46668 |  0:00:58s
epoch 35 | loss: 0.18756 | val_0_rmse: 0.41812 | val_1_rmse: 0.45926 |  0:00:59s
epoch 36 | loss: 0.19356 | val_0_rmse: 0.43202 | val_1_rmse: 0.46893 |  0:01:01s
epoch 37 | loss: 0.19262 | val_0_rmse: 0.42184 | val_1_rmse: 0.46498 |  0:01:03s
epoch 38 | loss: 0.18687 | val_0_rmse: 0.43044 | val_1_rmse: 0.47549 |  0:01:04s
epoch 39 | loss: 0.19141 | val_0_rmse: 0.42484 | val_1_rmse: 0.4718  |  0:01:06s
epoch 40 | loss: 0.20159 | val_0_rmse: 0.44465 | val_1_rmse: 0.49519 |  0:01:08s
epoch 41 | loss: 0.195   | val_0_rmse: 0.42022 | val_1_rmse: 0.46884 |  0:01:09s
epoch 42 | loss: 0.18997 | val_0_rmse: 0.42024 | val_1_rmse: 0.46604 |  0:01:11s
epoch 43 | loss: 0.18681 | val_0_rmse: 0.43476 | val_1_rmse: 0.47732 |  0:01:12s
epoch 44 | loss: 0.18745 | val_0_rmse: 0.66932 | val_1_rmse: 0.7027  |  0:01:14s
epoch 45 | loss: 0.18665 | val_0_rmse: 0.42298 | val_1_rmse: 0.47759 |  0:01:16s
epoch 46 | loss: 0.18479 | val_0_rmse: 0.42646 | val_1_rmse: 0.47388 |  0:01:17s
epoch 47 | loss: 0.18021 | val_0_rmse: 0.40841 | val_1_rmse: 0.46412 |  0:01:19s
epoch 48 | loss: 0.18169 | val_0_rmse: 0.40895 | val_1_rmse: 0.46298 |  0:01:21s
epoch 49 | loss: 0.18037 | val_0_rmse: 0.4136  | val_1_rmse: 0.46909 |  0:01:22s
epoch 50 | loss: 0.18034 | val_0_rmse: 0.43018 | val_1_rmse: 0.47865 |  0:01:24s
epoch 51 | loss: 0.17802 | val_0_rmse: 0.4136  | val_1_rmse: 0.47084 |  0:01:26s
epoch 52 | loss: 0.17497 | val_0_rmse: 0.40973 | val_1_rmse: 0.46453 |  0:01:27s
epoch 53 | loss: 0.17615 | val_0_rmse: 0.40848 | val_1_rmse: 0.46827 |  0:01:29s
epoch 54 | loss: 0.1754  | val_0_rmse: 0.40588 | val_1_rmse: 0.46478 |  0:01:31s
epoch 55 | loss: 0.17535 | val_0_rmse: 0.40995 | val_1_rmse: 0.46901 |  0:01:32s
epoch 56 | loss: 0.17411 | val_0_rmse: 0.40647 | val_1_rmse: 0.47283 |  0:01:34s
epoch 57 | loss: 0.1733  | val_0_rmse: 0.40396 | val_1_rmse: 0.46479 |  0:01:36s
epoch 58 | loss: 0.17183 | val_0_rmse: 0.40803 | val_1_rmse: 0.46641 |  0:01:37s
epoch 59 | loss: 0.1713  | val_0_rmse: 0.40469 | val_1_rmse: 0.46863 |  0:01:39s
epoch 60 | loss: 0.17135 | val_0_rmse: 0.40464 | val_1_rmse: 0.46934 |  0:01:41s
epoch 61 | loss: 0.17156 | val_0_rmse: 0.40294 | val_1_rmse: 0.46435 |  0:01:42s
epoch 62 | loss: 0.17655 | val_0_rmse: 0.42646 | val_1_rmse: 0.48566 |  0:01:44s
epoch 63 | loss: 0.18338 | val_0_rmse: 0.43922 | val_1_rmse: 0.49008 |  0:01:46s
epoch 64 | loss: 0.18624 | val_0_rmse: 0.63793 | val_1_rmse: 0.69151 |  0:01:47s
epoch 65 | loss: 0.19716 | val_0_rmse: 0.44725 | val_1_rmse: 0.50936 |  0:01:49s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.45926
Best weights from best epoch are automatically used!
ended training at: 22:10:58
Feature importance:
Mean squared error is of 910777288.4113818
Mean absolute error:20626.246822836445
MAPE:0.23897967708243967
R2 score:0.8044343575728536
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:10:59
epoch 0  | loss: 1.54859 | val_0_rmse: 0.97972 | val_1_rmse: 1.00923 |  0:00:01s
epoch 1  | loss: 0.70777 | val_0_rmse: 0.7352  | val_1_rmse: 0.73248 |  0:00:03s
epoch 2  | loss: 0.44375 | val_0_rmse: 0.65979 | val_1_rmse: 0.66672 |  0:00:04s
epoch 3  | loss: 0.34118 | val_0_rmse: 0.60281 | val_1_rmse: 0.60584 |  0:00:06s
epoch 4  | loss: 0.28902 | val_0_rmse: 0.5983  | val_1_rmse: 0.5979  |  0:00:08s
epoch 5  | loss: 0.26201 | val_0_rmse: 0.58409 | val_1_rmse: 0.59074 |  0:00:09s
epoch 6  | loss: 0.24121 | val_0_rmse: 0.56578 | val_1_rmse: 0.57337 |  0:00:11s
epoch 7  | loss: 0.22746 | val_0_rmse: 0.54775 | val_1_rmse: 0.55408 |  0:00:13s
epoch 8  | loss: 0.21708 | val_0_rmse: 0.5562  | val_1_rmse: 0.56595 |  0:00:14s
epoch 9  | loss: 0.2186  | val_0_rmse: 0.53579 | val_1_rmse: 0.5447  |  0:00:16s
epoch 10 | loss: 0.20872 | val_0_rmse: 0.50074 | val_1_rmse: 0.50942 |  0:00:18s
epoch 11 | loss: 0.20947 | val_0_rmse: 0.49449 | val_1_rmse: 0.50488 |  0:00:19s
epoch 12 | loss: 0.20277 | val_0_rmse: 0.48754 | val_1_rmse: 0.50044 |  0:00:21s
epoch 13 | loss: 0.20129 | val_0_rmse: 0.49874 | val_1_rmse: 0.51166 |  0:00:23s
epoch 14 | loss: 0.20066 | val_0_rmse: 0.48465 | val_1_rmse: 0.49496 |  0:00:24s
epoch 15 | loss: 0.19891 | val_0_rmse: 0.46216 | val_1_rmse: 0.47739 |  0:00:26s
epoch 16 | loss: 0.19085 | val_0_rmse: 0.48193 | val_1_rmse: 0.49953 |  0:00:28s
epoch 17 | loss: 0.19018 | val_0_rmse: 0.45915 | val_1_rmse: 0.47573 |  0:00:29s
epoch 18 | loss: 0.18632 | val_0_rmse: 0.45348 | val_1_rmse: 0.47613 |  0:00:31s
epoch 19 | loss: 0.1879  | val_0_rmse: 0.45752 | val_1_rmse: 0.48292 |  0:00:33s
epoch 20 | loss: 0.18297 | val_0_rmse: 0.44556 | val_1_rmse: 0.46974 |  0:00:34s
epoch 21 | loss: 0.18168 | val_0_rmse: 0.44836 | val_1_rmse: 0.47618 |  0:00:36s
epoch 22 | loss: 0.18604 | val_0_rmse: 0.44385 | val_1_rmse: 0.47228 |  0:00:38s
epoch 23 | loss: 0.18334 | val_0_rmse: 0.43794 | val_1_rmse: 0.46608 |  0:00:39s
epoch 24 | loss: 0.17807 | val_0_rmse: 0.42746 | val_1_rmse: 0.46657 |  0:00:41s
epoch 25 | loss: 0.17867 | val_0_rmse: 0.426   | val_1_rmse: 0.46932 |  0:00:43s
epoch 26 | loss: 0.17672 | val_0_rmse: 0.41702 | val_1_rmse: 0.46836 |  0:00:44s
epoch 27 | loss: 0.17658 | val_0_rmse: 0.41714 | val_1_rmse: 0.47582 |  0:00:46s
epoch 28 | loss: 0.17495 | val_0_rmse: 0.41041 | val_1_rmse: 0.46388 |  0:00:48s
epoch 29 | loss: 0.17454 | val_0_rmse: 0.43039 | val_1_rmse: 0.48519 |  0:00:49s
epoch 30 | loss: 0.173   | val_0_rmse: 0.40313 | val_1_rmse: 0.4742  |  0:00:51s
epoch 31 | loss: 0.17323 | val_0_rmse: 0.42469 | val_1_rmse: 0.48413 |  0:00:53s
epoch 32 | loss: 0.17493 | val_0_rmse: 0.39876 | val_1_rmse: 0.4721  |  0:00:54s
epoch 33 | loss: 0.17608 | val_0_rmse: 0.41594 | val_1_rmse: 0.49546 |  0:00:56s
epoch 34 | loss: 0.1769  | val_0_rmse: 0.3994  | val_1_rmse: 0.51127 |  0:00:58s
epoch 35 | loss: 0.17241 | val_0_rmse: 0.3939  | val_1_rmse: 0.51603 |  0:00:59s
epoch 36 | loss: 0.17285 | val_0_rmse: 0.40022 | val_1_rmse: 0.5087  |  0:01:01s
epoch 37 | loss: 0.17549 | val_0_rmse: 0.39517 | val_1_rmse: 0.53164 |  0:01:03s
epoch 38 | loss: 0.17503 | val_0_rmse: 0.39506 | val_1_rmse: 0.50809 |  0:01:04s
epoch 39 | loss: 0.16891 | val_0_rmse: 0.43577 | val_1_rmse: 0.56766 |  0:01:06s
epoch 40 | loss: 0.17029 | val_0_rmse: 0.39577 | val_1_rmse: 0.51096 |  0:01:07s
epoch 41 | loss: 0.16901 | val_0_rmse: 0.38949 | val_1_rmse: 0.48823 |  0:01:09s
epoch 42 | loss: 0.17076 | val_0_rmse: 0.39109 | val_1_rmse: 0.51483 |  0:01:11s
epoch 43 | loss: 0.16837 | val_0_rmse: 0.38952 | val_1_rmse: 0.50168 |  0:01:12s
epoch 44 | loss: 0.16669 | val_0_rmse: 0.38852 | val_1_rmse: 0.53544 |  0:01:14s
epoch 45 | loss: 0.16572 | val_0_rmse: 0.39415 | val_1_rmse: 0.55509 |  0:01:16s
epoch 46 | loss: 0.1727  | val_0_rmse: 0.39118 | val_1_rmse: 0.58084 |  0:01:17s
epoch 47 | loss: 0.16878 | val_0_rmse: 0.39115 | val_1_rmse: 0.51919 |  0:01:19s
epoch 48 | loss: 0.16742 | val_0_rmse: 0.38544 | val_1_rmse: 0.62968 |  0:01:21s
epoch 49 | loss: 0.16486 | val_0_rmse: 0.39631 | val_1_rmse: 0.59316 |  0:01:22s
epoch 50 | loss: 0.16469 | val_0_rmse: 0.38272 | val_1_rmse: 0.5892  |  0:01:24s
epoch 51 | loss: 0.16456 | val_0_rmse: 0.4122  | val_1_rmse: 0.62536 |  0:01:26s
epoch 52 | loss: 0.16244 | val_0_rmse: 0.38322 | val_1_rmse: 0.77335 |  0:01:27s
epoch 53 | loss: 0.1618  | val_0_rmse: 0.38869 | val_1_rmse: 0.77034 |  0:01:29s
epoch 54 | loss: 0.1607  | val_0_rmse: 0.40026 | val_1_rmse: 0.60299 |  0:01:31s
epoch 55 | loss: 0.16154 | val_0_rmse: 0.38205 | val_1_rmse: 0.69743 |  0:01:32s
epoch 56 | loss: 0.1607  | val_0_rmse: 0.47505 | val_1_rmse: 0.71684 |  0:01:34s
epoch 57 | loss: 0.15982 | val_0_rmse: 0.37844 | val_1_rmse: 0.69689 |  0:01:36s
epoch 58 | loss: 0.15915 | val_0_rmse: 0.396   | val_1_rmse: 0.85201 |  0:01:37s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.46388
Best weights from best epoch are automatically used!
ended training at: 22:12:37
Feature importance:
Mean squared error is of 869844842.3529803
Mean absolute error:20431.501089625428
MAPE:0.25547500881514085
R2 score:0.8105537402962436
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:12:37
epoch 0  | loss: 1.47215 | val_0_rmse: 0.99063 | val_1_rmse: 0.97819 |  0:00:01s
epoch 1  | loss: 0.77753 | val_0_rmse: 0.7953  | val_1_rmse: 0.78067 |  0:00:03s
epoch 2  | loss: 0.45963 | val_0_rmse: 0.74879 | val_1_rmse: 0.73844 |  0:00:04s
epoch 3  | loss: 0.35685 | val_0_rmse: 0.64162 | val_1_rmse: 0.63743 |  0:00:06s
epoch 4  | loss: 0.30701 | val_0_rmse: 0.59624 | val_1_rmse: 0.59484 |  0:00:08s
epoch 5  | loss: 0.2795  | val_0_rmse: 0.61853 | val_1_rmse: 0.61406 |  0:00:09s
epoch 6  | loss: 0.27076 | val_0_rmse: 0.55737 | val_1_rmse: 0.5608  |  0:00:11s
epoch 7  | loss: 0.25177 | val_0_rmse: 0.55594 | val_1_rmse: 0.5607  |  0:00:13s
epoch 8  | loss: 0.23356 | val_0_rmse: 0.51282 | val_1_rmse: 0.52081 |  0:00:14s
epoch 9  | loss: 0.2235  | val_0_rmse: 0.52471 | val_1_rmse: 0.53005 |  0:00:16s
epoch 10 | loss: 0.21867 | val_0_rmse: 0.5342  | val_1_rmse: 0.54078 |  0:00:18s
epoch 11 | loss: 0.21549 | val_0_rmse: 0.5047  | val_1_rmse: 0.51066 |  0:00:19s
epoch 12 | loss: 0.20782 | val_0_rmse: 0.48754 | val_1_rmse: 0.49501 |  0:00:21s
epoch 13 | loss: 0.20819 | val_0_rmse: 0.48812 | val_1_rmse: 0.49732 |  0:00:23s
epoch 14 | loss: 0.21035 | val_0_rmse: 0.48367 | val_1_rmse: 0.49309 |  0:00:24s
epoch 15 | loss: 0.20146 | val_0_rmse: 0.47671 | val_1_rmse: 0.48963 |  0:00:26s
epoch 16 | loss: 0.19804 | val_0_rmse: 0.47135 | val_1_rmse: 0.48334 |  0:00:28s
epoch 17 | loss: 0.20216 | val_0_rmse: 0.46578 | val_1_rmse: 0.48281 |  0:00:29s
epoch 18 | loss: 0.19641 | val_0_rmse: 0.47053 | val_1_rmse: 0.48776 |  0:00:31s
epoch 19 | loss: 0.19684 | val_0_rmse: 0.46106 | val_1_rmse: 0.47976 |  0:00:33s
epoch 20 | loss: 0.19137 | val_0_rmse: 0.46519 | val_1_rmse: 0.48037 |  0:00:34s
epoch 21 | loss: 0.19269 | val_0_rmse: 0.45198 | val_1_rmse: 0.47061 |  0:00:36s
epoch 22 | loss: 0.19753 | val_0_rmse: 0.45399 | val_1_rmse: 0.48325 |  0:00:38s
epoch 23 | loss: 0.19209 | val_0_rmse: 0.43595 | val_1_rmse: 0.45961 |  0:00:39s
epoch 24 | loss: 0.18742 | val_0_rmse: 0.43652 | val_1_rmse: 0.46006 |  0:00:41s
epoch 25 | loss: 0.18701 | val_0_rmse: 0.42581 | val_1_rmse: 0.45803 |  0:00:43s
epoch 26 | loss: 0.18398 | val_0_rmse: 0.42837 | val_1_rmse: 0.46232 |  0:00:44s
epoch 27 | loss: 0.18411 | val_0_rmse: 0.44432 | val_1_rmse: 0.4702  |  0:00:46s
epoch 28 | loss: 0.1881  | val_0_rmse: 0.42194 | val_1_rmse: 0.46137 |  0:00:48s
epoch 29 | loss: 0.18919 | val_0_rmse: 0.42638 | val_1_rmse: 0.45896 |  0:00:49s
epoch 30 | loss: 0.18542 | val_0_rmse: 0.41321 | val_1_rmse: 0.45872 |  0:00:51s
epoch 31 | loss: 0.18471 | val_0_rmse: 0.41476 | val_1_rmse: 0.45575 |  0:00:53s
epoch 32 | loss: 0.17981 | val_0_rmse: 0.40823 | val_1_rmse: 0.44887 |  0:00:54s
epoch 33 | loss: 0.18415 | val_0_rmse: 0.40994 | val_1_rmse: 0.45234 |  0:00:56s
epoch 34 | loss: 0.17981 | val_0_rmse: 0.41632 | val_1_rmse: 0.45468 |  0:00:58s
epoch 35 | loss: 0.17812 | val_0_rmse: 0.40786 | val_1_rmse: 0.45853 |  0:00:59s
epoch 36 | loss: 0.1802  | val_0_rmse: 0.40034 | val_1_rmse: 0.45393 |  0:01:01s
epoch 37 | loss: 0.17862 | val_0_rmse: 0.40406 | val_1_rmse: 0.45277 |  0:01:03s
epoch 38 | loss: 0.17788 | val_0_rmse: 0.40044 | val_1_rmse: 0.4503  |  0:01:04s
epoch 39 | loss: 0.17723 | val_0_rmse: 0.40777 | val_1_rmse: 0.45357 |  0:01:06s
epoch 40 | loss: 0.17836 | val_0_rmse: 0.39604 | val_1_rmse: 0.45236 |  0:01:07s
epoch 41 | loss: 0.18607 | val_0_rmse: 0.40635 | val_1_rmse: 0.4562  |  0:01:09s
epoch 42 | loss: 0.17634 | val_0_rmse: 0.40666 | val_1_rmse: 0.46567 |  0:01:11s
epoch 43 | loss: 0.17681 | val_0_rmse: 0.39958 | val_1_rmse: 0.4526  |  0:01:12s
epoch 44 | loss: 0.17463 | val_0_rmse: 0.39924 | val_1_rmse: 0.46337 |  0:01:14s
epoch 45 | loss: 0.1732  | val_0_rmse: 0.39891 | val_1_rmse: 0.45436 |  0:01:16s
epoch 46 | loss: 0.17738 | val_0_rmse: 0.39755 | val_1_rmse: 0.46267 |  0:01:17s
epoch 47 | loss: 0.17905 | val_0_rmse: 0.39816 | val_1_rmse: 0.45661 |  0:01:19s
epoch 48 | loss: 0.17053 | val_0_rmse: 0.39825 | val_1_rmse: 0.45681 |  0:01:21s
epoch 49 | loss: 0.17424 | val_0_rmse: 0.39958 | val_1_rmse: 0.46324 |  0:01:22s
epoch 50 | loss: 0.17337 | val_0_rmse: 0.40639 | val_1_rmse: 0.46193 |  0:01:24s
epoch 51 | loss: 0.17522 | val_0_rmse: 0.40238 | val_1_rmse: 0.47384 |  0:01:26s
epoch 52 | loss: 0.17663 | val_0_rmse: 0.40415 | val_1_rmse: 0.46237 |  0:01:27s
epoch 53 | loss: 0.18426 | val_0_rmse: 0.40784 | val_1_rmse: 0.47556 |  0:01:29s
epoch 54 | loss: 0.17816 | val_0_rmse: 0.3953  | val_1_rmse: 0.46342 |  0:01:31s
epoch 55 | loss: 0.17129 | val_0_rmse: 0.40198 | val_1_rmse: 0.46153 |  0:01:32s
epoch 56 | loss: 0.1679  | val_0_rmse: 0.40301 | val_1_rmse: 0.47436 |  0:01:34s
epoch 57 | loss: 0.17325 | val_0_rmse: 0.42026 | val_1_rmse: 0.47503 |  0:01:36s
epoch 58 | loss: 0.17554 | val_0_rmse: 0.40001 | val_1_rmse: 0.47689 |  0:01:37s
epoch 59 | loss: 0.16999 | val_0_rmse: 0.39496 | val_1_rmse: 0.46873 |  0:01:39s
epoch 60 | loss: 0.17297 | val_0_rmse: 0.40013 | val_1_rmse: 0.46276 |  0:01:41s
epoch 61 | loss: 0.16912 | val_0_rmse: 0.39562 | val_1_rmse: 0.47629 |  0:01:42s
epoch 62 | loss: 0.17093 | val_0_rmse: 0.40983 | val_1_rmse: 0.4713  |  0:01:44s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.44887
Best weights from best epoch are automatically used!
ended training at: 22:14:22
Feature importance:
Mean squared error is of 867694386.9300355
Mean absolute error:19927.939140322385
MAPE:0.22476272772087022
R2 score:0.8046633677115032
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:14:23
epoch 0  | loss: 0.63704 | val_0_rmse: 0.69662 | val_1_rmse: 0.69358 |  0:00:03s
epoch 1  | loss: 0.34967 | val_0_rmse: 0.62311 | val_1_rmse: 0.62674 |  0:00:06s
epoch 2  | loss: 0.29517 | val_0_rmse: 0.56677 | val_1_rmse: 0.56769 |  0:00:10s
epoch 3  | loss: 0.2614  | val_0_rmse: 0.52507 | val_1_rmse: 0.53128 |  0:00:13s
epoch 4  | loss: 0.24477 | val_0_rmse: 0.5575  | val_1_rmse: 0.56133 |  0:00:17s
epoch 5  | loss: 0.23068 | val_0_rmse: 0.49125 | val_1_rmse: 0.49654 |  0:00:20s
epoch 6  | loss: 0.2186  | val_0_rmse: 0.4736  | val_1_rmse: 0.47945 |  0:00:24s
epoch 7  | loss: 0.21334 | val_0_rmse: 0.45395 | val_1_rmse: 0.45889 |  0:00:27s
epoch 8  | loss: 0.20982 | val_0_rmse: 0.45733 | val_1_rmse: 0.46181 |  0:00:30s
epoch 9  | loss: 0.2119  | val_0_rmse: 0.44658 | val_1_rmse: 0.45287 |  0:00:34s
epoch 10 | loss: 0.20822 | val_0_rmse: 0.44453 | val_1_rmse: 0.44946 |  0:00:37s
epoch 11 | loss: 0.20379 | val_0_rmse: 0.43    | val_1_rmse: 0.43945 |  0:00:41s
epoch 12 | loss: 0.20061 | val_0_rmse: 0.42173 | val_1_rmse: 0.43121 |  0:00:44s
epoch 13 | loss: 0.20008 | val_0_rmse: 0.4284  | val_1_rmse: 0.43787 |  0:00:48s
epoch 14 | loss: 0.19784 | val_0_rmse: 0.4238  | val_1_rmse: 0.43354 |  0:00:51s
epoch 15 | loss: 0.19407 | val_0_rmse: 0.42024 | val_1_rmse: 0.43047 |  0:00:54s
epoch 16 | loss: 0.20026 | val_0_rmse: 0.42298 | val_1_rmse: 0.43181 |  0:00:58s
epoch 17 | loss: 0.19679 | val_0_rmse: 0.43593 | val_1_rmse: 0.44738 |  0:01:01s
epoch 18 | loss: 0.18995 | val_0_rmse: 0.43034 | val_1_rmse: 0.44144 |  0:01:05s
epoch 19 | loss: 0.18824 | val_0_rmse: 0.43327 | val_1_rmse: 0.44311 |  0:01:08s
epoch 20 | loss: 0.18601 | val_0_rmse: 0.41444 | val_1_rmse: 0.42534 |  0:01:11s
epoch 21 | loss: 0.18604 | val_0_rmse: 0.41351 | val_1_rmse: 0.42553 |  0:01:15s
epoch 22 | loss: 0.18428 | val_0_rmse: 0.41549 | val_1_rmse: 0.42639 |  0:01:18s
epoch 23 | loss: 0.18422 | val_0_rmse: 0.42789 | val_1_rmse: 0.43922 |  0:01:22s
epoch 24 | loss: 0.18731 | val_0_rmse: 0.41844 | val_1_rmse: 0.43019 |  0:01:25s
epoch 25 | loss: 0.19027 | val_0_rmse: 0.44827 | val_1_rmse: 0.457   |  0:01:29s
epoch 26 | loss: 0.18956 | val_0_rmse: 0.42231 | val_1_rmse: 0.43546 |  0:01:32s
epoch 27 | loss: 0.18239 | val_0_rmse: 0.40827 | val_1_rmse: 0.42084 |  0:01:35s
epoch 28 | loss: 0.18558 | val_0_rmse: 0.4211  | val_1_rmse: 0.43489 |  0:01:39s
epoch 29 | loss: 0.185   | val_0_rmse: 0.42177 | val_1_rmse: 0.43524 |  0:01:42s
epoch 30 | loss: 0.18821 | val_0_rmse: 0.44514 | val_1_rmse: 0.45879 |  0:01:46s
epoch 31 | loss: 0.18846 | val_0_rmse: 0.41012 | val_1_rmse: 0.42258 |  0:01:49s
epoch 32 | loss: 0.18522 | val_0_rmse: 0.41484 | val_1_rmse: 0.42855 |  0:01:53s
epoch 33 | loss: 0.18198 | val_0_rmse: 0.41134 | val_1_rmse: 0.42376 |  0:01:56s
epoch 34 | loss: 0.18081 | val_0_rmse: 0.41081 | val_1_rmse: 0.42585 |  0:01:59s
epoch 35 | loss: 0.1773  | val_0_rmse: 0.41224 | val_1_rmse: 0.42631 |  0:02:03s
epoch 36 | loss: 0.17595 | val_0_rmse: 0.4053  | val_1_rmse: 0.41891 |  0:02:06s
epoch 37 | loss: 0.17671 | val_0_rmse: 0.41025 | val_1_rmse: 0.42344 |  0:02:10s
epoch 38 | loss: 0.17823 | val_0_rmse: 0.40879 | val_1_rmse: 0.42357 |  0:02:13s
epoch 39 | loss: 0.19383 | val_0_rmse: 0.4289  | val_1_rmse: 0.43949 |  0:02:17s
epoch 40 | loss: 0.19135 | val_0_rmse: 0.42451 | val_1_rmse: 0.43475 |  0:02:20s
epoch 41 | loss: 0.19327 | val_0_rmse: 0.42478 | val_1_rmse: 0.43719 |  0:02:23s
epoch 42 | loss: 0.18736 | val_0_rmse: 0.4107  | val_1_rmse: 0.42392 |  0:02:27s
epoch 43 | loss: 0.18693 | val_0_rmse: 0.44816 | val_1_rmse: 0.45869 |  0:02:30s
epoch 44 | loss: 0.1873  | val_0_rmse: 0.41226 | val_1_rmse: 0.42554 |  0:02:34s
epoch 45 | loss: 0.17991 | val_0_rmse: 0.42041 | val_1_rmse: 0.4345  |  0:02:37s
epoch 46 | loss: 0.17894 | val_0_rmse: 0.46207 | val_1_rmse: 0.47187 |  0:02:41s
epoch 47 | loss: 0.18512 | val_0_rmse: 0.41501 | val_1_rmse: 0.42629 |  0:02:44s
epoch 48 | loss: 0.1785  | val_0_rmse: 0.41685 | val_1_rmse: 0.43393 |  0:02:47s
epoch 49 | loss: 0.1802  | val_0_rmse: 0.42664 | val_1_rmse: 0.43923 |  0:02:51s
epoch 50 | loss: 0.18973 | val_0_rmse: 0.42041 | val_1_rmse: 0.43115 |  0:02:54s
epoch 51 | loss: 0.18241 | val_0_rmse: 0.42506 | val_1_rmse: 0.43701 |  0:02:58s
epoch 52 | loss: 0.20187 | val_0_rmse: 0.47664 | val_1_rmse: 0.48258 |  0:03:01s
epoch 53 | loss: 0.19184 | val_0_rmse: 0.41985 | val_1_rmse: 0.42884 |  0:03:05s
epoch 54 | loss: 0.1939  | val_0_rmse: 0.45402 | val_1_rmse: 0.45995 |  0:03:08s
epoch 55 | loss: 0.22931 | val_0_rmse: 0.45867 | val_1_rmse: 0.46378 |  0:03:11s
epoch 56 | loss: 0.22388 | val_0_rmse: 0.43527 | val_1_rmse: 0.44341 |  0:03:15s
epoch 57 | loss: 0.20504 | val_0_rmse: 0.45636 | val_1_rmse: 0.4653  |  0:03:18s
epoch 58 | loss: 0.18888 | val_0_rmse: 0.42635 | val_1_rmse: 0.43492 |  0:03:22s
epoch 59 | loss: 0.18642 | val_0_rmse: 0.41818 | val_1_rmse: 0.42734 |  0:03:25s
epoch 60 | loss: 0.19005 | val_0_rmse: 0.42898 | val_1_rmse: 0.43658 |  0:03:28s
epoch 61 | loss: 0.18474 | val_0_rmse: 0.40846 | val_1_rmse: 0.41768 |  0:03:32s
epoch 62 | loss: 0.17942 | val_0_rmse: 0.40919 | val_1_rmse: 0.41741 |  0:03:35s
epoch 63 | loss: 0.17935 | val_0_rmse: 0.43856 | val_1_rmse: 0.45044 |  0:03:39s
epoch 64 | loss: 0.18256 | val_0_rmse: 0.40915 | val_1_rmse: 0.4213  |  0:03:42s
epoch 65 | loss: 0.17821 | val_0_rmse: 0.40579 | val_1_rmse: 0.41756 |  0:03:46s
epoch 66 | loss: 0.17634 | val_0_rmse: 0.41417 | val_1_rmse: 0.42714 |  0:03:49s
epoch 67 | loss: 0.17114 | val_0_rmse: 0.39983 | val_1_rmse: 0.41133 |  0:03:52s
epoch 68 | loss: 0.17377 | val_0_rmse: 0.40334 | val_1_rmse: 0.41668 |  0:03:56s
epoch 69 | loss: 0.17844 | val_0_rmse: 0.44376 | val_1_rmse: 0.45561 |  0:03:59s
epoch 70 | loss: 0.17314 | val_0_rmse: 0.40689 | val_1_rmse: 0.41995 |  0:04:03s
epoch 71 | loss: 0.17325 | val_0_rmse: 0.42073 | val_1_rmse: 0.43384 |  0:04:06s
epoch 72 | loss: 0.17731 | val_0_rmse: 0.40609 | val_1_rmse: 0.41973 |  0:04:10s
epoch 73 | loss: 0.17106 | val_0_rmse: 0.39213 | val_1_rmse: 0.40572 |  0:04:13s
epoch 74 | loss: 0.17147 | val_0_rmse: 0.42782 | val_1_rmse: 0.44459 |  0:04:16s
epoch 75 | loss: 0.17927 | val_0_rmse: 0.40026 | val_1_rmse: 0.41472 |  0:04:20s
epoch 76 | loss: 0.17652 | val_0_rmse: 0.44884 | val_1_rmse: 0.45803 |  0:04:23s
epoch 77 | loss: 0.21633 | val_0_rmse: 0.4626  | val_1_rmse: 0.47578 |  0:04:27s
epoch 78 | loss: 0.18926 | val_0_rmse: 0.41532 | val_1_rmse: 0.42747 |  0:04:30s
epoch 79 | loss: 0.18185 | val_0_rmse: 0.4113  | val_1_rmse: 0.4252  |  0:04:34s
epoch 80 | loss: 0.17879 | val_0_rmse: 0.41393 | val_1_rmse: 0.42638 |  0:04:37s
epoch 81 | loss: 0.1882  | val_0_rmse: 0.41574 | val_1_rmse: 0.4423  |  0:04:40s
epoch 82 | loss: 0.17901 | val_0_rmse: 0.44291 | val_1_rmse: 0.46419 |  0:04:44s
epoch 83 | loss: 0.17985 | val_0_rmse: 0.40229 | val_1_rmse: 0.4187  |  0:04:47s
epoch 84 | loss: 0.17606 | val_0_rmse: 0.42958 | val_1_rmse: 0.44459 |  0:04:51s
epoch 85 | loss: 0.17766 | val_0_rmse: 0.42374 | val_1_rmse: 0.43889 |  0:04:54s
epoch 86 | loss: 0.18102 | val_0_rmse: 0.43024 | val_1_rmse: 0.44261 |  0:04:58s
epoch 87 | loss: 0.20131 | val_0_rmse: 0.56316 | val_1_rmse: 0.57039 |  0:05:01s
epoch 88 | loss: 0.20188 | val_0_rmse: 0.41931 | val_1_rmse: 0.43068 |  0:05:04s
epoch 89 | loss: 0.18823 | val_0_rmse: 0.42054 | val_1_rmse: 0.43206 |  0:05:08s
epoch 90 | loss: 0.18348 | val_0_rmse: 0.4399  | val_1_rmse: 0.45418 |  0:05:11s
epoch 91 | loss: 0.17847 | val_0_rmse: 0.40147 | val_1_rmse: 0.41702 |  0:05:15s
epoch 92 | loss: 0.17448 | val_0_rmse: 0.40971 | val_1_rmse: 0.42504 |  0:05:18s
epoch 93 | loss: 0.17441 | val_0_rmse: 0.40117 | val_1_rmse: 0.41745 |  0:05:21s
epoch 94 | loss: 0.17217 | val_0_rmse: 0.40079 | val_1_rmse: 0.41666 |  0:05:25s
epoch 95 | loss: 0.17283 | val_0_rmse: 0.40187 | val_1_rmse: 0.41821 |  0:05:28s
epoch 96 | loss: 0.1719  | val_0_rmse: 0.39367 | val_1_rmse: 0.41056 |  0:05:32s
epoch 97 | loss: 0.16979 | val_0_rmse: 0.39566 | val_1_rmse: 0.41354 |  0:05:35s
epoch 98 | loss: 0.17515 | val_0_rmse: 0.40262 | val_1_rmse: 0.41982 |  0:05:39s
epoch 99 | loss: 0.1704  | val_0_rmse: 0.40564 | val_1_rmse: 0.42225 |  0:05:42s
epoch 100| loss: 0.16832 | val_0_rmse: 0.40061 | val_1_rmse: 0.41997 |  0:05:46s
epoch 101| loss: 0.16888 | val_0_rmse: 0.4022  | val_1_rmse: 0.42025 |  0:05:49s
epoch 102| loss: 0.169   | val_0_rmse: 0.3971  | val_1_rmse: 0.41597 |  0:05:52s
epoch 103| loss: 0.167   | val_0_rmse: 0.39402 | val_1_rmse: 0.41391 |  0:05:56s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.40572
Best weights from best epoch are automatically used!
ended training at: 22:20:20
Feature importance:
Mean squared error is of 9248100575.970734
Mean absolute error:66177.15032146894
MAPE:0.27329045269579516
R2 score:0.8407444763141179
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:20:21
epoch 0  | loss: 0.64428 | val_0_rmse: 0.67091 | val_1_rmse: 0.66325 |  0:00:03s
epoch 1  | loss: 0.36614 | val_0_rmse: 0.65599 | val_1_rmse: 0.64983 |  0:00:06s
epoch 2  | loss: 0.31073 | val_0_rmse: 0.6278  | val_1_rmse: 0.62435 |  0:00:10s
epoch 3  | loss: 0.29031 | val_0_rmse: 0.65355 | val_1_rmse: 0.65712 |  0:00:13s
epoch 4  | loss: 0.28576 | val_0_rmse: 0.59958 | val_1_rmse: 0.60098 |  0:00:17s
epoch 5  | loss: 0.2661  | val_0_rmse: 0.59317 | val_1_rmse: 0.59997 |  0:00:20s
epoch 6  | loss: 0.2558  | val_0_rmse: 0.53401 | val_1_rmse: 0.52289 |  0:00:23s
epoch 7  | loss: 0.25872 | val_0_rmse: 0.49805 | val_1_rmse: 0.49276 |  0:00:27s
epoch 8  | loss: 0.25914 | val_0_rmse: 0.4961  | val_1_rmse: 0.48768 |  0:00:30s
epoch 9  | loss: 0.25557 | val_0_rmse: 0.50398 | val_1_rmse: 0.50107 |  0:00:34s
epoch 10 | loss: 0.24783 | val_0_rmse: 0.48267 | val_1_rmse: 0.47598 |  0:00:37s
epoch 11 | loss: 0.24957 | val_0_rmse: 0.48182 | val_1_rmse: 0.47389 |  0:00:41s
epoch 12 | loss: 0.23964 | val_0_rmse: 0.46883 | val_1_rmse: 0.46127 |  0:00:44s
epoch 13 | loss: 0.24632 | val_0_rmse: 0.50308 | val_1_rmse: 0.50193 |  0:00:47s
epoch 14 | loss: 0.24926 | val_0_rmse: 0.47101 | val_1_rmse: 0.46871 |  0:00:51s
epoch 15 | loss: 0.23092 | val_0_rmse: 0.47534 | val_1_rmse: 0.47136 |  0:00:54s
epoch 16 | loss: 0.2265  | val_0_rmse: 0.46743 | val_1_rmse: 0.45973 |  0:00:58s
epoch 17 | loss: 0.22615 | val_0_rmse: 0.46299 | val_1_rmse: 0.46207 |  0:01:01s
epoch 18 | loss: 0.22421 | val_0_rmse: 0.4703  | val_1_rmse: 0.46674 |  0:01:05s
epoch 19 | loss: 0.21931 | val_0_rmse: 0.49919 | val_1_rmse: 0.49814 |  0:01:08s
epoch 20 | loss: 0.2191  | val_0_rmse: 0.47923 | val_1_rmse: 0.47511 |  0:01:11s
epoch 21 | loss: 0.21458 | val_0_rmse: 0.47216 | val_1_rmse: 0.46544 |  0:01:15s
epoch 22 | loss: 0.20678 | val_0_rmse: 0.46952 | val_1_rmse: 0.4699  |  0:01:18s
epoch 23 | loss: 0.20244 | val_0_rmse: 0.43413 | val_1_rmse: 0.42744 |  0:01:22s
epoch 24 | loss: 0.19841 | val_0_rmse: 0.44999 | val_1_rmse: 0.44749 |  0:01:25s
epoch 25 | loss: 0.2031  | val_0_rmse: 0.42515 | val_1_rmse: 0.42166 |  0:01:29s
epoch 26 | loss: 0.19808 | val_0_rmse: 0.42348 | val_1_rmse: 0.422   |  0:01:32s
epoch 27 | loss: 0.19368 | val_0_rmse: 0.41938 | val_1_rmse: 0.41843 |  0:01:35s
epoch 28 | loss: 0.1909  | val_0_rmse: 0.44578 | val_1_rmse: 0.44284 |  0:01:39s
epoch 29 | loss: 0.18947 | val_0_rmse: 0.4178  | val_1_rmse: 0.4143  |  0:01:42s
epoch 30 | loss: 0.18768 | val_0_rmse: 0.41369 | val_1_rmse: 0.41209 |  0:01:46s
epoch 31 | loss: 0.19135 | val_0_rmse: 0.43506 | val_1_rmse: 0.43191 |  0:01:49s
epoch 32 | loss: 0.18655 | val_0_rmse: 0.43887 | val_1_rmse: 0.44062 |  0:01:53s
epoch 33 | loss: 0.18635 | val_0_rmse: 0.4243  | val_1_rmse: 0.42561 |  0:01:56s
epoch 34 | loss: 0.18456 | val_0_rmse: 0.45145 | val_1_rmse: 0.44961 |  0:01:59s
epoch 35 | loss: 0.18379 | val_0_rmse: 0.4125  | val_1_rmse: 0.41171 |  0:02:03s
epoch 36 | loss: 0.18391 | val_0_rmse: 0.44292 | val_1_rmse: 0.44261 |  0:02:06s
epoch 37 | loss: 0.18314 | val_0_rmse: 0.43319 | val_1_rmse: 0.43294 |  0:02:10s
epoch 38 | loss: 0.1796  | val_0_rmse: 0.42939 | val_1_rmse: 0.43037 |  0:02:13s
epoch 39 | loss: 0.17827 | val_0_rmse: 0.40502 | val_1_rmse: 0.40612 |  0:02:16s
epoch 40 | loss: 0.17835 | val_0_rmse: 0.434   | val_1_rmse: 0.43406 |  0:02:20s
epoch 41 | loss: 0.178   | val_0_rmse: 0.4058  | val_1_rmse: 0.40623 |  0:02:23s
epoch 42 | loss: 0.17806 | val_0_rmse: 0.40542 | val_1_rmse: 0.40847 |  0:02:27s
epoch 43 | loss: 0.17614 | val_0_rmse: 0.41622 | val_1_rmse: 0.41667 |  0:02:30s
epoch 44 | loss: 0.18177 | val_0_rmse: 0.40747 | val_1_rmse: 0.41071 |  0:02:34s
epoch 45 | loss: 0.17515 | val_0_rmse: 0.40832 | val_1_rmse: 0.41156 |  0:02:37s
epoch 46 | loss: 0.17486 | val_0_rmse: 0.406   | val_1_rmse: 0.40642 |  0:02:40s
epoch 47 | loss: 0.17285 | val_0_rmse: 0.3995  | val_1_rmse: 0.40113 |  0:02:44s
epoch 48 | loss: 0.17398 | val_0_rmse: 0.4133  | val_1_rmse: 0.41348 |  0:02:47s
epoch 49 | loss: 0.17422 | val_0_rmse: 0.41011 | val_1_rmse: 0.41421 |  0:02:51s
epoch 50 | loss: 0.17166 | val_0_rmse: 0.41703 | val_1_rmse: 0.41957 |  0:02:54s
epoch 51 | loss: 0.17592 | val_0_rmse: 0.40304 | val_1_rmse: 0.40638 |  0:02:57s
epoch 52 | loss: 0.17889 | val_0_rmse: 0.43037 | val_1_rmse: 0.43177 |  0:03:01s
epoch 53 | loss: 0.17756 | val_0_rmse: 0.41526 | val_1_rmse: 0.41918 |  0:03:04s
epoch 54 | loss: 0.1745  | val_0_rmse: 0.41234 | val_1_rmse: 0.41512 |  0:03:08s
epoch 55 | loss: 0.17283 | val_0_rmse: 0.39839 | val_1_rmse: 0.39951 |  0:03:11s
epoch 56 | loss: 0.17338 | val_0_rmse: 0.40224 | val_1_rmse: 0.40326 |  0:03:14s
epoch 57 | loss: 0.17374 | val_0_rmse: 0.39649 | val_1_rmse: 0.40185 |  0:03:18s
epoch 58 | loss: 0.17216 | val_0_rmse: 0.42422 | val_1_rmse: 0.4286  |  0:03:21s
epoch 59 | loss: 0.16966 | val_0_rmse: 0.44855 | val_1_rmse: 0.4502  |  0:03:25s
epoch 60 | loss: 0.17315 | val_0_rmse: 0.4244  | val_1_rmse: 0.42399 |  0:03:28s
epoch 61 | loss: 0.17204 | val_0_rmse: 0.41217 | val_1_rmse: 0.41664 |  0:03:31s
epoch 62 | loss: 0.16925 | val_0_rmse: 0.41125 | val_1_rmse: 0.41733 |  0:03:35s
epoch 63 | loss: 0.17031 | val_0_rmse: 0.39502 | val_1_rmse: 0.40217 |  0:03:38s
epoch 64 | loss: 0.19784 | val_0_rmse: 0.67022 | val_1_rmse: 0.66936 |  0:03:42s
epoch 65 | loss: 0.31081 | val_0_rmse: 0.49685 | val_1_rmse: 0.49544 |  0:03:45s
epoch 66 | loss: 0.23725 | val_0_rmse: 0.52336 | val_1_rmse: 0.51867 |  0:03:49s
epoch 67 | loss: 0.21519 | val_0_rmse: 0.4643  | val_1_rmse: 0.45644 |  0:03:52s
epoch 68 | loss: 0.21174 | val_0_rmse: 0.44131 | val_1_rmse: 0.43756 |  0:03:55s
epoch 69 | loss: 0.19625 | val_0_rmse: 0.44109 | val_1_rmse: 0.4394  |  0:03:59s
epoch 70 | loss: 0.21203 | val_0_rmse: 0.46041 | val_1_rmse: 0.45381 |  0:04:02s
epoch 71 | loss: 0.21919 | val_0_rmse: 0.4367  | val_1_rmse: 0.43055 |  0:04:06s
epoch 72 | loss: 0.19767 | val_0_rmse: 0.43571 | val_1_rmse: 0.43064 |  0:04:09s
epoch 73 | loss: 0.1955  | val_0_rmse: 0.43664 | val_1_rmse: 0.43581 |  0:04:12s
epoch 74 | loss: 0.19264 | val_0_rmse: 0.45439 | val_1_rmse: 0.44951 |  0:04:16s
epoch 75 | loss: 0.18924 | val_0_rmse: 0.41694 | val_1_rmse: 0.416   |  0:04:19s
epoch 76 | loss: 0.1842  | val_0_rmse: 0.41579 | val_1_rmse: 0.41631 |  0:04:23s
epoch 77 | loss: 0.18299 | val_0_rmse: 0.42536 | val_1_rmse: 0.42639 |  0:04:26s
epoch 78 | loss: 0.18537 | val_0_rmse: 0.4113  | val_1_rmse: 0.41048 |  0:04:29s
epoch 79 | loss: 0.18184 | val_0_rmse: 0.41617 | val_1_rmse: 0.41596 |  0:04:33s
epoch 80 | loss: 0.1811  | val_0_rmse: 0.40655 | val_1_rmse: 0.4082  |  0:04:36s
epoch 81 | loss: 0.18063 | val_0_rmse: 0.47475 | val_1_rmse: 0.47771 |  0:04:40s
epoch 82 | loss: 0.18417 | val_0_rmse: 0.41881 | val_1_rmse: 0.42046 |  0:04:43s
epoch 83 | loss: 0.17851 | val_0_rmse: 0.41235 | val_1_rmse: 0.41177 |  0:04:46s
epoch 84 | loss: 0.18823 | val_0_rmse: 0.42949 | val_1_rmse: 0.42922 |  0:04:50s
epoch 85 | loss: 0.18009 | val_0_rmse: 0.42106 | val_1_rmse: 0.42158 |  0:04:53s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.39951
Best weights from best epoch are automatically used!
ended training at: 22:25:16
Feature importance:
Mean squared error is of 9606003475.680798
Mean absolute error:67449.79233160181
MAPE:0.29801580573186165
R2 score:0.8309370287286477
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:25:16
epoch 0  | loss: 0.63056 | val_0_rmse: 0.70079 | val_1_rmse: 0.70258 |  0:00:03s
epoch 1  | loss: 0.33569 | val_0_rmse: 0.63637 | val_1_rmse: 0.6386  |  0:00:06s
epoch 2  | loss: 0.28367 | val_0_rmse: 0.64205 | val_1_rmse: 0.64527 |  0:00:10s
epoch 3  | loss: 0.26149 | val_0_rmse: 0.59676 | val_1_rmse: 0.60266 |  0:00:13s
epoch 4  | loss: 0.24838 | val_0_rmse: 0.58389 | val_1_rmse: 0.58744 |  0:00:17s
epoch 5  | loss: 0.23537 | val_0_rmse: 0.48925 | val_1_rmse: 0.49558 |  0:00:20s
epoch 6  | loss: 0.22278 | val_0_rmse: 0.50506 | val_1_rmse: 0.51205 |  0:00:24s
epoch 7  | loss: 0.2195  | val_0_rmse: 0.47072 | val_1_rmse: 0.47682 |  0:00:27s
epoch 8  | loss: 0.2218  | val_0_rmse: 0.4744  | val_1_rmse: 0.48068 |  0:00:30s
epoch 9  | loss: 0.21233 | val_0_rmse: 0.44363 | val_1_rmse: 0.45868 |  0:00:34s
epoch 10 | loss: 0.22741 | val_0_rmse: 0.49824 | val_1_rmse: 0.50813 |  0:00:37s
epoch 11 | loss: 0.25307 | val_0_rmse: 0.49551 | val_1_rmse: 0.50612 |  0:00:41s
epoch 12 | loss: 0.22751 | val_0_rmse: 0.48644 | val_1_rmse: 0.49507 |  0:00:44s
epoch 13 | loss: 0.23289 | val_0_rmse: 0.46157 | val_1_rmse: 0.4732  |  0:00:47s
epoch 14 | loss: 0.2231  | val_0_rmse: 0.44979 | val_1_rmse: 0.46177 |  0:00:51s
epoch 15 | loss: 0.21647 | val_0_rmse: 0.45222 | val_1_rmse: 0.46443 |  0:00:54s
epoch 16 | loss: 0.21916 | val_0_rmse: 0.4991  | val_1_rmse: 0.51469 |  0:00:58s
epoch 17 | loss: 0.21847 | val_0_rmse: 0.46544 | val_1_rmse: 0.48141 |  0:01:01s
epoch 18 | loss: 0.25106 | val_0_rmse: 0.48786 | val_1_rmse: 0.49918 |  0:01:05s
epoch 19 | loss: 0.26728 | val_0_rmse: 0.5402  | val_1_rmse: 0.55301 |  0:01:08s
epoch 20 | loss: 0.23903 | val_0_rmse: 0.46792 | val_1_rmse: 0.4851  |  0:01:11s
epoch 21 | loss: 0.22785 | val_0_rmse: 0.49323 | val_1_rmse: 0.49968 |  0:01:15s
epoch 22 | loss: 0.21886 | val_0_rmse: 0.44884 | val_1_rmse: 0.46095 |  0:01:18s
epoch 23 | loss: 0.21103 | val_0_rmse: 0.44733 | val_1_rmse: 0.46122 |  0:01:22s
epoch 24 | loss: 0.20836 | val_0_rmse: 0.45009 | val_1_rmse: 0.46141 |  0:01:25s
epoch 25 | loss: 0.20742 | val_0_rmse: 0.45786 | val_1_rmse: 0.47    |  0:01:29s
epoch 26 | loss: 0.20748 | val_0_rmse: 0.44    | val_1_rmse: 0.45328 |  0:01:32s
epoch 27 | loss: 0.20285 | val_0_rmse: 0.4599  | val_1_rmse: 0.46982 |  0:01:35s
epoch 28 | loss: 0.20717 | val_0_rmse: 0.44225 | val_1_rmse: 0.45325 |  0:01:39s
epoch 29 | loss: 0.20252 | val_0_rmse: 0.4306  | val_1_rmse: 0.44488 |  0:01:42s
epoch 30 | loss: 0.19968 | val_0_rmse: 0.43579 | val_1_rmse: 0.4514  |  0:01:46s
epoch 31 | loss: 0.20216 | val_0_rmse: 0.43085 | val_1_rmse: 0.44733 |  0:01:49s
epoch 32 | loss: 0.19776 | val_0_rmse: 0.43944 | val_1_rmse: 0.45257 |  0:01:53s
epoch 33 | loss: 0.19786 | val_0_rmse: 0.42582 | val_1_rmse: 0.44019 |  0:01:56s
epoch 34 | loss: 0.19758 | val_0_rmse: 0.43548 | val_1_rmse: 0.45312 |  0:01:59s
epoch 35 | loss: 0.1949  | val_0_rmse: 0.43141 | val_1_rmse: 0.44515 |  0:02:03s
epoch 36 | loss: 0.19265 | val_0_rmse: 0.42991 | val_1_rmse: 0.44612 |  0:02:06s
epoch 37 | loss: 0.19263 | val_0_rmse: 0.42108 | val_1_rmse: 0.43926 |  0:02:10s
epoch 38 | loss: 0.19382 | val_0_rmse: 0.42341 | val_1_rmse: 0.43904 |  0:02:13s
epoch 39 | loss: 0.18885 | val_0_rmse: 0.42133 | val_1_rmse: 0.43659 |  0:02:16s
epoch 40 | loss: 0.18968 | val_0_rmse: 0.435   | val_1_rmse: 0.44982 |  0:02:20s
epoch 41 | loss: 0.18807 | val_0_rmse: 0.42516 | val_1_rmse: 0.43939 |  0:02:23s
epoch 42 | loss: 0.18911 | val_0_rmse: 0.42345 | val_1_rmse: 0.43902 |  0:02:27s
epoch 43 | loss: 0.19    | val_0_rmse: 0.41689 | val_1_rmse: 0.43414 |  0:02:30s
epoch 44 | loss: 0.18742 | val_0_rmse: 0.41716 | val_1_rmse: 0.43482 |  0:02:34s
epoch 45 | loss: 0.18788 | val_0_rmse: 0.41869 | val_1_rmse: 0.43635 |  0:02:37s
epoch 46 | loss: 0.18782 | val_0_rmse: 0.42576 | val_1_rmse: 0.44275 |  0:02:40s
epoch 47 | loss: 0.18534 | val_0_rmse: 0.4275  | val_1_rmse: 0.44192 |  0:02:44s
epoch 48 | loss: 0.18667 | val_0_rmse: 0.41944 | val_1_rmse: 0.43816 |  0:02:47s
epoch 49 | loss: 0.1889  | val_0_rmse: 0.41845 | val_1_rmse: 0.43754 |  0:02:51s
epoch 50 | loss: 0.18736 | val_0_rmse: 0.43743 | val_1_rmse: 0.45263 |  0:02:54s
epoch 51 | loss: 0.17725 | val_0_rmse: 0.40316 | val_1_rmse: 0.42344 |  0:02:58s
epoch 52 | loss: 0.17516 | val_0_rmse: 0.41254 | val_1_rmse: 0.4339  |  0:03:01s
epoch 53 | loss: 0.17877 | val_0_rmse: 0.40866 | val_1_rmse: 0.43146 |  0:03:04s
epoch 54 | loss: 0.17926 | val_0_rmse: 0.41284 | val_1_rmse: 0.43316 |  0:03:08s
epoch 55 | loss: 0.17531 | val_0_rmse: 0.41082 | val_1_rmse: 0.43608 |  0:03:11s
epoch 56 | loss: 0.17835 | val_0_rmse: 0.41184 | val_1_rmse: 0.431   |  0:03:15s
epoch 57 | loss: 0.18511 | val_0_rmse: 0.41578 | val_1_rmse: 0.43643 |  0:03:18s
epoch 58 | loss: 0.17874 | val_0_rmse: 0.41318 | val_1_rmse: 0.4318  |  0:03:22s
epoch 59 | loss: 0.1819  | val_0_rmse: 0.41356 | val_1_rmse: 0.43218 |  0:03:25s
epoch 60 | loss: 0.1747  | val_0_rmse: 0.40852 | val_1_rmse: 0.42737 |  0:03:28s
epoch 61 | loss: 0.17625 | val_0_rmse: 0.42601 | val_1_rmse: 0.45088 |  0:03:32s
epoch 62 | loss: 0.17366 | val_0_rmse: 0.40047 | val_1_rmse: 0.42292 |  0:03:35s
epoch 63 | loss: 0.17398 | val_0_rmse: 0.39996 | val_1_rmse: 0.42086 |  0:03:39s
epoch 64 | loss: 0.17546 | val_0_rmse: 0.3993  | val_1_rmse: 0.42218 |  0:03:42s
epoch 65 | loss: 0.17191 | val_0_rmse: 0.40095 | val_1_rmse: 0.42356 |  0:03:46s
epoch 66 | loss: 0.17184 | val_0_rmse: 0.39899 | val_1_rmse: 0.42382 |  0:03:49s
epoch 67 | loss: 0.17416 | val_0_rmse: 0.41094 | val_1_rmse: 0.43204 |  0:03:52s
epoch 68 | loss: 0.17512 | val_0_rmse: 0.40055 | val_1_rmse: 0.42259 |  0:03:56s
epoch 69 | loss: 0.17194 | val_0_rmse: 0.39463 | val_1_rmse: 0.42009 |  0:03:59s
epoch 70 | loss: 0.17029 | val_0_rmse: 0.40032 | val_1_rmse: 0.42104 |  0:04:03s
epoch 71 | loss: 0.16853 | val_0_rmse: 0.39843 | val_1_rmse: 0.42088 |  0:04:06s
epoch 72 | loss: 0.17036 | val_0_rmse: 0.39715 | val_1_rmse: 0.42189 |  0:04:10s
epoch 73 | loss: 0.17194 | val_0_rmse: 0.39602 | val_1_rmse: 0.4182  |  0:04:13s
epoch 74 | loss: 0.17377 | val_0_rmse: 0.403   | val_1_rmse: 0.42567 |  0:04:16s
epoch 75 | loss: 0.17116 | val_0_rmse: 0.39231 | val_1_rmse: 0.41776 |  0:04:20s
epoch 76 | loss: 0.16888 | val_0_rmse: 0.40075 | val_1_rmse: 0.42445 |  0:04:23s
epoch 77 | loss: 0.17035 | val_0_rmse: 0.39692 | val_1_rmse: 0.42203 |  0:04:27s
epoch 78 | loss: 0.17006 | val_0_rmse: 0.39729 | val_1_rmse: 0.42166 |  0:04:30s
epoch 79 | loss: 0.16929 | val_0_rmse: 0.40356 | val_1_rmse: 0.42805 |  0:04:34s
epoch 80 | loss: 0.16967 | val_0_rmse: 0.3927  | val_1_rmse: 0.41894 |  0:04:37s
epoch 81 | loss: 0.16747 | val_0_rmse: 0.39633 | val_1_rmse: 0.42206 |  0:04:40s
epoch 82 | loss: 0.16815 | val_0_rmse: 0.39025 | val_1_rmse: 0.41652 |  0:04:44s
epoch 83 | loss: 0.16999 | val_0_rmse: 0.41181 | val_1_rmse: 0.43853 |  0:04:47s
epoch 84 | loss: 0.16796 | val_0_rmse: 0.3941  | val_1_rmse: 0.42182 |  0:04:51s
epoch 85 | loss: 0.17199 | val_0_rmse: 0.40147 | val_1_rmse: 0.42349 |  0:04:54s
epoch 86 | loss: 0.17533 | val_0_rmse: 0.43236 | val_1_rmse: 0.45717 |  0:04:58s
epoch 87 | loss: 0.17598 | val_0_rmse: 0.40519 | val_1_rmse: 0.43165 |  0:05:01s
epoch 88 | loss: 0.17477 | val_0_rmse: 0.40163 | val_1_rmse: 0.42895 |  0:05:04s
epoch 89 | loss: 0.1746  | val_0_rmse: 0.39746 | val_1_rmse: 0.42655 |  0:05:08s
epoch 90 | loss: 0.17485 | val_0_rmse: 0.39349 | val_1_rmse: 0.42152 |  0:05:11s
epoch 91 | loss: 0.16798 | val_0_rmse: 0.39615 | val_1_rmse: 0.42593 |  0:05:15s
epoch 92 | loss: 0.16897 | val_0_rmse: 0.39688 | val_1_rmse: 0.42429 |  0:05:18s
epoch 93 | loss: 0.1688  | val_0_rmse: 0.42118 | val_1_rmse: 0.44666 |  0:05:22s
epoch 94 | loss: 0.16896 | val_0_rmse: 0.39301 | val_1_rmse: 0.42334 |  0:05:25s
epoch 95 | loss: 0.16533 | val_0_rmse: 0.38905 | val_1_rmse: 0.41585 |  0:05:29s
epoch 96 | loss: 0.16698 | val_0_rmse: 0.39761 | val_1_rmse: 0.42383 |  0:05:32s
epoch 97 | loss: 0.16823 | val_0_rmse: 0.40951 | val_1_rmse: 0.43661 |  0:05:35s
epoch 98 | loss: 0.16694 | val_0_rmse: 0.39954 | val_1_rmse: 0.42468 |  0:05:39s
epoch 99 | loss: 0.16653 | val_0_rmse: 0.3902  | val_1_rmse: 0.42081 |  0:05:42s
epoch 100| loss: 0.16612 | val_0_rmse: 0.40178 | val_1_rmse: 0.42933 |  0:05:46s
epoch 101| loss: 0.16505 | val_0_rmse: 0.39222 | val_1_rmse: 0.42288 |  0:05:49s
epoch 102| loss: 0.16319 | val_0_rmse: 0.40012 | val_1_rmse: 0.43037 |  0:05:53s
epoch 103| loss: 0.16265 | val_0_rmse: 0.38952 | val_1_rmse: 0.41981 |  0:05:56s
epoch 104| loss: 0.16473 | val_0_rmse: 0.43292 | val_1_rmse: 0.45973 |  0:05:59s
epoch 105| loss: 0.16457 | val_0_rmse: 0.39656 | val_1_rmse: 0.42682 |  0:06:03s
epoch 106| loss: 0.16428 | val_0_rmse: 0.38678 | val_1_rmse: 0.41628 |  0:06:06s
epoch 107| loss: 0.16574 | val_0_rmse: 0.39635 | val_1_rmse: 0.42762 |  0:06:10s
epoch 108| loss: 0.16176 | val_0_rmse: 0.38669 | val_1_rmse: 0.41647 |  0:06:13s
epoch 109| loss: 0.16395 | val_0_rmse: 0.39512 | val_1_rmse: 0.42532 |  0:06:17s
epoch 110| loss: 0.16289 | val_0_rmse: 0.39753 | val_1_rmse: 0.42851 |  0:06:20s
epoch 111| loss: 0.1626  | val_0_rmse: 0.38593 | val_1_rmse: 0.41761 |  0:06:23s
epoch 112| loss: 0.16327 | val_0_rmse: 0.38838 | val_1_rmse: 0.41837 |  0:06:27s
epoch 113| loss: 0.16665 | val_0_rmse: 0.3961  | val_1_rmse: 0.42533 |  0:06:30s
epoch 114| loss: 0.16116 | val_0_rmse: 0.39708 | val_1_rmse: 0.42777 |  0:06:34s
epoch 115| loss: 0.16191 | val_0_rmse: 0.39348 | val_1_rmse: 0.4241  |  0:06:37s
epoch 116| loss: 0.17022 | val_0_rmse: 0.40203 | val_1_rmse: 0.43098 |  0:06:40s
epoch 117| loss: 0.18745 | val_0_rmse: 0.46458 | val_1_rmse: 0.48284 |  0:06:44s
epoch 118| loss: 0.19389 | val_0_rmse: 0.41108 | val_1_rmse: 0.43764 |  0:06:47s
epoch 119| loss: 0.18375 | val_0_rmse: 0.43537 | val_1_rmse: 0.45669 |  0:06:51s
epoch 120| loss: 0.18163 | val_0_rmse: 0.40775 | val_1_rmse: 0.43114 |  0:06:54s
epoch 121| loss: 0.1732  | val_0_rmse: 0.39777 | val_1_rmse: 0.42305 |  0:06:58s
epoch 122| loss: 0.17385 | val_0_rmse: 0.39923 | val_1_rmse: 0.42692 |  0:07:01s
epoch 123| loss: 0.17067 | val_0_rmse: 0.40841 | val_1_rmse: 0.43285 |  0:07:04s
epoch 124| loss: 0.16859 | val_0_rmse: 0.39597 | val_1_rmse: 0.42245 |  0:07:08s
epoch 125| loss: 0.16917 | val_0_rmse: 0.39205 | val_1_rmse: 0.41742 |  0:07:11s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.41585
Best weights from best epoch are automatically used!
ended training at: 22:32:29
Feature importance:
Mean squared error is of 9390569816.39979
Mean absolute error:66865.16778412246
MAPE:0.2920125310155049
R2 score:0.835402391151145
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:32:29
epoch 0  | loss: 0.59912 | val_0_rmse: 0.67861 | val_1_rmse: 0.6769  |  0:00:03s
epoch 1  | loss: 0.33069 | val_0_rmse: 0.61101 | val_1_rmse: 0.6079  |  0:00:06s
epoch 2  | loss: 0.31046 | val_0_rmse: 0.60551 | val_1_rmse: 0.59919 |  0:00:10s
epoch 3  | loss: 0.2694  | val_0_rmse: 0.58307 | val_1_rmse: 0.58227 |  0:00:13s
epoch 4  | loss: 0.25162 | val_0_rmse: 0.52263 | val_1_rmse: 0.51531 |  0:00:17s
epoch 5  | loss: 0.23874 | val_0_rmse: 0.53478 | val_1_rmse: 0.5266  |  0:00:20s
epoch 6  | loss: 0.23791 | val_0_rmse: 0.50118 | val_1_rmse: 0.49822 |  0:00:24s
epoch 7  | loss: 0.23061 | val_0_rmse: 0.46911 | val_1_rmse: 0.46883 |  0:00:27s
epoch 8  | loss: 0.22724 | val_0_rmse: 0.46542 | val_1_rmse: 0.46421 |  0:00:30s
epoch 9  | loss: 0.21397 | val_0_rmse: 0.4769  | val_1_rmse: 0.47237 |  0:00:34s
epoch 10 | loss: 0.21534 | val_0_rmse: 0.45027 | val_1_rmse: 0.45188 |  0:00:37s
epoch 11 | loss: 0.23784 | val_0_rmse: 0.45962 | val_1_rmse: 0.45729 |  0:00:41s
epoch 12 | loss: 0.2258  | val_0_rmse: 0.46004 | val_1_rmse: 0.46126 |  0:00:44s
epoch 13 | loss: 0.21382 | val_0_rmse: 0.46826 | val_1_rmse: 0.46794 |  0:00:48s
epoch 14 | loss: 0.2221  | val_0_rmse: 0.46367 | val_1_rmse: 0.46161 |  0:00:51s
epoch 15 | loss: 0.2472  | val_0_rmse: 0.48003 | val_1_rmse: 0.48044 |  0:00:54s
epoch 16 | loss: 0.22804 | val_0_rmse: 0.45128 | val_1_rmse: 0.44688 |  0:00:58s
epoch 17 | loss: 0.21126 | val_0_rmse: 0.44357 | val_1_rmse: 0.44415 |  0:01:01s
epoch 18 | loss: 0.21148 | val_0_rmse: 0.47668 | val_1_rmse: 0.47507 |  0:01:05s
epoch 19 | loss: 0.20797 | val_0_rmse: 0.43069 | val_1_rmse: 0.43073 |  0:01:08s
epoch 20 | loss: 0.20418 | val_0_rmse: 0.43195 | val_1_rmse: 0.43241 |  0:01:12s
epoch 21 | loss: 0.21178 | val_0_rmse: 0.43637 | val_1_rmse: 0.43647 |  0:01:15s
epoch 22 | loss: 0.20805 | val_0_rmse: 0.45877 | val_1_rmse: 0.46025 |  0:01:18s
epoch 23 | loss: 0.204   | val_0_rmse: 0.44258 | val_1_rmse: 0.44063 |  0:01:22s
epoch 24 | loss: 0.20444 | val_0_rmse: 0.46218 | val_1_rmse: 0.46358 |  0:01:25s
epoch 25 | loss: 0.19807 | val_0_rmse: 0.42561 | val_1_rmse: 0.42623 |  0:01:29s
epoch 26 | loss: 0.20072 | val_0_rmse: 0.42839 | val_1_rmse: 0.42988 |  0:01:32s
epoch 27 | loss: 0.19657 | val_0_rmse: 0.42612 | val_1_rmse: 0.42954 |  0:01:35s
epoch 28 | loss: 0.1932  | val_0_rmse: 0.42553 | val_1_rmse: 0.42687 |  0:01:39s
epoch 29 | loss: 0.19762 | val_0_rmse: 0.44732 | val_1_rmse: 0.44413 |  0:01:42s
epoch 30 | loss: 0.19847 | val_0_rmse: 0.43868 | val_1_rmse: 0.44201 |  0:01:46s
epoch 31 | loss: 0.19276 | val_0_rmse: 0.42799 | val_1_rmse: 0.42632 |  0:01:49s
epoch 32 | loss: 0.19088 | val_0_rmse: 0.41736 | val_1_rmse: 0.41824 |  0:01:53s
epoch 33 | loss: 0.18948 | val_0_rmse: 0.43351 | val_1_rmse: 0.43715 |  0:01:56s
epoch 34 | loss: 0.18881 | val_0_rmse: 0.41875 | val_1_rmse: 0.41939 |  0:02:00s
epoch 35 | loss: 0.18766 | val_0_rmse: 0.43954 | val_1_rmse: 0.44613 |  0:02:03s
epoch 36 | loss: 0.18734 | val_0_rmse: 0.45154 | val_1_rmse: 0.45141 |  0:02:06s
epoch 37 | loss: 0.19034 | val_0_rmse: 0.46423 | val_1_rmse: 0.46862 |  0:02:10s
epoch 38 | loss: 0.19285 | val_0_rmse: 0.42683 | val_1_rmse: 0.42722 |  0:02:13s
epoch 39 | loss: 0.18716 | val_0_rmse: 0.43466 | val_1_rmse: 0.43834 |  0:02:17s
epoch 40 | loss: 0.19211 | val_0_rmse: 0.43013 | val_1_rmse: 0.43277 |  0:02:20s
epoch 41 | loss: 0.18917 | val_0_rmse: 0.42108 | val_1_rmse: 0.424   |  0:02:23s
epoch 42 | loss: 0.18626 | val_0_rmse: 0.42566 | val_1_rmse: 0.43118 |  0:02:27s
epoch 43 | loss: 0.18601 | val_0_rmse: 0.43082 | val_1_rmse: 0.43083 |  0:02:30s
epoch 44 | loss: 0.19358 | val_0_rmse: 0.43078 | val_1_rmse: 0.434   |  0:02:34s
epoch 45 | loss: 0.19143 | val_0_rmse: 0.4294  | val_1_rmse: 0.43087 |  0:02:37s
epoch 46 | loss: 0.19769 | val_0_rmse: 0.45511 | val_1_rmse: 0.45652 |  0:02:41s
epoch 47 | loss: 0.1991  | val_0_rmse: 0.43785 | val_1_rmse: 0.44067 |  0:02:44s
epoch 48 | loss: 0.194   | val_0_rmse: 0.45693 | val_1_rmse: 0.4591  |  0:02:48s
epoch 49 | loss: 0.20036 | val_0_rmse: 0.45046 | val_1_rmse: 0.45088 |  0:02:51s
epoch 50 | loss: 0.20971 | val_0_rmse: 0.43995 | val_1_rmse: 0.44109 |  0:02:54s
epoch 51 | loss: 0.21277 | val_0_rmse: 0.52112 | val_1_rmse: 0.52202 |  0:02:58s
epoch 52 | loss: 0.2119  | val_0_rmse: 0.43031 | val_1_rmse: 0.43521 |  0:03:01s
epoch 53 | loss: 0.19512 | val_0_rmse: 0.44291 | val_1_rmse: 0.44748 |  0:03:05s
epoch 54 | loss: 0.20548 | val_0_rmse: 0.45235 | val_1_rmse: 0.45238 |  0:03:08s
epoch 55 | loss: 0.20383 | val_0_rmse: 0.43298 | val_1_rmse: 0.4379  |  0:03:12s
epoch 56 | loss: 0.19836 | val_0_rmse: 0.44264 | val_1_rmse: 0.44342 |  0:03:15s
epoch 57 | loss: 0.2124  | val_0_rmse: 0.47326 | val_1_rmse: 0.47241 |  0:03:18s
epoch 58 | loss: 0.2121  | val_0_rmse: 0.48513 | val_1_rmse: 0.48312 |  0:03:22s
epoch 59 | loss: 0.2472  | val_0_rmse: 0.57555 | val_1_rmse: 0.57052 |  0:03:25s
epoch 60 | loss: 0.25259 | val_0_rmse: 0.49181 | val_1_rmse: 0.49154 |  0:03:29s
epoch 61 | loss: 0.24223 | val_0_rmse: 0.45608 | val_1_rmse: 0.45793 |  0:03:32s
epoch 62 | loss: 0.21677 | val_0_rmse: 0.46826 | val_1_rmse: 0.468   |  0:03:35s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.41824
Best weights from best epoch are automatically used!
ended training at: 22:36:06
Feature importance:
Mean squared error is of 10001405406.380001
Mean absolute error:69335.65230358506
MAPE:0.28872393705743166
R2 score:0.8265043261278116
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:36:07
epoch 0  | loss: 0.66247 | val_0_rmse: 0.71964 | val_1_rmse: 0.72281 |  0:00:03s
epoch 1  | loss: 0.34632 | val_0_rmse: 0.71516 | val_1_rmse: 0.71248 |  0:00:06s
epoch 2  | loss: 0.2837  | val_0_rmse: 0.71524 | val_1_rmse: 0.69471 |  0:00:10s
epoch 3  | loss: 0.25844 | val_0_rmse: 0.60207 | val_1_rmse: 0.59816 |  0:00:13s
epoch 4  | loss: 0.24812 | val_0_rmse: 0.56337 | val_1_rmse: 0.55911 |  0:00:17s
epoch 5  | loss: 0.25523 | val_0_rmse: 0.49996 | val_1_rmse: 0.49815 |  0:00:20s
epoch 6  | loss: 0.2389  | val_0_rmse: 0.49594 | val_1_rmse: 0.49668 |  0:00:24s
epoch 7  | loss: 0.24009 | val_0_rmse: 0.46331 | val_1_rmse: 0.46362 |  0:00:27s
epoch 8  | loss: 0.22646 | val_0_rmse: 0.53032 | val_1_rmse: 0.53088 |  0:00:30s
epoch 9  | loss: 0.22743 | val_0_rmse: 0.44679 | val_1_rmse: 0.4485  |  0:00:34s
epoch 10 | loss: 0.21714 | val_0_rmse: 0.44752 | val_1_rmse: 0.44918 |  0:00:37s
epoch 11 | loss: 0.21048 | val_0_rmse: 0.43946 | val_1_rmse: 0.44157 |  0:00:41s
epoch 12 | loss: 0.22246 | val_0_rmse: 0.4593  | val_1_rmse: 0.45988 |  0:00:44s
epoch 13 | loss: 0.22181 | val_0_rmse: 0.46591 | val_1_rmse: 0.46937 |  0:00:48s
epoch 14 | loss: 0.22562 | val_0_rmse: 0.46785 | val_1_rmse: 0.47118 |  0:00:51s
epoch 15 | loss: 0.2355  | val_0_rmse: 0.48156 | val_1_rmse: 0.48142 |  0:00:55s
epoch 16 | loss: 0.22457 | val_0_rmse: 0.55285 | val_1_rmse: 0.55266 |  0:00:58s
epoch 17 | loss: 0.22662 | val_0_rmse: 0.44125 | val_1_rmse: 0.44305 |  0:01:01s
epoch 18 | loss: 0.21131 | val_0_rmse: 0.43857 | val_1_rmse: 0.43966 |  0:01:05s
epoch 19 | loss: 0.20991 | val_0_rmse: 0.43996 | val_1_rmse: 0.44157 |  0:01:08s
epoch 20 | loss: 0.21094 | val_0_rmse: 0.44867 | val_1_rmse: 0.45063 |  0:01:12s
epoch 21 | loss: 0.20692 | val_0_rmse: 0.43232 | val_1_rmse: 0.43302 |  0:01:15s
epoch 22 | loss: 0.20533 | val_0_rmse: 0.45283 | val_1_rmse: 0.45399 |  0:01:19s
epoch 23 | loss: 0.20326 | val_0_rmse: 0.44927 | val_1_rmse: 0.45193 |  0:01:22s
epoch 24 | loss: 0.20299 | val_0_rmse: 0.41792 | val_1_rmse: 0.42229 |  0:01:25s
epoch 25 | loss: 0.19638 | val_0_rmse: 0.44558 | val_1_rmse: 0.44926 |  0:01:29s
epoch 26 | loss: 0.20222 | val_0_rmse: 0.45196 | val_1_rmse: 0.45594 |  0:01:32s
epoch 27 | loss: 0.20163 | val_0_rmse: 0.46248 | val_1_rmse: 0.46832 |  0:01:36s
epoch 28 | loss: 0.20112 | val_0_rmse: 0.42489 | val_1_rmse: 0.4297  |  0:01:39s
epoch 29 | loss: 0.21397 | val_0_rmse: 0.47002 | val_1_rmse: 0.4712  |  0:01:43s
epoch 30 | loss: 0.20869 | val_0_rmse: 0.48644 | val_1_rmse: 0.49009 |  0:01:46s
epoch 31 | loss: 0.20642 | val_0_rmse: 0.43572 | val_1_rmse: 0.44133 |  0:01:50s
epoch 32 | loss: 0.20141 | val_0_rmse: 0.43641 | val_1_rmse: 0.43786 |  0:01:53s
epoch 33 | loss: 0.20836 | val_0_rmse: 0.48539 | val_1_rmse: 0.48882 |  0:01:56s
epoch 34 | loss: 0.21125 | val_0_rmse: 0.5199  | val_1_rmse: 0.52244 |  0:02:00s
epoch 35 | loss: 0.20237 | val_0_rmse: 0.42788 | val_1_rmse: 0.43223 |  0:02:03s
epoch 36 | loss: 0.19277 | val_0_rmse: 0.42804 | val_1_rmse: 0.43337 |  0:02:07s
epoch 37 | loss: 0.19177 | val_0_rmse: 0.43606 | val_1_rmse: 0.44284 |  0:02:10s
epoch 38 | loss: 0.19069 | val_0_rmse: 0.42447 | val_1_rmse: 0.43058 |  0:02:14s
epoch 39 | loss: 0.18562 | val_0_rmse: 0.43145 | val_1_rmse: 0.43567 |  0:02:17s
epoch 40 | loss: 0.18765 | val_0_rmse: 0.41321 | val_1_rmse: 0.41973 |  0:02:20s
epoch 41 | loss: 0.18486 | val_0_rmse: 0.41261 | val_1_rmse: 0.4162  |  0:02:24s
epoch 42 | loss: 0.18226 | val_0_rmse: 0.43416 | val_1_rmse: 0.4396  |  0:02:27s
epoch 43 | loss: 0.18171 | val_0_rmse: 0.45229 | val_1_rmse: 0.4608  |  0:02:31s
epoch 44 | loss: 0.18184 | val_0_rmse: 0.40811 | val_1_rmse: 0.41682 |  0:02:34s
epoch 45 | loss: 0.18011 | val_0_rmse: 0.40134 | val_1_rmse: 0.40973 |  0:02:38s
epoch 46 | loss: 0.17965 | val_0_rmse: 0.40467 | val_1_rmse: 0.41312 |  0:02:41s
epoch 47 | loss: 0.17869 | val_0_rmse: 0.40294 | val_1_rmse: 0.41267 |  0:02:44s
epoch 48 | loss: 0.18303 | val_0_rmse: 0.42535 | val_1_rmse: 0.42979 |  0:02:48s
epoch 49 | loss: 0.17968 | val_0_rmse: 0.39877 | val_1_rmse: 0.40707 |  0:02:51s
epoch 50 | loss: 0.17715 | val_0_rmse: 0.40395 | val_1_rmse: 0.41399 |  0:02:55s
epoch 51 | loss: 0.18044 | val_0_rmse: 0.40311 | val_1_rmse: 0.41421 |  0:02:58s
epoch 52 | loss: 0.17426 | val_0_rmse: 0.40399 | val_1_rmse: 0.41501 |  0:03:02s
epoch 53 | loss: 0.17885 | val_0_rmse: 0.40814 | val_1_rmse: 0.41825 |  0:03:05s
epoch 54 | loss: 0.17805 | val_0_rmse: 0.43079 | val_1_rmse: 0.44168 |  0:03:08s
epoch 55 | loss: 0.17594 | val_0_rmse: 0.40165 | val_1_rmse: 0.41224 |  0:03:12s
epoch 56 | loss: 0.17703 | val_0_rmse: 0.45531 | val_1_rmse: 0.46715 |  0:03:15s
epoch 57 | loss: 0.17922 | val_0_rmse: 0.40138 | val_1_rmse: 0.41558 |  0:03:19s
epoch 58 | loss: 0.17293 | val_0_rmse: 0.40601 | val_1_rmse: 0.41923 |  0:03:22s
epoch 59 | loss: 0.1766  | val_0_rmse: 0.40225 | val_1_rmse: 0.4137  |  0:03:26s
epoch 60 | loss: 0.17803 | val_0_rmse: 0.42561 | val_1_rmse: 0.43934 |  0:03:29s
epoch 61 | loss: 0.17519 | val_0_rmse: 0.39393 | val_1_rmse: 0.4064  |  0:03:33s
epoch 62 | loss: 0.17379 | val_0_rmse: 0.39759 | val_1_rmse: 0.4099  |  0:03:36s
epoch 63 | loss: 0.1731  | val_0_rmse: 0.40985 | val_1_rmse: 0.42159 |  0:03:39s
epoch 64 | loss: 0.17175 | val_0_rmse: 0.41155 | val_1_rmse: 0.42755 |  0:03:43s
epoch 65 | loss: 0.17255 | val_0_rmse: 0.39321 | val_1_rmse: 0.40657 |  0:03:46s
epoch 66 | loss: 0.16828 | val_0_rmse: 0.41608 | val_1_rmse: 0.42798 |  0:03:50s
epoch 67 | loss: 0.16998 | val_0_rmse: 0.39146 | val_1_rmse: 0.40484 |  0:03:53s
epoch 68 | loss: 0.17024 | val_0_rmse: 0.38881 | val_1_rmse: 0.40285 |  0:03:56s
epoch 69 | loss: 0.17128 | val_0_rmse: 0.4169  | val_1_rmse: 0.43113 |  0:04:00s
epoch 70 | loss: 0.17055 | val_0_rmse: 0.38752 | val_1_rmse: 0.40117 |  0:04:03s
epoch 71 | loss: 0.16679 | val_0_rmse: 0.39967 | val_1_rmse: 0.41355 |  0:04:07s
epoch 72 | loss: 0.1683  | val_0_rmse: 0.38856 | val_1_rmse: 0.40089 |  0:04:10s
epoch 73 | loss: 0.16989 | val_0_rmse: 0.39183 | val_1_rmse: 0.40742 |  0:04:14s
epoch 74 | loss: 0.17172 | val_0_rmse: 0.39318 | val_1_rmse: 0.40863 |  0:04:17s
epoch 75 | loss: 0.16711 | val_0_rmse: 0.39365 | val_1_rmse: 0.40829 |  0:04:20s
epoch 76 | loss: 0.16961 | val_0_rmse: 0.38904 | val_1_rmse: 0.4038  |  0:04:24s
epoch 77 | loss: 0.16705 | val_0_rmse: 0.39124 | val_1_rmse: 0.40575 |  0:04:27s
epoch 78 | loss: 0.16712 | val_0_rmse: 0.40094 | val_1_rmse: 0.41744 |  0:04:31s
epoch 79 | loss: 0.16594 | val_0_rmse: 0.38956 | val_1_rmse: 0.40761 |  0:04:34s
epoch 80 | loss: 0.16615 | val_0_rmse: 0.38531 | val_1_rmse: 0.40241 |  0:04:38s
epoch 81 | loss: 0.16765 | val_0_rmse: 0.39763 | val_1_rmse: 0.41293 |  0:04:41s
epoch 82 | loss: 0.1663  | val_0_rmse: 0.41144 | val_1_rmse: 0.42836 |  0:04:44s
epoch 83 | loss: 0.16854 | val_0_rmse: 0.38542 | val_1_rmse: 0.4041  |  0:04:48s
epoch 84 | loss: 0.16779 | val_0_rmse: 0.40265 | val_1_rmse: 0.41977 |  0:04:51s
epoch 85 | loss: 0.16655 | val_0_rmse: 0.39159 | val_1_rmse: 0.41119 |  0:04:55s
epoch 86 | loss: 0.16504 | val_0_rmse: 0.38918 | val_1_rmse: 0.4043  |  0:04:58s
epoch 87 | loss: 0.16819 | val_0_rmse: 0.39093 | val_1_rmse: 0.40984 |  0:05:01s
epoch 88 | loss: 0.16541 | val_0_rmse: 0.40356 | val_1_rmse: 0.4185  |  0:05:05s
epoch 89 | loss: 0.1667  | val_0_rmse: 0.39606 | val_1_rmse: 0.41363 |  0:05:08s
epoch 90 | loss: 0.16511 | val_0_rmse: 0.39483 | val_1_rmse: 0.41455 |  0:05:12s
epoch 91 | loss: 0.16441 | val_0_rmse: 0.38014 | val_1_rmse: 0.39857 |  0:05:15s
epoch 92 | loss: 0.16338 | val_0_rmse: 0.38048 | val_1_rmse: 0.4009  |  0:05:19s
epoch 93 | loss: 0.16464 | val_0_rmse: 0.38508 | val_1_rmse: 0.40299 |  0:05:22s
epoch 94 | loss: 0.16315 | val_0_rmse: 0.38935 | val_1_rmse: 0.40849 |  0:05:25s
epoch 95 | loss: 0.16203 | val_0_rmse: 0.41757 | val_1_rmse: 0.43619 |  0:05:29s
epoch 96 | loss: 0.16083 | val_0_rmse: 0.38347 | val_1_rmse: 0.40506 |  0:05:32s
epoch 97 | loss: 0.16339 | val_0_rmse: 0.42262 | val_1_rmse: 0.44378 |  0:05:36s
epoch 98 | loss: 0.16375 | val_0_rmse: 0.38262 | val_1_rmse: 0.40288 |  0:05:39s
epoch 99 | loss: 0.16178 | val_0_rmse: 0.38108 | val_1_rmse: 0.40163 |  0:05:43s
epoch 100| loss: 0.15958 | val_0_rmse: 0.37784 | val_1_rmse: 0.39838 |  0:05:46s
epoch 101| loss: 0.16284 | val_0_rmse: 0.38451 | val_1_rmse: 0.405   |  0:05:49s
epoch 102| loss: 0.16342 | val_0_rmse: 0.42575 | val_1_rmse: 0.4458  |  0:05:53s
epoch 103| loss: 0.16031 | val_0_rmse: 0.38439 | val_1_rmse: 0.40558 |  0:05:56s
epoch 104| loss: 0.16031 | val_0_rmse: 0.38047 | val_1_rmse: 0.40186 |  0:06:00s
epoch 105| loss: 0.16082 | val_0_rmse: 0.3824  | val_1_rmse: 0.40598 |  0:06:03s
epoch 106| loss: 0.16292 | val_0_rmse: 0.40873 | val_1_rmse: 0.43084 |  0:06:07s
epoch 107| loss: 0.16043 | val_0_rmse: 0.38061 | val_1_rmse: 0.40342 |  0:06:10s
epoch 108| loss: 0.16115 | val_0_rmse: 0.37605 | val_1_rmse: 0.39928 |  0:06:13s
epoch 109| loss: 0.1587  | val_0_rmse: 0.37666 | val_1_rmse: 0.39758 |  0:06:17s
epoch 110| loss: 0.16049 | val_0_rmse: 0.38153 | val_1_rmse: 0.4068  |  0:06:20s
epoch 111| loss: 0.15887 | val_0_rmse: 0.37615 | val_1_rmse: 0.39756 |  0:06:24s
epoch 112| loss: 0.15987 | val_0_rmse: 0.37717 | val_1_rmse: 0.39882 |  0:06:27s
epoch 113| loss: 0.15901 | val_0_rmse: 0.37892 | val_1_rmse: 0.39952 |  0:06:31s
epoch 114| loss: 0.16    | val_0_rmse: 0.4196  | val_1_rmse: 0.44036 |  0:06:34s
epoch 115| loss: 0.15955 | val_0_rmse: 0.38293 | val_1_rmse: 0.40613 |  0:06:37s
epoch 116| loss: 0.16047 | val_0_rmse: 0.43989 | val_1_rmse: 0.45901 |  0:06:41s
epoch 117| loss: 0.16078 | val_0_rmse: 0.37398 | val_1_rmse: 0.39695 |  0:06:44s
epoch 118| loss: 0.1606  | val_0_rmse: 0.4199  | val_1_rmse: 0.44181 |  0:06:48s
epoch 119| loss: 0.15676 | val_0_rmse: 0.38178 | val_1_rmse: 0.40504 |  0:06:51s
epoch 120| loss: 0.15699 | val_0_rmse: 0.38034 | val_1_rmse: 0.40842 |  0:06:55s
epoch 121| loss: 0.1561  | val_0_rmse: 0.37058 | val_1_rmse: 0.3949  |  0:06:58s
epoch 122| loss: 0.1591  | val_0_rmse: 0.37864 | val_1_rmse: 0.40162 |  0:07:01s
epoch 123| loss: 0.15886 | val_0_rmse: 0.37552 | val_1_rmse: 0.40165 |  0:07:05s
epoch 124| loss: 0.1556  | val_0_rmse: 0.38149 | val_1_rmse: 0.40796 |  0:07:08s
epoch 125| loss: 0.15566 | val_0_rmse: 0.38725 | val_1_rmse: 0.41307 |  0:07:12s
epoch 126| loss: 0.15621 | val_0_rmse: 0.39488 | val_1_rmse: 0.41514 |  0:07:15s
epoch 127| loss: 0.15866 | val_0_rmse: 0.36912 | val_1_rmse: 0.39636 |  0:07:19s
epoch 128| loss: 0.15696 | val_0_rmse: 0.38157 | val_1_rmse: 0.40743 |  0:07:22s
epoch 129| loss: 0.15727 | val_0_rmse: 0.38972 | val_1_rmse: 0.41117 |  0:07:25s
epoch 130| loss: 0.15529 | val_0_rmse: 0.39058 | val_1_rmse: 0.41592 |  0:07:29s
epoch 131| loss: 0.15784 | val_0_rmse: 0.37397 | val_1_rmse: 0.39805 |  0:07:32s
epoch 132| loss: 0.15997 | val_0_rmse: 0.36958 | val_1_rmse: 0.39713 |  0:07:36s
epoch 133| loss: 0.15546 | val_0_rmse: 0.36895 | val_1_rmse: 0.39794 |  0:07:39s
epoch 134| loss: 0.15556 | val_0_rmse: 0.37623 | val_1_rmse: 0.40489 |  0:07:43s
epoch 135| loss: 0.15449 | val_0_rmse: 0.37285 | val_1_rmse: 0.39943 |  0:07:46s
epoch 136| loss: 0.15477 | val_0_rmse: 0.41705 | val_1_rmse: 0.444   |  0:07:49s
epoch 137| loss: 0.15817 | val_0_rmse: 0.384   | val_1_rmse: 0.41061 |  0:07:53s
epoch 138| loss: 0.15648 | val_0_rmse: 0.36753 | val_1_rmse: 0.39618 |  0:07:56s
epoch 139| loss: 0.15599 | val_0_rmse: 0.37367 | val_1_rmse: 0.40363 |  0:08:00s
epoch 140| loss: 0.155   | val_0_rmse: 0.3845  | val_1_rmse: 0.41206 |  0:08:03s
epoch 141| loss: 0.15586 | val_0_rmse: 0.36809 | val_1_rmse: 0.39999 |  0:08:07s
epoch 142| loss: 0.15264 | val_0_rmse: 0.38305 | val_1_rmse: 0.41207 |  0:08:10s
epoch 143| loss: 0.15431 | val_0_rmse: 0.36895 | val_1_rmse: 0.40501 |  0:08:13s
epoch 144| loss: 0.15554 | val_0_rmse: 0.39522 | val_1_rmse: 0.42339 |  0:08:17s
epoch 145| loss: 0.15414 | val_0_rmse: 0.37831 | val_1_rmse: 0.41785 |  0:08:20s
epoch 146| loss: 0.15125 | val_0_rmse: 0.3732  | val_1_rmse: 0.41143 |  0:08:24s
epoch 147| loss: 0.15234 | val_0_rmse: 0.36671 | val_1_rmse: 0.39881 |  0:08:27s
epoch 148| loss: 0.15325 | val_0_rmse: 0.36969 | val_1_rmse: 0.40103 |  0:08:31s
epoch 149| loss: 0.15346 | val_0_rmse: 0.3858  | val_1_rmse: 0.41501 |  0:08:34s
Stop training because you reached max_epochs = 150 with best_epoch = 121 and best_val_1_rmse = 0.3949
Best weights from best epoch are automatically used!
ended training at: 22:44:42
Feature importance:
Mean squared error is of 9001907423.875721
Mean absolute error:65737.25703764925
MAPE:0.274152741844583
R2 score:0.8453878996618489
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:44:43
epoch 0  | loss: 1.27095 | val_0_rmse: 0.91779 | val_1_rmse: 0.9331  |  0:00:01s
epoch 1  | loss: 0.70888 | val_0_rmse: 0.76095 | val_1_rmse: 0.7718  |  0:00:02s
epoch 2  | loss: 0.50721 | val_0_rmse: 0.63923 | val_1_rmse: 0.64223 |  0:00:03s
epoch 3  | loss: 0.41066 | val_0_rmse: 0.65708 | val_1_rmse: 0.66078 |  0:00:04s
epoch 4  | loss: 0.37386 | val_0_rmse: 0.71081 | val_1_rmse: 0.72628 |  0:00:05s
epoch 5  | loss: 0.34155 | val_0_rmse: 0.63044 | val_1_rmse: 0.62891 |  0:00:06s
epoch 6  | loss: 0.30074 | val_0_rmse: 0.62192 | val_1_rmse: 0.62405 |  0:00:07s
epoch 7  | loss: 0.28318 | val_0_rmse: 0.60715 | val_1_rmse: 0.60917 |  0:00:09s
epoch 8  | loss: 0.26212 | val_0_rmse: 0.59583 | val_1_rmse: 0.5978  |  0:00:10s
epoch 9  | loss: 0.25    | val_0_rmse: 0.58216 | val_1_rmse: 0.58009 |  0:00:11s
epoch 10 | loss: 0.26183 | val_0_rmse: 0.64254 | val_1_rmse: 0.64098 |  0:00:12s
epoch 11 | loss: 0.25752 | val_0_rmse: 0.65341 | val_1_rmse: 0.64972 |  0:00:13s
epoch 12 | loss: 0.24489 | val_0_rmse: 0.6475  | val_1_rmse: 0.64072 |  0:00:14s
epoch 13 | loss: 0.24243 | val_0_rmse: 0.62465 | val_1_rmse: 0.61327 |  0:00:15s
epoch 14 | loss: 0.23583 | val_0_rmse: 0.57159 | val_1_rmse: 0.5729  |  0:00:17s
epoch 15 | loss: 0.22981 | val_0_rmse: 0.53886 | val_1_rmse: 0.54855 |  0:00:18s
epoch 16 | loss: 0.22737 | val_0_rmse: 0.51136 | val_1_rmse: 0.51959 |  0:00:19s
epoch 17 | loss: 0.22696 | val_0_rmse: 0.51584 | val_1_rmse: 0.5195  |  0:00:20s
epoch 18 | loss: 0.22462 | val_0_rmse: 0.50374 | val_1_rmse: 0.51162 |  0:00:21s
epoch 19 | loss: 0.22548 | val_0_rmse: 0.49157 | val_1_rmse: 0.4992  |  0:00:22s
epoch 20 | loss: 0.21978 | val_0_rmse: 0.49831 | val_1_rmse: 0.5037  |  0:00:23s
epoch 21 | loss: 0.22146 | val_0_rmse: 0.54299 | val_1_rmse: 0.54051 |  0:00:25s
epoch 22 | loss: 0.2367  | val_0_rmse: 0.61046 | val_1_rmse: 0.61095 |  0:00:26s
epoch 23 | loss: 0.23982 | val_0_rmse: 0.51983 | val_1_rmse: 0.5351  |  0:00:27s
epoch 24 | loss: 0.24139 | val_0_rmse: 0.51755 | val_1_rmse: 0.52098 |  0:00:28s
epoch 25 | loss: 0.24042 | val_0_rmse: 0.47905 | val_1_rmse: 0.48557 |  0:00:29s
epoch 26 | loss: 0.23458 | val_0_rmse: 0.47609 | val_1_rmse: 0.48562 |  0:00:30s
epoch 27 | loss: 0.22411 | val_0_rmse: 0.52667 | val_1_rmse: 0.53208 |  0:00:31s
epoch 28 | loss: 0.23656 | val_0_rmse: 0.476   | val_1_rmse: 0.48272 |  0:00:32s
epoch 29 | loss: 0.22667 | val_0_rmse: 0.4737  | val_1_rmse: 0.47745 |  0:00:34s
epoch 30 | loss: 0.2314  | val_0_rmse: 0.4905  | val_1_rmse: 0.4973  |  0:00:35s
epoch 31 | loss: 0.2417  | val_0_rmse: 0.49272 | val_1_rmse: 0.50591 |  0:00:36s
epoch 32 | loss: 0.23117 | val_0_rmse: 0.47156 | val_1_rmse: 0.48702 |  0:00:37s
epoch 33 | loss: 0.22646 | val_0_rmse: 0.47195 | val_1_rmse: 0.47775 |  0:00:38s
epoch 34 | loss: 0.23574 | val_0_rmse: 0.54705 | val_1_rmse: 0.54133 |  0:00:39s
epoch 35 | loss: 0.22971 | val_0_rmse: 0.4697  | val_1_rmse: 0.47706 |  0:00:40s
epoch 36 | loss: 0.21871 | val_0_rmse: 0.45551 | val_1_rmse: 0.46537 |  0:00:42s
epoch 37 | loss: 0.24647 | val_0_rmse: 0.48546 | val_1_rmse: 0.49416 |  0:00:43s
epoch 38 | loss: 0.23945 | val_0_rmse: 0.46919 | val_1_rmse: 0.47947 |  0:00:44s
epoch 39 | loss: 0.23054 | val_0_rmse: 0.47911 | val_1_rmse: 0.48951 |  0:00:45s
epoch 40 | loss: 0.22541 | val_0_rmse: 0.46442 | val_1_rmse: 0.4764  |  0:00:46s
epoch 41 | loss: 0.2201  | val_0_rmse: 0.44916 | val_1_rmse: 0.46046 |  0:00:47s
epoch 42 | loss: 0.23063 | val_0_rmse: 0.46122 | val_1_rmse: 0.47093 |  0:00:48s
epoch 43 | loss: 0.2268  | val_0_rmse: 0.45609 | val_1_rmse: 0.46828 |  0:00:50s
epoch 44 | loss: 0.2226  | val_0_rmse: 0.45012 | val_1_rmse: 0.46411 |  0:00:51s
epoch 45 | loss: 0.21559 | val_0_rmse: 0.45455 | val_1_rmse: 0.46932 |  0:00:52s
epoch 46 | loss: 0.21496 | val_0_rmse: 0.46621 | val_1_rmse: 0.48177 |  0:00:53s
epoch 47 | loss: 0.21148 | val_0_rmse: 0.46871 | val_1_rmse: 0.48809 |  0:00:54s
epoch 48 | loss: 0.21021 | val_0_rmse: 0.44752 | val_1_rmse: 0.46017 |  0:00:55s
epoch 49 | loss: 0.20663 | val_0_rmse: 0.43834 | val_1_rmse: 0.45398 |  0:00:56s
epoch 50 | loss: 0.20291 | val_0_rmse: 0.43951 | val_1_rmse: 0.4546  |  0:00:58s
epoch 51 | loss: 0.203   | val_0_rmse: 0.4319  | val_1_rmse: 0.44708 |  0:00:59s
epoch 52 | loss: 0.20141 | val_0_rmse: 0.44013 | val_1_rmse: 0.45572 |  0:01:00s
epoch 53 | loss: 0.19708 | val_0_rmse: 0.43362 | val_1_rmse: 0.45222 |  0:01:01s
epoch 54 | loss: 0.19839 | val_0_rmse: 0.43019 | val_1_rmse: 0.44725 |  0:01:02s
epoch 55 | loss: 0.19685 | val_0_rmse: 0.43042 | val_1_rmse: 0.45045 |  0:01:03s
epoch 56 | loss: 0.1982  | val_0_rmse: 0.42857 | val_1_rmse: 0.44687 |  0:01:04s
epoch 57 | loss: 0.19873 | val_0_rmse: 0.42415 | val_1_rmse: 0.44564 |  0:01:05s
epoch 58 | loss: 0.19355 | val_0_rmse: 0.42435 | val_1_rmse: 0.44591 |  0:01:07s
epoch 59 | loss: 0.19545 | val_0_rmse: 0.43547 | val_1_rmse: 0.45492 |  0:01:08s
epoch 60 | loss: 0.19413 | val_0_rmse: 0.4277  | val_1_rmse: 0.44752 |  0:01:09s
epoch 61 | loss: 0.19198 | val_0_rmse: 0.42094 | val_1_rmse: 0.44439 |  0:01:10s
epoch 62 | loss: 0.19136 | val_0_rmse: 0.42511 | val_1_rmse: 0.44559 |  0:01:11s
epoch 63 | loss: 0.18986 | val_0_rmse: 0.42039 | val_1_rmse: 0.44264 |  0:01:12s
epoch 64 | loss: 0.1919  | val_0_rmse: 0.42239 | val_1_rmse: 0.44584 |  0:01:13s
epoch 65 | loss: 0.1943  | val_0_rmse: 0.4199  | val_1_rmse: 0.44575 |  0:01:15s
epoch 66 | loss: 0.18831 | val_0_rmse: 0.41896 | val_1_rmse: 0.4454  |  0:01:16s
epoch 67 | loss: 0.1876  | val_0_rmse: 0.4364  | val_1_rmse: 0.46528 |  0:01:17s
epoch 68 | loss: 0.1952  | val_0_rmse: 0.42709 | val_1_rmse: 0.4548  |  0:01:18s
epoch 69 | loss: 0.19548 | val_0_rmse: 0.42305 | val_1_rmse: 0.44908 |  0:01:19s
epoch 70 | loss: 0.18798 | val_0_rmse: 0.42708 | val_1_rmse: 0.45208 |  0:01:20s
epoch 71 | loss: 0.18886 | val_0_rmse: 0.41855 | val_1_rmse: 0.4462  |  0:01:21s
epoch 72 | loss: 0.18911 | val_0_rmse: 0.42069 | val_1_rmse: 0.45058 |  0:01:23s
epoch 73 | loss: 0.19204 | val_0_rmse: 0.41822 | val_1_rmse: 0.44543 |  0:01:24s
epoch 74 | loss: 0.18995 | val_0_rmse: 0.4287  | val_1_rmse: 0.45779 |  0:01:25s
epoch 75 | loss: 0.18587 | val_0_rmse: 0.41548 | val_1_rmse: 0.44589 |  0:01:26s
epoch 76 | loss: 0.18735 | val_0_rmse: 0.41454 | val_1_rmse: 0.44745 |  0:01:27s
epoch 77 | loss: 0.18819 | val_0_rmse: 0.41629 | val_1_rmse: 0.45296 |  0:01:28s
epoch 78 | loss: 0.18466 | val_0_rmse: 0.41614 | val_1_rmse: 0.4546  |  0:01:29s
epoch 79 | loss: 0.18748 | val_0_rmse: 0.42128 | val_1_rmse: 0.46546 |  0:01:30s
epoch 80 | loss: 0.18383 | val_0_rmse: 0.42717 | val_1_rmse: 0.46526 |  0:01:32s
epoch 81 | loss: 0.18519 | val_0_rmse: 0.41819 | val_1_rmse: 0.46872 |  0:01:33s
epoch 82 | loss: 0.18418 | val_0_rmse: 0.41157 | val_1_rmse: 0.45997 |  0:01:34s
epoch 83 | loss: 0.18134 | val_0_rmse: 0.41522 | val_1_rmse: 0.45767 |  0:01:35s
epoch 84 | loss: 0.18517 | val_0_rmse: 0.42634 | val_1_rmse: 0.47735 |  0:01:36s
epoch 85 | loss: 0.18793 | val_0_rmse: 0.42108 | val_1_rmse: 0.47141 |  0:01:37s
epoch 86 | loss: 0.18463 | val_0_rmse: 0.42747 | val_1_rmse: 0.48471 |  0:01:38s
epoch 87 | loss: 0.18002 | val_0_rmse: 0.42669 | val_1_rmse: 0.47341 |  0:01:40s
epoch 88 | loss: 0.18491 | val_0_rmse: 0.41214 | val_1_rmse: 0.46705 |  0:01:41s
epoch 89 | loss: 0.18547 | val_0_rmse: 0.42185 | val_1_rmse: 0.46916 |  0:01:42s
epoch 90 | loss: 0.1839  | val_0_rmse: 0.40713 | val_1_rmse: 0.45526 |  0:01:43s
epoch 91 | loss: 0.18369 | val_0_rmse: 0.40554 | val_1_rmse: 0.46351 |  0:01:44s
epoch 92 | loss: 0.18282 | val_0_rmse: 0.40739 | val_1_rmse: 0.47136 |  0:01:45s
epoch 93 | loss: 0.18694 | val_0_rmse: 0.41561 | val_1_rmse: 0.47687 |  0:01:46s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.44264
Best weights from best epoch are automatically used!
ended training at: 22:46:30
Feature importance:
Mean squared error is of 6154129138.613233
Mean absolute error:56028.99306352745
MAPE:0.1524052883814759
R2 score:0.7986053438832533
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:46:31
epoch 0  | loss: 1.23533 | val_0_rmse: 0.96028 | val_1_rmse: 0.95092 |  0:00:01s
epoch 1  | loss: 0.49494 | val_0_rmse: 0.78515 | val_1_rmse: 0.77452 |  0:00:02s
epoch 2  | loss: 0.36454 | val_0_rmse: 0.64569 | val_1_rmse: 0.64534 |  0:00:03s
epoch 3  | loss: 0.32112 | val_0_rmse: 0.58843 | val_1_rmse: 0.59826 |  0:00:04s
epoch 4  | loss: 0.29301 | val_0_rmse: 0.57963 | val_1_rmse: 0.58572 |  0:00:05s
epoch 5  | loss: 0.28284 | val_0_rmse: 0.57217 | val_1_rmse: 0.58327 |  0:00:06s
epoch 6  | loss: 0.27939 | val_0_rmse: 0.57537 | val_1_rmse: 0.58394 |  0:00:07s
epoch 7  | loss: 0.27359 | val_0_rmse: 0.56143 | val_1_rmse: 0.57316 |  0:00:09s
epoch 8  | loss: 0.27169 | val_0_rmse: 0.54654 | val_1_rmse: 0.55612 |  0:00:10s
epoch 9  | loss: 0.26394 | val_0_rmse: 0.54893 | val_1_rmse: 0.554   |  0:00:11s
epoch 10 | loss: 0.25696 | val_0_rmse: 0.54998 | val_1_rmse: 0.55687 |  0:00:12s
epoch 11 | loss: 0.25359 | val_0_rmse: 0.54645 | val_1_rmse: 0.55418 |  0:00:13s
epoch 12 | loss: 0.24964 | val_0_rmse: 0.5687  | val_1_rmse: 0.5825  |  0:00:14s
epoch 13 | loss: 0.25486 | val_0_rmse: 0.54968 | val_1_rmse: 0.55737 |  0:00:15s
epoch 14 | loss: 0.24207 | val_0_rmse: 0.55472 | val_1_rmse: 0.56621 |  0:00:17s
epoch 15 | loss: 0.23966 | val_0_rmse: 0.54776 | val_1_rmse: 0.55749 |  0:00:18s
epoch 16 | loss: 0.23223 | val_0_rmse: 0.52484 | val_1_rmse: 0.53962 |  0:00:19s
epoch 17 | loss: 0.22908 | val_0_rmse: 0.5436  | val_1_rmse: 0.55381 |  0:00:20s
epoch 18 | loss: 0.22892 | val_0_rmse: 0.51461 | val_1_rmse: 0.52604 |  0:00:21s
epoch 19 | loss: 0.22951 | val_0_rmse: 0.515   | val_1_rmse: 0.53924 |  0:00:22s
epoch 20 | loss: 0.22834 | val_0_rmse: 0.50137 | val_1_rmse: 0.51951 |  0:00:23s
epoch 21 | loss: 0.22214 | val_0_rmse: 0.49628 | val_1_rmse: 0.51547 |  0:00:24s
epoch 22 | loss: 0.22295 | val_0_rmse: 0.49659 | val_1_rmse: 0.5181  |  0:00:26s
epoch 23 | loss: 0.22691 | val_0_rmse: 0.51084 | val_1_rmse: 0.53655 |  0:00:27s
epoch 24 | loss: 0.22063 | val_0_rmse: 0.49565 | val_1_rmse: 0.51923 |  0:00:28s
epoch 25 | loss: 0.2211  | val_0_rmse: 0.50225 | val_1_rmse: 0.51981 |  0:00:29s
epoch 26 | loss: 0.22201 | val_0_rmse: 0.4724  | val_1_rmse: 0.4954  |  0:00:30s
epoch 27 | loss: 0.2166  | val_0_rmse: 0.47815 | val_1_rmse: 0.49819 |  0:00:31s
epoch 28 | loss: 0.21679 | val_0_rmse: 0.45991 | val_1_rmse: 0.48218 |  0:00:32s
epoch 29 | loss: 0.21149 | val_0_rmse: 0.46094 | val_1_rmse: 0.48443 |  0:00:34s
epoch 30 | loss: 0.21284 | val_0_rmse: 0.45717 | val_1_rmse: 0.47685 |  0:00:35s
epoch 31 | loss: 0.21259 | val_0_rmse: 0.45614 | val_1_rmse: 0.47586 |  0:00:36s
epoch 32 | loss: 0.20655 | val_0_rmse: 0.45455 | val_1_rmse: 0.47975 |  0:00:37s
epoch 33 | loss: 0.21139 | val_0_rmse: 0.46251 | val_1_rmse: 0.48822 |  0:00:38s
epoch 34 | loss: 0.21002 | val_0_rmse: 0.44815 | val_1_rmse: 0.4727  |  0:00:39s
epoch 35 | loss: 0.20793 | val_0_rmse: 0.44966 | val_1_rmse: 0.47355 |  0:00:40s
epoch 36 | loss: 0.20363 | val_0_rmse: 0.44518 | val_1_rmse: 0.47576 |  0:00:42s
epoch 37 | loss: 0.20986 | val_0_rmse: 0.46145 | val_1_rmse: 0.48562 |  0:00:43s
epoch 38 | loss: 0.20499 | val_0_rmse: 0.4465  | val_1_rmse: 0.47496 |  0:00:44s
epoch 39 | loss: 0.20428 | val_0_rmse: 0.43527 | val_1_rmse: 0.4672  |  0:00:45s
epoch 40 | loss: 0.20305 | val_0_rmse: 0.44116 | val_1_rmse: 0.47512 |  0:00:46s
epoch 41 | loss: 0.20187 | val_0_rmse: 0.43277 | val_1_rmse: 0.46541 |  0:00:47s
epoch 42 | loss: 0.20123 | val_0_rmse: 0.43487 | val_1_rmse: 0.45967 |  0:00:48s
epoch 43 | loss: 0.19932 | val_0_rmse: 0.44962 | val_1_rmse: 0.48033 |  0:00:49s
epoch 44 | loss: 0.20423 | val_0_rmse: 0.43901 | val_1_rmse: 0.46885 |  0:00:51s
epoch 45 | loss: 0.20341 | val_0_rmse: 0.43963 | val_1_rmse: 0.47569 |  0:00:52s
epoch 46 | loss: 0.20253 | val_0_rmse: 0.42646 | val_1_rmse: 0.45446 |  0:00:53s
epoch 47 | loss: 0.19712 | val_0_rmse: 0.43462 | val_1_rmse: 0.46234 |  0:00:54s
epoch 48 | loss: 0.19499 | val_0_rmse: 0.43094 | val_1_rmse: 0.46735 |  0:00:55s
epoch 49 | loss: 0.19661 | val_0_rmse: 0.42858 | val_1_rmse: 0.46457 |  0:00:56s
epoch 50 | loss: 0.19975 | val_0_rmse: 0.43129 | val_1_rmse: 0.47771 |  0:00:57s
epoch 51 | loss: 0.19469 | val_0_rmse: 0.42544 | val_1_rmse: 0.4666  |  0:00:59s
epoch 52 | loss: 0.19625 | val_0_rmse: 0.42906 | val_1_rmse: 0.46357 |  0:01:00s
epoch 53 | loss: 0.19487 | val_0_rmse: 0.42377 | val_1_rmse: 0.46452 |  0:01:01s
epoch 54 | loss: 0.19317 | val_0_rmse: 0.42779 | val_1_rmse: 0.46606 |  0:01:02s
epoch 55 | loss: 0.20055 | val_0_rmse: 0.42975 | val_1_rmse: 0.46705 |  0:01:03s
epoch 56 | loss: 0.2022  | val_0_rmse: 0.43115 | val_1_rmse: 0.47423 |  0:01:04s
epoch 57 | loss: 0.19444 | val_0_rmse: 0.4262  | val_1_rmse: 0.46859 |  0:01:05s
epoch 58 | loss: 0.19681 | val_0_rmse: 0.42094 | val_1_rmse: 0.45769 |  0:01:06s
epoch 59 | loss: 0.18972 | val_0_rmse: 0.41823 | val_1_rmse: 0.46022 |  0:01:08s
epoch 60 | loss: 0.19012 | val_0_rmse: 0.42281 | val_1_rmse: 0.46131 |  0:01:09s
epoch 61 | loss: 0.1903  | val_0_rmse: 0.41712 | val_1_rmse: 0.45713 |  0:01:10s
epoch 62 | loss: 0.19147 | val_0_rmse: 0.42558 | val_1_rmse: 0.46465 |  0:01:11s
epoch 63 | loss: 0.18995 | val_0_rmse: 0.41867 | val_1_rmse: 0.46139 |  0:01:12s
epoch 64 | loss: 0.18719 | val_0_rmse: 0.41969 | val_1_rmse: 0.45994 |  0:01:13s
epoch 65 | loss: 0.19337 | val_0_rmse: 0.42037 | val_1_rmse: 0.46588 |  0:01:14s
epoch 66 | loss: 0.18856 | val_0_rmse: 0.41389 | val_1_rmse: 0.4589  |  0:01:16s
epoch 67 | loss: 0.1907  | val_0_rmse: 0.41892 | val_1_rmse: 0.46177 |  0:01:17s
epoch 68 | loss: 0.19052 | val_0_rmse: 0.41574 | val_1_rmse: 0.46369 |  0:01:18s
epoch 69 | loss: 0.1867  | val_0_rmse: 0.41302 | val_1_rmse: 0.45581 |  0:01:19s
epoch 70 | loss: 0.18364 | val_0_rmse: 0.41681 | val_1_rmse: 0.46475 |  0:01:20s
epoch 71 | loss: 0.19023 | val_0_rmse: 0.41583 | val_1_rmse: 0.46165 |  0:01:21s
epoch 72 | loss: 0.18591 | val_0_rmse: 0.42257 | val_1_rmse: 0.47408 |  0:01:22s
epoch 73 | loss: 0.1836  | val_0_rmse: 0.40985 | val_1_rmse: 0.45857 |  0:01:23s
epoch 74 | loss: 0.18302 | val_0_rmse: 0.41114 | val_1_rmse: 0.45867 |  0:01:25s
epoch 75 | loss: 0.18176 | val_0_rmse: 0.40996 | val_1_rmse: 0.46733 |  0:01:26s
epoch 76 | loss: 0.18233 | val_0_rmse: 0.40959 | val_1_rmse: 0.45867 |  0:01:27s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.45446
Best weights from best epoch are automatically used!
ended training at: 22:47:59
Feature importance:
Mean squared error is of 6305280839.7709255
Mean absolute error:56351.07417924886
MAPE:0.1517643839635949
R2 score:0.7890730465374044
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:47:59
epoch 0  | loss: 1.27597 | val_0_rmse: 0.93617 | val_1_rmse: 0.94419 |  0:00:01s
epoch 1  | loss: 0.74366 | val_0_rmse: 0.81952 | val_1_rmse: 0.82664 |  0:00:02s
epoch 2  | loss: 0.573   | val_0_rmse: 0.76921 | val_1_rmse: 0.77784 |  0:00:03s
epoch 3  | loss: 0.4314  | val_0_rmse: 0.74044 | val_1_rmse: 0.74977 |  0:00:04s
epoch 4  | loss: 0.3309  | val_0_rmse: 0.69855 | val_1_rmse: 0.70598 |  0:00:05s
epoch 5  | loss: 0.28474 | val_0_rmse: 0.67139 | val_1_rmse: 0.6779  |  0:00:06s
epoch 6  | loss: 0.27315 | val_0_rmse: 0.66349 | val_1_rmse: 0.67019 |  0:00:08s
epoch 7  | loss: 0.26227 | val_0_rmse: 0.69477 | val_1_rmse: 0.70074 |  0:00:09s
epoch 8  | loss: 0.25327 | val_0_rmse: 0.6682  | val_1_rmse: 0.67683 |  0:00:10s
epoch 9  | loss: 0.25148 | val_0_rmse: 0.63983 | val_1_rmse: 0.65012 |  0:00:11s
epoch 10 | loss: 0.25831 | val_0_rmse: 0.65412 | val_1_rmse: 0.66125 |  0:00:12s
epoch 11 | loss: 0.25047 | val_0_rmse: 0.62517 | val_1_rmse: 0.63532 |  0:00:13s
epoch 12 | loss: 0.24548 | val_0_rmse: 0.59861 | val_1_rmse: 0.60901 |  0:00:14s
epoch 13 | loss: 0.25471 | val_0_rmse: 0.62681 | val_1_rmse: 0.63495 |  0:00:15s
epoch 14 | loss: 0.24475 | val_0_rmse: 0.59681 | val_1_rmse: 0.60608 |  0:00:17s
epoch 15 | loss: 0.24624 | val_0_rmse: 0.56918 | val_1_rmse: 0.57659 |  0:00:18s
epoch 16 | loss: 0.23668 | val_0_rmse: 0.55901 | val_1_rmse: 0.5617  |  0:00:19s
epoch 17 | loss: 0.2346  | val_0_rmse: 0.55503 | val_1_rmse: 0.55951 |  0:00:20s
epoch 18 | loss: 0.23217 | val_0_rmse: 0.53489 | val_1_rmse: 0.53937 |  0:00:21s
epoch 19 | loss: 0.23791 | val_0_rmse: 0.5627  | val_1_rmse: 0.56756 |  0:00:22s
epoch 20 | loss: 0.24019 | val_0_rmse: 0.53369 | val_1_rmse: 0.53673 |  0:00:23s
epoch 21 | loss: 0.23982 | val_0_rmse: 0.53262 | val_1_rmse: 0.5376  |  0:00:25s
epoch 22 | loss: 0.23386 | val_0_rmse: 0.50561 | val_1_rmse: 0.50715 |  0:00:26s
epoch 23 | loss: 0.23383 | val_0_rmse: 0.50027 | val_1_rmse: 0.50027 |  0:00:27s
epoch 24 | loss: 0.22799 | val_0_rmse: 0.50312 | val_1_rmse: 0.50366 |  0:00:28s
epoch 25 | loss: 0.22959 | val_0_rmse: 0.51299 | val_1_rmse: 0.51796 |  0:00:29s
epoch 26 | loss: 0.23409 | val_0_rmse: 0.48471 | val_1_rmse: 0.48678 |  0:00:30s
epoch 27 | loss: 0.23111 | val_0_rmse: 0.47559 | val_1_rmse: 0.47663 |  0:00:31s
epoch 28 | loss: 0.22904 | val_0_rmse: 0.4819  | val_1_rmse: 0.48229 |  0:00:33s
epoch 29 | loss: 0.22697 | val_0_rmse: 0.47529 | val_1_rmse: 0.47864 |  0:00:34s
epoch 30 | loss: 0.22814 | val_0_rmse: 0.48432 | val_1_rmse: 0.48189 |  0:00:35s
epoch 31 | loss: 0.22499 | val_0_rmse: 0.46877 | val_1_rmse: 0.47445 |  0:00:36s
epoch 32 | loss: 0.22588 | val_0_rmse: 0.46685 | val_1_rmse: 0.47018 |  0:00:37s
epoch 33 | loss: 0.2231  | val_0_rmse: 0.45753 | val_1_rmse: 0.4613  |  0:00:38s
epoch 34 | loss: 0.22308 | val_0_rmse: 0.45911 | val_1_rmse: 0.46159 |  0:00:39s
epoch 35 | loss: 0.22343 | val_0_rmse: 0.46224 | val_1_rmse: 0.46189 |  0:00:40s
epoch 36 | loss: 0.22494 | val_0_rmse: 0.45803 | val_1_rmse: 0.46033 |  0:00:42s
epoch 37 | loss: 0.21678 | val_0_rmse: 0.45749 | val_1_rmse: 0.46095 |  0:00:43s
epoch 38 | loss: 0.22    | val_0_rmse: 0.45898 | val_1_rmse: 0.46249 |  0:00:44s
epoch 39 | loss: 0.21551 | val_0_rmse: 0.45205 | val_1_rmse: 0.4574  |  0:00:45s
epoch 40 | loss: 0.21777 | val_0_rmse: 0.45209 | val_1_rmse: 0.45654 |  0:00:46s
epoch 41 | loss: 0.21382 | val_0_rmse: 0.45343 | val_1_rmse: 0.45482 |  0:00:47s
epoch 42 | loss: 0.21649 | val_0_rmse: 0.46322 | val_1_rmse: 0.46284 |  0:00:48s
epoch 43 | loss: 0.22063 | val_0_rmse: 0.4666  | val_1_rmse: 0.47246 |  0:00:50s
epoch 44 | loss: 0.22647 | val_0_rmse: 0.44639 | val_1_rmse: 0.45386 |  0:00:51s
epoch 45 | loss: 0.21623 | val_0_rmse: 0.4504  | val_1_rmse: 0.4602  |  0:00:52s
epoch 46 | loss: 0.21355 | val_0_rmse: 0.44766 | val_1_rmse: 0.45364 |  0:00:53s
epoch 47 | loss: 0.21472 | val_0_rmse: 0.45685 | val_1_rmse: 0.46179 |  0:00:54s
epoch 48 | loss: 0.21901 | val_0_rmse: 0.4474  | val_1_rmse: 0.45186 |  0:00:55s
epoch 49 | loss: 0.22013 | val_0_rmse: 0.44856 | val_1_rmse: 0.45399 |  0:00:56s
epoch 50 | loss: 0.21183 | val_0_rmse: 0.45228 | val_1_rmse: 0.46588 |  0:00:58s
epoch 51 | loss: 0.21741 | val_0_rmse: 0.45518 | val_1_rmse: 0.46608 |  0:00:59s
epoch 52 | loss: 0.22011 | val_0_rmse: 0.44491 | val_1_rmse: 0.45637 |  0:01:00s
epoch 53 | loss: 0.21128 | val_0_rmse: 0.44766 | val_1_rmse: 0.4568  |  0:01:01s
epoch 54 | loss: 0.21479 | val_0_rmse: 0.44523 | val_1_rmse: 0.45937 |  0:01:02s
epoch 55 | loss: 0.21143 | val_0_rmse: 0.45476 | val_1_rmse: 0.47017 |  0:01:03s
epoch 56 | loss: 0.21034 | val_0_rmse: 0.44481 | val_1_rmse: 0.45647 |  0:01:04s
epoch 57 | loss: 0.20727 | val_0_rmse: 0.44278 | val_1_rmse: 0.45087 |  0:01:05s
epoch 58 | loss: 0.20878 | val_0_rmse: 0.44034 | val_1_rmse: 0.45828 |  0:01:07s
epoch 59 | loss: 0.21622 | val_0_rmse: 0.44682 | val_1_rmse: 0.45836 |  0:01:08s
epoch 60 | loss: 0.21209 | val_0_rmse: 0.43831 | val_1_rmse: 0.45253 |  0:01:09s
epoch 61 | loss: 0.20762 | val_0_rmse: 0.44008 | val_1_rmse: 0.45402 |  0:01:10s
epoch 62 | loss: 0.20964 | val_0_rmse: 0.4398  | val_1_rmse: 0.45138 |  0:01:11s
epoch 63 | loss: 0.2069  | val_0_rmse: 0.43932 | val_1_rmse: 0.4546  |  0:01:12s
epoch 64 | loss: 0.20575 | val_0_rmse: 0.43318 | val_1_rmse: 0.45178 |  0:01:13s
epoch 65 | loss: 0.20406 | val_0_rmse: 0.43635 | val_1_rmse: 0.45811 |  0:01:15s
epoch 66 | loss: 0.20351 | val_0_rmse: 0.43848 | val_1_rmse: 0.45144 |  0:01:16s
epoch 67 | loss: 0.20872 | val_0_rmse: 0.43617 | val_1_rmse: 0.44975 |  0:01:17s
epoch 68 | loss: 0.20088 | val_0_rmse: 0.43287 | val_1_rmse: 0.45312 |  0:01:18s
epoch 69 | loss: 0.20213 | val_0_rmse: 0.43164 | val_1_rmse: 0.4517  |  0:01:19s
epoch 70 | loss: 0.20163 | val_0_rmse: 0.42713 | val_1_rmse: 0.45029 |  0:01:20s
epoch 71 | loss: 0.20203 | val_0_rmse: 0.44148 | val_1_rmse: 0.4637  |  0:01:21s
epoch 72 | loss: 0.20149 | val_0_rmse: 0.4373  | val_1_rmse: 0.45851 |  0:01:22s
epoch 73 | loss: 0.20897 | val_0_rmse: 0.43751 | val_1_rmse: 0.45355 |  0:01:24s
epoch 74 | loss: 0.20546 | val_0_rmse: 0.43568 | val_1_rmse: 0.45087 |  0:01:25s
epoch 75 | loss: 0.20215 | val_0_rmse: 0.44835 | val_1_rmse: 0.46852 |  0:01:26s
epoch 76 | loss: 0.21738 | val_0_rmse: 0.45726 | val_1_rmse: 0.47282 |  0:01:27s
epoch 77 | loss: 0.22181 | val_0_rmse: 0.44937 | val_1_rmse: 0.46545 |  0:01:28s
epoch 78 | loss: 0.21627 | val_0_rmse: 0.44593 | val_1_rmse: 0.45781 |  0:01:29s
epoch 79 | loss: 0.20782 | val_0_rmse: 0.43802 | val_1_rmse: 0.45143 |  0:01:30s
epoch 80 | loss: 0.21833 | val_0_rmse: 0.48215 | val_1_rmse: 0.49929 |  0:01:32s
epoch 81 | loss: 0.23883 | val_0_rmse: 0.48462 | val_1_rmse: 0.49042 |  0:01:33s
epoch 82 | loss: 0.23885 | val_0_rmse: 0.47528 | val_1_rmse: 0.48751 |  0:01:34s
epoch 83 | loss: 0.23561 | val_0_rmse: 0.46401 | val_1_rmse: 0.46678 |  0:01:35s
epoch 84 | loss: 0.21999 | val_0_rmse: 0.45845 | val_1_rmse: 0.46383 |  0:01:36s
epoch 85 | loss: 0.2319  | val_0_rmse: 0.48956 | val_1_rmse: 0.49217 |  0:01:37s
epoch 86 | loss: 0.24275 | val_0_rmse: 0.47677 | val_1_rmse: 0.47269 |  0:01:38s
epoch 87 | loss: 0.23439 | val_0_rmse: 0.47492 | val_1_rmse: 0.47181 |  0:01:39s
epoch 88 | loss: 0.23237 | val_0_rmse: 0.46318 | val_1_rmse: 0.46412 |  0:01:41s
epoch 89 | loss: 0.22891 | val_0_rmse: 0.45792 | val_1_rmse: 0.46087 |  0:01:42s
epoch 90 | loss: 0.21974 | val_0_rmse: 0.4612  | val_1_rmse: 0.46599 |  0:01:43s
epoch 91 | loss: 0.21682 | val_0_rmse: 0.45252 | val_1_rmse: 0.452   |  0:01:44s
epoch 92 | loss: 0.22071 | val_0_rmse: 0.45454 | val_1_rmse: 0.45963 |  0:01:45s
epoch 93 | loss: 0.22002 | val_0_rmse: 0.4569  | val_1_rmse: 0.45941 |  0:01:46s
epoch 94 | loss: 0.21769 | val_0_rmse: 0.44855 | val_1_rmse: 0.45172 |  0:01:47s
epoch 95 | loss: 0.21585 | val_0_rmse: 0.45476 | val_1_rmse: 0.46017 |  0:01:48s
epoch 96 | loss: 0.21794 | val_0_rmse: 0.45756 | val_1_rmse: 0.46316 |  0:01:50s
epoch 97 | loss: 0.21528 | val_0_rmse: 0.45762 | val_1_rmse: 0.46574 |  0:01:51s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.44975
Best weights from best epoch are automatically used!
ended training at: 22:49:50
Feature importance:
Mean squared error is of 6804874863.448653
Mean absolute error:58184.668602933736
MAPE:0.15360566285829363
R2 score:0.779933814803641
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:49:50
epoch 0  | loss: 1.41072 | val_0_rmse: 0.99284 | val_1_rmse: 0.98493 |  0:00:01s
epoch 1  | loss: 0.68535 | val_0_rmse: 0.79307 | val_1_rmse: 0.78141 |  0:00:02s
epoch 2  | loss: 0.45516 | val_0_rmse: 0.66157 | val_1_rmse: 0.65053 |  0:00:03s
epoch 3  | loss: 0.387   | val_0_rmse: 0.63897 | val_1_rmse: 0.62942 |  0:00:04s
epoch 4  | loss: 0.34003 | val_0_rmse: 0.64615 | val_1_rmse: 0.639   |  0:00:05s
epoch 5  | loss: 0.31768 | val_0_rmse: 0.60271 | val_1_rmse: 0.59776 |  0:00:06s
epoch 6  | loss: 0.29051 | val_0_rmse: 0.61444 | val_1_rmse: 0.60602 |  0:00:07s
epoch 7  | loss: 0.27978 | val_0_rmse: 0.58133 | val_1_rmse: 0.57383 |  0:00:09s
epoch 8  | loss: 0.27281 | val_0_rmse: 0.58786 | val_1_rmse: 0.57795 |  0:00:10s
epoch 9  | loss: 0.26336 | val_0_rmse: 0.57427 | val_1_rmse: 0.56529 |  0:00:11s
epoch 10 | loss: 0.27756 | val_0_rmse: 0.60584 | val_1_rmse: 0.60013 |  0:00:12s
epoch 11 | loss: 0.25649 | val_0_rmse: 0.57078 | val_1_rmse: 0.56753 |  0:00:13s
epoch 12 | loss: 0.24905 | val_0_rmse: 0.57673 | val_1_rmse: 0.5738  |  0:00:14s
epoch 13 | loss: 0.24968 | val_0_rmse: 0.56632 | val_1_rmse: 0.55742 |  0:00:15s
epoch 14 | loss: 0.24861 | val_0_rmse: 0.56449 | val_1_rmse: 0.55252 |  0:00:17s
epoch 15 | loss: 0.2465  | val_0_rmse: 0.54443 | val_1_rmse: 0.53675 |  0:00:18s
epoch 16 | loss: 0.24602 | val_0_rmse: 0.54552 | val_1_rmse: 0.54118 |  0:00:19s
epoch 17 | loss: 0.23893 | val_0_rmse: 0.53734 | val_1_rmse: 0.53266 |  0:00:20s
epoch 18 | loss: 0.23274 | val_0_rmse: 0.52523 | val_1_rmse: 0.519   |  0:00:21s
epoch 19 | loss: 0.2381  | val_0_rmse: 0.52069 | val_1_rmse: 0.51319 |  0:00:22s
epoch 20 | loss: 0.23633 | val_0_rmse: 0.512   | val_1_rmse: 0.50266 |  0:00:23s
epoch 21 | loss: 0.23278 | val_0_rmse: 0.50647 | val_1_rmse: 0.49686 |  0:00:25s
epoch 22 | loss: 0.23065 | val_0_rmse: 0.49492 | val_1_rmse: 0.48583 |  0:00:26s
epoch 23 | loss: 0.23186 | val_0_rmse: 0.50357 | val_1_rmse: 0.49985 |  0:00:27s
epoch 24 | loss: 0.22986 | val_0_rmse: 0.48614 | val_1_rmse: 0.47798 |  0:00:28s
epoch 25 | loss: 0.22352 | val_0_rmse: 0.47933 | val_1_rmse: 0.47666 |  0:00:29s
epoch 26 | loss: 0.22363 | val_0_rmse: 0.47598 | val_1_rmse: 0.47339 |  0:00:30s
epoch 27 | loss: 0.21899 | val_0_rmse: 0.46929 | val_1_rmse: 0.4673  |  0:00:31s
epoch 28 | loss: 0.21689 | val_0_rmse: 0.46038 | val_1_rmse: 0.45366 |  0:00:33s
epoch 29 | loss: 0.21863 | val_0_rmse: 0.47382 | val_1_rmse: 0.47328 |  0:00:34s
epoch 30 | loss: 0.21884 | val_0_rmse: 0.45626 | val_1_rmse: 0.45312 |  0:00:35s
epoch 31 | loss: 0.21393 | val_0_rmse: 0.45572 | val_1_rmse: 0.44926 |  0:00:36s
epoch 32 | loss: 0.22106 | val_0_rmse: 0.49867 | val_1_rmse: 0.48918 |  0:00:37s
epoch 33 | loss: 0.22911 | val_0_rmse: 0.45958 | val_1_rmse: 0.45563 |  0:00:38s
epoch 34 | loss: 0.2207  | val_0_rmse: 0.45205 | val_1_rmse: 0.44878 |  0:00:39s
epoch 35 | loss: 0.2166  | val_0_rmse: 0.44497 | val_1_rmse: 0.44318 |  0:00:41s
epoch 36 | loss: 0.21299 | val_0_rmse: 0.44479 | val_1_rmse: 0.44356 |  0:00:42s
epoch 37 | loss: 0.21021 | val_0_rmse: 0.44377 | val_1_rmse: 0.444   |  0:00:43s
epoch 38 | loss: 0.20919 | val_0_rmse: 0.44638 | val_1_rmse: 0.44702 |  0:00:44s
epoch 39 | loss: 0.2105  | val_0_rmse: 0.44468 | val_1_rmse: 0.44977 |  0:00:45s
epoch 40 | loss: 0.21522 | val_0_rmse: 0.44187 | val_1_rmse: 0.44915 |  0:00:46s
epoch 41 | loss: 0.21364 | val_0_rmse: 0.47456 | val_1_rmse: 0.48229 |  0:00:47s
epoch 42 | loss: 0.22523 | val_0_rmse: 0.46197 | val_1_rmse: 0.45652 |  0:00:48s
epoch 43 | loss: 0.22014 | val_0_rmse: 0.45579 | val_1_rmse: 0.45363 |  0:00:50s
epoch 44 | loss: 0.21342 | val_0_rmse: 0.449   | val_1_rmse: 0.4505  |  0:00:51s
epoch 45 | loss: 0.21295 | val_0_rmse: 0.44483 | val_1_rmse: 0.44182 |  0:00:52s
epoch 46 | loss: 0.21695 | val_0_rmse: 0.46022 | val_1_rmse: 0.46245 |  0:00:53s
epoch 47 | loss: 0.21668 | val_0_rmse: 0.44769 | val_1_rmse: 0.44952 |  0:00:54s
epoch 48 | loss: 0.21683 | val_0_rmse: 0.45067 | val_1_rmse: 0.45637 |  0:00:55s
epoch 49 | loss: 0.21793 | val_0_rmse: 0.44085 | val_1_rmse: 0.44236 |  0:00:56s
epoch 50 | loss: 0.21078 | val_0_rmse: 0.44075 | val_1_rmse: 0.44117 |  0:00:58s
epoch 51 | loss: 0.21716 | val_0_rmse: 0.44229 | val_1_rmse: 0.4444  |  0:00:59s
epoch 52 | loss: 0.21193 | val_0_rmse: 0.43932 | val_1_rmse: 0.44323 |  0:01:00s
epoch 53 | loss: 0.20802 | val_0_rmse: 0.43671 | val_1_rmse: 0.4419  |  0:01:01s
epoch 54 | loss: 0.21188 | val_0_rmse: 0.46407 | val_1_rmse: 0.46931 |  0:01:02s
epoch 55 | loss: 0.20425 | val_0_rmse: 0.44652 | val_1_rmse: 0.45049 |  0:01:03s
epoch 56 | loss: 0.20775 | val_0_rmse: 0.43748 | val_1_rmse: 0.44088 |  0:01:04s
epoch 57 | loss: 0.20623 | val_0_rmse: 0.44075 | val_1_rmse: 0.44496 |  0:01:06s
epoch 58 | loss: 0.20479 | val_0_rmse: 0.43169 | val_1_rmse: 0.43978 |  0:01:07s
epoch 59 | loss: 0.20102 | val_0_rmse: 0.42896 | val_1_rmse: 0.43562 |  0:01:08s
epoch 60 | loss: 0.19717 | val_0_rmse: 0.43461 | val_1_rmse: 0.4379  |  0:01:09s
epoch 61 | loss: 0.20143 | val_0_rmse: 0.42973 | val_1_rmse: 0.43887 |  0:01:10s
epoch 62 | loss: 0.20429 | val_0_rmse: 0.44821 | val_1_rmse: 0.45323 |  0:01:11s
epoch 63 | loss: 0.2178  | val_0_rmse: 0.45167 | val_1_rmse: 0.47007 |  0:01:12s
epoch 64 | loss: 0.20663 | val_0_rmse: 0.43596 | val_1_rmse: 0.45169 |  0:01:13s
epoch 65 | loss: 0.20318 | val_0_rmse: 0.44028 | val_1_rmse: 0.44967 |  0:01:15s
epoch 66 | loss: 0.20445 | val_0_rmse: 0.43669 | val_1_rmse: 0.4511  |  0:01:16s
epoch 67 | loss: 0.20371 | val_0_rmse: 0.43514 | val_1_rmse: 0.44508 |  0:01:17s
epoch 68 | loss: 0.20326 | val_0_rmse: 0.44746 | val_1_rmse: 0.45577 |  0:01:18s
epoch 69 | loss: 0.1999  | val_0_rmse: 0.42954 | val_1_rmse: 0.43964 |  0:01:19s
epoch 70 | loss: 0.19746 | val_0_rmse: 0.4271  | val_1_rmse: 0.44219 |  0:01:20s
epoch 71 | loss: 0.19586 | val_0_rmse: 0.43358 | val_1_rmse: 0.4447  |  0:01:21s
epoch 72 | loss: 0.19397 | val_0_rmse: 0.42442 | val_1_rmse: 0.43693 |  0:01:23s
epoch 73 | loss: 0.19461 | val_0_rmse: 0.42896 | val_1_rmse: 0.44167 |  0:01:24s
epoch 74 | loss: 0.18996 | val_0_rmse: 0.42323 | val_1_rmse: 0.43983 |  0:01:25s
epoch 75 | loss: 0.19666 | val_0_rmse: 0.42348 | val_1_rmse: 0.44399 |  0:01:26s
epoch 76 | loss: 0.20047 | val_0_rmse: 0.42082 | val_1_rmse: 0.43647 |  0:01:27s
epoch 77 | loss: 0.19956 | val_0_rmse: 0.42089 | val_1_rmse: 0.44088 |  0:01:28s
epoch 78 | loss: 0.19109 | val_0_rmse: 0.42383 | val_1_rmse: 0.44296 |  0:01:29s
epoch 79 | loss: 0.1901  | val_0_rmse: 0.41978 | val_1_rmse: 0.43832 |  0:01:30s
epoch 80 | loss: 0.19101 | val_0_rmse: 0.43229 | val_1_rmse: 0.44977 |  0:01:32s
epoch 81 | loss: 0.18842 | val_0_rmse: 0.42032 | val_1_rmse: 0.4441  |  0:01:33s
epoch 82 | loss: 0.19119 | val_0_rmse: 0.41958 | val_1_rmse: 0.44158 |  0:01:34s
epoch 83 | loss: 0.19155 | val_0_rmse: 0.41995 | val_1_rmse: 0.44302 |  0:01:35s
epoch 84 | loss: 0.18834 | val_0_rmse: 0.42036 | val_1_rmse: 0.4445  |  0:01:36s
epoch 85 | loss: 0.197   | val_0_rmse: 0.43624 | val_1_rmse: 0.45688 |  0:01:37s
epoch 86 | loss: 0.19417 | val_0_rmse: 0.41588 | val_1_rmse: 0.43862 |  0:01:38s
epoch 87 | loss: 0.18966 | val_0_rmse: 0.41628 | val_1_rmse: 0.44089 |  0:01:40s
epoch 88 | loss: 0.18439 | val_0_rmse: 0.41925 | val_1_rmse: 0.4516  |  0:01:41s
epoch 89 | loss: 0.19244 | val_0_rmse: 0.4243  | val_1_rmse: 0.45179 |  0:01:42s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.43562
Best weights from best epoch are automatically used!
ended training at: 22:51:33
Feature importance:
Mean squared error is of 6111236898.308864
Mean absolute error:55985.32477968513
MAPE:0.1503943737277891
R2 score:0.792929195285307
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:51:33
epoch 0  | loss: 1.31858 | val_0_rmse: 0.93605 | val_1_rmse: 0.92739 |  0:00:01s
epoch 1  | loss: 0.7504  | val_0_rmse: 0.73523 | val_1_rmse: 0.72157 |  0:00:02s
epoch 2  | loss: 0.47178 | val_0_rmse: 0.64949 | val_1_rmse: 0.63955 |  0:00:03s
epoch 3  | loss: 0.34304 | val_0_rmse: 0.55722 | val_1_rmse: 0.5474  |  0:00:04s
epoch 4  | loss: 0.30892 | val_0_rmse: 0.57905 | val_1_rmse: 0.56454 |  0:00:05s
epoch 5  | loss: 0.30215 | val_0_rmse: 0.54475 | val_1_rmse: 0.52889 |  0:00:06s
epoch 6  | loss: 0.29738 | val_0_rmse: 0.55987 | val_1_rmse: 0.54419 |  0:00:08s
epoch 7  | loss: 0.2851  | val_0_rmse: 0.57306 | val_1_rmse: 0.56071 |  0:00:09s
epoch 8  | loss: 0.31243 | val_0_rmse: 0.57345 | val_1_rmse: 0.5575  |  0:00:10s
epoch 9  | loss: 0.29197 | val_0_rmse: 0.57284 | val_1_rmse: 0.55386 |  0:00:11s
epoch 10 | loss: 0.31421 | val_0_rmse: 0.58775 | val_1_rmse: 0.57386 |  0:00:12s
epoch 11 | loss: 0.38863 | val_0_rmse: 0.55145 | val_1_rmse: 0.53322 |  0:00:13s
epoch 12 | loss: 0.33109 | val_0_rmse: 0.60172 | val_1_rmse: 0.58464 |  0:00:14s
epoch 13 | loss: 0.33452 | val_0_rmse: 0.53509 | val_1_rmse: 0.52026 |  0:00:16s
epoch 14 | loss: 0.31112 | val_0_rmse: 0.54392 | val_1_rmse: 0.52843 |  0:00:17s
epoch 15 | loss: 0.29676 | val_0_rmse: 0.54038 | val_1_rmse: 0.52726 |  0:00:18s
epoch 16 | loss: 0.30351 | val_0_rmse: 0.54885 | val_1_rmse: 0.53486 |  0:00:19s
epoch 17 | loss: 0.30718 | val_0_rmse: 0.56716 | val_1_rmse: 0.54182 |  0:00:20s
epoch 18 | loss: 0.30212 | val_0_rmse: 0.53628 | val_1_rmse: 0.5191  |  0:00:21s
epoch 19 | loss: 0.30737 | val_0_rmse: 0.55653 | val_1_rmse: 0.53923 |  0:00:22s
epoch 20 | loss: 0.29024 | val_0_rmse: 0.53338 | val_1_rmse: 0.52206 |  0:00:24s
epoch 21 | loss: 0.28825 | val_0_rmse: 0.51831 | val_1_rmse: 0.50524 |  0:00:25s
epoch 22 | loss: 0.28737 | val_0_rmse: 0.52414 | val_1_rmse: 0.51235 |  0:00:26s
epoch 23 | loss: 0.28089 | val_0_rmse: 0.51812 | val_1_rmse: 0.5033  |  0:00:27s
epoch 24 | loss: 0.27632 | val_0_rmse: 0.56256 | val_1_rmse: 0.55108 |  0:00:28s
epoch 25 | loss: 0.28079 | val_0_rmse: 0.51289 | val_1_rmse: 0.50399 |  0:00:29s
epoch 26 | loss: 0.27171 | val_0_rmse: 0.5056  | val_1_rmse: 0.49622 |  0:00:30s
epoch 27 | loss: 0.27327 | val_0_rmse: 0.50311 | val_1_rmse: 0.49284 |  0:00:32s
epoch 28 | loss: 0.28237 | val_0_rmse: 0.52993 | val_1_rmse: 0.51823 |  0:00:33s
epoch 29 | loss: 0.28106 | val_0_rmse: 0.5146  | val_1_rmse: 0.50397 |  0:00:34s
epoch 30 | loss: 0.28296 | val_0_rmse: 0.51576 | val_1_rmse: 0.49906 |  0:00:35s
epoch 31 | loss: 0.28291 | val_0_rmse: 0.5144  | val_1_rmse: 0.50322 |  0:00:36s
epoch 32 | loss: 0.2742  | val_0_rmse: 0.51484 | val_1_rmse: 0.50494 |  0:00:37s
epoch 33 | loss: 0.27142 | val_0_rmse: 0.5095  | val_1_rmse: 0.49895 |  0:00:38s
epoch 34 | loss: 0.26421 | val_0_rmse: 0.51747 | val_1_rmse: 0.50743 |  0:00:39s
epoch 35 | loss: 0.26917 | val_0_rmse: 0.50245 | val_1_rmse: 0.49416 |  0:00:41s
epoch 36 | loss: 0.27041 | val_0_rmse: 0.53927 | val_1_rmse: 0.53401 |  0:00:42s
epoch 37 | loss: 0.26671 | val_0_rmse: 0.52044 | val_1_rmse: 0.51827 |  0:00:43s
epoch 38 | loss: 0.26413 | val_0_rmse: 0.49587 | val_1_rmse: 0.49218 |  0:00:44s
epoch 39 | loss: 0.26559 | val_0_rmse: 0.499   | val_1_rmse: 0.49353 |  0:00:45s
epoch 40 | loss: 0.26164 | val_0_rmse: 0.49443 | val_1_rmse: 0.49072 |  0:00:46s
epoch 41 | loss: 0.26537 | val_0_rmse: 0.49734 | val_1_rmse: 0.4889  |  0:00:47s
epoch 42 | loss: 0.26175 | val_0_rmse: 0.49388 | val_1_rmse: 0.48824 |  0:00:49s
epoch 43 | loss: 0.25659 | val_0_rmse: 0.49493 | val_1_rmse: 0.49045 |  0:00:50s
epoch 44 | loss: 0.25684 | val_0_rmse: 0.5028  | val_1_rmse: 0.49839 |  0:00:51s
epoch 45 | loss: 0.26237 | val_0_rmse: 0.48888 | val_1_rmse: 0.48149 |  0:00:52s
epoch 46 | loss: 0.2645  | val_0_rmse: 0.4924  | val_1_rmse: 0.48583 |  0:00:53s
epoch 47 | loss: 0.25896 | val_0_rmse: 0.50397 | val_1_rmse: 0.49801 |  0:00:54s
epoch 48 | loss: 0.25938 | val_0_rmse: 0.49003 | val_1_rmse: 0.48493 |  0:00:55s
epoch 49 | loss: 0.25688 | val_0_rmse: 0.49029 | val_1_rmse: 0.48357 |  0:00:57s
epoch 50 | loss: 0.25761 | val_0_rmse: 0.49852 | val_1_rmse: 0.49504 |  0:00:58s
epoch 51 | loss: 0.25743 | val_0_rmse: 0.49582 | val_1_rmse: 0.49496 |  0:00:59s
epoch 52 | loss: 0.2571  | val_0_rmse: 0.49166 | val_1_rmse: 0.48694 |  0:01:00s
epoch 53 | loss: 0.25619 | val_0_rmse: 0.52251 | val_1_rmse: 0.52013 |  0:01:01s
epoch 54 | loss: 0.25713 | val_0_rmse: 0.48979 | val_1_rmse: 0.48419 |  0:01:02s
epoch 55 | loss: 0.25951 | val_0_rmse: 0.48853 | val_1_rmse: 0.48222 |  0:01:03s
epoch 56 | loss: 0.25527 | val_0_rmse: 0.4956  | val_1_rmse: 0.49367 |  0:01:04s
epoch 57 | loss: 0.26196 | val_0_rmse: 0.50221 | val_1_rmse: 0.49741 |  0:01:06s
epoch 58 | loss: 0.26112 | val_0_rmse: 0.49023 | val_1_rmse: 0.49024 |  0:01:07s
epoch 59 | loss: 0.26393 | val_0_rmse: 0.52996 | val_1_rmse: 0.5163  |  0:01:08s
epoch 60 | loss: 0.27634 | val_0_rmse: 0.52287 | val_1_rmse: 0.51987 |  0:01:09s
epoch 61 | loss: 0.28089 | val_0_rmse: 0.51424 | val_1_rmse: 0.51036 |  0:01:10s
epoch 62 | loss: 0.28388 | val_0_rmse: 0.51202 | val_1_rmse: 0.50947 |  0:01:11s
epoch 63 | loss: 0.28072 | val_0_rmse: 0.51852 | val_1_rmse: 1.2247  |  0:01:12s
epoch 64 | loss: 0.28183 | val_0_rmse: 0.56443 | val_1_rmse: 0.56471 |  0:01:14s
epoch 65 | loss: 0.28503 | val_0_rmse: 0.51156 | val_1_rmse: 0.50509 |  0:01:15s
epoch 66 | loss: 0.27329 | val_0_rmse: 0.51396 | val_1_rmse: 0.5103  |  0:01:16s
epoch 67 | loss: 0.2677  | val_0_rmse: 0.52528 | val_1_rmse: 0.5203  |  0:01:17s
epoch 68 | loss: 0.26297 | val_0_rmse: 0.50128 | val_1_rmse: 0.50008 |  0:01:18s
epoch 69 | loss: 0.26348 | val_0_rmse: 0.49805 | val_1_rmse: 0.49572 |  0:01:19s
epoch 70 | loss: 0.25866 | val_0_rmse: 0.49671 | val_1_rmse: 0.49408 |  0:01:20s
epoch 71 | loss: 0.26529 | val_0_rmse: 0.53852 | val_1_rmse: 0.53792 |  0:01:21s
epoch 72 | loss: 0.26696 | val_0_rmse: 0.53455 | val_1_rmse: 0.53596 |  0:01:23s
epoch 73 | loss: 0.27449 | val_0_rmse: 0.50413 | val_1_rmse: 0.5041  |  0:01:24s
epoch 74 | loss: 0.27172 | val_0_rmse: 0.52888 | val_1_rmse: 0.53039 |  0:01:25s
epoch 75 | loss: 0.27126 | val_0_rmse: 0.50737 | val_1_rmse: 0.50112 |  0:01:26s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.48149
Best weights from best epoch are automatically used!
ended training at: 22:53:00
Feature importance:
Mean squared error is of 7702842648.624085
Mean absolute error:63028.309912019475
MAPE:0.16656768317567106
R2 score:0.7548369276017696
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:53:00
epoch 0  | loss: 2.0773  | val_0_rmse: 0.99723 | val_1_rmse: 0.995   |  0:00:00s
epoch 1  | loss: 1.23204 | val_0_rmse: 0.99835 | val_1_rmse: 0.99557 |  0:00:01s
epoch 2  | loss: 1.08739 | val_0_rmse: 0.99639 | val_1_rmse: 0.9941  |  0:00:01s
epoch 3  | loss: 0.99347 | val_0_rmse: 0.9916  | val_1_rmse: 0.98806 |  0:00:02s
epoch 4  | loss: 0.96344 | val_0_rmse: 0.97874 | val_1_rmse: 0.97404 |  0:00:03s
epoch 5  | loss: 0.93797 | val_0_rmse: 0.95725 | val_1_rmse: 0.96457 |  0:00:03s
epoch 6  | loss: 0.90152 | val_0_rmse: 0.98338 | val_1_rmse: 0.98802 |  0:00:04s
epoch 7  | loss: 0.84022 | val_0_rmse: 0.94975 | val_1_rmse: 0.95745 |  0:00:05s
epoch 8  | loss: 0.80238 | val_0_rmse: 0.95608 | val_1_rmse: 0.96307 |  0:00:05s
epoch 9  | loss: 0.73529 | val_0_rmse: 0.91617 | val_1_rmse: 0.92633 |  0:00:06s
epoch 10 | loss: 0.6643  | val_0_rmse: 0.89161 | val_1_rmse: 0.9105  |  0:00:06s
epoch 11 | loss: 0.61138 | val_0_rmse: 0.87954 | val_1_rmse: 0.90246 |  0:00:07s
epoch 12 | loss: 0.56514 | val_0_rmse: 0.86507 | val_1_rmse: 0.88544 |  0:00:08s
epoch 13 | loss: 0.50719 | val_0_rmse: 0.85371 | val_1_rmse: 0.87232 |  0:00:08s
epoch 14 | loss: 0.48174 | val_0_rmse: 0.9142  | val_1_rmse: 0.94029 |  0:00:09s
epoch 15 | loss: 0.45073 | val_0_rmse: 0.85419 | val_1_rmse: 0.87775 |  0:00:10s
epoch 16 | loss: 0.43144 | val_0_rmse: 0.84814 | val_1_rmse: 0.88127 |  0:00:10s
epoch 17 | loss: 0.40973 | val_0_rmse: 0.84043 | val_1_rmse: 0.87626 |  0:00:11s
epoch 18 | loss: 0.37877 | val_0_rmse: 0.83967 | val_1_rmse: 0.87894 |  0:00:11s
epoch 19 | loss: 0.36252 | val_0_rmse: 0.82899 | val_1_rmse: 0.86542 |  0:00:12s
epoch 20 | loss: 0.33752 | val_0_rmse: 0.83866 | val_1_rmse: 0.87929 |  0:00:13s
epoch 21 | loss: 0.31851 | val_0_rmse: 0.80853 | val_1_rmse: 0.84235 |  0:00:13s
epoch 22 | loss: 0.31361 | val_0_rmse: 0.82259 | val_1_rmse: 0.86327 |  0:00:14s
epoch 23 | loss: 0.29775 | val_0_rmse: 0.79172 | val_1_rmse: 0.83038 |  0:00:15s
epoch 24 | loss: 0.28063 | val_0_rmse: 0.78037 | val_1_rmse: 0.82108 |  0:00:15s
epoch 25 | loss: 0.27633 | val_0_rmse: 0.76335 | val_1_rmse: 0.80331 |  0:00:16s
epoch 26 | loss: 0.26958 | val_0_rmse: 0.76567 | val_1_rmse: 0.80386 |  0:00:16s
epoch 27 | loss: 0.25699 | val_0_rmse: 0.77776 | val_1_rmse: 0.82044 |  0:00:17s
epoch 28 | loss: 0.25343 | val_0_rmse: 0.76923 | val_1_rmse: 0.8103  |  0:00:18s
epoch 29 | loss: 0.25392 | val_0_rmse: 0.75464 | val_1_rmse: 0.8003  |  0:00:18s
epoch 30 | loss: 0.24975 | val_0_rmse: 0.73766 | val_1_rmse: 0.7742  |  0:00:19s
epoch 31 | loss: 0.2433  | val_0_rmse: 0.7629  | val_1_rmse: 0.81694 |  0:00:20s
epoch 32 | loss: 0.24079 | val_0_rmse: 0.74071 | val_1_rmse: 0.77834 |  0:00:20s
epoch 33 | loss: 0.24153 | val_0_rmse: 0.72846 | val_1_rmse: 0.77828 |  0:00:21s
epoch 34 | loss: 0.23645 | val_0_rmse: 0.73176 | val_1_rmse: 0.77874 |  0:00:21s
epoch 35 | loss: 0.23687 | val_0_rmse: 0.74351 | val_1_rmse: 0.79266 |  0:00:22s
epoch 36 | loss: 0.23511 | val_0_rmse: 0.72635 | val_1_rmse: 0.77257 |  0:00:23s
epoch 37 | loss: 0.23179 | val_0_rmse: 0.72939 | val_1_rmse: 0.78682 |  0:00:23s
epoch 38 | loss: 0.22966 | val_0_rmse: 0.70641 | val_1_rmse: 0.75025 |  0:00:24s
epoch 39 | loss: 0.21837 | val_0_rmse: 0.71219 | val_1_rmse: 0.76239 |  0:00:25s
epoch 40 | loss: 0.22883 | val_0_rmse: 0.69459 | val_1_rmse: 0.74034 |  0:00:25s
epoch 41 | loss: 0.21846 | val_0_rmse: 0.70968 | val_1_rmse: 0.76281 |  0:00:26s
epoch 42 | loss: 0.22085 | val_0_rmse: 0.70245 | val_1_rmse: 0.75497 |  0:00:26s
epoch 43 | loss: 0.21849 | val_0_rmse: 0.68519 | val_1_rmse: 0.74459 |  0:00:27s
epoch 44 | loss: 0.21051 | val_0_rmse: 0.65614 | val_1_rmse: 0.71058 |  0:00:28s
epoch 45 | loss: 0.21449 | val_0_rmse: 0.65511 | val_1_rmse: 0.70832 |  0:00:28s
epoch 46 | loss: 0.21245 | val_0_rmse: 0.65511 | val_1_rmse: 0.70781 |  0:00:29s
epoch 47 | loss: 0.20576 | val_0_rmse: 0.64428 | val_1_rmse: 0.69797 |  0:00:30s
epoch 48 | loss: 0.21298 | val_0_rmse: 0.63449 | val_1_rmse: 0.69537 |  0:00:30s
epoch 49 | loss: 0.20542 | val_0_rmse: 0.6237  | val_1_rmse: 0.67981 |  0:00:31s
epoch 50 | loss: 0.20523 | val_0_rmse: 0.6254  | val_1_rmse: 0.69051 |  0:00:32s
epoch 51 | loss: 0.20533 | val_0_rmse: 0.62184 | val_1_rmse: 0.67998 |  0:00:32s
epoch 52 | loss: 0.2106  | val_0_rmse: 0.60818 | val_1_rmse: 0.67221 |  0:00:33s
epoch 53 | loss: 0.2066  | val_0_rmse: 0.60953 | val_1_rmse: 0.67057 |  0:00:33s
epoch 54 | loss: 0.20216 | val_0_rmse: 0.57791 | val_1_rmse: 0.64667 |  0:00:34s
epoch 55 | loss: 0.1962  | val_0_rmse: 0.58601 | val_1_rmse: 0.65059 |  0:00:35s
epoch 56 | loss: 0.20215 | val_0_rmse: 0.57858 | val_1_rmse: 0.63872 |  0:00:35s
epoch 57 | loss: 0.19464 | val_0_rmse: 0.56332 | val_1_rmse: 0.63183 |  0:00:36s
epoch 58 | loss: 0.18931 | val_0_rmse: 0.55895 | val_1_rmse: 0.63289 |  0:00:37s
epoch 59 | loss: 0.19101 | val_0_rmse: 0.55693 | val_1_rmse: 0.6141  |  0:00:37s
epoch 60 | loss: 0.19097 | val_0_rmse: 0.55044 | val_1_rmse: 0.63357 |  0:00:38s
epoch 61 | loss: 0.1932  | val_0_rmse: 0.54739 | val_1_rmse: 0.60741 |  0:00:38s
epoch 62 | loss: 0.18651 | val_0_rmse: 0.53201 | val_1_rmse: 0.61062 |  0:00:39s
epoch 63 | loss: 0.18348 | val_0_rmse: 0.53309 | val_1_rmse: 0.60453 |  0:00:40s
epoch 64 | loss: 0.18821 | val_0_rmse: 0.53265 | val_1_rmse: 0.62124 |  0:00:40s
epoch 65 | loss: 0.19355 | val_0_rmse: 0.51431 | val_1_rmse: 0.60334 |  0:00:41s
epoch 66 | loss: 0.19389 | val_0_rmse: 0.5058  | val_1_rmse: 0.59474 |  0:00:42s
epoch 67 | loss: 0.19321 | val_0_rmse: 0.50594 | val_1_rmse: 0.59764 |  0:00:42s
epoch 68 | loss: 0.19136 | val_0_rmse: 0.50017 | val_1_rmse: 0.58598 |  0:00:43s
epoch 69 | loss: 0.18616 | val_0_rmse: 0.50135 | val_1_rmse: 0.58774 |  0:00:44s
epoch 70 | loss: 0.18737 | val_0_rmse: 0.48888 | val_1_rmse: 0.57798 |  0:00:44s
epoch 71 | loss: 0.1846  | val_0_rmse: 0.47559 | val_1_rmse: 0.5753  |  0:00:45s
epoch 72 | loss: 0.17782 | val_0_rmse: 0.47554 | val_1_rmse: 0.56275 |  0:00:45s
epoch 73 | loss: 0.18163 | val_0_rmse: 0.46162 | val_1_rmse: 0.56171 |  0:00:46s
epoch 74 | loss: 0.18231 | val_0_rmse: 0.46307 | val_1_rmse: 0.58077 |  0:00:47s
epoch 75 | loss: 0.18907 | val_0_rmse: 0.46035 | val_1_rmse: 0.55807 |  0:00:47s
epoch 76 | loss: 0.18516 | val_0_rmse: 0.45367 | val_1_rmse: 0.55155 |  0:00:48s
epoch 77 | loss: 0.17431 | val_0_rmse: 0.45409 | val_1_rmse: 0.58214 |  0:00:49s
epoch 78 | loss: 0.18038 | val_0_rmse: 0.45466 | val_1_rmse: 0.55212 |  0:00:49s
epoch 79 | loss: 0.17864 | val_0_rmse: 0.43369 | val_1_rmse: 0.54971 |  0:00:50s
epoch 80 | loss: 0.17863 | val_0_rmse: 0.43788 | val_1_rmse: 0.5535  |  0:00:50s
epoch 81 | loss: 0.17515 | val_0_rmse: 0.42943 | val_1_rmse: 0.54798 |  0:00:51s
epoch 82 | loss: 0.16971 | val_0_rmse: 0.42051 | val_1_rmse: 0.55045 |  0:00:52s
epoch 83 | loss: 0.17417 | val_0_rmse: 0.4209  | val_1_rmse: 0.55951 |  0:00:52s
epoch 84 | loss: 0.1786  | val_0_rmse: 0.45332 | val_1_rmse: 0.55895 |  0:00:53s
epoch 85 | loss: 0.1832  | val_0_rmse: 0.40991 | val_1_rmse: 0.54934 |  0:00:54s
epoch 86 | loss: 0.16957 | val_0_rmse: 0.40578 | val_1_rmse: 0.53805 |  0:00:54s
epoch 87 | loss: 0.17052 | val_0_rmse: 0.39672 | val_1_rmse: 0.54587 |  0:00:55s
epoch 88 | loss: 0.16171 | val_0_rmse: 0.3995  | val_1_rmse: 0.54556 |  0:00:55s
epoch 89 | loss: 0.16404 | val_0_rmse: 0.39719 | val_1_rmse: 0.55013 |  0:00:56s
epoch 90 | loss: 0.1732  | val_0_rmse: 0.3924  | val_1_rmse: 0.54394 |  0:00:57s
epoch 91 | loss: 0.16259 | val_0_rmse: 0.39328 | val_1_rmse: 0.54427 |  0:00:57s
epoch 92 | loss: 0.16457 | val_0_rmse: 0.38569 | val_1_rmse: 0.55228 |  0:00:58s
epoch 93 | loss: 0.15898 | val_0_rmse: 0.38888 | val_1_rmse: 0.53565 |  0:00:59s
epoch 94 | loss: 0.16296 | val_0_rmse: 0.38326 | val_1_rmse: 0.55012 |  0:00:59s
epoch 95 | loss: 0.16306 | val_0_rmse: 0.39826 | val_1_rmse: 0.53895 |  0:01:00s
epoch 96 | loss: 0.15798 | val_0_rmse: 0.37739 | val_1_rmse: 0.53744 |  0:01:00s
epoch 97 | loss: 0.15845 | val_0_rmse: 0.36813 | val_1_rmse: 0.55072 |  0:01:01s
epoch 98 | loss: 0.16179 | val_0_rmse: 0.37732 | val_1_rmse: 0.54363 |  0:01:02s
epoch 99 | loss: 0.15987 | val_0_rmse: 0.36695 | val_1_rmse: 0.55205 |  0:01:02s
epoch 100| loss: 0.15426 | val_0_rmse: 0.37701 | val_1_rmse: 0.54729 |  0:01:03s
epoch 101| loss: 0.15558 | val_0_rmse: 0.36183 | val_1_rmse: 0.55244 |  0:01:04s
epoch 102| loss: 0.1502  | val_0_rmse: 0.37052 | val_1_rmse: 0.54349 |  0:01:04s
epoch 103| loss: 0.15665 | val_0_rmse: 0.3649  | val_1_rmse: 0.55326 |  0:01:05s
epoch 104| loss: 0.15955 | val_0_rmse: 0.36156 | val_1_rmse: 0.55077 |  0:01:05s
epoch 105| loss: 0.15798 | val_0_rmse: 0.37813 | val_1_rmse: 0.54628 |  0:01:06s
epoch 106| loss: 0.15348 | val_0_rmse: 0.36335 | val_1_rmse: 0.55598 |  0:01:07s
epoch 107| loss: 0.15787 | val_0_rmse: 0.361   | val_1_rmse: 0.54321 |  0:01:07s
epoch 108| loss: 0.15237 | val_0_rmse: 0.35969 | val_1_rmse: 0.54182 |  0:01:08s
epoch 109| loss: 0.15481 | val_0_rmse: 0.3761  | val_1_rmse: 0.5864  |  0:01:09s
epoch 110| loss: 0.15056 | val_0_rmse: 0.3614  | val_1_rmse: 0.53689 |  0:01:09s
epoch 111| loss: 0.14068 | val_0_rmse: 0.349   | val_1_rmse: 0.54772 |  0:01:10s
epoch 112| loss: 0.1483  | val_0_rmse: 0.3516  | val_1_rmse: 0.56374 |  0:01:10s
epoch 113| loss: 0.14525 | val_0_rmse: 0.34621 | val_1_rmse: 0.55138 |  0:01:11s
epoch 114| loss: 0.14528 | val_0_rmse: 0.34238 | val_1_rmse: 0.55538 |  0:01:12s
epoch 115| loss: 0.1408  | val_0_rmse: 0.3501  | val_1_rmse: 0.55151 |  0:01:12s
epoch 116| loss: 0.14658 | val_0_rmse: 0.34526 | val_1_rmse: 0.56475 |  0:01:13s
epoch 117| loss: 0.14398 | val_0_rmse: 0.34309 | val_1_rmse: 0.54702 |  0:01:14s
epoch 118| loss: 0.14169 | val_0_rmse: 0.33939 | val_1_rmse: 0.55156 |  0:01:14s
epoch 119| loss: 0.14253 | val_0_rmse: 0.34057 | val_1_rmse: 0.57356 |  0:01:15s
epoch 120| loss: 0.14    | val_0_rmse: 0.33543 | val_1_rmse: 0.54855 |  0:01:15s
epoch 121| loss: 0.13796 | val_0_rmse: 0.33712 | val_1_rmse: 0.55532 |  0:01:16s
epoch 122| loss: 0.1416  | val_0_rmse: 0.33638 | val_1_rmse: 0.57019 |  0:01:17s
epoch 123| loss: 0.13744 | val_0_rmse: 0.34039 | val_1_rmse: 0.55735 |  0:01:17s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.53565
Best weights from best epoch are automatically used!
ended training at: 22:54:19
Feature importance:
Mean squared error is of 23110556361.03462
Mean absolute error:107347.64875755782
MAPE:0.18037101934199293
R2 score:0.723754569356021
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:54:19
epoch 0  | loss: 2.04601 | val_0_rmse: 1.00793 | val_1_rmse: 0.97714 |  0:00:00s
epoch 1  | loss: 1.13265 | val_0_rmse: 1.0057  | val_1_rmse: 0.97645 |  0:00:01s
epoch 2  | loss: 0.93742 | val_0_rmse: 0.87323 | val_1_rmse: 0.84319 |  0:00:01s
epoch 3  | loss: 0.7733  | val_0_rmse: 0.89529 | val_1_rmse: 0.85992 |  0:00:02s
epoch 4  | loss: 0.70336 | val_0_rmse: 0.90875 | val_1_rmse: 0.87733 |  0:00:03s
epoch 5  | loss: 0.7004  | val_0_rmse: 0.87786 | val_1_rmse: 0.84069 |  0:00:03s
epoch 6  | loss: 0.68788 | val_0_rmse: 0.85712 | val_1_rmse: 0.82784 |  0:00:04s
epoch 7  | loss: 0.68029 | val_0_rmse: 0.84713 | val_1_rmse: 0.81404 |  0:00:05s
epoch 8  | loss: 0.67396 | val_0_rmse: 0.84127 | val_1_rmse: 0.81245 |  0:00:05s
epoch 9  | loss: 0.65953 | val_0_rmse: 0.84315 | val_1_rmse: 0.81674 |  0:00:06s
epoch 10 | loss: 0.62777 | val_0_rmse: 0.87009 | val_1_rmse: 0.8364  |  0:00:06s
epoch 11 | loss: 0.5856  | val_0_rmse: 0.8616  | val_1_rmse: 0.82838 |  0:00:07s
epoch 12 | loss: 0.51154 | val_0_rmse: 0.80604 | val_1_rmse: 0.78064 |  0:00:08s
epoch 13 | loss: 0.46245 | val_0_rmse: 0.74481 | val_1_rmse: 0.73118 |  0:00:08s
epoch 14 | loss: 0.44587 | val_0_rmse: 0.73808 | val_1_rmse: 0.71662 |  0:00:09s
epoch 15 | loss: 0.42733 | val_0_rmse: 0.75381 | val_1_rmse: 0.73416 |  0:00:10s
epoch 16 | loss: 0.39256 | val_0_rmse: 0.70478 | val_1_rmse: 0.68765 |  0:00:10s
epoch 17 | loss: 0.38337 | val_0_rmse: 0.68133 | val_1_rmse: 0.66876 |  0:00:11s
epoch 18 | loss: 0.37705 | val_0_rmse: 0.69489 | val_1_rmse: 0.67407 |  0:00:12s
epoch 19 | loss: 0.35583 | val_0_rmse: 0.67855 | val_1_rmse: 0.65661 |  0:00:12s
epoch 20 | loss: 0.34634 | val_0_rmse: 0.66953 | val_1_rmse: 0.65409 |  0:00:13s
epoch 21 | loss: 0.34578 | val_0_rmse: 0.68244 | val_1_rmse: 0.66835 |  0:00:13s
epoch 22 | loss: 0.34058 | val_0_rmse: 0.70178 | val_1_rmse: 0.68321 |  0:00:14s
epoch 23 | loss: 0.33277 | val_0_rmse: 0.6631  | val_1_rmse: 0.6536  |  0:00:15s
epoch 24 | loss: 0.33373 | val_0_rmse: 0.66978 | val_1_rmse: 0.65671 |  0:00:15s
epoch 25 | loss: 0.30952 | val_0_rmse: 0.66903 | val_1_rmse: 0.65438 |  0:00:16s
epoch 26 | loss: 0.30724 | val_0_rmse: 0.65079 | val_1_rmse: 0.64018 |  0:00:17s
epoch 27 | loss: 0.30259 | val_0_rmse: 0.65545 | val_1_rmse: 0.64372 |  0:00:17s
epoch 28 | loss: 0.29329 | val_0_rmse: 0.63693 | val_1_rmse: 0.63123 |  0:00:18s
epoch 29 | loss: 0.28263 | val_0_rmse: 0.6428  | val_1_rmse: 0.63199 |  0:00:18s
epoch 30 | loss: 0.28894 | val_0_rmse: 0.6275  | val_1_rmse: 0.62236 |  0:00:19s
epoch 31 | loss: 0.28326 | val_0_rmse: 0.61721 | val_1_rmse: 0.61004 |  0:00:20s
epoch 32 | loss: 0.26833 | val_0_rmse: 0.61982 | val_1_rmse: 0.61345 |  0:00:20s
epoch 33 | loss: 0.26505 | val_0_rmse: 0.62163 | val_1_rmse: 0.61951 |  0:00:21s
epoch 34 | loss: 0.26571 | val_0_rmse: 0.62031 | val_1_rmse: 0.61588 |  0:00:22s
epoch 35 | loss: 0.26366 | val_0_rmse: 0.62543 | val_1_rmse: 0.61892 |  0:00:22s
epoch 36 | loss: 0.26777 | val_0_rmse: 0.62518 | val_1_rmse: 0.61972 |  0:00:23s
epoch 37 | loss: 0.26073 | val_0_rmse: 0.60592 | val_1_rmse: 0.60574 |  0:00:23s
epoch 38 | loss: 0.25843 | val_0_rmse: 0.59584 | val_1_rmse: 0.60313 |  0:00:24s
epoch 39 | loss: 0.26366 | val_0_rmse: 0.59905 | val_1_rmse: 0.59981 |  0:00:25s
epoch 40 | loss: 0.25967 | val_0_rmse: 0.61287 | val_1_rmse: 0.6099  |  0:00:25s
epoch 41 | loss: 0.25276 | val_0_rmse: 0.59652 | val_1_rmse: 0.6027  |  0:00:26s
epoch 42 | loss: 0.25047 | val_0_rmse: 0.60205 | val_1_rmse: 0.59973 |  0:00:27s
epoch 43 | loss: 0.24754 | val_0_rmse: 0.58647 | val_1_rmse: 0.59618 |  0:00:27s
epoch 44 | loss: 0.25033 | val_0_rmse: 0.59104 | val_1_rmse: 0.5897  |  0:00:28s
epoch 45 | loss: 0.24296 | val_0_rmse: 0.59096 | val_1_rmse: 0.59002 |  0:00:28s
epoch 46 | loss: 0.24308 | val_0_rmse: 0.5735  | val_1_rmse: 0.57668 |  0:00:29s
epoch 47 | loss: 0.24099 | val_0_rmse: 0.56724 | val_1_rmse: 0.5808  |  0:00:30s
epoch 48 | loss: 0.24295 | val_0_rmse: 0.58236 | val_1_rmse: 0.587   |  0:00:30s
epoch 49 | loss: 0.24643 | val_0_rmse: 0.58989 | val_1_rmse: 0.58659 |  0:00:31s
epoch 50 | loss: 0.23551 | val_0_rmse: 0.56543 | val_1_rmse: 0.57104 |  0:00:32s
epoch 51 | loss: 0.25102 | val_0_rmse: 0.56992 | val_1_rmse: 0.58109 |  0:00:32s
epoch 52 | loss: 0.23379 | val_0_rmse: 0.55386 | val_1_rmse: 0.5596  |  0:00:33s
epoch 53 | loss: 0.23074 | val_0_rmse: 0.55547 | val_1_rmse: 0.55841 |  0:00:34s
epoch 54 | loss: 0.2291  | val_0_rmse: 0.55972 | val_1_rmse: 0.56779 |  0:00:34s
epoch 55 | loss: 0.22989 | val_0_rmse: 0.54083 | val_1_rmse: 0.55568 |  0:00:35s
epoch 56 | loss: 0.23166 | val_0_rmse: 0.52984 | val_1_rmse: 0.54795 |  0:00:35s
epoch 57 | loss: 0.22339 | val_0_rmse: 0.52003 | val_1_rmse: 0.53315 |  0:00:36s
epoch 58 | loss: 0.23026 | val_0_rmse: 0.52205 | val_1_rmse: 0.5398  |  0:00:37s
epoch 59 | loss: 0.22008 | val_0_rmse: 0.50885 | val_1_rmse: 0.53811 |  0:00:37s
epoch 60 | loss: 0.23269 | val_0_rmse: 0.51382 | val_1_rmse: 0.53794 |  0:00:38s
epoch 61 | loss: 0.23144 | val_0_rmse: 0.50262 | val_1_rmse: 0.52975 |  0:00:39s
epoch 62 | loss: 0.22373 | val_0_rmse: 0.50637 | val_1_rmse: 0.53249 |  0:00:39s
epoch 63 | loss: 0.2228  | val_0_rmse: 0.49456 | val_1_rmse: 0.52442 |  0:00:40s
epoch 64 | loss: 0.22106 | val_0_rmse: 0.5064  | val_1_rmse: 0.53108 |  0:00:40s
epoch 65 | loss: 0.22199 | val_0_rmse: 0.49349 | val_1_rmse: 0.52265 |  0:00:41s
epoch 66 | loss: 0.21983 | val_0_rmse: 0.49339 | val_1_rmse: 0.52214 |  0:00:42s
epoch 67 | loss: 0.21736 | val_0_rmse: 0.49027 | val_1_rmse: 0.52547 |  0:00:42s
epoch 68 | loss: 0.21605 | val_0_rmse: 0.50174 | val_1_rmse: 0.53062 |  0:00:43s
epoch 69 | loss: 0.21551 | val_0_rmse: 0.48526 | val_1_rmse: 0.51957 |  0:00:44s
epoch 70 | loss: 0.21273 | val_0_rmse: 0.48671 | val_1_rmse: 0.51971 |  0:00:44s
epoch 71 | loss: 0.21488 | val_0_rmse: 0.47818 | val_1_rmse: 0.51213 |  0:00:45s
epoch 72 | loss: 0.21102 | val_0_rmse: 0.47954 | val_1_rmse: 0.51323 |  0:00:46s
epoch 73 | loss: 0.21051 | val_0_rmse: 0.46972 | val_1_rmse: 0.51255 |  0:00:46s
epoch 74 | loss: 0.21226 | val_0_rmse: 0.47278 | val_1_rmse: 0.51503 |  0:00:47s
epoch 75 | loss: 0.21066 | val_0_rmse: 0.48239 | val_1_rmse: 0.51492 |  0:00:47s
epoch 76 | loss: 0.21048 | val_0_rmse: 0.45626 | val_1_rmse: 0.51086 |  0:00:48s
epoch 77 | loss: 0.2159  | val_0_rmse: 0.47048 | val_1_rmse: 0.52631 |  0:00:49s
epoch 78 | loss: 0.2177  | val_0_rmse: 0.47026 | val_1_rmse: 0.5191  |  0:00:49s
epoch 79 | loss: 0.21085 | val_0_rmse: 0.45378 | val_1_rmse: 0.51139 |  0:00:50s
epoch 80 | loss: 0.20509 | val_0_rmse: 0.44528 | val_1_rmse: 0.51167 |  0:00:51s
epoch 81 | loss: 0.20211 | val_0_rmse: 0.44069 | val_1_rmse: 0.50274 |  0:00:51s
epoch 82 | loss: 0.20124 | val_0_rmse: 0.44446 | val_1_rmse: 0.50304 |  0:00:52s
epoch 83 | loss: 0.20161 | val_0_rmse: 0.43691 | val_1_rmse: 0.51008 |  0:00:52s
epoch 84 | loss: 0.20041 | val_0_rmse: 0.43615 | val_1_rmse: 0.5058  |  0:00:53s
epoch 85 | loss: 0.20293 | val_0_rmse: 0.4576  | val_1_rmse: 0.51057 |  0:00:54s
epoch 86 | loss: 0.21263 | val_0_rmse: 0.4368  | val_1_rmse: 0.51418 |  0:00:54s
epoch 87 | loss: 0.20168 | val_0_rmse: 0.43114 | val_1_rmse: 0.5144  |  0:00:55s
epoch 88 | loss: 0.19907 | val_0_rmse: 0.43803 | val_1_rmse: 0.50631 |  0:00:56s
epoch 89 | loss: 0.19247 | val_0_rmse: 0.42519 | val_1_rmse: 0.50754 |  0:00:56s
epoch 90 | loss: 0.19297 | val_0_rmse: 0.42329 | val_1_rmse: 0.50644 |  0:00:57s
epoch 91 | loss: 0.19378 | val_0_rmse: 0.42718 | val_1_rmse: 0.51193 |  0:00:57s
epoch 92 | loss: 0.20131 | val_0_rmse: 0.42514 | val_1_rmse: 0.51845 |  0:00:58s
epoch 93 | loss: 0.19759 | val_0_rmse: 0.42396 | val_1_rmse: 0.51871 |  0:00:59s
epoch 94 | loss: 0.1927  | val_0_rmse: 0.41807 | val_1_rmse: 0.52093 |  0:00:59s
epoch 95 | loss: 0.18912 | val_0_rmse: 0.42025 | val_1_rmse: 0.51673 |  0:01:00s
epoch 96 | loss: 0.19253 | val_0_rmse: 0.42615 | val_1_rmse: 0.52015 |  0:01:01s
epoch 97 | loss: 0.19558 | val_0_rmse: 0.41972 | val_1_rmse: 0.51365 |  0:01:01s
epoch 98 | loss: 0.19409 | val_0_rmse: 0.42093 | val_1_rmse: 0.51174 |  0:01:02s
epoch 99 | loss: 0.19392 | val_0_rmse: 0.4156  | val_1_rmse: 0.5201  |  0:01:02s
epoch 100| loss: 0.20073 | val_0_rmse: 0.4161  | val_1_rmse: 0.50745 |  0:01:03s
epoch 101| loss: 0.19346 | val_0_rmse: 0.41583 | val_1_rmse: 0.50553 |  0:01:04s
epoch 102| loss: 0.19861 | val_0_rmse: 0.42482 | val_1_rmse: 0.51892 |  0:01:04s
epoch 103| loss: 0.20455 | val_0_rmse: 0.4227  | val_1_rmse: 0.52038 |  0:01:05s
epoch 104| loss: 0.19707 | val_0_rmse: 0.41415 | val_1_rmse: 0.50507 |  0:01:06s
epoch 105| loss: 0.19179 | val_0_rmse: 0.41565 | val_1_rmse: 0.5095  |  0:01:06s
epoch 106| loss: 0.19922 | val_0_rmse: 0.44187 | val_1_rmse: 0.53646 |  0:01:07s
epoch 107| loss: 0.2291  | val_0_rmse: 0.44201 | val_1_rmse: 0.53838 |  0:01:07s
epoch 108| loss: 0.23265 | val_0_rmse: 0.46988 | val_1_rmse: 0.55642 |  0:01:08s
epoch 109| loss: 0.23194 | val_0_rmse: 0.45813 | val_1_rmse: 0.54772 |  0:01:09s
epoch 110| loss: 0.2203  | val_0_rmse: 0.44098 | val_1_rmse: 0.54848 |  0:01:09s
epoch 111| loss: 0.22061 | val_0_rmse: 0.43071 | val_1_rmse: 0.53172 |  0:01:10s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.50274
Best weights from best epoch are automatically used!
ended training at: 22:55:30
Feature importance:
Mean squared error is of 22956114930.93265
Mean absolute error:107786.36158770372
MAPE:0.1767696270877669
R2 score:0.7216527337158718
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:55:30
epoch 0  | loss: 2.09579 | val_0_rmse: 0.99906 | val_1_rmse: 0.99878 |  0:00:00s
epoch 1  | loss: 1.08654 | val_0_rmse: 0.98536 | val_1_rmse: 0.98579 |  0:00:01s
epoch 2  | loss: 0.98817 | val_0_rmse: 0.97736 | val_1_rmse: 0.97757 |  0:00:01s
epoch 3  | loss: 0.8893  | val_0_rmse: 0.93304 | val_1_rmse: 0.92979 |  0:00:02s
epoch 4  | loss: 0.73004 | val_0_rmse: 0.8926  | val_1_rmse: 0.8953  |  0:00:03s
epoch 5  | loss: 0.60626 | val_0_rmse: 0.76826 | val_1_rmse: 0.75547 |  0:00:03s
epoch 6  | loss: 0.5045  | val_0_rmse: 0.69504 | val_1_rmse: 0.6724  |  0:00:04s
epoch 7  | loss: 0.45668 | val_0_rmse: 0.69835 | val_1_rmse: 0.67274 |  0:00:05s
epoch 8  | loss: 0.4247  | val_0_rmse: 0.70732 | val_1_rmse: 0.68838 |  0:00:05s
epoch 9  | loss: 0.39874 | val_0_rmse: 0.62589 | val_1_rmse: 0.60028 |  0:00:06s
epoch 10 | loss: 0.38884 | val_0_rmse: 0.60201 | val_1_rmse: 0.57668 |  0:00:06s
epoch 11 | loss: 0.38656 | val_0_rmse: 0.61552 | val_1_rmse: 0.59456 |  0:00:07s
epoch 12 | loss: 0.39618 | val_0_rmse: 0.6319  | val_1_rmse: 0.60589 |  0:00:08s
epoch 13 | loss: 0.38669 | val_0_rmse: 0.61854 | val_1_rmse: 0.60101 |  0:00:08s
epoch 14 | loss: 0.38576 | val_0_rmse: 0.59184 | val_1_rmse: 0.57086 |  0:00:09s
epoch 15 | loss: 0.36518 | val_0_rmse: 0.60308 | val_1_rmse: 0.57869 |  0:00:10s
epoch 16 | loss: 0.36593 | val_0_rmse: 0.57821 | val_1_rmse: 0.55418 |  0:00:10s
epoch 17 | loss: 0.36476 | val_0_rmse: 0.58136 | val_1_rmse: 0.55731 |  0:00:11s
epoch 18 | loss: 0.3497  | val_0_rmse: 0.57851 | val_1_rmse: 0.55305 |  0:00:12s
epoch 19 | loss: 0.35265 | val_0_rmse: 0.58046 | val_1_rmse: 0.55672 |  0:00:12s
epoch 20 | loss: 0.35143 | val_0_rmse: 0.58832 | val_1_rmse: 0.56435 |  0:00:13s
epoch 21 | loss: 0.34397 | val_0_rmse: 0.57559 | val_1_rmse: 0.55098 |  0:00:13s
epoch 22 | loss: 0.34197 | val_0_rmse: 0.56937 | val_1_rmse: 0.5476  |  0:00:14s
epoch 23 | loss: 0.34208 | val_0_rmse: 0.56993 | val_1_rmse: 0.54869 |  0:00:15s
epoch 24 | loss: 0.33818 | val_0_rmse: 0.5729  | val_1_rmse: 0.55082 |  0:00:15s
epoch 25 | loss: 0.34416 | val_0_rmse: 0.58547 | val_1_rmse: 0.57032 |  0:00:16s
epoch 26 | loss: 0.33632 | val_0_rmse: 0.57503 | val_1_rmse: 0.55055 |  0:00:17s
epoch 27 | loss: 0.33493 | val_0_rmse: 0.57168 | val_1_rmse: 0.55133 |  0:00:17s
epoch 28 | loss: 0.34301 | val_0_rmse: 0.5699  | val_1_rmse: 0.55037 |  0:00:18s
epoch 29 | loss: 0.33266 | val_0_rmse: 0.56192 | val_1_rmse: 0.54577 |  0:00:18s
epoch 30 | loss: 0.33305 | val_0_rmse: 0.5669  | val_1_rmse: 0.54665 |  0:00:19s
epoch 31 | loss: 0.32412 | val_0_rmse: 0.56957 | val_1_rmse: 0.55129 |  0:00:20s
epoch 32 | loss: 0.32134 | val_0_rmse: 0.59147 | val_1_rmse: 0.57381 |  0:00:20s
epoch 33 | loss: 0.32793 | val_0_rmse: 0.55531 | val_1_rmse: 0.5418  |  0:00:21s
epoch 34 | loss: 0.33001 | val_0_rmse: 0.56289 | val_1_rmse: 0.55145 |  0:00:22s
epoch 35 | loss: 0.3298  | val_0_rmse: 0.56554 | val_1_rmse: 0.55149 |  0:00:22s
epoch 36 | loss: 0.33472 | val_0_rmse: 0.56065 | val_1_rmse: 0.54511 |  0:00:23s
epoch 37 | loss: 0.34042 | val_0_rmse: 0.5747  | val_1_rmse: 0.56433 |  0:00:23s
epoch 38 | loss: 0.33073 | val_0_rmse: 0.56043 | val_1_rmse: 0.55209 |  0:00:24s
epoch 39 | loss: 0.3238  | val_0_rmse: 0.57122 | val_1_rmse: 0.57319 |  0:00:25s
epoch 40 | loss: 0.30792 | val_0_rmse: 0.56052 | val_1_rmse: 0.5513  |  0:00:25s
epoch 41 | loss: 0.31314 | val_0_rmse: 0.55078 | val_1_rmse: 0.54922 |  0:00:26s
epoch 42 | loss: 0.299   | val_0_rmse: 0.56089 | val_1_rmse: 0.55711 |  0:00:27s
epoch 43 | loss: 0.31017 | val_0_rmse: 0.56263 | val_1_rmse: 0.55989 |  0:00:27s
epoch 44 | loss: 0.30057 | val_0_rmse: 0.54154 | val_1_rmse: 0.5465  |  0:00:28s
epoch 45 | loss: 0.30312 | val_0_rmse: 0.5741  | val_1_rmse: 0.57176 |  0:00:28s
epoch 46 | loss: 0.3094  | val_0_rmse: 0.55752 | val_1_rmse: 0.5567  |  0:00:29s
epoch 47 | loss: 0.30702 | val_0_rmse: 0.54862 | val_1_rmse: 0.56065 |  0:00:30s
epoch 48 | loss: 0.30344 | val_0_rmse: 0.54352 | val_1_rmse: 0.54683 |  0:00:30s
epoch 49 | loss: 0.30965 | val_0_rmse: 0.56787 | val_1_rmse: 0.57358 |  0:00:31s
epoch 50 | loss: 0.29229 | val_0_rmse: 0.5479  | val_1_rmse: 0.55901 |  0:00:32s
epoch 51 | loss: 0.28822 | val_0_rmse: 0.53825 | val_1_rmse: 0.54131 |  0:00:32s
epoch 52 | loss: 0.28376 | val_0_rmse: 0.54393 | val_1_rmse: 0.55011 |  0:00:33s
epoch 53 | loss: 0.28607 | val_0_rmse: 0.53317 | val_1_rmse: 0.54031 |  0:00:34s
epoch 54 | loss: 0.29259 | val_0_rmse: 0.53237 | val_1_rmse: 0.53938 |  0:00:34s
epoch 55 | loss: 0.27439 | val_0_rmse: 0.529   | val_1_rmse: 0.53659 |  0:00:35s
epoch 56 | loss: 0.27561 | val_0_rmse: 0.53338 | val_1_rmse: 0.53671 |  0:00:35s
epoch 57 | loss: 0.27983 | val_0_rmse: 0.52948 | val_1_rmse: 0.53604 |  0:00:36s
epoch 58 | loss: 0.27799 | val_0_rmse: 0.52426 | val_1_rmse: 0.53314 |  0:00:37s
epoch 59 | loss: 0.27646 | val_0_rmse: 0.5221  | val_1_rmse: 0.53137 |  0:00:37s
epoch 60 | loss: 0.27654 | val_0_rmse: 0.55052 | val_1_rmse: 0.57112 |  0:00:38s
epoch 61 | loss: 0.28173 | val_0_rmse: 0.51626 | val_1_rmse: 0.53538 |  0:00:39s
epoch 62 | loss: 0.28125 | val_0_rmse: 0.51442 | val_1_rmse: 0.53346 |  0:00:39s
epoch 63 | loss: 0.28397 | val_0_rmse: 0.62052 | val_1_rmse: 0.67779 |  0:00:40s
epoch 64 | loss: 0.27789 | val_0_rmse: 0.51989 | val_1_rmse: 0.53951 |  0:00:40s
epoch 65 | loss: 0.27538 | val_0_rmse: 0.52136 | val_1_rmse: 0.53862 |  0:00:41s
epoch 66 | loss: 0.27979 | val_0_rmse: 0.52319 | val_1_rmse: 0.52942 |  0:00:42s
epoch 67 | loss: 0.27392 | val_0_rmse: 0.53034 | val_1_rmse: 0.52663 |  0:00:42s
epoch 68 | loss: 0.26954 | val_0_rmse: 0.51954 | val_1_rmse: 0.51617 |  0:00:43s
epoch 69 | loss: 0.27729 | val_0_rmse: 0.53988 | val_1_rmse: 0.54384 |  0:00:44s
epoch 70 | loss: 0.27895 | val_0_rmse: 0.53496 | val_1_rmse: 0.54212 |  0:00:44s
epoch 71 | loss: 0.27949 | val_0_rmse: 0.51849 | val_1_rmse: 0.52185 |  0:00:45s
epoch 72 | loss: 0.27461 | val_0_rmse: 0.50872 | val_1_rmse: 0.51127 |  0:00:45s
epoch 73 | loss: 0.27123 | val_0_rmse: 0.51258 | val_1_rmse: 0.52243 |  0:00:46s
epoch 74 | loss: 0.29056 | val_0_rmse: 0.51289 | val_1_rmse: 0.52254 |  0:00:47s
epoch 75 | loss: 0.28234 | val_0_rmse: 0.51939 | val_1_rmse: 0.54394 |  0:00:47s
epoch 76 | loss: 0.28065 | val_0_rmse: 0.4983  | val_1_rmse: 0.51356 |  0:00:48s
epoch 77 | loss: 0.27072 | val_0_rmse: 0.49741 | val_1_rmse: 0.50597 |  0:00:49s
epoch 78 | loss: 0.25928 | val_0_rmse: 0.49249 | val_1_rmse: 0.50593 |  0:00:49s
epoch 79 | loss: 0.2901  | val_0_rmse: 0.52911 | val_1_rmse: 0.53144 |  0:00:50s
epoch 80 | loss: 0.30384 | val_0_rmse: 0.5252  | val_1_rmse: 0.52982 |  0:00:51s
epoch 81 | loss: 0.28715 | val_0_rmse: 0.53201 | val_1_rmse: 0.54669 |  0:00:51s
epoch 82 | loss: 0.27849 | val_0_rmse: 0.51016 | val_1_rmse: 0.53026 |  0:00:52s
epoch 83 | loss: 0.27387 | val_0_rmse: 0.51375 | val_1_rmse: 0.54233 |  0:00:52s
epoch 84 | loss: 0.27177 | val_0_rmse: 0.51125 | val_1_rmse: 0.54794 |  0:00:53s
epoch 85 | loss: 0.26706 | val_0_rmse: 0.50381 | val_1_rmse: 0.54091 |  0:00:54s
epoch 86 | loss: 0.26565 | val_0_rmse: 0.49828 | val_1_rmse: 0.5328  |  0:00:54s
epoch 87 | loss: 0.27342 | val_0_rmse: 0.50668 | val_1_rmse: 0.53261 |  0:00:55s
epoch 88 | loss: 0.26638 | val_0_rmse: 0.50766 | val_1_rmse: 0.5231  |  0:00:56s
epoch 89 | loss: 0.26403 | val_0_rmse: 0.49382 | val_1_rmse: 0.50807 |  0:00:56s
epoch 90 | loss: 0.25303 | val_0_rmse: 0.49367 | val_1_rmse: 0.51273 |  0:00:57s
epoch 91 | loss: 0.2601  | val_0_rmse: 0.50806 | val_1_rmse: 0.52131 |  0:00:57s
epoch 92 | loss: 0.24798 | val_0_rmse: 0.4975  | val_1_rmse: 0.5107  |  0:00:58s
epoch 93 | loss: 0.24858 | val_0_rmse: 0.4932  | val_1_rmse: 0.51002 |  0:00:59s
epoch 94 | loss: 0.24449 | val_0_rmse: 0.47719 | val_1_rmse: 0.50646 |  0:00:59s
epoch 95 | loss: 0.24198 | val_0_rmse: 0.46547 | val_1_rmse: 0.50227 |  0:01:00s
epoch 96 | loss: 0.23589 | val_0_rmse: 0.4713  | val_1_rmse: 0.50599 |  0:01:01s
epoch 97 | loss: 0.23928 | val_0_rmse: 0.4707  | val_1_rmse: 0.51683 |  0:01:01s
epoch 98 | loss: 0.2374  | val_0_rmse: 0.48054 | val_1_rmse: 0.50652 |  0:01:02s
epoch 99 | loss: 0.23632 | val_0_rmse: 0.47887 | val_1_rmse: 0.51532 |  0:01:02s
epoch 100| loss: 0.23151 | val_0_rmse: 0.4698  | val_1_rmse: 0.49787 |  0:01:03s
epoch 101| loss: 0.23953 | val_0_rmse: 0.4802  | val_1_rmse: 0.5111  |  0:01:04s
epoch 102| loss: 0.2374  | val_0_rmse: 0.46331 | val_1_rmse: 0.50084 |  0:01:04s
epoch 103| loss: 0.23439 | val_0_rmse: 0.46057 | val_1_rmse: 0.49551 |  0:01:05s
epoch 104| loss: 0.23098 | val_0_rmse: 0.45326 | val_1_rmse: 0.49157 |  0:01:06s
epoch 105| loss: 0.22855 | val_0_rmse: 0.45564 | val_1_rmse: 0.50197 |  0:01:06s
epoch 106| loss: 0.2223  | val_0_rmse: 0.45046 | val_1_rmse: 0.48844 |  0:01:07s
epoch 107| loss: 0.22321 | val_0_rmse: 0.46043 | val_1_rmse: 0.49946 |  0:01:07s
epoch 108| loss: 0.22196 | val_0_rmse: 0.47743 | val_1_rmse: 0.50648 |  0:01:08s
epoch 109| loss: 0.22856 | val_0_rmse: 0.45239 | val_1_rmse: 0.49586 |  0:01:09s
epoch 110| loss: 0.21849 | val_0_rmse: 0.48201 | val_1_rmse: 0.52839 |  0:01:09s
epoch 111| loss: 0.22595 | val_0_rmse: 0.46236 | val_1_rmse: 0.49874 |  0:01:10s
epoch 112| loss: 0.2225  | val_0_rmse: 0.46271 | val_1_rmse: 0.49785 |  0:01:11s
epoch 113| loss: 0.22203 | val_0_rmse: 0.45503 | val_1_rmse: 0.51497 |  0:01:11s
epoch 114| loss: 0.22125 | val_0_rmse: 0.44063 | val_1_rmse: 0.49099 |  0:01:12s
epoch 115| loss: 0.21531 | val_0_rmse: 0.45279 | val_1_rmse: 0.50229 |  0:01:13s
epoch 116| loss: 0.2214  | val_0_rmse: 0.45027 | val_1_rmse: 0.50246 |  0:01:13s
epoch 117| loss: 0.21481 | val_0_rmse: 0.43688 | val_1_rmse: 0.49316 |  0:01:14s
epoch 118| loss: 0.21433 | val_0_rmse: 0.44283 | val_1_rmse: 0.50426 |  0:01:14s
epoch 119| loss: 0.21271 | val_0_rmse: 0.43718 | val_1_rmse: 0.50179 |  0:01:15s
epoch 120| loss: 0.21195 | val_0_rmse: 0.4334  | val_1_rmse: 0.49513 |  0:01:16s
epoch 121| loss: 0.20669 | val_0_rmse: 0.44389 | val_1_rmse: 0.50562 |  0:01:16s
epoch 122| loss: 0.21    | val_0_rmse: 0.42866 | val_1_rmse: 0.50988 |  0:01:17s
epoch 123| loss: 0.20941 | val_0_rmse: 0.4326  | val_1_rmse: 0.50863 |  0:01:18s
epoch 124| loss: 0.20386 | val_0_rmse: 0.42895 | val_1_rmse: 0.50813 |  0:01:18s
epoch 125| loss: 0.20755 | val_0_rmse: 0.43339 | val_1_rmse: 0.5058  |  0:01:19s
epoch 126| loss: 0.2044  | val_0_rmse: 0.42638 | val_1_rmse: 0.50479 |  0:01:19s
epoch 127| loss: 0.19941 | val_0_rmse: 0.42237 | val_1_rmse: 0.50156 |  0:01:20s
epoch 128| loss: 0.1983  | val_0_rmse: 0.42229 | val_1_rmse: 0.50055 |  0:01:21s
epoch 129| loss: 0.19539 | val_0_rmse: 0.42717 | val_1_rmse: 0.50556 |  0:01:21s
epoch 130| loss: 0.19871 | val_0_rmse: 0.42799 | val_1_rmse: 0.51414 |  0:01:22s
epoch 131| loss: 0.20181 | val_0_rmse: 0.42388 | val_1_rmse: 0.51587 |  0:01:23s
epoch 132| loss: 0.19669 | val_0_rmse: 0.43056 | val_1_rmse: 0.50725 |  0:01:23s
epoch 133| loss: 0.2056  | val_0_rmse: 0.41673 | val_1_rmse: 0.51634 |  0:01:24s
epoch 134| loss: 0.19335 | val_0_rmse: 0.42176 | val_1_rmse: 0.50611 |  0:01:24s
epoch 135| loss: 0.1902  | val_0_rmse: 0.41705 | val_1_rmse: 0.51417 |  0:01:25s
epoch 136| loss: 0.19737 | val_0_rmse: 0.42799 | val_1_rmse: 0.508   |  0:01:26s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.48844
Best weights from best epoch are automatically used!
ended training at: 22:56:56
Feature importance:
Mean squared error is of 18811670127.630234
Mean absolute error:100505.83039018139
MAPE:0.1764360161302892
R2 score:0.7697935016238131
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:56:56
epoch 0  | loss: 1.96446 | val_0_rmse: 1.00278 | val_1_rmse: 0.99538 |  0:00:00s
epoch 1  | loss: 1.21831 | val_0_rmse: 1.0041  | val_1_rmse: 0.99544 |  0:00:01s
epoch 2  | loss: 1.04261 | val_0_rmse: 1.00335 | val_1_rmse: 0.99424 |  0:00:01s
epoch 3  | loss: 1.01806 | val_0_rmse: 1.00199 | val_1_rmse: 0.99322 |  0:00:02s
epoch 4  | loss: 1.00598 | val_0_rmse: 1.00431 | val_1_rmse: 0.99694 |  0:00:03s
epoch 5  | loss: 0.98817 | val_0_rmse: 0.99691 | val_1_rmse: 0.98942 |  0:00:03s
epoch 6  | loss: 0.9452  | val_0_rmse: 0.99807 | val_1_rmse: 0.99343 |  0:00:04s
epoch 7  | loss: 0.87551 | val_0_rmse: 0.95598 | val_1_rmse: 0.95628 |  0:00:04s
epoch 8  | loss: 0.81083 | val_0_rmse: 0.90442 | val_1_rmse: 0.89347 |  0:00:05s
epoch 9  | loss: 0.71249 | val_0_rmse: 0.8782  | val_1_rmse: 0.86192 |  0:00:06s
epoch 10 | loss: 0.60476 | val_0_rmse: 0.8608  | val_1_rmse: 0.83979 |  0:00:06s
epoch 11 | loss: 0.5223  | val_0_rmse: 0.89808 | val_1_rmse: 0.86505 |  0:00:07s
epoch 12 | loss: 0.47558 | val_0_rmse: 0.90098 | val_1_rmse: 0.86178 |  0:00:08s
epoch 13 | loss: 0.42663 | val_0_rmse: 0.81649 | val_1_rmse: 0.78501 |  0:00:08s
epoch 14 | loss: 0.39538 | val_0_rmse: 0.80525 | val_1_rmse: 0.76931 |  0:00:09s
epoch 15 | loss: 0.37392 | val_0_rmse: 0.83395 | val_1_rmse: 0.79794 |  0:00:10s
epoch 16 | loss: 0.34645 | val_0_rmse: 0.79843 | val_1_rmse: 0.769   |  0:00:10s
epoch 17 | loss: 0.32924 | val_0_rmse: 0.81065 | val_1_rmse: 0.78344 |  0:00:11s
epoch 18 | loss: 0.31476 | val_0_rmse: 0.7985  | val_1_rmse: 0.7748  |  0:00:11s
epoch 19 | loss: 0.28737 | val_0_rmse: 0.78589 | val_1_rmse: 0.76251 |  0:00:12s
epoch 20 | loss: 0.27711 | val_0_rmse: 0.77392 | val_1_rmse: 0.74861 |  0:00:13s
epoch 21 | loss: 0.27391 | val_0_rmse: 0.80051 | val_1_rmse: 0.77828 |  0:00:13s
epoch 22 | loss: 0.25951 | val_0_rmse: 0.77065 | val_1_rmse: 0.74704 |  0:00:14s
epoch 23 | loss: 0.2558  | val_0_rmse: 0.74793 | val_1_rmse: 0.72992 |  0:00:15s
epoch 24 | loss: 0.25581 | val_0_rmse: 0.75924 | val_1_rmse: 0.74819 |  0:00:15s
epoch 25 | loss: 0.24973 | val_0_rmse: 0.73392 | val_1_rmse: 0.72573 |  0:00:16s
epoch 26 | loss: 0.2496  | val_0_rmse: 0.74971 | val_1_rmse: 0.74086 |  0:00:16s
epoch 27 | loss: 0.23521 | val_0_rmse: 0.75406 | val_1_rmse: 0.74606 |  0:00:17s
epoch 28 | loss: 0.23472 | val_0_rmse: 0.73382 | val_1_rmse: 0.72299 |  0:00:18s
epoch 29 | loss: 0.24153 | val_0_rmse: 0.73184 | val_1_rmse: 0.72389 |  0:00:18s
epoch 30 | loss: 0.2326  | val_0_rmse: 0.71476 | val_1_rmse: 0.71082 |  0:00:19s
epoch 31 | loss: 0.22752 | val_0_rmse: 0.71355 | val_1_rmse: 0.70737 |  0:00:20s
epoch 32 | loss: 0.22213 | val_0_rmse: 0.73826 | val_1_rmse: 0.73038 |  0:00:20s
epoch 33 | loss: 0.21966 | val_0_rmse: 0.72138 | val_1_rmse: 0.70998 |  0:00:21s
epoch 34 | loss: 0.2175  | val_0_rmse: 0.69735 | val_1_rmse: 0.69327 |  0:00:21s
epoch 35 | loss: 0.21357 | val_0_rmse: 0.69517 | val_1_rmse: 0.69629 |  0:00:22s
epoch 36 | loss: 0.21805 | val_0_rmse: 0.68169 | val_1_rmse: 0.68226 |  0:00:23s
epoch 37 | loss: 0.21559 | val_0_rmse: 0.6968  | val_1_rmse: 0.70171 |  0:00:23s
epoch 38 | loss: 0.21322 | val_0_rmse: 0.67196 | val_1_rmse: 0.673   |  0:00:24s
epoch 39 | loss: 0.21921 | val_0_rmse: 0.67392 | val_1_rmse: 0.67914 |  0:00:25s
epoch 40 | loss: 0.21053 | val_0_rmse: 0.66588 | val_1_rmse: 0.67713 |  0:00:25s
epoch 41 | loss: 0.21357 | val_0_rmse: 0.66649 | val_1_rmse: 0.67379 |  0:00:26s
epoch 42 | loss: 0.20456 | val_0_rmse: 0.65293 | val_1_rmse: 0.66107 |  0:00:27s
epoch 43 | loss: 0.21104 | val_0_rmse: 0.71136 | val_1_rmse: 0.71255 |  0:00:27s
epoch 44 | loss: 0.20831 | val_0_rmse: 0.68855 | val_1_rmse: 0.68937 |  0:00:28s
epoch 45 | loss: 0.2127  | val_0_rmse: 0.66055 | val_1_rmse: 0.67052 |  0:00:28s
epoch 46 | loss: 0.20647 | val_0_rmse: 0.62717 | val_1_rmse: 0.63778 |  0:00:29s
epoch 47 | loss: 0.20303 | val_0_rmse: 0.62677 | val_1_rmse: 0.64692 |  0:00:30s
epoch 48 | loss: 0.20117 | val_0_rmse: 0.62537 | val_1_rmse: 0.63273 |  0:00:30s
epoch 49 | loss: 0.20264 | val_0_rmse: 0.5975  | val_1_rmse: 0.61497 |  0:00:31s
epoch 50 | loss: 0.19788 | val_0_rmse: 0.58901 | val_1_rmse: 0.61736 |  0:00:32s
epoch 51 | loss: 0.19584 | val_0_rmse: 0.5975  | val_1_rmse: 0.62724 |  0:00:32s
epoch 52 | loss: 0.19348 | val_0_rmse: 0.58902 | val_1_rmse: 0.62147 |  0:00:33s
epoch 53 | loss: 0.18677 | val_0_rmse: 0.5481  | val_1_rmse: 0.58508 |  0:00:33s
epoch 54 | loss: 0.18567 | val_0_rmse: 0.56027 | val_1_rmse: 0.59688 |  0:00:34s
epoch 55 | loss: 0.1945  | val_0_rmse: 0.55692 | val_1_rmse: 0.58306 |  0:00:35s
epoch 56 | loss: 0.19254 | val_0_rmse: 0.55339 | val_1_rmse: 0.59511 |  0:00:35s
epoch 57 | loss: 0.19206 | val_0_rmse: 0.56466 | val_1_rmse: 0.60504 |  0:00:36s
epoch 58 | loss: 0.19326 | val_0_rmse: 0.53324 | val_1_rmse: 0.57831 |  0:00:37s
epoch 59 | loss: 0.18797 | val_0_rmse: 0.54945 | val_1_rmse: 0.59335 |  0:00:37s
epoch 60 | loss: 0.18705 | val_0_rmse: 0.52082 | val_1_rmse: 0.57279 |  0:00:38s
epoch 61 | loss: 0.18623 | val_0_rmse: 0.52514 | val_1_rmse: 0.57384 |  0:00:39s
epoch 62 | loss: 0.17935 | val_0_rmse: 0.61137 | val_1_rmse: 0.64862 |  0:00:39s
epoch 63 | loss: 0.18265 | val_0_rmse: 0.50376 | val_1_rmse: 0.56557 |  0:00:40s
epoch 64 | loss: 0.18687 | val_0_rmse: 0.49764 | val_1_rmse: 0.55802 |  0:00:40s
epoch 65 | loss: 0.178   | val_0_rmse: 0.49406 | val_1_rmse: 0.56183 |  0:00:41s
epoch 66 | loss: 0.17396 | val_0_rmse: 0.48632 | val_1_rmse: 0.55326 |  0:00:42s
epoch 67 | loss: 0.1722  | val_0_rmse: 0.49145 | val_1_rmse: 0.55541 |  0:00:42s
epoch 68 | loss: 0.17448 | val_0_rmse: 0.48594 | val_1_rmse: 0.55903 |  0:00:43s
epoch 69 | loss: 0.17595 | val_0_rmse: 0.46199 | val_1_rmse: 0.54232 |  0:00:44s
epoch 70 | loss: 0.17685 | val_0_rmse: 0.47794 | val_1_rmse: 0.55758 |  0:00:44s
epoch 71 | loss: 0.17413 | val_0_rmse: 0.48095 | val_1_rmse: 0.54734 |  0:00:45s
epoch 72 | loss: 0.1709  | val_0_rmse: 0.44631 | val_1_rmse: 0.53821 |  0:00:45s
epoch 73 | loss: 0.16986 | val_0_rmse: 0.45285 | val_1_rmse: 0.54657 |  0:00:46s
epoch 74 | loss: 0.16985 | val_0_rmse: 0.44285 | val_1_rmse: 0.53593 |  0:00:47s
epoch 75 | loss: 0.17226 | val_0_rmse: 0.4363  | val_1_rmse: 0.54061 |  0:00:47s
epoch 76 | loss: 0.16848 | val_0_rmse: 0.42736 | val_1_rmse: 0.53279 |  0:00:48s
epoch 77 | loss: 0.16599 | val_0_rmse: 0.43701 | val_1_rmse: 0.53523 |  0:00:49s
epoch 78 | loss: 0.16157 | val_0_rmse: 0.42176 | val_1_rmse: 0.53584 |  0:00:49s
epoch 79 | loss: 0.16441 | val_0_rmse: 0.43039 | val_1_rmse: 0.5367  |  0:00:50s
epoch 80 | loss: 0.16076 | val_0_rmse: 0.4153  | val_1_rmse: 0.53881 |  0:00:50s
epoch 81 | loss: 0.16299 | val_0_rmse: 0.42129 | val_1_rmse: 0.54255 |  0:00:51s
epoch 82 | loss: 0.16885 | val_0_rmse: 0.42681 | val_1_rmse: 0.54383 |  0:00:52s
epoch 83 | loss: 0.16332 | val_0_rmse: 0.40497 | val_1_rmse: 0.5252  |  0:00:52s
epoch 84 | loss: 0.15966 | val_0_rmse: 0.41399 | val_1_rmse: 0.54774 |  0:00:53s
epoch 85 | loss: 0.16294 | val_0_rmse: 0.40793 | val_1_rmse: 0.53286 |  0:00:54s
epoch 86 | loss: 0.1607  | val_0_rmse: 0.40212 | val_1_rmse: 0.54236 |  0:00:54s
epoch 87 | loss: 0.16835 | val_0_rmse: 0.38178 | val_1_rmse: 0.53197 |  0:00:55s
epoch 88 | loss: 0.15543 | val_0_rmse: 0.3965  | val_1_rmse: 0.53132 |  0:00:56s
epoch 89 | loss: 0.15273 | val_0_rmse: 0.38112 | val_1_rmse: 0.53752 |  0:00:56s
epoch 90 | loss: 0.1522  | val_0_rmse: 0.37914 | val_1_rmse: 0.52935 |  0:00:57s
epoch 91 | loss: 0.15397 | val_0_rmse: 0.36866 | val_1_rmse: 0.52751 |  0:00:57s
epoch 92 | loss: 0.15049 | val_0_rmse: 0.37078 | val_1_rmse: 0.52571 |  0:00:58s
epoch 93 | loss: 0.1533  | val_0_rmse: 0.37009 | val_1_rmse: 0.53163 |  0:00:59s
epoch 94 | loss: 0.1532  | val_0_rmse: 0.36788 | val_1_rmse: 0.52712 |  0:00:59s
epoch 95 | loss: 0.15255 | val_0_rmse: 0.36026 | val_1_rmse: 0.532   |  0:01:00s
epoch 96 | loss: 0.15941 | val_0_rmse: 0.37219 | val_1_rmse: 0.52126 |  0:01:01s
epoch 97 | loss: 0.15301 | val_0_rmse: 0.36755 | val_1_rmse: 0.53251 |  0:01:01s
epoch 98 | loss: 0.15301 | val_0_rmse: 0.35913 | val_1_rmse: 0.53178 |  0:01:02s
epoch 99 | loss: 0.14977 | val_0_rmse: 0.35906 | val_1_rmse: 0.52393 |  0:01:02s
epoch 100| loss: 0.14971 | val_0_rmse: 0.35712 | val_1_rmse: 0.54815 |  0:01:03s
epoch 101| loss: 0.14964 | val_0_rmse: 0.37038 | val_1_rmse: 0.53294 |  0:01:04s
epoch 102| loss: 0.15283 | val_0_rmse: 0.34934 | val_1_rmse: 0.52439 |  0:01:04s
epoch 103| loss: 0.14579 | val_0_rmse: 0.35697 | val_1_rmse: 0.55155 |  0:01:05s
epoch 104| loss: 0.14376 | val_0_rmse: 0.35552 | val_1_rmse: 0.533   |  0:01:06s
epoch 105| loss: 0.1452  | val_0_rmse: 0.35121 | val_1_rmse: 0.53047 |  0:01:06s
epoch 106| loss: 0.14669 | val_0_rmse: 0.35351 | val_1_rmse: 0.53482 |  0:01:07s
epoch 107| loss: 0.1473  | val_0_rmse: 0.35312 | val_1_rmse: 0.54792 |  0:01:07s
epoch 108| loss: 0.14067 | val_0_rmse: 0.35094 | val_1_rmse: 0.53233 |  0:01:08s
epoch 109| loss: 0.14334 | val_0_rmse: 0.34357 | val_1_rmse: 0.53402 |  0:01:09s
epoch 110| loss: 0.14183 | val_0_rmse: 0.34906 | val_1_rmse: 0.53332 |  0:01:09s
epoch 111| loss: 0.14106 | val_0_rmse: 0.3396  | val_1_rmse: 0.5362  |  0:01:10s
epoch 112| loss: 0.14211 | val_0_rmse: 0.342   | val_1_rmse: 0.53307 |  0:01:11s
epoch 113| loss: 0.13823 | val_0_rmse: 0.33417 | val_1_rmse: 0.54487 |  0:01:11s
epoch 114| loss: 0.13712 | val_0_rmse: 0.33927 | val_1_rmse: 0.5266  |  0:01:12s
epoch 115| loss: 0.14708 | val_0_rmse: 0.3339  | val_1_rmse: 0.5371  |  0:01:12s
epoch 116| loss: 0.14235 | val_0_rmse: 0.3346  | val_1_rmse: 0.53722 |  0:01:13s
epoch 117| loss: 0.13936 | val_0_rmse: 0.33726 | val_1_rmse: 0.53112 |  0:01:14s
epoch 118| loss: 0.14015 | val_0_rmse: 0.3622  | val_1_rmse: 0.54257 |  0:01:14s
epoch 119| loss: 0.13497 | val_0_rmse: 0.33693 | val_1_rmse: 0.55079 |  0:01:15s
epoch 120| loss: 0.14021 | val_0_rmse: 0.33615 | val_1_rmse: 0.55231 |  0:01:16s
epoch 121| loss: 0.1388  | val_0_rmse: 0.34372 | val_1_rmse: 0.54649 |  0:01:16s
epoch 122| loss: 0.13425 | val_0_rmse: 0.33786 | val_1_rmse: 0.54223 |  0:01:17s
epoch 123| loss: 0.14556 | val_0_rmse: 0.34274 | val_1_rmse: 0.56165 |  0:01:18s
epoch 124| loss: 0.13496 | val_0_rmse: 0.32533 | val_1_rmse: 0.53302 |  0:01:18s
epoch 125| loss: 0.13177 | val_0_rmse: 0.32491 | val_1_rmse: 0.53737 |  0:01:19s
epoch 126| loss: 0.12582 | val_0_rmse: 0.3181  | val_1_rmse: 0.53729 |  0:01:19s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.52126
Best weights from best epoch are automatically used!
ended training at: 22:58:17
Feature importance:
Mean squared error is of 21204407701.83242
Mean absolute error:105720.04073245268
MAPE:0.1874555380399423
R2 score:0.7360725821217853
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:58:17
epoch 0  | loss: 1.89318 | val_0_rmse: 1.00078 | val_1_rmse: 0.99362 |  0:00:00s
epoch 1  | loss: 1.20642 | val_0_rmse: 0.99688 | val_1_rmse: 0.99003 |  0:00:01s
epoch 2  | loss: 1.03787 | val_0_rmse: 0.98922 | val_1_rmse: 0.98082 |  0:00:01s
epoch 3  | loss: 0.93913 | val_0_rmse: 0.9349  | val_1_rmse: 0.91435 |  0:00:02s
epoch 4  | loss: 0.83269 | val_0_rmse: 0.93192 | val_1_rmse: 0.91732 |  0:00:03s
epoch 5  | loss: 0.75361 | val_0_rmse: 0.89581 | val_1_rmse: 0.88786 |  0:00:03s
epoch 6  | loss: 0.66799 | val_0_rmse: 0.86399 | val_1_rmse: 0.84797 |  0:00:04s
epoch 7  | loss: 0.59039 | val_0_rmse: 0.91775 | val_1_rmse: 0.90345 |  0:00:05s
epoch 8  | loss: 0.53729 | val_0_rmse: 0.84782 | val_1_rmse: 0.82218 |  0:00:05s
epoch 9  | loss: 0.48995 | val_0_rmse: 0.82411 | val_1_rmse: 0.79903 |  0:00:06s
epoch 10 | loss: 0.44741 | val_0_rmse: 0.81429 | val_1_rmse: 0.79482 |  0:00:06s
epoch 11 | loss: 0.41057 | val_0_rmse: 0.80467 | val_1_rmse: 0.78209 |  0:00:07s
epoch 12 | loss: 0.38683 | val_0_rmse: 0.79738 | val_1_rmse: 0.77522 |  0:00:08s
epoch 13 | loss: 0.36442 | val_0_rmse: 0.7766  | val_1_rmse: 0.75212 |  0:00:08s
epoch 14 | loss: 0.35517 | val_0_rmse: 0.75938 | val_1_rmse: 0.73496 |  0:00:09s
epoch 15 | loss: 0.34578 | val_0_rmse: 0.80491 | val_1_rmse: 0.78329 |  0:00:10s
epoch 16 | loss: 0.33854 | val_0_rmse: 0.80236 | val_1_rmse: 0.78076 |  0:00:10s
epoch 17 | loss: 0.31969 | val_0_rmse: 0.77645 | val_1_rmse: 0.75427 |  0:00:11s
epoch 18 | loss: 0.31478 | val_0_rmse: 0.84112 | val_1_rmse: 0.81418 |  0:00:11s
epoch 19 | loss: 0.30247 | val_0_rmse: 0.80178 | val_1_rmse: 0.77638 |  0:00:12s
epoch 20 | loss: 0.2942  | val_0_rmse: 0.76864 | val_1_rmse: 0.73976 |  0:00:13s
epoch 21 | loss: 0.2961  | val_0_rmse: 0.77775 | val_1_rmse: 0.75285 |  0:00:13s
epoch 22 | loss: 0.28869 | val_0_rmse: 0.80128 | val_1_rmse: 0.778   |  0:00:14s
epoch 23 | loss: 0.29031 | val_0_rmse: 0.76865 | val_1_rmse: 0.74326 |  0:00:15s
epoch 24 | loss: 0.27937 | val_0_rmse: 0.78148 | val_1_rmse: 0.76    |  0:00:15s
epoch 25 | loss: 0.27261 | val_0_rmse: 0.75308 | val_1_rmse: 0.73157 |  0:00:16s
epoch 26 | loss: 0.26776 | val_0_rmse: 0.76439 | val_1_rmse: 0.7443  |  0:00:17s
epoch 27 | loss: 0.26577 | val_0_rmse: 0.76004 | val_1_rmse: 0.74126 |  0:00:17s
epoch 28 | loss: 0.26429 | val_0_rmse: 0.74999 | val_1_rmse: 0.73183 |  0:00:18s
epoch 29 | loss: 0.25667 | val_0_rmse: 0.73533 | val_1_rmse: 0.71589 |  0:00:18s
epoch 30 | loss: 0.2631  | val_0_rmse: 0.72787 | val_1_rmse: 0.70815 |  0:00:19s
epoch 31 | loss: 0.26146 | val_0_rmse: 0.7369  | val_1_rmse: 0.71341 |  0:00:20s
epoch 32 | loss: 0.25848 | val_0_rmse: 0.7194  | val_1_rmse: 0.70095 |  0:00:20s
epoch 33 | loss: 0.25754 | val_0_rmse: 0.71554 | val_1_rmse: 0.6938  |  0:00:21s
epoch 34 | loss: 0.24861 | val_0_rmse: 0.70101 | val_1_rmse: 0.6813  |  0:00:22s
epoch 35 | loss: 0.24658 | val_0_rmse: 0.70019 | val_1_rmse: 0.67573 |  0:00:22s
epoch 36 | loss: 0.25065 | val_0_rmse: 0.70815 | val_1_rmse: 0.68301 |  0:00:23s
epoch 37 | loss: 0.24672 | val_0_rmse: 0.68524 | val_1_rmse: 0.66497 |  0:00:23s
epoch 38 | loss: 0.24511 | val_0_rmse: 0.68505 | val_1_rmse: 0.66834 |  0:00:24s
epoch 39 | loss: 0.24032 | val_0_rmse: 0.70408 | val_1_rmse: 0.68572 |  0:00:25s
epoch 40 | loss: 0.24247 | val_0_rmse: 0.68013 | val_1_rmse: 0.6572  |  0:00:25s
epoch 41 | loss: 0.24129 | val_0_rmse: 0.66899 | val_1_rmse: 0.65032 |  0:00:26s
epoch 42 | loss: 0.23279 | val_0_rmse: 0.65342 | val_1_rmse: 0.63666 |  0:00:27s
epoch 43 | loss: 0.24657 | val_0_rmse: 0.65737 | val_1_rmse: 0.63975 |  0:00:27s
epoch 44 | loss: 0.2422  | val_0_rmse: 0.66622 | val_1_rmse: 0.64878 |  0:00:28s
epoch 45 | loss: 0.22752 | val_0_rmse: 0.63006 | val_1_rmse: 0.61984 |  0:00:28s
epoch 46 | loss: 0.23255 | val_0_rmse: 0.64526 | val_1_rmse: 0.6302  |  0:00:29s
epoch 47 | loss: 0.23204 | val_0_rmse: 0.64198 | val_1_rmse: 0.62738 |  0:00:30s
epoch 48 | loss: 0.23001 | val_0_rmse: 0.62482 | val_1_rmse: 0.61397 |  0:00:30s
epoch 49 | loss: 0.23469 | val_0_rmse: 0.62198 | val_1_rmse: 0.60918 |  0:00:31s
epoch 50 | loss: 0.22705 | val_0_rmse: 0.60458 | val_1_rmse: 0.59393 |  0:00:32s
epoch 51 | loss: 0.2222  | val_0_rmse: 0.61475 | val_1_rmse: 0.6014  |  0:00:32s
epoch 52 | loss: 0.22999 | val_0_rmse: 0.59647 | val_1_rmse: 0.59019 |  0:00:33s
epoch 53 | loss: 0.22279 | val_0_rmse: 0.59822 | val_1_rmse: 0.59515 |  0:00:34s
epoch 54 | loss: 0.23968 | val_0_rmse: 0.62139 | val_1_rmse: 0.618   |  0:00:34s
epoch 55 | loss: 0.23986 | val_0_rmse: 0.58233 | val_1_rmse: 0.57753 |  0:00:35s
epoch 56 | loss: 0.2226  | val_0_rmse: 0.58863 | val_1_rmse: 0.58517 |  0:00:35s
epoch 57 | loss: 0.22822 | val_0_rmse: 0.57793 | val_1_rmse: 0.5714  |  0:00:36s
epoch 58 | loss: 0.22247 | val_0_rmse: 0.56591 | val_1_rmse: 0.55973 |  0:00:37s
epoch 59 | loss: 0.22078 | val_0_rmse: 0.57597 | val_1_rmse: 0.57018 |  0:00:37s
epoch 60 | loss: 0.22299 | val_0_rmse: 0.56126 | val_1_rmse: 0.55616 |  0:00:38s
epoch 61 | loss: 0.22118 | val_0_rmse: 0.57155 | val_1_rmse: 0.56654 |  0:00:39s
epoch 62 | loss: 0.21949 | val_0_rmse: 0.55565 | val_1_rmse: 0.56213 |  0:00:39s
epoch 63 | loss: 0.22162 | val_0_rmse: 0.55465 | val_1_rmse: 0.56044 |  0:00:40s
epoch 64 | loss: 0.21982 | val_0_rmse: 0.55325 | val_1_rmse: 0.56675 |  0:00:40s
epoch 65 | loss: 0.22222 | val_0_rmse: 0.53792 | val_1_rmse: 0.54321 |  0:00:41s
epoch 66 | loss: 0.22245 | val_0_rmse: 0.53933 | val_1_rmse: 0.55115 |  0:00:42s
epoch 67 | loss: 0.21818 | val_0_rmse: 0.52099 | val_1_rmse: 0.53567 |  0:00:42s
epoch 68 | loss: 0.21714 | val_0_rmse: 0.5269  | val_1_rmse: 0.54188 |  0:00:43s
epoch 69 | loss: 0.21747 | val_0_rmse: 0.51429 | val_1_rmse: 0.525   |  0:00:44s
epoch 70 | loss: 0.21502 | val_0_rmse: 0.49334 | val_1_rmse: 0.52837 |  0:00:44s
epoch 71 | loss: 0.21183 | val_0_rmse: 0.4941  | val_1_rmse: 0.53247 |  0:00:45s
epoch 72 | loss: 0.21851 | val_0_rmse: 0.48402 | val_1_rmse: 0.51868 |  0:00:46s
epoch 73 | loss: 0.20795 | val_0_rmse: 0.49547 | val_1_rmse: 0.52645 |  0:00:46s
epoch 74 | loss: 0.21273 | val_0_rmse: 0.5084  | val_1_rmse: 0.53292 |  0:00:47s
epoch 75 | loss: 0.21099 | val_0_rmse: 0.47615 | val_1_rmse: 0.50926 |  0:00:47s
epoch 76 | loss: 0.20807 | val_0_rmse: 0.47104 | val_1_rmse: 0.51202 |  0:00:48s
epoch 77 | loss: 0.20892 | val_0_rmse: 0.47066 | val_1_rmse: 0.52491 |  0:00:49s
epoch 78 | loss: 0.20466 | val_0_rmse: 0.46129 | val_1_rmse: 0.51776 |  0:00:49s
epoch 79 | loss: 0.2074  | val_0_rmse: 0.48019 | val_1_rmse: 0.52041 |  0:00:50s
epoch 80 | loss: 0.20549 | val_0_rmse: 0.4617  | val_1_rmse: 0.51688 |  0:00:51s
epoch 81 | loss: 0.19758 | val_0_rmse: 0.45587 | val_1_rmse: 0.51384 |  0:00:51s
epoch 82 | loss: 0.20343 | val_0_rmse: 0.44761 | val_1_rmse: 0.49891 |  0:00:52s
epoch 83 | loss: 0.20007 | val_0_rmse: 0.45909 | val_1_rmse: 0.51047 |  0:00:53s
epoch 84 | loss: 0.20209 | val_0_rmse: 0.44774 | val_1_rmse: 0.50826 |  0:00:53s
epoch 85 | loss: 0.19863 | val_0_rmse: 0.4456  | val_1_rmse: 0.51424 |  0:00:54s
epoch 86 | loss: 0.19459 | val_0_rmse: 0.43628 | val_1_rmse: 0.50256 |  0:00:54s
epoch 87 | loss: 0.19162 | val_0_rmse: 0.44264 | val_1_rmse: 0.51032 |  0:00:55s
epoch 88 | loss: 0.19632 | val_0_rmse: 0.42652 | val_1_rmse: 0.51078 |  0:00:56s
epoch 89 | loss: 0.19641 | val_0_rmse: 0.42767 | val_1_rmse: 0.50364 |  0:00:56s
epoch 90 | loss: 0.1935  | val_0_rmse: 0.42733 | val_1_rmse: 0.49774 |  0:00:57s
epoch 91 | loss: 0.19571 | val_0_rmse: 0.4251  | val_1_rmse: 0.51698 |  0:00:58s
epoch 92 | loss: 0.18787 | val_0_rmse: 0.43036 | val_1_rmse: 0.50323 |  0:00:58s
epoch 93 | loss: 0.1877  | val_0_rmse: 0.42767 | val_1_rmse: 0.50196 |  0:00:59s
epoch 94 | loss: 0.18668 | val_0_rmse: 0.41399 | val_1_rmse: 0.50501 |  0:00:59s
epoch 95 | loss: 0.18734 | val_0_rmse: 0.41751 | val_1_rmse: 0.5211  |  0:01:00s
epoch 96 | loss: 0.19045 | val_0_rmse: 0.41707 | val_1_rmse: 0.50488 |  0:01:01s
epoch 97 | loss: 0.19003 | val_0_rmse: 0.41919 | val_1_rmse: 0.50246 |  0:01:01s
epoch 98 | loss: 0.1898  | val_0_rmse: 0.42833 | val_1_rmse: 0.52051 |  0:01:02s
epoch 99 | loss: 0.19201 | val_0_rmse: 0.4119  | val_1_rmse: 0.50903 |  0:01:03s
epoch 100| loss: 0.18309 | val_0_rmse: 0.41462 | val_1_rmse: 0.50141 |  0:01:03s
epoch 101| loss: 0.18636 | val_0_rmse: 0.42382 | val_1_rmse: 0.50487 |  0:01:04s
epoch 102| loss: 0.18727 | val_0_rmse: 0.40397 | val_1_rmse: 0.50602 |  0:01:04s
epoch 103| loss: 0.17905 | val_0_rmse: 0.40665 | val_1_rmse: 0.51097 |  0:01:05s
epoch 104| loss: 0.1835  | val_0_rmse: 0.41626 | val_1_rmse: 0.5051  |  0:01:06s
epoch 105| loss: 0.18167 | val_0_rmse: 0.40246 | val_1_rmse: 0.50949 |  0:01:06s
epoch 106| loss: 0.18245 | val_0_rmse: 0.40486 | val_1_rmse: 0.52242 |  0:01:07s
epoch 107| loss: 0.18048 | val_0_rmse: 0.40345 | val_1_rmse: 0.52001 |  0:01:08s
epoch 108| loss: 0.17795 | val_0_rmse: 0.40102 | val_1_rmse: 0.5177  |  0:01:08s
epoch 109| loss: 0.18287 | val_0_rmse: 0.40113 | val_1_rmse: 0.52472 |  0:01:09s
epoch 110| loss: 0.1793  | val_0_rmse: 0.39749 | val_1_rmse: 0.50882 |  0:01:09s
epoch 111| loss: 0.1736  | val_0_rmse: 0.39355 | val_1_rmse: 0.5077  |  0:01:10s
epoch 112| loss: 0.17714 | val_0_rmse: 0.39321 | val_1_rmse: 0.51842 |  0:01:11s
epoch 113| loss: 0.17637 | val_0_rmse: 0.40145 | val_1_rmse: 0.51489 |  0:01:11s
epoch 114| loss: 0.178   | val_0_rmse: 0.39666 | val_1_rmse: 0.51912 |  0:01:12s
epoch 115| loss: 0.18145 | val_0_rmse: 0.40022 | val_1_rmse: 0.50918 |  0:01:13s
epoch 116| loss: 0.17595 | val_0_rmse: 0.39332 | val_1_rmse: 0.51124 |  0:01:13s
epoch 117| loss: 0.17675 | val_0_rmse: 0.3977  | val_1_rmse: 0.50437 |  0:01:14s
epoch 118| loss: 0.17273 | val_0_rmse: 0.39027 | val_1_rmse: 0.51161 |  0:01:14s
epoch 119| loss: 0.18119 | val_0_rmse: 0.40241 | val_1_rmse: 0.50008 |  0:01:15s
epoch 120| loss: 0.18421 | val_0_rmse: 0.40905 | val_1_rmse: 0.52127 |  0:01:16s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.49774
Best weights from best epoch are automatically used!
ended training at: 22:59:33
Feature importance:
Mean squared error is of 22063805278.669926
Mean absolute error:106465.2593552182
MAPE:0.17811868379924597
R2 score:0.7349436922342912
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:59:33
epoch 0  | loss: 3.64374 | val_0_rmse: 1.10128 | val_1_rmse: 1.15237 |  0:00:00s
epoch 1  | loss: 2.46683 | val_0_rmse: 1.08164 | val_1_rmse: 1.14131 |  0:00:00s
epoch 2  | loss: 2.28379 | val_0_rmse: 0.98694 | val_1_rmse: 1.03138 |  0:00:00s
epoch 3  | loss: 1.56956 | val_0_rmse: 0.98717 | val_1_rmse: 1.02029 |  0:00:00s
epoch 4  | loss: 1.2505  | val_0_rmse: 0.98214 | val_1_rmse: 1.01689 |  0:00:00s
epoch 5  | loss: 1.42248 | val_0_rmse: 0.97965 | val_1_rmse: 1.01737 |  0:00:01s
epoch 6  | loss: 1.12469 | val_0_rmse: 0.98119 | val_1_rmse: 1.01961 |  0:00:01s
epoch 7  | loss: 1.11714 | val_0_rmse: 0.97936 | val_1_rmse: 1.01789 |  0:00:01s
epoch 8  | loss: 0.97846 | val_0_rmse: 0.96715 | val_1_rmse: 1.0099  |  0:00:01s
epoch 9  | loss: 0.90028 | val_0_rmse: 0.95385 | val_1_rmse: 0.99733 |  0:00:01s
epoch 10 | loss: 0.89401 | val_0_rmse: 0.94608 | val_1_rmse: 0.98634 |  0:00:01s
epoch 11 | loss: 0.8522  | val_0_rmse: 0.94593 | val_1_rmse: 0.9875  |  0:00:02s
epoch 12 | loss: 0.80426 | val_0_rmse: 0.93406 | val_1_rmse: 0.98182 |  0:00:02s
epoch 13 | loss: 0.72839 | val_0_rmse: 0.90299 | val_1_rmse: 0.94191 |  0:00:02s
epoch 14 | loss: 0.67373 | val_0_rmse: 0.90039 | val_1_rmse: 0.90874 |  0:00:02s
epoch 15 | loss: 0.6477  | val_0_rmse: 0.87119 | val_1_rmse: 0.90751 |  0:00:02s
epoch 16 | loss: 0.6168  | val_0_rmse: 0.84182 | val_1_rmse: 0.87035 |  0:00:02s
epoch 17 | loss: 0.55471 | val_0_rmse: 0.83387 | val_1_rmse: 0.84487 |  0:00:02s
epoch 18 | loss: 0.50836 | val_0_rmse: 0.8567  | val_1_rmse: 0.84451 |  0:00:03s
epoch 19 | loss: 0.49246 | val_0_rmse: 0.87103 | val_1_rmse: 0.86766 |  0:00:03s
epoch 20 | loss: 0.48007 | val_0_rmse: 0.83566 | val_1_rmse: 0.85228 |  0:00:03s
epoch 21 | loss: 0.46613 | val_0_rmse: 0.82801 | val_1_rmse: 0.81407 |  0:00:03s
epoch 22 | loss: 0.41511 | val_0_rmse: 0.92406 | val_1_rmse: 0.87261 |  0:00:03s
epoch 23 | loss: 0.44503 | val_0_rmse: 0.85225 | val_1_rmse: 0.80321 |  0:00:03s
epoch 24 | loss: 0.42203 | val_0_rmse: 0.81264 | val_1_rmse: 0.7717  |  0:00:04s
epoch 25 | loss: 0.41856 | val_0_rmse: 0.81702 | val_1_rmse: 0.76902 |  0:00:04s
epoch 26 | loss: 0.37627 | val_0_rmse: 0.86623 | val_1_rmse: 0.79725 |  0:00:04s
epoch 27 | loss: 0.37089 | val_0_rmse: 0.83908 | val_1_rmse: 0.76489 |  0:00:04s
epoch 28 | loss: 0.36178 | val_0_rmse: 0.77973 | val_1_rmse: 0.72219 |  0:00:04s
epoch 29 | loss: 0.3492  | val_0_rmse: 0.75498 | val_1_rmse: 0.70872 |  0:00:04s
epoch 30 | loss: 0.34587 | val_0_rmse: 0.75267 | val_1_rmse: 0.70774 |  0:00:05s
epoch 31 | loss: 0.34943 | val_0_rmse: 0.74586 | val_1_rmse: 0.7103  |  0:00:05s
epoch 32 | loss: 0.35255 | val_0_rmse: 0.73454 | val_1_rmse: 0.7031  |  0:00:05s
epoch 33 | loss: 0.33871 | val_0_rmse: 0.73522 | val_1_rmse: 0.70489 |  0:00:05s
epoch 34 | loss: 0.37128 | val_0_rmse: 0.72826 | val_1_rmse: 0.70928 |  0:00:05s
epoch 35 | loss: 0.32669 | val_0_rmse: 0.7206  | val_1_rmse: 0.71135 |  0:00:05s
epoch 36 | loss: 0.32485 | val_0_rmse: 0.71035 | val_1_rmse: 0.70332 |  0:00:06s
epoch 37 | loss: 0.29931 | val_0_rmse: 0.71788 | val_1_rmse: 0.71522 |  0:00:06s
epoch 38 | loss: 0.31921 | val_0_rmse: 0.70896 | val_1_rmse: 0.71217 |  0:00:06s
epoch 39 | loss: 0.29239 | val_0_rmse: 0.68766 | val_1_rmse: 0.69541 |  0:00:06s
epoch 40 | loss: 0.31065 | val_0_rmse: 0.67537 | val_1_rmse: 0.68739 |  0:00:06s
epoch 41 | loss: 0.29302 | val_0_rmse: 0.68012 | val_1_rmse: 0.68109 |  0:00:06s
epoch 42 | loss: 0.30101 | val_0_rmse: 0.69715 | val_1_rmse: 0.69141 |  0:00:07s
epoch 43 | loss: 0.29036 | val_0_rmse: 0.68086 | val_1_rmse: 0.68223 |  0:00:07s
epoch 44 | loss: 0.27678 | val_0_rmse: 0.66572 | val_1_rmse: 0.67582 |  0:00:07s
epoch 45 | loss: 0.28207 | val_0_rmse: 0.65901 | val_1_rmse: 0.66554 |  0:00:07s
epoch 46 | loss: 0.27224 | val_0_rmse: 0.65711 | val_1_rmse: 0.65914 |  0:00:07s
epoch 47 | loss: 0.26324 | val_0_rmse: 0.66107 | val_1_rmse: 0.66492 |  0:00:07s
epoch 48 | loss: 0.26411 | val_0_rmse: 0.6614  | val_1_rmse: 0.67286 |  0:00:08s
epoch 49 | loss: 0.26949 | val_0_rmse: 0.66305 | val_1_rmse: 0.674   |  0:00:08s
epoch 50 | loss: 0.2549  | val_0_rmse: 0.66796 | val_1_rmse: 0.67336 |  0:00:08s
epoch 51 | loss: 0.25742 | val_0_rmse: 0.67675 | val_1_rmse: 0.67934 |  0:00:08s
epoch 52 | loss: 0.25709 | val_0_rmse: 0.6618  | val_1_rmse: 0.67659 |  0:00:08s
epoch 53 | loss: 0.25818 | val_0_rmse: 0.65659 | val_1_rmse: 0.68042 |  0:00:08s
epoch 54 | loss: 0.2695  | val_0_rmse: 0.65304 | val_1_rmse: 0.68012 |  0:00:09s
epoch 55 | loss: 0.23961 | val_0_rmse: 0.65431 | val_1_rmse: 0.67644 |  0:00:09s
epoch 56 | loss: 0.23807 | val_0_rmse: 0.65762 | val_1_rmse: 0.67537 |  0:00:09s
epoch 57 | loss: 0.24278 | val_0_rmse: 0.65557 | val_1_rmse: 0.67274 |  0:00:09s
epoch 58 | loss: 0.24976 | val_0_rmse: 0.65807 | val_1_rmse: 0.68001 |  0:00:09s
epoch 59 | loss: 0.23113 | val_0_rmse: 0.66263 | val_1_rmse: 0.68298 |  0:00:09s
epoch 60 | loss: 0.22337 | val_0_rmse: 0.64963 | val_1_rmse: 0.67448 |  0:00:09s
epoch 61 | loss: 0.21408 | val_0_rmse: 0.64028 | val_1_rmse: 0.66675 |  0:00:10s
epoch 62 | loss: 0.21786 | val_0_rmse: 0.63998 | val_1_rmse: 0.66612 |  0:00:10s
epoch 63 | loss: 0.21791 | val_0_rmse: 0.64497 | val_1_rmse: 0.67242 |  0:00:10s
epoch 64 | loss: 0.21759 | val_0_rmse: 0.6535  | val_1_rmse: 0.67796 |  0:00:10s
epoch 65 | loss: 0.21294 | val_0_rmse: 0.64892 | val_1_rmse: 0.67409 |  0:00:10s
epoch 66 | loss: 0.21261 | val_0_rmse: 0.64119 | val_1_rmse: 0.67132 |  0:00:10s
epoch 67 | loss: 0.21663 | val_0_rmse: 0.63153 | val_1_rmse: 0.6684  |  0:00:11s
epoch 68 | loss: 0.21862 | val_0_rmse: 0.63269 | val_1_rmse: 0.67634 |  0:00:11s
epoch 69 | loss: 0.21534 | val_0_rmse: 0.64031 | val_1_rmse: 0.68626 |  0:00:11s
epoch 70 | loss: 0.20459 | val_0_rmse: 0.6482  | val_1_rmse: 0.68124 |  0:00:11s
epoch 71 | loss: 0.20414 | val_0_rmse: 0.6592  | val_1_rmse: 0.68009 |  0:00:11s
epoch 72 | loss: 0.1998  | val_0_rmse: 0.66408 | val_1_rmse: 0.69114 |  0:00:11s
epoch 73 | loss: 0.20093 | val_0_rmse: 0.67974 | val_1_rmse: 0.69991 |  0:00:12s
epoch 74 | loss: 0.20497 | val_0_rmse: 0.67642 | val_1_rmse: 0.6953  |  0:00:12s
epoch 75 | loss: 0.20961 | val_0_rmse: 0.67074 | val_1_rmse: 0.70063 |  0:00:12s
epoch 76 | loss: 0.21336 | val_0_rmse: 0.66302 | val_1_rmse: 0.70325 |  0:00:12s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.65914
Best weights from best epoch are automatically used!
ended training at: 22:59:46
Feature importance:
Mean squared error is of 3829054317.5899086
Mean absolute error:45404.79758163265
MAPE:0.4102871966137176
R2 score:0.5119921510571812
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:59:46
epoch 0  | loss: 3.87347 | val_0_rmse: 1.20328 | val_1_rmse: 1.23194 |  0:00:00s
epoch 1  | loss: 2.62451 | val_0_rmse: 1.04971 | val_1_rmse: 1.07703 |  0:00:00s
epoch 2  | loss: 1.65174 | val_0_rmse: 0.99876 | val_1_rmse: 1.02346 |  0:00:00s
epoch 3  | loss: 1.35412 | val_0_rmse: 1.01634 | val_1_rmse: 1.03923 |  0:00:00s
epoch 4  | loss: 1.2948  | val_0_rmse: 1.00335 | val_1_rmse: 1.02783 |  0:00:00s
epoch 5  | loss: 1.14385 | val_0_rmse: 0.99857 | val_1_rmse: 1.02411 |  0:00:00s
epoch 6  | loss: 1.09514 | val_0_rmse: 0.99422 | val_1_rmse: 1.01825 |  0:00:01s
epoch 7  | loss: 1.05696 | val_0_rmse: 0.98687 | val_1_rmse: 1.01208 |  0:00:01s
epoch 8  | loss: 0.992   | val_0_rmse: 0.98548 | val_1_rmse: 1.00871 |  0:00:01s
epoch 9  | loss: 0.93886 | val_0_rmse: 0.98067 | val_1_rmse: 1.00322 |  0:00:01s
epoch 10 | loss: 0.94033 | val_0_rmse: 0.97691 | val_1_rmse: 0.99936 |  0:00:01s
epoch 11 | loss: 0.91578 | val_0_rmse: 0.97693 | val_1_rmse: 1.00626 |  0:00:01s
epoch 12 | loss: 0.88412 | val_0_rmse: 0.97914 | val_1_rmse: 1.01623 |  0:00:02s
epoch 13 | loss: 0.87175 | val_0_rmse: 0.96443 | val_1_rmse: 0.99301 |  0:00:02s
epoch 14 | loss: 0.77177 | val_0_rmse: 0.93971 | val_1_rmse: 0.97256 |  0:00:02s
epoch 15 | loss: 0.78166 | val_0_rmse: 0.93458 | val_1_rmse: 0.96939 |  0:00:02s
epoch 16 | loss: 0.71346 | val_0_rmse: 0.90529 | val_1_rmse: 0.95096 |  0:00:02s
epoch 17 | loss: 0.68518 | val_0_rmse: 0.87757 | val_1_rmse: 0.94116 |  0:00:02s
epoch 18 | loss: 0.6832  | val_0_rmse: 0.90239 | val_1_rmse: 0.96423 |  0:00:03s
epoch 19 | loss: 0.67883 | val_0_rmse: 0.85239 | val_1_rmse: 0.92912 |  0:00:03s
epoch 20 | loss: 0.61595 | val_0_rmse: 0.81115 | val_1_rmse: 0.88771 |  0:00:03s
epoch 21 | loss: 0.60776 | val_0_rmse: 0.80162 | val_1_rmse: 0.86434 |  0:00:03s
epoch 22 | loss: 0.58712 | val_0_rmse: 0.82443 | val_1_rmse: 0.90356 |  0:00:03s
epoch 23 | loss: 0.58985 | val_0_rmse: 0.81071 | val_1_rmse: 0.89018 |  0:00:03s
epoch 24 | loss: 0.56553 | val_0_rmse: 0.81235 | val_1_rmse: 0.86618 |  0:00:04s
epoch 25 | loss: 0.56155 | val_0_rmse: 0.83654 | val_1_rmse: 0.88266 |  0:00:04s
epoch 26 | loss: 0.54966 | val_0_rmse: 0.86714 | val_1_rmse: 0.91917 |  0:00:04s
epoch 27 | loss: 0.5578  | val_0_rmse: 0.83171 | val_1_rmse: 0.89279 |  0:00:04s
epoch 28 | loss: 0.54461 | val_0_rmse: 0.76867 | val_1_rmse: 0.84353 |  0:00:04s
epoch 29 | loss: 0.53521 | val_0_rmse: 0.74719 | val_1_rmse: 0.82496 |  0:00:04s
epoch 30 | loss: 0.55258 | val_0_rmse: 0.73621 | val_1_rmse: 0.79311 |  0:00:05s
epoch 31 | loss: 0.50779 | val_0_rmse: 0.75075 | val_1_rmse: 0.80627 |  0:00:05s
epoch 32 | loss: 0.53123 | val_0_rmse: 0.69504 | val_1_rmse: 0.74869 |  0:00:05s
epoch 33 | loss: 0.49517 | val_0_rmse: 0.66713 | val_1_rmse: 0.71727 |  0:00:05s
epoch 34 | loss: 0.4686  | val_0_rmse: 0.66633 | val_1_rmse: 0.72655 |  0:00:05s
epoch 35 | loss: 0.47034 | val_0_rmse: 0.66935 | val_1_rmse: 0.72805 |  0:00:05s
epoch 36 | loss: 0.45592 | val_0_rmse: 0.66025 | val_1_rmse: 0.71525 |  0:00:06s
epoch 37 | loss: 0.44633 | val_0_rmse: 0.66355 | val_1_rmse: 0.71838 |  0:00:06s
epoch 38 | loss: 0.42231 | val_0_rmse: 0.67251 | val_1_rmse: 0.7258  |  0:00:06s
epoch 39 | loss: 0.4191  | val_0_rmse: 0.68909 | val_1_rmse: 0.74839 |  0:00:06s
epoch 40 | loss: 0.40238 | val_0_rmse: 0.70414 | val_1_rmse: 0.76646 |  0:00:06s
epoch 41 | loss: 0.388   | val_0_rmse: 0.71526 | val_1_rmse: 0.77449 |  0:00:06s
epoch 42 | loss: 0.37745 | val_0_rmse: 0.70855 | val_1_rmse: 0.76187 |  0:00:07s
epoch 43 | loss: 0.36351 | val_0_rmse: 0.70213 | val_1_rmse: 0.75196 |  0:00:07s
epoch 44 | loss: 0.36607 | val_0_rmse: 0.70121 | val_1_rmse: 0.75176 |  0:00:07s
epoch 45 | loss: 0.3613  | val_0_rmse: 0.70452 | val_1_rmse: 0.75974 |  0:00:07s
epoch 46 | loss: 0.35614 | val_0_rmse: 0.69882 | val_1_rmse: 0.75489 |  0:00:07s
epoch 47 | loss: 0.32938 | val_0_rmse: 0.69216 | val_1_rmse: 0.74514 |  0:00:07s
epoch 48 | loss: 0.32381 | val_0_rmse: 0.68951 | val_1_rmse: 0.74274 |  0:00:07s
epoch 49 | loss: 0.31295 | val_0_rmse: 0.69193 | val_1_rmse: 0.74855 |  0:00:08s
epoch 50 | loss: 0.33523 | val_0_rmse: 0.69422 | val_1_rmse: 0.75761 |  0:00:08s
epoch 51 | loss: 0.30254 | val_0_rmse: 0.67832 | val_1_rmse: 0.73893 |  0:00:08s
epoch 52 | loss: 0.30891 | val_0_rmse: 0.68055 | val_1_rmse: 0.73854 |  0:00:08s
epoch 53 | loss: 0.3169  | val_0_rmse: 0.70203 | val_1_rmse: 0.7635  |  0:00:08s
epoch 54 | loss: 0.30429 | val_0_rmse: 0.7059  | val_1_rmse: 0.76861 |  0:00:08s
epoch 55 | loss: 0.28345 | val_0_rmse: 0.68424 | val_1_rmse: 0.74008 |  0:00:09s
epoch 56 | loss: 0.29274 | val_0_rmse: 0.68548 | val_1_rmse: 0.74189 |  0:00:09s
epoch 57 | loss: 0.2846  | val_0_rmse: 0.68706 | val_1_rmse: 0.74215 |  0:00:09s
epoch 58 | loss: 0.29982 | val_0_rmse: 0.67938 | val_1_rmse: 0.72921 |  0:00:09s
epoch 59 | loss: 0.28666 | val_0_rmse: 0.68331 | val_1_rmse: 0.73835 |  0:00:09s
epoch 60 | loss: 0.27595 | val_0_rmse: 0.69236 | val_1_rmse: 0.75384 |  0:00:09s
epoch 61 | loss: 0.28005 | val_0_rmse: 0.68851 | val_1_rmse: 0.7479  |  0:00:10s
epoch 62 | loss: 0.27415 | val_0_rmse: 0.67661 | val_1_rmse: 0.72996 |  0:00:10s
epoch 63 | loss: 0.28906 | val_0_rmse: 0.66881 | val_1_rmse: 0.7182  |  0:00:10s
epoch 64 | loss: 0.28304 | val_0_rmse: 0.66996 | val_1_rmse: 0.73038 |  0:00:10s
epoch 65 | loss: 0.26914 | val_0_rmse: 0.6691  | val_1_rmse: 0.72803 |  0:00:10s
epoch 66 | loss: 0.26909 | val_0_rmse: 0.66491 | val_1_rmse: 0.71811 |  0:00:10s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.71525
Best weights from best epoch are automatically used!
ended training at: 22:59:57
Feature importance:
Mean squared error is of 3594659924.8516746
Mean absolute error:42543.18943523242
MAPE:0.38542688599882935
R2 score:0.48623798481775904
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:59:57
epoch 0  | loss: 5.37558 | val_0_rmse: 1.10193 | val_1_rmse: 1.09726 |  0:00:00s
epoch 1  | loss: 2.23303 | val_0_rmse: 1.00303 | val_1_rmse: 1.0216  |  0:00:00s
epoch 2  | loss: 1.59643 | val_0_rmse: 0.98175 | val_1_rmse: 1.01036 |  0:00:00s
epoch 3  | loss: 1.42737 | val_0_rmse: 0.9866  | val_1_rmse: 1.01747 |  0:00:00s
epoch 4  | loss: 1.17743 | val_0_rmse: 0.98523 | val_1_rmse: 1.01492 |  0:00:00s
epoch 5  | loss: 1.07324 | val_0_rmse: 0.98257 | val_1_rmse: 1.01145 |  0:00:00s
epoch 6  | loss: 1.02697 | val_0_rmse: 0.98283 | val_1_rmse: 1.00909 |  0:00:01s
epoch 7  | loss: 1.02806 | val_0_rmse: 0.98184 | val_1_rmse: 1.00856 |  0:00:01s
epoch 8  | loss: 0.99514 | val_0_rmse: 0.97762 | val_1_rmse: 1.00775 |  0:00:01s
epoch 9  | loss: 0.9702  | val_0_rmse: 0.97535 | val_1_rmse: 1.00764 |  0:00:01s
epoch 10 | loss: 0.92791 | val_0_rmse: 0.96594 | val_1_rmse: 0.9937  |  0:00:01s
epoch 11 | loss: 0.91306 | val_0_rmse: 0.95687 | val_1_rmse: 0.98416 |  0:00:01s
epoch 12 | loss: 0.91162 | val_0_rmse: 0.947   | val_1_rmse: 0.97329 |  0:00:02s
epoch 13 | loss: 0.84179 | val_0_rmse: 0.93212 | val_1_rmse: 0.95404 |  0:00:02s
epoch 14 | loss: 0.75963 | val_0_rmse: 0.91808 | val_1_rmse: 0.93796 |  0:00:02s
epoch 15 | loss: 0.72573 | val_0_rmse: 0.87732 | val_1_rmse: 0.90226 |  0:00:02s
epoch 16 | loss: 0.72236 | val_0_rmse: 0.90128 | val_1_rmse: 0.93361 |  0:00:02s
epoch 17 | loss: 0.64833 | val_0_rmse: 0.88371 | val_1_rmse: 0.91008 |  0:00:03s
epoch 18 | loss: 0.61819 | val_0_rmse: 0.85977 | val_1_rmse: 0.89038 |  0:00:03s
epoch 19 | loss: 0.60992 | val_0_rmse: 0.91525 | val_1_rmse: 0.95351 |  0:00:03s
epoch 20 | loss: 0.59539 | val_0_rmse: 0.85342 | val_1_rmse: 0.88061 |  0:00:03s
epoch 21 | loss: 0.54019 | val_0_rmse: 0.81451 | val_1_rmse: 0.83369 |  0:00:03s
epoch 22 | loss: 0.55207 | val_0_rmse: 0.82755 | val_1_rmse: 0.84467 |  0:00:03s
epoch 23 | loss: 0.52724 | val_0_rmse: 0.82676 | val_1_rmse: 0.84616 |  0:00:03s
epoch 24 | loss: 0.48038 | val_0_rmse: 0.78039 | val_1_rmse: 0.79796 |  0:00:04s
epoch 25 | loss: 0.47486 | val_0_rmse: 0.75261 | val_1_rmse: 0.76944 |  0:00:04s
epoch 26 | loss: 0.46255 | val_0_rmse: 0.75193 | val_1_rmse: 0.77108 |  0:00:04s
epoch 27 | loss: 0.44289 | val_0_rmse: 0.74318 | val_1_rmse: 0.76547 |  0:00:04s
epoch 28 | loss: 0.40132 | val_0_rmse: 0.70826 | val_1_rmse: 0.71932 |  0:00:04s
epoch 29 | loss: 0.39684 | val_0_rmse: 0.70436 | val_1_rmse: 0.71722 |  0:00:04s
epoch 30 | loss: 0.38292 | val_0_rmse: 0.71169 | val_1_rmse: 0.73589 |  0:00:05s
epoch 31 | loss: 0.35964 | val_0_rmse: 0.69071 | val_1_rmse: 0.70944 |  0:00:05s
epoch 32 | loss: 0.34352 | val_0_rmse: 0.68073 | val_1_rmse: 0.6933  |  0:00:05s
epoch 33 | loss: 0.33581 | val_0_rmse: 0.6802  | val_1_rmse: 0.69834 |  0:00:05s
epoch 34 | loss: 0.32258 | val_0_rmse: 0.67506 | val_1_rmse: 0.69627 |  0:00:05s
epoch 35 | loss: 0.32936 | val_0_rmse: 0.67461 | val_1_rmse: 0.69114 |  0:00:05s
epoch 36 | loss: 0.31464 | val_0_rmse: 0.67995 | val_1_rmse: 0.69782 |  0:00:06s
epoch 37 | loss: 0.29871 | val_0_rmse: 0.67955 | val_1_rmse: 0.70668 |  0:00:06s
epoch 38 | loss: 0.29355 | val_0_rmse: 0.703   | val_1_rmse: 0.73906 |  0:00:06s
epoch 39 | loss: 0.30012 | val_0_rmse: 0.68074 | val_1_rmse: 0.71037 |  0:00:06s
epoch 40 | loss: 0.28018 | val_0_rmse: 0.66535 | val_1_rmse: 0.68795 |  0:00:06s
epoch 41 | loss: 0.28576 | val_0_rmse: 0.68019 | val_1_rmse: 0.70592 |  0:00:06s
epoch 42 | loss: 0.27737 | val_0_rmse: 0.69205 | val_1_rmse: 0.72068 |  0:00:07s
epoch 43 | loss: 0.26591 | val_0_rmse: 0.689   | val_1_rmse: 0.71162 |  0:00:07s
epoch 44 | loss: 0.26766 | val_0_rmse: 0.67676 | val_1_rmse: 0.68974 |  0:00:07s
epoch 45 | loss: 0.26306 | val_0_rmse: 0.67913 | val_1_rmse: 0.6941  |  0:00:07s
epoch 46 | loss: 0.25488 | val_0_rmse: 0.6777  | val_1_rmse: 0.69911 |  0:00:07s
epoch 47 | loss: 0.25513 | val_0_rmse: 0.66711 | val_1_rmse: 0.68984 |  0:00:07s
epoch 48 | loss: 0.2458  | val_0_rmse: 0.66206 | val_1_rmse: 0.68454 |  0:00:08s
epoch 49 | loss: 0.24406 | val_0_rmse: 0.66476 | val_1_rmse: 0.68538 |  0:00:08s
epoch 50 | loss: 0.23735 | val_0_rmse: 0.66351 | val_1_rmse: 0.68838 |  0:00:08s
epoch 51 | loss: 0.23326 | val_0_rmse: 0.65906 | val_1_rmse: 0.6876  |  0:00:08s
epoch 52 | loss: 0.24264 | val_0_rmse: 0.659   | val_1_rmse: 0.6911  |  0:00:08s
epoch 53 | loss: 0.22061 | val_0_rmse: 0.65976 | val_1_rmse: 0.69666 |  0:00:08s
epoch 54 | loss: 0.23351 | val_0_rmse: 0.65231 | val_1_rmse: 0.68926 |  0:00:08s
epoch 55 | loss: 0.23658 | val_0_rmse: 0.65561 | val_1_rmse: 0.69249 |  0:00:09s
epoch 56 | loss: 0.22165 | val_0_rmse: 0.6634  | val_1_rmse: 0.70512 |  0:00:09s
epoch 57 | loss: 0.22644 | val_0_rmse: 0.66298 | val_1_rmse: 0.70562 |  0:00:09s
epoch 58 | loss: 0.22929 | val_0_rmse: 0.65512 | val_1_rmse: 0.69642 |  0:00:09s
epoch 59 | loss: 0.21702 | val_0_rmse: 0.65522 | val_1_rmse: 0.69696 |  0:00:09s
epoch 60 | loss: 0.22513 | val_0_rmse: 0.65949 | val_1_rmse: 0.70645 |  0:00:09s
epoch 61 | loss: 0.20853 | val_0_rmse: 0.68005 | val_1_rmse: 0.73886 |  0:00:10s
epoch 62 | loss: 0.21921 | val_0_rmse: 0.66298 | val_1_rmse: 0.7189  |  0:00:10s
epoch 63 | loss: 0.21169 | val_0_rmse: 0.64133 | val_1_rmse: 0.68901 |  0:00:10s
epoch 64 | loss: 0.22136 | val_0_rmse: 0.6395  | val_1_rmse: 0.68774 |  0:00:10s
epoch 65 | loss: 0.21089 | val_0_rmse: 0.64385 | val_1_rmse: 0.69633 |  0:00:10s
epoch 66 | loss: 0.20835 | val_0_rmse: 0.64729 | val_1_rmse: 0.70375 |  0:00:10s
epoch 67 | loss: 0.20846 | val_0_rmse: 0.63588 | val_1_rmse: 0.68565 |  0:00:11s
epoch 68 | loss: 0.21485 | val_0_rmse: 0.63598 | val_1_rmse: 0.68306 |  0:00:11s
epoch 69 | loss: 0.20536 | val_0_rmse: 0.65025 | val_1_rmse: 0.70683 |  0:00:11s
epoch 70 | loss: 0.20702 | val_0_rmse: 0.66419 | val_1_rmse: 0.72699 |  0:00:11s
epoch 71 | loss: 0.20541 | val_0_rmse: 0.64335 | val_1_rmse: 0.69719 |  0:00:11s
epoch 72 | loss: 0.20533 | val_0_rmse: 0.64321 | val_1_rmse: 0.69297 |  0:00:11s
epoch 73 | loss: 0.1945  | val_0_rmse: 0.66078 | val_1_rmse: 0.71686 |  0:00:12s
epoch 74 | loss: 0.19386 | val_0_rmse: 0.65472 | val_1_rmse: 0.70821 |  0:00:12s
epoch 75 | loss: 0.19621 | val_0_rmse: 0.64519 | val_1_rmse: 0.69408 |  0:00:12s
epoch 76 | loss: 0.19115 | val_0_rmse: 0.65    | val_1_rmse: 0.70316 |  0:00:12s
epoch 77 | loss: 0.17957 | val_0_rmse: 0.64915 | val_1_rmse: 0.70516 |  0:00:12s
epoch 78 | loss: 0.1903  | val_0_rmse: 0.64132 | val_1_rmse: 0.69145 |  0:00:12s
epoch 79 | loss: 0.19439 | val_0_rmse: 0.64315 | val_1_rmse: 0.69049 |  0:00:12s
epoch 80 | loss: 0.1778  | val_0_rmse: 0.65255 | val_1_rmse: 0.70945 |  0:00:13s
epoch 81 | loss: 0.17061 | val_0_rmse: 0.64329 | val_1_rmse: 0.69952 |  0:00:13s
epoch 82 | loss: 0.18838 | val_0_rmse: 0.63902 | val_1_rmse: 0.70003 |  0:00:13s
epoch 83 | loss: 0.18124 | val_0_rmse: 0.63986 | val_1_rmse: 0.7094  |  0:00:13s
epoch 84 | loss: 0.17513 | val_0_rmse: 0.63644 | val_1_rmse: 0.70668 |  0:00:13s
epoch 85 | loss: 0.18059 | val_0_rmse: 0.63064 | val_1_rmse: 0.69876 |  0:00:13s
epoch 86 | loss: 0.17435 | val_0_rmse: 0.63112 | val_1_rmse: 0.70613 |  0:00:14s
epoch 87 | loss: 0.17712 | val_0_rmse: 0.65112 | val_1_rmse: 0.73524 |  0:00:14s
epoch 88 | loss: 0.17343 | val_0_rmse: 0.64445 | val_1_rmse: 0.72264 |  0:00:14s
epoch 89 | loss: 0.17101 | val_0_rmse: 0.627   | val_1_rmse: 0.69031 |  0:00:14s
epoch 90 | loss: 0.16139 | val_0_rmse: 0.62717 | val_1_rmse: 0.68773 |  0:00:14s
epoch 91 | loss: 0.17486 | val_0_rmse: 0.64249 | val_1_rmse: 0.71046 |  0:00:14s
epoch 92 | loss: 0.17766 | val_0_rmse: 0.64516 | val_1_rmse: 0.71679 |  0:00:14s
epoch 93 | loss: 0.15029 | val_0_rmse: 0.62563 | val_1_rmse: 0.69264 |  0:00:15s
epoch 94 | loss: 0.15622 | val_0_rmse: 0.62554 | val_1_rmse: 0.69777 |  0:00:15s
epoch 95 | loss: 0.15573 | val_0_rmse: 0.64263 | val_1_rmse: 0.72631 |  0:00:15s
epoch 96 | loss: 0.15685 | val_0_rmse: 0.64707 | val_1_rmse: 0.73096 |  0:00:15s
epoch 97 | loss: 0.16038 | val_0_rmse: 0.62333 | val_1_rmse: 0.69296 |  0:00:15s
epoch 98 | loss: 0.15532 | val_0_rmse: 0.61657 | val_1_rmse: 0.6819  |  0:00:15s
epoch 99 | loss: 0.16478 | val_0_rmse: 0.62454 | val_1_rmse: 0.70314 |  0:00:16s
epoch 100| loss: 0.14412 | val_0_rmse: 0.64428 | val_1_rmse: 0.72987 |  0:00:16s
epoch 101| loss: 0.14963 | val_0_rmse: 0.62237 | val_1_rmse: 0.70017 |  0:00:16s
epoch 102| loss: 0.14146 | val_0_rmse: 0.62281 | val_1_rmse: 0.70106 |  0:00:16s
epoch 103| loss: 0.16785 | val_0_rmse: 0.63349 | val_1_rmse: 0.71741 |  0:00:16s
epoch 104| loss: 0.14873 | val_0_rmse: 0.63317 | val_1_rmse: 0.71541 |  0:00:16s
epoch 105| loss: 0.14853 | val_0_rmse: 0.63264 | val_1_rmse: 0.71132 |  0:00:17s
epoch 106| loss: 0.15181 | val_0_rmse: 0.63058 | val_1_rmse: 0.70763 |  0:00:17s
epoch 107| loss: 0.15052 | val_0_rmse: 0.63687 | val_1_rmse: 0.71581 |  0:00:17s
epoch 108| loss: 0.14363 | val_0_rmse: 0.62686 | val_1_rmse: 0.70446 |  0:00:17s
epoch 109| loss: 0.15054 | val_0_rmse: 0.62041 | val_1_rmse: 0.69706 |  0:00:17s
epoch 110| loss: 0.13401 | val_0_rmse: 0.63469 | val_1_rmse: 0.7226  |  0:00:17s
epoch 111| loss: 0.13944 | val_0_rmse: 0.62507 | val_1_rmse: 0.71616 |  0:00:17s
epoch 112| loss: 0.14687 | val_0_rmse: 0.60744 | val_1_rmse: 0.69381 |  0:00:18s
epoch 113| loss: 0.13909 | val_0_rmse: 0.60477 | val_1_rmse: 0.69702 |  0:00:18s
epoch 114| loss: 0.14844 | val_0_rmse: 0.60242 | val_1_rmse: 0.69435 |  0:00:18s
epoch 115| loss: 0.13929 | val_0_rmse: 0.59695 | val_1_rmse: 0.67541 |  0:00:18s
epoch 116| loss: 0.13696 | val_0_rmse: 0.60123 | val_1_rmse: 0.67913 |  0:00:18s
epoch 117| loss: 0.13085 | val_0_rmse: 0.61155 | val_1_rmse: 0.70099 |  0:00:18s
epoch 118| loss: 0.134   | val_0_rmse: 0.60987 | val_1_rmse: 0.70274 |  0:00:19s
epoch 119| loss: 0.14077 | val_0_rmse: 0.59396 | val_1_rmse: 0.6791  |  0:00:19s
epoch 120| loss: 0.13654 | val_0_rmse: 0.5935  | val_1_rmse: 0.67363 |  0:00:19s
epoch 121| loss: 0.14261 | val_0_rmse: 0.60174 | val_1_rmse: 0.69602 |  0:00:19s
epoch 122| loss: 0.13873 | val_0_rmse: 0.59553 | val_1_rmse: 0.70457 |  0:00:19s
epoch 123| loss: 0.13495 | val_0_rmse: 0.61989 | val_1_rmse: 0.73034 |  0:00:19s
epoch 124| loss: 0.135   | val_0_rmse: 0.642   | val_1_rmse: 0.74651 |  0:00:20s
epoch 125| loss: 0.13756 | val_0_rmse: 0.62101 | val_1_rmse: 0.717   |  0:00:20s
epoch 126| loss: 0.13847 | val_0_rmse: 0.60911 | val_1_rmse: 0.70092 |  0:00:20s
epoch 127| loss: 0.12537 | val_0_rmse: 0.59983 | val_1_rmse: 0.69231 |  0:00:20s
epoch 128| loss: 0.13577 | val_0_rmse: 0.59919 | val_1_rmse: 0.7     |  0:00:20s
epoch 129| loss: 0.12823 | val_0_rmse: 0.60617 | val_1_rmse: 0.71347 |  0:00:20s
epoch 130| loss: 0.11905 | val_0_rmse: 0.5895  | val_1_rmse: 0.69066 |  0:00:21s
epoch 131| loss: 0.12204 | val_0_rmse: 0.58732 | val_1_rmse: 0.68555 |  0:00:21s
epoch 132| loss: 0.13346 | val_0_rmse: 0.58313 | val_1_rmse: 0.68462 |  0:00:21s
epoch 133| loss: 0.13004 | val_0_rmse: 0.57876 | val_1_rmse: 0.69059 |  0:00:21s
epoch 134| loss: 0.12253 | val_0_rmse: 0.57482 | val_1_rmse: 0.6951  |  0:00:21s
epoch 135| loss: 0.12151 | val_0_rmse: 0.56522 | val_1_rmse: 0.67562 |  0:00:21s
epoch 136| loss: 0.14789 | val_0_rmse: 0.56247 | val_1_rmse: 0.66848 |  0:00:22s
epoch 137| loss: 0.12601 | val_0_rmse: 0.56185 | val_1_rmse: 0.67596 |  0:00:22s
epoch 138| loss: 0.11734 | val_0_rmse: 0.56947 | val_1_rmse: 0.6942  |  0:00:22s
epoch 139| loss: 0.12206 | val_0_rmse: 0.55255 | val_1_rmse: 0.67366 |  0:00:22s
epoch 140| loss: 0.11069 | val_0_rmse: 0.55159 | val_1_rmse: 0.65702 |  0:00:22s
epoch 141| loss: 0.12123 | val_0_rmse: 0.55198 | val_1_rmse: 0.66404 |  0:00:22s
epoch 142| loss: 0.12278 | val_0_rmse: 0.55199 | val_1_rmse: 0.68214 |  0:00:22s
epoch 143| loss: 0.11711 | val_0_rmse: 0.54042 | val_1_rmse: 0.66901 |  0:00:23s
epoch 144| loss: 0.11424 | val_0_rmse: 0.53368 | val_1_rmse: 0.65488 |  0:00:23s
epoch 145| loss: 0.12639 | val_0_rmse: 0.54226 | val_1_rmse: 0.68217 |  0:00:23s
epoch 146| loss: 0.11479 | val_0_rmse: 0.57057 | val_1_rmse: 0.72027 |  0:00:23s
epoch 147| loss: 0.11893 | val_0_rmse: 0.53636 | val_1_rmse: 0.67669 |  0:00:23s
epoch 148| loss: 0.11489 | val_0_rmse: 0.52726 | val_1_rmse: 0.67101 |  0:00:23s
epoch 149| loss: 0.11744 | val_0_rmse: 0.53621 | val_1_rmse: 0.6937  |  0:00:24s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.65488
Best weights from best epoch are automatically used!
ended training at: 23:00:21
Feature importance:
Mean squared error is of 3375111837.9791455
Mean absolute error:42250.85006802721
MAPE:0.39395385580090986
R2 score:0.5668235615517672
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:00:21
epoch 0  | loss: 4.17859 | val_0_rmse: 1.21113 | val_1_rmse: 1.16585 |  0:00:00s
epoch 1  | loss: 2.67661 | val_0_rmse: 1.20775 | val_1_rmse: 1.16269 |  0:00:00s
epoch 2  | loss: 1.83379 | val_0_rmse: 1.03755 | val_1_rmse: 1.00451 |  0:00:00s
epoch 3  | loss: 1.45423 | val_0_rmse: 0.99886 | val_1_rmse: 0.97755 |  0:00:00s
epoch 4  | loss: 1.29736 | val_0_rmse: 1.00048 | val_1_rmse: 0.98269 |  0:00:00s
epoch 5  | loss: 1.16767 | val_0_rmse: 0.998   | val_1_rmse: 0.98024 |  0:00:00s
epoch 6  | loss: 1.03234 | val_0_rmse: 0.99952 | val_1_rmse: 0.98501 |  0:00:01s
epoch 7  | loss: 1.09583 | val_0_rmse: 1.00683 | val_1_rmse: 0.99643 |  0:00:01s
epoch 8  | loss: 1.00393 | val_0_rmse: 1.00695 | val_1_rmse: 0.99501 |  0:00:01s
epoch 9  | loss: 1.00304 | val_0_rmse: 1.00407 | val_1_rmse: 0.99144 |  0:00:01s
epoch 10 | loss: 1.01269 | val_0_rmse: 0.99631 | val_1_rmse: 0.97922 |  0:00:01s
epoch 11 | loss: 0.94883 | val_0_rmse: 0.98948 | val_1_rmse: 0.96688 |  0:00:01s
epoch 12 | loss: 0.90558 | val_0_rmse: 0.97038 | val_1_rmse: 0.95346 |  0:00:02s
epoch 13 | loss: 0.85472 | val_0_rmse: 0.94579 | val_1_rmse: 0.93797 |  0:00:02s
epoch 14 | loss: 0.81748 | val_0_rmse: 0.90748 | val_1_rmse: 0.9182  |  0:00:02s
epoch 15 | loss: 0.73972 | val_0_rmse: 0.8694  | val_1_rmse: 0.87026 |  0:00:02s
epoch 16 | loss: 0.66364 | val_0_rmse: 0.85883 | val_1_rmse: 0.86841 |  0:00:02s
epoch 17 | loss: 0.61461 | val_0_rmse: 0.89256 | val_1_rmse: 0.89591 |  0:00:02s
epoch 18 | loss: 0.56575 | val_0_rmse: 0.97878 | val_1_rmse: 0.98486 |  0:00:03s
epoch 19 | loss: 0.54447 | val_0_rmse: 0.88955 | val_1_rmse: 0.8825  |  0:00:03s
epoch 20 | loss: 0.51043 | val_0_rmse: 0.80175 | val_1_rmse: 0.78655 |  0:00:03s
epoch 21 | loss: 0.49361 | val_0_rmse: 0.85242 | val_1_rmse: 0.83746 |  0:00:03s
epoch 22 | loss: 0.47366 | val_0_rmse: 0.81587 | val_1_rmse: 0.78086 |  0:00:03s
epoch 23 | loss: 0.46426 | val_0_rmse: 0.73959 | val_1_rmse: 0.70189 |  0:00:03s
epoch 24 | loss: 0.4476  | val_0_rmse: 0.77546 | val_1_rmse: 0.75007 |  0:00:04s
epoch 25 | loss: 0.41611 | val_0_rmse: 0.79324 | val_1_rmse: 0.777   |  0:00:04s
epoch 26 | loss: 0.39653 | val_0_rmse: 0.74586 | val_1_rmse: 0.7185  |  0:00:04s
epoch 27 | loss: 0.41315 | val_0_rmse: 0.74952 | val_1_rmse: 0.72814 |  0:00:04s
epoch 28 | loss: 0.4051  | val_0_rmse: 0.75552 | val_1_rmse: 0.74366 |  0:00:04s
epoch 29 | loss: 0.40072 | val_0_rmse: 0.73466 | val_1_rmse: 0.71721 |  0:00:04s
epoch 30 | loss: 0.34888 | val_0_rmse: 0.76292 | val_1_rmse: 0.75302 |  0:00:05s
epoch 31 | loss: 0.33581 | val_0_rmse: 0.76427 | val_1_rmse: 0.75827 |  0:00:05s
epoch 32 | loss: 0.33494 | val_0_rmse: 0.75182 | val_1_rmse: 0.74902 |  0:00:05s
epoch 33 | loss: 0.31565 | val_0_rmse: 0.78309 | val_1_rmse: 0.78142 |  0:00:05s
epoch 34 | loss: 0.3154  | val_0_rmse: 0.7842  | val_1_rmse: 0.77371 |  0:00:05s
epoch 35 | loss: 0.30432 | val_0_rmse: 0.72317 | val_1_rmse: 0.7084  |  0:00:05s
epoch 36 | loss: 0.29837 | val_0_rmse: 0.71258 | val_1_rmse: 0.69466 |  0:00:06s
epoch 37 | loss: 0.28519 | val_0_rmse: 0.72029 | val_1_rmse: 0.6963  |  0:00:06s
epoch 38 | loss: 0.27391 | val_0_rmse: 0.71272 | val_1_rmse: 0.68741 |  0:00:06s
epoch 39 | loss: 0.265   | val_0_rmse: 0.71431 | val_1_rmse: 0.68806 |  0:00:06s
epoch 40 | loss: 0.2557  | val_0_rmse: 0.72982 | val_1_rmse: 0.70469 |  0:00:06s
epoch 41 | loss: 0.26443 | val_0_rmse: 0.68797 | val_1_rmse: 0.66373 |  0:00:06s
epoch 42 | loss: 0.25232 | val_0_rmse: 0.67018 | val_1_rmse: 0.64767 |  0:00:06s
epoch 43 | loss: 0.24982 | val_0_rmse: 0.66863 | val_1_rmse: 0.65061 |  0:00:07s
epoch 44 | loss: 0.25828 | val_0_rmse: 0.65541 | val_1_rmse: 0.64599 |  0:00:07s
epoch 45 | loss: 0.23523 | val_0_rmse: 0.64727 | val_1_rmse: 0.64588 |  0:00:07s
epoch 46 | loss: 0.24821 | val_0_rmse: 0.64477 | val_1_rmse: 0.64409 |  0:00:07s
epoch 47 | loss: 0.23791 | val_0_rmse: 0.64772 | val_1_rmse: 0.6471  |  0:00:07s
epoch 48 | loss: 0.25071 | val_0_rmse: 0.65294 | val_1_rmse: 0.65085 |  0:00:07s
epoch 49 | loss: 0.22682 | val_0_rmse: 0.65827 | val_1_rmse: 0.66405 |  0:00:08s
epoch 50 | loss: 0.23359 | val_0_rmse: 0.67018 | val_1_rmse: 0.68222 |  0:00:08s
epoch 51 | loss: 0.2361  | val_0_rmse: 0.68921 | val_1_rmse: 0.70224 |  0:00:08s
epoch 52 | loss: 0.23485 | val_0_rmse: 0.66595 | val_1_rmse: 0.67801 |  0:00:08s
epoch 53 | loss: 0.22429 | val_0_rmse: 0.65749 | val_1_rmse: 0.66898 |  0:00:08s
epoch 54 | loss: 0.21551 | val_0_rmse: 0.69194 | val_1_rmse: 0.69844 |  0:00:08s
epoch 55 | loss: 0.21859 | val_0_rmse: 0.73387 | val_1_rmse: 0.72688 |  0:00:09s
epoch 56 | loss: 0.22524 | val_0_rmse: 0.68294 | val_1_rmse: 0.67806 |  0:00:09s
epoch 57 | loss: 0.23311 | val_0_rmse: 0.66297 | val_1_rmse: 0.6613  |  0:00:09s
epoch 58 | loss: 0.22615 | val_0_rmse: 0.68154 | val_1_rmse: 0.6794  |  0:00:09s
epoch 59 | loss: 0.23225 | val_0_rmse: 0.67029 | val_1_rmse: 0.67254 |  0:00:09s
epoch 60 | loss: 0.22117 | val_0_rmse: 0.64985 | val_1_rmse: 0.65771 |  0:00:09s
epoch 61 | loss: 0.21209 | val_0_rmse: 0.65808 | val_1_rmse: 0.67108 |  0:00:10s
epoch 62 | loss: 0.21407 | val_0_rmse: 0.66824 | val_1_rmse: 0.67861 |  0:00:10s
epoch 63 | loss: 0.22073 | val_0_rmse: 0.65793 | val_1_rmse: 0.66648 |  0:00:10s
epoch 64 | loss: 0.21727 | val_0_rmse: 0.65227 | val_1_rmse: 0.66308 |  0:00:10s
epoch 65 | loss: 0.1993  | val_0_rmse: 0.66976 | val_1_rmse: 0.67955 |  0:00:10s
epoch 66 | loss: 0.22358 | val_0_rmse: 0.65986 | val_1_rmse: 0.67226 |  0:00:10s
epoch 67 | loss: 0.21053 | val_0_rmse: 0.64703 | val_1_rmse: 0.66151 |  0:00:10s
epoch 68 | loss: 0.19753 | val_0_rmse: 0.64653 | val_1_rmse: 0.66258 |  0:00:11s
epoch 69 | loss: 0.2106  | val_0_rmse: 0.6422  | val_1_rmse: 0.65894 |  0:00:11s
epoch 70 | loss: 0.21316 | val_0_rmse: 0.63482 | val_1_rmse: 0.65179 |  0:00:11s
epoch 71 | loss: 0.2048  | val_0_rmse: 0.63046 | val_1_rmse: 0.64865 |  0:00:11s
epoch 72 | loss: 0.20565 | val_0_rmse: 0.63563 | val_1_rmse: 0.65524 |  0:00:11s
epoch 73 | loss: 0.20083 | val_0_rmse: 0.64489 | val_1_rmse: 0.66177 |  0:00:11s
epoch 74 | loss: 0.20525 | val_0_rmse: 0.64063 | val_1_rmse: 0.66021 |  0:00:12s
epoch 75 | loss: 0.21289 | val_0_rmse: 0.64082 | val_1_rmse: 0.6556  |  0:00:12s
epoch 76 | loss: 0.20319 | val_0_rmse: 0.66205 | val_1_rmse: 0.67347 |  0:00:12s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.64409
Best weights from best epoch are automatically used!
ended training at: 23:00:34
Feature importance:
Mean squared error is of 3336518736.250566
Mean absolute error:42256.644047052156
MAPE:0.41676749267960034
R2 score:0.5548205804649523
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:00:34
epoch 0  | loss: 4.30072 | val_0_rmse: 1.0668  | val_1_rmse: 1.07501 |  0:00:00s
epoch 1  | loss: 2.34527 | val_0_rmse: 1.0155  | val_1_rmse: 1.02994 |  0:00:00s
epoch 2  | loss: 1.65825 | val_0_rmse: 1.00691 | val_1_rmse: 1.025   |  0:00:00s
epoch 3  | loss: 1.70208 | val_0_rmse: 1.00542 | val_1_rmse: 1.02274 |  0:00:00s
epoch 4  | loss: 1.42771 | val_0_rmse: 1.00569 | val_1_rmse: 1.02402 |  0:00:00s
epoch 5  | loss: 1.17981 | val_0_rmse: 1.00782 | val_1_rmse: 1.02521 |  0:00:00s
epoch 6  | loss: 1.09392 | val_0_rmse: 1.00613 | val_1_rmse: 1.02301 |  0:00:01s
epoch 7  | loss: 1.06264 | val_0_rmse: 1.00539 | val_1_rmse: 1.0208  |  0:00:01s
epoch 8  | loss: 1.04616 | val_0_rmse: 1.00481 | val_1_rmse: 1.01946 |  0:00:01s
epoch 9  | loss: 1.0005  | val_0_rmse: 1.00592 | val_1_rmse: 1.02052 |  0:00:01s
epoch 10 | loss: 1.00601 | val_0_rmse: 1.00428 | val_1_rmse: 1.01675 |  0:00:01s
epoch 11 | loss: 1.00006 | val_0_rmse: 1.00593 | val_1_rmse: 1.02071 |  0:00:01s
epoch 12 | loss: 0.97794 | val_0_rmse: 1.00085 | val_1_rmse: 1.01588 |  0:00:02s
epoch 13 | loss: 0.91562 | val_0_rmse: 0.98297 | val_1_rmse: 0.99427 |  0:00:02s
epoch 14 | loss: 0.9145  | val_0_rmse: 0.96953 | val_1_rmse: 0.9805  |  0:00:02s
epoch 15 | loss: 0.8951  | val_0_rmse: 0.99077 | val_1_rmse: 1.00517 |  0:00:02s
epoch 16 | loss: 0.89572 | val_0_rmse: 0.96462 | val_1_rmse: 0.98016 |  0:00:02s
epoch 17 | loss: 0.86882 | val_0_rmse: 0.96405 | val_1_rmse: 0.9808  |  0:00:02s
epoch 18 | loss: 0.88122 | val_0_rmse: 0.96466 | val_1_rmse: 0.98189 |  0:00:03s
epoch 19 | loss: 0.84989 | val_0_rmse: 0.9673  | val_1_rmse: 0.98143 |  0:00:03s
epoch 20 | loss: 0.82412 | val_0_rmse: 0.9669  | val_1_rmse: 0.98349 |  0:00:03s
epoch 21 | loss: 0.8254  | val_0_rmse: 0.93998 | val_1_rmse: 0.96241 |  0:00:03s
epoch 22 | loss: 0.82018 | val_0_rmse: 0.92104 | val_1_rmse: 0.94923 |  0:00:03s
epoch 23 | loss: 0.77168 | val_0_rmse: 0.91311 | val_1_rmse: 0.94128 |  0:00:03s
epoch 24 | loss: 0.73089 | val_0_rmse: 0.8649  | val_1_rmse: 0.8938  |  0:00:04s
epoch 25 | loss: 0.67194 | val_0_rmse: 0.82645 | val_1_rmse: 0.85961 |  0:00:04s
epoch 26 | loss: 0.65985 | val_0_rmse: 0.81038 | val_1_rmse: 0.84191 |  0:00:04s
epoch 27 | loss: 0.62106 | val_0_rmse: 0.79194 | val_1_rmse: 0.82297 |  0:00:04s
epoch 28 | loss: 0.61611 | val_0_rmse: 0.79089 | val_1_rmse: 0.82676 |  0:00:04s
epoch 29 | loss: 0.60039 | val_0_rmse: 0.78697 | val_1_rmse: 0.82639 |  0:00:04s
epoch 30 | loss: 0.55193 | val_0_rmse: 0.78282 | val_1_rmse: 0.82073 |  0:00:05s
epoch 31 | loss: 0.53881 | val_0_rmse: 0.77485 | val_1_rmse: 0.81789 |  0:00:05s
epoch 32 | loss: 0.50187 | val_0_rmse: 0.77098 | val_1_rmse: 0.80851 |  0:00:05s
epoch 33 | loss: 0.47948 | val_0_rmse: 0.77423 | val_1_rmse: 0.80059 |  0:00:05s
epoch 34 | loss: 0.46041 | val_0_rmse: 0.77341 | val_1_rmse: 0.79972 |  0:00:05s
epoch 35 | loss: 0.46099 | val_0_rmse: 0.76156 | val_1_rmse: 0.78378 |  0:00:05s
epoch 36 | loss: 0.43151 | val_0_rmse: 0.7738  | val_1_rmse: 0.78856 |  0:00:06s
epoch 37 | loss: 0.40185 | val_0_rmse: 0.77356 | val_1_rmse: 0.78212 |  0:00:06s
epoch 38 | loss: 0.37867 | val_0_rmse: 0.72411 | val_1_rmse: 0.74239 |  0:00:06s
epoch 39 | loss: 0.37552 | val_0_rmse: 0.71533 | val_1_rmse: 0.72903 |  0:00:06s
epoch 40 | loss: 0.34128 | val_0_rmse: 0.72301 | val_1_rmse: 0.72711 |  0:00:06s
epoch 41 | loss: 0.33314 | val_0_rmse: 0.70973 | val_1_rmse: 0.71717 |  0:00:06s
epoch 42 | loss: 0.32546 | val_0_rmse: 0.6998  | val_1_rmse: 0.70651 |  0:00:07s
epoch 43 | loss: 0.30268 | val_0_rmse: 0.70315 | val_1_rmse: 0.70912 |  0:00:07s
epoch 44 | loss: 0.30713 | val_0_rmse: 0.69849 | val_1_rmse: 0.70739 |  0:00:07s
epoch 45 | loss: 0.27136 | val_0_rmse: 0.69541 | val_1_rmse: 0.70853 |  0:00:07s
epoch 46 | loss: 0.27222 | val_0_rmse: 0.68525 | val_1_rmse: 0.69921 |  0:00:07s
epoch 47 | loss: 0.25433 | val_0_rmse: 0.6788  | val_1_rmse: 0.69516 |  0:00:07s
epoch 48 | loss: 0.2676  | val_0_rmse: 0.67432 | val_1_rmse: 0.69325 |  0:00:08s
epoch 49 | loss: 0.22915 | val_0_rmse: 0.67263 | val_1_rmse: 0.69096 |  0:00:08s
epoch 50 | loss: 0.23883 | val_0_rmse: 0.6682  | val_1_rmse: 0.68859 |  0:00:08s
epoch 51 | loss: 0.23939 | val_0_rmse: 0.66363 | val_1_rmse: 0.683   |  0:00:08s
epoch 52 | loss: 0.25088 | val_0_rmse: 0.66539 | val_1_rmse: 0.68183 |  0:00:08s
epoch 53 | loss: 0.24094 | val_0_rmse: 0.67048 | val_1_rmse: 0.6873  |  0:00:08s
epoch 54 | loss: 0.23086 | val_0_rmse: 0.66597 | val_1_rmse: 0.68571 |  0:00:09s
epoch 55 | loss: 0.22089 | val_0_rmse: 0.66015 | val_1_rmse: 0.68129 |  0:00:09s
epoch 56 | loss: 0.20848 | val_0_rmse: 0.65185 | val_1_rmse: 0.67562 |  0:00:09s
epoch 57 | loss: 0.21007 | val_0_rmse: 0.64635 | val_1_rmse: 0.67414 |  0:00:09s
epoch 58 | loss: 0.21925 | val_0_rmse: 0.64316 | val_1_rmse: 0.6706  |  0:00:09s
epoch 59 | loss: 0.20164 | val_0_rmse: 0.64073 | val_1_rmse: 0.66581 |  0:00:09s
epoch 60 | loss: 0.2224  | val_0_rmse: 0.65643 | val_1_rmse: 0.68391 |  0:00:10s
epoch 61 | loss: 0.21809 | val_0_rmse: 0.68522 | val_1_rmse: 0.71419 |  0:00:10s
epoch 62 | loss: 0.21991 | val_0_rmse: 0.6455  | val_1_rmse: 0.67037 |  0:00:10s
epoch 63 | loss: 0.21072 | val_0_rmse: 0.66631 | val_1_rmse: 0.68204 |  0:00:10s
epoch 64 | loss: 0.2158  | val_0_rmse: 0.67703 | val_1_rmse: 0.69448 |  0:00:10s
epoch 65 | loss: 0.20918 | val_0_rmse: 0.64973 | val_1_rmse: 0.66948 |  0:00:10s
epoch 66 | loss: 0.1994  | val_0_rmse: 0.64152 | val_1_rmse: 0.66377 |  0:00:10s
epoch 67 | loss: 0.19721 | val_0_rmse: 0.63771 | val_1_rmse: 0.66091 |  0:00:11s
epoch 68 | loss: 0.2089  | val_0_rmse: 0.63356 | val_1_rmse: 0.65602 |  0:00:11s
epoch 69 | loss: 0.19854 | val_0_rmse: 0.63393 | val_1_rmse: 0.65831 |  0:00:11s
epoch 70 | loss: 0.19762 | val_0_rmse: 0.62833 | val_1_rmse: 0.65394 |  0:00:11s
epoch 71 | loss: 0.19333 | val_0_rmse: 0.62685 | val_1_rmse: 0.65347 |  0:00:11s
epoch 72 | loss: 0.19906 | val_0_rmse: 0.62826 | val_1_rmse: 0.65785 |  0:00:11s
epoch 73 | loss: 0.19409 | val_0_rmse: 0.62995 | val_1_rmse: 0.65975 |  0:00:12s
epoch 74 | loss: 0.18492 | val_0_rmse: 0.63354 | val_1_rmse: 0.66182 |  0:00:12s
epoch 75 | loss: 0.19042 | val_0_rmse: 0.63261 | val_1_rmse: 0.66085 |  0:00:12s
epoch 76 | loss: 0.18474 | val_0_rmse: 0.63381 | val_1_rmse: 0.6617  |  0:00:12s
epoch 77 | loss: 0.18822 | val_0_rmse: 0.63967 | val_1_rmse: 0.66837 |  0:00:12s
epoch 78 | loss: 0.17929 | val_0_rmse: 0.63241 | val_1_rmse: 0.66247 |  0:00:12s
epoch 79 | loss: 0.16628 | val_0_rmse: 0.62218 | val_1_rmse: 0.65611 |  0:00:13s
epoch 80 | loss: 0.17484 | val_0_rmse: 0.62336 | val_1_rmse: 0.66214 |  0:00:13s
epoch 81 | loss: 0.17549 | val_0_rmse: 0.62457 | val_1_rmse: 0.66628 |  0:00:13s
epoch 82 | loss: 0.16897 | val_0_rmse: 0.62322 | val_1_rmse: 0.66477 |  0:00:13s
epoch 83 | loss: 0.17148 | val_0_rmse: 0.62965 | val_1_rmse: 0.66981 |  0:00:13s
epoch 84 | loss: 0.16591 | val_0_rmse: 0.63532 | val_1_rmse: 0.67212 |  0:00:13s
epoch 85 | loss: 0.1711  | val_0_rmse: 0.64904 | val_1_rmse: 0.68406 |  0:00:14s
epoch 86 | loss: 0.17416 | val_0_rmse: 0.64161 | val_1_rmse: 0.67882 |  0:00:14s
epoch 87 | loss: 0.18436 | val_0_rmse: 0.63363 | val_1_rmse: 0.67041 |  0:00:14s
epoch 88 | loss: 0.1803  | val_0_rmse: 0.62217 | val_1_rmse: 0.65701 |  0:00:14s
epoch 89 | loss: 0.16944 | val_0_rmse: 0.62825 | val_1_rmse: 0.66119 |  0:00:14s
epoch 90 | loss: 0.17451 | val_0_rmse: 0.61434 | val_1_rmse: 0.65223 |  0:00:14s
epoch 91 | loss: 0.17345 | val_0_rmse: 0.60731 | val_1_rmse: 0.652   |  0:00:15s
epoch 92 | loss: 0.18427 | val_0_rmse: 0.61536 | val_1_rmse: 0.66177 |  0:00:15s
epoch 93 | loss: 0.18123 | val_0_rmse: 0.6174  | val_1_rmse: 0.66049 |  0:00:15s
epoch 94 | loss: 0.18646 | val_0_rmse: 0.606   | val_1_rmse: 0.65239 |  0:00:15s
epoch 95 | loss: 0.16122 | val_0_rmse: 0.59675 | val_1_rmse: 0.65151 |  0:00:15s
epoch 96 | loss: 0.16301 | val_0_rmse: 0.60287 | val_1_rmse: 0.66332 |  0:00:15s
epoch 97 | loss: 0.16171 | val_0_rmse: 0.60406 | val_1_rmse: 0.66156 |  0:00:15s
epoch 98 | loss: 0.16216 | val_0_rmse: 0.5887  | val_1_rmse: 0.64202 |  0:00:16s
epoch 99 | loss: 0.15249 | val_0_rmse: 0.5902  | val_1_rmse: 0.63938 |  0:00:16s
epoch 100| loss: 0.1588  | val_0_rmse: 0.59852 | val_1_rmse: 0.65067 |  0:00:16s
epoch 101| loss: 0.15586 | val_0_rmse: 0.61289 | val_1_rmse: 0.66773 |  0:00:16s
epoch 102| loss: 0.15355 | val_0_rmse: 0.59519 | val_1_rmse: 0.65227 |  0:00:16s
epoch 103| loss: 0.16286 | val_0_rmse: 0.59027 | val_1_rmse: 0.64639 |  0:00:16s
epoch 104| loss: 0.15818 | val_0_rmse: 0.59372 | val_1_rmse: 0.64627 |  0:00:17s
epoch 105| loss: 0.14239 | val_0_rmse: 0.59792 | val_1_rmse: 0.64634 |  0:00:17s
epoch 106| loss: 0.14641 | val_0_rmse: 0.58718 | val_1_rmse: 0.63708 |  0:00:17s
epoch 107| loss: 0.15155 | val_0_rmse: 0.5907  | val_1_rmse: 0.6459  |  0:00:17s
epoch 108| loss: 0.16372 | val_0_rmse: 0.59969 | val_1_rmse: 0.65832 |  0:00:17s
epoch 109| loss: 0.16297 | val_0_rmse: 0.58326 | val_1_rmse: 0.64336 |  0:00:17s
epoch 110| loss: 0.15341 | val_0_rmse: 0.57533 | val_1_rmse: 0.63197 |  0:00:18s
epoch 111| loss: 0.14819 | val_0_rmse: 0.57573 | val_1_rmse: 0.63408 |  0:00:18s
epoch 112| loss: 0.1448  | val_0_rmse: 0.58875 | val_1_rmse: 0.64628 |  0:00:18s
epoch 113| loss: 0.15878 | val_0_rmse: 0.58558 | val_1_rmse: 0.63667 |  0:00:18s
epoch 114| loss: 0.1497  | val_0_rmse: 0.57928 | val_1_rmse: 0.62675 |  0:00:18s
epoch 115| loss: 0.14013 | val_0_rmse: 0.58242 | val_1_rmse: 0.63136 |  0:00:18s
epoch 116| loss: 0.14223 | val_0_rmse: 0.57709 | val_1_rmse: 0.63179 |  0:00:19s
epoch 117| loss: 0.14621 | val_0_rmse: 0.56436 | val_1_rmse: 0.62557 |  0:00:19s
epoch 118| loss: 0.14652 | val_0_rmse: 0.5652  | val_1_rmse: 0.6338  |  0:00:19s
epoch 119| loss: 0.13476 | val_0_rmse: 0.56265 | val_1_rmse: 0.63502 |  0:00:19s
epoch 120| loss: 0.13748 | val_0_rmse: 0.56652 | val_1_rmse: 0.63894 |  0:00:19s
epoch 121| loss: 0.14467 | val_0_rmse: 0.57012 | val_1_rmse: 0.64073 |  0:00:19s
epoch 122| loss: 0.12877 | val_0_rmse: 0.56784 | val_1_rmse: 0.63932 |  0:00:19s
epoch 123| loss: 0.13628 | val_0_rmse: 0.56781 | val_1_rmse: 0.63977 |  0:00:20s
epoch 124| loss: 0.13533 | val_0_rmse: 0.57179 | val_1_rmse: 0.64586 |  0:00:20s
epoch 125| loss: 0.1337  | val_0_rmse: 0.56065 | val_1_rmse: 0.641   |  0:00:20s
epoch 126| loss: 0.13225 | val_0_rmse: 0.5664  | val_1_rmse: 0.6501  |  0:00:20s
epoch 127| loss: 0.1314  | val_0_rmse: 0.55671 | val_1_rmse: 0.64187 |  0:00:20s
epoch 128| loss: 0.1338  | val_0_rmse: 0.54718 | val_1_rmse: 0.63118 |  0:00:20s
epoch 129| loss: 0.13062 | val_0_rmse: 0.54715 | val_1_rmse: 0.6349  |  0:00:21s
epoch 130| loss: 0.12735 | val_0_rmse: 0.55559 | val_1_rmse: 0.64565 |  0:00:21s
epoch 131| loss: 0.13182 | val_0_rmse: 0.55475 | val_1_rmse: 0.64372 |  0:00:21s
epoch 132| loss: 0.12794 | val_0_rmse: 0.55853 | val_1_rmse: 0.6428  |  0:00:21s
epoch 133| loss: 0.12942 | val_0_rmse: 0.567   | val_1_rmse: 0.6481  |  0:00:21s
epoch 134| loss: 0.13029 | val_0_rmse: 0.54606 | val_1_rmse: 0.63726 |  0:00:21s
epoch 135| loss: 0.11946 | val_0_rmse: 0.56681 | val_1_rmse: 0.65996 |  0:00:22s
epoch 136| loss: 0.12683 | val_0_rmse: 0.54069 | val_1_rmse: 0.63808 |  0:00:22s
epoch 137| loss: 0.13247 | val_0_rmse: 0.55235 | val_1_rmse: 0.64585 |  0:00:22s
epoch 138| loss: 0.12756 | val_0_rmse: 0.53012 | val_1_rmse: 0.62348 |  0:00:22s
epoch 139| loss: 0.12546 | val_0_rmse: 0.53912 | val_1_rmse: 0.63377 |  0:00:22s
epoch 140| loss: 0.12464 | val_0_rmse: 0.52034 | val_1_rmse: 0.62126 |  0:00:22s
epoch 141| loss: 0.12083 | val_0_rmse: 0.53541 | val_1_rmse: 0.63905 |  0:00:23s
epoch 142| loss: 0.1185  | val_0_rmse: 0.53874 | val_1_rmse: 0.64187 |  0:00:23s
epoch 143| loss: 0.13262 | val_0_rmse: 0.53232 | val_1_rmse: 0.63288 |  0:00:23s
epoch 144| loss: 0.12043 | val_0_rmse: 0.52953 | val_1_rmse: 0.62753 |  0:00:23s
epoch 145| loss: 0.11588 | val_0_rmse: 0.52625 | val_1_rmse: 0.63063 |  0:00:23s
epoch 146| loss: 0.11714 | val_0_rmse: 0.53898 | val_1_rmse: 0.6438  |  0:00:23s
epoch 147| loss: 0.11109 | val_0_rmse: 0.5206  | val_1_rmse: 0.6278  |  0:00:23s
epoch 148| loss: 0.11118 | val_0_rmse: 0.51735 | val_1_rmse: 0.62317 |  0:00:24s
epoch 149| loss: 0.11741 | val_0_rmse: 0.51755 | val_1_rmse: 0.62922 |  0:00:24s
Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_val_1_rmse = 0.62126
Best weights from best epoch are automatically used!
ended training at: 23:00:58
Feature importance:
Mean squared error is of 2484915879.9691586
Mean absolute error:35633.91650807823
MAPE:0.31755661216680203
R2 score:0.6280506145217919
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:00:58
epoch 0  | loss: 3.06403 | val_0_rmse: 0.99589 | val_1_rmse: 0.99825 |  0:00:00s
epoch 1  | loss: 1.49059 | val_0_rmse: 0.99786 | val_1_rmse: 0.99692 |  0:00:00s
epoch 2  | loss: 1.20824 | val_0_rmse: 0.99551 | val_1_rmse: 0.99401 |  0:00:01s
epoch 3  | loss: 0.88677 | val_0_rmse: 0.87471 | val_1_rmse: 0.89968 |  0:00:01s
epoch 4  | loss: 0.73759 | val_0_rmse: 0.82505 | val_1_rmse: 0.85709 |  0:00:01s
epoch 5  | loss: 0.66201 | val_0_rmse: 0.93922 | val_1_rmse: 1.01582 |  0:00:02s
epoch 6  | loss: 0.61564 | val_0_rmse: 0.84202 | val_1_rmse: 0.89162 |  0:00:02s
epoch 7  | loss: 0.57651 | val_0_rmse: 0.81634 | val_1_rmse: 0.87106 |  0:00:02s
epoch 8  | loss: 0.51641 | val_0_rmse: 0.78244 | val_1_rmse: 0.82954 |  0:00:03s
epoch 9  | loss: 0.49951 | val_0_rmse: 0.97068 | val_1_rmse: 0.9999  |  0:00:03s
epoch 10 | loss: 0.47639 | val_0_rmse: 0.72178 | val_1_rmse: 0.74346 |  0:00:03s
epoch 11 | loss: 0.48993 | val_0_rmse: 0.70629 | val_1_rmse: 0.7447  |  0:00:04s
epoch 12 | loss: 0.47496 | val_0_rmse: 0.72656 | val_1_rmse: 0.75767 |  0:00:04s
epoch 13 | loss: 0.45    | val_0_rmse: 0.69555 | val_1_rmse: 0.73994 |  0:00:04s
epoch 14 | loss: 0.44229 | val_0_rmse: 0.69792 | val_1_rmse: 0.73781 |  0:00:05s
epoch 15 | loss: 0.44922 | val_0_rmse: 0.68787 | val_1_rmse: 0.72888 |  0:00:05s
epoch 16 | loss: 0.44028 | val_0_rmse: 0.72147 | val_1_rmse: 0.7697  |  0:00:06s
epoch 17 | loss: 0.42214 | val_0_rmse: 0.70718 | val_1_rmse: 0.74125 |  0:00:06s
epoch 18 | loss: 0.45283 | val_0_rmse: 0.77749 | val_1_rmse: 0.84856 |  0:00:06s
epoch 19 | loss: 0.45297 | val_0_rmse: 0.69838 | val_1_rmse: 0.73927 |  0:00:07s
epoch 20 | loss: 0.44994 | val_0_rmse: 0.75298 | val_1_rmse: 0.81188 |  0:00:07s
epoch 21 | loss: 0.45885 | val_0_rmse: 0.69277 | val_1_rmse: 0.75331 |  0:00:07s
epoch 22 | loss: 0.45609 | val_0_rmse: 0.69372 | val_1_rmse: 0.74711 |  0:00:08s
epoch 23 | loss: 0.44118 | val_0_rmse: 0.66516 | val_1_rmse: 0.70834 |  0:00:08s
epoch 24 | loss: 0.43961 | val_0_rmse: 0.69103 | val_1_rmse: 0.74812 |  0:00:08s
epoch 25 | loss: 0.44854 | val_0_rmse: 0.70095 | val_1_rmse: 0.7559  |  0:00:09s
epoch 26 | loss: 0.43843 | val_0_rmse: 0.7411  | val_1_rmse: 0.8034  |  0:00:09s
epoch 27 | loss: 0.43617 | val_0_rmse: 0.70102 | val_1_rmse: 0.76224 |  0:00:09s
epoch 28 | loss: 0.43226 | val_0_rmse: 0.70259 | val_1_rmse: 0.7558  |  0:00:10s
epoch 29 | loss: 0.43272 | val_0_rmse: 0.70982 | val_1_rmse: 0.7664  |  0:00:10s
epoch 30 | loss: 0.42269 | val_0_rmse: 0.69611 | val_1_rmse: 0.75501 |  0:00:10s
epoch 31 | loss: 0.42376 | val_0_rmse: 0.69669 | val_1_rmse: 0.75266 |  0:00:11s
epoch 32 | loss: 0.42812 | val_0_rmse: 0.67916 | val_1_rmse: 0.73169 |  0:00:11s
epoch 33 | loss: 0.42275 | val_0_rmse: 0.6857  | val_1_rmse: 0.73726 |  0:00:11s
epoch 34 | loss: 0.42926 | val_0_rmse: 0.66376 | val_1_rmse: 0.70098 |  0:00:12s
epoch 35 | loss: 0.42663 | val_0_rmse: 0.68571 | val_1_rmse: 0.73344 |  0:00:12s
epoch 36 | loss: 0.41655 | val_0_rmse: 0.6916  | val_1_rmse: 0.74438 |  0:00:12s
epoch 37 | loss: 0.41045 | val_0_rmse: 0.69012 | val_1_rmse: 0.74076 |  0:00:13s
epoch 38 | loss: 0.40366 | val_0_rmse: 0.69049 | val_1_rmse: 0.74957 |  0:00:13s
epoch 39 | loss: 0.4196  | val_0_rmse: 0.70121 | val_1_rmse: 0.76509 |  0:00:14s
epoch 40 | loss: 0.40948 | val_0_rmse: 0.68057 | val_1_rmse: 0.73614 |  0:00:14s
epoch 41 | loss: 0.40272 | val_0_rmse: 0.67964 | val_1_rmse: 0.72602 |  0:00:14s
epoch 42 | loss: 0.40834 | val_0_rmse: 0.67893 | val_1_rmse: 0.72867 |  0:00:15s
epoch 43 | loss: 0.40765 | val_0_rmse: 0.67211 | val_1_rmse: 0.70898 |  0:00:15s
epoch 44 | loss: 0.42761 | val_0_rmse: 0.67661 | val_1_rmse: 0.7163  |  0:00:15s
epoch 45 | loss: 0.42476 | val_0_rmse: 0.67443 | val_1_rmse: 0.73409 |  0:00:16s
epoch 46 | loss: 0.41952 | val_0_rmse: 0.68873 | val_1_rmse: 0.73149 |  0:00:16s
epoch 47 | loss: 0.42159 | val_0_rmse: 0.68799 | val_1_rmse: 0.73444 |  0:00:16s
epoch 48 | loss: 0.42677 | val_0_rmse: 0.69248 | val_1_rmse: 0.7345  |  0:00:17s
epoch 49 | loss: 0.42549 | val_0_rmse: 0.69442 | val_1_rmse: 0.74239 |  0:00:17s
epoch 50 | loss: 0.41112 | val_0_rmse: 0.70513 | val_1_rmse: 0.75758 |  0:00:17s
epoch 51 | loss: 0.40479 | val_0_rmse: 0.68271 | val_1_rmse: 0.73346 |  0:00:18s
epoch 52 | loss: 0.41527 | val_0_rmse: 0.66868 | val_1_rmse: 0.72108 |  0:00:18s
epoch 53 | loss: 0.40561 | val_0_rmse: 0.66805 | val_1_rmse: 0.72545 |  0:00:18s
epoch 54 | loss: 0.40968 | val_0_rmse: 0.67182 | val_1_rmse: 0.72625 |  0:00:19s
epoch 55 | loss: 0.40667 | val_0_rmse: 0.68012 | val_1_rmse: 0.73328 |  0:00:19s
epoch 56 | loss: 0.4076  | val_0_rmse: 0.67295 | val_1_rmse: 0.7117  |  0:00:19s
epoch 57 | loss: 0.41233 | val_0_rmse: 0.67005 | val_1_rmse: 0.71904 |  0:00:20s
epoch 58 | loss: 0.40179 | val_0_rmse: 0.70248 | val_1_rmse: 0.75025 |  0:00:20s
epoch 59 | loss: 0.40602 | val_0_rmse: 0.66647 | val_1_rmse: 0.70379 |  0:00:20s
epoch 60 | loss: 0.41757 | val_0_rmse: 0.67578 | val_1_rmse: 0.70833 |  0:00:21s
epoch 61 | loss: 0.40489 | val_0_rmse: 0.65242 | val_1_rmse: 0.69585 |  0:00:21s
epoch 62 | loss: 0.38766 | val_0_rmse: 0.6463  | val_1_rmse: 0.69176 |  0:00:22s
epoch 63 | loss: 0.39711 | val_0_rmse: 0.64442 | val_1_rmse: 0.68993 |  0:00:22s
epoch 64 | loss: 0.38477 | val_0_rmse: 0.64535 | val_1_rmse: 0.68646 |  0:00:22s
epoch 65 | loss: 0.38338 | val_0_rmse: 0.64865 | val_1_rmse: 0.69113 |  0:00:23s
epoch 66 | loss: 0.3892  | val_0_rmse: 0.64074 | val_1_rmse: 0.68166 |  0:00:23s
epoch 67 | loss: 0.37477 | val_0_rmse: 0.6278  | val_1_rmse: 0.67546 |  0:00:23s
epoch 68 | loss: 0.36892 | val_0_rmse: 0.62245 | val_1_rmse: 0.66222 |  0:00:24s
epoch 69 | loss: 0.36173 | val_0_rmse: 0.6224  | val_1_rmse: 0.67411 |  0:00:24s
epoch 70 | loss: 0.37354 | val_0_rmse: 0.61803 | val_1_rmse: 0.66524 |  0:00:24s
epoch 71 | loss: 0.37146 | val_0_rmse: 0.62329 | val_1_rmse: 0.66425 |  0:00:25s
epoch 72 | loss: 0.36884 | val_0_rmse: 0.62593 | val_1_rmse: 0.66899 |  0:00:25s
epoch 73 | loss: 0.36283 | val_0_rmse: 0.63282 | val_1_rmse: 0.6653  |  0:00:25s
epoch 74 | loss: 0.37155 | val_0_rmse: 0.62539 | val_1_rmse: 0.66298 |  0:00:26s
epoch 75 | loss: 0.36072 | val_0_rmse: 0.63076 | val_1_rmse: 0.66985 |  0:00:26s
epoch 76 | loss: 0.36456 | val_0_rmse: 0.62699 | val_1_rmse: 0.66144 |  0:00:26s
epoch 77 | loss: 0.36752 | val_0_rmse: 0.62996 | val_1_rmse: 0.67289 |  0:00:27s
epoch 78 | loss: 0.36448 | val_0_rmse: 0.63101 | val_1_rmse: 0.67616 |  0:00:27s
epoch 79 | loss: 0.36187 | val_0_rmse: 0.62843 | val_1_rmse: 0.67505 |  0:00:28s
epoch 80 | loss: 0.3564  | val_0_rmse: 0.6393  | val_1_rmse: 0.67796 |  0:00:28s
epoch 81 | loss: 0.36036 | val_0_rmse: 0.63162 | val_1_rmse: 0.66682 |  0:00:28s
epoch 82 | loss: 0.36576 | val_0_rmse: 0.62888 | val_1_rmse: 0.66834 |  0:00:29s
epoch 83 | loss: 0.36695 | val_0_rmse: 0.6427  | val_1_rmse: 0.67106 |  0:00:29s
epoch 84 | loss: 0.36174 | val_0_rmse: 0.65908 | val_1_rmse: 0.68453 |  0:00:29s
epoch 85 | loss: 0.36942 | val_0_rmse: 0.63283 | val_1_rmse: 0.66329 |  0:00:30s
epoch 86 | loss: 0.35958 | val_0_rmse: 0.61692 | val_1_rmse: 0.6552  |  0:00:30s
epoch 87 | loss: 0.34973 | val_0_rmse: 0.62006 | val_1_rmse: 0.65639 |  0:00:30s
epoch 88 | loss: 0.35169 | val_0_rmse: 0.61652 | val_1_rmse: 0.65194 |  0:00:31s
epoch 89 | loss: 0.35007 | val_0_rmse: 0.62446 | val_1_rmse: 0.65826 |  0:00:31s
epoch 90 | loss: 0.35373 | val_0_rmse: 0.62572 | val_1_rmse: 0.65659 |  0:00:31s
epoch 91 | loss: 0.35298 | val_0_rmse: 0.61447 | val_1_rmse: 0.64957 |  0:00:32s
epoch 92 | loss: 0.35093 | val_0_rmse: 0.60137 | val_1_rmse: 0.64233 |  0:00:32s
epoch 93 | loss: 0.34835 | val_0_rmse: 0.60327 | val_1_rmse: 0.6487  |  0:00:33s
epoch 94 | loss: 0.35125 | val_0_rmse: 0.60907 | val_1_rmse: 0.64597 |  0:00:33s
epoch 95 | loss: 0.35585 | val_0_rmse: 0.63284 | val_1_rmse: 0.66373 |  0:00:33s
epoch 96 | loss: 0.36056 | val_0_rmse: 0.606   | val_1_rmse: 0.64966 |  0:00:34s
epoch 97 | loss: 0.36125 | val_0_rmse: 0.61119 | val_1_rmse: 0.66275 |  0:00:34s
epoch 98 | loss: 0.35271 | val_0_rmse: 0.61374 | val_1_rmse: 0.65263 |  0:00:34s
epoch 99 | loss: 0.35583 | val_0_rmse: 0.60808 | val_1_rmse: 0.65229 |  0:00:35s
epoch 100| loss: 0.35139 | val_0_rmse: 0.61475 | val_1_rmse: 0.66085 |  0:00:35s
epoch 101| loss: 0.35867 | val_0_rmse: 0.61565 | val_1_rmse: 0.66265 |  0:00:35s
epoch 102| loss: 0.35575 | val_0_rmse: 0.6131  | val_1_rmse: 0.66113 |  0:00:36s
epoch 103| loss: 0.35426 | val_0_rmse: 0.60718 | val_1_rmse: 0.65676 |  0:00:36s
epoch 104| loss: 0.3446  | val_0_rmse: 0.60923 | val_1_rmse: 0.65711 |  0:00:36s
epoch 105| loss: 0.34771 | val_0_rmse: 0.61132 | val_1_rmse: 0.65529 |  0:00:37s
epoch 106| loss: 0.34609 | val_0_rmse: 0.61043 | val_1_rmse: 0.6518  |  0:00:37s
epoch 107| loss: 0.35153 | val_0_rmse: 0.62292 | val_1_rmse: 0.666   |  0:00:37s
epoch 108| loss: 0.34658 | val_0_rmse: 0.60592 | val_1_rmse: 0.64826 |  0:00:38s
epoch 109| loss: 0.35252 | val_0_rmse: 0.65685 | val_1_rmse: 0.69404 |  0:00:38s
epoch 110| loss: 0.34855 | val_0_rmse: 0.58571 | val_1_rmse: 0.65193 |  0:00:38s
epoch 111| loss: 0.34518 | val_0_rmse: 0.58335 | val_1_rmse: 0.65485 |  0:00:39s
epoch 112| loss: 0.3467  | val_0_rmse: 0.58519 | val_1_rmse: 0.64175 |  0:00:39s
epoch 113| loss: 0.35097 | val_0_rmse: 0.59347 | val_1_rmse: 0.64206 |  0:00:40s
epoch 114| loss: 0.3458  | val_0_rmse: 0.59015 | val_1_rmse: 0.64413 |  0:00:40s
epoch 115| loss: 0.34297 | val_0_rmse: 0.58899 | val_1_rmse: 0.63947 |  0:00:40s
epoch 116| loss: 0.34124 | val_0_rmse: 0.58515 | val_1_rmse: 0.64114 |  0:00:41s
epoch 117| loss: 0.3507  | val_0_rmse: 0.58733 | val_1_rmse: 0.64007 |  0:00:41s
epoch 118| loss: 0.34537 | val_0_rmse: 0.59274 | val_1_rmse: 0.6427  |  0:00:41s
epoch 119| loss: 0.33644 | val_0_rmse: 0.58456 | val_1_rmse: 0.63849 |  0:00:42s
epoch 120| loss: 0.33882 | val_0_rmse: 0.5816  | val_1_rmse: 0.64331 |  0:00:42s
epoch 121| loss: 0.33829 | val_0_rmse: 0.58602 | val_1_rmse: 0.6344  |  0:00:42s
epoch 122| loss: 0.33972 | val_0_rmse: 0.58802 | val_1_rmse: 0.63447 |  0:00:43s
epoch 123| loss: 0.33309 | val_0_rmse: 0.58861 | val_1_rmse: 0.64329 |  0:00:43s
epoch 124| loss: 0.34075 | val_0_rmse: 0.59063 | val_1_rmse: 0.63119 |  0:00:43s
epoch 125| loss: 0.34147 | val_0_rmse: 0.62077 | val_1_rmse: 0.66116 |  0:00:44s
epoch 126| loss: 0.33606 | val_0_rmse: 0.58306 | val_1_rmse: 0.63975 |  0:00:44s
epoch 127| loss: 0.33954 | val_0_rmse: 0.58885 | val_1_rmse: 0.65044 |  0:00:44s
epoch 128| loss: 0.33691 | val_0_rmse: 0.58908 | val_1_rmse: 0.63628 |  0:00:45s
epoch 129| loss: 0.33897 | val_0_rmse: 0.57602 | val_1_rmse: 0.63818 |  0:00:45s
epoch 130| loss: 0.33492 | val_0_rmse: 0.57911 | val_1_rmse: 0.63613 |  0:00:45s
epoch 131| loss: 0.33782 | val_0_rmse: 0.59552 | val_1_rmse: 0.63926 |  0:00:46s
epoch 132| loss: 0.33304 | val_0_rmse: 0.61181 | val_1_rmse: 0.65183 |  0:00:46s
epoch 133| loss: 0.33309 | val_0_rmse: 0.60678 | val_1_rmse: 0.6464  |  0:00:47s
epoch 134| loss: 0.33738 | val_0_rmse: 0.58704 | val_1_rmse: 0.63726 |  0:00:47s
epoch 135| loss: 0.33561 | val_0_rmse: 0.57606 | val_1_rmse: 0.63587 |  0:00:47s
epoch 136| loss: 0.33335 | val_0_rmse: 0.58251 | val_1_rmse: 0.63456 |  0:00:48s
epoch 137| loss: 0.33487 | val_0_rmse: 0.58342 | val_1_rmse: 0.63216 |  0:00:48s
epoch 138| loss: 0.33516 | val_0_rmse: 0.5814  | val_1_rmse: 0.63506 |  0:00:48s
epoch 139| loss: 0.34102 | val_0_rmse: 0.58212 | val_1_rmse: 0.62999 |  0:00:49s
epoch 140| loss: 0.34086 | val_0_rmse: 0.58047 | val_1_rmse: 0.63497 |  0:00:49s
epoch 141| loss: 0.33966 | val_0_rmse: 0.58773 | val_1_rmse: 0.63711 |  0:00:49s
epoch 142| loss: 0.34524 | val_0_rmse: 0.59378 | val_1_rmse: 0.63094 |  0:00:50s
epoch 143| loss: 0.34352 | val_0_rmse: 0.60525 | val_1_rmse: 0.63097 |  0:00:50s
epoch 144| loss: 0.34662 | val_0_rmse: 0.5859  | val_1_rmse: 0.6368  |  0:00:50s
epoch 145| loss: 0.34021 | val_0_rmse: 0.59356 | val_1_rmse: 0.64502 |  0:00:51s
epoch 146| loss: 0.34038 | val_0_rmse: 0.59482 | val_1_rmse: 0.63658 |  0:00:51s
epoch 147| loss: 0.33729 | val_0_rmse: 0.59629 | val_1_rmse: 0.63309 |  0:00:51s
epoch 148| loss: 0.346   | val_0_rmse: 0.60082 | val_1_rmse: 0.64659 |  0:00:52s
epoch 149| loss: 0.34111 | val_0_rmse: 0.59429 | val_1_rmse: 0.64334 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.62999
Best weights from best epoch are automatically used!
ended training at: 23:01:51
Feature importance:
Mean squared error is of 3267867624.375886
Mean absolute error:38777.62188603566
MAPE:0.39061990355582
R2 score:0.6158582626170568
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:01:51
epoch 0  | loss: 3.95979 | val_0_rmse: 1.08538 | val_1_rmse: 1.1083  |  0:00:00s
epoch 1  | loss: 1.88822 | val_0_rmse: 1.02825 | val_1_rmse: 1.0536  |  0:00:00s
epoch 2  | loss: 1.32051 | val_0_rmse: 1.01863 | val_1_rmse: 1.04284 |  0:00:01s
epoch 3  | loss: 1.0539  | val_0_rmse: 1.02787 | val_1_rmse: 1.053   |  0:00:01s
epoch 4  | loss: 0.88    | val_0_rmse: 0.80256 | val_1_rmse: 0.8137  |  0:00:01s
epoch 5  | loss: 0.73077 | val_0_rmse: 0.78725 | val_1_rmse: 0.80175 |  0:00:02s
epoch 6  | loss: 0.63918 | val_0_rmse: 0.78695 | val_1_rmse: 0.81756 |  0:00:02s
epoch 7  | loss: 0.55123 | val_0_rmse: 0.73833 | val_1_rmse: 0.76818 |  0:00:02s
epoch 8  | loss: 0.53224 | val_0_rmse: 0.74174 | val_1_rmse: 0.76428 |  0:00:03s
epoch 9  | loss: 0.51541 | val_0_rmse: 0.7749  | val_1_rmse: 0.7946  |  0:00:03s
epoch 10 | loss: 0.51517 | val_0_rmse: 0.69658 | val_1_rmse: 0.72206 |  0:00:03s
epoch 11 | loss: 0.50101 | val_0_rmse: 0.69608 | val_1_rmse: 0.72409 |  0:00:04s
epoch 12 | loss: 0.50347 | val_0_rmse: 0.71054 | val_1_rmse: 0.73784 |  0:00:04s
epoch 13 | loss: 0.49518 | val_0_rmse: 0.70003 | val_1_rmse: 0.72736 |  0:00:04s
epoch 14 | loss: 0.48022 | val_0_rmse: 0.69366 | val_1_rmse: 0.72362 |  0:00:05s
epoch 15 | loss: 0.48314 | val_0_rmse: 0.71011 | val_1_rmse: 0.74403 |  0:00:05s
epoch 16 | loss: 0.48013 | val_0_rmse: 0.68966 | val_1_rmse: 0.71754 |  0:00:05s
epoch 17 | loss: 0.47899 | val_0_rmse: 0.70249 | val_1_rmse: 0.72467 |  0:00:06s
epoch 18 | loss: 0.46645 | val_0_rmse: 0.69065 | val_1_rmse: 0.71516 |  0:00:06s
epoch 19 | loss: 0.44055 | val_0_rmse: 0.70744 | val_1_rmse: 0.75059 |  0:00:06s
epoch 20 | loss: 0.43296 | val_0_rmse: 0.70335 | val_1_rmse: 0.75453 |  0:00:07s
epoch 21 | loss: 0.42166 | val_0_rmse: 0.72215 | val_1_rmse: 0.78218 |  0:00:07s
epoch 22 | loss: 0.40558 | val_0_rmse: 0.71328 | val_1_rmse: 0.78356 |  0:00:08s
epoch 23 | loss: 0.39754 | val_0_rmse: 0.71956 | val_1_rmse: 0.78227 |  0:00:08s
epoch 24 | loss: 0.39006 | val_0_rmse: 0.73974 | val_1_rmse: 0.81125 |  0:00:08s
epoch 25 | loss: 0.38599 | val_0_rmse: 0.7314  | val_1_rmse: 0.7959  |  0:00:09s
epoch 26 | loss: 0.37426 | val_0_rmse: 0.73024 | val_1_rmse: 0.7952  |  0:00:09s
epoch 27 | loss: 0.36492 | val_0_rmse: 0.69602 | val_1_rmse: 0.76353 |  0:00:09s
epoch 28 | loss: 0.35261 | val_0_rmse: 0.71125 | val_1_rmse: 0.77976 |  0:00:10s
epoch 29 | loss: 0.35642 | val_0_rmse: 0.70329 | val_1_rmse: 0.77532 |  0:00:10s
epoch 30 | loss: 0.34976 | val_0_rmse: 0.71153 | val_1_rmse: 0.78615 |  0:00:10s
epoch 31 | loss: 0.35519 | val_0_rmse: 0.69806 | val_1_rmse: 0.76303 |  0:00:11s
epoch 32 | loss: 0.34948 | val_0_rmse: 0.68307 | val_1_rmse: 0.76066 |  0:00:11s
epoch 33 | loss: 0.3378  | val_0_rmse: 0.69791 | val_1_rmse: 0.77286 |  0:00:11s
epoch 34 | loss: 0.3321  | val_0_rmse: 0.69422 | val_1_rmse: 0.77195 |  0:00:12s
epoch 35 | loss: 0.34162 | val_0_rmse: 0.69474 | val_1_rmse: 0.77118 |  0:00:12s
epoch 36 | loss: 0.32091 | val_0_rmse: 0.70873 | val_1_rmse: 0.77948 |  0:00:12s
epoch 37 | loss: 0.32851 | val_0_rmse: 0.71362 | val_1_rmse: 0.76752 |  0:00:13s
epoch 38 | loss: 0.32506 | val_0_rmse: 0.6887  | val_1_rmse: 0.74368 |  0:00:13s
epoch 39 | loss: 0.32773 | val_0_rmse: 0.70033 | val_1_rmse: 0.75726 |  0:00:13s
epoch 40 | loss: 0.32731 | val_0_rmse: 0.69408 | val_1_rmse: 0.7805  |  0:00:14s
epoch 41 | loss: 0.32358 | val_0_rmse: 0.69669 | val_1_rmse: 0.77134 |  0:00:14s
epoch 42 | loss: 0.31582 | val_0_rmse: 0.66572 | val_1_rmse: 0.739   |  0:00:14s
epoch 43 | loss: 0.32579 | val_0_rmse: 0.68971 | val_1_rmse: 0.75199 |  0:00:15s
epoch 44 | loss: 0.31658 | val_0_rmse: 0.67139 | val_1_rmse: 0.73918 |  0:00:15s
epoch 45 | loss: 0.31862 | val_0_rmse: 0.68112 | val_1_rmse: 0.74647 |  0:00:15s
epoch 46 | loss: 0.31866 | val_0_rmse: 0.67763 | val_1_rmse: 0.73901 |  0:00:16s
epoch 47 | loss: 0.31665 | val_0_rmse: 0.6799  | val_1_rmse: 0.74612 |  0:00:16s
epoch 48 | loss: 0.30927 | val_0_rmse: 0.69576 | val_1_rmse: 0.75182 |  0:00:17s

Early stopping occured at epoch 48 with best_epoch = 18 and best_val_1_rmse = 0.71516
Best weights from best epoch are automatically used!
ended training at: 23:02:09
Feature importance:
Mean squared error is of 3883742076.420134
Mean absolute error:43448.340545919986
MAPE:0.36111352798534546
R2 score:0.4727287987746517
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:02:09
epoch 0  | loss: 3.3654  | val_0_rmse: 0.98675 | val_1_rmse: 0.99819 |  0:00:00s
epoch 1  | loss: 1.39873 | val_0_rmse: 0.94021 | val_1_rmse: 0.95185 |  0:00:00s
epoch 2  | loss: 1.01679 | val_0_rmse: 0.86999 | val_1_rmse: 0.88313 |  0:00:01s
epoch 3  | loss: 0.75704 | val_0_rmse: 0.86571 | val_1_rmse: 0.86018 |  0:00:01s
epoch 4  | loss: 0.66451 | val_0_rmse: 0.80324 | val_1_rmse: 0.80005 |  0:00:01s
epoch 5  | loss: 0.62426 | val_0_rmse: 0.77822 | val_1_rmse: 0.7776  |  0:00:02s
epoch 6  | loss: 0.57483 | val_0_rmse: 0.77729 | val_1_rmse: 0.78269 |  0:00:02s
epoch 7  | loss: 0.52902 | val_0_rmse: 0.76893 | val_1_rmse: 0.77465 |  0:00:02s
epoch 8  | loss: 0.48685 | val_0_rmse: 0.74336 | val_1_rmse: 0.7498  |  0:00:03s
epoch 9  | loss: 0.46352 | val_0_rmse: 0.74673 | val_1_rmse: 0.74856 |  0:00:03s
epoch 10 | loss: 0.43682 | val_0_rmse: 0.77158 | val_1_rmse: 0.77065 |  0:00:03s
epoch 11 | loss: 0.40248 | val_0_rmse: 0.73585 | val_1_rmse: 0.73109 |  0:00:04s
epoch 12 | loss: 0.3927  | val_0_rmse: 0.76429 | val_1_rmse: 0.76257 |  0:00:04s
epoch 13 | loss: 0.37931 | val_0_rmse: 0.71819 | val_1_rmse: 0.715   |  0:00:04s
epoch 14 | loss: 0.38445 | val_0_rmse: 0.72408 | val_1_rmse: 0.72709 |  0:00:05s
epoch 15 | loss: 0.37373 | val_0_rmse: 0.72883 | val_1_rmse: 0.73408 |  0:00:05s
epoch 16 | loss: 0.36089 | val_0_rmse: 0.69926 | val_1_rmse: 0.70032 |  0:00:06s
epoch 17 | loss: 0.34814 | val_0_rmse: 0.69369 | val_1_rmse: 0.69697 |  0:00:06s
epoch 18 | loss: 0.35932 | val_0_rmse: 0.6734  | val_1_rmse: 0.67518 |  0:00:06s
epoch 19 | loss: 0.34091 | val_0_rmse: 0.68892 | val_1_rmse: 0.69067 |  0:00:07s
epoch 20 | loss: 0.33898 | val_0_rmse: 0.67542 | val_1_rmse: 0.67375 |  0:00:07s
epoch 21 | loss: 0.33591 | val_0_rmse: 0.66884 | val_1_rmse: 0.66633 |  0:00:07s
epoch 22 | loss: 0.33854 | val_0_rmse: 0.70638 | val_1_rmse: 0.70945 |  0:00:08s
epoch 23 | loss: 0.34022 | val_0_rmse: 0.66699 | val_1_rmse: 0.66068 |  0:00:08s
epoch 24 | loss: 0.32717 | val_0_rmse: 0.66827 | val_1_rmse: 0.66609 |  0:00:08s
epoch 25 | loss: 0.3241  | val_0_rmse: 0.65887 | val_1_rmse: 0.65922 |  0:00:09s
epoch 26 | loss: 0.31905 | val_0_rmse: 0.654   | val_1_rmse: 0.65539 |  0:00:09s
epoch 27 | loss: 0.32252 | val_0_rmse: 0.65971 | val_1_rmse: 0.65908 |  0:00:09s
epoch 28 | loss: 0.31372 | val_0_rmse: 0.66527 | val_1_rmse: 0.6669  |  0:00:10s
epoch 29 | loss: 0.3056  | val_0_rmse: 0.65431 | val_1_rmse: 0.65874 |  0:00:10s
epoch 30 | loss: 0.31031 | val_0_rmse: 0.66838 | val_1_rmse: 0.67807 |  0:00:10s
epoch 31 | loss: 0.2995  | val_0_rmse: 0.65611 | val_1_rmse: 0.66671 |  0:00:11s
epoch 32 | loss: 0.30239 | val_0_rmse: 0.65447 | val_1_rmse: 0.66615 |  0:00:11s
epoch 33 | loss: 0.30315 | val_0_rmse: 0.66362 | val_1_rmse: 0.67835 |  0:00:11s
epoch 34 | loss: 0.29228 | val_0_rmse: 0.6623  | val_1_rmse: 0.67704 |  0:00:12s
epoch 35 | loss: 0.28824 | val_0_rmse: 0.6662  | val_1_rmse: 0.67726 |  0:00:12s
epoch 36 | loss: 0.28715 | val_0_rmse: 0.65777 | val_1_rmse: 0.66852 |  0:00:12s
epoch 37 | loss: 0.2901  | val_0_rmse: 0.65014 | val_1_rmse: 0.66497 |  0:00:13s
epoch 38 | loss: 0.29119 | val_0_rmse: 0.65499 | val_1_rmse: 0.67176 |  0:00:13s
epoch 39 | loss: 0.28737 | val_0_rmse: 0.63783 | val_1_rmse: 0.65685 |  0:00:14s
epoch 40 | loss: 0.28554 | val_0_rmse: 0.64211 | val_1_rmse: 0.65943 |  0:00:14s
epoch 41 | loss: 0.28455 | val_0_rmse: 0.64316 | val_1_rmse: 0.66503 |  0:00:14s
epoch 42 | loss: 0.28452 | val_0_rmse: 0.64858 | val_1_rmse: 0.66948 |  0:00:15s
epoch 43 | loss: 0.29396 | val_0_rmse: 0.64558 | val_1_rmse: 0.66389 |  0:00:15s
epoch 44 | loss: 0.28943 | val_0_rmse: 0.62534 | val_1_rmse: 0.6428  |  0:00:15s
epoch 45 | loss: 0.28409 | val_0_rmse: 0.63487 | val_1_rmse: 0.65314 |  0:00:16s
epoch 46 | loss: 0.28127 | val_0_rmse: 0.62561 | val_1_rmse: 0.64301 |  0:00:16s
epoch 47 | loss: 0.282   | val_0_rmse: 0.60698 | val_1_rmse: 0.62511 |  0:00:16s
epoch 48 | loss: 0.27658 | val_0_rmse: 0.63087 | val_1_rmse: 0.65531 |  0:00:17s
epoch 49 | loss: 0.2811  | val_0_rmse: 0.62725 | val_1_rmse: 0.64967 |  0:00:17s
epoch 50 | loss: 0.28341 | val_0_rmse: 0.60816 | val_1_rmse: 0.62793 |  0:00:17s
epoch 51 | loss: 0.27614 | val_0_rmse: 0.61261 | val_1_rmse: 0.6361  |  0:00:18s
epoch 52 | loss: 0.27587 | val_0_rmse: 0.59593 | val_1_rmse: 0.61805 |  0:00:18s
epoch 53 | loss: 0.27164 | val_0_rmse: 0.59768 | val_1_rmse: 0.61729 |  0:00:18s
epoch 54 | loss: 0.27713 | val_0_rmse: 0.5965  | val_1_rmse: 0.62073 |  0:00:19s
epoch 55 | loss: 0.27352 | val_0_rmse: 0.60328 | val_1_rmse: 0.62814 |  0:00:19s
epoch 56 | loss: 0.26851 | val_0_rmse: 0.62365 | val_1_rmse: 0.65183 |  0:00:19s
epoch 57 | loss: 0.2734  | val_0_rmse: 0.59592 | val_1_rmse: 0.62392 |  0:00:20s
epoch 58 | loss: 0.27447 | val_0_rmse: 0.59922 | val_1_rmse: 0.63132 |  0:00:20s
epoch 59 | loss: 0.27373 | val_0_rmse: 0.59903 | val_1_rmse: 0.63591 |  0:00:21s
epoch 60 | loss: 0.27007 | val_0_rmse: 0.58415 | val_1_rmse: 0.62872 |  0:00:21s
epoch 61 | loss: 0.27568 | val_0_rmse: 0.5906  | val_1_rmse: 0.62908 |  0:00:21s
epoch 62 | loss: 0.26487 | val_0_rmse: 0.59445 | val_1_rmse: 0.63787 |  0:00:22s
epoch 63 | loss: 0.26781 | val_0_rmse: 0.60204 | val_1_rmse: 0.6429  |  0:00:22s
epoch 64 | loss: 0.27176 | val_0_rmse: 0.58001 | val_1_rmse: 0.62657 |  0:00:22s
epoch 65 | loss: 0.26414 | val_0_rmse: 0.58181 | val_1_rmse: 0.62674 |  0:00:23s
epoch 66 | loss: 0.25619 | val_0_rmse: 0.56621 | val_1_rmse: 0.60988 |  0:00:23s
epoch 67 | loss: 0.26386 | val_0_rmse: 0.57548 | val_1_rmse: 0.61844 |  0:00:23s
epoch 68 | loss: 0.25676 | val_0_rmse: 0.57346 | val_1_rmse: 0.62569 |  0:00:24s
epoch 69 | loss: 0.25286 | val_0_rmse: 0.58055 | val_1_rmse: 0.63159 |  0:00:24s
epoch 70 | loss: 0.25062 | val_0_rmse: 0.58097 | val_1_rmse: 0.62788 |  0:00:24s
epoch 71 | loss: 0.25559 | val_0_rmse: 0.57244 | val_1_rmse: 0.62061 |  0:00:25s
epoch 72 | loss: 0.25691 | val_0_rmse: 0.57928 | val_1_rmse: 0.62609 |  0:00:25s
epoch 73 | loss: 0.25875 | val_0_rmse: 0.56502 | val_1_rmse: 0.60528 |  0:00:25s
epoch 74 | loss: 0.25475 | val_0_rmse: 0.5713  | val_1_rmse: 0.6067  |  0:00:26s
epoch 75 | loss: 0.24773 | val_0_rmse: 0.57362 | val_1_rmse: 0.61221 |  0:00:26s
epoch 76 | loss: 0.24361 | val_0_rmse: 0.57399 | val_1_rmse: 0.62115 |  0:00:26s
epoch 77 | loss: 0.25779 | val_0_rmse: 0.55677 | val_1_rmse: 0.60892 |  0:00:27s
epoch 78 | loss: 0.24807 | val_0_rmse: 0.56254 | val_1_rmse: 0.60574 |  0:00:27s
epoch 79 | loss: 0.24247 | val_0_rmse: 0.57517 | val_1_rmse: 0.61575 |  0:00:27s
epoch 80 | loss: 0.25112 | val_0_rmse: 0.55395 | val_1_rmse: 0.60794 |  0:00:28s
epoch 81 | loss: 0.24369 | val_0_rmse: 0.56816 | val_1_rmse: 0.63142 |  0:00:28s
epoch 82 | loss: 0.24699 | val_0_rmse: 0.53278 | val_1_rmse: 0.58639 |  0:00:29s
epoch 83 | loss: 0.24483 | val_0_rmse: 0.52548 | val_1_rmse: 0.58186 |  0:00:29s
epoch 84 | loss: 0.2392  | val_0_rmse: 0.52463 | val_1_rmse: 0.57858 |  0:00:29s
epoch 85 | loss: 0.24228 | val_0_rmse: 0.52822 | val_1_rmse: 0.58192 |  0:00:30s
epoch 86 | loss: 0.23461 | val_0_rmse: 0.5232  | val_1_rmse: 0.58297 |  0:00:30s
epoch 87 | loss: 0.23074 | val_0_rmse: 0.53729 | val_1_rmse: 0.59529 |  0:00:30s
epoch 88 | loss: 0.23377 | val_0_rmse: 0.52669 | val_1_rmse: 0.59196 |  0:00:31s
epoch 89 | loss: 0.23187 | val_0_rmse: 0.5177  | val_1_rmse: 0.58765 |  0:00:31s
epoch 90 | loss: 0.23726 | val_0_rmse: 0.51404 | val_1_rmse: 0.58582 |  0:00:31s
epoch 91 | loss: 0.22064 | val_0_rmse: 0.50757 | val_1_rmse: 0.583   |  0:00:32s
epoch 92 | loss: 0.22739 | val_0_rmse: 0.51307 | val_1_rmse: 0.58711 |  0:00:32s
epoch 93 | loss: 0.23103 | val_0_rmse: 0.5103  | val_1_rmse: 0.5848  |  0:00:32s
epoch 94 | loss: 0.22931 | val_0_rmse: 0.51254 | val_1_rmse: 0.58404 |  0:00:33s
epoch 95 | loss: 0.232   | val_0_rmse: 0.49987 | val_1_rmse: 0.58709 |  0:00:33s
epoch 96 | loss: 0.22967 | val_0_rmse: 0.49307 | val_1_rmse: 0.58535 |  0:00:33s
epoch 97 | loss: 0.22636 | val_0_rmse: 0.50304 | val_1_rmse: 0.59484 |  0:00:34s
epoch 98 | loss: 0.22325 | val_0_rmse: 0.50887 | val_1_rmse: 0.60071 |  0:00:34s
epoch 99 | loss: 0.23036 | val_0_rmse: 0.48436 | val_1_rmse: 0.58577 |  0:00:34s
epoch 100| loss: 0.2243  | val_0_rmse: 0.48826 | val_1_rmse: 0.588   |  0:00:35s
epoch 101| loss: 0.22377 | val_0_rmse: 0.49464 | val_1_rmse: 0.59607 |  0:00:35s
epoch 102| loss: 0.22416 | val_0_rmse: 0.4905  | val_1_rmse: 0.59893 |  0:00:36s
epoch 103| loss: 0.22423 | val_0_rmse: 0.49634 | val_1_rmse: 0.60825 |  0:00:36s
epoch 104| loss: 0.22555 | val_0_rmse: 0.47188 | val_1_rmse: 0.59889 |  0:00:36s
epoch 105| loss: 0.22897 | val_0_rmse: 0.48252 | val_1_rmse: 0.60692 |  0:00:37s
epoch 106| loss: 0.22496 | val_0_rmse: 0.47555 | val_1_rmse: 0.60162 |  0:00:37s
epoch 107| loss: 0.22118 | val_0_rmse: 0.48865 | val_1_rmse: 0.58984 |  0:00:37s
epoch 108| loss: 0.22437 | val_0_rmse: 0.48482 | val_1_rmse: 0.59425 |  0:00:38s
epoch 109| loss: 0.22892 | val_0_rmse: 0.48832 | val_1_rmse: 0.59478 |  0:00:38s
epoch 110| loss: 0.21386 | val_0_rmse: 0.48514 | val_1_rmse: 0.59433 |  0:00:38s
epoch 111| loss: 0.22167 | val_0_rmse: 0.47323 | val_1_rmse: 0.58145 |  0:00:39s
epoch 112| loss: 0.2287  | val_0_rmse: 0.48001 | val_1_rmse: 0.58066 |  0:00:39s
epoch 113| loss: 0.22215 | val_0_rmse: 0.46637 | val_1_rmse: 0.5848  |  0:00:39s
epoch 114| loss: 0.23074 | val_0_rmse: 0.48096 | val_1_rmse: 0.59827 |  0:00:40s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.57858
Best weights from best epoch are automatically used!
ended training at: 23:02:49
Feature importance:
Mean squared error is of 3011207839.562457
Mean absolute error:36217.66712112691
MAPE:0.3382576354616937
R2 score:0.6480083253286142
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:02:49
epoch 0  | loss: 3.58821 | val_0_rmse: 1.0038  | val_1_rmse: 1.02111 |  0:00:00s
epoch 1  | loss: 1.78231 | val_0_rmse: 0.98026 | val_1_rmse: 0.99964 |  0:00:00s
epoch 2  | loss: 1.15778 | val_0_rmse: 0.91666 | val_1_rmse: 0.94356 |  0:00:01s
epoch 3  | loss: 0.97546 | val_0_rmse: 0.80152 | val_1_rmse: 0.86902 |  0:00:01s
epoch 4  | loss: 0.69534 | val_0_rmse: 0.88758 | val_1_rmse: 0.98192 |  0:00:01s
epoch 5  | loss: 0.59073 | val_0_rmse: 0.79104 | val_1_rmse: 0.86497 |  0:00:02s
epoch 6  | loss: 0.54907 | val_0_rmse: 0.78124 | val_1_rmse: 0.86341 |  0:00:02s
epoch 7  | loss: 0.49434 | val_0_rmse: 0.78341 | val_1_rmse: 0.85339 |  0:00:02s
epoch 8  | loss: 0.44712 | val_0_rmse: 0.82496 | val_1_rmse: 0.9032  |  0:00:03s
epoch 9  | loss: 0.41735 | val_0_rmse: 0.77799 | val_1_rmse: 0.87296 |  0:00:03s
epoch 10 | loss: 0.40061 | val_0_rmse: 0.76947 | val_1_rmse: 0.84304 |  0:00:03s
epoch 11 | loss: 0.38613 | val_0_rmse: 0.71402 | val_1_rmse: 0.78223 |  0:00:04s
epoch 12 | loss: 0.37041 | val_0_rmse: 0.7209  | val_1_rmse: 0.79908 |  0:00:04s
epoch 13 | loss: 0.36952 | val_0_rmse: 0.69422 | val_1_rmse: 0.75821 |  0:00:04s
epoch 14 | loss: 0.361   | val_0_rmse: 0.72247 | val_1_rmse: 0.79685 |  0:00:05s
epoch 15 | loss: 0.35523 | val_0_rmse: 0.68119 | val_1_rmse: 0.75458 |  0:00:05s
epoch 16 | loss: 0.35145 | val_0_rmse: 0.66039 | val_1_rmse: 0.72507 |  0:00:06s
epoch 17 | loss: 0.34496 | val_0_rmse: 0.70119 | val_1_rmse: 0.7841  |  0:00:06s
epoch 18 | loss: 0.33891 | val_0_rmse: 0.66708 | val_1_rmse: 0.73839 |  0:00:06s
epoch 19 | loss: 0.33576 | val_0_rmse: 0.67612 | val_1_rmse: 0.75549 |  0:00:07s
epoch 20 | loss: 0.33167 | val_0_rmse: 0.6591  | val_1_rmse: 0.7356  |  0:00:07s
epoch 21 | loss: 0.32639 | val_0_rmse: 0.67319 | val_1_rmse: 0.75872 |  0:00:07s
epoch 22 | loss: 0.32001 | val_0_rmse: 0.66734 | val_1_rmse: 0.74796 |  0:00:08s
epoch 23 | loss: 0.32391 | val_0_rmse: 0.65327 | val_1_rmse: 0.73405 |  0:00:08s
epoch 24 | loss: 0.31575 | val_0_rmse: 0.65227 | val_1_rmse: 0.7355  |  0:00:08s
epoch 25 | loss: 0.32433 | val_0_rmse: 0.64306 | val_1_rmse: 0.72195 |  0:00:09s
epoch 26 | loss: 0.3134  | val_0_rmse: 0.64516 | val_1_rmse: 0.7231  |  0:00:09s
epoch 27 | loss: 0.32382 | val_0_rmse: 0.6549  | val_1_rmse: 0.73958 |  0:00:09s
epoch 28 | loss: 0.31068 | val_0_rmse: 0.64716 | val_1_rmse: 0.73144 |  0:00:10s
epoch 29 | loss: 0.31494 | val_0_rmse: 0.65801 | val_1_rmse: 0.74091 |  0:00:10s
epoch 30 | loss: 0.31079 | val_0_rmse: 0.64687 | val_1_rmse: 0.72427 |  0:00:10s
epoch 31 | loss: 0.3115  | val_0_rmse: 0.64906 | val_1_rmse: 0.73084 |  0:00:11s
epoch 32 | loss: 0.31357 | val_0_rmse: 0.66225 | val_1_rmse: 0.74764 |  0:00:11s
epoch 33 | loss: 0.31526 | val_0_rmse: 0.65642 | val_1_rmse: 0.73878 |  0:00:11s
epoch 34 | loss: 0.31745 | val_0_rmse: 0.64211 | val_1_rmse: 0.71476 |  0:00:12s
epoch 35 | loss: 0.30342 | val_0_rmse: 0.64362 | val_1_rmse: 0.71987 |  0:00:12s
epoch 36 | loss: 0.30957 | val_0_rmse: 0.63688 | val_1_rmse: 0.71024 |  0:00:12s
epoch 37 | loss: 0.30353 | val_0_rmse: 0.63901 | val_1_rmse: 0.71205 |  0:00:13s
epoch 38 | loss: 0.29187 | val_0_rmse: 0.62839 | val_1_rmse: 0.70154 |  0:00:13s
epoch 39 | loss: 0.29679 | val_0_rmse: 0.62435 | val_1_rmse: 0.69152 |  0:00:14s
epoch 40 | loss: 0.29965 | val_0_rmse: 0.63142 | val_1_rmse: 0.7023  |  0:00:14s
epoch 41 | loss: 0.293   | val_0_rmse: 0.61866 | val_1_rmse: 0.69103 |  0:00:14s
epoch 42 | loss: 0.29214 | val_0_rmse: 0.62664 | val_1_rmse: 0.70111 |  0:00:15s
epoch 43 | loss: 0.29055 | val_0_rmse: 0.62421 | val_1_rmse: 0.70047 |  0:00:15s
epoch 44 | loss: 0.28549 | val_0_rmse: 0.63086 | val_1_rmse: 0.71277 |  0:00:15s
epoch 45 | loss: 0.29112 | val_0_rmse: 0.61296 | val_1_rmse: 0.6855  |  0:00:16s
epoch 46 | loss: 0.29006 | val_0_rmse: 0.62257 | val_1_rmse: 0.70353 |  0:00:16s
epoch 47 | loss: 0.28738 | val_0_rmse: 0.60954 | val_1_rmse: 0.68393 |  0:00:16s
epoch 48 | loss: 0.2939  | val_0_rmse: 0.61073 | val_1_rmse: 0.68627 |  0:00:17s
epoch 49 | loss: 0.29495 | val_0_rmse: 0.59877 | val_1_rmse: 0.6658  |  0:00:17s
epoch 50 | loss: 0.29003 | val_0_rmse: 0.60584 | val_1_rmse: 0.67868 |  0:00:17s
epoch 51 | loss: 0.28746 | val_0_rmse: 0.61317 | val_1_rmse: 0.69079 |  0:00:18s
epoch 52 | loss: 0.28271 | val_0_rmse: 0.6157  | val_1_rmse: 0.69197 |  0:00:18s
epoch 53 | loss: 0.28054 | val_0_rmse: 0.6114  | val_1_rmse: 0.69094 |  0:00:18s
epoch 54 | loss: 0.2829  | val_0_rmse: 0.60363 | val_1_rmse: 0.68468 |  0:00:19s
epoch 55 | loss: 0.28787 | val_0_rmse: 0.6262  | val_1_rmse: 0.71256 |  0:00:19s
epoch 56 | loss: 0.28627 | val_0_rmse: 0.6042  | val_1_rmse: 0.68914 |  0:00:20s
epoch 57 | loss: 0.2772  | val_0_rmse: 0.59645 | val_1_rmse: 0.67412 |  0:00:20s
epoch 58 | loss: 0.27481 | val_0_rmse: 0.6057  | val_1_rmse: 0.6935  |  0:00:20s
epoch 59 | loss: 0.27837 | val_0_rmse: 0.59808 | val_1_rmse: 0.68179 |  0:00:21s
epoch 60 | loss: 0.27929 | val_0_rmse: 0.60015 | val_1_rmse: 0.68378 |  0:00:21s
epoch 61 | loss: 0.27112 | val_0_rmse: 0.60504 | val_1_rmse: 0.69604 |  0:00:21s
epoch 62 | loss: 0.2788  | val_0_rmse: 0.59257 | val_1_rmse: 0.67251 |  0:00:22s
epoch 63 | loss: 0.27788 | val_0_rmse: 0.59491 | val_1_rmse: 0.68285 |  0:00:22s
epoch 64 | loss: 0.28374 | val_0_rmse: 0.59233 | val_1_rmse: 0.67973 |  0:00:22s
epoch 65 | loss: 0.27463 | val_0_rmse: 0.60468 | val_1_rmse: 0.69567 |  0:00:23s
epoch 66 | loss: 0.27911 | val_0_rmse: 0.58966 | val_1_rmse: 0.67804 |  0:00:23s
epoch 67 | loss: 0.27757 | val_0_rmse: 0.58853 | val_1_rmse: 0.66729 |  0:00:23s
epoch 68 | loss: 0.2809  | val_0_rmse: 0.58772 | val_1_rmse: 0.67422 |  0:00:24s
epoch 69 | loss: 0.27542 | val_0_rmse: 0.58687 | val_1_rmse: 0.66817 |  0:00:24s
epoch 70 | loss: 0.27277 | val_0_rmse: 0.58786 | val_1_rmse: 0.67767 |  0:00:24s
epoch 71 | loss: 0.26886 | val_0_rmse: 0.58265 | val_1_rmse: 0.67221 |  0:00:25s
epoch 72 | loss: 0.2741  | val_0_rmse: 0.58196 | val_1_rmse: 0.67258 |  0:00:25s
epoch 73 | loss: 0.2685  | val_0_rmse: 0.56956 | val_1_rmse: 0.6537  |  0:00:25s
epoch 74 | loss: 0.26572 | val_0_rmse: 0.57708 | val_1_rmse: 0.66939 |  0:00:26s
epoch 75 | loss: 0.26835 | val_0_rmse: 0.57178 | val_1_rmse: 0.65736 |  0:00:26s
epoch 76 | loss: 0.27008 | val_0_rmse: 0.57154 | val_1_rmse: 0.66543 |  0:00:26s
epoch 77 | loss: 0.26942 | val_0_rmse: 0.56956 | val_1_rmse: 0.66553 |  0:00:27s
epoch 78 | loss: 0.27492 | val_0_rmse: 0.58181 | val_1_rmse: 0.68971 |  0:00:27s
epoch 79 | loss: 0.27316 | val_0_rmse: 0.56198 | val_1_rmse: 0.64981 |  0:00:28s
epoch 80 | loss: 0.26859 | val_0_rmse: 0.56655 | val_1_rmse: 0.65585 |  0:00:28s
epoch 81 | loss: 0.27771 | val_0_rmse: 0.56476 | val_1_rmse: 0.65758 |  0:00:28s
epoch 82 | loss: 0.2685  | val_0_rmse: 0.5736  | val_1_rmse: 0.67255 |  0:00:29s
epoch 83 | loss: 0.26313 | val_0_rmse: 0.55357 | val_1_rmse: 0.65479 |  0:00:29s
epoch 84 | loss: 0.26872 | val_0_rmse: 0.556   | val_1_rmse: 0.66071 |  0:00:29s
epoch 85 | loss: 0.26448 | val_0_rmse: 0.55329 | val_1_rmse: 0.65162 |  0:00:30s
epoch 86 | loss: 0.27007 | val_0_rmse: 0.54795 | val_1_rmse: 0.64514 |  0:00:30s
epoch 87 | loss: 0.26335 | val_0_rmse: 0.5406  | val_1_rmse: 0.63424 |  0:00:30s
epoch 88 | loss: 0.26453 | val_0_rmse: 0.53056 | val_1_rmse: 0.62351 |  0:00:31s
epoch 89 | loss: 0.26804 | val_0_rmse: 0.54149 | val_1_rmse: 0.64045 |  0:00:31s
epoch 90 | loss: 0.26559 | val_0_rmse: 0.55005 | val_1_rmse: 0.65231 |  0:00:31s
epoch 91 | loss: 0.26781 | val_0_rmse: 0.55104 | val_1_rmse: 0.65513 |  0:00:32s
epoch 92 | loss: 0.27314 | val_0_rmse: 0.54526 | val_1_rmse: 0.65848 |  0:00:32s
epoch 93 | loss: 0.26729 | val_0_rmse: 0.55433 | val_1_rmse: 0.67851 |  0:00:32s
epoch 94 | loss: 0.26433 | val_0_rmse: 0.54982 | val_1_rmse: 0.66337 |  0:00:33s
epoch 95 | loss: 0.26701 | val_0_rmse: 0.54589 | val_1_rmse: 0.66696 |  0:00:33s
epoch 96 | loss: 0.26621 | val_0_rmse: 0.53533 | val_1_rmse: 0.64414 |  0:00:33s
epoch 97 | loss: 0.26441 | val_0_rmse: 0.532   | val_1_rmse: 0.65124 |  0:00:34s
epoch 98 | loss: 0.26197 | val_0_rmse: 0.53079 | val_1_rmse: 0.62974 |  0:00:34s
epoch 99 | loss: 0.26004 | val_0_rmse: 0.53715 | val_1_rmse: 0.62649 |  0:00:35s
epoch 100| loss: 0.25496 | val_0_rmse: 0.52566 | val_1_rmse: 0.62839 |  0:00:35s
epoch 101| loss: 0.26627 | val_0_rmse: 0.5257  | val_1_rmse: 0.63061 |  0:00:35s
epoch 102| loss: 0.26307 | val_0_rmse: 0.51844 | val_1_rmse: 0.6292  |  0:00:36s
epoch 103| loss: 0.25521 | val_0_rmse: 0.51201 | val_1_rmse: 0.63129 |  0:00:36s
epoch 104| loss: 0.26197 | val_0_rmse: 0.52015 | val_1_rmse: 0.63384 |  0:00:36s
epoch 105| loss: 0.26137 | val_0_rmse: 0.5163  | val_1_rmse: 0.63315 |  0:00:37s
epoch 106| loss: 0.25774 | val_0_rmse: 0.51381 | val_1_rmse: 0.6304  |  0:00:37s
epoch 107| loss: 0.25388 | val_0_rmse: 0.50896 | val_1_rmse: 0.63304 |  0:00:37s
epoch 108| loss: 0.24896 | val_0_rmse: 0.50505 | val_1_rmse: 0.62731 |  0:00:38s
epoch 109| loss: 0.25376 | val_0_rmse: 0.50238 | val_1_rmse: 0.62206 |  0:00:38s
epoch 110| loss: 0.2552  | val_0_rmse: 0.50871 | val_1_rmse: 0.6221  |  0:00:38s
epoch 111| loss: 0.2588  | val_0_rmse: 0.51438 | val_1_rmse: 0.62903 |  0:00:39s
epoch 112| loss: 0.25379 | val_0_rmse: 0.51467 | val_1_rmse: 0.62253 |  0:00:39s
epoch 113| loss: 0.26315 | val_0_rmse: 0.50766 | val_1_rmse: 0.62001 |  0:00:39s
epoch 114| loss: 0.25359 | val_0_rmse: 0.50665 | val_1_rmse: 0.60516 |  0:00:40s
epoch 115| loss: 0.25659 | val_0_rmse: 0.49978 | val_1_rmse: 0.61    |  0:00:40s
epoch 116| loss: 0.25695 | val_0_rmse: 0.5025  | val_1_rmse: 0.60517 |  0:00:40s
epoch 117| loss: 0.24796 | val_0_rmse: 0.50348 | val_1_rmse: 0.61327 |  0:00:41s
epoch 118| loss: 0.25709 | val_0_rmse: 0.5019  | val_1_rmse: 0.61802 |  0:00:41s
epoch 119| loss: 0.25034 | val_0_rmse: 0.50251 | val_1_rmse: 0.62975 |  0:00:42s
epoch 120| loss: 0.25354 | val_0_rmse: 0.50452 | val_1_rmse: 0.62975 |  0:00:42s
epoch 121| loss: 0.24593 | val_0_rmse: 0.50127 | val_1_rmse: 0.62053 |  0:00:42s
epoch 122| loss: 0.2453  | val_0_rmse: 0.5067  | val_1_rmse: 0.62561 |  0:00:43s
epoch 123| loss: 0.24558 | val_0_rmse: 0.492   | val_1_rmse: 0.6213  |  0:00:43s
epoch 124| loss: 0.24689 | val_0_rmse: 0.49073 | val_1_rmse: 0.62469 |  0:00:43s
epoch 125| loss: 0.25564 | val_0_rmse: 0.49091 | val_1_rmse: 0.61651 |  0:00:44s
epoch 126| loss: 0.24784 | val_0_rmse: 0.48367 | val_1_rmse: 0.6131  |  0:00:44s
epoch 127| loss: 0.23945 | val_0_rmse: 0.4933  | val_1_rmse: 0.63543 |  0:00:44s
epoch 128| loss: 0.25804 | val_0_rmse: 0.49051 | val_1_rmse: 0.62721 |  0:00:45s
epoch 129| loss: 0.25954 | val_0_rmse: 0.48832 | val_1_rmse: 0.6228  |  0:00:45s
epoch 130| loss: 0.24977 | val_0_rmse: 0.49011 | val_1_rmse: 0.61764 |  0:00:45s
epoch 131| loss: 0.25164 | val_0_rmse: 0.48807 | val_1_rmse: 0.60921 |  0:00:46s
epoch 132| loss: 0.24984 | val_0_rmse: 0.48638 | val_1_rmse: 0.60094 |  0:00:46s
epoch 133| loss: 0.25348 | val_0_rmse: 0.48513 | val_1_rmse: 0.60422 |  0:00:46s
epoch 134| loss: 0.25316 | val_0_rmse: 0.49477 | val_1_rmse: 0.60888 |  0:00:47s
epoch 135| loss: 0.25551 | val_0_rmse: 0.48387 | val_1_rmse: 0.60032 |  0:00:47s
epoch 136| loss: 0.25636 | val_0_rmse: 0.49322 | val_1_rmse: 0.59952 |  0:00:47s
epoch 137| loss: 0.24301 | val_0_rmse: 0.48184 | val_1_rmse: 0.59767 |  0:00:48s
epoch 138| loss: 0.24882 | val_0_rmse: 0.48402 | val_1_rmse: 0.59601 |  0:00:48s
epoch 139| loss: 0.2417  | val_0_rmse: 0.48195 | val_1_rmse: 0.60615 |  0:00:49s
epoch 140| loss: 0.24024 | val_0_rmse: 0.47421 | val_1_rmse: 0.60078 |  0:00:49s
epoch 141| loss: 0.2428  | val_0_rmse: 0.47993 | val_1_rmse: 0.60219 |  0:00:49s
epoch 142| loss: 0.23539 | val_0_rmse: 0.46896 | val_1_rmse: 0.5992  |  0:00:50s
epoch 143| loss: 0.23833 | val_0_rmse: 0.47295 | val_1_rmse: 0.59306 |  0:00:50s
epoch 144| loss: 0.24148 | val_0_rmse: 0.47413 | val_1_rmse: 0.60793 |  0:00:50s
epoch 145| loss: 0.24386 | val_0_rmse: 0.48655 | val_1_rmse: 0.61609 |  0:00:51s
epoch 146| loss: 0.24913 | val_0_rmse: 0.48996 | val_1_rmse: 0.6215  |  0:00:51s
epoch 147| loss: 0.25115 | val_0_rmse: 0.49159 | val_1_rmse: 0.60868 |  0:00:51s
epoch 148| loss: 0.25512 | val_0_rmse: 0.48996 | val_1_rmse: 0.59715 |  0:00:52s
epoch 149| loss: 0.24374 | val_0_rmse: 0.48326 | val_1_rmse: 0.58953 |  0:00:52s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.58953
Best weights from best epoch are automatically used!
ended training at: 23:03:42
Feature importance:
Mean squared error is of 2865623059.6429105
Mean absolute error:34474.03408003502
MAPE:0.32345316904875177
R2 score:0.6583491567735777
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:03:42
epoch 0  | loss: 3.24496 | val_0_rmse: 0.99864 | val_1_rmse: 0.94767 |  0:00:00s
epoch 1  | loss: 1.37312 | val_0_rmse: 0.99812 | val_1_rmse: 0.94952 |  0:00:00s
epoch 2  | loss: 1.06816 | val_0_rmse: 0.85889 | val_1_rmse: 0.83006 |  0:00:01s
epoch 3  | loss: 0.8349  | val_0_rmse: 0.83467 | val_1_rmse: 0.79836 |  0:00:01s
epoch 4  | loss: 0.77032 | val_0_rmse: 0.81234 | val_1_rmse: 0.7767  |  0:00:01s
epoch 5  | loss: 0.7142  | val_0_rmse: 0.81118 | val_1_rmse: 0.7735  |  0:00:02s
epoch 6  | loss: 0.62177 | val_0_rmse: 0.82986 | val_1_rmse: 0.77946 |  0:00:02s
epoch 7  | loss: 0.57878 | val_0_rmse: 0.84639 | val_1_rmse: 0.79339 |  0:00:02s
epoch 8  | loss: 0.52553 | val_0_rmse: 0.84523 | val_1_rmse: 0.80682 |  0:00:03s
epoch 9  | loss: 0.49776 | val_0_rmse: 0.80145 | val_1_rmse: 0.75737 |  0:00:03s
epoch 10 | loss: 0.47311 | val_0_rmse: 0.75645 | val_1_rmse: 0.74    |  0:00:03s
epoch 11 | loss: 0.43548 | val_0_rmse: 0.69186 | val_1_rmse: 0.66963 |  0:00:04s
epoch 12 | loss: 0.41271 | val_0_rmse: 0.70694 | val_1_rmse: 0.68404 |  0:00:04s
epoch 13 | loss: 0.40175 | val_0_rmse: 0.69496 | val_1_rmse: 0.67142 |  0:00:04s
epoch 14 | loss: 0.38533 | val_0_rmse: 0.68098 | val_1_rmse: 0.65222 |  0:00:05s
epoch 15 | loss: 0.37832 | val_0_rmse: 0.67716 | val_1_rmse: 0.64998 |  0:00:05s
epoch 16 | loss: 0.37445 | val_0_rmse: 0.6674  | val_1_rmse: 0.64216 |  0:00:06s
epoch 17 | loss: 0.37424 | val_0_rmse: 0.66784 | val_1_rmse: 0.64123 |  0:00:06s
epoch 18 | loss: 0.36228 | val_0_rmse: 0.67799 | val_1_rmse: 0.65216 |  0:00:06s
epoch 19 | loss: 0.36102 | val_0_rmse: 0.67733 | val_1_rmse: 0.65229 |  0:00:07s
epoch 20 | loss: 0.3591  | val_0_rmse: 0.6807  | val_1_rmse: 0.65611 |  0:00:07s
epoch 21 | loss: 0.35816 | val_0_rmse: 0.67468 | val_1_rmse: 0.65044 |  0:00:07s
epoch 22 | loss: 0.34923 | val_0_rmse: 0.67457 | val_1_rmse: 0.6515  |  0:00:08s
epoch 23 | loss: 0.35371 | val_0_rmse: 0.66396 | val_1_rmse: 0.64258 |  0:00:08s
epoch 24 | loss: 0.34482 | val_0_rmse: 0.66241 | val_1_rmse: 0.64262 |  0:00:08s
epoch 25 | loss: 0.34685 | val_0_rmse: 0.66465 | val_1_rmse: 0.64566 |  0:00:09s
epoch 26 | loss: 0.34889 | val_0_rmse: 0.66953 | val_1_rmse: 0.65019 |  0:00:09s
epoch 27 | loss: 0.35752 | val_0_rmse: 0.68907 | val_1_rmse: 0.65392 |  0:00:09s
epoch 28 | loss: 0.35341 | val_0_rmse: 0.70383 | val_1_rmse: 0.68662 |  0:00:10s
epoch 29 | loss: 0.35252 | val_0_rmse: 0.6707  | val_1_rmse: 0.65208 |  0:00:10s
epoch 30 | loss: 0.33752 | val_0_rmse: 0.67576 | val_1_rmse: 0.65793 |  0:00:10s
epoch 31 | loss: 0.33326 | val_0_rmse: 0.65873 | val_1_rmse: 0.64377 |  0:00:11s
epoch 32 | loss: 0.32941 | val_0_rmse: 0.66182 | val_1_rmse: 0.64427 |  0:00:11s
epoch 33 | loss: 0.33273 | val_0_rmse: 0.6469  | val_1_rmse: 0.63008 |  0:00:11s
epoch 34 | loss: 0.32777 | val_0_rmse: 0.64806 | val_1_rmse: 0.63082 |  0:00:12s
epoch 35 | loss: 0.32736 | val_0_rmse: 0.64523 | val_1_rmse: 0.63079 |  0:00:12s
epoch 36 | loss: 0.32709 | val_0_rmse: 0.64101 | val_1_rmse: 0.62395 |  0:00:13s
epoch 37 | loss: 0.31832 | val_0_rmse: 0.6389  | val_1_rmse: 0.61834 |  0:00:13s
epoch 38 | loss: 0.31444 | val_0_rmse: 0.63145 | val_1_rmse: 0.61348 |  0:00:13s
epoch 39 | loss: 0.31431 | val_0_rmse: 0.64406 | val_1_rmse: 0.62718 |  0:00:14s
epoch 40 | loss: 0.31442 | val_0_rmse: 0.6269  | val_1_rmse: 0.6124  |  0:00:14s
epoch 41 | loss: 0.31916 | val_0_rmse: 0.63141 | val_1_rmse: 0.61196 |  0:00:14s
epoch 42 | loss: 0.32145 | val_0_rmse: 0.62048 | val_1_rmse: 0.59907 |  0:00:15s
epoch 43 | loss: 0.32184 | val_0_rmse: 0.6281  | val_1_rmse: 0.6091  |  0:00:15s
epoch 44 | loss: 0.31502 | val_0_rmse: 0.62657 | val_1_rmse: 0.60708 |  0:00:15s
epoch 45 | loss: 0.3133  | val_0_rmse: 0.63082 | val_1_rmse: 0.61367 |  0:00:16s
epoch 46 | loss: 0.31227 | val_0_rmse: 0.62653 | val_1_rmse: 0.60964 |  0:00:16s
epoch 47 | loss: 0.3107  | val_0_rmse: 0.61427 | val_1_rmse: 0.5957  |  0:00:16s
epoch 48 | loss: 0.30966 | val_0_rmse: 0.61175 | val_1_rmse: 0.5972  |  0:00:17s
epoch 49 | loss: 0.31428 | val_0_rmse: 0.60925 | val_1_rmse: 0.59516 |  0:00:17s
epoch 50 | loss: 0.31563 | val_0_rmse: 0.61391 | val_1_rmse: 0.59861 |  0:00:17s
epoch 51 | loss: 0.30307 | val_0_rmse: 0.61609 | val_1_rmse: 0.60268 |  0:00:18s
epoch 52 | loss: 0.30948 | val_0_rmse: 0.61654 | val_1_rmse: 0.60177 |  0:00:18s
epoch 53 | loss: 0.3058  | val_0_rmse: 0.61036 | val_1_rmse: 0.60252 |  0:00:19s
epoch 54 | loss: 0.30117 | val_0_rmse: 0.60293 | val_1_rmse: 0.59157 |  0:00:19s
epoch 55 | loss: 0.30268 | val_0_rmse: 0.60964 | val_1_rmse: 0.601   |  0:00:19s
epoch 56 | loss: 0.30103 | val_0_rmse: 0.59604 | val_1_rmse: 0.58587 |  0:00:20s
epoch 57 | loss: 0.29636 | val_0_rmse: 0.5995  | val_1_rmse: 0.59602 |  0:00:20s
epoch 58 | loss: 0.29554 | val_0_rmse: 0.60864 | val_1_rmse: 0.60117 |  0:00:20s
epoch 59 | loss: 0.29685 | val_0_rmse: 0.58486 | val_1_rmse: 0.58064 |  0:00:21s
epoch 60 | loss: 0.29205 | val_0_rmse: 0.58721 | val_1_rmse: 0.57156 |  0:00:21s
epoch 61 | loss: 0.29532 | val_0_rmse: 0.59041 | val_1_rmse: 0.58451 |  0:00:21s
epoch 62 | loss: 0.28687 | val_0_rmse: 0.59374 | val_1_rmse: 0.59315 |  0:00:22s
epoch 63 | loss: 0.29959 | val_0_rmse: 0.60436 | val_1_rmse: 0.6007  |  0:00:22s
epoch 64 | loss: 0.29126 | val_0_rmse: 0.60697 | val_1_rmse: 0.59897 |  0:00:22s
epoch 65 | loss: 0.29005 | val_0_rmse: 0.581   | val_1_rmse: 0.57352 |  0:00:23s
epoch 66 | loss: 0.28678 | val_0_rmse: 0.59288 | val_1_rmse: 0.58967 |  0:00:23s
epoch 67 | loss: 0.27897 | val_0_rmse: 0.57793 | val_1_rmse: 0.5758  |  0:00:23s
epoch 68 | loss: 0.28809 | val_0_rmse: 0.59565 | val_1_rmse: 0.59524 |  0:00:24s
epoch 69 | loss: 0.28515 | val_0_rmse: 0.58497 | val_1_rmse: 0.59881 |  0:00:24s
epoch 70 | loss: 0.28973 | val_0_rmse: 0.57619 | val_1_rmse: 0.59046 |  0:00:24s
epoch 71 | loss: 0.28114 | val_0_rmse: 0.58513 | val_1_rmse: 0.58635 |  0:00:25s
epoch 72 | loss: 0.28373 | val_0_rmse: 0.60156 | val_1_rmse: 0.60525 |  0:00:25s
epoch 73 | loss: 0.28431 | val_0_rmse: 0.57158 | val_1_rmse: 0.59917 |  0:00:26s
epoch 74 | loss: 0.27828 | val_0_rmse: 0.56995 | val_1_rmse: 0.59515 |  0:00:26s
epoch 75 | loss: 0.27461 | val_0_rmse: 0.56775 | val_1_rmse: 0.57973 |  0:00:26s
epoch 76 | loss: 0.27561 | val_0_rmse: 0.55988 | val_1_rmse: 0.58032 |  0:00:27s
epoch 77 | loss: 0.2817  | val_0_rmse: 0.55075 | val_1_rmse: 0.57056 |  0:00:27s
epoch 78 | loss: 0.27513 | val_0_rmse: 0.5598  | val_1_rmse: 0.57726 |  0:00:27s
epoch 79 | loss: 0.2701  | val_0_rmse: 0.54898 | val_1_rmse: 0.56041 |  0:00:28s
epoch 80 | loss: 0.27368 | val_0_rmse: 0.54768 | val_1_rmse: 0.56375 |  0:00:28s
epoch 81 | loss: 0.26744 | val_0_rmse: 0.54921 | val_1_rmse: 0.56491 |  0:00:28s
epoch 82 | loss: 0.26853 | val_0_rmse: 0.55167 | val_1_rmse: 0.57009 |  0:00:29s
epoch 83 | loss: 0.26712 | val_0_rmse: 0.55767 | val_1_rmse: 0.57647 |  0:00:29s
epoch 84 | loss: 0.26555 | val_0_rmse: 0.5475  | val_1_rmse: 0.56896 |  0:00:29s
epoch 85 | loss: 0.26506 | val_0_rmse: 0.55595 | val_1_rmse: 0.58085 |  0:00:30s
epoch 86 | loss: 0.2594  | val_0_rmse: 0.54842 | val_1_rmse: 0.56554 |  0:00:30s
epoch 87 | loss: 0.26421 | val_0_rmse: 0.57365 | val_1_rmse: 0.59653 |  0:00:30s
epoch 88 | loss: 0.26319 | val_0_rmse: 0.55596 | val_1_rmse: 0.57881 |  0:00:31s
epoch 89 | loss: 0.26362 | val_0_rmse: 0.54158 | val_1_rmse: 0.56206 |  0:00:31s
epoch 90 | loss: 0.2545  | val_0_rmse: 0.53518 | val_1_rmse: 0.56144 |  0:00:31s
epoch 91 | loss: 0.2603  | val_0_rmse: 0.5347  | val_1_rmse: 0.56323 |  0:00:32s
epoch 92 | loss: 0.25608 | val_0_rmse: 0.53291 | val_1_rmse: 0.5694  |  0:00:32s
epoch 93 | loss: 0.26118 | val_0_rmse: 0.54213 | val_1_rmse: 0.57991 |  0:00:33s
epoch 94 | loss: 0.25958 | val_0_rmse: 0.52837 | val_1_rmse: 0.56527 |  0:00:33s
epoch 95 | loss: 0.26315 | val_0_rmse: 0.53766 | val_1_rmse: 0.55613 |  0:00:33s
epoch 96 | loss: 0.26133 | val_0_rmse: 0.543   | val_1_rmse: 0.56904 |  0:00:34s
epoch 97 | loss: 0.25412 | val_0_rmse: 0.52508 | val_1_rmse: 0.54825 |  0:00:34s
epoch 98 | loss: 0.26181 | val_0_rmse: 0.51677 | val_1_rmse: 0.54554 |  0:00:34s
epoch 99 | loss: 0.25713 | val_0_rmse: 0.51482 | val_1_rmse: 0.55688 |  0:00:35s
epoch 100| loss: 0.25687 | val_0_rmse: 0.51324 | val_1_rmse: 0.55397 |  0:00:35s
epoch 101| loss: 0.25508 | val_0_rmse: 0.52156 | val_1_rmse: 0.56014 |  0:00:35s
epoch 102| loss: 0.24764 | val_0_rmse: 0.50572 | val_1_rmse: 0.55175 |  0:00:36s
epoch 103| loss: 0.25723 | val_0_rmse: 0.50471 | val_1_rmse: 0.55326 |  0:00:36s
epoch 104| loss: 0.25146 | val_0_rmse: 0.50609 | val_1_rmse: 0.56395 |  0:00:36s
epoch 105| loss: 0.25442 | val_0_rmse: 0.507   | val_1_rmse: 0.55134 |  0:00:37s
epoch 106| loss: 0.247   | val_0_rmse: 0.51187 | val_1_rmse: 0.55477 |  0:00:37s
epoch 107| loss: 0.24997 | val_0_rmse: 0.51677 | val_1_rmse: 0.55913 |  0:00:37s
epoch 108| loss: 0.25462 | val_0_rmse: 0.50602 | val_1_rmse: 0.5546  |  0:00:38s
epoch 109| loss: 0.24697 | val_0_rmse: 0.51013 | val_1_rmse: 0.56551 |  0:00:38s
epoch 110| loss: 0.25652 | val_0_rmse: 0.50511 | val_1_rmse: 0.562   |  0:00:38s
epoch 111| loss: 0.24416 | val_0_rmse: 0.50402 | val_1_rmse: 0.55313 |  0:00:39s
epoch 112| loss: 0.24215 | val_0_rmse: 0.49982 | val_1_rmse: 0.56203 |  0:00:39s
epoch 113| loss: 0.24364 | val_0_rmse: 0.49736 | val_1_rmse: 0.55879 |  0:00:40s
epoch 114| loss: 0.2433  | val_0_rmse: 0.49124 | val_1_rmse: 0.55137 |  0:00:40s
epoch 115| loss: 0.24301 | val_0_rmse: 0.51017 | val_1_rmse: 0.55798 |  0:00:40s
epoch 116| loss: 0.23845 | val_0_rmse: 0.49045 | val_1_rmse: 0.55164 |  0:00:41s
epoch 117| loss: 0.24598 | val_0_rmse: 0.48544 | val_1_rmse: 0.54694 |  0:00:41s
epoch 118| loss: 0.23989 | val_0_rmse: 0.49236 | val_1_rmse: 0.55457 |  0:00:41s
epoch 119| loss: 0.23839 | val_0_rmse: 0.49035 | val_1_rmse: 0.55761 |  0:00:42s
epoch 120| loss: 0.24161 | val_0_rmse: 0.49776 | val_1_rmse: 0.56633 |  0:00:42s
epoch 121| loss: 0.24324 | val_0_rmse: 0.50942 | val_1_rmse: 0.56716 |  0:00:42s
epoch 122| loss: 0.2442  | val_0_rmse: 0.49094 | val_1_rmse: 0.56225 |  0:00:43s
epoch 123| loss: 0.23886 | val_0_rmse: 0.49337 | val_1_rmse: 0.56458 |  0:00:43s
epoch 124| loss: 0.24435 | val_0_rmse: 0.49537 | val_1_rmse: 0.56893 |  0:00:43s
epoch 125| loss: 0.23979 | val_0_rmse: 0.48792 | val_1_rmse: 0.56337 |  0:00:44s
epoch 126| loss: 0.24166 | val_0_rmse: 0.47865 | val_1_rmse: 0.56163 |  0:00:44s
epoch 127| loss: 0.24123 | val_0_rmse: 0.48125 | val_1_rmse: 0.57595 |  0:00:44s
epoch 128| loss: 0.24673 | val_0_rmse: 0.48621 | val_1_rmse: 0.56136 |  0:00:45s

Early stopping occured at epoch 128 with best_epoch = 98 and best_val_1_rmse = 0.54554
Best weights from best epoch are automatically used!
ended training at: 23:04:27
Feature importance:
Mean squared error is of 2819411494.785916
Mean absolute error:36008.79296466469
MAPE:0.30607430653267126
R2 score:0.7016245028642447
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:05:27
epoch 0  | loss: 0.47679 | val_0_rmse: 0.62059 | val_1_rmse: 0.616   |  0:00:44s
epoch 1  | loss: 0.2084  | val_0_rmse: 0.4906  | val_1_rmse: 0.49134 |  0:01:28s
epoch 2  | loss: 0.18413 | val_0_rmse: 0.40908 | val_1_rmse: 0.414   |  0:02:12s
epoch 3  | loss: 0.16977 | val_0_rmse: 0.38595 | val_1_rmse: 0.39462 |  0:02:55s
epoch 4  | loss: 0.16419 | val_0_rmse: 0.39142 | val_1_rmse: 0.40393 |  0:03:38s
epoch 5  | loss: 0.15743 | val_0_rmse: 0.42155 | val_1_rmse: 0.41158 |  0:04:21s
epoch 6  | loss: 0.15288 | val_0_rmse: 0.45356 | val_1_rmse: 0.46467 |  0:05:05s
epoch 7  | loss: 0.14961 | val_0_rmse: 0.49373 | val_1_rmse: 0.51051 |  0:05:48s
epoch 8  | loss: 0.14637 | val_0_rmse: 0.42269 | val_1_rmse: 0.43068 |  0:06:31s
epoch 9  | loss: 0.14427 | val_0_rmse: 0.44031 | val_1_rmse: 0.40617 |  0:07:14s
epoch 10 | loss: 0.1415  | val_0_rmse: 0.52386 | val_1_rmse: 0.4635  |  0:07:57s
epoch 11 | loss: 0.13972 | val_0_rmse: 0.49485 | val_1_rmse: 0.39667 |  0:08:40s
epoch 12 | loss: 0.13827 | val_0_rmse: 0.66096 | val_1_rmse: 0.54489 |  0:09:24s
epoch 13 | loss: 0.13751 | val_0_rmse: 0.40694 | val_1_rmse: 0.38534 |  0:10:07s
epoch 14 | loss: 0.1368  | val_0_rmse: 0.39554 | val_1_rmse: 0.40299 |  0:10:50s
epoch 15 | loss: 0.1355  | val_0_rmse: 0.51835 | val_1_rmse: 0.42552 |  0:11:33s
epoch 16 | loss: 0.13677 | val_0_rmse: 0.6326  | val_1_rmse: 0.61743 |  0:12:16s
epoch 17 | loss: 0.15133 | val_0_rmse: 0.37744 | val_1_rmse: 0.39534 |  0:13:00s
epoch 18 | loss: 0.13561 | val_0_rmse: 0.37471 | val_1_rmse: 0.39016 |  0:13:43s
epoch 19 | loss: 0.1342  | val_0_rmse: 0.38292 | val_1_rmse: 0.40002 |  0:14:26s
epoch 20 | loss: 0.13315 | val_0_rmse: 0.37348 | val_1_rmse: 0.39131 |  0:15:09s
epoch 21 | loss: 0.14651 | val_0_rmse: 0.40928 | val_1_rmse: 0.42695 |  0:15:53s
epoch 22 | loss: 0.13516 | val_0_rmse: 0.50321 | val_1_rmse: 0.51537 |  0:16:37s
epoch 23 | loss: 0.13192 | val_0_rmse: 0.40366 | val_1_rmse: 0.42378 |  0:17:21s
epoch 24 | loss: 0.13062 | val_0_rmse: 0.39056 | val_1_rmse: 0.38846 |  0:18:04s
epoch 25 | loss: 0.12996 | val_0_rmse: 0.4159  | val_1_rmse: 0.43621 |  0:18:47s
epoch 26 | loss: 0.12847 | val_0_rmse: 0.47951 | val_1_rmse: 0.46552 |  0:19:30s
epoch 27 | loss: 0.12913 | val_0_rmse: 0.40736 | val_1_rmse: 0.40396 |  0:20:14s
epoch 28 | loss: 0.12702 | val_0_rmse: 0.50902 | val_1_rmse: 0.51377 |  0:20:57s
epoch 29 | loss: 0.12682 | val_0_rmse: 0.43608 | val_1_rmse: 0.4022  |  0:21:40s
epoch 30 | loss: 0.12756 | val_0_rmse: 0.39991 | val_1_rmse: 0.44422 |  0:22:23s
epoch 31 | loss: 0.12752 | val_0_rmse: 0.446   | val_1_rmse: 0.42684 |  0:23:07s
epoch 32 | loss: 0.12617 | val_0_rmse: 0.38768 | val_1_rmse: 0.4358  |  0:23:50s
epoch 33 | loss: 0.12498 | val_0_rmse: 0.41987 | val_1_rmse: 0.44862 |  0:24:33s
epoch 34 | loss: 0.12546 | val_0_rmse: 0.51719 | val_1_rmse: 0.50204 |  0:25:16s
epoch 35 | loss: 0.12433 | val_0_rmse: 0.4456  | val_1_rmse: 0.42552 |  0:25:59s
epoch 36 | loss: 0.12362 | val_0_rmse: 0.3966  | val_1_rmse: 0.43311 |  0:26:42s
epoch 37 | loss: 0.12202 | val_0_rmse: 0.38667 | val_1_rmse: 0.4281  |  0:27:26s
epoch 38 | loss: 0.12253 | val_0_rmse: 0.46482 | val_1_rmse: 0.44272 |  0:28:09s
epoch 39 | loss: 0.12097 | val_0_rmse: 0.47394 | val_1_rmse: 0.43443 |  0:28:52s
epoch 40 | loss: 0.12469 | val_0_rmse: 0.49825 | val_1_rmse: 0.46135 |  0:29:36s
epoch 41 | loss: 0.12148 | val_0_rmse: 0.44392 | val_1_rmse: 0.42144 |  0:30:19s
epoch 42 | loss: 0.12103 | val_0_rmse: 0.66494 | val_1_rmse: 1.39829 |  0:31:02s
epoch 43 | loss: 0.12003 | val_0_rmse: 0.78906 | val_1_rmse: 1.99287 |  0:31:46s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.38534
Best weights from best epoch are automatically used!
ended training at: 23:37:39
Feature importance:
Mean squared error is of 6804289699.217355
Mean absolute error:52123.196703570226
MAPE:0.28690965222350545
R2 score:0.846066923368499
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:38:03
epoch 0  | loss: 0.54443 | val_0_rmse: 0.63621 | val_1_rmse: 0.63214 |  0:00:43s
epoch 1  | loss: 0.24997 | val_0_rmse: 0.5312  | val_1_rmse: 0.53157 |  0:01:27s
epoch 2  | loss: 0.20234 | val_0_rmse: 0.45578 | val_1_rmse: 0.45713 |  0:02:10s
epoch 3  | loss: 0.18857 | val_0_rmse: 0.39195 | val_1_rmse: 0.40258 |  0:02:54s
epoch 4  | loss: 0.17417 | val_0_rmse: 0.41739 | val_1_rmse: 0.42668 |  0:03:37s
epoch 5  | loss: 0.1797  | val_0_rmse: 0.41856 | val_1_rmse: 0.42978 |  0:04:21s
epoch 6  | loss: 0.17789 | val_0_rmse: 0.41591 | val_1_rmse: 0.42884 |  0:05:04s
epoch 7  | loss: 0.18842 | val_0_rmse: 0.44843 | val_1_rmse: 0.45636 |  0:05:48s
epoch 8  | loss: 0.1904  | val_0_rmse: 0.40768 | val_1_rmse: 0.42294 |  0:06:31s
epoch 9  | loss: 0.17694 | val_0_rmse: 0.46191 | val_1_rmse: 0.47461 |  0:07:15s
epoch 10 | loss: 0.17146 | val_0_rmse: 0.45901 | val_1_rmse: 0.47318 |  0:07:58s
epoch 11 | loss: 0.16535 | val_0_rmse: 0.41706 | val_1_rmse: 0.43336 |  0:08:42s
epoch 12 | loss: 0.16438 | val_0_rmse: 0.40847 | val_1_rmse: 0.42517 |  0:09:25s
epoch 13 | loss: 0.15639 | val_0_rmse: 0.48573 | val_1_rmse: 0.50104 |  0:10:09s
epoch 14 | loss: 0.1498  | val_0_rmse: 0.40253 | val_1_rmse: 0.41934 |  0:10:53s
epoch 15 | loss: 0.14973 | val_0_rmse: 0.40023 | val_1_rmse: 0.4228  |  0:11:36s
epoch 16 | loss: 0.15176 | val_0_rmse: 0.52533 | val_1_rmse: 0.54174 |  0:12:20s
epoch 17 | loss: 0.18093 | val_0_rmse: 0.41898 | val_1_rmse: 0.43506 |  0:13:03s
epoch 18 | loss: 0.16258 | val_0_rmse: 0.40903 | val_1_rmse: 0.42828 |  0:13:47s
epoch 19 | loss: 0.15823 | val_0_rmse: 0.49312 | val_1_rmse: 0.51127 |  0:14:32s
epoch 20 | loss: 0.17026 | val_0_rmse: 0.41349 | val_1_rmse: 0.43104 |  0:15:15s
epoch 21 | loss: 0.17142 | val_0_rmse: 0.4473  | val_1_rmse: 0.45935 |  0:15:58s
epoch 22 | loss: 0.15695 | val_0_rmse: 0.40769 | val_1_rmse: 0.42231 |  0:16:41s
epoch 23 | loss: 0.15526 | val_0_rmse: 0.41543 | val_1_rmse: 0.43757 |  0:17:24s
epoch 24 | loss: 0.15845 | val_0_rmse: 0.46524 | val_1_rmse: 0.47611 |  0:18:07s
epoch 25 | loss: 0.15164 | val_0_rmse: 0.42791 | val_1_rmse: 0.44487 |  0:18:50s
epoch 26 | loss: 0.16307 | val_0_rmse: 0.41358 | val_1_rmse: 0.42926 |  0:19:33s
epoch 27 | loss: 0.15866 | val_0_rmse: 0.39548 | val_1_rmse: 0.4154  |  0:20:16s
epoch 28 | loss: 0.14751 | val_0_rmse: 0.41176 | val_1_rmse: 0.42875 |  0:21:00s
epoch 29 | loss: 0.18011 | val_0_rmse: 0.45767 | val_1_rmse: 0.47536 |  0:21:43s
epoch 30 | loss: 0.17607 | val_0_rmse: 0.46414 | val_1_rmse: 0.47813 |  0:22:26s
epoch 31 | loss: 0.15488 | val_0_rmse: 0.44046 | val_1_rmse: 0.45695 |  0:23:09s
epoch 32 | loss: 0.14689 | val_0_rmse: 0.38661 | val_1_rmse: 0.40821 |  0:23:52s
epoch 33 | loss: 0.14601 | val_0_rmse: 0.43635 | val_1_rmse: 0.4565  |  0:24:35s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.40258
Best weights from best epoch are automatically used!
ended training at: 00:03:03
Feature importance:
Mean squared error is of 7032970515.424086
Mean absolute error:56231.0244450859
MAPE:0.33725319238923435
R2 score:0.8415622827080963
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:03:27
epoch 0  | loss: 0.45698 | val_0_rmse: 0.65729 | val_1_rmse: 0.65915 |  0:00:44s
epoch 1  | loss: 0.20836 | val_0_rmse: 0.51723 | val_1_rmse: 0.51917 |  0:01:27s
epoch 2  | loss: 0.18435 | val_0_rmse: 0.46718 | val_1_rmse: 0.47086 |  0:02:11s
epoch 3  | loss: 0.17137 | val_0_rmse: 0.38316 | val_1_rmse: 0.38964 |  0:02:55s
epoch 4  | loss: 0.16469 | val_0_rmse: 0.41171 | val_1_rmse: 0.41442 |  0:03:39s
epoch 5  | loss: 0.1603  | val_0_rmse: 0.61618 | val_1_rmse: 0.6282  |  0:04:23s
epoch 6  | loss: 0.15646 | val_0_rmse: 0.4275  | val_1_rmse: 0.43571 |  0:05:07s
epoch 7  | loss: 0.15325 | val_0_rmse: 0.42725 | val_1_rmse: 0.42677 |  0:05:50s
epoch 8  | loss: 0.15104 | val_0_rmse: 0.38423 | val_1_rmse: 0.40271 |  0:06:34s
epoch 9  | loss: 0.14797 | val_0_rmse: 0.79588 | val_1_rmse: 0.79724 |  0:07:17s
epoch 10 | loss: 0.14536 | val_0_rmse: 0.83155 | val_1_rmse: 0.83267 |  0:08:01s
epoch 11 | loss: 0.14569 | val_0_rmse: 0.83557 | val_1_rmse: 0.83403 |  0:08:46s
epoch 12 | loss: 0.14303 | val_0_rmse: 0.41562 | val_1_rmse: 0.43178 |  0:09:29s
epoch 13 | loss: 0.1408  | val_0_rmse: 0.39839 | val_1_rmse: 0.41409 |  0:10:13s
epoch 14 | loss: 0.13541 | val_0_rmse: 0.37518 | val_1_rmse: 0.39502 |  0:10:56s
epoch 15 | loss: 0.13621 | val_0_rmse: 0.37445 | val_1_rmse: 0.40169 |  0:11:40s
epoch 16 | loss: 0.13548 | val_0_rmse: 0.53676 | val_1_rmse: 0.55399 |  0:12:23s
epoch 17 | loss: 0.13277 | val_0_rmse: 0.3791  | val_1_rmse: 0.40364 |  0:13:07s
epoch 18 | loss: 0.13372 | val_0_rmse: 0.38917 | val_1_rmse: 0.41842 |  0:13:50s
epoch 19 | loss: 0.1318  | val_0_rmse: 0.56828 | val_1_rmse: 0.56153 |  0:14:34s
epoch 20 | loss: 0.13129 | val_0_rmse: 0.38237 | val_1_rmse: 0.40466 |  0:15:17s
epoch 21 | loss: 0.13036 | val_0_rmse: 0.36847 | val_1_rmse: 0.39055 |  0:16:01s
epoch 22 | loss: 0.12882 | val_0_rmse: 0.36532 | val_1_rmse: 0.39642 |  0:16:44s
epoch 23 | loss: 0.12855 | val_0_rmse: 0.36609 | val_1_rmse: 0.39446 |  0:17:28s
epoch 24 | loss: 0.12951 | val_0_rmse: 0.36611 | val_1_rmse: 0.39246 |  0:18:12s
epoch 25 | loss: 0.12782 | val_0_rmse: 0.36927 | val_1_rmse: 0.39576 |  0:18:55s
epoch 26 | loss: 0.12657 | val_0_rmse: 0.35968 | val_1_rmse: 0.38325 |  0:19:40s
epoch 27 | loss: 0.12707 | val_0_rmse: 0.7704  | val_1_rmse: 0.87072 |  0:20:23s
epoch 28 | loss: 0.17139 | val_0_rmse: 0.65526 | val_1_rmse: 0.64663 |  0:21:07s
epoch 29 | loss: 0.13743 | val_0_rmse: 0.39767 | val_1_rmse: 0.4004  |  0:21:50s
epoch 30 | loss: 0.13322 | val_0_rmse: 0.40721 | val_1_rmse: 0.42433 |  0:22:33s
epoch 31 | loss: 0.1308  | val_0_rmse: 0.37833 | val_1_rmse: 0.39648 |  0:23:17s
epoch 32 | loss: 0.12778 | val_0_rmse: 0.37229 | val_1_rmse: 0.39003 |  0:24:00s
epoch 33 | loss: 0.12667 | val_0_rmse: 0.45998 | val_1_rmse: 0.47979 |  0:24:44s
epoch 34 | loss: 0.12521 | val_0_rmse: 0.39635 | val_1_rmse: 0.41523 |  0:25:27s
epoch 35 | loss: 0.12458 | val_0_rmse: 0.37737 | val_1_rmse: 0.40053 |  0:26:10s
epoch 36 | loss: 0.12288 | val_0_rmse: 0.36389 | val_1_rmse: 0.3901  |  0:26:54s
epoch 37 | loss: 0.12363 | val_0_rmse: 0.40315 | val_1_rmse: 0.40566 |  0:27:37s
epoch 38 | loss: 0.12304 | val_0_rmse: 0.46833 | val_1_rmse: 0.478   |  0:28:20s
epoch 39 | loss: 0.1249  | val_0_rmse: 0.75013 | val_1_rmse: 0.49486 |  0:29:04s
epoch 40 | loss: 0.13605 | val_0_rmse: 1.00988 | val_1_rmse: 0.59685 |  0:29:47s
epoch 41 | loss: 0.12398 | val_0_rmse: 0.99572 | val_1_rmse: 0.81121 |  0:30:31s
epoch 42 | loss: 0.12279 | val_0_rmse: 1.08937 | val_1_rmse: 0.94067 |  0:31:14s
epoch 43 | loss: 0.12134 | val_0_rmse: 1.12018 | val_1_rmse: 0.39358 |  0:31:57s
epoch 44 | loss: 0.11954 | val_0_rmse: 1.81163 | val_1_rmse: 1.16068 |  0:32:41s
epoch 45 | loss: 0.1195  | val_0_rmse: 2.08287 | val_1_rmse: 1.59258 |  0:33:24s
epoch 46 | loss: 0.11954 | val_0_rmse: 1.69342 | val_1_rmse: 0.39622 |  0:34:07s
epoch 47 | loss: 0.12104 | val_0_rmse: 1.41791 | val_1_rmse: 0.39072 |  0:34:52s
epoch 48 | loss: 0.12043 | val_0_rmse: 1.54244 | val_1_rmse: 0.40852 |  0:35:35s
epoch 49 | loss: 0.12098 | val_0_rmse: 1.99827 | val_1_rmse: 0.67319 |  0:36:19s
epoch 50 | loss: 0.11897 | val_0_rmse: 1.88523 | val_1_rmse: 0.39858 |  0:37:02s
epoch 51 | loss: 0.11802 | val_0_rmse: 1.41801 | val_1_rmse: 0.4402  |  0:37:45s
epoch 52 | loss: 0.11866 | val_0_rmse: 0.42859 | val_1_rmse: 0.39539 |  0:38:29s
epoch 53 | loss: 0.11711 | val_0_rmse: 1.39529 | val_1_rmse: 0.40973 |  0:39:12s
epoch 54 | loss: 0.11781 | val_0_rmse: 1.62732 | val_1_rmse: 0.4043  |  0:39:55s
epoch 55 | loss: 0.11774 | val_0_rmse: 1.76107 | val_1_rmse: 1.35795 |  0:40:39s
epoch 56 | loss: 0.11801 | val_0_rmse: 1.64706 | val_1_rmse: 0.42014 |  0:41:23s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.38325
Best weights from best epoch are automatically used!
ended training at: 00:45:20
Feature importance:
Mean squared error is of 6556747584.835209
Mean absolute error:51641.35187330662
MAPE:0.2988049150894174
R2 score:0.8506218179889681
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:45:40
epoch 0  | loss: 0.465   | val_0_rmse: 0.65417 | val_1_rmse: 0.66299 |  0:00:43s
epoch 1  | loss: 0.21562 | val_0_rmse: 0.52687 | val_1_rmse: 0.53365 |  0:01:26s
epoch 2  | loss: 0.18773 | val_0_rmse: 0.45275 | val_1_rmse: 0.461   |  0:02:10s
epoch 3  | loss: 0.18048 | val_0_rmse: 0.41458 | val_1_rmse: 0.42953 |  0:02:53s
epoch 4  | loss: 0.17265 | val_0_rmse: 0.39428 | val_1_rmse: 0.41116 |  0:03:36s
epoch 5  | loss: 0.16652 | val_0_rmse: 0.39602 | val_1_rmse: 0.42322 |  0:04:20s
epoch 6  | loss: 0.16444 | val_0_rmse: 0.52306 | val_1_rmse: 0.53735 |  0:05:03s
epoch 7  | loss: 0.16035 | val_0_rmse: 0.44004 | val_1_rmse: 0.45956 |  0:05:47s
epoch 8  | loss: 0.1586  | val_0_rmse: 0.67285 | val_1_rmse: 0.69024 |  0:06:30s
epoch 9  | loss: 0.16338 | val_0_rmse: 0.59704 | val_1_rmse: 0.61604 |  0:07:14s
epoch 10 | loss: 0.1543  | val_0_rmse: 0.39302 | val_1_rmse: 0.42072 |  0:07:58s
epoch 11 | loss: 0.15094 | val_0_rmse: 0.39307 | val_1_rmse: 0.41949 |  0:08:42s
epoch 12 | loss: 0.14933 | val_0_rmse: 0.45149 | val_1_rmse: 0.46642 |  0:09:25s
epoch 13 | loss: 0.14514 | val_0_rmse: 0.7797  | val_1_rmse: 0.79873 |  0:10:08s
epoch 14 | loss: 0.14501 | val_0_rmse: 0.46619 | val_1_rmse: 0.49277 |  0:10:52s
epoch 15 | loss: 0.14658 | val_0_rmse: 0.51555 | val_1_rmse: 0.44524 |  0:11:35s
epoch 16 | loss: 0.14723 | val_0_rmse: 0.38627 | val_1_rmse: 0.40656 |  0:12:19s
epoch 17 | loss: 0.14875 | val_0_rmse: 0.37737 | val_1_rmse: 0.40186 |  0:13:03s
epoch 18 | loss: 0.14339 | val_0_rmse: 0.38915 | val_1_rmse: 0.41459 |  0:13:47s
epoch 19 | loss: 0.14026 | val_0_rmse: 0.38136 | val_1_rmse: 0.40569 |  0:14:30s
epoch 20 | loss: 0.14165 | val_0_rmse: 0.37508 | val_1_rmse: 0.39817 |  0:15:14s
epoch 21 | loss: 0.13699 | val_0_rmse: 0.40913 | val_1_rmse: 0.43546 |  0:15:57s
epoch 22 | loss: 0.13634 | val_0_rmse: 0.37979 | val_1_rmse: 0.40546 |  0:16:40s
epoch 23 | loss: 0.13622 | val_0_rmse: 0.48151 | val_1_rmse: 0.50509 |  0:17:24s
epoch 24 | loss: 0.13679 | val_0_rmse: 0.39627 | val_1_rmse: 0.4177  |  0:18:07s
epoch 25 | loss: 0.1331  | val_0_rmse: 0.40424 | val_1_rmse: 0.42878 |  0:18:51s
epoch 26 | loss: 0.13104 | val_0_rmse: 0.39581 | val_1_rmse: 0.42087 |  0:19:34s
epoch 27 | loss: 0.13207 | val_0_rmse: 0.38633 | val_1_rmse: 0.41738 |  0:20:17s
epoch 28 | loss: 0.13088 | val_0_rmse: 0.39411 | val_1_rmse: 0.43463 |  0:21:01s
epoch 29 | loss: 0.13092 | val_0_rmse: 0.36945 | val_1_rmse: 0.39773 |  0:21:44s
epoch 30 | loss: 0.12902 | val_0_rmse: 0.50331 | val_1_rmse: 0.53185 |  0:22:28s
epoch 31 | loss: 0.14555 | val_0_rmse: 2.19308 | val_1_rmse: 2.75602 |  0:23:12s
epoch 32 | loss: 0.13989 | val_0_rmse: 0.40676 | val_1_rmse: 0.43229 |  0:23:56s
epoch 33 | loss: 0.13241 | val_0_rmse: 0.41346 | val_1_rmse: 0.43732 |  0:24:39s
epoch 34 | loss: 0.12776 | val_0_rmse: 0.37057 | val_1_rmse: 0.39998 |  0:25:22s
epoch 35 | loss: 0.12579 | val_0_rmse: 0.36814 | val_1_rmse: 0.39914 |  0:26:06s
epoch 36 | loss: 0.12547 | val_0_rmse: 0.37953 | val_1_rmse: 0.40868 |  0:26:49s
epoch 37 | loss: 0.12426 | val_0_rmse: 0.37674 | val_1_rmse: 0.40597 |  0:27:32s
epoch 38 | loss: 0.12521 | val_0_rmse: 0.36598 | val_1_rmse: 0.3971  |  0:28:16s
epoch 39 | loss: 0.12391 | val_0_rmse: 0.38746 | val_1_rmse: 0.41801 |  0:28:59s
epoch 40 | loss: 0.12799 | val_0_rmse: 0.45002 | val_1_rmse: 0.47528 |  0:29:42s
epoch 41 | loss: 0.12379 | val_0_rmse: 0.36259 | val_1_rmse: 0.39385 |  0:30:26s
epoch 42 | loss: 0.12383 | val_0_rmse: 0.40553 | val_1_rmse: 0.43659 |  0:31:09s
epoch 43 | loss: 0.12251 | val_0_rmse: 0.36066 | val_1_rmse: 0.3898  |  0:31:53s
epoch 44 | loss: 0.12231 | val_0_rmse: 0.38191 | val_1_rmse: 0.40614 |  0:32:36s
epoch 45 | loss: 0.12184 | val_0_rmse: 0.36652 | val_1_rmse: 0.40014 |  0:33:19s
epoch 46 | loss: 0.1216  | val_0_rmse: 0.3955  | val_1_rmse: 0.4289  |  0:34:03s
epoch 47 | loss: 0.12302 | val_0_rmse: 0.38627 | val_1_rmse: 0.4067  |  0:34:46s
epoch 48 | loss: 0.12166 | val_0_rmse: 0.38488 | val_1_rmse: 0.42228 |  0:35:29s
epoch 49 | loss: 0.12175 | val_0_rmse: 0.47144 | val_1_rmse: 0.44055 |  0:36:12s
epoch 50 | loss: 0.12039 | val_0_rmse: 0.41055 | val_1_rmse: 0.40357 |  0:36:56s
epoch 51 | loss: 0.11941 | val_0_rmse: 0.44351 | val_1_rmse: 0.40073 |  0:37:39s
epoch 52 | loss: 0.11946 | val_0_rmse: 0.40945 | val_1_rmse: 0.40289 |  0:38:23s
epoch 53 | loss: 0.12022 | val_0_rmse: 0.36537 | val_1_rmse: 0.40368 |  0:39:07s
epoch 54 | loss: 0.12027 | val_0_rmse: 0.5202  | val_1_rmse: 0.40085 |  0:39:50s
epoch 55 | loss: 0.11954 | val_0_rmse: 0.51758 | val_1_rmse: 0.46365 |  0:40:34s
epoch 56 | loss: 0.1209  | val_0_rmse: 1.30174 | val_1_rmse: 1.18253 |  0:41:17s
epoch 57 | loss: 0.14584 | val_0_rmse: 1.03123 | val_1_rmse: 1.18654 |  0:42:00s
epoch 58 | loss: 0.15413 | val_0_rmse: 10.59282| val_1_rmse: 12.25144|  0:42:43s
epoch 59 | loss: 0.1328  | val_0_rmse: 1.13647 | val_1_rmse: 2.00357 |  0:43:27s
epoch 60 | loss: 0.13223 | val_0_rmse: 0.43955 | val_1_rmse: 0.49574 |  0:44:10s
epoch 61 | loss: 0.1272  | val_0_rmse: 0.41517 | val_1_rmse: 0.46875 |  0:44:53s
epoch 62 | loss: 0.1231  | val_0_rmse: 0.48371 | val_1_rmse: 0.54238 |  0:45:37s
epoch 63 | loss: 0.1222  | val_0_rmse: 0.53328 | val_1_rmse: 0.60337 |  0:46:20s
epoch 64 | loss: 0.11983 | val_0_rmse: 0.55867 | val_1_rmse: 0.61308 |  0:47:03s
epoch 65 | loss: 0.1197  | val_0_rmse: 0.67635 | val_1_rmse: 0.73454 |  0:47:46s
epoch 66 | loss: 0.11844 | val_0_rmse: 0.59239 | val_1_rmse: 0.67076 |  0:48:30s
epoch 67 | loss: 0.11826 | val_0_rmse: 0.54219 | val_1_rmse: 0.61276 |  0:49:13s
epoch 68 | loss: 0.11746 | val_0_rmse: 0.60126 | val_1_rmse: 0.6792  |  0:49:56s
epoch 69 | loss: 0.11851 | val_0_rmse: 0.91377 | val_1_rmse: 1.05726 |  0:50:39s
epoch 70 | loss: 0.11841 | val_0_rmse: 0.64834 | val_1_rmse: 0.69952 |  0:51:23s
epoch 71 | loss: 0.11646 | val_0_rmse: 0.59731 | val_1_rmse: 0.70765 |  0:52:06s
epoch 72 | loss: 0.11678 | val_0_rmse: 0.49235 | val_1_rmse: 0.57997 |  0:52:50s
epoch 73 | loss: 0.11665 | val_0_rmse: 0.72139 | val_1_rmse: 0.76188 |  0:53:34s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.3898
Best weights from best epoch are automatically used!
ended training at: 01:39:39
Feature importance:
Mean squared error is of 6446022230.29365
Mean absolute error:51027.222489332285
MAPE:0.3011827067596303
R2 score:0.851085200886477
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:40:42
epoch 0  | loss: 0.47226 | val_0_rmse: 0.65149 | val_1_rmse: 0.65381 |  0:00:43s
epoch 1  | loss: 0.23478 | val_0_rmse: 0.52061 | val_1_rmse: 0.52512 |  0:01:27s
epoch 2  | loss: 0.2104  | val_0_rmse: 0.43373 | val_1_rmse: 0.44072 |  0:02:10s
epoch 3  | loss: 0.19619 | val_0_rmse: 0.41037 | val_1_rmse: 0.41591 |  0:02:54s
epoch 4  | loss: 0.18464 | val_0_rmse: 0.41015 | val_1_rmse: 0.41885 |  0:03:37s
epoch 5  | loss: 0.18845 | val_0_rmse: 0.42779 | val_1_rmse: 0.43713 |  0:04:21s
epoch 6  | loss: 0.17861 | val_0_rmse: 0.42871 | val_1_rmse: 0.43833 |  0:05:04s
epoch 7  | loss: 0.16985 | val_0_rmse: 0.4025  | val_1_rmse: 0.4149  |  0:05:48s
epoch 8  | loss: 0.16661 | val_0_rmse: 0.59323 | val_1_rmse: 0.51247 |  0:06:31s
epoch 9  | loss: 0.16099 | val_0_rmse: 0.45403 | val_1_rmse: 0.5306  |  0:07:15s
epoch 10 | loss: 0.17045 | val_0_rmse: 0.42531 | val_1_rmse: 0.43811 |  0:07:59s
epoch 11 | loss: 0.16498 | val_0_rmse: 0.51489 | val_1_rmse: 0.52333 |  0:08:42s
epoch 12 | loss: 0.16211 | val_0_rmse: 0.47928 | val_1_rmse: 0.48781 |  0:09:26s
epoch 13 | loss: 0.16282 | val_0_rmse: 0.39783 | val_1_rmse: 0.41085 |  0:10:10s
epoch 14 | loss: 0.14827 | val_0_rmse: 0.39142 | val_1_rmse: 0.40539 |  0:10:53s
epoch 15 | loss: 0.1428  | val_0_rmse: 0.75603 | val_1_rmse: 0.76354 |  0:11:37s
epoch 16 | loss: 0.14604 | val_0_rmse: 0.38449 | val_1_rmse: 0.403   |  0:12:21s
epoch 17 | loss: 0.14211 | val_0_rmse: 0.38116 | val_1_rmse: 0.39694 |  0:13:06s
epoch 18 | loss: 0.1385  | val_0_rmse: 0.40405 | val_1_rmse: 0.42306 |  0:13:50s
epoch 19 | loss: 0.13902 | val_0_rmse: 0.38352 | val_1_rmse: 0.40279 |  0:14:33s
epoch 20 | loss: 0.13608 | val_0_rmse: 0.4004  | val_1_rmse: 0.41944 |  0:15:17s
epoch 21 | loss: 0.13306 | val_0_rmse: 0.43742 | val_1_rmse: 0.45416 |  0:16:00s
epoch 22 | loss: 0.13331 | val_0_rmse: 0.38033 | val_1_rmse: 0.40212 |  0:16:44s
epoch 23 | loss: 0.1326  | val_0_rmse: 0.38235 | val_1_rmse: 0.40715 |  0:17:27s
epoch 24 | loss: 0.13204 | val_0_rmse: 0.3802  | val_1_rmse: 0.40396 |  0:18:11s
epoch 25 | loss: 0.1337  | val_0_rmse: 0.4021  | val_1_rmse: 0.42111 |  0:18:55s
epoch 26 | loss: 0.13145 | val_0_rmse: 0.40322 | val_1_rmse: 0.42571 |  0:19:38s
epoch 27 | loss: 0.13185 | val_0_rmse: 0.40574 | val_1_rmse: 0.42724 |  0:20:22s
epoch 28 | loss: 0.13149 | val_0_rmse: 0.38831 | val_1_rmse: 0.40998 |  0:21:06s
epoch 29 | loss: 0.13089 | val_0_rmse: 0.41315 | val_1_rmse: 0.43877 |  0:21:49s
epoch 30 | loss: 0.13834 | val_0_rmse: 0.38655 | val_1_rmse: 0.40572 |  0:22:33s
epoch 31 | loss: 0.13254 | val_0_rmse: 0.38675 | val_1_rmse: 0.41007 |  0:23:17s
epoch 32 | loss: 0.13102 | val_0_rmse: 0.38747 | val_1_rmse: 0.41543 |  0:24:01s
epoch 33 | loss: 0.12881 | val_0_rmse: 0.38258 | val_1_rmse: 0.41329 |  0:24:44s
epoch 34 | loss: 0.12817 | val_0_rmse: 0.37432 | val_1_rmse: 0.40165 |  0:25:28s
epoch 35 | loss: 0.12648 | val_0_rmse: 0.39407 | val_1_rmse: 0.41586 |  0:26:12s
epoch 36 | loss: 0.12562 | val_0_rmse: 0.36651 | val_1_rmse: 0.39674 |  0:26:56s
epoch 37 | loss: 0.12741 | val_0_rmse: 0.36935 | val_1_rmse: 0.3968  |  0:27:39s
epoch 38 | loss: 0.12516 | val_0_rmse: 0.38694 | val_1_rmse: 0.41489 |  0:28:24s
epoch 39 | loss: 0.12836 | val_0_rmse: 0.38887 | val_1_rmse: 0.41237 |  0:29:08s
epoch 40 | loss: 0.12838 | val_0_rmse: 0.66939 | val_1_rmse: 0.67849 |  0:29:52s
epoch 41 | loss: 0.12481 | val_0_rmse: 0.37825 | val_1_rmse: 0.4083  |  0:30:35s
epoch 42 | loss: 0.12638 | val_0_rmse: 0.36872 | val_1_rmse: 0.3972  |  0:31:19s
epoch 43 | loss: 0.12992 | val_0_rmse: 0.38693 | val_1_rmse: 0.4132  |  0:32:03s
epoch 44 | loss: 0.12859 | val_0_rmse: 0.45381 | val_1_rmse: 0.48042 |  0:32:47s
epoch 45 | loss: 0.12595 | val_0_rmse: 0.37059 | val_1_rmse: 0.40284 |  0:33:31s
epoch 46 | loss: 0.12298 | val_0_rmse: 0.36725 | val_1_rmse: 0.40322 |  0:34:14s
epoch 47 | loss: 0.12385 | val_0_rmse: 0.36876 | val_1_rmse: 0.40524 |  0:34:57s
epoch 48 | loss: 0.1224  | val_0_rmse: 0.3718  | val_1_rmse: 0.4074  |  0:35:41s
epoch 49 | loss: 0.13913 | val_0_rmse: 0.53001 | val_1_rmse: 0.53075 |  0:36:24s
epoch 50 | loss: 0.15536 | val_0_rmse: 0.41953 | val_1_rmse: 0.44243 |  0:37:08s
epoch 51 | loss: 0.12839 | val_0_rmse: 0.3951  | val_1_rmse: 0.42261 |  0:37:51s
epoch 52 | loss: 0.12627 | val_0_rmse: 0.40023 | val_1_rmse: 0.43604 |  0:38:34s
epoch 53 | loss: 0.12565 | val_0_rmse: 0.42373 | val_1_rmse: 0.45394 |  0:39:17s
epoch 54 | loss: 0.12639 | val_0_rmse: 0.37859 | val_1_rmse: 0.40792 |  0:40:01s
epoch 55 | loss: 0.12351 | val_0_rmse: 0.41589 | val_1_rmse: 0.45041 |  0:40:44s
epoch 56 | loss: 0.12235 | val_0_rmse: 0.3984  | val_1_rmse: 0.45222 |  0:41:27s
epoch 57 | loss: 0.12147 | val_0_rmse: 0.40375 | val_1_rmse: 0.51981 |  0:42:11s
epoch 58 | loss: 0.12077 | val_0_rmse: 0.37819 | val_1_rmse: 0.43748 |  0:42:54s
epoch 59 | loss: 0.12197 | val_0_rmse: 0.39858 | val_1_rmse: 0.42653 |  0:43:38s
epoch 60 | loss: 0.12119 | val_0_rmse: 0.38526 | val_1_rmse: 0.41956 |  0:44:22s
epoch 61 | loss: 0.11966 | val_0_rmse: 0.37266 | val_1_rmse: 0.41629 |  0:45:05s
epoch 62 | loss: 0.12112 | val_0_rmse: 0.37136 | val_1_rmse: 0.41802 |  0:45:48s
epoch 63 | loss: 0.11863 | val_0_rmse: 0.44715 | val_1_rmse: 0.48722 |  0:46:32s
epoch 64 | loss: 0.11867 | val_0_rmse: 0.40524 | val_1_rmse: 0.44369 |  0:47:15s
epoch 65 | loss: 0.11858 | val_0_rmse: 0.44559 | val_1_rmse: 0.48814 |  0:47:58s
epoch 66 | loss: 0.11759 | val_0_rmse: 0.43286 | val_1_rmse: 0.47415 |  0:48:42s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.39674
Best weights from best epoch are automatically used!
ended training at: 02:29:48
Feature importance:
Mean squared error is of 6976020972.245344
Mean absolute error:51794.217302125006
MAPE:0.2882248770675038
R2 score:0.8439530720144589
------------------------------------------------------------------
