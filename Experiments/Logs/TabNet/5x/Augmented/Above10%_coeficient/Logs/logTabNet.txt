TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:51:34
epoch 0  | loss: 1.44164 | val_0_rmse: 1.01199 | val_1_rmse: 1.01432 |  0:00:05s
epoch 1  | loss: 1.03531 | val_0_rmse: 1.01228 | val_1_rmse: 1.0143  |  0:00:07s
epoch 2  | loss: 1.028   | val_0_rmse: 1.01237 | val_1_rmse: 1.01488 |  0:00:09s
epoch 3  | loss: 1.0309  | val_0_rmse: 1.01226 | val_1_rmse: 1.01438 |  0:00:11s
epoch 4  | loss: 1.02617 | val_0_rmse: 1.01225 | val_1_rmse: 1.01424 |  0:00:12s
epoch 5  | loss: 1.02411 | val_0_rmse: 1.00864 | val_1_rmse: 1.01009 |  0:00:14s
epoch 6  | loss: 1.0225  | val_0_rmse: 0.99752 | val_1_rmse: 0.99759 |  0:00:16s
epoch 7  | loss: 1.0138  | val_0_rmse: 0.99057 | val_1_rmse: 0.99014 |  0:00:18s
epoch 8  | loss: 0.99968 | val_0_rmse: 0.98794 | val_1_rmse: 0.98999 |  0:00:20s
epoch 9  | loss: 0.97901 | val_0_rmse: 0.95115 | val_1_rmse: 0.95797 |  0:00:22s
epoch 10 | loss: 0.90363 | val_0_rmse: 0.93934 | val_1_rmse: 0.94738 |  0:00:24s
epoch 11 | loss: 0.83747 | val_0_rmse: 0.91287 | val_1_rmse: 0.91958 |  0:00:25s
epoch 12 | loss: 0.79336 | val_0_rmse: 0.88761 | val_1_rmse: 0.89762 |  0:00:27s
epoch 13 | loss: 0.74885 | val_0_rmse: 0.88156 | val_1_rmse: 0.88856 |  0:00:29s
epoch 14 | loss: 0.73772 | val_0_rmse: 0.86735 | val_1_rmse: 0.87745 |  0:00:31s
epoch 15 | loss: 0.72073 | val_0_rmse: 0.86091 | val_1_rmse: 0.8755  |  0:00:33s
epoch 16 | loss: 0.70248 | val_0_rmse: 0.85051 | val_1_rmse: 0.87066 |  0:00:35s
epoch 17 | loss: 0.67555 | val_0_rmse: 0.84184 | val_1_rmse: 0.84924 |  0:00:36s
epoch 18 | loss: 0.66941 | val_0_rmse: 0.87821 | val_1_rmse: 0.89942 |  0:00:38s
epoch 19 | loss: 0.69268 | val_0_rmse: 0.83045 | val_1_rmse: 0.84509 |  0:00:40s
epoch 20 | loss: 0.66421 | val_0_rmse: 0.96888 | val_1_rmse: 1.00219 |  0:00:42s
epoch 21 | loss: 0.66259 | val_0_rmse: 0.80109 | val_1_rmse: 0.82009 |  0:00:44s
epoch 22 | loss: 0.64354 | val_0_rmse: 0.89147 | val_1_rmse: 0.89712 |  0:00:46s
epoch 23 | loss: 0.63505 | val_0_rmse: 0.77568 | val_1_rmse: 0.80119 |  0:00:47s
epoch 24 | loss: 0.60345 | val_0_rmse: 0.79774 | val_1_rmse: 0.82283 |  0:00:49s
epoch 25 | loss: 0.61135 | val_0_rmse: 0.78004 | val_1_rmse: 0.80544 |  0:00:51s
epoch 26 | loss: 0.60893 | val_0_rmse: 0.78343 | val_1_rmse: 0.79297 |  0:00:53s
epoch 27 | loss: 0.64449 | val_0_rmse: 0.86392 | val_1_rmse: 0.89447 |  0:00:55s
epoch 28 | loss: 0.61392 | val_0_rmse: 0.75811 | val_1_rmse: 0.7773  |  0:00:57s
epoch 29 | loss: 0.57978 | val_0_rmse: 0.73888 | val_1_rmse: 0.73771 |  0:00:59s
epoch 30 | loss: 0.56747 | val_0_rmse: 0.73891 | val_1_rmse: 0.75552 |  0:01:00s
epoch 31 | loss: 0.56393 | val_0_rmse: 0.83008 | val_1_rmse: 0.84113 |  0:01:02s
epoch 32 | loss: 0.58791 | val_0_rmse: 0.75972 | val_1_rmse: 0.78758 |  0:01:04s
epoch 33 | loss: 0.57507 | val_0_rmse: 0.7256  | val_1_rmse: 0.76995 |  0:01:06s
epoch 34 | loss: 0.55425 | val_0_rmse: 0.77255 | val_1_rmse: 0.82046 |  0:01:08s
epoch 35 | loss: 0.53403 | val_0_rmse: 0.83116 | val_1_rmse: 0.81407 |  0:01:10s
epoch 36 | loss: 0.54837 | val_0_rmse: 0.71592 | val_1_rmse: 0.7282  |  0:01:11s
epoch 37 | loss: 0.54084 | val_0_rmse: 0.82304 | val_1_rmse: 0.77533 |  0:01:13s
epoch 38 | loss: 0.54335 | val_0_rmse: 0.73441 | val_1_rmse: 0.80323 |  0:01:15s
epoch 39 | loss: 0.53289 | val_0_rmse: 0.78187 | val_1_rmse: 0.80155 |  0:01:17s
epoch 40 | loss: 0.5072  | val_0_rmse: 0.70181 | val_1_rmse: 0.77664 |  0:01:19s
epoch 41 | loss: 0.52171 | val_0_rmse: 0.69859 | val_1_rmse: 0.7977  |  0:01:21s
epoch 42 | loss: 0.51029 | val_0_rmse: 1.01351 | val_1_rmse: 1.07114 |  0:01:22s
epoch 43 | loss: 0.51382 | val_0_rmse: 0.70275 | val_1_rmse: 0.75575 |  0:01:24s
epoch 44 | loss: 0.48667 | val_0_rmse: 0.68784 | val_1_rmse: 0.77886 |  0:01:26s
epoch 45 | loss: 0.48719 | val_0_rmse: 0.68417 | val_1_rmse: 0.7709  |  0:01:28s
epoch 46 | loss: 0.47441 | val_0_rmse: 0.67848 | val_1_rmse: 0.75622 |  0:01:30s
epoch 47 | loss: 0.47926 | val_0_rmse: 0.78375 | val_1_rmse: 0.76786 |  0:01:32s
epoch 48 | loss: 0.47387 | val_0_rmse: 0.7109  | val_1_rmse: 0.81227 |  0:01:34s
epoch 49 | loss: 0.51902 | val_0_rmse: 0.71897 | val_1_rmse: 0.78293 |  0:01:35s
epoch 50 | loss: 0.49029 | val_0_rmse: 0.71784 | val_1_rmse: 0.77001 |  0:01:37s
epoch 51 | loss: 0.46035 | val_0_rmse: 0.70972 | val_1_rmse: 0.81112 |  0:01:39s
epoch 52 | loss: 0.49305 | val_0_rmse: 0.68235 | val_1_rmse: 0.76521 |  0:01:41s
epoch 53 | loss: 0.45978 | val_0_rmse: 0.7129  | val_1_rmse: 0.78608 |  0:01:43s
epoch 54 | loss: 0.45888 | val_0_rmse: 0.80311 | val_1_rmse: 0.86414 |  0:01:45s
epoch 55 | loss: 0.46754 | val_0_rmse: 0.69977 | val_1_rmse: 0.81276 |  0:01:46s
epoch 56 | loss: 0.45138 | val_0_rmse: 1.27801 | val_1_rmse: 1.37468 |  0:01:48s
epoch 57 | loss: 0.45104 | val_0_rmse: 0.67243 | val_1_rmse: 0.76612 |  0:01:50s
epoch 58 | loss: 0.4434  | val_0_rmse: 0.65495 | val_1_rmse: 0.79257 |  0:01:52s
epoch 59 | loss: 0.42865 | val_0_rmse: 0.79689 | val_1_rmse: 0.82759 |  0:01:54s
epoch 60 | loss: 0.44238 | val_0_rmse: 0.68074 | val_1_rmse: 0.78218 |  0:01:56s
epoch 61 | loss: 0.44415 | val_0_rmse: 0.69519 | val_1_rmse: 0.74941 |  0:01:57s
epoch 62 | loss: 0.41752 | val_0_rmse: 0.75586 | val_1_rmse: 0.82551 |  0:01:59s
epoch 63 | loss: 0.41821 | val_0_rmse: 0.62342 | val_1_rmse: 0.77319 |  0:02:01s
epoch 64 | loss: 0.41179 | val_0_rmse: 0.65657 | val_1_rmse: 0.78274 |  0:02:03s
epoch 65 | loss: 0.40769 | val_0_rmse: 0.6216  | val_1_rmse: 0.7938  |  0:02:05s
epoch 66 | loss: 0.41535 | val_0_rmse: 0.64784 | val_1_rmse: 0.78791 |  0:02:07s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.7282
Best weights from best epoch are automatically used!
ended training at: 23:53:41
Feature importance:
Mean squared error is of 0.044211852281482125
Mean absolute error:0.13946868491019665
MAPE:0.1484296103070215
R2 score:0.299938774753698
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:53:42
epoch 0  | loss: 1.39196 | val_0_rmse: 1.00541 | val_1_rmse: 1.00378 |  0:00:01s
epoch 1  | loss: 1.01141 | val_0_rmse: 1.00531 | val_1_rmse: 1.00314 |  0:00:03s
epoch 2  | loss: 1.01296 | val_0_rmse: 1.00467 | val_1_rmse: 1.0028  |  0:00:05s
epoch 3  | loss: 1.01086 | val_0_rmse: 1.00243 | val_1_rmse: 1.00074 |  0:00:07s
epoch 4  | loss: 1.0092  | val_0_rmse: 1.00092 | val_1_rmse: 0.99928 |  0:00:09s
epoch 5  | loss: 1.00253 | val_0_rmse: 0.97274 | val_1_rmse: 0.96892 |  0:00:11s
epoch 6  | loss: 0.97315 | val_0_rmse: 0.95116 | val_1_rmse: 0.9386  |  0:00:12s
epoch 7  | loss: 0.93297 | val_0_rmse: 0.93512 | val_1_rmse: 0.92121 |  0:00:14s
epoch 8  | loss: 0.88306 | val_0_rmse: 0.92062 | val_1_rmse: 0.90711 |  0:00:16s
epoch 9  | loss: 0.83548 | val_0_rmse: 0.9197  | val_1_rmse: 0.91026 |  0:00:18s
epoch 10 | loss: 0.80852 | val_0_rmse: 0.88912 | val_1_rmse: 0.87604 |  0:00:20s
epoch 11 | loss: 0.78211 | val_0_rmse: 0.88181 | val_1_rmse: 0.86822 |  0:00:22s
epoch 12 | loss: 0.75773 | val_0_rmse: 0.86835 | val_1_rmse: 0.85374 |  0:00:23s
epoch 13 | loss: 0.7496  | val_0_rmse: 0.88221 | val_1_rmse: 0.8682  |  0:00:25s
epoch 14 | loss: 0.74041 | val_0_rmse: 0.87242 | val_1_rmse: 0.86194 |  0:00:27s
epoch 15 | loss: 0.71607 | val_0_rmse: 0.84781 | val_1_rmse: 0.84005 |  0:00:29s
epoch 16 | loss: 0.70229 | val_0_rmse: 0.85115 | val_1_rmse: 0.84139 |  0:00:31s
epoch 17 | loss: 0.68595 | val_0_rmse: 0.86185 | val_1_rmse: 0.86059 |  0:00:33s
epoch 18 | loss: 0.69702 | val_0_rmse: 0.90821 | val_1_rmse: 0.9059  |  0:00:35s
epoch 19 | loss: 0.76455 | val_0_rmse: 0.92607 | val_1_rmse: 0.92807 |  0:00:36s
epoch 20 | loss: 0.70478 | val_0_rmse: 0.83421 | val_1_rmse: 0.84123 |  0:00:38s
epoch 21 | loss: 0.68206 | val_0_rmse: 0.81972 | val_1_rmse: 0.83126 |  0:00:40s
epoch 22 | loss: 0.6728  | val_0_rmse: 0.83367 | val_1_rmse: 0.85092 |  0:00:42s
epoch 23 | loss: 0.67908 | val_0_rmse: 0.81118 | val_1_rmse: 0.8135  |  0:00:44s
epoch 24 | loss: 0.64893 | val_0_rmse: 0.81568 | val_1_rmse: 0.8243  |  0:00:46s
epoch 25 | loss: 0.6999  | val_0_rmse: 0.84766 | val_1_rmse: 0.85708 |  0:00:47s
epoch 26 | loss: 0.66199 | val_0_rmse: 0.789   | val_1_rmse: 0.85976 |  0:00:49s
epoch 27 | loss: 0.66128 | val_0_rmse: 0.86123 | val_1_rmse: 0.87003 |  0:00:51s
epoch 28 | loss: 0.6638  | val_0_rmse: 0.81749 | val_1_rmse: 0.84143 |  0:00:53s
epoch 29 | loss: 0.61004 | val_0_rmse: 0.76278 | val_1_rmse: 0.9261  |  0:00:55s
epoch 30 | loss: 0.60108 | val_0_rmse: 0.81601 | val_1_rmse: 0.82655 |  0:00:57s
epoch 31 | loss: 0.62621 | val_0_rmse: 0.82709 | val_1_rmse: 0.8555  |  0:00:58s
epoch 32 | loss: 0.65254 | val_0_rmse: 0.9107  | val_1_rmse: 0.95529 |  0:01:00s
epoch 33 | loss: 0.63142 | val_0_rmse: 0.80683 | val_1_rmse: 0.91188 |  0:01:02s
epoch 34 | loss: 0.5906  | val_0_rmse: 1.05244 | val_1_rmse: 1.16356 |  0:01:04s
epoch 35 | loss: 0.61131 | val_0_rmse: 0.95585 | val_1_rmse: 0.98015 |  0:01:06s
epoch 36 | loss: 0.6033  | val_0_rmse: 0.7767  | val_1_rmse: 0.92424 |  0:01:07s
epoch 37 | loss: 0.61578 | val_0_rmse: 0.91775 | val_1_rmse: 1.00507 |  0:01:09s
epoch 38 | loss: 0.57335 | val_0_rmse: 0.75591 | val_1_rmse: 0.87737 |  0:01:11s
epoch 39 | loss: 0.54589 | val_0_rmse: 0.75871 | val_1_rmse: 0.85162 |  0:01:13s
epoch 40 | loss: 0.54773 | val_0_rmse: 0.85565 | val_1_rmse: 1.10515 |  0:01:15s
epoch 41 | loss: 0.53612 | val_0_rmse: 0.74726 | val_1_rmse: 0.82439 |  0:01:17s
epoch 42 | loss: 0.52331 | val_0_rmse: 0.7781  | val_1_rmse: 0.8531  |  0:01:18s
epoch 43 | loss: 0.55409 | val_0_rmse: 0.97915 | val_1_rmse: 1.0506  |  0:01:20s
epoch 44 | loss: 0.51196 | val_0_rmse: 0.72189 | val_1_rmse: 0.83169 |  0:01:22s
epoch 45 | loss: 0.50429 | val_0_rmse: 0.73524 | val_1_rmse: 0.83657 |  0:01:24s
epoch 46 | loss: 0.50268 | val_0_rmse: 0.82027 | val_1_rmse: 0.93129 |  0:01:26s
epoch 47 | loss: 0.52813 | val_0_rmse: 0.82995 | val_1_rmse: 0.90859 |  0:01:28s
epoch 48 | loss: 0.49174 | val_0_rmse: 0.80828 | val_1_rmse: 0.96778 |  0:01:30s
epoch 49 | loss: 0.49676 | val_0_rmse: 0.97242 | val_1_rmse: 1.2004  |  0:01:31s
epoch 50 | loss: 0.48714 | val_0_rmse: 0.85171 | val_1_rmse: 1.08527 |  0:01:33s
epoch 51 | loss: 0.50622 | val_0_rmse: 0.75245 | val_1_rmse: 0.9202  |  0:01:35s
epoch 52 | loss: 0.48996 | val_0_rmse: 0.73196 | val_1_rmse: 0.871   |  0:01:37s
epoch 53 | loss: 0.48944 | val_0_rmse: 0.74638 | val_1_rmse: 0.98985 |  0:01:39s

Early stopping occured at epoch 53 with best_epoch = 23 and best_val_1_rmse = 0.8135
Best weights from best epoch are automatically used!
ended training at: 23:55:22
Feature importance:
Mean squared error is of 0.04692631958469421
Mean absolute error:0.14135868528766352
MAPE:0.15177434837668802
R2 score:0.31035769015752024
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:55:23
epoch 0  | loss: 1.49948 | val_0_rmse: 1.0091  | val_1_rmse: 0.99387 |  0:00:01s
epoch 1  | loss: 1.02546 | val_0_rmse: 1.00914 | val_1_rmse: 0.99394 |  0:00:03s
epoch 2  | loss: 1.02183 | val_0_rmse: 1.00839 | val_1_rmse: 0.99264 |  0:00:05s
epoch 3  | loss: 1.01973 | val_0_rmse: 1.007   | val_1_rmse: 0.97993 |  0:00:07s
epoch 4  | loss: 1.01044 | val_0_rmse: 0.99969 | val_1_rmse: 0.97942 |  0:00:09s
epoch 5  | loss: 0.98933 | val_0_rmse: 0.98722 | val_1_rmse: 0.96661 |  0:00:11s
epoch 6  | loss: 0.98438 | val_0_rmse: 1.03856 | val_1_rmse: 1.01125 |  0:00:12s
epoch 7  | loss: 0.98682 | val_0_rmse: 0.98458 | val_1_rmse: 0.96197 |  0:00:14s
epoch 8  | loss: 0.98318 | val_0_rmse: 0.96365 | val_1_rmse: 0.94232 |  0:00:16s
epoch 9  | loss: 0.9365  | val_0_rmse: 0.94069 | val_1_rmse: 0.92221 |  0:00:18s
epoch 10 | loss: 0.88228 | val_0_rmse: 0.90427 | val_1_rmse: 0.8835  |  0:00:20s
epoch 11 | loss: 0.84546 | val_0_rmse: 0.9185  | val_1_rmse: 0.89779 |  0:00:22s
epoch 12 | loss: 0.81352 | val_0_rmse: 0.90243 | val_1_rmse: 0.88425 |  0:00:23s
epoch 13 | loss: 0.80644 | val_0_rmse: 0.93251 | val_1_rmse: 0.91747 |  0:00:25s
epoch 14 | loss: 0.7992  | val_0_rmse: 0.92781 | val_1_rmse: 0.9149  |  0:00:27s
epoch 15 | loss: 0.78445 | val_0_rmse: 0.89307 | val_1_rmse: 0.88198 |  0:00:29s
epoch 16 | loss: 0.77175 | val_0_rmse: 1.03759 | val_1_rmse: 1.02331 |  0:00:31s
epoch 17 | loss: 0.74542 | val_0_rmse: 0.87682 | val_1_rmse: 0.86657 |  0:00:33s
epoch 18 | loss: 0.72082 | val_0_rmse: 0.84333 | val_1_rmse: 0.85147 |  0:00:35s
epoch 19 | loss: 0.7265  | val_0_rmse: 0.90109 | val_1_rmse: 0.90529 |  0:00:36s
epoch 20 | loss: 0.73551 | val_0_rmse: 1.28985 | val_1_rmse: 1.28069 |  0:00:38s
epoch 21 | loss: 0.69882 | val_0_rmse: 0.82533 | val_1_rmse: 0.8341  |  0:00:40s
epoch 22 | loss: 0.66799 | val_0_rmse: 0.81994 | val_1_rmse: 0.83996 |  0:00:42s
epoch 23 | loss: 0.68027 | val_0_rmse: 0.81191 | val_1_rmse: 0.85817 |  0:00:44s
epoch 24 | loss: 0.69411 | val_0_rmse: 0.82021 | val_1_rmse: 0.8706  |  0:00:46s
epoch 25 | loss: 0.67382 | val_0_rmse: 0.79687 | val_1_rmse: 0.86225 |  0:00:47s
epoch 26 | loss: 0.63892 | val_0_rmse: 0.77534 | val_1_rmse: 0.83151 |  0:00:49s
epoch 27 | loss: 0.61486 | val_0_rmse: 0.76981 | val_1_rmse: 0.83178 |  0:00:51s
epoch 28 | loss: 0.63144 | val_0_rmse: 0.77254 | val_1_rmse: 0.82958 |  0:00:53s
epoch 29 | loss: 0.61222 | val_0_rmse: 0.85014 | val_1_rmse: 0.86302 |  0:00:55s
epoch 30 | loss: 0.62287 | val_0_rmse: 1.96693 | val_1_rmse: 2.00331 |  0:00:57s
epoch 31 | loss: 0.62403 | val_0_rmse: 0.96786 | val_1_rmse: 0.96769 |  0:00:58s
epoch 32 | loss: 0.61457 | val_0_rmse: 0.80903 | val_1_rmse: 0.83021 |  0:01:00s
epoch 33 | loss: 0.60978 | val_0_rmse: 1.00095 | val_1_rmse: 1.10857 |  0:01:02s
epoch 34 | loss: 0.64042 | val_0_rmse: 0.80947 | val_1_rmse: 0.86546 |  0:01:04s
epoch 35 | loss: 0.61724 | val_0_rmse: 0.81582 | val_1_rmse: 0.87416 |  0:01:06s
epoch 36 | loss: 0.59103 | val_0_rmse: 0.79545 | val_1_rmse: 0.84418 |  0:01:08s
epoch 37 | loss: 0.58665 | val_0_rmse: 0.75248 | val_1_rmse: 0.82463 |  0:01:10s
epoch 38 | loss: 0.58709 | val_0_rmse: 0.79081 | val_1_rmse: 0.88395 |  0:01:11s
epoch 39 | loss: 0.59667 | val_0_rmse: 0.79018 | val_1_rmse: 0.85393 |  0:01:13s
epoch 40 | loss: 0.57411 | val_0_rmse: 0.81839 | val_1_rmse: 0.85903 |  0:01:15s
epoch 41 | loss: 0.55119 | val_0_rmse: 0.77718 | val_1_rmse: 0.86805 |  0:01:17s
epoch 42 | loss: 0.5458  | val_0_rmse: 0.74297 | val_1_rmse: 0.8301  |  0:01:19s
epoch 43 | loss: 0.55619 | val_0_rmse: 0.78774 | val_1_rmse: 0.87354 |  0:01:21s
epoch 44 | loss: 0.5464  | val_0_rmse: 0.8274  | val_1_rmse: 0.89076 |  0:01:22s
epoch 45 | loss: 0.54415 | val_0_rmse: 0.76452 | val_1_rmse: 0.82544 |  0:01:24s
epoch 46 | loss: 0.53353 | val_0_rmse: 0.74799 | val_1_rmse: 0.84236 |  0:01:26s
epoch 47 | loss: 0.54577 | val_0_rmse: 0.79012 | val_1_rmse: 0.83426 |  0:01:28s
epoch 48 | loss: 0.56342 | val_0_rmse: 0.92922 | val_1_rmse: 0.94491 |  0:01:30s
epoch 49 | loss: 0.53156 | val_0_rmse: 0.7311  | val_1_rmse: 0.80613 |  0:01:32s
epoch 50 | loss: 0.53606 | val_0_rmse: 0.82409 | val_1_rmse: 0.91162 |  0:01:34s
epoch 51 | loss: 0.52662 | val_0_rmse: 0.71864 | val_1_rmse: 0.81103 |  0:01:35s
epoch 52 | loss: 0.51182 | val_0_rmse: 0.71184 | val_1_rmse: 0.81948 |  0:01:37s
epoch 53 | loss: 0.51211 | val_0_rmse: 0.76103 | val_1_rmse: 0.84144 |  0:01:39s
epoch 54 | loss: 0.50214 | val_0_rmse: 0.73144 | val_1_rmse: 0.81543 |  0:01:41s
epoch 55 | loss: 0.49832 | val_0_rmse: 0.73847 | val_1_rmse: 0.8212  |  0:01:43s
epoch 56 | loss: 0.49684 | val_0_rmse: 0.72531 | val_1_rmse: 0.80736 |  0:01:45s
epoch 57 | loss: 0.51223 | val_0_rmse: 0.71831 | val_1_rmse: 0.81651 |  0:01:46s
epoch 58 | loss: 0.5007  | val_0_rmse: 0.71161 | val_1_rmse: 0.80771 |  0:01:48s
epoch 59 | loss: 0.50242 | val_0_rmse: 0.73575 | val_1_rmse: 0.81661 |  0:01:50s
epoch 60 | loss: 0.50776 | val_0_rmse: 0.75    | val_1_rmse: 0.88519 |  0:01:52s
epoch 61 | loss: 0.5056  | val_0_rmse: 0.72344 | val_1_rmse: 0.80501 |  0:01:54s
epoch 62 | loss: 0.48246 | val_0_rmse: 0.69941 | val_1_rmse: 0.80559 |  0:01:56s
epoch 63 | loss: 0.48188 | val_0_rmse: 0.69747 | val_1_rmse: 0.8241  |  0:01:58s
epoch 64 | loss: 0.48185 | val_0_rmse: 0.73594 | val_1_rmse: 0.8365  |  0:01:59s
epoch 65 | loss: 0.4809  | val_0_rmse: 0.72651 | val_1_rmse: 0.8523  |  0:02:01s
epoch 66 | loss: 0.49126 | val_0_rmse: 0.71012 | val_1_rmse: 0.80283 |  0:02:03s
epoch 67 | loss: 0.47869 | val_0_rmse: 0.71368 | val_1_rmse: 0.81023 |  0:02:05s
epoch 68 | loss: 0.4707  | val_0_rmse: 0.68807 | val_1_rmse: 0.80796 |  0:02:07s
epoch 69 | loss: 0.46542 | val_0_rmse: 0.68079 | val_1_rmse: 0.8053  |  0:02:09s
epoch 70 | loss: 0.46108 | val_0_rmse: 0.68788 | val_1_rmse: 0.80417 |  0:02:10s
epoch 71 | loss: 0.45045 | val_0_rmse: 0.71152 | val_1_rmse: 0.84212 |  0:02:12s
epoch 72 | loss: 0.45389 | val_0_rmse: 0.71089 | val_1_rmse: 0.86287 |  0:02:14s
epoch 73 | loss: 0.45719 | val_0_rmse: 0.71622 | val_1_rmse: 0.8818  |  0:02:16s
epoch 74 | loss: 0.45229 | val_0_rmse: 0.69046 | val_1_rmse: 0.80875 |  0:02:18s
epoch 75 | loss: 0.45341 | val_0_rmse: 0.6912  | val_1_rmse: 0.81184 |  0:02:20s
epoch 76 | loss: 0.44988 | val_0_rmse: 0.67383 | val_1_rmse: 0.81821 |  0:02:21s
epoch 77 | loss: 0.44893 | val_0_rmse: 0.7044  | val_1_rmse: 0.80557 |  0:02:23s
epoch 78 | loss: 0.44954 | val_0_rmse: 0.70568 | val_1_rmse: 0.82425 |  0:02:25s
epoch 79 | loss: 0.44776 | val_0_rmse: 0.66338 | val_1_rmse: 0.79681 |  0:02:27s
epoch 80 | loss: 0.44519 | val_0_rmse: 0.65828 | val_1_rmse: 0.81056 |  0:02:29s
epoch 81 | loss: 0.45458 | val_0_rmse: 0.69661 | val_1_rmse: 0.86084 |  0:02:31s
epoch 82 | loss: 0.4497  | val_0_rmse: 0.72552 | val_1_rmse: 0.82943 |  0:02:32s
epoch 83 | loss: 0.43845 | val_0_rmse: 0.68555 | val_1_rmse: 0.80653 |  0:02:34s
epoch 84 | loss: 0.45343 | val_0_rmse: 0.66452 | val_1_rmse: 0.82076 |  0:02:36s
epoch 85 | loss: 0.43662 | val_0_rmse: 0.66026 | val_1_rmse: 0.80801 |  0:02:38s
epoch 86 | loss: 0.43032 | val_0_rmse: 0.667   | val_1_rmse: 0.82323 |  0:02:40s
epoch 87 | loss: 0.43792 | val_0_rmse: 0.66334 | val_1_rmse: 0.80651 |  0:02:42s
epoch 88 | loss: 0.43946 | val_0_rmse: 0.65467 | val_1_rmse: 0.80689 |  0:02:44s
epoch 89 | loss: 0.44148 | val_0_rmse: 0.64644 | val_1_rmse: 0.79753 |  0:02:45s
epoch 90 | loss: 0.44069 | val_0_rmse: 0.66729 | val_1_rmse: 0.81549 |  0:02:47s
epoch 91 | loss: 0.42636 | val_0_rmse: 0.65852 | val_1_rmse: 0.82089 |  0:02:49s
epoch 92 | loss: 0.42945 | val_0_rmse: 0.65517 | val_1_rmse: 0.82628 |  0:02:51s
epoch 93 | loss: 0.43093 | val_0_rmse: 0.62619 | val_1_rmse: 0.80372 |  0:02:53s
epoch 94 | loss: 0.43774 | val_0_rmse: 0.64638 | val_1_rmse: 0.80122 |  0:02:55s
epoch 95 | loss: 0.42963 | val_0_rmse: 0.6351  | val_1_rmse: 0.81285 |  0:02:56s
epoch 96 | loss: 0.43424 | val_0_rmse: 0.69717 | val_1_rmse: 0.85616 |  0:02:58s
epoch 97 | loss: 0.42963 | val_0_rmse: 0.64483 | val_1_rmse: 0.80257 |  0:03:00s
epoch 98 | loss: 0.42921 | val_0_rmse: 0.65252 | val_1_rmse: 0.80512 |  0:03:02s
epoch 99 | loss: 0.4246  | val_0_rmse: 0.6349  | val_1_rmse: 0.80635 |  0:03:04s
epoch 100| loss: 0.41637 | val_0_rmse: 0.64703 | val_1_rmse: 0.81888 |  0:03:06s
epoch 101| loss: 0.41805 | val_0_rmse: 0.70866 | val_1_rmse: 0.89008 |  0:03:07s
epoch 102| loss: 0.41706 | val_0_rmse: 0.63647 | val_1_rmse: 0.83026 |  0:03:09s
epoch 103| loss: 0.42159 | val_0_rmse: 0.70253 | val_1_rmse: 0.82267 |  0:03:11s
epoch 104| loss: 0.42089 | val_0_rmse: 0.65193 | val_1_rmse: 0.81973 |  0:03:13s
epoch 105| loss: 0.42763 | val_0_rmse: 0.7859  | val_1_rmse: 0.95973 |  0:03:15s
epoch 106| loss: 0.42461 | val_0_rmse: 0.67652 | val_1_rmse: 0.85748 |  0:03:17s
epoch 107| loss: 0.42257 | val_0_rmse: 0.64333 | val_1_rmse: 0.83679 |  0:03:18s
epoch 108| loss: 0.41934 | val_0_rmse: 0.65305 | val_1_rmse: 0.806   |  0:03:20s
epoch 109| loss: 0.42278 | val_0_rmse: 0.70929 | val_1_rmse: 0.83417 |  0:03:22s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.79681
Best weights from best epoch are automatically used!
ended training at: 23:58:46
Feature importance:
Mean squared error is of 0.042691708075010514
Mean absolute error:0.1372902536265904
MAPE:0.14900029775264206
R2 score:0.36156542909133715
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:58:47
epoch 0  | loss: 1.43549 | val_0_rmse: 1.01542 | val_1_rmse: 0.98398 |  0:00:01s
epoch 1  | loss: 1.03512 | val_0_rmse: 1.01593 | val_1_rmse: 0.98408 |  0:00:03s
epoch 2  | loss: 1.03485 | val_0_rmse: 1.01556 | val_1_rmse: 0.98399 |  0:00:05s
epoch 3  | loss: 1.03363 | val_0_rmse: 1.0156  | val_1_rmse: 0.98391 |  0:00:07s
epoch 4  | loss: 1.03379 | val_0_rmse: 1.01555 | val_1_rmse: 0.98392 |  0:00:09s
epoch 5  | loss: 1.0334  | val_0_rmse: 1.015   | val_1_rmse: 0.98328 |  0:00:11s
epoch 6  | loss: 1.03234 | val_0_rmse: 1.01437 | val_1_rmse: 0.98271 |  0:00:12s
epoch 7  | loss: 1.02573 | val_0_rmse: 0.98938 | val_1_rmse: 0.95783 |  0:00:14s
epoch 8  | loss: 0.97481 | val_0_rmse: 0.99996 | val_1_rmse: 0.97036 |  0:00:16s
epoch 9  | loss: 0.89307 | val_0_rmse: 0.91545 | val_1_rmse: 0.8809  |  0:00:18s
epoch 10 | loss: 0.83476 | val_0_rmse: 0.95022 | val_1_rmse: 0.92214 |  0:00:20s
epoch 11 | loss: 0.8198  | val_0_rmse: 0.89783 | val_1_rmse: 0.86751 |  0:00:22s
epoch 12 | loss: 0.80789 | val_0_rmse: 0.89905 | val_1_rmse: 0.87602 |  0:00:24s
epoch 13 | loss: 0.77171 | val_0_rmse: 0.87397 | val_1_rmse: 0.85836 |  0:00:25s
epoch 14 | loss: 0.75588 | val_0_rmse: 0.91397 | val_1_rmse: 0.89916 |  0:00:27s
epoch 15 | loss: 0.73741 | val_0_rmse: 0.89604 | val_1_rmse: 0.87154 |  0:00:29s
epoch 16 | loss: 0.72445 | val_0_rmse: 0.90606 | val_1_rmse: 0.92119 |  0:00:31s
epoch 17 | loss: 0.72535 | val_0_rmse: 0.8334  | val_1_rmse: 0.83017 |  0:00:33s
epoch 18 | loss: 0.67369 | val_0_rmse: 1.01517 | val_1_rmse: 1.03306 |  0:00:35s
epoch 19 | loss: 0.67453 | val_0_rmse: 0.84575 | val_1_rmse: 0.83042 |  0:00:37s
epoch 20 | loss: 0.64265 | val_0_rmse: 0.79767 | val_1_rmse: 0.80746 |  0:00:38s
epoch 21 | loss: 0.64873 | val_0_rmse: 0.79637 | val_1_rmse: 0.80874 |  0:00:40s
epoch 22 | loss: 0.62833 | val_0_rmse: 1.0934  | val_1_rmse: 1.1308  |  0:00:42s
epoch 23 | loss: 0.63941 | val_0_rmse: 0.78317 | val_1_rmse: 0.81317 |  0:00:44s
epoch 24 | loss: 0.60368 | val_0_rmse: 0.90901 | val_1_rmse: 0.87994 |  0:00:46s
epoch 25 | loss: 0.6023  | val_0_rmse: 0.75465 | val_1_rmse: 0.80882 |  0:00:48s
epoch 26 | loss: 0.58661 | val_0_rmse: 0.7618  | val_1_rmse: 0.82162 |  0:00:50s
epoch 27 | loss: 0.58634 | val_0_rmse: 0.78902 | val_1_rmse: 0.81743 |  0:00:51s
epoch 28 | loss: 0.5924  | val_0_rmse: 0.77333 | val_1_rmse: 0.81002 |  0:00:53s
epoch 29 | loss: 0.57199 | val_0_rmse: 0.79737 | val_1_rmse: 0.81587 |  0:00:55s
epoch 30 | loss: 0.56702 | val_0_rmse: 0.77896 | val_1_rmse: 0.85867 |  0:00:57s
epoch 31 | loss: 0.55358 | val_0_rmse: 0.79502 | val_1_rmse: 0.84435 |  0:00:59s
epoch 32 | loss: 0.57119 | val_0_rmse: 0.72263 | val_1_rmse: 0.76341 |  0:01:01s
epoch 33 | loss: 0.54772 | val_0_rmse: 0.72453 | val_1_rmse: 0.77014 |  0:01:02s
epoch 34 | loss: 0.52423 | val_0_rmse: 0.72451 | val_1_rmse: 0.78504 |  0:01:04s
epoch 35 | loss: 0.56678 | val_0_rmse: 0.76599 | val_1_rmse: 0.80929 |  0:01:06s
epoch 36 | loss: 0.53385 | val_0_rmse: 0.7363  | val_1_rmse: 0.81449 |  0:01:08s
epoch 37 | loss: 0.51665 | val_0_rmse: 0.73916 | val_1_rmse: 0.81449 |  0:01:10s
epoch 38 | loss: 0.53162 | val_0_rmse: 0.95611 | val_1_rmse: 1.06448 |  0:01:12s
epoch 39 | loss: 0.52437 | val_0_rmse: 1.15972 | val_1_rmse: 1.27188 |  0:01:14s
epoch 40 | loss: 0.50279 | val_0_rmse: 0.74784 | val_1_rmse: 0.82114 |  0:01:15s
epoch 41 | loss: 0.50985 | val_0_rmse: 0.68694 | val_1_rmse: 0.79871 |  0:01:17s
epoch 42 | loss: 0.49915 | val_0_rmse: 0.70572 | val_1_rmse: 0.79739 |  0:01:19s
epoch 43 | loss: 0.48881 | val_0_rmse: 0.71158 | val_1_rmse: 0.80778 |  0:01:21s
epoch 44 | loss: 0.48087 | val_0_rmse: 0.76207 | val_1_rmse: 0.82269 |  0:01:23s
epoch 45 | loss: 0.4942  | val_0_rmse: 0.77337 | val_1_rmse: 0.83167 |  0:01:25s
epoch 46 | loss: 0.53624 | val_0_rmse: 2.0687  | val_1_rmse: 2.17433 |  0:01:27s
epoch 47 | loss: 0.53882 | val_0_rmse: 0.79783 | val_1_rmse: 0.83526 |  0:01:28s
epoch 48 | loss: 0.49889 | val_0_rmse: 0.95201 | val_1_rmse: 1.13623 |  0:01:30s
epoch 49 | loss: 0.48204 | val_0_rmse: 0.79115 | val_1_rmse: 0.84551 |  0:01:32s
epoch 50 | loss: 0.48253 | val_0_rmse: 0.76454 | val_1_rmse: 0.8806  |  0:01:34s
epoch 51 | loss: 0.47667 | val_0_rmse: 0.67313 | val_1_rmse: 0.82144 |  0:01:36s
epoch 52 | loss: 0.47325 | val_0_rmse: 0.67921 | val_1_rmse: 0.80244 |  0:01:38s
epoch 53 | loss: 0.45911 | val_0_rmse: 0.71714 | val_1_rmse: 0.81611 |  0:01:39s
epoch 54 | loss: 0.44664 | val_0_rmse: 0.67269 | val_1_rmse: 0.79748 |  0:01:41s
epoch 55 | loss: 0.4446  | val_0_rmse: 0.71297 | val_1_rmse: 0.85024 |  0:01:43s
epoch 56 | loss: 0.45082 | val_0_rmse: 0.71135 | val_1_rmse: 0.83632 |  0:01:45s
epoch 57 | loss: 0.4502  | val_0_rmse: 0.76112 | val_1_rmse: 0.8651  |  0:01:47s
epoch 58 | loss: 0.45212 | val_0_rmse: 0.71081 | val_1_rmse: 0.81334 |  0:01:49s
epoch 59 | loss: 0.43794 | val_0_rmse: 0.68044 | val_1_rmse: 0.8089  |  0:01:51s
epoch 60 | loss: 0.4423  | val_0_rmse: 0.6868  | val_1_rmse: 0.80735 |  0:01:52s
epoch 61 | loss: 0.43546 | val_0_rmse: 0.67486 | val_1_rmse: 0.81761 |  0:01:54s
epoch 62 | loss: 0.43363 | val_0_rmse: 0.68571 | val_1_rmse: 0.82202 |  0:01:56s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.76341
Best weights from best epoch are automatically used!
ended training at: 00:00:44
Feature importance:
Mean squared error is of 0.04022131617761707
Mean absolute error:0.1360849971485794
MAPE:0.1497277567730438
R2 score:0.38187224912321716
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:00:45
epoch 0  | loss: 1.32754 | val_0_rmse: 0.96367 | val_1_rmse: 1.17006 |  0:00:01s
epoch 1  | loss: 0.93352 | val_0_rmse: 0.96315 | val_1_rmse: 1.16974 |  0:00:03s
epoch 2  | loss: 0.93093 | val_0_rmse: 0.96311 | val_1_rmse: 1.1696  |  0:00:05s
epoch 3  | loss: 0.93009 | val_0_rmse: 0.96297 | val_1_rmse: 1.16952 |  0:00:07s
epoch 4  | loss: 0.92681 | val_0_rmse: 0.96158 | val_1_rmse: 1.16924 |  0:00:09s
epoch 5  | loss: 0.91901 | val_0_rmse: 0.94913 | val_1_rmse: 1.16066 |  0:00:11s
epoch 6  | loss: 0.91434 | val_0_rmse: 0.95912 | val_1_rmse: 1.1664  |  0:00:13s
epoch 7  | loss: 0.90893 | val_0_rmse: 0.95981 | val_1_rmse: 1.16765 |  0:00:14s
epoch 8  | loss: 0.90948 | val_0_rmse: 0.95865 | val_1_rmse: 1.16806 |  0:00:16s
epoch 9  | loss: 0.90953 | val_0_rmse: 0.9445  | val_1_rmse: 1.15545 |  0:00:18s
epoch 10 | loss: 0.90297 | val_0_rmse: 0.94509 | val_1_rmse: 1.15643 |  0:00:20s
epoch 11 | loss: 0.89705 | val_0_rmse: 0.94356 | val_1_rmse: 1.15544 |  0:00:22s
epoch 12 | loss: 0.89744 | val_0_rmse: 0.94575 | val_1_rmse: 1.15723 |  0:00:24s
epoch 13 | loss: 0.89664 | val_0_rmse: 0.94395 | val_1_rmse: 1.1562  |  0:00:25s
epoch 14 | loss: 0.89456 | val_0_rmse: 0.94842 | val_1_rmse: 1.15862 |  0:00:27s
epoch 15 | loss: 0.90035 | val_0_rmse: 0.9476  | val_1_rmse: 1.15883 |  0:00:29s
epoch 16 | loss: 0.89909 | val_0_rmse: 0.94503 | val_1_rmse: 1.15725 |  0:00:31s
epoch 17 | loss: 0.89355 | val_0_rmse: 0.94593 | val_1_rmse: 1.15646 |  0:00:33s
epoch 18 | loss: 0.89556 | val_0_rmse: 0.94565 | val_1_rmse: 1.15676 |  0:00:35s
epoch 19 | loss: 0.89527 | val_0_rmse: 0.94529 | val_1_rmse: 1.15632 |  0:00:37s
epoch 20 | loss: 0.89425 | val_0_rmse: 0.94491 | val_1_rmse: 1.15551 |  0:00:38s
epoch 21 | loss: 0.89398 | val_0_rmse: 0.94567 | val_1_rmse: 1.15808 |  0:00:40s
epoch 22 | loss: 0.89411 | val_0_rmse: 0.94467 | val_1_rmse: 1.1564  |  0:00:42s
epoch 23 | loss: 0.8929  | val_0_rmse: 0.94469 | val_1_rmse: 1.156   |  0:00:44s
epoch 24 | loss: 0.89435 | val_0_rmse: 0.94648 | val_1_rmse: 1.15699 |  0:00:46s
epoch 25 | loss: 0.89634 | val_0_rmse: 0.94791 | val_1_rmse: 1.15872 |  0:00:48s
epoch 26 | loss: 0.89797 | val_0_rmse: 0.94749 | val_1_rmse: 1.15658 |  0:00:50s
epoch 27 | loss: 0.89601 | val_0_rmse: 0.9459  | val_1_rmse: 1.15625 |  0:00:51s
epoch 28 | loss: 0.89386 | val_0_rmse: 0.94427 | val_1_rmse: 1.15554 |  0:00:53s
epoch 29 | loss: 0.89435 | val_0_rmse: 0.94457 | val_1_rmse: 1.15656 |  0:00:55s
epoch 30 | loss: 0.89187 | val_0_rmse: 0.9657  | val_1_rmse: 1.17762 |  0:00:57s
epoch 31 | loss: 0.89105 | val_0_rmse: 0.95928 | val_1_rmse: 1.16749 |  0:00:59s
epoch 32 | loss: 0.89283 | val_0_rmse: 0.95759 | val_1_rmse: 1.16572 |  0:01:01s
epoch 33 | loss: 0.89141 | val_0_rmse: 0.94311 | val_1_rmse: 1.1553  |  0:01:03s
epoch 34 | loss: 0.88987 | val_0_rmse: 0.9437  | val_1_rmse: 1.15544 |  0:01:04s
epoch 35 | loss: 0.89149 | val_0_rmse: 0.9444  | val_1_rmse: 1.15556 |  0:01:06s
epoch 36 | loss: 0.89185 | val_0_rmse: 0.94405 | val_1_rmse: 1.15567 |  0:01:08s
epoch 37 | loss: 0.89173 | val_0_rmse: 0.94348 | val_1_rmse: 1.15569 |  0:01:10s
epoch 38 | loss: 0.89184 | val_0_rmse: 0.94304 | val_1_rmse: 1.15522 |  0:01:12s
epoch 39 | loss: 0.89092 | val_0_rmse: 0.94335 | val_1_rmse: 1.15516 |  0:01:14s
epoch 40 | loss: 0.89063 | val_0_rmse: 0.94332 | val_1_rmse: 1.15532 |  0:01:15s
epoch 41 | loss: 0.88975 | val_0_rmse: 0.94316 | val_1_rmse: 1.15539 |  0:01:17s
epoch 42 | loss: 0.8895  | val_0_rmse: 0.94279 | val_1_rmse: 1.15502 |  0:01:19s
epoch 43 | loss: 0.88889 | val_0_rmse: 0.94222 | val_1_rmse: 1.15462 |  0:01:21s
epoch 44 | loss: 0.88944 | val_0_rmse: 0.94305 | val_1_rmse: 1.15535 |  0:01:23s
epoch 45 | loss: 0.88734 | val_0_rmse: 0.94175 | val_1_rmse: 1.15473 |  0:01:25s
epoch 46 | loss: 0.8852  | val_0_rmse: 0.94621 | val_1_rmse: 1.157   |  0:01:27s
epoch 47 | loss: 0.87989 | val_0_rmse: 0.93482 | val_1_rmse: 1.1485  |  0:01:28s
epoch 48 | loss: 0.87139 | val_0_rmse: 0.93171 | val_1_rmse: 1.14563 |  0:01:30s
epoch 49 | loss: 0.866   | val_0_rmse: 0.926   | val_1_rmse: 1.1411  |  0:01:32s
epoch 50 | loss: 0.84648 | val_0_rmse: 0.91633 | val_1_rmse: 1.1359  |  0:01:34s
epoch 51 | loss: 0.82993 | val_0_rmse: 0.90945 | val_1_rmse: 1.12956 |  0:01:36s
epoch 52 | loss: 0.82021 | val_0_rmse: 0.90022 | val_1_rmse: 1.11538 |  0:01:38s
epoch 53 | loss: 0.81626 | val_0_rmse: 0.89878 | val_1_rmse: 1.11528 |  0:01:40s
epoch 54 | loss: 0.80147 | val_0_rmse: 0.88956 | val_1_rmse: 1.10736 |  0:01:41s
epoch 55 | loss: 0.787   | val_0_rmse: 0.88969 | val_1_rmse: 1.10219 |  0:01:43s
epoch 56 | loss: 0.78098 | val_0_rmse: 0.87894 | val_1_rmse: 1.09613 |  0:01:45s
epoch 57 | loss: 0.76155 | val_0_rmse: 0.866   | val_1_rmse: 1.08641 |  0:01:47s
epoch 58 | loss: 0.7547  | val_0_rmse: 0.86036 | val_1_rmse: 1.07789 |  0:01:49s
epoch 59 | loss: 0.73613 | val_0_rmse: 0.84954 | val_1_rmse: 1.06928 |  0:01:51s
epoch 60 | loss: 0.7206  | val_0_rmse: 0.84119 | val_1_rmse: 1.0521  |  0:01:53s
epoch 61 | loss: 0.69878 | val_0_rmse: 0.84406 | val_1_rmse: 1.05633 |  0:01:55s
epoch 62 | loss: 0.67805 | val_0_rmse: 0.81642 | val_1_rmse: 1.03533 |  0:01:56s
epoch 63 | loss: 0.69819 | val_0_rmse: 0.82679 | val_1_rmse: 1.04572 |  0:01:58s
epoch 64 | loss: 0.68942 | val_0_rmse: 0.82626 | val_1_rmse: 1.05064 |  0:02:00s
epoch 65 | loss: 0.68326 | val_0_rmse: 0.82217 | val_1_rmse: 1.0425  |  0:02:02s
epoch 66 | loss: 0.68155 | val_0_rmse: 0.83594 | val_1_rmse: 1.05451 |  0:02:04s
epoch 67 | loss: 0.6634  | val_0_rmse: 0.81213 | val_1_rmse: 1.03283 |  0:02:06s
epoch 68 | loss: 0.64234 | val_0_rmse: 0.80466 | val_1_rmse: 1.03222 |  0:02:07s
epoch 69 | loss: 0.63113 | val_0_rmse: 0.79433 | val_1_rmse: 1.02768 |  0:02:09s
epoch 70 | loss: 0.62127 | val_0_rmse: 0.77402 | val_1_rmse: 1.0031  |  0:02:11s
epoch 71 | loss: 0.6195  | val_0_rmse: 0.77367 | val_1_rmse: 1.00195 |  0:02:13s
epoch 72 | loss: 0.60639 | val_0_rmse: 0.79462 | val_1_rmse: 1.02774 |  0:02:15s
epoch 73 | loss: 0.59522 | val_0_rmse: 0.7501  | val_1_rmse: 0.98831 |  0:02:17s
epoch 74 | loss: 0.5857  | val_0_rmse: 0.74909 | val_1_rmse: 0.99029 |  0:02:19s
epoch 75 | loss: 0.58086 | val_0_rmse: 0.77303 | val_1_rmse: 1.00625 |  0:02:20s
epoch 76 | loss: 0.57049 | val_0_rmse: 0.74952 | val_1_rmse: 0.99295 |  0:02:22s
epoch 77 | loss: 0.57197 | val_0_rmse: 0.786   | val_1_rmse: 1.0195  |  0:02:24s
epoch 78 | loss: 0.59354 | val_0_rmse: 0.82017 | val_1_rmse: 1.02682 |  0:02:26s
epoch 79 | loss: 0.63837 | val_0_rmse: 0.89195 | val_1_rmse: 1.09647 |  0:02:28s
epoch 80 | loss: 0.64964 | val_0_rmse: 0.80584 | val_1_rmse: 1.01606 |  0:02:30s
epoch 81 | loss: 0.63969 | val_0_rmse: 0.90629 | val_1_rmse: 1.11352 |  0:02:32s
epoch 82 | loss: 0.65488 | val_0_rmse: 0.80529 | val_1_rmse: 1.03926 |  0:02:33s
epoch 83 | loss: 0.66256 | val_0_rmse: 0.82259 | val_1_rmse: 1.04397 |  0:02:35s
epoch 84 | loss: 0.64962 | val_0_rmse: 0.78988 | val_1_rmse: 1.02724 |  0:02:37s
epoch 85 | loss: 0.63113 | val_0_rmse: 0.79248 | val_1_rmse: 1.02727 |  0:02:39s
epoch 86 | loss: 0.63758 | val_0_rmse: 0.79357 | val_1_rmse: 1.03081 |  0:02:41s
epoch 87 | loss: 0.63756 | val_0_rmse: 0.79897 | val_1_rmse: 1.04039 |  0:02:43s
epoch 88 | loss: 0.62235 | val_0_rmse: 0.77014 | val_1_rmse: 1.01301 |  0:02:45s
epoch 89 | loss: 0.60049 | val_0_rmse: 0.76366 | val_1_rmse: 1.01006 |  0:02:46s
epoch 90 | loss: 0.59209 | val_0_rmse: 0.75971 | val_1_rmse: 1.00598 |  0:02:48s
epoch 91 | loss: 0.58052 | val_0_rmse: 0.74818 | val_1_rmse: 0.98839 |  0:02:50s
epoch 92 | loss: 0.574   | val_0_rmse: 0.76622 | val_1_rmse: 1.00112 |  0:02:52s
epoch 93 | loss: 0.57551 | val_0_rmse: 0.7647  | val_1_rmse: 1.0031  |  0:02:54s
epoch 94 | loss: 0.56389 | val_0_rmse: 0.74339 | val_1_rmse: 0.98936 |  0:02:56s
epoch 95 | loss: 0.56021 | val_0_rmse: 0.75042 | val_1_rmse: 0.99109 |  0:02:57s
epoch 96 | loss: 0.55311 | val_0_rmse: 0.73246 | val_1_rmse: 0.98145 |  0:02:59s
epoch 97 | loss: 0.55244 | val_0_rmse: 0.73495 | val_1_rmse: 0.98825 |  0:03:01s
epoch 98 | loss: 0.57309 | val_0_rmse: 0.80349 | val_1_rmse: 1.03958 |  0:03:03s
epoch 99 | loss: 0.57738 | val_0_rmse: 0.79036 | val_1_rmse: 1.03897 |  0:03:05s
epoch 100| loss: 0.57169 | val_0_rmse: 0.74709 | val_1_rmse: 1.00146 |  0:03:07s
epoch 101| loss: 0.56594 | val_0_rmse: 0.73474 | val_1_rmse: 0.99105 |  0:03:09s
epoch 102| loss: 0.55793 | val_0_rmse: 0.7507  | val_1_rmse: 0.99474 |  0:03:10s
epoch 103| loss: 0.55802 | val_0_rmse: 0.7487  | val_1_rmse: 1.01212 |  0:03:12s
epoch 104| loss: 0.57784 | val_0_rmse: 0.74674 | val_1_rmse: 0.99916 |  0:03:14s
epoch 105| loss: 0.57007 | val_0_rmse: 0.76903 | val_1_rmse: 1.02093 |  0:03:16s
epoch 106| loss: 0.60607 | val_0_rmse: 0.89305 | val_1_rmse: 1.10259 |  0:03:18s
epoch 107| loss: 0.72069 | val_0_rmse: 0.8573  | val_1_rmse: 1.07121 |  0:03:20s
epoch 108| loss: 0.69204 | val_0_rmse: 0.90576 | val_1_rmse: 1.12163 |  0:03:22s
epoch 109| loss: 0.67622 | val_0_rmse: 0.79902 | val_1_rmse: 1.0283  |  0:03:23s
epoch 110| loss: 0.66167 | val_0_rmse: 0.80065 | val_1_rmse: 1.02998 |  0:03:25s
epoch 111| loss: 0.65725 | val_0_rmse: 0.90391 | val_1_rmse: 1.11138 |  0:03:27s
epoch 112| loss: 0.64976 | val_0_rmse: 0.83787 | val_1_rmse: 1.06419 |  0:03:29s
epoch 113| loss: 0.62231 | val_0_rmse: 0.78627 | val_1_rmse: 1.02691 |  0:03:31s
epoch 114| loss: 0.61977 | val_0_rmse: 1.0337  | val_1_rmse: 1.21716 |  0:03:33s
epoch 115| loss: 0.63045 | val_0_rmse: 0.79606 | val_1_rmse: 1.0264  |  0:03:35s
epoch 116| loss: 0.65382 | val_0_rmse: 1.01225 | val_1_rmse: 1.12289 |  0:03:36s
epoch 117| loss: 0.65884 | val_0_rmse: 0.7942  | val_1_rmse: 1.02243 |  0:03:38s
epoch 118| loss: 0.63148 | val_0_rmse: 0.79024 | val_1_rmse: 1.02207 |  0:03:40s
epoch 119| loss: 0.6237  | val_0_rmse: 0.77221 | val_1_rmse: 1.0047  |  0:03:42s
epoch 120| loss: 0.6088  | val_0_rmse: 0.78229 | val_1_rmse: 1.01419 |  0:03:44s
epoch 121| loss: 0.60388 | val_0_rmse: 0.76461 | val_1_rmse: 0.99687 |  0:03:46s
epoch 122| loss: 0.60316 | val_0_rmse: 0.77158 | val_1_rmse: 1.0102  |  0:03:48s
epoch 123| loss: 0.61118 | val_0_rmse: 0.77119 | val_1_rmse: 1.00877 |  0:03:49s
epoch 124| loss: 0.5947  | val_0_rmse: 0.77284 | val_1_rmse: 1.00958 |  0:03:51s
epoch 125| loss: 0.60536 | val_0_rmse: 0.76462 | val_1_rmse: 1.00347 |  0:03:53s
epoch 126| loss: 0.60949 | val_0_rmse: 0.86108 | val_1_rmse: 1.07117 |  0:03:55s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.98145
Best weights from best epoch are automatically used!
ended training at: 00:04:41
Feature importance:
Mean squared error is of 0.041654030013836246
Mean absolute error:0.13401393826302108
MAPE:0.14400113893982944
R2 score:0.36799849563346554
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:04:44
epoch 0  | loss: 1.19805 | val_0_rmse: 0.99767 | val_1_rmse: 0.99363 |  0:00:07s
epoch 1  | loss: 0.99361 | val_0_rmse: 0.9815  | val_1_rmse: 0.97879 |  0:00:14s
epoch 2  | loss: 0.95727 | val_0_rmse: 0.9604  | val_1_rmse: 0.95797 |  0:00:20s
epoch 3  | loss: 0.91105 | val_0_rmse: 0.93627 | val_1_rmse: 0.93291 |  0:00:28s
epoch 4  | loss: 0.86746 | val_0_rmse: 0.94405 | val_1_rmse: 0.94052 |  0:00:35s
epoch 5  | loss: 0.83993 | val_0_rmse: 0.92564 | val_1_rmse: 0.92402 |  0:00:42s
epoch 6  | loss: 0.82549 | val_0_rmse: 0.91914 | val_1_rmse: 0.91793 |  0:00:49s
epoch 7  | loss: 0.81557 | val_0_rmse: 0.90743 | val_1_rmse: 0.91056 |  0:00:56s
epoch 8  | loss: 0.80372 | val_0_rmse: 0.89802 | val_1_rmse: 0.90524 |  0:01:03s
epoch 9  | loss: 0.79364 | val_0_rmse: 0.89326 | val_1_rmse: 0.90483 |  0:01:10s
epoch 10 | loss: 0.78223 | val_0_rmse: 0.88168 | val_1_rmse: 0.89846 |  0:01:17s
epoch 11 | loss: 0.77274 | val_0_rmse: 0.88174 | val_1_rmse: 0.90394 |  0:01:24s
epoch 12 | loss: 0.77006 | val_0_rmse: 0.86262 | val_1_rmse: 0.88966 |  0:01:31s
epoch 13 | loss: 0.75857 | val_0_rmse: 0.86985 | val_1_rmse: 0.90816 |  0:01:38s
epoch 14 | loss: 0.75632 | val_0_rmse: 0.85751 | val_1_rmse: 0.89643 |  0:01:45s
epoch 15 | loss: 0.74645 | val_0_rmse: 0.86067 | val_1_rmse: 0.90174 |  0:01:52s
epoch 16 | loss: 0.74032 | val_0_rmse: 0.89833 | val_1_rmse: 0.92749 |  0:01:59s
epoch 17 | loss: 0.7337  | val_0_rmse: 0.8513  | val_1_rmse: 0.89722 |  0:02:06s
epoch 18 | loss: 0.7277  | val_0_rmse: 0.8673  | val_1_rmse: 0.91782 |  0:02:12s
epoch 19 | loss: 0.71794 | val_0_rmse: 0.83857 | val_1_rmse: 0.89699 |  0:02:19s
epoch 20 | loss: 0.71164 | val_0_rmse: 0.83714 | val_1_rmse: 0.89906 |  0:02:26s
epoch 21 | loss: 0.70773 | val_0_rmse: 0.83854 | val_1_rmse: 0.90006 |  0:02:33s
epoch 22 | loss: 0.70359 | val_0_rmse: 0.83275 | val_1_rmse: 0.89489 |  0:02:40s
epoch 23 | loss: 0.69219 | val_0_rmse: 0.83469 | val_1_rmse: 0.90926 |  0:02:47s
epoch 24 | loss: 0.68859 | val_0_rmse: 0.83397 | val_1_rmse: 0.91385 |  0:02:54s
epoch 25 | loss: 0.68759 | val_0_rmse: 0.8265  | val_1_rmse: 0.91705 |  0:03:01s
epoch 26 | loss: 0.68226 | val_0_rmse: 0.82677 | val_1_rmse: 0.90497 |  0:03:08s
epoch 27 | loss: 0.67638 | val_0_rmse: 0.84117 | val_1_rmse: 0.9218  |  0:03:15s
epoch 28 | loss: 0.6743  | val_0_rmse: 0.82299 | val_1_rmse: 0.91196 |  0:03:22s
epoch 29 | loss: 0.66633 | val_0_rmse: 0.87056 | val_1_rmse: 0.908   |  0:03:29s
epoch 30 | loss: 0.66829 | val_0_rmse: 0.82222 | val_1_rmse: 0.90941 |  0:03:36s
epoch 31 | loss: 0.66351 | val_0_rmse: 1.02646 | val_1_rmse: 1.03404 |  0:03:43s
epoch 32 | loss: 0.67636 | val_0_rmse: 0.82686 | val_1_rmse: 0.90122 |  0:03:50s
epoch 33 | loss: 0.66136 | val_0_rmse: 0.83009 | val_1_rmse: 0.91776 |  0:03:57s
epoch 34 | loss: 0.65276 | val_0_rmse: 0.81656 | val_1_rmse: 0.91297 |  0:04:04s
epoch 35 | loss: 0.68908 | val_0_rmse: 0.82362 | val_1_rmse: 0.90711 |  0:04:11s
epoch 36 | loss: 0.66159 | val_0_rmse: 0.81733 | val_1_rmse: 0.90168 |  0:04:18s
epoch 37 | loss: 0.64974 | val_0_rmse: 0.83856 | val_1_rmse: 0.92862 |  0:04:24s
epoch 38 | loss: 0.64132 | val_0_rmse: 0.79907 | val_1_rmse: 0.91825 |  0:04:31s
epoch 39 | loss: 0.63654 | val_0_rmse: 0.8028  | val_1_rmse: 0.9046  |  0:04:38s
epoch 40 | loss: 0.62766 | val_0_rmse: 0.79975 | val_1_rmse: 0.92524 |  0:04:45s
epoch 41 | loss: 0.62766 | val_0_rmse: 0.79814 | val_1_rmse: 0.93297 |  0:04:52s
epoch 42 | loss: 0.62253 | val_0_rmse: 0.80299 | val_1_rmse: 0.91458 |  0:04:59s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.88966
Best weights from best epoch are automatically used!
ended training at: 00:09:47
Feature importance:
Mean squared error is of 0.06324371756200814
Mean absolute error:0.1837677274763504
MAPE:0.20098089292780208
R2 score:0.19396523601748372
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:09:49
epoch 0  | loss: 1.19479 | val_0_rmse: 1.00095 | val_1_rmse: 1.02961 |  0:00:06s
epoch 1  | loss: 1.00092 | val_0_rmse: 1.00019 | val_1_rmse: 1.02817 |  0:00:13s
epoch 2  | loss: 0.99778 | val_0_rmse: 0.9997  | val_1_rmse: 1.02783 |  0:00:20s
epoch 3  | loss: 0.99036 | val_0_rmse: 0.99465 | val_1_rmse: 1.02276 |  0:00:27s
epoch 4  | loss: 0.97339 | val_0_rmse: 0.98964 | val_1_rmse: 1.01374 |  0:00:34s
epoch 5  | loss: 0.94595 | val_0_rmse: 0.95426 | val_1_rmse: 0.98147 |  0:00:41s
epoch 6  | loss: 0.87486 | val_0_rmse: 0.92429 | val_1_rmse: 0.95099 |  0:00:48s
epoch 7  | loss: 0.84206 | val_0_rmse: 0.91067 | val_1_rmse: 0.9413  |  0:00:55s
epoch 8  | loss: 0.81896 | val_0_rmse: 0.90277 | val_1_rmse: 0.93706 |  0:01:02s
epoch 9  | loss: 0.80125 | val_0_rmse: 0.89536 | val_1_rmse: 0.93681 |  0:01:09s
epoch 10 | loss: 0.78942 | val_0_rmse: 0.89093 | val_1_rmse: 0.93592 |  0:01:16s
epoch 11 | loss: 0.78644 | val_0_rmse: 0.8762  | val_1_rmse: 0.92889 |  0:01:23s
epoch 12 | loss: 0.76601 | val_0_rmse: 0.86363 | val_1_rmse: 0.92595 |  0:01:30s
epoch 13 | loss: 0.76185 | val_0_rmse: 0.86388 | val_1_rmse: 0.93398 |  0:01:37s
epoch 14 | loss: 0.75247 | val_0_rmse: 0.85115 | val_1_rmse: 0.92453 |  0:01:44s
epoch 15 | loss: 0.74715 | val_0_rmse: 0.8542  | val_1_rmse: 0.9288  |  0:01:51s
epoch 16 | loss: 0.74582 | val_0_rmse: 0.84873 | val_1_rmse: 0.9255  |  0:01:58s
epoch 17 | loss: 0.73589 | val_0_rmse: 0.84547 | val_1_rmse: 0.93167 |  0:02:05s
epoch 18 | loss: 0.73144 | val_0_rmse: 0.84255 | val_1_rmse: 0.93233 |  0:02:12s
epoch 19 | loss: 0.72119 | val_0_rmse: 0.85918 | val_1_rmse: 0.93143 |  0:02:19s
epoch 20 | loss: 0.71535 | val_0_rmse: 0.84823 | val_1_rmse: 0.9378  |  0:02:26s
epoch 21 | loss: 0.71312 | val_0_rmse: 0.87995 | val_1_rmse: 0.96417 |  0:02:33s
epoch 22 | loss: 0.70477 | val_0_rmse: 0.88433 | val_1_rmse: 0.95442 |  0:02:40s
epoch 23 | loss: 0.69964 | val_0_rmse: 0.83658 | val_1_rmse: 0.94315 |  0:02:47s
epoch 24 | loss: 0.69326 | val_0_rmse: 0.83902 | val_1_rmse: 0.93205 |  0:02:54s
epoch 25 | loss: 0.6882  | val_0_rmse: 0.85782 | val_1_rmse: 0.98535 |  0:03:01s
epoch 26 | loss: 0.68645 | val_0_rmse: 0.84423 | val_1_rmse: 0.97748 |  0:03:08s
epoch 27 | loss: 0.68084 | val_0_rmse: 0.83728 | val_1_rmse: 0.94256 |  0:03:15s
epoch 28 | loss: 0.67458 | val_0_rmse: 0.83478 | val_1_rmse: 0.99217 |  0:03:22s
epoch 29 | loss: 0.67052 | val_0_rmse: 0.84824 | val_1_rmse: 0.96256 |  0:03:29s
epoch 30 | loss: 0.66654 | val_0_rmse: 0.85029 | val_1_rmse: 0.99133 |  0:03:36s
epoch 31 | loss: 0.66447 | val_0_rmse: 0.82478 | val_1_rmse: 0.95439 |  0:03:43s
epoch 32 | loss: 0.65772 | val_0_rmse: 0.88064 | val_1_rmse: 1.11215 |  0:03:50s
epoch 33 | loss: 0.65364 | val_0_rmse: 0.90508 | val_1_rmse: 0.9783  |  0:03:57s
epoch 34 | loss: 0.65212 | val_0_rmse: 0.82231 | val_1_rmse: 0.9787  |  0:04:03s
epoch 35 | loss: 0.65188 | val_0_rmse: 0.81335 | val_1_rmse: 0.96301 |  0:04:10s
epoch 36 | loss: 0.63865 | val_0_rmse: 0.99037 | val_1_rmse: 0.9631  |  0:04:17s
epoch 37 | loss: 0.63536 | val_0_rmse: 0.82243 | val_1_rmse: 0.94614 |  0:04:24s
epoch 38 | loss: 0.63431 | val_0_rmse: 0.88922 | val_1_rmse: 1.00952 |  0:04:31s
epoch 39 | loss: 0.63955 | val_0_rmse: 0.82991 | val_1_rmse: 1.03498 |  0:04:38s
epoch 40 | loss: 0.62871 | val_0_rmse: 0.82358 | val_1_rmse: 0.98148 |  0:04:45s
epoch 41 | loss: 0.62772 | val_0_rmse: 0.81136 | val_1_rmse: 0.95565 |  0:04:52s
epoch 42 | loss: 0.62609 | val_0_rmse: 0.80661 | val_1_rmse: 0.97266 |  0:04:59s
epoch 43 | loss: 0.63211 | val_0_rmse: 0.83265 | val_1_rmse: 1.00753 |  0:05:06s
epoch 44 | loss: 0.62431 | val_0_rmse: 0.80905 | val_1_rmse: 0.95542 |  0:05:13s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.92453
Best weights from best epoch are automatically used!
ended training at: 00:15:06
Feature importance:
Mean squared error is of 0.059636046373108625
Mean absolute error:0.1854104740351892
MAPE:0.20253971027399073
R2 score:0.1907481746794497
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:15:08
epoch 0  | loss: 1.17009 | val_0_rmse: 1.00075 | val_1_rmse: 1.01806 |  0:00:06s
epoch 1  | loss: 1.00197 | val_0_rmse: 1.0023  | val_1_rmse: 1.01915 |  0:00:13s
epoch 2  | loss: 0.99391 | val_0_rmse: 0.96423 | val_1_rmse: 0.98225 |  0:00:20s
epoch 3  | loss: 0.94398 | val_0_rmse: 0.94921 | val_1_rmse: 0.96874 |  0:00:27s
epoch 4  | loss: 0.88907 | val_0_rmse: 0.93807 | val_1_rmse: 0.95802 |  0:00:34s
epoch 5  | loss: 0.85622 | val_0_rmse: 0.92666 | val_1_rmse: 0.94925 |  0:00:41s
epoch 6  | loss: 0.83414 | val_0_rmse: 0.91987 | val_1_rmse: 0.94264 |  0:00:48s
epoch 7  | loss: 0.81614 | val_0_rmse: 0.90879 | val_1_rmse: 0.93938 |  0:00:55s
epoch 8  | loss: 0.80208 | val_0_rmse: 0.89778 | val_1_rmse: 0.92975 |  0:01:02s
epoch 9  | loss: 0.78415 | val_0_rmse: 0.89022 | val_1_rmse: 0.92773 |  0:01:09s
epoch 10 | loss: 0.77764 | val_0_rmse: 0.87773 | val_1_rmse: 0.92335 |  0:01:16s
epoch 11 | loss: 0.77062 | val_0_rmse: 0.86888 | val_1_rmse: 0.91971 |  0:01:23s
epoch 12 | loss: 0.765   | val_0_rmse: 0.87539 | val_1_rmse: 0.93603 |  0:01:30s
epoch 13 | loss: 0.75576 | val_0_rmse: 0.85658 | val_1_rmse: 0.91337 |  0:01:37s
epoch 14 | loss: 0.74774 | val_0_rmse: 0.8592  | val_1_rmse: 0.92259 |  0:01:44s
epoch 15 | loss: 0.74444 | val_0_rmse: 0.85187 | val_1_rmse: 0.91524 |  0:01:51s
epoch 16 | loss: 0.73921 | val_0_rmse: 0.8494  | val_1_rmse: 0.9211  |  0:01:58s
epoch 17 | loss: 0.73081 | val_0_rmse: 0.85381 | val_1_rmse: 0.9283  |  0:02:05s
epoch 18 | loss: 0.72769 | val_0_rmse: 0.84132 | val_1_rmse: 0.92988 |  0:02:12s
epoch 19 | loss: 0.72443 | val_0_rmse: 0.84002 | val_1_rmse: 0.92946 |  0:02:19s
epoch 20 | loss: 0.71691 | val_0_rmse: 0.84467 | val_1_rmse: 0.94086 |  0:02:26s
epoch 21 | loss: 0.71521 | val_0_rmse: 0.87636 | val_1_rmse: 0.95476 |  0:02:33s
epoch 22 | loss: 0.7083  | val_0_rmse: 0.8743  | val_1_rmse: 0.94297 |  0:02:40s
epoch 23 | loss: 0.70288 | val_0_rmse: 0.83908 | val_1_rmse: 0.93498 |  0:02:47s
epoch 24 | loss: 0.69608 | val_0_rmse: 0.87053 | val_1_rmse: 0.92728 |  0:02:54s
epoch 25 | loss: 0.76001 | val_0_rmse: 0.97975 | val_1_rmse: 0.97197 |  0:03:01s
epoch 26 | loss: 0.75293 | val_0_rmse: 0.8784  | val_1_rmse: 0.96093 |  0:03:07s
epoch 27 | loss: 0.75012 | val_0_rmse: 0.97068 | val_1_rmse: 1.04766 |  0:03:14s
epoch 28 | loss: 0.73731 | val_0_rmse: 0.87753 | val_1_rmse: 0.96227 |  0:03:21s
epoch 29 | loss: 0.7626  | val_0_rmse: 0.88949 | val_1_rmse: 0.94928 |  0:03:28s
epoch 30 | loss: 0.73951 | val_0_rmse: 0.86556 | val_1_rmse: 0.946   |  0:03:35s
epoch 31 | loss: 0.71893 | val_0_rmse: 0.85498 | val_1_rmse: 0.9525  |  0:03:42s
epoch 32 | loss: 0.7044  | val_0_rmse: 0.86044 | val_1_rmse: 0.9393  |  0:03:49s
epoch 33 | loss: 0.69856 | val_0_rmse: 0.8392  | val_1_rmse: 0.95248 |  0:03:56s
epoch 34 | loss: 0.68776 | val_0_rmse: 0.84488 | val_1_rmse: 0.95058 |  0:04:03s
epoch 35 | loss: 0.68532 | val_0_rmse: 0.83072 | val_1_rmse: 0.946   |  0:04:10s
epoch 36 | loss: 0.67997 | val_0_rmse: 0.83794 | val_1_rmse: 0.95594 |  0:04:17s
epoch 37 | loss: 0.67495 | val_0_rmse: 0.84591 | val_1_rmse: 0.94367 |  0:04:24s
epoch 38 | loss: 0.67172 | val_0_rmse: 0.82027 | val_1_rmse: 0.95022 |  0:04:31s
epoch 39 | loss: 0.66613 | val_0_rmse: 0.81952 | val_1_rmse: 0.94106 |  0:04:38s
epoch 40 | loss: 0.6608  | val_0_rmse: 0.83701 | val_1_rmse: 0.95378 |  0:04:45s
epoch 41 | loss: 0.66224 | val_0_rmse: 0.84191 | val_1_rmse: 0.94512 |  0:04:52s
epoch 42 | loss: 0.66232 | val_0_rmse: 0.82133 | val_1_rmse: 0.94314 |  0:04:59s
epoch 43 | loss: 0.65135 | val_0_rmse: 0.82951 | val_1_rmse: 0.96797 |  0:05:06s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.91337
Best weights from best epoch are automatically used!
ended training at: 00:20:18
Feature importance:
Mean squared error is of 0.060326302414835956
Mean absolute error:0.18392428874922453
MAPE:0.20141261442266162
R2 score:0.19613542728210975
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:20:21
epoch 0  | loss: 1.13362 | val_0_rmse: 0.99962 | val_1_rmse: 1.00822 |  0:00:06s
epoch 1  | loss: 0.99598 | val_0_rmse: 0.97052 | val_1_rmse: 0.98043 |  0:00:13s
epoch 2  | loss: 0.96354 | val_0_rmse: 0.95103 | val_1_rmse: 0.96683 |  0:00:20s
epoch 3  | loss: 0.90301 | val_0_rmse: 0.93888 | val_1_rmse: 0.95708 |  0:00:27s
epoch 4  | loss: 0.86625 | val_0_rmse: 0.93463 | val_1_rmse: 0.95548 |  0:00:34s
epoch 5  | loss: 0.84323 | val_0_rmse: 0.92532 | val_1_rmse: 0.95002 |  0:00:41s
epoch 6  | loss: 0.82302 | val_0_rmse: 0.9155  | val_1_rmse: 0.93967 |  0:00:48s
epoch 7  | loss: 0.81073 | val_0_rmse: 0.90399 | val_1_rmse: 0.93336 |  0:00:55s
epoch 8  | loss: 0.794   | val_0_rmse: 0.90412 | val_1_rmse: 0.93663 |  0:01:02s
epoch 9  | loss: 0.78516 | val_0_rmse: 0.90227 | val_1_rmse: 0.93281 |  0:01:09s
epoch 10 | loss: 0.77672 | val_0_rmse: 0.87699 | val_1_rmse: 0.91511 |  0:01:16s
epoch 11 | loss: 0.76927 | val_0_rmse: 0.87062 | val_1_rmse: 0.91321 |  0:01:23s
epoch 12 | loss: 0.75891 | val_0_rmse: 0.86262 | val_1_rmse: 0.91024 |  0:01:30s
epoch 13 | loss: 0.75065 | val_0_rmse: 0.85237 | val_1_rmse: 0.91229 |  0:01:37s
epoch 14 | loss: 0.74616 | val_0_rmse: 0.8517  | val_1_rmse: 0.90928 |  0:01:44s
epoch 15 | loss: 0.73481 | val_0_rmse: 0.8487  | val_1_rmse: 0.91549 |  0:01:51s
epoch 16 | loss: 0.7324  | val_0_rmse: 0.83648 | val_1_rmse: 0.91117 |  0:01:58s
epoch 17 | loss: 0.72337 | val_0_rmse: 0.86263 | val_1_rmse: 0.96975 |  0:02:05s
epoch 18 | loss: 0.7164  | val_0_rmse: 0.83968 | val_1_rmse: 0.92325 |  0:02:12s
epoch 19 | loss: 0.71577 | val_0_rmse: 0.83561 | val_1_rmse: 0.91636 |  0:02:19s
epoch 20 | loss: 0.70987 | val_0_rmse: 0.84352 | val_1_rmse: 0.92242 |  0:02:26s
epoch 21 | loss: 0.70587 | val_0_rmse: 1.28624 | val_1_rmse: 0.91752 |  0:02:33s
epoch 22 | loss: 0.70627 | val_0_rmse: 0.83977 | val_1_rmse: 0.92914 |  0:02:40s
epoch 23 | loss: 0.7016  | val_0_rmse: 0.82789 | val_1_rmse: 0.92071 |  0:02:47s
epoch 24 | loss: 0.69343 | val_0_rmse: 0.82411 | val_1_rmse: 0.91853 |  0:02:54s
epoch 25 | loss: 0.69637 | val_0_rmse: 0.82668 | val_1_rmse: 0.91537 |  0:03:01s
epoch 26 | loss: 0.68199 | val_0_rmse: 0.82095 | val_1_rmse: 0.9161  |  0:03:08s
epoch 27 | loss: 0.67808 | val_0_rmse: 0.82273 | val_1_rmse: 0.92226 |  0:03:15s
epoch 28 | loss: 0.67174 | val_0_rmse: 0.81848 | val_1_rmse: 0.92707 |  0:03:22s
epoch 29 | loss: 0.67036 | val_0_rmse: 0.81666 | val_1_rmse: 0.92861 |  0:03:29s
epoch 30 | loss: 0.66787 | val_0_rmse: 0.81481 | val_1_rmse: 0.92455 |  0:03:36s
epoch 31 | loss: 0.663   | val_0_rmse: 0.81658 | val_1_rmse: 0.92448 |  0:03:43s
epoch 32 | loss: 0.66072 | val_0_rmse: 0.80794 | val_1_rmse: 0.91509 |  0:03:50s
epoch 33 | loss: 0.65773 | val_0_rmse: 0.8232  | val_1_rmse: 0.92626 |  0:03:57s
epoch 34 | loss: 0.64853 | val_0_rmse: 0.81495 | val_1_rmse: 0.95284 |  0:04:04s
epoch 35 | loss: 0.64873 | val_0_rmse: 0.82505 | val_1_rmse: 0.9292  |  0:04:11s
epoch 36 | loss: 0.64991 | val_0_rmse: 0.82791 | val_1_rmse: 0.93632 |  0:04:18s
epoch 37 | loss: 0.64012 | val_0_rmse: 0.998   | val_1_rmse: 0.94031 |  0:04:25s
epoch 38 | loss: 0.63658 | val_0_rmse: 0.85737 | val_1_rmse: 0.92049 |  0:04:32s
epoch 39 | loss: 0.63466 | val_0_rmse: 0.85653 | val_1_rmse: 0.93724 |  0:04:39s
epoch 40 | loss: 0.63384 | val_0_rmse: 0.84927 | val_1_rmse: 0.92747 |  0:04:46s
epoch 41 | loss: 0.62861 | val_0_rmse: 0.82719 | val_1_rmse: 0.92816 |  0:04:53s
epoch 42 | loss: 0.6286  | val_0_rmse: 0.80711 | val_1_rmse: 0.92774 |  0:05:00s
epoch 43 | loss: 0.63614 | val_0_rmse: 0.79377 | val_1_rmse: 0.92201 |  0:05:07s
epoch 44 | loss: 0.62457 | val_0_rmse: 0.8923  | val_1_rmse: 0.93212 |  0:05:14s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.90928
Best weights from best epoch are automatically used!
ended training at: 00:25:39
Feature importance:
Mean squared error is of 0.06185247862210314
Mean absolute error:0.18340369675934115
MAPE:0.19953404433246333
R2 score:0.1948998583995486
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:25:41
epoch 0  | loss: 1.14288 | val_0_rmse: 0.9842  | val_1_rmse: 0.97325 |  0:00:07s
epoch 1  | loss: 0.97548 | val_0_rmse: 0.97118 | val_1_rmse: 0.95833 |  0:00:13s
epoch 2  | loss: 0.94015 | val_0_rmse: 0.94478 | val_1_rmse: 0.93551 |  0:00:20s
epoch 3  | loss: 0.87941 | val_0_rmse: 0.92644 | val_1_rmse: 0.92171 |  0:00:27s
epoch 4  | loss: 0.84484 | val_0_rmse: 0.91909 | val_1_rmse: 0.9143  |  0:00:34s
epoch 5  | loss: 0.82734 | val_0_rmse: 0.92678 | val_1_rmse: 0.92595 |  0:00:41s
epoch 6  | loss: 0.80279 | val_0_rmse: 0.90518 | val_1_rmse: 0.90708 |  0:00:48s
epoch 7  | loss: 0.79337 | val_0_rmse: 0.90718 | val_1_rmse: 0.90592 |  0:00:55s
epoch 8  | loss: 0.78392 | val_0_rmse: 0.89978 | val_1_rmse: 0.90375 |  0:01:02s
epoch 9  | loss: 0.76945 | val_0_rmse: 0.90308 | val_1_rmse: 0.91314 |  0:01:09s
epoch 10 | loss: 0.75975 | val_0_rmse: 0.87654 | val_1_rmse: 0.88917 |  0:01:16s
epoch 11 | loss: 0.75335 | val_0_rmse: 0.87775 | val_1_rmse: 0.88959 |  0:01:23s
epoch 12 | loss: 0.74022 | val_0_rmse: 0.92813 | val_1_rmse: 0.95546 |  0:01:30s
epoch 13 | loss: 0.73749 | val_0_rmse: 0.84088 | val_1_rmse: 0.88614 |  0:01:37s
epoch 14 | loss: 0.72301 | val_0_rmse: 0.83287 | val_1_rmse: 0.88218 |  0:01:44s
epoch 15 | loss: 0.71742 | val_0_rmse: 0.82677 | val_1_rmse: 0.89715 |  0:01:51s
epoch 16 | loss: 0.70392 | val_0_rmse: 0.82605 | val_1_rmse: 0.89898 |  0:01:58s
epoch 17 | loss: 0.70111 | val_0_rmse: 0.86314 | val_1_rmse: 0.91353 |  0:02:05s
epoch 18 | loss: 0.69388 | val_0_rmse: 0.84112 | val_1_rmse: 0.91753 |  0:02:12s
epoch 19 | loss: 0.68345 | val_0_rmse: 0.84249 | val_1_rmse: 0.92429 |  0:02:19s
epoch 20 | loss: 0.67905 | val_0_rmse: 0.84072 | val_1_rmse: 0.91969 |  0:02:26s
epoch 21 | loss: 0.67169 | val_0_rmse: 0.82237 | val_1_rmse: 0.91323 |  0:02:33s
epoch 22 | loss: 0.67004 | val_0_rmse: 0.84785 | val_1_rmse: 0.9176  |  0:02:40s
epoch 23 | loss: 0.66651 | val_0_rmse: 0.83728 | val_1_rmse: 0.90585 |  0:02:47s
epoch 24 | loss: 0.66847 | val_0_rmse: 0.86308 | val_1_rmse: 0.95049 |  0:02:54s
epoch 25 | loss: 0.66013 | val_0_rmse: 0.89107 | val_1_rmse: 0.99229 |  0:03:01s
epoch 26 | loss: 0.65187 | val_0_rmse: 0.813   | val_1_rmse: 0.91785 |  0:03:08s
epoch 27 | loss: 0.64556 | val_0_rmse: 0.88121 | val_1_rmse: 0.96808 |  0:03:15s
epoch 28 | loss: 0.63601 | val_0_rmse: 0.81208 | val_1_rmse: 0.9109  |  0:03:22s
epoch 29 | loss: 0.63243 | val_0_rmse: 0.81057 | val_1_rmse: 0.90156 |  0:03:29s
epoch 30 | loss: 0.63108 | val_0_rmse: 0.80958 | val_1_rmse: 0.90788 |  0:03:35s
epoch 31 | loss: 0.625   | val_0_rmse: 0.82456 | val_1_rmse: 0.93088 |  0:03:42s
epoch 32 | loss: 0.62192 | val_0_rmse: 0.80809 | val_1_rmse: 0.94007 |  0:03:49s
epoch 33 | loss: 0.62078 | val_0_rmse: 0.79007 | val_1_rmse: 0.91041 |  0:03:56s
epoch 34 | loss: 0.61799 | val_0_rmse: 0.8033  | val_1_rmse: 0.92317 |  0:04:03s
epoch 35 | loss: 0.61437 | val_0_rmse: 0.84435 | val_1_rmse: 0.96299 |  0:04:10s
epoch 36 | loss: 0.60323 | val_0_rmse: 0.83305 | val_1_rmse: 0.96534 |  0:04:17s
epoch 37 | loss: 0.60415 | val_0_rmse: 0.80167 | val_1_rmse: 0.91618 |  0:04:24s
epoch 38 | loss: 0.59655 | val_0_rmse: 0.78351 | val_1_rmse: 0.92047 |  0:04:32s
epoch 39 | loss: 0.5991  | val_0_rmse: 1.0493  | val_1_rmse: 1.17529 |  0:04:39s
epoch 40 | loss: 0.59153 | val_0_rmse: 0.80694 | val_1_rmse: 0.95697 |  0:04:46s
epoch 41 | loss: 0.58759 | val_0_rmse: 0.79963 | val_1_rmse: 0.90552 |  0:04:53s
epoch 42 | loss: 0.58622 | val_0_rmse: 0.7919  | val_1_rmse: 0.94108 |  0:05:00s
epoch 43 | loss: 0.58872 | val_0_rmse: 0.91659 | val_1_rmse: 0.94863 |  0:05:08s
epoch 44 | loss: 0.58234 | val_0_rmse: 1.37146 | val_1_rmse: 0.90329 |  0:05:15s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.88218
Best weights from best epoch are automatically used!
ended training at: 00:31:00
Feature importance:
Mean squared error is of 0.06527478065367312
Mean absolute error:0.1835786030686479
MAPE:0.1980889140096664
R2 score:0.1987989519165848
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:31:02
epoch 0  | loss: 2.58348 | val_0_rmse: 1.01225 | val_1_rmse: 0.98996 |  0:00:01s
epoch 1  | loss: 1.08847 | val_0_rmse: 1.01354 | val_1_rmse: 0.99008 |  0:00:02s
epoch 2  | loss: 1.03073 | val_0_rmse: 1.01125 | val_1_rmse: 0.98667 |  0:00:03s
epoch 3  | loss: 1.00433 | val_0_rmse: 0.9863  | val_1_rmse: 0.965   |  0:00:04s
epoch 4  | loss: 0.97495 | val_0_rmse: 0.97448 | val_1_rmse: 0.95002 |  0:00:05s
epoch 5  | loss: 0.93888 | val_0_rmse: 0.96228 | val_1_rmse: 0.93508 |  0:00:06s
epoch 6  | loss: 0.91272 | val_0_rmse: 0.96178 | val_1_rmse: 0.94207 |  0:00:07s
epoch 7  | loss: 0.89449 | val_0_rmse: 0.94452 | val_1_rmse: 0.921   |  0:00:08s
epoch 8  | loss: 0.87823 | val_0_rmse: 0.94629 | val_1_rmse: 0.92424 |  0:00:09s
epoch 9  | loss: 0.86197 | val_0_rmse: 0.92503 | val_1_rmse: 0.89954 |  0:00:10s
epoch 10 | loss: 0.85311 | val_0_rmse: 0.93233 | val_1_rmse: 0.90817 |  0:00:11s
epoch 11 | loss: 0.83477 | val_0_rmse: 0.92591 | val_1_rmse: 0.89963 |  0:00:13s
epoch 12 | loss: 0.83277 | val_0_rmse: 0.91652 | val_1_rmse: 0.89202 |  0:00:14s
epoch 13 | loss: 0.82147 | val_0_rmse: 0.92234 | val_1_rmse: 0.89964 |  0:00:15s
epoch 14 | loss: 0.8141  | val_0_rmse: 0.90685 | val_1_rmse: 0.88545 |  0:00:16s
epoch 15 | loss: 0.81244 | val_0_rmse: 0.90661 | val_1_rmse: 0.88413 |  0:00:17s
epoch 16 | loss: 0.80397 | val_0_rmse: 0.91004 | val_1_rmse: 0.88768 |  0:00:18s
epoch 17 | loss: 0.79399 | val_0_rmse: 0.90918 | val_1_rmse: 0.8837  |  0:00:19s
epoch 18 | loss: 0.79264 | val_0_rmse: 0.89928 | val_1_rmse: 0.87267 |  0:00:20s
epoch 19 | loss: 0.7838  | val_0_rmse: 0.90057 | val_1_rmse: 0.87459 |  0:00:21s
epoch 20 | loss: 0.7816  | val_0_rmse: 0.89806 | val_1_rmse: 0.87499 |  0:00:22s
epoch 21 | loss: 0.7739  | val_0_rmse: 0.89339 | val_1_rmse: 0.86806 |  0:00:23s
epoch 22 | loss: 0.7674  | val_0_rmse: 0.89286 | val_1_rmse: 0.8687  |  0:00:24s
epoch 23 | loss: 0.76101 | val_0_rmse: 0.88245 | val_1_rmse: 0.86193 |  0:00:25s
epoch 24 | loss: 0.76205 | val_0_rmse: 0.8874  | val_1_rmse: 0.85843 |  0:00:26s
epoch 25 | loss: 0.75612 | val_0_rmse: 0.88709 | val_1_rmse: 0.87158 |  0:00:27s
epoch 26 | loss: 0.75938 | val_0_rmse: 0.88037 | val_1_rmse: 0.85774 |  0:00:28s
epoch 27 | loss: 0.74963 | val_0_rmse: 0.87459 | val_1_rmse: 0.85388 |  0:00:29s
epoch 28 | loss: 0.74474 | val_0_rmse: 0.8695  | val_1_rmse: 0.86027 |  0:00:31s
epoch 29 | loss: 0.75174 | val_0_rmse: 0.87498 | val_1_rmse: 0.85656 |  0:00:32s
epoch 30 | loss: 0.75009 | val_0_rmse: 0.86931 | val_1_rmse: 0.85747 |  0:00:33s
epoch 31 | loss: 0.7476  | val_0_rmse: 0.87139 | val_1_rmse: 0.85693 |  0:00:34s
epoch 32 | loss: 0.76485 | val_0_rmse: 0.87052 | val_1_rmse: 0.85121 |  0:00:35s
epoch 33 | loss: 0.75237 | val_0_rmse: 0.8727  | val_1_rmse: 0.8549  |  0:00:36s
epoch 34 | loss: 0.75353 | val_0_rmse: 0.86971 | val_1_rmse: 0.85726 |  0:00:37s
epoch 35 | loss: 0.74773 | val_0_rmse: 0.87113 | val_1_rmse: 0.86363 |  0:00:39s
epoch 36 | loss: 0.7424  | val_0_rmse: 0.86471 | val_1_rmse: 0.85883 |  0:00:40s
epoch 37 | loss: 0.73601 | val_0_rmse: 0.86085 | val_1_rmse: 0.85268 |  0:00:41s
epoch 38 | loss: 0.73098 | val_0_rmse: 0.85561 | val_1_rmse: 0.84683 |  0:00:42s
epoch 39 | loss: 0.72159 | val_0_rmse: 0.84932 | val_1_rmse: 0.85399 |  0:00:43s
epoch 40 | loss: 0.72042 | val_0_rmse: 0.85288 | val_1_rmse: 0.85272 |  0:00:44s
epoch 41 | loss: 0.72873 | val_0_rmse: 0.84618 | val_1_rmse: 0.84398 |  0:00:45s
epoch 42 | loss: 0.72387 | val_0_rmse: 0.84814 | val_1_rmse: 0.85156 |  0:00:46s
epoch 43 | loss: 0.71065 | val_0_rmse: 0.8356  | val_1_rmse: 0.84154 |  0:00:47s
epoch 44 | loss: 0.72167 | val_0_rmse: 0.84503 | val_1_rmse: 0.84749 |  0:00:48s
epoch 45 | loss: 0.73375 | val_0_rmse: 0.87434 | val_1_rmse: 0.87557 |  0:00:49s
epoch 46 | loss: 0.72996 | val_0_rmse: 0.84392 | val_1_rmse: 0.84174 |  0:00:50s
epoch 47 | loss: 0.71829 | val_0_rmse: 0.84449 | val_1_rmse: 0.85001 |  0:00:51s
epoch 48 | loss: 0.70672 | val_0_rmse: 0.83714 | val_1_rmse: 0.84296 |  0:00:52s
epoch 49 | loss: 0.7018  | val_0_rmse: 0.83307 | val_1_rmse: 0.84177 |  0:00:53s
epoch 50 | loss: 0.69521 | val_0_rmse: 0.81757 | val_1_rmse: 0.83447 |  0:00:55s
epoch 51 | loss: 0.68461 | val_0_rmse: 0.8097  | val_1_rmse: 0.8346  |  0:00:56s
epoch 52 | loss: 0.69095 | val_0_rmse: 0.81568 | val_1_rmse: 0.8493  |  0:00:57s
epoch 53 | loss: 0.69804 | val_0_rmse: 0.8138  | val_1_rmse: 0.8466  |  0:00:58s
epoch 54 | loss: 0.6924  | val_0_rmse: 0.81663 | val_1_rmse: 0.84556 |  0:00:59s
epoch 55 | loss: 0.67757 | val_0_rmse: 0.86452 | val_1_rmse: 0.94065 |  0:01:00s
epoch 56 | loss: 0.69244 | val_0_rmse: 0.81281 | val_1_rmse: 0.84636 |  0:01:01s
epoch 57 | loss: 0.68371 | val_0_rmse: 0.81922 | val_1_rmse: 0.8542  |  0:01:02s
epoch 58 | loss: 0.67101 | val_0_rmse: 0.80263 | val_1_rmse: 0.84569 |  0:01:03s
epoch 59 | loss: 0.6727  | val_0_rmse: 0.79715 | val_1_rmse: 0.84419 |  0:01:04s
epoch 60 | loss: 0.66916 | val_0_rmse: 0.79732 | val_1_rmse: 0.84421 |  0:01:05s
epoch 61 | loss: 0.65881 | val_0_rmse: 0.78583 | val_1_rmse: 0.84474 |  0:01:06s
epoch 62 | loss: 0.65579 | val_0_rmse: 0.78579 | val_1_rmse: 0.85687 |  0:01:07s
epoch 63 | loss: 0.65879 | val_0_rmse: 0.78848 | val_1_rmse: 0.85216 |  0:01:08s
epoch 64 | loss: 0.6621  | val_0_rmse: 0.79646 | val_1_rmse: 0.86618 |  0:01:09s
epoch 65 | loss: 0.7098  | val_0_rmse: 0.82388 | val_1_rmse: 0.86318 |  0:01:10s
epoch 66 | loss: 0.68903 | val_0_rmse: 0.81504 | val_1_rmse: 0.86691 |  0:01:11s
epoch 67 | loss: 0.67023 | val_0_rmse: 0.81597 | val_1_rmse: 0.85561 |  0:01:12s
epoch 68 | loss: 0.66497 | val_0_rmse: 0.79979 | val_1_rmse: 0.84639 |  0:01:13s
epoch 69 | loss: 0.64955 | val_0_rmse: 0.79538 | val_1_rmse: 0.85188 |  0:01:14s
epoch 70 | loss: 0.66797 | val_0_rmse: 0.79786 | val_1_rmse: 0.857   |  0:01:15s
epoch 71 | loss: 0.67302 | val_0_rmse: 0.79126 | val_1_rmse: 0.85801 |  0:01:16s
epoch 72 | loss: 0.64726 | val_0_rmse: 0.78418 | val_1_rmse: 0.84703 |  0:01:17s
epoch 73 | loss: 0.62492 | val_0_rmse: 0.76759 | val_1_rmse: 0.8498  |  0:01:18s
epoch 74 | loss: 0.61632 | val_0_rmse: 0.76683 | val_1_rmse: 0.8666  |  0:01:19s
epoch 75 | loss: 0.65682 | val_0_rmse: 0.79466 | val_1_rmse: 0.84518 |  0:01:20s
epoch 76 | loss: 0.66515 | val_0_rmse: 0.79136 | val_1_rmse: 0.84382 |  0:01:21s
epoch 77 | loss: 0.63952 | val_0_rmse: 0.77835 | val_1_rmse: 0.8494  |  0:01:22s
epoch 78 | loss: 0.6354  | val_0_rmse: 0.77942 | val_1_rmse: 0.84304 |  0:01:23s
epoch 79 | loss: 0.64409 | val_0_rmse: 0.78711 | val_1_rmse: 0.88294 |  0:01:24s
epoch 80 | loss: 0.6436  | val_0_rmse: 0.78368 | val_1_rmse: 0.8516  |  0:01:25s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.83447
Best weights from best epoch are automatically used!
ended training at: 00:32:28
Feature importance:
Mean squared error is of 0.06141025102451219
Mean absolute error:0.18241617356346843
MAPE:0.1969457171931476
R2 score:0.29239685725208564
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:32:28
epoch 0  | loss: 2.32457 | val_0_rmse: 0.95606 | val_1_rmse: 0.99117 |  0:00:01s
epoch 1  | loss: 0.94736 | val_0_rmse: 0.95302 | val_1_rmse: 0.98759 |  0:00:02s
epoch 2  | loss: 0.9016  | val_0_rmse: 0.94112 | val_1_rmse: 0.97572 |  0:00:03s
epoch 3  | loss: 0.86143 | val_0_rmse: 0.92002 | val_1_rmse: 0.95895 |  0:00:04s
epoch 4  | loss: 0.83984 | val_0_rmse: 0.90773 | val_1_rmse: 0.93938 |  0:00:05s
epoch 5  | loss: 0.81538 | val_0_rmse: 0.89829 | val_1_rmse: 0.93445 |  0:00:06s
epoch 6  | loss: 0.77928 | val_0_rmse: 0.89078 | val_1_rmse: 0.92515 |  0:00:07s
epoch 7  | loss: 0.76452 | val_0_rmse: 0.87264 | val_1_rmse: 0.90691 |  0:00:08s
epoch 8  | loss: 0.73814 | val_0_rmse: 0.86422 | val_1_rmse: 0.89774 |  0:00:09s
epoch 9  | loss: 0.72046 | val_0_rmse: 0.87124 | val_1_rmse: 0.907   |  0:00:10s
epoch 10 | loss: 0.71625 | val_0_rmse: 0.87067 | val_1_rmse: 0.90269 |  0:00:11s
epoch 11 | loss: 0.70896 | val_0_rmse: 0.85405 | val_1_rmse: 0.89102 |  0:00:12s
epoch 12 | loss: 0.69283 | val_0_rmse: 0.85318 | val_1_rmse: 0.88769 |  0:00:13s
epoch 13 | loss: 0.68768 | val_0_rmse: 0.86301 | val_1_rmse: 0.89583 |  0:00:14s
epoch 14 | loss: 0.67631 | val_0_rmse: 0.84528 | val_1_rmse: 0.87904 |  0:00:15s
epoch 15 | loss: 0.66583 | val_0_rmse: 0.85349 | val_1_rmse: 0.88516 |  0:00:16s
epoch 16 | loss: 0.65531 | val_0_rmse: 0.83587 | val_1_rmse: 0.87224 |  0:00:17s
epoch 17 | loss: 0.65255 | val_0_rmse: 0.83441 | val_1_rmse: 0.87198 |  0:00:18s
epoch 18 | loss: 0.65744 | val_0_rmse: 0.8288  | val_1_rmse: 0.8706  |  0:00:19s
epoch 19 | loss: 0.6463  | val_0_rmse: 0.83017 | val_1_rmse: 0.86907 |  0:00:20s
epoch 20 | loss: 0.63438 | val_0_rmse: 0.82542 | val_1_rmse: 0.86925 |  0:00:21s
epoch 21 | loss: 0.6287  | val_0_rmse: 0.82931 | val_1_rmse: 0.87032 |  0:00:22s
epoch 22 | loss: 0.62087 | val_0_rmse: 0.8164  | val_1_rmse: 0.86161 |  0:00:23s
epoch 23 | loss: 0.62031 | val_0_rmse: 0.82059 | val_1_rmse: 0.86104 |  0:00:24s
epoch 24 | loss: 0.62517 | val_0_rmse: 0.81419 | val_1_rmse: 0.85881 |  0:00:25s
epoch 25 | loss: 0.61412 | val_0_rmse: 0.82432 | val_1_rmse: 0.87665 |  0:00:26s
epoch 26 | loss: 0.61141 | val_0_rmse: 0.80688 | val_1_rmse: 0.84896 |  0:00:27s
epoch 27 | loss: 0.61568 | val_0_rmse: 0.80435 | val_1_rmse: 0.8514  |  0:00:28s
epoch 28 | loss: 0.61245 | val_0_rmse: 0.83625 | val_1_rmse: 0.88317 |  0:00:29s
epoch 29 | loss: 0.60451 | val_0_rmse: 0.79692 | val_1_rmse: 0.85728 |  0:00:30s
epoch 30 | loss: 0.59599 | val_0_rmse: 0.7868  | val_1_rmse: 0.84667 |  0:00:31s
epoch 31 | loss: 0.59633 | val_0_rmse: 0.79577 | val_1_rmse: 0.84729 |  0:00:32s
epoch 32 | loss: 0.59547 | val_0_rmse: 0.78538 | val_1_rmse: 0.85349 |  0:00:34s
epoch 33 | loss: 0.59527 | val_0_rmse: 0.78018 | val_1_rmse: 0.83932 |  0:00:35s
epoch 34 | loss: 0.58911 | val_0_rmse: 0.77505 | val_1_rmse: 0.83725 |  0:00:36s
epoch 35 | loss: 0.58475 | val_0_rmse: 0.78765 | val_1_rmse: 0.85059 |  0:00:37s
epoch 36 | loss: 0.58204 | val_0_rmse: 0.76396 | val_1_rmse: 0.83453 |  0:00:38s
epoch 37 | loss: 0.56502 | val_0_rmse: 0.76983 | val_1_rmse: 0.84059 |  0:00:39s
epoch 38 | loss: 0.56496 | val_0_rmse: 0.77139 | val_1_rmse: 0.85004 |  0:00:40s
epoch 39 | loss: 0.56529 | val_0_rmse: 0.74854 | val_1_rmse: 0.83399 |  0:00:41s
epoch 40 | loss: 0.55887 | val_0_rmse: 0.76411 | val_1_rmse: 0.84503 |  0:00:42s
epoch 41 | loss: 0.57884 | val_0_rmse: 0.75426 | val_1_rmse: 0.83705 |  0:00:43s
epoch 42 | loss: 0.56643 | val_0_rmse: 0.74894 | val_1_rmse: 0.8353  |  0:00:44s
epoch 43 | loss: 0.55999 | val_0_rmse: 0.74488 | val_1_rmse: 0.83633 |  0:00:45s
epoch 44 | loss: 0.55018 | val_0_rmse: 0.73902 | val_1_rmse: 0.83777 |  0:00:46s
epoch 45 | loss: 0.54922 | val_0_rmse: 0.72983 | val_1_rmse: 0.8285  |  0:00:47s
epoch 46 | loss: 0.55544 | val_0_rmse: 0.72664 | val_1_rmse: 0.83803 |  0:00:48s
epoch 47 | loss: 0.54885 | val_0_rmse: 0.72841 | val_1_rmse: 0.84657 |  0:00:49s
epoch 48 | loss: 0.54762 | val_0_rmse: 0.72679 | val_1_rmse: 0.83573 |  0:00:50s
epoch 49 | loss: 0.53862 | val_0_rmse: 0.71875 | val_1_rmse: 0.83081 |  0:00:51s
epoch 50 | loss: 0.53911 | val_0_rmse: 0.72875 | val_1_rmse: 0.84538 |  0:00:52s
epoch 51 | loss: 0.55705 | val_0_rmse: 0.73738 | val_1_rmse: 0.84453 |  0:00:53s
epoch 52 | loss: 0.55418 | val_0_rmse: 0.7425  | val_1_rmse: 0.84042 |  0:00:54s
epoch 53 | loss: 0.55608 | val_0_rmse: 0.71954 | val_1_rmse: 0.8499  |  0:00:55s
epoch 54 | loss: 0.54645 | val_0_rmse: 0.71274 | val_1_rmse: 0.83931 |  0:00:56s
epoch 55 | loss: 0.5367  | val_0_rmse: 0.73627 | val_1_rmse: 0.84961 |  0:00:57s
epoch 56 | loss: 0.53891 | val_0_rmse: 0.72173 | val_1_rmse: 0.86374 |  0:00:58s
epoch 57 | loss: 0.52391 | val_0_rmse: 0.7022  | val_1_rmse: 0.83824 |  0:00:59s
epoch 58 | loss: 0.52315 | val_0_rmse: 0.70582 | val_1_rmse: 0.8467  |  0:01:00s
epoch 59 | loss: 0.52134 | val_0_rmse: 0.7152  | val_1_rmse: 0.86346 |  0:01:01s
epoch 60 | loss: 0.51468 | val_0_rmse: 0.6966  | val_1_rmse: 0.84587 |  0:01:02s
epoch 61 | loss: 0.52106 | val_0_rmse: 0.70045 | val_1_rmse: 0.83158 |  0:01:03s
epoch 62 | loss: 0.51762 | val_0_rmse: 0.7165  | val_1_rmse: 0.85167 |  0:01:04s
epoch 63 | loss: 0.52085 | val_0_rmse: 0.69236 | val_1_rmse: 0.8302  |  0:01:05s
epoch 64 | loss: 0.52011 | val_0_rmse: 0.69915 | val_1_rmse: 0.83923 |  0:01:06s
epoch 65 | loss: 0.51545 | val_0_rmse: 0.69881 | val_1_rmse: 0.87346 |  0:01:07s
epoch 66 | loss: 0.52347 | val_0_rmse: 0.69392 | val_1_rmse: 0.84197 |  0:01:08s
epoch 67 | loss: 0.50801 | val_0_rmse: 0.68414 | val_1_rmse: 0.83919 |  0:01:09s
epoch 68 | loss: 0.49987 | val_0_rmse: 0.68787 | val_1_rmse: 0.83443 |  0:01:11s
epoch 69 | loss: 0.51515 | val_0_rmse: 0.69894 | val_1_rmse: 0.85089 |  0:01:12s
epoch 70 | loss: 0.51231 | val_0_rmse: 0.68314 | val_1_rmse: 0.85584 |  0:01:13s
epoch 71 | loss: 0.51233 | val_0_rmse: 0.68643 | val_1_rmse: 0.84166 |  0:01:14s
epoch 72 | loss: 0.5044  | val_0_rmse: 0.69748 | val_1_rmse: 0.84679 |  0:01:15s
epoch 73 | loss: 0.50105 | val_0_rmse: 0.72739 | val_1_rmse: 0.86784 |  0:01:16s
epoch 74 | loss: 0.52035 | val_0_rmse: 0.70669 | val_1_rmse: 0.8397  |  0:01:17s
epoch 75 | loss: 0.51062 | val_0_rmse: 0.7019  | val_1_rmse: 0.86089 |  0:01:18s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.8285
Best weights from best epoch are automatically used!
ended training at: 00:33:47
Feature importance:
Mean squared error is of 0.0948582903481346
Mean absolute error:0.1873687036015956
MAPE:0.20115783140664548
R2 score:0.22807145540367235
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:33:48
epoch 0  | loss: 2.36969 | val_0_rmse: 0.98053 | val_1_rmse: 0.91386 |  0:00:01s
epoch 1  | loss: 1.00694 | val_0_rmse: 0.97983 | val_1_rmse: 0.91415 |  0:00:02s
epoch 2  | loss: 0.93403 | val_0_rmse: 0.97805 | val_1_rmse: 0.91195 |  0:00:03s
epoch 3  | loss: 0.91125 | val_0_rmse: 0.95684 | val_1_rmse: 0.88844 |  0:00:04s
epoch 4  | loss: 0.88981 | val_0_rmse: 0.93632 | val_1_rmse: 0.87153 |  0:00:05s
epoch 5  | loss: 0.87976 | val_0_rmse: 0.92477 | val_1_rmse: 0.85555 |  0:00:06s
epoch 6  | loss: 0.84924 | val_0_rmse: 0.91018 | val_1_rmse: 0.84235 |  0:00:07s
epoch 7  | loss: 0.82465 | val_0_rmse: 0.89681 | val_1_rmse: 0.83378 |  0:00:08s
epoch 8  | loss: 0.81157 | val_0_rmse: 0.90259 | val_1_rmse: 0.83031 |  0:00:09s
epoch 9  | loss: 0.7983  | val_0_rmse: 0.88349 | val_1_rmse: 0.81056 |  0:00:10s
epoch 10 | loss: 0.77878 | val_0_rmse: 0.89784 | val_1_rmse: 0.82745 |  0:00:11s
epoch 11 | loss: 0.76273 | val_0_rmse: 0.887   | val_1_rmse: 0.81445 |  0:00:12s
epoch 12 | loss: 0.74822 | val_0_rmse: 0.87638 | val_1_rmse: 0.80376 |  0:00:13s
epoch 13 | loss: 0.74312 | val_0_rmse: 0.88408 | val_1_rmse: 0.81231 |  0:00:14s
epoch 14 | loss: 0.7432  | val_0_rmse: 0.88609 | val_1_rmse: 0.8111  |  0:00:15s
epoch 15 | loss: 0.72985 | val_0_rmse: 0.87983 | val_1_rmse: 0.80835 |  0:00:16s
epoch 16 | loss: 0.72893 | val_0_rmse: 0.88427 | val_1_rmse: 0.81521 |  0:00:17s
epoch 17 | loss: 0.72307 | val_0_rmse: 0.87101 | val_1_rmse: 0.80111 |  0:00:18s
epoch 18 | loss: 0.71817 | val_0_rmse: 0.86731 | val_1_rmse: 0.79681 |  0:00:19s
epoch 19 | loss: 0.70875 | val_0_rmse: 0.86838 | val_1_rmse: 0.79766 |  0:00:20s
epoch 20 | loss: 0.71056 | val_0_rmse: 0.87135 | val_1_rmse: 0.80431 |  0:00:21s
epoch 21 | loss: 0.70468 | val_0_rmse: 0.86189 | val_1_rmse: 0.79147 |  0:00:22s
epoch 22 | loss: 0.69728 | val_0_rmse: 0.87414 | val_1_rmse: 0.80945 |  0:00:23s
epoch 23 | loss: 0.69451 | val_0_rmse: 0.85365 | val_1_rmse: 0.78694 |  0:00:24s
epoch 24 | loss: 0.69505 | val_0_rmse: 0.8557  | val_1_rmse: 0.79132 |  0:00:25s
epoch 25 | loss: 0.68786 | val_0_rmse: 0.85242 | val_1_rmse: 0.78949 |  0:00:26s
epoch 26 | loss: 0.6818  | val_0_rmse: 0.84472 | val_1_rmse: 0.7805  |  0:00:27s
epoch 27 | loss: 0.67675 | val_0_rmse: 0.84964 | val_1_rmse: 0.78436 |  0:00:28s
epoch 28 | loss: 0.67204 | val_0_rmse: 0.84262 | val_1_rmse: 0.78233 |  0:00:29s
epoch 29 | loss: 0.67224 | val_0_rmse: 0.8391  | val_1_rmse: 0.77881 |  0:00:30s
epoch 30 | loss: 0.66132 | val_0_rmse: 0.84931 | val_1_rmse: 0.78608 |  0:00:31s
epoch 31 | loss: 0.6676  | val_0_rmse: 0.82872 | val_1_rmse: 0.77475 |  0:00:32s
epoch 32 | loss: 0.65793 | val_0_rmse: 0.83027 | val_1_rmse: 0.77244 |  0:00:33s
epoch 33 | loss: 0.65145 | val_0_rmse: 0.82413 | val_1_rmse: 0.77097 |  0:00:34s
epoch 34 | loss: 0.64892 | val_0_rmse: 0.82699 | val_1_rmse: 0.77651 |  0:00:36s
epoch 35 | loss: 0.65323 | val_0_rmse: 0.81156 | val_1_rmse: 0.76337 |  0:00:37s
epoch 36 | loss: 0.63777 | val_0_rmse: 0.8053  | val_1_rmse: 0.76507 |  0:00:38s
epoch 37 | loss: 0.6416  | val_0_rmse: 0.80278 | val_1_rmse: 0.76136 |  0:00:39s
epoch 38 | loss: 0.63954 | val_0_rmse: 0.7959  | val_1_rmse: 0.76591 |  0:00:40s
epoch 39 | loss: 0.63905 | val_0_rmse: 0.79981 | val_1_rmse: 0.76801 |  0:00:41s
epoch 40 | loss: 0.63733 | val_0_rmse: 0.80531 | val_1_rmse: 0.76743 |  0:00:42s
epoch 41 | loss: 0.62087 | val_0_rmse: 0.78703 | val_1_rmse: 0.76449 |  0:00:43s
epoch 42 | loss: 0.62894 | val_0_rmse: 0.7907  | val_1_rmse: 0.76213 |  0:00:44s
epoch 43 | loss: 0.62754 | val_0_rmse: 0.78453 | val_1_rmse: 0.77216 |  0:00:45s
epoch 44 | loss: 0.62152 | val_0_rmse: 0.77777 | val_1_rmse: 0.76508 |  0:00:46s
epoch 45 | loss: 0.61526 | val_0_rmse: 0.77479 | val_1_rmse: 0.76358 |  0:00:47s
epoch 46 | loss: 0.62128 | val_0_rmse: 0.7759  | val_1_rmse: 0.76534 |  0:00:48s
epoch 47 | loss: 0.6083  | val_0_rmse: 0.76497 | val_1_rmse: 0.76669 |  0:00:49s
epoch 48 | loss: 0.61113 | val_0_rmse: 0.77688 | val_1_rmse: 0.77344 |  0:00:50s
epoch 49 | loss: 0.60998 | val_0_rmse: 0.76043 | val_1_rmse: 0.76852 |  0:00:51s
epoch 50 | loss: 0.60848 | val_0_rmse: 0.77616 | val_1_rmse: 0.76432 |  0:00:52s
epoch 51 | loss: 0.59753 | val_0_rmse: 0.76095 | val_1_rmse: 0.77496 |  0:00:53s
epoch 52 | loss: 0.60407 | val_0_rmse: 0.76232 | val_1_rmse: 0.77292 |  0:00:54s
epoch 53 | loss: 0.60101 | val_0_rmse: 0.76521 | val_1_rmse: 0.76793 |  0:00:55s
epoch 54 | loss: 0.59301 | val_0_rmse: 0.76316 | val_1_rmse: 0.76018 |  0:00:56s
epoch 55 | loss: 0.59981 | val_0_rmse: 0.75663 | val_1_rmse: 0.79333 |  0:00:57s
epoch 56 | loss: 0.596   | val_0_rmse: 0.76067 | val_1_rmse: 0.7814  |  0:00:58s
epoch 57 | loss: 0.59207 | val_0_rmse: 0.75016 | val_1_rmse: 0.77361 |  0:00:59s
epoch 58 | loss: 0.58475 | val_0_rmse: 0.74065 | val_1_rmse: 0.77228 |  0:01:00s
epoch 59 | loss: 0.58341 | val_0_rmse: 0.75283 | val_1_rmse: 0.76409 |  0:01:01s
epoch 60 | loss: 0.58296 | val_0_rmse: 0.74921 | val_1_rmse: 0.76615 |  0:01:02s
epoch 61 | loss: 0.57821 | val_0_rmse: 0.75218 | val_1_rmse: 0.78009 |  0:01:03s
epoch 62 | loss: 0.57126 | val_0_rmse: 0.73653 | val_1_rmse: 0.77696 |  0:01:04s
epoch 63 | loss: 0.57269 | val_0_rmse: 0.74659 | val_1_rmse: 0.7787  |  0:01:05s
epoch 64 | loss: 0.57593 | val_0_rmse: 0.74892 | val_1_rmse: 0.77443 |  0:01:06s
epoch 65 | loss: 0.56819 | val_0_rmse: 0.73012 | val_1_rmse: 0.77579 |  0:01:07s
epoch 66 | loss: 0.57101 | val_0_rmse: 0.72922 | val_1_rmse: 0.76551 |  0:01:08s
epoch 67 | loss: 0.55969 | val_0_rmse: 0.74138 | val_1_rmse: 0.76689 |  0:01:09s
epoch 68 | loss: 0.58115 | val_0_rmse: 0.73689 | val_1_rmse: 0.7818  |  0:01:10s
epoch 69 | loss: 0.56842 | val_0_rmse: 0.73074 | val_1_rmse: 0.78703 |  0:01:11s
epoch 70 | loss: 0.5655  | val_0_rmse: 0.73472 | val_1_rmse: 0.79157 |  0:01:12s
epoch 71 | loss: 0.55999 | val_0_rmse: 0.7242  | val_1_rmse: 0.77379 |  0:01:13s
epoch 72 | loss: 0.55188 | val_0_rmse: 0.71574 | val_1_rmse: 0.79405 |  0:01:14s
epoch 73 | loss: 0.54756 | val_0_rmse: 0.71461 | val_1_rmse: 0.771   |  0:01:15s
epoch 74 | loss: 0.5447  | val_0_rmse: 0.71067 | val_1_rmse: 0.77948 |  0:01:16s
epoch 75 | loss: 0.54254 | val_0_rmse: 0.72393 | val_1_rmse: 0.77716 |  0:01:18s
epoch 76 | loss: 0.5468  | val_0_rmse: 0.7387  | val_1_rmse: 0.76719 |  0:01:19s
epoch 77 | loss: 0.56952 | val_0_rmse: 0.72515 | val_1_rmse: 0.77428 |  0:01:20s
epoch 78 | loss: 0.55883 | val_0_rmse: 0.72075 | val_1_rmse: 0.76769 |  0:01:21s
epoch 79 | loss: 0.53993 | val_0_rmse: 0.72403 | val_1_rmse: 0.79638 |  0:01:22s
epoch 80 | loss: 0.54934 | val_0_rmse: 0.7308  | val_1_rmse: 0.81835 |  0:01:23s
epoch 81 | loss: 0.60427 | val_0_rmse: 0.74604 | val_1_rmse: 0.80044 |  0:01:24s
epoch 82 | loss: 0.57465 | val_0_rmse: 0.73966 | val_1_rmse: 0.77597 |  0:01:25s
epoch 83 | loss: 0.56909 | val_0_rmse: 0.73912 | val_1_rmse: 0.78369 |  0:01:26s
epoch 84 | loss: 0.55818 | val_0_rmse: 0.7215  | val_1_rmse: 0.79405 |  0:01:27s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.76018
Best weights from best epoch are automatically used!
ended training at: 00:35:15
Feature importance:
Mean squared error is of 0.0965174039882359
Mean absolute error:0.18570152899567988
MAPE:0.20032640343090657
R2 score:0.19097903670855831
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:35:16
epoch 0  | loss: 2.44559 | val_0_rmse: 0.99727 | val_1_rmse: 1.03118 |  0:00:01s
epoch 1  | loss: 1.02882 | val_0_rmse: 0.99827 | val_1_rmse: 1.03446 |  0:00:02s
epoch 2  | loss: 0.9731  | val_0_rmse: 0.98237 | val_1_rmse: 1.01765 |  0:00:03s
epoch 3  | loss: 0.94077 | val_0_rmse: 0.97337 | val_1_rmse: 1.00648 |  0:00:04s
epoch 4  | loss: 0.92247 | val_0_rmse: 0.96573 | val_1_rmse: 0.99782 |  0:00:05s
epoch 5  | loss: 0.90042 | val_0_rmse: 0.97049 | val_1_rmse: 1.00394 |  0:00:06s
epoch 6  | loss: 0.88466 | val_0_rmse: 0.95141 | val_1_rmse: 0.99071 |  0:00:07s
epoch 7  | loss: 0.87478 | val_0_rmse: 0.94381 | val_1_rmse: 0.9759  |  0:00:08s
epoch 8  | loss: 0.87545 | val_0_rmse: 0.94098 | val_1_rmse: 0.97335 |  0:00:09s
epoch 9  | loss: 0.86102 | val_0_rmse: 0.94516 | val_1_rmse: 0.97959 |  0:00:10s
epoch 10 | loss: 0.84796 | val_0_rmse: 0.92747 | val_1_rmse: 0.96343 |  0:00:11s
epoch 11 | loss: 0.83913 | val_0_rmse: 0.91529 | val_1_rmse: 0.95268 |  0:00:12s
epoch 12 | loss: 0.82513 | val_0_rmse: 0.91501 | val_1_rmse: 0.95836 |  0:00:13s
epoch 13 | loss: 0.8171  | val_0_rmse: 0.91206 | val_1_rmse: 0.95013 |  0:00:14s
epoch 14 | loss: 0.79965 | val_0_rmse: 0.90672 | val_1_rmse: 0.94116 |  0:00:15s
epoch 15 | loss: 0.80169 | val_0_rmse: 0.91265 | val_1_rmse: 0.94581 |  0:00:16s
epoch 16 | loss: 0.79073 | val_0_rmse: 0.89832 | val_1_rmse: 0.93677 |  0:00:17s
epoch 17 | loss: 0.78062 | val_0_rmse: 0.91005 | val_1_rmse: 0.94042 |  0:00:18s
epoch 18 | loss: 0.78265 | val_0_rmse: 0.90027 | val_1_rmse: 0.92647 |  0:00:19s
epoch 19 | loss: 0.7995  | val_0_rmse: 0.87969 | val_1_rmse: 0.91525 |  0:00:20s
epoch 20 | loss: 0.77422 | val_0_rmse: 0.88003 | val_1_rmse: 0.91708 |  0:00:21s
epoch 21 | loss: 0.77936 | val_0_rmse: 0.89668 | val_1_rmse: 0.93107 |  0:00:22s
epoch 22 | loss: 0.79186 | val_0_rmse: 0.88982 | val_1_rmse: 0.92383 |  0:00:23s
epoch 23 | loss: 0.76877 | val_0_rmse: 0.87474 | val_1_rmse: 0.91022 |  0:00:24s
epoch 24 | loss: 0.75281 | val_0_rmse: 0.87795 | val_1_rmse: 0.91183 |  0:00:25s
epoch 25 | loss: 0.75091 | val_0_rmse: 0.87833 | val_1_rmse: 0.91195 |  0:00:26s
epoch 26 | loss: 0.73737 | val_0_rmse: 0.86801 | val_1_rmse: 0.89774 |  0:00:27s
epoch 27 | loss: 0.72049 | val_0_rmse: 0.8565  | val_1_rmse: 0.89351 |  0:00:28s
epoch 28 | loss: 0.73612 | val_0_rmse: 0.85717 | val_1_rmse: 0.89429 |  0:00:29s
epoch 29 | loss: 0.72279 | val_0_rmse: 0.85407 | val_1_rmse: 0.89074 |  0:00:30s
epoch 30 | loss: 0.71661 | val_0_rmse: 0.85501 | val_1_rmse: 0.88795 |  0:00:31s
epoch 31 | loss: 0.70545 | val_0_rmse: 0.84535 | val_1_rmse: 0.88584 |  0:00:33s
epoch 32 | loss: 0.70806 | val_0_rmse: 0.85581 | val_1_rmse: 0.89818 |  0:00:34s
epoch 33 | loss: 0.71423 | val_0_rmse: 0.8496  | val_1_rmse: 0.89058 |  0:00:35s
epoch 34 | loss: 0.71149 | val_0_rmse: 0.85264 | val_1_rmse: 0.89956 |  0:00:36s
epoch 35 | loss: 0.69695 | val_0_rmse: 0.84894 | val_1_rmse: 0.89043 |  0:00:37s
epoch 36 | loss: 0.70746 | val_0_rmse: 0.84684 | val_1_rmse: 0.88911 |  0:00:38s
epoch 37 | loss: 0.69051 | val_0_rmse: 0.84585 | val_1_rmse: 0.90077 |  0:00:39s
epoch 38 | loss: 0.69081 | val_0_rmse: 0.83301 | val_1_rmse: 0.89306 |  0:00:40s
epoch 39 | loss: 0.69291 | val_0_rmse: 0.82838 | val_1_rmse: 0.88418 |  0:00:41s
epoch 40 | loss: 0.69486 | val_0_rmse: 0.83082 | val_1_rmse: 0.89244 |  0:00:42s
epoch 41 | loss: 0.68781 | val_0_rmse: 0.83182 | val_1_rmse: 0.88849 |  0:00:43s
epoch 42 | loss: 0.68672 | val_0_rmse: 0.82477 | val_1_rmse: 0.88183 |  0:00:44s
epoch 43 | loss: 0.67444 | val_0_rmse: 0.81753 | val_1_rmse: 0.89116 |  0:00:45s
epoch 44 | loss: 0.68835 | val_0_rmse: 0.81445 | val_1_rmse: 0.88057 |  0:00:46s
epoch 45 | loss: 0.682   | val_0_rmse: 0.81587 | val_1_rmse: 0.90858 |  0:00:47s
epoch 46 | loss: 0.66736 | val_0_rmse: 0.80367 | val_1_rmse: 0.89828 |  0:00:48s
epoch 47 | loss: 0.65346 | val_0_rmse: 0.80578 | val_1_rmse: 0.95413 |  0:00:49s
epoch 48 | loss: 0.67207 | val_0_rmse: 0.80295 | val_1_rmse: 0.97861 |  0:00:50s
epoch 49 | loss: 0.67252 | val_0_rmse: 0.8151  | val_1_rmse: 0.98367 |  0:00:51s
epoch 50 | loss: 0.68683 | val_0_rmse: 0.80035 | val_1_rmse: 0.92268 |  0:00:52s
epoch 51 | loss: 0.6768  | val_0_rmse: 0.79805 | val_1_rmse: 0.91614 |  0:00:53s
epoch 52 | loss: 0.67242 | val_0_rmse: 0.80419 | val_1_rmse: 0.93752 |  0:00:54s
epoch 53 | loss: 0.6546  | val_0_rmse: 0.7943  | val_1_rmse: 0.9335  |  0:00:55s
epoch 54 | loss: 0.65571 | val_0_rmse: 0.80054 | val_1_rmse: 0.96546 |  0:00:56s
epoch 55 | loss: 0.65592 | val_0_rmse: 0.78044 | val_1_rmse: 0.94254 |  0:00:57s
epoch 56 | loss: 0.65798 | val_0_rmse: 0.7833  | val_1_rmse: 0.94704 |  0:00:58s
epoch 57 | loss: 0.66406 | val_0_rmse: 0.80086 | val_1_rmse: 0.94979 |  0:00:59s
epoch 58 | loss: 0.68634 | val_0_rmse: 0.82837 | val_1_rmse: 0.89695 |  0:01:00s
epoch 59 | loss: 0.69053 | val_0_rmse: 0.81551 | val_1_rmse: 0.88951 |  0:01:01s
epoch 60 | loss: 0.67348 | val_0_rmse: 0.79576 | val_1_rmse: 0.89535 |  0:01:02s
epoch 61 | loss: 0.65647 | val_0_rmse: 0.81911 | val_1_rmse: 0.97854 |  0:01:03s
epoch 62 | loss: 0.65371 | val_0_rmse: 0.79544 | val_1_rmse: 1.07388 |  0:01:05s
epoch 63 | loss: 0.64992 | val_0_rmse: 0.79355 | val_1_rmse: 1.2836  |  0:01:06s
epoch 64 | loss: 0.63603 | val_0_rmse: 0.80516 | val_1_rmse: 1.2802  |  0:01:07s
epoch 65 | loss: 0.6175  | val_0_rmse: 0.80734 | val_1_rmse: 1.40781 |  0:01:08s
epoch 66 | loss: 0.61486 | val_0_rmse: 0.83493 | val_1_rmse: 1.55961 |  0:01:09s
epoch 67 | loss: 0.62894 | val_0_rmse: 0.82074 | val_1_rmse: 1.47392 |  0:01:10s
epoch 68 | loss: 0.62071 | val_0_rmse: 0.78931 | val_1_rmse: 1.28128 |  0:01:11s
epoch 69 | loss: 0.62701 | val_0_rmse: 0.83727 | val_1_rmse: 1.60122 |  0:01:12s
epoch 70 | loss: 0.61639 | val_0_rmse: 0.87309 | val_1_rmse: 1.78943 |  0:01:13s
epoch 71 | loss: 0.60727 | val_0_rmse: 0.89293 | val_1_rmse: 1.74615 |  0:01:14s
epoch 72 | loss: 0.62336 | val_0_rmse: 0.90896 | val_1_rmse: 1.93195 |  0:01:15s
epoch 73 | loss: 0.61186 | val_0_rmse: 0.89661 | val_1_rmse: 1.6948  |  0:01:16s
epoch 74 | loss: 0.62044 | val_0_rmse: 2.84493 | val_1_rmse: 2.00157 |  0:01:17s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.88057
Best weights from best epoch are automatically used!
ended training at: 00:36:34
Feature importance:
Mean squared error is of 0.06533930563236046
Mean absolute error:0.18320121300789483
MAPE:0.2052259850502935
R2 score:0.2589658505477831
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:36:34
epoch 0  | loss: 2.42981 | val_0_rmse: 1.03266 | val_1_rmse: 0.93477 |  0:00:01s
epoch 1  | loss: 1.09384 | val_0_rmse: 1.02977 | val_1_rmse: 0.93274 |  0:00:02s
epoch 2  | loss: 1.05826 | val_0_rmse: 1.02749 | val_1_rmse: 0.93133 |  0:00:03s
epoch 3  | loss: 1.02318 | val_0_rmse: 1.0276  | val_1_rmse: 0.92671 |  0:00:04s
epoch 4  | loss: 1.00032 | val_0_rmse: 1.00282 | val_1_rmse: 0.89489 |  0:00:05s
epoch 5  | loss: 0.95366 | val_0_rmse: 1.00449 | val_1_rmse: 0.89243 |  0:00:06s
epoch 6  | loss: 0.94131 | val_0_rmse: 0.98676 | val_1_rmse: 0.87208 |  0:00:07s
epoch 7  | loss: 0.92845 | val_0_rmse: 0.98024 | val_1_rmse: 0.87433 |  0:00:08s
epoch 8  | loss: 0.90908 | val_0_rmse: 0.96391 | val_1_rmse: 0.85527 |  0:00:09s
epoch 9  | loss: 0.89124 | val_0_rmse: 0.97528 | val_1_rmse: 0.85305 |  0:00:10s
epoch 10 | loss: 0.89236 | val_0_rmse: 0.93933 | val_1_rmse: 0.83278 |  0:00:11s
epoch 11 | loss: 0.87182 | val_0_rmse: 0.93143 | val_1_rmse: 0.826   |  0:00:12s
epoch 12 | loss: 0.86391 | val_0_rmse: 0.93309 | val_1_rmse: 0.83036 |  0:00:13s
epoch 13 | loss: 0.84485 | val_0_rmse: 0.92508 | val_1_rmse: 0.81451 |  0:00:14s
epoch 14 | loss: 0.84555 | val_0_rmse: 0.93213 | val_1_rmse: 0.8176  |  0:00:15s
epoch 15 | loss: 0.83275 | val_0_rmse: 0.91854 | val_1_rmse: 0.8108  |  0:00:16s
epoch 16 | loss: 0.83875 | val_0_rmse: 0.92568 | val_1_rmse: 0.82391 |  0:00:17s
epoch 17 | loss: 0.84004 | val_0_rmse: 0.94348 | val_1_rmse: 0.83376 |  0:00:18s
epoch 18 | loss: 0.83307 | val_0_rmse: 0.9319  | val_1_rmse: 0.81914 |  0:00:19s
epoch 19 | loss: 0.82169 | val_0_rmse: 0.92632 | val_1_rmse: 0.8306  |  0:00:20s
epoch 20 | loss: 0.81201 | val_0_rmse: 0.9083  | val_1_rmse: 0.81299 |  0:00:21s
epoch 21 | loss: 0.80304 | val_0_rmse: 0.91428 | val_1_rmse: 0.8255  |  0:00:22s
epoch 22 | loss: 0.80067 | val_0_rmse: 0.90422 | val_1_rmse: 0.79814 |  0:00:23s
epoch 23 | loss: 0.80315 | val_0_rmse: 0.91861 | val_1_rmse: 0.82159 |  0:00:24s
epoch 24 | loss: 0.79286 | val_0_rmse: 0.91958 | val_1_rmse: 0.82422 |  0:00:25s
epoch 25 | loss: 0.78309 | val_0_rmse: 0.90491 | val_1_rmse: 0.80305 |  0:00:26s
epoch 26 | loss: 0.79481 | val_0_rmse: 0.91194 | val_1_rmse: 0.8103  |  0:00:27s
epoch 27 | loss: 0.80044 | val_0_rmse: 0.91833 | val_1_rmse: 0.81473 |  0:00:29s
epoch 28 | loss: 0.79177 | val_0_rmse: 0.89653 | val_1_rmse: 0.80608 |  0:00:30s
epoch 29 | loss: 0.79142 | val_0_rmse: 0.89787 | val_1_rmse: 0.79896 |  0:00:31s
epoch 30 | loss: 0.79162 | val_0_rmse: 0.89644 | val_1_rmse: 0.7984  |  0:00:32s
epoch 31 | loss: 0.76805 | val_0_rmse: 0.88315 | val_1_rmse: 0.79023 |  0:00:33s
epoch 32 | loss: 0.75738 | val_0_rmse: 0.87462 | val_1_rmse: 0.78846 |  0:00:34s
epoch 33 | loss: 0.75693 | val_0_rmse: 0.89698 | val_1_rmse: 0.82778 |  0:00:35s
epoch 34 | loss: 0.75224 | val_0_rmse: 0.87752 | val_1_rmse: 0.79558 |  0:00:36s
epoch 35 | loss: 0.72834 | val_0_rmse: 0.89806 | val_1_rmse: 0.81274 |  0:00:37s
epoch 36 | loss: 0.77696 | val_0_rmse: 0.87183 | val_1_rmse: 0.79532 |  0:00:38s
epoch 37 | loss: 0.75314 | val_0_rmse: 0.87666 | val_1_rmse: 0.79795 |  0:00:39s
epoch 38 | loss: 0.74761 | val_0_rmse: 0.85203 | val_1_rmse: 0.77829 |  0:00:40s
epoch 39 | loss: 0.72867 | val_0_rmse: 0.85111 | val_1_rmse: 0.78513 |  0:00:41s
epoch 40 | loss: 0.74219 | val_0_rmse: 0.8426  | val_1_rmse: 0.78211 |  0:00:42s
epoch 41 | loss: 0.73249 | val_0_rmse: 0.84031 | val_1_rmse: 0.78291 |  0:00:43s
epoch 42 | loss: 0.70878 | val_0_rmse: 0.87736 | val_1_rmse: 0.81287 |  0:00:44s
epoch 43 | loss: 0.7334  | val_0_rmse: 0.85699 | val_1_rmse: 0.80261 |  0:00:45s
epoch 44 | loss: 0.73748 | val_0_rmse: 0.84024 | val_1_rmse: 0.7863  |  0:00:46s
epoch 45 | loss: 0.71398 | val_0_rmse: 0.83659 | val_1_rmse: 0.79654 |  0:00:47s
epoch 46 | loss: 0.72273 | val_0_rmse: 0.83956 | val_1_rmse: 0.79021 |  0:00:48s
epoch 47 | loss: 0.71636 | val_0_rmse: 0.83094 | val_1_rmse: 0.79695 |  0:00:49s
epoch 48 | loss: 0.7169  | val_0_rmse: 0.82697 | val_1_rmse: 0.7992  |  0:00:50s
epoch 49 | loss: 0.68684 | val_0_rmse: 0.83087 | val_1_rmse: 0.79744 |  0:00:51s
epoch 50 | loss: 0.7124  | val_0_rmse: 0.80358 | val_1_rmse: 0.78507 |  0:00:52s
epoch 51 | loss: 0.6864  | val_0_rmse: 0.87347 | val_1_rmse: 0.86288 |  0:00:53s
epoch 52 | loss: 0.70757 | val_0_rmse: 0.8164  | val_1_rmse: 0.79159 |  0:00:54s
epoch 53 | loss: 0.71746 | val_0_rmse: 0.83183 | val_1_rmse: 0.79346 |  0:00:55s
epoch 54 | loss: 0.71186 | val_0_rmse: 0.82022 | val_1_rmse: 0.79269 |  0:00:56s
epoch 55 | loss: 0.68645 | val_0_rmse: 0.81755 | val_1_rmse: 0.81005 |  0:00:57s
epoch 56 | loss: 0.67831 | val_0_rmse: 0.81484 | val_1_rmse: 0.79327 |  0:00:58s
epoch 57 | loss: 0.69535 | val_0_rmse: 0.963   | val_1_rmse: 0.99508 |  0:00:59s
epoch 58 | loss: 0.68756 | val_0_rmse: 0.81236 | val_1_rmse: 0.79884 |  0:01:00s
epoch 59 | loss: 0.69966 | val_0_rmse: 0.80944 | val_1_rmse: 0.80957 |  0:01:01s
epoch 60 | loss: 0.68178 | val_0_rmse: 0.84024 | val_1_rmse: 0.80721 |  0:01:02s
epoch 61 | loss: 0.68534 | val_0_rmse: 0.84095 | val_1_rmse: 0.82232 |  0:01:03s
epoch 62 | loss: 0.74094 | val_0_rmse: 0.85541 | val_1_rmse: 0.81091 |  0:01:04s
epoch 63 | loss: 0.70299 | val_0_rmse: 0.84283 | val_1_rmse: 0.79598 |  0:01:05s
epoch 64 | loss: 0.70272 | val_0_rmse: 0.82837 | val_1_rmse: 0.81483 |  0:01:07s
epoch 65 | loss: 0.71532 | val_0_rmse: 0.83683 | val_1_rmse: 0.79699 |  0:01:08s
epoch 66 | loss: 0.72689 | val_0_rmse: 0.82607 | val_1_rmse: 0.79988 |  0:01:09s
epoch 67 | loss: 0.69581 | val_0_rmse: 0.80878 | val_1_rmse: 0.80793 |  0:01:10s
epoch 68 | loss: 0.6686  | val_0_rmse: 0.86603 | val_1_rmse: 0.85067 |  0:01:11s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.77829
Best weights from best epoch are automatically used!
ended training at: 00:37:46
Feature importance:
Mean squared error is of 0.06537754542045958
Mean absolute error:0.18466094432463734
MAPE:0.1995120937889327
R2 score:0.24933926655604888
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:37:46
epoch 0  | loss: 1.37573 | val_0_rmse: 0.95382 | val_1_rmse: 0.90361 |  0:00:00s
epoch 1  | loss: 0.69785 | val_0_rmse: 0.81381 | val_1_rmse: 0.7801  |  0:00:01s
epoch 2  | loss: 0.54719 | val_0_rmse: 0.83287 | val_1_rmse: 0.81718 |  0:00:02s
epoch 3  | loss: 0.43862 | val_0_rmse: 0.89955 | val_1_rmse: 0.86799 |  0:00:03s
epoch 4  | loss: 0.35187 | val_0_rmse: 1.21207 | val_1_rmse: 1.18185 |  0:00:04s
epoch 5  | loss: 0.33098 | val_0_rmse: 0.84093 | val_1_rmse: 0.82819 |  0:00:05s
epoch 6  | loss: 0.31546 | val_0_rmse: 0.69385 | val_1_rmse: 0.67866 |  0:00:06s
epoch 7  | loss: 0.30665 | val_0_rmse: 0.70158 | val_1_rmse: 0.68895 |  0:00:07s
epoch 8  | loss: 0.29661 | val_0_rmse: 0.68345 | val_1_rmse: 0.66663 |  0:00:08s
epoch 9  | loss: 0.26828 | val_0_rmse: 0.73493 | val_1_rmse: 0.71443 |  0:00:09s
epoch 10 | loss: 0.26448 | val_0_rmse: 0.68597 | val_1_rmse: 0.66393 |  0:00:10s
epoch 11 | loss: 0.27727 | val_0_rmse: 0.58461 | val_1_rmse: 0.55989 |  0:00:11s
epoch 12 | loss: 0.25334 | val_0_rmse: 1.21561 | val_1_rmse: 1.18018 |  0:00:12s
epoch 13 | loss: 0.26948 | val_0_rmse: 0.68193 | val_1_rmse: 0.66793 |  0:00:13s
epoch 14 | loss: 0.26355 | val_0_rmse: 0.59611 | val_1_rmse: 0.582   |  0:00:14s
epoch 15 | loss: 0.25695 | val_0_rmse: 0.58791 | val_1_rmse: 0.57437 |  0:00:15s
epoch 16 | loss: 0.25073 | val_0_rmse: 0.75773 | val_1_rmse: 0.71842 |  0:00:16s
epoch 17 | loss: 0.26033 | val_0_rmse: 0.75619 | val_1_rmse: 0.72798 |  0:00:17s
epoch 18 | loss: 0.24605 | val_0_rmse: 0.84321 | val_1_rmse: 0.80501 |  0:00:18s
epoch 19 | loss: 0.23728 | val_0_rmse: 0.52751 | val_1_rmse: 0.51647 |  0:00:19s
epoch 20 | loss: 0.23107 | val_0_rmse: 0.56762 | val_1_rmse: 0.55953 |  0:00:19s
epoch 21 | loss: 0.24388 | val_0_rmse: 0.52944 | val_1_rmse: 0.52077 |  0:00:20s
epoch 22 | loss: 0.2354  | val_0_rmse: 0.49907 | val_1_rmse: 0.4868  |  0:00:21s
epoch 23 | loss: 0.22554 | val_0_rmse: 0.52916 | val_1_rmse: 0.5076  |  0:00:22s
epoch 24 | loss: 0.22962 | val_0_rmse: 0.49344 | val_1_rmse: 0.46687 |  0:00:23s
epoch 25 | loss: 0.21595 | val_0_rmse: 0.54483 | val_1_rmse: 0.53372 |  0:00:24s
epoch 26 | loss: 0.22993 | val_0_rmse: 0.51658 | val_1_rmse: 0.48797 |  0:00:25s
epoch 27 | loss: 0.23566 | val_0_rmse: 0.49864 | val_1_rmse: 0.47968 |  0:00:26s
epoch 28 | loss: 0.23972 | val_0_rmse: 0.47895 | val_1_rmse: 0.45012 |  0:00:27s
epoch 29 | loss: 0.25964 | val_0_rmse: 0.54045 | val_1_rmse: 0.52146 |  0:00:28s
epoch 30 | loss: 0.25814 | val_0_rmse: 0.50027 | val_1_rmse: 0.47083 |  0:00:29s
epoch 31 | loss: 0.24927 | val_0_rmse: 0.52191 | val_1_rmse: 0.49041 |  0:00:30s
epoch 32 | loss: 0.22919 | val_0_rmse: 0.46784 | val_1_rmse: 0.44862 |  0:00:31s
epoch 33 | loss: 0.21896 | val_0_rmse: 0.46799 | val_1_rmse: 0.44855 |  0:00:32s
epoch 34 | loss: 0.22729 | val_0_rmse: 0.64996 | val_1_rmse: 0.63819 |  0:00:33s
epoch 35 | loss: 0.24755 | val_0_rmse: 0.5613  | val_1_rmse: 0.54137 |  0:00:34s
epoch 36 | loss: 0.27839 | val_0_rmse: 0.55944 | val_1_rmse: 0.53927 |  0:00:35s
epoch 37 | loss: 0.29938 | val_0_rmse: 0.52776 | val_1_rmse: 0.50809 |  0:00:36s
epoch 38 | loss: 0.28574 | val_0_rmse: 0.65426 | val_1_rmse: 0.6236  |  0:00:37s
epoch 39 | loss: 0.26203 | val_0_rmse: 0.56156 | val_1_rmse: 0.52614 |  0:00:37s
epoch 40 | loss: 0.25173 | val_0_rmse: 0.5214  | val_1_rmse: 0.48732 |  0:00:38s
epoch 41 | loss: 0.25337 | val_0_rmse: 0.45768 | val_1_rmse: 0.44063 |  0:00:39s
epoch 42 | loss: 0.23077 | val_0_rmse: 0.44022 | val_1_rmse: 0.42085 |  0:00:40s
epoch 43 | loss: 0.22784 | val_0_rmse: 0.5119  | val_1_rmse: 0.50125 |  0:00:41s
epoch 44 | loss: 0.2436  | val_0_rmse: 0.62055 | val_1_rmse: 0.60763 |  0:00:42s
epoch 45 | loss: 0.22327 | val_0_rmse: 0.4551  | val_1_rmse: 0.43429 |  0:00:43s
epoch 46 | loss: 0.2377  | val_0_rmse: 0.50639 | val_1_rmse: 0.51175 |  0:00:44s
epoch 47 | loss: 0.22188 | val_0_rmse: 0.44133 | val_1_rmse: 0.43553 |  0:00:45s
epoch 48 | loss: 0.20607 | val_0_rmse: 0.4451  | val_1_rmse: 0.43451 |  0:00:46s
epoch 49 | loss: 0.20207 | val_0_rmse: 0.43321 | val_1_rmse: 0.40971 |  0:00:47s
epoch 50 | loss: 0.20018 | val_0_rmse: 0.42931 | val_1_rmse: 0.41816 |  0:00:48s
epoch 51 | loss: 0.19899 | val_0_rmse: 0.42905 | val_1_rmse: 0.41642 |  0:00:49s
epoch 52 | loss: 0.19473 | val_0_rmse: 0.41109 | val_1_rmse: 0.39721 |  0:00:50s
epoch 53 | loss: 0.19774 | val_0_rmse: 0.55428 | val_1_rmse: 0.55102 |  0:00:51s
epoch 54 | loss: 0.20628 | val_0_rmse: 0.43997 | val_1_rmse: 0.42476 |  0:00:52s
epoch 55 | loss: 0.20533 | val_0_rmse: 0.47807 | val_1_rmse: 0.47559 |  0:00:53s
epoch 56 | loss: 0.20496 | val_0_rmse: 0.42365 | val_1_rmse: 0.4185  |  0:00:54s
epoch 57 | loss: 0.1977  | val_0_rmse: 0.4513  | val_1_rmse: 0.43875 |  0:00:55s
epoch 58 | loss: 0.1893  | val_0_rmse: 0.4485  | val_1_rmse: 0.44667 |  0:00:55s
epoch 59 | loss: 0.19145 | val_0_rmse: 0.4173  | val_1_rmse: 0.41247 |  0:00:56s
epoch 60 | loss: 0.2224  | val_0_rmse: 0.67882 | val_1_rmse: 0.69348 |  0:00:57s
epoch 61 | loss: 0.24224 | val_0_rmse: 0.56001 | val_1_rmse: 0.5741  |  0:00:58s
epoch 62 | loss: 0.22154 | val_0_rmse: 0.52489 | val_1_rmse: 0.49451 |  0:00:59s
epoch 63 | loss: 0.24138 | val_0_rmse: 0.6386  | val_1_rmse: 0.61084 |  0:01:00s
epoch 64 | loss: 0.24615 | val_0_rmse: 0.52318 | val_1_rmse: 0.50596 |  0:01:01s
epoch 65 | loss: 0.25009 | val_0_rmse: 0.49865 | val_1_rmse: 0.48495 |  0:01:02s
epoch 66 | loss: 0.26895 | val_0_rmse: 0.70118 | val_1_rmse: 0.70643 |  0:01:03s
epoch 67 | loss: 0.27353 | val_0_rmse: 0.51652 | val_1_rmse: 0.51838 |  0:01:04s
epoch 68 | loss: 0.23102 | val_0_rmse: 0.44628 | val_1_rmse: 0.43699 |  0:01:05s
epoch 69 | loss: 0.22477 | val_0_rmse: 0.45708 | val_1_rmse: 0.45445 |  0:01:06s
epoch 70 | loss: 0.21747 | val_0_rmse: 0.50425 | val_1_rmse: 0.49778 |  0:01:07s
epoch 71 | loss: 0.23853 | val_0_rmse: 0.50744 | val_1_rmse: 0.49941 |  0:01:08s
epoch 72 | loss: 0.23994 | val_0_rmse: 0.65029 | val_1_rmse: 0.64599 |  0:01:09s
epoch 73 | loss: 0.23528 | val_0_rmse: 0.47962 | val_1_rmse: 0.47379 |  0:01:10s
epoch 74 | loss: 0.24274 | val_0_rmse: 0.47282 | val_1_rmse: 0.48299 |  0:01:11s
epoch 75 | loss: 0.22744 | val_0_rmse: 0.45628 | val_1_rmse: 0.43809 |  0:01:12s
epoch 76 | loss: 0.22458 | val_0_rmse: 0.47469 | val_1_rmse: 0.46644 |  0:01:13s
epoch 77 | loss: 0.22393 | val_0_rmse: 0.48213 | val_1_rmse: 0.47017 |  0:01:13s
epoch 78 | loss: 0.23454 | val_0_rmse: 0.59945 | val_1_rmse: 0.56369 |  0:01:14s
epoch 79 | loss: 0.2409  | val_0_rmse: 0.48876 | val_1_rmse: 0.48282 |  0:01:15s
epoch 80 | loss: 0.22616 | val_0_rmse: 0.45676 | val_1_rmse: 0.44149 |  0:01:16s
epoch 81 | loss: 0.21968 | val_0_rmse: 0.43679 | val_1_rmse: 0.44278 |  0:01:17s
epoch 82 | loss: 0.2028  | val_0_rmse: 0.44864 | val_1_rmse: 0.43431 |  0:01:18s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.39721
Best weights from best epoch are automatically used!
ended training at: 00:39:05
Feature importance:
Mean squared error is of 0.09017457878677206
Mean absolute error:0.19447847805174792
MAPE:0.26249720620956873
R2 score:0.8111324028752414
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:39:06
epoch 0  | loss: 1.43663 | val_0_rmse: 0.97262 | val_1_rmse: 0.93929 |  0:00:00s
epoch 1  | loss: 0.87201 | val_0_rmse: 0.88448 | val_1_rmse: 0.85326 |  0:00:01s
epoch 2  | loss: 0.67882 | val_0_rmse: 0.877   | val_1_rmse: 0.8531  |  0:00:02s
epoch 3  | loss: 0.58607 | val_0_rmse: 0.7798  | val_1_rmse: 0.77061 |  0:00:03s
epoch 4  | loss: 0.5131  | val_0_rmse: 0.82016 | val_1_rmse: 0.80578 |  0:00:04s
epoch 5  | loss: 0.42584 | val_0_rmse: 0.77358 | val_1_rmse: 0.74515 |  0:00:05s
epoch 6  | loss: 0.39328 | val_0_rmse: 0.79478 | val_1_rmse: 0.77334 |  0:00:06s
epoch 7  | loss: 0.34966 | val_0_rmse: 0.72165 | val_1_rmse: 0.70932 |  0:00:07s
epoch 8  | loss: 0.34633 | val_0_rmse: 0.66334 | val_1_rmse: 0.64652 |  0:00:08s
epoch 9  | loss: 0.32011 | val_0_rmse: 0.67499 | val_1_rmse: 0.65545 |  0:00:09s
epoch 10 | loss: 0.31056 | val_0_rmse: 0.62998 | val_1_rmse: 0.60983 |  0:00:10s
epoch 11 | loss: 0.28436 | val_0_rmse: 0.6109  | val_1_rmse: 0.58773 |  0:00:11s
epoch 12 | loss: 0.28068 | val_0_rmse: 0.62673 | val_1_rmse: 0.59988 |  0:00:12s
epoch 13 | loss: 0.27219 | val_0_rmse: 0.57004 | val_1_rmse: 0.55548 |  0:00:13s
epoch 14 | loss: 0.25719 | val_0_rmse: 0.55049 | val_1_rmse: 0.52936 |  0:00:14s
epoch 15 | loss: 0.25535 | val_0_rmse: 0.57742 | val_1_rmse: 0.56397 |  0:00:15s
epoch 16 | loss: 0.28254 | val_0_rmse: 0.53143 | val_1_rmse: 0.51127 |  0:00:16s
epoch 17 | loss: 0.2587  | val_0_rmse: 0.59283 | val_1_rmse: 0.56361 |  0:00:17s
epoch 18 | loss: 0.26474 | val_0_rmse: 0.52626 | val_1_rmse: 0.51408 |  0:00:18s
epoch 19 | loss: 0.24796 | val_0_rmse: 0.53229 | val_1_rmse: 0.5131  |  0:00:18s
epoch 20 | loss: 0.2274  | val_0_rmse: 0.52507 | val_1_rmse: 0.51042 |  0:00:19s
epoch 21 | loss: 0.25102 | val_0_rmse: 0.50425 | val_1_rmse: 0.49097 |  0:00:20s
epoch 22 | loss: 0.22926 | val_0_rmse: 0.51507 | val_1_rmse: 0.5054  |  0:00:21s
epoch 23 | loss: 0.21197 | val_0_rmse: 0.5467  | val_1_rmse: 0.55403 |  0:00:22s
epoch 24 | loss: 0.21134 | val_0_rmse: 0.49261 | val_1_rmse: 0.48423 |  0:00:23s
epoch 25 | loss: 0.2016  | val_0_rmse: 0.47616 | val_1_rmse: 0.4863  |  0:00:24s
epoch 26 | loss: 0.2129  | val_0_rmse: 0.44655 | val_1_rmse: 0.44512 |  0:00:25s
epoch 27 | loss: 0.20718 | val_0_rmse: 0.45246 | val_1_rmse: 0.44675 |  0:00:26s
epoch 28 | loss: 0.21532 | val_0_rmse: 0.50592 | val_1_rmse: 0.47365 |  0:00:27s
epoch 29 | loss: 0.20487 | val_0_rmse: 0.48685 | val_1_rmse: 0.48245 |  0:00:28s
epoch 30 | loss: 0.21776 | val_0_rmse: 0.46545 | val_1_rmse: 0.46479 |  0:00:29s
epoch 31 | loss: 0.19807 | val_0_rmse: 0.43295 | val_1_rmse: 0.43076 |  0:00:30s
epoch 32 | loss: 0.19642 | val_0_rmse: 0.43905 | val_1_rmse: 0.4498  |  0:00:31s
epoch 33 | loss: 0.1806  | val_0_rmse: 0.40497 | val_1_rmse: 0.42262 |  0:00:32s
epoch 34 | loss: 0.17987 | val_0_rmse: 0.40421 | val_1_rmse: 0.40178 |  0:00:33s
epoch 35 | loss: 0.18544 | val_0_rmse: 0.41854 | val_1_rmse: 0.42891 |  0:00:34s
epoch 36 | loss: 0.18188 | val_0_rmse: 0.4095  | val_1_rmse: 0.40812 |  0:00:35s
epoch 37 | loss: 0.18624 | val_0_rmse: 0.41082 | val_1_rmse: 0.43022 |  0:00:36s
epoch 38 | loss: 0.18694 | val_0_rmse: 0.39812 | val_1_rmse: 0.408   |  0:00:37s
epoch 39 | loss: 0.17509 | val_0_rmse: 0.39967 | val_1_rmse: 0.40726 |  0:00:37s
epoch 40 | loss: 0.17271 | val_0_rmse: 0.39458 | val_1_rmse: 0.41329 |  0:00:38s
epoch 41 | loss: 0.18221 | val_0_rmse: 0.3918  | val_1_rmse: 0.39734 |  0:00:39s
epoch 42 | loss: 0.18167 | val_0_rmse: 0.42196 | val_1_rmse: 0.43737 |  0:00:40s
epoch 43 | loss: 0.19198 | val_0_rmse: 0.49047 | val_1_rmse: 0.5067  |  0:00:41s
epoch 44 | loss: 0.21168 | val_0_rmse: 0.43654 | val_1_rmse: 0.45563 |  0:00:42s
epoch 45 | loss: 0.19526 | val_0_rmse: 0.45076 | val_1_rmse: 0.46593 |  0:00:43s
epoch 46 | loss: 0.17869 | val_0_rmse: 0.4466  | val_1_rmse: 0.4576  |  0:00:44s
epoch 47 | loss: 0.20476 | val_0_rmse: 0.42534 | val_1_rmse: 0.43881 |  0:00:45s
epoch 48 | loss: 0.17886 | val_0_rmse: 0.39032 | val_1_rmse: 0.40886 |  0:00:46s
epoch 49 | loss: 0.17251 | val_0_rmse: 0.39688 | val_1_rmse: 0.41547 |  0:00:47s
epoch 50 | loss: 0.17864 | val_0_rmse: 0.39253 | val_1_rmse: 0.41134 |  0:00:48s
epoch 51 | loss: 0.17145 | val_0_rmse: 0.38277 | val_1_rmse: 0.40493 |  0:00:49s
epoch 52 | loss: 0.16439 | val_0_rmse: 0.37631 | val_1_rmse: 0.38224 |  0:00:50s
epoch 53 | loss: 0.16295 | val_0_rmse: 0.39737 | val_1_rmse: 0.41913 |  0:00:51s
epoch 54 | loss: 0.16397 | val_0_rmse: 0.45048 | val_1_rmse: 0.47422 |  0:00:52s
epoch 55 | loss: 0.16668 | val_0_rmse: 0.40077 | val_1_rmse: 0.42909 |  0:00:53s
epoch 56 | loss: 0.16757 | val_0_rmse: 0.37879 | val_1_rmse: 0.39816 |  0:00:54s
epoch 57 | loss: 0.15302 | val_0_rmse: 0.36966 | val_1_rmse: 0.38664 |  0:00:54s
epoch 58 | loss: 0.15694 | val_0_rmse: 0.37218 | val_1_rmse: 0.39385 |  0:00:56s
epoch 59 | loss: 0.1556  | val_0_rmse: 0.42204 | val_1_rmse: 0.43779 |  0:00:56s
epoch 60 | loss: 0.16182 | val_0_rmse: 0.37632 | val_1_rmse: 0.39653 |  0:00:57s
epoch 61 | loss: 0.17994 | val_0_rmse: 0.41222 | val_1_rmse: 0.42778 |  0:00:58s
epoch 62 | loss: 0.18045 | val_0_rmse: 0.3862  | val_1_rmse: 0.40689 |  0:00:59s
epoch 63 | loss: 0.16174 | val_0_rmse: 0.37953 | val_1_rmse: 0.39477 |  0:01:00s
epoch 64 | loss: 0.17345 | val_0_rmse: 0.40858 | val_1_rmse: 0.4157  |  0:01:01s
epoch 65 | loss: 0.1708  | val_0_rmse: 0.4205  | val_1_rmse: 0.43254 |  0:01:02s
epoch 66 | loss: 0.18202 | val_0_rmse: 0.45488 | val_1_rmse: 0.47339 |  0:01:03s
epoch 67 | loss: 0.18988 | val_0_rmse: 0.43356 | val_1_rmse: 0.46802 |  0:01:04s
epoch 68 | loss: 0.16961 | val_0_rmse: 0.41356 | val_1_rmse: 0.44897 |  0:01:05s
epoch 69 | loss: 0.16304 | val_0_rmse: 0.38414 | val_1_rmse: 0.40419 |  0:01:06s
epoch 70 | loss: 0.16417 | val_0_rmse: 0.38208 | val_1_rmse: 0.4072  |  0:01:07s
epoch 71 | loss: 0.17551 | val_0_rmse: 0.45929 | val_1_rmse: 0.48112 |  0:01:08s
epoch 72 | loss: 0.16804 | val_0_rmse: 0.38952 | val_1_rmse: 0.40585 |  0:01:09s
epoch 73 | loss: 0.15752 | val_0_rmse: 0.39096 | val_1_rmse: 0.40397 |  0:01:10s
epoch 74 | loss: 0.15842 | val_0_rmse: 0.37298 | val_1_rmse: 0.3838  |  0:01:11s
epoch 75 | loss: 0.15732 | val_0_rmse: 0.37139 | val_1_rmse: 0.38563 |  0:01:11s
epoch 76 | loss: 0.15036 | val_0_rmse: 0.36107 | val_1_rmse: 0.39682 |  0:01:13s
epoch 77 | loss: 0.1526  | val_0_rmse: 0.37921 | val_1_rmse: 0.41543 |  0:01:13s
epoch 78 | loss: 0.15419 | val_0_rmse: 0.37366 | val_1_rmse: 0.37921 |  0:01:14s
epoch 79 | loss: 0.14864 | val_0_rmse: 0.37564 | val_1_rmse: 0.39251 |  0:01:15s
epoch 80 | loss: 0.14883 | val_0_rmse: 0.37008 | val_1_rmse: 0.39084 |  0:01:16s
epoch 81 | loss: 0.14669 | val_0_rmse: 0.35921 | val_1_rmse: 0.37521 |  0:01:17s
epoch 82 | loss: 0.14695 | val_0_rmse: 0.44121 | val_1_rmse: 0.45857 |  0:01:18s
epoch 83 | loss: 0.15304 | val_0_rmse: 0.36296 | val_1_rmse: 0.38704 |  0:01:19s
epoch 84 | loss: 0.14788 | val_0_rmse: 0.38899 | val_1_rmse: 0.40911 |  0:01:20s
epoch 85 | loss: 0.15778 | val_0_rmse: 0.39541 | val_1_rmse: 0.4188  |  0:01:21s
epoch 86 | loss: 0.16201 | val_0_rmse: 0.40178 | val_1_rmse: 0.42198 |  0:01:22s
epoch 87 | loss: 0.16902 | val_0_rmse: 0.38445 | val_1_rmse: 0.4054  |  0:01:23s
epoch 88 | loss: 0.18216 | val_0_rmse: 0.45707 | val_1_rmse: 0.47628 |  0:01:24s
epoch 89 | loss: 0.172   | val_0_rmse: 0.39964 | val_1_rmse: 0.40542 |  0:01:25s
epoch 90 | loss: 0.16029 | val_0_rmse: 0.40747 | val_1_rmse: 0.40886 |  0:01:26s
epoch 91 | loss: 0.15323 | val_0_rmse: 0.36577 | val_1_rmse: 0.38784 |  0:01:27s
epoch 92 | loss: 0.15107 | val_0_rmse: 0.38552 | val_1_rmse: 0.40174 |  0:01:28s
epoch 93 | loss: 0.15545 | val_0_rmse: 0.38554 | val_1_rmse: 0.39919 |  0:01:28s
epoch 94 | loss: 0.15175 | val_0_rmse: 0.38975 | val_1_rmse: 0.39972 |  0:01:30s
epoch 95 | loss: 0.15386 | val_0_rmse: 0.41633 | val_1_rmse: 0.42022 |  0:01:30s
epoch 96 | loss: 0.15321 | val_0_rmse: 0.37065 | val_1_rmse: 0.39299 |  0:01:31s
epoch 97 | loss: 0.16067 | val_0_rmse: 0.43354 | val_1_rmse: 0.4651  |  0:01:32s
epoch 98 | loss: 0.29319 | val_0_rmse: 0.60547 | val_1_rmse: 0.59596 |  0:01:33s
epoch 99 | loss: 0.28601 | val_0_rmse: 0.52704 | val_1_rmse: 0.51369 |  0:01:34s
epoch 100| loss: 0.27952 | val_0_rmse: 0.55219 | val_1_rmse: 0.55013 |  0:01:35s
epoch 101| loss: 0.26239 | val_0_rmse: 0.53856 | val_1_rmse: 0.52848 |  0:01:36s
epoch 102| loss: 0.23928 | val_0_rmse: 0.496   | val_1_rmse: 0.49943 |  0:01:37s
epoch 103| loss: 0.2343  | val_0_rmse: 0.52347 | val_1_rmse: 0.5072  |  0:01:38s
epoch 104| loss: 0.21597 | val_0_rmse: 0.50719 | val_1_rmse: 0.51873 |  0:01:39s
epoch 105| loss: 0.20738 | val_0_rmse: 0.4205  | val_1_rmse: 0.42479 |  0:01:40s
epoch 106| loss: 0.20174 | val_0_rmse: 0.42797 | val_1_rmse: 0.42267 |  0:01:41s
epoch 107| loss: 0.20073 | val_0_rmse: 0.463   | val_1_rmse: 0.48635 |  0:01:42s
epoch 108| loss: 0.19337 | val_0_rmse: 0.44939 | val_1_rmse: 0.45743 |  0:01:43s
epoch 109| loss: 0.19606 | val_0_rmse: 0.49651 | val_1_rmse: 0.48544 |  0:01:44s
epoch 110| loss: 0.19025 | val_0_rmse: 0.42529 | val_1_rmse: 0.44124 |  0:01:45s
epoch 111| loss: 0.1863  | val_0_rmse: 0.39788 | val_1_rmse: 0.39917 |  0:01:46s

Early stopping occured at epoch 111 with best_epoch = 81 and best_val_1_rmse = 0.37521
Best weights from best epoch are automatically used!
ended training at: 00:40:52
Feature importance:
Mean squared error is of 0.06796138420340071
Mean absolute error:0.17435078039788474
MAPE:0.21848717973177814
R2 score:0.8561830248595278
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:40:53
epoch 0  | loss: 1.46961 | val_0_rmse: 0.85698 | val_1_rmse: 0.85881 |  0:00:00s
epoch 1  | loss: 0.70485 | val_0_rmse: 0.79367 | val_1_rmse: 0.79623 |  0:00:01s
epoch 2  | loss: 0.5745  | val_0_rmse: 0.76996 | val_1_rmse: 0.76668 |  0:00:02s
epoch 3  | loss: 0.45933 | val_0_rmse: 0.83047 | val_1_rmse: 0.83081 |  0:00:03s
epoch 4  | loss: 0.43186 | val_0_rmse: 0.77098 | val_1_rmse: 0.77767 |  0:00:04s
epoch 5  | loss: 0.37087 | val_0_rmse: 0.7761  | val_1_rmse: 0.7819  |  0:00:05s
epoch 6  | loss: 0.3412  | val_0_rmse: 0.85292 | val_1_rmse: 0.85798 |  0:00:06s
epoch 7  | loss: 0.35049 | val_0_rmse: 0.63011 | val_1_rmse: 0.62078 |  0:00:07s
epoch 8  | loss: 0.32871 | val_0_rmse: 0.7259  | val_1_rmse: 0.72111 |  0:00:08s
epoch 9  | loss: 0.32458 | val_0_rmse: 0.64028 | val_1_rmse: 0.63944 |  0:00:09s
epoch 10 | loss: 0.27269 | val_0_rmse: 0.76039 | val_1_rmse: 0.76553 |  0:00:10s
epoch 11 | loss: 0.26946 | val_0_rmse: 0.60181 | val_1_rmse: 0.59811 |  0:00:11s
epoch 12 | loss: 0.26016 | val_0_rmse: 0.76891 | val_1_rmse: 0.75874 |  0:00:12s
epoch 13 | loss: 0.23489 | val_0_rmse: 0.64631 | val_1_rmse: 0.64383 |  0:00:13s
epoch 14 | loss: 0.24327 | val_0_rmse: 0.70211 | val_1_rmse: 0.70383 |  0:00:14s
epoch 15 | loss: 0.2586  | val_0_rmse: 0.78241 | val_1_rmse: 0.77977 |  0:00:15s
epoch 16 | loss: 0.24032 | val_0_rmse: 0.67556 | val_1_rmse: 0.67678 |  0:00:16s
epoch 17 | loss: 0.24838 | val_0_rmse: 0.54536 | val_1_rmse: 0.54462 |  0:00:17s
epoch 18 | loss: 0.22788 | val_0_rmse: 0.77441 | val_1_rmse: 0.76811 |  0:00:18s
epoch 19 | loss: 0.21193 | val_0_rmse: 0.71695 | val_1_rmse: 0.70836 |  0:00:18s
epoch 20 | loss: 0.22606 | val_0_rmse: 0.66233 | val_1_rmse: 0.6629  |  0:00:19s
epoch 21 | loss: 0.21297 | val_0_rmse: 0.58784 | val_1_rmse: 0.58785 |  0:00:20s
epoch 22 | loss: 0.21266 | val_0_rmse: 0.69519 | val_1_rmse: 0.69427 |  0:00:21s
epoch 23 | loss: 0.20422 | val_0_rmse: 0.54272 | val_1_rmse: 0.54576 |  0:00:22s
epoch 24 | loss: 0.20277 | val_0_rmse: 0.6613  | val_1_rmse: 0.66089 |  0:00:23s
epoch 25 | loss: 0.20126 | val_0_rmse: 0.52971 | val_1_rmse: 0.52636 |  0:00:24s
epoch 26 | loss: 0.21166 | val_0_rmse: 0.48947 | val_1_rmse: 0.49814 |  0:00:25s
epoch 27 | loss: 0.20471 | val_0_rmse: 0.83886 | val_1_rmse: 0.84284 |  0:00:26s
epoch 28 | loss: 0.26724 | val_0_rmse: 0.48591 | val_1_rmse: 0.49552 |  0:00:27s
epoch 29 | loss: 0.2377  | val_0_rmse: 0.47767 | val_1_rmse: 0.47676 |  0:00:28s
epoch 30 | loss: 0.21668 | val_0_rmse: 0.44169 | val_1_rmse: 0.43694 |  0:00:29s
epoch 31 | loss: 0.19906 | val_0_rmse: 0.4803  | val_1_rmse: 0.47594 |  0:00:30s
epoch 32 | loss: 0.19824 | val_0_rmse: 0.47644 | val_1_rmse: 0.4879  |  0:00:31s
epoch 33 | loss: 0.18774 | val_0_rmse: 0.49999 | val_1_rmse: 0.49855 |  0:00:32s
epoch 34 | loss: 0.20191 | val_0_rmse: 0.45892 | val_1_rmse: 0.4644  |  0:00:33s
epoch 35 | loss: 0.20276 | val_0_rmse: 0.41196 | val_1_rmse: 0.4236  |  0:00:34s
epoch 36 | loss: 0.20346 | val_0_rmse: 0.44501 | val_1_rmse: 0.45931 |  0:00:35s
epoch 37 | loss: 0.1953  | val_0_rmse: 0.44381 | val_1_rmse: 0.44694 |  0:00:36s
epoch 38 | loss: 0.19382 | val_0_rmse: 0.48646 | val_1_rmse: 0.49229 |  0:00:37s
epoch 39 | loss: 0.19278 | val_0_rmse: 0.40644 | val_1_rmse: 0.41397 |  0:00:37s
epoch 40 | loss: 0.18879 | val_0_rmse: 0.44799 | val_1_rmse: 0.45039 |  0:00:38s
epoch 41 | loss: 0.18429 | val_0_rmse: 0.42009 | val_1_rmse: 0.43523 |  0:00:39s
epoch 42 | loss: 0.18492 | val_0_rmse: 0.39358 | val_1_rmse: 0.39832 |  0:00:40s
epoch 43 | loss: 0.1772  | val_0_rmse: 0.41085 | val_1_rmse: 0.414   |  0:00:41s
epoch 44 | loss: 0.19068 | val_0_rmse: 0.41055 | val_1_rmse: 0.41961 |  0:00:42s
epoch 45 | loss: 0.18301 | val_0_rmse: 0.41271 | val_1_rmse: 0.42626 |  0:00:43s
epoch 46 | loss: 0.19272 | val_0_rmse: 0.42532 | val_1_rmse: 0.43991 |  0:00:44s
epoch 47 | loss: 0.18617 | val_0_rmse: 0.42172 | val_1_rmse: 0.43068 |  0:00:45s
epoch 48 | loss: 0.18697 | val_0_rmse: 0.41842 | val_1_rmse: 0.43239 |  0:00:46s
epoch 49 | loss: 0.17918 | val_0_rmse: 0.41018 | val_1_rmse: 0.41882 |  0:00:47s
epoch 50 | loss: 0.18545 | val_0_rmse: 0.42457 | val_1_rmse: 0.43286 |  0:00:48s
epoch 51 | loss: 0.18667 | val_0_rmse: 0.40786 | val_1_rmse: 0.41998 |  0:00:49s
epoch 52 | loss: 0.18567 | val_0_rmse: 0.46734 | val_1_rmse: 0.47405 |  0:00:50s
epoch 53 | loss: 0.18529 | val_0_rmse: 0.39184 | val_1_rmse: 0.39097 |  0:00:51s
epoch 54 | loss: 0.17269 | val_0_rmse: 0.41516 | val_1_rmse: 0.41183 |  0:00:52s
epoch 55 | loss: 0.16674 | val_0_rmse: 0.41483 | val_1_rmse: 0.42057 |  0:00:53s
epoch 56 | loss: 0.16944 | val_0_rmse: 0.40736 | val_1_rmse: 0.40836 |  0:00:54s
epoch 57 | loss: 0.1711  | val_0_rmse: 0.41861 | val_1_rmse: 0.41424 |  0:00:55s
epoch 58 | loss: 0.17678 | val_0_rmse: 0.37959 | val_1_rmse: 0.38291 |  0:00:56s
epoch 59 | loss: 0.16645 | val_0_rmse: 0.38539 | val_1_rmse: 0.39304 |  0:00:56s
epoch 60 | loss: 0.17428 | val_0_rmse: 0.40195 | val_1_rmse: 0.40799 |  0:00:57s
epoch 61 | loss: 0.17136 | val_0_rmse: 0.42096 | val_1_rmse: 0.43013 |  0:00:58s
epoch 62 | loss: 0.16778 | val_0_rmse: 0.37843 | val_1_rmse: 0.3849  |  0:00:59s
epoch 63 | loss: 0.16564 | val_0_rmse: 0.44858 | val_1_rmse: 0.44836 |  0:01:00s
epoch 64 | loss: 0.1657  | val_0_rmse: 0.40401 | val_1_rmse: 0.40954 |  0:01:01s
epoch 65 | loss: 0.16639 | val_0_rmse: 0.44585 | val_1_rmse: 0.44655 |  0:01:02s
epoch 66 | loss: 0.16961 | val_0_rmse: 0.41587 | val_1_rmse: 0.41929 |  0:01:03s
epoch 67 | loss: 0.16708 | val_0_rmse: 0.3804  | val_1_rmse: 0.38428 |  0:01:04s
epoch 68 | loss: 0.16733 | val_0_rmse: 0.38424 | val_1_rmse: 0.39403 |  0:01:05s
epoch 69 | loss: 0.18288 | val_0_rmse: 0.41301 | val_1_rmse: 0.41436 |  0:01:06s
epoch 70 | loss: 0.17069 | val_0_rmse: 0.41313 | val_1_rmse: 0.41326 |  0:01:07s
epoch 71 | loss: 0.16923 | val_0_rmse: 0.38069 | val_1_rmse: 0.37978 |  0:01:08s
epoch 72 | loss: 0.16906 | val_0_rmse: 0.37315 | val_1_rmse: 0.37758 |  0:01:09s
epoch 73 | loss: 0.15936 | val_0_rmse: 0.4105  | val_1_rmse: 0.42008 |  0:01:10s
epoch 74 | loss: 0.16583 | val_0_rmse: 0.40555 | val_1_rmse: 0.41009 |  0:01:11s
epoch 75 | loss: 0.15999 | val_0_rmse: 0.46101 | val_1_rmse: 0.46222 |  0:01:12s
epoch 76 | loss: 0.15594 | val_0_rmse: 0.38537 | val_1_rmse: 0.38905 |  0:01:13s
epoch 77 | loss: 0.16008 | val_0_rmse: 0.37453 | val_1_rmse: 0.37369 |  0:01:14s
epoch 78 | loss: 0.174   | val_0_rmse: 0.40997 | val_1_rmse: 0.40976 |  0:01:15s
epoch 79 | loss: 0.1763  | val_0_rmse: 0.381   | val_1_rmse: 0.3847  |  0:01:16s
epoch 80 | loss: 0.16322 | val_0_rmse: 0.42631 | val_1_rmse: 0.43426 |  0:01:16s
epoch 81 | loss: 0.16301 | val_0_rmse: 0.38581 | val_1_rmse: 0.39292 |  0:01:17s
epoch 82 | loss: 0.16266 | val_0_rmse: 0.57378 | val_1_rmse: 0.56785 |  0:01:18s
epoch 83 | loss: 0.15845 | val_0_rmse: 0.40315 | val_1_rmse: 0.4005  |  0:01:19s
epoch 84 | loss: 0.16137 | val_0_rmse: 0.52442 | val_1_rmse: 0.52303 |  0:01:20s
epoch 85 | loss: 0.18269 | val_0_rmse: 0.4136  | val_1_rmse: 0.41925 |  0:01:21s
epoch 86 | loss: 0.16748 | val_0_rmse: 0.39392 | val_1_rmse: 0.39691 |  0:01:22s
epoch 87 | loss: 0.16076 | val_0_rmse: 0.38074 | val_1_rmse: 0.37997 |  0:01:23s
epoch 88 | loss: 0.15614 | val_0_rmse: 0.39311 | val_1_rmse: 0.39739 |  0:01:24s
epoch 89 | loss: 0.15765 | val_0_rmse: 0.40011 | val_1_rmse: 0.40948 |  0:01:25s
epoch 90 | loss: 0.15736 | val_0_rmse: 0.36932 | val_1_rmse: 0.37352 |  0:01:26s
epoch 91 | loss: 0.15437 | val_0_rmse: 0.37257 | val_1_rmse: 0.38239 |  0:01:27s
epoch 92 | loss: 0.15069 | val_0_rmse: 0.36601 | val_1_rmse: 0.37397 |  0:01:28s
epoch 93 | loss: 0.14906 | val_0_rmse: 0.36922 | val_1_rmse: 0.37523 |  0:01:29s
epoch 94 | loss: 0.15276 | val_0_rmse: 0.38548 | val_1_rmse: 0.38949 |  0:01:30s
epoch 95 | loss: 0.14487 | val_0_rmse: 0.35513 | val_1_rmse: 0.36273 |  0:01:31s
epoch 96 | loss: 0.15095 | val_0_rmse: 0.41238 | val_1_rmse: 0.41474 |  0:01:32s
epoch 97 | loss: 0.1558  | val_0_rmse: 0.37403 | val_1_rmse: 0.37716 |  0:01:33s
epoch 98 | loss: 0.15307 | val_0_rmse: 0.43579 | val_1_rmse: 0.442   |  0:01:33s
epoch 99 | loss: 0.17378 | val_0_rmse: 0.44866 | val_1_rmse: 0.45594 |  0:01:34s
epoch 100| loss: 0.16181 | val_0_rmse: 0.38347 | val_1_rmse: 0.39191 |  0:01:35s
epoch 101| loss: 0.16289 | val_0_rmse: 0.38021 | val_1_rmse: 0.38573 |  0:01:36s
epoch 102| loss: 0.1577  | val_0_rmse: 0.41794 | val_1_rmse: 0.42013 |  0:01:37s
epoch 103| loss: 0.15502 | val_0_rmse: 0.36684 | val_1_rmse: 0.37812 |  0:01:38s
epoch 104| loss: 0.14552 | val_0_rmse: 0.38988 | val_1_rmse: 0.40106 |  0:01:39s
epoch 105| loss: 0.14409 | val_0_rmse: 0.36122 | val_1_rmse: 0.37672 |  0:01:40s
epoch 106| loss: 0.1406  | val_0_rmse: 0.35246 | val_1_rmse: 0.36668 |  0:01:41s
epoch 107| loss: 0.146   | val_0_rmse: 0.34886 | val_1_rmse: 0.36586 |  0:01:42s
epoch 108| loss: 0.14626 | val_0_rmse: 0.36258 | val_1_rmse: 0.37929 |  0:01:43s
epoch 109| loss: 0.1421  | val_0_rmse: 0.34575 | val_1_rmse: 0.36462 |  0:01:44s
epoch 110| loss: 0.14916 | val_0_rmse: 0.37442 | val_1_rmse: 0.38773 |  0:01:45s
epoch 111| loss: 0.14747 | val_0_rmse: 0.35595 | val_1_rmse: 0.36751 |  0:01:46s
epoch 112| loss: 0.14801 | val_0_rmse: 0.36682 | val_1_rmse: 0.38245 |  0:01:47s
epoch 113| loss: 0.14725 | val_0_rmse: 0.37365 | val_1_rmse: 0.38555 |  0:01:48s
epoch 114| loss: 0.14904 | val_0_rmse: 0.34875 | val_1_rmse: 0.36067 |  0:01:49s
epoch 115| loss: 0.14991 | val_0_rmse: 0.38896 | val_1_rmse: 0.3935  |  0:01:50s
epoch 116| loss: 0.14902 | val_0_rmse: 0.36766 | val_1_rmse: 0.37274 |  0:01:50s
epoch 117| loss: 0.14134 | val_0_rmse: 0.372   | val_1_rmse: 0.38181 |  0:01:51s
epoch 118| loss: 0.14957 | val_0_rmse: 0.3833  | val_1_rmse: 0.38831 |  0:01:52s
epoch 119| loss: 0.1449  | val_0_rmse: 0.35092 | val_1_rmse: 0.36341 |  0:01:53s
epoch 120| loss: 0.14822 | val_0_rmse: 0.3547  | val_1_rmse: 0.36104 |  0:01:54s
epoch 121| loss: 0.1594  | val_0_rmse: 0.40515 | val_1_rmse: 0.40859 |  0:01:55s
epoch 122| loss: 0.15125 | val_0_rmse: 0.36389 | val_1_rmse: 0.36494 |  0:01:56s
epoch 123| loss: 0.14315 | val_0_rmse: 0.37899 | val_1_rmse: 0.38911 |  0:01:57s
epoch 124| loss: 0.14023 | val_0_rmse: 0.34963 | val_1_rmse: 0.35556 |  0:01:58s
epoch 125| loss: 0.13926 | val_0_rmse: 0.38829 | val_1_rmse: 0.40063 |  0:01:59s
epoch 126| loss: 0.13493 | val_0_rmse: 0.358   | val_1_rmse: 0.37949 |  0:02:00s
epoch 127| loss: 0.14009 | val_0_rmse: 0.34372 | val_1_rmse: 0.3553  |  0:02:01s
epoch 128| loss: 0.13899 | val_0_rmse: 0.34469 | val_1_rmse: 0.36007 |  0:02:02s
epoch 129| loss: 0.1404  | val_0_rmse: 0.34053 | val_1_rmse: 0.35797 |  0:02:03s
epoch 130| loss: 0.13823 | val_0_rmse: 0.34035 | val_1_rmse: 0.36092 |  0:02:04s
epoch 131| loss: 0.14664 | val_0_rmse: 0.42022 | val_1_rmse: 0.42598 |  0:02:05s
epoch 132| loss: 0.14675 | val_0_rmse: 0.35582 | val_1_rmse: 0.36721 |  0:02:06s
epoch 133| loss: 0.13832 | val_0_rmse: 0.36167 | val_1_rmse: 0.37199 |  0:02:07s
epoch 134| loss: 0.13947 | val_0_rmse: 0.35712 | val_1_rmse: 0.37231 |  0:02:08s
epoch 135| loss: 0.13083 | val_0_rmse: 0.33124 | val_1_rmse: 0.34724 |  0:02:08s
epoch 136| loss: 0.13387 | val_0_rmse: 0.34211 | val_1_rmse: 0.35808 |  0:02:09s
epoch 137| loss: 0.13672 | val_0_rmse: 0.34541 | val_1_rmse: 0.35981 |  0:02:10s
epoch 138| loss: 0.13868 | val_0_rmse: 0.33794 | val_1_rmse: 0.35532 |  0:02:11s
epoch 139| loss: 0.13863 | val_0_rmse: 0.34589 | val_1_rmse: 0.35991 |  0:02:12s
epoch 140| loss: 0.13639 | val_0_rmse: 0.39814 | val_1_rmse: 0.40915 |  0:02:13s
epoch 141| loss: 0.15204 | val_0_rmse: 0.35927 | val_1_rmse: 0.36832 |  0:02:14s
epoch 142| loss: 0.14289 | val_0_rmse: 0.37641 | val_1_rmse: 0.38799 |  0:02:15s
epoch 143| loss: 0.13735 | val_0_rmse: 0.34476 | val_1_rmse: 0.36494 |  0:02:16s
epoch 144| loss: 0.13221 | val_0_rmse: 0.37199 | val_1_rmse: 0.38404 |  0:02:17s
epoch 145| loss: 0.13827 | val_0_rmse: 0.35246 | val_1_rmse: 0.36863 |  0:02:18s
epoch 146| loss: 0.14187 | val_0_rmse: 0.37891 | val_1_rmse: 0.38749 |  0:02:19s
epoch 147| loss: 0.13689 | val_0_rmse: 0.34504 | val_1_rmse: 0.36064 |  0:02:20s
epoch 148| loss: 0.13529 | val_0_rmse: 0.36465 | val_1_rmse: 0.37865 |  0:02:21s
epoch 149| loss: 0.13903 | val_0_rmse: 0.40069 | val_1_rmse: 0.42024 |  0:02:22s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.34724
Best weights from best epoch are automatically used!
ended training at: 00:43:16
Feature importance:
Mean squared error is of 0.06759602695158667
Mean absolute error:0.16299786890659185
MAPE:0.19329131439523037
R2 score:0.8594200762051047
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:43:16
epoch 0  | loss: 1.45569 | val_0_rmse: 0.86995 | val_1_rmse: 0.8598  |  0:00:00s
epoch 1  | loss: 0.6743  | val_0_rmse: 0.77744 | val_1_rmse: 0.76384 |  0:00:01s
epoch 2  | loss: 0.49995 | val_0_rmse: 0.77085 | val_1_rmse: 0.75633 |  0:00:02s
epoch 3  | loss: 0.43649 | val_0_rmse: 0.76327 | val_1_rmse: 0.76516 |  0:00:03s
epoch 4  | loss: 0.39456 | val_0_rmse: 0.98773 | val_1_rmse: 0.98229 |  0:00:04s
epoch 5  | loss: 0.35081 | val_0_rmse: 0.7222  | val_1_rmse: 0.71864 |  0:00:05s
epoch 6  | loss: 0.3088  | val_0_rmse: 0.71067 | val_1_rmse: 0.70135 |  0:00:06s
epoch 7  | loss: 0.2981  | val_0_rmse: 0.83207 | val_1_rmse: 0.82596 |  0:00:07s
epoch 8  | loss: 0.27514 | val_0_rmse: 0.9095  | val_1_rmse: 0.8921  |  0:00:08s
epoch 9  | loss: 0.25983 | val_0_rmse: 0.77695 | val_1_rmse: 0.77868 |  0:00:09s
epoch 10 | loss: 0.27809 | val_0_rmse: 0.79656 | val_1_rmse: 0.78868 |  0:00:10s
epoch 11 | loss: 0.26126 | val_0_rmse: 0.67678 | val_1_rmse: 0.67887 |  0:00:11s
epoch 12 | loss: 0.26036 | val_0_rmse: 0.64505 | val_1_rmse: 0.64894 |  0:00:12s
epoch 13 | loss: 0.2409  | val_0_rmse: 0.65464 | val_1_rmse: 0.66566 |  0:00:13s
epoch 14 | loss: 0.26143 | val_0_rmse: 0.6112  | val_1_rmse: 0.61462 |  0:00:14s
epoch 15 | loss: 0.28665 | val_0_rmse: 0.57505 | val_1_rmse: 0.57812 |  0:00:15s
epoch 16 | loss: 0.26386 | val_0_rmse: 0.64696 | val_1_rmse: 0.65845 |  0:00:16s
epoch 17 | loss: 0.25908 | val_0_rmse: 0.796   | val_1_rmse: 0.79954 |  0:00:17s
epoch 18 | loss: 0.24313 | val_0_rmse: 0.52583 | val_1_rmse: 0.53235 |  0:00:17s
epoch 19 | loss: 0.23991 | val_0_rmse: 0.67492 | val_1_rmse: 0.66796 |  0:00:18s
epoch 20 | loss: 0.23614 | val_0_rmse: 0.53906 | val_1_rmse: 0.52946 |  0:00:19s
epoch 21 | loss: 0.22645 | val_0_rmse: 0.61384 | val_1_rmse: 0.61048 |  0:00:20s
epoch 22 | loss: 0.21831 | val_0_rmse: 0.59582 | val_1_rmse: 0.59691 |  0:00:21s
epoch 23 | loss: 0.21071 | val_0_rmse: 0.6033  | val_1_rmse: 0.59657 |  0:00:22s
epoch 24 | loss: 0.2062  | val_0_rmse: 0.59323 | val_1_rmse: 0.59902 |  0:00:23s
epoch 25 | loss: 0.20387 | val_0_rmse: 0.58559 | val_1_rmse: 0.59738 |  0:00:24s
epoch 26 | loss: 0.20275 | val_0_rmse: 0.58712 | val_1_rmse: 0.58907 |  0:00:25s
epoch 27 | loss: 0.19806 | val_0_rmse: 0.54764 | val_1_rmse: 0.54244 |  0:00:26s
epoch 28 | loss: 0.2006  | val_0_rmse: 0.5018  | val_1_rmse: 0.50341 |  0:00:27s
epoch 29 | loss: 0.21665 | val_0_rmse: 0.45404 | val_1_rmse: 0.45392 |  0:00:28s
epoch 30 | loss: 0.19998 | val_0_rmse: 0.46196 | val_1_rmse: 0.46975 |  0:00:29s
epoch 31 | loss: 0.20959 | val_0_rmse: 0.48489 | val_1_rmse: 0.49989 |  0:00:30s
epoch 32 | loss: 0.19609 | val_0_rmse: 0.48967 | val_1_rmse: 0.50059 |  0:00:31s
epoch 33 | loss: 0.20134 | val_0_rmse: 0.46293 | val_1_rmse: 0.47303 |  0:00:32s
epoch 34 | loss: 0.20782 | val_0_rmse: 0.49535 | val_1_rmse: 0.50354 |  0:00:33s
epoch 35 | loss: 0.21225 | val_0_rmse: 0.48051 | val_1_rmse: 0.48705 |  0:00:33s
epoch 36 | loss: 0.21302 | val_0_rmse: 0.43153 | val_1_rmse: 0.44245 |  0:00:35s
epoch 37 | loss: 0.2055  | val_0_rmse: 0.42282 | val_1_rmse: 0.4345  |  0:00:35s
epoch 38 | loss: 0.19736 | val_0_rmse: 0.42871 | val_1_rmse: 0.44199 |  0:00:36s
epoch 39 | loss: 0.20048 | val_0_rmse: 0.46082 | val_1_rmse: 0.47177 |  0:00:37s
epoch 40 | loss: 0.19752 | val_0_rmse: 0.46759 | val_1_rmse: 0.48368 |  0:00:38s
epoch 41 | loss: 0.2158  | val_0_rmse: 0.42477 | val_1_rmse: 0.43629 |  0:00:39s
epoch 42 | loss: 0.20107 | val_0_rmse: 0.41169 | val_1_rmse: 0.42091 |  0:00:40s
epoch 43 | loss: 0.18817 | val_0_rmse: 0.43463 | val_1_rmse: 0.44838 |  0:00:41s
epoch 44 | loss: 0.19746 | val_0_rmse: 0.42677 | val_1_rmse: 0.44119 |  0:00:42s
epoch 45 | loss: 0.18714 | val_0_rmse: 0.50298 | val_1_rmse: 0.51511 |  0:00:43s
epoch 46 | loss: 0.18673 | val_0_rmse: 0.5933  | val_1_rmse: 0.59528 |  0:00:44s
epoch 47 | loss: 0.20633 | val_0_rmse: 0.46308 | val_1_rmse: 0.48101 |  0:00:45s
epoch 48 | loss: 0.18748 | val_0_rmse: 0.41615 | val_1_rmse: 0.43568 |  0:00:46s
epoch 49 | loss: 0.19021 | val_0_rmse: 0.40247 | val_1_rmse: 0.42066 |  0:00:47s
epoch 50 | loss: 0.1788  | val_0_rmse: 0.40905 | val_1_rmse: 0.42767 |  0:00:48s
epoch 51 | loss: 0.18213 | val_0_rmse: 0.46203 | val_1_rmse: 0.47583 |  0:00:49s
epoch 52 | loss: 0.18795 | val_0_rmse: 0.43901 | val_1_rmse: 0.45649 |  0:00:50s
epoch 53 | loss: 0.18134 | val_0_rmse: 0.4179  | val_1_rmse: 0.44144 |  0:00:51s
epoch 54 | loss: 0.18091 | val_0_rmse: 0.41971 | val_1_rmse: 0.43973 |  0:00:52s
epoch 55 | loss: 0.17426 | val_0_rmse: 0.39435 | val_1_rmse: 0.41407 |  0:00:52s
epoch 56 | loss: 0.17525 | val_0_rmse: 0.4141  | val_1_rmse: 0.43457 |  0:00:53s
epoch 57 | loss: 0.17424 | val_0_rmse: 0.42164 | val_1_rmse: 0.43859 |  0:00:54s
epoch 58 | loss: 0.17906 | val_0_rmse: 0.43291 | val_1_rmse: 0.45437 |  0:00:55s
epoch 59 | loss: 0.1743  | val_0_rmse: 0.51493 | val_1_rmse: 0.53006 |  0:00:56s
epoch 60 | loss: 0.16809 | val_0_rmse: 0.39089 | val_1_rmse: 0.41208 |  0:00:57s
epoch 61 | loss: 0.1677  | val_0_rmse: 0.38771 | val_1_rmse: 0.41023 |  0:00:58s
epoch 62 | loss: 0.16882 | val_0_rmse: 0.39185 | val_1_rmse: 0.40797 |  0:00:59s
epoch 63 | loss: 0.16702 | val_0_rmse: 0.39238 | val_1_rmse: 0.40815 |  0:01:00s
epoch 64 | loss: 0.16357 | val_0_rmse: 0.42795 | val_1_rmse: 0.45079 |  0:01:01s
epoch 65 | loss: 0.172   | val_0_rmse: 0.40852 | val_1_rmse: 0.42949 |  0:01:02s
epoch 66 | loss: 0.16627 | val_0_rmse: 0.41267 | val_1_rmse: 0.42987 |  0:01:03s
epoch 67 | loss: 0.1824  | val_0_rmse: 0.40224 | val_1_rmse: 0.42069 |  0:01:04s
epoch 68 | loss: 0.16999 | val_0_rmse: 0.43964 | val_1_rmse: 0.45694 |  0:01:05s
epoch 69 | loss: 0.17003 | val_0_rmse: 0.40648 | val_1_rmse: 0.42962 |  0:01:06s
epoch 70 | loss: 0.17038 | val_0_rmse: 0.44253 | val_1_rmse: 0.46348 |  0:01:07s
epoch 71 | loss: 0.17571 | val_0_rmse: 0.44238 | val_1_rmse: 0.46202 |  0:01:08s
epoch 72 | loss: 0.17842 | val_0_rmse: 0.39833 | val_1_rmse: 0.41811 |  0:01:09s
epoch 73 | loss: 0.17371 | val_0_rmse: 0.38512 | val_1_rmse: 0.40767 |  0:01:09s
epoch 74 | loss: 0.15823 | val_0_rmse: 0.5114  | val_1_rmse: 0.51947 |  0:01:10s
epoch 75 | loss: 0.17194 | val_0_rmse: 0.40535 | val_1_rmse: 0.4282  |  0:01:11s
epoch 76 | loss: 0.16888 | val_0_rmse: 0.38075 | val_1_rmse: 0.4003  |  0:01:12s
epoch 77 | loss: 0.16031 | val_0_rmse: 0.37463 | val_1_rmse: 0.39956 |  0:01:13s
epoch 78 | loss: 0.1534  | val_0_rmse: 0.39499 | val_1_rmse: 0.41571 |  0:01:14s
epoch 79 | loss: 0.17274 | val_0_rmse: 0.37479 | val_1_rmse: 0.40136 |  0:01:15s
epoch 80 | loss: 0.16342 | val_0_rmse: 0.37668 | val_1_rmse: 0.40386 |  0:01:16s
epoch 81 | loss: 0.15889 | val_0_rmse: 0.48718 | val_1_rmse: 0.50015 |  0:01:17s
epoch 82 | loss: 0.15832 | val_0_rmse: 0.42097 | val_1_rmse: 0.44144 |  0:01:18s
epoch 83 | loss: 0.16304 | val_0_rmse: 0.49835 | val_1_rmse: 0.51656 |  0:01:19s
epoch 84 | loss: 0.17082 | val_0_rmse: 0.39695 | val_1_rmse: 0.42075 |  0:01:20s
epoch 85 | loss: 0.16603 | val_0_rmse: 0.37843 | val_1_rmse: 0.40701 |  0:01:21s
epoch 86 | loss: 0.16149 | val_0_rmse: 0.42019 | val_1_rmse: 0.44762 |  0:01:22s
epoch 87 | loss: 0.15358 | val_0_rmse: 0.38584 | val_1_rmse: 0.41364 |  0:01:23s
epoch 88 | loss: 0.15899 | val_0_rmse: 0.3757  | val_1_rmse: 0.41023 |  0:01:24s
epoch 89 | loss: 0.18706 | val_0_rmse: 0.57965 | val_1_rmse: 0.60748 |  0:01:25s
epoch 90 | loss: 0.20998 | val_0_rmse: 0.61773 | val_1_rmse: 0.60639 |  0:01:26s
epoch 91 | loss: 0.2196  | val_0_rmse: 0.4392  | val_1_rmse: 0.44826 |  0:01:27s
epoch 92 | loss: 0.19559 | val_0_rmse: 0.5004  | val_1_rmse: 0.50835 |  0:01:27s
epoch 93 | loss: 0.20214 | val_0_rmse: 0.53886 | val_1_rmse: 0.54466 |  0:01:28s
epoch 94 | loss: 0.19135 | val_0_rmse: 0.41085 | val_1_rmse: 0.42472 |  0:01:29s
epoch 95 | loss: 0.18052 | val_0_rmse: 0.46942 | val_1_rmse: 0.48351 |  0:01:30s
epoch 96 | loss: 0.18642 | val_0_rmse: 0.40471 | val_1_rmse: 0.41904 |  0:01:31s
epoch 97 | loss: 0.18767 | val_0_rmse: 0.45226 | val_1_rmse: 0.46963 |  0:01:32s
epoch 98 | loss: 0.19262 | val_0_rmse: 0.45406 | val_1_rmse: 0.47444 |  0:01:33s
epoch 99 | loss: 0.20332 | val_0_rmse: 0.52079 | val_1_rmse: 0.54708 |  0:01:34s
epoch 100| loss: 0.34479 | val_0_rmse: 0.63822 | val_1_rmse: 0.64861 |  0:01:35s
epoch 101| loss: 0.31063 | val_0_rmse: 0.57434 | val_1_rmse: 0.56644 |  0:01:36s
epoch 102| loss: 0.26902 | val_0_rmse: 0.57838 | val_1_rmse: 0.59457 |  0:01:37s
epoch 103| loss: 0.24649 | val_0_rmse: 0.61296 | val_1_rmse: 0.60599 |  0:01:38s
epoch 104| loss: 0.31549 | val_0_rmse: 0.61477 | val_1_rmse: 0.61438 |  0:01:39s
epoch 105| loss: 0.31225 | val_0_rmse: 0.53323 | val_1_rmse: 0.54341 |  0:01:40s
epoch 106| loss: 0.25756 | val_0_rmse: 0.51894 | val_1_rmse: 0.5212  |  0:01:41s
epoch 107| loss: 0.2361  | val_0_rmse: 0.49272 | val_1_rmse: 0.48309 |  0:01:42s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.39956
Best weights from best epoch are automatically used!
ended training at: 00:44:59
Feature importance:
Mean squared error is of 0.07433050178924389
Mean absolute error:0.17809473499534886
MAPE:0.2204432989629771
R2 score:0.8410699629137063
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:44:59
epoch 0  | loss: 1.43002 | val_0_rmse: 0.95149 | val_1_rmse: 0.96565 |  0:00:00s
epoch 1  | loss: 0.69804 | val_0_rmse: 1.6667  | val_1_rmse: 1.63391 |  0:00:01s
epoch 2  | loss: 0.48971 | val_0_rmse: 1.19069 | val_1_rmse: 1.18296 |  0:00:02s
epoch 3  | loss: 0.41088 | val_0_rmse: 0.75975 | val_1_rmse: 0.7593  |  0:00:03s
epoch 4  | loss: 0.38275 | val_0_rmse: 0.67421 | val_1_rmse: 0.67585 |  0:00:04s
epoch 5  | loss: 0.38213 | val_0_rmse: 0.697   | val_1_rmse: 0.70869 |  0:00:05s
epoch 6  | loss: 0.4044  | val_0_rmse: 0.83173 | val_1_rmse: 0.81563 |  0:00:06s
epoch 7  | loss: 0.34146 | val_0_rmse: 0.7044  | val_1_rmse: 0.68708 |  0:00:07s
epoch 8  | loss: 0.31098 | val_0_rmse: 0.64188 | val_1_rmse: 0.64421 |  0:00:08s
epoch 9  | loss: 0.28908 | val_0_rmse: 0.61977 | val_1_rmse: 0.63477 |  0:00:09s
epoch 10 | loss: 0.2899  | val_0_rmse: 0.61388 | val_1_rmse: 0.62764 |  0:00:10s
epoch 11 | loss: 0.28667 | val_0_rmse: 0.85014 | val_1_rmse: 0.85883 |  0:00:11s
epoch 12 | loss: 0.28126 | val_0_rmse: 0.64587 | val_1_rmse: 0.65297 |  0:00:12s
epoch 13 | loss: 0.27339 | val_0_rmse: 0.71144 | val_1_rmse: 0.70715 |  0:00:13s
epoch 14 | loss: 0.25551 | val_0_rmse: 1.01473 | val_1_rmse: 1.01768 |  0:00:14s
epoch 15 | loss: 0.26587 | val_0_rmse: 0.66825 | val_1_rmse: 0.67172 |  0:00:15s
epoch 16 | loss: 0.26337 | val_0_rmse: 0.54483 | val_1_rmse: 0.57901 |  0:00:16s
epoch 17 | loss: 0.26207 | val_0_rmse: 0.55533 | val_1_rmse: 0.5609  |  0:00:17s
epoch 18 | loss: 0.24622 | val_0_rmse: 0.54361 | val_1_rmse: 0.54765 |  0:00:17s
epoch 19 | loss: 0.2366  | val_0_rmse: 0.65995 | val_1_rmse: 0.65897 |  0:00:18s
epoch 20 | loss: 0.28858 | val_0_rmse: 0.67467 | val_1_rmse: 0.67167 |  0:00:19s
epoch 21 | loss: 0.30943 | val_0_rmse: 0.51407 | val_1_rmse: 0.52534 |  0:00:20s
epoch 22 | loss: 0.29123 | val_0_rmse: 0.7084  | val_1_rmse: 0.71737 |  0:00:21s
epoch 23 | loss: 0.24666 | val_0_rmse: 0.49762 | val_1_rmse: 0.51717 |  0:00:22s
epoch 24 | loss: 0.22717 | val_0_rmse: 0.47347 | val_1_rmse: 0.50485 |  0:00:23s
epoch 25 | loss: 0.21987 | val_0_rmse: 0.48088 | val_1_rmse: 0.50327 |  0:00:24s
epoch 26 | loss: 0.21171 | val_0_rmse: 0.50696 | val_1_rmse: 0.52324 |  0:00:25s
epoch 27 | loss: 0.22081 | val_0_rmse: 0.51947 | val_1_rmse: 0.55719 |  0:00:26s
epoch 28 | loss: 0.22875 | val_0_rmse: 0.48597 | val_1_rmse: 0.51107 |  0:00:27s
epoch 29 | loss: 0.20717 | val_0_rmse: 0.45323 | val_1_rmse: 0.49212 |  0:00:28s
epoch 30 | loss: 0.20632 | val_0_rmse: 0.44267 | val_1_rmse: 0.4837  |  0:00:29s
epoch 31 | loss: 0.20574 | val_0_rmse: 0.47534 | val_1_rmse: 0.51805 |  0:00:30s
epoch 32 | loss: 0.21061 | val_0_rmse: 0.44363 | val_1_rmse: 0.4682  |  0:00:31s
epoch 33 | loss: 0.21441 | val_0_rmse: 0.57258 | val_1_rmse: 0.61072 |  0:00:32s
epoch 34 | loss: 0.22935 | val_0_rmse: 0.4897  | val_1_rmse: 0.49593 |  0:00:33s
epoch 35 | loss: 0.21169 | val_0_rmse: 0.44758 | val_1_rmse: 0.48881 |  0:00:33s
epoch 36 | loss: 0.19698 | val_0_rmse: 0.44244 | val_1_rmse: 0.46322 |  0:00:35s
epoch 37 | loss: 0.20204 | val_0_rmse: 0.43212 | val_1_rmse: 0.45502 |  0:00:35s
epoch 38 | loss: 0.19452 | val_0_rmse: 0.42705 | val_1_rmse: 0.45235 |  0:00:36s
epoch 39 | loss: 0.18667 | val_0_rmse: 0.45018 | val_1_rmse: 0.46282 |  0:00:37s
epoch 40 | loss: 0.18673 | val_0_rmse: 0.44678 | val_1_rmse: 0.45821 |  0:00:38s
epoch 41 | loss: 0.18222 | val_0_rmse: 0.40401 | val_1_rmse: 0.43129 |  0:00:39s
epoch 42 | loss: 0.18692 | val_0_rmse: 0.39444 | val_1_rmse: 0.41114 |  0:00:40s
epoch 43 | loss: 0.21134 | val_0_rmse: 0.6877  | val_1_rmse: 0.72798 |  0:00:41s
epoch 44 | loss: 0.1975  | val_0_rmse: 0.43711 | val_1_rmse: 0.46748 |  0:00:42s
epoch 45 | loss: 0.18977 | val_0_rmse: 0.40687 | val_1_rmse: 0.42379 |  0:00:43s
epoch 46 | loss: 0.17555 | val_0_rmse: 0.39775 | val_1_rmse: 0.41959 |  0:00:44s
epoch 47 | loss: 0.17026 | val_0_rmse: 0.3976  | val_1_rmse: 0.42343 |  0:00:45s
epoch 48 | loss: 0.17254 | val_0_rmse: 0.41483 | val_1_rmse: 0.43378 |  0:00:46s
epoch 49 | loss: 0.17807 | val_0_rmse: 0.46208 | val_1_rmse: 0.5053  |  0:00:47s
epoch 50 | loss: 0.17453 | val_0_rmse: 0.3965  | val_1_rmse: 0.42768 |  0:00:48s
epoch 51 | loss: 0.17283 | val_0_rmse: 0.45229 | val_1_rmse: 0.47298 |  0:00:49s
epoch 52 | loss: 0.18158 | val_0_rmse: 0.44427 | val_1_rmse: 0.46898 |  0:00:50s
epoch 53 | loss: 0.18451 | val_0_rmse: 0.4101  | val_1_rmse: 0.4284  |  0:00:51s
epoch 54 | loss: 0.17019 | val_0_rmse: 0.39025 | val_1_rmse: 0.42097 |  0:00:51s
epoch 55 | loss: 0.16964 | val_0_rmse: 0.39594 | val_1_rmse: 0.43318 |  0:00:52s
epoch 56 | loss: 0.16832 | val_0_rmse: 0.43455 | val_1_rmse: 0.45837 |  0:00:53s
epoch 57 | loss: 0.17765 | val_0_rmse: 0.43695 | val_1_rmse: 0.4781  |  0:00:54s
epoch 58 | loss: 0.19112 | val_0_rmse: 0.41387 | val_1_rmse: 0.43862 |  0:00:55s
epoch 59 | loss: 0.19264 | val_0_rmse: 0.53677 | val_1_rmse: 0.54206 |  0:00:56s
epoch 60 | loss: 0.18503 | val_0_rmse: 0.39687 | val_1_rmse: 0.41921 |  0:00:57s
epoch 61 | loss: 0.18532 | val_0_rmse: 0.4052  | val_1_rmse: 0.44078 |  0:00:58s
epoch 62 | loss: 0.17072 | val_0_rmse: 0.55503 | val_1_rmse: 0.60592 |  0:00:59s
epoch 63 | loss: 0.17603 | val_0_rmse: 0.49302 | val_1_rmse: 0.53407 |  0:01:00s
epoch 64 | loss: 0.16773 | val_0_rmse: 0.3839  | val_1_rmse: 0.40948 |  0:01:01s
epoch 65 | loss: 0.16812 | val_0_rmse: 0.39655 | val_1_rmse: 0.43066 |  0:01:02s
epoch 66 | loss: 0.17042 | val_0_rmse: 0.39748 | val_1_rmse: 0.44959 |  0:01:03s
epoch 67 | loss: 0.16688 | val_0_rmse: 0.48267 | val_1_rmse: 0.50524 |  0:01:04s
epoch 68 | loss: 0.24431 | val_0_rmse: 0.57929 | val_1_rmse: 0.62839 |  0:01:05s
epoch 69 | loss: 0.21612 | val_0_rmse: 0.48673 | val_1_rmse: 0.51748 |  0:01:06s
epoch 70 | loss: 0.20284 | val_0_rmse: 0.41768 | val_1_rmse: 0.4546  |  0:01:07s
epoch 71 | loss: 0.1869  | val_0_rmse: 0.41341 | val_1_rmse: 0.45521 |  0:01:08s
epoch 72 | loss: 0.19531 | val_0_rmse: 0.41288 | val_1_rmse: 0.43567 |  0:01:08s
epoch 73 | loss: 0.1889  | val_0_rmse: 0.44239 | val_1_rmse: 0.46711 |  0:01:09s
epoch 74 | loss: 0.17423 | val_0_rmse: 0.39855 | val_1_rmse: 0.44929 |  0:01:10s
epoch 75 | loss: 0.16489 | val_0_rmse: 0.41018 | val_1_rmse: 0.45141 |  0:01:11s
epoch 76 | loss: 0.16369 | val_0_rmse: 0.40841 | val_1_rmse: 0.44516 |  0:01:12s
epoch 77 | loss: 0.1597  | val_0_rmse: 0.38103 | val_1_rmse: 0.41747 |  0:01:13s
epoch 78 | loss: 0.18245 | val_0_rmse: 0.41028 | val_1_rmse: 0.47223 |  0:01:14s
epoch 79 | loss: 0.16408 | val_0_rmse: 0.42674 | val_1_rmse: 0.45284 |  0:01:15s
epoch 80 | loss: 0.18946 | val_0_rmse: 0.55577 | val_1_rmse: 0.57528 |  0:01:16s
epoch 81 | loss: 0.25197 | val_0_rmse: 0.6244  | val_1_rmse: 0.64212 |  0:01:17s
epoch 82 | loss: 0.234   | val_0_rmse: 0.55884 | val_1_rmse: 0.57937 |  0:01:18s
epoch 83 | loss: 0.23605 | val_0_rmse: 0.5673  | val_1_rmse: 0.59898 |  0:01:19s
epoch 84 | loss: 0.20989 | val_0_rmse: 0.43885 | val_1_rmse: 0.471   |  0:01:20s
epoch 85 | loss: 0.20297 | val_0_rmse: 0.43505 | val_1_rmse: 0.47315 |  0:01:21s
epoch 86 | loss: 0.21127 | val_0_rmse: 0.44303 | val_1_rmse: 0.48034 |  0:01:22s
epoch 87 | loss: 0.20762 | val_0_rmse: 0.44486 | val_1_rmse: 0.48488 |  0:01:23s
epoch 88 | loss: 0.19492 | val_0_rmse: 0.41115 | val_1_rmse: 0.4379  |  0:01:24s
epoch 89 | loss: 0.17879 | val_0_rmse: 0.41181 | val_1_rmse: 0.44954 |  0:01:25s
epoch 90 | loss: 0.17484 | val_0_rmse: 0.40135 | val_1_rmse: 0.4427  |  0:01:25s
epoch 91 | loss: 0.17844 | val_0_rmse: 0.38704 | val_1_rmse: 0.44385 |  0:01:26s
epoch 92 | loss: 0.16971 | val_0_rmse: 0.40227 | val_1_rmse: 0.44998 |  0:01:27s
epoch 93 | loss: 0.17412 | val_0_rmse: 0.54125 | val_1_rmse: 0.55917 |  0:01:28s
epoch 94 | loss: 0.16537 | val_0_rmse: 0.41807 | val_1_rmse: 0.47523 |  0:01:29s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.40948
Best weights from best epoch are automatically used!
ended training at: 00:46:29
Feature importance:
Mean squared error is of 0.07099917035933276
Mean absolute error:0.18260114274993677
MAPE:0.2247168591112802
R2 score:0.8513694191402422
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:46:30
epoch 0  | loss: 1.58428 | val_0_rmse: 0.98163 | val_1_rmse: 1.05235 |  0:00:00s
epoch 1  | loss: 1.00887 | val_0_rmse: 0.93954 | val_1_rmse: 0.99951 |  0:00:00s
epoch 2  | loss: 0.87988 | val_0_rmse: 0.82936 | val_1_rmse: 0.86739 |  0:00:01s
epoch 3  | loss: 0.72709 | val_0_rmse: 0.84667 | val_1_rmse: 0.85403 |  0:00:01s
epoch 4  | loss: 0.68561 | val_0_rmse: 0.93338 | val_1_rmse: 0.93608 |  0:00:01s
epoch 5  | loss: 0.68517 | val_0_rmse: 0.87152 | val_1_rmse: 0.87855 |  0:00:01s
epoch 6  | loss: 0.68646 | val_0_rmse: 0.8102  | val_1_rmse: 0.82173 |  0:00:02s
epoch 7  | loss: 0.66    | val_0_rmse: 0.8127  | val_1_rmse: 0.8281  |  0:00:02s
epoch 8  | loss: 0.6582  | val_0_rmse: 0.79814 | val_1_rmse: 0.82064 |  0:00:02s
epoch 9  | loss: 0.65031 | val_0_rmse: 0.80285 | val_1_rmse: 0.84311 |  0:00:03s
epoch 10 | loss: 0.64357 | val_0_rmse: 0.80131 | val_1_rmse: 0.83076 |  0:00:03s
epoch 11 | loss: 0.64359 | val_0_rmse: 0.79793 | val_1_rmse: 0.83138 |  0:00:03s
epoch 12 | loss: 0.63903 | val_0_rmse: 0.79448 | val_1_rmse: 0.82608 |  0:00:04s
epoch 13 | loss: 0.64965 | val_0_rmse: 0.80225 | val_1_rmse: 0.83387 |  0:00:04s
epoch 14 | loss: 0.64303 | val_0_rmse: 0.79751 | val_1_rmse: 0.83549 |  0:00:04s
epoch 15 | loss: 0.64359 | val_0_rmse: 0.79055 | val_1_rmse: 0.82094 |  0:00:05s
epoch 16 | loss: 0.64609 | val_0_rmse: 0.79454 | val_1_rmse: 0.82947 |  0:00:05s
epoch 17 | loss: 0.62891 | val_0_rmse: 0.79908 | val_1_rmse: 0.84602 |  0:00:05s
epoch 18 | loss: 0.63744 | val_0_rmse: 0.80179 | val_1_rmse: 0.85055 |  0:00:05s
epoch 19 | loss: 0.62014 | val_0_rmse: 0.78718 | val_1_rmse: 0.8247  |  0:00:06s
epoch 20 | loss: 0.62394 | val_0_rmse: 0.79232 | val_1_rmse: 0.82083 |  0:00:06s
epoch 21 | loss: 0.60326 | val_0_rmse: 0.80286 | val_1_rmse: 0.82935 |  0:00:06s
epoch 22 | loss: 0.60421 | val_0_rmse: 0.79876 | val_1_rmse: 0.8248  |  0:00:07s
epoch 23 | loss: 0.6057  | val_0_rmse: 0.7977  | val_1_rmse: 0.83822 |  0:00:07s
epoch 24 | loss: 0.60466 | val_0_rmse: 0.79151 | val_1_rmse: 0.83744 |  0:00:07s
epoch 25 | loss: 0.59427 | val_0_rmse: 0.79484 | val_1_rmse: 0.84355 |  0:00:08s
epoch 26 | loss: 0.59223 | val_0_rmse: 0.7835  | val_1_rmse: 0.82903 |  0:00:08s
epoch 27 | loss: 0.59041 | val_0_rmse: 0.78667 | val_1_rmse: 0.83154 |  0:00:08s
epoch 28 | loss: 0.59272 | val_0_rmse: 0.78879 | val_1_rmse: 0.83791 |  0:00:09s
epoch 29 | loss: 0.57971 | val_0_rmse: 0.77576 | val_1_rmse: 0.81865 |  0:00:09s
epoch 30 | loss: 0.58656 | val_0_rmse: 0.7808  | val_1_rmse: 0.82289 |  0:00:09s
epoch 31 | loss: 0.57989 | val_0_rmse: 0.77706 | val_1_rmse: 0.81862 |  0:00:09s
epoch 32 | loss: 0.58371 | val_0_rmse: 0.78156 | val_1_rmse: 0.8195  |  0:00:10s
epoch 33 | loss: 0.57001 | val_0_rmse: 0.78196 | val_1_rmse: 0.82375 |  0:00:10s
epoch 34 | loss: 0.57064 | val_0_rmse: 0.78492 | val_1_rmse: 0.82408 |  0:00:10s
epoch 35 | loss: 0.56573 | val_0_rmse: 0.78891 | val_1_rmse: 0.82615 |  0:00:11s
epoch 36 | loss: 0.56564 | val_0_rmse: 0.77699 | val_1_rmse: 0.81918 |  0:00:11s
epoch 37 | loss: 0.55689 | val_0_rmse: 0.78546 | val_1_rmse: 0.82101 |  0:00:11s
epoch 38 | loss: 0.56088 | val_0_rmse: 0.77387 | val_1_rmse: 0.82429 |  0:00:12s
epoch 39 | loss: 0.58135 | val_0_rmse: 0.76878 | val_1_rmse: 0.82014 |  0:00:12s
epoch 40 | loss: 0.55892 | val_0_rmse: 0.76916 | val_1_rmse: 0.81041 |  0:00:12s
epoch 41 | loss: 0.56578 | val_0_rmse: 0.75708 | val_1_rmse: 0.81317 |  0:00:13s
epoch 42 | loss: 0.5592  | val_0_rmse: 0.76904 | val_1_rmse: 0.83258 |  0:00:13s
epoch 43 | loss: 0.55966 | val_0_rmse: 0.76123 | val_1_rmse: 0.8286  |  0:00:13s
epoch 44 | loss: 0.55908 | val_0_rmse: 0.76413 | val_1_rmse: 0.82834 |  0:00:14s
epoch 45 | loss: 0.55526 | val_0_rmse: 0.75927 | val_1_rmse: 0.82409 |  0:00:14s
epoch 46 | loss: 0.55272 | val_0_rmse: 0.75317 | val_1_rmse: 0.82152 |  0:00:14s
epoch 47 | loss: 0.55028 | val_0_rmse: 0.76032 | val_1_rmse: 0.82486 |  0:00:15s
epoch 48 | loss: 0.56037 | val_0_rmse: 0.75829 | val_1_rmse: 0.82351 |  0:00:15s
epoch 49 | loss: 0.55489 | val_0_rmse: 0.75836 | val_1_rmse: 0.82419 |  0:00:15s
epoch 50 | loss: 0.55673 | val_0_rmse: 0.76357 | val_1_rmse: 0.83139 |  0:00:15s
epoch 51 | loss: 0.55732 | val_0_rmse: 0.7575  | val_1_rmse: 0.82292 |  0:00:16s
epoch 52 | loss: 0.55108 | val_0_rmse: 0.75482 | val_1_rmse: 0.82575 |  0:00:16s
epoch 53 | loss: 0.54924 | val_0_rmse: 0.75868 | val_1_rmse: 0.82261 |  0:00:16s
epoch 54 | loss: 0.55308 | val_0_rmse: 0.74895 | val_1_rmse: 0.81351 |  0:00:17s
epoch 55 | loss: 0.54722 | val_0_rmse: 0.74628 | val_1_rmse: 0.81408 |  0:00:17s
epoch 56 | loss: 0.55892 | val_0_rmse: 0.76054 | val_1_rmse: 0.82377 |  0:00:17s
epoch 57 | loss: 0.55747 | val_0_rmse: 0.75453 | val_1_rmse: 0.8182  |  0:00:18s
epoch 58 | loss: 0.54675 | val_0_rmse: 0.74854 | val_1_rmse: 0.82467 |  0:00:18s
epoch 59 | loss: 0.54191 | val_0_rmse: 0.75279 | val_1_rmse: 0.8284  |  0:00:18s
epoch 60 | loss: 0.54778 | val_0_rmse: 0.74669 | val_1_rmse: 0.81426 |  0:00:19s
epoch 61 | loss: 0.54351 | val_0_rmse: 0.74692 | val_1_rmse: 0.81271 |  0:00:19s
epoch 62 | loss: 0.52962 | val_0_rmse: 0.74566 | val_1_rmse: 0.81589 |  0:00:19s
epoch 63 | loss: 0.5348  | val_0_rmse: 0.74285 | val_1_rmse: 0.81338 |  0:00:19s
epoch 64 | loss: 0.53261 | val_0_rmse: 0.7715  | val_1_rmse: 0.83814 |  0:00:20s
epoch 65 | loss: 0.53691 | val_0_rmse: 0.74385 | val_1_rmse: 0.82211 |  0:00:20s
epoch 66 | loss: 0.5301  | val_0_rmse: 0.73801 | val_1_rmse: 0.81586 |  0:00:20s
epoch 67 | loss: 0.523   | val_0_rmse: 0.7446  | val_1_rmse: 0.82152 |  0:00:21s
epoch 68 | loss: 0.52983 | val_0_rmse: 0.74237 | val_1_rmse: 0.82002 |  0:00:21s
epoch 69 | loss: 0.52972 | val_0_rmse: 0.73957 | val_1_rmse: 0.81067 |  0:00:21s
epoch 70 | loss: 0.52007 | val_0_rmse: 0.72986 | val_1_rmse: 0.81435 |  0:00:22s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.81041
Best weights from best epoch are automatically used!
ended training at: 00:46:52
Feature importance:
Mean squared error is of 0.039664398229717485
Mean absolute error:0.15132631498469237
MAPE:0.16121148526480464
R2 score:0.41613983800400955
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:46:53
epoch 0  | loss: 1.6     | val_0_rmse: 0.96693 | val_1_rmse: 0.95643 |  0:00:00s
epoch 1  | loss: 1.05084 | val_0_rmse: 0.96908 | val_1_rmse: 0.95889 |  0:00:00s
epoch 2  | loss: 0.91515 | val_0_rmse: 0.88074 | val_1_rmse: 0.86605 |  0:00:00s
epoch 3  | loss: 0.76657 | val_0_rmse: 0.91979 | val_1_rmse: 0.8994  |  0:00:01s
epoch 4  | loss: 0.6873  | val_0_rmse: 0.82937 | val_1_rmse: 0.80959 |  0:00:01s
epoch 5  | loss: 0.6973  | val_0_rmse: 0.95024 | val_1_rmse: 0.93614 |  0:00:01s
epoch 6  | loss: 0.66231 | val_0_rmse: 0.8752  | val_1_rmse: 0.87246 |  0:00:02s
epoch 7  | loss: 0.65474 | val_0_rmse: 0.80514 | val_1_rmse: 0.79943 |  0:00:02s
epoch 8  | loss: 0.64373 | val_0_rmse: 0.81682 | val_1_rmse: 0.81292 |  0:00:02s
epoch 9  | loss: 0.64659 | val_0_rmse: 0.79684 | val_1_rmse: 0.79167 |  0:00:03s
epoch 10 | loss: 0.63872 | val_0_rmse: 0.80447 | val_1_rmse: 0.80086 |  0:00:03s
epoch 11 | loss: 0.63519 | val_0_rmse: 0.80485 | val_1_rmse: 0.80289 |  0:00:03s
epoch 12 | loss: 0.6303  | val_0_rmse: 0.79911 | val_1_rmse: 0.79582 |  0:00:04s
epoch 13 | loss: 0.62846 | val_0_rmse: 0.79604 | val_1_rmse: 0.79278 |  0:00:04s
epoch 14 | loss: 0.62513 | val_0_rmse: 0.79571 | val_1_rmse: 0.79586 |  0:00:04s
epoch 15 | loss: 0.61472 | val_0_rmse: 0.80289 | val_1_rmse: 0.80534 |  0:00:04s
epoch 16 | loss: 0.61473 | val_0_rmse: 0.80836 | val_1_rmse: 0.81095 |  0:00:05s
epoch 17 | loss: 0.61888 | val_0_rmse: 0.802   | val_1_rmse: 0.80247 |  0:00:05s
epoch 18 | loss: 0.64519 | val_0_rmse: 0.81591 | val_1_rmse: 0.81329 |  0:00:05s
epoch 19 | loss: 0.61771 | val_0_rmse: 0.80637 | val_1_rmse: 0.80718 |  0:00:06s
epoch 20 | loss: 0.60779 | val_0_rmse: 0.80252 | val_1_rmse: 0.80237 |  0:00:06s
epoch 21 | loss: 0.61094 | val_0_rmse: 0.79442 | val_1_rmse: 0.79387 |  0:00:06s
epoch 22 | loss: 0.5986  | val_0_rmse: 0.79149 | val_1_rmse: 0.78773 |  0:00:07s
epoch 23 | loss: 0.5981  | val_0_rmse: 0.79664 | val_1_rmse: 0.78912 |  0:00:07s
epoch 24 | loss: 0.59154 | val_0_rmse: 0.78959 | val_1_rmse: 0.7834  |  0:00:07s
epoch 25 | loss: 0.59468 | val_0_rmse: 0.78894 | val_1_rmse: 0.7842  |  0:00:08s
epoch 26 | loss: 0.59341 | val_0_rmse: 0.78624 | val_1_rmse: 0.781   |  0:00:08s
epoch 27 | loss: 0.58766 | val_0_rmse: 0.786   | val_1_rmse: 0.7834  |  0:00:08s
epoch 28 | loss: 0.58865 | val_0_rmse: 0.78541 | val_1_rmse: 0.78013 |  0:00:08s
epoch 29 | loss: 0.58837 | val_0_rmse: 0.7837  | val_1_rmse: 0.7822  |  0:00:09s
epoch 30 | loss: 0.57041 | val_0_rmse: 0.79388 | val_1_rmse: 0.80269 |  0:00:09s
epoch 31 | loss: 0.57408 | val_0_rmse: 0.77883 | val_1_rmse: 0.77827 |  0:00:09s
epoch 32 | loss: 0.58273 | val_0_rmse: 0.80182 | val_1_rmse: 0.80236 |  0:00:10s
epoch 33 | loss: 0.56864 | val_0_rmse: 0.79337 | val_1_rmse: 0.79145 |  0:00:10s
epoch 34 | loss: 0.55871 | val_0_rmse: 0.7937  | val_1_rmse: 0.79398 |  0:00:10s
epoch 35 | loss: 0.56854 | val_0_rmse: 0.78285 | val_1_rmse: 0.78483 |  0:00:11s
epoch 36 | loss: 0.57264 | val_0_rmse: 0.78211 | val_1_rmse: 0.78258 |  0:00:11s
epoch 37 | loss: 0.55172 | val_0_rmse: 0.78618 | val_1_rmse: 0.78965 |  0:00:11s
epoch 38 | loss: 0.55021 | val_0_rmse: 0.78432 | val_1_rmse: 0.78102 |  0:00:12s
epoch 39 | loss: 0.55393 | val_0_rmse: 0.77579 | val_1_rmse: 0.77285 |  0:00:12s
epoch 40 | loss: 0.551   | val_0_rmse: 0.78457 | val_1_rmse: 0.78003 |  0:00:12s
epoch 41 | loss: 0.55257 | val_0_rmse: 0.77531 | val_1_rmse: 0.77076 |  0:00:13s
epoch 42 | loss: 0.55394 | val_0_rmse: 0.77612 | val_1_rmse: 0.76921 |  0:00:13s
epoch 43 | loss: 0.53947 | val_0_rmse: 0.77468 | val_1_rmse: 0.76765 |  0:00:13s
epoch 44 | loss: 0.54269 | val_0_rmse: 0.77447 | val_1_rmse: 0.77172 |  0:00:14s
epoch 45 | loss: 0.54943 | val_0_rmse: 0.77384 | val_1_rmse: 0.77274 |  0:00:14s
epoch 46 | loss: 0.54457 | val_0_rmse: 0.77628 | val_1_rmse: 0.77422 |  0:00:14s
epoch 47 | loss: 0.53749 | val_0_rmse: 0.76962 | val_1_rmse: 0.76938 |  0:00:14s
epoch 48 | loss: 0.54501 | val_0_rmse: 0.76826 | val_1_rmse: 0.7649  |  0:00:15s
epoch 49 | loss: 0.53444 | val_0_rmse: 0.77421 | val_1_rmse: 0.76736 |  0:00:15s
epoch 50 | loss: 0.53782 | val_0_rmse: 0.76834 | val_1_rmse: 0.75996 |  0:00:15s
epoch 51 | loss: 0.5402  | val_0_rmse: 0.76845 | val_1_rmse: 0.75785 |  0:00:16s
epoch 52 | loss: 0.5456  | val_0_rmse: 0.76688 | val_1_rmse: 0.76263 |  0:00:16s
epoch 53 | loss: 0.53419 | val_0_rmse: 0.76132 | val_1_rmse: 0.76255 |  0:00:16s
epoch 54 | loss: 0.53919 | val_0_rmse: 0.76184 | val_1_rmse: 0.76403 |  0:00:17s
epoch 55 | loss: 0.53725 | val_0_rmse: 0.76499 | val_1_rmse: 0.76663 |  0:00:17s
epoch 56 | loss: 0.53892 | val_0_rmse: 0.75838 | val_1_rmse: 0.76674 |  0:00:17s
epoch 57 | loss: 0.55451 | val_0_rmse: 0.76541 | val_1_rmse: 0.76905 |  0:00:18s
epoch 58 | loss: 0.56373 | val_0_rmse: 0.78343 | val_1_rmse: 0.77957 |  0:00:18s
epoch 59 | loss: 0.58349 | val_0_rmse: 0.77366 | val_1_rmse: 0.76979 |  0:00:18s
epoch 60 | loss: 0.57449 | val_0_rmse: 0.76974 | val_1_rmse: 0.77038 |  0:00:18s
epoch 61 | loss: 0.5746  | val_0_rmse: 0.76789 | val_1_rmse: 0.7705  |  0:00:19s
epoch 62 | loss: 0.5666  | val_0_rmse: 0.77048 | val_1_rmse: 0.7756  |  0:00:19s
epoch 63 | loss: 0.57025 | val_0_rmse: 0.78514 | val_1_rmse: 0.78389 |  0:00:19s
epoch 64 | loss: 0.57373 | val_0_rmse: 0.7802  | val_1_rmse: 0.78506 |  0:00:20s
epoch 65 | loss: 0.57869 | val_0_rmse: 0.77343 | val_1_rmse: 0.79131 |  0:00:20s
epoch 66 | loss: 0.57852 | val_0_rmse: 0.77216 | val_1_rmse: 0.77959 |  0:00:20s
epoch 67 | loss: 0.57343 | val_0_rmse: 0.77012 | val_1_rmse: 0.77502 |  0:00:21s
epoch 68 | loss: 0.56762 | val_0_rmse: 0.76819 | val_1_rmse: 0.77169 |  0:00:21s
epoch 69 | loss: 0.57108 | val_0_rmse: 0.76909 | val_1_rmse: 0.7728  |  0:00:21s
epoch 70 | loss: 0.56754 | val_0_rmse: 0.7575  | val_1_rmse: 0.76717 |  0:00:22s
epoch 71 | loss: 0.56693 | val_0_rmse: 0.75262 | val_1_rmse: 0.7701  |  0:00:22s
epoch 72 | loss: 0.54898 | val_0_rmse: 0.7506  | val_1_rmse: 0.76272 |  0:00:22s
epoch 73 | loss: 0.5476  | val_0_rmse: 0.75171 | val_1_rmse: 0.7641  |  0:00:23s
epoch 74 | loss: 0.55621 | val_0_rmse: 0.74675 | val_1_rmse: 0.76156 |  0:00:23s
epoch 75 | loss: 0.55072 | val_0_rmse: 0.74365 | val_1_rmse: 0.76221 |  0:00:23s
epoch 76 | loss: 0.54813 | val_0_rmse: 0.74479 | val_1_rmse: 0.76705 |  0:00:24s
epoch 77 | loss: 0.55038 | val_0_rmse: 0.74317 | val_1_rmse: 0.75828 |  0:00:24s
epoch 78 | loss: 0.55553 | val_0_rmse: 0.74356 | val_1_rmse: 0.75663 |  0:00:24s
epoch 79 | loss: 0.54175 | val_0_rmse: 0.74193 | val_1_rmse: 0.76178 |  0:00:24s
epoch 80 | loss: 0.53547 | val_0_rmse: 0.73546 | val_1_rmse: 0.76058 |  0:00:25s
epoch 81 | loss: 0.53504 | val_0_rmse: 0.73124 | val_1_rmse: 0.75913 |  0:00:25s
epoch 82 | loss: 0.53631 | val_0_rmse: 0.73136 | val_1_rmse: 0.75609 |  0:00:25s
epoch 83 | loss: 0.53062 | val_0_rmse: 0.73146 | val_1_rmse: 0.75433 |  0:00:26s
epoch 84 | loss: 0.52766 | val_0_rmse: 0.73315 | val_1_rmse: 0.75981 |  0:00:26s
epoch 85 | loss: 0.53128 | val_0_rmse: 0.72716 | val_1_rmse: 0.75702 |  0:00:26s
epoch 86 | loss: 0.52431 | val_0_rmse: 0.72905 | val_1_rmse: 0.75262 |  0:00:27s
epoch 87 | loss: 0.52778 | val_0_rmse: 0.73518 | val_1_rmse: 0.75433 |  0:00:27s
epoch 88 | loss: 0.53289 | val_0_rmse: 0.73415 | val_1_rmse: 0.75747 |  0:00:27s
epoch 89 | loss: 0.52922 | val_0_rmse: 0.73224 | val_1_rmse: 0.76357 |  0:00:28s
epoch 90 | loss: 0.52678 | val_0_rmse: 0.73369 | val_1_rmse: 0.76018 |  0:00:28s
epoch 91 | loss: 0.52426 | val_0_rmse: 0.73016 | val_1_rmse: 0.76048 |  0:00:28s
epoch 92 | loss: 0.52255 | val_0_rmse: 0.72371 | val_1_rmse: 0.7555  |  0:00:28s
epoch 93 | loss: 0.5188  | val_0_rmse: 0.72832 | val_1_rmse: 0.76299 |  0:00:29s
epoch 94 | loss: 0.51884 | val_0_rmse: 0.71745 | val_1_rmse: 0.75975 |  0:00:29s
epoch 95 | loss: 0.51581 | val_0_rmse: 0.71515 | val_1_rmse: 0.75216 |  0:00:29s
epoch 96 | loss: 0.52415 | val_0_rmse: 0.71912 | val_1_rmse: 0.7519  |  0:00:30s
epoch 97 | loss: 0.51323 | val_0_rmse: 0.72011 | val_1_rmse: 0.76073 |  0:00:30s
epoch 98 | loss: 0.51394 | val_0_rmse: 0.71382 | val_1_rmse: 0.75257 |  0:00:30s
epoch 99 | loss: 0.51402 | val_0_rmse: 0.71692 | val_1_rmse: 0.75865 |  0:00:31s
epoch 100| loss: 0.54123 | val_0_rmse: 0.7269  | val_1_rmse: 0.76294 |  0:00:31s
epoch 101| loss: 0.54291 | val_0_rmse: 0.7358  | val_1_rmse: 0.76604 |  0:00:31s
epoch 102| loss: 0.53073 | val_0_rmse: 0.7302  | val_1_rmse: 0.76793 |  0:00:32s
epoch 103| loss: 0.52301 | val_0_rmse: 0.71425 | val_1_rmse: 0.76515 |  0:00:32s
epoch 104| loss: 0.52343 | val_0_rmse: 0.72283 | val_1_rmse: 0.77201 |  0:00:32s
epoch 105| loss: 0.52881 | val_0_rmse: 0.71606 | val_1_rmse: 0.75144 |  0:00:32s
epoch 106| loss: 0.51961 | val_0_rmse: 0.71327 | val_1_rmse: 0.74844 |  0:00:33s
epoch 107| loss: 0.52128 | val_0_rmse: 0.71122 | val_1_rmse: 0.74794 |  0:00:33s
epoch 108| loss: 0.51438 | val_0_rmse: 0.73223 | val_1_rmse: 0.75954 |  0:00:34s
epoch 109| loss: 0.5232  | val_0_rmse: 0.73282 | val_1_rmse: 0.77022 |  0:00:34s
epoch 110| loss: 0.52461 | val_0_rmse: 0.72413 | val_1_rmse: 0.76884 |  0:00:34s
epoch 111| loss: 0.52469 | val_0_rmse: 0.73673 | val_1_rmse: 0.77354 |  0:00:34s
epoch 112| loss: 0.55586 | val_0_rmse: 0.74268 | val_1_rmse: 0.77131 |  0:00:35s
epoch 113| loss: 0.55922 | val_0_rmse: 0.74524 | val_1_rmse: 0.78114 |  0:00:35s
epoch 114| loss: 0.55766 | val_0_rmse: 0.7395  | val_1_rmse: 0.76811 |  0:00:35s
epoch 115| loss: 0.5526  | val_0_rmse: 0.7337  | val_1_rmse: 0.77251 |  0:00:36s
epoch 116| loss: 0.56586 | val_0_rmse: 0.73422 | val_1_rmse: 0.77258 |  0:00:36s
epoch 117| loss: 0.54597 | val_0_rmse: 0.72849 | val_1_rmse: 0.75586 |  0:00:36s
epoch 118| loss: 0.56311 | val_0_rmse: 0.73604 | val_1_rmse: 0.75603 |  0:00:37s
epoch 119| loss: 0.56565 | val_0_rmse: 0.72852 | val_1_rmse: 0.74808 |  0:00:37s
epoch 120| loss: 0.57745 | val_0_rmse: 0.74107 | val_1_rmse: 0.75975 |  0:00:37s
epoch 121| loss: 0.55186 | val_0_rmse: 0.7401  | val_1_rmse: 0.76788 |  0:00:38s
epoch 122| loss: 0.54995 | val_0_rmse: 0.73563 | val_1_rmse: 0.76435 |  0:00:38s
epoch 123| loss: 0.54731 | val_0_rmse: 0.73058 | val_1_rmse: 0.75945 |  0:00:38s
epoch 124| loss: 0.5438  | val_0_rmse: 0.72229 | val_1_rmse: 0.7537  |  0:00:38s
epoch 125| loss: 0.5347  | val_0_rmse: 0.7248  | val_1_rmse: 0.75537 |  0:00:39s
epoch 126| loss: 0.53018 | val_0_rmse: 0.72125 | val_1_rmse: 0.75718 |  0:00:39s
epoch 127| loss: 0.52703 | val_0_rmse: 0.71661 | val_1_rmse: 0.75784 |  0:00:39s
epoch 128| loss: 0.52578 | val_0_rmse: 0.7189  | val_1_rmse: 0.74916 |  0:00:40s
epoch 129| loss: 0.53326 | val_0_rmse: 0.71778 | val_1_rmse: 0.75008 |  0:00:40s
epoch 130| loss: 0.528   | val_0_rmse: 0.71575 | val_1_rmse: 0.76318 |  0:00:40s
epoch 131| loss: 0.52866 | val_0_rmse: 0.71869 | val_1_rmse: 0.76427 |  0:00:41s
epoch 132| loss: 0.53123 | val_0_rmse: 0.71828 | val_1_rmse: 0.76092 |  0:00:41s
epoch 133| loss: 0.52324 | val_0_rmse: 0.71125 | val_1_rmse: 0.75654 |  0:00:41s
epoch 134| loss: 0.51845 | val_0_rmse: 0.70725 | val_1_rmse: 0.74939 |  0:00:42s
epoch 135| loss: 0.51149 | val_0_rmse: 0.71561 | val_1_rmse: 0.7611  |  0:00:42s
epoch 136| loss: 0.51609 | val_0_rmse: 0.7046  | val_1_rmse: 0.75583 |  0:00:42s
epoch 137| loss: 0.50583 | val_0_rmse: 0.70055 | val_1_rmse: 0.75717 |  0:00:42s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.74794
Best weights from best epoch are automatically used!
ended training at: 00:47:36
Feature importance:
Mean squared error is of 0.03846989168189965
Mean absolute error:0.13686548735557258
MAPE:0.1435255824852247
R2 score:0.43990061284859905
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:47:36
epoch 0  | loss: 1.63207 | val_0_rmse: 0.95374 | val_1_rmse: 1.07217 |  0:00:00s
epoch 1  | loss: 0.97846 | val_0_rmse: 0.95119 | val_1_rmse: 1.06461 |  0:00:00s
epoch 2  | loss: 0.8649  | val_0_rmse: 0.89355 | val_1_rmse: 0.95841 |  0:00:01s
epoch 3  | loss: 0.72403 | val_0_rmse: 0.81086 | val_1_rmse: 0.90503 |  0:00:01s
epoch 4  | loss: 0.66088 | val_0_rmse: 0.84436 | val_1_rmse: 0.89078 |  0:00:01s
epoch 5  | loss: 0.65285 | val_0_rmse: 0.78408 | val_1_rmse: 0.87622 |  0:00:02s
epoch 6  | loss: 0.63516 | val_0_rmse: 0.8351  | val_1_rmse: 0.88619 |  0:00:02s
epoch 7  | loss: 0.63209 | val_0_rmse: 0.79706 | val_1_rmse: 0.87474 |  0:00:02s
epoch 8  | loss: 0.6385  | val_0_rmse: 0.78393 | val_1_rmse: 0.86134 |  0:00:02s
epoch 9  | loss: 0.61633 | val_0_rmse: 0.77631 | val_1_rmse: 0.84748 |  0:00:03s
epoch 10 | loss: 0.62324 | val_0_rmse: 0.78104 | val_1_rmse: 0.84616 |  0:00:03s
epoch 11 | loss: 0.61191 | val_0_rmse: 0.78201 | val_1_rmse: 0.86062 |  0:00:03s
epoch 12 | loss: 0.60225 | val_0_rmse: 0.78454 | val_1_rmse: 0.84499 |  0:00:04s
epoch 13 | loss: 0.59687 | val_0_rmse: 0.77575 | val_1_rmse: 0.84119 |  0:00:04s
epoch 14 | loss: 0.59209 | val_0_rmse: 0.77556 | val_1_rmse: 0.86666 |  0:00:04s
epoch 15 | loss: 0.5987  | val_0_rmse: 0.77405 | val_1_rmse: 0.85782 |  0:00:05s
epoch 16 | loss: 0.59672 | val_0_rmse: 0.77428 | val_1_rmse: 0.84714 |  0:00:05s
epoch 17 | loss: 0.59211 | val_0_rmse: 0.76872 | val_1_rmse: 0.84064 |  0:00:05s
epoch 18 | loss: 0.59408 | val_0_rmse: 0.76862 | val_1_rmse: 0.84962 |  0:00:06s
epoch 19 | loss: 0.58796 | val_0_rmse: 0.78521 | val_1_rmse: 0.85037 |  0:00:06s
epoch 20 | loss: 0.58665 | val_0_rmse: 0.76785 | val_1_rmse: 0.83806 |  0:00:06s
epoch 21 | loss: 0.59044 | val_0_rmse: 0.76465 | val_1_rmse: 0.84228 |  0:00:07s
epoch 22 | loss: 0.58521 | val_0_rmse: 0.77223 | val_1_rmse: 0.8434  |  0:00:07s
epoch 23 | loss: 0.59917 | val_0_rmse: 0.76887 | val_1_rmse: 0.84586 |  0:00:07s
epoch 24 | loss: 0.58683 | val_0_rmse: 0.76532 | val_1_rmse: 0.84165 |  0:00:07s
epoch 25 | loss: 0.57896 | val_0_rmse: 0.77275 | val_1_rmse: 0.84775 |  0:00:08s
epoch 26 | loss: 0.5815  | val_0_rmse: 0.76891 | val_1_rmse: 0.85485 |  0:00:08s
epoch 27 | loss: 0.57246 | val_0_rmse: 0.75994 | val_1_rmse: 0.84265 |  0:00:08s
epoch 28 | loss: 0.56501 | val_0_rmse: 0.76895 | val_1_rmse: 0.84924 |  0:00:09s
epoch 29 | loss: 0.56354 | val_0_rmse: 0.76301 | val_1_rmse: 0.84477 |  0:00:09s
epoch 30 | loss: 0.56396 | val_0_rmse: 0.7658  | val_1_rmse: 0.85205 |  0:00:09s
epoch 31 | loss: 0.55989 | val_0_rmse: 0.76565 | val_1_rmse: 0.85002 |  0:00:10s
epoch 32 | loss: 0.56021 | val_0_rmse: 0.76296 | val_1_rmse: 0.84648 |  0:00:10s
epoch 33 | loss: 0.56284 | val_0_rmse: 0.76125 | val_1_rmse: 0.84376 |  0:00:10s
epoch 34 | loss: 0.5594  | val_0_rmse: 0.7605  | val_1_rmse: 0.84375 |  0:00:10s
epoch 35 | loss: 0.56181 | val_0_rmse: 0.77262 | val_1_rmse: 0.84746 |  0:00:11s
epoch 36 | loss: 0.56661 | val_0_rmse: 0.77049 | val_1_rmse: 0.85185 |  0:00:11s
epoch 37 | loss: 0.57089 | val_0_rmse: 0.76295 | val_1_rmse: 0.84139 |  0:00:11s
epoch 38 | loss: 0.56261 | val_0_rmse: 0.7669  | val_1_rmse: 0.84744 |  0:00:12s
epoch 39 | loss: 0.56221 | val_0_rmse: 0.76925 | val_1_rmse: 0.85428 |  0:00:12s
epoch 40 | loss: 0.54909 | val_0_rmse: 0.76172 | val_1_rmse: 0.84975 |  0:00:12s
epoch 41 | loss: 0.55543 | val_0_rmse: 0.76244 | val_1_rmse: 0.85083 |  0:00:13s
epoch 42 | loss: 0.55187 | val_0_rmse: 0.76012 | val_1_rmse: 0.85134 |  0:00:13s
epoch 43 | loss: 0.5524  | val_0_rmse: 0.75791 | val_1_rmse: 0.85135 |  0:00:13s
epoch 44 | loss: 0.5613  | val_0_rmse: 0.75844 | val_1_rmse: 0.85492 |  0:00:14s
epoch 45 | loss: 0.54935 | val_0_rmse: 0.75154 | val_1_rmse: 0.84627 |  0:00:14s
epoch 46 | loss: 0.54362 | val_0_rmse: 0.75012 | val_1_rmse: 0.83898 |  0:00:14s
epoch 47 | loss: 0.55039 | val_0_rmse: 0.74907 | val_1_rmse: 0.84403 |  0:00:15s
epoch 48 | loss: 0.55314 | val_0_rmse: 0.74662 | val_1_rmse: 0.8365  |  0:00:15s
epoch 49 | loss: 0.54953 | val_0_rmse: 0.74923 | val_1_rmse: 0.84443 |  0:00:15s
epoch 50 | loss: 0.54799 | val_0_rmse: 0.74641 | val_1_rmse: 0.83763 |  0:00:16s
epoch 51 | loss: 0.54589 | val_0_rmse: 0.75016 | val_1_rmse: 0.85013 |  0:00:16s
epoch 52 | loss: 0.54852 | val_0_rmse: 0.74473 | val_1_rmse: 0.84101 |  0:00:16s
epoch 53 | loss: 0.54842 | val_0_rmse: 0.74651 | val_1_rmse: 0.84849 |  0:00:16s
epoch 54 | loss: 0.54463 | val_0_rmse: 0.74676 | val_1_rmse: 0.85045 |  0:00:17s
epoch 55 | loss: 0.55158 | val_0_rmse: 0.75    | val_1_rmse: 0.85104 |  0:00:17s
epoch 56 | loss: 0.54967 | val_0_rmse: 0.75402 | val_1_rmse: 0.8571  |  0:00:17s
epoch 57 | loss: 0.56396 | val_0_rmse: 0.75151 | val_1_rmse: 0.85296 |  0:00:18s
epoch 58 | loss: 0.56014 | val_0_rmse: 0.753   | val_1_rmse: 0.85476 |  0:00:18s
epoch 59 | loss: 0.5536  | val_0_rmse: 0.75595 | val_1_rmse: 0.84933 |  0:00:18s
epoch 60 | loss: 0.56298 | val_0_rmse: 0.76163 | val_1_rmse: 0.85671 |  0:00:19s
epoch 61 | loss: 0.56723 | val_0_rmse: 0.77511 | val_1_rmse: 0.84326 |  0:00:19s
epoch 62 | loss: 0.58173 | val_0_rmse: 0.76618 | val_1_rmse: 0.84661 |  0:00:19s
epoch 63 | loss: 0.58314 | val_0_rmse: 0.76399 | val_1_rmse: 0.84449 |  0:00:19s
epoch 64 | loss: 0.5714  | val_0_rmse: 0.76689 | val_1_rmse: 0.83758 |  0:00:20s
epoch 65 | loss: 0.57387 | val_0_rmse: 0.75957 | val_1_rmse: 0.84387 |  0:00:20s
epoch 66 | loss: 0.58045 | val_0_rmse: 0.75851 | val_1_rmse: 0.84475 |  0:00:20s
epoch 67 | loss: 0.57527 | val_0_rmse: 0.76159 | val_1_rmse: 0.83993 |  0:00:21s
epoch 68 | loss: 0.57209 | val_0_rmse: 0.76104 | val_1_rmse: 0.85636 |  0:00:21s
epoch 69 | loss: 0.57689 | val_0_rmse: 0.75504 | val_1_rmse: 0.84507 |  0:00:21s
epoch 70 | loss: 0.57328 | val_0_rmse: 0.75389 | val_1_rmse: 0.84612 |  0:00:22s
epoch 71 | loss: 0.57123 | val_0_rmse: 0.75673 | val_1_rmse: 0.85543 |  0:00:22s
epoch 72 | loss: 0.56716 | val_0_rmse: 0.75544 | val_1_rmse: 0.84626 |  0:00:22s
epoch 73 | loss: 0.56669 | val_0_rmse: 0.75422 | val_1_rmse: 0.86022 |  0:00:23s
epoch 74 | loss: 0.57147 | val_0_rmse: 0.76117 | val_1_rmse: 0.87265 |  0:00:23s
epoch 75 | loss: 0.56845 | val_0_rmse: 0.75725 | val_1_rmse: 0.86319 |  0:00:23s
epoch 76 | loss: 0.56703 | val_0_rmse: 0.75549 | val_1_rmse: 0.85588 |  0:00:23s
epoch 77 | loss: 0.56823 | val_0_rmse: 0.75406 | val_1_rmse: 0.85513 |  0:00:24s
epoch 78 | loss: 0.57238 | val_0_rmse: 0.75546 | val_1_rmse: 0.8599  |  0:00:24s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.8365
Best weights from best epoch are automatically used!
ended training at: 00:48:01
Feature importance:
Mean squared error is of 0.04261343676022983
Mean absolute error:0.153527085126271
MAPE:0.15766199699233366
R2 score:0.39402462070360234
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:48:01
epoch 0  | loss: 1.79546 | val_0_rmse: 1.01132 | val_1_rmse: 0.95962 |  0:00:00s
epoch 1  | loss: 1.02049 | val_0_rmse: 0.98566 | val_1_rmse: 0.93371 |  0:00:00s
epoch 2  | loss: 0.84167 | val_0_rmse: 0.91429 | val_1_rmse: 0.86178 |  0:00:00s
epoch 3  | loss: 0.73627 | val_0_rmse: 0.85294 | val_1_rmse: 0.81692 |  0:00:01s
epoch 4  | loss: 0.70657 | val_0_rmse: 0.84061 | val_1_rmse: 0.81332 |  0:00:01s
epoch 5  | loss: 0.6663  | val_0_rmse: 0.81746 | val_1_rmse: 0.79408 |  0:00:01s
epoch 6  | loss: 0.65501 | val_0_rmse: 0.81978 | val_1_rmse: 0.79016 |  0:00:02s
epoch 7  | loss: 0.63804 | val_0_rmse: 0.8111  | val_1_rmse: 0.78928 |  0:00:02s
epoch 8  | loss: 0.63603 | val_0_rmse: 0.8128  | val_1_rmse: 0.78991 |  0:00:02s
epoch 9  | loss: 0.63909 | val_0_rmse: 0.80571 | val_1_rmse: 0.78451 |  0:00:03s
epoch 10 | loss: 0.62936 | val_0_rmse: 0.80035 | val_1_rmse: 0.78065 |  0:00:03s
epoch 11 | loss: 0.64111 | val_0_rmse: 0.80194 | val_1_rmse: 0.77992 |  0:00:03s
epoch 12 | loss: 0.62991 | val_0_rmse: 0.80306 | val_1_rmse: 0.77697 |  0:00:04s
epoch 13 | loss: 0.62058 | val_0_rmse: 0.79107 | val_1_rmse: 0.775   |  0:00:04s
epoch 14 | loss: 0.62223 | val_0_rmse: 0.79307 | val_1_rmse: 0.77308 |  0:00:04s
epoch 15 | loss: 0.61907 | val_0_rmse: 0.80581 | val_1_rmse: 0.78181 |  0:00:05s
epoch 16 | loss: 0.61594 | val_0_rmse: 0.8014  | val_1_rmse: 0.78417 |  0:00:05s
epoch 17 | loss: 0.61718 | val_0_rmse: 0.78929 | val_1_rmse: 0.77483 |  0:00:05s
epoch 18 | loss: 0.61604 | val_0_rmse: 0.7929  | val_1_rmse: 0.77325 |  0:00:05s
epoch 19 | loss: 0.61114 | val_0_rmse: 0.78733 | val_1_rmse: 0.78194 |  0:00:06s
epoch 20 | loss: 0.61191 | val_0_rmse: 0.78535 | val_1_rmse: 0.76592 |  0:00:06s
epoch 21 | loss: 0.60054 | val_0_rmse: 0.77842 | val_1_rmse: 0.76915 |  0:00:06s
epoch 22 | loss: 0.59874 | val_0_rmse: 0.78293 | val_1_rmse: 0.76895 |  0:00:07s
epoch 23 | loss: 0.57874 | val_0_rmse: 0.78669 | val_1_rmse: 0.76884 |  0:00:07s
epoch 24 | loss: 0.58957 | val_0_rmse: 0.78323 | val_1_rmse: 0.77868 |  0:00:07s
epoch 25 | loss: 0.58793 | val_0_rmse: 0.78315 | val_1_rmse: 0.76711 |  0:00:08s
epoch 26 | loss: 0.59526 | val_0_rmse: 0.7917  | val_1_rmse: 0.77377 |  0:00:08s
epoch 27 | loss: 0.59334 | val_0_rmse: 0.78217 | val_1_rmse: 0.77399 |  0:00:08s
epoch 28 | loss: 0.58849 | val_0_rmse: 0.77823 | val_1_rmse: 0.77832 |  0:00:09s
epoch 29 | loss: 0.57327 | val_0_rmse: 0.78    | val_1_rmse: 0.77215 |  0:00:09s
epoch 30 | loss: 0.57796 | val_0_rmse: 0.78197 | val_1_rmse: 0.78461 |  0:00:09s
epoch 31 | loss: 0.57535 | val_0_rmse: 0.7727  | val_1_rmse: 0.78486 |  0:00:10s
epoch 32 | loss: 0.57731 | val_0_rmse: 0.78549 | val_1_rmse: 0.77889 |  0:00:10s
epoch 33 | loss: 0.57954 | val_0_rmse: 0.77358 | val_1_rmse: 0.78831 |  0:00:10s
epoch 34 | loss: 0.5729  | val_0_rmse: 0.79436 | val_1_rmse: 0.80056 |  0:00:11s
epoch 35 | loss: 0.56447 | val_0_rmse: 0.77247 | val_1_rmse: 0.77566 |  0:00:11s
epoch 36 | loss: 0.56811 | val_0_rmse: 0.77049 | val_1_rmse: 0.78347 |  0:00:11s
epoch 37 | loss: 0.55879 | val_0_rmse: 0.79046 | val_1_rmse: 0.83132 |  0:00:11s
epoch 38 | loss: 0.56249 | val_0_rmse: 0.76735 | val_1_rmse: 0.78672 |  0:00:12s
epoch 39 | loss: 0.56085 | val_0_rmse: 0.77798 | val_1_rmse: 0.81186 |  0:00:12s
epoch 40 | loss: 0.56441 | val_0_rmse: 0.77325 | val_1_rmse: 0.80791 |  0:00:12s
epoch 41 | loss: 0.56836 | val_0_rmse: 0.76317 | val_1_rmse: 0.79137 |  0:00:13s
epoch 42 | loss: 0.56084 | val_0_rmse: 0.77164 | val_1_rmse: 0.79552 |  0:00:13s
epoch 43 | loss: 0.56403 | val_0_rmse: 0.76119 | val_1_rmse: 0.78062 |  0:00:13s
epoch 44 | loss: 0.57913 | val_0_rmse: 0.76412 | val_1_rmse: 0.77333 |  0:00:14s
epoch 45 | loss: 0.55959 | val_0_rmse: 0.76563 | val_1_rmse: 0.79119 |  0:00:14s
epoch 46 | loss: 0.56693 | val_0_rmse: 0.76676 | val_1_rmse: 0.77883 |  0:00:14s
epoch 47 | loss: 0.55997 | val_0_rmse: 0.75569 | val_1_rmse: 0.77671 |  0:00:15s
epoch 48 | loss: 0.54912 | val_0_rmse: 0.77068 | val_1_rmse: 0.78087 |  0:00:15s
epoch 49 | loss: 0.55332 | val_0_rmse: 0.76094 | val_1_rmse: 0.76539 |  0:00:15s
epoch 50 | loss: 0.55027 | val_0_rmse: 0.75232 | val_1_rmse: 0.77582 |  0:00:15s
epoch 51 | loss: 0.55371 | val_0_rmse: 0.75714 | val_1_rmse: 0.76744 |  0:00:16s
epoch 52 | loss: 0.54801 | val_0_rmse: 0.75586 | val_1_rmse: 0.76701 |  0:00:16s
epoch 53 | loss: 0.54216 | val_0_rmse: 0.76029 | val_1_rmse: 0.79373 |  0:00:16s
epoch 54 | loss: 0.55692 | val_0_rmse: 0.76459 | val_1_rmse: 0.78023 |  0:00:17s
epoch 55 | loss: 0.54838 | val_0_rmse: 0.75567 | val_1_rmse: 0.78921 |  0:00:17s
epoch 56 | loss: 0.54256 | val_0_rmse: 0.75831 | val_1_rmse: 0.77791 |  0:00:17s
epoch 57 | loss: 0.53933 | val_0_rmse: 0.76736 | val_1_rmse: 0.76597 |  0:00:18s
epoch 58 | loss: 0.54981 | val_0_rmse: 0.75376 | val_1_rmse: 0.76925 |  0:00:18s
epoch 59 | loss: 0.54363 | val_0_rmse: 0.75258 | val_1_rmse: 0.75626 |  0:00:18s
epoch 60 | loss: 0.55603 | val_0_rmse: 0.74251 | val_1_rmse: 0.76901 |  0:00:19s
epoch 61 | loss: 0.54908 | val_0_rmse: 0.76777 | val_1_rmse: 0.82507 |  0:00:19s
epoch 62 | loss: 0.55543 | val_0_rmse: 0.74954 | val_1_rmse: 0.77684 |  0:00:19s
epoch 63 | loss: 0.56186 | val_0_rmse: 0.75371 | val_1_rmse: 0.78297 |  0:00:19s
epoch 64 | loss: 0.55892 | val_0_rmse: 0.7482  | val_1_rmse: 0.77905 |  0:00:20s
epoch 65 | loss: 0.53699 | val_0_rmse: 0.75264 | val_1_rmse: 0.77488 |  0:00:20s
epoch 66 | loss: 0.5372  | val_0_rmse: 0.74244 | val_1_rmse: 0.76356 |  0:00:20s
epoch 67 | loss: 0.53813 | val_0_rmse: 0.74043 | val_1_rmse: 0.7565  |  0:00:21s
epoch 68 | loss: 0.53236 | val_0_rmse: 0.74063 | val_1_rmse: 0.77626 |  0:00:21s
epoch 69 | loss: 0.54429 | val_0_rmse: 0.74314 | val_1_rmse: 0.77302 |  0:00:21s
epoch 70 | loss: 0.53857 | val_0_rmse: 0.745   | val_1_rmse: 0.7739  |  0:00:22s
epoch 71 | loss: 0.53364 | val_0_rmse: 0.7519  | val_1_rmse: 0.75991 |  0:00:22s
epoch 72 | loss: 0.54919 | val_0_rmse: 0.76759 | val_1_rmse: 0.75326 |  0:00:22s
epoch 73 | loss: 0.54471 | val_0_rmse: 0.74314 | val_1_rmse: 0.77271 |  0:00:23s
epoch 74 | loss: 0.54429 | val_0_rmse: 0.74529 | val_1_rmse: 0.76245 |  0:00:23s
epoch 75 | loss: 0.53852 | val_0_rmse: 0.73895 | val_1_rmse: 0.78804 |  0:00:23s
epoch 76 | loss: 0.54401 | val_0_rmse: 0.75309 | val_1_rmse: 0.81767 |  0:00:24s
epoch 77 | loss: 0.54654 | val_0_rmse: 0.73733 | val_1_rmse: 0.75723 |  0:00:24s
epoch 78 | loss: 0.53654 | val_0_rmse: 0.75355 | val_1_rmse: 0.7824  |  0:00:24s
epoch 79 | loss: 0.55345 | val_0_rmse: 0.74094 | val_1_rmse: 0.75305 |  0:00:25s
epoch 80 | loss: 0.54315 | val_0_rmse: 0.74645 | val_1_rmse: 0.7745  |  0:00:25s
epoch 81 | loss: 0.54649 | val_0_rmse: 0.74498 | val_1_rmse: 0.77169 |  0:00:25s
epoch 82 | loss: 0.56459 | val_0_rmse: 0.75505 | val_1_rmse: 0.77217 |  0:00:25s
epoch 83 | loss: 0.56604 | val_0_rmse: 0.75939 | val_1_rmse: 0.77602 |  0:00:26s
epoch 84 | loss: 0.55484 | val_0_rmse: 0.74989 | val_1_rmse: 0.74689 |  0:00:26s
epoch 85 | loss: 0.55354 | val_0_rmse: 0.74809 | val_1_rmse: 0.75902 |  0:00:26s
epoch 86 | loss: 0.56221 | val_0_rmse: 0.74792 | val_1_rmse: 0.75593 |  0:00:27s
epoch 87 | loss: 0.56729 | val_0_rmse: 0.76299 | val_1_rmse: 0.75662 |  0:00:27s
epoch 88 | loss: 0.56636 | val_0_rmse: 0.74814 | val_1_rmse: 0.75498 |  0:00:27s
epoch 89 | loss: 0.55963 | val_0_rmse: 0.74848 | val_1_rmse: 0.75493 |  0:00:28s
epoch 90 | loss: 0.55418 | val_0_rmse: 0.73848 | val_1_rmse: 0.75339 |  0:00:28s
epoch 91 | loss: 0.55167 | val_0_rmse: 0.74391 | val_1_rmse: 0.7917  |  0:00:28s
epoch 92 | loss: 0.56131 | val_0_rmse: 0.73517 | val_1_rmse: 0.75671 |  0:00:29s
epoch 93 | loss: 0.55063 | val_0_rmse: 0.73536 | val_1_rmse: 0.75901 |  0:00:29s
epoch 94 | loss: 0.54771 | val_0_rmse: 0.7368  | val_1_rmse: 0.76836 |  0:00:29s
epoch 95 | loss: 0.55054 | val_0_rmse: 0.73832 | val_1_rmse: 0.76111 |  0:00:29s
epoch 96 | loss: 0.55779 | val_0_rmse: 0.74152 | val_1_rmse: 0.75723 |  0:00:30s
epoch 97 | loss: 0.54972 | val_0_rmse: 0.74012 | val_1_rmse: 0.75487 |  0:00:30s
epoch 98 | loss: 0.54365 | val_0_rmse: 0.742   | val_1_rmse: 0.75384 |  0:00:30s
epoch 99 | loss: 0.54352 | val_0_rmse: 0.72898 | val_1_rmse: 0.74885 |  0:00:31s
epoch 100| loss: 0.54418 | val_0_rmse: 0.72876 | val_1_rmse: 0.75297 |  0:00:31s
epoch 101| loss: 0.5371  | val_0_rmse: 0.72462 | val_1_rmse: 0.7648  |  0:00:31s
epoch 102| loss: 0.52604 | val_0_rmse: 0.72849 | val_1_rmse: 0.74731 |  0:00:32s
epoch 103| loss: 0.52446 | val_0_rmse: 0.73233 | val_1_rmse: 0.79096 |  0:00:32s
epoch 104| loss: 0.52788 | val_0_rmse: 0.72178 | val_1_rmse: 0.75518 |  0:00:32s
epoch 105| loss: 0.53961 | val_0_rmse: 0.72603 | val_1_rmse: 0.75288 |  0:00:33s
epoch 106| loss: 0.53576 | val_0_rmse: 0.74272 | val_1_rmse: 0.81384 |  0:00:33s
epoch 107| loss: 0.53832 | val_0_rmse: 0.71886 | val_1_rmse: 0.75806 |  0:00:33s
epoch 108| loss: 0.54661 | val_0_rmse: 0.72026 | val_1_rmse: 0.76947 |  0:00:34s
epoch 109| loss: 0.54128 | val_0_rmse: 0.73015 | val_1_rmse: 0.79858 |  0:00:34s
epoch 110| loss: 0.54705 | val_0_rmse: 0.72771 | val_1_rmse: 0.76743 |  0:00:34s
epoch 111| loss: 0.54067 | val_0_rmse: 0.72599 | val_1_rmse: 0.76386 |  0:00:35s
epoch 112| loss: 0.53851 | val_0_rmse: 0.72624 | val_1_rmse: 0.76575 |  0:00:35s
epoch 113| loss: 0.53175 | val_0_rmse: 0.72085 | val_1_rmse: 0.7584  |  0:00:35s
epoch 114| loss: 0.52684 | val_0_rmse: 0.71618 | val_1_rmse: 0.75951 |  0:00:36s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.74689
Best weights from best epoch are automatically used!
ended training at: 00:48:37
Feature importance:
Mean squared error is of 0.03788243829042153
Mean absolute error:0.14678734734755186
MAPE:0.15588829026062775
R2 score:0.3905143207370615
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:48:38
epoch 0  | loss: 1.78481 | val_0_rmse: 0.99988 | val_1_rmse: 0.99764 |  0:00:00s
epoch 1  | loss: 1.15258 | val_0_rmse: 0.99597 | val_1_rmse: 0.99686 |  0:00:00s
epoch 2  | loss: 1.05509 | val_0_rmse: 0.99208 | val_1_rmse: 0.99206 |  0:00:00s
epoch 3  | loss: 0.97829 | val_0_rmse: 0.97959 | val_1_rmse: 0.98062 |  0:00:01s
epoch 4  | loss: 0.92377 | val_0_rmse: 0.93453 | val_1_rmse: 0.94115 |  0:00:01s
epoch 5  | loss: 0.87687 | val_0_rmse: 0.95111 | val_1_rmse: 0.9678  |  0:00:01s
epoch 6  | loss: 0.86457 | val_0_rmse: 0.91414 | val_1_rmse: 0.92856 |  0:00:02s
epoch 7  | loss: 0.85661 | val_0_rmse: 0.92548 | val_1_rmse: 0.93538 |  0:00:02s
epoch 8  | loss: 0.84861 | val_0_rmse: 0.91611 | val_1_rmse: 0.92538 |  0:00:02s
epoch 9  | loss: 0.85189 | val_0_rmse: 0.91222 | val_1_rmse: 0.93145 |  0:00:03s
epoch 10 | loss: 0.84181 | val_0_rmse: 0.91011 | val_1_rmse: 0.92525 |  0:00:03s
epoch 11 | loss: 0.83213 | val_0_rmse: 0.9146  | val_1_rmse: 0.9241  |  0:00:03s
epoch 12 | loss: 0.82968 | val_0_rmse: 0.91545 | val_1_rmse: 0.92274 |  0:00:04s
epoch 13 | loss: 0.82167 | val_0_rmse: 0.91119 | val_1_rmse: 0.92436 |  0:00:04s
epoch 14 | loss: 0.82203 | val_0_rmse: 0.90928 | val_1_rmse: 0.92482 |  0:00:04s
epoch 15 | loss: 0.81495 | val_0_rmse: 0.90856 | val_1_rmse: 0.91931 |  0:00:05s
epoch 16 | loss: 0.81385 | val_0_rmse: 0.91394 | val_1_rmse: 0.91883 |  0:00:05s
epoch 17 | loss: 0.81559 | val_0_rmse: 0.90846 | val_1_rmse: 0.9182  |  0:00:05s
epoch 18 | loss: 0.79932 | val_0_rmse: 0.90575 | val_1_rmse: 0.92187 |  0:00:05s
epoch 19 | loss: 0.803   | val_0_rmse: 0.88615 | val_1_rmse: 0.91201 |  0:00:06s
epoch 20 | loss: 0.77756 | val_0_rmse: 0.87956 | val_1_rmse: 0.90748 |  0:00:06s
epoch 21 | loss: 0.7773  | val_0_rmse: 0.86896 | val_1_rmse: 0.89258 |  0:00:06s
epoch 22 | loss: 0.75483 | val_0_rmse: 0.86185 | val_1_rmse: 0.88197 |  0:00:07s
epoch 23 | loss: 0.75903 | val_0_rmse: 0.84818 | val_1_rmse: 0.87315 |  0:00:07s
epoch 24 | loss: 0.74421 | val_0_rmse: 0.84487 | val_1_rmse: 0.86484 |  0:00:07s
epoch 25 | loss: 0.7158  | val_0_rmse: 0.83787 | val_1_rmse: 0.85141 |  0:00:08s
epoch 26 | loss: 0.71547 | val_0_rmse: 0.83046 | val_1_rmse: 0.84308 |  0:00:08s
epoch 27 | loss: 0.72152 | val_0_rmse: 0.83439 | val_1_rmse: 0.84877 |  0:00:08s
epoch 28 | loss: 0.7069  | val_0_rmse: 0.82265 | val_1_rmse: 0.84824 |  0:00:09s
epoch 29 | loss: 0.69614 | val_0_rmse: 0.82347 | val_1_rmse: 0.84117 |  0:00:09s
epoch 30 | loss: 0.69841 | val_0_rmse: 0.83151 | val_1_rmse: 0.84973 |  0:00:09s
epoch 31 | loss: 0.68169 | val_0_rmse: 0.81408 | val_1_rmse: 0.84693 |  0:00:10s
epoch 32 | loss: 0.66501 | val_0_rmse: 0.81367 | val_1_rmse: 0.83891 |  0:00:10s
epoch 33 | loss: 0.685   | val_0_rmse: 0.81024 | val_1_rmse: 0.85307 |  0:00:10s
epoch 34 | loss: 0.65181 | val_0_rmse: 0.81822 | val_1_rmse: 0.8631  |  0:00:11s
epoch 35 | loss: 0.65384 | val_0_rmse: 0.80592 | val_1_rmse: 0.83628 |  0:00:11s
epoch 36 | loss: 0.65444 | val_0_rmse: 0.80519 | val_1_rmse: 0.82441 |  0:00:11s
epoch 37 | loss: 0.63652 | val_0_rmse: 0.8073  | val_1_rmse: 0.83059 |  0:00:12s
epoch 38 | loss: 0.64586 | val_0_rmse: 0.80348 | val_1_rmse: 0.82958 |  0:00:12s
epoch 39 | loss: 0.64425 | val_0_rmse: 0.83871 | val_1_rmse: 0.85674 |  0:00:12s
epoch 40 | loss: 0.6529  | val_0_rmse: 0.90363 | val_1_rmse: 0.92655 |  0:00:13s
epoch 41 | loss: 0.64371 | val_0_rmse: 0.9107  | val_1_rmse: 0.93316 |  0:00:13s
epoch 42 | loss: 0.62931 | val_0_rmse: 0.84252 | val_1_rmse: 0.86326 |  0:00:13s
epoch 43 | loss: 0.61945 | val_0_rmse: 0.80917 | val_1_rmse: 0.82389 |  0:00:13s
epoch 44 | loss: 0.60761 | val_0_rmse: 0.79459 | val_1_rmse: 0.80901 |  0:00:14s
epoch 45 | loss: 0.59682 | val_0_rmse: 0.78517 | val_1_rmse: 0.81004 |  0:00:14s
epoch 46 | loss: 0.58225 | val_0_rmse: 0.7735  | val_1_rmse: 0.80055 |  0:00:14s
epoch 47 | loss: 0.59168 | val_0_rmse: 0.7772  | val_1_rmse: 0.80907 |  0:00:15s
epoch 48 | loss: 0.57642 | val_0_rmse: 0.78726 | val_1_rmse: 0.81327 |  0:00:15s
epoch 49 | loss: 0.59204 | val_0_rmse: 0.79596 | val_1_rmse: 0.81806 |  0:00:15s
epoch 50 | loss: 0.59136 | val_0_rmse: 0.78034 | val_1_rmse: 0.80506 |  0:00:16s
epoch 51 | loss: 0.57834 | val_0_rmse: 0.79071 | val_1_rmse: 0.80129 |  0:00:16s
epoch 52 | loss: 0.58235 | val_0_rmse: 0.7724  | val_1_rmse: 0.79188 |  0:00:16s
epoch 53 | loss: 0.57618 | val_0_rmse: 0.76388 | val_1_rmse: 0.79351 |  0:00:17s
epoch 54 | loss: 0.55914 | val_0_rmse: 0.76081 | val_1_rmse: 0.79597 |  0:00:17s
epoch 55 | loss: 0.5614  | val_0_rmse: 0.7595  | val_1_rmse: 0.79579 |  0:00:17s
epoch 56 | loss: 0.55573 | val_0_rmse: 0.75875 | val_1_rmse: 0.78683 |  0:00:18s
epoch 57 | loss: 0.54834 | val_0_rmse: 0.75416 | val_1_rmse: 0.78235 |  0:00:18s
epoch 58 | loss: 0.54728 | val_0_rmse: 0.75156 | val_1_rmse: 0.7815  |  0:00:18s
epoch 59 | loss: 0.54825 | val_0_rmse: 0.75281 | val_1_rmse: 0.78301 |  0:00:18s
epoch 60 | loss: 0.53946 | val_0_rmse: 0.74461 | val_1_rmse: 0.77964 |  0:00:19s
epoch 61 | loss: 0.53846 | val_0_rmse: 0.74594 | val_1_rmse: 0.77812 |  0:00:19s
epoch 62 | loss: 0.54134 | val_0_rmse: 0.7491  | val_1_rmse: 0.78198 |  0:00:19s
epoch 63 | loss: 0.53973 | val_0_rmse: 0.7463  | val_1_rmse: 0.78891 |  0:00:20s
epoch 64 | loss: 0.52364 | val_0_rmse: 0.75013 | val_1_rmse: 0.78614 |  0:00:20s
epoch 65 | loss: 0.53962 | val_0_rmse: 0.74328 | val_1_rmse: 0.7849  |  0:00:20s
epoch 66 | loss: 0.53655 | val_0_rmse: 0.73999 | val_1_rmse: 0.79031 |  0:00:21s
epoch 67 | loss: 0.53736 | val_0_rmse: 0.74223 | val_1_rmse: 0.79714 |  0:00:21s
epoch 68 | loss: 0.52752 | val_0_rmse: 0.7399  | val_1_rmse: 0.79672 |  0:00:21s
epoch 69 | loss: 0.52706 | val_0_rmse: 0.73556 | val_1_rmse: 0.79245 |  0:00:22s
epoch 70 | loss: 0.52143 | val_0_rmse: 0.73425 | val_1_rmse: 0.78614 |  0:00:22s
epoch 71 | loss: 0.52466 | val_0_rmse: 0.73858 | val_1_rmse: 0.78499 |  0:00:22s
epoch 72 | loss: 0.52529 | val_0_rmse: 0.73683 | val_1_rmse: 0.7896  |  0:00:23s
epoch 73 | loss: 0.52049 | val_0_rmse: 0.74545 | val_1_rmse: 0.78935 |  0:00:23s
epoch 74 | loss: 0.51893 | val_0_rmse: 0.7372  | val_1_rmse: 0.7958  |  0:00:23s
epoch 75 | loss: 0.52341 | val_0_rmse: 0.73111 | val_1_rmse: 0.78279 |  0:00:24s
epoch 76 | loss: 0.5103  | val_0_rmse: 0.73008 | val_1_rmse: 0.78619 |  0:00:24s
epoch 77 | loss: 0.51813 | val_0_rmse: 0.7332  | val_1_rmse: 0.78741 |  0:00:24s
epoch 78 | loss: 0.52237 | val_0_rmse: 0.72858 | val_1_rmse: 0.78301 |  0:00:24s
epoch 79 | loss: 0.51234 | val_0_rmse: 0.72566 | val_1_rmse: 0.78505 |  0:00:25s
epoch 80 | loss: 0.51977 | val_0_rmse: 0.72814 | val_1_rmse: 0.78558 |  0:00:25s
epoch 81 | loss: 0.51772 | val_0_rmse: 0.72469 | val_1_rmse: 0.78063 |  0:00:25s
epoch 82 | loss: 0.50549 | val_0_rmse: 0.72616 | val_1_rmse: 0.77779 |  0:00:26s
epoch 83 | loss: 0.50793 | val_0_rmse: 0.71926 | val_1_rmse: 0.77338 |  0:00:26s
epoch 84 | loss: 0.50808 | val_0_rmse: 0.71858 | val_1_rmse: 0.77977 |  0:00:26s
epoch 85 | loss: 0.51114 | val_0_rmse: 0.71534 | val_1_rmse: 0.78539 |  0:00:27s
epoch 86 | loss: 0.51003 | val_0_rmse: 0.71135 | val_1_rmse: 0.77944 |  0:00:27s
epoch 87 | loss: 0.50635 | val_0_rmse: 0.72627 | val_1_rmse: 0.78464 |  0:00:27s
epoch 88 | loss: 0.50427 | val_0_rmse: 0.72752 | val_1_rmse: 0.79385 |  0:00:28s
epoch 89 | loss: 0.50682 | val_0_rmse: 0.73743 | val_1_rmse: 0.77864 |  0:00:28s
epoch 90 | loss: 0.51506 | val_0_rmse: 0.71668 | val_1_rmse: 0.78831 |  0:00:28s
epoch 91 | loss: 0.50813 | val_0_rmse: 0.71238 | val_1_rmse: 0.79677 |  0:00:29s
epoch 92 | loss: 0.51847 | val_0_rmse: 0.71364 | val_1_rmse: 0.77846 |  0:00:29s
epoch 93 | loss: 0.51031 | val_0_rmse: 0.71488 | val_1_rmse: 0.79102 |  0:00:29s
epoch 94 | loss: 0.51935 | val_0_rmse: 0.71066 | val_1_rmse: 0.77601 |  0:00:29s
epoch 95 | loss: 0.51057 | val_0_rmse: 0.70824 | val_1_rmse: 0.77749 |  0:00:30s
epoch 96 | loss: 0.50991 | val_0_rmse: 0.708   | val_1_rmse: 0.78861 |  0:00:30s
epoch 97 | loss: 0.4995  | val_0_rmse: 0.70098 | val_1_rmse: 0.77961 |  0:00:30s
epoch 98 | loss: 0.49494 | val_0_rmse: 0.69886 | val_1_rmse: 0.78627 |  0:00:31s
epoch 99 | loss: 0.50086 | val_0_rmse: 0.71341 | val_1_rmse: 0.77625 |  0:00:31s
epoch 100| loss: 0.50041 | val_0_rmse: 0.69111 | val_1_rmse: 0.78493 |  0:00:31s
epoch 101| loss: 0.49027 | val_0_rmse: 0.69682 | val_1_rmse: 0.77816 |  0:00:32s
epoch 102| loss: 0.50082 | val_0_rmse: 0.69484 | val_1_rmse: 0.78847 |  0:00:32s
epoch 103| loss: 0.49051 | val_0_rmse: 0.70963 | val_1_rmse: 0.82596 |  0:00:32s
epoch 104| loss: 0.48341 | val_0_rmse: 0.72161 | val_1_rmse: 0.78623 |  0:00:33s
epoch 105| loss: 0.48495 | val_0_rmse: 0.69709 | val_1_rmse: 0.80503 |  0:00:33s
epoch 106| loss: 0.48606 | val_0_rmse: 0.69022 | val_1_rmse: 0.7707  |  0:00:33s
epoch 107| loss: 0.47698 | val_0_rmse: 0.69983 | val_1_rmse: 0.77823 |  0:00:34s
epoch 108| loss: 0.48269 | val_0_rmse: 0.68614 | val_1_rmse: 0.81038 |  0:00:34s
epoch 109| loss: 0.478   | val_0_rmse: 0.68429 | val_1_rmse: 0.77795 |  0:00:34s
epoch 110| loss: 0.47914 | val_0_rmse: 0.68286 | val_1_rmse: 0.7947  |  0:00:35s
epoch 111| loss: 0.48579 | val_0_rmse: 0.71435 | val_1_rmse: 0.84568 |  0:00:35s
epoch 112| loss: 0.47974 | val_0_rmse: 0.68387 | val_1_rmse: 0.78152 |  0:00:35s
epoch 113| loss: 0.46529 | val_0_rmse: 0.67014 | val_1_rmse: 0.78743 |  0:00:36s
epoch 114| loss: 0.46408 | val_0_rmse: 0.66235 | val_1_rmse: 0.78725 |  0:00:36s
epoch 115| loss: 0.46875 | val_0_rmse: 0.66487 | val_1_rmse: 0.78068 |  0:00:36s
epoch 116| loss: 0.45726 | val_0_rmse: 0.67284 | val_1_rmse: 0.78293 |  0:00:36s
epoch 117| loss: 0.45317 | val_0_rmse: 0.67579 | val_1_rmse: 0.82931 |  0:00:37s
epoch 118| loss: 0.45408 | val_0_rmse: 0.6798  | val_1_rmse: 0.78762 |  0:00:37s
epoch 119| loss: 0.47494 | val_0_rmse: 0.66655 | val_1_rmse: 0.78769 |  0:00:37s
epoch 120| loss: 0.46248 | val_0_rmse: 0.67457 | val_1_rmse: 0.78141 |  0:00:38s
epoch 121| loss: 0.45769 | val_0_rmse: 0.69165 | val_1_rmse: 0.7953  |  0:00:38s
epoch 122| loss: 0.46622 | val_0_rmse: 0.71738 | val_1_rmse: 0.89306 |  0:00:38s
epoch 123| loss: 0.47239 | val_0_rmse: 0.67402 | val_1_rmse: 0.78726 |  0:00:39s
epoch 124| loss: 0.4683  | val_0_rmse: 0.66145 | val_1_rmse: 0.79804 |  0:00:39s
epoch 125| loss: 0.45844 | val_0_rmse: 0.66769 | val_1_rmse: 0.7893  |  0:00:39s
epoch 126| loss: 0.45807 | val_0_rmse: 0.6554  | val_1_rmse: 0.78724 |  0:00:40s
epoch 127| loss: 0.44835 | val_0_rmse: 0.66609 | val_1_rmse: 0.77585 |  0:00:40s
epoch 128| loss: 0.45767 | val_0_rmse: 0.66752 | val_1_rmse: 0.83476 |  0:00:40s
epoch 129| loss: 0.45632 | val_0_rmse: 0.67256 | val_1_rmse: 0.78575 |  0:00:40s
epoch 130| loss: 0.46721 | val_0_rmse: 0.66266 | val_1_rmse: 0.80532 |  0:00:41s
epoch 131| loss: 0.4551  | val_0_rmse: 0.65056 | val_1_rmse: 0.7907  |  0:00:41s
epoch 132| loss: 0.45865 | val_0_rmse: 0.64873 | val_1_rmse: 0.79845 |  0:00:41s
epoch 133| loss: 0.44626 | val_0_rmse: 0.65532 | val_1_rmse: 0.81239 |  0:00:42s
epoch 134| loss: 0.44097 | val_0_rmse: 0.65797 | val_1_rmse: 0.78217 |  0:00:42s
epoch 135| loss: 0.44033 | val_0_rmse: 0.6383  | val_1_rmse: 0.79462 |  0:00:42s
epoch 136| loss: 0.44961 | val_0_rmse: 0.66016 | val_1_rmse: 0.78534 |  0:00:43s

Early stopping occured at epoch 136 with best_epoch = 106 and best_val_1_rmse = 0.7707
Best weights from best epoch are automatically used!
ended training at: 00:49:21
Feature importance:
Mean squared error is of 0.03983765333036699
Mean absolute error:0.14430500706677526
MAPE:0.14192392634050163
R2 score:0.3714063859440633
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:49:22
epoch 0  | loss: 2.45401 | val_0_rmse: 0.99705 | val_1_rmse: 0.94947 |  0:00:00s
epoch 1  | loss: 1.28431 | val_0_rmse: 0.99129 | val_1_rmse: 0.93807 |  0:00:01s
epoch 2  | loss: 1.11153 | val_0_rmse: 0.991   | val_1_rmse: 0.93914 |  0:00:02s
epoch 3  | loss: 1.01352 | val_0_rmse: 0.99195 | val_1_rmse: 0.94024 |  0:00:02s
epoch 4  | loss: 0.98671 | val_0_rmse: 0.99104 | val_1_rmse: 0.93979 |  0:00:03s
epoch 5  | loss: 0.98879 | val_0_rmse: 0.99175 | val_1_rmse: 0.9408  |  0:00:03s
epoch 6  | loss: 0.98845 | val_0_rmse: 0.99188 | val_1_rmse: 0.94066 |  0:00:04s
epoch 7  | loss: 0.98677 | val_0_rmse: 0.99247 | val_1_rmse: 0.94069 |  0:00:05s
epoch 8  | loss: 0.9863  | val_0_rmse: 0.99183 | val_1_rmse: 0.94044 |  0:00:05s
epoch 9  | loss: 0.98632 | val_0_rmse: 0.99232 | val_1_rmse: 0.94067 |  0:00:06s
epoch 10 | loss: 0.98688 | val_0_rmse: 0.99118 | val_1_rmse: 0.94036 |  0:00:07s
epoch 11 | loss: 0.98684 | val_0_rmse: 0.99235 | val_1_rmse: 0.94063 |  0:00:07s
epoch 12 | loss: 0.98633 | val_0_rmse: 0.99175 | val_1_rmse: 0.94011 |  0:00:08s
epoch 13 | loss: 0.9845  | val_0_rmse: 0.99113 | val_1_rmse: 0.93955 |  0:00:08s
epoch 14 | loss: 0.98662 | val_0_rmse: 0.99405 | val_1_rmse: 0.94237 |  0:00:09s
epoch 15 | loss: 0.98568 | val_0_rmse: 0.98923 | val_1_rmse: 0.93928 |  0:00:10s
epoch 16 | loss: 0.98407 | val_0_rmse: 0.99325 | val_1_rmse: 0.94149 |  0:00:10s
epoch 17 | loss: 0.98474 | val_0_rmse: 0.9925  | val_1_rmse: 0.94164 |  0:00:11s
epoch 18 | loss: 0.9856  | val_0_rmse: 0.99229 | val_1_rmse: 0.94067 |  0:00:12s
epoch 19 | loss: 0.98417 | val_0_rmse: 0.99214 | val_1_rmse: 0.94045 |  0:00:12s
epoch 20 | loss: 0.98341 | val_0_rmse: 0.99206 | val_1_rmse: 0.93989 |  0:00:13s
epoch 21 | loss: 0.98526 | val_0_rmse: 0.99154 | val_1_rmse: 0.93986 |  0:00:13s
epoch 22 | loss: 0.98444 | val_0_rmse: 0.99212 | val_1_rmse: 0.94033 |  0:00:14s
epoch 23 | loss: 0.98155 | val_0_rmse: 0.9927  | val_1_rmse: 0.94088 |  0:00:15s
epoch 24 | loss: 0.98346 | val_0_rmse: 0.99193 | val_1_rmse: 0.94108 |  0:00:15s
epoch 25 | loss: 0.983   | val_0_rmse: 0.99005 | val_1_rmse: 0.93775 |  0:00:16s
epoch 26 | loss: 0.97999 | val_0_rmse: 0.98794 | val_1_rmse: 0.93487 |  0:00:17s
epoch 27 | loss: 0.97641 | val_0_rmse: 0.98672 | val_1_rmse: 0.93348 |  0:00:17s
epoch 28 | loss: 0.97734 | val_0_rmse: 0.9903  | val_1_rmse: 0.93435 |  0:00:18s
epoch 29 | loss: 0.97755 | val_0_rmse: 0.98572 | val_1_rmse: 0.93586 |  0:00:19s
epoch 30 | loss: 0.97255 | val_0_rmse: 0.98531 | val_1_rmse: 0.93346 |  0:00:19s
epoch 31 | loss: 0.97088 | val_0_rmse: 0.98694 | val_1_rmse: 0.93428 |  0:00:20s
epoch 32 | loss: 0.97062 | val_0_rmse: 0.9852  | val_1_rmse: 0.93818 |  0:00:20s
epoch 33 | loss: 0.97453 | val_0_rmse: 0.98538 | val_1_rmse: 0.93221 |  0:00:21s
epoch 34 | loss: 0.97392 | val_0_rmse: 0.98481 | val_1_rmse: 0.93107 |  0:00:22s
epoch 35 | loss: 0.9765  | val_0_rmse: 0.98523 | val_1_rmse: 0.93266 |  0:00:22s
epoch 36 | loss: 0.97327 | val_0_rmse: 0.98888 | val_1_rmse: 0.93358 |  0:00:23s
epoch 37 | loss: 0.97387 | val_0_rmse: 0.98683 | val_1_rmse: 0.93201 |  0:00:24s
epoch 38 | loss: 0.97417 | val_0_rmse: 0.98901 | val_1_rmse: 0.93507 |  0:00:24s
epoch 39 | loss: 0.97433 | val_0_rmse: 0.98325 | val_1_rmse: 0.93061 |  0:00:25s
epoch 40 | loss: 0.97061 | val_0_rmse: 0.98252 | val_1_rmse: 0.93037 |  0:00:25s
epoch 41 | loss: 0.97176 | val_0_rmse: 0.98241 | val_1_rmse: 0.92802 |  0:00:26s
epoch 42 | loss: 0.9668  | val_0_rmse: 0.98074 | val_1_rmse: 0.9263  |  0:00:27s
epoch 43 | loss: 0.96456 | val_0_rmse: 0.98109 | val_1_rmse: 0.92564 |  0:00:27s
epoch 44 | loss: 0.96604 | val_0_rmse: 0.9821  | val_1_rmse: 0.92969 |  0:00:28s
epoch 45 | loss: 0.96199 | val_0_rmse: 0.97111 | val_1_rmse: 0.91536 |  0:00:29s
epoch 46 | loss: 0.94973 | val_0_rmse: 0.96277 | val_1_rmse: 0.90306 |  0:00:29s
epoch 47 | loss: 0.94291 | val_0_rmse: 0.95956 | val_1_rmse: 0.89921 |  0:00:30s
epoch 48 | loss: 0.93604 | val_0_rmse: 0.958   | val_1_rmse: 0.89491 |  0:00:31s
epoch 49 | loss: 0.92072 | val_0_rmse: 0.95004 | val_1_rmse: 0.8885  |  0:00:31s
epoch 50 | loss: 0.9081  | val_0_rmse: 0.94309 | val_1_rmse: 0.88799 |  0:00:32s
epoch 51 | loss: 0.90535 | val_0_rmse: 0.93955 | val_1_rmse: 0.88644 |  0:00:33s
epoch 52 | loss: 0.89474 | val_0_rmse: 0.94001 | val_1_rmse: 0.8874  |  0:00:33s
epoch 53 | loss: 0.88225 | val_0_rmse: 0.93267 | val_1_rmse: 0.87775 |  0:00:34s
epoch 54 | loss: 0.87667 | val_0_rmse: 0.93629 | val_1_rmse: 0.88709 |  0:00:34s
epoch 55 | loss: 0.87606 | val_0_rmse: 0.92411 | val_1_rmse: 0.87649 |  0:00:35s
epoch 56 | loss: 0.87664 | val_0_rmse: 0.92263 | val_1_rmse: 0.87444 |  0:00:36s
epoch 57 | loss: 0.86976 | val_0_rmse: 0.92157 | val_1_rmse: 0.87109 |  0:00:36s
epoch 58 | loss: 0.86434 | val_0_rmse: 0.9234  | val_1_rmse: 0.87414 |  0:00:37s
epoch 59 | loss: 0.86285 | val_0_rmse: 0.92121 | val_1_rmse: 0.87189 |  0:00:38s
epoch 60 | loss: 0.85699 | val_0_rmse: 0.92246 | val_1_rmse: 0.87499 |  0:00:38s
epoch 61 | loss: 0.8525  | val_0_rmse: 0.91855 | val_1_rmse: 0.8731  |  0:00:39s
epoch 62 | loss: 0.84706 | val_0_rmse: 0.91778 | val_1_rmse: 0.87032 |  0:00:39s
epoch 63 | loss: 0.84911 | val_0_rmse: 0.9192  | val_1_rmse: 0.87694 |  0:00:40s
epoch 64 | loss: 0.84195 | val_0_rmse: 0.9224  | val_1_rmse: 0.87861 |  0:00:41s
epoch 65 | loss: 0.85198 | val_0_rmse: 0.92447 | val_1_rmse: 0.88336 |  0:00:41s
epoch 66 | loss: 0.85389 | val_0_rmse: 0.92029 | val_1_rmse: 0.87825 |  0:00:42s
epoch 67 | loss: 0.85323 | val_0_rmse: 0.91993 | val_1_rmse: 0.87743 |  0:00:43s
epoch 68 | loss: 0.84573 | val_0_rmse: 0.91813 | val_1_rmse: 0.87996 |  0:00:43s
epoch 69 | loss: 0.83879 | val_0_rmse: 0.91926 | val_1_rmse: 0.88013 |  0:00:44s
epoch 70 | loss: 0.83715 | val_0_rmse: 0.91664 | val_1_rmse: 0.8827  |  0:00:44s
epoch 71 | loss: 0.84111 | val_0_rmse: 0.91461 | val_1_rmse: 0.8742  |  0:00:45s
epoch 72 | loss: 0.82952 | val_0_rmse: 0.90821 | val_1_rmse: 0.8734  |  0:00:46s
epoch 73 | loss: 0.82979 | val_0_rmse: 0.90915 | val_1_rmse: 0.87317 |  0:00:46s
epoch 74 | loss: 0.82892 | val_0_rmse: 0.90793 | val_1_rmse: 0.86945 |  0:00:47s
epoch 75 | loss: 0.831   | val_0_rmse: 0.90938 | val_1_rmse: 0.86458 |  0:00:48s
epoch 76 | loss: 0.82078 | val_0_rmse: 0.90851 | val_1_rmse: 0.86549 |  0:00:48s
epoch 77 | loss: 0.82122 | val_0_rmse: 0.90236 | val_1_rmse: 0.86042 |  0:00:49s
epoch 78 | loss: 0.81329 | val_0_rmse: 0.89621 | val_1_rmse: 0.86518 |  0:00:50s
epoch 79 | loss: 0.80207 | val_0_rmse: 0.89203 | val_1_rmse: 0.86483 |  0:00:50s
epoch 80 | loss: 0.79553 | val_0_rmse: 0.88869 | val_1_rmse: 0.8628  |  0:00:51s
epoch 81 | loss: 0.79892 | val_0_rmse: 0.89113 | val_1_rmse: 0.85861 |  0:00:51s
epoch 82 | loss: 0.80056 | val_0_rmse: 0.90576 | val_1_rmse: 0.86943 |  0:00:52s
epoch 83 | loss: 0.79246 | val_0_rmse: 0.89466 | val_1_rmse: 0.85657 |  0:00:53s
epoch 84 | loss: 0.79299 | val_0_rmse: 0.8893  | val_1_rmse: 0.85499 |  0:00:53s
epoch 85 | loss: 0.78669 | val_0_rmse: 0.88285 | val_1_rmse: 0.85276 |  0:00:54s
epoch 86 | loss: 0.7746  | val_0_rmse: 0.89425 | val_1_rmse: 0.85118 |  0:00:55s
epoch 87 | loss: 0.77407 | val_0_rmse: 0.87039 | val_1_rmse: 0.84447 |  0:00:55s
epoch 88 | loss: 0.75508 | val_0_rmse: 0.87233 | val_1_rmse: 0.85457 |  0:00:56s
epoch 89 | loss: 0.74332 | val_0_rmse: 0.85176 | val_1_rmse: 0.83739 |  0:00:57s
epoch 90 | loss: 0.72881 | val_0_rmse: 0.84523 | val_1_rmse: 0.82662 |  0:00:57s
epoch 91 | loss: 0.7001  | val_0_rmse: 0.81356 | val_1_rmse: 0.82838 |  0:00:58s
epoch 92 | loss: 0.70694 | val_0_rmse: 1.10727 | val_1_rmse: 0.94869 |  0:00:58s
epoch 93 | loss: 0.70115 | val_0_rmse: 0.95423 | val_1_rmse: 0.95251 |  0:00:59s
epoch 94 | loss: 0.71484 | val_0_rmse: 1.00244 | val_1_rmse: 1.003   |  0:01:00s
epoch 95 | loss: 0.68371 | val_0_rmse: 0.87519 | val_1_rmse: 0.92911 |  0:01:00s
epoch 96 | loss: 0.63913 | val_0_rmse: 0.82118 | val_1_rmse: 0.88376 |  0:01:01s
epoch 97 | loss: 0.60832 | val_0_rmse: 0.75589 | val_1_rmse: 0.84431 |  0:01:02s
epoch 98 | loss: 0.56691 | val_0_rmse: 0.75574 | val_1_rmse: 0.8433  |  0:01:02s
epoch 99 | loss: 0.5495  | val_0_rmse: 1.04418 | val_1_rmse: 0.90412 |  0:01:03s
epoch 100| loss: 0.52249 | val_0_rmse: 0.73808 | val_1_rmse: 0.84383 |  0:01:03s
epoch 101| loss: 0.51288 | val_0_rmse: 0.77936 | val_1_rmse: 0.88037 |  0:01:04s
epoch 102| loss: 0.50125 | val_0_rmse: 0.94746 | val_1_rmse: 0.89071 |  0:01:05s
epoch 103| loss: 0.49788 | val_0_rmse: 0.86187 | val_1_rmse: 0.82537 |  0:01:05s
epoch 104| loss: 0.50852 | val_0_rmse: 0.82911 | val_1_rmse: 0.78303 |  0:01:06s
epoch 105| loss: 0.48616 | val_0_rmse: 0.78828 | val_1_rmse: 0.81062 |  0:01:07s
epoch 106| loss: 0.48346 | val_0_rmse: 0.81128 | val_1_rmse: 0.81935 |  0:01:07s
epoch 107| loss: 0.48248 | val_0_rmse: 0.82575 | val_1_rmse: 0.81933 |  0:01:08s
epoch 108| loss: 0.49249 | val_0_rmse: 0.77126 | val_1_rmse: 0.79831 |  0:01:09s
epoch 109| loss: 0.47253 | val_0_rmse: 2.25248 | val_1_rmse: 0.76117 |  0:01:09s
epoch 110| loss: 0.55863 | val_0_rmse: 0.85092 | val_1_rmse: 0.82615 |  0:01:10s
epoch 111| loss: 0.51045 | val_0_rmse: 0.78299 | val_1_rmse: 0.80334 |  0:01:10s
epoch 112| loss: 0.4889  | val_0_rmse: 1.1283  | val_1_rmse: 0.98663 |  0:01:11s
epoch 113| loss: 0.51071 | val_0_rmse: 0.80822 | val_1_rmse: 0.87136 |  0:01:12s
epoch 114| loss: 0.52642 | val_0_rmse: 0.72076 | val_1_rmse: 0.83129 |  0:01:12s
epoch 115| loss: 0.5483  | val_0_rmse: 0.80326 | val_1_rmse: 0.87649 |  0:01:13s
epoch 116| loss: 0.56567 | val_0_rmse: 0.71374 | val_1_rmse: 0.81963 |  0:01:14s
epoch 117| loss: 0.58624 | val_0_rmse: 0.75291 | val_1_rmse: 0.86177 |  0:01:14s
epoch 118| loss: 0.53105 | val_0_rmse: 0.75833 | val_1_rmse: 0.85587 |  0:01:15s
epoch 119| loss: 0.5365  | val_0_rmse: 0.71401 | val_1_rmse: 0.82465 |  0:01:15s
epoch 120| loss: 0.50429 | val_0_rmse: 0.68921 | val_1_rmse: 0.80998 |  0:01:16s
epoch 121| loss: 0.48862 | val_0_rmse: 0.69639 | val_1_rmse: 0.82779 |  0:01:17s
epoch 122| loss: 0.49444 | val_0_rmse: 0.686   | val_1_rmse: 0.82244 |  0:01:17s
epoch 123| loss: 0.47657 | val_0_rmse: 0.68844 | val_1_rmse: 0.8185  |  0:01:18s
epoch 124| loss: 0.47467 | val_0_rmse: 0.68153 | val_1_rmse: 0.81153 |  0:01:19s
epoch 125| loss: 0.47603 | val_0_rmse: 0.67177 | val_1_rmse: 0.81434 |  0:01:19s
epoch 126| loss: 0.46663 | val_0_rmse: 0.68119 | val_1_rmse: 0.83812 |  0:01:20s
epoch 127| loss: 0.47034 | val_0_rmse: 0.68884 | val_1_rmse: 0.83952 |  0:01:20s
epoch 128| loss: 0.47057 | val_0_rmse: 0.66521 | val_1_rmse: 0.80723 |  0:01:21s
epoch 129| loss: 0.45836 | val_0_rmse: 0.67511 | val_1_rmse: 0.82108 |  0:01:22s
epoch 130| loss: 0.46207 | val_0_rmse: 0.67185 | val_1_rmse: 0.81074 |  0:01:22s
epoch 131| loss: 0.45112 | val_0_rmse: 0.65517 | val_1_rmse: 0.80436 |  0:01:23s
epoch 132| loss: 0.43756 | val_0_rmse: 0.64564 | val_1_rmse: 0.80526 |  0:01:24s
epoch 133| loss: 0.42826 | val_0_rmse: 0.65373 | val_1_rmse: 0.8094  |  0:01:24s
epoch 134| loss: 0.45048 | val_0_rmse: 0.63952 | val_1_rmse: 0.80515 |  0:01:25s
epoch 135| loss: 0.42692 | val_0_rmse: 0.64379 | val_1_rmse: 0.81673 |  0:01:26s
epoch 136| loss: 0.41666 | val_0_rmse: 0.63161 | val_1_rmse: 0.80514 |  0:01:26s
epoch 137| loss: 0.42195 | val_0_rmse: 0.63228 | val_1_rmse: 0.81884 |  0:01:27s
epoch 138| loss: 0.42272 | val_0_rmse: 0.62703 | val_1_rmse: 0.79636 |  0:01:27s
epoch 139| loss: 0.42622 | val_0_rmse: 0.63012 | val_1_rmse: 0.79307 |  0:01:28s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.76117
Best weights from best epoch are automatically used!
ended training at: 00:50:51
Feature importance:
Mean squared error is of 0.07591343212281554
Mean absolute error:0.1762516771270972
MAPE:0.1789201675622446
R2 score:0.27225474637360425
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:50:51
epoch 0  | loss: 2.06167 | val_0_rmse: 1.03415 | val_1_rmse: 0.90349 |  0:00:00s
epoch 1  | loss: 1.30435 | val_0_rmse: 1.03305 | val_1_rmse: 0.9061  |  0:00:01s
epoch 2  | loss: 1.1115  | val_0_rmse: 1.03359 | val_1_rmse: 0.90473 |  0:00:01s
epoch 3  | loss: 1.07551 | val_0_rmse: 1.03315 | val_1_rmse: 0.90494 |  0:00:02s
epoch 4  | loss: 1.07332 | val_0_rmse: 1.03341 | val_1_rmse: 0.90486 |  0:00:03s
epoch 5  | loss: 1.07406 | val_0_rmse: 1.03306 | val_1_rmse: 0.90493 |  0:00:03s
epoch 6  | loss: 1.07206 | val_0_rmse: 1.03391 | val_1_rmse: 0.90703 |  0:00:04s
epoch 7  | loss: 1.07291 | val_0_rmse: 1.0332  | val_1_rmse: 0.90494 |  0:00:05s
epoch 8  | loss: 1.07017 | val_0_rmse: 1.03237 | val_1_rmse: 0.90468 |  0:00:05s
epoch 9  | loss: 1.07267 | val_0_rmse: 1.03256 | val_1_rmse: 0.90536 |  0:00:06s
epoch 10 | loss: 1.07068 | val_0_rmse: 1.03266 | val_1_rmse: 0.90504 |  0:00:07s
epoch 11 | loss: 1.0696  | val_0_rmse: 1.0325  | val_1_rmse: 0.90484 |  0:00:07s
epoch 12 | loss: 1.06918 | val_0_rmse: 1.03276 | val_1_rmse: 0.90471 |  0:00:08s
epoch 13 | loss: 1.06701 | val_0_rmse: 1.03286 | val_1_rmse: 0.90493 |  0:00:08s
epoch 14 | loss: 1.06757 | val_0_rmse: 1.03234 | val_1_rmse: 0.90443 |  0:00:09s
epoch 15 | loss: 1.06608 | val_0_rmse: 1.03188 | val_1_rmse: 0.90376 |  0:00:10s
epoch 16 | loss: 1.06729 | val_0_rmse: 1.03077 | val_1_rmse: 0.90322 |  0:00:10s
epoch 17 | loss: 1.06631 | val_0_rmse: 1.02996 | val_1_rmse: 0.90285 |  0:00:11s
epoch 18 | loss: 1.06543 | val_0_rmse: 1.02919 | val_1_rmse: 0.90274 |  0:00:12s
epoch 19 | loss: 1.06727 | val_0_rmse: 1.0307  | val_1_rmse: 0.90302 |  0:00:12s
epoch 20 | loss: 1.06541 | val_0_rmse: 1.0304  | val_1_rmse: 0.90261 |  0:00:13s
epoch 21 | loss: 1.06406 | val_0_rmse: 1.02831 | val_1_rmse: 0.89973 |  0:00:13s
epoch 22 | loss: 1.06444 | val_0_rmse: 1.02763 | val_1_rmse: 0.8995  |  0:00:14s
epoch 23 | loss: 1.06209 | val_0_rmse: 1.0264  | val_1_rmse: 0.89794 |  0:00:15s
epoch 24 | loss: 1.05899 | val_0_rmse: 1.01658 | val_1_rmse: 0.888   |  0:00:15s
epoch 25 | loss: 1.05251 | val_0_rmse: 1.01077 | val_1_rmse: 0.88499 |  0:00:16s
epoch 26 | loss: 1.04781 | val_0_rmse: 1.00638 | val_1_rmse: 0.87806 |  0:00:17s
epoch 27 | loss: 1.03583 | val_0_rmse: 0.9931  | val_1_rmse: 0.86458 |  0:00:17s
epoch 28 | loss: 1.01428 | val_0_rmse: 0.99507 | val_1_rmse: 0.86282 |  0:00:18s
epoch 29 | loss: 0.99602 | val_0_rmse: 0.98938 | val_1_rmse: 0.8579  |  0:00:19s
epoch 30 | loss: 0.98053 | val_0_rmse: 0.98486 | val_1_rmse: 0.85126 |  0:00:19s
epoch 31 | loss: 0.96927 | val_0_rmse: 0.98537 | val_1_rmse: 0.8639  |  0:00:20s
epoch 32 | loss: 0.95841 | val_0_rmse: 0.9827  | val_1_rmse: 0.85077 |  0:00:20s
epoch 33 | loss: 0.94807 | val_0_rmse: 0.97359 | val_1_rmse: 0.85025 |  0:00:21s
epoch 34 | loss: 0.93978 | val_0_rmse: 0.97277 | val_1_rmse: 0.84766 |  0:00:22s
epoch 35 | loss: 0.92877 | val_0_rmse: 0.97196 | val_1_rmse: 0.84338 |  0:00:22s
epoch 36 | loss: 0.91895 | val_0_rmse: 0.96694 | val_1_rmse: 0.84528 |  0:00:23s
epoch 37 | loss: 0.89582 | val_0_rmse: 0.94956 | val_1_rmse: 0.84087 |  0:00:24s
epoch 38 | loss: 0.86524 | val_0_rmse: 0.9159  | val_1_rmse: 0.83089 |  0:00:24s
epoch 39 | loss: 0.94429 | val_0_rmse: 0.96993 | val_1_rmse: 0.84501 |  0:00:25s
epoch 40 | loss: 0.92112 | val_0_rmse: 0.97448 | val_1_rmse: 0.84858 |  0:00:26s
epoch 41 | loss: 0.91105 | val_0_rmse: 0.96749 | val_1_rmse: 0.84524 |  0:00:26s
epoch 42 | loss: 0.902   | val_0_rmse: 0.95783 | val_1_rmse: 0.83642 |  0:00:27s
epoch 43 | loss: 0.89374 | val_0_rmse: 0.95593 | val_1_rmse: 0.82791 |  0:00:27s
epoch 44 | loss: 0.87937 | val_0_rmse: 0.94238 | val_1_rmse: 0.81602 |  0:00:28s
epoch 45 | loss: 0.86183 | val_0_rmse: 0.93247 | val_1_rmse: 0.80234 |  0:00:29s
epoch 46 | loss: 0.84877 | val_0_rmse: 0.91636 | val_1_rmse: 0.7972  |  0:00:29s
epoch 47 | loss: 0.81788 | val_0_rmse: 0.91134 | val_1_rmse: 0.78375 |  0:00:30s
epoch 48 | loss: 0.78132 | val_0_rmse: 0.93536 | val_1_rmse: 0.81916 |  0:00:31s
epoch 49 | loss: 0.69168 | val_0_rmse: 0.98387 | val_1_rmse: 0.85866 |  0:00:31s
epoch 50 | loss: 0.68887 | val_0_rmse: 0.88907 | val_1_rmse: 0.82196 |  0:00:32s
epoch 51 | loss: 0.66839 | val_0_rmse: 0.83097 | val_1_rmse: 0.80465 |  0:00:33s
epoch 52 | loss: 0.57858 | val_0_rmse: 0.75621 | val_1_rmse: 0.72057 |  0:00:33s
epoch 53 | loss: 0.59986 | val_0_rmse: 0.78078 | val_1_rmse: 0.79124 |  0:00:34s
epoch 54 | loss: 0.69147 | val_0_rmse: 1.06156 | val_1_rmse: 0.9819  |  0:00:35s
epoch 55 | loss: 0.62925 | val_0_rmse: 0.75668 | val_1_rmse: 0.75521 |  0:00:35s
epoch 56 | loss: 0.67808 | val_0_rmse: 0.89742 | val_1_rmse: 0.83592 |  0:00:36s
epoch 57 | loss: 0.54781 | val_0_rmse: 0.8148  | val_1_rmse: 0.77663 |  0:00:37s
epoch 58 | loss: 0.58115 | val_0_rmse: 0.74727 | val_1_rmse: 0.73957 |  0:00:37s
epoch 59 | loss: 0.66471 | val_0_rmse: 0.82958 | val_1_rmse: 0.83505 |  0:00:38s
epoch 60 | loss: 0.76032 | val_0_rmse: 0.87657 | val_1_rmse: 0.83244 |  0:00:38s
epoch 61 | loss: 0.71207 | val_0_rmse: 0.75399 | val_1_rmse: 0.76248 |  0:00:39s
epoch 62 | loss: 0.5744  | val_0_rmse: 0.79318 | val_1_rmse: 0.78424 |  0:00:40s
epoch 63 | loss: 0.60901 | val_0_rmse: 0.77708 | val_1_rmse: 0.75757 |  0:00:40s
epoch 64 | loss: 0.64847 | val_0_rmse: 0.73565 | val_1_rmse: 0.72716 |  0:00:41s
epoch 65 | loss: 0.53967 | val_0_rmse: 0.70941 | val_1_rmse: 0.70791 |  0:00:42s
epoch 66 | loss: 0.50579 | val_0_rmse: 0.75383 | val_1_rmse: 0.76204 |  0:00:42s
epoch 67 | loss: 0.47397 | val_0_rmse: 0.70411 | val_1_rmse: 0.72907 |  0:00:43s
epoch 68 | loss: 0.51704 | val_0_rmse: 0.77634 | val_1_rmse: 0.78294 |  0:00:43s
epoch 69 | loss: 0.50774 | val_0_rmse: 0.70766 | val_1_rmse: 0.72667 |  0:00:44s
epoch 70 | loss: 0.4789  | val_0_rmse: 0.78589 | val_1_rmse: 0.79061 |  0:00:45s
epoch 71 | loss: 0.52337 | val_0_rmse: 0.69538 | val_1_rmse: 0.71113 |  0:00:45s
epoch 72 | loss: 0.52433 | val_0_rmse: 0.68863 | val_1_rmse: 0.70462 |  0:00:46s
epoch 73 | loss: 0.50072 | val_0_rmse: 0.76948 | val_1_rmse: 0.79441 |  0:00:47s
epoch 74 | loss: 0.50637 | val_0_rmse: 0.6811  | val_1_rmse: 0.72654 |  0:00:47s
epoch 75 | loss: 0.55718 | val_0_rmse: 0.67779 | val_1_rmse: 0.71413 |  0:00:48s
epoch 76 | loss: 0.45784 | val_0_rmse: 0.71692 | val_1_rmse: 0.75324 |  0:00:48s
epoch 77 | loss: 0.45933 | val_0_rmse: 0.75281 | val_1_rmse: 0.81252 |  0:00:49s
epoch 78 | loss: 0.43641 | val_0_rmse: 0.69221 | val_1_rmse: 0.76528 |  0:00:50s
epoch 79 | loss: 0.4401  | val_0_rmse: 0.67689 | val_1_rmse: 0.74255 |  0:00:50s
epoch 80 | loss: 0.44705 | val_0_rmse: 0.67635 | val_1_rmse: 0.73172 |  0:00:51s
epoch 81 | loss: 0.4503  | val_0_rmse: 0.69235 | val_1_rmse: 0.75457 |  0:00:52s
epoch 82 | loss: 0.45197 | val_0_rmse: 0.73638 | val_1_rmse: 0.77478 |  0:00:52s
epoch 83 | loss: 0.4966  | val_0_rmse: 0.67195 | val_1_rmse: 0.73579 |  0:00:53s
epoch 84 | loss: 0.4815  | val_0_rmse: 0.68234 | val_1_rmse: 0.74388 |  0:00:54s
epoch 85 | loss: 0.43964 | val_0_rmse: 0.65744 | val_1_rmse: 0.73526 |  0:00:54s
epoch 86 | loss: 0.46399 | val_0_rmse: 0.6565  | val_1_rmse: 0.74085 |  0:00:55s
epoch 87 | loss: 0.44712 | val_0_rmse: 0.66798 | val_1_rmse: 0.73355 |  0:00:55s
epoch 88 | loss: 0.45885 | val_0_rmse: 0.64227 | val_1_rmse: 0.72675 |  0:00:56s
epoch 89 | loss: 0.42358 | val_0_rmse: 0.66149 | val_1_rmse: 0.72855 |  0:00:57s
epoch 90 | loss: 0.41908 | val_0_rmse: 0.6746  | val_1_rmse: 0.74693 |  0:00:57s
epoch 91 | loss: 0.40393 | val_0_rmse: 0.6325  | val_1_rmse: 0.72335 |  0:00:58s
epoch 92 | loss: 0.41546 | val_0_rmse: 0.63588 | val_1_rmse: 0.72177 |  0:00:59s
epoch 93 | loss: 0.4042  | val_0_rmse: 0.63246 | val_1_rmse: 0.74807 |  0:00:59s
epoch 94 | loss: 0.40336 | val_0_rmse: 0.65793 | val_1_rmse: 0.76238 |  0:01:00s
epoch 95 | loss: 0.40832 | val_0_rmse: 0.62831 | val_1_rmse: 0.7276  |  0:01:00s
epoch 96 | loss: 0.39893 | val_0_rmse: 0.61613 | val_1_rmse: 0.73652 |  0:01:01s
epoch 97 | loss: 0.3994  | val_0_rmse: 0.62305 | val_1_rmse: 0.73805 |  0:01:02s
epoch 98 | loss: 0.38756 | val_0_rmse: 0.61584 | val_1_rmse: 0.73906 |  0:01:02s
epoch 99 | loss: 0.38674 | val_0_rmse: 0.61852 | val_1_rmse: 0.75862 |  0:01:03s
epoch 100| loss: 0.41035 | val_0_rmse: 0.61052 | val_1_rmse: 0.7414  |  0:01:04s
epoch 101| loss: 0.40028 | val_0_rmse: 0.6078  | val_1_rmse: 0.73769 |  0:01:04s
epoch 102| loss: 0.37917 | val_0_rmse: 0.62613 | val_1_rmse: 0.76637 |  0:01:05s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.70462
Best weights from best epoch are automatically used!
ended training at: 00:51:57
Feature importance:
Mean squared error is of 0.046250792518725596
Mean absolute error:0.16205148178494877
MAPE:0.1684218919645704
R2 score:0.4558252173724774
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:51:57
epoch 0  | loss: 2.08034 | val_0_rmse: 0.96262 | val_1_rmse: 1.14794 |  0:00:00s
epoch 1  | loss: 1.19122 | val_0_rmse: 0.96039 | val_1_rmse: 1.14517 |  0:00:01s
epoch 2  | loss: 0.97071 | val_0_rmse: 0.96069 | val_1_rmse: 1.14518 |  0:00:02s
epoch 3  | loss: 0.93341 | val_0_rmse: 0.96182 | val_1_rmse: 1.14712 |  0:00:02s
epoch 4  | loss: 0.9152  | val_0_rmse: 0.95703 | val_1_rmse: 1.14181 |  0:00:03s
epoch 5  | loss: 0.89205 | val_0_rmse: 0.93055 | val_1_rmse: 1.12146 |  0:00:03s
epoch 6  | loss: 0.84955 | val_0_rmse: 0.92098 | val_1_rmse: 1.11171 |  0:00:04s
epoch 7  | loss: 0.83796 | val_0_rmse: 0.92954 | val_1_rmse: 1.11637 |  0:00:05s
epoch 8  | loss: 0.84602 | val_0_rmse: 0.9129  | val_1_rmse: 1.10047 |  0:00:05s
epoch 9  | loss: 0.83656 | val_0_rmse: 0.89548 | val_1_rmse: 1.0878  |  0:00:06s
epoch 10 | loss: 0.82413 | val_0_rmse: 0.88986 | val_1_rmse: 1.08076 |  0:00:07s
epoch 11 | loss: 0.82138 | val_0_rmse: 0.89089 | val_1_rmse: 1.08982 |  0:00:07s
epoch 12 | loss: 0.80963 | val_0_rmse: 0.88016 | val_1_rmse: 1.07354 |  0:00:08s
epoch 13 | loss: 0.81166 | val_0_rmse: 0.89784 | val_1_rmse: 1.09765 |  0:00:08s
epoch 14 | loss: 0.79717 | val_0_rmse: 0.88327 | val_1_rmse: 1.03476 |  0:00:09s
epoch 15 | loss: 0.78784 | val_0_rmse: 0.87643 | val_1_rmse: 1.06546 |  0:00:10s
epoch 16 | loss: 0.80359 | val_0_rmse: 1.01832 | val_1_rmse: 1.13537 |  0:00:10s
epoch 17 | loss: 0.77807 | val_0_rmse: 0.87555 | val_1_rmse: 1.02914 |  0:00:11s
epoch 18 | loss: 0.81294 | val_0_rmse: 0.87823 | val_1_rmse: 1.04455 |  0:00:12s
epoch 19 | loss: 0.79636 | val_0_rmse: 0.87519 | val_1_rmse: 1.03131 |  0:00:12s
epoch 20 | loss: 0.80869 | val_0_rmse: 0.87497 | val_1_rmse: 1.0334  |  0:00:13s
epoch 21 | loss: 0.78964 | val_0_rmse: 0.89279 | val_1_rmse: 1.0165  |  0:00:13s
epoch 22 | loss: 0.78752 | val_0_rmse: 0.91066 | val_1_rmse: 1.03365 |  0:00:14s
epoch 23 | loss: 0.77473 | val_0_rmse: 0.91787 | val_1_rmse: 1.03549 |  0:00:15s
epoch 24 | loss: 0.77187 | val_0_rmse: 0.86592 | val_1_rmse: 1.01082 |  0:00:15s
epoch 25 | loss: 0.78684 | val_0_rmse: 0.86053 | val_1_rmse: 1.01072 |  0:00:16s
epoch 26 | loss: 0.78778 | val_0_rmse: 0.86718 | val_1_rmse: 1.04554 |  0:00:17s
epoch 27 | loss: 0.78447 | val_0_rmse: 0.86141 | val_1_rmse: 1.02617 |  0:00:17s
epoch 28 | loss: 0.77281 | val_0_rmse: 0.87042 | val_1_rmse: 1.01537 |  0:00:18s
epoch 29 | loss: 0.78013 | val_0_rmse: 0.87401 | val_1_rmse: 1.0472  |  0:00:19s
epoch 30 | loss: 0.77611 | val_0_rmse: 0.8644  | val_1_rmse: 1.03773 |  0:00:19s
epoch 31 | loss: 0.76168 | val_0_rmse: 0.89449 | val_1_rmse: 1.01828 |  0:00:20s
epoch 32 | loss: 0.78723 | val_0_rmse: 0.87908 | val_1_rmse: 1.06517 |  0:00:21s
epoch 33 | loss: 0.79434 | val_0_rmse: 0.89286 | val_1_rmse: 1.02648 |  0:00:21s
epoch 34 | loss: 0.78895 | val_0_rmse: 0.91572 | val_1_rmse: 1.04192 |  0:00:22s
epoch 35 | loss: 0.7805  | val_0_rmse: 0.8732  | val_1_rmse: 1.03185 |  0:00:22s
epoch 36 | loss: 0.77886 | val_0_rmse: 0.86471 | val_1_rmse: 1.02908 |  0:00:23s
epoch 37 | loss: 0.77097 | val_0_rmse: 0.86384 | val_1_rmse: 1.01909 |  0:00:24s
epoch 38 | loss: 0.77945 | val_0_rmse: 0.85796 | val_1_rmse: 1.02116 |  0:00:24s
epoch 39 | loss: 0.77496 | val_0_rmse: 0.86616 | val_1_rmse: 1.00994 |  0:00:25s
epoch 40 | loss: 0.7788  | val_0_rmse: 0.86302 | val_1_rmse: 1.03047 |  0:00:26s
epoch 41 | loss: 0.76493 | val_0_rmse: 0.861   | val_1_rmse: 1.02204 |  0:00:26s
epoch 42 | loss: 0.75478 | val_0_rmse: 0.85918 | val_1_rmse: 1.0179  |  0:00:27s
epoch 43 | loss: 0.76483 | val_0_rmse: 0.87025 | val_1_rmse: 1.00509 |  0:00:27s
epoch 44 | loss: 0.75941 | val_0_rmse: 0.86219 | val_1_rmse: 0.99528 |  0:00:28s
epoch 45 | loss: 0.74892 | val_0_rmse: 0.8737  | val_1_rmse: 0.98984 |  0:00:29s
epoch 46 | loss: 0.74871 | val_0_rmse: 0.85117 | val_1_rmse: 0.98442 |  0:00:29s
epoch 47 | loss: 0.75352 | val_0_rmse: 0.84573 | val_1_rmse: 0.99696 |  0:00:30s
epoch 48 | loss: 0.71433 | val_0_rmse: 0.87683 | val_1_rmse: 0.97134 |  0:00:31s
epoch 49 | loss: 0.75196 | val_0_rmse: 0.84832 | val_1_rmse: 0.98105 |  0:00:31s
epoch 50 | loss: 0.71555 | val_0_rmse: 0.84375 | val_1_rmse: 1.00899 |  0:00:32s
epoch 51 | loss: 0.7088  | val_0_rmse: 0.83145 | val_1_rmse: 0.9634  |  0:00:33s
epoch 52 | loss: 0.6879  | val_0_rmse: 0.8364  | val_1_rmse: 1.00561 |  0:00:33s
epoch 53 | loss: 0.70306 | val_0_rmse: 0.82742 | val_1_rmse: 0.99803 |  0:00:34s
epoch 54 | loss: 0.69821 | val_0_rmse: 0.8226  | val_1_rmse: 0.96417 |  0:00:34s
epoch 55 | loss: 0.70936 | val_0_rmse: 0.80586 | val_1_rmse: 0.91821 |  0:00:35s
epoch 56 | loss: 0.70342 | val_0_rmse: 0.80407 | val_1_rmse: 0.92836 |  0:00:36s
epoch 57 | loss: 0.7142  | val_0_rmse: 0.83595 | val_1_rmse: 0.95798 |  0:00:36s
epoch 58 | loss: 0.71649 | val_0_rmse: 0.80111 | val_1_rmse: 0.96284 |  0:00:37s
epoch 59 | loss: 0.68391 | val_0_rmse: 0.79454 | val_1_rmse: 0.94812 |  0:00:38s
epoch 60 | loss: 0.66078 | val_0_rmse: 0.7906  | val_1_rmse: 0.94434 |  0:00:38s
epoch 61 | loss: 0.63055 | val_0_rmse: 0.8041  | val_1_rmse: 0.93343 |  0:00:39s
epoch 62 | loss: 0.64512 | val_0_rmse: 0.83872 | val_1_rmse: 0.99539 |  0:00:39s
epoch 63 | loss: 0.72449 | val_0_rmse: 0.88265 | val_1_rmse: 1.00131 |  0:00:40s
epoch 64 | loss: 0.65131 | val_0_rmse: 0.80996 | val_1_rmse: 0.92812 |  0:00:41s
epoch 65 | loss: 0.72413 | val_0_rmse: 0.86793 | val_1_rmse: 1.06089 |  0:00:41s
epoch 66 | loss: 0.72792 | val_0_rmse: 0.80373 | val_1_rmse: 0.92128 |  0:00:42s
epoch 67 | loss: 0.67348 | val_0_rmse: 0.78619 | val_1_rmse: 0.85957 |  0:00:43s
epoch 68 | loss: 0.61487 | val_0_rmse: 0.77343 | val_1_rmse: 0.84643 |  0:00:43s
epoch 69 | loss: 0.6317  | val_0_rmse: 0.78256 | val_1_rmse: 0.94882 |  0:00:44s
epoch 70 | loss: 0.62683 | val_0_rmse: 0.80464 | val_1_rmse: 0.85694 |  0:00:45s
epoch 71 | loss: 0.60946 | val_0_rmse: 0.76767 | val_1_rmse: 0.91704 |  0:00:45s
epoch 72 | loss: 0.65263 | val_0_rmse: 0.7518  | val_1_rmse: 0.89625 |  0:00:46s
epoch 73 | loss: 0.64751 | val_0_rmse: 0.81373 | val_1_rmse: 0.94011 |  0:00:47s
epoch 74 | loss: 0.67377 | val_0_rmse: 0.79672 | val_1_rmse: 0.94451 |  0:00:47s
epoch 75 | loss: 0.57498 | val_0_rmse: 0.74425 | val_1_rmse: 0.83117 |  0:00:48s
epoch 76 | loss: 0.55023 | val_0_rmse: 0.76888 | val_1_rmse: 0.90771 |  0:00:48s
epoch 77 | loss: 0.54779 | val_0_rmse: 0.73086 | val_1_rmse: 0.82956 |  0:00:49s
epoch 78 | loss: 0.52865 | val_0_rmse: 0.7151  | val_1_rmse: 0.8852  |  0:00:50s
epoch 79 | loss: 0.50939 | val_0_rmse: 0.72344 | val_1_rmse: 0.88403 |  0:00:50s
epoch 80 | loss: 0.50969 | val_0_rmse: 0.71508 | val_1_rmse: 0.87649 |  0:00:51s
epoch 81 | loss: 0.49308 | val_0_rmse: 0.70335 | val_1_rmse: 0.87138 |  0:00:52s
epoch 82 | loss: 0.47913 | val_0_rmse: 0.76786 | val_1_rmse: 0.86777 |  0:00:52s
epoch 83 | loss: 0.4893  | val_0_rmse: 0.70907 | val_1_rmse: 0.85336 |  0:00:53s
epoch 84 | loss: 0.48225 | val_0_rmse: 0.68463 | val_1_rmse: 0.89232 |  0:00:53s
epoch 85 | loss: 0.52612 | val_0_rmse: 0.72446 | val_1_rmse: 0.86566 |  0:00:54s
epoch 86 | loss: 0.5201  | val_0_rmse: 0.75066 | val_1_rmse: 0.9191  |  0:00:55s
epoch 87 | loss: 0.52248 | val_0_rmse: 0.70557 | val_1_rmse: 0.84761 |  0:00:55s
epoch 88 | loss: 0.47621 | val_0_rmse: 0.69973 | val_1_rmse: 0.87409 |  0:00:56s
epoch 89 | loss: 0.46144 | val_0_rmse: 0.67727 | val_1_rmse: 0.90883 |  0:00:57s
epoch 90 | loss: 0.46031 | val_0_rmse: 0.7416  | val_1_rmse: 0.89044 |  0:00:57s
epoch 91 | loss: 0.47208 | val_0_rmse: 0.74977 | val_1_rmse: 0.90376 |  0:00:58s
epoch 92 | loss: 0.45018 | val_0_rmse: 0.70714 | val_1_rmse: 0.87002 |  0:00:58s
epoch 93 | loss: 0.45471 | val_0_rmse: 0.66931 | val_1_rmse: 0.85933 |  0:00:59s
epoch 94 | loss: 0.44743 | val_0_rmse: 0.70257 | val_1_rmse: 0.8601  |  0:01:00s
epoch 95 | loss: 0.4509  | val_0_rmse: 0.70257 | val_1_rmse: 0.87022 |  0:01:00s
epoch 96 | loss: 0.44452 | val_0_rmse: 0.73443 | val_1_rmse: 0.88798 |  0:01:01s
epoch 97 | loss: 0.44752 | val_0_rmse: 0.746   | val_1_rmse: 0.88584 |  0:01:02s
epoch 98 | loss: 0.45125 | val_0_rmse: 0.74423 | val_1_rmse: 0.88748 |  0:01:02s
epoch 99 | loss: 0.43979 | val_0_rmse: 0.78078 | val_1_rmse: 0.87589 |  0:01:03s
epoch 100| loss: 0.4461  | val_0_rmse: 0.76981 | val_1_rmse: 0.87622 |  0:01:04s
epoch 101| loss: 0.434   | val_0_rmse: 0.70134 | val_1_rmse: 0.87343 |  0:01:04s
epoch 102| loss: 0.43377 | val_0_rmse: 0.75373 | val_1_rmse: 0.85902 |  0:01:05s
epoch 103| loss: 0.42694 | val_0_rmse: 0.74397 | val_1_rmse: 0.85634 |  0:01:05s
epoch 104| loss: 0.44221 | val_0_rmse: 0.80777 | val_1_rmse: 0.88086 |  0:01:06s
epoch 105| loss: 0.43713 | val_0_rmse: 0.68704 | val_1_rmse: 0.88455 |  0:01:07s
epoch 106| loss: 0.41957 | val_0_rmse: 0.7859  | val_1_rmse: 0.88132 |  0:01:07s
epoch 107| loss: 0.42049 | val_0_rmse: 0.68104 | val_1_rmse: 0.91393 |  0:01:08s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.82956
Best weights from best epoch are automatically used!
ended training at: 00:53:06
Feature importance:
Mean squared error is of 0.06254635590746817
Mean absolute error:0.1743930190874403
MAPE:0.17930025943618924
R2 score:0.31355293312335475
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:53:06
epoch 0  | loss: 2.05668 | val_0_rmse: 0.96631 | val_1_rmse: 0.95575 |  0:00:00s
epoch 1  | loss: 1.12708 | val_0_rmse: 0.96629 | val_1_rmse: 0.95609 |  0:00:01s
epoch 2  | loss: 0.9917  | val_0_rmse: 0.96626 | val_1_rmse: 0.95616 |  0:00:01s
epoch 3  | loss: 0.94745 | val_0_rmse: 0.96622 | val_1_rmse: 0.95597 |  0:00:02s
epoch 4  | loss: 0.94015 | val_0_rmse: 0.96437 | val_1_rmse: 0.95731 |  0:00:03s
epoch 5  | loss: 0.93382 | val_0_rmse: 0.96238 | val_1_rmse: 0.9547  |  0:00:03s
epoch 6  | loss: 0.93199 | val_0_rmse: 0.96203 | val_1_rmse: 0.95436 |  0:00:04s
epoch 7  | loss: 0.93095 | val_0_rmse: 0.96063 | val_1_rmse: 0.95377 |  0:00:04s
epoch 8  | loss: 0.92549 | val_0_rmse: 0.95948 | val_1_rmse: 0.95604 |  0:00:05s
epoch 9  | loss: 0.9259  | val_0_rmse: 0.95967 | val_1_rmse: 0.95368 |  0:00:06s
epoch 10 | loss: 0.92551 | val_0_rmse: 0.9588  | val_1_rmse: 0.95205 |  0:00:07s
epoch 11 | loss: 0.92012 | val_0_rmse: 0.95292 | val_1_rmse: 0.9491  |  0:00:07s
epoch 12 | loss: 0.91585 | val_0_rmse: 0.94875 | val_1_rmse: 0.94472 |  0:00:08s
epoch 13 | loss: 0.9083  | val_0_rmse: 0.94714 | val_1_rmse: 0.94496 |  0:00:08s
epoch 14 | loss: 0.90383 | val_0_rmse: 0.94385 | val_1_rmse: 0.94127 |  0:00:09s
epoch 15 | loss: 0.90056 | val_0_rmse: 0.94079 | val_1_rmse: 0.93729 |  0:00:10s
epoch 16 | loss: 0.90034 | val_0_rmse: 0.94104 | val_1_rmse: 0.93867 |  0:00:10s
epoch 17 | loss: 0.898   | val_0_rmse: 0.93942 | val_1_rmse: 0.93681 |  0:00:11s
epoch 18 | loss: 0.89343 | val_0_rmse: 0.93823 | val_1_rmse: 0.93687 |  0:00:12s
epoch 19 | loss: 0.88739 | val_0_rmse: 0.93886 | val_1_rmse: 0.93871 |  0:00:12s
epoch 20 | loss: 0.88486 | val_0_rmse: 0.93683 | val_1_rmse: 0.93554 |  0:00:13s
epoch 21 | loss: 0.88403 | val_0_rmse: 0.93752 | val_1_rmse: 0.939   |  0:00:13s
epoch 22 | loss: 0.87492 | val_0_rmse: 0.93829 | val_1_rmse: 0.94007 |  0:00:14s
epoch 23 | loss: 0.87069 | val_0_rmse: 0.93584 | val_1_rmse: 0.93659 |  0:00:15s
epoch 24 | loss: 0.86644 | val_0_rmse: 0.93418 | val_1_rmse: 0.9344  |  0:00:15s
epoch 25 | loss: 0.85611 | val_0_rmse: 0.93204 | val_1_rmse: 0.93314 |  0:00:16s
epoch 26 | loss: 0.85176 | val_0_rmse: 0.9299  | val_1_rmse: 0.93136 |  0:00:17s
epoch 27 | loss: 0.84631 | val_0_rmse: 0.91625 | val_1_rmse: 0.91347 |  0:00:17s
epoch 28 | loss: 0.82954 | val_0_rmse: 0.91306 | val_1_rmse: 0.91515 |  0:00:18s
epoch 29 | loss: 0.81736 | val_0_rmse: 0.90252 | val_1_rmse: 0.90478 |  0:00:19s
epoch 30 | loss: 0.80574 | val_0_rmse: 0.88998 | val_1_rmse: 0.89706 |  0:00:19s
epoch 31 | loss: 0.79001 | val_0_rmse: 0.8724  | val_1_rmse: 0.87349 |  0:00:20s
epoch 32 | loss: 0.76815 | val_0_rmse: 0.86477 | val_1_rmse: 0.86713 |  0:00:20s
epoch 33 | loss: 0.76662 | val_0_rmse: 0.8709  | val_1_rmse: 0.87511 |  0:00:21s
epoch 34 | loss: 0.72944 | val_0_rmse: 0.88563 | val_1_rmse: 0.88559 |  0:00:22s
epoch 35 | loss: 0.69892 | val_0_rmse: 0.85908 | val_1_rmse: 0.87249 |  0:00:22s
epoch 36 | loss: 0.72705 | val_0_rmse: 0.84278 | val_1_rmse: 0.83689 |  0:00:23s
epoch 37 | loss: 0.69335 | val_0_rmse: 0.83475 | val_1_rmse: 0.8469  |  0:00:24s
epoch 38 | loss: 0.66533 | val_0_rmse: 0.86721 | val_1_rmse: 0.86791 |  0:00:24s
epoch 39 | loss: 0.69946 | val_0_rmse: 0.85333 | val_1_rmse: 0.8261  |  0:00:25s
epoch 40 | loss: 0.65179 | val_0_rmse: 0.8296  | val_1_rmse: 0.8176  |  0:00:26s
epoch 41 | loss: 0.70796 | val_0_rmse: 0.83538 | val_1_rmse: 0.82988 |  0:00:26s
epoch 42 | loss: 0.70257 | val_0_rmse: 0.87028 | val_1_rmse: 0.83601 |  0:00:27s
epoch 43 | loss: 0.80133 | val_0_rmse: 0.89292 | val_1_rmse: 0.89219 |  0:00:27s
epoch 44 | loss: 0.73448 | val_0_rmse: 0.90191 | val_1_rmse: 0.90033 |  0:00:28s
epoch 45 | loss: 0.69456 | val_0_rmse: 0.8365  | val_1_rmse: 0.84274 |  0:00:29s
epoch 46 | loss: 0.65458 | val_0_rmse: 0.78974 | val_1_rmse: 0.79729 |  0:00:29s
epoch 47 | loss: 0.635   | val_0_rmse: 0.75129 | val_1_rmse: 0.75333 |  0:00:30s
epoch 48 | loss: 0.59371 | val_0_rmse: 0.80138 | val_1_rmse: 0.84869 |  0:00:31s
epoch 49 | loss: 0.59147 | val_0_rmse: 0.72928 | val_1_rmse: 0.73178 |  0:00:31s
epoch 50 | loss: 0.58632 | val_0_rmse: 0.78541 | val_1_rmse: 0.838   |  0:00:32s
epoch 51 | loss: 0.69337 | val_0_rmse: 0.87748 | val_1_rmse: 0.89126 |  0:00:32s
epoch 52 | loss: 0.70754 | val_0_rmse: 0.85798 | val_1_rmse: 0.8625  |  0:00:33s
epoch 53 | loss: 0.68358 | val_0_rmse: 0.84065 | val_1_rmse: 0.86235 |  0:00:34s
epoch 54 | loss: 0.67046 | val_0_rmse: 0.81594 | val_1_rmse: 0.8411  |  0:00:34s
epoch 55 | loss: 0.64192 | val_0_rmse: 0.80175 | val_1_rmse: 0.82786 |  0:00:35s
epoch 56 | loss: 0.59734 | val_0_rmse: 0.77889 | val_1_rmse: 0.77553 |  0:00:36s
epoch 57 | loss: 0.60775 | val_0_rmse: 0.79597 | val_1_rmse: 0.78179 |  0:00:36s
epoch 58 | loss: 0.59157 | val_0_rmse: 0.76508 | val_1_rmse: 0.73293 |  0:00:37s
epoch 59 | loss: 0.59211 | val_0_rmse: 0.76211 | val_1_rmse: 0.77307 |  0:00:38s
epoch 60 | loss: 0.57808 | val_0_rmse: 0.75624 | val_1_rmse: 0.72927 |  0:00:38s
epoch 61 | loss: 0.53086 | val_0_rmse: 0.79925 | val_1_rmse: 0.72247 |  0:00:39s
epoch 62 | loss: 0.49983 | val_0_rmse: 0.8531  | val_1_rmse: 0.71204 |  0:00:40s
epoch 63 | loss: 0.51871 | val_0_rmse: 0.78633 | val_1_rmse: 0.74517 |  0:00:40s
epoch 64 | loss: 0.48578 | val_0_rmse: 0.81506 | val_1_rmse: 0.83544 |  0:00:41s
epoch 65 | loss: 0.511   | val_0_rmse: 0.73476 | val_1_rmse: 0.71183 |  0:00:41s
epoch 66 | loss: 0.48876 | val_0_rmse: 0.83458 | val_1_rmse: 0.74617 |  0:00:42s
epoch 67 | loss: 0.51492 | val_0_rmse: 0.80602 | val_1_rmse: 0.72656 |  0:00:43s
epoch 68 | loss: 0.47696 | val_0_rmse: 0.73198 | val_1_rmse: 0.75025 |  0:00:43s
epoch 69 | loss: 0.48113 | val_0_rmse: 0.7806  | val_1_rmse: 0.72457 |  0:00:44s
epoch 70 | loss: 0.46127 | val_0_rmse: 0.72546 | val_1_rmse: 0.72909 |  0:00:45s
epoch 71 | loss: 0.51622 | val_0_rmse: 0.6896  | val_1_rmse: 0.74438 |  0:00:45s
epoch 72 | loss: 0.47322 | val_0_rmse: 0.69478 | val_1_rmse: 0.73114 |  0:00:46s
epoch 73 | loss: 0.45785 | val_0_rmse: 0.67938 | val_1_rmse: 0.72411 |  0:00:46s
epoch 74 | loss: 0.46702 | val_0_rmse: 0.72062 | val_1_rmse: 0.7663  |  0:00:47s
epoch 75 | loss: 0.51989 | val_0_rmse: 0.67625 | val_1_rmse: 0.72813 |  0:00:48s
epoch 76 | loss: 0.47823 | val_0_rmse: 0.70127 | val_1_rmse: 0.72753 |  0:00:48s
epoch 77 | loss: 0.4537  | val_0_rmse: 0.69961 | val_1_rmse: 0.71404 |  0:00:49s
epoch 78 | loss: 0.43149 | val_0_rmse: 0.68574 | val_1_rmse: 0.7384  |  0:00:50s
epoch 79 | loss: 0.44434 | val_0_rmse: 0.7184  | val_1_rmse: 0.75826 |  0:00:50s
epoch 80 | loss: 0.42835 | val_0_rmse: 0.68966 | val_1_rmse: 0.72837 |  0:00:51s
epoch 81 | loss: 0.42737 | val_0_rmse: 0.65299 | val_1_rmse: 0.71365 |  0:00:52s
epoch 82 | loss: 0.42979 | val_0_rmse: 0.64521 | val_1_rmse: 0.72212 |  0:00:52s
epoch 83 | loss: 0.42206 | val_0_rmse: 0.67974 | val_1_rmse: 0.7524  |  0:00:53s
epoch 84 | loss: 0.40632 | val_0_rmse: 0.65401 | val_1_rmse: 0.74391 |  0:00:53s
epoch 85 | loss: 0.41203 | val_0_rmse: 0.65743 | val_1_rmse: 0.75643 |  0:00:54s
epoch 86 | loss: 0.41047 | val_0_rmse: 0.65121 | val_1_rmse: 0.72052 |  0:00:55s
epoch 87 | loss: 0.41992 | val_0_rmse: 0.6734  | val_1_rmse: 0.75236 |  0:00:55s
epoch 88 | loss: 0.40542 | val_0_rmse: 0.66821 | val_1_rmse: 0.73585 |  0:00:56s
epoch 89 | loss: 0.39933 | val_0_rmse: 0.62029 | val_1_rmse: 0.72364 |  0:00:57s
epoch 90 | loss: 0.40164 | val_0_rmse: 0.73251 | val_1_rmse: 0.77632 |  0:00:57s
epoch 91 | loss: 0.42275 | val_0_rmse: 0.63688 | val_1_rmse: 0.7416  |  0:00:58s
epoch 92 | loss: 0.4333  | val_0_rmse: 0.63723 | val_1_rmse: 0.72682 |  0:00:58s
epoch 93 | loss: 0.43683 | val_0_rmse: 0.63129 | val_1_rmse: 0.71507 |  0:00:59s
epoch 94 | loss: 0.41727 | val_0_rmse: 0.65846 | val_1_rmse: 0.74452 |  0:01:00s
epoch 95 | loss: 0.40838 | val_0_rmse: 0.66651 | val_1_rmse: 0.75048 |  0:01:00s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.71183
Best weights from best epoch are automatically used!
ended training at: 00:54:07
Feature importance:
Mean squared error is of 0.0770887383525095
Mean absolute error:0.17030931096250745
MAPE:0.16784807140027655
R2 score:0.3404727330298156
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:54:08
epoch 0  | loss: 2.82321 | val_0_rmse: 1.01154 | val_1_rmse: 0.97002 |  0:00:00s
epoch 1  | loss: 1.33539 | val_0_rmse: 1.01488 | val_1_rmse: 0.97283 |  0:00:01s
epoch 2  | loss: 1.10816 | val_0_rmse: 1.01103 | val_1_rmse: 0.96869 |  0:00:01s
epoch 3  | loss: 1.05152 | val_0_rmse: 1.011   | val_1_rmse: 0.96824 |  0:00:02s
epoch 4  | loss: 1.03181 | val_0_rmse: 1.01127 | val_1_rmse: 0.96886 |  0:00:03s
epoch 5  | loss: 1.0272  | val_0_rmse: 1.01147 | val_1_rmse: 0.96922 |  0:00:03s
epoch 6  | loss: 1.02684 | val_0_rmse: 1.01152 | val_1_rmse: 0.96934 |  0:00:04s
epoch 7  | loss: 1.02412 | val_0_rmse: 1.0114  | val_1_rmse: 0.96912 |  0:00:05s
epoch 8  | loss: 1.02471 | val_0_rmse: 1.01101 | val_1_rmse: 0.96867 |  0:00:05s
epoch 9  | loss: 1.02536 | val_0_rmse: 1.01102 | val_1_rmse: 0.96897 |  0:00:06s
epoch 10 | loss: 1.02396 | val_0_rmse: 1.00883 | val_1_rmse: 0.96618 |  0:00:07s
epoch 11 | loss: 1.02497 | val_0_rmse: 1.00772 | val_1_rmse: 0.9656  |  0:00:07s
epoch 12 | loss: 1.02046 | val_0_rmse: 1.00394 | val_1_rmse: 0.96042 |  0:00:08s
epoch 13 | loss: 1.0159  | val_0_rmse: 1.00194 | val_1_rmse: 0.95675 |  0:00:08s
epoch 14 | loss: 1.00591 | val_0_rmse: 0.99911 | val_1_rmse: 0.95372 |  0:00:09s
epoch 15 | loss: 0.99197 | val_0_rmse: 0.9899  | val_1_rmse: 0.94317 |  0:00:10s
epoch 16 | loss: 0.98143 | val_0_rmse: 0.97705 | val_1_rmse: 0.93069 |  0:00:10s
epoch 17 | loss: 0.96913 | val_0_rmse: 0.98707 | val_1_rmse: 0.94098 |  0:00:11s
epoch 18 | loss: 0.95565 | val_0_rmse: 0.98732 | val_1_rmse: 0.94161 |  0:00:12s
epoch 19 | loss: 0.93127 | val_0_rmse: 0.96011 | val_1_rmse: 0.91729 |  0:00:12s
epoch 20 | loss: 0.92113 | val_0_rmse: 0.9491  | val_1_rmse: 0.9103  |  0:00:13s
epoch 21 | loss: 0.89815 | val_0_rmse: 0.93499 | val_1_rmse: 0.90119 |  0:00:13s
epoch 22 | loss: 0.85417 | val_0_rmse: 0.90503 | val_1_rmse: 0.89844 |  0:00:14s
epoch 23 | loss: 0.91887 | val_0_rmse: 0.93324 | val_1_rmse: 0.90021 |  0:00:15s
epoch 24 | loss: 0.87062 | val_0_rmse: 0.87576 | val_1_rmse: 0.87933 |  0:00:15s
epoch 25 | loss: 0.82548 | val_0_rmse: 0.86507 | val_1_rmse: 0.88742 |  0:00:16s
epoch 26 | loss: 0.8189  | val_0_rmse: 0.95087 | val_1_rmse: 0.91111 |  0:00:17s
epoch 27 | loss: 0.85967 | val_0_rmse: 0.87491 | val_1_rmse: 0.88457 |  0:00:17s
epoch 28 | loss: 0.77389 | val_0_rmse: 0.85234 | val_1_rmse: 0.88197 |  0:00:18s
epoch 29 | loss: 0.7463  | val_0_rmse: 0.78615 | val_1_rmse: 0.85647 |  0:00:19s
epoch 30 | loss: 0.67852 | val_0_rmse: 0.77929 | val_1_rmse: 0.83405 |  0:00:19s
epoch 31 | loss: 0.62504 | val_0_rmse: 0.78235 | val_1_rmse: 0.85391 |  0:00:20s
epoch 32 | loss: 0.86937 | val_0_rmse: 0.93044 | val_1_rmse: 0.88305 |  0:00:21s
epoch 33 | loss: 0.83913 | val_0_rmse: 0.91625 | val_1_rmse: 0.86969 |  0:00:21s
epoch 34 | loss: 0.82503 | val_0_rmse: 0.90691 | val_1_rmse: 0.85494 |  0:00:22s
epoch 35 | loss: 0.80011 | val_0_rmse: 0.89526 | val_1_rmse: 0.84558 |  0:00:22s
epoch 36 | loss: 0.78184 | val_0_rmse: 0.88347 | val_1_rmse: 0.84775 |  0:00:23s
epoch 37 | loss: 0.74967 | val_0_rmse: 0.85389 | val_1_rmse: 0.84037 |  0:00:24s
epoch 38 | loss: 0.75075 | val_0_rmse: 0.87396 | val_1_rmse: 0.84748 |  0:00:24s
epoch 39 | loss: 0.7252  | val_0_rmse: 0.84129 | val_1_rmse: 0.8274  |  0:00:25s
epoch 40 | loss: 0.6847  | val_0_rmse: 0.74514 | val_1_rmse: 0.85524 |  0:00:26s
epoch 41 | loss: 0.69092 | val_0_rmse: 0.81885 | val_1_rmse: 0.8561  |  0:00:26s
epoch 42 | loss: 0.65488 | val_0_rmse: 0.74725 | val_1_rmse: 0.8127  |  0:00:27s
epoch 43 | loss: 0.61278 | val_0_rmse: 0.73581 | val_1_rmse: 0.79755 |  0:00:27s
epoch 44 | loss: 0.54323 | val_0_rmse: 0.76928 | val_1_rmse: 0.83152 |  0:00:28s
epoch 45 | loss: 0.58577 | val_0_rmse: 0.73818 | val_1_rmse: 0.81697 |  0:00:29s
epoch 46 | loss: 0.62791 | val_0_rmse: 0.79372 | val_1_rmse: 0.84527 |  0:00:29s
epoch 47 | loss: 0.58097 | val_0_rmse: 0.72319 | val_1_rmse: 0.82535 |  0:00:30s
epoch 48 | loss: 0.55063 | val_0_rmse: 0.77737 | val_1_rmse: 0.81853 |  0:00:31s
epoch 49 | loss: 0.54014 | val_0_rmse: 0.87618 | val_1_rmse: 0.79675 |  0:00:31s
epoch 50 | loss: 0.52667 | val_0_rmse: 0.73153 | val_1_rmse: 0.8125  |  0:00:32s
epoch 51 | loss: 0.51254 | val_0_rmse: 0.71654 | val_1_rmse: 0.81592 |  0:00:32s
epoch 52 | loss: 0.50224 | val_0_rmse: 0.7344  | val_1_rmse: 0.78598 |  0:00:33s
epoch 53 | loss: 0.48251 | val_0_rmse: 0.71719 | val_1_rmse: 0.77116 |  0:00:34s
epoch 54 | loss: 0.48474 | val_0_rmse: 0.7289  | val_1_rmse: 0.80735 |  0:00:34s
epoch 55 | loss: 0.4889  | val_0_rmse: 0.72256 | val_1_rmse: 0.81254 |  0:00:35s
epoch 56 | loss: 0.46623 | val_0_rmse: 0.73136 | val_1_rmse: 0.80414 |  0:00:36s
epoch 57 | loss: 0.45342 | val_0_rmse: 0.72724 | val_1_rmse: 0.80364 |  0:00:36s
epoch 58 | loss: 0.44227 | val_0_rmse: 0.7042  | val_1_rmse: 0.80407 |  0:00:37s
epoch 59 | loss: 0.44954 | val_0_rmse: 0.71498 | val_1_rmse: 0.82699 |  0:00:38s
epoch 60 | loss: 0.46185 | val_0_rmse: 0.6853  | val_1_rmse: 0.79325 |  0:00:38s
epoch 61 | loss: 0.46009 | val_0_rmse: 0.69455 | val_1_rmse: 0.80103 |  0:00:39s
epoch 62 | loss: 0.45683 | val_0_rmse: 0.74582 | val_1_rmse: 0.82759 |  0:00:40s
epoch 63 | loss: 0.48163 | val_0_rmse: 0.68406 | val_1_rmse: 0.80135 |  0:00:40s
epoch 64 | loss: 0.45279 | val_0_rmse: 0.68023 | val_1_rmse: 0.80092 |  0:00:41s
epoch 65 | loss: 0.47797 | val_0_rmse: 0.71018 | val_1_rmse: 0.81403 |  0:00:41s
epoch 66 | loss: 0.46249 | val_0_rmse: 0.68154 | val_1_rmse: 0.77512 |  0:00:42s
epoch 67 | loss: 0.49569 | val_0_rmse: 0.67725 | val_1_rmse: 0.77362 |  0:00:43s
epoch 68 | loss: 0.44244 | val_0_rmse: 0.67967 | val_1_rmse: 0.801   |  0:00:43s
epoch 69 | loss: 0.44742 | val_0_rmse: 0.67521 | val_1_rmse: 0.79845 |  0:00:44s
epoch 70 | loss: 0.43567 | val_0_rmse: 0.67038 | val_1_rmse: 0.7977  |  0:00:45s
epoch 71 | loss: 0.44245 | val_0_rmse: 0.66032 | val_1_rmse: 0.80819 |  0:00:45s
epoch 72 | loss: 0.41777 | val_0_rmse: 0.65953 | val_1_rmse: 0.80176 |  0:00:46s
epoch 73 | loss: 0.41089 | val_0_rmse: 0.68076 | val_1_rmse: 0.81064 |  0:00:46s
epoch 74 | loss: 0.41914 | val_0_rmse: 0.65677 | val_1_rmse: 0.8223  |  0:00:47s
epoch 75 | loss: 0.40129 | val_0_rmse: 0.64838 | val_1_rmse: 0.80887 |  0:00:48s
epoch 76 | loss: 0.39993 | val_0_rmse: 0.63468 | val_1_rmse: 0.80886 |  0:00:48s
epoch 77 | loss: 0.39164 | val_0_rmse: 0.62521 | val_1_rmse: 0.80969 |  0:00:49s
epoch 78 | loss: 0.38773 | val_0_rmse: 0.6241  | val_1_rmse: 0.80367 |  0:00:50s
epoch 79 | loss: 0.40203 | val_0_rmse: 0.63723 | val_1_rmse: 0.81015 |  0:00:50s
epoch 80 | loss: 0.3964  | val_0_rmse: 0.63457 | val_1_rmse: 0.81526 |  0:00:51s
epoch 81 | loss: 0.39701 | val_0_rmse: 0.65237 | val_1_rmse: 0.83955 |  0:00:51s
epoch 82 | loss: 0.38014 | val_0_rmse: 0.63033 | val_1_rmse: 0.82349 |  0:00:52s
epoch 83 | loss: 0.38742 | val_0_rmse: 0.65491 | val_1_rmse: 0.85883 |  0:00:53s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.77116
Best weights from best epoch are automatically used!
ended training at: 00:55:01
Feature importance:
Mean squared error is of 0.05542804170104713
Mean absolute error:0.1632532007909207
MAPE:0.1746089110723444
R2 score:0.37729830454131585
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:02
epoch 0  | loss: 3.25549 | val_0_rmse: 1.02079 | val_1_rmse: 0.92856 |  0:00:00s
epoch 1  | loss: 2.9423  | val_0_rmse: 1.01856 | val_1_rmse: 0.93573 |  0:00:00s
epoch 2  | loss: 1.82405 | val_0_rmse: 1.02634 | val_1_rmse: 0.96118 |  0:00:00s
epoch 3  | loss: 1.71073 | val_0_rmse: 1.03295 | val_1_rmse: 0.96745 |  0:00:00s
epoch 4  | loss: 1.61024 | val_0_rmse: 1.02363 | val_1_rmse: 0.94802 |  0:00:00s
epoch 5  | loss: 1.46953 | val_0_rmse: 1.02066 | val_1_rmse: 0.94093 |  0:00:00s
epoch 6  | loss: 1.36895 | val_0_rmse: 1.01981 | val_1_rmse: 0.93625 |  0:00:00s
epoch 7  | loss: 1.40766 | val_0_rmse: 1.01968 | val_1_rmse: 0.93315 |  0:00:00s
epoch 8  | loss: 1.38148 | val_0_rmse: 1.0201  | val_1_rmse: 0.93093 |  0:00:00s
epoch 9  | loss: 1.24415 | val_0_rmse: 1.01894 | val_1_rmse: 0.93121 |  0:00:00s
epoch 10 | loss: 1.11771 | val_0_rmse: 1.01854 | val_1_rmse: 0.9335  |  0:00:00s
epoch 11 | loss: 1.12941 | val_0_rmse: 1.01792 | val_1_rmse: 0.93352 |  0:00:01s
epoch 12 | loss: 1.14402 | val_0_rmse: 1.0175  | val_1_rmse: 0.93249 |  0:00:01s
epoch 13 | loss: 1.07122 | val_0_rmse: 1.01809 | val_1_rmse: 0.9318  |  0:00:01s
epoch 14 | loss: 1.05563 | val_0_rmse: 1.0187  | val_1_rmse: 0.93141 |  0:00:01s
epoch 15 | loss: 1.07086 | val_0_rmse: 1.0189  | val_1_rmse: 0.93112 |  0:00:01s
epoch 16 | loss: 1.05311 | val_0_rmse: 1.01838 | val_1_rmse: 0.93163 |  0:00:01s
epoch 17 | loss: 1.04951 | val_0_rmse: 1.01776 | val_1_rmse: 0.93265 |  0:00:01s
epoch 18 | loss: 1.03286 | val_0_rmse: 1.01773 | val_1_rmse: 0.93304 |  0:00:01s
epoch 19 | loss: 1.02201 | val_0_rmse: 1.01775 | val_1_rmse: 0.93381 |  0:00:01s
epoch 20 | loss: 1.04456 | val_0_rmse: 1.01763 | val_1_rmse: 0.93369 |  0:00:01s
epoch 21 | loss: 1.04241 | val_0_rmse: 1.01772 | val_1_rmse: 0.93382 |  0:00:01s
epoch 22 | loss: 1.03743 | val_0_rmse: 1.01674 | val_1_rmse: 0.93419 |  0:00:01s
epoch 23 | loss: 1.03228 | val_0_rmse: 1.01478 | val_1_rmse: 0.93469 |  0:00:02s
epoch 24 | loss: 1.03628 | val_0_rmse: 1.01067 | val_1_rmse: 0.93139 |  0:00:02s
epoch 25 | loss: 1.03115 | val_0_rmse: 1.00568 | val_1_rmse: 0.93265 |  0:00:02s
epoch 26 | loss: 1.03228 | val_0_rmse: 0.99974 | val_1_rmse: 0.92387 |  0:00:02s
epoch 27 | loss: 1.0263  | val_0_rmse: 1.00323 | val_1_rmse: 0.93869 |  0:00:02s
epoch 28 | loss: 1.03207 | val_0_rmse: 0.99869 | val_1_rmse: 0.9378  |  0:00:02s
epoch 29 | loss: 1.03249 | val_0_rmse: 0.99172 | val_1_rmse: 0.93057 |  0:00:02s
epoch 30 | loss: 1.03249 | val_0_rmse: 0.99315 | val_1_rmse: 0.93156 |  0:00:02s
epoch 31 | loss: 1.01676 | val_0_rmse: 0.99784 | val_1_rmse: 0.93257 |  0:00:02s
epoch 32 | loss: 1.01183 | val_0_rmse: 1.00529 | val_1_rmse: 0.93734 |  0:00:02s
epoch 33 | loss: 1.01725 | val_0_rmse: 1.01329 | val_1_rmse: 0.94275 |  0:00:02s
epoch 34 | loss: 1.01632 | val_0_rmse: 1.01685 | val_1_rmse: 0.94684 |  0:00:02s
epoch 35 | loss: 1.01158 | val_0_rmse: 1.00848 | val_1_rmse: 0.93577 |  0:00:03s
epoch 36 | loss: 1.00641 | val_0_rmse: 1.00556 | val_1_rmse: 0.92789 |  0:00:03s
epoch 37 | loss: 1.00282 | val_0_rmse: 1.00492 | val_1_rmse: 0.92644 |  0:00:03s
epoch 38 | loss: 0.98844 | val_0_rmse: 1.00697 | val_1_rmse: 0.92944 |  0:00:03s
epoch 39 | loss: 0.97065 | val_0_rmse: 1.00835 | val_1_rmse: 0.93376 |  0:00:03s
epoch 40 | loss: 1.00055 | val_0_rmse: 1.0209  | val_1_rmse: 0.93913 |  0:00:03s
epoch 41 | loss: 0.97367 | val_0_rmse: 1.01616 | val_1_rmse: 0.94354 |  0:00:03s
epoch 42 | loss: 0.95351 | val_0_rmse: 1.017   | val_1_rmse: 0.94666 |  0:00:03s
epoch 43 | loss: 0.98338 | val_0_rmse: 1.01986 | val_1_rmse: 0.95172 |  0:00:03s
epoch 44 | loss: 0.94398 | val_0_rmse: 1.02418 | val_1_rmse: 0.95781 |  0:00:03s
epoch 45 | loss: 0.9395  | val_0_rmse: 1.03404 | val_1_rmse: 0.9725  |  0:00:03s
epoch 46 | loss: 0.94127 | val_0_rmse: 1.04124 | val_1_rmse: 0.98331 |  0:00:03s
epoch 47 | loss: 0.94134 | val_0_rmse: 1.03655 | val_1_rmse: 0.98697 |  0:00:04s
epoch 48 | loss: 0.94789 | val_0_rmse: 1.0189  | val_1_rmse: 0.9701  |  0:00:04s
epoch 49 | loss: 0.94359 | val_0_rmse: 0.99865 | val_1_rmse: 0.94758 |  0:00:04s
epoch 50 | loss: 0.9205  | val_0_rmse: 0.99656 | val_1_rmse: 0.94427 |  0:00:04s
epoch 51 | loss: 0.92131 | val_0_rmse: 0.99934 | val_1_rmse: 0.95265 |  0:00:04s
epoch 52 | loss: 0.91083 | val_0_rmse: 1.00667 | val_1_rmse: 0.9631  |  0:00:04s
epoch 53 | loss: 0.92877 | val_0_rmse: 1.01279 | val_1_rmse: 0.97474 |  0:00:04s
epoch 54 | loss: 0.8959  | val_0_rmse: 1.01389 | val_1_rmse: 0.9749  |  0:00:04s
epoch 55 | loss: 0.88994 | val_0_rmse: 1.0169  | val_1_rmse: 0.98035 |  0:00:04s
epoch 56 | loss: 0.86616 | val_0_rmse: 1.02447 | val_1_rmse: 0.9845  |  0:00:04s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.92387
Best weights from best epoch are automatically used!
ended training at: 00:55:07
Feature importance:
Mean squared error is of 0.08696238469596682
Mean absolute error:0.21408086417564093
MAPE:0.2239905869380372
R2 score:0.01352895427247891
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:07
epoch 0  | loss: 2.96313 | val_0_rmse: 0.96988 | val_1_rmse: 1.08209 |  0:00:00s
epoch 1  | loss: 2.39696 | val_0_rmse: 0.96418 | val_1_rmse: 1.09726 |  0:00:00s
epoch 2  | loss: 2.2032  | val_0_rmse: 0.97305 | val_1_rmse: 1.11631 |  0:00:00s
epoch 3  | loss: 1.51518 | val_0_rmse: 0.96276 | val_1_rmse: 1.09903 |  0:00:00s
epoch 4  | loss: 1.47413 | val_0_rmse: 0.96089 | val_1_rmse: 1.09386 |  0:00:00s
epoch 5  | loss: 1.2809  | val_0_rmse: 0.9608  | val_1_rmse: 1.09553 |  0:00:00s
epoch 6  | loss: 1.35277 | val_0_rmse: 0.96098 | val_1_rmse: 1.09306 |  0:00:00s
epoch 7  | loss: 1.09789 | val_0_rmse: 0.96255 | val_1_rmse: 1.0889  |  0:00:00s
epoch 8  | loss: 1.1235  | val_0_rmse: 0.96513 | val_1_rmse: 1.08738 |  0:00:00s
epoch 9  | loss: 1.05369 | val_0_rmse: 0.9645  | val_1_rmse: 1.08801 |  0:00:00s
epoch 10 | loss: 0.96805 | val_0_rmse: 0.96346 | val_1_rmse: 1.08882 |  0:00:00s
epoch 11 | loss: 0.95112 | val_0_rmse: 0.96227 | val_1_rmse: 1.09048 |  0:00:01s
epoch 12 | loss: 0.99827 | val_0_rmse: 0.96192 | val_1_rmse: 1.09253 |  0:00:01s
epoch 13 | loss: 0.95348 | val_0_rmse: 0.96196 | val_1_rmse: 1.09482 |  0:00:01s
epoch 14 | loss: 0.9399  | val_0_rmse: 0.96196 | val_1_rmse: 1.09525 |  0:00:01s
epoch 15 | loss: 0.96579 | val_0_rmse: 0.96171 | val_1_rmse: 1.09298 |  0:00:01s
epoch 16 | loss: 0.96424 | val_0_rmse: 0.96206 | val_1_rmse: 1.09098 |  0:00:01s
epoch 17 | loss: 0.95905 | val_0_rmse: 0.96195 | val_1_rmse: 1.09128 |  0:00:01s
epoch 18 | loss: 0.94151 | val_0_rmse: 0.9618  | val_1_rmse: 1.09384 |  0:00:01s
epoch 19 | loss: 0.91932 | val_0_rmse: 0.96208 | val_1_rmse: 1.09605 |  0:00:01s
epoch 20 | loss: 0.93262 | val_0_rmse: 0.96213 | val_1_rmse: 1.09615 |  0:00:01s
epoch 21 | loss: 0.93044 | val_0_rmse: 0.96174 | val_1_rmse: 1.09397 |  0:00:01s
epoch 22 | loss: 0.91258 | val_0_rmse: 0.96166 | val_1_rmse: 1.09169 |  0:00:01s
epoch 23 | loss: 0.91582 | val_0_rmse: 0.96181 | val_1_rmse: 1.09023 |  0:00:02s
epoch 24 | loss: 0.9116  | val_0_rmse: 0.9614  | val_1_rmse: 1.08839 |  0:00:02s
epoch 25 | loss: 0.91469 | val_0_rmse: 0.96006 | val_1_rmse: 1.08566 |  0:00:02s
epoch 26 | loss: 0.90294 | val_0_rmse: 0.95748 | val_1_rmse: 1.08205 |  0:00:02s
epoch 27 | loss: 0.90348 | val_0_rmse: 0.95256 | val_1_rmse: 1.07696 |  0:00:02s
epoch 28 | loss: 0.89649 | val_0_rmse: 0.94514 | val_1_rmse: 1.07182 |  0:00:02s
epoch 29 | loss: 0.89265 | val_0_rmse: 0.94014 | val_1_rmse: 1.06521 |  0:00:02s
epoch 30 | loss: 0.8928  | val_0_rmse: 0.94334 | val_1_rmse: 1.06534 |  0:00:02s
epoch 31 | loss: 0.88248 | val_0_rmse: 0.96118 | val_1_rmse: 1.07328 |  0:00:02s
epoch 32 | loss: 0.90101 | val_0_rmse: 0.96069 | val_1_rmse: 1.07384 |  0:00:02s
epoch 33 | loss: 0.87048 | val_0_rmse: 0.96181 | val_1_rmse: 1.07648 |  0:00:02s
epoch 34 | loss: 0.85471 | val_0_rmse: 0.96977 | val_1_rmse: 1.08338 |  0:00:02s
epoch 35 | loss: 0.85336 | val_0_rmse: 0.97744 | val_1_rmse: 1.09073 |  0:00:03s
epoch 36 | loss: 0.84951 | val_0_rmse: 0.97948 | val_1_rmse: 1.09694 |  0:00:03s
epoch 37 | loss: 0.85982 | val_0_rmse: 0.9719  | val_1_rmse: 1.09969 |  0:00:03s
epoch 38 | loss: 0.8628  | val_0_rmse: 0.95608 | val_1_rmse: 1.09299 |  0:00:03s
epoch 39 | loss: 0.83177 | val_0_rmse: 0.94853 | val_1_rmse: 1.08903 |  0:00:03s
epoch 40 | loss: 0.83145 | val_0_rmse: 0.94171 | val_1_rmse: 1.09219 |  0:00:03s
epoch 41 | loss: 0.82405 | val_0_rmse: 0.93332 | val_1_rmse: 1.09204 |  0:00:03s
epoch 42 | loss: 0.82981 | val_0_rmse: 0.92611 | val_1_rmse: 1.08921 |  0:00:03s
epoch 43 | loss: 0.82269 | val_0_rmse: 0.92053 | val_1_rmse: 1.08748 |  0:00:03s
epoch 44 | loss: 0.81496 | val_0_rmse: 0.91771 | val_1_rmse: 1.08675 |  0:00:03s
epoch 45 | loss: 0.79961 | val_0_rmse: 0.9185  | val_1_rmse: 1.09038 |  0:00:03s
epoch 46 | loss: 0.80503 | val_0_rmse: 0.92112 | val_1_rmse: 1.09451 |  0:00:03s
epoch 47 | loss: 0.78533 | val_0_rmse: 0.92015 | val_1_rmse: 1.09667 |  0:00:04s
epoch 48 | loss: 0.78521 | val_0_rmse: 0.91057 | val_1_rmse: 1.08816 |  0:00:04s
epoch 49 | loss: 0.8019  | val_0_rmse: 0.90691 | val_1_rmse: 1.08227 |  0:00:04s
epoch 50 | loss: 0.77375 | val_0_rmse: 0.90661 | val_1_rmse: 1.07847 |  0:00:04s
epoch 51 | loss: 0.77104 | val_0_rmse: 0.90522 | val_1_rmse: 1.07324 |  0:00:04s
epoch 52 | loss: 0.77748 | val_0_rmse: 0.90245 | val_1_rmse: 1.06595 |  0:00:04s
epoch 53 | loss: 0.75208 | val_0_rmse: 0.90222 | val_1_rmse: 1.06019 |  0:00:04s
epoch 54 | loss: 0.75805 | val_0_rmse: 0.90281 | val_1_rmse: 1.05585 |  0:00:04s
epoch 55 | loss: 0.76045 | val_0_rmse: 0.90231 | val_1_rmse: 1.05258 |  0:00:04s
epoch 56 | loss: 0.7578  | val_0_rmse: 0.90279 | val_1_rmse: 1.05223 |  0:00:04s
epoch 57 | loss: 0.74715 | val_0_rmse: 0.90375 | val_1_rmse: 1.05054 |  0:00:04s
epoch 58 | loss: 0.73283 | val_0_rmse: 0.90502 | val_1_rmse: 1.05046 |  0:00:05s
epoch 59 | loss: 0.75086 | val_0_rmse: 0.90691 | val_1_rmse: 1.05142 |  0:00:05s
epoch 60 | loss: 0.75498 | val_0_rmse: 0.90755 | val_1_rmse: 1.05419 |  0:00:05s
epoch 61 | loss: 0.75047 | val_0_rmse: 0.90649 | val_1_rmse: 1.05487 |  0:00:05s
epoch 62 | loss: 0.73538 | val_0_rmse: 0.90662 | val_1_rmse: 1.05467 |  0:00:05s
epoch 63 | loss: 0.72509 | val_0_rmse: 0.906   | val_1_rmse: 1.0558  |  0:00:05s
epoch 64 | loss: 0.7182  | val_0_rmse: 0.9075  | val_1_rmse: 1.06015 |  0:00:05s
epoch 65 | loss: 0.72496 | val_0_rmse: 0.90624 | val_1_rmse: 1.05661 |  0:00:05s
epoch 66 | loss: 0.72341 | val_0_rmse: 0.90615 | val_1_rmse: 1.05704 |  0:00:05s
epoch 67 | loss: 0.71307 | val_0_rmse: 0.90731 | val_1_rmse: 1.06029 |  0:00:05s
epoch 68 | loss: 0.70658 | val_0_rmse: 0.90824 | val_1_rmse: 1.06223 |  0:00:05s
epoch 69 | loss: 0.70033 | val_0_rmse: 0.91074 | val_1_rmse: 1.06501 |  0:00:05s
epoch 70 | loss: 0.6815  | val_0_rmse: 0.90411 | val_1_rmse: 1.05001 |  0:00:06s
epoch 71 | loss: 0.68456 | val_0_rmse: 0.90559 | val_1_rmse: 1.04498 |  0:00:06s
epoch 72 | loss: 0.67101 | val_0_rmse: 0.90917 | val_1_rmse: 1.04839 |  0:00:06s
epoch 73 | loss: 0.66927 | val_0_rmse: 0.91123 | val_1_rmse: 1.05582 |  0:00:06s
epoch 74 | loss: 0.66575 | val_0_rmse: 0.91091 | val_1_rmse: 1.06192 |  0:00:06s
epoch 75 | loss: 0.63616 | val_0_rmse: 0.91443 | val_1_rmse: 1.07108 |  0:00:06s
epoch 76 | loss: 0.63454 | val_0_rmse: 0.92343 | val_1_rmse: 1.08502 |  0:00:06s
epoch 77 | loss: 0.63472 | val_0_rmse: 0.92549 | val_1_rmse: 1.089   |  0:00:06s
epoch 78 | loss: 0.64301 | val_0_rmse: 0.92398 | val_1_rmse: 1.0856  |  0:00:06s
epoch 79 | loss: 0.63216 | val_0_rmse: 0.92424 | val_1_rmse: 1.08134 |  0:00:06s
epoch 80 | loss: 0.64682 | val_0_rmse: 0.93261 | val_1_rmse: 1.08842 |  0:00:06s
epoch 81 | loss: 0.63225 | val_0_rmse: 0.93336 | val_1_rmse: 1.08783 |  0:00:06s
epoch 82 | loss: 0.61012 | val_0_rmse: 0.92972 | val_1_rmse: 1.08396 |  0:00:07s
epoch 83 | loss: 0.63405 | val_0_rmse: 0.92115 | val_1_rmse: 1.06946 |  0:00:07s
epoch 84 | loss: 0.60243 | val_0_rmse: 0.90976 | val_1_rmse: 1.04953 |  0:00:07s
epoch 85 | loss: 0.60711 | val_0_rmse: 0.90274 | val_1_rmse: 1.03597 |  0:00:07s
epoch 86 | loss: 0.60424 | val_0_rmse: 0.89792 | val_1_rmse: 1.03037 |  0:00:07s
epoch 87 | loss: 0.61801 | val_0_rmse: 0.8938  | val_1_rmse: 1.02072 |  0:00:07s
epoch 88 | loss: 0.5989  | val_0_rmse: 0.89332 | val_1_rmse: 1.02438 |  0:00:07s
epoch 89 | loss: 0.61716 | val_0_rmse: 0.89138 | val_1_rmse: 1.03016 |  0:00:07s
epoch 90 | loss: 0.57162 | val_0_rmse: 0.88775 | val_1_rmse: 1.04246 |  0:00:07s
epoch 91 | loss: 0.60744 | val_0_rmse: 0.88806 | val_1_rmse: 1.06399 |  0:00:07s
epoch 92 | loss: 0.59792 | val_0_rmse: 0.88728 | val_1_rmse: 1.06788 |  0:00:07s
epoch 93 | loss: 0.55041 | val_0_rmse: 0.88463 | val_1_rmse: 1.05821 |  0:00:08s
epoch 94 | loss: 0.57252 | val_0_rmse: 0.88945 | val_1_rmse: 1.06781 |  0:00:08s
epoch 95 | loss: 0.56379 | val_0_rmse: 0.88911 | val_1_rmse: 1.05764 |  0:00:08s
epoch 96 | loss: 0.54804 | val_0_rmse: 0.88209 | val_1_rmse: 1.02966 |  0:00:08s
epoch 97 | loss: 0.53499 | val_0_rmse: 0.86415 | val_1_rmse: 0.98845 |  0:00:08s
epoch 98 | loss: 0.56823 | val_0_rmse: 0.86547 | val_1_rmse: 0.98518 |  0:00:08s
epoch 99 | loss: 0.54363 | val_0_rmse: 0.87028 | val_1_rmse: 0.99961 |  0:00:08s
epoch 100| loss: 0.54149 | val_0_rmse: 0.87163 | val_1_rmse: 1.00886 |  0:00:08s
epoch 101| loss: 0.57016 | val_0_rmse: 0.87388 | val_1_rmse: 1.01695 |  0:00:08s
epoch 102| loss: 0.52507 | val_0_rmse: 0.86674 | val_1_rmse: 1.00485 |  0:00:08s
epoch 103| loss: 0.52141 | val_0_rmse: 0.86739 | val_1_rmse: 0.99667 |  0:00:08s
epoch 104| loss: 0.52145 | val_0_rmse: 0.87401 | val_1_rmse: 1.0012  |  0:00:08s
epoch 105| loss: 0.52321 | val_0_rmse: 0.88912 | val_1_rmse: 1.02375 |  0:00:09s
epoch 106| loss: 0.52386 | val_0_rmse: 0.88822 | val_1_rmse: 1.02824 |  0:00:09s
epoch 107| loss: 0.51741 | val_0_rmse: 0.8717  | val_1_rmse: 1.00123 |  0:00:09s
epoch 108| loss: 0.52558 | val_0_rmse: 0.88159 | val_1_rmse: 1.01207 |  0:00:09s
epoch 109| loss: 0.50802 | val_0_rmse: 0.91341 | val_1_rmse: 1.04858 |  0:00:09s
epoch 110| loss: 0.48612 | val_0_rmse: 0.94562 | val_1_rmse: 1.07247 |  0:00:09s
epoch 111| loss: 0.51314 | val_0_rmse: 0.94569 | val_1_rmse: 1.0692  |  0:00:09s
epoch 112| loss: 0.52474 | val_0_rmse: 0.92678 | val_1_rmse: 1.03414 |  0:00:09s
epoch 113| loss: 0.50216 | val_0_rmse: 0.91565 | val_1_rmse: 1.01476 |  0:00:09s
epoch 114| loss: 0.50642 | val_0_rmse: 0.92739 | val_1_rmse: 1.02446 |  0:00:09s
epoch 115| loss: 0.50609 | val_0_rmse: 0.95349 | val_1_rmse: 1.03694 |  0:00:09s
epoch 116| loss: 0.49916 | val_0_rmse: 0.96212 | val_1_rmse: 1.052   |  0:00:09s
epoch 117| loss: 0.47273 | val_0_rmse: 0.95865 | val_1_rmse: 1.04173 |  0:00:10s
epoch 118| loss: 0.49583 | val_0_rmse: 0.93786 | val_1_rmse: 1.01429 |  0:00:10s
epoch 119| loss: 0.47618 | val_0_rmse: 0.9057  | val_1_rmse: 1.00949 |  0:00:10s
epoch 120| loss: 0.48509 | val_0_rmse: 0.88721 | val_1_rmse: 0.99469 |  0:00:10s
epoch 121| loss: 0.45342 | val_0_rmse: 0.88625 | val_1_rmse: 0.9666  |  0:00:10s
epoch 122| loss: 0.44252 | val_0_rmse: 0.93995 | val_1_rmse: 0.97129 |  0:00:10s
epoch 123| loss: 0.45504 | val_0_rmse: 0.91819 | val_1_rmse: 0.97861 |  0:00:10s
epoch 124| loss: 0.43719 | val_0_rmse: 0.87658 | val_1_rmse: 0.99632 |  0:00:10s
epoch 125| loss: 0.46905 | val_0_rmse: 0.85768 | val_1_rmse: 1.00221 |  0:00:10s
epoch 126| loss: 0.45621 | val_0_rmse: 0.86656 | val_1_rmse: 0.98467 |  0:00:10s
epoch 127| loss: 0.44414 | val_0_rmse: 0.85514 | val_1_rmse: 1.00318 |  0:00:10s
epoch 128| loss: 0.42846 | val_0_rmse: 0.85818 | val_1_rmse: 1.02557 |  0:00:10s
epoch 129| loss: 0.46749 | val_0_rmse: 0.85006 | val_1_rmse: 1.01481 |  0:00:11s
epoch 130| loss: 0.45127 | val_0_rmse: 0.83967 | val_1_rmse: 1.0058  |  0:00:11s
epoch 131| loss: 0.43806 | val_0_rmse: 0.84105 | val_1_rmse: 1.0187  |  0:00:11s
epoch 132| loss: 0.41805 | val_0_rmse: 0.85243 | val_1_rmse: 1.02875 |  0:00:11s
epoch 133| loss: 0.44381 | val_0_rmse: 0.8727  | val_1_rmse: 1.00065 |  0:00:11s
epoch 134| loss: 0.40967 | val_0_rmse: 0.91035 | val_1_rmse: 0.98899 |  0:00:11s
epoch 135| loss: 0.42125 | val_0_rmse: 0.88658 | val_1_rmse: 0.98807 |  0:00:11s
epoch 136| loss: 0.39379 | val_0_rmse: 0.87213 | val_1_rmse: 0.98266 |  0:00:11s
epoch 137| loss: 0.4108  | val_0_rmse: 0.87321 | val_1_rmse: 0.96659 |  0:00:11s
epoch 138| loss: 0.40243 | val_0_rmse: 0.89825 | val_1_rmse: 0.96952 |  0:00:11s
epoch 139| loss: 0.39786 | val_0_rmse: 0.93166 | val_1_rmse: 0.99178 |  0:00:11s
epoch 140| loss: 0.39347 | val_0_rmse: 0.91194 | val_1_rmse: 1.00642 |  0:00:11s
epoch 141| loss: 0.38214 | val_0_rmse: 0.88724 | val_1_rmse: 1.00708 |  0:00:12s
epoch 142| loss: 0.40043 | val_0_rmse: 0.86696 | val_1_rmse: 0.99097 |  0:00:12s
epoch 143| loss: 0.41028 | val_0_rmse: 0.86015 | val_1_rmse: 0.99391 |  0:00:12s
epoch 144| loss: 0.41277 | val_0_rmse: 0.86584 | val_1_rmse: 0.99106 |  0:00:12s
epoch 145| loss: 0.38239 | val_0_rmse: 0.87325 | val_1_rmse: 0.9963  |  0:00:12s
epoch 146| loss: 0.41426 | val_0_rmse: 0.854   | val_1_rmse: 0.99415 |  0:00:12s
epoch 147| loss: 0.38438 | val_0_rmse: 0.83842 | val_1_rmse: 0.97641 |  0:00:12s
epoch 148| loss: 0.40023 | val_0_rmse: 0.83328 | val_1_rmse: 0.96501 |  0:00:12s
epoch 149| loss: 0.41418 | val_0_rmse: 0.83339 | val_1_rmse: 0.98305 |  0:00:12s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.96501
Best weights from best epoch are automatically used!
ended training at: 00:55:20
Feature importance:
Mean squared error is of 0.08636362336433029
Mean absolute error:0.20924960091040487
MAPE:0.2156209533757286
R2 score:0.10957755559540117
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:20
epoch 0  | loss: 4.03062 | val_0_rmse: 1.01185 | val_1_rmse: 1.01475 |  0:00:00s
epoch 1  | loss: 1.79864 | val_0_rmse: 0.99357 | val_1_rmse: 0.99601 |  0:00:00s
epoch 2  | loss: 1.90852 | val_0_rmse: 0.98844 | val_1_rmse: 0.99877 |  0:00:00s
epoch 3  | loss: 1.61992 | val_0_rmse: 0.99013 | val_1_rmse: 0.99818 |  0:00:00s
epoch 4  | loss: 1.55292 | val_0_rmse: 0.99122 | val_1_rmse: 0.99623 |  0:00:00s
epoch 5  | loss: 1.31537 | val_0_rmse: 0.99108 | val_1_rmse: 0.99605 |  0:00:00s
epoch 6  | loss: 1.07853 | val_0_rmse: 0.99236 | val_1_rmse: 0.99871 |  0:00:00s
epoch 7  | loss: 1.06639 | val_0_rmse: 0.99273 | val_1_rmse: 0.99851 |  0:00:00s
epoch 8  | loss: 1.11569 | val_0_rmse: 0.99259 | val_1_rmse: 0.99822 |  0:00:00s
epoch 9  | loss: 1.0598  | val_0_rmse: 0.99241 | val_1_rmse: 0.99812 |  0:00:00s
epoch 10 | loss: 1.08905 | val_0_rmse: 0.99216 | val_1_rmse: 0.99906 |  0:00:00s
epoch 11 | loss: 1.05531 | val_0_rmse: 0.99263 | val_1_rmse: 0.99983 |  0:00:01s
epoch 12 | loss: 1.05224 | val_0_rmse: 0.993   | val_1_rmse: 1.00032 |  0:00:01s
epoch 13 | loss: 0.96926 | val_0_rmse: 0.99266 | val_1_rmse: 1.00054 |  0:00:01s
epoch 14 | loss: 1.04546 | val_0_rmse: 0.99215 | val_1_rmse: 0.99987 |  0:00:01s
epoch 15 | loss: 1.01718 | val_0_rmse: 0.99141 | val_1_rmse: 0.99892 |  0:00:01s
epoch 16 | loss: 0.95747 | val_0_rmse: 0.99049 | val_1_rmse: 0.99793 |  0:00:01s
epoch 17 | loss: 0.98423 | val_0_rmse: 0.99024 | val_1_rmse: 0.99748 |  0:00:01s
epoch 18 | loss: 0.99806 | val_0_rmse: 0.99039 | val_1_rmse: 0.99733 |  0:00:01s
epoch 19 | loss: 0.98231 | val_0_rmse: 0.9908  | val_1_rmse: 0.9976  |  0:00:01s
epoch 20 | loss: 0.95431 | val_0_rmse: 0.99077 | val_1_rmse: 0.99744 |  0:00:01s
epoch 21 | loss: 0.93031 | val_0_rmse: 0.98992 | val_1_rmse: 0.99738 |  0:00:01s
epoch 22 | loss: 0.94811 | val_0_rmse: 0.98879 | val_1_rmse: 0.99534 |  0:00:01s
epoch 23 | loss: 0.97656 | val_0_rmse: 0.98828 | val_1_rmse: 0.99419 |  0:00:02s
epoch 24 | loss: 0.93409 | val_0_rmse: 0.98794 | val_1_rmse: 0.99346 |  0:00:02s
epoch 25 | loss: 0.9193  | val_0_rmse: 0.98694 | val_1_rmse: 0.99503 |  0:00:02s
epoch 26 | loss: 0.94082 | val_0_rmse: 0.98607 | val_1_rmse: 0.99758 |  0:00:02s
epoch 27 | loss: 0.90076 | val_0_rmse: 0.98497 | val_1_rmse: 1.00222 |  0:00:02s
epoch 28 | loss: 0.89314 | val_0_rmse: 0.98374 | val_1_rmse: 1.00386 |  0:00:02s
epoch 29 | loss: 0.92333 | val_0_rmse: 0.97897 | val_1_rmse: 0.99435 |  0:00:02s
epoch 30 | loss: 0.92929 | val_0_rmse: 0.97542 | val_1_rmse: 0.98713 |  0:00:02s
epoch 31 | loss: 0.91349 | val_0_rmse: 0.97039 | val_1_rmse: 0.98568 |  0:00:02s
epoch 32 | loss: 0.90913 | val_0_rmse: 0.96984 | val_1_rmse: 0.98521 |  0:00:02s
epoch 33 | loss: 0.94075 | val_0_rmse: 0.97189 | val_1_rmse: 0.98462 |  0:00:03s
epoch 34 | loss: 0.92316 | val_0_rmse: 0.97309 | val_1_rmse: 0.98158 |  0:00:03s
epoch 35 | loss: 0.91751 | val_0_rmse: 0.97227 | val_1_rmse: 0.97554 |  0:00:03s
epoch 36 | loss: 0.91209 | val_0_rmse: 0.96949 | val_1_rmse: 0.97335 |  0:00:03s
epoch 37 | loss: 0.8993  | val_0_rmse: 0.96769 | val_1_rmse: 0.96543 |  0:00:03s
epoch 38 | loss: 0.89897 | val_0_rmse: 0.96547 | val_1_rmse: 0.94576 |  0:00:03s
epoch 39 | loss: 0.92804 | val_0_rmse: 0.96537 | val_1_rmse: 0.9366  |  0:00:03s
epoch 40 | loss: 0.9099  | val_0_rmse: 0.96895 | val_1_rmse: 0.93592 |  0:00:03s
epoch 41 | loss: 0.90186 | val_0_rmse: 0.96714 | val_1_rmse: 0.94725 |  0:00:03s
epoch 42 | loss: 0.95151 | val_0_rmse: 0.96717 | val_1_rmse: 0.95276 |  0:00:03s
epoch 43 | loss: 0.89864 | val_0_rmse: 0.9678  | val_1_rmse: 0.96529 |  0:00:03s
epoch 44 | loss: 0.89276 | val_0_rmse: 0.96846 | val_1_rmse: 0.97113 |  0:00:04s
epoch 45 | loss: 0.8952  | val_0_rmse: 0.9674  | val_1_rmse: 0.96994 |  0:00:04s
epoch 46 | loss: 0.88978 | val_0_rmse: 0.96492 | val_1_rmse: 0.96809 |  0:00:04s
epoch 47 | loss: 0.89315 | val_0_rmse: 0.96338 | val_1_rmse: 0.96972 |  0:00:04s
epoch 48 | loss: 0.88914 | val_0_rmse: 0.96405 | val_1_rmse: 0.97255 |  0:00:04s
epoch 49 | loss: 0.88294 | val_0_rmse: 0.96944 | val_1_rmse: 0.97824 |  0:00:04s
epoch 50 | loss: 0.8786  | val_0_rmse: 0.97359 | val_1_rmse: 0.98023 |  0:00:04s
epoch 51 | loss: 0.87996 | val_0_rmse: 0.9682  | val_1_rmse: 0.97507 |  0:00:04s
epoch 52 | loss: 0.88371 | val_0_rmse: 0.96755 | val_1_rmse: 0.97578 |  0:00:04s
epoch 53 | loss: 0.8714  | val_0_rmse: 0.96442 | val_1_rmse: 0.97449 |  0:00:04s
epoch 54 | loss: 0.87286 | val_0_rmse: 0.96242 | val_1_rmse: 0.97308 |  0:00:04s
epoch 55 | loss: 0.87888 | val_0_rmse: 0.96227 | val_1_rmse: 0.97257 |  0:00:05s
epoch 56 | loss: 0.86119 | val_0_rmse: 0.96235 | val_1_rmse: 0.97391 |  0:00:05s
epoch 57 | loss: 0.88875 | val_0_rmse: 0.96305 | val_1_rmse: 0.97749 |  0:00:05s
epoch 58 | loss: 0.87611 | val_0_rmse: 0.96356 | val_1_rmse: 0.97887 |  0:00:05s
epoch 59 | loss: 0.87756 | val_0_rmse: 0.96579 | val_1_rmse: 0.98019 |  0:00:05s
epoch 60 | loss: 0.88163 | val_0_rmse: 0.9674  | val_1_rmse: 0.98157 |  0:00:05s
epoch 61 | loss: 0.87883 | val_0_rmse: 0.96884 | val_1_rmse: 0.98527 |  0:00:05s
epoch 62 | loss: 0.85279 | val_0_rmse: 0.97089 | val_1_rmse: 0.98977 |  0:00:05s
epoch 63 | loss: 0.85184 | val_0_rmse: 0.97091 | val_1_rmse: 0.99117 |  0:00:05s
epoch 64 | loss: 0.85585 | val_0_rmse: 0.96992 | val_1_rmse: 0.9928  |  0:00:05s
epoch 65 | loss: 0.85047 | val_0_rmse: 0.97325 | val_1_rmse: 0.99972 |  0:00:05s
epoch 66 | loss: 0.84109 | val_0_rmse: 0.97149 | val_1_rmse: 0.99715 |  0:00:05s
epoch 67 | loss: 0.85485 | val_0_rmse: 0.96776 | val_1_rmse: 0.99325 |  0:00:06s
epoch 68 | loss: 0.87382 | val_0_rmse: 0.96628 | val_1_rmse: 0.99008 |  0:00:06s
epoch 69 | loss: 0.84071 | val_0_rmse: 0.96985 | val_1_rmse: 0.99061 |  0:00:06s
epoch 70 | loss: 0.84776 | val_0_rmse: 0.96996 | val_1_rmse: 0.9931  |  0:00:06s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.93592
Best weights from best epoch are automatically used!
ended training at: 00:55:27
Feature importance:
Mean squared error is of 0.0944419768340793
Mean absolute error:0.20952807959528794
MAPE:0.2224759358993124
R2 score:-0.008493192922309989
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:27
epoch 0  | loss: 2.62104 | val_0_rmse: 1.03    | val_1_rmse: 1.11699 |  0:00:00s
epoch 1  | loss: 2.89376 | val_0_rmse: 1.02712 | val_1_rmse: 1.10898 |  0:00:00s
epoch 2  | loss: 2.36496 | val_0_rmse: 1.00789 | val_1_rmse: 1.09139 |  0:00:00s
epoch 3  | loss: 1.7836  | val_0_rmse: 1.0063  | val_1_rmse: 1.09274 |  0:00:00s
epoch 4  | loss: 1.73727 | val_0_rmse: 1.00599 | val_1_rmse: 1.09061 |  0:00:00s
epoch 5  | loss: 1.82364 | val_0_rmse: 1.00554 | val_1_rmse: 1.08932 |  0:00:00s
epoch 6  | loss: 1.38577 | val_0_rmse: 1.00412 | val_1_rmse: 1.08759 |  0:00:00s
epoch 7  | loss: 1.33647 | val_0_rmse: 1.00348 | val_1_rmse: 1.08736 |  0:00:00s
epoch 8  | loss: 1.34933 | val_0_rmse: 1.0031  | val_1_rmse: 1.08824 |  0:00:00s
epoch 9  | loss: 1.11469 | val_0_rmse: 1.0038  | val_1_rmse: 1.08844 |  0:00:00s
epoch 10 | loss: 1.11457 | val_0_rmse: 1.00374 | val_1_rmse: 1.0887  |  0:00:00s
epoch 11 | loss: 1.06754 | val_0_rmse: 1.00238 | val_1_rmse: 1.0877  |  0:00:01s
epoch 12 | loss: 1.10949 | val_0_rmse: 1.00224 | val_1_rmse: 1.08672 |  0:00:01s
epoch 13 | loss: 1.07374 | val_0_rmse: 1.00239 | val_1_rmse: 1.08638 |  0:00:01s
epoch 14 | loss: 1.07287 | val_0_rmse: 1.00287 | val_1_rmse: 1.08658 |  0:00:01s
epoch 15 | loss: 1.034   | val_0_rmse: 1.00396 | val_1_rmse: 1.08679 |  0:00:01s
epoch 16 | loss: 1.03792 | val_0_rmse: 1.00394 | val_1_rmse: 1.08753 |  0:00:01s
epoch 17 | loss: 1.03798 | val_0_rmse: 1.0044  | val_1_rmse: 1.08795 |  0:00:01s
epoch 18 | loss: 1.0593  | val_0_rmse: 1.00512 | val_1_rmse: 1.08949 |  0:00:01s
epoch 19 | loss: 1.01471 | val_0_rmse: 1.00668 | val_1_rmse: 1.09023 |  0:00:01s
epoch 20 | loss: 1.02343 | val_0_rmse: 1.00638 | val_1_rmse: 1.08915 |  0:00:01s
epoch 21 | loss: 1.02236 | val_0_rmse: 1.00546 | val_1_rmse: 1.08797 |  0:00:01s
epoch 22 | loss: 1.00754 | val_0_rmse: 1.00571 | val_1_rmse: 1.08812 |  0:00:01s
epoch 23 | loss: 1.0164  | val_0_rmse: 1.00599 | val_1_rmse: 1.0883  |  0:00:02s
epoch 24 | loss: 1.02331 | val_0_rmse: 1.00523 | val_1_rmse: 1.08846 |  0:00:02s
epoch 25 | loss: 1.00542 | val_0_rmse: 1.00488 | val_1_rmse: 1.08802 |  0:00:02s
epoch 26 | loss: 0.98727 | val_0_rmse: 1.00432 | val_1_rmse: 1.08753 |  0:00:02s
epoch 27 | loss: 0.98762 | val_0_rmse: 1.00409 | val_1_rmse: 1.08737 |  0:00:02s
epoch 28 | loss: 1.00333 | val_0_rmse: 1.00413 | val_1_rmse: 1.08734 |  0:00:02s
epoch 29 | loss: 1.00068 | val_0_rmse: 1.0044  | val_1_rmse: 1.08734 |  0:00:02s
epoch 30 | loss: 0.99712 | val_0_rmse: 1.00452 | val_1_rmse: 1.08752 |  0:00:02s
epoch 31 | loss: 1.01034 | val_0_rmse: 1.00454 | val_1_rmse: 1.0878  |  0:00:02s
epoch 32 | loss: 1.00841 | val_0_rmse: 1.00426 | val_1_rmse: 1.08785 |  0:00:02s
epoch 33 | loss: 1.00032 | val_0_rmse: 1.00375 | val_1_rmse: 1.08758 |  0:00:02s
epoch 34 | loss: 0.98799 | val_0_rmse: 1.00352 | val_1_rmse: 1.08776 |  0:00:03s
epoch 35 | loss: 0.99214 | val_0_rmse: 1.00343 | val_1_rmse: 1.08891 |  0:00:03s
epoch 36 | loss: 0.99502 | val_0_rmse: 1.00434 | val_1_rmse: 1.09198 |  0:00:03s
epoch 37 | loss: 1.00099 | val_0_rmse: 1.00638 | val_1_rmse: 1.09649 |  0:00:03s
epoch 38 | loss: 0.95783 | val_0_rmse: 1.00956 | val_1_rmse: 1.10265 |  0:00:03s
epoch 39 | loss: 1.00031 | val_0_rmse: 1.01008 | val_1_rmse: 1.104   |  0:00:03s
epoch 40 | loss: 0.99403 | val_0_rmse: 1.00831 | val_1_rmse: 1.10198 |  0:00:03s
epoch 41 | loss: 0.96415 | val_0_rmse: 1.00633 | val_1_rmse: 1.10069 |  0:00:03s
epoch 42 | loss: 1.0048  | val_0_rmse: 1.00344 | val_1_rmse: 1.09994 |  0:00:03s
epoch 43 | loss: 0.96091 | val_0_rmse: 1.00283 | val_1_rmse: 1.09809 |  0:00:03s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 1.08638
Best weights from best epoch are automatically used!
ended training at: 00:55:31
Feature importance:
Mean squared error is of 0.07348454219472778
Mean absolute error:0.19305589541724927
MAPE:0.24067387213194358
R2 score:0.005611476096893386
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:31
epoch 0  | loss: 3.8486  | val_0_rmse: 1.05215 | val_1_rmse: 0.89381 |  0:00:00s
epoch 1  | loss: 2.6486  | val_0_rmse: 1.0581  | val_1_rmse: 0.91049 |  0:00:00s
epoch 2  | loss: 2.17357 | val_0_rmse: 1.05743 | val_1_rmse: 0.91623 |  0:00:00s
epoch 3  | loss: 1.64898 | val_0_rmse: 1.04249 | val_1_rmse: 0.89225 |  0:00:00s
epoch 4  | loss: 1.55551 | val_0_rmse: 1.03848 | val_1_rmse: 0.88336 |  0:00:00s
epoch 5  | loss: 1.60951 | val_0_rmse: 1.03458 | val_1_rmse: 0.87149 |  0:00:00s
epoch 6  | loss: 1.44382 | val_0_rmse: 1.03318 | val_1_rmse: 0.86599 |  0:00:00s
epoch 7  | loss: 1.32327 | val_0_rmse: 1.03223 | val_1_rmse: 0.86262 |  0:00:00s
epoch 8  | loss: 1.27572 | val_0_rmse: 1.03231 | val_1_rmse: 0.86278 |  0:00:00s
epoch 9  | loss: 1.15393 | val_0_rmse: 1.03222 | val_1_rmse: 0.86168 |  0:00:00s
epoch 10 | loss: 1.19136 | val_0_rmse: 1.03209 | val_1_rmse: 0.85836 |  0:00:00s
epoch 11 | loss: 1.30759 | val_0_rmse: 1.03159 | val_1_rmse: 0.85861 |  0:00:01s
epoch 12 | loss: 1.16782 | val_0_rmse: 1.03154 | val_1_rmse: 0.86019 |  0:00:01s
epoch 13 | loss: 1.19602 | val_0_rmse: 1.03157 | val_1_rmse: 0.86296 |  0:00:01s
epoch 14 | loss: 1.09929 | val_0_rmse: 1.0319  | val_1_rmse: 0.86469 |  0:00:01s
epoch 15 | loss: 1.12383 | val_0_rmse: 1.03226 | val_1_rmse: 0.86189 |  0:00:01s
epoch 16 | loss: 1.07086 | val_0_rmse: 1.03349 | val_1_rmse: 0.85973 |  0:00:01s
epoch 17 | loss: 1.10206 | val_0_rmse: 1.03288 | val_1_rmse: 0.85824 |  0:00:01s
epoch 18 | loss: 1.06506 | val_0_rmse: 1.03183 | val_1_rmse: 0.85996 |  0:00:01s
epoch 19 | loss: 1.07206 | val_0_rmse: 1.03205 | val_1_rmse: 0.86284 |  0:00:01s
epoch 20 | loss: 1.06551 | val_0_rmse: 1.03222 | val_1_rmse: 0.86317 |  0:00:01s
epoch 21 | loss: 1.07271 | val_0_rmse: 1.03147 | val_1_rmse: 0.86408 |  0:00:01s
epoch 22 | loss: 1.05435 | val_0_rmse: 1.03085 | val_1_rmse: 0.86362 |  0:00:02s
epoch 23 | loss: 1.06658 | val_0_rmse: 1.03112 | val_1_rmse: 0.86035 |  0:00:02s
epoch 24 | loss: 1.05304 | val_0_rmse: 1.03227 | val_1_rmse: 0.85692 |  0:00:02s
epoch 25 | loss: 1.05931 | val_0_rmse: 1.03231 | val_1_rmse: 0.85682 |  0:00:02s
epoch 26 | loss: 1.04744 | val_0_rmse: 1.03174 | val_1_rmse: 0.85663 |  0:00:02s
epoch 27 | loss: 1.06315 | val_0_rmse: 1.02997 | val_1_rmse: 0.85954 |  0:00:02s
epoch 28 | loss: 1.05857 | val_0_rmse: 1.02968 | val_1_rmse: 0.86238 |  0:00:02s
epoch 29 | loss: 1.04497 | val_0_rmse: 1.02987 | val_1_rmse: 0.86118 |  0:00:02s
epoch 30 | loss: 1.05505 | val_0_rmse: 1.0305  | val_1_rmse: 0.85905 |  0:00:02s
epoch 31 | loss: 1.05515 | val_0_rmse: 1.03159 | val_1_rmse: 0.85713 |  0:00:02s
epoch 32 | loss: 1.04961 | val_0_rmse: 1.03144 | val_1_rmse: 0.85818 |  0:00:02s
epoch 33 | loss: 1.04806 | val_0_rmse: 1.03107 | val_1_rmse: 0.85958 |  0:00:02s
epoch 34 | loss: 1.04667 | val_0_rmse: 1.03075 | val_1_rmse: 0.8613  |  0:00:03s
epoch 35 | loss: 1.05144 | val_0_rmse: 1.03049 | val_1_rmse: 0.86232 |  0:00:03s
epoch 36 | loss: 1.05399 | val_0_rmse: 1.0303  | val_1_rmse: 0.86207 |  0:00:03s
epoch 37 | loss: 1.04157 | val_0_rmse: 1.03036 | val_1_rmse: 0.86218 |  0:00:03s
epoch 38 | loss: 1.04014 | val_0_rmse: 1.03046 | val_1_rmse: 0.86351 |  0:00:03s
epoch 39 | loss: 1.04312 | val_0_rmse: 1.03082 | val_1_rmse: 0.86643 |  0:00:03s
epoch 40 | loss: 1.04529 | val_0_rmse: 1.0316  | val_1_rmse: 0.86993 |  0:00:03s
epoch 41 | loss: 1.03343 | val_0_rmse: 1.03209 | val_1_rmse: 0.8723  |  0:00:03s
epoch 42 | loss: 1.04311 | val_0_rmse: 1.03156 | val_1_rmse: 0.87027 |  0:00:03s
epoch 43 | loss: 1.03622 | val_0_rmse: 1.03055 | val_1_rmse: 0.8674  |  0:00:03s
epoch 44 | loss: 1.0362  | val_0_rmse: 1.02997 | val_1_rmse: 0.86581 |  0:00:03s
epoch 45 | loss: 1.03493 | val_0_rmse: 1.02917 | val_1_rmse: 0.86709 |  0:00:03s
epoch 46 | loss: 1.0174  | val_0_rmse: 1.02911 | val_1_rmse: 0.86673 |  0:00:04s
epoch 47 | loss: 1.03483 | val_0_rmse: 1.02889 | val_1_rmse: 0.86817 |  0:00:04s
epoch 48 | loss: 1.02479 | val_0_rmse: 1.02835 | val_1_rmse: 0.86869 |  0:00:04s
epoch 49 | loss: 1.04355 | val_0_rmse: 1.0284  | val_1_rmse: 0.86744 |  0:00:04s
epoch 50 | loss: 1.02016 | val_0_rmse: 1.03106 | val_1_rmse: 0.86809 |  0:00:04s
epoch 51 | loss: 1.00901 | val_0_rmse: 1.03192 | val_1_rmse: 0.87033 |  0:00:04s
epoch 52 | loss: 1.02945 | val_0_rmse: 1.02981 | val_1_rmse: 0.87131 |  0:00:04s
epoch 53 | loss: 1.0216  | val_0_rmse: 1.02871 | val_1_rmse: 0.87377 |  0:00:04s
epoch 54 | loss: 1.01288 | val_0_rmse: 1.02926 | val_1_rmse: 0.87561 |  0:00:04s
epoch 55 | loss: 1.01904 | val_0_rmse: 1.03061 | val_1_rmse: 0.87659 |  0:00:04s
epoch 56 | loss: 1.01495 | val_0_rmse: 1.03351 | val_1_rmse: 0.87228 |  0:00:04s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.85663
Best weights from best epoch are automatically used!
ended training at: 00:55:36
Feature importance:
Mean squared error is of 0.09020424119820938
Mean absolute error:0.19916189625161435
MAPE:0.19982072708209708
R2 score:-0.008355298569296954
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:55:37
epoch 0  | loss: 2.09626 | val_0_rmse: 1.04906 | val_1_rmse: 1.05928 |  0:00:00s
epoch 1  | loss: 1.18856 | val_0_rmse: 0.98857 | val_1_rmse: 1.00155 |  0:00:00s
epoch 2  | loss: 1.05483 | val_0_rmse: 0.98647 | val_1_rmse: 0.99931 |  0:00:00s
epoch 3  | loss: 0.98162 | val_0_rmse: 0.96457 | val_1_rmse: 0.97879 |  0:00:01s
epoch 4  | loss: 0.94274 | val_0_rmse: 0.92735 | val_1_rmse: 0.94632 |  0:00:01s
epoch 5  | loss: 0.8983  | val_0_rmse: 0.91381 | val_1_rmse: 0.93518 |  0:00:01s
epoch 6  | loss: 0.89315 | val_0_rmse: 0.90282 | val_1_rmse: 0.93084 |  0:00:02s
epoch 7  | loss: 0.86209 | val_0_rmse: 0.90517 | val_1_rmse: 0.94338 |  0:00:02s
epoch 8  | loss: 0.86012 | val_0_rmse: 0.90042 | val_1_rmse: 0.92654 |  0:00:02s
epoch 9  | loss: 0.82847 | val_0_rmse: 0.89465 | val_1_rmse: 0.91405 |  0:00:03s
epoch 10 | loss: 0.80992 | val_0_rmse: 0.91368 | val_1_rmse: 0.93997 |  0:00:03s
epoch 11 | loss: 0.79019 | val_0_rmse: 0.92546 | val_1_rmse: 0.9375  |  0:00:03s
epoch 12 | loss: 0.78451 | val_0_rmse: 0.88713 | val_1_rmse: 0.9012  |  0:00:04s
epoch 13 | loss: 0.765   | val_0_rmse: 0.88511 | val_1_rmse: 0.90426 |  0:00:04s
epoch 14 | loss: 0.76086 | val_0_rmse: 0.88467 | val_1_rmse: 0.89905 |  0:00:04s
epoch 15 | loss: 0.76158 | val_0_rmse: 0.87909 | val_1_rmse: 0.88875 |  0:00:04s
epoch 16 | loss: 0.75603 | val_0_rmse: 0.88266 | val_1_rmse: 0.89367 |  0:00:05s
epoch 17 | loss: 0.75957 | val_0_rmse: 0.87796 | val_1_rmse: 0.892   |  0:00:05s
epoch 18 | loss: 0.7561  | val_0_rmse: 0.87453 | val_1_rmse: 0.88999 |  0:00:05s
epoch 19 | loss: 0.75035 | val_0_rmse: 0.86977 | val_1_rmse: 0.88205 |  0:00:06s
epoch 20 | loss: 0.74347 | val_0_rmse: 0.87091 | val_1_rmse: 0.88168 |  0:00:06s
epoch 21 | loss: 0.74713 | val_0_rmse: 0.87412 | val_1_rmse: 0.88373 |  0:00:06s
epoch 22 | loss: 0.75182 | val_0_rmse: 0.87704 | val_1_rmse: 0.89225 |  0:00:06s
epoch 23 | loss: 0.76069 | val_0_rmse: 0.87304 | val_1_rmse: 0.89136 |  0:00:07s
epoch 24 | loss: 0.74875 | val_0_rmse: 0.87428 | val_1_rmse: 0.8873  |  0:00:07s
epoch 25 | loss: 0.75013 | val_0_rmse: 0.87368 | val_1_rmse: 0.88658 |  0:00:07s
epoch 26 | loss: 0.75452 | val_0_rmse: 0.88156 | val_1_rmse: 0.90008 |  0:00:08s
epoch 27 | loss: 0.75636 | val_0_rmse: 0.87503 | val_1_rmse: 0.8932  |  0:00:08s
epoch 28 | loss: 0.74227 | val_0_rmse: 0.86784 | val_1_rmse: 0.88294 |  0:00:08s
epoch 29 | loss: 0.74475 | val_0_rmse: 0.86801 | val_1_rmse: 0.88019 |  0:00:08s
epoch 30 | loss: 0.74481 | val_0_rmse: 0.86675 | val_1_rmse: 0.88053 |  0:00:09s
epoch 31 | loss: 0.73997 | val_0_rmse: 0.86655 | val_1_rmse: 0.88174 |  0:00:09s
epoch 32 | loss: 0.74412 | val_0_rmse: 0.8646  | val_1_rmse: 0.88152 |  0:00:09s
epoch 33 | loss: 0.74349 | val_0_rmse: 0.86402 | val_1_rmse: 0.88256 |  0:00:10s
epoch 34 | loss: 0.74471 | val_0_rmse: 0.86398 | val_1_rmse: 0.88025 |  0:00:10s
epoch 35 | loss: 0.74264 | val_0_rmse: 0.86801 | val_1_rmse: 0.88378 |  0:00:10s
epoch 36 | loss: 0.7381  | val_0_rmse: 0.86572 | val_1_rmse: 0.87911 |  0:00:11s
epoch 37 | loss: 0.74006 | val_0_rmse: 0.86668 | val_1_rmse: 0.88336 |  0:00:11s
epoch 38 | loss: 0.73732 | val_0_rmse: 0.86395 | val_1_rmse: 0.87847 |  0:00:11s
epoch 39 | loss: 0.73837 | val_0_rmse: 0.86501 | val_1_rmse: 0.88051 |  0:00:11s
epoch 40 | loss: 0.73281 | val_0_rmse: 0.86328 | val_1_rmse: 0.88088 |  0:00:12s
epoch 41 | loss: 0.73902 | val_0_rmse: 0.8626  | val_1_rmse: 0.87883 |  0:00:12s
epoch 42 | loss: 0.72419 | val_0_rmse: 0.86426 | val_1_rmse: 0.88187 |  0:00:12s
epoch 43 | loss: 0.73267 | val_0_rmse: 0.86297 | val_1_rmse: 0.88013 |  0:00:13s
epoch 44 | loss: 0.73468 | val_0_rmse: 0.86218 | val_1_rmse: 0.87617 |  0:00:13s
epoch 45 | loss: 0.74361 | val_0_rmse: 0.86486 | val_1_rmse: 0.88597 |  0:00:13s
epoch 46 | loss: 0.73827 | val_0_rmse: 0.86878 | val_1_rmse: 0.89115 |  0:00:13s
epoch 47 | loss: 0.72849 | val_0_rmse: 0.85723 | val_1_rmse: 0.87694 |  0:00:14s
epoch 48 | loss: 0.72123 | val_0_rmse: 0.86169 | val_1_rmse: 0.88737 |  0:00:14s
epoch 49 | loss: 0.72526 | val_0_rmse: 0.85681 | val_1_rmse: 0.8801  |  0:00:14s
epoch 50 | loss: 0.72639 | val_0_rmse: 0.85661 | val_1_rmse: 0.87401 |  0:00:15s
epoch 51 | loss: 0.72352 | val_0_rmse: 0.85996 | val_1_rmse: 0.88215 |  0:00:15s
epoch 52 | loss: 0.71519 | val_0_rmse: 0.85385 | val_1_rmse: 0.8803  |  0:00:15s
epoch 53 | loss: 0.72873 | val_0_rmse: 0.857   | val_1_rmse: 0.87892 |  0:00:16s
epoch 54 | loss: 0.72722 | val_0_rmse: 0.85699 | val_1_rmse: 0.87593 |  0:00:16s
epoch 55 | loss: 0.71653 | val_0_rmse: 0.85661 | val_1_rmse: 0.8738  |  0:00:16s
epoch 56 | loss: 0.71802 | val_0_rmse: 0.85784 | val_1_rmse: 0.87457 |  0:00:16s
epoch 57 | loss: 0.71498 | val_0_rmse: 0.85356 | val_1_rmse: 0.86905 |  0:00:17s
epoch 58 | loss: 0.71164 | val_0_rmse: 0.84636 | val_1_rmse: 0.86377 |  0:00:17s
epoch 59 | loss: 0.70756 | val_0_rmse: 0.84577 | val_1_rmse: 0.86413 |  0:00:17s
epoch 60 | loss: 0.70709 | val_0_rmse: 0.83927 | val_1_rmse: 0.8602  |  0:00:18s
epoch 61 | loss: 0.70449 | val_0_rmse: 0.83733 | val_1_rmse: 0.86201 |  0:00:18s
epoch 62 | loss: 0.70007 | val_0_rmse: 0.85143 | val_1_rmse: 0.87551 |  0:00:18s
epoch 63 | loss: 0.70189 | val_0_rmse: 0.8357  | val_1_rmse: 0.86246 |  0:00:18s
epoch 64 | loss: 0.69789 | val_0_rmse: 0.83621 | val_1_rmse: 0.85965 |  0:00:19s
epoch 65 | loss: 0.68987 | val_0_rmse: 0.82568 | val_1_rmse: 0.84995 |  0:00:19s
epoch 66 | loss: 0.6823  | val_0_rmse: 0.82322 | val_1_rmse: 0.85103 |  0:00:19s
epoch 67 | loss: 0.6799  | val_0_rmse: 0.82596 | val_1_rmse: 0.8534  |  0:00:20s
epoch 68 | loss: 0.68413 | val_0_rmse: 0.82285 | val_1_rmse: 0.85088 |  0:00:20s
epoch 69 | loss: 0.68222 | val_0_rmse: 0.82309 | val_1_rmse: 0.85811 |  0:00:20s
epoch 70 | loss: 0.67477 | val_0_rmse: 0.81731 | val_1_rmse: 0.85416 |  0:00:20s
epoch 71 | loss: 0.67783 | val_0_rmse: 0.81679 | val_1_rmse: 0.85092 |  0:00:21s
epoch 72 | loss: 0.67278 | val_0_rmse: 0.81948 | val_1_rmse: 0.84789 |  0:00:21s
epoch 73 | loss: 0.66644 | val_0_rmse: 0.82078 | val_1_rmse: 0.85801 |  0:00:21s
epoch 74 | loss: 0.66568 | val_0_rmse: 0.8179  | val_1_rmse: 0.857   |  0:00:22s
epoch 75 | loss: 0.6714  | val_0_rmse: 0.82027 | val_1_rmse: 0.8624  |  0:00:22s
epoch 76 | loss: 0.66072 | val_0_rmse: 0.82151 | val_1_rmse: 0.86843 |  0:00:22s
epoch 77 | loss: 0.66486 | val_0_rmse: 0.82955 | val_1_rmse: 0.87184 |  0:00:23s
epoch 78 | loss: 0.6729  | val_0_rmse: 0.82612 | val_1_rmse: 0.8623  |  0:00:23s
epoch 79 | loss: 0.66779 | val_0_rmse: 0.81895 | val_1_rmse: 0.85785 |  0:00:23s
epoch 80 | loss: 0.65794 | val_0_rmse: 0.80602 | val_1_rmse: 0.86063 |  0:00:24s
epoch 81 | loss: 0.65242 | val_0_rmse: 0.80979 | val_1_rmse: 0.85956 |  0:00:24s
epoch 82 | loss: 0.64584 | val_0_rmse: 0.80767 | val_1_rmse: 0.85766 |  0:00:24s
epoch 83 | loss: 0.65418 | val_0_rmse: 0.81129 | val_1_rmse: 0.85689 |  0:00:24s
epoch 84 | loss: 0.64189 | val_0_rmse: 0.81034 | val_1_rmse: 0.85106 |  0:00:25s
epoch 85 | loss: 0.64915 | val_0_rmse: 0.81016 | val_1_rmse: 0.8583  |  0:00:25s
epoch 86 | loss: 0.64441 | val_0_rmse: 0.80761 | val_1_rmse: 0.86828 |  0:00:25s
epoch 87 | loss: 0.66011 | val_0_rmse: 0.80119 | val_1_rmse: 0.85971 |  0:00:26s
epoch 88 | loss: 0.65291 | val_0_rmse: 0.80785 | val_1_rmse: 0.85825 |  0:00:26s
epoch 89 | loss: 0.66777 | val_0_rmse: 0.81001 | val_1_rmse: 0.85803 |  0:00:26s
epoch 90 | loss: 0.65345 | val_0_rmse: 0.81164 | val_1_rmse: 0.85876 |  0:00:26s
epoch 91 | loss: 0.64859 | val_0_rmse: 0.82472 | val_1_rmse: 0.8575  |  0:00:27s
epoch 92 | loss: 0.68148 | val_0_rmse: 0.8265  | val_1_rmse: 0.85697 |  0:00:27s
epoch 93 | loss: 0.66812 | val_0_rmse: 0.83209 | val_1_rmse: 0.85758 |  0:00:27s
epoch 94 | loss: 0.66264 | val_0_rmse: 0.83144 | val_1_rmse: 0.86182 |  0:00:28s
epoch 95 | loss: 0.65426 | val_0_rmse: 0.81726 | val_1_rmse: 0.85649 |  0:00:28s
epoch 96 | loss: 0.6517  | val_0_rmse: 0.81296 | val_1_rmse: 0.84688 |  0:00:28s
epoch 97 | loss: 0.64799 | val_0_rmse: 0.81412 | val_1_rmse: 0.85866 |  0:00:29s
epoch 98 | loss: 0.64624 | val_0_rmse: 0.83221 | val_1_rmse: 0.87775 |  0:00:29s
epoch 99 | loss: 0.64119 | val_0_rmse: 0.80305 | val_1_rmse: 0.85118 |  0:00:29s
epoch 100| loss: 0.63141 | val_0_rmse: 0.79471 | val_1_rmse: 0.84377 |  0:00:29s
epoch 101| loss: 0.62849 | val_0_rmse: 0.78964 | val_1_rmse: 0.83795 |  0:00:30s
epoch 102| loss: 0.62747 | val_0_rmse: 0.78967 | val_1_rmse: 0.83914 |  0:00:30s
epoch 103| loss: 0.62569 | val_0_rmse: 0.78409 | val_1_rmse: 0.84053 |  0:00:30s
epoch 104| loss: 0.61172 | val_0_rmse: 0.77668 | val_1_rmse: 0.83981 |  0:00:31s
epoch 105| loss: 0.62525 | val_0_rmse: 0.77962 | val_1_rmse: 0.84519 |  0:00:31s
epoch 106| loss: 0.61657 | val_0_rmse: 0.78551 | val_1_rmse: 0.84861 |  0:00:31s
epoch 107| loss: 0.60676 | val_0_rmse: 0.78918 | val_1_rmse: 0.84948 |  0:00:31s
epoch 108| loss: 0.61842 | val_0_rmse: 0.79782 | val_1_rmse: 0.8416  |  0:00:32s
epoch 109| loss: 0.61463 | val_0_rmse: 0.77284 | val_1_rmse: 0.83151 |  0:00:32s
epoch 110| loss: 0.60152 | val_0_rmse: 0.77867 | val_1_rmse: 0.83361 |  0:00:32s
epoch 111| loss: 0.60268 | val_0_rmse: 0.77442 | val_1_rmse: 0.82857 |  0:00:33s
epoch 112| loss: 0.59927 | val_0_rmse: 0.76783 | val_1_rmse: 0.82511 |  0:00:33s
epoch 113| loss: 0.59736 | val_0_rmse: 0.76706 | val_1_rmse: 0.8317  |  0:00:33s
epoch 114| loss: 0.60425 | val_0_rmse: 0.77056 | val_1_rmse: 0.82887 |  0:00:33s
epoch 115| loss: 0.59863 | val_0_rmse: 0.7704  | val_1_rmse: 0.82425 |  0:00:34s
epoch 116| loss: 0.61411 | val_0_rmse: 0.76972 | val_1_rmse: 0.82694 |  0:00:34s
epoch 117| loss: 0.60217 | val_0_rmse: 0.77722 | val_1_rmse: 0.8389  |  0:00:34s
epoch 118| loss: 0.61132 | val_0_rmse: 0.77418 | val_1_rmse: 0.84962 |  0:00:35s
epoch 119| loss: 0.62211 | val_0_rmse: 0.77727 | val_1_rmse: 0.84079 |  0:00:35s
epoch 120| loss: 0.61345 | val_0_rmse: 0.79014 | val_1_rmse: 0.84218 |  0:00:35s
epoch 121| loss: 0.59847 | val_0_rmse: 0.77059 | val_1_rmse: 0.83541 |  0:00:36s
epoch 122| loss: 0.61239 | val_0_rmse: 0.77458 | val_1_rmse: 0.83735 |  0:00:36s
epoch 123| loss: 0.61779 | val_0_rmse: 0.78285 | val_1_rmse: 0.82866 |  0:00:36s
epoch 124| loss: 0.61506 | val_0_rmse: 0.77309 | val_1_rmse: 0.82235 |  0:00:36s
epoch 125| loss: 0.6063  | val_0_rmse: 0.82144 | val_1_rmse: 0.8816  |  0:00:37s
epoch 126| loss: 0.62338 | val_0_rmse: 0.79459 | val_1_rmse: 0.86025 |  0:00:37s
epoch 127| loss: 0.60805 | val_0_rmse: 0.76801 | val_1_rmse: 0.84066 |  0:00:37s
epoch 128| loss: 0.6029  | val_0_rmse: 0.77795 | val_1_rmse: 0.84697 |  0:00:38s
epoch 129| loss: 0.60202 | val_0_rmse: 0.77732 | val_1_rmse: 0.84129 |  0:00:38s
epoch 130| loss: 0.5969  | val_0_rmse: 0.76829 | val_1_rmse: 0.83124 |  0:00:38s
epoch 131| loss: 0.60199 | val_0_rmse: 0.79609 | val_1_rmse: 0.84119 |  0:00:38s
epoch 132| loss: 0.59787 | val_0_rmse: 0.77216 | val_1_rmse: 0.82373 |  0:00:39s
epoch 133| loss: 0.58762 | val_0_rmse: 0.76868 | val_1_rmse: 0.82419 |  0:00:39s
epoch 134| loss: 0.58361 | val_0_rmse: 0.77087 | val_1_rmse: 0.82861 |  0:00:39s
epoch 135| loss: 0.58809 | val_0_rmse: 0.76123 | val_1_rmse: 0.8223  |  0:00:40s
epoch 136| loss: 0.58895 | val_0_rmse: 0.78105 | val_1_rmse: 0.84923 |  0:00:40s
epoch 137| loss: 0.5765  | val_0_rmse: 0.76053 | val_1_rmse: 0.8319  |  0:00:40s
epoch 138| loss: 0.57577 | val_0_rmse: 0.75212 | val_1_rmse: 0.82879 |  0:00:41s
epoch 139| loss: 0.58563 | val_0_rmse: 0.7615  | val_1_rmse: 0.84468 |  0:00:41s
epoch 140| loss: 0.58505 | val_0_rmse: 0.75494 | val_1_rmse: 0.82929 |  0:00:41s
epoch 141| loss: 0.59848 | val_0_rmse: 0.79324 | val_1_rmse: 0.85988 |  0:00:42s
epoch 142| loss: 0.61815 | val_0_rmse: 0.78736 | val_1_rmse: 0.85029 |  0:00:42s
epoch 143| loss: 0.60351 | val_0_rmse: 0.78575 | val_1_rmse: 0.85457 |  0:00:42s
epoch 144| loss: 0.61556 | val_0_rmse: 0.77526 | val_1_rmse: 0.84475 |  0:00:42s
epoch 145| loss: 0.5891  | val_0_rmse: 0.76235 | val_1_rmse: 0.8373  |  0:00:43s
epoch 146| loss: 0.60197 | val_0_rmse: 0.75776 | val_1_rmse: 0.8387  |  0:00:43s
epoch 147| loss: 0.58957 | val_0_rmse: 0.76566 | val_1_rmse: 0.83923 |  0:00:43s
epoch 148| loss: 0.59435 | val_0_rmse: 0.76598 | val_1_rmse: 0.82688 |  0:00:44s
epoch 149| loss: 0.59005 | val_0_rmse: 0.75546 | val_1_rmse: 0.83245 |  0:00:44s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.8223
Best weights from best epoch are automatically used!
ended training at: 00:56:21
Feature importance:
Mean squared error is of 0.05511648111539919
Mean absolute error:0.17178313181627422
MAPE:0.19181203865275384
R2 score:0.3716593421086084
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:56:22
epoch 0  | loss: 1.94826 | val_0_rmse: 0.98857 | val_1_rmse: 0.96882 |  0:00:00s
epoch 1  | loss: 1.15335 | val_0_rmse: 0.99184 | val_1_rmse: 0.97141 |  0:00:00s
epoch 2  | loss: 1.03432 | val_0_rmse: 0.98993 | val_1_rmse: 0.96741 |  0:00:00s
epoch 3  | loss: 0.99743 | val_0_rmse: 0.98644 | val_1_rmse: 0.96428 |  0:00:01s
epoch 4  | loss: 0.98261 | val_0_rmse: 0.98686 | val_1_rmse: 0.96441 |  0:00:01s
epoch 5  | loss: 0.97694 | val_0_rmse: 0.98725 | val_1_rmse: 0.96529 |  0:00:01s
epoch 6  | loss: 0.97713 | val_0_rmse: 0.98756 | val_1_rmse: 0.96424 |  0:00:02s
epoch 7  | loss: 0.97418 | val_0_rmse: 0.98507 | val_1_rmse: 0.96066 |  0:00:02s
epoch 8  | loss: 0.96938 | val_0_rmse: 0.98019 | val_1_rmse: 0.95661 |  0:00:02s
epoch 9  | loss: 0.95851 | val_0_rmse: 0.96707 | val_1_rmse: 0.94088 |  0:00:02s
epoch 10 | loss: 0.95049 | val_0_rmse: 0.9503  | val_1_rmse: 0.92762 |  0:00:03s
epoch 11 | loss: 0.92759 | val_0_rmse: 0.9349  | val_1_rmse: 0.91399 |  0:00:03s
epoch 12 | loss: 0.90374 | val_0_rmse: 0.9234  | val_1_rmse: 0.89942 |  0:00:03s
epoch 13 | loss: 0.8787  | val_0_rmse: 0.91947 | val_1_rmse: 0.89376 |  0:00:04s
epoch 14 | loss: 0.83781 | val_0_rmse: 0.90514 | val_1_rmse: 0.88554 |  0:00:04s
epoch 15 | loss: 0.83392 | val_0_rmse: 0.89729 | val_1_rmse: 0.88154 |  0:00:04s
epoch 16 | loss: 0.82116 | val_0_rmse: 0.89622 | val_1_rmse: 0.87553 |  0:00:04s
epoch 17 | loss: 0.79637 | val_0_rmse: 0.879   | val_1_rmse: 0.85545 |  0:00:05s
epoch 18 | loss: 0.76654 | val_0_rmse: 0.87645 | val_1_rmse: 0.85601 |  0:00:05s
epoch 19 | loss: 0.75373 | val_0_rmse: 0.8864  | val_1_rmse: 0.87323 |  0:00:05s
epoch 20 | loss: 0.74881 | val_0_rmse: 0.88014 | val_1_rmse: 0.85647 |  0:00:06s
epoch 21 | loss: 0.74332 | val_0_rmse: 0.88121 | val_1_rmse: 0.87102 |  0:00:06s
epoch 22 | loss: 0.73152 | val_0_rmse: 0.86158 | val_1_rmse: 0.84496 |  0:00:06s
epoch 23 | loss: 0.72394 | val_0_rmse: 0.86248 | val_1_rmse: 0.84354 |  0:00:07s
epoch 24 | loss: 0.71503 | val_0_rmse: 0.86202 | val_1_rmse: 0.84616 |  0:00:07s
epoch 25 | loss: 0.70332 | val_0_rmse: 0.86004 | val_1_rmse: 0.84465 |  0:00:07s
epoch 26 | loss: 0.69971 | val_0_rmse: 0.8593  | val_1_rmse: 0.84351 |  0:00:07s
epoch 27 | loss: 0.69661 | val_0_rmse: 0.85877 | val_1_rmse: 0.84224 |  0:00:08s
epoch 28 | loss: 0.68713 | val_0_rmse: 0.85612 | val_1_rmse: 0.84332 |  0:00:08s
epoch 29 | loss: 0.67118 | val_0_rmse: 0.8555  | val_1_rmse: 0.84836 |  0:00:08s
epoch 30 | loss: 0.6798  | val_0_rmse: 0.84687 | val_1_rmse: 0.8429  |  0:00:09s
epoch 31 | loss: 0.69453 | val_0_rmse: 0.86876 | val_1_rmse: 0.86612 |  0:00:09s
epoch 32 | loss: 0.65857 | val_0_rmse: 0.85579 | val_1_rmse: 0.84792 |  0:00:09s
epoch 33 | loss: 0.65701 | val_0_rmse: 0.84396 | val_1_rmse: 0.83509 |  0:00:09s
epoch 34 | loss: 0.64117 | val_0_rmse: 0.84435 | val_1_rmse: 0.83687 |  0:00:10s
epoch 35 | loss: 0.65693 | val_0_rmse: 0.85365 | val_1_rmse: 0.84972 |  0:00:10s
epoch 36 | loss: 0.65249 | val_0_rmse: 0.84444 | val_1_rmse: 0.83202 |  0:00:10s
epoch 37 | loss: 0.67223 | val_0_rmse: 0.84341 | val_1_rmse: 0.82647 |  0:00:11s
epoch 38 | loss: 0.66545 | val_0_rmse: 0.85826 | val_1_rmse: 0.84625 |  0:00:11s
epoch 39 | loss: 0.65238 | val_0_rmse: 0.85652 | val_1_rmse: 0.84707 |  0:00:11s
epoch 40 | loss: 0.64765 | val_0_rmse: 0.84336 | val_1_rmse: 0.83154 |  0:00:12s
epoch 41 | loss: 0.65807 | val_0_rmse: 0.84641 | val_1_rmse: 0.83247 |  0:00:12s
epoch 42 | loss: 0.66301 | val_0_rmse: 0.84318 | val_1_rmse: 0.83326 |  0:00:12s
epoch 43 | loss: 0.64312 | val_0_rmse: 0.82737 | val_1_rmse: 0.82805 |  0:00:13s
epoch 44 | loss: 0.63832 | val_0_rmse: 0.83618 | val_1_rmse: 0.83158 |  0:00:13s
epoch 45 | loss: 0.64942 | val_0_rmse: 0.82239 | val_1_rmse: 0.82167 |  0:00:13s
epoch 46 | loss: 0.6298  | val_0_rmse: 0.82796 | val_1_rmse: 0.83047 |  0:00:13s
epoch 47 | loss: 0.62542 | val_0_rmse: 0.81569 | val_1_rmse: 0.82182 |  0:00:14s
epoch 48 | loss: 0.61246 | val_0_rmse: 0.80828 | val_1_rmse: 0.81193 |  0:00:14s
epoch 49 | loss: 0.62005 | val_0_rmse: 0.79596 | val_1_rmse: 0.80144 |  0:00:14s
epoch 50 | loss: 0.60911 | val_0_rmse: 0.79895 | val_1_rmse: 0.80567 |  0:00:15s
epoch 51 | loss: 0.59887 | val_0_rmse: 0.8088  | val_1_rmse: 0.81904 |  0:00:15s
epoch 52 | loss: 0.60496 | val_0_rmse: 0.80483 | val_1_rmse: 0.81309 |  0:00:15s
epoch 53 | loss: 0.59447 | val_0_rmse: 0.80921 | val_1_rmse: 0.81501 |  0:00:15s
epoch 54 | loss: 0.59703 | val_0_rmse: 0.81001 | val_1_rmse: 0.81353 |  0:00:16s
epoch 55 | loss: 0.59355 | val_0_rmse: 0.79759 | val_1_rmse: 0.80999 |  0:00:16s
epoch 56 | loss: 0.57803 | val_0_rmse: 0.79428 | val_1_rmse: 0.81282 |  0:00:16s
epoch 57 | loss: 0.57714 | val_0_rmse: 0.7882  | val_1_rmse: 0.80433 |  0:00:17s
epoch 58 | loss: 0.59379 | val_0_rmse: 0.78315 | val_1_rmse: 0.7994  |  0:00:17s
epoch 59 | loss: 0.57836 | val_0_rmse: 0.7858  | val_1_rmse: 0.80623 |  0:00:17s
epoch 60 | loss: 0.57137 | val_0_rmse: 0.78472 | val_1_rmse: 0.80596 |  0:00:17s
epoch 61 | loss: 0.57816 | val_0_rmse: 0.7809  | val_1_rmse: 0.79988 |  0:00:18s
epoch 62 | loss: 0.57319 | val_0_rmse: 0.7875  | val_1_rmse: 0.80287 |  0:00:18s
epoch 63 | loss: 0.57026 | val_0_rmse: 0.78078 | val_1_rmse: 0.80938 |  0:00:18s
epoch 64 | loss: 0.55981 | val_0_rmse: 0.77757 | val_1_rmse: 0.80287 |  0:00:19s
epoch 65 | loss: 0.56229 | val_0_rmse: 0.77881 | val_1_rmse: 0.79916 |  0:00:19s
epoch 66 | loss: 0.56771 | val_0_rmse: 0.78778 | val_1_rmse: 0.81419 |  0:00:19s
epoch 67 | loss: 0.57346 | val_0_rmse: 0.77809 | val_1_rmse: 0.80832 |  0:00:20s
epoch 68 | loss: 0.56663 | val_0_rmse: 0.77998 | val_1_rmse: 0.80052 |  0:00:20s
epoch 69 | loss: 0.55403 | val_0_rmse: 0.77884 | val_1_rmse: 0.80662 |  0:00:20s
epoch 70 | loss: 0.55174 | val_0_rmse: 0.78037 | val_1_rmse: 0.81115 |  0:00:20s
epoch 71 | loss: 0.55414 | val_0_rmse: 0.78091 | val_1_rmse: 0.80947 |  0:00:21s
epoch 72 | loss: 0.5645  | val_0_rmse: 0.77462 | val_1_rmse: 0.80034 |  0:00:21s
epoch 73 | loss: 0.55599 | val_0_rmse: 0.7666  | val_1_rmse: 0.80273 |  0:00:21s
epoch 74 | loss: 0.55487 | val_0_rmse: 0.76653 | val_1_rmse: 0.79937 |  0:00:22s
epoch 75 | loss: 0.54475 | val_0_rmse: 0.772   | val_1_rmse: 0.80773 |  0:00:22s
epoch 76 | loss: 0.54173 | val_0_rmse: 0.76286 | val_1_rmse: 0.79721 |  0:00:22s
epoch 77 | loss: 0.54258 | val_0_rmse: 0.7633  | val_1_rmse: 0.79522 |  0:00:22s
epoch 78 | loss: 0.53599 | val_0_rmse: 0.76222 | val_1_rmse: 0.7995  |  0:00:23s
epoch 79 | loss: 0.54399 | val_0_rmse: 0.75893 | val_1_rmse: 0.80147 |  0:00:23s
epoch 80 | loss: 0.53993 | val_0_rmse: 0.75832 | val_1_rmse: 0.8206  |  0:00:23s
epoch 81 | loss: 0.54265 | val_0_rmse: 0.74826 | val_1_rmse: 0.79776 |  0:00:24s
epoch 82 | loss: 0.53319 | val_0_rmse: 0.74915 | val_1_rmse: 0.78313 |  0:00:24s
epoch 83 | loss: 0.54144 | val_0_rmse: 0.74594 | val_1_rmse: 0.79446 |  0:00:24s
epoch 84 | loss: 0.53402 | val_0_rmse: 0.74666 | val_1_rmse: 0.79751 |  0:00:24s
epoch 85 | loss: 0.5274  | val_0_rmse: 0.74287 | val_1_rmse: 0.79695 |  0:00:25s
epoch 86 | loss: 0.53008 | val_0_rmse: 0.73725 | val_1_rmse: 0.80548 |  0:00:25s
epoch 87 | loss: 0.52895 | val_0_rmse: 0.73827 | val_1_rmse: 0.81012 |  0:00:25s
epoch 88 | loss: 0.52429 | val_0_rmse: 0.74402 | val_1_rmse: 0.81206 |  0:00:26s
epoch 89 | loss: 0.52245 | val_0_rmse: 0.73548 | val_1_rmse: 0.79782 |  0:00:26s
epoch 90 | loss: 0.52803 | val_0_rmse: 0.73398 | val_1_rmse: 0.79548 |  0:00:26s
epoch 91 | loss: 0.5123  | val_0_rmse: 0.72968 | val_1_rmse: 0.79946 |  0:00:27s
epoch 92 | loss: 0.51189 | val_0_rmse: 0.73279 | val_1_rmse: 0.7909  |  0:00:27s
epoch 93 | loss: 0.5146  | val_0_rmse: 0.73152 | val_1_rmse: 0.78561 |  0:00:27s
epoch 94 | loss: 0.51123 | val_0_rmse: 0.73017 | val_1_rmse: 0.79743 |  0:00:27s
epoch 95 | loss: 0.51341 | val_0_rmse: 0.72646 | val_1_rmse: 0.79053 |  0:00:28s
epoch 96 | loss: 0.50971 | val_0_rmse: 0.72259 | val_1_rmse: 0.78526 |  0:00:28s
epoch 97 | loss: 0.51488 | val_0_rmse: 0.726   | val_1_rmse: 0.80009 |  0:00:28s
epoch 98 | loss: 0.51156 | val_0_rmse: 0.72305 | val_1_rmse: 0.80695 |  0:00:29s
epoch 99 | loss: 0.51    | val_0_rmse: 0.7254  | val_1_rmse: 0.79593 |  0:00:29s
epoch 100| loss: 0.50633 | val_0_rmse: 0.71133 | val_1_rmse: 0.79081 |  0:00:29s
epoch 101| loss: 0.49552 | val_0_rmse: 0.70874 | val_1_rmse: 0.79813 |  0:00:29s
epoch 102| loss: 0.49948 | val_0_rmse: 0.70646 | val_1_rmse: 0.79846 |  0:00:30s
epoch 103| loss: 0.50506 | val_0_rmse: 0.70509 | val_1_rmse: 0.7979  |  0:00:30s
epoch 104| loss: 0.49581 | val_0_rmse: 0.70877 | val_1_rmse: 0.78737 |  0:00:30s
epoch 105| loss: 0.49747 | val_0_rmse: 0.70342 | val_1_rmse: 0.78825 |  0:00:31s
epoch 106| loss: 0.48991 | val_0_rmse: 0.70348 | val_1_rmse: 0.78905 |  0:00:31s
epoch 107| loss: 0.4873  | val_0_rmse: 0.69974 | val_1_rmse: 0.7932  |  0:00:31s
epoch 108| loss: 0.49237 | val_0_rmse: 0.70212 | val_1_rmse: 0.78709 |  0:00:31s
epoch 109| loss: 0.4879  | val_0_rmse: 0.69305 | val_1_rmse: 0.79321 |  0:00:32s
epoch 110| loss: 0.47971 | val_0_rmse: 0.68892 | val_1_rmse: 0.80654 |  0:00:32s
epoch 111| loss: 0.48622 | val_0_rmse: 0.69548 | val_1_rmse: 0.80737 |  0:00:32s
epoch 112| loss: 0.49693 | val_0_rmse: 0.69606 | val_1_rmse: 0.80253 |  0:00:33s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.78313
Best weights from best epoch are automatically used!
ended training at: 00:56:55
Feature importance:
Mean squared error is of 0.06490436727820241
Mean absolute error:0.1755035865497611
MAPE:0.18483917659590487
R2 score:0.3405995226543287
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:56:55
epoch 0  | loss: 1.90833 | val_0_rmse: 0.99279 | val_1_rmse: 1.00532 |  0:00:00s
epoch 1  | loss: 1.14734 | val_0_rmse: 0.99223 | val_1_rmse: 1.0052  |  0:00:00s
epoch 2  | loss: 1.03712 | val_0_rmse: 0.99167 | val_1_rmse: 1.00645 |  0:00:00s
epoch 3  | loss: 0.99713 | val_0_rmse: 0.99175 | val_1_rmse: 1.00617 |  0:00:01s
epoch 4  | loss: 0.98908 | val_0_rmse: 0.99189 | val_1_rmse: 1.0072  |  0:00:01s
epoch 5  | loss: 0.99311 | val_0_rmse: 0.98861 | val_1_rmse: 1.00347 |  0:00:01s
epoch 6  | loss: 0.99161 | val_0_rmse: 0.98349 | val_1_rmse: 0.99985 |  0:00:02s
epoch 7  | loss: 0.97983 | val_0_rmse: 0.98304 | val_1_rmse: 1.0022  |  0:00:02s
epoch 8  | loss: 0.97282 | val_0_rmse: 0.97996 | val_1_rmse: 0.9986  |  0:00:02s
epoch 9  | loss: 0.96019 | val_0_rmse: 0.97628 | val_1_rmse: 0.99404 |  0:00:02s
epoch 10 | loss: 0.95999 | val_0_rmse: 0.97816 | val_1_rmse: 0.99949 |  0:00:03s
epoch 11 | loss: 0.94703 | val_0_rmse: 0.97625 | val_1_rmse: 0.9958  |  0:00:03s
epoch 12 | loss: 0.94704 | val_0_rmse: 0.97959 | val_1_rmse: 1.00405 |  0:00:03s
epoch 13 | loss: 0.94711 | val_0_rmse: 0.97622 | val_1_rmse: 0.9995  |  0:00:04s
epoch 14 | loss: 0.94057 | val_0_rmse: 0.97196 | val_1_rmse: 0.98838 |  0:00:04s
epoch 15 | loss: 0.93802 | val_0_rmse: 0.97189 | val_1_rmse: 0.99214 |  0:00:04s
epoch 16 | loss: 0.93647 | val_0_rmse: 0.97282 | val_1_rmse: 0.99397 |  0:00:04s
epoch 17 | loss: 0.9319  | val_0_rmse: 0.9717  | val_1_rmse: 0.98811 |  0:00:05s
epoch 18 | loss: 0.93238 | val_0_rmse: 0.96978 | val_1_rmse: 0.9861  |  0:00:05s
epoch 19 | loss: 0.92639 | val_0_rmse: 0.96827 | val_1_rmse: 0.98842 |  0:00:05s
epoch 20 | loss: 0.9089  | val_0_rmse: 0.96369 | val_1_rmse: 0.9839  |  0:00:06s
epoch 21 | loss: 0.90502 | val_0_rmse: 0.95851 | val_1_rmse: 0.97763 |  0:00:06s
epoch 22 | loss: 0.89827 | val_0_rmse: 0.95268 | val_1_rmse: 0.97253 |  0:00:06s
epoch 23 | loss: 0.87737 | val_0_rmse: 0.94718 | val_1_rmse: 0.96688 |  0:00:07s
epoch 24 | loss: 0.86649 | val_0_rmse: 0.934   | val_1_rmse: 0.95431 |  0:00:07s
epoch 25 | loss: 0.84065 | val_0_rmse: 0.90469 | val_1_rmse: 0.92603 |  0:00:07s
epoch 26 | loss: 0.81498 | val_0_rmse: 0.89296 | val_1_rmse: 0.9149  |  0:00:07s
epoch 27 | loss: 0.80392 | val_0_rmse: 0.88494 | val_1_rmse: 0.90977 |  0:00:08s
epoch 28 | loss: 0.77567 | val_0_rmse: 0.87839 | val_1_rmse: 0.90303 |  0:00:08s
epoch 29 | loss: 0.75826 | val_0_rmse: 0.88091 | val_1_rmse: 0.90531 |  0:00:08s
epoch 30 | loss: 0.72941 | val_0_rmse: 0.87347 | val_1_rmse: 0.89243 |  0:00:09s
epoch 31 | loss: 0.74103 | val_0_rmse: 0.87236 | val_1_rmse: 0.88927 |  0:00:09s
epoch 32 | loss: 0.71934 | val_0_rmse: 0.89057 | val_1_rmse: 0.90874 |  0:00:09s
epoch 33 | loss: 0.69247 | val_0_rmse: 0.86404 | val_1_rmse: 0.88875 |  0:00:09s
epoch 34 | loss: 0.66803 | val_0_rmse: 0.86638 | val_1_rmse: 0.89105 |  0:00:10s
epoch 35 | loss: 0.67147 | val_0_rmse: 0.86285 | val_1_rmse: 0.88913 |  0:00:10s
epoch 36 | loss: 0.66647 | val_0_rmse: 0.86131 | val_1_rmse: 0.88885 |  0:00:10s
epoch 37 | loss: 0.65401 | val_0_rmse: 0.85421 | val_1_rmse: 0.88716 |  0:00:11s
epoch 38 | loss: 0.64957 | val_0_rmse: 0.82847 | val_1_rmse: 0.85803 |  0:00:11s
epoch 39 | loss: 0.64118 | val_0_rmse: 0.84904 | val_1_rmse: 0.88753 |  0:00:11s
epoch 40 | loss: 0.64102 | val_0_rmse: 0.83694 | val_1_rmse: 0.86337 |  0:00:12s
epoch 41 | loss: 0.64134 | val_0_rmse: 0.82442 | val_1_rmse: 0.85805 |  0:00:12s
epoch 42 | loss: 0.61526 | val_0_rmse: 0.85832 | val_1_rmse: 0.89927 |  0:00:12s
epoch 43 | loss: 0.6147  | val_0_rmse: 0.82681 | val_1_rmse: 0.84966 |  0:00:12s
epoch 44 | loss: 0.60762 | val_0_rmse: 0.84339 | val_1_rmse: 0.88912 |  0:00:13s
epoch 45 | loss: 0.60501 | val_0_rmse: 0.81416 | val_1_rmse: 0.84852 |  0:00:13s
epoch 46 | loss: 0.6032  | val_0_rmse: 0.8418  | val_1_rmse: 0.86922 |  0:00:13s
epoch 47 | loss: 0.59421 | val_0_rmse: 0.83435 | val_1_rmse: 0.88202 |  0:00:14s
epoch 48 | loss: 0.59383 | val_0_rmse: 0.83152 | val_1_rmse: 0.87595 |  0:00:14s
epoch 49 | loss: 0.58875 | val_0_rmse: 0.83185 | val_1_rmse: 0.87337 |  0:00:14s
epoch 50 | loss: 0.58462 | val_0_rmse: 0.80879 | val_1_rmse: 0.85243 |  0:00:14s
epoch 51 | loss: 0.57529 | val_0_rmse: 0.8146  | val_1_rmse: 0.8665  |  0:00:15s
epoch 52 | loss: 0.57212 | val_0_rmse: 0.79584 | val_1_rmse: 0.84542 |  0:00:15s
epoch 53 | loss: 0.5675  | val_0_rmse: 0.80245 | val_1_rmse: 0.84819 |  0:00:15s
epoch 54 | loss: 0.56866 | val_0_rmse: 0.80478 | val_1_rmse: 0.85203 |  0:00:16s
epoch 55 | loss: 0.56289 | val_0_rmse: 0.85433 | val_1_rmse: 0.92326 |  0:00:16s
epoch 56 | loss: 0.56428 | val_0_rmse: 0.7991  | val_1_rmse: 0.85346 |  0:00:16s
epoch 57 | loss: 0.55724 | val_0_rmse: 0.79973 | val_1_rmse: 0.85373 |  0:00:17s
epoch 58 | loss: 0.56256 | val_0_rmse: 0.79549 | val_1_rmse: 0.8468  |  0:00:17s
epoch 59 | loss: 0.54797 | val_0_rmse: 0.79748 | val_1_rmse: 0.84813 |  0:00:17s
epoch 60 | loss: 0.55436 | val_0_rmse: 0.79509 | val_1_rmse: 0.85353 |  0:00:17s
epoch 61 | loss: 0.5488  | val_0_rmse: 0.78889 | val_1_rmse: 0.86295 |  0:00:18s
epoch 62 | loss: 0.54591 | val_0_rmse: 0.7878  | val_1_rmse: 0.84989 |  0:00:18s
epoch 63 | loss: 0.54787 | val_0_rmse: 0.78345 | val_1_rmse: 0.84461 |  0:00:18s
epoch 64 | loss: 0.54647 | val_0_rmse: 0.79458 | val_1_rmse: 0.84339 |  0:00:19s
epoch 65 | loss: 0.533   | val_0_rmse: 0.80081 | val_1_rmse: 0.88624 |  0:00:19s
epoch 66 | loss: 0.53409 | val_0_rmse: 0.78555 | val_1_rmse: 0.86674 |  0:00:19s
epoch 67 | loss: 0.54884 | val_0_rmse: 0.78901 | val_1_rmse: 0.84227 |  0:00:20s
epoch 68 | loss: 0.54149 | val_0_rmse: 0.77958 | val_1_rmse: 0.86584 |  0:00:20s
epoch 69 | loss: 0.53529 | val_0_rmse: 0.77685 | val_1_rmse: 0.84452 |  0:00:20s
epoch 70 | loss: 0.54881 | val_0_rmse: 0.77828 | val_1_rmse: 0.83975 |  0:00:20s
epoch 71 | loss: 0.53905 | val_0_rmse: 0.76797 | val_1_rmse: 0.83315 |  0:00:21s
epoch 72 | loss: 0.54042 | val_0_rmse: 0.79735 | val_1_rmse: 0.87163 |  0:00:21s
epoch 73 | loss: 0.53185 | val_0_rmse: 0.76223 | val_1_rmse: 0.8235  |  0:00:21s
epoch 74 | loss: 0.52553 | val_0_rmse: 0.79364 | val_1_rmse: 0.8404  |  0:00:22s
epoch 75 | loss: 0.52698 | val_0_rmse: 0.75976 | val_1_rmse: 0.83239 |  0:00:22s
epoch 76 | loss: 0.5294  | val_0_rmse: 0.754   | val_1_rmse: 0.83667 |  0:00:22s
epoch 77 | loss: 0.52075 | val_0_rmse: 0.76591 | val_1_rmse: 0.85149 |  0:00:22s
epoch 78 | loss: 0.52663 | val_0_rmse: 0.76463 | val_1_rmse: 0.85387 |  0:00:23s
epoch 79 | loss: 0.52297 | val_0_rmse: 0.75396 | val_1_rmse: 0.82406 |  0:00:23s
epoch 80 | loss: 0.52903 | val_0_rmse: 0.75235 | val_1_rmse: 0.81937 |  0:00:23s
epoch 81 | loss: 0.51481 | val_0_rmse: 0.74584 | val_1_rmse: 0.82765 |  0:00:24s
epoch 82 | loss: 0.50884 | val_0_rmse: 0.76548 | val_1_rmse: 0.85765 |  0:00:24s
epoch 83 | loss: 0.50516 | val_0_rmse: 0.75411 | val_1_rmse: 0.82216 |  0:00:24s
epoch 84 | loss: 0.50115 | val_0_rmse: 0.73911 | val_1_rmse: 0.81125 |  0:00:24s
epoch 85 | loss: 0.50694 | val_0_rmse: 0.7472  | val_1_rmse: 0.82881 |  0:00:25s
epoch 86 | loss: 0.5061  | val_0_rmse: 0.74002 | val_1_rmse: 0.80973 |  0:00:25s
epoch 87 | loss: 0.5093  | val_0_rmse: 0.74409 | val_1_rmse: 0.83328 |  0:00:25s
epoch 88 | loss: 0.50716 | val_0_rmse: 0.7632  | val_1_rmse: 0.82138 |  0:00:26s
epoch 89 | loss: 0.52223 | val_0_rmse: 0.74782 | val_1_rmse: 0.81869 |  0:00:26s
epoch 90 | loss: 0.5196  | val_0_rmse: 0.75355 | val_1_rmse: 0.82547 |  0:00:26s
epoch 91 | loss: 0.51616 | val_0_rmse: 0.73711 | val_1_rmse: 0.82489 |  0:00:26s
epoch 92 | loss: 0.51025 | val_0_rmse: 0.75999 | val_1_rmse: 0.86045 |  0:00:27s
epoch 93 | loss: 0.50588 | val_0_rmse: 0.74023 | val_1_rmse: 0.8138  |  0:00:27s
epoch 94 | loss: 0.51098 | val_0_rmse: 0.7171  | val_1_rmse: 0.82515 |  0:00:27s
epoch 95 | loss: 0.49625 | val_0_rmse: 0.71928 | val_1_rmse: 0.82092 |  0:00:28s
epoch 96 | loss: 0.49824 | val_0_rmse: 0.74048 | val_1_rmse: 0.82052 |  0:00:28s
epoch 97 | loss: 0.5044  | val_0_rmse: 0.73615 | val_1_rmse: 0.81848 |  0:00:28s
epoch 98 | loss: 0.48877 | val_0_rmse: 0.70879 | val_1_rmse: 0.81907 |  0:00:29s
epoch 99 | loss: 0.4947  | val_0_rmse: 0.72157 | val_1_rmse: 0.83385 |  0:00:29s
epoch 100| loss: 0.50342 | val_0_rmse: 0.72154 | val_1_rmse: 0.81456 |  0:00:29s
epoch 101| loss: 0.48948 | val_0_rmse: 0.70884 | val_1_rmse: 0.80816 |  0:00:29s
epoch 102| loss: 0.47857 | val_0_rmse: 0.71497 | val_1_rmse: 0.80719 |  0:00:30s
epoch 103| loss: 0.48229 | val_0_rmse: 0.6932  | val_1_rmse: 0.81081 |  0:00:30s
epoch 104| loss: 0.4817  | val_0_rmse: 0.69589 | val_1_rmse: 0.83111 |  0:00:30s
epoch 105| loss: 0.47867 | val_0_rmse: 0.69129 | val_1_rmse: 0.81879 |  0:00:31s
epoch 106| loss: 0.47168 | val_0_rmse: 0.69606 | val_1_rmse: 0.81992 |  0:00:31s
epoch 107| loss: 0.47809 | val_0_rmse: 0.70019 | val_1_rmse: 0.82634 |  0:00:31s
epoch 108| loss: 0.46918 | val_0_rmse: 0.70304 | val_1_rmse: 0.86122 |  0:00:31s
epoch 109| loss: 0.48153 | val_0_rmse: 0.69545 | val_1_rmse: 0.81195 |  0:00:32s
epoch 110| loss: 0.47404 | val_0_rmse: 0.71057 | val_1_rmse: 0.80989 |  0:00:32s
epoch 111| loss: 0.47091 | val_0_rmse: 0.69545 | val_1_rmse: 0.81659 |  0:00:32s
epoch 112| loss: 0.46858 | val_0_rmse: 0.68362 | val_1_rmse: 0.79793 |  0:00:33s
epoch 113| loss: 0.4678  | val_0_rmse: 0.67594 | val_1_rmse: 0.79967 |  0:00:33s
epoch 114| loss: 0.46817 | val_0_rmse: 0.67316 | val_1_rmse: 0.81534 |  0:00:33s
epoch 115| loss: 0.47799 | val_0_rmse: 0.68055 | val_1_rmse: 0.80414 |  0:00:33s
epoch 116| loss: 0.46834 | val_0_rmse: 0.68397 | val_1_rmse: 0.79903 |  0:00:34s
epoch 117| loss: 0.45337 | val_0_rmse: 0.70207 | val_1_rmse: 0.80164 |  0:00:34s
epoch 118| loss: 0.46525 | val_0_rmse: 0.68563 | val_1_rmse: 0.83693 |  0:00:34s
epoch 119| loss: 0.46268 | val_0_rmse: 0.67602 | val_1_rmse: 0.82449 |  0:00:35s
epoch 120| loss: 0.46505 | val_0_rmse: 0.71103 | val_1_rmse: 0.8286  |  0:00:35s
epoch 121| loss: 0.45763 | val_0_rmse: 0.68266 | val_1_rmse: 0.81072 |  0:00:35s
epoch 122| loss: 0.47632 | val_0_rmse: 0.68223 | val_1_rmse: 0.79695 |  0:00:36s
epoch 123| loss: 0.46602 | val_0_rmse: 0.69503 | val_1_rmse: 0.79789 |  0:00:36s
epoch 124| loss: 0.47752 | val_0_rmse: 0.74032 | val_1_rmse: 0.90398 |  0:00:36s
epoch 125| loss: 0.45881 | val_0_rmse: 0.66256 | val_1_rmse: 0.80449 |  0:00:37s
epoch 126| loss: 0.46478 | val_0_rmse: 0.66433 | val_1_rmse: 0.81106 |  0:00:37s
epoch 127| loss: 0.45089 | val_0_rmse: 0.67669 | val_1_rmse: 0.85557 |  0:00:37s
epoch 128| loss: 0.44253 | val_0_rmse: 0.64695 | val_1_rmse: 0.8044  |  0:00:37s
epoch 129| loss: 0.42701 | val_0_rmse: 0.64309 | val_1_rmse: 0.80859 |  0:00:38s
epoch 130| loss: 0.43962 | val_0_rmse: 0.6499  | val_1_rmse: 0.78473 |  0:00:38s
epoch 131| loss: 0.45573 | val_0_rmse: 0.66663 | val_1_rmse: 0.79461 |  0:00:38s
epoch 132| loss: 0.43548 | val_0_rmse: 0.64823 | val_1_rmse: 0.81714 |  0:00:39s
epoch 133| loss: 0.44427 | val_0_rmse: 0.64661 | val_1_rmse: 0.82835 |  0:00:39s
epoch 134| loss: 0.45339 | val_0_rmse: 0.64527 | val_1_rmse: 0.80291 |  0:00:39s
epoch 135| loss: 0.44589 | val_0_rmse: 0.64307 | val_1_rmse: 0.80789 |  0:00:39s
epoch 136| loss: 0.42547 | val_0_rmse: 0.63881 | val_1_rmse: 0.79232 |  0:00:40s
epoch 137| loss: 0.43241 | val_0_rmse: 0.64991 | val_1_rmse: 0.80223 |  0:00:40s
epoch 138| loss: 0.42441 | val_0_rmse: 0.69036 | val_1_rmse: 0.81341 |  0:00:40s
epoch 139| loss: 0.42963 | val_0_rmse: 0.62697 | val_1_rmse: 0.80267 |  0:00:41s
epoch 140| loss: 0.41302 | val_0_rmse: 0.63116 | val_1_rmse: 0.80655 |  0:00:41s
epoch 141| loss: 0.42417 | val_0_rmse: 0.62709 | val_1_rmse: 0.81596 |  0:00:41s
epoch 142| loss: 0.41664 | val_0_rmse: 0.63788 | val_1_rmse: 0.82411 |  0:00:41s
epoch 143| loss: 0.40118 | val_0_rmse: 0.62399 | val_1_rmse: 0.80216 |  0:00:42s
epoch 144| loss: 0.40239 | val_0_rmse: 0.61733 | val_1_rmse: 0.81125 |  0:00:42s
epoch 145| loss: 0.40417 | val_0_rmse: 0.6209  | val_1_rmse: 0.80359 |  0:00:42s
epoch 146| loss: 0.40645 | val_0_rmse: 0.61526 | val_1_rmse: 0.8343  |  0:00:43s
epoch 147| loss: 0.41233 | val_0_rmse: 0.62396 | val_1_rmse: 0.79907 |  0:00:43s
epoch 148| loss: 0.41112 | val_0_rmse: 0.60906 | val_1_rmse: 0.83255 |  0:00:43s
epoch 149| loss: 0.42136 | val_0_rmse: 0.6157  | val_1_rmse: 0.79721 |  0:00:43s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.78473
Best weights from best epoch are automatically used!
ended training at: 00:57:39
Feature importance:
Mean squared error is of 0.06382539338225081
Mean absolute error:0.1734812870808536
MAPE:0.19612023031002127
R2 score:0.303253448792996
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:57:40
epoch 0  | loss: 2.14712 | val_0_rmse: 0.99877 | val_1_rmse: 1.00562 |  0:00:00s
epoch 1  | loss: 1.19969 | val_0_rmse: 0.99662 | val_1_rmse: 1.01048 |  0:00:00s
epoch 2  | loss: 1.05502 | val_0_rmse: 1.00038 | val_1_rmse: 1.01164 |  0:00:00s
epoch 3  | loss: 1.00334 | val_0_rmse: 1.00258 | val_1_rmse: 1.01077 |  0:00:01s
epoch 4  | loss: 0.99513 | val_0_rmse: 0.9962  | val_1_rmse: 1.00703 |  0:00:01s
epoch 5  | loss: 0.99527 | val_0_rmse: 0.99225 | val_1_rmse: 1.00339 |  0:00:01s
epoch 6  | loss: 0.96657 | val_0_rmse: 0.97779 | val_1_rmse: 0.99554 |  0:00:02s
epoch 7  | loss: 0.9705  | val_0_rmse: 0.97904 | val_1_rmse: 0.99649 |  0:00:02s
epoch 8  | loss: 0.95293 | val_0_rmse: 0.97626 | val_1_rmse: 0.99292 |  0:00:02s
epoch 9  | loss: 0.95568 | val_0_rmse: 0.98679 | val_1_rmse: 1.00685 |  0:00:02s
epoch 10 | loss: 0.95    | val_0_rmse: 0.97075 | val_1_rmse: 1.00403 |  0:00:03s
epoch 11 | loss: 0.94941 | val_0_rmse: 0.97389 | val_1_rmse: 0.99628 |  0:00:03s
epoch 12 | loss: 0.95077 | val_0_rmse: 0.97184 | val_1_rmse: 1.00516 |  0:00:03s
epoch 13 | loss: 0.94615 | val_0_rmse: 0.97406 | val_1_rmse: 1.00852 |  0:00:04s
epoch 14 | loss: 0.94889 | val_0_rmse: 0.97031 | val_1_rmse: 0.99535 |  0:00:04s
epoch 15 | loss: 0.94668 | val_0_rmse: 0.96874 | val_1_rmse: 0.99648 |  0:00:04s
epoch 16 | loss: 0.94803 | val_0_rmse: 0.96938 | val_1_rmse: 0.99963 |  0:00:04s
epoch 17 | loss: 0.94196 | val_0_rmse: 0.96974 | val_1_rmse: 0.99619 |  0:00:05s
epoch 18 | loss: 0.94203 | val_0_rmse: 0.96698 | val_1_rmse: 0.99733 |  0:00:05s
epoch 19 | loss: 0.93941 | val_0_rmse: 0.96775 | val_1_rmse: 0.99735 |  0:00:05s
epoch 20 | loss: 0.93657 | val_0_rmse: 0.96709 | val_1_rmse: 0.99719 |  0:00:06s
epoch 21 | loss: 0.92895 | val_0_rmse: 0.96792 | val_1_rmse: 1.00361 |  0:00:06s
epoch 22 | loss: 0.92871 | val_0_rmse: 0.96336 | val_1_rmse: 1.00298 |  0:00:06s
epoch 23 | loss: 0.92697 | val_0_rmse: 0.95916 | val_1_rmse: 0.99962 |  0:00:06s
epoch 24 | loss: 0.92905 | val_0_rmse: 0.95844 | val_1_rmse: 0.99046 |  0:00:07s
epoch 25 | loss: 0.92436 | val_0_rmse: 0.95641 | val_1_rmse: 0.98392 |  0:00:07s
epoch 26 | loss: 0.91791 | val_0_rmse: 0.94939 | val_1_rmse: 0.98776 |  0:00:07s
epoch 27 | loss: 0.90483 | val_0_rmse: 0.9375  | val_1_rmse: 0.97548 |  0:00:08s
epoch 28 | loss: 0.87919 | val_0_rmse: 0.92087 | val_1_rmse: 0.9525  |  0:00:08s
epoch 29 | loss: 0.84454 | val_0_rmse: 0.90369 | val_1_rmse: 0.93848 |  0:00:08s
epoch 30 | loss: 0.82324 | val_0_rmse: 0.89688 | val_1_rmse: 0.92352 |  0:00:09s
epoch 31 | loss: 0.79638 | val_0_rmse: 0.88957 | val_1_rmse: 0.91815 |  0:00:09s
epoch 32 | loss: 0.76237 | val_0_rmse: 0.91281 | val_1_rmse: 0.94079 |  0:00:09s
epoch 33 | loss: 0.76254 | val_0_rmse: 0.95282 | val_1_rmse: 0.99151 |  0:00:09s
epoch 34 | loss: 0.74578 | val_0_rmse: 0.89763 | val_1_rmse: 0.94218 |  0:00:10s
epoch 35 | loss: 0.73073 | val_0_rmse: 0.91931 | val_1_rmse: 0.95739 |  0:00:10s
epoch 36 | loss: 0.71338 | val_0_rmse: 0.84568 | val_1_rmse: 0.88333 |  0:00:10s
epoch 37 | loss: 0.71764 | val_0_rmse: 0.84882 | val_1_rmse: 0.88613 |  0:00:11s
epoch 38 | loss: 0.68759 | val_0_rmse: 0.83976 | val_1_rmse: 0.87607 |  0:00:11s
epoch 39 | loss: 0.67382 | val_0_rmse: 0.83166 | val_1_rmse: 0.86422 |  0:00:11s
epoch 40 | loss: 0.66541 | val_0_rmse: 0.84668 | val_1_rmse: 0.867   |  0:00:12s
epoch 41 | loss: 0.68348 | val_0_rmse: 0.83889 | val_1_rmse: 0.86571 |  0:00:12s
epoch 42 | loss: 0.66177 | val_0_rmse: 0.83031 | val_1_rmse: 0.85927 |  0:00:12s
epoch 43 | loss: 0.64906 | val_0_rmse: 0.84128 | val_1_rmse: 0.86658 |  0:00:13s
epoch 44 | loss: 0.63853 | val_0_rmse: 0.82942 | val_1_rmse: 0.85708 |  0:00:13s
epoch 45 | loss: 0.63299 | val_0_rmse: 0.81861 | val_1_rmse: 0.84479 |  0:00:13s
epoch 46 | loss: 0.62672 | val_0_rmse: 0.82343 | val_1_rmse: 0.85528 |  0:00:13s
epoch 47 | loss: 0.62761 | val_0_rmse: 0.8219  | val_1_rmse: 0.85389 |  0:00:14s
epoch 48 | loss: 0.62335 | val_0_rmse: 0.81772 | val_1_rmse: 0.84577 |  0:00:14s
epoch 49 | loss: 0.63605 | val_0_rmse: 0.81498 | val_1_rmse: 0.84452 |  0:00:14s
epoch 50 | loss: 0.6246  | val_0_rmse: 0.83136 | val_1_rmse: 0.86218 |  0:00:15s
epoch 51 | loss: 0.62502 | val_0_rmse: 0.80535 | val_1_rmse: 0.83768 |  0:00:15s
epoch 52 | loss: 0.62856 | val_0_rmse: 0.79346 | val_1_rmse: 0.83453 |  0:00:15s
epoch 53 | loss: 0.61523 | val_0_rmse: 0.79696 | val_1_rmse: 0.84311 |  0:00:15s
epoch 54 | loss: 0.59835 | val_0_rmse: 0.81172 | val_1_rmse: 0.85008 |  0:00:16s
epoch 55 | loss: 0.59817 | val_0_rmse: 0.79143 | val_1_rmse: 0.83459 |  0:00:16s
epoch 56 | loss: 0.59415 | val_0_rmse: 0.80478 | val_1_rmse: 0.84724 |  0:00:16s
epoch 57 | loss: 0.60449 | val_0_rmse: 0.7908  | val_1_rmse: 0.83615 |  0:00:17s
epoch 58 | loss: 0.5792  | val_0_rmse: 0.78977 | val_1_rmse: 0.83352 |  0:00:17s
epoch 59 | loss: 0.5703  | val_0_rmse: 0.78341 | val_1_rmse: 0.83401 |  0:00:17s
epoch 60 | loss: 0.57714 | val_0_rmse: 0.80045 | val_1_rmse: 0.84414 |  0:00:18s
epoch 61 | loss: 0.58898 | val_0_rmse: 0.78634 | val_1_rmse: 0.83167 |  0:00:18s
epoch 62 | loss: 0.57418 | val_0_rmse: 0.80385 | val_1_rmse: 0.84621 |  0:00:18s
epoch 63 | loss: 0.59594 | val_0_rmse: 0.79123 | val_1_rmse: 0.84157 |  0:00:18s
epoch 64 | loss: 0.57751 | val_0_rmse: 0.80352 | val_1_rmse: 0.84851 |  0:00:19s
epoch 65 | loss: 0.58233 | val_0_rmse: 0.79092 | val_1_rmse: 0.84241 |  0:00:19s
epoch 66 | loss: 0.56509 | val_0_rmse: 0.78495 | val_1_rmse: 0.84031 |  0:00:19s
epoch 67 | loss: 0.5645  | val_0_rmse: 0.789   | val_1_rmse: 0.84434 |  0:00:20s
epoch 68 | loss: 0.56303 | val_0_rmse: 0.77975 | val_1_rmse: 0.83027 |  0:00:20s
epoch 69 | loss: 0.56452 | val_0_rmse: 0.77944 | val_1_rmse: 0.83096 |  0:00:20s
epoch 70 | loss: 0.55895 | val_0_rmse: 0.77882 | val_1_rmse: 0.84    |  0:00:21s
epoch 71 | loss: 0.57343 | val_0_rmse: 0.77847 | val_1_rmse: 0.83907 |  0:00:21s
epoch 72 | loss: 0.56649 | val_0_rmse: 0.8064  | val_1_rmse: 0.87043 |  0:00:21s
epoch 73 | loss: 0.57187 | val_0_rmse: 0.77525 | val_1_rmse: 0.83765 |  0:00:21s
epoch 74 | loss: 0.5684  | val_0_rmse: 0.77826 | val_1_rmse: 0.83485 |  0:00:22s
epoch 75 | loss: 0.56433 | val_0_rmse: 0.76434 | val_1_rmse: 0.83768 |  0:00:22s
epoch 76 | loss: 0.5521  | val_0_rmse: 0.7803  | val_1_rmse: 0.84507 |  0:00:22s
epoch 77 | loss: 0.55726 | val_0_rmse: 0.77264 | val_1_rmse: 0.83619 |  0:00:23s
epoch 78 | loss: 0.55803 | val_0_rmse: 0.7608  | val_1_rmse: 0.83113 |  0:00:23s
epoch 79 | loss: 0.5619  | val_0_rmse: 0.7605  | val_1_rmse: 0.83466 |  0:00:23s
epoch 80 | loss: 0.54687 | val_0_rmse: 0.75875 | val_1_rmse: 0.84875 |  0:00:23s
epoch 81 | loss: 0.53939 | val_0_rmse: 0.74664 | val_1_rmse: 0.82831 |  0:00:24s
epoch 82 | loss: 0.52623 | val_0_rmse: 0.74437 | val_1_rmse: 0.82167 |  0:00:24s
epoch 83 | loss: 0.52797 | val_0_rmse: 0.74188 | val_1_rmse: 0.818   |  0:00:24s
epoch 84 | loss: 0.54055 | val_0_rmse: 0.74206 | val_1_rmse: 0.81842 |  0:00:25s
epoch 85 | loss: 0.52328 | val_0_rmse: 0.74334 | val_1_rmse: 0.83312 |  0:00:25s
epoch 86 | loss: 0.54038 | val_0_rmse: 0.73257 | val_1_rmse: 0.83097 |  0:00:25s
epoch 87 | loss: 0.52228 | val_0_rmse: 0.74374 | val_1_rmse: 0.857   |  0:00:25s
epoch 88 | loss: 0.52098 | val_0_rmse: 0.73418 | val_1_rmse: 0.84171 |  0:00:26s
epoch 89 | loss: 0.52368 | val_0_rmse: 0.7235  | val_1_rmse: 0.83737 |  0:00:26s
epoch 90 | loss: 0.51687 | val_0_rmse: 0.73969 | val_1_rmse: 0.86961 |  0:00:26s
epoch 91 | loss: 0.50975 | val_0_rmse: 0.72273 | val_1_rmse: 0.84476 |  0:00:27s
epoch 92 | loss: 0.50202 | val_0_rmse: 0.71589 | val_1_rmse: 0.83558 |  0:00:27s
epoch 93 | loss: 0.49726 | val_0_rmse: 0.73445 | val_1_rmse: 0.85303 |  0:00:27s
epoch 94 | loss: 0.50057 | val_0_rmse: 0.72166 | val_1_rmse: 0.8265  |  0:00:27s
epoch 95 | loss: 0.49796 | val_0_rmse: 0.71302 | val_1_rmse: 0.82322 |  0:00:28s
epoch 96 | loss: 0.49455 | val_0_rmse: 0.716   | val_1_rmse: 0.83648 |  0:00:28s
epoch 97 | loss: 0.50028 | val_0_rmse: 0.70108 | val_1_rmse: 0.82641 |  0:00:28s
epoch 98 | loss: 0.49487 | val_0_rmse: 0.70863 | val_1_rmse: 0.83632 |  0:00:29s
epoch 99 | loss: 0.49567 | val_0_rmse: 0.70739 | val_1_rmse: 0.83751 |  0:00:29s
epoch 100| loss: 0.49691 | val_0_rmse: 0.72138 | val_1_rmse: 0.84519 |  0:00:29s
epoch 101| loss: 0.48507 | val_0_rmse: 0.6988  | val_1_rmse: 0.82142 |  0:00:30s
epoch 102| loss: 0.4871  | val_0_rmse: 0.69732 | val_1_rmse: 0.82403 |  0:00:30s
epoch 103| loss: 0.48159 | val_0_rmse: 0.68902 | val_1_rmse: 0.83007 |  0:00:30s
epoch 104| loss: 0.48982 | val_0_rmse: 0.70466 | val_1_rmse: 0.83597 |  0:00:30s
epoch 105| loss: 0.48593 | val_0_rmse: 0.6881  | val_1_rmse: 0.81868 |  0:00:31s
epoch 106| loss: 0.47291 | val_0_rmse: 0.70663 | val_1_rmse: 0.82121 |  0:00:31s
epoch 107| loss: 0.49419 | val_0_rmse: 0.69819 | val_1_rmse: 0.83746 |  0:00:31s
epoch 108| loss: 0.5047  | val_0_rmse: 0.7132  | val_1_rmse: 0.84937 |  0:00:32s
epoch 109| loss: 0.50124 | val_0_rmse: 0.7058  | val_1_rmse: 0.82025 |  0:00:32s
epoch 110| loss: 0.49837 | val_0_rmse: 0.6898  | val_1_rmse: 0.82548 |  0:00:32s
epoch 111| loss: 0.49953 | val_0_rmse: 0.69439 | val_1_rmse: 0.83375 |  0:00:33s
epoch 112| loss: 0.48834 | val_0_rmse: 0.69356 | val_1_rmse: 0.82438 |  0:00:33s
epoch 113| loss: 0.47552 | val_0_rmse: 0.68105 | val_1_rmse: 0.83461 |  0:00:33s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.818
Best weights from best epoch are automatically used!
ended training at: 00:58:14
Feature importance:
Mean squared error is of 0.059324032149284806
Mean absolute error:0.17464293841240122
MAPE:0.19411208060012092
R2 score:0.3299100551581695
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:58:14
epoch 0  | loss: 2.2357  | val_0_rmse: 1.02654 | val_1_rmse: 0.90535 |  0:00:00s
epoch 1  | loss: 1.39505 | val_0_rmse: 1.02007 | val_1_rmse: 0.90293 |  0:00:00s
epoch 2  | loss: 1.13287 | val_0_rmse: 1.02159 | val_1_rmse: 0.90465 |  0:00:00s
epoch 3  | loss: 1.09378 | val_0_rmse: 1.02303 | val_1_rmse: 0.90568 |  0:00:01s
epoch 4  | loss: 1.0551  | val_0_rmse: 1.02463 | val_1_rmse: 0.90817 |  0:00:01s
epoch 5  | loss: 1.04745 | val_0_rmse: 1.01908 | val_1_rmse: 0.89949 |  0:00:01s
epoch 6  | loss: 1.01879 | val_0_rmse: 0.99882 | val_1_rmse: 0.87869 |  0:00:02s
epoch 7  | loss: 0.96848 | val_0_rmse: 0.97144 | val_1_rmse: 0.85283 |  0:00:02s
epoch 8  | loss: 0.92067 | val_0_rmse: 0.96586 | val_1_rmse: 0.8473  |  0:00:02s
epoch 9  | loss: 0.89992 | val_0_rmse: 0.93122 | val_1_rmse: 0.81656 |  0:00:02s
epoch 10 | loss: 0.85554 | val_0_rmse: 0.96488 | val_1_rmse: 0.8969  |  0:00:03s
epoch 11 | loss: 0.82296 | val_0_rmse: 0.9372  | val_1_rmse: 0.85253 |  0:00:03s
epoch 12 | loss: 0.8326  | val_0_rmse: 0.92445 | val_1_rmse: 0.8348  |  0:00:03s
epoch 13 | loss: 0.82089 | val_0_rmse: 0.90648 | val_1_rmse: 0.83632 |  0:00:04s
epoch 14 | loss: 0.8346  | val_0_rmse: 0.90472 | val_1_rmse: 0.82263 |  0:00:04s
epoch 15 | loss: 0.82645 | val_0_rmse: 0.90259 | val_1_rmse: 0.82484 |  0:00:04s
epoch 16 | loss: 0.8115  | val_0_rmse: 0.9028  | val_1_rmse: 0.8345  |  0:00:05s
epoch 17 | loss: 0.80315 | val_0_rmse: 0.9015  | val_1_rmse: 0.83132 |  0:00:05s
epoch 18 | loss: 0.79098 | val_0_rmse: 0.89748 | val_1_rmse: 0.82281 |  0:00:05s
epoch 19 | loss: 0.80262 | val_0_rmse: 0.88893 | val_1_rmse: 0.80355 |  0:00:05s
epoch 20 | loss: 0.80493 | val_0_rmse: 0.89348 | val_1_rmse: 0.81486 |  0:00:06s
epoch 21 | loss: 0.84059 | val_0_rmse: 0.89854 | val_1_rmse: 0.82244 |  0:00:06s
epoch 22 | loss: 0.78637 | val_0_rmse: 0.88915 | val_1_rmse: 0.80414 |  0:00:06s
epoch 23 | loss: 0.78908 | val_0_rmse: 0.8937  | val_1_rmse: 0.81128 |  0:00:07s
epoch 24 | loss: 0.79231 | val_0_rmse: 0.8977  | val_1_rmse: 0.81062 |  0:00:07s
epoch 25 | loss: 0.78577 | val_0_rmse: 0.89476 | val_1_rmse: 0.82206 |  0:00:07s
epoch 26 | loss: 0.78144 | val_0_rmse: 0.88052 | val_1_rmse: 0.80288 |  0:00:07s
epoch 27 | loss: 0.78673 | val_0_rmse: 0.88742 | val_1_rmse: 0.80386 |  0:00:08s
epoch 28 | loss: 0.78222 | val_0_rmse: 0.88254 | val_1_rmse: 0.81032 |  0:00:08s
epoch 29 | loss: 0.77763 | val_0_rmse: 0.88614 | val_1_rmse: 0.81341 |  0:00:08s
epoch 30 | loss: 0.76631 | val_0_rmse: 0.88028 | val_1_rmse: 0.80087 |  0:00:09s
epoch 31 | loss: 0.75915 | val_0_rmse: 0.87935 | val_1_rmse: 0.80065 |  0:00:09s
epoch 32 | loss: 0.77263 | val_0_rmse: 0.89032 | val_1_rmse: 0.82077 |  0:00:09s
epoch 33 | loss: 0.76238 | val_0_rmse: 0.87978 | val_1_rmse: 0.80604 |  0:00:09s
epoch 34 | loss: 0.74831 | val_0_rmse: 0.875   | val_1_rmse: 0.79647 |  0:00:10s
epoch 35 | loss: 0.75034 | val_0_rmse: 0.8878  | val_1_rmse: 0.81633 |  0:00:10s
epoch 36 | loss: 0.7383  | val_0_rmse: 0.86779 | val_1_rmse: 0.7944  |  0:00:10s
epoch 37 | loss: 0.7325  | val_0_rmse: 0.86842 | val_1_rmse: 0.79718 |  0:00:11s
epoch 38 | loss: 0.70816 | val_0_rmse: 0.86196 | val_1_rmse: 0.78661 |  0:00:11s
epoch 39 | loss: 0.70461 | val_0_rmse: 0.85873 | val_1_rmse: 0.78225 |  0:00:11s
epoch 40 | loss: 0.71789 | val_0_rmse: 0.85501 | val_1_rmse: 0.78174 |  0:00:12s
epoch 41 | loss: 0.73855 | val_0_rmse: 0.87786 | val_1_rmse: 0.80838 |  0:00:12s
epoch 42 | loss: 0.75156 | val_0_rmse: 0.87078 | val_1_rmse: 0.80282 |  0:00:12s
epoch 43 | loss: 0.74565 | val_0_rmse: 0.84933 | val_1_rmse: 0.77506 |  0:00:12s
epoch 44 | loss: 0.71884 | val_0_rmse: 0.87554 | val_1_rmse: 0.81062 |  0:00:13s
epoch 45 | loss: 0.76953 | val_0_rmse: 0.88394 | val_1_rmse: 0.80902 |  0:00:13s
epoch 46 | loss: 0.73619 | val_0_rmse: 0.88028 | val_1_rmse: 0.80504 |  0:00:13s
epoch 47 | loss: 0.73023 | val_0_rmse: 0.88086 | val_1_rmse: 0.80592 |  0:00:14s
epoch 48 | loss: 0.73475 | val_0_rmse: 0.87106 | val_1_rmse: 0.8012  |  0:00:14s
epoch 49 | loss: 0.71465 | val_0_rmse: 0.86004 | val_1_rmse: 0.79413 |  0:00:14s
epoch 50 | loss: 0.71662 | val_0_rmse: 0.85287 | val_1_rmse: 0.7834  |  0:00:14s
epoch 51 | loss: 0.71903 | val_0_rmse: 0.85892 | val_1_rmse: 0.78482 |  0:00:15s
epoch 52 | loss: 0.72301 | val_0_rmse: 0.86778 | val_1_rmse: 0.79846 |  0:00:15s
epoch 53 | loss: 0.70989 | val_0_rmse: 0.85723 | val_1_rmse: 0.7806  |  0:00:15s
epoch 54 | loss: 0.72425 | val_0_rmse: 0.88562 | val_1_rmse: 0.8223  |  0:00:16s
epoch 55 | loss: 0.71866 | val_0_rmse: 0.86123 | val_1_rmse: 0.78976 |  0:00:16s
epoch 56 | loss: 0.72078 | val_0_rmse: 0.8764  | val_1_rmse: 0.81246 |  0:00:16s
epoch 57 | loss: 0.69634 | val_0_rmse: 0.87313 | val_1_rmse: 0.81163 |  0:00:17s
epoch 58 | loss: 0.7026  | val_0_rmse: 0.86484 | val_1_rmse: 0.80308 |  0:00:17s
epoch 59 | loss: 0.69156 | val_0_rmse: 0.89059 | val_1_rmse: 0.83829 |  0:00:17s
epoch 60 | loss: 0.69793 | val_0_rmse: 0.86435 | val_1_rmse: 0.80937 |  0:00:18s
epoch 61 | loss: 0.69334 | val_0_rmse: 0.86711 | val_1_rmse: 0.81422 |  0:00:18s
epoch 62 | loss: 0.67831 | val_0_rmse: 0.85339 | val_1_rmse: 0.79891 |  0:00:18s
epoch 63 | loss: 0.68847 | val_0_rmse: 0.8637  | val_1_rmse: 0.8119  |  0:00:18s
epoch 64 | loss: 0.68667 | val_0_rmse: 0.83864 | val_1_rmse: 0.7819  |  0:00:19s
epoch 65 | loss: 0.6778  | val_0_rmse: 0.8444  | val_1_rmse: 0.7909  |  0:00:19s
epoch 66 | loss: 0.67929 | val_0_rmse: 0.83863 | val_1_rmse: 0.77491 |  0:00:19s
epoch 67 | loss: 0.68587 | val_0_rmse: 0.8407  | val_1_rmse: 0.77476 |  0:00:20s
epoch 68 | loss: 0.67167 | val_0_rmse: 0.84113 | val_1_rmse: 0.78855 |  0:00:20s
epoch 69 | loss: 0.66464 | val_0_rmse: 0.83029 | val_1_rmse: 0.7753  |  0:00:20s
epoch 70 | loss: 0.66519 | val_0_rmse: 0.83464 | val_1_rmse: 0.77933 |  0:00:20s
epoch 71 | loss: 0.65429 | val_0_rmse: 0.82216 | val_1_rmse: 0.76246 |  0:00:21s
epoch 72 | loss: 0.64815 | val_0_rmse: 0.82423 | val_1_rmse: 0.76755 |  0:00:21s
epoch 73 | loss: 0.65069 | val_0_rmse: 0.82841 | val_1_rmse: 0.77224 |  0:00:21s
epoch 74 | loss: 0.64506 | val_0_rmse: 0.82473 | val_1_rmse: 0.76642 |  0:00:22s
epoch 75 | loss: 0.64283 | val_0_rmse: 0.82757 | val_1_rmse: 0.77143 |  0:00:22s
epoch 76 | loss: 0.64639 | val_0_rmse: 0.82215 | val_1_rmse: 0.76803 |  0:00:22s
epoch 77 | loss: 0.64688 | val_0_rmse: 0.81723 | val_1_rmse: 0.76769 |  0:00:22s
epoch 78 | loss: 0.642   | val_0_rmse: 0.81523 | val_1_rmse: 0.76478 |  0:00:23s
epoch 79 | loss: 0.62905 | val_0_rmse: 0.82543 | val_1_rmse: 0.78013 |  0:00:23s
epoch 80 | loss: 0.6316  | val_0_rmse: 0.80618 | val_1_rmse: 0.76012 |  0:00:23s
epoch 81 | loss: 0.63554 | val_0_rmse: 0.80769 | val_1_rmse: 0.75977 |  0:00:24s
epoch 82 | loss: 0.63126 | val_0_rmse: 0.8098  | val_1_rmse: 0.7592  |  0:00:24s
epoch 83 | loss: 0.62955 | val_0_rmse: 0.80773 | val_1_rmse: 0.76196 |  0:00:24s
epoch 84 | loss: 0.63064 | val_0_rmse: 0.8225  | val_1_rmse: 0.77874 |  0:00:25s
epoch 85 | loss: 0.62659 | val_0_rmse: 0.81481 | val_1_rmse: 0.77732 |  0:00:25s
epoch 86 | loss: 0.62988 | val_0_rmse: 0.80653 | val_1_rmse: 0.7635  |  0:00:25s
epoch 87 | loss: 0.63241 | val_0_rmse: 0.82054 | val_1_rmse: 0.782   |  0:00:25s
epoch 88 | loss: 0.62575 | val_0_rmse: 0.7939  | val_1_rmse: 0.75133 |  0:00:26s
epoch 89 | loss: 0.63105 | val_0_rmse: 0.81772 | val_1_rmse: 0.79805 |  0:00:26s
epoch 90 | loss: 0.61619 | val_0_rmse: 0.7991  | val_1_rmse: 0.77209 |  0:00:26s
epoch 91 | loss: 0.61493 | val_0_rmse: 0.7886  | val_1_rmse: 0.75968 |  0:00:27s
epoch 92 | loss: 0.62061 | val_0_rmse: 0.79349 | val_1_rmse: 0.76712 |  0:00:27s
epoch 93 | loss: 0.61144 | val_0_rmse: 0.79347 | val_1_rmse: 0.77094 |  0:00:27s
epoch 94 | loss: 0.61303 | val_0_rmse: 0.79353 | val_1_rmse: 0.77636 |  0:00:27s
epoch 95 | loss: 0.61156 | val_0_rmse: 0.78445 | val_1_rmse: 0.76047 |  0:00:28s
epoch 96 | loss: 0.61361 | val_0_rmse: 0.79944 | val_1_rmse: 0.77944 |  0:00:28s
epoch 97 | loss: 0.59839 | val_0_rmse: 0.77797 | val_1_rmse: 0.76147 |  0:00:28s
epoch 98 | loss: 0.60105 | val_0_rmse: 0.7936  | val_1_rmse: 0.78118 |  0:00:29s
epoch 99 | loss: 0.60855 | val_0_rmse: 0.78012 | val_1_rmse: 0.76178 |  0:00:29s
epoch 100| loss: 0.60946 | val_0_rmse: 0.79195 | val_1_rmse: 0.77039 |  0:00:29s
epoch 101| loss: 0.60387 | val_0_rmse: 0.80378 | val_1_rmse: 0.77827 |  0:00:29s
epoch 102| loss: 0.6106  | val_0_rmse: 0.79913 | val_1_rmse: 0.77    |  0:00:30s
epoch 103| loss: 0.60898 | val_0_rmse: 0.78218 | val_1_rmse: 0.75833 |  0:00:30s
epoch 104| loss: 0.60283 | val_0_rmse: 0.78035 | val_1_rmse: 0.75781 |  0:00:30s
epoch 105| loss: 0.60031 | val_0_rmse: 0.77552 | val_1_rmse: 0.75509 |  0:00:31s
epoch 106| loss: 0.60264 | val_0_rmse: 0.77392 | val_1_rmse: 0.75695 |  0:00:31s
epoch 107| loss: 0.60054 | val_0_rmse: 0.77788 | val_1_rmse: 0.77064 |  0:00:31s
epoch 108| loss: 0.6095  | val_0_rmse: 0.77674 | val_1_rmse: 0.77348 |  0:00:31s
epoch 109| loss: 0.60757 | val_0_rmse: 0.77104 | val_1_rmse: 0.76413 |  0:00:32s
epoch 110| loss: 0.6027  | val_0_rmse: 0.7703  | val_1_rmse: 0.7719  |  0:00:32s
epoch 111| loss: 0.59955 | val_0_rmse: 0.75892 | val_1_rmse: 0.7548  |  0:00:32s
epoch 112| loss: 0.59309 | val_0_rmse: 0.79106 | val_1_rmse: 0.80616 |  0:00:33s
epoch 113| loss: 0.59054 | val_0_rmse: 0.77436 | val_1_rmse: 0.77944 |  0:00:33s
epoch 114| loss: 0.5804  | val_0_rmse: 0.75367 | val_1_rmse: 0.76372 |  0:00:33s
epoch 115| loss: 0.58804 | val_0_rmse: 0.76438 | val_1_rmse: 0.76916 |  0:00:34s
epoch 116| loss: 0.59004 | val_0_rmse: 0.77025 | val_1_rmse: 0.76919 |  0:00:34s
epoch 117| loss: 0.58374 | val_0_rmse: 0.81877 | val_1_rmse: 0.8001  |  0:00:34s
epoch 118| loss: 0.57509 | val_0_rmse: 0.7834  | val_1_rmse: 0.76871 |  0:00:34s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.75133
Best weights from best epoch are automatically used!
ended training at: 00:58:49
Feature importance:
Mean squared error is of 0.06001864869829813
Mean absolute error:0.17390271599094953
MAPE:0.19053599754813313
R2 score:0.3189891019352733
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:59:05
epoch 0  | loss: 1.04852 | val_0_rmse: 0.99893 | val_1_rmse: 0.98859 |  0:00:21s
epoch 1  | loss: 0.99781 | val_0_rmse: 0.9953  | val_1_rmse: 0.98504 |  0:00:43s
epoch 2  | loss: 0.84311 | val_0_rmse: 0.85999 | val_1_rmse: 0.85932 |  0:01:04s
epoch 3  | loss: 0.67968 | val_0_rmse: 0.81229 | val_1_rmse: 0.80708 |  0:01:26s
epoch 4  | loss: 0.63465 | val_0_rmse: 0.75838 | val_1_rmse: 0.7562  |  0:01:47s
epoch 5  | loss: 0.60098 | val_0_rmse: 0.77769 | val_1_rmse: 0.8021  |  0:02:09s
epoch 6  | loss: 0.59766 | val_0_rmse: 0.74956 | val_1_rmse: 0.76761 |  0:02:31s
epoch 7  | loss: 0.56863 | val_0_rmse: 0.86776 | val_1_rmse: 0.86179 |  0:02:52s
epoch 8  | loss: 0.55807 | val_0_rmse: 0.77918 | val_1_rmse: 0.7907  |  0:03:14s
epoch 9  | loss: 0.54003 | val_0_rmse: 0.77497 | val_1_rmse: 0.82912 |  0:03:35s
epoch 10 | loss: 0.52324 | val_0_rmse: 0.74068 | val_1_rmse: 0.7864  |  0:03:57s
epoch 11 | loss: 0.51093 | val_0_rmse: 0.76153 | val_1_rmse: 0.78689 |  0:04:19s
epoch 12 | loss: 0.50494 | val_0_rmse: 0.72068 | val_1_rmse: 0.76984 |  0:04:40s
epoch 13 | loss: 0.49556 | val_0_rmse: 0.71068 | val_1_rmse: 0.77261 |  0:05:02s
epoch 14 | loss: 0.48485 | val_0_rmse: 0.783   | val_1_rmse: 0.82196 |  0:05:23s
epoch 15 | loss: 0.48497 | val_0_rmse: 0.70638 | val_1_rmse: 0.77418 |  0:05:45s
epoch 16 | loss: 0.4728  | val_0_rmse: 0.70211 | val_1_rmse: 0.75743 |  0:06:07s
epoch 17 | loss: 0.47656 | val_0_rmse: 1.32498 | val_1_rmse: 0.75988 |  0:06:28s
epoch 18 | loss: 0.46713 | val_0_rmse: 0.73889 | val_1_rmse: 0.79936 |  0:06:50s
epoch 19 | loss: 0.4597  | val_0_rmse: 0.71087 | val_1_rmse: 0.76185 |  0:07:11s
epoch 20 | loss: 0.45349 | val_0_rmse: 0.76491 | val_1_rmse: 0.81992 |  0:07:34s
epoch 21 | loss: 0.4486  | val_0_rmse: 0.68263 | val_1_rmse: 0.74659 |  0:07:55s
epoch 22 | loss: 0.44973 | val_0_rmse: 0.70503 | val_1_rmse: 0.79325 |  0:08:17s
epoch 23 | loss: 0.43886 | val_0_rmse: 0.68319 | val_1_rmse: 0.75485 |  0:08:38s
epoch 24 | loss: 0.44061 | val_0_rmse: 0.69599 | val_1_rmse: 0.78587 |  0:09:00s
epoch 25 | loss: 0.43388 | val_0_rmse: 0.67249 | val_1_rmse: 0.75119 |  0:09:21s
epoch 26 | loss: 0.42661 | val_0_rmse: 0.6712  | val_1_rmse: 0.73525 |  0:09:43s
epoch 27 | loss: 0.4287  | val_0_rmse: 0.67216 | val_1_rmse: 0.73214 |  0:10:05s
epoch 28 | loss: 0.42461 | val_0_rmse: 0.71571 | val_1_rmse: 0.79095 |  0:10:26s
epoch 29 | loss: 0.41713 | val_0_rmse: 0.72045 | val_1_rmse: 0.79746 |  0:10:48s
epoch 30 | loss: 0.41546 | val_0_rmse: 0.86136 | val_1_rmse: 0.87167 |  0:11:09s
epoch 31 | loss: 0.41817 | val_0_rmse: 0.67037 | val_1_rmse: 0.77008 |  0:11:31s
epoch 32 | loss: 0.41882 | val_0_rmse: 0.65907 | val_1_rmse: 0.72615 |  0:11:52s
epoch 33 | loss: 0.41389 | val_0_rmse: 0.69637 | val_1_rmse: 0.75683 |  0:12:14s
epoch 34 | loss: 0.40331 | val_0_rmse: 0.65937 | val_1_rmse: 0.76471 |  0:12:35s
epoch 35 | loss: 0.40619 | val_0_rmse: 0.66046 | val_1_rmse: 0.74959 |  0:12:57s
epoch 36 | loss: 0.40585 | val_0_rmse: 0.69841 | val_1_rmse: 0.8156  |  0:13:19s
epoch 37 | loss: 0.40142 | val_0_rmse: 0.64893 | val_1_rmse: 0.72538 |  0:13:40s
epoch 38 | loss: 0.39868 | val_0_rmse: 0.73324 | val_1_rmse: 0.85152 |  0:14:02s
epoch 39 | loss: 0.39253 | val_0_rmse: 0.66811 | val_1_rmse: 0.78744 |  0:14:23s
epoch 40 | loss: 0.39229 | val_0_rmse: 0.65851 | val_1_rmse: 0.76905 |  0:14:45s
epoch 41 | loss: 0.38952 | val_0_rmse: 0.65088 | val_1_rmse: 0.73324 |  0:15:06s
epoch 42 | loss: 0.38804 | val_0_rmse: 0.6386  | val_1_rmse: 0.74981 |  0:15:28s
epoch 43 | loss: 0.38812 | val_0_rmse: 0.6507  | val_1_rmse: 0.73086 |  0:15:49s
epoch 44 | loss: 0.40013 | val_0_rmse: 0.71207 | val_1_rmse: 0.82928 |  0:16:11s
epoch 45 | loss: 0.38897 | val_0_rmse: 0.65665 | val_1_rmse: 0.73739 |  0:16:33s
epoch 46 | loss: 0.38741 | val_0_rmse: 0.73415 | val_1_rmse: 0.77872 |  0:16:54s
epoch 47 | loss: 0.41271 | val_0_rmse: 1.66397 | val_1_rmse: 0.8902  |  0:17:16s
epoch 48 | loss: 0.45223 | val_0_rmse: 0.67693 | val_1_rmse: 0.74656 |  0:17:38s
epoch 49 | loss: 0.40726 | val_0_rmse: 2.16588 | val_1_rmse: 0.72757 |  0:17:59s
epoch 50 | loss: 0.39358 | val_0_rmse: 1.53923 | val_1_rmse: 1.0751  |  0:18:21s
epoch 51 | loss: 0.38212 | val_0_rmse: 2.08255 | val_1_rmse: 1.93482 |  0:18:42s
epoch 52 | loss: 0.38231 | val_0_rmse: 0.69108 | val_1_rmse: 0.76664 |  0:19:04s
epoch 53 | loss: 0.37847 | val_0_rmse: 0.63335 | val_1_rmse: 0.75258 |  0:19:26s
epoch 54 | loss: 0.37505 | val_0_rmse: 0.75649 | val_1_rmse: 0.80815 |  0:19:47s
epoch 55 | loss: 0.37801 | val_0_rmse: 0.79037 | val_1_rmse: 0.90183 |  0:20:09s
epoch 56 | loss: 0.36968 | val_0_rmse: 0.74442 | val_1_rmse: 0.7643  |  0:20:31s
epoch 57 | loss: 0.37138 | val_0_rmse: 0.64423 | val_1_rmse: 0.75936 |  0:20:52s
epoch 58 | loss: 0.36833 | val_0_rmse: 0.6584  | val_1_rmse: 0.75797 |  0:21:14s
epoch 59 | loss: 0.37    | val_0_rmse: 0.76432 | val_1_rmse: 0.81634 |  0:21:35s
epoch 60 | loss: 0.37039 | val_0_rmse: 0.78095 | val_1_rmse: 0.81868 |  0:21:57s
epoch 61 | loss: 0.36558 | val_0_rmse: 0.664   | val_1_rmse: 0.75554 |  0:22:19s
epoch 62 | loss: 0.36378 | val_0_rmse: 0.66384 | val_1_rmse: 0.75792 |  0:22:41s
epoch 63 | loss: 0.35926 | val_0_rmse: 0.73895 | val_1_rmse: 0.80089 |  0:23:03s
epoch 64 | loss: 0.36192 | val_0_rmse: 0.69578 | val_1_rmse: 0.85697 |  0:23:24s
epoch 65 | loss: 0.35955 | val_0_rmse: 0.63239 | val_1_rmse: 0.76994 |  0:23:46s
epoch 66 | loss: 0.36118 | val_0_rmse: 0.62893 | val_1_rmse: 0.75418 |  0:24:08s
epoch 67 | loss: 0.3569  | val_0_rmse: 0.6374  | val_1_rmse: 0.74927 |  0:24:29s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.72538
Best weights from best epoch are automatically used!
ended training at: 01:23:47
Feature importance:
Mean squared error is of 0.06769731113796197
Mean absolute error:0.17735251499630922
MAPE:0.19893677004395102
R2 score:0.4682130920115868
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:23:59
epoch 0  | loss: 0.96961 | val_0_rmse: 0.88764 | val_1_rmse: 0.90934 |  0:00:21s
epoch 1  | loss: 0.72191 | val_0_rmse: 0.82482 | val_1_rmse: 0.84545 |  0:00:43s
epoch 2  | loss: 0.64769 | val_0_rmse: 0.79783 | val_1_rmse: 0.81869 |  0:01:04s
epoch 3  | loss: 0.60086 | val_0_rmse: 0.7622  | val_1_rmse: 0.78057 |  0:01:26s
epoch 4  | loss: 0.58069 | val_0_rmse: 0.76516 | val_1_rmse: 0.78152 |  0:01:47s
epoch 5  | loss: 0.55586 | val_0_rmse: 0.73043 | val_1_rmse: 0.75737 |  0:02:08s
epoch 6  | loss: 0.5507  | val_0_rmse: 0.75182 | val_1_rmse: 0.79436 |  0:02:30s
epoch 7  | loss: 0.54331 | val_0_rmse: 0.71585 | val_1_rmse: 0.75764 |  0:02:51s
epoch 8  | loss: 0.53543 | val_0_rmse: 0.74465 | val_1_rmse: 0.76167 |  0:03:13s
epoch 9  | loss: 0.5218  | val_0_rmse: 0.71512 | val_1_rmse: 0.74302 |  0:03:34s
epoch 10 | loss: 0.51993 | val_0_rmse: 0.71985 | val_1_rmse: 0.74875 |  0:03:55s
epoch 11 | loss: 0.50386 | val_0_rmse: 0.71997 | val_1_rmse: 0.74546 |  0:04:17s
epoch 12 | loss: 0.49549 | val_0_rmse: 0.77103 | val_1_rmse: 0.79868 |  0:04:38s
epoch 13 | loss: 0.49641 | val_0_rmse: 0.72658 | val_1_rmse: 0.7466  |  0:05:00s
epoch 14 | loss: 0.49984 | val_0_rmse: 0.73252 | val_1_rmse: 0.7722  |  0:05:21s
epoch 15 | loss: 0.48599 | val_0_rmse: 0.72327 | val_1_rmse: 0.74258 |  0:05:42s
epoch 16 | loss: 0.47172 | val_0_rmse: 0.71515 | val_1_rmse: 0.73128 |  0:06:04s
epoch 17 | loss: 0.4702  | val_0_rmse: 0.76209 | val_1_rmse: 0.76213 |  0:06:25s
epoch 18 | loss: 0.47717 | val_0_rmse: 0.74392 | val_1_rmse: 0.75888 |  0:06:46s
epoch 19 | loss: 0.45968 | val_0_rmse: 0.82924 | val_1_rmse: 0.79513 |  0:07:08s
epoch 20 | loss: 0.46275 | val_0_rmse: 0.98657 | val_1_rmse: 0.95228 |  0:07:29s
epoch 21 | loss: 0.47174 | val_0_rmse: 0.76061 | val_1_rmse: 0.81242 |  0:07:51s
epoch 22 | loss: 0.47057 | val_0_rmse: 0.70054 | val_1_rmse: 0.74627 |  0:08:12s
epoch 23 | loss: 0.44829 | val_0_rmse: 0.86923 | val_1_rmse: 0.72343 |  0:08:33s
epoch 24 | loss: 0.46425 | val_0_rmse: 0.80977 | val_1_rmse: 0.83514 |  0:08:55s
epoch 25 | loss: 0.45553 | val_0_rmse: 0.73405 | val_1_rmse: 0.78819 |  0:09:16s
epoch 26 | loss: 0.44732 | val_0_rmse: 0.70704 | val_1_rmse: 0.75767 |  0:09:38s
epoch 27 | loss: 0.43437 | val_0_rmse: 0.7252  | val_1_rmse: 0.77099 |  0:09:59s
epoch 28 | loss: 0.42656 | val_0_rmse: 0.68579 | val_1_rmse: 0.73808 |  0:10:21s
epoch 29 | loss: 0.41741 | val_0_rmse: 0.70971 | val_1_rmse: 0.7583  |  0:10:42s
epoch 30 | loss: 0.41723 | val_0_rmse: 0.68923 | val_1_rmse: 0.75801 |  0:11:03s
epoch 31 | loss: 0.41069 | val_0_rmse: 0.79793 | val_1_rmse: 0.86747 |  0:11:25s
epoch 32 | loss: 0.41039 | val_0_rmse: 0.66568 | val_1_rmse: 0.7279  |  0:11:46s
epoch 33 | loss: 0.41075 | val_0_rmse: 0.76318 | val_1_rmse: 0.79003 |  0:12:08s
epoch 34 | loss: 0.40311 | val_0_rmse: 0.69727 | val_1_rmse: 0.73697 |  0:12:29s
epoch 35 | loss: 0.40102 | val_0_rmse: 0.67517 | val_1_rmse: 0.73705 |  0:12:52s
epoch 36 | loss: 0.39285 | val_0_rmse: 0.66582 | val_1_rmse: 0.75455 |  0:13:13s
epoch 37 | loss: 0.3995  | val_0_rmse: 0.72095 | val_1_rmse: 0.77944 |  0:13:35s
epoch 38 | loss: 0.41109 | val_0_rmse: 0.70248 | val_1_rmse: 0.7716  |  0:13:56s
epoch 39 | loss: 0.39846 | val_0_rmse: 0.67368 | val_1_rmse: 0.73512 |  0:14:18s
epoch 40 | loss: 0.38788 | val_0_rmse: 0.66397 | val_1_rmse: 0.73747 |  0:14:39s
epoch 41 | loss: 0.38838 | val_0_rmse: 0.65253 | val_1_rmse: 0.7448  |  0:15:01s
epoch 42 | loss: 0.39254 | val_0_rmse: 0.66468 | val_1_rmse: 0.76516 |  0:15:22s
epoch 43 | loss: 0.38241 | val_0_rmse: 0.75388 | val_1_rmse: 0.81183 |  0:15:44s
epoch 44 | loss: 0.38703 | val_0_rmse: 0.6534  | val_1_rmse: 0.7297  |  0:16:05s
epoch 45 | loss: 0.37997 | val_0_rmse: 0.66535 | val_1_rmse: 0.74051 |  0:16:27s
epoch 46 | loss: 0.37684 | val_0_rmse: 0.67033 | val_1_rmse: 0.75479 |  0:16:48s
epoch 47 | loss: 0.3739  | val_0_rmse: 0.66753 | val_1_rmse: 0.74995 |  0:17:10s
epoch 48 | loss: 0.39407 | val_0_rmse: 0.66122 | val_1_rmse: 0.72208 |  0:17:32s
epoch 49 | loss: 0.38672 | val_0_rmse: 0.64651 | val_1_rmse: 0.73764 |  0:17:54s
epoch 50 | loss: 0.37669 | val_0_rmse: 0.67138 | val_1_rmse: 0.74923 |  0:18:15s
epoch 51 | loss: 0.37157 | val_0_rmse: 0.66509 | val_1_rmse: 0.77343 |  0:18:37s
epoch 52 | loss: 0.36923 | val_0_rmse: 0.68745 | val_1_rmse: 0.80779 |  0:18:59s
epoch 53 | loss: 0.36775 | val_0_rmse: 0.6937  | val_1_rmse: 0.79794 |  0:19:20s
epoch 54 | loss: 0.37234 | val_0_rmse: 1.25207 | val_1_rmse: 1.36456 |  0:19:42s
epoch 55 | loss: 0.36853 | val_0_rmse: 0.63117 | val_1_rmse: 0.74249 |  0:20:03s
epoch 56 | loss: 0.3621  | val_0_rmse: 0.63791 | val_1_rmse: 0.73739 |  0:20:24s
epoch 57 | loss: 0.36048 | val_0_rmse: 0.63841 | val_1_rmse: 0.75153 |  0:20:46s
epoch 58 | loss: 0.35987 | val_0_rmse: 0.62791 | val_1_rmse: 0.73588 |  0:21:07s
epoch 59 | loss: 0.35604 | val_0_rmse: 0.64791 | val_1_rmse: 0.74589 |  0:21:29s
epoch 60 | loss: 0.35494 | val_0_rmse: 0.64147 | val_1_rmse: 0.75379 |  0:21:50s
epoch 61 | loss: 0.35349 | val_0_rmse: 0.63911 | val_1_rmse: 0.73911 |  0:22:12s
epoch 62 | loss: 0.37383 | val_0_rmse: 0.69345 | val_1_rmse: 0.76742 |  0:22:33s
epoch 63 | loss: 0.3591  | val_0_rmse: 0.64942 | val_1_rmse: 0.78596 |  0:22:55s
epoch 64 | loss: 0.36094 | val_0_rmse: 0.66036 | val_1_rmse: 0.75298 |  0:23:16s
epoch 65 | loss: 0.36097 | val_0_rmse: 0.83274 | val_1_rmse: 0.9674  |  0:23:38s
epoch 66 | loss: 0.35386 | val_0_rmse: 0.72674 | val_1_rmse: 0.79709 |  0:23:59s
epoch 67 | loss: 0.35385 | val_0_rmse: 0.62621 | val_1_rmse: 0.73602 |  0:24:21s
epoch 68 | loss: 0.34622 | val_0_rmse: 0.6254  | val_1_rmse: 0.73226 |  0:24:42s
epoch 69 | loss: 0.34868 | val_0_rmse: 0.65785 | val_1_rmse: 0.76021 |  0:25:04s
epoch 70 | loss: 0.35014 | val_0_rmse: 0.77343 | val_1_rmse: 0.83062 |  0:25:25s
epoch 71 | loss: 0.36645 | val_0_rmse: 0.79547 | val_1_rmse: 0.87139 |  0:25:47s
epoch 72 | loss: 0.36835 | val_0_rmse: 0.63141 | val_1_rmse: 0.75228 |  0:26:08s
epoch 73 | loss: 0.34882 | val_0_rmse: 0.61256 | val_1_rmse: 0.73564 |  0:26:30s
epoch 74 | loss: 0.34442 | val_0_rmse: 0.63309 | val_1_rmse: 0.76055 |  0:26:51s
epoch 75 | loss: 0.34472 | val_0_rmse: 0.63009 | val_1_rmse: 0.7675  |  0:27:12s
epoch 76 | loss: 0.33832 | val_0_rmse: 0.63354 | val_1_rmse: 0.74787 |  0:27:34s
epoch 77 | loss: 0.34063 | val_0_rmse: 0.61978 | val_1_rmse: 0.74866 |  0:27:56s
epoch 78 | loss: 0.34542 | val_0_rmse: 0.66576 | val_1_rmse: 0.7513  |  0:28:17s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.72208
Best weights from best epoch are automatically used!
ended training at: 01:52:28
Feature importance:
Mean squared error is of 0.0742676957872983
Mean absolute error:0.18196926216404494
MAPE:0.20137892298674936
R2 score:0.4345411553324665
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:52:41
epoch 0  | loss: 1.01047 | val_0_rmse: 0.93054 | val_1_rmse: 0.96015 |  0:00:21s
epoch 1  | loss: 0.78785 | val_0_rmse: 0.83171 | val_1_rmse: 0.86769 |  0:00:43s
epoch 2  | loss: 0.66778 | val_0_rmse: 0.80747 | val_1_rmse: 0.84069 |  0:01:04s
epoch 3  | loss: 0.61888 | val_0_rmse: 0.77796 | val_1_rmse: 0.81555 |  0:01:26s
epoch 4  | loss: 0.58891 | val_0_rmse: 0.76198 | val_1_rmse: 0.79869 |  0:01:47s
epoch 5  | loss: 0.58772 | val_0_rmse: 0.73743 | val_1_rmse: 0.78302 |  0:02:08s
epoch 6  | loss: 0.56858 | val_0_rmse: 0.7126  | val_1_rmse: 0.77006 |  0:02:30s
epoch 7  | loss: 0.54293 | val_0_rmse: 0.73318 | val_1_rmse: 0.79953 |  0:02:51s
epoch 8  | loss: 0.54003 | val_0_rmse: 0.71604 | val_1_rmse: 0.77635 |  0:03:13s
epoch 9  | loss: 0.53159 | val_0_rmse: 0.72759 | val_1_rmse: 0.80845 |  0:03:34s
epoch 10 | loss: 0.53489 | val_0_rmse: 0.7167  | val_1_rmse: 0.78706 |  0:03:56s
epoch 11 | loss: 0.52754 | val_0_rmse: 0.7463  | val_1_rmse: 0.8336  |  0:04:17s
epoch 12 | loss: 0.50857 | val_0_rmse: 0.69839 | val_1_rmse: 0.77909 |  0:04:39s
epoch 13 | loss: 0.50205 | val_0_rmse: 0.72823 | val_1_rmse: 0.79851 |  0:05:00s
epoch 14 | loss: 0.50221 | val_0_rmse: 0.73268 | val_1_rmse: 0.82282 |  0:05:22s
epoch 15 | loss: 0.49187 | val_0_rmse: 0.70637 | val_1_rmse: 0.77363 |  0:05:43s
epoch 16 | loss: 0.48296 | val_0_rmse: 0.69969 | val_1_rmse: 0.79283 |  0:06:05s
epoch 17 | loss: 0.47787 | val_0_rmse: 0.73889 | val_1_rmse: 0.83387 |  0:06:26s
epoch 18 | loss: 0.46794 | val_0_rmse: 0.72368 | val_1_rmse: 0.8053  |  0:06:48s
epoch 19 | loss: 0.47524 | val_0_rmse: 0.68247 | val_1_rmse: 0.76593 |  0:07:09s
epoch 20 | loss: 0.45747 | val_0_rmse: 0.7356  | val_1_rmse: 0.76383 |  0:07:31s
epoch 21 | loss: 0.47115 | val_0_rmse: 0.7057  | val_1_rmse: 0.78427 |  0:07:52s
epoch 22 | loss: 0.45372 | val_0_rmse: 0.6866  | val_1_rmse: 0.76842 |  0:08:14s
epoch 23 | loss: 0.45384 | val_0_rmse: 0.72031 | val_1_rmse: 0.81446 |  0:08:35s
epoch 24 | loss: 0.46059 | val_0_rmse: 0.68743 | val_1_rmse: 0.77798 |  0:08:57s
epoch 25 | loss: 0.45048 | val_0_rmse: 0.67346 | val_1_rmse: 0.75435 |  0:09:18s
epoch 26 | loss: 0.44755 | val_0_rmse: 0.68095 | val_1_rmse: 0.76768 |  0:09:40s
epoch 27 | loss: 0.43613 | val_0_rmse: 0.66695 | val_1_rmse: 0.77195 |  0:10:01s
epoch 28 | loss: 0.43484 | val_0_rmse: 0.68133 | val_1_rmse: 0.77593 |  0:10:23s
epoch 29 | loss: 0.43228 | val_0_rmse: 0.65648 | val_1_rmse: 0.7453  |  0:10:44s
epoch 30 | loss: 0.43125 | val_0_rmse: 0.67277 | val_1_rmse: 0.76355 |  0:11:06s
epoch 31 | loss: 0.42273 | val_0_rmse: 0.66022 | val_1_rmse: 0.75599 |  0:11:27s
epoch 32 | loss: 0.42025 | val_0_rmse: 0.6597  | val_1_rmse: 0.75184 |  0:11:49s
epoch 33 | loss: 0.41751 | val_0_rmse: 0.65728 | val_1_rmse: 0.76351 |  0:12:10s
epoch 34 | loss: 0.42793 | val_0_rmse: 0.68593 | val_1_rmse: 0.79345 |  0:12:32s
epoch 35 | loss: 0.42772 | val_0_rmse: 0.65702 | val_1_rmse: 0.75391 |  0:12:53s
epoch 36 | loss: 0.42668 | val_0_rmse: 0.66983 | val_1_rmse: 0.77762 |  0:13:15s
epoch 37 | loss: 0.40903 | val_0_rmse: 0.64362 | val_1_rmse: 0.7599  |  0:13:36s
epoch 38 | loss: 0.40804 | val_0_rmse: 0.65072 | val_1_rmse: 0.75693 |  0:13:58s
epoch 39 | loss: 0.40069 | val_0_rmse: 0.64726 | val_1_rmse: 0.75547 |  0:14:20s
epoch 40 | loss: 0.40454 | val_0_rmse: 0.6553  | val_1_rmse: 0.77705 |  0:14:42s
epoch 41 | loss: 0.39781 | val_0_rmse: 0.65375 | val_1_rmse: 0.77986 |  0:15:04s
epoch 42 | loss: 0.40035 | val_0_rmse: 0.64153 | val_1_rmse: 0.76288 |  0:15:25s
epoch 43 | loss: 0.39354 | val_0_rmse: 0.67005 | val_1_rmse: 0.79126 |  0:15:47s
epoch 44 | loss: 0.39265 | val_0_rmse: 0.64208 | val_1_rmse: 0.7648  |  0:16:08s
epoch 45 | loss: 0.38838 | val_0_rmse: 0.65072 | val_1_rmse: 0.77838 |  0:16:30s
epoch 46 | loss: 0.38965 | val_0_rmse: 0.67184 | val_1_rmse: 0.76679 |  0:16:52s
epoch 47 | loss: 0.3867  | val_0_rmse: 0.63182 | val_1_rmse: 0.76063 |  0:17:13s
epoch 48 | loss: 0.38478 | val_0_rmse: 0.67162 | val_1_rmse: 0.81343 |  0:17:35s
epoch 49 | loss: 0.38798 | val_0_rmse: 0.66611 | val_1_rmse: 0.7987  |  0:17:57s
epoch 50 | loss: 0.38148 | val_0_rmse: 0.67691 | val_1_rmse: 0.82923 |  0:18:19s
epoch 51 | loss: 0.381   | val_0_rmse: 0.6496  | val_1_rmse: 0.7867  |  0:18:40s
epoch 52 | loss: 0.38305 | val_0_rmse: 0.67476 | val_1_rmse: 0.8192  |  0:19:02s
epoch 53 | loss: 0.38247 | val_0_rmse: 0.64166 | val_1_rmse: 0.79027 |  0:19:24s
epoch 54 | loss: 0.3813  | val_0_rmse: 0.64378 | val_1_rmse: 0.77328 |  0:19:45s
epoch 55 | loss: 0.37256 | val_0_rmse: 0.63934 | val_1_rmse: 0.76126 |  0:20:07s
epoch 56 | loss: 0.36895 | val_0_rmse: 0.63316 | val_1_rmse: 0.76791 |  0:20:29s
epoch 57 | loss: 0.37376 | val_0_rmse: 0.62812 | val_1_rmse: 0.7619  |  0:20:51s
epoch 58 | loss: 0.36555 | val_0_rmse: 0.65233 | val_1_rmse: 0.77959 |  0:21:12s
epoch 59 | loss: 0.36949 | val_0_rmse: 0.65816 | val_1_rmse: 0.79835 |  0:21:34s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.7453
Best weights from best epoch are automatically used!
ended training at: 02:14:27
Feature importance:
Mean squared error is of 0.455178917532755
Mean absolute error:0.1856887417580794
MAPE:0.20354023833180396
R2 score:-2.65713852972026
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:14:39
epoch 0  | loss: 1.05341 | val_0_rmse: 1.00277 | val_1_rmse: 0.96802 |  0:00:21s
epoch 1  | loss: 1.00533 | val_0_rmse: 1.00169 | val_1_rmse: 0.96696 |  0:00:43s
epoch 2  | loss: 0.89822 | val_0_rmse: 0.84211 | val_1_rmse: 0.81834 |  0:01:04s
epoch 3  | loss: 0.72428 | val_0_rmse: 0.86369 | val_1_rmse: 0.85557 |  0:01:25s
epoch 4  | loss: 0.64393 | val_0_rmse: 0.76317 | val_1_rmse: 0.75481 |  0:01:47s
epoch 5  | loss: 0.57576 | val_0_rmse: 0.76165 | val_1_rmse: 0.76529 |  0:02:08s
epoch 6  | loss: 0.56228 | val_0_rmse: 0.79403 | val_1_rmse: 0.79674 |  0:02:29s
epoch 7  | loss: 0.55733 | val_0_rmse: 0.73606 | val_1_rmse: 0.7583  |  0:02:51s
epoch 8  | loss: 0.55157 | val_0_rmse: 0.726   | val_1_rmse: 0.74696 |  0:03:12s
epoch 9  | loss: 0.52146 | val_0_rmse: 0.73813 | val_1_rmse: 0.76686 |  0:03:33s
epoch 10 | loss: 0.50389 | val_0_rmse: 0.744   | val_1_rmse: 0.78728 |  0:03:55s
epoch 11 | loss: 0.49885 | val_0_rmse: 0.79718 | val_1_rmse: 0.78259 |  0:04:16s
epoch 12 | loss: 0.48639 | val_0_rmse: 1.12602 | val_1_rmse: 0.77053 |  0:04:38s
epoch 13 | loss: 0.49069 | val_0_rmse: 1.27649 | val_1_rmse: 0.90814 |  0:04:59s
epoch 14 | loss: 0.47731 | val_0_rmse: 0.81643 | val_1_rmse: 0.83686 |  0:05:21s
epoch 15 | loss: 0.47222 | val_0_rmse: 0.85994 | val_1_rmse: 0.76679 |  0:05:43s
epoch 16 | loss: 0.46867 | val_0_rmse: 0.96915 | val_1_rmse: 0.81721 |  0:06:04s
epoch 17 | loss: 0.45747 | val_0_rmse: 1.43218 | val_1_rmse: 0.81809 |  0:06:26s
epoch 18 | loss: 0.45521 | val_0_rmse: 0.83572 | val_1_rmse: 0.77821 |  0:06:47s
epoch 19 | loss: 0.44363 | val_0_rmse: 0.86672 | val_1_rmse: 0.82418 |  0:07:09s
epoch 20 | loss: 0.44298 | val_0_rmse: 0.7913  | val_1_rmse: 0.80141 |  0:07:31s
epoch 21 | loss: 0.4401  | val_0_rmse: 0.72266 | val_1_rmse: 0.76418 |  0:07:53s
epoch 22 | loss: 0.43797 | val_0_rmse: 0.72177 | val_1_rmse: 0.76007 |  0:08:14s
epoch 23 | loss: 0.42854 | val_0_rmse: 0.72149 | val_1_rmse: 0.7796  |  0:08:36s
epoch 24 | loss: 0.42689 | val_0_rmse: 0.69895 | val_1_rmse: 0.75315 |  0:08:57s
epoch 25 | loss: 0.4282  | val_0_rmse: 0.75716 | val_1_rmse: 0.82395 |  0:09:19s
epoch 26 | loss: 0.42136 | val_0_rmse: 0.73627 | val_1_rmse: 0.81161 |  0:09:40s
epoch 27 | loss: 0.42196 | val_0_rmse: 0.80195 | val_1_rmse: 0.81413 |  0:10:01s
epoch 28 | loss: 0.41858 | val_0_rmse: 1.02502 | val_1_rmse: 0.82854 |  0:10:23s
epoch 29 | loss: 0.41523 | val_0_rmse: 0.7372  | val_1_rmse: 0.7671  |  0:10:44s
epoch 30 | loss: 0.4153  | val_0_rmse: 0.705   | val_1_rmse: 0.78215 |  0:11:06s
epoch 31 | loss: 0.41225 | val_0_rmse: 0.74069 | val_1_rmse: 0.80235 |  0:11:27s
epoch 32 | loss: 0.40953 | val_0_rmse: 0.80358 | val_1_rmse: 0.92486 |  0:11:49s
epoch 33 | loss: 0.41032 | val_0_rmse: 0.82664 | val_1_rmse: 0.9434  |  0:12:11s
epoch 34 | loss: 0.40473 | val_0_rmse: 0.85157 | val_1_rmse: 0.86449 |  0:12:32s
epoch 35 | loss: 0.40136 | val_0_rmse: 1.03156 | val_1_rmse: 1.03153 |  0:12:54s
epoch 36 | loss: 0.40071 | val_0_rmse: 0.87896 | val_1_rmse: 0.87264 |  0:13:15s
epoch 37 | loss: 0.40076 | val_0_rmse: 0.85857 | val_1_rmse: 0.85536 |  0:13:37s
epoch 38 | loss: 0.39872 | val_0_rmse: 0.72417 | val_1_rmse: 0.81587 |  0:13:59s

Early stopping occured at epoch 38 with best_epoch = 8 and best_val_1_rmse = 0.74696
Best weights from best epoch are automatically used!
ended training at: 02:28:50
Feature importance:
Mean squared error is of 0.07492536163978851
Mean absolute error:0.1920887350466526
MAPE:0.22739142045580182
R2 score:0.4165887299874753
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:29:00
epoch 0  | loss: 1.04604 | val_0_rmse: 0.99857 | val_1_rmse: 1.01127 |  0:00:21s
epoch 1  | loss: 0.9411  | val_0_rmse: 0.89597 | val_1_rmse: 0.91753 |  0:00:42s
epoch 2  | loss: 0.73865 | val_0_rmse: 0.83818 | val_1_rmse: 0.85254 |  0:01:04s
epoch 3  | loss: 0.64916 | val_0_rmse: 0.78816 | val_1_rmse: 0.81625 |  0:01:25s
epoch 4  | loss: 0.62765 | val_0_rmse: 0.75865 | val_1_rmse: 0.78244 |  0:01:46s
epoch 5  | loss: 0.60358 | val_0_rmse: 0.75643 | val_1_rmse: 0.77646 |  0:02:08s
epoch 6  | loss: 0.58145 | val_0_rmse: 0.78905 | val_1_rmse: 0.81556 |  0:02:29s
epoch 7  | loss: 0.58454 | val_0_rmse: 0.7947  | val_1_rmse: 0.83427 |  0:02:51s
epoch 8  | loss: 0.57018 | val_0_rmse: 0.74055 | val_1_rmse: 0.78653 |  0:03:12s
epoch 9  | loss: 0.55524 | val_0_rmse: 0.75056 | val_1_rmse: 0.79361 |  0:03:34s
epoch 10 | loss: 0.5366  | val_0_rmse: 0.76469 | val_1_rmse: 0.78762 |  0:03:55s
epoch 11 | loss: 0.52899 | val_0_rmse: 0.71982 | val_1_rmse: 0.76578 |  0:04:16s
epoch 12 | loss: 0.51799 | val_0_rmse: 0.73263 | val_1_rmse: 0.76036 |  0:04:38s
epoch 13 | loss: 0.51386 | val_0_rmse: 0.72109 | val_1_rmse: 0.77072 |  0:05:00s
epoch 14 | loss: 0.502   | val_0_rmse: 0.72987 | val_1_rmse: 0.78455 |  0:05:21s
epoch 15 | loss: 0.497   | val_0_rmse: 0.80638 | val_1_rmse: 0.85586 |  0:05:42s
epoch 16 | loss: 0.49498 | val_0_rmse: 0.72462 | val_1_rmse: 0.79903 |  0:06:04s
epoch 17 | loss: 0.48761 | val_0_rmse: 0.76545 | val_1_rmse: 0.85789 |  0:06:25s
epoch 18 | loss: 0.47827 | val_0_rmse: 0.74025 | val_1_rmse: 0.80111 |  0:06:47s
epoch 19 | loss: 0.48119 | val_0_rmse: 0.90355 | val_1_rmse: 0.78293 |  0:07:08s
epoch 20 | loss: 0.47661 | val_0_rmse: 0.71827 | val_1_rmse: 0.77309 |  0:07:30s
epoch 21 | loss: 0.46765 | val_0_rmse: 0.80206 | val_1_rmse: 0.85523 |  0:07:51s
epoch 22 | loss: 0.45491 | val_0_rmse: 0.68585 | val_1_rmse: 0.76223 |  0:08:12s
epoch 23 | loss: 0.46163 | val_0_rmse: 0.70874 | val_1_rmse: 0.77308 |  0:08:35s
epoch 24 | loss: 0.45404 | val_0_rmse: 0.69339 | val_1_rmse: 0.7631  |  0:08:56s
epoch 25 | loss: 0.4532  | val_0_rmse: 0.71171 | val_1_rmse: 0.79328 |  0:09:18s
epoch 26 | loss: 0.44015 | val_0_rmse: 0.68832 | val_1_rmse: 0.78081 |  0:09:39s
epoch 27 | loss: 0.4367  | val_0_rmse: 0.67525 | val_1_rmse: 0.76439 |  0:10:01s
epoch 28 | loss: 0.43809 | val_0_rmse: 0.7241  | val_1_rmse: 0.81477 |  0:10:23s
epoch 29 | loss: 0.43507 | val_0_rmse: 0.68152 | val_1_rmse: 0.75802 |  0:10:44s
epoch 30 | loss: 0.43174 | val_0_rmse: 0.78445 | val_1_rmse: 1.01837 |  0:11:06s
epoch 31 | loss: 0.42035 | val_0_rmse: 0.72711 | val_1_rmse: 0.8042  |  0:11:28s
epoch 32 | loss: 0.41655 | val_0_rmse: 0.73343 | val_1_rmse: 0.82318 |  0:11:49s
epoch 33 | loss: 0.41296 | val_0_rmse: 0.73563 | val_1_rmse: 0.83492 |  0:12:11s
epoch 34 | loss: 0.40903 | val_0_rmse: 0.73851 | val_1_rmse: 0.8127  |  0:12:33s
epoch 35 | loss: 0.41275 | val_0_rmse: 0.97801 | val_1_rmse: 1.12586 |  0:12:54s
epoch 36 | loss: 0.41465 | val_0_rmse: 0.70738 | val_1_rmse: 0.77903 |  0:13:16s
epoch 37 | loss: 0.40556 | val_0_rmse: 0.6819  | val_1_rmse: 0.78882 |  0:13:38s
epoch 38 | loss: 0.40275 | val_0_rmse: 0.69408 | val_1_rmse: 0.79419 |  0:13:59s
epoch 39 | loss: 0.40106 | val_0_rmse: 0.73685 | val_1_rmse: 0.9068  |  0:14:21s
epoch 40 | loss: 0.41388 | val_0_rmse: 0.80573 | val_1_rmse: 0.84594 |  0:14:43s
epoch 41 | loss: 0.41047 | val_0_rmse: 0.72516 | val_1_rmse: 0.83858 |  0:15:05s
epoch 42 | loss: 0.43074 | val_0_rmse: 0.67748 | val_1_rmse: 0.76381 |  0:15:26s
epoch 43 | loss: 0.43798 | val_0_rmse: 0.91482 | val_1_rmse: 0.92224 |  0:15:48s
epoch 44 | loss: 0.42466 | val_0_rmse: 0.77096 | val_1_rmse: 0.85658 |  0:16:10s
epoch 45 | loss: 0.40323 | val_0_rmse: 0.71776 | val_1_rmse: 0.78664 |  0:16:31s
epoch 46 | loss: 0.39411 | val_0_rmse: 0.66564 | val_1_rmse: 0.78665 |  0:16:53s
epoch 47 | loss: 0.38822 | val_0_rmse: 0.67835 | val_1_rmse: 0.77825 |  0:17:15s
epoch 48 | loss: 0.39346 | val_0_rmse: 0.79437 | val_1_rmse: 0.76925 |  0:17:36s
epoch 49 | loss: 0.38331 | val_0_rmse: 0.68293 | val_1_rmse: 0.79921 |  0:17:58s
epoch 50 | loss: 0.38311 | val_0_rmse: 0.7002  | val_1_rmse: 0.77715 |  0:18:20s
epoch 51 | loss: 0.37769 | val_0_rmse: 0.6661  | val_1_rmse: 0.77828 |  0:18:41s
epoch 52 | loss: 0.37789 | val_0_rmse: 0.73771 | val_1_rmse: 0.79396 |  0:19:03s
epoch 53 | loss: 0.37533 | val_0_rmse: 0.6975  | val_1_rmse: 0.83477 |  0:19:25s
epoch 54 | loss: 0.38023 | val_0_rmse: 0.73601 | val_1_rmse: 0.84892 |  0:19:46s
epoch 55 | loss: 0.376   | val_0_rmse: 0.71909 | val_1_rmse: 0.81854 |  0:20:08s
epoch 56 | loss: 0.37407 | val_0_rmse: 0.72505 | val_1_rmse: 0.79743 |  0:20:29s
epoch 57 | loss: 0.36978 | val_0_rmse: 0.73339 | val_1_rmse: 0.80284 |  0:20:50s
epoch 58 | loss: 0.37185 | val_0_rmse: 0.68525 | val_1_rmse: 0.79742 |  0:21:12s
epoch 59 | loss: 0.36924 | val_0_rmse: 0.67427 | val_1_rmse: 0.81427 |  0:21:33s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.75802
Best weights from best epoch are automatically used!
ended training at: 02:50:45
Feature importance:
Mean squared error is of 0.07630973316372
Mean absolute error:0.18124787717438437
MAPE:0.20541302437088999
R2 score:0.37655406715272133
------------------------------------------------------------------
