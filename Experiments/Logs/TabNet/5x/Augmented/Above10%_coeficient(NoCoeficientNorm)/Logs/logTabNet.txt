TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:36:39
epoch 0  | loss: 0.71271 | val_0_rmse: 0.27599 | val_1_rmse: 0.27473 |  0:00:03s
epoch 1  | loss: 0.08472 | val_0_rmse: 0.27933 | val_1_rmse: 0.27856 |  0:00:05s
epoch 2  | loss: 0.0719  | val_0_rmse: 0.25114 | val_1_rmse: 0.25405 |  0:00:07s
epoch 3  | loss: 0.06586 | val_0_rmse: 0.24848 | val_1_rmse: 0.2507  |  0:00:09s
epoch 4  | loss: 0.06232 | val_0_rmse: 0.25015 | val_1_rmse: 0.25141 |  0:00:10s
epoch 5  | loss: 0.06198 | val_0_rmse: 0.25178 | val_1_rmse: 0.25301 |  0:00:12s
epoch 6  | loss: 0.06212 | val_0_rmse: 0.2468  | val_1_rmse: 0.2496  |  0:00:14s
epoch 7  | loss: 0.06202 | val_0_rmse: 0.24562 | val_1_rmse: 0.24846 |  0:00:16s
epoch 8  | loss: 0.06278 | val_0_rmse: 0.25446 | val_1_rmse: 0.25642 |  0:00:18s
epoch 9  | loss: 0.06288 | val_0_rmse: 0.25111 | val_1_rmse: 0.2576  |  0:00:19s
epoch 10 | loss: 0.06243 | val_0_rmse: 0.25268 | val_1_rmse: 0.25584 |  0:00:21s
epoch 11 | loss: 0.06137 | val_0_rmse: 0.24606 | val_1_rmse: 0.2551  |  0:00:23s
epoch 12 | loss: 0.05997 | val_0_rmse: 0.24311 | val_1_rmse: 0.25085 |  0:00:25s
epoch 13 | loss: 0.05943 | val_0_rmse: 0.24363 | val_1_rmse: 0.25373 |  0:00:26s
epoch 14 | loss: 0.06018 | val_0_rmse: 0.24381 | val_1_rmse: 0.25096 |  0:00:28s
epoch 15 | loss: 0.05999 | val_0_rmse: 0.24225 | val_1_rmse: 0.25174 |  0:00:30s
epoch 16 | loss: 0.05934 | val_0_rmse: 0.24123 | val_1_rmse: 0.25158 |  0:00:32s
epoch 17 | loss: 0.05939 | val_0_rmse: 0.24155 | val_1_rmse: 0.25406 |  0:00:34s
epoch 18 | loss: 0.06064 | val_0_rmse: 0.242   | val_1_rmse: 0.25109 |  0:00:35s
epoch 19 | loss: 0.05893 | val_0_rmse: 0.24255 | val_1_rmse: 0.25429 |  0:00:37s
epoch 20 | loss: 0.0584  | val_0_rmse: 0.24049 | val_1_rmse: 0.25684 |  0:00:39s
epoch 21 | loss: 0.05882 | val_0_rmse: 0.23973 | val_1_rmse: 0.25817 |  0:00:41s
epoch 22 | loss: 0.05912 | val_0_rmse: 0.23916 | val_1_rmse: 0.24899 |  0:00:43s
epoch 23 | loss: 0.0586  | val_0_rmse: 0.24034 | val_1_rmse: 0.2537  |  0:00:44s
epoch 24 | loss: 0.05854 | val_0_rmse: 0.24092 | val_1_rmse: 0.25376 |  0:00:46s
epoch 25 | loss: 0.0588  | val_0_rmse: 0.25743 | val_1_rmse: 0.27296 |  0:00:48s
epoch 26 | loss: 0.0625  | val_0_rmse: 0.25728 | val_1_rmse: 0.27445 |  0:00:50s
epoch 27 | loss: 0.05954 | val_0_rmse: 0.23622 | val_1_rmse: 0.24474 |  0:00:51s
epoch 28 | loss: 0.05877 | val_0_rmse: 0.25824 | val_1_rmse: 0.26682 |  0:00:53s
epoch 29 | loss: 0.05913 | val_0_rmse: 0.24045 | val_1_rmse: 0.24777 |  0:00:55s
epoch 30 | loss: 0.05922 | val_0_rmse: 0.25689 | val_1_rmse: 0.27041 |  0:00:57s
epoch 31 | loss: 0.06034 | val_0_rmse: 0.2382  | val_1_rmse: 0.24746 |  0:00:59s
epoch 32 | loss: 0.05797 | val_0_rmse: 0.23647 | val_1_rmse: 0.25012 |  0:01:00s
epoch 33 | loss: 0.06062 | val_0_rmse: 0.25612 | val_1_rmse: 0.25985 |  0:01:02s
epoch 34 | loss: 0.05801 | val_0_rmse: 0.242   | val_1_rmse: 0.25127 |  0:01:04s
epoch 35 | loss: 0.05741 | val_0_rmse: 0.23396 | val_1_rmse: 0.24638 |  0:01:06s
epoch 36 | loss: 0.05652 | val_0_rmse: 0.23844 | val_1_rmse: 0.24866 |  0:01:07s
epoch 37 | loss: 0.05729 | val_0_rmse: 0.23865 | val_1_rmse: 0.25109 |  0:01:09s
epoch 38 | loss: 0.05639 | val_0_rmse: 0.23468 | val_1_rmse: 0.24577 |  0:01:11s
epoch 39 | loss: 0.05703 | val_0_rmse: 0.23881 | val_1_rmse: 0.25223 |  0:01:13s
epoch 40 | loss: 0.05704 | val_0_rmse: 0.24058 | val_1_rmse: 0.24658 |  0:01:15s
epoch 41 | loss: 0.05642 | val_0_rmse: 0.23348 | val_1_rmse: 0.24979 |  0:01:16s
epoch 42 | loss: 0.05619 | val_0_rmse: 0.23491 | val_1_rmse: 0.24889 |  0:01:18s
epoch 43 | loss: 0.05536 | val_0_rmse: 0.23416 | val_1_rmse: 0.24908 |  0:01:20s
epoch 44 | loss: 0.05621 | val_0_rmse: 0.23375 | val_1_rmse: 0.24333 |  0:01:22s
epoch 45 | loss: 0.0553  | val_0_rmse: 0.23158 | val_1_rmse: 0.24522 |  0:01:24s
epoch 46 | loss: 0.05465 | val_0_rmse: 0.2328  | val_1_rmse: 0.24908 |  0:01:25s
epoch 47 | loss: 0.05533 | val_0_rmse: 0.23345 | val_1_rmse: 0.24808 |  0:01:27s
epoch 48 | loss: 0.05558 | val_0_rmse: 0.23076 | val_1_rmse: 0.24359 |  0:01:29s
epoch 49 | loss: 0.05436 | val_0_rmse: 0.23032 | val_1_rmse: 0.24777 |  0:01:31s
epoch 50 | loss: 0.0545  | val_0_rmse: 0.23451 | val_1_rmse: 0.25166 |  0:01:32s
epoch 51 | loss: 0.05642 | val_0_rmse: 0.23583 | val_1_rmse: 0.24847 |  0:01:34s
epoch 52 | loss: 0.05706 | val_0_rmse: 0.23679 | val_1_rmse: 0.249   |  0:01:36s
epoch 53 | loss: 0.05723 | val_0_rmse: 0.23756 | val_1_rmse: 0.25347 |  0:01:38s
epoch 54 | loss: 0.05611 | val_0_rmse: 0.23299 | val_1_rmse: 0.24783 |  0:01:40s
epoch 55 | loss: 0.05604 | val_0_rmse: 0.24093 | val_1_rmse: 0.25571 |  0:01:41s
epoch 56 | loss: 0.05764 | val_0_rmse: 0.23611 | val_1_rmse: 0.25212 |  0:01:43s
epoch 57 | loss: 0.05705 | val_0_rmse: 0.23393 | val_1_rmse: 0.25274 |  0:01:45s
epoch 58 | loss: 0.05601 | val_0_rmse: 0.23308 | val_1_rmse: 0.24482 |  0:01:47s
epoch 59 | loss: 0.05565 | val_0_rmse: 0.2327  | val_1_rmse: 0.25401 |  0:01:49s
epoch 60 | loss: 0.05535 | val_0_rmse: 0.23782 | val_1_rmse: 0.27121 |  0:01:50s
epoch 61 | loss: 0.05539 | val_0_rmse: 0.22984 | val_1_rmse: 0.24589 |  0:01:52s
epoch 62 | loss: 0.055   | val_0_rmse: 0.23115 | val_1_rmse: 0.25017 |  0:01:54s
epoch 63 | loss: 0.0545  | val_0_rmse: 0.22928 | val_1_rmse: 0.24896 |  0:01:56s
epoch 64 | loss: 0.0539  | val_0_rmse: 0.23304 | val_1_rmse: 0.25243 |  0:01:57s
epoch 65 | loss: 0.0543  | val_0_rmse: 0.22969 | val_1_rmse: 0.25059 |  0:01:59s
epoch 66 | loss: 0.05489 | val_0_rmse: 0.23103 | val_1_rmse: 0.25512 |  0:02:01s
epoch 67 | loss: 0.05351 | val_0_rmse: 0.22744 | val_1_rmse: 0.24257 |  0:02:03s
epoch 68 | loss: 0.054   | val_0_rmse: 0.2285  | val_1_rmse: 0.24569 |  0:02:05s
epoch 69 | loss: 0.05341 | val_0_rmse: 0.22958 | val_1_rmse: 0.24748 |  0:02:06s
epoch 70 | loss: 0.05417 | val_0_rmse: 0.25628 | val_1_rmse: 0.2768  |  0:02:08s
epoch 71 | loss: 0.05624 | val_0_rmse: 0.23844 | val_1_rmse: 0.25079 |  0:02:10s
epoch 72 | loss: 0.05534 | val_0_rmse: 0.23325 | val_1_rmse: 0.24945 |  0:02:12s
epoch 73 | loss: 0.05406 | val_0_rmse: 0.22806 | val_1_rmse: 0.25043 |  0:02:13s
epoch 74 | loss: 0.05306 | val_0_rmse: 0.22908 | val_1_rmse: 0.25166 |  0:02:15s
epoch 75 | loss: 0.05258 | val_0_rmse: 0.22754 | val_1_rmse: 0.24765 |  0:02:17s
epoch 76 | loss: 0.05337 | val_0_rmse: 0.23367 | val_1_rmse: 0.25454 |  0:02:19s
epoch 77 | loss: 0.05462 | val_0_rmse: 0.23171 | val_1_rmse: 0.24641 |  0:02:21s
epoch 78 | loss: 0.05421 | val_0_rmse: 0.23214 | val_1_rmse: 0.24564 |  0:02:22s
epoch 79 | loss: 0.05383 | val_0_rmse: 0.22703 | val_1_rmse: 0.24568 |  0:02:24s
epoch 80 | loss: 0.05361 | val_0_rmse: 0.22891 | val_1_rmse: 0.24461 |  0:02:26s
epoch 81 | loss: 0.05301 | val_0_rmse: 0.22896 | val_1_rmse: 0.24605 |  0:02:28s
epoch 82 | loss: 0.05374 | val_0_rmse: 0.22625 | val_1_rmse: 0.24048 |  0:02:29s
epoch 83 | loss: 0.0529  | val_0_rmse: 0.2298  | val_1_rmse: 0.24414 |  0:02:31s
epoch 84 | loss: 0.05333 | val_0_rmse: 0.23248 | val_1_rmse: 0.24908 |  0:02:33s
epoch 85 | loss: 0.0535  | val_0_rmse: 0.22666 | val_1_rmse: 0.2437  |  0:02:35s
epoch 86 | loss: 0.05275 | val_0_rmse: 0.2266  | val_1_rmse: 0.24595 |  0:02:37s
epoch 87 | loss: 0.05292 | val_0_rmse: 0.2336  | val_1_rmse: 0.24726 |  0:02:38s
epoch 88 | loss: 0.05413 | val_0_rmse: 0.22595 | val_1_rmse: 0.24595 |  0:02:40s
epoch 89 | loss: 0.05318 | val_0_rmse: 0.22566 | val_1_rmse: 0.24422 |  0:02:42s
epoch 90 | loss: 0.05256 | val_0_rmse: 0.22521 | val_1_rmse: 0.24796 |  0:02:44s
epoch 91 | loss: 0.05242 | val_0_rmse: 0.2251  | val_1_rmse: 0.2385  |  0:02:46s
epoch 92 | loss: 0.05182 | val_0_rmse: 0.22426 | val_1_rmse: 0.23861 |  0:02:47s
epoch 93 | loss: 0.05337 | val_0_rmse: 0.22637 | val_1_rmse: 0.24428 |  0:02:49s
epoch 94 | loss: 0.0515  | val_0_rmse: 0.22307 | val_1_rmse: 0.23994 |  0:02:51s
epoch 95 | loss: 0.0513  | val_0_rmse: 0.2232  | val_1_rmse: 0.24123 |  0:02:53s
epoch 96 | loss: 0.0516  | val_0_rmse: 0.22541 | val_1_rmse: 0.24112 |  0:02:54s
epoch 97 | loss: 0.05133 | val_0_rmse: 0.22272 | val_1_rmse: 0.24267 |  0:02:56s
epoch 98 | loss: 0.05024 | val_0_rmse: 0.2199  | val_1_rmse: 0.23657 |  0:02:58s
epoch 99 | loss: 0.05192 | val_0_rmse: 0.22053 | val_1_rmse: 0.2378  |  0:03:00s
epoch 100| loss: 0.05164 | val_0_rmse: 0.22271 | val_1_rmse: 0.23741 |  0:03:02s
epoch 101| loss: 0.05139 | val_0_rmse: 0.22269 | val_1_rmse: 0.24101 |  0:03:03s
epoch 102| loss: 0.05105 | val_0_rmse: 0.22309 | val_1_rmse: 0.24099 |  0:03:05s
epoch 103| loss: 0.05016 | val_0_rmse: 0.2219  | val_1_rmse: 0.2383  |  0:03:07s
epoch 104| loss: 0.04985 | val_0_rmse: 0.21876 | val_1_rmse: 0.23667 |  0:03:09s
epoch 105| loss: 0.04954 | val_0_rmse: 0.22069 | val_1_rmse: 0.244   |  0:03:11s
epoch 106| loss: 0.04947 | val_0_rmse: 0.2188  | val_1_rmse: 0.23953 |  0:03:12s
epoch 107| loss: 0.04958 | val_0_rmse: 0.21746 | val_1_rmse: 0.23783 |  0:03:14s
epoch 108| loss: 0.04968 | val_0_rmse: 0.21858 | val_1_rmse: 0.24421 |  0:03:16s
epoch 109| loss: 0.04887 | val_0_rmse: 0.22113 | val_1_rmse: 0.24415 |  0:03:18s
epoch 110| loss: 0.04956 | val_0_rmse: 0.22294 | val_1_rmse: 0.23947 |  0:03:19s
epoch 111| loss: 0.05003 | val_0_rmse: 0.21587 | val_1_rmse: 0.23651 |  0:03:21s
epoch 112| loss: 0.04859 | val_0_rmse: 0.21693 | val_1_rmse: 0.24151 |  0:03:23s
epoch 113| loss: 0.04953 | val_0_rmse: 0.21846 | val_1_rmse: 0.2432  |  0:03:25s
epoch 114| loss: 0.04897 | val_0_rmse: 0.21775 | val_1_rmse: 0.23997 |  0:03:27s
epoch 115| loss: 0.04813 | val_0_rmse: 0.21593 | val_1_rmse: 0.24309 |  0:03:28s
epoch 116| loss: 0.04777 | val_0_rmse: 0.21669 | val_1_rmse: 0.23873 |  0:03:30s
epoch 117| loss: 0.04865 | val_0_rmse: 0.21608 | val_1_rmse: 0.24309 |  0:03:32s
epoch 118| loss: 0.049   | val_0_rmse: 0.21518 | val_1_rmse: 0.24293 |  0:03:34s
epoch 119| loss: 0.04836 | val_0_rmse: 0.21534 | val_1_rmse: 0.24364 |  0:03:35s
epoch 120| loss: 0.04771 | val_0_rmse: 0.21669 | val_1_rmse: 0.24574 |  0:03:37s
epoch 121| loss: 0.04849 | val_0_rmse: 0.21242 | val_1_rmse: 0.23993 |  0:03:39s
epoch 122| loss: 0.04785 | val_0_rmse: 0.2214  | val_1_rmse: 0.2466  |  0:03:41s
epoch 123| loss: 0.05028 | val_0_rmse: 0.2183  | val_1_rmse: 0.24462 |  0:03:43s
epoch 124| loss: 0.04829 | val_0_rmse: 0.21393 | val_1_rmse: 0.24078 |  0:03:44s
epoch 125| loss: 0.04811 | val_0_rmse: 0.21413 | val_1_rmse: 0.24261 |  0:03:46s
epoch 126| loss: 0.04726 | val_0_rmse: 0.21356 | val_1_rmse: 0.24279 |  0:03:48s
epoch 127| loss: 0.04792 | val_0_rmse: 0.22101 | val_1_rmse: 0.25066 |  0:03:50s
epoch 128| loss: 0.04864 | val_0_rmse: 0.22265 | val_1_rmse: 0.2454  |  0:03:51s
epoch 129| loss: 0.04896 | val_0_rmse: 0.21206 | val_1_rmse: 0.23874 |  0:03:53s
epoch 130| loss: 0.04716 | val_0_rmse: 0.21327 | val_1_rmse: 0.24364 |  0:03:55s
epoch 131| loss: 0.04754 | val_0_rmse: 0.21218 | val_1_rmse: 0.24302 |  0:03:57s
epoch 132| loss: 0.04711 | val_0_rmse: 0.21121 | val_1_rmse: 0.23642 |  0:03:59s
epoch 133| loss: 0.04736 | val_0_rmse: 0.21184 | val_1_rmse: 0.23567 |  0:04:00s
epoch 134| loss: 0.0477  | val_0_rmse: 0.212   | val_1_rmse: 0.23704 |  0:04:02s
epoch 135| loss: 0.04741 | val_0_rmse: 0.21754 | val_1_rmse: 0.24307 |  0:04:04s
epoch 136| loss: 0.04747 | val_0_rmse: 0.21231 | val_1_rmse: 0.2441  |  0:04:06s
epoch 137| loss: 0.04613 | val_0_rmse: 0.21108 | val_1_rmse: 0.23904 |  0:04:08s
epoch 138| loss: 0.04686 | val_0_rmse: 0.21167 | val_1_rmse: 0.24173 |  0:04:10s
epoch 139| loss: 0.04641 | val_0_rmse: 0.20964 | val_1_rmse: 0.23859 |  0:04:11s
epoch 140| loss: 0.04681 | val_0_rmse: 0.21084 | val_1_rmse: 0.2384  |  0:04:13s
epoch 141| loss: 0.04641 | val_0_rmse: 0.20904 | val_1_rmse: 0.23916 |  0:04:15s
epoch 142| loss: 0.04648 | val_0_rmse: 0.21166 | val_1_rmse: 0.24086 |  0:04:17s
epoch 143| loss: 0.04614 | val_0_rmse: 0.22185 | val_1_rmse: 0.25642 |  0:04:18s
epoch 144| loss: 0.04655 | val_0_rmse: 0.20782 | val_1_rmse: 0.23856 |  0:04:20s
epoch 145| loss: 0.04625 | val_0_rmse: 0.21239 | val_1_rmse: 0.24866 |  0:04:22s
epoch 146| loss: 0.04595 | val_0_rmse: 0.20937 | val_1_rmse: 0.23905 |  0:04:24s
epoch 147| loss: 0.04507 | val_0_rmse: 0.21061 | val_1_rmse: 0.2399  |  0:04:26s
epoch 148| loss: 0.04636 | val_0_rmse: 0.21478 | val_1_rmse: 0.24766 |  0:04:27s
epoch 149| loss: 0.04733 | val_0_rmse: 0.21053 | val_1_rmse: 0.2456  |  0:04:29s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.23567
Best weights from best epoch are automatically used!
ended training at: 20:41:09
Feature importance:
Mean squared error is of 0.05238103541487368
Mean absolute error:0.15375775674234674
MAPE:0.16302703352958525
R2 score:0.246669462037596
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:41:10
epoch 0  | loss: 0.78429 | val_0_rmse: 0.29834 | val_1_rmse: 0.30941 |  0:00:01s
epoch 1  | loss: 0.09277 | val_0_rmse: 0.29424 | val_1_rmse: 0.30569 |  0:00:03s
epoch 2  | loss: 0.07174 | val_0_rmse: 0.24266 | val_1_rmse: 0.25625 |  0:00:05s
epoch 3  | loss: 0.06693 | val_0_rmse: 0.24166 | val_1_rmse: 0.2554  |  0:00:07s
epoch 4  | loss: 0.06272 | val_0_rmse: 0.24104 | val_1_rmse: 0.25551 |  0:00:08s
epoch 5  | loss: 0.06272 | val_0_rmse: 0.2443  | val_1_rmse: 0.25868 |  0:00:10s
epoch 6  | loss: 0.06052 | val_0_rmse: 0.24395 | val_1_rmse: 0.25799 |  0:00:12s
epoch 7  | loss: 0.05934 | val_0_rmse: 0.23879 | val_1_rmse: 0.25321 |  0:00:14s
epoch 8  | loss: 0.0588  | val_0_rmse: 0.24055 | val_1_rmse: 0.25483 |  0:00:16s
epoch 9  | loss: 0.05876 | val_0_rmse: 0.23912 | val_1_rmse: 0.25327 |  0:00:17s
epoch 10 | loss: 0.05917 | val_0_rmse: 0.24235 | val_1_rmse: 0.25588 |  0:00:19s
epoch 11 | loss: 0.05886 | val_0_rmse: 0.23971 | val_1_rmse: 0.25338 |  0:00:21s
epoch 12 | loss: 0.06056 | val_0_rmse: 0.23926 | val_1_rmse: 0.25312 |  0:00:23s
epoch 13 | loss: 0.05817 | val_0_rmse: 0.2392  | val_1_rmse: 0.25325 |  0:00:25s
epoch 14 | loss: 0.05833 | val_0_rmse: 0.24169 | val_1_rmse: 0.25535 |  0:00:26s
epoch 15 | loss: 0.05882 | val_0_rmse: 0.23925 | val_1_rmse: 0.25299 |  0:00:28s
epoch 16 | loss: 0.05838 | val_0_rmse: 0.23983 | val_1_rmse: 0.25347 |  0:00:30s
epoch 17 | loss: 0.05855 | val_0_rmse: 0.23805 | val_1_rmse: 0.25158 |  0:00:32s
epoch 18 | loss: 0.05777 | val_0_rmse: 0.23803 | val_1_rmse: 0.25183 |  0:00:34s
epoch 19 | loss: 0.05719 | val_0_rmse: 0.23925 | val_1_rmse: 0.25276 |  0:00:35s
epoch 20 | loss: 0.05824 | val_0_rmse: 0.23789 | val_1_rmse: 0.25122 |  0:00:37s
epoch 21 | loss: 0.0572  | val_0_rmse: 0.23772 | val_1_rmse: 0.25114 |  0:00:39s
epoch 22 | loss: 0.0568  | val_0_rmse: 0.23809 | val_1_rmse: 0.2523  |  0:00:41s
epoch 23 | loss: 0.05668 | val_0_rmse: 0.24386 | val_1_rmse: 0.25688 |  0:00:43s
epoch 24 | loss: 0.05908 | val_0_rmse: 0.23719 | val_1_rmse: 0.25074 |  0:00:44s
epoch 25 | loss: 0.05634 | val_0_rmse: 0.23662 | val_1_rmse: 0.24944 |  0:00:46s
epoch 26 | loss: 0.05647 | val_0_rmse: 0.23705 | val_1_rmse: 0.25019 |  0:00:48s
epoch 27 | loss: 0.05689 | val_0_rmse: 0.23628 | val_1_rmse: 0.24875 |  0:00:50s
epoch 28 | loss: 0.05663 | val_0_rmse: 0.23656 | val_1_rmse: 0.24918 |  0:00:51s
epoch 29 | loss: 0.05626 | val_0_rmse: 0.23673 | val_1_rmse: 0.25    |  0:00:53s
epoch 30 | loss: 0.05648 | val_0_rmse: 0.2364  | val_1_rmse: 0.24932 |  0:00:55s
epoch 31 | loss: 0.05656 | val_0_rmse: 0.23733 | val_1_rmse: 0.24987 |  0:00:57s
epoch 32 | loss: 0.05665 | val_0_rmse: 0.24074 | val_1_rmse: 0.25361 |  0:00:59s
epoch 33 | loss: 0.05698 | val_0_rmse: 0.23932 | val_1_rmse: 0.25198 |  0:01:00s
epoch 34 | loss: 0.05654 | val_0_rmse: 0.25101 | val_1_rmse: 0.26267 |  0:01:02s
epoch 35 | loss: 0.05734 | val_0_rmse: 0.23566 | val_1_rmse: 0.24905 |  0:01:04s
epoch 36 | loss: 0.05661 | val_0_rmse: 0.23727 | val_1_rmse: 0.25124 |  0:01:06s
epoch 37 | loss: 0.05602 | val_0_rmse: 0.23559 | val_1_rmse: 0.24897 |  0:01:08s
epoch 38 | loss: 0.05554 | val_0_rmse: 0.23586 | val_1_rmse: 0.24899 |  0:01:09s
epoch 39 | loss: 0.05553 | val_0_rmse: 0.23553 | val_1_rmse: 0.24957 |  0:01:11s
epoch 40 | loss: 0.05549 | val_0_rmse: 0.23497 | val_1_rmse: 0.24941 |  0:01:13s
epoch 41 | loss: 0.0551  | val_0_rmse: 0.25006 | val_1_rmse: 0.26406 |  0:01:15s
epoch 42 | loss: 0.05688 | val_0_rmse: 0.23406 | val_1_rmse: 0.24943 |  0:01:16s
epoch 43 | loss: 0.05459 | val_0_rmse: 0.2336  | val_1_rmse: 0.25081 |  0:01:18s
epoch 44 | loss: 0.05413 | val_0_rmse: 0.23018 | val_1_rmse: 0.24741 |  0:01:20s
epoch 45 | loss: 0.0543  | val_0_rmse: 0.23148 | val_1_rmse: 0.24881 |  0:01:22s
epoch 46 | loss: 0.0544  | val_0_rmse: 0.22926 | val_1_rmse: 0.24757 |  0:01:24s
epoch 47 | loss: 0.05436 | val_0_rmse: 0.231   | val_1_rmse: 0.24802 |  0:01:25s
epoch 48 | loss: 0.05371 | val_0_rmse: 0.22866 | val_1_rmse: 0.24665 |  0:01:27s
epoch 49 | loss: 0.05356 | val_0_rmse: 0.22884 | val_1_rmse: 0.24747 |  0:01:29s
epoch 50 | loss: 0.05306 | val_0_rmse: 0.22959 | val_1_rmse: 0.24713 |  0:01:31s
epoch 51 | loss: 0.0537  | val_0_rmse: 0.22841 | val_1_rmse: 0.248   |  0:01:33s
epoch 52 | loss: 0.05318 | val_0_rmse: 0.23083 | val_1_rmse: 0.24819 |  0:01:34s
epoch 53 | loss: 0.05371 | val_0_rmse: 0.22722 | val_1_rmse: 0.24538 |  0:01:36s
epoch 54 | loss: 0.05334 | val_0_rmse: 0.22578 | val_1_rmse: 0.2454  |  0:01:38s
epoch 55 | loss: 0.05232 | val_0_rmse: 0.22682 | val_1_rmse: 0.24722 |  0:01:40s
epoch 56 | loss: 0.05232 | val_0_rmse: 0.22566 | val_1_rmse: 0.24555 |  0:01:42s
epoch 57 | loss: 0.05176 | val_0_rmse: 0.22454 | val_1_rmse: 0.24482 |  0:01:43s
epoch 58 | loss: 0.05156 | val_0_rmse: 0.22297 | val_1_rmse: 0.24437 |  0:01:45s
epoch 59 | loss: 0.05149 | val_0_rmse: 0.22302 | val_1_rmse: 0.24441 |  0:01:47s
epoch 60 | loss: 0.05109 | val_0_rmse: 0.22481 | val_1_rmse: 0.24688 |  0:01:49s
epoch 61 | loss: 0.0518  | val_0_rmse: 0.22383 | val_1_rmse: 0.24478 |  0:01:51s
epoch 62 | loss: 0.05089 | val_0_rmse: 0.22348 | val_1_rmse: 0.24404 |  0:01:52s
epoch 63 | loss: 0.05145 | val_0_rmse: 0.22499 | val_1_rmse: 0.24471 |  0:01:54s
epoch 64 | loss: 0.05157 | val_0_rmse: 0.22544 | val_1_rmse: 0.24651 |  0:01:56s
epoch 65 | loss: 0.05117 | val_0_rmse: 0.22325 | val_1_rmse: 0.24598 |  0:01:58s
epoch 66 | loss: 0.05084 | val_0_rmse: 0.22242 | val_1_rmse: 0.24423 |  0:02:00s
epoch 67 | loss: 0.05129 | val_0_rmse: 0.22383 | val_1_rmse: 0.24734 |  0:02:01s
epoch 68 | loss: 0.05067 | val_0_rmse: 0.2303  | val_1_rmse: 0.25061 |  0:02:03s
epoch 69 | loss: 0.05352 | val_0_rmse: 0.23006 | val_1_rmse: 0.25239 |  0:02:05s
epoch 70 | loss: 0.05298 | val_0_rmse: 0.22915 | val_1_rmse: 0.25078 |  0:02:07s
epoch 71 | loss: 0.0515  | val_0_rmse: 0.22695 | val_1_rmse: 0.24836 |  0:02:08s
epoch 72 | loss: 0.05191 | val_0_rmse: 0.22153 | val_1_rmse: 0.24382 |  0:02:10s
epoch 73 | loss: 0.05063 | val_0_rmse: 0.22064 | val_1_rmse: 0.24563 |  0:02:12s
epoch 74 | loss: 0.0504  | val_0_rmse: 0.22012 | val_1_rmse: 0.2443  |  0:02:14s
epoch 75 | loss: 0.05003 | val_0_rmse: 0.22084 | val_1_rmse: 0.24692 |  0:02:16s
epoch 76 | loss: 0.05096 | val_0_rmse: 0.21962 | val_1_rmse: 0.24465 |  0:02:17s
epoch 77 | loss: 0.04985 | val_0_rmse: 0.2198  | val_1_rmse: 0.24504 |  0:02:19s
epoch 78 | loss: 0.04938 | val_0_rmse: 0.21872 | val_1_rmse: 0.24543 |  0:02:21s
epoch 79 | loss: 0.05002 | val_0_rmse: 0.22341 | val_1_rmse: 0.24567 |  0:02:23s
epoch 80 | loss: 0.04954 | val_0_rmse: 0.21869 | val_1_rmse: 0.24173 |  0:02:25s
epoch 81 | loss: 0.0488  | val_0_rmse: 0.21781 | val_1_rmse: 0.24463 |  0:02:26s
epoch 82 | loss: 0.04933 | val_0_rmse: 0.21844 | val_1_rmse: 0.24461 |  0:02:28s
epoch 83 | loss: 0.04921 | val_0_rmse: 0.21655 | val_1_rmse: 0.24281 |  0:02:30s
epoch 84 | loss: 0.04865 | val_0_rmse: 0.2168  | val_1_rmse: 0.24337 |  0:02:32s
epoch 85 | loss: 0.04872 | val_0_rmse: 0.21724 | val_1_rmse: 0.2435  |  0:02:34s
epoch 86 | loss: 0.04867 | val_0_rmse: 0.21614 | val_1_rmse: 0.24351 |  0:02:35s
epoch 87 | loss: 0.0481  | val_0_rmse: 0.2173  | val_1_rmse: 0.24419 |  0:02:37s
epoch 88 | loss: 0.04862 | val_0_rmse: 0.21708 | val_1_rmse: 0.24224 |  0:02:39s
epoch 89 | loss: 0.04812 | val_0_rmse: 0.22032 | val_1_rmse: 0.24739 |  0:02:41s
epoch 90 | loss: 0.0486  | val_0_rmse: 0.21522 | val_1_rmse: 0.24323 |  0:02:42s
epoch 91 | loss: 0.04757 | val_0_rmse: 0.21458 | val_1_rmse: 0.24537 |  0:02:44s
epoch 92 | loss: 0.04782 | val_0_rmse: 0.21535 | val_1_rmse: 0.24104 |  0:02:46s
epoch 93 | loss: 0.04826 | val_0_rmse: 0.21464 | val_1_rmse: 0.24233 |  0:02:48s
epoch 94 | loss: 0.04788 | val_0_rmse: 0.21418 | val_1_rmse: 0.24047 |  0:02:50s
epoch 95 | loss: 0.04825 | val_0_rmse: 0.21606 | val_1_rmse: 0.24534 |  0:02:51s
epoch 96 | loss: 0.04821 | val_0_rmse: 0.21397 | val_1_rmse: 0.23938 |  0:02:53s
epoch 97 | loss: 0.04888 | val_0_rmse: 0.21478 | val_1_rmse: 0.2442  |  0:02:55s
epoch 98 | loss: 0.04716 | val_0_rmse: 0.21331 | val_1_rmse: 0.24036 |  0:02:57s
epoch 99 | loss: 0.04679 | val_0_rmse: 0.21246 | val_1_rmse: 0.24216 |  0:02:59s
epoch 100| loss: 0.04663 | val_0_rmse: 0.2107  | val_1_rmse: 0.24257 |  0:03:00s
epoch 101| loss: 0.04677 | val_0_rmse: 0.21343 | val_1_rmse: 0.24449 |  0:03:02s
epoch 102| loss: 0.04718 | val_0_rmse: 0.21372 | val_1_rmse: 0.24075 |  0:03:04s
epoch 103| loss: 0.04665 | val_0_rmse: 0.21443 | val_1_rmse: 0.24392 |  0:03:06s
epoch 104| loss: 0.04667 | val_0_rmse: 0.21353 | val_1_rmse: 0.2431  |  0:03:08s
epoch 105| loss: 0.0466  | val_0_rmse: 0.21393 | val_1_rmse: 0.24208 |  0:03:09s
epoch 106| loss: 0.04689 | val_0_rmse: 0.2211  | val_1_rmse: 0.24063 |  0:03:11s
epoch 107| loss: 0.04695 | val_0_rmse: 0.21036 | val_1_rmse: 0.23824 |  0:03:13s
epoch 108| loss: 0.04823 | val_0_rmse: 0.2152  | val_1_rmse: 0.23896 |  0:03:15s
epoch 109| loss: 0.04907 | val_0_rmse: 0.2166  | val_1_rmse: 0.24485 |  0:03:16s
epoch 110| loss: 0.04736 | val_0_rmse: 0.21394 | val_1_rmse: 0.24138 |  0:03:18s
epoch 111| loss: 0.04859 | val_0_rmse: 0.21638 | val_1_rmse: 0.24466 |  0:03:20s
epoch 112| loss: 0.04735 | val_0_rmse: 0.21153 | val_1_rmse: 0.24135 |  0:03:22s
epoch 113| loss: 0.0463  | val_0_rmse: 0.2119  | val_1_rmse: 0.24477 |  0:03:24s
epoch 114| loss: 0.0465  | val_0_rmse: 0.20931 | val_1_rmse: 0.24446 |  0:03:25s
epoch 115| loss: 0.04636 | val_0_rmse: 0.21057 | val_1_rmse: 0.24746 |  0:03:27s
epoch 116| loss: 0.04556 | val_0_rmse: 0.211   | val_1_rmse: 0.24395 |  0:03:29s
epoch 117| loss: 0.04661 | val_0_rmse: 0.20963 | val_1_rmse: 0.24479 |  0:03:31s
epoch 118| loss: 0.04567 | val_0_rmse: 0.20844 | val_1_rmse: 0.24314 |  0:03:33s
epoch 119| loss: 0.04594 | val_0_rmse: 0.21158 | val_1_rmse: 0.24576 |  0:03:34s
epoch 120| loss: 0.04673 | val_0_rmse: 0.21554 | val_1_rmse: 0.25301 |  0:03:36s
epoch 121| loss: 0.04668 | val_0_rmse: 0.21153 | val_1_rmse: 0.24648 |  0:03:38s
epoch 122| loss: 0.0466  | val_0_rmse: 0.2079  | val_1_rmse: 0.24438 |  0:03:40s
epoch 123| loss: 0.04641 | val_0_rmse: 0.21222 | val_1_rmse: 0.24525 |  0:03:41s
epoch 124| loss: 0.04667 | val_0_rmse: 0.20806 | val_1_rmse: 0.244   |  0:03:43s
epoch 125| loss: 0.0461  | val_0_rmse: 0.20987 | val_1_rmse: 0.24772 |  0:03:45s
epoch 126| loss: 0.04608 | val_0_rmse: 0.20962 | val_1_rmse: 0.24671 |  0:03:47s
epoch 127| loss: 0.04556 | val_0_rmse: 0.21301 | val_1_rmse: 0.24499 |  0:03:49s
epoch 128| loss: 0.04537 | val_0_rmse: 0.20927 | val_1_rmse: 0.24188 |  0:03:50s
epoch 129| loss: 0.04503 | val_0_rmse: 0.20714 | val_1_rmse: 0.24309 |  0:03:52s
epoch 130| loss: 0.04532 | val_0_rmse: 0.20683 | val_1_rmse: 0.24562 |  0:03:54s
epoch 131| loss: 0.04441 | val_0_rmse: 0.20853 | val_1_rmse: 0.24565 |  0:03:56s
epoch 132| loss: 0.04573 | val_0_rmse: 0.21452 | val_1_rmse: 0.24933 |  0:03:58s
epoch 133| loss: 0.04697 | val_0_rmse: 0.21074 | val_1_rmse: 0.24525 |  0:03:59s
epoch 134| loss: 0.04625 | val_0_rmse: 0.20997 | val_1_rmse: 0.24575 |  0:04:01s
epoch 135| loss: 0.0455  | val_0_rmse: 0.21431 | val_1_rmse: 0.25605 |  0:04:03s
epoch 136| loss: 0.04686 | val_0_rmse: 0.20783 | val_1_rmse: 0.24773 |  0:04:05s
epoch 137| loss: 0.04553 | val_0_rmse: 0.20928 | val_1_rmse: 0.24632 |  0:04:06s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.23824
Best weights from best epoch are automatically used!
ended training at: 20:45:18
Feature importance:
Mean squared error is of 0.05545450286698942
Mean absolute error:0.1557301513628852
MAPE:0.17278087333917133
R2 score:0.22934481082049063
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:45:18
epoch 0  | loss: 0.7326  | val_0_rmse: 0.26449 | val_1_rmse: 0.26764 |  0:00:01s
epoch 1  | loss: 0.08859 | val_0_rmse: 0.24695 | val_1_rmse: 0.25148 |  0:00:03s
epoch 2  | loss: 0.06817 | val_0_rmse: 0.23995 | val_1_rmse: 0.24577 |  0:00:05s
epoch 3  | loss: 0.06479 | val_0_rmse: 0.24045 | val_1_rmse: 0.24594 |  0:00:07s
epoch 4  | loss: 0.06153 | val_0_rmse: 0.24224 | val_1_rmse: 0.24756 |  0:00:08s
epoch 5  | loss: 0.05986 | val_0_rmse: 0.24108 | val_1_rmse: 0.24676 |  0:00:10s
epoch 6  | loss: 0.06013 | val_0_rmse: 0.24211 | val_1_rmse: 0.2473  |  0:00:12s
epoch 7  | loss: 0.05832 | val_0_rmse: 0.24075 | val_1_rmse: 0.24663 |  0:00:14s
epoch 8  | loss: 0.05908 | val_0_rmse: 0.25121 | val_1_rmse: 0.25568 |  0:00:16s
epoch 9  | loss: 0.06034 | val_0_rmse: 0.25765 | val_1_rmse: 0.26474 |  0:00:17s
epoch 10 | loss: 0.06168 | val_0_rmse: 0.24043 | val_1_rmse: 0.24556 |  0:00:19s
epoch 11 | loss: 0.05845 | val_0_rmse: 0.2394  | val_1_rmse: 0.24523 |  0:00:21s
epoch 12 | loss: 0.05817 | val_0_rmse: 0.24022 | val_1_rmse: 0.24571 |  0:00:23s
epoch 13 | loss: 0.05849 | val_0_rmse: 0.23889 | val_1_rmse: 0.2442  |  0:00:25s
epoch 14 | loss: 0.058   | val_0_rmse: 0.24253 | val_1_rmse: 0.24851 |  0:00:26s
epoch 15 | loss: 0.05948 | val_0_rmse: 0.24173 | val_1_rmse: 0.24677 |  0:00:28s
epoch 16 | loss: 0.05985 | val_0_rmse: 0.24119 | val_1_rmse: 0.24662 |  0:00:30s
epoch 17 | loss: 0.05881 | val_0_rmse: 0.24357 | val_1_rmse: 0.24761 |  0:00:32s
epoch 18 | loss: 0.06156 | val_0_rmse: 0.2428  | val_1_rmse: 0.24925 |  0:00:34s
epoch 19 | loss: 0.05993 | val_0_rmse: 0.23905 | val_1_rmse: 0.24459 |  0:00:35s
epoch 20 | loss: 0.05985 | val_0_rmse: 0.23828 | val_1_rmse: 0.24317 |  0:00:37s
epoch 21 | loss: 0.05882 | val_0_rmse: 0.24328 | val_1_rmse: 0.24947 |  0:00:39s
epoch 22 | loss: 0.05825 | val_0_rmse: 0.24515 | val_1_rmse: 0.24963 |  0:00:41s
epoch 23 | loss: 0.05884 | val_0_rmse: 0.24037 | val_1_rmse: 0.24561 |  0:00:43s
epoch 24 | loss: 0.05931 | val_0_rmse: 0.2395  | val_1_rmse: 0.24376 |  0:00:44s
epoch 25 | loss: 0.05819 | val_0_rmse: 0.23872 | val_1_rmse: 0.24285 |  0:00:46s
epoch 26 | loss: 0.05738 | val_0_rmse: 0.24148 | val_1_rmse: 0.24517 |  0:00:48s
epoch 27 | loss: 0.05888 | val_0_rmse: 0.23767 | val_1_rmse: 0.24219 |  0:00:50s
epoch 28 | loss: 0.05741 | val_0_rmse: 0.23818 | val_1_rmse: 0.24416 |  0:00:52s
epoch 29 | loss: 0.05786 | val_0_rmse: 0.24038 | val_1_rmse: 0.24638 |  0:00:53s
epoch 30 | loss: 0.05777 | val_0_rmse: 0.23883 | val_1_rmse: 0.24492 |  0:00:55s
epoch 31 | loss: 0.05774 | val_0_rmse: 0.23876 | val_1_rmse: 0.24364 |  0:00:57s
epoch 32 | loss: 0.05732 | val_0_rmse: 0.23878 | val_1_rmse: 0.24438 |  0:00:59s
epoch 33 | loss: 0.05714 | val_0_rmse: 0.23784 | val_1_rmse: 0.24236 |  0:01:00s
epoch 34 | loss: 0.05749 | val_0_rmse: 0.24051 | val_1_rmse: 0.24693 |  0:01:02s
epoch 35 | loss: 0.05709 | val_0_rmse: 0.24283 | val_1_rmse: 0.24687 |  0:01:04s
epoch 36 | loss: 0.05785 | val_0_rmse: 0.23651 | val_1_rmse: 0.24241 |  0:01:06s
epoch 37 | loss: 0.05693 | val_0_rmse: 0.23564 | val_1_rmse: 0.24182 |  0:01:08s
epoch 38 | loss: 0.05581 | val_0_rmse: 0.23595 | val_1_rmse: 0.24068 |  0:01:09s
epoch 39 | loss: 0.05716 | val_0_rmse: 0.2399  | val_1_rmse: 0.24609 |  0:01:11s
epoch 40 | loss: 0.05659 | val_0_rmse: 0.23632 | val_1_rmse: 0.24239 |  0:01:13s
epoch 41 | loss: 0.05576 | val_0_rmse: 0.23368 | val_1_rmse: 0.23931 |  0:01:15s
epoch 42 | loss: 0.05561 | val_0_rmse: 0.23357 | val_1_rmse: 0.24019 |  0:01:17s
epoch 43 | loss: 0.05556 | val_0_rmse: 0.2336  | val_1_rmse: 0.23987 |  0:01:18s
epoch 44 | loss: 0.05601 | val_0_rmse: 0.2341  | val_1_rmse: 0.24707 |  0:01:20s
epoch 45 | loss: 0.05729 | val_0_rmse: 0.24667 | val_1_rmse: 0.25306 |  0:01:22s
epoch 46 | loss: 0.05744 | val_0_rmse: 0.23239 | val_1_rmse: 0.23761 |  0:01:24s
epoch 47 | loss: 0.05676 | val_0_rmse: 0.23868 | val_1_rmse: 0.24316 |  0:01:26s
epoch 48 | loss: 0.05764 | val_0_rmse: 0.23895 | val_1_rmse: 0.2465  |  0:01:27s
epoch 49 | loss: 0.05733 | val_0_rmse: 0.23388 | val_1_rmse: 0.24128 |  0:01:29s
epoch 50 | loss: 0.05662 | val_0_rmse: 0.23326 | val_1_rmse: 0.23966 |  0:01:31s
epoch 51 | loss: 0.05552 | val_0_rmse: 0.23702 | val_1_rmse: 0.24478 |  0:01:33s
epoch 52 | loss: 0.05487 | val_0_rmse: 0.23214 | val_1_rmse: 0.23915 |  0:01:35s
epoch 53 | loss: 0.05493 | val_0_rmse: 0.23177 | val_1_rmse: 0.23805 |  0:01:36s
epoch 54 | loss: 0.0545  | val_0_rmse: 0.23216 | val_1_rmse: 0.23918 |  0:01:38s
epoch 55 | loss: 0.05438 | val_0_rmse: 0.23082 | val_1_rmse: 0.23906 |  0:01:40s
epoch 56 | loss: 0.05423 | val_0_rmse: 0.2341  | val_1_rmse: 0.2409  |  0:01:42s
epoch 57 | loss: 0.0546  | val_0_rmse: 0.23398 | val_1_rmse: 0.24296 |  0:01:43s
epoch 58 | loss: 0.05614 | val_0_rmse: 0.23824 | val_1_rmse: 0.24472 |  0:01:45s
epoch 59 | loss: 0.05709 | val_0_rmse: 0.23694 | val_1_rmse: 0.24363 |  0:01:47s
epoch 60 | loss: 0.05428 | val_0_rmse: 0.23227 | val_1_rmse: 0.23934 |  0:01:49s
epoch 61 | loss: 0.05476 | val_0_rmse: 0.23125 | val_1_rmse: 0.23803 |  0:01:51s
epoch 62 | loss: 0.05365 | val_0_rmse: 0.22859 | val_1_rmse: 0.23593 |  0:01:52s
epoch 63 | loss: 0.05271 | val_0_rmse: 0.22878 | val_1_rmse: 0.23657 |  0:01:54s
epoch 64 | loss: 0.05335 | val_0_rmse: 0.23394 | val_1_rmse: 0.24263 |  0:01:56s
epoch 65 | loss: 0.05405 | val_0_rmse: 0.22916 | val_1_rmse: 0.23841 |  0:01:58s
epoch 66 | loss: 0.05331 | val_0_rmse: 0.22959 | val_1_rmse: 0.23671 |  0:02:00s
epoch 67 | loss: 0.05302 | val_0_rmse: 0.22788 | val_1_rmse: 0.23619 |  0:02:01s
epoch 68 | loss: 0.05284 | val_0_rmse: 0.22999 | val_1_rmse: 0.23618 |  0:02:03s
epoch 69 | loss: 0.05314 | val_0_rmse: 0.23191 | val_1_rmse: 0.23826 |  0:02:05s
epoch 70 | loss: 0.0539  | val_0_rmse: 0.22771 | val_1_rmse: 0.23602 |  0:02:07s
epoch 71 | loss: 0.0528  | val_0_rmse: 0.23459 | val_1_rmse: 0.2435  |  0:02:09s
epoch 72 | loss: 0.05345 | val_0_rmse: 0.22829 | val_1_rmse: 0.23727 |  0:02:10s
epoch 73 | loss: 0.05185 | val_0_rmse: 0.2267  | val_1_rmse: 0.23483 |  0:02:12s
epoch 74 | loss: 0.05166 | val_0_rmse: 0.22728 | val_1_rmse: 0.23645 |  0:02:14s
epoch 75 | loss: 0.05253 | val_0_rmse: 0.2268  | val_1_rmse: 0.23573 |  0:02:16s
epoch 76 | loss: 0.05186 | val_0_rmse: 0.22516 | val_1_rmse: 0.23443 |  0:02:17s
epoch 77 | loss: 0.05214 | val_0_rmse: 0.22633 | val_1_rmse: 0.23681 |  0:02:19s
epoch 78 | loss: 0.05132 | val_0_rmse: 0.22509 | val_1_rmse: 0.23587 |  0:02:21s
epoch 79 | loss: 0.0516  | val_0_rmse: 0.23022 | val_1_rmse: 0.23891 |  0:02:23s
epoch 80 | loss: 0.05214 | val_0_rmse: 0.22605 | val_1_rmse: 0.23536 |  0:02:25s
epoch 81 | loss: 0.05151 | val_0_rmse: 0.22334 | val_1_rmse: 0.23366 |  0:02:26s
epoch 82 | loss: 0.05086 | val_0_rmse: 0.22468 | val_1_rmse: 0.23505 |  0:02:28s
epoch 83 | loss: 0.05119 | val_0_rmse: 0.22692 | val_1_rmse: 0.24015 |  0:02:30s
epoch 84 | loss: 0.05153 | val_0_rmse: 0.22411 | val_1_rmse: 0.23484 |  0:02:32s
epoch 85 | loss: 0.05157 | val_0_rmse: 0.22391 | val_1_rmse: 0.23559 |  0:02:34s
epoch 86 | loss: 0.0506  | val_0_rmse: 0.22376 | val_1_rmse: 0.23352 |  0:02:35s
epoch 87 | loss: 0.05119 | val_0_rmse: 0.22335 | val_1_rmse: 0.23539 |  0:02:37s
epoch 88 | loss: 0.05123 | val_0_rmse: 0.22515 | val_1_rmse: 0.23466 |  0:02:39s
epoch 89 | loss: 0.05102 | val_0_rmse: 0.22535 | val_1_rmse: 0.23666 |  0:02:41s
epoch 90 | loss: 0.05157 | val_0_rmse: 0.22686 | val_1_rmse: 0.24071 |  0:02:43s
epoch 91 | loss: 0.05107 | val_0_rmse: 0.2226  | val_1_rmse: 0.23424 |  0:02:44s
epoch 92 | loss: 0.05102 | val_0_rmse: 0.22255 | val_1_rmse: 0.23458 |  0:02:46s
epoch 93 | loss: 0.05044 | val_0_rmse: 0.22691 | val_1_rmse: 0.23862 |  0:02:48s
epoch 94 | loss: 0.05114 | val_0_rmse: 0.22244 | val_1_rmse: 0.23386 |  0:02:50s
epoch 95 | loss: 0.05031 | val_0_rmse: 0.2227  | val_1_rmse: 0.23511 |  0:02:52s
epoch 96 | loss: 0.05076 | val_0_rmse: 0.22217 | val_1_rmse: 0.23289 |  0:02:53s
epoch 97 | loss: 0.05059 | val_0_rmse: 0.22968 | val_1_rmse: 0.24015 |  0:02:55s
epoch 98 | loss: 0.0504  | val_0_rmse: 0.22121 | val_1_rmse: 0.2343  |  0:02:57s
epoch 99 | loss: 0.05009 | val_0_rmse: 0.2226  | val_1_rmse: 0.23552 |  0:02:59s
epoch 100| loss: 0.05064 | val_0_rmse: 0.22173 | val_1_rmse: 0.23463 |  0:03:00s
epoch 101| loss: 0.05025 | val_0_rmse: 0.22402 | val_1_rmse: 0.23566 |  0:03:02s
epoch 102| loss: 0.05124 | val_0_rmse: 0.22276 | val_1_rmse: 0.23345 |  0:03:04s
epoch 103| loss: 0.05014 | val_0_rmse: 0.22153 | val_1_rmse: 0.23366 |  0:03:06s
epoch 104| loss: 0.04983 | val_0_rmse: 0.22141 | val_1_rmse: 0.23188 |  0:03:08s
epoch 105| loss: 0.04973 | val_0_rmse: 0.22165 | val_1_rmse: 0.23178 |  0:03:09s
epoch 106| loss: 0.05045 | val_0_rmse: 0.22385 | val_1_rmse: 0.23608 |  0:03:11s
epoch 107| loss: 0.05002 | val_0_rmse: 0.22168 | val_1_rmse: 0.23538 |  0:03:13s
epoch 108| loss: 0.05447 | val_0_rmse: 0.23147 | val_1_rmse: 0.23707 |  0:03:15s
epoch 109| loss: 0.05447 | val_0_rmse: 0.23036 | val_1_rmse: 0.2398  |  0:03:17s
epoch 110| loss: 0.0527  | val_0_rmse: 0.22753 | val_1_rmse: 0.23501 |  0:03:18s
epoch 111| loss: 0.05183 | val_0_rmse: 0.22588 | val_1_rmse: 0.23391 |  0:03:20s
epoch 112| loss: 0.05178 | val_0_rmse: 0.22602 | val_1_rmse: 0.2356  |  0:03:22s
epoch 113| loss: 0.05101 | val_0_rmse: 0.22166 | val_1_rmse: 0.23124 |  0:03:24s
epoch 114| loss: 0.05163 | val_0_rmse: 0.22602 | val_1_rmse: 0.23709 |  0:03:26s
epoch 115| loss: 0.05106 | val_0_rmse: 0.22183 | val_1_rmse: 0.23182 |  0:03:27s
epoch 116| loss: 0.0507  | val_0_rmse: 0.22122 | val_1_rmse: 0.23271 |  0:03:29s
epoch 117| loss: 0.0503  | val_0_rmse: 0.22132 | val_1_rmse: 0.23145 |  0:03:31s
epoch 118| loss: 0.04962 | val_0_rmse: 0.22685 | val_1_rmse: 0.23902 |  0:03:33s
epoch 119| loss: 0.05037 | val_0_rmse: 0.22149 | val_1_rmse: 0.23189 |  0:03:35s
epoch 120| loss: 0.04939 | val_0_rmse: 0.22085 | val_1_rmse: 0.23111 |  0:03:36s
epoch 121| loss: 0.04963 | val_0_rmse: 0.21977 | val_1_rmse: 0.23245 |  0:03:38s
epoch 122| loss: 0.04945 | val_0_rmse: 0.22111 | val_1_rmse: 0.23194 |  0:03:40s
epoch 123| loss: 0.04945 | val_0_rmse: 0.21951 | val_1_rmse: 0.2319  |  0:03:42s
epoch 124| loss: 0.04907 | val_0_rmse: 0.21904 | val_1_rmse: 0.23229 |  0:03:44s
epoch 125| loss: 0.04918 | val_0_rmse: 0.2278  | val_1_rmse: 0.2383  |  0:03:45s
epoch 126| loss: 0.0512  | val_0_rmse: 0.21921 | val_1_rmse: 0.23383 |  0:03:47s
epoch 127| loss: 0.05079 | val_0_rmse: 0.22295 | val_1_rmse: 0.23679 |  0:03:49s
epoch 128| loss: 0.04922 | val_0_rmse: 0.21743 | val_1_rmse: 0.23232 |  0:03:51s
epoch 129| loss: 0.04911 | val_0_rmse: 0.21774 | val_1_rmse: 0.23287 |  0:03:52s
epoch 130| loss: 0.04871 | val_0_rmse: 0.2177  | val_1_rmse: 0.23193 |  0:03:54s
epoch 131| loss: 0.04933 | val_0_rmse: 0.21822 | val_1_rmse: 0.23063 |  0:03:56s
epoch 132| loss: 0.0493  | val_0_rmse: 0.22334 | val_1_rmse: 0.23805 |  0:03:58s
epoch 133| loss: 0.04866 | val_0_rmse: 0.21847 | val_1_rmse: 0.23331 |  0:04:00s
epoch 134| loss: 0.04875 | val_0_rmse: 0.21615 | val_1_rmse: 0.23288 |  0:04:01s
epoch 135| loss: 0.0482  | val_0_rmse: 0.21607 | val_1_rmse: 0.23242 |  0:04:03s
epoch 136| loss: 0.0488  | val_0_rmse: 0.21866 | val_1_rmse: 0.23442 |  0:04:05s
epoch 137| loss: 0.04847 | val_0_rmse: 0.21593 | val_1_rmse: 0.23159 |  0:04:07s
epoch 138| loss: 0.04796 | val_0_rmse: 0.21517 | val_1_rmse: 0.2297  |  0:04:09s
epoch 139| loss: 0.04811 | val_0_rmse: 0.215   | val_1_rmse: 0.23084 |  0:04:10s
epoch 140| loss: 0.04794 | val_0_rmse: 0.21452 | val_1_rmse: 0.23159 |  0:04:12s
epoch 141| loss: 0.04752 | val_0_rmse: 0.21519 | val_1_rmse: 0.23165 |  0:04:14s
epoch 142| loss: 0.04751 | val_0_rmse: 0.21529 | val_1_rmse: 0.23104 |  0:04:16s
epoch 143| loss: 0.04783 | val_0_rmse: 0.21431 | val_1_rmse: 0.23014 |  0:04:18s
epoch 144| loss: 0.0475  | val_0_rmse: 0.21616 | val_1_rmse: 0.23415 |  0:04:19s
epoch 145| loss: 0.04891 | val_0_rmse: 0.23209 | val_1_rmse: 0.24262 |  0:04:21s
epoch 146| loss: 0.04876 | val_0_rmse: 0.22233 | val_1_rmse: 0.23796 |  0:04:23s
epoch 147| loss: 0.04926 | val_0_rmse: 0.21686 | val_1_rmse: 0.22968 |  0:04:25s
epoch 148| loss: 0.04799 | val_0_rmse: 0.21933 | val_1_rmse: 0.23305 |  0:04:27s
epoch 149| loss: 0.04866 | val_0_rmse: 0.21593 | val_1_rmse: 0.23037 |  0:04:28s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.22968
Best weights from best epoch are automatically used!
ended training at: 20:49:48
Feature importance:
Mean squared error is of 0.05725688692473276
Mean absolute error:0.1563900264390378
MAPE:0.16964720481601886
R2 score:0.23760653844952284
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:49:48
epoch 0  | loss: 0.63668 | val_0_rmse: 0.34151 | val_1_rmse: 0.3233  |  0:00:01s
epoch 1  | loss: 0.08898 | val_0_rmse: 0.30129 | val_1_rmse: 0.28137 |  0:00:03s
epoch 2  | loss: 0.07846 | val_0_rmse: 0.27677 | val_1_rmse: 0.25903 |  0:00:05s
epoch 3  | loss: 0.07577 | val_0_rmse: 0.26855 | val_1_rmse: 0.24779 |  0:00:07s
epoch 4  | loss: 0.07382 | val_0_rmse: 0.27007 | val_1_rmse: 0.25093 |  0:00:08s
epoch 5  | loss: 0.07234 | val_0_rmse: 0.26733 | val_1_rmse: 0.24768 |  0:00:10s
epoch 6  | loss: 0.07199 | val_0_rmse: 0.26646 | val_1_rmse: 0.24578 |  0:00:12s
epoch 7  | loss: 0.0721  | val_0_rmse: 0.27058 | val_1_rmse: 0.25155 |  0:00:14s
epoch 8  | loss: 0.07127 | val_0_rmse: 0.26789 | val_1_rmse: 0.24686 |  0:00:16s
epoch 9  | loss: 0.07173 | val_0_rmse: 0.2729  | val_1_rmse: 0.25548 |  0:00:17s
epoch 10 | loss: 0.07091 | val_0_rmse: 0.26462 | val_1_rmse: 0.24409 |  0:00:19s
epoch 11 | loss: 0.07035 | val_0_rmse: 0.26311 | val_1_rmse: 0.24357 |  0:00:21s
epoch 12 | loss: 0.07029 | val_0_rmse: 0.26326 | val_1_rmse: 0.24369 |  0:00:23s
epoch 13 | loss: 0.06964 | val_0_rmse: 0.26296 | val_1_rmse: 0.24338 |  0:00:25s
epoch 14 | loss: 0.06992 | val_0_rmse: 0.26827 | val_1_rmse: 0.2476  |  0:00:26s
epoch 15 | loss: 0.07023 | val_0_rmse: 0.26585 | val_1_rmse: 0.24518 |  0:00:28s
epoch 16 | loss: 0.06974 | val_0_rmse: 0.26509 | val_1_rmse: 0.24458 |  0:00:30s
epoch 17 | loss: 0.06996 | val_0_rmse: 0.26288 | val_1_rmse: 0.24375 |  0:00:32s
epoch 18 | loss: 0.06956 | val_0_rmse: 0.26445 | val_1_rmse: 0.24404 |  0:00:34s
epoch 19 | loss: 0.0697  | val_0_rmse: 0.2639  | val_1_rmse: 0.24308 |  0:00:35s
epoch 20 | loss: 0.07047 | val_0_rmse: 0.26373 | val_1_rmse: 0.24471 |  0:00:37s
epoch 21 | loss: 0.07039 | val_0_rmse: 0.26255 | val_1_rmse: 0.24331 |  0:00:39s
epoch 22 | loss: 0.06989 | val_0_rmse: 0.27175 | val_1_rmse: 0.25408 |  0:00:41s
epoch 23 | loss: 0.07231 | val_0_rmse: 0.26529 | val_1_rmse: 0.24692 |  0:00:43s
epoch 24 | loss: 0.07178 | val_0_rmse: 0.2649  | val_1_rmse: 0.24476 |  0:00:44s
epoch 25 | loss: 0.06978 | val_0_rmse: 0.2629  | val_1_rmse: 0.2439  |  0:00:46s
epoch 26 | loss: 0.06934 | val_0_rmse: 0.26748 | val_1_rmse: 0.24716 |  0:00:48s
epoch 27 | loss: 0.07095 | val_0_rmse: 0.26797 | val_1_rmse: 0.24972 |  0:00:50s
epoch 28 | loss: 0.07064 | val_0_rmse: 0.27027 | val_1_rmse: 0.25005 |  0:00:52s
epoch 29 | loss: 0.07021 | val_0_rmse: 0.26723 | val_1_rmse: 0.25004 |  0:00:53s
epoch 30 | loss: 0.06926 | val_0_rmse: 0.26063 | val_1_rmse: 0.24116 |  0:00:55s
epoch 31 | loss: 0.06848 | val_0_rmse: 0.26065 | val_1_rmse: 0.24084 |  0:00:57s
epoch 32 | loss: 0.06794 | val_0_rmse: 0.25691 | val_1_rmse: 0.23906 |  0:00:59s
epoch 33 | loss: 0.06645 | val_0_rmse: 0.25296 | val_1_rmse: 0.23507 |  0:01:01s
epoch 34 | loss: 0.0653  | val_0_rmse: 0.25546 | val_1_rmse: 0.23733 |  0:01:02s
epoch 35 | loss: 0.06401 | val_0_rmse: 0.24952 | val_1_rmse: 0.23227 |  0:01:04s
epoch 36 | loss: 0.06225 | val_0_rmse: 0.24843 | val_1_rmse: 0.23208 |  0:01:06s
epoch 37 | loss: 0.06186 | val_0_rmse: 0.24378 | val_1_rmse: 0.22838 |  0:01:08s
epoch 38 | loss: 0.06186 | val_0_rmse: 0.24659 | val_1_rmse: 0.23218 |  0:01:09s
epoch 39 | loss: 0.06051 | val_0_rmse: 0.24304 | val_1_rmse: 0.2289  |  0:01:11s
epoch 40 | loss: 0.05984 | val_0_rmse: 0.24731 | val_1_rmse: 0.23493 |  0:01:13s
epoch 41 | loss: 0.06002 | val_0_rmse: 0.24329 | val_1_rmse: 0.22877 |  0:01:15s
epoch 42 | loss: 0.05989 | val_0_rmse: 0.2396  | val_1_rmse: 0.22577 |  0:01:17s
epoch 43 | loss: 0.0582  | val_0_rmse: 0.23974 | val_1_rmse: 0.22817 |  0:01:19s
epoch 44 | loss: 0.05801 | val_0_rmse: 0.23817 | val_1_rmse: 0.22598 |  0:01:20s
epoch 45 | loss: 0.05797 | val_0_rmse: 0.23744 | val_1_rmse: 0.22774 |  0:01:22s
epoch 46 | loss: 0.05678 | val_0_rmse: 0.24183 | val_1_rmse: 0.23234 |  0:01:24s
epoch 47 | loss: 0.05751 | val_0_rmse: 0.2491  | val_1_rmse: 0.24091 |  0:01:26s
epoch 48 | loss: 0.05783 | val_0_rmse: 0.2418  | val_1_rmse: 0.23245 |  0:01:27s
epoch 49 | loss: 0.05822 | val_0_rmse: 0.23295 | val_1_rmse: 0.22845 |  0:01:29s
epoch 50 | loss: 0.05614 | val_0_rmse: 0.23386 | val_1_rmse: 0.22302 |  0:01:31s
epoch 51 | loss: 0.05605 | val_0_rmse: 0.22974 | val_1_rmse: 0.22624 |  0:01:33s
epoch 52 | loss: 0.05593 | val_0_rmse: 0.23393 | val_1_rmse: 0.22573 |  0:01:35s
epoch 53 | loss: 0.05463 | val_0_rmse: 0.2298  | val_1_rmse: 0.22316 |  0:01:36s
epoch 54 | loss: 0.05398 | val_0_rmse: 0.22966 | val_1_rmse: 0.22484 |  0:01:38s
epoch 55 | loss: 0.05323 | val_0_rmse: 0.2245  | val_1_rmse: 0.2221  |  0:01:40s
epoch 56 | loss: 0.05307 | val_0_rmse: 0.33978 | val_1_rmse: 0.34195 |  0:01:42s
epoch 57 | loss: 0.05653 | val_0_rmse: 0.24006 | val_1_rmse: 0.2317  |  0:01:44s
epoch 58 | loss: 0.05483 | val_0_rmse: 0.22573 | val_1_rmse: 0.22593 |  0:01:45s
epoch 59 | loss: 0.05261 | val_0_rmse: 0.22344 | val_1_rmse: 0.22715 |  0:01:47s
epoch 60 | loss: 0.05277 | val_0_rmse: 0.22142 | val_1_rmse: 0.23041 |  0:01:49s
epoch 61 | loss: 0.05274 | val_0_rmse: 0.22282 | val_1_rmse: 0.22834 |  0:01:51s
epoch 62 | loss: 0.0542  | val_0_rmse: 0.22423 | val_1_rmse: 0.21744 |  0:01:53s
epoch 63 | loss: 0.05341 | val_0_rmse: 0.22253 | val_1_rmse: 0.23028 |  0:01:54s
epoch 64 | loss: 0.05544 | val_0_rmse: 0.22849 | val_1_rmse: 0.2327  |  0:01:56s
epoch 65 | loss: 0.04993 | val_0_rmse: 0.23427 | val_1_rmse: 0.2233  |  0:01:58s
epoch 66 | loss: 0.05145 | val_0_rmse: 0.21958 | val_1_rmse: 0.22703 |  0:02:00s
epoch 67 | loss: 0.04927 | val_0_rmse: 0.26144 | val_1_rmse: 0.2715  |  0:02:02s
epoch 68 | loss: 0.05114 | val_0_rmse: 0.2217  | val_1_rmse: 0.22683 |  0:02:03s
epoch 69 | loss: 0.05088 | val_0_rmse: 0.26312 | val_1_rmse: 0.27533 |  0:02:05s
epoch 70 | loss: 0.05073 | val_0_rmse: 0.26931 | val_1_rmse: 0.31464 |  0:02:07s
epoch 71 | loss: 0.06243 | val_0_rmse: 0.24227 | val_1_rmse: 0.23037 |  0:02:09s
epoch 72 | loss: 0.05739 | val_0_rmse: 0.25121 | val_1_rmse: 0.2209  |  0:02:10s
epoch 73 | loss: 0.05662 | val_0_rmse: 0.2346  | val_1_rmse: 0.22344 |  0:02:12s
epoch 74 | loss: 0.05684 | val_0_rmse: 0.23434 | val_1_rmse: 0.22088 |  0:02:14s
epoch 75 | loss: 0.05568 | val_0_rmse: 0.24133 | val_1_rmse: 0.22589 |  0:02:16s
epoch 76 | loss: 0.05566 | val_0_rmse: 0.23821 | val_1_rmse: 0.22478 |  0:02:18s
epoch 77 | loss: 0.05576 | val_0_rmse: 0.23197 | val_1_rmse: 0.21741 |  0:02:19s
epoch 78 | loss: 0.05508 | val_0_rmse: 0.23365 | val_1_rmse: 0.2173  |  0:02:21s
epoch 79 | loss: 0.05473 | val_0_rmse: 0.23369 | val_1_rmse: 0.22093 |  0:02:23s
epoch 80 | loss: 0.05469 | val_0_rmse: 0.24403 | val_1_rmse: 0.2299  |  0:02:25s
epoch 81 | loss: 0.0547  | val_0_rmse: 0.23253 | val_1_rmse: 0.22017 |  0:02:27s
epoch 82 | loss: 0.0538  | val_0_rmse: 0.22785 | val_1_rmse: 0.2145  |  0:02:28s
epoch 83 | loss: 0.05374 | val_0_rmse: 0.22569 | val_1_rmse: 0.21442 |  0:02:30s
epoch 84 | loss: 0.05242 | val_0_rmse: 0.22485 | val_1_rmse: 0.21456 |  0:02:32s
epoch 85 | loss: 0.0518  | val_0_rmse: 0.22487 | val_1_rmse: 0.21503 |  0:02:34s
epoch 86 | loss: 0.05102 | val_0_rmse: 0.22172 | val_1_rmse: 0.21563 |  0:02:36s
epoch 87 | loss: 0.05088 | val_0_rmse: 0.22425 | val_1_rmse: 0.21699 |  0:02:37s
epoch 88 | loss: 0.05091 | val_0_rmse: 0.23456 | val_1_rmse: 0.22536 |  0:02:39s
epoch 89 | loss: 0.04996 | val_0_rmse: 0.2165  | val_1_rmse: 0.21531 |  0:02:41s
epoch 90 | loss: 0.05277 | val_0_rmse: 0.31275 | val_1_rmse: 0.32698 |  0:02:43s
epoch 91 | loss: 0.05124 | val_0_rmse: 0.22204 | val_1_rmse: 0.21625 |  0:02:45s
epoch 92 | loss: 0.04951 | val_0_rmse: 0.21273 | val_1_rmse: 0.23718 |  0:02:46s
epoch 93 | loss: 0.05151 | val_0_rmse: 0.23049 | val_1_rmse: 0.24152 |  0:02:48s
epoch 94 | loss: 0.04985 | val_0_rmse: 0.22584 | val_1_rmse: 0.2266  |  0:02:50s
epoch 95 | loss: 0.05026 | val_0_rmse: 0.24405 | val_1_rmse: 0.22956 |  0:02:52s
epoch 96 | loss: 0.04901 | val_0_rmse: 0.21642 | val_1_rmse: 0.21836 |  0:02:53s
epoch 97 | loss: 0.04809 | val_0_rmse: 0.21693 | val_1_rmse: 0.22935 |  0:02:55s
epoch 98 | loss: 0.04786 | val_0_rmse: 0.20934 | val_1_rmse: 0.23564 |  0:02:57s
epoch 99 | loss: 0.04944 | val_0_rmse: 0.23521 | val_1_rmse: 0.22581 |  0:02:59s
epoch 100| loss: 0.05023 | val_0_rmse: 0.22584 | val_1_rmse: 0.22543 |  0:03:01s
epoch 101| loss: 0.04678 | val_0_rmse: 0.21988 | val_1_rmse: 0.24256 |  0:03:02s
epoch 102| loss: 0.04866 | val_0_rmse: 0.21215 | val_1_rmse: 0.21875 |  0:03:04s
epoch 103| loss: 0.04619 | val_0_rmse: 0.22051 | val_1_rmse: 0.22264 |  0:03:06s
epoch 104| loss: 0.04835 | val_0_rmse: 0.21902 | val_1_rmse: 0.22672 |  0:03:08s
epoch 105| loss: 0.045   | val_0_rmse: 0.21539 | val_1_rmse: 0.22816 |  0:03:10s
epoch 106| loss: 0.0471  | val_0_rmse: 0.25038 | val_1_rmse: 0.2889  |  0:03:11s
epoch 107| loss: 0.04599 | val_0_rmse: 0.22248 | val_1_rmse: 0.24769 |  0:03:13s
epoch 108| loss: 0.04515 | val_0_rmse: 0.22224 | val_1_rmse: 0.24206 |  0:03:15s
epoch 109| loss: 0.04517 | val_0_rmse: 0.22148 | val_1_rmse: 0.2232  |  0:03:17s
epoch 110| loss: 0.04483 | val_0_rmse: 0.28643 | val_1_rmse: 0.34373 |  0:03:18s
epoch 111| loss: 0.04562 | val_0_rmse: 0.22032 | val_1_rmse: 0.21887 |  0:03:20s
epoch 112| loss: 0.04829 | val_0_rmse: 0.20738 | val_1_rmse: 0.23507 |  0:03:22s
epoch 113| loss: 0.045   | val_0_rmse: 0.25349 | val_1_rmse: 0.33621 |  0:03:24s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.21442
Best weights from best epoch are automatically used!
ended training at: 20:53:13
Feature importance:
Mean squared error is of 0.058830405183676764
Mean absolute error:0.15431152033271958
MAPE:0.17124017207886716
R2 score:0.2188451263240564
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:53:14
epoch 0  | loss: 0.83389 | val_0_rmse: 0.42227 | val_1_rmse: 0.42089 |  0:00:01s
epoch 1  | loss: 0.08447 | val_0_rmse: 0.26419 | val_1_rmse: 0.26943 |  0:00:03s
epoch 2  | loss: 0.07307 | val_0_rmse: 0.262   | val_1_rmse: 0.26686 |  0:00:05s
epoch 3  | loss: 0.07115 | val_0_rmse: 0.26077 | val_1_rmse: 0.26469 |  0:00:07s
epoch 4  | loss: 0.07039 | val_0_rmse: 0.26122 | val_1_rmse: 0.26556 |  0:00:08s
epoch 5  | loss: 0.07047 | val_0_rmse: 0.26948 | val_1_rmse: 0.27252 |  0:00:10s
epoch 6  | loss: 0.07006 | val_0_rmse: 0.26036 | val_1_rmse: 0.2666  |  0:00:12s
epoch 7  | loss: 0.06833 | val_0_rmse: 0.25726 | val_1_rmse: 0.26287 |  0:00:14s
epoch 8  | loss: 0.06898 | val_0_rmse: 0.25861 | val_1_rmse: 0.26372 |  0:00:16s
epoch 9  | loss: 0.06598 | val_0_rmse: 0.25786 | val_1_rmse: 0.26325 |  0:00:17s
epoch 10 | loss: 0.066   | val_0_rmse: 0.25315 | val_1_rmse: 0.25943 |  0:00:19s
epoch 11 | loss: 0.06391 | val_0_rmse: 0.2581  | val_1_rmse: 0.26458 |  0:00:21s
epoch 12 | loss: 0.06288 | val_0_rmse: 0.25306 | val_1_rmse: 0.25802 |  0:00:23s
epoch 13 | loss: 0.06065 | val_0_rmse: 0.24842 | val_1_rmse: 0.25416 |  0:00:24s
epoch 14 | loss: 0.06052 | val_0_rmse: 0.24983 | val_1_rmse: 0.25492 |  0:00:26s
epoch 15 | loss: 0.06157 | val_0_rmse: 0.24192 | val_1_rmse: 0.25056 |  0:00:28s
epoch 16 | loss: 0.0612  | val_0_rmse: 0.24141 | val_1_rmse: 0.24621 |  0:00:30s
epoch 17 | loss: 0.05884 | val_0_rmse: 0.23803 | val_1_rmse: 0.24307 |  0:00:32s
epoch 18 | loss: 0.05793 | val_0_rmse: 0.23956 | val_1_rmse: 0.24438 |  0:00:33s
epoch 19 | loss: 0.05815 | val_0_rmse: 0.23845 | val_1_rmse: 0.24328 |  0:00:35s
epoch 20 | loss: 0.05852 | val_0_rmse: 0.23832 | val_1_rmse: 0.24256 |  0:00:37s
epoch 21 | loss: 0.05878 | val_0_rmse: 0.24084 | val_1_rmse: 0.24556 |  0:00:39s
epoch 22 | loss: 0.05765 | val_0_rmse: 0.2478  | val_1_rmse: 0.25159 |  0:00:41s
epoch 23 | loss: 0.05931 | val_0_rmse: 0.24065 | val_1_rmse: 0.24406 |  0:00:42s
epoch 24 | loss: 0.05844 | val_0_rmse: 0.23922 | val_1_rmse: 0.24322 |  0:00:44s
epoch 25 | loss: 0.05786 | val_0_rmse: 0.23925 | val_1_rmse: 0.24322 |  0:00:46s
epoch 26 | loss: 0.05834 | val_0_rmse: 0.23766 | val_1_rmse: 0.24066 |  0:00:48s
epoch 27 | loss: 0.05781 | val_0_rmse: 0.24044 | val_1_rmse: 0.24222 |  0:00:49s
epoch 28 | loss: 0.05825 | val_0_rmse: 0.24365 | val_1_rmse: 0.24746 |  0:00:51s
epoch 29 | loss: 0.05805 | val_0_rmse: 0.24741 | val_1_rmse: 0.25132 |  0:00:53s
epoch 30 | loss: 0.05874 | val_0_rmse: 0.23832 | val_1_rmse: 0.243   |  0:00:55s
epoch 31 | loss: 0.05804 | val_0_rmse: 0.2448  | val_1_rmse: 0.24896 |  0:00:57s
epoch 32 | loss: 0.05784 | val_0_rmse: 0.23897 | val_1_rmse: 0.2443  |  0:00:58s
epoch 33 | loss: 0.05863 | val_0_rmse: 0.23846 | val_1_rmse: 0.24359 |  0:01:00s
epoch 34 | loss: 0.058   | val_0_rmse: 0.23816 | val_1_rmse: 0.24231 |  0:01:02s
epoch 35 | loss: 0.05788 | val_0_rmse: 0.23906 | val_1_rmse: 0.24438 |  0:01:04s
epoch 36 | loss: 0.05772 | val_0_rmse: 0.23972 | val_1_rmse: 0.24504 |  0:01:06s
epoch 37 | loss: 0.05765 | val_0_rmse: 0.24299 | val_1_rmse: 0.247   |  0:01:07s
epoch 38 | loss: 0.0584  | val_0_rmse: 0.24154 | val_1_rmse: 0.24562 |  0:01:09s
epoch 39 | loss: 0.0587  | val_0_rmse: 0.23892 | val_1_rmse: 0.24312 |  0:01:11s
epoch 40 | loss: 0.05769 | val_0_rmse: 0.2385  | val_1_rmse: 0.24301 |  0:01:13s
epoch 41 | loss: 0.05784 | val_0_rmse: 0.23714 | val_1_rmse: 0.24296 |  0:01:14s
epoch 42 | loss: 0.05701 | val_0_rmse: 0.2403  | val_1_rmse: 0.24574 |  0:01:16s
epoch 43 | loss: 0.05729 | val_0_rmse: 0.2372  | val_1_rmse: 0.24391 |  0:01:18s
epoch 44 | loss: 0.05692 | val_0_rmse: 0.23829 | val_1_rmse: 0.24417 |  0:01:20s
epoch 45 | loss: 0.05725 | val_0_rmse: 0.23839 | val_1_rmse: 0.24312 |  0:01:22s
epoch 46 | loss: 0.0578  | val_0_rmse: 0.24625 | val_1_rmse: 0.25144 |  0:01:23s
epoch 47 | loss: 0.05708 | val_0_rmse: 0.23636 | val_1_rmse: 0.24228 |  0:01:25s
epoch 48 | loss: 0.05674 | val_0_rmse: 0.24313 | val_1_rmse: 0.24915 |  0:01:27s
epoch 49 | loss: 0.05667 | val_0_rmse: 0.24332 | val_1_rmse: 0.25135 |  0:01:29s
epoch 50 | loss: 0.0575  | val_0_rmse: 0.24395 | val_1_rmse: 0.2498  |  0:01:30s
epoch 51 | loss: 0.05762 | val_0_rmse: 0.23588 | val_1_rmse: 0.24265 |  0:01:32s
epoch 52 | loss: 0.0566  | val_0_rmse: 0.23502 | val_1_rmse: 0.24173 |  0:01:34s
epoch 53 | loss: 0.05601 | val_0_rmse: 0.23451 | val_1_rmse: 0.24169 |  0:01:36s
epoch 54 | loss: 0.05549 | val_0_rmse: 0.2355  | val_1_rmse: 0.24249 |  0:01:38s
epoch 55 | loss: 0.05653 | val_0_rmse: 0.23593 | val_1_rmse: 0.24251 |  0:01:39s
epoch 56 | loss: 0.05698 | val_0_rmse: 0.23579 | val_1_rmse: 0.24235 |  0:01:41s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.24066
Best weights from best epoch are automatically used!
ended training at: 20:54:57
Feature importance:
Mean squared error is of 0.06270979835834783
Mean absolute error:0.15941890807995496
MAPE:0.1803834342543484
R2 score:0.18713141593169513
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:54:59
epoch 0  | loss: 0.30928 | val_0_rmse: 0.2765  | val_1_rmse: 0.27384 |  0:00:06s
epoch 1  | loss: 0.07888 | val_0_rmse: 0.27215 | val_1_rmse: 0.27003 |  0:00:13s
epoch 2  | loss: 0.07713 | val_0_rmse: 0.26979 | val_1_rmse: 0.26703 |  0:00:20s
epoch 3  | loss: 0.07674 | val_0_rmse: 0.27012 | val_1_rmse: 0.26753 |  0:00:27s
epoch 4  | loss: 0.07607 | val_0_rmse: 0.274   | val_1_rmse: 0.27096 |  0:00:34s
epoch 5  | loss: 0.07495 | val_0_rmse: 0.29387 | val_1_rmse: 0.29066 |  0:00:40s
epoch 6  | loss: 0.07632 | val_0_rmse: 0.27083 | val_1_rmse: 0.26894 |  0:00:47s
epoch 7  | loss: 0.07427 | val_0_rmse: 0.27144 | val_1_rmse: 0.26969 |  0:00:55s
epoch 8  | loss: 0.07462 | val_0_rmse: 0.26956 | val_1_rmse: 0.26753 |  0:01:01s
epoch 9  | loss: 0.07373 | val_0_rmse: 0.27105 | val_1_rmse: 0.26887 |  0:01:08s
epoch 10 | loss: 0.07372 | val_0_rmse: 0.27019 | val_1_rmse: 0.26797 |  0:01:15s
epoch 11 | loss: 0.07444 | val_0_rmse: 0.2776  | val_1_rmse: 0.27487 |  0:01:22s
epoch 12 | loss: 0.07473 | val_0_rmse: 0.27039 | val_1_rmse: 0.2682  |  0:01:29s
epoch 13 | loss: 0.07307 | val_0_rmse: 0.27231 | val_1_rmse: 0.27049 |  0:01:36s
epoch 14 | loss: 0.07317 | val_0_rmse: 0.27026 | val_1_rmse: 0.26791 |  0:01:42s
epoch 15 | loss: 0.07324 | val_0_rmse: 0.27281 | val_1_rmse: 0.27184 |  0:01:49s
epoch 16 | loss: 0.07314 | val_0_rmse: 0.27059 | val_1_rmse: 0.26998 |  0:01:56s
epoch 17 | loss: 0.07316 | val_0_rmse: 0.2703  | val_1_rmse: 0.26948 |  0:02:03s
epoch 18 | loss: 0.07307 | val_0_rmse: 0.27049 | val_1_rmse: 0.26943 |  0:02:10s
epoch 19 | loss: 0.07319 | val_0_rmse: 0.26889 | val_1_rmse: 0.26696 |  0:02:17s
epoch 20 | loss: 0.07353 | val_0_rmse: 0.29584 | val_1_rmse: 0.30151 |  0:02:23s
epoch 21 | loss: 0.07253 | val_0_rmse: 0.29507 | val_1_rmse: 0.30091 |  0:02:30s
epoch 22 | loss: 0.07248 | val_0_rmse: 0.29787 | val_1_rmse: 0.30982 |  0:02:37s
epoch 23 | loss: 0.07251 | val_0_rmse: 0.28731 | val_1_rmse: 0.29108 |  0:02:44s
epoch 24 | loss: 0.07276 | val_0_rmse: 0.28648 | val_1_rmse: 0.29284 |  0:02:51s
epoch 25 | loss: 0.07314 | val_0_rmse: 0.27235 | val_1_rmse: 0.27222 |  0:02:58s
epoch 26 | loss: 0.07282 | val_0_rmse: 0.27285 | val_1_rmse: 0.27218 |  0:03:04s
epoch 27 | loss: 0.07258 | val_0_rmse: 0.27121 | val_1_rmse: 0.27332 |  0:03:11s
epoch 28 | loss: 0.07258 | val_0_rmse: 0.27661 | val_1_rmse: 0.28731 |  0:03:18s
epoch 29 | loss: 0.07308 | val_0_rmse: 0.27716 | val_1_rmse: 0.27705 |  0:03:25s
epoch 30 | loss: 0.07313 | val_0_rmse: 0.27243 | val_1_rmse: 0.27154 |  0:03:32s
epoch 31 | loss: 0.0727  | val_0_rmse: 0.27404 | val_1_rmse: 0.27347 |  0:03:39s
epoch 32 | loss: 0.07223 | val_0_rmse: 0.27148 | val_1_rmse: 0.27152 |  0:03:45s
epoch 33 | loss: 0.07235 | val_0_rmse: 0.28597 | val_1_rmse: 0.29283 |  0:03:52s
epoch 34 | loss: 0.07384 | val_0_rmse: 0.27516 | val_1_rmse: 0.28118 |  0:03:59s
epoch 35 | loss: 0.07247 | val_0_rmse: 0.27278 | val_1_rmse: 0.27515 |  0:04:06s
epoch 36 | loss: 0.07265 | val_0_rmse: 0.27949 | val_1_rmse: 0.29829 |  0:04:13s
epoch 37 | loss: 0.07204 | val_0_rmse: 0.28723 | val_1_rmse: 0.2815  |  0:04:20s
epoch 38 | loss: 0.07226 | val_0_rmse: 0.27024 | val_1_rmse: 0.27031 |  0:04:26s
epoch 39 | loss: 0.07298 | val_0_rmse: 0.29659 | val_1_rmse: 0.2727  |  0:04:33s
epoch 40 | loss: 0.07209 | val_0_rmse: 0.29264 | val_1_rmse: 0.27051 |  0:04:40s
epoch 41 | loss: 0.07269 | val_0_rmse: 0.27603 | val_1_rmse: 0.27767 |  0:04:47s
epoch 42 | loss: 0.07223 | val_0_rmse: 0.27354 | val_1_rmse: 0.27072 |  0:04:54s
epoch 43 | loss: 0.07212 | val_0_rmse: 0.27993 | val_1_rmse: 0.27439 |  0:05:01s
epoch 44 | loss: 0.07202 | val_0_rmse: 0.27248 | val_1_rmse: 0.27568 |  0:05:07s
epoch 45 | loss: 0.07211 | val_0_rmse: 0.27699 | val_1_rmse: 0.28725 |  0:05:14s
epoch 46 | loss: 0.07221 | val_0_rmse: 0.28229 | val_1_rmse: 0.27908 |  0:05:21s
epoch 47 | loss: 0.07225 | val_0_rmse: 0.27515 | val_1_rmse: 0.28207 |  0:05:28s
epoch 48 | loss: 0.07226 | val_0_rmse: 0.26912 | val_1_rmse: 0.26865 |  0:05:35s
epoch 49 | loss: 0.07207 | val_0_rmse: 0.26976 | val_1_rmse: 0.26921 |  0:05:42s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.26696
Best weights from best epoch are automatically used!
ended training at: 21:00:44
Feature importance:
Mean squared error is of 0.07116025288279235
Mean absolute error:0.2016716994105732
MAPE:0.22583521308155585
R2 score:0.06588390106834185
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:00:46
epoch 0  | loss: 0.34653 | val_0_rmse: 0.30361 | val_1_rmse: 0.29893 |  0:00:06s
epoch 1  | loss: 0.08135 | val_0_rmse: 0.28614 | val_1_rmse: 0.28103 |  0:00:13s
epoch 2  | loss: 0.08224 | val_0_rmse: 0.28162 | val_1_rmse: 0.27679 |  0:00:20s
epoch 3  | loss: 0.08048 | val_0_rmse: 0.28123 | val_1_rmse: 0.27666 |  0:00:27s
epoch 4  | loss: 0.08011 | val_0_rmse: 0.28112 | val_1_rmse: 0.27607 |  0:00:34s
epoch 5  | loss: 0.07981 | val_0_rmse: 0.28479 | val_1_rmse: 0.27984 |  0:00:40s
epoch 6  | loss: 0.08032 | val_0_rmse: 0.27988 | val_1_rmse: 0.275   |  0:00:47s
epoch 7  | loss: 0.08066 | val_0_rmse: 0.27944 | val_1_rmse: 0.27417 |  0:00:54s
epoch 8  | loss: 0.07889 | val_0_rmse: 0.28196 | val_1_rmse: 0.27678 |  0:01:01s
epoch 9  | loss: 0.07832 | val_0_rmse: 0.2776  | val_1_rmse: 0.28074 |  0:01:08s
epoch 10 | loss: 0.07537 | val_0_rmse: 0.2774  | val_1_rmse: 0.27346 |  0:01:15s
epoch 11 | loss: 0.07552 | val_0_rmse: 0.27556 | val_1_rmse: 0.27065 |  0:01:21s
epoch 12 | loss: 0.07485 | val_0_rmse: 0.27072 | val_1_rmse: 0.26663 |  0:01:28s
epoch 13 | loss: 0.0735  | val_0_rmse: 0.26848 | val_1_rmse: 0.26477 |  0:01:35s
epoch 14 | loss: 0.07293 | val_0_rmse: 0.27164 | val_1_rmse: 0.26864 |  0:01:42s
epoch 15 | loss: 0.07284 | val_0_rmse: 0.2673  | val_1_rmse: 0.26463 |  0:01:49s
epoch 16 | loss: 0.07183 | val_0_rmse: 0.2705  | val_1_rmse: 0.26759 |  0:01:55s
epoch 17 | loss: 0.07253 | val_0_rmse: 0.2762  | val_1_rmse: 0.31816 |  0:02:02s
epoch 18 | loss: 0.07123 | val_0_rmse: 0.26424 | val_1_rmse: 0.26165 |  0:02:09s
epoch 19 | loss: 0.07149 | val_0_rmse: 0.26631 | val_1_rmse: 0.26346 |  0:02:16s
epoch 20 | loss: 0.07103 | val_0_rmse: 0.36679 | val_1_rmse: 0.67251 |  0:02:23s
epoch 21 | loss: 0.07229 | val_0_rmse: 0.28618 | val_1_rmse: 0.28249 |  0:02:29s
epoch 22 | loss: 0.07486 | val_0_rmse: 0.27517 | val_1_rmse: 0.27279 |  0:02:36s
epoch 23 | loss: 0.07223 | val_0_rmse: 0.26957 | val_1_rmse: 0.26356 |  0:02:43s
epoch 24 | loss: 0.07098 | val_0_rmse: 0.26547 | val_1_rmse: 0.26345 |  0:02:50s
epoch 25 | loss: 0.07045 | val_0_rmse: 0.26487 | val_1_rmse: 0.26461 |  0:02:57s
epoch 26 | loss: 0.07024 | val_0_rmse: 0.26604 | val_1_rmse: 0.26483 |  0:03:04s
epoch 27 | loss: 0.07048 | val_0_rmse: 0.2706  | val_1_rmse: 0.27057 |  0:03:10s
epoch 28 | loss: 0.07006 | val_0_rmse: 0.26564 | val_1_rmse: 0.26363 |  0:03:17s
epoch 29 | loss: 0.06919 | val_0_rmse: 0.26573 | val_1_rmse: 0.26464 |  0:03:24s
epoch 30 | loss: 0.06935 | val_0_rmse: 0.27133 | val_1_rmse: 0.26894 |  0:03:31s
epoch 31 | loss: 0.06947 | val_0_rmse: 0.26676 | val_1_rmse: 0.27315 |  0:03:38s
epoch 32 | loss: 0.0693  | val_0_rmse: 0.26326 | val_1_rmse: 0.26293 |  0:03:45s
epoch 33 | loss: 0.06959 | val_0_rmse: 0.26485 | val_1_rmse: 0.26502 |  0:03:51s
epoch 34 | loss: 0.06912 | val_0_rmse: 0.26072 | val_1_rmse: 0.26083 |  0:03:58s
epoch 35 | loss: 0.07075 | val_0_rmse: 0.26761 | val_1_rmse: 0.26688 |  0:04:05s
epoch 36 | loss: 0.07185 | val_0_rmse: 0.26572 | val_1_rmse: 0.2629  |  0:04:12s
epoch 37 | loss: 0.0714  | val_0_rmse: 0.26344 | val_1_rmse: 0.26133 |  0:04:19s
epoch 38 | loss: 0.07065 | val_0_rmse: 0.26639 | val_1_rmse: 0.26497 |  0:04:25s
epoch 39 | loss: 0.07333 | val_0_rmse: 0.26732 | val_1_rmse: 0.26717 |  0:04:32s
epoch 40 | loss: 0.0711  | val_0_rmse: 0.26701 | val_1_rmse: 0.26686 |  0:04:39s
epoch 41 | loss: 0.06931 | val_0_rmse: 0.26188 | val_1_rmse: 0.2614  |  0:04:46s
epoch 42 | loss: 0.06901 | val_0_rmse: 0.26578 | val_1_rmse: 0.26629 |  0:04:53s
epoch 43 | loss: 0.06942 | val_0_rmse: 0.26193 | val_1_rmse: 0.26511 |  0:05:00s
epoch 44 | loss: 0.06902 | val_0_rmse: 0.26068 | val_1_rmse: 0.26066 |  0:05:06s
epoch 45 | loss: 0.06894 | val_0_rmse: 0.26253 | val_1_rmse: 0.26555 |  0:05:13s
epoch 46 | loss: 0.06815 | val_0_rmse: 0.25995 | val_1_rmse: 0.26097 |  0:05:20s
epoch 47 | loss: 0.06783 | val_0_rmse: 0.25927 | val_1_rmse: 0.25911 |  0:05:27s
epoch 48 | loss: 0.06813 | val_0_rmse: 0.26055 | val_1_rmse: 0.25897 |  0:05:34s
epoch 49 | loss: 0.06782 | val_0_rmse: 0.26429 | val_1_rmse: 0.27205 |  0:05:41s
epoch 50 | loss: 0.06857 | val_0_rmse: 0.26215 | val_1_rmse: 0.26022 |  0:05:47s
epoch 51 | loss: 0.06836 | val_0_rmse: 0.25881 | val_1_rmse: 0.25811 |  0:05:54s
epoch 52 | loss: 0.06763 | val_0_rmse: 0.31885 | val_1_rmse: 0.37721 |  0:06:01s
epoch 53 | loss: 0.06871 | val_0_rmse: 0.26093 | val_1_rmse: 0.31    |  0:06:08s
epoch 54 | loss: 0.06887 | val_0_rmse: 0.26373 | val_1_rmse: 0.26965 |  0:06:15s
epoch 55 | loss: 0.06844 | val_0_rmse: 0.25961 | val_1_rmse: 0.26527 |  0:06:22s
epoch 56 | loss: 0.06789 | val_0_rmse: 0.25902 | val_1_rmse: 0.26136 |  0:06:28s
epoch 57 | loss: 0.06756 | val_0_rmse: 0.26009 | val_1_rmse: 0.26809 |  0:06:35s
epoch 58 | loss: 0.07133 | val_0_rmse: 0.28217 | val_1_rmse: 0.28209 |  0:06:42s
epoch 59 | loss: 0.07628 | val_0_rmse: 0.273   | val_1_rmse: 0.27052 |  0:06:49s
epoch 60 | loss: 0.07546 | val_0_rmse: 0.2769  | val_1_rmse: 0.27121 |  0:06:56s
epoch 61 | loss: 0.07525 | val_0_rmse: 0.39939 | val_1_rmse: 0.76527 |  0:07:03s
epoch 62 | loss: 0.07395 | val_0_rmse: 0.27245 | val_1_rmse: 0.26938 |  0:07:09s
epoch 63 | loss: 0.07366 | val_0_rmse: 0.47441 | val_1_rmse: 0.61398 |  0:07:16s
epoch 64 | loss: 0.07266 | val_0_rmse: 0.59267 | val_1_rmse: 1.08388 |  0:07:23s
epoch 65 | loss: 0.0762  | val_0_rmse: 0.29175 | val_1_rmse: 0.3271  |  0:07:30s
epoch 66 | loss: 0.08398 | val_0_rmse: 0.28348 | val_1_rmse: 0.29496 |  0:07:37s
epoch 67 | loss: 0.07438 | val_0_rmse: 0.27424 | val_1_rmse: 0.2709  |  0:07:43s
epoch 68 | loss: 0.07473 | val_0_rmse: 0.27512 | val_1_rmse: 0.26927 |  0:07:50s
epoch 69 | loss: 0.07436 | val_0_rmse: 0.27255 | val_1_rmse: 0.27101 |  0:07:57s
epoch 70 | loss: 0.07436 | val_0_rmse: 0.28199 | val_1_rmse: 0.28428 |  0:08:04s
epoch 71 | loss: 0.07424 | val_0_rmse: 0.27566 | val_1_rmse: 0.27938 |  0:08:11s
epoch 72 | loss: 0.07341 | val_0_rmse: 0.2755  | val_1_rmse: 0.27517 |  0:08:18s
epoch 73 | loss: 0.07325 | val_0_rmse: 0.27202 | val_1_rmse: 0.27293 |  0:08:24s
epoch 74 | loss: 0.07269 | val_0_rmse: 0.27242 | val_1_rmse: 0.27521 |  0:08:31s
epoch 75 | loss: 0.07378 | val_0_rmse: 0.27496 | val_1_rmse: 0.36864 |  0:08:38s
epoch 76 | loss: 0.07484 | val_0_rmse: 0.27919 | val_1_rmse: 0.27946 |  0:08:45s
epoch 77 | loss: 0.07769 | val_0_rmse: 0.28133 | val_1_rmse: 0.27707 |  0:08:52s
epoch 78 | loss: 0.07754 | val_0_rmse: 0.27922 | val_1_rmse: 0.27691 |  0:08:59s
epoch 79 | loss: 0.07523 | val_0_rmse: 0.27706 | val_1_rmse: 0.27312 |  0:09:05s
epoch 80 | loss: 0.07445 | val_0_rmse: 0.27812 | val_1_rmse: 0.27856 |  0:09:12s
epoch 81 | loss: 0.07508 | val_0_rmse: 0.27297 | val_1_rmse: 0.27093 |  0:09:19s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.25811
Best weights from best epoch are automatically used!
ended training at: 21:10:08
Feature importance:
Mean squared error is of 0.0667277875320403
Mean absolute error:0.19507969840607328
MAPE:0.21535324821038782
R2 score:0.10344674643834173
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:10:10
epoch 0  | loss: 0.2858  | val_0_rmse: 0.28283 | val_1_rmse: 0.27191 |  0:00:06s
epoch 1  | loss: 0.08193 | val_0_rmse: 0.28494 | val_1_rmse: 0.27326 |  0:00:13s
epoch 2  | loss: 0.08158 | val_0_rmse: 0.28337 | val_1_rmse: 0.27177 |  0:00:20s
epoch 3  | loss: 0.08083 | val_0_rmse: 0.2802  | val_1_rmse: 0.26942 |  0:00:27s
epoch 4  | loss: 0.08082 | val_0_rmse: 0.27756 | val_1_rmse: 0.2669  |  0:00:34s
epoch 5  | loss: 0.08037 | val_0_rmse: 0.27653 | val_1_rmse: 0.26607 |  0:00:40s
epoch 6  | loss: 0.07939 | val_0_rmse: 0.28102 | val_1_rmse: 0.26959 |  0:00:48s
epoch 7  | loss: 0.07785 | val_0_rmse: 0.27873 | val_1_rmse: 0.26989 |  0:00:55s
epoch 8  | loss: 0.07699 | val_0_rmse: 0.27177 | val_1_rmse: 0.26162 |  0:01:02s
epoch 9  | loss: 0.07494 | val_0_rmse: 0.26794 | val_1_rmse: 0.25824 |  0:01:08s
epoch 10 | loss: 0.07277 | val_0_rmse: 0.26554 | val_1_rmse: 0.25604 |  0:01:15s
epoch 11 | loss: 0.07203 | val_0_rmse: 0.26657 | val_1_rmse: 0.25751 |  0:01:22s
epoch 12 | loss: 0.07174 | val_0_rmse: 0.2635  | val_1_rmse: 0.25619 |  0:01:29s
epoch 13 | loss: 0.0711  | val_0_rmse: 0.26279 | val_1_rmse: 0.25791 |  0:01:36s
epoch 14 | loss: 0.07047 | val_0_rmse: 0.26241 | val_1_rmse: 0.25704 |  0:01:43s
epoch 15 | loss: 0.06957 | val_0_rmse: 0.26156 | val_1_rmse: 0.25624 |  0:01:50s
epoch 16 | loss: 0.06928 | val_0_rmse: 0.26133 | val_1_rmse: 0.25588 |  0:01:57s
epoch 17 | loss: 0.06978 | val_0_rmse: 0.26166 | val_1_rmse: 0.25475 |  0:02:03s
epoch 18 | loss: 0.0708  | val_0_rmse: 0.26677 | val_1_rmse: 0.27086 |  0:02:10s
epoch 19 | loss: 0.07017 | val_0_rmse: 0.26466 | val_1_rmse: 0.26113 |  0:02:17s
epoch 20 | loss: 0.06919 | val_0_rmse: 0.26355 | val_1_rmse: 0.28732 |  0:02:24s
epoch 21 | loss: 0.06963 | val_0_rmse: 0.26679 | val_1_rmse: 0.26642 |  0:02:31s
epoch 22 | loss: 0.06901 | val_0_rmse: 0.26505 | val_1_rmse: 0.26846 |  0:02:38s
epoch 23 | loss: 0.06833 | val_0_rmse: 0.5296  | val_1_rmse: 0.67851 |  0:02:45s
epoch 24 | loss: 0.06883 | val_0_rmse: 0.25866 | val_1_rmse: 0.25557 |  0:02:52s
epoch 25 | loss: 0.06727 | val_0_rmse: 0.25794 | val_1_rmse: 0.26725 |  0:02:58s
epoch 26 | loss: 0.06692 | val_0_rmse: 0.25662 | val_1_rmse: 0.25263 |  0:03:05s
epoch 27 | loss: 0.06727 | val_0_rmse: 0.25705 | val_1_rmse: 0.25945 |  0:03:12s
epoch 28 | loss: 0.06608 | val_0_rmse: 0.25803 | val_1_rmse: 0.255   |  0:03:19s
epoch 29 | loss: 0.06573 | val_0_rmse: 0.25925 | val_1_rmse: 0.25673 |  0:03:26s
epoch 30 | loss: 0.06597 | val_0_rmse: 0.25558 | val_1_rmse: 0.25297 |  0:03:33s
epoch 31 | loss: 0.06542 | val_0_rmse: 0.27216 | val_1_rmse: 0.27039 |  0:03:40s
epoch 32 | loss: 0.06506 | val_0_rmse: 0.26012 | val_1_rmse: 0.25823 |  0:03:47s
epoch 33 | loss: 0.06515 | val_0_rmse: 0.2554  | val_1_rmse: 0.25519 |  0:03:53s
epoch 34 | loss: 0.06507 | val_0_rmse: 0.26111 | val_1_rmse: 0.2572  |  0:04:00s
epoch 35 | loss: 0.065   | val_0_rmse: 0.25974 | val_1_rmse: 0.25509 |  0:04:07s
epoch 36 | loss: 0.06493 | val_0_rmse: 0.26139 | val_1_rmse: 0.25795 |  0:04:14s
epoch 37 | loss: 0.06515 | val_0_rmse: 0.25679 | val_1_rmse: 0.25516 |  0:04:21s
epoch 38 | loss: 0.06389 | val_0_rmse: 0.25321 | val_1_rmse: 0.2502  |  0:04:28s
epoch 39 | loss: 0.06382 | val_0_rmse: 0.26344 | val_1_rmse: 0.25895 |  0:04:35s
epoch 40 | loss: 0.06381 | val_0_rmse: 0.29162 | val_1_rmse: 0.32692 |  0:04:41s
epoch 41 | loss: 0.06316 | val_0_rmse: 0.27059 | val_1_rmse: 0.29063 |  0:04:48s
epoch 42 | loss: 0.0641  | val_0_rmse: 0.2546  | val_1_rmse: 0.25186 |  0:04:55s
epoch 43 | loss: 0.06357 | val_0_rmse: 0.26482 | val_1_rmse: 0.26189 |  0:05:02s
epoch 44 | loss: 0.06474 | val_0_rmse: 0.25633 | val_1_rmse: 0.25221 |  0:05:09s
epoch 45 | loss: 0.06309 | val_0_rmse: 0.25355 | val_1_rmse: 0.25311 |  0:05:16s
epoch 46 | loss: 0.06357 | val_0_rmse: 0.2643  | val_1_rmse: 0.25485 |  0:05:23s
epoch 47 | loss: 0.06271 | val_0_rmse: 0.25627 | val_1_rmse: 0.2542  |  0:05:30s
epoch 48 | loss: 0.06244 | val_0_rmse: 0.25406 | val_1_rmse: 0.25219 |  0:05:36s
epoch 49 | loss: 0.06246 | val_0_rmse: 0.25446 | val_1_rmse: 0.25059 |  0:05:43s
epoch 50 | loss: 0.06205 | val_0_rmse: 0.25105 | val_1_rmse: 0.25306 |  0:05:50s
epoch 51 | loss: 0.06423 | val_0_rmse: 0.25466 | val_1_rmse: 0.25153 |  0:05:57s
epoch 52 | loss: 0.0704  | val_0_rmse: 0.27252 | val_1_rmse: 0.26521 |  0:06:04s
epoch 53 | loss: 0.0708  | val_0_rmse: 0.27528 | val_1_rmse: 0.26611 |  0:06:11s
epoch 54 | loss: 0.07256 | val_0_rmse: 0.26661 | val_1_rmse: 0.25702 |  0:06:17s
epoch 55 | loss: 0.06981 | val_0_rmse: 0.26639 | val_1_rmse: 0.25631 |  0:06:24s
epoch 56 | loss: 0.07018 | val_0_rmse: 0.27171 | val_1_rmse: 0.26504 |  0:06:31s
epoch 57 | loss: 0.07074 | val_0_rmse: 0.28995 | val_1_rmse: 0.31944 |  0:06:38s
epoch 58 | loss: 0.06899 | val_0_rmse: 0.29713 | val_1_rmse: 0.33435 |  0:06:45s
epoch 59 | loss: 0.06815 | val_0_rmse: 0.27942 | val_1_rmse: 0.29904 |  0:06:52s
epoch 60 | loss: 0.06785 | val_0_rmse: 0.26037 | val_1_rmse: 0.25429 |  0:06:59s
epoch 61 | loss: 0.06868 | val_0_rmse: 0.28731 | val_1_rmse: 0.27051 |  0:07:05s
epoch 62 | loss: 0.07099 | val_0_rmse: 0.29546 | val_1_rmse: 0.30202 |  0:07:12s
epoch 63 | loss: 0.07608 | val_0_rmse: 0.28067 | val_1_rmse: 0.27448 |  0:07:19s
epoch 64 | loss: 0.07508 | val_0_rmse: 0.27346 | val_1_rmse: 0.26497 |  0:07:26s
epoch 65 | loss: 0.07294 | val_0_rmse: 0.27048 | val_1_rmse: 0.2658  |  0:07:33s
epoch 66 | loss: 0.0717  | val_0_rmse: 0.50537 | val_1_rmse: 0.71281 |  0:07:40s
epoch 67 | loss: 0.07007 | val_0_rmse: 0.26632 | val_1_rmse: 0.26409 |  0:07:46s
epoch 68 | loss: 0.07028 | val_0_rmse: 0.26993 | val_1_rmse: 0.27395 |  0:07:53s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.2502
Best weights from best epoch are automatically used!
ended training at: 21:18:07
Feature importance:
Mean squared error is of 0.06496848330209189
Mean absolute error:0.18978203306796282
MAPE:0.20777980229056672
R2 score:0.11917195699908545
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:18:09
epoch 0  | loss: 0.2875  | val_0_rmse: 0.28023 | val_1_rmse: 0.28108 |  0:00:06s
epoch 1  | loss: 0.0813  | val_0_rmse: 0.27709 | val_1_rmse: 0.27798 |  0:00:13s
epoch 2  | loss: 0.07969 | val_0_rmse: 0.27478 | val_1_rmse: 0.27585 |  0:00:20s
epoch 3  | loss: 0.07968 | val_0_rmse: 0.27218 | val_1_rmse: 0.27292 |  0:00:27s
epoch 4  | loss: 0.07693 | val_0_rmse: 0.27065 | val_1_rmse: 0.27116 |  0:00:34s
epoch 5  | loss: 0.07584 | val_0_rmse: 0.27178 | val_1_rmse: 0.27244 |  0:00:40s
epoch 6  | loss: 0.07369 | val_0_rmse: 0.26841 | val_1_rmse: 0.26956 |  0:00:47s
epoch 7  | loss: 0.07258 | val_0_rmse: 0.26454 | val_1_rmse: 0.2662  |  0:00:54s
epoch 8  | loss: 0.07212 | val_0_rmse: 0.27387 | val_1_rmse: 0.27573 |  0:01:01s
epoch 9  | loss: 0.0709  | val_0_rmse: 0.26216 | val_1_rmse: 0.26402 |  0:01:08s
epoch 10 | loss: 0.06986 | val_0_rmse: 0.26176 | val_1_rmse: 0.26443 |  0:01:15s
epoch 11 | loss: 0.06946 | val_0_rmse: 0.26233 | val_1_rmse: 0.26536 |  0:01:22s
epoch 12 | loss: 0.07004 | val_0_rmse: 0.26114 | val_1_rmse: 0.26503 |  0:01:28s
epoch 13 | loss: 0.06935 | val_0_rmse: 0.26039 | val_1_rmse: 0.26563 |  0:01:35s
epoch 14 | loss: 0.06881 | val_0_rmse: 0.26096 | val_1_rmse: 0.26528 |  0:01:42s
epoch 15 | loss: 0.06877 | val_0_rmse: 0.2618  | val_1_rmse: 0.26806 |  0:01:49s
epoch 16 | loss: 0.06819 | val_0_rmse: 0.25905 | val_1_rmse: 0.26404 |  0:01:56s
epoch 17 | loss: 0.06856 | val_0_rmse: 0.26187 | val_1_rmse: 0.26579 |  0:02:03s
epoch 18 | loss: 0.06932 | val_0_rmse: 0.25994 | val_1_rmse: 0.26387 |  0:02:09s
epoch 19 | loss: 0.0688  | val_0_rmse: 0.26324 | val_1_rmse: 0.26633 |  0:02:16s
epoch 20 | loss: 0.0696  | val_0_rmse: 0.26298 | val_1_rmse: 0.2668  |  0:02:23s
epoch 21 | loss: 0.06902 | val_0_rmse: 0.26322 | val_1_rmse: 0.2654  |  0:02:30s
epoch 22 | loss: 0.0684  | val_0_rmse: 0.26114 | val_1_rmse: 0.26668 |  0:02:37s
epoch 23 | loss: 0.0693  | val_0_rmse: 0.26943 | val_1_rmse: 0.27038 |  0:02:44s
epoch 24 | loss: 0.07133 | val_0_rmse: 0.27052 | val_1_rmse: 0.27462 |  0:02:51s
epoch 25 | loss: 0.06987 | val_0_rmse: 0.28053 | val_1_rmse: 0.26662 |  0:02:57s
epoch 26 | loss: 0.06919 | val_0_rmse: 0.2726  | val_1_rmse: 0.27263 |  0:03:04s
epoch 27 | loss: 0.06986 | val_0_rmse: 0.2615  | val_1_rmse: 0.2641  |  0:03:11s
epoch 28 | loss: 0.06765 | val_0_rmse: 0.25858 | val_1_rmse: 0.26239 |  0:03:18s
epoch 29 | loss: 0.06777 | val_0_rmse: 0.26042 | val_1_rmse: 0.26327 |  0:03:25s
epoch 30 | loss: 0.06763 | val_0_rmse: 0.26026 | val_1_rmse: 0.31357 |  0:03:32s
epoch 31 | loss: 0.0683  | val_0_rmse: 0.26119 | val_1_rmse: 0.26544 |  0:03:39s
epoch 32 | loss: 0.06722 | val_0_rmse: 0.25886 | val_1_rmse: 0.27571 |  0:03:45s
epoch 33 | loss: 0.06629 | val_0_rmse: 0.25552 | val_1_rmse: 0.27338 |  0:03:52s
epoch 34 | loss: 0.06583 | val_0_rmse: 0.25593 | val_1_rmse: 0.284   |  0:03:59s
epoch 35 | loss: 0.06616 | val_0_rmse: 0.25819 | val_1_rmse: 0.26152 |  0:04:06s
epoch 36 | loss: 0.06634 | val_0_rmse: 0.26245 | val_1_rmse: 0.26526 |  0:04:13s
epoch 37 | loss: 0.06664 | val_0_rmse: 0.2729  | val_1_rmse: 0.27809 |  0:04:20s
epoch 38 | loss: 0.06612 | val_0_rmse: 0.25986 | val_1_rmse: 0.26391 |  0:04:26s
epoch 39 | loss: 0.06497 | val_0_rmse: 0.25511 | val_1_rmse: 0.25987 |  0:04:33s
epoch 40 | loss: 0.06463 | val_0_rmse: 0.28436 | val_1_rmse: 0.25894 |  0:04:40s
epoch 41 | loss: 0.06425 | val_0_rmse: 0.26411 | val_1_rmse: 0.25957 |  0:04:47s
epoch 42 | loss: 0.06353 | val_0_rmse: 0.26967 | val_1_rmse: 0.25813 |  0:04:54s
epoch 43 | loss: 0.06364 | val_0_rmse: 0.26528 | val_1_rmse: 0.2591  |  0:05:01s
epoch 44 | loss: 0.06368 | val_0_rmse: 0.25167 | val_1_rmse: 0.26614 |  0:05:08s
epoch 45 | loss: 0.06451 | val_0_rmse: 0.25285 | val_1_rmse: 0.26055 |  0:05:14s
epoch 46 | loss: 0.0632  | val_0_rmse: 0.2507  | val_1_rmse: 0.25929 |  0:05:21s
epoch 47 | loss: 0.0636  | val_0_rmse: 0.25832 | val_1_rmse: 0.27855 |  0:05:28s
epoch 48 | loss: 0.06243 | val_0_rmse: 0.26911 | val_1_rmse: 0.3664  |  0:05:35s
epoch 49 | loss: 0.0619  | val_0_rmse: 0.24979 | val_1_rmse: 0.25756 |  0:05:42s
epoch 50 | loss: 0.06198 | val_0_rmse: 0.25661 | val_1_rmse: 0.2597  |  0:05:49s
epoch 51 | loss: 0.06239 | val_0_rmse: 0.28709 | val_1_rmse: 0.25891 |  0:05:55s
epoch 52 | loss: 0.06234 | val_0_rmse: 0.29597 | val_1_rmse: 0.26398 |  0:06:02s
epoch 53 | loss: 0.06255 | val_0_rmse: 0.27691 | val_1_rmse: 0.26066 |  0:06:09s
epoch 54 | loss: 0.06239 | val_0_rmse: 0.27025 | val_1_rmse: 0.26149 |  0:06:16s
epoch 55 | loss: 0.06235 | val_0_rmse: 0.27796 | val_1_rmse: 0.25996 |  0:06:23s
epoch 56 | loss: 0.06234 | val_0_rmse: 0.25516 | val_1_rmse: 0.25956 |  0:06:30s
epoch 57 | loss: 0.0614  | val_0_rmse: 0.24814 | val_1_rmse: 0.26019 |  0:06:36s
epoch 58 | loss: 0.06067 | val_0_rmse: 0.24901 | val_1_rmse: 0.25951 |  0:06:43s
epoch 59 | loss: 0.06088 | val_0_rmse: 0.24572 | val_1_rmse: 0.25831 |  0:06:50s
epoch 60 | loss: 0.06077 | val_0_rmse: 0.24649 | val_1_rmse: 0.25723 |  0:06:57s
epoch 61 | loss: 0.06056 | val_0_rmse: 0.27665 | val_1_rmse: 0.2566  |  0:07:04s
epoch 62 | loss: 0.06029 | val_0_rmse: 0.25498 | val_1_rmse: 0.29866 |  0:07:11s
epoch 63 | loss: 0.06063 | val_0_rmse: 0.27367 | val_1_rmse: 0.26587 |  0:07:18s
epoch 64 | loss: 0.06015 | val_0_rmse: 0.29704 | val_1_rmse: 0.25955 |  0:07:24s
epoch 65 | loss: 0.06061 | val_0_rmse: 0.24666 | val_1_rmse: 0.30736 |  0:07:31s
epoch 66 | loss: 0.05983 | val_0_rmse: 0.2497  | val_1_rmse: 0.46448 |  0:07:38s
epoch 67 | loss: 0.06054 | val_0_rmse: 0.24819 | val_1_rmse: 0.51638 |  0:07:45s
epoch 68 | loss: 0.06191 | val_0_rmse: 0.2513  | val_1_rmse: 0.44167 |  0:07:52s
epoch 69 | loss: 0.06592 | val_0_rmse: 0.27806 | val_1_rmse: 0.26875 |  0:07:59s
epoch 70 | loss: 0.06743 | val_0_rmse: 0.25613 | val_1_rmse: 0.2612  |  0:08:06s
epoch 71 | loss: 0.06566 | val_0_rmse: 0.26702 | val_1_rmse: 0.26808 |  0:08:13s
epoch 72 | loss: 0.06581 | val_0_rmse: 0.25847 | val_1_rmse: 0.26725 |  0:08:20s
epoch 73 | loss: 0.06436 | val_0_rmse: 0.26994 | val_1_rmse: 0.27412 |  0:08:27s
epoch 74 | loss: 0.07525 | val_0_rmse: 0.29646 | val_1_rmse: 0.27462 |  0:08:34s
epoch 75 | loss: 0.07204 | val_0_rmse: 0.26559 | val_1_rmse: 0.26819 |  0:08:40s
epoch 76 | loss: 0.07082 | val_0_rmse: 0.26394 | val_1_rmse: 0.26724 |  0:08:47s
epoch 77 | loss: 0.06928 | val_0_rmse: 0.26182 | val_1_rmse: 0.26672 |  0:08:54s
epoch 78 | loss: 0.06788 | val_0_rmse: 0.25845 | val_1_rmse: 0.26406 |  0:09:01s
epoch 79 | loss: 0.06659 | val_0_rmse: 0.25783 | val_1_rmse: 0.26538 |  0:09:08s
epoch 80 | loss: 0.06563 | val_0_rmse: 0.25549 | val_1_rmse: 0.26409 |  0:09:15s
epoch 81 | loss: 0.06513 | val_0_rmse: 0.25365 | val_1_rmse: 0.26243 |  0:09:22s
epoch 82 | loss: 0.0641  | val_0_rmse: 0.2528  | val_1_rmse: 0.2626  |  0:09:29s
epoch 83 | loss: 0.06328 | val_0_rmse: 0.25139 | val_1_rmse: 0.26154 |  0:09:36s
epoch 84 | loss: 0.06341 | val_0_rmse: 0.25394 | val_1_rmse: 0.26532 |  0:09:42s
epoch 85 | loss: 0.0631  | val_0_rmse: 0.24892 | val_1_rmse: 0.26072 |  0:09:49s
epoch 86 | loss: 0.06174 | val_0_rmse: 0.24804 | val_1_rmse: 0.25913 |  0:09:56s
epoch 87 | loss: 0.0615  | val_0_rmse: 0.25019 | val_1_rmse: 0.26226 |  0:10:03s
epoch 88 | loss: 0.06132 | val_0_rmse: 0.2544  | val_1_rmse: 0.26626 |  0:10:10s
epoch 89 | loss: 0.06059 | val_0_rmse: 0.24791 | val_1_rmse: 0.26164 |  0:10:17s
epoch 90 | loss: 0.06039 | val_0_rmse: 0.26351 | val_1_rmse: 0.27974 |  0:10:24s
epoch 91 | loss: 0.06002 | val_0_rmse: 0.25382 | val_1_rmse: 0.2705  |  0:10:31s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.2566
Best weights from best epoch are automatically used!
ended training at: 21:28:43
Feature importance:
Mean squared error is of 0.11839495247245913
Mean absolute error:0.19236216222942995
MAPE:0.21223615345517305
R2 score:-0.544225657239529
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:28:45
epoch 0  | loss: 0.30158 | val_0_rmse: 0.2831  | val_1_rmse: 0.27153 |  0:00:06s
epoch 1  | loss: 0.08295 | val_0_rmse: 0.28308 | val_1_rmse: 0.27109 |  0:00:13s
epoch 2  | loss: 0.08124 | val_0_rmse: 0.2825  | val_1_rmse: 0.27113 |  0:00:20s
epoch 3  | loss: 0.0806  | val_0_rmse: 0.28329 | val_1_rmse: 0.27222 |  0:00:27s
epoch 4  | loss: 0.08049 | val_0_rmse: 0.28304 | val_1_rmse: 0.27153 |  0:00:34s
epoch 5  | loss: 0.08154 | val_0_rmse: 0.28484 | val_1_rmse: 0.2742  |  0:00:41s
epoch 6  | loss: 0.08129 | val_0_rmse: 0.28528 | val_1_rmse: 0.27316 |  0:00:47s
epoch 7  | loss: 0.07989 | val_0_rmse: 0.28146 | val_1_rmse: 0.27008 |  0:00:54s
epoch 8  | loss: 0.08004 | val_0_rmse: 0.28119 | val_1_rmse: 0.2701  |  0:01:01s
epoch 9  | loss: 0.07986 | val_0_rmse: 0.28233 | val_1_rmse: 0.27059 |  0:01:08s
epoch 10 | loss: 0.08023 | val_0_rmse: 0.28188 | val_1_rmse: 0.27106 |  0:01:15s
epoch 11 | loss: 0.08021 | val_0_rmse: 0.2835  | val_1_rmse: 0.27311 |  0:01:22s
epoch 12 | loss: 0.08045 | val_0_rmse: 0.2814  | val_1_rmse: 0.27039 |  0:01:28s
epoch 13 | loss: 0.08102 | val_0_rmse: 0.2834  | val_1_rmse: 0.27236 |  0:01:35s
epoch 14 | loss: 0.08031 | val_0_rmse: 0.28185 | val_1_rmse: 0.27038 |  0:01:42s
epoch 15 | loss: 0.08114 | val_0_rmse: 0.28655 | val_1_rmse: 0.27898 |  0:01:49s
epoch 16 | loss: 0.08054 | val_0_rmse: 0.28483 | val_1_rmse: 0.2732  |  0:01:56s
epoch 17 | loss: 0.08055 | val_0_rmse: 0.28437 | val_1_rmse: 0.27387 |  0:02:03s
epoch 18 | loss: 0.07986 | val_0_rmse: 0.28343 | val_1_rmse: 0.27452 |  0:02:09s
epoch 19 | loss: 0.08115 | val_0_rmse: 0.2869  | val_1_rmse: 0.27841 |  0:02:16s
epoch 20 | loss: 0.08078 | val_0_rmse: 0.28643 | val_1_rmse: 0.27732 |  0:02:23s
epoch 21 | loss: 0.08056 | val_0_rmse: 0.28621 | val_1_rmse: 0.27613 |  0:02:30s
epoch 22 | loss: 0.08062 | val_0_rmse: 0.28458 | val_1_rmse: 0.2741  |  0:02:37s
epoch 23 | loss: 0.08042 | val_0_rmse: 0.28444 | val_1_rmse: 0.27575 |  0:02:44s
epoch 24 | loss: 0.07992 | val_0_rmse: 0.29289 | val_1_rmse: 0.28989 |  0:02:50s
epoch 25 | loss: 0.08045 | val_0_rmse: 0.28173 | val_1_rmse: 0.27028 |  0:02:57s
epoch 26 | loss: 0.07992 | val_0_rmse: 0.28705 | val_1_rmse: 0.27953 |  0:03:04s
epoch 27 | loss: 0.08023 | val_0_rmse: 0.28737 | val_1_rmse: 0.27885 |  0:03:11s
epoch 28 | loss: 0.08012 | val_0_rmse: 0.28609 | val_1_rmse: 0.27961 |  0:03:18s
epoch 29 | loss: 0.08004 | val_0_rmse: 0.33199 | val_1_rmse: 0.32892 |  0:03:25s
epoch 30 | loss: 0.07982 | val_0_rmse: 0.30585 | val_1_rmse: 0.30232 |  0:03:31s
epoch 31 | loss: 0.08037 | val_0_rmse: 0.30716 | val_1_rmse: 0.29844 |  0:03:38s
epoch 32 | loss: 0.07992 | val_0_rmse: 0.32275 | val_1_rmse: 0.33109 |  0:03:45s
epoch 33 | loss: 0.07993 | val_0_rmse: 0.31063 | val_1_rmse: 0.31329 |  0:03:52s
epoch 34 | loss: 0.0798  | val_0_rmse: 0.3142  | val_1_rmse: 0.31816 |  0:03:59s
epoch 35 | loss: 0.08012 | val_0_rmse: 0.3336  | val_1_rmse: 0.34317 |  0:04:06s
epoch 36 | loss: 0.08009 | val_0_rmse: 0.32124 | val_1_rmse: 0.33132 |  0:04:12s
epoch 37 | loss: 0.08113 | val_0_rmse: 0.28406 | val_1_rmse: 0.27328 |  0:04:19s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.27008
Best weights from best epoch are automatically used!
ended training at: 21:33:08
Feature importance:
Mean squared error is of 0.07443497404396991
Mean absolute error:0.2075109247598772
MAPE:0.23535099406652138
R2 score:0.0023001631531328615
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:33:10
epoch 0  | loss: 2.44164 | val_0_rmse: 0.60043 | val_1_rmse: 0.63285 |  0:00:00s
epoch 1  | loss: 0.16776 | val_0_rmse: 0.42865 | val_1_rmse: 0.47143 |  0:00:02s
epoch 2  | loss: 0.1066  | val_0_rmse: 0.29866 | val_1_rmse: 0.35609 |  0:00:03s
epoch 3  | loss: 0.1     | val_0_rmse: 0.30767 | val_1_rmse: 0.36516 |  0:00:04s
epoch 4  | loss: 0.09448 | val_0_rmse: 0.31469 | val_1_rmse: 0.37022 |  0:00:05s
epoch 5  | loss: 0.09088 | val_0_rmse: 0.29686 | val_1_rmse: 0.35382 |  0:00:06s
epoch 6  | loss: 0.08802 | val_0_rmse: 0.29142 | val_1_rmse: 0.34771 |  0:00:07s
epoch 7  | loss: 0.08647 | val_0_rmse: 0.28836 | val_1_rmse: 0.34396 |  0:00:08s
epoch 8  | loss: 0.08559 | val_0_rmse: 0.28626 | val_1_rmse: 0.34269 |  0:00:09s
epoch 9  | loss: 0.08379 | val_0_rmse: 0.28842 | val_1_rmse: 0.346   |  0:00:10s
epoch 10 | loss: 0.08321 | val_0_rmse: 0.28708 | val_1_rmse: 0.34304 |  0:00:11s
epoch 11 | loss: 0.08305 | val_0_rmse: 0.28894 | val_1_rmse: 0.34679 |  0:00:12s
epoch 12 | loss: 0.08276 | val_0_rmse: 0.28569 | val_1_rmse: 0.34218 |  0:00:13s
epoch 13 | loss: 0.08258 | val_0_rmse: 0.28479 | val_1_rmse: 0.34133 |  0:00:14s
epoch 14 | loss: 0.08152 | val_0_rmse: 0.28508 | val_1_rmse: 0.34219 |  0:00:15s
epoch 15 | loss: 0.08193 | val_0_rmse: 0.28876 | val_1_rmse: 0.34492 |  0:00:16s
epoch 16 | loss: 0.08068 | val_0_rmse: 0.29302 | val_1_rmse: 0.34823 |  0:00:17s
epoch 17 | loss: 0.08127 | val_0_rmse: 0.28882 | val_1_rmse: 0.34566 |  0:00:18s
epoch 18 | loss: 0.08006 | val_0_rmse: 0.2831  | val_1_rmse: 0.34164 |  0:00:19s
epoch 19 | loss: 0.08074 | val_0_rmse: 0.28616 | val_1_rmse: 0.34321 |  0:00:20s
epoch 20 | loss: 0.08127 | val_0_rmse: 0.28414 | val_1_rmse: 0.34313 |  0:00:21s
epoch 21 | loss: 0.08107 | val_0_rmse: 0.28282 | val_1_rmse: 0.34207 |  0:00:22s
epoch 22 | loss: 0.07957 | val_0_rmse: 0.28041 | val_1_rmse: 0.34076 |  0:00:23s
epoch 23 | loss: 0.08027 | val_0_rmse: 0.27812 | val_1_rmse: 0.34033 |  0:00:24s
epoch 24 | loss: 0.07835 | val_0_rmse: 0.27723 | val_1_rmse: 0.33898 |  0:00:25s
epoch 25 | loss: 0.07693 | val_0_rmse: 0.27607 | val_1_rmse: 0.33848 |  0:00:26s
epoch 26 | loss: 0.07729 | val_0_rmse: 0.28011 | val_1_rmse: 0.34027 |  0:00:27s
epoch 27 | loss: 0.07753 | val_0_rmse: 0.28188 | val_1_rmse: 0.34444 |  0:00:28s
epoch 28 | loss: 0.07848 | val_0_rmse: 0.27574 | val_1_rmse: 0.3383  |  0:00:29s
epoch 29 | loss: 0.07567 | val_0_rmse: 0.27889 | val_1_rmse: 0.34109 |  0:00:30s
epoch 30 | loss: 0.07619 | val_0_rmse: 0.27326 | val_1_rmse: 0.33657 |  0:00:31s
epoch 31 | loss: 0.07562 | val_0_rmse: 0.27832 | val_1_rmse: 0.3399  |  0:00:32s
epoch 32 | loss: 0.07883 | val_0_rmse: 0.27932 | val_1_rmse: 0.34246 |  0:00:33s
epoch 33 | loss: 0.07837 | val_0_rmse: 0.2734  | val_1_rmse: 0.33744 |  0:00:34s
epoch 34 | loss: 0.07586 | val_0_rmse: 0.27879 | val_1_rmse: 0.34186 |  0:00:35s
epoch 35 | loss: 0.07449 | val_0_rmse: 0.28021 | val_1_rmse: 0.34283 |  0:00:36s
epoch 36 | loss: 0.07594 | val_0_rmse: 0.27233 | val_1_rmse: 0.33696 |  0:00:37s
epoch 37 | loss: 0.0748  | val_0_rmse: 0.26958 | val_1_rmse: 0.33418 |  0:00:38s
epoch 38 | loss: 0.07449 | val_0_rmse: 0.2784  | val_1_rmse: 0.34323 |  0:00:39s
epoch 39 | loss: 0.07485 | val_0_rmse: 0.26855 | val_1_rmse: 0.33413 |  0:00:40s
epoch 40 | loss: 0.07271 | val_0_rmse: 0.266   | val_1_rmse: 0.33329 |  0:00:41s
epoch 41 | loss: 0.07269 | val_0_rmse: 0.2664  | val_1_rmse: 0.33289 |  0:00:42s
epoch 42 | loss: 0.07511 | val_0_rmse: 0.28343 | val_1_rmse: 0.34858 |  0:00:43s
epoch 43 | loss: 0.07546 | val_0_rmse: 0.26738 | val_1_rmse: 0.33552 |  0:00:44s
epoch 44 | loss: 0.07319 | val_0_rmse: 0.26402 | val_1_rmse: 0.33411 |  0:00:45s
epoch 45 | loss: 0.07134 | val_0_rmse: 0.26682 | val_1_rmse: 0.33543 |  0:00:46s
epoch 46 | loss: 0.0719  | val_0_rmse: 0.26191 | val_1_rmse: 0.33216 |  0:00:47s
epoch 47 | loss: 0.07091 | val_0_rmse: 0.2649  | val_1_rmse: 0.33293 |  0:00:48s
epoch 48 | loss: 0.07171 | val_0_rmse: 0.26322 | val_1_rmse: 0.33101 |  0:00:49s
epoch 49 | loss: 0.07124 | val_0_rmse: 0.26234 | val_1_rmse: 0.33134 |  0:00:50s
epoch 50 | loss: 0.07043 | val_0_rmse: 0.26289 | val_1_rmse: 0.33278 |  0:00:51s
epoch 51 | loss: 0.07026 | val_0_rmse: 0.26075 | val_1_rmse: 0.33155 |  0:00:52s
epoch 52 | loss: 0.07036 | val_0_rmse: 0.26151 | val_1_rmse: 0.33238 |  0:00:53s
epoch 53 | loss: 0.07056 | val_0_rmse: 0.26752 | val_1_rmse: 0.33616 |  0:00:54s
epoch 54 | loss: 0.07297 | val_0_rmse: 0.26726 | val_1_rmse: 0.33677 |  0:00:55s
epoch 55 | loss: 0.07208 | val_0_rmse: 0.26497 | val_1_rmse: 0.3367  |  0:00:56s
epoch 56 | loss: 0.07136 | val_0_rmse: 0.26294 | val_1_rmse: 0.3321  |  0:00:57s
epoch 57 | loss: 0.07071 | val_0_rmse: 0.26348 | val_1_rmse: 0.33331 |  0:00:58s
epoch 58 | loss: 0.07115 | val_0_rmse: 0.25963 | val_1_rmse: 0.3312  |  0:00:59s
epoch 59 | loss: 0.06988 | val_0_rmse: 0.25723 | val_1_rmse: 0.33349 |  0:01:00s
epoch 60 | loss: 0.07021 | val_0_rmse: 0.25962 | val_1_rmse: 0.33167 |  0:01:01s
epoch 61 | loss: 0.07002 | val_0_rmse: 0.26108 | val_1_rmse: 0.33388 |  0:01:02s
epoch 62 | loss: 0.06999 | val_0_rmse: 0.25902 | val_1_rmse: 0.33342 |  0:01:03s
epoch 63 | loss: 0.06898 | val_0_rmse: 0.25901 | val_1_rmse: 0.33279 |  0:01:04s
epoch 64 | loss: 0.06906 | val_0_rmse: 0.25771 | val_1_rmse: 0.33136 |  0:01:05s
epoch 65 | loss: 0.06855 | val_0_rmse: 0.25761 | val_1_rmse: 0.33052 |  0:01:06s
epoch 66 | loss: 0.07068 | val_0_rmse: 0.25957 | val_1_rmse: 0.33405 |  0:01:07s
epoch 67 | loss: 0.06852 | val_0_rmse: 0.25696 | val_1_rmse: 0.33326 |  0:01:08s
epoch 68 | loss: 0.0684  | val_0_rmse: 0.25728 | val_1_rmse: 0.33315 |  0:01:09s
epoch 69 | loss: 0.06884 | val_0_rmse: 0.25803 | val_1_rmse: 0.3342  |  0:01:10s
epoch 70 | loss: 0.06954 | val_0_rmse: 0.2674  | val_1_rmse: 0.33976 |  0:01:11s
epoch 71 | loss: 0.07019 | val_0_rmse: 0.25915 | val_1_rmse: 0.33392 |  0:01:11s
epoch 72 | loss: 0.06833 | val_0_rmse: 0.25604 | val_1_rmse: 0.32422 |  0:01:12s
epoch 73 | loss: 0.06847 | val_0_rmse: 0.26654 | val_1_rmse: 0.34113 |  0:01:14s
epoch 74 | loss: 0.06896 | val_0_rmse: 0.25595 | val_1_rmse: 0.32796 |  0:01:15s
epoch 75 | loss: 0.06727 | val_0_rmse: 0.25791 | val_1_rmse: 0.33095 |  0:01:16s
epoch 76 | loss: 0.06777 | val_0_rmse: 0.25895 | val_1_rmse: 0.33165 |  0:01:16s
epoch 77 | loss: 0.06872 | val_0_rmse: 0.2561  | val_1_rmse: 0.32995 |  0:01:17s
epoch 78 | loss: 0.06847 | val_0_rmse: 0.25745 | val_1_rmse: 0.32974 |  0:01:18s
epoch 79 | loss: 0.06754 | val_0_rmse: 0.25428 | val_1_rmse: 0.32905 |  0:01:19s
epoch 80 | loss: 0.06653 | val_0_rmse: 0.25088 | val_1_rmse: 0.32769 |  0:01:20s
epoch 81 | loss: 0.06607 | val_0_rmse: 0.24894 | val_1_rmse: 0.32576 |  0:01:21s
epoch 82 | loss: 0.06517 | val_0_rmse: 0.25294 | val_1_rmse: 0.32971 |  0:01:22s
epoch 83 | loss: 0.06589 | val_0_rmse: 0.25294 | val_1_rmse: 0.32882 |  0:01:23s
epoch 84 | loss: 0.06612 | val_0_rmse: 0.25363 | val_1_rmse: 0.32819 |  0:01:24s
epoch 85 | loss: 0.06501 | val_0_rmse: 0.26473 | val_1_rmse: 0.33974 |  0:01:25s
epoch 86 | loss: 0.06817 | val_0_rmse: 0.25639 | val_1_rmse: 0.32943 |  0:01:26s
epoch 87 | loss: 0.0662  | val_0_rmse: 0.25085 | val_1_rmse: 0.33343 |  0:01:27s
epoch 88 | loss: 0.06562 | val_0_rmse: 0.25041 | val_1_rmse: 0.32879 |  0:01:28s
epoch 89 | loss: 0.06563 | val_0_rmse: 0.25118 | val_1_rmse: 0.32892 |  0:01:29s
epoch 90 | loss: 0.06514 | val_0_rmse: 0.25659 | val_1_rmse: 0.33118 |  0:01:30s
epoch 91 | loss: 0.06576 | val_0_rmse: 0.25289 | val_1_rmse: 0.331   |  0:01:31s
epoch 92 | loss: 0.06581 | val_0_rmse: 0.25066 | val_1_rmse: 0.32737 |  0:01:32s
epoch 93 | loss: 0.06514 | val_0_rmse: 0.25242 | val_1_rmse: 0.32982 |  0:01:33s
epoch 94 | loss: 0.0682  | val_0_rmse: 0.25558 | val_1_rmse: 0.33179 |  0:01:34s
epoch 95 | loss: 0.06724 | val_0_rmse: 0.25224 | val_1_rmse: 0.33291 |  0:01:35s
epoch 96 | loss: 0.06705 | val_0_rmse: 0.25498 | val_1_rmse: 0.33088 |  0:01:36s
epoch 97 | loss: 0.06698 | val_0_rmse: 0.26357 | val_1_rmse: 0.33472 |  0:01:37s
epoch 98 | loss: 0.06853 | val_0_rmse: 0.25632 | val_1_rmse: 0.33035 |  0:01:38s
epoch 99 | loss: 0.06757 | val_0_rmse: 0.26048 | val_1_rmse: 0.32881 |  0:01:39s
epoch 100| loss: 0.06707 | val_0_rmse: 0.25703 | val_1_rmse: 0.33111 |  0:01:40s
epoch 101| loss: 0.06524 | val_0_rmse: 0.25171 | val_1_rmse: 0.32772 |  0:01:41s
epoch 102| loss: 0.06719 | val_0_rmse: 0.26379 | val_1_rmse: 0.3324  |  0:01:42s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.32422
Best weights from best epoch are automatically used!
ended training at: 21:34:53
Feature importance:
Mean squared error is of 0.06892647303644693
Mean absolute error:0.1936839602643401
MAPE:0.21606724372077288
R2 score:0.18395580218695717
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:34:53
epoch 0  | loss: 2.57537 | val_0_rmse: 0.36765 | val_1_rmse: 0.38528 |  0:00:01s
epoch 1  | loss: 0.14808 | val_0_rmse: 0.34234 | val_1_rmse: 0.35676 |  0:00:02s
epoch 2  | loss: 0.09998 | val_0_rmse: 0.29881 | val_1_rmse: 0.3166  |  0:00:03s
epoch 3  | loss: 0.0878  | val_0_rmse: 0.29111 | val_1_rmse: 0.30555 |  0:00:04s
epoch 4  | loss: 0.08726 | val_0_rmse: 0.29357 | val_1_rmse: 0.30988 |  0:00:05s
epoch 5  | loss: 0.08288 | val_0_rmse: 0.29058 | val_1_rmse: 0.3065  |  0:00:06s
epoch 6  | loss: 0.08148 | val_0_rmse: 0.28811 | val_1_rmse: 0.30188 |  0:00:07s
epoch 7  | loss: 0.08049 | val_0_rmse: 0.29293 | val_1_rmse: 0.30804 |  0:00:08s
epoch 8  | loss: 0.08004 | val_0_rmse: 0.28411 | val_1_rmse: 0.29892 |  0:00:09s
epoch 9  | loss: 0.08024 | val_0_rmse: 0.29361 | val_1_rmse: 0.30876 |  0:00:10s
epoch 10 | loss: 0.08016 | val_0_rmse: 0.2902  | val_1_rmse: 0.30341 |  0:00:11s
epoch 11 | loss: 0.07909 | val_0_rmse: 0.28399 | val_1_rmse: 0.2991  |  0:00:12s
epoch 12 | loss: 0.0791  | val_0_rmse: 0.28427 | val_1_rmse: 0.29891 |  0:00:13s
epoch 13 | loss: 0.07826 | val_0_rmse: 0.28156 | val_1_rmse: 0.29731 |  0:00:14s
epoch 14 | loss: 0.07815 | val_0_rmse: 0.28378 | val_1_rmse: 0.29728 |  0:00:15s
epoch 15 | loss: 0.07628 | val_0_rmse: 0.28359 | val_1_rmse: 0.29817 |  0:00:16s
epoch 16 | loss: 0.07626 | val_0_rmse: 0.28259 | val_1_rmse: 0.29584 |  0:00:17s
epoch 17 | loss: 0.07602 | val_0_rmse: 0.28192 | val_1_rmse: 0.29745 |  0:00:18s
epoch 18 | loss: 0.07734 | val_0_rmse: 0.28477 | val_1_rmse: 0.29819 |  0:00:19s
epoch 19 | loss: 0.07972 | val_0_rmse: 0.28926 | val_1_rmse: 0.3018  |  0:00:20s
epoch 20 | loss: 0.07584 | val_0_rmse: 0.28383 | val_1_rmse: 0.2958  |  0:00:21s
epoch 21 | loss: 0.07511 | val_0_rmse: 0.27581 | val_1_rmse: 0.29079 |  0:00:22s
epoch 22 | loss: 0.07526 | val_0_rmse: 0.27371 | val_1_rmse: 0.28818 |  0:00:23s
epoch 23 | loss: 0.07362 | val_0_rmse: 0.27134 | val_1_rmse: 0.28839 |  0:00:24s
epoch 24 | loss: 0.07371 | val_0_rmse: 0.2722  | val_1_rmse: 0.29034 |  0:00:25s
epoch 25 | loss: 0.07302 | val_0_rmse: 0.27315 | val_1_rmse: 0.28968 |  0:00:26s
epoch 26 | loss: 0.07271 | val_0_rmse: 0.27169 | val_1_rmse: 0.28762 |  0:00:27s
epoch 27 | loss: 0.07249 | val_0_rmse: 0.26968 | val_1_rmse: 0.28413 |  0:00:28s
epoch 28 | loss: 0.07248 | val_0_rmse: 0.26719 | val_1_rmse: 0.28218 |  0:00:29s
epoch 29 | loss: 0.07204 | val_0_rmse: 0.2678  | val_1_rmse: 0.28481 |  0:00:30s
epoch 30 | loss: 0.07169 | val_0_rmse: 0.2674  | val_1_rmse: 0.28495 |  0:00:31s
epoch 31 | loss: 0.07069 | val_0_rmse: 0.26606 | val_1_rmse: 0.28346 |  0:00:32s
epoch 32 | loss: 0.07103 | val_0_rmse: 0.26756 | val_1_rmse: 0.28376 |  0:00:33s
epoch 33 | loss: 0.07079 | val_0_rmse: 0.26874 | val_1_rmse: 0.28509 |  0:00:34s
epoch 34 | loss: 0.07106 | val_0_rmse: 0.26573 | val_1_rmse: 0.28176 |  0:00:35s
epoch 35 | loss: 0.07101 | val_0_rmse: 0.26845 | val_1_rmse: 0.2862  |  0:00:36s
epoch 36 | loss: 0.07322 | val_0_rmse: 0.26606 | val_1_rmse: 0.28145 |  0:00:37s
epoch 37 | loss: 0.0704  | val_0_rmse: 0.26278 | val_1_rmse: 0.2755  |  0:00:38s
epoch 38 | loss: 0.06978 | val_0_rmse: 0.26225 | val_1_rmse: 0.27658 |  0:00:39s
epoch 39 | loss: 0.06966 | val_0_rmse: 0.26174 | val_1_rmse: 0.27685 |  0:00:40s
epoch 40 | loss: 0.06847 | val_0_rmse: 0.26134 | val_1_rmse: 0.27753 |  0:00:41s
epoch 41 | loss: 0.06926 | val_0_rmse: 0.26078 | val_1_rmse: 0.27711 |  0:00:42s
epoch 42 | loss: 0.06868 | val_0_rmse: 0.26025 | val_1_rmse: 0.27546 |  0:00:43s
epoch 43 | loss: 0.06815 | val_0_rmse: 0.26253 | val_1_rmse: 0.27872 |  0:00:44s
epoch 44 | loss: 0.06756 | val_0_rmse: 0.25819 | val_1_rmse: 0.27277 |  0:00:45s
epoch 45 | loss: 0.06741 | val_0_rmse: 0.25835 | val_1_rmse: 0.27531 |  0:00:46s
epoch 46 | loss: 0.0671  | val_0_rmse: 0.25623 | val_1_rmse: 0.27557 |  0:00:47s
epoch 47 | loss: 0.06705 | val_0_rmse: 0.25729 | val_1_rmse: 0.27628 |  0:00:48s
epoch 48 | loss: 0.06721 | val_0_rmse: 0.25724 | val_1_rmse: 0.27553 |  0:00:49s
epoch 49 | loss: 0.06758 | val_0_rmse: 0.25676 | val_1_rmse: 0.27626 |  0:00:50s
epoch 50 | loss: 0.06742 | val_0_rmse: 0.25615 | val_1_rmse: 0.2743  |  0:00:51s
epoch 51 | loss: 0.06711 | val_0_rmse: 0.25591 | val_1_rmse: 0.27354 |  0:00:52s
epoch 52 | loss: 0.06616 | val_0_rmse: 0.25419 | val_1_rmse: 0.27228 |  0:00:53s
epoch 53 | loss: 0.06562 | val_0_rmse: 0.25407 | val_1_rmse: 0.27155 |  0:00:54s
epoch 54 | loss: 0.06595 | val_0_rmse: 0.26028 | val_1_rmse: 0.27694 |  0:00:55s
epoch 55 | loss: 0.06746 | val_0_rmse: 0.25919 | val_1_rmse: 0.27854 |  0:00:56s
epoch 56 | loss: 0.06725 | val_0_rmse: 0.25593 | val_1_rmse: 0.27459 |  0:00:57s
epoch 57 | loss: 0.06634 | val_0_rmse: 0.25468 | val_1_rmse: 0.27218 |  0:00:58s
epoch 58 | loss: 0.06582 | val_0_rmse: 0.25317 | val_1_rmse: 0.27184 |  0:00:59s
epoch 59 | loss: 0.06568 | val_0_rmse: 0.25405 | val_1_rmse: 0.27233 |  0:01:00s
epoch 60 | loss: 0.06573 | val_0_rmse: 0.25287 | val_1_rmse: 0.27333 |  0:01:01s
epoch 61 | loss: 0.06511 | val_0_rmse: 0.25216 | val_1_rmse: 0.27161 |  0:01:02s
epoch 62 | loss: 0.06503 | val_0_rmse: 0.25203 | val_1_rmse: 0.27215 |  0:01:03s
epoch 63 | loss: 0.06514 | val_0_rmse: 0.25271 | val_1_rmse: 0.27213 |  0:01:04s
epoch 64 | loss: 0.0664  | val_0_rmse: 0.25661 | val_1_rmse: 0.2768  |  0:01:05s
epoch 65 | loss: 0.06685 | val_0_rmse: 0.25663 | val_1_rmse: 0.27495 |  0:01:06s
epoch 66 | loss: 0.06625 | val_0_rmse: 0.25514 | val_1_rmse: 0.27344 |  0:01:07s
epoch 67 | loss: 0.06523 | val_0_rmse: 0.25172 | val_1_rmse: 0.27091 |  0:01:08s
epoch 68 | loss: 0.06448 | val_0_rmse: 0.25257 | val_1_rmse: 0.27307 |  0:01:09s
epoch 69 | loss: 0.06573 | val_0_rmse: 0.26092 | val_1_rmse: 0.27972 |  0:01:10s
epoch 70 | loss: 0.06619 | val_0_rmse: 0.25465 | val_1_rmse: 0.27513 |  0:01:11s
epoch 71 | loss: 0.06491 | val_0_rmse: 0.25303 | val_1_rmse: 0.27398 |  0:01:12s
epoch 72 | loss: 0.06422 | val_0_rmse: 0.25229 | val_1_rmse: 0.27561 |  0:01:13s
epoch 73 | loss: 0.06544 | val_0_rmse: 0.26414 | val_1_rmse: 0.2876  |  0:01:13s
epoch 74 | loss: 0.06641 | val_0_rmse: 0.2499  | val_1_rmse: 0.27292 |  0:01:14s
epoch 75 | loss: 0.06538 | val_0_rmse: 0.25088 | val_1_rmse: 0.27309 |  0:01:16s
epoch 76 | loss: 0.06557 | val_0_rmse: 0.25809 | val_1_rmse: 0.27963 |  0:01:17s
epoch 77 | loss: 0.06594 | val_0_rmse: 0.25173 | val_1_rmse: 0.27435 |  0:01:18s
epoch 78 | loss: 0.06436 | val_0_rmse: 0.25066 | val_1_rmse: 0.27393 |  0:01:19s
epoch 79 | loss: 0.06421 | val_0_rmse: 0.25302 | val_1_rmse: 0.27383 |  0:01:20s
epoch 80 | loss: 0.06341 | val_0_rmse: 0.24959 | val_1_rmse: 0.27266 |  0:01:20s
epoch 81 | loss: 0.06393 | val_0_rmse: 0.24958 | val_1_rmse: 0.2745  |  0:01:21s
epoch 82 | loss: 0.06368 | val_0_rmse: 0.24906 | val_1_rmse: 0.27245 |  0:01:22s
epoch 83 | loss: 0.06367 | val_0_rmse: 0.24864 | val_1_rmse: 0.2746  |  0:01:24s
epoch 84 | loss: 0.06354 | val_0_rmse: 0.25569 | val_1_rmse: 0.27888 |  0:01:25s
epoch 85 | loss: 0.06295 | val_0_rmse: 0.25031 | val_1_rmse: 0.27458 |  0:01:26s
epoch 86 | loss: 0.06306 | val_0_rmse: 0.24869 | val_1_rmse: 0.27346 |  0:01:26s
epoch 87 | loss: 0.06438 | val_0_rmse: 0.24814 | val_1_rmse: 0.27295 |  0:01:27s
epoch 88 | loss: 0.0627  | val_0_rmse: 0.24804 | val_1_rmse: 0.2739  |  0:01:28s
epoch 89 | loss: 0.06284 | val_0_rmse: 0.24683 | val_1_rmse: 0.27262 |  0:01:29s
epoch 90 | loss: 0.06265 | val_0_rmse: 0.24987 | val_1_rmse: 0.27471 |  0:01:30s
epoch 91 | loss: 0.06232 | val_0_rmse: 0.24814 | val_1_rmse: 0.27165 |  0:01:32s
epoch 92 | loss: 0.06308 | val_0_rmse: 0.24928 | val_1_rmse: 0.2739  |  0:01:32s
epoch 93 | loss: 0.06233 | val_0_rmse: 0.24688 | val_1_rmse: 0.2731  |  0:01:33s
epoch 94 | loss: 0.06251 | val_0_rmse: 0.24818 | val_1_rmse: 0.27339 |  0:01:34s
epoch 95 | loss: 0.06252 | val_0_rmse: 0.24624 | val_1_rmse: 0.27158 |  0:01:35s
epoch 96 | loss: 0.06274 | val_0_rmse: 0.24772 | val_1_rmse: 0.27402 |  0:01:36s
epoch 97 | loss: 0.06279 | val_0_rmse: 0.246   | val_1_rmse: 0.27197 |  0:01:37s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.27091
Best weights from best epoch are automatically used!
ended training at: 21:36:32
Feature importance:
Mean squared error is of 0.10771710162597302
Mean absolute error:0.20250775286083927
MAPE:0.21698149229155467
R2 score:0.15393252104699962
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:36:32
epoch 0  | loss: 2.31433 | val_0_rmse: 0.34538 | val_1_rmse: 0.38858 |  0:00:01s
epoch 1  | loss: 0.15921 | val_0_rmse: 0.31308 | val_1_rmse: 0.35926 |  0:00:02s
epoch 2  | loss: 0.1153  | val_0_rmse: 0.31365 | val_1_rmse: 0.35989 |  0:00:03s
epoch 3  | loss: 0.09739 | val_0_rmse: 0.32849 | val_1_rmse: 0.37198 |  0:00:04s
epoch 4  | loss: 0.09418 | val_0_rmse: 0.30665 | val_1_rmse: 0.35285 |  0:00:05s
epoch 5  | loss: 0.09231 | val_0_rmse: 0.31289 | val_1_rmse: 0.35851 |  0:00:06s
epoch 6  | loss: 0.09049 | val_0_rmse: 0.2986  | val_1_rmse: 0.3447  |  0:00:07s
epoch 7  | loss: 0.09005 | val_0_rmse: 0.29874 | val_1_rmse: 0.34441 |  0:00:08s
epoch 8  | loss: 0.08829 | val_0_rmse: 0.29592 | val_1_rmse: 0.34261 |  0:00:08s
epoch 9  | loss: 0.08671 | val_0_rmse: 0.29563 | val_1_rmse: 0.34208 |  0:00:10s
epoch 10 | loss: 0.08593 | val_0_rmse: 0.29168 | val_1_rmse: 0.33857 |  0:00:11s
epoch 11 | loss: 0.08489 | val_0_rmse: 0.28941 | val_1_rmse: 0.33578 |  0:00:12s
epoch 12 | loss: 0.08391 | val_0_rmse: 0.28828 | val_1_rmse: 0.33549 |  0:00:13s
epoch 13 | loss: 0.08221 | val_0_rmse: 0.28965 | val_1_rmse: 0.3368  |  0:00:14s
epoch 14 | loss: 0.08193 | val_0_rmse: 0.31053 | val_1_rmse: 0.35591 |  0:00:15s
epoch 15 | loss: 0.08549 | val_0_rmse: 0.28418 | val_1_rmse: 0.3314  |  0:00:16s
epoch 16 | loss: 0.0818  | val_0_rmse: 0.28546 | val_1_rmse: 0.33302 |  0:00:17s
epoch 17 | loss: 0.08176 | val_0_rmse: 0.28928 | val_1_rmse: 0.33661 |  0:00:18s
epoch 18 | loss: 0.07993 | val_0_rmse: 0.28278 | val_1_rmse: 0.33105 |  0:00:19s
epoch 19 | loss: 0.07981 | val_0_rmse: 0.28725 | val_1_rmse: 0.33582 |  0:00:20s
epoch 20 | loss: 0.07985 | val_0_rmse: 0.28397 | val_1_rmse: 0.33435 |  0:00:21s
epoch 21 | loss: 0.07964 | val_0_rmse: 0.28825 | val_1_rmse: 0.33782 |  0:00:22s
epoch 22 | loss: 0.08166 | val_0_rmse: 0.27957 | val_1_rmse: 0.32989 |  0:00:23s
epoch 23 | loss: 0.0789  | val_0_rmse: 0.27914 | val_1_rmse: 0.32829 |  0:00:24s
epoch 24 | loss: 0.08028 | val_0_rmse: 0.27933 | val_1_rmse: 0.32855 |  0:00:25s
epoch 25 | loss: 0.07823 | val_0_rmse: 0.27671 | val_1_rmse: 0.3279  |  0:00:26s
epoch 26 | loss: 0.07852 | val_0_rmse: 0.27857 | val_1_rmse: 0.33137 |  0:00:27s
epoch 27 | loss: 0.07746 | val_0_rmse: 0.2807  | val_1_rmse: 0.33375 |  0:00:28s
epoch 28 | loss: 0.07699 | val_0_rmse: 0.27701 | val_1_rmse: 0.33083 |  0:00:29s
epoch 29 | loss: 0.07737 | val_0_rmse: 0.2794  | val_1_rmse: 0.33297 |  0:00:30s
epoch 30 | loss: 0.07793 | val_0_rmse: 0.27887 | val_1_rmse: 0.33215 |  0:00:31s
epoch 31 | loss: 0.07666 | val_0_rmse: 0.27645 | val_1_rmse: 0.33068 |  0:00:32s
epoch 32 | loss: 0.07624 | val_0_rmse: 0.28414 | val_1_rmse: 0.33578 |  0:00:33s
epoch 33 | loss: 0.07986 | val_0_rmse: 0.29198 | val_1_rmse: 0.34583 |  0:00:34s
epoch 34 | loss: 0.07877 | val_0_rmse: 0.2766  | val_1_rmse: 0.33054 |  0:00:35s
epoch 35 | loss: 0.07581 | val_0_rmse: 0.27199 | val_1_rmse: 0.32704 |  0:00:36s
epoch 36 | loss: 0.07502 | val_0_rmse: 0.27127 | val_1_rmse: 0.32782 |  0:00:37s
epoch 37 | loss: 0.07462 | val_0_rmse: 0.27214 | val_1_rmse: 0.32825 |  0:00:38s
epoch 38 | loss: 0.07487 | val_0_rmse: 0.27058 | val_1_rmse: 0.32767 |  0:00:39s
epoch 39 | loss: 0.07747 | val_0_rmse: 0.27325 | val_1_rmse: 0.3272  |  0:00:40s
epoch 40 | loss: 0.07602 | val_0_rmse: 0.27325 | val_1_rmse: 0.32982 |  0:00:41s
epoch 41 | loss: 0.07623 | val_0_rmse: 0.2761  | val_1_rmse: 0.33257 |  0:00:42s
epoch 42 | loss: 0.07515 | val_0_rmse: 0.27518 | val_1_rmse: 0.3324  |  0:00:43s
epoch 43 | loss: 0.07425 | val_0_rmse: 0.27112 | val_1_rmse: 0.32933 |  0:00:44s
epoch 44 | loss: 0.0741  | val_0_rmse: 0.27111 | val_1_rmse: 0.32852 |  0:00:45s
epoch 45 | loss: 0.07363 | val_0_rmse: 0.27145 | val_1_rmse: 0.33014 |  0:00:46s
epoch 46 | loss: 0.07327 | val_0_rmse: 0.26686 | val_1_rmse: 0.32802 |  0:00:47s
epoch 47 | loss: 0.07338 | val_0_rmse: 0.27941 | val_1_rmse: 0.33615 |  0:00:48s
epoch 48 | loss: 0.07532 | val_0_rmse: 0.26717 | val_1_rmse: 0.32838 |  0:00:49s
epoch 49 | loss: 0.07333 | val_0_rmse: 0.26625 | val_1_rmse: 0.32816 |  0:00:50s
epoch 50 | loss: 0.07407 | val_0_rmse: 0.26953 | val_1_rmse: 0.32984 |  0:00:51s
epoch 51 | loss: 0.07218 | val_0_rmse: 0.26785 | val_1_rmse: 0.32594 |  0:00:52s
epoch 52 | loss: 0.07233 | val_0_rmse: 0.27151 | val_1_rmse: 0.32624 |  0:00:53s
epoch 53 | loss: 0.07271 | val_0_rmse: 0.2794  | val_1_rmse: 0.33319 |  0:00:54s
epoch 54 | loss: 0.07428 | val_0_rmse: 0.2785  | val_1_rmse: 0.33136 |  0:00:55s
epoch 55 | loss: 0.07526 | val_0_rmse: 0.27493 | val_1_rmse: 0.32807 |  0:00:56s
epoch 56 | loss: 0.07384 | val_0_rmse: 0.27077 | val_1_rmse: 0.32459 |  0:00:57s
epoch 57 | loss: 0.07225 | val_0_rmse: 0.27517 | val_1_rmse: 0.32423 |  0:00:58s
epoch 58 | loss: 0.07155 | val_0_rmse: 0.27366 | val_1_rmse: 0.32456 |  0:00:59s
epoch 59 | loss: 0.0704  | val_0_rmse: 0.27125 | val_1_rmse: 0.32662 |  0:01:00s
epoch 60 | loss: 0.07113 | val_0_rmse: 0.2722  | val_1_rmse: 0.32463 |  0:01:01s
epoch 61 | loss: 0.07106 | val_0_rmse: 0.27797 | val_1_rmse: 0.33    |  0:01:02s
epoch 62 | loss: 0.07123 | val_0_rmse: 0.27516 | val_1_rmse: 0.32539 |  0:01:03s
epoch 63 | loss: 0.07094 | val_0_rmse: 0.27472 | val_1_rmse: 0.32565 |  0:01:04s
epoch 64 | loss: 0.07055 | val_0_rmse: 0.27663 | val_1_rmse: 0.33349 |  0:01:05s
epoch 65 | loss: 0.07134 | val_0_rmse: 0.26066 | val_1_rmse: 0.32367 |  0:01:06s
epoch 66 | loss: 0.07121 | val_0_rmse: 0.26425 | val_1_rmse: 0.32228 |  0:01:07s
epoch 67 | loss: 0.07143 | val_0_rmse: 0.26309 | val_1_rmse: 0.32255 |  0:01:08s
epoch 68 | loss: 0.06956 | val_0_rmse: 0.26033 | val_1_rmse: 0.32274 |  0:01:09s
epoch 69 | loss: 0.06879 | val_0_rmse: 0.25779 | val_1_rmse: 0.32252 |  0:01:10s
epoch 70 | loss: 0.06895 | val_0_rmse: 0.26522 | val_1_rmse: 0.32571 |  0:01:11s
epoch 71 | loss: 0.0704  | val_0_rmse: 0.25797 | val_1_rmse: 0.32402 |  0:01:12s
epoch 72 | loss: 0.06979 | val_0_rmse: 0.25769 | val_1_rmse: 0.32561 |  0:01:13s
epoch 73 | loss: 0.06922 | val_0_rmse: 0.25724 | val_1_rmse: 0.32096 |  0:01:14s
epoch 74 | loss: 0.06848 | val_0_rmse: 0.25437 | val_1_rmse: 0.32186 |  0:01:15s
epoch 75 | loss: 0.06889 | val_0_rmse: 0.26195 | val_1_rmse: 0.3305  |  0:01:16s
epoch 76 | loss: 0.06874 | val_0_rmse: 0.25495 | val_1_rmse: 0.32374 |  0:01:17s
epoch 77 | loss: 0.06756 | val_0_rmse: 0.26149 | val_1_rmse: 0.32699 |  0:01:18s
epoch 78 | loss: 0.06846 | val_0_rmse: 0.2569  | val_1_rmse: 0.3247  |  0:01:19s
epoch 79 | loss: 0.0673  | val_0_rmse: 0.25799 | val_1_rmse: 0.3244  |  0:01:20s
epoch 80 | loss: 0.06755 | val_0_rmse: 0.25326 | val_1_rmse: 0.3215  |  0:01:21s
epoch 81 | loss: 0.06685 | val_0_rmse: 0.25877 | val_1_rmse: 0.32018 |  0:01:22s
epoch 82 | loss: 0.06799 | val_0_rmse: 0.25421 | val_1_rmse: 0.32121 |  0:01:23s
epoch 83 | loss: 0.06747 | val_0_rmse: 0.25945 | val_1_rmse: 0.3252  |  0:01:24s
epoch 84 | loss: 0.06595 | val_0_rmse: 0.25697 | val_1_rmse: 0.32701 |  0:01:24s
epoch 85 | loss: 0.06727 | val_0_rmse: 0.25554 | val_1_rmse: 0.32575 |  0:01:25s
epoch 86 | loss: 0.06707 | val_0_rmse: 0.25268 | val_1_rmse: 0.32006 |  0:01:27s
epoch 87 | loss: 0.06507 | val_0_rmse: 0.2548  | val_1_rmse: 0.32231 |  0:01:28s
epoch 88 | loss: 0.06654 | val_0_rmse: 0.25073 | val_1_rmse: 0.32153 |  0:01:29s
epoch 89 | loss: 0.06535 | val_0_rmse: 0.25403 | val_1_rmse: 0.3279  |  0:01:30s
epoch 90 | loss: 0.06587 | val_0_rmse: 0.25169 | val_1_rmse: 0.3235  |  0:01:31s
epoch 91 | loss: 0.06582 | val_0_rmse: 0.25338 | val_1_rmse: 0.32933 |  0:01:31s
epoch 92 | loss: 0.06538 | val_0_rmse: 0.24982 | val_1_rmse: 0.32268 |  0:01:32s
epoch 93 | loss: 0.06362 | val_0_rmse: 0.25166 | val_1_rmse: 0.325   |  0:01:33s
epoch 94 | loss: 0.06517 | val_0_rmse: 0.25804 | val_1_rmse: 0.33327 |  0:01:34s
epoch 95 | loss: 0.06675 | val_0_rmse: 0.25772 | val_1_rmse: 0.32602 |  0:01:36s
epoch 96 | loss: 0.06594 | val_0_rmse: 0.25149 | val_1_rmse: 0.32454 |  0:01:37s
epoch 97 | loss: 0.06542 | val_0_rmse: 0.25331 | val_1_rmse: 0.32958 |  0:01:37s
epoch 98 | loss: 0.0662  | val_0_rmse: 0.2597  | val_1_rmse: 0.32423 |  0:01:38s
epoch 99 | loss: 0.06919 | val_0_rmse: 0.2585  | val_1_rmse: 0.32465 |  0:01:39s
epoch 100| loss: 0.0662  | val_0_rmse: 0.25575 | val_1_rmse: 0.32816 |  0:01:40s
epoch 101| loss: 0.06736 | val_0_rmse: 0.25874 | val_1_rmse: 0.33144 |  0:01:41s
epoch 102| loss: 0.06715 | val_0_rmse: 0.25405 | val_1_rmse: 0.32486 |  0:01:42s
epoch 103| loss: 0.0652  | val_0_rmse: 0.25131 | val_1_rmse: 0.32499 |  0:01:43s
epoch 104| loss: 0.06622 | val_0_rmse: 0.25298 | val_1_rmse: 0.32246 |  0:01:44s
epoch 105| loss: 0.06511 | val_0_rmse: 0.2501  | val_1_rmse: 0.32295 |  0:01:45s
epoch 106| loss: 0.06506 | val_0_rmse: 0.24975 | val_1_rmse: 0.32725 |  0:01:46s
epoch 107| loss: 0.065   | val_0_rmse: 0.25    | val_1_rmse: 0.32324 |  0:01:47s
epoch 108| loss: 0.06538 | val_0_rmse: 0.25493 | val_1_rmse: 0.32334 |  0:01:48s
epoch 109| loss: 0.06518 | val_0_rmse: 0.26676 | val_1_rmse: 0.3437  |  0:01:49s
epoch 110| loss: 0.06537 | val_0_rmse: 0.25359 | val_1_rmse: 0.32778 |  0:01:50s
epoch 111| loss: 0.06433 | val_0_rmse: 0.24944 | val_1_rmse: 0.32468 |  0:01:51s
epoch 112| loss: 0.06256 | val_0_rmse: 0.25694 | val_1_rmse: 0.32759 |  0:01:52s
epoch 113| loss: 0.06313 | val_0_rmse: 0.25268 | val_1_rmse: 0.33465 |  0:01:53s
epoch 114| loss: 0.06342 | val_0_rmse: 0.2481  | val_1_rmse: 0.32672 |  0:01:54s
epoch 115| loss: 0.06293 | val_0_rmse: 0.24754 | val_1_rmse: 0.32628 |  0:01:55s
epoch 116| loss: 0.06305 | val_0_rmse: 0.2483  | val_1_rmse: 0.32586 |  0:01:56s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.32006
Best weights from best epoch are automatically used!
ended training at: 21:38:30
Feature importance:
Mean squared error is of 0.0681811939381408
Mean absolute error:0.19183877096723895
MAPE:0.21029175686947563
R2 score:0.19810748672212508
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:38:30
epoch 0  | loss: 2.53122 | val_0_rmse: 0.55745 | val_1_rmse: 0.54952 |  0:00:00s
epoch 1  | loss: 0.20613 | val_0_rmse: 0.33816 | val_1_rmse: 0.3225  |  0:00:01s
epoch 2  | loss: 0.12001 | val_0_rmse: 0.3238  | val_1_rmse: 0.30312 |  0:00:03s
epoch 3  | loss: 0.10459 | val_0_rmse: 0.31858 | val_1_rmse: 0.29864 |  0:00:04s
epoch 4  | loss: 0.10079 | val_0_rmse: 0.31114 | val_1_rmse: 0.29089 |  0:00:05s
epoch 5  | loss: 0.1     | val_0_rmse: 0.31059 | val_1_rmse: 0.29189 |  0:00:06s
epoch 6  | loss: 0.10055 | val_0_rmse: 0.31661 | val_1_rmse: 0.29917 |  0:00:07s
epoch 7  | loss: 0.09737 | val_0_rmse: 0.31202 | val_1_rmse: 0.29465 |  0:00:08s
epoch 8  | loss: 0.09539 | val_0_rmse: 0.31177 | val_1_rmse: 0.29202 |  0:00:09s
epoch 9  | loss: 0.09515 | val_0_rmse: 0.30293 | val_1_rmse: 0.28418 |  0:00:09s
epoch 10 | loss: 0.09488 | val_0_rmse: 0.30372 | val_1_rmse: 0.28445 |  0:00:10s
epoch 11 | loss: 0.0927  | val_0_rmse: 0.30279 | val_1_rmse: 0.2842  |  0:00:12s
epoch 12 | loss: 0.0915  | val_0_rmse: 0.30101 | val_1_rmse: 0.28357 |  0:00:13s
epoch 13 | loss: 0.09116 | val_0_rmse: 0.29969 | val_1_rmse: 0.28172 |  0:00:14s
epoch 14 | loss: 0.09191 | val_0_rmse: 0.30194 | val_1_rmse: 0.28323 |  0:00:15s
epoch 15 | loss: 0.09255 | val_0_rmse: 0.30446 | val_1_rmse: 0.28533 |  0:00:16s
epoch 16 | loss: 0.08951 | val_0_rmse: 0.29968 | val_1_rmse: 0.28023 |  0:00:17s
epoch 17 | loss: 0.08942 | val_0_rmse: 0.30178 | val_1_rmse: 0.28239 |  0:00:18s
epoch 18 | loss: 0.08893 | val_0_rmse: 0.29791 | val_1_rmse: 0.27924 |  0:00:18s
epoch 19 | loss: 0.08745 | val_0_rmse: 0.30023 | val_1_rmse: 0.28029 |  0:00:19s
epoch 20 | loss: 0.08695 | val_0_rmse: 0.2987  | val_1_rmse: 0.27897 |  0:00:21s
epoch 21 | loss: 0.08783 | val_0_rmse: 0.29429 | val_1_rmse: 0.27573 |  0:00:22s
epoch 22 | loss: 0.086   | val_0_rmse: 0.29526 | val_1_rmse: 0.27765 |  0:00:23s
epoch 23 | loss: 0.0865  | val_0_rmse: 0.29654 | val_1_rmse: 0.27892 |  0:00:24s
epoch 24 | loss: 0.08613 | val_0_rmse: 0.29482 | val_1_rmse: 0.27668 |  0:00:25s
epoch 25 | loss: 0.08517 | val_0_rmse: 0.29391 | val_1_rmse: 0.27542 |  0:00:26s
epoch 26 | loss: 0.08524 | val_0_rmse: 0.29296 | val_1_rmse: 0.27591 |  0:00:27s
epoch 27 | loss: 0.08483 | val_0_rmse: 0.29226 | val_1_rmse: 0.27466 |  0:00:27s
epoch 28 | loss: 0.08491 | val_0_rmse: 0.29666 | val_1_rmse: 0.28003 |  0:00:28s
epoch 29 | loss: 0.08758 | val_0_rmse: 0.29571 | val_1_rmse: 0.27805 |  0:00:30s
epoch 30 | loss: 0.08476 | val_0_rmse: 0.29036 | val_1_rmse: 0.2739  |  0:00:31s
epoch 31 | loss: 0.08247 | val_0_rmse: 0.29162 | val_1_rmse: 0.27498 |  0:00:32s
epoch 32 | loss: 0.08488 | val_0_rmse: 0.28899 | val_1_rmse: 0.2733  |  0:00:33s
epoch 33 | loss: 0.08362 | val_0_rmse: 0.28911 | val_1_rmse: 0.2753  |  0:00:34s
epoch 34 | loss: 0.08537 | val_0_rmse: 0.28959 | val_1_rmse: 0.27446 |  0:00:35s
epoch 35 | loss: 0.08312 | val_0_rmse: 0.28878 | val_1_rmse: 0.27372 |  0:00:35s
epoch 36 | loss: 0.08416 | val_0_rmse: 0.28361 | val_1_rmse: 0.27175 |  0:00:36s
epoch 37 | loss: 0.08204 | val_0_rmse: 0.28543 | val_1_rmse: 0.27191 |  0:00:37s
epoch 38 | loss: 0.0832  | val_0_rmse: 0.28808 | val_1_rmse: 0.27416 |  0:00:39s
epoch 39 | loss: 0.08167 | val_0_rmse: 0.28164 | val_1_rmse: 0.27165 |  0:00:40s
epoch 40 | loss: 0.0822  | val_0_rmse: 0.28147 | val_1_rmse: 0.27269 |  0:00:41s
epoch 41 | loss: 0.0807  | val_0_rmse: 0.28189 | val_1_rmse: 0.27413 |  0:00:42s
epoch 42 | loss: 0.08051 | val_0_rmse: 0.27836 | val_1_rmse: 0.27247 |  0:00:43s
epoch 43 | loss: 0.08106 | val_0_rmse: 0.28035 | val_1_rmse: 0.27747 |  0:00:43s
epoch 44 | loss: 0.08102 | val_0_rmse: 0.28128 | val_1_rmse: 0.2737  |  0:00:44s
epoch 45 | loss: 0.08135 | val_0_rmse: 0.28162 | val_1_rmse: 0.2736  |  0:00:45s
epoch 46 | loss: 0.07969 | val_0_rmse: 0.27899 | val_1_rmse: 0.2747  |  0:00:47s
epoch 47 | loss: 0.07903 | val_0_rmse: 0.27829 | val_1_rmse: 0.27671 |  0:00:48s
epoch 48 | loss: 0.08023 | val_0_rmse: 0.28035 | val_1_rmse: 0.27648 |  0:00:49s
epoch 49 | loss: 0.08147 | val_0_rmse: 0.27835 | val_1_rmse: 0.27134 |  0:00:50s
epoch 50 | loss: 0.07956 | val_0_rmse: 0.27823 | val_1_rmse: 0.26936 |  0:00:51s
epoch 51 | loss: 0.07863 | val_0_rmse: 0.27594 | val_1_rmse: 0.27135 |  0:00:52s
epoch 52 | loss: 0.07818 | val_0_rmse: 0.27467 | val_1_rmse: 0.2726  |  0:00:52s
epoch 53 | loss: 0.08126 | val_0_rmse: 0.2752  | val_1_rmse: 0.27475 |  0:00:53s
epoch 54 | loss: 0.08134 | val_0_rmse: 0.28616 | val_1_rmse: 0.28888 |  0:00:54s
epoch 55 | loss: 0.08063 | val_0_rmse: 0.27674 | val_1_rmse: 0.27285 |  0:00:55s
epoch 56 | loss: 0.07815 | val_0_rmse: 0.2753  | val_1_rmse: 0.27407 |  0:00:57s
epoch 57 | loss: 0.0766  | val_0_rmse: 0.27582 | val_1_rmse: 0.27569 |  0:00:58s
epoch 58 | loss: 0.07875 | val_0_rmse: 0.27228 | val_1_rmse: 0.26888 |  0:00:58s
epoch 59 | loss: 0.07695 | val_0_rmse: 0.27982 | val_1_rmse: 0.28309 |  0:00:59s
epoch 60 | loss: 0.08099 | val_0_rmse: 0.27789 | val_1_rmse: 0.27196 |  0:01:00s
epoch 61 | loss: 0.07732 | val_0_rmse: 0.27325 | val_1_rmse: 0.27443 |  0:01:01s
epoch 62 | loss: 0.07664 | val_0_rmse: 0.27162 | val_1_rmse: 0.27366 |  0:01:02s
epoch 63 | loss: 0.07696 | val_0_rmse: 0.27326 | val_1_rmse: 0.27453 |  0:01:03s
epoch 64 | loss: 0.0775  | val_0_rmse: 0.27396 | val_1_rmse: 0.27422 |  0:01:04s
epoch 65 | loss: 0.07679 | val_0_rmse: 0.27323 | val_1_rmse: 0.2723  |  0:01:05s
epoch 66 | loss: 0.07699 | val_0_rmse: 0.27035 | val_1_rmse: 0.2721  |  0:01:06s
epoch 67 | loss: 0.07535 | val_0_rmse: 0.27875 | val_1_rmse: 0.27978 |  0:01:07s
epoch 68 | loss: 0.07706 | val_0_rmse: 0.28078 | val_1_rmse: 0.28235 |  0:01:08s
epoch 69 | loss: 0.08128 | val_0_rmse: 0.28414 | val_1_rmse: 0.28721 |  0:01:09s
epoch 70 | loss: 0.08067 | val_0_rmse: 0.28465 | val_1_rmse: 0.288   |  0:01:10s
epoch 71 | loss: 0.07952 | val_0_rmse: 0.28699 | val_1_rmse: 0.30162 |  0:01:11s
epoch 72 | loss: 0.07908 | val_0_rmse: 0.27977 | val_1_rmse: 0.27422 |  0:01:12s
epoch 73 | loss: 0.0785  | val_0_rmse: 0.27572 | val_1_rmse: 0.28106 |  0:01:13s
epoch 74 | loss: 0.07787 | val_0_rmse: 0.27907 | val_1_rmse: 0.29836 |  0:01:14s
epoch 75 | loss: 0.07611 | val_0_rmse: 0.27868 | val_1_rmse: 0.29804 |  0:01:15s
epoch 76 | loss: 0.07697 | val_0_rmse: 0.27738 | val_1_rmse: 0.29585 |  0:01:16s
epoch 77 | loss: 0.07797 | val_0_rmse: 0.27585 | val_1_rmse: 0.27067 |  0:01:17s
epoch 78 | loss: 0.07593 | val_0_rmse: 0.28145 | val_1_rmse: 0.2814  |  0:01:18s
epoch 79 | loss: 0.07793 | val_0_rmse: 0.27269 | val_1_rmse: 0.27241 |  0:01:19s
epoch 80 | loss: 0.07824 | val_0_rmse: 0.27148 | val_1_rmse: 0.27504 |  0:01:20s
epoch 81 | loss: 0.07586 | val_0_rmse: 0.27109 | val_1_rmse: 0.27689 |  0:01:21s
epoch 82 | loss: 0.07598 | val_0_rmse: 0.27343 | val_1_rmse: 0.28034 |  0:01:22s
epoch 83 | loss: 0.0769  | val_0_rmse: 0.28229 | val_1_rmse: 0.28536 |  0:01:23s
epoch 84 | loss: 0.0792  | val_0_rmse: 0.27317 | val_1_rmse: 0.28004 |  0:01:24s
epoch 85 | loss: 0.07376 | val_0_rmse: 0.26778 | val_1_rmse: 0.27349 |  0:01:25s
epoch 86 | loss: 0.07397 | val_0_rmse: 0.27363 | val_1_rmse: 0.27642 |  0:01:26s
epoch 87 | loss: 0.0762  | val_0_rmse: 0.26896 | val_1_rmse: 0.27167 |  0:01:27s
epoch 88 | loss: 0.07404 | val_0_rmse: 0.27225 | val_1_rmse: 0.27469 |  0:01:28s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.26888
Best weights from best epoch are automatically used!
ended training at: 21:39:59
Feature importance:
Mean squared error is of 0.06685661465045807
Mean absolute error:0.19231826496013713
MAPE:0.20768565478517514
R2 score:0.2126870365580773
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:40:00
epoch 0  | loss: 2.4961  | val_0_rmse: 0.47055 | val_1_rmse: 0.47152 |  0:00:00s
epoch 1  | loss: 0.16313 | val_0_rmse: 0.32952 | val_1_rmse: 0.33466 |  0:00:02s
epoch 2  | loss: 0.10932 | val_0_rmse: 0.32279 | val_1_rmse: 0.32977 |  0:00:03s
epoch 3  | loss: 0.09735 | val_0_rmse: 0.29894 | val_1_rmse: 0.31004 |  0:00:04s
epoch 4  | loss: 0.0952  | val_0_rmse: 0.29741 | val_1_rmse: 0.30849 |  0:00:05s
epoch 5  | loss: 0.09106 | val_0_rmse: 0.30202 | val_1_rmse: 0.31223 |  0:00:06s
epoch 6  | loss: 0.08872 | val_0_rmse: 0.30112 | val_1_rmse: 0.31272 |  0:00:07s
epoch 7  | loss: 0.08803 | val_0_rmse: 0.30524 | val_1_rmse: 0.31688 |  0:00:08s
epoch 8  | loss: 0.08785 | val_0_rmse: 0.30108 | val_1_rmse: 0.31265 |  0:00:09s
epoch 9  | loss: 0.08673 | val_0_rmse: 0.29919 | val_1_rmse: 0.31122 |  0:00:10s
epoch 10 | loss: 0.08602 | val_0_rmse: 0.29582 | val_1_rmse: 0.30776 |  0:00:11s
epoch 11 | loss: 0.08539 | val_0_rmse: 0.29161 | val_1_rmse: 0.30321 |  0:00:12s
epoch 12 | loss: 0.08243 | val_0_rmse: 0.2902  | val_1_rmse: 0.30119 |  0:00:13s
epoch 13 | loss: 0.08194 | val_0_rmse: 0.288   | val_1_rmse: 0.29945 |  0:00:14s
epoch 14 | loss: 0.08193 | val_0_rmse: 0.28592 | val_1_rmse: 0.29788 |  0:00:15s
epoch 15 | loss: 0.08087 | val_0_rmse: 0.28877 | val_1_rmse: 0.29957 |  0:00:16s
epoch 16 | loss: 0.08128 | val_0_rmse: 0.28569 | val_1_rmse: 0.29949 |  0:00:17s
epoch 17 | loss: 0.0836  | val_0_rmse: 0.29583 | val_1_rmse: 0.30612 |  0:00:18s
epoch 18 | loss: 0.0821  | val_0_rmse: 0.28238 | val_1_rmse: 0.29487 |  0:00:19s
epoch 19 | loss: 0.07838 | val_0_rmse: 0.2794  | val_1_rmse: 0.2931  |  0:00:20s
epoch 20 | loss: 0.07851 | val_0_rmse: 0.28054 | val_1_rmse: 0.29628 |  0:00:21s
epoch 21 | loss: 0.07806 | val_0_rmse: 0.2823  | val_1_rmse: 0.29808 |  0:00:22s
epoch 22 | loss: 0.07837 | val_0_rmse: 0.2782  | val_1_rmse: 0.29279 |  0:00:23s
epoch 23 | loss: 0.07748 | val_0_rmse: 0.28246 | val_1_rmse: 0.29523 |  0:00:24s
epoch 24 | loss: 0.07687 | val_0_rmse: 0.27905 | val_1_rmse: 0.29304 |  0:00:25s
epoch 25 | loss: 0.0762  | val_0_rmse: 0.28019 | val_1_rmse: 0.29365 |  0:00:26s
epoch 26 | loss: 0.07658 | val_0_rmse: 0.27638 | val_1_rmse: 0.29079 |  0:00:27s
epoch 27 | loss: 0.0767  | val_0_rmse: 0.27519 | val_1_rmse: 0.28918 |  0:00:28s
epoch 28 | loss: 0.07579 | val_0_rmse: 0.2744  | val_1_rmse: 0.28921 |  0:00:29s
epoch 29 | loss: 0.07637 | val_0_rmse: 0.27358 | val_1_rmse: 0.28775 |  0:00:30s
epoch 30 | loss: 0.07498 | val_0_rmse: 0.27309 | val_1_rmse: 0.28766 |  0:00:31s
epoch 31 | loss: 0.07592 | val_0_rmse: 0.27625 | val_1_rmse: 0.29255 |  0:00:32s
epoch 32 | loss: 0.07637 | val_0_rmse: 0.27189 | val_1_rmse: 0.28735 |  0:00:33s
epoch 33 | loss: 0.07505 | val_0_rmse: 0.27428 | val_1_rmse: 0.28862 |  0:00:34s
epoch 34 | loss: 0.07416 | val_0_rmse: 0.2727  | val_1_rmse: 0.28693 |  0:00:35s
epoch 35 | loss: 0.07365 | val_0_rmse: 0.27073 | val_1_rmse: 0.28518 |  0:00:36s
epoch 36 | loss: 0.07398 | val_0_rmse: 0.27013 | val_1_rmse: 0.2838  |  0:00:37s
epoch 37 | loss: 0.07319 | val_0_rmse: 0.28064 | val_1_rmse: 0.29293 |  0:00:38s
epoch 38 | loss: 0.07418 | val_0_rmse: 0.27133 | val_1_rmse: 0.28472 |  0:00:39s
epoch 39 | loss: 0.07222 | val_0_rmse: 0.2691  | val_1_rmse: 0.28246 |  0:00:40s
epoch 40 | loss: 0.07321 | val_0_rmse: 0.26682 | val_1_rmse: 0.28172 |  0:00:41s
epoch 41 | loss: 0.07257 | val_0_rmse: 0.26664 | val_1_rmse: 0.28327 |  0:00:42s
epoch 42 | loss: 0.07313 | val_0_rmse: 0.26705 | val_1_rmse: 0.28251 |  0:00:43s
epoch 43 | loss: 0.07248 | val_0_rmse: 0.26476 | val_1_rmse: 0.2799  |  0:00:44s
epoch 44 | loss: 0.07179 | val_0_rmse: 0.26839 | val_1_rmse: 0.28266 |  0:00:45s
epoch 45 | loss: 0.07217 | val_0_rmse: 0.26752 | val_1_rmse: 0.28105 |  0:00:46s
epoch 46 | loss: 0.07167 | val_0_rmse: 0.26627 | val_1_rmse: 0.28148 |  0:00:47s
epoch 47 | loss: 0.07323 | val_0_rmse: 0.26889 | val_1_rmse: 0.28306 |  0:00:48s
epoch 48 | loss: 0.07222 | val_0_rmse: 0.26994 | val_1_rmse: 0.28565 |  0:00:49s
epoch 49 | loss: 0.07149 | val_0_rmse: 0.26801 | val_1_rmse: 0.28385 |  0:00:50s
epoch 50 | loss: 0.07089 | val_0_rmse: 0.26964 | val_1_rmse: 0.28474 |  0:00:51s
epoch 51 | loss: 0.07135 | val_0_rmse: 0.26307 | val_1_rmse: 0.28095 |  0:00:52s
epoch 52 | loss: 0.07066 | val_0_rmse: 0.26235 | val_1_rmse: 0.28146 |  0:00:53s
epoch 53 | loss: 0.07228 | val_0_rmse: 0.26273 | val_1_rmse: 0.28186 |  0:00:54s
epoch 54 | loss: 0.07186 | val_0_rmse: 0.26137 | val_1_rmse: 0.28101 |  0:00:55s
epoch 55 | loss: 0.06922 | val_0_rmse: 0.26125 | val_1_rmse: 0.27932 |  0:00:56s
epoch 56 | loss: 0.06931 | val_0_rmse: 0.26138 | val_1_rmse: 0.2782  |  0:00:57s
epoch 57 | loss: 0.06976 | val_0_rmse: 0.25897 | val_1_rmse: 0.27735 |  0:00:58s
epoch 58 | loss: 0.06976 | val_0_rmse: 0.26031 | val_1_rmse: 0.28058 |  0:00:59s
epoch 59 | loss: 0.06932 | val_0_rmse: 0.25858 | val_1_rmse: 0.27786 |  0:01:00s
epoch 60 | loss: 0.06887 | val_0_rmse: 0.26307 | val_1_rmse: 0.28182 |  0:01:01s
epoch 61 | loss: 0.07005 | val_0_rmse: 0.25994 | val_1_rmse: 0.27991 |  0:01:02s
epoch 62 | loss: 0.06937 | val_0_rmse: 0.25848 | val_1_rmse: 0.27804 |  0:01:03s
epoch 63 | loss: 0.06945 | val_0_rmse: 0.26067 | val_1_rmse: 0.27981 |  0:01:04s
epoch 64 | loss: 0.06895 | val_0_rmse: 0.25953 | val_1_rmse: 0.27898 |  0:01:05s
epoch 65 | loss: 0.06933 | val_0_rmse: 0.25912 | val_1_rmse: 0.27748 |  0:01:06s
epoch 66 | loss: 0.06882 | val_0_rmse: 0.25932 | val_1_rmse: 0.27917 |  0:01:07s
epoch 67 | loss: 0.06971 | val_0_rmse: 0.25867 | val_1_rmse: 0.27797 |  0:01:08s
epoch 68 | loss: 0.06864 | val_0_rmse: 0.25892 | val_1_rmse: 0.27838 |  0:01:09s
epoch 69 | loss: 0.06818 | val_0_rmse: 0.25794 | val_1_rmse: 0.27788 |  0:01:10s
epoch 70 | loss: 0.06793 | val_0_rmse: 0.25628 | val_1_rmse: 0.27591 |  0:01:11s
epoch 71 | loss: 0.06815 | val_0_rmse: 0.25545 | val_1_rmse: 0.27531 |  0:01:12s
epoch 72 | loss: 0.06857 | val_0_rmse: 0.25421 | val_1_rmse: 0.2741  |  0:01:13s
epoch 73 | loss: 0.06767 | val_0_rmse: 0.25686 | val_1_rmse: 0.27694 |  0:01:14s
epoch 74 | loss: 0.06766 | val_0_rmse: 0.25775 | val_1_rmse: 0.27766 |  0:01:15s
epoch 75 | loss: 0.06737 | val_0_rmse: 0.25405 | val_1_rmse: 0.27434 |  0:01:16s
epoch 76 | loss: 0.06668 | val_0_rmse: 0.25451 | val_1_rmse: 0.27545 |  0:01:17s
epoch 77 | loss: 0.06624 | val_0_rmse: 0.2594  | val_1_rmse: 0.28032 |  0:01:18s
epoch 78 | loss: 0.06756 | val_0_rmse: 0.25357 | val_1_rmse: 0.27319 |  0:01:19s
epoch 79 | loss: 0.06667 | val_0_rmse: 0.255   | val_1_rmse: 0.27542 |  0:01:20s
epoch 80 | loss: 0.0677  | val_0_rmse: 0.25577 | val_1_rmse: 0.28032 |  0:01:21s
epoch 81 | loss: 0.06766 | val_0_rmse: 0.25433 | val_1_rmse: 0.2788  |  0:01:22s
epoch 82 | loss: 0.06584 | val_0_rmse: 0.26331 | val_1_rmse: 0.28654 |  0:01:23s
epoch 83 | loss: 0.06632 | val_0_rmse: 0.25437 | val_1_rmse: 0.27986 |  0:01:24s
epoch 84 | loss: 0.06623 | val_0_rmse: 0.25204 | val_1_rmse: 0.2756  |  0:01:25s
epoch 85 | loss: 0.06797 | val_0_rmse: 0.25712 | val_1_rmse: 0.27652 |  0:01:26s
epoch 86 | loss: 0.06724 | val_0_rmse: 0.25456 | val_1_rmse: 0.27728 |  0:01:27s
epoch 87 | loss: 0.06666 | val_0_rmse: 0.2559  | val_1_rmse: 0.28002 |  0:01:28s
epoch 88 | loss: 0.06755 | val_0_rmse: 0.25578 | val_1_rmse: 0.28282 |  0:01:29s
epoch 89 | loss: 0.06687 | val_0_rmse: 0.25343 | val_1_rmse: 0.27677 |  0:01:30s
epoch 90 | loss: 0.06592 | val_0_rmse: 0.25316 | val_1_rmse: 0.27399 |  0:01:31s
epoch 91 | loss: 0.06702 | val_0_rmse: 0.26272 | val_1_rmse: 0.28178 |  0:01:32s
epoch 92 | loss: 0.06532 | val_0_rmse: 0.25353 | val_1_rmse: 0.27366 |  0:01:33s
epoch 93 | loss: 0.06595 | val_0_rmse: 0.25788 | val_1_rmse: 0.27653 |  0:01:34s
epoch 94 | loss: 0.06907 | val_0_rmse: 0.25966 | val_1_rmse: 0.27968 |  0:01:35s
epoch 95 | loss: 0.06926 | val_0_rmse: 0.26317 | val_1_rmse: 0.28101 |  0:01:36s
epoch 96 | loss: 0.07309 | val_0_rmse: 0.2682  | val_1_rmse: 0.28919 |  0:01:37s
epoch 97 | loss: 0.07233 | val_0_rmse: 0.26499 | val_1_rmse: 0.28635 |  0:01:38s
epoch 98 | loss: 0.07176 | val_0_rmse: 0.26678 | val_1_rmse: 0.28897 |  0:01:39s
epoch 99 | loss: 0.07158 | val_0_rmse: 0.27743 | val_1_rmse: 0.30222 |  0:01:40s
epoch 100| loss: 0.07039 | val_0_rmse: 0.2608  | val_1_rmse: 0.27994 |  0:01:41s
epoch 101| loss: 0.06971 | val_0_rmse: 0.25916 | val_1_rmse: 0.28025 |  0:01:42s
epoch 102| loss: 0.06937 | val_0_rmse: 0.26304 | val_1_rmse: 0.28762 |  0:01:43s
epoch 103| loss: 0.06841 | val_0_rmse: 0.25955 | val_1_rmse: 0.28364 |  0:01:44s
epoch 104| loss: 0.06761 | val_0_rmse: 0.25808 | val_1_rmse: 0.2841  |  0:01:45s
epoch 105| loss: 0.06659 | val_0_rmse: 0.25975 | val_1_rmse: 0.28209 |  0:01:46s
epoch 106| loss: 0.06628 | val_0_rmse: 0.25777 | val_1_rmse: 0.28183 |  0:01:47s
epoch 107| loss: 0.066   | val_0_rmse: 0.25173 | val_1_rmse: 0.27637 |  0:01:48s
epoch 108| loss: 0.06593 | val_0_rmse: 0.25121 | val_1_rmse: 0.27589 |  0:01:49s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.27319
Best weights from best epoch are automatically used!
ended training at: 21:41:50
Feature importance:
Mean squared error is of 0.09433007702122105
Mean absolute error:0.1946676908080264
MAPE:0.2133144214914706
R2 score:0.18638872393700467
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:41:51
epoch 0  | loss: 1.13783 | val_0_rmse: 0.71456 | val_1_rmse: 0.70637 |  0:00:00s
epoch 1  | loss: 0.5051  | val_0_rmse: 0.68986 | val_1_rmse: 0.68156 |  0:00:01s
epoch 2  | loss: 0.45821 | val_0_rmse: 0.6678  | val_1_rmse: 0.67104 |  0:00:02s
epoch 3  | loss: 0.3695  | val_0_rmse: 0.58997 | val_1_rmse: 0.59287 |  0:00:03s
epoch 4  | loss: 0.3125  | val_0_rmse: 0.58326 | val_1_rmse: 0.58525 |  0:00:04s
epoch 5  | loss: 0.28209 | val_0_rmse: 0.56105 | val_1_rmse: 0.56614 |  0:00:05s
epoch 6  | loss: 0.26765 | val_0_rmse: 0.55103 | val_1_rmse: 0.55847 |  0:00:06s
epoch 7  | loss: 0.26006 | val_0_rmse: 0.52366 | val_1_rmse: 0.53358 |  0:00:07s
epoch 8  | loss: 0.23929 | val_0_rmse: 0.51504 | val_1_rmse: 0.5254  |  0:00:08s
epoch 9  | loss: 0.23124 | val_0_rmse: 0.53194 | val_1_rmse: 0.5401  |  0:00:09s
epoch 10 | loss: 0.22891 | val_0_rmse: 0.51869 | val_1_rmse: 0.53076 |  0:00:10s
epoch 11 | loss: 0.2258  | val_0_rmse: 0.50691 | val_1_rmse: 0.52129 |  0:00:11s
epoch 12 | loss: 0.2503  | val_0_rmse: 0.52677 | val_1_rmse: 0.54161 |  0:00:12s
epoch 13 | loss: 0.24756 | val_0_rmse: 0.53894 | val_1_rmse: 0.54696 |  0:00:12s
epoch 14 | loss: 0.23273 | val_0_rmse: 0.51174 | val_1_rmse: 0.52396 |  0:00:13s
epoch 15 | loss: 0.23022 | val_0_rmse: 0.52288 | val_1_rmse: 0.53853 |  0:00:14s
epoch 16 | loss: 0.23027 | val_0_rmse: 0.49085 | val_1_rmse: 0.5078  |  0:00:15s
epoch 17 | loss: 0.21359 | val_0_rmse: 0.4811  | val_1_rmse: 0.50236 |  0:00:16s
epoch 18 | loss: 0.21171 | val_0_rmse: 0.49516 | val_1_rmse: 0.51164 |  0:00:17s
epoch 19 | loss: 0.20808 | val_0_rmse: 0.48836 | val_1_rmse: 0.50483 |  0:00:18s
epoch 20 | loss: 0.20251 | val_0_rmse: 0.46628 | val_1_rmse: 0.49096 |  0:00:19s
epoch 21 | loss: 0.20348 | val_0_rmse: 0.47756 | val_1_rmse: 0.49937 |  0:00:20s
epoch 22 | loss: 0.19779 | val_0_rmse: 0.45616 | val_1_rmse: 0.48582 |  0:00:21s
epoch 23 | loss: 0.1924  | val_0_rmse: 0.4526  | val_1_rmse: 0.479   |  0:00:22s
epoch 24 | loss: 0.19622 | val_0_rmse: 0.45156 | val_1_rmse: 0.47968 |  0:00:22s
epoch 25 | loss: 0.19798 | val_0_rmse: 0.46065 | val_1_rmse: 0.48994 |  0:00:23s
epoch 26 | loss: 0.19214 | val_0_rmse: 0.46572 | val_1_rmse: 0.49899 |  0:00:24s
epoch 27 | loss: 0.19827 | val_0_rmse: 0.44759 | val_1_rmse: 0.47977 |  0:00:25s
epoch 28 | loss: 0.19602 | val_0_rmse: 0.4436  | val_1_rmse: 0.48327 |  0:00:26s
epoch 29 | loss: 0.19801 | val_0_rmse: 0.43575 | val_1_rmse: 0.46818 |  0:00:27s
epoch 30 | loss: 0.19642 | val_0_rmse: 0.44188 | val_1_rmse: 0.4732  |  0:00:28s
epoch 31 | loss: 0.19786 | val_0_rmse: 0.44034 | val_1_rmse: 0.47798 |  0:00:29s
epoch 32 | loss: 0.20219 | val_0_rmse: 0.43294 | val_1_rmse: 0.47131 |  0:00:30s
epoch 33 | loss: 0.19534 | val_0_rmse: 0.43368 | val_1_rmse: 0.47019 |  0:00:31s
epoch 34 | loss: 0.18706 | val_0_rmse: 0.4215  | val_1_rmse: 0.46758 |  0:00:32s
epoch 35 | loss: 0.19164 | val_0_rmse: 0.44411 | val_1_rmse: 0.48143 |  0:00:33s
epoch 36 | loss: 0.19875 | val_0_rmse: 0.42503 | val_1_rmse: 0.47826 |  0:00:34s
epoch 37 | loss: 0.18368 | val_0_rmse: 0.42965 | val_1_rmse: 0.46705 |  0:00:34s
epoch 38 | loss: 0.18508 | val_0_rmse: 0.45458 | val_1_rmse: 0.50052 |  0:00:35s
epoch 39 | loss: 0.18604 | val_0_rmse: 0.41554 | val_1_rmse: 0.46345 |  0:00:36s
epoch 40 | loss: 0.18077 | val_0_rmse: 0.41011 | val_1_rmse: 0.45661 |  0:00:37s
epoch 41 | loss: 0.1781  | val_0_rmse: 0.4103  | val_1_rmse: 0.45149 |  0:00:38s
epoch 42 | loss: 0.18015 | val_0_rmse: 0.4071  | val_1_rmse: 0.45258 |  0:00:39s
epoch 43 | loss: 0.18123 | val_0_rmse: 0.43265 | val_1_rmse: 0.46413 |  0:00:40s
epoch 44 | loss: 0.18443 | val_0_rmse: 0.41775 | val_1_rmse: 0.45644 |  0:00:41s
epoch 45 | loss: 0.18149 | val_0_rmse: 0.46211 | val_1_rmse: 0.48985 |  0:00:42s
epoch 46 | loss: 0.18523 | val_0_rmse: 0.41837 | val_1_rmse: 0.46549 |  0:00:43s
epoch 47 | loss: 0.18154 | val_0_rmse: 0.4069  | val_1_rmse: 0.46584 |  0:00:44s
epoch 48 | loss: 0.18232 | val_0_rmse: 0.41509 | val_1_rmse: 0.45624 |  0:00:45s
epoch 49 | loss: 0.18658 | val_0_rmse: 0.44422 | val_1_rmse: 0.48272 |  0:00:45s
epoch 50 | loss: 0.17877 | val_0_rmse: 0.41667 | val_1_rmse: 0.4664  |  0:00:46s
epoch 51 | loss: 0.17731 | val_0_rmse: 0.42111 | val_1_rmse: 0.46896 |  0:00:47s
epoch 52 | loss: 0.17652 | val_0_rmse: 0.41392 | val_1_rmse: 0.46038 |  0:00:48s
epoch 53 | loss: 0.18057 | val_0_rmse: 0.42077 | val_1_rmse: 0.46746 |  0:00:49s
epoch 54 | loss: 0.18689 | val_0_rmse: 0.43912 | val_1_rmse: 0.47761 |  0:00:50s
epoch 55 | loss: 0.18165 | val_0_rmse: 0.43559 | val_1_rmse: 0.48809 |  0:00:51s
epoch 56 | loss: 0.17728 | val_0_rmse: 0.41459 | val_1_rmse: 0.45441 |  0:00:52s
epoch 57 | loss: 0.17431 | val_0_rmse: 0.40346 | val_1_rmse: 0.45369 |  0:00:53s
epoch 58 | loss: 0.17025 | val_0_rmse: 0.39849 | val_1_rmse: 0.44504 |  0:00:54s
epoch 59 | loss: 0.1691  | val_0_rmse: 0.40576 | val_1_rmse: 0.44866 |  0:00:55s
epoch 60 | loss: 0.17444 | val_0_rmse: 0.39701 | val_1_rmse: 0.44423 |  0:00:55s
epoch 61 | loss: 0.18079 | val_0_rmse: 0.45379 | val_1_rmse: 0.48498 |  0:00:56s
epoch 62 | loss: 0.17851 | val_0_rmse: 0.40855 | val_1_rmse: 0.44629 |  0:00:57s
epoch 63 | loss: 0.17067 | val_0_rmse: 0.39866 | val_1_rmse: 0.44917 |  0:00:58s
epoch 64 | loss: 0.17183 | val_0_rmse: 0.40124 | val_1_rmse: 0.44547 |  0:00:59s
epoch 65 | loss: 0.16935 | val_0_rmse: 0.40682 | val_1_rmse: 0.4457  |  0:01:00s
epoch 66 | loss: 0.17686 | val_0_rmse: 0.40489 | val_1_rmse: 0.44482 |  0:01:01s
epoch 67 | loss: 0.18107 | val_0_rmse: 0.41757 | val_1_rmse: 0.45968 |  0:01:02s
epoch 68 | loss: 0.17382 | val_0_rmse: 0.41349 | val_1_rmse: 0.45414 |  0:01:03s
epoch 69 | loss: 0.17201 | val_0_rmse: 0.40323 | val_1_rmse: 0.44318 |  0:01:04s
epoch 70 | loss: 0.17043 | val_0_rmse: 0.39937 | val_1_rmse: 0.43787 |  0:01:05s
epoch 71 | loss: 0.16799 | val_0_rmse: 0.4029  | val_1_rmse: 0.44673 |  0:01:06s
epoch 72 | loss: 0.16327 | val_0_rmse: 0.39913 | val_1_rmse: 0.44122 |  0:01:06s
epoch 73 | loss: 0.1734  | val_0_rmse: 0.41923 | val_1_rmse: 0.46788 |  0:01:07s
epoch 74 | loss: 0.18262 | val_0_rmse: 0.40608 | val_1_rmse: 0.45483 |  0:01:08s
epoch 75 | loss: 0.17447 | val_0_rmse: 0.40097 | val_1_rmse: 0.44165 |  0:01:09s
epoch 76 | loss: 0.17237 | val_0_rmse: 0.44305 | val_1_rmse: 0.48466 |  0:01:10s
epoch 77 | loss: 0.16948 | val_0_rmse: 0.40303 | val_1_rmse: 0.44482 |  0:01:11s
epoch 78 | loss: 0.17204 | val_0_rmse: 0.39845 | val_1_rmse: 0.43855 |  0:01:12s
epoch 79 | loss: 0.166   | val_0_rmse: 0.39795 | val_1_rmse: 0.44255 |  0:01:13s
epoch 80 | loss: 0.16233 | val_0_rmse: 0.39155 | val_1_rmse: 0.43193 |  0:01:14s
epoch 81 | loss: 0.1659  | val_0_rmse: 0.39171 | val_1_rmse: 0.44128 |  0:01:15s
epoch 82 | loss: 0.16424 | val_0_rmse: 0.3917  | val_1_rmse: 0.43933 |  0:01:16s
epoch 83 | loss: 0.16584 | val_0_rmse: 0.3895  | val_1_rmse: 0.43528 |  0:01:16s
epoch 84 | loss: 0.16728 | val_0_rmse: 0.47658 | val_1_rmse: 0.52481 |  0:01:17s
epoch 85 | loss: 0.16914 | val_0_rmse: 0.38948 | val_1_rmse: 0.43552 |  0:01:18s
epoch 86 | loss: 0.169   | val_0_rmse: 0.39753 | val_1_rmse: 0.44045 |  0:01:19s
epoch 87 | loss: 0.1685  | val_0_rmse: 0.39139 | val_1_rmse: 0.43814 |  0:01:20s
epoch 88 | loss: 0.16476 | val_0_rmse: 0.39273 | val_1_rmse: 0.43914 |  0:01:21s
epoch 89 | loss: 0.16543 | val_0_rmse: 0.40122 | val_1_rmse: 0.44395 |  0:01:22s
epoch 90 | loss: 0.16676 | val_0_rmse: 0.38869 | val_1_rmse: 0.43151 |  0:01:23s
epoch 91 | loss: 0.15949 | val_0_rmse: 0.3937  | val_1_rmse: 0.43435 |  0:01:24s
epoch 92 | loss: 0.16144 | val_0_rmse: 0.39411 | val_1_rmse: 0.43875 |  0:01:25s
epoch 93 | loss: 0.16    | val_0_rmse: 0.38589 | val_1_rmse: 0.43056 |  0:01:26s
epoch 94 | loss: 0.16168 | val_0_rmse: 0.38448 | val_1_rmse: 0.43232 |  0:01:26s
epoch 95 | loss: 0.16635 | val_0_rmse: 0.38416 | val_1_rmse: 0.43034 |  0:01:27s
epoch 96 | loss: 0.16499 | val_0_rmse: 0.38597 | val_1_rmse: 0.43752 |  0:01:28s
epoch 97 | loss: 0.16243 | val_0_rmse: 0.38074 | val_1_rmse: 0.42837 |  0:01:29s
epoch 98 | loss: 0.16076 | val_0_rmse: 0.40941 | val_1_rmse: 0.45966 |  0:01:30s
epoch 99 | loss: 0.16033 | val_0_rmse: 0.3872  | val_1_rmse: 0.43591 |  0:01:31s
epoch 100| loss: 0.16351 | val_0_rmse: 0.39077 | val_1_rmse: 0.43622 |  0:01:32s
epoch 101| loss: 0.16263 | val_0_rmse: 0.39131 | val_1_rmse: 0.44044 |  0:01:33s
epoch 102| loss: 0.15892 | val_0_rmse: 0.38751 | val_1_rmse: 0.43537 |  0:01:34s
epoch 103| loss: 0.15959 | val_0_rmse: 0.38848 | val_1_rmse: 0.43577 |  0:01:35s
epoch 104| loss: 0.16108 | val_0_rmse: 0.383   | val_1_rmse: 0.43342 |  0:01:36s
epoch 105| loss: 0.15888 | val_0_rmse: 0.3931  | val_1_rmse: 0.43602 |  0:01:37s
epoch 106| loss: 0.15979 | val_0_rmse: 0.39263 | val_1_rmse: 0.43878 |  0:01:38s
epoch 107| loss: 0.15792 | val_0_rmse: 0.38452 | val_1_rmse: 0.43609 |  0:01:38s
epoch 108| loss: 0.16088 | val_0_rmse: 0.38455 | val_1_rmse: 0.43535 |  0:01:39s
epoch 109| loss: 0.15833 | val_0_rmse: 0.41204 | val_1_rmse: 0.45333 |  0:01:40s
epoch 110| loss: 0.16104 | val_0_rmse: 0.39312 | val_1_rmse: 0.44684 |  0:01:41s
epoch 111| loss: 0.15896 | val_0_rmse: 0.38705 | val_1_rmse: 0.44492 |  0:01:42s
epoch 112| loss: 0.16036 | val_0_rmse: 0.39656 | val_1_rmse: 0.45063 |  0:01:43s
epoch 113| loss: 0.15883 | val_0_rmse: 0.46358 | val_1_rmse: 0.49924 |  0:01:44s
epoch 114| loss: 0.1684  | val_0_rmse: 0.39922 | val_1_rmse: 0.44461 |  0:01:45s
epoch 115| loss: 0.16256 | val_0_rmse: 0.39684 | val_1_rmse: 0.44671 |  0:01:46s
epoch 116| loss: 0.16946 | val_0_rmse: 0.39932 | val_1_rmse: 0.45162 |  0:01:47s
epoch 117| loss: 0.164   | val_0_rmse: 0.39875 | val_1_rmse: 0.44558 |  0:01:48s
epoch 118| loss: 0.16161 | val_0_rmse: 0.41119 | val_1_rmse: 0.45612 |  0:01:48s
epoch 119| loss: 0.16246 | val_0_rmse: 0.38139 | val_1_rmse: 0.43249 |  0:01:49s
epoch 120| loss: 0.17044 | val_0_rmse: 0.38739 | val_1_rmse: 0.43518 |  0:01:50s
epoch 121| loss: 0.15847 | val_0_rmse: 0.38544 | val_1_rmse: 0.43387 |  0:01:51s
epoch 122| loss: 0.15732 | val_0_rmse: 0.41325 | val_1_rmse: 0.46283 |  0:01:52s
epoch 123| loss: 0.15948 | val_0_rmse: 0.40412 | val_1_rmse: 0.4432  |  0:01:53s
epoch 124| loss: 0.15146 | val_0_rmse: 0.37793 | val_1_rmse: 0.43342 |  0:01:54s
epoch 125| loss: 0.1527  | val_0_rmse: 0.38489 | val_1_rmse: 0.44087 |  0:01:55s
epoch 126| loss: 0.15351 | val_0_rmse: 0.38038 | val_1_rmse: 0.43429 |  0:01:56s
epoch 127| loss: 0.15111 | val_0_rmse: 0.40833 | val_1_rmse: 0.45141 |  0:01:57s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.42837
Best weights from best epoch are automatically used!
ended training at: 21:43:48
Feature importance:
Mean squared error is of 0.15214486133493982
Mean absolute error:0.2665402255176275
MAPE:0.3365433008343403
R2 score:0.6834761537605382
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:43:49
epoch 0  | loss: 1.15428 | val_0_rmse: 0.6786  | val_1_rmse: 0.67359 |  0:00:00s
epoch 1  | loss: 0.36784 | val_0_rmse: 0.61357 | val_1_rmse: 0.60336 |  0:00:01s
epoch 2  | loss: 0.31861 | val_0_rmse: 0.57256 | val_1_rmse: 0.56408 |  0:00:02s
epoch 3  | loss: 0.30709 | val_0_rmse: 0.56853 | val_1_rmse: 0.56445 |  0:00:03s
epoch 4  | loss: 0.28237 | val_0_rmse: 0.5635  | val_1_rmse: 0.54858 |  0:00:04s
epoch 5  | loss: 0.28133 | val_0_rmse: 0.53771 | val_1_rmse: 0.52627 |  0:00:05s
epoch 6  | loss: 0.26575 | val_0_rmse: 0.52772 | val_1_rmse: 0.51217 |  0:00:06s
epoch 7  | loss: 0.26604 | val_0_rmse: 0.52073 | val_1_rmse: 0.50374 |  0:00:07s
epoch 8  | loss: 0.25559 | val_0_rmse: 0.5405  | val_1_rmse: 0.52809 |  0:00:08s
epoch 9  | loss: 0.2568  | val_0_rmse: 0.52934 | val_1_rmse: 0.52579 |  0:00:09s
epoch 10 | loss: 0.24322 | val_0_rmse: 0.50766 | val_1_rmse: 0.49793 |  0:00:10s
epoch 11 | loss: 0.24597 | val_0_rmse: 0.50781 | val_1_rmse: 0.49362 |  0:00:11s
epoch 12 | loss: 0.23882 | val_0_rmse: 0.4975  | val_1_rmse: 0.48348 |  0:00:11s
epoch 13 | loss: 0.23481 | val_0_rmse: 0.49096 | val_1_rmse: 0.48391 |  0:00:12s
epoch 14 | loss: 0.22758 | val_0_rmse: 0.48332 | val_1_rmse: 0.47438 |  0:00:13s
epoch 15 | loss: 0.23495 | val_0_rmse: 0.48999 | val_1_rmse: 0.48041 |  0:00:14s
epoch 16 | loss: 0.23594 | val_0_rmse: 0.4894  | val_1_rmse: 0.47753 |  0:00:15s
epoch 17 | loss: 0.22836 | val_0_rmse: 0.48306 | val_1_rmse: 0.46875 |  0:00:16s
epoch 18 | loss: 0.22481 | val_0_rmse: 0.48784 | val_1_rmse: 0.47777 |  0:00:17s
epoch 19 | loss: 0.22428 | val_0_rmse: 0.4875  | val_1_rmse: 0.48448 |  0:00:18s
epoch 20 | loss: 0.22842 | val_0_rmse: 0.47869 | val_1_rmse: 0.47138 |  0:00:19s
epoch 21 | loss: 0.22285 | val_0_rmse: 0.47302 | val_1_rmse: 0.46662 |  0:00:20s
epoch 22 | loss: 0.21568 | val_0_rmse: 0.47849 | val_1_rmse: 0.46882 |  0:00:21s
epoch 23 | loss: 0.21594 | val_0_rmse: 0.46544 | val_1_rmse: 0.45559 |  0:00:22s
epoch 24 | loss: 0.21704 | val_0_rmse: 0.46915 | val_1_rmse: 0.45883 |  0:00:22s
epoch 25 | loss: 0.21651 | val_0_rmse: 0.46398 | val_1_rmse: 0.45204 |  0:00:23s
epoch 26 | loss: 0.21255 | val_0_rmse: 0.46017 | val_1_rmse: 0.45572 |  0:00:24s
epoch 27 | loss: 0.21399 | val_0_rmse: 0.4553  | val_1_rmse: 0.44515 |  0:00:25s
epoch 28 | loss: 0.21076 | val_0_rmse: 0.46762 | val_1_rmse: 0.45129 |  0:00:26s
epoch 29 | loss: 0.20981 | val_0_rmse: 0.44666 | val_1_rmse: 0.43505 |  0:00:27s
epoch 30 | loss: 0.20507 | val_0_rmse: 0.44278 | val_1_rmse: 0.43333 |  0:00:28s
epoch 31 | loss: 0.19895 | val_0_rmse: 0.46602 | val_1_rmse: 0.45326 |  0:00:29s
epoch 32 | loss: 0.21439 | val_0_rmse: 0.46685 | val_1_rmse: 0.4708  |  0:00:30s
epoch 33 | loss: 0.24025 | val_0_rmse: 0.49168 | val_1_rmse: 0.49172 |  0:00:31s
epoch 34 | loss: 0.23624 | val_0_rmse: 0.48232 | val_1_rmse: 0.47895 |  0:00:32s
epoch 35 | loss: 0.21271 | val_0_rmse: 0.45448 | val_1_rmse: 0.44901 |  0:00:32s
epoch 36 | loss: 0.21515 | val_0_rmse: 0.45033 | val_1_rmse: 0.44792 |  0:00:33s
epoch 37 | loss: 0.21213 | val_0_rmse: 0.44468 | val_1_rmse: 0.44942 |  0:00:34s
epoch 38 | loss: 0.20564 | val_0_rmse: 0.4492  | val_1_rmse: 0.44516 |  0:00:35s
epoch 39 | loss: 0.20685 | val_0_rmse: 0.45525 | val_1_rmse: 0.43797 |  0:00:36s
epoch 40 | loss: 0.21335 | val_0_rmse: 0.46455 | val_1_rmse: 0.46215 |  0:00:37s
epoch 41 | loss: 0.22367 | val_0_rmse: 0.49699 | val_1_rmse: 0.47722 |  0:00:38s
epoch 42 | loss: 0.23647 | val_0_rmse: 0.48099 | val_1_rmse: 0.4713  |  0:00:39s
epoch 43 | loss: 0.22678 | val_0_rmse: 0.46487 | val_1_rmse: 0.44797 |  0:00:40s
epoch 44 | loss: 0.21652 | val_0_rmse: 0.49596 | val_1_rmse: 0.48886 |  0:00:41s
epoch 45 | loss: 0.25981 | val_0_rmse: 0.51235 | val_1_rmse: 0.4986  |  0:00:42s
epoch 46 | loss: 0.25572 | val_0_rmse: 0.49985 | val_1_rmse: 0.49506 |  0:00:43s
epoch 47 | loss: 0.25448 | val_0_rmse: 0.49925 | val_1_rmse: 0.49482 |  0:00:43s
epoch 48 | loss: 0.2474  | val_0_rmse: 0.49031 | val_1_rmse: 0.49133 |  0:00:44s
epoch 49 | loss: 0.23696 | val_0_rmse: 0.48163 | val_1_rmse: 0.48059 |  0:00:45s
epoch 50 | loss: 0.23453 | val_0_rmse: 0.48436 | val_1_rmse: 0.48933 |  0:00:46s
epoch 51 | loss: 0.23114 | val_0_rmse: 0.47334 | val_1_rmse: 0.46858 |  0:00:47s
epoch 52 | loss: 0.23177 | val_0_rmse: 0.46996 | val_1_rmse: 0.46806 |  0:00:48s
epoch 53 | loss: 0.22593 | val_0_rmse: 0.47524 | val_1_rmse: 0.48468 |  0:00:49s
epoch 54 | loss: 0.22059 | val_0_rmse: 0.47338 | val_1_rmse: 0.45367 |  0:00:50s
epoch 55 | loss: 0.21631 | val_0_rmse: 0.46261 | val_1_rmse: 0.44698 |  0:00:51s
epoch 56 | loss: 0.21292 | val_0_rmse: 0.45248 | val_1_rmse: 0.43987 |  0:00:52s
epoch 57 | loss: 0.21044 | val_0_rmse: 0.44952 | val_1_rmse: 0.45599 |  0:00:53s
epoch 58 | loss: 0.21479 | val_0_rmse: 0.45422 | val_1_rmse: 0.46662 |  0:00:53s
epoch 59 | loss: 0.20505 | val_0_rmse: 0.46011 | val_1_rmse: 0.47274 |  0:00:54s
epoch 60 | loss: 0.20295 | val_0_rmse: 0.45366 | val_1_rmse: 0.4413  |  0:00:55s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.43333
Best weights from best epoch are automatically used!
ended training at: 21:44:45
Feature importance:
Mean squared error is of 0.21440200826296377
Mean absolute error:0.305984713755934
MAPE:0.38321796919034157
R2 score:0.5963536279281151
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:44:45
epoch 0  | loss: 1.11675 | val_0_rmse: 0.67493 | val_1_rmse: 0.67065 |  0:00:00s
epoch 1  | loss: 0.39027 | val_0_rmse: 0.6679  | val_1_rmse: 0.66522 |  0:00:01s
epoch 2  | loss: 0.36376 | val_0_rmse: 0.61341 | val_1_rmse: 0.6095  |  0:00:02s
epoch 3  | loss: 0.3392  | val_0_rmse: 0.59322 | val_1_rmse: 0.5891  |  0:00:03s
epoch 4  | loss: 0.32689 | val_0_rmse: 0.58315 | val_1_rmse: 0.57713 |  0:00:04s
epoch 5  | loss: 0.30658 | val_0_rmse: 0.58115 | val_1_rmse: 0.57885 |  0:00:05s
epoch 6  | loss: 0.28861 | val_0_rmse: 0.54636 | val_1_rmse: 0.54333 |  0:00:06s
epoch 7  | loss: 0.27729 | val_0_rmse: 0.53705 | val_1_rmse: 0.53444 |  0:00:07s
epoch 8  | loss: 0.25769 | val_0_rmse: 0.52427 | val_1_rmse: 0.5314  |  0:00:08s
epoch 9  | loss: 0.26302 | val_0_rmse: 0.53154 | val_1_rmse: 0.5193  |  0:00:09s
epoch 10 | loss: 0.24272 | val_0_rmse: 0.50311 | val_1_rmse: 0.48953 |  0:00:10s
epoch 11 | loss: 0.2406  | val_0_rmse: 0.53687 | val_1_rmse: 0.53079 |  0:00:11s
epoch 12 | loss: 0.23938 | val_0_rmse: 0.56943 | val_1_rmse: 0.55089 |  0:00:12s
epoch 13 | loss: 0.23452 | val_0_rmse: 0.52769 | val_1_rmse: 0.52027 |  0:00:12s
epoch 14 | loss: 0.23086 | val_0_rmse: 0.49499 | val_1_rmse: 0.4818  |  0:00:13s
epoch 15 | loss: 0.22369 | val_0_rmse: 0.49556 | val_1_rmse: 0.48398 |  0:00:14s
epoch 16 | loss: 0.22276 | val_0_rmse: 0.50869 | val_1_rmse: 0.49766 |  0:00:15s
epoch 17 | loss: 0.2278  | val_0_rmse: 0.48786 | val_1_rmse: 0.48016 |  0:00:16s
epoch 18 | loss: 0.22274 | val_0_rmse: 0.50616 | val_1_rmse: 0.49944 |  0:00:17s
epoch 19 | loss: 0.22333 | val_0_rmse: 0.50152 | val_1_rmse: 0.4912  |  0:00:18s
epoch 20 | loss: 0.2116  | val_0_rmse: 0.48387 | val_1_rmse: 0.48055 |  0:00:19s
epoch 21 | loss: 0.20795 | val_0_rmse: 0.4704  | val_1_rmse: 0.46392 |  0:00:20s
epoch 22 | loss: 0.20754 | val_0_rmse: 0.47221 | val_1_rmse: 0.4669  |  0:00:21s
epoch 23 | loss: 0.20947 | val_0_rmse: 0.46459 | val_1_rmse: 0.45199 |  0:00:22s
epoch 24 | loss: 0.2102  | val_0_rmse: 0.46544 | val_1_rmse: 0.46033 |  0:00:23s
epoch 25 | loss: 0.21656 | val_0_rmse: 0.45184 | val_1_rmse: 0.44746 |  0:00:23s
epoch 26 | loss: 0.20722 | val_0_rmse: 0.48583 | val_1_rmse: 0.47677 |  0:00:24s
epoch 27 | loss: 0.21089 | val_0_rmse: 0.45169 | val_1_rmse: 0.44729 |  0:00:25s
epoch 28 | loss: 0.20465 | val_0_rmse: 0.44654 | val_1_rmse: 0.44724 |  0:00:26s
epoch 29 | loss: 0.20121 | val_0_rmse: 0.45535 | val_1_rmse: 0.45605 |  0:00:27s
epoch 30 | loss: 0.19684 | val_0_rmse: 0.44108 | val_1_rmse: 0.4413  |  0:00:28s
epoch 31 | loss: 0.19906 | val_0_rmse: 0.45228 | val_1_rmse: 0.45604 |  0:00:29s
epoch 32 | loss: 0.20531 | val_0_rmse: 0.45314 | val_1_rmse: 0.45286 |  0:00:30s
epoch 33 | loss: 0.19657 | val_0_rmse: 0.45657 | val_1_rmse: 0.45193 |  0:00:31s
epoch 34 | loss: 0.19523 | val_0_rmse: 0.43829 | val_1_rmse: 0.44465 |  0:00:32s
epoch 35 | loss: 0.20184 | val_0_rmse: 0.45158 | val_1_rmse: 0.45619 |  0:00:33s
epoch 36 | loss: 0.20506 | val_0_rmse: 0.44099 | val_1_rmse: 0.44201 |  0:00:33s
epoch 37 | loss: 0.20375 | val_0_rmse: 0.4334  | val_1_rmse: 0.43643 |  0:00:34s
epoch 38 | loss: 0.1981  | val_0_rmse: 0.45706 | val_1_rmse: 0.46609 |  0:00:35s
epoch 39 | loss: 0.19796 | val_0_rmse: 0.45698 | val_1_rmse: 0.45551 |  0:00:36s
epoch 40 | loss: 0.19385 | val_0_rmse: 0.43534 | val_1_rmse: 0.44378 |  0:00:37s
epoch 41 | loss: 0.19532 | val_0_rmse: 0.43786 | val_1_rmse: 0.43998 |  0:00:38s
epoch 42 | loss: 0.1982  | val_0_rmse: 0.44896 | val_1_rmse: 0.44726 |  0:00:39s
epoch 43 | loss: 0.19362 | val_0_rmse: 0.43621 | val_1_rmse: 0.44057 |  0:00:40s
epoch 44 | loss: 0.19067 | val_0_rmse: 0.42272 | val_1_rmse: 0.42301 |  0:00:41s
epoch 45 | loss: 0.18482 | val_0_rmse: 0.44054 | val_1_rmse: 0.45163 |  0:00:42s
epoch 46 | loss: 0.19031 | val_0_rmse: 0.42832 | val_1_rmse: 0.42575 |  0:00:43s
epoch 47 | loss: 0.1879  | val_0_rmse: 0.42247 | val_1_rmse: 0.42004 |  0:00:44s
epoch 48 | loss: 0.19018 | val_0_rmse: 0.4149  | val_1_rmse: 0.42776 |  0:00:45s
epoch 49 | loss: 0.18203 | val_0_rmse: 0.41469 | val_1_rmse: 0.41006 |  0:00:45s
epoch 50 | loss: 0.18125 | val_0_rmse: 0.42298 | val_1_rmse: 0.42415 |  0:00:46s
epoch 51 | loss: 0.18348 | val_0_rmse: 0.41695 | val_1_rmse: 0.41546 |  0:00:47s
epoch 52 | loss: 0.17942 | val_0_rmse: 0.43209 | val_1_rmse: 0.43856 |  0:00:48s
epoch 53 | loss: 0.18069 | val_0_rmse: 0.41227 | val_1_rmse: 0.4192  |  0:00:49s
epoch 54 | loss: 0.17682 | val_0_rmse: 0.41028 | val_1_rmse: 0.41579 |  0:00:50s
epoch 55 | loss: 0.17596 | val_0_rmse: 0.42779 | val_1_rmse: 0.43712 |  0:00:51s
epoch 56 | loss: 0.18188 | val_0_rmse: 0.4116  | val_1_rmse: 0.41176 |  0:00:52s
epoch 57 | loss: 0.18651 | val_0_rmse: 0.42035 | val_1_rmse: 0.41694 |  0:00:53s
epoch 58 | loss: 0.19074 | val_0_rmse: 0.42445 | val_1_rmse: 0.43301 |  0:00:54s
epoch 59 | loss: 0.17934 | val_0_rmse: 0.413   | val_1_rmse: 0.4201  |  0:00:55s
epoch 60 | loss: 0.17438 | val_0_rmse: 0.40976 | val_1_rmse: 0.42394 |  0:00:55s
epoch 61 | loss: 0.1754  | val_0_rmse: 0.40985 | val_1_rmse: 0.41523 |  0:00:56s
epoch 62 | loss: 0.17694 | val_0_rmse: 0.41717 | val_1_rmse: 0.42041 |  0:00:57s
epoch 63 | loss: 0.18087 | val_0_rmse: 0.4152  | val_1_rmse: 0.41577 |  0:00:58s
epoch 64 | loss: 0.17728 | val_0_rmse: 0.42667 | val_1_rmse: 0.43679 |  0:00:59s
epoch 65 | loss: 0.17523 | val_0_rmse: 0.41858 | val_1_rmse: 0.42802 |  0:01:00s
epoch 66 | loss: 0.17723 | val_0_rmse: 0.41584 | val_1_rmse: 0.42189 |  0:01:01s
epoch 67 | loss: 0.17939 | val_0_rmse: 0.40923 | val_1_rmse: 0.41036 |  0:01:02s
epoch 68 | loss: 0.17912 | val_0_rmse: 0.40905 | val_1_rmse: 0.42205 |  0:01:03s
epoch 69 | loss: 0.17581 | val_0_rmse: 0.40937 | val_1_rmse: 0.41142 |  0:01:04s
epoch 70 | loss: 0.17286 | val_0_rmse: 0.4151  | val_1_rmse: 0.41983 |  0:01:05s
epoch 71 | loss: 0.18338 | val_0_rmse: 0.47765 | val_1_rmse: 0.4868  |  0:01:06s
epoch 72 | loss: 0.18476 | val_0_rmse: 0.43962 | val_1_rmse: 0.4458  |  0:01:06s
epoch 73 | loss: 0.19142 | val_0_rmse: 0.47116 | val_1_rmse: 0.46508 |  0:01:07s
epoch 74 | loss: 0.18392 | val_0_rmse: 0.41987 | val_1_rmse: 0.4262  |  0:01:08s
epoch 75 | loss: 0.18086 | val_0_rmse: 0.42718 | val_1_rmse: 0.43585 |  0:01:09s
epoch 76 | loss: 0.19133 | val_0_rmse: 0.42276 | val_1_rmse: 0.43439 |  0:01:10s
epoch 77 | loss: 0.17831 | val_0_rmse: 0.41399 | val_1_rmse: 0.41941 |  0:01:11s
epoch 78 | loss: 0.18165 | val_0_rmse: 0.4298  | val_1_rmse: 0.42887 |  0:01:12s
epoch 79 | loss: 0.18222 | val_0_rmse: 0.41765 | val_1_rmse: 0.42    |  0:01:13s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.41006
Best weights from best epoch are automatically used!
ended training at: 21:45:59
Feature importance:
Mean squared error is of 0.18412613232848482
Mean absolute error:0.29049389202532105
MAPE:0.37223108235824476
R2 score:0.6436359647684885
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:46:00
epoch 0  | loss: 1.23522 | val_0_rmse: 0.70303 | val_1_rmse: 0.69957 |  0:00:00s
epoch 1  | loss: 0.39005 | val_0_rmse: 0.60765 | val_1_rmse: 0.60492 |  0:00:01s
epoch 2  | loss: 0.31701 | val_0_rmse: 0.56196 | val_1_rmse: 0.55683 |  0:00:02s
epoch 3  | loss: 0.30017 | val_0_rmse: 0.55323 | val_1_rmse: 0.55335 |  0:00:03s
epoch 4  | loss: 0.28207 | val_0_rmse: 0.54505 | val_1_rmse: 0.54766 |  0:00:04s
epoch 5  | loss: 0.27642 | val_0_rmse: 0.54783 | val_1_rmse: 0.54592 |  0:00:05s
epoch 6  | loss: 0.26324 | val_0_rmse: 0.5388  | val_1_rmse: 0.53519 |  0:00:06s
epoch 7  | loss: 0.26849 | val_0_rmse: 0.53998 | val_1_rmse: 0.53732 |  0:00:07s
epoch 8  | loss: 0.25896 | val_0_rmse: 0.52717 | val_1_rmse: 0.52445 |  0:00:08s
epoch 9  | loss: 0.24355 | val_0_rmse: 0.52805 | val_1_rmse: 0.52773 |  0:00:09s
epoch 10 | loss: 0.25384 | val_0_rmse: 0.54451 | val_1_rmse: 0.5415  |  0:00:10s
epoch 11 | loss: 0.26566 | val_0_rmse: 0.52383 | val_1_rmse: 0.51855 |  0:00:10s
epoch 12 | loss: 0.24633 | val_0_rmse: 0.50238 | val_1_rmse: 0.50126 |  0:00:11s
epoch 13 | loss: 0.23436 | val_0_rmse: 0.50539 | val_1_rmse: 0.50689 |  0:00:12s
epoch 14 | loss: 0.23251 | val_0_rmse: 0.51767 | val_1_rmse: 0.52139 |  0:00:13s
epoch 15 | loss: 0.22719 | val_0_rmse: 0.49527 | val_1_rmse: 0.50186 |  0:00:14s
epoch 16 | loss: 0.22729 | val_0_rmse: 0.4862  | val_1_rmse: 0.494   |  0:00:15s
epoch 17 | loss: 0.22981 | val_0_rmse: 0.50565 | val_1_rmse: 0.51502 |  0:00:16s
epoch 18 | loss: 0.23498 | val_0_rmse: 0.52816 | val_1_rmse: 0.53643 |  0:00:17s
epoch 19 | loss: 0.24466 | val_0_rmse: 0.51488 | val_1_rmse: 0.52    |  0:00:18s
epoch 20 | loss: 0.23189 | val_0_rmse: 0.48536 | val_1_rmse: 0.49741 |  0:00:19s
epoch 21 | loss: 0.22924 | val_0_rmse: 0.48066 | val_1_rmse: 0.49005 |  0:00:20s
epoch 22 | loss: 0.21785 | val_0_rmse: 0.47358 | val_1_rmse: 0.48251 |  0:00:21s
epoch 23 | loss: 0.21492 | val_0_rmse: 0.49109 | val_1_rmse: 0.50129 |  0:00:22s
epoch 24 | loss: 0.21243 | val_0_rmse: 0.47853 | val_1_rmse: 0.49041 |  0:00:22s
epoch 25 | loss: 0.20934 | val_0_rmse: 0.46438 | val_1_rmse: 0.48374 |  0:00:23s
epoch 26 | loss: 0.20631 | val_0_rmse: 0.47686 | val_1_rmse: 0.48896 |  0:00:24s
epoch 27 | loss: 0.20547 | val_0_rmse: 0.47967 | val_1_rmse: 0.49257 |  0:00:25s
epoch 28 | loss: 0.20424 | val_0_rmse: 0.45471 | val_1_rmse: 0.47055 |  0:00:26s
epoch 29 | loss: 0.20842 | val_0_rmse: 0.44837 | val_1_rmse: 0.46857 |  0:00:27s
epoch 30 | loss: 0.20359 | val_0_rmse: 0.44811 | val_1_rmse: 0.46435 |  0:00:28s
epoch 31 | loss: 0.20884 | val_0_rmse: 0.46129 | val_1_rmse: 0.47602 |  0:00:29s
epoch 32 | loss: 0.20575 | val_0_rmse: 0.44465 | val_1_rmse: 0.45926 |  0:00:30s
epoch 33 | loss: 0.19967 | val_0_rmse: 0.46684 | val_1_rmse: 0.47484 |  0:00:31s
epoch 34 | loss: 0.2018  | val_0_rmse: 0.44399 | val_1_rmse: 0.46388 |  0:00:32s
epoch 35 | loss: 0.19964 | val_0_rmse: 0.43548 | val_1_rmse: 0.45676 |  0:00:32s
epoch 36 | loss: 0.20036 | val_0_rmse: 0.42881 | val_1_rmse: 0.45214 |  0:00:33s
epoch 37 | loss: 0.19305 | val_0_rmse: 0.45998 | val_1_rmse: 0.48042 |  0:00:34s
epoch 38 | loss: 0.19723 | val_0_rmse: 0.44488 | val_1_rmse: 0.47    |  0:00:35s
epoch 39 | loss: 0.21732 | val_0_rmse: 0.46428 | val_1_rmse: 0.46468 |  0:00:36s
epoch 40 | loss: 0.2146  | val_0_rmse: 0.46262 | val_1_rmse: 0.47939 |  0:00:37s
epoch 41 | loss: 0.20189 | val_0_rmse: 0.43464 | val_1_rmse: 0.45184 |  0:00:38s
epoch 42 | loss: 0.1962  | val_0_rmse: 0.44408 | val_1_rmse: 0.46602 |  0:00:39s
epoch 43 | loss: 0.20218 | val_0_rmse: 0.44872 | val_1_rmse: 0.46779 |  0:00:40s
epoch 44 | loss: 0.19928 | val_0_rmse: 0.44357 | val_1_rmse: 0.46973 |  0:00:41s
epoch 45 | loss: 0.20752 | val_0_rmse: 0.46511 | val_1_rmse: 0.48627 |  0:00:42s
epoch 46 | loss: 0.20464 | val_0_rmse: 0.43815 | val_1_rmse: 0.46028 |  0:00:42s
epoch 47 | loss: 0.20172 | val_0_rmse: 0.43642 | val_1_rmse: 0.45979 |  0:00:43s
epoch 48 | loss: 0.19387 | val_0_rmse: 0.4396  | val_1_rmse: 0.44929 |  0:00:44s
epoch 49 | loss: 0.19114 | val_0_rmse: 0.42242 | val_1_rmse: 0.43903 |  0:00:45s
epoch 50 | loss: 0.18732 | val_0_rmse: 0.41921 | val_1_rmse: 0.44288 |  0:00:46s
epoch 51 | loss: 0.19261 | val_0_rmse: 0.41808 | val_1_rmse: 0.43909 |  0:00:47s
epoch 52 | loss: 0.18671 | val_0_rmse: 0.42629 | val_1_rmse: 0.45552 |  0:00:48s
epoch 53 | loss: 0.19151 | val_0_rmse: 0.4287  | val_1_rmse: 0.4504  |  0:00:49s
epoch 54 | loss: 0.1911  | val_0_rmse: 0.42006 | val_1_rmse: 0.44879 |  0:00:50s
epoch 55 | loss: 0.18434 | val_0_rmse: 0.42403 | val_1_rmse: 0.45221 |  0:00:51s
epoch 56 | loss: 0.18855 | val_0_rmse: 0.41339 | val_1_rmse: 0.44364 |  0:00:52s
epoch 57 | loss: 0.18338 | val_0_rmse: 0.41548 | val_1_rmse: 0.44096 |  0:00:52s
epoch 58 | loss: 0.18687 | val_0_rmse: 0.43536 | val_1_rmse: 0.45766 |  0:00:53s
epoch 59 | loss: 0.18399 | val_0_rmse: 0.41797 | val_1_rmse: 0.45146 |  0:00:54s
epoch 60 | loss: 0.19547 | val_0_rmse: 0.43099 | val_1_rmse: 0.44305 |  0:00:55s
epoch 61 | loss: 0.19535 | val_0_rmse: 0.42314 | val_1_rmse: 0.44257 |  0:00:56s
epoch 62 | loss: 0.18618 | val_0_rmse: 0.43135 | val_1_rmse: 0.45403 |  0:00:57s
epoch 63 | loss: 0.18686 | val_0_rmse: 0.4433  | val_1_rmse: 0.46789 |  0:00:58s
epoch 64 | loss: 0.18517 | val_0_rmse: 0.41588 | val_1_rmse: 0.44341 |  0:00:59s
epoch 65 | loss: 0.17731 | val_0_rmse: 0.43899 | val_1_rmse: 0.47152 |  0:01:00s
epoch 66 | loss: 0.17892 | val_0_rmse: 0.42073 | val_1_rmse: 0.44678 |  0:01:01s
epoch 67 | loss: 0.17892 | val_0_rmse: 0.41014 | val_1_rmse: 0.43271 |  0:01:02s
epoch 68 | loss: 0.17526 | val_0_rmse: 0.40691 | val_1_rmse: 0.43629 |  0:01:03s
epoch 69 | loss: 0.17715 | val_0_rmse: 0.41419 | val_1_rmse: 0.44508 |  0:01:04s
epoch 70 | loss: 0.17713 | val_0_rmse: 0.41152 | val_1_rmse: 0.44517 |  0:01:04s
epoch 71 | loss: 0.17719 | val_0_rmse: 0.4117  | val_1_rmse: 0.43468 |  0:01:05s
epoch 72 | loss: 0.1817  | val_0_rmse: 0.41967 | val_1_rmse: 0.44899 |  0:01:06s
epoch 73 | loss: 0.18074 | val_0_rmse: 0.41392 | val_1_rmse: 0.45748 |  0:01:07s
epoch 74 | loss: 0.17556 | val_0_rmse: 0.41124 | val_1_rmse: 0.44028 |  0:01:08s
epoch 75 | loss: 0.17355 | val_0_rmse: 0.4073  | val_1_rmse: 0.43622 |  0:01:09s
epoch 76 | loss: 0.17262 | val_0_rmse: 0.40135 | val_1_rmse: 0.43781 |  0:01:10s
epoch 77 | loss: 0.17293 | val_0_rmse: 0.41444 | val_1_rmse: 0.44533 |  0:01:11s
epoch 78 | loss: 0.17096 | val_0_rmse: 0.41195 | val_1_rmse: 0.44689 |  0:01:12s
epoch 79 | loss: 0.16968 | val_0_rmse: 0.4064  | val_1_rmse: 0.43482 |  0:01:13s
epoch 80 | loss: 0.1714  | val_0_rmse: 0.41013 | val_1_rmse: 0.439   |  0:01:14s
epoch 81 | loss: 0.17615 | val_0_rmse: 0.40884 | val_1_rmse: 0.43932 |  0:01:14s
epoch 82 | loss: 0.17681 | val_0_rmse: 0.42743 | val_1_rmse: 0.46627 |  0:01:15s
epoch 83 | loss: 0.18102 | val_0_rmse: 0.42619 | val_1_rmse: 0.46247 |  0:01:16s
epoch 84 | loss: 0.17754 | val_0_rmse: 0.40544 | val_1_rmse: 0.44937 |  0:01:17s
epoch 85 | loss: 0.17759 | val_0_rmse: 0.41142 | val_1_rmse: 0.44255 |  0:01:18s
epoch 86 | loss: 0.18372 | val_0_rmse: 0.40313 | val_1_rmse: 0.44011 |  0:01:19s
epoch 87 | loss: 0.17093 | val_0_rmse: 0.4055  | val_1_rmse: 0.449   |  0:01:20s
epoch 88 | loss: 0.16855 | val_0_rmse: 0.39943 | val_1_rmse: 0.43437 |  0:01:21s
epoch 89 | loss: 0.16655 | val_0_rmse: 0.40776 | val_1_rmse: 0.44281 |  0:01:22s
epoch 90 | loss: 0.17555 | val_0_rmse: 0.41342 | val_1_rmse: 0.44163 |  0:01:23s
epoch 91 | loss: 0.17905 | val_0_rmse: 0.40721 | val_1_rmse: 0.44137 |  0:01:24s
epoch 92 | loss: 0.17669 | val_0_rmse: 0.42862 | val_1_rmse: 0.46066 |  0:01:25s
epoch 93 | loss: 0.19091 | val_0_rmse: 0.43314 | val_1_rmse: 0.45777 |  0:01:25s
epoch 94 | loss: 0.17924 | val_0_rmse: 0.41493 | val_1_rmse: 0.45561 |  0:01:26s
epoch 95 | loss: 0.17734 | val_0_rmse: 0.40571 | val_1_rmse: 0.44    |  0:01:27s
epoch 96 | loss: 0.17144 | val_0_rmse: 0.42393 | val_1_rmse: 0.46955 |  0:01:28s
epoch 97 | loss: 0.172   | val_0_rmse: 0.41846 | val_1_rmse: 0.4568  |  0:01:29s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.43271
Best weights from best epoch are automatically used!
ended training at: 21:47:29
Feature importance:
Mean squared error is of 0.18848228258729546
Mean absolute error:0.29685222752737206
MAPE:0.3822028188595217
R2 score:0.6293023711367569
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:47:30
epoch 0  | loss: 1.08429 | val_0_rmse: 0.61111 | val_1_rmse: 0.60716 |  0:00:00s
epoch 1  | loss: 0.40374 | val_0_rmse: 0.59389 | val_1_rmse: 0.59267 |  0:00:01s
epoch 2  | loss: 0.34396 | val_0_rmse: 0.59232 | val_1_rmse: 0.58899 |  0:00:02s
epoch 3  | loss: 0.30383 | val_0_rmse: 0.56642 | val_1_rmse: 0.57077 |  0:00:03s
epoch 4  | loss: 0.29464 | val_0_rmse: 0.55803 | val_1_rmse: 0.56072 |  0:00:04s
epoch 5  | loss: 0.27485 | val_0_rmse: 0.5382  | val_1_rmse: 0.53797 |  0:00:05s
epoch 6  | loss: 0.2631  | val_0_rmse: 0.54078 | val_1_rmse: 0.54433 |  0:00:06s
epoch 7  | loss: 0.25568 | val_0_rmse: 0.52358 | val_1_rmse: 0.52464 |  0:00:07s
epoch 8  | loss: 0.24253 | val_0_rmse: 0.51838 | val_1_rmse: 0.51836 |  0:00:08s
epoch 9  | loss: 0.24451 | val_0_rmse: 0.50541 | val_1_rmse: 0.50927 |  0:00:09s
epoch 10 | loss: 0.23687 | val_0_rmse: 0.50606 | val_1_rmse: 0.50801 |  0:00:10s
epoch 11 | loss: 0.23094 | val_0_rmse: 0.50674 | val_1_rmse: 0.50814 |  0:00:10s
epoch 12 | loss: 0.22168 | val_0_rmse: 0.49677 | val_1_rmse: 0.49786 |  0:00:11s
epoch 13 | loss: 0.21519 | val_0_rmse: 0.48175 | val_1_rmse: 0.49135 |  0:00:12s
epoch 14 | loss: 0.21574 | val_0_rmse: 0.48096 | val_1_rmse: 0.49445 |  0:00:13s
epoch 15 | loss: 0.2139  | val_0_rmse: 0.52531 | val_1_rmse: 0.52826 |  0:00:14s
epoch 16 | loss: 0.2163  | val_0_rmse: 0.52621 | val_1_rmse: 0.52846 |  0:00:15s
epoch 17 | loss: 0.21947 | val_0_rmse: 0.47309 | val_1_rmse: 0.49033 |  0:00:16s
epoch 18 | loss: 0.21348 | val_0_rmse: 0.47503 | val_1_rmse: 0.48554 |  0:00:17s
epoch 19 | loss: 0.21188 | val_0_rmse: 0.46432 | val_1_rmse: 0.47341 |  0:00:18s
epoch 20 | loss: 0.20966 | val_0_rmse: 0.4682  | val_1_rmse: 0.47827 |  0:00:19s
epoch 21 | loss: 0.21047 | val_0_rmse: 0.46945 | val_1_rmse: 0.48354 |  0:00:20s
epoch 22 | loss: 0.23087 | val_0_rmse: 0.50837 | val_1_rmse: 0.50957 |  0:00:20s
epoch 23 | loss: 0.22877 | val_0_rmse: 0.47903 | val_1_rmse: 0.48404 |  0:00:21s
epoch 24 | loss: 0.21092 | val_0_rmse: 0.45846 | val_1_rmse: 0.47336 |  0:00:22s
epoch 25 | loss: 0.21281 | val_0_rmse: 0.44946 | val_1_rmse: 0.47334 |  0:00:23s
epoch 26 | loss: 0.20529 | val_0_rmse: 0.44679 | val_1_rmse: 0.46924 |  0:00:24s
epoch 27 | loss: 0.2059  | val_0_rmse: 0.45574 | val_1_rmse: 0.47314 |  0:00:25s
epoch 28 | loss: 0.20303 | val_0_rmse: 0.45128 | val_1_rmse: 0.47433 |  0:00:26s
epoch 29 | loss: 0.20157 | val_0_rmse: 0.44545 | val_1_rmse: 0.46066 |  0:00:27s
epoch 30 | loss: 0.19874 | val_0_rmse: 0.43129 | val_1_rmse: 0.4567  |  0:00:28s
epoch 31 | loss: 0.19852 | val_0_rmse: 0.44826 | val_1_rmse: 0.46508 |  0:00:29s
epoch 32 | loss: 0.20041 | val_0_rmse: 0.4434  | val_1_rmse: 0.46711 |  0:00:30s
epoch 33 | loss: 0.20029 | val_0_rmse: 0.45558 | val_1_rmse: 0.47024 |  0:00:31s
epoch 34 | loss: 0.19843 | val_0_rmse: 0.43221 | val_1_rmse: 0.45837 |  0:00:31s
epoch 35 | loss: 0.19158 | val_0_rmse: 0.42904 | val_1_rmse: 0.456   |  0:00:32s
epoch 36 | loss: 0.18884 | val_0_rmse: 0.42634 | val_1_rmse: 0.44565 |  0:00:33s
epoch 37 | loss: 0.1841  | val_0_rmse: 0.41476 | val_1_rmse: 0.44083 |  0:00:34s
epoch 38 | loss: 0.18261 | val_0_rmse: 0.42243 | val_1_rmse: 0.45166 |  0:00:35s
epoch 39 | loss: 0.18501 | val_0_rmse: 0.41728 | val_1_rmse: 0.4432  |  0:00:36s
epoch 40 | loss: 0.18061 | val_0_rmse: 0.40933 | val_1_rmse: 0.43436 |  0:00:37s
epoch 41 | loss: 0.17788 | val_0_rmse: 0.41348 | val_1_rmse: 0.43723 |  0:00:38s
epoch 42 | loss: 0.18152 | val_0_rmse: 0.4301  | val_1_rmse: 0.44619 |  0:00:39s
epoch 43 | loss: 0.18165 | val_0_rmse: 0.41024 | val_1_rmse: 0.4322  |  0:00:40s
epoch 44 | loss: 0.17789 | val_0_rmse: 0.41339 | val_1_rmse: 0.43652 |  0:00:41s
epoch 45 | loss: 0.17624 | val_0_rmse: 0.40735 | val_1_rmse: 0.43456 |  0:00:41s
epoch 46 | loss: 0.17897 | val_0_rmse: 0.41555 | val_1_rmse: 0.43584 |  0:00:42s
epoch 47 | loss: 0.1782  | val_0_rmse: 0.41158 | val_1_rmse: 0.43471 |  0:00:43s
epoch 48 | loss: 0.17498 | val_0_rmse: 0.41145 | val_1_rmse: 0.43669 |  0:00:44s
epoch 49 | loss: 0.17772 | val_0_rmse: 0.42256 | val_1_rmse: 0.4546  |  0:00:45s
epoch 50 | loss: 0.18761 | val_0_rmse: 0.42634 | val_1_rmse: 0.44434 |  0:00:46s
epoch 51 | loss: 0.1884  | val_0_rmse: 0.41018 | val_1_rmse: 0.43839 |  0:00:47s
epoch 52 | loss: 0.17786 | val_0_rmse: 0.40442 | val_1_rmse: 0.42983 |  0:00:48s
epoch 53 | loss: 0.17612 | val_0_rmse: 0.40265 | val_1_rmse: 0.42834 |  0:00:49s
epoch 54 | loss: 0.17514 | val_0_rmse: 0.40627 | val_1_rmse: 0.43368 |  0:00:50s
epoch 55 | loss: 0.17456 | val_0_rmse: 0.4061  | val_1_rmse: 0.43039 |  0:00:51s
epoch 56 | loss: 0.17076 | val_0_rmse: 0.40499 | val_1_rmse: 0.43826 |  0:00:51s
epoch 57 | loss: 0.17299 | val_0_rmse: 0.4054  | val_1_rmse: 0.43096 |  0:00:52s
epoch 58 | loss: 0.16983 | val_0_rmse: 0.39812 | val_1_rmse: 0.42637 |  0:00:53s
epoch 59 | loss: 0.17357 | val_0_rmse: 0.39876 | val_1_rmse: 0.43069 |  0:00:54s
epoch 60 | loss: 0.16861 | val_0_rmse: 0.40254 | val_1_rmse: 0.43229 |  0:00:55s
epoch 61 | loss: 0.1693  | val_0_rmse: 0.401   | val_1_rmse: 0.43115 |  0:00:56s
epoch 62 | loss: 0.16848 | val_0_rmse: 0.39616 | val_1_rmse: 0.42671 |  0:00:57s
epoch 63 | loss: 0.16655 | val_0_rmse: 0.4058  | val_1_rmse: 0.42711 |  0:00:58s
epoch 64 | loss: 0.16928 | val_0_rmse: 0.39906 | val_1_rmse: 0.43202 |  0:00:59s
epoch 65 | loss: 0.17191 | val_0_rmse: 0.39416 | val_1_rmse: 0.41691 |  0:01:00s
epoch 66 | loss: 0.16824 | val_0_rmse: 0.39574 | val_1_rmse: 0.42006 |  0:01:01s
epoch 67 | loss: 0.167   | val_0_rmse: 0.39599 | val_1_rmse: 0.43108 |  0:01:01s
epoch 68 | loss: 0.16476 | val_0_rmse: 0.40576 | val_1_rmse: 0.42844 |  0:01:02s
epoch 69 | loss: 0.16968 | val_0_rmse: 0.41099 | val_1_rmse: 0.44425 |  0:01:03s
epoch 70 | loss: 0.17505 | val_0_rmse: 0.39489 | val_1_rmse: 0.42247 |  0:01:04s
epoch 71 | loss: 0.16908 | val_0_rmse: 0.40662 | val_1_rmse: 0.43604 |  0:01:05s
epoch 72 | loss: 0.16783 | val_0_rmse: 0.39514 | val_1_rmse: 0.42176 |  0:01:06s
epoch 73 | loss: 0.16711 | val_0_rmse: 0.40127 | val_1_rmse: 0.42394 |  0:01:07s
epoch 74 | loss: 0.16857 | val_0_rmse: 0.40041 | val_1_rmse: 0.43458 |  0:01:08s
epoch 75 | loss: 0.16751 | val_0_rmse: 0.40251 | val_1_rmse: 0.42587 |  0:01:09s
epoch 76 | loss: 0.16808 | val_0_rmse: 0.3922  | val_1_rmse: 0.42445 |  0:01:10s
epoch 77 | loss: 0.16451 | val_0_rmse: 0.40274 | val_1_rmse: 0.42484 |  0:01:11s
epoch 78 | loss: 0.16534 | val_0_rmse: 0.3944  | val_1_rmse: 0.4294  |  0:01:11s
epoch 79 | loss: 0.16529 | val_0_rmse: 0.38921 | val_1_rmse: 0.41988 |  0:01:12s
epoch 80 | loss: 0.16516 | val_0_rmse: 0.39094 | val_1_rmse: 0.42755 |  0:01:13s
epoch 81 | loss: 0.16261 | val_0_rmse: 0.39148 | val_1_rmse: 0.43006 |  0:01:14s
epoch 82 | loss: 0.16163 | val_0_rmse: 0.39219 | val_1_rmse: 0.41881 |  0:01:15s
epoch 83 | loss: 0.16059 | val_0_rmse: 0.39015 | val_1_rmse: 0.42623 |  0:01:16s
epoch 84 | loss: 0.15971 | val_0_rmse: 0.3911  | val_1_rmse: 0.42306 |  0:01:17s
epoch 85 | loss: 0.16143 | val_0_rmse: 0.38703 | val_1_rmse: 0.41444 |  0:01:18s
epoch 86 | loss: 0.15734 | val_0_rmse: 0.38519 | val_1_rmse: 0.423   |  0:01:19s
epoch 87 | loss: 0.16456 | val_0_rmse: 0.38596 | val_1_rmse: 0.42479 |  0:01:20s
epoch 88 | loss: 0.16094 | val_0_rmse: 0.38534 | val_1_rmse: 0.4235  |  0:01:21s
epoch 89 | loss: 0.15962 | val_0_rmse: 0.38295 | val_1_rmse: 0.42382 |  0:01:21s
epoch 90 | loss: 0.15601 | val_0_rmse: 0.38885 | val_1_rmse: 0.41764 |  0:01:22s
epoch 91 | loss: 0.1592  | val_0_rmse: 0.39115 | val_1_rmse: 0.42638 |  0:01:23s
epoch 92 | loss: 0.15963 | val_0_rmse: 0.3865  | val_1_rmse: 0.42157 |  0:01:24s
epoch 93 | loss: 0.1561  | val_0_rmse: 0.3873  | val_1_rmse: 0.42594 |  0:01:25s
epoch 94 | loss: 0.16034 | val_0_rmse: 0.38807 | val_1_rmse: 0.4191  |  0:01:26s
epoch 95 | loss: 0.1609  | val_0_rmse: 0.3991  | val_1_rmse: 0.42423 |  0:01:27s
epoch 96 | loss: 0.16402 | val_0_rmse: 0.39605 | val_1_rmse: 0.4323  |  0:01:28s
epoch 97 | loss: 0.15834 | val_0_rmse: 0.40456 | val_1_rmse: 0.44283 |  0:01:29s
epoch 98 | loss: 0.16289 | val_0_rmse: 0.3966  | val_1_rmse: 0.42508 |  0:01:30s
epoch 99 | loss: 0.16486 | val_0_rmse: 0.40593 | val_1_rmse: 0.45399 |  0:01:31s
epoch 100| loss: 0.16208 | val_0_rmse: 0.39292 | val_1_rmse: 0.41954 |  0:01:31s
epoch 101| loss: 0.16314 | val_0_rmse: 0.42732 | val_1_rmse: 0.44775 |  0:01:32s
epoch 102| loss: 0.16851 | val_0_rmse: 0.40309 | val_1_rmse: 0.44504 |  0:01:33s
epoch 103| loss: 0.16052 | val_0_rmse: 0.38888 | val_1_rmse: 0.424   |  0:01:34s
epoch 104| loss: 0.15716 | val_0_rmse: 0.38545 | val_1_rmse: 0.42737 |  0:01:35s
epoch 105| loss: 0.15758 | val_0_rmse: 0.39023 | val_1_rmse: 0.42304 |  0:01:36s
epoch 106| loss: 0.16134 | val_0_rmse: 0.38414 | val_1_rmse: 0.42337 |  0:01:37s
epoch 107| loss: 0.1581  | val_0_rmse: 0.38428 | val_1_rmse: 0.41888 |  0:01:38s
epoch 108| loss: 0.15788 | val_0_rmse: 0.38231 | val_1_rmse: 0.42576 |  0:01:39s
epoch 109| loss: 0.15558 | val_0_rmse: 0.39069 | val_1_rmse: 0.43701 |  0:01:40s
epoch 110| loss: 0.16278 | val_0_rmse: 0.37885 | val_1_rmse: 0.42017 |  0:01:41s
epoch 111| loss: 0.15748 | val_0_rmse: 0.37915 | val_1_rmse: 0.41769 |  0:01:41s
epoch 112| loss: 0.15244 | val_0_rmse: 0.3799  | val_1_rmse: 0.42364 |  0:01:42s
epoch 113| loss: 0.15287 | val_0_rmse: 0.38344 | val_1_rmse: 0.41953 |  0:01:43s
epoch 114| loss: 0.15393 | val_0_rmse: 0.38294 | val_1_rmse: 0.42427 |  0:01:44s
epoch 115| loss: 0.15525 | val_0_rmse: 0.38752 | val_1_rmse: 0.43281 |  0:01:45s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.41444
Best weights from best epoch are automatically used!
ended training at: 21:49:16
Feature importance:
Mean squared error is of 0.16849746886230738
Mean absolute error:0.27264210570009345
MAPE:0.33536749290540857
R2 score:0.6720784797082437
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:49:17
epoch 0  | loss: 1.57509 | val_0_rmse: 0.28271 | val_1_rmse: 0.27132 |  0:00:00s
epoch 1  | loss: 0.32671 | val_0_rmse: 0.27352 | val_1_rmse: 0.26273 |  0:00:00s
epoch 2  | loss: 0.15854 | val_0_rmse: 0.31193 | val_1_rmse: 0.29642 |  0:00:00s
epoch 3  | loss: 0.09977 | val_0_rmse: 0.32987 | val_1_rmse: 0.31512 |  0:00:01s
epoch 4  | loss: 0.07436 | val_0_rmse: 0.26163 | val_1_rmse: 0.24667 |  0:00:01s
epoch 5  | loss: 0.06617 | val_0_rmse: 0.2581  | val_1_rmse: 0.24598 |  0:00:01s
epoch 6  | loss: 0.06083 | val_0_rmse: 0.26735 | val_1_rmse: 0.25625 |  0:00:02s
epoch 7  | loss: 0.05869 | val_0_rmse: 0.23055 | val_1_rmse: 0.22004 |  0:00:02s
epoch 8  | loss: 0.05693 | val_0_rmse: 0.2246  | val_1_rmse: 0.21446 |  0:00:02s
epoch 9  | loss: 0.05477 | val_0_rmse: 0.23327 | val_1_rmse: 0.22277 |  0:00:03s
epoch 10 | loss: 0.05206 | val_0_rmse: 0.22317 | val_1_rmse: 0.21378 |  0:00:03s
epoch 11 | loss: 0.05263 | val_0_rmse: 0.22075 | val_1_rmse: 0.21251 |  0:00:03s
epoch 12 | loss: 0.05225 | val_0_rmse: 0.22713 | val_1_rmse: 0.2186  |  0:00:03s
epoch 13 | loss: 0.05186 | val_0_rmse: 0.22003 | val_1_rmse: 0.21216 |  0:00:04s
epoch 14 | loss: 0.05053 | val_0_rmse: 0.21973 | val_1_rmse: 0.21196 |  0:00:04s
epoch 15 | loss: 0.05052 | val_0_rmse: 0.22049 | val_1_rmse: 0.21248 |  0:00:04s
epoch 16 | loss: 0.05064 | val_0_rmse: 0.21941 | val_1_rmse: 0.21138 |  0:00:05s
epoch 17 | loss: 0.04935 | val_0_rmse: 0.21843 | val_1_rmse: 0.21082 |  0:00:05s
epoch 18 | loss: 0.05016 | val_0_rmse: 0.21659 | val_1_rmse: 0.20984 |  0:00:05s
epoch 19 | loss: 0.04987 | val_0_rmse: 0.21748 | val_1_rmse: 0.21026 |  0:00:06s
epoch 20 | loss: 0.04967 | val_0_rmse: 0.21737 | val_1_rmse: 0.21144 |  0:00:06s
epoch 21 | loss: 0.0512  | val_0_rmse: 0.2177  | val_1_rmse: 0.21017 |  0:00:06s
epoch 22 | loss: 0.05029 | val_0_rmse: 0.23017 | val_1_rmse: 0.22153 |  0:00:07s
epoch 23 | loss: 0.05255 | val_0_rmse: 0.23472 | val_1_rmse: 0.22675 |  0:00:07s
epoch 24 | loss: 0.05289 | val_0_rmse: 0.21752 | val_1_rmse: 0.21163 |  0:00:07s
epoch 25 | loss: 0.05135 | val_0_rmse: 0.21668 | val_1_rmse: 0.2101  |  0:00:07s
epoch 26 | loss: 0.04962 | val_0_rmse: 0.21886 | val_1_rmse: 0.21157 |  0:00:08s
epoch 27 | loss: 0.04809 | val_0_rmse: 0.21922 | val_1_rmse: 0.21178 |  0:00:08s
epoch 28 | loss: 0.04737 | val_0_rmse: 0.21912 | val_1_rmse: 0.21146 |  0:00:08s
epoch 29 | loss: 0.04832 | val_0_rmse: 0.21769 | val_1_rmse: 0.21112 |  0:00:09s
epoch 30 | loss: 0.0493  | val_0_rmse: 0.22014 | val_1_rmse: 0.21255 |  0:00:09s
epoch 31 | loss: 0.0477  | val_0_rmse: 0.21842 | val_1_rmse: 0.21078 |  0:00:09s
epoch 32 | loss: 0.04752 | val_0_rmse: 0.21707 | val_1_rmse: 0.21026 |  0:00:10s
epoch 33 | loss: 0.04836 | val_0_rmse: 0.21818 | val_1_rmse: 0.21249 |  0:00:10s
epoch 34 | loss: 0.04892 | val_0_rmse: 0.21988 | val_1_rmse: 0.21282 |  0:00:10s
epoch 35 | loss: 0.04753 | val_0_rmse: 0.22118 | val_1_rmse: 0.21353 |  0:00:10s
epoch 36 | loss: 0.04703 | val_0_rmse: 0.2174  | val_1_rmse: 0.21015 |  0:00:11s
epoch 37 | loss: 0.04784 | val_0_rmse: 0.22016 | val_1_rmse: 0.21262 |  0:00:11s
epoch 38 | loss: 0.04736 | val_0_rmse: 0.21718 | val_1_rmse: 0.21039 |  0:00:11s
epoch 39 | loss: 0.04758 | val_0_rmse: 0.21959 | val_1_rmse: 0.21222 |  0:00:12s
epoch 40 | loss: 0.04775 | val_0_rmse: 0.21565 | val_1_rmse: 0.2091  |  0:00:12s
epoch 41 | loss: 0.04787 | val_0_rmse: 0.21781 | val_1_rmse: 0.2116  |  0:00:12s
epoch 42 | loss: 0.04942 | val_0_rmse: 0.21475 | val_1_rmse: 0.20773 |  0:00:13s
epoch 43 | loss: 0.04823 | val_0_rmse: 0.21647 | val_1_rmse: 0.20965 |  0:00:13s
epoch 44 | loss: 0.04703 | val_0_rmse: 0.21871 | val_1_rmse: 0.21287 |  0:00:13s
epoch 45 | loss: 0.04832 | val_0_rmse: 0.21497 | val_1_rmse: 0.20851 |  0:00:13s
epoch 46 | loss: 0.04783 | val_0_rmse: 0.21665 | val_1_rmse: 0.21126 |  0:00:14s
epoch 47 | loss: 0.04759 | val_0_rmse: 0.22071 | val_1_rmse: 0.21377 |  0:00:14s
epoch 48 | loss: 0.04767 | val_0_rmse: 0.21881 | val_1_rmse: 0.21156 |  0:00:14s
epoch 49 | loss: 0.0478  | val_0_rmse: 0.21713 | val_1_rmse: 0.21025 |  0:00:15s
epoch 50 | loss: 0.04601 | val_0_rmse: 0.21696 | val_1_rmse: 0.21024 |  0:00:15s
epoch 51 | loss: 0.04656 | val_0_rmse: 0.21759 | val_1_rmse: 0.21141 |  0:00:15s
epoch 52 | loss: 0.04685 | val_0_rmse: 0.21903 | val_1_rmse: 0.21182 |  0:00:16s
epoch 53 | loss: 0.04653 | val_0_rmse: 0.21807 | val_1_rmse: 0.21104 |  0:00:16s
epoch 54 | loss: 0.04649 | val_0_rmse: 0.21741 | val_1_rmse: 0.21104 |  0:00:16s
epoch 55 | loss: 0.04589 | val_0_rmse: 0.21676 | val_1_rmse: 0.21043 |  0:00:16s
epoch 56 | loss: 0.04623 | val_0_rmse: 0.21469 | val_1_rmse: 0.20884 |  0:00:17s
epoch 57 | loss: 0.04639 | val_0_rmse: 0.21513 | val_1_rmse: 0.20964 |  0:00:17s
epoch 58 | loss: 0.04622 | val_0_rmse: 0.21796 | val_1_rmse: 0.21155 |  0:00:17s
epoch 59 | loss: 0.04575 | val_0_rmse: 0.21623 | val_1_rmse: 0.2114  |  0:00:18s
epoch 60 | loss: 0.04703 | val_0_rmse: 0.21512 | val_1_rmse: 0.21116 |  0:00:18s
epoch 61 | loss: 0.04778 | val_0_rmse: 0.2155  | val_1_rmse: 0.21027 |  0:00:18s
epoch 62 | loss: 0.04585 | val_0_rmse: 0.22011 | val_1_rmse: 0.21458 |  0:00:19s
epoch 63 | loss: 0.04607 | val_0_rmse: 0.21563 | val_1_rmse: 0.21001 |  0:00:19s
epoch 64 | loss: 0.0458  | val_0_rmse: 0.21347 | val_1_rmse: 0.20921 |  0:00:19s
epoch 65 | loss: 0.04556 | val_0_rmse: 0.21741 | val_1_rmse: 0.2153  |  0:00:19s
epoch 66 | loss: 0.04575 | val_0_rmse: 0.21274 | val_1_rmse: 0.20991 |  0:00:20s
epoch 67 | loss: 0.04468 | val_0_rmse: 0.21245 | val_1_rmse: 0.20929 |  0:00:20s
epoch 68 | loss: 0.04451 | val_0_rmse: 0.21579 | val_1_rmse: 0.21244 |  0:00:20s
epoch 69 | loss: 0.04546 | val_0_rmse: 0.20953 | val_1_rmse: 0.2075  |  0:00:21s
epoch 70 | loss: 0.04533 | val_0_rmse: 0.20909 | val_1_rmse: 0.20753 |  0:00:21s
epoch 71 | loss: 0.04482 | val_0_rmse: 0.20903 | val_1_rmse: 0.20753 |  0:00:21s
epoch 72 | loss: 0.04379 | val_0_rmse: 0.20991 | val_1_rmse: 0.20773 |  0:00:22s
epoch 73 | loss: 0.04549 | val_0_rmse: 0.20943 | val_1_rmse: 0.20602 |  0:00:22s
epoch 74 | loss: 0.04546 | val_0_rmse: 0.21544 | val_1_rmse: 0.21486 |  0:00:22s
epoch 75 | loss: 0.04664 | val_0_rmse: 0.21146 | val_1_rmse: 0.20999 |  0:00:22s
epoch 76 | loss: 0.04667 | val_0_rmse: 0.219   | val_1_rmse: 0.21467 |  0:00:23s
epoch 77 | loss: 0.04874 | val_0_rmse: 0.23414 | val_1_rmse: 0.22877 |  0:00:23s
epoch 78 | loss: 0.05095 | val_0_rmse: 0.21361 | val_1_rmse: 0.21321 |  0:00:23s
epoch 79 | loss: 0.04887 | val_0_rmse: 0.21441 | val_1_rmse: 0.2131  |  0:00:24s
epoch 80 | loss: 0.04765 | val_0_rmse: 0.22967 | val_1_rmse: 0.22548 |  0:00:24s
epoch 81 | loss: 0.04836 | val_0_rmse: 0.21103 | val_1_rmse: 0.20771 |  0:00:24s
epoch 82 | loss: 0.0446  | val_0_rmse: 0.20922 | val_1_rmse: 0.20759 |  0:00:25s
epoch 83 | loss: 0.04501 | val_0_rmse: 0.20875 | val_1_rmse: 0.20649 |  0:00:25s
epoch 84 | loss: 0.04472 | val_0_rmse: 0.20784 | val_1_rmse: 0.20707 |  0:00:25s
epoch 85 | loss: 0.04413 | val_0_rmse: 0.20592 | val_1_rmse: 0.20686 |  0:00:26s
epoch 86 | loss: 0.0432  | val_0_rmse: 0.20943 | val_1_rmse: 0.21172 |  0:00:26s
epoch 87 | loss: 0.04469 | val_0_rmse: 0.20773 | val_1_rmse: 0.20818 |  0:00:26s
epoch 88 | loss: 0.04324 | val_0_rmse: 0.20689 | val_1_rmse: 0.2071  |  0:00:26s
epoch 89 | loss: 0.04278 | val_0_rmse: 0.20756 | val_1_rmse: 0.20884 |  0:00:27s
epoch 90 | loss: 0.0436  | val_0_rmse: 0.20755 | val_1_rmse: 0.20815 |  0:00:27s
epoch 91 | loss: 0.04418 | val_0_rmse: 0.20731 | val_1_rmse: 0.20861 |  0:00:27s
epoch 92 | loss: 0.04255 | val_0_rmse: 0.20468 | val_1_rmse: 0.20706 |  0:00:28s
epoch 93 | loss: 0.04211 | val_0_rmse: 0.20532 | val_1_rmse: 0.20706 |  0:00:28s
epoch 94 | loss: 0.0424  | val_0_rmse: 0.20568 | val_1_rmse: 0.2069  |  0:00:28s
epoch 95 | loss: 0.04312 | val_0_rmse: 0.20355 | val_1_rmse: 0.2066  |  0:00:28s
epoch 96 | loss: 0.04229 | val_0_rmse: 0.20296 | val_1_rmse: 0.207   |  0:00:29s
epoch 97 | loss: 0.04179 | val_0_rmse: 0.20317 | val_1_rmse: 0.20668 |  0:00:29s
epoch 98 | loss: 0.04226 | val_0_rmse: 0.20291 | val_1_rmse: 0.20551 |  0:00:29s
epoch 99 | loss: 0.04296 | val_0_rmse: 0.20381 | val_1_rmse: 0.20725 |  0:00:30s
epoch 100| loss: 0.0416  | val_0_rmse: 0.20356 | val_1_rmse: 0.20709 |  0:00:30s
epoch 101| loss: 0.04307 | val_0_rmse: 0.21498 | val_1_rmse: 0.21695 |  0:00:30s
epoch 102| loss: 0.04404 | val_0_rmse: 0.20088 | val_1_rmse: 0.20658 |  0:00:31s
epoch 103| loss: 0.04256 | val_0_rmse: 0.20057 | val_1_rmse: 0.2082  |  0:00:31s
epoch 104| loss: 0.04133 | val_0_rmse: 0.19963 | val_1_rmse: 0.20935 |  0:00:31s
epoch 105| loss: 0.0418  | val_0_rmse: 0.20323 | val_1_rmse: 0.21113 |  0:00:31s
epoch 106| loss: 0.04277 | val_0_rmse: 0.20278 | val_1_rmse: 0.20875 |  0:00:32s
epoch 107| loss: 0.04226 | val_0_rmse: 0.20162 | val_1_rmse: 0.20595 |  0:00:32s
epoch 108| loss: 0.04249 | val_0_rmse: 0.20491 | val_1_rmse: 0.20745 |  0:00:32s
epoch 109| loss: 0.04207 | val_0_rmse: 0.20164 | val_1_rmse: 0.20534 |  0:00:33s
epoch 110| loss: 0.04138 | val_0_rmse: 0.20161 | val_1_rmse: 0.20506 |  0:00:33s
epoch 111| loss: 0.04105 | val_0_rmse: 0.20213 | val_1_rmse: 0.20421 |  0:00:33s
epoch 112| loss: 0.04086 | val_0_rmse: 0.20157 | val_1_rmse: 0.20526 |  0:00:34s
epoch 113| loss: 0.04246 | val_0_rmse: 0.20056 | val_1_rmse: 0.20559 |  0:00:34s
epoch 114| loss: 0.04077 | val_0_rmse: 0.20124 | val_1_rmse: 0.20517 |  0:00:34s
epoch 115| loss: 0.04051 | val_0_rmse: 0.20359 | val_1_rmse: 0.20948 |  0:00:34s
epoch 116| loss: 0.04166 | val_0_rmse: 0.20012 | val_1_rmse: 0.20413 |  0:00:35s
epoch 117| loss: 0.04275 | val_0_rmse: 0.20585 | val_1_rmse: 0.20846 |  0:00:35s
epoch 118| loss: 0.04281 | val_0_rmse: 0.20505 | val_1_rmse: 0.20716 |  0:00:35s
epoch 119| loss: 0.04266 | val_0_rmse: 0.20308 | val_1_rmse: 0.20709 |  0:00:36s
epoch 120| loss: 0.04166 | val_0_rmse: 0.20081 | val_1_rmse: 0.20522 |  0:00:36s
epoch 121| loss: 0.04138 | val_0_rmse: 0.2039  | val_1_rmse: 0.20785 |  0:00:36s
epoch 122| loss: 0.04143 | val_0_rmse: 0.19992 | val_1_rmse: 0.20579 |  0:00:37s
epoch 123| loss: 0.04163 | val_0_rmse: 0.20131 | val_1_rmse: 0.20995 |  0:00:37s
epoch 124| loss: 0.04173 | val_0_rmse: 0.20287 | val_1_rmse: 0.20864 |  0:00:37s
epoch 125| loss: 0.04162 | val_0_rmse: 0.20382 | val_1_rmse: 0.20678 |  0:00:38s
epoch 126| loss: 0.04376 | val_0_rmse: 0.21213 | val_1_rmse: 0.20848 |  0:00:38s
epoch 127| loss: 0.04369 | val_0_rmse: 0.20832 | val_1_rmse: 0.20743 |  0:00:38s
epoch 128| loss: 0.0439  | val_0_rmse: 0.2025  | val_1_rmse: 0.20899 |  0:00:38s
epoch 129| loss: 0.04308 | val_0_rmse: 0.20254 | val_1_rmse: 0.20944 |  0:00:39s
epoch 130| loss: 0.04236 | val_0_rmse: 0.20084 | val_1_rmse: 0.20619 |  0:00:39s
epoch 131| loss: 0.0419  | val_0_rmse: 0.19931 | val_1_rmse: 0.20588 |  0:00:39s
epoch 132| loss: 0.04061 | val_0_rmse: 0.19857 | val_1_rmse: 0.20695 |  0:00:40s
epoch 133| loss: 0.04125 | val_0_rmse: 0.19886 | val_1_rmse: 0.2071  |  0:00:40s
epoch 134| loss: 0.04175 | val_0_rmse: 0.20005 | val_1_rmse: 0.20731 |  0:00:40s
epoch 135| loss: 0.04208 | val_0_rmse: 0.20199 | val_1_rmse: 0.20832 |  0:00:41s
epoch 136| loss: 0.04274 | val_0_rmse: 0.20649 | val_1_rmse: 0.21599 |  0:00:41s
epoch 137| loss: 0.04351 | val_0_rmse: 0.20056 | val_1_rmse: 0.20976 |  0:00:41s
epoch 138| loss: 0.04157 | val_0_rmse: 0.20028 | val_1_rmse: 0.20652 |  0:00:41s
epoch 139| loss: 0.04215 | val_0_rmse: 0.20006 | val_1_rmse: 0.20639 |  0:00:42s
epoch 140| loss: 0.04128 | val_0_rmse: 0.20013 | val_1_rmse: 0.20745 |  0:00:42s
epoch 141| loss: 0.04142 | val_0_rmse: 0.19922 | val_1_rmse: 0.20745 |  0:00:42s
epoch 142| loss: 0.04194 | val_0_rmse: 0.20057 | val_1_rmse: 0.20821 |  0:00:43s
epoch 143| loss: 0.04076 | val_0_rmse: 0.20148 | val_1_rmse: 0.20851 |  0:00:43s
epoch 144| loss: 0.04115 | val_0_rmse: 0.19972 | val_1_rmse: 0.2088  |  0:00:43s
epoch 145| loss: 0.04055 | val_0_rmse: 0.19943 | val_1_rmse: 0.20755 |  0:00:43s
epoch 146| loss: 0.04075 | val_0_rmse: 0.2033  | val_1_rmse: 0.21327 |  0:00:44s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.20413
Best weights from best epoch are automatically used!
ended training at: 21:50:01
Feature importance:
Mean squared error is of 0.039664943515742485
Mean absolute error:0.14986456866217465
MAPE:0.1596789738822406
R2 score:0.3648978110373261
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:50:02
epoch 0  | loss: 1.32149 | val_0_rmse: 0.32913 | val_1_rmse: 0.33342 |  0:00:00s
epoch 1  | loss: 0.30899 | val_0_rmse: 0.24813 | val_1_rmse: 0.26709 |  0:00:00s
epoch 2  | loss: 0.12674 | val_0_rmse: 0.28996 | val_1_rmse: 0.31354 |  0:00:00s
epoch 3  | loss: 0.07811 | val_0_rmse: 0.28391 | val_1_rmse: 0.30687 |  0:00:01s
epoch 4  | loss: 0.06911 | val_0_rmse: 0.24721 | val_1_rmse: 0.26342 |  0:00:01s
epoch 5  | loss: 0.05846 | val_0_rmse: 0.28626 | val_1_rmse: 0.30985 |  0:00:01s
epoch 6  | loss: 0.05661 | val_0_rmse: 0.2679  | val_1_rmse: 0.29006 |  0:00:02s
epoch 7  | loss: 0.05745 | val_0_rmse: 0.24504 | val_1_rmse: 0.26462 |  0:00:02s
epoch 8  | loss: 0.05446 | val_0_rmse: 0.23421 | val_1_rmse: 0.24875 |  0:00:02s
epoch 9  | loss: 0.05338 | val_0_rmse: 0.24616 | val_1_rmse: 0.26673 |  0:00:03s
epoch 10 | loss: 0.05472 | val_0_rmse: 0.2479  | val_1_rmse: 0.26843 |  0:00:03s
epoch 11 | loss: 0.055   | val_0_rmse: 0.22885 | val_1_rmse: 0.24421 |  0:00:03s
epoch 12 | loss: 0.05271 | val_0_rmse: 0.23168 | val_1_rmse: 0.24867 |  0:00:04s
epoch 13 | loss: 0.05236 | val_0_rmse: 0.23863 | val_1_rmse: 0.25991 |  0:00:04s
epoch 14 | loss: 0.05304 | val_0_rmse: 0.23069 | val_1_rmse: 0.24896 |  0:00:04s
epoch 15 | loss: 0.05208 | val_0_rmse: 0.22515 | val_1_rmse: 0.23926 |  0:00:04s
epoch 16 | loss: 0.05123 | val_0_rmse: 0.23056 | val_1_rmse: 0.24543 |  0:00:05s
epoch 17 | loss: 0.05061 | val_0_rmse: 0.23046 | val_1_rmse: 0.24732 |  0:00:05s
epoch 18 | loss: 0.05175 | val_0_rmse: 0.22467 | val_1_rmse: 0.23757 |  0:00:05s
epoch 19 | loss: 0.05015 | val_0_rmse: 0.22721 | val_1_rmse: 0.24459 |  0:00:06s
epoch 20 | loss: 0.04995 | val_0_rmse: 0.22536 | val_1_rmse: 0.24214 |  0:00:06s
epoch 21 | loss: 0.04934 | val_0_rmse: 0.22524 | val_1_rmse: 0.24228 |  0:00:06s
epoch 22 | loss: 0.04949 | val_0_rmse: 0.22507 | val_1_rmse: 0.24052 |  0:00:07s
epoch 23 | loss: 0.04913 | val_0_rmse: 0.22359 | val_1_rmse: 0.23588 |  0:00:07s
epoch 24 | loss: 0.0502  | val_0_rmse: 0.22476 | val_1_rmse: 0.23908 |  0:00:07s
epoch 25 | loss: 0.04985 | val_0_rmse: 0.23026 | val_1_rmse: 0.24901 |  0:00:07s
epoch 26 | loss: 0.05043 | val_0_rmse: 0.22619 | val_1_rmse: 0.24375 |  0:00:08s
epoch 27 | loss: 0.0491  | val_0_rmse: 0.22319 | val_1_rmse: 0.23583 |  0:00:08s
epoch 28 | loss: 0.04891 | val_0_rmse: 0.22369 | val_1_rmse: 0.23756 |  0:00:08s
epoch 29 | loss: 0.04873 | val_0_rmse: 0.22958 | val_1_rmse: 0.24741 |  0:00:09s
epoch 30 | loss: 0.04941 | val_0_rmse: 0.226   | val_1_rmse: 0.24338 |  0:00:09s
epoch 31 | loss: 0.04926 | val_0_rmse: 0.2206  | val_1_rmse: 0.23239 |  0:00:09s
epoch 32 | loss: 0.04861 | val_0_rmse: 0.21997 | val_1_rmse: 0.23302 |  0:00:10s
epoch 33 | loss: 0.04811 | val_0_rmse: 0.22    | val_1_rmse: 0.23538 |  0:00:10s
epoch 34 | loss: 0.04694 | val_0_rmse: 0.21896 | val_1_rmse: 0.23293 |  0:00:10s
epoch 35 | loss: 0.04712 | val_0_rmse: 0.21661 | val_1_rmse: 0.22963 |  0:00:10s
epoch 36 | loss: 0.04714 | val_0_rmse: 0.21453 | val_1_rmse: 0.22636 |  0:00:11s
epoch 37 | loss: 0.04741 | val_0_rmse: 0.2135  | val_1_rmse: 0.22988 |  0:00:11s
epoch 38 | loss: 0.04709 | val_0_rmse: 0.2266  | val_1_rmse: 0.24722 |  0:00:11s
epoch 39 | loss: 0.04634 | val_0_rmse: 0.22044 | val_1_rmse: 0.23918 |  0:00:12s
epoch 40 | loss: 0.04599 | val_0_rmse: 0.2183  | val_1_rmse: 0.2358  |  0:00:12s
epoch 41 | loss: 0.04505 | val_0_rmse: 0.21557 | val_1_rmse: 0.22647 |  0:00:12s
epoch 42 | loss: 0.0454  | val_0_rmse: 0.22626 | val_1_rmse: 0.24332 |  0:00:13s
epoch 43 | loss: 0.04514 | val_0_rmse: 0.22473 | val_1_rmse: 0.2441  |  0:00:13s
epoch 44 | loss: 0.0457  | val_0_rmse: 0.21222 | val_1_rmse: 0.22202 |  0:00:13s
epoch 45 | loss: 0.04639 | val_0_rmse: 0.21026 | val_1_rmse: 0.22033 |  0:00:14s
epoch 46 | loss: 0.04592 | val_0_rmse: 0.21931 | val_1_rmse: 0.23641 |  0:00:14s
epoch 47 | loss: 0.04517 | val_0_rmse: 0.21109 | val_1_rmse: 0.22488 |  0:00:14s
epoch 48 | loss: 0.04519 | val_0_rmse: 0.20843 | val_1_rmse: 0.21507 |  0:00:14s
epoch 49 | loss: 0.04497 | val_0_rmse: 0.20834 | val_1_rmse: 0.21534 |  0:00:15s
epoch 50 | loss: 0.04507 | val_0_rmse: 0.21413 | val_1_rmse: 0.22783 |  0:00:15s
epoch 51 | loss: 0.04453 | val_0_rmse: 0.20864 | val_1_rmse: 0.21969 |  0:00:15s
epoch 52 | loss: 0.0441  | val_0_rmse: 0.20754 | val_1_rmse: 0.21401 |  0:00:16s
epoch 53 | loss: 0.04366 | val_0_rmse: 0.20855 | val_1_rmse: 0.21904 |  0:00:16s
epoch 54 | loss: 0.04325 | val_0_rmse: 0.20888 | val_1_rmse: 0.21925 |  0:00:16s
epoch 55 | loss: 0.04254 | val_0_rmse: 0.20932 | val_1_rmse: 0.21512 |  0:00:17s
epoch 56 | loss: 0.04348 | val_0_rmse: 0.21202 | val_1_rmse: 0.22239 |  0:00:17s
epoch 57 | loss: 0.043   | val_0_rmse: 0.21009 | val_1_rmse: 0.22077 |  0:00:17s
epoch 58 | loss: 0.04337 | val_0_rmse: 0.20773 | val_1_rmse: 0.2137  |  0:00:18s
epoch 59 | loss: 0.04323 | val_0_rmse: 0.20812 | val_1_rmse: 0.21528 |  0:00:18s
epoch 60 | loss: 0.04265 | val_0_rmse: 0.21193 | val_1_rmse: 0.22282 |  0:00:18s
epoch 61 | loss: 0.04367 | val_0_rmse: 0.20905 | val_1_rmse: 0.21772 |  0:00:18s
epoch 62 | loss: 0.04311 | val_0_rmse: 0.21209 | val_1_rmse: 0.22056 |  0:00:19s
epoch 63 | loss: 0.04374 | val_0_rmse: 0.20929 | val_1_rmse: 0.21518 |  0:00:19s
epoch 64 | loss: 0.04339 | val_0_rmse: 0.20842 | val_1_rmse: 0.21455 |  0:00:19s
epoch 65 | loss: 0.04352 | val_0_rmse: 0.20925 | val_1_rmse: 0.21859 |  0:00:20s
epoch 66 | loss: 0.04307 | val_0_rmse: 0.20861 | val_1_rmse: 0.21921 |  0:00:20s
epoch 67 | loss: 0.04263 | val_0_rmse: 0.20747 | val_1_rmse: 0.21797 |  0:00:20s
epoch 68 | loss: 0.04296 | val_0_rmse: 0.20775 | val_1_rmse: 0.21853 |  0:00:20s
epoch 69 | loss: 0.04278 | val_0_rmse: 0.20778 | val_1_rmse: 0.215   |  0:00:21s
epoch 70 | loss: 0.04217 | val_0_rmse: 0.20895 | val_1_rmse: 0.21857 |  0:00:21s
epoch 71 | loss: 0.04247 | val_0_rmse: 0.20684 | val_1_rmse: 0.21283 |  0:00:21s
epoch 72 | loss: 0.04266 | val_0_rmse: 0.20947 | val_1_rmse: 0.21649 |  0:00:22s
epoch 73 | loss: 0.04245 | val_0_rmse: 0.20982 | val_1_rmse: 0.21783 |  0:00:22s
epoch 74 | loss: 0.04249 | val_0_rmse: 0.2094  | val_1_rmse: 0.2194  |  0:00:22s
epoch 75 | loss: 0.04236 | val_0_rmse: 0.20926 | val_1_rmse: 0.21838 |  0:00:23s
epoch 76 | loss: 0.04263 | val_0_rmse: 0.20836 | val_1_rmse: 0.21791 |  0:00:23s
epoch 77 | loss: 0.04231 | val_0_rmse: 0.21309 | val_1_rmse: 0.2243  |  0:00:23s
epoch 78 | loss: 0.04304 | val_0_rmse: 0.2233  | val_1_rmse: 0.23516 |  0:00:24s
epoch 79 | loss: 0.04442 | val_0_rmse: 0.2083  | val_1_rmse: 0.21597 |  0:00:24s
epoch 80 | loss: 0.04365 | val_0_rmse: 0.20859 | val_1_rmse: 0.21305 |  0:00:24s
epoch 81 | loss: 0.04221 | val_0_rmse: 0.20743 | val_1_rmse: 0.21438 |  0:00:25s
epoch 82 | loss: 0.04293 | val_0_rmse: 0.20542 | val_1_rmse: 0.21379 |  0:00:25s
epoch 83 | loss: 0.04174 | val_0_rmse: 0.20579 | val_1_rmse: 0.21583 |  0:00:25s
epoch 84 | loss: 0.04121 | val_0_rmse: 0.20642 | val_1_rmse: 0.21602 |  0:00:25s
epoch 85 | loss: 0.04225 | val_0_rmse: 0.20862 | val_1_rmse: 0.22068 |  0:00:26s
epoch 86 | loss: 0.04201 | val_0_rmse: 0.2065  | val_1_rmse: 0.21521 |  0:00:26s
epoch 87 | loss: 0.04311 | val_0_rmse: 0.21214 | val_1_rmse: 0.21547 |  0:00:26s
epoch 88 | loss: 0.0425  | val_0_rmse: 0.20939 | val_1_rmse: 0.21798 |  0:00:27s
epoch 89 | loss: 0.04344 | val_0_rmse: 0.21145 | val_1_rmse: 0.22674 |  0:00:27s
epoch 90 | loss: 0.04142 | val_0_rmse: 0.20408 | val_1_rmse: 0.21386 |  0:00:27s
epoch 91 | loss: 0.04182 | val_0_rmse: 0.2036  | val_1_rmse: 0.2118  |  0:00:28s
epoch 92 | loss: 0.04259 | val_0_rmse: 0.20414 | val_1_rmse: 0.21402 |  0:00:28s
epoch 93 | loss: 0.04157 | val_0_rmse: 0.20367 | val_1_rmse: 0.21505 |  0:00:28s
epoch 94 | loss: 0.04178 | val_0_rmse: 0.203   | val_1_rmse: 0.21422 |  0:00:28s
epoch 95 | loss: 0.04179 | val_0_rmse: 0.20269 | val_1_rmse: 0.21471 |  0:00:29s
epoch 96 | loss: 0.04156 | val_0_rmse: 0.20222 | val_1_rmse: 0.21214 |  0:00:29s
epoch 97 | loss: 0.04149 | val_0_rmse: 0.20139 | val_1_rmse: 0.21032 |  0:00:29s
epoch 98 | loss: 0.04151 | val_0_rmse: 0.20251 | val_1_rmse: 0.21587 |  0:00:30s
epoch 99 | loss: 0.04107 | val_0_rmse: 0.20115 | val_1_rmse: 0.21347 |  0:00:30s
epoch 100| loss: 0.04104 | val_0_rmse: 0.2025  | val_1_rmse: 0.21638 |  0:00:30s
epoch 101| loss: 0.04125 | val_0_rmse: 0.20073 | val_1_rmse: 0.21005 |  0:00:31s
epoch 102| loss: 0.04055 | val_0_rmse: 0.20271 | val_1_rmse: 0.21699 |  0:00:31s
epoch 103| loss: 0.041   | val_0_rmse: 0.20242 | val_1_rmse: 0.21771 |  0:00:31s
epoch 104| loss: 0.04098 | val_0_rmse: 0.20072 | val_1_rmse: 0.21323 |  0:00:31s
epoch 105| loss: 0.04147 | val_0_rmse: 0.20132 | val_1_rmse: 0.21375 |  0:00:32s
epoch 106| loss: 0.04158 | val_0_rmse: 0.20287 | val_1_rmse: 0.21341 |  0:00:32s
epoch 107| loss: 0.04143 | val_0_rmse: 0.19903 | val_1_rmse: 0.21238 |  0:00:32s
epoch 108| loss: 0.04129 | val_0_rmse: 0.202   | val_1_rmse: 0.22024 |  0:00:33s
epoch 109| loss: 0.04109 | val_0_rmse: 0.20407 | val_1_rmse: 0.22213 |  0:00:33s
epoch 110| loss: 0.04192 | val_0_rmse: 0.19812 | val_1_rmse: 0.21241 |  0:00:33s
epoch 111| loss: 0.04084 | val_0_rmse: 0.19763 | val_1_rmse: 0.21347 |  0:00:34s
epoch 112| loss: 0.04022 | val_0_rmse: 0.19658 | val_1_rmse: 0.21256 |  0:00:34s
epoch 113| loss: 0.0407  | val_0_rmse: 0.19681 | val_1_rmse: 0.21341 |  0:00:34s
epoch 114| loss: 0.04004 | val_0_rmse: 0.19904 | val_1_rmse: 0.21934 |  0:00:35s
epoch 115| loss: 0.04063 | val_0_rmse: 0.19872 | val_1_rmse: 0.21852 |  0:00:35s
epoch 116| loss: 0.04081 | val_0_rmse: 0.19572 | val_1_rmse: 0.21309 |  0:00:35s
epoch 117| loss: 0.03949 | val_0_rmse: 0.19564 | val_1_rmse: 0.21153 |  0:00:35s
epoch 118| loss: 0.03951 | val_0_rmse: 0.19597 | val_1_rmse: 0.21419 |  0:00:36s
epoch 119| loss: 0.0397  | val_0_rmse: 0.19729 | val_1_rmse: 0.21252 |  0:00:36s
epoch 120| loss: 0.04092 | val_0_rmse: 0.19545 | val_1_rmse: 0.21263 |  0:00:36s
epoch 121| loss: 0.03942 | val_0_rmse: 0.19671 | val_1_rmse: 0.21539 |  0:00:37s
epoch 122| loss: 0.03961 | val_0_rmse: 0.19417 | val_1_rmse: 0.21267 |  0:00:37s
epoch 123| loss: 0.03936 | val_0_rmse: 0.19409 | val_1_rmse: 0.21491 |  0:00:37s
epoch 124| loss: 0.03948 | val_0_rmse: 0.20097 | val_1_rmse: 0.22475 |  0:00:38s
epoch 125| loss: 0.03998 | val_0_rmse: 0.20086 | val_1_rmse: 0.22512 |  0:00:38s
epoch 126| loss: 0.04143 | val_0_rmse: 0.19475 | val_1_rmse: 0.21126 |  0:00:38s
epoch 127| loss: 0.04091 | val_0_rmse: 0.19975 | val_1_rmse: 0.21268 |  0:00:38s
epoch 128| loss: 0.04014 | val_0_rmse: 0.19371 | val_1_rmse: 0.2126  |  0:00:39s
epoch 129| loss: 0.03972 | val_0_rmse: 0.19362 | val_1_rmse: 0.21285 |  0:00:39s
epoch 130| loss: 0.0386  | val_0_rmse: 0.19455 | val_1_rmse: 0.21648 |  0:00:39s
epoch 131| loss: 0.03931 | val_0_rmse: 0.19347 | val_1_rmse: 0.21156 |  0:00:40s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.21005
Best weights from best epoch are automatically used!
ended training at: 21:50:42
Feature importance:
Mean squared error is of 0.04625744330996359
Mean absolute error:0.15826364777712074
MAPE:0.1667121579110432
R2 score:0.3446087665831644
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:50:42
epoch 0  | loss: 1.38203 | val_0_rmse: 0.40128 | val_1_rmse: 0.39379 |  0:00:00s
epoch 1  | loss: 0.41032 | val_0_rmse: 0.29622 | val_1_rmse: 0.31156 |  0:00:00s
epoch 2  | loss: 0.1488  | val_0_rmse: 0.35303 | val_1_rmse: 0.37007 |  0:00:00s
epoch 3  | loss: 0.10107 | val_0_rmse: 0.33253 | val_1_rmse: 0.3455  |  0:00:01s
epoch 4  | loss: 0.08428 | val_0_rmse: 0.27939 | val_1_rmse: 0.29317 |  0:00:01s
epoch 5  | loss: 0.07388 | val_0_rmse: 0.27244 | val_1_rmse: 0.28585 |  0:00:01s
epoch 6  | loss: 0.06718 | val_0_rmse: 0.25973 | val_1_rmse: 0.26949 |  0:00:02s
epoch 7  | loss: 0.06133 | val_0_rmse: 0.24448 | val_1_rmse: 0.25164 |  0:00:02s
epoch 8  | loss: 0.05754 | val_0_rmse: 0.23977 | val_1_rmse: 0.2478  |  0:00:02s
epoch 9  | loss: 0.05534 | val_0_rmse: 0.22352 | val_1_rmse: 0.23465 |  0:00:03s
epoch 10 | loss: 0.05225 | val_0_rmse: 0.21809 | val_1_rmse: 0.2269  |  0:00:03s
epoch 11 | loss: 0.0502  | val_0_rmse: 0.21915 | val_1_rmse: 0.22902 |  0:00:03s
epoch 12 | loss: 0.04856 | val_0_rmse: 0.21756 | val_1_rmse: 0.22726 |  0:00:04s
epoch 13 | loss: 0.04781 | val_0_rmse: 0.21494 | val_1_rmse: 0.22446 |  0:00:04s
epoch 14 | loss: 0.04759 | val_0_rmse: 0.21487 | val_1_rmse: 0.22477 |  0:00:04s
epoch 15 | loss: 0.04729 | val_0_rmse: 0.22345 | val_1_rmse: 0.23545 |  0:00:04s
epoch 16 | loss: 0.04695 | val_0_rmse: 0.21823 | val_1_rmse: 0.22884 |  0:00:05s
epoch 17 | loss: 0.04733 | val_0_rmse: 0.22237 | val_1_rmse: 0.23224 |  0:00:05s
epoch 18 | loss: 0.04763 | val_0_rmse: 0.21757 | val_1_rmse: 0.22688 |  0:00:05s
epoch 19 | loss: 0.04771 | val_0_rmse: 0.21833 | val_1_rmse: 0.22822 |  0:00:06s
epoch 20 | loss: 0.04803 | val_0_rmse: 0.22061 | val_1_rmse: 0.23325 |  0:00:06s
epoch 21 | loss: 0.04917 | val_0_rmse: 0.21774 | val_1_rmse: 0.22982 |  0:00:06s
epoch 22 | loss: 0.0484  | val_0_rmse: 0.21436 | val_1_rmse: 0.22338 |  0:00:07s
epoch 23 | loss: 0.04752 | val_0_rmse: 0.22287 | val_1_rmse: 0.22842 |  0:00:07s
epoch 24 | loss: 0.04724 | val_0_rmse: 0.21456 | val_1_rmse: 0.22502 |  0:00:07s
epoch 25 | loss: 0.0461  | val_0_rmse: 0.21484 | val_1_rmse: 0.22372 |  0:00:07s
epoch 26 | loss: 0.04513 | val_0_rmse: 0.21496 | val_1_rmse: 0.22239 |  0:00:08s
epoch 27 | loss: 0.04562 | val_0_rmse: 0.22078 | val_1_rmse: 0.2268  |  0:00:08s
epoch 28 | loss: 0.04512 | val_0_rmse: 0.21647 | val_1_rmse: 0.22354 |  0:00:08s
epoch 29 | loss: 0.04513 | val_0_rmse: 0.21478 | val_1_rmse: 0.22378 |  0:00:09s
epoch 30 | loss: 0.04615 | val_0_rmse: 0.21368 | val_1_rmse: 0.22217 |  0:00:09s
epoch 31 | loss: 0.04543 | val_0_rmse: 0.21812 | val_1_rmse: 0.22383 |  0:00:09s
epoch 32 | loss: 0.04478 | val_0_rmse: 0.21151 | val_1_rmse: 0.2213  |  0:00:10s
epoch 33 | loss: 0.04492 | val_0_rmse: 0.21238 | val_1_rmse: 0.22225 |  0:00:10s
epoch 34 | loss: 0.04418 | val_0_rmse: 0.21304 | val_1_rmse: 0.22051 |  0:00:10s
epoch 35 | loss: 0.04467 | val_0_rmse: 0.21049 | val_1_rmse: 0.22074 |  0:00:10s
epoch 36 | loss: 0.04463 | val_0_rmse: 0.21237 | val_1_rmse: 0.223   |  0:00:11s
epoch 37 | loss: 0.04397 | val_0_rmse: 0.20927 | val_1_rmse: 0.21911 |  0:00:11s
epoch 38 | loss: 0.04422 | val_0_rmse: 0.21065 | val_1_rmse: 0.22024 |  0:00:11s
epoch 39 | loss: 0.04368 | val_0_rmse: 0.21158 | val_1_rmse: 0.2227  |  0:00:12s
epoch 40 | loss: 0.04389 | val_0_rmse: 0.20953 | val_1_rmse: 0.21988 |  0:00:12s
epoch 41 | loss: 0.04409 | val_0_rmse: 0.21073 | val_1_rmse: 0.21936 |  0:00:12s
epoch 42 | loss: 0.04424 | val_0_rmse: 0.20895 | val_1_rmse: 0.21864 |  0:00:13s
epoch 43 | loss: 0.0437  | val_0_rmse: 0.20905 | val_1_rmse: 0.21943 |  0:00:13s
epoch 44 | loss: 0.04346 | val_0_rmse: 0.21024 | val_1_rmse: 0.22298 |  0:00:13s
epoch 45 | loss: 0.04442 | val_0_rmse: 0.20987 | val_1_rmse: 0.22187 |  0:00:13s
epoch 46 | loss: 0.04515 | val_0_rmse: 0.20863 | val_1_rmse: 0.22025 |  0:00:14s
epoch 47 | loss: 0.04421 | val_0_rmse: 0.21063 | val_1_rmse: 0.21991 |  0:00:14s
epoch 48 | loss: 0.04399 | val_0_rmse: 0.20959 | val_1_rmse: 0.21983 |  0:00:14s
epoch 49 | loss: 0.04303 | val_0_rmse: 0.2067  | val_1_rmse: 0.21847 |  0:00:15s
epoch 50 | loss: 0.04286 | val_0_rmse: 0.20898 | val_1_rmse: 0.22227 |  0:00:15s
epoch 51 | loss: 0.04418 | val_0_rmse: 0.20801 | val_1_rmse: 0.2204  |  0:00:15s
epoch 52 | loss: 0.04269 | val_0_rmse: 0.20592 | val_1_rmse: 0.21761 |  0:00:16s
epoch 53 | loss: 0.04273 | val_0_rmse: 0.20768 | val_1_rmse: 0.21808 |  0:00:16s
epoch 54 | loss: 0.04367 | val_0_rmse: 0.21289 | val_1_rmse: 0.22126 |  0:00:16s
epoch 55 | loss: 0.04294 | val_0_rmse: 0.20778 | val_1_rmse: 0.21991 |  0:00:17s
epoch 56 | loss: 0.04231 | val_0_rmse: 0.20693 | val_1_rmse: 0.2175  |  0:00:17s
epoch 57 | loss: 0.04228 | val_0_rmse: 0.21046 | val_1_rmse: 0.21951 |  0:00:17s
epoch 58 | loss: 0.0425  | val_0_rmse: 0.20753 | val_1_rmse: 0.21817 |  0:00:17s
epoch 59 | loss: 0.04158 | val_0_rmse: 0.20561 | val_1_rmse: 0.21785 |  0:00:18s
epoch 60 | loss: 0.04232 | val_0_rmse: 0.20827 | val_1_rmse: 0.21944 |  0:00:18s
epoch 61 | loss: 0.04243 | val_0_rmse: 0.20848 | val_1_rmse: 0.21846 |  0:00:18s
epoch 62 | loss: 0.04084 | val_0_rmse: 0.20491 | val_1_rmse: 0.21803 |  0:00:19s
epoch 63 | loss: 0.04271 | val_0_rmse: 0.20433 | val_1_rmse: 0.21771 |  0:00:19s
epoch 64 | loss: 0.04123 | val_0_rmse: 0.20638 | val_1_rmse: 0.21703 |  0:00:19s
epoch 65 | loss: 0.04207 | val_0_rmse: 0.2114  | val_1_rmse: 0.22135 |  0:00:20s
epoch 66 | loss: 0.04274 | val_0_rmse: 0.20381 | val_1_rmse: 0.21605 |  0:00:20s
epoch 67 | loss: 0.04215 | val_0_rmse: 0.20312 | val_1_rmse: 0.21631 |  0:00:20s
epoch 68 | loss: 0.04279 | val_0_rmse: 0.20435 | val_1_rmse: 0.21803 |  0:00:20s
epoch 69 | loss: 0.04344 | val_0_rmse: 0.20331 | val_1_rmse: 0.21627 |  0:00:21s
epoch 70 | loss: 0.0411  | val_0_rmse: 0.20777 | val_1_rmse: 0.21826 |  0:00:21s
epoch 71 | loss: 0.0418  | val_0_rmse: 0.20817 | val_1_rmse: 0.21842 |  0:00:21s
epoch 72 | loss: 0.04126 | val_0_rmse: 0.202   | val_1_rmse: 0.21423 |  0:00:22s
epoch 73 | loss: 0.04167 | val_0_rmse: 0.20519 | val_1_rmse: 0.22101 |  0:00:22s
epoch 74 | loss: 0.04271 | val_0_rmse: 0.20459 | val_1_rmse: 0.22065 |  0:00:22s
epoch 75 | loss: 0.04195 | val_0_rmse: 0.20253 | val_1_rmse: 0.21594 |  0:00:23s
epoch 76 | loss: 0.04234 | val_0_rmse: 0.2058  | val_1_rmse: 0.21891 |  0:00:23s
epoch 77 | loss: 0.0412  | val_0_rmse: 0.20371 | val_1_rmse: 0.21778 |  0:00:23s
epoch 78 | loss: 0.04086 | val_0_rmse: 0.2039  | val_1_rmse: 0.21722 |  0:00:23s
epoch 79 | loss: 0.04263 | val_0_rmse: 0.20049 | val_1_rmse: 0.21618 |  0:00:24s
epoch 80 | loss: 0.04057 | val_0_rmse: 0.20042 | val_1_rmse: 0.21619 |  0:00:24s
epoch 81 | loss: 0.04074 | val_0_rmse: 0.20034 | val_1_rmse: 0.2152  |  0:00:24s
epoch 82 | loss: 0.04111 | val_0_rmse: 0.20487 | val_1_rmse: 0.21821 |  0:00:25s
epoch 83 | loss: 0.04077 | val_0_rmse: 0.20226 | val_1_rmse: 0.21718 |  0:00:25s
epoch 84 | loss: 0.04031 | val_0_rmse: 0.20486 | val_1_rmse: 0.21799 |  0:00:25s
epoch 85 | loss: 0.04098 | val_0_rmse: 0.20076 | val_1_rmse: 0.21492 |  0:00:26s
epoch 86 | loss: 0.03976 | val_0_rmse: 0.19954 | val_1_rmse: 0.21444 |  0:00:26s
epoch 87 | loss: 0.03995 | val_0_rmse: 0.20048 | val_1_rmse: 0.21575 |  0:00:26s
epoch 88 | loss: 0.04037 | val_0_rmse: 0.20172 | val_1_rmse: 0.2186  |  0:00:26s
epoch 89 | loss: 0.04144 | val_0_rmse: 0.19871 | val_1_rmse: 0.21501 |  0:00:27s
epoch 90 | loss: 0.03983 | val_0_rmse: 0.20514 | val_1_rmse: 0.21885 |  0:00:27s
epoch 91 | loss: 0.04043 | val_0_rmse: 0.20861 | val_1_rmse: 0.22218 |  0:00:27s
epoch 92 | loss: 0.04053 | val_0_rmse: 0.19922 | val_1_rmse: 0.21719 |  0:00:28s
epoch 93 | loss: 0.04087 | val_0_rmse: 0.19735 | val_1_rmse: 0.21458 |  0:00:28s
epoch 94 | loss: 0.04029 | val_0_rmse: 0.20152 | val_1_rmse: 0.2153  |  0:00:28s
epoch 95 | loss: 0.03948 | val_0_rmse: 0.19886 | val_1_rmse: 0.213   |  0:00:29s
epoch 96 | loss: 0.04004 | val_0_rmse: 0.19965 | val_1_rmse: 0.21575 |  0:00:29s
epoch 97 | loss: 0.04021 | val_0_rmse: 0.19768 | val_1_rmse: 0.21462 |  0:00:29s
epoch 98 | loss: 0.03916 | val_0_rmse: 0.19688 | val_1_rmse: 0.21395 |  0:00:29s
epoch 99 | loss: 0.04042 | val_0_rmse: 0.19772 | val_1_rmse: 0.21332 |  0:00:30s
epoch 100| loss: 0.04069 | val_0_rmse: 0.19817 | val_1_rmse: 0.21318 |  0:00:30s
epoch 101| loss: 0.04019 | val_0_rmse: 0.19583 | val_1_rmse: 0.21272 |  0:00:30s
epoch 102| loss: 0.03937 | val_0_rmse: 0.19855 | val_1_rmse: 0.21434 |  0:00:31s
epoch 103| loss: 0.04104 | val_0_rmse: 0.19909 | val_1_rmse: 0.21543 |  0:00:31s
epoch 104| loss: 0.04098 | val_0_rmse: 0.19811 | val_1_rmse: 0.21502 |  0:00:31s
epoch 105| loss: 0.04075 | val_0_rmse: 0.20217 | val_1_rmse: 0.21722 |  0:00:32s
epoch 106| loss: 0.04127 | val_0_rmse: 0.20027 | val_1_rmse: 0.216   |  0:00:32s
epoch 107| loss: 0.04113 | val_0_rmse: 0.20072 | val_1_rmse: 0.21591 |  0:00:32s
epoch 108| loss: 0.04208 | val_0_rmse: 0.20679 | val_1_rmse: 0.22253 |  0:00:32s
epoch 109| loss: 0.04279 | val_0_rmse: 0.20196 | val_1_rmse: 0.2163  |  0:00:33s
epoch 110| loss: 0.04113 | val_0_rmse: 0.20431 | val_1_rmse: 0.21776 |  0:00:33s
epoch 111| loss: 0.04193 | val_0_rmse: 0.19999 | val_1_rmse: 0.21724 |  0:00:33s
epoch 112| loss: 0.0413  | val_0_rmse: 0.1995  | val_1_rmse: 0.21719 |  0:00:34s
epoch 113| loss: 0.04061 | val_0_rmse: 0.19924 | val_1_rmse: 0.2158  |  0:00:34s
epoch 114| loss: 0.03971 | val_0_rmse: 0.1976  | val_1_rmse: 0.21547 |  0:00:34s
epoch 115| loss: 0.04054 | val_0_rmse: 0.19731 | val_1_rmse: 0.21572 |  0:00:35s
epoch 116| loss: 0.03985 | val_0_rmse: 0.19569 | val_1_rmse: 0.21421 |  0:00:35s
epoch 117| loss: 0.03898 | val_0_rmse: 0.19538 | val_1_rmse: 0.21425 |  0:00:35s
epoch 118| loss: 0.03861 | val_0_rmse: 0.1967  | val_1_rmse: 0.21378 |  0:00:35s
epoch 119| loss: 0.03932 | val_0_rmse: 0.19535 | val_1_rmse: 0.21399 |  0:00:36s
epoch 120| loss: 0.03837 | val_0_rmse: 0.19383 | val_1_rmse: 0.21162 |  0:00:36s
epoch 121| loss: 0.03929 | val_0_rmse: 0.19634 | val_1_rmse: 0.21506 |  0:00:36s
epoch 122| loss: 0.0391  | val_0_rmse: 0.19439 | val_1_rmse: 0.21352 |  0:00:37s
epoch 123| loss: 0.03922 | val_0_rmse: 0.19321 | val_1_rmse: 0.21299 |  0:00:37s
epoch 124| loss: 0.04025 | val_0_rmse: 0.193   | val_1_rmse: 0.21407 |  0:00:37s
epoch 125| loss: 0.0383  | val_0_rmse: 0.19299 | val_1_rmse: 0.21477 |  0:00:38s
epoch 126| loss: 0.03808 | val_0_rmse: 0.19309 | val_1_rmse: 0.21593 |  0:00:38s
epoch 127| loss: 0.0383  | val_0_rmse: 0.19274 | val_1_rmse: 0.21585 |  0:00:38s
epoch 128| loss: 0.03832 | val_0_rmse: 0.19452 | val_1_rmse: 0.21598 |  0:00:39s
epoch 129| loss: 0.03845 | val_0_rmse: 0.19275 | val_1_rmse: 0.21546 |  0:00:39s
epoch 130| loss: 0.0381  | val_0_rmse: 0.193   | val_1_rmse: 0.21692 |  0:00:39s
epoch 131| loss: 0.03867 | val_0_rmse: 0.19314 | val_1_rmse: 0.21651 |  0:00:39s
epoch 132| loss: 0.03808 | val_0_rmse: 0.19321 | val_1_rmse: 0.21716 |  0:00:40s
epoch 133| loss: 0.03776 | val_0_rmse: 0.19584 | val_1_rmse: 0.21745 |  0:00:40s
epoch 134| loss: 0.03868 | val_0_rmse: 0.19154 | val_1_rmse: 0.21258 |  0:00:40s
epoch 135| loss: 0.03821 | val_0_rmse: 0.19137 | val_1_rmse: 0.21342 |  0:00:41s
epoch 136| loss: 0.03821 | val_0_rmse: 0.19218 | val_1_rmse: 0.21402 |  0:00:41s
epoch 137| loss: 0.03817 | val_0_rmse: 0.19707 | val_1_rmse: 0.21969 |  0:00:41s
epoch 138| loss: 0.03955 | val_0_rmse: 0.1911  | val_1_rmse: 0.2126  |  0:00:41s
epoch 139| loss: 0.03785 | val_0_rmse: 0.19016 | val_1_rmse: 0.21192 |  0:00:42s
epoch 140| loss: 0.03697 | val_0_rmse: 0.19114 | val_1_rmse: 0.21279 |  0:00:42s
epoch 141| loss: 0.0372  | val_0_rmse: 0.18914 | val_1_rmse: 0.21215 |  0:00:42s
epoch 142| loss: 0.03746 | val_0_rmse: 0.19055 | val_1_rmse: 0.21488 |  0:00:43s
epoch 143| loss: 0.03715 | val_0_rmse: 0.18992 | val_1_rmse: 0.2159  |  0:00:43s
epoch 144| loss: 0.03832 | val_0_rmse: 0.18866 | val_1_rmse: 0.21243 |  0:00:43s
epoch 145| loss: 0.03753 | val_0_rmse: 0.19238 | val_1_rmse: 0.21629 |  0:00:44s
epoch 146| loss: 0.03758 | val_0_rmse: 0.19257 | val_1_rmse: 0.21648 |  0:00:44s
epoch 147| loss: 0.03737 | val_0_rmse: 0.18861 | val_1_rmse: 0.21565 |  0:00:44s
epoch 148| loss: 0.03676 | val_0_rmse: 0.18797 | val_1_rmse: 0.21603 |  0:00:44s
epoch 149| loss: 0.03682 | val_0_rmse: 0.18683 | val_1_rmse: 0.21452 |  0:00:45s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.21162
Best weights from best epoch are automatically used!
ended training at: 21:51:28
Feature importance:
Mean squared error is of 0.048592266517863955
Mean absolute error:0.1601086668038691
MAPE:0.16092401391408187
R2 score:0.33853192885144945
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:51:28
epoch 0  | loss: 1.36995 | val_0_rmse: 0.29278 | val_1_rmse: 0.29964 |  0:00:00s
epoch 1  | loss: 0.52813 | val_0_rmse: 0.27654 | val_1_rmse: 0.28618 |  0:00:00s
epoch 2  | loss: 0.16447 | val_0_rmse: 0.26177 | val_1_rmse: 0.26818 |  0:00:00s
epoch 3  | loss: 0.09549 | val_0_rmse: 0.27813 | val_1_rmse: 0.28528 |  0:00:01s
epoch 4  | loss: 0.08912 | val_0_rmse: 0.27363 | val_1_rmse: 0.27706 |  0:00:01s
epoch 5  | loss: 0.07442 | val_0_rmse: 0.26773 | val_1_rmse: 0.27065 |  0:00:01s
epoch 6  | loss: 0.06256 | val_0_rmse: 0.23913 | val_1_rmse: 0.24246 |  0:00:02s
epoch 7  | loss: 0.05598 | val_0_rmse: 0.23387 | val_1_rmse: 0.23695 |  0:00:02s
epoch 8  | loss: 0.05327 | val_0_rmse: 0.23983 | val_1_rmse: 0.24117 |  0:00:02s
epoch 9  | loss: 0.05318 | val_0_rmse: 0.24176 | val_1_rmse: 0.2424  |  0:00:03s
epoch 10 | loss: 0.05234 | val_0_rmse: 0.23491 | val_1_rmse: 0.23568 |  0:00:03s
epoch 11 | loss: 0.05097 | val_0_rmse: 0.22665 | val_1_rmse: 0.22783 |  0:00:03s
epoch 12 | loss: 0.0508  | val_0_rmse: 0.21919 | val_1_rmse: 0.2205  |  0:00:04s
epoch 13 | loss: 0.04955 | val_0_rmse: 0.23705 | val_1_rmse: 0.23866 |  0:00:04s
epoch 14 | loss: 0.05036 | val_0_rmse: 0.22181 | val_1_rmse: 0.22378 |  0:00:04s
epoch 15 | loss: 0.04904 | val_0_rmse: 0.21911 | val_1_rmse: 0.22087 |  0:00:04s
epoch 16 | loss: 0.04899 | val_0_rmse: 0.21902 | val_1_rmse: 0.22156 |  0:00:05s
epoch 17 | loss: 0.04931 | val_0_rmse: 0.22063 | val_1_rmse: 0.22321 |  0:00:05s
epoch 18 | loss: 0.04919 | val_0_rmse: 0.221   | val_1_rmse: 0.22385 |  0:00:05s
epoch 19 | loss: 0.04931 | val_0_rmse: 0.2247  | val_1_rmse: 0.2275  |  0:00:06s
epoch 20 | loss: 0.04862 | val_0_rmse: 0.21988 | val_1_rmse: 0.22211 |  0:00:06s
epoch 21 | loss: 0.04796 | val_0_rmse: 0.21717 | val_1_rmse: 0.21938 |  0:00:06s
epoch 22 | loss: 0.04972 | val_0_rmse: 0.21842 | val_1_rmse: 0.22062 |  0:00:07s
epoch 23 | loss: 0.04974 | val_0_rmse: 0.21566 | val_1_rmse: 0.21688 |  0:00:07s
epoch 24 | loss: 0.05017 | val_0_rmse: 0.22293 | val_1_rmse: 0.22558 |  0:00:07s
epoch 25 | loss: 0.0496  | val_0_rmse: 0.22514 | val_1_rmse: 0.22786 |  0:00:07s
epoch 26 | loss: 0.04904 | val_0_rmse: 0.22482 | val_1_rmse: 0.22706 |  0:00:08s
epoch 27 | loss: 0.04881 | val_0_rmse: 0.21764 | val_1_rmse: 0.22004 |  0:00:08s
epoch 28 | loss: 0.04757 | val_0_rmse: 0.21685 | val_1_rmse: 0.2185  |  0:00:08s
epoch 29 | loss: 0.04751 | val_0_rmse: 0.21908 | val_1_rmse: 0.2209  |  0:00:09s
epoch 30 | loss: 0.0484  | val_0_rmse: 0.22072 | val_1_rmse: 0.22293 |  0:00:09s
epoch 31 | loss: 0.04817 | val_0_rmse: 0.2147  | val_1_rmse: 0.21694 |  0:00:09s
epoch 32 | loss: 0.04721 | val_0_rmse: 0.21429 | val_1_rmse: 0.21634 |  0:00:09s
epoch 33 | loss: 0.04692 | val_0_rmse: 0.21445 | val_1_rmse: 0.21694 |  0:00:10s
epoch 34 | loss: 0.04674 | val_0_rmse: 0.21375 | val_1_rmse: 0.21586 |  0:00:10s
epoch 35 | loss: 0.04613 | val_0_rmse: 0.21573 | val_1_rmse: 0.21845 |  0:00:10s
epoch 36 | loss: 0.0461  | val_0_rmse: 0.21642 | val_1_rmse: 0.21943 |  0:00:11s
epoch 37 | loss: 0.04641 | val_0_rmse: 0.21498 | val_1_rmse: 0.21783 |  0:00:11s
epoch 38 | loss: 0.04665 | val_0_rmse: 0.21487 | val_1_rmse: 0.21749 |  0:00:11s
epoch 39 | loss: 0.04566 | val_0_rmse: 0.2152  | val_1_rmse: 0.21747 |  0:00:12s
epoch 40 | loss: 0.0454  | val_0_rmse: 0.215   | val_1_rmse: 0.21701 |  0:00:12s
epoch 41 | loss: 0.04524 | val_0_rmse: 0.21466 | val_1_rmse: 0.21672 |  0:00:12s
epoch 42 | loss: 0.04564 | val_0_rmse: 0.21411 | val_1_rmse: 0.21665 |  0:00:12s
epoch 43 | loss: 0.04568 | val_0_rmse: 0.21685 | val_1_rmse: 0.22003 |  0:00:13s
epoch 44 | loss: 0.04564 | val_0_rmse: 0.21528 | val_1_rmse: 0.21905 |  0:00:13s
epoch 45 | loss: 0.04493 | val_0_rmse: 0.21378 | val_1_rmse: 0.21735 |  0:00:13s
epoch 46 | loss: 0.045   | val_0_rmse: 0.21357 | val_1_rmse: 0.21667 |  0:00:14s
epoch 47 | loss: 0.0453  | val_0_rmse: 0.21498 | val_1_rmse: 0.21925 |  0:00:14s
epoch 48 | loss: 0.04625 | val_0_rmse: 0.21513 | val_1_rmse: 0.21891 |  0:00:14s
epoch 49 | loss: 0.04621 | val_0_rmse: 0.21465 | val_1_rmse: 0.21803 |  0:00:15s
epoch 50 | loss: 0.04556 | val_0_rmse: 0.21454 | val_1_rmse: 0.21803 |  0:00:15s
epoch 51 | loss: 0.0451  | val_0_rmse: 0.21868 | val_1_rmse: 0.22018 |  0:00:15s
epoch 52 | loss: 0.04577 | val_0_rmse: 0.22869 | val_1_rmse: 0.232   |  0:00:16s
epoch 53 | loss: 0.04765 | val_0_rmse: 0.21625 | val_1_rmse: 0.21918 |  0:00:16s
epoch 54 | loss: 0.04502 | val_0_rmse: 0.21294 | val_1_rmse: 0.21725 |  0:00:16s
epoch 55 | loss: 0.04506 | val_0_rmse: 0.21381 | val_1_rmse: 0.21881 |  0:00:16s
epoch 56 | loss: 0.04543 | val_0_rmse: 0.21301 | val_1_rmse: 0.21856 |  0:00:17s
epoch 57 | loss: 0.04588 | val_0_rmse: 0.21259 | val_1_rmse: 0.21887 |  0:00:17s
epoch 58 | loss: 0.04416 | val_0_rmse: 0.21318 | val_1_rmse: 0.21994 |  0:00:17s
epoch 59 | loss: 0.04409 | val_0_rmse: 0.21093 | val_1_rmse: 0.21821 |  0:00:18s
epoch 60 | loss: 0.04556 | val_0_rmse: 0.21237 | val_1_rmse: 0.21833 |  0:00:18s
epoch 61 | loss: 0.04571 | val_0_rmse: 0.21417 | val_1_rmse: 0.21839 |  0:00:18s
epoch 62 | loss: 0.04582 | val_0_rmse: 0.21236 | val_1_rmse: 0.21616 |  0:00:19s
epoch 63 | loss: 0.04483 | val_0_rmse: 0.21984 | val_1_rmse: 0.2247  |  0:00:19s
epoch 64 | loss: 0.04601 | val_0_rmse: 0.21585 | val_1_rmse: 0.2226  |  0:00:19s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.21586
Best weights from best epoch are automatically used!
ended training at: 21:51:48
Feature importance:
Mean squared error is of 0.046016438088608974
Mean absolute error:0.1578045482804112
MAPE:0.16674174942225486
R2 score:0.2723418502963896
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:51:48
epoch 0  | loss: 1.73327 | val_0_rmse: 0.30546 | val_1_rmse: 0.30717 |  0:00:00s
epoch 1  | loss: 0.30142 | val_0_rmse: 0.25773 | val_1_rmse: 0.2645  |  0:00:00s
epoch 2  | loss: 0.12831 | val_0_rmse: 0.284   | val_1_rmse: 0.29044 |  0:00:00s
epoch 3  | loss: 0.08425 | val_0_rmse: 0.28961 | val_1_rmse: 0.29737 |  0:00:01s
epoch 4  | loss: 0.06642 | val_0_rmse: 0.25206 | val_1_rmse: 0.25975 |  0:00:01s
epoch 5  | loss: 0.06108 | val_0_rmse: 0.22666 | val_1_rmse: 0.23367 |  0:00:01s
epoch 6  | loss: 0.05521 | val_0_rmse: 0.22198 | val_1_rmse: 0.22696 |  0:00:02s
epoch 7  | loss: 0.05526 | val_0_rmse: 0.22525 | val_1_rmse: 0.23144 |  0:00:02s
epoch 8  | loss: 0.05292 | val_0_rmse: 0.22223 | val_1_rmse: 0.22821 |  0:00:02s
epoch 9  | loss: 0.05181 | val_0_rmse: 0.21868 | val_1_rmse: 0.2226  |  0:00:02s
epoch 10 | loss: 0.05136 | val_0_rmse: 0.21828 | val_1_rmse: 0.22358 |  0:00:03s
epoch 11 | loss: 0.05082 | val_0_rmse: 0.2203  | val_1_rmse: 0.22658 |  0:00:03s
epoch 12 | loss: 0.05053 | val_0_rmse: 0.22479 | val_1_rmse: 0.23181 |  0:00:04s
epoch 13 | loss: 0.05013 | val_0_rmse: 0.23077 | val_1_rmse: 0.23845 |  0:00:04s
epoch 14 | loss: 0.05055 | val_0_rmse: 0.2201  | val_1_rmse: 0.22665 |  0:00:04s
epoch 15 | loss: 0.04928 | val_0_rmse: 0.22026 | val_1_rmse: 0.2267  |  0:00:04s
epoch 16 | loss: 0.05218 | val_0_rmse: 0.22096 | val_1_rmse: 0.22833 |  0:00:05s
epoch 17 | loss: 0.05064 | val_0_rmse: 0.23619 | val_1_rmse: 0.24475 |  0:00:05s
epoch 18 | loss: 0.0508  | val_0_rmse: 0.22435 | val_1_rmse: 0.23251 |  0:00:05s
epoch 19 | loss: 0.05028 | val_0_rmse: 0.22195 | val_1_rmse: 0.22817 |  0:00:06s
epoch 20 | loss: 0.05065 | val_0_rmse: 0.21945 | val_1_rmse: 0.22645 |  0:00:06s
epoch 21 | loss: 0.05066 | val_0_rmse: 0.23475 | val_1_rmse: 0.24348 |  0:00:06s
epoch 22 | loss: 0.05085 | val_0_rmse: 0.22564 | val_1_rmse: 0.23386 |  0:00:06s
epoch 23 | loss: 0.04952 | val_0_rmse: 0.22045 | val_1_rmse: 0.22708 |  0:00:07s
epoch 24 | loss: 0.0492  | val_0_rmse: 0.2217  | val_1_rmse: 0.22974 |  0:00:07s
epoch 25 | loss: 0.04829 | val_0_rmse: 0.22781 | val_1_rmse: 0.2362  |  0:00:07s
epoch 26 | loss: 0.04838 | val_0_rmse: 0.22156 | val_1_rmse: 0.22925 |  0:00:08s
epoch 27 | loss: 0.04775 | val_0_rmse: 0.22068 | val_1_rmse: 0.22898 |  0:00:08s
epoch 28 | loss: 0.04947 | val_0_rmse: 0.22587 | val_1_rmse: 0.23431 |  0:00:08s
epoch 29 | loss: 0.04785 | val_0_rmse: 0.2204  | val_1_rmse: 0.22775 |  0:00:09s
epoch 30 | loss: 0.04698 | val_0_rmse: 0.21888 | val_1_rmse: 0.22623 |  0:00:09s
epoch 31 | loss: 0.04683 | val_0_rmse: 0.22104 | val_1_rmse: 0.22915 |  0:00:09s
epoch 32 | loss: 0.04637 | val_0_rmse: 0.2222  | val_1_rmse: 0.23064 |  0:00:09s
epoch 33 | loss: 0.05196 | val_0_rmse: 0.22065 | val_1_rmse: 0.2288  |  0:00:10s
epoch 34 | loss: 0.04786 | val_0_rmse: 0.2206  | val_1_rmse: 0.22821 |  0:00:10s
epoch 35 | loss: 0.04714 | val_0_rmse: 0.22114 | val_1_rmse: 0.22653 |  0:00:10s
epoch 36 | loss: 0.0469  | val_0_rmse: 0.21735 | val_1_rmse: 0.22179 |  0:00:11s
epoch 37 | loss: 0.04677 | val_0_rmse: 0.21684 | val_1_rmse: 0.22169 |  0:00:11s
epoch 38 | loss: 0.04615 | val_0_rmse: 0.23096 | val_1_rmse: 0.2383  |  0:00:11s
epoch 39 | loss: 0.04894 | val_0_rmse: 0.22242 | val_1_rmse: 0.22885 |  0:00:12s
epoch 40 | loss: 0.0496  | val_0_rmse: 0.22729 | val_1_rmse: 0.22849 |  0:00:12s
epoch 41 | loss: 0.05104 | val_0_rmse: 0.2161  | val_1_rmse: 0.22027 |  0:00:12s
epoch 42 | loss: 0.04879 | val_0_rmse: 0.22174 | val_1_rmse: 0.22902 |  0:00:12s
epoch 43 | loss: 0.04831 | val_0_rmse: 0.21563 | val_1_rmse: 0.22012 |  0:00:13s
epoch 44 | loss: 0.04865 | val_0_rmse: 0.21689 | val_1_rmse: 0.21961 |  0:00:13s
epoch 45 | loss: 0.04693 | val_0_rmse: 0.21439 | val_1_rmse: 0.21936 |  0:00:13s
epoch 46 | loss: 0.04815 | val_0_rmse: 0.21386 | val_1_rmse: 0.22098 |  0:00:14s
epoch 47 | loss: 0.04738 | val_0_rmse: 0.21187 | val_1_rmse: 0.21954 |  0:00:14s
epoch 48 | loss: 0.04777 | val_0_rmse: 0.21236 | val_1_rmse: 0.21817 |  0:00:14s
epoch 49 | loss: 0.04771 | val_0_rmse: 0.21192 | val_1_rmse: 0.21821 |  0:00:15s
epoch 50 | loss: 0.04673 | val_0_rmse: 0.22392 | val_1_rmse: 0.23327 |  0:00:15s
epoch 51 | loss: 0.04913 | val_0_rmse: 0.2153  | val_1_rmse: 0.22436 |  0:00:15s
epoch 52 | loss: 0.04741 | val_0_rmse: 0.21428 | val_1_rmse: 0.22041 |  0:00:16s
epoch 53 | loss: 0.0469  | val_0_rmse: 0.21259 | val_1_rmse: 0.21861 |  0:00:16s
epoch 54 | loss: 0.0473  | val_0_rmse: 0.21393 | val_1_rmse: 0.22208 |  0:00:16s
epoch 55 | loss: 0.04624 | val_0_rmse: 0.21207 | val_1_rmse: 0.22005 |  0:00:16s
epoch 56 | loss: 0.04593 | val_0_rmse: 0.21011 | val_1_rmse: 0.21785 |  0:00:17s
epoch 57 | loss: 0.04557 | val_0_rmse: 0.21079 | val_1_rmse: 0.21812 |  0:00:17s
epoch 58 | loss: 0.04546 | val_0_rmse: 0.21446 | val_1_rmse: 0.22283 |  0:00:17s
epoch 59 | loss: 0.0448  | val_0_rmse: 0.20858 | val_1_rmse: 0.21524 |  0:00:18s
epoch 60 | loss: 0.04398 | val_0_rmse: 0.20745 | val_1_rmse: 0.21444 |  0:00:18s
epoch 61 | loss: 0.04409 | val_0_rmse: 0.21136 | val_1_rmse: 0.22061 |  0:00:18s
epoch 62 | loss: 0.04419 | val_0_rmse: 0.20804 | val_1_rmse: 0.21716 |  0:00:19s
epoch 63 | loss: 0.0436  | val_0_rmse: 0.20842 | val_1_rmse: 0.21689 |  0:00:19s
epoch 64 | loss: 0.04367 | val_0_rmse: 0.21455 | val_1_rmse: 0.22417 |  0:00:19s
epoch 65 | loss: 0.04306 | val_0_rmse: 0.20561 | val_1_rmse: 0.21368 |  0:00:19s
epoch 66 | loss: 0.04275 | val_0_rmse: 0.20537 | val_1_rmse: 0.21324 |  0:00:20s
epoch 67 | loss: 0.04284 | val_0_rmse: 0.207   | val_1_rmse: 0.21626 |  0:00:20s
epoch 68 | loss: 0.04223 | val_0_rmse: 0.20916 | val_1_rmse: 0.21876 |  0:00:20s
epoch 69 | loss: 0.04295 | val_0_rmse: 0.20442 | val_1_rmse: 0.21389 |  0:00:21s
epoch 70 | loss: 0.0417  | val_0_rmse: 0.2084  | val_1_rmse: 0.21881 |  0:00:21s
epoch 71 | loss: 0.04205 | val_0_rmse: 0.20441 | val_1_rmse: 0.2143  |  0:00:21s
epoch 72 | loss: 0.04167 | val_0_rmse: 0.20695 | val_1_rmse: 0.2172  |  0:00:22s
epoch 73 | loss: 0.04378 | val_0_rmse: 0.20435 | val_1_rmse: 0.21438 |  0:00:22s
epoch 74 | loss: 0.04281 | val_0_rmse: 0.20326 | val_1_rmse: 0.21168 |  0:00:22s
epoch 75 | loss: 0.04283 | val_0_rmse: 0.20553 | val_1_rmse: 0.21654 |  0:00:22s
epoch 76 | loss: 0.04326 | val_0_rmse: 0.20589 | val_1_rmse: 0.21625 |  0:00:23s
epoch 77 | loss: 0.04296 | val_0_rmse: 0.21269 | val_1_rmse: 0.22328 |  0:00:23s
epoch 78 | loss: 0.04442 | val_0_rmse: 0.20748 | val_1_rmse: 0.21801 |  0:00:23s
epoch 79 | loss: 0.04348 | val_0_rmse: 0.2138  | val_1_rmse: 0.22225 |  0:00:24s
epoch 80 | loss: 0.04471 | val_0_rmse: 0.20753 | val_1_rmse: 0.21752 |  0:00:24s
epoch 81 | loss: 0.04229 | val_0_rmse: 0.21379 | val_1_rmse: 0.22483 |  0:00:24s
epoch 82 | loss: 0.04351 | val_0_rmse: 0.20749 | val_1_rmse: 0.21533 |  0:00:25s
epoch 83 | loss: 0.04363 | val_0_rmse: 0.20882 | val_1_rmse: 0.2137  |  0:00:25s
epoch 84 | loss: 0.04342 | val_0_rmse: 0.20643 | val_1_rmse: 0.21697 |  0:00:25s
epoch 85 | loss: 0.04323 | val_0_rmse: 0.21379 | val_1_rmse: 0.22464 |  0:00:25s
epoch 86 | loss: 0.04238 | val_0_rmse: 0.20568 | val_1_rmse: 0.21568 |  0:00:26s
epoch 87 | loss: 0.04356 | val_0_rmse: 0.20759 | val_1_rmse: 0.2174  |  0:00:26s
epoch 88 | loss: 0.04292 | val_0_rmse: 0.20357 | val_1_rmse: 0.21157 |  0:00:26s
epoch 89 | loss: 0.04418 | val_0_rmse: 0.20439 | val_1_rmse: 0.21049 |  0:00:27s
epoch 90 | loss: 0.04319 | val_0_rmse: 0.21274 | val_1_rmse: 0.22113 |  0:00:27s
epoch 91 | loss: 0.04362 | val_0_rmse: 0.20196 | val_1_rmse: 0.21087 |  0:00:27s
epoch 92 | loss: 0.0438  | val_0_rmse: 0.20359 | val_1_rmse: 0.21188 |  0:00:28s
epoch 93 | loss: 0.04342 | val_0_rmse: 0.20216 | val_1_rmse: 0.21346 |  0:00:28s
epoch 94 | loss: 0.04101 | val_0_rmse: 0.20735 | val_1_rmse: 0.21919 |  0:00:28s
epoch 95 | loss: 0.04115 | val_0_rmse: 0.20124 | val_1_rmse: 0.21089 |  0:00:29s
epoch 96 | loss: 0.04081 | val_0_rmse: 0.2011  | val_1_rmse: 0.21214 |  0:00:29s
epoch 97 | loss: 0.03991 | val_0_rmse: 0.20006 | val_1_rmse: 0.21211 |  0:00:29s
epoch 98 | loss: 0.04086 | val_0_rmse: 0.20192 | val_1_rmse: 0.21514 |  0:00:29s
epoch 99 | loss: 0.03922 | val_0_rmse: 0.19914 | val_1_rmse: 0.21145 |  0:00:30s
epoch 100| loss: 0.0398  | val_0_rmse: 0.19859 | val_1_rmse: 0.21025 |  0:00:30s
epoch 101| loss: 0.04047 | val_0_rmse: 0.19905 | val_1_rmse: 0.21007 |  0:00:30s
epoch 102| loss: 0.04029 | val_0_rmse: 0.20005 | val_1_rmse: 0.21097 |  0:00:31s
epoch 103| loss: 0.04051 | val_0_rmse: 0.20977 | val_1_rmse: 0.22153 |  0:00:31s
epoch 104| loss: 0.04134 | val_0_rmse: 0.19898 | val_1_rmse: 0.20918 |  0:00:31s
epoch 105| loss: 0.04059 | val_0_rmse: 0.19776 | val_1_rmse: 0.20656 |  0:00:32s
epoch 106| loss: 0.03969 | val_0_rmse: 0.19899 | val_1_rmse: 0.20984 |  0:00:32s
epoch 107| loss: 0.04038 | val_0_rmse: 0.19785 | val_1_rmse: 0.20916 |  0:00:32s
epoch 108| loss: 0.03936 | val_0_rmse: 0.19772 | val_1_rmse: 0.2079  |  0:00:32s
epoch 109| loss: 0.03997 | val_0_rmse: 0.19786 | val_1_rmse: 0.2093  |  0:00:33s
epoch 110| loss: 0.03942 | val_0_rmse: 0.19985 | val_1_rmse: 0.21204 |  0:00:33s
epoch 111| loss: 0.03954 | val_0_rmse: 0.19916 | val_1_rmse: 0.21073 |  0:00:33s
epoch 112| loss: 0.03928 | val_0_rmse: 0.19662 | val_1_rmse: 0.20611 |  0:00:34s
epoch 113| loss: 0.0394  | val_0_rmse: 0.19732 | val_1_rmse: 0.20506 |  0:00:34s
epoch 114| loss: 0.0393  | val_0_rmse: 0.19681 | val_1_rmse: 0.20544 |  0:00:34s
epoch 115| loss: 0.03898 | val_0_rmse: 0.19635 | val_1_rmse: 0.20727 |  0:00:35s
epoch 116| loss: 0.03957 | val_0_rmse: 0.20246 | val_1_rmse: 0.21607 |  0:00:35s
epoch 117| loss: 0.04023 | val_0_rmse: 0.19692 | val_1_rmse: 0.20605 |  0:00:35s
epoch 118| loss: 0.03981 | val_0_rmse: 0.19663 | val_1_rmse: 0.20655 |  0:00:35s
epoch 119| loss: 0.03931 | val_0_rmse: 0.19605 | val_1_rmse: 0.20695 |  0:00:36s
epoch 120| loss: 0.03875 | val_0_rmse: 0.19615 | val_1_rmse: 0.20674 |  0:00:36s
epoch 121| loss: 0.03899 | val_0_rmse: 0.19587 | val_1_rmse: 0.20793 |  0:00:36s
epoch 122| loss: 0.03884 | val_0_rmse: 0.19774 | val_1_rmse: 0.2076  |  0:00:37s
epoch 123| loss: 0.03892 | val_0_rmse: 0.19541 | val_1_rmse: 0.20597 |  0:00:37s
epoch 124| loss: 0.03878 | val_0_rmse: 0.19583 | val_1_rmse: 0.2096  |  0:00:37s
epoch 125| loss: 0.03942 | val_0_rmse: 0.19483 | val_1_rmse: 0.20717 |  0:00:38s
epoch 126| loss: 0.03849 | val_0_rmse: 0.19507 | val_1_rmse: 0.20436 |  0:00:38s
epoch 127| loss: 0.03908 | val_0_rmse: 0.19592 | val_1_rmse: 0.20718 |  0:00:38s
epoch 128| loss: 0.03926 | val_0_rmse: 0.19543 | val_1_rmse: 0.20676 |  0:00:38s
epoch 129| loss: 0.03912 | val_0_rmse: 0.2012  | val_1_rmse: 0.21322 |  0:00:39s
epoch 130| loss: 0.03879 | val_0_rmse: 0.1956  | val_1_rmse: 0.20722 |  0:00:39s
epoch 131| loss: 0.0392  | val_0_rmse: 0.19257 | val_1_rmse: 0.20398 |  0:00:39s
epoch 132| loss: 0.03835 | val_0_rmse: 0.19524 | val_1_rmse: 0.20485 |  0:00:40s
epoch 133| loss: 0.03872 | val_0_rmse: 0.19427 | val_1_rmse: 0.20655 |  0:00:40s
epoch 134| loss: 0.03817 | val_0_rmse: 0.2005  | val_1_rmse: 0.21398 |  0:00:40s
epoch 135| loss: 0.03819 | val_0_rmse: 0.19264 | val_1_rmse: 0.20297 |  0:00:41s
epoch 136| loss: 0.03824 | val_0_rmse: 0.19511 | val_1_rmse: 0.20156 |  0:00:41s
epoch 137| loss: 0.0375  | val_0_rmse: 0.19423 | val_1_rmse: 0.20585 |  0:00:41s
epoch 138| loss: 0.03838 | val_0_rmse: 0.19485 | val_1_rmse: 0.20446 |  0:00:42s
epoch 139| loss: 0.03874 | val_0_rmse: 0.19393 | val_1_rmse: 0.20265 |  0:00:42s
epoch 140| loss: 0.03791 | val_0_rmse: 0.1914  | val_1_rmse: 0.20253 |  0:00:42s
epoch 141| loss: 0.03811 | val_0_rmse: 0.19902 | val_1_rmse: 0.21274 |  0:00:42s
epoch 142| loss: 0.04033 | val_0_rmse: 0.1923  | val_1_rmse: 0.20403 |  0:00:43s
epoch 143| loss: 0.03924 | val_0_rmse: 0.19631 | val_1_rmse: 0.2053  |  0:00:43s
epoch 144| loss: 0.03913 | val_0_rmse: 0.19005 | val_1_rmse: 0.20272 |  0:00:43s
epoch 145| loss: 0.03726 | val_0_rmse: 0.19044 | val_1_rmse: 0.20451 |  0:00:44s
epoch 146| loss: 0.03857 | val_0_rmse: 0.19034 | val_1_rmse: 0.20455 |  0:00:44s
epoch 147| loss: 0.03776 | val_0_rmse: 0.18974 | val_1_rmse: 0.20169 |  0:00:44s
epoch 148| loss: 0.03728 | val_0_rmse: 0.19063 | val_1_rmse: 0.20194 |  0:00:45s
epoch 149| loss: 0.03682 | val_0_rmse: 0.1911  | val_1_rmse: 0.20599 |  0:00:45s
Stop training because you reached max_epochs = 150 with best_epoch = 136 and best_val_1_rmse = 0.20156
Best weights from best epoch are automatically used!
ended training at: 21:52:34
Feature importance:
Mean squared error is of 0.04911893508689179
Mean absolute error:0.15986860758398472
MAPE:0.1694590909741342
R2 score:0.25551908264183554
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:52:34
epoch 0  | loss: 1.57935 | val_0_rmse: 0.49496 | val_1_rmse: 0.4754  |  0:00:00s
epoch 1  | loss: 0.53056 | val_0_rmse: 0.38341 | val_1_rmse: 0.35982 |  0:00:01s
epoch 2  | loss: 0.19799 | val_0_rmse: 0.33483 | val_1_rmse: 0.3095  |  0:00:01s
epoch 3  | loss: 0.12323 | val_0_rmse: 0.35668 | val_1_rmse: 0.33274 |  0:00:02s
epoch 4  | loss: 0.1112  | val_0_rmse: 0.30979 | val_1_rmse: 0.28762 |  0:00:03s
epoch 5  | loss: 0.1079  | val_0_rmse: 0.30959 | val_1_rmse: 0.28509 |  0:00:03s
epoch 6  | loss: 0.10144 | val_0_rmse: 0.31154 | val_1_rmse: 0.28609 |  0:00:04s
epoch 7  | loss: 0.09739 | val_0_rmse: 0.30995 | val_1_rmse: 0.2848  |  0:00:04s
epoch 8  | loss: 0.09791 | val_0_rmse: 0.30902 | val_1_rmse: 0.28398 |  0:00:05s
epoch 9  | loss: 0.09763 | val_0_rmse: 0.30918 | val_1_rmse: 0.28472 |  0:00:06s
epoch 10 | loss: 0.09676 | val_0_rmse: 0.30792 | val_1_rmse: 0.28313 |  0:00:06s
epoch 11 | loss: 0.09736 | val_0_rmse: 0.30694 | val_1_rmse: 0.28252 |  0:00:07s
epoch 12 | loss: 0.09524 | val_0_rmse: 0.30956 | val_1_rmse: 0.28537 |  0:00:08s
epoch 13 | loss: 0.09535 | val_0_rmse: 0.30674 | val_1_rmse: 0.28241 |  0:00:08s
epoch 14 | loss: 0.09607 | val_0_rmse: 0.30657 | val_1_rmse: 0.28219 |  0:00:09s
epoch 15 | loss: 0.09411 | val_0_rmse: 0.30587 | val_1_rmse: 0.28136 |  0:00:09s
epoch 16 | loss: 0.09548 | val_0_rmse: 0.30752 | val_1_rmse: 0.28293 |  0:00:10s
epoch 17 | loss: 0.09525 | val_0_rmse: 0.30543 | val_1_rmse: 0.28125 |  0:00:11s
epoch 18 | loss: 0.0955  | val_0_rmse: 0.30536 | val_1_rmse: 0.2814  |  0:00:11s
epoch 19 | loss: 0.09534 | val_0_rmse: 0.31192 | val_1_rmse: 0.28691 |  0:00:12s
epoch 20 | loss: 0.09555 | val_0_rmse: 0.30494 | val_1_rmse: 0.28031 |  0:00:12s
epoch 21 | loss: 0.09608 | val_0_rmse: 0.30526 | val_1_rmse: 0.28085 |  0:00:13s
epoch 22 | loss: 0.0953  | val_0_rmse: 0.30805 | val_1_rmse: 0.28313 |  0:00:14s
epoch 23 | loss: 0.09636 | val_0_rmse: 0.31699 | val_1_rmse: 0.29252 |  0:00:14s
epoch 24 | loss: 0.09845 | val_0_rmse: 0.30533 | val_1_rmse: 0.28068 |  0:00:15s
epoch 25 | loss: 0.09629 | val_0_rmse: 0.30476 | val_1_rmse: 0.2808  |  0:00:15s
epoch 26 | loss: 0.09387 | val_0_rmse: 0.30461 | val_1_rmse: 0.28035 |  0:00:16s
epoch 27 | loss: 0.09402 | val_0_rmse: 0.30503 | val_1_rmse: 0.28034 |  0:00:17s
epoch 28 | loss: 0.09443 | val_0_rmse: 0.30593 | val_1_rmse: 0.2813  |  0:00:17s
epoch 29 | loss: 0.09464 | val_0_rmse: 0.30713 | val_1_rmse: 0.28234 |  0:00:18s
epoch 30 | loss: 0.09542 | val_0_rmse: 0.30496 | val_1_rmse: 0.28072 |  0:00:19s
epoch 31 | loss: 0.09457 | val_0_rmse: 0.30566 | val_1_rmse: 0.28062 |  0:00:19s
epoch 32 | loss: 0.09514 | val_0_rmse: 0.30486 | val_1_rmse: 0.28065 |  0:00:20s
epoch 33 | loss: 0.09473 | val_0_rmse: 0.30579 | val_1_rmse: 0.28131 |  0:00:20s
epoch 34 | loss: 0.09518 | val_0_rmse: 0.3079  | val_1_rmse: 0.28423 |  0:00:21s
epoch 35 | loss: 0.09543 | val_0_rmse: 0.30579 | val_1_rmse: 0.28171 |  0:00:22s
epoch 36 | loss: 0.09454 | val_0_rmse: 0.30564 | val_1_rmse: 0.28099 |  0:00:22s
epoch 37 | loss: 0.09356 | val_0_rmse: 0.30524 | val_1_rmse: 0.28115 |  0:00:23s
epoch 38 | loss: 0.0932  | val_0_rmse: 0.305   | val_1_rmse: 0.28091 |  0:00:23s
epoch 39 | loss: 0.09332 | val_0_rmse: 0.30495 | val_1_rmse: 0.28071 |  0:00:24s
epoch 40 | loss: 0.09305 | val_0_rmse: 0.30539 | val_1_rmse: 0.28032 |  0:00:25s
epoch 41 | loss: 0.09314 | val_0_rmse: 0.3051  | val_1_rmse: 0.2804  |  0:00:25s
epoch 42 | loss: 0.0931  | val_0_rmse: 0.30529 | val_1_rmse: 0.2803  |  0:00:26s
epoch 43 | loss: 0.09294 | val_0_rmse: 0.30438 | val_1_rmse: 0.27942 |  0:00:26s
epoch 44 | loss: 0.09325 | val_0_rmse: 0.30406 | val_1_rmse: 0.2791  |  0:00:27s
epoch 45 | loss: 0.093   | val_0_rmse: 0.30424 | val_1_rmse: 0.27931 |  0:00:28s
epoch 46 | loss: 0.09334 | val_0_rmse: 0.30454 | val_1_rmse: 0.27998 |  0:00:28s
epoch 47 | loss: 0.09294 | val_0_rmse: 0.3047  | val_1_rmse: 0.28069 |  0:00:29s
epoch 48 | loss: 0.09321 | val_0_rmse: 0.30516 | val_1_rmse: 0.28148 |  0:00:30s
epoch 49 | loss: 0.09348 | val_0_rmse: 0.30445 | val_1_rmse: 0.28057 |  0:00:30s
epoch 50 | loss: 0.0929  | val_0_rmse: 0.30521 | val_1_rmse: 0.28089 |  0:00:31s
epoch 51 | loss: 0.09351 | val_0_rmse: 0.30839 | val_1_rmse: 0.28368 |  0:00:31s
epoch 52 | loss: 0.09432 | val_0_rmse: 0.31142 | val_1_rmse: 0.28682 |  0:00:32s
epoch 53 | loss: 0.0941  | val_0_rmse: 0.31043 | val_1_rmse: 0.28588 |  0:00:33s
epoch 54 | loss: 0.09382 | val_0_rmse: 0.30485 | val_1_rmse: 0.28071 |  0:00:33s
epoch 55 | loss: 0.09294 | val_0_rmse: 0.30422 | val_1_rmse: 0.28065 |  0:00:34s
epoch 56 | loss: 0.09362 | val_0_rmse: 0.3042  | val_1_rmse: 0.28058 |  0:00:34s
epoch 57 | loss: 0.09278 | val_0_rmse: 0.3043  | val_1_rmse: 0.28107 |  0:00:35s
epoch 58 | loss: 0.09282 | val_0_rmse: 0.30407 | val_1_rmse: 0.28073 |  0:00:36s
epoch 59 | loss: 0.09302 | val_0_rmse: 0.305   | val_1_rmse: 0.28132 |  0:00:36s
epoch 60 | loss: 0.09341 | val_0_rmse: 0.30415 | val_1_rmse: 0.28132 |  0:00:37s
epoch 61 | loss: 0.09423 | val_0_rmse: 0.30458 | val_1_rmse: 0.28148 |  0:00:37s
epoch 62 | loss: 0.09446 | val_0_rmse: 0.30416 | val_1_rmse: 0.28126 |  0:00:38s
epoch 63 | loss: 0.09398 | val_0_rmse: 0.30478 | val_1_rmse: 0.28281 |  0:00:39s
epoch 64 | loss: 0.0937  | val_0_rmse: 0.30403 | val_1_rmse: 0.28112 |  0:00:39s
epoch 65 | loss: 0.09307 | val_0_rmse: 0.30446 | val_1_rmse: 0.28203 |  0:00:40s
epoch 66 | loss: 0.09365 | val_0_rmse: 0.3051  | val_1_rmse: 0.28202 |  0:00:41s
epoch 67 | loss: 0.09358 | val_0_rmse: 0.30377 | val_1_rmse: 0.28107 |  0:00:41s
epoch 68 | loss: 0.09333 | val_0_rmse: 0.30374 | val_1_rmse: 0.28122 |  0:00:42s
epoch 69 | loss: 0.09388 | val_0_rmse: 0.30466 | val_1_rmse: 0.28147 |  0:00:42s
epoch 70 | loss: 0.09397 | val_0_rmse: 0.30385 | val_1_rmse: 0.28231 |  0:00:43s
epoch 71 | loss: 0.09274 | val_0_rmse: 0.30396 | val_1_rmse: 0.28166 |  0:00:44s
epoch 72 | loss: 0.09286 | val_0_rmse: 0.30404 | val_1_rmse: 0.28164 |  0:00:44s
epoch 73 | loss: 0.09242 | val_0_rmse: 0.30393 | val_1_rmse: 0.2814  |  0:00:45s
epoch 74 | loss: 0.09272 | val_0_rmse: 0.30289 | val_1_rmse: 0.28059 |  0:00:45s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.2791
Best weights from best epoch are automatically used!
ended training at: 21:53:20
Feature importance:
Mean squared error is of 0.09106247936197936
Mean absolute error:0.1968875616602247
MAPE:0.2031676422400798
R2 score:0.006219168979644141
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:53:21
epoch 0  | loss: 1.78573 | val_0_rmse: 0.40973 | val_1_rmse: 0.42751 |  0:00:00s
epoch 1  | loss: 0.38056 | val_0_rmse: 0.36644 | val_1_rmse: 0.38495 |  0:00:01s
epoch 2  | loss: 0.19022 | val_0_rmse: 0.29395 | val_1_rmse: 0.31365 |  0:00:01s
epoch 3  | loss: 0.12917 | val_0_rmse: 0.31818 | val_1_rmse: 0.33772 |  0:00:02s
epoch 4  | loss: 0.1021  | val_0_rmse: 0.29879 | val_1_rmse: 0.32022 |  0:00:03s
epoch 5  | loss: 0.09663 | val_0_rmse: 0.29697 | val_1_rmse: 0.31636 |  0:00:03s
epoch 6  | loss: 0.09287 | val_0_rmse: 0.29503 | val_1_rmse: 0.31459 |  0:00:04s
epoch 7  | loss: 0.09058 | val_0_rmse: 0.29546 | val_1_rmse: 0.31464 |  0:00:04s
epoch 8  | loss: 0.08852 | val_0_rmse: 0.29701 | val_1_rmse: 0.31562 |  0:00:05s
epoch 9  | loss: 0.08896 | val_0_rmse: 0.29571 | val_1_rmse: 0.31503 |  0:00:06s
epoch 10 | loss: 0.08808 | val_0_rmse: 0.29395 | val_1_rmse: 0.31298 |  0:00:06s
epoch 11 | loss: 0.08779 | val_0_rmse: 0.29544 | val_1_rmse: 0.31501 |  0:00:07s
epoch 12 | loss: 0.0877  | val_0_rmse: 0.29631 | val_1_rmse: 0.31496 |  0:00:07s
epoch 13 | loss: 0.08863 | val_0_rmse: 0.29497 | val_1_rmse: 0.3142  |  0:00:08s
epoch 14 | loss: 0.08759 | val_0_rmse: 0.29347 | val_1_rmse: 0.3128  |  0:00:09s
epoch 15 | loss: 0.0876  | val_0_rmse: 0.29596 | val_1_rmse: 0.31571 |  0:00:09s
epoch 16 | loss: 0.08799 | val_0_rmse: 0.29466 | val_1_rmse: 0.31401 |  0:00:10s
epoch 17 | loss: 0.08801 | val_0_rmse: 0.29572 | val_1_rmse: 0.31417 |  0:00:11s
epoch 18 | loss: 0.08835 | val_0_rmse: 0.29883 | val_1_rmse: 0.31691 |  0:00:11s
epoch 19 | loss: 0.08891 | val_0_rmse: 0.29382 | val_1_rmse: 0.31237 |  0:00:12s
epoch 20 | loss: 0.08863 | val_0_rmse: 0.29436 | val_1_rmse: 0.31387 |  0:00:12s
epoch 21 | loss: 0.08734 | val_0_rmse: 0.29433 | val_1_rmse: 0.31341 |  0:00:13s
epoch 22 | loss: 0.08724 | val_0_rmse: 0.2947  | val_1_rmse: 0.3131  |  0:00:14s
epoch 23 | loss: 0.08855 | val_0_rmse: 0.29607 | val_1_rmse: 0.31451 |  0:00:14s
epoch 24 | loss: 0.08704 | val_0_rmse: 0.29488 | val_1_rmse: 0.3141  |  0:00:15s
epoch 25 | loss: 0.08844 | val_0_rmse: 0.29544 | val_1_rmse: 0.31524 |  0:00:15s
epoch 26 | loss: 0.08789 | val_0_rmse: 0.29674 | val_1_rmse: 0.31633 |  0:00:16s
epoch 27 | loss: 0.08804 | val_0_rmse: 0.29411 | val_1_rmse: 0.31321 |  0:00:17s
epoch 28 | loss: 0.08752 | val_0_rmse: 0.29392 | val_1_rmse: 0.31309 |  0:00:17s
epoch 29 | loss: 0.08685 | val_0_rmse: 0.29443 | val_1_rmse: 0.31401 |  0:00:18s
epoch 30 | loss: 0.08681 | val_0_rmse: 0.29406 | val_1_rmse: 0.31295 |  0:00:18s
epoch 31 | loss: 0.0876  | val_0_rmse: 0.2969  | val_1_rmse: 0.31709 |  0:00:19s
epoch 32 | loss: 0.08846 | val_0_rmse: 0.29395 | val_1_rmse: 0.31317 |  0:00:20s
epoch 33 | loss: 0.08775 | val_0_rmse: 0.29306 | val_1_rmse: 0.3122  |  0:00:20s
epoch 34 | loss: 0.0875  | val_0_rmse: 0.29577 | val_1_rmse: 0.31426 |  0:00:21s
epoch 35 | loss: 0.08705 | val_0_rmse: 0.29433 | val_1_rmse: 0.31343 |  0:00:21s
epoch 36 | loss: 0.08669 | val_0_rmse: 0.29238 | val_1_rmse: 0.31133 |  0:00:22s
epoch 37 | loss: 0.08665 | val_0_rmse: 0.29466 | val_1_rmse: 0.3139  |  0:00:23s
epoch 38 | loss: 0.08646 | val_0_rmse: 0.29342 | val_1_rmse: 0.31248 |  0:00:23s
epoch 39 | loss: 0.08669 | val_0_rmse: 0.29422 | val_1_rmse: 0.31444 |  0:00:24s
epoch 40 | loss: 0.08618 | val_0_rmse: 0.29624 | val_1_rmse: 0.3161  |  0:00:24s
epoch 41 | loss: 0.08822 | val_0_rmse: 0.29365 | val_1_rmse: 0.31311 |  0:00:25s
epoch 42 | loss: 0.08677 | val_0_rmse: 0.29673 | val_1_rmse: 0.31527 |  0:00:26s
epoch 43 | loss: 0.08778 | val_0_rmse: 0.2951  | val_1_rmse: 0.3137  |  0:00:26s
epoch 44 | loss: 0.08814 | val_0_rmse: 0.29329 | val_1_rmse: 0.313   |  0:00:27s
epoch 45 | loss: 0.08768 | val_0_rmse: 0.29331 | val_1_rmse: 0.3136  |  0:00:28s
epoch 46 | loss: 0.08593 | val_0_rmse: 0.29113 | val_1_rmse: 0.3107  |  0:00:28s
epoch 47 | loss: 0.08615 | val_0_rmse: 0.29406 | val_1_rmse: 0.31392 |  0:00:29s
epoch 48 | loss: 0.08652 | val_0_rmse: 0.29426 | val_1_rmse: 0.31451 |  0:00:29s
epoch 49 | loss: 0.08555 | val_0_rmse: 0.29066 | val_1_rmse: 0.31019 |  0:00:30s
epoch 50 | loss: 0.08482 | val_0_rmse: 0.29053 | val_1_rmse: 0.31061 |  0:00:31s
epoch 51 | loss: 0.08503 | val_0_rmse: 0.28958 | val_1_rmse: 0.30859 |  0:00:31s
epoch 52 | loss: 0.08486 | val_0_rmse: 0.28947 | val_1_rmse: 0.30899 |  0:00:32s
epoch 53 | loss: 0.08455 | val_0_rmse: 0.28757 | val_1_rmse: 0.30687 |  0:00:32s
epoch 54 | loss: 0.08384 | val_0_rmse: 0.28846 | val_1_rmse: 0.30713 |  0:00:33s
epoch 55 | loss: 0.08403 | val_0_rmse: 0.28762 | val_1_rmse: 0.30692 |  0:00:34s
epoch 56 | loss: 0.08362 | val_0_rmse: 0.28729 | val_1_rmse: 0.30613 |  0:00:34s
epoch 57 | loss: 0.08347 | val_0_rmse: 0.28478 | val_1_rmse: 0.30343 |  0:00:35s
epoch 58 | loss: 0.0827  | val_0_rmse: 0.28453 | val_1_rmse: 0.30333 |  0:00:35s
epoch 59 | loss: 0.08246 | val_0_rmse: 0.28623 | val_1_rmse: 0.30515 |  0:00:36s
epoch 60 | loss: 0.08189 | val_0_rmse: 0.2841  | val_1_rmse: 0.30282 |  0:00:37s
epoch 61 | loss: 0.08182 | val_0_rmse: 0.28433 | val_1_rmse: 0.30256 |  0:00:37s
epoch 62 | loss: 0.0826  | val_0_rmse: 0.28512 | val_1_rmse: 0.30317 |  0:00:38s
epoch 63 | loss: 0.08143 | val_0_rmse: 0.28412 | val_1_rmse: 0.30245 |  0:00:39s
epoch 64 | loss: 0.08135 | val_0_rmse: 0.28339 | val_1_rmse: 0.30233 |  0:00:39s
epoch 65 | loss: 0.08161 | val_0_rmse: 0.28512 | val_1_rmse: 0.30507 |  0:00:40s
epoch 66 | loss: 0.08131 | val_0_rmse: 0.2859  | val_1_rmse: 0.30653 |  0:00:40s
epoch 67 | loss: 0.08148 | val_0_rmse: 0.29045 | val_1_rmse: 0.31172 |  0:00:41s
epoch 68 | loss: 0.08052 | val_0_rmse: 0.28808 | val_1_rmse: 0.30931 |  0:00:42s
epoch 69 | loss: 0.07952 | val_0_rmse: 0.29257 | val_1_rmse: 0.31417 |  0:00:42s
epoch 70 | loss: 0.08104 | val_0_rmse: 0.28952 | val_1_rmse: 0.31115 |  0:00:43s
epoch 71 | loss: 0.0798  | val_0_rmse: 0.28502 | val_1_rmse: 0.30762 |  0:00:44s
epoch 72 | loss: 0.07976 | val_0_rmse: 0.28134 | val_1_rmse: 0.30485 |  0:00:44s
epoch 73 | loss: 0.07958 | val_0_rmse: 0.28092 | val_1_rmse: 0.30542 |  0:00:45s
epoch 74 | loss: 0.07969 | val_0_rmse: 0.28006 | val_1_rmse: 0.30656 |  0:00:45s
epoch 75 | loss: 0.07919 | val_0_rmse: 0.27963 | val_1_rmse: 0.30651 |  0:00:46s
epoch 76 | loss: 0.07757 | val_0_rmse: 0.27943 | val_1_rmse: 0.30601 |  0:00:47s
epoch 77 | loss: 0.0776  | val_0_rmse: 0.27949 | val_1_rmse: 0.30621 |  0:00:47s
epoch 78 | loss: 0.0777  | val_0_rmse: 0.27648 | val_1_rmse: 0.30475 |  0:00:48s
epoch 79 | loss: 0.0785  | val_0_rmse: 0.27446 | val_1_rmse: 0.30515 |  0:00:48s
epoch 80 | loss: 0.07859 | val_0_rmse: 0.27797 | val_1_rmse: 0.30561 |  0:00:49s
epoch 81 | loss: 0.07817 | val_0_rmse: 0.27511 | val_1_rmse: 0.30407 |  0:00:50s
epoch 82 | loss: 0.07709 | val_0_rmse: 0.27671 | val_1_rmse: 0.30806 |  0:00:50s
epoch 83 | loss: 0.07771 | val_0_rmse: 0.27175 | val_1_rmse: 0.30724 |  0:00:51s
epoch 84 | loss: 0.07628 | val_0_rmse: 0.26937 | val_1_rmse: 0.30602 |  0:00:51s
epoch 85 | loss: 0.07509 | val_0_rmse: 0.26955 | val_1_rmse: 0.30886 |  0:00:52s
epoch 86 | loss: 0.07486 | val_0_rmse: 0.30782 | val_1_rmse: 0.308   |  0:00:53s
epoch 87 | loss: 0.07887 | val_0_rmse: 0.32747 | val_1_rmse: 0.31034 |  0:00:53s
epoch 88 | loss: 0.0777  | val_0_rmse: 0.27304 | val_1_rmse: 0.30502 |  0:00:54s
epoch 89 | loss: 0.07467 | val_0_rmse: 0.281   | val_1_rmse: 0.30957 |  0:00:54s
epoch 90 | loss: 0.0807  | val_0_rmse: 0.28014 | val_1_rmse: 0.30546 |  0:00:55s
epoch 91 | loss: 0.0805  | val_0_rmse: 0.2809  | val_1_rmse: 0.30734 |  0:00:56s
epoch 92 | loss: 0.0797  | val_0_rmse: 0.2791  | val_1_rmse: 0.3053  |  0:00:56s
epoch 93 | loss: 0.07863 | val_0_rmse: 0.27983 | val_1_rmse: 0.30768 |  0:00:57s
epoch 94 | loss: 0.07927 | val_0_rmse: 0.27921 | val_1_rmse: 0.30607 |  0:00:58s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.30233
Best weights from best epoch are automatically used!
ended training at: 21:54:19
Feature importance:
Mean squared error is of 0.09749310798080081
Mean absolute error:0.1930816767565057
MAPE:0.20477577945424452
R2 score:0.054728125869012434
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:54:20
epoch 0  | loss: 1.70326 | val_0_rmse: 0.33146 | val_1_rmse: 0.29718 |  0:00:00s
epoch 1  | loss: 0.40035 | val_0_rmse: 0.31555 | val_1_rmse: 0.2797  |  0:00:01s
epoch 2  | loss: 0.22183 | val_0_rmse: 0.34915 | val_1_rmse: 0.31746 |  0:00:01s
epoch 3  | loss: 0.14757 | val_0_rmse: 0.33194 | val_1_rmse: 0.30028 |  0:00:02s
epoch 4  | loss: 0.12017 | val_0_rmse: 0.32366 | val_1_rmse: 0.28982 |  0:00:03s
epoch 5  | loss: 0.11312 | val_0_rmse: 0.3199  | val_1_rmse: 0.28465 |  0:00:03s
epoch 6  | loss: 0.1062  | val_0_rmse: 0.31715 | val_1_rmse: 0.2813  |  0:00:04s
epoch 7  | loss: 0.10593 | val_0_rmse: 0.31686 | val_1_rmse: 0.2808  |  0:00:04s
epoch 8  | loss: 0.10362 | val_0_rmse: 0.32334 | val_1_rmse: 0.2882  |  0:00:05s
epoch 9  | loss: 0.10344 | val_0_rmse: 0.31902 | val_1_rmse: 0.28401 |  0:00:06s
epoch 10 | loss: 0.10635 | val_0_rmse: 0.31883 | val_1_rmse: 0.28359 |  0:00:06s
epoch 11 | loss: 0.10211 | val_0_rmse: 0.31962 | val_1_rmse: 0.28443 |  0:00:07s
epoch 12 | loss: 0.10381 | val_0_rmse: 0.31641 | val_1_rmse: 0.28134 |  0:00:07s
epoch 13 | loss: 0.10215 | val_0_rmse: 0.31896 | val_1_rmse: 0.28379 |  0:00:08s
epoch 14 | loss: 0.10105 | val_0_rmse: 0.3169  | val_1_rmse: 0.28135 |  0:00:09s
epoch 15 | loss: 0.10332 | val_0_rmse: 0.31491 | val_1_rmse: 0.2793  |  0:00:09s
epoch 16 | loss: 0.10083 | val_0_rmse: 0.3167  | val_1_rmse: 0.28133 |  0:00:10s
epoch 17 | loss: 0.10075 | val_0_rmse: 0.31528 | val_1_rmse: 0.27992 |  0:00:10s
epoch 18 | loss: 0.10084 | val_0_rmse: 0.31619 | val_1_rmse: 0.28125 |  0:00:11s
epoch 19 | loss: 0.10106 | val_0_rmse: 0.31633 | val_1_rmse: 0.28127 |  0:00:12s
epoch 20 | loss: 0.1005  | val_0_rmse: 0.31503 | val_1_rmse: 0.27958 |  0:00:12s
epoch 21 | loss: 0.10017 | val_0_rmse: 0.31538 | val_1_rmse: 0.28007 |  0:00:13s
epoch 22 | loss: 0.09995 | val_0_rmse: 0.31529 | val_1_rmse: 0.28006 |  0:00:14s
epoch 23 | loss: 0.09989 | val_0_rmse: 0.31475 | val_1_rmse: 0.27978 |  0:00:14s
epoch 24 | loss: 0.10077 | val_0_rmse: 0.31612 | val_1_rmse: 0.2813  |  0:00:15s
epoch 25 | loss: 0.10103 | val_0_rmse: 0.31517 | val_1_rmse: 0.27982 |  0:00:15s
epoch 26 | loss: 0.10074 | val_0_rmse: 0.31485 | val_1_rmse: 0.27944 |  0:00:16s
epoch 27 | loss: 0.10122 | val_0_rmse: 0.31544 | val_1_rmse: 0.27992 |  0:00:17s
epoch 28 | loss: 0.10041 | val_0_rmse: 0.31537 | val_1_rmse: 0.27996 |  0:00:17s
epoch 29 | loss: 0.10174 | val_0_rmse: 0.31411 | val_1_rmse: 0.27863 |  0:00:18s
epoch 30 | loss: 0.10034 | val_0_rmse: 0.31447 | val_1_rmse: 0.27896 |  0:00:18s
epoch 31 | loss: 0.09956 | val_0_rmse: 0.31333 | val_1_rmse: 0.2777  |  0:00:19s
epoch 32 | loss: 0.09982 | val_0_rmse: 0.31451 | val_1_rmse: 0.27908 |  0:00:20s
epoch 33 | loss: 0.1008  | val_0_rmse: 0.3132  | val_1_rmse: 0.27753 |  0:00:20s
epoch 34 | loss: 0.10106 | val_0_rmse: 0.31938 | val_1_rmse: 0.28448 |  0:00:21s
epoch 35 | loss: 0.10216 | val_0_rmse: 0.3139  | val_1_rmse: 0.2783  |  0:00:21s
epoch 36 | loss: 0.10326 | val_0_rmse: 0.31545 | val_1_rmse: 0.28009 |  0:00:22s
epoch 37 | loss: 0.101   | val_0_rmse: 0.31496 | val_1_rmse: 0.2797  |  0:00:23s
epoch 38 | loss: 0.09908 | val_0_rmse: 0.31364 | val_1_rmse: 0.27816 |  0:00:23s
epoch 39 | loss: 0.09902 | val_0_rmse: 0.31621 | val_1_rmse: 0.28121 |  0:00:24s
epoch 40 | loss: 0.09837 | val_0_rmse: 0.31294 | val_1_rmse: 0.27767 |  0:00:24s
epoch 41 | loss: 0.09842 | val_0_rmse: 0.31527 | val_1_rmse: 0.28059 |  0:00:25s
epoch 42 | loss: 0.09853 | val_0_rmse: 0.31368 | val_1_rmse: 0.27898 |  0:00:26s
epoch 43 | loss: 0.09855 | val_0_rmse: 0.30982 | val_1_rmse: 0.27499 |  0:00:26s
epoch 44 | loss: 0.09639 | val_0_rmse: 0.31045 | val_1_rmse: 0.27571 |  0:00:27s
epoch 45 | loss: 0.09599 | val_0_rmse: 0.31003 | val_1_rmse: 0.27547 |  0:00:28s
epoch 46 | loss: 0.09652 | val_0_rmse: 0.30931 | val_1_rmse: 0.27483 |  0:00:28s
epoch 47 | loss: 0.09618 | val_0_rmse: 0.30843 | val_1_rmse: 0.27393 |  0:00:29s
epoch 48 | loss: 0.09629 | val_0_rmse: 0.30827 | val_1_rmse: 0.27369 |  0:00:29s
epoch 49 | loss: 0.09467 | val_0_rmse: 0.30806 | val_1_rmse: 0.27374 |  0:00:30s
epoch 50 | loss: 0.096   | val_0_rmse: 0.30712 | val_1_rmse: 0.27307 |  0:00:31s
epoch 51 | loss: 0.09455 | val_0_rmse: 0.30679 | val_1_rmse: 0.27335 |  0:00:31s
epoch 52 | loss: 0.09418 | val_0_rmse: 0.30554 | val_1_rmse: 0.27201 |  0:00:32s
epoch 53 | loss: 0.09409 | val_0_rmse: 0.30576 | val_1_rmse: 0.2728  |  0:00:33s
epoch 54 | loss: 0.09338 | val_0_rmse: 0.30477 | val_1_rmse: 0.27262 |  0:00:33s
epoch 55 | loss: 0.09236 | val_0_rmse: 0.30635 | val_1_rmse: 0.27604 |  0:00:34s
epoch 56 | loss: 0.09306 | val_0_rmse: 0.30567 | val_1_rmse: 0.27587 |  0:00:34s
epoch 57 | loss: 0.09055 | val_0_rmse: 0.30207 | val_1_rmse: 0.27359 |  0:00:35s
epoch 58 | loss: 0.0898  | val_0_rmse: 0.29941 | val_1_rmse: 0.27377 |  0:00:36s
epoch 59 | loss: 0.0877  | val_0_rmse: 0.29554 | val_1_rmse: 0.27376 |  0:00:36s
epoch 60 | loss: 0.08921 | val_0_rmse: 0.3028  | val_1_rmse: 0.27799 |  0:00:37s
epoch 61 | loss: 0.08596 | val_0_rmse: 0.29571 | val_1_rmse: 0.2736  |  0:00:37s
epoch 62 | loss: 0.08365 | val_0_rmse: 0.28812 | val_1_rmse: 0.27442 |  0:00:38s
epoch 63 | loss: 0.08941 | val_0_rmse: 0.30173 | val_1_rmse: 0.27357 |  0:00:39s
epoch 64 | loss: 0.0932  | val_0_rmse: 0.30343 | val_1_rmse: 0.2716  |  0:00:39s
epoch 65 | loss: 0.09304 | val_0_rmse: 0.30142 | val_1_rmse: 0.27184 |  0:00:40s
epoch 66 | loss: 0.09218 | val_0_rmse: 0.29402 | val_1_rmse: 0.26834 |  0:00:40s
epoch 67 | loss: 0.08933 | val_0_rmse: 0.28568 | val_1_rmse: 0.26607 |  0:00:41s
epoch 68 | loss: 0.08486 | val_0_rmse: 0.27717 | val_1_rmse: 0.26206 |  0:00:42s
epoch 69 | loss: 0.07999 | val_0_rmse: 0.27342 | val_1_rmse: 0.26809 |  0:00:42s
epoch 70 | loss: 0.08423 | val_0_rmse: 0.2772  | val_1_rmse: 0.26984 |  0:00:43s
epoch 71 | loss: 0.08168 | val_0_rmse: 0.26342 | val_1_rmse: 0.26721 |  0:00:44s
epoch 72 | loss: 0.07846 | val_0_rmse: 0.25379 | val_1_rmse: 0.26654 |  0:00:44s
epoch 73 | loss: 0.07021 | val_0_rmse: 0.26114 | val_1_rmse: 0.26936 |  0:00:45s
epoch 74 | loss: 0.07418 | val_0_rmse: 0.25004 | val_1_rmse: 0.2563  |  0:00:45s
epoch 75 | loss: 0.07153 | val_0_rmse: 0.25199 | val_1_rmse: 0.26245 |  0:00:46s
epoch 76 | loss: 0.06608 | val_0_rmse: 0.25641 | val_1_rmse: 0.27979 |  0:00:47s
epoch 77 | loss: 0.065   | val_0_rmse: 0.24452 | val_1_rmse: 0.25688 |  0:00:47s
epoch 78 | loss: 0.06207 | val_0_rmse: 0.24529 | val_1_rmse: 0.25389 |  0:00:48s
epoch 79 | loss: 0.06186 | val_0_rmse: 0.24309 | val_1_rmse: 0.25663 |  0:00:48s
epoch 80 | loss: 0.06852 | val_0_rmse: 0.242   | val_1_rmse: 0.25394 |  0:00:49s
epoch 81 | loss: 0.06497 | val_0_rmse: 0.24903 | val_1_rmse: 0.25796 |  0:00:50s
epoch 82 | loss: 0.06551 | val_0_rmse: 0.24296 | val_1_rmse: 0.25764 |  0:00:50s
epoch 83 | loss: 0.06352 | val_0_rmse: 0.24447 | val_1_rmse: 0.26315 |  0:00:51s
epoch 84 | loss: 0.05937 | val_0_rmse: 0.24263 | val_1_rmse: 0.25866 |  0:00:51s
epoch 85 | loss: 0.05898 | val_0_rmse: 0.24041 | val_1_rmse: 0.26372 |  0:00:52s
epoch 86 | loss: 0.06072 | val_0_rmse: 0.24409 | val_1_rmse: 0.25921 |  0:00:53s
epoch 87 | loss: 0.06103 | val_0_rmse: 0.24467 | val_1_rmse: 0.26398 |  0:00:53s
epoch 88 | loss: 0.06063 | val_0_rmse: 0.24338 | val_1_rmse: 0.26405 |  0:00:54s
epoch 89 | loss: 0.05945 | val_0_rmse: 0.24295 | val_1_rmse: 0.26348 |  0:00:55s
epoch 90 | loss: 0.06143 | val_0_rmse: 0.24225 | val_1_rmse: 0.25778 |  0:00:55s
epoch 91 | loss: 0.0585  | val_0_rmse: 0.24041 | val_1_rmse: 0.25709 |  0:00:56s
epoch 92 | loss: 0.06081 | val_0_rmse: 0.24516 | val_1_rmse: 0.25496 |  0:00:56s
epoch 93 | loss: 0.05964 | val_0_rmse: 0.24257 | val_1_rmse: 0.25699 |  0:00:57s
epoch 94 | loss: 0.06274 | val_0_rmse: 0.24246 | val_1_rmse: 0.25925 |  0:00:58s
epoch 95 | loss: 0.06087 | val_0_rmse: 0.24142 | val_1_rmse: 0.25873 |  0:00:58s
epoch 96 | loss: 0.06043 | val_0_rmse: 0.23977 | val_1_rmse: 0.25494 |  0:00:59s
epoch 97 | loss: 0.05998 | val_0_rmse: 0.23672 | val_1_rmse: 0.2521  |  0:00:59s
epoch 98 | loss: 0.05737 | val_0_rmse: 0.23672 | val_1_rmse: 0.25837 |  0:01:00s
epoch 99 | loss: 0.0626  | val_0_rmse: 0.24339 | val_1_rmse: 0.24755 |  0:01:01s
epoch 100| loss: 0.05918 | val_0_rmse: 0.23666 | val_1_rmse: 0.25308 |  0:01:01s
epoch 101| loss: 0.05863 | val_0_rmse: 0.23988 | val_1_rmse: 0.26106 |  0:01:02s
epoch 102| loss: 0.0572  | val_0_rmse: 0.23897 | val_1_rmse: 0.25831 |  0:01:02s
epoch 103| loss: 0.05962 | val_0_rmse: 0.24525 | val_1_rmse: 0.2613  |  0:01:03s
epoch 104| loss: 0.05859 | val_0_rmse: 0.23902 | val_1_rmse: 0.25925 |  0:01:04s
epoch 105| loss: 0.05792 | val_0_rmse: 0.23863 | val_1_rmse: 0.26619 |  0:01:04s
epoch 106| loss: 0.05805 | val_0_rmse: 0.23828 | val_1_rmse: 0.26684 |  0:01:05s
epoch 107| loss: 0.05825 | val_0_rmse: 0.24575 | val_1_rmse: 0.27035 |  0:01:05s
epoch 108| loss: 0.0593  | val_0_rmse: 0.24621 | val_1_rmse: 0.26832 |  0:01:06s
epoch 109| loss: 0.05863 | val_0_rmse: 0.24283 | val_1_rmse: 0.26822 |  0:01:07s
epoch 110| loss: 0.05765 | val_0_rmse: 0.2406  | val_1_rmse: 0.26561 |  0:01:07s
epoch 111| loss: 0.05698 | val_0_rmse: 0.2408  | val_1_rmse: 0.26702 |  0:01:08s
epoch 112| loss: 0.05626 | val_0_rmse: 0.24299 | val_1_rmse: 0.2669  |  0:01:08s
epoch 113| loss: 0.05548 | val_0_rmse: 0.24254 | val_1_rmse: 0.26821 |  0:01:09s
epoch 114| loss: 0.05542 | val_0_rmse: 0.24453 | val_1_rmse: 0.26753 |  0:01:10s
epoch 115| loss: 0.05601 | val_0_rmse: 0.2419  | val_1_rmse: 0.25986 |  0:01:10s
epoch 116| loss: 0.05559 | val_0_rmse: 0.23918 | val_1_rmse: 0.25847 |  0:01:11s
epoch 117| loss: 0.05546 | val_0_rmse: 0.24099 | val_1_rmse: 0.2592  |  0:01:12s
epoch 118| loss: 0.05508 | val_0_rmse: 0.2461  | val_1_rmse: 0.26272 |  0:01:12s
epoch 119| loss: 0.05538 | val_0_rmse: 0.23973 | val_1_rmse: 0.26603 |  0:01:13s
epoch 120| loss: 0.05434 | val_0_rmse: 0.23744 | val_1_rmse: 0.25549 |  0:01:13s
epoch 121| loss: 0.05483 | val_0_rmse: 0.23685 | val_1_rmse: 0.25844 |  0:01:14s
epoch 122| loss: 0.05425 | val_0_rmse: 0.24046 | val_1_rmse: 0.26775 |  0:01:15s
epoch 123| loss: 0.05397 | val_0_rmse: 0.24104 | val_1_rmse: 0.26407 |  0:01:15s
epoch 124| loss: 0.05448 | val_0_rmse: 0.23755 | val_1_rmse: 0.26765 |  0:01:16s
epoch 125| loss: 0.0531  | val_0_rmse: 0.23775 | val_1_rmse: 0.26872 |  0:01:16s
epoch 126| loss: 0.05459 | val_0_rmse: 0.24022 | val_1_rmse: 0.25973 |  0:01:17s
epoch 127| loss: 0.05542 | val_0_rmse: 0.23306 | val_1_rmse: 0.27668 |  0:01:18s
epoch 128| loss: 0.05417 | val_0_rmse: 0.23177 | val_1_rmse: 0.28327 |  0:01:18s
epoch 129| loss: 0.05348 | val_0_rmse: 0.2354  | val_1_rmse: 0.27777 |  0:01:19s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.24755
Best weights from best epoch are automatically used!
ended training at: 21:55:39
Feature importance:
Mean squared error is of 0.07565236112631142
Mean absolute error:0.18873278834727766
MAPE:0.18977676229063895
R2 score:0.01951390726545077
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:55:40
epoch 0  | loss: 1.62785 | val_0_rmse: 0.43287 | val_1_rmse: 0.39628 |  0:00:00s
epoch 1  | loss: 0.57349 | val_0_rmse: 0.32225 | val_1_rmse: 0.28038 |  0:00:01s
epoch 2  | loss: 0.19613 | val_0_rmse: 0.3498  | val_1_rmse: 0.31189 |  0:00:01s
epoch 3  | loss: 0.14415 | val_0_rmse: 0.31402 | val_1_rmse: 0.27139 |  0:00:02s
epoch 4  | loss: 0.10777 | val_0_rmse: 0.30133 | val_1_rmse: 0.25922 |  0:00:03s
epoch 5  | loss: 0.10275 | val_0_rmse: 0.30374 | val_1_rmse: 0.26181 |  0:00:03s
epoch 6  | loss: 0.09648 | val_0_rmse: 0.30492 | val_1_rmse: 0.26433 |  0:00:04s
epoch 7  | loss: 0.09445 | val_0_rmse: 0.30264 | val_1_rmse: 0.26112 |  0:00:05s
epoch 8  | loss: 0.09361 | val_0_rmse: 0.30202 | val_1_rmse: 0.26105 |  0:00:05s
epoch 9  | loss: 0.09268 | val_0_rmse: 0.30076 | val_1_rmse: 0.26068 |  0:00:06s
epoch 10 | loss: 0.0917  | val_0_rmse: 0.30166 | val_1_rmse: 0.26023 |  0:00:06s
epoch 11 | loss: 0.09245 | val_0_rmse: 0.30041 | val_1_rmse: 0.2598  |  0:00:07s
epoch 12 | loss: 0.09197 | val_0_rmse: 0.3086  | val_1_rmse: 0.26611 |  0:00:08s
epoch 13 | loss: 0.09199 | val_0_rmse: 0.30033 | val_1_rmse: 0.25937 |  0:00:08s
epoch 14 | loss: 0.09131 | val_0_rmse: 0.30489 | val_1_rmse: 0.26269 |  0:00:09s
epoch 15 | loss: 0.09121 | val_0_rmse: 0.30335 | val_1_rmse: 0.26117 |  0:00:09s
epoch 16 | loss: 0.09174 | val_0_rmse: 0.30104 | val_1_rmse: 0.26022 |  0:00:10s
epoch 17 | loss: 0.09091 | val_0_rmse: 0.30087 | val_1_rmse: 0.25989 |  0:00:11s
epoch 18 | loss: 0.09086 | val_0_rmse: 0.3055  | val_1_rmse: 0.2625  |  0:00:11s
epoch 19 | loss: 0.09091 | val_0_rmse: 0.3006  | val_1_rmse: 0.26023 |  0:00:12s
epoch 20 | loss: 0.0917  | val_0_rmse: 0.30352 | val_1_rmse: 0.26109 |  0:00:12s
epoch 21 | loss: 0.09094 | val_0_rmse: 0.30742 | val_1_rmse: 0.26517 |  0:00:13s
epoch 22 | loss: 0.09317 | val_0_rmse: 0.30031 | val_1_rmse: 0.26008 |  0:00:14s
epoch 23 | loss: 0.09228 | val_0_rmse: 0.30088 | val_1_rmse: 0.26106 |  0:00:14s
epoch 24 | loss: 0.09158 | val_0_rmse: 0.30171 | val_1_rmse: 0.26039 |  0:00:15s
epoch 25 | loss: 0.09124 | val_0_rmse: 0.30665 | val_1_rmse: 0.26365 |  0:00:15s
epoch 26 | loss: 0.09172 | val_0_rmse: 0.30089 | val_1_rmse: 0.26114 |  0:00:16s
epoch 27 | loss: 0.09126 | val_0_rmse: 0.30133 | val_1_rmse: 0.25947 |  0:00:17s
epoch 28 | loss: 0.09127 | val_0_rmse: 0.30186 | val_1_rmse: 0.25985 |  0:00:17s
epoch 29 | loss: 0.09105 | val_0_rmse: 0.30165 | val_1_rmse: 0.25968 |  0:00:18s
epoch 30 | loss: 0.09052 | val_0_rmse: 0.30223 | val_1_rmse: 0.25962 |  0:00:18s
epoch 31 | loss: 0.09057 | val_0_rmse: 0.301   | val_1_rmse: 0.2589  |  0:00:19s
epoch 32 | loss: 0.09054 | val_0_rmse: 0.30036 | val_1_rmse: 0.25957 |  0:00:20s
epoch 33 | loss: 0.09084 | val_0_rmse: 0.30077 | val_1_rmse: 0.25929 |  0:00:20s
epoch 34 | loss: 0.0908  | val_0_rmse: 0.30124 | val_1_rmse: 0.26006 |  0:00:21s
epoch 35 | loss: 0.09152 | val_0_rmse: 0.31035 | val_1_rmse: 0.26822 |  0:00:22s
epoch 36 | loss: 0.09173 | val_0_rmse: 0.30349 | val_1_rmse: 0.262   |  0:00:22s
epoch 37 | loss: 0.09146 | val_0_rmse: 0.30377 | val_1_rmse: 0.26206 |  0:00:23s
epoch 38 | loss: 0.09139 | val_0_rmse: 0.30107 | val_1_rmse: 0.2613  |  0:00:24s
epoch 39 | loss: 0.09226 | val_0_rmse: 0.30326 | val_1_rmse: 0.26114 |  0:00:24s
epoch 40 | loss: 0.09089 | val_0_rmse: 0.30376 | val_1_rmse: 0.26152 |  0:00:25s
epoch 41 | loss: 0.09037 | val_0_rmse: 0.30054 | val_1_rmse: 0.2591  |  0:00:25s
epoch 42 | loss: 0.09047 | val_0_rmse: 0.30113 | val_1_rmse: 0.25964 |  0:00:26s
epoch 43 | loss: 0.0908  | val_0_rmse: 0.29996 | val_1_rmse: 0.2604  |  0:00:27s
epoch 44 | loss: 0.09053 | val_0_rmse: 0.3001  | val_1_rmse: 0.25892 |  0:00:27s
epoch 45 | loss: 0.09049 | val_0_rmse: 0.30314 | val_1_rmse: 0.26155 |  0:00:28s
epoch 46 | loss: 0.09076 | val_0_rmse: 0.30291 | val_1_rmse: 0.26135 |  0:00:29s
epoch 47 | loss: 0.09067 | val_0_rmse: 0.29985 | val_1_rmse: 0.25984 |  0:00:29s
epoch 48 | loss: 0.09024 | val_0_rmse: 0.29958 | val_1_rmse: 0.25902 |  0:00:30s
epoch 49 | loss: 0.09009 | val_0_rmse: 0.30247 | val_1_rmse: 0.26093 |  0:00:30s
epoch 50 | loss: 0.08997 | val_0_rmse: 0.30013 | val_1_rmse: 0.25915 |  0:00:31s
epoch 51 | loss: 0.09011 | val_0_rmse: 0.30218 | val_1_rmse: 0.26085 |  0:00:32s
epoch 52 | loss: 0.08974 | val_0_rmse: 0.29979 | val_1_rmse: 0.25947 |  0:00:32s
epoch 53 | loss: 0.09015 | val_0_rmse: 0.30237 | val_1_rmse: 0.26156 |  0:00:33s
epoch 54 | loss: 0.09044 | val_0_rmse: 0.30838 | val_1_rmse: 0.26698 |  0:00:33s
epoch 55 | loss: 0.09204 | val_0_rmse: 0.30156 | val_1_rmse: 0.2608  |  0:00:34s
epoch 56 | loss: 0.09067 | val_0_rmse: 0.30008 | val_1_rmse: 0.2612  |  0:00:35s
epoch 57 | loss: 0.0918  | val_0_rmse: 0.30126 | val_1_rmse: 0.26298 |  0:00:35s
epoch 58 | loss: 0.09013 | val_0_rmse: 0.29992 | val_1_rmse: 0.2608  |  0:00:36s
epoch 59 | loss: 0.09101 | val_0_rmse: 0.30013 | val_1_rmse: 0.26077 |  0:00:37s
epoch 60 | loss: 0.09085 | val_0_rmse: 0.30093 | val_1_rmse: 0.26045 |  0:00:37s
epoch 61 | loss: 0.09081 | val_0_rmse: 0.30241 | val_1_rmse: 0.26132 |  0:00:38s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.2589
Best weights from best epoch are automatically used!
ended training at: 21:56:18
Feature importance:
Mean squared error is of 0.11739722030402817
Mean absolute error:0.20538187916083184
MAPE:0.21014828379465475
R2 score:-0.011842778358807982
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:56:19
epoch 0  | loss: 1.68067 | val_0_rmse: 0.32566 | val_1_rmse: 0.29862 |  0:00:00s
epoch 1  | loss: 0.38211 | val_0_rmse: 0.31248 | val_1_rmse: 0.27461 |  0:00:01s
epoch 2  | loss: 0.18435 | val_0_rmse: 0.37225 | val_1_rmse: 0.33524 |  0:00:01s
epoch 3  | loss: 0.13423 | val_0_rmse: 0.32394 | val_1_rmse: 0.28659 |  0:00:02s
epoch 4  | loss: 0.11107 | val_0_rmse: 0.30964 | val_1_rmse: 0.27346 |  0:00:03s
epoch 5  | loss: 0.10872 | val_0_rmse: 0.31823 | val_1_rmse: 0.28075 |  0:00:03s
epoch 6  | loss: 0.10221 | val_0_rmse: 0.30738 | val_1_rmse: 0.2722  |  0:00:04s
epoch 7  | loss: 0.09941 | val_0_rmse: 0.30771 | val_1_rmse: 0.27235 |  0:00:04s
epoch 8  | loss: 0.09979 | val_0_rmse: 0.3151  | val_1_rmse: 0.27686 |  0:00:05s
epoch 9  | loss: 0.10306 | val_0_rmse: 0.30919 | val_1_rmse: 0.27271 |  0:00:06s
epoch 10 | loss: 0.10038 | val_0_rmse: 0.30756 | val_1_rmse: 0.27351 |  0:00:06s
epoch 11 | loss: 0.09854 | val_0_rmse: 0.30983 | val_1_rmse: 0.27722 |  0:00:07s
epoch 12 | loss: 0.09772 | val_0_rmse: 0.3082  | val_1_rmse: 0.27418 |  0:00:07s
epoch 13 | loss: 0.09579 | val_0_rmse: 0.30756 | val_1_rmse: 0.27242 |  0:00:08s
epoch 14 | loss: 0.0963  | val_0_rmse: 0.30746 | val_1_rmse: 0.27273 |  0:00:09s
epoch 15 | loss: 0.09677 | val_0_rmse: 0.30797 | val_1_rmse: 0.27244 |  0:00:09s
epoch 16 | loss: 0.09917 | val_0_rmse: 0.30871 | val_1_rmse: 0.27261 |  0:00:10s
epoch 17 | loss: 0.09772 | val_0_rmse: 0.30827 | val_1_rmse: 0.272   |  0:00:11s
epoch 18 | loss: 0.09985 | val_0_rmse: 0.3062  | val_1_rmse: 0.27154 |  0:00:11s
epoch 19 | loss: 0.09728 | val_0_rmse: 0.30811 | val_1_rmse: 0.2753  |  0:00:12s
epoch 20 | loss: 0.09657 | val_0_rmse: 0.30871 | val_1_rmse: 0.27566 |  0:00:12s
epoch 21 | loss: 0.09565 | val_0_rmse: 0.31    | val_1_rmse: 0.27804 |  0:00:13s
epoch 22 | loss: 0.09627 | val_0_rmse: 0.30857 | val_1_rmse: 0.27541 |  0:00:14s
epoch 23 | loss: 0.09622 | val_0_rmse: 0.31002 | val_1_rmse: 0.27743 |  0:00:14s
epoch 24 | loss: 0.09706 | val_0_rmse: 0.31239 | val_1_rmse: 0.28109 |  0:00:15s
epoch 25 | loss: 0.09649 | val_0_rmse: 0.31086 | val_1_rmse: 0.27878 |  0:00:15s
epoch 26 | loss: 0.09625 | val_0_rmse: 0.31297 | val_1_rmse: 0.2819  |  0:00:16s
epoch 27 | loss: 0.09704 | val_0_rmse: 0.31499 | val_1_rmse: 0.28454 |  0:00:17s
epoch 28 | loss: 0.09676 | val_0_rmse: 0.3094  | val_1_rmse: 0.2764  |  0:00:17s
epoch 29 | loss: 0.09528 | val_0_rmse: 0.30726 | val_1_rmse: 0.27263 |  0:00:18s
epoch 30 | loss: 0.09522 | val_0_rmse: 0.30723 | val_1_rmse: 0.2724  |  0:00:18s
epoch 31 | loss: 0.09533 | val_0_rmse: 0.30781 | val_1_rmse: 0.2742  |  0:00:19s
epoch 32 | loss: 0.09526 | val_0_rmse: 0.30775 | val_1_rmse: 0.27391 |  0:00:20s
epoch 33 | loss: 0.09537 | val_0_rmse: 0.30797 | val_1_rmse: 0.27285 |  0:00:20s
epoch 34 | loss: 0.09528 | val_0_rmse: 0.30708 | val_1_rmse: 0.27284 |  0:00:21s
epoch 35 | loss: 0.09497 | val_0_rmse: 0.30751 | val_1_rmse: 0.27364 |  0:00:21s
epoch 36 | loss: 0.09512 | val_0_rmse: 0.30845 | val_1_rmse: 0.27583 |  0:00:22s
epoch 37 | loss: 0.09654 | val_0_rmse: 0.30776 | val_1_rmse: 0.27484 |  0:00:23s
epoch 38 | loss: 0.09617 | val_0_rmse: 0.30794 | val_1_rmse: 0.27311 |  0:00:23s
epoch 39 | loss: 0.09564 | val_0_rmse: 0.30772 | val_1_rmse: 0.27326 |  0:00:24s
epoch 40 | loss: 0.09552 | val_0_rmse: 0.30784 | val_1_rmse: 0.27439 |  0:00:25s
epoch 41 | loss: 0.09551 | val_0_rmse: 0.30721 | val_1_rmse: 0.27201 |  0:00:25s
epoch 42 | loss: 0.0951  | val_0_rmse: 0.30743 | val_1_rmse: 0.27227 |  0:00:26s
epoch 43 | loss: 0.09566 | val_0_rmse: 0.30711 | val_1_rmse: 0.27265 |  0:00:26s
epoch 44 | loss: 0.09558 | val_0_rmse: 0.30744 | val_1_rmse: 0.27369 |  0:00:27s
epoch 45 | loss: 0.09615 | val_0_rmse: 0.30881 | val_1_rmse: 0.27667 |  0:00:28s
epoch 46 | loss: 0.09648 | val_0_rmse: 0.31105 | val_1_rmse: 0.27939 |  0:00:28s
epoch 47 | loss: 0.09551 | val_0_rmse: 0.30817 | val_1_rmse: 0.27552 |  0:00:29s
epoch 48 | loss: 0.09537 | val_0_rmse: 0.30863 | val_1_rmse: 0.27632 |  0:00:29s

Early stopping occured at epoch 48 with best_epoch = 18 and best_val_1_rmse = 0.27154
Best weights from best epoch are automatically used!
ended training at: 21:56:49
Feature importance:
Mean squared error is of 0.09576840662182785
Mean absolute error:0.1942971637945485
MAPE:0.20193048042195222
R2 score:0.0053002879910626355
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:56:49
epoch 0  | loss: 3.55125 | val_0_rmse: 0.7376  | val_1_rmse: 0.73494 |  0:00:00s
epoch 1  | loss: 1.59938 | val_0_rmse: 0.4423  | val_1_rmse: 0.43304 |  0:00:00s
epoch 2  | loss: 0.99184 | val_0_rmse: 0.32493 | val_1_rmse: 0.30769 |  0:00:00s
epoch 3  | loss: 1.22298 | val_0_rmse: 0.31209 | val_1_rmse: 0.29913 |  0:00:00s
epoch 4  | loss: 0.86704 | val_0_rmse: 0.35867 | val_1_rmse: 0.34059 |  0:00:00s
epoch 5  | loss: 0.64212 | val_0_rmse: 0.47023 | val_1_rmse: 0.45562 |  0:00:00s
epoch 6  | loss: 0.51002 | val_0_rmse: 0.48318 | val_1_rmse: 0.47344 |  0:00:00s
epoch 7  | loss: 0.64117 | val_0_rmse: 0.43757 | val_1_rmse: 0.43137 |  0:00:00s
epoch 8  | loss: 0.49876 | val_0_rmse: 0.38552 | val_1_rmse: 0.38135 |  0:00:00s
epoch 9  | loss: 0.31187 | val_0_rmse: 0.37721 | val_1_rmse: 0.37355 |  0:00:00s
epoch 10 | loss: 0.38064 | val_0_rmse: 0.35814 | val_1_rmse: 0.35417 |  0:00:00s
epoch 11 | loss: 0.2902  | val_0_rmse: 0.32474 | val_1_rmse: 0.32057 |  0:00:00s
epoch 12 | loss: 0.39083 | val_0_rmse: 0.30202 | val_1_rmse: 0.29824 |  0:00:01s
epoch 13 | loss: 0.22966 | val_0_rmse: 0.29338 | val_1_rmse: 0.29046 |  0:00:01s
epoch 14 | loss: 0.23608 | val_0_rmse: 0.29667 | val_1_rmse: 0.29296 |  0:00:01s
epoch 15 | loss: 0.20462 | val_0_rmse: 0.30101 | val_1_rmse: 0.29649 |  0:00:01s
epoch 16 | loss: 0.17488 | val_0_rmse: 0.32115 | val_1_rmse: 0.31598 |  0:00:01s
epoch 17 | loss: 0.17349 | val_0_rmse: 0.33984 | val_1_rmse: 0.33457 |  0:00:01s
epoch 18 | loss: 0.18062 | val_0_rmse: 0.31517 | val_1_rmse: 0.30939 |  0:00:01s
epoch 19 | loss: 0.1249  | val_0_rmse: 0.2977  | val_1_rmse: 0.29119 |  0:00:01s
epoch 20 | loss: 0.1324  | val_0_rmse: 0.29648 | val_1_rmse: 0.28997 |  0:00:01s
epoch 21 | loss: 0.13968 | val_0_rmse: 0.33045 | val_1_rmse: 0.32386 |  0:00:01s
epoch 22 | loss: 0.15016 | val_0_rmse: 0.31396 | val_1_rmse: 0.30735 |  0:00:01s
epoch 23 | loss: 0.11197 | val_0_rmse: 0.30143 | val_1_rmse: 0.29602 |  0:00:01s
epoch 24 | loss: 0.12831 | val_0_rmse: 0.30106 | val_1_rmse: 0.29646 |  0:00:02s
epoch 25 | loss: 0.10591 | val_0_rmse: 0.31256 | val_1_rmse: 0.30747 |  0:00:02s
epoch 26 | loss: 0.10975 | val_0_rmse: 0.32223 | val_1_rmse: 0.31728 |  0:00:02s
epoch 27 | loss: 0.09644 | val_0_rmse: 0.30448 | val_1_rmse: 0.30014 |  0:00:02s
epoch 28 | loss: 0.09953 | val_0_rmse: 0.29582 | val_1_rmse: 0.29288 |  0:00:02s
epoch 29 | loss: 0.10819 | val_0_rmse: 0.29855 | val_1_rmse: 0.29629 |  0:00:02s
epoch 30 | loss: 0.0981  | val_0_rmse: 0.30905 | val_1_rmse: 0.30685 |  0:00:02s
epoch 31 | loss: 0.094   | val_0_rmse: 0.30627 | val_1_rmse: 0.30335 |  0:00:02s
epoch 32 | loss: 0.09926 | val_0_rmse: 0.29805 | val_1_rmse: 0.29589 |  0:00:02s
epoch 33 | loss: 0.09389 | val_0_rmse: 0.29993 | val_1_rmse: 0.29822 |  0:00:02s
epoch 34 | loss: 0.09194 | val_0_rmse: 0.30224 | val_1_rmse: 0.30067 |  0:00:02s
epoch 35 | loss: 0.09002 | val_0_rmse: 0.30247 | val_1_rmse: 0.3018  |  0:00:02s
epoch 36 | loss: 0.08743 | val_0_rmse: 0.29698 | val_1_rmse: 0.29874 |  0:00:03s
epoch 37 | loss: 0.08894 | val_0_rmse: 0.29744 | val_1_rmse: 0.29993 |  0:00:03s
epoch 38 | loss: 0.08373 | val_0_rmse: 0.30041 | val_1_rmse: 0.30312 |  0:00:03s
epoch 39 | loss: 0.08396 | val_0_rmse: 0.3008  | val_1_rmse: 0.30477 |  0:00:03s
epoch 40 | loss: 0.08561 | val_0_rmse: 0.29356 | val_1_rmse: 0.29864 |  0:00:03s
epoch 41 | loss: 0.08484 | val_0_rmse: 0.29609 | val_1_rmse: 0.30168 |  0:00:03s
epoch 42 | loss: 0.08675 | val_0_rmse: 0.30214 | val_1_rmse: 0.30779 |  0:00:03s
epoch 43 | loss: 0.08704 | val_0_rmse: 0.29656 | val_1_rmse: 0.3033  |  0:00:03s
epoch 44 | loss: 0.08459 | val_0_rmse: 0.29413 | val_1_rmse: 0.30173 |  0:00:03s
epoch 45 | loss: 0.08492 | val_0_rmse: 0.30157 | val_1_rmse: 0.30896 |  0:00:03s
epoch 46 | loss: 0.08223 | val_0_rmse: 0.29698 | val_1_rmse: 0.30519 |  0:00:03s
epoch 47 | loss: 0.08198 | val_0_rmse: 0.29206 | val_1_rmse: 0.30143 |  0:00:03s
epoch 48 | loss: 0.08049 | val_0_rmse: 0.29747 | val_1_rmse: 0.30755 |  0:00:04s
epoch 49 | loss: 0.08157 | val_0_rmse: 0.30676 | val_1_rmse: 0.31672 |  0:00:04s
epoch 50 | loss: 0.08033 | val_0_rmse: 0.29637 | val_1_rmse: 0.30788 |  0:00:04s

Early stopping occured at epoch 50 with best_epoch = 20 and best_val_1_rmse = 0.28997
Best weights from best epoch are automatically used!
ended training at: 21:56:54
Feature importance:
Mean squared error is of 0.10352670375267127
Mean absolute error:0.20709002734848408
MAPE:0.21120707263674254
R2 score:0.00020738867787406523
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:56:54
epoch 0  | loss: 3.81002 | val_0_rmse: 0.81299 | val_1_rmse: 0.83284 |  0:00:00s
epoch 1  | loss: 1.59455 | val_0_rmse: 0.56985 | val_1_rmse: 0.59076 |  0:00:00s
epoch 2  | loss: 1.12105 | val_0_rmse: 0.45767 | val_1_rmse: 0.47687 |  0:00:00s
epoch 3  | loss: 1.37417 | val_0_rmse: 0.49468 | val_1_rmse: 0.51291 |  0:00:00s
epoch 4  | loss: 0.8511  | val_0_rmse: 0.57115 | val_1_rmse: 0.59149 |  0:00:00s
epoch 5  | loss: 0.59607 | val_0_rmse: 0.64757 | val_1_rmse: 0.66843 |  0:00:00s
epoch 6  | loss: 0.68595 | val_0_rmse: 0.64106 | val_1_rmse: 0.66048 |  0:00:00s
epoch 7  | loss: 0.5143  | val_0_rmse: 0.56247 | val_1_rmse: 0.58121 |  0:00:00s
epoch 8  | loss: 0.34024 | val_0_rmse: 0.48258 | val_1_rmse: 0.50111 |  0:00:00s
epoch 9  | loss: 0.5136  | val_0_rmse: 0.49104 | val_1_rmse: 0.51054 |  0:00:00s
epoch 10 | loss: 0.32158 | val_0_rmse: 0.54125 | val_1_rmse: 0.56223 |  0:00:01s
epoch 11 | loss: 0.25993 | val_0_rmse: 0.53305 | val_1_rmse: 0.55471 |  0:00:01s
epoch 12 | loss: 0.2777  | val_0_rmse: 0.5168  | val_1_rmse: 0.53772 |  0:00:01s
epoch 13 | loss: 0.37777 | val_0_rmse: 0.51113 | val_1_rmse: 0.53028 |  0:00:01s
epoch 14 | loss: 0.28006 | val_0_rmse: 0.47467 | val_1_rmse: 0.49008 |  0:00:01s
epoch 15 | loss: 0.31385 | val_0_rmse: 0.40497 | val_1_rmse: 0.41762 |  0:00:01s
epoch 16 | loss: 0.22596 | val_0_rmse: 0.32822 | val_1_rmse: 0.33674 |  0:00:01s
epoch 17 | loss: 0.1585  | val_0_rmse: 0.30415 | val_1_rmse: 0.30906 |  0:00:01s
epoch 18 | loss: 0.20695 | val_0_rmse: 0.34134 | val_1_rmse: 0.35367 |  0:00:01s
epoch 19 | loss: 0.13789 | val_0_rmse: 0.40147 | val_1_rmse: 0.41803 |  0:00:01s
epoch 20 | loss: 0.1786  | val_0_rmse: 0.43092 | val_1_rmse: 0.44947 |  0:00:01s
epoch 21 | loss: 0.1527  | val_0_rmse: 0.39009 | val_1_rmse: 0.40724 |  0:00:01s
epoch 22 | loss: 0.11808 | val_0_rmse: 0.33586 | val_1_rmse: 0.34778 |  0:00:02s
epoch 23 | loss: 0.13271 | val_0_rmse: 0.32319 | val_1_rmse: 0.33289 |  0:00:02s
epoch 24 | loss: 0.11815 | val_0_rmse: 0.32961 | val_1_rmse: 0.33971 |  0:00:02s
epoch 25 | loss: 0.09678 | val_0_rmse: 0.33949 | val_1_rmse: 0.35109 |  0:00:02s
epoch 26 | loss: 0.10747 | val_0_rmse: 0.31707 | val_1_rmse: 0.32716 |  0:00:02s
epoch 27 | loss: 0.0995  | val_0_rmse: 0.29949 | val_1_rmse: 0.30532 |  0:00:02s
epoch 28 | loss: 0.09704 | val_0_rmse: 0.29849 | val_1_rmse: 0.3035  |  0:00:02s
epoch 29 | loss: 0.09366 | val_0_rmse: 0.30535 | val_1_rmse: 0.31277 |  0:00:02s
epoch 30 | loss: 0.09179 | val_0_rmse: 0.31468 | val_1_rmse: 0.32489 |  0:00:02s
epoch 31 | loss: 0.09679 | val_0_rmse: 0.30676 | val_1_rmse: 0.3141  |  0:00:02s
epoch 32 | loss: 0.09608 | val_0_rmse: 0.29932 | val_1_rmse: 0.30224 |  0:00:02s
epoch 33 | loss: 0.0988  | val_0_rmse: 0.30049 | val_1_rmse: 0.30351 |  0:00:02s
epoch 34 | loss: 0.09317 | val_0_rmse: 0.30617 | val_1_rmse: 0.31033 |  0:00:03s
epoch 35 | loss: 0.09474 | val_0_rmse: 0.30606 | val_1_rmse: 0.3091  |  0:00:03s
epoch 36 | loss: 0.09382 | val_0_rmse: 0.30009 | val_1_rmse: 0.30139 |  0:00:03s
epoch 37 | loss: 0.09509 | val_0_rmse: 0.29729 | val_1_rmse: 0.29592 |  0:00:03s
epoch 38 | loss: 0.09431 | val_0_rmse: 0.30164 | val_1_rmse: 0.30206 |  0:00:03s
epoch 39 | loss: 0.0931  | val_0_rmse: 0.31199 | val_1_rmse: 0.31528 |  0:00:03s
epoch 40 | loss: 0.09581 | val_0_rmse: 0.30731 | val_1_rmse: 0.30918 |  0:00:03s
epoch 41 | loss: 0.0912  | val_0_rmse: 0.29762 | val_1_rmse: 0.29565 |  0:00:03s
epoch 42 | loss: 0.09305 | val_0_rmse: 0.29705 | val_1_rmse: 0.29613 |  0:00:03s
epoch 43 | loss: 0.08976 | val_0_rmse: 0.30572 | val_1_rmse: 0.30924 |  0:00:03s
epoch 44 | loss: 0.09083 | val_0_rmse: 0.31351 | val_1_rmse: 0.31891 |  0:00:03s
epoch 45 | loss: 0.08897 | val_0_rmse: 0.30101 | val_1_rmse: 0.30469 |  0:00:03s
epoch 46 | loss: 0.08461 | val_0_rmse: 0.29614 | val_1_rmse: 0.29775 |  0:00:04s
epoch 47 | loss: 0.09117 | val_0_rmse: 0.2998  | val_1_rmse: 0.30385 |  0:00:04s
epoch 48 | loss: 0.08943 | val_0_rmse: 0.3086  | val_1_rmse: 0.31573 |  0:00:04s
epoch 49 | loss: 0.08884 | val_0_rmse: 0.3034  | val_1_rmse: 0.30856 |  0:00:04s
epoch 50 | loss: 0.08689 | val_0_rmse: 0.29778 | val_1_rmse: 0.30014 |  0:00:04s
epoch 51 | loss: 0.08392 | val_0_rmse: 0.29721 | val_1_rmse: 0.29921 |  0:00:04s
epoch 52 | loss: 0.08702 | val_0_rmse: 0.29765 | val_1_rmse: 0.29996 |  0:00:04s
epoch 53 | loss: 0.08334 | val_0_rmse: 0.29855 | val_1_rmse: 0.30137 |  0:00:04s
epoch 54 | loss: 0.08665 | val_0_rmse: 0.30085 | val_1_rmse: 0.30447 |  0:00:04s
epoch 55 | loss: 0.08558 | val_0_rmse: 0.29993 | val_1_rmse: 0.30328 |  0:00:04s
epoch 56 | loss: 0.08769 | val_0_rmse: 0.29678 | val_1_rmse: 0.29876 |  0:00:04s
epoch 57 | loss: 0.08372 | val_0_rmse: 0.29604 | val_1_rmse: 0.29796 |  0:00:04s
epoch 58 | loss: 0.08996 | val_0_rmse: 0.3016  | val_1_rmse: 0.30645 |  0:00:05s
epoch 59 | loss: 0.08793 | val_0_rmse: 0.30458 | val_1_rmse: 0.31068 |  0:00:05s
epoch 60 | loss: 0.08581 | val_0_rmse: 0.29826 | val_1_rmse: 0.30264 |  0:00:05s
epoch 61 | loss: 0.08422 | val_0_rmse: 0.29555 | val_1_rmse: 0.29807 |  0:00:05s
epoch 62 | loss: 0.08557 | val_0_rmse: 0.2968  | val_1_rmse: 0.30027 |  0:00:05s
epoch 63 | loss: 0.08611 | val_0_rmse: 0.30148 | val_1_rmse: 0.30627 |  0:00:05s
epoch 64 | loss: 0.08502 | val_0_rmse: 0.29881 | val_1_rmse: 0.30271 |  0:00:05s
epoch 65 | loss: 0.08489 | val_0_rmse: 0.29663 | val_1_rmse: 0.29954 |  0:00:05s
epoch 66 | loss: 0.08313 | val_0_rmse: 0.2974  | val_1_rmse: 0.3012  |  0:00:05s
epoch 67 | loss: 0.08262 | val_0_rmse: 0.30165 | val_1_rmse: 0.30704 |  0:00:05s
epoch 68 | loss: 0.08721 | val_0_rmse: 0.30264 | val_1_rmse: 0.3086  |  0:00:05s
epoch 69 | loss: 0.08725 | val_0_rmse: 0.29733 | val_1_rmse: 0.30163 |  0:00:05s
epoch 70 | loss: 0.08322 | val_0_rmse: 0.29562 | val_1_rmse: 0.2986  |  0:00:06s
epoch 71 | loss: 0.08675 | val_0_rmse: 0.29729 | val_1_rmse: 0.30202 |  0:00:06s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.29565
Best weights from best epoch are automatically used!
ended training at: 21:57:00
Feature importance:
Mean squared error is of 0.10256916112727875
Mean absolute error:0.22006989500205318
MAPE:0.2509951455428058
R2 score:-0.020307888440590283
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:01
epoch 0  | loss: 3.56933 | val_0_rmse: 0.66132 | val_1_rmse: 0.66415 |  0:00:00s
epoch 1  | loss: 2.21929 | val_0_rmse: 0.4127  | val_1_rmse: 0.4164  |  0:00:00s
epoch 2  | loss: 1.34534 | val_0_rmse: 0.35022 | val_1_rmse: 0.34985 |  0:00:00s
epoch 3  | loss: 1.30613 | val_0_rmse: 0.32034 | val_1_rmse: 0.31769 |  0:00:00s
epoch 4  | loss: 0.94883 | val_0_rmse: 0.33028 | val_1_rmse: 0.32236 |  0:00:00s
epoch 5  | loss: 0.63325 | val_0_rmse: 0.39357 | val_1_rmse: 0.38816 |  0:00:00s
epoch 6  | loss: 0.52241 | val_0_rmse: 0.4254  | val_1_rmse: 0.41837 |  0:00:00s
epoch 7  | loss: 0.48632 | val_0_rmse: 0.43927 | val_1_rmse: 0.43014 |  0:00:00s
epoch 8  | loss: 0.51253 | val_0_rmse: 0.42784 | val_1_rmse: 0.41694 |  0:00:00s
epoch 9  | loss: 0.3057  | val_0_rmse: 0.36404 | val_1_rmse: 0.34841 |  0:00:00s
epoch 10 | loss: 0.27694 | val_0_rmse: 0.3735  | val_1_rmse: 0.35931 |  0:00:00s
epoch 11 | loss: 0.23289 | val_0_rmse: 0.38209 | val_1_rmse: 0.37061 |  0:00:01s
epoch 12 | loss: 0.30758 | val_0_rmse: 0.3836  | val_1_rmse: 0.37332 |  0:00:01s
epoch 13 | loss: 0.25335 | val_0_rmse: 0.34848 | val_1_rmse: 0.33573 |  0:00:01s
epoch 14 | loss: 0.15102 | val_0_rmse: 0.32875 | val_1_rmse: 0.31479 |  0:00:01s
epoch 15 | loss: 0.1795  | val_0_rmse: 0.35758 | val_1_rmse: 0.34671 |  0:00:01s
epoch 16 | loss: 0.14106 | val_0_rmse: 0.35027 | val_1_rmse: 0.33937 |  0:00:01s
epoch 17 | loss: 0.138   | val_0_rmse: 0.33304 | val_1_rmse: 0.32141 |  0:00:01s
epoch 18 | loss: 0.12314 | val_0_rmse: 0.31836 | val_1_rmse: 0.30646 |  0:00:01s
epoch 19 | loss: 0.12098 | val_0_rmse: 0.31707 | val_1_rmse: 0.30584 |  0:00:01s
epoch 20 | loss: 0.13745 | val_0_rmse: 0.31601 | val_1_rmse: 0.30591 |  0:00:01s
epoch 21 | loss: 0.11628 | val_0_rmse: 0.32949 | val_1_rmse: 0.32239 |  0:00:01s
epoch 22 | loss: 0.12507 | val_0_rmse: 0.34135 | val_1_rmse: 0.33486 |  0:00:01s
epoch 23 | loss: 0.11283 | val_0_rmse: 0.34463 | val_1_rmse: 0.33697 |  0:00:02s
epoch 24 | loss: 0.10469 | val_0_rmse: 0.33699 | val_1_rmse: 0.32788 |  0:00:02s
epoch 25 | loss: 0.10439 | val_0_rmse: 0.32593 | val_1_rmse: 0.31522 |  0:00:02s
epoch 26 | loss: 0.10522 | val_0_rmse: 0.32804 | val_1_rmse: 0.31742 |  0:00:02s
epoch 27 | loss: 0.10459 | val_0_rmse: 0.33776 | val_1_rmse: 0.32812 |  0:00:02s
epoch 28 | loss: 0.10032 | val_0_rmse: 0.33873 | val_1_rmse: 0.32891 |  0:00:02s
epoch 29 | loss: 0.10159 | val_0_rmse: 0.32689 | val_1_rmse: 0.31577 |  0:00:02s
epoch 30 | loss: 0.1008  | val_0_rmse: 0.32032 | val_1_rmse: 0.30835 |  0:00:02s
epoch 31 | loss: 0.10022 | val_0_rmse: 0.31914 | val_1_rmse: 0.30636 |  0:00:02s
epoch 32 | loss: 0.10297 | val_0_rmse: 0.32416 | val_1_rmse: 0.31198 |  0:00:02s
epoch 33 | loss: 0.09997 | val_0_rmse: 0.31757 | val_1_rmse: 0.3051  |  0:00:02s
epoch 34 | loss: 0.09992 | val_0_rmse: 0.31518 | val_1_rmse: 0.30248 |  0:00:02s
epoch 35 | loss: 0.10079 | val_0_rmse: 0.3245  | val_1_rmse: 0.31298 |  0:00:03s
epoch 36 | loss: 0.09861 | val_0_rmse: 0.32639 | val_1_rmse: 0.3152  |  0:00:03s
epoch 37 | loss: 0.09747 | val_0_rmse: 0.31449 | val_1_rmse: 0.3019  |  0:00:03s
epoch 38 | loss: 0.09975 | val_0_rmse: 0.31221 | val_1_rmse: 0.29909 |  0:00:03s
epoch 39 | loss: 0.09957 | val_0_rmse: 0.32239 | val_1_rmse: 0.31072 |  0:00:03s
epoch 40 | loss: 0.09936 | val_0_rmse: 0.32536 | val_1_rmse: 0.31389 |  0:00:03s
epoch 41 | loss: 0.09797 | val_0_rmse: 0.31469 | val_1_rmse: 0.30171 |  0:00:03s
epoch 42 | loss: 0.09538 | val_0_rmse: 0.3113  | val_1_rmse: 0.29752 |  0:00:03s
epoch 43 | loss: 0.09777 | val_0_rmse: 0.31648 | val_1_rmse: 0.30422 |  0:00:03s
epoch 44 | loss: 0.09784 | val_0_rmse: 0.32081 | val_1_rmse: 0.30967 |  0:00:03s
epoch 45 | loss: 0.09762 | val_0_rmse: 0.31254 | val_1_rmse: 0.30044 |  0:00:03s
epoch 46 | loss: 0.09622 | val_0_rmse: 0.31035 | val_1_rmse: 0.29749 |  0:00:03s
epoch 47 | loss: 0.09602 | val_0_rmse: 0.31158 | val_1_rmse: 0.29892 |  0:00:04s
epoch 48 | loss: 0.09584 | val_0_rmse: 0.31635 | val_1_rmse: 0.30435 |  0:00:04s
epoch 49 | loss: 0.09936 | val_0_rmse: 0.31348 | val_1_rmse: 0.30082 |  0:00:04s
epoch 50 | loss: 0.10067 | val_0_rmse: 0.31025 | val_1_rmse: 0.29684 |  0:00:04s
epoch 51 | loss: 0.0951  | val_0_rmse: 0.31029 | val_1_rmse: 0.29675 |  0:00:04s
epoch 52 | loss: 0.09713 | val_0_rmse: 0.31433 | val_1_rmse: 0.3015  |  0:00:04s
epoch 53 | loss: 0.09592 | val_0_rmse: 0.31423 | val_1_rmse: 0.30115 |  0:00:04s
epoch 54 | loss: 0.09487 | val_0_rmse: 0.30963 | val_1_rmse: 0.29505 |  0:00:04s
epoch 55 | loss: 0.09595 | val_0_rmse: 0.30934 | val_1_rmse: 0.29458 |  0:00:04s
epoch 56 | loss: 0.10122 | val_0_rmse: 0.30962 | val_1_rmse: 0.29542 |  0:00:04s
epoch 57 | loss: 0.09876 | val_0_rmse: 0.31112 | val_1_rmse: 0.29805 |  0:00:04s
epoch 58 | loss: 0.0979  | val_0_rmse: 0.30967 | val_1_rmse: 0.29718 |  0:00:04s
epoch 59 | loss: 0.09549 | val_0_rmse: 0.30906 | val_1_rmse: 0.2962  |  0:00:05s
epoch 60 | loss: 0.09469 | val_0_rmse: 0.31099 | val_1_rmse: 0.2988  |  0:00:05s
epoch 61 | loss: 0.0951  | val_0_rmse: 0.31201 | val_1_rmse: 0.29904 |  0:00:05s
epoch 62 | loss: 0.09423 | val_0_rmse: 0.31277 | val_1_rmse: 0.29998 |  0:00:05s
epoch 63 | loss: 0.09316 | val_0_rmse: 0.31552 | val_1_rmse: 0.30396 |  0:00:05s
epoch 64 | loss: 0.09348 | val_0_rmse: 0.31447 | val_1_rmse: 0.3029  |  0:00:05s
epoch 65 | loss: 0.09589 | val_0_rmse: 0.31478 | val_1_rmse: 0.30265 |  0:00:05s
epoch 66 | loss: 0.09391 | val_0_rmse: 0.31322 | val_1_rmse: 0.30039 |  0:00:05s
epoch 67 | loss: 0.09333 | val_0_rmse: 0.31579 | val_1_rmse: 0.30336 |  0:00:05s
epoch 68 | loss: 0.09294 | val_0_rmse: 0.3128  | val_1_rmse: 0.29934 |  0:00:05s
epoch 69 | loss: 0.09498 | val_0_rmse: 0.3108  | val_1_rmse: 0.29645 |  0:00:05s
epoch 70 | loss: 0.09366 | val_0_rmse: 0.31365 | val_1_rmse: 0.30015 |  0:00:05s
epoch 71 | loss: 0.0892  | val_0_rmse: 0.31439 | val_1_rmse: 0.30102 |  0:00:06s
epoch 72 | loss: 0.09157 | val_0_rmse: 0.31163 | val_1_rmse: 0.29753 |  0:00:06s
epoch 73 | loss: 0.09192 | val_0_rmse: 0.31134 | val_1_rmse: 0.29709 |  0:00:06s
epoch 74 | loss: 0.09201 | val_0_rmse: 0.31365 | val_1_rmse: 0.3005  |  0:00:06s
epoch 75 | loss: 0.09239 | val_0_rmse: 0.31309 | val_1_rmse: 0.2998  |  0:00:06s
epoch 76 | loss: 0.0898  | val_0_rmse: 0.31142 | val_1_rmse: 0.29754 |  0:00:06s
epoch 77 | loss: 0.0916  | val_0_rmse: 0.31322 | val_1_rmse: 0.30032 |  0:00:06s
epoch 78 | loss: 0.08955 | val_0_rmse: 0.31241 | val_1_rmse: 0.29913 |  0:00:06s
epoch 79 | loss: 0.09211 | val_0_rmse: 0.3106  | val_1_rmse: 0.29636 |  0:00:06s
epoch 80 | loss: 0.08984 | val_0_rmse: 0.31168 | val_1_rmse: 0.2978  |  0:00:06s
epoch 81 | loss: 0.0901  | val_0_rmse: 0.31366 | val_1_rmse: 0.30057 |  0:00:06s
epoch 82 | loss: 0.0888  | val_0_rmse: 0.31129 | val_1_rmse: 0.29738 |  0:00:06s
epoch 83 | loss: 0.08945 | val_0_rmse: 0.31123 | val_1_rmse: 0.29744 |  0:00:06s
epoch 84 | loss: 0.08877 | val_0_rmse: 0.31361 | val_1_rmse: 0.30089 |  0:00:07s
epoch 85 | loss: 0.08857 | val_0_rmse: 0.31118 | val_1_rmse: 0.29796 |  0:00:07s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.29458
Best weights from best epoch are automatically used!
ended training at: 21:57:08
Feature importance:
Mean squared error is of 0.07434572179463406
Mean absolute error:0.2017096639239604
MAPE:0.22980032566763833
R2 score:-0.017052510251700737
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:08
epoch 0  | loss: 3.50562 | val_0_rmse: 0.8249  | val_1_rmse: 0.81366 |  0:00:00s
epoch 1  | loss: 1.95965 | val_0_rmse: 0.60329 | val_1_rmse: 0.59488 |  0:00:00s
epoch 2  | loss: 0.8434  | val_0_rmse: 0.43873 | val_1_rmse: 0.43665 |  0:00:00s
epoch 3  | loss: 1.44794 | val_0_rmse: 0.37224 | val_1_rmse: 0.37272 |  0:00:00s
epoch 4  | loss: 1.00942 | val_0_rmse: 0.43847 | val_1_rmse: 0.4325  |  0:00:00s
epoch 5  | loss: 0.51602 | val_0_rmse: 0.49669 | val_1_rmse: 0.48814 |  0:00:00s
epoch 6  | loss: 0.37816 | val_0_rmse: 0.51387 | val_1_rmse: 0.50497 |  0:00:00s
epoch 7  | loss: 0.47532 | val_0_rmse: 0.42865 | val_1_rmse: 0.42431 |  0:00:00s
epoch 8  | loss: 0.66735 | val_0_rmse: 0.35309 | val_1_rmse: 0.35503 |  0:00:00s
epoch 9  | loss: 0.43678 | val_0_rmse: 0.33092 | val_1_rmse: 0.33664 |  0:00:00s
epoch 10 | loss: 0.32477 | val_0_rmse: 0.31903 | val_1_rmse: 0.32728 |  0:00:00s
epoch 11 | loss: 0.21131 | val_0_rmse: 0.33023 | val_1_rmse: 0.3373  |  0:00:01s
epoch 12 | loss: 0.31851 | val_0_rmse: 0.35025 | val_1_rmse: 0.35543 |  0:00:01s
epoch 13 | loss: 0.15954 | val_0_rmse: 0.38165 | val_1_rmse: 0.38493 |  0:00:01s
epoch 14 | loss: 0.21354 | val_0_rmse: 0.41054 | val_1_rmse: 0.4127  |  0:00:01s
epoch 15 | loss: 0.14744 | val_0_rmse: 0.39785 | val_1_rmse: 0.40183 |  0:00:01s
epoch 16 | loss: 0.13802 | val_0_rmse: 0.35523 | val_1_rmse: 0.36362 |  0:00:01s
epoch 17 | loss: 0.13874 | val_0_rmse: 0.33856 | val_1_rmse: 0.34901 |  0:00:01s
epoch 18 | loss: 0.13345 | val_0_rmse: 0.39017 | val_1_rmse: 0.39535 |  0:00:01s
epoch 19 | loss: 0.12298 | val_0_rmse: 0.3659  | val_1_rmse: 0.37305 |  0:00:01s
epoch 20 | loss: 0.11817 | val_0_rmse: 0.3193  | val_1_rmse: 0.33302 |  0:00:01s
epoch 21 | loss: 0.1137  | val_0_rmse: 0.322   | val_1_rmse: 0.33455 |  0:00:02s
epoch 22 | loss: 0.11077 | val_0_rmse: 0.34392 | val_1_rmse: 0.35263 |  0:00:02s
epoch 23 | loss: 0.1025  | val_0_rmse: 0.36738 | val_1_rmse: 0.37347 |  0:00:02s
epoch 24 | loss: 0.10678 | val_0_rmse: 0.34564 | val_1_rmse: 0.3548  |  0:00:02s
epoch 25 | loss: 0.10859 | val_0_rmse: 0.31054 | val_1_rmse: 0.32562 |  0:00:02s
epoch 26 | loss: 0.10054 | val_0_rmse: 0.30697 | val_1_rmse: 0.32299 |  0:00:02s
epoch 27 | loss: 0.10019 | val_0_rmse: 0.32101 | val_1_rmse: 0.33327 |  0:00:02s
epoch 28 | loss: 0.09791 | val_0_rmse: 0.31718 | val_1_rmse: 0.33003 |  0:00:02s
epoch 29 | loss: 0.09775 | val_0_rmse: 0.30349 | val_1_rmse: 0.32014 |  0:00:02s
epoch 30 | loss: 0.09466 | val_0_rmse: 0.3016  | val_1_rmse: 0.31853 |  0:00:02s
epoch 31 | loss: 0.09875 | val_0_rmse: 0.30941 | val_1_rmse: 0.32322 |  0:00:02s
epoch 32 | loss: 0.09347 | val_0_rmse: 0.31838 | val_1_rmse: 0.32949 |  0:00:02s
epoch 33 | loss: 0.09805 | val_0_rmse: 0.31048 | val_1_rmse: 0.32241 |  0:00:03s
epoch 34 | loss: 0.09493 | val_0_rmse: 0.3012  | val_1_rmse: 0.31582 |  0:00:03s
epoch 35 | loss: 0.09436 | val_0_rmse: 0.30085 | val_1_rmse: 0.3158  |  0:00:03s
epoch 36 | loss: 0.09371 | val_0_rmse: 0.30987 | val_1_rmse: 0.32177 |  0:00:03s
epoch 37 | loss: 0.09698 | val_0_rmse: 0.31406 | val_1_rmse: 0.32533 |  0:00:03s
epoch 38 | loss: 0.09453 | val_0_rmse: 0.30301 | val_1_rmse: 0.31774 |  0:00:03s
epoch 39 | loss: 0.09206 | val_0_rmse: 0.29941 | val_1_rmse: 0.31678 |  0:00:03s
epoch 40 | loss: 0.09581 | val_0_rmse: 0.30528 | val_1_rmse: 0.31936 |  0:00:03s
epoch 41 | loss: 0.09289 | val_0_rmse: 0.31927 | val_1_rmse: 0.32993 |  0:00:03s
epoch 42 | loss: 0.09258 | val_0_rmse: 0.31019 | val_1_rmse: 0.32263 |  0:00:03s
epoch 43 | loss: 0.0922  | val_0_rmse: 0.30128 | val_1_rmse: 0.31672 |  0:00:03s
epoch 44 | loss: 0.09394 | val_0_rmse: 0.30243 | val_1_rmse: 0.3173  |  0:00:03s
epoch 45 | loss: 0.09284 | val_0_rmse: 0.3101  | val_1_rmse: 0.3226  |  0:00:04s
epoch 46 | loss: 0.09211 | val_0_rmse: 0.30748 | val_1_rmse: 0.32075 |  0:00:04s
epoch 47 | loss: 0.09226 | val_0_rmse: 0.29978 | val_1_rmse: 0.31651 |  0:00:04s
epoch 48 | loss: 0.0905  | val_0_rmse: 0.29944 | val_1_rmse: 0.31643 |  0:00:04s
epoch 49 | loss: 0.0906  | val_0_rmse: 0.30418 | val_1_rmse: 0.31835 |  0:00:04s
epoch 50 | loss: 0.08814 | val_0_rmse: 0.30354 | val_1_rmse: 0.31781 |  0:00:04s
epoch 51 | loss: 0.0877  | val_0_rmse: 0.29918 | val_1_rmse: 0.31594 |  0:00:04s
epoch 52 | loss: 0.08782 | val_0_rmse: 0.29919 | val_1_rmse: 0.3158  |  0:00:04s
epoch 53 | loss: 0.0891  | val_0_rmse: 0.30601 | val_1_rmse: 0.31935 |  0:00:04s
epoch 54 | loss: 0.08796 | val_0_rmse: 0.30504 | val_1_rmse: 0.31857 |  0:00:04s
epoch 55 | loss: 0.08773 | val_0_rmse: 0.29901 | val_1_rmse: 0.31612 |  0:00:04s
epoch 56 | loss: 0.0884  | val_0_rmse: 0.29877 | val_1_rmse: 0.31709 |  0:00:04s
epoch 57 | loss: 0.08987 | val_0_rmse: 0.30182 | val_1_rmse: 0.31801 |  0:00:04s
epoch 58 | loss: 0.08881 | val_0_rmse: 0.30528 | val_1_rmse: 0.3204  |  0:00:05s
epoch 59 | loss: 0.09344 | val_0_rmse: 0.2992  | val_1_rmse: 0.31677 |  0:00:05s
epoch 60 | loss: 0.08668 | val_0_rmse: 0.2985  | val_1_rmse: 0.3171  |  0:00:05s
epoch 61 | loss: 0.08832 | val_0_rmse: 0.30048 | val_1_rmse: 0.31704 |  0:00:05s
epoch 62 | loss: 0.08822 | val_0_rmse: 0.30738 | val_1_rmse: 0.32155 |  0:00:05s
epoch 63 | loss: 0.08928 | val_0_rmse: 0.30252 | val_1_rmse: 0.31865 |  0:00:05s
epoch 64 | loss: 0.08714 | val_0_rmse: 0.29878 | val_1_rmse: 0.31758 |  0:00:05s
epoch 65 | loss: 0.09055 | val_0_rmse: 0.29959 | val_1_rmse: 0.318   |  0:00:05s
epoch 66 | loss: 0.08958 | val_0_rmse: 0.30464 | val_1_rmse: 0.32101 |  0:00:05s
epoch 67 | loss: 0.08793 | val_0_rmse: 0.30434 | val_1_rmse: 0.32116 |  0:00:05s
epoch 68 | loss: 0.08807 | val_0_rmse: 0.29941 | val_1_rmse: 0.31899 |  0:00:05s
epoch 69 | loss: 0.087   | val_0_rmse: 0.299   | val_1_rmse: 0.31952 |  0:00:05s
epoch 70 | loss: 0.08919 | val_0_rmse: 0.30097 | val_1_rmse: 0.3197  |  0:00:06s
epoch 71 | loss: 0.08843 | val_0_rmse: 0.30308 | val_1_rmse: 0.32058 |  0:00:06s
epoch 72 | loss: 0.08773 | val_0_rmse: 0.29975 | val_1_rmse: 0.31859 |  0:00:06s
epoch 73 | loss: 0.08859 | val_0_rmse: 0.29902 | val_1_rmse: 0.31876 |  0:00:06s
epoch 74 | loss: 0.08841 | val_0_rmse: 0.3008  | val_1_rmse: 0.31911 |  0:00:06s
epoch 75 | loss: 0.08696 | val_0_rmse: 0.30186 | val_1_rmse: 0.32005 |  0:00:06s
epoch 76 | loss: 0.08735 | val_0_rmse: 0.29906 | val_1_rmse: 0.31974 |  0:00:06s
epoch 77 | loss: 0.0866  | val_0_rmse: 0.29904 | val_1_rmse: 0.32042 |  0:00:06s
epoch 78 | loss: 0.08823 | val_0_rmse: 0.30094 | val_1_rmse: 0.32037 |  0:00:06s
epoch 79 | loss: 0.08767 | val_0_rmse: 0.30152 | val_1_rmse: 0.32105 |  0:00:06s
epoch 80 | loss: 0.08716 | val_0_rmse: 0.29932 | val_1_rmse: 0.32066 |  0:00:06s
epoch 81 | loss: 0.08617 | val_0_rmse: 0.29904 | val_1_rmse: 0.32121 |  0:00:06s
epoch 82 | loss: 0.08574 | val_0_rmse: 0.30035 | val_1_rmse: 0.321   |  0:00:07s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.3158
Best weights from best epoch are automatically used!
ended training at: 21:57:15
Feature importance:
Mean squared error is of 0.08348300146271485
Mean absolute error:0.20229852979067367
MAPE:0.2454305369058102
R2 score:-0.024474594821139117
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:16
epoch 0  | loss: 3.69605 | val_0_rmse: 0.57798 | val_1_rmse: 0.57026 |  0:00:00s
epoch 1  | loss: 2.19984 | val_0_rmse: 0.39536 | val_1_rmse: 0.38461 |  0:00:00s
epoch 2  | loss: 1.21633 | val_0_rmse: 0.36388 | val_1_rmse: 0.35606 |  0:00:00s
epoch 3  | loss: 1.11052 | val_0_rmse: 0.36156 | val_1_rmse: 0.35251 |  0:00:00s
epoch 4  | loss: 0.82623 | val_0_rmse: 0.4446  | val_1_rmse: 0.43771 |  0:00:00s
epoch 5  | loss: 0.37458 | val_0_rmse: 0.49874 | val_1_rmse: 0.49342 |  0:00:00s
epoch 6  | loss: 0.4346  | val_0_rmse: 0.49461 | val_1_rmse: 0.48899 |  0:00:00s
epoch 7  | loss: 0.40618 | val_0_rmse: 0.38706 | val_1_rmse: 0.3834  |  0:00:00s
epoch 8  | loss: 0.32874 | val_0_rmse: 0.31605 | val_1_rmse: 0.31223 |  0:00:00s
epoch 9  | loss: 0.52385 | val_0_rmse: 0.30507 | val_1_rmse: 0.30116 |  0:00:00s
epoch 10 | loss: 0.54102 | val_0_rmse: 0.3155  | val_1_rmse: 0.31063 |  0:00:00s
epoch 11 | loss: 0.44147 | val_0_rmse: 0.31299 | val_1_rmse: 0.30606 |  0:00:01s
epoch 12 | loss: 0.26392 | val_0_rmse: 0.38011 | val_1_rmse: 0.37468 |  0:00:01s
epoch 13 | loss: 0.22028 | val_0_rmse: 0.39834 | val_1_rmse: 0.39313 |  0:00:01s
epoch 14 | loss: 0.19933 | val_0_rmse: 0.35772 | val_1_rmse: 0.35053 |  0:00:01s
epoch 15 | loss: 0.17259 | val_0_rmse: 0.31989 | val_1_rmse: 0.31051 |  0:00:01s
epoch 16 | loss: 0.159   | val_0_rmse: 0.32661 | val_1_rmse: 0.31625 |  0:00:01s
epoch 17 | loss: 0.16036 | val_0_rmse: 0.34146 | val_1_rmse: 0.33058 |  0:00:01s
epoch 18 | loss: 0.11684 | val_0_rmse: 0.34068 | val_1_rmse: 0.33084 |  0:00:01s
epoch 19 | loss: 0.12224 | val_0_rmse: 0.3258  | val_1_rmse: 0.31545 |  0:00:01s
epoch 20 | loss: 0.1183  | val_0_rmse: 0.31403 | val_1_rmse: 0.30356 |  0:00:01s
epoch 21 | loss: 0.11008 | val_0_rmse: 0.31435 | val_1_rmse: 0.30374 |  0:00:01s
epoch 22 | loss: 0.10749 | val_0_rmse: 0.32495 | val_1_rmse: 0.31464 |  0:00:01s
epoch 23 | loss: 0.10165 | val_0_rmse: 0.33955 | val_1_rmse: 0.32913 |  0:00:01s
epoch 24 | loss: 0.09947 | val_0_rmse: 0.3348  | val_1_rmse: 0.32441 |  0:00:02s
epoch 25 | loss: 0.10489 | val_0_rmse: 0.3198  | val_1_rmse: 0.31017 |  0:00:02s
epoch 26 | loss: 0.10364 | val_0_rmse: 0.30926 | val_1_rmse: 0.30152 |  0:00:02s
epoch 27 | loss: 0.10024 | val_0_rmse: 0.30827 | val_1_rmse: 0.30191 |  0:00:02s
epoch 28 | loss: 0.10135 | val_0_rmse: 0.31263 | val_1_rmse: 0.30634 |  0:00:02s
epoch 29 | loss: 0.10322 | val_0_rmse: 0.31154 | val_1_rmse: 0.30448 |  0:00:02s
epoch 30 | loss: 0.09521 | val_0_rmse: 0.31105 | val_1_rmse: 0.30266 |  0:00:02s
epoch 31 | loss: 0.09916 | val_0_rmse: 0.31192 | val_1_rmse: 0.30121 |  0:00:02s
epoch 32 | loss: 0.09312 | val_0_rmse: 0.31178 | val_1_rmse: 0.29997 |  0:00:02s
epoch 33 | loss: 0.10235 | val_0_rmse: 0.31072 | val_1_rmse: 0.29745 |  0:00:02s
epoch 34 | loss: 0.09171 | val_0_rmse: 0.31111 | val_1_rmse: 0.29721 |  0:00:02s
epoch 35 | loss: 0.10919 | val_0_rmse: 0.31267 | val_1_rmse: 0.30066 |  0:00:02s
epoch 36 | loss: 0.09924 | val_0_rmse: 0.30871 | val_1_rmse: 0.29828 |  0:00:03s
epoch 37 | loss: 0.09346 | val_0_rmse: 0.30545 | val_1_rmse: 0.29509 |  0:00:03s
epoch 38 | loss: 0.09361 | val_0_rmse: 0.3058  | val_1_rmse: 0.29586 |  0:00:03s
epoch 39 | loss: 0.09564 | val_0_rmse: 0.30822 | val_1_rmse: 0.29874 |  0:00:03s
epoch 40 | loss: 0.09339 | val_0_rmse: 0.30974 | val_1_rmse: 0.30161 |  0:00:03s
epoch 41 | loss: 0.09518 | val_0_rmse: 0.30891 | val_1_rmse: 0.30094 |  0:00:03s
epoch 42 | loss: 0.09591 | val_0_rmse: 0.30521 | val_1_rmse: 0.29696 |  0:00:03s
epoch 43 | loss: 0.09322 | val_0_rmse: 0.30456 | val_1_rmse: 0.29638 |  0:00:03s
epoch 44 | loss: 0.09515 | val_0_rmse: 0.30507 | val_1_rmse: 0.2979  |  0:00:03s
epoch 45 | loss: 0.09232 | val_0_rmse: 0.30843 | val_1_rmse: 0.30145 |  0:00:03s
epoch 46 | loss: 0.09467 | val_0_rmse: 0.30594 | val_1_rmse: 0.29863 |  0:00:03s
epoch 47 | loss: 0.09509 | val_0_rmse: 0.3044  | val_1_rmse: 0.29527 |  0:00:03s
epoch 48 | loss: 0.09422 | val_0_rmse: 0.30436 | val_1_rmse: 0.29359 |  0:00:04s
epoch 49 | loss: 0.09428 | val_0_rmse: 0.31203 | val_1_rmse: 0.30279 |  0:00:04s
epoch 50 | loss: 0.09408 | val_0_rmse: 0.31781 | val_1_rmse: 0.31016 |  0:00:04s
epoch 51 | loss: 0.09569 | val_0_rmse: 0.30617 | val_1_rmse: 0.29791 |  0:00:04s
epoch 52 | loss: 0.09154 | val_0_rmse: 0.30164 | val_1_rmse: 0.29361 |  0:00:04s
epoch 53 | loss: 0.09402 | val_0_rmse: 0.30453 | val_1_rmse: 0.2961  |  0:00:04s
epoch 54 | loss: 0.09243 | val_0_rmse: 0.3116  | val_1_rmse: 0.30357 |  0:00:04s
epoch 55 | loss: 0.09342 | val_0_rmse: 0.30663 | val_1_rmse: 0.29843 |  0:00:04s
epoch 56 | loss: 0.09234 | val_0_rmse: 0.30194 | val_1_rmse: 0.29281 |  0:00:04s
epoch 57 | loss: 0.09396 | val_0_rmse: 0.30209 | val_1_rmse: 0.29253 |  0:00:04s
epoch 58 | loss: 0.09308 | val_0_rmse: 0.30682 | val_1_rmse: 0.29739 |  0:00:04s
epoch 59 | loss: 0.09473 | val_0_rmse: 0.30913 | val_1_rmse: 0.29952 |  0:00:04s
epoch 60 | loss: 0.09494 | val_0_rmse: 0.30271 | val_1_rmse: 0.29227 |  0:00:05s
epoch 61 | loss: 0.09075 | val_0_rmse: 0.30382 | val_1_rmse: 0.29329 |  0:00:05s
epoch 62 | loss: 0.09622 | val_0_rmse: 0.30286 | val_1_rmse: 0.29306 |  0:00:05s
epoch 63 | loss: 0.09302 | val_0_rmse: 0.31068 | val_1_rmse: 0.30221 |  0:00:05s
epoch 64 | loss: 0.09306 | val_0_rmse: 0.30687 | val_1_rmse: 0.29785 |  0:00:05s
epoch 65 | loss: 0.09085 | val_0_rmse: 0.30264 | val_1_rmse: 0.29259 |  0:00:05s
epoch 66 | loss: 0.09222 | val_0_rmse: 0.30295 | val_1_rmse: 0.29279 |  0:00:05s
epoch 67 | loss: 0.09175 | val_0_rmse: 0.3041  | val_1_rmse: 0.29609 |  0:00:05s
epoch 68 | loss: 0.09014 | val_0_rmse: 0.3064  | val_1_rmse: 0.29917 |  0:00:05s
epoch 69 | loss: 0.09287 | val_0_rmse: 0.30313 | val_1_rmse: 0.29552 |  0:00:05s
epoch 70 | loss: 0.09103 | val_0_rmse: 0.30405 | val_1_rmse: 0.29615 |  0:00:05s
epoch 71 | loss: 0.0924  | val_0_rmse: 0.30301 | val_1_rmse: 0.29532 |  0:00:05s
epoch 72 | loss: 0.09019 | val_0_rmse: 0.30324 | val_1_rmse: 0.29576 |  0:00:06s
epoch 73 | loss: 0.08998 | val_0_rmse: 0.3027  | val_1_rmse: 0.29502 |  0:00:06s
epoch 74 | loss: 0.09046 | val_0_rmse: 0.30246 | val_1_rmse: 0.29455 |  0:00:06s
epoch 75 | loss: 0.09213 | val_0_rmse: 0.30293 | val_1_rmse: 0.29536 |  0:00:06s
epoch 76 | loss: 0.09259 | val_0_rmse: 0.30267 | val_1_rmse: 0.29559 |  0:00:06s
epoch 77 | loss: 0.08941 | val_0_rmse: 0.30345 | val_1_rmse: 0.29638 |  0:00:06s
epoch 78 | loss: 0.08908 | val_0_rmse: 0.30242 | val_1_rmse: 0.2953  |  0:00:06s
epoch 79 | loss: 0.09203 | val_0_rmse: 0.30253 | val_1_rmse: 0.2956  |  0:00:06s
epoch 80 | loss: 0.09063 | val_0_rmse: 0.30307 | val_1_rmse: 0.29633 |  0:00:06s
epoch 81 | loss: 0.08958 | val_0_rmse: 0.3031  | val_1_rmse: 0.29674 |  0:00:06s
epoch 82 | loss: 0.0913  | val_0_rmse: 0.30294 | val_1_rmse: 0.29673 |  0:00:06s
epoch 83 | loss: 0.09027 | val_0_rmse: 0.30349 | val_1_rmse: 0.29739 |  0:00:06s
epoch 84 | loss: 0.08936 | val_0_rmse: 0.30334 | val_1_rmse: 0.29745 |  0:00:07s
epoch 85 | loss: 0.08927 | val_0_rmse: 0.30339 | val_1_rmse: 0.29791 |  0:00:07s
epoch 86 | loss: 0.08931 | val_0_rmse: 0.30344 | val_1_rmse: 0.29792 |  0:00:07s
epoch 87 | loss: 0.09146 | val_0_rmse: 0.30287 | val_1_rmse: 0.297   |  0:00:07s
epoch 88 | loss: 0.08928 | val_0_rmse: 0.30262 | val_1_rmse: 0.29658 |  0:00:07s
epoch 89 | loss: 0.09122 | val_0_rmse: 0.30418 | val_1_rmse: 0.29809 |  0:00:07s
epoch 90 | loss: 0.08916 | val_0_rmse: 0.30508 | val_1_rmse: 0.29882 |  0:00:07s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.29227
Best weights from best epoch are automatically used!
ended training at: 21:57:23
Feature importance:
Mean squared error is of 0.09026487796332486
Mean absolute error:0.21478268519447136
MAPE:0.24284565429823776
R2 score:-0.018506331990592928
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:57:24
epoch 0  | loss: 2.36773 | val_0_rmse: 0.31646 | val_1_rmse: 0.32123 |  0:00:00s
epoch 1  | loss: 0.70824 | val_0_rmse: 0.52905 | val_1_rmse: 0.54266 |  0:00:00s
epoch 2  | loss: 0.20743 | val_0_rmse: 0.43802 | val_1_rmse: 0.44932 |  0:00:00s
epoch 3  | loss: 0.12058 | val_0_rmse: 0.39284 | val_1_rmse: 0.40489 |  0:00:01s
epoch 4  | loss: 0.10723 | val_0_rmse: 0.32842 | val_1_rmse: 0.33899 |  0:00:01s
epoch 5  | loss: 0.0998  | val_0_rmse: 0.29508 | val_1_rmse: 0.30382 |  0:00:01s
epoch 6  | loss: 0.09505 | val_0_rmse: 0.31151 | val_1_rmse: 0.32076 |  0:00:02s
epoch 7  | loss: 0.09459 | val_0_rmse: 0.29929 | val_1_rmse: 0.30779 |  0:00:02s
epoch 8  | loss: 0.09145 | val_0_rmse: 0.28624 | val_1_rmse: 0.29305 |  0:00:02s
epoch 9  | loss: 0.08739 | val_0_rmse: 0.28995 | val_1_rmse: 0.29982 |  0:00:02s
epoch 10 | loss: 0.08224 | val_0_rmse: 0.27841 | val_1_rmse: 0.28843 |  0:00:03s
epoch 11 | loss: 0.07876 | val_0_rmse: 0.27166 | val_1_rmse: 0.28032 |  0:00:03s
epoch 12 | loss: 0.07729 | val_0_rmse: 0.27452 | val_1_rmse: 0.28277 |  0:00:03s
epoch 13 | loss: 0.07782 | val_0_rmse: 0.28172 | val_1_rmse: 0.29075 |  0:00:04s
epoch 14 | loss: 0.07677 | val_0_rmse: 0.26977 | val_1_rmse: 0.27822 |  0:00:04s
epoch 15 | loss: 0.07719 | val_0_rmse: 0.28259 | val_1_rmse: 0.29239 |  0:00:04s
epoch 16 | loss: 0.07705 | val_0_rmse: 0.27088 | val_1_rmse: 0.27929 |  0:00:04s
epoch 17 | loss: 0.07669 | val_0_rmse: 0.26966 | val_1_rmse: 0.2779  |  0:00:05s
epoch 18 | loss: 0.0775  | val_0_rmse: 0.28129 | val_1_rmse: 0.2908  |  0:00:05s
epoch 19 | loss: 0.07648 | val_0_rmse: 0.27365 | val_1_rmse: 0.28231 |  0:00:05s
epoch 20 | loss: 0.07788 | val_0_rmse: 0.27167 | val_1_rmse: 0.27912 |  0:00:06s
epoch 21 | loss: 0.07792 | val_0_rmse: 0.28481 | val_1_rmse: 0.29547 |  0:00:06s
epoch 22 | loss: 0.07747 | val_0_rmse: 0.26954 | val_1_rmse: 0.27895 |  0:00:06s
epoch 23 | loss: 0.07588 | val_0_rmse: 0.27142 | val_1_rmse: 0.28236 |  0:00:06s
epoch 24 | loss: 0.07524 | val_0_rmse: 0.27246 | val_1_rmse: 0.28404 |  0:00:07s
epoch 25 | loss: 0.07506 | val_0_rmse: 0.26806 | val_1_rmse: 0.27858 |  0:00:07s
epoch 26 | loss: 0.07415 | val_0_rmse: 0.28046 | val_1_rmse: 0.29142 |  0:00:07s
epoch 27 | loss: 0.0745  | val_0_rmse: 0.26909 | val_1_rmse: 0.27871 |  0:00:08s
epoch 28 | loss: 0.07385 | val_0_rmse: 0.27989 | val_1_rmse: 0.28983 |  0:00:08s
epoch 29 | loss: 0.07445 | val_0_rmse: 0.27179 | val_1_rmse: 0.281   |  0:00:08s
epoch 30 | loss: 0.07397 | val_0_rmse: 0.27456 | val_1_rmse: 0.28534 |  0:00:08s
epoch 31 | loss: 0.0733  | val_0_rmse: 0.2698  | val_1_rmse: 0.28045 |  0:00:09s
epoch 32 | loss: 0.07334 | val_0_rmse: 0.268   | val_1_rmse: 0.27811 |  0:00:09s
epoch 33 | loss: 0.0727  | val_0_rmse: 0.2709  | val_1_rmse: 0.28196 |  0:00:09s
epoch 34 | loss: 0.073   | val_0_rmse: 0.27055 | val_1_rmse: 0.28122 |  0:00:09s
epoch 35 | loss: 0.07281 | val_0_rmse: 0.26999 | val_1_rmse: 0.28055 |  0:00:10s
epoch 36 | loss: 0.07274 | val_0_rmse: 0.27665 | val_1_rmse: 0.28884 |  0:00:10s
epoch 37 | loss: 0.07379 | val_0_rmse: 0.26852 | val_1_rmse: 0.27863 |  0:00:10s
epoch 38 | loss: 0.07239 | val_0_rmse: 0.27188 | val_1_rmse: 0.28255 |  0:00:11s
epoch 39 | loss: 0.07249 | val_0_rmse: 0.27248 | val_1_rmse: 0.28244 |  0:00:11s
epoch 40 | loss: 0.07381 | val_0_rmse: 0.2713  | val_1_rmse: 0.27963 |  0:00:11s
epoch 41 | loss: 0.0746  | val_0_rmse: 0.26887 | val_1_rmse: 0.27757 |  0:00:11s
epoch 42 | loss: 0.0725  | val_0_rmse: 0.26979 | val_1_rmse: 0.27951 |  0:00:12s
epoch 43 | loss: 0.07309 | val_0_rmse: 0.27065 | val_1_rmse: 0.28191 |  0:00:12s
epoch 44 | loss: 0.0728  | val_0_rmse: 0.26855 | val_1_rmse: 0.27945 |  0:00:12s
epoch 45 | loss: 0.07233 | val_0_rmse: 0.26708 | val_1_rmse: 0.277   |  0:00:13s
epoch 46 | loss: 0.07271 | val_0_rmse: 0.27309 | val_1_rmse: 0.28395 |  0:00:13s
epoch 47 | loss: 0.07217 | val_0_rmse: 0.26704 | val_1_rmse: 0.27718 |  0:00:13s
epoch 48 | loss: 0.07338 | val_0_rmse: 0.27356 | val_1_rmse: 0.28537 |  0:00:13s
epoch 49 | loss: 0.07336 | val_0_rmse: 0.27154 | val_1_rmse: 0.2833  |  0:00:14s
epoch 50 | loss: 0.07263 | val_0_rmse: 0.26924 | val_1_rmse: 0.27976 |  0:00:14s
epoch 51 | loss: 0.0741  | val_0_rmse: 0.27427 | val_1_rmse: 0.2857  |  0:00:14s
epoch 52 | loss: 0.07391 | val_0_rmse: 0.273   | val_1_rmse: 0.28361 |  0:00:15s
epoch 53 | loss: 0.07357 | val_0_rmse: 0.26935 | val_1_rmse: 0.27925 |  0:00:15s
epoch 54 | loss: 0.07273 | val_0_rmse: 0.27179 | val_1_rmse: 0.28279 |  0:00:15s
epoch 55 | loss: 0.07237 | val_0_rmse: 0.271   | val_1_rmse: 0.28144 |  0:00:15s
epoch 56 | loss: 0.07327 | val_0_rmse: 0.26864 | val_1_rmse: 0.27833 |  0:00:16s
epoch 57 | loss: 0.07212 | val_0_rmse: 0.2729  | val_1_rmse: 0.28463 |  0:00:16s
epoch 58 | loss: 0.07111 | val_0_rmse: 0.27011 | val_1_rmse: 0.28136 |  0:00:16s
epoch 59 | loss: 0.07123 | val_0_rmse: 0.26776 | val_1_rmse: 0.27791 |  0:00:17s
epoch 60 | loss: 0.07186 | val_0_rmse: 0.2803  | val_1_rmse: 0.29179 |  0:00:17s
epoch 61 | loss: 0.07258 | val_0_rmse: 0.26861 | val_1_rmse: 0.27783 |  0:00:17s
epoch 62 | loss: 0.07327 | val_0_rmse: 0.27126 | val_1_rmse: 0.28181 |  0:00:17s
epoch 63 | loss: 0.07112 | val_0_rmse: 0.26821 | val_1_rmse: 0.27937 |  0:00:18s
epoch 64 | loss: 0.07267 | val_0_rmse: 0.26768 | val_1_rmse: 0.27767 |  0:00:18s
epoch 65 | loss: 0.07167 | val_0_rmse: 0.27641 | val_1_rmse: 0.2871  |  0:00:18s
epoch 66 | loss: 0.07222 | val_0_rmse: 0.26855 | val_1_rmse: 0.27999 |  0:00:19s
epoch 67 | loss: 0.07299 | val_0_rmse: 0.26749 | val_1_rmse: 0.27915 |  0:00:19s
epoch 68 | loss: 0.07081 | val_0_rmse: 0.27001 | val_1_rmse: 0.28212 |  0:00:19s
epoch 69 | loss: 0.07163 | val_0_rmse: 0.26679 | val_1_rmse: 0.27815 |  0:00:19s
epoch 70 | loss: 0.07124 | val_0_rmse: 0.26542 | val_1_rmse: 0.27697 |  0:00:20s
epoch 71 | loss: 0.07029 | val_0_rmse: 0.26588 | val_1_rmse: 0.27679 |  0:00:20s
epoch 72 | loss: 0.07065 | val_0_rmse: 0.26814 | val_1_rmse: 0.28019 |  0:00:20s
epoch 73 | loss: 0.07112 | val_0_rmse: 0.26742 | val_1_rmse: 0.27918 |  0:00:21s
epoch 74 | loss: 0.07213 | val_0_rmse: 0.26493 | val_1_rmse: 0.27726 |  0:00:21s
epoch 75 | loss: 0.07102 | val_0_rmse: 0.26664 | val_1_rmse: 0.27774 |  0:00:21s
epoch 76 | loss: 0.07168 | val_0_rmse: 0.26816 | val_1_rmse: 0.27963 |  0:00:21s
epoch 77 | loss: 0.07218 | val_0_rmse: 0.26726 | val_1_rmse: 0.27972 |  0:00:22s
epoch 78 | loss: 0.07137 | val_0_rmse: 0.27316 | val_1_rmse: 0.28674 |  0:00:22s
epoch 79 | loss: 0.07088 | val_0_rmse: 0.274   | val_1_rmse: 0.28458 |  0:00:22s
epoch 80 | loss: 0.07383 | val_0_rmse: 0.27612 | val_1_rmse: 0.2891  |  0:00:23s
epoch 81 | loss: 0.07352 | val_0_rmse: 0.26523 | val_1_rmse: 0.27863 |  0:00:23s
epoch 82 | loss: 0.07408 | val_0_rmse: 0.26675 | val_1_rmse: 0.2793  |  0:00:23s
epoch 83 | loss: 0.07171 | val_0_rmse: 0.26561 | val_1_rmse: 0.279   |  0:00:23s
epoch 84 | loss: 0.07092 | val_0_rmse: 0.26413 | val_1_rmse: 0.27562 |  0:00:24s
epoch 85 | loss: 0.07015 | val_0_rmse: 0.26482 | val_1_rmse: 0.27638 |  0:00:24s
epoch 86 | loss: 0.07058 | val_0_rmse: 0.26501 | val_1_rmse: 0.27541 |  0:00:24s
epoch 87 | loss: 0.07029 | val_0_rmse: 0.26409 | val_1_rmse: 0.27447 |  0:00:25s
epoch 88 | loss: 0.07188 | val_0_rmse: 0.26558 | val_1_rmse: 0.2757  |  0:00:25s
epoch 89 | loss: 0.06985 | val_0_rmse: 0.26249 | val_1_rmse: 0.27175 |  0:00:25s
epoch 90 | loss: 0.07059 | val_0_rmse: 0.26261 | val_1_rmse: 0.27222 |  0:00:25s
epoch 91 | loss: 0.07061 | val_0_rmse: 0.26596 | val_1_rmse: 0.27714 |  0:00:26s
epoch 92 | loss: 0.07063 | val_0_rmse: 0.26265 | val_1_rmse: 0.27222 |  0:00:26s
epoch 93 | loss: 0.07096 | val_0_rmse: 0.26224 | val_1_rmse: 0.27217 |  0:00:26s
epoch 94 | loss: 0.0693  | val_0_rmse: 0.26152 | val_1_rmse: 0.27155 |  0:00:26s
epoch 95 | loss: 0.06992 | val_0_rmse: 0.26236 | val_1_rmse: 0.27315 |  0:00:27s
epoch 96 | loss: 0.06928 | val_0_rmse: 0.26103 | val_1_rmse: 0.27247 |  0:00:27s
epoch 97 | loss: 0.0688  | val_0_rmse: 0.26125 | val_1_rmse: 0.27223 |  0:00:27s
epoch 98 | loss: 0.06883 | val_0_rmse: 0.26107 | val_1_rmse: 0.27176 |  0:00:28s
epoch 99 | loss: 0.06896 | val_0_rmse: 0.26343 | val_1_rmse: 0.27599 |  0:00:28s
epoch 100| loss: 0.06893 | val_0_rmse: 0.26254 | val_1_rmse: 0.27345 |  0:00:28s
epoch 101| loss: 0.07042 | val_0_rmse: 0.26133 | val_1_rmse: 0.27208 |  0:00:28s
epoch 102| loss: 0.06887 | val_0_rmse: 0.26288 | val_1_rmse: 0.27615 |  0:00:29s
epoch 103| loss: 0.07093 | val_0_rmse: 0.26163 | val_1_rmse: 0.27422 |  0:00:29s
epoch 104| loss: 0.06975 | val_0_rmse: 0.26273 | val_1_rmse: 0.27449 |  0:00:29s
epoch 105| loss: 0.06838 | val_0_rmse: 0.26009 | val_1_rmse: 0.27273 |  0:00:30s
epoch 106| loss: 0.06879 | val_0_rmse: 0.26042 | val_1_rmse: 0.27295 |  0:00:30s
epoch 107| loss: 0.06885 | val_0_rmse: 0.26575 | val_1_rmse: 0.27954 |  0:00:30s
epoch 108| loss: 0.06945 | val_0_rmse: 0.26118 | val_1_rmse: 0.27368 |  0:00:30s
epoch 109| loss: 0.06889 | val_0_rmse: 0.26077 | val_1_rmse: 0.27285 |  0:00:31s
epoch 110| loss: 0.06837 | val_0_rmse: 0.25959 | val_1_rmse: 0.27239 |  0:00:31s
epoch 111| loss: 0.06833 | val_0_rmse: 0.25996 | val_1_rmse: 0.27271 |  0:00:31s
epoch 112| loss: 0.06968 | val_0_rmse: 0.26365 | val_1_rmse: 0.27696 |  0:00:32s
epoch 113| loss: 0.06904 | val_0_rmse: 0.25943 | val_1_rmse: 0.27141 |  0:00:32s
epoch 114| loss: 0.06842 | val_0_rmse: 0.26158 | val_1_rmse: 0.27391 |  0:00:32s
epoch 115| loss: 0.06753 | val_0_rmse: 0.26014 | val_1_rmse: 0.27227 |  0:00:32s
epoch 116| loss: 0.06694 | val_0_rmse: 0.26054 | val_1_rmse: 0.27412 |  0:00:33s
epoch 117| loss: 0.06784 | val_0_rmse: 0.25917 | val_1_rmse: 0.27251 |  0:00:33s
epoch 118| loss: 0.06797 | val_0_rmse: 0.26519 | val_1_rmse: 0.27912 |  0:00:33s
epoch 119| loss: 0.06964 | val_0_rmse: 0.25992 | val_1_rmse: 0.27481 |  0:00:33s
epoch 120| loss: 0.06862 | val_0_rmse: 0.25985 | val_1_rmse: 0.27501 |  0:00:34s
epoch 121| loss: 0.06842 | val_0_rmse: 0.25926 | val_1_rmse: 0.27345 |  0:00:34s
epoch 122| loss: 0.06887 | val_0_rmse: 0.25796 | val_1_rmse: 0.27239 |  0:00:34s
epoch 123| loss: 0.06817 | val_0_rmse: 0.25942 | val_1_rmse: 0.2735  |  0:00:35s
epoch 124| loss: 0.06737 | val_0_rmse: 0.25782 | val_1_rmse: 0.27281 |  0:00:35s
epoch 125| loss: 0.06724 | val_0_rmse: 0.25737 | val_1_rmse: 0.27307 |  0:00:35s
epoch 126| loss: 0.0673  | val_0_rmse: 0.25663 | val_1_rmse: 0.27267 |  0:00:36s
epoch 127| loss: 0.06734 | val_0_rmse: 0.2585  | val_1_rmse: 0.2736  |  0:00:36s
epoch 128| loss: 0.06803 | val_0_rmse: 0.26248 | val_1_rmse: 0.27971 |  0:00:36s
epoch 129| loss: 0.06882 | val_0_rmse: 0.25774 | val_1_rmse: 0.27409 |  0:00:36s
epoch 130| loss: 0.06764 | val_0_rmse: 0.25864 | val_1_rmse: 0.27351 |  0:00:37s
epoch 131| loss: 0.06752 | val_0_rmse: 0.2562  | val_1_rmse: 0.2722  |  0:00:37s
epoch 132| loss: 0.06807 | val_0_rmse: 0.25641 | val_1_rmse: 0.27162 |  0:00:37s
epoch 133| loss: 0.06656 | val_0_rmse: 0.25589 | val_1_rmse: 0.27179 |  0:00:38s
epoch 134| loss: 0.06599 | val_0_rmse: 0.25617 | val_1_rmse: 0.2727  |  0:00:38s
epoch 135| loss: 0.06733 | val_0_rmse: 0.25618 | val_1_rmse: 0.27354 |  0:00:38s
epoch 136| loss: 0.06653 | val_0_rmse: 0.25693 | val_1_rmse: 0.27364 |  0:00:38s
epoch 137| loss: 0.06754 | val_0_rmse: 0.25589 | val_1_rmse: 0.27301 |  0:00:39s
epoch 138| loss: 0.06743 | val_0_rmse: 0.25895 | val_1_rmse: 0.27584 |  0:00:39s
epoch 139| loss: 0.0685  | val_0_rmse: 0.2591  | val_1_rmse: 0.27288 |  0:00:39s
epoch 140| loss: 0.06747 | val_0_rmse: 0.26156 | val_1_rmse: 0.27687 |  0:00:39s
epoch 141| loss: 0.06737 | val_0_rmse: 0.25817 | val_1_rmse: 0.27352 |  0:00:40s
epoch 142| loss: 0.06677 | val_0_rmse: 0.25725 | val_1_rmse: 0.27363 |  0:00:40s
epoch 143| loss: 0.06662 | val_0_rmse: 0.25565 | val_1_rmse: 0.2728  |  0:00:40s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.27141
Best weights from best epoch are automatically used!
ended training at: 21:58:05
Feature importance:
Mean squared error is of 0.06244945943182236
Mean absolute error:0.19065631902041066
MAPE:0.22318334847009008
R2 score:0.23715107645804978
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:58:05
epoch 0  | loss: 2.32057 | val_0_rmse: 0.39391 | val_1_rmse: 0.38367 |  0:00:00s
epoch 1  | loss: 0.46461 | val_0_rmse: 0.44262 | val_1_rmse: 0.43522 |  0:00:00s
epoch 2  | loss: 0.1573  | val_0_rmse: 0.33773 | val_1_rmse: 0.32989 |  0:00:00s
epoch 3  | loss: 0.11194 | val_0_rmse: 0.31369 | val_1_rmse: 0.3053  |  0:00:01s
epoch 4  | loss: 0.1041  | val_0_rmse: 0.32512 | val_1_rmse: 0.31561 |  0:00:01s
epoch 5  | loss: 0.09745 | val_0_rmse: 0.30881 | val_1_rmse: 0.29621 |  0:00:01s
epoch 6  | loss: 0.09571 | val_0_rmse: 0.30202 | val_1_rmse: 0.29193 |  0:00:01s
epoch 7  | loss: 0.09495 | val_0_rmse: 0.30673 | val_1_rmse: 0.29733 |  0:00:02s
epoch 8  | loss: 0.09385 | val_0_rmse: 0.30167 | val_1_rmse: 0.29056 |  0:00:02s
epoch 9  | loss: 0.09245 | val_0_rmse: 0.30038 | val_1_rmse: 0.28932 |  0:00:02s
epoch 10 | loss: 0.09147 | val_0_rmse: 0.30057 | val_1_rmse: 0.29001 |  0:00:03s
epoch 11 | loss: 0.09131 | val_0_rmse: 0.29902 | val_1_rmse: 0.28793 |  0:00:03s
epoch 12 | loss: 0.08995 | val_0_rmse: 0.29847 | val_1_rmse: 0.28688 |  0:00:03s
epoch 13 | loss: 0.08966 | val_0_rmse: 0.29987 | val_1_rmse: 0.28805 |  0:00:03s
epoch 14 | loss: 0.09036 | val_0_rmse: 0.29843 | val_1_rmse: 0.28622 |  0:00:04s
epoch 15 | loss: 0.08923 | val_0_rmse: 0.29819 | val_1_rmse: 0.28592 |  0:00:04s
epoch 16 | loss: 0.09008 | val_0_rmse: 0.29918 | val_1_rmse: 0.28679 |  0:00:04s
epoch 17 | loss: 0.08933 | val_0_rmse: 0.29964 | val_1_rmse: 0.28707 |  0:00:05s
epoch 18 | loss: 0.08877 | val_0_rmse: 0.29961 | val_1_rmse: 0.2873  |  0:00:05s
epoch 19 | loss: 0.0891  | val_0_rmse: 0.30013 | val_1_rmse: 0.28745 |  0:00:05s
epoch 20 | loss: 0.08892 | val_0_rmse: 0.2973  | val_1_rmse: 0.28471 |  0:00:05s
epoch 21 | loss: 0.08856 | val_0_rmse: 0.29571 | val_1_rmse: 0.28309 |  0:00:06s
epoch 22 | loss: 0.08943 | val_0_rmse: 0.2965  | val_1_rmse: 0.28384 |  0:00:06s
epoch 23 | loss: 0.08894 | val_0_rmse: 0.29924 | val_1_rmse: 0.2864  |  0:00:06s
epoch 24 | loss: 0.08937 | val_0_rmse: 0.29499 | val_1_rmse: 0.28204 |  0:00:07s
epoch 25 | loss: 0.08848 | val_0_rmse: 0.29363 | val_1_rmse: 0.28107 |  0:00:07s
epoch 26 | loss: 0.08739 | val_0_rmse: 0.29769 | val_1_rmse: 0.28528 |  0:00:07s
epoch 27 | loss: 0.08726 | val_0_rmse: 0.29106 | val_1_rmse: 0.27826 |  0:00:07s
epoch 28 | loss: 0.08631 | val_0_rmse: 0.29177 | val_1_rmse: 0.27956 |  0:00:08s
epoch 29 | loss: 0.08585 | val_0_rmse: 0.30116 | val_1_rmse: 0.28887 |  0:00:08s
epoch 30 | loss: 0.08688 | val_0_rmse: 0.28875 | val_1_rmse: 0.27608 |  0:00:08s
epoch 31 | loss: 0.08462 | val_0_rmse: 0.29322 | val_1_rmse: 0.28184 |  0:00:09s
epoch 32 | loss: 0.08428 | val_0_rmse: 0.29162 | val_1_rmse: 0.28041 |  0:00:09s
epoch 33 | loss: 0.08304 | val_0_rmse: 0.29049 | val_1_rmse: 0.27944 |  0:00:09s
epoch 34 | loss: 0.08323 | val_0_rmse: 0.29389 | val_1_rmse: 0.28427 |  0:00:10s
epoch 35 | loss: 0.08239 | val_0_rmse: 0.28731 | val_1_rmse: 0.27705 |  0:00:10s
epoch 36 | loss: 0.08115 | val_0_rmse: 0.28853 | val_1_rmse: 0.27855 |  0:00:10s
epoch 37 | loss: 0.081   | val_0_rmse: 0.28956 | val_1_rmse: 0.28035 |  0:00:10s
epoch 38 | loss: 0.08031 | val_0_rmse: 0.28912 | val_1_rmse: 0.28011 |  0:00:11s
epoch 39 | loss: 0.08042 | val_0_rmse: 0.28712 | val_1_rmse: 0.27832 |  0:00:11s
epoch 40 | loss: 0.07948 | val_0_rmse: 0.28052 | val_1_rmse: 0.26943 |  0:00:11s
epoch 41 | loss: 0.08007 | val_0_rmse: 0.28368 | val_1_rmse: 0.27595 |  0:00:12s
epoch 42 | loss: 0.07991 | val_0_rmse: 0.28454 | val_1_rmse: 0.27399 |  0:00:12s
epoch 43 | loss: 0.07928 | val_0_rmse: 0.27676 | val_1_rmse: 0.26541 |  0:00:12s
epoch 44 | loss: 0.07955 | val_0_rmse: 0.28112 | val_1_rmse: 0.27155 |  0:00:12s
epoch 45 | loss: 0.07666 | val_0_rmse: 0.27934 | val_1_rmse: 0.27025 |  0:00:13s
epoch 46 | loss: 0.07716 | val_0_rmse: 0.27756 | val_1_rmse: 0.2692  |  0:00:13s
epoch 47 | loss: 0.07533 | val_0_rmse: 0.27818 | val_1_rmse: 0.2706  |  0:00:13s
epoch 48 | loss: 0.07553 | val_0_rmse: 0.28943 | val_1_rmse: 0.28238 |  0:00:14s
epoch 49 | loss: 0.07673 | val_0_rmse: 0.28177 | val_1_rmse: 0.27226 |  0:00:14s
epoch 50 | loss: 0.07595 | val_0_rmse: 0.27558 | val_1_rmse: 0.26627 |  0:00:14s
epoch 51 | loss: 0.07423 | val_0_rmse: 0.27381 | val_1_rmse: 0.26182 |  0:00:14s
epoch 52 | loss: 0.07417 | val_0_rmse: 0.27261 | val_1_rmse: 0.26131 |  0:00:15s
epoch 53 | loss: 0.07347 | val_0_rmse: 0.27396 | val_1_rmse: 0.26234 |  0:00:15s
epoch 54 | loss: 0.07286 | val_0_rmse: 0.27678 | val_1_rmse: 0.2644  |  0:00:15s
epoch 55 | loss: 0.07265 | val_0_rmse: 0.27717 | val_1_rmse: 0.2651  |  0:00:15s
epoch 56 | loss: 0.07303 | val_0_rmse: 0.27459 | val_1_rmse: 0.26352 |  0:00:16s
epoch 57 | loss: 0.07209 | val_0_rmse: 0.2708  | val_1_rmse: 0.26147 |  0:00:16s
epoch 58 | loss: 0.07201 | val_0_rmse: 0.27227 | val_1_rmse: 0.26256 |  0:00:16s
epoch 59 | loss: 0.07217 | val_0_rmse: 0.27279 | val_1_rmse: 0.26382 |  0:00:17s
epoch 60 | loss: 0.07021 | val_0_rmse: 0.26875 | val_1_rmse: 0.25936 |  0:00:17s
epoch 61 | loss: 0.07073 | val_0_rmse: 0.27133 | val_1_rmse: 0.26252 |  0:00:17s
epoch 62 | loss: 0.07074 | val_0_rmse: 0.26772 | val_1_rmse: 0.2582  |  0:00:17s
epoch 63 | loss: 0.07038 | val_0_rmse: 0.27103 | val_1_rmse: 0.26235 |  0:00:18s
epoch 64 | loss: 0.071   | val_0_rmse: 0.2738  | val_1_rmse: 0.2662  |  0:00:18s
epoch 65 | loss: 0.07159 | val_0_rmse: 0.26677 | val_1_rmse: 0.25813 |  0:00:18s
epoch 66 | loss: 0.06908 | val_0_rmse: 0.26687 | val_1_rmse: 0.25782 |  0:00:19s
epoch 67 | loss: 0.06965 | val_0_rmse: 0.26536 | val_1_rmse: 0.2549  |  0:00:19s
epoch 68 | loss: 0.07019 | val_0_rmse: 0.26228 | val_1_rmse: 0.2518  |  0:00:19s
epoch 69 | loss: 0.06875 | val_0_rmse: 0.26343 | val_1_rmse: 0.25472 |  0:00:19s
epoch 70 | loss: 0.06926 | val_0_rmse: 0.26687 | val_1_rmse: 0.25911 |  0:00:20s
epoch 71 | loss: 0.06871 | val_0_rmse: 0.26564 | val_1_rmse: 0.2568  |  0:00:20s
epoch 72 | loss: 0.07047 | val_0_rmse: 0.26502 | val_1_rmse: 0.25713 |  0:00:20s
epoch 73 | loss: 0.07104 | val_0_rmse: 0.26279 | val_1_rmse: 0.25464 |  0:00:21s
epoch 74 | loss: 0.06993 | val_0_rmse: 0.26167 | val_1_rmse: 0.25273 |  0:00:21s
epoch 75 | loss: 0.06883 | val_0_rmse: 0.26232 | val_1_rmse: 0.25342 |  0:00:21s
epoch 76 | loss: 0.06938 | val_0_rmse: 0.26236 | val_1_rmse: 0.25581 |  0:00:21s
epoch 77 | loss: 0.06907 | val_0_rmse: 0.26032 | val_1_rmse: 0.25471 |  0:00:22s
epoch 78 | loss: 0.0685  | val_0_rmse: 0.25986 | val_1_rmse: 0.25401 |  0:00:22s
epoch 79 | loss: 0.06778 | val_0_rmse: 0.2594  | val_1_rmse: 0.2539  |  0:00:22s
epoch 80 | loss: 0.068   | val_0_rmse: 0.26697 | val_1_rmse: 0.26269 |  0:00:23s
epoch 81 | loss: 0.06993 | val_0_rmse: 0.26549 | val_1_rmse: 0.26277 |  0:00:23s
epoch 82 | loss: 0.06922 | val_0_rmse: 0.26002 | val_1_rmse: 0.25618 |  0:00:23s
epoch 83 | loss: 0.06848 | val_0_rmse: 0.25907 | val_1_rmse: 0.25532 |  0:00:23s
epoch 84 | loss: 0.06796 | val_0_rmse: 0.26397 | val_1_rmse: 0.25993 |  0:00:24s
epoch 85 | loss: 0.06649 | val_0_rmse: 0.25975 | val_1_rmse: 0.25352 |  0:00:24s
epoch 86 | loss: 0.06758 | val_0_rmse: 0.25895 | val_1_rmse: 0.25163 |  0:00:24s
epoch 87 | loss: 0.06823 | val_0_rmse: 0.25897 | val_1_rmse: 0.25373 |  0:00:24s
epoch 88 | loss: 0.06797 | val_0_rmse: 0.25729 | val_1_rmse: 0.25116 |  0:00:25s
epoch 89 | loss: 0.06642 | val_0_rmse: 0.25755 | val_1_rmse: 0.25126 |  0:00:25s
epoch 90 | loss: 0.06657 | val_0_rmse: 0.25825 | val_1_rmse: 0.25404 |  0:00:25s
epoch 91 | loss: 0.0665  | val_0_rmse: 0.25959 | val_1_rmse: 0.25472 |  0:00:26s
epoch 92 | loss: 0.0689  | val_0_rmse: 0.25476 | val_1_rmse: 0.2506  |  0:00:26s
epoch 93 | loss: 0.06742 | val_0_rmse: 0.25615 | val_1_rmse: 0.25304 |  0:00:26s
epoch 94 | loss: 0.0665  | val_0_rmse: 0.25533 | val_1_rmse: 0.25123 |  0:00:27s
epoch 95 | loss: 0.06545 | val_0_rmse: 0.2536  | val_1_rmse: 0.2493  |  0:00:27s
epoch 96 | loss: 0.06476 | val_0_rmse: 0.25466 | val_1_rmse: 0.25171 |  0:00:27s
epoch 97 | loss: 0.06515 | val_0_rmse: 0.25378 | val_1_rmse: 0.25015 |  0:00:27s
epoch 98 | loss: 0.06452 | val_0_rmse: 0.25896 | val_1_rmse: 0.25925 |  0:00:28s
epoch 99 | loss: 0.06493 | val_0_rmse: 0.25843 | val_1_rmse: 0.25376 |  0:00:28s
epoch 100| loss: 0.06748 | val_0_rmse: 0.25241 | val_1_rmse: 0.2494  |  0:00:28s
epoch 101| loss: 0.0664  | val_0_rmse: 0.26144 | val_1_rmse: 0.26194 |  0:00:29s
epoch 102| loss: 0.06682 | val_0_rmse: 0.25139 | val_1_rmse: 0.24944 |  0:00:29s
epoch 103| loss: 0.06407 | val_0_rmse: 0.25132 | val_1_rmse: 0.25155 |  0:00:29s
epoch 104| loss: 0.06514 | val_0_rmse: 0.25035 | val_1_rmse: 0.25251 |  0:00:29s
epoch 105| loss: 0.06445 | val_0_rmse: 0.2503  | val_1_rmse: 0.25403 |  0:00:30s
epoch 106| loss: 0.0632  | val_0_rmse: 0.24956 | val_1_rmse: 0.25315 |  0:00:30s
epoch 107| loss: 0.06376 | val_0_rmse: 0.25042 | val_1_rmse: 0.25593 |  0:00:30s
epoch 108| loss: 0.06464 | val_0_rmse: 0.2551  | val_1_rmse: 0.26082 |  0:00:31s
epoch 109| loss: 0.06681 | val_0_rmse: 0.25571 | val_1_rmse: 0.25769 |  0:00:31s
epoch 110| loss: 0.06702 | val_0_rmse: 0.25689 | val_1_rmse: 0.25969 |  0:00:31s
epoch 111| loss: 0.06567 | val_0_rmse: 0.25168 | val_1_rmse: 0.25266 |  0:00:31s
epoch 112| loss: 0.06626 | val_0_rmse: 0.25009 | val_1_rmse: 0.25475 |  0:00:32s
epoch 113| loss: 0.0637  | val_0_rmse: 0.25043 | val_1_rmse: 0.25666 |  0:00:32s
epoch 114| loss: 0.06377 | val_0_rmse: 0.24933 | val_1_rmse: 0.25681 |  0:00:32s
epoch 115| loss: 0.06236 | val_0_rmse: 0.24923 | val_1_rmse: 0.25532 |  0:00:32s
epoch 116| loss: 0.06382 | val_0_rmse: 0.24872 | val_1_rmse: 0.25377 |  0:00:33s
epoch 117| loss: 0.06322 | val_0_rmse: 0.2479  | val_1_rmse: 0.25166 |  0:00:33s
epoch 118| loss: 0.06335 | val_0_rmse: 0.24842 | val_1_rmse: 0.2533  |  0:00:33s
epoch 119| loss: 0.06284 | val_0_rmse: 0.24792 | val_1_rmse: 0.25486 |  0:00:34s
epoch 120| loss: 0.06395 | val_0_rmse: 0.24686 | val_1_rmse: 0.25355 |  0:00:34s
epoch 121| loss: 0.06208 | val_0_rmse: 0.24741 | val_1_rmse: 0.2578  |  0:00:34s
epoch 122| loss: 0.0614  | val_0_rmse: 0.24573 | val_1_rmse: 0.25626 |  0:00:34s
epoch 123| loss: 0.06244 | val_0_rmse: 0.246   | val_1_rmse: 0.25704 |  0:00:35s
epoch 124| loss: 0.06153 | val_0_rmse: 0.24562 | val_1_rmse: 0.25667 |  0:00:35s
epoch 125| loss: 0.06081 | val_0_rmse: 0.24566 | val_1_rmse: 0.26104 |  0:00:35s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.2493
Best weights from best epoch are automatically used!
ended training at: 21:58:41
Feature importance:
Mean squared error is of 0.07718430735741316
Mean absolute error:0.1970330807628423
MAPE:0.21417481658165152
R2 score:0.16388978121171782
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:58:42
epoch 0  | loss: 2.14253 | val_0_rmse: 0.35892 | val_1_rmse: 0.3563  |  0:00:00s
epoch 1  | loss: 0.34474 | val_0_rmse: 0.46058 | val_1_rmse: 0.43676 |  0:00:00s
epoch 2  | loss: 0.19354 | val_0_rmse: 0.32596 | val_1_rmse: 0.31135 |  0:00:00s
epoch 3  | loss: 0.14892 | val_0_rmse: 0.33514 | val_1_rmse: 0.32055 |  0:00:01s
epoch 4  | loss: 0.11426 | val_0_rmse: 0.33473 | val_1_rmse: 0.32236 |  0:00:01s
epoch 5  | loss: 0.1065  | val_0_rmse: 0.32874 | val_1_rmse: 0.31707 |  0:00:01s
epoch 6  | loss: 0.09927 | val_0_rmse: 0.31473 | val_1_rmse: 0.30794 |  0:00:01s
epoch 7  | loss: 0.0958  | val_0_rmse: 0.3215  | val_1_rmse: 0.31229 |  0:00:02s
epoch 8  | loss: 0.09624 | val_0_rmse: 0.31159 | val_1_rmse: 0.30531 |  0:00:02s
epoch 9  | loss: 0.09401 | val_0_rmse: 0.32548 | val_1_rmse: 0.31474 |  0:00:02s
epoch 10 | loss: 0.09526 | val_0_rmse: 0.30639 | val_1_rmse: 0.30211 |  0:00:03s
epoch 11 | loss: 0.09334 | val_0_rmse: 0.31786 | val_1_rmse: 0.30817 |  0:00:03s
epoch 12 | loss: 0.09213 | val_0_rmse: 0.30811 | val_1_rmse: 0.30168 |  0:00:03s
epoch 13 | loss: 0.09169 | val_0_rmse: 0.31144 | val_1_rmse: 0.30354 |  0:00:03s
epoch 14 | loss: 0.09003 | val_0_rmse: 0.30717 | val_1_rmse: 0.30133 |  0:00:04s
epoch 15 | loss: 0.09184 | val_0_rmse: 0.31077 | val_1_rmse: 0.30286 |  0:00:04s
epoch 16 | loss: 0.0919  | val_0_rmse: 0.3051  | val_1_rmse: 0.30047 |  0:00:04s
epoch 17 | loss: 0.09148 | val_0_rmse: 0.31046 | val_1_rmse: 0.30282 |  0:00:05s
epoch 18 | loss: 0.09087 | val_0_rmse: 0.30506 | val_1_rmse: 0.30045 |  0:00:05s
epoch 19 | loss: 0.09108 | val_0_rmse: 0.3074  | val_1_rmse: 0.30078 |  0:00:05s
epoch 20 | loss: 0.0911  | val_0_rmse: 0.30776 | val_1_rmse: 0.30176 |  0:00:06s
epoch 21 | loss: 0.0911  | val_0_rmse: 0.30395 | val_1_rmse: 0.29971 |  0:00:06s
epoch 22 | loss: 0.09087 | val_0_rmse: 0.3094  | val_1_rmse: 0.3015  |  0:00:06s
epoch 23 | loss: 0.09127 | val_0_rmse: 0.30208 | val_1_rmse: 0.29839 |  0:00:06s
epoch 24 | loss: 0.09094 | val_0_rmse: 0.30739 | val_1_rmse: 0.29958 |  0:00:07s
epoch 25 | loss: 0.09083 | val_0_rmse: 0.3074  | val_1_rmse: 0.29955 |  0:00:07s
epoch 26 | loss: 0.09038 | val_0_rmse: 0.30391 | val_1_rmse: 0.29774 |  0:00:07s
epoch 27 | loss: 0.09    | val_0_rmse: 0.30833 | val_1_rmse: 0.29982 |  0:00:08s
epoch 28 | loss: 0.09064 | val_0_rmse: 0.30835 | val_1_rmse: 0.2998  |  0:00:08s
epoch 29 | loss: 0.09022 | val_0_rmse: 0.30115 | val_1_rmse: 0.29561 |  0:00:08s
epoch 30 | loss: 0.08827 | val_0_rmse: 0.31129 | val_1_rmse: 0.30215 |  0:00:08s
epoch 31 | loss: 0.08707 | val_0_rmse: 0.29842 | val_1_rmse: 0.29488 |  0:00:09s
epoch 32 | loss: 0.08647 | val_0_rmse: 0.29985 | val_1_rmse: 0.29477 |  0:00:09s
epoch 33 | loss: 0.08505 | val_0_rmse: 0.30333 | val_1_rmse: 0.29554 |  0:00:09s
epoch 34 | loss: 0.08415 | val_0_rmse: 0.29273 | val_1_rmse: 0.28674 |  0:00:10s
epoch 35 | loss: 0.08312 | val_0_rmse: 0.30735 | val_1_rmse: 0.29445 |  0:00:10s
epoch 36 | loss: 0.08181 | val_0_rmse: 0.28444 | val_1_rmse: 0.27653 |  0:00:10s
epoch 37 | loss: 0.08156 | val_0_rmse: 0.28708 | val_1_rmse: 0.27662 |  0:00:10s
epoch 38 | loss: 0.08081 | val_0_rmse: 0.30375 | val_1_rmse: 0.28944 |  0:00:11s
epoch 39 | loss: 0.07965 | val_0_rmse: 0.28025 | val_1_rmse: 0.27166 |  0:00:11s
epoch 40 | loss: 0.07962 | val_0_rmse: 0.27773 | val_1_rmse: 0.26913 |  0:00:11s
epoch 41 | loss: 0.07801 | val_0_rmse: 0.29424 | val_1_rmse: 0.2816  |  0:00:12s
epoch 42 | loss: 0.07627 | val_0_rmse: 0.28131 | val_1_rmse: 0.27267 |  0:00:12s
epoch 43 | loss: 0.07598 | val_0_rmse: 0.28495 | val_1_rmse: 0.2752  |  0:00:12s
epoch 44 | loss: 0.07614 | val_0_rmse: 0.29289 | val_1_rmse: 0.28095 |  0:00:12s
epoch 45 | loss: 0.07697 | val_0_rmse: 0.27284 | val_1_rmse: 0.2705  |  0:00:13s
epoch 46 | loss: 0.07662 | val_0_rmse: 0.29682 | val_1_rmse: 0.28557 |  0:00:13s
epoch 47 | loss: 0.07592 | val_0_rmse: 0.26907 | val_1_rmse: 0.26744 |  0:00:13s
epoch 48 | loss: 0.07441 | val_0_rmse: 0.2793  | val_1_rmse: 0.27295 |  0:00:14s
epoch 49 | loss: 0.07345 | val_0_rmse: 0.27269 | val_1_rmse: 0.2681  |  0:00:14s
epoch 50 | loss: 0.07374 | val_0_rmse: 0.26792 | val_1_rmse: 0.26455 |  0:00:14s
epoch 51 | loss: 0.07304 | val_0_rmse: 0.28164 | val_1_rmse: 0.27335 |  0:00:14s
epoch 52 | loss: 0.07573 | val_0_rmse: 0.26588 | val_1_rmse: 0.26435 |  0:00:15s
epoch 53 | loss: 0.07077 | val_0_rmse: 0.27967 | val_1_rmse: 0.27243 |  0:00:15s
epoch 54 | loss: 0.07173 | val_0_rmse: 0.26578 | val_1_rmse: 0.26457 |  0:00:15s
epoch 55 | loss: 0.071   | val_0_rmse: 0.27691 | val_1_rmse: 0.26797 |  0:00:15s
epoch 56 | loss: 0.07055 | val_0_rmse: 0.27398 | val_1_rmse: 0.26623 |  0:00:16s
epoch 57 | loss: 0.06975 | val_0_rmse: 0.26522 | val_1_rmse: 0.26242 |  0:00:16s
epoch 58 | loss: 0.07025 | val_0_rmse: 0.27703 | val_1_rmse: 0.26895 |  0:00:16s
epoch 59 | loss: 0.06845 | val_0_rmse: 0.26479 | val_1_rmse: 0.26404 |  0:00:17s
epoch 60 | loss: 0.06914 | val_0_rmse: 0.27095 | val_1_rmse: 0.2655  |  0:00:17s
epoch 61 | loss: 0.06817 | val_0_rmse: 0.26614 | val_1_rmse: 0.26351 |  0:00:17s
epoch 62 | loss: 0.06788 | val_0_rmse: 0.26366 | val_1_rmse: 0.2632  |  0:00:17s
epoch 63 | loss: 0.06909 | val_0_rmse: 0.27624 | val_1_rmse: 0.26996 |  0:00:18s
epoch 64 | loss: 0.06971 | val_0_rmse: 0.26326 | val_1_rmse: 0.26364 |  0:00:18s
epoch 65 | loss: 0.06727 | val_0_rmse: 0.2707  | val_1_rmse: 0.2671  |  0:00:18s
epoch 66 | loss: 0.06802 | val_0_rmse: 0.26083 | val_1_rmse: 0.26338 |  0:00:19s
epoch 67 | loss: 0.06879 | val_0_rmse: 0.27732 | val_1_rmse: 0.271   |  0:00:19s
epoch 68 | loss: 0.06749 | val_0_rmse: 0.26168 | val_1_rmse: 0.26386 |  0:00:19s
epoch 69 | loss: 0.06854 | val_0_rmse: 0.26297 | val_1_rmse: 0.25995 |  0:00:19s
epoch 70 | loss: 0.06757 | val_0_rmse: 0.26311 | val_1_rmse: 0.26154 |  0:00:20s
epoch 71 | loss: 0.06735 | val_0_rmse: 0.26324 | val_1_rmse: 0.26513 |  0:00:20s
epoch 72 | loss: 0.06695 | val_0_rmse: 0.26452 | val_1_rmse: 0.26384 |  0:00:20s
epoch 73 | loss: 0.06732 | val_0_rmse: 0.26149 | val_1_rmse: 0.26162 |  0:00:21s
epoch 74 | loss: 0.06706 | val_0_rmse: 0.26466 | val_1_rmse: 0.26345 |  0:00:21s
epoch 75 | loss: 0.06667 | val_0_rmse: 0.26139 | val_1_rmse: 0.26496 |  0:00:21s
epoch 76 | loss: 0.06722 | val_0_rmse: 0.26321 | val_1_rmse: 0.26531 |  0:00:21s
epoch 77 | loss: 0.06618 | val_0_rmse: 0.26066 | val_1_rmse: 0.2655  |  0:00:22s
epoch 78 | loss: 0.06636 | val_0_rmse: 0.26051 | val_1_rmse: 0.26243 |  0:00:22s
epoch 79 | loss: 0.06598 | val_0_rmse: 0.26071 | val_1_rmse: 0.26226 |  0:00:22s
epoch 80 | loss: 0.06674 | val_0_rmse: 0.26386 | val_1_rmse: 0.26275 |  0:00:23s
epoch 81 | loss: 0.06608 | val_0_rmse: 0.26225 | val_1_rmse: 0.2614  |  0:00:23s
epoch 82 | loss: 0.06642 | val_0_rmse: 0.25935 | val_1_rmse: 0.26054 |  0:00:23s
epoch 83 | loss: 0.06597 | val_0_rmse: 0.26199 | val_1_rmse: 0.26244 |  0:00:24s
epoch 84 | loss: 0.06632 | val_0_rmse: 0.25761 | val_1_rmse: 0.26355 |  0:00:24s
epoch 85 | loss: 0.06646 | val_0_rmse: 0.25963 | val_1_rmse: 0.26162 |  0:00:24s
epoch 86 | loss: 0.06578 | val_0_rmse: 0.26087 | val_1_rmse: 0.26295 |  0:00:24s
epoch 87 | loss: 0.0692  | val_0_rmse: 0.26632 | val_1_rmse: 0.27102 |  0:00:25s
epoch 88 | loss: 0.07073 | val_0_rmse: 0.26302 | val_1_rmse: 0.26906 |  0:00:25s
epoch 89 | loss: 0.06941 | val_0_rmse: 0.26466 | val_1_rmse: 0.2676  |  0:00:25s
epoch 90 | loss: 0.06762 | val_0_rmse: 0.25904 | val_1_rmse: 0.26343 |  0:00:25s
epoch 91 | loss: 0.06617 | val_0_rmse: 0.25957 | val_1_rmse: 0.26361 |  0:00:26s
epoch 92 | loss: 0.06659 | val_0_rmse: 0.2595  | val_1_rmse: 0.26435 |  0:00:26s
epoch 93 | loss: 0.06638 | val_0_rmse: 0.25607 | val_1_rmse: 0.26527 |  0:00:26s
epoch 94 | loss: 0.06792 | val_0_rmse: 0.25534 | val_1_rmse: 0.26143 |  0:00:27s
epoch 95 | loss: 0.06519 | val_0_rmse: 0.25633 | val_1_rmse: 0.26158 |  0:00:27s
epoch 96 | loss: 0.06605 | val_0_rmse: 0.25588 | val_1_rmse: 0.26168 |  0:00:27s
epoch 97 | loss: 0.06566 | val_0_rmse: 0.25459 | val_1_rmse: 0.26548 |  0:00:27s
epoch 98 | loss: 0.0668  | val_0_rmse: 0.25534 | val_1_rmse: 0.26688 |  0:00:28s
epoch 99 | loss: 0.0657  | val_0_rmse: 0.2543  | val_1_rmse: 0.26278 |  0:00:28s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.25995
Best weights from best epoch are automatically used!
ended training at: 21:59:10
Feature importance:
Mean squared error is of 0.06970747568589178
Mean absolute error:0.19211770127157884
MAPE:0.20903512883719963
R2 score:0.14604027168629397
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:59:11
epoch 0  | loss: 2.15891 | val_0_rmse: 0.3466  | val_1_rmse: 0.33904 |  0:00:00s
epoch 1  | loss: 0.59471 | val_0_rmse: 0.39918 | val_1_rmse: 0.39071 |  0:00:00s
epoch 2  | loss: 0.2806  | val_0_rmse: 0.35485 | val_1_rmse: 0.34772 |  0:00:00s
epoch 3  | loss: 0.18804 | val_0_rmse: 0.36505 | val_1_rmse: 0.36219 |  0:00:01s
epoch 4  | loss: 0.12888 | val_0_rmse: 0.34462 | val_1_rmse: 0.3372  |  0:00:01s
epoch 5  | loss: 0.11036 | val_0_rmse: 0.32302 | val_1_rmse: 0.31241 |  0:00:01s
epoch 6  | loss: 0.10233 | val_0_rmse: 0.31478 | val_1_rmse: 0.30427 |  0:00:01s
epoch 7  | loss: 0.09782 | val_0_rmse: 0.29952 | val_1_rmse: 0.28757 |  0:00:02s
epoch 8  | loss: 0.09847 | val_0_rmse: 0.29886 | val_1_rmse: 0.28581 |  0:00:02s
epoch 9  | loss: 0.09228 | val_0_rmse: 0.30284 | val_1_rmse: 0.29084 |  0:00:02s
epoch 10 | loss: 0.0918  | val_0_rmse: 0.29878 | val_1_rmse: 0.2859  |  0:00:03s
epoch 11 | loss: 0.09306 | val_0_rmse: 0.30272 | val_1_rmse: 0.2892  |  0:00:03s
epoch 12 | loss: 0.09053 | val_0_rmse: 0.29687 | val_1_rmse: 0.28328 |  0:00:03s
epoch 13 | loss: 0.09288 | val_0_rmse: 0.30219 | val_1_rmse: 0.28849 |  0:00:03s
epoch 14 | loss: 0.09166 | val_0_rmse: 0.29728 | val_1_rmse: 0.28431 |  0:00:04s
epoch 15 | loss: 0.08972 | val_0_rmse: 0.29669 | val_1_rmse: 0.28418 |  0:00:04s
epoch 16 | loss: 0.08938 | val_0_rmse: 0.29619 | val_1_rmse: 0.28378 |  0:00:04s
epoch 17 | loss: 0.08913 | val_0_rmse: 0.29879 | val_1_rmse: 0.28681 |  0:00:05s
epoch 18 | loss: 0.09116 | val_0_rmse: 0.29511 | val_1_rmse: 0.28238 |  0:00:05s
epoch 19 | loss: 0.08997 | val_0_rmse: 0.30015 | val_1_rmse: 0.28709 |  0:00:05s
epoch 20 | loss: 0.08989 | val_0_rmse: 0.29604 | val_1_rmse: 0.28269 |  0:00:05s
epoch 21 | loss: 0.08814 | val_0_rmse: 0.29369 | val_1_rmse: 0.28033 |  0:00:06s
epoch 22 | loss: 0.08839 | val_0_rmse: 0.29222 | val_1_rmse: 0.27802 |  0:00:06s
epoch 23 | loss: 0.08883 | val_0_rmse: 0.29717 | val_1_rmse: 0.28364 |  0:00:06s
epoch 24 | loss: 0.08933 | val_0_rmse: 0.2923  | val_1_rmse: 0.27871 |  0:00:07s
epoch 25 | loss: 0.08726 | val_0_rmse: 0.29175 | val_1_rmse: 0.27862 |  0:00:07s
epoch 26 | loss: 0.08811 | val_0_rmse: 0.30086 | val_1_rmse: 0.28673 |  0:00:07s
epoch 27 | loss: 0.08915 | val_0_rmse: 0.2907  | val_1_rmse: 0.27594 |  0:00:07s
epoch 28 | loss: 0.08688 | val_0_rmse: 0.29009 | val_1_rmse: 0.27612 |  0:00:08s
epoch 29 | loss: 0.08536 | val_0_rmse: 0.29182 | val_1_rmse: 0.27852 |  0:00:08s
epoch 30 | loss: 0.08618 | val_0_rmse: 0.29066 | val_1_rmse: 0.27715 |  0:00:08s
epoch 31 | loss: 0.08424 | val_0_rmse: 0.28821 | val_1_rmse: 0.27529 |  0:00:08s
epoch 32 | loss: 0.08407 | val_0_rmse: 0.28936 | val_1_rmse: 0.27638 |  0:00:09s
epoch 33 | loss: 0.0837  | val_0_rmse: 0.28806 | val_1_rmse: 0.27506 |  0:00:09s
epoch 34 | loss: 0.08265 | val_0_rmse: 0.28668 | val_1_rmse: 0.2745  |  0:00:10s
epoch 35 | loss: 0.08526 | val_0_rmse: 0.28678 | val_1_rmse: 0.27376 |  0:00:10s
epoch 36 | loss: 0.0823  | val_0_rmse: 0.28428 | val_1_rmse: 0.2715  |  0:00:10s
epoch 37 | loss: 0.08172 | val_0_rmse: 0.28433 | val_1_rmse: 0.27153 |  0:00:10s
epoch 38 | loss: 0.08116 | val_0_rmse: 0.29027 | val_1_rmse: 0.27667 |  0:00:11s
epoch 39 | loss: 0.08164 | val_0_rmse: 0.28693 | val_1_rmse: 0.27319 |  0:00:11s
epoch 40 | loss: 0.08268 | val_0_rmse: 0.28341 | val_1_rmse: 0.26997 |  0:00:11s
epoch 41 | loss: 0.08016 | val_0_rmse: 0.2823  | val_1_rmse: 0.26865 |  0:00:11s
epoch 42 | loss: 0.08136 | val_0_rmse: 0.29273 | val_1_rmse: 0.27897 |  0:00:12s
epoch 43 | loss: 0.08101 | val_0_rmse: 0.28413 | val_1_rmse: 0.27076 |  0:00:12s
epoch 44 | loss: 0.0803  | val_0_rmse: 0.2835  | val_1_rmse: 0.27101 |  0:00:12s
epoch 45 | loss: 0.08187 | val_0_rmse: 0.28984 | val_1_rmse: 0.27664 |  0:00:13s
epoch 46 | loss: 0.08137 | val_0_rmse: 0.29151 | val_1_rmse: 0.27804 |  0:00:13s
epoch 47 | loss: 0.08295 | val_0_rmse: 0.28506 | val_1_rmse: 0.2713  |  0:00:13s
epoch 48 | loss: 0.08157 | val_0_rmse: 0.29366 | val_1_rmse: 0.28045 |  0:00:13s
epoch 49 | loss: 0.07978 | val_0_rmse: 0.27607 | val_1_rmse: 0.26313 |  0:00:14s
epoch 50 | loss: 0.07931 | val_0_rmse: 0.27617 | val_1_rmse: 0.26271 |  0:00:14s
epoch 51 | loss: 0.07903 | val_0_rmse: 0.2799  | val_1_rmse: 0.26656 |  0:00:14s
epoch 52 | loss: 0.07703 | val_0_rmse: 0.27439 | val_1_rmse: 0.26225 |  0:00:15s
epoch 53 | loss: 0.07532 | val_0_rmse: 0.27976 | val_1_rmse: 0.26819 |  0:00:15s
epoch 54 | loss: 0.0752  | val_0_rmse: 0.27427 | val_1_rmse: 0.26406 |  0:00:15s
epoch 55 | loss: 0.07559 | val_0_rmse: 0.2768  | val_1_rmse: 0.26445 |  0:00:15s
epoch 56 | loss: 0.07385 | val_0_rmse: 0.27168 | val_1_rmse: 0.25846 |  0:00:16s
epoch 57 | loss: 0.07362 | val_0_rmse: 0.27159 | val_1_rmse: 0.25794 |  0:00:16s
epoch 58 | loss: 0.0733  | val_0_rmse: 0.27494 | val_1_rmse: 0.26136 |  0:00:16s
epoch 59 | loss: 0.07364 | val_0_rmse: 0.26983 | val_1_rmse: 0.2568  |  0:00:17s
epoch 60 | loss: 0.074   | val_0_rmse: 0.28375 | val_1_rmse: 0.27059 |  0:00:17s
epoch 61 | loss: 0.07397 | val_0_rmse: 0.26876 | val_1_rmse: 0.25699 |  0:00:17s
epoch 62 | loss: 0.07258 | val_0_rmse: 0.26917 | val_1_rmse: 0.25759 |  0:00:17s
epoch 63 | loss: 0.07136 | val_0_rmse: 0.2693  | val_1_rmse: 0.25717 |  0:00:18s
epoch 64 | loss: 0.07096 | val_0_rmse: 0.26839 | val_1_rmse: 0.25632 |  0:00:18s
epoch 65 | loss: 0.07088 | val_0_rmse: 0.26703 | val_1_rmse: 0.25418 |  0:00:18s
epoch 66 | loss: 0.06967 | val_0_rmse: 0.26642 | val_1_rmse: 0.25378 |  0:00:19s
epoch 67 | loss: 0.06944 | val_0_rmse: 0.26505 | val_1_rmse: 0.25332 |  0:00:19s
epoch 68 | loss: 0.0688  | val_0_rmse: 0.26646 | val_1_rmse: 0.25445 |  0:00:19s
epoch 69 | loss: 0.06946 | val_0_rmse: 0.26466 | val_1_rmse: 0.25361 |  0:00:19s
epoch 70 | loss: 0.06921 | val_0_rmse: 0.27017 | val_1_rmse: 0.25902 |  0:00:20s
epoch 71 | loss: 0.06947 | val_0_rmse: 0.26348 | val_1_rmse: 0.25449 |  0:00:20s
epoch 72 | loss: 0.06845 | val_0_rmse: 0.26502 | val_1_rmse: 0.2561  |  0:00:20s
epoch 73 | loss: 0.06981 | val_0_rmse: 0.26522 | val_1_rmse: 0.25614 |  0:00:21s
epoch 74 | loss: 0.06959 | val_0_rmse: 0.26342 | val_1_rmse: 0.25503 |  0:00:21s
epoch 75 | loss: 0.07026 | val_0_rmse: 0.27177 | val_1_rmse: 0.26079 |  0:00:21s
epoch 76 | loss: 0.06944 | val_0_rmse: 0.26379 | val_1_rmse: 0.25491 |  0:00:21s
epoch 77 | loss: 0.06939 | val_0_rmse: 0.26744 | val_1_rmse: 0.25762 |  0:00:22s
epoch 78 | loss: 0.06824 | val_0_rmse: 0.26064 | val_1_rmse: 0.25188 |  0:00:22s
epoch 79 | loss: 0.06675 | val_0_rmse: 0.26362 | val_1_rmse: 0.25571 |  0:00:22s
epoch 80 | loss: 0.06695 | val_0_rmse: 0.26058 | val_1_rmse: 0.2538  |  0:00:22s
epoch 81 | loss: 0.06811 | val_0_rmse: 0.2713  | val_1_rmse: 0.26253 |  0:00:23s
epoch 82 | loss: 0.06915 | val_0_rmse: 0.25923 | val_1_rmse: 0.2535  |  0:00:23s
epoch 83 | loss: 0.06694 | val_0_rmse: 0.26955 | val_1_rmse: 0.26103 |  0:00:23s
epoch 84 | loss: 0.06721 | val_0_rmse: 0.25791 | val_1_rmse: 0.25123 |  0:00:24s
epoch 85 | loss: 0.06693 | val_0_rmse: 0.26334 | val_1_rmse: 0.25536 |  0:00:24s
epoch 86 | loss: 0.06661 | val_0_rmse: 0.25722 | val_1_rmse: 0.25138 |  0:00:24s
epoch 87 | loss: 0.06826 | val_0_rmse: 0.25763 | val_1_rmse: 0.25194 |  0:00:24s
epoch 88 | loss: 0.06603 | val_0_rmse: 0.25814 | val_1_rmse: 0.25196 |  0:00:25s
epoch 89 | loss: 0.06593 | val_0_rmse: 0.25656 | val_1_rmse: 0.24979 |  0:00:25s
epoch 90 | loss: 0.06535 | val_0_rmse: 0.25774 | val_1_rmse: 0.25    |  0:00:25s
epoch 91 | loss: 0.06543 | val_0_rmse: 0.25557 | val_1_rmse: 0.24825 |  0:00:26s
epoch 92 | loss: 0.06553 | val_0_rmse: 0.25716 | val_1_rmse: 0.24976 |  0:00:26s
epoch 93 | loss: 0.06535 | val_0_rmse: 0.25599 | val_1_rmse: 0.24959 |  0:00:26s
epoch 94 | loss: 0.06576 | val_0_rmse: 0.25537 | val_1_rmse: 0.25043 |  0:00:26s
epoch 95 | loss: 0.06604 | val_0_rmse: 0.26031 | val_1_rmse: 0.2539  |  0:00:27s
epoch 96 | loss: 0.06584 | val_0_rmse: 0.25591 | val_1_rmse: 0.2526  |  0:00:27s
epoch 97 | loss: 0.06495 | val_0_rmse: 0.25679 | val_1_rmse: 0.2512  |  0:00:27s
epoch 98 | loss: 0.06494 | val_0_rmse: 0.25323 | val_1_rmse: 0.24921 |  0:00:28s
epoch 99 | loss: 0.06594 | val_0_rmse: 0.25443 | val_1_rmse: 0.25196 |  0:00:28s
epoch 100| loss: 0.06857 | val_0_rmse: 0.26513 | val_1_rmse: 0.26017 |  0:00:28s
epoch 101| loss: 0.06791 | val_0_rmse: 0.25386 | val_1_rmse: 0.25248 |  0:00:29s
epoch 102| loss: 0.06578 | val_0_rmse: 0.2577  | val_1_rmse: 0.25343 |  0:00:29s
epoch 103| loss: 0.06542 | val_0_rmse: 0.25269 | val_1_rmse: 0.24941 |  0:00:29s
epoch 104| loss: 0.06525 | val_0_rmse: 0.25336 | val_1_rmse: 0.2522  |  0:00:29s
epoch 105| loss: 0.0645  | val_0_rmse: 0.25174 | val_1_rmse: 0.25028 |  0:00:30s
epoch 106| loss: 0.06476 | val_0_rmse: 0.25212 | val_1_rmse: 0.25215 |  0:00:30s
epoch 107| loss: 0.06364 | val_0_rmse: 0.2524  | val_1_rmse: 0.25486 |  0:00:30s
epoch 108| loss: 0.06463 | val_0_rmse: 0.25421 | val_1_rmse: 0.25461 |  0:00:31s
epoch 109| loss: 0.06392 | val_0_rmse: 0.25241 | val_1_rmse: 0.25229 |  0:00:31s
epoch 110| loss: 0.06426 | val_0_rmse: 0.25391 | val_1_rmse: 0.25465 |  0:00:31s
epoch 111| loss: 0.0642  | val_0_rmse: 0.25474 | val_1_rmse: 0.25689 |  0:00:31s
epoch 112| loss: 0.06525 | val_0_rmse: 0.25727 | val_1_rmse: 0.25558 |  0:00:32s
epoch 113| loss: 0.0651  | val_0_rmse: 0.25133 | val_1_rmse: 0.2508  |  0:00:32s
epoch 114| loss: 0.06468 | val_0_rmse: 0.25258 | val_1_rmse: 0.2532  |  0:00:32s
epoch 115| loss: 0.06553 | val_0_rmse: 0.2567  | val_1_rmse: 0.25771 |  0:00:32s
epoch 116| loss: 0.06662 | val_0_rmse: 0.25641 | val_1_rmse: 0.25959 |  0:00:33s
epoch 117| loss: 0.06584 | val_0_rmse: 0.25332 | val_1_rmse: 0.25783 |  0:00:33s
epoch 118| loss: 0.06711 | val_0_rmse: 0.25357 | val_1_rmse: 0.25838 |  0:00:33s
epoch 119| loss: 0.0673  | val_0_rmse: 0.25491 | val_1_rmse: 0.2615  |  0:00:34s
epoch 120| loss: 0.06628 | val_0_rmse: 0.25217 | val_1_rmse: 0.25749 |  0:00:34s
epoch 121| loss: 0.06518 | val_0_rmse: 0.25005 | val_1_rmse: 0.25215 |  0:00:34s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.24825
Best weights from best epoch are automatically used!
ended training at: 21:59:45
Feature importance:
Mean squared error is of 0.07261770904929792
Mean absolute error:0.19469173652089475
MAPE:0.21796562734010386
R2 score:0.1881802361779743
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:59:46
epoch 0  | loss: 2.39978 | val_0_rmse: 0.3521  | val_1_rmse: 0.3598  |  0:00:00s
epoch 1  | loss: 0.60386 | val_0_rmse: 0.38542 | val_1_rmse: 0.39564 |  0:00:00s
epoch 2  | loss: 0.24567 | val_0_rmse: 0.30688 | val_1_rmse: 0.31285 |  0:00:00s
epoch 3  | loss: 0.13715 | val_0_rmse: 0.33092 | val_1_rmse: 0.33908 |  0:00:01s
epoch 4  | loss: 0.14473 | val_0_rmse: 0.33561 | val_1_rmse: 0.34367 |  0:00:01s
epoch 5  | loss: 0.11765 | val_0_rmse: 0.31511 | val_1_rmse: 0.32165 |  0:00:01s
epoch 6  | loss: 0.11127 | val_0_rmse: 0.31049 | val_1_rmse: 0.31746 |  0:00:01s
epoch 7  | loss: 0.09975 | val_0_rmse: 0.30656 | val_1_rmse: 0.31109 |  0:00:02s
epoch 8  | loss: 0.0952  | val_0_rmse: 0.29856 | val_1_rmse: 0.30157 |  0:00:02s
epoch 9  | loss: 0.09203 | val_0_rmse: 0.29949 | val_1_rmse: 0.30366 |  0:00:02s
epoch 10 | loss: 0.09315 | val_0_rmse: 0.30624 | val_1_rmse: 0.31199 |  0:00:03s
epoch 11 | loss: 0.09276 | val_0_rmse: 0.29815 | val_1_rmse: 0.30299 |  0:00:03s
epoch 12 | loss: 0.09034 | val_0_rmse: 0.29781 | val_1_rmse: 0.30241 |  0:00:03s
epoch 13 | loss: 0.09091 | val_0_rmse: 0.29921 | val_1_rmse: 0.30416 |  0:00:03s
epoch 14 | loss: 0.09025 | val_0_rmse: 0.29612 | val_1_rmse: 0.29968 |  0:00:04s
epoch 15 | loss: 0.0877  | val_0_rmse: 0.29539 | val_1_rmse: 0.29856 |  0:00:04s
epoch 16 | loss: 0.08932 | val_0_rmse: 0.29524 | val_1_rmse: 0.29859 |  0:00:04s
epoch 17 | loss: 0.0906  | val_0_rmse: 0.29827 | val_1_rmse: 0.30295 |  0:00:05s
epoch 18 | loss: 0.0891  | val_0_rmse: 0.29499 | val_1_rmse: 0.29931 |  0:00:05s
epoch 19 | loss: 0.08803 | val_0_rmse: 0.29875 | val_1_rmse: 0.30179 |  0:00:05s
epoch 20 | loss: 0.08994 | val_0_rmse: 0.30452 | val_1_rmse: 0.31046 |  0:00:05s
epoch 21 | loss: 0.08997 | val_0_rmse: 0.29415 | val_1_rmse: 0.29771 |  0:00:06s
epoch 22 | loss: 0.08749 | val_0_rmse: 0.29479 | val_1_rmse: 0.29771 |  0:00:06s
epoch 23 | loss: 0.08803 | val_0_rmse: 0.2931  | val_1_rmse: 0.29793 |  0:00:06s
epoch 24 | loss: 0.0868  | val_0_rmse: 0.29199 | val_1_rmse: 0.29664 |  0:00:07s
epoch 25 | loss: 0.08463 | val_0_rmse: 0.29154 | val_1_rmse: 0.29478 |  0:00:07s
epoch 26 | loss: 0.08589 | val_0_rmse: 0.29648 | val_1_rmse: 0.30262 |  0:00:07s
epoch 27 | loss: 0.08639 | val_0_rmse: 0.29021 | val_1_rmse: 0.29349 |  0:00:07s
epoch 28 | loss: 0.08469 | val_0_rmse: 0.29042 | val_1_rmse: 0.2962  |  0:00:08s
epoch 29 | loss: 0.08413 | val_0_rmse: 0.28479 | val_1_rmse: 0.28959 |  0:00:08s
epoch 30 | loss: 0.08402 | val_0_rmse: 0.28423 | val_1_rmse: 0.28934 |  0:00:08s
epoch 31 | loss: 0.08412 | val_0_rmse: 0.28527 | val_1_rmse: 0.29151 |  0:00:09s
epoch 32 | loss: 0.0825  | val_0_rmse: 0.28281 | val_1_rmse: 0.28843 |  0:00:09s
epoch 33 | loss: 0.08165 | val_0_rmse: 0.28075 | val_1_rmse: 0.28671 |  0:00:09s
epoch 34 | loss: 0.08206 | val_0_rmse: 0.28831 | val_1_rmse: 0.29546 |  0:00:10s
epoch 35 | loss: 0.0825  | val_0_rmse: 0.27832 | val_1_rmse: 0.28395 |  0:00:10s
epoch 36 | loss: 0.08154 | val_0_rmse: 0.28119 | val_1_rmse: 0.28809 |  0:00:10s
epoch 37 | loss: 0.08014 | val_0_rmse: 0.28139 | val_1_rmse: 0.28866 |  0:00:10s
epoch 38 | loss: 0.07997 | val_0_rmse: 0.27766 | val_1_rmse: 0.28489 |  0:00:11s
epoch 39 | loss: 0.07967 | val_0_rmse: 0.27678 | val_1_rmse: 0.28427 |  0:00:11s
epoch 40 | loss: 0.07808 | val_0_rmse: 0.27475 | val_1_rmse: 0.2822  |  0:00:11s
epoch 41 | loss: 0.07839 | val_0_rmse: 0.27475 | val_1_rmse: 0.28355 |  0:00:12s
epoch 42 | loss: 0.07832 | val_0_rmse: 0.27222 | val_1_rmse: 0.28077 |  0:00:12s
epoch 43 | loss: 0.07809 | val_0_rmse: 0.27857 | val_1_rmse: 0.28765 |  0:00:12s
epoch 44 | loss: 0.07825 | val_0_rmse: 0.27149 | val_1_rmse: 0.27995 |  0:00:12s
epoch 45 | loss: 0.07832 | val_0_rmse: 0.27255 | val_1_rmse: 0.27983 |  0:00:13s
epoch 46 | loss: 0.07945 | val_0_rmse: 0.27379 | val_1_rmse: 0.2837  |  0:00:13s
epoch 47 | loss: 0.07575 | val_0_rmse: 0.27061 | val_1_rmse: 0.28029 |  0:00:13s
epoch 48 | loss: 0.07624 | val_0_rmse: 0.27356 | val_1_rmse: 0.28424 |  0:00:14s
epoch 49 | loss: 0.07502 | val_0_rmse: 0.26713 | val_1_rmse: 0.27696 |  0:00:14s
epoch 50 | loss: 0.07414 | val_0_rmse: 0.26686 | val_1_rmse: 0.27699 |  0:00:14s
epoch 51 | loss: 0.07362 | val_0_rmse: 0.26643 | val_1_rmse: 0.27649 |  0:00:14s
epoch 52 | loss: 0.07328 | val_0_rmse: 0.26873 | val_1_rmse: 0.27907 |  0:00:15s
epoch 53 | loss: 0.07228 | val_0_rmse: 0.26592 | val_1_rmse: 0.27624 |  0:00:15s
epoch 54 | loss: 0.07264 | val_0_rmse: 0.26584 | val_1_rmse: 0.27598 |  0:00:15s
epoch 55 | loss: 0.07255 | val_0_rmse: 0.26816 | val_1_rmse: 0.27858 |  0:00:16s
epoch 56 | loss: 0.07214 | val_0_rmse: 0.26614 | val_1_rmse: 0.27605 |  0:00:16s
epoch 57 | loss: 0.07108 | val_0_rmse: 0.27802 | val_1_rmse: 0.28904 |  0:00:16s
epoch 58 | loss: 0.07404 | val_0_rmse: 0.26443 | val_1_rmse: 0.27515 |  0:00:16s
epoch 59 | loss: 0.07237 | val_0_rmse: 0.26518 | val_1_rmse: 0.27531 |  0:00:17s
epoch 60 | loss: 0.0728  | val_0_rmse: 0.27025 | val_1_rmse: 0.28172 |  0:00:17s
epoch 61 | loss: 0.07124 | val_0_rmse: 0.26753 | val_1_rmse: 0.2782  |  0:00:17s
epoch 62 | loss: 0.07315 | val_0_rmse: 0.26718 | val_1_rmse: 0.27893 |  0:00:18s
epoch 63 | loss: 0.07107 | val_0_rmse: 0.26303 | val_1_rmse: 0.27345 |  0:00:18s
epoch 64 | loss: 0.07003 | val_0_rmse: 0.26749 | val_1_rmse: 0.27762 |  0:00:18s
epoch 65 | loss: 0.06918 | val_0_rmse: 0.26329 | val_1_rmse: 0.27291 |  0:00:18s
epoch 66 | loss: 0.06873 | val_0_rmse: 0.27178 | val_1_rmse: 0.28276 |  0:00:19s
epoch 67 | loss: 0.0712  | val_0_rmse: 0.26291 | val_1_rmse: 0.27399 |  0:00:19s
epoch 68 | loss: 0.07026 | val_0_rmse: 0.27191 | val_1_rmse: 0.28509 |  0:00:19s
epoch 69 | loss: 0.07137 | val_0_rmse: 0.26281 | val_1_rmse: 0.27601 |  0:00:19s
epoch 70 | loss: 0.06995 | val_0_rmse: 0.26192 | val_1_rmse: 0.27456 |  0:00:20s
epoch 71 | loss: 0.06843 | val_0_rmse: 0.26362 | val_1_rmse: 0.27588 |  0:00:20s
epoch 72 | loss: 0.06778 | val_0_rmse: 0.26174 | val_1_rmse: 0.27408 |  0:00:20s
epoch 73 | loss: 0.06852 | val_0_rmse: 0.26532 | val_1_rmse: 0.27734 |  0:00:21s
epoch 74 | loss: 0.06838 | val_0_rmse: 0.2602  | val_1_rmse: 0.271   |  0:00:21s
epoch 75 | loss: 0.06765 | val_0_rmse: 0.26018 | val_1_rmse: 0.27136 |  0:00:21s
epoch 76 | loss: 0.06765 | val_0_rmse: 0.2647  | val_1_rmse: 0.27658 |  0:00:21s
epoch 77 | loss: 0.06933 | val_0_rmse: 0.25931 | val_1_rmse: 0.27062 |  0:00:22s
epoch 78 | loss: 0.06824 | val_0_rmse: 0.25974 | val_1_rmse: 0.27345 |  0:00:22s
epoch 79 | loss: 0.06772 | val_0_rmse: 0.25877 | val_1_rmse: 0.27269 |  0:00:22s
epoch 80 | loss: 0.06665 | val_0_rmse: 0.25797 | val_1_rmse: 0.27093 |  0:00:23s
epoch 81 | loss: 0.06707 | val_0_rmse: 0.25847 | val_1_rmse: 0.27231 |  0:00:23s
epoch 82 | loss: 0.06698 | val_0_rmse: 0.25894 | val_1_rmse: 0.27284 |  0:00:23s
epoch 83 | loss: 0.06615 | val_0_rmse: 0.25731 | val_1_rmse: 0.27041 |  0:00:23s
epoch 84 | loss: 0.06674 | val_0_rmse: 0.26638 | val_1_rmse: 0.28002 |  0:00:24s
epoch 85 | loss: 0.06815 | val_0_rmse: 0.2599  | val_1_rmse: 0.27282 |  0:00:24s
epoch 86 | loss: 0.06776 | val_0_rmse: 0.26246 | val_1_rmse: 0.27698 |  0:00:24s
epoch 87 | loss: 0.06804 | val_0_rmse: 0.25659 | val_1_rmse: 0.2712  |  0:00:25s
epoch 88 | loss: 0.06659 | val_0_rmse: 0.25607 | val_1_rmse: 0.27081 |  0:00:25s
epoch 89 | loss: 0.06694 | val_0_rmse: 0.25989 | val_1_rmse: 0.27524 |  0:00:25s
epoch 90 | loss: 0.06712 | val_0_rmse: 0.25475 | val_1_rmse: 0.26886 |  0:00:25s
epoch 91 | loss: 0.06554 | val_0_rmse: 0.25884 | val_1_rmse: 0.27292 |  0:00:26s
epoch 92 | loss: 0.06664 | val_0_rmse: 0.2555  | val_1_rmse: 0.26942 |  0:00:26s
epoch 93 | loss: 0.06656 | val_0_rmse: 0.2542  | val_1_rmse: 0.26802 |  0:00:26s
epoch 94 | loss: 0.06641 | val_0_rmse: 0.25552 | val_1_rmse: 0.26947 |  0:00:27s
epoch 95 | loss: 0.06588 | val_0_rmse: 0.25525 | val_1_rmse: 0.26945 |  0:00:27s
epoch 96 | loss: 0.06536 | val_0_rmse: 0.25448 | val_1_rmse: 0.26907 |  0:00:27s
epoch 97 | loss: 0.06514 | val_0_rmse: 0.25404 | val_1_rmse: 0.26952 |  0:00:28s
epoch 98 | loss: 0.06521 | val_0_rmse: 0.25518 | val_1_rmse: 0.27054 |  0:00:28s
epoch 99 | loss: 0.06529 | val_0_rmse: 0.25442 | val_1_rmse: 0.26711 |  0:00:28s
epoch 100| loss: 0.06476 | val_0_rmse: 0.25761 | val_1_rmse: 0.27096 |  0:00:28s
epoch 101| loss: 0.06599 | val_0_rmse: 0.25315 | val_1_rmse: 0.26688 |  0:00:29s
epoch 102| loss: 0.06439 | val_0_rmse: 0.25469 | val_1_rmse: 0.26941 |  0:00:29s
epoch 103| loss: 0.06596 | val_0_rmse: 0.25415 | val_1_rmse: 0.26907 |  0:00:29s
epoch 104| loss: 0.06536 | val_0_rmse: 0.25321 | val_1_rmse: 0.26842 |  0:00:30s
epoch 105| loss: 0.0655  | val_0_rmse: 0.25427 | val_1_rmse: 0.27004 |  0:00:30s
epoch 106| loss: 0.06475 | val_0_rmse: 0.25202 | val_1_rmse: 0.26831 |  0:00:30s
epoch 107| loss: 0.06445 | val_0_rmse: 0.25207 | val_1_rmse: 0.27088 |  0:00:30s
epoch 108| loss: 0.06542 | val_0_rmse: 0.25346 | val_1_rmse: 0.27239 |  0:00:31s
epoch 109| loss: 0.06405 | val_0_rmse: 0.25206 | val_1_rmse: 0.27027 |  0:00:31s
epoch 110| loss: 0.06539 | val_0_rmse: 0.25302 | val_1_rmse: 0.27193 |  0:00:31s
epoch 111| loss: 0.06466 | val_0_rmse: 0.25322 | val_1_rmse: 0.27203 |  0:00:31s
epoch 112| loss: 0.06471 | val_0_rmse: 0.25637 | val_1_rmse: 0.27587 |  0:00:32s
epoch 113| loss: 0.06459 | val_0_rmse: 0.2546  | val_1_rmse: 0.2737  |  0:00:32s
epoch 114| loss: 0.06565 | val_0_rmse: 0.25477 | val_1_rmse: 0.27602 |  0:00:32s
epoch 115| loss: 0.06499 | val_0_rmse: 0.24789 | val_1_rmse: 0.2705  |  0:00:33s
epoch 116| loss: 0.06466 | val_0_rmse: 0.24658 | val_1_rmse: 0.26997 |  0:00:33s
epoch 117| loss: 0.06423 | val_0_rmse: 0.25059 | val_1_rmse: 0.27313 |  0:00:33s
epoch 118| loss: 0.06405 | val_0_rmse: 0.24676 | val_1_rmse: 0.26871 |  0:00:33s
epoch 119| loss: 0.06389 | val_0_rmse: 0.25447 | val_1_rmse: 0.27733 |  0:00:34s
epoch 120| loss: 0.065   | val_0_rmse: 0.24887 | val_1_rmse: 0.271   |  0:00:34s
epoch 121| loss: 0.0625  | val_0_rmse: 0.24834 | val_1_rmse: 0.27176 |  0:00:34s
epoch 122| loss: 0.0624  | val_0_rmse: 0.24621 | val_1_rmse: 0.27112 |  0:00:35s
epoch 123| loss: 0.06169 | val_0_rmse: 0.24675 | val_1_rmse: 0.27214 |  0:00:35s
epoch 124| loss: 0.06274 | val_0_rmse: 0.24917 | val_1_rmse: 0.27491 |  0:00:35s
epoch 125| loss: 0.06358 | val_0_rmse: 0.24554 | val_1_rmse: 0.27092 |  0:00:35s
epoch 126| loss: 0.06274 | val_0_rmse: 0.24819 | val_1_rmse: 0.27322 |  0:00:36s
epoch 127| loss: 0.06304 | val_0_rmse: 0.24512 | val_1_rmse: 0.26994 |  0:00:36s
epoch 128| loss: 0.06201 | val_0_rmse: 0.24781 | val_1_rmse: 0.27398 |  0:00:36s
epoch 129| loss: 0.06262 | val_0_rmse: 0.24436 | val_1_rmse: 0.27176 |  0:00:37s
epoch 130| loss: 0.06222 | val_0_rmse: 0.24452 | val_1_rmse: 0.27215 |  0:00:37s
epoch 131| loss: 0.06233 | val_0_rmse: 0.24539 | val_1_rmse: 0.27389 |  0:00:37s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.26688
Best weights from best epoch are automatically used!
ended training at: 22:00:23
Feature importance:
Mean squared error is of 0.06807798922030207
Mean absolute error:0.19362047375554214
MAPE:0.221686084830858
R2 score:0.19931107333595532
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:00:33
epoch 0  | loss: 0.19021 | val_0_rmse: 0.34318 | val_1_rmse: 0.34155 |  0:00:21s
epoch 1  | loss: 0.12348 | val_0_rmse: 0.34261 | val_1_rmse: 0.34094 |  0:00:42s
epoch 2  | loss: 0.12599 | val_0_rmse: 0.34841 | val_1_rmse: 0.3466  |  0:01:04s
epoch 3  | loss: 0.12163 | val_0_rmse: 0.35204 | val_1_rmse: 0.35069 |  0:01:25s
epoch 4  | loss: 0.12084 | val_0_rmse: 0.34694 | val_1_rmse: 0.34506 |  0:01:46s
epoch 5  | loss: 0.12103 | val_0_rmse: 0.34649 | val_1_rmse: 0.34481 |  0:02:08s
epoch 6  | loss: 0.11958 | val_0_rmse: 0.34484 | val_1_rmse: 0.34341 |  0:02:29s
epoch 7  | loss: 0.11923 | val_0_rmse: 0.34471 | val_1_rmse: 0.34342 |  0:02:51s
epoch 8  | loss: 0.11953 | val_0_rmse: 0.34453 | val_1_rmse: 0.34337 |  0:03:12s
epoch 9  | loss: 0.11932 | val_0_rmse: 0.34847 | val_1_rmse: 0.35256 |  0:03:34s
epoch 10 | loss: 0.11934 | val_0_rmse: 0.34936 | val_1_rmse: 0.34851 |  0:03:55s
epoch 11 | loss: 0.1193  | val_0_rmse: 0.34367 | val_1_rmse: 0.34371 |  0:04:17s
epoch 12 | loss: 0.11794 | val_0_rmse: 0.356   | val_1_rmse: 0.35799 |  0:04:39s
epoch 13 | loss: 0.11889 | val_0_rmse: 0.35853 | val_1_rmse: 0.35547 |  0:05:00s
epoch 14 | loss: 0.11929 | val_0_rmse: 0.35291 | val_1_rmse: 0.35277 |  0:05:22s
epoch 15 | loss: 0.1198  | val_0_rmse: 0.34502 | val_1_rmse: 0.34449 |  0:05:44s
epoch 16 | loss: 0.11929 | val_0_rmse: 0.34464 | val_1_rmse: 0.34382 |  0:06:06s
epoch 17 | loss: 0.11915 | val_0_rmse: 0.34438 | val_1_rmse: 0.34349 |  0:06:27s
epoch 18 | loss: 0.11931 | val_0_rmse: 0.34609 | val_1_rmse: 0.34299 |  0:06:49s
epoch 19 | loss: 0.11939 | val_0_rmse: 0.34506 | val_1_rmse: 0.34255 |  0:07:11s
epoch 20 | loss: 0.1188  | val_0_rmse: 0.34508 | val_1_rmse: 0.34213 |  0:07:33s
epoch 21 | loss: 0.11892 | val_0_rmse: 0.35491 | val_1_rmse: 0.34742 |  0:07:54s
epoch 22 | loss: 0.11991 | val_0_rmse: 0.34417 | val_1_rmse: 0.34235 |  0:08:16s
epoch 23 | loss: 0.11935 | val_0_rmse: 0.34466 | val_1_rmse: 0.34312 |  0:08:37s
epoch 24 | loss: 0.11954 | val_0_rmse: 0.34531 | val_1_rmse: 0.34349 |  0:08:59s
epoch 25 | loss: 0.12101 | val_0_rmse: 0.34564 | val_1_rmse: 0.34413 |  0:09:21s
epoch 26 | loss: 0.11967 | val_0_rmse: 0.34476 | val_1_rmse: 0.3433  |  0:09:42s
epoch 27 | loss: 0.11916 | val_0_rmse: 0.34467 | val_1_rmse: 0.34296 |  0:10:04s
epoch 28 | loss: 0.11927 | val_0_rmse: 0.34429 | val_1_rmse: 0.34253 |  0:10:25s
epoch 29 | loss: 0.11931 | val_0_rmse: 0.34451 | val_1_rmse: 0.34248 |  0:10:48s
epoch 30 | loss: 0.11932 | val_0_rmse: 0.34491 | val_1_rmse: 0.34314 |  0:11:09s
epoch 31 | loss: 0.11941 | val_0_rmse: 0.34501 | val_1_rmse: 0.34325 |  0:11:30s

Early stopping occured at epoch 31 with best_epoch = 1 and best_val_1_rmse = 0.34094
Best weights from best epoch are automatically used!
ended training at: 22:12:15
Feature importance:
Mean squared error is of 0.11593061629654475
Mean absolute error:0.2272955937976967
MAPE:0.26772191472017903
R2 score:0.05827925362551345
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_0.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:12:20
epoch 0  | loss: 0.18947 | val_0_rmse: 0.34945 | val_1_rmse: 0.35286 |  0:00:21s
epoch 1  | loss: 0.12133 | val_0_rmse: 0.33486 | val_1_rmse: 0.33477 |  0:00:42s
epoch 2  | loss: 0.10982 | val_0_rmse: 0.31609 | val_1_rmse: 0.31532 |  0:01:03s
epoch 3  | loss: 0.10691 | val_0_rmse: 0.33749 | val_1_rmse: 0.33684 |  0:01:24s
epoch 4  | loss: 0.10207 | val_0_rmse: 0.312   | val_1_rmse: 0.31115 |  0:01:45s
epoch 5  | loss: 0.09916 | val_0_rmse: 0.31717 | val_1_rmse: 0.31687 |  0:02:07s
epoch 6  | loss: 0.09866 | val_0_rmse: 0.30996 | val_1_rmse: 0.30821 |  0:02:28s
epoch 7  | loss: 0.09731 | val_0_rmse: 0.30794 | val_1_rmse: 0.30815 |  0:02:49s
epoch 8  | loss: 0.09779 | val_0_rmse: 0.31096 | val_1_rmse: 0.30897 |  0:03:10s
epoch 9  | loss: 0.09536 | val_0_rmse: 0.30679 | val_1_rmse: 0.3067  |  0:03:32s
epoch 10 | loss: 0.0947  | val_0_rmse: 0.30181 | val_1_rmse: 0.30422 |  0:03:53s
epoch 11 | loss: 0.0946  | val_0_rmse: 0.30404 | val_1_rmse: 0.30622 |  0:04:14s
epoch 12 | loss: 0.0924  | val_0_rmse: 0.30117 | val_1_rmse: 0.3049  |  0:04:36s
epoch 13 | loss: 0.09191 | val_0_rmse: 0.30909 | val_1_rmse: 0.31222 |  0:04:57s
epoch 14 | loss: 0.09031 | val_0_rmse: 0.30388 | val_1_rmse: 0.30584 |  0:05:18s
epoch 15 | loss: 0.08989 | val_0_rmse: 0.29265 | val_1_rmse: 0.29677 |  0:05:39s
epoch 16 | loss: 0.08704 | val_0_rmse: 0.29655 | val_1_rmse: 0.30554 |  0:06:01s
epoch 17 | loss: 0.0878  | val_0_rmse: 0.29276 | val_1_rmse: 0.29834 |  0:06:22s
epoch 18 | loss: 0.08763 | val_0_rmse: 0.29922 | val_1_rmse: 0.30519 |  0:06:43s
epoch 19 | loss: 0.08768 | val_0_rmse: 0.31414 | val_1_rmse: 0.33141 |  0:07:05s
epoch 20 | loss: 0.08752 | val_0_rmse: 0.29136 | val_1_rmse: 0.29763 |  0:07:26s
epoch 21 | loss: 0.08518 | val_0_rmse: 0.29131 | val_1_rmse: 0.29838 |  0:07:47s
epoch 22 | loss: 0.08649 | val_0_rmse: 0.29074 | val_1_rmse: 0.30225 |  0:08:08s
epoch 23 | loss: 0.08634 | val_0_rmse: 0.29927 | val_1_rmse: 0.30966 |  0:08:30s
epoch 24 | loss: 0.0872  | val_0_rmse: 0.29839 | val_1_rmse: 0.32121 |  0:08:51s
epoch 25 | loss: 0.0837  | val_0_rmse: 0.30416 | val_1_rmse: 0.32469 |  0:09:12s
epoch 26 | loss: 0.08338 | val_0_rmse: 0.29028 | val_1_rmse: 0.30205 |  0:09:34s
epoch 27 | loss: 0.08169 | val_0_rmse: 0.30096 | val_1_rmse: 0.30799 |  0:09:55s
epoch 28 | loss: 0.0855  | val_0_rmse: 0.29344 | val_1_rmse: 0.29611 |  0:10:16s
epoch 29 | loss: 0.08185 | val_0_rmse: 0.3478  | val_1_rmse: 0.3115  |  0:10:37s
epoch 30 | loss: 0.08256 | val_0_rmse: 0.30211 | val_1_rmse: 0.31472 |  0:10:59s
epoch 31 | loss: 0.08549 | val_0_rmse: 0.31275 | val_1_rmse: 0.57383 |  0:11:20s
epoch 32 | loss: 0.08444 | val_0_rmse: 0.34827 | val_1_rmse: 0.36305 |  0:11:41s
epoch 33 | loss: 0.08307 | val_0_rmse: 0.30018 | val_1_rmse: 0.32272 |  0:12:03s
epoch 34 | loss: 0.09088 | val_0_rmse: 0.45133 | val_1_rmse: 0.7903  |  0:12:24s
epoch 35 | loss: 0.11999 | val_0_rmse: 0.34753 | val_1_rmse: 0.35244 |  0:12:45s
epoch 36 | loss: 0.11553 | val_0_rmse: 0.34855 | val_1_rmse: 0.35479 |  0:13:06s
epoch 37 | loss: 0.11361 | val_0_rmse: 0.33779 | val_1_rmse: 0.3429  |  0:13:28s
epoch 38 | loss: 0.11071 | val_0_rmse: 0.33192 | val_1_rmse: 0.33668 |  0:13:49s
epoch 39 | loss: 0.10456 | val_0_rmse: 0.315   | val_1_rmse: 0.32101 |  0:14:11s
epoch 40 | loss: 0.10163 | val_0_rmse: 0.32618 | val_1_rmse: 0.32456 |  0:14:32s
epoch 41 | loss: 0.1001  | val_0_rmse: 0.3104  | val_1_rmse: 0.31473 |  0:14:54s
epoch 42 | loss: 0.09702 | val_0_rmse: 0.30617 | val_1_rmse: 0.31423 |  0:15:15s
epoch 43 | loss: 0.09501 | val_0_rmse: 0.31029 | val_1_rmse: 0.31711 |  0:15:36s
epoch 44 | loss: 0.09322 | val_0_rmse: 0.30501 | val_1_rmse: 0.31154 |  0:15:58s
epoch 45 | loss: 0.09139 | val_0_rmse: 0.30377 | val_1_rmse: 0.30944 |  0:16:19s
epoch 46 | loss: 0.09011 | val_0_rmse: 0.29955 | val_1_rmse: 0.31371 |  0:16:40s
epoch 47 | loss: 0.08766 | val_0_rmse: 0.31121 | val_1_rmse: 0.3209  |  0:17:01s
epoch 48 | loss: 0.08974 | val_0_rmse: 0.29667 | val_1_rmse: 0.30172 |  0:17:23s
epoch 49 | loss: 0.08949 | val_0_rmse: 0.30311 | val_1_rmse: 0.30943 |  0:17:44s
epoch 50 | loss: 0.08798 | val_0_rmse: 0.30612 | val_1_rmse: 0.31337 |  0:18:05s
epoch 51 | loss: 0.08616 | val_0_rmse: 0.34757 | val_1_rmse: 0.36025 |  0:18:27s
epoch 52 | loss: 0.08533 | val_0_rmse: 0.29526 | val_1_rmse: 0.30955 |  0:18:48s
epoch 53 | loss: 0.08528 | val_0_rmse: 0.29343 | val_1_rmse: 0.30346 |  0:19:09s
epoch 54 | loss: 0.08495 | val_0_rmse: 0.29724 | val_1_rmse: 0.30593 |  0:19:31s
epoch 55 | loss: 0.08432 | val_0_rmse: 0.29531 | val_1_rmse: 0.30969 |  0:19:52s
epoch 56 | loss: 0.08227 | val_0_rmse: 0.29494 | val_1_rmse: 0.31026 |  0:20:13s
epoch 57 | loss: 0.08128 | val_0_rmse: 0.2957  | val_1_rmse: 0.30851 |  0:20:34s
epoch 58 | loss: 0.08138 | val_0_rmse: 0.30669 | val_1_rmse: 0.32001 |  0:20:56s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.29611
Best weights from best epoch are automatically used!
ended training at: 22:33:27
Feature importance:
Mean squared error is of 0.08710595269908548
Mean absolute error:0.20568405234904133
MAPE:0.23619668714308342
R2 score:0.27461628381418157
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_1.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:33:32
epoch 0  | loss: 0.19593 | val_0_rmse: 0.35774 | val_1_rmse: 0.3447  |  0:00:21s
epoch 1  | loss: 0.12769 | val_0_rmse: 0.34262 | val_1_rmse: 0.32884 |  0:00:42s
epoch 2  | loss: 0.11248 | val_0_rmse: 0.33636 | val_1_rmse: 0.32469 |  0:01:03s
epoch 3  | loss: 0.10715 | val_0_rmse: 0.32321 | val_1_rmse: 0.31696 |  0:01:24s
epoch 4  | loss: 0.10308 | val_0_rmse: 0.32387 | val_1_rmse: 0.31449 |  0:01:46s
epoch 5  | loss: 0.09905 | val_0_rmse: 0.30506 | val_1_rmse: 0.30044 |  0:02:07s
epoch 6  | loss: 0.09729 | val_0_rmse: 0.30997 | val_1_rmse: 0.30769 |  0:02:28s
epoch 7  | loss: 0.09828 | val_0_rmse: 0.29887 | val_1_rmse: 0.29849 |  0:02:49s
epoch 8  | loss: 0.09303 | val_0_rmse: 0.29909 | val_1_rmse: 0.30209 |  0:03:10s
epoch 9  | loss: 0.09145 | val_0_rmse: 0.31109 | val_1_rmse: 0.31707 |  0:03:32s
epoch 10 | loss: 0.09425 | val_0_rmse: 0.30533 | val_1_rmse: 0.3286  |  0:03:53s
epoch 11 | loss: 0.09049 | val_0_rmse: 0.29773 | val_1_rmse: 0.30509 |  0:04:14s
epoch 12 | loss: 0.08747 | val_0_rmse: 0.29834 | val_1_rmse: 0.30055 |  0:04:35s
epoch 13 | loss: 0.08857 | val_0_rmse: 0.29738 | val_1_rmse: 0.29448 |  0:04:57s
epoch 14 | loss: 0.08612 | val_0_rmse: 0.29003 | val_1_rmse: 0.29675 |  0:05:18s
epoch 15 | loss: 0.08434 | val_0_rmse: 0.28769 | val_1_rmse: 0.29302 |  0:05:39s
epoch 16 | loss: 0.08557 | val_0_rmse: 0.29859 | val_1_rmse: 0.30482 |  0:06:00s
epoch 17 | loss: 0.08491 | val_0_rmse: 0.28866 | val_1_rmse: 0.29575 |  0:06:22s
epoch 18 | loss: 0.08346 | val_0_rmse: 0.2933  | val_1_rmse: 0.29762 |  0:06:43s
epoch 19 | loss: 0.08235 | val_0_rmse: 0.35831 | val_1_rmse: 0.29072 |  0:07:04s
epoch 20 | loss: 0.08184 | val_0_rmse: 0.31483 | val_1_rmse: 0.29389 |  0:07:25s
epoch 21 | loss: 0.08046 | val_0_rmse: 0.28033 | val_1_rmse: 0.28922 |  0:07:46s
epoch 22 | loss: 0.08127 | val_0_rmse: 0.36956 | val_1_rmse: 0.29864 |  0:08:08s
epoch 23 | loss: 0.08013 | val_0_rmse: 0.27884 | val_1_rmse: 0.2917  |  0:08:30s
epoch 24 | loss: 0.07898 | val_0_rmse: 0.28052 | val_1_rmse: 0.29229 |  0:08:51s
epoch 25 | loss: 0.08071 | val_0_rmse: 0.30226 | val_1_rmse: 0.3115  |  0:09:12s
epoch 26 | loss: 0.08143 | val_0_rmse: 0.28092 | val_1_rmse: 0.28982 |  0:09:34s
epoch 27 | loss: 0.07831 | val_0_rmse: 0.28221 | val_1_rmse: 0.29037 |  0:09:55s
epoch 28 | loss: 0.07842 | val_0_rmse: 0.27969 | val_1_rmse: 0.30008 |  0:10:16s
epoch 29 | loss: 0.07686 | val_0_rmse: 0.27682 | val_1_rmse: 0.2897  |  0:10:37s
epoch 30 | loss: 0.07682 | val_0_rmse: 0.27433 | val_1_rmse: 0.29278 |  0:10:58s
epoch 31 | loss: 0.07823 | val_0_rmse: 0.30612 | val_1_rmse: 0.32259 |  0:11:20s
epoch 32 | loss: 0.07852 | val_0_rmse: 0.281   | val_1_rmse: 0.29586 |  0:11:41s
epoch 33 | loss: 0.07633 | val_0_rmse: 0.28005 | val_1_rmse: 0.29292 |  0:12:02s
epoch 34 | loss: 0.07557 | val_0_rmse: 0.28591 | val_1_rmse: 0.29994 |  0:12:24s
epoch 35 | loss: 0.07495 | val_0_rmse: 0.27444 | val_1_rmse: 0.28758 |  0:12:45s
epoch 36 | loss: 0.07642 | val_0_rmse: 0.2755  | val_1_rmse: 0.29689 |  0:13:06s
epoch 37 | loss: 0.07428 | val_0_rmse: 0.2702  | val_1_rmse: 0.28607 |  0:13:27s
epoch 38 | loss: 0.0743  | val_0_rmse: 0.27193 | val_1_rmse: 0.29177 |  0:13:49s
epoch 39 | loss: 0.07386 | val_0_rmse: 0.27835 | val_1_rmse: 0.29518 |  0:14:10s
epoch 40 | loss: 0.07328 | val_0_rmse: 0.27241 | val_1_rmse: 0.28667 |  0:14:31s
epoch 41 | loss: 0.07297 | val_0_rmse: 0.27004 | val_1_rmse: 0.28791 |  0:14:52s
epoch 42 | loss: 0.07164 | val_0_rmse: 0.27481 | val_1_rmse: 0.29728 |  0:15:14s
epoch 43 | loss: 0.073   | val_0_rmse: 0.26628 | val_1_rmse: 0.28578 |  0:15:35s
epoch 44 | loss: 0.07263 | val_0_rmse: 0.27957 | val_1_rmse: 0.30361 |  0:15:56s
epoch 45 | loss: 0.07269 | val_0_rmse: 0.27002 | val_1_rmse: 0.29413 |  0:16:17s
epoch 46 | loss: 0.07115 | val_0_rmse: 0.26769 | val_1_rmse: 0.29243 |  0:16:39s
epoch 47 | loss: 0.07153 | val_0_rmse: 0.27147 | val_1_rmse: 0.29578 |  0:17:00s
epoch 48 | loss: 0.07203 | val_0_rmse: 0.27383 | val_1_rmse: 0.31193 |  0:17:21s
epoch 49 | loss: 0.0712  | val_0_rmse: 0.27171 | val_1_rmse: 0.29261 |  0:17:42s
epoch 50 | loss: 0.07143 | val_0_rmse: 0.27885 | val_1_rmse: 0.29961 |  0:18:04s
epoch 51 | loss: 0.07235 | val_0_rmse: 0.2688  | val_1_rmse: 0.28888 |  0:18:25s
epoch 52 | loss: 0.07161 | val_0_rmse: 0.26493 | val_1_rmse: 0.28746 |  0:18:46s
epoch 53 | loss: 0.07098 | val_0_rmse: 0.26777 | val_1_rmse: 0.29338 |  0:19:07s
epoch 54 | loss: 0.0703  | val_0_rmse: 0.26769 | val_1_rmse: 0.29427 |  0:19:29s
epoch 55 | loss: 0.06935 | val_0_rmse: 0.2657  | val_1_rmse: 0.29479 |  0:19:50s
epoch 56 | loss: 0.06991 | val_0_rmse: 0.26549 | val_1_rmse: 0.28981 |  0:20:11s
epoch 57 | loss: 0.07003 | val_0_rmse: 0.26566 | val_1_rmse: 0.29007 |  0:20:32s
epoch 58 | loss: 0.06919 | val_0_rmse: 0.26496 | val_1_rmse: 0.28634 |  0:20:54s
epoch 59 | loss: 0.06913 | val_0_rmse: 0.26332 | val_1_rmse: 0.2863  |  0:21:15s
epoch 60 | loss: 0.06806 | val_0_rmse: 0.26441 | val_1_rmse: 0.29275 |  0:21:37s
epoch 61 | loss: 0.06879 | val_0_rmse: 0.26295 | val_1_rmse: 0.2935  |  0:21:58s
epoch 62 | loss: 0.06825 | val_0_rmse: 0.25997 | val_1_rmse: 0.29025 |  0:22:19s
epoch 63 | loss: 0.06785 | val_0_rmse: 0.26551 | val_1_rmse: 0.28987 |  0:22:41s
epoch 64 | loss: 0.06867 | val_0_rmse: 0.27071 | val_1_rmse: 0.30015 |  0:23:02s
epoch 65 | loss: 0.06859 | val_0_rmse: 0.26662 | val_1_rmse: 0.30132 |  0:23:24s
epoch 66 | loss: 0.06779 | val_0_rmse: 0.27391 | val_1_rmse: 0.30015 |  0:23:46s
epoch 67 | loss: 0.06792 | val_0_rmse: 0.26519 | val_1_rmse: 0.29697 |  0:24:07s
epoch 68 | loss: 0.06648 | val_0_rmse: 0.27143 | val_1_rmse: 0.30222 |  0:24:28s
epoch 69 | loss: 0.07732 | val_0_rmse: 0.32924 | val_1_rmse: 0.3465  |  0:24:50s
epoch 70 | loss: 0.10289 | val_0_rmse: 0.36881 | val_1_rmse: 0.32125 |  0:25:11s
epoch 71 | loss: 0.10034 | val_0_rmse: 0.32106 | val_1_rmse: 0.31461 |  0:25:33s
epoch 72 | loss: 0.09799 | val_0_rmse: 0.36116 | val_1_rmse: 0.3056  |  0:25:54s
epoch 73 | loss: 0.09311 | val_0_rmse: 0.33126 | val_1_rmse: 0.34133 |  0:26:15s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.28578
Best weights from best epoch are automatically used!
ended training at: 22:59:58
Feature importance:
Mean squared error is of 0.08558876184457449
Mean absolute error:0.20252790386156402
MAPE:0.226470260603595
R2 score:0.2926872279095094
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_2.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:00:04
epoch 0  | loss: 0.19611 | val_0_rmse: 0.35463 | val_1_rmse: 0.34717 |  0:00:21s
epoch 1  | loss: 0.12725 | val_0_rmse: 0.35299 | val_1_rmse: 0.3455  |  0:00:42s
epoch 2  | loss: 0.12636 | val_0_rmse: 0.35201 | val_1_rmse: 0.34486 |  0:01:03s
epoch 3  | loss: 0.11824 | val_0_rmse: 0.34361 | val_1_rmse: 0.33452 |  0:01:24s
epoch 4  | loss: 0.10659 | val_0_rmse: 0.31628 | val_1_rmse: 0.3095  |  0:01:45s
epoch 5  | loss: 0.10492 | val_0_rmse: 0.31997 | val_1_rmse: 0.31326 |  0:02:06s
epoch 6  | loss: 0.10315 | val_0_rmse: 0.31449 | val_1_rmse: 0.30739 |  0:02:28s
epoch 7  | loss: 0.0992  | val_0_rmse: 0.31879 | val_1_rmse: 0.31325 |  0:02:49s
epoch 8  | loss: 0.09728 | val_0_rmse: 0.30889 | val_1_rmse: 0.30467 |  0:03:10s
epoch 9  | loss: 0.09546 | val_0_rmse: 0.33361 | val_1_rmse: 0.33843 |  0:03:31s
epoch 10 | loss: 0.09356 | val_0_rmse: 0.30064 | val_1_rmse: 0.30063 |  0:03:52s
epoch 11 | loss: 0.09159 | val_0_rmse: 0.32087 | val_1_rmse: 0.37953 |  0:04:14s
epoch 12 | loss: 0.09274 | val_0_rmse: 0.31349 | val_1_rmse: 0.3381  |  0:04:35s
epoch 13 | loss: 0.0903  | val_0_rmse: 0.29579 | val_1_rmse: 0.30311 |  0:04:56s
epoch 14 | loss: 0.08799 | val_0_rmse: 0.30424 | val_1_rmse: 0.3092  |  0:05:17s
epoch 15 | loss: 0.08738 | val_0_rmse: 0.30413 | val_1_rmse: 0.3079  |  0:05:39s
epoch 16 | loss: 0.08902 | val_0_rmse: 0.30157 | val_1_rmse: 0.31872 |  0:06:00s
epoch 17 | loss: 0.08615 | val_0_rmse: 0.29632 | val_1_rmse: 0.3218  |  0:06:21s
epoch 18 | loss: 0.08392 | val_0_rmse: 0.28655 | val_1_rmse: 0.31489 |  0:06:42s
epoch 19 | loss: 0.08385 | val_0_rmse: 0.29428 | val_1_rmse: 0.30422 |  0:07:03s
epoch 20 | loss: 0.08088 | val_0_rmse: 0.29645 | val_1_rmse: 0.33809 |  0:07:25s
epoch 21 | loss: 0.08967 | val_0_rmse: 0.34251 | val_1_rmse: 0.32449 |  0:07:46s
epoch 22 | loss: 0.101   | val_0_rmse: 0.34552 | val_1_rmse: 0.34624 |  0:08:07s
epoch 23 | loss: 0.11091 | val_0_rmse: 0.34379 | val_1_rmse: 0.35264 |  0:08:28s
epoch 24 | loss: 0.10807 | val_0_rmse: 0.33632 | val_1_rmse: 0.35937 |  0:08:50s
epoch 25 | loss: 0.10768 | val_0_rmse: 0.35177 | val_1_rmse: 0.34322 |  0:09:11s
epoch 26 | loss: 0.10659 | val_0_rmse: 0.33062 | val_1_rmse: 0.34279 |  0:09:32s
epoch 27 | loss: 0.10557 | val_0_rmse: 0.33895 | val_1_rmse: 0.34416 |  0:09:53s
epoch 28 | loss: 0.09949 | val_0_rmse: 2.6508  | val_1_rmse: 0.32002 |  0:10:14s
epoch 29 | loss: 0.09279 | val_0_rmse: 0.33084 | val_1_rmse: 0.33201 |  0:10:36s
epoch 30 | loss: 0.09066 | val_0_rmse: 0.31474 | val_1_rmse: 0.30469 |  0:10:57s
epoch 31 | loss: 0.08994 | val_0_rmse: 0.3172  | val_1_rmse: 0.32095 |  0:11:18s
epoch 32 | loss: 0.08852 | val_0_rmse: 0.31926 | val_1_rmse: 0.3137  |  0:11:39s
epoch 33 | loss: 0.08823 | val_0_rmse: 0.31349 | val_1_rmse: 0.30275 |  0:12:01s
epoch 34 | loss: 0.0864  | val_0_rmse: 0.32022 | val_1_rmse: 0.31068 |  0:12:22s
epoch 35 | loss: 0.08633 | val_0_rmse: 0.3441  | val_1_rmse: 0.33181 |  0:12:44s
epoch 36 | loss: 0.08699 | val_0_rmse: 0.31862 | val_1_rmse: 0.30013 |  0:13:05s
epoch 37 | loss: 0.08828 | val_0_rmse: 0.32281 | val_1_rmse: 0.29993 |  0:13:26s
epoch 38 | loss: 0.08744 | val_0_rmse: 0.31118 | val_1_rmse: 0.29793 |  0:13:47s
epoch 39 | loss: 0.08827 | val_0_rmse: 0.32111 | val_1_rmse: 0.31926 |  0:14:09s
epoch 40 | loss: 0.08584 | val_0_rmse: 0.32414 | val_1_rmse: 0.31332 |  0:14:30s
epoch 41 | loss: 0.08666 | val_0_rmse: 0.32615 | val_1_rmse: 0.32847 |  0:14:51s
epoch 42 | loss: 0.08761 | val_0_rmse: 0.30658 | val_1_rmse: 0.29834 |  0:15:13s
epoch 43 | loss: 0.08873 | val_0_rmse: 0.35386 | val_1_rmse: 0.34501 |  0:15:34s
epoch 44 | loss: 0.08863 | val_0_rmse: 0.32435 | val_1_rmse: 0.30584 |  0:15:55s
epoch 45 | loss: 0.08579 | val_0_rmse: 0.32511 | val_1_rmse: 0.31594 |  0:16:17s
epoch 46 | loss: 0.09053 | val_0_rmse: 0.33354 | val_1_rmse: 0.35049 |  0:16:38s
epoch 47 | loss: 0.09047 | val_0_rmse: 0.3387  | val_1_rmse: 0.33248 |  0:17:00s
epoch 48 | loss: 0.08916 | val_0_rmse: 0.32624 | val_1_rmse: 0.31848 |  0:17:21s
epoch 49 | loss: 0.08772 | val_0_rmse: 0.328   | val_1_rmse: 0.31523 |  0:17:43s
epoch 50 | loss: 0.0902  | val_0_rmse: 0.3082  | val_1_rmse: 0.31476 |  0:18:04s
epoch 51 | loss: 0.08932 | val_0_rmse: 0.39592 | val_1_rmse: 0.36031 |  0:18:25s
epoch 52 | loss: 0.08818 | val_0_rmse: 0.30135 | val_1_rmse: 0.31081 |  0:18:47s
epoch 53 | loss: 0.08948 | val_0_rmse: 0.53153 | val_1_rmse: 0.30928 |  0:19:08s
epoch 54 | loss: 0.09064 | val_0_rmse: 0.31049 | val_1_rmse: 4.26459 |  0:19:30s
epoch 55 | loss: 0.08901 | val_0_rmse: 0.30896 | val_1_rmse: 2.39059 |  0:19:51s
epoch 56 | loss: 0.08675 | val_0_rmse: 0.3094  | val_1_rmse: 0.30988 |  0:20:12s
epoch 57 | loss: 0.08502 | val_0_rmse: 0.30839 | val_1_rmse: 0.3106  |  0:20:34s
epoch 58 | loss: 0.08521 | val_0_rmse: 0.32123 | val_1_rmse: 0.31724 |  0:20:55s
epoch 59 | loss: 0.08441 | val_0_rmse: 0.31134 | val_1_rmse: 0.31867 |  0:21:17s
epoch 60 | loss: 0.08353 | val_0_rmse: 3.05814 | val_1_rmse: 4.31599 |  0:21:38s
epoch 61 | loss: 0.08575 | val_0_rmse: 0.31931 | val_1_rmse: 0.33593 |  0:22:00s
epoch 62 | loss: 0.08403 | val_0_rmse: 0.30212 | val_1_rmse: 0.3184  |  0:22:21s
epoch 63 | loss: 0.08383 | val_0_rmse: 1.84701 | val_1_rmse: 2.57515 |  0:22:42s
epoch 64 | loss: 0.08327 | val_0_rmse: 0.29926 | val_1_rmse: 0.31324 |  0:23:04s
epoch 65 | loss: 0.08421 | val_0_rmse: 0.30104 | val_1_rmse: 0.3111  |  0:23:25s
epoch 66 | loss: 0.08448 | val_0_rmse: 0.32464 | val_1_rmse: 0.33793 |  0:23:46s
epoch 67 | loss: 0.08462 | val_0_rmse: 0.31375 | val_1_rmse: 0.31348 |  0:24:08s
epoch 68 | loss: 0.08386 | val_0_rmse: 0.32132 | val_1_rmse: 0.31329 |  0:24:29s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.29793
Best weights from best epoch are automatically used!
ended training at: 23:24:43
Feature importance:
Mean squared error is of 0.09460422382605746
Mean absolute error:0.209805333853774
MAPE:0.2337737210774805
R2 score:0.23849583001866537
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_3.zip
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:24:49
epoch 0  | loss: 0.19068 | val_0_rmse: 0.35487 | val_1_rmse: 0.35686 |  0:00:21s
epoch 1  | loss: 0.11936 | val_0_rmse: 0.33225 | val_1_rmse: 0.33414 |  0:00:42s
epoch 2  | loss: 0.10715 | val_0_rmse: 0.32038 | val_1_rmse: 0.32358 |  0:01:03s
epoch 3  | loss: 0.10219 | val_0_rmse: 0.31846 | val_1_rmse: 0.32299 |  0:01:24s
epoch 4  | loss: 0.1009  | val_0_rmse: 0.31438 | val_1_rmse: 0.319   |  0:01:45s
epoch 5  | loss: 0.09847 | val_0_rmse: 0.31848 | val_1_rmse: 0.32346 |  0:02:07s
epoch 6  | loss: 0.09737 | val_0_rmse: 0.31373 | val_1_rmse: 0.31959 |  0:02:29s
epoch 7  | loss: 0.10081 | val_0_rmse: 0.30972 | val_1_rmse: 0.31674 |  0:02:50s
epoch 8  | loss: 0.09674 | val_0_rmse: 0.36069 | val_1_rmse: 0.31727 |  0:03:11s
epoch 9  | loss: 0.09553 | val_0_rmse: 0.31136 | val_1_rmse: 0.31862 |  0:03:32s
epoch 10 | loss: 0.09481 | val_0_rmse: 0.30238 | val_1_rmse: 0.30892 |  0:03:53s
epoch 11 | loss: 0.09179 | val_0_rmse: 0.29806 | val_1_rmse: 0.30698 |  0:04:14s
epoch 12 | loss: 0.09165 | val_0_rmse: 0.2983  | val_1_rmse: 0.30585 |  0:04:36s
epoch 13 | loss: 0.09033 | val_0_rmse: 0.30015 | val_1_rmse: 0.30806 |  0:04:57s
epoch 14 | loss: 0.08904 | val_0_rmse: 0.29866 | val_1_rmse: 0.30806 |  0:05:18s
epoch 15 | loss: 0.09507 | val_0_rmse: 0.30132 | val_1_rmse: 0.31096 |  0:05:40s
epoch 16 | loss: 0.09079 | val_0_rmse: 0.30369 | val_1_rmse: 0.3144  |  0:06:01s
epoch 17 | loss: 0.08874 | val_0_rmse: 0.29968 | val_1_rmse: 0.32084 |  0:06:22s
epoch 18 | loss: 0.08856 | val_0_rmse: 0.29238 | val_1_rmse: 0.30257 |  0:06:43s
epoch 19 | loss: 0.09386 | val_0_rmse: 0.3296  | val_1_rmse: 0.33719 |  0:07:04s
epoch 20 | loss: 0.09384 | val_0_rmse: 0.33428 | val_1_rmse: 0.37356 |  0:07:26s
epoch 21 | loss: 0.08965 | val_0_rmse: 0.32154 | val_1_rmse: 0.34199 |  0:07:47s
epoch 22 | loss: 0.08692 | val_0_rmse: 0.31899 | val_1_rmse: 0.3565  |  0:08:08s
epoch 23 | loss: 0.08602 | val_0_rmse: 0.30627 | val_1_rmse: 0.3453  |  0:08:29s
epoch 24 | loss: 0.08724 | val_0_rmse: 0.29623 | val_1_rmse: 0.30849 |  0:08:50s
epoch 25 | loss: 0.0848  | val_0_rmse: 0.29937 | val_1_rmse: 0.30285 |  0:09:12s
epoch 26 | loss: 0.08381 | val_0_rmse: 0.28724 | val_1_rmse: 0.29974 |  0:09:33s
epoch 27 | loss: 0.08516 | val_0_rmse: 0.29572 | val_1_rmse: 0.31052 |  0:09:54s
epoch 28 | loss: 0.08475 | val_0_rmse: 0.29467 | val_1_rmse: 0.31466 |  0:10:15s
epoch 29 | loss: 0.08694 | val_0_rmse: 0.29228 | val_1_rmse: 0.30313 |  0:10:37s
epoch 30 | loss: 0.08611 | val_0_rmse: 0.29304 | val_1_rmse: 0.30568 |  0:10:58s
epoch 31 | loss: 0.08532 | val_0_rmse: 0.29118 | val_1_rmse: 0.30397 |  0:11:19s
epoch 32 | loss: 0.08478 | val_0_rmse: 0.29389 | val_1_rmse: 0.30903 |  0:11:40s
epoch 33 | loss: 0.08615 | val_0_rmse: 0.29598 | val_1_rmse: 0.31492 |  0:12:02s
epoch 34 | loss: 0.08364 | val_0_rmse: 0.29041 | val_1_rmse: 0.30584 |  0:12:23s
epoch 35 | loss: 0.08389 | val_0_rmse: 0.29514 | val_1_rmse: 0.30996 |  0:12:44s
epoch 36 | loss: 0.08435 | val_0_rmse: 0.29256 | val_1_rmse: 0.30627 |  0:13:05s
epoch 37 | loss: 0.08155 | val_0_rmse: 0.28906 | val_1_rmse: 0.3053  |  0:13:27s
epoch 38 | loss: 0.08113 | val_0_rmse: 0.28461 | val_1_rmse: 0.30054 |  0:13:48s
epoch 39 | loss: 0.08043 | val_0_rmse: 1.31625 | val_1_rmse: 0.31251 |  0:14:09s
epoch 40 | loss: 0.08097 | val_0_rmse: 0.37276 | val_1_rmse: 0.33273 |  0:14:30s
epoch 41 | loss: 0.08007 | val_0_rmse: 0.29274 | val_1_rmse: 0.31293 |  0:14:52s
epoch 42 | loss: 0.07869 | val_0_rmse: 0.3004  | val_1_rmse: 0.30196 |  0:15:13s
epoch 43 | loss: 0.07865 | val_0_rmse: 0.28348 | val_1_rmse: 0.30911 |  0:15:34s
epoch 44 | loss: 0.07863 | val_0_rmse: 0.28233 | val_1_rmse: 0.30344 |  0:15:55s
epoch 45 | loss: 0.0787  | val_0_rmse: 0.28358 | val_1_rmse: 0.31463 |  0:16:17s
epoch 46 | loss: 0.07957 | val_0_rmse: 0.29353 | val_1_rmse: 0.32928 |  0:16:38s
epoch 47 | loss: 0.07933 | val_0_rmse: 0.28086 | val_1_rmse: 0.30205 |  0:16:59s
epoch 48 | loss: 0.07685 | val_0_rmse: 0.27914 | val_1_rmse: 0.29912 |  0:17:20s
epoch 49 | loss: 0.07651 | val_0_rmse: 0.33452 | val_1_rmse: 0.30529 |  0:17:42s
epoch 50 | loss: 0.07672 | val_0_rmse: 0.28224 | val_1_rmse: 0.30693 |  0:18:03s
epoch 51 | loss: 0.08049 | val_0_rmse: 0.28263 | val_1_rmse: 0.35399 |  0:18:24s
epoch 52 | loss: 0.07782 | val_0_rmse: 0.28986 | val_1_rmse: 0.44342 |  0:18:46s
epoch 53 | loss: 0.07667 | val_0_rmse: 0.322   | val_1_rmse: 0.34403 |  0:19:07s
epoch 54 | loss: 0.07566 | val_0_rmse: 0.27855 | val_1_rmse: 0.30132 |  0:19:28s
epoch 55 | loss: 0.07489 | val_0_rmse: 0.44517 | val_1_rmse: 0.42603 |  0:19:49s
epoch 56 | loss: 0.07735 | val_0_rmse: 0.30434 | val_1_rmse: 0.33023 |  0:20:11s
epoch 57 | loss: 0.07844 | val_0_rmse: 0.30007 | val_1_rmse: 0.33348 |  0:20:32s
epoch 58 | loss: 0.07707 | val_0_rmse: 0.2792  | val_1_rmse: 0.30963 |  0:20:53s
epoch 59 | loss: 0.07528 | val_0_rmse: 0.27896 | val_1_rmse: 0.31305 |  0:21:14s
epoch 60 | loss: 0.07398 | val_0_rmse: 0.28599 | val_1_rmse: 0.31866 |  0:21:36s
epoch 61 | loss: 0.07371 | val_0_rmse: 0.27635 | val_1_rmse: 0.3066  |  0:21:57s
epoch 62 | loss: 0.07396 | val_0_rmse: 0.27487 | val_1_rmse: 0.31245 |  0:22:19s
epoch 63 | loss: 0.07395 | val_0_rmse: 0.28203 | val_1_rmse: 0.30951 |  0:22:40s
epoch 64 | loss: 0.07285 | val_0_rmse: 0.27994 | val_1_rmse: 0.30485 |  0:23:01s
epoch 65 | loss: 0.07249 | val_0_rmse: 0.29164 | val_1_rmse: 0.32297 |  0:23:23s
epoch 66 | loss: 0.07421 | val_0_rmse: 0.29652 | val_1_rmse: 0.3245  |  0:23:44s
epoch 67 | loss: 0.07398 | val_0_rmse: 0.29926 | val_1_rmse: 0.32031 |  0:24:06s
epoch 68 | loss: 0.07204 | val_0_rmse: 0.28502 | val_1_rmse: 0.31524 |  0:24:27s
epoch 69 | loss: 0.07174 | val_0_rmse: 0.3397  | val_1_rmse: 0.31344 |  0:24:48s
epoch 70 | loss: 0.07776 | val_0_rmse: 0.29471 | val_1_rmse: 0.32964 |  0:25:10s
epoch 71 | loss: 0.09181 | val_0_rmse: 0.52722 | val_1_rmse: 0.5275  |  0:25:31s
epoch 72 | loss: 0.10179 | val_0_rmse: 0.32155 | val_1_rmse: 0.33714 |  0:25:52s
epoch 73 | loss: 0.09571 | val_0_rmse: 0.31358 | val_1_rmse: 0.33048 |  0:26:13s
epoch 74 | loss: 0.09337 | val_0_rmse: 0.31293 | val_1_rmse: 0.33185 |  0:26:35s
epoch 75 | loss: 0.0925  | val_0_rmse: 0.32655 | val_1_rmse: 0.3466  |  0:26:56s
epoch 76 | loss: 0.09091 | val_0_rmse: 0.31866 | val_1_rmse: 0.35066 |  0:27:17s
epoch 77 | loss: 0.08995 | val_0_rmse: 0.31347 | val_1_rmse: 0.33964 |  0:27:38s
epoch 78 | loss: 0.08988 | val_0_rmse: 0.36733 | val_1_rmse: 0.39061 |  0:28:00s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.29912
Best weights from best epoch are automatically used!
ended training at: 23:52:59
Feature importance:
Mean squared error is of 0.08577898041557097
Mean absolute error:0.20683529988999802
MAPE:0.2353545889311602
R2 score:0.3091918506272727
------------------------------------------------------------------
Successfully saved model at C:\Users\mmend\PycharmProjects\Estagio\Models\Logs\TabNet\5x\Augmented\Above10%_coeficient(NoCoeficientNorm)\Logs\\\Model_saves\tabnet_model_4.zip
