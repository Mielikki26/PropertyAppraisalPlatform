TabNet Logs:

Saving copy of script...
In this script the datasets were geopy augmented.This is done to test the possibility that the number of features being used is too low and adding more is helpful
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:07:39
epoch 0  | loss: 1.22437 | val_0_rmse: 0.9994  | val_1_rmse: 1.00695 |  0:00:03s
epoch 1  | loss: 0.92934 | val_0_rmse: 0.93934 | val_1_rmse: 0.9461  |  0:00:05s
epoch 2  | loss: 0.73717 | val_0_rmse: 0.98811 | val_1_rmse: 0.99938 |  0:00:07s
epoch 3  | loss: 0.57054 | val_0_rmse: 0.81519 | val_1_rmse: 0.83347 |  0:00:09s
epoch 4  | loss: 0.49739 | val_0_rmse: 0.83717 | val_1_rmse: 0.83543 |  0:00:11s
epoch 5  | loss: 0.44979 | val_0_rmse: 0.86433 | val_1_rmse: 0.87601 |  0:00:13s
epoch 6  | loss: 0.43012 | val_0_rmse: 0.77605 | val_1_rmse: 0.7938  |  0:00:15s
epoch 7  | loss: 0.39466 | val_0_rmse: 0.71962 | val_1_rmse: 0.74331 |  0:00:17s
epoch 8  | loss: 0.38472 | val_0_rmse: 0.73649 | val_1_rmse: 0.75899 |  0:00:18s
epoch 9  | loss: 0.36497 | val_0_rmse: 0.73692 | val_1_rmse: 0.75204 |  0:00:20s
epoch 10 | loss: 0.35664 | val_0_rmse: 0.71377 | val_1_rmse: 0.73129 |  0:00:22s
epoch 11 | loss: 0.34011 | val_0_rmse: 0.69858 | val_1_rmse: 0.71492 |  0:00:24s
epoch 12 | loss: 0.329   | val_0_rmse: 0.65366 | val_1_rmse: 0.67595 |  0:00:26s
epoch 13 | loss: 0.34348 | val_0_rmse: 0.64092 | val_1_rmse: 0.66759 |  0:00:28s
epoch 14 | loss: 0.32292 | val_0_rmse: 0.63223 | val_1_rmse: 0.65537 |  0:00:30s
epoch 15 | loss: 0.31691 | val_0_rmse: 0.61992 | val_1_rmse: 0.64383 |  0:00:32s
epoch 16 | loss: 0.30319 | val_0_rmse: 0.65819 | val_1_rmse: 0.67627 |  0:00:34s
epoch 17 | loss: 0.31352 | val_0_rmse: 0.59129 | val_1_rmse: 0.61175 |  0:00:35s
epoch 18 | loss: 0.30128 | val_0_rmse: 0.58734 | val_1_rmse: 0.60669 |  0:00:37s
epoch 19 | loss: 0.30571 | val_0_rmse: 0.56196 | val_1_rmse: 0.58353 |  0:00:39s
epoch 20 | loss: 0.30203 | val_0_rmse: 0.55752 | val_1_rmse: 0.57301 |  0:00:41s
epoch 21 | loss: 0.29499 | val_0_rmse: 0.55288 | val_1_rmse: 0.57174 |  0:00:43s
epoch 22 | loss: 0.31983 | val_0_rmse: 0.57081 | val_1_rmse: 0.59212 |  0:00:45s
epoch 23 | loss: 0.30716 | val_0_rmse: 0.55224 | val_1_rmse: 0.57499 |  0:00:47s
epoch 24 | loss: 0.30866 | val_0_rmse: 0.55102 | val_1_rmse: 0.5818  |  0:00:49s
epoch 25 | loss: 0.3025  | val_0_rmse: 0.53112 | val_1_rmse: 0.56023 |  0:00:50s
epoch 26 | loss: 0.2876  | val_0_rmse: 0.51105 | val_1_rmse: 0.53903 |  0:00:52s
epoch 27 | loss: 0.27643 | val_0_rmse: 0.5116  | val_1_rmse: 0.53814 |  0:00:54s
epoch 28 | loss: 0.28414 | val_0_rmse: 0.53532 | val_1_rmse: 0.56214 |  0:00:56s
epoch 29 | loss: 0.30077 | val_0_rmse: 0.53049 | val_1_rmse: 0.5635  |  0:00:58s
epoch 30 | loss: 0.29301 | val_0_rmse: 0.51945 | val_1_rmse: 0.54525 |  0:01:00s
epoch 31 | loss: 0.29582 | val_0_rmse: 0.56765 | val_1_rmse: 0.58738 |  0:01:02s
epoch 32 | loss: 0.31588 | val_0_rmse: 0.57894 | val_1_rmse: 0.60249 |  0:01:04s
epoch 33 | loss: 0.28687 | val_0_rmse: 0.53912 | val_1_rmse: 0.57875 |  0:01:06s
epoch 34 | loss: 0.28498 | val_0_rmse: 0.54461 | val_1_rmse: 0.58243 |  0:01:07s
epoch 35 | loss: 0.27694 | val_0_rmse: 0.50318 | val_1_rmse: 0.53173 |  0:01:09s
epoch 36 | loss: 0.2673  | val_0_rmse: 0.55415 | val_1_rmse: 0.52733 |  0:01:11s
epoch 37 | loss: 0.26573 | val_0_rmse: 0.51306 | val_1_rmse: 0.55025 |  0:01:13s
epoch 38 | loss: 0.29577 | val_0_rmse: 0.52606 | val_1_rmse: 0.54938 |  0:01:15s
epoch 39 | loss: 0.28335 | val_0_rmse: 0.50515 | val_1_rmse: 0.53409 |  0:01:17s
epoch 40 | loss: 0.27152 | val_0_rmse: 0.49745 | val_1_rmse: 0.53206 |  0:01:19s
epoch 41 | loss: 0.25721 | val_0_rmse: 0.49058 | val_1_rmse: 0.53002 |  0:01:21s
epoch 42 | loss: 0.25872 | val_0_rmse: 0.48431 | val_1_rmse: 0.52058 |  0:01:22s
epoch 43 | loss: 0.25349 | val_0_rmse: 0.49093 | val_1_rmse: 0.53054 |  0:01:24s
epoch 44 | loss: 0.25818 | val_0_rmse: 0.50449 | val_1_rmse: 0.53641 |  0:01:26s
epoch 45 | loss: 0.25289 | val_0_rmse: 0.48366 | val_1_rmse: 0.52846 |  0:01:28s
epoch 46 | loss: 0.24508 | val_0_rmse: 0.47501 | val_1_rmse: 0.51136 |  0:01:30s
epoch 47 | loss: 0.24558 | val_0_rmse: 0.47861 | val_1_rmse: 0.51767 |  0:01:32s
epoch 48 | loss: 0.24478 | val_0_rmse: 0.49024 | val_1_rmse: 0.52754 |  0:01:34s
epoch 49 | loss: 0.25288 | val_0_rmse: 0.49816 | val_1_rmse: 0.53953 |  0:01:36s
epoch 50 | loss: 0.25238 | val_0_rmse: 0.47955 | val_1_rmse: 0.52475 |  0:01:37s
epoch 51 | loss: 0.25242 | val_0_rmse: 0.48397 | val_1_rmse: 0.52586 |  0:01:39s
epoch 52 | loss: 0.24739 | val_0_rmse: 0.47906 | val_1_rmse: 0.52986 |  0:01:41s
epoch 53 | loss: 0.24066 | val_0_rmse: 0.4718  | val_1_rmse: 0.52049 |  0:01:43s
epoch 54 | loss: 0.23811 | val_0_rmse: 0.46932 | val_1_rmse: 0.51625 |  0:01:45s
epoch 55 | loss: 0.23539 | val_0_rmse: 0.4679  | val_1_rmse: 0.51755 |  0:01:47s
epoch 56 | loss: 0.24852 | val_0_rmse: 0.48818 | val_1_rmse: 0.52496 |  0:01:49s
epoch 57 | loss: 0.24249 | val_0_rmse: 0.47723 | val_1_rmse: 0.52021 |  0:01:51s
epoch 58 | loss: 0.23858 | val_0_rmse: 0.47493 | val_1_rmse: 0.51731 |  0:01:52s
epoch 59 | loss: 0.24202 | val_0_rmse: 0.46872 | val_1_rmse: 0.51419 |  0:01:54s
epoch 60 | loss: 0.23412 | val_0_rmse: 0.46086 | val_1_rmse: 0.51363 |  0:01:56s
epoch 61 | loss: 0.22912 | val_0_rmse: 0.46748 | val_1_rmse: 0.51566 |  0:01:58s
epoch 62 | loss: 0.23412 | val_0_rmse: 0.45766 | val_1_rmse: 0.51106 |  0:02:00s
epoch 63 | loss: 0.23183 | val_0_rmse: 0.45877 | val_1_rmse: 0.5122  |  0:02:02s
epoch 64 | loss: 0.23202 | val_0_rmse: 0.45792 | val_1_rmse: 0.51095 |  0:02:04s
epoch 65 | loss: 0.22842 | val_0_rmse: 0.45296 | val_1_rmse: 0.50833 |  0:02:06s
epoch 66 | loss: 0.2244  | val_0_rmse: 0.45736 | val_1_rmse: 0.51222 |  0:02:07s
epoch 67 | loss: 0.22223 | val_0_rmse: 0.45894 | val_1_rmse: 0.51281 |  0:02:09s
epoch 68 | loss: 0.22495 | val_0_rmse: 0.45676 | val_1_rmse: 0.51427 |  0:02:11s
epoch 69 | loss: 0.22314 | val_0_rmse: 0.65377 | val_1_rmse: 0.71466 |  0:02:13s
epoch 70 | loss: 0.22473 | val_0_rmse: 0.44738 | val_1_rmse: 0.51232 |  0:02:15s
epoch 71 | loss: 0.22413 | val_0_rmse: 0.45128 | val_1_rmse: 0.51305 |  0:02:17s
epoch 72 | loss: 0.22526 | val_0_rmse: 0.45051 | val_1_rmse: 0.51634 |  0:02:19s
epoch 73 | loss: 0.22735 | val_0_rmse: 0.46063 | val_1_rmse: 0.52249 |  0:02:21s
epoch 74 | loss: 0.22047 | val_0_rmse: 0.45659 | val_1_rmse: 0.51709 |  0:02:22s
epoch 75 | loss: 0.22254 | val_0_rmse: 0.45093 | val_1_rmse: 0.51392 |  0:02:24s
epoch 76 | loss: 0.22124 | val_0_rmse: 0.45391 | val_1_rmse: 0.51972 |  0:02:26s
epoch 77 | loss: 0.22215 | val_0_rmse: 0.44997 | val_1_rmse: 0.51654 |  0:02:28s
epoch 78 | loss: 0.22595 | val_0_rmse: 0.44999 | val_1_rmse: 0.5149  |  0:02:30s
epoch 79 | loss: 0.23484 | val_0_rmse: 0.47734 | val_1_rmse: 0.53441 |  0:02:32s
epoch 80 | loss: 0.24303 | val_0_rmse: 0.48365 | val_1_rmse: 0.53945 |  0:02:34s
epoch 81 | loss: 0.23503 | val_0_rmse: 0.47926 | val_1_rmse: 0.54669 |  0:02:36s
epoch 82 | loss: 0.22947 | val_0_rmse: 0.48177 | val_1_rmse: 0.54319 |  0:02:37s
epoch 83 | loss: 0.24316 | val_0_rmse: 0.46083 | val_1_rmse: 0.52528 |  0:02:39s
epoch 84 | loss: 0.23732 | val_0_rmse: 0.46541 | val_1_rmse: 0.52801 |  0:02:41s
epoch 85 | loss: 0.23321 | val_0_rmse: 0.46597 | val_1_rmse: 0.53383 |  0:02:43s
epoch 86 | loss: 0.2488  | val_0_rmse: 0.49349 | val_1_rmse: 0.55894 |  0:02:45s
epoch 87 | loss: 0.24656 | val_0_rmse: 0.47443 | val_1_rmse: 0.5265  |  0:02:47s
epoch 88 | loss: 0.24538 | val_0_rmse: 0.48088 | val_1_rmse: 0.53527 |  0:02:49s
epoch 89 | loss: 0.23778 | val_0_rmse: 0.46542 | val_1_rmse: 0.52902 |  0:02:51s
epoch 90 | loss: 0.22779 | val_0_rmse: 0.47065 | val_1_rmse: 0.53146 |  0:02:52s
epoch 91 | loss: 0.22727 | val_0_rmse: 0.45188 | val_1_rmse: 0.51665 |  0:02:54s
epoch 92 | loss: 0.21934 | val_0_rmse: 0.45984 | val_1_rmse: 0.52755 |  0:02:56s
epoch 93 | loss: 0.22779 | val_0_rmse: 0.45436 | val_1_rmse: 0.52032 |  0:02:58s
epoch 94 | loss: 0.2226  | val_0_rmse: 0.45255 | val_1_rmse: 0.52222 |  0:03:00s
epoch 95 | loss: 0.21885 | val_0_rmse: 0.46969 | val_1_rmse: 0.54507 |  0:03:02s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.50833
Best weights from best epoch are automatically used!
ended training at: 06:10:42
Feature importance:
Mean squared error is of 5369111263.11793
Mean absolute error:48959.97332170102
MAPE:0.15124762904739678
R2 score:0.7577012411574402
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:10:43
epoch 0  | loss: 1.25913 | val_0_rmse: 0.99072 | val_1_rmse: 0.98682 |  0:00:01s
epoch 1  | loss: 0.82146 | val_0_rmse: 0.87394 | val_1_rmse: 0.86312 |  0:00:03s
epoch 2  | loss: 0.63212 | val_0_rmse: 0.79644 | val_1_rmse: 0.79139 |  0:00:05s
epoch 3  | loss: 0.49294 | val_0_rmse: 0.80038 | val_1_rmse: 0.79688 |  0:00:07s
epoch 4  | loss: 0.43778 | val_0_rmse: 0.73514 | val_1_rmse: 0.73282 |  0:00:09s
epoch 5  | loss: 0.4052  | val_0_rmse: 0.69421 | val_1_rmse: 0.69554 |  0:00:11s
epoch 6  | loss: 0.36389 | val_0_rmse: 0.68682 | val_1_rmse: 0.69191 |  0:00:13s
epoch 7  | loss: 0.34572 | val_0_rmse: 0.66178 | val_1_rmse: 0.66596 |  0:00:15s
epoch 8  | loss: 0.32957 | val_0_rmse: 0.67795 | val_1_rmse: 0.68068 |  0:00:16s
epoch 9  | loss: 0.32843 | val_0_rmse: 0.65789 | val_1_rmse: 0.66651 |  0:00:18s
epoch 10 | loss: 0.32729 | val_0_rmse: 0.62807 | val_1_rmse: 0.64007 |  0:00:20s
epoch 11 | loss: 0.31847 | val_0_rmse: 0.65817 | val_1_rmse: 0.66594 |  0:00:22s
epoch 12 | loss: 0.31354 | val_0_rmse: 0.62294 | val_1_rmse: 0.63017 |  0:00:24s
epoch 13 | loss: 0.30769 | val_0_rmse: 0.59128 | val_1_rmse: 0.60144 |  0:00:26s
epoch 14 | loss: 0.30004 | val_0_rmse: 0.58602 | val_1_rmse: 0.59725 |  0:00:28s
epoch 15 | loss: 0.29332 | val_0_rmse: 0.56736 | val_1_rmse: 0.58032 |  0:00:30s
epoch 16 | loss: 0.29104 | val_0_rmse: 0.56739 | val_1_rmse: 0.57936 |  0:00:32s
epoch 17 | loss: 0.28643 | val_0_rmse: 0.5619  | val_1_rmse: 0.57147 |  0:00:33s
epoch 18 | loss: 0.28298 | val_0_rmse: 0.54397 | val_1_rmse: 0.56002 |  0:00:35s
epoch 19 | loss: 0.27654 | val_0_rmse: 0.53802 | val_1_rmse: 0.55188 |  0:00:37s
epoch 20 | loss: 0.27494 | val_0_rmse: 0.54356 | val_1_rmse: 0.55589 |  0:00:39s
epoch 21 | loss: 0.28105 | val_0_rmse: 0.51631 | val_1_rmse: 0.53052 |  0:00:41s
epoch 22 | loss: 0.27549 | val_0_rmse: 0.51542 | val_1_rmse: 0.53362 |  0:00:43s
epoch 23 | loss: 0.29104 | val_0_rmse: 0.52628 | val_1_rmse: 0.54507 |  0:00:45s
epoch 24 | loss: 0.27888 | val_0_rmse: 0.5214  | val_1_rmse: 0.54141 |  0:00:47s
epoch 25 | loss: 0.2933  | val_0_rmse: 0.51345 | val_1_rmse: 0.53744 |  0:00:49s
epoch 26 | loss: 0.28229 | val_0_rmse: 0.50944 | val_1_rmse: 0.5299  |  0:00:50s
epoch 27 | loss: 0.27478 | val_0_rmse: 0.5041  | val_1_rmse: 0.52211 |  0:00:52s
epoch 28 | loss: 0.27978 | val_0_rmse: 0.52023 | val_1_rmse: 0.53936 |  0:00:54s
epoch 29 | loss: 0.28939 | val_0_rmse: 0.51541 | val_1_rmse: 0.53932 |  0:00:56s
epoch 30 | loss: 0.29327 | val_0_rmse: 0.52049 | val_1_rmse: 0.54602 |  0:00:58s
epoch 31 | loss: 0.28443 | val_0_rmse: 0.50583 | val_1_rmse: 0.52941 |  0:01:00s
epoch 32 | loss: 0.27915 | val_0_rmse: 0.50891 | val_1_rmse: 0.53357 |  0:01:02s
epoch 33 | loss: 0.27359 | val_0_rmse: 0.49675 | val_1_rmse: 0.53138 |  0:01:04s
epoch 34 | loss: 0.27362 | val_0_rmse: 0.50702 | val_1_rmse: 0.53349 |  0:01:05s
epoch 35 | loss: 0.28623 | val_0_rmse: 0.51725 | val_1_rmse: 0.53831 |  0:01:07s
epoch 36 | loss: 0.28002 | val_0_rmse: 0.50762 | val_1_rmse: 0.53872 |  0:01:09s
epoch 37 | loss: 0.27042 | val_0_rmse: 0.50346 | val_1_rmse: 0.53612 |  0:01:11s
epoch 38 | loss: 0.27264 | val_0_rmse: 0.49845 | val_1_rmse: 0.52553 |  0:01:13s
epoch 39 | loss: 0.28535 | val_0_rmse: 0.53934 | val_1_rmse: 0.56119 |  0:01:15s
epoch 40 | loss: 0.2877  | val_0_rmse: 0.51729 | val_1_rmse: 0.54655 |  0:01:17s
epoch 41 | loss: 0.28695 | val_0_rmse: 0.52776 | val_1_rmse: 0.56227 |  0:01:19s
epoch 42 | loss: 0.27516 | val_0_rmse: 0.49949 | val_1_rmse: 0.53351 |  0:01:21s
epoch 43 | loss: 0.26179 | val_0_rmse: 0.49635 | val_1_rmse: 0.53364 |  0:01:22s
epoch 44 | loss: 0.27245 | val_0_rmse: 0.5072  | val_1_rmse: 0.53572 |  0:01:24s
epoch 45 | loss: 0.26123 | val_0_rmse: 0.49728 | val_1_rmse: 0.53622 |  0:01:26s
epoch 46 | loss: 0.26095 | val_0_rmse: 0.49051 | val_1_rmse: 0.52085 |  0:01:28s
epoch 47 | loss: 0.25538 | val_0_rmse: 0.48544 | val_1_rmse: 0.5227  |  0:01:30s
epoch 48 | loss: 0.25772 | val_0_rmse: 0.48732 | val_1_rmse: 0.52566 |  0:01:32s
epoch 49 | loss: 0.25512 | val_0_rmse: 0.5037  | val_1_rmse: 0.52953 |  0:01:34s
epoch 50 | loss: 0.26819 | val_0_rmse: 0.50746 | val_1_rmse: 0.54443 |  0:01:36s
epoch 51 | loss: 0.25614 | val_0_rmse: 0.48502 | val_1_rmse: 0.52308 |  0:01:37s
epoch 52 | loss: 0.25102 | val_0_rmse: 0.49213 | val_1_rmse: 0.52555 |  0:01:39s
epoch 53 | loss: 0.25227 | val_0_rmse: 0.4873  | val_1_rmse: 0.52116 |  0:01:41s
epoch 54 | loss: 0.25308 | val_0_rmse: 0.50003 | val_1_rmse: 0.53447 |  0:01:43s
epoch 55 | loss: 0.25764 | val_0_rmse: 0.48331 | val_1_rmse: 0.5248  |  0:01:45s
epoch 56 | loss: 0.24534 | val_0_rmse: 0.48479 | val_1_rmse: 0.52395 |  0:01:47s
epoch 57 | loss: 0.24798 | val_0_rmse: 0.47927 | val_1_rmse: 0.5161  |  0:01:49s
epoch 58 | loss: 0.24942 | val_0_rmse: 0.48617 | val_1_rmse: 0.52352 |  0:01:51s
epoch 59 | loss: 0.25653 | val_0_rmse: 0.48652 | val_1_rmse: 0.52313 |  0:01:53s
epoch 60 | loss: 0.25179 | val_0_rmse: 0.48379 | val_1_rmse: 0.52191 |  0:01:54s
epoch 61 | loss: 0.24905 | val_0_rmse: 0.48668 | val_1_rmse: 0.5281  |  0:01:56s
epoch 62 | loss: 0.24921 | val_0_rmse: 0.48389 | val_1_rmse: 0.52639 |  0:01:58s
epoch 63 | loss: 0.25034 | val_0_rmse: 0.48302 | val_1_rmse: 0.5188  |  0:02:00s
epoch 64 | loss: 0.25193 | val_0_rmse: 0.47992 | val_1_rmse: 0.52031 |  0:02:02s
epoch 65 | loss: 0.24964 | val_0_rmse: 0.48372 | val_1_rmse: 0.52126 |  0:02:04s
epoch 66 | loss: 0.25015 | val_0_rmse: 0.479   | val_1_rmse: 0.52224 |  0:02:06s
epoch 67 | loss: 0.24717 | val_0_rmse: 0.4788  | val_1_rmse: 0.52422 |  0:02:07s
epoch 68 | loss: 0.24208 | val_0_rmse: 0.47636 | val_1_rmse: 0.52422 |  0:02:09s
epoch 69 | loss: 0.24398 | val_0_rmse: 0.47766 | val_1_rmse: 0.51787 |  0:02:11s
epoch 70 | loss: 0.23918 | val_0_rmse: 0.47871 | val_1_rmse: 0.52031 |  0:02:13s
epoch 71 | loss: 0.23826 | val_0_rmse: 0.48708 | val_1_rmse: 0.52772 |  0:02:15s
epoch 72 | loss: 0.23853 | val_0_rmse: 0.47047 | val_1_rmse: 0.51774 |  0:02:17s
epoch 73 | loss: 0.24031 | val_0_rmse: 0.4698  | val_1_rmse: 0.51883 |  0:02:19s
epoch 74 | loss: 0.23855 | val_0_rmse: 0.46713 | val_1_rmse: 0.51246 |  0:02:21s
epoch 75 | loss: 0.24352 | val_0_rmse: 0.46865 | val_1_rmse: 0.51584 |  0:02:22s
epoch 76 | loss: 0.23088 | val_0_rmse: 0.46715 | val_1_rmse: 0.51453 |  0:02:24s
epoch 77 | loss: 0.23477 | val_0_rmse: 0.46788 | val_1_rmse: 0.5155  |  0:02:26s
epoch 78 | loss: 0.22972 | val_0_rmse: 0.46041 | val_1_rmse: 0.51571 |  0:02:28s
epoch 79 | loss: 0.22994 | val_0_rmse: 0.46479 | val_1_rmse: 0.51311 |  0:02:30s
epoch 80 | loss: 0.24188 | val_0_rmse: 0.47251 | val_1_rmse: 0.52316 |  0:02:32s
epoch 81 | loss: 0.24182 | val_0_rmse: 0.48347 | val_1_rmse: 0.52934 |  0:02:34s
epoch 82 | loss: 0.25554 | val_0_rmse: 0.51482 | val_1_rmse: 0.55219 |  0:02:36s
epoch 83 | loss: 0.27131 | val_0_rmse: 0.50664 | val_1_rmse: 0.54034 |  0:02:37s
epoch 84 | loss: 0.2697  | val_0_rmse: 0.50637 | val_1_rmse: 0.54195 |  0:02:39s
epoch 85 | loss: 0.30495 | val_0_rmse: 0.51474 | val_1_rmse: 0.54727 |  0:02:41s
epoch 86 | loss: 0.2825  | val_0_rmse: 0.51696 | val_1_rmse: 0.55911 |  0:02:43s
epoch 87 | loss: 0.27796 | val_0_rmse: 0.53056 | val_1_rmse: 0.56415 |  0:02:45s
epoch 88 | loss: 0.30466 | val_0_rmse: 0.52916 | val_1_rmse: 0.56418 |  0:02:47s
epoch 89 | loss: 0.28926 | val_0_rmse: 0.53259 | val_1_rmse: 0.56955 |  0:02:49s
epoch 90 | loss: 0.30434 | val_0_rmse: 0.53353 | val_1_rmse: 0.56806 |  0:02:51s
epoch 91 | loss: 0.29576 | val_0_rmse: 0.50754 | val_1_rmse: 0.54105 |  0:02:52s
epoch 92 | loss: 0.27126 | val_0_rmse: 0.49672 | val_1_rmse: 0.5293  |  0:02:54s
epoch 93 | loss: 0.26379 | val_0_rmse: 0.49337 | val_1_rmse: 0.5297  |  0:02:56s
epoch 94 | loss: 0.25494 | val_0_rmse: 0.48731 | val_1_rmse: 0.52575 |  0:02:58s
epoch 95 | loss: 0.25049 | val_0_rmse: 0.48233 | val_1_rmse: 0.52153 |  0:03:00s
epoch 96 | loss: 0.24949 | val_0_rmse: 0.48239 | val_1_rmse: 0.52276 |  0:03:02s
epoch 97 | loss: 0.24936 | val_0_rmse: 0.47617 | val_1_rmse: 0.5161  |  0:03:04s
epoch 98 | loss: 0.244   | val_0_rmse: 0.48035 | val_1_rmse: 0.51891 |  0:03:06s
epoch 99 | loss: 0.25034 | val_0_rmse: 0.47844 | val_1_rmse: 0.5183  |  0:03:07s
epoch 100| loss: 0.24531 | val_0_rmse: 0.47065 | val_1_rmse: 0.51538 |  0:03:09s
epoch 101| loss: 0.24061 | val_0_rmse: 0.4677  | val_1_rmse: 0.51471 |  0:03:11s
epoch 102| loss: 0.23918 | val_0_rmse: 0.46732 | val_1_rmse: 0.51491 |  0:03:13s
epoch 103| loss: 0.23746 | val_0_rmse: 0.47275 | val_1_rmse: 0.52445 |  0:03:15s
epoch 104| loss: 0.2319  | val_0_rmse: 0.46434 | val_1_rmse: 0.51595 |  0:03:17s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.51246
Best weights from best epoch are automatically used!
ended training at: 06:14:01
Feature importance:
Mean squared error is of 5835433820.257717
Mean absolute error:50537.87816938201
MAPE:0.1561514136689059
R2 score:0.7451345552533972
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:14:01
epoch 0  | loss: 1.21616 | val_0_rmse: 1.00348 | val_1_rmse: 0.99502 |  0:00:01s
epoch 1  | loss: 1.00046 | val_0_rmse: 0.97353 | val_1_rmse: 0.96917 |  0:00:03s
epoch 2  | loss: 0.81298 | val_0_rmse: 0.93657 | val_1_rmse: 0.94129 |  0:00:05s
epoch 3  | loss: 0.54618 | val_0_rmse: 0.82328 | val_1_rmse: 0.82599 |  0:00:07s
epoch 4  | loss: 0.44317 | val_0_rmse: 0.78745 | val_1_rmse: 0.78667 |  0:00:09s
epoch 5  | loss: 0.39561 | val_0_rmse: 0.77361 | val_1_rmse: 0.77306 |  0:00:11s
epoch 6  | loss: 0.34481 | val_0_rmse: 0.7365  | val_1_rmse: 0.73866 |  0:00:13s
epoch 7  | loss: 0.3161  | val_0_rmse: 0.70595 | val_1_rmse: 0.7064  |  0:00:15s
epoch 8  | loss: 0.30618 | val_0_rmse: 0.69678 | val_1_rmse: 0.69684 |  0:00:17s
epoch 9  | loss: 0.29588 | val_0_rmse: 0.67323 | val_1_rmse: 0.67571 |  0:00:18s
epoch 10 | loss: 0.28668 | val_0_rmse: 0.67653 | val_1_rmse: 0.67829 |  0:00:20s
epoch 11 | loss: 0.29884 | val_0_rmse: 0.65992 | val_1_rmse: 0.66442 |  0:00:22s
epoch 12 | loss: 0.28956 | val_0_rmse: 0.63658 | val_1_rmse: 0.63772 |  0:00:24s
epoch 13 | loss: 0.28295 | val_0_rmse: 0.63341 | val_1_rmse: 0.63675 |  0:00:26s
epoch 14 | loss: 0.27237 | val_0_rmse: 0.5958  | val_1_rmse: 0.60147 |  0:00:28s
epoch 15 | loss: 0.26893 | val_0_rmse: 0.58517 | val_1_rmse: 0.58943 |  0:00:30s
epoch 16 | loss: 0.26239 | val_0_rmse: 0.6146  | val_1_rmse: 0.62044 |  0:00:32s
epoch 17 | loss: 0.27012 | val_0_rmse: 0.57699 | val_1_rmse: 0.58428 |  0:00:33s
epoch 18 | loss: 0.28116 | val_0_rmse: 0.55057 | val_1_rmse: 0.56128 |  0:00:35s
epoch 19 | loss: 0.2628  | val_0_rmse: 0.54082 | val_1_rmse: 0.54865 |  0:00:37s
epoch 20 | loss: 0.257   | val_0_rmse: 0.53965 | val_1_rmse: 0.54977 |  0:00:39s
epoch 21 | loss: 0.26215 | val_0_rmse: 0.52113 | val_1_rmse: 0.53312 |  0:00:41s
epoch 22 | loss: 0.25379 | val_0_rmse: 0.51061 | val_1_rmse: 0.52629 |  0:00:43s
epoch 23 | loss: 0.24937 | val_0_rmse: 0.49105 | val_1_rmse: 0.50837 |  0:00:45s
epoch 24 | loss: 0.24957 | val_0_rmse: 0.493   | val_1_rmse: 0.51254 |  0:00:47s
epoch 25 | loss: 0.24719 | val_0_rmse: 0.50199 | val_1_rmse: 0.52071 |  0:00:49s
epoch 26 | loss: 0.24238 | val_0_rmse: 0.47321 | val_1_rmse: 0.49977 |  0:00:50s
epoch 27 | loss: 0.24507 | val_0_rmse: 0.47587 | val_1_rmse: 0.50213 |  0:00:52s
epoch 28 | loss: 0.24388 | val_0_rmse: 0.48091 | val_1_rmse: 0.50333 |  0:00:54s
epoch 29 | loss: 0.23815 | val_0_rmse: 0.46207 | val_1_rmse: 0.49412 |  0:00:56s
epoch 30 | loss: 0.23484 | val_0_rmse: 0.47203 | val_1_rmse: 0.49961 |  0:00:58s
epoch 31 | loss: 0.23718 | val_0_rmse: 0.47837 | val_1_rmse: 0.50764 |  0:01:00s
epoch 32 | loss: 0.2368  | val_0_rmse: 0.54605 | val_1_rmse: 0.5658  |  0:01:02s
epoch 33 | loss: 0.23705 | val_0_rmse: 0.46055 | val_1_rmse: 0.4935  |  0:01:04s
epoch 34 | loss: 0.23934 | val_0_rmse: 0.46976 | val_1_rmse: 0.50495 |  0:01:05s
epoch 35 | loss: 0.23995 | val_0_rmse: 0.45433 | val_1_rmse: 0.49101 |  0:01:07s
epoch 36 | loss: 0.23108 | val_0_rmse: 0.46243 | val_1_rmse: 0.49609 |  0:01:09s
epoch 37 | loss: 0.23007 | val_0_rmse: 0.46244 | val_1_rmse: 0.49827 |  0:01:11s
epoch 38 | loss: 0.23057 | val_0_rmse: 0.45087 | val_1_rmse: 0.49016 |  0:01:13s
epoch 39 | loss: 0.22929 | val_0_rmse: 0.45195 | val_1_rmse: 0.49743 |  0:01:15s
epoch 40 | loss: 0.23205 | val_0_rmse: 0.45137 | val_1_rmse: 0.49736 |  0:01:17s
epoch 41 | loss: 0.22244 | val_0_rmse: 0.45221 | val_1_rmse: 0.49233 |  0:01:19s
epoch 42 | loss: 0.2305  | val_0_rmse: 0.46074 | val_1_rmse: 0.49605 |  0:01:21s
epoch 43 | loss: 0.22988 | val_0_rmse: 0.4813  | val_1_rmse: 0.51141 |  0:01:22s
epoch 44 | loss: 0.22983 | val_0_rmse: 0.4818  | val_1_rmse: 0.52565 |  0:01:24s
epoch 45 | loss: 0.22956 | val_0_rmse: 0.45222 | val_1_rmse: 0.49233 |  0:01:26s
epoch 46 | loss: 0.2232  | val_0_rmse: 0.46163 | val_1_rmse: 0.50204 |  0:01:28s
epoch 47 | loss: 0.22368 | val_0_rmse: 0.44472 | val_1_rmse: 0.49004 |  0:01:30s
epoch 48 | loss: 0.2155  | val_0_rmse: 0.45092 | val_1_rmse: 0.49579 |  0:01:32s
epoch 49 | loss: 0.21668 | val_0_rmse: 0.44843 | val_1_rmse: 0.49578 |  0:01:34s
epoch 50 | loss: 0.22694 | val_0_rmse: 0.45242 | val_1_rmse: 0.49621 |  0:01:36s
epoch 51 | loss: 0.21546 | val_0_rmse: 0.4403  | val_1_rmse: 0.49492 |  0:01:37s
epoch 52 | loss: 0.2107  | val_0_rmse: 0.43247 | val_1_rmse: 0.48715 |  0:01:39s
epoch 53 | loss: 0.21481 | val_0_rmse: 0.439   | val_1_rmse: 0.49494 |  0:01:41s
epoch 54 | loss: 0.21477 | val_0_rmse: 0.43933 | val_1_rmse: 0.49563 |  0:01:43s
epoch 55 | loss: 0.2165  | val_0_rmse: 0.43495 | val_1_rmse: 0.49731 |  0:01:45s
epoch 56 | loss: 0.21342 | val_0_rmse: 0.43807 | val_1_rmse: 0.5056  |  0:01:47s
epoch 57 | loss: 0.20576 | val_0_rmse: 0.4444  | val_1_rmse: 0.50679 |  0:01:49s
epoch 58 | loss: 0.22057 | val_0_rmse: 0.44663 | val_1_rmse: 0.49894 |  0:01:51s
epoch 59 | loss: 0.21414 | val_0_rmse: 0.43839 | val_1_rmse: 0.49282 |  0:01:53s
epoch 60 | loss: 0.21059 | val_0_rmse: 0.43346 | val_1_rmse: 0.49614 |  0:01:54s
epoch 61 | loss: 0.21372 | val_0_rmse: 0.44794 | val_1_rmse: 0.51036 |  0:01:56s
epoch 62 | loss: 0.21374 | val_0_rmse: 0.45475 | val_1_rmse: 0.50994 |  0:01:58s
epoch 63 | loss: 0.2107  | val_0_rmse: 0.43311 | val_1_rmse: 0.49686 |  0:02:00s
epoch 64 | loss: 0.21612 | val_0_rmse: 0.45765 | val_1_rmse: 0.51855 |  0:02:02s
epoch 65 | loss: 0.2162  | val_0_rmse: 0.45831 | val_1_rmse: 0.53037 |  0:02:04s
epoch 66 | loss: 0.21018 | val_0_rmse: 0.43019 | val_1_rmse: 0.49505 |  0:02:06s
epoch 67 | loss: 0.21088 | val_0_rmse: 0.43362 | val_1_rmse: 0.497   |  0:02:08s
epoch 68 | loss: 0.20155 | val_0_rmse: 0.42754 | val_1_rmse: 0.49737 |  0:02:10s
epoch 69 | loss: 0.20554 | val_0_rmse: 0.43816 | val_1_rmse: 0.51104 |  0:02:11s
epoch 70 | loss: 0.2005  | val_0_rmse: 0.42302 | val_1_rmse: 0.4953  |  0:02:13s
epoch 71 | loss: 0.20411 | val_0_rmse: 0.42674 | val_1_rmse: 0.50225 |  0:02:15s
epoch 72 | loss: 0.20256 | val_0_rmse: 0.42001 | val_1_rmse: 0.49821 |  0:02:17s
epoch 73 | loss: 0.20308 | val_0_rmse: 0.43401 | val_1_rmse: 0.51271 |  0:02:19s
epoch 74 | loss: 0.19874 | val_0_rmse: 0.4254  | val_1_rmse: 0.50548 |  0:02:21s
epoch 75 | loss: 0.1969  | val_0_rmse: 0.42393 | val_1_rmse: 0.50376 |  0:02:23s
epoch 76 | loss: 0.19958 | val_0_rmse: 0.41467 | val_1_rmse: 0.50286 |  0:02:25s
epoch 77 | loss: 0.19768 | val_0_rmse: 0.41042 | val_1_rmse: 0.49843 |  0:02:26s
epoch 78 | loss: 0.19281 | val_0_rmse: 0.4135  | val_1_rmse: 0.49278 |  0:02:28s
epoch 79 | loss: 0.20018 | val_0_rmse: 0.41609 | val_1_rmse: 0.50327 |  0:02:30s
epoch 80 | loss: 0.20149 | val_0_rmse: 0.43106 | val_1_rmse: 0.51502 |  0:02:32s
epoch 81 | loss: 0.19822 | val_0_rmse: 0.41325 | val_1_rmse: 0.50791 |  0:02:34s
epoch 82 | loss: 0.1932  | val_0_rmse: 0.41377 | val_1_rmse: 0.50553 |  0:02:36s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.48715
Best weights from best epoch are automatically used!
ended training at: 06:16:39
Feature importance:
Mean squared error is of 5828531393.676751
Mean absolute error:50275.76774719506
MAPE:0.1597159345169223
R2 score:0.7383252876393389
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:16:39
epoch 0  | loss: 1.23869 | val_0_rmse: 0.99754 | val_1_rmse: 0.99261 |  0:00:01s
epoch 1  | loss: 0.94148 | val_0_rmse: 0.97084 | val_1_rmse: 0.96588 |  0:00:03s
epoch 2  | loss: 0.76848 | val_0_rmse: 0.9412  | val_1_rmse: 0.93752 |  0:00:05s
epoch 3  | loss: 0.58363 | val_0_rmse: 0.82253 | val_1_rmse: 0.81608 |  0:00:07s
epoch 4  | loss: 0.47449 | val_0_rmse: 0.79836 | val_1_rmse: 0.79129 |  0:00:09s
epoch 5  | loss: 0.41164 | val_0_rmse: 0.78373 | val_1_rmse: 0.77604 |  0:00:11s
epoch 6  | loss: 0.36607 | val_0_rmse: 0.76236 | val_1_rmse: 0.75164 |  0:00:13s
epoch 7  | loss: 0.3641  | val_0_rmse: 0.75535 | val_1_rmse: 0.74437 |  0:00:15s
epoch 8  | loss: 0.32223 | val_0_rmse: 0.71487 | val_1_rmse: 0.70635 |  0:00:17s
epoch 9  | loss: 0.30947 | val_0_rmse: 0.69086 | val_1_rmse: 0.68245 |  0:00:18s
epoch 10 | loss: 0.30108 | val_0_rmse: 0.67285 | val_1_rmse: 0.66299 |  0:00:20s
epoch 11 | loss: 0.2877  | val_0_rmse: 0.67096 | val_1_rmse: 0.66317 |  0:00:22s
epoch 12 | loss: 0.27998 | val_0_rmse: 0.65103 | val_1_rmse: 0.64182 |  0:00:24s
epoch 13 | loss: 0.27668 | val_0_rmse: 0.6247  | val_1_rmse: 0.61729 |  0:00:26s
epoch 14 | loss: 0.27099 | val_0_rmse: 0.62111 | val_1_rmse: 0.61479 |  0:00:28s
epoch 15 | loss: 0.25801 | val_0_rmse: 0.59179 | val_1_rmse: 0.59019 |  0:00:30s
epoch 16 | loss: 0.26358 | val_0_rmse: 0.58918 | val_1_rmse: 0.58714 |  0:00:32s
epoch 17 | loss: 0.25856 | val_0_rmse: 0.62353 | val_1_rmse: 0.62175 |  0:00:34s
epoch 18 | loss: 0.26062 | val_0_rmse: 0.56667 | val_1_rmse: 0.56791 |  0:00:35s
epoch 19 | loss: 0.25451 | val_0_rmse: 0.54675 | val_1_rmse: 0.55138 |  0:00:37s
epoch 20 | loss: 0.24968 | val_0_rmse: 0.56096 | val_1_rmse: 0.56215 |  0:00:39s
epoch 21 | loss: 0.24777 | val_0_rmse: 0.54422 | val_1_rmse: 0.5492  |  0:00:41s
epoch 22 | loss: 0.24753 | val_0_rmse: 0.51201 | val_1_rmse: 0.51942 |  0:00:43s
epoch 23 | loss: 0.24822 | val_0_rmse: 0.50445 | val_1_rmse: 0.51493 |  0:00:45s
epoch 24 | loss: 0.24699 | val_0_rmse: 0.4905  | val_1_rmse: 0.50724 |  0:00:47s
epoch 25 | loss: 0.24993 | val_0_rmse: 0.48838 | val_1_rmse: 0.50815 |  0:00:49s
epoch 26 | loss: 0.24323 | val_0_rmse: 0.50816 | val_1_rmse: 0.52882 |  0:00:51s
epoch 27 | loss: 0.24526 | val_0_rmse: 0.48723 | val_1_rmse: 0.51228 |  0:00:52s
epoch 28 | loss: 0.24568 | val_0_rmse: 0.47088 | val_1_rmse: 0.49851 |  0:00:54s
epoch 29 | loss: 0.25359 | val_0_rmse: 0.47932 | val_1_rmse: 0.50111 |  0:00:56s
epoch 30 | loss: 0.24537 | val_0_rmse: 0.46305 | val_1_rmse: 0.49223 |  0:00:58s
epoch 31 | loss: 0.23733 | val_0_rmse: 0.5225  | val_1_rmse: 0.55505 |  0:01:00s
epoch 32 | loss: 0.23465 | val_0_rmse: 0.45891 | val_1_rmse: 0.49331 |  0:01:02s
epoch 33 | loss: 0.23377 | val_0_rmse: 0.4879  | val_1_rmse: 0.52234 |  0:01:04s
epoch 34 | loss: 0.2341  | val_0_rmse: 0.45561 | val_1_rmse: 0.49022 |  0:01:06s
epoch 35 | loss: 0.23374 | val_0_rmse: 0.44799 | val_1_rmse: 0.48384 |  0:01:08s
epoch 36 | loss: 0.23024 | val_0_rmse: 0.46027 | val_1_rmse: 0.49914 |  0:01:09s
epoch 37 | loss: 0.22468 | val_0_rmse: 0.44905 | val_1_rmse: 0.4908  |  0:01:11s
epoch 38 | loss: 0.22112 | val_0_rmse: 0.45103 | val_1_rmse: 0.49245 |  0:01:13s
epoch 39 | loss: 0.22023 | val_0_rmse: 0.44564 | val_1_rmse: 0.49383 |  0:01:15s
epoch 40 | loss: 0.22064 | val_0_rmse: 0.44151 | val_1_rmse: 0.48957 |  0:01:17s
epoch 41 | loss: 0.22442 | val_0_rmse: 0.49333 | val_1_rmse: 0.52716 |  0:01:19s
epoch 42 | loss: 0.21797 | val_0_rmse: 0.43501 | val_1_rmse: 0.48802 |  0:01:21s
epoch 43 | loss: 0.2206  | val_0_rmse: 0.4822  | val_1_rmse: 0.53238 |  0:01:23s
epoch 44 | loss: 0.2227  | val_0_rmse: 0.45771 | val_1_rmse: 0.51226 |  0:01:24s
epoch 45 | loss: 0.21495 | val_0_rmse: 0.43917 | val_1_rmse: 0.49662 |  0:01:26s
epoch 46 | loss: 0.21984 | val_0_rmse: 0.44948 | val_1_rmse: 0.50063 |  0:01:28s
epoch 47 | loss: 0.21396 | val_0_rmse: 0.48334 | val_1_rmse: 0.53987 |  0:01:30s
epoch 48 | loss: 0.21424 | val_0_rmse: 0.45416 | val_1_rmse: 0.50394 |  0:01:32s
epoch 49 | loss: 0.21373 | val_0_rmse: 0.44407 | val_1_rmse: 0.49733 |  0:01:34s
epoch 50 | loss: 0.21554 | val_0_rmse: 0.48452 | val_1_rmse: 0.53666 |  0:01:36s
epoch 51 | loss: 0.21428 | val_0_rmse: 0.44191 | val_1_rmse: 0.49803 |  0:01:38s
epoch 52 | loss: 0.21093 | val_0_rmse: 0.4353  | val_1_rmse: 0.499   |  0:01:39s
epoch 53 | loss: 0.21291 | val_0_rmse: 0.47828 | val_1_rmse: 0.53567 |  0:01:41s
epoch 54 | loss: 0.21536 | val_0_rmse: 0.43659 | val_1_rmse: 0.50439 |  0:01:43s
epoch 55 | loss: 0.20935 | val_0_rmse: 0.43155 | val_1_rmse: 0.49802 |  0:01:45s
epoch 56 | loss: 0.20877 | val_0_rmse: 0.43785 | val_1_rmse: 0.49502 |  0:01:47s
epoch 57 | loss: 0.2079  | val_0_rmse: 0.43519 | val_1_rmse: 0.50103 |  0:01:49s
epoch 58 | loss: 0.20477 | val_0_rmse: 0.42868 | val_1_rmse: 0.50125 |  0:01:51s
epoch 59 | loss: 0.20399 | val_0_rmse: 0.43136 | val_1_rmse: 0.50163 |  0:01:53s
epoch 60 | loss: 0.20438 | val_0_rmse: 0.42443 | val_1_rmse: 0.48952 |  0:01:54s
epoch 61 | loss: 0.2061  | val_0_rmse: 0.41893 | val_1_rmse: 0.49501 |  0:01:56s
epoch 62 | loss: 0.20677 | val_0_rmse: 0.52973 | val_1_rmse: 0.59827 |  0:01:58s
epoch 63 | loss: 0.2094  | val_0_rmse: 0.43164 | val_1_rmse: 0.50673 |  0:02:00s
epoch 64 | loss: 0.20511 | val_0_rmse: 0.44997 | val_1_rmse: 0.52022 |  0:02:02s
epoch 65 | loss: 0.20275 | val_0_rmse: 0.42485 | val_1_rmse: 0.49718 |  0:02:04s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.48384
Best weights from best epoch are automatically used!
ended training at: 06:18:44
Feature importance:
Mean squared error is of 5831740844.313578
Mean absolute error:49627.972860280395
MAPE:0.153236299506019
R2 score:0.7446991659003096
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:18:44
epoch 0  | loss: 1.2002  | val_0_rmse: 0.93106 | val_1_rmse: 0.93551 |  0:00:01s
epoch 1  | loss: 0.80018 | val_0_rmse: 0.90248 | val_1_rmse: 0.90629 |  0:00:03s
epoch 2  | loss: 0.72346 | val_0_rmse: 0.8777  | val_1_rmse: 0.88253 |  0:00:05s
epoch 3  | loss: 0.6527  | val_0_rmse: 0.82258 | val_1_rmse: 0.83379 |  0:00:07s
epoch 4  | loss: 0.57455 | val_0_rmse: 0.82522 | val_1_rmse: 0.8276  |  0:00:09s
epoch 5  | loss: 0.52243 | val_0_rmse: 0.8147  | val_1_rmse: 0.81699 |  0:00:11s
epoch 6  | loss: 0.48563 | val_0_rmse: 0.78169 | val_1_rmse: 0.78902 |  0:00:13s
epoch 7  | loss: 0.46518 | val_0_rmse: 0.77227 | val_1_rmse: 0.77986 |  0:00:15s
epoch 8  | loss: 0.45111 | val_0_rmse: 0.75492 | val_1_rmse: 0.75906 |  0:00:16s
epoch 9  | loss: 0.43304 | val_0_rmse: 0.75204 | val_1_rmse: 0.75857 |  0:00:18s
epoch 10 | loss: 0.38843 | val_0_rmse: 0.71597 | val_1_rmse: 0.72377 |  0:00:20s
epoch 11 | loss: 0.36848 | val_0_rmse: 0.67969 | val_1_rmse: 0.68545 |  0:00:22s
epoch 12 | loss: 0.36059 | val_0_rmse: 0.67465 | val_1_rmse: 0.68479 |  0:00:24s
epoch 13 | loss: 0.33483 | val_0_rmse: 0.6571  | val_1_rmse: 0.66376 |  0:00:26s
epoch 14 | loss: 0.32465 | val_0_rmse: 0.61991 | val_1_rmse: 0.62636 |  0:00:28s
epoch 15 | loss: 0.32014 | val_0_rmse: 0.61915 | val_1_rmse: 0.62835 |  0:00:30s
epoch 16 | loss: 0.32118 | val_0_rmse: 0.61837 | val_1_rmse: 0.63483 |  0:00:32s
epoch 17 | loss: 0.30299 | val_0_rmse: 0.59164 | val_1_rmse: 0.60824 |  0:00:34s
epoch 18 | loss: 0.30174 | val_0_rmse: 0.58379 | val_1_rmse: 0.60185 |  0:00:35s
epoch 19 | loss: 0.30163 | val_0_rmse: 0.55869 | val_1_rmse: 0.57322 |  0:00:37s
epoch 20 | loss: 0.29695 | val_0_rmse: 0.55572 | val_1_rmse: 0.56772 |  0:00:39s
epoch 21 | loss: 0.29535 | val_0_rmse: 0.55606 | val_1_rmse: 0.57076 |  0:00:41s
epoch 22 | loss: 0.2896  | val_0_rmse: 0.5285  | val_1_rmse: 0.5421  |  0:00:43s
epoch 23 | loss: 0.28916 | val_0_rmse: 0.53632 | val_1_rmse: 0.55491 |  0:00:45s
epoch 24 | loss: 0.28274 | val_0_rmse: 0.51791 | val_1_rmse: 0.5412  |  0:00:47s
epoch 25 | loss: 0.27388 | val_0_rmse: 0.51029 | val_1_rmse: 0.53404 |  0:00:49s
epoch 26 | loss: 0.27281 | val_0_rmse: 0.50128 | val_1_rmse: 0.52826 |  0:00:51s
epoch 27 | loss: 0.27778 | val_0_rmse: 0.49855 | val_1_rmse: 0.52425 |  0:00:52s
epoch 28 | loss: 0.27334 | val_0_rmse: 0.49655 | val_1_rmse: 0.52441 |  0:00:54s
epoch 29 | loss: 0.26829 | val_0_rmse: 0.49885 | val_1_rmse: 0.53075 |  0:00:56s
epoch 30 | loss: 0.26169 | val_0_rmse: 0.49386 | val_1_rmse: 0.51943 |  0:00:58s
epoch 31 | loss: 0.27213 | val_0_rmse: 0.49792 | val_1_rmse: 0.52546 |  0:01:00s
epoch 32 | loss: 0.26578 | val_0_rmse: 0.49721 | val_1_rmse: 0.51755 |  0:01:02s
epoch 33 | loss: 0.27028 | val_0_rmse: 0.50189 | val_1_rmse: 0.53249 |  0:01:04s
epoch 34 | loss: 0.28243 | val_0_rmse: 0.51544 | val_1_rmse: 0.54214 |  0:01:06s
epoch 35 | loss: 0.28469 | val_0_rmse: 0.49955 | val_1_rmse: 0.53094 |  0:01:08s
epoch 36 | loss: 0.27112 | val_0_rmse: 0.49117 | val_1_rmse: 0.52012 |  0:01:09s
epoch 37 | loss: 0.2655  | val_0_rmse: 0.50704 | val_1_rmse: 0.53781 |  0:01:11s
epoch 38 | loss: 0.28474 | val_0_rmse: 0.50444 | val_1_rmse: 0.54478 |  0:01:13s
epoch 39 | loss: 0.27512 | val_0_rmse: 0.50044 | val_1_rmse: 0.53439 |  0:01:15s
epoch 40 | loss: 0.27054 | val_0_rmse: 0.49994 | val_1_rmse: 0.52653 |  0:01:17s
epoch 41 | loss: 0.26499 | val_0_rmse: 0.4887  | val_1_rmse: 0.51524 |  0:01:19s
epoch 42 | loss: 0.26204 | val_0_rmse: 0.48507 | val_1_rmse: 0.51939 |  0:01:21s
epoch 43 | loss: 0.26057 | val_0_rmse: 0.49001 | val_1_rmse: 0.52531 |  0:01:23s
epoch 44 | loss: 0.26162 | val_0_rmse: 0.48596 | val_1_rmse: 0.5236  |  0:01:25s
epoch 45 | loss: 0.25843 | val_0_rmse: 0.48035 | val_1_rmse: 0.50936 |  0:01:26s
epoch 46 | loss: 0.25358 | val_0_rmse: 0.47936 | val_1_rmse: 0.51376 |  0:01:28s
epoch 47 | loss: 0.25477 | val_0_rmse: 0.48415 | val_1_rmse: 0.51317 |  0:01:30s
epoch 48 | loss: 0.25607 | val_0_rmse: 0.47913 | val_1_rmse: 0.51287 |  0:01:32s
epoch 49 | loss: 0.25281 | val_0_rmse: 0.48507 | val_1_rmse: 0.52466 |  0:01:34s
epoch 50 | loss: 0.25243 | val_0_rmse: 0.48967 | val_1_rmse: 0.52965 |  0:01:36s
epoch 51 | loss: 0.25732 | val_0_rmse: 0.4799  | val_1_rmse: 0.52092 |  0:01:38s
epoch 52 | loss: 0.26115 | val_0_rmse: 0.48979 | val_1_rmse: 0.53263 |  0:01:40s
epoch 53 | loss: 0.25501 | val_0_rmse: 0.48671 | val_1_rmse: 0.52649 |  0:01:42s
epoch 54 | loss: 0.24972 | val_0_rmse: 0.47574 | val_1_rmse: 0.51738 |  0:01:43s
epoch 55 | loss: 0.24337 | val_0_rmse: 0.47237 | val_1_rmse: 0.51261 |  0:01:45s
epoch 56 | loss: 0.24137 | val_0_rmse: 0.46551 | val_1_rmse: 0.51456 |  0:01:47s
epoch 57 | loss: 0.23866 | val_0_rmse: 0.46624 | val_1_rmse: 0.51105 |  0:01:49s
epoch 58 | loss: 0.24113 | val_0_rmse: 0.47019 | val_1_rmse: 0.5202  |  0:01:51s
epoch 59 | loss: 0.23935 | val_0_rmse: 0.47155 | val_1_rmse: 0.51551 |  0:01:53s
epoch 60 | loss: 0.2445  | val_0_rmse: 0.47223 | val_1_rmse: 0.52012 |  0:01:55s
epoch 61 | loss: 0.23964 | val_0_rmse: 0.46801 | val_1_rmse: 0.51966 |  0:01:57s
epoch 62 | loss: 0.23428 | val_0_rmse: 0.46251 | val_1_rmse: 0.51439 |  0:01:58s
epoch 63 | loss: 0.23517 | val_0_rmse: 0.46641 | val_1_rmse: 0.51181 |  0:02:00s
epoch 64 | loss: 0.23721 | val_0_rmse: 0.46635 | val_1_rmse: 0.51425 |  0:02:02s
epoch 65 | loss: 0.24309 | val_0_rmse: 0.46569 | val_1_rmse: 0.51726 |  0:02:04s
epoch 66 | loss: 0.23552 | val_0_rmse: 0.47336 | val_1_rmse: 0.52458 |  0:02:06s
epoch 67 | loss: 0.23723 | val_0_rmse: 0.47044 | val_1_rmse: 0.52275 |  0:02:08s
epoch 68 | loss: 0.23312 | val_0_rmse: 0.46109 | val_1_rmse: 0.51557 |  0:02:10s
epoch 69 | loss: 0.23444 | val_0_rmse: 0.45982 | val_1_rmse: 0.51227 |  0:02:12s
epoch 70 | loss: 0.23816 | val_0_rmse: 0.47245 | val_1_rmse: 0.53041 |  0:02:14s
epoch 71 | loss: 0.23584 | val_0_rmse: 0.4635  | val_1_rmse: 0.51376 |  0:02:16s
epoch 72 | loss: 0.23025 | val_0_rmse: 0.46115 | val_1_rmse: 0.51514 |  0:02:17s
epoch 73 | loss: 0.23989 | val_0_rmse: 0.5393  | val_1_rmse: 0.57977 |  0:02:19s
epoch 74 | loss: 0.25919 | val_0_rmse: 0.50103 | val_1_rmse: 0.54689 |  0:02:21s
epoch 75 | loss: 0.245   | val_0_rmse: 0.47549 | val_1_rmse: 0.52279 |  0:02:23s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.50936
Best weights from best epoch are automatically used!
ended training at: 06:21:09
Feature importance:
Mean squared error is of 5936918413.386222
Mean absolute error:51706.711176833094
MAPE:0.16351263448521317
R2 score:0.7364871026195143
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:23:27
epoch 0  | loss: 0.79232 | val_0_rmse: 0.7526  | val_1_rmse: 0.75212 |  0:00:11s
epoch 1  | loss: 0.44787 | val_0_rmse: 0.73782 | val_1_rmse: 0.72859 |  0:00:22s
epoch 2  | loss: 0.38454 | val_0_rmse: 0.66412 | val_1_rmse: 0.66176 |  0:00:33s
epoch 3  | loss: 0.33748 | val_0_rmse: 0.64027 | val_1_rmse: 0.63723 |  0:00:44s
epoch 4  | loss: 0.31977 | val_0_rmse: 0.60204 | val_1_rmse: 0.60188 |  0:00:55s
epoch 5  | loss: 0.3099  | val_0_rmse: 0.5898  | val_1_rmse: 0.59482 |  0:01:06s
epoch 6  | loss: 0.30742 | val_0_rmse: 0.56035 | val_1_rmse: 0.57176 |  0:01:17s
epoch 7  | loss: 0.30003 | val_0_rmse: 0.54035 | val_1_rmse: 0.5583  |  0:01:28s
epoch 8  | loss: 0.28875 | val_0_rmse: 0.52453 | val_1_rmse: 0.54393 |  0:01:39s
epoch 9  | loss: 0.28415 | val_0_rmse: 0.51838 | val_1_rmse: 0.54451 |  0:01:50s
epoch 10 | loss: 0.27717 | val_0_rmse: 0.51443 | val_1_rmse: 0.54259 |  0:02:01s
epoch 11 | loss: 0.27726 | val_0_rmse: 0.51488 | val_1_rmse: 0.54767 |  0:02:12s
epoch 12 | loss: 0.27283 | val_0_rmse: 0.52805 | val_1_rmse: 0.5636  |  0:02:23s
epoch 13 | loss: 0.27111 | val_0_rmse: 0.5255  | val_1_rmse: 0.56713 |  0:02:34s
epoch 14 | loss: 0.27824 | val_0_rmse: 0.55138 | val_1_rmse: 0.59136 |  0:02:45s
epoch 15 | loss: 0.28081 | val_0_rmse: 0.53666 | val_1_rmse: 0.57447 |  0:02:56s
epoch 16 | loss: 0.27719 | val_0_rmse: 0.58127 | val_1_rmse: 0.62039 |  0:03:07s
epoch 17 | loss: 0.26869 | val_0_rmse: 0.58602 | val_1_rmse: 0.5838  |  0:03:18s
epoch 18 | loss: 0.26365 | val_0_rmse: 0.59127 | val_1_rmse: 0.63684 |  0:03:29s
epoch 19 | loss: 0.26322 | val_0_rmse: 0.53044 | val_1_rmse: 0.57361 |  0:03:40s
epoch 20 | loss: 0.25603 | val_0_rmse: 0.54702 | val_1_rmse: 0.56613 |  0:03:51s
epoch 21 | loss: 0.25421 | val_0_rmse: 0.53193 | val_1_rmse: 0.58213 |  0:04:02s
epoch 22 | loss: 0.25271 | val_0_rmse: 0.53355 | val_1_rmse: 0.57983 |  0:04:13s
epoch 23 | loss: 0.2649  | val_0_rmse: 0.5502  | val_1_rmse: 0.60679 |  0:04:24s
epoch 24 | loss: 0.28026 | val_0_rmse: 0.55094 | val_1_rmse: 0.59285 |  0:04:35s
epoch 25 | loss: 0.26284 | val_0_rmse: 0.56767 | val_1_rmse: 0.61699 |  0:04:46s
epoch 26 | loss: 0.25665 | val_0_rmse: 0.52886 | val_1_rmse: 0.5805  |  0:04:57s
epoch 27 | loss: 0.25075 | val_0_rmse: 0.5412  | val_1_rmse: 0.58334 |  0:05:08s
epoch 28 | loss: 0.25918 | val_0_rmse: 0.54483 | val_1_rmse: 0.69484 |  0:05:19s
epoch 29 | loss: 0.25976 | val_0_rmse: 0.53604 | val_1_rmse: 0.59132 |  0:05:30s
epoch 30 | loss: 0.25059 | val_0_rmse: 0.56825 | val_1_rmse: 0.62414 |  0:05:41s
epoch 31 | loss: 0.25668 | val_0_rmse: 0.54691 | val_1_rmse: 0.60229 |  0:05:52s
epoch 32 | loss: 0.25423 | val_0_rmse: 0.53217 | val_1_rmse: 0.59096 |  0:06:03s
epoch 33 | loss: 0.31237 | val_0_rmse: 0.70575 | val_1_rmse: 0.71506 |  0:06:14s
epoch 34 | loss: 0.63826 | val_0_rmse: 14.58376| val_1_rmse: 15.12388|  0:06:25s
epoch 35 | loss: 0.39923 | val_0_rmse: 1.18164 | val_1_rmse: 1.21228 |  0:06:36s
epoch 36 | loss: 0.52542 | val_0_rmse: 224.52216| val_1_rmse: 224.55091|  0:06:47s
epoch 37 | loss: 0.53958 | val_0_rmse: 16.92822| val_1_rmse: 16.02188|  0:06:58s
epoch 38 | loss: 0.53812 | val_0_rmse: 136.18352| val_1_rmse: 136.08267|  0:07:09s
epoch 39 | loss: 0.5379  | val_0_rmse: 60.76106| val_1_rmse: 60.76206|  0:07:20s
epoch 40 | loss: 0.53717 | val_0_rmse: 261.04236| val_1_rmse: 261.06798|  0:07:31s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.54259
Best weights from best epoch are automatically used!
ended training at: 06:31:04
Feature importance:
Mean squared error is of 1771914773.1742885
Mean absolute error:29892.985031113534
MAPE:0.26692732674210384
R2 score:0.7364949257700031
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:31:07
epoch 0  | loss: 0.79731 | val_0_rmse: 0.80262 | val_1_rmse: 0.78878 |  0:00:11s
epoch 1  | loss: 0.45265 | val_0_rmse: 0.77685 | val_1_rmse: 0.76597 |  0:00:22s
epoch 2  | loss: 0.38624 | val_0_rmse: 0.69067 | val_1_rmse: 0.68206 |  0:00:33s
epoch 3  | loss: 0.35937 | val_0_rmse: 0.68133 | val_1_rmse: 0.674   |  0:00:44s
epoch 4  | loss: 0.3257  | val_0_rmse: 0.62145 | val_1_rmse: 0.61693 |  0:00:55s
epoch 5  | loss: 0.30368 | val_0_rmse: 0.58262 | val_1_rmse: 0.58242 |  0:01:06s
epoch 6  | loss: 0.29192 | val_0_rmse: 0.55093 | val_1_rmse: 0.55838 |  0:01:17s
epoch 7  | loss: 0.2851  | val_0_rmse: 0.69534 | val_1_rmse: 0.69645 |  0:01:28s
epoch 8  | loss: 0.29012 | val_0_rmse: 0.5503  | val_1_rmse: 0.56997 |  0:01:39s
epoch 9  | loss: 0.29134 | val_0_rmse: 0.52193 | val_1_rmse: 0.53786 |  0:01:50s
epoch 10 | loss: 0.29004 | val_0_rmse: 0.54805 | val_1_rmse: 0.57555 |  0:02:01s
epoch 11 | loss: 0.28694 | val_0_rmse: 0.51852 | val_1_rmse: 0.54941 |  0:02:12s
epoch 12 | loss: 0.27695 | val_0_rmse: 0.51671 | val_1_rmse: 0.54757 |  0:02:23s
epoch 13 | loss: 0.27059 | val_0_rmse: 0.52599 | val_1_rmse: 0.55472 |  0:02:34s
epoch 14 | loss: 0.26936 | val_0_rmse: 0.61133 | val_1_rmse: 0.6231  |  0:02:45s
epoch 15 | loss: 0.26291 | val_0_rmse: 0.52999 | val_1_rmse: 0.56948 |  0:02:56s
epoch 16 | loss: 0.26178 | val_0_rmse: 0.51847 | val_1_rmse: 0.55831 |  0:03:07s
epoch 17 | loss: 0.2598  | val_0_rmse: 0.58262 | val_1_rmse: 0.61721 |  0:03:18s
epoch 18 | loss: 0.25999 | val_0_rmse: 0.5269  | val_1_rmse: 0.57059 |  0:03:29s
epoch 19 | loss: 0.25527 | val_0_rmse: 0.52667 | val_1_rmse: 0.57841 |  0:03:40s
epoch 20 | loss: 0.25182 | val_0_rmse: 0.51504 | val_1_rmse: 0.55622 |  0:03:51s
epoch 21 | loss: 0.25027 | val_0_rmse: 0.51637 | val_1_rmse: 0.5597  |  0:04:02s
epoch 22 | loss: 0.24821 | val_0_rmse: 0.51118 | val_1_rmse: 0.5636  |  0:04:13s
epoch 23 | loss: 0.24738 | val_0_rmse: 0.51534 | val_1_rmse: 0.56311 |  0:04:24s
epoch 24 | loss: 0.24645 | val_0_rmse: 0.51096 | val_1_rmse: 0.55714 |  0:04:35s
epoch 25 | loss: 0.24831 | val_0_rmse: 0.59356 | val_1_rmse: 0.55712 |  0:04:47s
epoch 26 | loss: 0.24708 | val_0_rmse: 0.53209 | val_1_rmse: 0.5809  |  0:04:58s
epoch 27 | loss: 0.24371 | val_0_rmse: 0.50935 | val_1_rmse: 0.56984 |  0:05:10s
epoch 28 | loss: 0.24121 | val_0_rmse: 0.50191 | val_1_rmse: 0.55479 |  0:05:21s
epoch 29 | loss: 0.23679 | val_0_rmse: 0.50308 | val_1_rmse: 0.56921 |  0:05:32s
epoch 30 | loss: 0.23948 | val_0_rmse: 0.54401 | val_1_rmse: 0.6159  |  0:05:43s
epoch 31 | loss: 0.25488 | val_0_rmse: 0.51746 | val_1_rmse: 0.57483 |  0:05:55s
epoch 32 | loss: 0.24734 | val_0_rmse: 0.51348 | val_1_rmse: 0.57495 |  0:06:06s
epoch 33 | loss: 0.25412 | val_0_rmse: 0.52156 | val_1_rmse: 0.56232 |  0:06:17s
epoch 34 | loss: 0.25117 | val_0_rmse: 0.76086 | val_1_rmse: 0.82394 |  0:06:29s
epoch 35 | loss: 0.24259 | val_0_rmse: 0.5781  | val_1_rmse: 0.61549 |  0:06:40s
epoch 36 | loss: 0.26419 | val_0_rmse: 0.50732 | val_1_rmse: 0.56035 |  0:06:51s
epoch 37 | loss: 0.25122 | val_0_rmse: 0.52018 | val_1_rmse: 0.57185 |  0:07:02s
epoch 38 | loss: 0.29469 | val_0_rmse: 1.25415 | val_1_rmse: 1.10613 |  0:07:14s
epoch 39 | loss: 0.29064 | val_0_rmse: 0.51685 | val_1_rmse: 0.56843 |  0:07:25s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.53786
Best weights from best epoch are automatically used!
ended training at: 06:38:39
Feature importance:
Mean squared error is of 1869219209.7680562
Mean absolute error:30350.38211983471
MAPE:0.2739496248415846
R2 score:0.7265549048453503
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:38:41
epoch 0  | loss: 0.79364 | val_0_rmse: 0.79395 | val_1_rmse: 0.80193 |  0:00:11s
epoch 1  | loss: 0.43385 | val_0_rmse: 0.72275 | val_1_rmse: 0.73248 |  0:00:22s
epoch 2  | loss: 0.35404 | val_0_rmse: 0.67157 | val_1_rmse: 0.68024 |  0:00:33s
epoch 3  | loss: 0.31903 | val_0_rmse: 0.64443 | val_1_rmse: 0.65308 |  0:00:44s
epoch 4  | loss: 0.30269 | val_0_rmse: 0.62326 | val_1_rmse: 0.63644 |  0:00:55s
epoch 5  | loss: 0.28772 | val_0_rmse: 0.5847  | val_1_rmse: 0.5998  |  0:01:06s
epoch 6  | loss: 0.28215 | val_0_rmse: 0.54857 | val_1_rmse: 0.57064 |  0:01:17s
epoch 7  | loss: 0.27604 | val_0_rmse: 0.53555 | val_1_rmse: 0.56174 |  0:01:28s
epoch 8  | loss: 0.27062 | val_0_rmse: 0.51829 | val_1_rmse: 0.55635 |  0:01:39s
epoch 9  | loss: 0.2708  | val_0_rmse: 0.49941 | val_1_rmse: 0.54145 |  0:01:50s
epoch 10 | loss: 0.26514 | val_0_rmse: 0.49985 | val_1_rmse: 0.54672 |  0:02:01s
epoch 11 | loss: 0.26024 | val_0_rmse: 0.49798 | val_1_rmse: 0.56883 |  0:02:12s
epoch 12 | loss: 0.26442 | val_0_rmse: 0.51516 | val_1_rmse: 0.56741 |  0:02:23s
epoch 13 | loss: 0.25928 | val_0_rmse: 0.5014  | val_1_rmse: 0.55936 |  0:02:34s
epoch 14 | loss: 0.25626 | val_0_rmse: 0.50367 | val_1_rmse: 0.60789 |  0:02:45s
epoch 15 | loss: 0.25202 | val_0_rmse: 0.50801 | val_1_rmse: 0.66951 |  0:02:56s
epoch 16 | loss: 0.25106 | val_0_rmse: 0.50839 | val_1_rmse: 0.58304 |  0:03:07s
epoch 17 | loss: 0.24972 | val_0_rmse: 0.55079 | val_1_rmse: 0.59492 |  0:03:18s
epoch 18 | loss: 0.24972 | val_0_rmse: 0.50932 | val_1_rmse: 0.57205 |  0:03:29s
epoch 19 | loss: 0.24977 | val_0_rmse: 0.53022 | val_1_rmse: 0.60889 |  0:03:40s
epoch 20 | loss: 0.25092 | val_0_rmse: 0.52008 | val_1_rmse: 0.60534 |  0:03:51s
epoch 21 | loss: 0.24573 | val_0_rmse: 0.52261 | val_1_rmse: 0.85386 |  0:04:02s
epoch 22 | loss: 0.24169 | val_0_rmse: 0.49767 | val_1_rmse: 0.93214 |  0:04:13s
epoch 23 | loss: 0.23843 | val_0_rmse: 0.50749 | val_1_rmse: 0.62122 |  0:04:24s
epoch 24 | loss: 0.23661 | val_0_rmse: 0.49518 | val_1_rmse: 0.81077 |  0:04:35s
epoch 25 | loss: 0.23384 | val_0_rmse: 0.49464 | val_1_rmse: 0.72068 |  0:04:47s
epoch 26 | loss: 0.23244 | val_0_rmse: 0.49165 | val_1_rmse: 0.59381 |  0:04:58s
epoch 27 | loss: 0.2301  | val_0_rmse: 0.50484 | val_1_rmse: 0.67011 |  0:05:09s
epoch 28 | loss: 0.23101 | val_0_rmse: 0.48578 | val_1_rmse: 0.55668 |  0:05:20s
epoch 29 | loss: 0.2282  | val_0_rmse: 0.48915 | val_1_rmse: 0.56286 |  0:05:31s
epoch 30 | loss: 0.22794 | val_0_rmse: 0.49357 | val_1_rmse: 0.56036 |  0:05:42s
epoch 31 | loss: 0.22578 | val_0_rmse: 0.48706 | val_1_rmse: 0.56103 |  0:05:53s
epoch 32 | loss: 0.22328 | val_0_rmse: 0.48441 | val_1_rmse: 0.56343 |  0:06:04s
epoch 33 | loss: 0.22239 | val_0_rmse: 0.47615 | val_1_rmse: 0.5546  |  0:06:15s
epoch 34 | loss: 0.22004 | val_0_rmse: 0.48054 | val_1_rmse: 0.5597  |  0:06:26s
epoch 35 | loss: 0.2182  | val_0_rmse: 0.48074 | val_1_rmse: 0.56153 |  0:06:37s
epoch 36 | loss: 0.21757 | val_0_rmse: 0.47558 | val_1_rmse: 0.55572 |  0:06:48s
epoch 37 | loss: 0.21624 | val_0_rmse: 0.51575 | val_1_rmse: 0.59678 |  0:06:59s
epoch 38 | loss: 0.21508 | val_0_rmse: 0.49706 | val_1_rmse: 0.57799 |  0:07:10s
epoch 39 | loss: 0.21448 | val_0_rmse: 0.48374 | val_1_rmse: 0.56645 |  0:07:21s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.54145
Best weights from best epoch are automatically used!
ended training at: 06:46:09
Feature importance:
Mean squared error is of 1761047791.847664
Mean absolute error:29579.84613503086
MAPE:0.2588036823416879
R2 score:0.7390386622048056
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:46:12
epoch 0  | loss: 0.85058 | val_0_rmse: 0.81659 | val_1_rmse: 0.81607 |  0:00:11s
epoch 1  | loss: 0.40202 | val_0_rmse: 0.72813 | val_1_rmse: 0.72565 |  0:00:22s
epoch 2  | loss: 0.32718 | val_0_rmse: 0.68566 | val_1_rmse: 0.68706 |  0:00:33s
epoch 3  | loss: 0.30501 | val_0_rmse: 0.65144 | val_1_rmse: 0.65447 |  0:00:44s
epoch 4  | loss: 0.29382 | val_0_rmse: 0.62578 | val_1_rmse: 0.63397 |  0:00:55s
epoch 5  | loss: 0.28322 | val_0_rmse: 0.57105 | val_1_rmse: 0.58645 |  0:01:06s
epoch 6  | loss: 0.27794 | val_0_rmse: 0.54937 | val_1_rmse: 0.56877 |  0:01:17s
epoch 7  | loss: 0.27374 | val_0_rmse: 0.5153  | val_1_rmse: 0.54709 |  0:01:28s
epoch 8  | loss: 0.26558 | val_0_rmse: 0.5181  | val_1_rmse: 0.55541 |  0:01:39s
epoch 9  | loss: 0.26587 | val_0_rmse: 0.49739 | val_1_rmse: 0.54046 |  0:01:50s
epoch 10 | loss: 0.26199 | val_0_rmse: 0.55701 | val_1_rmse: 0.59842 |  0:02:01s
epoch 11 | loss: 0.26194 | val_0_rmse: 0.49562 | val_1_rmse: 0.55041 |  0:02:12s
epoch 12 | loss: 0.25809 | val_0_rmse: 0.51121 | val_1_rmse: 0.55943 |  0:02:23s
epoch 13 | loss: 0.25155 | val_0_rmse: 0.53052 | val_1_rmse: 0.5871  |  0:02:34s
epoch 14 | loss: 0.24933 | val_0_rmse: 0.51363 | val_1_rmse: 0.57525 |  0:02:45s
epoch 15 | loss: 0.24911 | val_0_rmse: 0.53957 | val_1_rmse: 0.58719 |  0:02:56s
epoch 16 | loss: 0.24789 | val_0_rmse: 0.51453 | val_1_rmse: 0.56989 |  0:03:07s
epoch 17 | loss: 0.24469 | val_0_rmse: 0.51686 | val_1_rmse: 0.57709 |  0:03:18s
epoch 18 | loss: 0.24204 | val_0_rmse: 0.5209  | val_1_rmse: 0.57696 |  0:03:29s
epoch 19 | loss: 0.23983 | val_0_rmse: 0.50994 | val_1_rmse: 0.58257 |  0:03:40s
epoch 20 | loss: 0.23826 | val_0_rmse: 0.61157 | val_1_rmse: 0.63859 |  0:03:51s
epoch 21 | loss: 0.23596 | val_0_rmse: 0.51304 | val_1_rmse: 0.58466 |  0:04:03s
epoch 22 | loss: 0.23269 | val_0_rmse: 0.52106 | val_1_rmse: 0.59597 |  0:04:14s
epoch 23 | loss: 0.23108 | val_0_rmse: 0.60213 | val_1_rmse: 0.63555 |  0:04:25s
epoch 24 | loss: 0.23147 | val_0_rmse: 0.51893 | val_1_rmse: 0.66717 |  0:04:36s
epoch 25 | loss: 0.22959 | val_0_rmse: 0.62632 | val_1_rmse: 1.23912 |  0:04:47s
epoch 26 | loss: 0.22879 | val_0_rmse: 0.54171 | val_1_rmse: 0.88763 |  0:04:58s
epoch 27 | loss: 0.22944 | val_0_rmse: 0.58617 | val_1_rmse: 0.63578 |  0:05:10s
epoch 28 | loss: 0.23824 | val_0_rmse: 0.49921 | val_1_rmse: 0.58608 |  0:05:21s
epoch 29 | loss: 0.22603 | val_0_rmse: 0.49181 | val_1_rmse: 0.58019 |  0:05:32s
epoch 30 | loss: 0.2211  | val_0_rmse: 0.48746 | val_1_rmse: 0.57635 |  0:05:43s
epoch 31 | loss: 0.21848 | val_0_rmse: 0.47968 | val_1_rmse: 0.56956 |  0:05:55s
epoch 32 | loss: 0.21639 | val_0_rmse: 0.47276 | val_1_rmse: 0.56831 |  0:06:06s
epoch 33 | loss: 0.21575 | val_0_rmse: 0.62389 | val_1_rmse: 0.56875 |  0:06:17s
epoch 34 | loss: 0.2161  | val_0_rmse: 0.49909 | val_1_rmse: 0.56772 |  0:06:28s
epoch 35 | loss: 0.21344 | val_0_rmse: 0.4778  | val_1_rmse: 0.57565 |  0:06:40s
epoch 36 | loss: 0.21393 | val_0_rmse: 0.47633 | val_1_rmse: 0.57763 |  0:06:51s
epoch 37 | loss: 0.2105  | val_0_rmse: 0.49617 | val_1_rmse: 0.57421 |  0:07:02s
epoch 38 | loss: 0.21029 | val_0_rmse: 0.48735 | val_1_rmse: 0.58406 |  0:07:13s
epoch 39 | loss: 0.20996 | val_0_rmse: 0.4685  | val_1_rmse: 0.56974 |  0:07:25s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.54046
Best weights from best epoch are automatically used!
ended training at: 06:53:43
Feature importance:
Mean squared error is of 1872383419.006709
Mean absolute error:30164.448460693733
MAPE:0.26105210293512343
R2 score:0.7237569275227598
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:53:45
epoch 0  | loss: 0.85231 | val_0_rmse: 0.81332 | val_1_rmse: 0.8304  |  0:00:11s
epoch 1  | loss: 0.43594 | val_0_rmse: 0.70614 | val_1_rmse: 0.72043 |  0:00:22s
epoch 2  | loss: 0.35282 | val_0_rmse: 0.68875 | val_1_rmse: 0.70193 |  0:00:33s
epoch 3  | loss: 0.32009 | val_0_rmse: 0.64419 | val_1_rmse: 0.65894 |  0:00:45s
epoch 4  | loss: 0.29793 | val_0_rmse: 0.61277 | val_1_rmse: 0.62559 |  0:00:56s
epoch 5  | loss: 0.28816 | val_0_rmse: 0.5731  | val_1_rmse: 0.58979 |  0:01:07s
epoch 6  | loss: 0.28223 | val_0_rmse: 0.5446  | val_1_rmse: 0.56575 |  0:01:19s
epoch 7  | loss: 0.27752 | val_0_rmse: 0.52604 | val_1_rmse: 0.55787 |  0:01:30s
epoch 8  | loss: 0.27513 | val_0_rmse: 0.50709 | val_1_rmse: 0.54264 |  0:01:41s
epoch 9  | loss: 0.26687 | val_0_rmse: 0.49318 | val_1_rmse: 0.53434 |  0:01:53s
epoch 10 | loss: 0.26496 | val_0_rmse: 0.50845 | val_1_rmse: 0.54972 |  0:02:04s
epoch 11 | loss: 0.26456 | val_0_rmse: 0.50448 | val_1_rmse: 0.55211 |  0:02:15s
epoch 12 | loss: 0.2581  | val_0_rmse: 0.50568 | val_1_rmse: 0.55622 |  0:02:26s
epoch 13 | loss: 0.26131 | val_0_rmse: 0.51865 | val_1_rmse: 0.5706  |  0:02:38s
epoch 14 | loss: 0.25504 | val_0_rmse: 0.5313  | val_1_rmse: 0.58936 |  0:02:49s
epoch 15 | loss: 0.25539 | val_0_rmse: 0.90467 | val_1_rmse: 1.0291  |  0:03:00s
epoch 16 | loss: 0.25441 | val_0_rmse: 0.50659 | val_1_rmse: 0.5703  |  0:03:12s
epoch 17 | loss: 0.24839 | val_0_rmse: 0.5337  | val_1_rmse: 0.58621 |  0:03:23s
epoch 18 | loss: 0.24898 | val_0_rmse: 0.55684 | val_1_rmse: 0.59964 |  0:03:34s
epoch 19 | loss: 0.24451 | val_0_rmse: 0.51602 | val_1_rmse: 0.57939 |  0:03:46s
epoch 20 | loss: 0.24517 | val_0_rmse: 0.52307 | val_1_rmse: 0.58597 |  0:03:57s
epoch 21 | loss: 0.24494 | val_0_rmse: 0.56791 | val_1_rmse: 0.78319 |  0:04:08s
epoch 22 | loss: 0.2437  | val_0_rmse: 0.50573 | val_1_rmse: 0.57343 |  0:04:19s
epoch 23 | loss: 0.24362 | val_0_rmse: 0.50969 | val_1_rmse: 0.64443 |  0:04:31s
epoch 24 | loss: 0.24994 | val_0_rmse: 0.53253 | val_1_rmse: 0.59618 |  0:04:42s
epoch 25 | loss: 0.23919 | val_0_rmse: 0.52174 | val_1_rmse: 0.58882 |  0:04:53s
epoch 26 | loss: 0.23333 | val_0_rmse: 0.50501 | val_1_rmse: 0.57971 |  0:05:04s
epoch 27 | loss: 0.23653 | val_0_rmse: 0.49414 | val_1_rmse: 0.56728 |  0:05:16s
epoch 28 | loss: 0.23162 | val_0_rmse: 0.48818 | val_1_rmse: 0.56279 |  0:05:27s
epoch 29 | loss: 0.22629 | val_0_rmse: 0.49237 | val_1_rmse: 0.5806  |  0:05:38s
epoch 30 | loss: 0.224   | val_0_rmse: 0.49008 | val_1_rmse: 0.56747 |  0:05:50s
epoch 31 | loss: 0.2207  | val_0_rmse: 0.48933 | val_1_rmse: 0.57096 |  0:06:01s
epoch 32 | loss: 0.22117 | val_0_rmse: 0.48723 | val_1_rmse: 0.56906 |  0:06:12s
epoch 33 | loss: 0.21921 | val_0_rmse: 0.48386 | val_1_rmse: 0.56985 |  0:06:24s
epoch 34 | loss: 0.21792 | val_0_rmse: 1.40041 | val_1_rmse: 1.61619 |  0:06:35s
epoch 35 | loss: 0.21849 | val_0_rmse: 0.49237 | val_1_rmse: 0.5784  |  0:06:46s
epoch 36 | loss: 0.21794 | val_0_rmse: 0.478   | val_1_rmse: 0.56547 |  0:06:57s
epoch 37 | loss: 0.21575 | val_0_rmse: 0.59701 | val_1_rmse: 0.65918 |  0:07:09s
epoch 38 | loss: 0.2158  | val_0_rmse: 0.478   | val_1_rmse: 0.56444 |  0:07:20s
epoch 39 | loss: 0.21502 | val_0_rmse: 0.50876 | val_1_rmse: 0.61598 |  0:07:31s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.53434
Best weights from best epoch are automatically used!
ended training at: 07:01:24
Feature importance:
Mean squared error is of 1796696206.8893723
Mean absolute error:30033.77331560354
MAPE:0.26010569237453335
R2 score:0.7298471741297253
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:01:30
epoch 0  | loss: 1.5461  | val_0_rmse: 0.95254 | val_1_rmse: 0.94667 |  0:00:01s
epoch 1  | loss: 0.62645 | val_0_rmse: 0.88482 | val_1_rmse: 0.87725 |  0:00:03s
epoch 2  | loss: 0.33921 | val_0_rmse: 0.59743 | val_1_rmse: 0.59534 |  0:00:04s
epoch 3  | loss: 0.26531 | val_0_rmse: 0.61461 | val_1_rmse: 0.60909 |  0:00:06s
epoch 4  | loss: 0.23416 | val_0_rmse: 0.55359 | val_1_rmse: 0.54722 |  0:00:08s
epoch 5  | loss: 0.2203  | val_0_rmse: 0.53711 | val_1_rmse: 0.53351 |  0:00:09s
epoch 6  | loss: 0.21874 | val_0_rmse: 0.57862 | val_1_rmse: 0.57115 |  0:00:11s
epoch 7  | loss: 0.20918 | val_0_rmse: 0.5333  | val_1_rmse: 0.52981 |  0:00:12s
epoch 8  | loss: 0.20131 | val_0_rmse: 0.53594 | val_1_rmse: 0.52783 |  0:00:14s
epoch 9  | loss: 0.20875 | val_0_rmse: 0.50897 | val_1_rmse: 0.50527 |  0:00:16s
epoch 10 | loss: 0.19891 | val_0_rmse: 0.5003  | val_1_rmse: 0.4982  |  0:00:17s
epoch 11 | loss: 0.19316 | val_0_rmse: 0.52153 | val_1_rmse: 0.51747 |  0:00:19s
epoch 12 | loss: 0.19365 | val_0_rmse: 0.47977 | val_1_rmse: 0.47757 |  0:00:21s
epoch 13 | loss: 0.19043 | val_0_rmse: 0.48905 | val_1_rmse: 0.48756 |  0:00:22s
epoch 14 | loss: 0.19053 | val_0_rmse: 0.47146 | val_1_rmse: 0.46887 |  0:00:24s
epoch 15 | loss: 0.1895  | val_0_rmse: 0.45956 | val_1_rmse: 0.46149 |  0:00:25s
epoch 16 | loss: 0.18133 | val_0_rmse: 0.45492 | val_1_rmse: 0.45568 |  0:00:27s
epoch 17 | loss: 0.18361 | val_0_rmse: 0.44294 | val_1_rmse: 0.44596 |  0:00:29s
epoch 18 | loss: 0.18691 | val_0_rmse: 0.45301 | val_1_rmse: 0.45181 |  0:00:30s
epoch 19 | loss: 0.17859 | val_0_rmse: 0.4445  | val_1_rmse: 0.44585 |  0:00:32s
epoch 20 | loss: 0.18474 | val_0_rmse: 0.43282 | val_1_rmse: 0.43643 |  0:00:33s
epoch 21 | loss: 0.18407 | val_0_rmse: 0.43932 | val_1_rmse: 0.4409  |  0:00:35s
epoch 22 | loss: 0.1825  | val_0_rmse: 0.4324  | val_1_rmse: 0.43749 |  0:00:37s
epoch 23 | loss: 0.18642 | val_0_rmse: 0.44837 | val_1_rmse: 0.45144 |  0:00:38s
epoch 24 | loss: 0.18291 | val_0_rmse: 0.42833 | val_1_rmse: 0.43336 |  0:00:40s
epoch 25 | loss: 0.1787  | val_0_rmse: 0.43559 | val_1_rmse: 0.43901 |  0:00:42s
epoch 26 | loss: 0.17433 | val_0_rmse: 0.42455 | val_1_rmse: 0.43204 |  0:00:43s
epoch 27 | loss: 0.17257 | val_0_rmse: 0.40635 | val_1_rmse: 0.41636 |  0:00:45s
epoch 28 | loss: 0.1705  | val_0_rmse: 0.41262 | val_1_rmse: 0.42058 |  0:00:46s
epoch 29 | loss: 0.16874 | val_0_rmse: 0.40042 | val_1_rmse: 0.41171 |  0:00:48s
epoch 30 | loss: 0.17018 | val_0_rmse: 0.43251 | val_1_rmse: 0.4394  |  0:00:50s
epoch 31 | loss: 0.18027 | val_0_rmse: 0.41488 | val_1_rmse: 0.42651 |  0:00:51s
epoch 32 | loss: 0.17472 | val_0_rmse: 0.4029  | val_1_rmse: 0.4133  |  0:00:53s
epoch 33 | loss: 0.17028 | val_0_rmse: 0.41911 | val_1_rmse: 0.43281 |  0:00:54s
epoch 34 | loss: 0.17315 | val_0_rmse: 0.39697 | val_1_rmse: 0.41267 |  0:00:56s
epoch 35 | loss: 0.17261 | val_0_rmse: 0.41087 | val_1_rmse: 0.41755 |  0:00:58s
epoch 36 | loss: 0.17082 | val_0_rmse: 0.4003  | val_1_rmse: 0.41202 |  0:00:59s
epoch 37 | loss: 0.17248 | val_0_rmse: 0.39846 | val_1_rmse: 0.41011 |  0:01:01s
epoch 38 | loss: 0.16986 | val_0_rmse: 0.39319 | val_1_rmse: 0.40902 |  0:01:02s
epoch 39 | loss: 0.16588 | val_0_rmse: 0.39022 | val_1_rmse: 0.40693 |  0:01:04s
epoch 40 | loss: 0.16188 | val_0_rmse: 0.39398 | val_1_rmse: 0.40908 |  0:01:06s
epoch 41 | loss: 0.18365 | val_0_rmse: 0.43926 | val_1_rmse: 0.44725 |  0:01:07s
epoch 42 | loss: 0.18214 | val_0_rmse: 0.39795 | val_1_rmse: 0.40985 |  0:01:09s
epoch 43 | loss: 0.17087 | val_0_rmse: 0.39625 | val_1_rmse: 0.41495 |  0:01:11s
epoch 44 | loss: 0.16549 | val_0_rmse: 0.40464 | val_1_rmse: 0.42168 |  0:01:12s
epoch 45 | loss: 0.16803 | val_0_rmse: 0.40538 | val_1_rmse: 0.41724 |  0:01:14s
epoch 46 | loss: 0.16698 | val_0_rmse: 0.41671 | val_1_rmse: 0.43353 |  0:01:15s
epoch 47 | loss: 0.16762 | val_0_rmse: 0.38981 | val_1_rmse: 0.40487 |  0:01:17s
epoch 48 | loss: 0.16339 | val_0_rmse: 0.40591 | val_1_rmse: 0.41962 |  0:01:19s
epoch 49 | loss: 0.16257 | val_0_rmse: 0.39523 | val_1_rmse: 0.41104 |  0:01:20s
epoch 50 | loss: 0.15969 | val_0_rmse: 0.38841 | val_1_rmse: 0.40831 |  0:01:22s
epoch 51 | loss: 0.15864 | val_0_rmse: 0.39464 | val_1_rmse: 0.41147 |  0:01:23s
epoch 52 | loss: 0.16289 | val_0_rmse: 0.38538 | val_1_rmse: 0.40449 |  0:01:25s
epoch 53 | loss: 0.16092 | val_0_rmse: 0.39862 | val_1_rmse: 0.41541 |  0:01:27s
epoch 54 | loss: 0.16247 | val_0_rmse: 0.38673 | val_1_rmse: 0.40182 |  0:01:28s
epoch 55 | loss: 0.15925 | val_0_rmse: 0.39042 | val_1_rmse: 0.40798 |  0:01:30s
epoch 56 | loss: 0.15942 | val_0_rmse: 0.39725 | val_1_rmse: 0.41688 |  0:01:32s
epoch 57 | loss: 0.16645 | val_0_rmse: 0.39794 | val_1_rmse: 0.41362 |  0:01:33s
epoch 58 | loss: 0.16259 | val_0_rmse: 0.39147 | val_1_rmse: 0.40758 |  0:01:35s
epoch 59 | loss: 0.16401 | val_0_rmse: 0.40966 | val_1_rmse: 0.42333 |  0:01:36s
epoch 60 | loss: 0.16731 | val_0_rmse: 0.40116 | val_1_rmse: 0.42224 |  0:01:38s
epoch 61 | loss: 0.16941 | val_0_rmse: 0.40393 | val_1_rmse: 0.42113 |  0:01:40s
epoch 62 | loss: 0.17085 | val_0_rmse: 0.39981 | val_1_rmse: 0.41654 |  0:01:41s
epoch 63 | loss: 0.16659 | val_0_rmse: 0.4134  | val_1_rmse: 0.43096 |  0:01:43s
epoch 64 | loss: 0.16813 | val_0_rmse: 0.39338 | val_1_rmse: 0.41046 |  0:01:44s
epoch 65 | loss: 0.16008 | val_0_rmse: 0.39916 | val_1_rmse: 0.41877 |  0:01:46s
epoch 66 | loss: 0.17071 | val_0_rmse: 0.42202 | val_1_rmse: 0.43702 |  0:01:48s
epoch 67 | loss: 0.16892 | val_0_rmse: 0.40658 | val_1_rmse: 0.42827 |  0:01:49s
epoch 68 | loss: 0.1747  | val_0_rmse: 0.4203  | val_1_rmse: 0.42883 |  0:01:51s
epoch 69 | loss: 0.17568 | val_0_rmse: 0.39946 | val_1_rmse: 0.41641 |  0:01:53s
epoch 70 | loss: 0.16891 | val_0_rmse: 0.41423 | val_1_rmse: 0.43013 |  0:01:54s
epoch 71 | loss: 0.16707 | val_0_rmse: 0.39325 | val_1_rmse: 0.40948 |  0:01:56s
epoch 72 | loss: 0.16357 | val_0_rmse: 0.39945 | val_1_rmse: 0.41843 |  0:01:57s
epoch 73 | loss: 0.16198 | val_0_rmse: 0.38902 | val_1_rmse: 0.41328 |  0:01:59s
epoch 74 | loss: 0.16246 | val_0_rmse: 0.39468 | val_1_rmse: 0.4076  |  0:02:01s
epoch 75 | loss: 0.1613  | val_0_rmse: 0.41232 | val_1_rmse: 0.42677 |  0:02:02s
epoch 76 | loss: 0.16957 | val_0_rmse: 0.40792 | val_1_rmse: 0.42318 |  0:02:04s
epoch 77 | loss: 0.17224 | val_0_rmse: 0.4001  | val_1_rmse: 0.42115 |  0:02:05s
epoch 78 | loss: 0.16538 | val_0_rmse: 0.40071 | val_1_rmse: 0.41779 |  0:02:07s
epoch 79 | loss: 0.15924 | val_0_rmse: 0.39155 | val_1_rmse: 0.41246 |  0:02:09s
epoch 80 | loss: 0.15573 | val_0_rmse: 0.38923 | val_1_rmse: 0.41137 |  0:02:10s
epoch 81 | loss: 0.16115 | val_0_rmse: 0.40717 | val_1_rmse: 0.42948 |  0:02:12s
epoch 82 | loss: 0.16688 | val_0_rmse: 0.39025 | val_1_rmse: 0.41321 |  0:02:14s
epoch 83 | loss: 0.1572  | val_0_rmse: 0.39897 | val_1_rmse: 0.42659 |  0:02:15s
epoch 84 | loss: 0.16037 | val_0_rmse: 0.41019 | val_1_rmse: 0.43222 |  0:02:17s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.40182
Best weights from best epoch are automatically used!
ended training at: 07:03:48
Feature importance:
Mean squared error is of 917407191.9978976
Mean absolute error:19754.960547757222
MAPE:0.20556657350696733
R2 score:0.792454855154359
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:03:49
epoch 0  | loss: 1.56954 | val_0_rmse: 0.92372 | val_1_rmse: 0.9131  |  0:00:01s
epoch 1  | loss: 0.62308 | val_0_rmse: 0.83542 | val_1_rmse: 0.82167 |  0:00:03s
epoch 2  | loss: 0.34819 | val_0_rmse: 0.85085 | val_1_rmse: 0.84378 |  0:00:04s
epoch 3  | loss: 0.2741  | val_0_rmse: 0.63937 | val_1_rmse: 0.63267 |  0:00:06s
epoch 4  | loss: 0.24631 | val_0_rmse: 0.59899 | val_1_rmse: 0.60325 |  0:00:08s
epoch 5  | loss: 0.25338 | val_0_rmse: 0.60714 | val_1_rmse: 0.61093 |  0:00:09s
epoch 6  | loss: 0.23864 | val_0_rmse: 0.57812 | val_1_rmse: 0.58341 |  0:00:11s
epoch 7  | loss: 0.22609 | val_0_rmse: 0.53375 | val_1_rmse: 0.53344 |  0:00:12s
epoch 8  | loss: 0.21419 | val_0_rmse: 0.50493 | val_1_rmse: 0.50661 |  0:00:14s
epoch 9  | loss: 0.20625 | val_0_rmse: 0.51718 | val_1_rmse: 0.51505 |  0:00:16s
epoch 10 | loss: 0.20527 | val_0_rmse: 0.51843 | val_1_rmse: 0.5203  |  0:00:17s
epoch 11 | loss: 0.20427 | val_0_rmse: 0.49216 | val_1_rmse: 0.49286 |  0:00:19s
epoch 12 | loss: 0.19647 | val_0_rmse: 0.49616 | val_1_rmse: 0.49921 |  0:00:21s
epoch 13 | loss: 0.1887  | val_0_rmse: 0.48126 | val_1_rmse: 0.4803  |  0:00:22s
epoch 14 | loss: 0.20335 | val_0_rmse: 0.49652 | val_1_rmse: 0.49198 |  0:00:24s
epoch 15 | loss: 0.23629 | val_0_rmse: 0.491   | val_1_rmse: 0.48844 |  0:00:25s
epoch 16 | loss: 0.21451 | val_0_rmse: 0.47095 | val_1_rmse: 0.46734 |  0:00:27s
epoch 17 | loss: 0.21437 | val_0_rmse: 0.46877 | val_1_rmse: 0.46898 |  0:00:29s
epoch 18 | loss: 0.20407 | val_0_rmse: 0.45803 | val_1_rmse: 0.46241 |  0:00:30s
epoch 19 | loss: 0.19584 | val_0_rmse: 0.44758 | val_1_rmse: 0.44869 |  0:00:32s
epoch 20 | loss: 0.1927  | val_0_rmse: 0.45329 | val_1_rmse: 0.45703 |  0:00:33s
epoch 21 | loss: 0.18613 | val_0_rmse: 0.43671 | val_1_rmse: 0.44104 |  0:00:35s
epoch 22 | loss: 0.1855  | val_0_rmse: 0.45216 | val_1_rmse: 0.45738 |  0:00:37s
epoch 23 | loss: 0.18878 | val_0_rmse: 0.43337 | val_1_rmse: 0.43899 |  0:00:38s
epoch 24 | loss: 0.18337 | val_0_rmse: 0.42795 | val_1_rmse: 0.43421 |  0:00:40s
epoch 25 | loss: 0.17952 | val_0_rmse: 0.41992 | val_1_rmse: 0.42605 |  0:00:42s
epoch 26 | loss: 0.17764 | val_0_rmse: 0.41956 | val_1_rmse: 0.42406 |  0:00:43s
epoch 27 | loss: 0.17393 | val_0_rmse: 0.41597 | val_1_rmse: 0.42199 |  0:00:45s
epoch 28 | loss: 0.17164 | val_0_rmse: 0.40012 | val_1_rmse: 0.40948 |  0:00:46s
epoch 29 | loss: 0.16993 | val_0_rmse: 0.40188 | val_1_rmse: 0.41154 |  0:00:48s
epoch 30 | loss: 0.17033 | val_0_rmse: 0.40516 | val_1_rmse: 0.41517 |  0:00:50s
epoch 31 | loss: 0.17003 | val_0_rmse: 0.39681 | val_1_rmse: 0.41032 |  0:00:51s
epoch 32 | loss: 0.16526 | val_0_rmse: 0.39645 | val_1_rmse: 0.41146 |  0:00:53s
epoch 33 | loss: 0.17034 | val_0_rmse: 0.39033 | val_1_rmse: 0.40661 |  0:00:54s
epoch 34 | loss: 0.16399 | val_0_rmse: 0.39469 | val_1_rmse: 0.40907 |  0:00:56s
epoch 35 | loss: 0.16677 | val_0_rmse: 0.38496 | val_1_rmse: 0.40574 |  0:00:58s
epoch 36 | loss: 0.16149 | val_0_rmse: 0.38433 | val_1_rmse: 0.40199 |  0:00:59s
epoch 37 | loss: 0.16133 | val_0_rmse: 0.38711 | val_1_rmse: 0.40655 |  0:01:01s
epoch 38 | loss: 0.16268 | val_0_rmse: 0.39935 | val_1_rmse: 0.4177  |  0:01:03s
epoch 39 | loss: 0.16362 | val_0_rmse: 0.38389 | val_1_rmse: 0.4048  |  0:01:04s
epoch 40 | loss: 0.15762 | val_0_rmse: 0.37943 | val_1_rmse: 0.40423 |  0:01:06s
epoch 41 | loss: 0.15686 | val_0_rmse: 0.37916 | val_1_rmse: 0.40316 |  0:01:07s
epoch 42 | loss: 0.15522 | val_0_rmse: 0.37825 | val_1_rmse: 0.40348 |  0:01:09s
epoch 43 | loss: 0.1533  | val_0_rmse: 0.38242 | val_1_rmse: 0.4112  |  0:01:11s
epoch 44 | loss: 0.15948 | val_0_rmse: 0.37781 | val_1_rmse: 0.40574 |  0:01:12s
epoch 45 | loss: 0.15508 | val_0_rmse: 0.38388 | val_1_rmse: 0.40971 |  0:01:14s
epoch 46 | loss: 0.15515 | val_0_rmse: 0.38435 | val_1_rmse: 0.4171  |  0:01:15s
epoch 47 | loss: 0.15232 | val_0_rmse: 0.38043 | val_1_rmse: 0.41035 |  0:01:17s
epoch 48 | loss: 0.15608 | val_0_rmse: 0.37959 | val_1_rmse: 0.40607 |  0:01:19s
epoch 49 | loss: 0.1528  | val_0_rmse: 0.38092 | val_1_rmse: 0.4076  |  0:01:20s
epoch 50 | loss: 0.1522  | val_0_rmse: 0.37679 | val_1_rmse: 0.40731 |  0:01:22s
epoch 51 | loss: 0.15236 | val_0_rmse: 0.37221 | val_1_rmse: 0.40252 |  0:01:23s
epoch 52 | loss: 0.15319 | val_0_rmse: 0.3745  | val_1_rmse: 0.40469 |  0:01:25s
epoch 53 | loss: 0.15395 | val_0_rmse: 0.37868 | val_1_rmse: 0.40659 |  0:01:27s
epoch 54 | loss: 0.15211 | val_0_rmse: 0.38994 | val_1_rmse: 0.41415 |  0:01:28s
epoch 55 | loss: 0.1504  | val_0_rmse: 0.38095 | val_1_rmse: 0.4123  |  0:01:30s
epoch 56 | loss: 0.15032 | val_0_rmse: 0.38185 | val_1_rmse: 0.4083  |  0:01:32s
epoch 57 | loss: 0.14937 | val_0_rmse: 0.37418 | val_1_rmse: 0.40627 |  0:01:33s
epoch 58 | loss: 0.14853 | val_0_rmse: 0.38513 | val_1_rmse: 0.40827 |  0:01:35s
epoch 59 | loss: 0.14838 | val_0_rmse: 0.37406 | val_1_rmse: 0.40371 |  0:01:36s
epoch 60 | loss: 0.14713 | val_0_rmse: 0.37653 | val_1_rmse: 0.40732 |  0:01:38s
epoch 61 | loss: 0.14857 | val_0_rmse: 0.37381 | val_1_rmse: 0.40732 |  0:01:40s
epoch 62 | loss: 0.1511  | val_0_rmse: 0.40256 | val_1_rmse: 0.43089 |  0:01:41s
epoch 63 | loss: 0.15061 | val_0_rmse: 0.37684 | val_1_rmse: 0.40507 |  0:01:43s
epoch 64 | loss: 0.14604 | val_0_rmse: 0.37945 | val_1_rmse: 0.41195 |  0:01:44s
epoch 65 | loss: 0.14567 | val_0_rmse: 0.38017 | val_1_rmse: 0.41323 |  0:01:46s
epoch 66 | loss: 0.1481  | val_0_rmse: 0.37687 | val_1_rmse: 0.41025 |  0:01:48s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.40199
Best weights from best epoch are automatically used!
ended training at: 07:05:37
Feature importance:
Mean squared error is of 943662489.2699059
Mean absolute error:20122.10569761962
MAPE:0.20984822340771608
R2 score:0.7945912925825815
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:05:38
epoch 0  | loss: 1.47077 | val_0_rmse: 0.83722 | val_1_rmse: 0.82374 |  0:00:01s
epoch 1  | loss: 0.43544 | val_0_rmse: 0.7471  | val_1_rmse: 0.74761 |  0:00:03s
epoch 2  | loss: 0.32358 | val_0_rmse: 0.76616 | val_1_rmse: 0.76539 |  0:00:04s
epoch 3  | loss: 0.30175 | val_0_rmse: 0.59819 | val_1_rmse: 0.58113 |  0:00:06s
epoch 4  | loss: 0.27034 | val_0_rmse: 0.55319 | val_1_rmse: 0.5434  |  0:00:08s
epoch 5  | loss: 0.25906 | val_0_rmse: 0.55318 | val_1_rmse: 0.54451 |  0:00:09s
epoch 6  | loss: 0.25098 | val_0_rmse: 0.5162  | val_1_rmse: 0.51175 |  0:00:11s
epoch 7  | loss: 0.24433 | val_0_rmse: 0.52044 | val_1_rmse: 0.51524 |  0:00:12s
epoch 8  | loss: 0.23755 | val_0_rmse: 0.50885 | val_1_rmse: 0.50369 |  0:00:14s
epoch 9  | loss: 0.22116 | val_0_rmse: 0.49341 | val_1_rmse: 0.48713 |  0:00:16s
epoch 10 | loss: 0.20803 | val_0_rmse: 0.4844  | val_1_rmse: 0.48026 |  0:00:17s
epoch 11 | loss: 0.20717 | val_0_rmse: 0.47253 | val_1_rmse: 0.46985 |  0:00:19s
epoch 12 | loss: 0.20643 | val_0_rmse: 0.4743  | val_1_rmse: 0.4669  |  0:00:21s
epoch 13 | loss: 0.20654 | val_0_rmse: 0.48012 | val_1_rmse: 0.47683 |  0:00:22s
epoch 14 | loss: 0.20428 | val_0_rmse: 0.46977 | val_1_rmse: 0.46746 |  0:00:24s
epoch 15 | loss: 0.20505 | val_0_rmse: 0.45931 | val_1_rmse: 0.46178 |  0:00:25s
epoch 16 | loss: 0.2064  | val_0_rmse: 0.4712  | val_1_rmse: 0.47449 |  0:00:27s
epoch 17 | loss: 0.19554 | val_0_rmse: 0.47    | val_1_rmse: 0.47096 |  0:00:29s
epoch 18 | loss: 0.19549 | val_0_rmse: 0.44904 | val_1_rmse: 0.45005 |  0:00:30s
epoch 19 | loss: 0.19542 | val_0_rmse: 0.4617  | val_1_rmse: 0.45953 |  0:00:32s
epoch 20 | loss: 0.19381 | val_0_rmse: 0.44754 | val_1_rmse: 0.44489 |  0:00:34s
epoch 21 | loss: 0.18681 | val_0_rmse: 0.45909 | val_1_rmse: 0.45918 |  0:00:35s
epoch 22 | loss: 0.19073 | val_0_rmse: 0.44623 | val_1_rmse: 0.44527 |  0:00:37s
epoch 23 | loss: 0.19084 | val_0_rmse: 0.44604 | val_1_rmse: 0.44831 |  0:00:38s
epoch 24 | loss: 0.18792 | val_0_rmse: 0.44104 | val_1_rmse: 0.44074 |  0:00:40s
epoch 25 | loss: 0.18383 | val_0_rmse: 0.4278  | val_1_rmse: 0.42818 |  0:00:42s
epoch 26 | loss: 0.1801  | val_0_rmse: 0.41818 | val_1_rmse: 0.42218 |  0:00:43s
epoch 27 | loss: 0.1812  | val_0_rmse: 0.41918 | val_1_rmse: 0.42228 |  0:00:45s
epoch 28 | loss: 0.18275 | val_0_rmse: 0.42298 | val_1_rmse: 0.42698 |  0:00:47s
epoch 29 | loss: 0.18215 | val_0_rmse: 0.41421 | val_1_rmse: 0.41943 |  0:00:48s
epoch 30 | loss: 0.1846  | val_0_rmse: 0.4231  | val_1_rmse: 0.43076 |  0:00:50s
epoch 31 | loss: 0.18832 | val_0_rmse: 0.43132 | val_1_rmse: 0.43617 |  0:00:51s
epoch 32 | loss: 0.17936 | val_0_rmse: 0.41162 | val_1_rmse: 0.41929 |  0:00:53s
epoch 33 | loss: 0.18566 | val_0_rmse: 0.41309 | val_1_rmse: 0.42141 |  0:00:55s
epoch 34 | loss: 0.17953 | val_0_rmse: 0.41973 | val_1_rmse: 0.42494 |  0:00:56s
epoch 35 | loss: 0.18236 | val_0_rmse: 0.41703 | val_1_rmse: 0.42069 |  0:00:58s
epoch 36 | loss: 0.17895 | val_0_rmse: 0.41509 | val_1_rmse: 0.42261 |  0:00:59s
epoch 37 | loss: 0.178   | val_0_rmse: 0.40624 | val_1_rmse: 0.4148  |  0:01:01s
epoch 38 | loss: 0.17459 | val_0_rmse: 0.40834 | val_1_rmse: 0.41871 |  0:01:03s
epoch 39 | loss: 0.17279 | val_0_rmse: 0.405   | val_1_rmse: 0.40983 |  0:01:04s
epoch 40 | loss: 0.17179 | val_0_rmse: 0.40189 | val_1_rmse: 0.41106 |  0:01:06s
epoch 41 | loss: 0.1742  | val_0_rmse: 0.41107 | val_1_rmse: 0.41575 |  0:01:07s
epoch 42 | loss: 0.1725  | val_0_rmse: 0.40343 | val_1_rmse: 0.41161 |  0:01:09s
epoch 43 | loss: 0.17163 | val_0_rmse: 0.40813 | val_1_rmse: 0.41464 |  0:01:11s
epoch 44 | loss: 0.1741  | val_0_rmse: 0.40763 | val_1_rmse: 0.41613 |  0:01:12s
epoch 45 | loss: 0.1725  | val_0_rmse: 0.40638 | val_1_rmse: 0.4126  |  0:01:14s
epoch 46 | loss: 0.17366 | val_0_rmse: 0.40038 | val_1_rmse: 0.41005 |  0:01:15s
epoch 47 | loss: 0.16647 | val_0_rmse: 0.39988 | val_1_rmse: 0.41076 |  0:01:17s
epoch 48 | loss: 0.16865 | val_0_rmse: 0.39947 | val_1_rmse: 0.40978 |  0:01:19s
epoch 49 | loss: 0.17014 | val_0_rmse: 0.40094 | val_1_rmse: 0.4135  |  0:01:20s
epoch 50 | loss: 0.16667 | val_0_rmse: 0.40261 | val_1_rmse: 0.41829 |  0:01:22s
epoch 51 | loss: 0.17043 | val_0_rmse: 0.40206 | val_1_rmse: 0.41461 |  0:01:24s
epoch 52 | loss: 0.1738  | val_0_rmse: 0.40564 | val_1_rmse: 0.42019 |  0:01:25s
epoch 53 | loss: 0.17291 | val_0_rmse: 0.40516 | val_1_rmse: 0.42188 |  0:01:27s
epoch 54 | loss: 0.17045 | val_0_rmse: 0.4012  | val_1_rmse: 0.41614 |  0:01:28s
epoch 55 | loss: 0.16933 | val_0_rmse: 0.39671 | val_1_rmse: 0.40942 |  0:01:30s
epoch 56 | loss: 0.16501 | val_0_rmse: 0.40587 | val_1_rmse: 0.42403 |  0:01:32s
epoch 57 | loss: 0.16933 | val_0_rmse: 0.43713 | val_1_rmse: 0.4524  |  0:01:33s
epoch 58 | loss: 0.18273 | val_0_rmse: 0.42392 | val_1_rmse: 0.43929 |  0:01:35s
epoch 59 | loss: 0.17667 | val_0_rmse: 0.41108 | val_1_rmse: 0.42971 |  0:01:36s
epoch 60 | loss: 0.17157 | val_0_rmse: 0.39889 | val_1_rmse: 0.40843 |  0:01:38s
epoch 61 | loss: 0.1688  | val_0_rmse: 0.40338 | val_1_rmse: 0.41177 |  0:01:40s
epoch 62 | loss: 0.17002 | val_0_rmse: 0.40345 | val_1_rmse: 0.41334 |  0:01:41s
epoch 63 | loss: 0.16989 | val_0_rmse: 0.39839 | val_1_rmse: 0.41074 |  0:01:43s
epoch 64 | loss: 0.16597 | val_0_rmse: 0.40205 | val_1_rmse: 0.41781 |  0:01:44s
epoch 65 | loss: 0.17275 | val_0_rmse: 0.4053  | val_1_rmse: 0.41879 |  0:01:46s
epoch 66 | loss: 0.18357 | val_0_rmse: 0.40899 | val_1_rmse: 0.4184  |  0:01:48s
epoch 67 | loss: 0.17606 | val_0_rmse: 0.40596 | val_1_rmse: 0.41418 |  0:01:49s
epoch 68 | loss: 0.17178 | val_0_rmse: 0.40928 | val_1_rmse: 0.4175  |  0:01:51s
epoch 69 | loss: 0.17135 | val_0_rmse: 0.40867 | val_1_rmse: 0.42172 |  0:01:53s
epoch 70 | loss: 0.17126 | val_0_rmse: 0.40082 | val_1_rmse: 0.41289 |  0:01:54s
epoch 71 | loss: 0.16881 | val_0_rmse: 0.39476 | val_1_rmse: 0.40801 |  0:01:56s
epoch 72 | loss: 0.1637  | val_0_rmse: 0.39512 | val_1_rmse: 0.40615 |  0:01:57s
epoch 73 | loss: 0.16541 | val_0_rmse: 0.39306 | val_1_rmse: 0.40364 |  0:01:59s
epoch 74 | loss: 0.16287 | val_0_rmse: 0.39564 | val_1_rmse: 0.40833 |  0:02:01s
epoch 75 | loss: 0.16154 | val_0_rmse: 0.39375 | val_1_rmse: 0.40731 |  0:02:02s
epoch 76 | loss: 0.16371 | val_0_rmse: 0.4063  | val_1_rmse: 0.42641 |  0:02:04s
epoch 77 | loss: 0.16423 | val_0_rmse: 0.39473 | val_1_rmse: 0.41215 |  0:02:05s
epoch 78 | loss: 0.16148 | val_0_rmse: 0.39288 | val_1_rmse: 0.41405 |  0:02:07s
epoch 79 | loss: 0.16117 | val_0_rmse: 0.38914 | val_1_rmse: 0.41042 |  0:02:09s
epoch 80 | loss: 0.16027 | val_0_rmse: 0.39325 | val_1_rmse: 0.41396 |  0:02:10s
epoch 81 | loss: 0.16062 | val_0_rmse: 0.39339 | val_1_rmse: 0.41199 |  0:02:12s
epoch 82 | loss: 0.15931 | val_0_rmse: 0.38764 | val_1_rmse: 0.40406 |  0:02:14s
epoch 83 | loss: 0.15818 | val_0_rmse: 0.38985 | val_1_rmse: 0.40662 |  0:02:15s
epoch 84 | loss: 0.16027 | val_0_rmse: 0.38964 | val_1_rmse: 0.40711 |  0:02:17s
epoch 85 | loss: 0.1588  | val_0_rmse: 0.38855 | val_1_rmse: 0.40507 |  0:02:18s
epoch 86 | loss: 0.15715 | val_0_rmse: 0.38832 | val_1_rmse: 0.41078 |  0:02:20s
epoch 87 | loss: 0.16132 | val_0_rmse: 0.39852 | val_1_rmse: 0.42115 |  0:02:22s
epoch 88 | loss: 0.17251 | val_0_rmse: 0.44013 | val_1_rmse: 0.45989 |  0:02:23s
epoch 89 | loss: 0.17576 | val_0_rmse: 0.40423 | val_1_rmse: 0.42041 |  0:02:25s
epoch 90 | loss: 0.1646  | val_0_rmse: 0.39445 | val_1_rmse: 0.41665 |  0:02:26s
epoch 91 | loss: 0.16219 | val_0_rmse: 0.39009 | val_1_rmse: 0.41225 |  0:02:28s
epoch 92 | loss: 0.16404 | val_0_rmse: 0.40072 | val_1_rmse: 0.42419 |  0:02:30s
epoch 93 | loss: 0.16562 | val_0_rmse: 0.40364 | val_1_rmse: 0.4243  |  0:02:31s
epoch 94 | loss: 0.16209 | val_0_rmse: 0.39108 | val_1_rmse: 0.41463 |  0:02:33s
epoch 95 | loss: 0.1616  | val_0_rmse: 0.38891 | val_1_rmse: 0.41492 |  0:02:34s
epoch 96 | loss: 0.15835 | val_0_rmse: 0.3963  | val_1_rmse: 0.4227  |  0:02:36s
epoch 97 | loss: 0.15775 | val_0_rmse: 0.38437 | val_1_rmse: 0.40971 |  0:02:38s
epoch 98 | loss: 0.15523 | val_0_rmse: 0.38206 | val_1_rmse: 0.40706 |  0:02:39s
epoch 99 | loss: 0.15938 | val_0_rmse: 0.38604 | val_1_rmse: 0.41265 |  0:02:41s
epoch 100| loss: 0.15771 | val_0_rmse: 0.3904  | val_1_rmse: 0.41622 |  0:02:43s
epoch 101| loss: 0.15705 | val_0_rmse: 0.38592 | val_1_rmse: 0.40854 |  0:02:44s
epoch 102| loss: 0.15567 | val_0_rmse: 0.38346 | val_1_rmse: 0.411   |  0:02:46s
epoch 103| loss: 0.15089 | val_0_rmse: 0.38429 | val_1_rmse: 0.41239 |  0:02:47s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.40364
Best weights from best epoch are automatically used!
ended training at: 07:08:26
Feature importance:
Mean squared error is of 911046616.1268414
Mean absolute error:19991.883170698515
MAPE:0.21590550526071464
R2 score:0.8021448501280961
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:08:26
epoch 0  | loss: 1.42522 | val_0_rmse: 0.86836 | val_1_rmse: 0.87836 |  0:00:01s
epoch 1  | loss: 0.51267 | val_0_rmse: 0.76127 | val_1_rmse: 0.76913 |  0:00:03s
epoch 2  | loss: 0.37149 | val_0_rmse: 0.6949  | val_1_rmse: 0.69773 |  0:00:04s
epoch 3  | loss: 0.32382 | val_0_rmse: 0.64454 | val_1_rmse: 0.64352 |  0:00:06s
epoch 4  | loss: 0.30192 | val_0_rmse: 0.64292 | val_1_rmse: 0.63382 |  0:00:08s
epoch 5  | loss: 0.26773 | val_0_rmse: 0.57731 | val_1_rmse: 0.57239 |  0:00:09s
epoch 6  | loss: 0.25138 | val_0_rmse: 0.57326 | val_1_rmse: 0.56309 |  0:00:11s
epoch 7  | loss: 0.23438 | val_0_rmse: 0.53968 | val_1_rmse: 0.52968 |  0:00:12s
epoch 8  | loss: 0.22697 | val_0_rmse: 0.53294 | val_1_rmse: 0.52053 |  0:00:14s
epoch 9  | loss: 0.21536 | val_0_rmse: 0.55094 | val_1_rmse: 0.54537 |  0:00:16s
epoch 10 | loss: 0.2089  | val_0_rmse: 0.4988  | val_1_rmse: 0.48653 |  0:00:17s
epoch 11 | loss: 0.20199 | val_0_rmse: 0.51774 | val_1_rmse: 0.51199 |  0:00:19s
epoch 12 | loss: 0.19688 | val_0_rmse: 0.50063 | val_1_rmse: 0.49173 |  0:00:20s
epoch 13 | loss: 0.19435 | val_0_rmse: 0.49683 | val_1_rmse: 0.48652 |  0:00:22s
epoch 14 | loss: 0.18697 | val_0_rmse: 0.48169 | val_1_rmse: 0.4698  |  0:00:24s
epoch 15 | loss: 0.1913  | val_0_rmse: 0.47699 | val_1_rmse: 0.46789 |  0:00:25s
epoch 16 | loss: 0.18255 | val_0_rmse: 0.45722 | val_1_rmse: 0.4498  |  0:00:27s
epoch 17 | loss: 0.17786 | val_0_rmse: 0.45099 | val_1_rmse: 0.44243 |  0:00:29s
epoch 18 | loss: 0.17522 | val_0_rmse: 0.45331 | val_1_rmse: 0.44563 |  0:00:30s
epoch 19 | loss: 0.17401 | val_0_rmse: 0.44441 | val_1_rmse: 0.43762 |  0:00:32s
epoch 20 | loss: 0.16782 | val_0_rmse: 0.44246 | val_1_rmse: 0.43608 |  0:00:33s
epoch 21 | loss: 0.17191 | val_0_rmse: 0.43607 | val_1_rmse: 0.43058 |  0:00:35s
epoch 22 | loss: 0.16754 | val_0_rmse: 0.43312 | val_1_rmse: 0.4293  |  0:00:37s
epoch 23 | loss: 0.16335 | val_0_rmse: 0.42723 | val_1_rmse: 0.42436 |  0:00:38s
epoch 24 | loss: 0.16706 | val_0_rmse: 0.40867 | val_1_rmse: 0.41002 |  0:00:40s
epoch 25 | loss: 0.1624  | val_0_rmse: 0.41354 | val_1_rmse: 0.41378 |  0:00:41s
epoch 26 | loss: 0.16526 | val_0_rmse: 0.41115 | val_1_rmse: 0.41511 |  0:00:43s
epoch 27 | loss: 0.16438 | val_0_rmse: 0.39942 | val_1_rmse: 0.40256 |  0:00:45s
epoch 28 | loss: 0.16142 | val_0_rmse: 0.39667 | val_1_rmse: 0.40114 |  0:00:46s
epoch 29 | loss: 0.15794 | val_0_rmse: 0.39428 | val_1_rmse: 0.3989  |  0:00:48s
epoch 30 | loss: 0.15912 | val_0_rmse: 0.39582 | val_1_rmse: 0.40424 |  0:00:49s
epoch 31 | loss: 0.161   | val_0_rmse: 0.39085 | val_1_rmse: 0.39893 |  0:00:51s
epoch 32 | loss: 0.1581  | val_0_rmse: 0.38226 | val_1_rmse: 0.39536 |  0:00:53s
epoch 33 | loss: 0.15738 | val_0_rmse: 0.38408 | val_1_rmse: 0.39598 |  0:00:54s
epoch 34 | loss: 0.156   | val_0_rmse: 0.38449 | val_1_rmse: 0.40144 |  0:00:56s
epoch 35 | loss: 0.15902 | val_0_rmse: 0.37765 | val_1_rmse: 0.39424 |  0:00:58s
epoch 36 | loss: 0.15611 | val_0_rmse: 0.38671 | val_1_rmse: 0.40197 |  0:00:59s
epoch 37 | loss: 0.15665 | val_0_rmse: 0.37851 | val_1_rmse: 0.40092 |  0:01:01s
epoch 38 | loss: 0.15339 | val_0_rmse: 0.37394 | val_1_rmse: 0.3985  |  0:01:02s
epoch 39 | loss: 0.15123 | val_0_rmse: 0.39313 | val_1_rmse: 0.41234 |  0:01:04s
epoch 40 | loss: 0.15353 | val_0_rmse: 0.37252 | val_1_rmse: 0.40058 |  0:01:06s
epoch 41 | loss: 0.155   | val_0_rmse: 0.37916 | val_1_rmse: 0.41208 |  0:01:07s
epoch 42 | loss: 0.15248 | val_0_rmse: 0.39166 | val_1_rmse: 0.4199  |  0:01:09s
epoch 43 | loss: 0.15454 | val_0_rmse: 0.37566 | val_1_rmse: 0.40181 |  0:01:10s
epoch 44 | loss: 0.1483  | val_0_rmse: 0.37247 | val_1_rmse: 0.40273 |  0:01:12s
epoch 45 | loss: 0.14973 | val_0_rmse: 0.38508 | val_1_rmse: 0.41667 |  0:01:14s
epoch 46 | loss: 0.15254 | val_0_rmse: 0.3783  | val_1_rmse: 0.40835 |  0:01:15s
epoch 47 | loss: 0.14712 | val_0_rmse: 0.37368 | val_1_rmse: 0.4047  |  0:01:17s
epoch 48 | loss: 0.14584 | val_0_rmse: 0.39251 | val_1_rmse: 0.42954 |  0:01:18s
epoch 49 | loss: 0.14644 | val_0_rmse: 0.37249 | val_1_rmse: 0.4066  |  0:01:20s
epoch 50 | loss: 0.14811 | val_0_rmse: 0.36614 | val_1_rmse: 0.40056 |  0:01:22s
epoch 51 | loss: 0.14835 | val_0_rmse: 0.39761 | val_1_rmse: 0.43133 |  0:01:23s
epoch 52 | loss: 0.14976 | val_0_rmse: 0.39408 | val_1_rmse: 0.41526 |  0:01:25s
epoch 53 | loss: 0.14851 | val_0_rmse: 0.37604 | val_1_rmse: 0.40893 |  0:01:27s
epoch 54 | loss: 0.14858 | val_0_rmse: 0.37396 | val_1_rmse: 0.411   |  0:01:28s
epoch 55 | loss: 0.1465  | val_0_rmse: 0.36898 | val_1_rmse: 0.40858 |  0:01:30s
epoch 56 | loss: 0.14849 | val_0_rmse: 0.36473 | val_1_rmse: 0.40719 |  0:01:31s
epoch 57 | loss: 0.14484 | val_0_rmse: 0.36847 | val_1_rmse: 0.41043 |  0:01:33s
epoch 58 | loss: 0.1492  | val_0_rmse: 0.36841 | val_1_rmse: 0.41067 |  0:01:35s
epoch 59 | loss: 0.14547 | val_0_rmse: 0.36785 | val_1_rmse: 0.4084  |  0:01:36s
epoch 60 | loss: 0.14573 | val_0_rmse: 0.52532 | val_1_rmse: 0.40886 |  0:01:38s
epoch 61 | loss: 0.14815 | val_0_rmse: 0.47907 | val_1_rmse: 0.44716 |  0:01:39s
epoch 62 | loss: 0.14786 | val_0_rmse: 0.37664 | val_1_rmse: 0.41241 |  0:01:41s
epoch 63 | loss: 0.14472 | val_0_rmse: 0.36305 | val_1_rmse: 0.40593 |  0:01:43s
epoch 64 | loss: 0.14232 | val_0_rmse: 0.36881 | val_1_rmse: 0.4155  |  0:01:44s
epoch 65 | loss: 0.14251 | val_0_rmse: 0.36843 | val_1_rmse: 0.4164  |  0:01:46s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.39424
Best weights from best epoch are automatically used!
ended training at: 07:10:13
Feature importance:
Mean squared error is of 954720853.438288
Mean absolute error:20348.69598110853
MAPE:0.21234205360087685
R2 score:0.7942567095147316
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:10:14
epoch 0  | loss: 1.43691 | val_0_rmse: 0.8385  | val_1_rmse: 0.84148 |  0:00:01s
epoch 1  | loss: 0.5087  | val_0_rmse: 0.66323 | val_1_rmse: 0.66016 |  0:00:03s
epoch 2  | loss: 0.39516 | val_0_rmse: 0.58557 | val_1_rmse: 0.5886  |  0:00:04s
epoch 3  | loss: 0.32314 | val_0_rmse: 0.6013  | val_1_rmse: 0.59985 |  0:00:06s
epoch 4  | loss: 0.27275 | val_0_rmse: 0.5748  | val_1_rmse: 0.57627 |  0:00:08s
epoch 5  | loss: 0.24609 | val_0_rmse: 0.56069 | val_1_rmse: 0.56591 |  0:00:09s
epoch 6  | loss: 0.23244 | val_0_rmse: 0.51934 | val_1_rmse: 0.52118 |  0:00:11s
epoch 7  | loss: 0.22462 | val_0_rmse: 0.50075 | val_1_rmse: 0.5077  |  0:00:12s
epoch 8  | loss: 0.21252 | val_0_rmse: 0.48523 | val_1_rmse: 0.48918 |  0:00:14s
epoch 9  | loss: 0.20955 | val_0_rmse: 0.48993 | val_1_rmse: 0.49508 |  0:00:16s
epoch 10 | loss: 0.19484 | val_0_rmse: 0.47984 | val_1_rmse: 0.484   |  0:00:17s
epoch 11 | loss: 0.19383 | val_0_rmse: 0.48    | val_1_rmse: 0.48668 |  0:00:19s
epoch 12 | loss: 0.20555 | val_0_rmse: 0.4767  | val_1_rmse: 0.48382 |  0:00:21s
epoch 13 | loss: 0.19349 | val_0_rmse: 0.48463 | val_1_rmse: 0.49237 |  0:00:22s
epoch 14 | loss: 0.1841  | val_0_rmse: 0.45574 | val_1_rmse: 0.46097 |  0:00:24s
epoch 15 | loss: 0.17646 | val_0_rmse: 0.44461 | val_1_rmse: 0.45158 |  0:00:25s
epoch 16 | loss: 0.17365 | val_0_rmse: 0.45366 | val_1_rmse: 0.4621  |  0:00:27s
epoch 17 | loss: 0.17312 | val_0_rmse: 0.45874 | val_1_rmse: 0.46969 |  0:00:29s
epoch 18 | loss: 0.17616 | val_0_rmse: 0.43753 | val_1_rmse: 0.44698 |  0:00:30s
epoch 19 | loss: 0.16876 | val_0_rmse: 0.44607 | val_1_rmse: 0.45943 |  0:00:32s
epoch 20 | loss: 0.16764 | val_0_rmse: 0.43364 | val_1_rmse: 0.44447 |  0:00:33s
epoch 21 | loss: 0.16801 | val_0_rmse: 0.43014 | val_1_rmse: 0.44497 |  0:00:35s
epoch 22 | loss: 0.17142 | val_0_rmse: 0.4102  | val_1_rmse: 0.42816 |  0:00:37s
epoch 23 | loss: 0.16343 | val_0_rmse: 0.41635 | val_1_rmse: 0.43208 |  0:00:38s
epoch 24 | loss: 0.16112 | val_0_rmse: 0.40757 | val_1_rmse: 0.42988 |  0:00:40s
epoch 25 | loss: 0.16106 | val_0_rmse: 0.40042 | val_1_rmse: 0.41983 |  0:00:42s
epoch 26 | loss: 0.16536 | val_0_rmse: 0.40228 | val_1_rmse: 0.42341 |  0:00:43s
epoch 27 | loss: 0.16341 | val_0_rmse: 0.39635 | val_1_rmse: 0.41778 |  0:00:45s
epoch 28 | loss: 0.15728 | val_0_rmse: 0.39667 | val_1_rmse: 0.41959 |  0:00:46s
epoch 29 | loss: 0.16039 | val_0_rmse: 0.40315 | val_1_rmse: 0.42545 |  0:00:48s
epoch 30 | loss: 0.16129 | val_0_rmse: 0.39133 | val_1_rmse: 0.41873 |  0:00:50s
epoch 31 | loss: 0.15852 | val_0_rmse: 0.38819 | val_1_rmse: 0.41796 |  0:00:51s
epoch 32 | loss: 0.15633 | val_0_rmse: 0.38382 | val_1_rmse: 0.41602 |  0:00:53s
epoch 33 | loss: 0.15622 | val_0_rmse: 0.38673 | val_1_rmse: 0.41649 |  0:00:54s
epoch 34 | loss: 0.15967 | val_0_rmse: 0.38709 | val_1_rmse: 0.42189 |  0:00:56s
epoch 35 | loss: 0.16008 | val_0_rmse: 0.37916 | val_1_rmse: 0.41324 |  0:00:58s
epoch 36 | loss: 0.15414 | val_0_rmse: 0.37745 | val_1_rmse: 0.4117  |  0:00:59s
epoch 37 | loss: 0.15561 | val_0_rmse: 0.37845 | val_1_rmse: 0.41222 |  0:01:01s
epoch 38 | loss: 0.15091 | val_0_rmse: 0.39205 | val_1_rmse: 0.42811 |  0:01:03s
epoch 39 | loss: 0.15602 | val_0_rmse: 0.37393 | val_1_rmse: 0.4107  |  0:01:04s
epoch 40 | loss: 0.15146 | val_0_rmse: 0.38215 | val_1_rmse: 0.42047 |  0:01:06s
epoch 41 | loss: 0.15301 | val_0_rmse: 0.37583 | val_1_rmse: 0.41594 |  0:01:07s
epoch 42 | loss: 0.15373 | val_0_rmse: 0.39848 | val_1_rmse: 0.43736 |  0:01:09s
epoch 43 | loss: 0.15864 | val_0_rmse: 0.3814  | val_1_rmse: 0.41689 |  0:01:11s
epoch 44 | loss: 0.15681 | val_0_rmse: 0.37808 | val_1_rmse: 0.41316 |  0:01:12s
epoch 45 | loss: 0.1577  | val_0_rmse: 0.3864  | val_1_rmse: 0.42791 |  0:01:14s
epoch 46 | loss: 0.15766 | val_0_rmse: 0.38242 | val_1_rmse: 0.42073 |  0:01:15s
epoch 47 | loss: 0.15734 | val_0_rmse: 0.40506 | val_1_rmse: 0.44101 |  0:01:17s
epoch 48 | loss: 0.15572 | val_0_rmse: 0.39019 | val_1_rmse: 0.4273  |  0:01:19s
epoch 49 | loss: 0.15423 | val_0_rmse: 0.37506 | val_1_rmse: 0.41627 |  0:01:20s
epoch 50 | loss: 0.1519  | val_0_rmse: 0.38299 | val_1_rmse: 0.41817 |  0:01:22s
epoch 51 | loss: 0.1526  | val_0_rmse: 0.39468 | val_1_rmse: 0.43174 |  0:01:23s
epoch 52 | loss: 0.15188 | val_0_rmse: 0.38522 | val_1_rmse: 0.42176 |  0:01:25s
epoch 53 | loss: 0.15111 | val_0_rmse: 0.37575 | val_1_rmse: 0.41975 |  0:01:27s
epoch 54 | loss: 0.14919 | val_0_rmse: 0.38741 | val_1_rmse: 0.43089 |  0:01:28s
epoch 55 | loss: 0.15004 | val_0_rmse: 0.37295 | val_1_rmse: 0.41794 |  0:01:30s
epoch 56 | loss: 0.15103 | val_0_rmse: 0.37658 | val_1_rmse: 0.42529 |  0:01:31s
epoch 57 | loss: 0.14933 | val_0_rmse: 0.37453 | val_1_rmse: 0.42201 |  0:01:33s
epoch 58 | loss: 0.15308 | val_0_rmse: 0.38073 | val_1_rmse: 0.42592 |  0:01:35s
epoch 59 | loss: 0.15721 | val_0_rmse: 0.40109 | val_1_rmse: 0.44358 |  0:01:36s
epoch 60 | loss: 0.15524 | val_0_rmse: 0.37698 | val_1_rmse: 0.41754 |  0:01:38s
epoch 61 | loss: 0.15042 | val_0_rmse: 0.38573 | val_1_rmse: 0.42518 |  0:01:40s
epoch 62 | loss: 0.14872 | val_0_rmse: 0.38323 | val_1_rmse: 0.41968 |  0:01:41s
epoch 63 | loss: 0.15166 | val_0_rmse: 0.38157 | val_1_rmse: 0.42139 |  0:01:43s
epoch 64 | loss: 0.1474  | val_0_rmse: 0.37172 | val_1_rmse: 0.41454 |  0:01:44s
epoch 65 | loss: 0.14817 | val_0_rmse: 0.38809 | val_1_rmse: 0.42987 |  0:01:46s
epoch 66 | loss: 0.14927 | val_0_rmse: 0.373   | val_1_rmse: 0.41614 |  0:01:48s
epoch 67 | loss: 0.15049 | val_0_rmse: 0.38407 | val_1_rmse: 0.43084 |  0:01:49s
epoch 68 | loss: 0.14536 | val_0_rmse: 0.37262 | val_1_rmse: 0.41419 |  0:01:51s
epoch 69 | loss: 0.14719 | val_0_rmse: 0.38494 | val_1_rmse: 0.43008 |  0:01:52s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.4107
Best weights from best epoch are automatically used!
ended training at: 07:12:07
Feature importance:
Mean squared error is of 844846043.6262223
Mean absolute error:19626.3908339467
MAPE:0.21939235109519148
R2 score:0.818181824328835
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:12:33
epoch 0  | loss: 0.66183 | val_0_rmse: 0.76384 | val_1_rmse: 0.77006 |  0:00:03s
epoch 1  | loss: 0.36065 | val_0_rmse: 0.74325 | val_1_rmse: 0.75193 |  0:00:06s
epoch 2  | loss: 0.31598 | val_0_rmse: 0.64973 | val_1_rmse: 0.65496 |  0:00:10s
epoch 3  | loss: 0.28278 | val_0_rmse: 0.58071 | val_1_rmse: 0.58803 |  0:00:13s
epoch 4  | loss: 0.27746 | val_0_rmse: 0.56618 | val_1_rmse: 0.57386 |  0:00:16s
epoch 5  | loss: 0.27958 | val_0_rmse: 0.54621 | val_1_rmse: 0.55589 |  0:00:20s
epoch 6  | loss: 0.26584 | val_0_rmse: 0.51166 | val_1_rmse: 0.52058 |  0:00:23s
epoch 7  | loss: 0.25955 | val_0_rmse: 0.49405 | val_1_rmse: 0.50319 |  0:00:26s
epoch 8  | loss: 0.24774 | val_0_rmse: 0.52497 | val_1_rmse: 0.52705 |  0:00:30s
epoch 9  | loss: 0.2434  | val_0_rmse: 0.47852 | val_1_rmse: 0.48727 |  0:00:33s
epoch 10 | loss: 0.23628 | val_0_rmse: 0.49487 | val_1_rmse: 0.50402 |  0:00:36s
epoch 11 | loss: 0.23408 | val_0_rmse: 0.47401 | val_1_rmse: 0.48283 |  0:00:40s
epoch 12 | loss: 0.23366 | val_0_rmse: 0.45475 | val_1_rmse: 0.46597 |  0:00:43s
epoch 13 | loss: 0.22797 | val_0_rmse: 0.47662 | val_1_rmse: 0.48657 |  0:00:46s
epoch 14 | loss: 0.22728 | val_0_rmse: 0.46607 | val_1_rmse: 0.47724 |  0:00:50s
epoch 15 | loss: 0.22563 | val_0_rmse: 0.46261 | val_1_rmse: 0.47293 |  0:00:53s
epoch 16 | loss: 0.2299  | val_0_rmse: 0.46563 | val_1_rmse: 0.48098 |  0:00:56s
epoch 17 | loss: 0.22972 | val_0_rmse: 0.51864 | val_1_rmse: 0.52491 |  0:01:00s
epoch 18 | loss: 0.23239 | val_0_rmse: 0.48383 | val_1_rmse: 0.4944  |  0:01:03s
epoch 19 | loss: 0.23011 | val_0_rmse: 0.46417 | val_1_rmse: 0.47425 |  0:01:06s
epoch 20 | loss: 0.22349 | val_0_rmse: 0.45365 | val_1_rmse: 0.46395 |  0:01:10s
epoch 21 | loss: 0.21802 | val_0_rmse: 0.45366 | val_1_rmse: 0.46645 |  0:01:13s
epoch 22 | loss: 0.21475 | val_0_rmse: 0.44513 | val_1_rmse: 0.45862 |  0:01:16s
epoch 23 | loss: 0.21527 | val_0_rmse: 0.44214 | val_1_rmse: 0.45389 |  0:01:20s
epoch 24 | loss: 0.21331 | val_0_rmse: 0.44267 | val_1_rmse: 0.45594 |  0:01:23s
epoch 25 | loss: 0.21008 | val_0_rmse: 0.48635 | val_1_rmse: 0.49614 |  0:01:26s
epoch 26 | loss: 0.2087  | val_0_rmse: 0.43818 | val_1_rmse: 0.45257 |  0:01:30s
epoch 27 | loss: 0.20653 | val_0_rmse: 0.43243 | val_1_rmse: 0.44815 |  0:01:33s
epoch 28 | loss: 0.20255 | val_0_rmse: 0.43652 | val_1_rmse: 0.45163 |  0:01:37s
epoch 29 | loss: 0.20734 | val_0_rmse: 0.43961 | val_1_rmse: 0.4551  |  0:01:40s
epoch 30 | loss: 0.20356 | val_0_rmse: 0.47477 | val_1_rmse: 0.48849 |  0:01:43s
epoch 31 | loss: 0.21289 | val_0_rmse: 0.43244 | val_1_rmse: 0.44524 |  0:01:47s
epoch 32 | loss: 0.20772 | val_0_rmse: 0.44565 | val_1_rmse: 0.46097 |  0:01:50s
epoch 33 | loss: 0.21268 | val_0_rmse: 0.47106 | val_1_rmse: 0.48203 |  0:01:53s
epoch 34 | loss: 0.20285 | val_0_rmse: 0.45213 | val_1_rmse: 0.46602 |  0:01:57s
epoch 35 | loss: 0.20692 | val_0_rmse: 0.44353 | val_1_rmse: 0.45486 |  0:02:00s
epoch 36 | loss: 0.20356 | val_0_rmse: 0.43421 | val_1_rmse: 0.44698 |  0:02:03s
epoch 37 | loss: 0.20373 | val_0_rmse: 0.43415 | val_1_rmse: 0.44759 |  0:02:07s
epoch 38 | loss: 0.20089 | val_0_rmse: 0.43425 | val_1_rmse: 0.44818 |  0:02:10s
epoch 39 | loss: 0.20059 | val_0_rmse: 0.44092 | val_1_rmse: 0.45648 |  0:02:13s
epoch 40 | loss: 0.20165 | val_0_rmse: 0.43871 | val_1_rmse: 0.45323 |  0:02:17s
epoch 41 | loss: 0.21269 | val_0_rmse: 0.45902 | val_1_rmse: 0.47276 |  0:02:20s
epoch 42 | loss: 0.20568 | val_0_rmse: 0.44339 | val_1_rmse: 0.4568  |  0:02:23s
epoch 43 | loss: 0.20855 | val_0_rmse: 0.48195 | val_1_rmse: 0.49704 |  0:02:27s
epoch 44 | loss: 0.21062 | val_0_rmse: 0.4503  | val_1_rmse: 0.46443 |  0:02:30s
epoch 45 | loss: 0.2016  | val_0_rmse: 0.43255 | val_1_rmse: 0.44733 |  0:02:33s
epoch 46 | loss: 0.20255 | val_0_rmse: 0.45318 | val_1_rmse: 0.46931 |  0:02:37s
epoch 47 | loss: 0.2071  | val_0_rmse: 0.44491 | val_1_rmse: 0.46005 |  0:02:40s
epoch 48 | loss: 0.20555 | val_0_rmse: 0.45595 | val_1_rmse: 0.46676 |  0:02:43s
epoch 49 | loss: 0.23055 | val_0_rmse: 0.50285 | val_1_rmse: 0.50659 |  0:02:47s
epoch 50 | loss: 0.23572 | val_0_rmse: 0.49123 | val_1_rmse: 0.49864 |  0:02:50s
epoch 51 | loss: 0.22024 | val_0_rmse: 0.46384 | val_1_rmse: 0.47175 |  0:02:53s
epoch 52 | loss: 0.21498 | val_0_rmse: 0.49967 | val_1_rmse: 0.50437 |  0:02:57s
epoch 53 | loss: 0.2257  | val_0_rmse: 0.46903 | val_1_rmse: 0.47889 |  0:03:00s
epoch 54 | loss: 0.21994 | val_0_rmse: 0.45337 | val_1_rmse: 0.46952 |  0:03:03s
epoch 55 | loss: 0.20925 | val_0_rmse: 0.45447 | val_1_rmse: 0.46438 |  0:03:07s
epoch 56 | loss: 0.22561 | val_0_rmse: 0.53413 | val_1_rmse: 0.53914 |  0:03:10s
epoch 57 | loss: 0.22234 | val_0_rmse: 0.52165 | val_1_rmse: 0.52956 |  0:03:13s
epoch 58 | loss: 0.21064 | val_0_rmse: 0.45878 | val_1_rmse: 0.47096 |  0:03:17s
epoch 59 | loss: 0.20328 | val_0_rmse: 0.44178 | val_1_rmse: 0.45396 |  0:03:20s
epoch 60 | loss: 0.20686 | val_0_rmse: 0.4517  | val_1_rmse: 0.46255 |  0:03:23s
epoch 61 | loss: 0.2195  | val_0_rmse: 0.46691 | val_1_rmse: 0.47778 |  0:03:27s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.44524
Best weights from best epoch are automatically used!
ended training at: 07:16:01
Feature importance:
Mean squared error is of 10927129309.312683
Mean absolute error:72389.66935093573
MAPE:0.2700124360128566
R2 score:0.8091990261490627
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:16:02
epoch 0  | loss: 0.65737 | val_0_rmse: 0.70342 | val_1_rmse: 0.70695 |  0:00:03s
epoch 1  | loss: 0.38645 | val_0_rmse: 0.70015 | val_1_rmse: 0.70957 |  0:00:06s
epoch 2  | loss: 0.32004 | val_0_rmse: 0.62438 | val_1_rmse: 0.629   |  0:00:10s
epoch 3  | loss: 0.29675 | val_0_rmse: 0.60965 | val_1_rmse: 0.61334 |  0:00:13s
epoch 4  | loss: 0.26719 | val_0_rmse: 0.58284 | val_1_rmse: 0.58611 |  0:00:16s
epoch 5  | loss: 0.25541 | val_0_rmse: 0.53155 | val_1_rmse: 0.53916 |  0:00:20s
epoch 6  | loss: 0.24433 | val_0_rmse: 0.49132 | val_1_rmse: 0.50018 |  0:00:23s
epoch 7  | loss: 0.23544 | val_0_rmse: 0.48063 | val_1_rmse: 0.48996 |  0:00:26s
epoch 8  | loss: 0.23028 | val_0_rmse: 0.47714 | val_1_rmse: 0.48687 |  0:00:30s
epoch 9  | loss: 0.22863 | val_0_rmse: 0.45937 | val_1_rmse: 0.46886 |  0:00:33s
epoch 10 | loss: 0.22523 | val_0_rmse: 0.46272 | val_1_rmse: 0.46953 |  0:00:37s
epoch 11 | loss: 0.22572 | val_0_rmse: 0.4541  | val_1_rmse: 0.46504 |  0:00:40s
epoch 12 | loss: 0.22093 | val_0_rmse: 0.47541 | val_1_rmse: 0.48518 |  0:00:43s
epoch 13 | loss: 0.22293 | val_0_rmse: 0.45695 | val_1_rmse: 0.46534 |  0:00:47s
epoch 14 | loss: 0.21791 | val_0_rmse: 0.45195 | val_1_rmse: 0.45881 |  0:00:50s
epoch 15 | loss: 0.21777 | val_0_rmse: 0.44316 | val_1_rmse: 0.44908 |  0:00:53s
epoch 16 | loss: 0.21972 | val_0_rmse: 0.44465 | val_1_rmse: 0.45479 |  0:00:57s
epoch 17 | loss: 0.21225 | val_0_rmse: 0.43775 | val_1_rmse: 0.4483  |  0:01:00s
epoch 18 | loss: 0.21486 | val_0_rmse: 0.47721 | val_1_rmse: 0.49278 |  0:01:03s
epoch 19 | loss: 0.22506 | val_0_rmse: 0.44965 | val_1_rmse: 0.45764 |  0:01:07s
epoch 20 | loss: 0.21671 | val_0_rmse: 0.45874 | val_1_rmse: 0.46504 |  0:01:10s
epoch 21 | loss: 0.21963 | val_0_rmse: 0.46289 | val_1_rmse: 0.47341 |  0:01:14s
epoch 22 | loss: 0.22424 | val_0_rmse: 0.50705 | val_1_rmse: 0.51746 |  0:01:17s
epoch 23 | loss: 0.22467 | val_0_rmse: 0.45098 | val_1_rmse: 0.46036 |  0:01:20s
epoch 24 | loss: 0.22634 | val_0_rmse: 0.46572 | val_1_rmse: 0.47204 |  0:01:24s
epoch 25 | loss: 0.21374 | val_0_rmse: 0.43522 | val_1_rmse: 0.44391 |  0:01:27s
epoch 26 | loss: 0.20552 | val_0_rmse: 0.43481 | val_1_rmse: 0.44277 |  0:01:30s
epoch 27 | loss: 0.24355 | val_0_rmse: 0.48536 | val_1_rmse: 0.49396 |  0:01:34s
epoch 28 | loss: 0.2306  | val_0_rmse: 0.45711 | val_1_rmse: 0.46542 |  0:01:37s
epoch 29 | loss: 0.21622 | val_0_rmse: 0.44125 | val_1_rmse: 0.45113 |  0:01:40s
epoch 30 | loss: 0.21185 | val_0_rmse: 0.43766 | val_1_rmse: 0.44879 |  0:01:44s
epoch 31 | loss: 0.21013 | val_0_rmse: 0.4488  | val_1_rmse: 0.45991 |  0:01:47s
epoch 32 | loss: 0.21146 | val_0_rmse: 0.4366  | val_1_rmse: 0.44838 |  0:01:50s
epoch 33 | loss: 0.20554 | val_0_rmse: 0.43859 | val_1_rmse: 0.44811 |  0:01:54s
epoch 34 | loss: 0.20447 | val_0_rmse: 0.43204 | val_1_rmse: 0.44595 |  0:01:57s
epoch 35 | loss: 0.2039  | val_0_rmse: 0.42834 | val_1_rmse: 0.43965 |  0:02:00s
epoch 36 | loss: 0.20066 | val_0_rmse: 0.4284  | val_1_rmse: 0.44183 |  0:02:04s
epoch 37 | loss: 0.19959 | val_0_rmse: 0.42775 | val_1_rmse: 0.44124 |  0:02:07s
epoch 38 | loss: 0.20033 | val_0_rmse: 0.4322  | val_1_rmse: 0.44553 |  0:02:11s
epoch 39 | loss: 0.20345 | val_0_rmse: 0.42864 | val_1_rmse: 0.44287 |  0:02:14s
epoch 40 | loss: 0.19836 | val_0_rmse: 0.4257  | val_1_rmse: 0.44326 |  0:02:17s
epoch 41 | loss: 0.19498 | val_0_rmse: 0.43443 | val_1_rmse: 0.44781 |  0:02:21s
epoch 42 | loss: 0.1939  | val_0_rmse: 0.41629 | val_1_rmse: 0.43449 |  0:02:24s
epoch 43 | loss: 0.19856 | val_0_rmse: 0.43566 | val_1_rmse: 0.44951 |  0:02:27s
epoch 44 | loss: 0.19539 | val_0_rmse: 0.41547 | val_1_rmse: 0.43315 |  0:02:31s
epoch 45 | loss: 0.19529 | val_0_rmse: 0.41585 | val_1_rmse: 0.43103 |  0:02:34s
epoch 46 | loss: 0.19473 | val_0_rmse: 0.42198 | val_1_rmse: 0.43833 |  0:02:37s
epoch 47 | loss: 0.19387 | val_0_rmse: 0.41344 | val_1_rmse: 0.43092 |  0:02:41s
epoch 48 | loss: 0.19097 | val_0_rmse: 0.41293 | val_1_rmse: 0.43165 |  0:02:44s
epoch 49 | loss: 0.19304 | val_0_rmse: 0.42474 | val_1_rmse: 0.4402  |  0:02:47s
epoch 50 | loss: 0.19432 | val_0_rmse: 0.42301 | val_1_rmse: 0.4391  |  0:02:51s
epoch 51 | loss: 0.19186 | val_0_rmse: 0.42131 | val_1_rmse: 0.43814 |  0:02:54s
epoch 52 | loss: 0.18969 | val_0_rmse: 0.42335 | val_1_rmse: 0.43977 |  0:02:58s
epoch 53 | loss: 0.19055 | val_0_rmse: 0.41417 | val_1_rmse: 0.43479 |  0:03:01s
epoch 54 | loss: 0.18891 | val_0_rmse: 0.43186 | val_1_rmse: 0.44929 |  0:03:04s
epoch 55 | loss: 0.1881  | val_0_rmse: 0.40971 | val_1_rmse: 0.42878 |  0:03:08s
epoch 56 | loss: 0.18614 | val_0_rmse: 0.41147 | val_1_rmse: 0.43059 |  0:03:11s
epoch 57 | loss: 0.18678 | val_0_rmse: 0.41137 | val_1_rmse: 0.43157 |  0:03:14s
epoch 58 | loss: 0.18467 | val_0_rmse: 0.40919 | val_1_rmse: 0.4292  |  0:03:18s
epoch 59 | loss: 0.19086 | val_0_rmse: 0.41664 | val_1_rmse: 0.43672 |  0:03:21s
epoch 60 | loss: 0.18702 | val_0_rmse: 0.40727 | val_1_rmse: 0.43009 |  0:03:24s
epoch 61 | loss: 0.18581 | val_0_rmse: 0.41083 | val_1_rmse: 0.43341 |  0:03:28s
epoch 62 | loss: 0.18479 | val_0_rmse: 0.41333 | val_1_rmse: 0.43365 |  0:03:31s
epoch 63 | loss: 0.18493 | val_0_rmse: 0.41749 | val_1_rmse: 0.43903 |  0:03:34s
epoch 64 | loss: 0.18147 | val_0_rmse: 0.41523 | val_1_rmse: 0.43892 |  0:03:38s
epoch 65 | loss: 0.18328 | val_0_rmse: 0.40481 | val_1_rmse: 0.42678 |  0:03:41s
epoch 66 | loss: 0.18202 | val_0_rmse: 0.41108 | val_1_rmse: 0.43579 |  0:03:44s
epoch 67 | loss: 0.18138 | val_0_rmse: 0.41088 | val_1_rmse: 0.43437 |  0:03:48s
epoch 68 | loss: 0.18167 | val_0_rmse: 0.40576 | val_1_rmse: 0.42997 |  0:03:51s
epoch 69 | loss: 0.18436 | val_0_rmse: 0.4101  | val_1_rmse: 0.43387 |  0:03:55s
epoch 70 | loss: 0.18321 | val_0_rmse: 0.40741 | val_1_rmse: 0.43512 |  0:03:58s
epoch 71 | loss: 0.18418 | val_0_rmse: 0.40728 | val_1_rmse: 0.43071 |  0:04:01s
epoch 72 | loss: 0.1805  | val_0_rmse: 0.40849 | val_1_rmse: 0.43314 |  0:04:05s
epoch 73 | loss: 0.17988 | val_0_rmse: 0.40749 | val_1_rmse: 0.43423 |  0:04:08s
epoch 74 | loss: 0.17872 | val_0_rmse: 0.40472 | val_1_rmse: 0.43113 |  0:04:11s
epoch 75 | loss: 0.17806 | val_0_rmse: 0.40652 | val_1_rmse: 0.43176 |  0:04:15s
epoch 76 | loss: 0.17696 | val_0_rmse: 0.41442 | val_1_rmse: 0.43596 |  0:04:18s
epoch 77 | loss: 0.18037 | val_0_rmse: 0.41541 | val_1_rmse: 0.44099 |  0:04:21s
epoch 78 | loss: 0.17851 | val_0_rmse: 0.40694 | val_1_rmse: 0.43081 |  0:04:25s
epoch 79 | loss: 0.17825 | val_0_rmse: 0.40927 | val_1_rmse: 0.43576 |  0:04:28s
epoch 80 | loss: 0.17468 | val_0_rmse: 0.40036 | val_1_rmse: 0.42727 |  0:04:31s
epoch 81 | loss: 0.17794 | val_0_rmse: 0.40092 | val_1_rmse: 0.42812 |  0:04:35s
epoch 82 | loss: 0.17718 | val_0_rmse: 0.40275 | val_1_rmse: 0.43003 |  0:04:38s
epoch 83 | loss: 0.17713 | val_0_rmse: 0.40541 | val_1_rmse: 0.43486 |  0:04:41s
epoch 84 | loss: 0.178   | val_0_rmse: 0.40582 | val_1_rmse: 0.43018 |  0:04:45s
epoch 85 | loss: 0.17694 | val_0_rmse: 0.4056  | val_1_rmse: 0.42978 |  0:04:48s
epoch 86 | loss: 0.17656 | val_0_rmse: 0.40594 | val_1_rmse: 0.43435 |  0:04:51s
epoch 87 | loss: 0.17841 | val_0_rmse: 0.40286 | val_1_rmse: 0.42861 |  0:04:55s
epoch 88 | loss: 0.17633 | val_0_rmse: 0.41904 | val_1_rmse: 0.443   |  0:04:58s
epoch 89 | loss: 0.175   | val_0_rmse: 0.40161 | val_1_rmse: 0.42908 |  0:05:02s
epoch 90 | loss: 0.17894 | val_0_rmse: 0.4038  | val_1_rmse: 0.43103 |  0:05:05s
epoch 91 | loss: 0.17981 | val_0_rmse: 0.41031 | val_1_rmse: 0.439   |  0:05:08s
epoch 92 | loss: 0.17452 | val_0_rmse: 0.40532 | val_1_rmse: 0.43403 |  0:05:12s
epoch 93 | loss: 0.17335 | val_0_rmse: 0.39898 | val_1_rmse: 0.43042 |  0:05:15s
epoch 94 | loss: 0.17247 | val_0_rmse: 0.39676 | val_1_rmse: 0.42638 |  0:05:18s
epoch 95 | loss: 0.17477 | val_0_rmse: 0.39621 | val_1_rmse: 0.42828 |  0:05:22s
epoch 96 | loss: 0.17793 | val_0_rmse: 0.39884 | val_1_rmse: 0.42854 |  0:05:25s
epoch 97 | loss: 0.17422 | val_0_rmse: 0.40116 | val_1_rmse: 0.4332  |  0:05:28s
epoch 98 | loss: 0.1748  | val_0_rmse: 0.39782 | val_1_rmse: 0.42661 |  0:05:32s
epoch 99 | loss: 0.17454 | val_0_rmse: 0.39616 | val_1_rmse: 0.42422 |  0:05:35s
epoch 100| loss: 0.17205 | val_0_rmse: 0.3978  | val_1_rmse: 0.42697 |  0:05:39s
epoch 101| loss: 0.17395 | val_0_rmse: 0.40304 | val_1_rmse: 0.43363 |  0:05:42s
epoch 102| loss: 0.17694 | val_0_rmse: 0.40977 | val_1_rmse: 0.43513 |  0:05:45s
epoch 103| loss: 0.17152 | val_0_rmse: 0.40187 | val_1_rmse: 0.43563 |  0:05:49s
epoch 104| loss: 0.16997 | val_0_rmse: 0.38985 | val_1_rmse: 0.42113 |  0:05:52s
epoch 105| loss: 0.17368 | val_0_rmse: 0.39626 | val_1_rmse: 0.43223 |  0:05:55s
epoch 106| loss: 0.16909 | val_0_rmse: 0.39709 | val_1_rmse: 0.42891 |  0:05:59s
epoch 107| loss: 0.17193 | val_0_rmse: 0.41828 | val_1_rmse: 0.44716 |  0:06:02s
epoch 108| loss: 0.17465 | val_0_rmse: 0.39327 | val_1_rmse: 0.42463 |  0:06:05s
epoch 109| loss: 0.17129 | val_0_rmse: 0.39505 | val_1_rmse: 0.42836 |  0:06:09s
epoch 110| loss: 0.17155 | val_0_rmse: 0.39263 | val_1_rmse: 0.42418 |  0:06:12s
epoch 111| loss: 0.17321 | val_0_rmse: 0.40324 | val_1_rmse: 0.43202 |  0:06:15s
epoch 112| loss: 0.17204 | val_0_rmse: 0.40761 | val_1_rmse: 0.43578 |  0:06:19s
epoch 113| loss: 0.17092 | val_0_rmse: 0.39901 | val_1_rmse: 0.42841 |  0:06:22s
epoch 114| loss: 0.16945 | val_0_rmse: 0.39318 | val_1_rmse: 0.4233  |  0:06:25s
epoch 115| loss: 0.16894 | val_0_rmse: 0.39579 | val_1_rmse: 0.42684 |  0:06:29s
epoch 116| loss: 0.17415 | val_0_rmse: 0.3962  | val_1_rmse: 0.42706 |  0:06:32s
epoch 117| loss: 0.17131 | val_0_rmse: 0.39424 | val_1_rmse: 0.42714 |  0:06:36s
epoch 118| loss: 0.16948 | val_0_rmse: 0.39779 | val_1_rmse: 0.43267 |  0:06:39s
epoch 119| loss: 0.17114 | val_0_rmse: 0.3926  | val_1_rmse: 0.42611 |  0:06:42s
epoch 120| loss: 0.1707  | val_0_rmse: 0.39722 | val_1_rmse: 0.43072 |  0:06:46s
epoch 121| loss: 0.16948 | val_0_rmse: 0.38957 | val_1_rmse: 0.42611 |  0:06:49s
epoch 122| loss: 0.16909 | val_0_rmse: 0.39246 | val_1_rmse: 0.42551 |  0:06:52s
epoch 123| loss: 0.16729 | val_0_rmse: 0.41607 | val_1_rmse: 0.44507 |  0:06:56s
epoch 124| loss: 0.16925 | val_0_rmse: 0.38661 | val_1_rmse: 0.42077 |  0:06:59s
epoch 125| loss: 0.16872 | val_0_rmse: 0.39292 | val_1_rmse: 0.42618 |  0:07:02s
epoch 126| loss: 0.17158 | val_0_rmse: 0.39037 | val_1_rmse: 0.42412 |  0:07:06s
epoch 127| loss: 0.17235 | val_0_rmse: 0.40294 | val_1_rmse: 0.43166 |  0:07:09s
epoch 128| loss: 0.17009 | val_0_rmse: 0.40253 | val_1_rmse: 0.43449 |  0:07:12s
epoch 129| loss: 0.17341 | val_0_rmse: 0.4184  | val_1_rmse: 0.44955 |  0:07:16s
epoch 130| loss: 0.17277 | val_0_rmse: 0.39819 | val_1_rmse: 0.42899 |  0:07:19s
epoch 131| loss: 0.17227 | val_0_rmse: 0.38848 | val_1_rmse: 0.42224 |  0:07:23s
epoch 132| loss: 0.16952 | val_0_rmse: 0.40115 | val_1_rmse: 0.43427 |  0:07:26s
epoch 133| loss: 0.16805 | val_0_rmse: 0.39513 | val_1_rmse: 0.42974 |  0:07:29s
epoch 134| loss: 0.17567 | val_0_rmse: 0.39587 | val_1_rmse: 0.42534 |  0:07:33s
epoch 135| loss: 0.17123 | val_0_rmse: 0.39223 | val_1_rmse: 0.42889 |  0:07:36s
epoch 136| loss: 0.16811 | val_0_rmse: 0.39301 | val_1_rmse: 0.42702 |  0:07:39s
epoch 137| loss: 0.17016 | val_0_rmse: 0.40666 | val_1_rmse: 0.4374  |  0:07:43s
epoch 138| loss: 0.18447 | val_0_rmse: 0.4061  | val_1_rmse: 0.43497 |  0:07:46s
epoch 139| loss: 0.17191 | val_0_rmse: 0.39667 | val_1_rmse: 0.42607 |  0:07:49s
epoch 140| loss: 0.17017 | val_0_rmse: 0.41039 | val_1_rmse: 0.44012 |  0:07:53s
epoch 141| loss: 0.17182 | val_0_rmse: 0.39051 | val_1_rmse: 0.42544 |  0:07:56s
epoch 142| loss: 0.16988 | val_0_rmse: 0.40434 | val_1_rmse: 0.43888 |  0:07:59s
epoch 143| loss: 0.16891 | val_0_rmse: 0.39923 | val_1_rmse: 0.43385 |  0:08:03s
epoch 144| loss: 0.17252 | val_0_rmse: 0.41506 | val_1_rmse: 0.44317 |  0:08:06s
epoch 145| loss: 0.16855 | val_0_rmse: 0.39092 | val_1_rmse: 0.42383 |  0:08:10s
epoch 146| loss: 0.16653 | val_0_rmse: 0.39092 | val_1_rmse: 0.42823 |  0:08:13s
epoch 147| loss: 0.16701 | val_0_rmse: 0.39082 | val_1_rmse: 0.42593 |  0:08:16s
epoch 148| loss: 0.16541 | val_0_rmse: 0.38666 | val_1_rmse: 0.42159 |  0:08:20s
epoch 149| loss: 0.16588 | val_0_rmse: 0.38967 | val_1_rmse: 0.42855 |  0:08:23s
Stop training because you reached max_epochs = 150 with best_epoch = 124 and best_val_1_rmse = 0.42077
Best weights from best epoch are automatically used!
ended training at: 07:24:26
Feature importance:
Mean squared error is of 9631051660.571272
Mean absolute error:66673.94045183354
MAPE:0.2547989309621929
R2 score:0.8325497477306326
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:24:27
epoch 0  | loss: 0.61773 | val_0_rmse: 0.72222 | val_1_rmse: 0.72809 |  0:00:03s
epoch 1  | loss: 0.36976 | val_0_rmse: 0.59647 | val_1_rmse: 0.59602 |  0:00:06s
epoch 2  | loss: 0.30569 | val_0_rmse: 0.56802 | val_1_rmse: 0.5685  |  0:00:10s
epoch 3  | loss: 0.3005  | val_0_rmse: 0.58362 | val_1_rmse: 0.5826  |  0:00:13s
epoch 4  | loss: 0.27742 | val_0_rmse: 0.53704 | val_1_rmse: 0.5415  |  0:00:16s
epoch 5  | loss: 0.2706  | val_0_rmse: 0.521   | val_1_rmse: 0.52426 |  0:00:20s
epoch 6  | loss: 0.25994 | val_0_rmse: 0.51321 | val_1_rmse: 0.51386 |  0:00:23s
epoch 7  | loss: 0.25462 | val_0_rmse: 0.5103  | val_1_rmse: 0.51339 |  0:00:26s
epoch 8  | loss: 0.2455  | val_0_rmse: 0.47751 | val_1_rmse: 0.47901 |  0:00:30s
epoch 9  | loss: 0.24731 | val_0_rmse: 0.48432 | val_1_rmse: 0.48888 |  0:00:33s
epoch 10 | loss: 0.24073 | val_0_rmse: 0.46702 | val_1_rmse: 0.47073 |  0:00:37s
epoch 11 | loss: 0.23148 | val_0_rmse: 0.46724 | val_1_rmse: 0.47325 |  0:00:40s
epoch 12 | loss: 0.23422 | val_0_rmse: 0.4611  | val_1_rmse: 0.46511 |  0:00:43s
epoch 13 | loss: 0.23569 | val_0_rmse: 0.46668 | val_1_rmse: 0.47017 |  0:00:47s
epoch 14 | loss: 0.23054 | val_0_rmse: 0.4577  | val_1_rmse: 0.46328 |  0:00:50s
epoch 15 | loss: 0.23954 | val_0_rmse: 0.4625  | val_1_rmse: 0.46414 |  0:00:53s
epoch 16 | loss: 0.22921 | val_0_rmse: 0.4555  | val_1_rmse: 0.4598  |  0:00:57s
epoch 17 | loss: 0.22664 | val_0_rmse: 0.46341 | val_1_rmse: 0.46808 |  0:01:00s
epoch 18 | loss: 0.22219 | val_0_rmse: 0.44989 | val_1_rmse: 0.45536 |  0:01:03s
epoch 19 | loss: 0.21963 | val_0_rmse: 0.44506 | val_1_rmse: 0.45037 |  0:01:07s
epoch 20 | loss: 0.21678 | val_0_rmse: 0.45054 | val_1_rmse: 0.45742 |  0:01:10s
epoch 21 | loss: 0.21715 | val_0_rmse: 0.44398 | val_1_rmse: 0.45035 |  0:01:14s
epoch 22 | loss: 0.2153  | val_0_rmse: 0.44662 | val_1_rmse: 0.45413 |  0:01:17s
epoch 23 | loss: 0.21295 | val_0_rmse: 0.47705 | val_1_rmse: 0.48551 |  0:01:20s
epoch 24 | loss: 0.21303 | val_0_rmse: 0.43706 | val_1_rmse: 0.44426 |  0:01:24s
epoch 25 | loss: 0.21136 | val_0_rmse: 0.44944 | val_1_rmse: 0.45541 |  0:01:27s
epoch 26 | loss: 0.20981 | val_0_rmse: 0.44556 | val_1_rmse: 0.45497 |  0:01:30s
epoch 27 | loss: 0.20917 | val_0_rmse: 0.43688 | val_1_rmse: 0.44382 |  0:01:34s
epoch 28 | loss: 0.21113 | val_0_rmse: 0.45044 | val_1_rmse: 0.46185 |  0:01:37s
epoch 29 | loss: 0.2093  | val_0_rmse: 0.45195 | val_1_rmse: 0.46105 |  0:01:41s
epoch 30 | loss: 0.20706 | val_0_rmse: 0.45129 | val_1_rmse: 0.45986 |  0:01:44s
epoch 31 | loss: 0.20208 | val_0_rmse: 0.44231 | val_1_rmse: 0.4532  |  0:01:47s
epoch 32 | loss: 0.20843 | val_0_rmse: 0.43555 | val_1_rmse: 0.44342 |  0:01:51s
epoch 33 | loss: 0.20405 | val_0_rmse: 0.43352 | val_1_rmse: 0.4406  |  0:01:54s
epoch 34 | loss: 0.20441 | val_0_rmse: 0.43701 | val_1_rmse: 0.44489 |  0:01:57s
epoch 35 | loss: 0.20704 | val_0_rmse: 0.46188 | val_1_rmse: 0.47077 |  0:02:01s
epoch 36 | loss: 0.20538 | val_0_rmse: 0.4401  | val_1_rmse: 0.44679 |  0:02:04s
epoch 37 | loss: 0.20114 | val_0_rmse: 0.43532 | val_1_rmse: 0.44314 |  0:02:07s
epoch 38 | loss: 0.19923 | val_0_rmse: 0.43139 | val_1_rmse: 0.44038 |  0:02:11s
epoch 39 | loss: 0.19981 | val_0_rmse: 0.44    | val_1_rmse: 0.45132 |  0:02:14s
epoch 40 | loss: 0.20001 | val_0_rmse: 0.43789 | val_1_rmse: 0.44718 |  0:02:18s
epoch 41 | loss: 0.19813 | val_0_rmse: 0.42808 | val_1_rmse: 0.43861 |  0:02:21s
epoch 42 | loss: 0.19866 | val_0_rmse: 0.43556 | val_1_rmse: 0.44628 |  0:02:24s
epoch 43 | loss: 0.20051 | val_0_rmse: 0.44344 | val_1_rmse: 0.45336 |  0:02:28s
epoch 44 | loss: 0.19928 | val_0_rmse: 0.42973 | val_1_rmse: 0.4396  |  0:02:31s
epoch 45 | loss: 0.19438 | val_0_rmse: 0.42495 | val_1_rmse: 0.43797 |  0:02:34s
epoch 46 | loss: 0.19794 | val_0_rmse: 0.43796 | val_1_rmse: 0.44717 |  0:02:38s
epoch 47 | loss: 0.19721 | val_0_rmse: 0.443   | val_1_rmse: 0.45369 |  0:02:41s
epoch 48 | loss: 0.19623 | val_0_rmse: 0.43685 | val_1_rmse: 0.44819 |  0:02:44s
epoch 49 | loss: 0.19977 | val_0_rmse: 0.47187 | val_1_rmse: 0.48112 |  0:02:48s
epoch 50 | loss: 0.19993 | val_0_rmse: 0.44502 | val_1_rmse: 0.45645 |  0:02:51s
epoch 51 | loss: 0.19675 | val_0_rmse: 0.44993 | val_1_rmse: 0.46139 |  0:02:54s
epoch 52 | loss: 0.19776 | val_0_rmse: 0.43431 | val_1_rmse: 0.44627 |  0:02:58s
epoch 53 | loss: 0.19396 | val_0_rmse: 0.42506 | val_1_rmse: 0.43556 |  0:03:01s
epoch 54 | loss: 0.19553 | val_0_rmse: 0.42869 | val_1_rmse: 0.44307 |  0:03:04s
epoch 55 | loss: 0.19507 | val_0_rmse: 0.42534 | val_1_rmse: 0.43564 |  0:03:08s
epoch 56 | loss: 0.19539 | val_0_rmse: 0.43651 | val_1_rmse: 0.44666 |  0:03:11s
epoch 57 | loss: 0.19346 | val_0_rmse: 0.43413 | val_1_rmse: 0.44718 |  0:03:15s
epoch 58 | loss: 0.19414 | val_0_rmse: 0.42975 | val_1_rmse: 0.44219 |  0:03:18s
epoch 59 | loss: 0.19507 | val_0_rmse: 0.42404 | val_1_rmse: 0.43462 |  0:03:21s
epoch 60 | loss: 0.19795 | val_0_rmse: 0.43023 | val_1_rmse: 0.44256 |  0:03:25s
epoch 61 | loss: 0.19199 | val_0_rmse: 0.42762 | val_1_rmse: 0.43958 |  0:03:28s
epoch 62 | loss: 0.19222 | val_0_rmse: 0.43454 | val_1_rmse: 0.44725 |  0:03:31s
epoch 63 | loss: 0.19216 | val_0_rmse: 0.41615 | val_1_rmse: 0.43089 |  0:03:35s
epoch 64 | loss: 0.19137 | val_0_rmse: 0.42276 | val_1_rmse: 0.43456 |  0:03:38s
epoch 65 | loss: 0.18858 | val_0_rmse: 0.41951 | val_1_rmse: 0.43278 |  0:03:41s
epoch 66 | loss: 0.19277 | val_0_rmse: 0.43957 | val_1_rmse: 0.45701 |  0:03:45s
epoch 67 | loss: 0.19242 | val_0_rmse: 0.42417 | val_1_rmse: 0.43692 |  0:03:48s
epoch 68 | loss: 0.19172 | val_0_rmse: 0.43367 | val_1_rmse: 0.44704 |  0:03:52s
epoch 69 | loss: 0.19217 | val_0_rmse: 0.42516 | val_1_rmse: 0.4364  |  0:03:55s
epoch 70 | loss: 0.18936 | val_0_rmse: 0.4389  | val_1_rmse: 0.4502  |  0:03:58s
epoch 71 | loss: 0.18955 | val_0_rmse: 0.41542 | val_1_rmse: 0.42936 |  0:04:02s
epoch 72 | loss: 0.19491 | val_0_rmse: 0.44867 | val_1_rmse: 0.45827 |  0:04:05s
epoch 73 | loss: 0.19439 | val_0_rmse: 0.44121 | val_1_rmse: 0.45087 |  0:04:08s
epoch 74 | loss: 0.19587 | val_0_rmse: 0.44979 | val_1_rmse: 0.46133 |  0:04:12s
epoch 75 | loss: 0.19644 | val_0_rmse: 0.44723 | val_1_rmse: 0.46089 |  0:04:15s
epoch 76 | loss: 0.19471 | val_0_rmse: 0.43052 | val_1_rmse: 0.4433  |  0:04:18s
epoch 77 | loss: 0.19286 | val_0_rmse: 0.4255  | val_1_rmse: 0.43924 |  0:04:22s
epoch 78 | loss: 0.18923 | val_0_rmse: 0.42568 | val_1_rmse: 0.43748 |  0:04:25s
epoch 79 | loss: 0.18853 | val_0_rmse: 0.42476 | val_1_rmse: 0.43998 |  0:04:29s
epoch 80 | loss: 0.18806 | val_0_rmse: 0.4389  | val_1_rmse: 0.44134 |  0:04:32s
epoch 81 | loss: 0.18914 | val_0_rmse: 0.42199 | val_1_rmse: 0.43574 |  0:04:35s
epoch 82 | loss: 0.18754 | val_0_rmse: 0.4216  | val_1_rmse: 0.43932 |  0:04:39s
epoch 83 | loss: 0.18817 | val_0_rmse: 0.43679 | val_1_rmse: 0.45211 |  0:04:42s
epoch 84 | loss: 0.18994 | val_0_rmse: 0.42565 | val_1_rmse: 0.43829 |  0:04:45s
epoch 85 | loss: 0.18704 | val_0_rmse: 0.41544 | val_1_rmse: 0.42817 |  0:04:49s
epoch 86 | loss: 0.1864  | val_0_rmse: 0.4259  | val_1_rmse: 0.43972 |  0:04:52s
epoch 87 | loss: 0.18728 | val_0_rmse: 0.43407 | val_1_rmse: 0.45004 |  0:04:55s
epoch 88 | loss: 0.19454 | val_0_rmse: 0.45698 | val_1_rmse: 0.46386 |  0:04:59s
epoch 89 | loss: 0.21656 | val_0_rmse: 0.45979 | val_1_rmse: 0.46704 |  0:05:02s
epoch 90 | loss: 0.21493 | val_0_rmse: 0.45874 | val_1_rmse: 0.46638 |  0:05:05s
epoch 91 | loss: 0.20483 | val_0_rmse: 0.44645 | val_1_rmse: 0.45351 |  0:05:09s
epoch 92 | loss: 0.2043  | val_0_rmse: 0.45051 | val_1_rmse: 0.45787 |  0:05:12s
epoch 93 | loss: 0.20383 | val_0_rmse: 0.43099 | val_1_rmse: 0.44043 |  0:05:16s
epoch 94 | loss: 0.19921 | val_0_rmse: 0.44055 | val_1_rmse: 0.45384 |  0:05:19s
epoch 95 | loss: 0.19476 | val_0_rmse: 0.4285  | val_1_rmse: 0.44001 |  0:05:22s
epoch 96 | loss: 0.19469 | val_0_rmse: 0.428   | val_1_rmse: 0.43994 |  0:05:26s
epoch 97 | loss: 0.19779 | val_0_rmse: 0.42854 | val_1_rmse: 0.44204 |  0:05:29s
epoch 98 | loss: 0.19732 | val_0_rmse: 0.47862 | val_1_rmse: 0.48791 |  0:05:32s
epoch 99 | loss: 0.19609 | val_0_rmse: 0.42864 | val_1_rmse: 0.44279 |  0:05:36s
epoch 100| loss: 0.19218 | val_0_rmse: 0.42214 | val_1_rmse: 0.43511 |  0:05:39s
epoch 101| loss: 0.1924  | val_0_rmse: 0.42209 | val_1_rmse: 0.43275 |  0:05:42s
epoch 102| loss: 0.19228 | val_0_rmse: 0.4252  | val_1_rmse: 0.4406  |  0:05:46s
epoch 103| loss: 0.1913  | val_0_rmse: 0.44225 | val_1_rmse: 0.45645 |  0:05:49s
epoch 104| loss: 0.19167 | val_0_rmse: 0.42702 | val_1_rmse: 0.44361 |  0:05:52s
epoch 105| loss: 0.189   | val_0_rmse: 0.41677 | val_1_rmse: 0.43262 |  0:05:56s
epoch 106| loss: 0.18875 | val_0_rmse: 0.41819 | val_1_rmse: 0.43517 |  0:05:59s
epoch 107| loss: 0.18606 | val_0_rmse: 0.4175  | val_1_rmse: 0.43339 |  0:06:02s
epoch 108| loss: 0.18926 | val_0_rmse: 0.41822 | val_1_rmse: 0.43674 |  0:06:06s
epoch 109| loss: 0.18751 | val_0_rmse: 0.41903 | val_1_rmse: 0.43772 |  0:06:09s
epoch 110| loss: 0.185   | val_0_rmse: 0.41646 | val_1_rmse: 0.43325 |  0:06:12s
epoch 111| loss: 0.1876  | val_0_rmse: 0.41472 | val_1_rmse: 0.43177 |  0:06:16s
epoch 112| loss: 0.18747 | val_0_rmse: 0.42991 | val_1_rmse: 0.44583 |  0:06:19s
epoch 113| loss: 0.18531 | val_0_rmse: 0.42033 | val_1_rmse: 0.43859 |  0:06:22s
epoch 114| loss: 0.18906 | val_0_rmse: 0.42655 | val_1_rmse: 0.44271 |  0:06:26s
epoch 115| loss: 0.18933 | val_0_rmse: 0.41589 | val_1_rmse: 0.43461 |  0:06:29s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.42817
Best weights from best epoch are automatically used!
ended training at: 07:30:58
Feature importance:
Mean squared error is of 10685423766.58824
Mean absolute error:70025.48280293934
MAPE:0.2588586797374071
R2 score:0.8184706768461457
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:30:58
epoch 0  | loss: 0.5874  | val_0_rmse: 0.72725 | val_1_rmse: 0.73391 |  0:00:03s
epoch 1  | loss: 0.34915 | val_0_rmse: 0.65566 | val_1_rmse: 0.6657  |  0:00:06s
epoch 2  | loss: 0.30119 | val_0_rmse: 0.57428 | val_1_rmse: 0.58151 |  0:00:10s
epoch 3  | loss: 0.29385 | val_0_rmse: 0.57227 | val_1_rmse: 0.5798  |  0:00:13s
epoch 4  | loss: 0.28707 | val_0_rmse: 0.56811 | val_1_rmse: 0.57808 |  0:00:16s
epoch 5  | loss: 0.26922 | val_0_rmse: 0.52872 | val_1_rmse: 0.53691 |  0:00:20s
epoch 6  | loss: 0.25423 | val_0_rmse: 0.51402 | val_1_rmse: 0.52152 |  0:00:23s
epoch 7  | loss: 0.26118 | val_0_rmse: 0.49237 | val_1_rmse: 0.50101 |  0:00:26s
epoch 8  | loss: 0.24383 | val_0_rmse: 0.50267 | val_1_rmse: 0.51177 |  0:00:30s
epoch 9  | loss: 0.23739 | val_0_rmse: 0.46659 | val_1_rmse: 0.47506 |  0:00:33s
epoch 10 | loss: 0.22849 | val_0_rmse: 0.46566 | val_1_rmse: 0.47187 |  0:00:36s
epoch 11 | loss: 0.22605 | val_0_rmse: 0.47258 | val_1_rmse: 0.48219 |  0:00:40s
epoch 12 | loss: 0.22635 | val_0_rmse: 0.46205 | val_1_rmse: 0.47076 |  0:00:43s
epoch 13 | loss: 0.2218  | val_0_rmse: 0.45032 | val_1_rmse: 0.46265 |  0:00:46s
epoch 14 | loss: 0.22137 | val_0_rmse: 0.45491 | val_1_rmse: 0.467   |  0:00:50s
epoch 15 | loss: 0.21972 | val_0_rmse: 0.44484 | val_1_rmse: 0.45676 |  0:00:53s
epoch 16 | loss: 0.21538 | val_0_rmse: 0.44859 | val_1_rmse: 0.45998 |  0:00:57s
epoch 17 | loss: 0.21636 | val_0_rmse: 0.45141 | val_1_rmse: 0.46509 |  0:01:00s
epoch 18 | loss: 0.21417 | val_0_rmse: 0.44762 | val_1_rmse: 0.46144 |  0:01:03s
epoch 19 | loss: 0.21187 | val_0_rmse: 0.45235 | val_1_rmse: 0.46853 |  0:01:07s
epoch 20 | loss: 0.20933 | val_0_rmse: 0.45322 | val_1_rmse: 0.46673 |  0:01:10s
epoch 21 | loss: 0.20758 | val_0_rmse: 0.44326 | val_1_rmse: 0.4594  |  0:01:13s
epoch 22 | loss: 0.20514 | val_0_rmse: 0.43709 | val_1_rmse: 0.44907 |  0:01:17s
epoch 23 | loss: 0.20345 | val_0_rmse: 0.43459 | val_1_rmse: 0.44827 |  0:01:20s
epoch 24 | loss: 0.20459 | val_0_rmse: 0.43682 | val_1_rmse: 0.45168 |  0:01:23s
epoch 25 | loss: 0.20559 | val_0_rmse: 0.43122 | val_1_rmse: 0.44516 |  0:01:27s
epoch 26 | loss: 0.20323 | val_0_rmse: 0.43165 | val_1_rmse: 0.44796 |  0:01:30s
epoch 27 | loss: 0.20137 | val_0_rmse: 0.43538 | val_1_rmse: 0.45281 |  0:01:34s
epoch 28 | loss: 0.20018 | val_0_rmse: 0.4297  | val_1_rmse: 0.44412 |  0:01:37s
epoch 29 | loss: 0.20173 | val_0_rmse: 0.44146 | val_1_rmse: 0.45626 |  0:01:40s
epoch 30 | loss: 0.199   | val_0_rmse: 0.42996 | val_1_rmse: 0.44691 |  0:01:44s
epoch 31 | loss: 0.19923 | val_0_rmse: 0.4284  | val_1_rmse: 0.44632 |  0:01:47s
epoch 32 | loss: 0.19855 | val_0_rmse: 0.45313 | val_1_rmse: 0.47402 |  0:01:50s
epoch 33 | loss: 0.19972 | val_0_rmse: 0.43136 | val_1_rmse: 0.44952 |  0:01:54s
epoch 34 | loss: 0.19897 | val_0_rmse: 0.43049 | val_1_rmse: 0.44795 |  0:01:57s
epoch 35 | loss: 0.19469 | val_0_rmse: 0.43756 | val_1_rmse: 0.45552 |  0:02:00s
epoch 36 | loss: 0.19787 | val_0_rmse: 0.43299 | val_1_rmse: 0.45161 |  0:02:04s
epoch 37 | loss: 0.19517 | val_0_rmse: 0.44047 | val_1_rmse: 0.45711 |  0:02:07s
epoch 38 | loss: 0.19538 | val_0_rmse: 0.42434 | val_1_rmse: 0.44187 |  0:02:10s
epoch 39 | loss: 0.19382 | val_0_rmse: 0.42186 | val_1_rmse: 0.44101 |  0:02:14s
epoch 40 | loss: 0.19412 | val_0_rmse: 0.46168 | val_1_rmse: 0.48161 |  0:02:17s
epoch 41 | loss: 0.20034 | val_0_rmse: 0.42454 | val_1_rmse: 0.44314 |  0:02:20s
epoch 42 | loss: 0.19473 | val_0_rmse: 0.41633 | val_1_rmse: 0.43522 |  0:02:24s
epoch 43 | loss: 0.19374 | val_0_rmse: 0.43198 | val_1_rmse: 0.44945 |  0:02:27s
epoch 44 | loss: 0.19379 | val_0_rmse: 0.42549 | val_1_rmse: 0.44275 |  0:02:31s
epoch 45 | loss: 0.19395 | val_0_rmse: 0.42082 | val_1_rmse: 0.44195 |  0:02:34s
epoch 46 | loss: 0.18883 | val_0_rmse: 0.43602 | val_1_rmse: 0.45706 |  0:02:37s
epoch 47 | loss: 0.18983 | val_0_rmse: 0.41396 | val_1_rmse: 0.43494 |  0:02:41s
epoch 48 | loss: 0.19162 | val_0_rmse: 0.41568 | val_1_rmse: 0.43627 |  0:02:44s
epoch 49 | loss: 0.18704 | val_0_rmse: 0.42477 | val_1_rmse: 0.44898 |  0:02:47s
epoch 50 | loss: 0.18668 | val_0_rmse: 0.43033 | val_1_rmse: 0.452   |  0:02:51s
epoch 51 | loss: 0.19193 | val_0_rmse: 0.41348 | val_1_rmse: 0.43213 |  0:02:54s
epoch 52 | loss: 0.18754 | val_0_rmse: 0.41222 | val_1_rmse: 0.43465 |  0:02:57s
epoch 53 | loss: 0.18964 | val_0_rmse: 0.42927 | val_1_rmse: 0.4504  |  0:03:01s
epoch 54 | loss: 0.18994 | val_0_rmse: 0.41019 | val_1_rmse: 0.43454 |  0:03:04s
epoch 55 | loss: 0.18627 | val_0_rmse: 0.43163 | val_1_rmse: 0.45491 |  0:03:08s
epoch 56 | loss: 0.18585 | val_0_rmse: 0.41716 | val_1_rmse: 0.44119 |  0:03:11s
epoch 57 | loss: 0.1833  | val_0_rmse: 0.41196 | val_1_rmse: 0.43548 |  0:03:14s
epoch 58 | loss: 0.18457 | val_0_rmse: 0.41937 | val_1_rmse: 0.44056 |  0:03:18s
epoch 59 | loss: 0.1843  | val_0_rmse: 0.42266 | val_1_rmse: 0.44634 |  0:03:21s
epoch 60 | loss: 0.1865  | val_0_rmse: 0.41285 | val_1_rmse: 0.4353  |  0:03:24s
epoch 61 | loss: 0.18417 | val_0_rmse: 0.41267 | val_1_rmse: 0.43744 |  0:03:28s
epoch 62 | loss: 0.18389 | val_0_rmse: 0.40985 | val_1_rmse: 0.43448 |  0:03:31s
epoch 63 | loss: 0.1819  | val_0_rmse: 0.41162 | val_1_rmse: 0.43554 |  0:03:34s
epoch 64 | loss: 0.18392 | val_0_rmse: 0.42267 | val_1_rmse: 0.44933 |  0:03:38s
epoch 65 | loss: 0.18452 | val_0_rmse: 0.44101 | val_1_rmse: 0.46017 |  0:03:41s
epoch 66 | loss: 0.18308 | val_0_rmse: 0.41119 | val_1_rmse: 0.43728 |  0:03:44s
epoch 67 | loss: 0.18381 | val_0_rmse: 0.41851 | val_1_rmse: 0.44329 |  0:03:48s
epoch 68 | loss: 0.18403 | val_0_rmse: 0.40819 | val_1_rmse: 0.43375 |  0:03:51s
epoch 69 | loss: 0.18504 | val_0_rmse: 0.40725 | val_1_rmse: 0.43393 |  0:03:54s
epoch 70 | loss: 0.18089 | val_0_rmse: 0.4097  | val_1_rmse: 0.4344  |  0:03:58s
epoch 71 | loss: 0.18017 | val_0_rmse: 0.40838 | val_1_rmse: 0.43485 |  0:04:01s
epoch 72 | loss: 0.18047 | val_0_rmse: 0.40192 | val_1_rmse: 0.42919 |  0:04:05s
epoch 73 | loss: 0.18042 | val_0_rmse: 0.40873 | val_1_rmse: 0.43621 |  0:04:08s
epoch 74 | loss: 0.17946 | val_0_rmse: 0.41369 | val_1_rmse: 0.4406  |  0:04:11s
epoch 75 | loss: 0.17809 | val_0_rmse: 0.40177 | val_1_rmse: 0.43168 |  0:04:15s
epoch 76 | loss: 0.18176 | val_0_rmse: 0.41236 | val_1_rmse: 0.4383  |  0:04:18s
epoch 77 | loss: 0.17772 | val_0_rmse: 0.41597 | val_1_rmse: 0.44586 |  0:04:21s
epoch 78 | loss: 0.179   | val_0_rmse: 0.40278 | val_1_rmse: 0.43274 |  0:04:25s
epoch 79 | loss: 0.17886 | val_0_rmse: 0.4099  | val_1_rmse: 0.43483 |  0:04:28s
epoch 80 | loss: 0.17895 | val_0_rmse: 0.41693 | val_1_rmse: 0.44451 |  0:04:31s
epoch 81 | loss: 0.17817 | val_0_rmse: 0.41464 | val_1_rmse: 0.44475 |  0:04:35s
epoch 82 | loss: 0.17993 | val_0_rmse: 0.40123 | val_1_rmse: 0.43284 |  0:04:38s
epoch 83 | loss: 0.17693 | val_0_rmse: 0.39926 | val_1_rmse: 0.42874 |  0:04:42s
epoch 84 | loss: 0.17777 | val_0_rmse: 0.40389 | val_1_rmse: 0.43307 |  0:04:45s
epoch 85 | loss: 0.17708 | val_0_rmse: 0.39703 | val_1_rmse: 0.42726 |  0:04:48s
epoch 86 | loss: 0.17916 | val_0_rmse: 0.41009 | val_1_rmse: 0.44164 |  0:04:52s
epoch 87 | loss: 0.17578 | val_0_rmse: 0.41473 | val_1_rmse: 0.44337 |  0:04:55s
epoch 88 | loss: 0.1767  | val_0_rmse: 0.41009 | val_1_rmse: 0.43956 |  0:04:58s
epoch 89 | loss: 0.17548 | val_0_rmse: 0.4051  | val_1_rmse: 0.43637 |  0:05:02s
epoch 90 | loss: 0.18108 | val_0_rmse: 0.40771 | val_1_rmse: 0.43783 |  0:05:05s
epoch 91 | loss: 0.17344 | val_0_rmse: 0.40498 | val_1_rmse: 0.43891 |  0:05:08s
epoch 92 | loss: 0.18125 | val_0_rmse: 0.44176 | val_1_rmse: 0.46531 |  0:05:12s
epoch 93 | loss: 0.17956 | val_0_rmse: 0.40644 | val_1_rmse: 0.43998 |  0:05:15s
epoch 94 | loss: 0.17799 | val_0_rmse: 0.40391 | val_1_rmse: 0.4374  |  0:05:19s
epoch 95 | loss: 0.173   | val_0_rmse: 0.41343 | val_1_rmse: 0.44888 |  0:05:22s
epoch 96 | loss: 0.17591 | val_0_rmse: 0.41141 | val_1_rmse: 0.44506 |  0:05:25s
epoch 97 | loss: 0.17472 | val_0_rmse: 0.39626 | val_1_rmse: 0.42898 |  0:05:29s
epoch 98 | loss: 0.17438 | val_0_rmse: 0.39771 | val_1_rmse: 0.43243 |  0:05:32s
epoch 99 | loss: 0.17493 | val_0_rmse: 0.3969  | val_1_rmse: 0.434   |  0:05:35s
epoch 100| loss: 0.17127 | val_0_rmse: 0.39305 | val_1_rmse: 0.43129 |  0:05:39s
epoch 101| loss: 0.17516 | val_0_rmse: 0.39824 | val_1_rmse: 0.43308 |  0:05:42s
epoch 102| loss: 0.17627 | val_0_rmse: 0.41162 | val_1_rmse: 0.44361 |  0:05:45s
epoch 103| loss: 0.17478 | val_0_rmse: 0.40214 | val_1_rmse: 0.43966 |  0:05:49s
epoch 104| loss: 0.17376 | val_0_rmse: 0.39473 | val_1_rmse: 0.43053 |  0:05:52s
epoch 105| loss: 0.17168 | val_0_rmse: 0.39142 | val_1_rmse: 0.42864 |  0:05:55s
epoch 106| loss: 0.17568 | val_0_rmse: 0.40182 | val_1_rmse: 0.43911 |  0:05:59s
epoch 107| loss: 0.17373 | val_0_rmse: 0.39726 | val_1_rmse: 0.43162 |  0:06:02s
epoch 108| loss: 0.17261 | val_0_rmse: 0.42468 | val_1_rmse: 0.45762 |  0:06:05s
epoch 109| loss: 0.17713 | val_0_rmse: 0.40402 | val_1_rmse: 0.43678 |  0:06:09s
epoch 110| loss: 0.17352 | val_0_rmse: 0.39975 | val_1_rmse: 0.43285 |  0:06:12s
epoch 111| loss: 0.17647 | val_0_rmse: 0.3904  | val_1_rmse: 0.42739 |  0:06:16s
epoch 112| loss: 0.17221 | val_0_rmse: 0.40302 | val_1_rmse: 0.43931 |  0:06:19s
epoch 113| loss: 0.17407 | val_0_rmse: 0.40218 | val_1_rmse: 0.43747 |  0:06:22s
epoch 114| loss: 0.17216 | val_0_rmse: 0.3883  | val_1_rmse: 0.42707 |  0:06:26s
epoch 115| loss: 0.16918 | val_0_rmse: 0.39493 | val_1_rmse: 0.43718 |  0:06:29s
epoch 116| loss: 0.17074 | val_0_rmse: 0.39228 | val_1_rmse: 0.43457 |  0:06:32s
epoch 117| loss: 0.17361 | val_0_rmse: 0.39219 | val_1_rmse: 0.43183 |  0:06:36s
epoch 118| loss: 0.16983 | val_0_rmse: 0.41336 | val_1_rmse: 0.45287 |  0:06:39s
epoch 119| loss: 0.16918 | val_0_rmse: 0.39132 | val_1_rmse: 0.43179 |  0:06:42s
epoch 120| loss: 0.16922 | val_0_rmse: 0.39217 | val_1_rmse: 0.43225 |  0:06:46s
epoch 121| loss: 0.1717  | val_0_rmse: 0.40631 | val_1_rmse: 0.44174 |  0:06:49s
epoch 122| loss: 0.1757  | val_0_rmse: 0.40733 | val_1_rmse: 0.44341 |  0:06:52s
epoch 123| loss: 0.1743  | val_0_rmse: 0.38902 | val_1_rmse: 0.42956 |  0:06:56s
epoch 124| loss: 0.1702  | val_0_rmse: 0.3905  | val_1_rmse: 0.42895 |  0:06:59s
epoch 125| loss: 0.17022 | val_0_rmse: 0.38775 | val_1_rmse: 0.43015 |  0:07:02s
epoch 126| loss: 0.17045 | val_0_rmse: 0.39365 | val_1_rmse: 0.43266 |  0:07:06s
epoch 127| loss: 0.17161 | val_0_rmse: 0.39517 | val_1_rmse: 0.43852 |  0:07:09s
epoch 128| loss: 0.16891 | val_0_rmse: 0.38576 | val_1_rmse: 0.43112 |  0:07:12s
epoch 129| loss: 0.16889 | val_0_rmse: 0.38726 | val_1_rmse: 0.43047 |  0:07:16s
epoch 130| loss: 0.16867 | val_0_rmse: 0.38841 | val_1_rmse: 0.43274 |  0:07:19s
epoch 131| loss: 0.17112 | val_0_rmse: 0.39771 | val_1_rmse: 0.43838 |  0:07:23s
epoch 132| loss: 0.17041 | val_0_rmse: 0.398   | val_1_rmse: 0.43879 |  0:07:26s
epoch 133| loss: 0.1709  | val_0_rmse: 0.39004 | val_1_rmse: 0.43327 |  0:07:29s
epoch 134| loss: 0.16739 | val_0_rmse: 0.38733 | val_1_rmse: 0.43277 |  0:07:33s
epoch 135| loss: 0.16681 | val_0_rmse: 0.39105 | val_1_rmse: 0.43441 |  0:07:36s
epoch 136| loss: 0.16863 | val_0_rmse: 0.40712 | val_1_rmse: 0.45205 |  0:07:39s
epoch 137| loss: 0.17006 | val_0_rmse: 0.40306 | val_1_rmse: 0.44696 |  0:07:43s
epoch 138| loss: 0.16833 | val_0_rmse: 0.39291 | val_1_rmse: 0.43748 |  0:07:46s
epoch 139| loss: 0.16754 | val_0_rmse: 0.40566 | val_1_rmse: 0.44762 |  0:07:49s
epoch 140| loss: 0.16947 | val_0_rmse: 0.39315 | val_1_rmse: 0.43373 |  0:07:53s
epoch 141| loss: 0.16853 | val_0_rmse: 0.38828 | val_1_rmse: 0.43235 |  0:07:56s
epoch 142| loss: 0.16826 | val_0_rmse: 0.38641 | val_1_rmse: 0.43195 |  0:08:00s
epoch 143| loss: 0.16909 | val_0_rmse: 0.41631 | val_1_rmse: 0.45714 |  0:08:03s
epoch 144| loss: 0.16647 | val_0_rmse: 0.40701 | val_1_rmse: 0.44507 |  0:08:06s

Early stopping occured at epoch 144 with best_epoch = 114 and best_val_1_rmse = 0.42707
Best weights from best epoch are automatically used!
ended training at: 07:39:06
Feature importance:
Mean squared error is of 10113986169.617085
Mean absolute error:67663.82794786726
MAPE:0.24437768062361895
R2 score:0.8222088448055463
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:39:06
epoch 0  | loss: 0.61386 | val_0_rmse: 0.72067 | val_1_rmse: 0.71129 |  0:00:03s
epoch 1  | loss: 0.36223 | val_0_rmse: 0.71816 | val_1_rmse: 0.71019 |  0:00:06s
epoch 2  | loss: 0.31599 | val_0_rmse: 0.64963 | val_1_rmse: 0.64814 |  0:00:10s
epoch 3  | loss: 0.29467 | val_0_rmse: 0.61616 | val_1_rmse: 0.60973 |  0:00:13s
epoch 4  | loss: 0.2767  | val_0_rmse: 0.57408 | val_1_rmse: 0.5754  |  0:00:16s
epoch 5  | loss: 0.32073 | val_0_rmse: 0.55994 | val_1_rmse: 0.56118 |  0:00:20s
epoch 6  | loss: 0.27641 | val_0_rmse: 0.58306 | val_1_rmse: 0.58965 |  0:00:23s
epoch 7  | loss: 0.26455 | val_0_rmse: 0.53757 | val_1_rmse: 0.54199 |  0:00:26s
epoch 8  | loss: 0.2594  | val_0_rmse: 0.5563  | val_1_rmse: 0.56001 |  0:00:30s
epoch 9  | loss: 0.25385 | val_0_rmse: 0.49178 | val_1_rmse: 0.49113 |  0:00:33s
epoch 10 | loss: 0.24376 | val_0_rmse: 0.47741 | val_1_rmse: 0.47748 |  0:00:37s
epoch 11 | loss: 0.23915 | val_0_rmse: 0.4846  | val_1_rmse: 0.48672 |  0:00:40s
epoch 12 | loss: 0.24039 | val_0_rmse: 0.56629 | val_1_rmse: 0.56725 |  0:00:43s
epoch 13 | loss: 0.23523 | val_0_rmse: 0.46399 | val_1_rmse: 0.46681 |  0:00:47s
epoch 14 | loss: 0.2345  | val_0_rmse: 0.46706 | val_1_rmse: 0.47092 |  0:00:50s
epoch 15 | loss: 0.22924 | val_0_rmse: 0.46279 | val_1_rmse: 0.46722 |  0:00:53s
epoch 16 | loss: 0.22816 | val_0_rmse: 0.46546 | val_1_rmse: 0.4697  |  0:00:57s
epoch 17 | loss: 0.22755 | val_0_rmse: 0.49402 | val_1_rmse: 0.49402 |  0:01:00s
epoch 18 | loss: 0.22617 | val_0_rmse: 0.49005 | val_1_rmse: 0.49471 |  0:01:03s
epoch 19 | loss: 0.22276 | val_0_rmse: 0.4602  | val_1_rmse: 0.4645  |  0:01:07s
epoch 20 | loss: 0.22194 | val_0_rmse: 0.46764 | val_1_rmse: 0.47208 |  0:01:10s
epoch 21 | loss: 0.22324 | val_0_rmse: 0.45725 | val_1_rmse: 0.46227 |  0:01:13s
epoch 22 | loss: 0.22212 | val_0_rmse: 0.4544  | val_1_rmse: 0.4582  |  0:01:17s
epoch 23 | loss: 0.22068 | val_0_rmse: 0.46328 | val_1_rmse: 0.46555 |  0:01:20s
epoch 24 | loss: 0.22004 | val_0_rmse: 0.46528 | val_1_rmse: 0.47166 |  0:01:24s
epoch 25 | loss: 0.22623 | val_0_rmse: 0.52307 | val_1_rmse: 0.52371 |  0:01:27s
epoch 26 | loss: 0.22727 | val_0_rmse: 0.45282 | val_1_rmse: 0.45834 |  0:01:30s
epoch 27 | loss: 0.21807 | val_0_rmse: 0.45929 | val_1_rmse: 0.46063 |  0:01:34s
epoch 28 | loss: 0.21704 | val_0_rmse: 0.47556 | val_1_rmse: 0.48171 |  0:01:37s
epoch 29 | loss: 0.21441 | val_0_rmse: 0.4508  | val_1_rmse: 0.45766 |  0:01:40s
epoch 30 | loss: 0.21356 | val_0_rmse: 0.45828 | val_1_rmse: 0.46492 |  0:01:44s
epoch 31 | loss: 0.21511 | val_0_rmse: 0.48409 | val_1_rmse: 0.49126 |  0:01:47s
epoch 32 | loss: 0.21654 | val_0_rmse: 0.45231 | val_1_rmse: 0.45744 |  0:01:50s
epoch 33 | loss: 0.2145  | val_0_rmse: 0.45835 | val_1_rmse: 0.46406 |  0:01:54s
epoch 34 | loss: 0.20936 | val_0_rmse: 0.45502 | val_1_rmse: 0.46275 |  0:01:57s
epoch 35 | loss: 0.21129 | val_0_rmse: 0.44083 | val_1_rmse: 0.44939 |  0:02:00s
epoch 36 | loss: 0.21145 | val_0_rmse: 0.44436 | val_1_rmse: 0.45075 |  0:02:04s
epoch 37 | loss: 0.20957 | val_0_rmse: 0.44671 | val_1_rmse: 0.45196 |  0:02:07s
epoch 38 | loss: 0.2117  | val_0_rmse: 0.44618 | val_1_rmse: 0.45385 |  0:02:10s
epoch 39 | loss: 0.20912 | val_0_rmse: 0.43785 | val_1_rmse: 0.44575 |  0:02:14s
epoch 40 | loss: 0.20871 | val_0_rmse: 0.46736 | val_1_rmse: 0.47536 |  0:02:17s
epoch 41 | loss: 0.20875 | val_0_rmse: 0.44761 | val_1_rmse: 0.45597 |  0:02:20s
epoch 42 | loss: 0.20569 | val_0_rmse: 0.4527  | val_1_rmse: 0.46083 |  0:02:24s
epoch 43 | loss: 0.20785 | val_0_rmse: 0.44658 | val_1_rmse: 0.45308 |  0:02:27s
epoch 44 | loss: 0.2154  | val_0_rmse: 0.45491 | val_1_rmse: 0.46172 |  0:02:31s
epoch 45 | loss: 0.22743 | val_0_rmse: 0.47993 | val_1_rmse: 0.49012 |  0:02:34s
epoch 46 | loss: 0.21478 | val_0_rmse: 0.4995  | val_1_rmse: 0.50612 |  0:02:37s
epoch 47 | loss: 0.24173 | val_0_rmse: 0.46671 | val_1_rmse: 0.47355 |  0:02:41s
epoch 48 | loss: 0.22193 | val_0_rmse: 0.48695 | val_1_rmse: 0.49341 |  0:02:44s
epoch 49 | loss: 0.21986 | val_0_rmse: 0.47024 | val_1_rmse: 0.47481 |  0:02:47s
epoch 50 | loss: 0.21842 | val_0_rmse: 0.45586 | val_1_rmse: 0.46136 |  0:02:51s
epoch 51 | loss: 0.2154  | val_0_rmse: 0.46461 | val_1_rmse: 0.46946 |  0:02:54s
epoch 52 | loss: 0.2148  | val_0_rmse: 0.46935 | val_1_rmse: 0.47675 |  0:02:58s
epoch 53 | loss: 0.21304 | val_0_rmse: 0.44964 | val_1_rmse: 0.45615 |  0:03:01s
epoch 54 | loss: 0.20998 | val_0_rmse: 0.45054 | val_1_rmse: 0.45778 |  0:03:04s
epoch 55 | loss: 0.2111  | val_0_rmse: 0.45928 | val_1_rmse: 0.46383 |  0:03:08s
epoch 56 | loss: 0.20935 | val_0_rmse: 0.44816 | val_1_rmse: 0.45576 |  0:03:11s
epoch 57 | loss: 0.23275 | val_0_rmse: 0.45791 | val_1_rmse: 0.46186 |  0:03:14s
epoch 58 | loss: 0.2113  | val_0_rmse: 0.44026 | val_1_rmse: 0.44502 |  0:03:18s
epoch 59 | loss: 0.21222 | val_0_rmse: 0.445   | val_1_rmse: 0.45187 |  0:03:21s
epoch 60 | loss: 0.20425 | val_0_rmse: 0.44635 | val_1_rmse: 0.45603 |  0:03:24s
epoch 61 | loss: 0.19996 | val_0_rmse: 0.43883 | val_1_rmse: 0.44753 |  0:03:28s
epoch 62 | loss: 0.19994 | val_0_rmse: 0.42762 | val_1_rmse: 0.43805 |  0:03:31s
epoch 63 | loss: 0.19768 | val_0_rmse: 0.43948 | val_1_rmse: 0.44667 |  0:03:34s
epoch 64 | loss: 0.19893 | val_0_rmse: 0.42217 | val_1_rmse: 0.43402 |  0:03:38s
epoch 65 | loss: 0.1966  | val_0_rmse: 0.43647 | val_1_rmse: 0.44657 |  0:03:41s
epoch 66 | loss: 0.19693 | val_0_rmse: 0.43061 | val_1_rmse: 0.44246 |  0:03:44s
epoch 67 | loss: 0.19256 | val_0_rmse: 0.45805 | val_1_rmse: 0.46857 |  0:03:48s
epoch 68 | loss: 0.19727 | val_0_rmse: 0.4258  | val_1_rmse: 0.43628 |  0:03:51s
epoch 69 | loss: 0.19631 | val_0_rmse: 0.43894 | val_1_rmse: 0.44788 |  0:03:55s
epoch 70 | loss: 0.1955  | val_0_rmse: 0.43107 | val_1_rmse: 0.4419  |  0:03:58s
epoch 71 | loss: 0.19112 | val_0_rmse: 0.46201 | val_1_rmse: 0.47191 |  0:04:01s
epoch 72 | loss: 0.18924 | val_0_rmse: 0.47129 | val_1_rmse: 0.48532 |  0:04:05s
epoch 73 | loss: 0.20819 | val_0_rmse: 0.46115 | val_1_rmse: 0.47028 |  0:04:08s
epoch 74 | loss: 0.2041  | val_0_rmse: 0.4315  | val_1_rmse: 0.44195 |  0:04:11s
epoch 75 | loss: 0.19481 | val_0_rmse: 0.42828 | val_1_rmse: 0.43872 |  0:04:15s
epoch 76 | loss: 0.19265 | val_0_rmse: 0.42355 | val_1_rmse: 0.43504 |  0:04:18s
epoch 77 | loss: 0.1914  | val_0_rmse: 0.41991 | val_1_rmse: 0.42977 |  0:04:21s
epoch 78 | loss: 0.19119 | val_0_rmse: 0.46727 | val_1_rmse: 0.47849 |  0:04:25s
epoch 79 | loss: 0.19394 | val_0_rmse: 0.41977 | val_1_rmse: 0.43124 |  0:04:28s
epoch 80 | loss: 0.19193 | val_0_rmse: 0.42647 | val_1_rmse: 0.43697 |  0:04:31s
epoch 81 | loss: 0.18875 | val_0_rmse: 0.4335  | val_1_rmse: 0.44748 |  0:04:35s
epoch 82 | loss: 0.18585 | val_0_rmse: 0.42033 | val_1_rmse: 0.43265 |  0:04:38s
epoch 83 | loss: 0.18838 | val_0_rmse: 0.41497 | val_1_rmse: 0.42965 |  0:04:41s
epoch 84 | loss: 0.18739 | val_0_rmse: 0.42242 | val_1_rmse: 0.43474 |  0:04:45s
epoch 85 | loss: 0.18387 | val_0_rmse: 0.4562  | val_1_rmse: 0.46844 |  0:04:48s
epoch 86 | loss: 0.18696 | val_0_rmse: 0.43406 | val_1_rmse: 0.44796 |  0:04:52s
epoch 87 | loss: 0.18741 | val_0_rmse: 0.41292 | val_1_rmse: 0.42851 |  0:04:55s
epoch 88 | loss: 0.18768 | val_0_rmse: 0.42735 | val_1_rmse: 0.44151 |  0:04:58s
epoch 89 | loss: 0.18567 | val_0_rmse: 0.41519 | val_1_rmse: 0.43065 |  0:05:02s
epoch 90 | loss: 0.18282 | val_0_rmse: 0.42875 | val_1_rmse: 0.44213 |  0:05:05s
epoch 91 | loss: 0.18176 | val_0_rmse: 0.41363 | val_1_rmse: 0.42932 |  0:05:08s
epoch 92 | loss: 0.18412 | val_0_rmse: 0.46167 | val_1_rmse: 0.47128 |  0:05:12s
epoch 93 | loss: 0.18854 | val_0_rmse: 0.48881 | val_1_rmse: 0.50431 |  0:05:15s
epoch 94 | loss: 0.18384 | val_0_rmse: 0.40908 | val_1_rmse: 0.42398 |  0:05:18s
epoch 95 | loss: 0.18064 | val_0_rmse: 0.41551 | val_1_rmse: 0.43283 |  0:05:22s
epoch 96 | loss: 0.18216 | val_0_rmse: 0.41972 | val_1_rmse: 0.43757 |  0:05:25s
epoch 97 | loss: 0.18434 | val_0_rmse: 0.41844 | val_1_rmse: 0.43576 |  0:05:28s
epoch 98 | loss: 0.18187 | val_0_rmse: 0.41455 | val_1_rmse: 0.43036 |  0:05:32s
epoch 99 | loss: 0.19979 | val_0_rmse: 0.45129 | val_1_rmse: 0.46134 |  0:05:35s
epoch 100| loss: 0.21534 | val_0_rmse: 0.61179 | val_1_rmse: 0.61892 |  0:05:39s
epoch 101| loss: 0.23859 | val_0_rmse: 0.49771 | val_1_rmse: 0.50007 |  0:05:42s
epoch 102| loss: 0.29188 | val_0_rmse: 0.51939 | val_1_rmse: 0.51583 |  0:05:45s
epoch 103| loss: 0.25323 | val_0_rmse: 0.4766  | val_1_rmse: 0.4801  |  0:05:49s
epoch 104| loss: 0.23533 | val_0_rmse: 0.46954 | val_1_rmse: 0.47685 |  0:05:52s
epoch 105| loss: 0.2242  | val_0_rmse: 0.46758 | val_1_rmse: 0.47249 |  0:05:55s
epoch 106| loss: 0.21457 | val_0_rmse: 0.46001 | val_1_rmse: 0.46331 |  0:05:59s
epoch 107| loss: 0.20821 | val_0_rmse: 0.44168 | val_1_rmse: 0.45326 |  0:06:02s
epoch 108| loss: 0.20781 | val_0_rmse: 0.48893 | val_1_rmse: 0.50028 |  0:06:05s
epoch 109| loss: 0.20646 | val_0_rmse: 0.46678 | val_1_rmse: 0.47769 |  0:06:09s
epoch 110| loss: 0.20809 | val_0_rmse: 0.47886 | val_1_rmse: 0.49183 |  0:06:12s
epoch 111| loss: 0.19877 | val_0_rmse: 0.47819 | val_1_rmse: 0.48846 |  0:06:15s
epoch 112| loss: 0.2223  | val_0_rmse: 0.45114 | val_1_rmse: 0.45843 |  0:06:19s
epoch 113| loss: 0.2049  | val_0_rmse: 0.43823 | val_1_rmse: 0.44734 |  0:06:22s
epoch 114| loss: 0.2041  | val_0_rmse: 0.47976 | val_1_rmse: 0.48664 |  0:06:25s
epoch 115| loss: 0.22377 | val_0_rmse: 0.47105 | val_1_rmse: 0.47936 |  0:06:29s
epoch 116| loss: 0.2036  | val_0_rmse: 0.44061 | val_1_rmse: 0.4481  |  0:06:32s
epoch 117| loss: 0.20589 | val_0_rmse: 0.43498 | val_1_rmse: 0.44437 |  0:06:36s
epoch 118| loss: 0.19824 | val_0_rmse: 0.44447 | val_1_rmse: 0.45696 |  0:06:39s
epoch 119| loss: 0.19462 | val_0_rmse: 0.43842 | val_1_rmse: 0.44992 |  0:06:42s
epoch 120| loss: 0.19018 | val_0_rmse: 0.42662 | val_1_rmse: 0.43817 |  0:06:46s
epoch 121| loss: 0.19034 | val_0_rmse: 0.41664 | val_1_rmse: 0.42897 |  0:06:49s
epoch 122| loss: 0.19164 | val_0_rmse: 0.41872 | val_1_rmse: 0.43289 |  0:06:52s
epoch 123| loss: 0.19308 | val_0_rmse: 0.4536  | val_1_rmse: 0.46345 |  0:06:56s
epoch 124| loss: 0.19912 | val_0_rmse: 0.52298 | val_1_rmse: 0.52952 |  0:06:59s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.42398
Best weights from best epoch are automatically used!
ended training at: 07:46:07
Feature importance:
Mean squared error is of 10385136538.248487
Mean absolute error:69329.194625563
MAPE:0.2481993444735381
R2 score:0.8181387099618571
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:46:09
epoch 0  | loss: 1.19086 | val_0_rmse: 0.76004 | val_1_rmse: 0.7494  |  0:00:01s
epoch 1  | loss: 0.53028 | val_0_rmse: 0.62278 | val_1_rmse: 0.62262 |  0:00:02s
epoch 2  | loss: 0.35476 | val_0_rmse: 0.56082 | val_1_rmse: 0.57627 |  0:00:03s
epoch 3  | loss: 0.32249 | val_0_rmse: 0.51858 | val_1_rmse: 0.53296 |  0:00:04s
epoch 4  | loss: 0.30491 | val_0_rmse: 0.51797 | val_1_rmse: 0.53187 |  0:00:05s
epoch 5  | loss: 0.32615 | val_0_rmse: 0.52577 | val_1_rmse: 0.5369  |  0:00:06s
epoch 6  | loss: 0.32288 | val_0_rmse: 0.52368 | val_1_rmse: 0.53455 |  0:00:07s
epoch 7  | loss: 0.29012 | val_0_rmse: 0.53697 | val_1_rmse: 0.5477  |  0:00:08s
epoch 8  | loss: 0.28426 | val_0_rmse: 0.53335 | val_1_rmse: 0.54475 |  0:00:10s
epoch 9  | loss: 0.27576 | val_0_rmse: 0.51094 | val_1_rmse: 0.52441 |  0:00:11s
epoch 10 | loss: 0.27176 | val_0_rmse: 0.52304 | val_1_rmse: 0.54146 |  0:00:12s
epoch 11 | loss: 0.26808 | val_0_rmse: 0.5194  | val_1_rmse: 0.53601 |  0:00:13s
epoch 12 | loss: 0.26108 | val_0_rmse: 0.50245 | val_1_rmse: 0.5181  |  0:00:14s
epoch 13 | loss: 0.25695 | val_0_rmse: 0.50474 | val_1_rmse: 0.52172 |  0:00:15s
epoch 14 | loss: 0.25089 | val_0_rmse: 0.49758 | val_1_rmse: 0.51952 |  0:00:16s
epoch 15 | loss: 0.24151 | val_0_rmse: 0.50589 | val_1_rmse: 0.51929 |  0:00:17s
epoch 16 | loss: 0.24176 | val_0_rmse: 0.49584 | val_1_rmse: 0.51351 |  0:00:18s
epoch 17 | loss: 0.23716 | val_0_rmse: 0.48867 | val_1_rmse: 0.50807 |  0:00:20s
epoch 18 | loss: 0.23301 | val_0_rmse: 0.50487 | val_1_rmse: 0.51593 |  0:00:21s
epoch 19 | loss: 0.2344  | val_0_rmse: 0.50495 | val_1_rmse: 0.52004 |  0:00:22s
epoch 20 | loss: 0.25263 | val_0_rmse: 0.52498 | val_1_rmse: 0.54112 |  0:00:23s
epoch 21 | loss: 0.25335 | val_0_rmse: 0.49789 | val_1_rmse: 0.50937 |  0:00:24s
epoch 22 | loss: 0.24323 | val_0_rmse: 0.49131 | val_1_rmse: 0.50695 |  0:00:25s
epoch 23 | loss: 0.23755 | val_0_rmse: 0.48098 | val_1_rmse: 0.4937  |  0:00:26s
epoch 24 | loss: 0.23555 | val_0_rmse: 0.48935 | val_1_rmse: 0.50567 |  0:00:27s
epoch 25 | loss: 0.23481 | val_0_rmse: 0.48102 | val_1_rmse: 0.49413 |  0:00:28s
epoch 26 | loss: 0.24028 | val_0_rmse: 0.47932 | val_1_rmse: 0.49509 |  0:00:30s
epoch 27 | loss: 0.23347 | val_0_rmse: 0.46615 | val_1_rmse: 0.48315 |  0:00:31s
epoch 28 | loss: 0.25387 | val_0_rmse: 0.51785 | val_1_rmse: 0.52723 |  0:00:32s
epoch 29 | loss: 0.26427 | val_0_rmse: 0.50412 | val_1_rmse: 0.51623 |  0:00:33s
epoch 30 | loss: 0.2608  | val_0_rmse: 0.49321 | val_1_rmse: 0.50844 |  0:00:34s
epoch 31 | loss: 0.2518  | val_0_rmse: 0.49725 | val_1_rmse: 0.51307 |  0:00:35s
epoch 32 | loss: 0.26459 | val_0_rmse: 0.50029 | val_1_rmse: 0.51214 |  0:00:36s
epoch 33 | loss: 0.26547 | val_0_rmse: 0.53218 | val_1_rmse: 0.539   |  0:00:37s
epoch 34 | loss: 0.25676 | val_0_rmse: 0.48668 | val_1_rmse: 0.50776 |  0:00:38s
epoch 35 | loss: 0.24741 | val_0_rmse: 0.47991 | val_1_rmse: 0.49352 |  0:00:40s
epoch 36 | loss: 0.25329 | val_0_rmse: 0.50377 | val_1_rmse: 0.52112 |  0:00:41s
epoch 37 | loss: 0.26208 | val_0_rmse: 0.53306 | val_1_rmse: 0.5382  |  0:00:42s
epoch 38 | loss: 0.2702  | val_0_rmse: 0.51607 | val_1_rmse: 0.53251 |  0:00:43s
epoch 39 | loss: 0.2658  | val_0_rmse: 0.50862 | val_1_rmse: 0.51995 |  0:00:44s
epoch 40 | loss: 0.2692  | val_0_rmse: 0.50265 | val_1_rmse: 0.52144 |  0:00:45s
epoch 41 | loss: 0.26494 | val_0_rmse: 0.49348 | val_1_rmse: 0.50887 |  0:00:46s
epoch 42 | loss: 0.26661 | val_0_rmse: 0.52683 | val_1_rmse: 0.54044 |  0:00:47s
epoch 43 | loss: 0.27125 | val_0_rmse: 0.53904 | val_1_rmse: 0.55305 |  0:00:48s
epoch 44 | loss: 0.2643  | val_0_rmse: 0.50353 | val_1_rmse: 0.51495 |  0:00:50s
epoch 45 | loss: 0.2596  | val_0_rmse: 0.48953 | val_1_rmse: 0.50382 |  0:00:51s
epoch 46 | loss: 0.26209 | val_0_rmse: 0.52714 | val_1_rmse: 0.54533 |  0:00:52s
epoch 47 | loss: 0.27632 | val_0_rmse: 0.50399 | val_1_rmse: 0.51266 |  0:00:53s
epoch 48 | loss: 0.26787 | val_0_rmse: 0.50751 | val_1_rmse: 0.51459 |  0:00:54s
epoch 49 | loss: 0.26864 | val_0_rmse: 0.49663 | val_1_rmse: 0.51384 |  0:00:55s
epoch 50 | loss: 0.2519  | val_0_rmse: 0.48339 | val_1_rmse: 0.49679 |  0:00:56s
epoch 51 | loss: 0.24803 | val_0_rmse: 0.48562 | val_1_rmse: 0.50157 |  0:00:57s
epoch 52 | loss: 0.24748 | val_0_rmse: 0.48387 | val_1_rmse: 0.49753 |  0:00:58s
epoch 53 | loss: 0.24678 | val_0_rmse: 0.47584 | val_1_rmse: 0.48914 |  0:01:00s
epoch 54 | loss: 0.25106 | val_0_rmse: 0.48349 | val_1_rmse: 0.50197 |  0:01:01s
epoch 55 | loss: 0.25005 | val_0_rmse: 0.47994 | val_1_rmse: 0.4952  |  0:01:02s
epoch 56 | loss: 0.24275 | val_0_rmse: 0.4807  | val_1_rmse: 0.49697 |  0:01:03s
epoch 57 | loss: 0.2415  | val_0_rmse: 0.48588 | val_1_rmse: 0.49928 |  0:01:04s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.48315
Best weights from best epoch are automatically used!
ended training at: 07:47:14
Feature importance:
Mean squared error is of 7192839569.801304
Mean absolute error:59431.42543105085
MAPE:0.1529258824280144
R2 score:0.7706096040236509
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:47:15
epoch 0  | loss: 1.2776  | val_0_rmse: 0.9143  | val_1_rmse: 0.93203 |  0:00:01s
epoch 1  | loss: 0.47946 | val_0_rmse: 0.60051 | val_1_rmse: 0.58942 |  0:00:02s
epoch 2  | loss: 0.34683 | val_0_rmse: 0.59449 | val_1_rmse: 0.59631 |  0:00:03s
epoch 3  | loss: 0.33348 | val_0_rmse: 0.55371 | val_1_rmse: 0.54135 |  0:00:04s
epoch 4  | loss: 0.31537 | val_0_rmse: 0.53693 | val_1_rmse: 0.5279  |  0:00:05s
epoch 5  | loss: 0.30507 | val_0_rmse: 0.52707 | val_1_rmse: 0.52385 |  0:00:06s
epoch 6  | loss: 0.30069 | val_0_rmse: 0.53196 | val_1_rmse: 0.52798 |  0:00:07s
epoch 7  | loss: 0.29984 | val_0_rmse: 0.51693 | val_1_rmse: 0.51314 |  0:00:08s
epoch 8  | loss: 0.28464 | val_0_rmse: 0.51855 | val_1_rmse: 0.5158  |  0:00:09s
epoch 9  | loss: 0.28844 | val_0_rmse: 0.52209 | val_1_rmse: 0.51862 |  0:00:11s
epoch 10 | loss: 0.29259 | val_0_rmse: 0.52717 | val_1_rmse: 0.52144 |  0:00:12s
epoch 11 | loss: 0.28898 | val_0_rmse: 0.52896 | val_1_rmse: 0.52283 |  0:00:13s
epoch 12 | loss: 0.28299 | val_0_rmse: 0.52214 | val_1_rmse: 0.51899 |  0:00:14s
epoch 13 | loss: 0.27321 | val_0_rmse: 0.54413 | val_1_rmse: 0.53732 |  0:00:15s
epoch 14 | loss: 0.27709 | val_0_rmse: 0.51235 | val_1_rmse: 0.50926 |  0:00:16s
epoch 15 | loss: 0.26992 | val_0_rmse: 0.52379 | val_1_rmse: 0.5204  |  0:00:17s
epoch 16 | loss: 0.26693 | val_0_rmse: 0.51723 | val_1_rmse: 0.51187 |  0:00:18s
epoch 17 | loss: 0.27104 | val_0_rmse: 0.51125 | val_1_rmse: 0.50907 |  0:00:20s
epoch 18 | loss: 0.26926 | val_0_rmse: 0.51742 | val_1_rmse: 0.51332 |  0:00:21s
epoch 19 | loss: 0.26228 | val_0_rmse: 0.51809 | val_1_rmse: 0.51467 |  0:00:22s
epoch 20 | loss: 0.25644 | val_0_rmse: 0.58113 | val_1_rmse: 0.58185 |  0:00:23s
epoch 21 | loss: 0.25867 | val_0_rmse: 0.50227 | val_1_rmse: 0.49814 |  0:00:24s
epoch 22 | loss: 0.25848 | val_0_rmse: 0.49248 | val_1_rmse: 0.49078 |  0:00:25s
epoch 23 | loss: 0.23713 | val_0_rmse: 0.49068 | val_1_rmse: 0.48956 |  0:00:26s
epoch 24 | loss: 0.24837 | val_0_rmse: 0.49857 | val_1_rmse: 0.49762 |  0:00:27s
epoch 25 | loss: 0.25345 | val_0_rmse: 0.50647 | val_1_rmse: 0.50085 |  0:00:28s
epoch 26 | loss: 0.24586 | val_0_rmse: 0.4759  | val_1_rmse: 0.47121 |  0:00:30s
epoch 27 | loss: 0.24192 | val_0_rmse: 0.49582 | val_1_rmse: 0.49284 |  0:00:31s
epoch 28 | loss: 0.24366 | val_0_rmse: 0.48128 | val_1_rmse: 0.48089 |  0:00:32s
epoch 29 | loss: 0.23556 | val_0_rmse: 0.47502 | val_1_rmse: 0.47335 |  0:00:33s
epoch 30 | loss: 0.23024 | val_0_rmse: 0.46581 | val_1_rmse: 0.4657  |  0:00:34s
epoch 31 | loss: 0.22945 | val_0_rmse: 0.47086 | val_1_rmse: 0.46977 |  0:00:35s
epoch 32 | loss: 0.24241 | val_0_rmse: 0.47037 | val_1_rmse: 0.4711  |  0:00:36s
epoch 33 | loss: 0.23527 | val_0_rmse: 0.48325 | val_1_rmse: 0.48431 |  0:00:37s
epoch 34 | loss: 0.22442 | val_0_rmse: 0.46216 | val_1_rmse: 0.46256 |  0:00:38s
epoch 35 | loss: 0.22625 | val_0_rmse: 0.4559  | val_1_rmse: 0.4549  |  0:00:40s
epoch 36 | loss: 0.22105 | val_0_rmse: 0.4557  | val_1_rmse: 0.45519 |  0:00:41s
epoch 37 | loss: 0.21585 | val_0_rmse: 0.45131 | val_1_rmse: 0.45306 |  0:00:42s
epoch 38 | loss: 0.21327 | val_0_rmse: 0.45305 | val_1_rmse: 0.45476 |  0:00:43s
epoch 39 | loss: 0.21575 | val_0_rmse: 0.4556  | val_1_rmse: 0.45435 |  0:00:44s
epoch 40 | loss: 0.21386 | val_0_rmse: 0.45732 | val_1_rmse: 0.45958 |  0:00:45s
epoch 41 | loss: 0.21449 | val_0_rmse: 0.45818 | val_1_rmse: 0.46007 |  0:00:46s
epoch 42 | loss: 0.21895 | val_0_rmse: 0.44792 | val_1_rmse: 0.44829 |  0:00:47s
epoch 43 | loss: 0.21427 | val_0_rmse: 0.45386 | val_1_rmse: 0.45528 |  0:00:49s
epoch 44 | loss: 0.21673 | val_0_rmse: 0.44492 | val_1_rmse: 0.44696 |  0:00:50s
epoch 45 | loss: 0.21668 | val_0_rmse: 0.44484 | val_1_rmse: 0.44731 |  0:00:51s
epoch 46 | loss: 0.21114 | val_0_rmse: 0.46771 | val_1_rmse: 0.46962 |  0:00:52s
epoch 47 | loss: 0.21331 | val_0_rmse: 0.44838 | val_1_rmse: 0.44888 |  0:00:53s
epoch 48 | loss: 0.21439 | val_0_rmse: 0.44409 | val_1_rmse: 0.44462 |  0:00:54s
epoch 49 | loss: 0.21357 | val_0_rmse: 0.47761 | val_1_rmse: 0.47698 |  0:00:55s
epoch 50 | loss: 0.20991 | val_0_rmse: 0.44414 | val_1_rmse: 0.44559 |  0:00:56s
epoch 51 | loss: 0.20825 | val_0_rmse: 0.44815 | val_1_rmse: 0.44988 |  0:00:57s
epoch 52 | loss: 0.20724 | val_0_rmse: 0.4389  | val_1_rmse: 0.442   |  0:00:59s
epoch 53 | loss: 0.20893 | val_0_rmse: 0.44176 | val_1_rmse: 0.44513 |  0:01:00s
epoch 54 | loss: 0.20671 | val_0_rmse: 0.4384  | val_1_rmse: 0.4435  |  0:01:01s
epoch 55 | loss: 0.2046  | val_0_rmse: 0.44253 | val_1_rmse: 0.44791 |  0:01:02s
epoch 56 | loss: 0.20432 | val_0_rmse: 0.43644 | val_1_rmse: 0.43981 |  0:01:03s
epoch 57 | loss: 0.20413 | val_0_rmse: 0.43765 | val_1_rmse: 0.44223 |  0:01:04s
epoch 58 | loss: 0.20622 | val_0_rmse: 0.43996 | val_1_rmse: 0.44363 |  0:01:05s
epoch 59 | loss: 0.20495 | val_0_rmse: 0.43668 | val_1_rmse: 0.43946 |  0:01:06s
epoch 60 | loss: 0.20524 | val_0_rmse: 0.44655 | val_1_rmse: 0.45054 |  0:01:07s
epoch 61 | loss: 0.2049  | val_0_rmse: 0.43447 | val_1_rmse: 0.4395  |  0:01:09s
epoch 62 | loss: 0.20315 | val_0_rmse: 0.43563 | val_1_rmse: 0.44047 |  0:01:10s
epoch 63 | loss: 0.203   | val_0_rmse: 0.43607 | val_1_rmse: 0.44233 |  0:01:11s
epoch 64 | loss: 0.20155 | val_0_rmse: 0.44328 | val_1_rmse: 0.45155 |  0:01:12s
epoch 65 | loss: 0.21203 | val_0_rmse: 0.46134 | val_1_rmse: 0.46608 |  0:01:13s
epoch 66 | loss: 0.21119 | val_0_rmse: 0.44087 | val_1_rmse: 0.44548 |  0:01:14s
epoch 67 | loss: 0.20889 | val_0_rmse: 0.44376 | val_1_rmse: 0.44765 |  0:01:15s
epoch 68 | loss: 0.20984 | val_0_rmse: 0.45437 | val_1_rmse: 0.45878 |  0:01:16s
epoch 69 | loss: 0.20832 | val_0_rmse: 0.44471 | val_1_rmse: 0.45057 |  0:01:17s
epoch 70 | loss: 0.20509 | val_0_rmse: 0.43696 | val_1_rmse: 0.4435  |  0:01:19s
epoch 71 | loss: 0.19951 | val_0_rmse: 0.43365 | val_1_rmse: 0.44133 |  0:01:20s
epoch 72 | loss: 0.19775 | val_0_rmse: 0.43512 | val_1_rmse: 0.4412  |  0:01:21s
epoch 73 | loss: 0.1997  | val_0_rmse: 0.44133 | val_1_rmse: 0.45186 |  0:01:22s
epoch 74 | loss: 0.20191 | val_0_rmse: 0.43844 | val_1_rmse: 0.44705 |  0:01:23s
epoch 75 | loss: 0.19873 | val_0_rmse: 0.44135 | val_1_rmse: 0.44785 |  0:01:24s
epoch 76 | loss: 0.19868 | val_0_rmse: 0.4289  | val_1_rmse: 0.43962 |  0:01:25s
epoch 77 | loss: 0.19916 | val_0_rmse: 0.42979 | val_1_rmse: 0.44028 |  0:01:26s
epoch 78 | loss: 0.19718 | val_0_rmse: 0.43726 | val_1_rmse: 0.44748 |  0:01:27s
epoch 79 | loss: 0.2012  | val_0_rmse: 0.43452 | val_1_rmse: 0.44556 |  0:01:29s
epoch 80 | loss: 0.19692 | val_0_rmse: 0.43129 | val_1_rmse: 0.44124 |  0:01:30s
epoch 81 | loss: 0.19578 | val_0_rmse: 0.43201 | val_1_rmse: 0.43787 |  0:01:31s
epoch 82 | loss: 0.1984  | val_0_rmse: 0.43119 | val_1_rmse: 0.44137 |  0:01:32s
epoch 83 | loss: 0.19564 | val_0_rmse: 0.42844 | val_1_rmse: 0.44011 |  0:01:33s
epoch 84 | loss: 0.19763 | val_0_rmse: 0.43279 | val_1_rmse: 0.44353 |  0:01:34s
epoch 85 | loss: 0.19844 | val_0_rmse: 0.43328 | val_1_rmse: 0.44578 |  0:01:35s
epoch 86 | loss: 0.19764 | val_0_rmse: 0.42449 | val_1_rmse: 0.43596 |  0:01:36s
epoch 87 | loss: 0.20103 | val_0_rmse: 0.42959 | val_1_rmse: 0.44207 |  0:01:37s
epoch 88 | loss: 0.19601 | val_0_rmse: 0.43643 | val_1_rmse: 0.44773 |  0:01:39s
epoch 89 | loss: 0.19329 | val_0_rmse: 0.42278 | val_1_rmse: 0.43634 |  0:01:40s
epoch 90 | loss: 0.19029 | val_0_rmse: 0.43222 | val_1_rmse: 0.44273 |  0:01:41s
epoch 91 | loss: 0.192   | val_0_rmse: 0.42805 | val_1_rmse: 0.44119 |  0:01:42s
epoch 92 | loss: 0.19371 | val_0_rmse: 0.42975 | val_1_rmse: 0.44273 |  0:01:43s
epoch 93 | loss: 0.19229 | val_0_rmse: 0.42832 | val_1_rmse: 0.44181 |  0:01:44s
epoch 94 | loss: 0.19292 | val_0_rmse: 0.42511 | val_1_rmse: 0.43861 |  0:01:45s
epoch 95 | loss: 0.19281 | val_0_rmse: 0.42124 | val_1_rmse: 0.4364  |  0:01:46s
epoch 96 | loss: 0.19165 | val_0_rmse: 0.44157 | val_1_rmse: 0.45897 |  0:01:47s
epoch 97 | loss: 0.19177 | val_0_rmse: 0.43557 | val_1_rmse: 0.45259 |  0:01:49s
epoch 98 | loss: 0.19239 | val_0_rmse: 0.42187 | val_1_rmse: 0.44309 |  0:01:50s
epoch 99 | loss: 0.19233 | val_0_rmse: 0.41964 | val_1_rmse: 0.43948 |  0:01:51s
epoch 100| loss: 0.19025 | val_0_rmse: 0.4234  | val_1_rmse: 0.439   |  0:01:52s
epoch 101| loss: 0.18775 | val_0_rmse: 0.41822 | val_1_rmse: 0.43626 |  0:01:53s
epoch 102| loss: 0.18515 | val_0_rmse: 0.41672 | val_1_rmse: 0.43609 |  0:01:54s
epoch 103| loss: 0.18725 | val_0_rmse: 0.42053 | val_1_rmse: 0.43549 |  0:01:55s
epoch 104| loss: 0.1902  | val_0_rmse: 0.42638 | val_1_rmse: 0.44184 |  0:01:56s
epoch 105| loss: 0.19053 | val_0_rmse: 0.42239 | val_1_rmse: 0.43948 |  0:01:58s
epoch 106| loss: 0.19164 | val_0_rmse: 0.43182 | val_1_rmse: 0.44802 |  0:01:59s
epoch 107| loss: 0.18971 | val_0_rmse: 0.41723 | val_1_rmse: 0.43878 |  0:02:00s
epoch 108| loss: 0.18708 | val_0_rmse: 0.41746 | val_1_rmse: 0.44236 |  0:02:01s
epoch 109| loss: 0.18641 | val_0_rmse: 0.41648 | val_1_rmse: 0.43486 |  0:02:02s
epoch 110| loss: 0.18552 | val_0_rmse: 0.42344 | val_1_rmse: 0.44508 |  0:02:03s
epoch 111| loss: 0.18479 | val_0_rmse: 0.41494 | val_1_rmse: 0.43926 |  0:02:04s
epoch 112| loss: 0.1841  | val_0_rmse: 0.41371 | val_1_rmse: 0.43974 |  0:02:05s
epoch 113| loss: 0.18455 | val_0_rmse: 0.41758 | val_1_rmse: 0.43991 |  0:02:06s
epoch 114| loss: 0.18581 | val_0_rmse: 0.41897 | val_1_rmse: 0.43996 |  0:02:08s
epoch 115| loss: 0.18527 | val_0_rmse: 0.42268 | val_1_rmse: 0.44412 |  0:02:09s
epoch 116| loss: 0.18483 | val_0_rmse: 0.41582 | val_1_rmse: 0.44182 |  0:02:10s
epoch 117| loss: 0.18219 | val_0_rmse: 0.41437 | val_1_rmse: 0.43693 |  0:02:11s
epoch 118| loss: 0.18235 | val_0_rmse: 0.41202 | val_1_rmse: 0.43809 |  0:02:12s
epoch 119| loss: 0.18377 | val_0_rmse: 0.4122  | val_1_rmse: 0.44284 |  0:02:13s
epoch 120| loss: 0.18244 | val_0_rmse: 0.4089  | val_1_rmse: 0.43865 |  0:02:14s
epoch 121| loss: 0.18123 | val_0_rmse: 0.4132  | val_1_rmse: 0.43754 |  0:02:15s
epoch 122| loss: 0.18098 | val_0_rmse: 0.41302 | val_1_rmse: 0.44201 |  0:02:16s
epoch 123| loss: 0.18039 | val_0_rmse: 0.40822 | val_1_rmse: 0.43787 |  0:02:18s
epoch 124| loss: 0.18178 | val_0_rmse: 0.41565 | val_1_rmse: 0.44456 |  0:02:19s
epoch 125| loss: 0.18379 | val_0_rmse: 0.41209 | val_1_rmse: 0.43848 |  0:02:20s
epoch 126| loss: 0.18416 | val_0_rmse: 0.42187 | val_1_rmse: 0.44865 |  0:02:21s
epoch 127| loss: 0.18267 | val_0_rmse: 0.42402 | val_1_rmse: 0.45353 |  0:02:22s
epoch 128| loss: 0.17791 | val_0_rmse: 0.40693 | val_1_rmse: 0.43908 |  0:02:23s
epoch 129| loss: 0.18949 | val_0_rmse: 0.44206 | val_1_rmse: 0.46448 |  0:02:24s
epoch 130| loss: 0.20676 | val_0_rmse: 0.43129 | val_1_rmse: 0.44871 |  0:02:25s
epoch 131| loss: 0.19384 | val_0_rmse: 0.42252 | val_1_rmse: 0.44231 |  0:02:26s
epoch 132| loss: 0.20869 | val_0_rmse: 0.47143 | val_1_rmse: 0.4786  |  0:02:28s
epoch 133| loss: 0.20826 | val_0_rmse: 0.43929 | val_1_rmse: 0.45057 |  0:02:29s
epoch 134| loss: 0.20087 | val_0_rmse: 0.43552 | val_1_rmse: 0.44948 |  0:02:30s
epoch 135| loss: 0.20409 | val_0_rmse: 0.43501 | val_1_rmse: 0.44943 |  0:02:31s
epoch 136| loss: 0.19896 | val_0_rmse: 0.43654 | val_1_rmse: 0.45262 |  0:02:32s
epoch 137| loss: 0.213   | val_0_rmse: 0.4677  | val_1_rmse: 0.48275 |  0:02:33s
epoch 138| loss: 0.21988 | val_0_rmse: 0.4504  | val_1_rmse: 0.4549  |  0:02:34s
epoch 139| loss: 0.21162 | val_0_rmse: 0.44097 | val_1_rmse: 0.4499  |  0:02:35s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.43486
Best weights from best epoch are automatically used!
ended training at: 07:49:51
Feature importance:
Mean squared error is of 6400662124.07826
Mean absolute error:56624.67077013785
MAPE:0.14771178527932985
R2 score:0.7862242231595317
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:49:51
epoch 0  | loss: 1.21862 | val_0_rmse: 0.90848 | val_1_rmse: 0.91063 |  0:00:01s
epoch 1  | loss: 0.55437 | val_0_rmse: 0.75223 | val_1_rmse: 0.78038 |  0:00:02s
epoch 2  | loss: 0.40439 | val_0_rmse: 0.62563 | val_1_rmse: 0.6402  |  0:00:03s
epoch 3  | loss: 0.32674 | val_0_rmse: 0.58562 | val_1_rmse: 0.59693 |  0:00:04s
epoch 4  | loss: 0.27603 | val_0_rmse: 0.5428  | val_1_rmse: 0.55183 |  0:00:05s
epoch 5  | loss: 0.26569 | val_0_rmse: 0.51755 | val_1_rmse: 0.5338  |  0:00:06s
epoch 6  | loss: 0.25283 | val_0_rmse: 0.50235 | val_1_rmse: 0.51101 |  0:00:07s
epoch 7  | loss: 0.24291 | val_0_rmse: 0.55992 | val_1_rmse: 0.5688  |  0:00:08s
epoch 8  | loss: 0.2343  | val_0_rmse: 0.50537 | val_1_rmse: 0.51917 |  0:00:10s
epoch 9  | loss: 0.22836 | val_0_rmse: 0.54232 | val_1_rmse: 0.55659 |  0:00:11s
epoch 10 | loss: 0.24725 | val_0_rmse: 0.52857 | val_1_rmse: 0.5475  |  0:00:12s
epoch 11 | loss: 0.22933 | val_0_rmse: 0.50613 | val_1_rmse: 0.52045 |  0:00:13s
epoch 12 | loss: 0.22448 | val_0_rmse: 0.49098 | val_1_rmse: 0.50674 |  0:00:14s
epoch 13 | loss: 0.21593 | val_0_rmse: 0.53467 | val_1_rmse: 0.54793 |  0:00:15s
epoch 14 | loss: 0.22109 | val_0_rmse: 0.49092 | val_1_rmse: 0.50923 |  0:00:16s
epoch 15 | loss: 0.22244 | val_0_rmse: 0.49829 | val_1_rmse: 0.51494 |  0:00:17s
epoch 16 | loss: 0.21542 | val_0_rmse: 0.49894 | val_1_rmse: 0.5151  |  0:00:18s
epoch 17 | loss: 0.21196 | val_0_rmse: 0.49494 | val_1_rmse: 0.50912 |  0:00:20s
epoch 18 | loss: 0.21493 | val_0_rmse: 0.49474 | val_1_rmse: 0.51481 |  0:00:21s
epoch 19 | loss: 0.21574 | val_0_rmse: 0.48305 | val_1_rmse: 0.50063 |  0:00:22s
epoch 20 | loss: 0.20874 | val_0_rmse: 0.47739 | val_1_rmse: 0.49527 |  0:00:23s
epoch 21 | loss: 0.21117 | val_0_rmse: 0.47654 | val_1_rmse: 0.49368 |  0:00:24s
epoch 22 | loss: 0.20799 | val_0_rmse: 0.46918 | val_1_rmse: 0.48588 |  0:00:25s
epoch 23 | loss: 0.20684 | val_0_rmse: 0.46817 | val_1_rmse: 0.48437 |  0:00:26s
epoch 24 | loss: 0.20858 | val_0_rmse: 0.48535 | val_1_rmse: 0.50496 |  0:00:27s
epoch 25 | loss: 0.21891 | val_0_rmse: 0.45557 | val_1_rmse: 0.47387 |  0:00:28s
epoch 26 | loss: 0.20761 | val_0_rmse: 0.46942 | val_1_rmse: 0.49134 |  0:00:30s
epoch 27 | loss: 0.2062  | val_0_rmse: 0.449   | val_1_rmse: 0.46866 |  0:00:31s
epoch 28 | loss: 0.20114 | val_0_rmse: 0.44572 | val_1_rmse: 0.46554 |  0:00:32s
epoch 29 | loss: 0.19874 | val_0_rmse: 0.45456 | val_1_rmse: 0.47424 |  0:00:33s
epoch 30 | loss: 0.19965 | val_0_rmse: 0.43939 | val_1_rmse: 0.45994 |  0:00:34s
epoch 31 | loss: 0.19737 | val_0_rmse: 0.43509 | val_1_rmse: 0.45729 |  0:00:35s
epoch 32 | loss: 0.19895 | val_0_rmse: 0.44362 | val_1_rmse: 0.46805 |  0:00:36s
epoch 33 | loss: 0.21183 | val_0_rmse: 0.4682  | val_1_rmse: 0.49425 |  0:00:37s
epoch 34 | loss: 0.21359 | val_0_rmse: 0.44958 | val_1_rmse: 0.47454 |  0:00:39s
epoch 35 | loss: 0.21095 | val_0_rmse: 0.45847 | val_1_rmse: 0.48    |  0:00:40s
epoch 36 | loss: 0.21266 | val_0_rmse: 0.46461 | val_1_rmse: 0.48517 |  0:00:41s
epoch 37 | loss: 0.21414 | val_0_rmse: 0.44404 | val_1_rmse: 0.46531 |  0:00:42s
epoch 38 | loss: 0.21805 | val_0_rmse: 0.45185 | val_1_rmse: 0.47078 |  0:00:43s
epoch 39 | loss: 0.21688 | val_0_rmse: 0.45677 | val_1_rmse: 0.47931 |  0:00:44s
epoch 40 | loss: 0.20823 | val_0_rmse: 0.44289 | val_1_rmse: 0.46388 |  0:00:45s
epoch 41 | loss: 0.20663 | val_0_rmse: 0.43657 | val_1_rmse: 0.4583  |  0:00:46s
epoch 42 | loss: 0.20264 | val_0_rmse: 0.44352 | val_1_rmse: 0.46483 |  0:00:47s
epoch 43 | loss: 0.21083 | val_0_rmse: 0.47469 | val_1_rmse: 0.49063 |  0:00:48s
epoch 44 | loss: 0.21971 | val_0_rmse: 0.45718 | val_1_rmse: 0.47307 |  0:00:50s
epoch 45 | loss: 0.22281 | val_0_rmse: 0.46319 | val_1_rmse: 0.47968 |  0:00:51s
epoch 46 | loss: 0.22042 | val_0_rmse: 0.44859 | val_1_rmse: 0.46654 |  0:00:52s
epoch 47 | loss: 0.211   | val_0_rmse: 0.44661 | val_1_rmse: 0.46634 |  0:00:53s
epoch 48 | loss: 0.21126 | val_0_rmse: 0.44174 | val_1_rmse: 0.46431 |  0:00:54s
epoch 49 | loss: 0.20872 | val_0_rmse: 0.45239 | val_1_rmse: 0.47186 |  0:00:55s
epoch 50 | loss: 0.2043  | val_0_rmse: 0.43817 | val_1_rmse: 0.45824 |  0:00:56s
epoch 51 | loss: 0.20886 | val_0_rmse: 0.47111 | val_1_rmse: 0.49092 |  0:00:57s
epoch 52 | loss: 0.20671 | val_0_rmse: 0.44491 | val_1_rmse: 0.46376 |  0:00:58s
epoch 53 | loss: 0.20757 | val_0_rmse: 0.44488 | val_1_rmse: 0.46822 |  0:01:00s
epoch 54 | loss: 0.21057 | val_0_rmse: 0.45126 | val_1_rmse: 0.47156 |  0:01:01s
epoch 55 | loss: 0.2089  | val_0_rmse: 0.45113 | val_1_rmse: 0.46885 |  0:01:02s
epoch 56 | loss: 0.20724 | val_0_rmse: 0.44727 | val_1_rmse: 0.46048 |  0:01:03s
epoch 57 | loss: 0.20465 | val_0_rmse: 0.43962 | val_1_rmse: 0.46032 |  0:01:04s
epoch 58 | loss: 0.20148 | val_0_rmse: 0.43158 | val_1_rmse: 0.45106 |  0:01:05s
epoch 59 | loss: 0.19435 | val_0_rmse: 0.4332  | val_1_rmse: 0.45777 |  0:01:06s
epoch 60 | loss: 0.19802 | val_0_rmse: 0.43751 | val_1_rmse: 0.45922 |  0:01:07s
epoch 61 | loss: 0.19666 | val_0_rmse: 0.42957 | val_1_rmse: 0.45279 |  0:01:09s
epoch 62 | loss: 0.19543 | val_0_rmse: 0.43128 | val_1_rmse: 0.45484 |  0:01:10s
epoch 63 | loss: 0.1943  | val_0_rmse: 0.42651 | val_1_rmse: 0.45073 |  0:01:11s
epoch 64 | loss: 0.19447 | val_0_rmse: 0.42704 | val_1_rmse: 0.45262 |  0:01:12s
epoch 65 | loss: 0.19172 | val_0_rmse: 0.42877 | val_1_rmse: 0.45161 |  0:01:13s
epoch 66 | loss: 0.19197 | val_0_rmse: 0.43626 | val_1_rmse: 0.4586  |  0:01:14s
epoch 67 | loss: 0.19767 | val_0_rmse: 0.43818 | val_1_rmse: 0.45762 |  0:01:15s
epoch 68 | loss: 0.20518 | val_0_rmse: 0.50479 | val_1_rmse: 0.52614 |  0:01:16s
epoch 69 | loss: 0.22581 | val_0_rmse: 0.50624 | val_1_rmse: 0.52213 |  0:01:17s
epoch 70 | loss: 0.2308  | val_0_rmse: 0.48081 | val_1_rmse: 0.49284 |  0:01:19s
epoch 71 | loss: 0.23583 | val_0_rmse: 0.58837 | val_1_rmse: 0.60048 |  0:01:20s
epoch 72 | loss: 0.23182 | val_0_rmse: 0.46646 | val_1_rmse: 0.48317 |  0:01:21s
epoch 73 | loss: 0.22386 | val_0_rmse: 0.44889 | val_1_rmse: 0.46734 |  0:01:22s
epoch 74 | loss: 0.22053 | val_0_rmse: 0.45662 | val_1_rmse: 0.47641 |  0:01:23s
epoch 75 | loss: 0.22404 | val_0_rmse: 0.44358 | val_1_rmse: 0.46529 |  0:01:24s
epoch 76 | loss: 0.21747 | val_0_rmse: 0.44743 | val_1_rmse: 0.46909 |  0:01:25s
epoch 77 | loss: 0.21844 | val_0_rmse: 0.44895 | val_1_rmse: 0.47033 |  0:01:26s
epoch 78 | loss: 0.21419 | val_0_rmse: 0.44458 | val_1_rmse: 0.46701 |  0:01:27s
epoch 79 | loss: 0.21467 | val_0_rmse: 0.4558  | val_1_rmse: 0.47877 |  0:01:29s
epoch 80 | loss: 0.21125 | val_0_rmse: 0.44389 | val_1_rmse: 0.46822 |  0:01:30s
epoch 81 | loss: 0.20863 | val_0_rmse: 0.47325 | val_1_rmse: 0.49679 |  0:01:31s
epoch 82 | loss: 0.2166  | val_0_rmse: 0.45565 | val_1_rmse: 0.47721 |  0:01:32s
epoch 83 | loss: 0.2142  | val_0_rmse: 0.45903 | val_1_rmse: 0.48278 |  0:01:33s
epoch 84 | loss: 0.21483 | val_0_rmse: 0.4398  | val_1_rmse: 0.46485 |  0:01:34s
epoch 85 | loss: 0.21539 | val_0_rmse: 0.46127 | val_1_rmse: 0.48544 |  0:01:35s
epoch 86 | loss: 0.21173 | val_0_rmse: 0.44613 | val_1_rmse: 0.47155 |  0:01:36s
epoch 87 | loss: 0.21924 | val_0_rmse: 0.46479 | val_1_rmse: 0.48904 |  0:01:37s
epoch 88 | loss: 0.22257 | val_0_rmse: 0.45269 | val_1_rmse: 0.47216 |  0:01:39s
epoch 89 | loss: 0.21854 | val_0_rmse: 0.45299 | val_1_rmse: 0.47533 |  0:01:40s
epoch 90 | loss: 0.21518 | val_0_rmse: 0.44627 | val_1_rmse: 0.46906 |  0:01:41s
epoch 91 | loss: 0.2144  | val_0_rmse: 0.44506 | val_1_rmse: 0.46685 |  0:01:42s
epoch 92 | loss: 0.21224 | val_0_rmse: 0.45107 | val_1_rmse: 0.47735 |  0:01:43s
epoch 93 | loss: 0.20417 | val_0_rmse: 0.43983 | val_1_rmse: 0.46544 |  0:01:44s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.45073
Best weights from best epoch are automatically used!
ended training at: 07:51:36
Feature importance:
Mean squared error is of 6528050193.497414
Mean absolute error:56383.37753796789
MAPE:0.14739291011983385
R2 score:0.789056199025937
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:51:37
epoch 0  | loss: 1.2136  | val_0_rmse: 0.91935 | val_1_rmse: 0.8994  |  0:00:01s
epoch 1  | loss: 0.68389 | val_0_rmse: 0.81571 | val_1_rmse: 0.80385 |  0:00:02s
epoch 2  | loss: 0.60961 | val_0_rmse: 0.72282 | val_1_rmse: 0.71322 |  0:00:03s
epoch 3  | loss: 0.46755 | val_0_rmse: 0.62937 | val_1_rmse: 0.62177 |  0:00:04s
epoch 4  | loss: 0.3731  | val_0_rmse: 0.58391 | val_1_rmse: 0.57297 |  0:00:05s
epoch 5  | loss: 0.35483 | val_0_rmse: 0.64683 | val_1_rmse: 0.64481 |  0:00:06s
epoch 6  | loss: 0.30409 | val_0_rmse: 0.53355 | val_1_rmse: 0.52065 |  0:00:07s
epoch 7  | loss: 0.27719 | val_0_rmse: 0.55693 | val_1_rmse: 0.54178 |  0:00:08s
epoch 8  | loss: 0.27563 | val_0_rmse: 0.57181 | val_1_rmse: 0.56394 |  0:00:10s
epoch 9  | loss: 0.27173 | val_0_rmse: 0.54726 | val_1_rmse: 0.53058 |  0:00:11s
epoch 10 | loss: 0.25885 | val_0_rmse: 0.56411 | val_1_rmse: 0.55333 |  0:00:12s
epoch 11 | loss: 0.25397 | val_0_rmse: 0.54684 | val_1_rmse: 0.53775 |  0:00:13s
epoch 12 | loss: 0.24368 | val_0_rmse: 0.5272  | val_1_rmse: 0.51544 |  0:00:14s
epoch 13 | loss: 0.24046 | val_0_rmse: 0.5296  | val_1_rmse: 0.51832 |  0:00:15s
epoch 14 | loss: 0.23002 | val_0_rmse: 0.53697 | val_1_rmse: 0.52436 |  0:00:16s
epoch 15 | loss: 0.22844 | val_0_rmse: 0.51932 | val_1_rmse: 0.50574 |  0:00:17s
epoch 16 | loss: 0.22587 | val_0_rmse: 0.51663 | val_1_rmse: 0.50931 |  0:00:19s
epoch 17 | loss: 0.22481 | val_0_rmse: 0.52322 | val_1_rmse: 0.51238 |  0:00:20s
epoch 18 | loss: 0.22251 | val_0_rmse: 0.4915  | val_1_rmse: 0.48214 |  0:00:21s
epoch 19 | loss: 0.22256 | val_0_rmse: 0.49737 | val_1_rmse: 0.48502 |  0:00:22s
epoch 20 | loss: 0.2265  | val_0_rmse: 0.50348 | val_1_rmse: 0.49614 |  0:00:23s
epoch 21 | loss: 0.22355 | val_0_rmse: 0.49302 | val_1_rmse: 0.4792  |  0:00:24s
epoch 22 | loss: 0.21997 | val_0_rmse: 0.47887 | val_1_rmse: 0.46802 |  0:00:25s
epoch 23 | loss: 0.21563 | val_0_rmse: 0.48121 | val_1_rmse: 0.46813 |  0:00:26s
epoch 24 | loss: 0.21113 | val_0_rmse: 0.47067 | val_1_rmse: 0.45937 |  0:00:27s
epoch 25 | loss: 0.21115 | val_0_rmse: 0.49529 | val_1_rmse: 0.48354 |  0:00:29s
epoch 26 | loss: 0.21817 | val_0_rmse: 0.4704  | val_1_rmse: 0.46293 |  0:00:30s
epoch 27 | loss: 0.22702 | val_0_rmse: 0.4882  | val_1_rmse: 0.4791  |  0:00:31s
epoch 28 | loss: 0.2347  | val_0_rmse: 0.47223 | val_1_rmse: 0.46239 |  0:00:32s
epoch 29 | loss: 0.22095 | val_0_rmse: 0.47227 | val_1_rmse: 0.46148 |  0:00:33s
epoch 30 | loss: 0.21765 | val_0_rmse: 0.47724 | val_1_rmse: 0.46882 |  0:00:34s
epoch 31 | loss: 0.22005 | val_0_rmse: 0.46937 | val_1_rmse: 0.45803 |  0:00:35s
epoch 32 | loss: 0.22424 | val_0_rmse: 0.48612 | val_1_rmse: 0.47354 |  0:00:36s
epoch 33 | loss: 0.22522 | val_0_rmse: 0.46532 | val_1_rmse: 0.45871 |  0:00:37s
epoch 34 | loss: 0.21995 | val_0_rmse: 0.47515 | val_1_rmse: 0.46652 |  0:00:39s
epoch 35 | loss: 0.21476 | val_0_rmse: 0.45653 | val_1_rmse: 0.45003 |  0:00:40s
epoch 36 | loss: 0.2085  | val_0_rmse: 0.4475  | val_1_rmse: 0.44099 |  0:00:41s
epoch 37 | loss: 0.20244 | val_0_rmse: 0.44688 | val_1_rmse: 0.4429  |  0:00:42s
epoch 38 | loss: 0.20549 | val_0_rmse: 0.43797 | val_1_rmse: 0.43376 |  0:00:43s
epoch 39 | loss: 0.1998  | val_0_rmse: 0.44725 | val_1_rmse: 0.44494 |  0:00:44s
epoch 40 | loss: 0.20393 | val_0_rmse: 0.45794 | val_1_rmse: 0.45474 |  0:00:45s
epoch 41 | loss: 0.20636 | val_0_rmse: 0.4608  | val_1_rmse: 0.45813 |  0:00:46s
epoch 42 | loss: 0.20461 | val_0_rmse: 0.43587 | val_1_rmse: 0.435   |  0:00:47s
epoch 43 | loss: 0.20037 | val_0_rmse: 0.43519 | val_1_rmse: 0.43466 |  0:00:49s
epoch 44 | loss: 0.20216 | val_0_rmse: 0.4348  | val_1_rmse: 0.43818 |  0:00:50s
epoch 45 | loss: 0.19856 | val_0_rmse: 0.43028 | val_1_rmse: 0.43212 |  0:00:51s
epoch 46 | loss: 0.20242 | val_0_rmse: 0.49223 | val_1_rmse: 0.48688 |  0:00:52s
epoch 47 | loss: 0.21206 | val_0_rmse: 0.44567 | val_1_rmse: 0.45222 |  0:00:53s
epoch 48 | loss: 0.20481 | val_0_rmse: 0.43797 | val_1_rmse: 0.43822 |  0:00:54s
epoch 49 | loss: 0.20132 | val_0_rmse: 0.49378 | val_1_rmse: 0.49752 |  0:00:55s
epoch 50 | loss: 0.21287 | val_0_rmse: 0.45623 | val_1_rmse: 0.45541 |  0:00:56s
epoch 51 | loss: 0.22652 | val_0_rmse: 0.46259 | val_1_rmse: 0.45827 |  0:00:58s
epoch 52 | loss: 0.22808 | val_0_rmse: 0.47369 | val_1_rmse: 0.47169 |  0:00:59s
epoch 53 | loss: 0.21696 | val_0_rmse: 0.44692 | val_1_rmse: 0.44607 |  0:01:00s
epoch 54 | loss: 0.21059 | val_0_rmse: 0.43628 | val_1_rmse: 0.43842 |  0:01:01s
epoch 55 | loss: 0.20353 | val_0_rmse: 0.43682 | val_1_rmse: 0.43772 |  0:01:02s
epoch 56 | loss: 0.20017 | val_0_rmse: 0.43087 | val_1_rmse: 0.43517 |  0:01:03s
epoch 57 | loss: 0.20007 | val_0_rmse: 0.44981 | val_1_rmse: 0.4526  |  0:01:04s
epoch 58 | loss: 0.2015  | val_0_rmse: 0.42848 | val_1_rmse: 0.43351 |  0:01:05s
epoch 59 | loss: 0.19722 | val_0_rmse: 0.42806 | val_1_rmse: 0.43398 |  0:01:06s
epoch 60 | loss: 0.19673 | val_0_rmse: 0.44542 | val_1_rmse: 0.45464 |  0:01:07s
epoch 61 | loss: 0.19666 | val_0_rmse: 0.42799 | val_1_rmse: 0.43207 |  0:01:09s
epoch 62 | loss: 0.19222 | val_0_rmse: 0.42469 | val_1_rmse: 0.43125 |  0:01:10s
epoch 63 | loss: 0.19524 | val_0_rmse: 0.42643 | val_1_rmse: 0.43469 |  0:01:11s
epoch 64 | loss: 0.19303 | val_0_rmse: 0.4261  | val_1_rmse: 0.43207 |  0:01:12s
epoch 65 | loss: 0.19593 | val_0_rmse: 0.4393  | val_1_rmse: 0.45109 |  0:01:13s
epoch 66 | loss: 0.19594 | val_0_rmse: 0.43372 | val_1_rmse: 0.44016 |  0:01:14s
epoch 67 | loss: 0.19378 | val_0_rmse: 0.42528 | val_1_rmse: 0.43434 |  0:01:15s
epoch 68 | loss: 0.19036 | val_0_rmse: 0.4212  | val_1_rmse: 0.43057 |  0:01:16s
epoch 69 | loss: 0.18949 | val_0_rmse: 0.42011 | val_1_rmse: 0.43203 |  0:01:18s
epoch 70 | loss: 0.19135 | val_0_rmse: 0.44593 | val_1_rmse: 0.45955 |  0:01:19s
epoch 71 | loss: 0.19153 | val_0_rmse: 0.43866 | val_1_rmse: 0.44587 |  0:01:20s
epoch 72 | loss: 0.19076 | val_0_rmse: 0.42284 | val_1_rmse: 0.43411 |  0:01:21s
epoch 73 | loss: 0.1885  | val_0_rmse: 0.42842 | val_1_rmse: 0.43966 |  0:01:22s
epoch 74 | loss: 0.19123 | val_0_rmse: 0.45982 | val_1_rmse: 0.46533 |  0:01:23s
epoch 75 | loss: 0.19468 | val_0_rmse: 0.42381 | val_1_rmse: 0.43764 |  0:01:24s
epoch 76 | loss: 0.18607 | val_0_rmse: 0.43101 | val_1_rmse: 0.44383 |  0:01:25s
epoch 77 | loss: 0.19201 | val_0_rmse: 0.43425 | val_1_rmse: 0.44541 |  0:01:26s
epoch 78 | loss: 0.19607 | val_0_rmse: 0.44489 | val_1_rmse: 0.45714 |  0:01:28s
epoch 79 | loss: 0.18752 | val_0_rmse: 0.41903 | val_1_rmse: 0.43266 |  0:01:29s
epoch 80 | loss: 0.18686 | val_0_rmse: 0.41401 | val_1_rmse: 0.43099 |  0:01:30s
epoch 81 | loss: 0.18414 | val_0_rmse: 0.42666 | val_1_rmse: 0.44294 |  0:01:31s
epoch 82 | loss: 0.19155 | val_0_rmse: 0.41907 | val_1_rmse: 0.43251 |  0:01:32s
epoch 83 | loss: 0.18928 | val_0_rmse: 0.42612 | val_1_rmse: 0.43955 |  0:01:33s
epoch 84 | loss: 0.19126 | val_0_rmse: 0.41948 | val_1_rmse: 0.43419 |  0:01:34s
epoch 85 | loss: 0.18842 | val_0_rmse: 0.41358 | val_1_rmse: 0.428   |  0:01:35s
epoch 86 | loss: 0.18455 | val_0_rmse: 0.41776 | val_1_rmse: 0.43733 |  0:01:36s
epoch 87 | loss: 0.18305 | val_0_rmse: 0.41391 | val_1_rmse: 0.43329 |  0:01:38s
epoch 88 | loss: 0.18345 | val_0_rmse: 0.42568 | val_1_rmse: 0.43997 |  0:01:39s
epoch 89 | loss: 0.18343 | val_0_rmse: 0.41335 | val_1_rmse: 0.43585 |  0:01:40s
epoch 90 | loss: 0.18343 | val_0_rmse: 0.41935 | val_1_rmse: 0.43971 |  0:01:41s
epoch 91 | loss: 0.1837  | val_0_rmse: 0.42148 | val_1_rmse: 0.43799 |  0:01:42s
epoch 92 | loss: 0.18347 | val_0_rmse: 0.41129 | val_1_rmse: 0.43332 |  0:01:43s
epoch 93 | loss: 0.18446 | val_0_rmse: 0.41431 | val_1_rmse: 0.43317 |  0:01:44s
epoch 94 | loss: 0.18048 | val_0_rmse: 0.41071 | val_1_rmse: 0.43221 |  0:01:45s
epoch 95 | loss: 0.18315 | val_0_rmse: 0.41024 | val_1_rmse: 0.43489 |  0:01:46s
epoch 96 | loss: 0.1784  | val_0_rmse: 0.41628 | val_1_rmse: 0.43519 |  0:01:48s
epoch 97 | loss: 0.18319 | val_0_rmse: 0.42006 | val_1_rmse: 0.44551 |  0:01:49s
epoch 98 | loss: 0.18189 | val_0_rmse: 0.41557 | val_1_rmse: 0.43711 |  0:01:50s
epoch 99 | loss: 0.1806  | val_0_rmse: 0.42552 | val_1_rmse: 0.44128 |  0:01:51s
epoch 100| loss: 0.1811  | val_0_rmse: 0.42154 | val_1_rmse: 0.44174 |  0:01:52s
epoch 101| loss: 0.18215 | val_0_rmse: 0.41437 | val_1_rmse: 0.44377 |  0:01:53s
epoch 102| loss: 0.17996 | val_0_rmse: 0.40884 | val_1_rmse: 0.43953 |  0:01:54s
epoch 103| loss: 0.17725 | val_0_rmse: 0.40678 | val_1_rmse: 0.43567 |  0:01:55s
epoch 104| loss: 0.17966 | val_0_rmse: 0.40947 | val_1_rmse: 0.43681 |  0:01:56s
epoch 105| loss: 0.18089 | val_0_rmse: 0.4034  | val_1_rmse: 0.43427 |  0:01:58s
epoch 106| loss: 0.17907 | val_0_rmse: 0.41567 | val_1_rmse: 0.44748 |  0:01:59s
epoch 107| loss: 0.17599 | val_0_rmse: 0.4036  | val_1_rmse: 0.43512 |  0:02:00s
epoch 108| loss: 0.17764 | val_0_rmse: 0.4196  | val_1_rmse: 0.4516  |  0:02:01s
epoch 109| loss: 0.18388 | val_0_rmse: 0.40174 | val_1_rmse: 0.43574 |  0:02:02s
epoch 110| loss: 0.18019 | val_0_rmse: 0.45053 | val_1_rmse: 0.47457 |  0:02:03s
epoch 111| loss: 0.17809 | val_0_rmse: 0.40115 | val_1_rmse: 0.43421 |  0:02:04s
epoch 112| loss: 0.1746  | val_0_rmse: 0.40908 | val_1_rmse: 0.44008 |  0:02:05s
epoch 113| loss: 0.17862 | val_0_rmse: 0.40182 | val_1_rmse: 0.4362  |  0:02:06s
epoch 114| loss: 0.17568 | val_0_rmse: 0.40097 | val_1_rmse: 0.43441 |  0:02:08s
epoch 115| loss: 0.17177 | val_0_rmse: 0.40147 | val_1_rmse: 0.43921 |  0:02:09s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.428
Best weights from best epoch are automatically used!
ended training at: 07:53:46
Feature importance:
Mean squared error is of 6372299874.274256
Mean absolute error:55819.548736327146
MAPE:0.14314281406353727
R2 score:0.792673018595546
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:53:46
epoch 0  | loss: 1.31596 | val_0_rmse: 0.94701 | val_1_rmse: 0.95024 |  0:00:01s
epoch 1  | loss: 0.64365 | val_0_rmse: 0.76406 | val_1_rmse: 0.77515 |  0:00:02s
epoch 2  | loss: 0.47964 | val_0_rmse: 0.66974 | val_1_rmse: 0.66992 |  0:00:03s
epoch 3  | loss: 0.40918 | val_0_rmse: 0.61725 | val_1_rmse: 0.62291 |  0:00:04s
epoch 4  | loss: 0.36039 | val_0_rmse: 0.59591 | val_1_rmse: 0.59387 |  0:00:05s
epoch 5  | loss: 0.30826 | val_0_rmse: 0.56724 | val_1_rmse: 0.5646  |  0:00:06s
epoch 6  | loss: 0.27649 | val_0_rmse: 0.57228 | val_1_rmse: 0.57054 |  0:00:07s
epoch 7  | loss: 0.25829 | val_0_rmse: 0.56552 | val_1_rmse: 0.5632  |  0:00:08s
epoch 8  | loss: 0.25566 | val_0_rmse: 0.55709 | val_1_rmse: 0.55083 |  0:00:10s
epoch 9  | loss: 0.25262 | val_0_rmse: 0.53036 | val_1_rmse: 0.52788 |  0:00:11s
epoch 10 | loss: 0.24646 | val_0_rmse: 0.57095 | val_1_rmse: 0.56576 |  0:00:12s
epoch 11 | loss: 0.24134 | val_0_rmse: 0.51152 | val_1_rmse: 0.50618 |  0:00:13s
epoch 12 | loss: 0.238   | val_0_rmse: 0.51403 | val_1_rmse: 0.50868 |  0:00:14s
epoch 13 | loss: 0.23462 | val_0_rmse: 0.51446 | val_1_rmse: 0.51065 |  0:00:15s
epoch 14 | loss: 0.22765 | val_0_rmse: 0.51707 | val_1_rmse: 0.51364 |  0:00:16s
epoch 15 | loss: 0.21639 | val_0_rmse: 0.5028  | val_1_rmse: 0.50338 |  0:00:17s
epoch 16 | loss: 0.22019 | val_0_rmse: 0.48963 | val_1_rmse: 0.48927 |  0:00:18s
epoch 17 | loss: 0.2163  | val_0_rmse: 0.48673 | val_1_rmse: 0.48892 |  0:00:20s
epoch 18 | loss: 0.215   | val_0_rmse: 0.48366 | val_1_rmse: 0.48716 |  0:00:21s
epoch 19 | loss: 0.21245 | val_0_rmse: 0.48473 | val_1_rmse: 0.48766 |  0:00:22s
epoch 20 | loss: 0.21247 | val_0_rmse: 0.48369 | val_1_rmse: 0.48631 |  0:00:23s
epoch 21 | loss: 0.21224 | val_0_rmse: 0.47199 | val_1_rmse: 0.47643 |  0:00:24s
epoch 22 | loss: 0.20748 | val_0_rmse: 0.46338 | val_1_rmse: 0.47057 |  0:00:25s
epoch 23 | loss: 0.2069  | val_0_rmse: 0.47372 | val_1_rmse: 0.48114 |  0:00:26s
epoch 24 | loss: 0.20624 | val_0_rmse: 0.46334 | val_1_rmse: 0.46919 |  0:00:27s
epoch 25 | loss: 0.20367 | val_0_rmse: 0.45734 | val_1_rmse: 0.46254 |  0:00:29s
epoch 26 | loss: 0.20322 | val_0_rmse: 0.46771 | val_1_rmse: 0.47222 |  0:00:30s
epoch 27 | loss: 0.20275 | val_0_rmse: 0.45235 | val_1_rmse: 0.45965 |  0:00:31s
epoch 28 | loss: 0.20147 | val_0_rmse: 0.46593 | val_1_rmse: 0.47282 |  0:00:32s
epoch 29 | loss: 0.20627 | val_0_rmse: 0.46086 | val_1_rmse: 0.46979 |  0:00:33s
epoch 30 | loss: 0.20429 | val_0_rmse: 0.44545 | val_1_rmse: 0.45239 |  0:00:34s
epoch 31 | loss: 0.19688 | val_0_rmse: 0.44723 | val_1_rmse: 0.46077 |  0:00:35s
epoch 32 | loss: 0.19455 | val_0_rmse: 0.43707 | val_1_rmse: 0.44697 |  0:00:36s
epoch 33 | loss: 0.19508 | val_0_rmse: 0.434   | val_1_rmse: 0.44775 |  0:00:38s
epoch 34 | loss: 0.19329 | val_0_rmse: 0.4316  | val_1_rmse: 0.44398 |  0:00:39s
epoch 35 | loss: 0.19623 | val_0_rmse: 0.42917 | val_1_rmse: 0.44457 |  0:00:40s
epoch 36 | loss: 0.19384 | val_0_rmse: 0.42671 | val_1_rmse: 0.44295 |  0:00:41s
epoch 37 | loss: 0.19408 | val_0_rmse: 0.42813 | val_1_rmse: 0.44129 |  0:00:42s
epoch 38 | loss: 0.19236 | val_0_rmse: 0.42641 | val_1_rmse: 0.44678 |  0:00:43s
epoch 39 | loss: 0.19483 | val_0_rmse: 0.437   | val_1_rmse: 0.45427 |  0:00:44s
epoch 40 | loss: 0.19263 | val_0_rmse: 0.4354  | val_1_rmse: 0.45411 |  0:00:45s
epoch 41 | loss: 0.19668 | val_0_rmse: 0.42597 | val_1_rmse: 0.44649 |  0:00:46s
epoch 42 | loss: 0.19314 | val_0_rmse: 0.42312 | val_1_rmse: 0.44516 |  0:00:47s
epoch 43 | loss: 0.1903  | val_0_rmse: 0.42622 | val_1_rmse: 0.44965 |  0:00:49s
epoch 44 | loss: 0.18629 | val_0_rmse: 0.41908 | val_1_rmse: 0.44944 |  0:00:50s
epoch 45 | loss: 0.19467 | val_0_rmse: 0.46308 | val_1_rmse: 0.48473 |  0:00:51s
epoch 46 | loss: 0.19734 | val_0_rmse: 0.42114 | val_1_rmse: 0.45065 |  0:00:52s
epoch 47 | loss: 0.18867 | val_0_rmse: 0.41539 | val_1_rmse: 0.4418  |  0:00:53s
epoch 48 | loss: 0.1846  | val_0_rmse: 0.41097 | val_1_rmse: 0.44288 |  0:00:54s
epoch 49 | loss: 0.18257 | val_0_rmse: 0.40941 | val_1_rmse: 0.44081 |  0:00:55s
epoch 50 | loss: 0.18383 | val_0_rmse: 0.42083 | val_1_rmse: 0.45669 |  0:00:56s
epoch 51 | loss: 0.18318 | val_0_rmse: 0.41249 | val_1_rmse: 0.44764 |  0:00:58s
epoch 52 | loss: 0.18467 | val_0_rmse: 0.41307 | val_1_rmse: 0.44964 |  0:00:59s
epoch 53 | loss: 0.1827  | val_0_rmse: 0.41336 | val_1_rmse: 0.44666 |  0:01:00s
epoch 54 | loss: 0.18197 | val_0_rmse: 0.41541 | val_1_rmse: 0.45232 |  0:01:01s
epoch 55 | loss: 0.18464 | val_0_rmse: 0.40688 | val_1_rmse: 0.44614 |  0:01:02s
epoch 56 | loss: 0.18092 | val_0_rmse: 0.41236 | val_1_rmse: 0.45206 |  0:01:03s
epoch 57 | loss: 0.18288 | val_0_rmse: 0.41118 | val_1_rmse: 0.45231 |  0:01:04s
epoch 58 | loss: 0.18178 | val_0_rmse: 0.40508 | val_1_rmse: 0.45003 |  0:01:05s
epoch 59 | loss: 0.17915 | val_0_rmse: 0.40374 | val_1_rmse: 0.4446  |  0:01:06s
epoch 60 | loss: 0.17741 | val_0_rmse: 0.40394 | val_1_rmse: 0.44885 |  0:01:08s
epoch 61 | loss: 0.17764 | val_0_rmse: 0.41133 | val_1_rmse: 0.45006 |  0:01:09s
epoch 62 | loss: 0.18177 | val_0_rmse: 0.40438 | val_1_rmse: 0.45486 |  0:01:10s
epoch 63 | loss: 0.18047 | val_0_rmse: 0.40687 | val_1_rmse: 0.44909 |  0:01:11s
epoch 64 | loss: 0.18072 | val_0_rmse: 0.40429 | val_1_rmse: 0.44313 |  0:01:12s
epoch 65 | loss: 0.17783 | val_0_rmse: 0.40171 | val_1_rmse: 0.44802 |  0:01:13s
epoch 66 | loss: 0.17495 | val_0_rmse: 0.39956 | val_1_rmse: 0.44491 |  0:01:14s
epoch 67 | loss: 0.17447 | val_0_rmse: 0.41901 | val_1_rmse: 0.46745 |  0:01:15s
epoch 68 | loss: 0.17718 | val_0_rmse: 0.39899 | val_1_rmse: 0.44671 |  0:01:16s
epoch 69 | loss: 0.17576 | val_0_rmse: 0.4004  | val_1_rmse: 0.44512 |  0:01:18s
epoch 70 | loss: 0.17179 | val_0_rmse: 0.39685 | val_1_rmse: 0.44954 |  0:01:19s
epoch 71 | loss: 0.17206 | val_0_rmse: 0.40523 | val_1_rmse: 0.45918 |  0:01:20s
epoch 72 | loss: 0.17493 | val_0_rmse: 0.40046 | val_1_rmse: 0.45375 |  0:01:21s
epoch 73 | loss: 0.17176 | val_0_rmse: 0.40023 | val_1_rmse: 0.44943 |  0:01:22s
epoch 74 | loss: 0.17051 | val_0_rmse: 0.39153 | val_1_rmse: 0.44654 |  0:01:23s
epoch 75 | loss: 0.1721  | val_0_rmse: 0.39081 | val_1_rmse: 0.45063 |  0:01:24s
epoch 76 | loss: 0.17047 | val_0_rmse: 0.3935  | val_1_rmse: 0.45343 |  0:01:25s
epoch 77 | loss: 0.17092 | val_0_rmse: 0.40415 | val_1_rmse: 0.46218 |  0:01:26s
epoch 78 | loss: 0.17031 | val_0_rmse: 0.39284 | val_1_rmse: 0.453   |  0:01:28s
epoch 79 | loss: 0.16715 | val_0_rmse: 0.39462 | val_1_rmse: 0.45708 |  0:01:29s

Early stopping occured at epoch 79 with best_epoch = 49 and best_val_1_rmse = 0.44081
Best weights from best epoch are automatically used!
ended training at: 07:55:16
Feature importance:
Mean squared error is of 5951158275.5528145
Mean absolute error:54700.59535800772
MAPE:0.1432959128385873
R2 score:0.8069702538596788
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:55:17
epoch 0  | loss: 1.93571 | val_0_rmse: 1.00528 | val_1_rmse: 0.9886  |  0:00:00s
epoch 1  | loss: 1.09713 | val_0_rmse: 1.00437 | val_1_rmse: 0.98808 |  0:00:01s
epoch 2  | loss: 1.03314 | val_0_rmse: 1.00529 | val_1_rmse: 0.98881 |  0:00:01s
epoch 3  | loss: 1.00345 | val_0_rmse: 1.00537 | val_1_rmse: 0.98818 |  0:00:02s
epoch 4  | loss: 0.99383 | val_0_rmse: 1.00207 | val_1_rmse: 0.9858  |  0:00:03s
epoch 5  | loss: 0.98043 | val_0_rmse: 0.99129 | val_1_rmse: 0.97361 |  0:00:03s
epoch 6  | loss: 0.95013 | val_0_rmse: 0.95619 | val_1_rmse: 0.939   |  0:00:04s
epoch 7  | loss: 0.87436 | val_0_rmse: 0.86433 | val_1_rmse: 0.86226 |  0:00:04s
epoch 8  | loss: 0.78358 | val_0_rmse: 0.84217 | val_1_rmse: 0.85238 |  0:00:05s
epoch 9  | loss: 0.68024 | val_0_rmse: 0.85568 | val_1_rmse: 0.87968 |  0:00:06s
epoch 10 | loss: 0.62415 | val_0_rmse: 0.79185 | val_1_rmse: 0.8226  |  0:00:06s
epoch 11 | loss: 0.56759 | val_0_rmse: 0.81925 | val_1_rmse: 0.85655 |  0:00:07s
epoch 12 | loss: 0.50721 | val_0_rmse: 0.86223 | val_1_rmse: 0.9028  |  0:00:07s
epoch 13 | loss: 0.4574  | val_0_rmse: 0.78883 | val_1_rmse: 0.82876 |  0:00:08s
epoch 14 | loss: 0.40708 | val_0_rmse: 0.76423 | val_1_rmse: 0.79767 |  0:00:09s
epoch 15 | loss: 0.36943 | val_0_rmse: 0.7602  | val_1_rmse: 0.78902 |  0:00:09s
epoch 16 | loss: 0.34601 | val_0_rmse: 0.74378 | val_1_rmse: 0.77591 |  0:00:10s
epoch 17 | loss: 0.32841 | val_0_rmse: 0.73054 | val_1_rmse: 0.75688 |  0:00:11s
epoch 18 | loss: 0.31175 | val_0_rmse: 0.74029 | val_1_rmse: 0.76529 |  0:00:11s
epoch 19 | loss: 0.30134 | val_0_rmse: 0.72001 | val_1_rmse: 0.74698 |  0:00:12s
epoch 20 | loss: 0.28951 | val_0_rmse: 0.74406 | val_1_rmse: 0.78301 |  0:00:12s
epoch 21 | loss: 0.2829  | val_0_rmse: 0.75994 | val_1_rmse: 0.79658 |  0:00:13s
epoch 22 | loss: 0.27999 | val_0_rmse: 0.72856 | val_1_rmse: 0.75519 |  0:00:14s
epoch 23 | loss: 0.26104 | val_0_rmse: 0.73704 | val_1_rmse: 0.76851 |  0:00:14s
epoch 24 | loss: 0.26539 | val_0_rmse: 0.73757 | val_1_rmse: 0.76977 |  0:00:15s
epoch 25 | loss: 0.26064 | val_0_rmse: 0.74234 | val_1_rmse: 0.7704  |  0:00:15s
epoch 26 | loss: 0.25075 | val_0_rmse: 0.71751 | val_1_rmse: 0.74488 |  0:00:16s
epoch 27 | loss: 0.24779 | val_0_rmse: 0.71837 | val_1_rmse: 0.7509  |  0:00:17s
epoch 28 | loss: 0.23721 | val_0_rmse: 0.70464 | val_1_rmse: 0.73549 |  0:00:17s
epoch 29 | loss: 0.23001 | val_0_rmse: 0.69861 | val_1_rmse: 0.72652 |  0:00:18s
epoch 30 | loss: 0.23313 | val_0_rmse: 0.70948 | val_1_rmse: 0.73915 |  0:00:19s
epoch 31 | loss: 0.23354 | val_0_rmse: 0.68597 | val_1_rmse: 0.71184 |  0:00:19s
epoch 32 | loss: 0.22351 | val_0_rmse: 0.69823 | val_1_rmse: 0.72986 |  0:00:20s
epoch 33 | loss: 0.22629 | val_0_rmse: 0.68829 | val_1_rmse: 0.71657 |  0:00:20s
epoch 34 | loss: 0.22416 | val_0_rmse: 0.66716 | val_1_rmse: 0.69503 |  0:00:21s
epoch 35 | loss: 0.23083 | val_0_rmse: 0.67994 | val_1_rmse: 0.71114 |  0:00:22s
epoch 36 | loss: 0.22059 | val_0_rmse: 0.65996 | val_1_rmse: 0.68447 |  0:00:22s
epoch 37 | loss: 0.21797 | val_0_rmse: 0.67014 | val_1_rmse: 0.70309 |  0:00:23s
epoch 38 | loss: 0.21473 | val_0_rmse: 0.64707 | val_1_rmse: 0.6764  |  0:00:23s
epoch 39 | loss: 0.2129  | val_0_rmse: 0.64919 | val_1_rmse: 0.67638 |  0:00:24s
epoch 40 | loss: 0.21208 | val_0_rmse: 0.65356 | val_1_rmse: 0.68302 |  0:00:25s
epoch 41 | loss: 0.20781 | val_0_rmse: 0.64105 | val_1_rmse: 0.6654  |  0:00:25s
epoch 42 | loss: 0.20727 | val_0_rmse: 0.63082 | val_1_rmse: 0.66026 |  0:00:26s
epoch 43 | loss: 0.20322 | val_0_rmse: 0.60925 | val_1_rmse: 0.63401 |  0:00:27s
epoch 44 | loss: 0.20348 | val_0_rmse: 0.63001 | val_1_rmse: 0.65848 |  0:00:27s
epoch 45 | loss: 0.20668 | val_0_rmse: 0.6169  | val_1_rmse: 0.64402 |  0:00:28s
epoch 46 | loss: 0.20068 | val_0_rmse: 0.60325 | val_1_rmse: 0.62823 |  0:00:28s
epoch 47 | loss: 0.19845 | val_0_rmse: 0.59551 | val_1_rmse: 0.62504 |  0:00:29s
epoch 48 | loss: 0.19426 | val_0_rmse: 0.59209 | val_1_rmse: 0.61935 |  0:00:30s
epoch 49 | loss: 0.20032 | val_0_rmse: 0.59354 | val_1_rmse: 0.61957 |  0:00:30s
epoch 50 | loss: 0.19666 | val_0_rmse: 0.60104 | val_1_rmse: 0.63312 |  0:00:31s
epoch 51 | loss: 0.19628 | val_0_rmse: 0.59205 | val_1_rmse: 0.6215  |  0:00:31s
epoch 52 | loss: 0.19792 | val_0_rmse: 0.57529 | val_1_rmse: 0.60102 |  0:00:32s
epoch 53 | loss: 0.18964 | val_0_rmse: 0.58181 | val_1_rmse: 0.61247 |  0:00:33s
epoch 54 | loss: 0.18883 | val_0_rmse: 0.56014 | val_1_rmse: 0.59292 |  0:00:33s
epoch 55 | loss: 0.19262 | val_0_rmse: 0.54762 | val_1_rmse: 0.58173 |  0:00:34s
epoch 56 | loss: 0.19253 | val_0_rmse: 0.56015 | val_1_rmse: 0.59545 |  0:00:35s
epoch 57 | loss: 0.19171 | val_0_rmse: 0.54831 | val_1_rmse: 0.58181 |  0:00:35s
epoch 58 | loss: 0.18884 | val_0_rmse: 0.53789 | val_1_rmse: 0.57183 |  0:00:36s
epoch 59 | loss: 0.18619 | val_0_rmse: 0.53288 | val_1_rmse: 0.57225 |  0:00:36s
epoch 60 | loss: 0.18878 | val_0_rmse: 0.51749 | val_1_rmse: 0.55247 |  0:00:37s
epoch 61 | loss: 0.18519 | val_0_rmse: 0.52324 | val_1_rmse: 0.56796 |  0:00:38s
epoch 62 | loss: 0.18986 | val_0_rmse: 0.52088 | val_1_rmse: 0.56079 |  0:00:38s
epoch 63 | loss: 0.18676 | val_0_rmse: 0.49778 | val_1_rmse: 0.53867 |  0:00:39s
epoch 64 | loss: 0.18234 | val_0_rmse: 0.51744 | val_1_rmse: 0.56717 |  0:00:39s
epoch 65 | loss: 0.18331 | val_0_rmse: 0.48857 | val_1_rmse: 0.53614 |  0:00:40s
epoch 66 | loss: 0.1808  | val_0_rmse: 0.49345 | val_1_rmse: 0.54803 |  0:00:41s
epoch 67 | loss: 0.17874 | val_0_rmse: 0.48086 | val_1_rmse: 0.52773 |  0:00:41s
epoch 68 | loss: 0.18389 | val_0_rmse: 0.48316 | val_1_rmse: 0.54594 |  0:00:42s
epoch 69 | loss: 0.18065 | val_0_rmse: 0.46782 | val_1_rmse: 0.5281  |  0:00:43s
epoch 70 | loss: 0.18054 | val_0_rmse: 0.46973 | val_1_rmse: 0.52665 |  0:00:43s
epoch 71 | loss: 0.18251 | val_0_rmse: 0.47762 | val_1_rmse: 0.54581 |  0:00:44s
epoch 72 | loss: 0.17571 | val_0_rmse: 0.45901 | val_1_rmse: 0.52284 |  0:00:44s
epoch 73 | loss: 0.17464 | val_0_rmse: 0.45447 | val_1_rmse: 0.5227  |  0:00:45s
epoch 74 | loss: 0.17516 | val_0_rmse: 0.44379 | val_1_rmse: 0.51195 |  0:00:46s
epoch 75 | loss: 0.17192 | val_0_rmse: 0.43895 | val_1_rmse: 0.50473 |  0:00:46s
epoch 76 | loss: 0.17181 | val_0_rmse: 0.43669 | val_1_rmse: 0.51045 |  0:00:47s
epoch 77 | loss: 0.17221 | val_0_rmse: 0.43063 | val_1_rmse: 0.49744 |  0:00:47s
epoch 78 | loss: 0.1693  | val_0_rmse: 0.42697 | val_1_rmse: 0.49954 |  0:00:48s
epoch 79 | loss: 0.1693  | val_0_rmse: 0.41504 | val_1_rmse: 0.5037  |  0:00:49s
epoch 80 | loss: 0.17036 | val_0_rmse: 0.41679 | val_1_rmse: 0.50138 |  0:00:49s
epoch 81 | loss: 0.1665  | val_0_rmse: 0.41466 | val_1_rmse: 0.50329 |  0:00:50s
epoch 82 | loss: 0.16181 | val_0_rmse: 0.41119 | val_1_rmse: 0.49352 |  0:00:50s
epoch 83 | loss: 0.16433 | val_0_rmse: 0.40519 | val_1_rmse: 0.49147 |  0:00:51s
epoch 84 | loss: 0.16147 | val_0_rmse: 0.41155 | val_1_rmse: 0.49992 |  0:00:52s
epoch 85 | loss: 0.16253 | val_0_rmse: 0.41185 | val_1_rmse: 0.50257 |  0:00:52s
epoch 86 | loss: 0.16206 | val_0_rmse: 0.39465 | val_1_rmse: 0.48942 |  0:00:53s
epoch 87 | loss: 0.16483 | val_0_rmse: 0.38967 | val_1_rmse: 0.48994 |  0:00:54s
epoch 88 | loss: 0.15984 | val_0_rmse: 0.38517 | val_1_rmse: 0.4811  |  0:00:54s
epoch 89 | loss: 0.16595 | val_0_rmse: 0.3979  | val_1_rmse: 0.49022 |  0:00:55s
epoch 90 | loss: 0.16497 | val_0_rmse: 0.38152 | val_1_rmse: 0.48014 |  0:00:55s
epoch 91 | loss: 0.16069 | val_0_rmse: 0.38863 | val_1_rmse: 0.48042 |  0:00:56s
epoch 92 | loss: 0.15434 | val_0_rmse: 0.38872 | val_1_rmse: 0.49735 |  0:00:57s
epoch 93 | loss: 0.1643  | val_0_rmse: 0.37277 | val_1_rmse: 0.47171 |  0:00:57s
epoch 94 | loss: 0.15664 | val_0_rmse: 0.37226 | val_1_rmse: 0.47636 |  0:00:58s
epoch 95 | loss: 0.15938 | val_0_rmse: 0.3756  | val_1_rmse: 0.48089 |  0:00:58s
epoch 96 | loss: 0.15715 | val_0_rmse: 0.37449 | val_1_rmse: 0.48561 |  0:00:59s
epoch 97 | loss: 0.15819 | val_0_rmse: 0.37701 | val_1_rmse: 0.47402 |  0:01:00s
epoch 98 | loss: 0.15712 | val_0_rmse: 0.37475 | val_1_rmse: 0.48062 |  0:01:00s
epoch 99 | loss: 0.1547  | val_0_rmse: 0.37211 | val_1_rmse: 0.4798  |  0:01:01s
epoch 100| loss: 0.15113 | val_0_rmse: 0.37407 | val_1_rmse: 0.48711 |  0:01:01s
epoch 101| loss: 0.15079 | val_0_rmse: 0.36672 | val_1_rmse: 0.47548 |  0:01:02s
epoch 102| loss: 0.15271 | val_0_rmse: 0.36777 | val_1_rmse: 0.48715 |  0:01:03s
epoch 103| loss: 0.1463  | val_0_rmse: 0.37012 | val_1_rmse: 0.49931 |  0:01:03s
epoch 104| loss: 0.1481  | val_0_rmse: 0.36566 | val_1_rmse: 0.48955 |  0:01:04s
epoch 105| loss: 0.14911 | val_0_rmse: 0.35569 | val_1_rmse: 0.48441 |  0:01:05s
epoch 106| loss: 0.14403 | val_0_rmse: 0.35959 | val_1_rmse: 0.48793 |  0:01:05s
epoch 107| loss: 0.14015 | val_0_rmse: 0.3604  | val_1_rmse: 0.49268 |  0:01:06s
epoch 108| loss: 0.14734 | val_0_rmse: 0.34788 | val_1_rmse: 0.47619 |  0:01:06s
epoch 109| loss: 0.14506 | val_0_rmse: 0.34888 | val_1_rmse: 0.4814  |  0:01:07s
epoch 110| loss: 0.14285 | val_0_rmse: 0.34561 | val_1_rmse: 0.47979 |  0:01:08s
epoch 111| loss: 0.13756 | val_0_rmse: 0.34504 | val_1_rmse: 0.48126 |  0:01:08s
epoch 112| loss: 0.13951 | val_0_rmse: 0.34839 | val_1_rmse: 0.47925 |  0:01:09s
epoch 113| loss: 0.13723 | val_0_rmse: 0.34494 | val_1_rmse: 0.4841  |  0:01:09s
epoch 114| loss: 0.13839 | val_0_rmse: 0.34108 | val_1_rmse: 0.48928 |  0:01:10s
epoch 115| loss: 0.13773 | val_0_rmse: 0.34659 | val_1_rmse: 0.47692 |  0:01:11s
epoch 116| loss: 0.13895 | val_0_rmse: 0.34304 | val_1_rmse: 0.48768 |  0:01:11s
epoch 117| loss: 0.13605 | val_0_rmse: 0.34039 | val_1_rmse: 0.48432 |  0:01:12s
epoch 118| loss: 0.14177 | val_0_rmse: 0.35349 | val_1_rmse: 0.49509 |  0:01:12s
epoch 119| loss: 0.13516 | val_0_rmse: 0.33412 | val_1_rmse: 0.49021 |  0:01:13s
epoch 120| loss: 0.13339 | val_0_rmse: 0.33455 | val_1_rmse: 0.48199 |  0:01:14s
epoch 121| loss: 0.13597 | val_0_rmse: 0.33823 | val_1_rmse: 0.4886  |  0:01:14s
epoch 122| loss: 0.13965 | val_0_rmse: 0.33345 | val_1_rmse: 0.47995 |  0:01:15s
epoch 123| loss: 0.13288 | val_0_rmse: 0.33563 | val_1_rmse: 0.49026 |  0:01:16s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.47171
Best weights from best epoch are automatically used!
ended training at: 07:56:33
Feature importance:
Mean squared error is of 21611653156.007153
Mean absolute error:105676.76218441116
MAPE:0.17189564773923346
R2 score:0.7417820886711798
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:56:33
epoch 0  | loss: 2.22416 | val_0_rmse: 0.99327 | val_1_rmse: 1.02094 |  0:00:00s
epoch 1  | loss: 1.13586 | val_0_rmse: 0.99466 | val_1_rmse: 1.0236  |  0:00:01s
epoch 2  | loss: 1.01888 | val_0_rmse: 0.99543 | val_1_rmse: 1.02525 |  0:00:01s
epoch 3  | loss: 0.969   | val_0_rmse: 0.97329 | val_1_rmse: 0.99292 |  0:00:02s
epoch 4  | loss: 0.93012 | val_0_rmse: 0.94582 | val_1_rmse: 0.94405 |  0:00:03s
epoch 5  | loss: 0.8745  | val_0_rmse: 0.94475 | val_1_rmse: 0.93259 |  0:00:03s
epoch 6  | loss: 0.83895 | val_0_rmse: 0.92221 | val_1_rmse: 0.9215  |  0:00:04s
epoch 7  | loss: 0.81328 | val_0_rmse: 0.91558 | val_1_rmse: 0.91176 |  0:00:04s
epoch 8  | loss: 0.76665 | val_0_rmse: 0.90266 | val_1_rmse: 0.89578 |  0:00:05s
epoch 9  | loss: 0.69437 | val_0_rmse: 0.89106 | val_1_rmse: 0.8934  |  0:00:06s
epoch 10 | loss: 0.599   | val_0_rmse: 0.83558 | val_1_rmse: 0.83297 |  0:00:06s
epoch 11 | loss: 0.5335  | val_0_rmse: 0.84355 | val_1_rmse: 0.83068 |  0:00:07s
epoch 12 | loss: 0.46874 | val_0_rmse: 0.84407 | val_1_rmse: 0.82863 |  0:00:08s
epoch 13 | loss: 0.40995 | val_0_rmse: 0.81319 | val_1_rmse: 0.80963 |  0:00:08s
epoch 14 | loss: 0.37082 | val_0_rmse: 0.81928 | val_1_rmse: 0.82575 |  0:00:09s
epoch 15 | loss: 0.33762 | val_0_rmse: 0.85796 | val_1_rmse: 0.86583 |  0:00:09s
epoch 16 | loss: 0.30607 | val_0_rmse: 0.79481 | val_1_rmse: 0.80808 |  0:00:10s
epoch 17 | loss: 0.28419 | val_0_rmse: 0.78332 | val_1_rmse: 0.80092 |  0:00:11s
epoch 18 | loss: 0.27609 | val_0_rmse: 0.78857 | val_1_rmse: 0.80238 |  0:00:11s
epoch 19 | loss: 0.25742 | val_0_rmse: 0.76425 | val_1_rmse: 0.78183 |  0:00:12s
epoch 20 | loss: 0.25356 | val_0_rmse: 0.72876 | val_1_rmse: 0.74577 |  0:00:12s
epoch 21 | loss: 0.24365 | val_0_rmse: 0.74044 | val_1_rmse: 0.75888 |  0:00:13s
epoch 22 | loss: 0.23833 | val_0_rmse: 0.72464 | val_1_rmse: 0.74264 |  0:00:14s
epoch 23 | loss: 0.23072 | val_0_rmse: 0.72149 | val_1_rmse: 0.73384 |  0:00:14s
epoch 24 | loss: 0.2316  | val_0_rmse: 0.72056 | val_1_rmse: 0.73298 |  0:00:15s
epoch 25 | loss: 0.22838 | val_0_rmse: 0.70445 | val_1_rmse: 0.71907 |  0:00:16s
epoch 26 | loss: 0.22375 | val_0_rmse: 0.7134  | val_1_rmse: 0.72567 |  0:00:16s
epoch 27 | loss: 0.2204  | val_0_rmse: 0.6912  | val_1_rmse: 0.70579 |  0:00:17s
epoch 28 | loss: 0.2119  | val_0_rmse: 0.68933 | val_1_rmse: 0.70459 |  0:00:17s
epoch 29 | loss: 0.21949 | val_0_rmse: 0.68111 | val_1_rmse: 0.69893 |  0:00:18s
epoch 30 | loss: 0.21227 | val_0_rmse: 0.67503 | val_1_rmse: 0.69075 |  0:00:19s
epoch 31 | loss: 0.21316 | val_0_rmse: 0.66072 | val_1_rmse: 0.67875 |  0:00:19s
epoch 32 | loss: 0.20441 | val_0_rmse: 0.67464 | val_1_rmse: 0.69029 |  0:00:20s
epoch 33 | loss: 0.20556 | val_0_rmse: 0.65764 | val_1_rmse: 0.67488 |  0:00:20s
epoch 34 | loss: 0.19833 | val_0_rmse: 0.67383 | val_1_rmse: 0.69128 |  0:00:21s
epoch 35 | loss: 0.19932 | val_0_rmse: 0.65452 | val_1_rmse: 0.67507 |  0:00:22s
epoch 36 | loss: 0.1953  | val_0_rmse: 0.636   | val_1_rmse: 0.65935 |  0:00:22s
epoch 37 | loss: 0.19264 | val_0_rmse: 0.65108 | val_1_rmse: 0.67146 |  0:00:23s
epoch 38 | loss: 0.20008 | val_0_rmse: 0.64126 | val_1_rmse: 0.65895 |  0:00:24s
epoch 39 | loss: 0.19367 | val_0_rmse: 0.62854 | val_1_rmse: 0.64859 |  0:00:24s
epoch 40 | loss: 0.19402 | val_0_rmse: 0.62311 | val_1_rmse: 0.6467  |  0:00:25s
epoch 41 | loss: 0.19478 | val_0_rmse: 0.62082 | val_1_rmse: 0.64562 |  0:00:25s
epoch 42 | loss: 0.19095 | val_0_rmse: 0.61642 | val_1_rmse: 0.64123 |  0:00:26s
epoch 43 | loss: 0.18913 | val_0_rmse: 0.60538 | val_1_rmse: 0.62789 |  0:00:27s
epoch 44 | loss: 0.18993 | val_0_rmse: 0.59668 | val_1_rmse: 0.62188 |  0:00:27s
epoch 45 | loss: 0.19249 | val_0_rmse: 0.60581 | val_1_rmse: 0.62898 |  0:00:28s
epoch 46 | loss: 0.18602 | val_0_rmse: 0.57802 | val_1_rmse: 0.60627 |  0:00:28s
epoch 47 | loss: 0.18807 | val_0_rmse: 0.57279 | val_1_rmse: 0.60168 |  0:00:29s
epoch 48 | loss: 0.1814  | val_0_rmse: 0.58319 | val_1_rmse: 0.60962 |  0:00:30s
epoch 49 | loss: 0.18194 | val_0_rmse: 0.59347 | val_1_rmse: 0.62425 |  0:00:30s
epoch 50 | loss: 0.18551 | val_0_rmse: 0.5597  | val_1_rmse: 0.59074 |  0:00:31s
epoch 51 | loss: 0.18    | val_0_rmse: 0.56073 | val_1_rmse: 0.59196 |  0:00:32s
epoch 52 | loss: 0.18556 | val_0_rmse: 0.55565 | val_1_rmse: 0.5898  |  0:00:32s
epoch 53 | loss: 0.17493 | val_0_rmse: 0.53508 | val_1_rmse: 0.57392 |  0:00:33s
epoch 54 | loss: 0.17882 | val_0_rmse: 0.54146 | val_1_rmse: 0.58051 |  0:00:33s
epoch 55 | loss: 0.17847 | val_0_rmse: 0.52409 | val_1_rmse: 0.56432 |  0:00:34s
epoch 56 | loss: 0.17611 | val_0_rmse: 0.52139 | val_1_rmse: 0.56359 |  0:00:35s
epoch 57 | loss: 0.17821 | val_0_rmse: 0.5291  | val_1_rmse: 0.57304 |  0:00:35s
epoch 58 | loss: 0.1706  | val_0_rmse: 0.51697 | val_1_rmse: 0.55712 |  0:00:36s
epoch 59 | loss: 0.18065 | val_0_rmse: 0.51813 | val_1_rmse: 0.56115 |  0:00:36s
epoch 60 | loss: 0.17493 | val_0_rmse: 0.5001  | val_1_rmse: 0.54809 |  0:00:37s
epoch 61 | loss: 0.17283 | val_0_rmse: 0.50778 | val_1_rmse: 0.55876 |  0:00:38s
epoch 62 | loss: 0.17209 | val_0_rmse: 0.49213 | val_1_rmse: 0.54179 |  0:00:38s
epoch 63 | loss: 0.17585 | val_0_rmse: 0.51245 | val_1_rmse: 0.56825 |  0:00:39s
epoch 64 | loss: 0.17413 | val_0_rmse: 0.48824 | val_1_rmse: 0.54156 |  0:00:40s
epoch 65 | loss: 0.17465 | val_0_rmse: 0.4786  | val_1_rmse: 0.53644 |  0:00:40s
epoch 66 | loss: 0.1712  | val_0_rmse: 0.47806 | val_1_rmse: 0.53282 |  0:00:41s
epoch 67 | loss: 0.1674  | val_0_rmse: 0.47284 | val_1_rmse: 0.53369 |  0:00:41s
epoch 68 | loss: 0.17118 | val_0_rmse: 0.46223 | val_1_rmse: 0.5246  |  0:00:42s
epoch 69 | loss: 0.17084 | val_0_rmse: 0.47186 | val_1_rmse: 0.53814 |  0:00:43s
epoch 70 | loss: 0.17268 | val_0_rmse: 0.45198 | val_1_rmse: 0.52049 |  0:00:43s
epoch 71 | loss: 0.17065 | val_0_rmse: 0.44701 | val_1_rmse: 0.51822 |  0:00:44s
epoch 72 | loss: 0.1674  | val_0_rmse: 0.44567 | val_1_rmse: 0.51654 |  0:00:45s
epoch 73 | loss: 0.16147 | val_0_rmse: 0.43222 | val_1_rmse: 0.50754 |  0:00:45s
epoch 74 | loss: 0.16821 | val_0_rmse: 0.43462 | val_1_rmse: 0.51248 |  0:00:46s
epoch 75 | loss: 0.16705 | val_0_rmse: 0.43866 | val_1_rmse: 0.51832 |  0:00:46s
epoch 76 | loss: 0.16938 | val_0_rmse: 0.42324 | val_1_rmse: 0.51229 |  0:00:47s
epoch 77 | loss: 0.16485 | val_0_rmse: 0.42136 | val_1_rmse: 0.50971 |  0:00:48s
epoch 78 | loss: 0.16938 | val_0_rmse: 0.42037 | val_1_rmse: 0.50893 |  0:00:48s
epoch 79 | loss: 0.16867 | val_0_rmse: 0.42419 | val_1_rmse: 0.5188  |  0:00:49s
epoch 80 | loss: 0.16485 | val_0_rmse: 0.40741 | val_1_rmse: 0.5014  |  0:00:49s
epoch 81 | loss: 0.16693 | val_0_rmse: 0.41265 | val_1_rmse: 0.50576 |  0:00:50s
epoch 82 | loss: 0.16847 | val_0_rmse: 0.41768 | val_1_rmse: 0.51735 |  0:00:51s
epoch 83 | loss: 0.16569 | val_0_rmse: 0.40266 | val_1_rmse: 0.4968  |  0:00:51s
epoch 84 | loss: 0.16806 | val_0_rmse: 0.40363 | val_1_rmse: 0.50261 |  0:00:52s
epoch 85 | loss: 0.16013 | val_0_rmse: 0.39499 | val_1_rmse: 0.50441 |  0:00:53s
epoch 86 | loss: 0.15812 | val_0_rmse: 0.39157 | val_1_rmse: 0.50456 |  0:00:53s
epoch 87 | loss: 0.15973 | val_0_rmse: 0.40161 | val_1_rmse: 0.51039 |  0:00:54s
epoch 88 | loss: 0.15782 | val_0_rmse: 0.39164 | val_1_rmse: 0.5069  |  0:00:54s
epoch 89 | loss: 0.15913 | val_0_rmse: 0.40109 | val_1_rmse: 0.51691 |  0:00:55s
epoch 90 | loss: 0.15474 | val_0_rmse: 0.38333 | val_1_rmse: 0.5031  |  0:00:56s
epoch 91 | loss: 0.15233 | val_0_rmse: 0.37784 | val_1_rmse: 0.49999 |  0:00:56s
epoch 92 | loss: 0.1519  | val_0_rmse: 0.39394 | val_1_rmse: 0.5215  |  0:00:57s
epoch 93 | loss: 0.15799 | val_0_rmse: 0.3782  | val_1_rmse: 0.49882 |  0:00:57s
epoch 94 | loss: 0.15566 | val_0_rmse: 0.36674 | val_1_rmse: 0.49988 |  0:00:58s
epoch 95 | loss: 0.15239 | val_0_rmse: 0.37665 | val_1_rmse: 0.50881 |  0:00:59s
epoch 96 | loss: 0.14808 | val_0_rmse: 0.37592 | val_1_rmse: 0.50619 |  0:00:59s
epoch 97 | loss: 0.15154 | val_0_rmse: 0.38241 | val_1_rmse: 0.51865 |  0:01:00s
epoch 98 | loss: 0.16006 | val_0_rmse: 0.37574 | val_1_rmse: 0.51046 |  0:01:00s
epoch 99 | loss: 0.15386 | val_0_rmse: 0.36113 | val_1_rmse: 0.4994  |  0:01:01s
epoch 100| loss: 0.14817 | val_0_rmse: 0.36234 | val_1_rmse: 0.49888 |  0:01:02s
epoch 101| loss: 0.14921 | val_0_rmse: 0.36022 | val_1_rmse: 0.50461 |  0:01:02s
epoch 102| loss: 0.14331 | val_0_rmse: 0.35427 | val_1_rmse: 0.50092 |  0:01:03s
epoch 103| loss: 0.14723 | val_0_rmse: 0.36477 | val_1_rmse: 0.50809 |  0:01:04s
epoch 104| loss: 0.15078 | val_0_rmse: 0.35199 | val_1_rmse: 0.49946 |  0:01:04s
epoch 105| loss: 0.14444 | val_0_rmse: 0.35722 | val_1_rmse: 0.51502 |  0:01:05s
epoch 106| loss: 0.14528 | val_0_rmse: 0.34697 | val_1_rmse: 0.49934 |  0:01:05s
epoch 107| loss: 0.14407 | val_0_rmse: 0.34686 | val_1_rmse: 0.50728 |  0:01:06s
epoch 108| loss: 0.13651 | val_0_rmse: 0.34283 | val_1_rmse: 0.50768 |  0:01:07s
epoch 109| loss: 0.14076 | val_0_rmse: 0.34542 | val_1_rmse: 0.51245 |  0:01:07s
epoch 110| loss: 0.14549 | val_0_rmse: 0.34232 | val_1_rmse: 0.50554 |  0:01:08s
epoch 111| loss: 0.1441  | val_0_rmse: 0.34912 | val_1_rmse: 0.51927 |  0:01:08s
epoch 112| loss: 0.14066 | val_0_rmse: 0.35093 | val_1_rmse: 0.50753 |  0:01:09s
epoch 113| loss: 0.14116 | val_0_rmse: 0.34549 | val_1_rmse: 0.52533 |  0:01:10s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.4968
Best weights from best epoch are automatically used!
ended training at: 07:57:44
Feature importance:
Mean squared error is of 22975801738.146957
Mean absolute error:107711.7251936777
MAPE:0.16691857613602157
R2 score:0.7303396998271663
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:57:44
epoch 0  | loss: 2.09028 | val_0_rmse: 0.99754 | val_1_rmse: 1.0045  |  0:00:00s
epoch 1  | loss: 1.11671 | val_0_rmse: 0.99764 | val_1_rmse: 1.00473 |  0:00:01s
epoch 2  | loss: 0.99552 | val_0_rmse: 0.99606 | val_1_rmse: 1.00348 |  0:00:01s
epoch 3  | loss: 0.97363 | val_0_rmse: 0.9926  | val_1_rmse: 0.99968 |  0:00:02s
epoch 4  | loss: 0.96336 | val_0_rmse: 0.98853 | val_1_rmse: 0.99421 |  0:00:03s
epoch 5  | loss: 0.9583  | val_0_rmse: 0.97563 | val_1_rmse: 0.98018 |  0:00:03s
epoch 6  | loss: 0.89863 | val_0_rmse: 0.89912 | val_1_rmse: 0.89307 |  0:00:04s
epoch 7  | loss: 0.75843 | val_0_rmse: 0.88637 | val_1_rmse: 0.87556 |  0:00:04s
epoch 8  | loss: 0.6454  | val_0_rmse: 0.95214 | val_1_rmse: 0.93637 |  0:00:05s
epoch 9  | loss: 0.52953 | val_0_rmse: 0.91426 | val_1_rmse: 0.89267 |  0:00:06s
epoch 10 | loss: 0.45215 | val_0_rmse: 0.89865 | val_1_rmse: 0.88194 |  0:00:06s
epoch 11 | loss: 0.39653 | val_0_rmse: 0.8634  | val_1_rmse: 0.84192 |  0:00:07s
epoch 12 | loss: 0.34696 | val_0_rmse: 0.83618 | val_1_rmse: 0.80873 |  0:00:07s
epoch 13 | loss: 0.32013 | val_0_rmse: 0.81975 | val_1_rmse: 0.79127 |  0:00:08s
epoch 14 | loss: 0.30256 | val_0_rmse: 0.80296 | val_1_rmse: 0.77568 |  0:00:09s
epoch 15 | loss: 0.28685 | val_0_rmse: 0.79449 | val_1_rmse: 0.76504 |  0:00:09s
epoch 16 | loss: 0.27644 | val_0_rmse: 0.7976  | val_1_rmse: 0.76736 |  0:00:10s
epoch 17 | loss: 0.26562 | val_0_rmse: 0.775   | val_1_rmse: 0.74706 |  0:00:11s
epoch 18 | loss: 0.25759 | val_0_rmse: 0.78492 | val_1_rmse: 0.75604 |  0:00:11s
epoch 19 | loss: 0.24671 | val_0_rmse: 0.77534 | val_1_rmse: 0.74721 |  0:00:12s
epoch 20 | loss: 0.24155 | val_0_rmse: 0.76172 | val_1_rmse: 0.73297 |  0:00:12s
epoch 21 | loss: 0.24135 | val_0_rmse: 0.77358 | val_1_rmse: 0.74421 |  0:00:13s
epoch 22 | loss: 0.23093 | val_0_rmse: 0.75814 | val_1_rmse: 0.73086 |  0:00:14s
epoch 23 | loss: 0.23157 | val_0_rmse: 0.76245 | val_1_rmse: 0.73429 |  0:00:14s
epoch 24 | loss: 0.22194 | val_0_rmse: 0.75081 | val_1_rmse: 0.72338 |  0:00:15s
epoch 25 | loss: 0.22577 | val_0_rmse: 0.7456  | val_1_rmse: 0.71857 |  0:00:16s
epoch 26 | loss: 0.21775 | val_0_rmse: 0.74538 | val_1_rmse: 0.72033 |  0:00:16s
epoch 27 | loss: 0.21485 | val_0_rmse: 0.73191 | val_1_rmse: 0.70763 |  0:00:17s
epoch 28 | loss: 0.21571 | val_0_rmse: 0.72921 | val_1_rmse: 0.70656 |  0:00:17s
epoch 29 | loss: 0.21256 | val_0_rmse: 0.72054 | val_1_rmse: 0.69859 |  0:00:18s
epoch 30 | loss: 0.20812 | val_0_rmse: 0.72451 | val_1_rmse: 0.70419 |  0:00:19s
epoch 31 | loss: 0.21435 | val_0_rmse: 0.71049 | val_1_rmse: 0.69046 |  0:00:19s
epoch 32 | loss: 0.20546 | val_0_rmse: 0.71259 | val_1_rmse: 0.69263 |  0:00:20s
epoch 33 | loss: 0.20364 | val_0_rmse: 0.71089 | val_1_rmse: 0.69493 |  0:00:20s
epoch 34 | loss: 0.20301 | val_0_rmse: 0.69012 | val_1_rmse: 0.67578 |  0:00:21s
epoch 35 | loss: 0.20673 | val_0_rmse: 0.67936 | val_1_rmse: 0.66344 |  0:00:22s
epoch 36 | loss: 0.1996  | val_0_rmse: 0.68632 | val_1_rmse: 0.67062 |  0:00:22s
epoch 37 | loss: 0.20344 | val_0_rmse: 0.67822 | val_1_rmse: 0.6658  |  0:00:23s
epoch 38 | loss: 0.20359 | val_0_rmse: 0.67331 | val_1_rmse: 0.65884 |  0:00:23s
epoch 39 | loss: 0.19987 | val_0_rmse: 0.66517 | val_1_rmse: 0.64951 |  0:00:24s
epoch 40 | loss: 0.19077 | val_0_rmse: 0.66382 | val_1_rmse: 0.6488  |  0:00:25s
epoch 41 | loss: 0.19369 | val_0_rmse: 0.64668 | val_1_rmse: 0.63393 |  0:00:25s
epoch 42 | loss: 0.19628 | val_0_rmse: 0.64785 | val_1_rmse: 0.63567 |  0:00:26s
epoch 43 | loss: 0.19457 | val_0_rmse: 0.65002 | val_1_rmse: 0.63781 |  0:00:27s
epoch 44 | loss: 0.19039 | val_0_rmse: 0.62502 | val_1_rmse: 0.61354 |  0:00:27s
epoch 45 | loss: 0.19402 | val_0_rmse: 0.64449 | val_1_rmse: 0.63543 |  0:00:28s
epoch 46 | loss: 0.19539 | val_0_rmse: 0.63511 | val_1_rmse: 0.62537 |  0:00:28s
epoch 47 | loss: 0.18883 | val_0_rmse: 0.60672 | val_1_rmse: 0.59954 |  0:00:29s
epoch 48 | loss: 0.18701 | val_0_rmse: 0.60342 | val_1_rmse: 0.59821 |  0:00:30s
epoch 49 | loss: 0.18481 | val_0_rmse: 0.59277 | val_1_rmse: 0.58918 |  0:00:30s
epoch 50 | loss: 0.17868 | val_0_rmse: 0.59234 | val_1_rmse: 0.58967 |  0:00:31s
epoch 51 | loss: 0.18145 | val_0_rmse: 0.59178 | val_1_rmse: 0.59007 |  0:00:31s
epoch 52 | loss: 0.17577 | val_0_rmse: 0.57543 | val_1_rmse: 0.57649 |  0:00:32s
epoch 53 | loss: 0.18143 | val_0_rmse: 0.57395 | val_1_rmse: 0.57447 |  0:00:33s
epoch 54 | loss: 0.18444 | val_0_rmse: 0.56816 | val_1_rmse: 0.57909 |  0:00:33s
epoch 55 | loss: 0.1877  | val_0_rmse: 0.56842 | val_1_rmse: 0.57447 |  0:00:34s
epoch 56 | loss: 0.17801 | val_0_rmse: 0.54532 | val_1_rmse: 0.55544 |  0:00:34s
epoch 57 | loss: 0.17539 | val_0_rmse: 0.56539 | val_1_rmse: 0.57333 |  0:00:35s
epoch 58 | loss: 0.17672 | val_0_rmse: 0.54325 | val_1_rmse: 0.55915 |  0:00:36s
epoch 59 | loss: 0.17127 | val_0_rmse: 0.5509  | val_1_rmse: 0.57052 |  0:00:36s
epoch 60 | loss: 0.17569 | val_0_rmse: 0.53713 | val_1_rmse: 0.55478 |  0:00:37s
epoch 61 | loss: 0.16843 | val_0_rmse: 0.51094 | val_1_rmse: 0.5291  |  0:00:38s
epoch 62 | loss: 0.16695 | val_0_rmse: 0.51215 | val_1_rmse: 0.53523 |  0:00:38s
epoch 63 | loss: 0.17054 | val_0_rmse: 0.50541 | val_1_rmse: 0.53148 |  0:00:39s
epoch 64 | loss: 0.16829 | val_0_rmse: 0.49826 | val_1_rmse: 0.525   |  0:00:39s
epoch 65 | loss: 0.16712 | val_0_rmse: 0.48204 | val_1_rmse: 0.51456 |  0:00:40s
epoch 66 | loss: 0.16449 | val_0_rmse: 0.5001  | val_1_rmse: 0.53052 |  0:00:41s
epoch 67 | loss: 0.1618  | val_0_rmse: 0.49237 | val_1_rmse: 0.52966 |  0:00:41s
epoch 68 | loss: 0.16328 | val_0_rmse: 0.46774 | val_1_rmse: 0.50648 |  0:00:42s
epoch 69 | loss: 0.1639  | val_0_rmse: 0.47664 | val_1_rmse: 0.51466 |  0:00:42s
epoch 70 | loss: 0.15911 | val_0_rmse: 0.46669 | val_1_rmse: 0.51351 |  0:00:43s
epoch 71 | loss: 0.16081 | val_0_rmse: 0.44962 | val_1_rmse: 0.4959  |  0:00:44s
epoch 72 | loss: 0.15727 | val_0_rmse: 0.45019 | val_1_rmse: 0.49968 |  0:00:44s
epoch 73 | loss: 0.15423 | val_0_rmse: 0.44544 | val_1_rmse: 0.49968 |  0:00:45s
epoch 74 | loss: 0.15898 | val_0_rmse: 0.43173 | val_1_rmse: 0.49491 |  0:00:46s
epoch 75 | loss: 0.15904 | val_0_rmse: 0.43601 | val_1_rmse: 0.489   |  0:00:46s
epoch 76 | loss: 0.15644 | val_0_rmse: 0.42978 | val_1_rmse: 0.49157 |  0:00:47s
epoch 77 | loss: 0.15449 | val_0_rmse: 0.43229 | val_1_rmse: 0.49889 |  0:00:47s
epoch 78 | loss: 0.15097 | val_0_rmse: 0.40891 | val_1_rmse: 0.48339 |  0:00:48s
epoch 79 | loss: 0.15379 | val_0_rmse: 0.41355 | val_1_rmse: 0.48845 |  0:00:49s
epoch 80 | loss: 0.15268 | val_0_rmse: 0.40132 | val_1_rmse: 0.48406 |  0:00:49s
epoch 81 | loss: 0.14518 | val_0_rmse: 0.40376 | val_1_rmse: 0.4839  |  0:00:50s
epoch 82 | loss: 0.1465  | val_0_rmse: 0.39519 | val_1_rmse: 0.47725 |  0:00:50s
epoch 83 | loss: 0.14324 | val_0_rmse: 0.38517 | val_1_rmse: 0.47862 |  0:00:51s
epoch 84 | loss: 0.14039 | val_0_rmse: 0.38622 | val_1_rmse: 0.48038 |  0:00:52s
epoch 85 | loss: 0.1423  | val_0_rmse: 0.38821 | val_1_rmse: 0.48179 |  0:00:52s
epoch 86 | loss: 0.14147 | val_0_rmse: 0.37397 | val_1_rmse: 0.48016 |  0:00:53s
epoch 87 | loss: 0.14615 | val_0_rmse: 0.36891 | val_1_rmse: 0.4802  |  0:00:53s
epoch 88 | loss: 0.14089 | val_0_rmse: 0.37288 | val_1_rmse: 0.47613 |  0:00:54s
epoch 89 | loss: 0.13948 | val_0_rmse: 0.3616  | val_1_rmse: 0.47913 |  0:00:55s
epoch 90 | loss: 0.13993 | val_0_rmse: 0.36636 | val_1_rmse: 0.4731  |  0:00:55s
epoch 91 | loss: 0.14435 | val_0_rmse: 0.36607 | val_1_rmse: 0.47508 |  0:00:56s
epoch 92 | loss: 0.14134 | val_0_rmse: 0.37844 | val_1_rmse: 0.49892 |  0:00:57s
epoch 93 | loss: 0.14604 | val_0_rmse: 0.36077 | val_1_rmse: 0.47587 |  0:00:57s
epoch 94 | loss: 0.14951 | val_0_rmse: 0.35322 | val_1_rmse: 0.47816 |  0:00:58s
epoch 95 | loss: 0.1433  | val_0_rmse: 0.3451  | val_1_rmse: 0.47751 |  0:00:58s
epoch 96 | loss: 0.14027 | val_0_rmse: 0.35882 | val_1_rmse: 0.47981 |  0:00:59s
epoch 97 | loss: 0.1397  | val_0_rmse: 0.33936 | val_1_rmse: 0.47649 |  0:01:00s
epoch 98 | loss: 0.1378  | val_0_rmse: 0.34142 | val_1_rmse: 0.47744 |  0:01:00s
epoch 99 | loss: 0.13732 | val_0_rmse: 0.3383  | val_1_rmse: 0.48025 |  0:01:01s
epoch 100| loss: 0.13731 | val_0_rmse: 0.33789 | val_1_rmse: 0.47461 |  0:01:01s
epoch 101| loss: 0.13238 | val_0_rmse: 0.33786 | val_1_rmse: 0.48337 |  0:01:02s
epoch 102| loss: 0.13665 | val_0_rmse: 0.33533 | val_1_rmse: 0.47785 |  0:01:03s
epoch 103| loss: 0.13744 | val_0_rmse: 0.33382 | val_1_rmse: 0.47519 |  0:01:03s
epoch 104| loss: 0.13731 | val_0_rmse: 0.33889 | val_1_rmse: 0.47603 |  0:01:04s
epoch 105| loss: 0.13066 | val_0_rmse: 0.32528 | val_1_rmse: 0.47861 |  0:01:04s
epoch 106| loss: 0.12834 | val_0_rmse: 0.3285  | val_1_rmse: 0.47463 |  0:01:05s
epoch 107| loss: 0.12845 | val_0_rmse: 0.32661 | val_1_rmse: 0.48335 |  0:01:06s
epoch 108| loss: 0.12665 | val_0_rmse: 0.33035 | val_1_rmse: 0.48245 |  0:01:06s
epoch 109| loss: 0.12567 | val_0_rmse: 0.32005 | val_1_rmse: 0.47834 |  0:01:07s
epoch 110| loss: 0.12939 | val_0_rmse: 0.32245 | val_1_rmse: 0.48742 |  0:01:07s
epoch 111| loss: 0.13016 | val_0_rmse: 0.33497 | val_1_rmse: 0.48603 |  0:01:08s
epoch 112| loss: 0.12622 | val_0_rmse: 0.32967 | val_1_rmse: 0.48102 |  0:01:09s
epoch 113| loss: 0.13222 | val_0_rmse: 0.32233 | val_1_rmse: 0.49715 |  0:01:09s
epoch 114| loss: 0.12689 | val_0_rmse: 0.31996 | val_1_rmse: 0.4786  |  0:01:10s
epoch 115| loss: 0.1217  | val_0_rmse: 0.31504 | val_1_rmse: 0.48138 |  0:01:10s
epoch 116| loss: 0.12571 | val_0_rmse: 0.31357 | val_1_rmse: 0.4845  |  0:01:11s
epoch 117| loss: 0.12434 | val_0_rmse: 0.31348 | val_1_rmse: 0.48447 |  0:01:12s
epoch 118| loss: 0.11801 | val_0_rmse: 0.30711 | val_1_rmse: 0.4878  |  0:01:12s
epoch 119| loss: 0.12235 | val_0_rmse: 0.31608 | val_1_rmse: 0.48294 |  0:01:13s
epoch 120| loss: 0.12061 | val_0_rmse: 0.31029 | val_1_rmse: 0.48322 |  0:01:14s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.4731
Best weights from best epoch are automatically used!
ended training at: 07:58:58
Feature importance:
Mean squared error is of 21132173145.13634
Mean absolute error:103227.0363364879
MAPE:0.16974011674869938
R2 score:0.74084834639727
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:58:58
epoch 0  | loss: 1.96905 | val_0_rmse: 1.00383 | val_1_rmse: 1.01543 |  0:00:00s
epoch 1  | loss: 1.19239 | val_0_rmse: 1.0022  | val_1_rmse: 1.01502 |  0:00:01s
epoch 2  | loss: 1.03091 | val_0_rmse: 0.99815 | val_1_rmse: 1.01233 |  0:00:01s
epoch 3  | loss: 0.96352 | val_0_rmse: 0.98403 | val_1_rmse: 0.99821 |  0:00:02s
epoch 4  | loss: 0.92808 | val_0_rmse: 0.97361 | val_1_rmse: 0.98465 |  0:00:03s
epoch 5  | loss: 0.88238 | val_0_rmse: 0.94248 | val_1_rmse: 0.95576 |  0:00:03s
epoch 6  | loss: 0.81808 | val_0_rmse: 0.91383 | val_1_rmse: 0.92246 |  0:00:04s
epoch 7  | loss: 0.77137 | val_0_rmse: 0.894   | val_1_rmse: 0.90076 |  0:00:04s
epoch 8  | loss: 0.71282 | val_0_rmse: 0.87535 | val_1_rmse: 0.88259 |  0:00:05s
epoch 9  | loss: 0.66635 | val_0_rmse: 0.8743  | val_1_rmse: 0.88294 |  0:00:06s
epoch 10 | loss: 0.61763 | val_0_rmse: 0.84844 | val_1_rmse: 0.84712 |  0:00:06s
epoch 11 | loss: 0.55121 | val_0_rmse: 0.85524 | val_1_rmse: 0.84401 |  0:00:07s
epoch 12 | loss: 0.46844 | val_0_rmse: 0.89364 | val_1_rmse: 0.88224 |  0:00:08s
epoch 13 | loss: 0.4209  | val_0_rmse: 0.87006 | val_1_rmse: 0.85292 |  0:00:08s
epoch 14 | loss: 0.38655 | val_0_rmse: 0.86312 | val_1_rmse: 0.84972 |  0:00:09s
epoch 15 | loss: 0.35054 | val_0_rmse: 0.81033 | val_1_rmse: 0.793   |  0:00:09s
epoch 16 | loss: 0.32284 | val_0_rmse: 0.81321 | val_1_rmse: 0.79509 |  0:00:10s
epoch 17 | loss: 0.2994  | val_0_rmse: 0.81396 | val_1_rmse: 0.7968  |  0:00:11s
epoch 18 | loss: 0.29622 | val_0_rmse: 0.78938 | val_1_rmse: 0.77076 |  0:00:11s
epoch 19 | loss: 0.28217 | val_0_rmse: 0.79438 | val_1_rmse: 0.77478 |  0:00:12s
epoch 20 | loss: 0.27014 | val_0_rmse: 0.79254 | val_1_rmse: 0.77645 |  0:00:12s
epoch 21 | loss: 0.25188 | val_0_rmse: 0.7656  | val_1_rmse: 0.74484 |  0:00:13s
epoch 22 | loss: 0.2575  | val_0_rmse: 0.78757 | val_1_rmse: 0.76074 |  0:00:14s
epoch 23 | loss: 0.24677 | val_0_rmse: 0.75222 | val_1_rmse: 0.72727 |  0:00:14s
epoch 24 | loss: 0.23949 | val_0_rmse: 0.7113  | val_1_rmse: 0.695   |  0:00:15s
epoch 25 | loss: 0.23206 | val_0_rmse: 0.76333 | val_1_rmse: 0.74148 |  0:00:15s
epoch 26 | loss: 0.23639 | val_0_rmse: 0.69617 | val_1_rmse: 0.67699 |  0:00:16s
epoch 27 | loss: 0.22576 | val_0_rmse: 0.69516 | val_1_rmse: 0.6794  |  0:00:17s
epoch 28 | loss: 0.22272 | val_0_rmse: 0.71607 | val_1_rmse: 0.69859 |  0:00:17s
epoch 29 | loss: 0.21572 | val_0_rmse: 0.68803 | val_1_rmse: 0.67322 |  0:00:18s
epoch 30 | loss: 0.21834 | val_0_rmse: 0.70513 | val_1_rmse: 0.68692 |  0:00:18s
epoch 31 | loss: 0.2111  | val_0_rmse: 0.67809 | val_1_rmse: 0.66129 |  0:00:19s
epoch 32 | loss: 0.20616 | val_0_rmse: 0.66088 | val_1_rmse: 0.65143 |  0:00:20s
epoch 33 | loss: 0.20788 | val_0_rmse: 0.6696  | val_1_rmse: 0.65258 |  0:00:20s
epoch 34 | loss: 0.20107 | val_0_rmse: 0.63853 | val_1_rmse: 0.62406 |  0:00:21s
epoch 35 | loss: 0.19744 | val_0_rmse: 0.63859 | val_1_rmse: 0.62547 |  0:00:22s
epoch 36 | loss: 0.20009 | val_0_rmse: 0.68252 | val_1_rmse: 0.65641 |  0:00:22s
epoch 37 | loss: 0.1986  | val_0_rmse: 0.64581 | val_1_rmse: 0.63058 |  0:00:23s
epoch 38 | loss: 0.19237 | val_0_rmse: 0.64347 | val_1_rmse: 0.62963 |  0:00:23s
epoch 39 | loss: 0.19575 | val_0_rmse: 0.6292  | val_1_rmse: 0.61951 |  0:00:24s
epoch 40 | loss: 0.18984 | val_0_rmse: 0.61559 | val_1_rmse: 0.60782 |  0:00:25s
epoch 41 | loss: 0.19047 | val_0_rmse: 0.611   | val_1_rmse: 0.60279 |  0:00:25s
epoch 42 | loss: 0.18936 | val_0_rmse: 0.62571 | val_1_rmse: 0.61748 |  0:00:26s
epoch 43 | loss: 0.18791 | val_0_rmse: 0.59454 | val_1_rmse: 0.59078 |  0:00:26s
epoch 44 | loss: 0.18364 | val_0_rmse: 0.60663 | val_1_rmse: 0.59672 |  0:00:27s
epoch 45 | loss: 0.18678 | val_0_rmse: 0.59527 | val_1_rmse: 0.59364 |  0:00:28s
epoch 46 | loss: 0.18357 | val_0_rmse: 0.59539 | val_1_rmse: 0.59301 |  0:00:28s
epoch 47 | loss: 0.18466 | val_0_rmse: 0.58138 | val_1_rmse: 0.58054 |  0:00:29s
epoch 48 | loss: 0.17934 | val_0_rmse: 0.56756 | val_1_rmse: 0.56873 |  0:00:30s
epoch 49 | loss: 0.17981 | val_0_rmse: 0.56652 | val_1_rmse: 0.56804 |  0:00:30s
epoch 50 | loss: 0.17944 | val_0_rmse: 0.57564 | val_1_rmse: 0.58029 |  0:00:31s
epoch 51 | loss: 0.18027 | val_0_rmse: 0.55502 | val_1_rmse: 0.56205 |  0:00:31s
epoch 52 | loss: 0.17967 | val_0_rmse: 0.55617 | val_1_rmse: 0.56018 |  0:00:32s
epoch 53 | loss: 0.18079 | val_0_rmse: 0.53275 | val_1_rmse: 0.54244 |  0:00:33s
epoch 54 | loss: 0.17396 | val_0_rmse: 0.54451 | val_1_rmse: 0.55893 |  0:00:33s
epoch 55 | loss: 0.17679 | val_0_rmse: 0.53089 | val_1_rmse: 0.54529 |  0:00:34s
epoch 56 | loss: 0.17507 | val_0_rmse: 0.51877 | val_1_rmse: 0.53398 |  0:00:34s
epoch 57 | loss: 0.18191 | val_0_rmse: 0.51975 | val_1_rmse: 0.53509 |  0:00:35s
epoch 58 | loss: 0.1776  | val_0_rmse: 0.52013 | val_1_rmse: 0.5423  |  0:00:36s
epoch 59 | loss: 0.16942 | val_0_rmse: 0.4917  | val_1_rmse: 0.51728 |  0:00:36s
epoch 60 | loss: 0.17205 | val_0_rmse: 0.49692 | val_1_rmse: 0.52085 |  0:00:37s
epoch 61 | loss: 0.17463 | val_0_rmse: 0.51782 | val_1_rmse: 0.54944 |  0:00:38s
epoch 62 | loss: 0.17705 | val_0_rmse: 0.4943  | val_1_rmse: 0.52399 |  0:00:38s
epoch 63 | loss: 0.17751 | val_0_rmse: 0.48014 | val_1_rmse: 0.51509 |  0:00:39s
epoch 64 | loss: 0.16594 | val_0_rmse: 0.48528 | val_1_rmse: 0.51664 |  0:00:39s
epoch 65 | loss: 0.17171 | val_0_rmse: 0.46344 | val_1_rmse: 0.50964 |  0:00:40s
epoch 66 | loss: 0.17088 | val_0_rmse: 0.46023 | val_1_rmse: 0.51015 |  0:00:41s
epoch 67 | loss: 0.16972 | val_0_rmse: 0.46297 | val_1_rmse: 0.50805 |  0:00:41s
epoch 68 | loss: 0.16681 | val_0_rmse: 0.45137 | val_1_rmse: 0.50241 |  0:00:42s
epoch 69 | loss: 0.17137 | val_0_rmse: 0.49567 | val_1_rmse: 0.54096 |  0:00:42s
epoch 70 | loss: 0.17017 | val_0_rmse: 0.44735 | val_1_rmse: 0.50181 |  0:00:43s
epoch 71 | loss: 0.16543 | val_0_rmse: 0.43647 | val_1_rmse: 0.49729 |  0:00:44s
epoch 72 | loss: 0.16002 | val_0_rmse: 0.4558  | val_1_rmse: 0.52044 |  0:00:44s
epoch 73 | loss: 0.16489 | val_0_rmse: 0.42486 | val_1_rmse: 0.49043 |  0:00:45s
epoch 74 | loss: 0.16357 | val_0_rmse: 0.42701 | val_1_rmse: 0.49853 |  0:00:45s
epoch 75 | loss: 0.15939 | val_0_rmse: 0.42642 | val_1_rmse: 0.49877 |  0:00:46s
epoch 76 | loss: 0.15242 | val_0_rmse: 0.41387 | val_1_rmse: 0.49029 |  0:00:47s
epoch 77 | loss: 0.15371 | val_0_rmse: 0.4133  | val_1_rmse: 0.49208 |  0:00:47s
epoch 78 | loss: 0.15695 | val_0_rmse: 0.40673 | val_1_rmse: 0.49035 |  0:00:48s
epoch 79 | loss: 0.15283 | val_0_rmse: 0.3967  | val_1_rmse: 0.48612 |  0:00:49s
epoch 80 | loss: 0.15013 | val_0_rmse: 0.39998 | val_1_rmse: 0.48519 |  0:00:49s
epoch 81 | loss: 0.15194 | val_0_rmse: 0.38866 | val_1_rmse: 0.48494 |  0:00:50s
epoch 82 | loss: 0.15401 | val_0_rmse: 0.38861 | val_1_rmse: 0.4867  |  0:00:50s
epoch 83 | loss: 0.14948 | val_0_rmse: 0.38482 | val_1_rmse: 0.48619 |  0:00:51s
epoch 84 | loss: 0.15579 | val_0_rmse: 0.39046 | val_1_rmse: 0.49794 |  0:00:52s
epoch 85 | loss: 0.14986 | val_0_rmse: 0.37736 | val_1_rmse: 0.48281 |  0:00:52s
epoch 86 | loss: 0.14466 | val_0_rmse: 0.37125 | val_1_rmse: 0.4797  |  0:00:53s
epoch 87 | loss: 0.15037 | val_0_rmse: 0.37879 | val_1_rmse: 0.49108 |  0:00:54s
epoch 88 | loss: 0.1497  | val_0_rmse: 0.37344 | val_1_rmse: 0.48497 |  0:00:54s
epoch 89 | loss: 0.15068 | val_0_rmse: 0.37154 | val_1_rmse: 0.49187 |  0:00:55s
epoch 90 | loss: 0.15374 | val_0_rmse: 0.38361 | val_1_rmse: 0.50536 |  0:00:55s
epoch 91 | loss: 0.15049 | val_0_rmse: 0.37588 | val_1_rmse: 0.49765 |  0:00:56s
epoch 92 | loss: 0.1504  | val_0_rmse: 0.36497 | val_1_rmse: 0.49773 |  0:00:57s
epoch 93 | loss: 0.15009 | val_0_rmse: 0.36666 | val_1_rmse: 0.4991  |  0:00:57s
epoch 94 | loss: 0.15599 | val_0_rmse: 0.3734  | val_1_rmse: 0.50647 |  0:00:58s
epoch 95 | loss: 0.14541 | val_0_rmse: 0.36756 | val_1_rmse: 0.5021  |  0:00:58s
epoch 96 | loss: 0.146   | val_0_rmse: 0.3587  | val_1_rmse: 0.49268 |  0:00:59s
epoch 97 | loss: 0.14655 | val_0_rmse: 0.35518 | val_1_rmse: 0.49582 |  0:01:00s
epoch 98 | loss: 0.13916 | val_0_rmse: 0.35729 | val_1_rmse: 0.49253 |  0:01:00s
epoch 99 | loss: 0.14067 | val_0_rmse: 0.34695 | val_1_rmse: 0.4871  |  0:01:01s
epoch 100| loss: 0.13495 | val_0_rmse: 0.34837 | val_1_rmse: 0.49693 |  0:01:01s
epoch 101| loss: 0.14024 | val_0_rmse: 0.33998 | val_1_rmse: 0.49437 |  0:01:02s
epoch 102| loss: 0.14049 | val_0_rmse: 0.34716 | val_1_rmse: 0.49255 |  0:01:03s
epoch 103| loss: 0.1447  | val_0_rmse: 0.37055 | val_1_rmse: 0.52186 |  0:01:03s
epoch 104| loss: 0.14624 | val_0_rmse: 0.34476 | val_1_rmse: 0.49587 |  0:01:04s
epoch 105| loss: 0.13865 | val_0_rmse: 0.34611 | val_1_rmse: 0.49481 |  0:01:04s
epoch 106| loss: 0.13504 | val_0_rmse: 0.33935 | val_1_rmse: 0.50758 |  0:01:05s
epoch 107| loss: 0.13615 | val_0_rmse: 0.34165 | val_1_rmse: 0.49946 |  0:01:06s
epoch 108| loss: 0.13533 | val_0_rmse: 0.3273  | val_1_rmse: 0.50017 |  0:01:06s
epoch 109| loss: 0.13695 | val_0_rmse: 0.34007 | val_1_rmse: 0.49914 |  0:01:07s
epoch 110| loss: 0.12951 | val_0_rmse: 0.33135 | val_1_rmse: 0.49853 |  0:01:08s
epoch 111| loss: 0.13597 | val_0_rmse: 0.32974 | val_1_rmse: 0.50808 |  0:01:08s
epoch 112| loss: 0.12896 | val_0_rmse: 0.33513 | val_1_rmse: 0.50016 |  0:01:09s
epoch 113| loss: 0.13621 | val_0_rmse: 0.3307  | val_1_rmse: 0.50876 |  0:01:09s
epoch 114| loss: 0.13538 | val_0_rmse: 0.3434  | val_1_rmse: 0.50538 |  0:01:10s
epoch 115| loss: 0.13197 | val_0_rmse: 0.32939 | val_1_rmse: 0.49151 |  0:01:11s
epoch 116| loss: 0.13188 | val_0_rmse: 0.3319  | val_1_rmse: 0.50627 |  0:01:11s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.4797
Best weights from best epoch are automatically used!
ended training at: 08:00:10
Feature importance:
Mean squared error is of 20564978804.180042
Mean absolute error:101988.13230494218
MAPE:0.17347393366265504
R2 score:0.7502858774508178
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:00:10
epoch 0  | loss: 1.99031 | val_0_rmse: 0.98868 | val_1_rmse: 1.01697 |  0:00:00s
epoch 1  | loss: 1.17178 | val_0_rmse: 0.99018 | val_1_rmse: 1.01828 |  0:00:01s
epoch 2  | loss: 1.0052  | val_0_rmse: 0.99232 | val_1_rmse: 1.02106 |  0:00:01s
epoch 3  | loss: 0.97719 | val_0_rmse: 0.98688 | val_1_rmse: 1.01538 |  0:00:02s
epoch 4  | loss: 0.9588  | val_0_rmse: 0.96783 | val_1_rmse: 0.99635 |  0:00:03s
epoch 5  | loss: 0.9065  | val_0_rmse: 0.92528 | val_1_rmse: 0.95149 |  0:00:03s
epoch 6  | loss: 0.78478 | val_0_rmse: 0.83936 | val_1_rmse: 0.85868 |  0:00:04s
epoch 7  | loss: 0.66434 | val_0_rmse: 0.82382 | val_1_rmse: 0.83698 |  0:00:04s
epoch 8  | loss: 0.54424 | val_0_rmse: 0.83472 | val_1_rmse: 0.84761 |  0:00:05s
epoch 9  | loss: 0.46128 | val_0_rmse: 0.854   | val_1_rmse: 0.87021 |  0:00:06s
epoch 10 | loss: 0.39116 | val_0_rmse: 0.86579 | val_1_rmse: 0.87619 |  0:00:06s
epoch 11 | loss: 0.34665 | val_0_rmse: 0.77022 | val_1_rmse: 0.78619 |  0:00:07s
epoch 12 | loss: 0.31851 | val_0_rmse: 0.74619 | val_1_rmse: 0.76473 |  0:00:07s
epoch 13 | loss: 0.31459 | val_0_rmse: 0.73258 | val_1_rmse: 0.7513  |  0:00:08s
epoch 14 | loss: 0.29268 | val_0_rmse: 0.76597 | val_1_rmse: 0.77785 |  0:00:09s
epoch 15 | loss: 0.28369 | val_0_rmse: 0.72392 | val_1_rmse: 0.74177 |  0:00:09s
epoch 16 | loss: 0.27786 | val_0_rmse: 0.80952 | val_1_rmse: 0.82856 |  0:00:10s
epoch 17 | loss: 0.27846 | val_0_rmse: 0.80366 | val_1_rmse: 0.81726 |  0:00:11s
epoch 18 | loss: 0.26655 | val_0_rmse: 0.69853 | val_1_rmse: 0.71349 |  0:00:11s
epoch 19 | loss: 0.26104 | val_0_rmse: 0.73217 | val_1_rmse: 0.74448 |  0:00:12s
epoch 20 | loss: 0.24301 | val_0_rmse: 0.72414 | val_1_rmse: 0.7446  |  0:00:12s
epoch 21 | loss: 0.24011 | val_0_rmse: 0.6884  | val_1_rmse: 0.70783 |  0:00:13s
epoch 22 | loss: 0.24398 | val_0_rmse: 0.69285 | val_1_rmse: 0.70984 |  0:00:14s
epoch 23 | loss: 0.22646 | val_0_rmse: 0.7298  | val_1_rmse: 0.74649 |  0:00:14s
epoch 24 | loss: 0.22955 | val_0_rmse: 0.68271 | val_1_rmse: 0.69823 |  0:00:15s
epoch 25 | loss: 0.22404 | val_0_rmse: 0.72756 | val_1_rmse: 0.74752 |  0:00:15s
epoch 26 | loss: 0.21385 | val_0_rmse: 0.69921 | val_1_rmse: 0.71799 |  0:00:16s
epoch 27 | loss: 0.22036 | val_0_rmse: 0.68048 | val_1_rmse: 0.70039 |  0:00:17s
epoch 28 | loss: 0.22058 | val_0_rmse: 0.68141 | val_1_rmse: 0.7065  |  0:00:17s
epoch 29 | loss: 0.22841 | val_0_rmse: 0.66962 | val_1_rmse: 0.68869 |  0:00:18s
epoch 30 | loss: 0.21769 | val_0_rmse: 0.71589 | val_1_rmse: 0.73548 |  0:00:19s
epoch 31 | loss: 0.2198  | val_0_rmse: 0.63836 | val_1_rmse: 0.65984 |  0:00:19s
epoch 32 | loss: 0.22081 | val_0_rmse: 0.71247 | val_1_rmse: 0.73251 |  0:00:20s
epoch 33 | loss: 0.22482 | val_0_rmse: 0.71694 | val_1_rmse: 0.73202 |  0:00:20s
epoch 34 | loss: 0.21964 | val_0_rmse: 0.65375 | val_1_rmse: 0.67081 |  0:00:21s
epoch 35 | loss: 0.21069 | val_0_rmse: 0.69815 | val_1_rmse: 0.72201 |  0:00:22s
epoch 36 | loss: 0.21443 | val_0_rmse: 0.69001 | val_1_rmse: 0.70641 |  0:00:22s
epoch 37 | loss: 0.21357 | val_0_rmse: 0.66148 | val_1_rmse: 0.68696 |  0:00:23s
epoch 38 | loss: 0.20431 | val_0_rmse: 0.6399  | val_1_rmse: 0.66096 |  0:00:23s
epoch 39 | loss: 0.20725 | val_0_rmse: 0.6174  | val_1_rmse: 0.64332 |  0:00:24s
epoch 40 | loss: 0.19576 | val_0_rmse: 0.63399 | val_1_rmse: 0.65927 |  0:00:25s
epoch 41 | loss: 0.1892  | val_0_rmse: 0.62486 | val_1_rmse: 0.6483  |  0:00:25s
epoch 42 | loss: 0.193   | val_0_rmse: 0.61515 | val_1_rmse: 0.64416 |  0:00:26s
epoch 43 | loss: 0.19019 | val_0_rmse: 0.61314 | val_1_rmse: 0.6363  |  0:00:27s
epoch 44 | loss: 0.19397 | val_0_rmse: 0.62579 | val_1_rmse: 0.65378 |  0:00:27s
epoch 45 | loss: 0.19327 | val_0_rmse: 0.59522 | val_1_rmse: 0.61756 |  0:00:28s
epoch 46 | loss: 0.20201 | val_0_rmse: 0.63179 | val_1_rmse: 0.65412 |  0:00:28s
epoch 47 | loss: 0.19516 | val_0_rmse: 0.62192 | val_1_rmse: 0.64411 |  0:00:29s
epoch 48 | loss: 0.19324 | val_0_rmse: 0.58358 | val_1_rmse: 0.60817 |  0:00:30s
epoch 49 | loss: 0.19225 | val_0_rmse: 0.62257 | val_1_rmse: 0.64048 |  0:00:30s
epoch 50 | loss: 0.18758 | val_0_rmse: 0.5773  | val_1_rmse: 0.60098 |  0:00:31s
epoch 51 | loss: 0.18533 | val_0_rmse: 0.58491 | val_1_rmse: 0.61032 |  0:00:31s
epoch 52 | loss: 0.1825  | val_0_rmse: 0.58678 | val_1_rmse: 0.6093  |  0:00:32s
epoch 53 | loss: 0.18213 | val_0_rmse: 0.55932 | val_1_rmse: 0.58901 |  0:00:33s
epoch 54 | loss: 0.18006 | val_0_rmse: 0.54166 | val_1_rmse: 0.57825 |  0:00:33s
epoch 55 | loss: 0.18211 | val_0_rmse: 0.56202 | val_1_rmse: 0.58972 |  0:00:34s
epoch 56 | loss: 0.18301 | val_0_rmse: 0.5541  | val_1_rmse: 0.59117 |  0:00:35s
epoch 57 | loss: 0.18819 | val_0_rmse: 0.52646 | val_1_rmse: 0.56535 |  0:00:35s
epoch 58 | loss: 0.17762 | val_0_rmse: 0.55614 | val_1_rmse: 0.58884 |  0:00:36s
epoch 59 | loss: 0.18121 | val_0_rmse: 0.55049 | val_1_rmse: 0.59283 |  0:00:36s
epoch 60 | loss: 0.18269 | val_0_rmse: 0.52603 | val_1_rmse: 0.56422 |  0:00:37s
epoch 61 | loss: 0.18078 | val_0_rmse: 0.51584 | val_1_rmse: 0.56007 |  0:00:38s
epoch 62 | loss: 0.18474 | val_0_rmse: 0.5137  | val_1_rmse: 0.55901 |  0:00:38s
epoch 63 | loss: 0.17925 | val_0_rmse: 0.51899 | val_1_rmse: 0.55507 |  0:00:39s
epoch 64 | loss: 0.17738 | val_0_rmse: 0.48008 | val_1_rmse: 0.53099 |  0:00:39s
epoch 65 | loss: 0.16838 | val_0_rmse: 0.49017 | val_1_rmse: 0.54165 |  0:00:40s
epoch 66 | loss: 0.17451 | val_0_rmse: 0.46607 | val_1_rmse: 0.52145 |  0:00:41s
epoch 67 | loss: 0.16723 | val_0_rmse: 0.46292 | val_1_rmse: 0.51729 |  0:00:41s
epoch 68 | loss: 0.16911 | val_0_rmse: 0.46473 | val_1_rmse: 0.52299 |  0:00:42s
epoch 69 | loss: 0.16511 | val_0_rmse: 0.44723 | val_1_rmse: 0.5154  |  0:00:43s
epoch 70 | loss: 0.16521 | val_0_rmse: 0.46119 | val_1_rmse: 0.52476 |  0:00:43s
epoch 71 | loss: 0.1633  | val_0_rmse: 0.44108 | val_1_rmse: 0.51589 |  0:00:44s
epoch 72 | loss: 0.16253 | val_0_rmse: 0.42927 | val_1_rmse: 0.50413 |  0:00:44s
epoch 73 | loss: 0.16635 | val_0_rmse: 0.43512 | val_1_rmse: 0.50916 |  0:00:45s
epoch 74 | loss: 0.15963 | val_0_rmse: 0.43735 | val_1_rmse: 0.51616 |  0:00:46s
epoch 75 | loss: 0.16278 | val_0_rmse: 0.42156 | val_1_rmse: 0.50141 |  0:00:46s
epoch 76 | loss: 0.16008 | val_0_rmse: 0.40944 | val_1_rmse: 0.49716 |  0:00:47s
epoch 77 | loss: 0.15575 | val_0_rmse: 0.41605 | val_1_rmse: 0.50453 |  0:00:47s
epoch 78 | loss: 0.15474 | val_0_rmse: 0.41923 | val_1_rmse: 0.51288 |  0:00:48s
epoch 79 | loss: 0.15834 | val_0_rmse: 0.39854 | val_1_rmse: 0.49003 |  0:00:49s
epoch 80 | loss: 0.15799 | val_0_rmse: 0.41117 | val_1_rmse: 0.50173 |  0:00:49s
epoch 81 | loss: 0.15741 | val_0_rmse: 0.39994 | val_1_rmse: 0.49773 |  0:00:50s
epoch 82 | loss: 0.15285 | val_0_rmse: 0.40287 | val_1_rmse: 0.49983 |  0:00:50s
epoch 83 | loss: 0.15376 | val_0_rmse: 0.39465 | val_1_rmse: 0.498   |  0:00:51s
epoch 84 | loss: 0.15246 | val_0_rmse: 0.39509 | val_1_rmse: 0.50684 |  0:00:52s
epoch 85 | loss: 0.14859 | val_0_rmse: 0.39744 | val_1_rmse: 0.50263 |  0:00:52s
epoch 86 | loss: 0.15238 | val_0_rmse: 0.38054 | val_1_rmse: 0.50324 |  0:00:53s
epoch 87 | loss: 0.14745 | val_0_rmse: 0.3745  | val_1_rmse: 0.49734 |  0:00:54s
epoch 88 | loss: 0.14515 | val_0_rmse: 0.36873 | val_1_rmse: 0.49501 |  0:00:54s
epoch 89 | loss: 0.14826 | val_0_rmse: 0.37182 | val_1_rmse: 0.49616 |  0:00:55s
epoch 90 | loss: 0.14135 | val_0_rmse: 0.36676 | val_1_rmse: 0.49534 |  0:00:55s
epoch 91 | loss: 0.14605 | val_0_rmse: 0.35503 | val_1_rmse: 0.48945 |  0:00:56s
epoch 92 | loss: 0.1446  | val_0_rmse: 0.35853 | val_1_rmse: 0.49483 |  0:00:57s
epoch 93 | loss: 0.14612 | val_0_rmse: 0.35546 | val_1_rmse: 0.49024 |  0:00:57s
epoch 94 | loss: 0.14354 | val_0_rmse: 0.35047 | val_1_rmse: 0.48966 |  0:00:58s
epoch 95 | loss: 0.15205 | val_0_rmse: 0.40662 | val_1_rmse: 0.52422 |  0:00:58s
epoch 96 | loss: 0.17343 | val_0_rmse: 0.46316 | val_1_rmse: 0.58082 |  0:00:59s
epoch 97 | loss: 0.17193 | val_0_rmse: 0.37569 | val_1_rmse: 0.49572 |  0:01:00s
epoch 98 | loss: 0.15667 | val_0_rmse: 0.36898 | val_1_rmse: 0.4986  |  0:01:00s
epoch 99 | loss: 0.15421 | val_0_rmse: 0.36902 | val_1_rmse: 0.49283 |  0:01:01s
epoch 100| loss: 0.14827 | val_0_rmse: 0.36406 | val_1_rmse: 0.49978 |  0:01:02s
epoch 101| loss: 0.1475  | val_0_rmse: 0.35387 | val_1_rmse: 0.51787 |  0:01:02s
epoch 102| loss: 0.14803 | val_0_rmse: 0.3586  | val_1_rmse: 0.5343  |  0:01:03s
epoch 103| loss: 0.14985 | val_0_rmse: 0.34938 | val_1_rmse: 0.49571 |  0:01:03s
epoch 104| loss: 0.14523 | val_0_rmse: 0.34449 | val_1_rmse: 0.49432 |  0:01:04s
epoch 105| loss: 0.1489  | val_0_rmse: 0.35254 | val_1_rmse: 0.50201 |  0:01:05s
epoch 106| loss: 0.14385 | val_0_rmse: 0.3643  | val_1_rmse: 0.51204 |  0:01:05s
epoch 107| loss: 0.1435  | val_0_rmse: 0.34468 | val_1_rmse: 0.50601 |  0:01:06s
epoch 108| loss: 0.14172 | val_0_rmse: 0.33651 | val_1_rmse: 0.49893 |  0:01:06s
epoch 109| loss: 0.13679 | val_0_rmse: 0.33834 | val_1_rmse: 0.5004  |  0:01:07s
epoch 110| loss: 0.13896 | val_0_rmse: 0.35936 | val_1_rmse: 0.52032 |  0:01:08s
epoch 111| loss: 0.15128 | val_0_rmse: 0.41326 | val_1_rmse: 0.51803 |  0:01:08s
epoch 112| loss: 0.15389 | val_0_rmse: 0.37372 | val_1_rmse: 0.53104 |  0:01:09s
epoch 113| loss: 0.15026 | val_0_rmse: 0.34583 | val_1_rmse: 0.51432 |  0:01:09s
epoch 114| loss: 0.14277 | val_0_rmse: 0.33971 | val_1_rmse: 0.50688 |  0:01:10s
epoch 115| loss: 0.14994 | val_0_rmse: 0.35059 | val_1_rmse: 0.51976 |  0:01:11s
epoch 116| loss: 0.14919 | val_0_rmse: 0.3428  | val_1_rmse: 0.50828 |  0:01:11s
epoch 117| loss: 0.14216 | val_0_rmse: 0.33157 | val_1_rmse: 0.50549 |  0:01:12s
epoch 118| loss: 0.13621 | val_0_rmse: 0.33323 | val_1_rmse: 0.51242 |  0:01:12s
epoch 119| loss: 0.13526 | val_0_rmse: 0.33126 | val_1_rmse: 0.51265 |  0:01:13s
epoch 120| loss: 0.13718 | val_0_rmse: 0.33246 | val_1_rmse: 0.50871 |  0:01:14s
epoch 121| loss: 0.14198 | val_0_rmse: 0.35121 | val_1_rmse: 0.52376 |  0:01:14s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.48945
Best weights from best epoch are automatically used!
ended training at: 08:01:25
Feature importance:
Mean squared error is of 21171403197.812504
Mean absolute error:105133.48866042325
MAPE:0.1703019018219799
R2 score:0.7495154094193984
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:01:26
epoch 0  | loss: 4.27373 | val_0_rmse: 1.21014 | val_1_rmse: 1.23547 |  0:00:00s
epoch 1  | loss: 2.29799 | val_0_rmse: 1.14324 | val_1_rmse: 1.17234 |  0:00:00s
epoch 2  | loss: 2.06922 | val_0_rmse: 1.00916 | val_1_rmse: 1.03335 |  0:00:00s
epoch 3  | loss: 1.81391 | val_0_rmse: 0.99662 | val_1_rmse: 1.01697 |  0:00:00s
epoch 4  | loss: 1.32027 | val_0_rmse: 0.99839 | val_1_rmse: 1.01979 |  0:00:00s
epoch 5  | loss: 1.41245 | val_0_rmse: 0.99772 | val_1_rmse: 1.01997 |  0:00:01s
epoch 6  | loss: 1.10264 | val_0_rmse: 0.9939  | val_1_rmse: 1.0166  |  0:00:01s
epoch 7  | loss: 1.0344  | val_0_rmse: 0.99389 | val_1_rmse: 1.01627 |  0:00:01s
epoch 8  | loss: 1.02459 | val_0_rmse: 0.99368 | val_1_rmse: 1.0149  |  0:00:01s
epoch 9  | loss: 1.00881 | val_0_rmse: 0.99238 | val_1_rmse: 1.01429 |  0:00:01s
epoch 10 | loss: 0.97639 | val_0_rmse: 0.99012 | val_1_rmse: 1.01234 |  0:00:01s
epoch 11 | loss: 0.93523 | val_0_rmse: 0.98645 | val_1_rmse: 1.00837 |  0:00:01s
epoch 12 | loss: 0.90651 | val_0_rmse: 0.98713 | val_1_rmse: 1.008   |  0:00:02s
epoch 13 | loss: 0.90477 | val_0_rmse: 0.98437 | val_1_rmse: 1.00402 |  0:00:02s
epoch 14 | loss: 0.89559 | val_0_rmse: 0.98241 | val_1_rmse: 1.00261 |  0:00:02s
epoch 15 | loss: 0.84245 | val_0_rmse: 0.97058 | val_1_rmse: 0.99677 |  0:00:02s
epoch 16 | loss: 0.84369 | val_0_rmse: 0.93898 | val_1_rmse: 0.97652 |  0:00:02s
epoch 17 | loss: 0.81162 | val_0_rmse: 0.93219 | val_1_rmse: 0.96261 |  0:00:02s
epoch 18 | loss: 0.79136 | val_0_rmse: 0.9271  | val_1_rmse: 0.95678 |  0:00:03s
epoch 19 | loss: 0.762   | val_0_rmse: 0.93159 | val_1_rmse: 0.9841  |  0:00:03s
epoch 20 | loss: 0.73689 | val_0_rmse: 0.93535 | val_1_rmse: 0.98404 |  0:00:03s
epoch 21 | loss: 0.67456 | val_0_rmse: 0.89543 | val_1_rmse: 0.93582 |  0:00:03s
epoch 22 | loss: 0.68576 | val_0_rmse: 0.86574 | val_1_rmse: 0.89698 |  0:00:03s
epoch 23 | loss: 0.66823 | val_0_rmse: 0.84178 | val_1_rmse: 0.86041 |  0:00:03s
epoch 24 | loss: 0.62349 | val_0_rmse: 0.82741 | val_1_rmse: 0.83671 |  0:00:04s
epoch 25 | loss: 0.59003 | val_0_rmse: 0.83107 | val_1_rmse: 0.83224 |  0:00:04s
epoch 26 | loss: 0.55422 | val_0_rmse: 0.83595 | val_1_rmse: 0.81355 |  0:00:04s
epoch 27 | loss: 0.55321 | val_0_rmse: 0.8115  | val_1_rmse: 0.7799  |  0:00:04s
epoch 28 | loss: 0.55988 | val_0_rmse: 0.76374 | val_1_rmse: 0.7544  |  0:00:04s
epoch 29 | loss: 0.5615  | val_0_rmse: 0.81298 | val_1_rmse: 0.81712 |  0:00:04s
epoch 30 | loss: 0.51093 | val_0_rmse: 0.80697 | val_1_rmse: 0.8204  |  0:00:05s
epoch 31 | loss: 0.47783 | val_0_rmse: 0.75995 | val_1_rmse: 0.77226 |  0:00:05s
epoch 32 | loss: 0.47574 | val_0_rmse: 0.76679 | val_1_rmse: 0.77368 |  0:00:05s
epoch 33 | loss: 0.4456  | val_0_rmse: 0.78161 | val_1_rmse: 0.79144 |  0:00:05s
epoch 34 | loss: 0.43794 | val_0_rmse: 0.7505  | val_1_rmse: 0.76841 |  0:00:05s
epoch 35 | loss: 0.411   | val_0_rmse: 0.74114 | val_1_rmse: 0.76171 |  0:00:05s
epoch 36 | loss: 0.40479 | val_0_rmse: 0.76546 | val_1_rmse: 0.78328 |  0:00:05s
epoch 37 | loss: 0.38334 | val_0_rmse: 0.75622 | val_1_rmse: 0.77579 |  0:00:06s
epoch 38 | loss: 0.37493 | val_0_rmse: 0.73062 | val_1_rmse: 0.75505 |  0:00:06s
epoch 39 | loss: 0.37204 | val_0_rmse: 0.72589 | val_1_rmse: 0.75208 |  0:00:06s
epoch 40 | loss: 0.35524 | val_0_rmse: 0.73786 | val_1_rmse: 0.76379 |  0:00:06s
epoch 41 | loss: 0.33708 | val_0_rmse: 0.74726 | val_1_rmse: 0.77168 |  0:00:06s
epoch 42 | loss: 0.33921 | val_0_rmse: 0.7377  | val_1_rmse: 0.76208 |  0:00:06s
epoch 43 | loss: 0.32658 | val_0_rmse: 0.73392 | val_1_rmse: 0.76101 |  0:00:07s
epoch 44 | loss: 0.31743 | val_0_rmse: 0.73877 | val_1_rmse: 0.7696  |  0:00:07s
epoch 45 | loss: 0.31461 | val_0_rmse: 0.73301 | val_1_rmse: 0.76421 |  0:00:07s
epoch 46 | loss: 0.30622 | val_0_rmse: 0.7355  | val_1_rmse: 0.76562 |  0:00:07s
epoch 47 | loss: 0.32147 | val_0_rmse: 0.73878 | val_1_rmse: 0.76602 |  0:00:07s
epoch 48 | loss: 0.31348 | val_0_rmse: 0.74471 | val_1_rmse: 0.76928 |  0:00:07s
epoch 49 | loss: 0.29978 | val_0_rmse: 0.73767 | val_1_rmse: 0.76097 |  0:00:07s
epoch 50 | loss: 0.30992 | val_0_rmse: 0.71903 | val_1_rmse: 0.74091 |  0:00:08s
epoch 51 | loss: 0.29463 | val_0_rmse: 0.70695 | val_1_rmse: 0.72747 |  0:00:08s
epoch 52 | loss: 0.29951 | val_0_rmse: 0.70934 | val_1_rmse: 0.73069 |  0:00:08s
epoch 53 | loss: 0.28226 | val_0_rmse: 0.72397 | val_1_rmse: 0.74918 |  0:00:08s
epoch 54 | loss: 0.28579 | val_0_rmse: 0.73394 | val_1_rmse: 0.75777 |  0:00:08s
epoch 55 | loss: 0.2867  | val_0_rmse: 0.73062 | val_1_rmse: 0.74979 |  0:00:08s
epoch 56 | loss: 0.2642  | val_0_rmse: 0.73982 | val_1_rmse: 0.75601 |  0:00:09s
epoch 57 | loss: 0.26944 | val_0_rmse: 0.75416 | val_1_rmse: 0.76928 |  0:00:09s
epoch 58 | loss: 0.25244 | val_0_rmse: 0.75595 | val_1_rmse: 0.77178 |  0:00:09s
epoch 59 | loss: 0.26276 | val_0_rmse: 0.75632 | val_1_rmse: 0.7751  |  0:00:09s
epoch 60 | loss: 0.26702 | val_0_rmse: 0.73405 | val_1_rmse: 0.75529 |  0:00:09s
epoch 61 | loss: 0.25741 | val_0_rmse: 0.71294 | val_1_rmse: 0.73522 |  0:00:09s
epoch 62 | loss: 0.25039 | val_0_rmse: 0.72144 | val_1_rmse: 0.73926 |  0:00:10s
epoch 63 | loss: 0.24282 | val_0_rmse: 0.71866 | val_1_rmse: 0.73267 |  0:00:10s
epoch 64 | loss: 0.25541 | val_0_rmse: 0.70041 | val_1_rmse: 0.7171  |  0:00:10s
epoch 65 | loss: 0.24687 | val_0_rmse: 0.7047  | val_1_rmse: 0.72221 |  0:00:10s
epoch 66 | loss: 0.25565 | val_0_rmse: 0.72662 | val_1_rmse: 0.73935 |  0:00:10s
epoch 67 | loss: 0.23446 | val_0_rmse: 0.72044 | val_1_rmse: 0.73014 |  0:00:10s
epoch 68 | loss: 0.24737 | val_0_rmse: 0.71324 | val_1_rmse: 0.72201 |  0:00:10s
epoch 69 | loss: 0.23578 | val_0_rmse: 0.71341 | val_1_rmse: 0.7204  |  0:00:11s
epoch 70 | loss: 0.23051 | val_0_rmse: 0.7111  | val_1_rmse: 0.71888 |  0:00:11s
epoch 71 | loss: 0.23367 | val_0_rmse: 0.70029 | val_1_rmse: 0.71487 |  0:00:11s
epoch 72 | loss: 0.22826 | val_0_rmse: 0.69294 | val_1_rmse: 0.71338 |  0:00:11s
epoch 73 | loss: 0.23353 | val_0_rmse: 0.67022 | val_1_rmse: 0.69746 |  0:00:11s
epoch 74 | loss: 0.23548 | val_0_rmse: 0.67    | val_1_rmse: 0.70045 |  0:00:11s
epoch 75 | loss: 0.21857 | val_0_rmse: 0.66812 | val_1_rmse: 0.69573 |  0:00:12s
epoch 76 | loss: 0.23148 | val_0_rmse: 0.67496 | val_1_rmse: 0.6977  |  0:00:12s
epoch 77 | loss: 0.22073 | val_0_rmse: 0.68542 | val_1_rmse: 0.70538 |  0:00:12s
epoch 78 | loss: 0.23707 | val_0_rmse: 0.68658 | val_1_rmse: 0.70872 |  0:00:12s
epoch 79 | loss: 0.23094 | val_0_rmse: 0.69893 | val_1_rmse: 0.7203  |  0:00:12s
epoch 80 | loss: 0.23511 | val_0_rmse: 0.67736 | val_1_rmse: 0.69965 |  0:00:12s
epoch 81 | loss: 0.22327 | val_0_rmse: 0.65453 | val_1_rmse: 0.68107 |  0:00:12s
epoch 82 | loss: 0.22675 | val_0_rmse: 0.65147 | val_1_rmse: 0.68334 |  0:00:13s
epoch 83 | loss: 0.21879 | val_0_rmse: 0.6702  | val_1_rmse: 0.70251 |  0:00:13s
epoch 84 | loss: 0.22438 | val_0_rmse: 0.67306 | val_1_rmse: 0.70717 |  0:00:13s
epoch 85 | loss: 0.21275 | val_0_rmse: 0.65696 | val_1_rmse: 0.69247 |  0:00:13s
epoch 86 | loss: 0.20742 | val_0_rmse: 0.66263 | val_1_rmse: 0.69433 |  0:00:13s
epoch 87 | loss: 0.2146  | val_0_rmse: 0.68338 | val_1_rmse: 0.71512 |  0:00:13s
epoch 88 | loss: 0.22644 | val_0_rmse: 0.6666  | val_1_rmse: 0.70092 |  0:00:14s
epoch 89 | loss: 0.22223 | val_0_rmse: 0.64219 | val_1_rmse: 0.67897 |  0:00:14s
epoch 90 | loss: 0.20328 | val_0_rmse: 0.64342 | val_1_rmse: 0.67597 |  0:00:14s
epoch 91 | loss: 0.20229 | val_0_rmse: 0.6511  | val_1_rmse: 0.68133 |  0:00:14s
epoch 92 | loss: 0.21011 | val_0_rmse: 0.67564 | val_1_rmse: 0.70855 |  0:00:14s
epoch 93 | loss: 0.20334 | val_0_rmse: 0.67808 | val_1_rmse: 0.71415 |  0:00:14s
epoch 94 | loss: 0.21171 | val_0_rmse: 0.68223 | val_1_rmse: 0.72032 |  0:00:15s
epoch 95 | loss: 0.21877 | val_0_rmse: 0.68351 | val_1_rmse: 0.71838 |  0:00:15s
epoch 96 | loss: 0.22842 | val_0_rmse: 0.66996 | val_1_rmse: 0.70047 |  0:00:15s
epoch 97 | loss: 0.21815 | val_0_rmse: 0.67205 | val_1_rmse: 0.69559 |  0:00:15s
epoch 98 | loss: 0.22218 | val_0_rmse: 0.68318 | val_1_rmse: 0.6991  |  0:00:15s
epoch 99 | loss: 0.20999 | val_0_rmse: 0.68036 | val_1_rmse: 0.69369 |  0:00:15s
epoch 100| loss: 0.22097 | val_0_rmse: 0.67287 | val_1_rmse: 0.69128 |  0:00:15s
epoch 101| loss: 0.20249 | val_0_rmse: 0.66339 | val_1_rmse: 0.68329 |  0:00:16s
epoch 102| loss: 0.19829 | val_0_rmse: 0.66083 | val_1_rmse: 0.67979 |  0:00:16s
epoch 103| loss: 0.21689 | val_0_rmse: 0.65048 | val_1_rmse: 0.67051 |  0:00:16s
epoch 104| loss: 0.19514 | val_0_rmse: 0.64123 | val_1_rmse: 0.6673  |  0:00:16s
epoch 105| loss: 0.18936 | val_0_rmse: 0.6306  | val_1_rmse: 0.66905 |  0:00:16s
epoch 106| loss: 0.19602 | val_0_rmse: 0.62292 | val_1_rmse: 0.67081 |  0:00:16s
epoch 107| loss: 0.18107 | val_0_rmse: 0.62156 | val_1_rmse: 0.67048 |  0:00:17s
epoch 108| loss: 0.18752 | val_0_rmse: 0.62695 | val_1_rmse: 0.6724  |  0:00:17s
epoch 109| loss: 0.18806 | val_0_rmse: 0.62608 | val_1_rmse: 0.67028 |  0:00:17s
epoch 110| loss: 0.18112 | val_0_rmse: 0.62676 | val_1_rmse: 0.6731  |  0:00:17s
epoch 111| loss: 0.17487 | val_0_rmse: 0.6237  | val_1_rmse: 0.67639 |  0:00:17s
epoch 112| loss: 0.17168 | val_0_rmse: 0.61701 | val_1_rmse: 0.67699 |  0:00:17s
epoch 113| loss: 0.16771 | val_0_rmse: 0.61383 | val_1_rmse: 0.67615 |  0:00:17s
epoch 114| loss: 0.17661 | val_0_rmse: 0.6157  | val_1_rmse: 0.68086 |  0:00:18s
epoch 115| loss: 0.17102 | val_0_rmse: 0.6113  | val_1_rmse: 0.67474 |  0:00:18s
epoch 116| loss: 0.16815 | val_0_rmse: 0.60583 | val_1_rmse: 0.66331 |  0:00:18s
epoch 117| loss: 0.16381 | val_0_rmse: 0.60305 | val_1_rmse: 0.65619 |  0:00:18s
epoch 118| loss: 0.16732 | val_0_rmse: 0.60254 | val_1_rmse: 0.65684 |  0:00:18s
epoch 119| loss: 0.18131 | val_0_rmse: 0.6048  | val_1_rmse: 0.66278 |  0:00:18s
epoch 120| loss: 0.16699 | val_0_rmse: 0.6155  | val_1_rmse: 0.67201 |  0:00:19s
epoch 121| loss: 0.16557 | val_0_rmse: 0.61312 | val_1_rmse: 0.66777 |  0:00:19s
epoch 122| loss: 0.16473 | val_0_rmse: 0.60565 | val_1_rmse: 0.66189 |  0:00:19s
epoch 123| loss: 0.16576 | val_0_rmse: 0.59782 | val_1_rmse: 0.66168 |  0:00:19s
epoch 124| loss: 0.16497 | val_0_rmse: 0.60015 | val_1_rmse: 0.66708 |  0:00:19s
epoch 125| loss: 0.15794 | val_0_rmse: 0.60996 | val_1_rmse: 0.67515 |  0:00:19s
epoch 126| loss: 0.16679 | val_0_rmse: 0.61025 | val_1_rmse: 0.67348 |  0:00:20s
epoch 127| loss: 0.16308 | val_0_rmse: 0.60922 | val_1_rmse: 0.67196 |  0:00:20s
epoch 128| loss: 0.16412 | val_0_rmse: 0.6122  | val_1_rmse: 0.67733 |  0:00:20s
epoch 129| loss: 0.16636 | val_0_rmse: 0.60375 | val_1_rmse: 0.66819 |  0:00:20s
epoch 130| loss: 0.15804 | val_0_rmse: 0.59492 | val_1_rmse: 0.65838 |  0:00:20s
epoch 131| loss: 0.15899 | val_0_rmse: 0.59016 | val_1_rmse: 0.65963 |  0:00:20s
epoch 132| loss: 0.16629 | val_0_rmse: 0.5996  | val_1_rmse: 0.67411 |  0:00:20s
epoch 133| loss: 0.15342 | val_0_rmse: 0.60891 | val_1_rmse: 0.68262 |  0:00:21s
epoch 134| loss: 0.15754 | val_0_rmse: 0.59199 | val_1_rmse: 0.67006 |  0:00:21s
epoch 135| loss: 0.16047 | val_0_rmse: 0.57232 | val_1_rmse: 0.65747 |  0:00:21s
epoch 136| loss: 0.15496 | val_0_rmse: 0.57025 | val_1_rmse: 0.65518 |  0:00:21s
epoch 137| loss: 0.14906 | val_0_rmse: 0.57964 | val_1_rmse: 0.66083 |  0:00:21s
epoch 138| loss: 0.14265 | val_0_rmse: 0.57457 | val_1_rmse: 0.6553  |  0:00:21s
epoch 139| loss: 0.14497 | val_0_rmse: 0.56717 | val_1_rmse: 0.65042 |  0:00:22s
epoch 140| loss: 0.14932 | val_0_rmse: 0.57422 | val_1_rmse: 0.65841 |  0:00:22s
epoch 141| loss: 0.14796 | val_0_rmse: 0.58554 | val_1_rmse: 0.66962 |  0:00:22s
epoch 142| loss: 0.1436  | val_0_rmse: 0.59773 | val_1_rmse: 0.68308 |  0:00:22s
epoch 143| loss: 0.16057 | val_0_rmse: 0.59175 | val_1_rmse: 0.67734 |  0:00:22s
epoch 144| loss: 0.14998 | val_0_rmse: 0.57971 | val_1_rmse: 0.66942 |  0:00:22s
epoch 145| loss: 0.15203 | val_0_rmse: 0.56301 | val_1_rmse: 0.66069 |  0:00:22s
epoch 146| loss: 0.15179 | val_0_rmse: 0.56234 | val_1_rmse: 0.65953 |  0:00:23s
epoch 147| loss: 0.15249 | val_0_rmse: 0.56206 | val_1_rmse: 0.65703 |  0:00:23s
epoch 148| loss: 0.1411  | val_0_rmse: 0.56114 | val_1_rmse: 0.66324 |  0:00:23s
epoch 149| loss: 0.14081 | val_0_rmse: 0.55459 | val_1_rmse: 0.66751 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_val_1_rmse = 0.65042
Best weights from best epoch are automatically used!
ended training at: 08:01:49
Feature importance:
Mean squared error is of 3238938566.1215396
Mean absolute error:40533.330520549884
MAPE:0.351416855250626
R2 score:0.5757143056599072
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:01:50
epoch 0  | loss: 5.11374 | val_0_rmse: 1.03041 | val_1_rmse: 1.03341 |  0:00:00s
epoch 1  | loss: 2.20542 | val_0_rmse: 1.03011 | val_1_rmse: 1.03066 |  0:00:00s
epoch 2  | loss: 1.67869 | val_0_rmse: 1.00601 | val_1_rmse: 1.00464 |  0:00:00s
epoch 3  | loss: 1.38002 | val_0_rmse: 1.01271 | val_1_rmse: 1.01164 |  0:00:00s
epoch 4  | loss: 1.21986 | val_0_rmse: 1.01123 | val_1_rmse: 1.01104 |  0:00:00s
epoch 5  | loss: 1.04777 | val_0_rmse: 1.01162 | val_1_rmse: 1.0129  |  0:00:00s
epoch 6  | loss: 1.03497 | val_0_rmse: 1.01471 | val_1_rmse: 1.01644 |  0:00:01s
epoch 7  | loss: 0.96601 | val_0_rmse: 1.00579 | val_1_rmse: 1.00882 |  0:00:01s
epoch 8  | loss: 0.93532 | val_0_rmse: 0.9805  | val_1_rmse: 0.97927 |  0:00:01s
epoch 9  | loss: 0.88871 | val_0_rmse: 0.97268 | val_1_rmse: 0.96374 |  0:00:01s
epoch 10 | loss: 0.90963 | val_0_rmse: 0.9496  | val_1_rmse: 0.9443  |  0:00:01s
epoch 11 | loss: 0.85998 | val_0_rmse: 0.85012 | val_1_rmse: 0.86702 |  0:00:01s
epoch 12 | loss: 0.74312 | val_0_rmse: 0.79944 | val_1_rmse: 0.86824 |  0:00:02s
epoch 13 | loss: 0.6803  | val_0_rmse: 0.87956 | val_1_rmse: 0.94309 |  0:00:02s
epoch 14 | loss: 0.66052 | val_0_rmse: 0.86818 | val_1_rmse: 0.92464 |  0:00:02s
epoch 15 | loss: 0.63861 | val_0_rmse: 0.88823 | val_1_rmse: 0.94249 |  0:00:02s
epoch 16 | loss: 0.63168 | val_0_rmse: 0.85349 | val_1_rmse: 0.93005 |  0:00:02s
epoch 17 | loss: 0.63865 | val_0_rmse: 0.845   | val_1_rmse: 0.90937 |  0:00:02s
epoch 18 | loss: 0.61533 | val_0_rmse: 0.87734 | val_1_rmse: 0.95089 |  0:00:03s
epoch 19 | loss: 0.59654 | val_0_rmse: 0.82117 | val_1_rmse: 0.87585 |  0:00:03s
epoch 20 | loss: 0.5686  | val_0_rmse: 0.79206 | val_1_rmse: 0.83619 |  0:00:03s
epoch 21 | loss: 0.56319 | val_0_rmse: 0.7989  | val_1_rmse: 0.85418 |  0:00:03s
epoch 22 | loss: 0.55214 | val_0_rmse: 0.77028 | val_1_rmse: 0.81895 |  0:00:03s
epoch 23 | loss: 0.55062 | val_0_rmse: 0.75758 | val_1_rmse: 0.78724 |  0:00:03s
epoch 24 | loss: 0.50795 | val_0_rmse: 0.79321 | val_1_rmse: 0.82689 |  0:00:03s
epoch 25 | loss: 0.50402 | val_0_rmse: 0.79019 | val_1_rmse: 0.83912 |  0:00:04s
epoch 26 | loss: 0.50521 | val_0_rmse: 0.76324 | val_1_rmse: 0.81422 |  0:00:04s
epoch 27 | loss: 0.4996  | val_0_rmse: 0.74857 | val_1_rmse: 0.78004 |  0:00:04s
epoch 28 | loss: 0.49876 | val_0_rmse: 0.7389  | val_1_rmse: 0.78072 |  0:00:04s
epoch 29 | loss: 0.50015 | val_0_rmse: 0.75077 | val_1_rmse: 0.79666 |  0:00:04s
epoch 30 | loss: 0.47396 | val_0_rmse: 0.73894 | val_1_rmse: 0.78217 |  0:00:04s
epoch 31 | loss: 0.47274 | val_0_rmse: 0.72456 | val_1_rmse: 0.76153 |  0:00:05s
epoch 32 | loss: 0.45335 | val_0_rmse: 0.73966 | val_1_rmse: 0.77541 |  0:00:05s
epoch 33 | loss: 0.45575 | val_0_rmse: 0.72308 | val_1_rmse: 0.75278 |  0:00:05s
epoch 34 | loss: 0.43083 | val_0_rmse: 0.7188  | val_1_rmse: 0.75104 |  0:00:05s
epoch 35 | loss: 0.43486 | val_0_rmse: 0.74302 | val_1_rmse: 0.78185 |  0:00:05s
epoch 36 | loss: 0.42895 | val_0_rmse: 0.73348 | val_1_rmse: 0.76661 |  0:00:05s
epoch 37 | loss: 0.42364 | val_0_rmse: 0.72561 | val_1_rmse: 0.75431 |  0:00:06s
epoch 38 | loss: 0.41889 | val_0_rmse: 0.7408  | val_1_rmse: 0.773   |  0:00:06s
epoch 39 | loss: 0.41366 | val_0_rmse: 0.75091 | val_1_rmse: 0.78878 |  0:00:06s
epoch 40 | loss: 0.42364 | val_0_rmse: 0.74257 | val_1_rmse: 0.78155 |  0:00:06s
epoch 41 | loss: 0.39079 | val_0_rmse: 0.71082 | val_1_rmse: 0.74455 |  0:00:06s
epoch 42 | loss: 0.41194 | val_0_rmse: 0.69839 | val_1_rmse: 0.73568 |  0:00:06s
epoch 43 | loss: 0.41115 | val_0_rmse: 0.71828 | val_1_rmse: 0.75647 |  0:00:06s
epoch 44 | loss: 0.38454 | val_0_rmse: 0.73113 | val_1_rmse: 0.77635 |  0:00:07s
epoch 45 | loss: 0.39499 | val_0_rmse: 0.6944  | val_1_rmse: 0.7284  |  0:00:07s
epoch 46 | loss: 0.38828 | val_0_rmse: 0.69228 | val_1_rmse: 0.7295  |  0:00:07s
epoch 47 | loss: 0.38096 | val_0_rmse: 0.73325 | val_1_rmse: 0.78242 |  0:00:07s
epoch 48 | loss: 0.40746 | val_0_rmse: 0.69529 | val_1_rmse: 0.72812 |  0:00:07s
epoch 49 | loss: 0.38079 | val_0_rmse: 0.69123 | val_1_rmse: 0.72353 |  0:00:07s
epoch 50 | loss: 0.37764 | val_0_rmse: 0.73233 | val_1_rmse: 0.77364 |  0:00:08s
epoch 51 | loss: 0.35871 | val_0_rmse: 0.71746 | val_1_rmse: 0.759   |  0:00:08s
epoch 52 | loss: 0.34985 | val_0_rmse: 0.69262 | val_1_rmse: 0.73221 |  0:00:08s
epoch 53 | loss: 0.36094 | val_0_rmse: 0.70431 | val_1_rmse: 0.74696 |  0:00:08s
epoch 54 | loss: 0.34349 | val_0_rmse: 0.72373 | val_1_rmse: 0.76812 |  0:00:08s
epoch 55 | loss: 0.35323 | val_0_rmse: 0.70232 | val_1_rmse: 0.74276 |  0:00:08s
epoch 56 | loss: 0.34404 | val_0_rmse: 0.70487 | val_1_rmse: 0.74755 |  0:00:08s
epoch 57 | loss: 0.34763 | val_0_rmse: 0.73097 | val_1_rmse: 0.78244 |  0:00:09s
epoch 58 | loss: 0.34302 | val_0_rmse: 0.71153 | val_1_rmse: 0.7601  |  0:00:09s
epoch 59 | loss: 0.34538 | val_0_rmse: 0.70166 | val_1_rmse: 0.74375 |  0:00:09s
epoch 60 | loss: 0.34264 | val_0_rmse: 0.72131 | val_1_rmse: 0.7644  |  0:00:09s
epoch 61 | loss: 0.32411 | val_0_rmse: 0.72062 | val_1_rmse: 0.76907 |  0:00:09s
epoch 62 | loss: 0.32406 | val_0_rmse: 0.70308 | val_1_rmse: 0.75023 |  0:00:09s
epoch 63 | loss: 0.32667 | val_0_rmse: 0.69886 | val_1_rmse: 0.74027 |  0:00:10s
epoch 64 | loss: 0.32354 | val_0_rmse: 0.69848 | val_1_rmse: 0.74108 |  0:00:10s
epoch 65 | loss: 0.31794 | val_0_rmse: 0.69114 | val_1_rmse: 0.73622 |  0:00:10s
epoch 66 | loss: 0.31684 | val_0_rmse: 0.68994 | val_1_rmse: 0.73717 |  0:00:10s
epoch 67 | loss: 0.32276 | val_0_rmse: 0.67827 | val_1_rmse: 0.71679 |  0:00:10s
epoch 68 | loss: 0.33479 | val_0_rmse: 0.67885 | val_1_rmse: 0.70997 |  0:00:10s
epoch 69 | loss: 0.34218 | val_0_rmse: 0.70027 | val_1_rmse: 0.73435 |  0:00:11s
epoch 70 | loss: 0.33154 | val_0_rmse: 0.71724 | val_1_rmse: 0.75693 |  0:00:11s
epoch 71 | loss: 0.32086 | val_0_rmse: 0.71055 | val_1_rmse: 0.74762 |  0:00:11s
epoch 72 | loss: 0.32517 | val_0_rmse: 0.71003 | val_1_rmse: 0.7479  |  0:00:11s
epoch 73 | loss: 0.32688 | val_0_rmse: 0.72883 | val_1_rmse: 0.7746  |  0:00:11s
epoch 74 | loss: 0.32307 | val_0_rmse: 0.73994 | val_1_rmse: 0.79364 |  0:00:11s
epoch 75 | loss: 0.32426 | val_0_rmse: 0.69139 | val_1_rmse: 0.74064 |  0:00:11s
epoch 76 | loss: 0.3061  | val_0_rmse: 0.67148 | val_1_rmse: 0.71298 |  0:00:12s
epoch 77 | loss: 0.319   | val_0_rmse: 0.68082 | val_1_rmse: 0.72561 |  0:00:12s
epoch 78 | loss: 0.30798 | val_0_rmse: 0.71401 | val_1_rmse: 0.76554 |  0:00:12s
epoch 79 | loss: 0.31318 | val_0_rmse: 0.685   | val_1_rmse: 0.73112 |  0:00:12s
epoch 80 | loss: 0.30208 | val_0_rmse: 0.66426 | val_1_rmse: 0.70341 |  0:00:12s
epoch 81 | loss: 0.30333 | val_0_rmse: 0.67874 | val_1_rmse: 0.7222  |  0:00:12s
epoch 82 | loss: 0.28784 | val_0_rmse: 0.70484 | val_1_rmse: 0.76096 |  0:00:13s
epoch 83 | loss: 0.29471 | val_0_rmse: 0.68554 | val_1_rmse: 0.73678 |  0:00:13s
epoch 84 | loss: 0.2808  | val_0_rmse: 0.67389 | val_1_rmse: 0.72759 |  0:00:13s
epoch 85 | loss: 0.2942  | val_0_rmse: 0.67199 | val_1_rmse: 0.71866 |  0:00:13s
epoch 86 | loss: 0.28454 | val_0_rmse: 0.67629 | val_1_rmse: 0.71634 |  0:00:13s
epoch 87 | loss: 0.28853 | val_0_rmse: 0.67919 | val_1_rmse: 0.72486 |  0:00:13s
epoch 88 | loss: 0.29034 | val_0_rmse: 0.68075 | val_1_rmse: 0.72691 |  0:00:13s
epoch 89 | loss: 0.2744  | val_0_rmse: 0.68365 | val_1_rmse: 0.72701 |  0:00:14s
epoch 90 | loss: 0.28556 | val_0_rmse: 0.68376 | val_1_rmse: 0.72498 |  0:00:14s
epoch 91 | loss: 0.28574 | val_0_rmse: 0.67922 | val_1_rmse: 0.71788 |  0:00:14s
epoch 92 | loss: 0.28617 | val_0_rmse: 0.68482 | val_1_rmse: 0.72694 |  0:00:14s
epoch 93 | loss: 0.28402 | val_0_rmse: 0.68834 | val_1_rmse: 0.73679 |  0:00:14s
epoch 94 | loss: 0.28524 | val_0_rmse: 0.66622 | val_1_rmse: 0.71266 |  0:00:14s
epoch 95 | loss: 0.282   | val_0_rmse: 0.66966 | val_1_rmse: 0.71246 |  0:00:15s
epoch 96 | loss: 0.28296 | val_0_rmse: 0.6872  | val_1_rmse: 0.72654 |  0:00:15s
epoch 97 | loss: 0.29014 | val_0_rmse: 0.69055 | val_1_rmse: 0.73425 |  0:00:15s
epoch 98 | loss: 0.29199 | val_0_rmse: 0.67881 | val_1_rmse: 0.72242 |  0:00:15s
epoch 99 | loss: 0.28204 | val_0_rmse: 0.66385 | val_1_rmse: 0.71776 |  0:00:15s
epoch 100| loss: 0.27858 | val_0_rmse: 0.65497 | val_1_rmse: 0.71681 |  0:00:15s
epoch 101| loss: 0.28999 | val_0_rmse: 0.65935 | val_1_rmse: 0.727   |  0:00:15s
epoch 102| loss: 0.28993 | val_0_rmse: 0.67144 | val_1_rmse: 0.7424  |  0:00:16s
epoch 103| loss: 0.28383 | val_0_rmse: 0.64882 | val_1_rmse: 0.7175  |  0:00:16s
epoch 104| loss: 0.27884 | val_0_rmse: 0.64209 | val_1_rmse: 0.70403 |  0:00:16s
epoch 105| loss: 0.28477 | val_0_rmse: 0.65372 | val_1_rmse: 0.71598 |  0:00:16s
epoch 106| loss: 0.2733  | val_0_rmse: 0.64897 | val_1_rmse: 0.71155 |  0:00:16s
epoch 107| loss: 0.2774  | val_0_rmse: 0.64199 | val_1_rmse: 0.70474 |  0:00:16s
epoch 108| loss: 0.27732 | val_0_rmse: 0.64407 | val_1_rmse: 0.70888 |  0:00:17s
epoch 109| loss: 0.26963 | val_0_rmse: 0.65188 | val_1_rmse: 0.71886 |  0:00:17s
epoch 110| loss: 0.26906 | val_0_rmse: 0.64231 | val_1_rmse: 0.70649 |  0:00:17s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.70341
Best weights from best epoch are automatically used!
ended training at: 08:02:07
Feature importance:
Mean squared error is of 2804728421.3375397
Mean absolute error:38109.024293827955
MAPE:0.31211581209585
R2 score:0.5982325050655524
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:07
epoch 0  | loss: 4.47668 | val_0_rmse: 1.03204 | val_1_rmse: 1.03002 |  0:00:00s
epoch 1  | loss: 2.49946 | val_0_rmse: 0.98793 | val_1_rmse: 1.00016 |  0:00:00s
epoch 2  | loss: 1.71214 | val_0_rmse: 0.98975 | val_1_rmse: 1.00847 |  0:00:00s
epoch 3  | loss: 1.34368 | val_0_rmse: 0.99589 | val_1_rmse: 1.00921 |  0:00:00s
epoch 4  | loss: 1.31779 | val_0_rmse: 0.99794 | val_1_rmse: 1.00832 |  0:00:00s
epoch 5  | loss: 1.25657 | val_0_rmse: 0.99409 | val_1_rmse: 1.00753 |  0:00:00s
epoch 6  | loss: 1.09255 | val_0_rmse: 0.99037 | val_1_rmse: 1.00393 |  0:00:01s
epoch 7  | loss: 1.03534 | val_0_rmse: 0.98444 | val_1_rmse: 0.99962 |  0:00:01s
epoch 8  | loss: 1.03639 | val_0_rmse: 0.97224 | val_1_rmse: 0.98782 |  0:00:01s
epoch 9  | loss: 1.00112 | val_0_rmse: 0.96463 | val_1_rmse: 0.97959 |  0:00:01s
epoch 10 | loss: 0.98327 | val_0_rmse: 0.95409 | val_1_rmse: 0.96811 |  0:00:01s
epoch 11 | loss: 0.93603 | val_0_rmse: 0.9528  | val_1_rmse: 0.9651  |  0:00:01s
epoch 12 | loss: 0.9257  | val_0_rmse: 0.95387 | val_1_rmse: 0.96201 |  0:00:02s
epoch 13 | loss: 0.92478 | val_0_rmse: 0.92581 | val_1_rmse: 0.92951 |  0:00:02s
epoch 14 | loss: 0.8927  | val_0_rmse: 0.87276 | val_1_rmse: 0.89176 |  0:00:02s
epoch 15 | loss: 0.79035 | val_0_rmse: 0.85389 | val_1_rmse: 0.89263 |  0:00:02s
epoch 16 | loss: 0.7839  | val_0_rmse: 0.8664  | val_1_rmse: 0.90203 |  0:00:02s
epoch 17 | loss: 0.70803 | val_0_rmse: 0.88263 | val_1_rmse: 0.90115 |  0:00:02s
epoch 18 | loss: 0.62365 | val_0_rmse: 0.91884 | val_1_rmse: 0.91585 |  0:00:03s
epoch 19 | loss: 0.55701 | val_0_rmse: 0.88497 | val_1_rmse: 0.87    |  0:00:03s
epoch 20 | loss: 0.53138 | val_0_rmse: 0.86406 | val_1_rmse: 0.84462 |  0:00:03s
epoch 21 | loss: 0.52119 | val_0_rmse: 0.79877 | val_1_rmse: 0.78423 |  0:00:03s
epoch 22 | loss: 0.48486 | val_0_rmse: 0.78139 | val_1_rmse: 0.76709 |  0:00:03s
epoch 23 | loss: 0.48124 | val_0_rmse: 0.77749 | val_1_rmse: 0.76589 |  0:00:03s
epoch 24 | loss: 0.45102 | val_0_rmse: 0.73528 | val_1_rmse: 0.73833 |  0:00:04s
epoch 25 | loss: 0.45157 | val_0_rmse: 0.75041 | val_1_rmse: 0.74883 |  0:00:04s
epoch 26 | loss: 0.43277 | val_0_rmse: 0.79184 | val_1_rmse: 0.7864  |  0:00:04s
epoch 27 | loss: 0.44243 | val_0_rmse: 0.74371 | val_1_rmse: 0.75585 |  0:00:04s
epoch 28 | loss: 0.44129 | val_0_rmse: 0.7063  | val_1_rmse: 0.72969 |  0:00:04s
epoch 29 | loss: 0.42564 | val_0_rmse: 0.70602 | val_1_rmse: 0.7242  |  0:00:04s
epoch 30 | loss: 0.41204 | val_0_rmse: 0.71061 | val_1_rmse: 0.72118 |  0:00:04s
epoch 31 | loss: 0.40516 | val_0_rmse: 0.72353 | val_1_rmse: 0.73697 |  0:00:05s
epoch 32 | loss: 0.40495 | val_0_rmse: 0.70836 | val_1_rmse: 0.72437 |  0:00:05s
epoch 33 | loss: 0.38661 | val_0_rmse: 0.68532 | val_1_rmse: 0.69085 |  0:00:05s
epoch 34 | loss: 0.39142 | val_0_rmse: 0.67887 | val_1_rmse: 0.68152 |  0:00:05s
epoch 35 | loss: 0.3947  | val_0_rmse: 0.69729 | val_1_rmse: 0.6967  |  0:00:05s
epoch 36 | loss: 0.38409 | val_0_rmse: 0.69157 | val_1_rmse: 0.70783 |  0:00:05s
epoch 37 | loss: 0.37101 | val_0_rmse: 0.67706 | val_1_rmse: 0.69733 |  0:00:06s
epoch 38 | loss: 0.36517 | val_0_rmse: 0.67609 | val_1_rmse: 0.70078 |  0:00:06s
epoch 39 | loss: 0.3648  | val_0_rmse: 0.68093 | val_1_rmse: 0.70662 |  0:00:06s
epoch 40 | loss: 0.35559 | val_0_rmse: 0.68814 | val_1_rmse: 0.71773 |  0:00:06s
epoch 41 | loss: 0.36368 | val_0_rmse: 0.68022 | val_1_rmse: 0.70614 |  0:00:06s
epoch 42 | loss: 0.35471 | val_0_rmse: 0.67221 | val_1_rmse: 0.69378 |  0:00:06s
epoch 43 | loss: 0.36781 | val_0_rmse: 0.67472 | val_1_rmse: 0.69403 |  0:00:07s
epoch 44 | loss: 0.37172 | val_0_rmse: 0.68327 | val_1_rmse: 0.69904 |  0:00:07s
epoch 45 | loss: 0.36537 | val_0_rmse: 0.67728 | val_1_rmse: 0.6972  |  0:00:07s
epoch 46 | loss: 0.34916 | val_0_rmse: 0.68266 | val_1_rmse: 0.70454 |  0:00:07s
epoch 47 | loss: 0.33385 | val_0_rmse: 0.6945  | val_1_rmse: 0.71196 |  0:00:07s
epoch 48 | loss: 0.33364 | val_0_rmse: 0.68714 | val_1_rmse: 0.70687 |  0:00:07s
epoch 49 | loss: 0.3366  | val_0_rmse: 0.68733 | val_1_rmse: 0.71078 |  0:00:07s
epoch 50 | loss: 0.32533 | val_0_rmse: 0.69577 | val_1_rmse: 0.71683 |  0:00:08s
epoch 51 | loss: 0.32479 | val_0_rmse: 0.68538 | val_1_rmse: 0.70766 |  0:00:08s
epoch 52 | loss: 0.32417 | val_0_rmse: 0.68096 | val_1_rmse: 0.69851 |  0:00:08s
epoch 53 | loss: 0.33577 | val_0_rmse: 0.67623 | val_1_rmse: 0.68902 |  0:00:08s
epoch 54 | loss: 0.31634 | val_0_rmse: 0.66883 | val_1_rmse: 0.68183 |  0:00:08s
epoch 55 | loss: 0.32418 | val_0_rmse: 0.67579 | val_1_rmse: 0.68128 |  0:00:08s
epoch 56 | loss: 0.32746 | val_0_rmse: 0.68484 | val_1_rmse: 0.68966 |  0:00:09s
epoch 57 | loss: 0.31818 | val_0_rmse: 0.68963 | val_1_rmse: 0.69414 |  0:00:09s
epoch 58 | loss: 0.31078 | val_0_rmse: 0.68115 | val_1_rmse: 0.68945 |  0:00:09s
epoch 59 | loss: 0.32729 | val_0_rmse: 0.67978 | val_1_rmse: 0.68574 |  0:00:09s
epoch 60 | loss: 0.30482 | val_0_rmse: 0.69519 | val_1_rmse: 0.69549 |  0:00:09s
epoch 61 | loss: 0.315   | val_0_rmse: 0.67371 | val_1_rmse: 0.6804  |  0:00:09s
epoch 62 | loss: 0.30454 | val_0_rmse: 0.66931 | val_1_rmse: 0.67875 |  0:00:09s
epoch 63 | loss: 0.32744 | val_0_rmse: 0.68748 | val_1_rmse: 0.68997 |  0:00:10s
epoch 64 | loss: 0.30163 | val_0_rmse: 0.6811  | val_1_rmse: 0.68491 |  0:00:10s
epoch 65 | loss: 0.30281 | val_0_rmse: 0.66238 | val_1_rmse: 0.673   |  0:00:10s
epoch 66 | loss: 0.3017  | val_0_rmse: 0.65958 | val_1_rmse: 0.66845 |  0:00:10s
epoch 67 | loss: 0.30743 | val_0_rmse: 0.66272 | val_1_rmse: 0.6698  |  0:00:10s
epoch 68 | loss: 0.3006  | val_0_rmse: 0.66677 | val_1_rmse: 0.67669 |  0:00:10s
epoch 69 | loss: 0.29123 | val_0_rmse: 0.6727  | val_1_rmse: 0.68364 |  0:00:11s
epoch 70 | loss: 0.2977  | val_0_rmse: 0.67396 | val_1_rmse: 0.68115 |  0:00:11s
epoch 71 | loss: 0.30303 | val_0_rmse: 0.66345 | val_1_rmse: 0.67675 |  0:00:11s
epoch 72 | loss: 0.30397 | val_0_rmse: 0.66106 | val_1_rmse: 0.67786 |  0:00:11s
epoch 73 | loss: 0.31082 | val_0_rmse: 0.66369 | val_1_rmse: 0.6788  |  0:00:11s
epoch 74 | loss: 0.2962  | val_0_rmse: 0.66947 | val_1_rmse: 0.68351 |  0:00:11s
epoch 75 | loss: 0.28576 | val_0_rmse: 0.66792 | val_1_rmse: 0.68072 |  0:00:12s
epoch 76 | loss: 0.2919  | val_0_rmse: 0.67902 | val_1_rmse: 0.6827  |  0:00:12s
epoch 77 | loss: 0.2992  | val_0_rmse: 0.6859  | val_1_rmse: 0.68065 |  0:00:12s
epoch 78 | loss: 0.3113  | val_0_rmse: 0.68106 | val_1_rmse: 0.6789  |  0:00:12s
epoch 79 | loss: 0.30224 | val_0_rmse: 0.67009 | val_1_rmse: 0.67181 |  0:00:12s
epoch 80 | loss: 0.29856 | val_0_rmse: 0.66705 | val_1_rmse: 0.6679  |  0:00:12s
epoch 81 | loss: 0.28527 | val_0_rmse: 0.66547 | val_1_rmse: 0.66868 |  0:00:12s
epoch 82 | loss: 0.28926 | val_0_rmse: 0.66604 | val_1_rmse: 0.67206 |  0:00:13s
epoch 83 | loss: 0.27866 | val_0_rmse: 0.67198 | val_1_rmse: 0.67912 |  0:00:13s
epoch 84 | loss: 0.28448 | val_0_rmse: 0.67225 | val_1_rmse: 0.68346 |  0:00:13s
epoch 85 | loss: 0.29068 | val_0_rmse: 0.67684 | val_1_rmse: 0.69314 |  0:00:13s
epoch 86 | loss: 0.27878 | val_0_rmse: 0.67795 | val_1_rmse: 0.69714 |  0:00:13s
epoch 87 | loss: 0.28785 | val_0_rmse: 0.67133 | val_1_rmse: 0.6899  |  0:00:13s
epoch 88 | loss: 0.27405 | val_0_rmse: 0.66533 | val_1_rmse: 0.68504 |  0:00:14s
epoch 89 | loss: 0.26762 | val_0_rmse: 0.65834 | val_1_rmse: 0.67563 |  0:00:14s
epoch 90 | loss: 0.26526 | val_0_rmse: 0.65791 | val_1_rmse: 0.67174 |  0:00:14s
epoch 91 | loss: 0.26028 | val_0_rmse: 0.65285 | val_1_rmse: 0.6672  |  0:00:14s
epoch 92 | loss: 0.27425 | val_0_rmse: 0.64937 | val_1_rmse: 0.66355 |  0:00:14s
epoch 93 | loss: 0.26303 | val_0_rmse: 0.6494  | val_1_rmse: 0.66203 |  0:00:14s
epoch 94 | loss: 0.27676 | val_0_rmse: 0.64422 | val_1_rmse: 0.65741 |  0:00:15s
epoch 95 | loss: 0.28464 | val_0_rmse: 0.64617 | val_1_rmse: 0.65846 |  0:00:15s
epoch 96 | loss: 0.27358 | val_0_rmse: 0.66279 | val_1_rmse: 0.67009 |  0:00:15s
epoch 97 | loss: 0.27595 | val_0_rmse: 0.66076 | val_1_rmse: 0.66837 |  0:00:15s
epoch 98 | loss: 0.28142 | val_0_rmse: 0.65293 | val_1_rmse: 0.66287 |  0:00:15s
epoch 99 | loss: 0.26198 | val_0_rmse: 0.65579 | val_1_rmse: 0.6638  |  0:00:15s
epoch 100| loss: 0.26854 | val_0_rmse: 0.65207 | val_1_rmse: 0.66317 |  0:00:15s
epoch 101| loss: 0.26279 | val_0_rmse: 0.64555 | val_1_rmse: 0.66296 |  0:00:16s
epoch 102| loss: 0.27448 | val_0_rmse: 0.64526 | val_1_rmse: 0.66437 |  0:00:16s
epoch 103| loss: 0.26519 | val_0_rmse: 0.65341 | val_1_rmse: 0.67156 |  0:00:16s
epoch 104| loss: 0.26686 | val_0_rmse: 0.64434 | val_1_rmse: 0.67405 |  0:00:16s
epoch 105| loss: 0.26503 | val_0_rmse: 0.63871 | val_1_rmse: 0.67498 |  0:00:16s
epoch 106| loss: 0.26241 | val_0_rmse: 0.6387  | val_1_rmse: 0.67354 |  0:00:16s
epoch 107| loss: 0.25534 | val_0_rmse: 0.64237 | val_1_rmse: 0.67498 |  0:00:17s
epoch 108| loss: 0.26811 | val_0_rmse: 0.64803 | val_1_rmse: 0.67932 |  0:00:17s
epoch 109| loss: 0.25543 | val_0_rmse: 0.64412 | val_1_rmse: 0.67778 |  0:00:17s
epoch 110| loss: 0.25773 | val_0_rmse: 0.6531  | val_1_rmse: 0.68463 |  0:00:17s
epoch 111| loss: 0.25929 | val_0_rmse: 0.65586 | val_1_rmse: 0.6874  |  0:00:17s
epoch 112| loss: 0.26275 | val_0_rmse: 0.63056 | val_1_rmse: 0.6697  |  0:00:17s
epoch 113| loss: 0.26332 | val_0_rmse: 0.62788 | val_1_rmse: 0.66843 |  0:00:17s
epoch 114| loss: 0.25964 | val_0_rmse: 0.62931 | val_1_rmse: 0.66584 |  0:00:18s
epoch 115| loss: 0.25331 | val_0_rmse: 0.641   | val_1_rmse: 0.67689 |  0:00:18s
epoch 116| loss: 0.26557 | val_0_rmse: 0.6296  | val_1_rmse: 0.66028 |  0:00:18s
epoch 117| loss: 0.25007 | val_0_rmse: 0.63228 | val_1_rmse: 0.6649  |  0:00:18s
epoch 118| loss: 0.2527  | val_0_rmse: 0.63659 | val_1_rmse: 0.66901 |  0:00:18s
epoch 119| loss: 0.24852 | val_0_rmse: 0.63298 | val_1_rmse: 0.66569 |  0:00:18s
epoch 120| loss: 0.24464 | val_0_rmse: 0.64063 | val_1_rmse: 0.66835 |  0:00:19s
epoch 121| loss: 0.24346 | val_0_rmse: 0.63956 | val_1_rmse: 0.66828 |  0:00:19s
epoch 122| loss: 0.24708 | val_0_rmse: 0.63188 | val_1_rmse: 0.65674 |  0:00:19s
epoch 123| loss: 0.24472 | val_0_rmse: 0.62676 | val_1_rmse: 0.65014 |  0:00:19s
epoch 124| loss: 0.24061 | val_0_rmse: 0.62236 | val_1_rmse: 0.65167 |  0:00:19s
epoch 125| loss: 0.24259 | val_0_rmse: 0.62055 | val_1_rmse: 0.64821 |  0:00:19s
epoch 126| loss: 0.25268 | val_0_rmse: 0.62722 | val_1_rmse: 0.65406 |  0:00:19s
epoch 127| loss: 0.23843 | val_0_rmse: 0.62298 | val_1_rmse: 0.64851 |  0:00:20s
epoch 128| loss: 0.2455  | val_0_rmse: 0.61855 | val_1_rmse: 0.64544 |  0:00:20s
epoch 129| loss: 0.24766 | val_0_rmse: 0.62803 | val_1_rmse: 0.65772 |  0:00:20s
epoch 130| loss: 0.24413 | val_0_rmse: 0.62248 | val_1_rmse: 0.65464 |  0:00:20s
epoch 131| loss: 0.24558 | val_0_rmse: 0.61757 | val_1_rmse: 0.64739 |  0:00:20s
epoch 132| loss: 0.24811 | val_0_rmse: 0.62    | val_1_rmse: 0.64967 |  0:00:20s
epoch 133| loss: 0.25607 | val_0_rmse: 0.62203 | val_1_rmse: 0.65918 |  0:00:21s
epoch 134| loss: 0.24096 | val_0_rmse: 0.63296 | val_1_rmse: 0.67776 |  0:00:21s
epoch 135| loss: 0.2452  | val_0_rmse: 0.62312 | val_1_rmse: 0.65997 |  0:00:21s
epoch 136| loss: 0.24305 | val_0_rmse: 0.61614 | val_1_rmse: 0.6445  |  0:00:21s
epoch 137| loss: 0.24385 | val_0_rmse: 0.61287 | val_1_rmse: 0.64208 |  0:00:21s
epoch 138| loss: 0.23416 | val_0_rmse: 0.60845 | val_1_rmse: 0.64352 |  0:00:21s
epoch 139| loss: 0.24027 | val_0_rmse: 0.6056  | val_1_rmse: 0.64447 |  0:00:22s
epoch 140| loss: 0.23394 | val_0_rmse: 0.60729 | val_1_rmse: 0.64423 |  0:00:22s
epoch 141| loss: 0.23811 | val_0_rmse: 0.60406 | val_1_rmse: 0.64126 |  0:00:22s
epoch 142| loss: 0.2288  | val_0_rmse: 0.60417 | val_1_rmse: 0.64243 |  0:00:22s
epoch 143| loss: 0.23408 | val_0_rmse: 0.60488 | val_1_rmse: 0.64531 |  0:00:22s
epoch 144| loss: 0.22535 | val_0_rmse: 0.61374 | val_1_rmse: 0.6591  |  0:00:22s
epoch 145| loss: 0.23474 | val_0_rmse: 0.60427 | val_1_rmse: 0.65209 |  0:00:22s
epoch 146| loss: 0.22363 | val_0_rmse: 0.59509 | val_1_rmse: 0.64054 |  0:00:23s
epoch 147| loss: 0.22772 | val_0_rmse: 0.59404 | val_1_rmse: 0.63573 |  0:00:23s
epoch 148| loss: 0.22774 | val_0_rmse: 0.59426 | val_1_rmse: 0.63677 |  0:00:23s
epoch 149| loss: 0.23128 | val_0_rmse: 0.59415 | val_1_rmse: 0.63983 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.63573
Best weights from best epoch are automatically used!
ended training at: 08:02:31
Feature importance:
Mean squared error is of 2373638903.84534
Mean absolute error:34355.94456873583
MAPE:0.29499426567161313
R2 score:0.6734850441713667
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:31
epoch 0  | loss: 3.96883 | val_0_rmse: 1.04448 | val_1_rmse: 1.05227 |  0:00:00s
epoch 1  | loss: 3.01302 | val_0_rmse: 1.01873 | val_1_rmse: 1.03106 |  0:00:00s
epoch 2  | loss: 2.09852 | val_0_rmse: 1.01174 | val_1_rmse: 1.01908 |  0:00:00s
epoch 3  | loss: 1.62421 | val_0_rmse: 1.01115 | val_1_rmse: 1.01925 |  0:00:00s
epoch 4  | loss: 1.44475 | val_0_rmse: 1.00902 | val_1_rmse: 1.0197  |  0:00:00s
epoch 5  | loss: 1.21753 | val_0_rmse: 1.00693 | val_1_rmse: 1.01911 |  0:00:00s
epoch 6  | loss: 1.15739 | val_0_rmse: 0.9966  | val_1_rmse: 1.0051  |  0:00:01s
epoch 7  | loss: 1.07333 | val_0_rmse: 0.99778 | val_1_rmse: 1.0085  |  0:00:01s
epoch 8  | loss: 1.02512 | val_0_rmse: 0.99857 | val_1_rmse: 1.01141 |  0:00:01s
epoch 9  | loss: 1.0014  | val_0_rmse: 1.00023 | val_1_rmse: 1.01306 |  0:00:01s
epoch 10 | loss: 0.99277 | val_0_rmse: 0.99961 | val_1_rmse: 1.01544 |  0:00:01s
epoch 11 | loss: 0.99024 | val_0_rmse: 1.00218 | val_1_rmse: 1.01669 |  0:00:01s
epoch 12 | loss: 0.92159 | val_0_rmse: 0.98663 | val_1_rmse: 0.9926  |  0:00:02s
epoch 13 | loss: 0.88267 | val_0_rmse: 0.90031 | val_1_rmse: 0.9074  |  0:00:02s
epoch 14 | loss: 0.85194 | val_0_rmse: 0.85105 | val_1_rmse: 0.8568  |  0:00:02s
epoch 15 | loss: 0.80466 | val_0_rmse: 0.81765 | val_1_rmse: 0.83984 |  0:00:02s
epoch 16 | loss: 0.72089 | val_0_rmse: 0.83729 | val_1_rmse: 0.85871 |  0:00:02s
epoch 17 | loss: 0.70511 | val_0_rmse: 0.82846 | val_1_rmse: 0.8595  |  0:00:02s
epoch 18 | loss: 0.65394 | val_0_rmse: 0.78316 | val_1_rmse: 0.78539 |  0:00:02s
epoch 19 | loss: 0.60604 | val_0_rmse: 0.77717 | val_1_rmse: 0.79048 |  0:00:03s
epoch 20 | loss: 0.58865 | val_0_rmse: 0.79373 | val_1_rmse: 0.80038 |  0:00:03s
epoch 21 | loss: 0.56974 | val_0_rmse: 0.77759 | val_1_rmse: 0.78078 |  0:00:03s
epoch 22 | loss: 0.5534  | val_0_rmse: 0.74975 | val_1_rmse: 0.75135 |  0:00:03s
epoch 23 | loss: 0.51574 | val_0_rmse: 0.73186 | val_1_rmse: 0.73973 |  0:00:03s
epoch 24 | loss: 0.50023 | val_0_rmse: 0.72634 | val_1_rmse: 0.73628 |  0:00:03s
epoch 25 | loss: 0.46998 | val_0_rmse: 0.72901 | val_1_rmse: 0.73846 |  0:00:04s
epoch 26 | loss: 0.46159 | val_0_rmse: 0.72293 | val_1_rmse: 0.72275 |  0:00:04s
epoch 27 | loss: 0.44899 | val_0_rmse: 0.70597 | val_1_rmse: 0.70278 |  0:00:04s
epoch 28 | loss: 0.4597  | val_0_rmse: 0.70528 | val_1_rmse: 0.70609 |  0:00:04s
epoch 29 | loss: 0.43097 | val_0_rmse: 0.70202 | val_1_rmse: 0.70495 |  0:00:04s
epoch 30 | loss: 0.43495 | val_0_rmse: 0.70326 | val_1_rmse: 0.70451 |  0:00:04s
epoch 31 | loss: 0.42271 | val_0_rmse: 0.71389 | val_1_rmse: 0.71613 |  0:00:05s
epoch 32 | loss: 0.42373 | val_0_rmse: 0.70007 | val_1_rmse: 0.69805 |  0:00:05s
epoch 33 | loss: 0.41732 | val_0_rmse: 0.69006 | val_1_rmse: 0.69259 |  0:00:05s
epoch 34 | loss: 0.38602 | val_0_rmse: 0.69753 | val_1_rmse: 0.70069 |  0:00:05s
epoch 35 | loss: 0.39661 | val_0_rmse: 0.67988 | val_1_rmse: 0.68338 |  0:00:05s
epoch 36 | loss: 0.37463 | val_0_rmse: 0.67701 | val_1_rmse: 0.68084 |  0:00:05s
epoch 37 | loss: 0.3817  | val_0_rmse: 0.68725 | val_1_rmse: 0.69493 |  0:00:06s
epoch 38 | loss: 0.38044 | val_0_rmse: 0.6799  | val_1_rmse: 0.68608 |  0:00:06s
epoch 39 | loss: 0.36468 | val_0_rmse: 0.67811 | val_1_rmse: 0.68573 |  0:00:06s
epoch 40 | loss: 0.37182 | val_0_rmse: 0.68438 | val_1_rmse: 0.69195 |  0:00:06s
epoch 41 | loss: 0.3619  | val_0_rmse: 0.68297 | val_1_rmse: 0.68921 |  0:00:06s
epoch 42 | loss: 0.35143 | val_0_rmse: 0.68352 | val_1_rmse: 0.69154 |  0:00:06s
epoch 43 | loss: 0.36247 | val_0_rmse: 0.69417 | val_1_rmse: 0.70539 |  0:00:06s
epoch 44 | loss: 0.34325 | val_0_rmse: 0.70076 | val_1_rmse: 0.71949 |  0:00:07s
epoch 45 | loss: 0.33995 | val_0_rmse: 0.68703 | val_1_rmse: 0.70625 |  0:00:07s
epoch 46 | loss: 0.33904 | val_0_rmse: 0.68379 | val_1_rmse: 0.70044 |  0:00:07s
epoch 47 | loss: 0.35197 | val_0_rmse: 0.67681 | val_1_rmse: 0.69297 |  0:00:07s
epoch 48 | loss: 0.35264 | val_0_rmse: 0.66616 | val_1_rmse: 0.68746 |  0:00:07s
epoch 49 | loss: 0.33356 | val_0_rmse: 0.65481 | val_1_rmse: 0.68235 |  0:00:07s
epoch 50 | loss: 0.33025 | val_0_rmse: 0.65461 | val_1_rmse: 0.67896 |  0:00:08s
epoch 51 | loss: 0.32359 | val_0_rmse: 0.65814 | val_1_rmse: 0.68025 |  0:00:08s
epoch 52 | loss: 0.32253 | val_0_rmse: 0.65961 | val_1_rmse: 0.68556 |  0:00:08s
epoch 53 | loss: 0.32883 | val_0_rmse: 0.65215 | val_1_rmse: 0.68291 |  0:00:08s
epoch 54 | loss: 0.30604 | val_0_rmse: 0.64864 | val_1_rmse: 0.67496 |  0:00:08s
epoch 55 | loss: 0.30853 | val_0_rmse: 0.66399 | val_1_rmse: 0.68474 |  0:00:08s
epoch 56 | loss: 0.30651 | val_0_rmse: 0.65857 | val_1_rmse: 0.6836  |  0:00:09s
epoch 57 | loss: 0.30701 | val_0_rmse: 0.64499 | val_1_rmse: 0.67573 |  0:00:09s
epoch 58 | loss: 0.2968  | val_0_rmse: 0.64792 | val_1_rmse: 0.67579 |  0:00:09s
epoch 59 | loss: 0.29067 | val_0_rmse: 0.64437 | val_1_rmse: 0.67711 |  0:00:09s
epoch 60 | loss: 0.29147 | val_0_rmse: 0.64872 | val_1_rmse: 0.68136 |  0:00:09s
epoch 61 | loss: 0.30074 | val_0_rmse: 0.65041 | val_1_rmse: 0.67641 |  0:00:09s
epoch 62 | loss: 0.28889 | val_0_rmse: 0.64908 | val_1_rmse: 0.67468 |  0:00:09s
epoch 63 | loss: 0.30363 | val_0_rmse: 0.64531 | val_1_rmse: 0.67293 |  0:00:10s
epoch 64 | loss: 0.30245 | val_0_rmse: 0.65017 | val_1_rmse: 0.6823  |  0:00:10s
epoch 65 | loss: 0.2765  | val_0_rmse: 0.64694 | val_1_rmse: 0.68191 |  0:00:10s
epoch 66 | loss: 0.2812  | val_0_rmse: 0.64753 | val_1_rmse: 0.67906 |  0:00:10s
epoch 67 | loss: 0.27751 | val_0_rmse: 0.64583 | val_1_rmse: 0.67964 |  0:00:10s
epoch 68 | loss: 0.29603 | val_0_rmse: 0.64213 | val_1_rmse: 0.67739 |  0:00:10s
epoch 69 | loss: 0.28119 | val_0_rmse: 0.64875 | val_1_rmse: 0.68246 |  0:00:11s
epoch 70 | loss: 0.28726 | val_0_rmse: 0.65317 | val_1_rmse: 0.69021 |  0:00:11s
epoch 71 | loss: 0.27275 | val_0_rmse: 0.63535 | val_1_rmse: 0.67507 |  0:00:11s
epoch 72 | loss: 0.28311 | val_0_rmse: 0.63995 | val_1_rmse: 0.67246 |  0:00:11s
epoch 73 | loss: 0.27587 | val_0_rmse: 0.67497 | val_1_rmse: 0.69886 |  0:00:11s
epoch 74 | loss: 0.26965 | val_0_rmse: 0.6901  | val_1_rmse: 0.72136 |  0:00:11s
epoch 75 | loss: 0.27959 | val_0_rmse: 0.6599  | val_1_rmse: 0.69735 |  0:00:11s
epoch 76 | loss: 0.26273 | val_0_rmse: 0.65409 | val_1_rmse: 0.68242 |  0:00:12s
epoch 77 | loss: 0.25012 | val_0_rmse: 0.65632 | val_1_rmse: 0.69086 |  0:00:12s
epoch 78 | loss: 0.25338 | val_0_rmse: 0.64655 | val_1_rmse: 0.69096 |  0:00:12s
epoch 79 | loss: 0.25664 | val_0_rmse: 0.6403  | val_1_rmse: 0.68885 |  0:00:12s
epoch 80 | loss: 0.25463 | val_0_rmse: 0.63965 | val_1_rmse: 0.67987 |  0:00:12s
epoch 81 | loss: 0.26685 | val_0_rmse: 0.63318 | val_1_rmse: 0.6748  |  0:00:12s
epoch 82 | loss: 0.2578  | val_0_rmse: 0.63441 | val_1_rmse: 0.69076 |  0:00:13s
epoch 83 | loss: 0.26816 | val_0_rmse: 0.62777 | val_1_rmse: 0.67374 |  0:00:13s
epoch 84 | loss: 0.2623  | val_0_rmse: 0.63497 | val_1_rmse: 0.66993 |  0:00:13s
epoch 85 | loss: 0.25612 | val_0_rmse: 0.63471 | val_1_rmse: 0.6733  |  0:00:13s
epoch 86 | loss: 0.25317 | val_0_rmse: 0.62838 | val_1_rmse: 0.66948 |  0:00:13s
epoch 87 | loss: 0.25506 | val_0_rmse: 0.62194 | val_1_rmse: 0.67059 |  0:00:13s
epoch 88 | loss: 0.25169 | val_0_rmse: 0.63141 | val_1_rmse: 0.68223 |  0:00:13s
epoch 89 | loss: 0.24043 | val_0_rmse: 0.6248  | val_1_rmse: 0.68352 |  0:00:14s
epoch 90 | loss: 0.24535 | val_0_rmse: 0.61559 | val_1_rmse: 0.68294 |  0:00:14s
epoch 91 | loss: 0.24137 | val_0_rmse: 0.63528 | val_1_rmse: 0.69539 |  0:00:14s
epoch 92 | loss: 0.24077 | val_0_rmse: 0.66325 | val_1_rmse: 0.72272 |  0:00:14s
epoch 93 | loss: 0.24797 | val_0_rmse: 0.61698 | val_1_rmse: 0.68646 |  0:00:14s
epoch 94 | loss: 0.23785 | val_0_rmse: 0.60735 | val_1_rmse: 0.67637 |  0:00:14s
epoch 95 | loss: 0.24743 | val_0_rmse: 0.62014 | val_1_rmse: 0.68157 |  0:00:15s
epoch 96 | loss: 0.23294 | val_0_rmse: 0.62712 | val_1_rmse: 0.71162 |  0:00:15s
epoch 97 | loss: 0.25013 | val_0_rmse: 0.61656 | val_1_rmse: 0.6983  |  0:00:15s
epoch 98 | loss: 0.22748 | val_0_rmse: 0.60256 | val_1_rmse: 0.67075 |  0:00:15s
epoch 99 | loss: 0.23826 | val_0_rmse: 0.60869 | val_1_rmse: 0.66428 |  0:00:15s
epoch 100| loss: 0.23074 | val_0_rmse: 0.61535 | val_1_rmse: 0.668   |  0:00:15s
epoch 101| loss: 0.22592 | val_0_rmse: 0.61903 | val_1_rmse: 0.66955 |  0:00:16s
epoch 102| loss: 0.22858 | val_0_rmse: 0.61322 | val_1_rmse: 0.6757  |  0:00:16s
epoch 103| loss: 0.22241 | val_0_rmse: 0.61234 | val_1_rmse: 0.68414 |  0:00:16s
epoch 104| loss: 0.22634 | val_0_rmse: 0.61664 | val_1_rmse: 0.67953 |  0:00:16s
epoch 105| loss: 0.21671 | val_0_rmse: 0.62805 | val_1_rmse: 0.68458 |  0:00:16s
epoch 106| loss: 0.21682 | val_0_rmse: 0.62453 | val_1_rmse: 0.68585 |  0:00:16s
epoch 107| loss: 0.21211 | val_0_rmse: 0.62008 | val_1_rmse: 0.69139 |  0:00:16s
epoch 108| loss: 0.2093  | val_0_rmse: 0.61715 | val_1_rmse: 0.69264 |  0:00:17s
epoch 109| loss: 0.2044  | val_0_rmse: 0.61946 | val_1_rmse: 0.69379 |  0:00:17s
epoch 110| loss: 0.19976 | val_0_rmse: 0.61671 | val_1_rmse: 0.69117 |  0:00:17s
epoch 111| loss: 0.21035 | val_0_rmse: 0.61829 | val_1_rmse: 0.69385 |  0:00:17s
epoch 112| loss: 0.19763 | val_0_rmse: 0.62934 | val_1_rmse: 0.70602 |  0:00:17s
epoch 113| loss: 0.21484 | val_0_rmse: 0.63286 | val_1_rmse: 0.70583 |  0:00:17s
epoch 114| loss: 0.20113 | val_0_rmse: 0.61637 | val_1_rmse: 0.69994 |  0:00:18s
epoch 115| loss: 0.20105 | val_0_rmse: 0.62885 | val_1_rmse: 0.7126  |  0:00:18s
epoch 116| loss: 0.19571 | val_0_rmse: 0.63434 | val_1_rmse: 0.7229  |  0:00:18s
epoch 117| loss: 0.21606 | val_0_rmse: 0.61832 | val_1_rmse: 0.71173 |  0:00:18s
epoch 118| loss: 0.20748 | val_0_rmse: 0.6131  | val_1_rmse: 0.70453 |  0:00:18s
epoch 119| loss: 0.21951 | val_0_rmse: 0.61857 | val_1_rmse: 0.70356 |  0:00:18s
epoch 120| loss: 0.21597 | val_0_rmse: 0.6142  | val_1_rmse: 0.70259 |  0:00:18s
epoch 121| loss: 0.21364 | val_0_rmse: 0.6097  | val_1_rmse: 0.69829 |  0:00:19s
epoch 122| loss: 0.20872 | val_0_rmse: 0.61012 | val_1_rmse: 0.69147 |  0:00:19s
epoch 123| loss: 0.2177  | val_0_rmse: 0.61226 | val_1_rmse: 0.68812 |  0:00:19s
epoch 124| loss: 0.20415 | val_0_rmse: 0.60988 | val_1_rmse: 0.68941 |  0:00:19s
epoch 125| loss: 0.20465 | val_0_rmse: 0.60289 | val_1_rmse: 0.68731 |  0:00:19s
epoch 126| loss: 0.2023  | val_0_rmse: 0.59799 | val_1_rmse: 0.68999 |  0:00:19s
epoch 127| loss: 0.19983 | val_0_rmse: 0.59195 | val_1_rmse: 0.68778 |  0:00:20s
epoch 128| loss: 0.19218 | val_0_rmse: 0.58931 | val_1_rmse: 0.6914  |  0:00:20s
epoch 129| loss: 0.18692 | val_0_rmse: 0.58825 | val_1_rmse: 0.69419 |  0:00:20s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.66428
Best weights from best epoch are automatically used!
ended training at: 08:02:51
Feature importance:
Mean squared error is of 2841257481.830548
Mean absolute error:37929.70970060942
MAPE:0.3166101146032853
R2 score:0.6064424170325117
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:02:51
epoch 0  | loss: 4.64649 | val_0_rmse: 1.02579 | val_1_rmse: 1.00048 |  0:00:00s
epoch 1  | loss: 2.30721 | val_0_rmse: 1.01571 | val_1_rmse: 0.98881 |  0:00:00s
epoch 2  | loss: 1.75144 | val_0_rmse: 1.00485 | val_1_rmse: 0.98022 |  0:00:00s
epoch 3  | loss: 1.67411 | val_0_rmse: 1.0051  | val_1_rmse: 0.98175 |  0:00:00s
epoch 4  | loss: 1.44332 | val_0_rmse: 1.00702 | val_1_rmse: 0.98532 |  0:00:00s
epoch 5  | loss: 1.19894 | val_0_rmse: 1.00585 | val_1_rmse: 0.98434 |  0:00:00s
epoch 6  | loss: 1.23939 | val_0_rmse: 1.00377 | val_1_rmse: 0.9817  |  0:00:01s
epoch 7  | loss: 1.12491 | val_0_rmse: 1.00304 | val_1_rmse: 0.98111 |  0:00:01s
epoch 8  | loss: 1.06192 | val_0_rmse: 1.00421 | val_1_rmse: 0.98318 |  0:00:01s
epoch 9  | loss: 1.06796 | val_0_rmse: 0.99997 | val_1_rmse: 0.97662 |  0:00:01s
epoch 10 | loss: 1.03961 | val_0_rmse: 0.99774 | val_1_rmse: 0.97244 |  0:00:01s
epoch 11 | loss: 1.02999 | val_0_rmse: 0.99712 | val_1_rmse: 0.97378 |  0:00:01s
epoch 12 | loss: 1.03375 | val_0_rmse: 0.98754 | val_1_rmse: 0.95916 |  0:00:02s
epoch 13 | loss: 1.00105 | val_0_rmse: 0.98601 | val_1_rmse: 0.95143 |  0:00:02s
epoch 14 | loss: 0.99762 | val_0_rmse: 0.98514 | val_1_rmse: 0.9503  |  0:00:02s
epoch 15 | loss: 0.95894 | val_0_rmse: 0.96994 | val_1_rmse: 0.93428 |  0:00:02s
epoch 16 | loss: 0.90062 | val_0_rmse: 0.93418 | val_1_rmse: 0.89916 |  0:00:02s
epoch 17 | loss: 0.86959 | val_0_rmse: 0.87676 | val_1_rmse: 0.84938 |  0:00:02s
epoch 18 | loss: 0.81147 | val_0_rmse: 0.85203 | val_1_rmse: 0.83489 |  0:00:03s
epoch 19 | loss: 0.73962 | val_0_rmse: 0.85211 | val_1_rmse: 0.84241 |  0:00:03s
epoch 20 | loss: 0.70542 | val_0_rmse: 0.83509 | val_1_rmse: 0.82446 |  0:00:03s
epoch 21 | loss: 0.64057 | val_0_rmse: 0.80586 | val_1_rmse: 0.79365 |  0:00:03s
epoch 22 | loss: 0.59075 | val_0_rmse: 0.77729 | val_1_rmse: 0.75515 |  0:00:03s
epoch 23 | loss: 0.54333 | val_0_rmse: 0.77512 | val_1_rmse: 0.7405  |  0:00:03s
epoch 24 | loss: 0.5136  | val_0_rmse: 0.77694 | val_1_rmse: 0.73595 |  0:00:04s
epoch 25 | loss: 0.49829 | val_0_rmse: 0.7542  | val_1_rmse: 0.7106  |  0:00:04s
epoch 26 | loss: 0.48752 | val_0_rmse: 0.74712 | val_1_rmse: 0.68788 |  0:00:04s
epoch 27 | loss: 0.44905 | val_0_rmse: 0.75588 | val_1_rmse: 0.68327 |  0:00:04s
epoch 28 | loss: 0.44691 | val_0_rmse: 0.74205 | val_1_rmse: 0.66853 |  0:00:04s
epoch 29 | loss: 0.44361 | val_0_rmse: 0.73394 | val_1_rmse: 0.67388 |  0:00:04s
epoch 30 | loss: 0.43886 | val_0_rmse: 0.7332  | val_1_rmse: 0.67993 |  0:00:05s
epoch 31 | loss: 0.41339 | val_0_rmse: 0.72125 | val_1_rmse: 0.67578 |  0:00:05s
epoch 32 | loss: 0.39617 | val_0_rmse: 0.71214 | val_1_rmse: 0.68253 |  0:00:05s
epoch 33 | loss: 0.38738 | val_0_rmse: 0.7124  | val_1_rmse: 0.676   |  0:00:05s
epoch 34 | loss: 0.38788 | val_0_rmse: 0.71799 | val_1_rmse: 0.67406 |  0:00:05s
epoch 35 | loss: 0.37362 | val_0_rmse: 0.71745 | val_1_rmse: 0.67203 |  0:00:05s
epoch 36 | loss: 0.35128 | val_0_rmse: 0.72228 | val_1_rmse: 0.68528 |  0:00:05s
epoch 37 | loss: 0.35011 | val_0_rmse: 0.72604 | val_1_rmse: 0.69667 |  0:00:06s
epoch 38 | loss: 0.34471 | val_0_rmse: 0.71536 | val_1_rmse: 0.68458 |  0:00:06s
epoch 39 | loss: 0.35407 | val_0_rmse: 0.70701 | val_1_rmse: 0.67534 |  0:00:06s
epoch 40 | loss: 0.35542 | val_0_rmse: 0.70716 | val_1_rmse: 0.675   |  0:00:06s
epoch 41 | loss: 0.32925 | val_0_rmse: 0.7086  | val_1_rmse: 0.67862 |  0:00:06s
epoch 42 | loss: 0.34181 | val_0_rmse: 0.7063  | val_1_rmse: 0.67098 |  0:00:06s
epoch 43 | loss: 0.32157 | val_0_rmse: 0.70049 | val_1_rmse: 0.66478 |  0:00:07s
epoch 44 | loss: 0.31763 | val_0_rmse: 0.69396 | val_1_rmse: 0.65823 |  0:00:07s
epoch 45 | loss: 0.31087 | val_0_rmse: 0.69613 | val_1_rmse: 0.66114 |  0:00:07s
epoch 46 | loss: 0.29568 | val_0_rmse: 0.69056 | val_1_rmse: 0.6534  |  0:00:07s
epoch 47 | loss: 0.28885 | val_0_rmse: 0.68637 | val_1_rmse: 0.64825 |  0:00:07s
epoch 48 | loss: 0.30014 | val_0_rmse: 0.69977 | val_1_rmse: 0.66286 |  0:00:07s
epoch 49 | loss: 0.29385 | val_0_rmse: 0.7106  | val_1_rmse: 0.67326 |  0:00:07s
epoch 50 | loss: 0.2916  | val_0_rmse: 0.70329 | val_1_rmse: 0.66291 |  0:00:08s
epoch 51 | loss: 0.28577 | val_0_rmse: 0.68874 | val_1_rmse: 0.64929 |  0:00:08s
epoch 52 | loss: 0.28534 | val_0_rmse: 0.68732 | val_1_rmse: 0.65476 |  0:00:08s
epoch 53 | loss: 0.27608 | val_0_rmse: 0.68418 | val_1_rmse: 0.65006 |  0:00:08s
epoch 54 | loss: 0.26617 | val_0_rmse: 0.69254 | val_1_rmse: 0.64708 |  0:00:08s
epoch 55 | loss: 0.27096 | val_0_rmse: 0.69161 | val_1_rmse: 0.64373 |  0:00:08s
epoch 56 | loss: 0.26246 | val_0_rmse: 0.68221 | val_1_rmse: 0.64361 |  0:00:09s
epoch 57 | loss: 0.264   | val_0_rmse: 0.6859  | val_1_rmse: 0.65708 |  0:00:09s
epoch 58 | loss: 0.26216 | val_0_rmse: 0.69366 | val_1_rmse: 0.66187 |  0:00:09s
epoch 59 | loss: 0.25209 | val_0_rmse: 0.68022 | val_1_rmse: 0.64441 |  0:00:09s
epoch 60 | loss: 0.24975 | val_0_rmse: 0.67257 | val_1_rmse: 0.642   |  0:00:09s
epoch 61 | loss: 0.2379  | val_0_rmse: 0.68288 | val_1_rmse: 0.659   |  0:00:09s
epoch 62 | loss: 0.2519  | val_0_rmse: 0.66842 | val_1_rmse: 0.64649 |  0:00:10s
epoch 63 | loss: 0.23719 | val_0_rmse: 0.66625 | val_1_rmse: 0.64726 |  0:00:10s
epoch 64 | loss: 0.2236  | val_0_rmse: 0.67593 | val_1_rmse: 0.65274 |  0:00:10s
epoch 65 | loss: 0.2282  | val_0_rmse: 0.67366 | val_1_rmse: 0.64526 |  0:00:10s
epoch 66 | loss: 0.22228 | val_0_rmse: 0.67068 | val_1_rmse: 0.63885 |  0:00:10s
epoch 67 | loss: 0.23008 | val_0_rmse: 0.68588 | val_1_rmse: 0.65292 |  0:00:10s
epoch 68 | loss: 0.24308 | val_0_rmse: 0.68552 | val_1_rmse: 0.65784 |  0:00:10s
epoch 69 | loss: 0.23414 | val_0_rmse: 0.66782 | val_1_rmse: 0.64219 |  0:00:11s
epoch 70 | loss: 0.24376 | val_0_rmse: 0.68214 | val_1_rmse: 0.64466 |  0:00:11s
epoch 71 | loss: 0.23406 | val_0_rmse: 0.69325 | val_1_rmse: 0.64652 |  0:00:11s
epoch 72 | loss: 0.23131 | val_0_rmse: 0.67758 | val_1_rmse: 0.63077 |  0:00:11s
epoch 73 | loss: 0.22673 | val_0_rmse: 0.66993 | val_1_rmse: 0.6266  |  0:00:11s
epoch 74 | loss: 0.22094 | val_0_rmse: 0.67854 | val_1_rmse: 0.63995 |  0:00:11s
epoch 75 | loss: 0.23882 | val_0_rmse: 0.67271 | val_1_rmse: 0.63784 |  0:00:12s
epoch 76 | loss: 0.21527 | val_0_rmse: 0.65321 | val_1_rmse: 0.62403 |  0:00:12s
epoch 77 | loss: 0.20821 | val_0_rmse: 0.65807 | val_1_rmse: 0.62658 |  0:00:12s
epoch 78 | loss: 0.20834 | val_0_rmse: 0.67099 | val_1_rmse: 0.63458 |  0:00:12s
epoch 79 | loss: 0.21707 | val_0_rmse: 0.67054 | val_1_rmse: 0.6312  |  0:00:12s
epoch 80 | loss: 0.20779 | val_0_rmse: 0.66906 | val_1_rmse: 0.6343  |  0:00:12s
epoch 81 | loss: 0.2017  | val_0_rmse: 0.67294 | val_1_rmse: 0.63994 |  0:00:13s
epoch 82 | loss: 0.21765 | val_0_rmse: 0.67253 | val_1_rmse: 0.63294 |  0:00:13s
epoch 83 | loss: 0.20496 | val_0_rmse: 0.67346 | val_1_rmse: 0.62756 |  0:00:13s
epoch 84 | loss: 0.20829 | val_0_rmse: 0.67632 | val_1_rmse: 0.62653 |  0:00:13s
epoch 85 | loss: 0.2     | val_0_rmse: 0.66654 | val_1_rmse: 0.61936 |  0:00:13s
epoch 86 | loss: 0.20308 | val_0_rmse: 0.66167 | val_1_rmse: 0.61932 |  0:00:13s
epoch 87 | loss: 0.19818 | val_0_rmse: 0.66592 | val_1_rmse: 0.62336 |  0:00:13s
epoch 88 | loss: 0.19064 | val_0_rmse: 0.6692  | val_1_rmse: 0.62548 |  0:00:14s
epoch 89 | loss: 0.21103 | val_0_rmse: 0.66144 | val_1_rmse: 0.62154 |  0:00:14s
epoch 90 | loss: 0.19713 | val_0_rmse: 0.67144 | val_1_rmse: 0.63499 |  0:00:14s
epoch 91 | loss: 0.19332 | val_0_rmse: 0.67279 | val_1_rmse: 0.63394 |  0:00:14s
epoch 92 | loss: 0.19541 | val_0_rmse: 0.68176 | val_1_rmse: 0.63565 |  0:00:14s
epoch 93 | loss: 0.19216 | val_0_rmse: 0.70377 | val_1_rmse: 0.65458 |  0:00:14s
epoch 94 | loss: 0.18775 | val_0_rmse: 0.69014 | val_1_rmse: 0.64397 |  0:00:15s
epoch 95 | loss: 0.17816 | val_0_rmse: 0.66106 | val_1_rmse: 0.62616 |  0:00:15s
epoch 96 | loss: 0.17478 | val_0_rmse: 0.64658 | val_1_rmse: 0.62007 |  0:00:15s
epoch 97 | loss: 0.17819 | val_0_rmse: 0.64076 | val_1_rmse: 0.61641 |  0:00:15s
epoch 98 | loss: 0.18304 | val_0_rmse: 0.64671 | val_1_rmse: 0.62004 |  0:00:15s
epoch 99 | loss: 0.1674  | val_0_rmse: 0.64755 | val_1_rmse: 0.61658 |  0:00:15s
epoch 100| loss: 0.18115 | val_0_rmse: 0.63853 | val_1_rmse: 0.61194 |  0:00:15s
epoch 101| loss: 0.1834  | val_0_rmse: 0.63135 | val_1_rmse: 0.61925 |  0:00:16s
epoch 102| loss: 0.19054 | val_0_rmse: 0.6244  | val_1_rmse: 0.62787 |  0:00:16s
epoch 103| loss: 0.19712 | val_0_rmse: 0.61656 | val_1_rmse: 0.62499 |  0:00:16s
epoch 104| loss: 0.21618 | val_0_rmse: 0.61358 | val_1_rmse: 0.61309 |  0:00:16s
epoch 105| loss: 0.19289 | val_0_rmse: 0.63274 | val_1_rmse: 0.61086 |  0:00:16s
epoch 106| loss: 0.19084 | val_0_rmse: 0.64993 | val_1_rmse: 0.61675 |  0:00:16s
epoch 107| loss: 0.18118 | val_0_rmse: 0.62447 | val_1_rmse: 0.60127 |  0:00:17s
epoch 108| loss: 0.19208 | val_0_rmse: 0.61025 | val_1_rmse: 0.59816 |  0:00:17s
epoch 109| loss: 0.1905  | val_0_rmse: 0.61532 | val_1_rmse: 0.60391 |  0:00:17s
epoch 110| loss: 0.18149 | val_0_rmse: 0.61387 | val_1_rmse: 0.60558 |  0:00:17s
epoch 111| loss: 0.17889 | val_0_rmse: 0.61171 | val_1_rmse: 0.61018 |  0:00:17s
epoch 112| loss: 0.19098 | val_0_rmse: 0.61072 | val_1_rmse: 0.60588 |  0:00:17s
epoch 113| loss: 0.16821 | val_0_rmse: 0.61364 | val_1_rmse: 0.60763 |  0:00:18s
epoch 114| loss: 0.18012 | val_0_rmse: 0.60794 | val_1_rmse: 0.60275 |  0:00:18s
epoch 115| loss: 0.16658 | val_0_rmse: 0.60814 | val_1_rmse: 0.60102 |  0:00:18s
epoch 116| loss: 0.16468 | val_0_rmse: 0.61629 | val_1_rmse: 0.6053  |  0:00:18s
epoch 117| loss: 0.16213 | val_0_rmse: 0.61391 | val_1_rmse: 0.60451 |  0:00:18s
epoch 118| loss: 0.15197 | val_0_rmse: 0.61333 | val_1_rmse: 0.60766 |  0:00:18s
epoch 119| loss: 0.16858 | val_0_rmse: 0.60124 | val_1_rmse: 0.60902 |  0:00:18s
epoch 120| loss: 0.15058 | val_0_rmse: 0.6029  | val_1_rmse: 0.61886 |  0:00:19s
epoch 121| loss: 0.14689 | val_0_rmse: 0.61051 | val_1_rmse: 0.61706 |  0:00:19s
epoch 122| loss: 0.15563 | val_0_rmse: 0.60385 | val_1_rmse: 0.61214 |  0:00:19s
epoch 123| loss: 0.16373 | val_0_rmse: 0.59821 | val_1_rmse: 0.6057  |  0:00:19s
epoch 124| loss: 0.15865 | val_0_rmse: 0.61967 | val_1_rmse: 0.61596 |  0:00:19s
epoch 125| loss: 0.14546 | val_0_rmse: 0.6114  | val_1_rmse: 0.60973 |  0:00:19s
epoch 126| loss: 0.15724 | val_0_rmse: 0.60091 | val_1_rmse: 0.61217 |  0:00:20s
epoch 127| loss: 0.15725 | val_0_rmse: 0.59642 | val_1_rmse: 0.60767 |  0:00:20s
epoch 128| loss: 0.14002 | val_0_rmse: 0.58895 | val_1_rmse: 0.60447 |  0:00:20s
epoch 129| loss: 0.1453  | val_0_rmse: 0.5856  | val_1_rmse: 0.60081 |  0:00:20s
epoch 130| loss: 0.13918 | val_0_rmse: 0.58375 | val_1_rmse: 0.59656 |  0:00:20s
epoch 131| loss: 0.14411 | val_0_rmse: 0.5869  | val_1_rmse: 0.59669 |  0:00:20s
epoch 132| loss: 0.1399  | val_0_rmse: 0.5813  | val_1_rmse: 0.59751 |  0:00:20s
epoch 133| loss: 0.15668 | val_0_rmse: 0.57722 | val_1_rmse: 0.60185 |  0:00:21s
epoch 134| loss: 0.14137 | val_0_rmse: 0.5792  | val_1_rmse: 0.61142 |  0:00:21s
epoch 135| loss: 0.14419 | val_0_rmse: 0.56643 | val_1_rmse: 0.60994 |  0:00:21s
epoch 136| loss: 0.15025 | val_0_rmse: 0.5551  | val_1_rmse: 0.61314 |  0:00:21s
epoch 137| loss: 0.13424 | val_0_rmse: 0.56174 | val_1_rmse: 0.60681 |  0:00:21s
epoch 138| loss: 0.13753 | val_0_rmse: 0.58171 | val_1_rmse: 0.61626 |  0:00:21s
epoch 139| loss: 0.13281 | val_0_rmse: 0.57023 | val_1_rmse: 0.61299 |  0:00:22s
epoch 140| loss: 0.13392 | val_0_rmse: 0.56304 | val_1_rmse: 0.61642 |  0:00:22s
epoch 141| loss: 0.13925 | val_0_rmse: 0.5809  | val_1_rmse: 0.62331 |  0:00:22s
epoch 142| loss: 0.14146 | val_0_rmse: 0.58865 | val_1_rmse: 0.62409 |  0:00:22s
epoch 143| loss: 0.13099 | val_0_rmse: 0.56879 | val_1_rmse: 0.62198 |  0:00:22s
epoch 144| loss: 0.14408 | val_0_rmse: 0.55176 | val_1_rmse: 0.61985 |  0:00:22s
epoch 145| loss: 0.13364 | val_0_rmse: 0.55681 | val_1_rmse: 0.61107 |  0:00:23s
epoch 146| loss: 0.12822 | val_0_rmse: 0.55694 | val_1_rmse: 0.61507 |  0:00:23s
epoch 147| loss: 0.13376 | val_0_rmse: 0.53975 | val_1_rmse: 0.61527 |  0:00:23s
epoch 148| loss: 0.11895 | val_0_rmse: 0.53467 | val_1_rmse: 0.61312 |  0:00:23s
epoch 149| loss: 0.11932 | val_0_rmse: 0.53087 | val_1_rmse: 0.60853 |  0:00:23s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.59656
Best weights from best epoch are automatically used!
ended training at: 08:03:15
Feature importance:
Mean squared error is of 2810660149.7490788
Mean absolute error:37806.3129058957
MAPE:0.3480102836258984
R2 score:0.5973837066467175
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:03:15
epoch 0  | loss: 3.36844 | val_0_rmse: 0.99309 | val_1_rmse: 0.99141 |  0:00:00s
epoch 1  | loss: 1.40249 | val_0_rmse: 0.97947 | val_1_rmse: 0.97597 |  0:00:00s
epoch 2  | loss: 0.96708 | val_0_rmse: 0.85886 | val_1_rmse: 0.86348 |  0:00:01s
epoch 3  | loss: 0.81166 | val_0_rmse: 0.82948 | val_1_rmse: 0.83777 |  0:00:01s
epoch 4  | loss: 0.71478 | val_0_rmse: 0.93433 | val_1_rmse: 0.9477  |  0:00:01s
epoch 5  | loss: 0.69942 | val_0_rmse: 0.85937 | val_1_rmse: 0.87656 |  0:00:02s
epoch 6  | loss: 0.65023 | val_0_rmse: 0.83048 | val_1_rmse: 0.84832 |  0:00:02s
epoch 7  | loss: 0.62136 | val_0_rmse: 0.81454 | val_1_rmse: 0.8278  |  0:00:02s
epoch 8  | loss: 0.59706 | val_0_rmse: 0.78982 | val_1_rmse: 0.80549 |  0:00:03s
epoch 9  | loss: 0.57194 | val_0_rmse: 0.76374 | val_1_rmse: 0.77463 |  0:00:03s
epoch 10 | loss: 0.54295 | val_0_rmse: 0.75861 | val_1_rmse: 0.7653  |  0:00:03s
epoch 11 | loss: 0.52783 | val_0_rmse: 0.78002 | val_1_rmse: 0.77915 |  0:00:04s
epoch 12 | loss: 0.53192 | val_0_rmse: 0.76391 | val_1_rmse: 0.7641  |  0:00:04s
epoch 13 | loss: 0.51069 | val_0_rmse: 0.75456 | val_1_rmse: 0.76025 |  0:00:04s
epoch 14 | loss: 0.51305 | val_0_rmse: 0.78795 | val_1_rmse: 0.78779 |  0:00:05s
epoch 15 | loss: 0.48958 | val_0_rmse: 0.7653  | val_1_rmse: 0.77412 |  0:00:05s
epoch 16 | loss: 0.48115 | val_0_rmse: 0.77028 | val_1_rmse: 0.77985 |  0:00:05s
epoch 17 | loss: 0.47048 | val_0_rmse: 0.75541 | val_1_rmse: 0.76714 |  0:00:06s
epoch 18 | loss: 0.45041 | val_0_rmse: 0.76479 | val_1_rmse: 0.77851 |  0:00:06s
epoch 19 | loss: 0.43973 | val_0_rmse: 0.74995 | val_1_rmse: 0.76785 |  0:00:06s
epoch 20 | loss: 0.43444 | val_0_rmse: 0.76138 | val_1_rmse: 0.77604 |  0:00:07s
epoch 21 | loss: 0.43367 | val_0_rmse: 0.75517 | val_1_rmse: 0.77005 |  0:00:07s
epoch 22 | loss: 0.42511 | val_0_rmse: 0.74028 | val_1_rmse: 0.75595 |  0:00:07s
epoch 23 | loss: 0.42474 | val_0_rmse: 0.74323 | val_1_rmse: 0.75068 |  0:00:08s
epoch 24 | loss: 0.41099 | val_0_rmse: 0.74473 | val_1_rmse: 0.7565  |  0:00:08s
epoch 25 | loss: 0.41752 | val_0_rmse: 0.74753 | val_1_rmse: 0.76704 |  0:00:08s
epoch 26 | loss: 0.42201 | val_0_rmse: 0.77206 | val_1_rmse: 0.79387 |  0:00:09s
epoch 27 | loss: 0.41245 | val_0_rmse: 0.7506  | val_1_rmse: 0.76827 |  0:00:09s
epoch 28 | loss: 0.40802 | val_0_rmse: 0.76922 | val_1_rmse: 0.77782 |  0:00:09s
epoch 29 | loss: 0.40682 | val_0_rmse: 0.7416  | val_1_rmse: 0.7518  |  0:00:10s
epoch 30 | loss: 0.40835 | val_0_rmse: 0.75958 | val_1_rmse: 0.77184 |  0:00:10s
epoch 31 | loss: 0.40917 | val_0_rmse: 0.74411 | val_1_rmse: 0.75764 |  0:00:11s
epoch 32 | loss: 0.39888 | val_0_rmse: 0.75871 | val_1_rmse: 0.77006 |  0:00:11s
epoch 33 | loss: 0.40324 | val_0_rmse: 0.73458 | val_1_rmse: 0.74775 |  0:00:11s
epoch 34 | loss: 0.39249 | val_0_rmse: 0.7393  | val_1_rmse: 0.74924 |  0:00:12s
epoch 35 | loss: 0.39283 | val_0_rmse: 0.73031 | val_1_rmse: 0.74137 |  0:00:12s
epoch 36 | loss: 0.40388 | val_0_rmse: 0.75361 | val_1_rmse: 0.76203 |  0:00:12s
epoch 37 | loss: 0.39346 | val_0_rmse: 0.73747 | val_1_rmse: 0.75004 |  0:00:13s
epoch 38 | loss: 0.38524 | val_0_rmse: 0.73706 | val_1_rmse: 0.75124 |  0:00:13s
epoch 39 | loss: 0.39019 | val_0_rmse: 0.73724 | val_1_rmse: 0.75122 |  0:00:13s
epoch 40 | loss: 0.38868 | val_0_rmse: 0.72781 | val_1_rmse: 0.74427 |  0:00:14s
epoch 41 | loss: 0.39164 | val_0_rmse: 0.7245  | val_1_rmse: 0.73735 |  0:00:14s
epoch 42 | loss: 0.38422 | val_0_rmse: 0.71222 | val_1_rmse: 0.73198 |  0:00:14s
epoch 43 | loss: 0.37616 | val_0_rmse: 0.72624 | val_1_rmse: 0.73985 |  0:00:15s
epoch 44 | loss: 0.37543 | val_0_rmse: 0.72611 | val_1_rmse: 0.7391  |  0:00:15s
epoch 45 | loss: 0.38071 | val_0_rmse: 0.71839 | val_1_rmse: 0.73261 |  0:00:15s
epoch 46 | loss: 0.37517 | val_0_rmse: 0.72252 | val_1_rmse: 0.73457 |  0:00:16s
epoch 47 | loss: 0.38279 | val_0_rmse: 0.72259 | val_1_rmse: 0.73906 |  0:00:16s
epoch 48 | loss: 0.3734  | val_0_rmse: 0.70909 | val_1_rmse: 0.73332 |  0:00:16s
epoch 49 | loss: 0.37032 | val_0_rmse: 0.71901 | val_1_rmse: 0.73242 |  0:00:17s
epoch 50 | loss: 0.36949 | val_0_rmse: 0.71196 | val_1_rmse: 0.72574 |  0:00:17s
epoch 51 | loss: 0.37423 | val_0_rmse: 0.71496 | val_1_rmse: 0.73386 |  0:00:17s
epoch 52 | loss: 0.37356 | val_0_rmse: 0.71634 | val_1_rmse: 0.74088 |  0:00:18s
epoch 53 | loss: 0.36705 | val_0_rmse: 0.72147 | val_1_rmse: 0.7443  |  0:00:18s
epoch 54 | loss: 0.37388 | val_0_rmse: 0.71641 | val_1_rmse: 0.73947 |  0:00:18s
epoch 55 | loss: 0.36102 | val_0_rmse: 0.69692 | val_1_rmse: 0.72249 |  0:00:19s
epoch 56 | loss: 0.36876 | val_0_rmse: 0.6977  | val_1_rmse: 0.72295 |  0:00:19s
epoch 57 | loss: 0.36731 | val_0_rmse: 0.70047 | val_1_rmse: 0.72617 |  0:00:19s
epoch 58 | loss: 0.36262 | val_0_rmse: 0.70802 | val_1_rmse: 0.73189 |  0:00:20s
epoch 59 | loss: 0.35905 | val_0_rmse: 0.6998  | val_1_rmse: 0.72271 |  0:00:20s
epoch 60 | loss: 0.36026 | val_0_rmse: 0.69361 | val_1_rmse: 0.71493 |  0:00:20s
epoch 61 | loss: 0.35888 | val_0_rmse: 0.69892 | val_1_rmse: 0.72304 |  0:00:21s
epoch 62 | loss: 0.35732 | val_0_rmse: 0.7028  | val_1_rmse: 0.73152 |  0:00:21s
epoch 63 | loss: 0.35657 | val_0_rmse: 0.68389 | val_1_rmse: 0.71319 |  0:00:21s
epoch 64 | loss: 0.35692 | val_0_rmse: 0.68859 | val_1_rmse: 0.71954 |  0:00:22s
epoch 65 | loss: 0.36013 | val_0_rmse: 0.68416 | val_1_rmse: 0.71531 |  0:00:22s
epoch 66 | loss: 0.35082 | val_0_rmse: 0.68493 | val_1_rmse: 0.71873 |  0:00:22s
epoch 67 | loss: 0.35527 | val_0_rmse: 0.70368 | val_1_rmse: 0.72783 |  0:00:23s
epoch 68 | loss: 0.34964 | val_0_rmse: 0.68055 | val_1_rmse: 0.7135  |  0:00:23s
epoch 69 | loss: 0.3577  | val_0_rmse: 0.68695 | val_1_rmse: 0.71834 |  0:00:24s
epoch 70 | loss: 0.35625 | val_0_rmse: 0.67673 | val_1_rmse: 0.71826 |  0:00:24s
epoch 71 | loss: 0.35071 | val_0_rmse: 0.6841  | val_1_rmse: 0.72029 |  0:00:24s
epoch 72 | loss: 0.35475 | val_0_rmse: 0.67217 | val_1_rmse: 0.71067 |  0:00:25s
epoch 73 | loss: 0.35153 | val_0_rmse: 0.67321 | val_1_rmse: 0.71499 |  0:00:25s
epoch 74 | loss: 0.33898 | val_0_rmse: 0.66884 | val_1_rmse: 0.71344 |  0:00:25s
epoch 75 | loss: 0.34501 | val_0_rmse: 0.66606 | val_1_rmse: 0.71331 |  0:00:26s
epoch 76 | loss: 0.34219 | val_0_rmse: 0.65723 | val_1_rmse: 0.70637 |  0:00:26s
epoch 77 | loss: 0.3429  | val_0_rmse: 0.6476  | val_1_rmse: 0.69861 |  0:00:26s
epoch 78 | loss: 0.33514 | val_0_rmse: 0.64751 | val_1_rmse: 0.69699 |  0:00:27s
epoch 79 | loss: 0.34348 | val_0_rmse: 0.64518 | val_1_rmse: 0.69811 |  0:00:27s
epoch 80 | loss: 0.34533 | val_0_rmse: 0.64454 | val_1_rmse: 0.69165 |  0:00:27s
epoch 81 | loss: 0.34216 | val_0_rmse: 0.64533 | val_1_rmse: 0.68817 |  0:00:28s
epoch 82 | loss: 0.33776 | val_0_rmse: 0.64338 | val_1_rmse: 0.69868 |  0:00:28s
epoch 83 | loss: 0.33796 | val_0_rmse: 0.6442  | val_1_rmse: 0.69303 |  0:00:28s
epoch 84 | loss: 0.34352 | val_0_rmse: 0.63426 | val_1_rmse: 0.68563 |  0:00:29s
epoch 85 | loss: 0.33757 | val_0_rmse: 0.63765 | val_1_rmse: 0.68784 |  0:00:29s
epoch 86 | loss: 0.33399 | val_0_rmse: 0.64598 | val_1_rmse: 0.69089 |  0:00:29s
epoch 87 | loss: 0.33778 | val_0_rmse: 0.64245 | val_1_rmse: 0.69163 |  0:00:30s
epoch 88 | loss: 0.33488 | val_0_rmse: 0.6426  | val_1_rmse: 0.68899 |  0:00:30s
epoch 89 | loss: 0.34021 | val_0_rmse: 0.63604 | val_1_rmse: 0.68343 |  0:00:30s
epoch 90 | loss: 0.33302 | val_0_rmse: 0.63472 | val_1_rmse: 0.68769 |  0:00:31s
epoch 91 | loss: 0.3338  | val_0_rmse: 0.62103 | val_1_rmse: 0.67588 |  0:00:31s
epoch 92 | loss: 0.32743 | val_0_rmse: 0.62198 | val_1_rmse: 0.67887 |  0:00:31s
epoch 93 | loss: 0.33565 | val_0_rmse: 0.61296 | val_1_rmse: 0.68171 |  0:00:32s
epoch 94 | loss: 0.33479 | val_0_rmse: 0.61625 | val_1_rmse: 0.6779  |  0:00:32s
epoch 95 | loss: 0.33471 | val_0_rmse: 0.61082 | val_1_rmse: 0.67062 |  0:00:32s
epoch 96 | loss: 0.34249 | val_0_rmse: 0.62066 | val_1_rmse: 0.68316 |  0:00:33s
epoch 97 | loss: 0.34298 | val_0_rmse: 0.62242 | val_1_rmse: 0.67924 |  0:00:33s
epoch 98 | loss: 0.34248 | val_0_rmse: 0.61369 | val_1_rmse: 0.67633 |  0:00:33s
epoch 99 | loss: 0.34489 | val_0_rmse: 0.61039 | val_1_rmse: 0.66665 |  0:00:34s
epoch 100| loss: 0.32877 | val_0_rmse: 0.60015 | val_1_rmse: 0.66495 |  0:00:34s
epoch 101| loss: 0.33875 | val_0_rmse: 0.60405 | val_1_rmse: 0.67342 |  0:00:34s
epoch 102| loss: 0.33688 | val_0_rmse: 0.60536 | val_1_rmse: 0.67862 |  0:00:35s
epoch 103| loss: 0.332   | val_0_rmse: 0.61015 | val_1_rmse: 0.67735 |  0:00:35s
epoch 104| loss: 0.33336 | val_0_rmse: 0.58844 | val_1_rmse: 0.66537 |  0:00:36s
epoch 105| loss: 0.33051 | val_0_rmse: 0.59057 | val_1_rmse: 0.66511 |  0:00:36s
epoch 106| loss: 0.32874 | val_0_rmse: 0.59277 | val_1_rmse: 0.66413 |  0:00:36s
epoch 107| loss: 0.31895 | val_0_rmse: 0.58795 | val_1_rmse: 0.65666 |  0:00:37s
epoch 108| loss: 0.32234 | val_0_rmse: 0.59484 | val_1_rmse: 0.6599  |  0:00:37s
epoch 109| loss: 0.32438 | val_0_rmse: 0.58698 | val_1_rmse: 0.65246 |  0:00:37s
epoch 110| loss: 0.32032 | val_0_rmse: 0.58952 | val_1_rmse: 0.66938 |  0:00:38s
epoch 111| loss: 0.32224 | val_0_rmse: 0.59009 | val_1_rmse: 0.6683  |  0:00:38s
epoch 112| loss: 0.32678 | val_0_rmse: 0.57638 | val_1_rmse: 0.65742 |  0:00:38s
epoch 113| loss: 0.3201  | val_0_rmse: 0.58272 | val_1_rmse: 0.65663 |  0:00:39s
epoch 114| loss: 0.32892 | val_0_rmse: 0.58406 | val_1_rmse: 0.65549 |  0:00:39s
epoch 115| loss: 0.32762 | val_0_rmse: 0.58419 | val_1_rmse: 0.65881 |  0:00:39s
epoch 116| loss: 0.32259 | val_0_rmse: 0.58218 | val_1_rmse: 0.66364 |  0:00:40s
epoch 117| loss: 0.31843 | val_0_rmse: 0.57565 | val_1_rmse: 0.65267 |  0:00:40s
epoch 118| loss: 0.32036 | val_0_rmse: 0.56596 | val_1_rmse: 0.65571 |  0:00:40s
epoch 119| loss: 0.31391 | val_0_rmse: 0.5678  | val_1_rmse: 0.65827 |  0:00:41s
epoch 120| loss: 0.31476 | val_0_rmse: 0.56851 | val_1_rmse: 0.66148 |  0:00:41s
epoch 121| loss: 0.31788 | val_0_rmse: 0.57879 | val_1_rmse: 0.66394 |  0:00:41s
epoch 122| loss: 0.33093 | val_0_rmse: 0.58809 | val_1_rmse: 0.67508 |  0:00:42s
epoch 123| loss: 0.32758 | val_0_rmse: 0.57366 | val_1_rmse: 0.66865 |  0:00:42s
epoch 124| loss: 0.33555 | val_0_rmse: 0.60567 | val_1_rmse: 0.67717 |  0:00:42s
epoch 125| loss: 0.36003 | val_0_rmse: 0.64171 | val_1_rmse: 0.70093 |  0:00:43s
epoch 126| loss: 0.35018 | val_0_rmse: 0.60444 | val_1_rmse: 0.67295 |  0:00:43s
epoch 127| loss: 0.35008 | val_0_rmse: 0.59431 | val_1_rmse: 0.6578  |  0:00:43s
epoch 128| loss: 0.34303 | val_0_rmse: 0.58883 | val_1_rmse: 0.64423 |  0:00:44s
epoch 129| loss: 0.33552 | val_0_rmse: 0.58303 | val_1_rmse: 0.64399 |  0:00:44s
epoch 130| loss: 0.33361 | val_0_rmse: 0.57735 | val_1_rmse: 0.63933 |  0:00:44s
epoch 131| loss: 0.33103 | val_0_rmse: 0.56951 | val_1_rmse: 0.63921 |  0:00:45s
epoch 132| loss: 0.32717 | val_0_rmse: 0.57332 | val_1_rmse: 0.64998 |  0:00:45s
epoch 133| loss: 0.33832 | val_0_rmse: 0.56976 | val_1_rmse: 0.65793 |  0:00:45s
epoch 134| loss: 0.33987 | val_0_rmse: 0.56774 | val_1_rmse: 0.64853 |  0:00:46s
epoch 135| loss: 0.33116 | val_0_rmse: 0.56285 | val_1_rmse: 0.64877 |  0:00:46s
epoch 136| loss: 0.3392  | val_0_rmse: 0.56613 | val_1_rmse: 0.64564 |  0:00:46s
epoch 137| loss: 0.32813 | val_0_rmse: 0.56781 | val_1_rmse: 0.65549 |  0:00:47s
epoch 138| loss: 0.32716 | val_0_rmse: 0.56502 | val_1_rmse: 0.65751 |  0:00:47s
epoch 139| loss: 0.32201 | val_0_rmse: 0.56189 | val_1_rmse: 0.65953 |  0:00:47s
epoch 140| loss: 0.32961 | val_0_rmse: 0.56657 | val_1_rmse: 0.65165 |  0:00:48s
epoch 141| loss: 0.327   | val_0_rmse: 0.56583 | val_1_rmse: 0.65355 |  0:00:48s
epoch 142| loss: 0.32925 | val_0_rmse: 0.56708 | val_1_rmse: 0.66088 |  0:00:49s
epoch 143| loss: 0.3271  | val_0_rmse: 0.5619  | val_1_rmse: 0.65347 |  0:00:49s
epoch 144| loss: 0.32601 | val_0_rmse: 0.57266 | val_1_rmse: 0.66135 |  0:00:49s
epoch 145| loss: 0.33099 | val_0_rmse: 0.56749 | val_1_rmse: 0.6469  |  0:00:50s
epoch 146| loss: 0.33268 | val_0_rmse: 0.56311 | val_1_rmse: 0.65241 |  0:00:50s
epoch 147| loss: 0.32782 | val_0_rmse: 0.56152 | val_1_rmse: 0.65522 |  0:00:50s
epoch 148| loss: 0.32414 | val_0_rmse: 0.55504 | val_1_rmse: 0.65174 |  0:00:51s
epoch 149| loss: 0.32432 | val_0_rmse: 0.55509 | val_1_rmse: 0.65075 |  0:00:51s
Stop training because you reached max_epochs = 150 with best_epoch = 131 and best_val_1_rmse = 0.63921
Best weights from best epoch are automatically used!
ended training at: 08:04:07
Feature importance:
Mean squared error is of 2628983374.8822002
Mean absolute error:34094.37237128607
MAPE:0.2856502769963516
R2 score:0.6627959499025469
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:04:07
epoch 0  | loss: 2.99458 | val_0_rmse: 1.01115 | val_1_rmse: 1.00289 |  0:00:00s
epoch 1  | loss: 1.75121 | val_0_rmse: 1.00816 | val_1_rmse: 0.99558 |  0:00:00s
epoch 2  | loss: 1.33704 | val_0_rmse: 1.0022  | val_1_rmse: 0.98981 |  0:00:01s
epoch 3  | loss: 1.17026 | val_0_rmse: 0.99716 | val_1_rmse: 0.98465 |  0:00:01s
epoch 4  | loss: 1.06373 | val_0_rmse: 1.00521 | val_1_rmse: 0.9917  |  0:00:01s
epoch 5  | loss: 0.90801 | val_0_rmse: 1.0203  | val_1_rmse: 1.01444 |  0:00:02s
epoch 6  | loss: 0.80605 | val_0_rmse: 1.32066 | val_1_rmse: 1.35369 |  0:00:02s
epoch 7  | loss: 0.71001 | val_0_rmse: 1.25351 | val_1_rmse: 1.3253  |  0:00:02s
epoch 8  | loss: 0.6765  | val_0_rmse: 0.8925  | val_1_rmse: 0.91511 |  0:00:03s
epoch 9  | loss: 0.63431 | val_0_rmse: 0.88867 | val_1_rmse: 0.91653 |  0:00:03s
epoch 10 | loss: 0.60138 | val_0_rmse: 0.83388 | val_1_rmse: 0.85782 |  0:00:03s
epoch 11 | loss: 0.56199 | val_0_rmse: 0.84715 | val_1_rmse: 0.87135 |  0:00:04s
epoch 12 | loss: 0.53437 | val_0_rmse: 0.80862 | val_1_rmse: 0.84057 |  0:00:04s
epoch 13 | loss: 0.51495 | val_0_rmse: 0.8314  | val_1_rmse: 0.8666  |  0:00:04s
epoch 14 | loss: 0.49308 | val_0_rmse: 0.79745 | val_1_rmse: 0.82594 |  0:00:05s
epoch 15 | loss: 0.50798 | val_0_rmse: 0.76366 | val_1_rmse: 0.79211 |  0:00:05s
epoch 16 | loss: 0.50021 | val_0_rmse: 0.77328 | val_1_rmse: 0.80275 |  0:00:05s
epoch 17 | loss: 0.48616 | val_0_rmse: 0.78529 | val_1_rmse: 0.80777 |  0:00:06s
epoch 18 | loss: 0.48387 | val_0_rmse: 0.78447 | val_1_rmse: 0.80206 |  0:00:06s
epoch 19 | loss: 0.47011 | val_0_rmse: 0.77408 | val_1_rmse: 0.79916 |  0:00:06s
epoch 20 | loss: 0.46557 | val_0_rmse: 0.76629 | val_1_rmse: 0.78602 |  0:00:07s
epoch 21 | loss: 0.47155 | val_0_rmse: 0.7579  | val_1_rmse: 0.77648 |  0:00:07s
epoch 22 | loss: 0.45982 | val_0_rmse: 0.76126 | val_1_rmse: 0.77139 |  0:00:07s
epoch 23 | loss: 0.45498 | val_0_rmse: 0.76591 | val_1_rmse: 0.77255 |  0:00:08s
epoch 24 | loss: 0.4528  | val_0_rmse: 0.76295 | val_1_rmse: 0.76925 |  0:00:08s
epoch 25 | loss: 0.44273 | val_0_rmse: 0.77447 | val_1_rmse: 0.77823 |  0:00:09s
epoch 26 | loss: 0.45398 | val_0_rmse: 0.75886 | val_1_rmse: 0.76602 |  0:00:09s
epoch 27 | loss: 0.4456  | val_0_rmse: 0.75022 | val_1_rmse: 0.76128 |  0:00:09s
epoch 28 | loss: 0.45054 | val_0_rmse: 0.74598 | val_1_rmse: 0.7645  |  0:00:10s
epoch 29 | loss: 0.43814 | val_0_rmse: 0.75328 | val_1_rmse: 0.78121 |  0:00:10s
epoch 30 | loss: 0.44005 | val_0_rmse: 0.74935 | val_1_rmse: 0.76681 |  0:00:10s
epoch 31 | loss: 0.43349 | val_0_rmse: 0.75281 | val_1_rmse: 0.76991 |  0:00:11s
epoch 32 | loss: 0.43441 | val_0_rmse: 0.74168 | val_1_rmse: 0.76471 |  0:00:11s
epoch 33 | loss: 0.435   | val_0_rmse: 0.73178 | val_1_rmse: 0.75502 |  0:00:11s
epoch 34 | loss: 0.42316 | val_0_rmse: 0.72886 | val_1_rmse: 0.76172 |  0:00:12s
epoch 35 | loss: 0.42836 | val_0_rmse: 0.7364  | val_1_rmse: 0.77045 |  0:00:12s
epoch 36 | loss: 0.4188  | val_0_rmse: 0.74102 | val_1_rmse: 0.77473 |  0:00:12s
epoch 37 | loss: 0.41781 | val_0_rmse: 0.73485 | val_1_rmse: 0.7626  |  0:00:13s
epoch 38 | loss: 0.41842 | val_0_rmse: 0.7319  | val_1_rmse: 0.75596 |  0:00:13s
epoch 39 | loss: 0.41807 | val_0_rmse: 0.74407 | val_1_rmse: 0.76342 |  0:00:13s
epoch 40 | loss: 0.41083 | val_0_rmse: 0.72841 | val_1_rmse: 0.7539  |  0:00:14s
epoch 41 | loss: 0.41233 | val_0_rmse: 0.71565 | val_1_rmse: 0.7514  |  0:00:14s
epoch 42 | loss: 0.40701 | val_0_rmse: 0.71705 | val_1_rmse: 0.74093 |  0:00:14s
epoch 43 | loss: 0.4019  | val_0_rmse: 0.7309  | val_1_rmse: 0.74691 |  0:00:15s
epoch 44 | loss: 0.40667 | val_0_rmse: 0.72949 | val_1_rmse: 0.73757 |  0:00:15s
epoch 45 | loss: 0.40667 | val_0_rmse: 0.73602 | val_1_rmse: 0.74362 |  0:00:15s
epoch 46 | loss: 0.39857 | val_0_rmse: 0.74608 | val_1_rmse: 0.75513 |  0:00:16s
epoch 47 | loss: 0.39346 | val_0_rmse: 0.73281 | val_1_rmse: 0.74577 |  0:00:16s
epoch 48 | loss: 0.40563 | val_0_rmse: 0.73837 | val_1_rmse: 0.75073 |  0:00:16s
epoch 49 | loss: 0.39908 | val_0_rmse: 0.71147 | val_1_rmse: 0.72961 |  0:00:17s
epoch 50 | loss: 0.39872 | val_0_rmse: 0.70712 | val_1_rmse: 0.72889 |  0:00:17s
epoch 51 | loss: 0.39604 | val_0_rmse: 0.70542 | val_1_rmse: 0.72763 |  0:00:17s
epoch 52 | loss: 0.39328 | val_0_rmse: 0.71911 | val_1_rmse: 0.73169 |  0:00:18s
epoch 53 | loss: 0.38551 | val_0_rmse: 0.70621 | val_1_rmse: 0.7232  |  0:00:18s
epoch 54 | loss: 0.3829  | val_0_rmse: 0.71049 | val_1_rmse: 0.72497 |  0:00:19s
epoch 55 | loss: 0.38404 | val_0_rmse: 0.70043 | val_1_rmse: 0.72159 |  0:00:19s
epoch 56 | loss: 0.3777  | val_0_rmse: 0.71585 | val_1_rmse: 0.7307  |  0:00:19s
epoch 57 | loss: 0.39414 | val_0_rmse: 0.70288 | val_1_rmse: 0.72455 |  0:00:20s
epoch 58 | loss: 0.38115 | val_0_rmse: 0.70569 | val_1_rmse: 0.7482  |  0:00:20s
epoch 59 | loss: 0.38418 | val_0_rmse: 0.69501 | val_1_rmse: 0.72027 |  0:00:20s
epoch 60 | loss: 0.38053 | val_0_rmse: 0.70517 | val_1_rmse: 0.72377 |  0:00:21s
epoch 61 | loss: 0.37881 | val_0_rmse: 0.70511 | val_1_rmse: 0.71902 |  0:00:21s
epoch 62 | loss: 0.38185 | val_0_rmse: 0.7079  | val_1_rmse: 0.72256 |  0:00:21s
epoch 63 | loss: 0.37835 | val_0_rmse: 0.68947 | val_1_rmse: 0.71335 |  0:00:22s
epoch 64 | loss: 0.37381 | val_0_rmse: 0.69268 | val_1_rmse: 0.71919 |  0:00:22s
epoch 65 | loss: 0.37739 | val_0_rmse: 0.69237 | val_1_rmse: 0.71072 |  0:00:22s
epoch 66 | loss: 0.37693 | val_0_rmse: 0.68845 | val_1_rmse: 0.71149 |  0:00:23s
epoch 67 | loss: 0.37103 | val_0_rmse: 0.69106 | val_1_rmse: 0.70751 |  0:00:23s
epoch 68 | loss: 0.37451 | val_0_rmse: 0.68745 | val_1_rmse: 0.70713 |  0:00:23s
epoch 69 | loss: 0.36838 | val_0_rmse: 0.68904 | val_1_rmse: 0.70927 |  0:00:24s
epoch 70 | loss: 0.36473 | val_0_rmse: 0.68817 | val_1_rmse: 0.71365 |  0:00:24s
epoch 71 | loss: 0.35929 | val_0_rmse: 0.68657 | val_1_rmse: 0.70508 |  0:00:24s
epoch 72 | loss: 0.36173 | val_0_rmse: 0.70498 | val_1_rmse: 0.7251  |  0:00:25s
epoch 73 | loss: 0.39746 | val_0_rmse: 0.70546 | val_1_rmse: 0.72154 |  0:00:25s
epoch 74 | loss: 0.39169 | val_0_rmse: 0.69793 | val_1_rmse: 0.71114 |  0:00:25s
epoch 75 | loss: 0.38262 | val_0_rmse: 0.69492 | val_1_rmse: 0.71243 |  0:00:26s
epoch 76 | loss: 0.38381 | val_0_rmse: 0.68632 | val_1_rmse: 0.71374 |  0:00:26s
epoch 77 | loss: 0.38403 | val_0_rmse: 0.66771 | val_1_rmse: 0.7012  |  0:00:26s
epoch 78 | loss: 0.37004 | val_0_rmse: 0.66048 | val_1_rmse: 0.68781 |  0:00:27s
epoch 79 | loss: 0.36993 | val_0_rmse: 0.66182 | val_1_rmse: 0.68991 |  0:00:27s
epoch 80 | loss: 0.38031 | val_0_rmse: 0.66291 | val_1_rmse: 0.68901 |  0:00:27s
epoch 81 | loss: 0.37342 | val_0_rmse: 0.67691 | val_1_rmse: 0.69849 |  0:00:28s
epoch 82 | loss: 0.36551 | val_0_rmse: 0.6514  | val_1_rmse: 0.67461 |  0:00:28s
epoch 83 | loss: 0.37517 | val_0_rmse: 0.65492 | val_1_rmse: 0.68265 |  0:00:29s
epoch 84 | loss: 0.36334 | val_0_rmse: 0.64985 | val_1_rmse: 0.68108 |  0:00:29s
epoch 85 | loss: 0.3586  | val_0_rmse: 0.64904 | val_1_rmse: 0.68142 |  0:00:29s
epoch 86 | loss: 0.3593  | val_0_rmse: 0.63976 | val_1_rmse: 0.67916 |  0:00:30s
epoch 87 | loss: 0.3555  | val_0_rmse: 0.63923 | val_1_rmse: 0.67835 |  0:00:30s
epoch 88 | loss: 0.36193 | val_0_rmse: 0.64444 | val_1_rmse: 0.67422 |  0:00:30s
epoch 89 | loss: 0.36068 | val_0_rmse: 0.65244 | val_1_rmse: 0.68393 |  0:00:31s
epoch 90 | loss: 0.36503 | val_0_rmse: 0.6504  | val_1_rmse: 0.67713 |  0:00:31s
epoch 91 | loss: 0.35004 | val_0_rmse: 0.64217 | val_1_rmse: 0.67133 |  0:00:31s
epoch 92 | loss: 0.35174 | val_0_rmse: 0.63704 | val_1_rmse: 0.67169 |  0:00:32s
epoch 93 | loss: 0.35568 | val_0_rmse: 0.65185 | val_1_rmse: 0.68073 |  0:00:32s
epoch 94 | loss: 0.34952 | val_0_rmse: 0.64253 | val_1_rmse: 0.67699 |  0:00:32s
epoch 95 | loss: 0.34828 | val_0_rmse: 0.64336 | val_1_rmse: 0.6793  |  0:00:33s
epoch 96 | loss: 0.34934 | val_0_rmse: 0.62495 | val_1_rmse: 0.66811 |  0:00:33s
epoch 97 | loss: 0.3427  | val_0_rmse: 0.62682 | val_1_rmse: 0.66434 |  0:00:33s
epoch 98 | loss: 0.34123 | val_0_rmse: 0.62506 | val_1_rmse: 0.67003 |  0:00:34s
epoch 99 | loss: 0.3404  | val_0_rmse: 0.63449 | val_1_rmse: 0.66932 |  0:00:34s
epoch 100| loss: 0.34367 | val_0_rmse: 0.63431 | val_1_rmse: 0.67039 |  0:00:34s
epoch 101| loss: 0.33804 | val_0_rmse: 0.62311 | val_1_rmse: 0.66827 |  0:00:35s
epoch 102| loss: 0.33378 | val_0_rmse: 0.62508 | val_1_rmse: 0.6628  |  0:00:35s
epoch 103| loss: 0.3441  | val_0_rmse: 0.61654 | val_1_rmse: 0.66284 |  0:00:35s
epoch 104| loss: 0.33328 | val_0_rmse: 0.61209 | val_1_rmse: 0.66564 |  0:00:36s
epoch 105| loss: 0.34439 | val_0_rmse: 0.61435 | val_1_rmse: 0.66619 |  0:00:36s
epoch 106| loss: 0.33817 | val_0_rmse: 0.60461 | val_1_rmse: 0.6692  |  0:00:36s
epoch 107| loss: 0.34217 | val_0_rmse: 0.59866 | val_1_rmse: 0.67303 |  0:00:37s
epoch 108| loss: 0.33373 | val_0_rmse: 0.60679 | val_1_rmse: 0.66565 |  0:00:37s
epoch 109| loss: 0.33339 | val_0_rmse: 0.59904 | val_1_rmse: 0.66792 |  0:00:37s
epoch 110| loss: 0.32883 | val_0_rmse: 0.59393 | val_1_rmse: 0.6737  |  0:00:38s
epoch 111| loss: 0.33125 | val_0_rmse: 0.59568 | val_1_rmse: 0.66581 |  0:00:38s
epoch 112| loss: 0.33363 | val_0_rmse: 0.6052  | val_1_rmse: 0.66307 |  0:00:38s
epoch 113| loss: 0.33049 | val_0_rmse: 0.59858 | val_1_rmse: 0.66128 |  0:00:39s
epoch 114| loss: 0.33199 | val_0_rmse: 0.60183 | val_1_rmse: 0.67615 |  0:00:39s
epoch 115| loss: 0.33219 | val_0_rmse: 0.59679 | val_1_rmse: 0.65177 |  0:00:39s
epoch 116| loss: 0.32707 | val_0_rmse: 0.58891 | val_1_rmse: 0.64723 |  0:00:40s
epoch 117| loss: 0.3236  | val_0_rmse: 0.59113 | val_1_rmse: 0.66046 |  0:00:40s
epoch 118| loss: 0.32869 | val_0_rmse: 0.58189 | val_1_rmse: 0.66478 |  0:00:40s
epoch 119| loss: 0.32408 | val_0_rmse: 0.57691 | val_1_rmse: 0.67259 |  0:00:41s
epoch 120| loss: 0.31814 | val_0_rmse: 0.57953 | val_1_rmse: 0.67248 |  0:00:41s
epoch 121| loss: 0.32286 | val_0_rmse: 0.57698 | val_1_rmse: 0.67009 |  0:00:42s
epoch 122| loss: 0.32318 | val_0_rmse: 0.58332 | val_1_rmse: 0.68645 |  0:00:42s
epoch 123| loss: 0.3214  | val_0_rmse: 0.57131 | val_1_rmse: 0.65836 |  0:00:42s
epoch 124| loss: 0.32537 | val_0_rmse: 0.57401 | val_1_rmse: 0.64955 |  0:00:43s
epoch 125| loss: 0.3202  | val_0_rmse: 0.56677 | val_1_rmse: 0.65794 |  0:00:43s
epoch 126| loss: 0.31696 | val_0_rmse: 0.56354 | val_1_rmse: 0.66256 |  0:00:43s
epoch 127| loss: 0.32324 | val_0_rmse: 0.56825 | val_1_rmse: 0.6616  |  0:00:44s
epoch 128| loss: 0.31593 | val_0_rmse: 0.56705 | val_1_rmse: 0.66062 |  0:00:44s
epoch 129| loss: 0.32143 | val_0_rmse: 0.56441 | val_1_rmse: 0.65215 |  0:00:44s
epoch 130| loss: 0.31807 | val_0_rmse: 0.56121 | val_1_rmse: 0.66043 |  0:00:45s
epoch 131| loss: 0.3164  | val_0_rmse: 0.5693  | val_1_rmse: 0.72458 |  0:00:45s
epoch 132| loss: 0.31696 | val_0_rmse: 0.55684 | val_1_rmse: 0.71771 |  0:00:45s
epoch 133| loss: 0.31867 | val_0_rmse: 0.55972 | val_1_rmse: 0.69516 |  0:00:46s
epoch 134| loss: 0.31497 | val_0_rmse: 0.55447 | val_1_rmse: 0.64908 |  0:00:46s
epoch 135| loss: 0.30846 | val_0_rmse: 0.56451 | val_1_rmse: 0.64708 |  0:00:46s
epoch 136| loss: 0.30699 | val_0_rmse: 0.55494 | val_1_rmse: 0.67656 |  0:00:47s
epoch 137| loss: 0.30948 | val_0_rmse: 0.55233 | val_1_rmse: 0.71359 |  0:00:47s
epoch 138| loss: 0.30238 | val_0_rmse: 0.55682 | val_1_rmse: 0.65556 |  0:00:47s
epoch 139| loss: 0.31293 | val_0_rmse: 0.54945 | val_1_rmse: 0.64985 |  0:00:48s
epoch 140| loss: 0.30426 | val_0_rmse: 0.54705 | val_1_rmse: 0.641   |  0:00:48s
epoch 141| loss: 0.30519 | val_0_rmse: 0.55379 | val_1_rmse: 0.63888 |  0:00:48s
epoch 142| loss: 0.30593 | val_0_rmse: 0.55509 | val_1_rmse: 0.65043 |  0:00:49s
epoch 143| loss: 0.30507 | val_0_rmse: 0.55775 | val_1_rmse: 0.65893 |  0:00:49s
epoch 144| loss: 0.30446 | val_0_rmse: 0.54603 | val_1_rmse: 0.64353 |  0:00:49s
epoch 145| loss: 0.30832 | val_0_rmse: 0.5593  | val_1_rmse: 0.65367 |  0:00:50s
epoch 146| loss: 0.31131 | val_0_rmse: 0.53781 | val_1_rmse: 0.64521 |  0:00:50s
epoch 147| loss: 0.30692 | val_0_rmse: 0.53599 | val_1_rmse: 0.65191 |  0:00:50s
epoch 148| loss: 0.30417 | val_0_rmse: 0.53044 | val_1_rmse: 0.65888 |  0:00:51s
epoch 149| loss: 0.30093 | val_0_rmse: 0.54724 | val_1_rmse: 0.67001 |  0:00:51s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.63888
Best weights from best epoch are automatically used!
ended training at: 08:04:59
Feature importance:
Mean squared error is of 3214785938.7616353
Mean absolute error:36767.786325697685
MAPE:0.27393942363025037
R2 score:0.6293283957024457
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:04:59
epoch 0  | loss: 3.46326 | val_0_rmse: 1.02933 | val_1_rmse: 1.09188 |  0:00:00s
epoch 1  | loss: 1.47338 | val_0_rmse: 0.98034 | val_1_rmse: 1.06086 |  0:00:00s
epoch 2  | loss: 1.02364 | val_0_rmse: 0.9653  | val_1_rmse: 1.04603 |  0:00:01s
epoch 3  | loss: 0.94092 | val_0_rmse: 0.95942 | val_1_rmse: 1.03622 |  0:00:01s
epoch 4  | loss: 0.86285 | val_0_rmse: 0.94064 | val_1_rmse: 1.02013 |  0:00:01s
epoch 5  | loss: 0.81515 | val_0_rmse: 0.89414 | val_1_rmse: 0.97195 |  0:00:02s
epoch 6  | loss: 0.76858 | val_0_rmse: 0.86891 | val_1_rmse: 0.9479  |  0:00:02s
epoch 7  | loss: 0.69472 | val_0_rmse: 0.82839 | val_1_rmse: 0.91006 |  0:00:02s
epoch 8  | loss: 0.62821 | val_0_rmse: 0.8146  | val_1_rmse: 0.90143 |  0:00:03s
epoch 9  | loss: 0.56285 | val_0_rmse: 0.77042 | val_1_rmse: 0.8625  |  0:00:03s
epoch 10 | loss: 0.54295 | val_0_rmse: 0.79922 | val_1_rmse: 0.88861 |  0:00:03s
epoch 11 | loss: 0.53508 | val_0_rmse: 0.79286 | val_1_rmse: 0.87066 |  0:00:04s
epoch 12 | loss: 0.50277 | val_0_rmse: 0.76923 | val_1_rmse: 0.84849 |  0:00:04s
epoch 13 | loss: 0.4942  | val_0_rmse: 0.75785 | val_1_rmse: 0.83879 |  0:00:04s
epoch 14 | loss: 0.48299 | val_0_rmse: 0.78319 | val_1_rmse: 0.85126 |  0:00:05s
epoch 15 | loss: 0.47281 | val_0_rmse: 0.74834 | val_1_rmse: 0.82963 |  0:00:05s
epoch 16 | loss: 0.46133 | val_0_rmse: 0.79321 | val_1_rmse: 0.86615 |  0:00:05s
epoch 17 | loss: 0.44819 | val_0_rmse: 0.76664 | val_1_rmse: 0.85353 |  0:00:06s
epoch 18 | loss: 0.43615 | val_0_rmse: 0.77642 | val_1_rmse: 0.85308 |  0:00:06s
epoch 19 | loss: 0.41808 | val_0_rmse: 0.77338 | val_1_rmse: 0.85571 |  0:00:06s
epoch 20 | loss: 0.4188  | val_0_rmse: 0.76217 | val_1_rmse: 0.84414 |  0:00:07s
epoch 21 | loss: 0.41719 | val_0_rmse: 0.75608 | val_1_rmse: 0.83429 |  0:00:07s
epoch 22 | loss: 0.40515 | val_0_rmse: 0.75155 | val_1_rmse: 0.82931 |  0:00:07s
epoch 23 | loss: 0.40812 | val_0_rmse: 0.74355 | val_1_rmse: 0.81961 |  0:00:08s
epoch 24 | loss: 0.39834 | val_0_rmse: 0.74657 | val_1_rmse: 0.82458 |  0:00:08s
epoch 25 | loss: 0.40377 | val_0_rmse: 0.73372 | val_1_rmse: 0.81201 |  0:00:08s
epoch 26 | loss: 0.40523 | val_0_rmse: 0.73985 | val_1_rmse: 0.82525 |  0:00:09s
epoch 27 | loss: 0.39934 | val_0_rmse: 0.72757 | val_1_rmse: 0.81577 |  0:00:09s
epoch 28 | loss: 0.39619 | val_0_rmse: 0.72946 | val_1_rmse: 0.8111  |  0:00:10s
epoch 29 | loss: 0.39438 | val_0_rmse: 0.73162 | val_1_rmse: 0.82082 |  0:00:10s
epoch 30 | loss: 0.39699 | val_0_rmse: 0.72632 | val_1_rmse: 0.81    |  0:00:10s
epoch 31 | loss: 0.39228 | val_0_rmse: 0.74239 | val_1_rmse: 0.82416 |  0:00:11s
epoch 32 | loss: 0.38896 | val_0_rmse: 0.73274 | val_1_rmse: 0.82239 |  0:00:11s
epoch 33 | loss: 0.38548 | val_0_rmse: 0.73419 | val_1_rmse: 0.82038 |  0:00:11s
epoch 34 | loss: 0.38449 | val_0_rmse: 0.7211  | val_1_rmse: 0.81012 |  0:00:12s
epoch 35 | loss: 0.38059 | val_0_rmse: 0.73058 | val_1_rmse: 0.8215  |  0:00:12s
epoch 36 | loss: 0.37225 | val_0_rmse: 0.72901 | val_1_rmse: 0.82292 |  0:00:12s
epoch 37 | loss: 0.37927 | val_0_rmse: 0.71955 | val_1_rmse: 0.81301 |  0:00:13s
epoch 38 | loss: 0.38062 | val_0_rmse: 0.72107 | val_1_rmse: 0.81342 |  0:00:13s
epoch 39 | loss: 0.39262 | val_0_rmse: 0.70236 | val_1_rmse: 0.79291 |  0:00:13s
epoch 40 | loss: 0.37307 | val_0_rmse: 0.70975 | val_1_rmse: 0.79785 |  0:00:14s
epoch 41 | loss: 0.39177 | val_0_rmse: 0.71067 | val_1_rmse: 0.80957 |  0:00:14s
epoch 42 | loss: 0.38863 | val_0_rmse: 0.70746 | val_1_rmse: 0.80071 |  0:00:14s
epoch 43 | loss: 0.38297 | val_0_rmse: 0.70449 | val_1_rmse: 0.79994 |  0:00:15s
epoch 44 | loss: 0.37571 | val_0_rmse: 0.69205 | val_1_rmse: 0.79048 |  0:00:15s
epoch 45 | loss: 0.37353 | val_0_rmse: 0.69252 | val_1_rmse: 0.78875 |  0:00:15s
epoch 46 | loss: 0.37209 | val_0_rmse: 0.69267 | val_1_rmse: 0.79046 |  0:00:16s
epoch 47 | loss: 0.36792 | val_0_rmse: 0.69728 | val_1_rmse: 0.79787 |  0:00:16s
epoch 48 | loss: 0.36433 | val_0_rmse: 0.68877 | val_1_rmse: 0.78433 |  0:00:16s
epoch 49 | loss: 0.36652 | val_0_rmse: 0.68902 | val_1_rmse: 0.77945 |  0:00:17s
epoch 50 | loss: 0.37267 | val_0_rmse: 0.68987 | val_1_rmse: 0.78497 |  0:00:17s
epoch 51 | loss: 0.37399 | val_0_rmse: 0.68674 | val_1_rmse: 0.77869 |  0:00:17s
epoch 52 | loss: 0.36489 | val_0_rmse: 0.68006 | val_1_rmse: 0.7742  |  0:00:18s
epoch 53 | loss: 0.36298 | val_0_rmse: 0.68766 | val_1_rmse: 0.77495 |  0:00:18s
epoch 54 | loss: 0.36182 | val_0_rmse: 0.69003 | val_1_rmse: 0.78115 |  0:00:19s
epoch 55 | loss: 0.35714 | val_0_rmse: 0.68956 | val_1_rmse: 0.78128 |  0:00:19s
epoch 56 | loss: 0.356   | val_0_rmse: 0.68811 | val_1_rmse: 0.77641 |  0:00:19s
epoch 57 | loss: 0.36489 | val_0_rmse: 0.69589 | val_1_rmse: 0.78708 |  0:00:20s
epoch 58 | loss: 0.36367 | val_0_rmse: 0.68874 | val_1_rmse: 0.77974 |  0:00:20s
epoch 59 | loss: 0.37108 | val_0_rmse: 0.6736  | val_1_rmse: 0.76859 |  0:00:20s
epoch 60 | loss: 0.36556 | val_0_rmse: 0.67125 | val_1_rmse: 0.76341 |  0:00:21s
epoch 61 | loss: 0.3617  | val_0_rmse: 0.67599 | val_1_rmse: 0.76905 |  0:00:21s
epoch 62 | loss: 0.35718 | val_0_rmse: 0.68867 | val_1_rmse: 0.78816 |  0:00:21s
epoch 63 | loss: 0.35369 | val_0_rmse: 0.67838 | val_1_rmse: 0.78095 |  0:00:22s
epoch 64 | loss: 0.34882 | val_0_rmse: 0.67124 | val_1_rmse: 0.76978 |  0:00:22s
epoch 65 | loss: 0.35425 | val_0_rmse: 0.66588 | val_1_rmse: 0.75909 |  0:00:22s
epoch 66 | loss: 0.35227 | val_0_rmse: 0.6743  | val_1_rmse: 0.75439 |  0:00:23s
epoch 67 | loss: 0.35272 | val_0_rmse: 0.66821 | val_1_rmse: 0.75654 |  0:00:23s
epoch 68 | loss: 0.35455 | val_0_rmse: 0.66911 | val_1_rmse: 0.75734 |  0:00:23s
epoch 69 | loss: 0.35292 | val_0_rmse: 0.66782 | val_1_rmse: 0.75961 |  0:00:24s
epoch 70 | loss: 0.35978 | val_0_rmse: 0.66638 | val_1_rmse: 0.75951 |  0:00:24s
epoch 71 | loss: 0.34823 | val_0_rmse: 0.66206 | val_1_rmse: 0.75298 |  0:00:24s
epoch 72 | loss: 0.36007 | val_0_rmse: 0.66042 | val_1_rmse: 0.75448 |  0:00:25s
epoch 73 | loss: 0.3474  | val_0_rmse: 0.65454 | val_1_rmse: 0.74861 |  0:00:25s
epoch 74 | loss: 0.35756 | val_0_rmse: 0.66036 | val_1_rmse: 0.75254 |  0:00:25s
epoch 75 | loss: 0.3541  | val_0_rmse: 0.65675 | val_1_rmse: 0.74841 |  0:00:26s
epoch 76 | loss: 0.35765 | val_0_rmse: 0.65577 | val_1_rmse: 0.74847 |  0:00:26s
epoch 77 | loss: 0.36214 | val_0_rmse: 0.65315 | val_1_rmse: 0.74347 |  0:00:26s
epoch 78 | loss: 0.36028 | val_0_rmse: 0.64663 | val_1_rmse: 0.73994 |  0:00:27s
epoch 79 | loss: 0.35542 | val_0_rmse: 0.64047 | val_1_rmse: 0.74118 |  0:00:27s
epoch 80 | loss: 0.35995 | val_0_rmse: 0.64506 | val_1_rmse: 0.74474 |  0:00:27s
epoch 81 | loss: 0.35119 | val_0_rmse: 0.6371  | val_1_rmse: 0.73839 |  0:00:28s
epoch 82 | loss: 0.3572  | val_0_rmse: 0.64181 | val_1_rmse: 0.73766 |  0:00:28s
epoch 83 | loss: 0.34483 | val_0_rmse: 0.6437  | val_1_rmse: 0.73958 |  0:00:29s
epoch 84 | loss: 0.34814 | val_0_rmse: 0.63718 | val_1_rmse: 0.73046 |  0:00:29s
epoch 85 | loss: 0.34679 | val_0_rmse: 0.63117 | val_1_rmse: 0.72133 |  0:00:29s
epoch 86 | loss: 0.34856 | val_0_rmse: 0.61999 | val_1_rmse: 0.72037 |  0:00:30s
epoch 87 | loss: 0.34059 | val_0_rmse: 0.71976 | val_1_rmse: 0.80261 |  0:00:30s
epoch 88 | loss: 0.34322 | val_0_rmse: 0.72469 | val_1_rmse: 0.81092 |  0:00:30s
epoch 89 | loss: 0.34427 | val_0_rmse: 0.66707 | val_1_rmse: 0.75735 |  0:00:31s
epoch 90 | loss: 0.34304 | val_0_rmse: 0.6392  | val_1_rmse: 0.72281 |  0:00:31s
epoch 91 | loss: 0.3514  | val_0_rmse: 0.64752 | val_1_rmse: 0.72849 |  0:00:31s
epoch 92 | loss: 0.36348 | val_0_rmse: 0.65457 | val_1_rmse: 0.74184 |  0:00:32s
epoch 93 | loss: 0.37263 | val_0_rmse: 0.65518 | val_1_rmse: 0.73687 |  0:00:32s
epoch 94 | loss: 0.38252 | val_0_rmse: 0.64739 | val_1_rmse: 0.73091 |  0:00:32s
epoch 95 | loss: 0.37417 | val_0_rmse: 0.64266 | val_1_rmse: 0.7339  |  0:00:33s
epoch 96 | loss: 0.36444 | val_0_rmse: 0.64284 | val_1_rmse: 0.73173 |  0:00:33s
epoch 97 | loss: 0.37108 | val_0_rmse: 0.64057 | val_1_rmse: 0.73877 |  0:00:33s
epoch 98 | loss: 0.36652 | val_0_rmse: 0.6304  | val_1_rmse: 0.72158 |  0:00:34s
epoch 99 | loss: 0.38512 | val_0_rmse: 0.62259 | val_1_rmse: 0.73202 |  0:00:34s
epoch 100| loss: 0.37615 | val_0_rmse: 0.62152 | val_1_rmse: 0.73237 |  0:00:34s
epoch 101| loss: 0.37    | val_0_rmse: 0.61998 | val_1_rmse: 0.74508 |  0:00:35s
epoch 102| loss: 0.36376 | val_0_rmse: 0.61051 | val_1_rmse: 0.73876 |  0:00:35s
epoch 103| loss: 0.36812 | val_0_rmse: 0.61593 | val_1_rmse: 0.73571 |  0:00:35s
epoch 104| loss: 0.36735 | val_0_rmse: 0.61567 | val_1_rmse: 0.72515 |  0:00:36s
epoch 105| loss: 0.35996 | val_0_rmse: 0.60917 | val_1_rmse: 0.72203 |  0:00:36s
epoch 106| loss: 0.35625 | val_0_rmse: 0.61558 | val_1_rmse: 0.73235 |  0:00:36s
epoch 107| loss: 0.35334 | val_0_rmse: 0.60885 | val_1_rmse: 0.73668 |  0:00:37s
epoch 108| loss: 0.3436  | val_0_rmse: 0.60002 | val_1_rmse: 0.72571 |  0:00:37s
epoch 109| loss: 0.34559 | val_0_rmse: 0.60214 | val_1_rmse: 0.72601 |  0:00:37s
epoch 110| loss: 0.34782 | val_0_rmse: 0.59818 | val_1_rmse: 0.7305  |  0:00:38s
epoch 111| loss: 0.34755 | val_0_rmse: 0.60358 | val_1_rmse: 0.72224 |  0:00:38s
epoch 112| loss: 0.35355 | val_0_rmse: 0.60052 | val_1_rmse: 0.7212  |  0:00:38s
epoch 113| loss: 0.354   | val_0_rmse: 0.60209 | val_1_rmse: 0.72533 |  0:00:39s
epoch 114| loss: 0.34878 | val_0_rmse: 0.60323 | val_1_rmse: 0.7258  |  0:00:39s
epoch 115| loss: 0.35195 | val_0_rmse: 0.59956 | val_1_rmse: 0.71822 |  0:00:39s
epoch 116| loss: 0.35159 | val_0_rmse: 0.60373 | val_1_rmse: 0.72276 |  0:00:40s
epoch 117| loss: 0.35413 | val_0_rmse: 0.60376 | val_1_rmse: 0.72258 |  0:00:40s
epoch 118| loss: 0.35148 | val_0_rmse: 0.60139 | val_1_rmse: 0.71879 |  0:00:40s
epoch 119| loss: 0.35011 | val_0_rmse: 0.59845 | val_1_rmse: 0.71529 |  0:00:41s
epoch 120| loss: 0.34383 | val_0_rmse: 0.60219 | val_1_rmse: 0.72884 |  0:00:41s
epoch 121| loss: 0.34767 | val_0_rmse: 0.60015 | val_1_rmse: 0.71897 |  0:00:41s
epoch 122| loss: 0.34572 | val_0_rmse: 0.59747 | val_1_rmse: 0.73148 |  0:00:42s
epoch 123| loss: 0.34959 | val_0_rmse: 0.58306 | val_1_rmse: 0.7086  |  0:00:42s
epoch 124| loss: 0.34206 | val_0_rmse: 0.58282 | val_1_rmse: 0.71009 |  0:00:42s
epoch 125| loss: 0.33733 | val_0_rmse: 0.58726 | val_1_rmse: 0.71571 |  0:00:43s
epoch 126| loss: 0.33615 | val_0_rmse: 0.57556 | val_1_rmse: 0.70799 |  0:00:43s
epoch 127| loss: 0.34154 | val_0_rmse: 0.57868 | val_1_rmse: 0.70908 |  0:00:44s
epoch 128| loss: 0.34104 | val_0_rmse: 0.57636 | val_1_rmse: 0.71243 |  0:00:44s
epoch 129| loss: 0.34112 | val_0_rmse: 0.57799 | val_1_rmse: 0.71121 |  0:00:44s
epoch 130| loss: 0.33751 | val_0_rmse: 0.57787 | val_1_rmse: 0.71164 |  0:00:45s
epoch 131| loss: 0.33701 | val_0_rmse: 0.57626 | val_1_rmse: 0.70345 |  0:00:45s
epoch 132| loss: 0.34184 | val_0_rmse: 0.5886  | val_1_rmse: 0.70574 |  0:00:45s
epoch 133| loss: 0.33927 | val_0_rmse: 0.58484 | val_1_rmse: 0.69653 |  0:00:46s
epoch 134| loss: 0.34479 | val_0_rmse: 0.59456 | val_1_rmse: 0.72031 |  0:00:46s
epoch 135| loss: 0.3471  | val_0_rmse: 0.5905  | val_1_rmse: 0.72361 |  0:00:46s
epoch 136| loss: 0.34061 | val_0_rmse: 0.58084 | val_1_rmse: 0.71179 |  0:00:47s
epoch 137| loss: 0.33669 | val_0_rmse: 0.57746 | val_1_rmse: 0.71057 |  0:00:47s
epoch 138| loss: 0.33734 | val_0_rmse: 0.5752  | val_1_rmse: 0.7068  |  0:00:47s
epoch 139| loss: 0.34148 | val_0_rmse: 0.58276 | val_1_rmse: 0.70877 |  0:00:48s
epoch 140| loss: 0.3375  | val_0_rmse: 0.57836 | val_1_rmse: 0.70578 |  0:00:48s
epoch 141| loss: 0.3379  | val_0_rmse: 0.57237 | val_1_rmse: 0.70438 |  0:00:48s
epoch 142| loss: 0.33711 | val_0_rmse: 0.57185 | val_1_rmse: 0.70445 |  0:00:49s
epoch 143| loss: 0.33325 | val_0_rmse: 0.5812  | val_1_rmse: 0.70791 |  0:00:49s
epoch 144| loss: 0.33557 | val_0_rmse: 0.58008 | val_1_rmse: 0.69942 |  0:00:49s
epoch 145| loss: 0.33837 | val_0_rmse: 0.58257 | val_1_rmse: 0.69856 |  0:00:50s
epoch 146| loss: 0.35019 | val_0_rmse: 0.59993 | val_1_rmse: 0.71845 |  0:00:50s
epoch 147| loss: 0.36782 | val_0_rmse: 0.61373 | val_1_rmse: 0.72088 |  0:00:50s
epoch 148| loss: 0.37923 | val_0_rmse: 0.62099 | val_1_rmse: 0.72526 |  0:00:51s
epoch 149| loss: 0.36593 | val_0_rmse: 0.61912 | val_1_rmse: 0.72772 |  0:00:51s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.69653
Best weights from best epoch are automatically used!
ended training at: 08:05:51
Feature importance:
Mean squared error is of 3169291727.7438297
Mean absolute error:37291.72113563773
MAPE:0.30636392821947545
R2 score:0.6334662424762157
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:05:51
epoch 0  | loss: 3.47809 | val_0_rmse: 0.996   | val_1_rmse: 1.02146 |  0:00:00s
epoch 1  | loss: 1.50366 | val_0_rmse: 0.97398 | val_1_rmse: 1.00193 |  0:00:00s
epoch 2  | loss: 1.06948 | val_0_rmse: 0.97887 | val_1_rmse: 1.00653 |  0:00:01s
epoch 3  | loss: 0.96704 | val_0_rmse: 0.97759 | val_1_rmse: 1.00248 |  0:00:01s
epoch 4  | loss: 0.86637 | val_0_rmse: 0.92037 | val_1_rmse: 0.94017 |  0:00:01s
epoch 5  | loss: 0.79551 | val_0_rmse: 0.88931 | val_1_rmse: 0.91449 |  0:00:02s
epoch 6  | loss: 0.71341 | val_0_rmse: 0.90381 | val_1_rmse: 0.91748 |  0:00:02s
epoch 7  | loss: 0.66735 | val_0_rmse: 0.84196 | val_1_rmse: 0.85698 |  0:00:02s
epoch 8  | loss: 0.5986  | val_0_rmse: 0.82033 | val_1_rmse: 0.8315  |  0:00:03s
epoch 9  | loss: 0.57018 | val_0_rmse: 0.85694 | val_1_rmse: 0.84877 |  0:00:03s
epoch 10 | loss: 0.55586 | val_0_rmse: 0.87921 | val_1_rmse: 0.85898 |  0:00:03s
epoch 11 | loss: 0.54864 | val_0_rmse: 0.85261 | val_1_rmse: 0.84188 |  0:00:04s
epoch 12 | loss: 0.52428 | val_0_rmse: 0.86943 | val_1_rmse: 0.85549 |  0:00:04s
epoch 13 | loss: 0.51577 | val_0_rmse: 0.80753 | val_1_rmse: 0.79586 |  0:00:04s
epoch 14 | loss: 0.48911 | val_0_rmse: 0.80233 | val_1_rmse: 0.827   |  0:00:05s
epoch 15 | loss: 0.49306 | val_0_rmse: 0.77685 | val_1_rmse: 0.76619 |  0:00:05s
epoch 16 | loss: 0.47936 | val_0_rmse: 0.7752  | val_1_rmse: 0.77042 |  0:00:05s
epoch 17 | loss: 0.47145 | val_0_rmse: 0.76382 | val_1_rmse: 0.7619  |  0:00:06s
epoch 18 | loss: 0.46036 | val_0_rmse: 0.75295 | val_1_rmse: 0.75698 |  0:00:06s
epoch 19 | loss: 0.45905 | val_0_rmse: 0.75013 | val_1_rmse: 0.75568 |  0:00:06s
epoch 20 | loss: 0.44995 | val_0_rmse: 0.75958 | val_1_rmse: 0.785   |  0:00:07s
epoch 21 | loss: 0.4605  | val_0_rmse: 0.73912 | val_1_rmse: 0.76294 |  0:00:07s
epoch 22 | loss: 0.45756 | val_0_rmse: 0.73091 | val_1_rmse: 0.75209 |  0:00:07s
epoch 23 | loss: 0.44505 | val_0_rmse: 0.73705 | val_1_rmse: 0.75345 |  0:00:08s
epoch 24 | loss: 0.43605 | val_0_rmse: 0.72575 | val_1_rmse: 0.73925 |  0:00:08s
epoch 25 | loss: 0.43088 | val_0_rmse: 0.71765 | val_1_rmse: 0.72783 |  0:00:08s
epoch 26 | loss: 0.4325  | val_0_rmse: 0.70335 | val_1_rmse: 0.71788 |  0:00:09s
epoch 27 | loss: 0.42018 | val_0_rmse: 0.70082 | val_1_rmse: 0.72141 |  0:00:09s
epoch 28 | loss: 0.41414 | val_0_rmse: 0.70649 | val_1_rmse: 0.73167 |  0:00:09s
epoch 29 | loss: 0.40807 | val_0_rmse: 0.69273 | val_1_rmse: 0.71216 |  0:00:10s
epoch 30 | loss: 0.40881 | val_0_rmse: 0.70303 | val_1_rmse: 0.71738 |  0:00:10s
epoch 31 | loss: 0.40306 | val_0_rmse: 0.69765 | val_1_rmse: 0.71189 |  0:00:11s
epoch 32 | loss: 0.40392 | val_0_rmse: 0.68952 | val_1_rmse: 0.70611 |  0:00:11s
epoch 33 | loss: 0.399   | val_0_rmse: 0.70143 | val_1_rmse: 0.72645 |  0:00:11s
epoch 34 | loss: 0.39805 | val_0_rmse: 0.69246 | val_1_rmse: 0.72227 |  0:00:12s
epoch 35 | loss: 0.41062 | val_0_rmse: 0.69367 | val_1_rmse: 0.7139  |  0:00:12s
epoch 36 | loss: 0.39284 | val_0_rmse: 0.69987 | val_1_rmse: 0.72378 |  0:00:12s
epoch 37 | loss: 0.3864  | val_0_rmse: 0.69318 | val_1_rmse: 0.74368 |  0:00:13s
epoch 38 | loss: 0.39082 | val_0_rmse: 0.69365 | val_1_rmse: 0.71729 |  0:00:13s
epoch 39 | loss: 0.39133 | val_0_rmse: 0.69774 | val_1_rmse: 0.72318 |  0:00:13s
epoch 40 | loss: 0.39416 | val_0_rmse: 0.68686 | val_1_rmse: 0.71543 |  0:00:14s
epoch 41 | loss: 0.38601 | val_0_rmse: 0.6945  | val_1_rmse: 0.72569 |  0:00:14s
epoch 42 | loss: 0.39766 | val_0_rmse: 0.68733 | val_1_rmse: 0.70855 |  0:00:14s
epoch 43 | loss: 0.38682 | val_0_rmse: 0.67799 | val_1_rmse: 0.70085 |  0:00:15s
epoch 44 | loss: 0.40248 | val_0_rmse: 0.67412 | val_1_rmse: 0.70711 |  0:00:15s
epoch 45 | loss: 0.39595 | val_0_rmse: 0.66812 | val_1_rmse: 0.7084  |  0:00:15s
epoch 46 | loss: 0.39782 | val_0_rmse: 0.66251 | val_1_rmse: 0.6939  |  0:00:16s
epoch 47 | loss: 0.40135 | val_0_rmse: 0.67063 | val_1_rmse: 0.6929  |  0:00:16s
epoch 48 | loss: 0.39896 | val_0_rmse: 0.67331 | val_1_rmse: 0.69107 |  0:00:16s
epoch 49 | loss: 0.38561 | val_0_rmse: 0.671   | val_1_rmse: 0.69474 |  0:00:17s
epoch 50 | loss: 0.38141 | val_0_rmse: 0.66388 | val_1_rmse: 0.68477 |  0:00:17s
epoch 51 | loss: 0.37741 | val_0_rmse: 0.66473 | val_1_rmse: 0.68203 |  0:00:17s
epoch 52 | loss: 0.37728 | val_0_rmse: 0.6585  | val_1_rmse: 0.67519 |  0:00:18s
epoch 53 | loss: 0.37174 | val_0_rmse: 0.65725 | val_1_rmse: 0.68055 |  0:00:18s
epoch 54 | loss: 0.36563 | val_0_rmse: 0.65394 | val_1_rmse: 0.67927 |  0:00:18s
epoch 55 | loss: 0.3666  | val_0_rmse: 0.64881 | val_1_rmse: 0.67639 |  0:00:19s
epoch 56 | loss: 0.36205 | val_0_rmse: 0.65609 | val_1_rmse: 0.67945 |  0:00:19s
epoch 57 | loss: 0.36417 | val_0_rmse: 0.64811 | val_1_rmse: 0.66799 |  0:00:20s
epoch 58 | loss: 0.36625 | val_0_rmse: 0.65104 | val_1_rmse: 0.67704 |  0:00:20s
epoch 59 | loss: 0.3647  | val_0_rmse: 0.64808 | val_1_rmse: 0.67866 |  0:00:20s
epoch 60 | loss: 0.36555 | val_0_rmse: 0.64931 | val_1_rmse: 0.67629 |  0:00:21s
epoch 61 | loss: 0.36512 | val_0_rmse: 0.6438  | val_1_rmse: 0.67148 |  0:00:21s
epoch 62 | loss: 0.36288 | val_0_rmse: 0.63762 | val_1_rmse: 0.66901 |  0:00:21s
epoch 63 | loss: 0.357   | val_0_rmse: 0.64204 | val_1_rmse: 0.66939 |  0:00:22s
epoch 64 | loss: 0.3593  | val_0_rmse: 0.6433  | val_1_rmse: 0.67162 |  0:00:22s
epoch 65 | loss: 0.35326 | val_0_rmse: 0.64699 | val_1_rmse: 0.67156 |  0:00:22s
epoch 66 | loss: 0.35157 | val_0_rmse: 0.64177 | val_1_rmse: 0.66641 |  0:00:23s
epoch 67 | loss: 0.33935 | val_0_rmse: 0.63377 | val_1_rmse: 0.65801 |  0:00:23s
epoch 68 | loss: 0.34694 | val_0_rmse: 0.63822 | val_1_rmse: 0.66222 |  0:00:23s
epoch 69 | loss: 0.34681 | val_0_rmse: 0.63996 | val_1_rmse: 0.66339 |  0:00:24s
epoch 70 | loss: 0.35157 | val_0_rmse: 0.6361  | val_1_rmse: 0.65982 |  0:00:24s
epoch 71 | loss: 0.35472 | val_0_rmse: 0.64116 | val_1_rmse: 0.66652 |  0:00:24s
epoch 72 | loss: 0.35006 | val_0_rmse: 0.62519 | val_1_rmse: 0.65427 |  0:00:25s
epoch 73 | loss: 0.34797 | val_0_rmse: 0.6284  | val_1_rmse: 0.66601 |  0:00:25s
epoch 74 | loss: 0.34938 | val_0_rmse: 0.63776 | val_1_rmse: 0.67063 |  0:00:25s
epoch 75 | loss: 0.34288 | val_0_rmse: 0.63257 | val_1_rmse: 0.6638  |  0:00:26s
epoch 76 | loss: 0.34608 | val_0_rmse: 0.62968 | val_1_rmse: 0.66078 |  0:00:26s
epoch 77 | loss: 0.33356 | val_0_rmse: 0.62633 | val_1_rmse: 0.65878 |  0:00:26s
epoch 78 | loss: 0.34145 | val_0_rmse: 0.61738 | val_1_rmse: 0.65244 |  0:00:27s
epoch 79 | loss: 0.34021 | val_0_rmse: 0.61463 | val_1_rmse: 0.64881 |  0:00:27s
epoch 80 | loss: 0.33612 | val_0_rmse: 0.61682 | val_1_rmse: 0.65347 |  0:00:28s
epoch 81 | loss: 0.33673 | val_0_rmse: 0.61316 | val_1_rmse: 0.6561  |  0:00:28s
epoch 82 | loss: 0.33316 | val_0_rmse: 0.61373 | val_1_rmse: 0.66062 |  0:00:28s
epoch 83 | loss: 0.32873 | val_0_rmse: 0.61806 | val_1_rmse: 0.66437 |  0:00:29s
epoch 84 | loss: 0.33172 | val_0_rmse: 0.61625 | val_1_rmse: 0.66327 |  0:00:29s
epoch 85 | loss: 0.32645 | val_0_rmse: 0.61349 | val_1_rmse: 0.65849 |  0:00:29s
epoch 86 | loss: 0.33021 | val_0_rmse: 0.61788 | val_1_rmse: 0.65832 |  0:00:30s
epoch 87 | loss: 0.32326 | val_0_rmse: 0.61098 | val_1_rmse: 0.65271 |  0:00:30s
epoch 88 | loss: 0.323   | val_0_rmse: 0.61284 | val_1_rmse: 0.66413 |  0:00:30s
epoch 89 | loss: 0.32823 | val_0_rmse: 0.60707 | val_1_rmse: 0.65901 |  0:00:31s
epoch 90 | loss: 0.33417 | val_0_rmse: 0.60965 | val_1_rmse: 0.66376 |  0:00:31s
epoch 91 | loss: 0.32712 | val_0_rmse: 0.60163 | val_1_rmse: 0.66063 |  0:00:31s
epoch 92 | loss: 0.32779 | val_0_rmse: 0.60775 | val_1_rmse: 0.65539 |  0:00:32s
epoch 93 | loss: 0.33035 | val_0_rmse: 0.60631 | val_1_rmse: 0.6572  |  0:00:32s
epoch 94 | loss: 0.33011 | val_0_rmse: 0.59966 | val_1_rmse: 0.65361 |  0:00:32s
epoch 95 | loss: 0.32534 | val_0_rmse: 0.60188 | val_1_rmse: 0.64733 |  0:00:33s
epoch 96 | loss: 0.32095 | val_0_rmse: 0.5971  | val_1_rmse: 0.64673 |  0:00:33s
epoch 97 | loss: 0.32343 | val_0_rmse: 0.58958 | val_1_rmse: 0.64644 |  0:00:33s
epoch 98 | loss: 0.31845 | val_0_rmse: 0.5868  | val_1_rmse: 0.64523 |  0:00:34s
epoch 99 | loss: 0.32154 | val_0_rmse: 0.59542 | val_1_rmse: 0.64682 |  0:00:34s
epoch 100| loss: 0.31804 | val_0_rmse: 0.59495 | val_1_rmse: 0.65035 |  0:00:34s
epoch 101| loss: 0.32376 | val_0_rmse: 0.58869 | val_1_rmse: 0.65392 |  0:00:35s
epoch 102| loss: 0.32509 | val_0_rmse: 0.58257 | val_1_rmse: 0.64914 |  0:00:35s
epoch 103| loss: 0.32101 | val_0_rmse: 0.5828  | val_1_rmse: 0.64831 |  0:00:35s
epoch 104| loss: 0.32225 | val_0_rmse: 0.58311 | val_1_rmse: 0.6522  |  0:00:36s
epoch 105| loss: 0.32375 | val_0_rmse: 0.57641 | val_1_rmse: 0.65516 |  0:00:36s
epoch 106| loss: 0.31319 | val_0_rmse: 0.57482 | val_1_rmse: 0.65569 |  0:00:36s
epoch 107| loss: 0.32147 | val_0_rmse: 0.57314 | val_1_rmse: 0.65522 |  0:00:37s
epoch 108| loss: 0.3219  | val_0_rmse: 0.57861 | val_1_rmse: 0.64468 |  0:00:37s
epoch 109| loss: 0.31878 | val_0_rmse: 0.57431 | val_1_rmse: 0.64782 |  0:00:37s
epoch 110| loss: 0.31882 | val_0_rmse: 0.57736 | val_1_rmse: 0.63781 |  0:00:38s
epoch 111| loss: 0.323   | val_0_rmse: 0.57864 | val_1_rmse: 0.63911 |  0:00:38s
epoch 112| loss: 0.31939 | val_0_rmse: 0.56729 | val_1_rmse: 0.63647 |  0:00:38s
epoch 113| loss: 0.31791 | val_0_rmse: 0.56741 | val_1_rmse: 0.63525 |  0:00:39s
epoch 114| loss: 0.31421 | val_0_rmse: 0.5652  | val_1_rmse: 0.63311 |  0:00:39s
epoch 115| loss: 0.31227 | val_0_rmse: 0.55924 | val_1_rmse: 0.63684 |  0:00:39s
epoch 116| loss: 0.3169  | val_0_rmse: 0.56276 | val_1_rmse: 0.63018 |  0:00:40s
epoch 117| loss: 0.30653 | val_0_rmse: 0.55969 | val_1_rmse: 0.63034 |  0:00:40s
epoch 118| loss: 0.31984 | val_0_rmse: 0.57021 | val_1_rmse: 0.6422  |  0:00:41s
epoch 119| loss: 0.31709 | val_0_rmse: 0.56644 | val_1_rmse: 0.63435 |  0:00:41s
epoch 120| loss: 0.31527 | val_0_rmse: 0.5589  | val_1_rmse: 0.63251 |  0:00:41s
epoch 121| loss: 0.29853 | val_0_rmse: 0.56196 | val_1_rmse: 0.6423  |  0:00:42s
epoch 122| loss: 0.30658 | val_0_rmse: 0.56279 | val_1_rmse: 0.64163 |  0:00:42s
epoch 123| loss: 0.30291 | val_0_rmse: 0.5566  | val_1_rmse: 0.63807 |  0:00:42s
epoch 124| loss: 0.30779 | val_0_rmse: 0.56909 | val_1_rmse: 0.64306 |  0:00:43s
epoch 125| loss: 0.30912 | val_0_rmse: 0.56125 | val_1_rmse: 0.63181 |  0:00:43s
epoch 126| loss: 0.31063 | val_0_rmse: 0.55249 | val_1_rmse: 0.6369  |  0:00:43s
epoch 127| loss: 0.29917 | val_0_rmse: 0.54761 | val_1_rmse: 0.63996 |  0:00:44s
epoch 128| loss: 0.3075  | val_0_rmse: 0.55071 | val_1_rmse: 0.6284  |  0:00:44s
epoch 129| loss: 0.30781 | val_0_rmse: 0.5545  | val_1_rmse: 0.62886 |  0:00:44s
epoch 130| loss: 0.30035 | val_0_rmse: 0.54356 | val_1_rmse: 0.64124 |  0:00:45s
epoch 131| loss: 0.30499 | val_0_rmse: 0.53902 | val_1_rmse: 0.63831 |  0:00:45s
epoch 132| loss: 0.30001 | val_0_rmse: 0.54251 | val_1_rmse: 0.64168 |  0:00:45s
epoch 133| loss: 0.30432 | val_0_rmse: 0.53824 | val_1_rmse: 0.64385 |  0:00:46s
epoch 134| loss: 0.29593 | val_0_rmse: 0.53589 | val_1_rmse: 0.64737 |  0:00:46s
epoch 135| loss: 0.30337 | val_0_rmse: 0.54817 | val_1_rmse: 0.63882 |  0:00:46s
epoch 136| loss: 0.30487 | val_0_rmse: 0.54034 | val_1_rmse: 0.63924 |  0:00:47s
epoch 137| loss: 0.30976 | val_0_rmse: 0.53535 | val_1_rmse: 0.63217 |  0:00:47s
epoch 138| loss: 0.30776 | val_0_rmse: 0.53711 | val_1_rmse: 0.62924 |  0:00:47s
epoch 139| loss: 0.30383 | val_0_rmse: 0.53684 | val_1_rmse: 0.6348  |  0:00:48s
epoch 140| loss: 0.29665 | val_0_rmse: 0.53951 | val_1_rmse: 0.63783 |  0:00:48s
epoch 141| loss: 0.29683 | val_0_rmse: 0.53825 | val_1_rmse: 0.63987 |  0:00:48s
epoch 142| loss: 0.30577 | val_0_rmse: 0.54166 | val_1_rmse: 0.65448 |  0:00:49s
epoch 143| loss: 0.30272 | val_0_rmse: 0.52894 | val_1_rmse: 0.63764 |  0:00:49s
epoch 144| loss: 0.29325 | val_0_rmse: 0.53483 | val_1_rmse: 0.64245 |  0:00:49s
epoch 145| loss: 0.2969  | val_0_rmse: 0.53375 | val_1_rmse: 0.64642 |  0:00:50s
epoch 146| loss: 0.28518 | val_0_rmse: 0.52339 | val_1_rmse: 0.63895 |  0:00:50s
epoch 147| loss: 0.29674 | val_0_rmse: 0.52733 | val_1_rmse: 0.63003 |  0:00:50s
epoch 148| loss: 0.29021 | val_0_rmse: 0.51794 | val_1_rmse: 0.61701 |  0:00:51s
epoch 149| loss: 0.29674 | val_0_rmse: 0.52679 | val_1_rmse: 0.62687 |  0:00:51s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.61701
Best weights from best epoch are automatically used!
ended training at: 08:06:43
Feature importance:
Mean squared error is of 2873272605.630716
Mean absolute error:34838.703259762304
MAPE:0.31166716702800945
R2 score:0.6449244579940604
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:06:43
epoch 0  | loss: 3.36092 | val_0_rmse: 0.99574 | val_1_rmse: 1.00619 |  0:00:00s
epoch 1  | loss: 1.50416 | val_0_rmse: 0.9958  | val_1_rmse: 1.00802 |  0:00:00s
epoch 2  | loss: 1.09807 | val_0_rmse: 0.99859 | val_1_rmse: 1.01071 |  0:00:01s
epoch 3  | loss: 1.03341 | val_0_rmse: 0.98678 | val_1_rmse: 0.99678 |  0:00:01s
epoch 4  | loss: 1.00099 | val_0_rmse: 0.98716 | val_1_rmse: 0.99663 |  0:00:01s
epoch 5  | loss: 0.92901 | val_0_rmse: 0.98904 | val_1_rmse: 0.99749 |  0:00:02s
epoch 6  | loss: 0.82839 | val_0_rmse: 0.89536 | val_1_rmse: 0.89015 |  0:00:02s
epoch 7  | loss: 0.72446 | val_0_rmse: 0.8679  | val_1_rmse: 0.86836 |  0:00:02s
epoch 8  | loss: 0.64723 | val_0_rmse: 0.84311 | val_1_rmse: 0.83717 |  0:00:03s
epoch 9  | loss: 0.62692 | val_0_rmse: 0.84206 | val_1_rmse: 0.83637 |  0:00:03s
epoch 10 | loss: 0.59158 | val_0_rmse: 0.85201 | val_1_rmse: 0.86069 |  0:00:03s
epoch 11 | loss: 0.56976 | val_0_rmse: 0.86165 | val_1_rmse: 0.87876 |  0:00:04s
epoch 12 | loss: 0.54277 | val_0_rmse: 0.84994 | val_1_rmse: 0.86485 |  0:00:04s
epoch 13 | loss: 0.52313 | val_0_rmse: 0.891   | val_1_rmse: 0.90549 |  0:00:04s
epoch 14 | loss: 0.51678 | val_0_rmse: 0.85598 | val_1_rmse: 0.86982 |  0:00:05s
epoch 15 | loss: 0.48621 | val_0_rmse: 0.87541 | val_1_rmse: 0.88699 |  0:00:05s
epoch 16 | loss: 0.48456 | val_0_rmse: 0.81762 | val_1_rmse: 0.82337 |  0:00:05s
epoch 17 | loss: 0.48376 | val_0_rmse: 0.83749 | val_1_rmse: 0.84565 |  0:00:06s
epoch 18 | loss: 0.47104 | val_0_rmse: 0.78996 | val_1_rmse: 0.7963  |  0:00:06s
epoch 19 | loss: 0.46136 | val_0_rmse: 0.82729 | val_1_rmse: 0.8365  |  0:00:06s
epoch 20 | loss: 0.44655 | val_0_rmse: 0.822   | val_1_rmse: 0.8299  |  0:00:07s
epoch 21 | loss: 0.44421 | val_0_rmse: 0.84883 | val_1_rmse: 0.86191 |  0:00:07s
epoch 22 | loss: 0.43424 | val_0_rmse: 0.79653 | val_1_rmse: 0.80084 |  0:00:07s
epoch 23 | loss: 0.44195 | val_0_rmse: 0.80475 | val_1_rmse: 0.81522 |  0:00:08s
epoch 24 | loss: 0.4386  | val_0_rmse: 0.81742 | val_1_rmse: 0.83416 |  0:00:08s
epoch 25 | loss: 0.43054 | val_0_rmse: 0.78822 | val_1_rmse: 0.80111 |  0:00:08s
epoch 26 | loss: 0.43002 | val_0_rmse: 0.76645 | val_1_rmse: 0.77157 |  0:00:09s
epoch 27 | loss: 0.41948 | val_0_rmse: 0.75249 | val_1_rmse: 0.75418 |  0:00:09s
epoch 28 | loss: 0.4131  | val_0_rmse: 0.7613  | val_1_rmse: 0.76221 |  0:00:09s
epoch 29 | loss: 0.41335 | val_0_rmse: 0.7713  | val_1_rmse: 0.77474 |  0:00:10s
epoch 30 | loss: 0.40912 | val_0_rmse: 0.75158 | val_1_rmse: 0.75215 |  0:00:10s
epoch 31 | loss: 0.40692 | val_0_rmse: 0.75058 | val_1_rmse: 0.75227 |  0:00:10s
epoch 32 | loss: 0.39865 | val_0_rmse: 0.77703 | val_1_rmse: 0.7848  |  0:00:11s
epoch 33 | loss: 0.40254 | val_0_rmse: 0.74682 | val_1_rmse: 0.74969 |  0:00:11s
epoch 34 | loss: 0.3906  | val_0_rmse: 0.75003 | val_1_rmse: 0.75508 |  0:00:12s
epoch 35 | loss: 0.39279 | val_0_rmse: 0.74144 | val_1_rmse: 0.74532 |  0:00:12s
epoch 36 | loss: 0.39141 | val_0_rmse: 0.74405 | val_1_rmse: 0.75219 |  0:00:12s
epoch 37 | loss: 0.39065 | val_0_rmse: 0.7427  | val_1_rmse: 0.75065 |  0:00:13s
epoch 38 | loss: 0.37693 | val_0_rmse: 0.74021 | val_1_rmse: 0.75121 |  0:00:13s
epoch 39 | loss: 0.38224 | val_0_rmse: 0.74486 | val_1_rmse: 0.75419 |  0:00:13s
epoch 40 | loss: 0.38751 | val_0_rmse: 0.73704 | val_1_rmse: 0.74759 |  0:00:14s
epoch 41 | loss: 0.38681 | val_0_rmse: 0.73144 | val_1_rmse: 0.74189 |  0:00:14s
epoch 42 | loss: 0.38167 | val_0_rmse: 0.73547 | val_1_rmse: 0.74161 |  0:00:14s
epoch 43 | loss: 0.38518 | val_0_rmse: 0.72031 | val_1_rmse: 0.7257  |  0:00:15s
epoch 44 | loss: 0.40074 | val_0_rmse: 0.73012 | val_1_rmse: 0.73308 |  0:00:15s
epoch 45 | loss: 0.43944 | val_0_rmse: 0.77037 | val_1_rmse: 0.7769  |  0:00:15s
epoch 46 | loss: 0.42896 | val_0_rmse: 0.74856 | val_1_rmse: 0.74984 |  0:00:16s
epoch 47 | loss: 0.41913 | val_0_rmse: 0.72758 | val_1_rmse: 0.73842 |  0:00:16s
epoch 48 | loss: 0.40246 | val_0_rmse: 0.71763 | val_1_rmse: 0.72764 |  0:00:16s
epoch 49 | loss: 0.39342 | val_0_rmse: 0.73791 | val_1_rmse: 0.74432 |  0:00:17s
epoch 50 | loss: 0.39783 | val_0_rmse: 0.74185 | val_1_rmse: 0.74657 |  0:00:17s
epoch 51 | loss: 0.39896 | val_0_rmse: 0.74861 | val_1_rmse: 0.75509 |  0:00:17s
epoch 52 | loss: 0.39355 | val_0_rmse: 0.73503 | val_1_rmse: 0.74244 |  0:00:18s
epoch 53 | loss: 0.39205 | val_0_rmse: 0.73236 | val_1_rmse: 0.73941 |  0:00:18s
epoch 54 | loss: 0.39104 | val_0_rmse: 0.72565 | val_1_rmse: 0.73381 |  0:00:18s
epoch 55 | loss: 0.38571 | val_0_rmse: 0.72384 | val_1_rmse: 0.73292 |  0:00:19s
epoch 56 | loss: 0.38643 | val_0_rmse: 0.70652 | val_1_rmse: 0.71716 |  0:00:19s
epoch 57 | loss: 0.37989 | val_0_rmse: 0.70491 | val_1_rmse: 0.71708 |  0:00:19s
epoch 58 | loss: 0.36957 | val_0_rmse: 0.71011 | val_1_rmse: 0.72646 |  0:00:20s
epoch 59 | loss: 0.37386 | val_0_rmse: 0.6961  | val_1_rmse: 0.70866 |  0:00:20s
epoch 60 | loss: 0.37561 | val_0_rmse: 0.70028 | val_1_rmse: 0.71366 |  0:00:20s
epoch 61 | loss: 0.37009 | val_0_rmse: 0.69674 | val_1_rmse: 0.70806 |  0:00:21s
epoch 62 | loss: 0.37653 | val_0_rmse: 0.6978  | val_1_rmse: 0.70968 |  0:00:21s
epoch 63 | loss: 0.36613 | val_0_rmse: 0.69749 | val_1_rmse: 0.71339 |  0:00:22s
epoch 64 | loss: 0.36719 | val_0_rmse: 0.69783 | val_1_rmse: 0.71328 |  0:00:22s
epoch 65 | loss: 0.36462 | val_0_rmse: 0.70772 | val_1_rmse: 0.71893 |  0:00:22s
epoch 66 | loss: 0.36467 | val_0_rmse: 0.70676 | val_1_rmse: 0.71791 |  0:00:23s
epoch 67 | loss: 0.36179 | val_0_rmse: 0.69639 | val_1_rmse: 0.70673 |  0:00:23s
epoch 68 | loss: 0.35432 | val_0_rmse: 0.68378 | val_1_rmse: 0.69769 |  0:00:23s
epoch 69 | loss: 0.362   | val_0_rmse: 0.67721 | val_1_rmse: 0.69518 |  0:00:24s
epoch 70 | loss: 0.36037 | val_0_rmse: 0.67627 | val_1_rmse: 0.69198 |  0:00:24s
epoch 71 | loss: 0.35471 | val_0_rmse: 0.67636 | val_1_rmse: 0.69165 |  0:00:24s
epoch 72 | loss: 0.3608  | val_0_rmse: 0.66294 | val_1_rmse: 0.68183 |  0:00:25s
epoch 73 | loss: 0.3515  | val_0_rmse: 0.66747 | val_1_rmse: 0.68463 |  0:00:25s
epoch 74 | loss: 0.35895 | val_0_rmse: 0.67466 | val_1_rmse: 0.69065 |  0:00:25s
epoch 75 | loss: 0.35386 | val_0_rmse: 0.6751  | val_1_rmse: 0.69181 |  0:00:26s
epoch 76 | loss: 0.35745 | val_0_rmse: 0.66569 | val_1_rmse: 0.67972 |  0:00:26s
epoch 77 | loss: 0.3687  | val_0_rmse: 0.66298 | val_1_rmse: 0.68321 |  0:00:26s
epoch 78 | loss: 0.36014 | val_0_rmse: 0.65308 | val_1_rmse: 0.67181 |  0:00:27s
epoch 79 | loss: 0.3549  | val_0_rmse: 0.66219 | val_1_rmse: 0.67767 |  0:00:27s
epoch 80 | loss: 0.35318 | val_0_rmse: 0.64936 | val_1_rmse: 0.67489 |  0:00:27s
epoch 81 | loss: 0.3506  | val_0_rmse: 0.64603 | val_1_rmse: 0.67153 |  0:00:28s
epoch 82 | loss: 0.34331 | val_0_rmse: 0.63798 | val_1_rmse: 0.66225 |  0:00:28s
epoch 83 | loss: 0.34876 | val_0_rmse: 0.64071 | val_1_rmse: 0.65994 |  0:00:28s
epoch 84 | loss: 0.34686 | val_0_rmse: 0.64133 | val_1_rmse: 0.66001 |  0:00:29s
epoch 85 | loss: 0.34872 | val_0_rmse: 0.63898 | val_1_rmse: 0.65595 |  0:00:29s
epoch 86 | loss: 0.34172 | val_0_rmse: 0.63679 | val_1_rmse: 0.65947 |  0:00:29s
epoch 87 | loss: 0.34088 | val_0_rmse: 0.63357 | val_1_rmse: 0.65686 |  0:00:30s
epoch 88 | loss: 0.34111 | val_0_rmse: 0.62982 | val_1_rmse: 0.65191 |  0:00:30s
epoch 89 | loss: 0.34511 | val_0_rmse: 0.62443 | val_1_rmse: 0.6496  |  0:00:30s
epoch 90 | loss: 0.34566 | val_0_rmse: 0.62814 | val_1_rmse: 0.65009 |  0:00:31s
epoch 91 | loss: 0.33549 | val_0_rmse: 0.62398 | val_1_rmse: 0.64649 |  0:00:31s
epoch 92 | loss: 0.3409  | val_0_rmse: 0.63188 | val_1_rmse: 0.65549 |  0:00:31s
epoch 93 | loss: 0.33627 | val_0_rmse: 0.62346 | val_1_rmse: 0.64955 |  0:00:32s
epoch 94 | loss: 0.33843 | val_0_rmse: 0.62461 | val_1_rmse: 0.65086 |  0:00:32s
epoch 95 | loss: 0.33288 | val_0_rmse: 0.62089 | val_1_rmse: 0.64944 |  0:00:33s
epoch 96 | loss: 0.33941 | val_0_rmse: 0.62089 | val_1_rmse: 0.64576 |  0:00:33s
epoch 97 | loss: 0.33662 | val_0_rmse: 0.61212 | val_1_rmse: 0.6381  |  0:00:33s
epoch 98 | loss: 0.33657 | val_0_rmse: 0.6096  | val_1_rmse: 0.63772 |  0:00:34s
epoch 99 | loss: 0.33637 | val_0_rmse: 0.62224 | val_1_rmse: 0.64572 |  0:00:34s
epoch 100| loss: 0.33686 | val_0_rmse: 0.61317 | val_1_rmse: 0.64263 |  0:00:34s
epoch 101| loss: 0.33276 | val_0_rmse: 0.61878 | val_1_rmse: 0.64176 |  0:00:35s
epoch 102| loss: 0.32727 | val_0_rmse: 0.62582 | val_1_rmse: 0.65552 |  0:00:35s
epoch 103| loss: 0.33145 | val_0_rmse: 0.60579 | val_1_rmse: 0.63691 |  0:00:35s
epoch 104| loss: 0.33535 | val_0_rmse: 0.59564 | val_1_rmse: 0.63183 |  0:00:36s
epoch 105| loss: 0.33282 | val_0_rmse: 0.59714 | val_1_rmse: 0.6331  |  0:00:36s
epoch 106| loss: 0.32364 | val_0_rmse: 0.59737 | val_1_rmse: 0.63301 |  0:00:36s
epoch 107| loss: 0.32041 | val_0_rmse: 0.5937  | val_1_rmse: 0.64183 |  0:00:37s
epoch 108| loss: 0.32571 | val_0_rmse: 0.58726 | val_1_rmse: 0.63234 |  0:00:37s
epoch 109| loss: 0.32113 | val_0_rmse: 0.58655 | val_1_rmse: 0.63144 |  0:00:37s
epoch 110| loss: 0.31937 | val_0_rmse: 0.57977 | val_1_rmse: 0.62504 |  0:00:38s
epoch 111| loss: 0.31767 | val_0_rmse: 0.5735  | val_1_rmse: 0.62505 |  0:00:38s
epoch 112| loss: 0.31584 | val_0_rmse: 0.57609 | val_1_rmse: 0.624   |  0:00:38s
epoch 113| loss: 0.31185 | val_0_rmse: 0.57157 | val_1_rmse: 0.62168 |  0:00:39s
epoch 114| loss: 0.31134 | val_0_rmse: 0.56863 | val_1_rmse: 0.62061 |  0:00:39s
epoch 115| loss: 0.31359 | val_0_rmse: 0.57418 | val_1_rmse: 0.62199 |  0:00:39s
epoch 116| loss: 0.31711 | val_0_rmse: 0.57134 | val_1_rmse: 0.6238  |  0:00:40s
epoch 117| loss: 0.31284 | val_0_rmse: 0.57201 | val_1_rmse: 0.62786 |  0:00:40s
epoch 118| loss: 0.31546 | val_0_rmse: 0.56827 | val_1_rmse: 0.6258  |  0:00:41s
epoch 119| loss: 0.30939 | val_0_rmse: 0.56562 | val_1_rmse: 0.62914 |  0:00:41s
epoch 120| loss: 0.30858 | val_0_rmse: 0.56015 | val_1_rmse: 0.63744 |  0:00:41s
epoch 121| loss: 0.29877 | val_0_rmse: 0.56006 | val_1_rmse: 0.63398 |  0:00:42s
epoch 122| loss: 0.30796 | val_0_rmse: 0.56484 | val_1_rmse: 0.62752 |  0:00:42s
epoch 123| loss: 0.30534 | val_0_rmse: 0.55776 | val_1_rmse: 0.6315  |  0:00:42s
epoch 124| loss: 0.29899 | val_0_rmse: 0.55889 | val_1_rmse: 0.62686 |  0:00:43s
epoch 125| loss: 0.30123 | val_0_rmse: 0.54927 | val_1_rmse: 0.62293 |  0:00:43s
epoch 126| loss: 0.31993 | val_0_rmse: 0.54976 | val_1_rmse: 0.62235 |  0:00:43s
epoch 127| loss: 0.30743 | val_0_rmse: 0.54381 | val_1_rmse: 0.62177 |  0:00:44s
epoch 128| loss: 0.30143 | val_0_rmse: 0.55566 | val_1_rmse: 0.62514 |  0:00:44s
epoch 129| loss: 0.30364 | val_0_rmse: 0.55377 | val_1_rmse: 0.61647 |  0:00:44s
epoch 130| loss: 0.30126 | val_0_rmse: 0.55092 | val_1_rmse: 0.62923 |  0:00:45s
epoch 131| loss: 0.29391 | val_0_rmse: 0.54644 | val_1_rmse: 0.61521 |  0:00:45s
epoch 132| loss: 0.3038  | val_0_rmse: 0.53332 | val_1_rmse: 0.61021 |  0:00:45s
epoch 133| loss: 0.30117 | val_0_rmse: 0.53356 | val_1_rmse: 0.61177 |  0:00:46s
epoch 134| loss: 0.29967 | val_0_rmse: 0.53008 | val_1_rmse: 0.61757 |  0:00:46s
epoch 135| loss: 0.28686 | val_0_rmse: 0.53079 | val_1_rmse: 0.61015 |  0:00:46s
epoch 136| loss: 0.28685 | val_0_rmse: 0.53224 | val_1_rmse: 0.61585 |  0:00:47s
epoch 137| loss: 0.29007 | val_0_rmse: 0.53475 | val_1_rmse: 0.61027 |  0:00:47s
epoch 138| loss: 0.29059 | val_0_rmse: 0.52547 | val_1_rmse: 0.61749 |  0:00:47s
epoch 139| loss: 0.29306 | val_0_rmse: 0.52241 | val_1_rmse: 0.63077 |  0:00:48s
epoch 140| loss: 0.28634 | val_0_rmse: 0.52247 | val_1_rmse: 0.62637 |  0:00:48s
epoch 141| loss: 0.29025 | val_0_rmse: 0.51809 | val_1_rmse: 0.62419 |  0:00:48s
epoch 142| loss: 0.28529 | val_0_rmse: 0.52071 | val_1_rmse: 0.61995 |  0:00:49s
epoch 143| loss: 0.28459 | val_0_rmse: 0.51482 | val_1_rmse: 0.62026 |  0:00:49s
epoch 144| loss: 0.28704 | val_0_rmse: 0.51959 | val_1_rmse: 0.63    |  0:00:49s
epoch 145| loss: 0.28036 | val_0_rmse: 0.5157  | val_1_rmse: 0.63077 |  0:00:50s
epoch 146| loss: 0.28503 | val_0_rmse: 0.51868 | val_1_rmse: 0.63564 |  0:00:50s
epoch 147| loss: 0.28284 | val_0_rmse: 0.50689 | val_1_rmse: 0.62681 |  0:00:50s
epoch 148| loss: 0.28219 | val_0_rmse: 0.50843 | val_1_rmse: 0.62664 |  0:00:51s
epoch 149| loss: 0.27727 | val_0_rmse: 0.51312 | val_1_rmse: 0.6222  |  0:00:51s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.61015
Best weights from best epoch are automatically used!
ended training at: 08:07:35
Feature importance:
Mean squared error is of 3128186252.927798
Mean absolute error:35577.66153284168
MAPE:0.29696305420289043
R2 score:0.6162260167786398
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:26:04
epoch 0  | loss: 0.51632 | val_0_rmse: 0.5746  | val_1_rmse: 0.57647 |  0:00:43s
epoch 1  | loss: 0.21649 | val_0_rmse: 0.49653 | val_1_rmse: 0.49821 |  0:01:27s
epoch 2  | loss: 0.18607 | val_0_rmse: 0.42768 | val_1_rmse: 0.43095 |  0:02:11s
epoch 3  | loss: 0.17992 | val_0_rmse: 0.39833 | val_1_rmse: 0.40616 |  0:02:54s
epoch 4  | loss: 0.16761 | val_0_rmse: 0.42736 | val_1_rmse: 0.43601 |  0:03:38s
epoch 5  | loss: 0.1653  | val_0_rmse: 0.43257 | val_1_rmse: 0.44105 |  0:04:22s
epoch 6  | loss: 0.16286 | val_0_rmse: 0.41885 | val_1_rmse: 0.42682 |  0:05:05s
epoch 7  | loss: 0.15489 | val_0_rmse: 0.46472 | val_1_rmse: 0.47823 |  0:05:49s
epoch 8  | loss: 0.15198 | val_0_rmse: 0.39305 | val_1_rmse: 0.41173 |  0:06:32s
epoch 9  | loss: 0.15    | val_0_rmse: 0.53436 | val_1_rmse: 0.54387 |  0:07:16s
epoch 10 | loss: 0.14803 | val_0_rmse: 0.4568  | val_1_rmse: 0.47351 |  0:08:00s
epoch 11 | loss: 0.14791 | val_0_rmse: 0.39003 | val_1_rmse: 0.41064 |  0:08:43s
epoch 12 | loss: 0.14641 | val_0_rmse: 0.41773 | val_1_rmse: 0.43559 |  0:09:27s
epoch 13 | loss: 0.14404 | val_0_rmse: 0.38588 | val_1_rmse: 0.40741 |  0:10:11s
epoch 14 | loss: 0.14184 | val_0_rmse: 0.39103 | val_1_rmse: 0.41248 |  0:10:54s
epoch 15 | loss: 0.13919 | val_0_rmse: 0.44009 | val_1_rmse: 0.42814 |  0:11:38s
epoch 16 | loss: 0.13917 | val_0_rmse: 0.38001 | val_1_rmse: 0.40381 |  0:12:21s
epoch 17 | loss: 0.13596 | val_0_rmse: 0.41994 | val_1_rmse: 0.43894 |  0:13:03s
epoch 18 | loss: 0.13617 | val_0_rmse: 0.45601 | val_1_rmse: 0.41442 |  0:13:44s
epoch 19 | loss: 0.13546 | val_0_rmse: 0.37839 | val_1_rmse: 0.40148 |  0:14:26s
epoch 20 | loss: 0.13455 | val_0_rmse: 0.39682 | val_1_rmse: 0.42219 |  0:15:08s
epoch 21 | loss: 0.13483 | val_0_rmse: 0.377   | val_1_rmse: 0.40391 |  0:15:50s
epoch 22 | loss: 0.13226 | val_0_rmse: 0.50395 | val_1_rmse: 0.52214 |  0:16:32s
epoch 23 | loss: 0.13227 | val_0_rmse: 0.37489 | val_1_rmse: 0.40223 |  0:17:14s
epoch 24 | loss: 0.13065 | val_0_rmse: 0.37792 | val_1_rmse: 0.40096 |  0:17:56s
epoch 25 | loss: 0.13085 | val_0_rmse: 0.38568 | val_1_rmse: 0.41125 |  0:18:38s
epoch 26 | loss: 0.13062 | val_0_rmse: 0.40533 | val_1_rmse: 0.51776 |  0:19:20s
epoch 27 | loss: 0.12927 | val_0_rmse: 0.37114 | val_1_rmse: 0.39712 |  0:20:02s
epoch 28 | loss: 0.13243 | val_0_rmse: 0.36711 | val_1_rmse: 0.39407 |  0:20:44s
epoch 29 | loss: 0.12949 | val_0_rmse: 0.37201 | val_1_rmse: 0.39961 |  0:21:27s
epoch 30 | loss: 0.12928 | val_0_rmse: 0.37609 | val_1_rmse: 0.40124 |  0:22:09s
epoch 31 | loss: 0.13254 | val_0_rmse: 0.41556 | val_1_rmse: 0.43738 |  0:22:52s
epoch 32 | loss: 0.13254 | val_0_rmse: 0.38307 | val_1_rmse: 0.41066 |  0:23:35s
epoch 33 | loss: 0.12817 | val_0_rmse: 0.38037 | val_1_rmse: 0.47059 |  0:24:18s
epoch 34 | loss: 0.12831 | val_0_rmse: 0.37626 | val_1_rmse: 0.44006 |  0:25:01s
epoch 35 | loss: 0.12691 | val_0_rmse: 0.37569 | val_1_rmse: 0.45583 |  0:25:44s
epoch 36 | loss: 0.12749 | val_0_rmse: 0.37275 | val_1_rmse: 0.40134 |  0:26:27s
epoch 37 | loss: 0.12627 | val_0_rmse: 0.38182 | val_1_rmse: 0.40661 |  0:27:11s
epoch 38 | loss: 0.1266  | val_0_rmse: 0.38129 | val_1_rmse: 0.41225 |  0:27:55s
epoch 39 | loss: 0.12752 | val_0_rmse: 0.38493 | val_1_rmse: 0.41545 |  0:28:38s
epoch 40 | loss: 0.12443 | val_0_rmse: 0.36828 | val_1_rmse: 0.40171 |  0:29:21s
epoch 41 | loss: 0.12344 | val_0_rmse: 0.37685 | val_1_rmse: 0.43357 |  0:30:04s
epoch 42 | loss: 0.12204 | val_0_rmse: 0.52821 | val_1_rmse: 0.58414 |  0:30:47s
epoch 43 | loss: 0.12167 | val_0_rmse: 0.37604 | val_1_rmse: 0.40558 |  0:31:30s
epoch 44 | loss: 0.12163 | val_0_rmse: 0.38366 | val_1_rmse: 0.46313 |  0:32:12s
epoch 45 | loss: 0.12137 | val_0_rmse: 0.54113 | val_1_rmse: 0.57883 |  0:32:55s
epoch 46 | loss: 0.12038 | val_0_rmse: 0.37313 | val_1_rmse: 0.40229 |  0:33:38s
epoch 47 | loss: 0.12101 | val_0_rmse: 0.44252 | val_1_rmse: 0.47561 |  0:34:21s
epoch 48 | loss: 0.11924 | val_0_rmse: 0.39999 | val_1_rmse: 0.42884 |  0:35:04s
epoch 49 | loss: 0.11933 | val_0_rmse: 0.37174 | val_1_rmse: 0.40216 |  0:35:47s
epoch 50 | loss: 0.11808 | val_0_rmse: 0.37162 | val_1_rmse: 0.40223 |  0:36:30s
epoch 51 | loss: 0.13184 | val_0_rmse: 0.37835 | val_1_rmse: 0.40705 |  0:37:13s
epoch 52 | loss: 0.12157 | val_0_rmse: 0.38878 | val_1_rmse: 0.41411 |  0:37:56s
epoch 53 | loss: 0.11947 | val_0_rmse: 0.4149  | val_1_rmse: 0.41558 |  0:38:39s
epoch 54 | loss: 0.11871 | val_0_rmse: 0.37563 | val_1_rmse: 0.40797 |  0:39:22s
epoch 55 | loss: 0.11842 | val_0_rmse: 0.37555 | val_1_rmse: 0.40474 |  0:40:05s
epoch 56 | loss: 0.11746 | val_0_rmse: 0.412   | val_1_rmse: 0.43686 |  0:40:48s
epoch 57 | loss: 0.11685 | val_0_rmse: 0.3798  | val_1_rmse: 0.41117 |  0:41:31s
epoch 58 | loss: 0.11739 | val_0_rmse: 1.42147 | val_1_rmse: 1.59962 |  0:42:15s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.39407
Best weights from best epoch are automatically used!
ended training at: 09:08:47
Feature importance:
Mean squared error is of 7348907474.809731
Mean absolute error:52091.988635644986
MAPE:0.24996101359692308
R2 score:0.8332921421412982
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:08:57
epoch 0  | loss: 0.49121 | val_0_rmse: 0.64587 | val_1_rmse: 0.64194 |  0:00:42s
epoch 1  | loss: 0.22205 | val_0_rmse: 0.52876 | val_1_rmse: 0.52525 |  0:01:25s
epoch 2  | loss: 0.19718 | val_0_rmse: 0.44337 | val_1_rmse: 0.44651 |  0:02:08s
epoch 3  | loss: 0.18129 | val_0_rmse: 0.43378 | val_1_rmse: 0.43954 |  0:02:50s
epoch 4  | loss: 0.18168 | val_0_rmse: 0.40238 | val_1_rmse: 0.41494 |  0:03:33s
epoch 5  | loss: 0.16329 | val_0_rmse: 0.4114  | val_1_rmse: 0.42778 |  0:04:16s
epoch 6  | loss: 0.15555 | val_0_rmse: 0.71849 | val_1_rmse: 0.41655 |  0:04:59s
epoch 7  | loss: 0.15135 | val_0_rmse: 0.42552 | val_1_rmse: 0.4444  |  0:05:41s
epoch 8  | loss: 0.14897 | val_0_rmse: 1.03441 | val_1_rmse: 0.50196 |  0:06:24s
epoch 9  | loss: 0.14597 | val_0_rmse: 0.40858 | val_1_rmse: 0.42972 |  0:07:07s
epoch 10 | loss: 0.14314 | val_0_rmse: 0.50107 | val_1_rmse: 0.51688 |  0:07:50s
epoch 11 | loss: 0.14079 | val_0_rmse: 0.39526 | val_1_rmse: 0.41776 |  0:08:32s
epoch 12 | loss: 0.1403  | val_0_rmse: 0.42302 | val_1_rmse: 0.44283 |  0:09:15s
epoch 13 | loss: 0.13959 | val_0_rmse: 0.38018 | val_1_rmse: 0.40492 |  0:09:58s
epoch 14 | loss: 0.13853 | val_0_rmse: 0.37925 | val_1_rmse: 0.40796 |  0:10:41s
epoch 15 | loss: 0.13745 | val_0_rmse: 0.38978 | val_1_rmse: 0.41442 |  0:11:23s
epoch 16 | loss: 0.13599 | val_0_rmse: 0.37582 | val_1_rmse: 0.40428 |  0:12:06s
epoch 17 | loss: 0.13343 | val_0_rmse: 0.37961 | val_1_rmse: 0.40736 |  0:12:49s
epoch 18 | loss: 0.13311 | val_0_rmse: 0.36583 | val_1_rmse: 0.39207 |  0:13:32s
epoch 19 | loss: 0.13143 | val_0_rmse: 0.36832 | val_1_rmse: 0.39523 |  0:14:15s
epoch 20 | loss: 0.13175 | val_0_rmse: 0.38027 | val_1_rmse: 0.41399 |  0:14:59s
epoch 21 | loss: 0.12986 | val_0_rmse: 0.36853 | val_1_rmse: 0.4028  |  0:15:42s
epoch 22 | loss: 0.12824 | val_0_rmse: 0.37032 | val_1_rmse: 0.40237 |  0:16:25s
epoch 23 | loss: 0.13025 | val_0_rmse: 0.36761 | val_1_rmse: 0.39967 |  0:17:08s
epoch 24 | loss: 0.12765 | val_0_rmse: 0.39539 | val_1_rmse: 0.42252 |  0:17:51s
epoch 25 | loss: 0.12774 | val_0_rmse: 0.36938 | val_1_rmse: 0.4048  |  0:18:34s
epoch 26 | loss: 0.12645 | val_0_rmse: 0.37502 | val_1_rmse: 0.40837 |  0:19:16s
epoch 27 | loss: 0.12654 | val_0_rmse: 0.37154 | val_1_rmse: 0.40068 |  0:19:59s
epoch 28 | loss: 0.12672 | val_0_rmse: 0.38151 | val_1_rmse: 0.41645 |  0:20:42s
epoch 29 | loss: 0.1255  | val_0_rmse: 0.36212 | val_1_rmse: 0.39407 |  0:21:25s
epoch 30 | loss: 0.12363 | val_0_rmse: 0.37132 | val_1_rmse: 0.40503 |  0:22:08s
epoch 31 | loss: 0.12289 | val_0_rmse: 0.37765 | val_1_rmse: 0.408   |  0:22:51s
epoch 32 | loss: 0.12245 | val_0_rmse: 0.3684  | val_1_rmse: 0.40296 |  0:23:34s
epoch 33 | loss: 0.12171 | val_0_rmse: 0.36545 | val_1_rmse: 0.39587 |  0:24:17s
epoch 34 | loss: 0.12209 | val_0_rmse: 0.39149 | val_1_rmse: 0.42195 |  0:24:59s
epoch 35 | loss: 0.12085 | val_0_rmse: 0.36193 | val_1_rmse: 0.39591 |  0:25:42s
epoch 36 | loss: 0.11978 | val_0_rmse: 0.49157 | val_1_rmse: 0.41011 |  0:26:25s
epoch 37 | loss: 0.12003 | val_0_rmse: 0.35994 | val_1_rmse: 0.39864 |  0:27:08s
epoch 38 | loss: 0.16114 | val_0_rmse: 2.02845 | val_1_rmse: 2.47986 |  0:27:51s
epoch 39 | loss: 0.19735 | val_0_rmse: 0.42854 | val_1_rmse: 0.45165 |  0:28:33s
epoch 40 | loss: 0.15757 | val_0_rmse: 0.50031 | val_1_rmse: 0.5245  |  0:29:16s
epoch 41 | loss: 0.14364 | val_0_rmse: 0.62615 | val_1_rmse: 0.64879 |  0:30:00s
epoch 42 | loss: 0.14083 | val_0_rmse: 0.39296 | val_1_rmse: 0.42853 |  0:30:43s
epoch 43 | loss: 0.13315 | val_0_rmse: 0.43053 | val_1_rmse: 0.46193 |  0:31:26s
epoch 44 | loss: 0.13668 | val_0_rmse: 0.38552 | val_1_rmse: 0.42113 |  0:32:09s
epoch 45 | loss: 0.13041 | val_0_rmse: 0.37843 | val_1_rmse: 0.41593 |  0:32:52s
epoch 46 | loss: 0.12845 | val_0_rmse: 0.38281 | val_1_rmse: 0.42112 |  0:33:35s
epoch 47 | loss: 0.12576 | val_0_rmse: 0.37532 | val_1_rmse: 0.41559 |  0:34:17s
epoch 48 | loss: 0.12753 | val_0_rmse: 0.39545 | val_1_rmse: 0.4363  |  0:35:00s

Early stopping occured at epoch 48 with best_epoch = 18 and best_val_1_rmse = 0.39207
Best weights from best epoch are automatically used!
ended training at: 09:44:25
Feature importance:
Mean squared error is of 7241270332.859886
Mean absolute error:51213.65945944574
MAPE:0.2615589925443571
R2 score:0.8356964078680669
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:44:35
epoch 0  | loss: 0.47242 | val_0_rmse: 0.62386 | val_1_rmse: 0.62326 |  0:00:42s
epoch 1  | loss: 0.21926 | val_0_rmse: 0.49137 | val_1_rmse: 0.49105 |  0:01:25s
epoch 2  | loss: 0.19385 | val_0_rmse: 0.43817 | val_1_rmse: 0.44109 |  0:02:08s
epoch 3  | loss: 0.18309 | val_0_rmse: 0.40805 | val_1_rmse: 0.41546 |  0:02:50s
epoch 4  | loss: 0.18249 | val_0_rmse: 0.99634 | val_1_rmse: 1.00573 |  0:03:33s
epoch 5  | loss: 0.2181  | val_0_rmse: 0.44194 | val_1_rmse: 0.44789 |  0:04:16s
epoch 6  | loss: 0.18822 | val_0_rmse: 0.41141 | val_1_rmse: 0.4789  |  0:04:59s
epoch 7  | loss: 0.16774 | val_0_rmse: 0.41244 | val_1_rmse: 0.45501 |  0:05:42s
epoch 8  | loss: 0.16259 | val_0_rmse: 0.40336 | val_1_rmse: 0.41523 |  0:06:25s
epoch 9  | loss: 0.15943 | val_0_rmse: 0.4352  | val_1_rmse: 0.44717 |  0:07:08s
epoch 10 | loss: 0.15444 | val_0_rmse: 0.38604 | val_1_rmse: 0.43664 |  0:07:50s
epoch 11 | loss: 0.15121 | val_0_rmse: 0.40226 | val_1_rmse: 0.41978 |  0:08:33s
epoch 12 | loss: 0.14877 | val_0_rmse: 0.39341 | val_1_rmse: 0.41199 |  0:09:17s
epoch 13 | loss: 0.1467  | val_0_rmse: 0.38645 | val_1_rmse: 0.40489 |  0:10:00s
epoch 14 | loss: 0.14515 | val_0_rmse: 0.38688 | val_1_rmse: 0.40703 |  0:10:43s
epoch 15 | loss: 0.14382 | val_0_rmse: 0.39191 | val_1_rmse: 0.41195 |  0:11:26s
epoch 16 | loss: 0.14225 | val_0_rmse: 0.43589 | val_1_rmse: 0.53337 |  0:12:09s
epoch 17 | loss: 0.1402  | val_0_rmse: 0.37909 | val_1_rmse: 0.43244 |  0:12:52s
epoch 18 | loss: 0.13927 | val_0_rmse: 0.37983 | val_1_rmse: 0.39911 |  0:13:34s
epoch 19 | loss: 0.13852 | val_0_rmse: 0.39062 | val_1_rmse: 0.41251 |  0:14:17s
epoch 20 | loss: 0.13921 | val_0_rmse: 0.38075 | val_1_rmse: 0.40465 |  0:15:00s
epoch 21 | loss: 0.14007 | val_0_rmse: 0.38052 | val_1_rmse: 0.40441 |  0:15:43s
epoch 22 | loss: 0.13652 | val_0_rmse: 0.38486 | val_1_rmse: 0.40979 |  0:16:26s
epoch 23 | loss: 0.13696 | val_0_rmse: 0.38056 | val_1_rmse: 0.40517 |  0:17:08s
epoch 24 | loss: 0.13539 | val_0_rmse: 0.39726 | val_1_rmse: 0.41915 |  0:17:51s
epoch 25 | loss: 0.1351  | val_0_rmse: 0.40719 | val_1_rmse: 0.43116 |  0:18:34s
epoch 26 | loss: 0.13436 | val_0_rmse: 0.37327 | val_1_rmse: 0.39801 |  0:19:17s
epoch 27 | loss: 0.13165 | val_0_rmse: 0.37233 | val_1_rmse: 0.39704 |  0:20:00s
epoch 28 | loss: 0.1319  | val_0_rmse: 0.37666 | val_1_rmse: 0.39735 |  0:20:43s
epoch 29 | loss: 0.13256 | val_0_rmse: 0.59157 | val_1_rmse: 0.6062  |  0:21:25s
epoch 30 | loss: 0.16956 | val_0_rmse: 0.4169  | val_1_rmse: 0.43516 |  0:22:08s
epoch 31 | loss: 0.14001 | val_0_rmse: 0.39004 | val_1_rmse: 0.41282 |  0:22:51s
epoch 32 | loss: 0.13741 | val_0_rmse: 0.39458 | val_1_rmse: 0.41911 |  0:23:34s
epoch 33 | loss: 0.13621 | val_0_rmse: 0.45752 | val_1_rmse: 0.47497 |  0:24:17s
epoch 34 | loss: 0.15118 | val_0_rmse: 0.42398 | val_1_rmse: 0.42893 |  0:25:00s
epoch 35 | loss: 0.14699 | val_0_rmse: 0.43868 | val_1_rmse: 0.4625  |  0:25:43s
epoch 36 | loss: 0.15464 | val_0_rmse: 0.41012 | val_1_rmse: 0.43159 |  0:26:26s
epoch 37 | loss: 0.14715 | val_0_rmse: 0.40874 | val_1_rmse: 0.42338 |  0:27:09s
epoch 38 | loss: 0.14086 | val_0_rmse: 0.3866  | val_1_rmse: 0.41253 |  0:27:52s
epoch 39 | loss: 0.13952 | val_0_rmse: 0.55165 | val_1_rmse: 0.57227 |  0:28:35s
epoch 40 | loss: 0.1387  | val_0_rmse: 0.41029 | val_1_rmse: 0.43571 |  0:29:17s
epoch 41 | loss: 0.13642 | val_0_rmse: 0.40115 | val_1_rmse: 0.42835 |  0:30:00s
epoch 42 | loss: 0.139   | val_0_rmse: 0.41356 | val_1_rmse: 0.44668 |  0:30:43s
epoch 43 | loss: 0.1367  | val_0_rmse: 0.46535 | val_1_rmse: 0.499   |  0:31:26s
epoch 44 | loss: 0.13235 | val_0_rmse: 0.38607 | val_1_rmse: 0.41765 |  0:32:09s
epoch 45 | loss: 0.13158 | val_0_rmse: 0.38388 | val_1_rmse: 0.41219 |  0:32:52s
epoch 46 | loss: 0.13137 | val_0_rmse: 0.42354 | val_1_rmse: 0.44861 |  0:33:35s
epoch 47 | loss: 0.13002 | val_0_rmse: 0.66795 | val_1_rmse: 0.55086 |  0:34:18s
epoch 48 | loss: 0.12971 | val_0_rmse: 2.37055 | val_1_rmse: 2.43158 |  0:35:00s
epoch 49 | loss: 0.13148 | val_0_rmse: 1.62973 | val_1_rmse: 1.6719  |  0:35:43s
epoch 50 | loss: 0.12942 | val_0_rmse: 1.42101 | val_1_rmse: 1.32231 |  0:36:26s
epoch 51 | loss: 0.12765 | val_0_rmse: 1.55993 | val_1_rmse: 1.5003  |  0:37:09s
epoch 52 | loss: 0.1281  | val_0_rmse: 0.5316  | val_1_rmse: 0.56154 |  0:37:52s
epoch 53 | loss: 0.12635 | val_0_rmse: 0.44356 | val_1_rmse: 0.46722 |  0:38:35s
epoch 54 | loss: 0.13119 | val_0_rmse: 0.95756 | val_1_rmse: 0.98226 |  0:39:18s
epoch 55 | loss: 0.13381 | val_0_rmse: 1.01726 | val_1_rmse: 1.00768 |  0:40:01s
epoch 56 | loss: 0.12894 | val_0_rmse: 1.0501  | val_1_rmse: 1.06062 |  0:40:44s
epoch 57 | loss: 0.12788 | val_0_rmse: 1.40887 | val_1_rmse: 1.4421  |  0:41:27s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.39704
Best weights from best epoch are automatically used!
ended training at: 10:26:29
Feature importance:
Mean squared error is of 23659073973.779495
Mean absolute error:52285.70660927858
MAPE:0.26781620230753883
R2 score:0.4634399456153083
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:26:38
epoch 0  | loss: 0.44918 | val_0_rmse: 0.64573 | val_1_rmse: 0.64636 |  0:00:42s
epoch 1  | loss: 0.20555 | val_0_rmse: 0.4952  | val_1_rmse: 0.49639 |  0:01:25s
epoch 2  | loss: 0.18012 | val_0_rmse: 0.4892  | val_1_rmse: 0.49485 |  0:02:08s
epoch 3  | loss: 0.17126 | val_0_rmse: 0.40856 | val_1_rmse: 0.41981 |  0:02:50s
epoch 4  | loss: 0.1624  | val_0_rmse: 0.3868  | val_1_rmse: 0.40542 |  0:03:33s
epoch 5  | loss: 0.16007 | val_0_rmse: 0.40275 | val_1_rmse: 0.42199 |  0:04:16s
epoch 6  | loss: 0.15567 | val_0_rmse: 0.40146 | val_1_rmse: 0.42379 |  0:04:58s
epoch 7  | loss: 0.15649 | val_0_rmse: 0.77569 | val_1_rmse: 0.78432 |  0:05:41s
epoch 8  | loss: 0.15747 | val_0_rmse: 0.39985 | val_1_rmse: 0.42307 |  0:06:24s
epoch 9  | loss: 0.16942 | val_0_rmse: 0.4529  | val_1_rmse: 1.21169 |  0:07:07s
epoch 10 | loss: 0.15043 | val_0_rmse: 0.39794 | val_1_rmse: 0.42069 |  0:07:50s
epoch 11 | loss: 0.14732 | val_0_rmse: 0.45727 | val_1_rmse: 0.4572  |  0:08:33s
epoch 12 | loss: 0.15953 | val_0_rmse: 0.4677  | val_1_rmse: 0.46161 |  0:09:15s
epoch 13 | loss: 0.14436 | val_0_rmse: 0.38852 | val_1_rmse: 0.41184 |  0:09:58s
epoch 14 | loss: 0.14367 | val_0_rmse: 0.51293 | val_1_rmse: 0.5313  |  0:10:41s
epoch 15 | loss: 0.15202 | val_0_rmse: 0.38924 | val_1_rmse: 0.41429 |  0:11:24s
epoch 16 | loss: 0.14134 | val_0_rmse: 0.38186 | val_1_rmse: 0.40967 |  0:12:07s
epoch 17 | loss: 0.13858 | val_0_rmse: 0.39171 | val_1_rmse: 0.41978 |  0:12:51s
epoch 18 | loss: 0.13649 | val_0_rmse: 0.3821  | val_1_rmse: 0.41414 |  0:13:33s
epoch 19 | loss: 0.13633 | val_0_rmse: 0.38057 | val_1_rmse: 0.41024 |  0:14:16s
epoch 20 | loss: 0.13387 | val_0_rmse: 0.37437 | val_1_rmse: 0.40299 |  0:14:59s
epoch 21 | loss: 0.13472 | val_0_rmse: 0.38376 | val_1_rmse: 0.41439 |  0:15:42s
epoch 22 | loss: 0.13367 | val_0_rmse: 0.45412 | val_1_rmse: 0.46926 |  0:16:25s
epoch 23 | loss: 0.13954 | val_0_rmse: 0.43022 | val_1_rmse: 0.45732 |  0:17:08s
epoch 24 | loss: 0.13436 | val_0_rmse: 0.37937 | val_1_rmse: 0.41984 |  0:17:51s
epoch 25 | loss: 0.13134 | val_0_rmse: 0.81393 | val_1_rmse: 0.83311 |  0:18:33s
epoch 26 | loss: 0.13036 | val_0_rmse: 0.37579 | val_1_rmse: 0.4063  |  0:19:16s
epoch 27 | loss: 0.12855 | val_0_rmse: 0.37931 | val_1_rmse: 0.41163 |  0:19:59s
epoch 28 | loss: 0.12856 | val_0_rmse: 0.37854 | val_1_rmse: 0.41669 |  0:20:42s
epoch 29 | loss: 0.12794 | val_0_rmse: 0.47963 | val_1_rmse: 0.50164 |  0:21:25s
epoch 30 | loss: 0.12636 | val_0_rmse: 0.63474 | val_1_rmse: 0.70688 |  0:22:07s
epoch 31 | loss: 0.12587 | val_0_rmse: 0.37756 | val_1_rmse: 0.4151  |  0:22:50s
epoch 32 | loss: 0.13252 | val_0_rmse: 0.38844 | val_1_rmse: 0.42958 |  0:23:33s
epoch 33 | loss: 0.14722 | val_0_rmse: 0.5355  | val_1_rmse: 0.55475 |  0:24:16s
epoch 34 | loss: 0.14935 | val_0_rmse: 36.48998| val_1_rmse: 36.49237|  0:24:59s
epoch 35 | loss: 0.15453 | val_0_rmse: 1.61393 | val_1_rmse: 1.74563 |  0:25:42s
epoch 36 | loss: 0.13676 | val_0_rmse: 1.97687 | val_1_rmse: 2.14014 |  0:26:25s
epoch 37 | loss: 0.13294 | val_0_rmse: 2.05865 | val_1_rmse: 2.23295 |  0:27:07s
epoch 38 | loss: 0.13154 | val_0_rmse: 1.9593  | val_1_rmse: 2.1353  |  0:27:51s
epoch 39 | loss: 0.13152 | val_0_rmse: 0.78738 | val_1_rmse: 0.87102 |  0:28:34s
epoch 40 | loss: 0.12878 | val_0_rmse: 0.79064 | val_1_rmse: 0.87603 |  0:29:17s
epoch 41 | loss: 0.13302 | val_0_rmse: 1.87275 | val_1_rmse: 2.02759 |  0:30:00s
epoch 42 | loss: 0.12779 | val_0_rmse: 1.89115 | val_1_rmse: 2.05029 |  0:30:43s
epoch 43 | loss: 0.12734 | val_0_rmse: 0.95483 | val_1_rmse: 1.03923 |  0:31:25s
epoch 44 | loss: 0.12623 | val_0_rmse: 1.84575 | val_1_rmse: 2.02681 |  0:32:08s
epoch 45 | loss: 0.12391 | val_0_rmse: 1.57295 | val_1_rmse: 1.70124 |  0:32:51s
epoch 46 | loss: 0.12384 | val_0_rmse: 1.51082 | val_1_rmse: 1.63238 |  0:33:34s
epoch 47 | loss: 0.12747 | val_0_rmse: 1.11294 | val_1_rmse: 1.22673 |  0:34:17s
epoch 48 | loss: 0.12417 | val_0_rmse: 1.72138 | val_1_rmse: 1.85746 |  0:35:00s
epoch 49 | loss: 0.12323 | val_0_rmse: 1.16412 | val_1_rmse: 1.25533 |  0:35:43s
epoch 50 | loss: 0.12187 | val_0_rmse: 3.72299 | val_1_rmse: 4.03125 |  0:36:25s

Early stopping occured at epoch 50 with best_epoch = 20 and best_val_1_rmse = 0.40299
Best weights from best epoch are automatically used!
ended training at: 11:03:31
Feature importance:
Mean squared error is of 7227876735.035729
Mean absolute error:51806.01300050664
MAPE:0.2598073081958333
R2 score:0.8373041541211732
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:03:39
epoch 0  | loss: 0.48347 | val_0_rmse: 0.60789 | val_1_rmse: 0.60224 |  0:00:42s
epoch 1  | loss: 0.22369 | val_0_rmse: 0.48981 | val_1_rmse: 0.48522 |  0:01:25s
epoch 2  | loss: 0.1902  | val_0_rmse: 0.45435 | val_1_rmse: 0.45224 |  0:02:08s
epoch 3  | loss: 0.17718 | val_0_rmse: 0.40398 | val_1_rmse: 0.40387 |  0:02:51s
epoch 4  | loss: 0.17028 | val_0_rmse: 0.41481 | val_1_rmse: 0.40419 |  0:03:34s
epoch 5  | loss: 0.17379 | val_0_rmse: 0.42424 | val_1_rmse: 0.4281  |  0:04:16s
epoch 6  | loss: 0.16641 | val_0_rmse: 0.41225 | val_1_rmse: 0.41982 |  0:04:59s
epoch 7  | loss: 0.16267 | val_0_rmse: 0.40882 | val_1_rmse: 0.417   |  0:05:43s
epoch 8  | loss: 0.16512 | val_0_rmse: 0.69518 | val_1_rmse: 0.69743 |  0:06:26s
epoch 9  | loss: 0.16573 | val_0_rmse: 0.40879 | val_1_rmse: 0.42155 |  0:07:09s
epoch 10 | loss: 0.15604 | val_0_rmse: 0.40192 | val_1_rmse: 0.42025 |  0:07:52s
epoch 11 | loss: 0.16215 | val_0_rmse: 0.41436 | val_1_rmse: 0.42647 |  0:08:35s
epoch 12 | loss: 0.16453 | val_0_rmse: 0.54238 | val_1_rmse: 0.55205 |  0:09:18s
epoch 13 | loss: 0.15362 | val_0_rmse: 0.39684 | val_1_rmse: 0.41133 |  0:10:01s
epoch 14 | loss: 0.14928 | val_0_rmse: 0.39315 | val_1_rmse: 0.41171 |  0:10:43s
epoch 15 | loss: 0.14912 | val_0_rmse: 0.38644 | val_1_rmse: 0.40473 |  0:11:26s
epoch 16 | loss: 0.14581 | val_0_rmse: 0.39426 | val_1_rmse: 0.41559 |  0:12:09s
epoch 17 | loss: 0.14605 | val_0_rmse: 0.3928  | val_1_rmse: 0.40899 |  0:12:52s
epoch 18 | loss: 0.18562 | val_0_rmse: 0.57432 | val_1_rmse: 0.5799  |  0:13:35s
epoch 19 | loss: 0.17387 | val_0_rmse: 0.41407 | val_1_rmse: 0.43011 |  0:14:18s
epoch 20 | loss: 0.16087 | val_0_rmse: 0.39202 | val_1_rmse: 0.40554 |  0:15:01s
epoch 21 | loss: 0.1587  | val_0_rmse: 0.47689 | val_1_rmse: 0.48868 |  0:15:44s
epoch 22 | loss: 0.15805 | val_0_rmse: 0.59659 | val_1_rmse: 0.43633 |  0:16:27s
epoch 23 | loss: 0.22471 | val_0_rmse: 0.45056 | val_1_rmse: 0.45804 |  0:17:10s
epoch 24 | loss: 0.1888  | val_0_rmse: 0.43888 | val_1_rmse: 0.45299 |  0:17:53s
epoch 25 | loss: 0.16509 | val_0_rmse: 0.40766 | val_1_rmse: 0.42158 |  0:18:36s
epoch 26 | loss: 0.18197 | val_0_rmse: 0.41523 | val_1_rmse: 0.42747 |  0:19:19s
epoch 27 | loss: 0.16358 | val_0_rmse: 0.44236 | val_1_rmse: 0.45427 |  0:20:02s
epoch 28 | loss: 0.17262 | val_0_rmse: 0.41758 | val_1_rmse: 0.42265 |  0:20:45s
epoch 29 | loss: 0.15703 | val_0_rmse: 0.41216 | val_1_rmse: 0.42273 |  0:21:28s
epoch 30 | loss: 0.15476 | val_0_rmse: 0.46404 | val_1_rmse: 0.47083 |  0:22:11s
epoch 31 | loss: 0.15022 | val_0_rmse: 0.41149 | val_1_rmse: 0.4268  |  0:22:54s
epoch 32 | loss: 0.16262 | val_0_rmse: 0.47264 | val_1_rmse: 0.43158 |  0:23:37s
epoch 33 | loss: 0.16222 | val_0_rmse: 1.70126 | val_1_rmse: 3.31418 |  0:24:20s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.40387
Best weights from best epoch are automatically used!
ended training at: 11:28:27
Feature importance:
Mean squared error is of 8590170957.591919
Mean absolute error:57113.37977671251
MAPE:0.27171524096890465
R2 score:0.8089406498201427
------------------------------------------------------------------
