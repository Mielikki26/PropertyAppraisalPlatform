TabNet Logs:

Saving copy of script...
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:19:06
epoch 0  | loss: 1.38562 | val_0_rmse: 1.00604 | val_1_rmse: 0.99786 |  0:00:05s
epoch 1  | loss: 1.01782 | val_0_rmse: 1.00718 | val_1_rmse: 0.99874 |  0:00:07s
epoch 2  | loss: 1.0176  | val_0_rmse: 1.00619 | val_1_rmse: 0.99823 |  0:00:09s
epoch 3  | loss: 1.01623 | val_0_rmse: 1.00653 | val_1_rmse: 0.99822 |  0:00:10s
epoch 4  | loss: 1.01416 | val_0_rmse: 1.00412 | val_1_rmse: 0.99601 |  0:00:12s
epoch 5  | loss: 1.01516 | val_0_rmse: 0.99839 | val_1_rmse: 0.98988 |  0:00:14s
epoch 6  | loss: 0.98414 | val_0_rmse: 0.9434  | val_1_rmse: 0.93645 |  0:00:16s
epoch 7  | loss: 0.89452 | val_0_rmse: 0.9152  | val_1_rmse: 0.903   |  0:00:18s
epoch 8  | loss: 0.81894 | val_0_rmse: 0.91035 | val_1_rmse: 0.90052 |  0:00:19s
epoch 9  | loss: 0.76687 | val_0_rmse: 0.91788 | val_1_rmse: 0.91313 |  0:00:21s
epoch 10 | loss: 0.72934 | val_0_rmse: 0.89297 | val_1_rmse: 0.87907 |  0:00:23s
epoch 11 | loss: 0.69893 | val_0_rmse: 0.85527 | val_1_rmse: 0.85628 |  0:00:25s
epoch 12 | loss: 0.66703 | val_0_rmse: 0.85697 | val_1_rmse: 0.86012 |  0:00:27s
epoch 13 | loss: 0.64272 | val_0_rmse: 0.84254 | val_1_rmse: 0.84798 |  0:00:28s
epoch 14 | loss: 0.624   | val_0_rmse: 0.84157 | val_1_rmse: 0.84475 |  0:00:30s
epoch 15 | loss: 0.60802 | val_0_rmse: 0.81866 | val_1_rmse: 0.83491 |  0:00:32s
epoch 16 | loss: 0.59301 | val_0_rmse: 0.81994 | val_1_rmse: 0.83135 |  0:00:34s
epoch 17 | loss: 0.57621 | val_0_rmse: 0.80226 | val_1_rmse: 0.81774 |  0:00:35s
epoch 18 | loss: 0.57015 | val_0_rmse: 0.79329 | val_1_rmse: 0.80477 |  0:00:37s
epoch 19 | loss: 0.56117 | val_0_rmse: 0.80299 | val_1_rmse: 0.83265 |  0:00:39s
epoch 20 | loss: 0.56299 | val_0_rmse: 0.77982 | val_1_rmse: 0.79848 |  0:00:41s
epoch 21 | loss: 0.54825 | val_0_rmse: 0.80321 | val_1_rmse: 0.81996 |  0:00:43s
epoch 22 | loss: 0.53831 | val_0_rmse: 0.80332 | val_1_rmse: 0.85097 |  0:00:45s
epoch 23 | loss: 0.5377  | val_0_rmse: 0.7551  | val_1_rmse: 0.78977 |  0:00:46s
epoch 24 | loss: 0.53302 | val_0_rmse: 0.7371  | val_1_rmse: 0.77491 |  0:00:48s
epoch 25 | loss: 0.53169 | val_0_rmse: 0.78339 | val_1_rmse: 0.81475 |  0:00:50s
epoch 26 | loss: 0.54196 | val_0_rmse: 0.78228 | val_1_rmse: 0.82212 |  0:00:52s
epoch 27 | loss: 0.52964 | val_0_rmse: 0.75474 | val_1_rmse: 0.82746 |  0:00:53s
epoch 28 | loss: 0.5184  | val_0_rmse: 0.7018  | val_1_rmse: 0.77028 |  0:00:55s
epoch 29 | loss: 0.51173 | val_0_rmse: 0.70031 | val_1_rmse: 0.77831 |  0:00:57s
epoch 30 | loss: 0.51305 | val_0_rmse: 0.71248 | val_1_rmse: 0.7733  |  0:00:59s
epoch 31 | loss: 0.49619 | val_0_rmse: 0.67785 | val_1_rmse: 0.76765 |  0:01:01s
epoch 32 | loss: 0.4987  | val_0_rmse: 0.69336 | val_1_rmse: 0.79588 |  0:01:02s
epoch 33 | loss: 0.49106 | val_0_rmse: 0.7044  | val_1_rmse: 0.81501 |  0:01:04s
epoch 34 | loss: 0.49524 | val_0_rmse: 0.68754 | val_1_rmse: 0.76882 |  0:01:06s
epoch 35 | loss: 0.4868  | val_0_rmse: 0.66846 | val_1_rmse: 0.76725 |  0:01:08s
epoch 36 | loss: 0.48224 | val_0_rmse: 0.65956 | val_1_rmse: 0.77085 |  0:01:09s
epoch 37 | loss: 0.48147 | val_0_rmse: 0.67257 | val_1_rmse: 0.80662 |  0:01:11s
epoch 38 | loss: 0.47745 | val_0_rmse: 0.66377 | val_1_rmse: 0.79189 |  0:01:13s
epoch 39 | loss: 0.47339 | val_0_rmse: 0.66938 | val_1_rmse: 0.8142  |  0:01:15s
epoch 40 | loss: 0.47557 | val_0_rmse: 0.74514 | val_1_rmse: 0.83923 |  0:01:17s
epoch 41 | loss: 0.48998 | val_0_rmse: 0.68508 | val_1_rmse: 0.80355 |  0:01:18s
epoch 42 | loss: 0.48693 | val_0_rmse: 0.69744 | val_1_rmse: 0.79651 |  0:01:20s
epoch 43 | loss: 0.47281 | val_0_rmse: 0.65341 | val_1_rmse: 0.79124 |  0:01:22s
epoch 44 | loss: 0.45774 | val_0_rmse: 0.66893 | val_1_rmse: 0.83844 |  0:01:24s
epoch 45 | loss: 0.45854 | val_0_rmse: 0.69159 | val_1_rmse: 0.81166 |  0:01:26s
epoch 46 | loss: 0.45968 | val_0_rmse: 0.65145 | val_1_rmse: 0.79706 |  0:01:27s
epoch 47 | loss: 0.44873 | val_0_rmse: 0.64678 | val_1_rmse: 0.80835 |  0:01:29s
epoch 48 | loss: 0.45096 | val_0_rmse: 0.64851 | val_1_rmse: 0.80405 |  0:01:31s
epoch 49 | loss: 0.4402  | val_0_rmse: 0.64823 | val_1_rmse: 0.80777 |  0:01:33s
epoch 50 | loss: 0.43405 | val_0_rmse: 0.65519 | val_1_rmse: 0.82876 |  0:01:34s
epoch 51 | loss: 0.43682 | val_0_rmse: 0.64417 | val_1_rmse: 0.83039 |  0:01:36s
epoch 52 | loss: 0.43035 | val_0_rmse: 0.63056 | val_1_rmse: 0.81026 |  0:01:38s
epoch 53 | loss: 0.4253  | val_0_rmse: 0.64186 | val_1_rmse: 0.80099 |  0:01:40s
epoch 54 | loss: 0.42206 | val_0_rmse: 0.63671 | val_1_rmse: 0.80945 |  0:01:41s
epoch 55 | loss: 0.41023 | val_0_rmse: 0.61752 | val_1_rmse: 0.80484 |  0:01:43s
epoch 56 | loss: 0.40589 | val_0_rmse: 0.60594 | val_1_rmse: 0.80477 |  0:01:45s
epoch 57 | loss: 0.40684 | val_0_rmse: 0.6549  | val_1_rmse: 0.89578 |  0:01:47s
epoch 58 | loss: 0.41133 | val_0_rmse: 0.60671 | val_1_rmse: 0.80962 |  0:01:49s
epoch 59 | loss: 0.41723 | val_0_rmse: 0.66685 | val_1_rmse: 0.89161 |  0:01:50s
epoch 60 | loss: 0.40436 | val_0_rmse: 0.63224 | val_1_rmse: 0.82643 |  0:01:52s
epoch 61 | loss: 0.42186 | val_0_rmse: 0.61005 | val_1_rmse: 0.8058  |  0:01:54s
epoch 62 | loss: 0.40759 | val_0_rmse: 0.63533 | val_1_rmse: 0.82777 |  0:01:56s
epoch 63 | loss: 0.40438 | val_0_rmse: 0.85347 | val_1_rmse: 1.08922 |  0:01:58s
epoch 64 | loss: 0.40364 | val_0_rmse: 0.60353 | val_1_rmse: 0.79904 |  0:01:59s
epoch 65 | loss: 0.39544 | val_0_rmse: 0.6182  | val_1_rmse: 0.80332 |  0:02:01s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.76725
Best weights from best epoch are automatically used!
ended training at: 03:21:09
Feature importance:
Mean squared error is of 0.042618081528538965
Mean absolute error:0.1305547551366906
MAPE:0.14084933895639137
R2 score:0.3747951495820786
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:21:10
epoch 0  | loss: 1.48284 | val_0_rmse: 1.00824 | val_1_rmse: 0.99315 |  0:00:01s
epoch 1  | loss: 1.02475 | val_0_rmse: 1.00747 | val_1_rmse: 0.99217 |  0:00:03s
epoch 2  | loss: 0.96    | val_0_rmse: 0.93449 | val_1_rmse: 0.92049 |  0:00:05s
epoch 3  | loss: 0.84862 | val_0_rmse: 0.90589 | val_1_rmse: 0.88663 |  0:00:07s
epoch 4  | loss: 0.81972 | val_0_rmse: 0.90944 | val_1_rmse: 0.89719 |  0:00:08s
epoch 5  | loss: 0.82467 | val_0_rmse: 0.90273 | val_1_rmse: 0.88626 |  0:00:10s
epoch 6  | loss: 0.81535 | val_0_rmse: 0.89092 | val_1_rmse: 0.87239 |  0:00:12s
epoch 7  | loss: 0.81285 | val_0_rmse: 0.89525 | val_1_rmse: 0.87793 |  0:00:14s
epoch 8  | loss: 0.8059  | val_0_rmse: 0.89028 | val_1_rmse: 0.87165 |  0:00:16s
epoch 9  | loss: 0.80266 | val_0_rmse: 0.89121 | val_1_rmse: 0.87384 |  0:00:17s
epoch 10 | loss: 0.80196 | val_0_rmse: 0.88783 | val_1_rmse: 0.8697  |  0:00:19s
epoch 11 | loss: 0.78479 | val_0_rmse: 0.87812 | val_1_rmse: 0.86026 |  0:00:21s
epoch 12 | loss: 0.77768 | val_0_rmse: 0.87738 | val_1_rmse: 0.86217 |  0:00:23s
epoch 13 | loss: 0.77065 | val_0_rmse: 0.86948 | val_1_rmse: 0.85834 |  0:00:25s
epoch 14 | loss: 0.76675 | val_0_rmse: 0.87791 | val_1_rmse: 0.866   |  0:00:26s
epoch 15 | loss: 0.76591 | val_0_rmse: 0.88872 | val_1_rmse: 0.87485 |  0:00:28s
epoch 16 | loss: 0.75396 | val_0_rmse: 0.86349 | val_1_rmse: 0.84774 |  0:00:30s
epoch 17 | loss: 0.7439  | val_0_rmse: 0.87217 | val_1_rmse: 0.85601 |  0:00:32s
epoch 18 | loss: 0.759   | val_0_rmse: 0.87033 | val_1_rmse: 0.84904 |  0:00:33s
epoch 19 | loss: 0.75015 | val_0_rmse: 0.86085 | val_1_rmse: 0.84329 |  0:00:35s
epoch 20 | loss: 0.72247 | val_0_rmse: 0.83417 | val_1_rmse: 0.81485 |  0:00:37s
epoch 21 | loss: 0.70269 | val_0_rmse: 0.82703 | val_1_rmse: 0.81208 |  0:00:39s
epoch 22 | loss: 0.69169 | val_0_rmse: 0.83507 | val_1_rmse: 0.82514 |  0:00:41s
epoch 23 | loss: 0.70931 | val_0_rmse: 0.83172 | val_1_rmse: 0.82403 |  0:00:42s
epoch 24 | loss: 0.68417 | val_0_rmse: 0.80789 | val_1_rmse: 0.79858 |  0:00:44s
epoch 25 | loss: 0.66738 | val_0_rmse: 0.80976 | val_1_rmse: 0.80342 |  0:00:46s
epoch 26 | loss: 0.66798 | val_0_rmse: 0.8049  | val_1_rmse: 0.79556 |  0:00:48s
epoch 27 | loss: 0.65904 | val_0_rmse: 0.7959  | val_1_rmse: 0.793   |  0:00:49s
epoch 28 | loss: 0.65448 | val_0_rmse: 0.7955  | val_1_rmse: 0.79555 |  0:00:51s
epoch 29 | loss: 0.64954 | val_0_rmse: 0.79861 | val_1_rmse: 0.78939 |  0:00:53s
epoch 30 | loss: 0.65282 | val_0_rmse: 0.78947 | val_1_rmse: 0.78429 |  0:00:55s
epoch 31 | loss: 0.6398  | val_0_rmse: 0.84401 | val_1_rmse: 0.84106 |  0:00:57s
epoch 32 | loss: 0.64543 | val_0_rmse: 0.78957 | val_1_rmse: 0.79278 |  0:00:58s
epoch 33 | loss: 0.63515 | val_0_rmse: 0.78328 | val_1_rmse: 0.78898 |  0:01:00s
epoch 34 | loss: 0.63123 | val_0_rmse: 0.77777 | val_1_rmse: 0.78389 |  0:01:02s
epoch 35 | loss: 0.63092 | val_0_rmse: 0.78337 | val_1_rmse: 0.78738 |  0:01:04s
epoch 36 | loss: 0.63889 | val_0_rmse: 0.77274 | val_1_rmse: 0.78228 |  0:01:06s
epoch 37 | loss: 0.6218  | val_0_rmse: 0.78327 | val_1_rmse: 0.79201 |  0:01:07s
epoch 38 | loss: 0.62286 | val_0_rmse: 0.76785 | val_1_rmse: 0.78287 |  0:01:09s
epoch 39 | loss: 0.61295 | val_0_rmse: 0.76597 | val_1_rmse: 0.7805  |  0:01:11s
epoch 40 | loss: 0.61146 | val_0_rmse: 0.76781 | val_1_rmse: 0.78287 |  0:01:13s
epoch 41 | loss: 0.6021  | val_0_rmse: 0.75718 | val_1_rmse: 0.77623 |  0:01:14s
epoch 42 | loss: 0.59639 | val_0_rmse: 0.75811 | val_1_rmse: 0.7749  |  0:01:16s
epoch 43 | loss: 0.60381 | val_0_rmse: 0.76423 | val_1_rmse: 0.78171 |  0:01:18s
epoch 44 | loss: 0.60505 | val_0_rmse: 0.76608 | val_1_rmse: 0.78229 |  0:01:20s
epoch 45 | loss: 0.60134 | val_0_rmse: 0.7585  | val_1_rmse: 0.77835 |  0:01:22s
epoch 46 | loss: 0.5932  | val_0_rmse: 0.75557 | val_1_rmse: 0.78019 |  0:01:23s
epoch 47 | loss: 0.59115 | val_0_rmse: 0.74741 | val_1_rmse: 0.77406 |  0:01:25s
epoch 48 | loss: 0.59044 | val_0_rmse: 0.77537 | val_1_rmse: 0.80666 |  0:01:27s
epoch 49 | loss: 0.58658 | val_0_rmse: 0.76121 | val_1_rmse: 0.78166 |  0:01:29s
epoch 50 | loss: 0.57807 | val_0_rmse: 0.74852 | val_1_rmse: 0.78584 |  0:01:30s
epoch 51 | loss: 0.57489 | val_0_rmse: 0.74287 | val_1_rmse: 0.77659 |  0:01:32s
epoch 52 | loss: 0.57719 | val_0_rmse: 0.74434 | val_1_rmse: 0.76717 |  0:01:34s
epoch 53 | loss: 0.57786 | val_0_rmse: 0.75546 | val_1_rmse: 0.77308 |  0:01:36s
epoch 54 | loss: 0.58467 | val_0_rmse: 0.74127 | val_1_rmse: 0.77493 |  0:01:38s
epoch 55 | loss: 0.56882 | val_0_rmse: 0.73519 | val_1_rmse: 0.77198 |  0:01:39s
epoch 56 | loss: 0.56405 | val_0_rmse: 0.73274 | val_1_rmse: 0.76277 |  0:01:41s
epoch 57 | loss: 0.55513 | val_0_rmse: 0.72973 | val_1_rmse: 0.76641 |  0:01:43s
epoch 58 | loss: 0.55913 | val_0_rmse: 0.74151 | val_1_rmse: 0.77341 |  0:01:45s
epoch 59 | loss: 0.55032 | val_0_rmse: 0.74069 | val_1_rmse: 0.77146 |  0:01:46s
epoch 60 | loss: 0.55003 | val_0_rmse: 0.7242  | val_1_rmse: 0.76914 |  0:01:48s
epoch 61 | loss: 0.54793 | val_0_rmse: 0.72197 | val_1_rmse: 0.76471 |  0:01:50s
epoch 62 | loss: 0.56815 | val_0_rmse: 0.7301  | val_1_rmse: 0.76038 |  0:01:52s
epoch 63 | loss: 0.55218 | val_0_rmse: 0.73614 | val_1_rmse: 0.78064 |  0:01:54s
epoch 64 | loss: 0.55317 | val_0_rmse: 0.71297 | val_1_rmse: 0.7555  |  0:01:55s
epoch 65 | loss: 0.54388 | val_0_rmse: 0.73801 | val_1_rmse: 0.77754 |  0:01:57s
epoch 66 | loss: 0.53601 | val_0_rmse: 0.7152  | val_1_rmse: 0.77988 |  0:01:59s
epoch 67 | loss: 0.52214 | val_0_rmse: 0.70394 | val_1_rmse: 0.75725 |  0:02:01s
epoch 68 | loss: 0.52017 | val_0_rmse: 0.69982 | val_1_rmse: 0.74831 |  0:02:02s
epoch 69 | loss: 0.51548 | val_0_rmse: 0.71081 | val_1_rmse: 0.7746  |  0:02:04s
epoch 70 | loss: 0.51393 | val_0_rmse: 0.69319 | val_1_rmse: 0.75486 |  0:02:06s
epoch 71 | loss: 0.51243 | val_0_rmse: 0.69307 | val_1_rmse: 0.76535 |  0:02:08s
epoch 72 | loss: 0.51137 | val_0_rmse: 0.68893 | val_1_rmse: 0.75642 |  0:02:10s
epoch 73 | loss: 0.50878 | val_0_rmse: 0.70402 | val_1_rmse: 0.77975 |  0:02:11s
epoch 74 | loss: 0.50198 | val_0_rmse: 0.71013 | val_1_rmse: 0.76276 |  0:02:13s
epoch 75 | loss: 0.5099  | val_0_rmse: 0.69484 | val_1_rmse: 0.7584  |  0:02:15s
epoch 76 | loss: 0.50988 | val_0_rmse: 0.69728 | val_1_rmse: 0.7892  |  0:02:17s
epoch 77 | loss: 0.50152 | val_0_rmse: 0.69052 | val_1_rmse: 0.77635 |  0:02:19s
epoch 78 | loss: 0.50858 | val_0_rmse: 0.6993  | val_1_rmse: 0.76824 |  0:02:20s
epoch 79 | loss: 0.50229 | val_0_rmse: 0.67944 | val_1_rmse: 0.74722 |  0:02:22s
epoch 80 | loss: 0.4946  | val_0_rmse: 0.67624 | val_1_rmse: 0.74835 |  0:02:24s
epoch 81 | loss: 0.4883  | val_0_rmse: 0.68836 | val_1_rmse: 0.76516 |  0:02:26s
epoch 82 | loss: 0.48634 | val_0_rmse: 0.68425 | val_1_rmse: 0.75708 |  0:02:28s
epoch 83 | loss: 0.48702 | val_0_rmse: 0.79615 | val_1_rmse: 0.89788 |  0:02:29s
epoch 84 | loss: 0.49054 | val_0_rmse: 0.68226 | val_1_rmse: 0.7779  |  0:02:31s
epoch 85 | loss: 0.49123 | val_0_rmse: 0.68966 | val_1_rmse: 0.76647 |  0:02:33s
epoch 86 | loss: 0.49705 | val_0_rmse: 0.67751 | val_1_rmse: 0.76001 |  0:02:35s
epoch 87 | loss: 0.51214 | val_0_rmse: 0.71568 | val_1_rmse: 0.78048 |  0:02:36s
epoch 88 | loss: 0.51277 | val_0_rmse: 0.70399 | val_1_rmse: 0.77743 |  0:02:38s
epoch 89 | loss: 0.49515 | val_0_rmse: 0.68278 | val_1_rmse: 0.7654  |  0:02:40s
epoch 90 | loss: 0.48001 | val_0_rmse: 0.67568 | val_1_rmse: 0.75646 |  0:02:42s
epoch 91 | loss: 0.47321 | val_0_rmse: 0.67979 | val_1_rmse: 0.785   |  0:02:44s
epoch 92 | loss: 0.47209 | val_0_rmse: 0.67377 | val_1_rmse: 0.75888 |  0:02:45s
epoch 93 | loss: 0.49496 | val_0_rmse: 0.68286 | val_1_rmse: 0.75944 |  0:02:47s
epoch 94 | loss: 0.48254 | val_0_rmse: 0.70566 | val_1_rmse: 0.8201  |  0:02:49s
epoch 95 | loss: 0.48426 | val_0_rmse: 0.68312 | val_1_rmse: 0.76403 |  0:02:51s
epoch 96 | loss: 0.48521 | val_0_rmse: 0.68056 | val_1_rmse: 0.76534 |  0:02:52s
epoch 97 | loss: 0.48998 | val_0_rmse: 0.67543 | val_1_rmse: 0.76665 |  0:02:54s
epoch 98 | loss: 0.48073 | val_0_rmse: 0.68654 | val_1_rmse: 0.76829 |  0:02:56s
epoch 99 | loss: 0.47884 | val_0_rmse: 0.66533 | val_1_rmse: 0.76387 |  0:02:58s
epoch 100| loss: 0.47415 | val_0_rmse: 0.68251 | val_1_rmse: 0.77522 |  0:03:00s
epoch 101| loss: 0.47154 | val_0_rmse: 0.65978 | val_1_rmse: 0.7645  |  0:03:01s
epoch 102| loss: 0.46596 | val_0_rmse: 0.66572 | val_1_rmse: 0.76298 |  0:03:03s
epoch 103| loss: 0.46753 | val_0_rmse: 0.66316 | val_1_rmse: 0.76982 |  0:03:05s
epoch 104| loss: 0.45965 | val_0_rmse: 0.67726 | val_1_rmse: 0.76206 |  0:03:07s
epoch 105| loss: 0.45408 | val_0_rmse: 0.65193 | val_1_rmse: 0.77053 |  0:03:08s
epoch 106| loss: 0.45446 | val_0_rmse: 0.6531  | val_1_rmse: 0.75931 |  0:03:10s
epoch 107| loss: 0.45887 | val_0_rmse: 0.65681 | val_1_rmse: 0.75821 |  0:03:12s
epoch 108| loss: 0.47309 | val_0_rmse: 0.65219 | val_1_rmse: 0.75898 |  0:03:14s
epoch 109| loss: 0.4561  | val_0_rmse: 0.66056 | val_1_rmse: 0.77262 |  0:03:16s

Early stopping occured at epoch 109 with best_epoch = 79 and best_val_1_rmse = 0.74722
Best weights from best epoch are automatically used!
ended training at: 03:24:26
Feature importance:
Mean squared error is of 0.03943566325744014
Mean absolute error:0.13537997213468292
MAPE:0.14736179092392526
R2 score:0.3897146687541657
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:24:27
epoch 0  | loss: 1.46812 | val_0_rmse: 1.006   | val_1_rmse: 0.99786 |  0:00:01s
epoch 1  | loss: 1.02551 | val_0_rmse: 0.97902 | val_1_rmse: 0.96526 |  0:00:03s
epoch 2  | loss: 0.97238 | val_0_rmse: 0.99444 | val_1_rmse: 0.98597 |  0:00:05s
epoch 3  | loss: 0.95809 | val_0_rmse: 0.99824 | val_1_rmse: 0.98827 |  0:00:07s
epoch 4  | loss: 0.95838 | val_0_rmse: 0.97951 | val_1_rmse: 0.97001 |  0:00:08s
epoch 5  | loss: 0.95217 | val_0_rmse: 0.95947 | val_1_rmse: 0.94992 |  0:00:10s
epoch 6  | loss: 0.94185 | val_0_rmse: 0.95887 | val_1_rmse: 0.94895 |  0:00:12s
epoch 7  | loss: 0.94004 | val_0_rmse: 0.98167 | val_1_rmse: 0.97339 |  0:00:14s
epoch 8  | loss: 0.97391 | val_0_rmse: 0.97255 | val_1_rmse: 0.96277 |  0:00:16s
epoch 9  | loss: 0.92624 | val_0_rmse: 0.95318 | val_1_rmse: 0.94044 |  0:00:17s
epoch 10 | loss: 0.90694 | val_0_rmse: 0.92715 | val_1_rmse: 0.91463 |  0:00:19s
epoch 11 | loss: 0.85705 | val_0_rmse: 0.90781 | val_1_rmse: 0.89905 |  0:00:21s
epoch 12 | loss: 0.85174 | val_0_rmse: 0.92897 | val_1_rmse: 0.92206 |  0:00:23s
epoch 13 | loss: 0.80675 | val_0_rmse: 0.89522 | val_1_rmse: 0.89136 |  0:00:25s
epoch 14 | loss: 0.77867 | val_0_rmse: 0.88847 | val_1_rmse: 0.88203 |  0:00:26s
epoch 15 | loss: 0.74763 | val_0_rmse: 0.85742 | val_1_rmse: 0.85043 |  0:00:28s
epoch 16 | loss: 0.72608 | val_0_rmse: 0.85957 | val_1_rmse: 0.85116 |  0:00:30s
epoch 17 | loss: 0.71119 | val_0_rmse: 0.85215 | val_1_rmse: 0.84201 |  0:00:32s
epoch 18 | loss: 0.7179  | val_0_rmse: 0.85097 | val_1_rmse: 0.83944 |  0:00:34s
epoch 19 | loss: 0.70571 | val_0_rmse: 0.84733 | val_1_rmse: 0.84333 |  0:00:35s
epoch 20 | loss: 0.69306 | val_0_rmse: 0.84265 | val_1_rmse: 0.83635 |  0:00:37s
epoch 21 | loss: 0.68933 | val_0_rmse: 0.85161 | val_1_rmse: 0.84555 |  0:00:39s
epoch 22 | loss: 0.68073 | val_0_rmse: 0.83759 | val_1_rmse: 0.83222 |  0:00:41s
epoch 23 | loss: 0.66721 | val_0_rmse: 0.82288 | val_1_rmse: 0.81055 |  0:00:43s
epoch 24 | loss: 0.65158 | val_0_rmse: 0.82139 | val_1_rmse: 0.81589 |  0:00:44s
epoch 25 | loss: 0.66277 | val_0_rmse: 0.85046 | val_1_rmse: 0.85064 |  0:00:46s
epoch 26 | loss: 0.66954 | val_0_rmse: 0.81733 | val_1_rmse: 0.8146  |  0:00:48s
epoch 27 | loss: 0.66265 | val_0_rmse: 0.81232 | val_1_rmse: 0.81043 |  0:00:50s
epoch 28 | loss: 0.65238 | val_0_rmse: 0.79493 | val_1_rmse: 0.8002  |  0:00:52s
epoch 29 | loss: 0.66049 | val_0_rmse: 0.80606 | val_1_rmse: 0.80082 |  0:00:53s
epoch 30 | loss: 0.6487  | val_0_rmse: 0.79199 | val_1_rmse: 0.78796 |  0:00:55s
epoch 31 | loss: 0.64564 | val_0_rmse: 0.89564 | val_1_rmse: 0.92093 |  0:00:57s
epoch 32 | loss: 0.63867 | val_0_rmse: 0.7957  | val_1_rmse: 0.80822 |  0:00:59s
epoch 33 | loss: 0.62877 | val_0_rmse: 0.84493 | val_1_rmse: 0.87284 |  0:01:01s
epoch 34 | loss: 0.63329 | val_0_rmse: 0.8222  | val_1_rmse: 0.83038 |  0:01:02s
epoch 35 | loss: 0.62917 | val_0_rmse: 0.78665 | val_1_rmse: 0.80516 |  0:01:04s
epoch 36 | loss: 0.67059 | val_0_rmse: 0.80665 | val_1_rmse: 0.8135  |  0:01:06s
epoch 37 | loss: 0.65428 | val_0_rmse: 0.80461 | val_1_rmse: 0.81227 |  0:01:08s
epoch 38 | loss: 0.6342  | val_0_rmse: 0.80849 | val_1_rmse: 0.81695 |  0:01:09s
epoch 39 | loss: 0.63713 | val_0_rmse: 0.77956 | val_1_rmse: 0.80304 |  0:01:11s
epoch 40 | loss: 0.62255 | val_0_rmse: 0.77276 | val_1_rmse: 0.80006 |  0:01:13s
epoch 41 | loss: 0.62979 | val_0_rmse: 0.76732 | val_1_rmse: 0.7839  |  0:01:15s
epoch 42 | loss: 0.61883 | val_0_rmse: 0.76976 | val_1_rmse: 0.79036 |  0:01:17s
epoch 43 | loss: 0.60861 | val_0_rmse: 0.75359 | val_1_rmse: 0.77865 |  0:01:18s
epoch 44 | loss: 0.59352 | val_0_rmse: 0.77858 | val_1_rmse: 0.83526 |  0:01:20s
epoch 45 | loss: 0.59253 | val_0_rmse: 0.76907 | val_1_rmse: 0.80051 |  0:01:22s
epoch 46 | loss: 0.59056 | val_0_rmse: 0.86109 | val_1_rmse: 0.91803 |  0:01:24s
epoch 47 | loss: 0.5805  | val_0_rmse: 0.75223 | val_1_rmse: 0.7714  |  0:01:26s
epoch 48 | loss: 0.58087 | val_0_rmse: 0.76809 | val_1_rmse: 0.78264 |  0:01:27s
epoch 49 | loss: 0.56065 | val_0_rmse: 0.72507 | val_1_rmse: 0.76353 |  0:01:29s
epoch 50 | loss: 0.55802 | val_0_rmse: 0.75173 | val_1_rmse: 0.77458 |  0:01:31s
epoch 51 | loss: 0.56329 | val_0_rmse: 0.79142 | val_1_rmse: 0.85391 |  0:01:33s
epoch 52 | loss: 0.55905 | val_0_rmse: 0.74354 | val_1_rmse: 0.77426 |  0:01:35s
epoch 53 | loss: 0.55368 | val_0_rmse: 0.72229 | val_1_rmse: 0.7735  |  0:01:36s
epoch 54 | loss: 0.54908 | val_0_rmse: 0.72919 | val_1_rmse: 0.7794  |  0:01:38s
epoch 55 | loss: 0.54288 | val_0_rmse: 0.74251 | val_1_rmse: 0.80627 |  0:01:40s
epoch 56 | loss: 0.53601 | val_0_rmse: 0.72948 | val_1_rmse: 0.76908 |  0:01:42s
epoch 57 | loss: 0.53512 | val_0_rmse: 0.71205 | val_1_rmse: 0.77495 |  0:01:44s
epoch 58 | loss: 0.53228 | val_0_rmse: 0.72103 | val_1_rmse: 0.76769 |  0:01:45s
epoch 59 | loss: 0.5333  | val_0_rmse: 0.72015 | val_1_rmse: 0.75761 |  0:01:47s
epoch 60 | loss: 0.52811 | val_0_rmse: 0.73961 | val_1_rmse: 0.80702 |  0:01:49s
epoch 61 | loss: 0.53438 | val_0_rmse: 0.72818 | val_1_rmse: 0.7719  |  0:01:51s
epoch 62 | loss: 0.55073 | val_0_rmse: 0.72885 | val_1_rmse: 0.77843 |  0:01:53s
epoch 63 | loss: 0.54123 | val_0_rmse: 0.73272 | val_1_rmse: 0.78362 |  0:01:54s
epoch 64 | loss: 0.55438 | val_0_rmse: 0.75819 | val_1_rmse: 0.79878 |  0:01:56s
epoch 65 | loss: 0.54009 | val_0_rmse: 0.73375 | val_1_rmse: 0.77461 |  0:01:58s
epoch 66 | loss: 0.54486 | val_0_rmse: 0.72884 | val_1_rmse: 0.78384 |  0:02:00s
epoch 67 | loss: 0.57222 | val_0_rmse: 0.73624 | val_1_rmse: 0.77796 |  0:02:01s
epoch 68 | loss: 0.55623 | val_0_rmse: 0.75567 | val_1_rmse: 0.79068 |  0:02:03s
epoch 69 | loss: 0.54073 | val_0_rmse: 0.74324 | val_1_rmse: 0.77857 |  0:02:05s
epoch 70 | loss: 0.53296 | val_0_rmse: 0.79068 | val_1_rmse: 0.83157 |  0:02:07s
epoch 71 | loss: 0.5385  | val_0_rmse: 0.7391  | val_1_rmse: 0.78808 |  0:02:09s
epoch 72 | loss: 0.53057 | val_0_rmse: 0.72755 | val_1_rmse: 0.80334 |  0:02:10s
epoch 73 | loss: 0.53631 | val_0_rmse: 0.71286 | val_1_rmse: 0.7816  |  0:02:12s
epoch 74 | loss: 0.52455 | val_0_rmse: 0.71422 | val_1_rmse: 0.7675  |  0:02:14s
epoch 75 | loss: 0.5288  | val_0_rmse: 0.78961 | val_1_rmse: 0.86596 |  0:02:16s
epoch 76 | loss: 0.53074 | val_0_rmse: 0.75833 | val_1_rmse: 0.80118 |  0:02:18s
epoch 77 | loss: 0.53715 | val_0_rmse: 0.70646 | val_1_rmse: 0.77294 |  0:02:19s
epoch 78 | loss: 0.51608 | val_0_rmse: 0.7007  | val_1_rmse: 0.76957 |  0:02:21s
epoch 79 | loss: 0.51839 | val_0_rmse: 0.69547 | val_1_rmse: 0.76808 |  0:02:23s
epoch 80 | loss: 0.50641 | val_0_rmse: 0.69397 | val_1_rmse: 0.76238 |  0:02:25s
epoch 81 | loss: 0.50153 | val_0_rmse: 0.69375 | val_1_rmse: 0.76075 |  0:02:27s
epoch 82 | loss: 0.49524 | val_0_rmse: 0.68792 | val_1_rmse: 0.76925 |  0:02:28s
epoch 83 | loss: 0.49483 | val_0_rmse: 0.73092 | val_1_rmse: 0.82079 |  0:02:30s
epoch 84 | loss: 0.49451 | val_0_rmse: 0.68546 | val_1_rmse: 0.75508 |  0:02:32s
epoch 85 | loss: 0.48654 | val_0_rmse: 0.67858 | val_1_rmse: 0.76737 |  0:02:34s
epoch 86 | loss: 0.48697 | val_0_rmse: 0.6783  | val_1_rmse: 0.75788 |  0:02:36s
epoch 87 | loss: 0.48923 | val_0_rmse: 0.68362 | val_1_rmse: 0.76229 |  0:02:37s
epoch 88 | loss: 0.48001 | val_0_rmse: 0.72322 | val_1_rmse: 0.81268 |  0:02:39s
epoch 89 | loss: 0.49626 | val_0_rmse: 0.71662 | val_1_rmse: 0.77127 |  0:02:41s
epoch 90 | loss: 0.48425 | val_0_rmse: 0.68959 | val_1_rmse: 0.75768 |  0:02:43s
epoch 91 | loss: 0.48004 | val_0_rmse: 0.68556 | val_1_rmse: 0.7674  |  0:02:44s
epoch 92 | loss: 0.47042 | val_0_rmse: 0.6639  | val_1_rmse: 0.76715 |  0:02:46s
epoch 93 | loss: 0.47725 | val_0_rmse: 0.6896  | val_1_rmse: 0.77074 |  0:02:48s
epoch 94 | loss: 0.48377 | val_0_rmse: 0.6849  | val_1_rmse: 0.78817 |  0:02:50s
epoch 95 | loss: 0.47945 | val_0_rmse: 0.66636 | val_1_rmse: 0.76936 |  0:02:52s
epoch 96 | loss: 0.47549 | val_0_rmse: 0.67188 | val_1_rmse: 0.78882 |  0:02:53s
epoch 97 | loss: 0.4858  | val_0_rmse: 0.68899 | val_1_rmse: 0.7747  |  0:02:55s
epoch 98 | loss: 0.48488 | val_0_rmse: 0.73566 | val_1_rmse: 0.89515 |  0:02:57s
epoch 99 | loss: 0.46962 | val_0_rmse: 0.67362 | val_1_rmse: 0.81117 |  0:02:59s
epoch 100| loss: 0.46845 | val_0_rmse: 0.66516 | val_1_rmse: 0.77069 |  0:03:01s
epoch 101| loss: 0.47122 | val_0_rmse: 0.66565 | val_1_rmse: 0.77036 |  0:03:02s
epoch 102| loss: 0.46963 | val_0_rmse: 0.66606 | val_1_rmse: 0.79047 |  0:03:04s
epoch 103| loss: 0.46601 | val_0_rmse: 0.655   | val_1_rmse: 0.77326 |  0:03:06s
epoch 104| loss: 0.46663 | val_0_rmse: 0.67739 | val_1_rmse: 0.79691 |  0:03:08s
epoch 105| loss: 0.46725 | val_0_rmse: 0.66568 | val_1_rmse: 0.77743 |  0:03:09s
epoch 106| loss: 0.45964 | val_0_rmse: 0.6578  | val_1_rmse: 0.7845  |  0:03:11s
epoch 107| loss: 0.44994 | val_0_rmse: 0.66696 | val_1_rmse: 0.77011 |  0:03:13s
epoch 108| loss: 0.45377 | val_0_rmse: 0.65189 | val_1_rmse: 0.77765 |  0:03:15s
epoch 109| loss: 0.4541  | val_0_rmse: 0.66577 | val_1_rmse: 0.78303 |  0:03:17s
epoch 110| loss: 0.45485 | val_0_rmse: 0.66604 | val_1_rmse: 0.78235 |  0:03:18s
epoch 111| loss: 0.45308 | val_0_rmse: 0.64948 | val_1_rmse: 0.78015 |  0:03:20s
epoch 112| loss: 0.44873 | val_0_rmse: 0.64259 | val_1_rmse: 0.78095 |  0:03:22s
epoch 113| loss: 0.44528 | val_0_rmse: 0.65077 | val_1_rmse: 0.7687  |  0:03:24s
epoch 114| loss: 0.45201 | val_0_rmse: 0.65223 | val_1_rmse: 0.81463 |  0:03:26s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.75508
Best weights from best epoch are automatically used!
ended training at: 03:27:54
Feature importance:
Mean squared error is of 0.04020639562054324
Mean absolute error:0.13327090608920034
MAPE:0.141064882432528
R2 score:0.3820550929673614
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:27:55
epoch 0  | loss: 1.3808  | val_0_rmse: 0.99119 | val_1_rmse: 1.01023 |  0:00:01s
epoch 1  | loss: 0.98866 | val_0_rmse: 0.98912 | val_1_rmse: 1.00814 |  0:00:03s
epoch 2  | loss: 0.97971 | val_0_rmse: 0.98312 | val_1_rmse: 1.00341 |  0:00:05s
epoch 3  | loss: 0.97246 | val_0_rmse: 0.97392 | val_1_rmse: 0.99548 |  0:00:07s
epoch 4  | loss: 0.96189 | val_0_rmse: 0.97534 | val_1_rmse: 0.99819 |  0:00:08s
epoch 5  | loss: 0.9484  | val_0_rmse: 0.96067 | val_1_rmse: 0.98598 |  0:00:10s
epoch 6  | loss: 0.92321 | val_0_rmse: 0.92162 | val_1_rmse: 0.9488  |  0:00:12s
epoch 7  | loss: 0.85255 | val_0_rmse: 0.89927 | val_1_rmse: 0.92619 |  0:00:14s
epoch 8  | loss: 0.7887  | val_0_rmse: 0.88925 | val_1_rmse: 0.91458 |  0:00:16s
epoch 9  | loss: 0.74673 | val_0_rmse: 0.86717 | val_1_rmse: 0.89225 |  0:00:17s
epoch 10 | loss: 0.72059 | val_0_rmse: 0.85907 | val_1_rmse: 0.8862  |  0:00:19s
epoch 11 | loss: 0.68636 | val_0_rmse: 0.84484 | val_1_rmse: 0.87439 |  0:00:21s
epoch 12 | loss: 0.6688  | val_0_rmse: 0.83842 | val_1_rmse: 0.86525 |  0:00:23s
epoch 13 | loss: 0.65775 | val_0_rmse: 0.82908 | val_1_rmse: 0.86636 |  0:00:25s
epoch 14 | loss: 0.6281  | val_0_rmse: 0.8187  | val_1_rmse: 0.8495  |  0:00:26s
epoch 15 | loss: 0.61302 | val_0_rmse: 0.85075 | val_1_rmse: 0.88864 |  0:00:28s
epoch 16 | loss: 0.60142 | val_0_rmse: 0.80789 | val_1_rmse: 0.84578 |  0:00:30s
epoch 17 | loss: 0.58868 | val_0_rmse: 0.83801 | val_1_rmse: 0.86751 |  0:00:32s
epoch 18 | loss: 0.5754  | val_0_rmse: 0.79002 | val_1_rmse: 0.83265 |  0:00:34s
epoch 19 | loss: 0.56875 | val_0_rmse: 0.81843 | val_1_rmse: 0.85592 |  0:00:35s
epoch 20 | loss: 0.56782 | val_0_rmse: 0.79068 | val_1_rmse: 0.83482 |  0:00:37s
epoch 21 | loss: 0.55151 | val_0_rmse: 0.76637 | val_1_rmse: 0.81454 |  0:00:39s
epoch 22 | loss: 0.56039 | val_0_rmse: 0.78449 | val_1_rmse: 0.83258 |  0:00:41s
epoch 23 | loss: 0.56382 | val_0_rmse: 0.76652 | val_1_rmse: 0.81399 |  0:00:42s
epoch 24 | loss: 0.55278 | val_0_rmse: 0.74566 | val_1_rmse: 0.80038 |  0:00:44s
epoch 25 | loss: 0.5391  | val_0_rmse: 0.74677 | val_1_rmse: 0.80748 |  0:00:46s
epoch 26 | loss: 0.52678 | val_0_rmse: 0.76865 | val_1_rmse: 0.83192 |  0:00:48s
epoch 27 | loss: 0.52769 | val_0_rmse: 0.71888 | val_1_rmse: 0.78616 |  0:00:50s
epoch 28 | loss: 0.51445 | val_0_rmse: 0.72178 | val_1_rmse: 0.78854 |  0:00:51s
epoch 29 | loss: 0.50832 | val_0_rmse: 0.72199 | val_1_rmse: 0.79961 |  0:00:53s
epoch 30 | loss: 0.50482 | val_0_rmse: 0.72223 | val_1_rmse: 0.80048 |  0:00:55s
epoch 31 | loss: 0.52092 | val_0_rmse: 0.7136  | val_1_rmse: 0.82184 |  0:00:57s
epoch 32 | loss: 0.50552 | val_0_rmse: 0.68757 | val_1_rmse: 0.79208 |  0:00:59s
epoch 33 | loss: 0.49867 | val_0_rmse: 0.71049 | val_1_rmse: 0.80544 |  0:01:00s
epoch 34 | loss: 0.49144 | val_0_rmse: 0.68228 | val_1_rmse: 0.79293 |  0:01:02s
epoch 35 | loss: 0.49683 | val_0_rmse: 0.70293 | val_1_rmse: 0.80258 |  0:01:04s
epoch 36 | loss: 0.48452 | val_0_rmse: 0.71323 | val_1_rmse: 0.82171 |  0:01:06s
epoch 37 | loss: 0.48991 | val_0_rmse: 0.67036 | val_1_rmse: 0.79581 |  0:01:08s
epoch 38 | loss: 0.48267 | val_0_rmse: 0.71266 | val_1_rmse: 0.81995 |  0:01:09s
epoch 39 | loss: 0.49038 | val_0_rmse: 0.69414 | val_1_rmse: 0.81346 |  0:01:11s
epoch 40 | loss: 0.48383 | val_0_rmse: 0.70692 | val_1_rmse: 0.85499 |  0:01:13s
epoch 41 | loss: 0.48887 | val_0_rmse: 0.66006 | val_1_rmse: 0.79534 |  0:01:15s
epoch 42 | loss: 0.47495 | val_0_rmse: 0.67103 | val_1_rmse: 0.79912 |  0:01:17s
epoch 43 | loss: 0.47553 | val_0_rmse: 0.66059 | val_1_rmse: 0.79381 |  0:01:18s
epoch 44 | loss: 0.46662 | val_0_rmse: 0.66287 | val_1_rmse: 0.82439 |  0:01:20s
epoch 45 | loss: 0.45896 | val_0_rmse: 0.64807 | val_1_rmse: 0.80908 |  0:01:22s
epoch 46 | loss: 0.46492 | val_0_rmse: 0.65829 | val_1_rmse: 0.8098  |  0:01:24s
epoch 47 | loss: 0.46004 | val_0_rmse: 0.70099 | val_1_rmse: 0.82347 |  0:01:26s
epoch 48 | loss: 0.45874 | val_0_rmse: 0.69114 | val_1_rmse: 0.81881 |  0:01:27s
epoch 49 | loss: 0.44779 | val_0_rmse: 0.65682 | val_1_rmse: 0.82668 |  0:01:29s
epoch 50 | loss: 0.44019 | val_0_rmse: 0.65333 | val_1_rmse: 0.80894 |  0:01:31s
epoch 51 | loss: 0.44027 | val_0_rmse: 0.66229 | val_1_rmse: 0.83202 |  0:01:33s
epoch 52 | loss: 0.4377  | val_0_rmse: 0.67837 | val_1_rmse: 0.87798 |  0:01:34s
epoch 53 | loss: 0.44176 | val_0_rmse: 0.66056 | val_1_rmse: 0.84828 |  0:01:36s
epoch 54 | loss: 0.44104 | val_0_rmse: 0.64637 | val_1_rmse: 0.81682 |  0:01:38s
epoch 55 | loss: 0.44199 | val_0_rmse: 0.64242 | val_1_rmse: 0.82908 |  0:01:40s
epoch 56 | loss: 0.43583 | val_0_rmse: 0.68021 | val_1_rmse: 0.86501 |  0:01:42s
epoch 57 | loss: 0.43502 | val_0_rmse: 0.64361 | val_1_rmse: 0.84188 |  0:01:43s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.78616
Best weights from best epoch are automatically used!
ended training at: 03:29:39
Feature importance:
Mean squared error is of 0.043578205445006776
Mean absolute error:0.1343922084549246
MAPE:0.14790635366036362
R2 score:0.3890184256085094
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:29:40
epoch 0  | loss: 1.3675  | val_0_rmse: 0.97832 | val_1_rmse: 0.97421 |  0:00:01s
epoch 1  | loss: 0.87418 | val_0_rmse: 0.88278 | val_1_rmse: 0.88202 |  0:00:03s
epoch 2  | loss: 0.81889 | val_0_rmse: 0.88701 | val_1_rmse: 0.88854 |  0:00:05s
epoch 3  | loss: 0.79722 | val_0_rmse: 0.89055 | val_1_rmse: 0.89729 |  0:00:07s
epoch 4  | loss: 0.78784 | val_0_rmse: 0.87714 | val_1_rmse: 0.87886 |  0:00:09s
epoch 5  | loss: 0.77388 | val_0_rmse: 0.87266 | val_1_rmse: 0.86964 |  0:00:10s
epoch 6  | loss: 0.75759 | val_0_rmse: 0.86289 | val_1_rmse: 0.85425 |  0:00:12s
epoch 7  | loss: 0.74321 | val_0_rmse: 0.86305 | val_1_rmse: 0.84609 |  0:00:14s
epoch 8  | loss: 0.73929 | val_0_rmse: 0.85476 | val_1_rmse: 0.84101 |  0:00:16s
epoch 9  | loss: 0.73492 | val_0_rmse: 0.86163 | val_1_rmse: 0.85407 |  0:00:18s
epoch 10 | loss: 0.72039 | val_0_rmse: 0.85529 | val_1_rmse: 0.8537  |  0:00:19s
epoch 11 | loss: 0.71682 | val_0_rmse: 0.85455 | val_1_rmse: 0.85299 |  0:00:21s
epoch 12 | loss: 0.69885 | val_0_rmse: 0.8416  | val_1_rmse: 0.83725 |  0:00:23s
epoch 13 | loss: 0.68728 | val_0_rmse: 0.84642 | val_1_rmse: 0.84718 |  0:00:25s
epoch 14 | loss: 0.68456 | val_0_rmse: 0.83949 | val_1_rmse: 0.84005 |  0:00:26s
epoch 15 | loss: 0.67767 | val_0_rmse: 0.84226 | val_1_rmse: 0.83828 |  0:00:28s
epoch 16 | loss: 0.6747  | val_0_rmse: 0.8354  | val_1_rmse: 0.83676 |  0:00:30s
epoch 17 | loss: 0.70415 | val_0_rmse: 0.84019 | val_1_rmse: 0.8358  |  0:00:32s
epoch 18 | loss: 0.7065  | val_0_rmse: 0.83892 | val_1_rmse: 0.83579 |  0:00:34s
epoch 19 | loss: 0.69259 | val_0_rmse: 0.83351 | val_1_rmse: 0.82887 |  0:00:36s
epoch 20 | loss: 0.69555 | val_0_rmse: 0.84113 | val_1_rmse: 0.83884 |  0:00:37s
epoch 21 | loss: 0.68978 | val_0_rmse: 0.82406 | val_1_rmse: 0.82681 |  0:00:39s
epoch 22 | loss: 0.6964  | val_0_rmse: 0.84254 | val_1_rmse: 0.84436 |  0:00:41s
epoch 23 | loss: 0.67355 | val_0_rmse: 0.81806 | val_1_rmse: 0.82073 |  0:00:43s
epoch 24 | loss: 0.67181 | val_0_rmse: 0.83277 | val_1_rmse: 0.8342  |  0:00:44s
epoch 25 | loss: 0.67777 | val_0_rmse: 0.82927 | val_1_rmse: 0.83006 |  0:00:46s
epoch 26 | loss: 0.67119 | val_0_rmse: 0.80954 | val_1_rmse: 0.82003 |  0:00:48s
epoch 27 | loss: 0.65946 | val_0_rmse: 0.80356 | val_1_rmse: 0.81348 |  0:00:50s
epoch 28 | loss: 0.66244 | val_0_rmse: 0.80374 | val_1_rmse: 0.81157 |  0:00:52s
epoch 29 | loss: 0.65462 | val_0_rmse: 0.81067 | val_1_rmse: 0.81603 |  0:00:54s
epoch 30 | loss: 0.65762 | val_0_rmse: 0.79715 | val_1_rmse: 0.80953 |  0:00:55s
epoch 31 | loss: 0.64957 | val_0_rmse: 0.79051 | val_1_rmse: 0.8052  |  0:00:57s
epoch 32 | loss: 0.64444 | val_0_rmse: 0.80636 | val_1_rmse: 0.82298 |  0:00:59s
epoch 33 | loss: 0.6407  | val_0_rmse: 0.79096 | val_1_rmse: 0.80928 |  0:01:01s
epoch 34 | loss: 0.64133 | val_0_rmse: 0.78699 | val_1_rmse: 0.80584 |  0:01:03s
epoch 35 | loss: 0.62991 | val_0_rmse: 0.78027 | val_1_rmse: 0.79852 |  0:01:04s
epoch 36 | loss: 0.62742 | val_0_rmse: 0.78161 | val_1_rmse: 0.7944  |  0:01:06s
epoch 37 | loss: 0.62599 | val_0_rmse: 0.78356 | val_1_rmse: 0.79866 |  0:01:08s
epoch 38 | loss: 0.6314  | val_0_rmse: 0.79389 | val_1_rmse: 0.81895 |  0:01:10s
epoch 39 | loss: 0.62462 | val_0_rmse: 0.78253 | val_1_rmse: 0.79513 |  0:01:12s
epoch 40 | loss: 0.62125 | val_0_rmse: 0.78941 | val_1_rmse: 0.80472 |  0:01:13s
epoch 41 | loss: 0.62049 | val_0_rmse: 0.78197 | val_1_rmse: 0.79561 |  0:01:15s
epoch 42 | loss: 0.62316 | val_0_rmse: 0.78605 | val_1_rmse: 0.8017  |  0:01:17s
epoch 43 | loss: 0.63194 | val_0_rmse: 0.78889 | val_1_rmse: 0.79475 |  0:01:19s
epoch 44 | loss: 0.63363 | val_0_rmse: 0.78714 | val_1_rmse: 0.81168 |  0:01:20s
epoch 45 | loss: 0.62798 | val_0_rmse: 0.78193 | val_1_rmse: 0.80752 |  0:01:22s
epoch 46 | loss: 0.63739 | val_0_rmse: 0.78406 | val_1_rmse: 0.79545 |  0:01:24s
epoch 47 | loss: 0.62006 | val_0_rmse: 0.7762  | val_1_rmse: 0.79677 |  0:01:26s
epoch 48 | loss: 0.60685 | val_0_rmse: 0.76295 | val_1_rmse: 0.78597 |  0:01:28s
epoch 49 | loss: 0.60004 | val_0_rmse: 0.7656  | val_1_rmse: 0.7855  |  0:01:29s
epoch 50 | loss: 0.60731 | val_0_rmse: 0.77177 | val_1_rmse: 0.78654 |  0:01:31s
epoch 51 | loss: 0.60081 | val_0_rmse: 0.75948 | val_1_rmse: 0.78886 |  0:01:33s
epoch 52 | loss: 0.61295 | val_0_rmse: 0.7821  | val_1_rmse: 0.79683 |  0:01:35s
epoch 53 | loss: 0.59747 | val_0_rmse: 0.75655 | val_1_rmse: 0.77938 |  0:01:37s
epoch 54 | loss: 0.58879 | val_0_rmse: 0.75424 | val_1_rmse: 0.78538 |  0:01:39s
epoch 55 | loss: 0.59    | val_0_rmse: 0.75312 | val_1_rmse: 0.78423 |  0:01:40s
epoch 56 | loss: 0.58813 | val_0_rmse: 0.7502  | val_1_rmse: 0.77724 |  0:01:42s
epoch 57 | loss: 0.58167 | val_0_rmse: 0.7609  | val_1_rmse: 0.79319 |  0:01:44s
epoch 58 | loss: 0.578   | val_0_rmse: 0.74564 | val_1_rmse: 0.77488 |  0:01:46s
epoch 59 | loss: 0.5751  | val_0_rmse: 0.76203 | val_1_rmse: 0.79097 |  0:01:48s
epoch 60 | loss: 0.58371 | val_0_rmse: 0.75393 | val_1_rmse: 0.77584 |  0:01:49s
epoch 61 | loss: 0.58531 | val_0_rmse: 0.75503 | val_1_rmse: 0.78618 |  0:01:51s
epoch 62 | loss: 0.57483 | val_0_rmse: 0.76864 | val_1_rmse: 0.78014 |  0:01:53s
epoch 63 | loss: 0.57495 | val_0_rmse: 0.74338 | val_1_rmse: 0.77166 |  0:01:55s
epoch 64 | loss: 0.5817  | val_0_rmse: 0.76004 | val_1_rmse: 0.79297 |  0:01:56s
epoch 65 | loss: 0.58192 | val_0_rmse: 0.74283 | val_1_rmse: 0.77516 |  0:01:58s
epoch 66 | loss: 0.56617 | val_0_rmse: 0.78933 | val_1_rmse: 0.79201 |  0:02:00s
epoch 67 | loss: 0.55729 | val_0_rmse: 0.74052 | val_1_rmse: 0.77085 |  0:02:02s
epoch 68 | loss: 0.5678  | val_0_rmse: 0.73525 | val_1_rmse: 0.77536 |  0:02:04s
epoch 69 | loss: 0.57261 | val_0_rmse: 0.75163 | val_1_rmse: 0.78537 |  0:02:05s
epoch 70 | loss: 0.55326 | val_0_rmse: 0.73971 | val_1_rmse: 0.77929 |  0:02:07s
epoch 71 | loss: 0.55912 | val_0_rmse: 0.76265 | val_1_rmse: 0.80069 |  0:02:09s
epoch 72 | loss: 0.55689 | val_0_rmse: 0.73854 | val_1_rmse: 0.76702 |  0:02:11s
epoch 73 | loss: 0.54567 | val_0_rmse: 0.73182 | val_1_rmse: 0.77097 |  0:02:13s
epoch 74 | loss: 0.5507  | val_0_rmse: 0.72364 | val_1_rmse: 0.77058 |  0:02:14s
epoch 75 | loss: 0.55164 | val_0_rmse: 0.72116 | val_1_rmse: 0.77145 |  0:02:16s
epoch 76 | loss: 0.55077 | val_0_rmse: 0.72993 | val_1_rmse: 0.77207 |  0:02:18s
epoch 77 | loss: 0.54547 | val_0_rmse: 0.72091 | val_1_rmse: 0.77035 |  0:02:20s
epoch 78 | loss: 0.53943 | val_0_rmse: 0.72053 | val_1_rmse: 0.76808 |  0:02:22s
epoch 79 | loss: 0.54362 | val_0_rmse: 0.72374 | val_1_rmse: 0.76823 |  0:02:23s
epoch 80 | loss: 0.54291 | val_0_rmse: 0.72098 | val_1_rmse: 0.76723 |  0:02:25s
epoch 81 | loss: 0.54571 | val_0_rmse: 0.71525 | val_1_rmse: 0.76228 |  0:02:27s
epoch 82 | loss: 0.53487 | val_0_rmse: 0.71285 | val_1_rmse: 0.76698 |  0:02:29s
epoch 83 | loss: 0.53359 | val_0_rmse: 0.71009 | val_1_rmse: 0.76155 |  0:02:31s
epoch 84 | loss: 0.52886 | val_0_rmse: 0.7135  | val_1_rmse: 0.77026 |  0:02:32s
epoch 85 | loss: 0.55383 | val_0_rmse: 0.72975 | val_1_rmse: 0.77897 |  0:02:34s
epoch 86 | loss: 0.55352 | val_0_rmse: 0.7333  | val_1_rmse: 0.77297 |  0:02:36s
epoch 87 | loss: 0.5308  | val_0_rmse: 0.73264 | val_1_rmse: 0.78497 |  0:02:38s
epoch 88 | loss: 0.53053 | val_0_rmse: 0.71188 | val_1_rmse: 0.77271 |  0:02:40s
epoch 89 | loss: 0.52745 | val_0_rmse: 0.70773 | val_1_rmse: 0.77522 |  0:02:41s
epoch 90 | loss: 0.52324 | val_0_rmse: 0.71141 | val_1_rmse: 0.76742 |  0:02:43s
epoch 91 | loss: 0.51716 | val_0_rmse: 0.74803 | val_1_rmse: 0.79321 |  0:02:45s
epoch 92 | loss: 0.52366 | val_0_rmse: 0.71155 | val_1_rmse: 0.76621 |  0:02:47s
epoch 93 | loss: 0.51634 | val_0_rmse: 0.70234 | val_1_rmse: 0.76325 |  0:02:49s
epoch 94 | loss: 0.51273 | val_0_rmse: 0.69676 | val_1_rmse: 0.76502 |  0:02:50s
epoch 95 | loss: 0.50874 | val_0_rmse: 0.70137 | val_1_rmse: 0.76035 |  0:02:52s
epoch 96 | loss: 0.5139  | val_0_rmse: 0.69702 | val_1_rmse: 0.76272 |  0:02:54s
epoch 97 | loss: 0.50621 | val_0_rmse: 0.70116 | val_1_rmse: 0.76664 |  0:02:56s
epoch 98 | loss: 0.51637 | val_0_rmse: 0.70568 | val_1_rmse: 0.77954 |  0:02:58s
epoch 99 | loss: 0.5163  | val_0_rmse: 0.69415 | val_1_rmse: 0.77093 |  0:02:59s
epoch 100| loss: 0.50289 | val_0_rmse: 0.69539 | val_1_rmse: 0.76139 |  0:03:01s
epoch 101| loss: 0.50941 | val_0_rmse: 0.71229 | val_1_rmse: 0.78217 |  0:03:03s
epoch 102| loss: 0.51626 | val_0_rmse: 0.69152 | val_1_rmse: 0.76226 |  0:03:05s
epoch 103| loss: 0.50959 | val_0_rmse: 0.69657 | val_1_rmse: 0.77428 |  0:03:07s
epoch 104| loss: 0.51072 | val_0_rmse: 0.70344 | val_1_rmse: 0.77659 |  0:03:08s
epoch 105| loss: 0.50428 | val_0_rmse: 0.70441 | val_1_rmse: 0.77679 |  0:03:10s
epoch 106| loss: 0.50044 | val_0_rmse: 0.71243 | val_1_rmse: 0.77853 |  0:03:12s
epoch 107| loss: 0.49492 | val_0_rmse: 0.68541 | val_1_rmse: 0.77431 |  0:03:14s
epoch 108| loss: 0.4946  | val_0_rmse: 0.68456 | val_1_rmse: 0.7701  |  0:03:16s
epoch 109| loss: 0.4956  | val_0_rmse: 0.68067 | val_1_rmse: 0.76979 |  0:03:17s
epoch 110| loss: 0.4911  | val_0_rmse: 0.68514 | val_1_rmse: 0.77449 |  0:03:19s
epoch 111| loss: 0.49048 | val_0_rmse: 0.69135 | val_1_rmse: 0.78456 |  0:03:21s
epoch 112| loss: 0.49145 | val_0_rmse: 0.68602 | val_1_rmse: 0.77513 |  0:03:23s
epoch 113| loss: 0.48454 | val_0_rmse: 0.77872 | val_1_rmse: 0.89679 |  0:03:25s
epoch 114| loss: 0.48502 | val_0_rmse: 0.67949 | val_1_rmse: 0.76181 |  0:03:26s
epoch 115| loss: 0.48524 | val_0_rmse: 0.69953 | val_1_rmse: 0.77001 |  0:03:28s
epoch 116| loss: 0.48571 | val_0_rmse: 0.67753 | val_1_rmse: 0.76924 |  0:03:30s
epoch 117| loss: 0.48249 | val_0_rmse: 0.67569 | val_1_rmse: 0.77504 |  0:03:32s
epoch 118| loss: 0.47893 | val_0_rmse: 0.68197 | val_1_rmse: 0.76601 |  0:03:34s
epoch 119| loss: 0.50515 | val_0_rmse: 0.703   | val_1_rmse: 0.78522 |  0:03:35s
epoch 120| loss: 0.50166 | val_0_rmse: 0.68824 | val_1_rmse: 0.77709 |  0:03:37s
epoch 121| loss: 0.49132 | val_0_rmse: 0.69105 | val_1_rmse: 0.77609 |  0:03:39s
epoch 122| loss: 0.485   | val_0_rmse: 0.68081 | val_1_rmse: 0.77691 |  0:03:41s
epoch 123| loss: 0.47993 | val_0_rmse: 0.68271 | val_1_rmse: 0.79349 |  0:03:43s
epoch 124| loss: 0.48571 | val_0_rmse: 0.67996 | val_1_rmse: 0.77542 |  0:03:44s
epoch 125| loss: 0.48222 | val_0_rmse: 0.68808 | val_1_rmse: 0.78617 |  0:03:46s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.76035
Best weights from best epoch are automatically used!
ended training at: 03:33:27
Feature importance:
Mean squared error is of 0.047629486893601004
Mean absolute error:0.13418633711542233
MAPE:0.14707476601502156
R2 score:0.394443454049047
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:34:31
epoch 0  | loss: 1.15498 | val_0_rmse: 1.00449 | val_1_rmse: 0.98707 |  0:00:06s
epoch 1  | loss: 1.00388 | val_0_rmse: 0.98834 | val_1_rmse: 0.97232 |  0:00:13s
epoch 2  | loss: 0.96056 | val_0_rmse: 0.94791 | val_1_rmse: 0.93272 |  0:00:20s
epoch 3  | loss: 0.89062 | val_0_rmse: 0.9324  | val_1_rmse: 0.91678 |  0:00:27s
epoch 4  | loss: 0.84625 | val_0_rmse: 0.93783 | val_1_rmse: 0.92411 |  0:00:34s
epoch 5  | loss: 0.82032 | val_0_rmse: 0.91244 | val_1_rmse: 0.90028 |  0:00:41s
epoch 6  | loss: 0.80182 | val_0_rmse: 0.90742 | val_1_rmse: 0.90048 |  0:00:48s
epoch 7  | loss: 0.7861  | val_0_rmse: 0.89607 | val_1_rmse: 0.89069 |  0:00:54s
epoch 8  | loss: 0.7693  | val_0_rmse: 0.88875 | val_1_rmse: 0.89275 |  0:01:01s
epoch 9  | loss: 0.75937 | val_0_rmse: 0.8741  | val_1_rmse: 0.88253 |  0:01:08s
epoch 10 | loss: 0.74935 | val_0_rmse: 0.86279 | val_1_rmse: 0.8801  |  0:01:15s
epoch 11 | loss: 0.74016 | val_0_rmse: 0.85248 | val_1_rmse: 0.87302 |  0:01:22s
epoch 12 | loss: 0.73045 | val_0_rmse: 0.83999 | val_1_rmse: 0.87186 |  0:01:29s
epoch 13 | loss: 0.72297 | val_0_rmse: 0.83669 | val_1_rmse: 0.87909 |  0:01:35s
epoch 14 | loss: 0.71481 | val_0_rmse: 0.83583 | val_1_rmse: 0.86907 |  0:01:42s
epoch 15 | loss: 0.70922 | val_0_rmse: 0.83912 | val_1_rmse: 0.87945 |  0:01:49s
epoch 16 | loss: 0.70474 | val_0_rmse: 0.82375 | val_1_rmse: 0.87395 |  0:01:56s
epoch 17 | loss: 0.6974  | val_0_rmse: 0.82319 | val_1_rmse: 0.88085 |  0:02:03s
epoch 18 | loss: 0.68986 | val_0_rmse: 0.81638 | val_1_rmse: 0.87764 |  0:02:10s
epoch 19 | loss: 0.68733 | val_0_rmse: 0.81682 | val_1_rmse: 0.87314 |  0:02:16s
epoch 20 | loss: 0.67811 | val_0_rmse: 0.82708 | val_1_rmse: 0.88884 |  0:02:23s
epoch 21 | loss: 0.67448 | val_0_rmse: 0.8164  | val_1_rmse: 0.87815 |  0:02:30s
epoch 22 | loss: 0.67181 | val_0_rmse: 0.81249 | val_1_rmse: 0.88273 |  0:02:37s
epoch 23 | loss: 0.6642  | val_0_rmse: 0.81952 | val_1_rmse: 0.87888 |  0:02:44s
epoch 24 | loss: 0.6635  | val_0_rmse: 0.81522 | val_1_rmse: 0.88437 |  0:02:51s
epoch 25 | loss: 0.6554  | val_0_rmse: 0.8074  | val_1_rmse: 0.88326 |  0:02:57s
epoch 26 | loss: 0.65296 | val_0_rmse: 0.80505 | val_1_rmse: 0.88249 |  0:03:04s
epoch 27 | loss: 0.64497 | val_0_rmse: 0.80205 | val_1_rmse: 0.88716 |  0:03:11s
epoch 28 | loss: 0.64296 | val_0_rmse: 0.81211 | val_1_rmse: 0.89327 |  0:03:18s
epoch 29 | loss: 0.64051 | val_0_rmse: 0.84005 | val_1_rmse: 0.88796 |  0:03:25s
epoch 30 | loss: 0.64192 | val_0_rmse: 0.80021 | val_1_rmse: 0.88301 |  0:03:32s
epoch 31 | loss: 0.64096 | val_0_rmse: 0.79077 | val_1_rmse: 0.89192 |  0:03:38s
epoch 32 | loss: 0.63032 | val_0_rmse: 0.793   | val_1_rmse: 0.88655 |  0:03:45s
epoch 33 | loss: 0.62551 | val_0_rmse: 0.78509 | val_1_rmse: 0.88676 |  0:03:52s
epoch 34 | loss: 0.6179  | val_0_rmse: 0.77988 | val_1_rmse: 0.88949 |  0:03:59s
epoch 35 | loss: 0.61666 | val_0_rmse: 0.77733 | val_1_rmse: 0.89077 |  0:04:06s
epoch 36 | loss: 0.61185 | val_0_rmse: 0.78339 | val_1_rmse: 0.89679 |  0:04:12s
epoch 37 | loss: 0.61159 | val_0_rmse: 0.78728 | val_1_rmse: 0.89529 |  0:04:19s
epoch 38 | loss: 0.60563 | val_0_rmse: 0.78058 | val_1_rmse: 0.89677 |  0:04:26s
epoch 39 | loss: 0.59911 | val_0_rmse: 0.78411 | val_1_rmse: 0.90963 |  0:04:33s
epoch 40 | loss: 0.60188 | val_0_rmse: 0.78799 | val_1_rmse: 0.90812 |  0:04:40s
epoch 41 | loss: 0.64    | val_0_rmse: 0.79405 | val_1_rmse: 0.89143 |  0:04:46s
epoch 42 | loss: 0.6159  | val_0_rmse: 0.83145 | val_1_rmse: 0.95861 |  0:04:53s
epoch 43 | loss: 0.6066  | val_0_rmse: 0.7787  | val_1_rmse: 0.91569 |  0:05:00s
epoch 44 | loss: 0.59599 | val_0_rmse: 0.7793  | val_1_rmse: 0.90059 |  0:05:07s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.86907
Best weights from best epoch are automatically used!
ended training at: 03:39:42
Feature importance:
Mean squared error is of 0.06581527160916177
Mean absolute error:0.18316244320815034
MAPE:0.19661025618856814
R2 score:0.1885584833728685
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:39:44
epoch 0  | loss: 1.20523 | val_0_rmse: 0.99607 | val_1_rmse: 1.01601 |  0:00:06s
epoch 1  | loss: 0.98847 | val_0_rmse: 0.99229 | val_1_rmse: 1.01221 |  0:00:13s
epoch 2  | loss: 0.98254 | val_0_rmse: 0.97159 | val_1_rmse: 0.99151 |  0:00:20s
epoch 3  | loss: 0.95308 | val_0_rmse: 0.96338 | val_1_rmse: 0.98184 |  0:00:27s
epoch 4  | loss: 0.9099  | val_0_rmse: 0.94158 | val_1_rmse: 0.95943 |  0:00:34s
epoch 5  | loss: 0.86597 | val_0_rmse: 0.92334 | val_1_rmse: 0.94345 |  0:00:40s
epoch 6  | loss: 0.83544 | val_0_rmse: 0.91233 | val_1_rmse: 0.93739 |  0:00:47s
epoch 7  | loss: 0.83193 | val_0_rmse: 0.90806 | val_1_rmse: 0.93242 |  0:00:54s
epoch 8  | loss: 0.81307 | val_0_rmse: 0.89791 | val_1_rmse: 0.92548 |  0:01:01s
epoch 9  | loss: 0.79541 | val_0_rmse: 0.8966  | val_1_rmse: 0.92107 |  0:01:08s
epoch 10 | loss: 0.78226 | val_0_rmse: 0.87971 | val_1_rmse: 0.91745 |  0:01:15s
epoch 11 | loss: 0.76737 | val_0_rmse: 0.86299 | val_1_rmse: 0.90669 |  0:01:22s
epoch 12 | loss: 0.76465 | val_0_rmse: 0.86595 | val_1_rmse: 0.91349 |  0:01:28s
epoch 13 | loss: 0.7555  | val_0_rmse: 0.85684 | val_1_rmse: 0.9079  |  0:01:35s
epoch 14 | loss: 0.74644 | val_0_rmse: 0.85256 | val_1_rmse: 0.91216 |  0:01:42s
epoch 15 | loss: 0.73076 | val_0_rmse: 0.84907 | val_1_rmse: 0.90989 |  0:01:49s
epoch 16 | loss: 0.72412 | val_0_rmse: 0.83606 | val_1_rmse: 0.91095 |  0:01:56s
epoch 17 | loss: 0.71524 | val_0_rmse: 0.84255 | val_1_rmse: 0.9111  |  0:02:03s
epoch 18 | loss: 0.71007 | val_0_rmse: 0.8426  | val_1_rmse: 0.90436 |  0:02:09s
epoch 19 | loss: 0.70622 | val_0_rmse: 0.8318  | val_1_rmse: 0.90035 |  0:02:16s
epoch 20 | loss: 0.6986  | val_0_rmse: 0.83811 | val_1_rmse: 0.91111 |  0:02:23s
epoch 21 | loss: 0.68791 | val_0_rmse: 0.81945 | val_1_rmse: 0.90869 |  0:02:30s
epoch 22 | loss: 0.68021 | val_0_rmse: 0.81876 | val_1_rmse: 0.90591 |  0:02:37s
epoch 23 | loss: 0.67337 | val_0_rmse: 1.05258 | val_1_rmse: 1.62657 |  0:02:44s
epoch 24 | loss: 0.66547 | val_0_rmse: 0.81056 | val_1_rmse: 0.90906 |  0:02:50s
epoch 25 | loss: 0.66051 | val_0_rmse: 0.81073 | val_1_rmse: 0.91907 |  0:02:57s
epoch 26 | loss: 0.65457 | val_0_rmse: 0.82784 | val_1_rmse: 0.93676 |  0:03:04s
epoch 27 | loss: 0.6525  | val_0_rmse: 0.80386 | val_1_rmse: 0.91356 |  0:03:11s
epoch 28 | loss: 0.64506 | val_0_rmse: 0.79724 | val_1_rmse: 0.90559 |  0:03:18s
epoch 29 | loss: 0.63694 | val_0_rmse: 0.7989  | val_1_rmse: 0.91096 |  0:03:25s
epoch 30 | loss: 0.6337  | val_0_rmse: 0.79944 | val_1_rmse: 0.92205 |  0:03:31s
epoch 31 | loss: 0.62594 | val_0_rmse: 0.90033 | val_1_rmse: 0.97667 |  0:03:38s
epoch 32 | loss: 0.62081 | val_0_rmse: 0.78555 | val_1_rmse: 0.90347 |  0:03:45s
epoch 33 | loss: 0.62045 | val_0_rmse: 0.81808 | val_1_rmse: 0.93221 |  0:03:52s
epoch 34 | loss: 0.62097 | val_0_rmse: 0.79769 | val_1_rmse: 0.91063 |  0:03:59s
epoch 35 | loss: 0.61863 | val_0_rmse: 0.78011 | val_1_rmse: 0.89718 |  0:04:06s
epoch 36 | loss: 0.60866 | val_0_rmse: 0.78754 | val_1_rmse: 0.90795 |  0:04:12s
epoch 37 | loss: 0.60377 | val_0_rmse: 0.78018 | val_1_rmse: 0.92937 |  0:04:19s
epoch 38 | loss: 0.59641 | val_0_rmse: 0.77272 | val_1_rmse: 0.91129 |  0:04:26s
epoch 39 | loss: 0.59463 | val_0_rmse: 0.91007 | val_1_rmse: 1.2424  |  0:04:33s
epoch 40 | loss: 0.59572 | val_0_rmse: 0.78066 | val_1_rmse: 0.91489 |  0:04:40s
epoch 41 | loss: 0.59272 | val_0_rmse: 0.77212 | val_1_rmse: 0.90977 |  0:04:47s
epoch 42 | loss: 0.59248 | val_0_rmse: 0.76968 | val_1_rmse: 0.91075 |  0:04:53s
epoch 43 | loss: 0.58822 | val_0_rmse: 0.77013 | val_1_rmse: 0.9102  |  0:05:00s
epoch 44 | loss: 0.58331 | val_0_rmse: 0.77123 | val_1_rmse: 0.92958 |  0:05:07s
epoch 45 | loss: 0.58339 | val_0_rmse: 0.76016 | val_1_rmse: 0.91876 |  0:05:14s
epoch 46 | loss: 0.57426 | val_0_rmse: 0.76737 | val_1_rmse: 0.93725 |  0:05:21s
epoch 47 | loss: 0.57327 | val_0_rmse: 0.76196 | val_1_rmse: 0.91237 |  0:05:28s
epoch 48 | loss: 0.57285 | val_0_rmse: 0.76428 | val_1_rmse: 0.91414 |  0:05:34s
epoch 49 | loss: 0.57064 | val_0_rmse: 0.77455 | val_1_rmse: 0.92161 |  0:05:41s
epoch 50 | loss: 0.56923 | val_0_rmse: 0.78688 | val_1_rmse: 0.96951 |  0:05:48s
epoch 51 | loss: 0.56506 | val_0_rmse: 0.7573  | val_1_rmse: 0.93015 |  0:05:55s
epoch 52 | loss: 0.56333 | val_0_rmse: 0.76371 | val_1_rmse: 0.9391  |  0:06:02s
epoch 53 | loss: 0.55891 | val_0_rmse: 0.75689 | val_1_rmse: 0.90201 |  0:06:09s
epoch 54 | loss: 0.5621  | val_0_rmse: 0.759   | val_1_rmse: 0.91611 |  0:06:15s
epoch 55 | loss: 0.55878 | val_0_rmse: 0.74578 | val_1_rmse: 0.92489 |  0:06:22s
epoch 56 | loss: 0.55466 | val_0_rmse: 0.77728 | val_1_rmse: 0.95306 |  0:06:29s
epoch 57 | loss: 0.55312 | val_0_rmse: 0.75185 | val_1_rmse: 0.91405 |  0:06:36s
epoch 58 | loss: 0.55019 | val_0_rmse: 0.74771 | val_1_rmse: 0.92007 |  0:06:43s
epoch 59 | loss: 0.54818 | val_0_rmse: 0.74842 | val_1_rmse: 0.92317 |  0:06:50s
epoch 60 | loss: 0.54503 | val_0_rmse: 0.79786 | val_1_rmse: 1.00024 |  0:06:56s
epoch 61 | loss: 0.54297 | val_0_rmse: 0.74937 | val_1_rmse: 0.9127  |  0:07:03s
epoch 62 | loss: 0.54388 | val_0_rmse: 0.75469 | val_1_rmse: 0.95363 |  0:07:10s
epoch 63 | loss: 0.54392 | val_0_rmse: 0.73769 | val_1_rmse: 0.92588 |  0:07:17s
epoch 64 | loss: 0.54075 | val_0_rmse: 0.73232 | val_1_rmse: 0.91858 |  0:07:24s
epoch 65 | loss: 0.53574 | val_0_rmse: 0.73682 | val_1_rmse: 0.93039 |  0:07:31s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.89718
Best weights from best epoch are automatically used!
ended training at: 03:47:19
Feature importance:
Mean squared error is of 0.06440979542661525
Mean absolute error:0.18691317319132508
MAPE:0.20233795745462962
R2 score:0.1682557216874353
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:47:21
epoch 0  | loss: 1.21563 | val_0_rmse: 1.00148 | val_1_rmse: 0.98227 |  0:00:06s
epoch 1  | loss: 1.00128 | val_0_rmse: 0.98446 | val_1_rmse: 0.96541 |  0:00:13s
epoch 2  | loss: 0.94846 | val_0_rmse: 0.94991 | val_1_rmse: 0.93238 |  0:00:20s
epoch 3  | loss: 0.8654  | val_0_rmse: 0.93823 | val_1_rmse: 0.92125 |  0:00:27s
epoch 4  | loss: 0.82888 | val_0_rmse: 0.92847 | val_1_rmse: 0.91418 |  0:00:34s
epoch 5  | loss: 0.81327 | val_0_rmse: 0.94944 | val_1_rmse: 0.93422 |  0:00:41s
epoch 6  | loss: 0.79899 | val_0_rmse: 0.90365 | val_1_rmse: 0.89424 |  0:00:48s
epoch 7  | loss: 0.78251 | val_0_rmse: 0.90401 | val_1_rmse: 0.89929 |  0:00:54s
epoch 8  | loss: 0.76639 | val_0_rmse: 0.88634 | val_1_rmse: 0.88972 |  0:01:01s
epoch 9  | loss: 0.7567  | val_0_rmse: 0.8764  | val_1_rmse: 0.88621 |  0:01:08s
epoch 10 | loss: 0.74972 | val_0_rmse: 0.87608 | val_1_rmse: 0.89231 |  0:01:15s
epoch 11 | loss: 0.74203 | val_0_rmse: 0.86194 | val_1_rmse: 0.88241 |  0:01:22s
epoch 12 | loss: 0.73441 | val_0_rmse: 0.85566 | val_1_rmse: 0.8773  |  0:01:29s
epoch 13 | loss: 0.72273 | val_0_rmse: 0.84493 | val_1_rmse: 0.86955 |  0:01:36s
epoch 14 | loss: 0.71465 | val_0_rmse: 0.82755 | val_1_rmse: 0.87033 |  0:01:42s
epoch 15 | loss: 0.70376 | val_0_rmse: 0.84906 | val_1_rmse: 0.8819  |  0:01:49s
epoch 16 | loss: 0.70147 | val_0_rmse: 0.83471 | val_1_rmse: 0.89386 |  0:01:56s
epoch 17 | loss: 0.69295 | val_0_rmse: 0.82245 | val_1_rmse: 0.87239 |  0:02:03s
epoch 18 | loss: 0.68582 | val_0_rmse: 0.8208  | val_1_rmse: 0.89155 |  0:02:10s
epoch 19 | loss: 0.68296 | val_0_rmse: 0.82355 | val_1_rmse: 0.88741 |  0:02:17s
epoch 20 | loss: 0.67234 | val_0_rmse: 0.8117  | val_1_rmse: 0.87446 |  0:02:24s
epoch 21 | loss: 0.66615 | val_0_rmse: 0.82908 | val_1_rmse: 0.90711 |  0:02:31s
epoch 22 | loss: 0.66143 | val_0_rmse: 0.80858 | val_1_rmse: 0.88064 |  0:02:38s
epoch 23 | loss: 0.65289 | val_0_rmse: 0.80347 | val_1_rmse: 0.88801 |  0:02:44s
epoch 24 | loss: 0.65333 | val_0_rmse: 0.80564 | val_1_rmse: 0.88626 |  0:02:51s
epoch 25 | loss: 0.64578 | val_0_rmse: 0.80401 | val_1_rmse: 0.88405 |  0:02:58s
epoch 26 | loss: 0.64257 | val_0_rmse: 0.79629 | val_1_rmse: 0.87862 |  0:03:05s
epoch 27 | loss: 0.64105 | val_0_rmse: 0.90791 | val_1_rmse: 0.93961 |  0:03:12s
epoch 28 | loss: 0.6365  | val_0_rmse: 0.79254 | val_1_rmse: 0.88626 |  0:03:19s
epoch 29 | loss: 0.64153 | val_0_rmse: 0.82045 | val_1_rmse: 0.90237 |  0:03:26s
epoch 30 | loss: 0.62754 | val_0_rmse: 0.79382 | val_1_rmse: 0.8965  |  0:03:32s
epoch 31 | loss: 0.63169 | val_0_rmse: 0.99083 | val_1_rmse: 1.10116 |  0:03:39s
epoch 32 | loss: 0.62094 | val_0_rmse: 0.88132 | val_1_rmse: 0.95911 |  0:03:46s
epoch 33 | loss: 0.69291 | val_0_rmse: 0.8366  | val_1_rmse: 0.91233 |  0:03:53s
epoch 34 | loss: 0.66104 | val_0_rmse: 0.81078 | val_1_rmse: 0.91038 |  0:04:00s
epoch 35 | loss: 0.63869 | val_0_rmse: 0.80492 | val_1_rmse: 0.93677 |  0:04:07s
epoch 36 | loss: 0.62087 | val_0_rmse: 0.80738 | val_1_rmse: 0.94649 |  0:04:14s
epoch 37 | loss: 0.61771 | val_0_rmse: 0.80568 | val_1_rmse: 0.96641 |  0:04:20s
epoch 38 | loss: 0.61091 | val_0_rmse: 0.87608 | val_1_rmse: 1.04257 |  0:04:27s
epoch 39 | loss: 0.6088  | val_0_rmse: 0.83475 | val_1_rmse: 0.97792 |  0:04:34s
epoch 40 | loss: 0.60245 | val_0_rmse: 0.83961 | val_1_rmse: 1.03513 |  0:04:41s
epoch 41 | loss: 0.5992  | val_0_rmse: 0.79489 | val_1_rmse: 0.91864 |  0:04:48s
epoch 42 | loss: 0.59553 | val_0_rmse: 0.78332 | val_1_rmse: 0.91192 |  0:04:55s
epoch 43 | loss: 0.59147 | val_0_rmse: 0.83151 | val_1_rmse: 0.97508 |  0:05:01s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.86955
Best weights from best epoch are automatically used!
ended training at: 03:52:26
Feature importance:
Mean squared error is of 0.06263982824309043
Mean absolute error:0.1834487787181943
MAPE:0.2024459350516224
R2 score:0.2055117953053781
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:52:28
epoch 0  | loss: 1.20756 | val_0_rmse: 1.00338 | val_1_rmse: 1.00153 |  0:00:06s
epoch 1  | loss: 1.0018  | val_0_rmse: 0.98602 | val_1_rmse: 0.9847  |  0:00:13s
epoch 2  | loss: 0.97618 | val_0_rmse: 0.97613 | val_1_rmse: 0.97353 |  0:00:20s
epoch 3  | loss: 0.95478 | val_0_rmse: 0.96746 | val_1_rmse: 0.96561 |  0:00:27s
epoch 4  | loss: 0.92687 | val_0_rmse: 0.95117 | val_1_rmse: 0.94896 |  0:00:34s
epoch 5  | loss: 0.8949  | val_0_rmse: 0.94105 | val_1_rmse: 0.93976 |  0:00:41s
epoch 6  | loss: 0.86665 | val_0_rmse: 0.92543 | val_1_rmse: 0.92713 |  0:00:48s
epoch 7  | loss: 0.84115 | val_0_rmse: 0.91567 | val_1_rmse: 0.92135 |  0:00:55s
epoch 8  | loss: 0.82248 | val_0_rmse: 0.91617 | val_1_rmse: 0.928   |  0:01:01s
epoch 9  | loss: 0.80588 | val_0_rmse: 0.89391 | val_1_rmse: 0.90674 |  0:01:08s
epoch 10 | loss: 0.79586 | val_0_rmse: 0.89305 | val_1_rmse: 0.90525 |  0:01:15s
epoch 11 | loss: 0.79064 | val_0_rmse: 0.87987 | val_1_rmse: 0.90018 |  0:01:22s
epoch 12 | loss: 0.78891 | val_0_rmse: 0.87101 | val_1_rmse: 0.90224 |  0:01:29s
epoch 13 | loss: 0.76633 | val_0_rmse: 0.8659  | val_1_rmse: 0.90238 |  0:01:36s
epoch 14 | loss: 0.75745 | val_0_rmse: 0.86023 | val_1_rmse: 0.8961  |  0:01:43s
epoch 15 | loss: 0.75598 | val_0_rmse: 0.86365 | val_1_rmse: 0.91257 |  0:01:50s
epoch 16 | loss: 0.74955 | val_0_rmse: 0.89825 | val_1_rmse: 0.90902 |  0:01:57s
epoch 17 | loss: 0.74171 | val_0_rmse: 0.86252 | val_1_rmse: 0.92284 |  0:02:03s
epoch 18 | loss: 0.73725 | val_0_rmse: 0.85288 | val_1_rmse: 0.90437 |  0:02:10s
epoch 19 | loss: 0.7319  | val_0_rmse: 1.07953 | val_1_rmse: 0.91226 |  0:02:17s
epoch 20 | loss: 0.72498 | val_0_rmse: 0.95238 | val_1_rmse: 0.91517 |  0:02:24s
epoch 21 | loss: 0.7176  | val_0_rmse: 0.84903 | val_1_rmse: 0.92566 |  0:02:31s
epoch 22 | loss: 0.7161  | val_0_rmse: 0.84801 | val_1_rmse: 0.90598 |  0:02:38s
epoch 23 | loss: 0.70963 | val_0_rmse: 0.92219 | val_1_rmse: 0.90484 |  0:02:45s
epoch 24 | loss: 0.70632 | val_0_rmse: 0.84756 | val_1_rmse: 0.92069 |  0:02:52s
epoch 25 | loss: 0.70003 | val_0_rmse: 0.83667 | val_1_rmse: 0.91142 |  0:02:58s
epoch 26 | loss: 0.70549 | val_0_rmse: 0.83287 | val_1_rmse: 0.90879 |  0:03:05s
epoch 27 | loss: 0.69707 | val_0_rmse: 0.83251 | val_1_rmse: 0.9049  |  0:03:12s
epoch 28 | loss: 0.6949  | val_0_rmse: 0.83589 | val_1_rmse: 0.94438 |  0:03:19s
epoch 29 | loss: 0.69022 | val_0_rmse: 0.83176 | val_1_rmse: 0.92923 |  0:03:26s
epoch 30 | loss: 0.68561 | val_0_rmse: 0.82397 | val_1_rmse: 0.91796 |  0:03:33s
epoch 31 | loss: 0.67645 | val_0_rmse: 0.8219  | val_1_rmse: 0.90794 |  0:03:40s
epoch 32 | loss: 0.67719 | val_0_rmse: 0.84981 | val_1_rmse: 0.91983 |  0:03:47s
epoch 33 | loss: 0.67029 | val_0_rmse: 0.81405 | val_1_rmse: 0.91242 |  0:03:53s
epoch 34 | loss: 0.6648  | val_0_rmse: 0.82511 | val_1_rmse: 0.91519 |  0:04:00s
epoch 35 | loss: 0.6601  | val_0_rmse: 0.81054 | val_1_rmse: 0.90381 |  0:04:07s
epoch 36 | loss: 0.65985 | val_0_rmse: 0.8186  | val_1_rmse: 0.92612 |  0:04:14s
epoch 37 | loss: 0.65622 | val_0_rmse: 0.81869 | val_1_rmse: 0.92979 |  0:04:21s
epoch 38 | loss: 0.64792 | val_0_rmse: 0.80795 | val_1_rmse: 0.9222  |  0:04:28s
epoch 39 | loss: 0.64336 | val_0_rmse: 0.80373 | val_1_rmse: 0.90758 |  0:04:35s
epoch 40 | loss: 0.64257 | val_0_rmse: 0.80173 | val_1_rmse: 0.91737 |  0:04:42s
epoch 41 | loss: 0.63715 | val_0_rmse: 0.79611 | val_1_rmse: 0.90388 |  0:04:48s
epoch 42 | loss: 0.63695 | val_0_rmse: 0.81123 | val_1_rmse: 0.91514 |  0:04:55s
epoch 43 | loss: 0.63559 | val_0_rmse: 0.90295 | val_1_rmse: 0.97729 |  0:05:02s
epoch 44 | loss: 0.62985 | val_0_rmse: 0.8021  | val_1_rmse: 0.91482 |  0:05:09s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.8961
Best weights from best epoch are automatically used!
ended training at: 03:57:41
Feature importance:
Mean squared error is of 0.05940270493481916
Mean absolute error:0.18434345167194505
MAPE:0.2030579481971313
R2 score:0.19320299966306453
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:57:43
epoch 0  | loss: 1.16249 | val_0_rmse: 1.00042 | val_1_rmse: 1.00975 |  0:00:06s
epoch 1  | loss: 0.99923 | val_0_rmse: 0.98645 | val_1_rmse: 0.99578 |  0:00:13s
epoch 2  | loss: 0.97399 | val_0_rmse: 0.9651  | val_1_rmse: 0.97163 |  0:00:20s
epoch 3  | loss: 0.91902 | val_0_rmse: 0.94826 | val_1_rmse: 0.95504 |  0:00:27s
epoch 4  | loss: 0.88032 | val_0_rmse: 0.94598 | val_1_rmse: 0.95084 |  0:00:34s
epoch 5  | loss: 0.8471  | val_0_rmse: 0.92472 | val_1_rmse: 0.93416 |  0:00:41s
epoch 6  | loss: 0.8194  | val_0_rmse: 0.92778 | val_1_rmse: 0.93972 |  0:00:48s
epoch 7  | loss: 0.81354 | val_0_rmse: 0.90928 | val_1_rmse: 0.92385 |  0:00:54s
epoch 8  | loss: 0.79196 | val_0_rmse: 0.89334 | val_1_rmse: 0.91608 |  0:01:01s
epoch 9  | loss: 0.77788 | val_0_rmse: 0.88507 | val_1_rmse: 0.91402 |  0:01:08s
epoch 10 | loss: 0.77108 | val_0_rmse: 0.87797 | val_1_rmse: 0.90924 |  0:01:15s
epoch 11 | loss: 0.7612  | val_0_rmse: 0.87191 | val_1_rmse: 0.92015 |  0:01:22s
epoch 12 | loss: 0.75329 | val_0_rmse: 0.8519  | val_1_rmse: 0.90252 |  0:01:29s
epoch 13 | loss: 0.74501 | val_0_rmse: 0.84964 | val_1_rmse: 0.90219 |  0:01:36s
epoch 14 | loss: 0.735   | val_0_rmse: 0.84383 | val_1_rmse: 0.90595 |  0:01:43s
epoch 15 | loss: 0.7316  | val_0_rmse: 0.84468 | val_1_rmse: 0.90916 |  0:01:50s
epoch 16 | loss: 0.72869 | val_0_rmse: 0.86087 | val_1_rmse: 0.91226 |  0:01:56s
epoch 17 | loss: 0.72071 | val_0_rmse: 0.84608 | val_1_rmse: 0.90576 |  0:02:03s
epoch 18 | loss: 0.71768 | val_0_rmse: 1.14538 | val_1_rmse: 1.4044  |  0:02:10s
epoch 19 | loss: 0.71193 | val_0_rmse: 0.8549  | val_1_rmse: 0.93671 |  0:02:17s
epoch 20 | loss: 0.71256 | val_0_rmse: 0.83652 | val_1_rmse: 0.90716 |  0:02:24s
epoch 21 | loss: 0.70279 | val_0_rmse: 0.83981 | val_1_rmse: 0.9065  |  0:02:31s
epoch 22 | loss: 0.69494 | val_0_rmse: 0.8452  | val_1_rmse: 0.91105 |  0:02:38s
epoch 23 | loss: 0.69125 | val_0_rmse: 0.91417 | val_1_rmse: 1.01422 |  0:02:45s
epoch 24 | loss: 0.6849  | val_0_rmse: 0.8256  | val_1_rmse: 0.90186 |  0:02:51s
epoch 25 | loss: 0.68133 | val_0_rmse: 0.81871 | val_1_rmse: 0.89586 |  0:02:58s
epoch 26 | loss: 0.67542 | val_0_rmse: 0.81365 | val_1_rmse: 0.89711 |  0:03:05s
epoch 27 | loss: 0.67015 | val_0_rmse: 0.81622 | val_1_rmse: 0.90746 |  0:03:12s
epoch 28 | loss: 0.66432 | val_0_rmse: 0.83013 | val_1_rmse: 0.90641 |  0:03:19s
epoch 29 | loss: 0.65907 | val_0_rmse: 0.81797 | val_1_rmse: 0.92045 |  0:03:26s
epoch 30 | loss: 0.65664 | val_0_rmse: 0.83143 | val_1_rmse: 0.91324 |  0:03:33s
epoch 31 | loss: 0.6535  | val_0_rmse: 0.8316  | val_1_rmse: 0.90771 |  0:03:40s
epoch 32 | loss: 0.64992 | val_0_rmse: 0.89168 | val_1_rmse: 0.9789  |  0:03:47s
epoch 33 | loss: 0.65053 | val_0_rmse: 0.8056  | val_1_rmse: 0.89923 |  0:03:53s
epoch 34 | loss: 0.64426 | val_0_rmse: 0.80095 | val_1_rmse: 0.91014 |  0:04:00s
epoch 35 | loss: 0.63389 | val_0_rmse: 0.84797 | val_1_rmse: 0.93038 |  0:04:07s
epoch 36 | loss: 0.63067 | val_0_rmse: 0.79383 | val_1_rmse: 0.90989 |  0:04:14s
epoch 37 | loss: 0.62981 | val_0_rmse: 0.7989  | val_1_rmse: 0.90655 |  0:04:21s
epoch 38 | loss: 0.62327 | val_0_rmse: 0.84637 | val_1_rmse: 0.92071 |  0:04:28s
epoch 39 | loss: 0.61993 | val_0_rmse: 0.80044 | val_1_rmse: 0.91559 |  0:04:35s
epoch 40 | loss: 0.6186  | val_0_rmse: 0.81939 | val_1_rmse: 0.94266 |  0:04:42s
epoch 41 | loss: 0.61738 | val_0_rmse: 0.81227 | val_1_rmse: 0.91581 |  0:04:48s
epoch 42 | loss: 0.61416 | val_0_rmse: 0.80391 | val_1_rmse: 0.93039 |  0:04:55s
epoch 43 | loss: 0.60813 | val_0_rmse: 0.78558 | val_1_rmse: 0.9095  |  0:05:02s
epoch 44 | loss: 0.59981 | val_0_rmse: 0.79195 | val_1_rmse: 0.90678 |  0:05:09s
epoch 45 | loss: 0.59843 | val_0_rmse: 0.82837 | val_1_rmse: 0.91812 |  0:05:16s
epoch 46 | loss: 0.59639 | val_0_rmse: 0.79008 | val_1_rmse: 0.91624 |  0:05:23s
epoch 47 | loss: 0.5927  | val_0_rmse: 0.78893 | val_1_rmse: 0.92549 |  0:05:30s
epoch 48 | loss: 0.58733 | val_0_rmse: 0.80888 | val_1_rmse: 0.94021 |  0:05:37s
epoch 49 | loss: 0.59013 | val_0_rmse: 0.77987 | val_1_rmse: 0.92362 |  0:05:43s
epoch 50 | loss: 0.58421 | val_0_rmse: 0.77754 | val_1_rmse: 0.92197 |  0:05:50s
epoch 51 | loss: 0.58225 | val_0_rmse: 0.77182 | val_1_rmse: 0.91833 |  0:05:57s
epoch 52 | loss: 0.57852 | val_0_rmse: 0.79075 | val_1_rmse: 0.94187 |  0:06:04s
epoch 53 | loss: 0.57486 | val_0_rmse: 0.79172 | val_1_rmse: 0.94823 |  0:06:11s
epoch 54 | loss: 0.5782  | val_0_rmse: 0.77971 | val_1_rmse: 0.91686 |  0:06:18s
epoch 55 | loss: 0.57371 | val_0_rmse: 0.78828 | val_1_rmse: 0.94429 |  0:06:25s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.89586
Best weights from best epoch are automatically used!
ended training at: 04:04:12
Feature importance:
Mean squared error is of 0.06167088322973737
Mean absolute error:0.18409255863974022
MAPE:0.19772801173721072
R2 score:0.17811006636752158
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:04:15
epoch 0  | loss: 2.42274 | val_0_rmse: 0.99653 | val_1_rmse: 1.00384 |  0:00:00s
epoch 1  | loss: 0.99266 | val_0_rmse: 0.99204 | val_1_rmse: 1.0005  |  0:00:01s
epoch 2  | loss: 0.98199 | val_0_rmse: 0.98095 | val_1_rmse: 0.98961 |  0:00:02s
epoch 3  | loss: 0.94216 | val_0_rmse: 0.97391 | val_1_rmse: 0.98273 |  0:00:03s
epoch 4  | loss: 0.90744 | val_0_rmse: 0.9742  | val_1_rmse: 0.98385 |  0:00:04s
epoch 5  | loss: 0.88097 | val_0_rmse: 0.94592 | val_1_rmse: 0.9489  |  0:00:06s
epoch 6  | loss: 0.85736 | val_0_rmse: 0.94941 | val_1_rmse: 0.95433 |  0:00:07s
epoch 7  | loss: 0.82667 | val_0_rmse: 0.9543  | val_1_rmse: 0.95668 |  0:00:08s
epoch 8  | loss: 0.80372 | val_0_rmse: 0.90709 | val_1_rmse: 0.91922 |  0:00:09s
epoch 9  | loss: 0.78307 | val_0_rmse: 0.90359 | val_1_rmse: 0.91637 |  0:00:10s
epoch 10 | loss: 0.77702 | val_0_rmse: 0.89884 | val_1_rmse: 0.91308 |  0:00:11s
epoch 11 | loss: 0.76796 | val_0_rmse: 0.90754 | val_1_rmse: 0.92172 |  0:00:12s
epoch 12 | loss: 0.7565  | val_0_rmse: 0.88598 | val_1_rmse: 0.90551 |  0:00:13s
epoch 13 | loss: 0.76175 | val_0_rmse: 0.89016 | val_1_rmse: 0.90843 |  0:00:14s
epoch 14 | loss: 0.74666 | val_0_rmse: 0.87788 | val_1_rmse: 0.89802 |  0:00:15s
epoch 15 | loss: 0.73459 | val_0_rmse: 0.86722 | val_1_rmse: 0.88815 |  0:00:16s
epoch 16 | loss: 0.72139 | val_0_rmse: 0.86014 | val_1_rmse: 0.88348 |  0:00:17s
epoch 17 | loss: 0.70332 | val_0_rmse: 0.85713 | val_1_rmse: 0.87828 |  0:00:18s
epoch 18 | loss: 0.70902 | val_0_rmse: 0.8538  | val_1_rmse: 0.87587 |  0:00:19s
epoch 19 | loss: 0.70234 | val_0_rmse: 0.85559 | val_1_rmse: 0.87472 |  0:00:20s
epoch 20 | loss: 0.69553 | val_0_rmse: 0.84886 | val_1_rmse: 0.87391 |  0:00:21s
epoch 21 | loss: 0.6869  | val_0_rmse: 0.84344 | val_1_rmse: 0.86962 |  0:00:22s
epoch 22 | loss: 0.68119 | val_0_rmse: 0.85011 | val_1_rmse: 0.87727 |  0:00:23s
epoch 23 | loss: 0.67377 | val_0_rmse: 0.83557 | val_1_rmse: 0.86663 |  0:00:24s
epoch 24 | loss: 0.66907 | val_0_rmse: 0.82776 | val_1_rmse: 0.86244 |  0:00:25s
epoch 25 | loss: 0.66349 | val_0_rmse: 0.83302 | val_1_rmse: 0.8648  |  0:00:26s
epoch 26 | loss: 0.67248 | val_0_rmse: 0.84648 | val_1_rmse: 0.87347 |  0:00:27s
epoch 27 | loss: 0.67744 | val_0_rmse: 0.8251  | val_1_rmse: 0.85531 |  0:00:28s
epoch 28 | loss: 0.66422 | val_0_rmse: 0.82033 | val_1_rmse: 0.85266 |  0:00:29s
epoch 29 | loss: 0.65885 | val_0_rmse: 0.84965 | val_1_rmse: 0.88067 |  0:00:30s
epoch 30 | loss: 0.65495 | val_0_rmse: 0.82792 | val_1_rmse: 0.86013 |  0:00:31s
epoch 31 | loss: 0.65289 | val_0_rmse: 0.82231 | val_1_rmse: 0.85937 |  0:00:32s
epoch 32 | loss: 0.64086 | val_0_rmse: 0.80622 | val_1_rmse: 0.85354 |  0:00:33s
epoch 33 | loss: 0.63913 | val_0_rmse: 0.79732 | val_1_rmse: 0.85004 |  0:00:34s
epoch 34 | loss: 0.63273 | val_0_rmse: 0.80902 | val_1_rmse: 0.85789 |  0:00:35s
epoch 35 | loss: 0.63428 | val_0_rmse: 0.80391 | val_1_rmse: 0.84764 |  0:00:36s
epoch 36 | loss: 0.64234 | val_0_rmse: 0.80825 | val_1_rmse: 0.8521  |  0:00:37s
epoch 37 | loss: 0.64142 | val_0_rmse: 0.79977 | val_1_rmse: 0.84852 |  0:00:38s
epoch 38 | loss: 0.62806 | val_0_rmse: 0.79314 | val_1_rmse: 0.84513 |  0:00:39s
epoch 39 | loss: 0.63088 | val_0_rmse: 0.79025 | val_1_rmse: 0.85124 |  0:00:40s
epoch 40 | loss: 0.62771 | val_0_rmse: 0.7922  | val_1_rmse: 0.84411 |  0:00:41s
epoch 41 | loss: 0.62563 | val_0_rmse: 0.78247 | val_1_rmse: 0.84063 |  0:00:42s
epoch 42 | loss: 0.61878 | val_0_rmse: 0.77555 | val_1_rmse: 0.84391 |  0:00:43s
epoch 43 | loss: 0.61551 | val_0_rmse: 0.78869 | val_1_rmse: 0.85798 |  0:00:44s
epoch 44 | loss: 0.61343 | val_0_rmse: 0.77518 | val_1_rmse: 0.84295 |  0:00:45s
epoch 45 | loss: 0.60234 | val_0_rmse: 0.77719 | val_1_rmse: 0.8406  |  0:00:46s
epoch 46 | loss: 0.6028  | val_0_rmse: 0.77049 | val_1_rmse: 0.84667 |  0:00:47s
epoch 47 | loss: 0.61351 | val_0_rmse: 0.77717 | val_1_rmse: 0.84276 |  0:00:48s
epoch 48 | loss: 0.61119 | val_0_rmse: 0.77552 | val_1_rmse: 0.848   |  0:00:49s
epoch 49 | loss: 0.59385 | val_0_rmse: 0.76541 | val_1_rmse: 0.84099 |  0:00:50s
epoch 50 | loss: 0.58924 | val_0_rmse: 0.76401 | val_1_rmse: 0.8486  |  0:00:51s
epoch 51 | loss: 0.58322 | val_0_rmse: 0.75147 | val_1_rmse: 0.8349  |  0:00:52s
epoch 52 | loss: 0.5812  | val_0_rmse: 0.75302 | val_1_rmse: 0.83624 |  0:00:53s
epoch 53 | loss: 0.58147 | val_0_rmse: 0.74875 | val_1_rmse: 0.84579 |  0:00:54s
epoch 54 | loss: 0.57789 | val_0_rmse: 0.75214 | val_1_rmse: 0.82852 |  0:00:55s
epoch 55 | loss: 0.58639 | val_0_rmse: 0.75276 | val_1_rmse: 0.83724 |  0:00:56s
epoch 56 | loss: 0.58649 | val_0_rmse: 0.74137 | val_1_rmse: 0.83768 |  0:00:57s
epoch 57 | loss: 0.57475 | val_0_rmse: 0.7463  | val_1_rmse: 0.83756 |  0:00:58s
epoch 58 | loss: 0.57355 | val_0_rmse: 0.74713 | val_1_rmse: 0.84349 |  0:00:59s
epoch 59 | loss: 0.58029 | val_0_rmse: 0.73643 | val_1_rmse: 0.84379 |  0:01:00s
epoch 60 | loss: 0.56738 | val_0_rmse: 0.73281 | val_1_rmse: 0.84392 |  0:01:01s
epoch 61 | loss: 0.57323 | val_0_rmse: 0.76023 | val_1_rmse: 0.85107 |  0:01:02s
epoch 62 | loss: 0.57715 | val_0_rmse: 0.73908 | val_1_rmse: 0.83278 |  0:01:03s
epoch 63 | loss: 0.55828 | val_0_rmse: 0.73226 | val_1_rmse: 0.84044 |  0:01:04s
epoch 64 | loss: 0.5605  | val_0_rmse: 0.73119 | val_1_rmse: 0.84116 |  0:01:05s
epoch 65 | loss: 0.55507 | val_0_rmse: 0.72287 | val_1_rmse: 0.83452 |  0:01:06s
epoch 66 | loss: 0.54903 | val_0_rmse: 0.72761 | val_1_rmse: 0.83837 |  0:01:07s
epoch 67 | loss: 0.55109 | val_0_rmse: 0.72029 | val_1_rmse: 0.84877 |  0:01:08s
epoch 68 | loss: 0.55103 | val_0_rmse: 0.71835 | val_1_rmse: 0.86372 |  0:01:09s
epoch 69 | loss: 0.55437 | val_0_rmse: 0.71838 | val_1_rmse: 0.85906 |  0:01:10s
epoch 70 | loss: 0.55091 | val_0_rmse: 0.73002 | val_1_rmse: 0.87591 |  0:01:11s
epoch 71 | loss: 0.54513 | val_0_rmse: 0.71849 | val_1_rmse: 0.86228 |  0:01:12s
epoch 72 | loss: 0.53826 | val_0_rmse: 0.71491 | val_1_rmse: 0.85344 |  0:01:13s
epoch 73 | loss: 0.53964 | val_0_rmse: 0.7212  | val_1_rmse: 0.86751 |  0:01:14s
epoch 74 | loss: 0.54936 | val_0_rmse: 0.72485 | val_1_rmse: 0.88332 |  0:01:15s
epoch 75 | loss: 0.54421 | val_0_rmse: 0.71408 | val_1_rmse: 0.85503 |  0:01:16s
epoch 76 | loss: 0.53193 | val_0_rmse: 0.72619 | val_1_rmse: 0.8738  |  0:01:17s
epoch 77 | loss: 0.53964 | val_0_rmse: 0.714   | val_1_rmse: 0.86675 |  0:01:18s
epoch 78 | loss: 0.52966 | val_0_rmse: 0.70808 | val_1_rmse: 0.87415 |  0:01:19s
epoch 79 | loss: 0.52328 | val_0_rmse: 0.70708 | val_1_rmse: 0.87814 |  0:01:20s
epoch 80 | loss: 0.53402 | val_0_rmse: 0.70804 | val_1_rmse: 0.85771 |  0:01:21s
epoch 81 | loss: 0.52616 | val_0_rmse: 0.70459 | val_1_rmse: 0.86828 |  0:01:22s
epoch 82 | loss: 0.51883 | val_0_rmse: 0.70162 | val_1_rmse: 0.87005 |  0:01:23s
epoch 83 | loss: 0.52492 | val_0_rmse: 0.71463 | val_1_rmse: 0.88442 |  0:01:24s
epoch 84 | loss: 0.5236  | val_0_rmse: 0.71081 | val_1_rmse: 0.87999 |  0:01:25s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.82852
Best weights from best epoch are automatically used!
ended training at: 04:05:42
Feature importance:
Mean squared error is of 0.08927165254268274
Mean absolute error:0.17911959025947813
MAPE:0.19094791323798846
R2 score:0.24760950009301952
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:05:42
epoch 0  | loss: 2.72263 | val_0_rmse: 1.0067  | val_1_rmse: 0.98209 |  0:00:00s
epoch 1  | loss: 1.05637 | val_0_rmse: 1.00274 | val_1_rmse: 0.97748 |  0:00:01s
epoch 2  | loss: 0.97674 | val_0_rmse: 0.97077 | val_1_rmse: 0.94752 |  0:00:02s
epoch 3  | loss: 0.93053 | val_0_rmse: 0.95907 | val_1_rmse: 0.93738 |  0:00:03s
epoch 4  | loss: 0.90082 | val_0_rmse: 0.92226 | val_1_rmse: 0.90682 |  0:00:05s
epoch 5  | loss: 0.86552 | val_0_rmse: 0.91276 | val_1_rmse: 0.90914 |  0:00:06s
epoch 6  | loss: 0.84895 | val_0_rmse: 0.9046  | val_1_rmse: 0.89867 |  0:00:07s
epoch 7  | loss: 0.82443 | val_0_rmse: 0.91175 | val_1_rmse: 0.90794 |  0:00:08s
epoch 8  | loss: 0.80526 | val_0_rmse: 0.88998 | val_1_rmse: 0.88699 |  0:00:09s
epoch 9  | loss: 0.78874 | val_0_rmse: 0.89697 | val_1_rmse: 0.89695 |  0:00:10s
epoch 10 | loss: 0.7757  | val_0_rmse: 0.90126 | val_1_rmse: 0.89892 |  0:00:10s
epoch 11 | loss: 0.75913 | val_0_rmse: 0.9121  | val_1_rmse: 0.91157 |  0:00:11s
epoch 12 | loss: 0.75379 | val_0_rmse: 0.88636 | val_1_rmse: 0.87953 |  0:00:12s
epoch 13 | loss: 0.75358 | val_0_rmse: 0.88708 | val_1_rmse: 0.88737 |  0:00:14s
epoch 14 | loss: 0.74552 | val_0_rmse: 0.86747 | val_1_rmse: 0.86379 |  0:00:15s
epoch 15 | loss: 0.72745 | val_0_rmse: 0.87567 | val_1_rmse: 0.87667 |  0:00:16s
epoch 16 | loss: 0.72225 | val_0_rmse: 0.8594  | val_1_rmse: 0.85867 |  0:00:17s
epoch 17 | loss: 0.70999 | val_0_rmse: 0.86204 | val_1_rmse: 0.86066 |  0:00:17s
epoch 18 | loss: 0.70065 | val_0_rmse: 0.86042 | val_1_rmse: 0.86258 |  0:00:18s
epoch 19 | loss: 0.6986  | val_0_rmse: 0.85895 | val_1_rmse: 0.86393 |  0:00:19s
epoch 20 | loss: 0.69306 | val_0_rmse: 0.84928 | val_1_rmse: 0.85411 |  0:00:21s
epoch 21 | loss: 0.68827 | val_0_rmse: 0.84802 | val_1_rmse: 0.84868 |  0:00:22s
epoch 22 | loss: 0.6858  | val_0_rmse: 0.85069 | val_1_rmse: 0.85784 |  0:00:23s
epoch 23 | loss: 0.67214 | val_0_rmse: 0.84109 | val_1_rmse: 0.84745 |  0:00:24s
epoch 24 | loss: 0.6767  | val_0_rmse: 0.84668 | val_1_rmse: 0.85947 |  0:00:24s
epoch 25 | loss: 0.67221 | val_0_rmse: 0.83488 | val_1_rmse: 0.84413 |  0:00:25s
epoch 26 | loss: 0.65628 | val_0_rmse: 0.83543 | val_1_rmse: 0.84878 |  0:00:26s
epoch 27 | loss: 0.64956 | val_0_rmse: 0.83741 | val_1_rmse: 0.84528 |  0:00:27s
epoch 28 | loss: 0.64585 | val_0_rmse: 0.82368 | val_1_rmse: 0.84248 |  0:00:28s
epoch 29 | loss: 0.66229 | val_0_rmse: 0.83207 | val_1_rmse: 0.85203 |  0:00:30s
epoch 30 | loss: 0.66513 | val_0_rmse: 0.8283  | val_1_rmse: 0.83778 |  0:00:31s
epoch 31 | loss: 0.67065 | val_0_rmse: 0.82853 | val_1_rmse: 0.8479  |  0:00:32s
epoch 32 | loss: 0.65866 | val_0_rmse: 0.82402 | val_1_rmse: 0.83263 |  0:00:32s
epoch 33 | loss: 0.65932 | val_0_rmse: 0.81577 | val_1_rmse: 0.83366 |  0:00:33s
epoch 34 | loss: 0.65076 | val_0_rmse: 0.81312 | val_1_rmse: 0.82947 |  0:00:34s
epoch 35 | loss: 0.64949 | val_0_rmse: 0.80498 | val_1_rmse: 0.82317 |  0:00:35s
epoch 36 | loss: 0.63552 | val_0_rmse: 0.79777 | val_1_rmse: 0.82119 |  0:00:36s
epoch 37 | loss: 0.63373 | val_0_rmse: 0.79617 | val_1_rmse: 0.82707 |  0:00:38s
epoch 38 | loss: 0.63379 | val_0_rmse: 0.83442 | val_1_rmse: 0.85305 |  0:00:39s
epoch 39 | loss: 0.66021 | val_0_rmse: 0.80967 | val_1_rmse: 0.83511 |  0:00:40s
epoch 40 | loss: 0.64519 | val_0_rmse: 0.80666 | val_1_rmse: 0.83878 |  0:00:40s
epoch 41 | loss: 0.63144 | val_0_rmse: 0.78824 | val_1_rmse: 0.81819 |  0:00:41s
epoch 42 | loss: 0.62407 | val_0_rmse: 0.78514 | val_1_rmse: 0.82705 |  0:00:42s
epoch 43 | loss: 0.61907 | val_0_rmse: 0.7792  | val_1_rmse: 0.81926 |  0:00:43s
epoch 44 | loss: 0.61127 | val_0_rmse: 0.77269 | val_1_rmse: 0.81891 |  0:00:44s
epoch 45 | loss: 0.61715 | val_0_rmse: 0.77371 | val_1_rmse: 0.81948 |  0:00:46s
epoch 46 | loss: 0.61114 | val_0_rmse: 0.78838 | val_1_rmse: 0.82551 |  0:00:46s
epoch 47 | loss: 0.60772 | val_0_rmse: 0.76661 | val_1_rmse: 0.81086 |  0:00:47s
epoch 48 | loss: 0.60438 | val_0_rmse: 0.76304 | val_1_rmse: 0.81414 |  0:00:48s
epoch 49 | loss: 0.61631 | val_0_rmse: 0.76337 | val_1_rmse: 0.82874 |  0:00:49s
epoch 50 | loss: 0.60545 | val_0_rmse: 0.76259 | val_1_rmse: 0.81651 |  0:00:50s
epoch 51 | loss: 0.5905  | val_0_rmse: 0.75321 | val_1_rmse: 0.81088 |  0:00:51s
epoch 52 | loss: 0.59056 | val_0_rmse: 0.75728 | val_1_rmse: 0.81911 |  0:00:52s
epoch 53 | loss: 0.6037  | val_0_rmse: 0.75954 | val_1_rmse: 0.81455 |  0:00:53s
epoch 54 | loss: 0.5988  | val_0_rmse: 0.75301 | val_1_rmse: 0.81699 |  0:00:54s
epoch 55 | loss: 0.59348 | val_0_rmse: 0.74625 | val_1_rmse: 0.81856 |  0:00:55s
epoch 56 | loss: 0.58739 | val_0_rmse: 0.74953 | val_1_rmse: 0.82778 |  0:00:56s
epoch 57 | loss: 0.5879  | val_0_rmse: 0.74272 | val_1_rmse: 0.8242  |  0:00:57s
epoch 58 | loss: 0.5806  | val_0_rmse: 0.73889 | val_1_rmse: 0.82123 |  0:00:58s
epoch 59 | loss: 0.57941 | val_0_rmse: 0.74433 | val_1_rmse: 0.82559 |  0:00:59s
epoch 60 | loss: 0.59037 | val_0_rmse: 0.74963 | val_1_rmse: 0.81233 |  0:01:00s
epoch 61 | loss: 0.57598 | val_0_rmse: 0.73099 | val_1_rmse: 0.81959 |  0:01:01s
epoch 62 | loss: 0.57029 | val_0_rmse: 0.73294 | val_1_rmse: 0.83349 |  0:01:02s
epoch 63 | loss: 0.57024 | val_0_rmse: 0.74207 | val_1_rmse: 0.82199 |  0:01:03s
epoch 64 | loss: 0.56901 | val_0_rmse: 0.72596 | val_1_rmse: 0.81896 |  0:01:04s
epoch 65 | loss: 0.56579 | val_0_rmse: 0.739   | val_1_rmse: 0.8403  |  0:01:05s
epoch 66 | loss: 0.56106 | val_0_rmse: 0.72845 | val_1_rmse: 0.8218  |  0:01:06s
epoch 67 | loss: 0.56792 | val_0_rmse: 0.7276  | val_1_rmse: 0.82474 |  0:01:07s
epoch 68 | loss: 0.56113 | val_0_rmse: 0.72211 | val_1_rmse: 0.8314  |  0:01:08s
epoch 69 | loss: 0.55147 | val_0_rmse: 0.75544 | val_1_rmse: 0.83656 |  0:01:09s
epoch 70 | loss: 0.5586  | val_0_rmse: 0.72246 | val_1_rmse: 0.81927 |  0:01:10s
epoch 71 | loss: 0.55081 | val_0_rmse: 0.71602 | val_1_rmse: 0.82526 |  0:01:11s
epoch 72 | loss: 0.55557 | val_0_rmse: 0.71668 | val_1_rmse: 0.82071 |  0:01:12s
epoch 73 | loss: 0.54478 | val_0_rmse: 0.72186 | val_1_rmse: 0.83488 |  0:01:13s
epoch 74 | loss: 0.54704 | val_0_rmse: 0.71393 | val_1_rmse: 0.81734 |  0:01:14s
epoch 75 | loss: 0.54846 | val_0_rmse: 0.71832 | val_1_rmse: 0.827   |  0:01:15s
epoch 76 | loss: 0.54616 | val_0_rmse: 0.70535 | val_1_rmse: 0.82398 |  0:01:16s
epoch 77 | loss: 0.53754 | val_0_rmse: 0.70944 | val_1_rmse: 0.83262 |  0:01:17s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.81086
Best weights from best epoch are automatically used!
ended training at: 04:07:00
Feature importance:
Mean squared error is of 0.06234200838852148
Mean absolute error:0.17919061632267538
MAPE:0.19296207098830373
R2 score:0.27685718674743787
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:07:01
epoch 0  | loss: 2.51667 | val_0_rmse: 0.99549 | val_1_rmse: 0.99135 |  0:00:00s
epoch 1  | loss: 1.04239 | val_0_rmse: 0.99459 | val_1_rmse: 0.99021 |  0:00:01s
epoch 2  | loss: 0.96571 | val_0_rmse: 0.97848 | val_1_rmse: 0.97464 |  0:00:02s
epoch 3  | loss: 0.90551 | val_0_rmse: 0.94528 | val_1_rmse: 0.94507 |  0:00:03s
epoch 4  | loss: 0.86578 | val_0_rmse: 0.93425 | val_1_rmse: 0.935   |  0:00:04s
epoch 5  | loss: 0.83079 | val_0_rmse: 0.93535 | val_1_rmse: 0.93963 |  0:00:05s
epoch 6  | loss: 0.81916 | val_0_rmse: 0.91674 | val_1_rmse: 0.91629 |  0:00:06s
epoch 7  | loss: 0.80089 | val_0_rmse: 0.90681 | val_1_rmse: 0.91256 |  0:00:08s
epoch 8  | loss: 0.78395 | val_0_rmse: 0.902   | val_1_rmse: 0.90975 |  0:00:09s
epoch 9  | loss: 0.775   | val_0_rmse: 0.88979 | val_1_rmse: 0.89978 |  0:00:10s
epoch 10 | loss: 0.76539 | val_0_rmse: 0.89475 | val_1_rmse: 0.90463 |  0:00:10s
epoch 11 | loss: 0.75393 | val_0_rmse: 0.91642 | val_1_rmse: 0.93143 |  0:00:11s
epoch 12 | loss: 0.74688 | val_0_rmse: 0.8837  | val_1_rmse: 0.89223 |  0:00:12s
epoch 13 | loss: 0.73854 | val_0_rmse: 0.87842 | val_1_rmse: 0.88835 |  0:00:13s
epoch 14 | loss: 0.73922 | val_0_rmse: 0.89455 | val_1_rmse: 0.90852 |  0:00:14s
epoch 15 | loss: 0.73441 | val_0_rmse: 0.88586 | val_1_rmse: 0.90062 |  0:00:15s
epoch 16 | loss: 0.76506 | val_0_rmse: 0.88832 | val_1_rmse: 0.90031 |  0:00:17s
epoch 17 | loss: 0.78982 | val_0_rmse: 0.8922  | val_1_rmse: 0.9068  |  0:00:17s
epoch 18 | loss: 0.76374 | val_0_rmse: 0.87478 | val_1_rmse: 0.88439 |  0:00:18s
epoch 19 | loss: 0.73951 | val_0_rmse: 0.87483 | val_1_rmse: 0.89321 |  0:00:19s
epoch 20 | loss: 0.72716 | val_0_rmse: 0.85597 | val_1_rmse: 0.87164 |  0:00:20s
epoch 21 | loss: 0.7282  | val_0_rmse: 0.85424 | val_1_rmse: 0.87627 |  0:00:21s
epoch 22 | loss: 0.71362 | val_0_rmse: 0.85011 | val_1_rmse: 0.86663 |  0:00:22s
epoch 23 | loss: 0.70571 | val_0_rmse: 0.8484  | val_1_rmse: 0.86561 |  0:00:23s
epoch 24 | loss: 0.70087 | val_0_rmse: 0.8599  | val_1_rmse: 0.88039 |  0:00:25s
epoch 25 | loss: 0.71349 | val_0_rmse: 0.85488 | val_1_rmse: 0.87697 |  0:00:25s
epoch 26 | loss: 0.70372 | val_0_rmse: 0.84281 | val_1_rmse: 0.8578  |  0:00:26s
epoch 27 | loss: 0.70425 | val_0_rmse: 0.84096 | val_1_rmse: 0.85911 |  0:00:27s
epoch 28 | loss: 0.68814 | val_0_rmse: 0.83978 | val_1_rmse: 0.85823 |  0:00:28s
epoch 29 | loss: 0.6899  | val_0_rmse: 0.84237 | val_1_rmse: 0.86728 |  0:00:29s
epoch 30 | loss: 0.68346 | val_0_rmse: 0.82947 | val_1_rmse: 0.85284 |  0:00:30s
epoch 31 | loss: 0.68011 | val_0_rmse: 0.85932 | val_1_rmse: 0.88449 |  0:00:31s
epoch 32 | loss: 0.69641 | val_0_rmse: 0.8377  | val_1_rmse: 0.86307 |  0:00:32s
epoch 33 | loss: 0.69223 | val_0_rmse: 0.85436 | val_1_rmse: 0.869   |  0:00:33s
epoch 34 | loss: 0.68248 | val_0_rmse: 0.82646 | val_1_rmse: 0.85037 |  0:00:34s
epoch 35 | loss: 0.68334 | val_0_rmse: 0.83532 | val_1_rmse: 0.86004 |  0:00:35s
epoch 36 | loss: 0.69272 | val_0_rmse: 0.83417 | val_1_rmse: 0.86037 |  0:00:36s
epoch 37 | loss: 0.67885 | val_0_rmse: 0.83266 | val_1_rmse: 0.86466 |  0:00:37s
epoch 38 | loss: 0.67441 | val_0_rmse: 0.82723 | val_1_rmse: 0.8658  |  0:00:38s
epoch 39 | loss: 0.67003 | val_0_rmse: 0.82826 | val_1_rmse: 0.87283 |  0:00:39s
epoch 40 | loss: 0.66947 | val_0_rmse: 0.81736 | val_1_rmse: 0.84954 |  0:00:40s
epoch 41 | loss: 0.66399 | val_0_rmse: 0.80826 | val_1_rmse: 0.85163 |  0:00:41s
epoch 42 | loss: 0.65444 | val_0_rmse: 0.8045  | val_1_rmse: 0.85553 |  0:00:42s
epoch 43 | loss: 0.65521 | val_0_rmse: 0.81686 | val_1_rmse: 0.84865 |  0:00:43s
epoch 44 | loss: 0.64728 | val_0_rmse: 0.79645 | val_1_rmse: 0.85673 |  0:00:44s
epoch 45 | loss: 0.64035 | val_0_rmse: 0.79636 | val_1_rmse: 0.85838 |  0:00:45s
epoch 46 | loss: 0.63703 | val_0_rmse: 0.8228  | val_1_rmse: 0.87859 |  0:00:46s
epoch 47 | loss: 0.63789 | val_0_rmse: 0.79572 | val_1_rmse: 0.85363 |  0:00:47s
epoch 48 | loss: 0.62858 | val_0_rmse: 0.79499 | val_1_rmse: 0.85211 |  0:00:48s
epoch 49 | loss: 0.63232 | val_0_rmse: 0.7831  | val_1_rmse: 0.8602  |  0:00:49s
epoch 50 | loss: 0.62306 | val_0_rmse: 0.77612 | val_1_rmse: 0.85008 |  0:00:50s
epoch 51 | loss: 0.6192  | val_0_rmse: 0.77972 | val_1_rmse: 0.84203 |  0:00:51s
epoch 52 | loss: 0.62192 | val_0_rmse: 0.77386 | val_1_rmse: 0.84437 |  0:00:52s
epoch 53 | loss: 0.62031 | val_0_rmse: 0.77976 | val_1_rmse: 0.84377 |  0:00:53s
epoch 54 | loss: 0.61259 | val_0_rmse: 0.77468 | val_1_rmse: 0.84731 |  0:00:54s
epoch 55 | loss: 0.61767 | val_0_rmse: 0.77864 | val_1_rmse: 0.85195 |  0:00:55s
epoch 56 | loss: 0.61361 | val_0_rmse: 0.7723  | val_1_rmse: 0.87035 |  0:00:56s
epoch 57 | loss: 0.61532 | val_0_rmse: 0.77502 | val_1_rmse: 0.86495 |  0:00:57s
epoch 58 | loss: 0.61457 | val_0_rmse: 0.76528 | val_1_rmse: 0.85556 |  0:00:58s
epoch 59 | loss: 0.61252 | val_0_rmse: 0.77662 | val_1_rmse: 0.86773 |  0:00:59s
epoch 60 | loss: 0.60988 | val_0_rmse: 0.76497 | val_1_rmse: 0.85455 |  0:01:00s
epoch 61 | loss: 0.60064 | val_0_rmse: 0.76113 | val_1_rmse: 0.85934 |  0:01:01s
epoch 62 | loss: 0.60004 | val_0_rmse: 0.75545 | val_1_rmse: 0.87105 |  0:01:02s
epoch 63 | loss: 0.59863 | val_0_rmse: 0.75174 | val_1_rmse: 0.87384 |  0:01:03s
epoch 64 | loss: 0.59308 | val_0_rmse: 0.76365 | val_1_rmse: 0.86505 |  0:01:04s
epoch 65 | loss: 0.59564 | val_0_rmse: 0.75569 | val_1_rmse: 0.85979 |  0:01:05s
epoch 66 | loss: 0.59118 | val_0_rmse: 0.74516 | val_1_rmse: 0.87011 |  0:01:06s
epoch 67 | loss: 0.58173 | val_0_rmse: 0.74756 | val_1_rmse: 0.87591 |  0:01:07s
epoch 68 | loss: 0.58165 | val_0_rmse: 0.74167 | val_1_rmse: 0.87846 |  0:01:08s
epoch 69 | loss: 0.57871 | val_0_rmse: 0.75628 | val_1_rmse: 0.88774 |  0:01:09s
epoch 70 | loss: 0.58694 | val_0_rmse: 0.75811 | val_1_rmse: 0.88043 |  0:01:10s
epoch 71 | loss: 0.57466 | val_0_rmse: 0.75906 | val_1_rmse: 0.89946 |  0:01:11s
epoch 72 | loss: 0.58509 | val_0_rmse: 0.74505 | val_1_rmse: 0.88127 |  0:01:12s
epoch 73 | loss: 0.57971 | val_0_rmse: 0.76143 | val_1_rmse: 0.89147 |  0:01:13s
epoch 74 | loss: 0.58536 | val_0_rmse: 0.74789 | val_1_rmse: 0.88285 |  0:01:14s
epoch 75 | loss: 0.58355 | val_0_rmse: 0.75284 | val_1_rmse: 0.87355 |  0:01:15s
epoch 76 | loss: 0.5775  | val_0_rmse: 0.75896 | val_1_rmse: 0.90333 |  0:01:16s
epoch 77 | loss: 0.58007 | val_0_rmse: 0.74779 | val_1_rmse: 0.86706 |  0:01:17s
epoch 78 | loss: 0.59221 | val_0_rmse: 0.76097 | val_1_rmse: 0.87418 |  0:01:18s
epoch 79 | loss: 0.59047 | val_0_rmse: 0.76977 | val_1_rmse: 0.88104 |  0:01:19s
epoch 80 | loss: 0.59946 | val_0_rmse: 0.76756 | val_1_rmse: 0.86925 |  0:01:20s
epoch 81 | loss: 0.59519 | val_0_rmse: 0.76338 | val_1_rmse: 0.88041 |  0:01:21s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.84203
Best weights from best epoch are automatically used!
ended training at: 04:08:23
Feature importance:
Mean squared error is of 0.09457216189145234
Mean absolute error:0.1829566313384571
MAPE:0.1978809092697396
R2 score:0.22132452547196668
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:08:24
epoch 0  | loss: 2.72215 | val_0_rmse: 1.0101  | val_1_rmse: 0.96008 |  0:00:00s
epoch 1  | loss: 1.06641 | val_0_rmse: 1.01513 | val_1_rmse: 0.96698 |  0:00:01s
epoch 2  | loss: 1.00386 | val_0_rmse: 1.03096 | val_1_rmse: 0.97708 |  0:00:03s
epoch 3  | loss: 0.97879 | val_0_rmse: 0.99206 | val_1_rmse: 0.93963 |  0:00:04s
epoch 4  | loss: 0.9538  | val_0_rmse: 0.96807 | val_1_rmse: 0.91274 |  0:00:05s
epoch 5  | loss: 0.92742 | val_0_rmse: 0.98406 | val_1_rmse: 0.92998 |  0:00:06s
epoch 6  | loss: 0.90443 | val_0_rmse: 0.97416 | val_1_rmse: 0.91903 |  0:00:07s
epoch 7  | loss: 0.88184 | val_0_rmse: 0.94325 | val_1_rmse: 0.88755 |  0:00:08s
epoch 8  | loss: 0.86486 | val_0_rmse: 0.93675 | val_1_rmse: 0.87891 |  0:00:09s
epoch 9  | loss: 0.84905 | val_0_rmse: 0.92251 | val_1_rmse: 0.86601 |  0:00:10s
epoch 10 | loss: 0.82529 | val_0_rmse: 0.91568 | val_1_rmse: 0.86125 |  0:00:11s
epoch 11 | loss: 0.80079 | val_0_rmse: 0.90276 | val_1_rmse: 0.8535  |  0:00:12s
epoch 12 | loss: 0.78118 | val_0_rmse: 0.89633 | val_1_rmse: 0.84676 |  0:00:13s
epoch 13 | loss: 0.76318 | val_0_rmse: 0.88353 | val_1_rmse: 0.84119 |  0:00:14s
epoch 14 | loss: 0.75749 | val_0_rmse: 0.88588 | val_1_rmse: 0.83691 |  0:00:15s
epoch 15 | loss: 0.74558 | val_0_rmse: 0.8817  | val_1_rmse: 0.83683 |  0:00:16s
epoch 16 | loss: 0.73624 | val_0_rmse: 0.86808 | val_1_rmse: 0.82368 |  0:00:17s
epoch 17 | loss: 0.72027 | val_0_rmse: 0.88052 | val_1_rmse: 0.84038 |  0:00:18s
epoch 18 | loss: 0.71763 | val_0_rmse: 0.86167 | val_1_rmse: 0.81755 |  0:00:19s
epoch 19 | loss: 0.70649 | val_0_rmse: 0.86565 | val_1_rmse: 0.81988 |  0:00:20s
epoch 20 | loss: 0.70498 | val_0_rmse: 0.86951 | val_1_rmse: 0.82529 |  0:00:21s
epoch 21 | loss: 0.70626 | val_0_rmse: 0.85348 | val_1_rmse: 0.8129  |  0:00:22s
epoch 22 | loss: 0.69496 | val_0_rmse: 0.85416 | val_1_rmse: 0.81252 |  0:00:23s
epoch 23 | loss: 0.68884 | val_0_rmse: 0.84607 | val_1_rmse: 0.81024 |  0:00:24s
epoch 24 | loss: 0.6864  | val_0_rmse: 0.8499  | val_1_rmse: 0.81349 |  0:00:25s
epoch 25 | loss: 0.68727 | val_0_rmse: 0.84771 | val_1_rmse: 0.81312 |  0:00:26s
epoch 26 | loss: 0.67995 | val_0_rmse: 0.84449 | val_1_rmse: 0.80905 |  0:00:27s
epoch 27 | loss: 0.67193 | val_0_rmse: 0.83467 | val_1_rmse: 0.80931 |  0:00:28s
epoch 28 | loss: 0.6689  | val_0_rmse: 0.84068 | val_1_rmse: 0.81827 |  0:00:29s
epoch 29 | loss: 0.66586 | val_0_rmse: 0.83145 | val_1_rmse: 0.80298 |  0:00:30s
epoch 30 | loss: 0.66123 | val_0_rmse: 0.82995 | val_1_rmse: 0.80892 |  0:00:31s
epoch 31 | loss: 0.66453 | val_0_rmse: 0.82367 | val_1_rmse: 0.80141 |  0:00:32s
epoch 32 | loss: 0.65505 | val_0_rmse: 0.82431 | val_1_rmse: 0.80318 |  0:00:33s
epoch 33 | loss: 0.65332 | val_0_rmse: 0.8137  | val_1_rmse: 0.79889 |  0:00:34s
epoch 34 | loss: 0.6508  | val_0_rmse: 0.81005 | val_1_rmse: 0.79864 |  0:00:35s
epoch 35 | loss: 0.64375 | val_0_rmse: 0.8144  | val_1_rmse: 0.80453 |  0:00:36s
epoch 36 | loss: 0.64012 | val_0_rmse: 0.80403 | val_1_rmse: 0.78839 |  0:00:37s
epoch 37 | loss: 0.63479 | val_0_rmse: 0.79665 | val_1_rmse: 0.79322 |  0:00:38s
epoch 38 | loss: 0.62792 | val_0_rmse: 0.80562 | val_1_rmse: 0.79982 |  0:00:39s
epoch 39 | loss: 0.63359 | val_0_rmse: 0.79875 | val_1_rmse: 0.80237 |  0:00:40s
epoch 40 | loss: 0.63429 | val_0_rmse: 0.78817 | val_1_rmse: 0.7969  |  0:00:41s
epoch 41 | loss: 0.6338  | val_0_rmse: 0.79506 | val_1_rmse: 0.79198 |  0:00:42s
epoch 42 | loss: 0.6377  | val_0_rmse: 0.78741 | val_1_rmse: 0.78991 |  0:00:43s
epoch 43 | loss: 0.62455 | val_0_rmse: 0.77553 | val_1_rmse: 0.80322 |  0:00:44s
epoch 44 | loss: 0.61935 | val_0_rmse: 0.77477 | val_1_rmse: 0.79712 |  0:00:45s
epoch 45 | loss: 0.61287 | val_0_rmse: 0.7742  | val_1_rmse: 0.79026 |  0:00:46s
epoch 46 | loss: 0.60735 | val_0_rmse: 0.76579 | val_1_rmse: 0.80829 |  0:00:47s
epoch 47 | loss: 0.61702 | val_0_rmse: 0.77854 | val_1_rmse: 0.79612 |  0:00:48s
epoch 48 | loss: 0.62    | val_0_rmse: 0.76762 | val_1_rmse: 0.79369 |  0:00:49s
epoch 49 | loss: 0.60905 | val_0_rmse: 0.76509 | val_1_rmse: 0.78651 |  0:00:50s
epoch 50 | loss: 0.60947 | val_0_rmse: 0.75867 | val_1_rmse: 0.79197 |  0:00:51s
epoch 51 | loss: 0.5976  | val_0_rmse: 0.76216 | val_1_rmse: 0.79071 |  0:00:52s
epoch 52 | loss: 0.60201 | val_0_rmse: 0.75421 | val_1_rmse: 0.78603 |  0:00:53s
epoch 53 | loss: 0.60008 | val_0_rmse: 0.75927 | val_1_rmse: 0.79741 |  0:00:54s
epoch 54 | loss: 0.5984  | val_0_rmse: 0.7567  | val_1_rmse: 0.78555 |  0:00:55s
epoch 55 | loss: 0.59782 | val_0_rmse: 0.75368 | val_1_rmse: 0.78827 |  0:00:56s
epoch 56 | loss: 0.58578 | val_0_rmse: 0.74764 | val_1_rmse: 0.79388 |  0:00:57s
epoch 57 | loss: 0.59275 | val_0_rmse: 0.74198 | val_1_rmse: 0.79121 |  0:00:58s
epoch 58 | loss: 0.58595 | val_0_rmse: 0.74566 | val_1_rmse: 0.80271 |  0:00:59s
epoch 59 | loss: 0.58825 | val_0_rmse: 0.74722 | val_1_rmse: 0.7985  |  0:01:00s
epoch 60 | loss: 0.58319 | val_0_rmse: 0.73117 | val_1_rmse: 0.79235 |  0:01:01s
epoch 61 | loss: 0.57542 | val_0_rmse: 0.73663 | val_1_rmse: 0.79853 |  0:01:02s
epoch 62 | loss: 0.57107 | val_0_rmse: 0.72927 | val_1_rmse: 0.80339 |  0:01:03s
epoch 63 | loss: 0.56375 | val_0_rmse: 0.72753 | val_1_rmse: 0.79948 |  0:01:04s
epoch 64 | loss: 0.56133 | val_0_rmse: 0.72554 | val_1_rmse: 0.808   |  0:01:04s
epoch 65 | loss: 0.55711 | val_0_rmse: 0.72844 | val_1_rmse: 0.80257 |  0:01:05s
epoch 66 | loss: 0.55416 | val_0_rmse: 0.72852 | val_1_rmse: 0.80545 |  0:01:07s
epoch 67 | loss: 0.55616 | val_0_rmse: 0.75109 | val_1_rmse: 0.81018 |  0:01:08s
epoch 68 | loss: 0.5619  | val_0_rmse: 0.7316  | val_1_rmse: 0.7933  |  0:01:09s
epoch 69 | loss: 0.56615 | val_0_rmse: 0.72609 | val_1_rmse: 0.80984 |  0:01:10s
epoch 70 | loss: 0.56246 | val_0_rmse: 0.72454 | val_1_rmse: 0.80493 |  0:01:10s
epoch 71 | loss: 0.55078 | val_0_rmse: 0.71679 | val_1_rmse: 0.8095  |  0:01:11s
epoch 72 | loss: 0.55796 | val_0_rmse: 0.72635 | val_1_rmse: 0.81313 |  0:01:12s
epoch 73 | loss: 0.55255 | val_0_rmse: 0.72126 | val_1_rmse: 0.7983  |  0:01:13s
epoch 74 | loss: 0.55181 | val_0_rmse: 0.71552 | val_1_rmse: 0.80945 |  0:01:14s
epoch 75 | loss: 0.55454 | val_0_rmse: 0.71247 | val_1_rmse: 0.81397 |  0:01:16s
epoch 76 | loss: 0.54843 | val_0_rmse: 0.71601 | val_1_rmse: 0.81052 |  0:01:17s
epoch 77 | loss: 0.54231 | val_0_rmse: 0.71805 | val_1_rmse: 0.80378 |  0:01:17s
epoch 78 | loss: 0.54506 | val_0_rmse: 0.72316 | val_1_rmse: 0.7968  |  0:01:18s
epoch 79 | loss: 0.55206 | val_0_rmse: 0.71213 | val_1_rmse: 0.80363 |  0:01:19s
epoch 80 | loss: 0.54542 | val_0_rmse: 0.71659 | val_1_rmse: 0.80526 |  0:01:20s
epoch 81 | loss: 0.54629 | val_0_rmse: 0.71489 | val_1_rmse: 0.81375 |  0:01:21s
epoch 82 | loss: 0.53909 | val_0_rmse: 0.71855 | val_1_rmse: 0.80795 |  0:01:22s
epoch 83 | loss: 0.53017 | val_0_rmse: 0.71519 | val_1_rmse: 0.81509 |  0:01:23s
epoch 84 | loss: 0.52462 | val_0_rmse: 0.71289 | val_1_rmse: 0.81307 |  0:01:24s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.78555
Best weights from best epoch are automatically used!
ended training at: 04:09:49
Feature importance:
Mean squared error is of 0.07241666484190228
Mean absolute error:0.1823715242371126
MAPE:0.1914515967332874
R2 score:0.230635608964757
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:09:50
epoch 0  | loss: 2.55558 | val_0_rmse: 1.00133 | val_1_rmse: 0.99082 |  0:00:00s
epoch 1  | loss: 1.05972 | val_0_rmse: 1.0009  | val_1_rmse: 0.99061 |  0:00:01s
epoch 2  | loss: 1.00947 | val_0_rmse: 0.99767 | val_1_rmse: 0.986   |  0:00:02s
epoch 3  | loss: 0.9819  | val_0_rmse: 0.99131 | val_1_rmse: 0.98006 |  0:00:03s
epoch 4  | loss: 0.95229 | val_0_rmse: 0.96702 | val_1_rmse: 0.95142 |  0:00:04s
epoch 5  | loss: 0.92391 | val_0_rmse: 0.95763 | val_1_rmse: 0.94313 |  0:00:05s
epoch 6  | loss: 0.91753 | val_0_rmse: 0.95585 | val_1_rmse: 0.94142 |  0:00:07s
epoch 7  | loss: 0.90042 | val_0_rmse: 0.94891 | val_1_rmse: 0.93158 |  0:00:08s
epoch 8  | loss: 0.86106 | val_0_rmse: 0.94425 | val_1_rmse: 0.92776 |  0:00:09s
epoch 9  | loss: 0.83723 | val_0_rmse: 0.93366 | val_1_rmse: 0.91783 |  0:00:10s
epoch 10 | loss: 0.81869 | val_0_rmse: 0.92416 | val_1_rmse: 0.90522 |  0:00:11s
epoch 11 | loss: 0.8002  | val_0_rmse: 0.89795 | val_1_rmse: 0.87634 |  0:00:12s
epoch 12 | loss: 0.78162 | val_0_rmse: 0.88881 | val_1_rmse: 0.85997 |  0:00:13s
epoch 13 | loss: 0.76782 | val_0_rmse: 0.89276 | val_1_rmse: 0.86281 |  0:00:14s
epoch 14 | loss: 0.77037 | val_0_rmse: 0.89263 | val_1_rmse: 0.87562 |  0:00:15s
epoch 15 | loss: 0.75957 | val_0_rmse: 0.88118 | val_1_rmse: 0.85663 |  0:00:16s
epoch 16 | loss: 0.74865 | val_0_rmse: 0.88024 | val_1_rmse: 0.8526  |  0:00:17s
epoch 17 | loss: 0.73234 | val_0_rmse: 0.87314 | val_1_rmse: 0.84873 |  0:00:18s
epoch 18 | loss: 0.72958 | val_0_rmse: 0.87482 | val_1_rmse: 0.84848 |  0:00:19s
epoch 19 | loss: 0.72757 | val_0_rmse: 0.88207 | val_1_rmse: 0.8512  |  0:00:20s
epoch 20 | loss: 0.7205  | val_0_rmse: 0.8653  | val_1_rmse: 0.84386 |  0:00:21s
epoch 21 | loss: 0.71356 | val_0_rmse: 0.85997 | val_1_rmse: 0.8428  |  0:00:22s
epoch 22 | loss: 0.6979  | val_0_rmse: 0.86805 | val_1_rmse: 0.84943 |  0:00:23s
epoch 23 | loss: 0.686   | val_0_rmse: 0.86369 | val_1_rmse: 0.84534 |  0:00:24s
epoch 24 | loss: 0.7008  | val_0_rmse: 0.86477 | val_1_rmse: 0.84672 |  0:00:25s
epoch 25 | loss: 0.6947  | val_0_rmse: 0.8474  | val_1_rmse: 0.82939 |  0:00:26s
epoch 26 | loss: 0.69271 | val_0_rmse: 0.8505  | val_1_rmse: 0.8433  |  0:00:27s
epoch 27 | loss: 0.69463 | val_0_rmse: 0.84701 | val_1_rmse: 0.84279 |  0:00:28s
epoch 28 | loss: 0.68665 | val_0_rmse: 0.84109 | val_1_rmse: 0.83247 |  0:00:29s
epoch 29 | loss: 0.69081 | val_0_rmse: 0.84632 | val_1_rmse: 0.83566 |  0:00:30s
epoch 30 | loss: 0.68788 | val_0_rmse: 0.84394 | val_1_rmse: 0.8334  |  0:00:31s
epoch 31 | loss: 0.67465 | val_0_rmse: 0.82949 | val_1_rmse: 0.82916 |  0:00:32s
epoch 32 | loss: 0.67493 | val_0_rmse: 0.82752 | val_1_rmse: 0.83598 |  0:00:33s
epoch 33 | loss: 0.66543 | val_0_rmse: 0.82733 | val_1_rmse: 0.83106 |  0:00:34s
epoch 34 | loss: 0.66204 | val_0_rmse: 0.82241 | val_1_rmse: 0.83308 |  0:00:35s
epoch 35 | loss: 0.66022 | val_0_rmse: 0.8387  | val_1_rmse: 0.8402  |  0:00:36s
epoch 36 | loss: 0.64757 | val_0_rmse: 0.82145 | val_1_rmse: 0.82529 |  0:00:37s
epoch 37 | loss: 0.65321 | val_0_rmse: 0.8118  | val_1_rmse: 0.82427 |  0:00:38s
epoch 38 | loss: 0.64833 | val_0_rmse: 0.80385 | val_1_rmse: 0.82295 |  0:00:39s
epoch 39 | loss: 0.64062 | val_0_rmse: 0.82358 | val_1_rmse: 0.82961 |  0:00:40s
epoch 40 | loss: 0.63069 | val_0_rmse: 0.80957 | val_1_rmse: 0.83355 |  0:00:41s
epoch 41 | loss: 0.63119 | val_0_rmse: 0.7961  | val_1_rmse: 0.81507 |  0:00:42s
epoch 42 | loss: 0.63406 | val_0_rmse: 0.7987  | val_1_rmse: 0.83035 |  0:00:43s
epoch 43 | loss: 0.64221 | val_0_rmse: 0.79761 | val_1_rmse: 0.83116 |  0:00:44s
epoch 44 | loss: 0.62845 | val_0_rmse: 0.82946 | val_1_rmse: 0.87916 |  0:00:45s
epoch 45 | loss: 0.6289  | val_0_rmse: 0.78637 | val_1_rmse: 0.82995 |  0:00:46s
epoch 46 | loss: 0.62263 | val_0_rmse: 0.77881 | val_1_rmse: 0.82457 |  0:00:47s
epoch 47 | loss: 0.6182  | val_0_rmse: 0.78664 | val_1_rmse: 0.83324 |  0:00:48s
epoch 48 | loss: 0.62564 | val_0_rmse: 0.78549 | val_1_rmse: 0.83472 |  0:00:49s
epoch 49 | loss: 0.61759 | val_0_rmse: 0.78469 | val_1_rmse: 0.84214 |  0:00:50s
epoch 50 | loss: 0.61021 | val_0_rmse: 0.80643 | val_1_rmse: 0.83628 |  0:00:51s
epoch 51 | loss: 0.62395 | val_0_rmse: 0.76969 | val_1_rmse: 0.83059 |  0:00:52s
epoch 52 | loss: 0.60394 | val_0_rmse: 0.77667 | val_1_rmse: 0.85113 |  0:00:53s
epoch 53 | loss: 0.60342 | val_0_rmse: 0.7597  | val_1_rmse: 0.82594 |  0:00:54s
epoch 54 | loss: 0.60544 | val_0_rmse: 0.76525 | val_1_rmse: 0.82739 |  0:00:55s
epoch 55 | loss: 0.61011 | val_0_rmse: 0.76426 | val_1_rmse: 0.81662 |  0:00:56s
epoch 56 | loss: 0.60089 | val_0_rmse: 0.75368 | val_1_rmse: 0.8229  |  0:00:57s
epoch 57 | loss: 0.59486 | val_0_rmse: 0.75553 | val_1_rmse: 0.83002 |  0:00:58s
epoch 58 | loss: 0.58608 | val_0_rmse: 0.7662  | val_1_rmse: 0.83173 |  0:00:59s
epoch 59 | loss: 0.58829 | val_0_rmse: 0.76395 | val_1_rmse: 0.85538 |  0:01:00s
epoch 60 | loss: 0.59726 | val_0_rmse: 0.7613  | val_1_rmse: 0.84231 |  0:01:01s
epoch 61 | loss: 0.59438 | val_0_rmse: 0.77697 | val_1_rmse: 0.84717 |  0:01:02s
epoch 62 | loss: 0.58936 | val_0_rmse: 0.74466 | val_1_rmse: 0.84037 |  0:01:03s
epoch 63 | loss: 0.57937 | val_0_rmse: 0.74508 | val_1_rmse: 0.83338 |  0:01:04s
epoch 64 | loss: 0.57526 | val_0_rmse: 0.76095 | val_1_rmse: 0.86505 |  0:01:05s
epoch 65 | loss: 0.58048 | val_0_rmse: 0.74698 | val_1_rmse: 0.82445 |  0:01:06s
epoch 66 | loss: 0.5856  | val_0_rmse: 0.74622 | val_1_rmse: 0.82959 |  0:01:07s
epoch 67 | loss: 0.58807 | val_0_rmse: 0.77589 | val_1_rmse: 0.84162 |  0:01:08s
epoch 68 | loss: 0.58018 | val_0_rmse: 0.74194 | val_1_rmse: 0.83695 |  0:01:09s
epoch 69 | loss: 0.56878 | val_0_rmse: 0.72661 | val_1_rmse: 0.84291 |  0:01:10s
epoch 70 | loss: 0.564   | val_0_rmse: 0.72872 | val_1_rmse: 0.85759 |  0:01:11s
epoch 71 | loss: 0.5568  | val_0_rmse: 0.75809 | val_1_rmse: 0.91957 |  0:01:12s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.81507
Best weights from best epoch are automatically used!
ended training at: 04:11:02
Feature importance:
Mean squared error is of 0.0597606782162677
Mean absolute error:0.18075215880843945
MAPE:0.19752532940241652
R2 score:0.31242282483546535
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:11:05
epoch 0  | loss: 1.3159  | val_0_rmse: 0.8138  | val_1_rmse: 0.81518 |  0:00:00s
epoch 1  | loss: 0.62081 | val_0_rmse: 0.88982 | val_1_rmse: 0.90969 |  0:00:01s
epoch 2  | loss: 0.51816 | val_0_rmse: 0.78466 | val_1_rmse: 0.799   |  0:00:02s
epoch 3  | loss: 0.47724 | val_0_rmse: 0.67198 | val_1_rmse: 0.66463 |  0:00:03s
epoch 4  | loss: 0.40925 | val_0_rmse: 0.66613 | val_1_rmse: 0.67856 |  0:00:04s
epoch 5  | loss: 0.35279 | val_0_rmse: 0.63175 | val_1_rmse: 0.63841 |  0:00:05s
epoch 6  | loss: 0.30218 | val_0_rmse: 0.57556 | val_1_rmse: 0.57283 |  0:00:06s
epoch 7  | loss: 0.28244 | val_0_rmse: 0.57111 | val_1_rmse: 0.57438 |  0:00:07s
epoch 8  | loss: 0.27689 | val_0_rmse: 0.56727 | val_1_rmse: 0.57246 |  0:00:08s
epoch 9  | loss: 0.28245 | val_0_rmse: 0.65218 | val_1_rmse: 0.65563 |  0:00:09s
epoch 10 | loss: 0.27287 | val_0_rmse: 0.57707 | val_1_rmse: 0.58023 |  0:00:10s
epoch 11 | loss: 0.25249 | val_0_rmse: 0.57801 | val_1_rmse: 0.56962 |  0:00:11s
epoch 12 | loss: 0.25508 | val_0_rmse: 0.59726 | val_1_rmse: 0.58941 |  0:00:11s
epoch 13 | loss: 0.24886 | val_0_rmse: 0.60789 | val_1_rmse: 0.60821 |  0:00:12s
epoch 14 | loss: 0.25876 | val_0_rmse: 0.60977 | val_1_rmse: 0.60827 |  0:00:13s
epoch 15 | loss: 0.23332 | val_0_rmse: 0.55618 | val_1_rmse: 0.55749 |  0:00:14s
epoch 16 | loss: 0.22251 | val_0_rmse: 0.53916 | val_1_rmse: 0.538   |  0:00:15s
epoch 17 | loss: 0.22527 | val_0_rmse: 0.5058  | val_1_rmse: 0.50861 |  0:00:16s
epoch 18 | loss: 0.23232 | val_0_rmse: 0.48688 | val_1_rmse: 0.48315 |  0:00:17s
epoch 19 | loss: 0.21483 | val_0_rmse: 0.48987 | val_1_rmse: 0.48839 |  0:00:18s
epoch 20 | loss: 0.19437 | val_0_rmse: 0.45873 | val_1_rmse: 0.46366 |  0:00:19s
epoch 21 | loss: 0.19432 | val_0_rmse: 0.50599 | val_1_rmse: 0.50738 |  0:00:20s
epoch 22 | loss: 0.18948 | val_0_rmse: 0.44806 | val_1_rmse: 0.45256 |  0:00:21s
epoch 23 | loss: 0.18576 | val_0_rmse: 0.44133 | val_1_rmse: 0.44752 |  0:00:22s
epoch 24 | loss: 0.19327 | val_0_rmse: 0.4577  | val_1_rmse: 0.46075 |  0:00:22s
epoch 25 | loss: 0.18591 | val_0_rmse: 0.45364 | val_1_rmse: 0.46225 |  0:00:23s
epoch 26 | loss: 0.18471 | val_0_rmse: 0.43151 | val_1_rmse: 0.43603 |  0:00:24s
epoch 27 | loss: 0.18304 | val_0_rmse: 0.44385 | val_1_rmse: 0.45671 |  0:00:25s
epoch 28 | loss: 0.1834  | val_0_rmse: 0.44129 | val_1_rmse: 0.44946 |  0:00:26s
epoch 29 | loss: 0.17735 | val_0_rmse: 0.44443 | val_1_rmse: 0.45071 |  0:00:27s
epoch 30 | loss: 0.1796  | val_0_rmse: 0.43729 | val_1_rmse: 0.44096 |  0:00:28s
epoch 31 | loss: 0.18183 | val_0_rmse: 0.41308 | val_1_rmse: 0.41666 |  0:00:29s
epoch 32 | loss: 0.17456 | val_0_rmse: 0.42071 | val_1_rmse: 0.42464 |  0:00:30s
epoch 33 | loss: 0.18263 | val_0_rmse: 0.43759 | val_1_rmse: 0.44136 |  0:00:31s
epoch 34 | loss: 0.19599 | val_0_rmse: 0.40848 | val_1_rmse: 0.41091 |  0:00:32s
epoch 35 | loss: 0.18417 | val_0_rmse: 0.40846 | val_1_rmse: 0.41317 |  0:00:33s
epoch 36 | loss: 0.17829 | val_0_rmse: 0.41525 | val_1_rmse: 0.42137 |  0:00:34s
epoch 37 | loss: 0.18378 | val_0_rmse: 0.44756 | val_1_rmse: 0.4517  |  0:00:34s
epoch 38 | loss: 0.18703 | val_0_rmse: 0.41692 | val_1_rmse: 0.41848 |  0:00:35s
epoch 39 | loss: 0.18272 | val_0_rmse: 0.40396 | val_1_rmse: 0.40635 |  0:00:36s
epoch 40 | loss: 0.17946 | val_0_rmse: 0.41591 | val_1_rmse: 0.41773 |  0:00:37s
epoch 41 | loss: 0.17293 | val_0_rmse: 0.44105 | val_1_rmse: 0.44661 |  0:00:38s
epoch 42 | loss: 0.17408 | val_0_rmse: 0.39519 | val_1_rmse: 0.40248 |  0:00:39s
epoch 43 | loss: 0.16366 | val_0_rmse: 0.38213 | val_1_rmse: 0.38588 |  0:00:40s
epoch 44 | loss: 0.15839 | val_0_rmse: 0.38185 | val_1_rmse: 0.38596 |  0:00:41s
epoch 45 | loss: 0.1648  | val_0_rmse: 0.38566 | val_1_rmse: 0.38826 |  0:00:42s
epoch 46 | loss: 0.16756 | val_0_rmse: 0.40203 | val_1_rmse: 0.40373 |  0:00:43s
epoch 47 | loss: 0.16414 | val_0_rmse: 0.42104 | val_1_rmse: 0.4231  |  0:00:44s
epoch 48 | loss: 0.16434 | val_0_rmse: 0.37741 | val_1_rmse: 0.38352 |  0:00:45s
epoch 49 | loss: 0.16622 | val_0_rmse: 0.43221 | val_1_rmse: 0.43886 |  0:00:45s
epoch 50 | loss: 0.15873 | val_0_rmse: 0.37664 | val_1_rmse: 0.38361 |  0:00:46s
epoch 51 | loss: 0.15911 | val_0_rmse: 0.38037 | val_1_rmse: 0.38553 |  0:00:47s
epoch 52 | loss: 0.15513 | val_0_rmse: 0.3759  | val_1_rmse: 0.38203 |  0:00:48s
epoch 53 | loss: 0.15961 | val_0_rmse: 0.4025  | val_1_rmse: 0.40692 |  0:00:49s
epoch 54 | loss: 0.19426 | val_0_rmse: 0.45858 | val_1_rmse: 0.45962 |  0:00:50s
epoch 55 | loss: 0.19418 | val_0_rmse: 0.41937 | val_1_rmse: 0.42065 |  0:00:51s
epoch 56 | loss: 0.17179 | val_0_rmse: 0.39622 | val_1_rmse: 0.40158 |  0:00:52s
epoch 57 | loss: 0.19751 | val_0_rmse: 0.44357 | val_1_rmse: 0.45396 |  0:00:53s
epoch 58 | loss: 0.20103 | val_0_rmse: 0.44426 | val_1_rmse: 0.45796 |  0:00:54s
epoch 59 | loss: 0.19329 | val_0_rmse: 0.44015 | val_1_rmse: 0.44521 |  0:00:55s
epoch 60 | loss: 0.18902 | val_0_rmse: 0.40624 | val_1_rmse: 0.41702 |  0:00:56s
epoch 61 | loss: 0.17114 | val_0_rmse: 0.40627 | val_1_rmse: 0.41493 |  0:00:57s
epoch 62 | loss: 0.16924 | val_0_rmse: 0.3853  | val_1_rmse: 0.3926  |  0:00:58s
epoch 63 | loss: 0.16584 | val_0_rmse: 0.37709 | val_1_rmse: 0.38355 |  0:00:59s
epoch 64 | loss: 0.16661 | val_0_rmse: 0.38262 | val_1_rmse: 0.38735 |  0:00:59s
epoch 65 | loss: 0.16305 | val_0_rmse: 0.38411 | val_1_rmse: 0.38922 |  0:01:00s
epoch 66 | loss: 0.16153 | val_0_rmse: 0.39456 | val_1_rmse: 0.40517 |  0:01:01s
epoch 67 | loss: 0.15666 | val_0_rmse: 0.3758  | val_1_rmse: 0.38575 |  0:01:02s
epoch 68 | loss: 0.15208 | val_0_rmse: 0.37745 | val_1_rmse: 0.38162 |  0:01:03s
epoch 69 | loss: 0.15356 | val_0_rmse: 0.36842 | val_1_rmse: 0.37742 |  0:01:04s
epoch 70 | loss: 0.15197 | val_0_rmse: 0.36375 | val_1_rmse: 0.37277 |  0:01:05s
epoch 71 | loss: 0.14904 | val_0_rmse: 0.35788 | val_1_rmse: 0.36309 |  0:01:06s
epoch 72 | loss: 0.14979 | val_0_rmse: 0.36051 | val_1_rmse: 0.36751 |  0:01:07s
epoch 73 | loss: 0.15084 | val_0_rmse: 0.4103  | val_1_rmse: 0.42108 |  0:01:08s
epoch 74 | loss: 0.15077 | val_0_rmse: 0.38493 | val_1_rmse: 0.401   |  0:01:09s
epoch 75 | loss: 0.15804 | val_0_rmse: 0.389   | val_1_rmse: 0.40555 |  0:01:10s
epoch 76 | loss: 0.16092 | val_0_rmse: 0.38914 | val_1_rmse: 0.39953 |  0:01:10s
epoch 77 | loss: 0.15864 | val_0_rmse: 0.40141 | val_1_rmse: 0.41124 |  0:01:11s
epoch 78 | loss: 0.1528  | val_0_rmse: 0.3591  | val_1_rmse: 0.37319 |  0:01:12s
epoch 79 | loss: 0.15805 | val_0_rmse: 0.36968 | val_1_rmse: 0.38271 |  0:01:13s
epoch 80 | loss: 0.15426 | val_0_rmse: 0.37672 | val_1_rmse: 0.38506 |  0:01:14s
epoch 81 | loss: 0.16083 | val_0_rmse: 0.38012 | val_1_rmse: 0.38893 |  0:01:15s
epoch 82 | loss: 0.15055 | val_0_rmse: 0.36176 | val_1_rmse: 0.37267 |  0:01:16s
epoch 83 | loss: 0.14361 | val_0_rmse: 0.36842 | val_1_rmse: 0.37365 |  0:01:17s
epoch 84 | loss: 0.14515 | val_0_rmse: 0.37303 | val_1_rmse: 0.38041 |  0:01:18s
epoch 85 | loss: 0.14603 | val_0_rmse: 0.36743 | val_1_rmse: 0.36744 |  0:01:19s
epoch 86 | loss: 0.14387 | val_0_rmse: 0.35593 | val_1_rmse: 0.36602 |  0:01:20s
epoch 87 | loss: 0.14984 | val_0_rmse: 0.37231 | val_1_rmse: 0.38076 |  0:01:21s
epoch 88 | loss: 0.14806 | val_0_rmse: 0.3718  | val_1_rmse: 0.3724  |  0:01:21s
epoch 89 | loss: 0.15104 | val_0_rmse: 0.36622 | val_1_rmse: 0.37602 |  0:01:22s
epoch 90 | loss: 0.14779 | val_0_rmse: 0.39551 | val_1_rmse: 0.40158 |  0:01:23s
epoch 91 | loss: 0.14899 | val_0_rmse: 0.36822 | val_1_rmse: 0.37646 |  0:01:24s
epoch 92 | loss: 0.14337 | val_0_rmse: 0.36854 | val_1_rmse: 0.38077 |  0:01:25s
epoch 93 | loss: 0.14536 | val_0_rmse: 0.35708 | val_1_rmse: 0.3691  |  0:01:26s
epoch 94 | loss: 0.14543 | val_0_rmse: 0.36235 | val_1_rmse: 0.37147 |  0:01:27s
epoch 95 | loss: 0.14539 | val_0_rmse: 0.37695 | val_1_rmse: 0.38619 |  0:01:28s
epoch 96 | loss: 0.14442 | val_0_rmse: 0.37043 | val_1_rmse: 0.37473 |  0:01:29s
epoch 97 | loss: 0.14158 | val_0_rmse: 0.36409 | val_1_rmse: 0.37084 |  0:01:30s
epoch 98 | loss: 0.14697 | val_0_rmse: 0.35909 | val_1_rmse: 0.3691  |  0:01:31s
epoch 99 | loss: 0.14377 | val_0_rmse: 0.36677 | val_1_rmse: 0.37163 |  0:01:32s
epoch 100| loss: 0.14581 | val_0_rmse: 0.36919 | val_1_rmse: 0.37546 |  0:01:33s
epoch 101| loss: 0.14153 | val_0_rmse: 0.35593 | val_1_rmse: 0.36423 |  0:01:33s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.36309
Best weights from best epoch are automatically used!
ended training at: 04:12:39
Feature importance:
Mean squared error is of 0.0796644654290848
Mean absolute error:0.17689512707910127
MAPE:0.20954026102856668
R2 score:0.8398414768473625
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:12:39
epoch 0  | loss: 1.36268 | val_0_rmse: 0.8683  | val_1_rmse: 0.84978 |  0:00:00s
epoch 1  | loss: 0.61223 | val_0_rmse: 0.79534 | val_1_rmse: 0.77999 |  0:00:01s
epoch 2  | loss: 0.43424 | val_0_rmse: 0.67473 | val_1_rmse: 0.65501 |  0:00:02s
epoch 3  | loss: 0.35413 | val_0_rmse: 0.68677 | val_1_rmse: 0.67282 |  0:00:03s
epoch 4  | loss: 0.32919 | val_0_rmse: 0.59921 | val_1_rmse: 0.58019 |  0:00:04s
epoch 5  | loss: 0.32429 | val_0_rmse: 0.58625 | val_1_rmse: 0.56664 |  0:00:05s
epoch 6  | loss: 0.30404 | val_0_rmse: 0.61158 | val_1_rmse: 0.59856 |  0:00:06s
epoch 7  | loss: 0.28841 | val_0_rmse: 0.63917 | val_1_rmse: 0.62066 |  0:00:07s
epoch 8  | loss: 0.26279 | val_0_rmse: 0.5916  | val_1_rmse: 0.57985 |  0:00:08s
epoch 9  | loss: 0.25926 | val_0_rmse: 0.55279 | val_1_rmse: 0.54321 |  0:00:09s
epoch 10 | loss: 0.2548  | val_0_rmse: 0.56173 | val_1_rmse: 0.54673 |  0:00:10s
epoch 11 | loss: 0.23028 | val_0_rmse: 0.53757 | val_1_rmse: 0.5263  |  0:00:11s
epoch 12 | loss: 0.21998 | val_0_rmse: 0.53571 | val_1_rmse: 0.51633 |  0:00:12s
epoch 13 | loss: 0.2129  | val_0_rmse: 0.5167  | val_1_rmse: 0.5034  |  0:00:13s
epoch 14 | loss: 0.21629 | val_0_rmse: 0.51665 | val_1_rmse: 0.49057 |  0:00:13s
epoch 15 | loss: 0.20696 | val_0_rmse: 0.50997 | val_1_rmse: 0.49609 |  0:00:14s
epoch 16 | loss: 0.20861 | val_0_rmse: 0.51905 | val_1_rmse: 0.49811 |  0:00:15s
epoch 17 | loss: 0.21266 | val_0_rmse: 0.52311 | val_1_rmse: 0.50226 |  0:00:16s
epoch 18 | loss: 0.22535 | val_0_rmse: 0.49484 | val_1_rmse: 0.47971 |  0:00:17s
epoch 19 | loss: 0.22165 | val_0_rmse: 0.49077 | val_1_rmse: 0.47608 |  0:00:18s
epoch 20 | loss: 0.22833 | val_0_rmse: 0.51241 | val_1_rmse: 0.49328 |  0:00:19s
epoch 21 | loss: 0.26156 | val_0_rmse: 0.52822 | val_1_rmse: 0.50703 |  0:00:20s
epoch 22 | loss: 0.24094 | val_0_rmse: 0.52057 | val_1_rmse: 0.49988 |  0:00:21s
epoch 23 | loss: 0.22687 | val_0_rmse: 0.4988  | val_1_rmse: 0.47815 |  0:00:22s
epoch 24 | loss: 0.22441 | val_0_rmse: 0.48046 | val_1_rmse: 0.46789 |  0:00:23s
epoch 25 | loss: 0.21188 | val_0_rmse: 0.46946 | val_1_rmse: 0.45371 |  0:00:24s
epoch 26 | loss: 0.20963 | val_0_rmse: 0.47841 | val_1_rmse: 0.46462 |  0:00:25s
epoch 27 | loss: 0.20131 | val_0_rmse: 0.44997 | val_1_rmse: 0.4377  |  0:00:25s
epoch 28 | loss: 0.19711 | val_0_rmse: 0.44584 | val_1_rmse: 0.42856 |  0:00:26s
epoch 29 | loss: 0.20272 | val_0_rmse: 0.44472 | val_1_rmse: 0.4227  |  0:00:27s
epoch 30 | loss: 0.20487 | val_0_rmse: 0.44275 | val_1_rmse: 0.42569 |  0:00:28s
epoch 31 | loss: 0.19653 | val_0_rmse: 0.4539  | val_1_rmse: 0.44534 |  0:00:29s
epoch 32 | loss: 0.19151 | val_0_rmse: 0.42528 | val_1_rmse: 0.41262 |  0:00:30s
epoch 33 | loss: 0.18565 | val_0_rmse: 0.42736 | val_1_rmse: 0.41673 |  0:00:31s
epoch 34 | loss: 0.19233 | val_0_rmse: 0.45904 | val_1_rmse: 0.43291 |  0:00:32s
epoch 35 | loss: 0.23024 | val_0_rmse: 0.45806 | val_1_rmse: 0.44267 |  0:00:33s
epoch 36 | loss: 0.20936 | val_0_rmse: 0.45229 | val_1_rmse: 0.4386  |  0:00:34s
epoch 37 | loss: 0.202   | val_0_rmse: 0.43249 | val_1_rmse: 0.43057 |  0:00:35s
epoch 38 | loss: 0.1987  | val_0_rmse: 0.43664 | val_1_rmse: 0.42742 |  0:00:36s
epoch 39 | loss: 0.19352 | val_0_rmse: 0.46776 | val_1_rmse: 0.46846 |  0:00:37s
epoch 40 | loss: 0.20021 | val_0_rmse: 0.42944 | val_1_rmse: 0.4198  |  0:00:37s
epoch 41 | loss: 0.20302 | val_0_rmse: 0.43096 | val_1_rmse: 0.41586 |  0:00:38s
epoch 42 | loss: 0.19887 | val_0_rmse: 0.42437 | val_1_rmse: 0.41837 |  0:00:39s
epoch 43 | loss: 0.20948 | val_0_rmse: 0.44503 | val_1_rmse: 0.43565 |  0:00:40s
epoch 44 | loss: 0.20979 | val_0_rmse: 0.46658 | val_1_rmse: 0.4509  |  0:00:41s
epoch 45 | loss: 0.21549 | val_0_rmse: 0.44092 | val_1_rmse: 0.43008 |  0:00:42s
epoch 46 | loss: 0.20336 | val_0_rmse: 0.41679 | val_1_rmse: 0.41307 |  0:00:43s
epoch 47 | loss: 0.19447 | val_0_rmse: 0.43388 | val_1_rmse: 0.43243 |  0:00:44s
epoch 48 | loss: 0.19654 | val_0_rmse: 0.42016 | val_1_rmse: 0.41849 |  0:00:45s
epoch 49 | loss: 0.19249 | val_0_rmse: 0.42678 | val_1_rmse: 0.42251 |  0:00:46s
epoch 50 | loss: 0.19311 | val_0_rmse: 0.41523 | val_1_rmse: 0.40823 |  0:00:47s
epoch 51 | loss: 0.18353 | val_0_rmse: 0.4009  | val_1_rmse: 0.38925 |  0:00:48s
epoch 52 | loss: 0.19428 | val_0_rmse: 0.41103 | val_1_rmse: 0.40129 |  0:00:49s
epoch 53 | loss: 0.19282 | val_0_rmse: 0.4095  | val_1_rmse: 0.40239 |  0:00:49s
epoch 54 | loss: 0.18897 | val_0_rmse: 0.42756 | val_1_rmse: 0.4142  |  0:00:50s
epoch 55 | loss: 0.19091 | val_0_rmse: 0.41025 | val_1_rmse: 0.40419 |  0:00:51s
epoch 56 | loss: 0.1907  | val_0_rmse: 0.41226 | val_1_rmse: 0.4019  |  0:00:52s
epoch 57 | loss: 0.18109 | val_0_rmse: 0.412   | val_1_rmse: 0.40408 |  0:00:53s
epoch 58 | loss: 0.18482 | val_0_rmse: 0.42752 | val_1_rmse: 0.41854 |  0:00:54s
epoch 59 | loss: 0.17556 | val_0_rmse: 0.39319 | val_1_rmse: 0.38261 |  0:00:55s
epoch 60 | loss: 0.20496 | val_0_rmse: 0.43891 | val_1_rmse: 0.43024 |  0:00:56s
epoch 61 | loss: 0.19909 | val_0_rmse: 0.44915 | val_1_rmse: 0.43382 |  0:00:57s
epoch 62 | loss: 0.19849 | val_0_rmse: 0.44321 | val_1_rmse: 0.43987 |  0:00:58s
epoch 63 | loss: 0.20591 | val_0_rmse: 0.45445 | val_1_rmse: 0.44243 |  0:00:59s
epoch 64 | loss: 0.18879 | val_0_rmse: 0.41597 | val_1_rmse: 0.40667 |  0:01:00s
epoch 65 | loss: 0.17661 | val_0_rmse: 0.40316 | val_1_rmse: 0.39551 |  0:01:01s
epoch 66 | loss: 0.17584 | val_0_rmse: 0.39737 | val_1_rmse: 0.39044 |  0:01:02s
epoch 67 | loss: 0.17061 | val_0_rmse: 0.38802 | val_1_rmse: 0.37759 |  0:01:02s
epoch 68 | loss: 0.17109 | val_0_rmse: 0.38802 | val_1_rmse: 0.38049 |  0:01:03s
epoch 69 | loss: 0.16746 | val_0_rmse: 0.39408 | val_1_rmse: 0.38615 |  0:01:04s
epoch 70 | loss: 0.17298 | val_0_rmse: 0.38691 | val_1_rmse: 0.3761  |  0:01:05s
epoch 71 | loss: 0.16361 | val_0_rmse: 0.42071 | val_1_rmse: 0.40749 |  0:01:06s
epoch 72 | loss: 0.17008 | val_0_rmse: 0.4027  | val_1_rmse: 0.39168 |  0:01:07s
epoch 73 | loss: 0.16948 | val_0_rmse: 0.38088 | val_1_rmse: 0.37188 |  0:01:08s
epoch 74 | loss: 0.15933 | val_0_rmse: 0.37898 | val_1_rmse: 0.3732  |  0:01:09s
epoch 75 | loss: 0.15977 | val_0_rmse: 0.38321 | val_1_rmse: 0.37919 |  0:01:10s
epoch 76 | loss: 0.16138 | val_0_rmse: 0.40751 | val_1_rmse: 0.39872 |  0:01:11s
epoch 77 | loss: 0.15814 | val_0_rmse: 0.3838  | val_1_rmse: 0.37536 |  0:01:12s
epoch 78 | loss: 0.15663 | val_0_rmse: 0.37972 | val_1_rmse: 0.37464 |  0:01:13s
epoch 79 | loss: 0.16124 | val_0_rmse: 0.40377 | val_1_rmse: 0.39463 |  0:01:13s
epoch 80 | loss: 0.16532 | val_0_rmse: 0.37575 | val_1_rmse: 0.37076 |  0:01:14s
epoch 81 | loss: 0.16356 | val_0_rmse: 0.37298 | val_1_rmse: 0.37268 |  0:01:15s
epoch 82 | loss: 0.16079 | val_0_rmse: 0.38602 | val_1_rmse: 0.38219 |  0:01:16s
epoch 83 | loss: 0.15637 | val_0_rmse: 0.39936 | val_1_rmse: 0.40404 |  0:01:17s
epoch 84 | loss: 0.15603 | val_0_rmse: 0.37161 | val_1_rmse: 0.37215 |  0:01:18s
epoch 85 | loss: 0.15472 | val_0_rmse: 0.37365 | val_1_rmse: 0.37217 |  0:01:19s
epoch 86 | loss: 0.15667 | val_0_rmse: 0.3701  | val_1_rmse: 0.36473 |  0:01:20s
epoch 87 | loss: 0.15476 | val_0_rmse: 0.37083 | val_1_rmse: 0.36778 |  0:01:21s
epoch 88 | loss: 0.15304 | val_0_rmse: 0.38725 | val_1_rmse: 0.37811 |  0:01:22s
epoch 89 | loss: 0.16515 | val_0_rmse: 0.38784 | val_1_rmse: 0.3819  |  0:01:23s
epoch 90 | loss: 0.15899 | val_0_rmse: 0.37675 | val_1_rmse: 0.37418 |  0:01:24s
epoch 91 | loss: 0.15417 | val_0_rmse: 0.37143 | val_1_rmse: 0.36468 |  0:01:25s
epoch 92 | loss: 0.1491  | val_0_rmse: 0.36503 | val_1_rmse: 0.36033 |  0:01:26s
epoch 93 | loss: 0.154   | val_0_rmse: 0.36107 | val_1_rmse: 0.35487 |  0:01:26s
epoch 94 | loss: 0.15251 | val_0_rmse: 0.37824 | val_1_rmse: 0.37947 |  0:01:27s
epoch 95 | loss: 0.15162 | val_0_rmse: 0.36076 | val_1_rmse: 0.35649 |  0:01:28s
epoch 96 | loss: 0.14705 | val_0_rmse: 0.36243 | val_1_rmse: 0.35721 |  0:01:29s
epoch 97 | loss: 0.14518 | val_0_rmse: 0.3693  | val_1_rmse: 0.36737 |  0:01:30s
epoch 98 | loss: 0.15072 | val_0_rmse: 0.35459 | val_1_rmse: 0.35256 |  0:01:31s
epoch 99 | loss: 0.16321 | val_0_rmse: 0.39562 | val_1_rmse: 0.39134 |  0:01:32s
epoch 100| loss: 0.15446 | val_0_rmse: 0.37212 | val_1_rmse: 0.36976 |  0:01:33s
epoch 101| loss: 0.15098 | val_0_rmse: 0.3662  | val_1_rmse: 0.3609  |  0:01:34s
epoch 102| loss: 0.14738 | val_0_rmse: 0.37019 | val_1_rmse: 0.36853 |  0:01:35s
epoch 103| loss: 0.14679 | val_0_rmse: 0.37612 | val_1_rmse: 0.37368 |  0:01:36s
epoch 104| loss: 0.14795 | val_0_rmse: 0.38047 | val_1_rmse: 0.37693 |  0:01:37s
epoch 105| loss: 0.14623 | val_0_rmse: 0.37811 | val_1_rmse: 0.36904 |  0:01:38s
epoch 106| loss: 0.15506 | val_0_rmse: 0.37226 | val_1_rmse: 0.36888 |  0:01:38s
epoch 107| loss: 0.14866 | val_0_rmse: 0.35785 | val_1_rmse: 0.35691 |  0:01:39s
epoch 108| loss: 0.14595 | val_0_rmse: 0.35352 | val_1_rmse: 0.35167 |  0:01:40s
epoch 109| loss: 0.14348 | val_0_rmse: 0.36104 | val_1_rmse: 0.35635 |  0:01:41s
epoch 110| loss: 0.13853 | val_0_rmse: 0.34652 | val_1_rmse: 0.34624 |  0:01:42s
epoch 111| loss: 0.13911 | val_0_rmse: 0.35317 | val_1_rmse: 0.35245 |  0:01:43s
epoch 112| loss: 0.14501 | val_0_rmse: 0.3562  | val_1_rmse: 0.35933 |  0:01:44s
epoch 113| loss: 0.14503 | val_0_rmse: 0.36134 | val_1_rmse: 0.36688 |  0:01:45s
epoch 114| loss: 0.14483 | val_0_rmse: 0.35019 | val_1_rmse: 0.35204 |  0:01:46s
epoch 115| loss: 0.14119 | val_0_rmse: 0.35597 | val_1_rmse: 0.35623 |  0:01:47s
epoch 116| loss: 0.14019 | val_0_rmse: 0.36129 | val_1_rmse: 0.36316 |  0:01:48s
epoch 117| loss: 0.14154 | val_0_rmse: 0.35865 | val_1_rmse: 0.35715 |  0:01:49s
epoch 118| loss: 0.14309 | val_0_rmse: 0.35902 | val_1_rmse: 0.35825 |  0:01:49s
epoch 119| loss: 0.14085 | val_0_rmse: 0.35543 | val_1_rmse: 0.35337 |  0:01:50s
epoch 120| loss: 0.14315 | val_0_rmse: 0.38837 | val_1_rmse: 0.39302 |  0:01:51s
epoch 121| loss: 0.13776 | val_0_rmse: 0.34678 | val_1_rmse: 0.34713 |  0:01:52s
epoch 122| loss: 0.13505 | val_0_rmse: 0.36361 | val_1_rmse: 0.36534 |  0:01:53s
epoch 123| loss: 0.13623 | val_0_rmse: 0.34649 | val_1_rmse: 0.34988 |  0:01:54s
epoch 124| loss: 0.13914 | val_0_rmse: 0.36569 | val_1_rmse: 0.37031 |  0:01:55s
epoch 125| loss: 0.14109 | val_0_rmse: 0.3857  | val_1_rmse: 0.38498 |  0:01:56s
epoch 126| loss: 0.13796 | val_0_rmse: 0.37015 | val_1_rmse: 0.37211 |  0:01:57s
epoch 127| loss: 0.17276 | val_0_rmse: 0.41967 | val_1_rmse: 0.41706 |  0:01:58s
epoch 128| loss: 0.18267 | val_0_rmse: 0.56066 | val_1_rmse: 0.56683 |  0:01:59s
epoch 129| loss: 0.1771  | val_0_rmse: 0.41467 | val_1_rmse: 0.41121 |  0:02:00s
epoch 130| loss: 0.18141 | val_0_rmse: 0.4235  | val_1_rmse: 0.41663 |  0:02:01s
epoch 131| loss: 0.16725 | val_0_rmse: 0.41744 | val_1_rmse: 0.41852 |  0:02:02s
epoch 132| loss: 0.16507 | val_0_rmse: 0.39643 | val_1_rmse: 0.39521 |  0:02:02s
epoch 133| loss: 0.16212 | val_0_rmse: 0.39033 | val_1_rmse: 0.39186 |  0:02:03s
epoch 134| loss: 0.15608 | val_0_rmse: 0.38035 | val_1_rmse: 0.38234 |  0:02:04s
epoch 135| loss: 0.1561  | val_0_rmse: 0.37773 | val_1_rmse: 0.37471 |  0:02:05s
epoch 136| loss: 0.14868 | val_0_rmse: 0.3667  | val_1_rmse: 0.36615 |  0:02:06s
epoch 137| loss: 0.1461  | val_0_rmse: 0.37531 | val_1_rmse: 0.37479 |  0:02:07s
epoch 138| loss: 0.14451 | val_0_rmse: 0.36484 | val_1_rmse: 0.36628 |  0:02:08s
epoch 139| loss: 0.14927 | val_0_rmse: 0.41632 | val_1_rmse: 0.42076 |  0:02:09s
epoch 140| loss: 0.1636  | val_0_rmse: 0.39306 | val_1_rmse: 0.39093 |  0:02:10s

Early stopping occured at epoch 140 with best_epoch = 110 and best_val_1_rmse = 0.34624
Best weights from best epoch are automatically used!
ended training at: 04:14:50
Feature importance:
Mean squared error is of 0.07129508924962336
Mean absolute error:0.16838654734556238
MAPE:0.18966263780698372
R2 score:0.8639439815811214
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:14:51
epoch 0  | loss: 1.3271  | val_0_rmse: 0.80232 | val_1_rmse: 0.8119  |  0:00:00s
epoch 1  | loss: 0.60324 | val_0_rmse: 0.73552 | val_1_rmse: 0.72819 |  0:00:01s
epoch 2  | loss: 0.4562  | val_0_rmse: 0.72257 | val_1_rmse: 0.70744 |  0:00:02s
epoch 3  | loss: 0.38567 | val_0_rmse: 0.67928 | val_1_rmse: 0.67069 |  0:00:03s
epoch 4  | loss: 0.35443 | val_0_rmse: 0.57983 | val_1_rmse: 0.56971 |  0:00:04s
epoch 5  | loss: 0.31863 | val_0_rmse: 0.60341 | val_1_rmse: 0.59465 |  0:00:05s
epoch 6  | loss: 0.30535 | val_0_rmse: 0.57738 | val_1_rmse: 0.5679  |  0:00:06s
epoch 7  | loss: 0.28102 | val_0_rmse: 0.56264 | val_1_rmse: 0.55974 |  0:00:07s
epoch 8  | loss: 0.26312 | val_0_rmse: 0.59187 | val_1_rmse: 0.58627 |  0:00:08s
epoch 9  | loss: 0.25323 | val_0_rmse: 0.52553 | val_1_rmse: 0.52308 |  0:00:09s
epoch 10 | loss: 0.24715 | val_0_rmse: 0.56706 | val_1_rmse: 0.56623 |  0:00:10s
epoch 11 | loss: 0.24559 | val_0_rmse: 0.51209 | val_1_rmse: 0.50877 |  0:00:11s
epoch 12 | loss: 0.22852 | val_0_rmse: 0.53969 | val_1_rmse: 0.53127 |  0:00:11s
epoch 13 | loss: 0.21411 | val_0_rmse: 0.5326  | val_1_rmse: 0.52107 |  0:00:12s
epoch 14 | loss: 0.21032 | val_0_rmse: 0.50665 | val_1_rmse: 0.50456 |  0:00:13s
epoch 15 | loss: 0.20291 | val_0_rmse: 0.4933  | val_1_rmse: 0.49026 |  0:00:14s
epoch 16 | loss: 0.19124 | val_0_rmse: 0.48551 | val_1_rmse: 0.48499 |  0:00:15s
epoch 17 | loss: 0.18926 | val_0_rmse: 0.52416 | val_1_rmse: 0.51275 |  0:00:16s
epoch 18 | loss: 0.19097 | val_0_rmse: 0.486   | val_1_rmse: 0.48141 |  0:00:17s
epoch 19 | loss: 0.18615 | val_0_rmse: 0.48011 | val_1_rmse: 0.48086 |  0:00:18s
epoch 20 | loss: 0.18961 | val_0_rmse: 0.45924 | val_1_rmse: 0.45218 |  0:00:19s
epoch 21 | loss: 0.1897  | val_0_rmse: 0.47416 | val_1_rmse: 0.47061 |  0:00:20s
epoch 22 | loss: 0.17458 | val_0_rmse: 0.44473 | val_1_rmse: 0.44197 |  0:00:21s
epoch 23 | loss: 0.18885 | val_0_rmse: 0.48379 | val_1_rmse: 0.48214 |  0:00:22s
epoch 24 | loss: 0.19808 | val_0_rmse: 0.45445 | val_1_rmse: 0.45376 |  0:00:23s
epoch 25 | loss: 0.20287 | val_0_rmse: 0.45903 | val_1_rmse: 0.46108 |  0:00:24s
epoch 26 | loss: 0.19291 | val_0_rmse: 0.4772  | val_1_rmse: 0.47393 |  0:00:24s
epoch 27 | loss: 0.19496 | val_0_rmse: 0.44623 | val_1_rmse: 0.44369 |  0:00:25s
epoch 28 | loss: 0.18846 | val_0_rmse: 0.42155 | val_1_rmse: 0.41962 |  0:00:26s
epoch 29 | loss: 0.17433 | val_0_rmse: 0.40616 | val_1_rmse: 0.40506 |  0:00:27s
epoch 30 | loss: 0.18388 | val_0_rmse: 0.41622 | val_1_rmse: 0.42047 |  0:00:28s
epoch 31 | loss: 0.18447 | val_0_rmse: 0.41481 | val_1_rmse: 0.41951 |  0:00:29s
epoch 32 | loss: 0.17566 | val_0_rmse: 0.40083 | val_1_rmse: 0.40448 |  0:00:30s
epoch 33 | loss: 0.17856 | val_0_rmse: 0.4259  | val_1_rmse: 0.43093 |  0:00:31s
epoch 34 | loss: 0.18036 | val_0_rmse: 0.42189 | val_1_rmse: 0.42536 |  0:00:32s
epoch 35 | loss: 0.17511 | val_0_rmse: 0.3981  | val_1_rmse: 0.40112 |  0:00:33s
epoch 36 | loss: 0.16836 | val_0_rmse: 0.42183 | val_1_rmse: 0.42109 |  0:00:34s
epoch 37 | loss: 0.16698 | val_0_rmse: 0.46661 | val_1_rmse: 0.46678 |  0:00:35s
epoch 38 | loss: 0.16562 | val_0_rmse: 0.38184 | val_1_rmse: 0.39047 |  0:00:35s
epoch 39 | loss: 0.1597  | val_0_rmse: 0.37463 | val_1_rmse: 0.38165 |  0:00:36s
epoch 40 | loss: 0.15778 | val_0_rmse: 0.38656 | val_1_rmse: 0.39031 |  0:00:37s
epoch 41 | loss: 0.16118 | val_0_rmse: 0.37865 | val_1_rmse: 0.38287 |  0:00:38s
epoch 42 | loss: 0.15691 | val_0_rmse: 0.3839  | val_1_rmse: 0.38945 |  0:00:39s
epoch 43 | loss: 0.15718 | val_0_rmse: 0.40887 | val_1_rmse: 0.40949 |  0:00:40s
epoch 44 | loss: 0.1564  | val_0_rmse: 0.36828 | val_1_rmse: 0.37956 |  0:00:41s
epoch 45 | loss: 0.15596 | val_0_rmse: 0.37345 | val_1_rmse: 0.38356 |  0:00:42s
epoch 46 | loss: 0.1556  | val_0_rmse: 0.38034 | val_1_rmse: 0.38515 |  0:00:43s
epoch 47 | loss: 0.16227 | val_0_rmse: 0.37989 | val_1_rmse: 0.39093 |  0:00:44s
epoch 48 | loss: 0.1574  | val_0_rmse: 0.41985 | val_1_rmse: 0.41507 |  0:00:45s
epoch 49 | loss: 0.15888 | val_0_rmse: 0.41015 | val_1_rmse: 0.40933 |  0:00:46s
epoch 50 | loss: 0.15724 | val_0_rmse: 0.37095 | val_1_rmse: 0.38142 |  0:00:47s
epoch 51 | loss: 0.14927 | val_0_rmse: 0.36867 | val_1_rmse: 0.37946 |  0:00:47s
epoch 52 | loss: 0.15359 | val_0_rmse: 0.3745  | val_1_rmse: 0.3808  |  0:00:48s
epoch 53 | loss: 0.16004 | val_0_rmse: 0.37951 | val_1_rmse: 0.38869 |  0:00:49s
epoch 54 | loss: 0.16152 | val_0_rmse: 0.36797 | val_1_rmse: 0.37246 |  0:00:50s
epoch 55 | loss: 0.15189 | val_0_rmse: 0.3873  | val_1_rmse: 0.3971  |  0:00:51s
epoch 56 | loss: 0.15253 | val_0_rmse: 0.38255 | val_1_rmse: 0.40179 |  0:00:52s
epoch 57 | loss: 0.14591 | val_0_rmse: 0.35959 | val_1_rmse: 0.36816 |  0:00:53s
epoch 58 | loss: 0.15098 | val_0_rmse: 0.36755 | val_1_rmse: 0.37947 |  0:00:54s
epoch 59 | loss: 0.15731 | val_0_rmse: 0.36394 | val_1_rmse: 0.37607 |  0:00:55s
epoch 60 | loss: 0.15334 | val_0_rmse: 0.35709 | val_1_rmse: 0.36561 |  0:00:56s
epoch 61 | loss: 0.14597 | val_0_rmse: 0.40354 | val_1_rmse: 0.41722 |  0:00:57s
epoch 62 | loss: 0.15697 | val_0_rmse: 0.4114  | val_1_rmse: 0.41638 |  0:00:58s
epoch 63 | loss: 0.15673 | val_0_rmse: 0.38375 | val_1_rmse: 0.38817 |  0:00:58s
epoch 64 | loss: 0.14843 | val_0_rmse: 0.37012 | val_1_rmse: 0.38768 |  0:00:59s
epoch 65 | loss: 0.14733 | val_0_rmse: 0.37074 | val_1_rmse: 0.3769  |  0:01:00s
epoch 66 | loss: 0.14961 | val_0_rmse: 0.3698  | val_1_rmse: 0.38336 |  0:01:01s
epoch 67 | loss: 0.14701 | val_0_rmse: 0.37463 | val_1_rmse: 0.3892  |  0:01:02s
epoch 68 | loss: 0.14574 | val_0_rmse: 0.3639  | val_1_rmse: 0.37471 |  0:01:03s
epoch 69 | loss: 0.14201 | val_0_rmse: 0.34893 | val_1_rmse: 0.35852 |  0:01:04s
epoch 70 | loss: 0.14168 | val_0_rmse: 0.34926 | val_1_rmse: 0.35899 |  0:01:05s
epoch 71 | loss: 0.14575 | val_0_rmse: 0.36483 | val_1_rmse: 0.37208 |  0:01:06s
epoch 72 | loss: 0.14536 | val_0_rmse: 0.3736  | val_1_rmse: 0.37866 |  0:01:07s
epoch 73 | loss: 0.14449 | val_0_rmse: 0.36209 | val_1_rmse: 0.37661 |  0:01:08s
epoch 74 | loss: 0.15384 | val_0_rmse: 0.42681 | val_1_rmse: 0.43652 |  0:01:09s
epoch 75 | loss: 0.16592 | val_0_rmse: 0.42696 | val_1_rmse: 0.43798 |  0:01:09s
epoch 76 | loss: 0.15761 | val_0_rmse: 0.38221 | val_1_rmse: 0.39217 |  0:01:10s
epoch 77 | loss: 0.15196 | val_0_rmse: 0.36413 | val_1_rmse: 0.37062 |  0:01:11s
epoch 78 | loss: 0.1515  | val_0_rmse: 0.38718 | val_1_rmse: 0.38917 |  0:01:12s
epoch 79 | loss: 0.15172 | val_0_rmse: 0.37596 | val_1_rmse: 0.383   |  0:01:13s
epoch 80 | loss: 0.14702 | val_0_rmse: 0.37    | val_1_rmse: 0.37577 |  0:01:14s
epoch 81 | loss: 0.14452 | val_0_rmse: 0.35969 | val_1_rmse: 0.36984 |  0:01:15s
epoch 82 | loss: 0.14474 | val_0_rmse: 0.35482 | val_1_rmse: 0.36306 |  0:01:16s
epoch 83 | loss: 0.1382  | val_0_rmse: 0.36753 | val_1_rmse: 0.37348 |  0:01:17s
epoch 84 | loss: 0.14119 | val_0_rmse: 0.35234 | val_1_rmse: 0.3607  |  0:01:18s
epoch 85 | loss: 0.13819 | val_0_rmse: 0.35271 | val_1_rmse: 0.36445 |  0:01:19s
epoch 86 | loss: 0.14131 | val_0_rmse: 0.34483 | val_1_rmse: 0.36237 |  0:01:20s
epoch 87 | loss: 0.14079 | val_0_rmse: 0.35115 | val_1_rmse: 0.37253 |  0:01:20s
epoch 88 | loss: 0.14179 | val_0_rmse: 0.34313 | val_1_rmse: 0.36043 |  0:01:21s
epoch 89 | loss: 0.15131 | val_0_rmse: 0.43143 | val_1_rmse: 0.44403 |  0:01:22s
epoch 90 | loss: 0.1543  | val_0_rmse: 0.36996 | val_1_rmse: 0.38363 |  0:01:23s
epoch 91 | loss: 0.14945 | val_0_rmse: 0.36166 | val_1_rmse: 0.37652 |  0:01:24s
epoch 92 | loss: 0.14515 | val_0_rmse: 0.35222 | val_1_rmse: 0.3666  |  0:01:25s
epoch 93 | loss: 0.13877 | val_0_rmse: 0.36687 | val_1_rmse: 0.38162 |  0:01:26s
epoch 94 | loss: 0.14343 | val_0_rmse: 0.36186 | val_1_rmse: 0.37579 |  0:01:27s
epoch 95 | loss: 0.14089 | val_0_rmse: 0.37208 | val_1_rmse: 0.38102 |  0:01:28s
epoch 96 | loss: 0.14104 | val_0_rmse: 0.38194 | val_1_rmse: 0.39887 |  0:01:29s
epoch 97 | loss: 0.13694 | val_0_rmse: 0.3432  | val_1_rmse: 0.35848 |  0:01:30s
epoch 98 | loss: 0.13646 | val_0_rmse: 0.35014 | val_1_rmse: 0.36686 |  0:01:31s
epoch 99 | loss: 0.13555 | val_0_rmse: 0.34581 | val_1_rmse: 0.36568 |  0:01:32s
epoch 100| loss: 0.13347 | val_0_rmse: 0.37157 | val_1_rmse: 0.3773  |  0:01:32s
epoch 101| loss: 0.14424 | val_0_rmse: 0.36823 | val_1_rmse: 0.38397 |  0:01:33s
epoch 102| loss: 0.14297 | val_0_rmse: 0.37744 | val_1_rmse: 0.38683 |  0:01:34s
epoch 103| loss: 0.13813 | val_0_rmse: 0.35924 | val_1_rmse: 0.37953 |  0:01:35s
epoch 104| loss: 0.13421 | val_0_rmse: 0.34672 | val_1_rmse: 0.3562  |  0:01:36s
epoch 105| loss: 0.13609 | val_0_rmse: 0.34475 | val_1_rmse: 0.36483 |  0:01:37s
epoch 106| loss: 0.1337  | val_0_rmse: 0.34639 | val_1_rmse: 0.36055 |  0:01:38s
epoch 107| loss: 0.13308 | val_0_rmse: 0.33596 | val_1_rmse: 0.35134 |  0:01:39s
epoch 108| loss: 0.13135 | val_0_rmse: 0.33433 | val_1_rmse: 0.35017 |  0:01:40s
epoch 109| loss: 0.13074 | val_0_rmse: 0.33603 | val_1_rmse: 0.35286 |  0:01:41s
epoch 110| loss: 0.1279  | val_0_rmse: 0.34187 | val_1_rmse: 0.3587  |  0:01:42s
epoch 111| loss: 0.13742 | val_0_rmse: 0.35206 | val_1_rmse: 0.37424 |  0:01:43s
epoch 112| loss: 0.1394  | val_0_rmse: 0.3664  | val_1_rmse: 0.38043 |  0:01:43s
epoch 113| loss: 0.14572 | val_0_rmse: 0.35925 | val_1_rmse: 0.37282 |  0:01:44s
epoch 114| loss: 0.1377  | val_0_rmse: 0.34542 | val_1_rmse: 0.359   |  0:01:45s
epoch 115| loss: 0.12988 | val_0_rmse: 0.33535 | val_1_rmse: 0.35308 |  0:01:46s
epoch 116| loss: 0.13511 | val_0_rmse: 0.35184 | val_1_rmse: 0.37129 |  0:01:47s
epoch 117| loss: 0.12888 | val_0_rmse: 0.35248 | val_1_rmse: 0.36535 |  0:01:48s
epoch 118| loss: 0.13077 | val_0_rmse: 0.33302 | val_1_rmse: 0.35339 |  0:01:49s
epoch 119| loss: 0.13242 | val_0_rmse: 0.33107 | val_1_rmse: 0.34744 |  0:01:50s
epoch 120| loss: 0.12972 | val_0_rmse: 0.33277 | val_1_rmse: 0.3518  |  0:01:51s
epoch 121| loss: 0.13191 | val_0_rmse: 0.35349 | val_1_rmse: 0.37237 |  0:01:52s
epoch 122| loss: 0.12974 | val_0_rmse: 0.3454  | val_1_rmse: 0.36105 |  0:01:53s
epoch 123| loss: 0.12652 | val_0_rmse: 0.35369 | val_1_rmse: 0.36965 |  0:01:54s
epoch 124| loss: 0.14221 | val_0_rmse: 0.35187 | val_1_rmse: 0.36465 |  0:01:54s
epoch 125| loss: 0.13586 | val_0_rmse: 0.38592 | val_1_rmse: 0.40361 |  0:01:55s
epoch 126| loss: 0.13335 | val_0_rmse: 0.3772  | val_1_rmse: 0.40001 |  0:01:56s
epoch 127| loss: 0.13301 | val_0_rmse: 0.34064 | val_1_rmse: 0.35431 |  0:01:57s
epoch 128| loss: 0.13928 | val_0_rmse: 0.3855  | val_1_rmse: 0.39152 |  0:01:58s
epoch 129| loss: 0.13682 | val_0_rmse: 0.34433 | val_1_rmse: 0.35635 |  0:01:59s
epoch 130| loss: 0.13357 | val_0_rmse: 0.34375 | val_1_rmse: 0.36158 |  0:02:00s
epoch 131| loss: 0.13237 | val_0_rmse: 0.33348 | val_1_rmse: 0.35304 |  0:02:01s
epoch 132| loss: 0.13238 | val_0_rmse: 0.35594 | val_1_rmse: 0.37125 |  0:02:02s
epoch 133| loss: 0.16276 | val_0_rmse: 0.39634 | val_1_rmse: 0.40733 |  0:02:03s
epoch 134| loss: 0.1494  | val_0_rmse: 0.35634 | val_1_rmse: 0.37057 |  0:02:04s
epoch 135| loss: 0.14456 | val_0_rmse: 0.34153 | val_1_rmse: 0.3607  |  0:02:05s
epoch 136| loss: 0.13502 | val_0_rmse: 0.34951 | val_1_rmse: 0.36253 |  0:02:06s
epoch 137| loss: 0.13533 | val_0_rmse: 0.35563 | val_1_rmse: 0.37401 |  0:02:06s
epoch 138| loss: 0.13149 | val_0_rmse: 0.33426 | val_1_rmse: 0.35697 |  0:02:07s
epoch 139| loss: 0.12695 | val_0_rmse: 0.34126 | val_1_rmse: 0.36495 |  0:02:08s
epoch 140| loss: 0.12924 | val_0_rmse: 0.33302 | val_1_rmse: 0.35005 |  0:02:09s
epoch 141| loss: 0.13203 | val_0_rmse: 0.34106 | val_1_rmse: 0.35819 |  0:02:10s
epoch 142| loss: 0.13083 | val_0_rmse: 0.34217 | val_1_rmse: 0.35929 |  0:02:11s
epoch 143| loss: 0.13685 | val_0_rmse: 0.34903 | val_1_rmse: 0.37167 |  0:02:12s
epoch 144| loss: 0.1374  | val_0_rmse: 0.35352 | val_1_rmse: 0.37571 |  0:02:13s
epoch 145| loss: 0.13638 | val_0_rmse: 0.37825 | val_1_rmse: 0.4031  |  0:02:14s
epoch 146| loss: 0.12875 | val_0_rmse: 0.33316 | val_1_rmse: 0.35651 |  0:02:15s
epoch 147| loss: 0.13327 | val_0_rmse: 0.3535  | val_1_rmse: 0.36962 |  0:02:16s
epoch 148| loss: 0.13705 | val_0_rmse: 0.33678 | val_1_rmse: 0.35766 |  0:02:17s
epoch 149| loss: 0.13776 | val_0_rmse: 0.35583 | val_1_rmse: 0.37156 |  0:02:17s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.34744
Best weights from best epoch are automatically used!
ended training at: 04:17:09
Feature importance:
Mean squared error is of 0.0613986094923988
Mean absolute error:0.15945018015142992
MAPE:0.18637753389140171
R2 score:0.8802163292302395
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:17:09
epoch 0  | loss: 1.28945 | val_0_rmse: 0.82047 | val_1_rmse: 0.84086 |  0:00:00s
epoch 1  | loss: 0.58293 | val_0_rmse: 0.78045 | val_1_rmse: 0.79837 |  0:00:01s
epoch 2  | loss: 0.49653 | val_0_rmse: 0.78091 | val_1_rmse: 0.77634 |  0:00:02s
epoch 3  | loss: 0.44588 | val_0_rmse: 0.73221 | val_1_rmse: 0.74245 |  0:00:03s
epoch 4  | loss: 0.40098 | val_0_rmse: 0.66971 | val_1_rmse: 0.67972 |  0:00:04s
epoch 5  | loss: 0.36291 | val_0_rmse: 0.63481 | val_1_rmse: 0.64481 |  0:00:05s
epoch 6  | loss: 0.32139 | val_0_rmse: 0.62661 | val_1_rmse: 0.639   |  0:00:06s
epoch 7  | loss: 0.31701 | val_0_rmse: 0.60747 | val_1_rmse: 0.62624 |  0:00:07s
epoch 8  | loss: 0.28989 | val_0_rmse: 0.56005 | val_1_rmse: 0.57545 |  0:00:08s
epoch 9  | loss: 0.29381 | val_0_rmse: 0.60308 | val_1_rmse: 0.61248 |  0:00:09s
epoch 10 | loss: 0.28766 | val_0_rmse: 0.63431 | val_1_rmse: 0.64516 |  0:00:10s
epoch 11 | loss: 0.26988 | val_0_rmse: 0.59055 | val_1_rmse: 0.59653 |  0:00:11s
epoch 12 | loss: 0.25064 | val_0_rmse: 0.54028 | val_1_rmse: 0.54588 |  0:00:11s
epoch 13 | loss: 0.23094 | val_0_rmse: 0.51967 | val_1_rmse: 0.52283 |  0:00:12s
epoch 14 | loss: 0.21415 | val_0_rmse: 0.52128 | val_1_rmse: 0.52328 |  0:00:13s
epoch 15 | loss: 0.21869 | val_0_rmse: 0.49789 | val_1_rmse: 0.49551 |  0:00:14s
epoch 16 | loss: 0.20776 | val_0_rmse: 0.51529 | val_1_rmse: 0.51833 |  0:00:15s
epoch 17 | loss: 0.21259 | val_0_rmse: 0.50283 | val_1_rmse: 0.4938  |  0:00:16s
epoch 18 | loss: 0.2029  | val_0_rmse: 0.52099 | val_1_rmse: 0.52933 |  0:00:17s
epoch 19 | loss: 0.19904 | val_0_rmse: 0.47417 | val_1_rmse: 0.47493 |  0:00:18s
epoch 20 | loss: 0.19569 | val_0_rmse: 0.46181 | val_1_rmse: 0.46181 |  0:00:19s
epoch 21 | loss: 0.19368 | val_0_rmse: 0.49373 | val_1_rmse: 0.48911 |  0:00:20s
epoch 22 | loss: 0.18844 | val_0_rmse: 0.43725 | val_1_rmse: 0.4485  |  0:00:21s
epoch 23 | loss: 0.18955 | val_0_rmse: 0.44679 | val_1_rmse: 0.44004 |  0:00:22s
epoch 24 | loss: 0.1844  | val_0_rmse: 0.43368 | val_1_rmse: 0.43288 |  0:00:23s
epoch 25 | loss: 0.18498 | val_0_rmse: 0.43043 | val_1_rmse: 0.42902 |  0:00:24s
epoch 26 | loss: 0.18314 | val_0_rmse: 0.42537 | val_1_rmse: 0.42117 |  0:00:24s
epoch 27 | loss: 0.18481 | val_0_rmse: 0.4391  | val_1_rmse: 0.43112 |  0:00:25s
epoch 28 | loss: 0.18935 | val_0_rmse: 0.4179  | val_1_rmse: 0.41462 |  0:00:26s
epoch 29 | loss: 0.18474 | val_0_rmse: 0.41168 | val_1_rmse: 0.4102  |  0:00:27s
epoch 30 | loss: 0.17351 | val_0_rmse: 0.44608 | val_1_rmse: 0.44317 |  0:00:28s
epoch 31 | loss: 0.17423 | val_0_rmse: 0.40525 | val_1_rmse: 0.40875 |  0:00:29s
epoch 32 | loss: 0.17024 | val_0_rmse: 0.39468 | val_1_rmse: 0.39776 |  0:00:30s
epoch 33 | loss: 0.17506 | val_0_rmse: 0.43025 | val_1_rmse: 0.42155 |  0:00:31s
epoch 34 | loss: 0.17828 | val_0_rmse: 0.39246 | val_1_rmse: 0.39172 |  0:00:32s
epoch 35 | loss: 0.17498 | val_0_rmse: 0.39547 | val_1_rmse: 0.39832 |  0:00:33s
epoch 36 | loss: 0.16957 | val_0_rmse: 0.38866 | val_1_rmse: 0.39428 |  0:00:34s
epoch 37 | loss: 0.16872 | val_0_rmse: 0.39428 | val_1_rmse: 0.39231 |  0:00:35s
epoch 38 | loss: 0.17625 | val_0_rmse: 0.42246 | val_1_rmse: 0.42218 |  0:00:35s
epoch 39 | loss: 0.17062 | val_0_rmse: 0.42517 | val_1_rmse: 0.4239  |  0:00:36s
epoch 40 | loss: 0.17982 | val_0_rmse: 0.40375 | val_1_rmse: 0.40631 |  0:00:37s
epoch 41 | loss: 0.17746 | val_0_rmse: 0.39843 | val_1_rmse: 0.4018  |  0:00:38s
epoch 42 | loss: 0.17367 | val_0_rmse: 0.38977 | val_1_rmse: 0.3937  |  0:00:39s
epoch 43 | loss: 0.16837 | val_0_rmse: 0.40264 | val_1_rmse: 0.40107 |  0:00:40s
epoch 44 | loss: 0.17297 | val_0_rmse: 0.39905 | val_1_rmse: 0.39849 |  0:00:41s
epoch 45 | loss: 0.16699 | val_0_rmse: 0.39463 | val_1_rmse: 0.39219 |  0:00:42s
epoch 46 | loss: 0.1707  | val_0_rmse: 0.39084 | val_1_rmse: 0.39001 |  0:00:43s
epoch 47 | loss: 0.1698  | val_0_rmse: 0.40264 | val_1_rmse: 0.40093 |  0:00:44s
epoch 48 | loss: 0.17278 | val_0_rmse: 0.38513 | val_1_rmse: 0.38394 |  0:00:45s
epoch 49 | loss: 0.17066 | val_0_rmse: 0.38595 | val_1_rmse: 0.38254 |  0:00:46s
epoch 50 | loss: 0.15997 | val_0_rmse: 0.46886 | val_1_rmse: 0.46304 |  0:00:46s
epoch 51 | loss: 0.18981 | val_0_rmse: 0.53336 | val_1_rmse: 0.54563 |  0:00:47s
epoch 52 | loss: 0.18567 | val_0_rmse: 0.40636 | val_1_rmse: 0.40428 |  0:00:48s
epoch 53 | loss: 0.17546 | val_0_rmse: 0.39846 | val_1_rmse: 0.39467 |  0:00:49s
epoch 54 | loss: 0.16682 | val_0_rmse: 0.43224 | val_1_rmse: 0.42465 |  0:00:50s
epoch 55 | loss: 0.17477 | val_0_rmse: 0.40212 | val_1_rmse: 0.39717 |  0:00:51s
epoch 56 | loss: 0.16952 | val_0_rmse: 0.38956 | val_1_rmse: 0.38719 |  0:00:52s
epoch 57 | loss: 0.16958 | val_0_rmse: 0.42042 | val_1_rmse: 0.42856 |  0:00:53s
epoch 58 | loss: 0.16616 | val_0_rmse: 0.3944  | val_1_rmse: 0.39154 |  0:00:54s
epoch 59 | loss: 0.17817 | val_0_rmse: 0.43556 | val_1_rmse: 0.43226 |  0:00:55s
epoch 60 | loss: 0.17758 | val_0_rmse: 0.42576 | val_1_rmse: 0.41737 |  0:00:56s
epoch 61 | loss: 0.16723 | val_0_rmse: 0.40422 | val_1_rmse: 0.40422 |  0:00:57s
epoch 62 | loss: 0.17394 | val_0_rmse: 0.38541 | val_1_rmse: 0.38555 |  0:00:58s
epoch 63 | loss: 0.16212 | val_0_rmse: 0.40425 | val_1_rmse: 0.40369 |  0:00:58s
epoch 64 | loss: 0.17356 | val_0_rmse: 0.38756 | val_1_rmse: 0.38984 |  0:00:59s
epoch 65 | loss: 0.16122 | val_0_rmse: 0.37876 | val_1_rmse: 0.38223 |  0:01:00s
epoch 66 | loss: 0.1575  | val_0_rmse: 0.40879 | val_1_rmse: 0.40847 |  0:01:01s
epoch 67 | loss: 0.16122 | val_0_rmse: 0.40415 | val_1_rmse: 0.40626 |  0:01:02s
epoch 68 | loss: 0.16593 | val_0_rmse: 0.39493 | val_1_rmse: 0.39417 |  0:01:03s
epoch 69 | loss: 0.17147 | val_0_rmse: 0.41243 | val_1_rmse: 0.42208 |  0:01:04s
epoch 70 | loss: 0.16694 | val_0_rmse: 0.3758  | val_1_rmse: 0.3799  |  0:01:05s
epoch 71 | loss: 0.1612  | val_0_rmse: 0.3897  | val_1_rmse: 0.3904  |  0:01:06s
epoch 72 | loss: 0.16146 | val_0_rmse: 0.37721 | val_1_rmse: 0.37445 |  0:01:07s
epoch 73 | loss: 0.15871 | val_0_rmse: 0.3803  | val_1_rmse: 0.37889 |  0:01:08s
epoch 74 | loss: 0.15577 | val_0_rmse: 0.37773 | val_1_rmse: 0.38203 |  0:01:08s
epoch 75 | loss: 0.15657 | val_0_rmse: 0.38472 | val_1_rmse: 0.38033 |  0:01:09s
epoch 76 | loss: 0.15872 | val_0_rmse: 0.42257 | val_1_rmse: 0.42721 |  0:01:10s
epoch 77 | loss: 0.15881 | val_0_rmse: 0.40057 | val_1_rmse: 0.40584 |  0:01:11s
epoch 78 | loss: 0.16839 | val_0_rmse: 0.38224 | val_1_rmse: 0.38168 |  0:01:12s
epoch 79 | loss: 0.15847 | val_0_rmse: 0.40727 | val_1_rmse: 0.40972 |  0:01:13s
epoch 80 | loss: 0.15941 | val_0_rmse: 0.38825 | val_1_rmse: 0.38573 |  0:01:14s
epoch 81 | loss: 0.16025 | val_0_rmse: 0.38778 | val_1_rmse: 0.39556 |  0:01:15s
epoch 82 | loss: 0.15965 | val_0_rmse: 0.39287 | val_1_rmse: 0.39483 |  0:01:16s
epoch 83 | loss: 0.15962 | val_0_rmse: 0.40019 | val_1_rmse: 0.39189 |  0:01:17s
epoch 84 | loss: 0.16673 | val_0_rmse: 0.39658 | val_1_rmse: 0.39827 |  0:01:18s
epoch 85 | loss: 0.1618  | val_0_rmse: 0.41566 | val_1_rmse: 0.40151 |  0:01:19s
epoch 86 | loss: 0.16401 | val_0_rmse: 0.3731  | val_1_rmse: 0.37402 |  0:01:19s
epoch 87 | loss: 0.15904 | val_0_rmse: 0.44387 | val_1_rmse: 0.4417  |  0:01:20s
epoch 88 | loss: 0.15633 | val_0_rmse: 0.3986  | val_1_rmse: 0.39465 |  0:01:21s
epoch 89 | loss: 0.16328 | val_0_rmse: 0.39839 | val_1_rmse: 0.39849 |  0:01:22s
epoch 90 | loss: 0.1567  | val_0_rmse: 0.38342 | val_1_rmse: 0.38118 |  0:01:23s
epoch 91 | loss: 0.1521  | val_0_rmse: 0.37359 | val_1_rmse: 0.37328 |  0:01:24s
epoch 92 | loss: 0.15344 | val_0_rmse: 0.38935 | val_1_rmse: 0.38807 |  0:01:25s
epoch 93 | loss: 0.15548 | val_0_rmse: 0.37587 | val_1_rmse: 0.3734  |  0:01:26s
epoch 94 | loss: 0.15606 | val_0_rmse: 0.39052 | val_1_rmse: 0.386   |  0:01:27s
epoch 95 | loss: 0.16961 | val_0_rmse: 0.44695 | val_1_rmse: 0.44833 |  0:01:28s
epoch 96 | loss: 0.19809 | val_0_rmse: 0.41646 | val_1_rmse: 0.41263 |  0:01:29s
epoch 97 | loss: 0.18271 | val_0_rmse: 0.56733 | val_1_rmse: 0.55467 |  0:01:30s
epoch 98 | loss: 0.17372 | val_0_rmse: 0.41221 | val_1_rmse: 0.40096 |  0:01:30s
epoch 99 | loss: 0.16413 | val_0_rmse: 0.4015  | val_1_rmse: 0.40609 |  0:01:31s
epoch 100| loss: 0.15651 | val_0_rmse: 0.37039 | val_1_rmse: 0.36894 |  0:01:32s
epoch 101| loss: 0.15522 | val_0_rmse: 0.41487 | val_1_rmse: 0.41892 |  0:01:33s
epoch 102| loss: 0.15577 | val_0_rmse: 0.39256 | val_1_rmse: 0.39266 |  0:01:34s
epoch 103| loss: 0.16387 | val_0_rmse: 0.39199 | val_1_rmse: 0.39556 |  0:01:35s
epoch 104| loss: 0.15803 | val_0_rmse: 0.42023 | val_1_rmse: 0.41631 |  0:01:36s
epoch 105| loss: 0.16257 | val_0_rmse: 0.37849 | val_1_rmse: 0.3778  |  0:01:37s
epoch 106| loss: 0.15683 | val_0_rmse: 0.39088 | val_1_rmse: 0.39749 |  0:01:38s
epoch 107| loss: 0.16956 | val_0_rmse: 0.6243  | val_1_rmse: 0.63012 |  0:01:39s
epoch 108| loss: 0.16966 | val_0_rmse: 0.43956 | val_1_rmse: 0.42466 |  0:01:40s
epoch 109| loss: 0.16457 | val_0_rmse: 0.38921 | val_1_rmse: 0.38496 |  0:01:41s
epoch 110| loss: 0.1615  | val_0_rmse: 0.41477 | val_1_rmse: 0.42485 |  0:01:41s
epoch 111| loss: 0.16133 | val_0_rmse: 0.38788 | val_1_rmse: 0.3851  |  0:01:43s
epoch 112| loss: 0.15943 | val_0_rmse: 0.37976 | val_1_rmse: 0.3817  |  0:01:43s
epoch 113| loss: 0.18102 | val_0_rmse: 0.41376 | val_1_rmse: 0.4104  |  0:01:44s
epoch 114| loss: 0.17994 | val_0_rmse: 0.4088  | val_1_rmse: 0.40762 |  0:01:45s
epoch 115| loss: 0.16783 | val_0_rmse: 0.40566 | val_1_rmse: 0.40658 |  0:01:46s
epoch 116| loss: 0.16436 | val_0_rmse: 0.38929 | val_1_rmse: 0.38452 |  0:01:47s
epoch 117| loss: 0.1557  | val_0_rmse: 0.38223 | val_1_rmse: 0.37701 |  0:01:48s
epoch 118| loss: 0.14988 | val_0_rmse: 0.3693  | val_1_rmse: 0.36937 |  0:01:49s
epoch 119| loss: 0.14763 | val_0_rmse: 0.38509 | val_1_rmse: 0.39213 |  0:01:50s
epoch 120| loss: 0.14638 | val_0_rmse: 0.38523 | val_1_rmse: 0.38694 |  0:01:51s
epoch 121| loss: 0.14306 | val_0_rmse: 0.36974 | val_1_rmse: 0.37653 |  0:01:52s
epoch 122| loss: 0.1405  | val_0_rmse: 0.35394 | val_1_rmse: 0.35643 |  0:01:53s
epoch 123| loss: 0.14419 | val_0_rmse: 0.35954 | val_1_rmse: 0.35949 |  0:01:54s
epoch 124| loss: 0.14454 | val_0_rmse: 0.395   | val_1_rmse: 0.40435 |  0:01:55s
epoch 125| loss: 0.14668 | val_0_rmse: 0.40389 | val_1_rmse: 0.41257 |  0:01:56s
epoch 126| loss: 0.15212 | val_0_rmse: 0.39316 | val_1_rmse: 0.39679 |  0:01:57s
epoch 127| loss: 0.16756 | val_0_rmse: 0.45841 | val_1_rmse: 0.46858 |  0:01:58s
epoch 128| loss: 0.16391 | val_0_rmse: 0.39642 | val_1_rmse: 0.39912 |  0:01:59s
epoch 129| loss: 0.15672 | val_0_rmse: 0.38492 | val_1_rmse: 0.38608 |  0:02:00s
epoch 130| loss: 0.15206 | val_0_rmse: 0.3924  | val_1_rmse: 0.39284 |  0:02:00s
epoch 131| loss: 0.14817 | val_0_rmse: 0.41958 | val_1_rmse: 0.42829 |  0:02:01s
epoch 132| loss: 0.14057 | val_0_rmse: 0.42109 | val_1_rmse: 0.41784 |  0:02:02s
epoch 133| loss: 0.14557 | val_0_rmse: 0.39879 | val_1_rmse: 0.3986  |  0:02:03s
epoch 134| loss: 0.14074 | val_0_rmse: 0.3493  | val_1_rmse: 0.35529 |  0:02:04s
epoch 135| loss: 0.13953 | val_0_rmse: 0.36987 | val_1_rmse: 0.37683 |  0:02:05s
epoch 136| loss: 0.14301 | val_0_rmse: 0.43397 | val_1_rmse: 0.44738 |  0:02:06s
epoch 137| loss: 0.14032 | val_0_rmse: 0.35814 | val_1_rmse: 0.36032 |  0:02:07s
epoch 138| loss: 0.14241 | val_0_rmse: 0.34864 | val_1_rmse: 0.35215 |  0:02:08s
epoch 139| loss: 0.13714 | val_0_rmse: 0.4999  | val_1_rmse: 0.50926 |  0:02:09s
epoch 140| loss: 0.14042 | val_0_rmse: 0.3575  | val_1_rmse: 0.3651  |  0:02:10s
epoch 141| loss: 0.13638 | val_0_rmse: 0.36392 | val_1_rmse: 0.37092 |  0:02:11s
epoch 142| loss: 0.13289 | val_0_rmse: 0.4078  | val_1_rmse: 0.40787 |  0:02:12s
epoch 143| loss: 0.13159 | val_0_rmse: 0.36549 | val_1_rmse: 0.37383 |  0:02:12s
epoch 144| loss: 0.1361  | val_0_rmse: 0.37662 | val_1_rmse: 0.3795  |  0:02:13s
epoch 145| loss: 0.13892 | val_0_rmse: 0.43285 | val_1_rmse: 0.43338 |  0:02:14s
epoch 146| loss: 0.13929 | val_0_rmse: 0.34505 | val_1_rmse: 0.35109 |  0:02:15s
epoch 147| loss: 0.13281 | val_0_rmse: 0.34715 | val_1_rmse: 0.34908 |  0:02:16s
epoch 148| loss: 0.13808 | val_0_rmse: 0.35463 | val_1_rmse: 0.36255 |  0:02:17s
epoch 149| loss: 0.13832 | val_0_rmse: 0.36309 | val_1_rmse: 0.37053 |  0:02:18s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.34908
Best weights from best epoch are automatically used!
ended training at: 04:19:28
Feature importance:
Mean squared error is of 0.06653091067653336
Mean absolute error:0.1665442017642587
MAPE:0.1990160841386188
R2 score:0.8568281642804462
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:19:29
epoch 0  | loss: 1.36178 | val_0_rmse: 0.79776 | val_1_rmse: 0.79494 |  0:00:00s
epoch 1  | loss: 0.60543 | val_0_rmse: 0.81474 | val_1_rmse: 0.84514 |  0:00:01s
epoch 2  | loss: 0.43161 | val_0_rmse: 0.6891  | val_1_rmse: 0.68967 |  0:00:02s
epoch 3  | loss: 0.39893 | val_0_rmse: 0.66879 | val_1_rmse: 0.66854 |  0:00:03s
epoch 4  | loss: 0.35207 | val_0_rmse: 0.65582 | val_1_rmse: 0.66978 |  0:00:04s
epoch 5  | loss: 0.32604 | val_0_rmse: 0.67135 | val_1_rmse: 0.67791 |  0:00:05s
epoch 6  | loss: 0.32077 | val_0_rmse: 0.72586 | val_1_rmse: 0.7288  |  0:00:06s
epoch 7  | loss: 0.2907  | val_0_rmse: 0.63016 | val_1_rmse: 0.61977 |  0:00:07s
epoch 8  | loss: 0.28571 | val_0_rmse: 0.59695 | val_1_rmse: 0.58899 |  0:00:08s
epoch 9  | loss: 0.27563 | val_0_rmse: 0.57595 | val_1_rmse: 0.56356 |  0:00:09s
epoch 10 | loss: 0.28709 | val_0_rmse: 0.57348 | val_1_rmse: 0.57991 |  0:00:10s
epoch 11 | loss: 0.28794 | val_0_rmse: 0.6161  | val_1_rmse: 0.6354  |  0:00:11s
epoch 12 | loss: 0.26534 | val_0_rmse: 0.53459 | val_1_rmse: 0.53849 |  0:00:11s
epoch 13 | loss: 0.25727 | val_0_rmse: 0.53849 | val_1_rmse: 0.5474  |  0:00:12s
epoch 14 | loss: 0.24235 | val_0_rmse: 0.54547 | val_1_rmse: 0.5572  |  0:00:13s
epoch 15 | loss: 0.22694 | val_0_rmse: 0.50964 | val_1_rmse: 0.51433 |  0:00:14s
epoch 16 | loss: 0.21362 | val_0_rmse: 0.48764 | val_1_rmse: 0.4984  |  0:00:15s
epoch 17 | loss: 0.21171 | val_0_rmse: 0.48654 | val_1_rmse: 0.48801 |  0:00:16s
epoch 18 | loss: 0.20241 | val_0_rmse: 0.48152 | val_1_rmse: 0.48595 |  0:00:17s
epoch 19 | loss: 0.19704 | val_0_rmse: 0.51867 | val_1_rmse: 0.52503 |  0:00:18s
epoch 20 | loss: 0.20367 | val_0_rmse: 0.47205 | val_1_rmse: 0.47953 |  0:00:19s
epoch 21 | loss: 0.1974  | val_0_rmse: 0.45998 | val_1_rmse: 0.47174 |  0:00:20s
epoch 22 | loss: 0.20908 | val_0_rmse: 0.49368 | val_1_rmse: 0.50846 |  0:00:21s
epoch 23 | loss: 0.20161 | val_0_rmse: 0.47821 | val_1_rmse: 0.48598 |  0:00:22s
epoch 24 | loss: 0.23719 | val_0_rmse: 0.48075 | val_1_rmse: 0.48401 |  0:00:22s
epoch 25 | loss: 0.22815 | val_0_rmse: 0.48517 | val_1_rmse: 0.48787 |  0:00:23s
epoch 26 | loss: 0.21696 | val_0_rmse: 0.47114 | val_1_rmse: 0.47666 |  0:00:24s
epoch 27 | loss: 0.21103 | val_0_rmse: 0.46184 | val_1_rmse: 0.46177 |  0:00:25s
epoch 28 | loss: 0.21997 | val_0_rmse: 0.4661  | val_1_rmse: 0.46779 |  0:00:26s
epoch 29 | loss: 0.22477 | val_0_rmse: 0.47004 | val_1_rmse: 0.47244 |  0:00:27s
epoch 30 | loss: 0.2169  | val_0_rmse: 0.45268 | val_1_rmse: 0.45532 |  0:00:28s
epoch 31 | loss: 0.21889 | val_0_rmse: 0.4632  | val_1_rmse: 0.46644 |  0:00:29s
epoch 32 | loss: 0.21756 | val_0_rmse: 0.47871 | val_1_rmse: 0.47563 |  0:00:30s
epoch 33 | loss: 0.21147 | val_0_rmse: 0.44475 | val_1_rmse: 0.44408 |  0:00:31s
epoch 34 | loss: 0.20182 | val_0_rmse: 0.42707 | val_1_rmse: 0.43627 |  0:00:32s
epoch 35 | loss: 0.20348 | val_0_rmse: 0.4419  | val_1_rmse: 0.45313 |  0:00:33s
epoch 36 | loss: 0.22307 | val_0_rmse: 0.46262 | val_1_rmse: 0.46929 |  0:00:34s
epoch 37 | loss: 0.25011 | val_0_rmse: 0.49555 | val_1_rmse: 0.50193 |  0:00:35s
epoch 38 | loss: 0.22826 | val_0_rmse: 0.44888 | val_1_rmse: 0.46121 |  0:00:35s
epoch 39 | loss: 0.21591 | val_0_rmse: 0.49059 | val_1_rmse: 0.47771 |  0:00:36s
epoch 40 | loss: 0.23831 | val_0_rmse: 0.47845 | val_1_rmse: 0.4819  |  0:00:37s
epoch 41 | loss: 0.24076 | val_0_rmse: 0.4831  | val_1_rmse: 0.48566 |  0:00:38s
epoch 42 | loss: 0.23816 | val_0_rmse: 0.46536 | val_1_rmse: 0.45714 |  0:00:39s
epoch 43 | loss: 0.22601 | val_0_rmse: 0.43653 | val_1_rmse: 0.43861 |  0:00:40s
epoch 44 | loss: 0.20582 | val_0_rmse: 0.42209 | val_1_rmse: 0.42555 |  0:00:41s
epoch 45 | loss: 0.19971 | val_0_rmse: 0.42658 | val_1_rmse: 0.42813 |  0:00:42s
epoch 46 | loss: 0.19187 | val_0_rmse: 0.42522 | val_1_rmse: 0.42769 |  0:00:43s
epoch 47 | loss: 0.18858 | val_0_rmse: 0.45872 | val_1_rmse: 0.47425 |  0:00:44s
epoch 48 | loss: 0.18296 | val_0_rmse: 0.40299 | val_1_rmse: 0.40492 |  0:00:45s
epoch 49 | loss: 0.18323 | val_0_rmse: 0.4044  | val_1_rmse: 0.4013  |  0:00:46s
epoch 50 | loss: 0.17858 | val_0_rmse: 0.40151 | val_1_rmse: 0.40397 |  0:00:46s
epoch 51 | loss: 0.18127 | val_0_rmse: 0.41112 | val_1_rmse: 0.41849 |  0:00:47s
epoch 52 | loss: 0.17603 | val_0_rmse: 0.39589 | val_1_rmse: 0.39884 |  0:00:48s
epoch 53 | loss: 0.20225 | val_0_rmse: 0.44282 | val_1_rmse: 0.43476 |  0:00:49s
epoch 54 | loss: 0.22395 | val_0_rmse: 0.45149 | val_1_rmse: 0.44502 |  0:00:50s
epoch 55 | loss: 0.20164 | val_0_rmse: 0.45737 | val_1_rmse: 0.47013 |  0:00:51s
epoch 56 | loss: 0.20354 | val_0_rmse: 0.42281 | val_1_rmse: 0.42021 |  0:00:52s
epoch 57 | loss: 0.18396 | val_0_rmse: 0.39515 | val_1_rmse: 0.39121 |  0:00:53s
epoch 58 | loss: 0.1784  | val_0_rmse: 0.39427 | val_1_rmse: 0.39685 |  0:00:54s
epoch 59 | loss: 0.20725 | val_0_rmse: 0.50422 | val_1_rmse: 0.48429 |  0:00:55s
epoch 60 | loss: 0.23253 | val_0_rmse: 0.49315 | val_1_rmse: 0.50355 |  0:00:56s
epoch 61 | loss: 0.19924 | val_0_rmse: 0.42375 | val_1_rmse: 0.42275 |  0:00:57s
epoch 62 | loss: 0.18675 | val_0_rmse: 0.40971 | val_1_rmse: 0.41374 |  0:00:58s
epoch 63 | loss: 0.18405 | val_0_rmse: 0.41027 | val_1_rmse: 0.41953 |  0:00:58s
epoch 64 | loss: 0.18284 | val_0_rmse: 0.41034 | val_1_rmse: 0.41439 |  0:00:59s
epoch 65 | loss: 0.17947 | val_0_rmse: 0.40868 | val_1_rmse: 0.4177  |  0:01:00s
epoch 66 | loss: 0.17986 | val_0_rmse: 0.40235 | val_1_rmse: 0.41142 |  0:01:01s
epoch 67 | loss: 0.17615 | val_0_rmse: 0.40259 | val_1_rmse: 0.41064 |  0:01:02s
epoch 68 | loss: 0.17988 | val_0_rmse: 0.40388 | val_1_rmse: 0.41392 |  0:01:03s
epoch 69 | loss: 0.17755 | val_0_rmse: 0.41187 | val_1_rmse: 0.42072 |  0:01:04s
epoch 70 | loss: 0.18026 | val_0_rmse: 0.39827 | val_1_rmse: 0.40045 |  0:01:05s
epoch 71 | loss: 0.17473 | val_0_rmse: 0.39866 | val_1_rmse: 0.40861 |  0:01:06s
epoch 72 | loss: 0.17043 | val_0_rmse: 0.41428 | val_1_rmse: 0.4272  |  0:01:07s
epoch 73 | loss: 0.17628 | val_0_rmse: 0.42275 | val_1_rmse: 0.43711 |  0:01:08s
epoch 74 | loss: 0.18313 | val_0_rmse: 0.40947 | val_1_rmse: 0.41534 |  0:01:08s
epoch 75 | loss: 0.17756 | val_0_rmse: 0.42293 | val_1_rmse: 0.4214  |  0:01:09s
epoch 76 | loss: 0.17962 | val_0_rmse: 0.39825 | val_1_rmse: 0.40612 |  0:01:10s
epoch 77 | loss: 0.17039 | val_0_rmse: 0.39903 | val_1_rmse: 0.40925 |  0:01:11s
epoch 78 | loss: 0.17031 | val_0_rmse: 0.38815 | val_1_rmse: 0.39711 |  0:01:12s
epoch 79 | loss: 0.16737 | val_0_rmse: 0.3896  | val_1_rmse: 0.39803 |  0:01:13s
epoch 80 | loss: 0.16869 | val_0_rmse: 0.39325 | val_1_rmse: 0.39378 |  0:01:14s
epoch 81 | loss: 0.17781 | val_0_rmse: 0.40663 | val_1_rmse: 0.4162  |  0:01:15s
epoch 82 | loss: 0.17565 | val_0_rmse: 0.40403 | val_1_rmse: 0.41248 |  0:01:16s
epoch 83 | loss: 0.17843 | val_0_rmse: 0.4503  | val_1_rmse: 0.44795 |  0:01:17s
epoch 84 | loss: 0.17669 | val_0_rmse: 0.39747 | val_1_rmse: 0.40857 |  0:01:18s
epoch 85 | loss: 0.17199 | val_0_rmse: 0.38416 | val_1_rmse: 0.39255 |  0:01:19s
epoch 86 | loss: 0.1681  | val_0_rmse: 0.40438 | val_1_rmse: 0.41342 |  0:01:19s
epoch 87 | loss: 0.17089 | val_0_rmse: 0.39957 | val_1_rmse: 0.40255 |  0:01:20s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.39121
Best weights from best epoch are automatically used!
ended training at: 04:20:50
Feature importance:
Mean squared error is of 0.09423838696006967
Mean absolute error:0.18678127936761976
MAPE:0.211751370409719
R2 score:0.8168980737898357
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:20:51
epoch 0  | loss: 1.70021 | val_0_rmse: 0.99713 | val_1_rmse: 0.98616 |  0:00:00s
epoch 1  | loss: 1.09818 | val_0_rmse: 0.99236 | val_1_rmse: 0.98598 |  0:00:00s
epoch 2  | loss: 0.94174 | val_0_rmse: 0.92996 | val_1_rmse: 0.92584 |  0:00:00s
epoch 3  | loss: 0.86423 | val_0_rmse: 0.90714 | val_1_rmse: 0.90799 |  0:00:01s
epoch 4  | loss: 0.83232 | val_0_rmse: 0.89878 | val_1_rmse: 0.89289 |  0:00:01s
epoch 5  | loss: 0.78416 | val_0_rmse: 0.88605 | val_1_rmse: 0.88638 |  0:00:01s
epoch 6  | loss: 0.74534 | val_0_rmse: 0.91719 | val_1_rmse: 0.92086 |  0:00:02s
epoch 7  | loss: 0.71994 | val_0_rmse: 0.86854 | val_1_rmse: 0.86932 |  0:00:02s
epoch 8  | loss: 0.68659 | val_0_rmse: 0.84985 | val_1_rmse: 0.85339 |  0:00:02s
epoch 9  | loss: 0.6723  | val_0_rmse: 0.82507 | val_1_rmse: 0.82371 |  0:00:03s
epoch 10 | loss: 0.66656 | val_0_rmse: 0.80369 | val_1_rmse: 0.80616 |  0:00:03s
epoch 11 | loss: 0.65206 | val_0_rmse: 0.80847 | val_1_rmse: 0.80882 |  0:00:03s
epoch 12 | loss: 0.64578 | val_0_rmse: 0.80778 | val_1_rmse: 0.80417 |  0:00:04s
epoch 13 | loss: 0.6397  | val_0_rmse: 0.8063  | val_1_rmse: 0.80228 |  0:00:04s
epoch 14 | loss: 0.64408 | val_0_rmse: 0.80588 | val_1_rmse: 0.80368 |  0:00:04s
epoch 15 | loss: 0.63175 | val_0_rmse: 0.81478 | val_1_rmse: 0.80994 |  0:00:04s
epoch 16 | loss: 0.63347 | val_0_rmse: 0.80556 | val_1_rmse: 0.80689 |  0:00:05s
epoch 17 | loss: 0.64459 | val_0_rmse: 0.80017 | val_1_rmse: 0.79897 |  0:00:05s
epoch 18 | loss: 0.62506 | val_0_rmse: 0.80827 | val_1_rmse: 0.81015 |  0:00:05s
epoch 19 | loss: 0.622   | val_0_rmse: 0.83215 | val_1_rmse: 0.82921 |  0:00:06s
epoch 20 | loss: 0.62667 | val_0_rmse: 0.80538 | val_1_rmse: 0.79969 |  0:00:06s
epoch 21 | loss: 0.61972 | val_0_rmse: 0.80448 | val_1_rmse: 0.80069 |  0:00:06s
epoch 22 | loss: 0.62129 | val_0_rmse: 0.80577 | val_1_rmse: 0.80639 |  0:00:07s
epoch 23 | loss: 0.62516 | val_0_rmse: 0.80222 | val_1_rmse: 0.79892 |  0:00:07s
epoch 24 | loss: 0.61807 | val_0_rmse: 0.79673 | val_1_rmse: 0.7939  |  0:00:07s
epoch 25 | loss: 0.61594 | val_0_rmse: 0.80232 | val_1_rmse: 0.79593 |  0:00:07s
epoch 26 | loss: 0.61999 | val_0_rmse: 0.79564 | val_1_rmse: 0.79377 |  0:00:08s
epoch 27 | loss: 0.60997 | val_0_rmse: 0.79739 | val_1_rmse: 0.79272 |  0:00:08s
epoch 28 | loss: 0.61869 | val_0_rmse: 0.79809 | val_1_rmse: 0.78782 |  0:00:08s
epoch 29 | loss: 0.61094 | val_0_rmse: 0.79678 | val_1_rmse: 0.78554 |  0:00:09s
epoch 30 | loss: 0.61911 | val_0_rmse: 0.79363 | val_1_rmse: 0.78672 |  0:00:09s
epoch 31 | loss: 0.61214 | val_0_rmse: 0.79637 | val_1_rmse: 0.7896  |  0:00:09s
epoch 32 | loss: 0.60802 | val_0_rmse: 0.80307 | val_1_rmse: 0.79634 |  0:00:10s
epoch 33 | loss: 0.61021 | val_0_rmse: 0.79427 | val_1_rmse: 0.78648 |  0:00:10s
epoch 34 | loss: 0.60667 | val_0_rmse: 0.79546 | val_1_rmse: 0.78961 |  0:00:10s
epoch 35 | loss: 0.60435 | val_0_rmse: 0.7914  | val_1_rmse: 0.78474 |  0:00:10s
epoch 36 | loss: 0.60929 | val_0_rmse: 0.79449 | val_1_rmse: 0.78794 |  0:00:11s
epoch 37 | loss: 0.61021 | val_0_rmse: 0.79188 | val_1_rmse: 0.78786 |  0:00:11s
epoch 38 | loss: 0.60758 | val_0_rmse: 0.79111 | val_1_rmse: 0.78982 |  0:00:11s
epoch 39 | loss: 0.60657 | val_0_rmse: 0.78924 | val_1_rmse: 0.79059 |  0:00:12s
epoch 40 | loss: 0.59902 | val_0_rmse: 0.78753 | val_1_rmse: 0.78324 |  0:00:12s
epoch 41 | loss: 0.59609 | val_0_rmse: 0.78662 | val_1_rmse: 0.78355 |  0:00:12s
epoch 42 | loss: 0.59421 | val_0_rmse: 0.78455 | val_1_rmse: 0.77925 |  0:00:13s
epoch 43 | loss: 0.6032  | val_0_rmse: 0.78784 | val_1_rmse: 0.78788 |  0:00:13s
epoch 44 | loss: 0.59954 | val_0_rmse: 0.78557 | val_1_rmse: 0.78314 |  0:00:13s
epoch 45 | loss: 0.61146 | val_0_rmse: 0.79242 | val_1_rmse: 0.78473 |  0:00:14s
epoch 46 | loss: 0.60307 | val_0_rmse: 0.79656 | val_1_rmse: 0.78713 |  0:00:14s
epoch 47 | loss: 0.6112  | val_0_rmse: 0.78991 | val_1_rmse: 0.78083 |  0:00:14s
epoch 48 | loss: 0.61422 | val_0_rmse: 0.79443 | val_1_rmse: 0.78394 |  0:00:14s
epoch 49 | loss: 0.61241 | val_0_rmse: 0.78866 | val_1_rmse: 0.78128 |  0:00:15s
epoch 50 | loss: 0.59871 | val_0_rmse: 0.7929  | val_1_rmse: 0.78544 |  0:00:15s
epoch 51 | loss: 0.5982  | val_0_rmse: 0.78925 | val_1_rmse: 0.78291 |  0:00:15s
epoch 52 | loss: 0.59623 | val_0_rmse: 0.78409 | val_1_rmse: 0.78195 |  0:00:16s
epoch 53 | loss: 0.59617 | val_0_rmse: 0.7853  | val_1_rmse: 0.78487 |  0:00:16s
epoch 54 | loss: 0.59537 | val_0_rmse: 0.78167 | val_1_rmse: 0.77856 |  0:00:16s
epoch 55 | loss: 0.60596 | val_0_rmse: 0.78755 | val_1_rmse: 0.79351 |  0:00:17s
epoch 56 | loss: 0.60137 | val_0_rmse: 0.7907  | val_1_rmse: 0.79631 |  0:00:17s
epoch 57 | loss: 0.59496 | val_0_rmse: 0.78118 | val_1_rmse: 0.78787 |  0:00:17s
epoch 58 | loss: 0.59042 | val_0_rmse: 0.7813  | val_1_rmse: 0.78762 |  0:00:17s
epoch 59 | loss: 0.58524 | val_0_rmse: 0.77469 | val_1_rmse: 0.77842 |  0:00:18s
epoch 60 | loss: 0.59043 | val_0_rmse: 0.78674 | val_1_rmse: 0.79511 |  0:00:18s
epoch 61 | loss: 0.5919  | val_0_rmse: 0.77421 | val_1_rmse: 0.77957 |  0:00:18s
epoch 62 | loss: 0.58384 | val_0_rmse: 0.77769 | val_1_rmse: 0.78936 |  0:00:19s
epoch 63 | loss: 0.58318 | val_0_rmse: 0.77952 | val_1_rmse: 0.78916 |  0:00:19s
epoch 64 | loss: 0.57955 | val_0_rmse: 0.77735 | val_1_rmse: 0.77974 |  0:00:19s
epoch 65 | loss: 0.57929 | val_0_rmse: 0.77467 | val_1_rmse: 0.78094 |  0:00:20s
epoch 66 | loss: 0.57578 | val_0_rmse: 0.77418 | val_1_rmse: 0.77891 |  0:00:20s
epoch 67 | loss: 0.5694  | val_0_rmse: 0.77103 | val_1_rmse: 0.77571 |  0:00:20s
epoch 68 | loss: 0.56433 | val_0_rmse: 0.76899 | val_1_rmse: 0.7754  |  0:00:20s
epoch 69 | loss: 0.56844 | val_0_rmse: 0.7688  | val_1_rmse: 0.77802 |  0:00:21s
epoch 70 | loss: 0.56636 | val_0_rmse: 0.76557 | val_1_rmse: 0.7743  |  0:00:21s
epoch 71 | loss: 0.56935 | val_0_rmse: 0.76566 | val_1_rmse: 0.77434 |  0:00:21s
epoch 72 | loss: 0.5697  | val_0_rmse: 0.76291 | val_1_rmse: 0.77008 |  0:00:22s
epoch 73 | loss: 0.55612 | val_0_rmse: 0.76187 | val_1_rmse: 0.76864 |  0:00:22s
epoch 74 | loss: 0.5634  | val_0_rmse: 0.76523 | val_1_rmse: 0.76795 |  0:00:22s
epoch 75 | loss: 0.56692 | val_0_rmse: 0.76417 | val_1_rmse: 0.76386 |  0:00:23s
epoch 76 | loss: 0.56383 | val_0_rmse: 0.75933 | val_1_rmse: 0.75389 |  0:00:23s
epoch 77 | loss: 0.56622 | val_0_rmse: 0.75769 | val_1_rmse: 0.75561 |  0:00:23s
epoch 78 | loss: 0.56401 | val_0_rmse: 0.75988 | val_1_rmse: 0.76004 |  0:00:24s
epoch 79 | loss: 0.5655  | val_0_rmse: 0.76447 | val_1_rmse: 0.76072 |  0:00:24s
epoch 80 | loss: 0.56879 | val_0_rmse: 0.75922 | val_1_rmse: 0.75521 |  0:00:24s
epoch 81 | loss: 0.56508 | val_0_rmse: 0.75891 | val_1_rmse: 0.75694 |  0:00:25s
epoch 82 | loss: 0.56579 | val_0_rmse: 0.75458 | val_1_rmse: 0.75574 |  0:00:25s
epoch 83 | loss: 0.56241 | val_0_rmse: 0.75198 | val_1_rmse: 0.75564 |  0:00:25s
epoch 84 | loss: 0.56734 | val_0_rmse: 0.74767 | val_1_rmse: 0.75589 |  0:00:25s
epoch 85 | loss: 0.5531  | val_0_rmse: 0.75153 | val_1_rmse: 0.75568 |  0:00:26s
epoch 86 | loss: 0.56324 | val_0_rmse: 0.7498  | val_1_rmse: 0.75869 |  0:00:26s
epoch 87 | loss: 0.56274 | val_0_rmse: 0.75017 | val_1_rmse: 0.75739 |  0:00:26s
epoch 88 | loss: 0.55993 | val_0_rmse: 0.74948 | val_1_rmse: 0.75601 |  0:00:27s
epoch 89 | loss: 0.56276 | val_0_rmse: 0.74793 | val_1_rmse: 0.75417 |  0:00:27s
epoch 90 | loss: 0.56188 | val_0_rmse: 0.74484 | val_1_rmse: 0.75521 |  0:00:27s
epoch 91 | loss: 0.55245 | val_0_rmse: 0.74448 | val_1_rmse: 0.75807 |  0:00:28s
epoch 92 | loss: 0.54646 | val_0_rmse: 0.7463  | val_1_rmse: 0.75747 |  0:00:28s
epoch 93 | loss: 0.55398 | val_0_rmse: 0.74262 | val_1_rmse: 0.75176 |  0:00:28s
epoch 94 | loss: 0.5506  | val_0_rmse: 0.7416  | val_1_rmse: 0.7575  |  0:00:28s
epoch 95 | loss: 0.5494  | val_0_rmse: 0.73631 | val_1_rmse: 0.75241 |  0:00:29s
epoch 96 | loss: 0.55054 | val_0_rmse: 0.73369 | val_1_rmse: 0.7578  |  0:00:29s
epoch 97 | loss: 0.54602 | val_0_rmse: 0.74107 | val_1_rmse: 0.77273 |  0:00:29s
epoch 98 | loss: 0.54465 | val_0_rmse: 0.73524 | val_1_rmse: 0.75577 |  0:00:30s
epoch 99 | loss: 0.54508 | val_0_rmse: 0.73092 | val_1_rmse: 0.75915 |  0:00:30s
epoch 100| loss: 0.54256 | val_0_rmse: 0.73366 | val_1_rmse: 0.7613  |  0:00:30s
epoch 101| loss: 0.53808 | val_0_rmse: 0.73297 | val_1_rmse: 0.75575 |  0:00:30s
epoch 102| loss: 0.53983 | val_0_rmse: 0.73187 | val_1_rmse: 0.75477 |  0:00:31s
epoch 103| loss: 0.54167 | val_0_rmse: 0.73568 | val_1_rmse: 0.76662 |  0:00:31s
epoch 104| loss: 0.54258 | val_0_rmse: 0.73122 | val_1_rmse: 0.76028 |  0:00:31s
epoch 105| loss: 0.54077 | val_0_rmse: 0.73351 | val_1_rmse: 0.76093 |  0:00:32s
epoch 106| loss: 0.54265 | val_0_rmse: 0.73054 | val_1_rmse: 0.76055 |  0:00:32s
epoch 107| loss: 0.54299 | val_0_rmse: 0.73439 | val_1_rmse: 0.76436 |  0:00:32s
epoch 108| loss: 0.53388 | val_0_rmse: 0.72991 | val_1_rmse: 0.75982 |  0:00:33s
epoch 109| loss: 0.53349 | val_0_rmse: 0.73015 | val_1_rmse: 0.76175 |  0:00:33s
epoch 110| loss: 0.53634 | val_0_rmse: 0.72186 | val_1_rmse: 0.76616 |  0:00:33s
epoch 111| loss: 0.53541 | val_0_rmse: 0.72573 | val_1_rmse: 0.76787 |  0:00:33s
epoch 112| loss: 0.53288 | val_0_rmse: 0.72623 | val_1_rmse: 0.78129 |  0:00:34s
epoch 113| loss: 0.54127 | val_0_rmse: 0.73009 | val_1_rmse: 0.7734  |  0:00:34s
epoch 114| loss: 0.53807 | val_0_rmse: 0.72438 | val_1_rmse: 0.77022 |  0:00:34s
epoch 115| loss: 0.52792 | val_0_rmse: 0.72421 | val_1_rmse: 0.78041 |  0:00:35s
epoch 116| loss: 0.5302  | val_0_rmse: 0.71904 | val_1_rmse: 0.77316 |  0:00:35s
epoch 117| loss: 0.52175 | val_0_rmse: 0.72993 | val_1_rmse: 0.77572 |  0:00:35s
epoch 118| loss: 0.52908 | val_0_rmse: 0.72138 | val_1_rmse: 0.76458 |  0:00:36s
epoch 119| loss: 0.52324 | val_0_rmse: 0.71571 | val_1_rmse: 0.76458 |  0:00:36s
epoch 120| loss: 0.52149 | val_0_rmse: 0.71164 | val_1_rmse: 0.76078 |  0:00:36s
epoch 121| loss: 0.52266 | val_0_rmse: 0.71161 | val_1_rmse: 0.75895 |  0:00:37s
epoch 122| loss: 0.52765 | val_0_rmse: 0.72173 | val_1_rmse: 0.7848  |  0:00:37s
epoch 123| loss: 0.54025 | val_0_rmse: 0.7238  | val_1_rmse: 0.76448 |  0:00:37s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.75176
Best weights from best epoch are automatically used!
ended training at: 04:21:28
Feature importance:
Mean squared error is of 0.04002531470706932
Mean absolute error:0.14579242002480175
MAPE:0.14840633639023712
R2 score:0.4105415226774677
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:21:29
epoch 0  | loss: 1.73367 | val_0_rmse: 0.9769  | val_1_rmse: 0.96681 |  0:00:00s
epoch 1  | loss: 1.06601 | val_0_rmse: 0.98097 | val_1_rmse: 0.97657 |  0:00:00s
epoch 2  | loss: 0.89426 | val_0_rmse: 1.01832 | val_1_rmse: 1.01879 |  0:00:00s
epoch 3  | loss: 0.82501 | val_0_rmse: 1.00525 | val_1_rmse: 1.01839 |  0:00:01s
epoch 4  | loss: 0.73969 | val_0_rmse: 0.96954 | val_1_rmse: 0.9788  |  0:00:01s
epoch 5  | loss: 0.69747 | val_0_rmse: 0.91253 | val_1_rmse: 0.9249  |  0:00:01s
epoch 6  | loss: 0.68001 | val_0_rmse: 0.89328 | val_1_rmse: 0.91102 |  0:00:02s
epoch 7  | loss: 0.67291 | val_0_rmse: 0.87751 | val_1_rmse: 0.88565 |  0:00:02s
epoch 8  | loss: 0.66307 | val_0_rmse: 0.89287 | val_1_rmse: 0.91248 |  0:00:02s
epoch 9  | loss: 0.64027 | val_0_rmse: 0.87305 | val_1_rmse: 0.87871 |  0:00:03s
epoch 10 | loss: 0.64568 | val_0_rmse: 0.85734 | val_1_rmse: 0.86315 |  0:00:03s
epoch 11 | loss: 0.6447  | val_0_rmse: 0.8329  | val_1_rmse: 0.83661 |  0:00:03s
epoch 12 | loss: 0.63155 | val_0_rmse: 0.81785 | val_1_rmse: 0.82947 |  0:00:03s
epoch 13 | loss: 0.65617 | val_0_rmse: 0.81213 | val_1_rmse: 0.8233  |  0:00:04s
epoch 14 | loss: 0.65904 | val_0_rmse: 0.81354 | val_1_rmse: 0.82425 |  0:00:04s
epoch 15 | loss: 0.6461  | val_0_rmse: 0.80371 | val_1_rmse: 0.81163 |  0:00:04s
epoch 16 | loss: 0.64628 | val_0_rmse: 0.80721 | val_1_rmse: 0.82226 |  0:00:05s
epoch 17 | loss: 0.63085 | val_0_rmse: 0.79689 | val_1_rmse: 0.80698 |  0:00:05s
epoch 18 | loss: 0.61793 | val_0_rmse: 0.80104 | val_1_rmse: 0.81078 |  0:00:05s
epoch 19 | loss: 0.62312 | val_0_rmse: 0.79713 | val_1_rmse: 0.80444 |  0:00:06s
epoch 20 | loss: 0.61523 | val_0_rmse: 0.81134 | val_1_rmse: 0.82256 |  0:00:06s
epoch 21 | loss: 0.60412 | val_0_rmse: 0.80864 | val_1_rmse: 0.81596 |  0:00:06s
epoch 22 | loss: 0.61593 | val_0_rmse: 0.80075 | val_1_rmse: 0.8107  |  0:00:07s
epoch 23 | loss: 0.6157  | val_0_rmse: 0.79036 | val_1_rmse: 0.79195 |  0:00:07s
epoch 24 | loss: 0.6132  | val_0_rmse: 0.79698 | val_1_rmse: 0.8022  |  0:00:07s
epoch 25 | loss: 0.61821 | val_0_rmse: 0.79561 | val_1_rmse: 0.79076 |  0:00:07s
epoch 26 | loss: 0.6103  | val_0_rmse: 0.79502 | val_1_rmse: 0.7889  |  0:00:08s
epoch 27 | loss: 0.60632 | val_0_rmse: 0.79635 | val_1_rmse: 0.79123 |  0:00:08s
epoch 28 | loss: 0.60147 | val_0_rmse: 0.796   | val_1_rmse: 0.78686 |  0:00:08s
epoch 29 | loss: 0.60189 | val_0_rmse: 0.7955  | val_1_rmse: 0.79605 |  0:00:09s
epoch 30 | loss: 0.59402 | val_0_rmse: 0.79051 | val_1_rmse: 0.78958 |  0:00:09s
epoch 31 | loss: 0.58985 | val_0_rmse: 0.78982 | val_1_rmse: 0.79377 |  0:00:09s
epoch 32 | loss: 0.58569 | val_0_rmse: 0.78718 | val_1_rmse: 0.79144 |  0:00:10s
epoch 33 | loss: 0.58926 | val_0_rmse: 0.78345 | val_1_rmse: 0.78991 |  0:00:10s
epoch 34 | loss: 0.58524 | val_0_rmse: 0.78988 | val_1_rmse: 0.8005  |  0:00:10s
epoch 35 | loss: 0.59046 | val_0_rmse: 0.78565 | val_1_rmse: 0.79443 |  0:00:10s
epoch 36 | loss: 0.58087 | val_0_rmse: 0.79557 | val_1_rmse: 0.80605 |  0:00:11s
epoch 37 | loss: 0.57844 | val_0_rmse: 0.78408 | val_1_rmse: 0.79644 |  0:00:11s
epoch 38 | loss: 0.57364 | val_0_rmse: 0.78029 | val_1_rmse: 0.79392 |  0:00:11s
epoch 39 | loss: 0.56507 | val_0_rmse: 0.78794 | val_1_rmse: 0.80164 |  0:00:12s
epoch 40 | loss: 0.57555 | val_0_rmse: 0.77376 | val_1_rmse: 0.78196 |  0:00:12s
epoch 41 | loss: 0.57037 | val_0_rmse: 0.77829 | val_1_rmse: 0.78254 |  0:00:12s
epoch 42 | loss: 0.56794 | val_0_rmse: 0.78931 | val_1_rmse: 0.79208 |  0:00:13s
epoch 43 | loss: 0.57333 | val_0_rmse: 0.77958 | val_1_rmse: 0.78561 |  0:00:13s
epoch 44 | loss: 0.574   | val_0_rmse: 0.77358 | val_1_rmse: 0.78511 |  0:00:13s
epoch 45 | loss: 0.57164 | val_0_rmse: 0.78139 | val_1_rmse: 0.79376 |  0:00:13s
epoch 46 | loss: 0.56866 | val_0_rmse: 0.77215 | val_1_rmse: 0.77975 |  0:00:14s
epoch 47 | loss: 0.56503 | val_0_rmse: 0.77382 | val_1_rmse: 0.78327 |  0:00:14s
epoch 48 | loss: 0.56114 | val_0_rmse: 0.78098 | val_1_rmse: 0.79076 |  0:00:14s
epoch 49 | loss: 0.56231 | val_0_rmse: 0.77125 | val_1_rmse: 0.78152 |  0:00:15s
epoch 50 | loss: 0.56337 | val_0_rmse: 0.76426 | val_1_rmse: 0.77409 |  0:00:15s
epoch 51 | loss: 0.56462 | val_0_rmse: 0.7606  | val_1_rmse: 0.77714 |  0:00:15s
epoch 52 | loss: 0.55816 | val_0_rmse: 0.76284 | val_1_rmse: 0.77468 |  0:00:16s
epoch 53 | loss: 0.5558  | val_0_rmse: 0.76655 | val_1_rmse: 0.78005 |  0:00:16s
epoch 54 | loss: 0.55738 | val_0_rmse: 0.76221 | val_1_rmse: 0.77906 |  0:00:16s
epoch 55 | loss: 0.5575  | val_0_rmse: 0.76015 | val_1_rmse: 0.77405 |  0:00:16s
epoch 56 | loss: 0.55629 | val_0_rmse: 0.77766 | val_1_rmse: 0.79229 |  0:00:17s
epoch 57 | loss: 0.5558  | val_0_rmse: 0.76196 | val_1_rmse: 0.77408 |  0:00:17s
epoch 58 | loss: 0.54758 | val_0_rmse: 0.7577  | val_1_rmse: 0.77164 |  0:00:17s
epoch 59 | loss: 0.55103 | val_0_rmse: 0.7615  | val_1_rmse: 0.77806 |  0:00:18s
epoch 60 | loss: 0.55284 | val_0_rmse: 0.76068 | val_1_rmse: 0.77516 |  0:00:18s
epoch 61 | loss: 0.54415 | val_0_rmse: 0.76099 | val_1_rmse: 0.77867 |  0:00:18s
epoch 62 | loss: 0.54721 | val_0_rmse: 0.76424 | val_1_rmse: 0.77321 |  0:00:19s
epoch 63 | loss: 0.55203 | val_0_rmse: 0.7593  | val_1_rmse: 0.77502 |  0:00:19s
epoch 64 | loss: 0.54989 | val_0_rmse: 0.75485 | val_1_rmse: 0.77401 |  0:00:19s
epoch 65 | loss: 0.55855 | val_0_rmse: 0.75079 | val_1_rmse: 0.77186 |  0:00:20s
epoch 66 | loss: 0.54144 | val_0_rmse: 0.75218 | val_1_rmse: 0.77341 |  0:00:20s
epoch 67 | loss: 0.54084 | val_0_rmse: 0.75218 | val_1_rmse: 0.77218 |  0:00:20s
epoch 68 | loss: 0.54312 | val_0_rmse: 0.74228 | val_1_rmse: 0.7758  |  0:00:20s
epoch 69 | loss: 0.54544 | val_0_rmse: 0.74829 | val_1_rmse: 0.77131 |  0:00:21s
epoch 70 | loss: 0.54521 | val_0_rmse: 0.75128 | val_1_rmse: 0.78099 |  0:00:21s
epoch 71 | loss: 0.55332 | val_0_rmse: 0.75495 | val_1_rmse: 0.77883 |  0:00:21s
epoch 72 | loss: 0.5412  | val_0_rmse: 0.75279 | val_1_rmse: 0.784   |  0:00:22s
epoch 73 | loss: 0.55139 | val_0_rmse: 0.74301 | val_1_rmse: 0.7768  |  0:00:22s
epoch 74 | loss: 0.52526 | val_0_rmse: 0.74729 | val_1_rmse: 0.77918 |  0:00:22s
epoch 75 | loss: 0.53753 | val_0_rmse: 0.74164 | val_1_rmse: 0.7804  |  0:00:23s
epoch 76 | loss: 0.5294  | val_0_rmse: 0.73573 | val_1_rmse: 0.77707 |  0:00:23s
epoch 77 | loss: 0.53006 | val_0_rmse: 0.7381  | val_1_rmse: 0.77114 |  0:00:23s
epoch 78 | loss: 0.53332 | val_0_rmse: 0.73087 | val_1_rmse: 0.76986 |  0:00:23s
epoch 79 | loss: 0.52614 | val_0_rmse: 0.74361 | val_1_rmse: 0.77943 |  0:00:24s
epoch 80 | loss: 0.53225 | val_0_rmse: 0.73596 | val_1_rmse: 0.7774  |  0:00:24s
epoch 81 | loss: 0.53975 | val_0_rmse: 0.73786 | val_1_rmse: 0.77608 |  0:00:24s
epoch 82 | loss: 0.5288  | val_0_rmse: 0.73965 | val_1_rmse: 0.7848  |  0:00:25s
epoch 83 | loss: 0.53023 | val_0_rmse: 0.73033 | val_1_rmse: 0.77172 |  0:00:25s
epoch 84 | loss: 0.52506 | val_0_rmse: 0.72476 | val_1_rmse: 0.7749  |  0:00:25s
epoch 85 | loss: 0.52385 | val_0_rmse: 0.72383 | val_1_rmse: 0.78216 |  0:00:26s
epoch 86 | loss: 0.53051 | val_0_rmse: 0.72016 | val_1_rmse: 0.78515 |  0:00:26s
epoch 87 | loss: 0.52012 | val_0_rmse: 0.71782 | val_1_rmse: 0.79348 |  0:00:26s
epoch 88 | loss: 0.52072 | val_0_rmse: 0.71886 | val_1_rmse: 0.79121 |  0:00:26s
epoch 89 | loss: 0.51877 | val_0_rmse: 0.71312 | val_1_rmse: 0.78851 |  0:00:27s
epoch 90 | loss: 0.52273 | val_0_rmse: 0.71442 | val_1_rmse: 0.78007 |  0:00:27s
epoch 91 | loss: 0.51368 | val_0_rmse: 0.71933 | val_1_rmse: 0.78592 |  0:00:27s
epoch 92 | loss: 0.51434 | val_0_rmse: 0.71576 | val_1_rmse: 0.78281 |  0:00:28s
epoch 93 | loss: 0.52427 | val_0_rmse: 0.71983 | val_1_rmse: 0.77616 |  0:00:28s
epoch 94 | loss: 0.52507 | val_0_rmse: 0.71768 | val_1_rmse: 0.76975 |  0:00:28s
epoch 95 | loss: 0.51868 | val_0_rmse: 0.72145 | val_1_rmse: 0.77694 |  0:00:29s
epoch 96 | loss: 0.52425 | val_0_rmse: 0.71605 | val_1_rmse: 0.77555 |  0:00:29s
epoch 97 | loss: 0.51602 | val_0_rmse: 0.72344 | val_1_rmse: 0.77955 |  0:00:29s
epoch 98 | loss: 0.52745 | val_0_rmse: 0.7181  | val_1_rmse: 0.78302 |  0:00:30s
epoch 99 | loss: 0.51833 | val_0_rmse: 0.71547 | val_1_rmse: 0.77219 |  0:00:30s
epoch 100| loss: 0.51959 | val_0_rmse: 0.70734 | val_1_rmse: 0.77581 |  0:00:30s
epoch 101| loss: 0.51273 | val_0_rmse: 0.70772 | val_1_rmse: 0.7787  |  0:00:31s
epoch 102| loss: 0.50957 | val_0_rmse: 0.70329 | val_1_rmse: 0.79213 |  0:00:31s
epoch 103| loss: 0.50739 | val_0_rmse: 0.69642 | val_1_rmse: 0.78012 |  0:00:31s
epoch 104| loss: 0.50134 | val_0_rmse: 0.70056 | val_1_rmse: 0.78159 |  0:00:31s
epoch 105| loss: 0.50406 | val_0_rmse: 0.70375 | val_1_rmse: 0.78682 |  0:00:32s
epoch 106| loss: 0.49707 | val_0_rmse: 0.69783 | val_1_rmse: 0.80515 |  0:00:32s
epoch 107| loss: 0.49722 | val_0_rmse: 0.70067 | val_1_rmse: 0.80283 |  0:00:32s
epoch 108| loss: 0.50661 | val_0_rmse: 0.70803 | val_1_rmse: 0.80241 |  0:00:33s
epoch 109| loss: 0.5063  | val_0_rmse: 0.69869 | val_1_rmse: 0.79578 |  0:00:33s
epoch 110| loss: 0.50469 | val_0_rmse: 0.71542 | val_1_rmse: 0.79801 |  0:00:33s
epoch 111| loss: 0.50351 | val_0_rmse: 0.69749 | val_1_rmse: 0.80178 |  0:00:33s
epoch 112| loss: 0.50708 | val_0_rmse: 0.69654 | val_1_rmse: 0.78508 |  0:00:34s
epoch 113| loss: 0.50035 | val_0_rmse: 0.70058 | val_1_rmse: 0.78328 |  0:00:34s
epoch 114| loss: 0.49601 | val_0_rmse: 0.68909 | val_1_rmse: 0.77897 |  0:00:34s
epoch 115| loss: 0.4958  | val_0_rmse: 0.69338 | val_1_rmse: 0.80579 |  0:00:35s
epoch 116| loss: 0.49906 | val_0_rmse: 0.6945  | val_1_rmse: 0.7972  |  0:00:35s
epoch 117| loss: 0.49478 | val_0_rmse: 0.69731 | val_1_rmse: 0.77946 |  0:00:35s
epoch 118| loss: 0.5016  | val_0_rmse: 0.69297 | val_1_rmse: 0.79002 |  0:00:36s
epoch 119| loss: 0.49317 | val_0_rmse: 0.69227 | val_1_rmse: 0.79896 |  0:00:36s
epoch 120| loss: 0.48823 | val_0_rmse: 0.68689 | val_1_rmse: 0.78095 |  0:00:36s
epoch 121| loss: 0.49647 | val_0_rmse: 0.6868  | val_1_rmse: 0.79225 |  0:00:36s
epoch 122| loss: 0.49236 | val_0_rmse: 0.687   | val_1_rmse: 0.79027 |  0:00:37s
epoch 123| loss: 0.47991 | val_0_rmse: 0.69258 | val_1_rmse: 0.79134 |  0:00:37s
epoch 124| loss: 0.49034 | val_0_rmse: 0.68063 | val_1_rmse: 0.78708 |  0:00:37s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.76975
Best weights from best epoch are automatically used!
ended training at: 04:22:07
Feature importance:
Mean squared error is of 0.039478854357493556
Mean absolute error:0.1441120799362114
MAPE:0.14827419706966954
R2 score:0.3823548340748403
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:22:07
epoch 0  | loss: 1.83513 | val_0_rmse: 0.99914 | val_1_rmse: 0.97171 |  0:00:00s
epoch 1  | loss: 1.09569 | val_0_rmse: 0.96503 | val_1_rmse: 0.94207 |  0:00:00s
epoch 2  | loss: 0.90417 | val_0_rmse: 0.90199 | val_1_rmse: 0.89765 |  0:00:01s
epoch 3  | loss: 0.82777 | val_0_rmse: 0.88158 | val_1_rmse: 0.8961  |  0:00:01s
epoch 4  | loss: 0.74478 | val_0_rmse: 0.84795 | val_1_rmse: 0.87306 |  0:00:01s
epoch 5  | loss: 0.71075 | val_0_rmse: 0.84441 | val_1_rmse: 0.88329 |  0:00:01s
epoch 6  | loss: 0.68096 | val_0_rmse: 0.81285 | val_1_rmse: 0.84891 |  0:00:02s
epoch 7  | loss: 0.66396 | val_0_rmse: 0.86999 | val_1_rmse: 0.92445 |  0:00:02s
epoch 8  | loss: 0.6557  | val_0_rmse: 0.84209 | val_1_rmse: 0.89324 |  0:00:02s
epoch 9  | loss: 0.63564 | val_0_rmse: 0.80498 | val_1_rmse: 0.83946 |  0:00:03s
epoch 10 | loss: 0.63413 | val_0_rmse: 0.8317  | val_1_rmse: 0.88202 |  0:00:03s
epoch 11 | loss: 0.63666 | val_0_rmse: 0.79884 | val_1_rmse: 0.8365  |  0:00:03s
epoch 12 | loss: 0.62478 | val_0_rmse: 0.81943 | val_1_rmse: 0.8668  |  0:00:04s
epoch 13 | loss: 0.62432 | val_0_rmse: 0.80926 | val_1_rmse: 0.85053 |  0:00:04s
epoch 14 | loss: 0.61629 | val_0_rmse: 0.82688 | val_1_rmse: 0.87    |  0:00:04s
epoch 15 | loss: 0.62107 | val_0_rmse: 0.80345 | val_1_rmse: 0.83895 |  0:00:04s
epoch 16 | loss: 0.62465 | val_0_rmse: 0.8063  | val_1_rmse: 0.84363 |  0:00:05s
epoch 17 | loss: 0.61724 | val_0_rmse: 0.79712 | val_1_rmse: 0.8229  |  0:00:05s
epoch 18 | loss: 0.62145 | val_0_rmse: 0.79976 | val_1_rmse: 0.83375 |  0:00:05s
epoch 19 | loss: 0.61509 | val_0_rmse: 0.7967  | val_1_rmse: 0.82908 |  0:00:06s
epoch 20 | loss: 0.6191  | val_0_rmse: 0.80859 | val_1_rmse: 0.84912 |  0:00:06s
epoch 21 | loss: 0.61263 | val_0_rmse: 0.79993 | val_1_rmse: 0.82425 |  0:00:06s
epoch 22 | loss: 0.60509 | val_0_rmse: 0.79674 | val_1_rmse: 0.82213 |  0:00:07s
epoch 23 | loss: 0.6055  | val_0_rmse: 0.79446 | val_1_rmse: 0.82041 |  0:00:07s
epoch 24 | loss: 0.59537 | val_0_rmse: 0.79276 | val_1_rmse: 0.82045 |  0:00:07s
epoch 25 | loss: 0.60145 | val_0_rmse: 0.79355 | val_1_rmse: 0.81881 |  0:00:07s
epoch 26 | loss: 0.60255 | val_0_rmse: 0.79656 | val_1_rmse: 0.81388 |  0:00:08s
epoch 27 | loss: 0.60002 | val_0_rmse: 0.79339 | val_1_rmse: 0.8182  |  0:00:08s
epoch 28 | loss: 0.59377 | val_0_rmse: 0.79422 | val_1_rmse: 0.81482 |  0:00:08s
epoch 29 | loss: 0.58841 | val_0_rmse: 0.78628 | val_1_rmse: 0.81634 |  0:00:09s
epoch 30 | loss: 0.59003 | val_0_rmse: 0.78596 | val_1_rmse: 0.81823 |  0:00:09s
epoch 31 | loss: 0.58981 | val_0_rmse: 0.78558 | val_1_rmse: 0.81646 |  0:00:09s
epoch 32 | loss: 0.58289 | val_0_rmse: 0.78815 | val_1_rmse: 0.82355 |  0:00:10s
epoch 33 | loss: 0.58557 | val_0_rmse: 0.78517 | val_1_rmse: 0.81922 |  0:00:10s
epoch 34 | loss: 0.58548 | val_0_rmse: 0.78919 | val_1_rmse: 0.82712 |  0:00:10s
epoch 35 | loss: 0.57552 | val_0_rmse: 0.78749 | val_1_rmse: 0.82012 |  0:00:10s
epoch 36 | loss: 0.57621 | val_0_rmse: 0.78726 | val_1_rmse: 0.83334 |  0:00:11s
epoch 37 | loss: 0.58042 | val_0_rmse: 0.78311 | val_1_rmse: 0.81746 |  0:00:11s
epoch 38 | loss: 0.57832 | val_0_rmse: 0.79234 | val_1_rmse: 0.82636 |  0:00:12s
epoch 39 | loss: 0.58295 | val_0_rmse: 0.7924  | val_1_rmse: 0.82534 |  0:00:12s
epoch 40 | loss: 0.57146 | val_0_rmse: 0.78202 | val_1_rmse: 0.8274  |  0:00:12s
epoch 41 | loss: 0.5771  | val_0_rmse: 0.78005 | val_1_rmse: 0.8179  |  0:00:12s
epoch 42 | loss: 0.57604 | val_0_rmse: 0.78408 | val_1_rmse: 0.82173 |  0:00:13s
epoch 43 | loss: 0.57068 | val_0_rmse: 0.787   | val_1_rmse: 0.82082 |  0:00:13s
epoch 44 | loss: 0.5677  | val_0_rmse: 0.78195 | val_1_rmse: 0.81071 |  0:00:13s
epoch 45 | loss: 0.56719 | val_0_rmse: 0.78205 | val_1_rmse: 0.8101  |  0:00:14s
epoch 46 | loss: 0.57081 | val_0_rmse: 0.77763 | val_1_rmse: 0.80122 |  0:00:14s
epoch 47 | loss: 0.56228 | val_0_rmse: 0.76966 | val_1_rmse: 0.81061 |  0:00:14s
epoch 48 | loss: 0.57592 | val_0_rmse: 0.77184 | val_1_rmse: 0.81663 |  0:00:15s
epoch 49 | loss: 0.5712  | val_0_rmse: 0.77385 | val_1_rmse: 0.81887 |  0:00:15s
epoch 50 | loss: 0.56858 | val_0_rmse: 0.77704 | val_1_rmse: 0.82706 |  0:00:15s
epoch 51 | loss: 0.5731  | val_0_rmse: 0.76703 | val_1_rmse: 0.81974 |  0:00:15s
epoch 52 | loss: 0.56245 | val_0_rmse: 0.76572 | val_1_rmse: 0.82278 |  0:00:16s
epoch 53 | loss: 0.55909 | val_0_rmse: 0.76976 | val_1_rmse: 0.81818 |  0:00:16s
epoch 54 | loss: 0.56689 | val_0_rmse: 0.76261 | val_1_rmse: 0.81984 |  0:00:16s
epoch 55 | loss: 0.5524  | val_0_rmse: 0.75868 | val_1_rmse: 0.80484 |  0:00:17s
epoch 56 | loss: 0.56635 | val_0_rmse: 0.77369 | val_1_rmse: 0.80994 |  0:00:17s
epoch 57 | loss: 0.55599 | val_0_rmse: 0.77654 | val_1_rmse: 0.80508 |  0:00:17s
epoch 58 | loss: 0.56127 | val_0_rmse: 0.76646 | val_1_rmse: 0.80982 |  0:00:18s
epoch 59 | loss: 0.56603 | val_0_rmse: 0.75547 | val_1_rmse: 0.80361 |  0:00:18s
epoch 60 | loss: 0.55731 | val_0_rmse: 0.79205 | val_1_rmse: 0.84836 |  0:00:18s
epoch 61 | loss: 0.56915 | val_0_rmse: 0.76937 | val_1_rmse: 0.81487 |  0:00:18s
epoch 62 | loss: 0.55629 | val_0_rmse: 0.76997 | val_1_rmse: 0.83644 |  0:00:19s
epoch 63 | loss: 0.56063 | val_0_rmse: 0.76548 | val_1_rmse: 0.82586 |  0:00:19s
epoch 64 | loss: 0.55849 | val_0_rmse: 0.76009 | val_1_rmse: 0.82469 |  0:00:19s
epoch 65 | loss: 0.55733 | val_0_rmse: 0.75759 | val_1_rmse: 0.81853 |  0:00:20s
epoch 66 | loss: 0.5485  | val_0_rmse: 0.75617 | val_1_rmse: 0.82074 |  0:00:20s
epoch 67 | loss: 0.55762 | val_0_rmse: 0.75593 | val_1_rmse: 0.81808 |  0:00:20s
epoch 68 | loss: 0.55577 | val_0_rmse: 0.75774 | val_1_rmse: 0.82562 |  0:00:20s
epoch 69 | loss: 0.54598 | val_0_rmse: 0.75157 | val_1_rmse: 0.82563 |  0:00:21s
epoch 70 | loss: 0.54984 | val_0_rmse: 0.74728 | val_1_rmse: 0.81127 |  0:00:21s
epoch 71 | loss: 0.55035 | val_0_rmse: 0.74831 | val_1_rmse: 0.81546 |  0:00:21s
epoch 72 | loss: 0.55505 | val_0_rmse: 0.75483 | val_1_rmse: 0.80716 |  0:00:22s
epoch 73 | loss: 0.56515 | val_0_rmse: 0.80476 | val_1_rmse: 0.83746 |  0:00:22s
epoch 74 | loss: 0.59133 | val_0_rmse: 0.80156 | val_1_rmse: 0.82271 |  0:00:22s
epoch 75 | loss: 0.59482 | val_0_rmse: 0.78847 | val_1_rmse: 0.8174  |  0:00:23s
epoch 76 | loss: 0.5906  | val_0_rmse: 0.76398 | val_1_rmse: 0.79727 |  0:00:23s
epoch 77 | loss: 0.5809  | val_0_rmse: 0.76452 | val_1_rmse: 0.79578 |  0:00:23s
epoch 78 | loss: 0.56719 | val_0_rmse: 0.77577 | val_1_rmse: 0.80186 |  0:00:24s
epoch 79 | loss: 0.57579 | val_0_rmse: 0.76551 | val_1_rmse: 0.82219 |  0:00:24s
epoch 80 | loss: 0.57665 | val_0_rmse: 0.7778  | val_1_rmse: 0.83302 |  0:00:24s
epoch 81 | loss: 0.57239 | val_0_rmse: 0.75541 | val_1_rmse: 0.79288 |  0:00:25s
epoch 82 | loss: 0.56846 | val_0_rmse: 0.76124 | val_1_rmse: 0.79834 |  0:00:25s
epoch 83 | loss: 0.56898 | val_0_rmse: 0.75328 | val_1_rmse: 0.794   |  0:00:25s
epoch 84 | loss: 0.57917 | val_0_rmse: 0.75926 | val_1_rmse: 0.79226 |  0:00:25s
epoch 85 | loss: 0.58694 | val_0_rmse: 0.77041 | val_1_rmse: 0.82556 |  0:00:26s
epoch 86 | loss: 0.58038 | val_0_rmse: 0.76407 | val_1_rmse: 0.81356 |  0:00:26s
epoch 87 | loss: 0.59607 | val_0_rmse: 0.78174 | val_1_rmse: 0.81699 |  0:00:26s
epoch 88 | loss: 0.61851 | val_0_rmse: 0.7903  | val_1_rmse: 0.81791 |  0:00:27s
epoch 89 | loss: 0.6242  | val_0_rmse: 0.78433 | val_1_rmse: 0.81259 |  0:00:27s
epoch 90 | loss: 0.62759 | val_0_rmse: 0.78996 | val_1_rmse: 0.82122 |  0:00:27s
epoch 91 | loss: 0.61291 | val_0_rmse: 0.78011 | val_1_rmse: 0.81204 |  0:00:28s
epoch 92 | loss: 0.60772 | val_0_rmse: 0.77252 | val_1_rmse: 0.80273 |  0:00:28s
epoch 93 | loss: 0.6123  | val_0_rmse: 0.77591 | val_1_rmse: 0.8105  |  0:00:28s
epoch 94 | loss: 0.60223 | val_0_rmse: 0.76839 | val_1_rmse: 0.80244 |  0:00:28s
epoch 95 | loss: 0.59588 | val_0_rmse: 0.77075 | val_1_rmse: 0.80963 |  0:00:29s
epoch 96 | loss: 0.59801 | val_0_rmse: 0.76304 | val_1_rmse: 0.7981  |  0:00:29s
epoch 97 | loss: 0.58099 | val_0_rmse: 0.75489 | val_1_rmse: 0.79752 |  0:00:29s
epoch 98 | loss: 0.58064 | val_0_rmse: 0.75757 | val_1_rmse: 0.80008 |  0:00:30s
epoch 99 | loss: 0.57903 | val_0_rmse: 0.75587 | val_1_rmse: 0.79615 |  0:00:30s
epoch 100| loss: 0.57082 | val_0_rmse: 0.75157 | val_1_rmse: 0.79075 |  0:00:30s
epoch 101| loss: 0.57608 | val_0_rmse: 0.75058 | val_1_rmse: 0.79124 |  0:00:31s
epoch 102| loss: 0.57319 | val_0_rmse: 0.74567 | val_1_rmse: 0.79271 |  0:00:31s
epoch 103| loss: 0.5673  | val_0_rmse: 0.74389 | val_1_rmse: 0.79894 |  0:00:31s
epoch 104| loss: 0.55309 | val_0_rmse: 0.74735 | val_1_rmse: 0.80201 |  0:00:31s
epoch 105| loss: 0.55891 | val_0_rmse: 0.73717 | val_1_rmse: 0.79598 |  0:00:32s
epoch 106| loss: 0.56038 | val_0_rmse: 0.73643 | val_1_rmse: 0.79128 |  0:00:32s
epoch 107| loss: 0.55001 | val_0_rmse: 0.73826 | val_1_rmse: 0.78515 |  0:00:32s
epoch 108| loss: 0.54488 | val_0_rmse: 0.73415 | val_1_rmse: 0.78933 |  0:00:33s
epoch 109| loss: 0.55205 | val_0_rmse: 0.72759 | val_1_rmse: 0.79497 |  0:00:33s
epoch 110| loss: 0.54215 | val_0_rmse: 0.72582 | val_1_rmse: 0.80502 |  0:00:33s
epoch 111| loss: 0.53811 | val_0_rmse: 0.72239 | val_1_rmse: 0.79385 |  0:00:34s
epoch 112| loss: 0.52863 | val_0_rmse: 0.72009 | val_1_rmse: 0.78713 |  0:00:34s
epoch 113| loss: 0.5287  | val_0_rmse: 0.71525 | val_1_rmse: 0.79233 |  0:00:34s
epoch 114| loss: 0.52383 | val_0_rmse: 0.71708 | val_1_rmse: 0.79693 |  0:00:35s
epoch 115| loss: 0.53144 | val_0_rmse: 0.71964 | val_1_rmse: 0.79695 |  0:00:35s
epoch 116| loss: 0.52984 | val_0_rmse: 0.71787 | val_1_rmse: 0.81219 |  0:00:35s
epoch 117| loss: 0.54049 | val_0_rmse: 0.71747 | val_1_rmse: 0.80261 |  0:00:35s
epoch 118| loss: 0.53178 | val_0_rmse: 0.71586 | val_1_rmse: 0.79637 |  0:00:36s
epoch 119| loss: 0.52281 | val_0_rmse: 0.71393 | val_1_rmse: 0.7992  |  0:00:36s
epoch 120| loss: 0.51236 | val_0_rmse: 0.71018 | val_1_rmse: 0.79283 |  0:00:36s
epoch 121| loss: 0.52124 | val_0_rmse: 0.70523 | val_1_rmse: 0.79856 |  0:00:37s
epoch 122| loss: 0.52693 | val_0_rmse: 0.7081  | val_1_rmse: 0.80251 |  0:00:37s
epoch 123| loss: 0.52471 | val_0_rmse: 0.71019 | val_1_rmse: 0.80074 |  0:00:37s
epoch 124| loss: 0.51838 | val_0_rmse: 0.70505 | val_1_rmse: 0.79504 |  0:00:38s
epoch 125| loss: 0.51515 | val_0_rmse: 0.72365 | val_1_rmse: 0.8042  |  0:00:38s
epoch 126| loss: 0.51695 | val_0_rmse: 0.71234 | val_1_rmse: 0.8125  |  0:00:38s
epoch 127| loss: 0.54185 | val_0_rmse: 0.75673 | val_1_rmse: 0.83056 |  0:00:38s
epoch 128| loss: 0.58407 | val_0_rmse: 0.75143 | val_1_rmse: 0.81971 |  0:00:39s
epoch 129| loss: 0.57256 | val_0_rmse: 0.74028 | val_1_rmse: 0.81491 |  0:00:39s
epoch 130| loss: 0.55343 | val_0_rmse: 0.74347 | val_1_rmse: 0.81311 |  0:00:39s
epoch 131| loss: 0.5631  | val_0_rmse: 0.74053 | val_1_rmse: 0.8048  |  0:00:40s
epoch 132| loss: 0.56126 | val_0_rmse: 0.74164 | val_1_rmse: 0.80382 |  0:00:40s
epoch 133| loss: 0.5611  | val_0_rmse: 0.73549 | val_1_rmse: 0.7946  |  0:00:40s
epoch 134| loss: 0.55704 | val_0_rmse: 0.7297  | val_1_rmse: 0.79074 |  0:00:41s
epoch 135| loss: 0.5465  | val_0_rmse: 0.72856 | val_1_rmse: 0.79713 |  0:00:41s
epoch 136| loss: 0.54244 | val_0_rmse: 0.72332 | val_1_rmse: 0.79033 |  0:00:41s
epoch 137| loss: 0.53495 | val_0_rmse: 0.72309 | val_1_rmse: 0.79072 |  0:00:41s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.78515
Best weights from best epoch are automatically used!
ended training at: 04:22:49
Feature importance:
Mean squared error is of 0.04119413905419075
Mean absolute error:0.14211409429557334
MAPE:0.14356187805105602
R2 score:0.4020163457544188
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:22:50
epoch 0  | loss: 1.58802 | val_0_rmse: 0.99869 | val_1_rmse: 0.96329 |  0:00:00s
epoch 1  | loss: 1.04325 | val_0_rmse: 0.93643 | val_1_rmse: 0.90522 |  0:00:00s
epoch 2  | loss: 0.82511 | val_0_rmse: 0.86505 | val_1_rmse: 0.84306 |  0:00:00s
epoch 3  | loss: 0.7163  | val_0_rmse: 0.86439 | val_1_rmse: 0.84955 |  0:00:01s
epoch 4  | loss: 0.69552 | val_0_rmse: 0.82665 | val_1_rmse: 0.82337 |  0:00:01s
epoch 5  | loss: 0.66649 | val_0_rmse: 0.81196 | val_1_rmse: 0.81364 |  0:00:01s
epoch 6  | loss: 0.65227 | val_0_rmse: 0.8505  | val_1_rmse: 0.86376 |  0:00:02s
epoch 7  | loss: 0.65032 | val_0_rmse: 0.81142 | val_1_rmse: 0.81657 |  0:00:02s
epoch 8  | loss: 0.6424  | val_0_rmse: 0.79931 | val_1_rmse: 0.80297 |  0:00:02s
epoch 9  | loss: 0.63447 | val_0_rmse: 0.79771 | val_1_rmse: 0.7991  |  0:00:03s
epoch 10 | loss: 0.62423 | val_0_rmse: 0.79558 | val_1_rmse: 0.79553 |  0:00:03s
epoch 11 | loss: 0.62744 | val_0_rmse: 0.79629 | val_1_rmse: 0.79379 |  0:00:03s
epoch 12 | loss: 0.6391  | val_0_rmse: 0.79687 | val_1_rmse: 0.79567 |  0:00:04s
epoch 13 | loss: 0.62542 | val_0_rmse: 0.7967  | val_1_rmse: 0.79318 |  0:00:04s
epoch 14 | loss: 0.63077 | val_0_rmse: 0.80336 | val_1_rmse: 0.80808 |  0:00:04s
epoch 15 | loss: 0.62964 | val_0_rmse: 0.7956  | val_1_rmse: 0.79169 |  0:00:04s
epoch 16 | loss: 0.62648 | val_0_rmse: 0.79514 | val_1_rmse: 0.79443 |  0:00:05s
epoch 17 | loss: 0.61271 | val_0_rmse: 0.79272 | val_1_rmse: 0.79291 |  0:00:05s
epoch 18 | loss: 0.61981 | val_0_rmse: 0.79361 | val_1_rmse: 0.79257 |  0:00:05s
epoch 19 | loss: 0.61456 | val_0_rmse: 0.79282 | val_1_rmse: 0.7897  |  0:00:06s
epoch 20 | loss: 0.61911 | val_0_rmse: 0.7906  | val_1_rmse: 0.78666 |  0:00:06s
epoch 21 | loss: 0.61152 | val_0_rmse: 0.79014 | val_1_rmse: 0.78557 |  0:00:06s
epoch 22 | loss: 0.61038 | val_0_rmse: 0.78999 | val_1_rmse: 0.78594 |  0:00:07s
epoch 23 | loss: 0.61017 | val_0_rmse: 0.79435 | val_1_rmse: 0.79052 |  0:00:07s
epoch 24 | loss: 0.61183 | val_0_rmse: 0.7951  | val_1_rmse: 0.79327 |  0:00:07s
epoch 25 | loss: 0.62876 | val_0_rmse: 0.79795 | val_1_rmse: 0.78861 |  0:00:08s
epoch 26 | loss: 0.6278  | val_0_rmse: 0.79438 | val_1_rmse: 0.7873  |  0:00:08s
epoch 27 | loss: 0.62447 | val_0_rmse: 0.79982 | val_1_rmse: 0.78952 |  0:00:08s
epoch 28 | loss: 0.62483 | val_0_rmse: 0.80707 | val_1_rmse: 0.79497 |  0:00:08s
epoch 29 | loss: 0.64232 | val_0_rmse: 0.79983 | val_1_rmse: 0.7906  |  0:00:09s
epoch 30 | loss: 0.63537 | val_0_rmse: 0.79807 | val_1_rmse: 0.79546 |  0:00:09s
epoch 31 | loss: 0.63669 | val_0_rmse: 0.79825 | val_1_rmse: 0.79174 |  0:00:09s
epoch 32 | loss: 0.6388  | val_0_rmse: 0.80181 | val_1_rmse: 0.80123 |  0:00:10s
epoch 33 | loss: 0.62064 | val_0_rmse: 0.81145 | val_1_rmse: 0.81311 |  0:00:10s
epoch 34 | loss: 0.63766 | val_0_rmse: 0.8012  | val_1_rmse: 0.79773 |  0:00:10s
epoch 35 | loss: 0.6254  | val_0_rmse: 0.79394 | val_1_rmse: 0.79088 |  0:00:10s
epoch 36 | loss: 0.62247 | val_0_rmse: 0.80104 | val_1_rmse: 0.7976  |  0:00:11s
epoch 37 | loss: 0.6393  | val_0_rmse: 0.80328 | val_1_rmse: 0.80497 |  0:00:11s
epoch 38 | loss: 0.63347 | val_0_rmse: 0.79801 | val_1_rmse: 0.80173 |  0:00:11s
epoch 39 | loss: 0.63192 | val_0_rmse: 0.79838 | val_1_rmse: 0.79695 |  0:00:12s
epoch 40 | loss: 0.63047 | val_0_rmse: 0.79904 | val_1_rmse: 0.80422 |  0:00:12s
epoch 41 | loss: 0.62616 | val_0_rmse: 0.7952  | val_1_rmse: 0.79951 |  0:00:12s
epoch 42 | loss: 0.62718 | val_0_rmse: 0.79362 | val_1_rmse: 0.79684 |  0:00:13s
epoch 43 | loss: 0.62042 | val_0_rmse: 0.79134 | val_1_rmse: 0.79375 |  0:00:13s
epoch 44 | loss: 0.62371 | val_0_rmse: 0.79268 | val_1_rmse: 0.79637 |  0:00:13s
epoch 45 | loss: 0.6253  | val_0_rmse: 0.79644 | val_1_rmse: 0.79492 |  0:00:14s
epoch 46 | loss: 0.62634 | val_0_rmse: 0.79549 | val_1_rmse: 0.79849 |  0:00:14s
epoch 47 | loss: 0.61397 | val_0_rmse: 0.79464 | val_1_rmse: 0.79798 |  0:00:14s
epoch 48 | loss: 0.61906 | val_0_rmse: 0.79356 | val_1_rmse: 0.79941 |  0:00:15s
epoch 49 | loss: 0.61616 | val_0_rmse: 0.7944  | val_1_rmse: 0.7966  |  0:00:15s
epoch 50 | loss: 0.61443 | val_0_rmse: 0.78874 | val_1_rmse: 0.7913  |  0:00:15s
epoch 51 | loss: 0.623   | val_0_rmse: 0.7869  | val_1_rmse: 0.78796 |  0:00:15s

Early stopping occured at epoch 51 with best_epoch = 21 and best_val_1_rmse = 0.78557
Best weights from best epoch are automatically used!
ended training at: 04:23:06
Feature importance:
Mean squared error is of 0.04048550135090244
Mean absolute error:0.14919382113064766
MAPE:0.15563278902096195
R2 score:0.35380966574886874
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:23:06
epoch 0  | loss: 1.92886 | val_0_rmse: 1.01694 | val_1_rmse: 0.9629  |  0:00:00s
epoch 1  | loss: 1.16797 | val_0_rmse: 1.0095  | val_1_rmse: 0.96219 |  0:00:00s
epoch 2  | loss: 1.02594 | val_0_rmse: 1.00429 | val_1_rmse: 0.9504  |  0:00:00s
epoch 3  | loss: 0.94072 | val_0_rmse: 0.94685 | val_1_rmse: 0.88311 |  0:00:01s
epoch 4  | loss: 0.84873 | val_0_rmse: 0.90165 | val_1_rmse: 0.86427 |  0:00:01s
epoch 5  | loss: 0.73914 | val_0_rmse: 0.9633  | val_1_rmse: 0.92981 |  0:00:01s
epoch 6  | loss: 0.70715 | val_0_rmse: 0.94289 | val_1_rmse: 0.89846 |  0:00:02s
epoch 7  | loss: 0.69953 | val_0_rmse: 0.9076  | val_1_rmse: 0.86521 |  0:00:02s
epoch 8  | loss: 0.69737 | val_0_rmse: 0.84265 | val_1_rmse: 0.79627 |  0:00:02s
epoch 9  | loss: 0.69237 | val_0_rmse: 0.82179 | val_1_rmse: 0.77214 |  0:00:03s
epoch 10 | loss: 0.67492 | val_0_rmse: 0.81484 | val_1_rmse: 0.76237 |  0:00:03s
epoch 11 | loss: 0.66559 | val_0_rmse: 0.80984 | val_1_rmse: 0.76224 |  0:00:03s
epoch 12 | loss: 0.64601 | val_0_rmse: 0.82626 | val_1_rmse: 0.78445 |  0:00:03s
epoch 13 | loss: 0.6687  | val_0_rmse: 0.8085  | val_1_rmse: 0.7728  |  0:00:04s
epoch 14 | loss: 0.67744 | val_0_rmse: 0.81885 | val_1_rmse: 0.78361 |  0:00:04s
epoch 15 | loss: 0.66102 | val_0_rmse: 0.83243 | val_1_rmse: 0.79432 |  0:00:04s
epoch 16 | loss: 0.65104 | val_0_rmse: 0.80422 | val_1_rmse: 0.76309 |  0:00:05s
epoch 17 | loss: 0.64737 | val_0_rmse: 0.80463 | val_1_rmse: 0.76994 |  0:00:05s
epoch 18 | loss: 0.63832 | val_0_rmse: 0.79962 | val_1_rmse: 0.75642 |  0:00:05s
epoch 19 | loss: 0.64091 | val_0_rmse: 0.8023  | val_1_rmse: 0.76766 |  0:00:06s
epoch 20 | loss: 0.6344  | val_0_rmse: 0.79986 | val_1_rmse: 0.76344 |  0:00:06s
epoch 21 | loss: 0.63384 | val_0_rmse: 0.80606 | val_1_rmse: 0.77683 |  0:00:06s
epoch 22 | loss: 0.63224 | val_0_rmse: 0.7973  | val_1_rmse: 0.76263 |  0:00:06s
epoch 23 | loss: 0.63291 | val_0_rmse: 0.80466 | val_1_rmse: 0.77244 |  0:00:07s
epoch 24 | loss: 0.633   | val_0_rmse: 0.7953  | val_1_rmse: 0.76025 |  0:00:07s
epoch 25 | loss: 0.62502 | val_0_rmse: 0.79852 | val_1_rmse: 0.7689  |  0:00:07s
epoch 26 | loss: 0.61966 | val_0_rmse: 0.79359 | val_1_rmse: 0.76351 |  0:00:08s
epoch 27 | loss: 0.61251 | val_0_rmse: 0.79455 | val_1_rmse: 0.76708 |  0:00:08s
epoch 28 | loss: 0.61426 | val_0_rmse: 0.79236 | val_1_rmse: 0.75968 |  0:00:08s
epoch 29 | loss: 0.624   | val_0_rmse: 0.80559 | val_1_rmse: 0.78151 |  0:00:09s
epoch 30 | loss: 0.62069 | val_0_rmse: 0.79092 | val_1_rmse: 0.76249 |  0:00:09s
epoch 31 | loss: 0.60541 | val_0_rmse: 0.7879  | val_1_rmse: 0.75507 |  0:00:09s
epoch 32 | loss: 0.61413 | val_0_rmse: 0.80115 | val_1_rmse: 0.77527 |  0:00:10s
epoch 33 | loss: 0.60526 | val_0_rmse: 0.79448 | val_1_rmse: 0.76441 |  0:00:10s
epoch 34 | loss: 0.59797 | val_0_rmse: 0.79485 | val_1_rmse: 0.75902 |  0:00:10s
epoch 35 | loss: 0.59685 | val_0_rmse: 0.79425 | val_1_rmse: 0.75617 |  0:00:10s
epoch 36 | loss: 0.60628 | val_0_rmse: 0.79365 | val_1_rmse: 0.75784 |  0:00:11s
epoch 37 | loss: 0.60401 | val_0_rmse: 0.78904 | val_1_rmse: 0.74758 |  0:00:11s
epoch 38 | loss: 0.59428 | val_0_rmse: 0.7866  | val_1_rmse: 0.7527  |  0:00:11s
epoch 39 | loss: 0.58869 | val_0_rmse: 0.79016 | val_1_rmse: 0.7627  |  0:00:12s
epoch 40 | loss: 0.5763  | val_0_rmse: 0.78705 | val_1_rmse: 0.75053 |  0:00:12s
epoch 41 | loss: 0.58685 | val_0_rmse: 0.79444 | val_1_rmse: 0.76298 |  0:00:12s
epoch 42 | loss: 0.59177 | val_0_rmse: 0.78352 | val_1_rmse: 0.74487 |  0:00:13s
epoch 43 | loss: 0.58736 | val_0_rmse: 0.78588 | val_1_rmse: 0.74992 |  0:00:13s
epoch 44 | loss: 0.58102 | val_0_rmse: 0.77907 | val_1_rmse: 0.74519 |  0:00:13s
epoch 45 | loss: 0.57389 | val_0_rmse: 0.78217 | val_1_rmse: 0.74619 |  0:00:13s
epoch 46 | loss: 0.57078 | val_0_rmse: 0.79192 | val_1_rmse: 0.76932 |  0:00:14s
epoch 47 | loss: 0.57034 | val_0_rmse: 0.7723  | val_1_rmse: 0.73925 |  0:00:14s
epoch 48 | loss: 0.56876 | val_0_rmse: 0.78763 | val_1_rmse: 0.76949 |  0:00:14s
epoch 49 | loss: 0.57183 | val_0_rmse: 0.77099 | val_1_rmse: 0.74872 |  0:00:15s
epoch 50 | loss: 0.56985 | val_0_rmse: 0.77615 | val_1_rmse: 0.7521  |  0:00:15s
epoch 51 | loss: 0.56072 | val_0_rmse: 0.78131 | val_1_rmse: 0.76335 |  0:00:15s
epoch 52 | loss: 0.55906 | val_0_rmse: 0.77585 | val_1_rmse: 0.76219 |  0:00:16s
epoch 53 | loss: 0.55399 | val_0_rmse: 0.78589 | val_1_rmse: 0.7686  |  0:00:16s
epoch 54 | loss: 0.55316 | val_0_rmse: 0.80549 | val_1_rmse: 0.79402 |  0:00:16s
epoch 55 | loss: 0.55962 | val_0_rmse: 0.7816  | val_1_rmse: 0.76556 |  0:00:16s
epoch 56 | loss: 0.55804 | val_0_rmse: 0.7766  | val_1_rmse: 0.7558  |  0:00:17s
epoch 57 | loss: 0.55593 | val_0_rmse: 0.77385 | val_1_rmse: 0.75288 |  0:00:17s
epoch 58 | loss: 0.55566 | val_0_rmse: 0.77294 | val_1_rmse: 0.74502 |  0:00:17s
epoch 59 | loss: 0.5631  | val_0_rmse: 0.76985 | val_1_rmse: 0.75424 |  0:00:18s
epoch 60 | loss: 0.55027 | val_0_rmse: 0.76202 | val_1_rmse: 0.74805 |  0:00:18s
epoch 61 | loss: 0.54863 | val_0_rmse: 0.76479 | val_1_rmse: 0.7546  |  0:00:18s
epoch 62 | loss: 0.54707 | val_0_rmse: 0.7714  | val_1_rmse: 0.76648 |  0:00:18s
epoch 63 | loss: 0.53815 | val_0_rmse: 0.76014 | val_1_rmse: 0.74577 |  0:00:19s
epoch 64 | loss: 0.53866 | val_0_rmse: 0.75826 | val_1_rmse: 0.7509  |  0:00:19s
epoch 65 | loss: 0.53001 | val_0_rmse: 0.74854 | val_1_rmse: 0.7426  |  0:00:20s
epoch 66 | loss: 0.53299 | val_0_rmse: 0.75061 | val_1_rmse: 0.7442  |  0:00:20s
epoch 67 | loss: 0.53279 | val_0_rmse: 0.75221 | val_1_rmse: 0.7539  |  0:00:20s
epoch 68 | loss: 0.53547 | val_0_rmse: 0.75375 | val_1_rmse: 0.75257 |  0:00:20s
epoch 69 | loss: 0.5317  | val_0_rmse: 0.76193 | val_1_rmse: 0.76472 |  0:00:21s
epoch 70 | loss: 0.53628 | val_0_rmse: 0.75671 | val_1_rmse: 0.75716 |  0:00:21s
epoch 71 | loss: 0.53668 | val_0_rmse: 0.76838 | val_1_rmse: 0.77057 |  0:00:21s
epoch 72 | loss: 0.54578 | val_0_rmse: 0.78352 | val_1_rmse: 0.78408 |  0:00:22s
epoch 73 | loss: 0.55189 | val_0_rmse: 0.75671 | val_1_rmse: 0.74743 |  0:00:22s
epoch 74 | loss: 0.55029 | val_0_rmse: 0.77393 | val_1_rmse: 0.76058 |  0:00:22s
epoch 75 | loss: 0.56395 | val_0_rmse: 0.75823 | val_1_rmse: 0.74932 |  0:00:22s
epoch 76 | loss: 0.54039 | val_0_rmse: 0.75195 | val_1_rmse: 0.74468 |  0:00:23s
epoch 77 | loss: 0.53823 | val_0_rmse: 0.75703 | val_1_rmse: 0.76595 |  0:00:23s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.73925
Best weights from best epoch are automatically used!
ended training at: 04:23:30
Feature importance:
Mean squared error is of 0.041188433674689724
Mean absolute error:0.15200850181601955
MAPE:0.15832810057174398
R2 score:0.3474979467022077
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:23:31
epoch 0  | loss: 2.3796  | val_0_rmse: 0.98785 | val_1_rmse: 1.06541 |  0:00:00s
epoch 1  | loss: 1.25016 | val_0_rmse: 0.98702 | val_1_rmse: 1.06476 |  0:00:01s
epoch 2  | loss: 1.01669 | val_0_rmse: 0.9864  | val_1_rmse: 1.06379 |  0:00:01s
epoch 3  | loss: 1.00181 | val_0_rmse: 0.9864  | val_1_rmse: 1.0639  |  0:00:02s
epoch 4  | loss: 0.9858  | val_0_rmse: 0.98695 | val_1_rmse: 1.06431 |  0:00:03s
epoch 5  | loss: 0.97918 | val_0_rmse: 0.98652 | val_1_rmse: 1.06385 |  0:00:03s
epoch 6  | loss: 0.97709 | val_0_rmse: 0.98654 | val_1_rmse: 1.06394 |  0:00:04s
epoch 7  | loss: 0.97594 | val_0_rmse: 0.98651 | val_1_rmse: 1.06391 |  0:00:04s
epoch 8  | loss: 0.97554 | val_0_rmse: 0.98651 | val_1_rmse: 1.06385 |  0:00:05s
epoch 9  | loss: 0.97346 | val_0_rmse: 0.98652 | val_1_rmse: 1.06394 |  0:00:06s
epoch 10 | loss: 0.97284 | val_0_rmse: 0.98615 | val_1_rmse: 1.06377 |  0:00:06s
epoch 11 | loss: 0.97169 | val_0_rmse: 0.98599 | val_1_rmse: 1.06346 |  0:00:07s
epoch 12 | loss: 0.97231 | val_0_rmse: 0.98608 | val_1_rmse: 1.06354 |  0:00:08s
epoch 13 | loss: 0.97093 | val_0_rmse: 0.98596 | val_1_rmse: 1.06376 |  0:00:08s
epoch 14 | loss: 0.9673  | val_0_rmse: 0.98547 | val_1_rmse: 1.0632  |  0:00:09s
epoch 15 | loss: 0.96577 | val_0_rmse: 0.98284 | val_1_rmse: 1.0603  |  0:00:09s
epoch 16 | loss: 0.95933 | val_0_rmse: 0.98058 | val_1_rmse: 1.05917 |  0:00:10s
epoch 17 | loss: 0.95979 | val_0_rmse: 0.97784 | val_1_rmse: 1.05673 |  0:00:11s
epoch 18 | loss: 0.96072 | val_0_rmse: 0.97733 | val_1_rmse: 1.0564  |  0:00:11s
epoch 19 | loss: 0.95761 | val_0_rmse: 0.97951 | val_1_rmse: 1.05854 |  0:00:12s
epoch 20 | loss: 0.95102 | val_0_rmse: 0.97577 | val_1_rmse: 1.05619 |  0:00:12s
epoch 21 | loss: 0.94379 | val_0_rmse: 0.97541 | val_1_rmse: 1.05633 |  0:00:13s
epoch 22 | loss: 0.94173 | val_0_rmse: 0.9691  | val_1_rmse: 1.05021 |  0:00:14s
epoch 23 | loss: 0.93358 | val_0_rmse: 0.96409 | val_1_rmse: 1.0475  |  0:00:14s
epoch 24 | loss: 0.93041 | val_0_rmse: 0.96186 | val_1_rmse: 1.04553 |  0:00:15s
epoch 25 | loss: 0.92615 | val_0_rmse: 0.95746 | val_1_rmse: 1.03901 |  0:00:16s
epoch 26 | loss: 0.92114 | val_0_rmse: 0.94715 | val_1_rmse: 1.03021 |  0:00:16s
epoch 27 | loss: 0.9157  | val_0_rmse: 0.95023 | val_1_rmse: 1.0302  |  0:00:17s
epoch 28 | loss: 0.90473 | val_0_rmse: 0.93475 | val_1_rmse: 1.00521 |  0:00:17s
epoch 29 | loss: 0.89271 | val_0_rmse: 0.92454 | val_1_rmse: 0.99539 |  0:00:18s
epoch 30 | loss: 0.88094 | val_0_rmse: 0.91941 | val_1_rmse: 0.98332 |  0:00:19s
epoch 31 | loss: 0.86299 | val_0_rmse: 0.91871 | val_1_rmse: 0.98295 |  0:00:19s
epoch 32 | loss: 0.83397 | val_0_rmse: 0.91878 | val_1_rmse: 0.98063 |  0:00:20s
epoch 33 | loss: 0.81292 | val_0_rmse: 0.90122 | val_1_rmse: 0.96482 |  0:00:21s
epoch 34 | loss: 0.79619 | val_0_rmse: 0.90404 | val_1_rmse: 0.96727 |  0:00:21s
epoch 35 | loss: 0.78087 | val_0_rmse: 0.89317 | val_1_rmse: 0.96661 |  0:00:22s
epoch 36 | loss: 0.77705 | val_0_rmse: 0.89173 | val_1_rmse: 0.95902 |  0:00:22s
epoch 37 | loss: 0.75483 | val_0_rmse: 0.88699 | val_1_rmse: 0.96143 |  0:00:23s
epoch 38 | loss: 0.73508 | val_0_rmse: 0.88698 | val_1_rmse: 0.95451 |  0:00:24s
epoch 39 | loss: 0.71801 | val_0_rmse: 0.87284 | val_1_rmse: 0.94836 |  0:00:24s
epoch 40 | loss: 0.70413 | val_0_rmse: 0.87105 | val_1_rmse: 0.9368  |  0:00:25s
epoch 41 | loss: 0.68176 | val_0_rmse: 0.82796 | val_1_rmse: 0.89776 |  0:00:26s
epoch 42 | loss: 0.6467  | val_0_rmse: 0.82655 | val_1_rmse: 0.82005 |  0:00:26s
epoch 43 | loss: 0.6556  | val_0_rmse: 0.82453 | val_1_rmse: 0.81828 |  0:00:27s
epoch 44 | loss: 0.63418 | val_0_rmse: 0.81273 | val_1_rmse: 0.80321 |  0:00:27s
epoch 45 | loss: 0.60987 | val_0_rmse: 0.81987 | val_1_rmse: 0.80427 |  0:00:28s
epoch 46 | loss: 0.60902 | val_0_rmse: 0.91854 | val_1_rmse: 0.85164 |  0:00:29s
epoch 47 | loss: 0.61397 | val_0_rmse: 0.80713 | val_1_rmse: 0.81566 |  0:00:29s
epoch 48 | loss: 0.59138 | val_0_rmse: 0.81431 | val_1_rmse: 0.80489 |  0:00:30s
epoch 49 | loss: 0.58572 | val_0_rmse: 0.82358 | val_1_rmse: 0.80785 |  0:00:30s
epoch 50 | loss: 0.59263 | val_0_rmse: 1.99338 | val_1_rmse: 1.98086 |  0:00:31s
epoch 51 | loss: 0.5813  | val_0_rmse: 0.80438 | val_1_rmse: 0.82813 |  0:00:32s
epoch 52 | loss: 0.59117 | val_0_rmse: 0.80347 | val_1_rmse: 0.7945  |  0:00:32s
epoch 53 | loss: 0.59625 | val_0_rmse: 0.78821 | val_1_rmse: 0.80398 |  0:00:33s
epoch 54 | loss: 0.57223 | val_0_rmse: 0.77958 | val_1_rmse: 0.79531 |  0:00:34s
epoch 55 | loss: 0.55943 | val_0_rmse: 0.79712 | val_1_rmse: 0.80723 |  0:00:34s
epoch 56 | loss: 0.5681  | val_0_rmse: 0.77214 | val_1_rmse: 0.78487 |  0:00:35s
epoch 57 | loss: 0.52644 | val_0_rmse: 0.76019 | val_1_rmse: 0.78475 |  0:00:35s
epoch 58 | loss: 0.51545 | val_0_rmse: 0.7528  | val_1_rmse: 0.78031 |  0:00:36s
epoch 59 | loss: 0.51331 | val_0_rmse: 0.74749 | val_1_rmse: 0.78168 |  0:00:37s
epoch 60 | loss: 0.51851 | val_0_rmse: 0.74082 | val_1_rmse: 0.8017  |  0:00:37s
epoch 61 | loss: 0.51139 | val_0_rmse: 0.73263 | val_1_rmse: 0.79193 |  0:00:38s
epoch 62 | loss: 0.51147 | val_0_rmse: 0.74642 | val_1_rmse: 0.79861 |  0:00:39s
epoch 63 | loss: 0.50166 | val_0_rmse: 0.74304 | val_1_rmse: 0.79532 |  0:00:39s
epoch 64 | loss: 0.49739 | val_0_rmse: 0.74783 | val_1_rmse: 0.7976  |  0:00:40s
epoch 65 | loss: 0.48069 | val_0_rmse: 0.72725 | val_1_rmse: 0.78533 |  0:00:40s
epoch 66 | loss: 0.48422 | val_0_rmse: 0.72186 | val_1_rmse: 0.7813  |  0:00:41s
epoch 67 | loss: 0.48017 | val_0_rmse: 0.71294 | val_1_rmse: 0.78723 |  0:00:42s
epoch 68 | loss: 0.47958 | val_0_rmse: 0.71225 | val_1_rmse: 0.79087 |  0:00:42s
epoch 69 | loss: 0.47794 | val_0_rmse: 0.70884 | val_1_rmse: 0.80339 |  0:00:43s
epoch 70 | loss: 0.47134 | val_0_rmse: 0.72144 | val_1_rmse: 0.78276 |  0:00:43s
epoch 71 | loss: 0.45757 | val_0_rmse: 0.70747 | val_1_rmse: 0.77584 |  0:00:44s
epoch 72 | loss: 0.45552 | val_0_rmse: 0.69017 | val_1_rmse: 0.7839  |  0:00:45s
epoch 73 | loss: 0.44813 | val_0_rmse: 0.6901  | val_1_rmse: 0.78197 |  0:00:45s
epoch 74 | loss: 0.44339 | val_0_rmse: 0.68207 | val_1_rmse: 0.78444 |  0:00:46s
epoch 75 | loss: 0.4488  | val_0_rmse: 0.68435 | val_1_rmse: 0.8062  |  0:00:46s
epoch 76 | loss: 0.44115 | val_0_rmse: 0.67889 | val_1_rmse: 0.81533 |  0:00:47s
epoch 77 | loss: 0.44294 | val_0_rmse: 0.68377 | val_1_rmse: 0.81012 |  0:00:48s
epoch 78 | loss: 0.42668 | val_0_rmse: 0.66583 | val_1_rmse: 0.79312 |  0:00:48s
epoch 79 | loss: 0.42449 | val_0_rmse: 0.65541 | val_1_rmse: 0.80879 |  0:00:49s
epoch 80 | loss: 0.42537 | val_0_rmse: 0.66563 | val_1_rmse: 0.79832 |  0:00:50s
epoch 81 | loss: 0.41978 | val_0_rmse: 0.66828 | val_1_rmse: 0.79685 |  0:00:50s
epoch 82 | loss: 0.41842 | val_0_rmse: 0.65717 | val_1_rmse: 0.82772 |  0:00:51s
epoch 83 | loss: 0.41739 | val_0_rmse: 0.65518 | val_1_rmse: 0.8221  |  0:00:52s
epoch 84 | loss: 0.41355 | val_0_rmse: 0.64378 | val_1_rmse: 0.8025  |  0:00:52s
epoch 85 | loss: 0.41217 | val_0_rmse: 0.66361 | val_1_rmse: 0.8089  |  0:00:53s
epoch 86 | loss: 0.40567 | val_0_rmse: 0.64657 | val_1_rmse: 0.78576 |  0:00:53s
epoch 87 | loss: 0.4048  | val_0_rmse: 0.62875 | val_1_rmse: 0.8058  |  0:00:54s
epoch 88 | loss: 0.40877 | val_0_rmse: 0.62565 | val_1_rmse: 0.80465 |  0:00:55s
epoch 89 | loss: 0.4121  | val_0_rmse: 0.63087 | val_1_rmse: 0.83964 |  0:00:55s
epoch 90 | loss: 0.40861 | val_0_rmse: 0.63715 | val_1_rmse: 0.83062 |  0:00:56s
epoch 91 | loss: 0.4022  | val_0_rmse: 0.62393 | val_1_rmse: 0.84873 |  0:00:56s
epoch 92 | loss: 0.3977  | val_0_rmse: 0.65191 | val_1_rmse: 0.86637 |  0:00:57s
epoch 93 | loss: 0.39283 | val_0_rmse: 0.63105 | val_1_rmse: 0.8622  |  0:00:58s
epoch 94 | loss: 0.39523 | val_0_rmse: 0.62051 | val_1_rmse: 0.83791 |  0:00:58s
epoch 95 | loss: 0.38831 | val_0_rmse: 0.61525 | val_1_rmse: 0.8862  |  0:00:59s
epoch 96 | loss: 0.38753 | val_0_rmse: 0.60732 | val_1_rmse: 0.83487 |  0:00:59s
epoch 97 | loss: 0.38329 | val_0_rmse: 0.5924  | val_1_rmse: 0.86001 |  0:01:00s
epoch 98 | loss: 0.38272 | val_0_rmse: 0.58412 | val_1_rmse: 0.85794 |  0:01:01s
epoch 99 | loss: 0.37848 | val_0_rmse: 0.58519 | val_1_rmse: 0.84032 |  0:01:01s
epoch 100| loss: 0.37391 | val_0_rmse: 0.57836 | val_1_rmse: 0.84538 |  0:01:02s
epoch 101| loss: 0.37475 | val_0_rmse: 0.56878 | val_1_rmse: 0.86339 |  0:01:03s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.77584
Best weights from best epoch are automatically used!
ended training at: 04:24:34
Feature importance:
Mean squared error is of 0.05859222170307715
Mean absolute error:0.15847369475046896
MAPE:0.16159785484429512
R2 score:0.312114859861103
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:24:35
epoch 0  | loss: 2.21931 | val_0_rmse: 0.98701 | val_1_rmse: 1.06921 |  0:00:00s
epoch 1  | loss: 1.18224 | val_0_rmse: 0.98293 | val_1_rmse: 1.06687 |  0:00:01s
epoch 2  | loss: 1.00757 | val_0_rmse: 0.98334 | val_1_rmse: 1.06733 |  0:00:01s
epoch 3  | loss: 0.99168 | val_0_rmse: 0.98327 | val_1_rmse: 1.06742 |  0:00:02s
epoch 4  | loss: 0.976   | val_0_rmse: 0.9832  | val_1_rmse: 1.0673  |  0:00:03s
epoch 5  | loss: 0.97016 | val_0_rmse: 0.98319 | val_1_rmse: 1.06722 |  0:00:03s
epoch 6  | loss: 0.96963 | val_0_rmse: 0.98319 | val_1_rmse: 1.0672  |  0:00:04s
epoch 7  | loss: 0.96812 | val_0_rmse: 0.98322 | val_1_rmse: 1.06718 |  0:00:04s
epoch 8  | loss: 0.96818 | val_0_rmse: 0.9832  | val_1_rmse: 1.0672  |  0:00:05s
epoch 9  | loss: 0.96745 | val_0_rmse: 0.98321 | val_1_rmse: 1.06719 |  0:00:06s
epoch 10 | loss: 0.96756 | val_0_rmse: 0.98313 | val_1_rmse: 1.06707 |  0:00:06s
epoch 11 | loss: 0.96395 | val_0_rmse: 0.97779 | val_1_rmse: 1.06252 |  0:00:07s
epoch 12 | loss: 0.95977 | val_0_rmse: 0.97372 | val_1_rmse: 1.05928 |  0:00:07s
epoch 13 | loss: 0.95668 | val_0_rmse: 0.97222 | val_1_rmse: 1.0606  |  0:00:08s
epoch 14 | loss: 0.95248 | val_0_rmse: 0.96942 | val_1_rmse: 1.05753 |  0:00:09s
epoch 15 | loss: 0.94993 | val_0_rmse: 0.97417 | val_1_rmse: 1.05886 |  0:00:09s
epoch 16 | loss: 0.94409 | val_0_rmse: 0.97885 | val_1_rmse: 1.06114 |  0:00:10s
epoch 17 | loss: 0.94271 | val_0_rmse: 0.97344 | val_1_rmse: 1.05385 |  0:00:10s
epoch 18 | loss: 0.94805 | val_0_rmse: 0.97295 | val_1_rmse: 1.05342 |  0:00:11s
epoch 19 | loss: 0.94829 | val_0_rmse: 0.96446 | val_1_rmse: 1.05642 |  0:00:12s
epoch 20 | loss: 0.9452  | val_0_rmse: 0.96261 | val_1_rmse: 1.05799 |  0:00:12s
epoch 21 | loss: 0.93639 | val_0_rmse: 0.96557 | val_1_rmse: 1.05657 |  0:00:13s
epoch 22 | loss: 0.93953 | val_0_rmse: 0.96044 | val_1_rmse: 1.04998 |  0:00:14s
epoch 23 | loss: 0.93848 | val_0_rmse: 0.96164 | val_1_rmse: 1.05149 |  0:00:14s
epoch 24 | loss: 0.93684 | val_0_rmse: 0.96506 | val_1_rmse: 1.05308 |  0:00:15s
epoch 25 | loss: 0.93569 | val_0_rmse: 0.966   | val_1_rmse: 1.05241 |  0:00:15s
epoch 26 | loss: 0.94178 | val_0_rmse: 0.96806 | val_1_rmse: 1.05299 |  0:00:16s
epoch 27 | loss: 0.94184 | val_0_rmse: 0.96907 | val_1_rmse: 1.05543 |  0:00:17s
epoch 28 | loss: 0.94203 | val_0_rmse: 0.97041 | val_1_rmse: 1.05915 |  0:00:17s
epoch 29 | loss: 0.93941 | val_0_rmse: 0.96461 | val_1_rmse: 1.05206 |  0:00:18s
epoch 30 | loss: 0.93612 | val_0_rmse: 0.96495 | val_1_rmse: 1.05361 |  0:00:18s
epoch 31 | loss: 0.93531 | val_0_rmse: 0.96447 | val_1_rmse: 1.05105 |  0:00:19s
epoch 32 | loss: 0.93583 | val_0_rmse: 0.96251 | val_1_rmse: 1.05039 |  0:00:20s
epoch 33 | loss: 0.93614 | val_0_rmse: 0.96433 | val_1_rmse: 1.05253 |  0:00:20s
epoch 34 | loss: 0.93556 | val_0_rmse: 0.96475 | val_1_rmse: 1.05399 |  0:00:21s
epoch 35 | loss: 0.93578 | val_0_rmse: 0.96519 | val_1_rmse: 1.05319 |  0:00:21s
epoch 36 | loss: 0.93832 | val_0_rmse: 0.96527 | val_1_rmse: 1.05296 |  0:00:22s
epoch 37 | loss: 0.93393 | val_0_rmse: 0.9664  | val_1_rmse: 1.05506 |  0:00:23s
epoch 38 | loss: 0.93326 | val_0_rmse: 0.96494 | val_1_rmse: 1.05231 |  0:00:23s
epoch 39 | loss: 0.93086 | val_0_rmse: 0.96777 | val_1_rmse: 1.05808 |  0:00:24s
epoch 40 | loss: 0.93197 | val_0_rmse: 0.96755 | val_1_rmse: 1.05392 |  0:00:25s
epoch 41 | loss: 0.93747 | val_0_rmse: 0.96651 | val_1_rmse: 1.05645 |  0:00:25s
epoch 42 | loss: 0.93266 | val_0_rmse: 0.96483 | val_1_rmse: 1.05179 |  0:00:26s
epoch 43 | loss: 0.92986 | val_0_rmse: 0.96323 | val_1_rmse: 1.0523  |  0:00:26s
epoch 44 | loss: 0.93085 | val_0_rmse: 0.96546 | val_1_rmse: 1.05193 |  0:00:27s
epoch 45 | loss: 0.94118 | val_0_rmse: 0.97804 | val_1_rmse: 1.05996 |  0:00:28s
epoch 46 | loss: 0.93645 | val_0_rmse: 0.96906 | val_1_rmse: 1.05526 |  0:00:28s
epoch 47 | loss: 0.93527 | val_0_rmse: 0.96481 | val_1_rmse: 1.05538 |  0:00:29s
epoch 48 | loss: 0.93548 | val_0_rmse: 0.9633  | val_1_rmse: 1.05197 |  0:00:29s
epoch 49 | loss: 0.9312  | val_0_rmse: 0.96243 | val_1_rmse: 1.04961 |  0:00:30s
epoch 50 | loss: 0.93038 | val_0_rmse: 0.96331 | val_1_rmse: 1.05176 |  0:00:31s
epoch 51 | loss: 0.93061 | val_0_rmse: 0.96341 | val_1_rmse: 1.05042 |  0:00:31s
epoch 52 | loss: 0.9301  | val_0_rmse: 0.96048 | val_1_rmse: 1.04825 |  0:00:32s
epoch 53 | loss: 0.9266  | val_0_rmse: 0.95753 | val_1_rmse: 1.0458  |  0:00:33s
epoch 54 | loss: 0.92452 | val_0_rmse: 0.95667 | val_1_rmse: 1.04538 |  0:00:33s
epoch 55 | loss: 0.92418 | val_0_rmse: 0.95669 | val_1_rmse: 1.04431 |  0:00:34s
epoch 56 | loss: 0.9246  | val_0_rmse: 0.95901 | val_1_rmse: 1.04502 |  0:00:34s
epoch 57 | loss: 0.92086 | val_0_rmse: 0.95726 | val_1_rmse: 1.04476 |  0:00:35s
epoch 58 | loss: 0.91704 | val_0_rmse: 0.95386 | val_1_rmse: 1.04165 |  0:00:36s
epoch 59 | loss: 0.91181 | val_0_rmse: 0.95495 | val_1_rmse: 1.04315 |  0:00:36s
epoch 60 | loss: 0.9071  | val_0_rmse: 0.95164 | val_1_rmse: 1.03984 |  0:00:37s
epoch 61 | loss: 0.89435 | val_0_rmse: 0.94236 | val_1_rmse: 1.03415 |  0:00:37s
epoch 62 | loss: 0.88446 | val_0_rmse: 0.94357 | val_1_rmse: 1.03408 |  0:00:38s
epoch 63 | loss: 0.87248 | val_0_rmse: 0.93747 | val_1_rmse: 1.02854 |  0:00:39s
epoch 64 | loss: 0.86027 | val_0_rmse: 0.93293 | val_1_rmse: 1.02461 |  0:00:39s
epoch 65 | loss: 0.85157 | val_0_rmse: 0.92284 | val_1_rmse: 1.01659 |  0:00:40s
epoch 66 | loss: 0.83786 | val_0_rmse: 0.92238 | val_1_rmse: 1.01717 |  0:00:41s
epoch 67 | loss: 0.81626 | val_0_rmse: 0.91415 | val_1_rmse: 1.00994 |  0:00:41s
epoch 68 | loss: 0.79867 | val_0_rmse: 0.90454 | val_1_rmse: 1.00085 |  0:00:42s
epoch 69 | loss: 0.791   | val_0_rmse: 0.89781 | val_1_rmse: 0.99267 |  0:00:42s
epoch 70 | loss: 0.76884 | val_0_rmse: 0.88693 | val_1_rmse: 0.98631 |  0:00:43s
epoch 71 | loss: 0.77025 | val_0_rmse: 0.89402 | val_1_rmse: 0.99196 |  0:00:44s
epoch 72 | loss: 0.74694 | val_0_rmse: 0.88298 | val_1_rmse: 0.9822  |  0:00:44s
epoch 73 | loss: 0.73986 | val_0_rmse: 0.88139 | val_1_rmse: 0.97848 |  0:00:45s
epoch 74 | loss: 0.73673 | val_0_rmse: 0.89096 | val_1_rmse: 0.98734 |  0:00:45s
epoch 75 | loss: 0.73924 | val_0_rmse: 0.8559  | val_1_rmse: 0.95746 |  0:00:46s
epoch 76 | loss: 0.72259 | val_0_rmse: 0.88595 | val_1_rmse: 0.97327 |  0:00:47s
epoch 77 | loss: 0.72498 | val_0_rmse: 0.85619 | val_1_rmse: 0.93778 |  0:00:47s
epoch 78 | loss: 0.70119 | val_0_rmse: 0.83852 | val_1_rmse: 0.91545 |  0:00:48s
epoch 79 | loss: 0.68803 | val_0_rmse: 0.86466 | val_1_rmse: 0.9453  |  0:00:49s
epoch 80 | loss: 0.66216 | val_0_rmse: 0.81817 | val_1_rmse: 0.8933  |  0:00:49s
epoch 81 | loss: 0.64563 | val_0_rmse: 0.78897 | val_1_rmse: 0.88048 |  0:00:50s
epoch 82 | loss: 0.64553 | val_0_rmse: 0.79038 | val_1_rmse: 0.89038 |  0:00:50s
epoch 83 | loss: 0.6307  | val_0_rmse: 0.78648 | val_1_rmse: 0.8844  |  0:00:51s
epoch 84 | loss: 0.618   | val_0_rmse: 0.80876 | val_1_rmse: 0.89531 |  0:00:52s
epoch 85 | loss: 0.62844 | val_0_rmse: 0.79015 | val_1_rmse: 0.87798 |  0:00:52s
epoch 86 | loss: 0.62471 | val_0_rmse: 0.80309 | val_1_rmse: 0.88536 |  0:00:53s
epoch 87 | loss: 0.61264 | val_0_rmse: 0.78593 | val_1_rmse: 0.8778  |  0:00:53s
epoch 88 | loss: 0.61291 | val_0_rmse: 0.81457 | val_1_rmse: 0.89468 |  0:00:54s
epoch 89 | loss: 0.60085 | val_0_rmse: 0.76602 | val_1_rmse: 0.85281 |  0:00:55s
epoch 90 | loss: 0.60186 | val_0_rmse: 0.77008 | val_1_rmse: 0.85358 |  0:00:55s
epoch 91 | loss: 0.59235 | val_0_rmse: 0.76084 | val_1_rmse: 0.84155 |  0:00:56s
epoch 92 | loss: 0.59035 | val_0_rmse: 0.75464 | val_1_rmse: 0.84497 |  0:00:57s
epoch 93 | loss: 0.57743 | val_0_rmse: 0.7525  | val_1_rmse: 0.84446 |  0:00:57s
epoch 94 | loss: 0.56644 | val_0_rmse: 0.74085 | val_1_rmse: 0.84246 |  0:00:58s
epoch 95 | loss: 0.5621  | val_0_rmse: 0.77127 | val_1_rmse: 0.89216 |  0:00:58s
epoch 96 | loss: 0.56799 | val_0_rmse: 0.74088 | val_1_rmse: 0.86539 |  0:00:59s
epoch 97 | loss: 0.55836 | val_0_rmse: 0.73673 | val_1_rmse: 0.85934 |  0:01:00s
epoch 98 | loss: 0.55568 | val_0_rmse: 0.74309 | val_1_rmse: 0.85754 |  0:01:00s
epoch 99 | loss: 0.55895 | val_0_rmse: 0.7382  | val_1_rmse: 0.85529 |  0:01:01s
epoch 100| loss: 0.56072 | val_0_rmse: 0.72893 | val_1_rmse: 0.84979 |  0:01:01s
epoch 101| loss: 0.54379 | val_0_rmse: 0.73012 | val_1_rmse: 0.85035 |  0:01:02s
epoch 102| loss: 0.55065 | val_0_rmse: 0.73644 | val_1_rmse: 0.86268 |  0:01:03s
epoch 103| loss: 0.55704 | val_0_rmse: 0.75754 | val_1_rmse: 0.87445 |  0:01:03s
epoch 104| loss: 0.55717 | val_0_rmse: 0.73862 | val_1_rmse: 0.88366 |  0:01:04s
epoch 105| loss: 0.56279 | val_0_rmse: 0.76172 | val_1_rmse: 0.8856  |  0:01:05s
epoch 106| loss: 0.55818 | val_0_rmse: 0.72715 | val_1_rmse: 0.84297 |  0:01:05s
epoch 107| loss: 0.55603 | val_0_rmse: 0.7306  | val_1_rmse: 0.84293 |  0:01:06s
epoch 108| loss: 0.54871 | val_0_rmse: 0.72512 | val_1_rmse: 0.84271 |  0:01:06s
epoch 109| loss: 0.55512 | val_0_rmse: 0.75751 | val_1_rmse: 0.87103 |  0:01:07s
epoch 110| loss: 0.55048 | val_0_rmse: 0.82162 | val_1_rmse: 0.90206 |  0:01:08s
epoch 111| loss: 0.5467  | val_0_rmse: 0.73022 | val_1_rmse: 0.83805 |  0:01:08s
epoch 112| loss: 0.54319 | val_0_rmse: 0.71781 | val_1_rmse: 0.84027 |  0:01:09s
epoch 113| loss: 0.54252 | val_0_rmse: 0.72958 | val_1_rmse: 0.83125 |  0:01:09s
epoch 114| loss: 0.54066 | val_0_rmse: 0.75926 | val_1_rmse: 0.85945 |  0:01:10s
epoch 115| loss: 0.53848 | val_0_rmse: 0.72054 | val_1_rmse: 0.83112 |  0:01:11s
epoch 116| loss: 0.53346 | val_0_rmse: 0.71556 | val_1_rmse: 0.86224 |  0:01:11s
epoch 117| loss: 0.51486 | val_0_rmse: 0.71505 | val_1_rmse: 0.87667 |  0:01:12s
epoch 118| loss: 0.52582 | val_0_rmse: 0.71288 | val_1_rmse: 0.87466 |  0:01:12s
epoch 119| loss: 0.52892 | val_0_rmse: 0.70554 | val_1_rmse: 0.87903 |  0:01:13s
epoch 120| loss: 0.52166 | val_0_rmse: 0.70397 | val_1_rmse: 0.91128 |  0:01:14s
epoch 121| loss: 0.52173 | val_0_rmse: 0.69387 | val_1_rmse: 0.89027 |  0:01:14s
epoch 122| loss: 0.51614 | val_0_rmse: 0.70385 | val_1_rmse: 0.89886 |  0:01:15s
epoch 123| loss: 0.50976 | val_0_rmse: 0.69289 | val_1_rmse: 0.92795 |  0:01:16s
epoch 124| loss: 0.50443 | val_0_rmse: 0.69289 | val_1_rmse: 1.04701 |  0:01:16s
epoch 125| loss: 0.48978 | val_0_rmse: 0.69412 | val_1_rmse: 1.08169 |  0:01:17s
epoch 126| loss: 0.48407 | val_0_rmse: 0.67615 | val_1_rmse: 1.26826 |  0:01:17s
epoch 127| loss: 0.48507 | val_0_rmse: 0.68    | val_1_rmse: 1.37654 |  0:01:18s
epoch 128| loss: 0.48566 | val_0_rmse: 0.67129 | val_1_rmse: 1.45161 |  0:01:19s
epoch 129| loss: 0.48459 | val_0_rmse: 0.66741 | val_1_rmse: 1.59946 |  0:01:19s
epoch 130| loss: 0.47316 | val_0_rmse: 0.66961 | val_1_rmse: 1.63981 |  0:01:20s
epoch 131| loss: 0.47441 | val_0_rmse: 0.66473 | val_1_rmse: 1.66184 |  0:01:20s
epoch 132| loss: 0.47425 | val_0_rmse: 0.66072 | val_1_rmse: 1.64537 |  0:01:21s
epoch 133| loss: 0.467   | val_0_rmse: 0.65959 | val_1_rmse: 1.69886 |  0:01:22s
epoch 134| loss: 0.46279 | val_0_rmse: 0.65487 | val_1_rmse: 1.71847 |  0:01:22s
epoch 135| loss: 0.45527 | val_0_rmse: 0.65656 | val_1_rmse: 1.81503 |  0:01:23s
epoch 136| loss: 0.45449 | val_0_rmse: 0.6507  | val_1_rmse: 1.89222 |  0:01:24s
epoch 137| loss: 0.45766 | val_0_rmse: 0.64893 | val_1_rmse: 1.94043 |  0:01:24s
epoch 138| loss: 0.44852 | val_0_rmse: 0.64668 | val_1_rmse: 2.0591  |  0:01:25s
epoch 139| loss: 0.44356 | val_0_rmse: 0.71286 | val_1_rmse: 2.18367 |  0:01:25s
epoch 140| loss: 0.44291 | val_0_rmse: 0.64599 | val_1_rmse: 1.90364 |  0:01:26s
epoch 141| loss: 0.44118 | val_0_rmse: 0.6473  | val_1_rmse: 1.98213 |  0:01:27s
epoch 142| loss: 0.43966 | val_0_rmse: 0.64182 | val_1_rmse: 1.94639 |  0:01:27s
epoch 143| loss: 0.43345 | val_0_rmse: 0.65874 | val_1_rmse: 2.18688 |  0:01:28s
epoch 144| loss: 0.43129 | val_0_rmse: 0.6336  | val_1_rmse: 2.02049 |  0:01:28s
epoch 145| loss: 0.43545 | val_0_rmse: 0.62978 | val_1_rmse: 2.01589 |  0:01:29s

Early stopping occured at epoch 145 with best_epoch = 115 and best_val_1_rmse = 0.83112
Best weights from best epoch are automatically used!
ended training at: 04:26:05
Feature importance:
Mean squared error is of 0.05139251026216837
Mean absolute error:0.15886060127338275
MAPE:0.1649642194957864
R2 score:0.46413348333854043
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:26:05
epoch 0  | loss: 2.6342  | val_0_rmse: 1.02116 | val_1_rmse: 0.9434  |  0:00:00s
epoch 1  | loss: 1.32379 | val_0_rmse: 1.0206  | val_1_rmse: 0.94199 |  0:00:01s
epoch 2  | loss: 1.08883 | val_0_rmse: 1.01958 | val_1_rmse: 0.94044 |  0:00:01s
epoch 3  | loss: 1.05074 | val_0_rmse: 1.01954 | val_1_rmse: 0.93986 |  0:00:02s
epoch 4  | loss: 1.05322 | val_0_rmse: 1.03015 | val_1_rmse: 0.9544  |  0:00:03s
epoch 5  | loss: 1.04344 | val_0_rmse: 1.01972 | val_1_rmse: 0.94021 |  0:00:03s
epoch 6  | loss: 1.04442 | val_0_rmse: 1.01948 | val_1_rmse: 0.94036 |  0:00:04s
epoch 7  | loss: 1.0412  | val_0_rmse: 1.01942 | val_1_rmse: 0.94046 |  0:00:05s
epoch 8  | loss: 1.04324 | val_0_rmse: 1.01946 | val_1_rmse: 0.94023 |  0:00:05s
epoch 9  | loss: 1.04015 | val_0_rmse: 1.0197  | val_1_rmse: 0.94072 |  0:00:06s
epoch 10 | loss: 1.04224 | val_0_rmse: 1.01948 | val_1_rmse: 0.94038 |  0:00:06s
epoch 11 | loss: 1.04052 | val_0_rmse: 1.01943 | val_1_rmse: 0.94043 |  0:00:07s
epoch 12 | loss: 1.04002 | val_0_rmse: 1.01942 | val_1_rmse: 0.94036 |  0:00:08s
epoch 13 | loss: 1.03939 | val_0_rmse: 1.01943 | val_1_rmse: 0.94031 |  0:00:08s
epoch 14 | loss: 1.03986 | val_0_rmse: 1.01946 | val_1_rmse: 0.94045 |  0:00:09s
epoch 15 | loss: 1.04092 | val_0_rmse: 1.01924 | val_1_rmse: 0.94022 |  0:00:09s
epoch 16 | loss: 1.04202 | val_0_rmse: 1.01905 | val_1_rmse: 0.94006 |  0:00:10s
epoch 17 | loss: 1.03858 | val_0_rmse: 1.01867 | val_1_rmse: 0.9401  |  0:00:11s
epoch 18 | loss: 1.0386  | val_0_rmse: 1.01855 | val_1_rmse: 0.93972 |  0:00:11s
epoch 19 | loss: 1.03768 | val_0_rmse: 1.01782 | val_1_rmse: 0.93894 |  0:00:12s
epoch 20 | loss: 1.03621 | val_0_rmse: 1.01561 | val_1_rmse: 0.93884 |  0:00:12s
epoch 21 | loss: 1.03542 | val_0_rmse: 1.01702 | val_1_rmse: 0.93862 |  0:00:13s
epoch 22 | loss: 1.0338  | val_0_rmse: 1.01628 | val_1_rmse: 0.93795 |  0:00:14s
epoch 23 | loss: 1.03429 | val_0_rmse: 1.0142  | val_1_rmse: 0.93613 |  0:00:14s
epoch 24 | loss: 1.03199 | val_0_rmse: 1.00874 | val_1_rmse: 0.93235 |  0:00:15s
epoch 25 | loss: 1.03007 | val_0_rmse: 0.99875 | val_1_rmse: 0.9248  |  0:00:16s
epoch 26 | loss: 1.01321 | val_0_rmse: 0.98497 | val_1_rmse: 0.91293 |  0:00:16s
epoch 27 | loss: 1.00175 | val_0_rmse: 0.97429 | val_1_rmse: 0.90288 |  0:00:17s
epoch 28 | loss: 0.98032 | val_0_rmse: 0.96891 | val_1_rmse: 0.89477 |  0:00:18s
epoch 29 | loss: 0.96279 | val_0_rmse: 0.94778 | val_1_rmse: 0.87972 |  0:00:18s
epoch 30 | loss: 0.92221 | val_0_rmse: 0.93828 | val_1_rmse: 0.8696  |  0:00:19s
epoch 31 | loss: 0.86255 | val_0_rmse: 0.90473 | val_1_rmse: 0.85623 |  0:00:19s
epoch 32 | loss: 0.81214 | val_0_rmse: 0.8695  | val_1_rmse: 0.83632 |  0:00:20s
epoch 33 | loss: 0.77485 | val_0_rmse: 0.85863 | val_1_rmse: 0.81689 |  0:00:21s
epoch 34 | loss: 0.75637 | val_0_rmse: 0.92138 | val_1_rmse: 0.86661 |  0:00:21s
epoch 35 | loss: 0.76006 | val_0_rmse: 0.95782 | val_1_rmse: 0.94377 |  0:00:22s
epoch 36 | loss: 0.7602  | val_0_rmse: 0.83859 | val_1_rmse: 0.78749 |  0:00:22s
epoch 37 | loss: 0.72655 | val_0_rmse: 0.83587 | val_1_rmse: 0.80614 |  0:00:23s
epoch 38 | loss: 0.68625 | val_0_rmse: 0.81751 | val_1_rmse: 0.79453 |  0:00:24s
epoch 39 | loss: 0.65715 | val_0_rmse: 0.81442 | val_1_rmse: 0.77792 |  0:00:24s
epoch 40 | loss: 0.65612 | val_0_rmse: 0.82673 | val_1_rmse: 0.80475 |  0:00:25s
epoch 41 | loss: 0.62621 | val_0_rmse: 0.8224  | val_1_rmse: 0.79865 |  0:00:26s
epoch 42 | loss: 0.60074 | val_0_rmse: 0.8061  | val_1_rmse: 0.78139 |  0:00:26s
epoch 43 | loss: 0.60414 | val_0_rmse: 0.79951 | val_1_rmse: 0.75473 |  0:00:27s
epoch 44 | loss: 0.59514 | val_0_rmse: 0.79765 | val_1_rmse: 0.77008 |  0:00:27s
epoch 45 | loss: 0.57416 | val_0_rmse: 0.78626 | val_1_rmse: 0.75036 |  0:00:28s
epoch 46 | loss: 0.57099 | val_0_rmse: 0.78111 | val_1_rmse: 0.76224 |  0:00:29s
epoch 47 | loss: 0.55858 | val_0_rmse: 0.77445 | val_1_rmse: 0.75495 |  0:00:29s
epoch 48 | loss: 0.56402 | val_0_rmse: 0.7879  | val_1_rmse: 0.78161 |  0:00:30s
epoch 49 | loss: 0.5585  | val_0_rmse: 0.80496 | val_1_rmse: 0.79812 |  0:00:30s
epoch 50 | loss: 0.55694 | val_0_rmse: 0.7757  | val_1_rmse: 0.77542 |  0:00:31s
epoch 51 | loss: 0.53844 | val_0_rmse: 0.76576 | val_1_rmse: 0.77053 |  0:00:32s
epoch 52 | loss: 0.53721 | val_0_rmse: 0.80804 | val_1_rmse: 0.81623 |  0:00:32s
epoch 53 | loss: 0.51907 | val_0_rmse: 0.75937 | val_1_rmse: 0.76066 |  0:00:33s
epoch 54 | loss: 0.52004 | val_0_rmse: 0.75211 | val_1_rmse: 0.77076 |  0:00:34s
epoch 55 | loss: 0.51725 | val_0_rmse: 0.76449 | val_1_rmse: 0.78461 |  0:00:34s
epoch 56 | loss: 0.51342 | val_0_rmse: 0.74494 | val_1_rmse: 0.75801 |  0:00:35s
epoch 57 | loss: 0.50818 | val_0_rmse: 0.7422  | val_1_rmse: 0.76256 |  0:00:35s
epoch 58 | loss: 0.50781 | val_0_rmse: 0.75199 | val_1_rmse: 0.78605 |  0:00:36s
epoch 59 | loss: 0.50609 | val_0_rmse: 0.75335 | val_1_rmse: 0.79118 |  0:00:37s
epoch 60 | loss: 0.50643 | val_0_rmse: 0.73975 | val_1_rmse: 0.77996 |  0:00:37s
epoch 61 | loss: 0.49548 | val_0_rmse: 0.73961 | val_1_rmse: 0.78748 |  0:00:38s
epoch 62 | loss: 0.49446 | val_0_rmse: 0.72532 | val_1_rmse: 0.7766  |  0:00:38s
epoch 63 | loss: 0.47998 | val_0_rmse: 0.72501 | val_1_rmse: 0.77414 |  0:00:39s
epoch 64 | loss: 0.47876 | val_0_rmse: 0.72317 | val_1_rmse: 0.76828 |  0:00:40s
epoch 65 | loss: 0.47974 | val_0_rmse: 0.72195 | val_1_rmse: 0.78823 |  0:00:40s
epoch 66 | loss: 0.48219 | val_0_rmse: 0.7197  | val_1_rmse: 0.79058 |  0:00:41s
epoch 67 | loss: 0.46565 | val_0_rmse: 0.71965 | val_1_rmse: 0.80398 |  0:00:42s
epoch 68 | loss: 0.44991 | val_0_rmse: 0.69959 | val_1_rmse: 0.77121 |  0:00:42s
epoch 69 | loss: 0.45801 | val_0_rmse: 0.69745 | val_1_rmse: 0.77319 |  0:00:43s
epoch 70 | loss: 0.44991 | val_0_rmse: 0.6959  | val_1_rmse: 0.77609 |  0:00:43s
epoch 71 | loss: 0.44687 | val_0_rmse: 0.68406 | val_1_rmse: 0.77614 |  0:00:44s
epoch 72 | loss: 0.44732 | val_0_rmse: 0.69078 | val_1_rmse: 0.81077 |  0:00:45s
epoch 73 | loss: 0.44052 | val_0_rmse: 0.67792 | val_1_rmse: 0.76736 |  0:00:45s
epoch 74 | loss: 0.43441 | val_0_rmse: 0.6821  | val_1_rmse: 0.80354 |  0:00:46s
epoch 75 | loss: 0.43029 | val_0_rmse: 0.686   | val_1_rmse: 0.76594 |  0:00:46s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.75036
Best weights from best epoch are automatically used!
ended training at: 04:26:52
Feature importance:
Mean squared error is of 0.04537404942306187
Mean absolute error:0.15867275046211488
MAPE:0.16056919458343707
R2 score:0.47363926052603744
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:26:53
epoch 0  | loss: 2.82889 | val_0_rmse: 0.98121 | val_1_rmse: 1.04103 |  0:00:00s
epoch 1  | loss: 1.23157 | val_0_rmse: 0.97961 | val_1_rmse: 1.0403  |  0:00:01s
epoch 2  | loss: 1.03412 | val_0_rmse: 0.97976 | val_1_rmse: 1.03964 |  0:00:01s
epoch 3  | loss: 0.97949 | val_0_rmse: 0.98097 | val_1_rmse: 1.04085 |  0:00:02s
epoch 4  | loss: 0.97303 | val_0_rmse: 0.98149 | val_1_rmse: 1.04119 |  0:00:03s
epoch 5  | loss: 0.9707  | val_0_rmse: 0.9814  | val_1_rmse: 1.04113 |  0:00:03s
epoch 6  | loss: 0.97173 | val_0_rmse: 0.98153 | val_1_rmse: 1.04115 |  0:00:04s
epoch 7  | loss: 0.96458 | val_0_rmse: 0.98135 | val_1_rmse: 1.04106 |  0:00:05s
epoch 8  | loss: 0.96512 | val_0_rmse: 0.9814  | val_1_rmse: 1.04107 |  0:00:05s
epoch 9  | loss: 0.96544 | val_0_rmse: 0.98149 | val_1_rmse: 1.04127 |  0:00:06s
epoch 10 | loss: 0.96377 | val_0_rmse: 0.98147 | val_1_rmse: 1.0412  |  0:00:06s
epoch 11 | loss: 0.96405 | val_0_rmse: 0.98145 | val_1_rmse: 1.04122 |  0:00:07s
epoch 12 | loss: 0.96443 | val_0_rmse: 0.98145 | val_1_rmse: 1.0412  |  0:00:08s
epoch 13 | loss: 0.96485 | val_0_rmse: 0.98117 | val_1_rmse: 1.04121 |  0:00:08s
epoch 14 | loss: 0.9642  | val_0_rmse: 0.98119 | val_1_rmse: 1.04106 |  0:00:09s
epoch 15 | loss: 0.96349 | val_0_rmse: 0.98123 | val_1_rmse: 1.04108 |  0:00:09s
epoch 16 | loss: 0.96342 | val_0_rmse: 0.98115 | val_1_rmse: 1.04099 |  0:00:10s
epoch 17 | loss: 0.96295 | val_0_rmse: 0.98131 | val_1_rmse: 1.04114 |  0:00:11s
epoch 18 | loss: 0.96248 | val_0_rmse: 0.98105 | val_1_rmse: 1.04091 |  0:00:11s
epoch 19 | loss: 0.96269 | val_0_rmse: 0.98134 | val_1_rmse: 1.04109 |  0:00:12s
epoch 20 | loss: 0.96337 | val_0_rmse: 0.98116 | val_1_rmse: 1.04117 |  0:00:13s
epoch 21 | loss: 0.96267 | val_0_rmse: 0.9814  | val_1_rmse: 1.04132 |  0:00:13s
epoch 22 | loss: 0.96036 | val_0_rmse: 0.9814  | val_1_rmse: 1.04141 |  0:00:14s
epoch 23 | loss: 0.95673 | val_0_rmse: 0.98128 | val_1_rmse: 1.04158 |  0:00:14s
epoch 24 | loss: 0.95372 | val_0_rmse: 0.98124 | val_1_rmse: 1.04151 |  0:00:15s
epoch 25 | loss: 0.94882 | val_0_rmse: 0.98132 | val_1_rmse: 1.04102 |  0:00:16s
epoch 26 | loss: 0.94982 | val_0_rmse: 0.97587 | val_1_rmse: 1.0367  |  0:00:16s
epoch 27 | loss: 0.9516  | val_0_rmse: 0.9767  | val_1_rmse: 1.03665 |  0:00:17s
epoch 28 | loss: 0.94397 | val_0_rmse: 0.97922 | val_1_rmse: 1.0363  |  0:00:18s
epoch 29 | loss: 0.94444 | val_0_rmse: 0.97992 | val_1_rmse: 1.04101 |  0:00:18s
epoch 30 | loss: 0.94611 | val_0_rmse: 0.98032 | val_1_rmse: 1.04065 |  0:00:19s
epoch 31 | loss: 0.94514 | val_0_rmse: 0.97931 | val_1_rmse: 1.03826 |  0:00:19s
epoch 32 | loss: 0.94037 | val_0_rmse: 0.97824 | val_1_rmse: 1.03483 |  0:00:20s
epoch 33 | loss: 0.94008 | val_0_rmse: 0.97735 | val_1_rmse: 1.0315  |  0:00:21s
epoch 34 | loss: 0.94067 | val_0_rmse: 0.97701 | val_1_rmse: 1.0322  |  0:00:21s
epoch 35 | loss: 0.93907 | val_0_rmse: 0.97732 | val_1_rmse: 1.03009 |  0:00:22s
epoch 36 | loss: 0.93469 | val_0_rmse: 0.97137 | val_1_rmse: 1.02935 |  0:00:23s
epoch 37 | loss: 0.93584 | val_0_rmse: 0.97143 | val_1_rmse: 1.02772 |  0:00:23s
epoch 38 | loss: 0.93731 | val_0_rmse: 0.97137 | val_1_rmse: 1.02692 |  0:00:24s
epoch 39 | loss: 0.93164 | val_0_rmse: 0.97176 | val_1_rmse: 1.02714 |  0:00:24s
epoch 40 | loss: 0.93355 | val_0_rmse: 0.97292 | val_1_rmse: 1.03056 |  0:00:25s
epoch 41 | loss: 0.93032 | val_0_rmse: 0.97297 | val_1_rmse: 1.03081 |  0:00:26s
epoch 42 | loss: 0.93081 | val_0_rmse: 0.96776 | val_1_rmse: 1.02735 |  0:00:26s
epoch 43 | loss: 0.92548 | val_0_rmse: 0.96454 | val_1_rmse: 1.02435 |  0:00:27s
epoch 44 | loss: 0.91873 | val_0_rmse: 0.96048 | val_1_rmse: 1.0262  |  0:00:27s
epoch 45 | loss: 0.9191  | val_0_rmse: 0.95547 | val_1_rmse: 1.02325 |  0:00:28s
epoch 46 | loss: 0.9102  | val_0_rmse: 0.9485  | val_1_rmse: 1.01986 |  0:00:29s
epoch 47 | loss: 0.90987 | val_0_rmse: 0.94027 | val_1_rmse: 1.00862 |  0:00:29s
epoch 48 | loss: 0.89878 | val_0_rmse: 0.93157 | val_1_rmse: 1.00003 |  0:00:30s
epoch 49 | loss: 0.89155 | val_0_rmse: 0.93385 | val_1_rmse: 0.99367 |  0:00:30s
epoch 50 | loss: 0.8767  | val_0_rmse: 0.92135 | val_1_rmse: 0.98825 |  0:00:31s
epoch 51 | loss: 0.85691 | val_0_rmse: 0.90963 | val_1_rmse: 0.9766  |  0:00:32s
epoch 52 | loss: 0.83783 | val_0_rmse: 0.91388 | val_1_rmse: 0.97691 |  0:00:32s
epoch 53 | loss: 0.81785 | val_0_rmse: 0.90269 | val_1_rmse: 0.96494 |  0:00:33s
epoch 54 | loss: 0.79925 | val_0_rmse: 0.89071 | val_1_rmse: 0.95653 |  0:00:34s
epoch 55 | loss: 0.77981 | val_0_rmse: 0.88091 | val_1_rmse: 0.95467 |  0:00:34s
epoch 56 | loss: 0.76448 | val_0_rmse: 0.89634 | val_1_rmse: 0.96862 |  0:00:35s
epoch 57 | loss: 0.75064 | val_0_rmse: 0.88848 | val_1_rmse: 0.96491 |  0:00:36s
epoch 58 | loss: 0.73574 | val_0_rmse: 0.86853 | val_1_rmse: 0.93762 |  0:00:36s
epoch 59 | loss: 0.72219 | val_0_rmse: 0.84647 | val_1_rmse: 0.91247 |  0:00:37s
epoch 60 | loss: 0.69377 | val_0_rmse: 0.84258 | val_1_rmse: 0.89477 |  0:00:37s
epoch 61 | loss: 0.6813  | val_0_rmse: 0.82371 | val_1_rmse: 0.87987 |  0:00:38s
epoch 62 | loss: 0.65052 | val_0_rmse: 0.80408 | val_1_rmse: 0.84791 |  0:00:39s
epoch 63 | loss: 0.64745 | val_0_rmse: 0.79275 | val_1_rmse: 0.84497 |  0:00:39s
epoch 64 | loss: 0.62635 | val_0_rmse: 0.82101 | val_1_rmse: 0.82704 |  0:00:40s
epoch 65 | loss: 0.63267 | val_0_rmse: 0.793   | val_1_rmse: 0.84309 |  0:00:40s
epoch 66 | loss: 0.62024 | val_0_rmse: 0.85464 | val_1_rmse: 0.87075 |  0:00:41s
epoch 67 | loss: 0.61353 | val_0_rmse: 0.78921 | val_1_rmse: 0.81003 |  0:00:42s
epoch 68 | loss: 0.6     | val_0_rmse: 0.78816 | val_1_rmse: 0.84106 |  0:00:42s
epoch 69 | loss: 0.62083 | val_0_rmse: 0.77321 | val_1_rmse: 0.82286 |  0:00:43s
epoch 70 | loss: 0.59419 | val_0_rmse: 0.77057 | val_1_rmse: 0.81406 |  0:00:44s
epoch 71 | loss: 0.5834  | val_0_rmse: 0.76882 | val_1_rmse: 0.80251 |  0:00:44s
epoch 72 | loss: 0.57245 | val_0_rmse: 0.75478 | val_1_rmse: 0.79947 |  0:00:45s
epoch 73 | loss: 0.55964 | val_0_rmse: 0.75018 | val_1_rmse: 0.79026 |  0:00:45s
epoch 74 | loss: 0.55568 | val_0_rmse: 0.74747 | val_1_rmse: 0.80978 |  0:00:46s
epoch 75 | loss: 0.5491  | val_0_rmse: 0.77346 | val_1_rmse: 0.8181  |  0:00:47s
epoch 76 | loss: 0.5552  | val_0_rmse: 0.74424 | val_1_rmse: 0.80815 |  0:00:47s
epoch 77 | loss: 0.54179 | val_0_rmse: 0.74272 | val_1_rmse: 0.82697 |  0:00:48s
epoch 78 | loss: 0.55016 | val_0_rmse: 0.75299 | val_1_rmse: 0.82577 |  0:00:48s
epoch 79 | loss: 0.55397 | val_0_rmse: 0.75049 | val_1_rmse: 0.82606 |  0:00:49s
epoch 80 | loss: 0.55553 | val_0_rmse: 0.73857 | val_1_rmse: 0.8001  |  0:00:50s
epoch 81 | loss: 0.54296 | val_0_rmse: 0.7299  | val_1_rmse: 0.81107 |  0:00:50s
epoch 82 | loss: 0.5411  | val_0_rmse: 0.72662 | val_1_rmse: 0.82396 |  0:00:51s
epoch 83 | loss: 0.53667 | val_0_rmse: 0.74906 | val_1_rmse: 0.84695 |  0:00:52s
epoch 84 | loss: 0.5252  | val_0_rmse: 0.71706 | val_1_rmse: 0.80413 |  0:00:52s
epoch 85 | loss: 0.52094 | val_0_rmse: 0.73369 | val_1_rmse: 0.83027 |  0:00:53s
epoch 86 | loss: 0.51455 | val_0_rmse: 0.70815 | val_1_rmse: 0.80635 |  0:00:53s
epoch 87 | loss: 0.50565 | val_0_rmse: 0.70569 | val_1_rmse: 0.84534 |  0:00:54s
epoch 88 | loss: 0.49936 | val_0_rmse: 0.70805 | val_1_rmse: 0.82651 |  0:00:55s
epoch 89 | loss: 0.50754 | val_0_rmse: 0.69729 | val_1_rmse: 0.81733 |  0:00:55s
epoch 90 | loss: 0.49529 | val_0_rmse: 0.69807 | val_1_rmse: 0.81927 |  0:00:56s
epoch 91 | loss: 0.49338 | val_0_rmse: 0.69    | val_1_rmse: 0.80314 |  0:00:56s
epoch 92 | loss: 0.49351 | val_0_rmse: 0.71154 | val_1_rmse: 0.82727 |  0:00:57s
epoch 93 | loss: 0.49076 | val_0_rmse: 0.7611  | val_1_rmse: 0.85219 |  0:00:58s
epoch 94 | loss: 0.4923  | val_0_rmse: 0.72804 | val_1_rmse: 0.87584 |  0:00:58s
epoch 95 | loss: 0.48742 | val_0_rmse: 0.68696 | val_1_rmse: 0.81345 |  0:00:59s
epoch 96 | loss: 0.48327 | val_0_rmse: 0.72077 | val_1_rmse: 0.86195 |  0:01:00s
epoch 97 | loss: 0.47591 | val_0_rmse: 0.68431 | val_1_rmse: 0.82729 |  0:01:00s
epoch 98 | loss: 0.46093 | val_0_rmse: 0.68401 | val_1_rmse: 0.84998 |  0:01:01s
epoch 99 | loss: 0.4591  | val_0_rmse: 0.66477 | val_1_rmse: 0.82971 |  0:01:01s
epoch 100| loss: 0.45964 | val_0_rmse: 0.7557  | val_1_rmse: 0.84535 |  0:01:02s
epoch 101| loss: 0.45612 | val_0_rmse: 0.67042 | val_1_rmse: 0.82438 |  0:01:03s
epoch 102| loss: 0.46298 | val_0_rmse: 0.65621 | val_1_rmse: 0.83941 |  0:01:03s
epoch 103| loss: 0.45865 | val_0_rmse: 0.65847 | val_1_rmse: 0.84875 |  0:01:04s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.79026
Best weights from best epoch are automatically used!
ended training at: 04:27:57
Feature importance:
Mean squared error is of 0.05237430639672277
Mean absolute error:0.16121926720281418
MAPE:0.16246808441292238
R2 score:0.4648923163082319
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:27:58
epoch 0  | loss: 2.11243 | val_0_rmse: 1.00793 | val_1_rmse: 0.94847 |  0:00:00s
epoch 1  | loss: 1.27652 | val_0_rmse: 1.00614 | val_1_rmse: 0.94682 |  0:00:01s
epoch 2  | loss: 1.07521 | val_0_rmse: 1.00569 | val_1_rmse: 0.94665 |  0:00:01s
epoch 3  | loss: 1.0243  | val_0_rmse: 1.00566 | val_1_rmse: 0.94705 |  0:00:02s
epoch 4  | loss: 1.01546 | val_0_rmse: 1.00688 | val_1_rmse: 0.94777 |  0:00:03s
epoch 5  | loss: 1.00353 | val_0_rmse: 1.00669 | val_1_rmse: 0.94727 |  0:00:03s
epoch 6  | loss: 0.97687 | val_0_rmse: 0.97717 | val_1_rmse: 0.9166  |  0:00:04s
epoch 7  | loss: 0.9495  | val_0_rmse: 0.95642 | val_1_rmse: 0.91449 |  0:00:05s
epoch 8  | loss: 0.93302 | val_0_rmse: 0.94392 | val_1_rmse: 0.90033 |  0:00:05s
epoch 9  | loss: 0.92479 | val_0_rmse: 0.93062 | val_1_rmse: 0.88786 |  0:00:06s
epoch 10 | loss: 0.89669 | val_0_rmse: 0.92974 | val_1_rmse: 0.88933 |  0:00:06s
epoch 11 | loss: 0.86546 | val_0_rmse: 0.92338 | val_1_rmse: 0.89518 |  0:00:07s
epoch 12 | loss: 0.8733  | val_0_rmse: 0.91393 | val_1_rmse: 0.87455 |  0:00:08s
epoch 13 | loss: 0.87557 | val_0_rmse: 0.9539  | val_1_rmse: 0.95937 |  0:00:08s
epoch 14 | loss: 0.87398 | val_0_rmse: 0.91441 | val_1_rmse: 0.87922 |  0:00:09s
epoch 15 | loss: 0.87992 | val_0_rmse: 0.91622 | val_1_rmse: 0.89019 |  0:00:09s
epoch 16 | loss: 0.8584  | val_0_rmse: 0.92086 | val_1_rmse: 0.91323 |  0:00:10s
epoch 17 | loss: 0.8573  | val_0_rmse: 0.94257 | val_1_rmse: 0.95727 |  0:00:11s
epoch 18 | loss: 0.85394 | val_0_rmse: 0.90152 | val_1_rmse: 0.88461 |  0:00:11s
epoch 19 | loss: 0.85301 | val_0_rmse: 0.94512 | val_1_rmse: 0.95923 |  0:00:12s
epoch 20 | loss: 0.89414 | val_0_rmse: 0.92186 | val_1_rmse: 0.87857 |  0:00:12s
epoch 21 | loss: 0.93612 | val_0_rmse: 0.93024 | val_1_rmse: 0.88749 |  0:00:13s
epoch 22 | loss: 0.91127 | val_0_rmse: 0.91456 | val_1_rmse: 0.89453 |  0:00:14s
epoch 23 | loss: 0.91137 | val_0_rmse: 0.91225 | val_1_rmse: 0.87834 |  0:00:14s
epoch 24 | loss: 0.91586 | val_0_rmse: 0.91652 | val_1_rmse: 0.88796 |  0:00:15s
epoch 25 | loss: 0.9008  | val_0_rmse: 0.91559 | val_1_rmse: 0.87989 |  0:00:15s
epoch 26 | loss: 0.88586 | val_0_rmse: 0.91079 | val_1_rmse: 0.88911 |  0:00:16s
epoch 27 | loss: 0.87692 | val_0_rmse: 0.91295 | val_1_rmse: 0.88543 |  0:00:17s
epoch 28 | loss: 0.87022 | val_0_rmse: 0.90531 | val_1_rmse: 0.89446 |  0:00:17s
epoch 29 | loss: 0.88126 | val_0_rmse: 0.90285 | val_1_rmse: 0.87306 |  0:00:18s
epoch 30 | loss: 0.87    | val_0_rmse: 0.90953 | val_1_rmse: 0.90009 |  0:00:19s
epoch 31 | loss: 0.88151 | val_0_rmse: 0.92451 | val_1_rmse: 0.92627 |  0:00:19s
epoch 32 | loss: 0.87678 | val_0_rmse: 0.90362 | val_1_rmse: 0.89555 |  0:00:20s
epoch 33 | loss: 0.87817 | val_0_rmse: 0.90883 | val_1_rmse: 0.86902 |  0:00:20s
epoch 34 | loss: 0.86683 | val_0_rmse: 0.92327 | val_1_rmse: 0.89344 |  0:00:21s
epoch 35 | loss: 0.88309 | val_0_rmse: 0.92548 | val_1_rmse: 0.87484 |  0:00:22s
epoch 36 | loss: 0.89475 | val_0_rmse: 0.95596 | val_1_rmse: 0.89532 |  0:00:22s
epoch 37 | loss: 0.8768  | val_0_rmse: 0.94044 | val_1_rmse: 0.88666 |  0:00:23s
epoch 38 | loss: 0.88268 | val_0_rmse: 0.92976 | val_1_rmse: 0.88124 |  0:00:23s
epoch 39 | loss: 0.88151 | val_0_rmse: 0.92524 | val_1_rmse: 0.87202 |  0:00:24s
epoch 40 | loss: 0.87131 | val_0_rmse: 0.93192 | val_1_rmse: 0.92167 |  0:00:25s
epoch 41 | loss: 0.86937 | val_0_rmse: 0.92016 | val_1_rmse: 0.89866 |  0:00:25s
epoch 42 | loss: 0.87254 | val_0_rmse: 0.91458 | val_1_rmse: 0.88542 |  0:00:26s
epoch 43 | loss: 0.86305 | val_0_rmse: 0.94421 | val_1_rmse: 0.93806 |  0:00:27s
epoch 44 | loss: 0.85594 | val_0_rmse: 0.91484 | val_1_rmse: 0.8881  |  0:00:27s
epoch 45 | loss: 0.86436 | val_0_rmse: 0.91208 | val_1_rmse: 0.88569 |  0:00:28s
epoch 46 | loss: 0.85246 | val_0_rmse: 0.91837 | val_1_rmse: 0.89927 |  0:00:28s
epoch 47 | loss: 0.87619 | val_0_rmse: 0.92734 | val_1_rmse: 0.8761  |  0:00:29s
epoch 48 | loss: 0.88604 | val_0_rmse: 0.91355 | val_1_rmse: 0.88123 |  0:00:30s
epoch 49 | loss: 0.89415 | val_0_rmse: 0.91617 | val_1_rmse: 0.88635 |  0:00:30s
epoch 50 | loss: 0.87428 | val_0_rmse: 0.91794 | val_1_rmse: 0.87168 |  0:00:31s
epoch 51 | loss: 0.87373 | val_0_rmse: 0.91322 | val_1_rmse: 0.8795  |  0:00:31s
epoch 52 | loss: 0.88001 | val_0_rmse: 0.91742 | val_1_rmse: 0.87384 |  0:00:32s
epoch 53 | loss: 0.88281 | val_0_rmse: 0.93189 | val_1_rmse: 0.87831 |  0:00:33s
epoch 54 | loss: 0.8798  | val_0_rmse: 0.9361  | val_1_rmse: 0.88626 |  0:00:33s
epoch 55 | loss: 0.89692 | val_0_rmse: 0.94074 | val_1_rmse: 0.88879 |  0:00:34s
epoch 56 | loss: 0.86804 | val_0_rmse: 0.91948 | val_1_rmse: 0.88169 |  0:00:35s
epoch 57 | loss: 0.84605 | val_0_rmse: 0.92694 | val_1_rmse: 0.8837  |  0:00:35s
epoch 58 | loss: 0.85228 | val_0_rmse: 0.93115 | val_1_rmse: 0.88159 |  0:00:36s
epoch 59 | loss: 0.87362 | val_0_rmse: 0.92421 | val_1_rmse: 0.87563 |  0:00:36s
epoch 60 | loss: 0.84875 | val_0_rmse: 0.91873 | val_1_rmse: 0.87342 |  0:00:37s
epoch 61 | loss: 0.84262 | val_0_rmse: 0.91595 | val_1_rmse: 0.88723 |  0:00:38s
epoch 62 | loss: 0.85797 | val_0_rmse: 0.92047 | val_1_rmse: 0.87393 |  0:00:38s
epoch 63 | loss: 0.85715 | val_0_rmse: 0.93281 | val_1_rmse: 0.93625 |  0:00:39s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.86902
Best weights from best epoch are automatically used!
ended training at: 04:28:37
Feature importance:
Mean squared error is of 0.08588459384572408
Mean absolute error:0.18402422628544143
MAPE:0.18930965587955523
R2 score:0.13784301375215946
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:28:38
epoch 0  | loss: 3.45754 | val_0_rmse: 1.00168 | val_1_rmse: 0.99608 |  0:00:00s
epoch 1  | loss: 2.43755 | val_0_rmse: 1.00373 | val_1_rmse: 0.99821 |  0:00:00s
epoch 2  | loss: 2.02741 | val_0_rmse: 0.98656 | val_1_rmse: 0.99142 |  0:00:00s
epoch 3  | loss: 1.55879 | val_0_rmse: 0.9718  | val_1_rmse: 0.98728 |  0:00:00s
epoch 4  | loss: 1.28111 | val_0_rmse: 0.97308 | val_1_rmse: 0.9866  |  0:00:00s
epoch 5  | loss: 1.23456 | val_0_rmse: 0.97492 | val_1_rmse: 0.98703 |  0:00:00s
epoch 6  | loss: 1.09517 | val_0_rmse: 0.97477 | val_1_rmse: 0.98715 |  0:00:00s
epoch 7  | loss: 1.08447 | val_0_rmse: 0.97343 | val_1_rmse: 0.98786 |  0:00:00s
epoch 8  | loss: 1.17221 | val_0_rmse: 0.97358 | val_1_rmse: 0.98861 |  0:00:00s
epoch 9  | loss: 1.04567 | val_0_rmse: 0.9748  | val_1_rmse: 0.98908 |  0:00:00s
epoch 10 | loss: 1.06595 | val_0_rmse: 0.97461 | val_1_rmse: 0.98886 |  0:00:00s
epoch 11 | loss: 1.05803 | val_0_rmse: 0.97364 | val_1_rmse: 0.98848 |  0:00:01s
epoch 12 | loss: 1.02409 | val_0_rmse: 0.97356 | val_1_rmse: 0.9877  |  0:00:01s
epoch 13 | loss: 0.99029 | val_0_rmse: 0.97247 | val_1_rmse: 0.98781 |  0:00:01s
epoch 14 | loss: 0.94686 | val_0_rmse: 0.96925 | val_1_rmse: 0.98924 |  0:00:01s
epoch 15 | loss: 0.92382 | val_0_rmse: 0.96811 | val_1_rmse: 0.99157 |  0:00:01s
epoch 16 | loss: 0.95572 | val_0_rmse: 0.967   | val_1_rmse: 0.99322 |  0:00:01s
epoch 17 | loss: 0.91076 | val_0_rmse: 0.9678  | val_1_rmse: 0.99238 |  0:00:01s
epoch 18 | loss: 0.94212 | val_0_rmse: 0.96803 | val_1_rmse: 0.99188 |  0:00:01s
epoch 19 | loss: 0.92292 | val_0_rmse: 0.96739 | val_1_rmse: 0.9916  |  0:00:01s
epoch 20 | loss: 0.9231  | val_0_rmse: 0.96654 | val_1_rmse: 0.99161 |  0:00:01s
epoch 21 | loss: 0.92522 | val_0_rmse: 0.96589 | val_1_rmse: 0.99189 |  0:00:01s
epoch 22 | loss: 0.92819 | val_0_rmse: 0.96558 | val_1_rmse: 0.99239 |  0:00:01s
epoch 23 | loss: 0.93775 | val_0_rmse: 0.96513 | val_1_rmse: 0.9926  |  0:00:01s
epoch 24 | loss: 0.92172 | val_0_rmse: 0.9646  | val_1_rmse: 0.99298 |  0:00:02s
epoch 25 | loss: 0.90801 | val_0_rmse: 0.96403 | val_1_rmse: 0.99285 |  0:00:02s
epoch 26 | loss: 0.91632 | val_0_rmse: 0.9632  | val_1_rmse: 0.99137 |  0:00:02s
epoch 27 | loss: 0.90517 | val_0_rmse: 0.963   | val_1_rmse: 0.98935 |  0:00:02s
epoch 28 | loss: 0.92183 | val_0_rmse: 0.96304 | val_1_rmse: 0.98798 |  0:00:02s
epoch 29 | loss: 0.90074 | val_0_rmse: 0.96129 | val_1_rmse: 0.98783 |  0:00:02s
epoch 30 | loss: 0.90526 | val_0_rmse: 0.95855 | val_1_rmse: 0.99035 |  0:00:02s
epoch 31 | loss: 0.8978  | val_0_rmse: 0.95727 | val_1_rmse: 0.99638 |  0:00:02s
epoch 32 | loss: 0.89227 | val_0_rmse: 0.95871 | val_1_rmse: 1.00373 |  0:00:02s
epoch 33 | loss: 0.9103  | val_0_rmse: 0.95944 | val_1_rmse: 1.00532 |  0:00:02s
epoch 34 | loss: 0.89976 | val_0_rmse: 0.95882 | val_1_rmse: 1.00268 |  0:00:02s

Early stopping occured at epoch 34 with best_epoch = 4 and best_val_1_rmse = 0.9866
Best weights from best epoch are automatically used!
ended training at: 04:28:41
Feature importance:
Mean squared error is of 0.1210779877108307
Mean absolute error:0.23048729540199142
MAPE:0.21990024580735448
R2 score:-0.02613881112461347
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:28:41
epoch 0  | loss: 3.46918 | val_0_rmse: 1.04339 | val_1_rmse: 0.91825 |  0:00:00s
epoch 1  | loss: 2.28747 | val_0_rmse: 1.02832 | val_1_rmse: 0.89371 |  0:00:00s
epoch 2  | loss: 1.71568 | val_0_rmse: 1.01947 | val_1_rmse: 0.8827  |  0:00:00s
epoch 3  | loss: 1.58475 | val_0_rmse: 1.01398 | val_1_rmse: 0.87179 |  0:00:00s
epoch 4  | loss: 1.86514 | val_0_rmse: 1.01269 | val_1_rmse: 0.86624 |  0:00:00s
epoch 5  | loss: 1.37167 | val_0_rmse: 1.01202 | val_1_rmse: 0.86382 |  0:00:00s
epoch 6  | loss: 1.35787 | val_0_rmse: 1.01205 | val_1_rmse: 0.86687 |  0:00:00s
epoch 7  | loss: 1.27313 | val_0_rmse: 1.01305 | val_1_rmse: 0.87068 |  0:00:00s
epoch 8  | loss: 1.1967  | val_0_rmse: 1.01266 | val_1_rmse: 0.86936 |  0:00:00s
epoch 9  | loss: 1.14944 | val_0_rmse: 1.01194 | val_1_rmse: 0.86586 |  0:00:00s
epoch 10 | loss: 1.12665 | val_0_rmse: 1.01191 | val_1_rmse: 0.8643  |  0:00:00s
epoch 11 | loss: 1.06619 | val_0_rmse: 1.01239 | val_1_rmse: 0.86648 |  0:00:01s
epoch 12 | loss: 1.14521 | val_0_rmse: 1.01264 | val_1_rmse: 0.86752 |  0:00:01s
epoch 13 | loss: 1.07684 | val_0_rmse: 1.01246 | val_1_rmse: 0.86705 |  0:00:01s
epoch 14 | loss: 1.09702 | val_0_rmse: 1.01201 | val_1_rmse: 0.86504 |  0:00:01s
epoch 15 | loss: 1.0618  | val_0_rmse: 1.01198 | val_1_rmse: 0.86394 |  0:00:01s
epoch 16 | loss: 1.03353 | val_0_rmse: 1.01208 | val_1_rmse: 0.86334 |  0:00:01s
epoch 17 | loss: 1.04778 | val_0_rmse: 1.01229 | val_1_rmse: 0.86299 |  0:00:01s
epoch 18 | loss: 1.06812 | val_0_rmse: 1.01256 | val_1_rmse: 0.86279 |  0:00:01s
epoch 19 | loss: 1.06338 | val_0_rmse: 1.01264 | val_1_rmse: 0.86251 |  0:00:01s
epoch 20 | loss: 1.04516 | val_0_rmse: 1.01262 | val_1_rmse: 0.86252 |  0:00:01s
epoch 21 | loss: 1.04034 | val_0_rmse: 1.01262 | val_1_rmse: 0.86289 |  0:00:01s
epoch 22 | loss: 1.04109 | val_0_rmse: 1.01238 | val_1_rmse: 0.86261 |  0:00:01s
epoch 23 | loss: 1.0263  | val_0_rmse: 1.01238 | val_1_rmse: 0.86279 |  0:00:02s
epoch 24 | loss: 1.03133 | val_0_rmse: 1.01244 | val_1_rmse: 0.86292 |  0:00:02s
epoch 25 | loss: 1.02605 | val_0_rmse: 1.01228 | val_1_rmse: 0.86334 |  0:00:02s
epoch 26 | loss: 1.02196 | val_0_rmse: 1.01205 | val_1_rmse: 0.86411 |  0:00:02s
epoch 27 | loss: 1.01987 | val_0_rmse: 1.01211 | val_1_rmse: 0.86499 |  0:00:02s
epoch 28 | loss: 1.04594 | val_0_rmse: 1.01224 | val_1_rmse: 0.86513 |  0:00:02s
epoch 29 | loss: 1.0233  | val_0_rmse: 1.01231 | val_1_rmse: 0.86486 |  0:00:02s
epoch 30 | loss: 1.01487 | val_0_rmse: 1.01233 | val_1_rmse: 0.8647  |  0:00:02s
epoch 31 | loss: 1.01566 | val_0_rmse: 1.01231 | val_1_rmse: 0.86489 |  0:00:02s
epoch 32 | loss: 1.023   | val_0_rmse: 1.01242 | val_1_rmse: 0.8656  |  0:00:02s
epoch 33 | loss: 1.02303 | val_0_rmse: 1.01269 | val_1_rmse: 0.86677 |  0:00:02s
epoch 34 | loss: 0.99818 | val_0_rmse: 1.01317 | val_1_rmse: 0.86819 |  0:00:02s
epoch 35 | loss: 1.00234 | val_0_rmse: 1.01358 | val_1_rmse: 0.86921 |  0:00:03s
epoch 36 | loss: 1.01834 | val_0_rmse: 1.01364 | val_1_rmse: 0.86916 |  0:00:03s
epoch 37 | loss: 0.99689 | val_0_rmse: 1.01283 | val_1_rmse: 0.8681  |  0:00:03s
epoch 38 | loss: 0.9991  | val_0_rmse: 1.00932 | val_1_rmse: 0.86403 |  0:00:03s
epoch 39 | loss: 1.01449 | val_0_rmse: 1.00642 | val_1_rmse: 0.86023 |  0:00:03s
epoch 40 | loss: 0.99372 | val_0_rmse: 1.00299 | val_1_rmse: 0.85664 |  0:00:03s
epoch 41 | loss: 0.98389 | val_0_rmse: 0.99783 | val_1_rmse: 0.85552 |  0:00:03s
epoch 42 | loss: 0.99543 | val_0_rmse: 0.99789 | val_1_rmse: 0.85512 |  0:00:03s
epoch 43 | loss: 0.99876 | val_0_rmse: 1.00243 | val_1_rmse: 0.85602 |  0:00:03s
epoch 44 | loss: 0.96862 | val_0_rmse: 1.00732 | val_1_rmse: 0.85693 |  0:00:03s
epoch 45 | loss: 0.97642 | val_0_rmse: 1.00673 | val_1_rmse: 0.85608 |  0:00:03s
epoch 46 | loss: 0.99169 | val_0_rmse: 1.00664 | val_1_rmse: 0.85545 |  0:00:03s
epoch 47 | loss: 0.99051 | val_0_rmse: 1.00455 | val_1_rmse: 0.85556 |  0:00:04s
epoch 48 | loss: 0.95367 | val_0_rmse: 1.00202 | val_1_rmse: 0.85667 |  0:00:04s
epoch 49 | loss: 0.94772 | val_0_rmse: 0.99977 | val_1_rmse: 0.85805 |  0:00:04s
epoch 50 | loss: 0.95653 | val_0_rmse: 0.99727 | val_1_rmse: 0.85736 |  0:00:04s
epoch 51 | loss: 0.94553 | val_0_rmse: 0.99791 | val_1_rmse: 0.85735 |  0:00:04s
epoch 52 | loss: 0.92667 | val_0_rmse: 0.99797 | val_1_rmse: 0.85574 |  0:00:04s
epoch 53 | loss: 0.93473 | val_0_rmse: 0.99913 | val_1_rmse: 0.84863 |  0:00:04s
epoch 54 | loss: 0.94398 | val_0_rmse: 1.00034 | val_1_rmse: 0.84627 |  0:00:04s
epoch 55 | loss: 0.90491 | val_0_rmse: 0.99871 | val_1_rmse: 0.84848 |  0:00:04s
epoch 56 | loss: 0.94235 | val_0_rmse: 0.99318 | val_1_rmse: 0.84728 |  0:00:04s
epoch 57 | loss: 0.91497 | val_0_rmse: 0.98837 | val_1_rmse: 0.84968 |  0:00:04s
epoch 58 | loss: 0.92745 | val_0_rmse: 0.98455 | val_1_rmse: 0.8436  |  0:00:04s
epoch 59 | loss: 0.89893 | val_0_rmse: 0.98853 | val_1_rmse: 0.84406 |  0:00:05s
epoch 60 | loss: 0.89124 | val_0_rmse: 0.99166 | val_1_rmse: 0.85535 |  0:00:05s
epoch 61 | loss: 0.8986  | val_0_rmse: 0.98378 | val_1_rmse: 0.85767 |  0:00:05s
epoch 62 | loss: 0.92069 | val_0_rmse: 0.97709 | val_1_rmse: 0.85301 |  0:00:05s
epoch 63 | loss: 0.89021 | val_0_rmse: 0.96783 | val_1_rmse: 0.85299 |  0:00:05s
epoch 64 | loss: 0.90916 | val_0_rmse: 0.96396 | val_1_rmse: 0.85732 |  0:00:05s
epoch 65 | loss: 0.89909 | val_0_rmse: 0.96399 | val_1_rmse: 0.86584 |  0:00:05s
epoch 66 | loss: 0.8669  | val_0_rmse: 0.96521 | val_1_rmse: 0.86499 |  0:00:05s
epoch 67 | loss: 0.87369 | val_0_rmse: 0.96631 | val_1_rmse: 0.8646  |  0:00:05s
epoch 68 | loss: 0.8625  | val_0_rmse: 0.96567 | val_1_rmse: 0.85716 |  0:00:05s
epoch 69 | loss: 0.86837 | val_0_rmse: 0.96586 | val_1_rmse: 0.86278 |  0:00:05s
epoch 70 | loss: 0.88639 | val_0_rmse: 0.9706  | val_1_rmse: 0.87134 |  0:00:06s
epoch 71 | loss: 0.90862 | val_0_rmse: 0.97607 | val_1_rmse: 0.87194 |  0:00:06s
epoch 72 | loss: 0.85674 | val_0_rmse: 0.97805 | val_1_rmse: 0.87071 |  0:00:06s
epoch 73 | loss: 0.86025 | val_0_rmse: 0.97995 | val_1_rmse: 0.87151 |  0:00:06s
epoch 74 | loss: 0.88129 | val_0_rmse: 0.983   | val_1_rmse: 0.87228 |  0:00:06s
epoch 75 | loss: 0.86519 | val_0_rmse: 0.9874  | val_1_rmse: 0.87385 |  0:00:06s
epoch 76 | loss: 0.9079  | val_0_rmse: 0.99428 | val_1_rmse: 0.87096 |  0:00:06s
epoch 77 | loss: 0.90032 | val_0_rmse: 0.99425 | val_1_rmse: 0.86206 |  0:00:06s
epoch 78 | loss: 0.8994  | val_0_rmse: 0.99278 | val_1_rmse: 0.85662 |  0:00:06s
epoch 79 | loss: 0.88413 | val_0_rmse: 0.9963  | val_1_rmse: 0.8584  |  0:00:06s
epoch 80 | loss: 0.89383 | val_0_rmse: 0.99708 | val_1_rmse: 0.85919 |  0:00:06s
epoch 81 | loss: 0.88499 | val_0_rmse: 0.99652 | val_1_rmse: 0.86397 |  0:00:06s
epoch 82 | loss: 0.88708 | val_0_rmse: 0.99691 | val_1_rmse: 0.86686 |  0:00:07s
epoch 83 | loss: 0.88424 | val_0_rmse: 0.9962  | val_1_rmse: 0.86856 |  0:00:07s
epoch 84 | loss: 0.90023 | val_0_rmse: 0.99664 | val_1_rmse: 0.87104 |  0:00:07s
epoch 85 | loss: 0.89007 | val_0_rmse: 0.99617 | val_1_rmse: 0.86771 |  0:00:07s
epoch 86 | loss: 0.86688 | val_0_rmse: 0.99317 | val_1_rmse: 0.85583 |  0:00:07s
epoch 87 | loss: 0.88352 | val_0_rmse: 0.99229 | val_1_rmse: 0.85282 |  0:00:07s
epoch 88 | loss: 0.89029 | val_0_rmse: 0.99346 | val_1_rmse: 0.84982 |  0:00:07s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.8436
Best weights from best epoch are automatically used!
ended training at: 04:28:49
Feature importance:
Mean squared error is of 0.10465103883682983
Mean absolute error:0.21488400967460014
MAPE:0.23644604996685462
R2 score:0.02958386039503602
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:28:49
epoch 0  | loss: 3.88126 | val_0_rmse: 1.05556 | val_1_rmse: 0.96856 |  0:00:00s
epoch 1  | loss: 2.44317 | val_0_rmse: 1.04472 | val_1_rmse: 0.9361  |  0:00:00s
epoch 2  | loss: 1.89326 | val_0_rmse: 1.04827 | val_1_rmse: 0.92429 |  0:00:00s
epoch 3  | loss: 1.68403 | val_0_rmse: 1.05234 | val_1_rmse: 0.92206 |  0:00:00s
epoch 4  | loss: 1.44077 | val_0_rmse: 1.04934 | val_1_rmse: 0.92604 |  0:00:00s
epoch 5  | loss: 1.39775 | val_0_rmse: 1.04798 | val_1_rmse: 0.9331  |  0:00:00s
epoch 6  | loss: 1.19726 | val_0_rmse: 1.04624 | val_1_rmse: 0.93169 |  0:00:00s
epoch 7  | loss: 1.28043 | val_0_rmse: 1.04415 | val_1_rmse: 0.93    |  0:00:00s
epoch 8  | loss: 1.24689 | val_0_rmse: 1.04341 | val_1_rmse: 0.92876 |  0:00:00s
epoch 9  | loss: 1.17497 | val_0_rmse: 1.04315 | val_1_rmse: 0.9317  |  0:00:00s
epoch 10 | loss: 1.16912 | val_0_rmse: 1.04267 | val_1_rmse: 0.93248 |  0:00:00s
epoch 11 | loss: 1.10696 | val_0_rmse: 1.04223 | val_1_rmse: 0.93168 |  0:00:01s
epoch 12 | loss: 1.09381 | val_0_rmse: 1.04153 | val_1_rmse: 0.9292  |  0:00:01s
epoch 13 | loss: 1.09822 | val_0_rmse: 1.04159 | val_1_rmse: 0.92757 |  0:00:01s
epoch 14 | loss: 1.13252 | val_0_rmse: 1.04153 | val_1_rmse: 0.9271  |  0:00:01s
epoch 15 | loss: 1.08571 | val_0_rmse: 1.0413  | val_1_rmse: 0.92885 |  0:00:01s
epoch 16 | loss: 1.09564 | val_0_rmse: 1.04132 | val_1_rmse: 0.93061 |  0:00:01s
epoch 17 | loss: 1.08207 | val_0_rmse: 1.04118 | val_1_rmse: 0.93256 |  0:00:01s
epoch 18 | loss: 1.0806  | val_0_rmse: 1.04096 | val_1_rmse: 0.93077 |  0:00:01s
epoch 19 | loss: 1.07628 | val_0_rmse: 1.04111 | val_1_rmse: 0.92845 |  0:00:01s
epoch 20 | loss: 1.07899 | val_0_rmse: 1.04146 | val_1_rmse: 0.92735 |  0:00:01s
epoch 21 | loss: 1.08217 | val_0_rmse: 1.04195 | val_1_rmse: 0.9268  |  0:00:01s
epoch 22 | loss: 1.06462 | val_0_rmse: 1.04242 | val_1_rmse: 0.92632 |  0:00:01s
epoch 23 | loss: 1.0849  | val_0_rmse: 1.04249 | val_1_rmse: 0.92628 |  0:00:01s
epoch 24 | loss: 1.08515 | val_0_rmse: 1.04222 | val_1_rmse: 0.92747 |  0:00:02s
epoch 25 | loss: 1.07414 | val_0_rmse: 1.04235 | val_1_rmse: 0.92768 |  0:00:02s
epoch 26 | loss: 1.06383 | val_0_rmse: 1.04245 | val_1_rmse: 0.92688 |  0:00:02s
epoch 27 | loss: 1.06992 | val_0_rmse: 1.04349 | val_1_rmse: 0.92526 |  0:00:02s
epoch 28 | loss: 1.0697  | val_0_rmse: 1.04508 | val_1_rmse: 0.92503 |  0:00:02s
epoch 29 | loss: 1.05761 | val_0_rmse: 1.0457  | val_1_rmse: 0.92577 |  0:00:02s
epoch 30 | loss: 1.06365 | val_0_rmse: 1.04527 | val_1_rmse: 0.92637 |  0:00:02s
epoch 31 | loss: 1.07764 | val_0_rmse: 1.04637 | val_1_rmse: 0.92617 |  0:00:02s
epoch 32 | loss: 1.07194 | val_0_rmse: 1.04642 | val_1_rmse: 0.92618 |  0:00:02s
epoch 33 | loss: 1.0682  | val_0_rmse: 1.0492  | val_1_rmse: 0.92521 |  0:00:02s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.92206
Best weights from best epoch are automatically used!
ended training at: 04:28:52
Feature importance:
Mean squared error is of 0.06790719905975207
Mean absolute error:0.19070344419216623
MAPE:0.21549624432471914
R2 score:-0.014242837365981975
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:28:52
epoch 0  | loss: 2.92155 | val_0_rmse: 1.04396 | val_1_rmse: 0.8899  |  0:00:00s
epoch 1  | loss: 2.9497  | val_0_rmse: 1.06989 | val_1_rmse: 0.92735 |  0:00:00s
epoch 2  | loss: 2.66514 | val_0_rmse: 1.06236 | val_1_rmse: 0.91562 |  0:00:00s
epoch 3  | loss: 2.46344 | val_0_rmse: 1.03913 | val_1_rmse: 0.87364 |  0:00:00s
epoch 4  | loss: 1.66188 | val_0_rmse: 1.03231 | val_1_rmse: 0.84984 |  0:00:00s
epoch 5  | loss: 1.43781 | val_0_rmse: 1.04458 | val_1_rmse: 0.8531  |  0:00:00s
epoch 6  | loss: 1.48249 | val_0_rmse: 1.03819 | val_1_rmse: 0.84953 |  0:00:00s
epoch 7  | loss: 1.7321  | val_0_rmse: 1.0324  | val_1_rmse: 0.85211 |  0:00:00s
epoch 8  | loss: 1.30686 | val_0_rmse: 1.03369 | val_1_rmse: 0.8588  |  0:00:00s
epoch 9  | loss: 1.26932 | val_0_rmse: 1.03256 | val_1_rmse: 0.85728 |  0:00:00s
epoch 10 | loss: 1.19238 | val_0_rmse: 1.03086 | val_1_rmse: 0.85376 |  0:00:00s
epoch 11 | loss: 1.24472 | val_0_rmse: 1.03142 | val_1_rmse: 0.85196 |  0:00:01s
epoch 12 | loss: 1.28397 | val_0_rmse: 1.03223 | val_1_rmse: 0.8522  |  0:00:01s
epoch 13 | loss: 1.13847 | val_0_rmse: 1.03209 | val_1_rmse: 0.85195 |  0:00:01s
epoch 14 | loss: 1.11788 | val_0_rmse: 1.03118 | val_1_rmse: 0.85105 |  0:00:01s
epoch 15 | loss: 1.167   | val_0_rmse: 1.03081 | val_1_rmse: 0.85003 |  0:00:01s
epoch 16 | loss: 1.10088 | val_0_rmse: 1.03123 | val_1_rmse: 0.85037 |  0:00:01s
epoch 17 | loss: 1.13019 | val_0_rmse: 1.03139 | val_1_rmse: 0.85056 |  0:00:01s
epoch 18 | loss: 1.0782  | val_0_rmse: 1.03162 | val_1_rmse: 0.85036 |  0:00:01s
epoch 19 | loss: 1.08939 | val_0_rmse: 1.03174 | val_1_rmse: 0.84996 |  0:00:01s
epoch 20 | loss: 1.08398 | val_0_rmse: 1.03113 | val_1_rmse: 0.84976 |  0:00:01s
epoch 21 | loss: 1.09189 | val_0_rmse: 1.03021 | val_1_rmse: 0.85034 |  0:00:01s
epoch 22 | loss: 1.07151 | val_0_rmse: 1.02989 | val_1_rmse: 0.85141 |  0:00:01s
epoch 23 | loss: 1.03451 | val_0_rmse: 1.02965 | val_1_rmse: 0.85227 |  0:00:01s
epoch 24 | loss: 1.06349 | val_0_rmse: 1.02926 | val_1_rmse: 0.85339 |  0:00:02s
epoch 25 | loss: 1.04352 | val_0_rmse: 1.02899 | val_1_rmse: 0.85373 |  0:00:02s
epoch 26 | loss: 1.03261 | val_0_rmse: 1.0284  | val_1_rmse: 0.8543  |  0:00:02s
epoch 27 | loss: 1.06102 | val_0_rmse: 1.02773 | val_1_rmse: 0.85417 |  0:00:02s
epoch 28 | loss: 1.02789 | val_0_rmse: 1.02717 | val_1_rmse: 0.85291 |  0:00:02s
epoch 29 | loss: 1.03194 | val_0_rmse: 1.0269  | val_1_rmse: 0.85145 |  0:00:02s
epoch 30 | loss: 1.02138 | val_0_rmse: 1.02581 | val_1_rmse: 0.84987 |  0:00:02s
epoch 31 | loss: 1.0242  | val_0_rmse: 1.02285 | val_1_rmse: 0.8469  |  0:00:02s
epoch 32 | loss: 1.00128 | val_0_rmse: 1.0221  | val_1_rmse: 0.85237 |  0:00:02s
epoch 33 | loss: 1.00702 | val_0_rmse: 1.02203 | val_1_rmse: 0.87252 |  0:00:02s
epoch 34 | loss: 0.98942 | val_0_rmse: 1.03081 | val_1_rmse: 0.9018  |  0:00:02s
epoch 35 | loss: 0.99007 | val_0_rmse: 1.02518 | val_1_rmse: 0.90006 |  0:00:02s
epoch 36 | loss: 0.99935 | val_0_rmse: 1.0206  | val_1_rmse: 0.89937 |  0:00:03s
epoch 37 | loss: 0.95969 | val_0_rmse: 1.01588 | val_1_rmse: 0.90098 |  0:00:03s
epoch 38 | loss: 0.92495 | val_0_rmse: 1.01023 | val_1_rmse: 0.89639 |  0:00:03s
epoch 39 | loss: 0.93333 | val_0_rmse: 1.00789 | val_1_rmse: 0.88933 |  0:00:03s
epoch 40 | loss: 0.9444  | val_0_rmse: 1.00369 | val_1_rmse: 0.88787 |  0:00:03s
epoch 41 | loss: 0.91672 | val_0_rmse: 0.99253 | val_1_rmse: 0.88196 |  0:00:03s
epoch 42 | loss: 0.91673 | val_0_rmse: 0.99567 | val_1_rmse: 0.88265 |  0:00:03s
epoch 43 | loss: 0.90202 | val_0_rmse: 1.00989 | val_1_rmse: 0.89896 |  0:00:03s
epoch 44 | loss: 0.90215 | val_0_rmse: 1.01837 | val_1_rmse: 0.91202 |  0:00:03s
epoch 45 | loss: 0.87677 | val_0_rmse: 1.0171  | val_1_rmse: 0.91663 |  0:00:03s
epoch 46 | loss: 0.88811 | val_0_rmse: 1.00889 | val_1_rmse: 0.91781 |  0:00:03s
epoch 47 | loss: 0.88694 | val_0_rmse: 1.00487 | val_1_rmse: 0.9206  |  0:00:03s
epoch 48 | loss: 0.86785 | val_0_rmse: 1.00726 | val_1_rmse: 0.93038 |  0:00:04s
epoch 49 | loss: 0.87728 | val_0_rmse: 1.01557 | val_1_rmse: 0.93293 |  0:00:04s
epoch 50 | loss: 0.84725 | val_0_rmse: 1.01366 | val_1_rmse: 0.92589 |  0:00:04s
epoch 51 | loss: 0.86728 | val_0_rmse: 1.00234 | val_1_rmse: 0.90786 |  0:00:04s
epoch 52 | loss: 0.85999 | val_0_rmse: 0.98604 | val_1_rmse: 0.89801 |  0:00:04s
epoch 53 | loss: 0.85452 | val_0_rmse: 0.98241 | val_1_rmse: 0.88388 |  0:00:04s
epoch 54 | loss: 0.85006 | val_0_rmse: 0.99049 | val_1_rmse: 0.90746 |  0:00:04s
epoch 55 | loss: 0.83144 | val_0_rmse: 1.00432 | val_1_rmse: 0.91871 |  0:00:04s
epoch 56 | loss: 0.82274 | val_0_rmse: 1.01331 | val_1_rmse: 0.93786 |  0:00:04s
epoch 57 | loss: 0.79761 | val_0_rmse: 1.00229 | val_1_rmse: 0.94848 |  0:00:04s
epoch 58 | loss: 0.80588 | val_0_rmse: 1.0024  | val_1_rmse: 0.95457 |  0:00:04s
epoch 59 | loss: 0.8266  | val_0_rmse: 1.00386 | val_1_rmse: 0.93544 |  0:00:04s
epoch 60 | loss: 0.79808 | val_0_rmse: 1.02421 | val_1_rmse: 0.95481 |  0:00:05s
epoch 61 | loss: 0.78966 | val_0_rmse: 1.01605 | val_1_rmse: 0.94776 |  0:00:05s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.8469
Best weights from best epoch are automatically used!
ended training at: 04:28:57
Feature importance:
Mean squared error is of 0.08989108885914657
Mean absolute error:0.19700757149115083
MAPE:0.2216744719261096
R2 score:0.01800288672332262
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:28:58
epoch 0  | loss: 4.75856 | val_0_rmse: 1.03356 | val_1_rmse: 0.95225 |  0:00:00s
epoch 1  | loss: 1.8208  | val_0_rmse: 1.01527 | val_1_rmse: 0.92635 |  0:00:00s
epoch 2  | loss: 1.88556 | val_0_rmse: 1.01733 | val_1_rmse: 0.92591 |  0:00:00s
epoch 3  | loss: 1.73114 | val_0_rmse: 1.01536 | val_1_rmse: 0.92486 |  0:00:00s
epoch 4  | loss: 1.44333 | val_0_rmse: 1.0154  | val_1_rmse: 0.93028 |  0:00:00s
epoch 5  | loss: 1.37106 | val_0_rmse: 1.01572 | val_1_rmse: 0.93193 |  0:00:00s
epoch 6  | loss: 1.33977 | val_0_rmse: 1.01437 | val_1_rmse: 0.93103 |  0:00:00s
epoch 7  | loss: 1.13271 | val_0_rmse: 1.01203 | val_1_rmse: 0.92919 |  0:00:00s
epoch 8  | loss: 1.26907 | val_0_rmse: 1.01044 | val_1_rmse: 0.92842 |  0:00:00s
epoch 9  | loss: 1.17002 | val_0_rmse: 1.00944 | val_1_rmse: 0.9284  |  0:00:00s
epoch 10 | loss: 1.07399 | val_0_rmse: 1.00982 | val_1_rmse: 0.92837 |  0:00:00s
epoch 11 | loss: 1.13199 | val_0_rmse: 1.01    | val_1_rmse: 0.92782 |  0:00:01s
epoch 12 | loss: 1.06326 | val_0_rmse: 1.00875 | val_1_rmse: 0.92856 |  0:00:01s
epoch 13 | loss: 1.10733 | val_0_rmse: 1.00803 | val_1_rmse: 0.93114 |  0:00:01s
epoch 14 | loss: 1.05707 | val_0_rmse: 1.00742 | val_1_rmse: 0.93288 |  0:00:01s
epoch 15 | loss: 1.06787 | val_0_rmse: 1.00809 | val_1_rmse: 0.93005 |  0:00:01s
epoch 16 | loss: 1.04515 | val_0_rmse: 1.0081  | val_1_rmse: 0.92853 |  0:00:01s
epoch 17 | loss: 1.03035 | val_0_rmse: 1.0088  | val_1_rmse: 0.92779 |  0:00:01s
epoch 18 | loss: 1.04551 | val_0_rmse: 1.00935 | val_1_rmse: 0.92705 |  0:00:01s
epoch 19 | loss: 1.02594 | val_0_rmse: 1.00981 | val_1_rmse: 0.92819 |  0:00:01s
epoch 20 | loss: 1.01492 | val_0_rmse: 1.01051 | val_1_rmse: 0.92811 |  0:00:01s
epoch 21 | loss: 1.01291 | val_0_rmse: 1.01136 | val_1_rmse: 0.92786 |  0:00:01s
epoch 22 | loss: 0.99544 | val_0_rmse: 1.01231 | val_1_rmse: 0.92772 |  0:00:01s
epoch 23 | loss: 1.00228 | val_0_rmse: 1.01326 | val_1_rmse: 0.92666 |  0:00:01s
epoch 24 | loss: 1.00317 | val_0_rmse: 1.0139  | val_1_rmse: 0.9268  |  0:00:02s
epoch 25 | loss: 0.98743 | val_0_rmse: 1.01393 | val_1_rmse: 0.92664 |  0:00:02s
epoch 26 | loss: 0.98417 | val_0_rmse: 1.01332 | val_1_rmse: 0.92574 |  0:00:02s
epoch 27 | loss: 0.96686 | val_0_rmse: 1.01105 | val_1_rmse: 0.92511 |  0:00:02s
epoch 28 | loss: 0.96591 | val_0_rmse: 1.00525 | val_1_rmse: 0.92995 |  0:00:02s
epoch 29 | loss: 0.97138 | val_0_rmse: 1.00327 | val_1_rmse: 0.93981 |  0:00:02s
epoch 30 | loss: 0.96024 | val_0_rmse: 1.00745 | val_1_rmse: 0.95084 |  0:00:02s
epoch 31 | loss: 0.95225 | val_0_rmse: 1.00833 | val_1_rmse: 0.95203 |  0:00:02s
epoch 32 | loss: 0.94911 | val_0_rmse: 1.00135 | val_1_rmse: 0.9514  |  0:00:02s
epoch 33 | loss: 0.94211 | val_0_rmse: 1.00011 | val_1_rmse: 0.96049 |  0:00:02s

Early stopping occured at epoch 33 with best_epoch = 3 and best_val_1_rmse = 0.92486
Best weights from best epoch are automatically used!
ended training at: 04:29:01
Feature importance:
Mean squared error is of 0.09245705580226889
Mean absolute error:0.2106836483326912
MAPE:0.22267121677398802
R2 score:-0.011297652359595567
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:29:01
epoch 0  | loss: 1.96191 | val_0_rmse: 1.00505 | val_1_rmse: 0.99497 |  0:00:00s
epoch 1  | loss: 1.14148 | val_0_rmse: 1.00182 | val_1_rmse: 0.99222 |  0:00:00s
epoch 2  | loss: 1.02993 | val_0_rmse: 1.00408 | val_1_rmse: 0.99467 |  0:00:00s
epoch 3  | loss: 1.01133 | val_0_rmse: 1.00324 | val_1_rmse: 0.99372 |  0:00:01s
epoch 4  | loss: 1.01721 | val_0_rmse: 1.00102 | val_1_rmse: 0.99259 |  0:00:01s
epoch 5  | loss: 0.99308 | val_0_rmse: 0.98733 | val_1_rmse: 0.98321 |  0:00:01s
epoch 6  | loss: 0.87646 | val_0_rmse: 1.05035 | val_1_rmse: 1.02216 |  0:00:02s
epoch 7  | loss: 0.86116 | val_0_rmse: 0.9259  | val_1_rmse: 0.92539 |  0:00:02s
epoch 8  | loss: 0.81877 | val_0_rmse: 0.94393 | val_1_rmse: 0.92903 |  0:00:02s
epoch 9  | loss: 0.79232 | val_0_rmse: 0.9986  | val_1_rmse: 0.98359 |  0:00:02s
epoch 10 | loss: 0.79059 | val_0_rmse: 0.89227 | val_1_rmse: 0.89814 |  0:00:03s
epoch 11 | loss: 0.78236 | val_0_rmse: 0.88485 | val_1_rmse: 0.87378 |  0:00:03s
epoch 12 | loss: 0.77969 | val_0_rmse: 0.88138 | val_1_rmse: 0.85695 |  0:00:03s
epoch 13 | loss: 0.77098 | val_0_rmse: 0.87067 | val_1_rmse: 0.84386 |  0:00:04s
epoch 14 | loss: 0.7689  | val_0_rmse: 0.85852 | val_1_rmse: 0.84676 |  0:00:04s
epoch 15 | loss: 0.74561 | val_0_rmse: 0.86377 | val_1_rmse: 0.87567 |  0:00:04s
epoch 16 | loss: 0.71504 | val_0_rmse: 0.91216 | val_1_rmse: 0.9235  |  0:00:05s
epoch 17 | loss: 0.70969 | val_0_rmse: 0.89875 | val_1_rmse: 0.91504 |  0:00:05s
epoch 18 | loss: 0.71143 | val_0_rmse: 0.88591 | val_1_rmse: 0.9044  |  0:00:05s
epoch 19 | loss: 0.72518 | val_0_rmse: 0.85717 | val_1_rmse: 0.87974 |  0:00:05s
epoch 20 | loss: 0.71098 | val_0_rmse: 0.88122 | val_1_rmse: 0.89603 |  0:00:06s
epoch 21 | loss: 0.70717 | val_0_rmse: 0.86703 | val_1_rmse: 0.8805  |  0:00:06s
epoch 22 | loss: 0.69544 | val_0_rmse: 0.88586 | val_1_rmse: 0.90413 |  0:00:06s
epoch 23 | loss: 0.67764 | val_0_rmse: 0.8368  | val_1_rmse: 0.85537 |  0:00:06s
epoch 24 | loss: 0.68051 | val_0_rmse: 0.85849 | val_1_rmse: 0.88392 |  0:00:07s
epoch 25 | loss: 0.67305 | val_0_rmse: 0.85222 | val_1_rmse: 0.87918 |  0:00:07s
epoch 26 | loss: 0.66752 | val_0_rmse: 0.88202 | val_1_rmse: 0.9148  |  0:00:07s
epoch 27 | loss: 0.6579  | val_0_rmse: 0.83976 | val_1_rmse: 0.87209 |  0:00:08s
epoch 28 | loss: 0.65236 | val_0_rmse: 0.84266 | val_1_rmse: 0.87798 |  0:00:08s
epoch 29 | loss: 0.65099 | val_0_rmse: 0.82562 | val_1_rmse: 0.86102 |  0:00:08s
epoch 30 | loss: 0.65836 | val_0_rmse: 0.81872 | val_1_rmse: 0.84599 |  0:00:08s
epoch 31 | loss: 0.65178 | val_0_rmse: 0.81614 | val_1_rmse: 0.83715 |  0:00:09s
epoch 32 | loss: 0.64799 | val_0_rmse: 0.81332 | val_1_rmse: 0.84666 |  0:00:09s
epoch 33 | loss: 0.63113 | val_0_rmse: 0.80528 | val_1_rmse: 0.82778 |  0:00:09s
epoch 34 | loss: 0.62718 | val_0_rmse: 0.81708 | val_1_rmse: 0.83337 |  0:00:10s
epoch 35 | loss: 0.62477 | val_0_rmse: 0.80188 | val_1_rmse: 0.82655 |  0:00:10s
epoch 36 | loss: 0.60859 | val_0_rmse: 0.79737 | val_1_rmse: 0.81766 |  0:00:10s
epoch 37 | loss: 0.61411 | val_0_rmse: 0.80015 | val_1_rmse: 0.8249  |  0:00:10s
epoch 38 | loss: 0.61387 | val_0_rmse: 0.81811 | val_1_rmse: 0.83405 |  0:00:11s
epoch 39 | loss: 0.61114 | val_0_rmse: 0.82517 | val_1_rmse: 0.85286 |  0:00:11s
epoch 40 | loss: 0.62518 | val_0_rmse: 0.80665 | val_1_rmse: 0.8221  |  0:00:11s
epoch 41 | loss: 0.61757 | val_0_rmse: 0.81228 | val_1_rmse: 0.83739 |  0:00:12s
epoch 42 | loss: 0.62508 | val_0_rmse: 0.81054 | val_1_rmse: 0.82502 |  0:00:12s
epoch 43 | loss: 0.59614 | val_0_rmse: 0.80308 | val_1_rmse: 0.8169  |  0:00:12s
epoch 44 | loss: 0.58345 | val_0_rmse: 0.81187 | val_1_rmse: 0.83654 |  0:00:12s
epoch 45 | loss: 0.58806 | val_0_rmse: 0.79736 | val_1_rmse: 0.81865 |  0:00:13s
epoch 46 | loss: 0.5866  | val_0_rmse: 0.79222 | val_1_rmse: 0.81956 |  0:00:13s
epoch 47 | loss: 0.57548 | val_0_rmse: 0.81285 | val_1_rmse: 0.83347 |  0:00:13s
epoch 48 | loss: 0.59669 | val_0_rmse: 0.79634 | val_1_rmse: 0.81773 |  0:00:14s
epoch 49 | loss: 0.5885  | val_0_rmse: 0.79616 | val_1_rmse: 0.82121 |  0:00:14s
epoch 50 | loss: 0.58836 | val_0_rmse: 0.84579 | val_1_rmse: 0.85154 |  0:00:14s
epoch 51 | loss: 0.597   | val_0_rmse: 0.805   | val_1_rmse: 0.82175 |  0:00:14s
epoch 52 | loss: 0.58947 | val_0_rmse: 0.79642 | val_1_rmse: 0.80659 |  0:00:15s
epoch 53 | loss: 0.58371 | val_0_rmse: 0.79776 | val_1_rmse: 0.81122 |  0:00:15s
epoch 54 | loss: 0.61229 | val_0_rmse: 0.78024 | val_1_rmse: 0.81016 |  0:00:15s
epoch 55 | loss: 0.61552 | val_0_rmse: 0.78055 | val_1_rmse: 0.81789 |  0:00:16s
epoch 56 | loss: 0.58968 | val_0_rmse: 0.79526 | val_1_rmse: 0.81639 |  0:00:16s
epoch 57 | loss: 0.58913 | val_0_rmse: 0.78164 | val_1_rmse: 0.80863 |  0:00:16s
epoch 58 | loss: 0.5814  | val_0_rmse: 0.79017 | val_1_rmse: 0.82515 |  0:00:16s
epoch 59 | loss: 0.59164 | val_0_rmse: 0.78024 | val_1_rmse: 0.81326 |  0:00:17s
epoch 60 | loss: 0.59306 | val_0_rmse: 0.78082 | val_1_rmse: 0.79914 |  0:00:17s
epoch 61 | loss: 0.587   | val_0_rmse: 0.78866 | val_1_rmse: 0.81136 |  0:00:17s
epoch 62 | loss: 0.57735 | val_0_rmse: 0.79223 | val_1_rmse: 0.81806 |  0:00:17s
epoch 63 | loss: 0.58129 | val_0_rmse: 0.78698 | val_1_rmse: 0.81865 |  0:00:18s
epoch 64 | loss: 0.6006  | val_0_rmse: 0.80944 | val_1_rmse: 0.86966 |  0:00:18s
epoch 65 | loss: 0.60778 | val_0_rmse: 0.78583 | val_1_rmse: 0.82059 |  0:00:18s
epoch 66 | loss: 0.63148 | val_0_rmse: 0.78687 | val_1_rmse: 0.83145 |  0:00:19s
epoch 67 | loss: 0.61862 | val_0_rmse: 0.79628 | val_1_rmse: 0.843   |  0:00:19s
epoch 68 | loss: 0.61589 | val_0_rmse: 0.83761 | val_1_rmse: 0.87515 |  0:00:19s
epoch 69 | loss: 0.61794 | val_0_rmse: 0.80206 | val_1_rmse: 0.84602 |  0:00:19s
epoch 70 | loss: 0.59798 | val_0_rmse: 0.79262 | val_1_rmse: 0.84034 |  0:00:20s
epoch 71 | loss: 0.58172 | val_0_rmse: 0.792   | val_1_rmse: 0.83855 |  0:00:20s
epoch 72 | loss: 0.57421 | val_0_rmse: 0.78654 | val_1_rmse: 0.82779 |  0:00:20s
epoch 73 | loss: 0.56219 | val_0_rmse: 0.77893 | val_1_rmse: 0.82906 |  0:00:21s
epoch 74 | loss: 0.56299 | val_0_rmse: 0.7907  | val_1_rmse: 0.82605 |  0:00:21s
epoch 75 | loss: 0.57051 | val_0_rmse: 0.79575 | val_1_rmse: 0.84291 |  0:00:21s
epoch 76 | loss: 0.54887 | val_0_rmse: 0.79468 | val_1_rmse: 0.84256 |  0:00:22s
epoch 77 | loss: 0.55686 | val_0_rmse: 0.77652 | val_1_rmse: 0.83028 |  0:00:22s
epoch 78 | loss: 0.56106 | val_0_rmse: 0.76858 | val_1_rmse: 0.82812 |  0:00:22s
epoch 79 | loss: 0.55665 | val_0_rmse: 0.76004 | val_1_rmse: 0.8099  |  0:00:22s
epoch 80 | loss: 0.54302 | val_0_rmse: 0.76262 | val_1_rmse: 0.81157 |  0:00:23s
epoch 81 | loss: 0.54285 | val_0_rmse: 0.75131 | val_1_rmse: 0.81117 |  0:00:23s
epoch 82 | loss: 0.55148 | val_0_rmse: 0.74465 | val_1_rmse: 0.80979 |  0:00:23s
epoch 83 | loss: 0.53991 | val_0_rmse: 0.75672 | val_1_rmse: 0.82727 |  0:00:24s
epoch 84 | loss: 0.53912 | val_0_rmse: 0.74281 | val_1_rmse: 0.8057  |  0:00:24s
epoch 85 | loss: 0.53118 | val_0_rmse: 0.74059 | val_1_rmse: 0.80324 |  0:00:24s
epoch 86 | loss: 0.51957 | val_0_rmse: 0.74501 | val_1_rmse: 0.81946 |  0:00:24s
epoch 87 | loss: 0.52584 | val_0_rmse: 0.73536 | val_1_rmse: 0.81572 |  0:00:25s
epoch 88 | loss: 0.51592 | val_0_rmse: 0.74528 | val_1_rmse: 0.81857 |  0:00:25s
epoch 89 | loss: 0.52721 | val_0_rmse: 0.73519 | val_1_rmse: 0.81697 |  0:00:25s
epoch 90 | loss: 0.52978 | val_0_rmse: 0.74338 | val_1_rmse: 0.8234  |  0:00:26s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.79914
Best weights from best epoch are automatically used!
ended training at: 04:29:27
Feature importance:
Mean squared error is of 0.052010018288706224
Mean absolute error:0.16723231015962636
MAPE:0.18782947194596503
R2 score:0.38227960271277905
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:29:28
epoch 0  | loss: 1.8123  | val_0_rmse: 1.0118  | val_1_rmse: 0.93512 |  0:00:00s
epoch 1  | loss: 1.1337  | val_0_rmse: 0.99368 | val_1_rmse: 0.91568 |  0:00:00s
epoch 2  | loss: 1.02191 | val_0_rmse: 0.99431 | val_1_rmse: 0.92843 |  0:00:00s
epoch 3  | loss: 0.98625 | val_0_rmse: 0.98711 | val_1_rmse: 0.91762 |  0:00:01s
epoch 4  | loss: 0.96498 | val_0_rmse: 0.9018  | val_1_rmse: 0.84827 |  0:00:01s
epoch 5  | loss: 0.86255 | val_0_rmse: 0.94749 | val_1_rmse: 0.91826 |  0:00:01s
epoch 6  | loss: 0.8532  | val_0_rmse: 0.89906 | val_1_rmse: 0.87224 |  0:00:01s
epoch 7  | loss: 0.81952 | val_0_rmse: 0.94656 | val_1_rmse: 0.94192 |  0:00:02s
epoch 8  | loss: 0.79147 | val_0_rmse: 0.95856 | val_1_rmse: 0.94789 |  0:00:02s
epoch 9  | loss: 0.78517 | val_0_rmse: 0.89406 | val_1_rmse: 0.87312 |  0:00:02s
epoch 10 | loss: 0.77365 | val_0_rmse: 0.90477 | val_1_rmse: 0.88579 |  0:00:03s
epoch 11 | loss: 0.76192 | val_0_rmse: 0.87706 | val_1_rmse: 0.85979 |  0:00:03s
epoch 12 | loss: 0.75867 | val_0_rmse: 0.85799 | val_1_rmse: 0.82448 |  0:00:03s
epoch 13 | loss: 0.7454  | val_0_rmse: 0.88095 | val_1_rmse: 0.85676 |  0:00:03s
epoch 14 | loss: 0.74423 | val_0_rmse: 0.86952 | val_1_rmse: 0.84176 |  0:00:04s
epoch 15 | loss: 0.73666 | val_0_rmse: 0.85474 | val_1_rmse: 0.81205 |  0:00:04s
epoch 16 | loss: 0.72395 | val_0_rmse: 0.8742  | val_1_rmse: 0.84294 |  0:00:04s
epoch 17 | loss: 0.71585 | val_0_rmse: 0.96869 | val_1_rmse: 0.94831 |  0:00:05s
epoch 18 | loss: 0.7144  | val_0_rmse: 0.85232 | val_1_rmse: 0.82997 |  0:00:05s
epoch 19 | loss: 0.70263 | val_0_rmse: 0.83635 | val_1_rmse: 0.81106 |  0:00:05s
epoch 20 | loss: 0.70573 | val_0_rmse: 0.83403 | val_1_rmse: 0.80125 |  0:00:05s
epoch 21 | loss: 0.70245 | val_0_rmse: 0.82757 | val_1_rmse: 0.79467 |  0:00:06s
epoch 22 | loss: 0.67931 | val_0_rmse: 0.82087 | val_1_rmse: 0.79092 |  0:00:06s
epoch 23 | loss: 0.67029 | val_0_rmse: 0.82685 | val_1_rmse: 0.7938  |  0:00:06s
epoch 24 | loss: 0.66926 | val_0_rmse: 0.82139 | val_1_rmse: 0.78716 |  0:00:07s
epoch 25 | loss: 0.67632 | val_0_rmse: 0.81874 | val_1_rmse: 0.78486 |  0:00:07s
epoch 26 | loss: 0.6627  | val_0_rmse: 0.81399 | val_1_rmse: 0.7905  |  0:00:07s
epoch 27 | loss: 0.64634 | val_0_rmse: 0.80906 | val_1_rmse: 0.78276 |  0:00:07s
epoch 28 | loss: 0.6443  | val_0_rmse: 0.81535 | val_1_rmse: 0.79403 |  0:00:08s
epoch 29 | loss: 0.65687 | val_0_rmse: 0.81329 | val_1_rmse: 0.78192 |  0:00:08s
epoch 30 | loss: 0.63504 | val_0_rmse: 0.81327 | val_1_rmse: 0.78947 |  0:00:08s
epoch 31 | loss: 0.63336 | val_0_rmse: 0.80279 | val_1_rmse: 0.77679 |  0:00:09s
epoch 32 | loss: 0.62134 | val_0_rmse: 0.81013 | val_1_rmse: 0.76976 |  0:00:09s
epoch 33 | loss: 0.62113 | val_0_rmse: 0.80887 | val_1_rmse: 0.78157 |  0:00:09s
epoch 34 | loss: 0.61634 | val_0_rmse: 0.81381 | val_1_rmse: 0.79426 |  0:00:10s
epoch 35 | loss: 0.61651 | val_0_rmse: 0.80599 | val_1_rmse: 0.78526 |  0:00:10s
epoch 36 | loss: 0.61858 | val_0_rmse: 0.80045 | val_1_rmse: 0.78049 |  0:00:10s
epoch 37 | loss: 0.62086 | val_0_rmse: 0.80328 | val_1_rmse: 0.78272 |  0:00:10s
epoch 38 | loss: 0.60332 | val_0_rmse: 0.80445 | val_1_rmse: 0.7802  |  0:00:11s
epoch 39 | loss: 0.60532 | val_0_rmse: 0.80438 | val_1_rmse: 0.78588 |  0:00:11s
epoch 40 | loss: 0.61609 | val_0_rmse: 0.79003 | val_1_rmse: 0.78237 |  0:00:11s
epoch 41 | loss: 0.60607 | val_0_rmse: 0.78748 | val_1_rmse: 0.7778  |  0:00:12s
epoch 42 | loss: 0.60261 | val_0_rmse: 0.8005  | val_1_rmse: 0.80826 |  0:00:12s
epoch 43 | loss: 0.60317 | val_0_rmse: 0.78918 | val_1_rmse: 0.78783 |  0:00:12s
epoch 44 | loss: 0.59815 | val_0_rmse: 0.82498 | val_1_rmse: 0.79527 |  0:00:12s
epoch 45 | loss: 0.5943  | val_0_rmse: 0.78841 | val_1_rmse: 0.77832 |  0:00:13s
epoch 46 | loss: 0.59199 | val_0_rmse: 0.79551 | val_1_rmse: 0.78553 |  0:00:13s
epoch 47 | loss: 0.58621 | val_0_rmse: 0.79944 | val_1_rmse: 0.7816  |  0:00:13s
epoch 48 | loss: 0.59047 | val_0_rmse: 0.8038  | val_1_rmse: 0.78593 |  0:00:14s
epoch 49 | loss: 0.60458 | val_0_rmse: 0.79987 | val_1_rmse: 0.78788 |  0:00:14s
epoch 50 | loss: 0.61516 | val_0_rmse: 0.80607 | val_1_rmse: 0.80159 |  0:00:14s
epoch 51 | loss: 0.61201 | val_0_rmse: 0.80517 | val_1_rmse: 0.78655 |  0:00:14s
epoch 52 | loss: 0.62514 | val_0_rmse: 0.81752 | val_1_rmse: 0.79363 |  0:00:15s
epoch 53 | loss: 0.63351 | val_0_rmse: 0.81083 | val_1_rmse: 0.78869 |  0:00:15s
epoch 54 | loss: 0.61638 | val_0_rmse: 0.80949 | val_1_rmse: 0.80203 |  0:00:15s
epoch 55 | loss: 0.60658 | val_0_rmse: 0.80587 | val_1_rmse: 0.79209 |  0:00:15s
epoch 56 | loss: 0.60947 | val_0_rmse: 0.80754 | val_1_rmse: 0.79933 |  0:00:16s
epoch 57 | loss: 0.60399 | val_0_rmse: 0.79422 | val_1_rmse: 0.77907 |  0:00:16s
epoch 58 | loss: 0.60117 | val_0_rmse: 0.79702 | val_1_rmse: 0.78606 |  0:00:16s
epoch 59 | loss: 0.59309 | val_0_rmse: 0.80102 | val_1_rmse: 0.79634 |  0:00:17s
epoch 60 | loss: 0.60082 | val_0_rmse: 0.79739 | val_1_rmse: 0.79372 |  0:00:17s
epoch 61 | loss: 0.5931  | val_0_rmse: 0.79351 | val_1_rmse: 0.78518 |  0:00:17s
epoch 62 | loss: 0.59185 | val_0_rmse: 0.78447 | val_1_rmse: 0.77018 |  0:00:17s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.76976
Best weights from best epoch are automatically used!
ended training at: 04:29:46
Feature importance:
Mean squared error is of 0.06349717426526806
Mean absolute error:0.17977517660804263
MAPE:0.19718133607137833
R2 score:0.35438143828985713
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:29:46
epoch 0  | loss: 2.04093 | val_0_rmse: 1.01774 | val_1_rmse: 1.01882 |  0:00:00s
epoch 1  | loss: 1.22417 | val_0_rmse: 1.00874 | val_1_rmse: 1.01356 |  0:00:00s
epoch 2  | loss: 1.09078 | val_0_rmse: 1.0079  | val_1_rmse: 1.01299 |  0:00:00s
epoch 3  | loss: 1.0688  | val_0_rmse: 1.00762 | val_1_rmse: 1.01284 |  0:00:01s
epoch 4  | loss: 1.02763 | val_0_rmse: 1.00669 | val_1_rmse: 1.01297 |  0:00:01s
epoch 5  | loss: 1.02871 | val_0_rmse: 1.0066  | val_1_rmse: 1.01326 |  0:00:01s
epoch 6  | loss: 1.02613 | val_0_rmse: 1.00606 | val_1_rmse: 1.01387 |  0:00:01s
epoch 7  | loss: 1.02752 | val_0_rmse: 1.00547 | val_1_rmse: 1.01197 |  0:00:02s
epoch 8  | loss: 1.01367 | val_0_rmse: 1.00185 | val_1_rmse: 1.0077  |  0:00:02s
epoch 9  | loss: 1.00715 | val_0_rmse: 0.99584 | val_1_rmse: 1.00336 |  0:00:02s
epoch 10 | loss: 1.00155 | val_0_rmse: 0.98823 | val_1_rmse: 0.99746 |  0:00:03s
epoch 11 | loss: 0.9846  | val_0_rmse: 0.97185 | val_1_rmse: 0.987   |  0:00:03s
epoch 12 | loss: 0.96862 | val_0_rmse: 0.94536 | val_1_rmse: 0.97732 |  0:00:03s
epoch 13 | loss: 0.94902 | val_0_rmse: 0.94272 | val_1_rmse: 0.95447 |  0:00:04s
epoch 14 | loss: 0.94586 | val_0_rmse: 0.95143 | val_1_rmse: 0.95479 |  0:00:04s
epoch 15 | loss: 0.90633 | val_0_rmse: 0.91647 | val_1_rmse: 0.93137 |  0:00:04s
epoch 16 | loss: 0.89145 | val_0_rmse: 0.91486 | val_1_rmse: 0.94469 |  0:00:04s
epoch 17 | loss: 0.88095 | val_0_rmse: 0.91864 | val_1_rmse: 0.95675 |  0:00:05s
epoch 18 | loss: 0.84382 | val_0_rmse: 0.89022 | val_1_rmse: 0.92038 |  0:00:05s
epoch 19 | loss: 0.83189 | val_0_rmse: 0.89744 | val_1_rmse: 0.94444 |  0:00:05s
epoch 20 | loss: 0.80586 | val_0_rmse: 0.87122 | val_1_rmse: 0.91447 |  0:00:06s
epoch 21 | loss: 0.78173 | val_0_rmse: 0.85588 | val_1_rmse: 0.89898 |  0:00:06s
epoch 22 | loss: 0.75367 | val_0_rmse: 0.85681 | val_1_rmse: 0.91898 |  0:00:06s
epoch 23 | loss: 0.72898 | val_0_rmse: 0.84441 | val_1_rmse: 0.88436 |  0:00:07s
epoch 24 | loss: 0.71208 | val_0_rmse: 0.8464  | val_1_rmse: 0.8873  |  0:00:07s
epoch 25 | loss: 0.69006 | val_0_rmse: 0.83777 | val_1_rmse: 0.88067 |  0:00:07s
epoch 26 | loss: 0.68078 | val_0_rmse: 0.81463 | val_1_rmse: 0.86833 |  0:00:07s
epoch 27 | loss: 0.66719 | val_0_rmse: 0.81031 | val_1_rmse: 0.86694 |  0:00:08s
epoch 28 | loss: 0.65272 | val_0_rmse: 0.80159 | val_1_rmse: 0.86053 |  0:00:08s
epoch 29 | loss: 0.64129 | val_0_rmse: 0.80858 | val_1_rmse: 0.87812 |  0:00:08s
epoch 30 | loss: 0.63548 | val_0_rmse: 0.82869 | val_1_rmse: 0.89529 |  0:00:09s
epoch 31 | loss: 0.61769 | val_0_rmse: 0.82233 | val_1_rmse: 0.89029 |  0:00:09s
epoch 32 | loss: 0.61693 | val_0_rmse: 0.80712 | val_1_rmse: 0.87594 |  0:00:09s
epoch 33 | loss: 0.61094 | val_0_rmse: 0.78962 | val_1_rmse: 0.85283 |  0:00:09s
epoch 34 | loss: 0.59844 | val_0_rmse: 0.7898  | val_1_rmse: 0.84458 |  0:00:10s
epoch 35 | loss: 0.58144 | val_0_rmse: 0.78821 | val_1_rmse: 0.84599 |  0:00:10s
epoch 36 | loss: 0.59025 | val_0_rmse: 0.78511 | val_1_rmse: 0.8558  |  0:00:10s
epoch 37 | loss: 0.57672 | val_0_rmse: 0.78098 | val_1_rmse: 0.84871 |  0:00:10s
epoch 38 | loss: 0.57457 | val_0_rmse: 0.77847 | val_1_rmse: 0.84275 |  0:00:11s
epoch 39 | loss: 0.5666  | val_0_rmse: 0.77487 | val_1_rmse: 0.83645 |  0:00:11s
epoch 40 | loss: 0.56548 | val_0_rmse: 0.78715 | val_1_rmse: 0.85081 |  0:00:11s
epoch 41 | loss: 0.57537 | val_0_rmse: 0.78308 | val_1_rmse: 0.84837 |  0:00:12s
epoch 42 | loss: 0.56869 | val_0_rmse: 0.77645 | val_1_rmse: 0.83709 |  0:00:12s
epoch 43 | loss: 0.56404 | val_0_rmse: 0.77108 | val_1_rmse: 0.83    |  0:00:12s
epoch 44 | loss: 0.56235 | val_0_rmse: 0.77395 | val_1_rmse: 0.84021 |  0:00:12s
epoch 45 | loss: 0.55652 | val_0_rmse: 0.77058 | val_1_rmse: 0.83149 |  0:00:13s
epoch 46 | loss: 0.55947 | val_0_rmse: 0.77128 | val_1_rmse: 0.83136 |  0:00:13s
epoch 47 | loss: 0.55604 | val_0_rmse: 0.77778 | val_1_rmse: 0.84893 |  0:00:13s
epoch 48 | loss: 0.56588 | val_0_rmse: 0.78053 | val_1_rmse: 0.84827 |  0:00:14s
epoch 49 | loss: 0.55976 | val_0_rmse: 0.78549 | val_1_rmse: 0.85407 |  0:00:14s
epoch 50 | loss: 0.56304 | val_0_rmse: 0.76968 | val_1_rmse: 0.823   |  0:00:14s
epoch 51 | loss: 0.55886 | val_0_rmse: 0.77298 | val_1_rmse: 0.8271  |  0:00:14s
epoch 52 | loss: 0.54546 | val_0_rmse: 0.76835 | val_1_rmse: 0.82535 |  0:00:15s
epoch 53 | loss: 0.55246 | val_0_rmse: 0.76727 | val_1_rmse: 0.81318 |  0:00:15s
epoch 54 | loss: 0.55811 | val_0_rmse: 0.76318 | val_1_rmse: 0.81222 |  0:00:15s
epoch 55 | loss: 0.54697 | val_0_rmse: 0.80028 | val_1_rmse: 0.87756 |  0:00:16s
epoch 56 | loss: 0.54181 | val_0_rmse: 0.76666 | val_1_rmse: 0.84175 |  0:00:16s
epoch 57 | loss: 0.53945 | val_0_rmse: 0.75291 | val_1_rmse: 0.83122 |  0:00:16s
epoch 58 | loss: 0.54056 | val_0_rmse: 0.75012 | val_1_rmse: 0.8215  |  0:00:16s
epoch 59 | loss: 0.53219 | val_0_rmse: 0.75329 | val_1_rmse: 0.82763 |  0:00:17s
epoch 60 | loss: 0.5316  | val_0_rmse: 0.75853 | val_1_rmse: 0.8237  |  0:00:17s
epoch 61 | loss: 0.53556 | val_0_rmse: 0.74901 | val_1_rmse: 0.82283 |  0:00:17s
epoch 62 | loss: 0.53426 | val_0_rmse: 0.7465  | val_1_rmse: 0.81645 |  0:00:18s
epoch 63 | loss: 0.52639 | val_0_rmse: 0.74915 | val_1_rmse: 0.82735 |  0:00:18s
epoch 64 | loss: 0.52176 | val_0_rmse: 0.75183 | val_1_rmse: 0.82126 |  0:00:18s
epoch 65 | loss: 0.50754 | val_0_rmse: 0.74276 | val_1_rmse: 0.82287 |  0:00:18s
epoch 66 | loss: 0.51471 | val_0_rmse: 0.74599 | val_1_rmse: 0.83249 |  0:00:19s
epoch 67 | loss: 0.52901 | val_0_rmse: 0.74106 | val_1_rmse: 0.82252 |  0:00:19s
epoch 68 | loss: 0.50701 | val_0_rmse: 0.74431 | val_1_rmse: 0.83378 |  0:00:19s
epoch 69 | loss: 0.5078  | val_0_rmse: 0.73553 | val_1_rmse: 0.82923 |  0:00:19s
epoch 70 | loss: 0.51227 | val_0_rmse: 0.76015 | val_1_rmse: 0.82336 |  0:00:20s
epoch 71 | loss: 0.51762 | val_0_rmse: 0.73997 | val_1_rmse: 0.82165 |  0:00:20s
epoch 72 | loss: 0.50112 | val_0_rmse: 0.73597 | val_1_rmse: 0.83385 |  0:00:20s
epoch 73 | loss: 0.50181 | val_0_rmse: 0.72881 | val_1_rmse: 0.81491 |  0:00:21s
epoch 74 | loss: 0.49405 | val_0_rmse: 0.72902 | val_1_rmse: 0.82562 |  0:00:21s
epoch 75 | loss: 0.49246 | val_0_rmse: 0.73214 | val_1_rmse: 0.84202 |  0:00:21s
epoch 76 | loss: 0.48688 | val_0_rmse: 0.72017 | val_1_rmse: 0.82796 |  0:00:21s
epoch 77 | loss: 0.49266 | val_0_rmse: 0.72986 | val_1_rmse: 0.83258 |  0:00:22s
epoch 78 | loss: 0.50715 | val_0_rmse: 0.74387 | val_1_rmse: 0.85033 |  0:00:22s
epoch 79 | loss: 0.50016 | val_0_rmse: 0.72629 | val_1_rmse: 0.83331 |  0:00:22s
epoch 80 | loss: 0.5007  | val_0_rmse: 0.73272 | val_1_rmse: 0.84519 |  0:00:23s
epoch 81 | loss: 0.50199 | val_0_rmse: 0.74655 | val_1_rmse: 0.85673 |  0:00:23s
epoch 82 | loss: 0.50512 | val_0_rmse: 0.7295  | val_1_rmse: 0.82352 |  0:00:23s
epoch 83 | loss: 0.49595 | val_0_rmse: 0.71605 | val_1_rmse: 0.83306 |  0:00:23s
epoch 84 | loss: 0.49009 | val_0_rmse: 0.70725 | val_1_rmse: 0.82714 |  0:00:24s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.81222
Best weights from best epoch are automatically used!
ended training at: 04:30:11
Feature importance:
Mean squared error is of 0.046606290156516746
Mean absolute error:0.16300931835704463
MAPE:0.18731420630956058
R2 score:0.38313583538774987
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:30:11
epoch 0  | loss: 2.09265 | val_0_rmse: 1.00584 | val_1_rmse: 1.01124 |  0:00:00s
epoch 1  | loss: 1.12168 | val_0_rmse: 0.97784 | val_1_rmse: 0.98281 |  0:00:00s
epoch 2  | loss: 0.97036 | val_0_rmse: 0.93017 | val_1_rmse: 0.93175 |  0:00:00s
epoch 3  | loss: 0.85397 | val_0_rmse: 0.8913  | val_1_rmse: 0.87492 |  0:00:01s
epoch 4  | loss: 0.79669 | val_0_rmse: 0.98948 | val_1_rmse: 0.95501 |  0:00:01s
epoch 5  | loss: 0.80306 | val_0_rmse: 0.91804 | val_1_rmse: 0.89323 |  0:00:01s
epoch 6  | loss: 0.77103 | val_0_rmse: 0.90871 | val_1_rmse: 0.88059 |  0:00:01s
epoch 7  | loss: 0.78086 | val_0_rmse: 0.8911  | val_1_rmse: 0.87195 |  0:00:02s
epoch 8  | loss: 0.77637 | val_0_rmse: 0.88228 | val_1_rmse: 0.85887 |  0:00:02s
epoch 9  | loss: 0.76434 | val_0_rmse: 0.91894 | val_1_rmse: 0.88239 |  0:00:02s
epoch 10 | loss: 0.75045 | val_0_rmse: 0.95325 | val_1_rmse: 0.91398 |  0:00:03s
epoch 11 | loss: 0.74885 | val_0_rmse: 0.88153 | val_1_rmse: 0.85866 |  0:00:03s
epoch 12 | loss: 0.73955 | val_0_rmse: 0.91042 | val_1_rmse: 0.87309 |  0:00:03s
epoch 13 | loss: 0.74314 | val_0_rmse: 0.88645 | val_1_rmse: 0.85996 |  0:00:03s
epoch 14 | loss: 0.74333 | val_0_rmse: 0.87275 | val_1_rmse: 0.85369 |  0:00:04s
epoch 15 | loss: 0.72865 | val_0_rmse: 0.86754 | val_1_rmse: 0.84035 |  0:00:04s
epoch 16 | loss: 0.72298 | val_0_rmse: 0.85833 | val_1_rmse: 0.83948 |  0:00:04s
epoch 17 | loss: 0.71579 | val_0_rmse: 0.8566  | val_1_rmse: 0.84487 |  0:00:05s
epoch 18 | loss: 0.70546 | val_0_rmse: 0.85864 | val_1_rmse: 0.84701 |  0:00:05s
epoch 19 | loss: 0.7083  | val_0_rmse: 0.8525  | val_1_rmse: 0.83838 |  0:00:05s
epoch 20 | loss: 0.69829 | val_0_rmse: 0.85618 | val_1_rmse: 0.848   |  0:00:05s
epoch 21 | loss: 0.6873  | val_0_rmse: 0.85728 | val_1_rmse: 0.84787 |  0:00:06s
epoch 22 | loss: 0.69238 | val_0_rmse: 0.85412 | val_1_rmse: 0.8475  |  0:00:06s
epoch 23 | loss: 0.68412 | val_0_rmse: 0.85335 | val_1_rmse: 0.84978 |  0:00:06s
epoch 24 | loss: 0.6915  | val_0_rmse: 0.83487 | val_1_rmse: 0.83165 |  0:00:07s
epoch 25 | loss: 0.6879  | val_0_rmse: 0.83326 | val_1_rmse: 0.83347 |  0:00:07s
epoch 26 | loss: 0.68659 | val_0_rmse: 0.84056 | val_1_rmse: 0.84041 |  0:00:07s
epoch 27 | loss: 0.68182 | val_0_rmse: 0.8412  | val_1_rmse: 0.83667 |  0:00:07s
epoch 28 | loss: 0.67946 | val_0_rmse: 0.84535 | val_1_rmse: 0.83848 |  0:00:08s
epoch 29 | loss: 0.67509 | val_0_rmse: 0.83953 | val_1_rmse: 0.83083 |  0:00:08s
epoch 30 | loss: 0.67158 | val_0_rmse: 0.83751 | val_1_rmse: 0.82744 |  0:00:08s
epoch 31 | loss: 0.65684 | val_0_rmse: 0.83298 | val_1_rmse: 0.82106 |  0:00:09s
epoch 32 | loss: 0.66055 | val_0_rmse: 0.84334 | val_1_rmse: 0.82969 |  0:00:09s
epoch 33 | loss: 0.64242 | val_0_rmse: 0.85899 | val_1_rmse: 0.84327 |  0:00:09s
epoch 34 | loss: 0.64789 | val_0_rmse: 0.8459  | val_1_rmse: 0.82692 |  0:00:09s
epoch 35 | loss: 0.64615 | val_0_rmse: 0.84205 | val_1_rmse: 0.82675 |  0:00:10s
epoch 36 | loss: 0.65875 | val_0_rmse: 0.84867 | val_1_rmse: 0.82694 |  0:00:10s
epoch 37 | loss: 0.65903 | val_0_rmse: 0.86254 | val_1_rmse: 0.83985 |  0:00:10s
epoch 38 | loss: 0.65866 | val_0_rmse: 0.85609 | val_1_rmse: 0.84257 |  0:00:11s
epoch 39 | loss: 0.64802 | val_0_rmse: 0.81654 | val_1_rmse: 0.8037  |  0:00:11s
epoch 40 | loss: 0.63619 | val_0_rmse: 0.84948 | val_1_rmse: 0.83564 |  0:00:11s
epoch 41 | loss: 0.64472 | val_0_rmse: 0.86432 | val_1_rmse: 0.84808 |  0:00:11s
epoch 42 | loss: 0.67341 | val_0_rmse: 0.82681 | val_1_rmse: 0.81336 |  0:00:12s
epoch 43 | loss: 0.65734 | val_0_rmse: 0.84054 | val_1_rmse: 0.82442 |  0:00:12s
epoch 44 | loss: 0.65292 | val_0_rmse: 0.83155 | val_1_rmse: 0.81629 |  0:00:12s
epoch 45 | loss: 0.6551  | val_0_rmse: 0.8248  | val_1_rmse: 0.81356 |  0:00:13s
epoch 46 | loss: 0.65143 | val_0_rmse: 0.82183 | val_1_rmse: 0.81024 |  0:00:13s
epoch 47 | loss: 0.62873 | val_0_rmse: 0.81642 | val_1_rmse: 0.80427 |  0:00:13s
epoch 48 | loss: 0.6454  | val_0_rmse: 0.82287 | val_1_rmse: 0.81354 |  0:00:13s
epoch 49 | loss: 0.64438 | val_0_rmse: 0.83071 | val_1_rmse: 0.81785 |  0:00:14s
epoch 50 | loss: 0.63449 | val_0_rmse: 0.81687 | val_1_rmse: 0.80752 |  0:00:14s
epoch 51 | loss: 0.63882 | val_0_rmse: 0.81196 | val_1_rmse: 0.80027 |  0:00:14s
epoch 52 | loss: 0.63363 | val_0_rmse: 0.82008 | val_1_rmse: 0.81264 |  0:00:15s
epoch 53 | loss: 0.63362 | val_0_rmse: 0.86403 | val_1_rmse: 0.86022 |  0:00:15s
epoch 54 | loss: 0.6469  | val_0_rmse: 0.83862 | val_1_rmse: 0.844   |  0:00:15s
epoch 55 | loss: 0.65135 | val_0_rmse: 0.82361 | val_1_rmse: 0.82991 |  0:00:15s
epoch 56 | loss: 0.65224 | val_0_rmse: 0.85895 | val_1_rmse: 0.86425 |  0:00:16s
epoch 57 | loss: 0.64756 | val_0_rmse: 0.85754 | val_1_rmse: 0.86252 |  0:00:16s
epoch 58 | loss: 0.64069 | val_0_rmse: 0.90292 | val_1_rmse: 0.88597 |  0:00:16s
epoch 59 | loss: 0.65788 | val_0_rmse: 0.87117 | val_1_rmse: 0.85905 |  0:00:17s
epoch 60 | loss: 0.65163 | val_0_rmse: 0.90637 | val_1_rmse: 0.90029 |  0:00:17s
epoch 61 | loss: 0.6571  | val_0_rmse: 0.91335 | val_1_rmse: 0.89611 |  0:00:17s
epoch 62 | loss: 0.66641 | val_0_rmse: 0.8688  | val_1_rmse: 0.8616  |  0:00:17s
epoch 63 | loss: 0.65297 | val_0_rmse: 0.83139 | val_1_rmse: 0.81327 |  0:00:18s
epoch 64 | loss: 0.65888 | val_0_rmse: 0.82205 | val_1_rmse: 0.79587 |  0:00:18s
epoch 65 | loss: 0.6518  | val_0_rmse: 0.8382  | val_1_rmse: 0.80554 |  0:00:18s
epoch 66 | loss: 0.65502 | val_0_rmse: 0.82342 | val_1_rmse: 0.79017 |  0:00:19s
epoch 67 | loss: 0.66228 | val_0_rmse: 0.82158 | val_1_rmse: 0.79381 |  0:00:19s
epoch 68 | loss: 0.64019 | val_0_rmse: 0.81401 | val_1_rmse: 0.80324 |  0:00:19s
epoch 69 | loss: 0.63674 | val_0_rmse: 0.81938 | val_1_rmse: 0.8133  |  0:00:19s
epoch 70 | loss: 0.63953 | val_0_rmse: 0.81733 | val_1_rmse: 0.81269 |  0:00:20s
epoch 71 | loss: 0.63175 | val_0_rmse: 0.80536 | val_1_rmse: 0.79402 |  0:00:20s
epoch 72 | loss: 0.62246 | val_0_rmse: 0.81093 | val_1_rmse: 0.80566 |  0:00:20s
epoch 73 | loss: 0.62476 | val_0_rmse: 0.8048  | val_1_rmse: 0.79697 |  0:00:21s
epoch 74 | loss: 0.62445 | val_0_rmse: 0.79773 | val_1_rmse: 0.78927 |  0:00:21s
epoch 75 | loss: 0.6322  | val_0_rmse: 0.80215 | val_1_rmse: 0.79696 |  0:00:21s
epoch 76 | loss: 0.61564 | val_0_rmse: 0.82546 | val_1_rmse: 0.81987 |  0:00:21s
epoch 77 | loss: 0.61676 | val_0_rmse: 0.80967 | val_1_rmse: 0.81308 |  0:00:22s
epoch 78 | loss: 0.62747 | val_0_rmse: 0.81209 | val_1_rmse: 0.81553 |  0:00:22s
epoch 79 | loss: 0.61286 | val_0_rmse: 0.80579 | val_1_rmse: 0.8048  |  0:00:22s
epoch 80 | loss: 0.61311 | val_0_rmse: 0.80674 | val_1_rmse: 0.80621 |  0:00:23s
epoch 81 | loss: 0.61207 | val_0_rmse: 0.80741 | val_1_rmse: 0.79926 |  0:00:23s
epoch 82 | loss: 0.61138 | val_0_rmse: 0.80336 | val_1_rmse: 0.79361 |  0:00:23s
epoch 83 | loss: 0.60797 | val_0_rmse: 0.8309  | val_1_rmse: 0.80786 |  0:00:23s
epoch 84 | loss: 0.60934 | val_0_rmse: 0.83476 | val_1_rmse: 0.81582 |  0:00:24s
epoch 85 | loss: 0.61206 | val_0_rmse: 0.79135 | val_1_rmse: 0.78855 |  0:00:24s
epoch 86 | loss: 0.60379 | val_0_rmse: 0.79991 | val_1_rmse: 0.79663 |  0:00:24s
epoch 87 | loss: 0.59943 | val_0_rmse: 0.83856 | val_1_rmse: 0.8278  |  0:00:25s
epoch 88 | loss: 0.61029 | val_0_rmse: 0.83761 | val_1_rmse: 0.82194 |  0:00:25s
epoch 89 | loss: 0.60984 | val_0_rmse: 0.80127 | val_1_rmse: 0.79985 |  0:00:25s
epoch 90 | loss: 0.61655 | val_0_rmse: 0.83938 | val_1_rmse: 0.83108 |  0:00:25s
epoch 91 | loss: 0.64334 | val_0_rmse: 0.85871 | val_1_rmse: 0.84331 |  0:00:26s
epoch 92 | loss: 0.65311 | val_0_rmse: 0.80423 | val_1_rmse: 0.81044 |  0:00:26s
epoch 93 | loss: 0.64015 | val_0_rmse: 0.80247 | val_1_rmse: 0.81076 |  0:00:26s
epoch 94 | loss: 0.62218 | val_0_rmse: 0.7921  | val_1_rmse: 0.79875 |  0:00:26s
epoch 95 | loss: 0.61364 | val_0_rmse: 0.79642 | val_1_rmse: 0.81128 |  0:00:27s
epoch 96 | loss: 0.6112  | val_0_rmse: 0.78458 | val_1_rmse: 0.8088  |  0:00:27s
epoch 97 | loss: 0.60508 | val_0_rmse: 0.78085 | val_1_rmse: 0.80525 |  0:00:27s
epoch 98 | loss: 0.61087 | val_0_rmse: 0.78297 | val_1_rmse: 0.80007 |  0:00:28s
epoch 99 | loss: 0.60509 | val_0_rmse: 0.77708 | val_1_rmse: 0.79466 |  0:00:28s
epoch 100| loss: 0.59949 | val_0_rmse: 0.77973 | val_1_rmse: 0.7977  |  0:00:28s
epoch 101| loss: 0.61266 | val_0_rmse: 0.76808 | val_1_rmse: 0.78178 |  0:00:28s
epoch 102| loss: 0.59698 | val_0_rmse: 0.77106 | val_1_rmse: 0.78282 |  0:00:29s
epoch 103| loss: 0.59424 | val_0_rmse: 0.77668 | val_1_rmse: 0.78564 |  0:00:29s
epoch 104| loss: 0.59192 | val_0_rmse: 0.76264 | val_1_rmse: 0.7748  |  0:00:29s
epoch 105| loss: 0.58759 | val_0_rmse: 0.76114 | val_1_rmse: 0.7657  |  0:00:30s
epoch 106| loss: 0.58204 | val_0_rmse: 0.76252 | val_1_rmse: 0.77087 |  0:00:30s
epoch 107| loss: 0.58819 | val_0_rmse: 0.76955 | val_1_rmse: 0.782   |  0:00:30s
epoch 108| loss: 0.58091 | val_0_rmse: 0.7638  | val_1_rmse: 0.77225 |  0:00:30s
epoch 109| loss: 0.58243 | val_0_rmse: 0.76186 | val_1_rmse: 0.7717  |  0:00:31s
epoch 110| loss: 0.5912  | val_0_rmse: 0.78865 | val_1_rmse: 0.78707 |  0:00:31s
epoch 111| loss: 0.59463 | val_0_rmse: 0.75899 | val_1_rmse: 0.76247 |  0:00:31s
epoch 112| loss: 0.58726 | val_0_rmse: 0.75731 | val_1_rmse: 0.76493 |  0:00:32s
epoch 113| loss: 0.59283 | val_0_rmse: 0.80204 | val_1_rmse: 0.80922 |  0:00:32s
epoch 114| loss: 0.60984 | val_0_rmse: 0.77615 | val_1_rmse: 0.78617 |  0:00:32s
epoch 115| loss: 0.60108 | val_0_rmse: 0.76561 | val_1_rmse: 0.7851  |  0:00:32s
epoch 116| loss: 0.60252 | val_0_rmse: 0.75407 | val_1_rmse: 0.77712 |  0:00:33s
epoch 117| loss: 0.59019 | val_0_rmse: 0.75308 | val_1_rmse: 0.77174 |  0:00:33s
epoch 118| loss: 0.61287 | val_0_rmse: 0.7553  | val_1_rmse: 0.77866 |  0:00:33s
epoch 119| loss: 0.60318 | val_0_rmse: 0.75114 | val_1_rmse: 0.77445 |  0:00:34s
epoch 120| loss: 0.5742  | val_0_rmse: 0.74393 | val_1_rmse: 0.77087 |  0:00:34s
epoch 121| loss: 0.56772 | val_0_rmse: 0.74412 | val_1_rmse: 0.77692 |  0:00:34s
epoch 122| loss: 0.57411 | val_0_rmse: 0.73779 | val_1_rmse: 0.76689 |  0:00:35s
epoch 123| loss: 0.57266 | val_0_rmse: 0.74375 | val_1_rmse: 0.77387 |  0:00:35s
epoch 124| loss: 0.57376 | val_0_rmse: 0.7397  | val_1_rmse: 0.76879 |  0:00:35s
epoch 125| loss: 0.56559 | val_0_rmse: 0.74072 | val_1_rmse: 0.76892 |  0:00:35s
epoch 126| loss: 0.56434 | val_0_rmse: 0.74176 | val_1_rmse: 0.76527 |  0:00:36s
epoch 127| loss: 0.5712  | val_0_rmse: 0.73714 | val_1_rmse: 0.76243 |  0:00:36s
epoch 128| loss: 0.58386 | val_0_rmse: 0.73571 | val_1_rmse: 0.76697 |  0:00:36s
epoch 129| loss: 0.57324 | val_0_rmse: 0.73802 | val_1_rmse: 0.76152 |  0:00:36s
epoch 130| loss: 0.56325 | val_0_rmse: 0.73544 | val_1_rmse: 0.76716 |  0:00:37s
epoch 131| loss: 0.55626 | val_0_rmse: 0.73699 | val_1_rmse: 0.76977 |  0:00:37s
epoch 132| loss: 0.55465 | val_0_rmse: 0.73716 | val_1_rmse: 0.77458 |  0:00:37s
epoch 133| loss: 0.55929 | val_0_rmse: 0.74647 | val_1_rmse: 0.76866 |  0:00:38s
epoch 134| loss: 0.56185 | val_0_rmse: 0.73952 | val_1_rmse: 0.77503 |  0:00:38s
epoch 135| loss: 0.54915 | val_0_rmse: 0.72972 | val_1_rmse: 0.76616 |  0:00:38s
epoch 136| loss: 0.54155 | val_0_rmse: 0.73371 | val_1_rmse: 0.76466 |  0:00:38s
epoch 137| loss: 0.55097 | val_0_rmse: 0.73095 | val_1_rmse: 0.75883 |  0:00:39s
epoch 138| loss: 0.55432 | val_0_rmse: 0.7418  | val_1_rmse: 0.759   |  0:00:39s
epoch 139| loss: 0.57512 | val_0_rmse: 0.74506 | val_1_rmse: 0.75137 |  0:00:39s
epoch 140| loss: 0.56403 | val_0_rmse: 0.74865 | val_1_rmse: 0.75312 |  0:00:40s
epoch 141| loss: 0.56209 | val_0_rmse: 0.74003 | val_1_rmse: 0.74699 |  0:00:40s
epoch 142| loss: 0.5667  | val_0_rmse: 0.73964 | val_1_rmse: 0.74728 |  0:00:40s
epoch 143| loss: 0.57539 | val_0_rmse: 0.75132 | val_1_rmse: 0.77198 |  0:00:40s
epoch 144| loss: 0.58777 | val_0_rmse: 0.74471 | val_1_rmse: 0.76398 |  0:00:41s
epoch 145| loss: 0.5694  | val_0_rmse: 0.74092 | val_1_rmse: 0.76357 |  0:00:41s
epoch 146| loss: 0.55886 | val_0_rmse: 0.74528 | val_1_rmse: 0.76743 |  0:00:41s
epoch 147| loss: 0.56306 | val_0_rmse: 0.74845 | val_1_rmse: 0.76861 |  0:00:42s
epoch 148| loss: 0.55124 | val_0_rmse: 0.74607 | val_1_rmse: 0.77795 |  0:00:42s
epoch 149| loss: 0.55581 | val_0_rmse: 0.72929 | val_1_rmse: 0.76018 |  0:00:42s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.74699
Best weights from best epoch are automatically used!
ended training at: 04:30:54
Feature importance:
Mean squared error is of 0.053277268718771954
Mean absolute error:0.1667264904914729
MAPE:0.18902885721327153
R2 score:0.374082380592176
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:30:54
epoch 0  | loss: 2.0749  | val_0_rmse: 1.01828 | val_1_rmse: 1.02042 |  0:00:00s
epoch 1  | loss: 1.28083 | val_0_rmse: 1.00566 | val_1_rmse: 1.01299 |  0:00:00s
epoch 2  | loss: 1.06136 | val_0_rmse: 1.00182 | val_1_rmse: 1.00955 |  0:00:00s
epoch 3  | loss: 1.01125 | val_0_rmse: 0.99832 | val_1_rmse: 1.0066  |  0:00:01s
epoch 4  | loss: 0.97915 | val_0_rmse: 0.98234 | val_1_rmse: 0.99345 |  0:00:01s
epoch 5  | loss: 0.92685 | val_0_rmse: 0.95253 | val_1_rmse: 0.96527 |  0:00:01s
epoch 6  | loss: 0.82739 | val_0_rmse: 0.9408  | val_1_rmse: 0.98895 |  0:00:02s
epoch 7  | loss: 0.84443 | val_0_rmse: 0.92782 | val_1_rmse: 0.9762  |  0:00:02s
epoch 8  | loss: 0.78578 | val_0_rmse: 0.92971 | val_1_rmse: 0.98286 |  0:00:02s
epoch 9  | loss: 0.7616  | val_0_rmse: 0.97967 | val_1_rmse: 1.04119 |  0:00:02s
epoch 10 | loss: 0.74942 | val_0_rmse: 0.92628 | val_1_rmse: 0.98073 |  0:00:03s
epoch 11 | loss: 0.74243 | val_0_rmse: 0.89019 | val_1_rmse: 0.93744 |  0:00:03s
epoch 12 | loss: 0.7346  | val_0_rmse: 0.90392 | val_1_rmse: 0.95658 |  0:00:03s
epoch 13 | loss: 0.72969 | val_0_rmse: 0.86067 | val_1_rmse: 0.90821 |  0:00:03s
epoch 14 | loss: 0.72969 | val_0_rmse: 0.85461 | val_1_rmse: 0.89971 |  0:00:04s
epoch 15 | loss: 0.73039 | val_0_rmse: 0.85895 | val_1_rmse: 0.90007 |  0:00:04s
epoch 16 | loss: 0.72077 | val_0_rmse: 0.85406 | val_1_rmse: 0.89551 |  0:00:04s
epoch 17 | loss: 0.71669 | val_0_rmse: 0.84999 | val_1_rmse: 0.89185 |  0:00:05s
epoch 18 | loss: 0.71089 | val_0_rmse: 0.84889 | val_1_rmse: 0.88762 |  0:00:05s
epoch 19 | loss: 0.70654 | val_0_rmse: 0.85734 | val_1_rmse: 0.90895 |  0:00:05s
epoch 20 | loss: 0.71268 | val_0_rmse: 0.8533  | val_1_rmse: 0.89746 |  0:00:06s
epoch 21 | loss: 0.71001 | val_0_rmse: 0.85    | val_1_rmse: 0.89051 |  0:00:06s
epoch 22 | loss: 0.71578 | val_0_rmse: 0.87039 | val_1_rmse: 0.91295 |  0:00:06s
epoch 23 | loss: 0.6995  | val_0_rmse: 0.83971 | val_1_rmse: 0.86698 |  0:00:06s
epoch 24 | loss: 0.69038 | val_0_rmse: 0.85365 | val_1_rmse: 0.88901 |  0:00:07s
epoch 25 | loss: 0.69488 | val_0_rmse: 0.84203 | val_1_rmse: 0.86889 |  0:00:07s
epoch 26 | loss: 0.68358 | val_0_rmse: 0.84015 | val_1_rmse: 0.87251 |  0:00:07s
epoch 27 | loss: 0.68142 | val_0_rmse: 0.83635 | val_1_rmse: 0.86983 |  0:00:07s
epoch 28 | loss: 0.67263 | val_0_rmse: 0.82614 | val_1_rmse: 0.8566  |  0:00:08s
epoch 29 | loss: 0.67521 | val_0_rmse: 0.84417 | val_1_rmse: 0.89528 |  0:00:08s
epoch 30 | loss: 0.67187 | val_0_rmse: 0.82532 | val_1_rmse: 0.84708 |  0:00:09s
epoch 31 | loss: 0.69461 | val_0_rmse: 0.84206 | val_1_rmse: 0.83788 |  0:00:09s
epoch 32 | loss: 0.69431 | val_0_rmse: 0.9135  | val_1_rmse: 0.91656 |  0:00:09s
epoch 33 | loss: 0.68748 | val_0_rmse: 0.83899 | val_1_rmse: 0.8507  |  0:00:09s
epoch 34 | loss: 0.68585 | val_0_rmse: 0.83659 | val_1_rmse: 0.84946 |  0:00:10s
epoch 35 | loss: 0.67833 | val_0_rmse: 0.8883  | val_1_rmse: 0.90179 |  0:00:10s
epoch 36 | loss: 0.66462 | val_0_rmse: 0.85062 | val_1_rmse: 0.85995 |  0:00:10s
epoch 37 | loss: 0.67695 | val_0_rmse: 0.85657 | val_1_rmse: 0.8786  |  0:00:10s
epoch 38 | loss: 0.652   | val_0_rmse: 0.90478 | val_1_rmse: 0.93329 |  0:00:11s
epoch 39 | loss: 0.64766 | val_0_rmse: 0.86014 | val_1_rmse: 0.88446 |  0:00:11s
epoch 40 | loss: 0.64724 | val_0_rmse: 0.82696 | val_1_rmse: 0.84959 |  0:00:11s
epoch 41 | loss: 0.6381  | val_0_rmse: 0.8181  | val_1_rmse: 0.84003 |  0:00:12s
epoch 42 | loss: 0.65396 | val_0_rmse: 0.81556 | val_1_rmse: 0.8394  |  0:00:12s
epoch 43 | loss: 0.64695 | val_0_rmse: 0.81456 | val_1_rmse: 0.82943 |  0:00:12s
epoch 44 | loss: 0.64174 | val_0_rmse: 0.81788 | val_1_rmse: 0.84753 |  0:00:12s
epoch 45 | loss: 0.63055 | val_0_rmse: 0.81629 | val_1_rmse: 0.84864 |  0:00:13s
epoch 46 | loss: 0.63125 | val_0_rmse: 0.81669 | val_1_rmse: 0.83849 |  0:00:13s
epoch 47 | loss: 0.63253 | val_0_rmse: 0.82061 | val_1_rmse: 0.84615 |  0:00:13s
epoch 48 | loss: 0.64463 | val_0_rmse: 0.81404 | val_1_rmse: 0.84268 |  0:00:14s
epoch 49 | loss: 0.65153 | val_0_rmse: 0.82088 | val_1_rmse: 0.85847 |  0:00:14s
epoch 50 | loss: 0.66077 | val_0_rmse: 0.83472 | val_1_rmse: 0.86787 |  0:00:14s
epoch 51 | loss: 0.64655 | val_0_rmse: 0.83473 | val_1_rmse: 0.86919 |  0:00:14s
epoch 52 | loss: 0.6402  | val_0_rmse: 0.8187  | val_1_rmse: 0.85786 |  0:00:15s
epoch 53 | loss: 0.64526 | val_0_rmse: 0.81324 | val_1_rmse: 0.85557 |  0:00:15s
epoch 54 | loss: 0.63655 | val_0_rmse: 0.81341 | val_1_rmse: 0.84931 |  0:00:15s
epoch 55 | loss: 0.63236 | val_0_rmse: 0.81311 | val_1_rmse: 0.85152 |  0:00:16s
epoch 56 | loss: 0.62179 | val_0_rmse: 0.81492 | val_1_rmse: 0.85265 |  0:00:16s
epoch 57 | loss: 0.63611 | val_0_rmse: 0.82169 | val_1_rmse: 0.86151 |  0:00:16s
epoch 58 | loss: 0.62233 | val_0_rmse: 0.83456 | val_1_rmse: 0.87661 |  0:00:16s
epoch 59 | loss: 0.62917 | val_0_rmse: 0.82042 | val_1_rmse: 0.86175 |  0:00:17s
epoch 60 | loss: 0.62648 | val_0_rmse: 0.80555 | val_1_rmse: 0.8396  |  0:00:17s
epoch 61 | loss: 0.62239 | val_0_rmse: 0.80605 | val_1_rmse: 0.84548 |  0:00:17s
epoch 62 | loss: 0.61812 | val_0_rmse: 0.80708 | val_1_rmse: 0.85178 |  0:00:18s
epoch 63 | loss: 0.61954 | val_0_rmse: 0.81399 | val_1_rmse: 0.84519 |  0:00:18s
epoch 64 | loss: 0.62554 | val_0_rmse: 0.81491 | val_1_rmse: 0.85802 |  0:00:18s
epoch 65 | loss: 0.62216 | val_0_rmse: 0.80785 | val_1_rmse: 0.84696 |  0:00:18s
epoch 66 | loss: 0.61567 | val_0_rmse: 0.80401 | val_1_rmse: 0.85047 |  0:00:19s
epoch 67 | loss: 0.60767 | val_0_rmse: 0.79529 | val_1_rmse: 0.83753 |  0:00:19s
epoch 68 | loss: 0.62424 | val_0_rmse: 0.80024 | val_1_rmse: 0.84532 |  0:00:19s
epoch 69 | loss: 0.60416 | val_0_rmse: 0.83675 | val_1_rmse: 0.8965  |  0:00:20s
epoch 70 | loss: 0.60833 | val_0_rmse: 0.83993 | val_1_rmse: 0.89256 |  0:00:20s
epoch 71 | loss: 0.62216 | val_0_rmse: 0.82664 | val_1_rmse: 0.86682 |  0:00:20s
epoch 72 | loss: 0.61116 | val_0_rmse: 0.84191 | val_1_rmse: 0.89952 |  0:00:20s
epoch 73 | loss: 0.60465 | val_0_rmse: 0.79622 | val_1_rmse: 0.8584  |  0:00:21s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.82943
Best weights from best epoch are automatically used!
ended training at: 04:31:16
Feature importance:
Mean squared error is of 0.057593521522636235
Mean absolute error:0.17852324198948397
MAPE:0.20290440738524862
R2 score:0.2823304016748598
------------------------------------------------------------------
Normalization used is Log & StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:35:45
epoch 0  | loss: 1.04881 | val_0_rmse: 0.9986  | val_1_rmse: 1.00163 |  0:00:21s
epoch 1  | loss: 0.82873 | val_0_rmse: 0.844   | val_1_rmse: 0.84266 |  0:00:42s
epoch 2  | loss: 0.65312 | val_0_rmse: 0.80549 | val_1_rmse: 0.80767 |  0:01:04s
epoch 3  | loss: 0.60446 | val_0_rmse: 0.78696 | val_1_rmse: 0.79496 |  0:01:25s
epoch 4  | loss: 0.56846 | val_0_rmse: 0.77473 | val_1_rmse: 0.79292 |  0:01:47s
epoch 5  | loss: 0.55507 | val_0_rmse: 0.73164 | val_1_rmse: 0.75231 |  0:02:08s
epoch 6  | loss: 0.53361 | val_0_rmse: 0.70806 | val_1_rmse: 0.74332 |  0:02:30s
epoch 7  | loss: 0.5242  | val_0_rmse: 0.70222 | val_1_rmse: 0.74465 |  0:02:51s
epoch 8  | loss: 0.50863 | val_0_rmse: 0.73211 | val_1_rmse: 0.82389 |  0:03:13s
epoch 9  | loss: 0.50275 | val_0_rmse: 0.7547  | val_1_rmse: 0.78533 |  0:03:35s
epoch 10 | loss: 0.50251 | val_0_rmse: 0.73631 | val_1_rmse: 0.84203 |  0:03:57s
epoch 11 | loss: 0.48844 | val_0_rmse: 0.79198 | val_1_rmse: 1.01072 |  0:04:19s
epoch 12 | loss: 0.48379 | val_0_rmse: 0.83904 | val_1_rmse: 0.87254 |  0:04:41s
epoch 13 | loss: 0.47434 | val_0_rmse: 0.69042 | val_1_rmse: 0.75246 |  0:05:02s
epoch 14 | loss: 0.46797 | val_0_rmse: 0.70235 | val_1_rmse: 0.76493 |  0:05:24s
epoch 15 | loss: 0.46026 | val_0_rmse: 0.70498 | val_1_rmse: 0.77053 |  0:05:45s
epoch 16 | loss: 0.46026 | val_0_rmse: 0.72926 | val_1_rmse: 0.78108 |  0:06:07s
epoch 17 | loss: 0.45389 | val_0_rmse: 0.74636 | val_1_rmse: 0.81379 |  0:06:28s
epoch 18 | loss: 0.45242 | val_0_rmse: 0.79153 | val_1_rmse: 0.84162 |  0:06:50s
epoch 19 | loss: 0.4417  | val_0_rmse: 0.69511 | val_1_rmse: 0.77121 |  0:07:11s
epoch 20 | loss: 0.43923 | val_0_rmse: 0.8419  | val_1_rmse: 0.88673 |  0:07:33s
epoch 21 | loss: 0.43598 | val_0_rmse: 0.9899  | val_1_rmse: 0.95617 |  0:07:54s
epoch 22 | loss: 0.42971 | val_0_rmse: 0.7795  | val_1_rmse: 0.83643 |  0:08:16s
epoch 23 | loss: 0.4269  | val_0_rmse: 0.69463 | val_1_rmse: 0.77807 |  0:08:38s
epoch 24 | loss: 0.42442 | val_0_rmse: 0.67918 | val_1_rmse: 0.76297 |  0:08:59s
epoch 25 | loss: 0.4185  | val_0_rmse: 0.75962 | val_1_rmse: 0.86302 |  0:09:21s
epoch 26 | loss: 0.41912 | val_0_rmse: 0.77189 | val_1_rmse: 0.81747 |  0:09:42s
epoch 27 | loss: 0.41997 | val_0_rmse: 0.83932 | val_1_rmse: 0.95822 |  0:10:04s
epoch 28 | loss: 0.41389 | val_0_rmse: 0.67801 | val_1_rmse: 0.75327 |  0:10:25s
epoch 29 | loss: 0.41473 | val_0_rmse: 0.69591 | val_1_rmse: 0.79199 |  0:10:47s
epoch 30 | loss: 0.4106  | val_0_rmse: 0.6806  | val_1_rmse: 0.76781 |  0:11:09s
epoch 31 | loss: 0.40699 | val_0_rmse: 0.77394 | val_1_rmse: 0.89684 |  0:11:30s
epoch 32 | loss: 0.40855 | val_0_rmse: 0.66175 | val_1_rmse: 0.79368 |  0:11:52s
epoch 33 | loss: 0.40238 | val_0_rmse: 0.75701 | val_1_rmse: 0.91335 |  0:12:13s
epoch 34 | loss: 0.40084 | val_0_rmse: 0.99837 | val_1_rmse: 1.12495 |  0:12:35s
epoch 35 | loss: 0.39902 | val_0_rmse: 0.83439 | val_1_rmse: 0.7894  |  0:12:57s
epoch 36 | loss: 0.3922  | val_0_rmse: 0.72786 | val_1_rmse: 0.84475 |  0:13:19s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.74332
Best weights from best epoch are automatically used!
ended training at: 04:49:17
Feature importance:
Mean squared error is of 0.06445934969349087
Mean absolute error:0.177751838003291
MAPE:0.1908151353767154
R2 score:0.4635847323513457
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:49:22
epoch 0  | loss: 0.93022 | val_0_rmse: 0.88809 | val_1_rmse: 0.90039 |  0:00:21s
epoch 1  | loss: 0.71119 | val_0_rmse: 0.8599  | val_1_rmse: 0.87061 |  0:00:43s
epoch 2  | loss: 0.64035 | val_0_rmse: 0.84108 | val_1_rmse: 0.84647 |  0:01:05s
epoch 3  | loss: 0.60142 | val_0_rmse: 0.77127 | val_1_rmse: 0.78315 |  0:01:26s
epoch 4  | loss: 0.57158 | val_0_rmse: 0.73871 | val_1_rmse: 0.75867 |  0:01:48s
epoch 5  | loss: 0.54692 | val_0_rmse: 0.7178  | val_1_rmse: 0.74444 |  0:02:09s
epoch 6  | loss: 0.53964 | val_0_rmse: 0.7113  | val_1_rmse: 0.7448  |  0:02:30s
epoch 7  | loss: 0.52915 | val_0_rmse: 0.71237 | val_1_rmse: 0.75285 |  0:02:52s
epoch 8  | loss: 0.5216  | val_0_rmse: 0.70718 | val_1_rmse: 0.75784 |  0:03:13s
epoch 9  | loss: 0.51033 | val_0_rmse: 0.7249  | val_1_rmse: 0.77464 |  0:03:34s
epoch 10 | loss: 0.50581 | val_0_rmse: 0.72356 | val_1_rmse: 0.76969 |  0:03:56s
epoch 11 | loss: 0.49959 | val_0_rmse: 0.73711 | val_1_rmse: 0.79061 |  0:04:17s
epoch 12 | loss: 0.48647 | val_0_rmse: 0.69382 | val_1_rmse: 0.74933 |  0:04:38s
epoch 13 | loss: 0.48073 | val_0_rmse: 0.69056 | val_1_rmse: 0.75147 |  0:05:00s
epoch 14 | loss: 0.47347 | val_0_rmse: 0.70384 | val_1_rmse: 0.76657 |  0:05:21s
epoch 15 | loss: 0.46979 | val_0_rmse: 0.68276 | val_1_rmse: 0.77478 |  0:05:42s
epoch 16 | loss: 0.46206 | val_0_rmse: 0.687   | val_1_rmse: 0.74962 |  0:06:04s
epoch 17 | loss: 0.46041 | val_0_rmse: 0.68336 | val_1_rmse: 0.77119 |  0:06:25s
epoch 18 | loss: 0.45671 | val_0_rmse: 0.72353 | val_1_rmse: 0.95081 |  0:06:47s
epoch 19 | loss: 0.45051 | val_0_rmse: 0.67625 | val_1_rmse: 0.79551 |  0:07:08s
epoch 20 | loss: 0.44882 | val_0_rmse: 0.72152 | val_1_rmse: 0.79571 |  0:07:30s
epoch 21 | loss: 0.44672 | val_0_rmse: 0.7375  | val_1_rmse: 0.83746 |  0:07:51s
epoch 22 | loss: 0.44434 | val_0_rmse: 0.67113 | val_1_rmse: 0.75453 |  0:08:13s
epoch 23 | loss: 0.43818 | val_0_rmse: 0.67482 | val_1_rmse: 0.74993 |  0:08:34s
epoch 24 | loss: 0.4357  | val_0_rmse: 0.69796 | val_1_rmse: 0.80203 |  0:08:56s
epoch 25 | loss: 0.43233 | val_0_rmse: 0.72027 | val_1_rmse: 0.74062 |  0:09:17s
epoch 26 | loss: 0.42883 | val_0_rmse: 0.65541 | val_1_rmse: 0.73945 |  0:09:39s
epoch 27 | loss: 0.42689 | val_0_rmse: 0.66087 | val_1_rmse: 0.74629 |  0:10:00s
epoch 28 | loss: 0.42655 | val_0_rmse: 0.6562  | val_1_rmse: 0.74838 |  0:10:22s
epoch 29 | loss: 0.42348 | val_0_rmse: 0.66549 | val_1_rmse: 0.75026 |  0:10:43s
epoch 30 | loss: 0.417   | val_0_rmse: 2.40878 | val_1_rmse: 0.75801 |  0:11:05s
epoch 31 | loss: 0.41723 | val_0_rmse: 0.6521  | val_1_rmse: 0.74929 |  0:11:26s
epoch 32 | loss: 0.41465 | val_0_rmse: 0.65492 | val_1_rmse: 0.7501  |  0:11:48s
epoch 33 | loss: 0.41122 | val_0_rmse: 0.65059 | val_1_rmse: 0.74925 |  0:12:09s
epoch 34 | loss: 0.40965 | val_0_rmse: 0.6473  | val_1_rmse: 0.73553 |  0:12:31s
epoch 35 | loss: 0.41144 | val_0_rmse: 0.65086 | val_1_rmse: 0.74389 |  0:12:53s
epoch 36 | loss: 0.40781 | val_0_rmse: 0.64775 | val_1_rmse: 0.75188 |  0:13:14s
epoch 37 | loss: 0.40603 | val_0_rmse: 0.66912 | val_1_rmse: 0.81379 |  0:13:36s
epoch 38 | loss: 0.40305 | val_0_rmse: 1.64595 | val_1_rmse: 0.77225 |  0:13:57s
epoch 39 | loss: 0.39868 | val_0_rmse: 0.64671 | val_1_rmse: 0.75539 |  0:14:19s
epoch 40 | loss: 0.39849 | val_0_rmse: 0.64542 | val_1_rmse: 0.75383 |  0:14:40s
epoch 41 | loss: 0.39902 | val_0_rmse: 0.64246 | val_1_rmse: 0.75239 |  0:15:02s
epoch 42 | loss: 0.3936  | val_0_rmse: 0.6852  | val_1_rmse: 0.78524 |  0:15:23s
epoch 43 | loss: 0.39697 | val_0_rmse: 0.64012 | val_1_rmse: 0.74264 |  0:15:44s
epoch 44 | loss: 0.3932  | val_0_rmse: 0.6712  | val_1_rmse: 0.74997 |  0:16:06s
epoch 45 | loss: 0.3911  | val_0_rmse: 0.63471 | val_1_rmse: 0.74463 |  0:16:28s
epoch 46 | loss: 0.38762 | val_0_rmse: 0.64623 | val_1_rmse: 0.75703 |  0:16:49s
epoch 47 | loss: 0.3871  | val_0_rmse: 0.63957 | val_1_rmse: 0.75596 |  0:17:10s
epoch 48 | loss: 0.38355 | val_0_rmse: 0.63648 | val_1_rmse: 0.74743 |  0:17:31s
epoch 49 | loss: 0.38276 | val_0_rmse: 0.64136 | val_1_rmse: 0.76206 |  0:17:53s
epoch 50 | loss: 0.37894 | val_0_rmse: 0.63274 | val_1_rmse: 0.74647 |  0:18:14s
epoch 51 | loss: 0.37851 | val_0_rmse: 0.65029 | val_1_rmse: 0.75965 |  0:18:35s
epoch 52 | loss: 0.38072 | val_0_rmse: 0.63007 | val_1_rmse: 0.74998 |  0:18:56s
epoch 53 | loss: 0.37891 | val_0_rmse: 0.6353  | val_1_rmse: 0.76232 |  0:19:18s
epoch 54 | loss: 0.37478 | val_0_rmse: 0.65552 | val_1_rmse: 0.74763 |  0:19:39s
epoch 55 | loss: 0.37462 | val_0_rmse: 0.62844 | val_1_rmse: 0.74295 |  0:20:00s
epoch 56 | loss: 0.37178 | val_0_rmse: 0.63067 | val_1_rmse: 0.74531 |  0:20:21s
epoch 57 | loss: 0.37776 | val_0_rmse: 0.67294 | val_1_rmse: 0.75061 |  0:20:43s
epoch 58 | loss: 0.37619 | val_0_rmse: 0.74062 | val_1_rmse: 0.76684 |  0:21:04s
epoch 59 | loss: 0.36836 | val_0_rmse: 0.6418  | val_1_rmse: 0.77875 |  0:21:25s
epoch 60 | loss: 0.36838 | val_0_rmse: 0.6335  | val_1_rmse: 0.75351 |  0:21:46s
epoch 61 | loss: 0.36544 | val_0_rmse: 0.62682 | val_1_rmse: 0.75484 |  0:22:08s
epoch 62 | loss: 0.36401 | val_0_rmse: 0.64107 | val_1_rmse: 0.75064 |  0:22:29s
epoch 63 | loss: 0.36944 | val_0_rmse: 0.62416 | val_1_rmse: 0.75464 |  0:22:50s
epoch 64 | loss: 0.36484 | val_0_rmse: 0.63428 | val_1_rmse: 0.764   |  0:23:11s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.73553
Best weights from best epoch are automatically used!
ended training at: 05:12:47
Feature importance:
Mean squared error is of 0.06256261849495702
Mean absolute error:0.17488498846611433
MAPE:0.19127711649378443
R2 score:0.49197295419860154
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:12:52
epoch 0  | loss: 1.05533 | val_0_rmse: 0.99821 | val_1_rmse: 0.99638 |  0:00:21s
epoch 1  | loss: 0.90714 | val_0_rmse: 0.94692 | val_1_rmse: 0.94874 |  0:00:42s
epoch 2  | loss: 0.72086 | val_0_rmse: 0.80263 | val_1_rmse: 0.80723 |  0:01:04s
epoch 3  | loss: 0.63137 | val_0_rmse: 0.8054  | val_1_rmse: 0.8141  |  0:01:25s
epoch 4  | loss: 0.5989  | val_0_rmse: 0.76029 | val_1_rmse: 0.77355 |  0:01:46s
epoch 5  | loss: 0.57433 | val_0_rmse: 0.76396 | val_1_rmse: 0.78065 |  0:02:07s
epoch 6  | loss: 0.55889 | val_0_rmse: 0.7391  | val_1_rmse: 0.7584  |  0:02:29s
epoch 7  | loss: 0.55622 | val_0_rmse: 0.75658 | val_1_rmse: 0.7594  |  0:02:50s
epoch 8  | loss: 0.53879 | val_0_rmse: 0.71712 | val_1_rmse: 0.74395 |  0:03:11s
epoch 9  | loss: 0.5239  | val_0_rmse: 0.73075 | val_1_rmse: 0.76027 |  0:03:32s
epoch 10 | loss: 0.5099  | val_0_rmse: 0.74955 | val_1_rmse: 0.79117 |  0:03:54s
epoch 11 | loss: 0.50264 | val_0_rmse: 0.82293 | val_1_rmse: 0.87867 |  0:04:15s
epoch 12 | loss: 0.49217 | val_0_rmse: 0.69747 | val_1_rmse: 0.74261 |  0:04:36s
epoch 13 | loss: 0.49085 | val_0_rmse: 0.73453 | val_1_rmse: 0.783   |  0:04:57s
epoch 14 | loss: 0.47574 | val_0_rmse: 0.74277 | val_1_rmse: 0.79611 |  0:05:18s
epoch 15 | loss: 0.4704  | val_0_rmse: 0.70556 | val_1_rmse: 0.76542 |  0:05:40s
epoch 16 | loss: 0.46376 | val_0_rmse: 0.70607 | val_1_rmse: 0.77382 |  0:06:01s
epoch 17 | loss: 0.45845 | val_0_rmse: 0.73707 | val_1_rmse: 0.80878 |  0:06:22s
epoch 18 | loss: 0.45283 | val_0_rmse: 0.68905 | val_1_rmse: 0.74334 |  0:06:43s
epoch 19 | loss: 0.44694 | val_0_rmse: 0.70709 | val_1_rmse: 0.77664 |  0:07:05s
epoch 20 | loss: 0.4558  | val_0_rmse: 0.6831  | val_1_rmse: 0.75268 |  0:07:26s
epoch 21 | loss: 0.44305 | val_0_rmse: 0.69199 | val_1_rmse: 0.76719 |  0:07:48s
epoch 22 | loss: 0.43586 | val_0_rmse: 0.70712 | val_1_rmse: 0.79065 |  0:08:09s
epoch 23 | loss: 0.44212 | val_0_rmse: 0.71652 | val_1_rmse: 0.80204 |  0:08:31s
epoch 24 | loss: 0.43255 | val_0_rmse: 0.71595 | val_1_rmse: 0.80561 |  0:08:52s
epoch 25 | loss: 0.42701 | val_0_rmse: 0.68231 | val_1_rmse: 0.77362 |  0:09:13s
epoch 26 | loss: 0.42211 | val_0_rmse: 0.71166 | val_1_rmse: 0.75955 |  0:09:34s
epoch 27 | loss: 0.42165 | val_0_rmse: 0.70256 | val_1_rmse: 0.80424 |  0:09:56s
epoch 28 | loss: 0.41782 | val_0_rmse: 0.70457 | val_1_rmse: 0.81236 |  0:10:17s
epoch 29 | loss: 0.41605 | val_0_rmse: 0.70959 | val_1_rmse: 0.78719 |  0:10:39s
epoch 30 | loss: 0.41389 | val_0_rmse: 0.7157  | val_1_rmse: 0.81391 |  0:11:00s
epoch 31 | loss: 0.41255 | val_0_rmse: 0.6946  | val_1_rmse: 0.75955 |  0:11:21s
epoch 32 | loss: 0.40844 | val_0_rmse: 0.67651 | val_1_rmse: 0.74856 |  0:11:42s
epoch 33 | loss: 0.40109 | val_0_rmse: 0.66217 | val_1_rmse: 0.75706 |  0:12:04s
epoch 34 | loss: 0.40016 | val_0_rmse: 0.67539 | val_1_rmse: 0.77761 |  0:12:25s
epoch 35 | loss: 0.39687 | val_0_rmse: 0.66255 | val_1_rmse: 0.75922 |  0:12:47s
epoch 36 | loss: 0.39566 | val_0_rmse: 0.70984 | val_1_rmse: 0.76659 |  0:13:08s
epoch 37 | loss: 0.39719 | val_0_rmse: 0.72424 | val_1_rmse: 0.82106 |  0:13:29s
epoch 38 | loss: 0.39857 | val_0_rmse: 6.49524 | val_1_rmse: 9.1784  |  0:13:51s
epoch 39 | loss: 0.38959 | val_0_rmse: 0.6908  | val_1_rmse: 0.79731 |  0:14:12s
epoch 40 | loss: 0.38677 | val_0_rmse: 0.67775 | val_1_rmse: 0.78991 |  0:14:33s
epoch 41 | loss: 0.38954 | val_0_rmse: 0.98455 | val_1_rmse: 1.24599 |  0:14:54s
epoch 42 | loss: 0.38459 | val_0_rmse: 0.78516 | val_1_rmse: 0.76214 |  0:15:16s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.74261
Best weights from best epoch are automatically used!
ended training at: 05:28:22
Feature importance:
Mean squared error is of 0.07279594330557956
Mean absolute error:0.17956707938005026
MAPE:0.19397188699784212
R2 score:0.4346832286680491
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:28:27
epoch 0  | loss: 1.02268 | val_0_rmse: 0.93641 | val_1_rmse: 0.93238 |  0:00:21s
epoch 1  | loss: 0.75583 | val_0_rmse: 0.85989 | val_1_rmse: 0.85255 |  0:00:42s
epoch 2  | loss: 0.64443 | val_0_rmse: 0.8038  | val_1_rmse: 0.80176 |  0:01:03s
epoch 3  | loss: 0.59461 | val_0_rmse: 0.77058 | val_1_rmse: 0.77287 |  0:01:25s
epoch 4  | loss: 0.56452 | val_0_rmse: 0.74482 | val_1_rmse: 0.75391 |  0:01:46s
epoch 5  | loss: 0.54413 | val_0_rmse: 0.71441 | val_1_rmse: 0.73566 |  0:02:07s
epoch 6  | loss: 0.53436 | val_0_rmse: 0.75895 | val_1_rmse: 0.78062 |  0:02:29s
epoch 7  | loss: 0.51825 | val_0_rmse: 0.70716 | val_1_rmse: 0.73745 |  0:02:50s
epoch 8  | loss: 0.50939 | val_0_rmse: 0.70364 | val_1_rmse: 0.74603 |  0:03:11s
epoch 9  | loss: 0.51099 | val_0_rmse: 0.69405 | val_1_rmse: 0.73685 |  0:03:33s
epoch 10 | loss: 0.4919  | val_0_rmse: 0.7132  | val_1_rmse: 0.75334 |  0:03:54s
epoch 11 | loss: 0.48812 | val_0_rmse: 0.6978  | val_1_rmse: 0.74185 |  0:04:15s
epoch 12 | loss: 0.48231 | val_0_rmse: 0.70673 | val_1_rmse: 0.75453 |  0:04:37s
epoch 13 | loss: 0.47975 | val_0_rmse: 0.68912 | val_1_rmse: 0.74201 |  0:04:58s
epoch 14 | loss: 0.47009 | val_0_rmse: 0.68455 | val_1_rmse: 0.74399 |  0:05:19s
epoch 15 | loss: 0.46258 | val_0_rmse: 0.68129 | val_1_rmse: 0.7388  |  0:05:41s
epoch 16 | loss: 0.45959 | val_0_rmse: 0.69912 | val_1_rmse: 0.74786 |  0:06:02s
epoch 17 | loss: 0.45205 | val_0_rmse: 0.73579 | val_1_rmse: 0.77994 |  0:06:23s
epoch 18 | loss: 0.44789 | val_0_rmse: 0.67939 | val_1_rmse: 0.73974 |  0:06:45s
epoch 19 | loss: 0.44409 | val_0_rmse: 0.71878 | val_1_rmse: 0.75897 |  0:07:07s
epoch 20 | loss: 0.43699 | val_0_rmse: 0.68138 | val_1_rmse: 0.75243 |  0:07:28s
epoch 21 | loss: 0.43569 | val_0_rmse: 0.66327 | val_1_rmse: 0.74348 |  0:07:50s
epoch 22 | loss: 0.42935 | val_0_rmse: 0.68884 | val_1_rmse: 0.74165 |  0:08:11s
epoch 23 | loss: 0.42832 | val_0_rmse: 0.67266 | val_1_rmse: 0.74159 |  0:08:33s
epoch 24 | loss: 0.42932 | val_0_rmse: 0.66525 | val_1_rmse: 0.75622 |  0:08:54s
epoch 25 | loss: 0.42387 | val_0_rmse: 0.66467 | val_1_rmse: 0.7507  |  0:09:15s
epoch 26 | loss: 0.42017 | val_0_rmse: 0.80579 | val_1_rmse: 0.85371 |  0:09:37s
epoch 27 | loss: 0.41491 | val_0_rmse: 0.69384 | val_1_rmse: 0.76241 |  0:09:58s
epoch 28 | loss: 0.42708 | val_0_rmse: 0.77254 | val_1_rmse: 0.79393 |  0:10:19s
epoch 29 | loss: 0.41132 | val_0_rmse: 0.65185 | val_1_rmse: 0.74399 |  0:10:41s
epoch 30 | loss: 0.40977 | val_0_rmse: 0.69611 | val_1_rmse: 0.77506 |  0:11:02s
epoch 31 | loss: 0.40663 | val_0_rmse: 0.64646 | val_1_rmse: 0.74548 |  0:11:24s
epoch 32 | loss: 0.39956 | val_0_rmse: 0.66657 | val_1_rmse: 0.75078 |  0:11:45s
epoch 33 | loss: 0.40026 | val_0_rmse: 0.64516 | val_1_rmse: 0.74468 |  0:12:06s
epoch 34 | loss: 0.39621 | val_0_rmse: 0.63577 | val_1_rmse: 0.73711 |  0:12:28s
epoch 35 | loss: 0.3927  | val_0_rmse: 0.65111 | val_1_rmse: 0.7478  |  0:12:49s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 0.73566
Best weights from best epoch are automatically used!
ended training at: 05:41:29
Feature importance:
Mean squared error is of 0.06603734205079105
Mean absolute error:0.178730775058696
MAPE:0.19610371217484865
R2 score:0.45951518142497216
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:41:35
epoch 0  | loss: 0.98743 | val_0_rmse: 0.94415 | val_1_rmse: 0.94647 |  0:00:21s
epoch 1  | loss: 0.69662 | val_0_rmse: 0.82919 | val_1_rmse: 0.82789 |  0:00:42s
epoch 2  | loss: 0.63331 | val_0_rmse: 0.8005  | val_1_rmse: 0.7991  |  0:01:03s
epoch 3  | loss: 0.60287 | val_0_rmse: 0.78076 | val_1_rmse: 0.78377 |  0:01:24s
epoch 4  | loss: 0.58271 | val_0_rmse: 0.74537 | val_1_rmse: 0.75224 |  0:01:46s
epoch 5  | loss: 0.56735 | val_0_rmse: 0.73028 | val_1_rmse: 0.74457 |  0:02:07s
epoch 6  | loss: 0.55509 | val_0_rmse: 0.73758 | val_1_rmse: 0.75743 |  0:02:28s
epoch 7  | loss: 0.54261 | val_0_rmse: 0.71745 | val_1_rmse: 0.74693 |  0:02:49s
epoch 8  | loss: 0.52897 | val_0_rmse: 0.71348 | val_1_rmse: 0.74143 |  0:03:11s
epoch 9  | loss: 0.52445 | val_0_rmse: 0.71368 | val_1_rmse: 0.75362 |  0:03:32s
epoch 10 | loss: 0.51512 | val_0_rmse: 0.71332 | val_1_rmse: 0.7437  |  0:03:53s
epoch 11 | loss: 0.512   | val_0_rmse: 0.70201 | val_1_rmse: 0.74222 |  0:04:14s
epoch 12 | loss: 0.49582 | val_0_rmse: 0.7302  | val_1_rmse: 0.74129 |  0:04:36s
epoch 13 | loss: 0.4974  | val_0_rmse: 0.70255 | val_1_rmse: 0.75194 |  0:04:57s
epoch 14 | loss: 0.48874 | val_0_rmse: 0.69585 | val_1_rmse: 0.75485 |  0:05:18s
epoch 15 | loss: 0.47721 | val_0_rmse: 0.69828 | val_1_rmse: 0.75328 |  0:05:40s
epoch 16 | loss: 0.47692 | val_0_rmse: 0.69467 | val_1_rmse: 0.75368 |  0:06:01s
epoch 17 | loss: 0.46969 | val_0_rmse: 0.68347 | val_1_rmse: 0.73899 |  0:06:22s
epoch 18 | loss: 0.45899 | val_0_rmse: 0.69721 | val_1_rmse: 0.7481  |  0:06:43s
epoch 19 | loss: 0.45892 | val_0_rmse: 0.70172 | val_1_rmse: 0.76605 |  0:07:05s
epoch 20 | loss: 0.45506 | val_0_rmse: 0.67131 | val_1_rmse: 0.73546 |  0:07:26s
epoch 21 | loss: 0.44907 | val_0_rmse: 0.80068 | val_1_rmse: 0.85363 |  0:07:47s
epoch 22 | loss: 0.44821 | val_0_rmse: 0.69341 | val_1_rmse: 0.73679 |  0:08:09s
epoch 23 | loss: 0.443   | val_0_rmse: 0.69614 | val_1_rmse: 0.73221 |  0:08:30s
epoch 24 | loss: 0.43598 | val_0_rmse: 0.69085 | val_1_rmse: 0.73718 |  0:08:51s
epoch 25 | loss: 0.43697 | val_0_rmse: 0.74742 | val_1_rmse: 0.80511 |  0:09:13s
epoch 26 | loss: 0.43037 | val_0_rmse: 0.74457 | val_1_rmse: 0.7433  |  0:09:35s
epoch 27 | loss: 0.42734 | val_0_rmse: 0.6668  | val_1_rmse: 0.74578 |  0:09:56s
epoch 28 | loss: 0.42413 | val_0_rmse: 0.67026 | val_1_rmse: 0.74905 |  0:10:17s
epoch 29 | loss: 0.42212 | val_0_rmse: 0.6975  | val_1_rmse: 0.74971 |  0:10:39s
epoch 30 | loss: 0.42074 | val_0_rmse: 0.67377 | val_1_rmse: 0.75593 |  0:11:00s
epoch 31 | loss: 0.41487 | val_0_rmse: 0.65175 | val_1_rmse: 0.74014 |  0:11:21s
epoch 32 | loss: 0.41375 | val_0_rmse: 0.65603 | val_1_rmse: 0.7467  |  0:11:43s
epoch 33 | loss: 0.41372 | val_0_rmse: 0.68134 | val_1_rmse: 0.76221 |  0:12:04s
epoch 34 | loss: 0.40846 | val_0_rmse: 0.67349 | val_1_rmse: 0.74617 |  0:12:25s
epoch 35 | loss: 0.40744 | val_0_rmse: 0.67187 | val_1_rmse: 0.75036 |  0:12:47s
epoch 36 | loss: 0.40233 | val_0_rmse: 0.70043 | val_1_rmse: 0.78302 |  0:13:08s
epoch 37 | loss: 0.40514 | val_0_rmse: 0.68313 | val_1_rmse: 0.76111 |  0:13:29s
epoch 38 | loss: 0.46169 | val_0_rmse: 0.67307 | val_1_rmse: 0.74647 |  0:13:51s
epoch 39 | loss: 0.42173 | val_0_rmse: 0.65628 | val_1_rmse: 0.74196 |  0:14:12s
epoch 40 | loss: 0.40938 | val_0_rmse: 0.65198 | val_1_rmse: 0.74412 |  0:14:33s
epoch 41 | loss: 0.40128 | val_0_rmse: 0.64133 | val_1_rmse: 0.72887 |  0:14:55s
epoch 42 | loss: 0.39676 | val_0_rmse: 0.65149 | val_1_rmse: 0.75349 |  0:15:16s
epoch 43 | loss: 0.3941  | val_0_rmse: 0.64712 | val_1_rmse: 0.73571 |  0:15:37s
epoch 44 | loss: 0.39094 | val_0_rmse: 0.64146 | val_1_rmse: 0.7423  |  0:15:59s
epoch 45 | loss: 0.38779 | val_0_rmse: 0.6419  | val_1_rmse: 0.73837 |  0:16:20s
epoch 46 | loss: 0.38677 | val_0_rmse: 0.64424 | val_1_rmse: 0.73425 |  0:16:41s
epoch 47 | loss: 0.38497 | val_0_rmse: 0.64162 | val_1_rmse: 0.74178 |  0:17:03s
epoch 48 | loss: 0.38431 | val_0_rmse: 0.64813 | val_1_rmse: 0.75455 |  0:17:24s
epoch 49 | loss: 0.382   | val_0_rmse: 0.64858 | val_1_rmse: 0.76093 |  0:17:45s
epoch 50 | loss: 0.38278 | val_0_rmse: 0.66214 | val_1_rmse: 0.7759  |  0:18:07s
epoch 51 | loss: 0.37977 | val_0_rmse: 0.63505 | val_1_rmse: 0.74691 |  0:18:28s
epoch 52 | loss: 0.3764  | val_0_rmse: 0.63872 | val_1_rmse: 0.74915 |  0:18:49s
epoch 53 | loss: 0.37621 | val_0_rmse: 0.63781 | val_1_rmse: 0.74679 |  0:19:11s
epoch 54 | loss: 0.37326 | val_0_rmse: 0.64714 | val_1_rmse: 0.73931 |  0:19:32s
epoch 55 | loss: 0.37623 | val_0_rmse: 2.22238 | val_1_rmse: 0.7422  |  0:19:54s
epoch 56 | loss: 0.37577 | val_0_rmse: 0.66423 | val_1_rmse: 0.78417 |  0:20:15s
epoch 57 | loss: 0.37264 | val_0_rmse: 0.63875 | val_1_rmse: 0.7552  |  0:20:36s
epoch 58 | loss: 0.37816 | val_0_rmse: 0.65841 | val_1_rmse: 0.76025 |  0:20:58s
epoch 59 | loss: 0.40984 | val_0_rmse: 0.65236 | val_1_rmse: 0.76295 |  0:21:19s
epoch 60 | loss: 0.38259 | val_0_rmse: 0.66523 | val_1_rmse: 0.77822 |  0:21:41s
epoch 61 | loss: 0.3767  | val_0_rmse: 0.64921 | val_1_rmse: 0.7583  |  0:22:02s
epoch 62 | loss: 0.36991 | val_0_rmse: 0.63685 | val_1_rmse: 0.75231 |  0:22:23s
epoch 63 | loss: 0.36894 | val_0_rmse: 0.64034 | val_1_rmse: 0.75782 |  0:22:45s
epoch 64 | loss: 0.36608 | val_0_rmse: 0.69973 | val_1_rmse: 0.78374 |  0:23:06s
epoch 65 | loss: 0.36618 | val_0_rmse: 0.63962 | val_1_rmse: 0.76055 |  0:23:27s
epoch 66 | loss: 0.36688 | val_0_rmse: 0.63466 | val_1_rmse: 0.75455 |  0:23:49s
epoch 67 | loss: 0.3636  | val_0_rmse: 0.62916 | val_1_rmse: 0.75223 |  0:24:10s
epoch 68 | loss: 0.36499 | val_0_rmse: 0.89773 | val_1_rmse: 1.07043 |  0:24:32s
epoch 69 | loss: 0.3652  | val_0_rmse: 0.64243 | val_1_rmse: 0.75876 |  0:24:53s
epoch 70 | loss: 0.36404 | val_0_rmse: 0.63242 | val_1_rmse: 0.75532 |  0:25:15s
epoch 71 | loss: 0.35899 | val_0_rmse: 0.63826 | val_1_rmse: 0.77159 |  0:25:36s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.72887
Best weights from best epoch are automatically used!
ended training at: 06:07:24
Feature importance:
Mean squared error is of 0.06605967576537292
Mean absolute error:0.1750042492248134
MAPE:0.18918840746383467
R2 score:0.4827170897072067
------------------------------------------------------------------
