TabNet Logs:

Saving copy of script...
In this script all the datasets are used and the features are normalized using the logarithmic function and SS Normalization
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:26:59
epoch 0  | loss: 0.76268 | val_0_rmse: 0.85372 | val_1_rmse: 0.84843 |  0:00:03s
epoch 1  | loss: 0.48662 | val_0_rmse: 0.66259 | val_1_rmse: 0.66056 |  0:00:04s
epoch 2  | loss: 0.42963 | val_0_rmse: 0.63281 | val_1_rmse: 0.62567 |  0:00:06s
epoch 3  | loss: 0.42964 | val_0_rmse: 0.63253 | val_1_rmse: 0.61866 |  0:00:07s
epoch 4  | loss: 0.41004 | val_0_rmse: 0.63296 | val_1_rmse: 0.62589 |  0:00:09s
epoch 5  | loss: 0.40335 | val_0_rmse: 0.60862 | val_1_rmse: 0.59209 |  0:00:10s
epoch 6  | loss: 0.38982 | val_0_rmse: 0.5991  | val_1_rmse: 0.588   |  0:00:12s
epoch 7  | loss: 0.37746 | val_0_rmse: 0.59965 | val_1_rmse: 0.58945 |  0:00:13s
epoch 8  | loss: 0.38085 | val_0_rmse: 0.60938 | val_1_rmse: 0.59801 |  0:00:15s
epoch 9  | loss: 0.38194 | val_0_rmse: 0.60409 | val_1_rmse: 0.59029 |  0:00:16s
epoch 10 | loss: 0.37181 | val_0_rmse: 0.59015 | val_1_rmse: 0.57668 |  0:00:18s
epoch 11 | loss: 0.36945 | val_0_rmse: 0.58546 | val_1_rmse: 0.57708 |  0:00:19s
epoch 12 | loss: 0.37022 | val_0_rmse: 0.58917 | val_1_rmse: 0.58022 |  0:00:21s
epoch 13 | loss: 0.36032 | val_0_rmse: 0.57992 | val_1_rmse: 0.57248 |  0:00:22s
epoch 14 | loss: 0.38196 | val_0_rmse: 0.60335 | val_1_rmse: 0.59556 |  0:00:23s
epoch 15 | loss: 0.37704 | val_0_rmse: 0.58701 | val_1_rmse: 0.5769  |  0:00:25s
epoch 16 | loss: 0.36391 | val_0_rmse: 0.57844 | val_1_rmse: 0.5719  |  0:00:26s
epoch 17 | loss: 0.35654 | val_0_rmse: 0.61098 | val_1_rmse: 0.595   |  0:00:28s
epoch 18 | loss: 0.36749 | val_0_rmse: 0.62035 | val_1_rmse: 0.6177  |  0:00:29s
epoch 19 | loss: 0.37005 | val_0_rmse: 0.59247 | val_1_rmse: 0.58438 |  0:00:31s
epoch 20 | loss: 0.3569  | val_0_rmse: 0.58922 | val_1_rmse: 0.57893 |  0:00:32s
epoch 21 | loss: 0.34966 | val_0_rmse: 0.57247 | val_1_rmse: 0.56171 |  0:00:34s
epoch 22 | loss: 0.34949 | val_0_rmse: 0.57692 | val_1_rmse: 0.57122 |  0:00:35s
epoch 23 | loss: 0.34359 | val_0_rmse: 0.58516 | val_1_rmse: 0.5797  |  0:00:37s
epoch 24 | loss: 0.35299 | val_0_rmse: 0.56934 | val_1_rmse: 0.56157 |  0:00:38s
epoch 25 | loss: 0.35594 | val_0_rmse: 0.57573 | val_1_rmse: 0.56133 |  0:00:40s
epoch 26 | loss: 0.34955 | val_0_rmse: 0.58727 | val_1_rmse: 0.57653 |  0:00:41s
epoch 27 | loss: 0.34919 | val_0_rmse: 0.56258 | val_1_rmse: 0.55159 |  0:00:42s
epoch 28 | loss: 0.34646 | val_0_rmse: 0.57429 | val_1_rmse: 0.56112 |  0:00:44s
epoch 29 | loss: 0.34835 | val_0_rmse: 0.57472 | val_1_rmse: 0.56616 |  0:00:45s
epoch 30 | loss: 0.35292 | val_0_rmse: 0.57083 | val_1_rmse: 0.55669 |  0:00:47s
epoch 31 | loss: 0.34843 | val_0_rmse: 0.60124 | val_1_rmse: 0.58662 |  0:00:48s
epoch 32 | loss: 0.34411 | val_0_rmse: 0.57875 | val_1_rmse: 0.56682 |  0:00:50s
epoch 33 | loss: 0.33771 | val_0_rmse: 0.56454 | val_1_rmse: 0.54935 |  0:00:51s
epoch 34 | loss: 0.34251 | val_0_rmse: 0.56675 | val_1_rmse: 0.55535 |  0:00:53s
epoch 35 | loss: 0.34746 | val_0_rmse: 0.57096 | val_1_rmse: 0.55645 |  0:00:54s
epoch 36 | loss: 0.35618 | val_0_rmse: 0.58237 | val_1_rmse: 0.57174 |  0:00:56s
epoch 37 | loss: 0.35156 | val_0_rmse: 0.5745  | val_1_rmse: 0.56573 |  0:00:57s
epoch 38 | loss: 0.35783 | val_0_rmse: 0.58207 | val_1_rmse: 0.56969 |  0:00:59s
epoch 39 | loss: 0.33812 | val_0_rmse: 0.55722 | val_1_rmse: 0.54749 |  0:01:00s
epoch 40 | loss: 0.33554 | val_0_rmse: 0.55805 | val_1_rmse: 0.54358 |  0:01:02s
epoch 41 | loss: 0.32498 | val_0_rmse: 0.55252 | val_1_rmse: 0.54084 |  0:01:03s
epoch 42 | loss: 0.32978 | val_0_rmse: 0.55944 | val_1_rmse: 0.54741 |  0:01:04s
epoch 43 | loss: 0.32732 | val_0_rmse: 0.57304 | val_1_rmse: 0.5616  |  0:01:06s
epoch 44 | loss: 0.33748 | val_0_rmse: 0.57802 | val_1_rmse: 0.56361 |  0:01:07s
epoch 45 | loss: 0.33695 | val_0_rmse: 0.55792 | val_1_rmse: 0.54487 |  0:01:09s
epoch 46 | loss: 0.33103 | val_0_rmse: 0.56093 | val_1_rmse: 0.54736 |  0:01:10s
epoch 47 | loss: 0.32301 | val_0_rmse: 0.55076 | val_1_rmse: 0.53779 |  0:01:12s
epoch 48 | loss: 0.32497 | val_0_rmse: 0.55978 | val_1_rmse: 0.54876 |  0:01:13s
epoch 49 | loss: 0.33334 | val_0_rmse: 0.58407 | val_1_rmse: 0.56932 |  0:01:15s
epoch 50 | loss: 0.35369 | val_0_rmse: 0.56643 | val_1_rmse: 0.55201 |  0:01:16s
epoch 51 | loss: 0.33407 | val_0_rmse: 0.55868 | val_1_rmse: 0.54616 |  0:01:18s
epoch 52 | loss: 0.33913 | val_0_rmse: 0.55715 | val_1_rmse: 0.54485 |  0:01:19s
epoch 53 | loss: 0.32802 | val_0_rmse: 0.55698 | val_1_rmse: 0.54573 |  0:01:20s
epoch 54 | loss: 0.32794 | val_0_rmse: 0.553   | val_1_rmse: 0.53929 |  0:01:22s
epoch 55 | loss: 0.32338 | val_0_rmse: 0.55254 | val_1_rmse: 0.54586 |  0:01:23s
epoch 56 | loss: 0.32469 | val_0_rmse: 0.56418 | val_1_rmse: 0.55054 |  0:01:25s
epoch 57 | loss: 0.34057 | val_0_rmse: 0.58358 | val_1_rmse: 0.56728 |  0:01:26s
epoch 58 | loss: 0.33562 | val_0_rmse: 0.57953 | val_1_rmse: 0.56695 |  0:01:28s
epoch 59 | loss: 0.33322 | val_0_rmse: 0.57958 | val_1_rmse: 0.56616 |  0:01:29s
epoch 60 | loss: 0.33532 | val_0_rmse: 0.58096 | val_1_rmse: 0.56804 |  0:01:31s
epoch 61 | loss: 0.32535 | val_0_rmse: 0.56376 | val_1_rmse: 0.55556 |  0:01:32s
epoch 62 | loss: 0.33135 | val_0_rmse: 0.54823 | val_1_rmse: 0.54271 |  0:01:34s
epoch 63 | loss: 0.32655 | val_0_rmse: 0.54883 | val_1_rmse: 0.54125 |  0:01:35s
epoch 64 | loss: 0.32791 | val_0_rmse: 0.54591 | val_1_rmse: 0.53806 |  0:01:37s
epoch 65 | loss: 0.31794 | val_0_rmse: 0.55667 | val_1_rmse: 0.54593 |  0:01:38s
epoch 66 | loss: 0.33048 | val_0_rmse: 0.5451  | val_1_rmse: 0.53774 |  0:01:39s
epoch 67 | loss: 0.32231 | val_0_rmse: 0.55024 | val_1_rmse: 0.5414  |  0:01:41s
epoch 68 | loss: 0.32038 | val_0_rmse: 0.5417  | val_1_rmse: 0.53302 |  0:01:42s
epoch 69 | loss: 0.31447 | val_0_rmse: 0.55494 | val_1_rmse: 0.5479  |  0:01:44s
epoch 70 | loss: 0.31316 | val_0_rmse: 0.5602  | val_1_rmse: 0.55693 |  0:01:45s
epoch 71 | loss: 0.32664 | val_0_rmse: 0.54585 | val_1_rmse: 0.5357  |  0:01:47s
epoch 72 | loss: 0.31321 | val_0_rmse: 0.53705 | val_1_rmse: 0.53049 |  0:01:48s
epoch 73 | loss: 0.3168  | val_0_rmse: 0.56694 | val_1_rmse: 0.55933 |  0:01:50s
epoch 74 | loss: 0.31811 | val_0_rmse: 0.54738 | val_1_rmse: 0.5426  |  0:01:51s
epoch 75 | loss: 0.32295 | val_0_rmse: 0.55435 | val_1_rmse: 0.54495 |  0:01:53s
epoch 76 | loss: 0.31977 | val_0_rmse: 0.55483 | val_1_rmse: 0.55076 |  0:01:54s
epoch 77 | loss: 0.32129 | val_0_rmse: 0.54476 | val_1_rmse: 0.53523 |  0:01:56s
epoch 78 | loss: 0.31725 | val_0_rmse: 0.54519 | val_1_rmse: 0.53606 |  0:01:57s
epoch 79 | loss: 0.31155 | val_0_rmse: 0.5401  | val_1_rmse: 0.53297 |  0:01:58s
epoch 80 | loss: 0.32081 | val_0_rmse: 0.57122 | val_1_rmse: 0.56564 |  0:02:00s
epoch 81 | loss: 0.31946 | val_0_rmse: 0.57889 | val_1_rmse: 0.5729  |  0:02:01s
epoch 82 | loss: 0.31369 | val_0_rmse: 0.53691 | val_1_rmse: 0.53362 |  0:02:03s
epoch 83 | loss: 0.31148 | val_0_rmse: 0.53629 | val_1_rmse: 0.52771 |  0:02:04s
epoch 84 | loss: 0.31354 | val_0_rmse: 0.54112 | val_1_rmse: 0.53156 |  0:02:06s
epoch 85 | loss: 0.30617 | val_0_rmse: 0.5397  | val_1_rmse: 0.52872 |  0:02:07s
epoch 86 | loss: 0.30633 | val_0_rmse: 0.54168 | val_1_rmse: 0.52926 |  0:02:09s
epoch 87 | loss: 0.31184 | val_0_rmse: 0.53454 | val_1_rmse: 0.52856 |  0:02:10s
epoch 88 | loss: 0.31774 | val_0_rmse: 0.55618 | val_1_rmse: 0.55233 |  0:02:12s
epoch 89 | loss: 0.31964 | val_0_rmse: 0.55795 | val_1_rmse: 0.55622 |  0:02:13s
epoch 90 | loss: 0.32847 | val_0_rmse: 0.5681  | val_1_rmse: 0.55702 |  0:02:14s
epoch 91 | loss: 0.32348 | val_0_rmse: 0.56011 | val_1_rmse: 0.54796 |  0:02:16s
epoch 92 | loss: 0.31971 | val_0_rmse: 0.53728 | val_1_rmse: 0.53177 |  0:02:17s
epoch 93 | loss: 0.31118 | val_0_rmse: 0.53978 | val_1_rmse: 0.53038 |  0:02:19s
epoch 94 | loss: 0.31816 | val_0_rmse: 0.54994 | val_1_rmse: 0.54471 |  0:02:20s
epoch 95 | loss: 0.32026 | val_0_rmse: 0.54051 | val_1_rmse: 0.53399 |  0:02:22s
epoch 96 | loss: 0.30757 | val_0_rmse: 0.53546 | val_1_rmse: 0.52623 |  0:02:23s
epoch 97 | loss: 0.30964 | val_0_rmse: 0.5456  | val_1_rmse: 0.53806 |  0:02:25s
epoch 98 | loss: 0.33115 | val_0_rmse: 0.5686  | val_1_rmse: 0.55641 |  0:02:26s
epoch 99 | loss: 0.31585 | val_0_rmse: 0.52958 | val_1_rmse: 0.52537 |  0:02:28s
epoch 100| loss: 0.31006 | val_0_rmse: 0.55035 | val_1_rmse: 0.54411 |  0:02:29s
epoch 101| loss: 0.31832 | val_0_rmse: 0.53766 | val_1_rmse: 0.53226 |  0:02:31s
epoch 102| loss: 0.30955 | val_0_rmse: 0.54203 | val_1_rmse: 0.54146 |  0:02:32s
epoch 103| loss: 0.30844 | val_0_rmse: 0.53698 | val_1_rmse: 0.53281 |  0:02:33s
epoch 104| loss: 0.31083 | val_0_rmse: 0.53986 | val_1_rmse: 0.53475 |  0:02:35s
epoch 105| loss: 0.30855 | val_0_rmse: 0.52825 | val_1_rmse: 0.5257  |  0:02:36s
epoch 106| loss: 0.30618 | val_0_rmse: 0.53095 | val_1_rmse: 0.52777 |  0:02:38s
epoch 107| loss: 0.31062 | val_0_rmse: 0.52763 | val_1_rmse: 0.52355 |  0:02:39s
epoch 108| loss: 0.30351 | val_0_rmse: 0.53445 | val_1_rmse: 0.52743 |  0:02:41s
epoch 109| loss: 0.31277 | val_0_rmse: 0.53493 | val_1_rmse: 0.53123 |  0:02:42s
epoch 110| loss: 0.3225  | val_0_rmse: 0.56498 | val_1_rmse: 0.55441 |  0:02:44s
epoch 111| loss: 0.31188 | val_0_rmse: 0.52919 | val_1_rmse: 0.52435 |  0:02:45s
epoch 112| loss: 0.30307 | val_0_rmse: 0.52464 | val_1_rmse: 0.51891 |  0:02:47s
epoch 113| loss: 0.30444 | val_0_rmse: 0.55427 | val_1_rmse: 0.54945 |  0:02:48s
epoch 114| loss: 0.3126  | val_0_rmse: 0.54097 | val_1_rmse: 0.53901 |  0:02:50s
epoch 115| loss: 0.30542 | val_0_rmse: 0.53139 | val_1_rmse: 0.53058 |  0:02:51s
epoch 116| loss: 0.30665 | val_0_rmse: 0.53705 | val_1_rmse: 0.53118 |  0:02:52s
epoch 117| loss: 0.3007  | val_0_rmse: 0.52917 | val_1_rmse: 0.52453 |  0:02:54s
epoch 118| loss: 0.30241 | val_0_rmse: 0.52464 | val_1_rmse: 0.52497 |  0:02:55s
epoch 119| loss: 0.29916 | val_0_rmse: 0.52824 | val_1_rmse: 0.52458 |  0:02:57s
epoch 120| loss: 0.29966 | val_0_rmse: 0.54474 | val_1_rmse: 0.54291 |  0:02:58s
epoch 121| loss: 0.29519 | val_0_rmse: 0.52728 | val_1_rmse: 0.53011 |  0:03:00s
epoch 122| loss: 0.30272 | val_0_rmse: 0.54426 | val_1_rmse: 0.54069 |  0:03:01s
epoch 123| loss: 0.30789 | val_0_rmse: 0.54191 | val_1_rmse: 0.53704 |  0:03:03s
epoch 124| loss: 0.31408 | val_0_rmse: 0.53635 | val_1_rmse: 0.53203 |  0:03:04s
epoch 125| loss: 0.30009 | val_0_rmse: 0.52846 | val_1_rmse: 0.52769 |  0:03:05s
epoch 126| loss: 0.30187 | val_0_rmse: 0.52614 | val_1_rmse: 0.52506 |  0:03:07s
epoch 127| loss: 0.29866 | val_0_rmse: 0.52257 | val_1_rmse: 0.52164 |  0:03:08s
epoch 128| loss: 0.29618 | val_0_rmse: 0.52168 | val_1_rmse: 0.52413 |  0:03:10s
epoch 129| loss: 0.30812 | val_0_rmse: 0.52236 | val_1_rmse: 0.52106 |  0:03:11s
epoch 130| loss: 0.30403 | val_0_rmse: 0.52148 | val_1_rmse: 0.52068 |  0:03:13s
epoch 131| loss: 0.29874 | val_0_rmse: 0.53035 | val_1_rmse: 0.52519 |  0:03:14s
epoch 132| loss: 0.298   | val_0_rmse: 0.52296 | val_1_rmse: 0.52057 |  0:03:16s
epoch 133| loss: 0.30404 | val_0_rmse: 0.52701 | val_1_rmse: 0.52568 |  0:03:17s
epoch 134| loss: 0.29297 | val_0_rmse: 0.52837 | val_1_rmse: 0.53355 |  0:03:19s
epoch 135| loss: 0.29439 | val_0_rmse: 0.5245  | val_1_rmse: 0.52511 |  0:03:20s
epoch 136| loss: 0.29874 | val_0_rmse: 0.51545 | val_1_rmse: 0.51796 |  0:03:21s
epoch 137| loss: 0.29276 | val_0_rmse: 0.52867 | val_1_rmse: 0.53369 |  0:03:23s
epoch 138| loss: 0.29489 | val_0_rmse: 0.52967 | val_1_rmse: 0.52934 |  0:03:24s
epoch 139| loss: 0.30002 | val_0_rmse: 0.52081 | val_1_rmse: 0.52293 |  0:03:26s
epoch 140| loss: 0.29183 | val_0_rmse: 0.52117 | val_1_rmse: 0.51973 |  0:03:27s
epoch 141| loss: 0.28956 | val_0_rmse: 0.54925 | val_1_rmse: 0.54624 |  0:03:29s
epoch 142| loss: 0.2992  | val_0_rmse: 0.51462 | val_1_rmse: 0.51547 |  0:03:30s
epoch 143| loss: 0.2921  | val_0_rmse: 0.51924 | val_1_rmse: 0.523   |  0:03:32s
epoch 144| loss: 0.29056 | val_0_rmse: 0.54345 | val_1_rmse: 0.54307 |  0:03:33s
epoch 145| loss: 0.29542 | val_0_rmse: 0.53233 | val_1_rmse: 0.53355 |  0:03:35s
epoch 146| loss: 0.30008 | val_0_rmse: 0.51494 | val_1_rmse: 0.51346 |  0:03:36s
epoch 147| loss: 0.28788 | val_0_rmse: 0.51961 | val_1_rmse: 0.52592 |  0:03:38s
epoch 148| loss: 0.30383 | val_0_rmse: 0.51558 | val_1_rmse: 0.51557 |  0:03:39s
epoch 149| loss: 0.28821 | val_0_rmse: 0.5118  | val_1_rmse: 0.52098 |  0:03:41s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.51346
Best weights from best epoch are automatically used!
ended training at: 05:30:41
Feature importance:
[('Area', 0.27836602327977217), ('Baths', 0.00016367930474602636), ('Beds', 0.018822959549424447), ('Latitude', 0.25832767517185573), ('Longitude', 0.224007685372354), ('Month', 0.118880754284535), ('Year', 0.10143122303731263)]
Mean squared error is of 6273796004.997665
Mean absolute error:54104.98904766372
MAPE:0.17277296184809623
R2 score:0.7216169726370314
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:30:41
epoch 0  | loss: 0.77904 | val_0_rmse: 0.82128 | val_1_rmse: 0.84082 |  0:00:01s
epoch 1  | loss: 0.52558 | val_0_rmse: 0.68355 | val_1_rmse: 0.70023 |  0:00:02s
epoch 2  | loss: 0.47169 | val_0_rmse: 0.67433 | val_1_rmse: 0.68606 |  0:00:04s
epoch 3  | loss: 0.45221 | val_0_rmse: 0.67736 | val_1_rmse: 0.68575 |  0:00:05s
epoch 4  | loss: 0.44309 | val_0_rmse: 0.66647 | val_1_rmse: 0.68252 |  0:00:07s
epoch 5  | loss: 0.41881 | val_0_rmse: 0.60726 | val_1_rmse: 0.61625 |  0:00:08s
epoch 6  | loss: 0.40291 | val_0_rmse: 0.63778 | val_1_rmse: 0.65053 |  0:00:10s
epoch 7  | loss: 0.39922 | val_0_rmse: 0.61624 | val_1_rmse: 0.62789 |  0:00:11s
epoch 8  | loss: 0.38771 | val_0_rmse: 0.59261 | val_1_rmse: 0.60437 |  0:00:13s
epoch 9  | loss: 0.38786 | val_0_rmse: 0.60001 | val_1_rmse: 0.61444 |  0:00:14s
epoch 10 | loss: 0.38251 | val_0_rmse: 0.59018 | val_1_rmse: 0.60165 |  0:00:16s
epoch 11 | loss: 0.36926 | val_0_rmse: 0.58221 | val_1_rmse: 0.59624 |  0:00:17s
epoch 12 | loss: 0.36517 | val_0_rmse: 0.59797 | val_1_rmse: 0.61067 |  0:00:19s
epoch 13 | loss: 0.37795 | val_0_rmse: 0.60582 | val_1_rmse: 0.61666 |  0:00:20s
epoch 14 | loss: 0.36645 | val_0_rmse: 0.57686 | val_1_rmse: 0.58983 |  0:00:22s
epoch 15 | loss: 0.35249 | val_0_rmse: 0.58083 | val_1_rmse: 0.59358 |  0:00:23s
epoch 16 | loss: 0.3726  | val_0_rmse: 0.58886 | val_1_rmse: 0.59872 |  0:00:25s
epoch 17 | loss: 0.36053 | val_0_rmse: 0.5669  | val_1_rmse: 0.57966 |  0:00:26s
epoch 18 | loss: 0.35606 | val_0_rmse: 0.56199 | val_1_rmse: 0.57426 |  0:00:27s
epoch 19 | loss: 0.35654 | val_0_rmse: 0.59245 | val_1_rmse: 0.60282 |  0:00:29s
epoch 20 | loss: 0.36287 | val_0_rmse: 0.56404 | val_1_rmse: 0.57709 |  0:00:30s
epoch 21 | loss: 0.35246 | val_0_rmse: 0.59316 | val_1_rmse: 0.60781 |  0:00:32s
epoch 22 | loss: 0.35798 | val_0_rmse: 0.57372 | val_1_rmse: 0.59057 |  0:00:33s
epoch 23 | loss: 0.35353 | val_0_rmse: 0.56908 | val_1_rmse: 0.58221 |  0:00:35s
epoch 24 | loss: 0.34991 | val_0_rmse: 0.56404 | val_1_rmse: 0.57883 |  0:00:36s
epoch 25 | loss: 0.34738 | val_0_rmse: 0.56477 | val_1_rmse: 0.57629 |  0:00:38s
epoch 26 | loss: 0.34433 | val_0_rmse: 0.58046 | val_1_rmse: 0.59483 |  0:00:39s
epoch 27 | loss: 0.35447 | val_0_rmse: 0.57362 | val_1_rmse: 0.59077 |  0:00:41s
epoch 28 | loss: 0.34539 | val_0_rmse: 0.56046 | val_1_rmse: 0.57426 |  0:00:42s
epoch 29 | loss: 0.34176 | val_0_rmse: 0.56202 | val_1_rmse: 0.57205 |  0:00:44s
epoch 30 | loss: 0.33518 | val_0_rmse: 0.56948 | val_1_rmse: 0.58579 |  0:00:45s
epoch 31 | loss: 0.3456  | val_0_rmse: 0.56259 | val_1_rmse: 0.58361 |  0:00:47s
epoch 32 | loss: 0.33845 | val_0_rmse: 0.5617  | val_1_rmse: 0.5752  |  0:00:48s
epoch 33 | loss: 0.3405  | val_0_rmse: 0.56672 | val_1_rmse: 0.58231 |  0:00:49s
epoch 34 | loss: 0.33437 | val_0_rmse: 0.56402 | val_1_rmse: 0.57958 |  0:00:51s
epoch 35 | loss: 0.32765 | val_0_rmse: 0.55044 | val_1_rmse: 0.56698 |  0:00:52s
epoch 36 | loss: 0.32999 | val_0_rmse: 0.57012 | val_1_rmse: 0.58133 |  0:00:54s
epoch 37 | loss: 0.34009 | val_0_rmse: 0.55181 | val_1_rmse: 0.57156 |  0:00:55s
epoch 38 | loss: 0.33907 | val_0_rmse: 0.57575 | val_1_rmse: 0.59014 |  0:00:57s
epoch 39 | loss: 0.33321 | val_0_rmse: 0.54924 | val_1_rmse: 0.5628  |  0:00:58s
epoch 40 | loss: 0.33381 | val_0_rmse: 0.55621 | val_1_rmse: 0.56884 |  0:01:00s
epoch 41 | loss: 0.34264 | val_0_rmse: 0.56906 | val_1_rmse: 0.58532 |  0:01:01s
epoch 42 | loss: 0.33657 | val_0_rmse: 0.55172 | val_1_rmse: 0.56849 |  0:01:03s
epoch 43 | loss: 0.33711 | val_0_rmse: 0.5654  | val_1_rmse: 0.58256 |  0:01:04s
epoch 44 | loss: 0.33987 | val_0_rmse: 0.56346 | val_1_rmse: 0.58012 |  0:01:06s
epoch 45 | loss: 0.33434 | val_0_rmse: 0.56331 | val_1_rmse: 0.57462 |  0:01:07s
epoch 46 | loss: 0.33911 | val_0_rmse: 0.55479 | val_1_rmse: 0.57611 |  0:01:09s
epoch 47 | loss: 0.32561 | val_0_rmse: 0.55754 | val_1_rmse: 0.57559 |  0:01:10s
epoch 48 | loss: 0.33213 | val_0_rmse: 0.56729 | val_1_rmse: 0.57907 |  0:01:12s
epoch 49 | loss: 0.33209 | val_0_rmse: 0.55066 | val_1_rmse: 0.5678  |  0:01:13s
epoch 50 | loss: 0.32166 | val_0_rmse: 0.55401 | val_1_rmse: 0.57536 |  0:01:14s
epoch 51 | loss: 0.32171 | val_0_rmse: 0.54351 | val_1_rmse: 0.56198 |  0:01:16s
epoch 52 | loss: 0.31933 | val_0_rmse: 0.54298 | val_1_rmse: 0.56008 |  0:01:17s
epoch 53 | loss: 0.32323 | val_0_rmse: 0.54839 | val_1_rmse: 0.56378 |  0:01:19s
epoch 54 | loss: 0.33791 | val_0_rmse: 0.56467 | val_1_rmse: 0.58013 |  0:01:20s
epoch 55 | loss: 0.32809 | val_0_rmse: 0.55459 | val_1_rmse: 0.5752  |  0:01:22s
epoch 56 | loss: 0.32695 | val_0_rmse: 0.53917 | val_1_rmse: 0.55883 |  0:01:23s
epoch 57 | loss: 0.32986 | val_0_rmse: 0.54169 | val_1_rmse: 0.55959 |  0:01:25s
epoch 58 | loss: 0.32348 | val_0_rmse: 0.55201 | val_1_rmse: 0.57261 |  0:01:26s
epoch 59 | loss: 0.33074 | val_0_rmse: 0.55185 | val_1_rmse: 0.56589 |  0:01:28s
epoch 60 | loss: 0.32751 | val_0_rmse: 0.55283 | val_1_rmse: 0.57254 |  0:01:29s
epoch 61 | loss: 0.32428 | val_0_rmse: 0.55174 | val_1_rmse: 0.57015 |  0:01:31s
epoch 62 | loss: 0.32026 | val_0_rmse: 0.54578 | val_1_rmse: 0.56724 |  0:01:32s
epoch 63 | loss: 0.31609 | val_0_rmse: 0.55654 | val_1_rmse: 0.57692 |  0:01:33s
epoch 64 | loss: 0.32497 | val_0_rmse: 0.54903 | val_1_rmse: 0.5592  |  0:01:35s
epoch 65 | loss: 0.32501 | val_0_rmse: 0.55997 | val_1_rmse: 0.5798  |  0:01:36s
epoch 66 | loss: 0.33004 | val_0_rmse: 0.55677 | val_1_rmse: 0.57682 |  0:01:38s
epoch 67 | loss: 0.32816 | val_0_rmse: 0.54866 | val_1_rmse: 0.56728 |  0:01:39s
epoch 68 | loss: 0.31841 | val_0_rmse: 0.53906 | val_1_rmse: 0.55698 |  0:01:41s
epoch 69 | loss: 0.31792 | val_0_rmse: 0.57385 | val_1_rmse: 0.59116 |  0:01:42s
epoch 70 | loss: 0.32595 | val_0_rmse: 0.5552  | val_1_rmse: 0.57576 |  0:01:44s
epoch 71 | loss: 0.32117 | val_0_rmse: 0.55214 | val_1_rmse: 0.57328 |  0:01:45s
epoch 72 | loss: 0.32142 | val_0_rmse: 0.5399  | val_1_rmse: 0.56211 |  0:01:47s
epoch 73 | loss: 0.3188  | val_0_rmse: 0.56354 | val_1_rmse: 0.58573 |  0:01:48s
epoch 74 | loss: 0.31604 | val_0_rmse: 0.54327 | val_1_rmse: 0.56189 |  0:01:50s
epoch 75 | loss: 0.31111 | val_0_rmse: 0.53175 | val_1_rmse: 0.55108 |  0:01:51s
epoch 76 | loss: 0.30766 | val_0_rmse: 0.5323  | val_1_rmse: 0.55259 |  0:01:52s
epoch 77 | loss: 0.30512 | val_0_rmse: 0.55471 | val_1_rmse: 0.57798 |  0:01:54s
epoch 78 | loss: 0.31991 | val_0_rmse: 0.55494 | val_1_rmse: 0.57722 |  0:01:55s
epoch 79 | loss: 0.31446 | val_0_rmse: 0.55871 | val_1_rmse: 0.57299 |  0:01:57s
epoch 80 | loss: 0.31818 | val_0_rmse: 0.53651 | val_1_rmse: 0.56122 |  0:01:58s
epoch 81 | loss: 0.3191  | val_0_rmse: 0.53642 | val_1_rmse: 0.55777 |  0:02:00s
epoch 82 | loss: 0.3198  | val_0_rmse: 0.5302  | val_1_rmse: 0.55388 |  0:02:01s
epoch 83 | loss: 0.30687 | val_0_rmse: 0.5302  | val_1_rmse: 0.55105 |  0:02:03s
epoch 84 | loss: 0.30897 | val_0_rmse: 0.54753 | val_1_rmse: 0.57172 |  0:02:04s
epoch 85 | loss: 0.30993 | val_0_rmse: 0.53105 | val_1_rmse: 0.55464 |  0:02:06s
epoch 86 | loss: 0.30713 | val_0_rmse: 0.53453 | val_1_rmse: 0.5582  |  0:02:07s
epoch 87 | loss: 0.31463 | val_0_rmse: 0.55698 | val_1_rmse: 0.58263 |  0:02:09s
epoch 88 | loss: 0.30778 | val_0_rmse: 0.54068 | val_1_rmse: 0.56584 |  0:02:10s
epoch 89 | loss: 0.30173 | val_0_rmse: 0.52112 | val_1_rmse: 0.54499 |  0:02:11s
epoch 90 | loss: 0.30404 | val_0_rmse: 0.52954 | val_1_rmse: 0.5504  |  0:02:13s
epoch 91 | loss: 0.30203 | val_0_rmse: 0.53206 | val_1_rmse: 0.55552 |  0:02:14s
epoch 92 | loss: 0.31164 | val_0_rmse: 0.53473 | val_1_rmse: 0.55752 |  0:02:16s
epoch 93 | loss: 0.30483 | val_0_rmse: 0.5468  | val_1_rmse: 0.56693 |  0:02:17s
epoch 94 | loss: 0.31178 | val_0_rmse: 0.53298 | val_1_rmse: 0.55639 |  0:02:19s
epoch 95 | loss: 0.3035  | val_0_rmse: 0.53253 | val_1_rmse: 0.55965 |  0:02:20s
epoch 96 | loss: 0.30492 | val_0_rmse: 0.52456 | val_1_rmse: 0.54914 |  0:02:22s
epoch 97 | loss: 0.30792 | val_0_rmse: 0.53488 | val_1_rmse: 0.56142 |  0:02:23s
epoch 98 | loss: 0.31    | val_0_rmse: 0.52554 | val_1_rmse: 0.54766 |  0:02:25s
epoch 99 | loss: 0.29542 | val_0_rmse: 0.5171  | val_1_rmse: 0.54354 |  0:02:26s
epoch 100| loss: 0.29767 | val_0_rmse: 0.53238 | val_1_rmse: 0.55957 |  0:02:28s
epoch 101| loss: 0.3     | val_0_rmse: 0.5263  | val_1_rmse: 0.55381 |  0:02:29s
epoch 102| loss: 0.30466 | val_0_rmse: 0.53073 | val_1_rmse: 0.55629 |  0:02:30s
epoch 103| loss: 0.31547 | val_0_rmse: 0.52535 | val_1_rmse: 0.55465 |  0:02:32s
epoch 104| loss: 0.3004  | val_0_rmse: 0.52192 | val_1_rmse: 0.55182 |  0:02:33s
epoch 105| loss: 0.31011 | val_0_rmse: 0.52038 | val_1_rmse: 0.54924 |  0:02:35s
epoch 106| loss: 0.29786 | val_0_rmse: 0.5327  | val_1_rmse: 0.56079 |  0:02:36s
epoch 107| loss: 0.30333 | val_0_rmse: 0.51888 | val_1_rmse: 0.54729 |  0:02:38s
epoch 108| loss: 0.30022 | val_0_rmse: 0.52196 | val_1_rmse: 0.5485  |  0:02:39s
epoch 109| loss: 0.29314 | val_0_rmse: 0.51928 | val_1_rmse: 0.55231 |  0:02:41s
epoch 110| loss: 0.29695 | val_0_rmse: 0.52429 | val_1_rmse: 0.5521  |  0:02:42s
epoch 111| loss: 0.29933 | val_0_rmse: 0.53552 | val_1_rmse: 0.56777 |  0:02:44s
epoch 112| loss: 0.29937 | val_0_rmse: 0.52483 | val_1_rmse: 0.5533  |  0:02:45s
epoch 113| loss: 0.29991 | val_0_rmse: 0.52228 | val_1_rmse: 0.54904 |  0:02:46s
epoch 114| loss: 0.29748 | val_0_rmse: 0.51664 | val_1_rmse: 0.54652 |  0:02:48s
epoch 115| loss: 0.29742 | val_0_rmse: 0.53465 | val_1_rmse: 0.56293 |  0:02:49s
epoch 116| loss: 0.29531 | val_0_rmse: 0.5341  | val_1_rmse: 0.56601 |  0:02:51s
epoch 117| loss: 0.29286 | val_0_rmse: 0.51679 | val_1_rmse: 0.54823 |  0:02:52s
epoch 118| loss: 0.2951  | val_0_rmse: 0.51615 | val_1_rmse: 0.54767 |  0:02:54s
epoch 119| loss: 0.29093 | val_0_rmse: 0.52761 | val_1_rmse: 0.55828 |  0:02:55s
epoch 120| loss: 0.2977  | val_0_rmse: 0.51078 | val_1_rmse: 0.54276 |  0:02:57s
epoch 121| loss: 0.29182 | val_0_rmse: 0.51973 | val_1_rmse: 0.55537 |  0:02:58s
epoch 122| loss: 0.29412 | val_0_rmse: 0.55141 | val_1_rmse: 0.58074 |  0:03:00s
epoch 123| loss: 0.29538 | val_0_rmse: 0.53355 | val_1_rmse: 0.56428 |  0:03:01s
epoch 124| loss: 0.28894 | val_0_rmse: 0.51519 | val_1_rmse: 0.54945 |  0:03:03s
epoch 125| loss: 0.29323 | val_0_rmse: 0.5318  | val_1_rmse: 0.57043 |  0:03:04s
epoch 126| loss: 0.30086 | val_0_rmse: 0.51205 | val_1_rmse: 0.54513 |  0:03:06s
epoch 127| loss: 0.29371 | val_0_rmse: 0.52041 | val_1_rmse: 0.55086 |  0:03:07s
epoch 128| loss: 0.29313 | val_0_rmse: 0.50568 | val_1_rmse: 0.54165 |  0:03:08s
epoch 129| loss: 0.2884  | val_0_rmse: 0.52832 | val_1_rmse: 0.5617  |  0:03:10s
epoch 130| loss: 0.29563 | val_0_rmse: 0.52692 | val_1_rmse: 0.5557  |  0:03:11s
epoch 131| loss: 0.29143 | val_0_rmse: 0.51822 | val_1_rmse: 0.55377 |  0:03:13s
epoch 132| loss: 0.28688 | val_0_rmse: 0.5157  | val_1_rmse: 0.55346 |  0:03:14s
epoch 133| loss: 0.28767 | val_0_rmse: 0.51606 | val_1_rmse: 0.551   |  0:03:16s
epoch 134| loss: 0.28803 | val_0_rmse: 0.53105 | val_1_rmse: 0.56275 |  0:03:17s
epoch 135| loss: 0.28579 | val_0_rmse: 0.51871 | val_1_rmse: 0.55763 |  0:03:19s
epoch 136| loss: 0.28876 | val_0_rmse: 0.52287 | val_1_rmse: 0.55479 |  0:03:20s
epoch 137| loss: 0.30068 | val_0_rmse: 0.53803 | val_1_rmse: 0.57549 |  0:03:22s
epoch 138| loss: 0.29864 | val_0_rmse: 0.52278 | val_1_rmse: 0.55657 |  0:03:23s
epoch 139| loss: 0.28727 | val_0_rmse: 0.53448 | val_1_rmse: 0.57081 |  0:03:25s
epoch 140| loss: 0.29365 | val_0_rmse: 0.53186 | val_1_rmse: 0.5684  |  0:03:26s
epoch 141| loss: 0.29192 | val_0_rmse: 0.52619 | val_1_rmse: 0.56082 |  0:03:28s
epoch 142| loss: 0.2887  | val_0_rmse: 0.53702 | val_1_rmse: 0.57347 |  0:03:29s
epoch 143| loss: 0.28462 | val_0_rmse: 0.54184 | val_1_rmse: 0.57643 |  0:03:30s
epoch 144| loss: 0.28157 | val_0_rmse: 0.50614 | val_1_rmse: 0.5492  |  0:03:32s
epoch 145| loss: 0.28689 | val_0_rmse: 0.52632 | val_1_rmse: 0.55524 |  0:03:33s
epoch 146| loss: 0.28644 | val_0_rmse: 0.50934 | val_1_rmse: 0.55367 |  0:03:35s
epoch 147| loss: 0.30457 | val_0_rmse: 0.52497 | val_1_rmse: 0.56059 |  0:03:36s
epoch 148| loss: 0.29499 | val_0_rmse: 0.51489 | val_1_rmse: 0.55086 |  0:03:38s
epoch 149| loss: 0.28871 | val_0_rmse: 0.50763 | val_1_rmse: 0.54401 |  0:03:39s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.54165
Best weights from best epoch are automatically used!
ended training at: 05:34:22
Feature importance:
[('Area', 0.24100423818436376), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.370920184393261), ('Longitude', 0.22494135443328694), ('Month', 0.0), ('Year', 0.1631342229890883)]
Mean squared error is of 5931387530.991461
Mean absolute error:52492.727460981885
MAPE:0.16574664526132782
R2 score:0.7334048720724851
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:34:22
epoch 0  | loss: 0.78473 | val_0_rmse: 0.78718 | val_1_rmse: 0.82416 |  0:00:01s
epoch 1  | loss: 0.53096 | val_0_rmse: 0.69545 | val_1_rmse: 0.71613 |  0:00:02s
epoch 2  | loss: 0.46052 | val_0_rmse: 0.70292 | val_1_rmse: 0.73169 |  0:00:04s
epoch 3  | loss: 0.43244 | val_0_rmse: 0.62918 | val_1_rmse: 0.6613  |  0:00:05s
epoch 4  | loss: 0.41716 | val_0_rmse: 0.64399 | val_1_rmse: 0.66582 |  0:00:07s
epoch 5  | loss: 0.41258 | val_0_rmse: 0.61943 | val_1_rmse: 0.64479 |  0:00:08s
epoch 6  | loss: 0.38292 | val_0_rmse: 0.61381 | val_1_rmse: 0.64503 |  0:00:10s
epoch 7  | loss: 0.39099 | val_0_rmse: 0.59855 | val_1_rmse: 0.62176 |  0:00:11s
epoch 8  | loss: 0.37341 | val_0_rmse: 0.58821 | val_1_rmse: 0.60506 |  0:00:13s
epoch 9  | loss: 0.37389 | val_0_rmse: 0.58586 | val_1_rmse: 0.60722 |  0:00:14s
epoch 10 | loss: 0.36598 | val_0_rmse: 0.58529 | val_1_rmse: 0.60565 |  0:00:16s
epoch 11 | loss: 0.37484 | val_0_rmse: 0.58431 | val_1_rmse: 0.60585 |  0:00:17s
epoch 12 | loss: 0.37642 | val_0_rmse: 0.58551 | val_1_rmse: 0.60419 |  0:00:19s
epoch 13 | loss: 0.38014 | val_0_rmse: 0.61099 | val_1_rmse: 0.63137 |  0:00:20s
epoch 14 | loss: 0.36154 | val_0_rmse: 0.58038 | val_1_rmse: 0.59768 |  0:00:22s
epoch 15 | loss: 0.3617  | val_0_rmse: 0.59302 | val_1_rmse: 0.61553 |  0:00:23s
epoch 16 | loss: 0.37345 | val_0_rmse: 0.5891  | val_1_rmse: 0.60765 |  0:00:25s
epoch 17 | loss: 0.36391 | val_0_rmse: 0.56844 | val_1_rmse: 0.58889 |  0:00:26s
epoch 18 | loss: 0.36825 | val_0_rmse: 0.58254 | val_1_rmse: 0.60356 |  0:00:28s
epoch 19 | loss: 0.35651 | val_0_rmse: 0.58019 | val_1_rmse: 0.60192 |  0:00:29s
epoch 20 | loss: 0.35167 | val_0_rmse: 0.57693 | val_1_rmse: 0.59837 |  0:00:31s
epoch 21 | loss: 0.34706 | val_0_rmse: 0.57858 | val_1_rmse: 0.60343 |  0:00:32s
epoch 22 | loss: 0.3474  | val_0_rmse: 0.57222 | val_1_rmse: 0.59492 |  0:00:33s
epoch 23 | loss: 0.34985 | val_0_rmse: 0.57618 | val_1_rmse: 0.59601 |  0:00:35s
epoch 24 | loss: 0.35441 | val_0_rmse: 0.57711 | val_1_rmse: 0.59778 |  0:00:36s
epoch 25 | loss: 0.34791 | val_0_rmse: 0.57305 | val_1_rmse: 0.59916 |  0:00:38s
epoch 26 | loss: 0.34386 | val_0_rmse: 0.57544 | val_1_rmse: 0.59516 |  0:00:39s
epoch 27 | loss: 0.34534 | val_0_rmse: 0.56712 | val_1_rmse: 0.58858 |  0:00:41s
epoch 28 | loss: 0.34192 | val_0_rmse: 0.57019 | val_1_rmse: 0.59342 |  0:00:42s
epoch 29 | loss: 0.36199 | val_0_rmse: 0.58983 | val_1_rmse: 0.61333 |  0:00:44s
epoch 30 | loss: 0.357   | val_0_rmse: 0.5683  | val_1_rmse: 0.58493 |  0:00:45s
epoch 31 | loss: 0.34688 | val_0_rmse: 0.5667  | val_1_rmse: 0.58695 |  0:00:47s
epoch 32 | loss: 0.3441  | val_0_rmse: 0.57118 | val_1_rmse: 0.58884 |  0:00:48s
epoch 33 | loss: 0.34717 | val_0_rmse: 0.56815 | val_1_rmse: 0.59045 |  0:00:50s
epoch 34 | loss: 0.3434  | val_0_rmse: 0.56932 | val_1_rmse: 0.59187 |  0:00:51s
epoch 35 | loss: 0.34416 | val_0_rmse: 0.57094 | val_1_rmse: 0.5943  |  0:00:53s
epoch 36 | loss: 0.3414  | val_0_rmse: 0.57218 | val_1_rmse: 0.59378 |  0:00:54s
epoch 37 | loss: 0.34052 | val_0_rmse: 0.56438 | val_1_rmse: 0.58637 |  0:00:55s
epoch 38 | loss: 0.33869 | val_0_rmse: 0.57031 | val_1_rmse: 0.59334 |  0:00:57s
epoch 39 | loss: 0.33616 | val_0_rmse: 0.5561  | val_1_rmse: 0.57568 |  0:00:58s
epoch 40 | loss: 0.34321 | val_0_rmse: 0.56885 | val_1_rmse: 0.58807 |  0:01:00s
epoch 41 | loss: 0.33267 | val_0_rmse: 0.56082 | val_1_rmse: 0.58215 |  0:01:02s
epoch 42 | loss: 0.33697 | val_0_rmse: 0.55134 | val_1_rmse: 0.57297 |  0:01:03s
epoch 43 | loss: 0.34249 | val_0_rmse: 0.55908 | val_1_rmse: 0.57568 |  0:01:05s
epoch 44 | loss: 0.34072 | val_0_rmse: 0.59816 | val_1_rmse: 0.62044 |  0:01:06s
epoch 45 | loss: 0.34542 | val_0_rmse: 0.55095 | val_1_rmse: 0.56878 |  0:01:07s
epoch 46 | loss: 0.33741 | val_0_rmse: 0.5736  | val_1_rmse: 0.59803 |  0:01:09s
epoch 47 | loss: 0.33308 | val_0_rmse: 0.56152 | val_1_rmse: 0.58231 |  0:01:10s
epoch 48 | loss: 0.33104 | val_0_rmse: 0.56458 | val_1_rmse: 0.58513 |  0:01:12s
epoch 49 | loss: 0.33227 | val_0_rmse: 0.57689 | val_1_rmse: 0.60238 |  0:01:13s
epoch 50 | loss: 0.33771 | val_0_rmse: 0.55948 | val_1_rmse: 0.58334 |  0:01:15s
epoch 51 | loss: 0.34118 | val_0_rmse: 0.54913 | val_1_rmse: 0.57322 |  0:01:16s
epoch 52 | loss: 0.33451 | val_0_rmse: 0.55686 | val_1_rmse: 0.57975 |  0:01:18s
epoch 53 | loss: 0.33159 | val_0_rmse: 0.54519 | val_1_rmse: 0.56903 |  0:01:19s
epoch 54 | loss: 0.3294  | val_0_rmse: 0.57023 | val_1_rmse: 0.59584 |  0:01:21s
epoch 55 | loss: 0.33832 | val_0_rmse: 0.55952 | val_1_rmse: 0.58091 |  0:01:22s
epoch 56 | loss: 0.3297  | val_0_rmse: 0.55623 | val_1_rmse: 0.57946 |  0:01:24s
epoch 57 | loss: 0.32574 | val_0_rmse: 0.55203 | val_1_rmse: 0.57287 |  0:01:25s
epoch 58 | loss: 0.3329  | val_0_rmse: 0.56274 | val_1_rmse: 0.58691 |  0:01:27s
epoch 59 | loss: 0.32917 | val_0_rmse: 0.55751 | val_1_rmse: 0.58243 |  0:01:28s
epoch 60 | loss: 0.32502 | val_0_rmse: 0.54544 | val_1_rmse: 0.57069 |  0:01:30s
epoch 61 | loss: 0.32252 | val_0_rmse: 0.53759 | val_1_rmse: 0.56476 |  0:01:31s
epoch 62 | loss: 0.3211  | val_0_rmse: 0.54001 | val_1_rmse: 0.56428 |  0:01:32s
epoch 63 | loss: 0.32476 | val_0_rmse: 0.56203 | val_1_rmse: 0.58716 |  0:01:34s
epoch 64 | loss: 0.34021 | val_0_rmse: 0.57706 | val_1_rmse: 0.60108 |  0:01:35s
epoch 65 | loss: 0.34514 | val_0_rmse: 0.56135 | val_1_rmse: 0.58419 |  0:01:37s
epoch 66 | loss: 0.32862 | val_0_rmse: 0.55835 | val_1_rmse: 0.58421 |  0:01:38s
epoch 67 | loss: 0.32496 | val_0_rmse: 0.55947 | val_1_rmse: 0.58251 |  0:01:40s
epoch 68 | loss: 0.33174 | val_0_rmse: 0.56032 | val_1_rmse: 0.58189 |  0:01:41s
epoch 69 | loss: 0.32618 | val_0_rmse: 0.5462  | val_1_rmse: 0.57303 |  0:01:43s
epoch 70 | loss: 0.32219 | val_0_rmse: 0.55185 | val_1_rmse: 0.5802  |  0:01:44s
epoch 71 | loss: 0.33498 | val_0_rmse: 0.55518 | val_1_rmse: 0.57668 |  0:01:46s
epoch 72 | loss: 0.33305 | val_0_rmse: 0.54739 | val_1_rmse: 0.57369 |  0:01:47s
epoch 73 | loss: 0.32748 | val_0_rmse: 0.59105 | val_1_rmse: 0.61877 |  0:01:49s
epoch 74 | loss: 0.3499  | val_0_rmse: 0.56523 | val_1_rmse: 0.59086 |  0:01:50s
epoch 75 | loss: 0.33125 | val_0_rmse: 0.55321 | val_1_rmse: 0.57358 |  0:01:52s
epoch 76 | loss: 0.32081 | val_0_rmse: 0.54147 | val_1_rmse: 0.56501 |  0:01:53s
epoch 77 | loss: 0.32256 | val_0_rmse: 0.54795 | val_1_rmse: 0.57197 |  0:01:54s
epoch 78 | loss: 0.31775 | val_0_rmse: 0.5555  | val_1_rmse: 0.58171 |  0:01:56s
epoch 79 | loss: 0.32775 | val_0_rmse: 0.54895 | val_1_rmse: 0.57384 |  0:01:57s
epoch 80 | loss: 0.31485 | val_0_rmse: 0.53612 | val_1_rmse: 0.56312 |  0:01:59s
epoch 81 | loss: 0.31475 | val_0_rmse: 0.53844 | val_1_rmse: 0.56323 |  0:02:00s
epoch 82 | loss: 0.32729 | val_0_rmse: 0.54092 | val_1_rmse: 0.56717 |  0:02:02s
epoch 83 | loss: 0.32212 | val_0_rmse: 0.54231 | val_1_rmse: 0.56921 |  0:02:03s
epoch 84 | loss: 0.32095 | val_0_rmse: 0.55241 | val_1_rmse: 0.57699 |  0:02:05s
epoch 85 | loss: 0.31563 | val_0_rmse: 0.54541 | val_1_rmse: 0.57212 |  0:02:06s
epoch 86 | loss: 0.32381 | val_0_rmse: 0.54338 | val_1_rmse: 0.5679  |  0:02:08s
epoch 87 | loss: 0.3118  | val_0_rmse: 0.53479 | val_1_rmse: 0.56049 |  0:02:09s
epoch 88 | loss: 0.31121 | val_0_rmse: 0.54015 | val_1_rmse: 0.56637 |  0:02:11s
epoch 89 | loss: 0.31505 | val_0_rmse: 0.53491 | val_1_rmse: 0.56388 |  0:02:12s
epoch 90 | loss: 0.3141  | val_0_rmse: 0.53485 | val_1_rmse: 0.56197 |  0:02:14s
epoch 91 | loss: 0.30594 | val_0_rmse: 0.54228 | val_1_rmse: 0.57473 |  0:02:15s
epoch 92 | loss: 0.3135  | val_0_rmse: 0.53475 | val_1_rmse: 0.56008 |  0:02:16s
epoch 93 | loss: 0.3085  | val_0_rmse: 0.53056 | val_1_rmse: 0.55698 |  0:02:18s
epoch 94 | loss: 0.31125 | val_0_rmse: 0.53282 | val_1_rmse: 0.56048 |  0:02:19s
epoch 95 | loss: 0.31875 | val_0_rmse: 0.54738 | val_1_rmse: 0.56995 |  0:02:21s
epoch 96 | loss: 0.31355 | val_0_rmse: 0.54804 | val_1_rmse: 0.57648 |  0:02:22s
epoch 97 | loss: 0.31756 | val_0_rmse: 0.54699 | val_1_rmse: 0.57284 |  0:02:24s
epoch 98 | loss: 0.30988 | val_0_rmse: 0.53677 | val_1_rmse: 0.56496 |  0:02:25s
epoch 99 | loss: 0.33088 | val_0_rmse: 0.54515 | val_1_rmse: 0.57197 |  0:02:27s
epoch 100| loss: 0.31538 | val_0_rmse: 0.5301  | val_1_rmse: 0.5557  |  0:02:28s
epoch 101| loss: 0.3091  | val_0_rmse: 0.52957 | val_1_rmse: 0.5582  |  0:02:30s
epoch 102| loss: 0.3056  | val_0_rmse: 0.53652 | val_1_rmse: 0.56634 |  0:02:31s
epoch 103| loss: 0.30829 | val_0_rmse: 0.54597 | val_1_rmse: 0.57386 |  0:02:33s
epoch 104| loss: 0.3123  | val_0_rmse: 0.53258 | val_1_rmse: 0.55862 |  0:02:34s
epoch 105| loss: 0.31364 | val_0_rmse: 0.53748 | val_1_rmse: 0.56794 |  0:02:36s
epoch 106| loss: 0.3072  | val_0_rmse: 0.52673 | val_1_rmse: 0.55492 |  0:02:37s
epoch 107| loss: 0.3055  | val_0_rmse: 0.52336 | val_1_rmse: 0.5551  |  0:02:39s
epoch 108| loss: 0.30292 | val_0_rmse: 0.53113 | val_1_rmse: 0.56077 |  0:02:40s
epoch 109| loss: 0.30265 | val_0_rmse: 0.5295  | val_1_rmse: 0.55848 |  0:02:41s
epoch 110| loss: 0.30411 | val_0_rmse: 0.52895 | val_1_rmse: 0.56023 |  0:02:43s
epoch 111| loss: 0.30524 | val_0_rmse: 0.52555 | val_1_rmse: 0.56096 |  0:02:44s
epoch 112| loss: 0.2984  | val_0_rmse: 0.51977 | val_1_rmse: 0.54923 |  0:02:46s
epoch 113| loss: 0.29704 | val_0_rmse: 0.53239 | val_1_rmse: 0.56287 |  0:02:47s
epoch 114| loss: 0.29963 | val_0_rmse: 0.53242 | val_1_rmse: 0.56625 |  0:02:49s
epoch 115| loss: 0.30015 | val_0_rmse: 0.52201 | val_1_rmse: 0.55612 |  0:02:50s
epoch 116| loss: 0.30129 | val_0_rmse: 0.53032 | val_1_rmse: 0.56172 |  0:02:52s
epoch 117| loss: 0.30017 | val_0_rmse: 0.53544 | val_1_rmse: 0.56414 |  0:02:53s
epoch 118| loss: 0.30807 | val_0_rmse: 0.5333  | val_1_rmse: 0.57023 |  0:02:55s
epoch 119| loss: 0.30808 | val_0_rmse: 0.52752 | val_1_rmse: 0.5568  |  0:02:56s
epoch 120| loss: 0.29677 | val_0_rmse: 0.52448 | val_1_rmse: 0.55534 |  0:02:58s
epoch 121| loss: 0.29666 | val_0_rmse: 0.52849 | val_1_rmse: 0.56309 |  0:02:59s
epoch 122| loss: 0.29854 | val_0_rmse: 0.51636 | val_1_rmse: 0.55011 |  0:03:01s
epoch 123| loss: 0.29885 | val_0_rmse: 0.52826 | val_1_rmse: 0.56186 |  0:03:02s
epoch 124| loss: 0.30326 | val_0_rmse: 0.53978 | val_1_rmse: 0.57317 |  0:03:04s
epoch 125| loss: 0.29641 | val_0_rmse: 0.51791 | val_1_rmse: 0.55257 |  0:03:05s
epoch 126| loss: 0.30441 | val_0_rmse: 0.52196 | val_1_rmse: 0.55947 |  0:03:06s
epoch 127| loss: 0.30414 | val_0_rmse: 0.51951 | val_1_rmse: 0.55121 |  0:03:08s
epoch 128| loss: 0.30405 | val_0_rmse: 0.51968 | val_1_rmse: 0.55344 |  0:03:09s
epoch 129| loss: 0.29029 | val_0_rmse: 0.52507 | val_1_rmse: 0.55671 |  0:03:11s
epoch 130| loss: 0.29624 | val_0_rmse: 0.52862 | val_1_rmse: 0.56128 |  0:03:12s
epoch 131| loss: 0.2939  | val_0_rmse: 0.51779 | val_1_rmse: 0.55453 |  0:03:14s
epoch 132| loss: 0.2997  | val_0_rmse: 0.54199 | val_1_rmse: 0.57253 |  0:03:15s
epoch 133| loss: 0.29679 | val_0_rmse: 0.51829 | val_1_rmse: 0.55091 |  0:03:17s
epoch 134| loss: 0.29327 | val_0_rmse: 0.51821 | val_1_rmse: 0.55355 |  0:03:18s
epoch 135| loss: 0.30035 | val_0_rmse: 0.53477 | val_1_rmse: 0.56965 |  0:03:20s
epoch 136| loss: 0.30279 | val_0_rmse: 0.54242 | val_1_rmse: 0.57576 |  0:03:21s
epoch 137| loss: 0.29016 | val_0_rmse: 0.53001 | val_1_rmse: 0.56441 |  0:03:23s
epoch 138| loss: 0.30396 | val_0_rmse: 0.51703 | val_1_rmse: 0.55401 |  0:03:24s
epoch 139| loss: 0.29327 | val_0_rmse: 0.51279 | val_1_rmse: 0.55019 |  0:03:26s
epoch 140| loss: 0.29356 | val_0_rmse: 0.52228 | val_1_rmse: 0.55181 |  0:03:27s
epoch 141| loss: 0.29812 | val_0_rmse: 0.52463 | val_1_rmse: 0.56008 |  0:03:28s
epoch 142| loss: 0.29182 | val_0_rmse: 0.52193 | val_1_rmse: 0.55199 |  0:03:30s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.54923
Best weights from best epoch are automatically used!
ended training at: 05:37:53
Feature importance:
[('Area', 0.2988022757926359), ('Baths', 0.02147094976662716), ('Beds', 0.0), ('Latitude', 0.35471315233497297), ('Longitude', 0.18125236990412108), ('Month', 0.048007803217244564), ('Year', 0.09575344898439835)]
Mean squared error is of 6870644268.162397
Mean absolute error:55061.472590691454
MAPE:0.16572229076371134
R2 score:0.6984155516553776
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:37:53
epoch 0  | loss: 0.70617 | val_0_rmse: 0.75237 | val_1_rmse: 0.7426  |  0:00:01s
epoch 1  | loss: 0.47906 | val_0_rmse: 0.67549 | val_1_rmse: 0.665   |  0:00:02s
epoch 2  | loss: 0.45655 | val_0_rmse: 0.64486 | val_1_rmse: 0.64024 |  0:00:04s
epoch 3  | loss: 0.43469 | val_0_rmse: 0.68339 | val_1_rmse: 0.68167 |  0:00:05s
epoch 4  | loss: 0.41529 | val_0_rmse: 0.69472 | val_1_rmse: 0.69034 |  0:00:07s
epoch 5  | loss: 0.41367 | val_0_rmse: 0.65352 | val_1_rmse: 0.65255 |  0:00:08s
epoch 6  | loss: 0.4014  | val_0_rmse: 0.61071 | val_1_rmse: 0.60833 |  0:00:10s
epoch 7  | loss: 0.38863 | val_0_rmse: 0.61019 | val_1_rmse: 0.60641 |  0:00:11s
epoch 8  | loss: 0.37829 | val_0_rmse: 0.6094  | val_1_rmse: 0.61128 |  0:00:13s
epoch 9  | loss: 0.38024 | val_0_rmse: 0.60841 | val_1_rmse: 0.59744 |  0:00:14s
epoch 10 | loss: 0.39016 | val_0_rmse: 0.59066 | val_1_rmse: 0.57982 |  0:00:16s
epoch 11 | loss: 0.37424 | val_0_rmse: 0.62234 | val_1_rmse: 0.61307 |  0:00:17s
epoch 12 | loss: 0.36729 | val_0_rmse: 0.59614 | val_1_rmse: 0.5962  |  0:00:19s
epoch 13 | loss: 0.36821 | val_0_rmse: 0.61113 | val_1_rmse: 0.61446 |  0:00:20s
epoch 14 | loss: 0.39985 | val_0_rmse: 0.60321 | val_1_rmse: 0.60206 |  0:00:21s
epoch 15 | loss: 0.38436 | val_0_rmse: 0.61864 | val_1_rmse: 0.61642 |  0:00:23s
epoch 16 | loss: 0.37052 | val_0_rmse: 0.58008 | val_1_rmse: 0.57615 |  0:00:24s
epoch 17 | loss: 0.36458 | val_0_rmse: 0.57528 | val_1_rmse: 0.56758 |  0:00:26s
epoch 18 | loss: 0.35533 | val_0_rmse: 0.59807 | val_1_rmse: 0.5923  |  0:00:27s
epoch 19 | loss: 0.35381 | val_0_rmse: 0.58157 | val_1_rmse: 0.57628 |  0:00:29s
epoch 20 | loss: 0.36302 | val_0_rmse: 0.5865  | val_1_rmse: 0.57674 |  0:00:30s
epoch 21 | loss: 0.35438 | val_0_rmse: 0.59954 | val_1_rmse: 0.59176 |  0:00:32s
epoch 22 | loss: 0.33953 | val_0_rmse: 0.601   | val_1_rmse: 0.59846 |  0:00:33s
epoch 23 | loss: 0.33869 | val_0_rmse: 0.57223 | val_1_rmse: 0.56479 |  0:00:35s
epoch 24 | loss: 0.34704 | val_0_rmse: 0.55607 | val_1_rmse: 0.55115 |  0:00:36s
epoch 25 | loss: 0.33928 | val_0_rmse: 0.57413 | val_1_rmse: 0.56899 |  0:00:38s
epoch 26 | loss: 0.34938 | val_0_rmse: 0.55903 | val_1_rmse: 0.55517 |  0:00:39s
epoch 27 | loss: 0.32749 | val_0_rmse: 0.54933 | val_1_rmse: 0.54208 |  0:00:41s
epoch 28 | loss: 0.32768 | val_0_rmse: 0.55708 | val_1_rmse: 0.55414 |  0:00:42s
epoch 29 | loss: 0.32681 | val_0_rmse: 0.56717 | val_1_rmse: 0.56435 |  0:00:44s
epoch 30 | loss: 0.32754 | val_0_rmse: 0.55264 | val_1_rmse: 0.55053 |  0:00:45s
epoch 31 | loss: 0.33612 | val_0_rmse: 0.58008 | val_1_rmse: 0.57785 |  0:00:46s
epoch 32 | loss: 0.33629 | val_0_rmse: 0.55522 | val_1_rmse: 0.55415 |  0:00:48s
epoch 33 | loss: 0.32371 | val_0_rmse: 0.56438 | val_1_rmse: 0.56228 |  0:00:49s
epoch 34 | loss: 0.32738 | val_0_rmse: 0.57853 | val_1_rmse: 0.57225 |  0:00:51s
epoch 35 | loss: 0.32721 | val_0_rmse: 0.58776 | val_1_rmse: 0.57994 |  0:00:52s
epoch 36 | loss: 0.3273  | val_0_rmse: 0.56536 | val_1_rmse: 0.56264 |  0:00:54s
epoch 37 | loss: 0.32202 | val_0_rmse: 0.56171 | val_1_rmse: 0.56102 |  0:00:55s
epoch 38 | loss: 0.32034 | val_0_rmse: 0.63119 | val_1_rmse: 0.62202 |  0:00:57s
epoch 39 | loss: 0.32014 | val_0_rmse: 0.58757 | val_1_rmse: 0.58331 |  0:00:58s
epoch 40 | loss: 0.33225 | val_0_rmse: 0.55427 | val_1_rmse: 0.55036 |  0:01:00s
epoch 41 | loss: 0.32068 | val_0_rmse: 0.55246 | val_1_rmse: 0.55326 |  0:01:01s
epoch 42 | loss: 0.32259 | val_0_rmse: 0.55246 | val_1_rmse: 0.55416 |  0:01:03s
epoch 43 | loss: 0.31564 | val_0_rmse: 0.54494 | val_1_rmse: 0.54379 |  0:01:04s
epoch 44 | loss: 0.31281 | val_0_rmse: 0.55135 | val_1_rmse: 0.54529 |  0:01:05s
epoch 45 | loss: 0.31408 | val_0_rmse: 0.57262 | val_1_rmse: 0.57175 |  0:01:07s
epoch 46 | loss: 0.32181 | val_0_rmse: 0.55093 | val_1_rmse: 0.55324 |  0:01:08s
epoch 47 | loss: 0.31454 | val_0_rmse: 0.63622 | val_1_rmse: 0.63424 |  0:01:10s
epoch 48 | loss: 0.32255 | val_0_rmse: 0.54406 | val_1_rmse: 0.54307 |  0:01:11s
epoch 49 | loss: 0.31535 | val_0_rmse: 0.55948 | val_1_rmse: 0.55249 |  0:01:13s
epoch 50 | loss: 0.31747 | val_0_rmse: 0.56212 | val_1_rmse: 0.56069 |  0:01:14s
epoch 51 | loss: 0.31791 | val_0_rmse: 0.54755 | val_1_rmse: 0.5431  |  0:01:16s
epoch 52 | loss: 0.31525 | val_0_rmse: 0.53382 | val_1_rmse: 0.52974 |  0:01:17s
epoch 53 | loss: 0.3115  | val_0_rmse: 0.53524 | val_1_rmse: 0.53411 |  0:01:19s
epoch 54 | loss: 0.30949 | val_0_rmse: 0.53817 | val_1_rmse: 0.53753 |  0:01:20s
epoch 55 | loss: 0.30567 | val_0_rmse: 0.54596 | val_1_rmse: 0.54    |  0:01:22s
epoch 56 | loss: 0.31626 | val_0_rmse: 0.55376 | val_1_rmse: 0.54879 |  0:01:23s
epoch 57 | loss: 0.3172  | val_0_rmse: 0.55435 | val_1_rmse: 0.55351 |  0:01:25s
epoch 58 | loss: 0.3135  | val_0_rmse: 0.56897 | val_1_rmse: 0.56576 |  0:01:26s
epoch 59 | loss: 0.31721 | val_0_rmse: 0.58307 | val_1_rmse: 0.57173 |  0:01:27s
epoch 60 | loss: 0.31926 | val_0_rmse: 0.55785 | val_1_rmse: 0.55498 |  0:01:29s
epoch 61 | loss: 0.30706 | val_0_rmse: 0.55111 | val_1_rmse: 0.54624 |  0:01:30s
epoch 62 | loss: 0.3097  | val_0_rmse: 0.55783 | val_1_rmse: 0.5613  |  0:01:32s
epoch 63 | loss: 0.3096  | val_0_rmse: 0.54406 | val_1_rmse: 0.54468 |  0:01:33s
epoch 64 | loss: 0.30065 | val_0_rmse: 0.58873 | val_1_rmse: 0.58663 |  0:01:35s
epoch 65 | loss: 0.32038 | val_0_rmse: 0.63798 | val_1_rmse: 0.63019 |  0:01:36s
epoch 66 | loss: 0.31169 | val_0_rmse: 0.56841 | val_1_rmse: 0.56948 |  0:01:38s
epoch 67 | loss: 0.30164 | val_0_rmse: 0.53205 | val_1_rmse: 0.53194 |  0:01:39s
epoch 68 | loss: 0.29466 | val_0_rmse: 0.51929 | val_1_rmse: 0.52041 |  0:01:41s
epoch 69 | loss: 0.29996 | val_0_rmse: 0.53606 | val_1_rmse: 0.53932 |  0:01:42s
epoch 70 | loss: 0.30111 | val_0_rmse: 0.55049 | val_1_rmse: 0.55382 |  0:01:44s
epoch 71 | loss: 0.30063 | val_0_rmse: 0.52124 | val_1_rmse: 0.51999 |  0:01:45s
epoch 72 | loss: 0.30148 | val_0_rmse: 0.54286 | val_1_rmse: 0.54273 |  0:01:47s
epoch 73 | loss: 0.30462 | val_0_rmse: 0.55041 | val_1_rmse: 0.55055 |  0:01:48s
epoch 74 | loss: 0.29507 | val_0_rmse: 0.62959 | val_1_rmse: 0.62835 |  0:01:49s
epoch 75 | loss: 0.3068  | val_0_rmse: 0.53879 | val_1_rmse: 0.54566 |  0:01:51s
epoch 76 | loss: 0.30505 | val_0_rmse: 0.62523 | val_1_rmse: 0.62058 |  0:01:52s
epoch 77 | loss: 0.29906 | val_0_rmse: 0.52193 | val_1_rmse: 0.5235  |  0:01:54s
epoch 78 | loss: 0.29648 | val_0_rmse: 0.52548 | val_1_rmse: 0.52808 |  0:01:55s
epoch 79 | loss: 0.30803 | val_0_rmse: 0.53687 | val_1_rmse: 0.53507 |  0:01:57s
epoch 80 | loss: 0.30865 | val_0_rmse: 0.55027 | val_1_rmse: 0.55489 |  0:01:58s
epoch 81 | loss: 0.30185 | val_0_rmse: 0.54944 | val_1_rmse: 0.54752 |  0:02:00s
epoch 82 | loss: 0.31865 | val_0_rmse: 0.54866 | val_1_rmse: 0.54698 |  0:02:01s
epoch 83 | loss: 0.30051 | val_0_rmse: 0.55156 | val_1_rmse: 0.55014 |  0:02:03s
epoch 84 | loss: 0.29226 | val_0_rmse: 0.52528 | val_1_rmse: 0.52223 |  0:02:04s
epoch 85 | loss: 0.28635 | val_0_rmse: 0.51819 | val_1_rmse: 0.52298 |  0:02:06s
epoch 86 | loss: 0.29299 | val_0_rmse: 0.54131 | val_1_rmse: 0.53935 |  0:02:07s
epoch 87 | loss: 0.28828 | val_0_rmse: 0.51688 | val_1_rmse: 0.51583 |  0:02:09s
epoch 88 | loss: 0.28868 | val_0_rmse: 0.51276 | val_1_rmse: 0.50896 |  0:02:10s
epoch 89 | loss: 0.29262 | val_0_rmse: 0.53929 | val_1_rmse: 0.53952 |  0:02:11s
epoch 90 | loss: 0.30042 | val_0_rmse: 0.54652 | val_1_rmse: 0.5426  |  0:02:13s
epoch 91 | loss: 0.30422 | val_0_rmse: 0.52112 | val_1_rmse: 0.52654 |  0:02:14s
epoch 92 | loss: 0.29229 | val_0_rmse: 0.55155 | val_1_rmse: 0.55683 |  0:02:16s
epoch 93 | loss: 0.29142 | val_0_rmse: 0.52486 | val_1_rmse: 0.52816 |  0:02:17s
epoch 94 | loss: 0.2901  | val_0_rmse: 0.51204 | val_1_rmse: 0.52033 |  0:02:19s
epoch 95 | loss: 0.28529 | val_0_rmse: 0.5241  | val_1_rmse: 0.52421 |  0:02:20s
epoch 96 | loss: 0.29219 | val_0_rmse: 0.5233  | val_1_rmse: 0.524   |  0:02:22s
epoch 97 | loss: 0.2985  | val_0_rmse: 0.54847 | val_1_rmse: 0.55103 |  0:02:23s
epoch 98 | loss: 0.30431 | val_0_rmse: 0.52892 | val_1_rmse: 0.52859 |  0:02:25s
epoch 99 | loss: 0.2945  | val_0_rmse: 0.55793 | val_1_rmse: 0.56304 |  0:02:26s
epoch 100| loss: 0.30489 | val_0_rmse: 0.52386 | val_1_rmse: 0.53027 |  0:02:28s
epoch 101| loss: 0.30799 | val_0_rmse: 0.56964 | val_1_rmse: 0.56847 |  0:02:29s
epoch 102| loss: 0.30144 | val_0_rmse: 0.53591 | val_1_rmse: 0.54385 |  0:02:31s
epoch 103| loss: 0.28824 | val_0_rmse: 0.55001 | val_1_rmse: 0.54967 |  0:02:32s
epoch 104| loss: 0.29515 | val_0_rmse: 0.54276 | val_1_rmse: 0.54246 |  0:02:33s
epoch 105| loss: 0.31235 | val_0_rmse: 0.5291  | val_1_rmse: 0.53125 |  0:02:35s
epoch 106| loss: 0.29869 | val_0_rmse: 0.53179 | val_1_rmse: 0.53728 |  0:02:36s
epoch 107| loss: 0.29236 | val_0_rmse: 0.51515 | val_1_rmse: 0.51938 |  0:02:38s
epoch 108| loss: 0.29849 | val_0_rmse: 0.55221 | val_1_rmse: 0.55916 |  0:02:39s
epoch 109| loss: 0.29573 | val_0_rmse: 0.52843 | val_1_rmse: 0.53228 |  0:02:41s
epoch 110| loss: 0.31597 | val_0_rmse: 0.6483  | val_1_rmse: 0.64751 |  0:02:42s
epoch 111| loss: 0.31696 | val_0_rmse: 0.55628 | val_1_rmse: 0.5528  |  0:02:44s
epoch 112| loss: 0.31065 | val_0_rmse: 0.56556 | val_1_rmse: 0.56255 |  0:02:45s
epoch 113| loss: 0.30496 | val_0_rmse: 0.53887 | val_1_rmse: 0.53985 |  0:02:47s
epoch 114| loss: 0.31346 | val_0_rmse: 0.54034 | val_1_rmse: 0.53563 |  0:02:48s
epoch 115| loss: 0.3124  | val_0_rmse: 0.54094 | val_1_rmse: 0.53844 |  0:02:50s
epoch 116| loss: 0.30332 | val_0_rmse: 0.54219 | val_1_rmse: 0.54488 |  0:02:51s
epoch 117| loss: 0.30306 | val_0_rmse: 0.52428 | val_1_rmse: 0.52024 |  0:02:53s
epoch 118| loss: 0.29925 | val_0_rmse: 0.54007 | val_1_rmse: 0.5347  |  0:02:54s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.50896
Best weights from best epoch are automatically used!
ended training at: 05:40:48
Feature importance:
[('Area', 0.2733654747423479), ('Baths', 0.006530653061052113), ('Beds', 0.0), ('Latitude', 0.3508113305018086), ('Longitude', 0.22722272918763803), ('Month', 0.0), ('Year', 0.14206981250715334)]
Mean squared error is of 6038044627.595296
Mean absolute error:52253.090429567026
MAPE:0.16865997544656164
R2 score:0.7327121490450359
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:40:48
epoch 0  | loss: 0.74108 | val_0_rmse: 0.75228 | val_1_rmse: 0.74505 |  0:00:01s
epoch 1  | loss: 0.50863 | val_0_rmse: 0.68687 | val_1_rmse: 0.68812 |  0:00:02s
epoch 2  | loss: 0.46525 | val_0_rmse: 0.64893 | val_1_rmse: 0.64131 |  0:00:04s
epoch 3  | loss: 0.43934 | val_0_rmse: 0.62379 | val_1_rmse: 0.63148 |  0:00:05s
epoch 4  | loss: 0.4075  | val_0_rmse: 0.63626 | val_1_rmse: 0.64163 |  0:00:07s
epoch 5  | loss: 0.39362 | val_0_rmse: 0.63439 | val_1_rmse: 0.63769 |  0:00:08s
epoch 6  | loss: 0.3916  | val_0_rmse: 0.61399 | val_1_rmse: 0.61495 |  0:00:10s
epoch 7  | loss: 0.38248 | val_0_rmse: 0.60985 | val_1_rmse: 0.61239 |  0:00:11s
epoch 8  | loss: 0.37745 | val_0_rmse: 0.61235 | val_1_rmse: 0.61165 |  0:00:13s
epoch 9  | loss: 0.3888  | val_0_rmse: 0.58429 | val_1_rmse: 0.58441 |  0:00:14s
epoch 10 | loss: 0.37142 | val_0_rmse: 0.62127 | val_1_rmse: 0.62854 |  0:00:16s
epoch 11 | loss: 0.37811 | val_0_rmse: 0.58018 | val_1_rmse: 0.58431 |  0:00:17s
epoch 12 | loss: 0.36101 | val_0_rmse: 0.57561 | val_1_rmse: 0.57949 |  0:00:18s
epoch 13 | loss: 0.3594  | val_0_rmse: 0.58938 | val_1_rmse: 0.59638 |  0:00:20s
epoch 14 | loss: 0.36702 | val_0_rmse: 0.58187 | val_1_rmse: 0.58964 |  0:00:21s
epoch 15 | loss: 0.35006 | val_0_rmse: 0.57113 | val_1_rmse: 0.57754 |  0:00:23s
epoch 16 | loss: 0.35331 | val_0_rmse: 0.57464 | val_1_rmse: 0.5875  |  0:00:24s
epoch 17 | loss: 0.3586  | val_0_rmse: 0.56649 | val_1_rmse: 0.57902 |  0:00:26s
epoch 18 | loss: 0.35764 | val_0_rmse: 0.56689 | val_1_rmse: 0.57901 |  0:00:27s
epoch 19 | loss: 0.35679 | val_0_rmse: 0.58862 | val_1_rmse: 0.59943 |  0:00:29s
epoch 20 | loss: 0.35464 | val_0_rmse: 0.60289 | val_1_rmse: 0.6125  |  0:00:30s
epoch 21 | loss: 0.34946 | val_0_rmse: 0.57386 | val_1_rmse: 0.58108 |  0:00:32s
epoch 22 | loss: 0.35542 | val_0_rmse: 0.58629 | val_1_rmse: 0.59458 |  0:00:33s
epoch 23 | loss: 0.35678 | val_0_rmse: 0.587   | val_1_rmse: 0.60044 |  0:00:35s
epoch 24 | loss: 0.36632 | val_0_rmse: 0.57959 | val_1_rmse: 0.59102 |  0:00:36s
epoch 25 | loss: 0.35302 | val_0_rmse: 0.58535 | val_1_rmse: 0.58984 |  0:00:38s
epoch 26 | loss: 0.35909 | val_0_rmse: 0.5949  | val_1_rmse: 0.60528 |  0:00:39s
epoch 27 | loss: 0.36098 | val_0_rmse: 0.57948 | val_1_rmse: 0.58816 |  0:00:40s
epoch 28 | loss: 0.36239 | val_0_rmse: 0.61024 | val_1_rmse: 0.61878 |  0:00:42s
epoch 29 | loss: 0.35045 | val_0_rmse: 0.56738 | val_1_rmse: 0.57426 |  0:00:43s
epoch 30 | loss: 0.33755 | val_0_rmse: 0.56721 | val_1_rmse: 0.5755  |  0:00:45s
epoch 31 | loss: 0.34748 | val_0_rmse: 0.57432 | val_1_rmse: 0.58009 |  0:00:46s
epoch 32 | loss: 0.34818 | val_0_rmse: 0.57772 | val_1_rmse: 0.5836  |  0:00:48s
epoch 33 | loss: 0.34596 | val_0_rmse: 0.56095 | val_1_rmse: 0.5675  |  0:00:49s
epoch 34 | loss: 0.34237 | val_0_rmse: 0.56809 | val_1_rmse: 0.57372 |  0:00:51s
epoch 35 | loss: 0.34003 | val_0_rmse: 0.58112 | val_1_rmse: 0.59523 |  0:00:52s
epoch 36 | loss: 0.3518  | val_0_rmse: 0.55985 | val_1_rmse: 0.5694  |  0:00:54s
epoch 37 | loss: 0.3427  | val_0_rmse: 0.55929 | val_1_rmse: 0.56819 |  0:00:55s
epoch 38 | loss: 0.3358  | val_0_rmse: 0.55671 | val_1_rmse: 0.56766 |  0:00:57s
epoch 39 | loss: 0.33922 | val_0_rmse: 0.56972 | val_1_rmse: 0.57833 |  0:00:58s
epoch 40 | loss: 0.33722 | val_0_rmse: 0.56008 | val_1_rmse: 0.5727  |  0:01:00s
epoch 41 | loss: 0.34259 | val_0_rmse: 0.56413 | val_1_rmse: 0.57772 |  0:01:01s
epoch 42 | loss: 0.3391  | val_0_rmse: 0.55507 | val_1_rmse: 0.5639  |  0:01:02s
epoch 43 | loss: 0.33754 | val_0_rmse: 0.57148 | val_1_rmse: 0.58556 |  0:01:04s
epoch 44 | loss: 0.34709 | val_0_rmse: 0.60395 | val_1_rmse: 0.60982 |  0:01:05s
epoch 45 | loss: 0.3775  | val_0_rmse: 0.61344 | val_1_rmse: 0.62883 |  0:01:07s
epoch 46 | loss: 0.37425 | val_0_rmse: 0.59297 | val_1_rmse: 0.60661 |  0:01:08s
epoch 47 | loss: 0.36984 | val_0_rmse: 0.60058 | val_1_rmse: 0.60798 |  0:01:10s
epoch 48 | loss: 0.35996 | val_0_rmse: 0.5802  | val_1_rmse: 0.58796 |  0:01:11s
epoch 49 | loss: 0.35482 | val_0_rmse: 0.57321 | val_1_rmse: 0.58666 |  0:01:13s
epoch 50 | loss: 0.35004 | val_0_rmse: 0.56586 | val_1_rmse: 0.57475 |  0:01:14s
epoch 51 | loss: 0.34048 | val_0_rmse: 0.56704 | val_1_rmse: 0.57989 |  0:01:16s
epoch 52 | loss: 0.34421 | val_0_rmse: 0.56381 | val_1_rmse: 0.57779 |  0:01:17s
epoch 53 | loss: 0.34217 | val_0_rmse: 0.56836 | val_1_rmse: 0.57698 |  0:01:19s
epoch 54 | loss: 0.34052 | val_0_rmse: 0.56032 | val_1_rmse: 0.57062 |  0:01:20s
epoch 55 | loss: 0.33746 | val_0_rmse: 0.56054 | val_1_rmse: 0.57348 |  0:01:21s
epoch 56 | loss: 0.33701 | val_0_rmse: 0.55612 | val_1_rmse: 0.57097 |  0:01:23s
epoch 57 | loss: 0.34102 | val_0_rmse: 0.56003 | val_1_rmse: 0.56784 |  0:01:24s
epoch 58 | loss: 0.3378  | val_0_rmse: 0.55035 | val_1_rmse: 0.55906 |  0:01:26s
epoch 59 | loss: 0.33065 | val_0_rmse: 0.54973 | val_1_rmse: 0.56488 |  0:01:27s
epoch 60 | loss: 0.3312  | val_0_rmse: 0.55051 | val_1_rmse: 0.56488 |  0:01:29s
epoch 61 | loss: 0.34177 | val_0_rmse: 0.56347 | val_1_rmse: 0.58036 |  0:01:30s
epoch 62 | loss: 0.33511 | val_0_rmse: 0.55702 | val_1_rmse: 0.57384 |  0:01:32s
epoch 63 | loss: 0.32749 | val_0_rmse: 0.55013 | val_1_rmse: 0.56718 |  0:01:33s
epoch 64 | loss: 0.32726 | val_0_rmse: 0.55171 | val_1_rmse: 0.56592 |  0:01:35s
epoch 65 | loss: 0.32716 | val_0_rmse: 0.58394 | val_1_rmse: 0.59401 |  0:01:36s
epoch 66 | loss: 0.32965 | val_0_rmse: 0.56496 | val_1_rmse: 0.57856 |  0:01:38s
epoch 67 | loss: 0.33151 | val_0_rmse: 0.5554  | val_1_rmse: 0.5734  |  0:01:39s
epoch 68 | loss: 0.32845 | val_0_rmse: 0.54727 | val_1_rmse: 0.56245 |  0:01:40s
epoch 69 | loss: 0.32649 | val_0_rmse: 0.55776 | val_1_rmse: 0.57198 |  0:01:42s
epoch 70 | loss: 0.33167 | val_0_rmse: 0.55331 | val_1_rmse: 0.5688  |  0:01:43s
epoch 71 | loss: 0.32891 | val_0_rmse: 0.55404 | val_1_rmse: 0.56829 |  0:01:45s
epoch 72 | loss: 0.32455 | val_0_rmse: 0.54658 | val_1_rmse: 0.56301 |  0:01:46s
epoch 73 | loss: 0.32534 | val_0_rmse: 0.56018 | val_1_rmse: 0.57683 |  0:01:48s
epoch 74 | loss: 0.33003 | val_0_rmse: 0.55753 | val_1_rmse: 0.56858 |  0:01:49s
epoch 75 | loss: 0.33586 | val_0_rmse: 0.55228 | val_1_rmse: 0.56624 |  0:01:51s
epoch 76 | loss: 0.33161 | val_0_rmse: 0.55038 | val_1_rmse: 0.56189 |  0:01:52s
epoch 77 | loss: 0.32655 | val_0_rmse: 0.54272 | val_1_rmse: 0.55568 |  0:01:54s
epoch 78 | loss: 0.32173 | val_0_rmse: 0.55396 | val_1_rmse: 0.5698  |  0:01:55s
epoch 79 | loss: 0.32457 | val_0_rmse: 0.54641 | val_1_rmse: 0.56488 |  0:01:57s
epoch 80 | loss: 0.32845 | val_0_rmse: 0.53891 | val_1_rmse: 0.55457 |  0:01:58s
epoch 81 | loss: 0.31615 | val_0_rmse: 0.55404 | val_1_rmse: 0.56749 |  0:01:59s
epoch 82 | loss: 0.31655 | val_0_rmse: 0.53864 | val_1_rmse: 0.55116 |  0:02:01s
epoch 83 | loss: 0.3213  | val_0_rmse: 0.53431 | val_1_rmse: 0.54741 |  0:02:02s
epoch 84 | loss: 0.32673 | val_0_rmse: 0.54303 | val_1_rmse: 0.55601 |  0:02:04s
epoch 85 | loss: 0.32585 | val_0_rmse: 0.55404 | val_1_rmse: 0.56755 |  0:02:05s
epoch 86 | loss: 0.31966 | val_0_rmse: 0.5394  | val_1_rmse: 0.55413 |  0:02:07s
epoch 87 | loss: 0.31898 | val_0_rmse: 0.57236 | val_1_rmse: 0.57723 |  0:02:08s
epoch 88 | loss: 0.31827 | val_0_rmse: 0.55472 | val_1_rmse: 0.57055 |  0:02:10s
epoch 89 | loss: 0.31614 | val_0_rmse: 0.5436  | val_1_rmse: 0.55582 |  0:02:11s
epoch 90 | loss: 0.31503 | val_0_rmse: 0.54758 | val_1_rmse: 0.5613  |  0:02:13s
epoch 91 | loss: 0.31837 | val_0_rmse: 0.55573 | val_1_rmse: 0.56882 |  0:02:14s
epoch 92 | loss: 0.31243 | val_0_rmse: 0.54676 | val_1_rmse: 0.55973 |  0:02:16s
epoch 93 | loss: 0.31878 | val_0_rmse: 0.53555 | val_1_rmse: 0.5458  |  0:02:17s
epoch 94 | loss: 0.31116 | val_0_rmse: 0.53781 | val_1_rmse: 0.55565 |  0:02:18s
epoch 95 | loss: 0.31314 | val_0_rmse: 0.52976 | val_1_rmse: 0.54115 |  0:02:20s
epoch 96 | loss: 0.31264 | val_0_rmse: 0.53078 | val_1_rmse: 0.54399 |  0:02:21s
epoch 97 | loss: 0.30682 | val_0_rmse: 0.53123 | val_1_rmse: 0.5453  |  0:02:23s
epoch 98 | loss: 0.30938 | val_0_rmse: 0.54045 | val_1_rmse: 0.55156 |  0:02:24s
epoch 99 | loss: 0.31265 | val_0_rmse: 0.56469 | val_1_rmse: 0.57458 |  0:02:26s
epoch 100| loss: 0.31572 | val_0_rmse: 0.53209 | val_1_rmse: 0.54737 |  0:02:27s
epoch 101| loss: 0.30881 | val_0_rmse: 0.53711 | val_1_rmse: 0.55161 |  0:02:29s
epoch 102| loss: 0.32664 | val_0_rmse: 0.62294 | val_1_rmse: 0.63054 |  0:02:30s
epoch 103| loss: 0.33652 | val_0_rmse: 0.5522  | val_1_rmse: 0.56631 |  0:02:32s
epoch 104| loss: 0.32458 | val_0_rmse: 0.54007 | val_1_rmse: 0.54823 |  0:02:33s
epoch 105| loss: 0.31758 | val_0_rmse: 0.53726 | val_1_rmse: 0.54905 |  0:02:35s
epoch 106| loss: 0.31322 | val_0_rmse: 0.53529 | val_1_rmse: 0.54953 |  0:02:36s
epoch 107| loss: 0.31048 | val_0_rmse: 0.53232 | val_1_rmse: 0.5423  |  0:02:38s
epoch 108| loss: 0.31883 | val_0_rmse: 0.60137 | val_1_rmse: 0.61232 |  0:02:39s
epoch 109| loss: 0.31925 | val_0_rmse: 0.5377  | val_1_rmse: 0.54666 |  0:02:40s
epoch 110| loss: 0.31388 | val_0_rmse: 0.56884 | val_1_rmse: 0.57664 |  0:02:42s
epoch 111| loss: 0.31739 | val_0_rmse: 0.53728 | val_1_rmse: 0.55007 |  0:02:43s
epoch 112| loss: 0.30652 | val_0_rmse: 0.53031 | val_1_rmse: 0.54422 |  0:02:45s
epoch 113| loss: 0.3058  | val_0_rmse: 0.53211 | val_1_rmse: 0.54515 |  0:02:46s
epoch 114| loss: 0.30534 | val_0_rmse: 0.53749 | val_1_rmse: 0.54983 |  0:02:48s
epoch 115| loss: 0.31078 | val_0_rmse: 0.53103 | val_1_rmse: 0.54471 |  0:02:49s
epoch 116| loss: 0.3087  | val_0_rmse: 0.53885 | val_1_rmse: 0.55476 |  0:02:51s
epoch 117| loss: 0.30908 | val_0_rmse: 0.53053 | val_1_rmse: 0.54382 |  0:02:52s
epoch 118| loss: 0.31464 | val_0_rmse: 0.54319 | val_1_rmse: 0.55514 |  0:02:54s
epoch 119| loss: 0.31355 | val_0_rmse: 0.54223 | val_1_rmse: 0.55547 |  0:02:55s
epoch 120| loss: 0.30432 | val_0_rmse: 0.54288 | val_1_rmse: 0.55735 |  0:02:57s
epoch 121| loss: 0.3054  | val_0_rmse: 0.52899 | val_1_rmse: 0.54494 |  0:02:58s
epoch 122| loss: 0.3025  | val_0_rmse: 0.53799 | val_1_rmse: 0.55315 |  0:02:59s
epoch 123| loss: 0.31595 | val_0_rmse: 0.52011 | val_1_rmse: 0.5358  |  0:03:01s
epoch 124| loss: 0.31191 | val_0_rmse: 0.53677 | val_1_rmse: 0.55293 |  0:03:02s
epoch 125| loss: 0.30598 | val_0_rmse: 0.53543 | val_1_rmse: 0.55132 |  0:03:04s
epoch 126| loss: 0.30603 | val_0_rmse: 0.52956 | val_1_rmse: 0.54613 |  0:03:05s
epoch 127| loss: 0.30569 | val_0_rmse: 0.52385 | val_1_rmse: 0.5378  |  0:03:07s
epoch 128| loss: 0.30351 | val_0_rmse: 0.54444 | val_1_rmse: 0.56359 |  0:03:08s
epoch 129| loss: 0.3035  | val_0_rmse: 0.52862 | val_1_rmse: 0.54239 |  0:03:10s
epoch 130| loss: 0.31018 | val_0_rmse: 0.5274  | val_1_rmse: 0.54406 |  0:03:11s
epoch 131| loss: 0.30227 | val_0_rmse: 0.53763 | val_1_rmse: 0.54758 |  0:03:13s
epoch 132| loss: 0.30297 | val_0_rmse: 0.53295 | val_1_rmse: 0.54654 |  0:03:14s
epoch 133| loss: 0.3033  | val_0_rmse: 0.52578 | val_1_rmse: 0.54244 |  0:03:16s
epoch 134| loss: 0.30144 | val_0_rmse: 0.53238 | val_1_rmse: 0.55042 |  0:03:17s
epoch 135| loss: 0.30026 | val_0_rmse: 0.52753 | val_1_rmse: 0.54118 |  0:03:19s
epoch 136| loss: 0.3024  | val_0_rmse: 0.52232 | val_1_rmse: 0.54101 |  0:03:20s
epoch 137| loss: 0.30372 | val_0_rmse: 0.52653 | val_1_rmse: 0.54171 |  0:03:21s
epoch 138| loss: 0.30289 | val_0_rmse: 0.54713 | val_1_rmse: 0.56383 |  0:03:23s
epoch 139| loss: 0.33202 | val_0_rmse: 0.5429  | val_1_rmse: 0.55816 |  0:03:24s
epoch 140| loss: 0.31466 | val_0_rmse: 0.52782 | val_1_rmse: 0.54384 |  0:03:26s
epoch 141| loss: 0.29971 | val_0_rmse: 0.52432 | val_1_rmse: 0.53978 |  0:03:27s
epoch 142| loss: 0.30138 | val_0_rmse: 0.51928 | val_1_rmse: 0.53573 |  0:03:29s
epoch 143| loss: 0.3021  | val_0_rmse: 0.51857 | val_1_rmse: 0.53605 |  0:03:30s
epoch 144| loss: 0.29829 | val_0_rmse: 0.5461  | val_1_rmse: 0.55926 |  0:03:32s
epoch 145| loss: 0.30328 | val_0_rmse: 0.53347 | val_1_rmse: 0.54531 |  0:03:33s
epoch 146| loss: 0.30558 | val_0_rmse: 0.52069 | val_1_rmse: 0.53878 |  0:03:35s
epoch 147| loss: 0.29255 | val_0_rmse: 0.53221 | val_1_rmse: 0.55257 |  0:03:36s
epoch 148| loss: 0.29664 | val_0_rmse: 0.51972 | val_1_rmse: 0.53533 |  0:03:37s
epoch 149| loss: 0.30809 | val_0_rmse: 0.52412 | val_1_rmse: 0.5441  |  0:03:39s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.53533
Best weights from best epoch are automatically used!
ended training at: 05:44:28
Feature importance:
[('Area', 0.17920190037180742), ('Baths', 0.13183155993644577), ('Beds', 0.00379138510325806), ('Latitude', 0.25791667588401207), ('Longitude', 0.30003922363279123), ('Month', 0.0), ('Year', 0.12721925507168547)]
Mean squared error is of 6532932830.681442
Mean absolute error:54535.090477956095
MAPE:0.17276279008321366
R2 score:0.7055122832636458
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:46:55
epoch 0  | loss: 0.6085  | val_0_rmse: 0.72494 | val_1_rmse: 0.72512 |  0:00:05s
epoch 1  | loss: 0.50751 | val_0_rmse: 0.69588 | val_1_rmse: 0.69789 |  0:00:11s
epoch 2  | loss: 0.45037 | val_0_rmse: 0.78125 | val_1_rmse: 0.78522 |  0:00:16s
epoch 3  | loss: 0.44193 | val_0_rmse: 0.85123 | val_1_rmse: 0.85715 |  0:00:22s
epoch 4  | loss: 0.43341 | val_0_rmse: 0.65279 | val_1_rmse: 0.65592 |  0:00:27s
epoch 5  | loss: 0.42463 | val_0_rmse: 0.64413 | val_1_rmse: 0.64893 |  0:00:33s
epoch 6  | loss: 0.42812 | val_0_rmse: 0.69402 | val_1_rmse: 0.69993 |  0:00:38s
epoch 7  | loss: 0.42843 | val_0_rmse: 0.65624 | val_1_rmse: 0.66345 |  0:00:44s
epoch 8  | loss: 0.42582 | val_0_rmse: 0.72344 | val_1_rmse: 0.72843 |  0:00:49s
epoch 9  | loss: 0.41413 | val_0_rmse: 0.65707 | val_1_rmse: 0.66002 |  0:00:55s
epoch 10 | loss: 0.42208 | val_0_rmse: 0.64168 | val_1_rmse: 0.64373 |  0:01:00s
epoch 11 | loss: 0.41696 | val_0_rmse: 0.67631 | val_1_rmse: 0.67744 |  0:01:06s
epoch 12 | loss: 0.40963 | val_0_rmse: 0.66846 | val_1_rmse: 0.66923 |  0:01:11s
epoch 13 | loss: 0.40312 | val_0_rmse: 0.69054 | val_1_rmse: 0.69162 |  0:01:17s
epoch 14 | loss: 0.41146 | val_0_rmse: 0.65351 | val_1_rmse: 0.65509 |  0:01:22s
epoch 15 | loss: 0.40341 | val_0_rmse: 0.67188 | val_1_rmse: 0.67836 |  0:01:28s
epoch 16 | loss: 0.40369 | val_0_rmse: 0.6275  | val_1_rmse: 0.62941 |  0:01:33s
epoch 17 | loss: 0.40052 | val_0_rmse: 0.6834  | val_1_rmse: 0.68765 |  0:01:39s
epoch 18 | loss: 0.40039 | val_0_rmse: 0.75847 | val_1_rmse: 0.76051 |  0:01:44s
epoch 19 | loss: 0.40159 | val_0_rmse: 0.63641 | val_1_rmse: 0.63804 |  0:01:50s
epoch 20 | loss: 0.3989  | val_0_rmse: 0.65697 | val_1_rmse: 0.66104 |  0:01:56s
epoch 21 | loss: 0.39955 | val_0_rmse: 0.66548 | val_1_rmse: 0.67115 |  0:02:01s
epoch 22 | loss: 0.40098 | val_0_rmse: 0.65652 | val_1_rmse: 0.65863 |  0:02:07s
epoch 23 | loss: 0.39925 | val_0_rmse: 0.69748 | val_1_rmse: 0.70164 |  0:02:12s
epoch 24 | loss: 0.39637 | val_0_rmse: 0.70606 | val_1_rmse: 0.71078 |  0:02:18s
epoch 25 | loss: 0.3982  | val_0_rmse: 0.66237 | val_1_rmse: 0.66348 |  0:02:23s
epoch 26 | loss: 0.3981  | val_0_rmse: 0.71086 | val_1_rmse: 0.71284 |  0:02:29s
epoch 27 | loss: 0.39356 | val_0_rmse: 0.6708  | val_1_rmse: 0.67185 |  0:02:34s
epoch 28 | loss: 0.39908 | val_0_rmse: 0.72291 | val_1_rmse: 0.72603 |  0:02:40s
epoch 29 | loss: 0.39929 | val_0_rmse: 0.64119 | val_1_rmse: 0.64781 |  0:02:45s
epoch 30 | loss: 0.39456 | val_0_rmse: 0.61857 | val_1_rmse: 0.62266 |  0:02:51s
epoch 31 | loss: 0.3915  | val_0_rmse: 0.69311 | val_1_rmse: 0.6975  |  0:02:56s
epoch 32 | loss: 0.39472 | val_0_rmse: 0.68496 | val_1_rmse: 0.6892  |  0:03:02s
epoch 33 | loss: 0.39104 | val_0_rmse: 0.78539 | val_1_rmse: 0.78653 |  0:03:07s
epoch 34 | loss: 0.39446 | val_0_rmse: 0.6346  | val_1_rmse: 0.63801 |  0:03:13s
epoch 35 | loss: 0.39282 | val_0_rmse: 0.71797 | val_1_rmse: 0.72059 |  0:03:18s
epoch 36 | loss: 0.40663 | val_0_rmse: 0.71397 | val_1_rmse: 0.71596 |  0:03:24s
epoch 37 | loss: 0.40861 | val_0_rmse: 0.63982 | val_1_rmse: 0.64427 |  0:03:30s
epoch 38 | loss: 0.40612 | val_0_rmse: 0.65162 | val_1_rmse: 0.65658 |  0:03:35s
epoch 39 | loss: 0.40641 | val_0_rmse: 0.66144 | val_1_rmse: 0.66638 |  0:03:41s
epoch 40 | loss: 0.4041  | val_0_rmse: 0.64079 | val_1_rmse: 0.64436 |  0:03:46s
epoch 41 | loss: 0.40584 | val_0_rmse: 0.63906 | val_1_rmse: 0.64381 |  0:03:52s
epoch 42 | loss: 0.4076  | val_0_rmse: 0.6729  | val_1_rmse: 0.67611 |  0:03:57s
epoch 43 | loss: 0.40277 | val_0_rmse: 0.68309 | val_1_rmse: 0.6878  |  0:04:03s
epoch 44 | loss: 0.4019  | val_0_rmse: 0.78462 | val_1_rmse: 0.78778 |  0:04:08s
epoch 45 | loss: 0.40086 | val_0_rmse: 0.643   | val_1_rmse: 0.64667 |  0:04:14s
epoch 46 | loss: 0.41464 | val_0_rmse: 0.7182  | val_1_rmse: 0.72351 |  0:04:19s
epoch 47 | loss: 0.40685 | val_0_rmse: 0.77695 | val_1_rmse: 0.78066 |  0:04:25s
epoch 48 | loss: 0.40227 | val_0_rmse: 0.80703 | val_1_rmse: 0.80865 |  0:04:30s
epoch 49 | loss: 0.39602 | val_0_rmse: 0.70458 | val_1_rmse: 0.70823 |  0:04:36s
epoch 50 | loss: 0.3944  | val_0_rmse: 0.65762 | val_1_rmse: 0.66275 |  0:04:41s
epoch 51 | loss: 0.39822 | val_0_rmse: 0.65133 | val_1_rmse: 0.65601 |  0:04:47s
epoch 52 | loss: 0.39541 | val_0_rmse: 0.64837 | val_1_rmse: 0.65042 |  0:04:52s
epoch 53 | loss: 0.39287 | val_0_rmse: 0.68847 | val_1_rmse: 0.69397 |  0:04:58s
epoch 54 | loss: 0.39908 | val_0_rmse: 0.67329 | val_1_rmse: 0.6767  |  0:05:03s
epoch 55 | loss: 0.39559 | val_0_rmse: 0.63338 | val_1_rmse: 0.63696 |  0:05:09s
epoch 56 | loss: 0.3975  | val_0_rmse: 0.67361 | val_1_rmse: 0.6787  |  0:05:14s
epoch 57 | loss: 0.39381 | val_0_rmse: 0.64396 | val_1_rmse: 0.64923 |  0:05:20s
epoch 58 | loss: 0.3936  | val_0_rmse: 0.6378  | val_1_rmse: 0.6414  |  0:05:25s
epoch 59 | loss: 0.39724 | val_0_rmse: 0.76216 | val_1_rmse: 0.7632  |  0:05:31s
epoch 60 | loss: 0.39674 | val_0_rmse: 0.68032 | val_1_rmse: 0.6833  |  0:05:37s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.62266
Best weights from best epoch are automatically used!
ended training at: 05:52:33
Feature importance:
[('Area', 0.4193323072742847), ('Baths', 0.2575019340410258), ('Beds', 0.03640390041665206), ('Latitude', 0.09633321314521863), ('Longitude', 0.19042864512281876), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2578887869.774507
Mean absolute error:35811.53872865797
MAPE:0.31863055792032324
R2 score:0.623542102889823
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:34
epoch 0  | loss: 0.60594 | val_0_rmse: 0.7221  | val_1_rmse: 0.71472 |  0:00:05s
epoch 1  | loss: 0.52752 | val_0_rmse: 0.70054 | val_1_rmse: 0.69469 |  0:00:11s
epoch 2  | loss: 0.49014 | val_0_rmse: 0.66352 | val_1_rmse: 0.65226 |  0:00:16s
epoch 3  | loss: 0.4559  | val_0_rmse: 0.65506 | val_1_rmse: 0.6458  |  0:00:22s
epoch 4  | loss: 0.44517 | val_0_rmse: 0.66592 | val_1_rmse: 0.65878 |  0:00:27s
epoch 5  | loss: 0.44138 | val_0_rmse: 0.67279 | val_1_rmse: 0.66417 |  0:00:33s
epoch 6  | loss: 0.44272 | val_0_rmse: 0.67331 | val_1_rmse: 0.66434 |  0:00:38s
epoch 7  | loss: 0.42638 | val_0_rmse: 0.67346 | val_1_rmse: 0.66331 |  0:00:44s
epoch 8  | loss: 0.42447 | val_0_rmse: 0.71272 | val_1_rmse: 0.70223 |  0:00:49s
epoch 9  | loss: 0.42349 | val_0_rmse: 0.63629 | val_1_rmse: 0.62733 |  0:00:55s
epoch 10 | loss: 0.41342 | val_0_rmse: 0.66383 | val_1_rmse: 0.65671 |  0:01:00s
epoch 11 | loss: 0.41079 | val_0_rmse: 0.67504 | val_1_rmse: 0.66461 |  0:01:06s
epoch 12 | loss: 0.40947 | val_0_rmse: 0.69534 | val_1_rmse: 0.6886  |  0:01:11s
epoch 13 | loss: 0.40668 | val_0_rmse: 0.77418 | val_1_rmse: 0.76533 |  0:01:17s
epoch 14 | loss: 0.40424 | val_0_rmse: 0.6353  | val_1_rmse: 0.62785 |  0:01:22s
epoch 15 | loss: 0.40334 | val_0_rmse: 0.66133 | val_1_rmse: 0.65572 |  0:01:28s
epoch 16 | loss: 0.40554 | val_0_rmse: 0.698   | val_1_rmse: 0.68827 |  0:01:34s
epoch 17 | loss: 0.40727 | val_0_rmse: 0.65629 | val_1_rmse: 0.64751 |  0:01:39s
epoch 18 | loss: 0.40435 | val_0_rmse: 0.69643 | val_1_rmse: 0.68825 |  0:01:45s
epoch 19 | loss: 0.40237 | val_0_rmse: 0.64923 | val_1_rmse: 0.64511 |  0:01:50s
epoch 20 | loss: 0.39422 | val_0_rmse: 0.68526 | val_1_rmse: 0.67868 |  0:01:56s
epoch 21 | loss: 0.39636 | val_0_rmse: 0.72509 | val_1_rmse: 0.7184  |  0:02:01s
epoch 22 | loss: 0.3928  | val_0_rmse: 0.64293 | val_1_rmse: 0.6356  |  0:02:07s
epoch 23 | loss: 0.3948  | val_0_rmse: 0.65801 | val_1_rmse: 0.65268 |  0:02:12s
epoch 24 | loss: 0.39313 | val_0_rmse: 0.66348 | val_1_rmse: 0.65731 |  0:02:18s
epoch 25 | loss: 0.39074 | val_0_rmse: 0.62547 | val_1_rmse: 0.61888 |  0:02:23s
epoch 26 | loss: 0.3883  | val_0_rmse: 0.63571 | val_1_rmse: 0.62776 |  0:02:29s
epoch 27 | loss: 0.38389 | val_0_rmse: 0.70157 | val_1_rmse: 0.69559 |  0:02:34s
epoch 28 | loss: 0.38656 | val_0_rmse: 0.64481 | val_1_rmse: 0.6381  |  0:02:40s
epoch 29 | loss: 0.38403 | val_0_rmse: 0.68697 | val_1_rmse: 0.67896 |  0:02:45s
epoch 30 | loss: 0.38707 | val_0_rmse: 0.79974 | val_1_rmse: 0.79378 |  0:02:51s
epoch 31 | loss: 0.38617 | val_0_rmse: 0.65998 | val_1_rmse: 0.65418 |  0:02:56s
epoch 32 | loss: 0.38682 | val_0_rmse: 0.62684 | val_1_rmse: 0.62097 |  0:03:02s
epoch 33 | loss: 0.38902 | val_0_rmse: 0.64835 | val_1_rmse: 0.64298 |  0:03:07s
epoch 34 | loss: 0.39789 | val_0_rmse: 0.65141 | val_1_rmse: 0.64427 |  0:03:13s
epoch 35 | loss: 0.38751 | val_0_rmse: 0.67504 | val_1_rmse: 0.66705 |  0:03:18s
epoch 36 | loss: 0.40212 | val_0_rmse: 0.62758 | val_1_rmse: 0.62166 |  0:03:24s
epoch 37 | loss: 0.38917 | val_0_rmse: 0.65701 | val_1_rmse: 0.65028 |  0:03:29s
epoch 38 | loss: 0.38664 | val_0_rmse: 0.64208 | val_1_rmse: 0.6347  |  0:03:35s
epoch 39 | loss: 0.395   | val_0_rmse: 0.63337 | val_1_rmse: 0.62866 |  0:03:40s
epoch 40 | loss: 0.39328 | val_0_rmse: 0.62415 | val_1_rmse: 0.6189  |  0:03:46s
epoch 41 | loss: 0.39504 | val_0_rmse: 0.64188 | val_1_rmse: 0.63507 |  0:03:52s
epoch 42 | loss: 0.39113 | val_0_rmse: 0.6455  | val_1_rmse: 0.6389  |  0:03:57s
epoch 43 | loss: 0.38801 | val_0_rmse: 0.75173 | val_1_rmse: 0.74548 |  0:04:03s
epoch 44 | loss: 0.38416 | val_0_rmse: 0.61818 | val_1_rmse: 0.61198 |  0:04:08s
epoch 45 | loss: 0.38171 | val_0_rmse: 0.65574 | val_1_rmse: 0.65174 |  0:04:14s
epoch 46 | loss: 0.38087 | val_0_rmse: 0.63542 | val_1_rmse: 0.62921 |  0:04:19s
epoch 47 | loss: 0.38001 | val_0_rmse: 0.62054 | val_1_rmse: 0.61263 |  0:04:25s
epoch 48 | loss: 0.38096 | val_0_rmse: 0.7459  | val_1_rmse: 0.73867 |  0:04:30s
epoch 49 | loss: 0.38113 | val_0_rmse: 0.66363 | val_1_rmse: 0.6572  |  0:04:36s
epoch 50 | loss: 0.38059 | val_0_rmse: 0.86848 | val_1_rmse: 0.86086 |  0:04:41s
epoch 51 | loss: 0.3804  | val_0_rmse: 0.6194  | val_1_rmse: 0.61227 |  0:04:47s
epoch 52 | loss: 0.38095 | val_0_rmse: 0.6303  | val_1_rmse: 0.62358 |  0:04:52s
epoch 53 | loss: 0.38306 | val_0_rmse: 0.61634 | val_1_rmse: 0.61023 |  0:04:58s
epoch 54 | loss: 0.38031 | val_0_rmse: 0.60779 | val_1_rmse: 0.60244 |  0:05:03s
epoch 55 | loss: 0.3789  | val_0_rmse: 0.6119  | val_1_rmse: 0.60696 |  0:05:09s
epoch 56 | loss: 0.3851  | val_0_rmse: 0.62941 | val_1_rmse: 0.62371 |  0:05:14s
epoch 57 | loss: 0.37822 | val_0_rmse: 0.70708 | val_1_rmse: 0.6995  |  0:05:20s
epoch 58 | loss: 0.37655 | val_0_rmse: 0.65407 | val_1_rmse: 0.64925 |  0:05:25s
epoch 59 | loss: 0.37847 | val_0_rmse: 0.62792 | val_1_rmse: 0.62042 |  0:05:31s
epoch 60 | loss: 0.38263 | val_0_rmse: 0.65182 | val_1_rmse: 0.64405 |  0:05:36s
epoch 61 | loss: 0.3869  | val_0_rmse: 0.64161 | val_1_rmse: 0.63578 |  0:05:42s
epoch 62 | loss: 0.38891 | val_0_rmse: 0.63919 | val_1_rmse: 0.63345 |  0:05:48s
epoch 63 | loss: 0.39293 | val_0_rmse: 0.63739 | val_1_rmse: 0.63139 |  0:05:53s
epoch 64 | loss: 0.38804 | val_0_rmse: 0.68909 | val_1_rmse: 0.68326 |  0:05:59s
epoch 65 | loss: 0.39112 | val_0_rmse: 0.79287 | val_1_rmse: 0.78924 |  0:06:04s
epoch 66 | loss: 0.38848 | val_0_rmse: 0.71071 | val_1_rmse: 0.70439 |  0:06:10s
epoch 67 | loss: 0.38762 | val_0_rmse: 0.62346 | val_1_rmse: 0.61738 |  0:06:15s
epoch 68 | loss: 0.39082 | val_0_rmse: 0.63538 | val_1_rmse: 0.62893 |  0:06:21s
epoch 69 | loss: 0.38816 | val_0_rmse: 0.61884 | val_1_rmse: 0.61422 |  0:06:26s
epoch 70 | loss: 0.38532 | val_0_rmse: 0.74681 | val_1_rmse: 0.74322 |  0:06:32s
epoch 71 | loss: 0.38684 | val_0_rmse: 0.66034 | val_1_rmse: 0.654   |  0:06:37s
epoch 72 | loss: 0.38449 | val_0_rmse: 0.66173 | val_1_rmse: 0.65563 |  0:06:43s
epoch 73 | loss: 0.38434 | val_0_rmse: 0.66702 | val_1_rmse: 0.66095 |  0:06:48s
epoch 74 | loss: 0.38551 | val_0_rmse: 0.67127 | val_1_rmse: 0.6633  |  0:06:54s
epoch 75 | loss: 0.38854 | val_0_rmse: 0.82595 | val_1_rmse: 0.81868 |  0:06:59s
epoch 76 | loss: 0.38527 | val_0_rmse: 0.6738  | val_1_rmse: 0.66651 |  0:07:05s
epoch 77 | loss: 0.38642 | val_0_rmse: 0.62886 | val_1_rmse: 0.62356 |  0:07:10s
epoch 78 | loss: 0.38448 | val_0_rmse: 0.75296 | val_1_rmse: 0.74653 |  0:07:16s
epoch 79 | loss: 0.38427 | val_0_rmse: 0.66553 | val_1_rmse: 0.66181 |  0:07:21s
epoch 80 | loss: 0.38674 | val_0_rmse: 0.68936 | val_1_rmse: 0.68356 |  0:07:27s
epoch 81 | loss: 0.38417 | val_0_rmse: 0.81475 | val_1_rmse: 0.80663 |  0:07:32s
epoch 82 | loss: 0.38486 | val_0_rmse: 0.66046 | val_1_rmse: 0.6537  |  0:07:38s
epoch 83 | loss: 0.38212 | val_0_rmse: 0.67812 | val_1_rmse: 0.67083 |  0:07:44s
epoch 84 | loss: 0.38243 | val_0_rmse: 0.87349 | val_1_rmse: 0.86785 |  0:07:49s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.60244
Best weights from best epoch are automatically used!
ended training at: 06:00:26
Feature importance:
[('Area', 0.42732353848034016), ('Baths', 0.12044408179765351), ('Beds', 0.0), ('Latitude', 0.361559762533181), ('Longitude', 0.09067261718882536), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2304067117.879961
Mean absolute error:34024.45074775041
MAPE:0.30758197196168685
R2 score:0.6605110728652055
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:00:26
epoch 0  | loss: 0.62198 | val_0_rmse: 0.72901 | val_1_rmse: 0.73933 |  0:00:05s
epoch 1  | loss: 0.51903 | val_0_rmse: 0.70067 | val_1_rmse: 0.71031 |  0:00:11s
epoch 2  | loss: 0.49725 | val_0_rmse: 0.72741 | val_1_rmse: 0.73529 |  0:00:17s
epoch 3  | loss: 0.47931 | val_0_rmse: 0.672   | val_1_rmse: 0.67729 |  0:00:22s
epoch 4  | loss: 0.44186 | val_0_rmse: 0.76671 | val_1_rmse: 0.76296 |  0:00:28s
epoch 5  | loss: 0.43181 | val_0_rmse: 0.64578 | val_1_rmse: 0.65155 |  0:00:33s
epoch 6  | loss: 0.43306 | val_0_rmse: 0.69938 | val_1_rmse: 0.69778 |  0:00:39s
epoch 7  | loss: 0.41984 | val_0_rmse: 0.64801 | val_1_rmse: 0.65389 |  0:00:44s
epoch 8  | loss: 0.42003 | val_0_rmse: 0.64077 | val_1_rmse: 0.64764 |  0:00:50s
epoch 9  | loss: 0.43027 | val_0_rmse: 0.65237 | val_1_rmse: 0.6619  |  0:00:55s
epoch 10 | loss: 0.42014 | val_0_rmse: 0.69821 | val_1_rmse: 0.70857 |  0:01:01s
epoch 11 | loss: 0.42075 | val_0_rmse: 0.64121 | val_1_rmse: 0.64647 |  0:01:06s
epoch 12 | loss: 0.41574 | val_0_rmse: 0.65224 | val_1_rmse: 0.65988 |  0:01:12s
epoch 13 | loss: 0.41768 | val_0_rmse: 0.66006 | val_1_rmse: 0.66406 |  0:01:17s
epoch 14 | loss: 0.41651 | val_0_rmse: 0.66966 | val_1_rmse: 0.67468 |  0:01:23s
epoch 15 | loss: 0.41619 | val_0_rmse: 0.63846 | val_1_rmse: 0.64585 |  0:01:28s
epoch 16 | loss: 0.41466 | val_0_rmse: 0.63915 | val_1_rmse: 0.64627 |  0:01:34s
epoch 17 | loss: 0.41193 | val_0_rmse: 0.67988 | val_1_rmse: 0.69113 |  0:01:39s
epoch 18 | loss: 0.41473 | val_0_rmse: 0.64127 | val_1_rmse: 0.64861 |  0:01:45s
epoch 19 | loss: 0.4197  | val_0_rmse: 0.7079  | val_1_rmse: 0.70831 |  0:01:51s
epoch 20 | loss: 0.41872 | val_0_rmse: 0.6382  | val_1_rmse: 0.64313 |  0:01:56s
epoch 21 | loss: 0.41409 | val_0_rmse: 0.64195 | val_1_rmse: 0.64883 |  0:02:02s
epoch 22 | loss: 0.41522 | val_0_rmse: 0.66523 | val_1_rmse: 0.66725 |  0:02:07s
epoch 23 | loss: 0.41259 | val_0_rmse: 0.63122 | val_1_rmse: 0.6369  |  0:02:13s
epoch 24 | loss: 0.41251 | val_0_rmse: 0.67076 | val_1_rmse: 0.67094 |  0:02:18s
epoch 25 | loss: 0.40968 | val_0_rmse: 0.62704 | val_1_rmse: 0.63375 |  0:02:24s
epoch 26 | loss: 0.40865 | val_0_rmse: 0.63675 | val_1_rmse: 0.64175 |  0:02:29s
epoch 27 | loss: 0.40743 | val_0_rmse: 0.66998 | val_1_rmse: 0.67463 |  0:02:35s
epoch 28 | loss: 0.41197 | val_0_rmse: 0.64825 | val_1_rmse: 0.65257 |  0:02:40s
epoch 29 | loss: 0.40636 | val_0_rmse: 0.6656  | val_1_rmse: 0.66631 |  0:02:46s
epoch 30 | loss: 0.40674 | val_0_rmse: 0.64717 | val_1_rmse: 0.65319 |  0:02:51s
epoch 31 | loss: 0.40377 | val_0_rmse: 0.64937 | val_1_rmse: 0.65242 |  0:02:57s
epoch 32 | loss: 0.40438 | val_0_rmse: 0.66254 | val_1_rmse: 0.66449 |  0:03:02s
epoch 33 | loss: 0.40595 | val_0_rmse: 0.63302 | val_1_rmse: 0.63882 |  0:03:08s
epoch 34 | loss: 0.40303 | val_0_rmse: 0.65156 | val_1_rmse: 0.65949 |  0:03:13s
epoch 35 | loss: 0.40032 | val_0_rmse: 0.6474  | val_1_rmse: 0.65519 |  0:03:19s
epoch 36 | loss: 0.40313 | val_0_rmse: 0.6453  | val_1_rmse: 0.65203 |  0:03:24s
epoch 37 | loss: 0.39939 | val_0_rmse: 0.67329 | val_1_rmse: 0.6762  |  0:03:30s
epoch 38 | loss: 0.40032 | val_0_rmse: 0.64382 | val_1_rmse: 0.64854 |  0:03:35s
epoch 39 | loss: 0.40156 | val_0_rmse: 0.65307 | val_1_rmse: 0.65988 |  0:03:41s
epoch 40 | loss: 0.39785 | val_0_rmse: 0.63754 | val_1_rmse: 0.64463 |  0:03:47s
epoch 41 | loss: 0.39943 | val_0_rmse: 0.67033 | val_1_rmse: 0.67465 |  0:03:52s
epoch 42 | loss: 0.39953 | val_0_rmse: 0.64744 | val_1_rmse: 0.6546  |  0:03:58s
epoch 43 | loss: 0.40154 | val_0_rmse: 0.63664 | val_1_rmse: 0.64197 |  0:04:03s
epoch 44 | loss: 0.39886 | val_0_rmse: 0.62098 | val_1_rmse: 0.62634 |  0:04:09s
epoch 45 | loss: 0.40052 | val_0_rmse: 0.65594 | val_1_rmse: 0.66187 |  0:04:14s
epoch 46 | loss: 0.39608 | val_0_rmse: 0.66467 | val_1_rmse: 0.66625 |  0:04:20s
epoch 47 | loss: 0.39717 | val_0_rmse: 0.62946 | val_1_rmse: 0.63592 |  0:04:25s
epoch 48 | loss: 0.39764 | val_0_rmse: 0.63914 | val_1_rmse: 0.64538 |  0:04:31s
epoch 49 | loss: 0.39587 | val_0_rmse: 0.63445 | val_1_rmse: 0.64117 |  0:04:36s
epoch 50 | loss: 0.39468 | val_0_rmse: 0.65841 | val_1_rmse: 0.66547 |  0:04:42s
epoch 51 | loss: 0.39652 | val_0_rmse: 0.64642 | val_1_rmse: 0.65256 |  0:04:47s
epoch 52 | loss: 0.3985  | val_0_rmse: 0.63999 | val_1_rmse: 0.64389 |  0:04:53s
epoch 53 | loss: 0.39742 | val_0_rmse: 0.61957 | val_1_rmse: 0.62552 |  0:04:59s
epoch 54 | loss: 0.39562 | val_0_rmse: 0.63327 | val_1_rmse: 0.63943 |  0:05:04s
epoch 55 | loss: 0.39632 | val_0_rmse: 0.65583 | val_1_rmse: 0.66085 |  0:05:10s
epoch 56 | loss: 0.39433 | val_0_rmse: 0.62192 | val_1_rmse: 0.62762 |  0:05:15s
epoch 57 | loss: 0.39411 | val_0_rmse: 0.63954 | val_1_rmse: 0.64492 |  0:05:21s
epoch 58 | loss: 0.39783 | val_0_rmse: 0.6489  | val_1_rmse: 0.65412 |  0:05:26s
epoch 59 | loss: 0.39512 | val_0_rmse: 0.64002 | val_1_rmse: 0.64936 |  0:05:32s
epoch 60 | loss: 0.39402 | val_0_rmse: 0.64818 | val_1_rmse: 0.65468 |  0:05:37s
epoch 61 | loss: 0.39402 | val_0_rmse: 0.65098 | val_1_rmse: 0.65908 |  0:05:43s
epoch 62 | loss: 0.39313 | val_0_rmse: 0.64299 | val_1_rmse: 0.65153 |  0:05:48s
epoch 63 | loss: 0.39274 | val_0_rmse: 0.67114 | val_1_rmse: 0.67866 |  0:05:54s
epoch 64 | loss: 0.39428 | val_0_rmse: 0.65338 | val_1_rmse: 0.65727 |  0:06:00s
epoch 65 | loss: 0.39461 | val_0_rmse: 0.67261 | val_1_rmse: 0.67495 |  0:06:05s
epoch 66 | loss: 0.39216 | val_0_rmse: 0.65035 | val_1_rmse: 0.65384 |  0:06:11s
epoch 67 | loss: 0.39394 | val_0_rmse: 0.69071 | val_1_rmse: 0.6919  |  0:06:16s
epoch 68 | loss: 0.39317 | val_0_rmse: 0.62506 | val_1_rmse: 0.62902 |  0:06:22s
epoch 69 | loss: 0.39544 | val_0_rmse: 0.6486  | val_1_rmse: 0.65008 |  0:06:27s
epoch 70 | loss: 0.39457 | val_0_rmse: 0.62592 | val_1_rmse: 0.63331 |  0:06:33s
epoch 71 | loss: 0.39227 | val_0_rmse: 0.64468 | val_1_rmse: 0.64883 |  0:06:38s
epoch 72 | loss: 0.39353 | val_0_rmse: 0.62151 | val_1_rmse: 0.62808 |  0:06:44s
epoch 73 | loss: 0.39281 | val_0_rmse: 0.68446 | val_1_rmse: 0.69057 |  0:06:49s
epoch 74 | loss: 0.39409 | val_0_rmse: 0.65176 | val_1_rmse: 0.6604  |  0:06:55s
epoch 75 | loss: 0.39234 | val_0_rmse: 0.61996 | val_1_rmse: 0.62517 |  0:07:01s
epoch 76 | loss: 0.39259 | val_0_rmse: 0.63122 | val_1_rmse: 0.63689 |  0:07:06s
epoch 77 | loss: 0.39052 | val_0_rmse: 0.66269 | val_1_rmse: 0.6654  |  0:07:12s
epoch 78 | loss: 0.39158 | val_0_rmse: 0.65149 | val_1_rmse: 0.65414 |  0:07:17s
epoch 79 | loss: 0.3921  | val_0_rmse: 0.63295 | val_1_rmse: 0.63794 |  0:07:23s
epoch 80 | loss: 0.39229 | val_0_rmse: 0.64588 | val_1_rmse: 0.65131 |  0:07:28s
epoch 81 | loss: 0.38942 | val_0_rmse: 0.67853 | val_1_rmse: 0.68862 |  0:07:34s
epoch 82 | loss: 0.39239 | val_0_rmse: 0.62329 | val_1_rmse: 0.62791 |  0:07:39s
epoch 83 | loss: 0.3936  | val_0_rmse: 0.6535  | val_1_rmse: 0.6638  |  0:07:45s
epoch 84 | loss: 0.39232 | val_0_rmse: 0.65287 | val_1_rmse: 0.6556  |  0:07:50s
epoch 85 | loss: 0.39045 | val_0_rmse: 0.6632  | val_1_rmse: 0.66857 |  0:07:56s
epoch 86 | loss: 0.39029 | val_0_rmse: 0.66277 | val_1_rmse: 0.66593 |  0:08:01s
epoch 87 | loss: 0.39163 | val_0_rmse: 0.63311 | val_1_rmse: 0.63657 |  0:08:07s
epoch 88 | loss: 0.39068 | val_0_rmse: 0.66826 | val_1_rmse: 0.67071 |  0:08:12s
epoch 89 | loss: 0.39004 | val_0_rmse: 0.64288 | val_1_rmse: 0.65082 |  0:08:18s
epoch 90 | loss: 0.39168 | val_0_rmse: 0.64115 | val_1_rmse: 0.64327 |  0:08:23s
epoch 91 | loss: 0.39234 | val_0_rmse: 0.62777 | val_1_rmse: 0.6309  |  0:08:29s
epoch 92 | loss: 0.38966 | val_0_rmse: 0.6279  | val_1_rmse: 0.63301 |  0:08:34s
epoch 93 | loss: 0.39122 | val_0_rmse: 0.63688 | val_1_rmse: 0.64059 |  0:08:40s
epoch 94 | loss: 0.39577 | val_0_rmse: 0.65428 | val_1_rmse: 0.66041 |  0:08:46s
epoch 95 | loss: 0.46379 | val_0_rmse: 0.67133 | val_1_rmse: 0.67862 |  0:08:51s
epoch 96 | loss: 0.45526 | val_0_rmse: 0.66579 | val_1_rmse: 0.67216 |  0:08:57s
epoch 97 | loss: 0.42377 | val_0_rmse: 0.71085 | val_1_rmse: 0.71122 |  0:09:02s
epoch 98 | loss: 0.40131 | val_0_rmse: 0.63551 | val_1_rmse: 0.64198 |  0:09:08s
epoch 99 | loss: 0.39523 | val_0_rmse: 0.6546  | val_1_rmse: 0.65603 |  0:09:13s
epoch 100| loss: 0.39895 | val_0_rmse: 0.63303 | val_1_rmse: 0.63951 |  0:09:19s
epoch 101| loss: 0.39451 | val_0_rmse: 0.62976 | val_1_rmse: 0.63696 |  0:09:24s
epoch 102| loss: 0.39333 | val_0_rmse: 0.65267 | val_1_rmse: 0.6562  |  0:09:30s
epoch 103| loss: 0.39674 | val_0_rmse: 0.67282 | val_1_rmse: 0.67618 |  0:09:35s
epoch 104| loss: 0.40788 | val_0_rmse: 0.65193 | val_1_rmse: 0.65601 |  0:09:41s
epoch 105| loss: 0.40011 | val_0_rmse: 0.63761 | val_1_rmse: 0.64294 |  0:09:46s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.62517
Best weights from best epoch are automatically used!
ended training at: 06:10:15
Feature importance:
[('Area', 0.18044271713642276), ('Baths', 0.06924913560731215), ('Beds', 0.0), ('Latitude', 0.47201671194448014), ('Longitude', 0.27829143531178496), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2565822117.564152
Mean absolute error:35624.5244378766
MAPE:0.30716432317759046
R2 score:0.6210940603299344
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:10:15
epoch 0  | loss: 0.61962 | val_0_rmse: 0.72545 | val_1_rmse: 0.72254 |  0:00:05s
epoch 1  | loss: 0.51093 | val_0_rmse: 0.6908  | val_1_rmse: 0.69125 |  0:00:11s
epoch 2  | loss: 0.44653 | val_0_rmse: 0.64635 | val_1_rmse: 0.64571 |  0:00:16s
epoch 3  | loss: 0.43163 | val_0_rmse: 0.64101 | val_1_rmse: 0.64032 |  0:00:22s
epoch 4  | loss: 0.42737 | val_0_rmse: 0.65985 | val_1_rmse: 0.6609  |  0:00:27s
epoch 5  | loss: 0.44541 | val_0_rmse: 0.65696 | val_1_rmse: 0.66058 |  0:00:33s
epoch 6  | loss: 0.42422 | val_0_rmse: 0.67506 | val_1_rmse: 0.67832 |  0:00:38s
epoch 7  | loss: 0.41596 | val_0_rmse: 0.64165 | val_1_rmse: 0.64332 |  0:00:44s
epoch 8  | loss: 0.41459 | val_0_rmse: 0.63882 | val_1_rmse: 0.64203 |  0:00:49s
epoch 9  | loss: 0.41432 | val_0_rmse: 0.64021 | val_1_rmse: 0.6432  |  0:00:55s
epoch 10 | loss: 0.4096  | val_0_rmse: 0.63047 | val_1_rmse: 0.63452 |  0:01:00s
epoch 11 | loss: 0.40998 | val_0_rmse: 0.64591 | val_1_rmse: 0.65024 |  0:01:06s
epoch 12 | loss: 0.41011 | val_0_rmse: 0.64374 | val_1_rmse: 0.64682 |  0:01:12s
epoch 13 | loss: 0.40998 | val_0_rmse: 0.65319 | val_1_rmse: 0.65602 |  0:01:17s
epoch 14 | loss: 0.40842 | val_0_rmse: 0.6314  | val_1_rmse: 0.63437 |  0:01:23s
epoch 15 | loss: 0.41325 | val_0_rmse: 0.63241 | val_1_rmse: 0.6346  |  0:01:28s
epoch 16 | loss: 0.40555 | val_0_rmse: 0.65664 | val_1_rmse: 0.65936 |  0:01:34s
epoch 17 | loss: 0.40831 | val_0_rmse: 0.67915 | val_1_rmse: 0.68027 |  0:01:39s
epoch 18 | loss: 0.4054  | val_0_rmse: 0.62592 | val_1_rmse: 0.6278  |  0:01:45s
epoch 19 | loss: 0.40456 | val_0_rmse: 0.67725 | val_1_rmse: 0.67787 |  0:01:50s
epoch 20 | loss: 0.40456 | val_0_rmse: 0.63559 | val_1_rmse: 0.63829 |  0:01:56s
epoch 21 | loss: 0.40201 | val_0_rmse: 0.62126 | val_1_rmse: 0.62306 |  0:02:01s
epoch 22 | loss: 0.3942  | val_0_rmse: 0.62514 | val_1_rmse: 0.62662 |  0:02:07s
epoch 23 | loss: 0.3928  | val_0_rmse: 0.62586 | val_1_rmse: 0.62747 |  0:02:12s
epoch 24 | loss: 0.39442 | val_0_rmse: 0.69996 | val_1_rmse: 0.70037 |  0:02:18s
epoch 25 | loss: 0.39805 | val_0_rmse: 0.67329 | val_1_rmse: 0.67524 |  0:02:23s
epoch 26 | loss: 0.39586 | val_0_rmse: 0.62595 | val_1_rmse: 0.62909 |  0:02:29s
epoch 27 | loss: 0.3953  | val_0_rmse: 0.61875 | val_1_rmse: 0.61959 |  0:02:34s
epoch 28 | loss: 0.39057 | val_0_rmse: 0.64005 | val_1_rmse: 0.64032 |  0:02:40s
epoch 29 | loss: 0.38985 | val_0_rmse: 0.63682 | val_1_rmse: 0.6379  |  0:02:45s
epoch 30 | loss: 0.39136 | val_0_rmse: 0.63442 | val_1_rmse: 0.63642 |  0:02:51s
epoch 31 | loss: 0.38942 | val_0_rmse: 0.62224 | val_1_rmse: 0.62263 |  0:02:56s
epoch 32 | loss: 0.39988 | val_0_rmse: 0.68984 | val_1_rmse: 0.69112 |  0:03:02s
epoch 33 | loss: 0.39304 | val_0_rmse: 0.62138 | val_1_rmse: 0.62348 |  0:03:07s
epoch 34 | loss: 0.39226 | val_0_rmse: 0.61299 | val_1_rmse: 0.61414 |  0:03:13s
epoch 35 | loss: 0.39085 | val_0_rmse: 0.63095 | val_1_rmse: 0.63327 |  0:03:19s
epoch 36 | loss: 0.39017 | val_0_rmse: 0.63657 | val_1_rmse: 0.63867 |  0:03:24s
epoch 37 | loss: 0.39179 | val_0_rmse: 0.68081 | val_1_rmse: 0.67792 |  0:03:30s
epoch 38 | loss: 0.38681 | val_0_rmse: 0.61475 | val_1_rmse: 0.61643 |  0:03:35s
epoch 39 | loss: 0.39005 | val_0_rmse: 0.64565 | val_1_rmse: 0.644   |  0:03:41s
epoch 40 | loss: 0.39015 | val_0_rmse: 0.76101 | val_1_rmse: 0.75755 |  0:03:46s
epoch 41 | loss: 0.413   | val_0_rmse: 0.68367 | val_1_rmse: 0.68283 |  0:03:52s
epoch 42 | loss: 0.40907 | val_0_rmse: 0.67046 | val_1_rmse: 0.67047 |  0:03:57s
epoch 43 | loss: 0.40542 | val_0_rmse: 0.65683 | val_1_rmse: 0.66064 |  0:04:03s
epoch 44 | loss: 0.40315 | val_0_rmse: 0.67988 | val_1_rmse: 0.68125 |  0:04:08s
epoch 45 | loss: 0.39912 | val_0_rmse: 0.66057 | val_1_rmse: 0.66196 |  0:04:14s
epoch 46 | loss: 0.40243 | val_0_rmse: 0.64134 | val_1_rmse: 0.64548 |  0:04:19s
epoch 47 | loss: 0.39676 | val_0_rmse: 0.67907 | val_1_rmse: 0.68292 |  0:04:25s
epoch 48 | loss: 0.40136 | val_0_rmse: 0.70757 | val_1_rmse: 0.70682 |  0:04:30s
epoch 49 | loss: 0.43886 | val_0_rmse: 0.64789 | val_1_rmse: 0.64932 |  0:04:36s
epoch 50 | loss: 0.42043 | val_0_rmse: 0.65027 | val_1_rmse: 0.65475 |  0:04:41s
epoch 51 | loss: 0.4093  | val_0_rmse: 0.62967 | val_1_rmse: 0.63308 |  0:04:47s
epoch 52 | loss: 0.41476 | val_0_rmse: 0.66516 | val_1_rmse: 0.66669 |  0:04:52s
epoch 53 | loss: 0.4026  | val_0_rmse: 0.63971 | val_1_rmse: 0.64362 |  0:04:58s
epoch 54 | loss: 0.40257 | val_0_rmse: 0.62941 | val_1_rmse: 0.63339 |  0:05:04s
epoch 55 | loss: 0.40393 | val_0_rmse: 0.64258 | val_1_rmse: 0.64415 |  0:05:09s
epoch 56 | loss: 0.40025 | val_0_rmse: 0.63961 | val_1_rmse: 0.6426  |  0:05:15s
epoch 57 | loss: 0.39568 | val_0_rmse: 0.62894 | val_1_rmse: 0.6319  |  0:05:20s
epoch 58 | loss: 0.39736 | val_0_rmse: 0.64236 | val_1_rmse: 0.64528 |  0:05:26s
epoch 59 | loss: 0.3942  | val_0_rmse: 0.63553 | val_1_rmse: 0.63809 |  0:05:31s
epoch 60 | loss: 0.39447 | val_0_rmse: 0.62372 | val_1_rmse: 0.62586 |  0:05:37s
epoch 61 | loss: 0.39532 | val_0_rmse: 0.64136 | val_1_rmse: 0.64477 |  0:05:42s
epoch 62 | loss: 0.39422 | val_0_rmse: 0.6266  | val_1_rmse: 0.62962 |  0:05:48s
epoch 63 | loss: 0.4044  | val_0_rmse: 0.69364 | val_1_rmse: 0.69616 |  0:05:53s
epoch 64 | loss: 0.42681 | val_0_rmse: 0.63745 | val_1_rmse: 0.63654 |  0:05:59s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.61414
Best weights from best epoch are automatically used!
ended training at: 06:16:16
Feature importance:
[('Area', 0.4391096592023766), ('Baths', 0.18663305991712628), ('Beds', 0.01568198471722556), ('Latitude', 0.25451286274573803), ('Longitude', 0.10406243341753356), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2516942111.8445845
Mean absolute error:35254.27678496596
MAPE:0.3112632470163459
R2 score:0.6317463507725114
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:16:17
epoch 0  | loss: 0.6083  | val_0_rmse: 0.71276 | val_1_rmse: 0.71561 |  0:00:05s
epoch 1  | loss: 0.47599 | val_0_rmse: 0.66778 | val_1_rmse: 0.67266 |  0:00:11s
epoch 2  | loss: 0.43001 | val_0_rmse: 0.65625 | val_1_rmse: 0.65895 |  0:00:16s
epoch 3  | loss: 0.4157  | val_0_rmse: 0.63482 | val_1_rmse: 0.63978 |  0:00:22s
epoch 4  | loss: 0.41016 | val_0_rmse: 0.62635 | val_1_rmse: 0.63039 |  0:00:27s
epoch 5  | loss: 0.40679 | val_0_rmse: 0.6381  | val_1_rmse: 0.64243 |  0:00:33s
epoch 6  | loss: 0.4035  | val_0_rmse: 0.6342  | val_1_rmse: 0.6394  |  0:00:38s
epoch 7  | loss: 0.3977  | val_0_rmse: 0.64602 | val_1_rmse: 0.65223 |  0:00:44s
epoch 8  | loss: 0.3988  | val_0_rmse: 0.67004 | val_1_rmse: 0.67769 |  0:00:49s
epoch 9  | loss: 0.39322 | val_0_rmse: 0.63841 | val_1_rmse: 0.64659 |  0:00:55s
epoch 10 | loss: 0.39638 | val_0_rmse: 0.66409 | val_1_rmse: 0.67039 |  0:01:00s
epoch 11 | loss: 0.39077 | val_0_rmse: 0.6156  | val_1_rmse: 0.62035 |  0:01:06s
epoch 12 | loss: 0.38505 | val_0_rmse: 0.61594 | val_1_rmse: 0.62413 |  0:01:12s
epoch 13 | loss: 0.37909 | val_0_rmse: 0.62256 | val_1_rmse: 0.62982 |  0:01:17s
epoch 14 | loss: 0.38027 | val_0_rmse: 0.612   | val_1_rmse: 0.62067 |  0:01:23s
epoch 15 | loss: 0.37482 | val_0_rmse: 0.6585  | val_1_rmse: 0.66642 |  0:01:28s
epoch 16 | loss: 0.37891 | val_0_rmse: 0.68255 | val_1_rmse: 0.68788 |  0:01:34s
epoch 17 | loss: 0.37504 | val_0_rmse: 0.65001 | val_1_rmse: 0.65931 |  0:01:39s
epoch 18 | loss: 0.38204 | val_0_rmse: 0.62482 | val_1_rmse: 0.63283 |  0:01:45s
epoch 19 | loss: 0.37267 | val_0_rmse: 0.79748 | val_1_rmse: 0.80008 |  0:01:50s
epoch 20 | loss: 0.37369 | val_0_rmse: 0.71636 | val_1_rmse: 0.72205 |  0:01:56s
epoch 21 | loss: 0.37104 | val_0_rmse: 0.60282 | val_1_rmse: 0.60904 |  0:02:01s
epoch 22 | loss: 0.37157 | val_0_rmse: 0.60458 | val_1_rmse: 0.61138 |  0:02:07s
epoch 23 | loss: 0.37064 | val_0_rmse: 0.7507  | val_1_rmse: 0.7543  |  0:02:12s
epoch 24 | loss: 0.37196 | val_0_rmse: 0.60699 | val_1_rmse: 0.61248 |  0:02:18s
epoch 25 | loss: 0.36915 | val_0_rmse: 0.73281 | val_1_rmse: 0.73729 |  0:02:24s
epoch 26 | loss: 0.36844 | val_0_rmse: 0.67198 | val_1_rmse: 0.67928 |  0:02:29s
epoch 27 | loss: 0.36802 | val_0_rmse: 0.69489 | val_1_rmse: 0.70222 |  0:02:35s
epoch 28 | loss: 0.36746 | val_0_rmse: 0.61299 | val_1_rmse: 0.62198 |  0:02:40s
epoch 29 | loss: 0.36767 | val_0_rmse: 0.63202 | val_1_rmse: 0.63676 |  0:02:46s
epoch 30 | loss: 0.36498 | val_0_rmse: 0.59685 | val_1_rmse: 0.60522 |  0:02:51s
epoch 31 | loss: 0.36633 | val_0_rmse: 0.62589 | val_1_rmse: 0.63374 |  0:02:57s
epoch 32 | loss: 0.36714 | val_0_rmse: 0.70148 | val_1_rmse: 0.70712 |  0:03:02s
epoch 33 | loss: 0.367   | val_0_rmse: 0.74786 | val_1_rmse: 0.75021 |  0:03:08s
epoch 34 | loss: 0.36611 | val_0_rmse: 0.60576 | val_1_rmse: 0.61318 |  0:03:13s
epoch 35 | loss: 0.36803 | val_0_rmse: 0.82579 | val_1_rmse: 0.82852 |  0:03:19s
epoch 36 | loss: 0.36363 | val_0_rmse: 0.66659 | val_1_rmse: 0.67243 |  0:03:24s
epoch 37 | loss: 0.3709  | val_0_rmse: 0.82323 | val_1_rmse: 0.82742 |  0:03:30s
epoch 38 | loss: 0.36643 | val_0_rmse: 0.64933 | val_1_rmse: 0.65823 |  0:03:35s
epoch 39 | loss: 0.36653 | val_0_rmse: 0.64268 | val_1_rmse: 0.65167 |  0:03:41s
epoch 40 | loss: 0.3661  | val_0_rmse: 0.64086 | val_1_rmse: 0.65013 |  0:03:46s
epoch 41 | loss: 0.36664 | val_0_rmse: 0.66166 | val_1_rmse: 0.67005 |  0:03:52s
epoch 42 | loss: 0.36628 | val_0_rmse: 0.61184 | val_1_rmse: 0.61913 |  0:03:57s
epoch 43 | loss: 0.36387 | val_0_rmse: 0.63847 | val_1_rmse: 0.64578 |  0:04:03s
epoch 44 | loss: 0.36599 | val_0_rmse: 0.72387 | val_1_rmse: 0.73247 |  0:04:08s
epoch 45 | loss: 0.36443 | val_0_rmse: 0.6905  | val_1_rmse: 0.69916 |  0:04:14s
epoch 46 | loss: 0.36262 | val_0_rmse: 0.6535  | val_1_rmse: 0.66269 |  0:04:20s
epoch 47 | loss: 0.36233 | val_0_rmse: 0.63166 | val_1_rmse: 0.6419  |  0:04:25s
epoch 48 | loss: 0.36041 | val_0_rmse: 0.76432 | val_1_rmse: 0.76972 |  0:04:31s
epoch 49 | loss: 0.36286 | val_0_rmse: 0.61548 | val_1_rmse: 0.62484 |  0:04:36s
epoch 50 | loss: 0.36139 | val_0_rmse: 0.68071 | val_1_rmse: 0.68675 |  0:04:42s
epoch 51 | loss: 0.36741 | val_0_rmse: 0.8425  | val_1_rmse: 0.84683 |  0:04:48s
epoch 52 | loss: 0.36583 | val_0_rmse: 0.72512 | val_1_rmse: 0.72989 |  0:04:53s
epoch 53 | loss: 0.3605  | val_0_rmse: 0.80518 | val_1_rmse: 0.81227 |  0:04:59s
epoch 54 | loss: 0.36178 | val_0_rmse: 0.86864 | val_1_rmse: 0.8705  |  0:05:04s
epoch 55 | loss: 0.36067 | val_0_rmse: 0.76795 | val_1_rmse: 0.77528 |  0:05:10s
epoch 56 | loss: 0.36883 | val_0_rmse: 0.61982 | val_1_rmse: 0.62948 |  0:05:15s
epoch 57 | loss: 0.36254 | val_0_rmse: 0.81807 | val_1_rmse: 0.81999 |  0:05:21s
epoch 58 | loss: 0.35894 | val_0_rmse: 0.65164 | val_1_rmse: 0.6612  |  0:05:26s
epoch 59 | loss: 0.36107 | val_0_rmse: 0.74248 | val_1_rmse: 0.7443  |  0:05:32s
epoch 60 | loss: 0.36081 | val_0_rmse: 0.62353 | val_1_rmse: 0.63285 |  0:05:37s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.60522
Best weights from best epoch are automatically used!
ended training at: 06:21:56
Feature importance:
[('Area', 0.36386222339284763), ('Baths', 0.2800031955886733), ('Beds', 0.03328708817895719), ('Latitude', 0.13129647617257076), ('Longitude', 0.19017609716045664), ('Month', 0.0), ('Year', 0.0013749195064944918)]
Mean squared error is of 2268828256.463958
Mean absolute error:33459.93147675311
MAPE:0.29920659594997434
R2 score:0.6630485422726116
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:06
epoch 0  | loss: 0.52208 | val_0_rmse: 0.58785 | val_1_rmse: 0.5784  |  0:00:02s
epoch 1  | loss: 0.31462 | val_0_rmse: 0.52879 | val_1_rmse: 0.52974 |  0:00:04s
epoch 2  | loss: 0.28212 | val_0_rmse: 0.5188  | val_1_rmse: 0.52143 |  0:00:06s
epoch 3  | loss: 0.27047 | val_0_rmse: 0.51357 | val_1_rmse: 0.51684 |  0:00:08s
epoch 4  | loss: 0.26682 | val_0_rmse: 0.50004 | val_1_rmse: 0.49978 |  0:00:10s
epoch 5  | loss: 0.25655 | val_0_rmse: 0.4932  | val_1_rmse: 0.49419 |  0:00:12s
epoch 6  | loss: 0.25832 | val_0_rmse: 0.51297 | val_1_rmse: 0.51556 |  0:00:14s
epoch 7  | loss: 0.24805 | val_0_rmse: 0.48778 | val_1_rmse: 0.49174 |  0:00:16s
epoch 8  | loss: 0.23825 | val_0_rmse: 0.48029 | val_1_rmse: 0.48245 |  0:00:18s
epoch 9  | loss: 0.23909 | val_0_rmse: 0.48347 | val_1_rmse: 0.48886 |  0:00:20s
epoch 10 | loss: 0.23973 | val_0_rmse: 0.4896  | val_1_rmse: 0.49358 |  0:00:22s
epoch 11 | loss: 0.23374 | val_0_rmse: 0.47082 | val_1_rmse: 0.47159 |  0:00:24s
epoch 12 | loss: 0.24555 | val_0_rmse: 0.49484 | val_1_rmse: 0.49909 |  0:00:26s
epoch 13 | loss: 0.23929 | val_0_rmse: 0.49182 | val_1_rmse: 0.49309 |  0:00:28s
epoch 14 | loss: 0.24138 | val_0_rmse: 0.47835 | val_1_rmse: 0.48165 |  0:00:30s
epoch 15 | loss: 0.23564 | val_0_rmse: 0.4788  | val_1_rmse: 0.48053 |  0:00:32s
epoch 16 | loss: 0.23281 | val_0_rmse: 0.47966 | val_1_rmse: 0.48362 |  0:00:34s
epoch 17 | loss: 0.2297  | val_0_rmse: 0.47994 | val_1_rmse: 0.4837  |  0:00:36s
epoch 18 | loss: 0.22986 | val_0_rmse: 0.49694 | val_1_rmse: 0.50199 |  0:00:38s
epoch 19 | loss: 0.23207 | val_0_rmse: 0.47041 | val_1_rmse: 0.47635 |  0:00:40s
epoch 20 | loss: 0.22969 | val_0_rmse: 0.46599 | val_1_rmse: 0.46875 |  0:00:42s
epoch 21 | loss: 0.22522 | val_0_rmse: 0.46939 | val_1_rmse: 0.46889 |  0:00:44s
epoch 22 | loss: 0.22911 | val_0_rmse: 0.4686  | val_1_rmse: 0.46966 |  0:00:46s
epoch 23 | loss: 0.22485 | val_0_rmse: 0.49553 | val_1_rmse: 0.49945 |  0:00:48s
epoch 24 | loss: 0.23269 | val_0_rmse: 0.49979 | val_1_rmse: 0.5036  |  0:00:50s
epoch 25 | loss: 0.23028 | val_0_rmse: 0.49065 | val_1_rmse: 0.4894  |  0:00:52s
epoch 26 | loss: 0.23036 | val_0_rmse: 0.4854  | val_1_rmse: 0.48896 |  0:00:54s
epoch 27 | loss: 0.22539 | val_0_rmse: 0.47413 | val_1_rmse: 0.47856 |  0:00:56s
epoch 28 | loss: 0.23812 | val_0_rmse: 0.47913 | val_1_rmse: 0.48025 |  0:00:58s
epoch 29 | loss: 0.23467 | val_0_rmse: 0.48635 | val_1_rmse: 0.4845  |  0:01:00s
epoch 30 | loss: 0.23277 | val_0_rmse: 0.47454 | val_1_rmse: 0.47694 |  0:01:02s
epoch 31 | loss: 0.22542 | val_0_rmse: 0.48268 | val_1_rmse: 0.48451 |  0:01:04s
epoch 32 | loss: 0.22245 | val_0_rmse: 0.48966 | val_1_rmse: 0.49684 |  0:01:06s
epoch 33 | loss: 0.22243 | val_0_rmse: 0.4915  | val_1_rmse: 0.49692 |  0:01:08s
epoch 34 | loss: 0.22573 | val_0_rmse: 0.49773 | val_1_rmse: 0.5016  |  0:01:10s
epoch 35 | loss: 0.22455 | val_0_rmse: 0.47495 | val_1_rmse: 0.4774  |  0:01:12s
epoch 36 | loss: 0.22106 | val_0_rmse: 0.47134 | val_1_rmse: 0.4729  |  0:01:14s
epoch 37 | loss: 0.21931 | val_0_rmse: 0.48595 | val_1_rmse: 0.49134 |  0:01:16s
epoch 38 | loss: 0.21905 | val_0_rmse: 0.47196 | val_1_rmse: 0.47424 |  0:01:18s
epoch 39 | loss: 0.21616 | val_0_rmse: 0.47248 | val_1_rmse: 0.47564 |  0:01:20s
epoch 40 | loss: 0.21812 | val_0_rmse: 0.45808 | val_1_rmse: 0.46077 |  0:01:22s
epoch 41 | loss: 0.21621 | val_0_rmse: 0.47945 | val_1_rmse: 0.48083 |  0:01:24s
epoch 42 | loss: 0.21906 | val_0_rmse: 0.49438 | val_1_rmse: 0.49976 |  0:01:26s
epoch 43 | loss: 0.2169  | val_0_rmse: 0.46649 | val_1_rmse: 0.46773 |  0:01:28s
epoch 44 | loss: 0.22039 | val_0_rmse: 0.46553 | val_1_rmse: 0.46886 |  0:01:30s
epoch 45 | loss: 0.21867 | val_0_rmse: 0.51152 | val_1_rmse: 0.51614 |  0:01:32s
epoch 46 | loss: 0.21938 | val_0_rmse: 0.47312 | val_1_rmse: 0.47754 |  0:01:34s
epoch 47 | loss: 0.22058 | val_0_rmse: 0.51719 | val_1_rmse: 0.52172 |  0:01:36s
epoch 48 | loss: 0.21547 | val_0_rmse: 0.45781 | val_1_rmse: 0.4616  |  0:01:38s
epoch 49 | loss: 0.21652 | val_0_rmse: 0.4818  | val_1_rmse: 0.48804 |  0:01:40s
epoch 50 | loss: 0.21573 | val_0_rmse: 0.47887 | val_1_rmse: 0.48071 |  0:01:42s
epoch 51 | loss: 0.21442 | val_0_rmse: 0.50596 | val_1_rmse: 0.50958 |  0:01:44s
epoch 52 | loss: 0.2153  | val_0_rmse: 0.47389 | val_1_rmse: 0.47717 |  0:01:46s
epoch 53 | loss: 0.21586 | val_0_rmse: 0.54013 | val_1_rmse: 0.54648 |  0:01:48s
epoch 54 | loss: 0.21188 | val_0_rmse: 0.45857 | val_1_rmse: 0.46385 |  0:01:50s
epoch 55 | loss: 0.2175  | val_0_rmse: 0.46768 | val_1_rmse: 0.47472 |  0:01:52s
epoch 56 | loss: 0.21381 | val_0_rmse: 0.48541 | val_1_rmse: 0.49244 |  0:01:54s
epoch 57 | loss: 0.21587 | val_0_rmse: 0.45801 | val_1_rmse: 0.46608 |  0:01:56s
epoch 58 | loss: 0.21272 | val_0_rmse: 0.45367 | val_1_rmse: 0.4583  |  0:01:58s
epoch 59 | loss: 0.21234 | val_0_rmse: 0.45218 | val_1_rmse: 0.4593  |  0:02:00s
epoch 60 | loss: 0.21223 | val_0_rmse: 0.45452 | val_1_rmse: 0.46019 |  0:02:02s
epoch 61 | loss: 0.2125  | val_0_rmse: 0.4713  | val_1_rmse: 0.4774  |  0:02:04s
epoch 62 | loss: 0.21156 | val_0_rmse: 0.48099 | val_1_rmse: 0.48695 |  0:02:06s
epoch 63 | loss: 0.21187 | val_0_rmse: 0.46185 | val_1_rmse: 0.46959 |  0:02:08s
epoch 64 | loss: 0.2136  | val_0_rmse: 0.45251 | val_1_rmse: 0.45639 |  0:02:10s
epoch 65 | loss: 0.21183 | val_0_rmse: 0.50325 | val_1_rmse: 0.51079 |  0:02:12s
epoch 66 | loss: 0.20976 | val_0_rmse: 0.45596 | val_1_rmse: 0.46278 |  0:02:14s
epoch 67 | loss: 0.21702 | val_0_rmse: 0.5584  | val_1_rmse: 0.56314 |  0:02:16s
epoch 68 | loss: 0.21337 | val_0_rmse: 0.49152 | val_1_rmse: 0.49888 |  0:02:18s
epoch 69 | loss: 0.2124  | val_0_rmse: 0.50462 | val_1_rmse: 0.51143 |  0:02:20s
epoch 70 | loss: 0.21253 | val_0_rmse: 0.46964 | val_1_rmse: 0.47679 |  0:02:22s
epoch 71 | loss: 0.21403 | val_0_rmse: 0.4671  | val_1_rmse: 0.47465 |  0:02:24s
epoch 72 | loss: 0.21978 | val_0_rmse: 0.49073 | val_1_rmse: 0.49519 |  0:02:26s
epoch 73 | loss: 0.22515 | val_0_rmse: 0.4759  | val_1_rmse: 0.48259 |  0:02:28s
epoch 74 | loss: 0.21899 | val_0_rmse: 0.51401 | val_1_rmse: 0.51823 |  0:02:30s
epoch 75 | loss: 0.21957 | val_0_rmse: 0.46118 | val_1_rmse: 0.46414 |  0:02:32s
epoch 76 | loss: 0.21632 | val_0_rmse: 0.48402 | val_1_rmse: 0.48775 |  0:02:34s
epoch 77 | loss: 0.21408 | val_0_rmse: 0.49889 | val_1_rmse: 0.50328 |  0:02:36s
epoch 78 | loss: 0.21258 | val_0_rmse: 0.5384  | val_1_rmse: 0.5461  |  0:02:38s
epoch 79 | loss: 0.21199 | val_0_rmse: 0.53825 | val_1_rmse: 0.54705 |  0:02:40s
epoch 80 | loss: 0.21842 | val_0_rmse: 0.49396 | val_1_rmse: 0.4963  |  0:02:42s
epoch 81 | loss: 0.21689 | val_0_rmse: 0.50752 | val_1_rmse: 0.51343 |  0:02:44s
epoch 82 | loss: 0.21165 | val_0_rmse: 0.45664 | val_1_rmse: 0.46235 |  0:02:46s
epoch 83 | loss: 0.21003 | val_0_rmse: 0.4608  | val_1_rmse: 0.4632  |  0:02:48s
epoch 84 | loss: 0.21338 | val_0_rmse: 0.49157 | val_1_rmse: 0.49809 |  0:02:50s
epoch 85 | loss: 0.21365 | val_0_rmse: 0.51801 | val_1_rmse: 0.52493 |  0:02:52s
epoch 86 | loss: 0.21137 | val_0_rmse: 0.4687  | val_1_rmse: 0.47485 |  0:02:54s
epoch 87 | loss: 0.21139 | val_0_rmse: 0.53366 | val_1_rmse: 0.54037 |  0:02:56s
epoch 88 | loss: 0.20946 | val_0_rmse: 0.53347 | val_1_rmse: 0.54154 |  0:02:58s
epoch 89 | loss: 0.20859 | val_0_rmse: 0.46967 | val_1_rmse: 0.477   |  0:03:00s
epoch 90 | loss: 0.21094 | val_0_rmse: 0.49663 | val_1_rmse: 0.5066  |  0:03:02s
epoch 91 | loss: 0.20906 | val_0_rmse: 0.45811 | val_1_rmse: 0.46596 |  0:03:04s
epoch 92 | loss: 0.21126 | val_0_rmse: 0.54869 | val_1_rmse: 0.5568  |  0:03:06s
epoch 93 | loss: 0.21081 | val_0_rmse: 0.51475 | val_1_rmse: 0.52034 |  0:03:08s
epoch 94 | loss: 0.21173 | val_0_rmse: 0.46348 | val_1_rmse: 0.47093 |  0:03:10s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.45639
Best weights from best epoch are automatically used!
ended training at: 06:25:17
Feature importance:
[('Area', 0.2585962111572111), ('Baths', 0.11343905717496432), ('Beds', 0.08434610455723358), ('Latitude', 0.2505127678182148), ('Longitude', 0.269042647944856), ('Month', 0.0), ('Year', 0.024063211347520208)]
Mean squared error is of 1029588997.4897985
Mean absolute error:21265.00753833983
MAPE:0.2429648466301741
R2 score:0.7387467006944337
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:25:18
epoch 0  | loss: 0.53992 | val_0_rmse: 0.64191 | val_1_rmse: 0.6482  |  0:00:02s
epoch 1  | loss: 0.32803 | val_0_rmse: 0.54562 | val_1_rmse: 0.5506  |  0:00:04s
epoch 2  | loss: 0.29555 | val_0_rmse: 0.52186 | val_1_rmse: 0.52625 |  0:00:06s
epoch 3  | loss: 0.27762 | val_0_rmse: 0.51303 | val_1_rmse: 0.51796 |  0:00:08s
epoch 4  | loss: 0.27001 | val_0_rmse: 0.51063 | val_1_rmse: 0.51606 |  0:00:10s
epoch 5  | loss: 0.26898 | val_0_rmse: 0.50797 | val_1_rmse: 0.5084  |  0:00:12s
epoch 6  | loss: 0.2598  | val_0_rmse: 0.50302 | val_1_rmse: 0.5081  |  0:00:14s
epoch 7  | loss: 0.25661 | val_0_rmse: 0.49067 | val_1_rmse: 0.49252 |  0:00:16s
epoch 8  | loss: 0.25356 | val_0_rmse: 0.5037  | val_1_rmse: 0.50793 |  0:00:18s
epoch 9  | loss: 0.25578 | val_0_rmse: 0.49657 | val_1_rmse: 0.50059 |  0:00:20s
epoch 10 | loss: 0.25775 | val_0_rmse: 0.50735 | val_1_rmse: 0.51119 |  0:00:22s
epoch 11 | loss: 0.25307 | val_0_rmse: 0.4879  | val_1_rmse: 0.4916  |  0:00:24s
epoch 12 | loss: 0.25008 | val_0_rmse: 0.48967 | val_1_rmse: 0.49059 |  0:00:26s
epoch 13 | loss: 0.25069 | val_0_rmse: 0.49505 | val_1_rmse: 0.49941 |  0:00:28s
epoch 14 | loss: 0.25047 | val_0_rmse: 0.49286 | val_1_rmse: 0.49789 |  0:00:30s
epoch 15 | loss: 0.25077 | val_0_rmse: 0.48631 | val_1_rmse: 0.48904 |  0:00:32s
epoch 16 | loss: 0.24556 | val_0_rmse: 0.48407 | val_1_rmse: 0.48545 |  0:00:34s
epoch 17 | loss: 0.24671 | val_0_rmse: 0.48788 | val_1_rmse: 0.48796 |  0:00:36s
epoch 18 | loss: 0.24832 | val_0_rmse: 0.4893  | val_1_rmse: 0.49309 |  0:00:38s
epoch 19 | loss: 0.24683 | val_0_rmse: 0.49467 | val_1_rmse: 0.49595 |  0:00:40s
epoch 20 | loss: 0.2475  | val_0_rmse: 0.48826 | val_1_rmse: 0.48952 |  0:00:42s
epoch 21 | loss: 0.25061 | val_0_rmse: 0.49439 | val_1_rmse: 0.49493 |  0:00:44s
epoch 22 | loss: 0.24594 | val_0_rmse: 0.50779 | val_1_rmse: 0.5049  |  0:00:46s
epoch 23 | loss: 0.24986 | val_0_rmse: 0.48744 | val_1_rmse: 0.48985 |  0:00:48s
epoch 24 | loss: 0.24478 | val_0_rmse: 0.48997 | val_1_rmse: 0.49313 |  0:00:50s
epoch 25 | loss: 0.24913 | val_0_rmse: 0.4867  | val_1_rmse: 0.48658 |  0:00:52s
epoch 26 | loss: 0.24525 | val_0_rmse: 0.48846 | val_1_rmse: 0.49212 |  0:00:54s
epoch 27 | loss: 0.24329 | val_0_rmse: 0.48537 | val_1_rmse: 0.48551 |  0:00:56s
epoch 28 | loss: 0.24643 | val_0_rmse: 0.48537 | val_1_rmse: 0.48698 |  0:00:58s
epoch 29 | loss: 0.24294 | val_0_rmse: 0.48515 | val_1_rmse: 0.4907  |  0:01:00s
epoch 30 | loss: 0.24212 | val_0_rmse: 0.5126  | val_1_rmse: 0.51205 |  0:01:02s
epoch 31 | loss: 0.24167 | val_0_rmse: 0.48337 | val_1_rmse: 0.48851 |  0:01:04s
epoch 32 | loss: 0.24239 | val_0_rmse: 0.4853  | val_1_rmse: 0.48705 |  0:01:06s
epoch 33 | loss: 0.24271 | val_0_rmse: 0.4784  | val_1_rmse: 0.48026 |  0:01:08s
epoch 34 | loss: 0.24444 | val_0_rmse: 0.48782 | val_1_rmse: 0.48846 |  0:01:10s
epoch 35 | loss: 0.24451 | val_0_rmse: 0.4918  | val_1_rmse: 0.49301 |  0:01:12s
epoch 36 | loss: 0.2425  | val_0_rmse: 0.49146 | val_1_rmse: 0.49209 |  0:01:14s
epoch 37 | loss: 0.24978 | val_0_rmse: 0.51228 | val_1_rmse: 0.50898 |  0:01:16s
epoch 38 | loss: 0.24425 | val_0_rmse: 0.48649 | val_1_rmse: 0.48691 |  0:01:18s
epoch 39 | loss: 0.24383 | val_0_rmse: 0.49081 | val_1_rmse: 0.4889  |  0:01:20s
epoch 40 | loss: 0.25073 | val_0_rmse: 0.49825 | val_1_rmse: 0.50271 |  0:01:22s
epoch 41 | loss: 0.24284 | val_0_rmse: 0.48873 | val_1_rmse: 0.48929 |  0:01:24s
epoch 42 | loss: 0.23922 | val_0_rmse: 0.52251 | val_1_rmse: 0.52693 |  0:01:26s
epoch 43 | loss: 0.23903 | val_0_rmse: 0.48234 | val_1_rmse: 0.48371 |  0:01:28s
epoch 44 | loss: 0.23855 | val_0_rmse: 0.47704 | val_1_rmse: 0.47745 |  0:01:30s
epoch 45 | loss: 0.23809 | val_0_rmse: 0.49471 | val_1_rmse: 0.4952  |  0:01:32s
epoch 46 | loss: 0.24295 | val_0_rmse: 0.49683 | val_1_rmse: 0.49501 |  0:01:34s
epoch 47 | loss: 0.24212 | val_0_rmse: 0.4823  | val_1_rmse: 0.48246 |  0:01:36s
epoch 48 | loss: 0.23727 | val_0_rmse: 0.4879  | val_1_rmse: 0.48721 |  0:01:38s
epoch 49 | loss: 0.23653 | val_0_rmse: 0.48282 | val_1_rmse: 0.48177 |  0:01:41s
epoch 50 | loss: 0.23798 | val_0_rmse: 0.49067 | val_1_rmse: 0.4904  |  0:01:43s
epoch 51 | loss: 0.24168 | val_0_rmse: 0.51623 | val_1_rmse: 0.51051 |  0:01:45s
epoch 52 | loss: 0.24017 | val_0_rmse: 0.50528 | val_1_rmse: 0.50772 |  0:01:47s
epoch 53 | loss: 0.23623 | val_0_rmse: 0.48744 | val_1_rmse: 0.48035 |  0:01:49s
epoch 54 | loss: 0.23603 | val_0_rmse: 0.4871  | val_1_rmse: 0.48779 |  0:01:51s
epoch 55 | loss: 0.23415 | val_0_rmse: 0.48581 | val_1_rmse: 0.48566 |  0:01:53s
epoch 56 | loss: 0.23301 | val_0_rmse: 0.4905  | val_1_rmse: 0.48812 |  0:01:55s
epoch 57 | loss: 0.23336 | val_0_rmse: 0.48977 | val_1_rmse: 0.49373 |  0:01:57s
epoch 58 | loss: 0.2398  | val_0_rmse: 0.48738 | val_1_rmse: 0.48921 |  0:01:59s
epoch 59 | loss: 0.24505 | val_0_rmse: 0.48925 | val_1_rmse: 0.48946 |  0:02:01s
epoch 60 | loss: 0.23394 | val_0_rmse: 0.4744  | val_1_rmse: 0.4741  |  0:02:03s
epoch 61 | loss: 0.23203 | val_0_rmse: 0.4748  | val_1_rmse: 0.4783  |  0:02:05s
epoch 62 | loss: 0.22958 | val_0_rmse: 0.49301 | val_1_rmse: 0.49174 |  0:02:07s
epoch 63 | loss: 0.2305  | val_0_rmse: 0.48604 | val_1_rmse: 0.48738 |  0:02:09s
epoch 64 | loss: 0.2331  | val_0_rmse: 0.49846 | val_1_rmse: 0.49873 |  0:02:11s
epoch 65 | loss: 0.23005 | val_0_rmse: 0.47674 | val_1_rmse: 0.47792 |  0:02:13s
epoch 66 | loss: 0.23046 | val_0_rmse: 0.48039 | val_1_rmse: 0.48076 |  0:02:15s
epoch 67 | loss: 0.23138 | val_0_rmse: 0.49253 | val_1_rmse: 0.49577 |  0:02:17s
epoch 68 | loss: 0.23253 | val_0_rmse: 0.47467 | val_1_rmse: 0.47688 |  0:02:19s
epoch 69 | loss: 0.23295 | val_0_rmse: 0.47335 | val_1_rmse: 0.474   |  0:02:21s
epoch 70 | loss: 0.2289  | val_0_rmse: 0.51163 | val_1_rmse: 0.51656 |  0:02:23s
epoch 71 | loss: 0.23151 | val_0_rmse: 0.49808 | val_1_rmse: 0.49645 |  0:02:25s
epoch 72 | loss: 0.2282  | val_0_rmse: 0.50588 | val_1_rmse: 0.50994 |  0:02:27s
epoch 73 | loss: 0.22951 | val_0_rmse: 0.51065 | val_1_rmse: 0.51416 |  0:02:29s
epoch 74 | loss: 0.2319  | val_0_rmse: 0.502   | val_1_rmse: 0.50626 |  0:02:31s
epoch 75 | loss: 0.23497 | val_0_rmse: 0.47634 | val_1_rmse: 0.48188 |  0:02:33s
epoch 76 | loss: 0.23053 | val_0_rmse: 0.47795 | val_1_rmse: 0.47927 |  0:02:35s
epoch 77 | loss: 0.2306  | val_0_rmse: 0.47422 | val_1_rmse: 0.47492 |  0:02:37s
epoch 78 | loss: 0.2285  | val_0_rmse: 0.47058 | val_1_rmse: 0.47269 |  0:02:39s
epoch 79 | loss: 0.23565 | val_0_rmse: 0.48953 | val_1_rmse: 0.49135 |  0:02:41s
epoch 80 | loss: 0.22926 | val_0_rmse: 0.47697 | val_1_rmse: 0.47771 |  0:02:43s
epoch 81 | loss: 0.2301  | val_0_rmse: 0.53163 | val_1_rmse: 0.53806 |  0:02:45s
epoch 82 | loss: 0.2321  | val_0_rmse: 0.47021 | val_1_rmse: 0.47253 |  0:02:47s
epoch 83 | loss: 0.22852 | val_0_rmse: 0.48088 | val_1_rmse: 0.4857  |  0:02:49s
epoch 84 | loss: 0.22497 | val_0_rmse: 0.49977 | val_1_rmse: 0.50511 |  0:02:51s
epoch 85 | loss: 0.22827 | val_0_rmse: 0.47999 | val_1_rmse: 0.48311 |  0:02:53s
epoch 86 | loss: 0.22724 | val_0_rmse: 0.50221 | val_1_rmse: 0.50739 |  0:02:55s
epoch 87 | loss: 0.22969 | val_0_rmse: 0.4792  | val_1_rmse: 0.48252 |  0:02:57s
epoch 88 | loss: 0.22914 | val_0_rmse: 0.46943 | val_1_rmse: 0.47258 |  0:02:59s
epoch 89 | loss: 0.23315 | val_0_rmse: 0.48971 | val_1_rmse: 0.49206 |  0:03:01s
epoch 90 | loss: 0.23549 | val_0_rmse: 0.49402 | val_1_rmse: 0.49589 |  0:03:03s
epoch 91 | loss: 0.23186 | val_0_rmse: 0.50758 | val_1_rmse: 0.50659 |  0:03:05s
epoch 92 | loss: 0.24635 | val_0_rmse: 0.5012  | val_1_rmse: 0.50372 |  0:03:07s
epoch 93 | loss: 0.23625 | val_0_rmse: 0.5332  | val_1_rmse: 0.53806 |  0:03:09s
epoch 94 | loss: 0.23462 | val_0_rmse: 0.47388 | val_1_rmse: 0.47407 |  0:03:11s
epoch 95 | loss: 0.23103 | val_0_rmse: 0.46917 | val_1_rmse: 0.47052 |  0:03:13s
epoch 96 | loss: 0.23032 | val_0_rmse: 0.50544 | val_1_rmse: 0.51048 |  0:03:15s
epoch 97 | loss: 0.23362 | val_0_rmse: 0.48384 | val_1_rmse: 0.48592 |  0:03:17s
epoch 98 | loss: 0.22834 | val_0_rmse: 0.47149 | val_1_rmse: 0.4735  |  0:03:19s
epoch 99 | loss: 0.23575 | val_0_rmse: 0.51025 | val_1_rmse: 0.51208 |  0:03:21s
epoch 100| loss: 0.2472  | val_0_rmse: 0.49131 | val_1_rmse: 0.49112 |  0:03:23s
epoch 101| loss: 0.24039 | val_0_rmse: 0.51178 | val_1_rmse: 0.51439 |  0:03:25s
epoch 102| loss: 0.24452 | val_0_rmse: 0.48669 | val_1_rmse: 0.48893 |  0:03:27s
epoch 103| loss: 0.24339 | val_0_rmse: 0.49088 | val_1_rmse: 0.49589 |  0:03:29s
epoch 104| loss: 0.24603 | val_0_rmse: 0.49436 | val_1_rmse: 0.49876 |  0:03:31s
epoch 105| loss: 0.24985 | val_0_rmse: 0.52268 | val_1_rmse: 0.52083 |  0:03:33s
epoch 106| loss: 0.24138 | val_0_rmse: 0.49171 | val_1_rmse: 0.49183 |  0:03:35s
epoch 107| loss: 0.23886 | val_0_rmse: 0.47839 | val_1_rmse: 0.47798 |  0:03:37s
epoch 108| loss: 0.23558 | val_0_rmse: 0.47687 | val_1_rmse: 0.47566 |  0:03:39s
epoch 109| loss: 0.23484 | val_0_rmse: 0.47877 | val_1_rmse: 0.48053 |  0:03:41s
epoch 110| loss: 0.2356  | val_0_rmse: 0.4858  | val_1_rmse: 0.48397 |  0:03:43s
epoch 111| loss: 0.23509 | val_0_rmse: 0.47963 | val_1_rmse: 0.47957 |  0:03:45s
epoch 112| loss: 0.23586 | val_0_rmse: 0.48258 | val_1_rmse: 0.48424 |  0:03:47s
epoch 113| loss: 0.2327  | val_0_rmse: 0.48216 | val_1_rmse: 0.48363 |  0:03:49s
epoch 114| loss: 0.23694 | val_0_rmse: 0.49033 | val_1_rmse: 0.49491 |  0:03:51s
epoch 115| loss: 0.24316 | val_0_rmse: 0.47928 | val_1_rmse: 0.47945 |  0:03:53s
epoch 116| loss: 0.23334 | val_0_rmse: 0.48123 | val_1_rmse: 0.479   |  0:03:55s
epoch 117| loss: 0.23069 | val_0_rmse: 0.47472 | val_1_rmse: 0.47559 |  0:03:57s
epoch 118| loss: 0.23025 | val_0_rmse: 0.47683 | val_1_rmse: 0.47582 |  0:03:59s
epoch 119| loss: 0.22804 | val_0_rmse: 0.47346 | val_1_rmse: 0.47314 |  0:04:01s
epoch 120| loss: 0.22629 | val_0_rmse: 0.50112 | val_1_rmse: 0.50475 |  0:04:03s
epoch 121| loss: 0.23588 | val_0_rmse: 0.48876 | val_1_rmse: 0.48797 |  0:04:06s
epoch 122| loss: 0.23322 | val_0_rmse: 0.50022 | val_1_rmse: 0.50347 |  0:04:08s
epoch 123| loss: 0.23002 | val_0_rmse: 0.47961 | val_1_rmse: 0.48206 |  0:04:10s
epoch 124| loss: 0.23209 | val_0_rmse: 0.49101 | val_1_rmse: 0.48885 |  0:04:12s
epoch 125| loss: 0.22992 | val_0_rmse: 0.48887 | val_1_rmse: 0.48907 |  0:04:14s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.47052
Best weights from best epoch are automatically used!
ended training at: 06:29:33
Feature importance:
[('Area', 0.4448258925928453), ('Baths', 0.11121424686536734), ('Beds', 0.0), ('Latitude', 0.16909298562754485), ('Longitude', 0.27486687491424255), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 985314412.2130561
Mean absolute error:20977.831460158082
MAPE:0.2401744950577181
R2 score:0.7517228835374508
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:29:33
epoch 0  | loss: 0.54457 | val_0_rmse: 0.6038  | val_1_rmse: 0.60447 |  0:00:02s
epoch 1  | loss: 0.32234 | val_0_rmse: 0.54318 | val_1_rmse: 0.53967 |  0:00:04s
epoch 2  | loss: 0.29416 | val_0_rmse: 0.51808 | val_1_rmse: 0.52138 |  0:00:06s
epoch 3  | loss: 0.27324 | val_0_rmse: 0.50299 | val_1_rmse: 0.50808 |  0:00:08s
epoch 4  | loss: 0.26635 | val_0_rmse: 0.50484 | val_1_rmse: 0.50757 |  0:00:10s
epoch 5  | loss: 0.26327 | val_0_rmse: 0.50391 | val_1_rmse: 0.5066  |  0:00:12s
epoch 6  | loss: 0.25623 | val_0_rmse: 0.50358 | val_1_rmse: 0.50892 |  0:00:14s
epoch 7  | loss: 0.25379 | val_0_rmse: 0.49487 | val_1_rmse: 0.4968  |  0:00:16s
epoch 8  | loss: 0.24805 | val_0_rmse: 0.48633 | val_1_rmse: 0.4903  |  0:00:18s
epoch 9  | loss: 0.24758 | val_0_rmse: 0.49249 | val_1_rmse: 0.49614 |  0:00:20s
epoch 10 | loss: 0.2446  | val_0_rmse: 0.48919 | val_1_rmse: 0.49435 |  0:00:22s
epoch 11 | loss: 0.24303 | val_0_rmse: 0.48375 | val_1_rmse: 0.48901 |  0:00:24s
epoch 12 | loss: 0.24516 | val_0_rmse: 0.48224 | val_1_rmse: 0.48779 |  0:00:26s
epoch 13 | loss: 0.24609 | val_0_rmse: 0.48101 | val_1_rmse: 0.48653 |  0:00:28s
epoch 14 | loss: 0.23707 | val_0_rmse: 0.47577 | val_1_rmse: 0.48126 |  0:00:30s
epoch 15 | loss: 0.23639 | val_0_rmse: 0.47692 | val_1_rmse: 0.48004 |  0:00:32s
epoch 16 | loss: 0.23754 | val_0_rmse: 0.48189 | val_1_rmse: 0.48395 |  0:00:34s
epoch 17 | loss: 0.25521 | val_0_rmse: 0.52283 | val_1_rmse: 0.52094 |  0:00:36s
epoch 18 | loss: 0.25099 | val_0_rmse: 0.50018 | val_1_rmse: 0.49932 |  0:00:38s
epoch 19 | loss: 0.24549 | val_0_rmse: 0.50442 | val_1_rmse: 0.50739 |  0:00:40s
epoch 20 | loss: 0.24983 | val_0_rmse: 0.51522 | val_1_rmse: 0.51864 |  0:00:42s
epoch 21 | loss: 0.24845 | val_0_rmse: 0.52645 | val_1_rmse: 0.52563 |  0:00:44s
epoch 22 | loss: 0.24548 | val_0_rmse: 0.5016  | val_1_rmse: 0.50594 |  0:00:46s
epoch 23 | loss: 0.2363  | val_0_rmse: 0.6194  | val_1_rmse: 0.62013 |  0:00:48s
epoch 24 | loss: 0.23914 | val_0_rmse: 0.47142 | val_1_rmse: 0.47648 |  0:00:50s
epoch 25 | loss: 0.23741 | val_0_rmse: 0.60549 | val_1_rmse: 0.61552 |  0:00:52s
epoch 26 | loss: 0.23394 | val_0_rmse: 0.58675 | val_1_rmse: 0.59959 |  0:00:54s
epoch 27 | loss: 0.22821 | val_0_rmse: 0.47536 | val_1_rmse: 0.48008 |  0:00:56s
epoch 28 | loss: 0.23548 | val_0_rmse: 0.52265 | val_1_rmse: 0.52857 |  0:00:58s
epoch 29 | loss: 0.23966 | val_0_rmse: 0.48969 | val_1_rmse: 0.494   |  0:01:00s
epoch 30 | loss: 0.22781 | val_0_rmse: 0.50036 | val_1_rmse: 0.50906 |  0:01:02s
epoch 31 | loss: 0.22708 | val_0_rmse: 0.45959 | val_1_rmse: 0.4644  |  0:01:04s
epoch 32 | loss: 0.22705 | val_0_rmse: 0.5292  | val_1_rmse: 0.53204 |  0:01:06s
epoch 33 | loss: 0.22461 | val_0_rmse: 0.48282 | val_1_rmse: 0.48688 |  0:01:08s
epoch 34 | loss: 0.23237 | val_0_rmse: 0.53183 | val_1_rmse: 0.54081 |  0:01:10s
epoch 35 | loss: 0.22595 | val_0_rmse: 0.512   | val_1_rmse: 0.52093 |  0:01:12s
epoch 36 | loss: 0.22779 | val_0_rmse: 0.46338 | val_1_rmse: 0.46905 |  0:01:14s
epoch 37 | loss: 0.22035 | val_0_rmse: 0.47366 | val_1_rmse: 0.48238 |  0:01:16s
epoch 38 | loss: 0.22068 | val_0_rmse: 0.53431 | val_1_rmse: 0.54632 |  0:01:18s
epoch 39 | loss: 0.21985 | val_0_rmse: 0.46707 | val_1_rmse: 0.47641 |  0:01:20s
epoch 40 | loss: 0.21831 | val_0_rmse: 0.45509 | val_1_rmse: 0.46228 |  0:01:22s
epoch 41 | loss: 0.2187  | val_0_rmse: 0.52622 | val_1_rmse: 0.54099 |  0:01:24s
epoch 42 | loss: 0.2184  | val_0_rmse: 0.50687 | val_1_rmse: 0.51542 |  0:01:26s
epoch 43 | loss: 0.21561 | val_0_rmse: 0.47764 | val_1_rmse: 0.48666 |  0:01:28s
epoch 44 | loss: 0.21758 | val_0_rmse: 0.46939 | val_1_rmse: 0.47754 |  0:01:30s
epoch 45 | loss: 0.2151  | val_0_rmse: 0.48879 | val_1_rmse: 0.49804 |  0:01:32s
epoch 46 | loss: 0.21468 | val_0_rmse: 0.46957 | val_1_rmse: 0.47356 |  0:01:34s
epoch 47 | loss: 0.21711 | val_0_rmse: 0.53042 | val_1_rmse: 0.53936 |  0:01:36s
epoch 48 | loss: 0.21929 | val_0_rmse: 0.49827 | val_1_rmse: 0.50567 |  0:01:38s
epoch 49 | loss: 0.21828 | val_0_rmse: 0.51763 | val_1_rmse: 0.52791 |  0:01:41s
epoch 50 | loss: 0.21747 | val_0_rmse: 0.46833 | val_1_rmse: 0.47503 |  0:01:42s
epoch 51 | loss: 0.22103 | val_0_rmse: 0.47377 | val_1_rmse: 0.48178 |  0:01:45s
epoch 52 | loss: 0.22297 | val_0_rmse: 0.50983 | val_1_rmse: 0.51906 |  0:01:47s
epoch 53 | loss: 0.22505 | val_0_rmse: 0.49498 | val_1_rmse: 0.4987  |  0:01:49s
epoch 54 | loss: 0.21771 | val_0_rmse: 0.50254 | val_1_rmse: 0.50677 |  0:01:51s
epoch 55 | loss: 0.21526 | val_0_rmse: 0.46802 | val_1_rmse: 0.47704 |  0:01:53s
epoch 56 | loss: 0.21356 | val_0_rmse: 0.45455 | val_1_rmse: 0.4574  |  0:01:55s
epoch 57 | loss: 0.21255 | val_0_rmse: 0.49612 | val_1_rmse: 0.5     |  0:01:57s
epoch 58 | loss: 0.21633 | val_0_rmse: 0.45653 | val_1_rmse: 0.46139 |  0:01:59s
epoch 59 | loss: 0.2119  | val_0_rmse: 0.49306 | val_1_rmse: 0.50022 |  0:02:01s
epoch 60 | loss: 0.21378 | val_0_rmse: 0.51461 | val_1_rmse: 0.52147 |  0:02:03s
epoch 61 | loss: 0.21431 | val_0_rmse: 0.48718 | val_1_rmse: 0.49717 |  0:02:05s
epoch 62 | loss: 0.21194 | val_0_rmse: 0.47424 | val_1_rmse: 0.48714 |  0:02:07s
epoch 63 | loss: 0.21459 | val_0_rmse: 0.51415 | val_1_rmse: 0.52084 |  0:02:09s
epoch 64 | loss: 0.21514 | val_0_rmse: 0.46608 | val_1_rmse: 0.47646 |  0:02:11s
epoch 65 | loss: 0.21392 | val_0_rmse: 0.48707 | val_1_rmse: 0.49541 |  0:02:13s
epoch 66 | loss: 0.21465 | val_0_rmse: 0.48019 | val_1_rmse: 0.4898  |  0:02:15s
epoch 67 | loss: 0.21206 | val_0_rmse: 0.51308 | val_1_rmse: 0.51973 |  0:02:17s
epoch 68 | loss: 0.21295 | val_0_rmse: 0.46522 | val_1_rmse: 0.47065 |  0:02:19s
epoch 69 | loss: 0.21127 | val_0_rmse: 0.50202 | val_1_rmse: 0.50989 |  0:02:21s
epoch 70 | loss: 0.20997 | val_0_rmse: 0.53414 | val_1_rmse: 0.54329 |  0:02:23s
epoch 71 | loss: 0.21243 | val_0_rmse: 0.48232 | val_1_rmse: 0.49052 |  0:02:25s
epoch 72 | loss: 0.215   | val_0_rmse: 0.46189 | val_1_rmse: 0.46708 |  0:02:27s
epoch 73 | loss: 0.22996 | val_0_rmse: 0.51284 | val_1_rmse: 0.51297 |  0:02:29s
epoch 74 | loss: 0.23567 | val_0_rmse: 0.50941 | val_1_rmse: 0.51823 |  0:02:31s
epoch 75 | loss: 0.22272 | val_0_rmse: 0.52409 | val_1_rmse: 0.53028 |  0:02:33s
epoch 76 | loss: 0.22374 | val_0_rmse: 0.47046 | val_1_rmse: 0.47629 |  0:02:35s
epoch 77 | loss: 0.2255  | val_0_rmse: 0.52049 | val_1_rmse: 0.52681 |  0:02:37s
epoch 78 | loss: 0.22328 | val_0_rmse: 0.49266 | val_1_rmse: 0.50006 |  0:02:39s
epoch 79 | loss: 0.23235 | val_0_rmse: 0.50454 | val_1_rmse: 0.50821 |  0:02:41s
epoch 80 | loss: 0.22316 | val_0_rmse: 0.51391 | val_1_rmse: 0.52257 |  0:02:43s
epoch 81 | loss: 0.22199 | val_0_rmse: 0.51767 | val_1_rmse: 0.5247  |  0:02:45s
epoch 82 | loss: 0.22604 | val_0_rmse: 0.53082 | val_1_rmse: 0.5425  |  0:02:47s
epoch 83 | loss: 0.2231  | val_0_rmse: 0.51468 | val_1_rmse: 0.52374 |  0:02:49s
epoch 84 | loss: 0.21633 | val_0_rmse: 0.47699 | val_1_rmse: 0.48597 |  0:02:51s
epoch 85 | loss: 0.2178  | val_0_rmse: 0.45775 | val_1_rmse: 0.46593 |  0:02:53s
epoch 86 | loss: 0.21688 | val_0_rmse: 0.48133 | val_1_rmse: 0.48834 |  0:02:55s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.4574
Best weights from best epoch are automatically used!
ended training at: 06:32:29
Feature importance:
[('Area', 0.40721155247446283), ('Baths', 0.11350570864338909), ('Beds', 0.0), ('Latitude', 0.14803058728137836), ('Longitude', 0.27405852119510243), ('Month', 0.03460725088646682), ('Year', 0.0225863795192004)]
Mean squared error is of 993957317.6742045
Mean absolute error:20756.71001272149
MAPE:0.23890475056626564
R2 score:0.751390931131682
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:32:29
epoch 0  | loss: 0.51361 | val_0_rmse: 0.59065 | val_1_rmse: 0.58508 |  0:00:02s
epoch 1  | loss: 0.3284  | val_0_rmse: 0.56648 | val_1_rmse: 0.55859 |  0:00:04s
epoch 2  | loss: 0.30338 | val_0_rmse: 0.52839 | val_1_rmse: 0.53593 |  0:00:06s
epoch 3  | loss: 0.28161 | val_0_rmse: 0.51661 | val_1_rmse: 0.52044 |  0:00:08s
epoch 4  | loss: 0.2713  | val_0_rmse: 0.50961 | val_1_rmse: 0.51639 |  0:00:10s
epoch 5  | loss: 0.27392 | val_0_rmse: 0.49717 | val_1_rmse: 0.50504 |  0:00:12s
epoch 6  | loss: 0.26101 | val_0_rmse: 0.51512 | val_1_rmse: 0.5227  |  0:00:14s
epoch 7  | loss: 0.25656 | val_0_rmse: 0.49146 | val_1_rmse: 0.49686 |  0:00:16s
epoch 8  | loss: 0.25271 | val_0_rmse: 0.48912 | val_1_rmse: 0.49386 |  0:00:18s
epoch 9  | loss: 0.24609 | val_0_rmse: 0.48594 | val_1_rmse: 0.49347 |  0:00:20s
epoch 10 | loss: 0.24422 | val_0_rmse: 0.4872  | val_1_rmse: 0.49236 |  0:00:22s
epoch 11 | loss: 0.24895 | val_0_rmse: 0.48671 | val_1_rmse: 0.4926  |  0:00:24s
epoch 12 | loss: 0.24205 | val_0_rmse: 0.48699 | val_1_rmse: 0.49479 |  0:00:26s
epoch 13 | loss: 0.2482  | val_0_rmse: 0.49139 | val_1_rmse: 0.49513 |  0:00:28s
epoch 14 | loss: 0.23871 | val_0_rmse: 0.55053 | val_1_rmse: 0.55861 |  0:00:30s
epoch 15 | loss: 0.24104 | val_0_rmse: 0.48359 | val_1_rmse: 0.48767 |  0:00:32s
epoch 16 | loss: 0.23809 | val_0_rmse: 0.48745 | val_1_rmse: 0.49261 |  0:00:34s
epoch 17 | loss: 0.23917 | val_0_rmse: 0.4833  | val_1_rmse: 0.48852 |  0:00:36s
epoch 18 | loss: 0.23937 | val_0_rmse: 0.51718 | val_1_rmse: 0.52325 |  0:00:38s
epoch 19 | loss: 0.26193 | val_0_rmse: 0.52728 | val_1_rmse: 0.53111 |  0:00:40s
epoch 20 | loss: 0.25292 | val_0_rmse: 0.48706 | val_1_rmse: 0.4924  |  0:00:42s
epoch 21 | loss: 0.24623 | val_0_rmse: 0.4902  | val_1_rmse: 0.49359 |  0:00:44s
epoch 22 | loss: 0.24384 | val_0_rmse: 0.48393 | val_1_rmse: 0.48967 |  0:00:46s
epoch 23 | loss: 0.24453 | val_0_rmse: 0.48295 | val_1_rmse: 0.48844 |  0:00:48s
epoch 24 | loss: 0.242   | val_0_rmse: 0.48279 | val_1_rmse: 0.48964 |  0:00:50s
epoch 25 | loss: 0.24077 | val_0_rmse: 0.48536 | val_1_rmse: 0.49027 |  0:00:52s
epoch 26 | loss: 0.24029 | val_0_rmse: 0.493   | val_1_rmse: 0.50127 |  0:00:54s
epoch 27 | loss: 0.24041 | val_0_rmse: 0.47932 | val_1_rmse: 0.48313 |  0:00:56s
epoch 28 | loss: 0.23988 | val_0_rmse: 0.47602 | val_1_rmse: 0.48011 |  0:00:58s
epoch 29 | loss: 0.23529 | val_0_rmse: 0.48223 | val_1_rmse: 0.48633 |  0:01:00s
epoch 30 | loss: 0.23836 | val_0_rmse: 0.48854 | val_1_rmse: 0.49275 |  0:01:02s
epoch 31 | loss: 0.2413  | val_0_rmse: 0.4924  | val_1_rmse: 0.49979 |  0:01:04s
epoch 32 | loss: 0.24656 | val_0_rmse: 0.5191  | val_1_rmse: 0.52518 |  0:01:06s
epoch 33 | loss: 0.24032 | val_0_rmse: 0.52879 | val_1_rmse: 0.53569 |  0:01:08s
epoch 34 | loss: 0.23384 | val_0_rmse: 0.48387 | val_1_rmse: 0.48768 |  0:01:10s
epoch 35 | loss: 0.23322 | val_0_rmse: 0.48954 | val_1_rmse: 0.49521 |  0:01:12s
epoch 36 | loss: 0.22945 | val_0_rmse: 0.48253 | val_1_rmse: 0.48791 |  0:01:14s
epoch 37 | loss: 0.23106 | val_0_rmse: 0.5014  | val_1_rmse: 0.50717 |  0:01:16s
epoch 38 | loss: 0.23319 | val_0_rmse: 0.46993 | val_1_rmse: 0.47429 |  0:01:18s
epoch 39 | loss: 0.22934 | val_0_rmse: 0.50417 | val_1_rmse: 0.51219 |  0:01:20s
epoch 40 | loss: 0.23114 | val_0_rmse: 0.48364 | val_1_rmse: 0.48845 |  0:01:22s
epoch 41 | loss: 0.23265 | val_0_rmse: 0.47173 | val_1_rmse: 0.4772  |  0:01:24s
epoch 42 | loss: 0.22748 | val_0_rmse: 0.51854 | val_1_rmse: 0.52913 |  0:01:26s
epoch 43 | loss: 0.22916 | val_0_rmse: 0.48481 | val_1_rmse: 0.48903 |  0:01:28s
epoch 44 | loss: 0.23092 | val_0_rmse: 0.47507 | val_1_rmse: 0.47911 |  0:01:30s
epoch 45 | loss: 0.22931 | val_0_rmse: 0.46939 | val_1_rmse: 0.47301 |  0:01:32s
epoch 46 | loss: 0.22931 | val_0_rmse: 0.47442 | val_1_rmse: 0.47882 |  0:01:34s
epoch 47 | loss: 0.22648 | val_0_rmse: 0.4688  | val_1_rmse: 0.4756  |  0:01:36s
epoch 48 | loss: 0.22899 | val_0_rmse: 0.47878 | val_1_rmse: 0.48171 |  0:01:38s
epoch 49 | loss: 0.22927 | val_0_rmse: 0.4776  | val_1_rmse: 0.48298 |  0:01:40s
epoch 50 | loss: 0.22649 | val_0_rmse: 0.47051 | val_1_rmse: 0.47563 |  0:01:42s
epoch 51 | loss: 0.22545 | val_0_rmse: 0.47579 | val_1_rmse: 0.48136 |  0:01:45s
epoch 52 | loss: 0.22651 | val_0_rmse: 0.46404 | val_1_rmse: 0.46961 |  0:01:47s
epoch 53 | loss: 0.22409 | val_0_rmse: 0.4647  | val_1_rmse: 0.46828 |  0:01:49s
epoch 54 | loss: 0.22445 | val_0_rmse: 0.46635 | val_1_rmse: 0.47262 |  0:01:51s
epoch 55 | loss: 0.22535 | val_0_rmse: 0.47242 | val_1_rmse: 0.47816 |  0:01:53s
epoch 56 | loss: 0.2234  | val_0_rmse: 0.46417 | val_1_rmse: 0.46748 |  0:01:55s
epoch 57 | loss: 0.22017 | val_0_rmse: 0.47552 | val_1_rmse: 0.48301 |  0:01:57s
epoch 58 | loss: 0.22328 | val_0_rmse: 0.46999 | val_1_rmse: 0.47349 |  0:01:59s
epoch 59 | loss: 0.2239  | val_0_rmse: 0.52289 | val_1_rmse: 0.53    |  0:02:01s
epoch 60 | loss: 0.22314 | val_0_rmse: 0.4826  | val_1_rmse: 0.48404 |  0:02:03s
epoch 61 | loss: 0.22591 | val_0_rmse: 0.49047 | val_1_rmse: 0.49245 |  0:02:05s
epoch 62 | loss: 0.22084 | val_0_rmse: 0.47161 | val_1_rmse: 0.4788  |  0:02:07s
epoch 63 | loss: 0.21823 | val_0_rmse: 0.46164 | val_1_rmse: 0.46579 |  0:02:09s
epoch 64 | loss: 0.21872 | val_0_rmse: 0.46767 | val_1_rmse: 0.47461 |  0:02:11s
epoch 65 | loss: 0.22022 | val_0_rmse: 0.46655 | val_1_rmse: 0.47337 |  0:02:13s
epoch 66 | loss: 0.21768 | val_0_rmse: 0.46032 | val_1_rmse: 0.46422 |  0:02:15s
epoch 67 | loss: 0.21725 | val_0_rmse: 0.50455 | val_1_rmse: 0.50541 |  0:02:17s
epoch 68 | loss: 0.21762 | val_0_rmse: 0.48061 | val_1_rmse: 0.48527 |  0:02:19s
epoch 69 | loss: 0.21694 | val_0_rmse: 0.4727  | val_1_rmse: 0.47855 |  0:02:21s
epoch 70 | loss: 0.21613 | val_0_rmse: 0.50858 | val_1_rmse: 0.5144  |  0:02:23s
epoch 71 | loss: 0.21478 | val_0_rmse: 0.45609 | val_1_rmse: 0.46224 |  0:02:25s
epoch 72 | loss: 0.2171  | val_0_rmse: 0.46093 | val_1_rmse: 0.46953 |  0:02:27s
epoch 73 | loss: 0.21404 | val_0_rmse: 0.45649 | val_1_rmse: 0.45992 |  0:02:29s
epoch 74 | loss: 0.21445 | val_0_rmse: 0.47136 | val_1_rmse: 0.47942 |  0:02:31s
epoch 75 | loss: 0.21564 | val_0_rmse: 0.4875  | val_1_rmse: 0.48455 |  0:02:33s
epoch 76 | loss: 0.21651 | val_0_rmse: 0.46154 | val_1_rmse: 0.46505 |  0:02:35s
epoch 77 | loss: 0.21505 | val_0_rmse: 0.46792 | val_1_rmse: 0.47121 |  0:02:37s
epoch 78 | loss: 0.21512 | val_0_rmse: 0.46666 | val_1_rmse: 0.47334 |  0:02:39s
epoch 79 | loss: 0.21239 | val_0_rmse: 0.46034 | val_1_rmse: 0.46872 |  0:02:41s
epoch 80 | loss: 0.21517 | val_0_rmse: 0.47826 | val_1_rmse: 0.48244 |  0:02:43s
epoch 81 | loss: 0.21734 | val_0_rmse: 0.57076 | val_1_rmse: 0.57211 |  0:02:45s
epoch 82 | loss: 0.21462 | val_0_rmse: 0.46696 | val_1_rmse: 0.47028 |  0:02:47s
epoch 83 | loss: 0.21113 | val_0_rmse: 0.46094 | val_1_rmse: 0.46814 |  0:02:49s
epoch 84 | loss: 0.20968 | val_0_rmse: 0.4603  | val_1_rmse: 0.46604 |  0:02:51s
epoch 85 | loss: 0.20932 | val_0_rmse: 0.47207 | val_1_rmse: 0.47889 |  0:02:53s
epoch 86 | loss: 0.21244 | val_0_rmse: 0.45477 | val_1_rmse: 0.46221 |  0:02:55s
epoch 87 | loss: 0.21202 | val_0_rmse: 0.45252 | val_1_rmse: 0.46078 |  0:02:57s
epoch 88 | loss: 0.21067 | val_0_rmse: 0.46993 | val_1_rmse: 0.4762  |  0:02:59s
epoch 89 | loss: 0.21118 | val_0_rmse: 0.49271 | val_1_rmse: 0.4916  |  0:03:01s
epoch 90 | loss: 0.20938 | val_0_rmse: 0.48351 | val_1_rmse: 0.48862 |  0:03:03s
epoch 91 | loss: 0.21263 | val_0_rmse: 0.45046 | val_1_rmse: 0.45752 |  0:03:06s
epoch 92 | loss: 0.21138 | val_0_rmse: 0.46186 | val_1_rmse: 0.47004 |  0:03:08s
epoch 93 | loss: 0.207   | val_0_rmse: 0.44995 | val_1_rmse: 0.45924 |  0:03:10s
epoch 94 | loss: 0.21019 | val_0_rmse: 0.45631 | val_1_rmse: 0.4626  |  0:03:12s
epoch 95 | loss: 0.21347 | val_0_rmse: 0.45231 | val_1_rmse: 0.45809 |  0:03:14s
epoch 96 | loss: 0.21072 | val_0_rmse: 0.47066 | val_1_rmse: 0.47307 |  0:03:16s
epoch 97 | loss: 0.20693 | val_0_rmse: 0.50907 | val_1_rmse: 0.50871 |  0:03:18s
epoch 98 | loss: 0.20821 | val_0_rmse: 0.45847 | val_1_rmse: 0.46652 |  0:03:20s
epoch 99 | loss: 0.21024 | val_0_rmse: 0.58983 | val_1_rmse: 0.58491 |  0:03:22s
epoch 100| loss: 0.21161 | val_0_rmse: 0.45007 | val_1_rmse: 0.45721 |  0:03:24s
epoch 101| loss: 0.20844 | val_0_rmse: 0.45835 | val_1_rmse: 0.46305 |  0:03:26s
epoch 102| loss: 0.2074  | val_0_rmse: 0.45715 | val_1_rmse: 0.46217 |  0:03:28s
epoch 103| loss: 0.20771 | val_0_rmse: 0.45755 | val_1_rmse: 0.46302 |  0:03:30s
epoch 104| loss: 0.20972 | val_0_rmse: 0.45541 | val_1_rmse: 0.4603  |  0:03:32s
epoch 105| loss: 0.20882 | val_0_rmse: 0.52933 | val_1_rmse: 0.52729 |  0:03:34s
epoch 106| loss: 0.20721 | val_0_rmse: 0.49137 | val_1_rmse: 0.49682 |  0:03:36s
epoch 107| loss: 0.20565 | val_0_rmse: 0.4555  | val_1_rmse: 0.4653  |  0:03:38s
epoch 108| loss: 0.20643 | val_0_rmse: 0.45132 | val_1_rmse: 0.45557 |  0:03:40s
epoch 109| loss: 0.20939 | val_0_rmse: 0.46952 | val_1_rmse: 0.47367 |  0:03:42s
epoch 110| loss: 0.2058  | val_0_rmse: 0.46913 | val_1_rmse: 0.47942 |  0:03:44s
epoch 111| loss: 0.20668 | val_0_rmse: 0.46214 | val_1_rmse: 0.46903 |  0:03:46s
epoch 112| loss: 0.20715 | val_0_rmse: 0.45569 | val_1_rmse: 0.46415 |  0:03:48s
epoch 113| loss: 0.20506 | val_0_rmse: 0.51009 | val_1_rmse: 0.50878 |  0:03:50s
epoch 114| loss: 0.20576 | val_0_rmse: 0.45489 | val_1_rmse: 0.46203 |  0:03:52s
epoch 115| loss: 0.20659 | val_0_rmse: 0.47236 | val_1_rmse: 0.47737 |  0:03:54s
epoch 116| loss: 0.20596 | val_0_rmse: 0.57538 | val_1_rmse: 0.56997 |  0:03:56s
epoch 117| loss: 0.20626 | val_0_rmse: 0.46484 | val_1_rmse: 0.47324 |  0:03:58s
epoch 118| loss: 0.20492 | val_0_rmse: 0.44846 | val_1_rmse: 0.45699 |  0:04:00s
epoch 119| loss: 0.20452 | val_0_rmse: 0.44896 | val_1_rmse: 0.45635 |  0:04:02s
epoch 120| loss: 0.20487 | val_0_rmse: 0.48342 | val_1_rmse: 0.48385 |  0:04:04s
epoch 121| loss: 0.20294 | val_0_rmse: 0.46439 | val_1_rmse: 0.47173 |  0:04:06s
epoch 122| loss: 0.20447 | val_0_rmse: 0.54777 | val_1_rmse: 0.54289 |  0:04:08s
epoch 123| loss: 0.20518 | val_0_rmse: 0.5295  | val_1_rmse: 0.53106 |  0:04:10s
epoch 124| loss: 0.20706 | val_0_rmse: 0.48942 | val_1_rmse: 0.49298 |  0:04:12s
epoch 125| loss: 0.20448 | val_0_rmse: 0.59002 | val_1_rmse: 0.58685 |  0:04:14s
epoch 126| loss: 0.20549 | val_0_rmse: 0.47245 | val_1_rmse: 0.48157 |  0:04:16s
epoch 127| loss: 0.20425 | val_0_rmse: 0.54077 | val_1_rmse: 0.54115 |  0:04:18s
epoch 128| loss: 0.20858 | val_0_rmse: 0.45818 | val_1_rmse: 0.46441 |  0:04:20s
epoch 129| loss: 0.21026 | val_0_rmse: 0.47539 | val_1_rmse: 0.48084 |  0:04:22s
epoch 130| loss: 0.20688 | val_0_rmse: 0.48606 | val_1_rmse: 0.48601 |  0:04:24s
epoch 131| loss: 0.20441 | val_0_rmse: 0.46142 | val_1_rmse: 0.47182 |  0:04:26s
epoch 132| loss: 0.20239 | val_0_rmse: 0.456   | val_1_rmse: 0.4618  |  0:04:28s
epoch 133| loss: 0.20254 | val_0_rmse: 0.48224 | val_1_rmse: 0.48059 |  0:04:30s
epoch 134| loss: 0.20543 | val_0_rmse: 0.56052 | val_1_rmse: 0.55459 |  0:04:32s
epoch 135| loss: 0.20169 | val_0_rmse: 0.52835 | val_1_rmse: 0.52571 |  0:04:34s
epoch 136| loss: 0.20187 | val_0_rmse: 0.51116 | val_1_rmse: 0.50858 |  0:04:36s
epoch 137| loss: 0.2025  | val_0_rmse: 0.46126 | val_1_rmse: 0.46511 |  0:04:38s
epoch 138| loss: 0.20145 | val_0_rmse: 0.45096 | val_1_rmse: 0.45773 |  0:04:40s

Early stopping occured at epoch 138 with best_epoch = 108 and best_val_1_rmse = 0.45557
Best weights from best epoch are automatically used!
ended training at: 06:37:11
Feature importance:
[('Area', 0.3098080352919259), ('Baths', 0.16495100593662615), ('Beds', 0.1230993419648312), ('Latitude', 0.01642936857935791), ('Longitude', 0.343473877966908), ('Month', 0.019241493349842095), ('Year', 0.022996876910508727)]
Mean squared error is of 1006315202.3424228
Mean absolute error:21104.554339064336
MAPE:0.2407651417881316
R2 score:0.7489254412026547
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:37:11
epoch 0  | loss: 0.53626 | val_0_rmse: 0.62544 | val_1_rmse: 0.61523 |  0:00:02s
epoch 1  | loss: 0.33782 | val_0_rmse: 0.54201 | val_1_rmse: 0.52556 |  0:00:04s
epoch 2  | loss: 0.29945 | val_0_rmse: 0.53856 | val_1_rmse: 0.52422 |  0:00:06s
epoch 3  | loss: 0.28173 | val_0_rmse: 0.51094 | val_1_rmse: 0.49715 |  0:00:08s
epoch 4  | loss: 0.27015 | val_0_rmse: 0.50208 | val_1_rmse: 0.48572 |  0:00:10s
epoch 5  | loss: 0.26464 | val_0_rmse: 0.50358 | val_1_rmse: 0.48866 |  0:00:12s
epoch 6  | loss: 0.26703 | val_0_rmse: 0.51316 | val_1_rmse: 0.49979 |  0:00:14s
epoch 7  | loss: 0.26477 | val_0_rmse: 0.49749 | val_1_rmse: 0.48342 |  0:00:16s
epoch 8  | loss: 0.26246 | val_0_rmse: 0.53453 | val_1_rmse: 0.5212  |  0:00:18s
epoch 9  | loss: 0.25765 | val_0_rmse: 0.49212 | val_1_rmse: 0.47601 |  0:00:20s
epoch 10 | loss: 0.25942 | val_0_rmse: 0.54095 | val_1_rmse: 0.51964 |  0:00:22s
epoch 11 | loss: 0.26431 | val_0_rmse: 0.49275 | val_1_rmse: 0.47694 |  0:00:24s
epoch 12 | loss: 0.25168 | val_0_rmse: 0.48775 | val_1_rmse: 0.47818 |  0:00:26s
epoch 13 | loss: 0.24455 | val_0_rmse: 0.531   | val_1_rmse: 0.51859 |  0:00:28s
epoch 14 | loss: 0.24487 | val_0_rmse: 0.51957 | val_1_rmse: 0.5082  |  0:00:30s
epoch 15 | loss: 0.23825 | val_0_rmse: 0.47509 | val_1_rmse: 0.46204 |  0:00:32s
epoch 16 | loss: 0.23395 | val_0_rmse: 0.49109 | val_1_rmse: 0.47927 |  0:00:34s
epoch 17 | loss: 0.23254 | val_0_rmse: 0.5145  | val_1_rmse: 0.50235 |  0:00:36s
epoch 18 | loss: 0.23556 | val_0_rmse: 0.47384 | val_1_rmse: 0.4622  |  0:00:38s
epoch 19 | loss: 0.2311  | val_0_rmse: 0.4657  | val_1_rmse: 0.45319 |  0:00:40s
epoch 20 | loss: 0.23341 | val_0_rmse: 0.56736 | val_1_rmse: 0.55765 |  0:00:42s
epoch 21 | loss: 0.22803 | val_0_rmse: 0.471   | val_1_rmse: 0.46087 |  0:00:44s
epoch 22 | loss: 0.22982 | val_0_rmse: 0.47694 | val_1_rmse: 0.46362 |  0:00:46s
epoch 23 | loss: 0.23109 | val_0_rmse: 0.48093 | val_1_rmse: 0.4679  |  0:00:48s
epoch 24 | loss: 0.22578 | val_0_rmse: 0.4664  | val_1_rmse: 0.45285 |  0:00:50s
epoch 25 | loss: 0.22912 | val_0_rmse: 0.47198 | val_1_rmse: 0.45932 |  0:00:52s
epoch 26 | loss: 0.22714 | val_0_rmse: 0.46105 | val_1_rmse: 0.45035 |  0:00:54s
epoch 27 | loss: 0.22539 | val_0_rmse: 0.46525 | val_1_rmse: 0.45472 |  0:00:56s
epoch 28 | loss: 0.23113 | val_0_rmse: 0.46282 | val_1_rmse: 0.45054 |  0:00:58s
epoch 29 | loss: 0.23121 | val_0_rmse: 0.49003 | val_1_rmse: 0.47835 |  0:01:00s
epoch 30 | loss: 0.22562 | val_0_rmse: 0.47952 | val_1_rmse: 0.46799 |  0:01:02s
epoch 31 | loss: 0.2216  | val_0_rmse: 0.46134 | val_1_rmse: 0.44907 |  0:01:04s
epoch 32 | loss: 0.23264 | val_0_rmse: 0.47362 | val_1_rmse: 0.46151 |  0:01:06s
epoch 33 | loss: 0.2347  | val_0_rmse: 0.48791 | val_1_rmse: 0.47604 |  0:01:08s
epoch 34 | loss: 0.22352 | val_0_rmse: 0.46172 | val_1_rmse: 0.44856 |  0:01:10s
epoch 35 | loss: 0.22469 | val_0_rmse: 0.47697 | val_1_rmse: 0.46557 |  0:01:12s
epoch 36 | loss: 0.22272 | val_0_rmse: 0.4576  | val_1_rmse: 0.4461  |  0:01:14s
epoch 37 | loss: 0.22055 | val_0_rmse: 0.47031 | val_1_rmse: 0.45721 |  0:01:16s
epoch 38 | loss: 0.22156 | val_0_rmse: 0.48871 | val_1_rmse: 0.47839 |  0:01:18s
epoch 39 | loss: 0.22119 | val_0_rmse: 0.47763 | val_1_rmse: 0.46601 |  0:01:20s
epoch 40 | loss: 0.21907 | val_0_rmse: 0.47567 | val_1_rmse: 0.46422 |  0:01:22s
epoch 41 | loss: 0.22055 | val_0_rmse: 0.4768  | val_1_rmse: 0.46873 |  0:01:24s
epoch 42 | loss: 0.22166 | val_0_rmse: 0.48707 | val_1_rmse: 0.47938 |  0:01:26s
epoch 43 | loss: 0.22472 | val_0_rmse: 0.49428 | val_1_rmse: 0.48279 |  0:01:28s
epoch 44 | loss: 0.22374 | val_0_rmse: 0.49226 | val_1_rmse: 0.48109 |  0:01:30s
epoch 45 | loss: 0.22048 | val_0_rmse: 0.46623 | val_1_rmse: 0.45309 |  0:01:32s
epoch 46 | loss: 0.21766 | val_0_rmse: 0.46349 | val_1_rmse: 0.45392 |  0:01:34s
epoch 47 | loss: 0.223   | val_0_rmse: 0.47702 | val_1_rmse: 0.46764 |  0:01:36s
epoch 48 | loss: 0.21627 | val_0_rmse: 0.46683 | val_1_rmse: 0.45464 |  0:01:38s
epoch 49 | loss: 0.21889 | val_0_rmse: 0.45545 | val_1_rmse: 0.44513 |  0:01:40s
epoch 50 | loss: 0.22041 | val_0_rmse: 0.48702 | val_1_rmse: 0.47583 |  0:01:43s
epoch 51 | loss: 0.21993 | val_0_rmse: 0.47636 | val_1_rmse: 0.46606 |  0:01:44s
epoch 52 | loss: 0.22223 | val_0_rmse: 0.46587 | val_1_rmse: 0.45468 |  0:01:47s
epoch 53 | loss: 0.21757 | val_0_rmse: 0.48008 | val_1_rmse: 0.47266 |  0:01:49s
epoch 54 | loss: 0.21744 | val_0_rmse: 0.4582  | val_1_rmse: 0.44846 |  0:01:51s
epoch 55 | loss: 0.2184  | val_0_rmse: 0.47371 | val_1_rmse: 0.46504 |  0:01:53s
epoch 56 | loss: 0.21905 | val_0_rmse: 0.45802 | val_1_rmse: 0.44699 |  0:01:55s
epoch 57 | loss: 0.21865 | val_0_rmse: 0.45901 | val_1_rmse: 0.4502  |  0:01:57s
epoch 58 | loss: 0.22156 | val_0_rmse: 0.46372 | val_1_rmse: 0.4528  |  0:01:59s
epoch 59 | loss: 0.22148 | val_0_rmse: 0.51352 | val_1_rmse: 0.50248 |  0:02:01s
epoch 60 | loss: 0.21937 | val_0_rmse: 0.47888 | val_1_rmse: 0.46842 |  0:02:03s
epoch 61 | loss: 0.21838 | val_0_rmse: 0.47057 | val_1_rmse: 0.45909 |  0:02:05s
epoch 62 | loss: 0.22845 | val_0_rmse: 0.47335 | val_1_rmse: 0.46273 |  0:02:07s
epoch 63 | loss: 0.22036 | val_0_rmse: 0.48837 | val_1_rmse: 0.47593 |  0:02:09s
epoch 64 | loss: 0.21995 | val_0_rmse: 0.53012 | val_1_rmse: 0.51951 |  0:02:11s
epoch 65 | loss: 0.2202  | val_0_rmse: 0.5044  | val_1_rmse: 0.49483 |  0:02:13s
epoch 66 | loss: 0.21843 | val_0_rmse: 0.4534  | val_1_rmse: 0.44317 |  0:02:15s
epoch 67 | loss: 0.21765 | val_0_rmse: 0.4576  | val_1_rmse: 0.44751 |  0:02:17s
epoch 68 | loss: 0.21481 | val_0_rmse: 0.46609 | val_1_rmse: 0.45523 |  0:02:19s
epoch 69 | loss: 0.21765 | val_0_rmse: 0.4642  | val_1_rmse: 0.454   |  0:02:21s
epoch 70 | loss: 0.21856 | val_0_rmse: 0.51636 | val_1_rmse: 0.50558 |  0:02:23s
epoch 71 | loss: 0.2136  | val_0_rmse: 0.47995 | val_1_rmse: 0.46908 |  0:02:25s
epoch 72 | loss: 0.21525 | val_0_rmse: 0.5308  | val_1_rmse: 0.52165 |  0:02:27s
epoch 73 | loss: 0.21727 | val_0_rmse: 0.50302 | val_1_rmse: 0.49028 |  0:02:29s
epoch 74 | loss: 0.21565 | val_0_rmse: 0.46602 | val_1_rmse: 0.45596 |  0:02:31s
epoch 75 | loss: 0.21762 | val_0_rmse: 0.46426 | val_1_rmse: 0.4515  |  0:02:33s
epoch 76 | loss: 0.21629 | val_0_rmse: 0.49135 | val_1_rmse: 0.47945 |  0:02:35s
epoch 77 | loss: 0.21317 | val_0_rmse: 0.49938 | val_1_rmse: 0.49063 |  0:02:37s
epoch 78 | loss: 0.21523 | val_0_rmse: 0.47149 | val_1_rmse: 0.46116 |  0:02:39s
epoch 79 | loss: 0.21437 | val_0_rmse: 0.47707 | val_1_rmse: 0.46741 |  0:02:41s
epoch 80 | loss: 0.21289 | val_0_rmse: 0.46742 | val_1_rmse: 0.45577 |  0:02:43s
epoch 81 | loss: 0.21587 | val_0_rmse: 0.47735 | val_1_rmse: 0.46773 |  0:02:45s
epoch 82 | loss: 0.21298 | val_0_rmse: 0.45991 | val_1_rmse: 0.44864 |  0:02:47s
epoch 83 | loss: 0.21839 | val_0_rmse: 0.47449 | val_1_rmse: 0.46443 |  0:02:49s
epoch 84 | loss: 0.21216 | val_0_rmse: 0.45641 | val_1_rmse: 0.44734 |  0:02:51s
epoch 85 | loss: 0.21354 | val_0_rmse: 0.46803 | val_1_rmse: 0.45787 |  0:02:53s
epoch 86 | loss: 0.21364 | val_0_rmse: 0.47247 | val_1_rmse: 0.46449 |  0:02:55s
epoch 87 | loss: 0.21492 | val_0_rmse: 0.47701 | val_1_rmse: 0.46898 |  0:02:57s
epoch 88 | loss: 0.21444 | val_0_rmse: 0.50935 | val_1_rmse: 0.50089 |  0:02:59s
epoch 89 | loss: 0.21349 | val_0_rmse: 0.46156 | val_1_rmse: 0.45082 |  0:03:01s
epoch 90 | loss: 0.21671 | val_0_rmse: 0.46827 | val_1_rmse: 0.45659 |  0:03:03s
epoch 91 | loss: 0.21377 | val_0_rmse: 0.50466 | val_1_rmse: 0.49492 |  0:03:05s
epoch 92 | loss: 0.23351 | val_0_rmse: 0.54836 | val_1_rmse: 0.53749 |  0:03:07s
epoch 93 | loss: 0.23682 | val_0_rmse: 0.52588 | val_1_rmse: 0.51435 |  0:03:09s
epoch 94 | loss: 0.23135 | val_0_rmse: 0.49393 | val_1_rmse: 0.48053 |  0:03:11s
epoch 95 | loss: 0.23301 | val_0_rmse: 0.49378 | val_1_rmse: 0.48198 |  0:03:13s
epoch 96 | loss: 0.23334 | val_0_rmse: 0.47512 | val_1_rmse: 0.46204 |  0:03:15s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.44317
Best weights from best epoch are automatically used!
ended training at: 06:40:27
Feature importance:
[('Area', 0.34862338096097945), ('Baths', 0.13504393491931993), ('Beds', 0.12244561464858135), ('Latitude', 0.10617190301246497), ('Longitude', 0.27683014664807215), ('Month', 0.010885019810582144), ('Year', 0.0)]
Mean squared error is of 1010645975.1519673
Mean absolute error:20966.052819111344
MAPE:0.24434586176366435
R2 score:0.7439199656902596
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:40:52
epoch 0  | loss: 0.46232 | val_0_rmse: 0.60282 | val_1_rmse: 0.60709 |  0:00:03s
epoch 1  | loss: 0.31783 | val_0_rmse: 0.5339  | val_1_rmse: 0.53019 |  0:00:06s
epoch 2  | loss: 0.28885 | val_0_rmse: 0.50431 | val_1_rmse: 0.50049 |  0:00:09s
epoch 3  | loss: 0.26086 | val_0_rmse: 0.48448 | val_1_rmse: 0.48004 |  0:00:12s
epoch 4  | loss: 0.25184 | val_0_rmse: 0.48409 | val_1_rmse: 0.4809  |  0:00:16s
epoch 5  | loss: 0.24967 | val_0_rmse: 0.48008 | val_1_rmse: 0.47565 |  0:00:19s
epoch 6  | loss: 0.24887 | val_0_rmse: 0.4803  | val_1_rmse: 0.47622 |  0:00:22s
epoch 7  | loss: 0.23323 | val_0_rmse: 0.45907 | val_1_rmse: 0.45222 |  0:00:25s
epoch 8  | loss: 0.23468 | val_0_rmse: 0.4624  | val_1_rmse: 0.46026 |  0:00:29s
epoch 9  | loss: 0.23192 | val_0_rmse: 0.45985 | val_1_rmse: 0.4558  |  0:00:32s
epoch 10 | loss: 0.23626 | val_0_rmse: 0.46382 | val_1_rmse: 0.45479 |  0:00:35s
epoch 11 | loss: 0.22847 | val_0_rmse: 0.45116 | val_1_rmse: 0.44411 |  0:00:38s
epoch 12 | loss: 0.22192 | val_0_rmse: 0.45476 | val_1_rmse: 0.45035 |  0:00:42s
epoch 13 | loss: 0.21878 | val_0_rmse: 0.46941 | val_1_rmse: 0.46137 |  0:00:45s
epoch 14 | loss: 0.22092 | val_0_rmse: 0.46515 | val_1_rmse: 0.4575  |  0:00:48s
epoch 15 | loss: 0.22021 | val_0_rmse: 0.45254 | val_1_rmse: 0.44484 |  0:00:51s
epoch 16 | loss: 0.21841 | val_0_rmse: 0.46032 | val_1_rmse: 0.45659 |  0:00:54s
epoch 17 | loss: 0.21861 | val_0_rmse: 0.44767 | val_1_rmse: 0.4425  |  0:00:58s
epoch 18 | loss: 0.2175  | val_0_rmse: 0.44894 | val_1_rmse: 0.44196 |  0:01:01s
epoch 19 | loss: 0.21418 | val_0_rmse: 0.45128 | val_1_rmse: 0.44742 |  0:01:04s
epoch 20 | loss: 0.21725 | val_0_rmse: 0.44355 | val_1_rmse: 0.44064 |  0:01:08s
epoch 21 | loss: 0.2141  | val_0_rmse: 0.458   | val_1_rmse: 0.45728 |  0:01:11s
epoch 22 | loss: 0.21335 | val_0_rmse: 0.44308 | val_1_rmse: 0.44013 |  0:01:14s
epoch 23 | loss: 0.21149 | val_0_rmse: 0.46463 | val_1_rmse: 0.45655 |  0:01:17s
epoch 24 | loss: 0.21397 | val_0_rmse: 0.47331 | val_1_rmse: 0.47133 |  0:01:20s
epoch 25 | loss: 0.21208 | val_0_rmse: 0.44618 | val_1_rmse: 0.44204 |  0:01:24s
epoch 26 | loss: 0.21272 | val_0_rmse: 0.44669 | val_1_rmse: 0.4419  |  0:01:27s
epoch 27 | loss: 0.21284 | val_0_rmse: 0.43902 | val_1_rmse: 0.43454 |  0:01:30s
epoch 28 | loss: 0.21079 | val_0_rmse: 0.45062 | val_1_rmse: 0.44954 |  0:01:33s
epoch 29 | loss: 0.20871 | val_0_rmse: 0.45042 | val_1_rmse: 0.44267 |  0:01:37s
epoch 30 | loss: 0.21037 | val_0_rmse: 0.45175 | val_1_rmse: 0.44962 |  0:01:40s
epoch 31 | loss: 0.21108 | val_0_rmse: 0.44147 | val_1_rmse: 0.43771 |  0:01:43s
epoch 32 | loss: 0.20939 | val_0_rmse: 0.43849 | val_1_rmse: 0.43244 |  0:01:46s
epoch 33 | loss: 0.20893 | val_0_rmse: 0.43991 | val_1_rmse: 0.43624 |  0:01:50s
epoch 34 | loss: 0.20529 | val_0_rmse: 0.43933 | val_1_rmse: 0.43626 |  0:01:53s
epoch 35 | loss: 0.20578 | val_0_rmse: 0.4369  | val_1_rmse: 0.43617 |  0:01:56s
epoch 36 | loss: 0.20695 | val_0_rmse: 0.43668 | val_1_rmse: 0.43685 |  0:01:59s
epoch 37 | loss: 0.20562 | val_0_rmse: 0.44645 | val_1_rmse: 0.44423 |  0:02:02s
epoch 38 | loss: 0.20477 | val_0_rmse: 0.44468 | val_1_rmse: 0.44254 |  0:02:06s
epoch 39 | loss: 0.20582 | val_0_rmse: 0.44654 | val_1_rmse: 0.44236 |  0:02:09s
epoch 40 | loss: 0.20196 | val_0_rmse: 0.43057 | val_1_rmse: 0.43043 |  0:02:12s
epoch 41 | loss: 0.20413 | val_0_rmse: 0.44474 | val_1_rmse: 0.44245 |  0:02:15s
epoch 42 | loss: 0.2045  | val_0_rmse: 0.4298  | val_1_rmse: 0.42844 |  0:02:19s
epoch 43 | loss: 0.20512 | val_0_rmse: 0.43287 | val_1_rmse: 0.42788 |  0:02:22s
epoch 44 | loss: 0.20565 | val_0_rmse: 0.43992 | val_1_rmse: 0.43541 |  0:02:25s
epoch 45 | loss: 0.20637 | val_0_rmse: 0.43659 | val_1_rmse: 0.43673 |  0:02:28s
epoch 46 | loss: 0.20372 | val_0_rmse: 0.43083 | val_1_rmse: 0.43113 |  0:02:32s
epoch 47 | loss: 0.20246 | val_0_rmse: 0.43389 | val_1_rmse: 0.42971 |  0:02:35s
epoch 48 | loss: 0.20132 | val_0_rmse: 0.4734  | val_1_rmse: 0.47131 |  0:02:38s
epoch 49 | loss: 0.20663 | val_0_rmse: 0.42954 | val_1_rmse: 0.42908 |  0:02:41s
epoch 50 | loss: 0.20041 | val_0_rmse: 0.43053 | val_1_rmse: 0.42875 |  0:02:45s
epoch 51 | loss: 0.20036 | val_0_rmse: 0.44165 | val_1_rmse: 0.44131 |  0:02:48s
epoch 52 | loss: 0.20563 | val_0_rmse: 0.45235 | val_1_rmse: 0.45027 |  0:02:51s
epoch 53 | loss: 0.2028  | val_0_rmse: 0.43821 | val_1_rmse: 0.43617 |  0:02:54s
epoch 54 | loss: 0.20171 | val_0_rmse: 0.44971 | val_1_rmse: 0.44924 |  0:02:58s
epoch 55 | loss: 0.20012 | val_0_rmse: 0.42327 | val_1_rmse: 0.42347 |  0:03:01s
epoch 56 | loss: 0.19759 | val_0_rmse: 0.43017 | val_1_rmse: 0.42928 |  0:03:04s
epoch 57 | loss: 0.19912 | val_0_rmse: 0.44853 | val_1_rmse: 0.44287 |  0:03:07s
epoch 58 | loss: 0.19765 | val_0_rmse: 0.43155 | val_1_rmse: 0.43107 |  0:03:11s
epoch 59 | loss: 0.19669 | val_0_rmse: 0.42955 | val_1_rmse: 0.42787 |  0:03:14s
epoch 60 | loss: 0.19836 | val_0_rmse: 0.4342  | val_1_rmse: 0.4347  |  0:03:17s
epoch 61 | loss: 0.20337 | val_0_rmse: 0.43088 | val_1_rmse: 0.42986 |  0:03:20s
epoch 62 | loss: 0.1967  | val_0_rmse: 0.43163 | val_1_rmse: 0.42993 |  0:03:23s
epoch 63 | loss: 0.20136 | val_0_rmse: 0.45368 | val_1_rmse: 0.45018 |  0:03:27s
epoch 64 | loss: 0.19862 | val_0_rmse: 0.43574 | val_1_rmse: 0.4361  |  0:03:30s
epoch 65 | loss: 0.19654 | val_0_rmse: 0.43791 | val_1_rmse: 0.43416 |  0:03:33s
epoch 66 | loss: 0.19961 | val_0_rmse: 0.43102 | val_1_rmse: 0.4267  |  0:03:36s
epoch 67 | loss: 0.19604 | val_0_rmse: 0.43317 | val_1_rmse: 0.43132 |  0:03:40s
epoch 68 | loss: 0.19965 | val_0_rmse: 0.44668 | val_1_rmse: 0.44634 |  0:03:43s
epoch 69 | loss: 0.20069 | val_0_rmse: 0.4366  | val_1_rmse: 0.43883 |  0:03:46s
epoch 70 | loss: 0.19468 | val_0_rmse: 0.42826 | val_1_rmse: 0.4317  |  0:03:49s
epoch 71 | loss: 0.19601 | val_0_rmse: 0.44756 | val_1_rmse: 0.44968 |  0:03:53s
epoch 72 | loss: 0.1973  | val_0_rmse: 0.42277 | val_1_rmse: 0.4216  |  0:03:56s
epoch 73 | loss: 0.19641 | val_0_rmse: 0.44422 | val_1_rmse: 0.44789 |  0:03:59s
epoch 74 | loss: 0.19414 | val_0_rmse: 0.43549 | val_1_rmse: 0.43655 |  0:04:02s
epoch 75 | loss: 0.19433 | val_0_rmse: 0.41943 | val_1_rmse: 0.41934 |  0:04:06s
epoch 76 | loss: 0.19474 | val_0_rmse: 0.42644 | val_1_rmse: 0.42764 |  0:04:09s
epoch 77 | loss: 0.19406 | val_0_rmse: 0.42833 | val_1_rmse: 0.42815 |  0:04:12s
epoch 78 | loss: 0.19585 | val_0_rmse: 0.43832 | val_1_rmse: 0.43943 |  0:04:15s
epoch 79 | loss: 0.1961  | val_0_rmse: 0.43338 | val_1_rmse: 0.43469 |  0:04:18s
epoch 80 | loss: 0.19435 | val_0_rmse: 0.43087 | val_1_rmse: 0.43518 |  0:04:22s
epoch 81 | loss: 0.19343 | val_0_rmse: 0.42477 | val_1_rmse: 0.42756 |  0:04:25s
epoch 82 | loss: 0.19525 | val_0_rmse: 0.42801 | val_1_rmse: 0.42805 |  0:04:28s
epoch 83 | loss: 0.19405 | val_0_rmse: 0.44434 | val_1_rmse: 0.44555 |  0:04:31s
epoch 84 | loss: 0.19272 | val_0_rmse: 0.43342 | val_1_rmse: 0.43542 |  0:04:35s
epoch 85 | loss: 0.19279 | val_0_rmse: 0.43124 | val_1_rmse: 0.4332  |  0:04:38s
epoch 86 | loss: 0.1981  | val_0_rmse: 0.42651 | val_1_rmse: 0.42919 |  0:04:41s
epoch 87 | loss: 0.19858 | val_0_rmse: 0.42555 | val_1_rmse: 0.42686 |  0:04:44s
epoch 88 | loss: 0.197   | val_0_rmse: 0.42572 | val_1_rmse: 0.42757 |  0:04:48s
epoch 89 | loss: 0.19523 | val_0_rmse: 0.43317 | val_1_rmse: 0.43446 |  0:04:51s
epoch 90 | loss: 0.19293 | val_0_rmse: 0.42825 | val_1_rmse: 0.42767 |  0:04:54s
epoch 91 | loss: 0.19535 | val_0_rmse: 0.42464 | val_1_rmse: 0.4269  |  0:04:57s
epoch 92 | loss: 0.1928  | val_0_rmse: 0.43852 | val_1_rmse: 0.44032 |  0:05:00s
epoch 93 | loss: 0.19412 | val_0_rmse: 0.42415 | val_1_rmse: 0.42765 |  0:05:04s
epoch 94 | loss: 0.1916  | val_0_rmse: 0.43255 | val_1_rmse: 0.43381 |  0:05:07s
epoch 95 | loss: 0.18917 | val_0_rmse: 0.42132 | val_1_rmse: 0.42418 |  0:05:10s
epoch 96 | loss: 0.18848 | val_0_rmse: 0.43309 | val_1_rmse: 0.43451 |  0:05:13s
epoch 97 | loss: 0.19194 | val_0_rmse: 0.42697 | val_1_rmse: 0.42923 |  0:05:17s
epoch 98 | loss: 0.19401 | val_0_rmse: 0.42342 | val_1_rmse: 0.42748 |  0:05:20s
epoch 99 | loss: 0.19217 | val_0_rmse: 0.41946 | val_1_rmse: 0.42283 |  0:05:23s
epoch 100| loss: 0.18957 | val_0_rmse: 0.42859 | val_1_rmse: 0.43006 |  0:05:26s
epoch 101| loss: 0.19095 | val_0_rmse: 0.42429 | val_1_rmse: 0.42756 |  0:05:30s
epoch 102| loss: 0.1929  | val_0_rmse: 0.41887 | val_1_rmse: 0.42383 |  0:05:33s
epoch 103| loss: 0.19574 | val_0_rmse: 0.42262 | val_1_rmse: 0.42927 |  0:05:36s
epoch 104| loss: 0.19127 | val_0_rmse: 0.44219 | val_1_rmse: 0.44811 |  0:05:39s
epoch 105| loss: 0.19078 | val_0_rmse: 0.44699 | val_1_rmse: 0.44965 |  0:05:43s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.41934
Best weights from best epoch are automatically used!
ended training at: 06:46:36
Feature importance:
[('Area', 0.04564082584560818), ('Baths', 0.10178640857252974), ('Beds', 0.21662637166663987), ('Latitude', 0.16884912378585926), ('Longitude', 0.21204084996704425), ('Month', 0.0260840349380559), ('Year', 0.2289723852242628)]
Mean squared error is of 10265253004.104216
Mean absolute error:69453.98495241493
MAPE:0.25803347399093857
R2 score:0.819424738186475
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:46:37
epoch 0  | loss: 0.45572 | val_0_rmse: 0.57052 | val_1_rmse: 0.57158 |  0:00:03s
epoch 1  | loss: 0.30811 | val_0_rmse: 0.51999 | val_1_rmse: 0.52351 |  0:00:06s
epoch 2  | loss: 0.28328 | val_0_rmse: 0.52673 | val_1_rmse: 0.52765 |  0:00:09s
epoch 3  | loss: 0.28029 | val_0_rmse: 0.50317 | val_1_rmse: 0.5064  |  0:00:13s
epoch 4  | loss: 0.2637  | val_0_rmse: 0.49029 | val_1_rmse: 0.49102 |  0:00:16s
epoch 5  | loss: 0.25858 | val_0_rmse: 0.51127 | val_1_rmse: 0.51421 |  0:00:19s
epoch 6  | loss: 0.25676 | val_0_rmse: 0.50782 | val_1_rmse: 0.51122 |  0:00:22s
epoch 7  | loss: 0.25087 | val_0_rmse: 0.47118 | val_1_rmse: 0.47288 |  0:00:26s
epoch 8  | loss: 0.24431 | val_0_rmse: 0.48101 | val_1_rmse: 0.48328 |  0:00:29s
epoch 9  | loss: 0.24313 | val_0_rmse: 0.49571 | val_1_rmse: 0.49882 |  0:00:32s
epoch 10 | loss: 0.25315 | val_0_rmse: 0.50984 | val_1_rmse: 0.51203 |  0:00:35s
epoch 11 | loss: 0.25103 | val_0_rmse: 0.48287 | val_1_rmse: 0.48351 |  0:00:38s
epoch 12 | loss: 0.24278 | val_0_rmse: 0.5087  | val_1_rmse: 0.51544 |  0:00:42s
epoch 13 | loss: 0.2629  | val_0_rmse: 0.50903 | val_1_rmse: 0.50928 |  0:00:45s
epoch 14 | loss: 0.25512 | val_0_rmse: 0.47775 | val_1_rmse: 0.48151 |  0:00:48s
epoch 15 | loss: 0.24677 | val_0_rmse: 0.48867 | val_1_rmse: 0.49316 |  0:00:51s
epoch 16 | loss: 0.24567 | val_0_rmse: 0.47521 | val_1_rmse: 0.47958 |  0:00:55s
epoch 17 | loss: 0.24134 | val_0_rmse: 0.47489 | val_1_rmse: 0.47831 |  0:00:58s
epoch 18 | loss: 0.23913 | val_0_rmse: 0.49817 | val_1_rmse: 0.50273 |  0:01:01s
epoch 19 | loss: 0.23593 | val_0_rmse: 0.47027 | val_1_rmse: 0.47281 |  0:01:04s
epoch 20 | loss: 0.23395 | val_0_rmse: 0.47494 | val_1_rmse: 0.47672 |  0:01:08s
epoch 21 | loss: 0.23182 | val_0_rmse: 0.52189 | val_1_rmse: 0.53083 |  0:01:11s
epoch 22 | loss: 0.24094 | val_0_rmse: 0.46693 | val_1_rmse: 0.47169 |  0:01:14s
epoch 23 | loss: 0.23104 | val_0_rmse: 0.45629 | val_1_rmse: 0.46003 |  0:01:17s
epoch 24 | loss: 0.2296  | val_0_rmse: 0.46841 | val_1_rmse: 0.47166 |  0:01:21s
epoch 25 | loss: 0.23209 | val_0_rmse: 0.46    | val_1_rmse: 0.46458 |  0:01:24s
epoch 26 | loss: 0.23192 | val_0_rmse: 0.46493 | val_1_rmse: 0.4702  |  0:01:27s
epoch 27 | loss: 0.22985 | val_0_rmse: 0.4565  | val_1_rmse: 0.45887 |  0:01:30s
epoch 28 | loss: 0.22386 | val_0_rmse: 0.45918 | val_1_rmse: 0.46743 |  0:01:34s
epoch 29 | loss: 0.22771 | val_0_rmse: 0.45819 | val_1_rmse: 0.46568 |  0:01:37s
epoch 30 | loss: 0.22407 | val_0_rmse: 0.45555 | val_1_rmse: 0.46383 |  0:01:40s
epoch 31 | loss: 0.22597 | val_0_rmse: 0.46206 | val_1_rmse: 0.4688  |  0:01:43s
epoch 32 | loss: 0.22871 | val_0_rmse: 0.45655 | val_1_rmse: 0.46438 |  0:01:46s
epoch 33 | loss: 0.22453 | val_0_rmse: 0.45349 | val_1_rmse: 0.46157 |  0:01:50s
epoch 34 | loss: 0.22029 | val_0_rmse: 0.45078 | val_1_rmse: 0.45844 |  0:01:53s
epoch 35 | loss: 0.21847 | val_0_rmse: 0.44639 | val_1_rmse: 0.45451 |  0:01:56s
epoch 36 | loss: 0.21702 | val_0_rmse: 0.45018 | val_1_rmse: 0.45865 |  0:01:59s
epoch 37 | loss: 0.22094 | val_0_rmse: 0.45438 | val_1_rmse: 0.46065 |  0:02:03s
epoch 38 | loss: 0.21914 | val_0_rmse: 0.44861 | val_1_rmse: 0.45834 |  0:02:06s
epoch 39 | loss: 0.21709 | val_0_rmse: 0.44836 | val_1_rmse: 0.45526 |  0:02:09s
epoch 40 | loss: 0.21521 | val_0_rmse: 0.44958 | val_1_rmse: 0.4591  |  0:02:12s
epoch 41 | loss: 0.2164  | val_0_rmse: 0.45382 | val_1_rmse: 0.45952 |  0:02:16s
epoch 42 | loss: 0.21566 | val_0_rmse: 0.44328 | val_1_rmse: 0.45185 |  0:02:19s
epoch 43 | loss: 0.21805 | val_0_rmse: 0.44916 | val_1_rmse: 0.45732 |  0:02:22s
epoch 44 | loss: 0.21541 | val_0_rmse: 0.44091 | val_1_rmse: 0.45027 |  0:02:25s
epoch 45 | loss: 0.21044 | val_0_rmse: 0.46687 | val_1_rmse: 0.47693 |  0:02:29s
epoch 46 | loss: 0.21201 | val_0_rmse: 0.44844 | val_1_rmse: 0.45743 |  0:02:32s
epoch 47 | loss: 0.21032 | val_0_rmse: 0.45313 | val_1_rmse: 0.46437 |  0:02:35s
epoch 48 | loss: 0.2115  | val_0_rmse: 0.44894 | val_1_rmse: 0.45576 |  0:02:38s
epoch 49 | loss: 0.21129 | val_0_rmse: 0.44687 | val_1_rmse: 0.45611 |  0:02:42s
epoch 50 | loss: 0.21127 | val_0_rmse: 0.43883 | val_1_rmse: 0.44559 |  0:02:45s
epoch 51 | loss: 0.2152  | val_0_rmse: 0.44997 | val_1_rmse: 0.45824 |  0:02:48s
epoch 52 | loss: 0.21401 | val_0_rmse: 0.45534 | val_1_rmse: 0.45975 |  0:02:51s
epoch 53 | loss: 0.20953 | val_0_rmse: 0.44334 | val_1_rmse: 0.45285 |  0:02:55s
epoch 54 | loss: 0.2118  | val_0_rmse: 0.44224 | val_1_rmse: 0.44785 |  0:02:58s
epoch 55 | loss: 0.21645 | val_0_rmse: 0.45512 | val_1_rmse: 0.46507 |  0:03:01s
epoch 56 | loss: 0.21192 | val_0_rmse: 0.44875 | val_1_rmse: 0.45505 |  0:03:04s
epoch 57 | loss: 0.21004 | val_0_rmse: 0.45315 | val_1_rmse: 0.46234 |  0:03:07s
epoch 58 | loss: 0.21238 | val_0_rmse: 0.46049 | val_1_rmse: 0.46586 |  0:03:11s
epoch 59 | loss: 0.20952 | val_0_rmse: 0.44701 | val_1_rmse: 0.45409 |  0:03:14s
epoch 60 | loss: 0.20933 | val_0_rmse: 0.43946 | val_1_rmse: 0.44924 |  0:03:17s
epoch 61 | loss: 0.21008 | val_0_rmse: 0.44668 | val_1_rmse: 0.45359 |  0:03:20s
epoch 62 | loss: 0.20849 | val_0_rmse: 0.44425 | val_1_rmse: 0.45239 |  0:03:24s
epoch 63 | loss: 0.20635 | val_0_rmse: 0.43742 | val_1_rmse: 0.44392 |  0:03:27s
epoch 64 | loss: 0.20705 | val_0_rmse: 0.45093 | val_1_rmse: 0.46148 |  0:03:30s
epoch 65 | loss: 0.20309 | val_0_rmse: 0.43124 | val_1_rmse: 0.43853 |  0:03:33s
epoch 66 | loss: 0.20349 | val_0_rmse: 0.44633 | val_1_rmse: 0.45595 |  0:03:37s
epoch 67 | loss: 0.20493 | val_0_rmse: 0.43472 | val_1_rmse: 0.44517 |  0:03:40s
epoch 68 | loss: 0.20826 | val_0_rmse: 0.44214 | val_1_rmse: 0.44973 |  0:03:43s
epoch 69 | loss: 0.21109 | val_0_rmse: 0.43469 | val_1_rmse: 0.44296 |  0:03:46s
epoch 70 | loss: 0.20955 | val_0_rmse: 0.44349 | val_1_rmse: 0.4492  |  0:03:50s
epoch 71 | loss: 0.21412 | val_0_rmse: 0.44615 | val_1_rmse: 0.4548  |  0:03:53s
epoch 72 | loss: 0.20609 | val_0_rmse: 0.43101 | val_1_rmse: 0.44073 |  0:03:56s
epoch 73 | loss: 0.2053  | val_0_rmse: 0.44235 | val_1_rmse: 0.45405 |  0:04:00s
epoch 74 | loss: 0.20216 | val_0_rmse: 0.43634 | val_1_rmse: 0.44489 |  0:04:03s
epoch 75 | loss: 0.20211 | val_0_rmse: 0.44871 | val_1_rmse: 0.45753 |  0:04:06s
epoch 76 | loss: 0.20575 | val_0_rmse: 0.43988 | val_1_rmse: 0.44954 |  0:04:09s
epoch 77 | loss: 0.20286 | val_0_rmse: 0.43798 | val_1_rmse: 0.44745 |  0:04:12s
epoch 78 | loss: 0.20386 | val_0_rmse: 0.4594  | val_1_rmse: 0.4681  |  0:04:16s
epoch 79 | loss: 0.20578 | val_0_rmse: 0.45641 | val_1_rmse: 0.46748 |  0:04:19s
epoch 80 | loss: 0.20271 | val_0_rmse: 0.42945 | val_1_rmse: 0.44062 |  0:04:22s
epoch 81 | loss: 0.20169 | val_0_rmse: 0.44543 | val_1_rmse: 0.45364 |  0:04:25s
epoch 82 | loss: 0.2055  | val_0_rmse: 0.44934 | val_1_rmse: 0.45866 |  0:04:29s
epoch 83 | loss: 0.20437 | val_0_rmse: 0.42621 | val_1_rmse: 0.43657 |  0:04:32s
epoch 84 | loss: 0.20129 | val_0_rmse: 0.43641 | val_1_rmse: 0.44338 |  0:04:35s
epoch 85 | loss: 0.22129 | val_0_rmse: 0.4526  | val_1_rmse: 0.45876 |  0:04:38s
epoch 86 | loss: 0.20903 | val_0_rmse: 0.44066 | val_1_rmse: 0.45051 |  0:04:42s
epoch 87 | loss: 0.2163  | val_0_rmse: 0.46149 | val_1_rmse: 0.46486 |  0:04:45s
epoch 88 | loss: 0.21299 | val_0_rmse: 0.4469  | val_1_rmse: 0.45326 |  0:04:48s
epoch 89 | loss: 0.21248 | val_0_rmse: 0.44397 | val_1_rmse: 0.45058 |  0:04:51s
epoch 90 | loss: 0.21055 | val_0_rmse: 0.43958 | val_1_rmse: 0.44676 |  0:04:55s
epoch 91 | loss: 0.20597 | val_0_rmse: 0.45328 | val_1_rmse: 0.46109 |  0:04:58s
epoch 92 | loss: 0.20531 | val_0_rmse: 0.4326  | val_1_rmse: 0.44149 |  0:05:01s
epoch 93 | loss: 0.205   | val_0_rmse: 0.45283 | val_1_rmse: 0.45978 |  0:05:04s
epoch 94 | loss: 0.20608 | val_0_rmse: 0.43082 | val_1_rmse: 0.4409  |  0:05:08s
epoch 95 | loss: 0.20276 | val_0_rmse: 0.44695 | val_1_rmse: 0.45342 |  0:05:11s
epoch 96 | loss: 0.20238 | val_0_rmse: 0.43159 | val_1_rmse: 0.44205 |  0:05:14s
epoch 97 | loss: 0.20331 | val_0_rmse: 0.42847 | val_1_rmse: 0.43916 |  0:05:17s
epoch 98 | loss: 0.20299 | val_0_rmse: 0.43951 | val_1_rmse: 0.44488 |  0:05:21s
epoch 99 | loss: 0.20308 | val_0_rmse: 0.43082 | val_1_rmse: 0.44108 |  0:05:24s
epoch 100| loss: 0.20257 | val_0_rmse: 0.43637 | val_1_rmse: 0.4436  |  0:05:27s
epoch 101| loss: 0.20436 | val_0_rmse: 0.42887 | val_1_rmse: 0.4372  |  0:05:30s
epoch 102| loss: 0.20011 | val_0_rmse: 0.44384 | val_1_rmse: 0.45369 |  0:05:34s
epoch 103| loss: 0.20643 | val_0_rmse: 0.46283 | val_1_rmse: 0.46796 |  0:05:37s
epoch 104| loss: 0.20735 | val_0_rmse: 0.43583 | val_1_rmse: 0.44384 |  0:05:40s
epoch 105| loss: 0.20679 | val_0_rmse: 0.43004 | val_1_rmse: 0.43978 |  0:05:43s
epoch 106| loss: 0.20247 | val_0_rmse: 0.44766 | val_1_rmse: 0.45439 |  0:05:47s
epoch 107| loss: 0.20196 | val_0_rmse: 0.43474 | val_1_rmse: 0.44346 |  0:05:50s
epoch 108| loss: 0.20307 | val_0_rmse: 0.44515 | val_1_rmse: 0.45457 |  0:05:53s
epoch 109| loss: 0.20191 | val_0_rmse: 0.44869 | val_1_rmse: 0.45726 |  0:05:56s
epoch 110| loss: 0.19916 | val_0_rmse: 0.42514 | val_1_rmse: 0.4348  |  0:05:59s
epoch 111| loss: 0.19708 | val_0_rmse: 0.44278 | val_1_rmse: 0.4541  |  0:06:03s
epoch 112| loss: 0.19855 | val_0_rmse: 0.43779 | val_1_rmse: 0.44563 |  0:06:06s
epoch 113| loss: 0.19898 | val_0_rmse: 0.42572 | val_1_rmse: 0.43636 |  0:06:09s
epoch 114| loss: 0.19842 | val_0_rmse: 0.42893 | val_1_rmse: 0.44052 |  0:06:12s
epoch 115| loss: 0.19791 | val_0_rmse: 0.42544 | val_1_rmse: 0.43584 |  0:06:16s
epoch 116| loss: 0.19669 | val_0_rmse: 0.42716 | val_1_rmse: 0.43727 |  0:06:19s
epoch 117| loss: 0.19656 | val_0_rmse: 0.43595 | val_1_rmse: 0.44405 |  0:06:22s
epoch 118| loss: 0.20116 | val_0_rmse: 0.44479 | val_1_rmse: 0.45681 |  0:06:25s
epoch 119| loss: 0.19637 | val_0_rmse: 0.43286 | val_1_rmse: 0.44307 |  0:06:29s
epoch 120| loss: 0.1955  | val_0_rmse: 0.42655 | val_1_rmse: 0.43898 |  0:06:32s
epoch 121| loss: 0.19523 | val_0_rmse: 0.42831 | val_1_rmse: 0.43982 |  0:06:35s
epoch 122| loss: 0.19728 | val_0_rmse: 0.42635 | val_1_rmse: 0.43658 |  0:06:38s
epoch 123| loss: 0.19458 | val_0_rmse: 0.43962 | val_1_rmse: 0.45055 |  0:06:42s
epoch 124| loss: 0.1998  | val_0_rmse: 0.43055 | val_1_rmse: 0.44206 |  0:06:45s
epoch 125| loss: 0.19701 | val_0_rmse: 0.4337  | val_1_rmse: 0.44191 |  0:06:48s
epoch 126| loss: 0.1967  | val_0_rmse: 0.42533 | val_1_rmse: 0.43679 |  0:06:51s
epoch 127| loss: 0.19457 | val_0_rmse: 0.45043 | val_1_rmse: 0.45975 |  0:06:55s
epoch 128| loss: 0.1961  | val_0_rmse: 0.42382 | val_1_rmse: 0.43564 |  0:06:58s
epoch 129| loss: 0.19202 | val_0_rmse: 0.43643 | val_1_rmse: 0.44578 |  0:07:01s
epoch 130| loss: 0.19272 | val_0_rmse: 0.42265 | val_1_rmse: 0.43558 |  0:07:04s
epoch 131| loss: 0.192   | val_0_rmse: 0.42283 | val_1_rmse: 0.43366 |  0:07:07s
epoch 132| loss: 0.19169 | val_0_rmse: 0.41551 | val_1_rmse: 0.4283  |  0:07:11s
epoch 133| loss: 0.19429 | val_0_rmse: 0.42176 | val_1_rmse: 0.43256 |  0:07:14s
epoch 134| loss: 0.19116 | val_0_rmse: 0.42328 | val_1_rmse: 0.4343  |  0:07:17s
epoch 135| loss: 0.19283 | val_0_rmse: 0.42686 | val_1_rmse: 0.43658 |  0:07:20s
epoch 136| loss: 0.1948  | val_0_rmse: 0.44191 | val_1_rmse: 0.45517 |  0:07:24s
epoch 137| loss: 0.19183 | val_0_rmse: 0.42098 | val_1_rmse: 0.43542 |  0:07:27s
epoch 138| loss: 0.19125 | val_0_rmse: 0.43702 | val_1_rmse: 0.44745 |  0:07:30s
epoch 139| loss: 0.19398 | val_0_rmse: 0.42659 | val_1_rmse: 0.44034 |  0:07:33s
epoch 140| loss: 0.19316 | val_0_rmse: 0.44163 | val_1_rmse: 0.45353 |  0:07:37s
epoch 141| loss: 0.1904  | val_0_rmse: 0.41984 | val_1_rmse: 0.43113 |  0:07:40s
epoch 142| loss: 0.18863 | val_0_rmse: 0.4178  | val_1_rmse: 0.43268 |  0:07:43s
epoch 143| loss: 0.19393 | val_0_rmse: 0.41765 | val_1_rmse: 0.4302  |  0:07:46s
epoch 144| loss: 0.19168 | val_0_rmse: 0.42008 | val_1_rmse: 0.43105 |  0:07:50s
epoch 145| loss: 0.18917 | val_0_rmse: 0.42773 | val_1_rmse: 0.44116 |  0:07:53s
epoch 146| loss: 0.19246 | val_0_rmse: 0.45435 | val_1_rmse: 0.46487 |  0:07:56s
epoch 147| loss: 0.19333 | val_0_rmse: 0.41601 | val_1_rmse: 0.42947 |  0:07:59s
epoch 148| loss: 0.18565 | val_0_rmse: 0.41645 | val_1_rmse: 0.42943 |  0:08:02s
epoch 149| loss: 0.18853 | val_0_rmse: 0.4241  | val_1_rmse: 0.4386  |  0:08:06s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.4283
Best weights from best epoch are automatically used!
ended training at: 06:54:44
Feature importance:
[('Area', 0.0), ('Baths', 0.1444519751785214), ('Beds', 0.20469461768522498), ('Latitude', 0.0912953437132918), ('Longitude', 0.052125511980956024), ('Month', 0.15284965102309822), ('Year', 0.35458290041890755)]
Mean squared error is of 9945888877.804537
Mean absolute error:68521.44602200083
MAPE:0.2533388221163876
R2 score:0.8263589044318573
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:54:44
epoch 0  | loss: 0.42913 | val_0_rmse: 0.56776 | val_1_rmse: 0.56084 |  0:00:03s
epoch 1  | loss: 0.30755 | val_0_rmse: 0.52991 | val_1_rmse: 0.52736 |  0:00:06s
epoch 2  | loss: 0.28139 | val_0_rmse: 0.50963 | val_1_rmse: 0.5081  |  0:00:09s
epoch 3  | loss: 0.26536 | val_0_rmse: 0.49983 | val_1_rmse: 0.50034 |  0:00:12s
epoch 4  | loss: 0.26262 | val_0_rmse: 0.49181 | val_1_rmse: 0.49014 |  0:00:16s
epoch 5  | loss: 0.25717 | val_0_rmse: 0.47844 | val_1_rmse: 0.47943 |  0:00:19s
epoch 6  | loss: 0.25521 | val_0_rmse: 0.48172 | val_1_rmse: 0.48151 |  0:00:22s
epoch 7  | loss: 0.26174 | val_0_rmse: 0.49935 | val_1_rmse: 0.49726 |  0:00:25s
epoch 8  | loss: 0.25244 | val_0_rmse: 0.49483 | val_1_rmse: 0.49416 |  0:00:29s
epoch 9  | loss: 0.25137 | val_0_rmse: 0.48359 | val_1_rmse: 0.48287 |  0:00:32s
epoch 10 | loss: 0.2433  | val_0_rmse: 0.49354 | val_1_rmse: 0.49142 |  0:00:35s
epoch 11 | loss: 0.2394  | val_0_rmse: 0.4774  | val_1_rmse: 0.4765  |  0:00:38s
epoch 12 | loss: 0.23478 | val_0_rmse: 0.46329 | val_1_rmse: 0.46248 |  0:00:42s
epoch 13 | loss: 0.23382 | val_0_rmse: 0.46129 | val_1_rmse: 0.45754 |  0:00:45s
epoch 14 | loss: 0.23527 | val_0_rmse: 0.47247 | val_1_rmse: 0.47367 |  0:00:48s
epoch 15 | loss: 0.23231 | val_0_rmse: 0.46753 | val_1_rmse: 0.46849 |  0:00:51s
epoch 16 | loss: 0.23168 | val_0_rmse: 0.46379 | val_1_rmse: 0.46457 |  0:00:55s
epoch 17 | loss: 0.23454 | val_0_rmse: 0.48405 | val_1_rmse: 0.48096 |  0:00:58s
epoch 18 | loss: 0.23252 | val_0_rmse: 0.48233 | val_1_rmse: 0.48097 |  0:01:01s
epoch 19 | loss: 0.24948 | val_0_rmse: 0.46229 | val_1_rmse: 0.46155 |  0:01:04s
epoch 20 | loss: 0.23015 | val_0_rmse: 0.47682 | val_1_rmse: 0.47311 |  0:01:08s
epoch 21 | loss: 0.23142 | val_0_rmse: 0.45966 | val_1_rmse: 0.45848 |  0:01:11s
epoch 22 | loss: 0.22865 | val_0_rmse: 0.44989 | val_1_rmse: 0.44957 |  0:01:14s
epoch 23 | loss: 0.22841 | val_0_rmse: 0.48576 | val_1_rmse: 0.48256 |  0:01:17s
epoch 24 | loss: 0.24965 | val_0_rmse: 0.4806  | val_1_rmse: 0.48242 |  0:01:21s
epoch 25 | loss: 0.23608 | val_0_rmse: 0.47046 | val_1_rmse: 0.46919 |  0:01:24s
epoch 26 | loss: 0.23194 | val_0_rmse: 0.46122 | val_1_rmse: 0.45918 |  0:01:27s
epoch 27 | loss: 0.22681 | val_0_rmse: 0.45984 | val_1_rmse: 0.46129 |  0:01:30s
epoch 28 | loss: 0.22247 | val_0_rmse: 0.46703 | val_1_rmse: 0.46754 |  0:01:34s
epoch 29 | loss: 0.22805 | val_0_rmse: 0.45612 | val_1_rmse: 0.45439 |  0:01:37s
epoch 30 | loss: 0.21974 | val_0_rmse: 0.467   | val_1_rmse: 0.46443 |  0:01:40s
epoch 31 | loss: 0.22251 | val_0_rmse: 0.45429 | val_1_rmse: 0.45397 |  0:01:43s
epoch 32 | loss: 0.22148 | val_0_rmse: 0.45096 | val_1_rmse: 0.45374 |  0:01:47s
epoch 33 | loss: 0.22372 | val_0_rmse: 0.45081 | val_1_rmse: 0.45058 |  0:01:50s
epoch 34 | loss: 0.22287 | val_0_rmse: 0.47321 | val_1_rmse: 0.47272 |  0:01:53s
epoch 35 | loss: 0.22646 | val_0_rmse: 0.45786 | val_1_rmse: 0.45944 |  0:01:56s
epoch 36 | loss: 0.22265 | val_0_rmse: 0.46275 | val_1_rmse: 0.45953 |  0:01:59s
epoch 37 | loss: 0.2247  | val_0_rmse: 0.45329 | val_1_rmse: 0.45305 |  0:02:03s
epoch 38 | loss: 0.22003 | val_0_rmse: 0.47517 | val_1_rmse: 0.47431 |  0:02:06s
epoch 39 | loss: 0.22701 | val_0_rmse: 0.45311 | val_1_rmse: 0.4515  |  0:02:09s
epoch 40 | loss: 0.21586 | val_0_rmse: 0.44672 | val_1_rmse: 0.44697 |  0:02:12s
epoch 41 | loss: 0.21414 | val_0_rmse: 0.45341 | val_1_rmse: 0.45665 |  0:02:16s
epoch 42 | loss: 0.22056 | val_0_rmse: 0.44588 | val_1_rmse: 0.44675 |  0:02:19s
epoch 43 | loss: 0.2187  | val_0_rmse: 0.44576 | val_1_rmse: 0.44572 |  0:02:22s
epoch 44 | loss: 0.22366 | val_0_rmse: 0.47822 | val_1_rmse: 0.48091 |  0:02:25s
epoch 45 | loss: 0.23596 | val_0_rmse: 0.47212 | val_1_rmse: 0.46962 |  0:02:29s
epoch 46 | loss: 0.22506 | val_0_rmse: 0.45671 | val_1_rmse: 0.45589 |  0:02:32s
epoch 47 | loss: 0.24563 | val_0_rmse: 0.51765 | val_1_rmse: 0.51791 |  0:02:35s
epoch 48 | loss: 0.23992 | val_0_rmse: 0.46842 | val_1_rmse: 0.46842 |  0:02:38s
epoch 49 | loss: 0.2208  | val_0_rmse: 0.45468 | val_1_rmse: 0.4538  |  0:02:42s
epoch 50 | loss: 0.22148 | val_0_rmse: 0.45249 | val_1_rmse: 0.45018 |  0:02:45s
epoch 51 | loss: 0.22084 | val_0_rmse: 0.46166 | val_1_rmse: 0.46257 |  0:02:48s
epoch 52 | loss: 0.21791 | val_0_rmse: 0.47071 | val_1_rmse: 0.46707 |  0:02:51s
epoch 53 | loss: 0.22016 | val_0_rmse: 0.47063 | val_1_rmse: 0.47058 |  0:02:55s
epoch 54 | loss: 0.22883 | val_0_rmse: 0.45804 | val_1_rmse: 0.45615 |  0:02:58s
epoch 55 | loss: 0.22308 | val_0_rmse: 0.47662 | val_1_rmse: 0.47471 |  0:03:01s
epoch 56 | loss: 0.22834 | val_0_rmse: 0.46858 | val_1_rmse: 0.47026 |  0:03:04s
epoch 57 | loss: 0.23319 | val_0_rmse: 0.47271 | val_1_rmse: 0.46895 |  0:03:07s
epoch 58 | loss: 0.21812 | val_0_rmse: 0.45346 | val_1_rmse: 0.45494 |  0:03:11s
epoch 59 | loss: 0.2158  | val_0_rmse: 0.46443 | val_1_rmse: 0.46633 |  0:03:14s
epoch 60 | loss: 0.21558 | val_0_rmse: 0.45526 | val_1_rmse: 0.45554 |  0:03:17s
epoch 61 | loss: 0.21842 | val_0_rmse: 0.46315 | val_1_rmse: 0.46293 |  0:03:20s
epoch 62 | loss: 0.22209 | val_0_rmse: 0.46407 | val_1_rmse: 0.46344 |  0:03:24s
epoch 63 | loss: 0.22352 | val_0_rmse: 0.45218 | val_1_rmse: 0.45059 |  0:03:27s
epoch 64 | loss: 0.22259 | val_0_rmse: 0.50434 | val_1_rmse: 0.5079  |  0:03:30s
epoch 65 | loss: 0.22182 | val_0_rmse: 0.44679 | val_1_rmse: 0.44606 |  0:03:33s
epoch 66 | loss: 0.21656 | val_0_rmse: 0.45658 | val_1_rmse: 0.45813 |  0:03:37s
epoch 67 | loss: 0.2184  | val_0_rmse: 0.45935 | val_1_rmse: 0.4598  |  0:03:40s
epoch 68 | loss: 0.21053 | val_0_rmse: 0.44488 | val_1_rmse: 0.44704 |  0:03:43s
epoch 69 | loss: 0.22051 | val_0_rmse: 0.47621 | val_1_rmse: 0.47173 |  0:03:46s
epoch 70 | loss: 0.23135 | val_0_rmse: 0.45961 | val_1_rmse: 0.46082 |  0:03:50s
epoch 71 | loss: 0.22494 | val_0_rmse: 0.45299 | val_1_rmse: 0.45337 |  0:03:53s
epoch 72 | loss: 0.21755 | val_0_rmse: 0.45384 | val_1_rmse: 0.45503 |  0:03:56s
epoch 73 | loss: 0.21532 | val_0_rmse: 0.44867 | val_1_rmse: 0.44836 |  0:03:59s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.44572
Best weights from best epoch are automatically used!
ended training at: 06:58:45
Feature importance:
[('Area', 0.11671592378414646), ('Baths', 0.0804765434294889), ('Beds', 0.13776245266200876), ('Latitude', 0.0785399332065418), ('Longitude', 0.14752732806168103), ('Month', 0.16568423886022007), ('Year', 0.27329357999591297)]
Mean squared error is of 11307139161.180084
Mean absolute error:73581.56413487035
MAPE:0.2705477846336506
R2 score:0.8037375898922864
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:58:45
epoch 0  | loss: 0.46198 | val_0_rmse: 0.55262 | val_1_rmse: 0.54954 |  0:00:03s
epoch 1  | loss: 0.28714 | val_0_rmse: 0.50629 | val_1_rmse: 0.51222 |  0:00:06s
epoch 2  | loss: 0.26086 | val_0_rmse: 0.49934 | val_1_rmse: 0.50531 |  0:00:09s
epoch 3  | loss: 0.26501 | val_0_rmse: 0.5074  | val_1_rmse: 0.51315 |  0:00:12s
epoch 4  | loss: 0.25902 | val_0_rmse: 0.48266 | val_1_rmse: 0.48754 |  0:00:16s
epoch 5  | loss: 0.25781 | val_0_rmse: 0.47904 | val_1_rmse: 0.48945 |  0:00:19s
epoch 6  | loss: 0.25435 | val_0_rmse: 0.48439 | val_1_rmse: 0.49218 |  0:00:22s
epoch 7  | loss: 0.24568 | val_0_rmse: 0.47153 | val_1_rmse: 0.48015 |  0:00:25s
epoch 8  | loss: 0.24084 | val_0_rmse: 0.46582 | val_1_rmse: 0.47633 |  0:00:29s
epoch 9  | loss: 0.23954 | val_0_rmse: 0.47402 | val_1_rmse: 0.48298 |  0:00:32s
epoch 10 | loss: 0.24319 | val_0_rmse: 0.46975 | val_1_rmse: 0.47678 |  0:00:35s
epoch 11 | loss: 0.24125 | val_0_rmse: 0.47853 | val_1_rmse: 0.48834 |  0:00:38s
epoch 12 | loss: 0.24073 | val_0_rmse: 0.46804 | val_1_rmse: 0.47679 |  0:00:42s
epoch 13 | loss: 0.23747 | val_0_rmse: 0.47852 | val_1_rmse: 0.48717 |  0:00:45s
epoch 14 | loss: 0.23424 | val_0_rmse: 0.46472 | val_1_rmse: 0.47571 |  0:00:48s
epoch 15 | loss: 0.23195 | val_0_rmse: 0.47356 | val_1_rmse: 0.47972 |  0:00:51s
epoch 16 | loss: 0.23038 | val_0_rmse: 0.46577 | val_1_rmse: 0.47547 |  0:00:55s
epoch 17 | loss: 0.23404 | val_0_rmse: 0.4603  | val_1_rmse: 0.46856 |  0:00:58s
epoch 18 | loss: 0.22652 | val_0_rmse: 0.45765 | val_1_rmse: 0.465   |  0:01:01s
epoch 19 | loss: 0.22676 | val_0_rmse: 0.47139 | val_1_rmse: 0.47992 |  0:01:04s
epoch 20 | loss: 0.22652 | val_0_rmse: 0.45109 | val_1_rmse: 0.46333 |  0:01:08s
epoch 21 | loss: 0.22197 | val_0_rmse: 0.44695 | val_1_rmse: 0.4554  |  0:01:11s
epoch 22 | loss: 0.23487 | val_0_rmse: 0.47583 | val_1_rmse: 0.48552 |  0:01:14s
epoch 23 | loss: 0.22764 | val_0_rmse: 0.45509 | val_1_rmse: 0.46375 |  0:01:17s
epoch 24 | loss: 0.22411 | val_0_rmse: 0.45316 | val_1_rmse: 0.46294 |  0:01:21s
epoch 25 | loss: 0.22364 | val_0_rmse: 0.45229 | val_1_rmse: 0.46126 |  0:01:24s
epoch 26 | loss: 0.22601 | val_0_rmse: 0.4614  | val_1_rmse: 0.471   |  0:01:27s
epoch 27 | loss: 0.22358 | val_0_rmse: 0.44824 | val_1_rmse: 0.45937 |  0:01:30s
epoch 28 | loss: 0.2173  | val_0_rmse: 0.45233 | val_1_rmse: 0.46159 |  0:01:33s
epoch 29 | loss: 0.22578 | val_0_rmse: 0.46034 | val_1_rmse: 0.47189 |  0:01:37s
epoch 30 | loss: 0.21799 | val_0_rmse: 0.4493  | val_1_rmse: 0.46014 |  0:01:40s
epoch 31 | loss: 0.22015 | val_0_rmse: 0.46687 | val_1_rmse: 0.47579 |  0:01:43s
epoch 32 | loss: 0.21657 | val_0_rmse: 0.45229 | val_1_rmse: 0.4604  |  0:01:46s
epoch 33 | loss: 0.21694 | val_0_rmse: 0.45041 | val_1_rmse: 0.46198 |  0:01:50s
epoch 34 | loss: 0.217   | val_0_rmse: 0.45575 | val_1_rmse: 0.46444 |  0:01:53s
epoch 35 | loss: 0.22081 | val_0_rmse: 0.45234 | val_1_rmse: 0.46202 |  0:01:56s
epoch 36 | loss: 0.22013 | val_0_rmse: 0.45248 | val_1_rmse: 0.4663  |  0:01:59s
epoch 37 | loss: 0.21662 | val_0_rmse: 0.46108 | val_1_rmse: 0.47367 |  0:02:03s
epoch 38 | loss: 0.21748 | val_0_rmse: 0.45933 | val_1_rmse: 0.46933 |  0:02:06s
epoch 39 | loss: 0.22059 | val_0_rmse: 0.45207 | val_1_rmse: 0.46151 |  0:02:09s
epoch 40 | loss: 0.22749 | val_0_rmse: 0.45782 | val_1_rmse: 0.46666 |  0:02:12s
epoch 41 | loss: 0.21782 | val_0_rmse: 0.45158 | val_1_rmse: 0.46025 |  0:02:15s
epoch 42 | loss: 0.21486 | val_0_rmse: 0.45106 | val_1_rmse: 0.46035 |  0:02:19s
epoch 43 | loss: 0.21852 | val_0_rmse: 0.44783 | val_1_rmse: 0.45778 |  0:02:22s
epoch 44 | loss: 0.21554 | val_0_rmse: 0.44179 | val_1_rmse: 0.45151 |  0:02:25s
epoch 45 | loss: 0.21245 | val_0_rmse: 0.46848 | val_1_rmse: 0.47868 |  0:02:28s
epoch 46 | loss: 0.21561 | val_0_rmse: 0.45571 | val_1_rmse: 0.46484 |  0:02:32s
epoch 47 | loss: 0.2106  | val_0_rmse: 0.45866 | val_1_rmse: 0.46856 |  0:02:35s
epoch 48 | loss: 0.21568 | val_0_rmse: 0.44489 | val_1_rmse: 0.45675 |  0:02:38s
epoch 49 | loss: 0.21158 | val_0_rmse: 0.43839 | val_1_rmse: 0.44978 |  0:02:41s
epoch 50 | loss: 0.21267 | val_0_rmse: 0.45746 | val_1_rmse: 0.46537 |  0:02:45s
epoch 51 | loss: 0.21405 | val_0_rmse: 0.45375 | val_1_rmse: 0.46467 |  0:02:48s
epoch 52 | loss: 0.21472 | val_0_rmse: 0.43998 | val_1_rmse: 0.45047 |  0:02:51s
epoch 53 | loss: 0.21387 | val_0_rmse: 0.46422 | val_1_rmse: 0.47272 |  0:02:54s
epoch 54 | loss: 0.21445 | val_0_rmse: 0.43811 | val_1_rmse: 0.44959 |  0:02:58s
epoch 55 | loss: 0.21037 | val_0_rmse: 0.43825 | val_1_rmse: 0.45096 |  0:03:01s
epoch 56 | loss: 0.21484 | val_0_rmse: 0.4347  | val_1_rmse: 0.44835 |  0:03:04s
epoch 57 | loss: 0.20983 | val_0_rmse: 0.44345 | val_1_rmse: 0.45509 |  0:03:07s
epoch 58 | loss: 0.20739 | val_0_rmse: 0.4433  | val_1_rmse: 0.45144 |  0:03:11s
epoch 59 | loss: 0.20612 | val_0_rmse: 0.45676 | val_1_rmse: 0.46627 |  0:03:14s
epoch 60 | loss: 0.20865 | val_0_rmse: 0.44999 | val_1_rmse: 0.45983 |  0:03:17s
epoch 61 | loss: 0.21338 | val_0_rmse: 0.44069 | val_1_rmse: 0.45265 |  0:03:20s
epoch 62 | loss: 0.21472 | val_0_rmse: 0.47856 | val_1_rmse: 0.48863 |  0:03:23s
epoch 63 | loss: 0.21433 | val_0_rmse: 0.44525 | val_1_rmse: 0.45766 |  0:03:27s
epoch 64 | loss: 0.21119 | val_0_rmse: 0.45834 | val_1_rmse: 0.47025 |  0:03:30s
epoch 65 | loss: 0.22035 | val_0_rmse: 0.44413 | val_1_rmse: 0.45437 |  0:03:33s
epoch 66 | loss: 0.20812 | val_0_rmse: 0.43982 | val_1_rmse: 0.45099 |  0:03:36s
epoch 67 | loss: 0.20406 | val_0_rmse: 0.44712 | val_1_rmse: 0.46121 |  0:03:40s
epoch 68 | loss: 0.20618 | val_0_rmse: 0.44074 | val_1_rmse: 0.45243 |  0:03:43s
epoch 69 | loss: 0.21262 | val_0_rmse: 0.47097 | val_1_rmse: 0.47762 |  0:03:46s
epoch 70 | loss: 0.21896 | val_0_rmse: 0.45001 | val_1_rmse: 0.4601  |  0:03:49s
epoch 71 | loss: 0.21138 | val_0_rmse: 0.44098 | val_1_rmse: 0.45381 |  0:03:53s
epoch 72 | loss: 0.20654 | val_0_rmse: 0.44385 | val_1_rmse: 0.45624 |  0:03:56s
epoch 73 | loss: 0.20627 | val_0_rmse: 0.45338 | val_1_rmse: 0.46522 |  0:03:59s
epoch 74 | loss: 0.20221 | val_0_rmse: 0.4325  | val_1_rmse: 0.44599 |  0:04:02s
epoch 75 | loss: 0.2029  | val_0_rmse: 0.44093 | val_1_rmse: 0.45308 |  0:04:06s
epoch 76 | loss: 0.20682 | val_0_rmse: 0.44939 | val_1_rmse: 0.46075 |  0:04:09s
epoch 77 | loss: 0.20738 | val_0_rmse: 0.43359 | val_1_rmse: 0.44576 |  0:04:12s
epoch 78 | loss: 0.20498 | val_0_rmse: 0.44327 | val_1_rmse: 0.45376 |  0:04:15s
epoch 79 | loss: 0.204   | val_0_rmse: 0.4492  | val_1_rmse: 0.46053 |  0:04:18s
epoch 80 | loss: 0.2067  | val_0_rmse: 0.44387 | val_1_rmse: 0.45492 |  0:04:22s
epoch 81 | loss: 0.20529 | val_0_rmse: 0.4335  | val_1_rmse: 0.44785 |  0:04:25s
epoch 82 | loss: 0.20242 | val_0_rmse: 0.43292 | val_1_rmse: 0.44612 |  0:04:28s
epoch 83 | loss: 0.20495 | val_0_rmse: 0.4451  | val_1_rmse: 0.45486 |  0:04:31s
epoch 84 | loss: 0.21253 | val_0_rmse: 0.44511 | val_1_rmse: 0.45587 |  0:04:35s
epoch 85 | loss: 0.20392 | val_0_rmse: 0.46312 | val_1_rmse: 0.47226 |  0:04:38s
epoch 86 | loss: 0.20654 | val_0_rmse: 0.4378  | val_1_rmse: 0.45006 |  0:04:41s
epoch 87 | loss: 0.20885 | val_0_rmse: 0.44562 | val_1_rmse: 0.45799 |  0:04:44s
epoch 88 | loss: 0.20338 | val_0_rmse: 0.43295 | val_1_rmse: 0.44629 |  0:04:48s
epoch 89 | loss: 0.20431 | val_0_rmse: 0.43598 | val_1_rmse: 0.44857 |  0:04:51s
epoch 90 | loss: 0.20173 | val_0_rmse: 0.43117 | val_1_rmse: 0.44425 |  0:04:54s
epoch 91 | loss: 0.20244 | val_0_rmse: 0.44008 | val_1_rmse: 0.45205 |  0:04:57s
epoch 92 | loss: 0.20343 | val_0_rmse: 0.45287 | val_1_rmse: 0.4682  |  0:05:01s
epoch 93 | loss: 0.20292 | val_0_rmse: 0.43571 | val_1_rmse: 0.45014 |  0:05:04s
epoch 94 | loss: 0.2016  | val_0_rmse: 0.4311  | val_1_rmse: 0.44606 |  0:05:07s
epoch 95 | loss: 0.20232 | val_0_rmse: 0.44077 | val_1_rmse: 0.45522 |  0:05:10s
epoch 96 | loss: 0.20564 | val_0_rmse: 0.44315 | val_1_rmse: 0.45653 |  0:05:14s
epoch 97 | loss: 0.20413 | val_0_rmse: 0.44071 | val_1_rmse: 0.45381 |  0:05:17s
epoch 98 | loss: 0.20341 | val_0_rmse: 0.43437 | val_1_rmse: 0.44801 |  0:05:20s
epoch 99 | loss: 0.20561 | val_0_rmse: 0.44182 | val_1_rmse: 0.45607 |  0:05:23s
epoch 100| loss: 0.20613 | val_0_rmse: 0.43112 | val_1_rmse: 0.44501 |  0:05:26s
epoch 101| loss: 0.20136 | val_0_rmse: 0.43911 | val_1_rmse: 0.45448 |  0:05:30s
epoch 102| loss: 0.21235 | val_0_rmse: 0.45393 | val_1_rmse: 0.46548 |  0:05:33s
epoch 103| loss: 0.20772 | val_0_rmse: 0.43777 | val_1_rmse: 0.45061 |  0:05:36s
epoch 104| loss: 0.20471 | val_0_rmse: 0.43426 | val_1_rmse: 0.44628 |  0:05:39s
epoch 105| loss: 0.20448 | val_0_rmse: 0.43096 | val_1_rmse: 0.44378 |  0:05:43s
epoch 106| loss: 0.20269 | val_0_rmse: 0.43075 | val_1_rmse: 0.44854 |  0:05:46s
epoch 107| loss: 0.20195 | val_0_rmse: 0.42858 | val_1_rmse: 0.4448  |  0:05:49s
epoch 108| loss: 0.19946 | val_0_rmse: 0.43058 | val_1_rmse: 0.4471  |  0:05:52s
epoch 109| loss: 0.19997 | val_0_rmse: 0.43942 | val_1_rmse: 0.45353 |  0:05:56s
epoch 110| loss: 0.20514 | val_0_rmse: 0.4271  | val_1_rmse: 0.44193 |  0:05:59s
epoch 111| loss: 0.19963 | val_0_rmse: 0.43215 | val_1_rmse: 0.446   |  0:06:02s
epoch 112| loss: 0.20622 | val_0_rmse: 0.43245 | val_1_rmse: 0.44536 |  0:06:05s
epoch 113| loss: 0.20574 | val_0_rmse: 0.4493  | val_1_rmse: 0.46238 |  0:06:08s
epoch 114| loss: 0.20195 | val_0_rmse: 0.43793 | val_1_rmse: 0.45122 |  0:06:12s
epoch 115| loss: 0.20404 | val_0_rmse: 0.43981 | val_1_rmse: 0.45439 |  0:06:15s
epoch 116| loss: 0.19755 | val_0_rmse: 0.428   | val_1_rmse: 0.44145 |  0:06:18s
epoch 117| loss: 0.19992 | val_0_rmse: 0.43933 | val_1_rmse: 0.45271 |  0:06:21s
epoch 118| loss: 0.20273 | val_0_rmse: 0.42929 | val_1_rmse: 0.44539 |  0:06:25s
epoch 119| loss: 0.19683 | val_0_rmse: 0.43452 | val_1_rmse: 0.4492  |  0:06:28s
epoch 120| loss: 0.20199 | val_0_rmse: 0.44252 | val_1_rmse: 0.45334 |  0:06:31s
epoch 121| loss: 0.19714 | val_0_rmse: 0.42933 | val_1_rmse: 0.44614 |  0:06:34s
epoch 122| loss: 0.20429 | val_0_rmse: 0.43606 | val_1_rmse: 0.4512  |  0:06:38s
epoch 123| loss: 0.19864 | val_0_rmse: 0.42815 | val_1_rmse: 0.44222 |  0:06:41s
epoch 124| loss: 0.19798 | val_0_rmse: 0.42603 | val_1_rmse: 0.44214 |  0:06:44s
epoch 125| loss: 0.2007  | val_0_rmse: 0.42576 | val_1_rmse: 0.44165 |  0:06:47s
epoch 126| loss: 0.19943 | val_0_rmse: 0.43348 | val_1_rmse: 0.44831 |  0:06:51s
epoch 127| loss: 0.19998 | val_0_rmse: 0.42595 | val_1_rmse: 0.44196 |  0:06:54s
epoch 128| loss: 0.19667 | val_0_rmse: 0.43599 | val_1_rmse: 0.45171 |  0:06:57s
epoch 129| loss: 0.20253 | val_0_rmse: 0.43269 | val_1_rmse: 0.44966 |  0:07:00s
epoch 130| loss: 0.20187 | val_0_rmse: 0.42839 | val_1_rmse: 0.44115 |  0:07:04s
epoch 131| loss: 0.20341 | val_0_rmse: 0.43396 | val_1_rmse: 0.44918 |  0:07:07s
epoch 132| loss: 0.20011 | val_0_rmse: 0.43533 | val_1_rmse: 0.44927 |  0:07:10s
epoch 133| loss: 0.19966 | val_0_rmse: 0.43137 | val_1_rmse: 0.44709 |  0:07:13s
epoch 134| loss: 0.20061 | val_0_rmse: 0.42824 | val_1_rmse: 0.44532 |  0:07:17s
epoch 135| loss: 0.20607 | val_0_rmse: 0.43811 | val_1_rmse: 0.45332 |  0:07:20s
epoch 136| loss: 0.20028 | val_0_rmse: 0.44756 | val_1_rmse: 0.46055 |  0:07:23s
epoch 137| loss: 0.19882 | val_0_rmse: 0.43685 | val_1_rmse: 0.44959 |  0:07:26s
epoch 138| loss: 0.20093 | val_0_rmse: 0.44116 | val_1_rmse: 0.45535 |  0:07:29s
epoch 139| loss: 0.20524 | val_0_rmse: 0.44727 | val_1_rmse: 0.46293 |  0:07:33s
epoch 140| loss: 0.20958 | val_0_rmse: 0.44072 | val_1_rmse: 0.45687 |  0:07:36s
epoch 141| loss: 0.20423 | val_0_rmse: 0.44229 | val_1_rmse: 0.45884 |  0:07:39s
epoch 142| loss: 0.20559 | val_0_rmse: 0.44056 | val_1_rmse: 0.45502 |  0:07:43s
epoch 143| loss: 0.23758 | val_0_rmse: 0.47892 | val_1_rmse: 0.48687 |  0:07:46s
epoch 144| loss: 0.23429 | val_0_rmse: 0.47211 | val_1_rmse: 0.47676 |  0:07:49s
epoch 145| loss: 0.21832 | val_0_rmse: 0.44903 | val_1_rmse: 0.4589  |  0:07:52s
epoch 146| loss: 0.24342 | val_0_rmse: 0.50098 | val_1_rmse: 0.50503 |  0:07:56s
epoch 147| loss: 0.24658 | val_0_rmse: 0.47554 | val_1_rmse: 0.48258 |  0:07:59s
epoch 148| loss: 0.23662 | val_0_rmse: 0.46304 | val_1_rmse: 0.46758 |  0:08:02s
epoch 149| loss: 0.22276 | val_0_rmse: 0.45281 | val_1_rmse: 0.46096 |  0:08:05s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.44115
Best weights from best epoch are automatically used!
ended training at: 07:06:52
Feature importance:
[('Area', 0.06204675165010902), ('Baths', 0.14694157250581866), ('Beds', 0.09314168675395053), ('Latitude', 0.1807830828247667), ('Longitude', 0.15236789506166185), ('Month', 0.0991057220844223), ('Year', 0.26561328911927096)]
Mean squared error is of 10905576936.51491
Mean absolute error:71897.58442504353
MAPE:0.26818922921274135
R2 score:0.8143133859471623
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:06:52
epoch 0  | loss: 0.45209 | val_0_rmse: 0.5725  | val_1_rmse: 0.56952 |  0:00:03s
epoch 1  | loss: 0.32052 | val_0_rmse: 0.5304  | val_1_rmse: 0.53416 |  0:00:06s
epoch 2  | loss: 0.28197 | val_0_rmse: 0.51586 | val_1_rmse: 0.51738 |  0:00:09s
epoch 3  | loss: 0.2645  | val_0_rmse: 0.48684 | val_1_rmse: 0.49107 |  0:00:12s
epoch 4  | loss: 0.25348 | val_0_rmse: 0.48232 | val_1_rmse: 0.48502 |  0:00:16s
epoch 5  | loss: 0.24405 | val_0_rmse: 0.47434 | val_1_rmse: 0.48002 |  0:00:19s
epoch 6  | loss: 0.24248 | val_0_rmse: 0.46821 | val_1_rmse: 0.47196 |  0:00:22s
epoch 7  | loss: 0.23742 | val_0_rmse: 0.47455 | val_1_rmse: 0.47879 |  0:00:25s
epoch 8  | loss: 0.23606 | val_0_rmse: 0.47803 | val_1_rmse: 0.48186 |  0:00:29s
epoch 9  | loss: 0.2391  | val_0_rmse: 0.4648  | val_1_rmse: 0.47054 |  0:00:32s
epoch 10 | loss: 0.2337  | val_0_rmse: 0.45777 | val_1_rmse: 0.46183 |  0:00:35s
epoch 11 | loss: 0.23378 | val_0_rmse: 0.4646  | val_1_rmse: 0.4693  |  0:00:38s
epoch 12 | loss: 0.23004 | val_0_rmse: 0.45972 | val_1_rmse: 0.46486 |  0:00:42s
epoch 13 | loss: 0.22667 | val_0_rmse: 0.45494 | val_1_rmse: 0.46128 |  0:00:45s
epoch 14 | loss: 0.22447 | val_0_rmse: 0.45501 | val_1_rmse: 0.46117 |  0:00:48s
epoch 15 | loss: 0.2215  | val_0_rmse: 0.45404 | val_1_rmse: 0.46074 |  0:00:51s
epoch 16 | loss: 0.22302 | val_0_rmse: 0.45553 | val_1_rmse: 0.46152 |  0:00:55s
epoch 17 | loss: 0.22457 | val_0_rmse: 0.46896 | val_1_rmse: 0.47533 |  0:00:58s
epoch 18 | loss: 0.21655 | val_0_rmse: 0.44925 | val_1_rmse: 0.45671 |  0:01:01s
epoch 19 | loss: 0.21965 | val_0_rmse: 0.45554 | val_1_rmse: 0.46226 |  0:01:04s
epoch 20 | loss: 0.22336 | val_0_rmse: 0.46371 | val_1_rmse: 0.47062 |  0:01:08s
epoch 21 | loss: 0.21863 | val_0_rmse: 0.44915 | val_1_rmse: 0.45611 |  0:01:11s
epoch 22 | loss: 0.21241 | val_0_rmse: 0.4508  | val_1_rmse: 0.45791 |  0:01:14s
epoch 23 | loss: 0.2158  | val_0_rmse: 0.45665 | val_1_rmse: 0.46296 |  0:01:17s
epoch 24 | loss: 0.22011 | val_0_rmse: 0.45276 | val_1_rmse: 0.45996 |  0:01:20s
epoch 25 | loss: 0.21548 | val_0_rmse: 0.4471  | val_1_rmse: 0.45392 |  0:01:24s
epoch 26 | loss: 0.21287 | val_0_rmse: 0.43843 | val_1_rmse: 0.44789 |  0:01:27s
epoch 27 | loss: 0.21047 | val_0_rmse: 0.43875 | val_1_rmse: 0.44744 |  0:01:30s
epoch 28 | loss: 0.21101 | val_0_rmse: 0.45374 | val_1_rmse: 0.46247 |  0:01:33s
epoch 29 | loss: 0.21183 | val_0_rmse: 0.45867 | val_1_rmse: 0.46582 |  0:01:37s
epoch 30 | loss: 0.21102 | val_0_rmse: 0.44153 | val_1_rmse: 0.45167 |  0:01:40s
epoch 31 | loss: 0.20886 | val_0_rmse: 0.45102 | val_1_rmse: 0.45766 |  0:01:43s
epoch 32 | loss: 0.20771 | val_0_rmse: 0.44414 | val_1_rmse: 0.45367 |  0:01:46s
epoch 33 | loss: 0.2123  | val_0_rmse: 0.44389 | val_1_rmse: 0.45279 |  0:01:50s
epoch 34 | loss: 0.20945 | val_0_rmse: 0.4384  | val_1_rmse: 0.44965 |  0:01:53s
epoch 35 | loss: 0.2102  | val_0_rmse: 0.45205 | val_1_rmse: 0.46182 |  0:01:56s
epoch 36 | loss: 0.20489 | val_0_rmse: 0.43939 | val_1_rmse: 0.45047 |  0:01:59s
epoch 37 | loss: 0.21221 | val_0_rmse: 0.42954 | val_1_rmse: 0.4392  |  0:02:03s
epoch 38 | loss: 0.20526 | val_0_rmse: 0.43875 | val_1_rmse: 0.45091 |  0:02:06s
epoch 39 | loss: 0.20199 | val_0_rmse: 0.43575 | val_1_rmse: 0.4473  |  0:02:09s
epoch 40 | loss: 0.20261 | val_0_rmse: 0.43399 | val_1_rmse: 0.44418 |  0:02:12s
epoch 41 | loss: 0.20417 | val_0_rmse: 0.43191 | val_1_rmse: 0.44538 |  0:02:15s
epoch 42 | loss: 0.20275 | val_0_rmse: 0.43697 | val_1_rmse: 0.45159 |  0:02:19s
epoch 43 | loss: 0.21113 | val_0_rmse: 0.43806 | val_1_rmse: 0.44922 |  0:02:22s
epoch 44 | loss: 0.20347 | val_0_rmse: 0.42731 | val_1_rmse: 0.4379  |  0:02:25s
epoch 45 | loss: 0.2053  | val_0_rmse: 0.44145 | val_1_rmse: 0.45049 |  0:02:28s
epoch 46 | loss: 0.20325 | val_0_rmse: 0.43615 | val_1_rmse: 0.44648 |  0:02:32s
epoch 47 | loss: 0.20084 | val_0_rmse: 0.43641 | val_1_rmse: 0.44608 |  0:02:35s
epoch 48 | loss: 0.20692 | val_0_rmse: 0.43966 | val_1_rmse: 0.45143 |  0:02:38s
epoch 49 | loss: 0.20308 | val_0_rmse: 0.4298  | val_1_rmse: 0.4413  |  0:02:41s
epoch 50 | loss: 0.20153 | val_0_rmse: 0.42513 | val_1_rmse: 0.43577 |  0:02:45s
epoch 51 | loss: 0.20545 | val_0_rmse: 0.44483 | val_1_rmse: 0.45529 |  0:02:48s
epoch 52 | loss: 0.20104 | val_0_rmse: 0.4446  | val_1_rmse: 0.45445 |  0:02:51s
epoch 53 | loss: 0.19972 | val_0_rmse: 0.42138 | val_1_rmse: 0.43246 |  0:02:54s
epoch 54 | loss: 0.2011  | val_0_rmse: 0.44838 | val_1_rmse: 0.46161 |  0:02:57s
epoch 55 | loss: 0.20399 | val_0_rmse: 0.43489 | val_1_rmse: 0.44451 |  0:03:01s
epoch 56 | loss: 0.20207 | val_0_rmse: 0.43293 | val_1_rmse: 0.4436  |  0:03:04s
epoch 57 | loss: 0.1976  | val_0_rmse: 0.42501 | val_1_rmse: 0.44053 |  0:03:07s
epoch 58 | loss: 0.20011 | val_0_rmse: 0.44458 | val_1_rmse: 0.45392 |  0:03:10s
epoch 59 | loss: 0.19969 | val_0_rmse: 0.42826 | val_1_rmse: 0.43711 |  0:03:14s
epoch 60 | loss: 0.2017  | val_0_rmse: 0.42091 | val_1_rmse: 0.43651 |  0:03:17s
epoch 61 | loss: 0.19753 | val_0_rmse: 0.42624 | val_1_rmse: 0.44035 |  0:03:20s
epoch 62 | loss: 0.19729 | val_0_rmse: 0.43694 | val_1_rmse: 0.44866 |  0:03:23s
epoch 63 | loss: 0.19811 | val_0_rmse: 0.4283  | val_1_rmse: 0.44116 |  0:03:27s
epoch 64 | loss: 0.20088 | val_0_rmse: 0.46959 | val_1_rmse: 0.48307 |  0:03:30s
epoch 65 | loss: 0.21124 | val_0_rmse: 0.44512 | val_1_rmse: 0.45678 |  0:03:33s
epoch 66 | loss: 0.20018 | val_0_rmse: 0.42732 | val_1_rmse: 0.43999 |  0:03:36s
epoch 67 | loss: 0.20316 | val_0_rmse: 0.43947 | val_1_rmse: 0.45227 |  0:03:40s
epoch 68 | loss: 0.20096 | val_0_rmse: 0.42754 | val_1_rmse: 0.44138 |  0:03:43s
epoch 69 | loss: 0.19895 | val_0_rmse: 0.43098 | val_1_rmse: 0.44455 |  0:03:46s
epoch 70 | loss: 0.19715 | val_0_rmse: 0.42642 | val_1_rmse: 0.43871 |  0:03:49s
epoch 71 | loss: 0.19986 | val_0_rmse: 0.43734 | val_1_rmse: 0.45025 |  0:03:52s
epoch 72 | loss: 0.19823 | val_0_rmse: 0.43062 | val_1_rmse: 0.44286 |  0:03:56s
epoch 73 | loss: 0.19322 | val_0_rmse: 0.41989 | val_1_rmse: 0.434   |  0:03:59s
epoch 74 | loss: 0.19721 | val_0_rmse: 0.42843 | val_1_rmse: 0.4368  |  0:04:02s
epoch 75 | loss: 0.20561 | val_0_rmse: 0.42722 | val_1_rmse: 0.44074 |  0:04:05s
epoch 76 | loss: 0.19845 | val_0_rmse: 0.42453 | val_1_rmse: 0.43708 |  0:04:09s
epoch 77 | loss: 0.20021 | val_0_rmse: 0.42691 | val_1_rmse: 0.4432  |  0:04:12s
epoch 78 | loss: 0.19847 | val_0_rmse: 0.4286  | val_1_rmse: 0.44114 |  0:04:15s
epoch 79 | loss: 0.19501 | val_0_rmse: 0.41884 | val_1_rmse: 0.43461 |  0:04:18s
epoch 80 | loss: 0.19588 | val_0_rmse: 0.42374 | val_1_rmse: 0.4382  |  0:04:22s
epoch 81 | loss: 0.19698 | val_0_rmse: 0.42479 | val_1_rmse: 0.43814 |  0:04:25s
epoch 82 | loss: 0.19607 | val_0_rmse: 0.42999 | val_1_rmse: 0.44121 |  0:04:28s
epoch 83 | loss: 0.19346 | val_0_rmse: 0.42893 | val_1_rmse: 0.4423  |  0:04:31s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.43246
Best weights from best epoch are automatically used!
ended training at: 07:11:25
Feature importance:
[('Area', 0.016058421753288556), ('Baths', 0.17409924608195734), ('Beds', 0.10779610636872994), ('Latitude', 0.16358084736194894), ('Longitude', 0.18892437141554172), ('Month', 0.10675151836708198), ('Year', 0.2427894886514515)]
Mean squared error is of 10523867871.222986
Mean absolute error:69815.76482508298
MAPE:0.26678568424623766
R2 score:0.8156845878615195
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:11:28
epoch 0  | loss: 0.65738 | val_0_rmse: 0.74924 | val_1_rmse: 0.75718 |  0:00:00s
epoch 1  | loss: 0.34679 | val_0_rmse: 0.58748 | val_1_rmse: 0.5944  |  0:00:01s
epoch 2  | loss: 0.30754 | val_0_rmse: 0.53859 | val_1_rmse: 0.55002 |  0:00:02s
epoch 3  | loss: 0.28299 | val_0_rmse: 0.52198 | val_1_rmse: 0.52904 |  0:00:03s
epoch 4  | loss: 0.30046 | val_0_rmse: 0.51905 | val_1_rmse: 0.52434 |  0:00:04s
epoch 5  | loss: 0.27944 | val_0_rmse: 0.51679 | val_1_rmse: 0.51508 |  0:00:05s
epoch 6  | loss: 0.2701  | val_0_rmse: 0.50813 | val_1_rmse: 0.5185  |  0:00:06s
epoch 7  | loss: 0.27225 | val_0_rmse: 0.51531 | val_1_rmse: 0.51969 |  0:00:07s
epoch 8  | loss: 0.26686 | val_0_rmse: 0.50378 | val_1_rmse: 0.50746 |  0:00:08s
epoch 9  | loss: 0.26812 | val_0_rmse: 0.50025 | val_1_rmse: 0.50502 |  0:00:09s
epoch 10 | loss: 0.26031 | val_0_rmse: 0.50992 | val_1_rmse: 0.52325 |  0:00:10s
epoch 11 | loss: 0.26235 | val_0_rmse: 0.49962 | val_1_rmse: 0.51334 |  0:00:11s
epoch 12 | loss: 0.25528 | val_0_rmse: 0.49594 | val_1_rmse: 0.50304 |  0:00:12s
epoch 13 | loss: 0.25729 | val_0_rmse: 0.51738 | val_1_rmse: 0.52819 |  0:00:13s
epoch 14 | loss: 0.25587 | val_0_rmse: 0.49425 | val_1_rmse: 0.49817 |  0:00:14s
epoch 15 | loss: 0.24876 | val_0_rmse: 0.48812 | val_1_rmse: 0.49353 |  0:00:15s
epoch 16 | loss: 0.24891 | val_0_rmse: 0.50152 | val_1_rmse: 0.49987 |  0:00:16s
epoch 17 | loss: 0.24963 | val_0_rmse: 0.48167 | val_1_rmse: 0.49225 |  0:00:17s
epoch 18 | loss: 0.25136 | val_0_rmse: 0.48359 | val_1_rmse: 0.48779 |  0:00:18s
epoch 19 | loss: 0.24863 | val_0_rmse: 0.49063 | val_1_rmse: 0.49504 |  0:00:19s
epoch 20 | loss: 0.25064 | val_0_rmse: 0.49531 | val_1_rmse: 0.5013  |  0:00:20s
epoch 21 | loss: 0.24349 | val_0_rmse: 0.47    | val_1_rmse: 0.47865 |  0:00:21s
epoch 22 | loss: 0.23479 | val_0_rmse: 0.4785  | val_1_rmse: 0.4844  |  0:00:22s
epoch 23 | loss: 0.23208 | val_0_rmse: 0.46575 | val_1_rmse: 0.46889 |  0:00:23s
epoch 24 | loss: 0.2278  | val_0_rmse: 0.47564 | val_1_rmse: 0.487   |  0:00:24s
epoch 25 | loss: 0.22644 | val_0_rmse: 0.46341 | val_1_rmse: 0.46237 |  0:00:25s
epoch 26 | loss: 0.22475 | val_0_rmse: 0.45756 | val_1_rmse: 0.45452 |  0:00:25s
epoch 27 | loss: 0.21769 | val_0_rmse: 0.45137 | val_1_rmse: 0.45185 |  0:00:26s
epoch 28 | loss: 0.21995 | val_0_rmse: 0.45602 | val_1_rmse: 0.46104 |  0:00:27s
epoch 29 | loss: 0.21876 | val_0_rmse: 0.45587 | val_1_rmse: 0.45617 |  0:00:28s
epoch 30 | loss: 0.21801 | val_0_rmse: 0.44728 | val_1_rmse: 0.45001 |  0:00:29s
epoch 31 | loss: 0.21802 | val_0_rmse: 0.45489 | val_1_rmse: 0.4576  |  0:00:30s
epoch 32 | loss: 0.23497 | val_0_rmse: 0.4969  | val_1_rmse: 0.50277 |  0:00:31s
epoch 33 | loss: 0.25032 | val_0_rmse: 0.48397 | val_1_rmse: 0.48808 |  0:00:32s
epoch 34 | loss: 0.23079 | val_0_rmse: 0.45614 | val_1_rmse: 0.45231 |  0:00:33s
epoch 35 | loss: 0.2214  | val_0_rmse: 0.46105 | val_1_rmse: 0.46061 |  0:00:34s
epoch 36 | loss: 0.22478 | val_0_rmse: 0.45234 | val_1_rmse: 0.45228 |  0:00:35s
epoch 37 | loss: 0.22058 | val_0_rmse: 0.45022 | val_1_rmse: 0.44735 |  0:00:36s
epoch 38 | loss: 0.21994 | val_0_rmse: 0.46573 | val_1_rmse: 0.46079 |  0:00:37s
epoch 39 | loss: 0.22188 | val_0_rmse: 0.45426 | val_1_rmse: 0.45281 |  0:00:38s
epoch 40 | loss: 0.22158 | val_0_rmse: 0.47205 | val_1_rmse: 0.47893 |  0:00:39s
epoch 41 | loss: 0.21972 | val_0_rmse: 0.45103 | val_1_rmse: 0.4518  |  0:00:40s
epoch 42 | loss: 0.21147 | val_0_rmse: 0.45568 | val_1_rmse: 0.4621  |  0:00:41s
epoch 43 | loss: 0.21974 | val_0_rmse: 0.45515 | val_1_rmse: 0.4646  |  0:00:42s
epoch 44 | loss: 0.21626 | val_0_rmse: 0.45637 | val_1_rmse: 0.46006 |  0:00:43s
epoch 45 | loss: 0.21269 | val_0_rmse: 0.44489 | val_1_rmse: 0.45229 |  0:00:44s
epoch 46 | loss: 0.21414 | val_0_rmse: 0.4374  | val_1_rmse: 0.43418 |  0:00:45s
epoch 47 | loss: 0.21012 | val_0_rmse: 0.43987 | val_1_rmse: 0.44006 |  0:00:46s
epoch 48 | loss: 0.2144  | val_0_rmse: 0.44082 | val_1_rmse: 0.44202 |  0:00:47s
epoch 49 | loss: 0.20775 | val_0_rmse: 0.44307 | val_1_rmse: 0.44909 |  0:00:48s
epoch 50 | loss: 0.20945 | val_0_rmse: 0.44287 | val_1_rmse: 0.45163 |  0:00:49s
epoch 51 | loss: 0.21109 | val_0_rmse: 0.44203 | val_1_rmse: 0.4452  |  0:00:49s
epoch 52 | loss: 0.21244 | val_0_rmse: 0.45199 | val_1_rmse: 0.4531  |  0:00:50s
epoch 53 | loss: 0.21275 | val_0_rmse: 0.43787 | val_1_rmse: 0.43986 |  0:00:51s
epoch 54 | loss: 0.2087  | val_0_rmse: 0.45477 | val_1_rmse: 0.46528 |  0:00:52s
epoch 55 | loss: 0.21073 | val_0_rmse: 0.45786 | val_1_rmse: 0.45354 |  0:00:53s
epoch 56 | loss: 0.21121 | val_0_rmse: 0.4419  | val_1_rmse: 0.44703 |  0:00:54s
epoch 57 | loss: 0.21    | val_0_rmse: 0.4364  | val_1_rmse: 0.44137 |  0:00:55s
epoch 58 | loss: 0.20662 | val_0_rmse: 0.43288 | val_1_rmse: 0.43618 |  0:00:56s
epoch 59 | loss: 0.20758 | val_0_rmse: 0.43259 | val_1_rmse: 0.43562 |  0:00:57s
epoch 60 | loss: 0.20188 | val_0_rmse: 0.43197 | val_1_rmse: 0.43323 |  0:00:58s
epoch 61 | loss: 0.20564 | val_0_rmse: 0.44515 | val_1_rmse: 0.44264 |  0:00:59s
epoch 62 | loss: 0.20951 | val_0_rmse: 0.45279 | val_1_rmse: 0.45703 |  0:01:00s
epoch 63 | loss: 0.21504 | val_0_rmse: 0.43736 | val_1_rmse: 0.44064 |  0:01:01s
epoch 64 | loss: 0.20704 | val_0_rmse: 0.44539 | val_1_rmse: 0.45208 |  0:01:02s
epoch 65 | loss: 0.2061  | val_0_rmse: 0.45883 | val_1_rmse: 0.45688 |  0:01:03s
epoch 66 | loss: 0.2064  | val_0_rmse: 0.43878 | val_1_rmse: 0.44682 |  0:01:04s
epoch 67 | loss: 0.20295 | val_0_rmse: 0.42969 | val_1_rmse: 0.43371 |  0:01:05s
epoch 68 | loss: 0.20528 | val_0_rmse: 0.43829 | val_1_rmse: 0.44382 |  0:01:06s
epoch 69 | loss: 0.20037 | val_0_rmse: 0.4391  | val_1_rmse: 0.4446  |  0:01:07s
epoch 70 | loss: 0.20271 | val_0_rmse: 0.43785 | val_1_rmse: 0.44526 |  0:01:08s
epoch 71 | loss: 0.20221 | val_0_rmse: 0.44076 | val_1_rmse: 0.43968 |  0:01:09s
epoch 72 | loss: 0.20682 | val_0_rmse: 0.43341 | val_1_rmse: 0.43782 |  0:01:10s
epoch 73 | loss: 0.20353 | val_0_rmse: 0.43651 | val_1_rmse: 0.43815 |  0:01:11s
epoch 74 | loss: 0.20256 | val_0_rmse: 0.42782 | val_1_rmse: 0.42972 |  0:01:11s
epoch 75 | loss: 0.20141 | val_0_rmse: 0.43244 | val_1_rmse: 0.43818 |  0:01:12s
epoch 76 | loss: 0.20412 | val_0_rmse: 0.43312 | val_1_rmse: 0.44053 |  0:01:13s
epoch 77 | loss: 0.20173 | val_0_rmse: 0.43988 | val_1_rmse: 0.44813 |  0:01:14s
epoch 78 | loss: 0.20153 | val_0_rmse: 0.43494 | val_1_rmse: 0.44441 |  0:01:15s
epoch 79 | loss: 0.19959 | val_0_rmse: 0.43793 | val_1_rmse: 0.44577 |  0:01:16s
epoch 80 | loss: 0.20261 | val_0_rmse: 0.45999 | val_1_rmse: 0.47275 |  0:01:17s
epoch 81 | loss: 0.20401 | val_0_rmse: 0.44346 | val_1_rmse: 0.44858 |  0:01:18s
epoch 82 | loss: 0.20398 | val_0_rmse: 0.43562 | val_1_rmse: 0.43813 |  0:01:19s
epoch 83 | loss: 0.20189 | val_0_rmse: 0.44    | val_1_rmse: 0.45413 |  0:01:20s
epoch 84 | loss: 0.19828 | val_0_rmse: 0.43197 | val_1_rmse: 0.4397  |  0:01:21s
epoch 85 | loss: 0.20002 | val_0_rmse: 0.44927 | val_1_rmse: 0.45169 |  0:01:22s
epoch 86 | loss: 0.20434 | val_0_rmse: 0.42795 | val_1_rmse: 0.43501 |  0:01:23s
epoch 87 | loss: 0.20118 | val_0_rmse: 0.4388  | val_1_rmse: 0.44754 |  0:01:24s
epoch 88 | loss: 0.19722 | val_0_rmse: 0.42326 | val_1_rmse: 0.42746 |  0:01:25s
epoch 89 | loss: 0.19475 | val_0_rmse: 0.42851 | val_1_rmse: 0.43542 |  0:01:26s
epoch 90 | loss: 0.19402 | val_0_rmse: 0.42657 | val_1_rmse: 0.4337  |  0:01:27s
epoch 91 | loss: 0.19985 | val_0_rmse: 0.42825 | val_1_rmse: 0.43321 |  0:01:28s
epoch 92 | loss: 0.20175 | val_0_rmse: 0.42579 | val_1_rmse: 0.43672 |  0:01:29s
epoch 93 | loss: 0.19781 | val_0_rmse: 0.43576 | val_1_rmse: 0.43946 |  0:01:30s
epoch 94 | loss: 0.20131 | val_0_rmse: 0.42294 | val_1_rmse: 0.43303 |  0:01:31s
epoch 95 | loss: 0.197   | val_0_rmse: 0.43234 | val_1_rmse: 0.43999 |  0:01:32s
epoch 96 | loss: 0.19912 | val_0_rmse: 0.42695 | val_1_rmse: 0.43399 |  0:01:32s
epoch 97 | loss: 0.19638 | val_0_rmse: 0.43093 | val_1_rmse: 0.43477 |  0:01:33s
epoch 98 | loss: 0.19975 | val_0_rmse: 0.42952 | val_1_rmse: 0.43237 |  0:01:34s
epoch 99 | loss: 0.19646 | val_0_rmse: 0.42623 | val_1_rmse: 0.43293 |  0:01:35s
epoch 100| loss: 0.19546 | val_0_rmse: 0.42485 | val_1_rmse: 0.43308 |  0:01:36s
epoch 101| loss: 0.19946 | val_0_rmse: 0.42576 | val_1_rmse: 0.43374 |  0:01:37s
epoch 102| loss: 0.19804 | val_0_rmse: 0.43983 | val_1_rmse: 0.44567 |  0:01:38s
epoch 103| loss: 0.19518 | val_0_rmse: 0.42017 | val_1_rmse: 0.42961 |  0:01:39s
epoch 104| loss: 0.19968 | val_0_rmse: 0.43008 | val_1_rmse: 0.43577 |  0:01:40s
epoch 105| loss: 0.19462 | val_0_rmse: 0.42259 | val_1_rmse: 0.4302  |  0:01:41s
epoch 106| loss: 0.19753 | val_0_rmse: 0.43379 | val_1_rmse: 0.43828 |  0:01:42s
epoch 107| loss: 0.19795 | val_0_rmse: 0.42626 | val_1_rmse: 0.43345 |  0:01:43s
epoch 108| loss: 0.19615 | val_0_rmse: 0.42398 | val_1_rmse: 0.4323  |  0:01:44s
epoch 109| loss: 0.19628 | val_0_rmse: 0.4468  | val_1_rmse: 0.45907 |  0:01:45s
epoch 110| loss: 0.2042  | val_0_rmse: 0.42691 | val_1_rmse: 0.43683 |  0:01:46s
epoch 111| loss: 0.19823 | val_0_rmse: 0.42466 | val_1_rmse: 0.4352  |  0:01:47s
epoch 112| loss: 0.193   | val_0_rmse: 0.44229 | val_1_rmse: 0.45626 |  0:01:48s
epoch 113| loss: 0.19805 | val_0_rmse: 0.42963 | val_1_rmse: 0.43634 |  0:01:49s
epoch 114| loss: 0.19627 | val_0_rmse: 0.42947 | val_1_rmse: 0.44074 |  0:01:50s
epoch 115| loss: 0.19082 | val_0_rmse: 0.42398 | val_1_rmse: 0.43441 |  0:01:51s
epoch 116| loss: 0.1953  | val_0_rmse: 0.43012 | val_1_rmse: 0.43896 |  0:01:52s
epoch 117| loss: 0.19579 | val_0_rmse: 0.43067 | val_1_rmse: 0.44415 |  0:01:53s
epoch 118| loss: 0.19649 | val_0_rmse: 0.4422  | val_1_rmse: 0.44263 |  0:01:54s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.42746
Best weights from best epoch are automatically used!
ended training at: 07:13:22
Feature importance:
[('Area', 0.5490811076039317), ('Baths', 0.00449910384466482), ('Beds', 0.02489209608167815), ('Latitude', 0.2818373570886782), ('Longitude', 0.06088293703725965), ('Month', 0.026201199748513163), ('Year', 0.05260619859527435)]
Mean squared error is of 6122022757.401244
Mean absolute error:54979.49576873114
MAPE:0.1422618259213802
R2 score:0.7989989378665161
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:13:22
epoch 0  | loss: 0.65808 | val_0_rmse: 0.67648 | val_1_rmse: 0.67337 |  0:00:00s
epoch 1  | loss: 0.35418 | val_0_rmse: 0.57693 | val_1_rmse: 0.57079 |  0:00:01s
epoch 2  | loss: 0.31678 | val_0_rmse: 0.54714 | val_1_rmse: 0.54762 |  0:00:02s
epoch 3  | loss: 0.28925 | val_0_rmse: 0.54033 | val_1_rmse: 0.53546 |  0:00:03s
epoch 4  | loss: 0.28184 | val_0_rmse: 0.52331 | val_1_rmse: 0.51866 |  0:00:04s
epoch 5  | loss: 0.28171 | val_0_rmse: 0.50657 | val_1_rmse: 0.49955 |  0:00:05s
epoch 6  | loss: 0.27203 | val_0_rmse: 0.53725 | val_1_rmse: 0.53545 |  0:00:06s
epoch 7  | loss: 0.2787  | val_0_rmse: 0.51796 | val_1_rmse: 0.52151 |  0:00:07s
epoch 8  | loss: 0.26707 | val_0_rmse: 0.52251 | val_1_rmse: 0.52774 |  0:00:08s
epoch 9  | loss: 0.26486 | val_0_rmse: 0.50358 | val_1_rmse: 0.50541 |  0:00:09s
epoch 10 | loss: 0.26413 | val_0_rmse: 0.52158 | val_1_rmse: 0.5267  |  0:00:10s
epoch 11 | loss: 0.26557 | val_0_rmse: 0.49806 | val_1_rmse: 0.49317 |  0:00:11s
epoch 12 | loss: 0.26118 | val_0_rmse: 0.50179 | val_1_rmse: 0.49797 |  0:00:12s
epoch 13 | loss: 0.25948 | val_0_rmse: 0.50556 | val_1_rmse: 0.50396 |  0:00:13s
epoch 14 | loss: 0.25659 | val_0_rmse: 0.50536 | val_1_rmse: 0.49711 |  0:00:14s
epoch 15 | loss: 0.25293 | val_0_rmse: 0.49046 | val_1_rmse: 0.48958 |  0:00:15s
epoch 16 | loss: 0.25146 | val_0_rmse: 0.49535 | val_1_rmse: 0.48906 |  0:00:16s
epoch 17 | loss: 0.25347 | val_0_rmse: 0.48684 | val_1_rmse: 0.48548 |  0:00:17s
epoch 18 | loss: 0.24953 | val_0_rmse: 0.49006 | val_1_rmse: 0.48377 |  0:00:18s
epoch 19 | loss: 0.2511  | val_0_rmse: 0.49947 | val_1_rmse: 0.49395 |  0:00:19s
epoch 20 | loss: 0.25882 | val_0_rmse: 0.48998 | val_1_rmse: 0.49243 |  0:00:20s
epoch 21 | loss: 0.25791 | val_0_rmse: 0.48973 | val_1_rmse: 0.49065 |  0:00:21s
epoch 22 | loss: 0.25879 | val_0_rmse: 0.51277 | val_1_rmse: 0.5066  |  0:00:22s
epoch 23 | loss: 0.25392 | val_0_rmse: 0.48725 | val_1_rmse: 0.4893  |  0:00:23s
epoch 24 | loss: 0.24974 | val_0_rmse: 0.50182 | val_1_rmse: 0.50605 |  0:00:24s
epoch 25 | loss: 0.25335 | val_0_rmse: 0.49556 | val_1_rmse: 0.5039  |  0:00:25s
epoch 26 | loss: 0.25209 | val_0_rmse: 0.48433 | val_1_rmse: 0.48346 |  0:00:26s
epoch 27 | loss: 0.25112 | val_0_rmse: 0.48934 | val_1_rmse: 0.48367 |  0:00:27s
epoch 28 | loss: 0.24526 | val_0_rmse: 0.49197 | val_1_rmse: 0.49666 |  0:00:28s
epoch 29 | loss: 0.24664 | val_0_rmse: 0.49198 | val_1_rmse: 0.49046 |  0:00:28s
epoch 30 | loss: 0.25741 | val_0_rmse: 0.50632 | val_1_rmse: 0.50245 |  0:00:29s
epoch 31 | loss: 0.25645 | val_0_rmse: 0.49842 | val_1_rmse: 0.49889 |  0:00:30s
epoch 32 | loss: 0.24946 | val_0_rmse: 0.48364 | val_1_rmse: 0.48393 |  0:00:31s
epoch 33 | loss: 0.24546 | val_0_rmse: 0.48555 | val_1_rmse: 0.48071 |  0:00:32s
epoch 34 | loss: 0.24524 | val_0_rmse: 0.47809 | val_1_rmse: 0.48074 |  0:00:33s
epoch 35 | loss: 0.24156 | val_0_rmse: 0.48205 | val_1_rmse: 0.4836  |  0:00:34s
epoch 36 | loss: 0.24667 | val_0_rmse: 0.48034 | val_1_rmse: 0.48182 |  0:00:35s
epoch 37 | loss: 0.25178 | val_0_rmse: 0.48913 | val_1_rmse: 0.49504 |  0:00:36s
epoch 38 | loss: 0.24613 | val_0_rmse: 0.48195 | val_1_rmse: 0.48265 |  0:00:37s
epoch 39 | loss: 0.24075 | val_0_rmse: 0.47458 | val_1_rmse: 0.47664 |  0:00:38s
epoch 40 | loss: 0.24151 | val_0_rmse: 0.48212 | val_1_rmse: 0.48352 |  0:00:39s
epoch 41 | loss: 0.24341 | val_0_rmse: 0.48041 | val_1_rmse: 0.47845 |  0:00:40s
epoch 42 | loss: 0.24453 | val_0_rmse: 0.48571 | val_1_rmse: 0.489   |  0:00:41s
epoch 43 | loss: 0.25246 | val_0_rmse: 0.48493 | val_1_rmse: 0.48402 |  0:00:42s
epoch 44 | loss: 0.244   | val_0_rmse: 0.47782 | val_1_rmse: 0.48227 |  0:00:43s
epoch 45 | loss: 0.23916 | val_0_rmse: 0.48337 | val_1_rmse: 0.48832 |  0:00:44s
epoch 46 | loss: 0.23937 | val_0_rmse: 0.48074 | val_1_rmse: 0.48546 |  0:00:45s
epoch 47 | loss: 0.2431  | val_0_rmse: 0.48883 | val_1_rmse: 0.49627 |  0:00:46s
epoch 48 | loss: 0.24139 | val_0_rmse: 0.47535 | val_1_rmse: 0.48068 |  0:00:47s
epoch 49 | loss: 0.23844 | val_0_rmse: 0.47542 | val_1_rmse: 0.47953 |  0:00:48s
epoch 50 | loss: 0.23633 | val_0_rmse: 0.48124 | val_1_rmse: 0.49366 |  0:00:49s
epoch 51 | loss: 0.23955 | val_0_rmse: 0.47849 | val_1_rmse: 0.48729 |  0:00:50s
epoch 52 | loss: 0.2413  | val_0_rmse: 0.47581 | val_1_rmse: 0.47592 |  0:00:51s
epoch 53 | loss: 0.24113 | val_0_rmse: 0.47772 | val_1_rmse: 0.47952 |  0:00:52s
epoch 54 | loss: 0.23652 | val_0_rmse: 0.47648 | val_1_rmse: 0.48614 |  0:00:53s
epoch 55 | loss: 0.24018 | val_0_rmse: 0.47504 | val_1_rmse: 0.47701 |  0:00:53s
epoch 56 | loss: 0.23709 | val_0_rmse: 0.47744 | val_1_rmse: 0.48407 |  0:00:54s
epoch 57 | loss: 0.23735 | val_0_rmse: 0.47976 | val_1_rmse: 0.48202 |  0:00:55s
epoch 58 | loss: 0.24012 | val_0_rmse: 0.52243 | val_1_rmse: 0.52262 |  0:00:56s
epoch 59 | loss: 0.24506 | val_0_rmse: 0.47866 | val_1_rmse: 0.48982 |  0:00:57s
epoch 60 | loss: 0.23437 | val_0_rmse: 0.47509 | val_1_rmse: 0.48286 |  0:00:58s
epoch 61 | loss: 0.24267 | val_0_rmse: 0.5241  | val_1_rmse: 0.52727 |  0:00:59s
epoch 62 | loss: 0.25425 | val_0_rmse: 0.50255 | val_1_rmse: 0.50841 |  0:01:00s
epoch 63 | loss: 0.24587 | val_0_rmse: 0.4798  | val_1_rmse: 0.48638 |  0:01:01s
epoch 64 | loss: 0.24291 | val_0_rmse: 0.47875 | val_1_rmse: 0.48471 |  0:01:02s
epoch 65 | loss: 0.24443 | val_0_rmse: 0.47705 | val_1_rmse: 0.48608 |  0:01:03s
epoch 66 | loss: 0.24382 | val_0_rmse: 0.49705 | val_1_rmse: 0.50068 |  0:01:04s
epoch 67 | loss: 0.2489  | val_0_rmse: 0.47699 | val_1_rmse: 0.4807  |  0:01:05s
epoch 68 | loss: 0.23964 | val_0_rmse: 0.47332 | val_1_rmse: 0.47989 |  0:01:06s
epoch 69 | loss: 0.24555 | val_0_rmse: 0.48768 | val_1_rmse: 0.49496 |  0:01:07s
epoch 70 | loss: 0.24662 | val_0_rmse: 0.47162 | val_1_rmse: 0.47711 |  0:01:08s
epoch 71 | loss: 0.23498 | val_0_rmse: 0.47214 | val_1_rmse: 0.47546 |  0:01:09s
epoch 72 | loss: 0.23405 | val_0_rmse: 0.47072 | val_1_rmse: 0.47643 |  0:01:10s
epoch 73 | loss: 0.23693 | val_0_rmse: 0.46918 | val_1_rmse: 0.47451 |  0:01:11s
epoch 74 | loss: 0.23499 | val_0_rmse: 0.46872 | val_1_rmse: 0.47188 |  0:01:12s
epoch 75 | loss: 0.23575 | val_0_rmse: 0.47227 | val_1_rmse: 0.47685 |  0:01:13s
epoch 76 | loss: 0.23665 | val_0_rmse: 0.47512 | val_1_rmse: 0.48305 |  0:01:14s
epoch 77 | loss: 0.2335  | val_0_rmse: 0.47298 | val_1_rmse: 0.47918 |  0:01:15s
epoch 78 | loss: 0.23511 | val_0_rmse: 0.47304 | val_1_rmse: 0.47887 |  0:01:16s
epoch 79 | loss: 0.23948 | val_0_rmse: 0.48505 | val_1_rmse: 0.49492 |  0:01:17s
epoch 80 | loss: 0.23484 | val_0_rmse: 0.46847 | val_1_rmse: 0.47467 |  0:01:18s
epoch 81 | loss: 0.23631 | val_0_rmse: 0.46661 | val_1_rmse: 0.47865 |  0:01:18s
epoch 82 | loss: 0.2408  | val_0_rmse: 0.46985 | val_1_rmse: 0.47959 |  0:01:19s
epoch 83 | loss: 0.23752 | val_0_rmse: 0.47302 | val_1_rmse: 0.47783 |  0:01:20s
epoch 84 | loss: 0.23727 | val_0_rmse: 0.47486 | val_1_rmse: 0.47806 |  0:01:21s
epoch 85 | loss: 0.23583 | val_0_rmse: 0.47246 | val_1_rmse: 0.47933 |  0:01:22s
epoch 86 | loss: 0.23353 | val_0_rmse: 0.46647 | val_1_rmse: 0.473   |  0:01:23s
epoch 87 | loss: 0.23733 | val_0_rmse: 0.46937 | val_1_rmse: 0.47891 |  0:01:24s
epoch 88 | loss: 0.22892 | val_0_rmse: 0.46891 | val_1_rmse: 0.47668 |  0:01:25s
epoch 89 | loss: 0.231   | val_0_rmse: 0.47304 | val_1_rmse: 0.48431 |  0:01:26s
epoch 90 | loss: 0.23347 | val_0_rmse: 0.47631 | val_1_rmse: 0.4846  |  0:01:27s
epoch 91 | loss: 0.23691 | val_0_rmse: 0.4733  | val_1_rmse: 0.48115 |  0:01:28s
epoch 92 | loss: 0.23174 | val_0_rmse: 0.46377 | val_1_rmse: 0.47046 |  0:01:29s
epoch 93 | loss: 0.23058 | val_0_rmse: 0.46346 | val_1_rmse: 0.47163 |  0:01:30s
epoch 94 | loss: 0.22519 | val_0_rmse: 0.46489 | val_1_rmse: 0.47238 |  0:01:31s
epoch 95 | loss: 0.23121 | val_0_rmse: 0.47296 | val_1_rmse: 0.4821  |  0:01:32s
epoch 96 | loss: 0.22838 | val_0_rmse: 0.46999 | val_1_rmse: 0.47667 |  0:01:33s
epoch 97 | loss: 0.22773 | val_0_rmse: 0.46396 | val_1_rmse: 0.47005 |  0:01:34s
epoch 98 | loss: 0.22246 | val_0_rmse: 0.46062 | val_1_rmse: 0.46705 |  0:01:35s
epoch 99 | loss: 0.22003 | val_0_rmse: 0.45732 | val_1_rmse: 0.46558 |  0:01:36s
epoch 100| loss: 0.21911 | val_0_rmse: 0.45944 | val_1_rmse: 0.47126 |  0:01:37s
epoch 101| loss: 0.22124 | val_0_rmse: 0.46744 | val_1_rmse: 0.47344 |  0:01:38s
epoch 102| loss: 0.22157 | val_0_rmse: 0.46572 | val_1_rmse: 0.46929 |  0:01:39s
epoch 103| loss: 0.23619 | val_0_rmse: 0.46581 | val_1_rmse: 0.46637 |  0:01:40s
epoch 104| loss: 0.23047 | val_0_rmse: 0.4657  | val_1_rmse: 0.4741  |  0:01:41s
epoch 105| loss: 0.22499 | val_0_rmse: 0.45359 | val_1_rmse: 0.46461 |  0:01:42s
epoch 106| loss: 0.21716 | val_0_rmse: 0.45347 | val_1_rmse: 0.46273 |  0:01:43s
epoch 107| loss: 0.21826 | val_0_rmse: 0.46    | val_1_rmse: 0.47157 |  0:01:44s
epoch 108| loss: 0.21974 | val_0_rmse: 0.47008 | val_1_rmse: 0.48324 |  0:01:45s
epoch 109| loss: 0.21899 | val_0_rmse: 0.45063 | val_1_rmse: 0.45712 |  0:01:45s
epoch 110| loss: 0.21921 | val_0_rmse: 0.46062 | val_1_rmse: 0.46813 |  0:01:46s
epoch 111| loss: 0.21652 | val_0_rmse: 0.459   | val_1_rmse: 0.46837 |  0:01:47s
epoch 112| loss: 0.21856 | val_0_rmse: 0.44967 | val_1_rmse: 0.45999 |  0:01:48s
epoch 113| loss: 0.22024 | val_0_rmse: 0.45973 | val_1_rmse: 0.46767 |  0:01:49s
epoch 114| loss: 0.21898 | val_0_rmse: 0.45533 | val_1_rmse: 0.46163 |  0:01:50s
epoch 115| loss: 0.21668 | val_0_rmse: 0.44925 | val_1_rmse: 0.4602  |  0:01:51s
epoch 116| loss: 0.21714 | val_0_rmse: 0.47292 | val_1_rmse: 0.47965 |  0:01:52s
epoch 117| loss: 0.22659 | val_0_rmse: 0.46611 | val_1_rmse: 0.47404 |  0:01:53s
epoch 118| loss: 0.22215 | val_0_rmse: 0.46724 | val_1_rmse: 0.47938 |  0:01:54s
epoch 119| loss: 0.22129 | val_0_rmse: 0.45355 | val_1_rmse: 0.4669  |  0:01:55s
epoch 120| loss: 0.21764 | val_0_rmse: 0.45096 | val_1_rmse: 0.46118 |  0:01:56s
epoch 121| loss: 0.22361 | val_0_rmse: 0.46697 | val_1_rmse: 0.47763 |  0:01:57s
epoch 122| loss: 0.21581 | val_0_rmse: 0.44942 | val_1_rmse: 0.45738 |  0:01:58s
epoch 123| loss: 0.21091 | val_0_rmse: 0.44733 | val_1_rmse: 0.46129 |  0:01:59s
epoch 124| loss: 0.21586 | val_0_rmse: 0.45012 | val_1_rmse: 0.46507 |  0:02:00s
epoch 125| loss: 0.21596 | val_0_rmse: 0.44938 | val_1_rmse: 0.45836 |  0:02:01s
epoch 126| loss: 0.21101 | val_0_rmse: 0.44517 | val_1_rmse: 0.45514 |  0:02:02s
epoch 127| loss: 0.21102 | val_0_rmse: 0.44488 | val_1_rmse: 0.45808 |  0:02:03s
epoch 128| loss: 0.20772 | val_0_rmse: 0.45064 | val_1_rmse: 0.46159 |  0:02:04s
epoch 129| loss: 0.2118  | val_0_rmse: 0.45412 | val_1_rmse: 0.46649 |  0:02:05s
epoch 130| loss: 0.21614 | val_0_rmse: 0.45469 | val_1_rmse: 0.46036 |  0:02:06s
epoch 131| loss: 0.21386 | val_0_rmse: 0.44633 | val_1_rmse: 0.4597  |  0:02:07s
epoch 132| loss: 0.21063 | val_0_rmse: 0.44031 | val_1_rmse: 0.45239 |  0:02:08s
epoch 133| loss: 0.20804 | val_0_rmse: 0.44496 | val_1_rmse: 0.45671 |  0:02:09s
epoch 134| loss: 0.21685 | val_0_rmse: 0.44463 | val_1_rmse: 0.4582  |  0:02:10s
epoch 135| loss: 0.22378 | val_0_rmse: 0.45661 | val_1_rmse: 0.46731 |  0:02:10s
epoch 136| loss: 0.21827 | val_0_rmse: 0.46392 | val_1_rmse: 0.47751 |  0:02:11s
epoch 137| loss: 0.21296 | val_0_rmse: 0.44557 | val_1_rmse: 0.44849 |  0:02:12s
epoch 138| loss: 0.21104 | val_0_rmse: 0.44478 | val_1_rmse: 0.45448 |  0:02:13s
epoch 139| loss: 0.2119  | val_0_rmse: 0.4397  | val_1_rmse: 0.45042 |  0:02:14s
epoch 140| loss: 0.20573 | val_0_rmse: 0.43997 | val_1_rmse: 0.45154 |  0:02:15s
epoch 141| loss: 0.21357 | val_0_rmse: 0.43887 | val_1_rmse: 0.45128 |  0:02:16s
epoch 142| loss: 0.20899 | val_0_rmse: 0.44366 | val_1_rmse: 0.45398 |  0:02:17s
epoch 143| loss: 0.20741 | val_0_rmse: 0.44687 | val_1_rmse: 0.45929 |  0:02:18s
epoch 144| loss: 0.2042  | val_0_rmse: 0.43601 | val_1_rmse: 0.44846 |  0:02:19s
epoch 145| loss: 0.20112 | val_0_rmse: 0.43544 | val_1_rmse: 0.4502  |  0:02:20s
epoch 146| loss: 0.20252 | val_0_rmse: 0.42971 | val_1_rmse: 0.44233 |  0:02:21s
epoch 147| loss: 0.20567 | val_0_rmse: 0.43757 | val_1_rmse: 0.45302 |  0:02:22s
epoch 148| loss: 0.20627 | val_0_rmse: 0.44955 | val_1_rmse: 0.45928 |  0:02:23s
epoch 149| loss: 0.21206 | val_0_rmse: 0.43709 | val_1_rmse: 0.45017 |  0:02:24s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.44233
Best weights from best epoch are automatically used!
ended training at: 07:15:47
Feature importance:
[('Area', 0.49501135053908474), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.3249173896344934), ('Longitude', 0.12586180901258334), ('Month', 0.054209450813838524), ('Year', 0.0)]
Mean squared error is of 6620004701.218589
Mean absolute error:56874.07508694011
MAPE:0.14370712777891506
R2 score:0.7926551992489774
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:15:47
epoch 0  | loss: 0.67512 | val_0_rmse: 0.69633 | val_1_rmse: 0.69627 |  0:00:00s
epoch 1  | loss: 0.37362 | val_0_rmse: 0.6088  | val_1_rmse: 0.61044 |  0:00:01s
epoch 2  | loss: 0.31661 | val_0_rmse: 0.56218 | val_1_rmse: 0.57126 |  0:00:02s
epoch 3  | loss: 0.29956 | val_0_rmse: 0.52718 | val_1_rmse: 0.53551 |  0:00:03s
epoch 4  | loss: 0.28712 | val_0_rmse: 0.5257  | val_1_rmse: 0.53597 |  0:00:04s
epoch 5  | loss: 0.26658 | val_0_rmse: 0.51493 | val_1_rmse: 0.52542 |  0:00:05s
epoch 6  | loss: 0.27057 | val_0_rmse: 0.51191 | val_1_rmse: 0.52637 |  0:00:06s
epoch 7  | loss: 0.26247 | val_0_rmse: 0.50088 | val_1_rmse: 0.51552 |  0:00:07s
epoch 8  | loss: 0.26376 | val_0_rmse: 0.4925  | val_1_rmse: 0.50535 |  0:00:08s
epoch 9  | loss: 0.26798 | val_0_rmse: 0.50583 | val_1_rmse: 0.51708 |  0:00:09s
epoch 10 | loss: 0.2556  | val_0_rmse: 0.48633 | val_1_rmse: 0.50581 |  0:00:10s
epoch 11 | loss: 0.24339 | val_0_rmse: 0.51294 | val_1_rmse: 0.5206  |  0:00:11s
epoch 12 | loss: 0.24381 | val_0_rmse: 0.4721  | val_1_rmse: 0.48599 |  0:00:12s
epoch 13 | loss: 0.24086 | val_0_rmse: 0.46898 | val_1_rmse: 0.48018 |  0:00:13s
epoch 14 | loss: 0.23903 | val_0_rmse: 0.46829 | val_1_rmse: 0.48064 |  0:00:14s
epoch 15 | loss: 0.24531 | val_0_rmse: 0.48726 | val_1_rmse: 0.50359 |  0:00:15s
epoch 16 | loss: 0.23698 | val_0_rmse: 0.46602 | val_1_rmse: 0.48038 |  0:00:16s
epoch 17 | loss: 0.2278  | val_0_rmse: 0.46397 | val_1_rmse: 0.48365 |  0:00:17s
epoch 18 | loss: 0.23063 | val_0_rmse: 0.46539 | val_1_rmse: 0.48225 |  0:00:18s
epoch 19 | loss: 0.22657 | val_0_rmse: 0.4594  | val_1_rmse: 0.47487 |  0:00:19s
epoch 20 | loss: 0.23122 | val_0_rmse: 0.47732 | val_1_rmse: 0.49739 |  0:00:20s
epoch 21 | loss: 0.23388 | val_0_rmse: 0.46645 | val_1_rmse: 0.482   |  0:00:21s
epoch 22 | loss: 0.2342  | val_0_rmse: 0.46779 | val_1_rmse: 0.48156 |  0:00:22s
epoch 23 | loss: 0.23291 | val_0_rmse: 0.4801  | val_1_rmse: 0.50188 |  0:00:23s
epoch 24 | loss: 0.23064 | val_0_rmse: 0.46004 | val_1_rmse: 0.4726  |  0:00:24s
epoch 25 | loss: 0.22734 | val_0_rmse: 0.47843 | val_1_rmse: 0.49116 |  0:00:24s
epoch 26 | loss: 0.22973 | val_0_rmse: 0.45997 | val_1_rmse: 0.47434 |  0:00:25s
epoch 27 | loss: 0.22262 | val_0_rmse: 0.46272 | val_1_rmse: 0.47756 |  0:00:26s
epoch 28 | loss: 0.22942 | val_0_rmse: 0.45521 | val_1_rmse: 0.46579 |  0:00:27s
epoch 29 | loss: 0.21718 | val_0_rmse: 0.4473  | val_1_rmse: 0.46427 |  0:00:28s
epoch 30 | loss: 0.21472 | val_0_rmse: 0.45061 | val_1_rmse: 0.47255 |  0:00:29s
epoch 31 | loss: 0.21537 | val_0_rmse: 0.44586 | val_1_rmse: 0.46286 |  0:00:30s
epoch 32 | loss: 0.21225 | val_0_rmse: 0.44114 | val_1_rmse: 0.45951 |  0:00:31s
epoch 33 | loss: 0.21139 | val_0_rmse: 0.43919 | val_1_rmse: 0.44926 |  0:00:32s
epoch 34 | loss: 0.20842 | val_0_rmse: 0.43781 | val_1_rmse: 0.44887 |  0:00:33s
epoch 35 | loss: 0.22176 | val_0_rmse: 0.46984 | val_1_rmse: 0.4858  |  0:00:34s
epoch 36 | loss: 0.21572 | val_0_rmse: 0.43965 | val_1_rmse: 0.45494 |  0:00:35s
epoch 37 | loss: 0.21022 | val_0_rmse: 0.44356 | val_1_rmse: 0.46084 |  0:00:36s
epoch 38 | loss: 0.20488 | val_0_rmse: 0.43809 | val_1_rmse: 0.45481 |  0:00:37s
epoch 39 | loss: 0.20519 | val_0_rmse: 0.43583 | val_1_rmse: 0.45412 |  0:00:38s
epoch 40 | loss: 0.20423 | val_0_rmse: 0.43549 | val_1_rmse: 0.45244 |  0:00:39s
epoch 41 | loss: 0.21059 | val_0_rmse: 0.44714 | val_1_rmse: 0.46071 |  0:00:40s
epoch 42 | loss: 0.21565 | val_0_rmse: 0.42937 | val_1_rmse: 0.44393 |  0:00:41s
epoch 43 | loss: 0.20539 | val_0_rmse: 0.44036 | val_1_rmse: 0.46324 |  0:00:42s
epoch 44 | loss: 0.21009 | val_0_rmse: 0.43148 | val_1_rmse: 0.44382 |  0:00:43s
epoch 45 | loss: 0.20063 | val_0_rmse: 0.43562 | val_1_rmse: 0.44728 |  0:00:44s
epoch 46 | loss: 0.20863 | val_0_rmse: 0.48607 | val_1_rmse: 0.50268 |  0:00:45s
epoch 47 | loss: 0.20648 | val_0_rmse: 0.44614 | val_1_rmse: 0.45893 |  0:00:46s
epoch 48 | loss: 0.20256 | val_0_rmse: 0.43783 | val_1_rmse: 0.44982 |  0:00:47s
epoch 49 | loss: 0.21038 | val_0_rmse: 0.43299 | val_1_rmse: 0.44585 |  0:00:47s
epoch 50 | loss: 0.20172 | val_0_rmse: 0.44047 | val_1_rmse: 0.45811 |  0:00:48s
epoch 51 | loss: 0.20265 | val_0_rmse: 0.45005 | val_1_rmse: 0.46857 |  0:00:49s
epoch 52 | loss: 0.20138 | val_0_rmse: 0.43322 | val_1_rmse: 0.45321 |  0:00:50s
epoch 53 | loss: 0.2038  | val_0_rmse: 0.43401 | val_1_rmse: 0.45219 |  0:00:51s
epoch 54 | loss: 0.19683 | val_0_rmse: 0.42897 | val_1_rmse: 0.44496 |  0:00:52s
epoch 55 | loss: 0.20096 | val_0_rmse: 0.44092 | val_1_rmse: 0.46171 |  0:00:53s
epoch 56 | loss: 0.20388 | val_0_rmse: 0.45038 | val_1_rmse: 0.4645  |  0:00:54s
epoch 57 | loss: 0.20194 | val_0_rmse: 0.42489 | val_1_rmse: 0.43965 |  0:00:55s
epoch 58 | loss: 0.20179 | val_0_rmse: 0.42982 | val_1_rmse: 0.44705 |  0:00:56s
epoch 59 | loss: 0.20027 | val_0_rmse: 0.44292 | val_1_rmse: 0.46224 |  0:00:57s
epoch 60 | loss: 0.20111 | val_0_rmse: 0.43491 | val_1_rmse: 0.45454 |  0:00:58s
epoch 61 | loss: 0.19499 | val_0_rmse: 0.43068 | val_1_rmse: 0.44477 |  0:00:59s
epoch 62 | loss: 0.19705 | val_0_rmse: 0.4334  | val_1_rmse: 0.45047 |  0:01:00s
epoch 63 | loss: 0.19948 | val_0_rmse: 0.42992 | val_1_rmse: 0.44183 |  0:01:01s
epoch 64 | loss: 0.20113 | val_0_rmse: 0.44036 | val_1_rmse: 0.45722 |  0:01:02s
epoch 65 | loss: 0.20306 | val_0_rmse: 0.42222 | val_1_rmse: 0.44206 |  0:01:03s
epoch 66 | loss: 0.20081 | val_0_rmse: 0.45135 | val_1_rmse: 0.46818 |  0:01:04s
epoch 67 | loss: 0.19545 | val_0_rmse: 0.42262 | val_1_rmse: 0.44249 |  0:01:05s
epoch 68 | loss: 0.19568 | val_0_rmse: 0.4373  | val_1_rmse: 0.4534  |  0:01:06s
epoch 69 | loss: 0.1989  | val_0_rmse: 0.42399 | val_1_rmse: 0.43949 |  0:01:07s
epoch 70 | loss: 0.19845 | val_0_rmse: 0.43423 | val_1_rmse: 0.45271 |  0:01:08s
epoch 71 | loss: 0.20122 | val_0_rmse: 0.42296 | val_1_rmse: 0.4441  |  0:01:09s
epoch 72 | loss: 0.19261 | val_0_rmse: 0.43049 | val_1_rmse: 0.44987 |  0:01:10s
epoch 73 | loss: 0.19405 | val_0_rmse: 0.42694 | val_1_rmse: 0.44445 |  0:01:11s
epoch 74 | loss: 0.1956  | val_0_rmse: 0.42739 | val_1_rmse: 0.44958 |  0:01:11s
epoch 75 | loss: 0.19596 | val_0_rmse: 0.42058 | val_1_rmse: 0.4359  |  0:01:12s
epoch 76 | loss: 0.19326 | val_0_rmse: 0.4229  | val_1_rmse: 0.43916 |  0:01:13s
epoch 77 | loss: 0.19794 | val_0_rmse: 0.42671 | val_1_rmse: 0.44386 |  0:01:14s
epoch 78 | loss: 0.19901 | val_0_rmse: 0.43292 | val_1_rmse: 0.45038 |  0:01:15s
epoch 79 | loss: 0.19384 | val_0_rmse: 0.43574 | val_1_rmse: 0.45326 |  0:01:16s
epoch 80 | loss: 0.19762 | val_0_rmse: 0.42461 | val_1_rmse: 0.44287 |  0:01:17s
epoch 81 | loss: 0.19635 | val_0_rmse: 0.42739 | val_1_rmse: 0.44841 |  0:01:18s
epoch 82 | loss: 0.19035 | val_0_rmse: 0.42441 | val_1_rmse: 0.44178 |  0:01:19s
epoch 83 | loss: 0.19367 | val_0_rmse: 0.42365 | val_1_rmse: 0.44307 |  0:01:20s
epoch 84 | loss: 0.18981 | val_0_rmse: 0.42016 | val_1_rmse: 0.43995 |  0:01:21s
epoch 85 | loss: 0.19286 | val_0_rmse: 0.4185  | val_1_rmse: 0.44023 |  0:01:22s
epoch 86 | loss: 0.1885  | val_0_rmse: 0.43371 | val_1_rmse: 0.45479 |  0:01:23s
epoch 87 | loss: 0.19174 | val_0_rmse: 0.41706 | val_1_rmse: 0.43953 |  0:01:24s
epoch 88 | loss: 0.19346 | val_0_rmse: 0.44435 | val_1_rmse: 0.47138 |  0:01:25s
epoch 89 | loss: 0.1946  | val_0_rmse: 0.42179 | val_1_rmse: 0.43879 |  0:01:26s
epoch 90 | loss: 0.19382 | val_0_rmse: 0.41722 | val_1_rmse: 0.44145 |  0:01:27s
epoch 91 | loss: 0.19329 | val_0_rmse: 0.4235  | val_1_rmse: 0.45088 |  0:01:28s
epoch 92 | loss: 0.18884 | val_0_rmse: 0.43278 | val_1_rmse: 0.45264 |  0:01:29s
epoch 93 | loss: 0.19299 | val_0_rmse: 0.42401 | val_1_rmse: 0.44506 |  0:01:30s
epoch 94 | loss: 0.18996 | val_0_rmse: 0.41876 | val_1_rmse: 0.44537 |  0:01:31s
epoch 95 | loss: 0.18849 | val_0_rmse: 0.4279  | val_1_rmse: 0.4503  |  0:01:31s
epoch 96 | loss: 0.19555 | val_0_rmse: 0.44474 | val_1_rmse: 0.46924 |  0:01:32s
epoch 97 | loss: 0.18922 | val_0_rmse: 0.41522 | val_1_rmse: 0.44241 |  0:01:33s
epoch 98 | loss: 0.18605 | val_0_rmse: 0.415   | val_1_rmse: 0.43605 |  0:01:34s
epoch 99 | loss: 0.18924 | val_0_rmse: 0.41376 | val_1_rmse: 0.43876 |  0:01:35s
epoch 100| loss: 0.18112 | val_0_rmse: 0.41697 | val_1_rmse: 0.43831 |  0:01:36s
epoch 101| loss: 0.19087 | val_0_rmse: 0.4138  | val_1_rmse: 0.43625 |  0:01:37s
epoch 102| loss: 0.18974 | val_0_rmse: 0.41676 | val_1_rmse: 0.43678 |  0:01:38s
epoch 103| loss: 0.18972 | val_0_rmse: 0.43353 | val_1_rmse: 0.45679 |  0:01:39s
epoch 104| loss: 0.18588 | val_0_rmse: 0.40871 | val_1_rmse: 0.43596 |  0:01:40s
epoch 105| loss: 0.18799 | val_0_rmse: 0.41891 | val_1_rmse: 0.44729 |  0:01:41s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.4359
Best weights from best epoch are automatically used!
ended training at: 07:17:29
Feature importance:
[('Area', 0.2609470656334938), ('Baths', 0.02389561391838388), ('Beds', 0.0), ('Latitude', 0.4144443667588024), ('Longitude', 0.26146158209773634), ('Month', 0.0), ('Year', 0.03925137159158361)]
Mean squared error is of 5878536334.972715
Mean absolute error:53803.4857554888
MAPE:0.13925900543645395
R2 score:0.8069557558166777
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:17:29
epoch 0  | loss: 0.64334 | val_0_rmse: 0.6257  | val_1_rmse: 0.63965 |  0:00:00s
epoch 1  | loss: 0.34534 | val_0_rmse: 0.6102  | val_1_rmse: 0.62016 |  0:00:01s
epoch 2  | loss: 0.29682 | val_0_rmse: 0.56519 | val_1_rmse: 0.58037 |  0:00:02s
epoch 3  | loss: 0.27111 | val_0_rmse: 0.50667 | val_1_rmse: 0.52359 |  0:00:03s
epoch 4  | loss: 0.26335 | val_0_rmse: 0.49653 | val_1_rmse: 0.51171 |  0:00:04s
epoch 5  | loss: 0.26182 | val_0_rmse: 0.51663 | val_1_rmse: 0.54427 |  0:00:05s
epoch 6  | loss: 0.26948 | val_0_rmse: 0.50856 | val_1_rmse: 0.52109 |  0:00:06s
epoch 7  | loss: 0.27082 | val_0_rmse: 0.51921 | val_1_rmse: 0.52773 |  0:00:07s
epoch 8  | loss: 0.27514 | val_0_rmse: 0.51059 | val_1_rmse: 0.51958 |  0:00:08s
epoch 9  | loss: 0.26853 | val_0_rmse: 0.50041 | val_1_rmse: 0.5056  |  0:00:09s
epoch 10 | loss: 0.25414 | val_0_rmse: 0.50669 | val_1_rmse: 0.52406 |  0:00:10s
epoch 11 | loss: 0.25827 | val_0_rmse: 0.48395 | val_1_rmse: 0.50045 |  0:00:11s
epoch 12 | loss: 0.24324 | val_0_rmse: 0.49573 | val_1_rmse: 0.50368 |  0:00:12s
epoch 13 | loss: 0.25495 | val_0_rmse: 0.49024 | val_1_rmse: 0.49727 |  0:00:13s
epoch 14 | loss: 0.25781 | val_0_rmse: 0.50096 | val_1_rmse: 0.51246 |  0:00:14s
epoch 15 | loss: 0.25621 | val_0_rmse: 0.48832 | val_1_rmse: 0.49614 |  0:00:15s
epoch 16 | loss: 0.25712 | val_0_rmse: 0.50319 | val_1_rmse: 0.50952 |  0:00:16s
epoch 17 | loss: 0.25413 | val_0_rmse: 0.49415 | val_1_rmse: 0.50672 |  0:00:17s
epoch 18 | loss: 0.24849 | val_0_rmse: 0.48238 | val_1_rmse: 0.50032 |  0:00:18s
epoch 19 | loss: 0.24703 | val_0_rmse: 0.48614 | val_1_rmse: 0.49801 |  0:00:19s
epoch 20 | loss: 0.25255 | val_0_rmse: 0.52767 | val_1_rmse: 0.53697 |  0:00:20s
epoch 21 | loss: 0.26232 | val_0_rmse: 0.49083 | val_1_rmse: 0.50897 |  0:00:21s
epoch 22 | loss: 0.25241 | val_0_rmse: 0.48999 | val_1_rmse: 0.50327 |  0:00:22s
epoch 23 | loss: 0.24076 | val_0_rmse: 0.47977 | val_1_rmse: 0.49422 |  0:00:23s
epoch 24 | loss: 0.24203 | val_0_rmse: 0.49029 | val_1_rmse: 0.50434 |  0:00:23s
epoch 25 | loss: 0.23997 | val_0_rmse: 0.47511 | val_1_rmse: 0.49154 |  0:00:25s
epoch 26 | loss: 0.23688 | val_0_rmse: 0.47599 | val_1_rmse: 0.49026 |  0:00:25s
epoch 27 | loss: 0.23768 | val_0_rmse: 0.47356 | val_1_rmse: 0.49272 |  0:00:26s
epoch 28 | loss: 0.23149 | val_0_rmse: 0.47455 | val_1_rmse: 0.48779 |  0:00:27s
epoch 29 | loss: 0.2333  | val_0_rmse: 0.46581 | val_1_rmse: 0.47567 |  0:00:28s
epoch 30 | loss: 0.23382 | val_0_rmse: 0.48344 | val_1_rmse: 0.50129 |  0:00:29s
epoch 31 | loss: 0.23976 | val_0_rmse: 0.47444 | val_1_rmse: 0.48965 |  0:00:30s
epoch 32 | loss: 0.23896 | val_0_rmse: 0.47884 | val_1_rmse: 0.49447 |  0:00:31s
epoch 33 | loss: 0.23503 | val_0_rmse: 0.47454 | val_1_rmse: 0.48992 |  0:00:32s
epoch 34 | loss: 0.23083 | val_0_rmse: 0.46695 | val_1_rmse: 0.48044 |  0:00:33s
epoch 35 | loss: 0.22982 | val_0_rmse: 0.46545 | val_1_rmse: 0.47599 |  0:00:34s
epoch 36 | loss: 0.229   | val_0_rmse: 0.47169 | val_1_rmse: 0.49151 |  0:00:35s
epoch 37 | loss: 0.23328 | val_0_rmse: 0.47678 | val_1_rmse: 0.49523 |  0:00:36s
epoch 38 | loss: 0.23145 | val_0_rmse: 0.46    | val_1_rmse: 0.47796 |  0:00:37s
epoch 39 | loss: 0.22529 | val_0_rmse: 0.46463 | val_1_rmse: 0.47856 |  0:00:38s
epoch 40 | loss: 0.22741 | val_0_rmse: 0.50608 | val_1_rmse: 0.51273 |  0:00:39s
epoch 41 | loss: 0.23795 | val_0_rmse: 0.48192 | val_1_rmse: 0.49331 |  0:00:40s
epoch 42 | loss: 0.24222 | val_0_rmse: 0.48244 | val_1_rmse: 0.49445 |  0:00:41s
epoch 43 | loss: 0.2354  | val_0_rmse: 0.47629 | val_1_rmse: 0.4881  |  0:00:42s
epoch 44 | loss: 0.23535 | val_0_rmse: 0.50952 | val_1_rmse: 0.52735 |  0:00:43s
epoch 45 | loss: 0.25186 | val_0_rmse: 0.48696 | val_1_rmse: 0.50182 |  0:00:44s
epoch 46 | loss: 0.23807 | val_0_rmse: 0.47457 | val_1_rmse: 0.48903 |  0:00:45s
epoch 47 | loss: 0.24167 | val_0_rmse: 0.51451 | val_1_rmse: 0.5271  |  0:00:46s
epoch 48 | loss: 0.2334  | val_0_rmse: 0.48284 | val_1_rmse: 0.50059 |  0:00:47s
epoch 49 | loss: 0.23766 | val_0_rmse: 0.46367 | val_1_rmse: 0.47774 |  0:00:47s
epoch 50 | loss: 0.23147 | val_0_rmse: 0.46373 | val_1_rmse: 0.48051 |  0:00:48s
epoch 51 | loss: 0.22826 | val_0_rmse: 0.47255 | val_1_rmse: 0.48959 |  0:00:49s
epoch 52 | loss: 0.22502 | val_0_rmse: 0.45071 | val_1_rmse: 0.46309 |  0:00:50s
epoch 53 | loss: 0.22427 | val_0_rmse: 0.4526  | val_1_rmse: 0.46391 |  0:00:51s
epoch 54 | loss: 0.21782 | val_0_rmse: 0.46023 | val_1_rmse: 0.47653 |  0:00:52s
epoch 55 | loss: 0.22162 | val_0_rmse: 0.45226 | val_1_rmse: 0.46924 |  0:00:53s
epoch 56 | loss: 0.22755 | val_0_rmse: 0.46496 | val_1_rmse: 0.47982 |  0:00:54s
epoch 57 | loss: 0.2238  | val_0_rmse: 0.45678 | val_1_rmse: 0.47449 |  0:00:55s
epoch 58 | loss: 0.22774 | val_0_rmse: 0.46061 | val_1_rmse: 0.47608 |  0:00:56s
epoch 59 | loss: 0.23249 | val_0_rmse: 0.45409 | val_1_rmse: 0.47151 |  0:00:57s
epoch 60 | loss: 0.22769 | val_0_rmse: 0.44553 | val_1_rmse: 0.46481 |  0:00:58s
epoch 61 | loss: 0.2185  | val_0_rmse: 0.46994 | val_1_rmse: 0.48935 |  0:00:59s
epoch 62 | loss: 0.21555 | val_0_rmse: 0.4492  | val_1_rmse: 0.46817 |  0:01:00s
epoch 63 | loss: 0.21236 | val_0_rmse: 0.44037 | val_1_rmse: 0.45985 |  0:01:01s
epoch 64 | loss: 0.21632 | val_0_rmse: 0.46077 | val_1_rmse: 0.4754  |  0:01:02s
epoch 65 | loss: 0.21681 | val_0_rmse: 0.43939 | val_1_rmse: 0.45956 |  0:01:03s
epoch 66 | loss: 0.21196 | val_0_rmse: 0.44122 | val_1_rmse: 0.45579 |  0:01:04s
epoch 67 | loss: 0.21171 | val_0_rmse: 0.44888 | val_1_rmse: 0.47387 |  0:01:05s
epoch 68 | loss: 0.21768 | val_0_rmse: 0.45376 | val_1_rmse: 0.47112 |  0:01:06s
epoch 69 | loss: 0.21633 | val_0_rmse: 0.43916 | val_1_rmse: 0.45447 |  0:01:07s
epoch 70 | loss: 0.20991 | val_0_rmse: 0.4354  | val_1_rmse: 0.45507 |  0:01:08s
epoch 71 | loss: 0.21316 | val_0_rmse: 0.48435 | val_1_rmse: 0.5043  |  0:01:09s
epoch 72 | loss: 0.22095 | val_0_rmse: 0.45072 | val_1_rmse: 0.46942 |  0:01:10s
epoch 73 | loss: 0.2162  | val_0_rmse: 0.43464 | val_1_rmse: 0.45112 |  0:01:11s
epoch 74 | loss: 0.20943 | val_0_rmse: 0.43283 | val_1_rmse: 0.44786 |  0:01:12s
epoch 75 | loss: 0.20581 | val_0_rmse: 0.4351  | val_1_rmse: 0.45137 |  0:01:12s
epoch 76 | loss: 0.20939 | val_0_rmse: 0.441   | val_1_rmse: 0.45468 |  0:01:13s
epoch 77 | loss: 0.21341 | val_0_rmse: 0.44252 | val_1_rmse: 0.45855 |  0:01:14s
epoch 78 | loss: 0.21632 | val_0_rmse: 0.44642 | val_1_rmse: 0.46389 |  0:01:15s
epoch 79 | loss: 0.20704 | val_0_rmse: 0.44543 | val_1_rmse: 0.46623 |  0:01:16s
epoch 80 | loss: 0.2073  | val_0_rmse: 0.44236 | val_1_rmse: 0.45677 |  0:01:17s
epoch 81 | loss: 0.2197  | val_0_rmse: 0.44267 | val_1_rmse: 0.45873 |  0:01:18s
epoch 82 | loss: 0.21283 | val_0_rmse: 0.43616 | val_1_rmse: 0.45422 |  0:01:19s
epoch 83 | loss: 0.20938 | val_0_rmse: 0.43737 | val_1_rmse: 0.45491 |  0:01:20s
epoch 84 | loss: 0.20693 | val_0_rmse: 0.43078 | val_1_rmse: 0.44752 |  0:01:21s
epoch 85 | loss: 0.20889 | val_0_rmse: 0.43492 | val_1_rmse: 0.45144 |  0:01:22s
epoch 86 | loss: 0.20662 | val_0_rmse: 0.44621 | val_1_rmse: 0.46309 |  0:01:23s
epoch 87 | loss: 0.21256 | val_0_rmse: 0.43852 | val_1_rmse: 0.45403 |  0:01:24s
epoch 88 | loss: 0.21017 | val_0_rmse: 0.42906 | val_1_rmse: 0.44611 |  0:01:25s
epoch 89 | loss: 0.21126 | val_0_rmse: 0.43629 | val_1_rmse: 0.45291 |  0:01:26s
epoch 90 | loss: 0.20456 | val_0_rmse: 0.42908 | val_1_rmse: 0.44624 |  0:01:27s
epoch 91 | loss: 0.20571 | val_0_rmse: 0.43361 | val_1_rmse: 0.44936 |  0:01:28s
epoch 92 | loss: 0.2053  | val_0_rmse: 0.46079 | val_1_rmse: 0.47884 |  0:01:29s
epoch 93 | loss: 0.21082 | val_0_rmse: 0.4322  | val_1_rmse: 0.44891 |  0:01:30s
epoch 94 | loss: 0.20368 | val_0_rmse: 0.43296 | val_1_rmse: 0.4471  |  0:01:31s
epoch 95 | loss: 0.20365 | val_0_rmse: 0.43132 | val_1_rmse: 0.44753 |  0:01:32s
epoch 96 | loss: 0.20741 | val_0_rmse: 0.43203 | val_1_rmse: 0.44791 |  0:01:33s
epoch 97 | loss: 0.20421 | val_0_rmse: 0.44369 | val_1_rmse: 0.46274 |  0:01:34s
epoch 98 | loss: 0.20869 | val_0_rmse: 0.4331  | val_1_rmse: 0.45174 |  0:01:35s
epoch 99 | loss: 0.20995 | val_0_rmse: 0.44024 | val_1_rmse: 0.46229 |  0:01:35s
epoch 100| loss: 0.20745 | val_0_rmse: 0.46688 | val_1_rmse: 0.49073 |  0:01:36s
epoch 101| loss: 0.21702 | val_0_rmse: 0.43748 | val_1_rmse: 0.45789 |  0:01:37s
epoch 102| loss: 0.21219 | val_0_rmse: 0.43958 | val_1_rmse: 0.46101 |  0:01:38s
epoch 103| loss: 0.20637 | val_0_rmse: 0.42834 | val_1_rmse: 0.44869 |  0:01:39s
epoch 104| loss: 0.20749 | val_0_rmse: 0.44233 | val_1_rmse: 0.46012 |  0:01:40s
epoch 105| loss: 0.20625 | val_0_rmse: 0.42907 | val_1_rmse: 0.4483  |  0:01:41s
epoch 106| loss: 0.20406 | val_0_rmse: 0.46524 | val_1_rmse: 0.48589 |  0:01:42s
epoch 107| loss: 0.20596 | val_0_rmse: 0.43132 | val_1_rmse: 0.44711 |  0:01:43s
epoch 108| loss: 0.20271 | val_0_rmse: 0.4341  | val_1_rmse: 0.45176 |  0:01:44s
epoch 109| loss: 0.20482 | val_0_rmse: 0.43267 | val_1_rmse: 0.45491 |  0:01:45s
epoch 110| loss: 0.20213 | val_0_rmse: 0.427   | val_1_rmse: 0.44485 |  0:01:46s
epoch 111| loss: 0.20565 | val_0_rmse: 0.43467 | val_1_rmse: 0.4551  |  0:01:47s
epoch 112| loss: 0.20093 | val_0_rmse: 0.42756 | val_1_rmse: 0.44953 |  0:01:48s
epoch 113| loss: 0.2013  | val_0_rmse: 0.44061 | val_1_rmse: 0.46144 |  0:01:49s
epoch 114| loss: 0.2064  | val_0_rmse: 0.43141 | val_1_rmse: 0.45253 |  0:01:50s
epoch 115| loss: 0.20488 | val_0_rmse: 0.42965 | val_1_rmse: 0.45089 |  0:01:51s
epoch 116| loss: 0.20091 | val_0_rmse: 0.42835 | val_1_rmse: 0.44704 |  0:01:52s
epoch 117| loss: 0.2049  | val_0_rmse: 0.42952 | val_1_rmse: 0.44928 |  0:01:53s
epoch 118| loss: 0.20506 | val_0_rmse: 0.42813 | val_1_rmse: 0.44848 |  0:01:54s
epoch 119| loss: 0.20148 | val_0_rmse: 0.42898 | val_1_rmse: 0.45409 |  0:01:55s
epoch 120| loss: 0.19994 | val_0_rmse: 0.43204 | val_1_rmse: 0.45118 |  0:01:56s
epoch 121| loss: 0.20234 | val_0_rmse: 0.42714 | val_1_rmse: 0.45018 |  0:01:56s
epoch 122| loss: 0.2002  | val_0_rmse: 0.43642 | val_1_rmse: 0.45332 |  0:01:57s
epoch 123| loss: 0.20316 | val_0_rmse: 0.43349 | val_1_rmse: 0.4489  |  0:01:58s
epoch 124| loss: 0.20187 | val_0_rmse: 0.43509 | val_1_rmse: 0.45288 |  0:01:59s
epoch 125| loss: 0.20141 | val_0_rmse: 0.43514 | val_1_rmse: 0.45285 |  0:02:00s
epoch 126| loss: 0.20803 | val_0_rmse: 0.43693 | val_1_rmse: 0.45544 |  0:02:01s
epoch 127| loss: 0.19965 | val_0_rmse: 0.4254  | val_1_rmse: 0.44136 |  0:02:02s
epoch 128| loss: 0.20023 | val_0_rmse: 0.42543 | val_1_rmse: 0.44419 |  0:02:03s
epoch 129| loss: 0.20214 | val_0_rmse: 0.42113 | val_1_rmse: 0.43887 |  0:02:04s
epoch 130| loss: 0.19973 | val_0_rmse: 0.44003 | val_1_rmse: 0.46131 |  0:02:05s
epoch 131| loss: 0.20209 | val_0_rmse: 0.4391  | val_1_rmse: 0.45832 |  0:02:06s
epoch 132| loss: 0.20083 | val_0_rmse: 0.43972 | val_1_rmse: 0.45442 |  0:02:07s
epoch 133| loss: 0.19787 | val_0_rmse: 0.42917 | val_1_rmse: 0.45041 |  0:02:08s
epoch 134| loss: 0.19684 | val_0_rmse: 0.42576 | val_1_rmse: 0.44612 |  0:02:09s
epoch 135| loss: 0.20754 | val_0_rmse: 0.42897 | val_1_rmse: 0.4491  |  0:02:10s
epoch 136| loss: 0.20064 | val_0_rmse: 0.44788 | val_1_rmse: 0.47075 |  0:02:11s
epoch 137| loss: 0.20224 | val_0_rmse: 0.42708 | val_1_rmse: 0.44672 |  0:02:12s
epoch 138| loss: 0.20296 | val_0_rmse: 0.42695 | val_1_rmse: 0.44344 |  0:02:13s
epoch 139| loss: 0.20164 | val_0_rmse: 0.43494 | val_1_rmse: 0.45871 |  0:02:14s
epoch 140| loss: 0.20333 | val_0_rmse: 0.43651 | val_1_rmse: 0.45357 |  0:02:15s
epoch 141| loss: 0.19978 | val_0_rmse: 0.4366  | val_1_rmse: 0.45621 |  0:02:16s
epoch 142| loss: 0.20231 | val_0_rmse: 0.43139 | val_1_rmse: 0.45393 |  0:02:17s
epoch 143| loss: 0.20251 | val_0_rmse: 0.42838 | val_1_rmse: 0.45072 |  0:02:18s
epoch 144| loss: 0.19773 | val_0_rmse: 0.43785 | val_1_rmse: 0.46025 |  0:02:19s
epoch 145| loss: 0.19813 | val_0_rmse: 0.45264 | val_1_rmse: 0.47119 |  0:02:19s
epoch 146| loss: 0.19983 | val_0_rmse: 0.4415  | val_1_rmse: 0.45935 |  0:02:20s
epoch 147| loss: 0.20342 | val_0_rmse: 0.43171 | val_1_rmse: 0.45111 |  0:02:21s
epoch 148| loss: 0.20194 | val_0_rmse: 0.44394 | val_1_rmse: 0.46625 |  0:02:22s
epoch 149| loss: 0.20203 | val_0_rmse: 0.43129 | val_1_rmse: 0.45166 |  0:02:23s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.43887
Best weights from best epoch are automatically used!
ended training at: 07:19:53
Feature importance:
[('Area', 0.2719523796307356), ('Baths', 0.057061884231131135), ('Beds', 0.0), ('Latitude', 0.2911087148387152), ('Longitude', 0.3798770212994181), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 5718411537.9478655
Mean absolute error:53395.37064796175
MAPE:0.13988501075580095
R2 score:0.8094402084874718
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:19:53
epoch 0  | loss: 0.684   | val_0_rmse: 0.68828 | val_1_rmse: 0.71497 |  0:00:00s
epoch 1  | loss: 0.32255 | val_0_rmse: 0.58377 | val_1_rmse: 0.58872 |  0:00:01s
epoch 2  | loss: 0.2904  | val_0_rmse: 0.57998 | val_1_rmse: 0.58739 |  0:00:02s
epoch 3  | loss: 0.28306 | val_0_rmse: 0.50016 | val_1_rmse: 0.50975 |  0:00:03s
epoch 4  | loss: 0.26222 | val_0_rmse: 0.49537 | val_1_rmse: 0.51043 |  0:00:04s
epoch 5  | loss: 0.25841 | val_0_rmse: 0.49879 | val_1_rmse: 0.50554 |  0:00:05s
epoch 6  | loss: 0.25179 | val_0_rmse: 0.48588 | val_1_rmse: 0.49973 |  0:00:06s
epoch 7  | loss: 0.25418 | val_0_rmse: 0.50699 | val_1_rmse: 0.51605 |  0:00:07s
epoch 8  | loss: 0.26728 | val_0_rmse: 0.50796 | val_1_rmse: 0.52328 |  0:00:08s
epoch 9  | loss: 0.26678 | val_0_rmse: 0.50648 | val_1_rmse: 0.52234 |  0:00:09s
epoch 10 | loss: 0.26113 | val_0_rmse: 0.49802 | val_1_rmse: 0.50472 |  0:00:10s
epoch 11 | loss: 0.2533  | val_0_rmse: 0.48549 | val_1_rmse: 0.49246 |  0:00:11s
epoch 12 | loss: 0.25078 | val_0_rmse: 0.48406 | val_1_rmse: 0.4961  |  0:00:12s
epoch 13 | loss: 0.24409 | val_0_rmse: 0.48669 | val_1_rmse: 0.49467 |  0:00:13s
epoch 14 | loss: 0.25531 | val_0_rmse: 0.48787 | val_1_rmse: 0.50953 |  0:00:14s
epoch 15 | loss: 0.26151 | val_0_rmse: 0.49587 | val_1_rmse: 0.50947 |  0:00:15s
epoch 16 | loss: 0.24873 | val_0_rmse: 0.48259 | val_1_rmse: 0.49651 |  0:00:16s
epoch 17 | loss: 0.24678 | val_0_rmse: 0.48333 | val_1_rmse: 0.49031 |  0:00:17s
epoch 18 | loss: 0.25115 | val_0_rmse: 0.48312 | val_1_rmse: 0.49127 |  0:00:18s
epoch 19 | loss: 0.24352 | val_0_rmse: 0.50301 | val_1_rmse: 0.51531 |  0:00:19s
epoch 20 | loss: 0.2532  | val_0_rmse: 0.51293 | val_1_rmse: 0.53123 |  0:00:20s
epoch 21 | loss: 0.2489  | val_0_rmse: 0.49319 | val_1_rmse: 0.50505 |  0:00:21s
epoch 22 | loss: 0.24688 | val_0_rmse: 0.49863 | val_1_rmse: 0.5097  |  0:00:22s
epoch 23 | loss: 0.25167 | val_0_rmse: 0.49215 | val_1_rmse: 0.50283 |  0:00:23s
epoch 24 | loss: 0.24744 | val_0_rmse: 0.49995 | val_1_rmse: 0.51203 |  0:00:23s
epoch 25 | loss: 0.25588 | val_0_rmse: 0.5002  | val_1_rmse: 0.51328 |  0:00:24s
epoch 26 | loss: 0.26143 | val_0_rmse: 0.49615 | val_1_rmse: 0.50597 |  0:00:25s
epoch 27 | loss: 0.2491  | val_0_rmse: 0.48964 | val_1_rmse: 0.49915 |  0:00:26s
epoch 28 | loss: 0.24381 | val_0_rmse: 0.47968 | val_1_rmse: 0.48604 |  0:00:27s
epoch 29 | loss: 0.24101 | val_0_rmse: 0.48003 | val_1_rmse: 0.48912 |  0:00:28s
epoch 30 | loss: 0.24141 | val_0_rmse: 0.49508 | val_1_rmse: 0.5043  |  0:00:29s
epoch 31 | loss: 0.24663 | val_0_rmse: 0.4933  | val_1_rmse: 0.50173 |  0:00:30s
epoch 32 | loss: 0.24369 | val_0_rmse: 0.47495 | val_1_rmse: 0.48802 |  0:00:31s
epoch 33 | loss: 0.23501 | val_0_rmse: 0.47066 | val_1_rmse: 0.48284 |  0:00:32s
epoch 34 | loss: 0.23939 | val_0_rmse: 0.47446 | val_1_rmse: 0.48944 |  0:00:33s
epoch 35 | loss: 0.23479 | val_0_rmse: 0.46735 | val_1_rmse: 0.481   |  0:00:34s
epoch 36 | loss: 0.24014 | val_0_rmse: 0.46641 | val_1_rmse: 0.47908 |  0:00:35s
epoch 37 | loss: 0.22923 | val_0_rmse: 0.46424 | val_1_rmse: 0.47408 |  0:00:36s
epoch 38 | loss: 0.23033 | val_0_rmse: 0.46341 | val_1_rmse: 0.47609 |  0:00:37s
epoch 39 | loss: 0.23601 | val_0_rmse: 0.46251 | val_1_rmse: 0.47501 |  0:00:38s
epoch 40 | loss: 0.23095 | val_0_rmse: 0.47191 | val_1_rmse: 0.48211 |  0:00:39s
epoch 41 | loss: 0.22945 | val_0_rmse: 0.48252 | val_1_rmse: 0.4876  |  0:00:40s
epoch 42 | loss: 0.23068 | val_0_rmse: 0.47179 | val_1_rmse: 0.48579 |  0:00:41s
epoch 43 | loss: 0.23465 | val_0_rmse: 0.48158 | val_1_rmse: 0.49403 |  0:00:42s
epoch 44 | loss: 0.22642 | val_0_rmse: 0.47458 | val_1_rmse: 0.47947 |  0:00:43s
epoch 45 | loss: 0.22371 | val_0_rmse: 0.46342 | val_1_rmse: 0.46965 |  0:00:44s
epoch 46 | loss: 0.2312  | val_0_rmse: 0.46077 | val_1_rmse: 0.47183 |  0:00:45s
epoch 47 | loss: 0.2266  | val_0_rmse: 0.46219 | val_1_rmse: 0.47118 |  0:00:46s
epoch 48 | loss: 0.22616 | val_0_rmse: 0.49729 | val_1_rmse: 0.50552 |  0:00:47s
epoch 49 | loss: 0.23511 | val_0_rmse: 0.47163 | val_1_rmse: 0.48801 |  0:00:48s
epoch 50 | loss: 0.23537 | val_0_rmse: 0.4862  | val_1_rmse: 0.49635 |  0:00:49s
epoch 51 | loss: 0.2507  | val_0_rmse: 0.51675 | val_1_rmse: 0.51462 |  0:00:50s
epoch 52 | loss: 0.236   | val_0_rmse: 0.474   | val_1_rmse: 0.49065 |  0:00:51s
epoch 53 | loss: 0.23268 | val_0_rmse: 0.46692 | val_1_rmse: 0.47835 |  0:00:52s
epoch 54 | loss: 0.22605 | val_0_rmse: 0.47003 | val_1_rmse: 0.48326 |  0:00:53s
epoch 55 | loss: 0.23039 | val_0_rmse: 0.46996 | val_1_rmse: 0.48239 |  0:00:53s
epoch 56 | loss: 0.22802 | val_0_rmse: 0.45544 | val_1_rmse: 0.47012 |  0:00:54s
epoch 57 | loss: 0.23331 | val_0_rmse: 0.52122 | val_1_rmse: 0.52788 |  0:00:55s
epoch 58 | loss: 0.24083 | val_0_rmse: 0.48631 | val_1_rmse: 0.49483 |  0:00:56s
epoch 59 | loss: 0.24565 | val_0_rmse: 0.5043  | val_1_rmse: 0.51711 |  0:00:57s
epoch 60 | loss: 0.2388  | val_0_rmse: 0.48328 | val_1_rmse: 0.49242 |  0:00:58s
epoch 61 | loss: 0.23763 | val_0_rmse: 0.50274 | val_1_rmse: 0.5115  |  0:00:59s
epoch 62 | loss: 0.24425 | val_0_rmse: 0.54663 | val_1_rmse: 0.56061 |  0:01:00s
epoch 63 | loss: 0.24392 | val_0_rmse: 0.47515 | val_1_rmse: 0.4864  |  0:01:01s
epoch 64 | loss: 0.23571 | val_0_rmse: 0.49022 | val_1_rmse: 0.50615 |  0:01:02s
epoch 65 | loss: 0.2288  | val_0_rmse: 0.46269 | val_1_rmse: 0.47547 |  0:01:03s
epoch 66 | loss: 0.22735 | val_0_rmse: 0.46563 | val_1_rmse: 0.4765  |  0:01:04s
epoch 67 | loss: 0.22903 | val_0_rmse: 0.4686  | val_1_rmse: 0.48372 |  0:01:05s
epoch 68 | loss: 0.22711 | val_0_rmse: 0.46452 | val_1_rmse: 0.47973 |  0:01:06s
epoch 69 | loss: 0.2227  | val_0_rmse: 0.45782 | val_1_rmse: 0.47294 |  0:01:07s
epoch 70 | loss: 0.22538 | val_0_rmse: 0.46432 | val_1_rmse: 0.47433 |  0:01:08s
epoch 71 | loss: 0.22519 | val_0_rmse: 0.46141 | val_1_rmse: 0.47648 |  0:01:09s
epoch 72 | loss: 0.23077 | val_0_rmse: 0.48115 | val_1_rmse: 0.49313 |  0:01:10s
epoch 73 | loss: 0.23983 | val_0_rmse: 0.47635 | val_1_rmse: 0.48764 |  0:01:11s
epoch 74 | loss: 0.23051 | val_0_rmse: 0.46045 | val_1_rmse: 0.47966 |  0:01:12s
epoch 75 | loss: 0.23117 | val_0_rmse: 0.46448 | val_1_rmse: 0.4713  |  0:01:13s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.46965
Best weights from best epoch are automatically used!
ended training at: 07:21:07
Feature importance:
[('Area', 0.33773157065729137), ('Baths', 0.009924779176774336), ('Beds', 0.005801993081547109), ('Latitude', 0.40405004750707163), ('Longitude', 0.11773055618223505), ('Month', 0.024630696337291987), ('Year', 0.1001303570577885)]
Mean squared error is of 7201609739.86543
Mean absolute error:60897.12038553725
MAPE:0.1632373426255989
R2 score:0.7679405635665093
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:21:07
epoch 0  | loss: 0.96244 | val_0_rmse: 1.0477  | val_1_rmse: 1.02295 |  0:00:00s
epoch 1  | loss: 0.47421 | val_0_rmse: 0.86202 | val_1_rmse: 0.8635  |  0:00:00s
epoch 2  | loss: 0.37675 | val_0_rmse: 0.68549 | val_1_rmse: 0.69639 |  0:00:01s
epoch 3  | loss: 0.35299 | val_0_rmse: 0.62086 | val_1_rmse: 0.63634 |  0:00:01s
epoch 4  | loss: 0.33141 | val_0_rmse: 0.57121 | val_1_rmse: 0.58658 |  0:00:02s
epoch 5  | loss: 0.30512 | val_0_rmse: 0.53247 | val_1_rmse: 0.53964 |  0:00:02s
epoch 6  | loss: 0.30225 | val_0_rmse: 0.5255  | val_1_rmse: 0.52741 |  0:00:03s
epoch 7  | loss: 0.29629 | val_0_rmse: 0.53445 | val_1_rmse: 0.53745 |  0:00:03s
epoch 8  | loss: 0.29629 | val_0_rmse: 0.51596 | val_1_rmse: 0.51826 |  0:00:04s
epoch 9  | loss: 0.29059 | val_0_rmse: 0.51968 | val_1_rmse: 0.52387 |  0:00:04s
epoch 10 | loss: 0.29364 | val_0_rmse: 0.52275 | val_1_rmse: 0.53558 |  0:00:05s
epoch 11 | loss: 0.2907  | val_0_rmse: 0.52836 | val_1_rmse: 0.54039 |  0:00:05s
epoch 12 | loss: 0.28405 | val_0_rmse: 0.51488 | val_1_rmse: 0.52145 |  0:00:06s
epoch 13 | loss: 0.28909 | val_0_rmse: 0.52716 | val_1_rmse: 0.52574 |  0:00:06s
epoch 14 | loss: 0.28099 | val_0_rmse: 0.51594 | val_1_rmse: 0.51934 |  0:00:06s
epoch 15 | loss: 0.279   | val_0_rmse: 0.50447 | val_1_rmse: 0.51139 |  0:00:07s
epoch 16 | loss: 0.28026 | val_0_rmse: 0.5131  | val_1_rmse: 0.52132 |  0:00:07s
epoch 17 | loss: 0.26878 | val_0_rmse: 0.50476 | val_1_rmse: 0.51649 |  0:00:08s
epoch 18 | loss: 0.27173 | val_0_rmse: 0.49871 | val_1_rmse: 0.51174 |  0:00:08s
epoch 19 | loss: 0.26221 | val_0_rmse: 0.49791 | val_1_rmse: 0.50614 |  0:00:09s
epoch 20 | loss: 0.26304 | val_0_rmse: 0.49637 | val_1_rmse: 0.50137 |  0:00:09s
epoch 21 | loss: 0.26483 | val_0_rmse: 0.51712 | val_1_rmse: 0.51836 |  0:00:10s
epoch 22 | loss: 0.25792 | val_0_rmse: 0.49514 | val_1_rmse: 0.49909 |  0:00:10s
epoch 23 | loss: 0.2664  | val_0_rmse: 0.48869 | val_1_rmse: 0.49799 |  0:00:11s
epoch 24 | loss: 0.25914 | val_0_rmse: 0.51258 | val_1_rmse: 0.52178 |  0:00:11s
epoch 25 | loss: 0.26376 | val_0_rmse: 0.50499 | val_1_rmse: 0.51323 |  0:00:12s
epoch 26 | loss: 0.27357 | val_0_rmse: 0.5111  | val_1_rmse: 0.51114 |  0:00:12s
epoch 27 | loss: 0.26911 | val_0_rmse: 0.4997  | val_1_rmse: 0.50239 |  0:00:12s
epoch 28 | loss: 0.26696 | val_0_rmse: 0.49442 | val_1_rmse: 0.49676 |  0:00:13s
epoch 29 | loss: 0.262   | val_0_rmse: 0.49527 | val_1_rmse: 0.49916 |  0:00:13s
epoch 30 | loss: 0.26619 | val_0_rmse: 0.49885 | val_1_rmse: 0.50074 |  0:00:14s
epoch 31 | loss: 0.26013 | val_0_rmse: 0.49227 | val_1_rmse: 0.50313 |  0:00:14s
epoch 32 | loss: 0.2686  | val_0_rmse: 0.49594 | val_1_rmse: 0.49966 |  0:00:15s
epoch 33 | loss: 0.26528 | val_0_rmse: 0.49095 | val_1_rmse: 0.50135 |  0:00:15s
epoch 34 | loss: 0.26243 | val_0_rmse: 0.50947 | val_1_rmse: 0.52225 |  0:00:16s
epoch 35 | loss: 0.26358 | val_0_rmse: 0.4934  | val_1_rmse: 0.50266 |  0:00:16s
epoch 36 | loss: 0.26341 | val_0_rmse: 0.48548 | val_1_rmse: 0.49561 |  0:00:17s
epoch 37 | loss: 0.25908 | val_0_rmse: 0.49197 | val_1_rmse: 0.50108 |  0:00:17s
epoch 38 | loss: 0.25035 | val_0_rmse: 0.48441 | val_1_rmse: 0.49467 |  0:00:17s
epoch 39 | loss: 0.25315 | val_0_rmse: 0.48576 | val_1_rmse: 0.49098 |  0:00:18s
epoch 40 | loss: 0.25682 | val_0_rmse: 0.49098 | val_1_rmse: 0.50112 |  0:00:18s
epoch 41 | loss: 0.25489 | val_0_rmse: 0.48065 | val_1_rmse: 0.48908 |  0:00:19s
epoch 42 | loss: 0.25196 | val_0_rmse: 0.47549 | val_1_rmse: 0.48665 |  0:00:19s
epoch 43 | loss: 0.25802 | val_0_rmse: 0.49898 | val_1_rmse: 0.51026 |  0:00:20s
epoch 44 | loss: 0.2677  | val_0_rmse: 0.50015 | val_1_rmse: 0.51757 |  0:00:20s
epoch 45 | loss: 0.25995 | val_0_rmse: 0.4973  | val_1_rmse: 0.50888 |  0:00:21s
epoch 46 | loss: 0.2663  | val_0_rmse: 0.49574 | val_1_rmse: 0.5044  |  0:00:21s
epoch 47 | loss: 0.2663  | val_0_rmse: 0.49238 | val_1_rmse: 0.50326 |  0:00:22s
epoch 48 | loss: 0.26068 | val_0_rmse: 0.48594 | val_1_rmse: 0.494   |  0:00:22s
epoch 49 | loss: 0.24742 | val_0_rmse: 0.47675 | val_1_rmse: 0.49426 |  0:00:23s
epoch 50 | loss: 0.24698 | val_0_rmse: 0.47706 | val_1_rmse: 0.49912 |  0:00:23s
epoch 51 | loss: 0.24188 | val_0_rmse: 0.4775  | val_1_rmse: 0.49068 |  0:00:23s
epoch 52 | loss: 0.23947 | val_0_rmse: 0.47522 | val_1_rmse: 0.48636 |  0:00:24s
epoch 53 | loss: 0.23841 | val_0_rmse: 0.47345 | val_1_rmse: 0.4826  |  0:00:24s
epoch 54 | loss: 0.2471  | val_0_rmse: 0.49912 | val_1_rmse: 0.50848 |  0:00:25s
epoch 55 | loss: 0.25001 | val_0_rmse: 0.47771 | val_1_rmse: 0.49102 |  0:00:25s
epoch 56 | loss: 0.24213 | val_0_rmse: 0.47373 | val_1_rmse: 0.49056 |  0:00:26s
epoch 57 | loss: 0.24178 | val_0_rmse: 0.4701  | val_1_rmse: 0.48336 |  0:00:26s
epoch 58 | loss: 0.24408 | val_0_rmse: 0.49527 | val_1_rmse: 0.50434 |  0:00:27s
epoch 59 | loss: 0.24909 | val_0_rmse: 0.48733 | val_1_rmse: 0.49692 |  0:00:27s
epoch 60 | loss: 0.24098 | val_0_rmse: 0.47513 | val_1_rmse: 0.48511 |  0:00:28s
epoch 61 | loss: 0.24161 | val_0_rmse: 0.47325 | val_1_rmse: 0.48895 |  0:00:28s
epoch 62 | loss: 0.24229 | val_0_rmse: 0.47731 | val_1_rmse: 0.49566 |  0:00:28s
epoch 63 | loss: 0.24363 | val_0_rmse: 0.47258 | val_1_rmse: 0.4927  |  0:00:29s
epoch 64 | loss: 0.24733 | val_0_rmse: 0.48845 | val_1_rmse: 0.50207 |  0:00:29s
epoch 65 | loss: 0.23941 | val_0_rmse: 0.46649 | val_1_rmse: 0.48684 |  0:00:30s
epoch 66 | loss: 0.23658 | val_0_rmse: 0.4716  | val_1_rmse: 0.49303 |  0:00:30s
epoch 67 | loss: 0.23472 | val_0_rmse: 0.47819 | val_1_rmse: 0.49756 |  0:00:31s
epoch 68 | loss: 0.24145 | val_0_rmse: 0.47091 | val_1_rmse: 0.48777 |  0:00:31s
epoch 69 | loss: 0.2328  | val_0_rmse: 0.47057 | val_1_rmse: 0.48971 |  0:00:32s
epoch 70 | loss: 0.23152 | val_0_rmse: 0.46549 | val_1_rmse: 0.48179 |  0:00:32s
epoch 71 | loss: 0.23251 | val_0_rmse: 0.46042 | val_1_rmse: 0.4793  |  0:00:33s
epoch 72 | loss: 0.23192 | val_0_rmse: 0.46795 | val_1_rmse: 0.48626 |  0:00:33s
epoch 73 | loss: 0.23573 | val_0_rmse: 0.46936 | val_1_rmse: 0.48432 |  0:00:34s
epoch 74 | loss: 0.24256 | val_0_rmse: 0.47521 | val_1_rmse: 0.49326 |  0:00:34s
epoch 75 | loss: 0.23831 | val_0_rmse: 0.47514 | val_1_rmse: 0.49207 |  0:00:34s
epoch 76 | loss: 0.23657 | val_0_rmse: 0.47009 | val_1_rmse: 0.48435 |  0:00:35s
epoch 77 | loss: 0.23436 | val_0_rmse: 0.46342 | val_1_rmse: 0.4817  |  0:00:35s
epoch 78 | loss: 0.23355 | val_0_rmse: 0.46891 | val_1_rmse: 0.49288 |  0:00:36s
epoch 79 | loss: 0.23862 | val_0_rmse: 0.46385 | val_1_rmse: 0.48546 |  0:00:36s
epoch 80 | loss: 0.23957 | val_0_rmse: 0.46235 | val_1_rmse: 0.48025 |  0:00:37s
epoch 81 | loss: 0.23133 | val_0_rmse: 0.46143 | val_1_rmse: 0.4818  |  0:00:37s
epoch 82 | loss: 0.23359 | val_0_rmse: 0.46677 | val_1_rmse: 0.48334 |  0:00:38s
epoch 83 | loss: 0.24543 | val_0_rmse: 0.49422 | val_1_rmse: 0.525   |  0:00:38s
epoch 84 | loss: 0.24367 | val_0_rmse: 0.49273 | val_1_rmse: 0.51616 |  0:00:39s
epoch 85 | loss: 0.246   | val_0_rmse: 0.47404 | val_1_rmse: 0.49755 |  0:00:39s
epoch 86 | loss: 0.24408 | val_0_rmse: 0.47258 | val_1_rmse: 0.48969 |  0:00:40s
epoch 87 | loss: 0.24406 | val_0_rmse: 0.47776 | val_1_rmse: 0.48923 |  0:00:40s
epoch 88 | loss: 0.24339 | val_0_rmse: 0.48056 | val_1_rmse: 0.49634 |  0:00:40s
epoch 89 | loss: 0.24547 | val_0_rmse: 0.47698 | val_1_rmse: 0.48935 |  0:00:41s
epoch 90 | loss: 0.23457 | val_0_rmse: 0.46824 | val_1_rmse: 0.48606 |  0:00:41s
epoch 91 | loss: 0.23457 | val_0_rmse: 0.47064 | val_1_rmse: 0.48273 |  0:00:42s
epoch 92 | loss: 0.23115 | val_0_rmse: 0.47109 | val_1_rmse: 0.49067 |  0:00:42s
epoch 93 | loss: 0.23097 | val_0_rmse: 0.46794 | val_1_rmse: 0.48565 |  0:00:43s
epoch 94 | loss: 0.23128 | val_0_rmse: 0.47264 | val_1_rmse: 0.49118 |  0:00:43s
epoch 95 | loss: 0.23699 | val_0_rmse: 0.4638  | val_1_rmse: 0.48655 |  0:00:44s
epoch 96 | loss: 0.23357 | val_0_rmse: 0.47335 | val_1_rmse: 0.48542 |  0:00:44s
epoch 97 | loss: 0.22605 | val_0_rmse: 0.45831 | val_1_rmse: 0.47799 |  0:00:45s
epoch 98 | loss: 0.22823 | val_0_rmse: 0.45885 | val_1_rmse: 0.47753 |  0:00:45s
epoch 99 | loss: 0.22345 | val_0_rmse: 0.45876 | val_1_rmse: 0.48071 |  0:00:45s
epoch 100| loss: 0.22814 | val_0_rmse: 0.4549  | val_1_rmse: 0.47758 |  0:00:46s
epoch 101| loss: 0.2273  | val_0_rmse: 0.4638  | val_1_rmse: 0.48336 |  0:00:46s
epoch 102| loss: 0.22766 | val_0_rmse: 0.455   | val_1_rmse: 0.48068 |  0:00:47s
epoch 103| loss: 0.23406 | val_0_rmse: 0.45771 | val_1_rmse: 0.48143 |  0:00:47s
epoch 104| loss: 0.22753 | val_0_rmse: 0.45851 | val_1_rmse: 0.48128 |  0:00:48s
epoch 105| loss: 0.22212 | val_0_rmse: 0.45752 | val_1_rmse: 0.48138 |  0:00:48s
epoch 106| loss: 0.22467 | val_0_rmse: 0.45482 | val_1_rmse: 0.48156 |  0:00:49s
epoch 107| loss: 0.22434 | val_0_rmse: 0.45212 | val_1_rmse: 0.48005 |  0:00:49s
epoch 108| loss: 0.22529 | val_0_rmse: 0.45466 | val_1_rmse: 0.48615 |  0:00:50s
epoch 109| loss: 0.22463 | val_0_rmse: 0.46095 | val_1_rmse: 0.4863  |  0:00:50s
epoch 110| loss: 0.2299  | val_0_rmse: 0.45422 | val_1_rmse: 0.48003 |  0:00:50s
epoch 111| loss: 0.22502 | val_0_rmse: 0.45418 | val_1_rmse: 0.48562 |  0:00:51s
epoch 112| loss: 0.22528 | val_0_rmse: 0.46897 | val_1_rmse: 0.49495 |  0:00:51s
epoch 113| loss: 0.22984 | val_0_rmse: 0.45713 | val_1_rmse: 0.48406 |  0:00:52s
epoch 114| loss: 0.22453 | val_0_rmse: 0.45911 | val_1_rmse: 0.4892  |  0:00:52s
epoch 115| loss: 0.22238 | val_0_rmse: 0.46184 | val_1_rmse: 0.48425 |  0:00:53s
epoch 116| loss: 0.22178 | val_0_rmse: 0.45477 | val_1_rmse: 0.48236 |  0:00:53s
epoch 117| loss: 0.22649 | val_0_rmse: 0.46014 | val_1_rmse: 0.48677 |  0:00:54s
epoch 118| loss: 0.22644 | val_0_rmse: 0.45861 | val_1_rmse: 0.48089 |  0:00:54s
epoch 119| loss: 0.23089 | val_0_rmse: 0.45085 | val_1_rmse: 0.4819  |  0:00:55s
epoch 120| loss: 0.22705 | val_0_rmse: 0.45224 | val_1_rmse: 0.47888 |  0:00:55s
epoch 121| loss: 0.22403 | val_0_rmse: 0.46173 | val_1_rmse: 0.48551 |  0:00:56s
epoch 122| loss: 0.22674 | val_0_rmse: 0.45726 | val_1_rmse: 0.48475 |  0:00:56s
epoch 123| loss: 0.23107 | val_0_rmse: 0.45628 | val_1_rmse: 0.48629 |  0:00:56s
epoch 124| loss: 0.23018 | val_0_rmse: 0.46338 | val_1_rmse: 0.48586 |  0:00:57s
epoch 125| loss: 0.22545 | val_0_rmse: 0.45029 | val_1_rmse: 0.47707 |  0:00:57s
epoch 126| loss: 0.22534 | val_0_rmse: 0.46703 | val_1_rmse: 0.50189 |  0:00:58s
epoch 127| loss: 0.2266  | val_0_rmse: 0.4632  | val_1_rmse: 0.48926 |  0:00:58s
epoch 128| loss: 0.22183 | val_0_rmse: 0.44837 | val_1_rmse: 0.48075 |  0:00:59s
epoch 129| loss: 0.21893 | val_0_rmse: 0.45933 | val_1_rmse: 0.48536 |  0:00:59s
epoch 130| loss: 0.22402 | val_0_rmse: 0.44999 | val_1_rmse: 0.48192 |  0:01:00s
epoch 131| loss: 0.22208 | val_0_rmse: 0.45661 | val_1_rmse: 0.48854 |  0:01:00s
epoch 132| loss: 0.21844 | val_0_rmse: 0.44813 | val_1_rmse: 0.48345 |  0:01:01s
epoch 133| loss: 0.2237  | val_0_rmse: 0.45559 | val_1_rmse: 0.49192 |  0:01:01s
epoch 134| loss: 0.22726 | val_0_rmse: 0.46468 | val_1_rmse: 0.49432 |  0:01:02s
epoch 135| loss: 0.22028 | val_0_rmse: 0.45887 | val_1_rmse: 0.4904  |  0:01:02s
epoch 136| loss: 0.22236 | val_0_rmse: 0.4521  | val_1_rmse: 0.4866  |  0:01:02s
epoch 137| loss: 0.22736 | val_0_rmse: 0.4592  | val_1_rmse: 0.48462 |  0:01:03s
epoch 138| loss: 0.22845 | val_0_rmse: 0.46017 | val_1_rmse: 0.48536 |  0:01:03s
epoch 139| loss: 0.22636 | val_0_rmse: 0.45943 | val_1_rmse: 0.49163 |  0:01:04s
epoch 140| loss: 0.22462 | val_0_rmse: 0.44717 | val_1_rmse: 0.48106 |  0:01:04s
epoch 141| loss: 0.21499 | val_0_rmse: 0.46038 | val_1_rmse: 0.49504 |  0:01:05s
epoch 142| loss: 0.22463 | val_0_rmse: 0.44629 | val_1_rmse: 0.48241 |  0:01:05s
epoch 143| loss: 0.21951 | val_0_rmse: 0.45187 | val_1_rmse: 0.48302 |  0:01:06s
epoch 144| loss: 0.216   | val_0_rmse: 0.44786 | val_1_rmse: 0.47849 |  0:01:06s
epoch 145| loss: 0.21612 | val_0_rmse: 0.44522 | val_1_rmse: 0.47821 |  0:01:07s
epoch 146| loss: 0.22046 | val_0_rmse: 0.44927 | val_1_rmse: 0.48066 |  0:01:07s
epoch 147| loss: 0.21761 | val_0_rmse: 0.44501 | val_1_rmse: 0.48054 |  0:01:08s
epoch 148| loss: 0.21572 | val_0_rmse: 0.45034 | val_1_rmse: 0.48668 |  0:01:08s
epoch 149| loss: 0.21532 | val_0_rmse: 0.44706 | val_1_rmse: 0.48016 |  0:01:08s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.47707
Best weights from best epoch are automatically used!
ended training at: 07:22:16
Feature importance:
[('Area', 0.1892213145147085), ('Baths', 0.011181500278099938), ('Beds', 0.09330493395453676), ('Latitude', 0.30478550894663814), ('Longitude', 0.3915144908936835), ('Month', 1.520549406476973e-05), ('Year', 0.009977045918268404)]
Mean squared error is of 21785176003.213238
Mean absolute error:104393.0912787855
MAPE:0.1710470599254941
R2 score:0.7328954688903659
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:22:17
epoch 0  | loss: 0.98247 | val_0_rmse: 1.30583 | val_1_rmse: 1.32528 |  0:00:00s
epoch 1  | loss: 0.53696 | val_0_rmse: 0.82422 | val_1_rmse: 0.84106 |  0:00:00s
epoch 2  | loss: 0.43443 | val_0_rmse: 0.91867 | val_1_rmse: 0.95078 |  0:00:01s
epoch 3  | loss: 0.35268 | val_0_rmse: 0.80124 | val_1_rmse: 0.8363  |  0:00:01s
epoch 4  | loss: 0.33188 | val_0_rmse: 0.62024 | val_1_rmse: 0.64764 |  0:00:02s
epoch 5  | loss: 0.29822 | val_0_rmse: 0.54147 | val_1_rmse: 0.56573 |  0:00:02s
epoch 6  | loss: 0.28275 | val_0_rmse: 0.52662 | val_1_rmse: 0.55412 |  0:00:03s
epoch 7  | loss: 0.27991 | val_0_rmse: 0.51231 | val_1_rmse: 0.54273 |  0:00:03s
epoch 8  | loss: 0.27297 | val_0_rmse: 0.5107  | val_1_rmse: 0.53731 |  0:00:04s
epoch 9  | loss: 0.27542 | val_0_rmse: 0.51954 | val_1_rmse: 0.55086 |  0:00:04s
epoch 10 | loss: 0.2811  | val_0_rmse: 0.51489 | val_1_rmse: 0.52622 |  0:00:05s
epoch 11 | loss: 0.26547 | val_0_rmse: 0.49506 | val_1_rmse: 0.51959 |  0:00:05s
epoch 12 | loss: 0.2614  | val_0_rmse: 0.49793 | val_1_rmse: 0.52626 |  0:00:06s
epoch 13 | loss: 0.26463 | val_0_rmse: 0.49351 | val_1_rmse: 0.50674 |  0:00:06s
epoch 14 | loss: 0.26799 | val_0_rmse: 0.48723 | val_1_rmse: 0.508   |  0:00:06s
epoch 15 | loss: 0.26051 | val_0_rmse: 0.48835 | val_1_rmse: 0.50163 |  0:00:07s
epoch 16 | loss: 0.25464 | val_0_rmse: 0.49257 | val_1_rmse: 0.50226 |  0:00:07s
epoch 17 | loss: 0.2523  | val_0_rmse: 0.48675 | val_1_rmse: 0.50653 |  0:00:08s
epoch 18 | loss: 0.2508  | val_0_rmse: 0.49027 | val_1_rmse: 0.51576 |  0:00:08s
epoch 19 | loss: 0.25662 | val_0_rmse: 0.49615 | val_1_rmse: 0.5103  |  0:00:09s
epoch 20 | loss: 0.2619  | val_0_rmse: 0.49808 | val_1_rmse: 0.51994 |  0:00:09s
epoch 21 | loss: 0.25695 | val_0_rmse: 0.5136  | val_1_rmse: 0.53583 |  0:00:10s
epoch 22 | loss: 0.25406 | val_0_rmse: 0.48072 | val_1_rmse: 0.50194 |  0:00:10s
epoch 23 | loss: 0.25291 | val_0_rmse: 0.48452 | val_1_rmse: 0.50438 |  0:00:11s
epoch 24 | loss: 0.24699 | val_0_rmse: 0.47713 | val_1_rmse: 0.50116 |  0:00:11s
epoch 25 | loss: 0.24846 | val_0_rmse: 0.47808 | val_1_rmse: 0.50244 |  0:00:12s
epoch 26 | loss: 0.2436  | val_0_rmse: 0.48507 | val_1_rmse: 0.51331 |  0:00:12s
epoch 27 | loss: 0.24623 | val_0_rmse: 0.48347 | val_1_rmse: 0.50499 |  0:00:12s
epoch 28 | loss: 0.24975 | val_0_rmse: 0.48305 | val_1_rmse: 0.50675 |  0:00:13s
epoch 29 | loss: 0.24005 | val_0_rmse: 0.47229 | val_1_rmse: 0.49677 |  0:00:13s
epoch 30 | loss: 0.24246 | val_0_rmse: 0.47395 | val_1_rmse: 0.49583 |  0:00:14s
epoch 31 | loss: 0.24074 | val_0_rmse: 0.48056 | val_1_rmse: 0.50466 |  0:00:14s
epoch 32 | loss: 0.24251 | val_0_rmse: 0.48795 | val_1_rmse: 0.51581 |  0:00:15s
epoch 33 | loss: 0.24074 | val_0_rmse: 0.47367 | val_1_rmse: 0.49252 |  0:00:15s
epoch 34 | loss: 0.23596 | val_0_rmse: 0.47492 | val_1_rmse: 0.49917 |  0:00:16s
epoch 35 | loss: 0.24018 | val_0_rmse: 0.47629 | val_1_rmse: 0.49968 |  0:00:16s
epoch 36 | loss: 0.23672 | val_0_rmse: 0.47791 | val_1_rmse: 0.50356 |  0:00:17s
epoch 37 | loss: 0.24717 | val_0_rmse: 0.47285 | val_1_rmse: 0.50291 |  0:00:17s
epoch 38 | loss: 0.24111 | val_0_rmse: 0.47014 | val_1_rmse: 0.49576 |  0:00:17s
epoch 39 | loss: 0.24182 | val_0_rmse: 0.47054 | val_1_rmse: 0.49528 |  0:00:18s
epoch 40 | loss: 0.2455  | val_0_rmse: 0.47608 | val_1_rmse: 0.50632 |  0:00:18s
epoch 41 | loss: 0.24854 | val_0_rmse: 0.48336 | val_1_rmse: 0.5073  |  0:00:19s
epoch 42 | loss: 0.23985 | val_0_rmse: 0.47445 | val_1_rmse: 0.49975 |  0:00:19s
epoch 43 | loss: 0.24054 | val_0_rmse: 0.46862 | val_1_rmse: 0.49261 |  0:00:20s
epoch 44 | loss: 0.24483 | val_0_rmse: 0.47298 | val_1_rmse: 0.50547 |  0:00:20s
epoch 45 | loss: 0.23925 | val_0_rmse: 0.46886 | val_1_rmse: 0.49436 |  0:00:21s
epoch 46 | loss: 0.23817 | val_0_rmse: 0.48318 | val_1_rmse: 0.50713 |  0:00:21s
epoch 47 | loss: 0.23479 | val_0_rmse: 0.47559 | val_1_rmse: 0.51269 |  0:00:22s
epoch 48 | loss: 0.23969 | val_0_rmse: 0.46569 | val_1_rmse: 0.50757 |  0:00:22s
epoch 49 | loss: 0.23644 | val_0_rmse: 0.47039 | val_1_rmse: 0.49715 |  0:00:22s
epoch 50 | loss: 0.23283 | val_0_rmse: 0.46243 | val_1_rmse: 0.50165 |  0:00:23s
epoch 51 | loss: 0.23243 | val_0_rmse: 0.4668  | val_1_rmse: 0.49714 |  0:00:23s
epoch 52 | loss: 0.22892 | val_0_rmse: 0.46396 | val_1_rmse: 0.49788 |  0:00:24s
epoch 53 | loss: 0.22854 | val_0_rmse: 0.45911 | val_1_rmse: 0.49032 |  0:00:24s
epoch 54 | loss: 0.22725 | val_0_rmse: 0.47269 | val_1_rmse: 0.50343 |  0:00:25s
epoch 55 | loss: 0.22954 | val_0_rmse: 0.47335 | val_1_rmse: 0.51006 |  0:00:25s
epoch 56 | loss: 0.23576 | val_0_rmse: 0.47522 | val_1_rmse: 0.50454 |  0:00:26s
epoch 57 | loss: 0.23177 | val_0_rmse: 0.46275 | val_1_rmse: 0.49276 |  0:00:26s
epoch 58 | loss: 0.22844 | val_0_rmse: 0.46495 | val_1_rmse: 0.49585 |  0:00:27s
epoch 59 | loss: 0.22711 | val_0_rmse: 0.45521 | val_1_rmse: 0.49458 |  0:00:27s
epoch 60 | loss: 0.23574 | val_0_rmse: 0.45702 | val_1_rmse: 0.4899  |  0:00:28s
epoch 61 | loss: 0.23768 | val_0_rmse: 0.46695 | val_1_rmse: 0.50389 |  0:00:28s
epoch 62 | loss: 0.23462 | val_0_rmse: 0.45627 | val_1_rmse: 0.49515 |  0:00:28s
epoch 63 | loss: 0.22806 | val_0_rmse: 0.4793  | val_1_rmse: 0.5093  |  0:00:29s
epoch 64 | loss: 0.23934 | val_0_rmse: 0.46991 | val_1_rmse: 0.50857 |  0:00:29s
epoch 65 | loss: 0.23905 | val_0_rmse: 0.45851 | val_1_rmse: 0.50303 |  0:00:30s
epoch 66 | loss: 0.23442 | val_0_rmse: 0.47094 | val_1_rmse: 0.50177 |  0:00:30s
epoch 67 | loss: 0.23842 | val_0_rmse: 0.47476 | val_1_rmse: 0.51172 |  0:00:31s
epoch 68 | loss: 0.24445 | val_0_rmse: 0.46178 | val_1_rmse: 0.49147 |  0:00:31s
epoch 69 | loss: 0.23203 | val_0_rmse: 0.46133 | val_1_rmse: 0.49593 |  0:00:32s
epoch 70 | loss: 0.22735 | val_0_rmse: 0.4574  | val_1_rmse: 0.49827 |  0:00:32s
epoch 71 | loss: 0.22625 | val_0_rmse: 0.4533  | val_1_rmse: 0.49005 |  0:00:33s
epoch 72 | loss: 0.22675 | val_0_rmse: 0.4659  | val_1_rmse: 0.49718 |  0:00:33s
epoch 73 | loss: 0.23012 | val_0_rmse: 0.46284 | val_1_rmse: 0.5058  |  0:00:33s
epoch 74 | loss: 0.23394 | val_0_rmse: 0.46943 | val_1_rmse: 0.50232 |  0:00:34s
epoch 75 | loss: 0.23024 | val_0_rmse: 0.45863 | val_1_rmse: 0.49576 |  0:00:34s
epoch 76 | loss: 0.22665 | val_0_rmse: 0.4566  | val_1_rmse: 0.50256 |  0:00:35s
epoch 77 | loss: 0.22334 | val_0_rmse: 0.45312 | val_1_rmse: 0.49403 |  0:00:35s
epoch 78 | loss: 0.22403 | val_0_rmse: 0.45869 | val_1_rmse: 0.48903 |  0:00:36s
epoch 79 | loss: 0.22845 | val_0_rmse: 0.46148 | val_1_rmse: 0.49937 |  0:00:36s
epoch 80 | loss: 0.23416 | val_0_rmse: 0.49638 | val_1_rmse: 0.52187 |  0:00:37s
epoch 81 | loss: 0.23524 | val_0_rmse: 0.48086 | val_1_rmse: 0.51431 |  0:00:37s
epoch 82 | loss: 0.23233 | val_0_rmse: 0.46458 | val_1_rmse: 0.49466 |  0:00:38s
epoch 83 | loss: 0.23276 | val_0_rmse: 0.46357 | val_1_rmse: 0.50302 |  0:00:38s
epoch 84 | loss: 0.23028 | val_0_rmse: 0.46846 | val_1_rmse: 0.50088 |  0:00:38s
epoch 85 | loss: 0.22766 | val_0_rmse: 0.46337 | val_1_rmse: 0.50927 |  0:00:39s
epoch 86 | loss: 0.2324  | val_0_rmse: 0.46823 | val_1_rmse: 0.51288 |  0:00:39s
epoch 87 | loss: 0.22713 | val_0_rmse: 0.45629 | val_1_rmse: 0.50037 |  0:00:40s
epoch 88 | loss: 0.22615 | val_0_rmse: 0.45518 | val_1_rmse: 0.50218 |  0:00:40s
epoch 89 | loss: 0.22991 | val_0_rmse: 0.46174 | val_1_rmse: 0.50055 |  0:00:41s
epoch 90 | loss: 0.2243  | val_0_rmse: 0.45588 | val_1_rmse: 0.4987  |  0:00:41s
epoch 91 | loss: 0.2241  | val_0_rmse: 0.45803 | val_1_rmse: 0.49721 |  0:00:42s
epoch 92 | loss: 0.22006 | val_0_rmse: 0.4542  | val_1_rmse: 0.49408 |  0:00:42s
epoch 93 | loss: 0.22027 | val_0_rmse: 0.45351 | val_1_rmse: 0.49291 |  0:00:43s
epoch 94 | loss: 0.22241 | val_0_rmse: 0.45531 | val_1_rmse: 0.49401 |  0:00:43s
epoch 95 | loss: 0.22463 | val_0_rmse: 0.45589 | val_1_rmse: 0.49441 |  0:00:44s
epoch 96 | loss: 0.22162 | val_0_rmse: 0.45385 | val_1_rmse: 0.50396 |  0:00:44s
epoch 97 | loss: 0.21998 | val_0_rmse: 0.453   | val_1_rmse: 0.49332 |  0:00:44s
epoch 98 | loss: 0.22069 | val_0_rmse: 0.45166 | val_1_rmse: 0.49282 |  0:00:45s
epoch 99 | loss: 0.21933 | val_0_rmse: 0.4516  | val_1_rmse: 0.5023  |  0:00:45s
epoch 100| loss: 0.22756 | val_0_rmse: 0.47032 | val_1_rmse: 0.51399 |  0:00:46s
epoch 101| loss: 0.23681 | val_0_rmse: 0.4651  | val_1_rmse: 0.5037  |  0:00:46s
epoch 102| loss: 0.23276 | val_0_rmse: 0.4591  | val_1_rmse: 0.49591 |  0:00:47s
epoch 103| loss: 0.23306 | val_0_rmse: 0.46107 | val_1_rmse: 0.48538 |  0:00:47s
epoch 104| loss: 0.24066 | val_0_rmse: 0.45839 | val_1_rmse: 0.49289 |  0:00:48s
epoch 105| loss: 0.23669 | val_0_rmse: 0.46284 | val_1_rmse: 0.5099  |  0:00:48s
epoch 106| loss: 0.2324  | val_0_rmse: 0.46752 | val_1_rmse: 0.49941 |  0:00:49s
epoch 107| loss: 0.23039 | val_0_rmse: 0.46829 | val_1_rmse: 0.50879 |  0:00:49s
epoch 108| loss: 0.23213 | val_0_rmse: 0.45838 | val_1_rmse: 0.50323 |  0:00:49s
epoch 109| loss: 0.22463 | val_0_rmse: 0.46793 | val_1_rmse: 0.50669 |  0:00:50s
epoch 110| loss: 0.22889 | val_0_rmse: 0.45948 | val_1_rmse: 0.50837 |  0:00:50s
epoch 111| loss: 0.22419 | val_0_rmse: 0.46072 | val_1_rmse: 0.49964 |  0:00:51s
epoch 112| loss: 0.22657 | val_0_rmse: 0.46263 | val_1_rmse: 0.50455 |  0:00:51s
epoch 113| loss: 0.23005 | val_0_rmse: 0.45457 | val_1_rmse: 0.49952 |  0:00:52s
epoch 114| loss: 0.22563 | val_0_rmse: 0.45871 | val_1_rmse: 0.4943  |  0:00:52s
epoch 115| loss: 0.22392 | val_0_rmse: 0.44827 | val_1_rmse: 0.48663 |  0:00:53s
epoch 116| loss: 0.21806 | val_0_rmse: 0.46624 | val_1_rmse: 0.50438 |  0:00:53s
epoch 117| loss: 0.23094 | val_0_rmse: 0.45587 | val_1_rmse: 0.49501 |  0:00:54s
epoch 118| loss: 0.23061 | val_0_rmse: 0.45818 | val_1_rmse: 0.492   |  0:00:54s
epoch 119| loss: 0.23079 | val_0_rmse: 0.45447 | val_1_rmse: 0.50726 |  0:00:54s
epoch 120| loss: 0.22174 | val_0_rmse: 0.44245 | val_1_rmse: 0.49628 |  0:00:55s
epoch 121| loss: 0.21737 | val_0_rmse: 0.44972 | val_1_rmse: 0.50462 |  0:00:55s
epoch 122| loss: 0.21198 | val_0_rmse: 0.44362 | val_1_rmse: 0.48969 |  0:00:56s
epoch 123| loss: 0.21258 | val_0_rmse: 0.44546 | val_1_rmse: 0.49529 |  0:00:56s
epoch 124| loss: 0.2253  | val_0_rmse: 0.47304 | val_1_rmse: 0.51859 |  0:00:57s
epoch 125| loss: 0.22008 | val_0_rmse: 0.45007 | val_1_rmse: 0.49595 |  0:00:57s
epoch 126| loss: 0.21729 | val_0_rmse: 0.44477 | val_1_rmse: 0.48718 |  0:00:58s
epoch 127| loss: 0.2152  | val_0_rmse: 0.44708 | val_1_rmse: 0.48356 |  0:00:58s
epoch 128| loss: 0.21449 | val_0_rmse: 0.46862 | val_1_rmse: 0.50735 |  0:00:59s
epoch 129| loss: 0.21523 | val_0_rmse: 0.44188 | val_1_rmse: 0.48746 |  0:00:59s
epoch 130| loss: 0.2164  | val_0_rmse: 0.44302 | val_1_rmse: 0.48388 |  0:00:59s
epoch 131| loss: 0.21472 | val_0_rmse: 0.45397 | val_1_rmse: 0.50224 |  0:01:00s
epoch 132| loss: 0.21738 | val_0_rmse: 0.44775 | val_1_rmse: 0.49716 |  0:01:00s
epoch 133| loss: 0.21658 | val_0_rmse: 0.44857 | val_1_rmse: 0.50209 |  0:01:01s
epoch 134| loss: 0.21589 | val_0_rmse: 0.44246 | val_1_rmse: 0.49294 |  0:01:01s
epoch 135| loss: 0.21471 | val_0_rmse: 0.45035 | val_1_rmse: 0.49332 |  0:01:02s
epoch 136| loss: 0.22123 | val_0_rmse: 0.4537  | val_1_rmse: 0.49668 |  0:01:02s
epoch 137| loss: 0.21054 | val_0_rmse: 0.44304 | val_1_rmse: 0.48672 |  0:01:03s
epoch 138| loss: 0.20857 | val_0_rmse: 0.43813 | val_1_rmse: 0.49072 |  0:01:03s
epoch 139| loss: 0.2111  | val_0_rmse: 0.44564 | val_1_rmse: 0.48826 |  0:01:04s
epoch 140| loss: 0.21257 | val_0_rmse: 0.45258 | val_1_rmse: 0.49363 |  0:01:04s
epoch 141| loss: 0.22268 | val_0_rmse: 0.47256 | val_1_rmse: 0.52183 |  0:01:04s
epoch 142| loss: 0.23137 | val_0_rmse: 0.45592 | val_1_rmse: 0.49358 |  0:01:05s
epoch 143| loss: 0.22542 | val_0_rmse: 0.45066 | val_1_rmse: 0.48461 |  0:01:05s
epoch 144| loss: 0.21612 | val_0_rmse: 0.45679 | val_1_rmse: 0.5074  |  0:01:06s
epoch 145| loss: 0.21827 | val_0_rmse: 0.44399 | val_1_rmse: 0.48673 |  0:01:06s
epoch 146| loss: 0.21589 | val_0_rmse: 0.44    | val_1_rmse: 0.4884  |  0:01:07s
epoch 147| loss: 0.21616 | val_0_rmse: 0.44597 | val_1_rmse: 0.49865 |  0:01:07s
epoch 148| loss: 0.20763 | val_0_rmse: 0.43891 | val_1_rmse: 0.48559 |  0:01:08s
epoch 149| loss: 0.20934 | val_0_rmse: 0.43819 | val_1_rmse: 0.4817  |  0:01:08s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.4817
Best weights from best epoch are automatically used!
ended training at: 07:23:25
Feature importance:
[('Area', 0.29822430448293064), ('Baths', 0.013770422692119144), ('Beds', 0.1285002316841956), ('Latitude', 0.2479878314694703), ('Longitude', 0.21975798939810812), ('Month', 0.011514987785239024), ('Year', 0.08024423248793719)]
Mean squared error is of 21898618001.172104
Mean absolute error:105676.369464051
MAPE:0.1694980694585132
R2 score:0.7259458543540638
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:23:26
epoch 0  | loss: 0.99332 | val_0_rmse: 1.01008 | val_1_rmse: 1.01143 |  0:00:00s
epoch 1  | loss: 0.47583 | val_0_rmse: 0.81175 | val_1_rmse: 0.83239 |  0:00:00s
epoch 2  | loss: 0.37149 | val_0_rmse: 0.65184 | val_1_rmse: 0.67568 |  0:00:01s
epoch 3  | loss: 0.3364  | val_0_rmse: 0.5794  | val_1_rmse: 0.61058 |  0:00:01s
epoch 4  | loss: 0.31784 | val_0_rmse: 0.58115 | val_1_rmse: 0.59435 |  0:00:02s
epoch 5  | loss: 0.30416 | val_0_rmse: 0.56143 | val_1_rmse: 0.57495 |  0:00:02s
epoch 6  | loss: 0.29872 | val_0_rmse: 0.54468 | val_1_rmse: 0.56512 |  0:00:03s
epoch 7  | loss: 0.29065 | val_0_rmse: 0.52277 | val_1_rmse: 0.55107 |  0:00:03s
epoch 8  | loss: 0.27778 | val_0_rmse: 0.49575 | val_1_rmse: 0.52991 |  0:00:04s
epoch 9  | loss: 0.26472 | val_0_rmse: 0.49109 | val_1_rmse: 0.52814 |  0:00:04s
epoch 10 | loss: 0.26878 | val_0_rmse: 0.49566 | val_1_rmse: 0.53111 |  0:00:05s
epoch 11 | loss: 0.25772 | val_0_rmse: 0.50102 | val_1_rmse: 0.5273  |  0:00:05s
epoch 12 | loss: 0.26002 | val_0_rmse: 0.4948  | val_1_rmse: 0.5326  |  0:00:06s
epoch 13 | loss: 0.25221 | val_0_rmse: 0.48508 | val_1_rmse: 0.52037 |  0:00:06s
epoch 14 | loss: 0.25012 | val_0_rmse: 0.48307 | val_1_rmse: 0.51586 |  0:00:06s
epoch 15 | loss: 0.24648 | val_0_rmse: 0.47777 | val_1_rmse: 0.51006 |  0:00:07s
epoch 16 | loss: 0.2571  | val_0_rmse: 0.48388 | val_1_rmse: 0.5116  |  0:00:07s
epoch 17 | loss: 0.25639 | val_0_rmse: 0.48753 | val_1_rmse: 0.51477 |  0:00:08s
epoch 18 | loss: 0.25309 | val_0_rmse: 0.50065 | val_1_rmse: 0.53623 |  0:00:08s
epoch 19 | loss: 0.25567 | val_0_rmse: 0.4885  | val_1_rmse: 0.52044 |  0:00:09s
epoch 20 | loss: 0.24732 | val_0_rmse: 0.47281 | val_1_rmse: 0.50537 |  0:00:09s
epoch 21 | loss: 0.24877 | val_0_rmse: 0.48124 | val_1_rmse: 0.51242 |  0:00:10s
epoch 22 | loss: 0.25072 | val_0_rmse: 0.49658 | val_1_rmse: 0.53013 |  0:00:10s
epoch 23 | loss: 0.25735 | val_0_rmse: 0.48647 | val_1_rmse: 0.52101 |  0:00:11s
epoch 24 | loss: 0.2543  | val_0_rmse: 0.4713  | val_1_rmse: 0.50284 |  0:00:11s
epoch 25 | loss: 0.24776 | val_0_rmse: 0.48691 | val_1_rmse: 0.51661 |  0:00:12s
epoch 26 | loss: 0.24419 | val_0_rmse: 0.46582 | val_1_rmse: 0.49981 |  0:00:12s
epoch 27 | loss: 0.2333  | val_0_rmse: 0.46507 | val_1_rmse: 0.4953  |  0:00:12s
epoch 28 | loss: 0.23766 | val_0_rmse: 0.47038 | val_1_rmse: 0.49979 |  0:00:13s
epoch 29 | loss: 0.23597 | val_0_rmse: 0.47596 | val_1_rmse: 0.5089  |  0:00:13s
epoch 30 | loss: 0.24072 | val_0_rmse: 0.46119 | val_1_rmse: 0.49748 |  0:00:14s
epoch 31 | loss: 0.23784 | val_0_rmse: 0.46155 | val_1_rmse: 0.4971  |  0:00:14s
epoch 32 | loss: 0.23228 | val_0_rmse: 0.47351 | val_1_rmse: 0.50927 |  0:00:15s
epoch 33 | loss: 0.24052 | val_0_rmse: 0.47033 | val_1_rmse: 0.50645 |  0:00:15s
epoch 34 | loss: 0.23695 | val_0_rmse: 0.46924 | val_1_rmse: 0.5034  |  0:00:16s
epoch 35 | loss: 0.23483 | val_0_rmse: 0.46208 | val_1_rmse: 0.49875 |  0:00:16s
epoch 36 | loss: 0.2337  | val_0_rmse: 0.456   | val_1_rmse: 0.49472 |  0:00:17s
epoch 37 | loss: 0.2348  | val_0_rmse: 0.46388 | val_1_rmse: 0.49598 |  0:00:17s
epoch 38 | loss: 0.23185 | val_0_rmse: 0.46674 | val_1_rmse: 0.50525 |  0:00:17s
epoch 39 | loss: 0.23084 | val_0_rmse: 0.45536 | val_1_rmse: 0.48955 |  0:00:18s
epoch 40 | loss: 0.23418 | val_0_rmse: 0.46437 | val_1_rmse: 0.50258 |  0:00:18s
epoch 41 | loss: 0.23327 | val_0_rmse: 0.46596 | val_1_rmse: 0.49954 |  0:00:19s
epoch 42 | loss: 0.23389 | val_0_rmse: 0.48517 | val_1_rmse: 0.51957 |  0:00:19s
epoch 43 | loss: 0.24649 | val_0_rmse: 0.46685 | val_1_rmse: 0.50638 |  0:00:20s
epoch 44 | loss: 0.23461 | val_0_rmse: 0.4654  | val_1_rmse: 0.50526 |  0:00:20s
epoch 45 | loss: 0.23363 | val_0_rmse: 0.47644 | val_1_rmse: 0.51891 |  0:00:21s
epoch 46 | loss: 0.2321  | val_0_rmse: 0.45529 | val_1_rmse: 0.49646 |  0:00:21s
epoch 47 | loss: 0.23314 | val_0_rmse: 0.45956 | val_1_rmse: 0.49817 |  0:00:22s
epoch 48 | loss: 0.23233 | val_0_rmse: 0.46467 | val_1_rmse: 0.49957 |  0:00:22s
epoch 49 | loss: 0.22809 | val_0_rmse: 0.46468 | val_1_rmse: 0.49722 |  0:00:22s
epoch 50 | loss: 0.23639 | val_0_rmse: 0.4525  | val_1_rmse: 0.49123 |  0:00:23s
epoch 51 | loss: 0.23299 | val_0_rmse: 0.48175 | val_1_rmse: 0.52287 |  0:00:23s
epoch 52 | loss: 0.23647 | val_0_rmse: 0.47934 | val_1_rmse: 0.51703 |  0:00:24s
epoch 53 | loss: 0.23534 | val_0_rmse: 0.46202 | val_1_rmse: 0.50144 |  0:00:24s
epoch 54 | loss: 0.22972 | val_0_rmse: 0.45925 | val_1_rmse: 0.49401 |  0:00:25s
epoch 55 | loss: 0.23352 | val_0_rmse: 0.49605 | val_1_rmse: 0.53045 |  0:00:25s
epoch 56 | loss: 0.24665 | val_0_rmse: 0.4749  | val_1_rmse: 0.50576 |  0:00:26s
epoch 57 | loss: 0.23494 | val_0_rmse: 0.47006 | val_1_rmse: 0.50799 |  0:00:26s
epoch 58 | loss: 0.23092 | val_0_rmse: 0.46817 | val_1_rmse: 0.50268 |  0:00:27s
epoch 59 | loss: 0.23224 | val_0_rmse: 0.46482 | val_1_rmse: 0.49918 |  0:00:27s
epoch 60 | loss: 0.2258  | val_0_rmse: 0.45533 | val_1_rmse: 0.49065 |  0:00:27s
epoch 61 | loss: 0.22955 | val_0_rmse: 0.46395 | val_1_rmse: 0.49682 |  0:00:28s
epoch 62 | loss: 0.22416 | val_0_rmse: 0.46424 | val_1_rmse: 0.49509 |  0:00:28s
epoch 63 | loss: 0.22488 | val_0_rmse: 0.45016 | val_1_rmse: 0.49108 |  0:00:29s
epoch 64 | loss: 0.22106 | val_0_rmse: 0.46343 | val_1_rmse: 0.50965 |  0:00:29s
epoch 65 | loss: 0.23315 | val_0_rmse: 0.45916 | val_1_rmse: 0.49761 |  0:00:30s
epoch 66 | loss: 0.24143 | val_0_rmse: 0.48898 | val_1_rmse: 0.51712 |  0:00:30s
epoch 67 | loss: 0.24294 | val_0_rmse: 0.47699 | val_1_rmse: 0.51455 |  0:00:31s
epoch 68 | loss: 0.24263 | val_0_rmse: 0.48317 | val_1_rmse: 0.51432 |  0:00:31s
epoch 69 | loss: 0.24485 | val_0_rmse: 0.48454 | val_1_rmse: 0.51451 |  0:00:32s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.48955
Best weights from best epoch are automatically used!
ended training at: 07:23:58
Feature importance:
[('Area', 0.21679205717523087), ('Baths', 0.05686739120689401), ('Beds', 0.13558829829322602), ('Latitude', 0.3048112566207113), ('Longitude', 0.21300569617853657), ('Month', 0.014387561073850696), ('Year', 0.05854773945155057)]
Mean squared error is of 22597307570.26315
Mean absolute error:106876.12968789431
MAPE:0.17574351347056655
R2 score:0.7271112856687949
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:23:58
epoch 0  | loss: 1.01739 | val_0_rmse: 0.92377 | val_1_rmse: 0.95227 |  0:00:00s
epoch 1  | loss: 0.48442 | val_0_rmse: 0.87608 | val_1_rmse: 0.88409 |  0:00:00s
epoch 2  | loss: 0.37104 | val_0_rmse: 0.7134  | val_1_rmse: 0.72754 |  0:00:01s
epoch 3  | loss: 0.32679 | val_0_rmse: 0.62069 | val_1_rmse: 0.64155 |  0:00:01s
epoch 4  | loss: 0.30969 | val_0_rmse: 0.57596 | val_1_rmse: 0.59682 |  0:00:02s
epoch 5  | loss: 0.31186 | val_0_rmse: 0.59134 | val_1_rmse: 0.60196 |  0:00:02s
epoch 6  | loss: 0.29892 | val_0_rmse: 0.54699 | val_1_rmse: 0.56306 |  0:00:03s
epoch 7  | loss: 0.28283 | val_0_rmse: 0.54351 | val_1_rmse: 0.55421 |  0:00:03s
epoch 8  | loss: 0.30736 | val_0_rmse: 0.53765 | val_1_rmse: 0.54541 |  0:00:04s
epoch 9  | loss: 0.29853 | val_0_rmse: 0.52233 | val_1_rmse: 0.53774 |  0:00:04s
epoch 10 | loss: 0.27602 | val_0_rmse: 0.50862 | val_1_rmse: 0.52391 |  0:00:05s
epoch 11 | loss: 0.276   | val_0_rmse: 0.50145 | val_1_rmse: 0.51345 |  0:00:05s
epoch 12 | loss: 0.26759 | val_0_rmse: 0.49498 | val_1_rmse: 0.50908 |  0:00:06s
epoch 13 | loss: 0.26457 | val_0_rmse: 0.49942 | val_1_rmse: 0.51431 |  0:00:06s
epoch 14 | loss: 0.265   | val_0_rmse: 0.49574 | val_1_rmse: 0.50881 |  0:00:06s
epoch 15 | loss: 0.25554 | val_0_rmse: 0.492   | val_1_rmse: 0.49948 |  0:00:07s
epoch 16 | loss: 0.25884 | val_0_rmse: 0.4937  | val_1_rmse: 0.50955 |  0:00:07s
epoch 17 | loss: 0.25834 | val_0_rmse: 0.48785 | val_1_rmse: 0.50181 |  0:00:08s
epoch 18 | loss: 0.25273 | val_0_rmse: 0.47643 | val_1_rmse: 0.49268 |  0:00:08s
epoch 19 | loss: 0.25058 | val_0_rmse: 0.47876 | val_1_rmse: 0.49416 |  0:00:09s
epoch 20 | loss: 0.24795 | val_0_rmse: 0.47439 | val_1_rmse: 0.49134 |  0:00:09s
epoch 21 | loss: 0.25317 | val_0_rmse: 0.48205 | val_1_rmse: 0.49717 |  0:00:10s
epoch 22 | loss: 0.25475 | val_0_rmse: 0.47451 | val_1_rmse: 0.4898  |  0:00:10s
epoch 23 | loss: 0.25121 | val_0_rmse: 0.47746 | val_1_rmse: 0.49718 |  0:00:11s
epoch 24 | loss: 0.24834 | val_0_rmse: 0.48503 | val_1_rmse: 0.49865 |  0:00:11s
epoch 25 | loss: 0.25219 | val_0_rmse: 0.47926 | val_1_rmse: 0.49819 |  0:00:12s
epoch 26 | loss: 0.24096 | val_0_rmse: 0.46687 | val_1_rmse: 0.48492 |  0:00:12s
epoch 27 | loss: 0.24041 | val_0_rmse: 0.46889 | val_1_rmse: 0.48823 |  0:00:12s
epoch 28 | loss: 0.23039 | val_0_rmse: 0.46631 | val_1_rmse: 0.48478 |  0:00:13s
epoch 29 | loss: 0.23427 | val_0_rmse: 0.46577 | val_1_rmse: 0.48098 |  0:00:13s
epoch 30 | loss: 0.23365 | val_0_rmse: 0.46814 | val_1_rmse: 0.49061 |  0:00:14s
epoch 31 | loss: 0.23339 | val_0_rmse: 0.46105 | val_1_rmse: 0.48703 |  0:00:14s
epoch 32 | loss: 0.23304 | val_0_rmse: 0.46451 | val_1_rmse: 0.48255 |  0:00:15s
epoch 33 | loss: 0.23198 | val_0_rmse: 0.47122 | val_1_rmse: 0.49755 |  0:00:15s
epoch 34 | loss: 0.23783 | val_0_rmse: 0.47083 | val_1_rmse: 0.49732 |  0:00:16s
epoch 35 | loss: 0.23307 | val_0_rmse: 0.46318 | val_1_rmse: 0.48842 |  0:00:16s
epoch 36 | loss: 0.23094 | val_0_rmse: 0.45547 | val_1_rmse: 0.48157 |  0:00:17s
epoch 37 | loss: 0.23613 | val_0_rmse: 0.45976 | val_1_rmse: 0.48631 |  0:00:17s
epoch 38 | loss: 0.23392 | val_0_rmse: 0.46997 | val_1_rmse: 0.49038 |  0:00:17s
epoch 39 | loss: 0.23373 | val_0_rmse: 0.45935 | val_1_rmse: 0.48321 |  0:00:18s
epoch 40 | loss: 0.23049 | val_0_rmse: 0.46036 | val_1_rmse: 0.48127 |  0:00:18s
epoch 41 | loss: 0.22284 | val_0_rmse: 0.46079 | val_1_rmse: 0.47646 |  0:00:19s
epoch 42 | loss: 0.23677 | val_0_rmse: 0.46604 | val_1_rmse: 0.48902 |  0:00:19s
epoch 43 | loss: 0.24335 | val_0_rmse: 0.45629 | val_1_rmse: 0.4769  |  0:00:20s
epoch 44 | loss: 0.23303 | val_0_rmse: 0.47489 | val_1_rmse: 0.49019 |  0:00:20s
epoch 45 | loss: 0.229   | val_0_rmse: 0.46647 | val_1_rmse: 0.49522 |  0:00:21s
epoch 46 | loss: 0.23484 | val_0_rmse: 0.45365 | val_1_rmse: 0.47155 |  0:00:21s
epoch 47 | loss: 0.22823 | val_0_rmse: 0.4565  | val_1_rmse: 0.48148 |  0:00:22s
epoch 48 | loss: 0.23065 | val_0_rmse: 0.47205 | val_1_rmse: 0.49862 |  0:00:22s
epoch 49 | loss: 0.22893 | val_0_rmse: 0.46299 | val_1_rmse: 0.49049 |  0:00:22s
epoch 50 | loss: 0.23058 | val_0_rmse: 0.45209 | val_1_rmse: 0.47653 |  0:00:23s
epoch 51 | loss: 0.22504 | val_0_rmse: 0.44809 | val_1_rmse: 0.47413 |  0:00:23s
epoch 52 | loss: 0.22481 | val_0_rmse: 0.45066 | val_1_rmse: 0.47838 |  0:00:24s
epoch 53 | loss: 0.2251  | val_0_rmse: 0.45503 | val_1_rmse: 0.47612 |  0:00:24s
epoch 54 | loss: 0.22861 | val_0_rmse: 0.45709 | val_1_rmse: 0.48249 |  0:00:25s
epoch 55 | loss: 0.22667 | val_0_rmse: 0.46081 | val_1_rmse: 0.47802 |  0:00:25s
epoch 56 | loss: 0.22606 | val_0_rmse: 0.45591 | val_1_rmse: 0.47639 |  0:00:26s
epoch 57 | loss: 0.22644 | val_0_rmse: 0.46443 | val_1_rmse: 0.49792 |  0:00:26s
epoch 58 | loss: 0.22726 | val_0_rmse: 0.45534 | val_1_rmse: 0.47617 |  0:00:27s
epoch 59 | loss: 0.22182 | val_0_rmse: 0.45188 | val_1_rmse: 0.47781 |  0:00:27s
epoch 60 | loss: 0.22421 | val_0_rmse: 0.45646 | val_1_rmse: 0.48177 |  0:00:28s
epoch 61 | loss: 0.22372 | val_0_rmse: 0.45048 | val_1_rmse: 0.47579 |  0:00:28s
epoch 62 | loss: 0.22148 | val_0_rmse: 0.44692 | val_1_rmse: 0.47346 |  0:00:28s
epoch 63 | loss: 0.22771 | val_0_rmse: 0.45141 | val_1_rmse: 0.47999 |  0:00:29s
epoch 64 | loss: 0.22668 | val_0_rmse: 0.45438 | val_1_rmse: 0.47839 |  0:00:29s
epoch 65 | loss: 0.22203 | val_0_rmse: 0.45722 | val_1_rmse: 0.48927 |  0:00:30s
epoch 66 | loss: 0.2266  | val_0_rmse: 0.46082 | val_1_rmse: 0.48153 |  0:00:30s
epoch 67 | loss: 0.22041 | val_0_rmse: 0.45053 | val_1_rmse: 0.47672 |  0:00:31s
epoch 68 | loss: 0.22289 | val_0_rmse: 0.44859 | val_1_rmse: 0.48156 |  0:00:31s
epoch 69 | loss: 0.22853 | val_0_rmse: 0.44924 | val_1_rmse: 0.47399 |  0:00:32s
epoch 70 | loss: 0.22844 | val_0_rmse: 0.46866 | val_1_rmse: 0.49926 |  0:00:32s
epoch 71 | loss: 0.22212 | val_0_rmse: 0.46031 | val_1_rmse: 0.48772 |  0:00:33s
epoch 72 | loss: 0.22461 | val_0_rmse: 0.45304 | val_1_rmse: 0.48971 |  0:00:33s
epoch 73 | loss: 0.22788 | val_0_rmse: 0.4667  | val_1_rmse: 0.49052 |  0:00:33s
epoch 74 | loss: 0.21859 | val_0_rmse: 0.45582 | val_1_rmse: 0.48491 |  0:00:34s
epoch 75 | loss: 0.21885 | val_0_rmse: 0.45058 | val_1_rmse: 0.4846  |  0:00:34s
epoch 76 | loss: 0.22545 | val_0_rmse: 0.44451 | val_1_rmse: 0.47314 |  0:00:35s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.47155
Best weights from best epoch are automatically used!
ended training at: 07:24:33
Feature importance:
[('Area', 0.34212270319479726), ('Baths', 0.07382082720111348), ('Beds', 0.058739038974895054), ('Latitude', 0.22515589810200565), ('Longitude', 0.21464489380876428), ('Month', 0.06287805090339174), ('Year', 0.022638587815032526)]
Mean squared error is of 19340263333.259026
Mean absolute error:99806.38679100947
MAPE:0.172902672607906
R2 score:0.7583543237126321
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:24:33
epoch 0  | loss: 1.01421 | val_0_rmse: 0.93035 | val_1_rmse: 0.87311 |  0:00:00s
epoch 1  | loss: 0.53656 | val_0_rmse: 0.76568 | val_1_rmse: 0.74471 |  0:00:00s
epoch 2  | loss: 0.44457 | val_0_rmse: 0.66695 | val_1_rmse: 0.65393 |  0:00:01s
epoch 3  | loss: 0.39229 | val_0_rmse: 0.62411 | val_1_rmse: 0.60712 |  0:00:01s
epoch 4  | loss: 0.36956 | val_0_rmse: 0.60276 | val_1_rmse: 0.58589 |  0:00:02s
epoch 5  | loss: 0.35404 | val_0_rmse: 0.60445 | val_1_rmse: 0.58251 |  0:00:02s
epoch 6  | loss: 0.33123 | val_0_rmse: 0.5681  | val_1_rmse: 0.54817 |  0:00:03s
epoch 7  | loss: 0.31076 | val_0_rmse: 0.55912 | val_1_rmse: 0.539   |  0:00:03s
epoch 8  | loss: 0.30566 | val_0_rmse: 0.5581  | val_1_rmse: 0.54495 |  0:00:04s
epoch 9  | loss: 0.30035 | val_0_rmse: 0.53066 | val_1_rmse: 0.52529 |  0:00:04s
epoch 10 | loss: 0.29028 | val_0_rmse: 0.52491 | val_1_rmse: 0.51661 |  0:00:05s
epoch 11 | loss: 0.27487 | val_0_rmse: 0.5065  | val_1_rmse: 0.49786 |  0:00:05s
epoch 12 | loss: 0.27263 | val_0_rmse: 0.50687 | val_1_rmse: 0.50954 |  0:00:06s
epoch 13 | loss: 0.28325 | val_0_rmse: 0.53318 | val_1_rmse: 0.53224 |  0:00:06s
epoch 14 | loss: 0.29631 | val_0_rmse: 0.54581 | val_1_rmse: 0.5365  |  0:00:06s
epoch 15 | loss: 0.27458 | val_0_rmse: 0.50624 | val_1_rmse: 0.50341 |  0:00:07s
epoch 16 | loss: 0.25909 | val_0_rmse: 0.49016 | val_1_rmse: 0.49018 |  0:00:07s
epoch 17 | loss: 0.25694 | val_0_rmse: 0.48707 | val_1_rmse: 0.48393 |  0:00:08s
epoch 18 | loss: 0.25323 | val_0_rmse: 0.48804 | val_1_rmse: 0.48031 |  0:00:08s
epoch 19 | loss: 0.26113 | val_0_rmse: 0.48703 | val_1_rmse: 0.48527 |  0:00:09s
epoch 20 | loss: 0.26534 | val_0_rmse: 0.47957 | val_1_rmse: 0.48062 |  0:00:09s
epoch 21 | loss: 0.2649  | val_0_rmse: 0.49434 | val_1_rmse: 0.48946 |  0:00:10s
epoch 22 | loss: 0.2614  | val_0_rmse: 0.49562 | val_1_rmse: 0.49386 |  0:00:10s
epoch 23 | loss: 0.25781 | val_0_rmse: 0.48842 | val_1_rmse: 0.48715 |  0:00:11s
epoch 24 | loss: 0.25438 | val_0_rmse: 0.49084 | val_1_rmse: 0.49909 |  0:00:11s
epoch 25 | loss: 0.26117 | val_0_rmse: 0.48736 | val_1_rmse: 0.49133 |  0:00:12s
epoch 26 | loss: 0.25042 | val_0_rmse: 0.47609 | val_1_rmse: 0.48371 |  0:00:12s
epoch 27 | loss: 0.24258 | val_0_rmse: 0.47997 | val_1_rmse: 0.48033 |  0:00:12s
epoch 28 | loss: 0.2467  | val_0_rmse: 0.49118 | val_1_rmse: 0.48342 |  0:00:13s
epoch 29 | loss: 0.25737 | val_0_rmse: 0.51432 | val_1_rmse: 0.51929 |  0:00:13s
epoch 30 | loss: 0.25885 | val_0_rmse: 0.48691 | val_1_rmse: 0.48451 |  0:00:14s
epoch 31 | loss: 0.2444  | val_0_rmse: 0.47174 | val_1_rmse: 0.47304 |  0:00:14s
epoch 32 | loss: 0.23741 | val_0_rmse: 0.46428 | val_1_rmse: 0.47384 |  0:00:15s
epoch 33 | loss: 0.23405 | val_0_rmse: 0.46824 | val_1_rmse: 0.47356 |  0:00:15s
epoch 34 | loss: 0.2348  | val_0_rmse: 0.46539 | val_1_rmse: 0.475   |  0:00:16s
epoch 35 | loss: 0.23991 | val_0_rmse: 0.46916 | val_1_rmse: 0.47744 |  0:00:16s
epoch 36 | loss: 0.23975 | val_0_rmse: 0.47904 | val_1_rmse: 0.48237 |  0:00:17s
epoch 37 | loss: 0.23829 | val_0_rmse: 0.47453 | val_1_rmse: 0.48951 |  0:00:17s
epoch 38 | loss: 0.24828 | val_0_rmse: 0.47143 | val_1_rmse: 0.47654 |  0:00:17s
epoch 39 | loss: 0.23839 | val_0_rmse: 0.4643  | val_1_rmse: 0.47559 |  0:00:18s
epoch 40 | loss: 0.24041 | val_0_rmse: 0.46841 | val_1_rmse: 0.48146 |  0:00:18s
epoch 41 | loss: 0.23376 | val_0_rmse: 0.46331 | val_1_rmse: 0.48154 |  0:00:19s
epoch 42 | loss: 0.23248 | val_0_rmse: 0.4566  | val_1_rmse: 0.46934 |  0:00:19s
epoch 43 | loss: 0.22996 | val_0_rmse: 0.45679 | val_1_rmse: 0.46828 |  0:00:20s
epoch 44 | loss: 0.22612 | val_0_rmse: 0.45326 | val_1_rmse: 0.46447 |  0:00:20s
epoch 45 | loss: 0.2282  | val_0_rmse: 0.45582 | val_1_rmse: 0.47002 |  0:00:21s
epoch 46 | loss: 0.2363  | val_0_rmse: 0.46414 | val_1_rmse: 0.48137 |  0:00:21s
epoch 47 | loss: 0.23404 | val_0_rmse: 0.45553 | val_1_rmse: 0.46971 |  0:00:22s
epoch 48 | loss: 0.22625 | val_0_rmse: 0.4556  | val_1_rmse: 0.47437 |  0:00:22s
epoch 49 | loss: 0.22963 | val_0_rmse: 0.463   | val_1_rmse: 0.47335 |  0:00:23s
epoch 50 | loss: 0.23038 | val_0_rmse: 0.46669 | val_1_rmse: 0.48154 |  0:00:23s
epoch 51 | loss: 0.22569 | val_0_rmse: 0.46332 | val_1_rmse: 0.48538 |  0:00:23s
epoch 52 | loss: 0.22756 | val_0_rmse: 0.45416 | val_1_rmse: 0.47161 |  0:00:24s
epoch 53 | loss: 0.22658 | val_0_rmse: 0.45743 | val_1_rmse: 0.47496 |  0:00:24s
epoch 54 | loss: 0.2253  | val_0_rmse: 0.45073 | val_1_rmse: 0.47372 |  0:00:25s
epoch 55 | loss: 0.22937 | val_0_rmse: 0.44868 | val_1_rmse: 0.46983 |  0:00:25s
epoch 56 | loss: 0.23143 | val_0_rmse: 0.45598 | val_1_rmse: 0.4791  |  0:00:26s
epoch 57 | loss: 0.2224  | val_0_rmse: 0.45228 | val_1_rmse: 0.47112 |  0:00:26s
epoch 58 | loss: 0.2248  | val_0_rmse: 0.45341 | val_1_rmse: 0.47161 |  0:00:27s
epoch 59 | loss: 0.23133 | val_0_rmse: 0.46848 | val_1_rmse: 0.48306 |  0:00:27s
epoch 60 | loss: 0.2276  | val_0_rmse: 0.45422 | val_1_rmse: 0.47175 |  0:00:28s
epoch 61 | loss: 0.22281 | val_0_rmse: 0.44896 | val_1_rmse: 0.47303 |  0:00:28s
epoch 62 | loss: 0.22444 | val_0_rmse: 0.45669 | val_1_rmse: 0.47763 |  0:00:28s
epoch 63 | loss: 0.22671 | val_0_rmse: 0.45398 | val_1_rmse: 0.46823 |  0:00:29s
epoch 64 | loss: 0.22048 | val_0_rmse: 0.44653 | val_1_rmse: 0.46912 |  0:00:29s
epoch 65 | loss: 0.21874 | val_0_rmse: 0.45056 | val_1_rmse: 0.48197 |  0:00:30s
epoch 66 | loss: 0.22105 | val_0_rmse: 0.45226 | val_1_rmse: 0.47949 |  0:00:30s
epoch 67 | loss: 0.22216 | val_0_rmse: 0.44523 | val_1_rmse: 0.46431 |  0:00:31s
epoch 68 | loss: 0.21571 | val_0_rmse: 0.44718 | val_1_rmse: 0.46972 |  0:00:31s
epoch 69 | loss: 0.21763 | val_0_rmse: 0.444   | val_1_rmse: 0.47311 |  0:00:32s
epoch 70 | loss: 0.22264 | val_0_rmse: 0.45109 | val_1_rmse: 0.4706  |  0:00:32s
epoch 71 | loss: 0.22823 | val_0_rmse: 0.45438 | val_1_rmse: 0.46748 |  0:00:33s
epoch 72 | loss: 0.21619 | val_0_rmse: 0.45036 | val_1_rmse: 0.46752 |  0:00:33s
epoch 73 | loss: 0.22029 | val_0_rmse: 0.45865 | val_1_rmse: 0.47826 |  0:00:34s
epoch 74 | loss: 0.22511 | val_0_rmse: 0.46774 | val_1_rmse: 0.49168 |  0:00:34s
epoch 75 | loss: 0.22612 | val_0_rmse: 0.4623  | val_1_rmse: 0.47295 |  0:00:34s
epoch 76 | loss: 0.22463 | val_0_rmse: 0.46461 | val_1_rmse: 0.47502 |  0:00:35s
epoch 77 | loss: 0.22424 | val_0_rmse: 0.46322 | val_1_rmse: 0.48417 |  0:00:35s
epoch 78 | loss: 0.21968 | val_0_rmse: 0.45034 | val_1_rmse: 0.46956 |  0:00:36s
epoch 79 | loss: 0.22069 | val_0_rmse: 0.4502  | val_1_rmse: 0.46475 |  0:00:36s
epoch 80 | loss: 0.22173 | val_0_rmse: 0.45092 | val_1_rmse: 0.47312 |  0:00:37s
epoch 81 | loss: 0.21742 | val_0_rmse: 0.4599  | val_1_rmse: 0.48692 |  0:00:37s
epoch 82 | loss: 0.22782 | val_0_rmse: 0.45819 | val_1_rmse: 0.48014 |  0:00:38s
epoch 83 | loss: 0.23384 | val_0_rmse: 0.45632 | val_1_rmse: 0.46753 |  0:00:38s
epoch 84 | loss: 0.23121 | val_0_rmse: 0.45838 | val_1_rmse: 0.48046 |  0:00:39s
epoch 85 | loss: 0.22224 | val_0_rmse: 0.44553 | val_1_rmse: 0.46713 |  0:00:39s
epoch 86 | loss: 0.21554 | val_0_rmse: 0.44893 | val_1_rmse: 0.46963 |  0:00:39s
epoch 87 | loss: 0.21213 | val_0_rmse: 0.4474  | val_1_rmse: 0.46939 |  0:00:40s
epoch 88 | loss: 0.21426 | val_0_rmse: 0.44372 | val_1_rmse: 0.4775  |  0:00:40s
epoch 89 | loss: 0.22089 | val_0_rmse: 0.44931 | val_1_rmse: 0.47245 |  0:00:41s
epoch 90 | loss: 0.21519 | val_0_rmse: 0.44819 | val_1_rmse: 0.4751  |  0:00:41s
epoch 91 | loss: 0.21854 | val_0_rmse: 0.44938 | val_1_rmse: 0.48234 |  0:00:42s
epoch 92 | loss: 0.21632 | val_0_rmse: 0.44729 | val_1_rmse: 0.47085 |  0:00:42s
epoch 93 | loss: 0.21546 | val_0_rmse: 0.44611 | val_1_rmse: 0.46592 |  0:00:43s
epoch 94 | loss: 0.21724 | val_0_rmse: 0.45008 | val_1_rmse: 0.47158 |  0:00:43s
epoch 95 | loss: 0.21533 | val_0_rmse: 0.45219 | val_1_rmse: 0.47232 |  0:00:44s
epoch 96 | loss: 0.22162 | val_0_rmse: 0.44636 | val_1_rmse: 0.46865 |  0:00:44s
epoch 97 | loss: 0.21727 | val_0_rmse: 0.44448 | val_1_rmse: 0.46668 |  0:00:44s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.46431
Best weights from best epoch are automatically used!
ended training at: 07:25:18
Feature importance:
[('Area', 0.22362300490883774), ('Baths', 0.05693169768686838), ('Beds', 0.09675321767642513), ('Latitude', 0.2181593264589633), ('Longitude', 0.3111633617271034), ('Month', 0.03791398962604141), ('Year', 0.05545540191576062)]
Mean squared error is of 20595793467.25377
Mean absolute error:102587.79425499476
MAPE:0.171438982773897
R2 score:0.7444604980804125
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:19
epoch 0  | loss: 1.60286 | val_0_rmse: 1.2775  | val_1_rmse: 1.24979 |  0:00:00s
epoch 1  | loss: 0.81193 | val_0_rmse: 1.24999 | val_1_rmse: 0.98221 |  0:00:00s
epoch 2  | loss: 0.70385 | val_0_rmse: 1.03429 | val_1_rmse: 1.00744 |  0:00:00s
epoch 3  | loss: 0.66901 | val_0_rmse: 0.80701 | val_1_rmse: 0.83141 |  0:00:00s
epoch 4  | loss: 0.60176 | val_0_rmse: 0.82685 | val_1_rmse: 0.91772 |  0:00:00s
epoch 5  | loss: 0.54616 | val_0_rmse: 0.78353 | val_1_rmse: 0.80402 |  0:00:00s
epoch 6  | loss: 0.52122 | val_0_rmse: 0.83403 | val_1_rmse: 0.82825 |  0:00:00s
epoch 7  | loss: 0.52606 | val_0_rmse: 0.81574 | val_1_rmse: 0.84518 |  0:00:01s
epoch 8  | loss: 0.49981 | val_0_rmse: 0.795   | val_1_rmse: 0.85564 |  0:00:01s
epoch 9  | loss: 0.49666 | val_0_rmse: 0.75483 | val_1_rmse: 0.81568 |  0:00:01s
epoch 10 | loss: 0.4758  | val_0_rmse: 0.72397 | val_1_rmse: 0.79126 |  0:00:01s
epoch 11 | loss: 0.47647 | val_0_rmse: 0.70402 | val_1_rmse: 0.77758 |  0:00:01s
epoch 12 | loss: 0.46292 | val_0_rmse: 0.69849 | val_1_rmse: 0.78709 |  0:00:01s
epoch 13 | loss: 0.46285 | val_0_rmse: 0.69685 | val_1_rmse: 0.76835 |  0:00:01s
epoch 14 | loss: 0.45403 | val_0_rmse: 0.7183  | val_1_rmse: 0.78982 |  0:00:02s
epoch 15 | loss: 0.47553 | val_0_rmse: 0.69208 | val_1_rmse: 0.77869 |  0:00:02s
epoch 16 | loss: 0.46057 | val_0_rmse: 0.68302 | val_1_rmse: 0.75966 |  0:00:02s
epoch 17 | loss: 0.46188 | val_0_rmse: 0.6944  | val_1_rmse: 0.75269 |  0:00:02s
epoch 18 | loss: 0.46693 | val_0_rmse: 0.67929 | val_1_rmse: 0.7388  |  0:00:02s
epoch 19 | loss: 0.44869 | val_0_rmse: 0.66835 | val_1_rmse: 0.73704 |  0:00:02s
epoch 20 | loss: 0.464   | val_0_rmse: 0.66941 | val_1_rmse: 0.7361  |  0:00:02s
epoch 21 | loss: 0.44244 | val_0_rmse: 0.68409 | val_1_rmse: 0.73803 |  0:00:02s
epoch 22 | loss: 0.44572 | val_0_rmse: 0.67242 | val_1_rmse: 0.73471 |  0:00:03s
epoch 23 | loss: 0.44645 | val_0_rmse: 0.66635 | val_1_rmse: 0.74108 |  0:00:03s
epoch 24 | loss: 0.44196 | val_0_rmse: 0.66237 | val_1_rmse: 0.7329  |  0:00:03s
epoch 25 | loss: 0.43927 | val_0_rmse: 0.66283 | val_1_rmse: 0.72619 |  0:00:03s
epoch 26 | loss: 0.43792 | val_0_rmse: 0.65212 | val_1_rmse: 0.7228  |  0:00:03s
epoch 27 | loss: 0.4411  | val_0_rmse: 0.653   | val_1_rmse: 0.7351  |  0:00:03s
epoch 28 | loss: 0.43634 | val_0_rmse: 0.64856 | val_1_rmse: 0.73426 |  0:00:03s
epoch 29 | loss: 0.43245 | val_0_rmse: 0.64874 | val_1_rmse: 0.73061 |  0:00:04s
epoch 30 | loss: 0.43515 | val_0_rmse: 0.65057 | val_1_rmse: 0.73285 |  0:00:04s
epoch 31 | loss: 0.44124 | val_0_rmse: 0.65222 | val_1_rmse: 0.71855 |  0:00:04s
epoch 32 | loss: 0.43958 | val_0_rmse: 0.66133 | val_1_rmse: 0.72287 |  0:00:04s
epoch 33 | loss: 0.44745 | val_0_rmse: 0.65579 | val_1_rmse: 0.72745 |  0:00:04s
epoch 34 | loss: 0.42708 | val_0_rmse: 0.65187 | val_1_rmse: 0.73197 |  0:00:04s
epoch 35 | loss: 0.43995 | val_0_rmse: 0.65211 | val_1_rmse: 0.73186 |  0:00:04s
epoch 36 | loss: 0.44134 | val_0_rmse: 0.65537 | val_1_rmse: 0.72791 |  0:00:05s
epoch 37 | loss: 0.43623 | val_0_rmse: 0.66097 | val_1_rmse: 0.7281  |  0:00:05s
epoch 38 | loss: 0.43422 | val_0_rmse: 0.65225 | val_1_rmse: 0.72711 |  0:00:05s
epoch 39 | loss: 0.42252 | val_0_rmse: 0.64896 | val_1_rmse: 0.72292 |  0:00:05s
epoch 40 | loss: 0.42943 | val_0_rmse: 0.65075 | val_1_rmse: 0.72315 |  0:00:05s
epoch 41 | loss: 0.43132 | val_0_rmse: 0.64811 | val_1_rmse: 0.72533 |  0:00:05s
epoch 42 | loss: 0.41841 | val_0_rmse: 0.64585 | val_1_rmse: 0.72738 |  0:00:05s
epoch 43 | loss: 0.42419 | val_0_rmse: 0.64996 | val_1_rmse: 0.72872 |  0:00:05s
epoch 44 | loss: 0.42556 | val_0_rmse: 0.64294 | val_1_rmse: 0.72515 |  0:00:06s
epoch 45 | loss: 0.41977 | val_0_rmse: 0.63675 | val_1_rmse: 0.7214  |  0:00:06s
epoch 46 | loss: 0.40927 | val_0_rmse: 0.6495  | val_1_rmse: 0.73099 |  0:00:06s
epoch 47 | loss: 0.41237 | val_0_rmse: 0.64319 | val_1_rmse: 0.727   |  0:00:06s
epoch 48 | loss: 0.4128  | val_0_rmse: 0.63366 | val_1_rmse: 0.72632 |  0:00:06s
epoch 49 | loss: 0.40299 | val_0_rmse: 0.63913 | val_1_rmse: 0.72957 |  0:00:06s
epoch 50 | loss: 0.3882  | val_0_rmse: 0.66218 | val_1_rmse: 0.75994 |  0:00:06s
epoch 51 | loss: 0.38957 | val_0_rmse: 0.62657 | val_1_rmse: 0.70162 |  0:00:07s
epoch 52 | loss: 0.38513 | val_0_rmse: 0.63957 | val_1_rmse: 0.71435 |  0:00:07s
epoch 53 | loss: 0.3947  | val_0_rmse: 0.60385 | val_1_rmse: 0.69212 |  0:00:07s
epoch 54 | loss: 0.38138 | val_0_rmse: 0.63875 | val_1_rmse: 0.72804 |  0:00:07s
epoch 55 | loss: 0.37789 | val_0_rmse: 0.61921 | val_1_rmse: 0.7152  |  0:00:07s
epoch 56 | loss: 0.3656  | val_0_rmse: 0.58945 | val_1_rmse: 0.69532 |  0:00:07s
epoch 57 | loss: 0.3746  | val_0_rmse: 0.59112 | val_1_rmse: 0.69321 |  0:00:07s
epoch 58 | loss: 0.35495 | val_0_rmse: 0.75271 | val_1_rmse: 0.82345 |  0:00:07s
epoch 59 | loss: 0.3702  | val_0_rmse: 0.71237 | val_1_rmse: 0.77829 |  0:00:08s
epoch 60 | loss: 0.37658 | val_0_rmse: 0.76636 | val_1_rmse: 0.82542 |  0:00:08s
epoch 61 | loss: 0.35724 | val_0_rmse: 0.89773 | val_1_rmse: 0.96443 |  0:00:08s
epoch 62 | loss: 0.36055 | val_0_rmse: 0.82568 | val_1_rmse: 0.9092  |  0:00:08s
epoch 63 | loss: 0.34797 | val_0_rmse: 0.67694 | val_1_rmse: 0.7799  |  0:00:08s
epoch 64 | loss: 0.35439 | val_0_rmse: 0.62815 | val_1_rmse: 0.73156 |  0:00:08s
epoch 65 | loss: 0.34927 | val_0_rmse: 0.62422 | val_1_rmse: 0.73449 |  0:00:08s
epoch 66 | loss: 0.3516  | val_0_rmse: 0.62578 | val_1_rmse: 0.72907 |  0:00:08s
epoch 67 | loss: 0.35683 | val_0_rmse: 0.60294 | val_1_rmse: 0.70224 |  0:00:09s
epoch 68 | loss: 0.36293 | val_0_rmse: 0.59933 | val_1_rmse: 0.70467 |  0:00:09s
epoch 69 | loss: 0.34275 | val_0_rmse: 0.60363 | val_1_rmse: 0.71341 |  0:00:09s
epoch 70 | loss: 0.34118 | val_0_rmse: 0.58131 | val_1_rmse: 0.69953 |  0:00:09s
epoch 71 | loss: 0.34764 | val_0_rmse: 0.60036 | val_1_rmse: 0.70578 |  0:00:09s
epoch 72 | loss: 0.33514 | val_0_rmse: 0.58767 | val_1_rmse: 0.69088 |  0:00:09s
epoch 73 | loss: 0.34588 | val_0_rmse: 0.59794 | val_1_rmse: 0.68798 |  0:00:09s
epoch 74 | loss: 0.35047 | val_0_rmse: 0.58048 | val_1_rmse: 0.67468 |  0:00:10s
epoch 75 | loss: 0.33328 | val_0_rmse: 0.58797 | val_1_rmse: 0.67877 |  0:00:10s
epoch 76 | loss: 0.33325 | val_0_rmse: 0.59234 | val_1_rmse: 0.68159 |  0:00:10s
epoch 77 | loss: 0.33709 | val_0_rmse: 0.59652 | val_1_rmse: 0.68175 |  0:00:10s
epoch 78 | loss: 0.33595 | val_0_rmse: 0.57403 | val_1_rmse: 0.66609 |  0:00:10s
epoch 79 | loss: 0.34424 | val_0_rmse: 0.56878 | val_1_rmse: 0.6649  |  0:00:10s
epoch 80 | loss: 0.32142 | val_0_rmse: 0.57241 | val_1_rmse: 0.66264 |  0:00:10s
epoch 81 | loss: 0.33189 | val_0_rmse: 0.57134 | val_1_rmse: 0.66816 |  0:00:11s
epoch 82 | loss: 0.3215  | val_0_rmse: 0.56995 | val_1_rmse: 0.67317 |  0:00:11s
epoch 83 | loss: 0.32023 | val_0_rmse: 0.56457 | val_1_rmse: 0.68326 |  0:00:11s
epoch 84 | loss: 0.32638 | val_0_rmse: 0.55981 | val_1_rmse: 0.67973 |  0:00:11s
epoch 85 | loss: 0.32898 | val_0_rmse: 0.57621 | val_1_rmse: 0.69379 |  0:00:11s
epoch 86 | loss: 0.31915 | val_0_rmse: 0.57771 | val_1_rmse: 0.70669 |  0:00:11s
epoch 87 | loss: 0.32036 | val_0_rmse: 0.57189 | val_1_rmse: 0.67697 |  0:00:11s
epoch 88 | loss: 0.32013 | val_0_rmse: 0.5561  | val_1_rmse: 0.66028 |  0:00:11s
epoch 89 | loss: 0.32328 | val_0_rmse: 0.55875 | val_1_rmse: 0.6567  |  0:00:12s
epoch 90 | loss: 0.31084 | val_0_rmse: 0.55362 | val_1_rmse: 0.64176 |  0:00:12s
epoch 91 | loss: 0.31362 | val_0_rmse: 0.5513  | val_1_rmse: 0.64054 |  0:00:12s
epoch 92 | loss: 0.33954 | val_0_rmse: 0.58622 | val_1_rmse: 0.67736 |  0:00:12s
epoch 93 | loss: 0.33727 | val_0_rmse: 0.7845  | val_1_rmse: 0.85727 |  0:00:12s
epoch 94 | loss: 0.33166 | val_0_rmse: 0.85951 | val_1_rmse: 0.92097 |  0:00:12s
epoch 95 | loss: 0.3516  | val_0_rmse: 0.79369 | val_1_rmse: 0.86851 |  0:00:12s
epoch 96 | loss: 0.35276 | val_0_rmse: 0.6465  | val_1_rmse: 0.73527 |  0:00:13s
epoch 97 | loss: 0.33907 | val_0_rmse: 0.54987 | val_1_rmse: 0.64376 |  0:00:13s
epoch 98 | loss: 0.30169 | val_0_rmse: 0.55939 | val_1_rmse: 0.64416 |  0:00:13s
epoch 99 | loss: 0.31651 | val_0_rmse: 0.56722 | val_1_rmse: 0.64842 |  0:00:13s
epoch 100| loss: 0.31862 | val_0_rmse: 0.57828 | val_1_rmse: 0.65963 |  0:00:13s
epoch 101| loss: 0.31603 | val_0_rmse: 0.57706 | val_1_rmse: 0.66124 |  0:00:13s
epoch 102| loss: 0.31731 | val_0_rmse: 0.56426 | val_1_rmse: 0.64567 |  0:00:13s
epoch 103| loss: 0.31303 | val_0_rmse: 0.56524 | val_1_rmse: 0.66154 |  0:00:13s
epoch 104| loss: 0.31375 | val_0_rmse: 0.57066 | val_1_rmse: 0.68429 |  0:00:14s
epoch 105| loss: 0.308   | val_0_rmse: 0.58398 | val_1_rmse: 0.69563 |  0:00:14s
epoch 106| loss: 0.31263 | val_0_rmse: 0.56065 | val_1_rmse: 0.66444 |  0:00:14s
epoch 107| loss: 0.31534 | val_0_rmse: 0.53561 | val_1_rmse: 0.64072 |  0:00:14s
epoch 108| loss: 0.31923 | val_0_rmse: 0.56331 | val_1_rmse: 0.64837 |  0:00:14s
epoch 109| loss: 0.31586 | val_0_rmse: 0.58207 | val_1_rmse: 0.68053 |  0:00:14s
epoch 110| loss: 0.29854 | val_0_rmse: 0.5958  | val_1_rmse: 0.70807 |  0:00:14s
epoch 111| loss: 0.31025 | val_0_rmse: 0.59024 | val_1_rmse: 0.70908 |  0:00:15s
epoch 112| loss: 0.29936 | val_0_rmse: 0.61938 | val_1_rmse: 0.70783 |  0:00:15s
epoch 113| loss: 0.32408 | val_0_rmse: 0.53664 | val_1_rmse: 0.63084 |  0:00:15s
epoch 114| loss: 0.33377 | val_0_rmse: 0.64461 | val_1_rmse: 0.72426 |  0:00:15s
epoch 115| loss: 0.33111 | val_0_rmse: 0.61183 | val_1_rmse: 0.70354 |  0:00:15s
epoch 116| loss: 0.3262  | val_0_rmse: 0.59259 | val_1_rmse: 0.66721 |  0:00:15s
epoch 117| loss: 0.30386 | val_0_rmse: 0.55678 | val_1_rmse: 0.65321 |  0:00:15s
epoch 118| loss: 0.32624 | val_0_rmse: 0.5522  | val_1_rmse: 0.66218 |  0:00:15s
epoch 119| loss: 0.29606 | val_0_rmse: 0.63685 | val_1_rmse: 0.73161 |  0:00:16s
epoch 120| loss: 0.31478 | val_0_rmse: 0.61809 | val_1_rmse: 0.73599 |  0:00:16s
epoch 121| loss: 0.31263 | val_0_rmse: 0.59591 | val_1_rmse: 0.72592 |  0:00:16s
epoch 122| loss: 0.31964 | val_0_rmse: 0.54405 | val_1_rmse: 0.67136 |  0:00:16s
epoch 123| loss: 0.30537 | val_0_rmse: 0.59324 | val_1_rmse: 0.69643 |  0:00:16s
epoch 124| loss: 0.3193  | val_0_rmse: 0.71619 | val_1_rmse: 0.80364 |  0:00:16s
epoch 125| loss: 0.31589 | val_0_rmse: 0.71259 | val_1_rmse: 0.79127 |  0:00:16s
epoch 126| loss: 0.31425 | val_0_rmse: 0.70403 | val_1_rmse: 0.77448 |  0:00:16s
epoch 127| loss: 0.30004 | val_0_rmse: 0.68625 | val_1_rmse: 0.75533 |  0:00:17s
epoch 128| loss: 0.30044 | val_0_rmse: 0.62414 | val_1_rmse: 0.70745 |  0:00:17s
epoch 129| loss: 0.30087 | val_0_rmse: 0.57658 | val_1_rmse: 0.67079 |  0:00:17s
epoch 130| loss: 0.29565 | val_0_rmse: 0.53965 | val_1_rmse: 0.65067 |  0:00:17s
epoch 131| loss: 0.30274 | val_0_rmse: 0.54087 | val_1_rmse: 0.65016 |  0:00:17s
epoch 132| loss: 0.29224 | val_0_rmse: 0.56808 | val_1_rmse: 0.67969 |  0:00:17s
epoch 133| loss: 0.28927 | val_0_rmse: 0.5474  | val_1_rmse: 0.66747 |  0:00:17s
epoch 134| loss: 0.28815 | val_0_rmse: 0.52815 | val_1_rmse: 0.6635  |  0:00:18s
epoch 135| loss: 0.29127 | val_0_rmse: 0.5502  | val_1_rmse: 0.68856 |  0:00:18s
epoch 136| loss: 0.29227 | val_0_rmse: 0.55615 | val_1_rmse: 0.67614 |  0:00:18s
epoch 137| loss: 0.29224 | val_0_rmse: 0.54116 | val_1_rmse: 0.66545 |  0:00:18s
epoch 138| loss: 0.2926  | val_0_rmse: 0.56099 | val_1_rmse: 0.69279 |  0:00:18s
epoch 139| loss: 0.29269 | val_0_rmse: 0.61864 | val_1_rmse: 0.73772 |  0:00:18s
epoch 140| loss: 0.28821 | val_0_rmse: 0.64272 | val_1_rmse: 0.75682 |  0:00:18s
epoch 141| loss: 0.29137 | val_0_rmse: 0.61649 | val_1_rmse: 0.7374  |  0:00:18s
epoch 142| loss: 0.31329 | val_0_rmse: 0.59354 | val_1_rmse: 0.70783 |  0:00:19s
epoch 143| loss: 0.2977  | val_0_rmse: 0.58277 | val_1_rmse: 0.71359 |  0:00:19s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.63084
Best weights from best epoch are automatically used!
ended training at: 07:25:38
Feature importance:
[('Area', 0.3018114339692341), ('Baths', 0.1687049119432097), ('Beds', 0.06175515251860559), ('Latitude', 0.4038137089758591), ('Longitude', 0.04441305840096397), ('Month', 0.00038210123921681637), ('Year', 0.019119632952910675)]
Mean squared error is of 2144893163.4515707
Mean absolute error:31941.831864114007
MAPE:0.2674902058964504
R2 score:0.6869729829370314
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:38
epoch 0  | loss: 1.55817 | val_0_rmse: 1.38182 | val_1_rmse: 1.33175 |  0:00:00s
epoch 1  | loss: 0.89048 | val_0_rmse: 0.89133 | val_1_rmse: 0.87427 |  0:00:00s
epoch 2  | loss: 0.62685 | val_0_rmse: 0.96456 | val_1_rmse: 0.91031 |  0:00:00s
epoch 3  | loss: 0.53997 | val_0_rmse: 0.86529 | val_1_rmse: 0.84452 |  0:00:00s
epoch 4  | loss: 0.51829 | val_0_rmse: 0.82305 | val_1_rmse: 0.84295 |  0:00:00s
epoch 5  | loss: 0.50757 | val_0_rmse: 0.80567 | val_1_rmse: 0.81657 |  0:00:00s
epoch 6  | loss: 0.50753 | val_0_rmse: 0.77243 | val_1_rmse: 0.78686 |  0:00:00s
epoch 7  | loss: 0.50705 | val_0_rmse: 0.7331  | val_1_rmse: 0.74741 |  0:00:01s
epoch 8  | loss: 0.48924 | val_0_rmse: 0.73219 | val_1_rmse: 0.74084 |  0:00:01s
epoch 9  | loss: 0.48721 | val_0_rmse: 0.74323 | val_1_rmse: 0.75557 |  0:00:01s
epoch 10 | loss: 0.48951 | val_0_rmse: 0.70629 | val_1_rmse: 0.70122 |  0:00:01s
epoch 11 | loss: 0.46536 | val_0_rmse: 0.71464 | val_1_rmse: 0.70705 |  0:00:01s
epoch 12 | loss: 0.46194 | val_0_rmse: 0.70193 | val_1_rmse: 0.70997 |  0:00:01s
epoch 13 | loss: 0.45578 | val_0_rmse: 0.68655 | val_1_rmse: 0.71141 |  0:00:01s
epoch 14 | loss: 0.45533 | val_0_rmse: 0.67331 | val_1_rmse: 0.71255 |  0:00:02s
epoch 15 | loss: 0.47324 | val_0_rmse: 0.66467 | val_1_rmse: 0.69947 |  0:00:02s
epoch 16 | loss: 0.46337 | val_0_rmse: 0.67135 | val_1_rmse: 0.70362 |  0:00:02s
epoch 17 | loss: 0.46525 | val_0_rmse: 0.66632 | val_1_rmse: 0.70087 |  0:00:02s
epoch 18 | loss: 0.45296 | val_0_rmse: 0.66712 | val_1_rmse: 0.70255 |  0:00:02s
epoch 19 | loss: 0.44233 | val_0_rmse: 0.66868 | val_1_rmse: 0.70543 |  0:00:02s
epoch 20 | loss: 0.44563 | val_0_rmse: 0.66219 | val_1_rmse: 0.69345 |  0:00:02s
epoch 21 | loss: 0.44752 | val_0_rmse: 0.66109 | val_1_rmse: 0.69189 |  0:00:02s
epoch 22 | loss: 0.45006 | val_0_rmse: 0.66047 | val_1_rmse: 0.6878  |  0:00:03s
epoch 23 | loss: 0.44167 | val_0_rmse: 0.65811 | val_1_rmse: 0.68672 |  0:00:03s
epoch 24 | loss: 0.44714 | val_0_rmse: 0.65241 | val_1_rmse: 0.68677 |  0:00:03s
epoch 25 | loss: 0.43629 | val_0_rmse: 0.65922 | val_1_rmse: 0.69991 |  0:00:03s
epoch 26 | loss: 0.44044 | val_0_rmse: 0.65079 | val_1_rmse: 0.69479 |  0:00:03s
epoch 27 | loss: 0.44593 | val_0_rmse: 0.65439 | val_1_rmse: 0.68982 |  0:00:03s
epoch 28 | loss: 0.43697 | val_0_rmse: 0.65422 | val_1_rmse: 0.6878  |  0:00:03s
epoch 29 | loss: 0.4371  | val_0_rmse: 0.64978 | val_1_rmse: 0.67566 |  0:00:04s
epoch 30 | loss: 0.43976 | val_0_rmse: 0.65387 | val_1_rmse: 0.68251 |  0:00:04s
epoch 31 | loss: 0.44151 | val_0_rmse: 0.64845 | val_1_rmse: 0.67212 |  0:00:04s
epoch 32 | loss: 0.43396 | val_0_rmse: 0.64629 | val_1_rmse: 0.67003 |  0:00:04s
epoch 33 | loss: 0.42933 | val_0_rmse: 0.64786 | val_1_rmse: 0.66777 |  0:00:04s
epoch 34 | loss: 0.42794 | val_0_rmse: 0.64686 | val_1_rmse: 0.66677 |  0:00:04s
epoch 35 | loss: 0.4261  | val_0_rmse: 0.64518 | val_1_rmse: 0.67382 |  0:00:04s
epoch 36 | loss: 0.43594 | val_0_rmse: 0.64145 | val_1_rmse: 0.67345 |  0:00:05s
epoch 37 | loss: 0.42681 | val_0_rmse: 0.64426 | val_1_rmse: 0.67323 |  0:00:05s
epoch 38 | loss: 0.4221  | val_0_rmse: 0.64447 | val_1_rmse: 0.67681 |  0:00:05s
epoch 39 | loss: 0.43896 | val_0_rmse: 0.64315 | val_1_rmse: 0.67826 |  0:00:05s
epoch 40 | loss: 0.42993 | val_0_rmse: 0.64302 | val_1_rmse: 0.67786 |  0:00:05s
epoch 41 | loss: 0.43159 | val_0_rmse: 0.64793 | val_1_rmse: 0.6777  |  0:00:05s
epoch 42 | loss: 0.43063 | val_0_rmse: 0.64787 | val_1_rmse: 0.67252 |  0:00:05s
epoch 43 | loss: 0.42544 | val_0_rmse: 0.65258 | val_1_rmse: 0.68351 |  0:00:05s
epoch 44 | loss: 0.43374 | val_0_rmse: 0.64968 | val_1_rmse: 0.689   |  0:00:06s
epoch 45 | loss: 0.42469 | val_0_rmse: 0.65535 | val_1_rmse: 0.68958 |  0:00:06s
epoch 46 | loss: 0.42893 | val_0_rmse: 0.64275 | val_1_rmse: 0.67714 |  0:00:06s
epoch 47 | loss: 0.4256  | val_0_rmse: 0.6437  | val_1_rmse: 0.67611 |  0:00:06s
epoch 48 | loss: 0.41842 | val_0_rmse: 0.6462  | val_1_rmse: 0.67318 |  0:00:06s
epoch 49 | loss: 0.42274 | val_0_rmse: 0.64351 | val_1_rmse: 0.67194 |  0:00:06s
epoch 50 | loss: 0.43208 | val_0_rmse: 0.63948 | val_1_rmse: 0.67508 |  0:00:06s
epoch 51 | loss: 0.41362 | val_0_rmse: 0.64428 | val_1_rmse: 0.68695 |  0:00:07s
epoch 52 | loss: 0.4136  | val_0_rmse: 0.6454  | val_1_rmse: 0.68622 |  0:00:07s
epoch 53 | loss: 0.41949 | val_0_rmse: 0.63761 | val_1_rmse: 0.67235 |  0:00:07s
epoch 54 | loss: 0.42902 | val_0_rmse: 0.63622 | val_1_rmse: 0.67171 |  0:00:07s
epoch 55 | loss: 0.42649 | val_0_rmse: 0.6372  | val_1_rmse: 0.67203 |  0:00:07s
epoch 56 | loss: 0.42272 | val_0_rmse: 0.63625 | val_1_rmse: 0.66594 |  0:00:07s
epoch 57 | loss: 0.42588 | val_0_rmse: 0.6434  | val_1_rmse: 0.67427 |  0:00:07s
epoch 58 | loss: 0.4248  | val_0_rmse: 0.64086 | val_1_rmse: 0.67049 |  0:00:07s
epoch 59 | loss: 0.41003 | val_0_rmse: 0.63882 | val_1_rmse: 0.66704 |  0:00:08s
epoch 60 | loss: 0.42651 | val_0_rmse: 0.63717 | val_1_rmse: 0.66468 |  0:00:08s
epoch 61 | loss: 0.42169 | val_0_rmse: 0.64287 | val_1_rmse: 0.66599 |  0:00:08s
epoch 62 | loss: 0.42188 | val_0_rmse: 0.64215 | val_1_rmse: 0.65626 |  0:00:08s
epoch 63 | loss: 0.42503 | val_0_rmse: 0.64487 | val_1_rmse: 0.66239 |  0:00:08s
epoch 64 | loss: 0.42069 | val_0_rmse: 0.64118 | val_1_rmse: 0.67185 |  0:00:08s
epoch 65 | loss: 0.41007 | val_0_rmse: 0.65521 | val_1_rmse: 0.68199 |  0:00:08s
epoch 66 | loss: 0.4219  | val_0_rmse: 0.64335 | val_1_rmse: 0.67006 |  0:00:09s
epoch 67 | loss: 0.41697 | val_0_rmse: 0.63777 | val_1_rmse: 0.66447 |  0:00:09s
epoch 68 | loss: 0.41261 | val_0_rmse: 0.63823 | val_1_rmse: 0.65863 |  0:00:09s
epoch 69 | loss: 0.40866 | val_0_rmse: 0.64397 | val_1_rmse: 0.66149 |  0:00:09s
epoch 70 | loss: 0.41418 | val_0_rmse: 0.65328 | val_1_rmse: 0.66873 |  0:00:09s
epoch 71 | loss: 0.42585 | val_0_rmse: 0.63833 | val_1_rmse: 0.66278 |  0:00:09s
epoch 72 | loss: 0.41884 | val_0_rmse: 0.64461 | val_1_rmse: 0.6758  |  0:00:09s
epoch 73 | loss: 0.42704 | val_0_rmse: 0.64077 | val_1_rmse: 0.68182 |  0:00:09s
epoch 74 | loss: 0.41892 | val_0_rmse: 0.63745 | val_1_rmse: 0.67312 |  0:00:10s
epoch 75 | loss: 0.41164 | val_0_rmse: 0.63739 | val_1_rmse: 0.67364 |  0:00:10s
epoch 76 | loss: 0.41995 | val_0_rmse: 0.64185 | val_1_rmse: 0.67569 |  0:00:10s
epoch 77 | loss: 0.4228  | val_0_rmse: 0.63952 | val_1_rmse: 0.67186 |  0:00:10s
epoch 78 | loss: 0.41871 | val_0_rmse: 0.63636 | val_1_rmse: 0.6627  |  0:00:10s
epoch 79 | loss: 0.41574 | val_0_rmse: 0.64083 | val_1_rmse: 0.66572 |  0:00:10s
epoch 80 | loss: 0.40993 | val_0_rmse: 0.65301 | val_1_rmse: 0.68443 |  0:00:10s
epoch 81 | loss: 0.42129 | val_0_rmse: 0.64719 | val_1_rmse: 0.68627 |  0:00:11s
epoch 82 | loss: 0.41836 | val_0_rmse: 0.64109 | val_1_rmse: 0.67258 |  0:00:11s
epoch 83 | loss: 0.41729 | val_0_rmse: 0.64739 | val_1_rmse: 0.66975 |  0:00:11s
epoch 84 | loss: 0.42807 | val_0_rmse: 0.64453 | val_1_rmse: 0.67204 |  0:00:11s
epoch 85 | loss: 0.41747 | val_0_rmse: 0.64169 | val_1_rmse: 0.67278 |  0:00:11s
epoch 86 | loss: 0.42749 | val_0_rmse: 0.63942 | val_1_rmse: 0.66911 |  0:00:11s
epoch 87 | loss: 0.4172  | val_0_rmse: 0.64558 | val_1_rmse: 0.6779  |  0:00:11s
epoch 88 | loss: 0.41568 | val_0_rmse: 0.63252 | val_1_rmse: 0.65807 |  0:00:11s
epoch 89 | loss: 0.41801 | val_0_rmse: 0.63057 | val_1_rmse: 0.66104 |  0:00:12s
epoch 90 | loss: 0.41201 | val_0_rmse: 0.62981 | val_1_rmse: 0.66732 |  0:00:12s
epoch 91 | loss: 0.40084 | val_0_rmse: 0.63005 | val_1_rmse: 0.67012 |  0:00:12s
epoch 92 | loss: 0.41028 | val_0_rmse: 0.63101 | val_1_rmse: 0.67225 |  0:00:12s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.65626
Best weights from best epoch are automatically used!
ended training at: 07:25:51
Feature importance:
[('Area', 0.3800512830482448), ('Baths', 0.15749216494009022), ('Beds', 0.11126106970494139), ('Latitude', 0.10718263801276348), ('Longitude', 0.08160445497537444), ('Month', 0.08533386452419404), ('Year', 0.07707452479439161)]
Mean squared error is of 3302142886.7570443
Mean absolute error:42041.490141140115
MAPE:0.387321649727098
R2 score:0.5442678064009487
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:25:51
epoch 0  | loss: 1.52502 | val_0_rmse: 1.33896 | val_1_rmse: 1.37    |  0:00:00s
epoch 1  | loss: 1.16988 | val_0_rmse: 1.05    | val_1_rmse: 1.053   |  0:00:00s
epoch 2  | loss: 0.7812  | val_0_rmse: 1.03135 | val_1_rmse: 0.95518 |  0:00:00s
epoch 3  | loss: 0.70712 | val_0_rmse: 1.14861 | val_1_rmse: 1.07249 |  0:00:00s
epoch 4  | loss: 0.6172  | val_0_rmse: 0.87111 | val_1_rmse: 0.83968 |  0:00:00s
epoch 5  | loss: 0.57419 | val_0_rmse: 0.98622 | val_1_rmse: 0.78055 |  0:00:00s
epoch 6  | loss: 0.53953 | val_0_rmse: 0.92172 | val_1_rmse: 0.69228 |  0:00:00s
epoch 7  | loss: 0.52312 | val_0_rmse: 0.72929 | val_1_rmse: 0.69424 |  0:00:01s
epoch 8  | loss: 0.51411 | val_0_rmse: 0.73168 | val_1_rmse: 0.70627 |  0:00:01s
epoch 9  | loss: 0.5033  | val_0_rmse: 0.741   | val_1_rmse: 0.71318 |  0:00:01s
epoch 10 | loss: 0.50096 | val_0_rmse: 0.73813 | val_1_rmse: 0.71105 |  0:00:01s
epoch 11 | loss: 0.49743 | val_0_rmse: 0.71806 | val_1_rmse: 0.68622 |  0:00:01s
epoch 12 | loss: 0.47959 | val_0_rmse: 0.70253 | val_1_rmse: 0.67188 |  0:00:01s
epoch 13 | loss: 0.48017 | val_0_rmse: 0.69623 | val_1_rmse: 0.66301 |  0:00:01s
epoch 14 | loss: 0.47788 | val_0_rmse: 0.69124 | val_1_rmse: 0.65275 |  0:00:02s
epoch 15 | loss: 0.47019 | val_0_rmse: 0.68731 | val_1_rmse: 0.65578 |  0:00:02s
epoch 16 | loss: 0.47194 | val_0_rmse: 0.69182 | val_1_rmse: 0.66925 |  0:00:02s
epoch 17 | loss: 0.46494 | val_0_rmse: 0.6951  | val_1_rmse: 0.67579 |  0:00:02s
epoch 18 | loss: 0.4618  | val_0_rmse: 0.68661 | val_1_rmse: 0.66815 |  0:00:02s
epoch 19 | loss: 0.46246 | val_0_rmse: 0.67862 | val_1_rmse: 0.65183 |  0:00:02s
epoch 20 | loss: 0.45424 | val_0_rmse: 0.67927 | val_1_rmse: 0.65232 |  0:00:02s
epoch 21 | loss: 0.46389 | val_0_rmse: 0.67644 | val_1_rmse: 0.64995 |  0:00:03s
epoch 22 | loss: 0.45817 | val_0_rmse: 0.66781 | val_1_rmse: 0.64641 |  0:00:03s
epoch 23 | loss: 0.45196 | val_0_rmse: 0.66884 | val_1_rmse: 0.6497  |  0:00:03s
epoch 24 | loss: 0.45207 | val_0_rmse: 0.67019 | val_1_rmse: 0.64943 |  0:00:03s
epoch 25 | loss: 0.44105 | val_0_rmse: 0.66184 | val_1_rmse: 0.63309 |  0:00:03s
epoch 26 | loss: 0.43807 | val_0_rmse: 0.65988 | val_1_rmse: 0.63385 |  0:00:03s
epoch 27 | loss: 0.45462 | val_0_rmse: 0.66128 | val_1_rmse: 0.63902 |  0:00:03s
epoch 28 | loss: 0.43831 | val_0_rmse: 0.66838 | val_1_rmse: 0.64863 |  0:00:03s
epoch 29 | loss: 0.44235 | val_0_rmse: 0.67159 | val_1_rmse: 0.64471 |  0:00:04s
epoch 30 | loss: 0.44884 | val_0_rmse: 0.65471 | val_1_rmse: 0.64137 |  0:00:04s
epoch 31 | loss: 0.44444 | val_0_rmse: 0.66184 | val_1_rmse: 0.64404 |  0:00:04s
epoch 32 | loss: 0.4466  | val_0_rmse: 0.65531 | val_1_rmse: 0.63275 |  0:00:04s
epoch 33 | loss: 0.44198 | val_0_rmse: 0.66063 | val_1_rmse: 0.63716 |  0:00:04s
epoch 34 | loss: 0.44671 | val_0_rmse: 0.65705 | val_1_rmse: 0.63736 |  0:00:04s
epoch 35 | loss: 0.43994 | val_0_rmse: 0.6611  | val_1_rmse: 0.63499 |  0:00:04s
epoch 36 | loss: 0.44313 | val_0_rmse: 0.65735 | val_1_rmse: 0.62893 |  0:00:04s
epoch 37 | loss: 0.43845 | val_0_rmse: 0.65105 | val_1_rmse: 0.62541 |  0:00:05s
epoch 38 | loss: 0.43797 | val_0_rmse: 0.65252 | val_1_rmse: 0.63192 |  0:00:05s
epoch 39 | loss: 0.44081 | val_0_rmse: 0.66057 | val_1_rmse: 0.63202 |  0:00:05s
epoch 40 | loss: 0.44589 | val_0_rmse: 0.66149 | val_1_rmse: 0.62394 |  0:00:05s
epoch 41 | loss: 0.44739 | val_0_rmse: 0.66242 | val_1_rmse: 0.63509 |  0:00:05s
epoch 42 | loss: 0.45086 | val_0_rmse: 0.66346 | val_1_rmse: 0.63134 |  0:00:05s
epoch 43 | loss: 0.44201 | val_0_rmse: 0.66733 | val_1_rmse: 0.6284  |  0:00:05s
epoch 44 | loss: 0.45263 | val_0_rmse: 0.66287 | val_1_rmse: 0.6296  |  0:00:06s
epoch 45 | loss: 0.44702 | val_0_rmse: 0.66521 | val_1_rmse: 0.64244 |  0:00:06s
epoch 46 | loss: 0.44776 | val_0_rmse: 0.66658 | val_1_rmse: 0.64794 |  0:00:06s
epoch 47 | loss: 0.45056 | val_0_rmse: 0.6777  | val_1_rmse: 0.64778 |  0:00:06s
epoch 48 | loss: 0.4431  | val_0_rmse: 0.6669  | val_1_rmse: 0.63143 |  0:00:06s
epoch 49 | loss: 0.45356 | val_0_rmse: 0.66315 | val_1_rmse: 0.63108 |  0:00:06s
epoch 50 | loss: 0.44248 | val_0_rmse: 0.65981 | val_1_rmse: 0.62608 |  0:00:06s
epoch 51 | loss: 0.43526 | val_0_rmse: 0.65729 | val_1_rmse: 0.62484 |  0:00:06s
epoch 52 | loss: 0.43032 | val_0_rmse: 0.65055 | val_1_rmse: 0.62702 |  0:00:07s
epoch 53 | loss: 0.43481 | val_0_rmse: 0.6558  | val_1_rmse: 0.63439 |  0:00:07s
epoch 54 | loss: 0.4401  | val_0_rmse: 0.65264 | val_1_rmse: 0.64241 |  0:00:07s
epoch 55 | loss: 0.44425 | val_0_rmse: 0.6578  | val_1_rmse: 0.65174 |  0:00:07s
epoch 56 | loss: 0.43611 | val_0_rmse: 0.6705  | val_1_rmse: 0.65143 |  0:00:07s
epoch 57 | loss: 0.43995 | val_0_rmse: 0.66139 | val_1_rmse: 0.64387 |  0:00:07s
epoch 58 | loss: 0.43625 | val_0_rmse: 0.66218 | val_1_rmse: 0.64665 |  0:00:07s
epoch 59 | loss: 0.42376 | val_0_rmse: 0.66019 | val_1_rmse: 0.6394  |  0:00:08s
epoch 60 | loss: 0.43372 | val_0_rmse: 0.65835 | val_1_rmse: 0.63211 |  0:00:08s
epoch 61 | loss: 0.43805 | val_0_rmse: 0.6661  | val_1_rmse: 0.64642 |  0:00:08s
epoch 62 | loss: 0.44357 | val_0_rmse: 0.66826 | val_1_rmse: 0.64691 |  0:00:08s
epoch 63 | loss: 0.4394  | val_0_rmse: 0.6583  | val_1_rmse: 0.63581 |  0:00:08s
epoch 64 | loss: 0.45193 | val_0_rmse: 0.66445 | val_1_rmse: 0.63865 |  0:00:08s
epoch 65 | loss: 0.4415  | val_0_rmse: 0.67323 | val_1_rmse: 0.65241 |  0:00:08s
epoch 66 | loss: 0.44221 | val_0_rmse: 0.67681 | val_1_rmse: 0.65394 |  0:00:08s
epoch 67 | loss: 0.43757 | val_0_rmse: 0.67138 | val_1_rmse: 0.63561 |  0:00:09s
epoch 68 | loss: 0.44605 | val_0_rmse: 0.66769 | val_1_rmse: 0.62837 |  0:00:09s
epoch 69 | loss: 0.43966 | val_0_rmse: 0.6699  | val_1_rmse: 0.6235  |  0:00:09s
epoch 70 | loss: 0.43956 | val_0_rmse: 0.6752  | val_1_rmse: 0.6199  |  0:00:09s
epoch 71 | loss: 0.44762 | val_0_rmse: 0.66691 | val_1_rmse: 0.62144 |  0:00:09s
epoch 72 | loss: 0.44054 | val_0_rmse: 0.66014 | val_1_rmse: 0.61631 |  0:00:09s
epoch 73 | loss: 0.43129 | val_0_rmse: 0.65849 | val_1_rmse: 0.62151 |  0:00:09s
epoch 74 | loss: 0.43512 | val_0_rmse: 0.65961 | val_1_rmse: 0.6209  |  0:00:10s
epoch 75 | loss: 0.42653 | val_0_rmse: 0.65741 | val_1_rmse: 0.62215 |  0:00:10s
epoch 76 | loss: 0.4331  | val_0_rmse: 0.64234 | val_1_rmse: 0.61074 |  0:00:10s
epoch 77 | loss: 0.4215  | val_0_rmse: 0.65352 | val_1_rmse: 0.62233 |  0:00:10s
epoch 78 | loss: 0.41934 | val_0_rmse: 0.64967 | val_1_rmse: 0.61103 |  0:00:10s
epoch 79 | loss: 0.41552 | val_0_rmse: 0.65756 | val_1_rmse: 0.61597 |  0:00:10s
epoch 80 | loss: 0.44146 | val_0_rmse: 0.6545  | val_1_rmse: 0.62411 |  0:00:10s
epoch 81 | loss: 0.44645 | val_0_rmse: 0.66104 | val_1_rmse: 0.6358  |  0:00:11s
epoch 82 | loss: 0.43049 | val_0_rmse: 0.66145 | val_1_rmse: 0.63975 |  0:00:11s
epoch 83 | loss: 0.42905 | val_0_rmse: 0.65331 | val_1_rmse: 0.63218 |  0:00:11s
epoch 84 | loss: 0.41851 | val_0_rmse: 0.67402 | val_1_rmse: 0.63664 |  0:00:11s
epoch 85 | loss: 0.41213 | val_0_rmse: 0.64081 | val_1_rmse: 0.61374 |  0:00:11s
epoch 86 | loss: 0.42199 | val_0_rmse: 0.63338 | val_1_rmse: 0.59601 |  0:00:11s
epoch 87 | loss: 0.4074  | val_0_rmse: 0.65795 | val_1_rmse: 0.60138 |  0:00:11s
epoch 88 | loss: 0.40093 | val_0_rmse: 0.62461 | val_1_rmse: 0.58703 |  0:00:11s
epoch 89 | loss: 0.40474 | val_0_rmse: 0.65698 | val_1_rmse: 0.60563 |  0:00:12s
epoch 90 | loss: 0.41335 | val_0_rmse: 0.75341 | val_1_rmse: 0.70755 |  0:00:12s
epoch 91 | loss: 0.39811 | val_0_rmse: 0.66664 | val_1_rmse: 0.62891 |  0:00:12s
epoch 92 | loss: 0.41799 | val_0_rmse: 0.66733 | val_1_rmse: 0.61734 |  0:00:12s
epoch 93 | loss: 0.36898 | val_0_rmse: 0.82183 | val_1_rmse: 0.76334 |  0:00:12s
epoch 94 | loss: 0.37286 | val_0_rmse: 0.85213 | val_1_rmse: 0.79215 |  0:00:12s
epoch 95 | loss: 0.37052 | val_0_rmse: 0.8213  | val_1_rmse: 0.77219 |  0:00:12s
epoch 96 | loss: 0.38419 | val_0_rmse: 0.67846 | val_1_rmse: 0.64005 |  0:00:12s
epoch 97 | loss: 0.36738 | val_0_rmse: 0.59776 | val_1_rmse: 0.5602  |  0:00:13s
epoch 98 | loss: 0.38211 | val_0_rmse: 0.61593 | val_1_rmse: 0.58379 |  0:00:13s
epoch 99 | loss: 0.38626 | val_0_rmse: 0.59513 | val_1_rmse: 0.55368 |  0:00:13s
epoch 100| loss: 0.35775 | val_0_rmse: 0.90301 | val_1_rmse: 0.85654 |  0:00:13s
epoch 101| loss: 0.35603 | val_0_rmse: 1.16629 | val_1_rmse: 1.11461 |  0:00:13s
epoch 102| loss: 0.3758  | val_0_rmse: 0.95513 | val_1_rmse: 0.90354 |  0:00:13s
epoch 103| loss: 0.34476 | val_0_rmse: 0.63843 | val_1_rmse: 0.58574 |  0:00:13s
epoch 104| loss: 0.36019 | val_0_rmse: 0.61789 | val_1_rmse: 0.57059 |  0:00:14s
epoch 105| loss: 0.35162 | val_0_rmse: 0.722   | val_1_rmse: 0.67469 |  0:00:14s
epoch 106| loss: 0.36282 | val_0_rmse: 0.67112 | val_1_rmse: 0.62456 |  0:00:14s
epoch 107| loss: 0.35185 | val_0_rmse: 0.58403 | val_1_rmse: 0.5489  |  0:00:14s
epoch 108| loss: 0.34015 | val_0_rmse: 0.60654 | val_1_rmse: 0.58121 |  0:00:14s
epoch 109| loss: 0.35288 | val_0_rmse: 0.64958 | val_1_rmse: 0.619   |  0:00:14s
epoch 110| loss: 0.33825 | val_0_rmse: 0.62344 | val_1_rmse: 0.59147 |  0:00:14s
epoch 111| loss: 0.33225 | val_0_rmse: 0.65592 | val_1_rmse: 0.63016 |  0:00:14s
epoch 112| loss: 0.33552 | val_0_rmse: 0.6644  | val_1_rmse: 0.63421 |  0:00:15s
epoch 113| loss: 0.34373 | val_0_rmse: 0.58859 | val_1_rmse: 0.56172 |  0:00:15s
epoch 114| loss: 0.33584 | val_0_rmse: 0.56631 | val_1_rmse: 0.53758 |  0:00:15s
epoch 115| loss: 0.32358 | val_0_rmse: 0.57709 | val_1_rmse: 0.55611 |  0:00:15s
epoch 116| loss: 0.31859 | val_0_rmse: 0.60295 | val_1_rmse: 0.58506 |  0:00:15s
epoch 117| loss: 0.33338 | val_0_rmse: 0.63098 | val_1_rmse: 0.61192 |  0:00:15s
epoch 118| loss: 0.32085 | val_0_rmse: 0.66346 | val_1_rmse: 0.64961 |  0:00:15s
epoch 119| loss: 0.33751 | val_0_rmse: 0.6471  | val_1_rmse: 0.63189 |  0:00:16s
epoch 120| loss: 0.33098 | val_0_rmse: 0.57753 | val_1_rmse: 0.55227 |  0:00:16s
epoch 121| loss: 0.32813 | val_0_rmse: 0.60251 | val_1_rmse: 0.57428 |  0:00:16s
epoch 122| loss: 0.32375 | val_0_rmse: 0.63227 | val_1_rmse: 0.61629 |  0:00:16s
epoch 123| loss: 0.34226 | val_0_rmse: 0.65122 | val_1_rmse: 0.64401 |  0:00:16s
epoch 124| loss: 0.33594 | val_0_rmse: 0.59182 | val_1_rmse: 0.58185 |  0:00:16s
epoch 125| loss: 0.32278 | val_0_rmse: 0.57897 | val_1_rmse: 0.56323 |  0:00:16s
epoch 126| loss: 0.33297 | val_0_rmse: 0.58085 | val_1_rmse: 0.56438 |  0:00:17s
epoch 127| loss: 0.33005 | val_0_rmse: 0.59812 | val_1_rmse: 0.5676  |  0:00:17s
epoch 128| loss: 0.31116 | val_0_rmse: 0.55817 | val_1_rmse: 0.52624 |  0:00:17s
epoch 129| loss: 0.31856 | val_0_rmse: 0.57373 | val_1_rmse: 0.55002 |  0:00:17s
epoch 130| loss: 0.3203  | val_0_rmse: 0.57259 | val_1_rmse: 0.5532  |  0:00:17s
epoch 131| loss: 0.32185 | val_0_rmse: 0.55425 | val_1_rmse: 0.52897 |  0:00:17s
epoch 132| loss: 0.3108  | val_0_rmse: 0.67647 | val_1_rmse: 0.63545 |  0:00:17s
epoch 133| loss: 0.31126 | val_0_rmse: 0.85526 | val_1_rmse: 0.81279 |  0:00:17s
epoch 134| loss: 0.31091 | val_0_rmse: 0.88241 | val_1_rmse: 0.84552 |  0:00:18s
epoch 135| loss: 0.31367 | val_0_rmse: 0.79997 | val_1_rmse: 0.77043 |  0:00:18s
epoch 136| loss: 0.30128 | val_0_rmse: 0.71199 | val_1_rmse: 0.68454 |  0:00:18s
epoch 137| loss: 0.30827 | val_0_rmse: 0.68966 | val_1_rmse: 0.65792 |  0:00:18s
epoch 138| loss: 0.302   | val_0_rmse: 0.64124 | val_1_rmse: 0.6095  |  0:00:18s
epoch 139| loss: 0.30681 | val_0_rmse: 0.63201 | val_1_rmse: 0.59182 |  0:00:18s
epoch 140| loss: 0.31165 | val_0_rmse: 0.66311 | val_1_rmse: 0.62056 |  0:00:18s
epoch 141| loss: 0.31754 | val_0_rmse: 0.6344  | val_1_rmse: 0.60836 |  0:00:18s
epoch 142| loss: 0.31492 | val_0_rmse: 0.58569 | val_1_rmse: 0.57066 |  0:00:19s
epoch 143| loss: 0.29964 | val_0_rmse: 0.65454 | val_1_rmse: 0.63844 |  0:00:19s
epoch 144| loss: 0.30219 | val_0_rmse: 0.63615 | val_1_rmse: 0.62413 |  0:00:19s
epoch 145| loss: 0.31878 | val_0_rmse: 0.62378 | val_1_rmse: 0.61538 |  0:00:19s
epoch 146| loss: 0.31023 | val_0_rmse: 0.69724 | val_1_rmse: 0.68974 |  0:00:19s
epoch 147| loss: 0.31141 | val_0_rmse: 0.70899 | val_1_rmse: 0.69291 |  0:00:19s
epoch 148| loss: 0.31628 | val_0_rmse: 0.54461 | val_1_rmse: 0.5366  |  0:00:19s
epoch 149| loss: 0.30002 | val_0_rmse: 0.59044 | val_1_rmse: 0.55885 |  0:00:20s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.52624
Best weights from best epoch are automatically used!
ended training at: 07:26:11
Feature importance:
[('Area', 0.336551758312263), ('Baths', 0.2390421790065519), ('Beds', 0.06398564274187987), ('Latitude', 0.2811140821573022), ('Longitude', 0.056069570973768364), ('Month', 0.006112841615109659), ('Year', 0.017123925193125)]
Mean squared error is of 3365641608.3172817
Mean absolute error:38502.66849948489
MAPE:0.318868463112799
R2 score:0.5781480991826167
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:26:11
epoch 0  | loss: 1.68035 | val_0_rmse: 0.98359 | val_1_rmse: 0.96637 |  0:00:00s
epoch 1  | loss: 0.82289 | val_0_rmse: 0.93786 | val_1_rmse: 0.93579 |  0:00:00s
epoch 2  | loss: 0.61111 | val_0_rmse: 0.82076 | val_1_rmse: 0.82171 |  0:00:00s
epoch 3  | loss: 0.57776 | val_0_rmse: 0.77739 | val_1_rmse: 0.74975 |  0:00:00s
epoch 4  | loss: 0.56268 | val_0_rmse: 0.76029 | val_1_rmse: 0.74326 |  0:00:00s
epoch 5  | loss: 0.5323  | val_0_rmse: 0.76386 | val_1_rmse: 0.75442 |  0:00:00s
epoch 6  | loss: 0.52145 | val_0_rmse: 0.76691 | val_1_rmse: 0.74259 |  0:00:00s
epoch 7  | loss: 0.51333 | val_0_rmse: 0.72922 | val_1_rmse: 0.72605 |  0:00:01s
epoch 8  | loss: 0.4996  | val_0_rmse: 0.71998 | val_1_rmse: 0.7211  |  0:00:01s
epoch 9  | loss: 0.50188 | val_0_rmse: 0.72289 | val_1_rmse: 0.72005 |  0:00:01s
epoch 10 | loss: 0.49372 | val_0_rmse: 0.71166 | val_1_rmse: 0.71665 |  0:00:01s
epoch 11 | loss: 0.50333 | val_0_rmse: 0.70605 | val_1_rmse: 0.72271 |  0:00:01s
epoch 12 | loss: 0.48883 | val_0_rmse: 0.71874 | val_1_rmse: 0.72078 |  0:00:01s
epoch 13 | loss: 0.48669 | val_0_rmse: 0.69661 | val_1_rmse: 0.71739 |  0:00:01s
epoch 14 | loss: 0.49195 | val_0_rmse: 0.68679 | val_1_rmse: 0.71766 |  0:00:02s
epoch 15 | loss: 0.47327 | val_0_rmse: 0.73381 | val_1_rmse: 0.74011 |  0:00:02s
epoch 16 | loss: 0.47726 | val_0_rmse: 0.70429 | val_1_rmse: 0.7237  |  0:00:02s
epoch 17 | loss: 0.46231 | val_0_rmse: 0.67737 | val_1_rmse: 0.71092 |  0:00:02s
epoch 18 | loss: 0.47644 | val_0_rmse: 0.69938 | val_1_rmse: 0.71554 |  0:00:02s
epoch 19 | loss: 0.45889 | val_0_rmse: 0.71675 | val_1_rmse: 0.71338 |  0:00:02s
epoch 20 | loss: 0.47141 | val_0_rmse: 0.68053 | val_1_rmse: 0.69736 |  0:00:02s
epoch 21 | loss: 0.46389 | val_0_rmse: 0.6841  | val_1_rmse: 0.6998  |  0:00:02s
epoch 22 | loss: 0.4559  | val_0_rmse: 0.68838 | val_1_rmse: 0.69801 |  0:00:03s
epoch 23 | loss: 0.46256 | val_0_rmse: 0.67729 | val_1_rmse: 0.70098 |  0:00:03s
epoch 24 | loss: 0.45675 | val_0_rmse: 0.67126 | val_1_rmse: 0.69186 |  0:00:03s
epoch 25 | loss: 0.44647 | val_0_rmse: 0.67238 | val_1_rmse: 0.68081 |  0:00:03s
epoch 26 | loss: 0.44234 | val_0_rmse: 0.65977 | val_1_rmse: 0.68533 |  0:00:03s
epoch 27 | loss: 0.45029 | val_0_rmse: 0.664   | val_1_rmse: 0.68967 |  0:00:03s
epoch 28 | loss: 0.44143 | val_0_rmse: 0.6608  | val_1_rmse: 0.69769 |  0:00:03s
epoch 29 | loss: 0.43989 | val_0_rmse: 0.65531 | val_1_rmse: 0.72456 |  0:00:04s
epoch 30 | loss: 0.43075 | val_0_rmse: 0.6569  | val_1_rmse: 0.72433 |  0:00:04s
epoch 31 | loss: 0.45056 | val_0_rmse: 0.65178 | val_1_rmse: 0.71203 |  0:00:04s
epoch 32 | loss: 0.43567 | val_0_rmse: 0.64924 | val_1_rmse: 0.71463 |  0:00:04s
epoch 33 | loss: 0.44228 | val_0_rmse: 0.65069 | val_1_rmse: 0.70452 |  0:00:04s
epoch 34 | loss: 0.43115 | val_0_rmse: 0.65255 | val_1_rmse: 0.69591 |  0:00:04s
epoch 35 | loss: 0.44096 | val_0_rmse: 0.65214 | val_1_rmse: 0.70773 |  0:00:04s
epoch 36 | loss: 0.42129 | val_0_rmse: 0.65918 | val_1_rmse: 0.71658 |  0:00:05s
epoch 37 | loss: 0.42044 | val_0_rmse: 0.65853 | val_1_rmse: 0.71869 |  0:00:05s
epoch 38 | loss: 0.43478 | val_0_rmse: 0.65279 | val_1_rmse: 0.72641 |  0:00:05s
epoch 39 | loss: 0.43165 | val_0_rmse: 0.6421  | val_1_rmse: 0.70949 |  0:00:05s
epoch 40 | loss: 0.4211  | val_0_rmse: 0.64683 | val_1_rmse: 0.70308 |  0:00:05s
epoch 41 | loss: 0.43089 | val_0_rmse: 0.65207 | val_1_rmse: 0.72262 |  0:00:05s
epoch 42 | loss: 0.44633 | val_0_rmse: 0.64841 | val_1_rmse: 0.7112  |  0:00:05s
epoch 43 | loss: 0.44048 | val_0_rmse: 0.6586  | val_1_rmse: 0.68972 |  0:00:05s
epoch 44 | loss: 0.43069 | val_0_rmse: 0.65429 | val_1_rmse: 0.71858 |  0:00:06s
epoch 45 | loss: 0.44217 | val_0_rmse: 0.65284 | val_1_rmse: 0.72544 |  0:00:06s
epoch 46 | loss: 0.42991 | val_0_rmse: 0.65344 | val_1_rmse: 0.71017 |  0:00:06s
epoch 47 | loss: 0.42957 | val_0_rmse: 0.65225 | val_1_rmse: 0.7075  |  0:00:06s
epoch 48 | loss: 0.4316  | val_0_rmse: 0.66109 | val_1_rmse: 0.73334 |  0:00:06s
epoch 49 | loss: 0.44384 | val_0_rmse: 0.65111 | val_1_rmse: 0.71073 |  0:00:06s
epoch 50 | loss: 0.44505 | val_0_rmse: 0.66786 | val_1_rmse: 0.69387 |  0:00:06s
epoch 51 | loss: 0.445   | val_0_rmse: 0.65103 | val_1_rmse: 0.69387 |  0:00:06s
epoch 52 | loss: 0.4311  | val_0_rmse: 0.66476 | val_1_rmse: 0.71823 |  0:00:07s
epoch 53 | loss: 0.44964 | val_0_rmse: 0.66865 | val_1_rmse: 0.72629 |  0:00:07s
epoch 54 | loss: 0.43593 | val_0_rmse: 0.66951 | val_1_rmse: 0.72301 |  0:00:07s
epoch 55 | loss: 0.44205 | val_0_rmse: 0.65139 | val_1_rmse: 0.71106 |  0:00:07s

Early stopping occured at epoch 55 with best_epoch = 25 and best_val_1_rmse = 0.68081
Best weights from best epoch are automatically used!
ended training at: 07:26:19
Feature importance:
[('Area', 0.3681146509214285), ('Baths', 0.08717126939603384), ('Beds', 0.09411485203370902), ('Latitude', 0.15751691308887447), ('Longitude', 0.19021525553978785), ('Month', 0.09560371197092307), ('Year', 0.007263347049243216)]
Mean squared error is of 3404531791.3099575
Mean absolute error:42039.99696586538
MAPE:0.3089623657987886
R2 score:0.5158663051061574
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:26:19
epoch 0  | loss: 1.57259 | val_0_rmse: 1.05446 | val_1_rmse: 1.14894 |  0:00:00s
epoch 1  | loss: 0.94066 | val_0_rmse: 1.08479 | val_1_rmse: 1.20787 |  0:00:00s
epoch 2  | loss: 0.67963 | val_0_rmse: 0.87559 | val_1_rmse: 1.0912  |  0:00:00s
epoch 3  | loss: 0.57789 | val_0_rmse: 0.98664 | val_1_rmse: 1.24203 |  0:00:00s
epoch 4  | loss: 0.54515 | val_0_rmse: 0.82851 | val_1_rmse: 0.97536 |  0:00:00s
epoch 5  | loss: 0.49317 | val_0_rmse: 0.78598 | val_1_rmse: 0.88666 |  0:00:00s
epoch 6  | loss: 0.47088 | val_0_rmse: 0.84011 | val_1_rmse: 0.95247 |  0:00:00s
epoch 7  | loss: 0.48826 | val_0_rmse: 0.79798 | val_1_rmse: 0.96011 |  0:00:01s
epoch 8  | loss: 0.4564  | val_0_rmse: 0.72655 | val_1_rmse: 0.83839 |  0:00:01s
epoch 9  | loss: 0.48783 | val_0_rmse: 0.71162 | val_1_rmse: 0.82157 |  0:00:01s
epoch 10 | loss: 0.44739 | val_0_rmse: 0.74712 | val_1_rmse: 0.82795 |  0:00:01s
epoch 11 | loss: 0.47581 | val_0_rmse: 0.70387 | val_1_rmse: 0.81446 |  0:00:01s
epoch 12 | loss: 0.45968 | val_0_rmse: 0.69247 | val_1_rmse: 0.81686 |  0:00:01s
epoch 13 | loss: 0.473   | val_0_rmse: 0.67666 | val_1_rmse: 0.80321 |  0:00:01s
epoch 14 | loss: 0.44508 | val_0_rmse: 0.69559 | val_1_rmse: 0.82299 |  0:00:02s
epoch 15 | loss: 0.44031 | val_0_rmse: 0.68642 | val_1_rmse: 0.82188 |  0:00:02s
epoch 16 | loss: 0.4465  | val_0_rmse: 0.65811 | val_1_rmse: 0.80395 |  0:00:02s
epoch 17 | loss: 0.4427  | val_0_rmse: 0.65456 | val_1_rmse: 0.79815 |  0:00:02s
epoch 18 | loss: 0.43181 | val_0_rmse: 0.65691 | val_1_rmse: 0.79467 |  0:00:02s
epoch 19 | loss: 0.43685 | val_0_rmse: 0.65456 | val_1_rmse: 0.79217 |  0:00:02s
epoch 20 | loss: 0.41681 | val_0_rmse: 0.64481 | val_1_rmse: 0.79001 |  0:00:02s
epoch 21 | loss: 0.43604 | val_0_rmse: 0.642   | val_1_rmse: 0.79193 |  0:00:02s
epoch 22 | loss: 0.42635 | val_0_rmse: 0.64119 | val_1_rmse: 0.79467 |  0:00:03s
epoch 23 | loss: 0.42615 | val_0_rmse: 0.63963 | val_1_rmse: 0.79642 |  0:00:03s
epoch 24 | loss: 0.41793 | val_0_rmse: 0.63528 | val_1_rmse: 0.79505 |  0:00:03s
epoch 25 | loss: 0.41501 | val_0_rmse: 0.63461 | val_1_rmse: 0.79764 |  0:00:03s
epoch 26 | loss: 0.41356 | val_0_rmse: 0.6352  | val_1_rmse: 0.79946 |  0:00:03s
epoch 27 | loss: 0.41003 | val_0_rmse: 0.63247 | val_1_rmse: 0.79091 |  0:00:03s
epoch 28 | loss: 0.41346 | val_0_rmse: 0.6334  | val_1_rmse: 0.79714 |  0:00:03s
epoch 29 | loss: 0.41636 | val_0_rmse: 0.63786 | val_1_rmse: 0.80483 |  0:00:04s
epoch 30 | loss: 0.40527 | val_0_rmse: 0.64182 | val_1_rmse: 0.80352 |  0:00:04s
epoch 31 | loss: 0.41151 | val_0_rmse: 0.63744 | val_1_rmse: 0.79538 |  0:00:04s
epoch 32 | loss: 0.4177  | val_0_rmse: 0.63311 | val_1_rmse: 0.78126 |  0:00:04s
epoch 33 | loss: 0.40339 | val_0_rmse: 0.63546 | val_1_rmse: 0.7892  |  0:00:04s
epoch 34 | loss: 0.40491 | val_0_rmse: 0.64167 | val_1_rmse: 0.80733 |  0:00:04s
epoch 35 | loss: 0.40421 | val_0_rmse: 0.63411 | val_1_rmse: 0.80582 |  0:00:04s
epoch 36 | loss: 0.40722 | val_0_rmse: 0.63063 | val_1_rmse: 0.79775 |  0:00:05s
epoch 37 | loss: 0.40517 | val_0_rmse: 0.62941 | val_1_rmse: 0.79383 |  0:00:05s
epoch 38 | loss: 0.40264 | val_0_rmse: 0.63358 | val_1_rmse: 0.79049 |  0:00:05s
epoch 39 | loss: 0.40612 | val_0_rmse: 0.62771 | val_1_rmse: 0.77428 |  0:00:05s
epoch 40 | loss: 0.40622 | val_0_rmse: 0.62187 | val_1_rmse: 0.76484 |  0:00:05s
epoch 41 | loss: 0.39561 | val_0_rmse: 0.62708 | val_1_rmse: 0.77789 |  0:00:05s
epoch 42 | loss: 0.40159 | val_0_rmse: 0.61937 | val_1_rmse: 0.77178 |  0:00:05s
epoch 43 | loss: 0.39811 | val_0_rmse: 0.62146 | val_1_rmse: 0.76333 |  0:00:05s
epoch 44 | loss: 0.39827 | val_0_rmse: 0.61907 | val_1_rmse: 0.76567 |  0:00:06s
epoch 45 | loss: 0.38353 | val_0_rmse: 0.63325 | val_1_rmse: 0.78414 |  0:00:06s
epoch 46 | loss: 0.39757 | val_0_rmse: 0.61538 | val_1_rmse: 0.76554 |  0:00:06s
epoch 47 | loss: 0.38914 | val_0_rmse: 0.6191  | val_1_rmse: 0.77514 |  0:00:06s
epoch 48 | loss: 0.40295 | val_0_rmse: 0.62075 | val_1_rmse: 0.78885 |  0:00:06s
epoch 49 | loss: 0.39385 | val_0_rmse: 0.6365  | val_1_rmse: 0.80184 |  0:00:06s
epoch 50 | loss: 0.39789 | val_0_rmse: 0.6233  | val_1_rmse: 0.78863 |  0:00:06s
epoch 51 | loss: 0.39765 | val_0_rmse: 0.6307  | val_1_rmse: 0.79475 |  0:00:07s
epoch 52 | loss: 0.39882 | val_0_rmse: 0.63868 | val_1_rmse: 0.81707 |  0:00:07s
epoch 53 | loss: 0.39746 | val_0_rmse: 0.64097 | val_1_rmse: 0.80334 |  0:00:07s
epoch 54 | loss: 0.39566 | val_0_rmse: 0.63515 | val_1_rmse: 0.78957 |  0:00:07s
epoch 55 | loss: 0.39504 | val_0_rmse: 0.62936 | val_1_rmse: 0.78047 |  0:00:07s
epoch 56 | loss: 0.40168 | val_0_rmse: 0.63079 | val_1_rmse: 0.79263 |  0:00:07s
epoch 57 | loss: 0.39206 | val_0_rmse: 0.63258 | val_1_rmse: 0.79443 |  0:00:07s
epoch 58 | loss: 0.39371 | val_0_rmse: 0.63334 | val_1_rmse: 0.80308 |  0:00:07s
epoch 59 | loss: 0.39149 | val_0_rmse: 0.61709 | val_1_rmse: 0.78428 |  0:00:08s
epoch 60 | loss: 0.38573 | val_0_rmse: 0.60918 | val_1_rmse: 0.77231 |  0:00:08s
epoch 61 | loss: 0.38506 | val_0_rmse: 0.61055 | val_1_rmse: 0.77378 |  0:00:08s
epoch 62 | loss: 0.38536 | val_0_rmse: 0.61736 | val_1_rmse: 0.76997 |  0:00:08s
epoch 63 | loss: 0.39256 | val_0_rmse: 0.61493 | val_1_rmse: 0.76975 |  0:00:08s
epoch 64 | loss: 0.39194 | val_0_rmse: 0.61124 | val_1_rmse: 0.76153 |  0:00:08s
epoch 65 | loss: 0.3771  | val_0_rmse: 0.6144  | val_1_rmse: 0.75901 |  0:00:08s
epoch 66 | loss: 0.38547 | val_0_rmse: 0.61872 | val_1_rmse: 0.7551  |  0:00:08s
epoch 67 | loss: 0.38886 | val_0_rmse: 0.62683 | val_1_rmse: 0.75499 |  0:00:09s
epoch 68 | loss: 0.38678 | val_0_rmse: 0.61696 | val_1_rmse: 0.75842 |  0:00:09s
epoch 69 | loss: 0.39231 | val_0_rmse: 0.61653 | val_1_rmse: 0.76342 |  0:00:09s
epoch 70 | loss: 0.38106 | val_0_rmse: 0.628   | val_1_rmse: 0.76011 |  0:00:09s
epoch 71 | loss: 0.38537 | val_0_rmse: 0.60941 | val_1_rmse: 0.75758 |  0:00:09s
epoch 72 | loss: 0.39368 | val_0_rmse: 0.60871 | val_1_rmse: 0.76529 |  0:00:09s
epoch 73 | loss: 0.38517 | val_0_rmse: 0.61528 | val_1_rmse: 0.74958 |  0:00:09s
epoch 74 | loss: 0.38903 | val_0_rmse: 0.6057  | val_1_rmse: 0.74986 |  0:00:10s
epoch 75 | loss: 0.37883 | val_0_rmse: 0.60304 | val_1_rmse: 0.77381 |  0:00:10s
epoch 76 | loss: 0.38293 | val_0_rmse: 0.60545 | val_1_rmse: 0.7753  |  0:00:10s
epoch 77 | loss: 0.37462 | val_0_rmse: 0.60069 | val_1_rmse: 0.75839 |  0:00:10s
epoch 78 | loss: 0.365   | val_0_rmse: 0.60082 | val_1_rmse: 0.75451 |  0:00:10s
epoch 79 | loss: 0.36562 | val_0_rmse: 0.60371 | val_1_rmse: 0.77142 |  0:00:10s
epoch 80 | loss: 0.38641 | val_0_rmse: 0.60418 | val_1_rmse: 0.77974 |  0:00:10s
epoch 81 | loss: 0.37453 | val_0_rmse: 0.59567 | val_1_rmse: 0.76709 |  0:00:10s
epoch 82 | loss: 0.36889 | val_0_rmse: 0.62045 | val_1_rmse: 0.7946  |  0:00:11s
epoch 83 | loss: 0.36374 | val_0_rmse: 0.62645 | val_1_rmse: 0.79932 |  0:00:11s
epoch 84 | loss: 0.36656 | val_0_rmse: 0.61585 | val_1_rmse: 0.7914  |  0:00:11s
epoch 85 | loss: 0.3941  | val_0_rmse: 0.6086  | val_1_rmse: 0.76225 |  0:00:11s
epoch 86 | loss: 0.37674 | val_0_rmse: 0.60879 | val_1_rmse: 0.7592  |  0:00:11s
epoch 87 | loss: 0.37943 | val_0_rmse: 0.59477 | val_1_rmse: 0.74139 |  0:00:11s
epoch 88 | loss: 0.37576 | val_0_rmse: 0.60394 | val_1_rmse: 0.7427  |  0:00:11s
epoch 89 | loss: 0.36323 | val_0_rmse: 0.61084 | val_1_rmse: 0.73481 |  0:00:12s
epoch 90 | loss: 0.36655 | val_0_rmse: 0.60506 | val_1_rmse: 0.75521 |  0:00:12s
epoch 91 | loss: 0.35443 | val_0_rmse: 0.61187 | val_1_rmse: 0.77008 |  0:00:12s
epoch 92 | loss: 0.35565 | val_0_rmse: 0.6102  | val_1_rmse: 0.76285 |  0:00:12s
epoch 93 | loss: 0.35373 | val_0_rmse: 0.60203 | val_1_rmse: 0.75439 |  0:00:12s
epoch 94 | loss: 0.34564 | val_0_rmse: 0.59505 | val_1_rmse: 0.74319 |  0:00:12s
epoch 95 | loss: 0.3593  | val_0_rmse: 0.60703 | val_1_rmse: 0.74944 |  0:00:12s
epoch 96 | loss: 0.35869 | val_0_rmse: 0.59302 | val_1_rmse: 0.76125 |  0:00:13s
epoch 97 | loss: 0.36462 | val_0_rmse: 0.60767 | val_1_rmse: 0.7815  |  0:00:13s
epoch 98 | loss: 0.33586 | val_0_rmse: 0.59706 | val_1_rmse: 0.76468 |  0:00:13s
epoch 99 | loss: 0.36838 | val_0_rmse: 0.59283 | val_1_rmse: 0.7508  |  0:00:13s
epoch 100| loss: 0.34049 | val_0_rmse: 0.58151 | val_1_rmse: 0.75415 |  0:00:13s
epoch 101| loss: 0.33059 | val_0_rmse: 0.57858 | val_1_rmse: 0.74775 |  0:00:13s
epoch 102| loss: 0.35976 | val_0_rmse: 0.5729  | val_1_rmse: 0.74007 |  0:00:13s
epoch 103| loss: 0.34739 | val_0_rmse: 0.6055  | val_1_rmse: 0.76229 |  0:00:13s
epoch 104| loss: 0.36387 | val_0_rmse: 0.58955 | val_1_rmse: 0.73356 |  0:00:14s
epoch 105| loss: 0.35205 | val_0_rmse: 0.59812 | val_1_rmse: 0.73688 |  0:00:14s
epoch 106| loss: 0.37537 | val_0_rmse: 0.59352 | val_1_rmse: 0.73817 |  0:00:14s
epoch 107| loss: 0.36372 | val_0_rmse: 0.6034  | val_1_rmse: 0.74884 |  0:00:14s
epoch 108| loss: 0.35295 | val_0_rmse: 0.60065 | val_1_rmse: 0.75243 |  0:00:14s
epoch 109| loss: 0.36515 | val_0_rmse: 0.58989 | val_1_rmse: 0.74114 |  0:00:14s
epoch 110| loss: 0.34944 | val_0_rmse: 0.58179 | val_1_rmse: 0.72982 |  0:00:14s
epoch 111| loss: 0.36182 | val_0_rmse: 0.58184 | val_1_rmse: 0.72841 |  0:00:15s
epoch 112| loss: 0.36472 | val_0_rmse: 0.59016 | val_1_rmse: 0.73138 |  0:00:15s
epoch 113| loss: 0.35258 | val_0_rmse: 0.5706  | val_1_rmse: 0.69002 |  0:00:15s
epoch 114| loss: 0.33349 | val_0_rmse: 0.67205 | val_1_rmse: 0.75935 |  0:00:15s
epoch 115| loss: 0.33865 | val_0_rmse: 0.6096  | val_1_rmse: 0.71502 |  0:00:15s
epoch 116| loss: 0.33776 | val_0_rmse: 0.58536 | val_1_rmse: 0.70082 |  0:00:15s
epoch 117| loss: 0.33351 | val_0_rmse: 0.61484 | val_1_rmse: 0.72276 |  0:00:15s
epoch 118| loss: 0.33806 | val_0_rmse: 0.59239 | val_1_rmse: 0.7023  |  0:00:15s
epoch 119| loss: 0.31502 | val_0_rmse: 0.66764 | val_1_rmse: 0.75588 |  0:00:16s
epoch 120| loss: 0.31358 | val_0_rmse: 0.65617 | val_1_rmse: 0.7488  |  0:00:16s
epoch 121| loss: 0.31851 | val_0_rmse: 0.5594  | val_1_rmse: 0.68553 |  0:00:16s
epoch 122| loss: 0.32093 | val_0_rmse: 0.57836 | val_1_rmse: 0.70987 |  0:00:16s
epoch 123| loss: 0.31236 | val_0_rmse: 0.60082 | val_1_rmse: 0.72785 |  0:00:16s
epoch 124| loss: 0.33146 | val_0_rmse: 0.63877 | val_1_rmse: 0.73799 |  0:00:16s
epoch 125| loss: 0.32249 | val_0_rmse: 0.6948  | val_1_rmse: 0.77768 |  0:00:16s
epoch 126| loss: 0.33376 | val_0_rmse: 0.63068 | val_1_rmse: 0.72665 |  0:00:17s
epoch 127| loss: 0.33596 | val_0_rmse: 0.62766 | val_1_rmse: 0.71718 |  0:00:17s
epoch 128| loss: 0.32333 | val_0_rmse: 0.58032 | val_1_rmse: 0.68366 |  0:00:17s
epoch 129| loss: 0.32842 | val_0_rmse: 0.56653 | val_1_rmse: 0.68582 |  0:00:17s
epoch 130| loss: 0.3355  | val_0_rmse: 0.56233 | val_1_rmse: 0.68828 |  0:00:17s
epoch 131| loss: 0.34307 | val_0_rmse: 0.59247 | val_1_rmse: 0.7087  |  0:00:17s
epoch 132| loss: 0.33334 | val_0_rmse: 0.66646 | val_1_rmse: 0.76771 |  0:00:17s
epoch 133| loss: 0.33517 | val_0_rmse: 0.63014 | val_1_rmse: 0.74387 |  0:00:17s
epoch 134| loss: 0.32593 | val_0_rmse: 0.61442 | val_1_rmse: 0.73663 |  0:00:18s
epoch 135| loss: 0.31054 | val_0_rmse: 0.56372 | val_1_rmse: 0.71145 |  0:00:18s
epoch 136| loss: 0.33041 | val_0_rmse: 0.54658 | val_1_rmse: 0.69367 |  0:00:18s
epoch 137| loss: 0.30195 | val_0_rmse: 0.54806 | val_1_rmse: 0.67591 |  0:00:18s
epoch 138| loss: 0.31449 | val_0_rmse: 0.57549 | val_1_rmse: 0.685   |  0:00:18s
epoch 139| loss: 0.30956 | val_0_rmse: 0.56129 | val_1_rmse: 0.67786 |  0:00:18s
epoch 140| loss: 0.31059 | val_0_rmse: 0.54508 | val_1_rmse: 0.66808 |  0:00:18s
epoch 141| loss: 0.31266 | val_0_rmse: 0.53672 | val_1_rmse: 0.66429 |  0:00:19s
epoch 142| loss: 0.31721 | val_0_rmse: 0.56508 | val_1_rmse: 0.67951 |  0:00:19s
epoch 143| loss: 0.31452 | val_0_rmse: 0.55875 | val_1_rmse: 0.66738 |  0:00:19s
epoch 144| loss: 0.30305 | val_0_rmse: 0.55854 | val_1_rmse: 0.66041 |  0:00:19s
epoch 145| loss: 0.32869 | val_0_rmse: 0.54397 | val_1_rmse: 0.65163 |  0:00:19s
epoch 146| loss: 0.2893  | val_0_rmse: 0.53638 | val_1_rmse: 0.64937 |  0:00:19s
epoch 147| loss: 0.31405 | val_0_rmse: 0.55062 | val_1_rmse: 0.64394 |  0:00:19s
epoch 148| loss: 0.3085  | val_0_rmse: 0.56051 | val_1_rmse: 0.65922 |  0:00:19s
epoch 149| loss: 0.30496 | val_0_rmse: 0.54327 | val_1_rmse: 0.66613 |  0:00:20s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.64394
Best weights from best epoch are automatically used!
ended training at: 07:26:39
Feature importance:
[('Area', 0.34792603961531043), ('Baths', 0.13696192078461306), ('Beds', 0.011949220424400324), ('Latitude', 0.2967083014885548), ('Longitude', 0.12548028501276245), ('Month', 0.05779139313334818), ('Year', 0.023182839541010736)]
Mean squared error is of 2701187509.1334515
Mean absolute error:37320.90025061813
MAPE:0.31265623377164764
R2 score:0.5801262756917737
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:26:39
epoch 0  | loss: 1.14479 | val_0_rmse: 1.19642 | val_1_rmse: 1.34414 |  0:00:00s
epoch 1  | loss: 0.76039 | val_0_rmse: 0.97703 | val_1_rmse: 1.02229 |  0:00:00s
epoch 2  | loss: 0.63301 | val_0_rmse: 0.79852 | val_1_rmse: 0.86952 |  0:00:00s
epoch 3  | loss: 0.60614 | val_0_rmse: 0.83593 | val_1_rmse: 0.8982  |  0:00:01s
epoch 4  | loss: 0.58021 | val_0_rmse: 0.78299 | val_1_rmse: 0.85611 |  0:00:01s
epoch 5  | loss: 0.56329 | val_0_rmse: 0.75752 | val_1_rmse: 0.81916 |  0:00:01s
epoch 6  | loss: 0.5498  | val_0_rmse: 0.7356  | val_1_rmse: 0.79447 |  0:00:02s
epoch 7  | loss: 0.53979 | val_0_rmse: 0.73143 | val_1_rmse: 0.79578 |  0:00:02s
epoch 8  | loss: 0.52907 | val_0_rmse: 0.73228 | val_1_rmse: 0.80518 |  0:00:02s
epoch 9  | loss: 0.52533 | val_0_rmse: 0.74175 | val_1_rmse: 0.82584 |  0:00:02s
epoch 10 | loss: 0.52047 | val_0_rmse: 0.73113 | val_1_rmse: 0.83085 |  0:00:03s
epoch 11 | loss: 0.52271 | val_0_rmse: 0.71671 | val_1_rmse: 0.81281 |  0:00:03s
epoch 12 | loss: 0.51818 | val_0_rmse: 0.71288 | val_1_rmse: 0.78811 |  0:00:03s
epoch 13 | loss: 0.50883 | val_0_rmse: 0.70969 | val_1_rmse: 0.79134 |  0:00:04s
epoch 14 | loss: 0.5172  | val_0_rmse: 0.71688 | val_1_rmse: 0.79314 |  0:00:04s
epoch 15 | loss: 0.5181  | val_0_rmse: 0.71512 | val_1_rmse: 0.78528 |  0:00:04s
epoch 16 | loss: 0.50938 | val_0_rmse: 0.72783 | val_1_rmse: 0.79876 |  0:00:05s
epoch 17 | loss: 0.51516 | val_0_rmse: 0.71839 | val_1_rmse: 0.80437 |  0:00:05s
epoch 18 | loss: 0.50801 | val_0_rmse: 0.71532 | val_1_rmse: 0.78878 |  0:00:05s
epoch 19 | loss: 0.50672 | val_0_rmse: 0.75046 | val_1_rmse: 0.81876 |  0:00:05s
epoch 20 | loss: 0.49319 | val_0_rmse: 0.72989 | val_1_rmse: 0.80985 |  0:00:06s
epoch 21 | loss: 0.49942 | val_0_rmse: 0.71867 | val_1_rmse: 0.80376 |  0:00:06s
epoch 22 | loss: 0.49716 | val_0_rmse: 0.70637 | val_1_rmse: 0.77982 |  0:00:06s
epoch 23 | loss: 0.48029 | val_0_rmse: 0.70509 | val_1_rmse: 0.80352 |  0:00:07s
epoch 24 | loss: 0.49221 | val_0_rmse: 0.69949 | val_1_rmse: 0.80052 |  0:00:07s
epoch 25 | loss: 0.48494 | val_0_rmse: 0.70894 | val_1_rmse: 0.80218 |  0:00:07s
epoch 26 | loss: 0.48748 | val_0_rmse: 0.7145  | val_1_rmse: 0.80169 |  0:00:07s
epoch 27 | loss: 0.48539 | val_0_rmse: 0.71904 | val_1_rmse: 0.80677 |  0:00:08s
epoch 28 | loss: 0.47483 | val_0_rmse: 0.69484 | val_1_rmse: 0.78781 |  0:00:08s
epoch 29 | loss: 0.46812 | val_0_rmse: 0.68752 | val_1_rmse: 0.78092 |  0:00:08s
epoch 30 | loss: 0.47431 | val_0_rmse: 0.68623 | val_1_rmse: 0.78792 |  0:00:09s
epoch 31 | loss: 0.46576 | val_0_rmse: 0.69448 | val_1_rmse: 0.78768 |  0:00:09s
epoch 32 | loss: 0.47198 | val_0_rmse: 0.6924  | val_1_rmse: 0.78416 |  0:00:09s
epoch 33 | loss: 0.48577 | val_0_rmse: 0.71204 | val_1_rmse: 0.8033  |  0:00:10s
epoch 34 | loss: 0.48524 | val_0_rmse: 0.6978  | val_1_rmse: 0.78466 |  0:00:10s
epoch 35 | loss: 0.49174 | val_0_rmse: 0.70781 | val_1_rmse: 0.78817 |  0:00:10s
epoch 36 | loss: 0.48704 | val_0_rmse: 0.70242 | val_1_rmse: 0.78541 |  0:00:10s
epoch 37 | loss: 0.48578 | val_0_rmse: 0.72691 | val_1_rmse: 0.80805 |  0:00:11s
epoch 38 | loss: 0.47779 | val_0_rmse: 0.70921 | val_1_rmse: 0.79174 |  0:00:11s
epoch 39 | loss: 0.48886 | val_0_rmse: 0.703   | val_1_rmse: 0.78832 |  0:00:11s
epoch 40 | loss: 0.47952 | val_0_rmse: 0.71414 | val_1_rmse: 0.79912 |  0:00:12s
epoch 41 | loss: 0.48368 | val_0_rmse: 0.70231 | val_1_rmse: 0.79395 |  0:00:12s
epoch 42 | loss: 0.4974  | val_0_rmse: 0.70628 | val_1_rmse: 0.80464 |  0:00:12s
epoch 43 | loss: 0.49325 | val_0_rmse: 0.71478 | val_1_rmse: 0.80318 |  0:00:12s
epoch 44 | loss: 0.47979 | val_0_rmse: 0.71221 | val_1_rmse: 0.79908 |  0:00:13s
epoch 45 | loss: 0.4797  | val_0_rmse: 0.70614 | val_1_rmse: 0.79791 |  0:00:13s
epoch 46 | loss: 0.47674 | val_0_rmse: 0.6997  | val_1_rmse: 0.78911 |  0:00:13s
epoch 47 | loss: 0.48143 | val_0_rmse: 0.70456 | val_1_rmse: 0.79548 |  0:00:14s
epoch 48 | loss: 0.46759 | val_0_rmse: 0.72114 | val_1_rmse: 0.81425 |  0:00:14s
epoch 49 | loss: 0.48384 | val_0_rmse: 0.71502 | val_1_rmse: 0.7969  |  0:00:14s
epoch 50 | loss: 0.46404 | val_0_rmse: 0.70312 | val_1_rmse: 0.79273 |  0:00:15s
epoch 51 | loss: 0.46217 | val_0_rmse: 0.71588 | val_1_rmse: 0.80662 |  0:00:15s
epoch 52 | loss: 0.46679 | val_0_rmse: 0.7111  | val_1_rmse: 0.8041  |  0:00:15s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.77982
Best weights from best epoch are automatically used!
ended training at: 07:26:55
Feature importance:
[('Area', 0.3411547401479817), ('Baths', 0.34648069599786446), ('Beds', 0.01941245168433416), ('Latitude', 0.05448839230838177), ('Longitude', 0.1671537268600678), ('Month', 0.031079393222465595), ('Year', 0.04023059977890455)]
Mean squared error is of 3362305774.510687
Mean absolute error:39293.905378766976
MAPE:0.32991929926134295
R2 score:0.5908817798262319
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:26:55
epoch 0  | loss: 1.32517 | val_0_rmse: 1.10804 | val_1_rmse: 1.00878 |  0:00:00s
epoch 1  | loss: 0.73797 | val_0_rmse: 0.89169 | val_1_rmse: 0.85581 |  0:00:00s
epoch 2  | loss: 0.67086 | val_0_rmse: 0.95544 | val_1_rmse: 0.96029 |  0:00:00s
epoch 3  | loss: 0.62031 | val_0_rmse: 0.80201 | val_1_rmse: 0.80248 |  0:00:01s
epoch 4  | loss: 0.58104 | val_0_rmse: 0.78959 | val_1_rmse: 0.74995 |  0:00:01s
epoch 5  | loss: 0.58019 | val_0_rmse: 0.75383 | val_1_rmse: 0.75306 |  0:00:01s
epoch 6  | loss: 0.56023 | val_0_rmse: 0.72366 | val_1_rmse: 0.72585 |  0:00:02s
epoch 7  | loss: 0.53233 | val_0_rmse: 0.70452 | val_1_rmse: 0.69762 |  0:00:02s
epoch 8  | loss: 0.52543 | val_0_rmse: 0.70435 | val_1_rmse: 0.70488 |  0:00:02s
epoch 9  | loss: 0.50375 | val_0_rmse: 0.72067 | val_1_rmse: 0.72027 |  0:00:02s
epoch 10 | loss: 0.51242 | val_0_rmse: 0.69443 | val_1_rmse: 0.67963 |  0:00:03s
epoch 11 | loss: 0.48807 | val_0_rmse: 0.70331 | val_1_rmse: 0.68704 |  0:00:03s
epoch 12 | loss: 0.49601 | val_0_rmse: 0.72581 | val_1_rmse: 0.70318 |  0:00:03s
epoch 13 | loss: 0.48425 | val_0_rmse: 0.70862 | val_1_rmse: 0.68515 |  0:00:04s
epoch 14 | loss: 0.47608 | val_0_rmse: 0.71652 | val_1_rmse: 0.69804 |  0:00:04s
epoch 15 | loss: 0.48438 | val_0_rmse: 0.69158 | val_1_rmse: 0.67311 |  0:00:04s
epoch 16 | loss: 0.4761  | val_0_rmse: 0.7104  | val_1_rmse: 0.69637 |  0:00:05s
epoch 17 | loss: 0.48557 | val_0_rmse: 0.69954 | val_1_rmse: 0.6919  |  0:00:05s
epoch 18 | loss: 0.5045  | val_0_rmse: 0.72404 | val_1_rmse: 0.71653 |  0:00:05s
epoch 19 | loss: 0.49623 | val_0_rmse: 0.69986 | val_1_rmse: 0.69115 |  0:00:05s
epoch 20 | loss: 0.47689 | val_0_rmse: 0.68727 | val_1_rmse: 0.67994 |  0:00:06s
epoch 21 | loss: 0.47377 | val_0_rmse: 0.69149 | val_1_rmse: 0.68051 |  0:00:06s
epoch 22 | loss: 0.46565 | val_0_rmse: 0.67562 | val_1_rmse: 0.66822 |  0:00:06s
epoch 23 | loss: 0.46778 | val_0_rmse: 0.69208 | val_1_rmse: 0.68928 |  0:00:07s
epoch 24 | loss: 0.45971 | val_0_rmse: 0.69593 | val_1_rmse: 0.69811 |  0:00:07s
epoch 25 | loss: 0.46424 | val_0_rmse: 0.73144 | val_1_rmse: 0.72329 |  0:00:07s
epoch 26 | loss: 0.47043 | val_0_rmse: 0.71764 | val_1_rmse: 0.70446 |  0:00:07s
epoch 27 | loss: 0.47003 | val_0_rmse: 0.69591 | val_1_rmse: 0.68806 |  0:00:08s
epoch 28 | loss: 0.47497 | val_0_rmse: 0.71417 | val_1_rmse: 0.70709 |  0:00:08s
epoch 29 | loss: 0.46986 | val_0_rmse: 0.67991 | val_1_rmse: 0.66684 |  0:00:08s
epoch 30 | loss: 0.47172 | val_0_rmse: 0.67764 | val_1_rmse: 0.67354 |  0:00:09s
epoch 31 | loss: 0.46635 | val_0_rmse: 0.68099 | val_1_rmse: 0.6797  |  0:00:09s
epoch 32 | loss: 0.46133 | val_0_rmse: 0.70858 | val_1_rmse: 0.70175 |  0:00:09s
epoch 33 | loss: 0.46728 | val_0_rmse: 0.68715 | val_1_rmse: 0.67142 |  0:00:10s
epoch 34 | loss: 0.4755  | val_0_rmse: 0.69727 | val_1_rmse: 0.67779 |  0:00:10s
epoch 35 | loss: 0.45606 | val_0_rmse: 0.70514 | val_1_rmse: 0.69457 |  0:00:10s
epoch 36 | loss: 0.45575 | val_0_rmse: 0.71692 | val_1_rmse: 0.70409 |  0:00:10s
epoch 37 | loss: 0.47246 | val_0_rmse: 0.7101  | val_1_rmse: 0.70132 |  0:00:11s
epoch 38 | loss: 0.46716 | val_0_rmse: 0.68687 | val_1_rmse: 0.67725 |  0:00:11s
epoch 39 | loss: 0.45561 | val_0_rmse: 0.68399 | val_1_rmse: 0.67979 |  0:00:11s
epoch 40 | loss: 0.45174 | val_0_rmse: 0.68833 | val_1_rmse: 0.68061 |  0:00:12s
epoch 41 | loss: 0.4565  | val_0_rmse: 0.68128 | val_1_rmse: 0.66789 |  0:00:12s
epoch 42 | loss: 0.4559  | val_0_rmse: 0.68541 | val_1_rmse: 0.67749 |  0:00:12s
epoch 43 | loss: 0.45042 | val_0_rmse: 0.67083 | val_1_rmse: 0.66129 |  0:00:12s
epoch 44 | loss: 0.45598 | val_0_rmse: 0.67975 | val_1_rmse: 0.67147 |  0:00:13s
epoch 45 | loss: 0.44966 | val_0_rmse: 0.67696 | val_1_rmse: 0.67355 |  0:00:13s
epoch 46 | loss: 0.45682 | val_0_rmse: 0.67594 | val_1_rmse: 0.67047 |  0:00:13s
epoch 47 | loss: 0.44874 | val_0_rmse: 0.66748 | val_1_rmse: 0.66386 |  0:00:14s
epoch 48 | loss: 0.4374  | val_0_rmse: 0.66856 | val_1_rmse: 0.66569 |  0:00:14s
epoch 49 | loss: 0.4429  | val_0_rmse: 0.66926 | val_1_rmse: 0.66251 |  0:00:14s
epoch 50 | loss: 0.44218 | val_0_rmse: 0.67251 | val_1_rmse: 0.6669  |  0:00:14s
epoch 51 | loss: 0.44026 | val_0_rmse: 0.66564 | val_1_rmse: 0.66389 |  0:00:15s
epoch 52 | loss: 0.44366 | val_0_rmse: 0.66957 | val_1_rmse: 0.66388 |  0:00:15s
epoch 53 | loss: 0.43575 | val_0_rmse: 0.6655  | val_1_rmse: 0.65919 |  0:00:15s
epoch 54 | loss: 0.44087 | val_0_rmse: 0.66513 | val_1_rmse: 0.66034 |  0:00:16s
epoch 55 | loss: 0.44922 | val_0_rmse: 0.66626 | val_1_rmse: 0.65793 |  0:00:16s
epoch 56 | loss: 0.4442  | val_0_rmse: 0.6646  | val_1_rmse: 0.65865 |  0:00:16s
epoch 57 | loss: 0.43252 | val_0_rmse: 0.66142 | val_1_rmse: 0.65963 |  0:00:17s
epoch 58 | loss: 0.44214 | val_0_rmse: 0.66403 | val_1_rmse: 0.66007 |  0:00:17s
epoch 59 | loss: 0.44014 | val_0_rmse: 0.67187 | val_1_rmse: 0.66323 |  0:00:17s
epoch 60 | loss: 0.44172 | val_0_rmse: 0.65618 | val_1_rmse: 0.65099 |  0:00:17s
epoch 61 | loss: 0.44399 | val_0_rmse: 0.66049 | val_1_rmse: 0.65615 |  0:00:18s
epoch 62 | loss: 0.43725 | val_0_rmse: 0.6554  | val_1_rmse: 0.65136 |  0:00:18s
epoch 63 | loss: 0.433   | val_0_rmse: 0.6692  | val_1_rmse: 0.66472 |  0:00:18s
epoch 64 | loss: 0.43522 | val_0_rmse: 0.67244 | val_1_rmse: 0.66891 |  0:00:19s
epoch 65 | loss: 0.43485 | val_0_rmse: 0.6746  | val_1_rmse: 0.67042 |  0:00:19s
epoch 66 | loss: 0.43402 | val_0_rmse: 0.6713  | val_1_rmse: 0.66893 |  0:00:19s
epoch 67 | loss: 0.43149 | val_0_rmse: 0.67269 | val_1_rmse: 0.67247 |  0:00:19s
epoch 68 | loss: 0.43229 | val_0_rmse: 0.66193 | val_1_rmse: 0.65467 |  0:00:20s
epoch 69 | loss: 0.4297  | val_0_rmse: 0.66062 | val_1_rmse: 0.65284 |  0:00:20s
epoch 70 | loss: 0.42784 | val_0_rmse: 0.67612 | val_1_rmse: 0.67797 |  0:00:20s
epoch 71 | loss: 0.43023 | val_0_rmse: 0.70505 | val_1_rmse: 0.70206 |  0:00:21s
epoch 72 | loss: 0.44282 | val_0_rmse: 0.67958 | val_1_rmse: 0.68172 |  0:00:21s
epoch 73 | loss: 0.44026 | val_0_rmse: 0.67252 | val_1_rmse: 0.66885 |  0:00:21s
epoch 74 | loss: 0.44906 | val_0_rmse: 0.69055 | val_1_rmse: 0.68361 |  0:00:22s
epoch 75 | loss: 0.45956 | val_0_rmse: 0.68932 | val_1_rmse: 0.68923 |  0:00:22s
epoch 76 | loss: 0.45546 | val_0_rmse: 0.68309 | val_1_rmse: 0.67883 |  0:00:22s
epoch 77 | loss: 0.45279 | val_0_rmse: 0.72003 | val_1_rmse: 0.7199  |  0:00:22s
epoch 78 | loss: 0.44351 | val_0_rmse: 0.67871 | val_1_rmse: 0.673   |  0:00:23s
epoch 79 | loss: 0.4465  | val_0_rmse: 0.66723 | val_1_rmse: 0.66164 |  0:00:23s
epoch 80 | loss: 0.44681 | val_0_rmse: 0.6623  | val_1_rmse: 0.65584 |  0:00:23s
epoch 81 | loss: 0.43989 | val_0_rmse: 0.68042 | val_1_rmse: 0.67134 |  0:00:24s
epoch 82 | loss: 0.44931 | val_0_rmse: 0.6624  | val_1_rmse: 0.65473 |  0:00:24s
epoch 83 | loss: 0.45055 | val_0_rmse: 0.67012 | val_1_rmse: 0.66544 |  0:00:24s
epoch 84 | loss: 0.45221 | val_0_rmse: 0.70566 | val_1_rmse: 0.70625 |  0:00:24s
epoch 85 | loss: 0.43999 | val_0_rmse: 0.68118 | val_1_rmse: 0.68027 |  0:00:25s
epoch 86 | loss: 0.45538 | val_0_rmse: 0.69829 | val_1_rmse: 0.69553 |  0:00:25s
epoch 87 | loss: 0.44264 | val_0_rmse: 0.67233 | val_1_rmse: 0.66811 |  0:00:25s
epoch 88 | loss: 0.43069 | val_0_rmse: 0.68151 | val_1_rmse: 0.67996 |  0:00:26s
epoch 89 | loss: 0.43423 | val_0_rmse: 0.68515 | val_1_rmse: 0.68001 |  0:00:26s
epoch 90 | loss: 0.4333  | val_0_rmse: 0.68059 | val_1_rmse: 0.67026 |  0:00:26s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.65099
Best weights from best epoch are automatically used!
ended training at: 07:27:22
Feature importance:
[('Area', 0.2847510953930402), ('Baths', 0.03135707784982771), ('Beds', 0.05172442442721233), ('Latitude', 0.3067763576443962), ('Longitude', 0.2626135166257537), ('Month', 0.037712990314080735), ('Year', 0.025064537745689053)]
Mean squared error is of 3543855931.1601233
Mean absolute error:39830.03991068018
MAPE:0.3407964017756471
R2 score:0.5849983912098342
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:27:22
epoch 0  | loss: 1.36473 | val_0_rmse: 1.17322 | val_1_rmse: 0.98842 |  0:00:00s
epoch 1  | loss: 0.73031 | val_0_rmse: 0.84236 | val_1_rmse: 0.81752 |  0:00:00s
epoch 2  | loss: 0.68206 | val_0_rmse: 0.81435 | val_1_rmse: 0.8152  |  0:00:00s
epoch 3  | loss: 0.63488 | val_0_rmse: 0.81494 | val_1_rmse: 0.8127  |  0:00:01s
epoch 4  | loss: 0.61297 | val_0_rmse: 0.82261 | val_1_rmse: 0.7918  |  0:00:01s
epoch 5  | loss: 0.61135 | val_0_rmse: 0.78028 | val_1_rmse: 0.7481  |  0:00:01s
epoch 6  | loss: 0.58069 | val_0_rmse: 0.75101 | val_1_rmse: 0.74654 |  0:00:02s
epoch 7  | loss: 0.57064 | val_0_rmse: 0.74652 | val_1_rmse: 0.75428 |  0:00:02s
epoch 8  | loss: 0.54816 | val_0_rmse: 0.73114 | val_1_rmse: 0.74303 |  0:00:02s
epoch 9  | loss: 0.53965 | val_0_rmse: 0.71898 | val_1_rmse: 0.73884 |  0:00:03s
epoch 10 | loss: 0.52933 | val_0_rmse: 0.72862 | val_1_rmse: 0.71588 |  0:00:03s
epoch 11 | loss: 0.50939 | val_0_rmse: 0.72025 | val_1_rmse: 0.71085 |  0:00:03s
epoch 12 | loss: 0.5086  | val_0_rmse: 0.75216 | val_1_rmse: 0.7232  |  0:00:03s
epoch 13 | loss: 0.49212 | val_0_rmse: 0.7366  | val_1_rmse: 0.6957  |  0:00:04s
epoch 14 | loss: 0.49131 | val_0_rmse: 0.7341  | val_1_rmse: 0.73665 |  0:00:04s
epoch 15 | loss: 0.49949 | val_0_rmse: 0.71206 | val_1_rmse: 0.71489 |  0:00:04s
epoch 16 | loss: 0.50654 | val_0_rmse: 0.71823 | val_1_rmse: 0.70854 |  0:00:05s
epoch 17 | loss: 0.50229 | val_0_rmse: 0.72274 | val_1_rmse: 0.70558 |  0:00:05s
epoch 18 | loss: 0.50823 | val_0_rmse: 0.72245 | val_1_rmse: 0.70667 |  0:00:05s
epoch 19 | loss: 0.48967 | val_0_rmse: 0.72891 | val_1_rmse: 0.70084 |  0:00:05s
epoch 20 | loss: 0.48537 | val_0_rmse: 0.7177  | val_1_rmse: 0.69215 |  0:00:06s
epoch 21 | loss: 0.48527 | val_0_rmse: 0.69399 | val_1_rmse: 0.68315 |  0:00:06s
epoch 22 | loss: 0.47588 | val_0_rmse: 0.69156 | val_1_rmse: 0.68952 |  0:00:06s
epoch 23 | loss: 0.47105 | val_0_rmse: 0.69121 | val_1_rmse: 0.70473 |  0:00:07s
epoch 24 | loss: 0.4734  | val_0_rmse: 0.6915  | val_1_rmse: 0.70641 |  0:00:07s
epoch 25 | loss: 0.48558 | val_0_rmse: 0.68748 | val_1_rmse: 0.7104  |  0:00:07s
epoch 26 | loss: 0.47768 | val_0_rmse: 0.6852  | val_1_rmse: 0.68956 |  0:00:08s
epoch 27 | loss: 0.5042  | val_0_rmse: 0.69846 | val_1_rmse: 0.69184 |  0:00:08s
epoch 28 | loss: 0.47796 | val_0_rmse: 0.69554 | val_1_rmse: 0.69566 |  0:00:08s
epoch 29 | loss: 0.47806 | val_0_rmse: 0.68288 | val_1_rmse: 0.69843 |  0:00:08s
epoch 30 | loss: 0.47559 | val_0_rmse: 0.69305 | val_1_rmse: 0.69845 |  0:00:09s
epoch 31 | loss: 0.48996 | val_0_rmse: 0.69644 | val_1_rmse: 0.68696 |  0:00:09s
epoch 32 | loss: 0.4855  | val_0_rmse: 0.70512 | val_1_rmse: 0.69638 |  0:00:09s
epoch 33 | loss: 0.49573 | val_0_rmse: 0.71298 | val_1_rmse: 0.71427 |  0:00:10s
epoch 34 | loss: 0.47886 | val_0_rmse: 0.71089 | val_1_rmse: 0.70989 |  0:00:10s
epoch 35 | loss: 0.48052 | val_0_rmse: 0.69969 | val_1_rmse: 0.69484 |  0:00:10s
epoch 36 | loss: 0.48062 | val_0_rmse: 0.70902 | val_1_rmse: 0.70665 |  0:00:10s
epoch 37 | loss: 0.47387 | val_0_rmse: 0.70032 | val_1_rmse: 0.69566 |  0:00:11s
epoch 38 | loss: 0.47559 | val_0_rmse: 0.70986 | val_1_rmse: 0.70921 |  0:00:11s
epoch 39 | loss: 0.47371 | val_0_rmse: 0.69314 | val_1_rmse: 0.69282 |  0:00:11s
epoch 40 | loss: 0.46736 | val_0_rmse: 0.71088 | val_1_rmse: 0.71066 |  0:00:12s
epoch 41 | loss: 0.4591  | val_0_rmse: 0.69935 | val_1_rmse: 0.7012  |  0:00:12s
epoch 42 | loss: 0.4592  | val_0_rmse: 0.70975 | val_1_rmse: 0.7167  |  0:00:12s
epoch 43 | loss: 0.45745 | val_0_rmse: 0.69568 | val_1_rmse: 0.69363 |  0:00:12s
epoch 44 | loss: 0.45788 | val_0_rmse: 0.69041 | val_1_rmse: 0.69142 |  0:00:13s
epoch 45 | loss: 0.45323 | val_0_rmse: 0.68759 | val_1_rmse: 0.6817  |  0:00:13s
epoch 46 | loss: 0.4493  | val_0_rmse: 0.69804 | val_1_rmse: 0.69692 |  0:00:13s
epoch 47 | loss: 0.4475  | val_0_rmse: 0.70357 | val_1_rmse: 0.69111 |  0:00:14s
epoch 48 | loss: 0.46154 | val_0_rmse: 0.68634 | val_1_rmse: 0.67778 |  0:00:14s
epoch 49 | loss: 0.4454  | val_0_rmse: 0.69258 | val_1_rmse: 0.68969 |  0:00:14s
epoch 50 | loss: 0.4524  | val_0_rmse: 0.69521 | val_1_rmse: 0.6935  |  0:00:15s
epoch 51 | loss: 0.46406 | val_0_rmse: 0.71104 | val_1_rmse: 0.70854 |  0:00:15s
epoch 52 | loss: 0.46023 | val_0_rmse: 0.69527 | val_1_rmse: 0.68701 |  0:00:15s
epoch 53 | loss: 0.45336 | val_0_rmse: 0.71162 | val_1_rmse: 0.70734 |  0:00:15s
epoch 54 | loss: 0.46329 | val_0_rmse: 0.69528 | val_1_rmse: 0.68825 |  0:00:16s
epoch 55 | loss: 0.45945 | val_0_rmse: 0.72524 | val_1_rmse: 0.72474 |  0:00:16s
epoch 56 | loss: 0.46501 | val_0_rmse: 0.69579 | val_1_rmse: 0.68998 |  0:00:16s
epoch 57 | loss: 0.46466 | val_0_rmse: 0.70571 | val_1_rmse: 0.69863 |  0:00:17s
epoch 58 | loss: 0.45938 | val_0_rmse: 0.70238 | val_1_rmse: 0.69464 |  0:00:17s
epoch 59 | loss: 0.46287 | val_0_rmse: 0.71137 | val_1_rmse: 0.70344 |  0:00:17s
epoch 60 | loss: 0.46395 | val_0_rmse: 0.74075 | val_1_rmse: 0.7294  |  0:00:17s
epoch 61 | loss: 0.46992 | val_0_rmse: 0.71401 | val_1_rmse: 0.70644 |  0:00:18s
epoch 62 | loss: 0.45344 | val_0_rmse: 0.71441 | val_1_rmse: 0.71075 |  0:00:18s
epoch 63 | loss: 0.45258 | val_0_rmse: 0.70851 | val_1_rmse: 0.69818 |  0:00:18s
epoch 64 | loss: 0.45222 | val_0_rmse: 0.7092  | val_1_rmse: 0.7015  |  0:00:19s
epoch 65 | loss: 0.44953 | val_0_rmse: 0.72878 | val_1_rmse: 0.72459 |  0:00:19s
epoch 66 | loss: 0.44988 | val_0_rmse: 0.70622 | val_1_rmse: 0.69816 |  0:00:19s
epoch 67 | loss: 0.45267 | val_0_rmse: 0.71071 | val_1_rmse: 0.7014  |  0:00:20s
epoch 68 | loss: 0.45319 | val_0_rmse: 0.70645 | val_1_rmse: 0.68945 |  0:00:20s
epoch 69 | loss: 0.45825 | val_0_rmse: 0.73341 | val_1_rmse: 0.72075 |  0:00:20s
epoch 70 | loss: 0.44596 | val_0_rmse: 0.70472 | val_1_rmse: 0.68732 |  0:00:20s
epoch 71 | loss: 0.44824 | val_0_rmse: 0.75141 | val_1_rmse: 0.74615 |  0:00:21s
epoch 72 | loss: 0.45684 | val_0_rmse: 0.71054 | val_1_rmse: 0.69405 |  0:00:21s
epoch 73 | loss: 0.44473 | val_0_rmse: 0.73739 | val_1_rmse: 0.73878 |  0:00:21s
epoch 74 | loss: 0.44983 | val_0_rmse: 0.7003  | val_1_rmse: 0.68649 |  0:00:22s
epoch 75 | loss: 0.44052 | val_0_rmse: 0.73485 | val_1_rmse: 0.72939 |  0:00:22s
epoch 76 | loss: 0.43597 | val_0_rmse: 0.7073  | val_1_rmse: 0.69821 |  0:00:22s
epoch 77 | loss: 0.43158 | val_0_rmse: 0.73291 | val_1_rmse: 0.72639 |  0:00:22s
epoch 78 | loss: 0.43645 | val_0_rmse: 0.72039 | val_1_rmse: 0.70968 |  0:00:23s

Early stopping occured at epoch 78 with best_epoch = 48 and best_val_1_rmse = 0.67778
Best weights from best epoch are automatically used!
ended training at: 07:27:45
Feature importance:
[('Area', 0.3669233396733669), ('Baths', 0.04153425933479601), ('Beds', 0.0), ('Latitude', 0.2080683063244296), ('Longitude', 0.2159354886760729), ('Month', 0.017479729480898886), ('Year', 0.1500588765104357)]
Mean squared error is of 3665908543.8532796
Mean absolute error:41489.72993516553
MAPE:0.34843865867568086
R2 score:0.5640548763787946
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:27:45
epoch 0  | loss: 1.11107 | val_0_rmse: 1.02141 | val_1_rmse: 0.99583 |  0:00:00s
epoch 1  | loss: 0.7182  | val_0_rmse: 0.97584 | val_1_rmse: 0.97806 |  0:00:00s
epoch 2  | loss: 0.59804 | val_0_rmse: 0.82511 | val_1_rmse: 0.85075 |  0:00:00s
epoch 3  | loss: 0.55078 | val_0_rmse: 0.75018 | val_1_rmse: 0.79944 |  0:00:01s
epoch 4  | loss: 0.55724 | val_0_rmse: 0.77809 | val_1_rmse: 0.83574 |  0:00:01s
epoch 5  | loss: 0.54663 | val_0_rmse: 0.7347  | val_1_rmse: 0.79854 |  0:00:01s
epoch 6  | loss: 0.54005 | val_0_rmse: 0.73868 | val_1_rmse: 0.78462 |  0:00:02s
epoch 7  | loss: 0.54134 | val_0_rmse: 0.73427 | val_1_rmse: 0.7987  |  0:00:02s
epoch 8  | loss: 0.52784 | val_0_rmse: 0.74787 | val_1_rmse: 0.7965  |  0:00:02s
epoch 9  | loss: 0.52638 | val_0_rmse: 0.71585 | val_1_rmse: 0.77265 |  0:00:02s
epoch 10 | loss: 0.51574 | val_0_rmse: 0.73408 | val_1_rmse: 0.77637 |  0:00:03s
epoch 11 | loss: 0.51289 | val_0_rmse: 0.7247  | val_1_rmse: 0.77393 |  0:00:03s
epoch 12 | loss: 0.52211 | val_0_rmse: 0.73784 | val_1_rmse: 0.78276 |  0:00:03s
epoch 13 | loss: 0.51788 | val_0_rmse: 0.7131  | val_1_rmse: 0.76864 |  0:00:04s
epoch 14 | loss: 0.50808 | val_0_rmse: 0.71146 | val_1_rmse: 0.7576  |  0:00:04s
epoch 15 | loss: 0.50317 | val_0_rmse: 0.70554 | val_1_rmse: 0.76873 |  0:00:04s
epoch 16 | loss: 0.50316 | val_0_rmse: 0.69742 | val_1_rmse: 0.75633 |  0:00:05s
epoch 17 | loss: 0.49271 | val_0_rmse: 0.69418 | val_1_rmse: 0.75241 |  0:00:05s
epoch 18 | loss: 0.48513 | val_0_rmse: 0.74388 | val_1_rmse: 0.79888 |  0:00:05s
epoch 19 | loss: 0.49407 | val_0_rmse: 0.77498 | val_1_rmse: 0.78466 |  0:00:05s
epoch 20 | loss: 0.48562 | val_0_rmse: 0.76566 | val_1_rmse: 0.79189 |  0:00:06s
epoch 21 | loss: 0.49011 | val_0_rmse: 0.71453 | val_1_rmse: 0.76175 |  0:00:06s
epoch 22 | loss: 0.49339 | val_0_rmse: 0.72984 | val_1_rmse: 0.76657 |  0:00:06s
epoch 23 | loss: 0.48972 | val_0_rmse: 0.71896 | val_1_rmse: 0.75713 |  0:00:07s
epoch 24 | loss: 0.49148 | val_0_rmse: 0.73683 | val_1_rmse: 0.77854 |  0:00:07s
epoch 25 | loss: 0.48118 | val_0_rmse: 0.71283 | val_1_rmse: 0.75764 |  0:00:07s
epoch 26 | loss: 0.47811 | val_0_rmse: 0.72024 | val_1_rmse: 0.76346 |  0:00:07s
epoch 27 | loss: 0.49221 | val_0_rmse: 0.6996  | val_1_rmse: 0.75052 |  0:00:08s
epoch 28 | loss: 0.49628 | val_0_rmse: 0.71662 | val_1_rmse: 0.76026 |  0:00:08s
epoch 29 | loss: 0.49544 | val_0_rmse: 0.70627 | val_1_rmse: 0.75748 |  0:00:08s
epoch 30 | loss: 0.49467 | val_0_rmse: 0.70459 | val_1_rmse: 0.7495  |  0:00:09s
epoch 31 | loss: 0.48733 | val_0_rmse: 0.7103  | val_1_rmse: 0.76221 |  0:00:09s
epoch 32 | loss: 0.4748  | val_0_rmse: 0.69923 | val_1_rmse: 0.75091 |  0:00:09s
epoch 33 | loss: 0.47941 | val_0_rmse: 0.69439 | val_1_rmse: 0.74363 |  0:00:10s
epoch 34 | loss: 0.47818 | val_0_rmse: 0.69674 | val_1_rmse: 0.74434 |  0:00:10s
epoch 35 | loss: 0.47963 | val_0_rmse: 0.70483 | val_1_rmse: 0.75205 |  0:00:10s
epoch 36 | loss: 0.4724  | val_0_rmse: 0.707   | val_1_rmse: 0.75392 |  0:00:10s
epoch 37 | loss: 0.47085 | val_0_rmse: 0.70292 | val_1_rmse: 0.74775 |  0:00:11s
epoch 38 | loss: 0.46307 | val_0_rmse: 0.71657 | val_1_rmse: 0.75564 |  0:00:11s
epoch 39 | loss: 0.46227 | val_0_rmse: 0.71641 | val_1_rmse: 0.75619 |  0:00:11s
epoch 40 | loss: 0.46667 | val_0_rmse: 0.7263  | val_1_rmse: 0.76785 |  0:00:12s
epoch 41 | loss: 0.46226 | val_0_rmse: 0.69014 | val_1_rmse: 0.742   |  0:00:12s
epoch 42 | loss: 0.4729  | val_0_rmse: 0.70784 | val_1_rmse: 0.74944 |  0:00:12s
epoch 43 | loss: 0.46238 | val_0_rmse: 0.69939 | val_1_rmse: 0.74735 |  0:00:12s
epoch 44 | loss: 0.45377 | val_0_rmse: 0.69729 | val_1_rmse: 0.74505 |  0:00:13s
epoch 45 | loss: 0.45462 | val_0_rmse: 0.68196 | val_1_rmse: 0.73746 |  0:00:13s
epoch 46 | loss: 0.45191 | val_0_rmse: 0.67854 | val_1_rmse: 0.73509 |  0:00:13s
epoch 47 | loss: 0.45179 | val_0_rmse: 0.69154 | val_1_rmse: 0.74931 |  0:00:14s
epoch 48 | loss: 0.44641 | val_0_rmse: 0.68422 | val_1_rmse: 0.73659 |  0:00:14s
epoch 49 | loss: 0.44303 | val_0_rmse: 0.68342 | val_1_rmse: 0.7365  |  0:00:14s
epoch 50 | loss: 0.43713 | val_0_rmse: 0.68572 | val_1_rmse: 0.74345 |  0:00:15s
epoch 51 | loss: 0.44316 | val_0_rmse: 0.68397 | val_1_rmse: 0.74137 |  0:00:15s
epoch 52 | loss: 0.44667 | val_0_rmse: 0.68541 | val_1_rmse: 0.73849 |  0:00:15s
epoch 53 | loss: 0.44462 | val_0_rmse: 0.68063 | val_1_rmse: 0.73767 |  0:00:15s
epoch 54 | loss: 0.44622 | val_0_rmse: 0.67994 | val_1_rmse: 0.73604 |  0:00:16s
epoch 55 | loss: 0.43969 | val_0_rmse: 0.67669 | val_1_rmse: 0.73066 |  0:00:16s
epoch 56 | loss: 0.44123 | val_0_rmse: 0.68268 | val_1_rmse: 0.73111 |  0:00:16s
epoch 57 | loss: 0.43698 | val_0_rmse: 0.69859 | val_1_rmse: 0.74878 |  0:00:17s
epoch 58 | loss: 0.4322  | val_0_rmse: 0.68553 | val_1_rmse: 0.74417 |  0:00:17s
epoch 59 | loss: 0.44556 | val_0_rmse: 0.72123 | val_1_rmse: 0.77066 |  0:00:17s
epoch 60 | loss: 0.452   | val_0_rmse: 0.68713 | val_1_rmse: 0.74805 |  0:00:17s
epoch 61 | loss: 0.44682 | val_0_rmse: 0.67433 | val_1_rmse: 0.72968 |  0:00:18s
epoch 62 | loss: 0.44475 | val_0_rmse: 0.67375 | val_1_rmse: 0.73255 |  0:00:18s
epoch 63 | loss: 0.43727 | val_0_rmse: 0.67472 | val_1_rmse: 0.7321  |  0:00:18s
epoch 64 | loss: 0.43898 | val_0_rmse: 0.67983 | val_1_rmse: 0.7403  |  0:00:19s
epoch 65 | loss: 0.44053 | val_0_rmse: 0.6713  | val_1_rmse: 0.73404 |  0:00:19s
epoch 66 | loss: 0.44251 | val_0_rmse: 0.66529 | val_1_rmse: 0.72567 |  0:00:19s
epoch 67 | loss: 0.43606 | val_0_rmse: 0.68548 | val_1_rmse: 0.74427 |  0:00:19s
epoch 68 | loss: 0.44307 | val_0_rmse: 0.68384 | val_1_rmse: 0.74519 |  0:00:20s
epoch 69 | loss: 0.44681 | val_0_rmse: 0.67401 | val_1_rmse: 0.73336 |  0:00:20s
epoch 70 | loss: 0.44353 | val_0_rmse: 0.68191 | val_1_rmse: 0.74826 |  0:00:20s
epoch 71 | loss: 0.44917 | val_0_rmse: 0.67207 | val_1_rmse: 0.73235 |  0:00:21s
epoch 72 | loss: 0.4462  | val_0_rmse: 0.67101 | val_1_rmse: 0.73239 |  0:00:21s
epoch 73 | loss: 0.43608 | val_0_rmse: 0.68744 | val_1_rmse: 0.74479 |  0:00:21s
epoch 74 | loss: 0.44477 | val_0_rmse: 0.68448 | val_1_rmse: 0.743   |  0:00:22s
epoch 75 | loss: 0.44004 | val_0_rmse: 0.68366 | val_1_rmse: 0.73916 |  0:00:22s
epoch 76 | loss: 0.43487 | val_0_rmse: 0.68643 | val_1_rmse: 0.74104 |  0:00:22s
epoch 77 | loss: 0.43719 | val_0_rmse: 0.67842 | val_1_rmse: 0.7337  |  0:00:22s
epoch 78 | loss: 0.43321 | val_0_rmse: 0.6847  | val_1_rmse: 0.73823 |  0:00:23s
epoch 79 | loss: 0.43266 | val_0_rmse: 0.66889 | val_1_rmse: 0.72782 |  0:00:23s
epoch 80 | loss: 0.43026 | val_0_rmse: 0.66817 | val_1_rmse: 0.7242  |  0:00:23s
epoch 81 | loss: 0.44498 | val_0_rmse: 0.67402 | val_1_rmse: 0.73246 |  0:00:24s
epoch 82 | loss: 0.43195 | val_0_rmse: 0.67089 | val_1_rmse: 0.73306 |  0:00:24s
epoch 83 | loss: 0.43954 | val_0_rmse: 0.6711  | val_1_rmse: 0.73151 |  0:00:24s
epoch 84 | loss: 0.4311  | val_0_rmse: 0.6845  | val_1_rmse: 0.74022 |  0:00:24s
epoch 85 | loss: 0.42782 | val_0_rmse: 0.667   | val_1_rmse: 0.72191 |  0:00:25s
epoch 86 | loss: 0.43288 | val_0_rmse: 0.66874 | val_1_rmse: 0.72379 |  0:00:25s
epoch 87 | loss: 0.43088 | val_0_rmse: 0.673   | val_1_rmse: 0.7283  |  0:00:25s
epoch 88 | loss: 0.42967 | val_0_rmse: 0.67608 | val_1_rmse: 0.7297  |  0:00:26s
epoch 89 | loss: 0.42842 | val_0_rmse: 0.67701 | val_1_rmse: 0.73409 |  0:00:26s
epoch 90 | loss: 0.43096 | val_0_rmse: 0.67263 | val_1_rmse: 0.72992 |  0:00:26s
epoch 91 | loss: 0.44362 | val_0_rmse: 0.6756  | val_1_rmse: 0.73206 |  0:00:27s
epoch 92 | loss: 0.45687 | val_0_rmse: 0.68601 | val_1_rmse: 0.74622 |  0:00:27s
epoch 93 | loss: 0.44662 | val_0_rmse: 0.68615 | val_1_rmse: 0.74128 |  0:00:27s
epoch 94 | loss: 0.43917 | val_0_rmse: 0.68059 | val_1_rmse: 0.7357  |  0:00:27s
epoch 95 | loss: 0.43188 | val_0_rmse: 0.6839  | val_1_rmse: 0.73814 |  0:00:28s
epoch 96 | loss: 0.4316  | val_0_rmse: 0.68801 | val_1_rmse: 0.74622 |  0:00:28s
epoch 97 | loss: 0.42803 | val_0_rmse: 0.68966 | val_1_rmse: 0.74601 |  0:00:28s
epoch 98 | loss: 0.43023 | val_0_rmse: 0.68053 | val_1_rmse: 0.73626 |  0:00:29s
epoch 99 | loss: 0.42944 | val_0_rmse: 0.6818  | val_1_rmse: 0.74167 |  0:00:29s
epoch 100| loss: 0.4263  | val_0_rmse: 0.68129 | val_1_rmse: 0.74938 |  0:00:29s
epoch 101| loss: 0.4323  | val_0_rmse: 0.67516 | val_1_rmse: 0.73922 |  0:00:29s
epoch 102| loss: 0.42851 | val_0_rmse: 0.67395 | val_1_rmse: 0.73915 |  0:00:30s
epoch 103| loss: 0.42463 | val_0_rmse: 0.66924 | val_1_rmse: 0.73464 |  0:00:30s
epoch 104| loss: 0.42863 | val_0_rmse: 0.68081 | val_1_rmse: 0.73968 |  0:00:30s
epoch 105| loss: 0.4281  | val_0_rmse: 0.67968 | val_1_rmse: 0.73301 |  0:00:31s
epoch 106| loss: 0.42213 | val_0_rmse: 0.67314 | val_1_rmse: 0.72924 |  0:00:31s
epoch 107| loss: 0.42156 | val_0_rmse: 0.68033 | val_1_rmse: 0.73864 |  0:00:31s
epoch 108| loss: 0.42168 | val_0_rmse: 0.68168 | val_1_rmse: 0.73975 |  0:00:32s
epoch 109| loss: 0.42324 | val_0_rmse: 0.68223 | val_1_rmse: 0.73981 |  0:00:32s
epoch 110| loss: 0.42664 | val_0_rmse: 0.67963 | val_1_rmse: 0.7347  |  0:00:32s
epoch 111| loss: 0.423   | val_0_rmse: 0.67541 | val_1_rmse: 0.73182 |  0:00:32s
epoch 112| loss: 0.43271 | val_0_rmse: 0.67606 | val_1_rmse: 0.73587 |  0:00:33s
epoch 113| loss: 0.42268 | val_0_rmse: 0.67014 | val_1_rmse: 0.72956 |  0:00:33s
epoch 114| loss: 0.42864 | val_0_rmse: 0.67158 | val_1_rmse: 0.72478 |  0:00:33s
epoch 115| loss: 0.42005 | val_0_rmse: 0.66885 | val_1_rmse: 0.73033 |  0:00:34s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.72191
Best weights from best epoch are automatically used!
ended training at: 07:28:20
Feature importance:
[('Area', 0.24122712831467075), ('Baths', 0.2624173184895126), ('Beds', 0.0627989293904579), ('Latitude', 0.2990769338084703), ('Longitude', 0.11007705437050201), ('Month', 0.0), ('Year', 0.024402635626386476)]
Mean squared error is of 3815695438.712206
Mean absolute error:41268.08679751168
MAPE:0.32110008277903995
R2 score:0.575717507605622
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:28:20
epoch 0  | loss: 1.35346 | val_0_rmse: 1.05426 | val_1_rmse: 1.07496 |  0:00:00s
epoch 1  | loss: 0.77804 | val_0_rmse: 0.85797 | val_1_rmse: 0.93252 |  0:00:00s
epoch 2  | loss: 0.64433 | val_0_rmse: 0.94071 | val_1_rmse: 1.02901 |  0:00:00s
epoch 3  | loss: 0.60237 | val_0_rmse: 0.86735 | val_1_rmse: 0.96987 |  0:00:01s
epoch 4  | loss: 0.59006 | val_0_rmse: 0.79358 | val_1_rmse: 0.89326 |  0:00:01s
epoch 5  | loss: 0.57139 | val_0_rmse: 0.80407 | val_1_rmse: 0.88856 |  0:00:01s
epoch 6  | loss: 0.56554 | val_0_rmse: 0.76446 | val_1_rmse: 0.8509  |  0:00:02s
epoch 7  | loss: 0.54954 | val_0_rmse: 0.74242 | val_1_rmse: 0.83081 |  0:00:02s
epoch 8  | loss: 0.54085 | val_0_rmse: 0.77599 | val_1_rmse: 0.84812 |  0:00:02s
epoch 9  | loss: 0.5326  | val_0_rmse: 0.75364 | val_1_rmse: 0.8086  |  0:00:02s
epoch 10 | loss: 0.53257 | val_0_rmse: 0.74859 | val_1_rmse: 0.83028 |  0:00:03s
epoch 11 | loss: 0.52386 | val_0_rmse: 0.75955 | val_1_rmse: 0.84768 |  0:00:03s
epoch 12 | loss: 0.51782 | val_0_rmse: 0.76102 | val_1_rmse: 0.83876 |  0:00:03s
epoch 13 | loss: 0.51597 | val_0_rmse: 0.74132 | val_1_rmse: 0.81774 |  0:00:04s
epoch 14 | loss: 0.50667 | val_0_rmse: 0.72344 | val_1_rmse: 0.80245 |  0:00:04s
epoch 15 | loss: 0.50641 | val_0_rmse: 0.82054 | val_1_rmse: 0.88691 |  0:00:04s
epoch 16 | loss: 0.50929 | val_0_rmse: 0.76746 | val_1_rmse: 0.83764 |  0:00:05s
epoch 17 | loss: 0.51312 | val_0_rmse: 0.73744 | val_1_rmse: 0.80925 |  0:00:05s
epoch 18 | loss: 0.50661 | val_0_rmse: 0.7816  | val_1_rmse: 0.85502 |  0:00:05s
epoch 19 | loss: 0.50018 | val_0_rmse: 0.88263 | val_1_rmse: 0.9371  |  0:00:05s
epoch 20 | loss: 0.49892 | val_0_rmse: 0.76278 | val_1_rmse: 0.83547 |  0:00:06s
epoch 21 | loss: 0.49204 | val_0_rmse: 0.78766 | val_1_rmse: 0.84992 |  0:00:06s
epoch 22 | loss: 0.49106 | val_0_rmse: 0.7008  | val_1_rmse: 0.77517 |  0:00:06s
epoch 23 | loss: 0.48768 | val_0_rmse: 0.757   | val_1_rmse: 0.79247 |  0:00:07s
epoch 24 | loss: 0.47721 | val_0_rmse: 0.72202 | val_1_rmse: 0.80559 |  0:00:07s
epoch 25 | loss: 0.47246 | val_0_rmse: 0.76558 | val_1_rmse: 0.84536 |  0:00:07s
epoch 26 | loss: 0.48148 | val_0_rmse: 0.70557 | val_1_rmse: 0.7871  |  0:00:07s
epoch 27 | loss: 0.47258 | val_0_rmse: 0.68943 | val_1_rmse: 0.77728 |  0:00:08s
epoch 28 | loss: 0.46173 | val_0_rmse: 0.72403 | val_1_rmse: 0.81185 |  0:00:08s
epoch 29 | loss: 0.46296 | val_0_rmse: 0.69143 | val_1_rmse: 0.77945 |  0:00:08s
epoch 30 | loss: 0.45504 | val_0_rmse: 0.71968 | val_1_rmse: 0.777   |  0:00:09s
epoch 31 | loss: 0.46291 | val_0_rmse: 0.72313 | val_1_rmse: 0.78363 |  0:00:09s
epoch 32 | loss: 0.46531 | val_0_rmse: 0.72478 | val_1_rmse: 0.79268 |  0:00:09s
epoch 33 | loss: 0.45606 | val_0_rmse: 0.70076 | val_1_rmse: 0.76915 |  0:00:10s
epoch 34 | loss: 0.45422 | val_0_rmse: 0.76632 | val_1_rmse: 0.82475 |  0:00:10s
epoch 35 | loss: 0.4609  | val_0_rmse: 0.7233  | val_1_rmse: 0.80275 |  0:00:10s
epoch 36 | loss: 0.45157 | val_0_rmse: 0.7475  | val_1_rmse: 0.81081 |  0:00:10s
epoch 37 | loss: 0.4513  | val_0_rmse: 0.71087 | val_1_rmse: 0.78319 |  0:00:11s
epoch 38 | loss: 0.44234 | val_0_rmse: 0.68713 | val_1_rmse: 0.76605 |  0:00:11s
epoch 39 | loss: 0.4328  | val_0_rmse: 0.67719 | val_1_rmse: 0.74739 |  0:00:11s
epoch 40 | loss: 0.43378 | val_0_rmse: 0.68254 | val_1_rmse: 0.75314 |  0:00:12s
epoch 41 | loss: 0.429   | val_0_rmse: 0.72867 | val_1_rmse: 0.76661 |  0:00:12s
epoch 42 | loss: 0.45659 | val_0_rmse: 0.74914 | val_1_rmse: 0.79004 |  0:00:12s
epoch 43 | loss: 0.44643 | val_0_rmse: 0.71499 | val_1_rmse: 0.78085 |  0:00:13s
epoch 44 | loss: 0.4521  | val_0_rmse: 0.73388 | val_1_rmse: 0.80638 |  0:00:13s
epoch 45 | loss: 0.44398 | val_0_rmse: 0.7186  | val_1_rmse: 0.77694 |  0:00:13s
epoch 46 | loss: 0.43885 | val_0_rmse: 0.68962 | val_1_rmse: 0.77324 |  0:00:13s
epoch 47 | loss: 0.47323 | val_0_rmse: 0.68871 | val_1_rmse: 0.77366 |  0:00:14s
epoch 48 | loss: 0.46024 | val_0_rmse: 0.6759  | val_1_rmse: 0.77518 |  0:00:14s
epoch 49 | loss: 0.43997 | val_0_rmse: 0.72361 | val_1_rmse: 0.79228 |  0:00:14s
epoch 50 | loss: 0.44845 | val_0_rmse: 0.74996 | val_1_rmse: 0.80837 |  0:00:15s
epoch 51 | loss: 0.4447  | val_0_rmse: 0.69089 | val_1_rmse: 0.76931 |  0:00:15s
epoch 52 | loss: 0.42789 | val_0_rmse: 0.70954 | val_1_rmse: 0.75285 |  0:00:15s
epoch 53 | loss: 0.42604 | val_0_rmse: 0.68066 | val_1_rmse: 0.75292 |  0:00:15s
epoch 54 | loss: 0.43882 | val_0_rmse: 0.67047 | val_1_rmse: 0.75337 |  0:00:16s
epoch 55 | loss: 0.41753 | val_0_rmse: 0.65657 | val_1_rmse: 0.74811 |  0:00:16s
epoch 56 | loss: 0.41718 | val_0_rmse: 0.6587  | val_1_rmse: 0.75431 |  0:00:16s
epoch 57 | loss: 0.42104 | val_0_rmse: 0.68499 | val_1_rmse: 0.75259 |  0:00:17s
epoch 58 | loss: 0.41322 | val_0_rmse: 0.7146  | val_1_rmse: 0.75897 |  0:00:17s
epoch 59 | loss: 0.41595 | val_0_rmse: 0.73366 | val_1_rmse: 0.77135 |  0:00:17s
epoch 60 | loss: 0.41828 | val_0_rmse: 0.67366 | val_1_rmse: 0.75464 |  0:00:17s
epoch 61 | loss: 0.41541 | val_0_rmse: 0.64093 | val_1_rmse: 0.73068 |  0:00:18s
epoch 62 | loss: 0.41136 | val_0_rmse: 0.65497 | val_1_rmse: 0.74339 |  0:00:18s
epoch 63 | loss: 0.40558 | val_0_rmse: 0.64447 | val_1_rmse: 0.72286 |  0:00:18s
epoch 64 | loss: 0.40311 | val_0_rmse: 0.63829 | val_1_rmse: 0.72337 |  0:00:19s
epoch 65 | loss: 0.41047 | val_0_rmse: 0.67837 | val_1_rmse: 0.74659 |  0:00:19s
epoch 66 | loss: 0.40045 | val_0_rmse: 0.68591 | val_1_rmse: 0.74909 |  0:00:19s
epoch 67 | loss: 0.40955 | val_0_rmse: 0.66701 | val_1_rmse: 0.72875 |  0:00:20s
epoch 68 | loss: 0.40445 | val_0_rmse: 0.68105 | val_1_rmse: 0.74413 |  0:00:20s
epoch 69 | loss: 0.40224 | val_0_rmse: 0.66616 | val_1_rmse: 0.74683 |  0:00:20s
epoch 70 | loss: 0.41609 | val_0_rmse: 0.65559 | val_1_rmse: 0.73986 |  0:00:20s
epoch 71 | loss: 0.40308 | val_0_rmse: 0.66102 | val_1_rmse: 0.7486  |  0:00:21s
epoch 72 | loss: 0.40014 | val_0_rmse: 0.64426 | val_1_rmse: 0.73982 |  0:00:21s
epoch 73 | loss: 0.40218 | val_0_rmse: 0.66405 | val_1_rmse: 0.75593 |  0:00:21s
epoch 74 | loss: 0.39921 | val_0_rmse: 0.65266 | val_1_rmse: 0.72903 |  0:00:22s
epoch 75 | loss: 0.39488 | val_0_rmse: 0.67778 | val_1_rmse: 0.74326 |  0:00:22s
epoch 76 | loss: 0.40817 | val_0_rmse: 0.65549 | val_1_rmse: 0.73009 |  0:00:22s
epoch 77 | loss: 0.40237 | val_0_rmse: 0.64222 | val_1_rmse: 0.72719 |  0:00:22s
epoch 78 | loss: 0.39632 | val_0_rmse: 0.6461  | val_1_rmse: 0.72815 |  0:00:23s
epoch 79 | loss: 0.39525 | val_0_rmse: 0.64599 | val_1_rmse: 0.73626 |  0:00:23s
epoch 80 | loss: 0.40769 | val_0_rmse: 0.64333 | val_1_rmse: 0.73594 |  0:00:23s
epoch 81 | loss: 0.39218 | val_0_rmse: 0.63592 | val_1_rmse: 0.74701 |  0:00:24s
epoch 82 | loss: 0.39699 | val_0_rmse: 0.64063 | val_1_rmse: 0.75938 |  0:00:24s
epoch 83 | loss: 0.40235 | val_0_rmse: 0.63507 | val_1_rmse: 0.73229 |  0:00:24s
epoch 84 | loss: 0.39327 | val_0_rmse: 0.6691  | val_1_rmse: 0.76302 |  0:00:24s
epoch 85 | loss: 0.40706 | val_0_rmse: 0.66616 | val_1_rmse: 0.75943 |  0:00:25s
epoch 86 | loss: 0.40139 | val_0_rmse: 0.64234 | val_1_rmse: 0.7536  |  0:00:25s
epoch 87 | loss: 0.40172 | val_0_rmse: 0.64251 | val_1_rmse: 0.745   |  0:00:25s
epoch 88 | loss: 0.39782 | val_0_rmse: 0.63313 | val_1_rmse: 0.74722 |  0:00:26s
epoch 89 | loss: 0.39671 | val_0_rmse: 0.64295 | val_1_rmse: 0.74471 |  0:00:26s
epoch 90 | loss: 0.41488 | val_0_rmse: 0.65398 | val_1_rmse: 0.73393 |  0:00:26s
epoch 91 | loss: 0.39911 | val_0_rmse: 0.64802 | val_1_rmse: 0.72848 |  0:00:27s
epoch 92 | loss: 0.38481 | val_0_rmse: 0.65364 | val_1_rmse: 0.74094 |  0:00:27s
epoch 93 | loss: 0.39589 | val_0_rmse: 0.6477  | val_1_rmse: 0.75606 |  0:00:27s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.72286
Best weights from best epoch are automatically used!
ended training at: 07:28:47
Feature importance:
[('Area', 0.18927768828163416), ('Baths', 0.18011972915159982), ('Beds', 0.035316851171190086), ('Latitude', 0.2884067630948657), ('Longitude', 0.24248880796239053), ('Month', 0.034699508642901125), ('Year', 0.029690651695418596)]
Mean squared error is of 3375286804.5121036
Mean absolute error:38295.976644357164
MAPE:0.3192018012070006
R2 score:0.6291230601624858
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:28:48
epoch 0  | loss: 0.3888  | val_0_rmse: 0.54107 | val_1_rmse: 0.53745 |  0:00:03s
epoch 1  | loss: 0.29735 | val_0_rmse: 0.53337 | val_1_rmse: 0.53001 |  0:00:07s
epoch 2  | loss: 0.29315 | val_0_rmse: 0.54163 | val_1_rmse: 0.53709 |  0:00:11s
epoch 3  | loss: 0.28931 | val_0_rmse: 0.52786 | val_1_rmse: 0.52341 |  0:00:15s
epoch 4  | loss: 0.28868 | val_0_rmse: 0.53104 | val_1_rmse: 0.52771 |  0:00:19s
epoch 5  | loss: 0.28797 | val_0_rmse: 0.53675 | val_1_rmse: 0.53275 |  0:00:23s
epoch 6  | loss: 0.28512 | val_0_rmse: 0.54633 | val_1_rmse: 0.54003 |  0:00:27s
epoch 7  | loss: 0.29092 | val_0_rmse: 0.52979 | val_1_rmse: 0.52578 |  0:00:30s
epoch 8  | loss: 0.28564 | val_0_rmse: 0.52491 | val_1_rmse: 0.52135 |  0:00:34s
epoch 9  | loss: 0.28965 | val_0_rmse: 0.5253  | val_1_rmse: 0.52155 |  0:00:38s
epoch 10 | loss: 0.29221 | val_0_rmse: 0.52826 | val_1_rmse: 0.52372 |  0:00:42s
epoch 11 | loss: 0.28518 | val_0_rmse: 0.52838 | val_1_rmse: 0.52499 |  0:00:46s
epoch 12 | loss: 0.28694 | val_0_rmse: 0.52875 | val_1_rmse: 0.52669 |  0:00:50s
epoch 13 | loss: 0.28414 | val_0_rmse: 0.53727 | val_1_rmse: 0.53349 |  0:00:54s
epoch 14 | loss: 0.28717 | val_0_rmse: 0.54528 | val_1_rmse: 0.54153 |  0:00:57s
epoch 15 | loss: 0.28285 | val_0_rmse: 0.52288 | val_1_rmse: 0.51756 |  0:01:01s
epoch 16 | loss: 0.28236 | val_0_rmse: 0.52732 | val_1_rmse: 0.52319 |  0:01:05s
epoch 17 | loss: 0.28056 | val_0_rmse: 0.52958 | val_1_rmse: 0.52572 |  0:01:09s
epoch 18 | loss: 0.27633 | val_0_rmse: 0.52014 | val_1_rmse: 0.51614 |  0:01:13s
epoch 19 | loss: 0.27662 | val_0_rmse: 0.51814 | val_1_rmse: 0.51386 |  0:01:17s
epoch 20 | loss: 0.27509 | val_0_rmse: 0.52104 | val_1_rmse: 0.5164  |  0:01:21s
epoch 21 | loss: 0.27469 | val_0_rmse: 0.51647 | val_1_rmse: 0.51094 |  0:01:24s
epoch 22 | loss: 0.27626 | val_0_rmse: 0.51999 | val_1_rmse: 0.51716 |  0:01:28s
epoch 23 | loss: 0.27401 | val_0_rmse: 0.51435 | val_1_rmse: 0.51083 |  0:01:32s
epoch 24 | loss: 0.27586 | val_0_rmse: 0.51795 | val_1_rmse: 0.51366 |  0:01:36s
epoch 25 | loss: 0.27491 | val_0_rmse: 0.5161  | val_1_rmse: 0.51314 |  0:01:40s
epoch 26 | loss: 0.27224 | val_0_rmse: 0.5201  | val_1_rmse: 0.51692 |  0:01:44s
epoch 27 | loss: 0.26576 | val_0_rmse: 0.54671 | val_1_rmse: 0.54172 |  0:01:48s
epoch 28 | loss: 0.26346 | val_0_rmse: 0.56993 | val_1_rmse: 0.56432 |  0:01:51s
epoch 29 | loss: 0.2629  | val_0_rmse: 0.51448 | val_1_rmse: 0.51099 |  0:01:55s
epoch 30 | loss: 0.26537 | val_0_rmse: 0.57137 | val_1_rmse: 0.56589 |  0:01:59s
epoch 31 | loss: 0.26368 | val_0_rmse: 0.51204 | val_1_rmse: 0.50891 |  0:02:03s
epoch 32 | loss: 0.26264 | val_0_rmse: 0.50551 | val_1_rmse: 0.50295 |  0:02:07s
epoch 33 | loss: 0.2571  | val_0_rmse: 0.49993 | val_1_rmse: 0.49508 |  0:02:11s
epoch 34 | loss: 0.25907 | val_0_rmse: 0.51423 | val_1_rmse: 0.51169 |  0:02:15s
epoch 35 | loss: 0.25703 | val_0_rmse: 0.51922 | val_1_rmse: 0.51667 |  0:02:19s
epoch 36 | loss: 0.25851 | val_0_rmse: 0.52567 | val_1_rmse: 0.52284 |  0:02:22s
epoch 37 | loss: 0.25819 | val_0_rmse: 0.50772 | val_1_rmse: 0.50499 |  0:02:26s
epoch 38 | loss: 0.25379 | val_0_rmse: 0.51551 | val_1_rmse: 0.51351 |  0:02:30s
epoch 39 | loss: 0.25242 | val_0_rmse: 0.53267 | val_1_rmse: 0.53005 |  0:02:34s
epoch 40 | loss: 0.25308 | val_0_rmse: 0.50947 | val_1_rmse: 0.50592 |  0:02:38s
epoch 41 | loss: 0.25246 | val_0_rmse: 0.5427  | val_1_rmse: 0.54245 |  0:02:42s
epoch 42 | loss: 0.2513  | val_0_rmse: 0.49597 | val_1_rmse: 0.49392 |  0:02:46s
epoch 43 | loss: 0.25084 | val_0_rmse: 0.49711 | val_1_rmse: 0.49576 |  0:02:49s
epoch 44 | loss: 0.25011 | val_0_rmse: 0.49275 | val_1_rmse: 0.49133 |  0:02:53s
epoch 45 | loss: 0.24986 | val_0_rmse: 0.49504 | val_1_rmse: 0.49389 |  0:02:57s
epoch 46 | loss: 0.24937 | val_0_rmse: 0.49678 | val_1_rmse: 0.49457 |  0:03:01s
epoch 47 | loss: 0.24871 | val_0_rmse: 0.56113 | val_1_rmse: 0.55499 |  0:03:05s
epoch 48 | loss: 0.24802 | val_0_rmse: 0.51968 | val_1_rmse: 0.51877 |  0:03:09s
epoch 49 | loss: 0.24844 | val_0_rmse: 0.54042 | val_1_rmse: 0.53869 |  0:03:12s
epoch 50 | loss: 0.25048 | val_0_rmse: 0.55093 | val_1_rmse: 0.54944 |  0:03:16s
epoch 51 | loss: 0.24874 | val_0_rmse: 0.63372 | val_1_rmse: 0.63354 |  0:03:20s
epoch 52 | loss: 0.24792 | val_0_rmse: 0.52341 | val_1_rmse: 0.52023 |  0:03:24s
epoch 53 | loss: 0.24725 | val_0_rmse: 0.62459 | val_1_rmse: 0.61994 |  0:03:28s
epoch 54 | loss: 0.24728 | val_0_rmse: 0.5221  | val_1_rmse: 0.5207  |  0:03:32s
epoch 55 | loss: 0.24934 | val_0_rmse: 0.53311 | val_1_rmse: 0.53088 |  0:03:36s
epoch 56 | loss: 0.2484  | val_0_rmse: 0.58992 | val_1_rmse: 0.58484 |  0:03:39s
epoch 57 | loss: 0.24932 | val_0_rmse: 0.4997  | val_1_rmse: 0.49762 |  0:03:43s
epoch 58 | loss: 0.24752 | val_0_rmse: 0.48912 | val_1_rmse: 0.48817 |  0:03:47s
epoch 59 | loss: 0.24554 | val_0_rmse: 0.73775 | val_1_rmse: 0.73655 |  0:03:51s
epoch 60 | loss: 0.24495 | val_0_rmse: 0.5599  | val_1_rmse: 0.55823 |  0:03:55s
epoch 61 | loss: 0.24483 | val_0_rmse: 0.51351 | val_1_rmse: 0.5117  |  0:03:59s
epoch 62 | loss: 0.24568 | val_0_rmse: 0.5081  | val_1_rmse: 0.50723 |  0:04:02s
epoch 63 | loss: 0.24416 | val_0_rmse: 0.49558 | val_1_rmse: 0.49543 |  0:04:06s
epoch 64 | loss: 0.24413 | val_0_rmse: 0.53575 | val_1_rmse: 0.53377 |  0:04:10s
epoch 65 | loss: 0.24502 | val_0_rmse: 0.4981  | val_1_rmse: 0.49606 |  0:04:14s
epoch 66 | loss: 0.24398 | val_0_rmse: 0.50219 | val_1_rmse: 0.50135 |  0:04:18s
epoch 67 | loss: 0.24371 | val_0_rmse: 0.64837 | val_1_rmse: 0.6461  |  0:04:22s
epoch 68 | loss: 0.24539 | val_0_rmse: 0.53179 | val_1_rmse: 0.52834 |  0:04:26s
epoch 69 | loss: 0.24412 | val_0_rmse: 0.53306 | val_1_rmse: 0.53196 |  0:04:30s
epoch 70 | loss: 0.24312 | val_0_rmse: 0.58501 | val_1_rmse: 0.5825  |  0:04:33s
epoch 71 | loss: 0.24298 | val_0_rmse: 0.50827 | val_1_rmse: 0.50494 |  0:04:37s
epoch 72 | loss: 0.24459 | val_0_rmse: 0.54791 | val_1_rmse: 0.54532 |  0:04:41s
epoch 73 | loss: 0.24388 | val_0_rmse: 0.48909 | val_1_rmse: 0.4863  |  0:04:45s
epoch 74 | loss: 0.24256 | val_0_rmse: 0.59722 | val_1_rmse: 0.59357 |  0:04:49s
epoch 75 | loss: 0.2443  | val_0_rmse: 0.52135 | val_1_rmse: 0.52098 |  0:04:53s
epoch 76 | loss: 0.24425 | val_0_rmse: 0.51105 | val_1_rmse: 0.50967 |  0:04:57s
epoch 77 | loss: 0.24158 | val_0_rmse: 0.61981 | val_1_rmse: 0.61626 |  0:05:00s
epoch 78 | loss: 0.24247 | val_0_rmse: 0.50418 | val_1_rmse: 0.5015  |  0:05:04s
epoch 79 | loss: 0.24194 | val_0_rmse: 0.53057 | val_1_rmse: 0.52889 |  0:05:08s
epoch 80 | loss: 0.24184 | val_0_rmse: 0.51871 | val_1_rmse: 0.51712 |  0:05:12s
epoch 81 | loss: 0.24038 | val_0_rmse: 0.5278  | val_1_rmse: 0.52706 |  0:05:16s
epoch 82 | loss: 0.24069 | val_0_rmse: 0.50255 | val_1_rmse: 0.50249 |  0:05:20s
epoch 83 | loss: 0.23926 | val_0_rmse: 0.49915 | val_1_rmse: 0.49806 |  0:05:23s
epoch 84 | loss: 0.24425 | val_0_rmse: 0.48871 | val_1_rmse: 0.48909 |  0:05:27s
epoch 85 | loss: 0.23952 | val_0_rmse: 0.56548 | val_1_rmse: 0.56169 |  0:05:31s
epoch 86 | loss: 0.2457  | val_0_rmse: 0.51779 | val_1_rmse: 0.5148  |  0:05:35s
epoch 87 | loss: 0.24699 | val_0_rmse: 0.68954 | val_1_rmse: 0.68898 |  0:05:39s
epoch 88 | loss: 0.24318 | val_0_rmse: 0.58334 | val_1_rmse: 0.58289 |  0:05:43s
epoch 89 | loss: 0.24542 | val_0_rmse: 0.55308 | val_1_rmse: 0.55009 |  0:05:47s
epoch 90 | loss: 0.24506 | val_0_rmse: 0.5682  | val_1_rmse: 0.56706 |  0:05:50s
epoch 91 | loss: 0.24145 | val_0_rmse: 0.49457 | val_1_rmse: 0.49395 |  0:05:54s
epoch 92 | loss: 0.23917 | val_0_rmse: 0.52565 | val_1_rmse: 0.52538 |  0:05:58s
epoch 93 | loss: 0.24062 | val_0_rmse: 0.49325 | val_1_rmse: 0.48993 |  0:06:02s
epoch 94 | loss: 0.23964 | val_0_rmse: 0.53424 | val_1_rmse: 0.53237 |  0:06:06s
epoch 95 | loss: 0.24065 | val_0_rmse: 0.54865 | val_1_rmse: 0.54722 |  0:06:10s
epoch 96 | loss: 0.23886 | val_0_rmse: 0.56364 | val_1_rmse: 0.56161 |  0:06:14s
epoch 97 | loss: 0.23994 | val_0_rmse: 0.54702 | val_1_rmse: 0.54572 |  0:06:17s
epoch 98 | loss: 0.24048 | val_0_rmse: 0.74483 | val_1_rmse: 0.74217 |  0:06:21s
epoch 99 | loss: 0.23905 | val_0_rmse: 0.69539 | val_1_rmse: 0.6909  |  0:06:25s
epoch 100| loss: 0.23997 | val_0_rmse: 0.63312 | val_1_rmse: 0.62916 |  0:06:29s
epoch 101| loss: 0.24035 | val_0_rmse: 0.58796 | val_1_rmse: 0.58857 |  0:06:33s
epoch 102| loss: 0.23785 | val_0_rmse: 0.72521 | val_1_rmse: 0.7223  |  0:06:37s
epoch 103| loss: 0.23686 | val_0_rmse: 0.50285 | val_1_rmse: 0.50198 |  0:06:40s

Early stopping occured at epoch 103 with best_epoch = 73 and best_val_1_rmse = 0.4863
Best weights from best epoch are automatically used!
ended training at: 07:35:30
Feature importance:
[('Area', 0.5736295721841912), ('Baths', 0.032409978008900574), ('Beds', 0.11867612245914214), ('Latitude', 0.171763683815011), ('Longitude', 0.035850898663255186), ('Month', 0.03493942817542989), ('Year', 0.032730316694069984)]
Mean squared error is of 945612937.4055653
Mean absolute error:20256.62557129805
MAPE:0.2926978805148336
R2 score:0.7213478265730298
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:35:31
epoch 0  | loss: 0.40206 | val_0_rmse: 0.5524  | val_1_rmse: 0.55464 |  0:00:03s
epoch 1  | loss: 0.31316 | val_0_rmse: 0.54211 | val_1_rmse: 0.54566 |  0:00:07s
epoch 2  | loss: 0.29781 | val_0_rmse: 0.54242 | val_1_rmse: 0.54535 |  0:00:11s
epoch 3  | loss: 0.29373 | val_0_rmse: 0.53445 | val_1_rmse: 0.53931 |  0:00:15s
epoch 4  | loss: 0.29009 | val_0_rmse: 0.53102 | val_1_rmse: 0.53615 |  0:00:19s
epoch 5  | loss: 0.29002 | val_0_rmse: 0.5289  | val_1_rmse: 0.53512 |  0:00:23s
epoch 6  | loss: 0.2894  | val_0_rmse: 0.53172 | val_1_rmse: 0.53631 |  0:00:27s
epoch 7  | loss: 0.28627 | val_0_rmse: 0.52134 | val_1_rmse: 0.52611 |  0:00:31s
epoch 8  | loss: 0.28198 | val_0_rmse: 0.52265 | val_1_rmse: 0.52589 |  0:00:35s
epoch 9  | loss: 0.28053 | val_0_rmse: 0.52283 | val_1_rmse: 0.52759 |  0:00:38s
epoch 10 | loss: 0.28262 | val_0_rmse: 0.52336 | val_1_rmse: 0.52679 |  0:00:42s
epoch 11 | loss: 0.28171 | val_0_rmse: 0.52236 | val_1_rmse: 0.52703 |  0:00:46s
epoch 12 | loss: 0.27856 | val_0_rmse: 0.52298 | val_1_rmse: 0.52699 |  0:00:50s
epoch 13 | loss: 0.2766  | val_0_rmse: 0.52377 | val_1_rmse: 0.5276  |  0:00:54s
epoch 14 | loss: 0.28177 | val_0_rmse: 0.52631 | val_1_rmse: 0.53085 |  0:00:58s
epoch 15 | loss: 0.27995 | val_0_rmse: 0.52152 | val_1_rmse: 0.52681 |  0:01:02s
epoch 16 | loss: 0.28091 | val_0_rmse: 0.524   | val_1_rmse: 0.52953 |  0:01:05s
epoch 17 | loss: 0.28078 | val_0_rmse: 0.51953 | val_1_rmse: 0.52526 |  0:01:09s
epoch 18 | loss: 0.27919 | val_0_rmse: 0.52529 | val_1_rmse: 0.52869 |  0:01:13s
epoch 19 | loss: 0.28035 | val_0_rmse: 0.51947 | val_1_rmse: 0.52324 |  0:01:17s
epoch 20 | loss: 0.2779  | val_0_rmse: 0.52028 | val_1_rmse: 0.52513 |  0:01:21s
epoch 21 | loss: 0.27956 | val_0_rmse: 0.52079 | val_1_rmse: 0.52485 |  0:01:25s
epoch 22 | loss: 0.27751 | val_0_rmse: 0.51823 | val_1_rmse: 0.52279 |  0:01:29s
epoch 23 | loss: 0.27469 | val_0_rmse: 0.52147 | val_1_rmse: 0.52662 |  0:01:32s
epoch 24 | loss: 0.27586 | val_0_rmse: 0.5252  | val_1_rmse: 0.52975 |  0:01:36s
epoch 25 | loss: 0.28016 | val_0_rmse: 0.51854 | val_1_rmse: 0.52262 |  0:01:40s
epoch 26 | loss: 0.27793 | val_0_rmse: 0.52455 | val_1_rmse: 0.52868 |  0:01:44s
epoch 27 | loss: 0.27593 | val_0_rmse: 0.51845 | val_1_rmse: 0.52255 |  0:01:48s
epoch 28 | loss: 0.27665 | val_0_rmse: 0.52976 | val_1_rmse: 0.53392 |  0:01:52s
epoch 29 | loss: 0.27539 | val_0_rmse: 0.52114 | val_1_rmse: 0.5263  |  0:01:56s
epoch 30 | loss: 0.27457 | val_0_rmse: 0.518   | val_1_rmse: 0.52064 |  0:02:00s
epoch 31 | loss: 0.27556 | val_0_rmse: 0.51889 | val_1_rmse: 0.52182 |  0:02:03s
epoch 32 | loss: 0.27695 | val_0_rmse: 0.51929 | val_1_rmse: 0.52266 |  0:02:07s
epoch 33 | loss: 0.27442 | val_0_rmse: 0.52227 | val_1_rmse: 0.52622 |  0:02:11s
epoch 34 | loss: 0.2739  | val_0_rmse: 0.52712 | val_1_rmse: 0.53076 |  0:02:15s
epoch 35 | loss: 0.2737  | val_0_rmse: 0.51381 | val_1_rmse: 0.51765 |  0:02:19s
epoch 36 | loss: 0.27171 | val_0_rmse: 0.51849 | val_1_rmse: 0.5231  |  0:02:23s
epoch 37 | loss: 0.27178 | val_0_rmse: 0.52213 | val_1_rmse: 0.52524 |  0:02:27s
epoch 38 | loss: 0.27202 | val_0_rmse: 0.51542 | val_1_rmse: 0.51779 |  0:02:30s
epoch 39 | loss: 0.27152 | val_0_rmse: 0.52272 | val_1_rmse: 0.5274  |  0:02:34s
epoch 40 | loss: 0.26892 | val_0_rmse: 0.51872 | val_1_rmse: 0.52231 |  0:02:38s
epoch 41 | loss: 0.27034 | val_0_rmse: 0.53531 | val_1_rmse: 0.53794 |  0:02:42s
epoch 42 | loss: 0.27023 | val_0_rmse: 0.52196 | val_1_rmse: 0.52743 |  0:02:46s
epoch 43 | loss: 0.27    | val_0_rmse: 0.52352 | val_1_rmse: 0.52878 |  0:02:50s
epoch 44 | loss: 0.26798 | val_0_rmse: 0.52145 | val_1_rmse: 0.52439 |  0:02:54s
epoch 45 | loss: 0.26643 | val_0_rmse: 0.51567 | val_1_rmse: 0.51805 |  0:02:57s
epoch 46 | loss: 0.26761 | val_0_rmse: 0.51613 | val_1_rmse: 0.51933 |  0:03:01s
epoch 47 | loss: 0.26658 | val_0_rmse: 0.51937 | val_1_rmse: 0.52347 |  0:03:05s
epoch 48 | loss: 0.26613 | val_0_rmse: 0.52201 | val_1_rmse: 0.52393 |  0:03:09s
epoch 49 | loss: 0.26723 | val_0_rmse: 0.51807 | val_1_rmse: 0.52135 |  0:03:13s
epoch 50 | loss: 0.26445 | val_0_rmse: 0.51594 | val_1_rmse: 0.52033 |  0:03:17s
epoch 51 | loss: 0.26628 | val_0_rmse: 0.57676 | val_1_rmse: 0.58027 |  0:03:20s
epoch 52 | loss: 0.26601 | val_0_rmse: 0.51997 | val_1_rmse: 0.5243  |  0:03:24s
epoch 53 | loss: 0.26561 | val_0_rmse: 0.52663 | val_1_rmse: 0.53142 |  0:03:28s
epoch 54 | loss: 0.26509 | val_0_rmse: 0.50791 | val_1_rmse: 0.5122  |  0:03:32s
epoch 55 | loss: 0.26549 | val_0_rmse: 0.51169 | val_1_rmse: 0.51576 |  0:03:36s
epoch 56 | loss: 0.26506 | val_0_rmse: 0.52776 | val_1_rmse: 0.532   |  0:03:40s
epoch 57 | loss: 0.26369 | val_0_rmse: 0.50894 | val_1_rmse: 0.51053 |  0:03:44s
epoch 58 | loss: 0.26333 | val_0_rmse: 0.51429 | val_1_rmse: 0.51645 |  0:03:48s
epoch 59 | loss: 0.26266 | val_0_rmse: 0.54143 | val_1_rmse: 0.54544 |  0:03:51s
epoch 60 | loss: 0.26081 | val_0_rmse: 0.50995 | val_1_rmse: 0.51208 |  0:03:55s
epoch 61 | loss: 0.26239 | val_0_rmse: 0.50802 | val_1_rmse: 0.51054 |  0:03:59s
epoch 62 | loss: 0.26114 | val_0_rmse: 0.54234 | val_1_rmse: 0.54688 |  0:04:03s
epoch 63 | loss: 0.26303 | val_0_rmse: 0.73104 | val_1_rmse: 0.73865 |  0:04:07s
epoch 64 | loss: 0.25845 | val_0_rmse: 0.50568 | val_1_rmse: 0.50839 |  0:04:11s
epoch 65 | loss: 0.25612 | val_0_rmse: 0.52332 | val_1_rmse: 0.52479 |  0:04:14s
epoch 66 | loss: 0.25637 | val_0_rmse: 0.50838 | val_1_rmse: 0.51045 |  0:04:18s
epoch 67 | loss: 0.2547  | val_0_rmse: 0.51534 | val_1_rmse: 0.51853 |  0:04:22s
epoch 68 | loss: 0.25438 | val_0_rmse: 0.53519 | val_1_rmse: 0.53707 |  0:04:26s
epoch 69 | loss: 0.25487 | val_0_rmse: 0.52622 | val_1_rmse: 0.52756 |  0:04:30s
epoch 70 | loss: 0.254   | val_0_rmse: 0.60422 | val_1_rmse: 0.60833 |  0:04:34s
epoch 71 | loss: 0.25233 | val_0_rmse: 0.50802 | val_1_rmse: 0.51143 |  0:04:38s
epoch 72 | loss: 0.25321 | val_0_rmse: 0.51244 | val_1_rmse: 0.51679 |  0:04:41s
epoch 73 | loss: 0.2513  | val_0_rmse: 0.5317  | val_1_rmse: 0.53381 |  0:04:45s
epoch 74 | loss: 0.25233 | val_0_rmse: 0.50322 | val_1_rmse: 0.50762 |  0:04:49s
epoch 75 | loss: 0.24917 | val_0_rmse: 0.54056 | val_1_rmse: 0.54451 |  0:04:53s
epoch 76 | loss: 0.25078 | val_0_rmse: 0.51199 | val_1_rmse: 0.51491 |  0:04:57s
epoch 77 | loss: 0.24988 | val_0_rmse: 0.49783 | val_1_rmse: 0.49995 |  0:05:01s
epoch 78 | loss: 0.25043 | val_0_rmse: 0.50312 | val_1_rmse: 0.50674 |  0:05:04s
epoch 79 | loss: 0.24724 | val_0_rmse: 0.49419 | val_1_rmse: 0.49674 |  0:05:08s
epoch 80 | loss: 0.24554 | val_0_rmse: 0.49554 | val_1_rmse: 0.49882 |  0:05:12s
epoch 81 | loss: 0.24808 | val_0_rmse: 0.55237 | val_1_rmse: 0.55648 |  0:05:16s
epoch 82 | loss: 0.24604 | val_0_rmse: 0.52534 | val_1_rmse: 0.52893 |  0:05:20s
epoch 83 | loss: 0.24453 | val_0_rmse: 0.53622 | val_1_rmse: 0.53999 |  0:05:24s
epoch 84 | loss: 0.24585 | val_0_rmse: 0.50259 | val_1_rmse: 0.5045  |  0:05:28s
epoch 85 | loss: 0.24573 | val_0_rmse: 0.53986 | val_1_rmse: 0.54481 |  0:05:32s
epoch 86 | loss: 0.24584 | val_0_rmse: 0.50752 | val_1_rmse: 0.51091 |  0:05:35s
epoch 87 | loss: 0.24467 | val_0_rmse: 0.49999 | val_1_rmse: 0.50213 |  0:05:39s
epoch 88 | loss: 0.24255 | val_0_rmse: 0.52457 | val_1_rmse: 0.5262  |  0:05:43s
epoch 89 | loss: 0.2422  | val_0_rmse: 0.50596 | val_1_rmse: 0.50991 |  0:05:47s
epoch 90 | loss: 0.24205 | val_0_rmse: 0.56177 | val_1_rmse: 0.56579 |  0:05:51s
epoch 91 | loss: 0.24465 | val_0_rmse: 0.4918  | val_1_rmse: 0.4942  |  0:05:55s
epoch 92 | loss: 0.24414 | val_0_rmse: 0.4939  | val_1_rmse: 0.49655 |  0:05:59s
epoch 93 | loss: 0.24292 | val_0_rmse: 0.50867 | val_1_rmse: 0.5122  |  0:06:02s
epoch 94 | loss: 0.24292 | val_0_rmse: 0.5348  | val_1_rmse: 0.5389  |  0:06:06s
epoch 95 | loss: 0.24155 | val_0_rmse: 0.51467 | val_1_rmse: 0.51891 |  0:06:10s
epoch 96 | loss: 0.24007 | val_0_rmse: 0.52459 | val_1_rmse: 0.52825 |  0:06:14s
epoch 97 | loss: 0.24068 | val_0_rmse: 0.50726 | val_1_rmse: 0.51012 |  0:06:18s
epoch 98 | loss: 0.24247 | val_0_rmse: 0.54717 | val_1_rmse: 0.55034 |  0:06:22s
epoch 99 | loss: 0.23985 | val_0_rmse: 0.48772 | val_1_rmse: 0.49065 |  0:06:26s
epoch 100| loss: 0.23823 | val_0_rmse: 0.53123 | val_1_rmse: 0.53742 |  0:06:29s
epoch 101| loss: 0.23901 | val_0_rmse: 0.51348 | val_1_rmse: 0.51767 |  0:06:33s
epoch 102| loss: 0.24004 | val_0_rmse: 0.56202 | val_1_rmse: 0.565   |  0:06:37s
epoch 103| loss: 0.24082 | val_0_rmse: 0.51185 | val_1_rmse: 0.51363 |  0:06:41s
epoch 104| loss: 0.24009 | val_0_rmse: 0.50507 | val_1_rmse: 0.50881 |  0:06:45s
epoch 105| loss: 0.23833 | val_0_rmse: 0.49419 | val_1_rmse: 0.49679 |  0:06:49s
epoch 106| loss: 0.23859 | val_0_rmse: 0.4833  | val_1_rmse: 0.48839 |  0:06:53s
epoch 107| loss: 0.24034 | val_0_rmse: 0.51437 | val_1_rmse: 0.51592 |  0:06:56s
epoch 108| loss: 0.23811 | val_0_rmse: 0.48985 | val_1_rmse: 0.49298 |  0:07:00s
epoch 109| loss: 0.23751 | val_0_rmse: 0.54586 | val_1_rmse: 0.54965 |  0:07:04s
epoch 110| loss: 0.23777 | val_0_rmse: 0.50064 | val_1_rmse: 0.50437 |  0:07:08s
epoch 111| loss: 0.23707 | val_0_rmse: 0.50098 | val_1_rmse: 0.50434 |  0:07:12s
epoch 112| loss: 0.23679 | val_0_rmse: 0.51515 | val_1_rmse: 0.51989 |  0:07:16s
epoch 113| loss: 0.23696 | val_0_rmse: 0.55306 | val_1_rmse: 0.55962 |  0:07:19s
epoch 114| loss: 0.23689 | val_0_rmse: 0.47938 | val_1_rmse: 0.48398 |  0:07:23s
epoch 115| loss: 0.23743 | val_0_rmse: 0.49272 | val_1_rmse: 0.49761 |  0:07:27s
epoch 116| loss: 0.23733 | val_0_rmse: 0.49717 | val_1_rmse: 0.49999 |  0:07:31s
epoch 117| loss: 0.23545 | val_0_rmse: 0.4992  | val_1_rmse: 0.48907 |  0:07:35s
epoch 118| loss: 0.23635 | val_0_rmse: 0.59525 | val_1_rmse: 0.58714 |  0:07:39s
epoch 119| loss: 0.23794 | val_0_rmse: 0.56028 | val_1_rmse: 0.56055 |  0:07:43s
epoch 120| loss: 0.23397 | val_0_rmse: 0.49857 | val_1_rmse: 0.50327 |  0:07:46s
epoch 121| loss: 0.23541 | val_0_rmse: 0.52531 | val_1_rmse: 0.53054 |  0:07:50s
epoch 122| loss: 0.23639 | val_0_rmse: 0.486   | val_1_rmse: 0.49059 |  0:07:54s
epoch 123| loss: 0.23522 | val_0_rmse: 0.54546 | val_1_rmse: 0.54807 |  0:07:58s
epoch 124| loss: 0.23498 | val_0_rmse: 0.48946 | val_1_rmse: 0.49136 |  0:08:02s
epoch 125| loss: 0.23262 | val_0_rmse: 0.58341 | val_1_rmse: 0.58207 |  0:08:06s
epoch 126| loss: 0.23361 | val_0_rmse: 0.60895 | val_1_rmse: 0.61417 |  0:08:10s
epoch 127| loss: 0.24035 | val_0_rmse: 0.57345 | val_1_rmse: 0.57715 |  0:08:13s
epoch 128| loss: 0.23428 | val_0_rmse: 0.48147 | val_1_rmse: 0.48571 |  0:08:17s
epoch 129| loss: 0.23316 | val_0_rmse: 0.50083 | val_1_rmse: 0.50592 |  0:08:21s
epoch 130| loss: 0.23415 | val_0_rmse: 0.51437 | val_1_rmse: 0.50207 |  0:08:25s
epoch 131| loss: 0.23426 | val_0_rmse: 0.52511 | val_1_rmse: 0.48662 |  0:08:29s
epoch 132| loss: 0.23507 | val_0_rmse: 0.54972 | val_1_rmse: 0.55339 |  0:08:33s
epoch 133| loss: 0.2318  | val_0_rmse: 0.60739 | val_1_rmse: 0.53047 |  0:08:37s
epoch 134| loss: 0.23431 | val_0_rmse: 0.55131 | val_1_rmse: 0.48459 |  0:08:40s
epoch 135| loss: 0.23119 | val_0_rmse: 0.52748 | val_1_rmse: 0.49    |  0:08:44s
epoch 136| loss: 0.23334 | val_0_rmse: 0.55703 | val_1_rmse: 0.513   |  0:08:48s
epoch 137| loss: 0.23188 | val_0_rmse: 0.60089 | val_1_rmse: 0.60882 |  0:08:52s
epoch 138| loss: 0.23451 | val_0_rmse: 0.59955 | val_1_rmse: 0.6026  |  0:08:56s
epoch 139| loss: 0.23426 | val_0_rmse: 0.51423 | val_1_rmse: 0.5186  |  0:09:00s
epoch 140| loss: 0.23079 | val_0_rmse: 0.59798 | val_1_rmse: 0.60076 |  0:09:04s
epoch 141| loss: 0.23205 | val_0_rmse: 0.56124 | val_1_rmse: 0.56655 |  0:09:07s
epoch 142| loss: 0.23192 | val_0_rmse: 0.59009 | val_1_rmse: 0.5828  |  0:09:11s
epoch 143| loss: 0.23121 | val_0_rmse: 0.6105  | val_1_rmse: 0.61716 |  0:09:15s
epoch 144| loss: 0.23364 | val_0_rmse: 0.51083 | val_1_rmse: 0.51585 |  0:09:19s

Early stopping occured at epoch 144 with best_epoch = 114 and best_val_1_rmse = 0.48398
Best weights from best epoch are automatically used!
ended training at: 07:44:51
Feature importance:
[('Area', 0.6052665723837989), ('Baths', 0.040573663542802), ('Beds', 0.16388163717677282), ('Latitude', 0.040635505386878684), ('Longitude', 0.14236620005392803), ('Month', 0.007276421455819564), ('Year', 0.0)]
Mean squared error is of 908172452.1973085
Mean absolute error:19708.034842910718
MAPE:0.2825033262776412
R2 score:0.7341124034485129
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:44:51
epoch 0  | loss: 0.38962 | val_0_rmse: 0.55063 | val_1_rmse: 0.55639 |  0:00:03s
epoch 1  | loss: 0.30282 | val_0_rmse: 0.53675 | val_1_rmse: 0.54095 |  0:00:07s
epoch 2  | loss: 0.29733 | val_0_rmse: 0.54043 | val_1_rmse: 0.54464 |  0:00:11s
epoch 3  | loss: 0.29884 | val_0_rmse: 0.53284 | val_1_rmse: 0.53967 |  0:00:15s
epoch 4  | loss: 0.2896  | val_0_rmse: 0.52787 | val_1_rmse: 0.53311 |  0:00:19s
epoch 5  | loss: 0.28727 | val_0_rmse: 0.52678 | val_1_rmse: 0.53203 |  0:00:23s
epoch 6  | loss: 0.28562 | val_0_rmse: 0.52832 | val_1_rmse: 0.53408 |  0:00:27s
epoch 7  | loss: 0.28612 | val_0_rmse: 0.52358 | val_1_rmse: 0.5294  |  0:00:30s
epoch 8  | loss: 0.28175 | val_0_rmse: 0.52189 | val_1_rmse: 0.52782 |  0:00:34s
epoch 9  | loss: 0.27847 | val_0_rmse: 0.53951 | val_1_rmse: 0.54409 |  0:00:38s
epoch 10 | loss: 0.27464 | val_0_rmse: 0.5342  | val_1_rmse: 0.54064 |  0:00:42s
epoch 11 | loss: 0.27012 | val_0_rmse: 0.6109  | val_1_rmse: 0.6146  |  0:00:46s
epoch 12 | loss: 0.26763 | val_0_rmse: 0.50991 | val_1_rmse: 0.51786 |  0:00:50s
epoch 13 | loss: 0.26598 | val_0_rmse: 0.52169 | val_1_rmse: 0.52749 |  0:00:53s
epoch 14 | loss: 0.26767 | val_0_rmse: 0.52598 | val_1_rmse: 0.5335  |  0:00:57s
epoch 15 | loss: 0.26372 | val_0_rmse: 0.51593 | val_1_rmse: 0.52334 |  0:01:01s
epoch 16 | loss: 0.26442 | val_0_rmse: 0.53595 | val_1_rmse: 0.5424  |  0:01:05s
epoch 17 | loss: 0.26164 | val_0_rmse: 0.55175 | val_1_rmse: 0.55799 |  0:01:09s
epoch 18 | loss: 0.26304 | val_0_rmse: 0.56336 | val_1_rmse: 0.56808 |  0:01:13s
epoch 19 | loss: 0.2648  | val_0_rmse: 0.52684 | val_1_rmse: 0.5349  |  0:01:17s
epoch 20 | loss: 0.26221 | val_0_rmse: 0.507   | val_1_rmse: 0.5154  |  0:01:20s
epoch 21 | loss: 0.26046 | val_0_rmse: 0.71189 | val_1_rmse: 0.71806 |  0:01:24s
epoch 22 | loss: 0.26074 | val_0_rmse: 0.52042 | val_1_rmse: 0.52725 |  0:01:28s
epoch 23 | loss: 0.25998 | val_0_rmse: 0.53038 | val_1_rmse: 0.53533 |  0:01:32s
epoch 24 | loss: 0.26133 | val_0_rmse: 0.5666  | val_1_rmse: 0.57576 |  0:01:36s
epoch 25 | loss: 0.25991 | val_0_rmse: 0.5161  | val_1_rmse: 0.52284 |  0:01:40s
epoch 26 | loss: 0.25929 | val_0_rmse: 0.50818 | val_1_rmse: 0.51682 |  0:01:44s
epoch 27 | loss: 0.25729 | val_0_rmse: 0.5058  | val_1_rmse: 0.51414 |  0:01:47s
epoch 28 | loss: 0.25922 | val_0_rmse: 0.50404 | val_1_rmse: 0.5128  |  0:01:51s
epoch 29 | loss: 0.25934 | val_0_rmse: 0.55697 | val_1_rmse: 0.56241 |  0:01:55s
epoch 30 | loss: 0.25766 | val_0_rmse: 0.52764 | val_1_rmse: 0.53505 |  0:01:59s
epoch 31 | loss: 0.25555 | val_0_rmse: 0.5192  | val_1_rmse: 0.5269  |  0:02:03s
epoch 32 | loss: 0.25419 | val_0_rmse: 0.49291 | val_1_rmse: 0.49996 |  0:02:07s
epoch 33 | loss: 0.25415 | val_0_rmse: 0.55958 | val_1_rmse: 0.56607 |  0:02:11s
epoch 34 | loss: 0.25334 | val_0_rmse: 0.53663 | val_1_rmse: 0.54462 |  0:02:14s
epoch 35 | loss: 0.25346 | val_0_rmse: 0.49787 | val_1_rmse: 0.5039  |  0:02:18s
epoch 36 | loss: 0.25402 | val_0_rmse: 0.58836 | val_1_rmse: 0.59218 |  0:02:22s
epoch 37 | loss: 0.25749 | val_0_rmse: 0.50839 | val_1_rmse: 0.51574 |  0:02:26s
epoch 38 | loss: 0.2533  | val_0_rmse: 0.49539 | val_1_rmse: 0.50406 |  0:02:30s
epoch 39 | loss: 0.25516 | val_0_rmse: 0.50094 | val_1_rmse: 0.50745 |  0:02:34s
epoch 40 | loss: 0.25204 | val_0_rmse: 0.49916 | val_1_rmse: 0.50464 |  0:02:38s
epoch 41 | loss: 0.25376 | val_0_rmse: 0.56169 | val_1_rmse: 0.56821 |  0:02:41s
epoch 42 | loss: 0.25231 | val_0_rmse: 0.5116  | val_1_rmse: 0.51816 |  0:02:45s
epoch 43 | loss: 0.25265 | val_0_rmse: 0.57969 | val_1_rmse: 0.58273 |  0:02:49s
epoch 44 | loss: 0.25175 | val_0_rmse: 0.66639 | val_1_rmse: 0.66936 |  0:02:53s
epoch 45 | loss: 0.25272 | val_0_rmse: 0.54821 | val_1_rmse: 0.55228 |  0:02:57s
epoch 46 | loss: 0.24995 | val_0_rmse: 0.58121 | val_1_rmse: 0.58803 |  0:03:01s
epoch 47 | loss: 0.24854 | val_0_rmse: 0.85582 | val_1_rmse: 0.8585  |  0:03:05s
epoch 48 | loss: 0.24883 | val_0_rmse: 0.48878 | val_1_rmse: 0.49605 |  0:03:08s
epoch 49 | loss: 0.24823 | val_0_rmse: 0.54049 | val_1_rmse: 0.54667 |  0:03:12s
epoch 50 | loss: 0.24787 | val_0_rmse: 0.50508 | val_1_rmse: 0.51247 |  0:03:16s
epoch 51 | loss: 0.24369 | val_0_rmse: 0.60137 | val_1_rmse: 0.60557 |  0:03:20s
epoch 52 | loss: 0.245   | val_0_rmse: 0.54118 | val_1_rmse: 0.54918 |  0:03:24s
epoch 53 | loss: 0.2463  | val_0_rmse: 0.50654 | val_1_rmse: 0.51401 |  0:03:28s
epoch 54 | loss: 0.24743 | val_0_rmse: 0.49718 | val_1_rmse: 0.50407 |  0:03:32s
epoch 55 | loss: 0.24517 | val_0_rmse: 0.52259 | val_1_rmse: 0.52667 |  0:03:35s
epoch 56 | loss: 0.24354 | val_0_rmse: 0.49148 | val_1_rmse: 0.49633 |  0:03:39s
epoch 57 | loss: 0.24263 | val_0_rmse: 0.55885 | val_1_rmse: 0.56417 |  0:03:43s
epoch 58 | loss: 0.24322 | val_0_rmse: 0.48983 | val_1_rmse: 0.49834 |  0:03:47s
epoch 59 | loss: 0.24296 | val_0_rmse: 0.52317 | val_1_rmse: 0.53048 |  0:03:51s
epoch 60 | loss: 0.2426  | val_0_rmse: 0.49739 | val_1_rmse: 0.50417 |  0:03:55s
epoch 61 | loss: 0.24457 | val_0_rmse: 0.49324 | val_1_rmse: 0.49787 |  0:03:59s
epoch 62 | loss: 0.23955 | val_0_rmse: 0.66443 | val_1_rmse: 0.67153 |  0:04:02s
epoch 63 | loss: 0.23901 | val_0_rmse: 0.54765 | val_1_rmse: 0.55469 |  0:04:06s
epoch 64 | loss: 0.24016 | val_0_rmse: 0.5078  | val_1_rmse: 0.51365 |  0:04:10s
epoch 65 | loss: 0.23894 | val_0_rmse: 0.49463 | val_1_rmse: 0.50407 |  0:04:14s
epoch 66 | loss: 0.2382  | val_0_rmse: 0.50358 | val_1_rmse: 0.50892 |  0:04:18s
epoch 67 | loss: 0.23838 | val_0_rmse: 0.52146 | val_1_rmse: 0.52764 |  0:04:22s
epoch 68 | loss: 0.23763 | val_0_rmse: 0.49456 | val_1_rmse: 0.50161 |  0:04:26s
epoch 69 | loss: 0.23535 | val_0_rmse: 0.55583 | val_1_rmse: 0.56166 |  0:04:29s
epoch 70 | loss: 0.23972 | val_0_rmse: 0.49067 | val_1_rmse: 0.49461 |  0:04:33s
epoch 71 | loss: 0.23626 | val_0_rmse: 0.57648 | val_1_rmse: 0.58409 |  0:04:37s
epoch 72 | loss: 0.23566 | val_0_rmse: 0.58476 | val_1_rmse: 0.58926 |  0:04:41s
epoch 73 | loss: 0.23507 | val_0_rmse: 0.53929 | val_1_rmse: 0.54799 |  0:04:45s
epoch 74 | loss: 0.23348 | val_0_rmse: 0.52526 | val_1_rmse: 0.53203 |  0:04:49s
epoch 75 | loss: 0.23799 | val_0_rmse: 0.53205 | val_1_rmse: 0.53768 |  0:04:53s
epoch 76 | loss: 0.23411 | val_0_rmse: 0.52256 | val_1_rmse: 0.52921 |  0:04:56s
epoch 77 | loss: 0.23391 | val_0_rmse: 0.99107 | val_1_rmse: 1.00226 |  0:05:00s
epoch 78 | loss: 0.23351 | val_0_rmse: 0.68711 | val_1_rmse: 0.69342 |  0:05:04s
epoch 79 | loss: 0.23256 | val_0_rmse: 0.52672 | val_1_rmse: 0.53432 |  0:05:08s
epoch 80 | loss: 0.23261 | val_0_rmse: 0.51329 | val_1_rmse: 0.52214 |  0:05:12s
epoch 81 | loss: 0.23199 | val_0_rmse: 0.54668 | val_1_rmse: 0.55277 |  0:05:16s
epoch 82 | loss: 0.233   | val_0_rmse: 0.48267 | val_1_rmse: 0.48947 |  0:05:19s
epoch 83 | loss: 0.23276 | val_0_rmse: 0.52061 | val_1_rmse: 0.52737 |  0:05:23s
epoch 84 | loss: 0.23096 | val_0_rmse: 0.67786 | val_1_rmse: 0.68007 |  0:05:27s
epoch 85 | loss: 0.23061 | val_0_rmse: 0.51205 | val_1_rmse: 0.52016 |  0:05:31s
epoch 86 | loss: 0.2299  | val_0_rmse: 0.50724 | val_1_rmse: 0.51052 |  0:05:35s
epoch 87 | loss: 0.23063 | val_0_rmse: 0.48379 | val_1_rmse: 0.49195 |  0:05:39s
epoch 88 | loss: 0.22972 | val_0_rmse: 0.52097 | val_1_rmse: 0.52675 |  0:05:43s
epoch 89 | loss: 0.23204 | val_0_rmse: 0.50312 | val_1_rmse: 0.51059 |  0:05:47s
epoch 90 | loss: 0.22998 | val_0_rmse: 0.63833 | val_1_rmse: 0.64674 |  0:05:50s
epoch 91 | loss: 0.23009 | val_0_rmse: 0.48663 | val_1_rmse: 0.4924  |  0:05:54s
epoch 92 | loss: 0.23901 | val_0_rmse: 0.55165 | val_1_rmse: 0.51058 |  0:05:58s
epoch 93 | loss: 0.24006 | val_0_rmse: 0.52977 | val_1_rmse: 0.51977 |  0:06:02s
epoch 94 | loss: 0.23839 | val_0_rmse: 0.54622 | val_1_rmse: 0.55253 |  0:06:06s
epoch 95 | loss: 0.23385 | val_0_rmse: 0.50184 | val_1_rmse: 0.51032 |  0:06:10s
epoch 96 | loss: 0.23109 | val_0_rmse: 0.53836 | val_1_rmse: 0.5478  |  0:06:13s
epoch 97 | loss: 0.23134 | val_0_rmse: 0.48891 | val_1_rmse: 0.49765 |  0:06:17s
epoch 98 | loss: 0.2299  | val_0_rmse: 0.50076 | val_1_rmse: 0.51067 |  0:06:21s
epoch 99 | loss: 0.227   | val_0_rmse: 0.47504 | val_1_rmse: 0.48176 |  0:06:25s
epoch 100| loss: 0.2289  | val_0_rmse: 0.48854 | val_1_rmse: 0.4945  |  0:06:29s
epoch 101| loss: 0.2278  | val_0_rmse: 0.47566 | val_1_rmse: 0.48204 |  0:06:33s
epoch 102| loss: 0.22993 | val_0_rmse: 0.48094 | val_1_rmse: 0.48763 |  0:06:37s
epoch 103| loss: 0.22826 | val_0_rmse: 0.47973 | val_1_rmse: 0.48724 |  0:06:41s
epoch 104| loss: 0.2268  | val_0_rmse: 0.51443 | val_1_rmse: 0.52251 |  0:06:44s
epoch 105| loss: 0.22644 | val_0_rmse: 0.52845 | val_1_rmse: 0.53209 |  0:06:48s
epoch 106| loss: 0.22692 | val_0_rmse: 0.49941 | val_1_rmse: 0.50841 |  0:06:52s
epoch 107| loss: 0.22381 | val_0_rmse: 0.51917 | val_1_rmse: 0.52557 |  0:06:56s
epoch 108| loss: 0.2251  | val_0_rmse: 0.51169 | val_1_rmse: 0.52121 |  0:07:00s
epoch 109| loss: 0.22638 | val_0_rmse: 0.50116 | val_1_rmse: 0.50818 |  0:07:04s
epoch 110| loss: 0.22361 | val_0_rmse: 0.51531 | val_1_rmse: 0.52281 |  0:07:08s
epoch 111| loss: 0.22271 | val_0_rmse: 0.4795  | val_1_rmse: 0.48769 |  0:07:11s
epoch 112| loss: 0.22752 | val_0_rmse: 0.5324  | val_1_rmse: 0.53839 |  0:07:15s
epoch 113| loss: 0.2231  | val_0_rmse: 0.52967 | val_1_rmse: 0.53547 |  0:07:19s
epoch 114| loss: 0.2222  | val_0_rmse: 0.53754 | val_1_rmse: 0.54586 |  0:07:23s
epoch 115| loss: 0.22241 | val_0_rmse: 0.49962 | val_1_rmse: 0.50963 |  0:07:27s
epoch 116| loss: 0.22212 | val_0_rmse: 0.59706 | val_1_rmse: 0.60542 |  0:07:31s
epoch 117| loss: 0.2237  | val_0_rmse: 0.55462 | val_1_rmse: 0.56134 |  0:07:35s
epoch 118| loss: 0.22316 | val_0_rmse: 0.52835 | val_1_rmse: 0.5341  |  0:07:38s
epoch 119| loss: 0.22201 | val_0_rmse: 0.54571 | val_1_rmse: 0.55342 |  0:07:42s
epoch 120| loss: 0.22552 | val_0_rmse: 0.51636 | val_1_rmse: 0.52075 |  0:07:46s
epoch 121| loss: 0.22263 | val_0_rmse: 0.47993 | val_1_rmse: 0.48844 |  0:07:50s
epoch 122| loss: 0.22271 | val_0_rmse: 0.48226 | val_1_rmse: 0.4901  |  0:07:54s
epoch 123| loss: 0.22371 | val_0_rmse: 0.49994 | val_1_rmse: 0.50956 |  0:07:58s
epoch 124| loss: 0.22237 | val_0_rmse: 0.50594 | val_1_rmse: 0.51017 |  0:08:01s
epoch 125| loss: 0.21956 | val_0_rmse: 0.49608 | val_1_rmse: 0.50529 |  0:08:05s
epoch 126| loss: 0.22167 | val_0_rmse: 0.53447 | val_1_rmse: 0.54358 |  0:08:09s
epoch 127| loss: 0.22859 | val_0_rmse: 0.57002 | val_1_rmse: 0.58022 |  0:08:13s
epoch 128| loss: 0.22906 | val_0_rmse: 0.55456 | val_1_rmse: 0.56191 |  0:08:17s
epoch 129| loss: 0.23357 | val_0_rmse: 0.57529 | val_1_rmse: 0.58344 |  0:08:21s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.48176
Best weights from best epoch are automatically used!
ended training at: 07:53:14
Feature importance:
[('Area', 0.5401656305600347), ('Baths', 0.034295779159277166), ('Beds', 0.15209828983430546), ('Latitude', 0.17907452973638843), ('Longitude', 0.09436577070999423), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 909020249.0456876
Mean absolute error:19977.726679339794
MAPE:0.2878466313742133
R2 score:0.7287088492916715
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 07:53:14
epoch 0  | loss: 0.39573 | val_0_rmse: 0.56269 | val_1_rmse: 0.56364 |  0:00:03s
epoch 1  | loss: 0.30639 | val_0_rmse: 0.53883 | val_1_rmse: 0.53959 |  0:00:07s
epoch 2  | loss: 0.30366 | val_0_rmse: 0.54783 | val_1_rmse: 0.54969 |  0:00:11s
epoch 3  | loss: 0.303   | val_0_rmse: 0.53745 | val_1_rmse: 0.53935 |  0:00:15s
epoch 4  | loss: 0.29007 | val_0_rmse: 0.52975 | val_1_rmse: 0.53204 |  0:00:19s
epoch 5  | loss: 0.28787 | val_0_rmse: 0.52874 | val_1_rmse: 0.5303  |  0:00:23s
epoch 6  | loss: 0.28744 | val_0_rmse: 0.5271  | val_1_rmse: 0.52734 |  0:00:27s
epoch 7  | loss: 0.28698 | val_0_rmse: 0.53089 | val_1_rmse: 0.53362 |  0:00:30s
epoch 8  | loss: 0.28245 | val_0_rmse: 0.51997 | val_1_rmse: 0.52238 |  0:00:34s
epoch 9  | loss: 0.28268 | val_0_rmse: 0.53225 | val_1_rmse: 0.53332 |  0:00:38s
epoch 10 | loss: 0.28069 | val_0_rmse: 0.51861 | val_1_rmse: 0.52191 |  0:00:42s
epoch 11 | loss: 0.27943 | val_0_rmse: 0.5214  | val_1_rmse: 0.52455 |  0:00:46s
epoch 12 | loss: 0.27944 | val_0_rmse: 0.52293 | val_1_rmse: 0.52466 |  0:00:50s
epoch 13 | loss: 0.2811  | val_0_rmse: 0.52855 | val_1_rmse: 0.52996 |  0:00:54s
epoch 14 | loss: 0.2819  | val_0_rmse: 0.51942 | val_1_rmse: 0.52154 |  0:00:57s
epoch 15 | loss: 0.27603 | val_0_rmse: 0.51368 | val_1_rmse: 0.51516 |  0:01:01s
epoch 16 | loss: 0.26986 | val_0_rmse: 0.51611 | val_1_rmse: 0.51761 |  0:01:05s
epoch 17 | loss: 0.26767 | val_0_rmse: 0.57588 | val_1_rmse: 0.57708 |  0:01:09s
epoch 18 | loss: 0.27169 | val_0_rmse: 0.53183 | val_1_rmse: 0.53255 |  0:01:13s
epoch 19 | loss: 0.27306 | val_0_rmse: 0.5393  | val_1_rmse: 0.54047 |  0:01:17s
epoch 20 | loss: 0.27133 | val_0_rmse: 0.51006 | val_1_rmse: 0.51122 |  0:01:20s
epoch 21 | loss: 0.26971 | val_0_rmse: 0.53742 | val_1_rmse: 0.53891 |  0:01:24s
epoch 22 | loss: 0.26884 | val_0_rmse: 0.52722 | val_1_rmse: 0.52839 |  0:01:28s
epoch 23 | loss: 0.2711  | val_0_rmse: 0.55149 | val_1_rmse: 0.55177 |  0:01:32s
epoch 24 | loss: 0.27106 | val_0_rmse: 0.51227 | val_1_rmse: 0.51322 |  0:01:36s
epoch 25 | loss: 0.26563 | val_0_rmse: 0.52689 | val_1_rmse: 0.53071 |  0:01:40s
epoch 26 | loss: 0.26635 | val_0_rmse: 0.51802 | val_1_rmse: 0.52003 |  0:01:44s
epoch 27 | loss: 0.26704 | val_0_rmse: 0.51302 | val_1_rmse: 0.51383 |  0:01:47s
epoch 28 | loss: 0.26636 | val_0_rmse: 0.5191  | val_1_rmse: 0.51866 |  0:01:51s
epoch 29 | loss: 0.2716  | val_0_rmse: 0.5239  | val_1_rmse: 0.52488 |  0:01:55s
epoch 30 | loss: 0.27447 | val_0_rmse: 0.53131 | val_1_rmse: 0.53076 |  0:01:59s
epoch 31 | loss: 0.27447 | val_0_rmse: 0.54073 | val_1_rmse: 0.53958 |  0:02:03s
epoch 32 | loss: 0.27017 | val_0_rmse: 0.51332 | val_1_rmse: 0.5103  |  0:02:07s
epoch 33 | loss: 0.26782 | val_0_rmse: 0.54145 | val_1_rmse: 0.54103 |  0:02:11s
epoch 34 | loss: 0.2703  | val_0_rmse: 0.51598 | val_1_rmse: 0.51334 |  0:02:14s
epoch 35 | loss: 0.26613 | val_0_rmse: 0.51504 | val_1_rmse: 0.51319 |  0:02:18s
epoch 36 | loss: 0.26973 | val_0_rmse: 0.52296 | val_1_rmse: 0.52155 |  0:02:22s
epoch 37 | loss: 0.26674 | val_0_rmse: 0.51188 | val_1_rmse: 0.51188 |  0:02:26s
epoch 38 | loss: 0.26449 | val_0_rmse: 0.6218  | val_1_rmse: 0.62216 |  0:02:30s
epoch 39 | loss: 0.26722 | val_0_rmse: 0.56091 | val_1_rmse: 0.55921 |  0:02:34s
epoch 40 | loss: 0.27337 | val_0_rmse: 0.53748 | val_1_rmse: 0.53705 |  0:02:38s
epoch 41 | loss: 0.27133 | val_0_rmse: 0.58646 | val_1_rmse: 0.58565 |  0:02:41s
epoch 42 | loss: 0.27354 | val_0_rmse: 0.54611 | val_1_rmse: 0.54506 |  0:02:45s
epoch 43 | loss: 0.27082 | val_0_rmse: 0.51514 | val_1_rmse: 0.51566 |  0:02:49s
epoch 44 | loss: 0.26987 | val_0_rmse: 0.51895 | val_1_rmse: 0.51787 |  0:02:53s
epoch 45 | loss: 0.27005 | val_0_rmse: 0.51533 | val_1_rmse: 0.51524 |  0:02:57s
epoch 46 | loss: 0.26769 | val_0_rmse: 0.53165 | val_1_rmse: 0.53073 |  0:03:01s
epoch 47 | loss: 0.2677  | val_0_rmse: 0.52252 | val_1_rmse: 0.52183 |  0:03:04s
epoch 48 | loss: 0.26865 | val_0_rmse: 0.51963 | val_1_rmse: 0.52044 |  0:03:08s
epoch 49 | loss: 0.26937 | val_0_rmse: 0.63245 | val_1_rmse: 0.6332  |  0:03:12s
epoch 50 | loss: 0.26733 | val_0_rmse: 0.52997 | val_1_rmse: 0.52951 |  0:03:16s
epoch 51 | loss: 0.26554 | val_0_rmse: 0.58705 | val_1_rmse: 0.58861 |  0:03:20s
epoch 52 | loss: 0.26368 | val_0_rmse: 0.51043 | val_1_rmse: 0.50961 |  0:03:24s
epoch 53 | loss: 0.26279 | val_0_rmse: 0.50593 | val_1_rmse: 0.50425 |  0:03:28s
epoch 54 | loss: 0.26021 | val_0_rmse: 0.51829 | val_1_rmse: 0.51852 |  0:03:31s
epoch 55 | loss: 0.25986 | val_0_rmse: 0.51047 | val_1_rmse: 0.50938 |  0:03:35s
epoch 56 | loss: 0.26046 | val_0_rmse: 0.53339 | val_1_rmse: 0.5348  |  0:03:39s
epoch 57 | loss: 0.25955 | val_0_rmse: 0.54584 | val_1_rmse: 0.5468  |  0:03:43s
epoch 58 | loss: 0.26074 | val_0_rmse: 0.51249 | val_1_rmse: 0.51205 |  0:03:47s
epoch 59 | loss: 0.25861 | val_0_rmse: 0.61758 | val_1_rmse: 0.61994 |  0:03:51s
epoch 60 | loss: 0.25804 | val_0_rmse: 0.5701  | val_1_rmse: 0.56989 |  0:03:55s
epoch 61 | loss: 0.2571  | val_0_rmse: 0.54355 | val_1_rmse: 0.54619 |  0:03:58s
epoch 62 | loss: 0.25614 | val_0_rmse: 0.52596 | val_1_rmse: 0.52765 |  0:04:02s
epoch 63 | loss: 0.25769 | val_0_rmse: 0.51808 | val_1_rmse: 0.52076 |  0:04:06s
epoch 64 | loss: 0.26564 | val_0_rmse: 0.58715 | val_1_rmse: 0.58726 |  0:04:10s
epoch 65 | loss: 0.2641  | val_0_rmse: 0.50665 | val_1_rmse: 0.50506 |  0:04:14s
epoch 66 | loss: 0.26102 | val_0_rmse: 0.51034 | val_1_rmse: 0.51065 |  0:04:18s
epoch 67 | loss: 0.26144 | val_0_rmse: 0.56541 | val_1_rmse: 0.56696 |  0:04:21s
epoch 68 | loss: 0.25688 | val_0_rmse: 0.53993 | val_1_rmse: 0.5409  |  0:04:25s
epoch 69 | loss: 0.25805 | val_0_rmse: 0.54482 | val_1_rmse: 0.54725 |  0:04:29s
epoch 70 | loss: 0.25778 | val_0_rmse: 0.50772 | val_1_rmse: 0.50872 |  0:04:33s
epoch 71 | loss: 0.25599 | val_0_rmse: 0.51686 | val_1_rmse: 0.51852 |  0:04:37s
epoch 72 | loss: 0.25644 | val_0_rmse: 0.53049 | val_1_rmse: 0.53259 |  0:04:41s
epoch 73 | loss: 0.25687 | val_0_rmse: 0.61787 | val_1_rmse: 0.61974 |  0:04:45s
epoch 74 | loss: 0.25664 | val_0_rmse: 0.50745 | val_1_rmse: 0.50865 |  0:04:48s
epoch 75 | loss: 0.25564 | val_0_rmse: 0.5019  | val_1_rmse: 0.50085 |  0:04:52s
epoch 76 | loss: 0.25438 | val_0_rmse: 0.51825 | val_1_rmse: 0.51949 |  0:04:56s
epoch 77 | loss: 0.25523 | val_0_rmse: 0.50774 | val_1_rmse: 0.50996 |  0:05:00s
epoch 78 | loss: 0.25581 | val_0_rmse: 0.53557 | val_1_rmse: 0.53949 |  0:05:04s
epoch 79 | loss: 0.25607 | val_0_rmse: 0.57357 | val_1_rmse: 0.57506 |  0:05:08s
epoch 80 | loss: 0.25487 | val_0_rmse: 0.51824 | val_1_rmse: 0.51929 |  0:05:12s
epoch 81 | loss: 0.25398 | val_0_rmse: 0.5686  | val_1_rmse: 0.56977 |  0:05:15s
epoch 82 | loss: 0.25389 | val_0_rmse: 0.55053 | val_1_rmse: 0.55089 |  0:05:19s
epoch 83 | loss: 0.25603 | val_0_rmse: 0.56731 | val_1_rmse: 0.57071 |  0:05:23s
epoch 84 | loss: 0.25621 | val_0_rmse: 0.51749 | val_1_rmse: 0.51957 |  0:05:27s
epoch 85 | loss: 0.25646 | val_0_rmse: 0.53253 | val_1_rmse: 0.53468 |  0:05:31s
epoch 86 | loss: 0.25649 | val_0_rmse: 0.52718 | val_1_rmse: 0.52898 |  0:05:35s
epoch 87 | loss: 0.2537  | val_0_rmse: 0.52498 | val_1_rmse: 0.5267  |  0:05:39s
epoch 88 | loss: 0.25628 | val_0_rmse: 0.61326 | val_1_rmse: 0.61622 |  0:05:42s
epoch 89 | loss: 0.25192 | val_0_rmse: 0.50298 | val_1_rmse: 0.50458 |  0:05:46s
epoch 90 | loss: 0.25266 | val_0_rmse: 0.50241 | val_1_rmse: 0.5011  |  0:05:50s
epoch 91 | loss: 0.25363 | val_0_rmse: 0.5185  | val_1_rmse: 0.5205  |  0:05:54s
epoch 92 | loss: 0.25435 | val_0_rmse: 0.49884 | val_1_rmse: 0.50034 |  0:05:58s
epoch 93 | loss: 0.25311 | val_0_rmse: 0.86284 | val_1_rmse: 0.87097 |  0:06:02s
epoch 94 | loss: 0.25336 | val_0_rmse: 0.53096 | val_1_rmse: 0.53228 |  0:06:05s
epoch 95 | loss: 0.25285 | val_0_rmse: 0.66295 | val_1_rmse: 0.66484 |  0:06:09s
epoch 96 | loss: 0.25855 | val_0_rmse: 0.65832 | val_1_rmse: 0.66142 |  0:06:13s
epoch 97 | loss: 0.25931 | val_0_rmse: 0.57881 | val_1_rmse: 0.57992 |  0:06:17s
epoch 98 | loss: 0.2577  | val_0_rmse: 0.5755  | val_1_rmse: 0.57835 |  0:06:21s
epoch 99 | loss: 0.25654 | val_0_rmse: 0.55419 | val_1_rmse: 0.5566  |  0:06:25s
epoch 100| loss: 0.25616 | val_0_rmse: 0.5133  | val_1_rmse: 0.5145  |  0:06:29s
epoch 101| loss: 0.25882 | val_0_rmse: 0.59476 | val_1_rmse: 0.59723 |  0:06:32s
epoch 102| loss: 0.2554  | val_0_rmse: 0.65205 | val_1_rmse: 0.65514 |  0:06:36s
epoch 103| loss: 0.25786 | val_0_rmse: 0.57869 | val_1_rmse: 0.57946 |  0:06:40s
epoch 104| loss: 0.25794 | val_0_rmse: 0.64705 | val_1_rmse: 0.65139 |  0:06:44s
epoch 105| loss: 0.25573 | val_0_rmse: 0.5147  | val_1_rmse: 0.51653 |  0:06:48s
epoch 106| loss: 0.25577 | val_0_rmse: 0.51231 | val_1_rmse: 0.51326 |  0:06:52s
epoch 107| loss: 0.25535 | val_0_rmse: 0.50995 | val_1_rmse: 0.51086 |  0:06:56s
epoch 108| loss: 0.2535  | val_0_rmse: 0.57418 | val_1_rmse: 0.57762 |  0:06:59s
epoch 109| loss: 0.25464 | val_0_rmse: 0.51238 | val_1_rmse: 0.51227 |  0:07:03s
epoch 110| loss: 0.25386 | val_0_rmse: 0.73722 | val_1_rmse: 0.74057 |  0:07:07s
epoch 111| loss: 0.25441 | val_0_rmse: 0.51382 | val_1_rmse: 0.5137  |  0:07:11s
epoch 112| loss: 0.2556  | val_0_rmse: 0.69773 | val_1_rmse: 0.69836 |  0:07:15s
epoch 113| loss: 0.25529 | val_0_rmse: 0.50966 | val_1_rmse: 0.51071 |  0:07:19s
epoch 114| loss: 0.25728 | val_0_rmse: 0.7152  | val_1_rmse: 0.71681 |  0:07:22s
epoch 115| loss: 0.25887 | val_0_rmse: 0.53179 | val_1_rmse: 0.53217 |  0:07:26s
epoch 116| loss: 0.25842 | val_0_rmse: 0.50994 | val_1_rmse: 0.51009 |  0:07:30s
epoch 117| loss: 0.25702 | val_0_rmse: 0.51159 | val_1_rmse: 0.51241 |  0:07:34s
epoch 118| loss: 0.2578  | val_0_rmse: 0.516   | val_1_rmse: 0.51658 |  0:07:38s
epoch 119| loss: 0.25493 | val_0_rmse: 0.50294 | val_1_rmse: 0.50215 |  0:07:42s
epoch 120| loss: 0.25636 | val_0_rmse: 0.84291 | val_1_rmse: 0.84832 |  0:07:46s
epoch 121| loss: 0.25761 | val_0_rmse: 0.60065 | val_1_rmse: 0.60124 |  0:07:49s
epoch 122| loss: 0.2563  | val_0_rmse: 0.65242 | val_1_rmse: 0.65478 |  0:07:53s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.50034
Best weights from best epoch are automatically used!
ended training at: 08:01:09
Feature importance:
[('Area', 0.5706510747385775), ('Baths', 0.0), ('Beds', 0.17954117533582029), ('Latitude', 0.0512407467017077), ('Longitude', 0.17884379829921418), ('Month', 0.019723204924680392), ('Year', 0.0)]
Mean squared error is of 1009568630.2069495
Mean absolute error:20915.966903991135
MAPE:0.30207357401162155
R2 score:0.7002009979021319
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:01:09
epoch 0  | loss: 0.39739 | val_0_rmse: 0.54097 | val_1_rmse: 0.54282 |  0:00:03s
epoch 1  | loss: 0.3013  | val_0_rmse: 0.53855 | val_1_rmse: 0.53978 |  0:00:07s
epoch 2  | loss: 0.2939  | val_0_rmse: 0.52725 | val_1_rmse: 0.52835 |  0:00:11s
epoch 3  | loss: 0.29148 | val_0_rmse: 0.53437 | val_1_rmse: 0.5359  |  0:00:15s
epoch 4  | loss: 0.29646 | val_0_rmse: 0.59098 | val_1_rmse: 0.58783 |  0:00:19s
epoch 5  | loss: 0.29013 | val_0_rmse: 0.53833 | val_1_rmse: 0.53794 |  0:00:23s
epoch 6  | loss: 0.28143 | val_0_rmse: 0.5205  | val_1_rmse: 0.52184 |  0:00:27s
epoch 7  | loss: 0.28093 | val_0_rmse: 0.52298 | val_1_rmse: 0.52427 |  0:00:30s
epoch 8  | loss: 0.27853 | val_0_rmse: 0.54566 | val_1_rmse: 0.54721 |  0:00:34s
epoch 9  | loss: 0.27631 | val_0_rmse: 0.54069 | val_1_rmse: 0.54016 |  0:00:38s
epoch 10 | loss: 0.27422 | val_0_rmse: 0.52    | val_1_rmse: 0.52115 |  0:00:42s
epoch 11 | loss: 0.27129 | val_0_rmse: 0.51576 | val_1_rmse: 0.51559 |  0:00:46s
epoch 12 | loss: 0.26856 | val_0_rmse: 0.52724 | val_1_rmse: 0.52847 |  0:00:50s
epoch 13 | loss: 0.26725 | val_0_rmse: 0.52929 | val_1_rmse: 0.53273 |  0:00:54s
epoch 14 | loss: 0.26804 | val_0_rmse: 0.7923  | val_1_rmse: 0.78657 |  0:00:57s
epoch 15 | loss: 0.26359 | val_0_rmse: 0.57154 | val_1_rmse: 0.56838 |  0:01:01s
epoch 16 | loss: 0.26557 | val_0_rmse: 0.59482 | val_1_rmse: 0.5915  |  0:01:05s
epoch 17 | loss: 0.26353 | val_0_rmse: 0.51898 | val_1_rmse: 0.51805 |  0:01:09s
epoch 18 | loss: 0.26226 | val_0_rmse: 0.70737 | val_1_rmse: 0.70191 |  0:01:13s
epoch 19 | loss: 0.26065 | val_0_rmse: 0.79071 | val_1_rmse: 0.78565 |  0:01:17s
epoch 20 | loss: 0.26071 | val_0_rmse: 0.50223 | val_1_rmse: 0.50307 |  0:01:20s
epoch 21 | loss: 0.26075 | val_0_rmse: 0.50155 | val_1_rmse: 0.50317 |  0:01:24s
epoch 22 | loss: 0.25911 | val_0_rmse: 0.5255  | val_1_rmse: 0.52719 |  0:01:28s
epoch 23 | loss: 0.25668 | val_0_rmse: 0.49968 | val_1_rmse: 0.50121 |  0:01:32s
epoch 24 | loss: 0.25994 | val_0_rmse: 0.50255 | val_1_rmse: 0.50333 |  0:01:36s
epoch 25 | loss: 0.25885 | val_0_rmse: 0.54289 | val_1_rmse: 0.5449  |  0:01:40s
epoch 26 | loss: 0.25646 | val_0_rmse: 0.63831 | val_1_rmse: 0.63648 |  0:01:44s
epoch 27 | loss: 0.25842 | val_0_rmse: 0.50285 | val_1_rmse: 0.50357 |  0:01:47s
epoch 28 | loss: 0.25749 | val_0_rmse: 0.50126 | val_1_rmse: 0.5028  |  0:01:51s
epoch 29 | loss: 0.25686 | val_0_rmse: 0.5747  | val_1_rmse: 0.57948 |  0:01:55s
epoch 30 | loss: 0.25939 | val_0_rmse: 0.55443 | val_1_rmse: 0.55481 |  0:01:59s
epoch 31 | loss: 0.25872 | val_0_rmse: 0.50848 | val_1_rmse: 0.51136 |  0:02:03s
epoch 32 | loss: 0.25753 | val_0_rmse: 0.57047 | val_1_rmse: 0.57152 |  0:02:07s
epoch 33 | loss: 0.25408 | val_0_rmse: 0.50051 | val_1_rmse: 0.5012  |  0:02:10s
epoch 34 | loss: 0.25485 | val_0_rmse: 0.55088 | val_1_rmse: 0.55266 |  0:02:14s
epoch 35 | loss: 0.25474 | val_0_rmse: 0.59022 | val_1_rmse: 0.59262 |  0:02:18s
epoch 36 | loss: 0.25309 | val_0_rmse: 0.53675 | val_1_rmse: 0.5363  |  0:02:22s
epoch 37 | loss: 0.25842 | val_0_rmse: 0.50384 | val_1_rmse: 0.50669 |  0:02:26s
epoch 38 | loss: 0.25318 | val_0_rmse: 0.51452 | val_1_rmse: 0.51484 |  0:02:30s
epoch 39 | loss: 0.25279 | val_0_rmse: 0.50115 | val_1_rmse: 0.50339 |  0:02:34s
epoch 40 | loss: 0.25532 | val_0_rmse: 0.50067 | val_1_rmse: 0.50226 |  0:02:37s
epoch 41 | loss: 0.2538  | val_0_rmse: 0.52073 | val_1_rmse: 0.52248 |  0:02:41s
epoch 42 | loss: 0.25327 | val_0_rmse: 0.49801 | val_1_rmse: 0.49978 |  0:02:45s
epoch 43 | loss: 0.25349 | val_0_rmse: 0.53151 | val_1_rmse: 0.53278 |  0:02:49s
epoch 44 | loss: 0.25636 | val_0_rmse: 0.64541 | val_1_rmse: 0.64334 |  0:02:53s
epoch 45 | loss: 0.25405 | val_0_rmse: 0.72043 | val_1_rmse: 0.71698 |  0:02:57s
epoch 46 | loss: 0.25187 | val_0_rmse: 0.59295 | val_1_rmse: 0.59777 |  0:03:01s
epoch 47 | loss: 0.25231 | val_0_rmse: 0.49613 | val_1_rmse: 0.49742 |  0:03:04s
epoch 48 | loss: 0.25121 | val_0_rmse: 0.49809 | val_1_rmse: 0.49942 |  0:03:08s
epoch 49 | loss: 0.25235 | val_0_rmse: 0.50188 | val_1_rmse: 0.50325 |  0:03:12s
epoch 50 | loss: 0.25549 | val_0_rmse: 0.50356 | val_1_rmse: 0.50479 |  0:03:16s
epoch 51 | loss: 0.25245 | val_0_rmse: 0.5097  | val_1_rmse: 0.51284 |  0:03:20s
epoch 52 | loss: 0.25356 | val_0_rmse: 0.49404 | val_1_rmse: 0.49695 |  0:03:24s
epoch 53 | loss: 0.25116 | val_0_rmse: 0.49648 | val_1_rmse: 0.49936 |  0:03:28s
epoch 54 | loss: 0.25162 | val_0_rmse: 0.54241 | val_1_rmse: 0.54443 |  0:03:31s
epoch 55 | loss: 0.25189 | val_0_rmse: 0.55003 | val_1_rmse: 0.55209 |  0:03:35s
epoch 56 | loss: 0.25206 | val_0_rmse: 0.49336 | val_1_rmse: 0.49402 |  0:03:39s
epoch 57 | loss: 0.25249 | val_0_rmse: 0.51593 | val_1_rmse: 0.51787 |  0:03:43s
epoch 58 | loss: 0.25401 | val_0_rmse: 0.50255 | val_1_rmse: 0.50354 |  0:03:47s
epoch 59 | loss: 0.24946 | val_0_rmse: 0.54863 | val_1_rmse: 0.55085 |  0:03:51s
epoch 60 | loss: 0.24946 | val_0_rmse: 0.66018 | val_1_rmse: 0.65937 |  0:03:55s
epoch 61 | loss: 0.25042 | val_0_rmse: 0.52073 | val_1_rmse: 0.52194 |  0:03:58s
epoch 62 | loss: 0.25031 | val_0_rmse: 0.55559 | val_1_rmse: 0.56005 |  0:04:02s
epoch 63 | loss: 0.24982 | val_0_rmse: 0.49333 | val_1_rmse: 0.49581 |  0:04:06s
epoch 64 | loss: 0.25148 | val_0_rmse: 0.55691 | val_1_rmse: 0.55999 |  0:04:10s
epoch 65 | loss: 0.2506  | val_0_rmse: 0.5826  | val_1_rmse: 0.58693 |  0:04:14s
epoch 66 | loss: 0.24981 | val_0_rmse: 0.75305 | val_1_rmse: 0.7511  |  0:04:18s
epoch 67 | loss: 0.25187 | val_0_rmse: 0.49733 | val_1_rmse: 0.49862 |  0:04:22s
epoch 68 | loss: 0.24865 | val_0_rmse: 0.50611 | val_1_rmse: 0.50896 |  0:04:25s
epoch 69 | loss: 0.25128 | val_0_rmse: 0.51294 | val_1_rmse: 0.51402 |  0:04:30s
epoch 70 | loss: 0.24889 | val_0_rmse: 0.51504 | val_1_rmse: 0.51777 |  0:04:33s
epoch 71 | loss: 0.25059 | val_0_rmse: 0.65232 | val_1_rmse: 0.6512  |  0:04:37s
epoch 72 | loss: 0.24874 | val_0_rmse: 0.5483  | val_1_rmse: 0.54707 |  0:04:41s
epoch 73 | loss: 0.25003 | val_0_rmse: 0.5171  | val_1_rmse: 0.51847 |  0:04:45s
epoch 74 | loss: 0.24786 | val_0_rmse: 0.50595 | val_1_rmse: 0.50676 |  0:04:49s
epoch 75 | loss: 0.2525  | val_0_rmse: 0.51156 | val_1_rmse: 0.51392 |  0:04:53s
epoch 76 | loss: 0.2502  | val_0_rmse: 0.50049 | val_1_rmse: 0.50302 |  0:04:57s
epoch 77 | loss: 0.24859 | val_0_rmse: 0.57764 | val_1_rmse: 0.58107 |  0:05:00s
epoch 78 | loss: 0.24792 | val_0_rmse: 0.52127 | val_1_rmse: 0.52337 |  0:05:04s
epoch 79 | loss: 0.24837 | val_0_rmse: 0.5085  | val_1_rmse: 0.51145 |  0:05:08s
epoch 80 | loss: 0.24974 | val_0_rmse: 0.52175 | val_1_rmse: 0.52608 |  0:05:12s
epoch 81 | loss: 0.24751 | val_0_rmse: 0.49106 | val_1_rmse: 0.49473 |  0:05:16s
epoch 82 | loss: 0.247   | val_0_rmse: 0.5664  | val_1_rmse: 0.5686  |  0:05:20s
epoch 83 | loss: 0.24676 | val_0_rmse: 0.52676 | val_1_rmse: 0.52766 |  0:05:24s
epoch 84 | loss: 0.24668 | val_0_rmse: 0.5261  | val_1_rmse: 0.52544 |  0:05:27s
epoch 85 | loss: 0.24611 | val_0_rmse: 0.49548 | val_1_rmse: 0.49729 |  0:05:31s
epoch 86 | loss: 0.24533 | val_0_rmse: 0.53706 | val_1_rmse: 0.53836 |  0:05:35s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.49402
Best weights from best epoch are automatically used!
ended training at: 08:06:46
Feature importance:
[('Area', 0.5921927404780047), ('Baths', 0.16258725740031588), ('Beds', 0.19453313519022036), ('Latitude', 0.050686866931459074), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 910911802.022325
Mean absolute error:20366.648769998847
MAPE:0.31300513185344536
R2 score:0.7245210288053594
------------------------------------------------------------------
Normalization used is Log Transformation with SS Normalization
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:27:59
epoch 0  | loss: 0.31652 | val_0_rmse: 0.52878 | val_1_rmse: 0.52835 |  0:00:17s
epoch 1  | loss: 0.27363 | val_0_rmse: 0.51458 | val_1_rmse: 0.51383 |  0:00:35s
epoch 2  | loss: 0.26639 | val_0_rmse: 0.51255 | val_1_rmse: 0.51069 |  0:00:53s
epoch 3  | loss: 0.2595  | val_0_rmse: 0.50044 | val_1_rmse: 0.49982 |  0:01:11s
epoch 4  | loss: 0.25469 | val_0_rmse: 0.51085 | val_1_rmse: 0.51088 |  0:01:29s
epoch 5  | loss: 0.24673 | val_0_rmse: 0.5539  | val_1_rmse: 0.55484 |  0:01:47s
epoch 6  | loss: 0.24146 | val_0_rmse: 0.49521 | val_1_rmse: 0.49484 |  0:02:04s
epoch 7  | loss: 0.2385  | val_0_rmse: 0.58319 | val_1_rmse: 0.58528 |  0:02:22s
epoch 8  | loss: 0.23436 | val_0_rmse: 0.47701 | val_1_rmse: 0.47644 |  0:02:40s
epoch 9  | loss: 0.23217 | val_0_rmse: 0.48124 | val_1_rmse: 0.48047 |  0:02:58s
epoch 10 | loss: 0.22914 | val_0_rmse: 0.51026 | val_1_rmse: 0.51029 |  0:03:16s
epoch 11 | loss: 0.22768 | val_0_rmse: 0.47404 | val_1_rmse: 0.47336 |  0:03:34s
epoch 12 | loss: 0.22777 | val_0_rmse: 0.49114 | val_1_rmse: 0.49114 |  0:03:52s
epoch 13 | loss: 0.22601 | val_0_rmse: 0.48072 | val_1_rmse: 0.47971 |  0:04:10s
epoch 14 | loss: 0.23362 | val_0_rmse: 0.61147 | val_1_rmse: 0.61211 |  0:04:28s
epoch 15 | loss: 0.22479 | val_0_rmse: 0.46948 | val_1_rmse: 0.46803 |  0:04:46s
epoch 16 | loss: 0.2293  | val_0_rmse: 0.70684 | val_1_rmse: 0.70806 |  0:05:04s
epoch 17 | loss: 0.22338 | val_0_rmse: 0.4823  | val_1_rmse: 0.48251 |  0:05:21s
epoch 18 | loss: 0.22281 | val_0_rmse: 0.52205 | val_1_rmse: 0.52218 |  0:05:39s
epoch 19 | loss: 0.22303 | val_0_rmse: 0.53491 | val_1_rmse: 0.5346  |  0:05:57s
epoch 20 | loss: 0.24816 | val_0_rmse: 0.57654 | val_1_rmse: 0.57606 |  0:06:15s
epoch 21 | loss: 0.2311  | val_0_rmse: 0.50696 | val_1_rmse: 0.50536 |  0:06:33s
epoch 22 | loss: 0.23157 | val_0_rmse: 0.60645 | val_1_rmse: 0.60767 |  0:06:51s
epoch 23 | loss: 0.22301 | val_0_rmse: 0.57347 | val_1_rmse: 0.57428 |  0:07:09s
epoch 24 | loss: 0.21997 | val_0_rmse: 0.50334 | val_1_rmse: 0.50326 |  0:07:26s
epoch 25 | loss: 0.2191  | val_0_rmse: 0.53344 | val_1_rmse: 0.53423 |  0:07:45s
epoch 26 | loss: 0.24458 | val_0_rmse: 0.54052 | val_1_rmse: 0.53871 |  0:08:02s
epoch 27 | loss: 0.27765 | val_0_rmse: 0.51405 | val_1_rmse: 0.51358 |  0:08:20s
epoch 28 | loss: 0.25835 | val_0_rmse: 0.49812 | val_1_rmse: 0.4967  |  0:08:38s
epoch 29 | loss: 0.25078 | val_0_rmse: 0.5008  | val_1_rmse: 0.50094 |  0:08:56s
epoch 30 | loss: 0.24064 | val_0_rmse: 0.67505 | val_1_rmse: 0.67621 |  0:09:14s
epoch 31 | loss: 0.23278 | val_0_rmse: 0.71222 | val_1_rmse: 0.71325 |  0:09:32s
epoch 32 | loss: 0.23595 | val_0_rmse: 0.53987 | val_1_rmse: 0.54076 |  0:09:50s
epoch 33 | loss: 0.24249 | val_0_rmse: 0.48173 | val_1_rmse: 0.4799  |  0:10:07s
epoch 34 | loss: 0.2295  | val_0_rmse: 0.65101 | val_1_rmse: 0.6518  |  0:10:25s
epoch 35 | loss: 0.22573 | val_0_rmse: 0.49846 | val_1_rmse: 0.49644 |  0:10:43s
epoch 36 | loss: 0.22579 | val_0_rmse: 0.53268 | val_1_rmse: 0.53336 |  0:11:01s
epoch 37 | loss: 0.22342 | val_0_rmse: 0.469   | val_1_rmse: 0.46786 |  0:11:19s
epoch 38 | loss: 0.22162 | val_0_rmse: 0.48906 | val_1_rmse: 0.48825 |  0:11:37s
epoch 39 | loss: 0.22079 | val_0_rmse: 0.63452 | val_1_rmse: 0.63545 |  0:11:55s
epoch 40 | loss: 0.22242 | val_0_rmse: 0.52299 | val_1_rmse: 0.52271 |  0:12:13s
epoch 41 | loss: 0.21937 | val_0_rmse: 0.51528 | val_1_rmse: 0.51463 |  0:12:30s
epoch 42 | loss: 0.21996 | val_0_rmse: 0.46349 | val_1_rmse: 0.46235 |  0:12:48s
epoch 43 | loss: 0.21894 | val_0_rmse: 0.62386 | val_1_rmse: 0.62416 |  0:13:06s
epoch 44 | loss: 0.2203  | val_0_rmse: 0.5031  | val_1_rmse: 0.50331 |  0:13:24s
epoch 45 | loss: 0.21893 | val_0_rmse: 0.49925 | val_1_rmse: 0.49917 |  0:13:42s
epoch 46 | loss: 0.21733 | val_0_rmse: 0.50532 | val_1_rmse: 0.5051  |  0:14:00s
epoch 47 | loss: 0.21802 | val_0_rmse: 0.62525 | val_1_rmse: 0.62686 |  0:14:18s
epoch 48 | loss: 0.2163  | val_0_rmse: 0.56467 | val_1_rmse: 0.56526 |  0:14:36s
epoch 49 | loss: 0.22912 | val_0_rmse: 0.57151 | val_1_rmse: 0.57141 |  0:14:54s
epoch 50 | loss: 0.22192 | val_0_rmse: 0.46365 | val_1_rmse: 0.46302 |  0:15:11s
epoch 51 | loss: 0.22146 | val_0_rmse: 0.46811 | val_1_rmse: 0.46729 |  0:15:29s
epoch 52 | loss: 0.2188  | val_0_rmse: 0.64134 | val_1_rmse: 0.64198 |  0:15:47s
epoch 53 | loss: 0.21834 | val_0_rmse: 0.46407 | val_1_rmse: 0.46376 |  0:16:05s
epoch 54 | loss: 0.22125 | val_0_rmse: 0.77802 | val_1_rmse: 0.77862 |  0:16:23s
epoch 55 | loss: 0.22238 | val_0_rmse: 0.52258 | val_1_rmse: 0.52239 |  0:16:41s
epoch 56 | loss: 0.21885 | val_0_rmse: 0.63069 | val_1_rmse: 0.63172 |  0:16:59s
epoch 57 | loss: 0.2172  | val_0_rmse: 0.53277 | val_1_rmse: 0.53283 |  0:17:17s
epoch 58 | loss: 0.21884 | val_0_rmse: 0.52309 | val_1_rmse: 0.52195 |  0:17:34s
epoch 59 | loss: 0.2215  | val_0_rmse: 0.53952 | val_1_rmse: 0.53905 |  0:17:52s
epoch 60 | loss: 0.21756 | val_0_rmse: 0.65847 | val_1_rmse: 0.65972 |  0:18:10s
epoch 61 | loss: 0.2199  | val_0_rmse: 0.65642 | val_1_rmse: 0.65758 |  0:18:28s
epoch 62 | loss: 0.22469 | val_0_rmse: 0.49064 | val_1_rmse: 0.49007 |  0:18:46s
epoch 63 | loss: 0.24766 | val_0_rmse: 0.56794 | val_1_rmse: 0.56979 |  0:19:04s
epoch 64 | loss: 0.23523 | val_0_rmse: 0.59077 | val_1_rmse: 0.59109 |  0:19:22s
epoch 65 | loss: 0.23426 | val_0_rmse: 0.52212 | val_1_rmse: 0.52186 |  0:19:40s
epoch 66 | loss: 0.23482 | val_0_rmse: 0.51722 | val_1_rmse: 0.5165  |  0:19:58s
epoch 67 | loss: 0.23295 | val_0_rmse: 0.49595 | val_1_rmse: 0.49555 |  0:20:16s
epoch 68 | loss: 0.23667 | val_0_rmse: 0.79972 | val_1_rmse: 0.80136 |  0:20:33s
epoch 69 | loss: 0.24379 | val_0_rmse: 0.51791 | val_1_rmse: 0.51741 |  0:20:51s
epoch 70 | loss: 0.24443 | val_0_rmse: 0.55123 | val_1_rmse: 0.55232 |  0:21:09s
epoch 71 | loss: 0.25165 | val_0_rmse: 0.61937 | val_1_rmse: 0.61775 |  0:21:27s
epoch 72 | loss: 0.23784 | val_0_rmse: 0.54204 | val_1_rmse: 0.5429  |  0:21:45s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.46235
Best weights from best epoch are automatically used!
ended training at: 08:49:50
Feature importance:
[('Area', 0.2040515690075612), ('Baths', 0.13342211868761375), ('Beds', 0.02874844838670744), ('Latitude', 0.4481117600838973), ('Longitude', 0.14996857441386413), ('Month', 0.0), ('Year', 0.03569752942035624)]
Mean squared error is of 12283651429.934551
Mean absolute error:67292.18860484008
MAPE:0.3802392251854688
R2 score:0.6884614582884819
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:49:52
epoch 0  | loss: 0.30047 | val_0_rmse: 0.49726 | val_1_rmse: 0.50272 |  0:00:18s
epoch 1  | loss: 0.26038 | val_0_rmse: 0.52984 | val_1_rmse: 0.53621 |  0:00:35s
epoch 2  | loss: 0.25287 | val_0_rmse: 0.52781 | val_1_rmse: 0.53373 |  0:00:54s
epoch 3  | loss: 0.24885 | val_0_rmse: 0.51667 | val_1_rmse: 0.52146 |  0:01:11s
epoch 4  | loss: 0.24171 | val_0_rmse: 0.52164 | val_1_rmse: 0.52417 |  0:01:29s
epoch 5  | loss: 0.23419 | val_0_rmse: 0.57928 | val_1_rmse: 0.58506 |  0:01:48s
epoch 6  | loss: 0.23833 | val_0_rmse: 0.60997 | val_1_rmse: 0.61408 |  0:02:05s
epoch 7  | loss: 0.24799 | val_0_rmse: 0.55346 | val_1_rmse: 0.55722 |  0:02:23s
epoch 8  | loss: 0.28166 | val_0_rmse: 0.55977 | val_1_rmse: 0.563   |  0:02:41s
epoch 9  | loss: 0.2775  | val_0_rmse: 0.59376 | val_1_rmse: 0.59484 |  0:02:59s
epoch 10 | loss: 0.27512 | val_0_rmse: 0.56366 | val_1_rmse: 0.56755 |  0:03:17s
epoch 11 | loss: 0.2836  | val_0_rmse: 0.71396 | val_1_rmse: 0.71436 |  0:03:35s
epoch 12 | loss: 0.27798 | val_0_rmse: 0.68394 | val_1_rmse: 0.6855  |  0:03:53s
epoch 13 | loss: 0.27627 | val_0_rmse: 0.62049 | val_1_rmse: 0.62205 |  0:04:11s
epoch 14 | loss: 0.27428 | val_0_rmse: 0.58905 | val_1_rmse: 0.59094 |  0:04:29s
epoch 15 | loss: 0.27399 | val_0_rmse: 0.62877 | val_1_rmse: 0.63084 |  0:04:47s
epoch 16 | loss: 0.27311 | val_0_rmse: 0.59029 | val_1_rmse: 0.59257 |  0:05:05s
epoch 17 | loss: 0.26878 | val_0_rmse: 0.49975 | val_1_rmse: 0.50458 |  0:05:23s
epoch 18 | loss: 0.28592 | val_0_rmse: 0.62772 | val_1_rmse: 0.62842 |  0:05:41s
epoch 19 | loss: 0.24356 | val_0_rmse: 0.62011 | val_1_rmse: 0.62062 |  0:05:58s
epoch 20 | loss: 0.23564 | val_0_rmse: 0.58523 | val_1_rmse: 0.5866  |  0:06:16s
epoch 21 | loss: 0.23219 | val_0_rmse: 0.70216 | val_1_rmse: 0.70217 |  0:06:34s
epoch 22 | loss: 0.23012 | val_0_rmse: 0.70297 | val_1_rmse: 0.70277 |  0:06:52s
epoch 23 | loss: 0.22813 | val_0_rmse: 0.77453 | val_1_rmse: 0.77358 |  0:07:10s
epoch 24 | loss: 0.22659 | val_0_rmse: 0.69259 | val_1_rmse: 0.69238 |  0:07:28s
epoch 25 | loss: 0.22386 | val_0_rmse: 0.6763  | val_1_rmse: 0.67725 |  0:07:46s
epoch 26 | loss: 0.22292 | val_0_rmse: 0.6242  | val_1_rmse: 0.62534 |  0:08:04s
epoch 27 | loss: 0.22373 | val_0_rmse: 0.6361  | val_1_rmse: 0.63643 |  0:08:22s
epoch 28 | loss: 0.22165 | val_0_rmse: 0.58139 | val_1_rmse: 0.58301 |  0:08:40s
epoch 29 | loss: 0.22042 | val_0_rmse: 0.725   | val_1_rmse: 0.72393 |  0:08:58s
epoch 30 | loss: 0.21926 | val_0_rmse: 0.60649 | val_1_rmse: 0.60693 |  0:09:16s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 0.50272
Best weights from best epoch are automatically used!
ended training at: 08:59:14
Feature importance:
[('Area', 0.2172550565526995), ('Baths', 0.1469072903219964), ('Beds', 0.1448187372617219), ('Latitude', 0.2115351430044616), ('Longitude', 0.15858460584190778), ('Month', 0.030646155804997605), ('Year', 0.09025301121221525)]
Mean squared error is of 13680713035.088003
Mean absolute error:70709.79198264422
MAPE:0.4073504562407908
R2 score:0.6513919050689256
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 08:59:15
epoch 0  | loss: 0.31768 | val_0_rmse: 0.52161 | val_1_rmse: 0.52019 |  0:00:17s
epoch 1  | loss: 0.27304 | val_0_rmse: 0.5166  | val_1_rmse: 0.51558 |  0:00:35s
epoch 2  | loss: 0.25702 | val_0_rmse: 0.5665  | val_1_rmse: 0.5668  |  0:00:53s
epoch 3  | loss: 0.24992 | val_0_rmse: 0.55254 | val_1_rmse: 0.55284 |  0:01:11s
epoch 4  | loss: 0.24226 | val_0_rmse: 0.54449 | val_1_rmse: 0.54351 |  0:01:29s
epoch 5  | loss: 0.23772 | val_0_rmse: 0.49056 | val_1_rmse: 0.49116 |  0:01:47s
epoch 6  | loss: 0.23481 | val_0_rmse: 0.52696 | val_1_rmse: 0.52753 |  0:02:05s
epoch 7  | loss: 0.23218 | val_0_rmse: 0.59944 | val_1_rmse: 0.5991  |  0:02:23s
epoch 8  | loss: 0.23102 | val_0_rmse: 0.47971 | val_1_rmse: 0.4786  |  0:02:40s
epoch 9  | loss: 0.22969 | val_0_rmse: 0.48181 | val_1_rmse: 0.48076 |  0:02:58s
epoch 10 | loss: 0.22991 | val_0_rmse: 0.50364 | val_1_rmse: 0.50376 |  0:03:16s
epoch 11 | loss: 0.23217 | val_0_rmse: 0.51662 | val_1_rmse: 0.5167  |  0:03:34s
epoch 12 | loss: 0.22794 | val_0_rmse: 0.50167 | val_1_rmse: 0.50264 |  0:03:52s
epoch 13 | loss: 0.22524 | val_0_rmse: 0.50427 | val_1_rmse: 0.50481 |  0:04:10s
epoch 14 | loss: 0.2295  | val_0_rmse: 0.50401 | val_1_rmse: 0.50336 |  0:04:28s
epoch 15 | loss: 0.22857 | val_0_rmse: 0.48985 | val_1_rmse: 0.48867 |  0:04:46s
epoch 16 | loss: 0.23293 | val_0_rmse: 0.4991  | val_1_rmse: 0.4995  |  0:05:04s
epoch 17 | loss: 0.22822 | val_0_rmse: 0.52252 | val_1_rmse: 0.52337 |  0:05:22s
epoch 18 | loss: 0.22833 | val_0_rmse: 0.495   | val_1_rmse: 0.49534 |  0:05:39s
epoch 19 | loss: 0.22573 | val_0_rmse: 0.53144 | val_1_rmse: 0.53166 |  0:05:57s
epoch 20 | loss: 0.22428 | val_0_rmse: 0.49945 | val_1_rmse: 0.49999 |  0:06:15s
epoch 21 | loss: 0.22384 | val_0_rmse: 0.521   | val_1_rmse: 0.52052 |  0:06:33s
epoch 22 | loss: 0.22243 | val_0_rmse: 0.47132 | val_1_rmse: 0.4727  |  0:06:51s
epoch 23 | loss: 0.22231 | val_0_rmse: 0.49111 | val_1_rmse: 0.49118 |  0:07:09s
epoch 24 | loss: 0.2225  | val_0_rmse: 0.52799 | val_1_rmse: 0.5284  |  0:07:27s
epoch 25 | loss: 0.22499 | val_0_rmse: 0.57925 | val_1_rmse: 0.58045 |  0:07:45s
epoch 26 | loss: 0.25608 | val_0_rmse: 0.52348 | val_1_rmse: 0.52537 |  0:08:03s
epoch 27 | loss: 0.2511  | val_0_rmse: 0.64669 | val_1_rmse: 0.64784 |  0:08:21s
epoch 28 | loss: 0.24062 | val_0_rmse: 0.51631 | val_1_rmse: 0.51728 |  0:08:38s
epoch 29 | loss: 0.23818 | val_0_rmse: 0.49971 | val_1_rmse: 0.5005  |  0:08:56s
epoch 30 | loss: 0.2377  | val_0_rmse: 0.53886 | val_1_rmse: 0.54029 |  0:09:14s
epoch 31 | loss: 0.2372  | val_0_rmse: 0.50054 | val_1_rmse: 0.50102 |  0:09:32s
epoch 32 | loss: 0.23498 | val_0_rmse: 0.58967 | val_1_rmse: 0.59135 |  0:09:50s
epoch 33 | loss: 0.23438 | val_0_rmse: 0.51622 | val_1_rmse: 0.5182  |  0:10:08s
epoch 34 | loss: 0.2336  | val_0_rmse: 0.50963 | val_1_rmse: 0.51046 |  0:10:26s
epoch 35 | loss: 0.23314 | val_0_rmse: 0.59809 | val_1_rmse: 0.60075 |  0:10:44s
epoch 36 | loss: 0.23259 | val_0_rmse: 0.51349 | val_1_rmse: 0.51489 |  0:11:02s
epoch 37 | loss: 0.23558 | val_0_rmse: 0.5177  | val_1_rmse: 0.51956 |  0:11:20s
epoch 38 | loss: 0.23575 | val_0_rmse: 0.49607 | val_1_rmse: 0.49766 |  0:11:37s
epoch 39 | loss: 0.23502 | val_0_rmse: 0.52083 | val_1_rmse: 0.52159 |  0:11:55s
epoch 40 | loss: 0.23291 | val_0_rmse: 0.50807 | val_1_rmse: 0.50852 |  0:12:13s
epoch 41 | loss: 0.23288 | val_0_rmse: 0.50319 | val_1_rmse: 0.50425 |  0:12:31s
epoch 42 | loss: 0.23162 | val_0_rmse: 0.48946 | val_1_rmse: 0.49087 |  0:12:49s
epoch 43 | loss: 0.23244 | val_0_rmse: 0.59    | val_1_rmse: 0.5922  |  0:13:07s
epoch 44 | loss: 0.23095 | val_0_rmse: 0.5352  | val_1_rmse: 0.53597 |  0:13:25s
epoch 45 | loss: 0.23204 | val_0_rmse: 0.53215 | val_1_rmse: 0.53301 |  0:13:43s
epoch 46 | loss: 0.23263 | val_0_rmse: 0.50952 | val_1_rmse: 0.50942 |  0:14:01s
epoch 47 | loss: 0.23178 | val_0_rmse: 0.48606 | val_1_rmse: 0.4877  |  0:14:18s
epoch 48 | loss: 0.2327  | val_0_rmse: 0.49139 | val_1_rmse: 0.49278 |  0:14:36s
epoch 49 | loss: 0.22956 | val_0_rmse: 0.58751 | val_1_rmse: 0.58979 |  0:14:54s
epoch 50 | loss: 0.23    | val_0_rmse: 0.58934 | val_1_rmse: 0.59206 |  0:15:12s
epoch 51 | loss: 0.23111 | val_0_rmse: 0.52923 | val_1_rmse: 0.53099 |  0:15:30s
epoch 52 | loss: 0.22952 | val_0_rmse: 0.49861 | val_1_rmse: 0.49942 |  0:15:48s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.4727
Best weights from best epoch are automatically used!
ended training at: 09:15:09
Feature importance:
[('Area', 0.33160906086754915), ('Baths', 0.0), ('Beds', 0.06460465791731591), ('Latitude', 0.3179655009441617), ('Longitude', 0.22838301824952353), ('Month', 9.60809293972242e-07), ('Year', 0.057436801212155705)]
Mean squared error is of 13249801203.713673
Mean absolute error:67510.82477238074
MAPE:0.3811489035567421
R2 score:0.6613629721826753
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:15:11
epoch 0  | loss: 0.31169 | val_0_rmse: 0.51269 | val_1_rmse: 0.51316 |  0:00:17s
epoch 1  | loss: 0.26884 | val_0_rmse: 0.51479 | val_1_rmse: 0.51494 |  0:00:35s
epoch 2  | loss: 0.25624 | val_0_rmse: 0.49802 | val_1_rmse: 0.49836 |  0:00:53s
epoch 3  | loss: 0.25367 | val_0_rmse: 0.50103 | val_1_rmse: 0.50123 |  0:01:11s
epoch 4  | loss: 0.25064 | val_0_rmse: 0.5181  | val_1_rmse: 0.51746 |  0:01:29s
epoch 5  | loss: 0.24817 | val_0_rmse: 0.61264 | val_1_rmse: 0.61322 |  0:01:47s
epoch 6  | loss: 0.2449  | val_0_rmse: 0.48533 | val_1_rmse: 0.48534 |  0:02:05s
epoch 7  | loss: 0.23877 | val_0_rmse: 0.51648 | val_1_rmse: 0.51662 |  0:02:23s
epoch 8  | loss: 0.23348 | val_0_rmse: 0.60902 | val_1_rmse: 0.60904 |  0:02:41s
epoch 9  | loss: 0.23177 | val_0_rmse: 0.50069 | val_1_rmse: 0.50035 |  0:02:59s
epoch 10 | loss: 0.22963 | val_0_rmse: 0.57482 | val_1_rmse: 0.57603 |  0:03:16s
epoch 11 | loss: 0.22892 | val_0_rmse: 0.70181 | val_1_rmse: 0.70272 |  0:03:35s
epoch 12 | loss: 0.22885 | val_0_rmse: 0.51719 | val_1_rmse: 0.51766 |  0:03:52s
epoch 13 | loss: 0.22786 | val_0_rmse: 0.65261 | val_1_rmse: 0.6542  |  0:04:10s
epoch 14 | loss: 0.22528 | val_0_rmse: 0.49695 | val_1_rmse: 0.49594 |  0:04:28s
epoch 15 | loss: 0.23188 | val_0_rmse: 0.59563 | val_1_rmse: 0.59584 |  0:04:46s
epoch 16 | loss: 0.23939 | val_0_rmse: 0.51213 | val_1_rmse: 0.51286 |  0:05:04s
epoch 17 | loss: 0.22823 | val_0_rmse: 0.47012 | val_1_rmse: 0.46937 |  0:05:22s
epoch 18 | loss: 0.22612 | val_0_rmse: 0.48466 | val_1_rmse: 0.48511 |  0:05:40s
epoch 19 | loss: 0.22374 | val_0_rmse: 0.64603 | val_1_rmse: 0.6468  |  0:05:58s
epoch 20 | loss: 0.22379 | val_0_rmse: 0.47368 | val_1_rmse: 0.47389 |  0:06:16s
epoch 21 | loss: 0.22204 | val_0_rmse: 0.50655 | val_1_rmse: 0.50706 |  0:06:34s
epoch 22 | loss: 0.22196 | val_0_rmse: 0.50126 | val_1_rmse: 0.50177 |  0:06:52s
epoch 23 | loss: 0.22015 | val_0_rmse: 0.5146  | val_1_rmse: 0.51479 |  0:07:10s
epoch 24 | loss: 0.22962 | val_0_rmse: 0.55192 | val_1_rmse: 0.55186 |  0:07:27s
epoch 25 | loss: 0.22259 | val_0_rmse: 0.47631 | val_1_rmse: 0.47566 |  0:07:46s
epoch 26 | loss: 0.22164 | val_0_rmse: 0.53166 | val_1_rmse: 0.53317 |  0:08:03s
epoch 27 | loss: 0.22035 | val_0_rmse: 0.51494 | val_1_rmse: 0.51497 |  0:08:21s
epoch 28 | loss: 0.22035 | val_0_rmse: 0.47603 | val_1_rmse: 0.47682 |  0:08:39s
epoch 29 | loss: 0.21952 | val_0_rmse: 0.60769 | val_1_rmse: 0.60844 |  0:08:57s
epoch 30 | loss: 0.22012 | val_0_rmse: 0.5657  | val_1_rmse: 0.56636 |  0:09:15s
epoch 31 | loss: 0.22061 | val_0_rmse: 0.57958 | val_1_rmse: 0.58074 |  0:09:33s
epoch 32 | loss: 0.21913 | val_0_rmse: 0.4689  | val_1_rmse: 0.46943 |  0:09:51s
epoch 33 | loss: 0.2184  | val_0_rmse: 0.60969 | val_1_rmse: 0.61044 |  0:10:09s
epoch 34 | loss: 0.21744 | val_0_rmse: 0.61289 | val_1_rmse: 0.6144  |  0:10:27s
epoch 35 | loss: 0.21828 | val_0_rmse: 0.46893 | val_1_rmse: 0.46809 |  0:10:45s
epoch 36 | loss: 0.21767 | val_0_rmse: 0.4767  | val_1_rmse: 0.47754 |  0:11:03s
epoch 37 | loss: 0.21835 | val_0_rmse: 0.54851 | val_1_rmse: 0.54921 |  0:11:21s
epoch 38 | loss: 0.22028 | val_0_rmse: 0.64409 | val_1_rmse: 0.64577 |  0:11:39s
epoch 39 | loss: 0.21926 | val_0_rmse: 0.48482 | val_1_rmse: 0.48576 |  0:11:57s
epoch 40 | loss: 0.21724 | val_0_rmse: 0.4623  | val_1_rmse: 0.46263 |  0:12:15s
epoch 41 | loss: 0.21653 | val_0_rmse: 0.49436 | val_1_rmse: 0.4948  |  0:12:33s
epoch 42 | loss: 0.21601 | val_0_rmse: 0.52753 | val_1_rmse: 0.52856 |  0:12:51s
epoch 43 | loss: 0.216   | val_0_rmse: 0.55462 | val_1_rmse: 0.55496 |  0:13:09s
epoch 44 | loss: 0.21632 | val_0_rmse: 0.53483 | val_1_rmse: 0.53462 |  0:13:26s
epoch 45 | loss: 0.21859 | val_0_rmse: 0.46782 | val_1_rmse: 0.46946 |  0:13:45s
epoch 46 | loss: 0.21685 | val_0_rmse: 0.55668 | val_1_rmse: 0.55778 |  0:14:02s
epoch 47 | loss: 0.21522 | val_0_rmse: 0.49501 | val_1_rmse: 0.49518 |  0:14:20s
epoch 48 | loss: 0.21623 | val_0_rmse: 0.58129 | val_1_rmse: 0.582   |  0:14:38s
epoch 49 | loss: 0.21476 | val_0_rmse: 0.57667 | val_1_rmse: 0.57802 |  0:14:56s
epoch 50 | loss: 0.21494 | val_0_rmse: 0.48648 | val_1_rmse: 0.4869  |  0:15:14s
epoch 51 | loss: 0.21518 | val_0_rmse: 0.4865  | val_1_rmse: 0.48733 |  0:15:32s
epoch 52 | loss: 0.21492 | val_0_rmse: 0.52035 | val_1_rmse: 0.521   |  0:15:50s
epoch 53 | loss: 0.21405 | val_0_rmse: 0.49901 | val_1_rmse: 0.50015 |  0:16:08s
epoch 54 | loss: 0.21384 | val_0_rmse: 0.62432 | val_1_rmse: 0.62537 |  0:16:26s
epoch 55 | loss: 0.21409 | val_0_rmse: 0.50261 | val_1_rmse: 0.50409 |  0:16:43s
epoch 56 | loss: 0.21537 | val_0_rmse: 0.66479 | val_1_rmse: 0.66633 |  0:17:01s
epoch 57 | loss: 0.21425 | val_0_rmse: 0.60547 | val_1_rmse: 0.60709 |  0:17:19s
epoch 58 | loss: 0.2156  | val_0_rmse: 0.47648 | val_1_rmse: 0.47652 |  0:17:37s
epoch 59 | loss: 0.21412 | val_0_rmse: 0.51391 | val_1_rmse: 0.51453 |  0:17:55s
epoch 60 | loss: 0.21408 | val_0_rmse: 0.46264 | val_1_rmse: 0.46337 |  0:18:13s
epoch 61 | loss: 0.21651 | val_0_rmse: 0.4618  | val_1_rmse: 0.46366 |  0:18:31s
epoch 62 | loss: 0.21383 | val_0_rmse: 0.61661 | val_1_rmse: 0.61769 |  0:18:49s
epoch 63 | loss: 0.21437 | val_0_rmse: 0.54961 | val_1_rmse: 0.54997 |  0:19:07s
epoch 64 | loss: 0.21478 | val_0_rmse: 0.61802 | val_1_rmse: 0.61912 |  0:19:25s
epoch 65 | loss: 0.21389 | val_0_rmse: 0.55918 | val_1_rmse: 0.56065 |  0:19:43s
epoch 66 | loss: 0.21344 | val_0_rmse: 0.67549 | val_1_rmse: 0.67579 |  0:20:00s
epoch 67 | loss: 0.21383 | val_0_rmse: 0.64879 | val_1_rmse: 0.64982 |  0:20:18s
epoch 68 | loss: 0.21391 | val_0_rmse: 0.55792 | val_1_rmse: 0.55919 |  0:20:36s
epoch 69 | loss: 0.21544 | val_0_rmse: 0.58505 | val_1_rmse: 0.58754 |  0:20:54s
epoch 70 | loss: 0.21334 | val_0_rmse: 0.4635  | val_1_rmse: 0.46385 |  0:21:12s

Early stopping occured at epoch 70 with best_epoch = 40 and best_val_1_rmse = 0.46263
Best weights from best epoch are automatically used!
ended training at: 09:36:29
Feature importance:
[('Area', 0.3613413173047192), ('Baths', 0.17191507918973356), ('Beds', 0.001119933595693351), ('Latitude', 0.09086881053313764), ('Longitude', 0.13366931474355173), ('Month', 0.23834572262998113), ('Year', 0.0027398220031833696)]
Mean squared error is of 13268017177.697983
Mean absolute error:67077.77628168427
MAPE:0.36270134092077466
R2 score:0.6628429935929578
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:36:30
epoch 0  | loss: 0.31127 | val_0_rmse: 0.51438 | val_1_rmse: 0.51632 |  0:00:17s
epoch 1  | loss: 0.26808 | val_0_rmse: 0.50732 | val_1_rmse: 0.50925 |  0:00:35s
epoch 2  | loss: 0.2616  | val_0_rmse: 0.50881 | val_1_rmse: 0.51025 |  0:00:53s
epoch 3  | loss: 0.25645 | val_0_rmse: 0.50463 | val_1_rmse: 0.50676 |  0:01:11s
epoch 4  | loss: 0.25179 | val_0_rmse: 0.51162 | val_1_rmse: 0.51291 |  0:01:29s
epoch 5  | loss: 0.25562 | val_0_rmse: 0.50608 | val_1_rmse: 0.50662 |  0:01:47s
epoch 6  | loss: 0.25211 | val_0_rmse: 0.51943 | val_1_rmse: 0.52099 |  0:02:05s
epoch 7  | loss: 0.24952 | val_0_rmse: 0.49476 | val_1_rmse: 0.49654 |  0:02:23s
epoch 8  | loss: 0.2467  | val_0_rmse: 0.52874 | val_1_rmse: 0.53071 |  0:02:41s
epoch 9  | loss: 0.24762 | val_0_rmse: 0.4944  | val_1_rmse: 0.49559 |  0:02:59s
epoch 10 | loss: 0.24669 | val_0_rmse: 0.51161 | val_1_rmse: 0.51351 |  0:03:17s
epoch 11 | loss: 0.24664 | val_0_rmse: 0.49308 | val_1_rmse: 0.49506 |  0:03:35s
epoch 12 | loss: 0.25046 | val_0_rmse: 0.50718 | val_1_rmse: 0.50933 |  0:03:53s
epoch 13 | loss: 0.25841 | val_0_rmse: 0.49545 | val_1_rmse: 0.49646 |  0:04:11s
epoch 14 | loss: 0.25236 | val_0_rmse: 0.50465 | val_1_rmse: 0.50489 |  0:04:28s
epoch 15 | loss: 0.25278 | val_0_rmse: 0.50481 | val_1_rmse: 0.5061  |  0:04:47s
epoch 16 | loss: 0.25017 | val_0_rmse: 0.49149 | val_1_rmse: 0.49174 |  0:05:04s
epoch 17 | loss: 0.25032 | val_0_rmse: 0.50492 | val_1_rmse: 0.50574 |  0:05:22s
epoch 18 | loss: 0.24639 | val_0_rmse: 0.51821 | val_1_rmse: 0.52035 |  0:05:40s
epoch 19 | loss: 0.24673 | val_0_rmse: 0.52315 | val_1_rmse: 0.52398 |  0:05:58s
epoch 20 | loss: 0.24438 | val_0_rmse: 0.50695 | val_1_rmse: 0.50861 |  0:06:16s
epoch 21 | loss: 0.24229 | val_0_rmse: 0.48836 | val_1_rmse: 0.48934 |  0:06:34s
epoch 22 | loss: 0.2404  | val_0_rmse: 0.51452 | val_1_rmse: 0.51595 |  0:06:52s
epoch 23 | loss: 0.2357  | val_0_rmse: 0.54791 | val_1_rmse: 0.54674 |  0:07:10s
epoch 24 | loss: 0.2284  | val_0_rmse: 0.66218 | val_1_rmse: 0.66186 |  0:07:28s
epoch 25 | loss: 0.22562 | val_0_rmse: 0.47132 | val_1_rmse: 0.47175 |  0:07:46s
epoch 26 | loss: 0.22404 | val_0_rmse: 0.47961 | val_1_rmse: 0.48095 |  0:08:04s
epoch 27 | loss: 0.22247 | val_0_rmse: 0.61269 | val_1_rmse: 0.61331 |  0:08:21s
epoch 28 | loss: 0.22179 | val_0_rmse: 0.58944 | val_1_rmse: 0.5905  |  0:08:39s
epoch 29 | loss: 0.22026 | val_0_rmse: 0.48246 | val_1_rmse: 0.4844  |  0:08:57s
epoch 30 | loss: 0.21958 | val_0_rmse: 0.68199 | val_1_rmse: 0.68169 |  0:09:15s
epoch 31 | loss: 0.2217  | val_0_rmse: 0.46482 | val_1_rmse: 0.46506 |  0:09:33s
epoch 32 | loss: 0.21966 | val_0_rmse: 0.49525 | val_1_rmse: 0.49455 |  0:09:51s
epoch 33 | loss: 0.22096 | val_0_rmse: 0.49539 | val_1_rmse: 0.49721 |  0:10:09s
epoch 34 | loss: 0.21967 | val_0_rmse: 0.47972 | val_1_rmse: 0.47961 |  0:10:27s
epoch 35 | loss: 0.21843 | val_0_rmse: 0.66927 | val_1_rmse: 0.66951 |  0:10:45s
epoch 36 | loss: 0.21747 | val_0_rmse: 0.67222 | val_1_rmse: 0.67198 |  0:11:03s
epoch 37 | loss: 0.21955 | val_0_rmse: 0.52024 | val_1_rmse: 0.522   |  0:11:20s
epoch 38 | loss: 0.21911 | val_0_rmse: 0.63912 | val_1_rmse: 0.6401  |  0:11:38s
epoch 39 | loss: 0.21678 | val_0_rmse: 0.50641 | val_1_rmse: 0.50858 |  0:11:56s
epoch 40 | loss: 0.21786 | val_0_rmse: 0.66986 | val_1_rmse: 0.67031 |  0:12:14s
epoch 41 | loss: 0.21714 | val_0_rmse: 0.74445 | val_1_rmse: 0.74452 |  0:12:32s
epoch 42 | loss: 0.2187  | val_0_rmse: 0.49628 | val_1_rmse: 0.49607 |  0:12:50s
epoch 43 | loss: 0.21709 | val_0_rmse: 0.56509 | val_1_rmse: 0.5651  |  0:13:08s
epoch 44 | loss: 0.216   | val_0_rmse: 0.51863 | val_1_rmse: 0.51918 |  0:13:26s
epoch 45 | loss: 0.21634 | val_0_rmse: 0.4844  | val_1_rmse: 0.48453 |  0:13:43s
epoch 46 | loss: 0.21503 | val_0_rmse: 0.49256 | val_1_rmse: 0.49345 |  0:14:01s
epoch 47 | loss: 0.21942 | val_0_rmse: 0.48223 | val_1_rmse: 0.48289 |  0:14:20s
epoch 48 | loss: 0.21611 | val_0_rmse: 0.58878 | val_1_rmse: 0.58976 |  0:14:37s
epoch 49 | loss: 0.21557 | val_0_rmse: 0.73514 | val_1_rmse: 0.7356  |  0:14:55s
epoch 50 | loss: 0.21512 | val_0_rmse: 0.4724  | val_1_rmse: 0.47299 |  0:15:13s
epoch 51 | loss: 0.21475 | val_0_rmse: 0.55271 | val_1_rmse: 0.55268 |  0:15:31s
epoch 52 | loss: 0.21478 | val_0_rmse: 0.67756 | val_1_rmse: 0.67796 |  0:15:49s
epoch 53 | loss: 0.21397 | val_0_rmse: 0.52674 | val_1_rmse: 0.52813 |  0:16:07s
epoch 54 | loss: 0.21402 | val_0_rmse: 0.52149 | val_1_rmse: 0.52266 |  0:16:25s
epoch 55 | loss: 0.21428 | val_0_rmse: 0.60964 | val_1_rmse: 0.60932 |  0:16:43s
epoch 56 | loss: 0.21354 | val_0_rmse: 0.52439 | val_1_rmse: 0.52321 |  0:17:01s
epoch 57 | loss: 0.21442 | val_0_rmse: 0.70292 | val_1_rmse: 0.70166 |  0:17:19s
epoch 58 | loss: 0.25341 | val_0_rmse: 0.52304 | val_1_rmse: 0.52467 |  0:17:37s
epoch 59 | loss: 0.24607 | val_0_rmse: 0.50436 | val_1_rmse: 0.50541 |  0:17:55s
epoch 60 | loss: 0.24465 | val_0_rmse: 0.50044 | val_1_rmse: 0.50178 |  0:18:12s
epoch 61 | loss: 0.23531 | val_0_rmse: 0.48156 | val_1_rmse: 0.48261 |  0:18:30s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.46506
Best weights from best epoch are automatically used!
ended training at: 09:55:06
Feature importance:
[('Area', 0.3876164174359944), ('Baths', 0.0525021359427735), ('Beds', 0.021424345616381395), ('Latitude', 0.32474479810797907), ('Longitude', 0.05529598703625131), ('Month', 0.002210356925009333), ('Year', 0.15620595893561098)]
Mean squared error is of 12851752112.79461
Mean absolute error:67279.16913645285
MAPE:0.3727885557761998
R2 score:0.673418139149319
------------------------------------------------------------------
