TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:53:24
epoch 0  | loss: 1.45304 | val_0_rmse: 1.01301 | val_1_rmse: 0.95343 |  0:00:05s
epoch 1  | loss: 1.01936 | val_0_rmse: 0.91343 | val_1_rmse: 0.88928 |  0:00:07s
epoch 2  | loss: 0.85514 | val_0_rmse: 0.90138 | val_1_rmse: 0.87142 |  0:00:09s
epoch 3  | loss: 0.82135 | val_0_rmse: 0.89344 | val_1_rmse: 0.85331 |  0:00:11s
epoch 4  | loss: 0.82304 | val_0_rmse: 0.89489 | val_1_rmse: 0.86321 |  0:00:13s
epoch 5  | loss: 0.82947 | val_0_rmse: 0.89038 | val_1_rmse: 0.84964 |  0:00:14s
epoch 6  | loss: 0.82561 | val_0_rmse: 0.89461 | val_1_rmse: 0.8595  |  0:00:16s
epoch 7  | loss: 0.80504 | val_0_rmse: 0.89946 | val_1_rmse: 0.86732 |  0:00:18s
epoch 8  | loss: 0.81893 | val_0_rmse: 0.89506 | val_1_rmse: 0.85309 |  0:00:20s
epoch 9  | loss: 0.81442 | val_0_rmse: 0.89511 | val_1_rmse: 0.86404 |  0:00:22s
epoch 10 | loss: 0.80241 | val_0_rmse: 0.89013 | val_1_rmse: 0.8491  |  0:00:24s
epoch 11 | loss: 0.80135 | val_0_rmse: 0.89268 | val_1_rmse: 0.84917 |  0:00:26s
epoch 12 | loss: 0.79648 | val_0_rmse: 0.89083 | val_1_rmse: 0.85776 |  0:00:27s
epoch 13 | loss: 0.78959 | val_0_rmse: 0.89148 | val_1_rmse: 0.86031 |  0:00:29s
epoch 14 | loss: 0.81009 | val_0_rmse: 0.89633 | val_1_rmse: 0.86174 |  0:00:31s
epoch 15 | loss: 0.81663 | val_0_rmse: 0.90863 | val_1_rmse: 0.8601  |  0:00:33s
epoch 16 | loss: 0.78109 | val_0_rmse: 0.87333 | val_1_rmse: 0.84025 |  0:00:35s
epoch 17 | loss: 0.76375 | val_0_rmse: 0.87084 | val_1_rmse: 0.83769 |  0:00:37s
epoch 18 | loss: 0.75619 | val_0_rmse: 0.86527 | val_1_rmse: 0.83044 |  0:00:39s
epoch 19 | loss: 0.7504  | val_0_rmse: 0.85716 | val_1_rmse: 0.82067 |  0:00:40s
epoch 20 | loss: 0.74593 | val_0_rmse: 0.8525  | val_1_rmse: 0.82096 |  0:00:42s
epoch 21 | loss: 0.74796 | val_0_rmse: 0.85671 | val_1_rmse: 0.82617 |  0:00:44s
epoch 22 | loss: 0.73177 | val_0_rmse: 0.8492  | val_1_rmse: 0.81582 |  0:00:46s
epoch 23 | loss: 0.73514 | val_0_rmse: 0.86127 | val_1_rmse: 0.83212 |  0:00:48s
epoch 24 | loss: 0.74988 | val_0_rmse: 0.87685 | val_1_rmse: 0.84765 |  0:00:50s
epoch 25 | loss: 0.76225 | val_0_rmse: 0.86938 | val_1_rmse: 0.84003 |  0:00:52s
epoch 26 | loss: 0.75462 | val_0_rmse: 0.86512 | val_1_rmse: 0.83638 |  0:00:54s
epoch 27 | loss: 0.74991 | val_0_rmse: 0.86394 | val_1_rmse: 0.8333  |  0:00:55s
epoch 28 | loss: 0.74684 | val_0_rmse: 0.86041 | val_1_rmse: 0.83164 |  0:00:57s
epoch 29 | loss: 0.74076 | val_0_rmse: 0.85738 | val_1_rmse: 0.83266 |  0:00:59s
epoch 30 | loss: 0.72936 | val_0_rmse: 0.84403 | val_1_rmse: 0.82003 |  0:01:01s
epoch 31 | loss: 0.71079 | val_0_rmse: 0.83309 | val_1_rmse: 0.81033 |  0:01:03s
epoch 32 | loss: 0.69923 | val_0_rmse: 0.83046 | val_1_rmse: 0.80102 |  0:01:05s
epoch 33 | loss: 0.70108 | val_0_rmse: 0.82862 | val_1_rmse: 0.80821 |  0:01:07s
epoch 34 | loss: 0.70246 | val_0_rmse: 0.82467 | val_1_rmse: 0.79569 |  0:01:08s
epoch 35 | loss: 0.6937  | val_0_rmse: 0.82238 | val_1_rmse: 0.79255 |  0:01:10s
epoch 36 | loss: 0.68734 | val_0_rmse: 0.81868 | val_1_rmse: 0.80123 |  0:01:12s
epoch 37 | loss: 0.68919 | val_0_rmse: 0.84365 | val_1_rmse: 0.82997 |  0:01:14s
epoch 38 | loss: 0.70033 | val_0_rmse: 0.82146 | val_1_rmse: 0.79105 |  0:01:16s
epoch 39 | loss: 0.68539 | val_0_rmse: 0.82556 | val_1_rmse: 0.79229 |  0:01:18s
epoch 40 | loss: 0.68588 | val_0_rmse: 0.81529 | val_1_rmse: 0.78423 |  0:01:20s
epoch 41 | loss: 0.68016 | val_0_rmse: 0.81964 | val_1_rmse: 0.80109 |  0:01:22s
epoch 42 | loss: 0.6888  | val_0_rmse: 0.81376 | val_1_rmse: 0.79549 |  0:01:23s
epoch 43 | loss: 0.67457 | val_0_rmse: 0.81372 | val_1_rmse: 0.79041 |  0:01:25s
epoch 44 | loss: 0.66935 | val_0_rmse: 0.81042 | val_1_rmse: 0.7979  |  0:01:27s
epoch 45 | loss: 0.67616 | val_0_rmse: 0.81475 | val_1_rmse: 0.79153 |  0:01:29s
epoch 46 | loss: 0.67596 | val_0_rmse: 0.81    | val_1_rmse: 0.7796  |  0:01:31s
epoch 47 | loss: 0.67343 | val_0_rmse: 0.81698 | val_1_rmse: 0.78782 |  0:01:33s
epoch 48 | loss: 0.67542 | val_0_rmse: 0.80572 | val_1_rmse: 0.77851 |  0:01:35s
epoch 49 | loss: 0.67174 | val_0_rmse: 0.80759 | val_1_rmse: 0.78979 |  0:01:36s
epoch 50 | loss: 0.66441 | val_0_rmse: 0.80597 | val_1_rmse: 0.78059 |  0:01:38s
epoch 51 | loss: 0.65762 | val_0_rmse: 0.80058 | val_1_rmse: 0.77977 |  0:01:40s
epoch 52 | loss: 0.66447 | val_0_rmse: 0.80514 | val_1_rmse: 0.7952  |  0:01:42s
epoch 53 | loss: 0.65606 | val_0_rmse: 0.79674 | val_1_rmse: 0.78073 |  0:01:44s
epoch 54 | loss: 0.65667 | val_0_rmse: 0.79811 | val_1_rmse: 0.78335 |  0:01:46s
epoch 55 | loss: 0.65307 | val_0_rmse: 0.79464 | val_1_rmse: 0.77637 |  0:01:48s
epoch 56 | loss: 0.6563  | val_0_rmse: 0.79713 | val_1_rmse: 0.78463 |  0:01:50s
epoch 57 | loss: 0.65419 | val_0_rmse: 0.79651 | val_1_rmse: 0.78641 |  0:01:51s
epoch 58 | loss: 0.65192 | val_0_rmse: 0.79754 | val_1_rmse: 0.77396 |  0:01:53s
epoch 59 | loss: 0.65389 | val_0_rmse: 0.79532 | val_1_rmse: 0.77886 |  0:01:55s
epoch 60 | loss: 0.64642 | val_0_rmse: 0.78738 | val_1_rmse: 0.77421 |  0:01:57s
epoch 61 | loss: 0.64187 | val_0_rmse: 0.78788 | val_1_rmse: 0.77339 |  0:01:59s
epoch 62 | loss: 0.64315 | val_0_rmse: 0.78937 | val_1_rmse: 0.78278 |  0:02:01s
epoch 63 | loss: 0.64643 | val_0_rmse: 0.79076 | val_1_rmse: 0.77806 |  0:02:03s
epoch 64 | loss: 0.64582 | val_0_rmse: 0.80717 | val_1_rmse: 0.79073 |  0:02:05s
epoch 65 | loss: 0.64371 | val_0_rmse: 0.79083 | val_1_rmse: 0.76879 |  0:02:06s
epoch 66 | loss: 0.63932 | val_0_rmse: 0.78669 | val_1_rmse: 0.76872 |  0:02:08s
epoch 67 | loss: 0.63551 | val_0_rmse: 0.7794  | val_1_rmse: 0.7742  |  0:02:10s
epoch 68 | loss: 0.62913 | val_0_rmse: 0.77763 | val_1_rmse: 0.77586 |  0:02:12s
epoch 69 | loss: 0.6246  | val_0_rmse: 0.77475 | val_1_rmse: 0.76767 |  0:02:14s
epoch 70 | loss: 0.62711 | val_0_rmse: 0.7774  | val_1_rmse: 0.77081 |  0:02:16s
epoch 71 | loss: 0.62469 | val_0_rmse: 0.77938 | val_1_rmse: 0.77191 |  0:02:18s
epoch 72 | loss: 0.6159  | val_0_rmse: 0.77782 | val_1_rmse: 0.76794 |  0:02:20s
epoch 73 | loss: 0.62054 | val_0_rmse: 0.7718  | val_1_rmse: 0.76892 |  0:02:21s
epoch 74 | loss: 0.62291 | val_0_rmse: 0.78485 | val_1_rmse: 0.77481 |  0:02:23s
epoch 75 | loss: 0.61368 | val_0_rmse: 0.76645 | val_1_rmse: 0.76389 |  0:02:25s
epoch 76 | loss: 0.6213  | val_0_rmse: 0.78283 | val_1_rmse: 0.78174 |  0:02:27s
epoch 77 | loss: 0.62266 | val_0_rmse: 0.77627 | val_1_rmse: 0.7677  |  0:02:29s
epoch 78 | loss: 0.63103 | val_0_rmse: 0.77744 | val_1_rmse: 0.75971 |  0:02:31s
epoch 79 | loss: 0.62056 | val_0_rmse: 0.77444 | val_1_rmse: 0.77173 |  0:02:33s
epoch 80 | loss: 0.61385 | val_0_rmse: 0.76949 | val_1_rmse: 0.76377 |  0:02:34s
epoch 81 | loss: 0.60887 | val_0_rmse: 0.7654  | val_1_rmse: 0.75826 |  0:02:36s
epoch 82 | loss: 0.60629 | val_0_rmse: 0.7622  | val_1_rmse: 0.76329 |  0:02:38s
epoch 83 | loss: 0.60964 | val_0_rmse: 0.77134 | val_1_rmse: 0.7592  |  0:02:40s
epoch 84 | loss: 0.61044 | val_0_rmse: 0.75813 | val_1_rmse: 0.7566  |  0:02:42s
epoch 85 | loss: 0.60439 | val_0_rmse: 0.77144 | val_1_rmse: 0.77855 |  0:02:44s
epoch 86 | loss: 0.60157 | val_0_rmse: 0.75566 | val_1_rmse: 0.75302 |  0:02:46s
epoch 87 | loss: 0.59427 | val_0_rmse: 0.75333 | val_1_rmse: 0.76101 |  0:02:48s
epoch 88 | loss: 0.60077 | val_0_rmse: 0.76058 | val_1_rmse: 0.75917 |  0:02:49s
epoch 89 | loss: 0.59468 | val_0_rmse: 0.76539 | val_1_rmse: 0.75875 |  0:02:51s
epoch 90 | loss: 0.58976 | val_0_rmse: 0.74402 | val_1_rmse: 0.75364 |  0:02:53s
epoch 91 | loss: 0.60155 | val_0_rmse: 0.75823 | val_1_rmse: 0.7649  |  0:02:55s
epoch 92 | loss: 0.59977 | val_0_rmse: 0.75888 | val_1_rmse: 0.76164 |  0:02:57s
epoch 93 | loss: 0.59618 | val_0_rmse: 0.76549 | val_1_rmse: 0.76649 |  0:02:59s
epoch 94 | loss: 0.59236 | val_0_rmse: 0.76689 | val_1_rmse: 0.76321 |  0:03:01s
epoch 95 | loss: 0.58858 | val_0_rmse: 0.75248 | val_1_rmse: 0.7558  |  0:03:02s
epoch 96 | loss: 0.58784 | val_0_rmse: 0.74948 | val_1_rmse: 0.76455 |  0:03:04s
epoch 97 | loss: 0.59403 | val_0_rmse: 0.7432  | val_1_rmse: 0.7492  |  0:03:06s
epoch 98 | loss: 0.58347 | val_0_rmse: 0.75065 | val_1_rmse: 0.76461 |  0:03:08s
epoch 99 | loss: 0.58781 | val_0_rmse: 0.74417 | val_1_rmse: 0.75552 |  0:03:10s
epoch 100| loss: 0.58342 | val_0_rmse: 0.7433  | val_1_rmse: 0.75957 |  0:03:12s
epoch 101| loss: 0.58308 | val_0_rmse: 0.75836 | val_1_rmse: 0.77863 |  0:03:14s
epoch 102| loss: 0.57911 | val_0_rmse: 0.75859 | val_1_rmse: 0.76938 |  0:03:15s
epoch 103| loss: 0.58946 | val_0_rmse: 0.74172 | val_1_rmse: 0.75335 |  0:03:17s
epoch 104| loss: 0.57476 | val_0_rmse: 0.74744 | val_1_rmse: 0.77155 |  0:03:19s
epoch 105| loss: 0.58012 | val_0_rmse: 0.74363 | val_1_rmse: 0.74856 |  0:03:21s
epoch 106| loss: 0.57587 | val_0_rmse: 0.73327 | val_1_rmse: 0.75445 |  0:03:23s
epoch 107| loss: 0.56688 | val_0_rmse: 0.72789 | val_1_rmse: 0.75091 |  0:03:25s
epoch 108| loss: 0.5719  | val_0_rmse: 0.73867 | val_1_rmse: 0.76762 |  0:03:27s
epoch 109| loss: 0.58794 | val_0_rmse: 0.7384  | val_1_rmse: 0.75645 |  0:03:28s
epoch 110| loss: 0.57279 | val_0_rmse: 0.7564  | val_1_rmse: 0.76473 |  0:03:30s
epoch 111| loss: 0.60994 | val_0_rmse: 0.78393 | val_1_rmse: 0.78915 |  0:03:32s
epoch 112| loss: 0.59972 | val_0_rmse: 0.74102 | val_1_rmse: 0.75614 |  0:03:34s
epoch 113| loss: 0.59349 | val_0_rmse: 0.74984 | val_1_rmse: 0.7587  |  0:03:36s
epoch 114| loss: 0.5836  | val_0_rmse: 0.74567 | val_1_rmse: 0.75503 |  0:03:38s
epoch 115| loss: 0.56708 | val_0_rmse: 0.72613 | val_1_rmse: 0.74357 |  0:03:40s
epoch 116| loss: 0.56251 | val_0_rmse: 0.7381  | val_1_rmse: 0.76617 |  0:03:42s
epoch 117| loss: 0.5812  | val_0_rmse: 0.73573 | val_1_rmse: 0.75493 |  0:03:43s
epoch 118| loss: 0.57377 | val_0_rmse: 0.74657 | val_1_rmse: 0.76967 |  0:03:45s
epoch 119| loss: 0.56679 | val_0_rmse: 0.73014 | val_1_rmse: 0.75558 |  0:03:47s
epoch 120| loss: 0.56721 | val_0_rmse: 0.73768 | val_1_rmse: 0.76118 |  0:03:49s
epoch 121| loss: 0.56311 | val_0_rmse: 0.74416 | val_1_rmse: 0.77256 |  0:03:51s
epoch 122| loss: 0.58168 | val_0_rmse: 0.78411 | val_1_rmse: 0.80141 |  0:03:53s
epoch 123| loss: 0.5684  | val_0_rmse: 0.72629 | val_1_rmse: 0.7457  |  0:03:55s
epoch 124| loss: 0.55821 | val_0_rmse: 0.73044 | val_1_rmse: 0.75491 |  0:03:56s
epoch 125| loss: 0.54995 | val_0_rmse: 0.73205 | val_1_rmse: 0.76363 |  0:03:58s
epoch 126| loss: 0.55745 | val_0_rmse: 0.73068 | val_1_rmse: 0.75274 |  0:04:00s
epoch 127| loss: 0.54928 | val_0_rmse: 0.7182  | val_1_rmse: 0.75176 |  0:04:02s
epoch 128| loss: 0.5482  | val_0_rmse: 0.72218 | val_1_rmse: 0.74381 |  0:04:04s
epoch 129| loss: 0.54837 | val_0_rmse: 0.71393 | val_1_rmse: 0.74495 |  0:04:06s
epoch 130| loss: 0.54326 | val_0_rmse: 0.71636 | val_1_rmse: 0.74671 |  0:04:08s
epoch 131| loss: 0.54077 | val_0_rmse: 0.71497 | val_1_rmse: 0.74682 |  0:04:09s
epoch 132| loss: 0.54965 | val_0_rmse: 0.7098  | val_1_rmse: 0.74645 |  0:04:11s
epoch 133| loss: 0.54463 | val_0_rmse: 0.7209  | val_1_rmse: 0.76035 |  0:04:13s
epoch 134| loss: 0.5445  | val_0_rmse: 0.71537 | val_1_rmse: 0.75176 |  0:04:15s
epoch 135| loss: 0.53995 | val_0_rmse: 0.71472 | val_1_rmse: 0.74782 |  0:04:17s
epoch 136| loss: 0.53577 | val_0_rmse: 0.70723 | val_1_rmse: 0.7476  |  0:04:19s
epoch 137| loss: 0.53404 | val_0_rmse: 0.71209 | val_1_rmse: 0.74836 |  0:04:21s
epoch 138| loss: 0.53481 | val_0_rmse: 0.70111 | val_1_rmse: 0.73311 |  0:04:23s
epoch 139| loss: 0.52775 | val_0_rmse: 0.72328 | val_1_rmse: 0.75038 |  0:04:24s
epoch 140| loss: 0.53604 | val_0_rmse: 0.70953 | val_1_rmse: 0.74806 |  0:04:26s
epoch 141| loss: 0.5323  | val_0_rmse: 0.70545 | val_1_rmse: 0.74878 |  0:04:28s
epoch 142| loss: 0.51818 | val_0_rmse: 0.69868 | val_1_rmse: 0.74041 |  0:04:30s
epoch 143| loss: 0.52499 | val_0_rmse: 0.7033  | val_1_rmse: 0.75232 |  0:04:32s
epoch 144| loss: 0.52721 | val_0_rmse: 0.69767 | val_1_rmse: 0.74972 |  0:04:34s
epoch 145| loss: 0.51608 | val_0_rmse: 0.69261 | val_1_rmse: 0.7479  |  0:04:36s
epoch 146| loss: 0.52027 | val_0_rmse: 0.69634 | val_1_rmse: 0.7524  |  0:04:37s
epoch 147| loss: 0.51432 | val_0_rmse: 0.69215 | val_1_rmse: 0.74678 |  0:04:39s
epoch 148| loss: 0.5203  | val_0_rmse: 0.70165 | val_1_rmse: 0.74442 |  0:04:41s
epoch 149| loss: 0.52899 | val_0_rmse: 0.70874 | val_1_rmse: 0.7595  |  0:04:43s
Stop training because you reached max_epochs = 150 with best_epoch = 138 and best_val_1_rmse = 0.73311
Best weights from best epoch are automatically used!
ended training at: 03:58:08
Feature importance:
Mean squared error is of 0.04882053588536935
Mean absolute error:0.13731492119209596
MAPE:0.14247439995892952
R2 score:0.3479526200574562
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:58:09
epoch 0  | loss: 1.50249 | val_0_rmse: 0.9883  | val_1_rmse: 1.0211  |  0:00:01s
epoch 1  | loss: 0.90055 | val_0_rmse: 0.948   | val_1_rmse: 0.96315 |  0:00:03s
epoch 2  | loss: 0.80906 | val_0_rmse: 0.87631 | val_1_rmse: 0.89949 |  0:00:05s
epoch 3  | loss: 0.77778 | val_0_rmse: 0.88035 | val_1_rmse: 0.90342 |  0:00:07s
epoch 4  | loss: 0.7724  | val_0_rmse: 0.8789  | val_1_rmse: 0.90301 |  0:00:09s
epoch 5  | loss: 0.77565 | val_0_rmse: 0.87384 | val_1_rmse: 0.90164 |  0:00:11s
epoch 6  | loss: 0.77167 | val_0_rmse: 0.87797 | val_1_rmse: 0.90136 |  0:00:13s
epoch 7  | loss: 0.76656 | val_0_rmse: 0.87365 | val_1_rmse: 0.89977 |  0:00:14s
epoch 8  | loss: 0.77109 | val_0_rmse: 0.87551 | val_1_rmse: 0.9008  |  0:00:16s
epoch 9  | loss: 0.76858 | val_0_rmse: 0.87608 | val_1_rmse: 0.90268 |  0:00:18s
epoch 10 | loss: 0.76986 | val_0_rmse: 0.87376 | val_1_rmse: 0.8992  |  0:00:20s
epoch 11 | loss: 0.76695 | val_0_rmse: 0.87432 | val_1_rmse: 0.90233 |  0:00:22s
epoch 12 | loss: 0.76788 | val_0_rmse: 0.87284 | val_1_rmse: 0.89923 |  0:00:24s
epoch 13 | loss: 0.76766 | val_0_rmse: 0.87409 | val_1_rmse: 0.90026 |  0:00:26s
epoch 14 | loss: 0.7639  | val_0_rmse: 0.8723  | val_1_rmse: 0.89871 |  0:00:27s
epoch 15 | loss: 0.76232 | val_0_rmse: 0.87115 | val_1_rmse: 0.8971  |  0:00:29s
epoch 16 | loss: 0.75968 | val_0_rmse: 0.87495 | val_1_rmse: 0.90219 |  0:00:31s
epoch 17 | loss: 0.7619  | val_0_rmse: 0.87136 | val_1_rmse: 0.89721 |  0:00:33s
epoch 18 | loss: 0.75959 | val_0_rmse: 0.86873 | val_1_rmse: 0.89464 |  0:00:35s
epoch 19 | loss: 0.75931 | val_0_rmse: 0.86959 | val_1_rmse: 0.89705 |  0:00:37s
epoch 20 | loss: 0.75476 | val_0_rmse: 0.86651 | val_1_rmse: 0.89513 |  0:00:39s
epoch 21 | loss: 0.74996 | val_0_rmse: 0.86292 | val_1_rmse: 0.89134 |  0:00:40s
epoch 22 | loss: 0.74391 | val_0_rmse: 0.86047 | val_1_rmse: 0.88963 |  0:00:42s
epoch 23 | loss: 0.73834 | val_0_rmse: 0.85209 | val_1_rmse: 0.88046 |  0:00:44s
epoch 24 | loss: 0.72517 | val_0_rmse: 0.84137 | val_1_rmse: 0.88473 |  0:00:46s
epoch 25 | loss: 0.7105  | val_0_rmse: 0.83359 | val_1_rmse: 0.87814 |  0:00:48s
epoch 26 | loss: 0.69883 | val_0_rmse: 0.82417 | val_1_rmse: 0.88105 |  0:00:50s
epoch 27 | loss: 0.68592 | val_0_rmse: 0.82916 | val_1_rmse: 0.86967 |  0:00:52s
epoch 28 | loss: 0.69292 | val_0_rmse: 0.81244 | val_1_rmse: 0.85678 |  0:00:54s
epoch 29 | loss: 0.6621  | val_0_rmse: 0.79112 | val_1_rmse: 0.83993 |  0:00:55s
epoch 30 | loss: 0.64874 | val_0_rmse: 0.79281 | val_1_rmse: 0.83589 |  0:00:57s
epoch 31 | loss: 0.62981 | val_0_rmse: 0.77688 | val_1_rmse: 0.83299 |  0:00:59s
epoch 32 | loss: 0.62604 | val_0_rmse: 0.7798  | val_1_rmse: 0.82894 |  0:01:01s
epoch 33 | loss: 0.62215 | val_0_rmse: 0.76892 | val_1_rmse: 0.82542 |  0:01:03s
epoch 34 | loss: 0.61178 | val_0_rmse: 0.76267 | val_1_rmse: 0.81473 |  0:01:05s
epoch 35 | loss: 0.61022 | val_0_rmse: 0.77192 | val_1_rmse: 0.81876 |  0:01:07s
epoch 36 | loss: 0.60418 | val_0_rmse: 0.75794 | val_1_rmse: 0.81556 |  0:01:08s
epoch 37 | loss: 0.58837 | val_0_rmse: 0.75381 | val_1_rmse: 0.81219 |  0:01:10s
epoch 38 | loss: 0.57481 | val_0_rmse: 0.7407  | val_1_rmse: 0.80176 |  0:01:12s
epoch 39 | loss: 0.57309 | val_0_rmse: 0.73685 | val_1_rmse: 0.80074 |  0:01:14s
epoch 40 | loss: 0.57367 | val_0_rmse: 0.74256 | val_1_rmse: 0.80103 |  0:01:16s
epoch 41 | loss: 0.5699  | val_0_rmse: 0.734   | val_1_rmse: 0.80161 |  0:01:18s
epoch 42 | loss: 0.55799 | val_0_rmse: 0.71922 | val_1_rmse: 0.78946 |  0:01:20s
epoch 43 | loss: 0.54933 | val_0_rmse: 0.72872 | val_1_rmse: 0.79931 |  0:01:21s
epoch 44 | loss: 0.54888 | val_0_rmse: 0.73298 | val_1_rmse: 0.79113 |  0:01:23s
epoch 45 | loss: 0.55846 | val_0_rmse: 0.73152 | val_1_rmse: 0.80549 |  0:01:25s
epoch 46 | loss: 0.5439  | val_0_rmse: 0.73275 | val_1_rmse: 0.79889 |  0:01:27s
epoch 47 | loss: 0.55087 | val_0_rmse: 0.71643 | val_1_rmse: 0.79678 |  0:01:29s
epoch 48 | loss: 0.54292 | val_0_rmse: 0.72962 | val_1_rmse: 0.79944 |  0:01:31s
epoch 49 | loss: 0.5406  | val_0_rmse: 0.70683 | val_1_rmse: 0.79448 |  0:01:33s
epoch 50 | loss: 0.53486 | val_0_rmse: 0.70611 | val_1_rmse: 0.78781 |  0:01:34s
epoch 51 | loss: 0.54077 | val_0_rmse: 0.70703 | val_1_rmse: 0.79323 |  0:01:36s
epoch 52 | loss: 0.52974 | val_0_rmse: 0.70884 | val_1_rmse: 0.79483 |  0:01:38s
epoch 53 | loss: 0.52473 | val_0_rmse: 0.70421 | val_1_rmse: 0.79595 |  0:01:40s
epoch 54 | loss: 0.50857 | val_0_rmse: 0.69394 | val_1_rmse: 0.80244 |  0:01:42s
epoch 55 | loss: 0.50998 | val_0_rmse: 0.68831 | val_1_rmse: 0.78724 |  0:01:44s
epoch 56 | loss: 0.50547 | val_0_rmse: 0.7114  | val_1_rmse: 0.8265  |  0:01:46s
epoch 57 | loss: 0.50024 | val_0_rmse: 0.69013 | val_1_rmse: 0.7905  |  0:01:47s
epoch 58 | loss: 0.49539 | val_0_rmse: 0.68025 | val_1_rmse: 0.78736 |  0:01:49s
epoch 59 | loss: 0.50162 | val_0_rmse: 0.68703 | val_1_rmse: 0.79211 |  0:01:51s
epoch 60 | loss: 0.49406 | val_0_rmse: 0.67771 | val_1_rmse: 0.78797 |  0:01:53s
epoch 61 | loss: 0.48491 | val_0_rmse: 0.67784 | val_1_rmse: 0.78926 |  0:01:55s
epoch 62 | loss: 0.49242 | val_0_rmse: 0.68382 | val_1_rmse: 0.79815 |  0:01:57s
epoch 63 | loss: 0.4892  | val_0_rmse: 0.67669 | val_1_rmse: 0.81093 |  0:01:59s
epoch 64 | loss: 0.48537 | val_0_rmse: 0.67824 | val_1_rmse: 0.80185 |  0:02:01s
epoch 65 | loss: 0.48056 | val_0_rmse: 0.688   | val_1_rmse: 0.79632 |  0:02:02s
epoch 66 | loss: 0.48811 | val_0_rmse: 0.69888 | val_1_rmse: 0.82157 |  0:02:04s
epoch 67 | loss: 0.48751 | val_0_rmse: 0.66509 | val_1_rmse: 0.79887 |  0:02:06s
epoch 68 | loss: 0.4675  | val_0_rmse: 0.66088 | val_1_rmse: 0.78753 |  0:02:08s
epoch 69 | loss: 0.46906 | val_0_rmse: 0.68356 | val_1_rmse: 0.7978  |  0:02:10s
epoch 70 | loss: 0.47296 | val_0_rmse: 0.67168 | val_1_rmse: 0.7963  |  0:02:12s
epoch 71 | loss: 0.45898 | val_0_rmse: 0.65565 | val_1_rmse: 0.79249 |  0:02:14s
epoch 72 | loss: 0.46253 | val_0_rmse: 0.65729 | val_1_rmse: 0.79132 |  0:02:15s
epoch 73 | loss: 0.46319 | val_0_rmse: 0.65509 | val_1_rmse: 0.79363 |  0:02:17s
epoch 74 | loss: 0.45355 | val_0_rmse: 0.64892 | val_1_rmse: 0.79952 |  0:02:19s
epoch 75 | loss: 0.4557  | val_0_rmse: 0.65021 | val_1_rmse: 0.78831 |  0:02:21s
epoch 76 | loss: 0.44553 | val_0_rmse: 0.65889 | val_1_rmse: 0.7974  |  0:02:23s
epoch 77 | loss: 0.44556 | val_0_rmse: 0.63933 | val_1_rmse: 0.78594 |  0:02:25s
epoch 78 | loss: 0.4446  | val_0_rmse: 0.63562 | val_1_rmse: 0.78742 |  0:02:27s
epoch 79 | loss: 0.44565 | val_0_rmse: 0.6458  | val_1_rmse: 0.8053  |  0:02:28s
epoch 80 | loss: 0.44386 | val_0_rmse: 0.65974 | val_1_rmse: 0.82006 |  0:02:30s
epoch 81 | loss: 0.44053 | val_0_rmse: 0.63837 | val_1_rmse: 0.79103 |  0:02:32s
epoch 82 | loss: 0.44972 | val_0_rmse: 0.65458 | val_1_rmse: 0.81752 |  0:02:34s
epoch 83 | loss: 0.45762 | val_0_rmse: 0.65342 | val_1_rmse: 0.7926  |  0:02:36s
epoch 84 | loss: 0.43568 | val_0_rmse: 0.63728 | val_1_rmse: 0.80516 |  0:02:38s
epoch 85 | loss: 0.43249 | val_0_rmse: 0.63463 | val_1_rmse: 0.78634 |  0:02:40s
epoch 86 | loss: 0.44111 | val_0_rmse: 0.63673 | val_1_rmse: 0.78616 |  0:02:41s
epoch 87 | loss: 0.42796 | val_0_rmse: 0.62777 | val_1_rmse: 0.78293 |  0:02:43s
epoch 88 | loss: 0.42506 | val_0_rmse: 0.62164 | val_1_rmse: 0.78472 |  0:02:45s
epoch 89 | loss: 0.42004 | val_0_rmse: 0.61757 | val_1_rmse: 0.79356 |  0:02:47s
epoch 90 | loss: 0.4195  | val_0_rmse: 0.62283 | val_1_rmse: 0.80934 |  0:02:49s
epoch 91 | loss: 0.41443 | val_0_rmse: 0.62655 | val_1_rmse: 0.79777 |  0:02:51s
epoch 92 | loss: 0.42548 | val_0_rmse: 0.62777 | val_1_rmse: 0.78074 |  0:02:53s
epoch 93 | loss: 0.43676 | val_0_rmse: 0.636   | val_1_rmse: 0.80091 |  0:02:55s
epoch 94 | loss: 0.42481 | val_0_rmse: 0.61712 | val_1_rmse: 0.78438 |  0:02:56s
epoch 95 | loss: 0.41563 | val_0_rmse: 0.623   | val_1_rmse: 0.78716 |  0:02:58s
epoch 96 | loss: 0.41015 | val_0_rmse: 0.61561 | val_1_rmse: 0.7961  |  0:03:00s
epoch 97 | loss: 0.40771 | val_0_rmse: 0.61884 | val_1_rmse: 0.79175 |  0:03:02s
epoch 98 | loss: 0.41    | val_0_rmse: 0.61128 | val_1_rmse: 0.80727 |  0:03:04s
epoch 99 | loss: 0.40728 | val_0_rmse: 0.62073 | val_1_rmse: 0.81613 |  0:03:06s
epoch 100| loss: 0.40879 | val_0_rmse: 0.60973 | val_1_rmse: 0.80333 |  0:03:08s
epoch 101| loss: 0.40035 | val_0_rmse: 0.61283 | val_1_rmse: 0.81265 |  0:03:09s
epoch 102| loss: 0.41585 | val_0_rmse: 0.63071 | val_1_rmse: 0.8181  |  0:03:11s
epoch 103| loss: 0.41104 | val_0_rmse: 0.6228  | val_1_rmse: 0.80923 |  0:03:13s
epoch 104| loss: 0.4025  | val_0_rmse: 0.64215 | val_1_rmse: 0.86357 |  0:03:15s
epoch 105| loss: 0.39724 | val_0_rmse: 0.61293 | val_1_rmse: 0.79694 |  0:03:17s
epoch 106| loss: 0.40709 | val_0_rmse: 0.59786 | val_1_rmse: 0.81676 |  0:03:19s
epoch 107| loss: 0.40199 | val_0_rmse: 0.5909  | val_1_rmse: 0.79497 |  0:03:21s
epoch 108| loss: 0.40103 | val_0_rmse: 0.60764 | val_1_rmse: 0.80153 |  0:03:22s
epoch 109| loss: 0.39633 | val_0_rmse: 0.59965 | val_1_rmse: 0.79684 |  0:03:24s
epoch 110| loss: 0.39608 | val_0_rmse: 0.60645 | val_1_rmse: 0.80458 |  0:03:26s
epoch 111| loss: 0.40364 | val_0_rmse: 0.60386 | val_1_rmse: 0.81568 |  0:03:28s
epoch 112| loss: 0.40024 | val_0_rmse: 0.59078 | val_1_rmse: 0.79535 |  0:03:30s
epoch 113| loss: 0.38277 | val_0_rmse: 0.60665 | val_1_rmse: 0.81241 |  0:03:32s
epoch 114| loss: 0.37617 | val_0_rmse: 0.5767  | val_1_rmse: 0.80968 |  0:03:34s
epoch 115| loss: 0.38165 | val_0_rmse: 0.60186 | val_1_rmse: 0.79311 |  0:03:35s
epoch 116| loss: 0.38449 | val_0_rmse: 0.58618 | val_1_rmse: 0.80655 |  0:03:37s
epoch 117| loss: 0.40056 | val_0_rmse: 0.6166  | val_1_rmse: 0.81382 |  0:03:39s
epoch 118| loss: 0.39768 | val_0_rmse: 0.60353 | val_1_rmse: 0.80052 |  0:03:41s
epoch 119| loss: 0.38473 | val_0_rmse: 0.59195 | val_1_rmse: 0.80719 |  0:03:43s
epoch 120| loss: 0.37683 | val_0_rmse: 0.58718 | val_1_rmse: 0.79824 |  0:03:45s
epoch 121| loss: 0.37539 | val_0_rmse: 0.5772  | val_1_rmse: 0.80416 |  0:03:47s
epoch 122| loss: 0.37347 | val_0_rmse: 0.62045 | val_1_rmse: 0.82366 |  0:03:48s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.78074
Best weights from best epoch are automatically used!
ended training at: 04:01:58
Feature importance:
Mean squared error is of 0.049836708249605774
Mean absolute error:0.13527382899641005
MAPE:0.1412241933022604
R2 score:0.3937977950985825
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:01:59
epoch 0  | loss: 1.35179 | val_0_rmse: 0.99696 | val_1_rmse: 1.02934 |  0:00:01s
epoch 1  | loss: 1.00414 | val_0_rmse: 0.99758 | val_1_rmse: 1.0304  |  0:00:03s
epoch 2  | loss: 0.99762 | val_0_rmse: 0.99606 | val_1_rmse: 1.02804 |  0:00:05s
epoch 3  | loss: 0.99361 | val_0_rmse: 0.98438 | val_1_rmse: 1.01897 |  0:00:07s
epoch 4  | loss: 0.98019 | val_0_rmse: 0.97162 | val_1_rmse: 1.00651 |  0:00:09s
epoch 5  | loss: 0.95526 | val_0_rmse: 0.93643 | val_1_rmse: 0.96988 |  0:00:11s
epoch 6  | loss: 0.88318 | val_0_rmse: 0.9023  | val_1_rmse: 0.94232 |  0:00:13s
epoch 7  | loss: 0.80232 | val_0_rmse: 0.87768 | val_1_rmse: 0.91237 |  0:00:14s
epoch 8  | loss: 0.75773 | val_0_rmse: 0.86846 | val_1_rmse: 0.89883 |  0:00:16s
epoch 9  | loss: 0.71721 | val_0_rmse: 0.84523 | val_1_rmse: 0.87803 |  0:00:18s
epoch 10 | loss: 0.69002 | val_0_rmse: 0.84232 | val_1_rmse: 0.87665 |  0:00:20s
epoch 11 | loss: 0.67379 | val_0_rmse: 0.83907 | val_1_rmse: 0.87735 |  0:00:22s
epoch 12 | loss: 0.65617 | val_0_rmse: 0.83008 | val_1_rmse: 0.86444 |  0:00:24s
epoch 13 | loss: 0.66774 | val_0_rmse: 0.83113 | val_1_rmse: 0.86818 |  0:00:26s
epoch 14 | loss: 0.65887 | val_0_rmse: 0.83408 | val_1_rmse: 0.86597 |  0:00:28s
epoch 15 | loss: 0.64374 | val_0_rmse: 0.81822 | val_1_rmse: 0.86272 |  0:00:29s
epoch 16 | loss: 0.62951 | val_0_rmse: 0.82135 | val_1_rmse: 0.85515 |  0:00:31s
epoch 17 | loss: 0.61182 | val_0_rmse: 0.79536 | val_1_rmse: 0.83542 |  0:00:33s
epoch 18 | loss: 0.59666 | val_0_rmse: 0.8274  | val_1_rmse: 0.86038 |  0:00:35s
epoch 19 | loss: 0.59422 | val_0_rmse: 0.79365 | val_1_rmse: 0.83908 |  0:00:37s
epoch 20 | loss: 0.5799  | val_0_rmse: 0.78716 | val_1_rmse: 0.82473 |  0:00:39s
epoch 21 | loss: 0.57631 | val_0_rmse: 0.81436 | val_1_rmse: 0.86581 |  0:00:41s
epoch 22 | loss: 0.56027 | val_0_rmse: 0.77061 | val_1_rmse: 0.82766 |  0:00:42s
epoch 23 | loss: 0.56309 | val_0_rmse: 0.77726 | val_1_rmse: 0.83524 |  0:00:44s
epoch 24 | loss: 0.55451 | val_0_rmse: 0.80735 | val_1_rmse: 0.84905 |  0:00:46s
epoch 25 | loss: 0.55286 | val_0_rmse: 0.75686 | val_1_rmse: 0.81239 |  0:00:48s
epoch 26 | loss: 0.54344 | val_0_rmse: 0.73194 | val_1_rmse: 0.79424 |  0:00:50s
epoch 27 | loss: 0.53759 | val_0_rmse: 0.76692 | val_1_rmse: 0.83194 |  0:00:52s
epoch 28 | loss: 0.54365 | val_0_rmse: 0.72174 | val_1_rmse: 0.78879 |  0:00:54s
epoch 29 | loss: 0.53422 | val_0_rmse: 0.73769 | val_1_rmse: 0.82129 |  0:00:55s
epoch 30 | loss: 0.52538 | val_0_rmse: 0.71328 | val_1_rmse: 0.78875 |  0:00:57s
epoch 31 | loss: 0.51765 | val_0_rmse: 0.70522 | val_1_rmse: 0.78708 |  0:00:59s
epoch 32 | loss: 0.51805 | val_0_rmse: 0.71878 | val_1_rmse: 0.79754 |  0:01:01s
epoch 33 | loss: 0.51062 | val_0_rmse: 0.70794 | val_1_rmse: 0.79546 |  0:01:03s
epoch 34 | loss: 0.50196 | val_0_rmse: 0.73637 | val_1_rmse: 0.82998 |  0:01:05s
epoch 35 | loss: 0.51254 | val_0_rmse: 0.69299 | val_1_rmse: 0.79013 |  0:01:07s
epoch 36 | loss: 0.49329 | val_0_rmse: 0.7042  | val_1_rmse: 0.80799 |  0:01:09s
epoch 37 | loss: 0.49871 | val_0_rmse: 0.69377 | val_1_rmse: 0.80953 |  0:01:10s
epoch 38 | loss: 0.50004 | val_0_rmse: 0.67457 | val_1_rmse: 0.79042 |  0:01:12s
epoch 39 | loss: 0.49475 | val_0_rmse: 0.70881 | val_1_rmse: 0.80865 |  0:01:14s
epoch 40 | loss: 0.49117 | val_0_rmse: 0.68955 | val_1_rmse: 0.7952  |  0:01:16s
epoch 41 | loss: 0.4768  | val_0_rmse: 0.72703 | val_1_rmse: 0.83404 |  0:01:18s
epoch 42 | loss: 0.47504 | val_0_rmse: 0.68223 | val_1_rmse: 0.82221 |  0:01:20s
epoch 43 | loss: 0.47572 | val_0_rmse: 0.66381 | val_1_rmse: 0.79445 |  0:01:22s
epoch 44 | loss: 0.48741 | val_0_rmse: 0.75273 | val_1_rmse: 0.90616 |  0:01:23s
epoch 45 | loss: 0.47979 | val_0_rmse: 0.66928 | val_1_rmse: 0.8018  |  0:01:25s
epoch 46 | loss: 0.46956 | val_0_rmse: 0.6803  | val_1_rmse: 0.81209 |  0:01:27s
epoch 47 | loss: 0.46325 | val_0_rmse: 0.657   | val_1_rmse: 0.82858 |  0:01:29s
epoch 48 | loss: 0.46261 | val_0_rmse: 0.67785 | val_1_rmse: 0.81867 |  0:01:31s
epoch 49 | loss: 0.46584 | val_0_rmse: 0.66996 | val_1_rmse: 0.81809 |  0:01:33s
epoch 50 | loss: 0.45639 | val_0_rmse: 0.68322 | val_1_rmse: 0.84314 |  0:01:35s
epoch 51 | loss: 0.4548  | val_0_rmse: 0.64548 | val_1_rmse: 0.82046 |  0:01:36s
epoch 52 | loss: 0.46666 | val_0_rmse: 0.68071 | val_1_rmse: 0.82142 |  0:01:38s
epoch 53 | loss: 0.45283 | val_0_rmse: 0.64439 | val_1_rmse: 0.80939 |  0:01:40s
epoch 54 | loss: 0.4457  | val_0_rmse: 0.63652 | val_1_rmse: 0.81472 |  0:01:42s
epoch 55 | loss: 0.43632 | val_0_rmse: 0.63706 | val_1_rmse: 0.83521 |  0:01:44s
epoch 56 | loss: 0.43895 | val_0_rmse: 0.75247 | val_1_rmse: 0.87304 |  0:01:46s
epoch 57 | loss: 0.4264  | val_0_rmse: 0.63522 | val_1_rmse: 0.83877 |  0:01:48s
epoch 58 | loss: 0.42945 | val_0_rmse: 0.62516 | val_1_rmse: 0.81184 |  0:01:49s
epoch 59 | loss: 0.43303 | val_0_rmse: 0.64539 | val_1_rmse: 0.81282 |  0:01:51s
epoch 60 | loss: 0.42885 | val_0_rmse: 0.63393 | val_1_rmse: 0.82894 |  0:01:53s
epoch 61 | loss: 0.4302  | val_0_rmse: 0.62918 | val_1_rmse: 0.81684 |  0:01:55s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.78708
Best weights from best epoch are automatically used!
ended training at: 04:03:55
Feature importance:
Mean squared error is of 0.04356607890716694
Mean absolute error:0.13181692878429285
MAPE:0.13947011936563683
R2 score:0.37854019626354674
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:03:56
epoch 0  | loss: 1.29671 | val_0_rmse: 0.98704 | val_1_rmse: 1.00599 |  0:00:01s
epoch 1  | loss: 0.97254 | val_0_rmse: 0.97621 | val_1_rmse: 0.99607 |  0:00:03s
epoch 2  | loss: 0.96201 | val_0_rmse: 0.96881 | val_1_rmse: 0.98696 |  0:00:05s
epoch 3  | loss: 0.94603 | val_0_rmse: 0.95348 | val_1_rmse: 0.96313 |  0:00:07s
epoch 4  | loss: 0.921   | val_0_rmse: 0.94195 | val_1_rmse: 0.94283 |  0:00:09s
epoch 5  | loss: 0.88972 | val_0_rmse: 0.91961 | val_1_rmse: 0.92507 |  0:00:11s
epoch 6  | loss: 0.86452 | val_0_rmse: 0.91512 | val_1_rmse: 0.91923 |  0:00:13s
epoch 7  | loss: 0.85162 | val_0_rmse: 0.90596 | val_1_rmse: 0.91207 |  0:00:14s
epoch 8  | loss: 0.83154 | val_0_rmse: 0.88918 | val_1_rmse: 0.89436 |  0:00:16s
epoch 9  | loss: 0.80096 | val_0_rmse: 0.8999  | val_1_rmse: 0.91014 |  0:00:18s
epoch 10 | loss: 0.77531 | val_0_rmse: 0.86586 | val_1_rmse: 0.87377 |  0:00:20s
epoch 11 | loss: 0.73665 | val_0_rmse: 0.85143 | val_1_rmse: 0.85974 |  0:00:22s
epoch 12 | loss: 0.70865 | val_0_rmse: 0.83914 | val_1_rmse: 0.85053 |  0:00:24s
epoch 13 | loss: 0.69559 | val_0_rmse: 0.83875 | val_1_rmse: 0.85154 |  0:00:26s
epoch 14 | loss: 0.68662 | val_0_rmse: 0.86332 | val_1_rmse: 0.87447 |  0:00:28s
epoch 15 | loss: 0.66913 | val_0_rmse: 0.84449 | val_1_rmse: 0.85875 |  0:00:29s
epoch 16 | loss: 0.65383 | val_0_rmse: 0.84181 | val_1_rmse: 0.85759 |  0:00:31s
epoch 17 | loss: 0.64547 | val_0_rmse: 0.8626  | val_1_rmse: 0.88783 |  0:00:33s
epoch 18 | loss: 0.63751 | val_0_rmse: 0.81991 | val_1_rmse: 0.84572 |  0:00:35s
epoch 19 | loss: 0.6297  | val_0_rmse: 0.80121 | val_1_rmse: 0.82823 |  0:00:37s
epoch 20 | loss: 0.62464 | val_0_rmse: 0.82261 | val_1_rmse: 0.85073 |  0:00:39s
epoch 21 | loss: 0.62787 | val_0_rmse: 0.81383 | val_1_rmse: 0.83955 |  0:00:41s
epoch 22 | loss: 0.60579 | val_0_rmse: 0.78387 | val_1_rmse: 0.82201 |  0:00:42s
epoch 23 | loss: 0.6048  | val_0_rmse: 0.79171 | val_1_rmse: 0.82803 |  0:00:44s
epoch 24 | loss: 0.5934  | val_0_rmse: 0.77363 | val_1_rmse: 0.80908 |  0:00:46s
epoch 25 | loss: 0.59705 | val_0_rmse: 0.76437 | val_1_rmse: 0.81102 |  0:00:48s
epoch 26 | loss: 0.57212 | val_0_rmse: 0.79464 | val_1_rmse: 0.82846 |  0:00:50s
epoch 27 | loss: 0.57494 | val_0_rmse: 0.74987 | val_1_rmse: 0.80178 |  0:00:52s
epoch 28 | loss: 0.56168 | val_0_rmse: 0.74152 | val_1_rmse: 0.79869 |  0:00:54s
epoch 29 | loss: 0.56991 | val_0_rmse: 0.72481 | val_1_rmse: 0.7792  |  0:00:55s
epoch 30 | loss: 0.55193 | val_0_rmse: 0.74524 | val_1_rmse: 0.79405 |  0:00:57s
epoch 31 | loss: 0.54593 | val_0_rmse: 0.7325  | val_1_rmse: 0.80068 |  0:00:59s
epoch 32 | loss: 0.54033 | val_0_rmse: 0.74712 | val_1_rmse: 0.79964 |  0:01:01s
epoch 33 | loss: 0.53544 | val_0_rmse: 0.724   | val_1_rmse: 0.80657 |  0:01:03s
epoch 34 | loss: 0.52653 | val_0_rmse: 0.7257  | val_1_rmse: 0.78417 |  0:01:05s
epoch 35 | loss: 0.52341 | val_0_rmse: 0.70697 | val_1_rmse: 0.79023 |  0:01:07s
epoch 36 | loss: 0.51883 | val_0_rmse: 0.70397 | val_1_rmse: 0.77963 |  0:01:09s
epoch 37 | loss: 0.52128 | val_0_rmse: 0.68942 | val_1_rmse: 0.77438 |  0:01:10s
epoch 38 | loss: 0.51956 | val_0_rmse: 0.7751  | val_1_rmse: 0.82563 |  0:01:12s
epoch 39 | loss: 0.51527 | val_0_rmse: 0.78871 | val_1_rmse: 0.83452 |  0:01:14s
epoch 40 | loss: 0.50638 | val_0_rmse: 1.5881  | val_1_rmse: 1.66615 |  0:01:16s
epoch 41 | loss: 0.51068 | val_0_rmse: 0.86313 | val_1_rmse: 0.9052  |  0:01:18s
epoch 42 | loss: 0.49845 | val_0_rmse: 0.68496 | val_1_rmse: 0.77545 |  0:01:20s
epoch 43 | loss: 0.48796 | val_0_rmse: 0.69339 | val_1_rmse: 0.78127 |  0:01:22s
epoch 44 | loss: 0.49836 | val_0_rmse: 0.70778 | val_1_rmse: 0.78427 |  0:01:23s
epoch 45 | loss: 0.48954 | val_0_rmse: 0.72049 | val_1_rmse: 0.82757 |  0:01:25s
epoch 46 | loss: 0.4803  | val_0_rmse: 0.75872 | val_1_rmse: 0.89963 |  0:01:27s
epoch 47 | loss: 0.48455 | val_0_rmse: 0.80803 | val_1_rmse: 0.85569 |  0:01:29s
epoch 48 | loss: 0.46969 | val_0_rmse: 0.76068 | val_1_rmse: 0.8861  |  0:01:31s
epoch 49 | loss: 0.47663 | val_0_rmse: 0.83475 | val_1_rmse: 0.87834 |  0:01:33s
epoch 50 | loss: 0.48137 | val_0_rmse: 0.67053 | val_1_rmse: 0.77693 |  0:01:35s
epoch 51 | loss: 0.46999 | val_0_rmse: 0.82546 | val_1_rmse: 0.9665  |  0:01:36s
epoch 52 | loss: 0.46761 | val_0_rmse: 0.6905  | val_1_rmse: 0.77493 |  0:01:38s
epoch 53 | loss: 0.46147 | val_0_rmse: 0.79073 | val_1_rmse: 0.84528 |  0:01:40s
epoch 54 | loss: 0.47875 | val_0_rmse: 0.7305  | val_1_rmse: 0.87476 |  0:01:42s
epoch 55 | loss: 0.47601 | val_0_rmse: 0.65838 | val_1_rmse: 0.78872 |  0:01:44s
epoch 56 | loss: 0.46461 | val_0_rmse: 0.72726 | val_1_rmse: 0.80098 |  0:01:46s
epoch 57 | loss: 0.46806 | val_0_rmse: 0.70384 | val_1_rmse: 0.84874 |  0:01:48s
epoch 58 | loss: 0.46297 | val_0_rmse: 0.6664  | val_1_rmse: 0.78289 |  0:01:50s
epoch 59 | loss: 0.45792 | val_0_rmse: 0.76172 | val_1_rmse: 0.82247 |  0:01:51s
epoch 60 | loss: 0.4607  | val_0_rmse: 0.72077 | val_1_rmse: 0.79995 |  0:01:53s
epoch 61 | loss: 0.44902 | val_0_rmse: 0.99666 | val_1_rmse: 1.12587 |  0:01:55s
epoch 62 | loss: 0.44701 | val_0_rmse: 0.63949 | val_1_rmse: 0.77298 |  0:01:57s
epoch 63 | loss: 0.44528 | val_0_rmse: 0.67441 | val_1_rmse: 0.78804 |  0:01:59s
epoch 64 | loss: 0.45171 | val_0_rmse: 0.65039 | val_1_rmse: 0.79875 |  0:02:01s
epoch 65 | loss: 0.45772 | val_0_rmse: 0.71132 | val_1_rmse: 0.81663 |  0:02:03s
epoch 66 | loss: 0.44921 | val_0_rmse: 0.64497 | val_1_rmse: 0.79193 |  0:02:04s
epoch 67 | loss: 0.4371  | val_0_rmse: 0.74177 | val_1_rmse: 0.8279  |  0:02:06s
epoch 68 | loss: 0.43365 | val_0_rmse: 0.88734 | val_1_rmse: 1.04509 |  0:02:08s
epoch 69 | loss: 0.43356 | val_0_rmse: 0.66574 | val_1_rmse: 0.82508 |  0:02:10s
epoch 70 | loss: 0.43609 | val_0_rmse: 0.64746 | val_1_rmse: 0.77055 |  0:02:12s
epoch 71 | loss: 0.43014 | val_0_rmse: 0.6397  | val_1_rmse: 0.77697 |  0:02:14s
epoch 72 | loss: 0.4276  | val_0_rmse: 0.77215 | val_1_rmse: 0.84038 |  0:02:16s
epoch 73 | loss: 0.42618 | val_0_rmse: 0.6491  | val_1_rmse: 0.77652 |  0:02:18s
epoch 74 | loss: 0.42203 | val_0_rmse: 0.74129 | val_1_rmse: 0.82335 |  0:02:19s
epoch 75 | loss: 0.42608 | val_0_rmse: 0.67437 | val_1_rmse: 0.79034 |  0:02:21s
epoch 76 | loss: 0.41551 | val_0_rmse: 0.63408 | val_1_rmse: 0.76967 |  0:02:23s
epoch 77 | loss: 0.42213 | val_0_rmse: 0.6271  | val_1_rmse: 0.79208 |  0:02:25s
epoch 78 | loss: 0.4195  | val_0_rmse: 0.6556  | val_1_rmse: 0.78064 |  0:02:27s
epoch 79 | loss: 0.41105 | val_0_rmse: 0.78178 | val_1_rmse: 0.85441 |  0:02:29s
epoch 80 | loss: 0.42105 | val_0_rmse: 0.68153 | val_1_rmse: 0.85339 |  0:02:31s
epoch 81 | loss: 0.42957 | val_0_rmse: 1.31652 | val_1_rmse: 1.44251 |  0:02:32s
epoch 82 | loss: 0.44944 | val_0_rmse: 0.82021 | val_1_rmse: 0.87016 |  0:02:34s
epoch 83 | loss: 0.42687 | val_0_rmse: 0.63487 | val_1_rmse: 0.79131 |  0:02:36s
epoch 84 | loss: 0.4103  | val_0_rmse: 0.67365 | val_1_rmse: 0.81453 |  0:02:38s
epoch 85 | loss: 0.40679 | val_0_rmse: 0.61445 | val_1_rmse: 0.81381 |  0:02:40s
epoch 86 | loss: 0.40047 | val_0_rmse: 0.64816 | val_1_rmse: 0.85466 |  0:02:42s
epoch 87 | loss: 0.4004  | val_0_rmse: 0.61458 | val_1_rmse: 0.79236 |  0:02:44s
epoch 88 | loss: 0.40702 | val_0_rmse: 0.65103 | val_1_rmse: 0.7888  |  0:02:46s
epoch 89 | loss: 0.41429 | val_0_rmse: 0.64415 | val_1_rmse: 0.81286 |  0:02:47s
epoch 90 | loss: 0.40442 | val_0_rmse: 0.62149 | val_1_rmse: 0.79026 |  0:02:49s
epoch 91 | loss: 0.40676 | val_0_rmse: 0.74848 | val_1_rmse: 0.85374 |  0:02:51s
epoch 92 | loss: 0.41732 | val_0_rmse: 0.67278 | val_1_rmse: 0.84429 |  0:02:53s
epoch 93 | loss: 0.39169 | val_0_rmse: 0.78451 | val_1_rmse: 0.90159 |  0:02:55s
epoch 94 | loss: 0.40221 | val_0_rmse: 0.67098 | val_1_rmse: 0.85092 |  0:02:57s
epoch 95 | loss: 0.39991 | val_0_rmse: 0.66519 | val_1_rmse: 0.80607 |  0:02:59s
epoch 96 | loss: 0.39304 | val_0_rmse: 0.66525 | val_1_rmse: 0.81883 |  0:03:00s
epoch 97 | loss: 0.39139 | val_0_rmse: 0.7544  | val_1_rmse: 0.84685 |  0:03:02s
epoch 98 | loss: 0.38239 | val_0_rmse: 0.64    | val_1_rmse: 0.81317 |  0:03:04s
epoch 99 | loss: 0.40675 | val_0_rmse: 0.71418 | val_1_rmse: 0.93964 |  0:03:06s
epoch 100| loss: 0.39407 | val_0_rmse: 0.75643 | val_1_rmse: 0.86539 |  0:03:08s
epoch 101| loss: 0.38737 | val_0_rmse: 0.61457 | val_1_rmse: 0.78798 |  0:03:10s
epoch 102| loss: 0.38373 | val_0_rmse: 0.6404  | val_1_rmse: 0.80294 |  0:03:12s
epoch 103| loss: 0.37635 | val_0_rmse: 0.59847 | val_1_rmse: 0.79391 |  0:03:13s
epoch 104| loss: 0.37714 | val_0_rmse: 0.58907 | val_1_rmse: 0.7941  |  0:03:15s
epoch 105| loss: 0.37581 | val_0_rmse: 0.58573 | val_1_rmse: 0.81221 |  0:03:17s
epoch 106| loss: 0.37821 | val_0_rmse: 0.65974 | val_1_rmse: 0.82057 |  0:03:19s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.76967
Best weights from best epoch are automatically used!
ended training at: 04:07:16
Feature importance:
Mean squared error is of 0.047289415563396005
Mean absolute error:0.1373454019748853
MAPE:0.1433553767762278
R2 score:0.4083975165382494
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:07:17
epoch 0  | loss: 1.35029 | val_0_rmse: 0.92092 | val_1_rmse: 0.94861 |  0:00:01s
epoch 1  | loss: 0.90042 | val_0_rmse: 0.92188 | val_1_rmse: 0.94926 |  0:00:03s
epoch 2  | loss: 0.80609 | val_0_rmse: 0.88234 | val_1_rmse: 0.91013 |  0:00:05s
epoch 3  | loss: 0.80173 | val_0_rmse: 0.88121 | val_1_rmse: 0.91233 |  0:00:07s
epoch 4  | loss: 0.79676 | val_0_rmse: 0.88185 | val_1_rmse: 0.91249 |  0:00:09s
epoch 5  | loss: 0.78941 | val_0_rmse: 0.88112 | val_1_rmse: 0.91064 |  0:00:11s
epoch 6  | loss: 0.78813 | val_0_rmse: 0.87968 | val_1_rmse: 0.91028 |  0:00:13s
epoch 7  | loss: 0.77762 | val_0_rmse: 0.88397 | val_1_rmse: 0.9154  |  0:00:14s
epoch 8  | loss: 0.77929 | val_0_rmse: 0.88127 | val_1_rmse: 0.91295 |  0:00:16s
epoch 9  | loss: 0.77752 | val_0_rmse: 0.88049 | val_1_rmse: 0.91187 |  0:00:18s
epoch 10 | loss: 0.7692  | val_0_rmse: 0.87911 | val_1_rmse: 0.91102 |  0:00:20s
epoch 11 | loss: 0.76546 | val_0_rmse: 0.87641 | val_1_rmse: 0.90886 |  0:00:22s
epoch 12 | loss: 0.76678 | val_0_rmse: 0.881   | val_1_rmse: 0.9136  |  0:00:24s
epoch 13 | loss: 0.76211 | val_0_rmse: 0.88207 | val_1_rmse: 0.91513 |  0:00:26s
epoch 14 | loss: 0.76341 | val_0_rmse: 0.87184 | val_1_rmse: 0.90645 |  0:00:28s
epoch 15 | loss: 0.76273 | val_0_rmse: 0.86966 | val_1_rmse: 0.90471 |  0:00:29s
epoch 16 | loss: 0.75372 | val_0_rmse: 0.86759 | val_1_rmse: 0.90439 |  0:00:31s
epoch 17 | loss: 0.75563 | val_0_rmse: 0.86609 | val_1_rmse: 0.90035 |  0:00:33s
epoch 18 | loss: 0.74306 | val_0_rmse: 0.86015 | val_1_rmse: 0.89668 |  0:00:35s
epoch 19 | loss: 0.74094 | val_0_rmse: 0.86266 | val_1_rmse: 0.90111 |  0:00:37s
epoch 20 | loss: 0.73942 | val_0_rmse: 0.8579  | val_1_rmse: 0.89454 |  0:00:39s
epoch 21 | loss: 0.73133 | val_0_rmse: 0.85108 | val_1_rmse: 0.88955 |  0:00:41s
epoch 22 | loss: 0.72743 | val_0_rmse: 0.84501 | val_1_rmse: 0.88537 |  0:00:43s
epoch 23 | loss: 0.72502 | val_0_rmse: 0.84575 | val_1_rmse: 0.88895 |  0:00:44s
epoch 24 | loss: 0.7147  | val_0_rmse: 0.83912 | val_1_rmse: 0.88431 |  0:00:46s
epoch 25 | loss: 0.71438 | val_0_rmse: 0.83275 | val_1_rmse: 0.88173 |  0:00:48s
epoch 26 | loss: 0.70551 | val_0_rmse: 0.82793 | val_1_rmse: 0.87792 |  0:00:50s
epoch 27 | loss: 0.69869 | val_0_rmse: 0.82289 | val_1_rmse: 0.87244 |  0:00:52s
epoch 28 | loss: 0.69022 | val_0_rmse: 0.81796 | val_1_rmse: 0.86835 |  0:00:54s
epoch 29 | loss: 0.67868 | val_0_rmse: 0.80937 | val_1_rmse: 0.8617  |  0:00:56s
epoch 30 | loss: 0.67003 | val_0_rmse: 0.80951 | val_1_rmse: 0.8625  |  0:00:58s
epoch 31 | loss: 0.6719  | val_0_rmse: 0.81279 | val_1_rmse: 0.85666 |  0:00:59s
epoch 32 | loss: 0.64945 | val_0_rmse: 0.79059 | val_1_rmse: 0.83575 |  0:01:01s
epoch 33 | loss: 0.64514 | val_0_rmse: 0.84056 | val_1_rmse: 0.87883 |  0:01:03s
epoch 34 | loss: 0.65425 | val_0_rmse: 0.79935 | val_1_rmse: 0.84322 |  0:01:05s
epoch 35 | loss: 0.62936 | val_0_rmse: 0.76629 | val_1_rmse: 0.81537 |  0:01:07s
epoch 36 | loss: 0.61403 | val_0_rmse: 0.76649 | val_1_rmse: 0.82482 |  0:01:09s
epoch 37 | loss: 0.61206 | val_0_rmse: 0.75884 | val_1_rmse: 0.82153 |  0:01:11s
epoch 38 | loss: 0.60397 | val_0_rmse: 0.75378 | val_1_rmse: 0.81596 |  0:01:13s
epoch 39 | loss: 0.59912 | val_0_rmse: 0.76284 | val_1_rmse: 0.82277 |  0:01:14s
epoch 40 | loss: 0.60117 | val_0_rmse: 0.75769 | val_1_rmse: 0.81929 |  0:01:16s
epoch 41 | loss: 0.58877 | val_0_rmse: 0.74267 | val_1_rmse: 0.81351 |  0:01:18s
epoch 42 | loss: 0.5834  | val_0_rmse: 0.76334 | val_1_rmse: 0.83688 |  0:01:20s
epoch 43 | loss: 0.58315 | val_0_rmse: 0.74379 | val_1_rmse: 0.81227 |  0:01:22s
epoch 44 | loss: 0.59042 | val_0_rmse: 0.73863 | val_1_rmse: 0.81066 |  0:01:24s
epoch 45 | loss: 0.56605 | val_0_rmse: 0.75168 | val_1_rmse: 0.81707 |  0:01:26s
epoch 46 | loss: 0.56628 | val_0_rmse: 0.73013 | val_1_rmse: 0.80224 |  0:01:28s
epoch 47 | loss: 0.56508 | val_0_rmse: 0.72618 | val_1_rmse: 0.79642 |  0:01:29s
epoch 48 | loss: 0.55217 | val_0_rmse: 0.71724 | val_1_rmse: 0.79642 |  0:01:31s
epoch 49 | loss: 0.54708 | val_0_rmse: 0.73243 | val_1_rmse: 0.80743 |  0:01:33s
epoch 50 | loss: 0.5612  | val_0_rmse: 0.72371 | val_1_rmse: 0.80684 |  0:01:35s
epoch 51 | loss: 0.54568 | val_0_rmse: 0.71467 | val_1_rmse: 0.79223 |  0:01:37s
epoch 52 | loss: 0.53923 | val_0_rmse: 0.71038 | val_1_rmse: 0.80866 |  0:01:39s
epoch 53 | loss: 0.53671 | val_0_rmse: 0.72123 | val_1_rmse: 0.81663 |  0:01:41s
epoch 54 | loss: 0.53741 | val_0_rmse: 0.70558 | val_1_rmse: 0.79695 |  0:01:42s
epoch 55 | loss: 0.53498 | val_0_rmse: 0.72001 | val_1_rmse: 0.79966 |  0:01:44s
epoch 56 | loss: 0.533   | val_0_rmse: 0.72675 | val_1_rmse: 0.8095  |  0:01:46s
epoch 57 | loss: 0.53008 | val_0_rmse: 0.70122 | val_1_rmse: 0.81435 |  0:01:48s
epoch 58 | loss: 0.51919 | val_0_rmse: 0.70215 | val_1_rmse: 0.79593 |  0:01:50s
epoch 59 | loss: 0.52737 | val_0_rmse: 0.70406 | val_1_rmse: 0.79282 |  0:01:52s
epoch 60 | loss: 0.52906 | val_0_rmse: 0.69882 | val_1_rmse: 0.79021 |  0:01:54s
epoch 61 | loss: 0.51215 | val_0_rmse: 0.6856  | val_1_rmse: 0.788   |  0:01:56s
epoch 62 | loss: 0.51351 | val_0_rmse: 0.70262 | val_1_rmse: 0.80097 |  0:01:57s
epoch 63 | loss: 0.5029  | val_0_rmse: 0.69269 | val_1_rmse: 0.79423 |  0:01:59s
epoch 64 | loss: 0.50382 | val_0_rmse: 0.68725 | val_1_rmse: 0.79112 |  0:02:01s
epoch 65 | loss: 0.51885 | val_0_rmse: 0.70313 | val_1_rmse: 0.80671 |  0:02:03s
epoch 66 | loss: 0.50365 | val_0_rmse: 0.68694 | val_1_rmse: 0.79733 |  0:02:05s
epoch 67 | loss: 0.48979 | val_0_rmse: 0.6815  | val_1_rmse: 0.80301 |  0:02:07s
epoch 68 | loss: 0.48523 | val_0_rmse: 0.69203 | val_1_rmse: 0.79117 |  0:02:09s
epoch 69 | loss: 0.49031 | val_0_rmse: 0.67314 | val_1_rmse: 0.7969  |  0:02:10s
epoch 70 | loss: 0.48769 | val_0_rmse: 0.6857  | val_1_rmse: 0.80818 |  0:02:12s
epoch 71 | loss: 0.49159 | val_0_rmse: 0.67247 | val_1_rmse: 0.78315 |  0:02:14s
epoch 72 | loss: 0.48883 | val_0_rmse: 0.69518 | val_1_rmse: 0.78304 |  0:02:16s
epoch 73 | loss: 0.48416 | val_0_rmse: 0.67278 | val_1_rmse: 0.80513 |  0:02:18s
epoch 74 | loss: 0.48056 | val_0_rmse: 0.68952 | val_1_rmse: 0.79391 |  0:02:20s
epoch 75 | loss: 0.48885 | val_0_rmse: 0.66889 | val_1_rmse: 0.79207 |  0:02:22s
epoch 76 | loss: 0.47064 | val_0_rmse: 0.66165 | val_1_rmse: 0.79874 |  0:02:24s
epoch 77 | loss: 0.46949 | val_0_rmse: 0.67735 | val_1_rmse: 0.81033 |  0:02:25s
epoch 78 | loss: 0.46689 | val_0_rmse: 0.66316 | val_1_rmse: 0.81334 |  0:02:27s
epoch 79 | loss: 0.46531 | val_0_rmse: 0.65363 | val_1_rmse: 0.78754 |  0:02:29s
epoch 80 | loss: 0.47012 | val_0_rmse: 0.65775 | val_1_rmse: 0.79363 |  0:02:31s
epoch 81 | loss: 0.46255 | val_0_rmse: 0.66536 | val_1_rmse: 0.80014 |  0:02:33s
epoch 82 | loss: 0.46449 | val_0_rmse: 0.66185 | val_1_rmse: 0.79267 |  0:02:35s
epoch 83 | loss: 0.46298 | val_0_rmse: 0.65195 | val_1_rmse: 0.79275 |  0:02:37s
epoch 84 | loss: 0.4525  | val_0_rmse: 0.67513 | val_1_rmse: 0.84493 |  0:02:38s
epoch 85 | loss: 0.45111 | val_0_rmse: 0.64572 | val_1_rmse: 0.79403 |  0:02:40s
epoch 86 | loss: 0.45526 | val_0_rmse: 0.65452 | val_1_rmse: 0.79473 |  0:02:42s
epoch 87 | loss: 0.45505 | val_0_rmse: 0.64304 | val_1_rmse: 0.79851 |  0:02:44s
epoch 88 | loss: 0.44416 | val_0_rmse: 0.63307 | val_1_rmse: 0.80046 |  0:02:46s
epoch 89 | loss: 0.43667 | val_0_rmse: 0.6302  | val_1_rmse: 0.80616 |  0:02:48s
epoch 90 | loss: 0.43029 | val_0_rmse: 0.63777 | val_1_rmse: 0.79563 |  0:02:50s
epoch 91 | loss: 0.45161 | val_0_rmse: 0.63329 | val_1_rmse: 0.79066 |  0:02:52s
epoch 92 | loss: 0.44525 | val_0_rmse: 0.64777 | val_1_rmse: 0.80485 |  0:02:53s
epoch 93 | loss: 0.44583 | val_0_rmse: 0.64404 | val_1_rmse: 0.80946 |  0:02:55s
epoch 94 | loss: 0.45919 | val_0_rmse: 0.64606 | val_1_rmse: 0.83034 |  0:02:57s
epoch 95 | loss: 0.4479  | val_0_rmse: 0.63728 | val_1_rmse: 0.79291 |  0:02:59s
epoch 96 | loss: 0.43031 | val_0_rmse: 0.63907 | val_1_rmse: 0.78543 |  0:03:01s
epoch 97 | loss: 0.43695 | val_0_rmse: 0.64543 | val_1_rmse: 0.8107  |  0:03:03s
epoch 98 | loss: 0.43701 | val_0_rmse: 0.62585 | val_1_rmse: 0.79058 |  0:03:05s
epoch 99 | loss: 0.43059 | val_0_rmse: 0.62285 | val_1_rmse: 0.80375 |  0:03:06s
epoch 100| loss: 0.42812 | val_0_rmse: 0.62482 | val_1_rmse: 0.81941 |  0:03:08s
epoch 101| loss: 0.4243  | val_0_rmse: 0.63239 | val_1_rmse: 0.81023 |  0:03:10s
epoch 102| loss: 0.42644 | val_0_rmse: 0.63362 | val_1_rmse: 0.80942 |  0:03:12s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.78304
Best weights from best epoch are automatically used!
ended training at: 04:10:30
Feature importance:
Mean squared error is of 0.044578585536250256
Mean absolute error:0.13397366466058952
MAPE:0.14304634910948927
R2 score:0.35335395450539187
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:11:32
epoch 0  | loss: 1.17679 | val_0_rmse: 1.00092 | val_1_rmse: 0.98929 |  0:00:07s
epoch 1  | loss: 1.00407 | val_0_rmse: 0.99697 | val_1_rmse: 0.9851  |  0:00:14s
epoch 2  | loss: 0.99874 | val_0_rmse: 0.97851 | val_1_rmse: 0.96661 |  0:00:21s
epoch 3  | loss: 0.95367 | val_0_rmse: 0.95968 | val_1_rmse: 0.94773 |  0:00:28s
epoch 4  | loss: 0.91305 | val_0_rmse: 0.93982 | val_1_rmse: 0.92551 |  0:00:35s
epoch 5  | loss: 0.88117 | val_0_rmse: 0.93508 | val_1_rmse: 0.92231 |  0:00:42s
epoch 6  | loss: 0.86452 | val_0_rmse: 0.92719 | val_1_rmse: 0.91823 |  0:00:49s
epoch 7  | loss: 0.84348 | val_0_rmse: 0.91884 | val_1_rmse: 0.91214 |  0:00:56s
epoch 8  | loss: 0.82972 | val_0_rmse: 0.91883 | val_1_rmse: 0.91353 |  0:01:03s
epoch 9  | loss: 0.82066 | val_0_rmse: 0.91225 | val_1_rmse: 0.90939 |  0:01:10s
epoch 10 | loss: 0.80172 | val_0_rmse: 0.89126 | val_1_rmse: 0.89415 |  0:01:17s
epoch 11 | loss: 0.79145 | val_0_rmse: 0.88281 | val_1_rmse: 0.89848 |  0:01:24s
epoch 12 | loss: 0.77604 | val_0_rmse: 0.87222 | val_1_rmse: 0.88847 |  0:01:31s
epoch 13 | loss: 0.76716 | val_0_rmse: 0.8678  | val_1_rmse: 0.8903  |  0:01:38s
epoch 14 | loss: 0.75784 | val_0_rmse: 0.85638 | val_1_rmse: 0.88901 |  0:01:45s
epoch 15 | loss: 0.74898 | val_0_rmse: 0.8606  | val_1_rmse: 0.90662 |  0:01:52s
epoch 16 | loss: 0.74086 | val_0_rmse: 0.84277 | val_1_rmse: 0.88349 |  0:01:59s
epoch 17 | loss: 0.73301 | val_0_rmse: 0.85144 | val_1_rmse: 0.91227 |  0:02:06s
epoch 18 | loss: 0.72138 | val_0_rmse: 0.86025 | val_1_rmse: 0.93404 |  0:02:13s
epoch 19 | loss: 0.7169  | val_0_rmse: 0.84273 | val_1_rmse: 0.89648 |  0:02:20s
epoch 20 | loss: 0.71066 | val_0_rmse: 0.83584 | val_1_rmse: 0.88864 |  0:02:27s
epoch 21 | loss: 0.70934 | val_0_rmse: 0.83511 | val_1_rmse: 0.89868 |  0:02:34s
epoch 22 | loss: 0.69698 | val_0_rmse: 0.83118 | val_1_rmse: 0.90012 |  0:02:41s
epoch 23 | loss: 0.6907  | val_0_rmse: 0.82387 | val_1_rmse: 0.89309 |  0:02:48s
epoch 24 | loss: 0.68373 | val_0_rmse: 0.82805 | val_1_rmse: 0.90184 |  0:02:55s
epoch 25 | loss: 0.68734 | val_0_rmse: 0.83854 | val_1_rmse: 0.91452 |  0:03:02s
epoch 26 | loss: 0.68486 | val_0_rmse: 0.85048 | val_1_rmse: 0.92407 |  0:03:09s
epoch 27 | loss: 0.67883 | val_0_rmse: 0.82706 | val_1_rmse: 0.89691 |  0:03:16s
epoch 28 | loss: 0.67053 | val_0_rmse: 0.83754 | val_1_rmse: 0.92048 |  0:03:23s
epoch 29 | loss: 0.66419 | val_0_rmse: 0.82593 | val_1_rmse: 0.8886  |  0:03:30s
epoch 30 | loss: 0.6547  | val_0_rmse: 0.80988 | val_1_rmse: 0.89356 |  0:03:37s
epoch 31 | loss: 0.65146 | val_0_rmse: 0.81484 | val_1_rmse: 0.87972 |  0:03:44s
epoch 32 | loss: 0.64593 | val_0_rmse: 0.80027 | val_1_rmse: 0.89421 |  0:03:51s
epoch 33 | loss: 0.64484 | val_0_rmse: 0.80043 | val_1_rmse: 0.89672 |  0:03:58s
epoch 34 | loss: 0.64056 | val_0_rmse: 0.79896 | val_1_rmse: 0.90448 |  0:04:05s
epoch 35 | loss: 0.63553 | val_0_rmse: 0.79547 | val_1_rmse: 0.8982  |  0:04:12s
epoch 36 | loss: 0.6334  | val_0_rmse: 0.78823 | val_1_rmse: 0.88683 |  0:04:19s
epoch 37 | loss: 0.62518 | val_0_rmse: 0.80225 | val_1_rmse: 0.88567 |  0:04:26s
epoch 38 | loss: 0.62376 | val_0_rmse: 0.79775 | val_1_rmse: 0.89861 |  0:04:33s
epoch 39 | loss: 0.62305 | val_0_rmse: 0.82319 | val_1_rmse: 1.09265 |  0:04:40s
epoch 40 | loss: 0.62251 | val_0_rmse: 0.80939 | val_1_rmse: 0.89543 |  0:04:47s
epoch 41 | loss: 0.62188 | val_0_rmse: 0.83571 | val_1_rmse: 0.95598 |  0:04:54s
epoch 42 | loss: 0.6195  | val_0_rmse: 0.78491 | val_1_rmse: 0.90757 |  0:05:01s
epoch 43 | loss: 0.60639 | val_0_rmse: 0.78101 | val_1_rmse: 0.90883 |  0:05:08s
epoch 44 | loss: 0.60492 | val_0_rmse: 0.7798  | val_1_rmse: 0.8869  |  0:05:15s
epoch 45 | loss: 0.60171 | val_0_rmse: 0.82954 | val_1_rmse: 1.01969 |  0:05:22s
epoch 46 | loss: 0.60074 | val_0_rmse: 0.78558 | val_1_rmse: 0.91763 |  0:05:29s
epoch 47 | loss: 0.60025 | val_0_rmse: 0.79143 | val_1_rmse: 0.90335 |  0:05:36s
epoch 48 | loss: 0.59563 | val_0_rmse: 0.76938 | val_1_rmse: 0.89284 |  0:05:43s
epoch 49 | loss: 0.58958 | val_0_rmse: 0.77256 | val_1_rmse: 0.88294 |  0:05:50s
epoch 50 | loss: 0.59106 | val_0_rmse: 0.8755  | val_1_rmse: 0.9516  |  0:05:57s
epoch 51 | loss: 0.65697 | val_0_rmse: 0.8455  | val_1_rmse: 0.93966 |  0:06:04s
epoch 52 | loss: 0.63649 | val_0_rmse: 0.93142 | val_1_rmse: 1.21356 |  0:06:11s
epoch 53 | loss: 0.61493 | val_0_rmse: 0.99952 | val_1_rmse: 1.37654 |  0:06:19s
epoch 54 | loss: 0.5994  | val_0_rmse: 1.08884 | val_1_rmse: 1.56306 |  0:06:26s
epoch 55 | loss: 0.59457 | val_0_rmse: 0.79779 | val_1_rmse: 0.9715  |  0:06:33s
epoch 56 | loss: 0.59034 | val_0_rmse: 0.8851  | val_1_rmse: 1.14701 |  0:06:40s
epoch 57 | loss: 0.58226 | val_0_rmse: 0.93409 | val_1_rmse: 1.25256 |  0:06:47s
epoch 58 | loss: 0.60681 | val_0_rmse: 0.81104 | val_1_rmse: 0.98199 |  0:06:54s
epoch 59 | loss: 0.59778 | val_0_rmse: 0.78825 | val_1_rmse: 0.95665 |  0:07:01s
epoch 60 | loss: 0.58658 | val_0_rmse: 0.79585 | val_1_rmse: 0.97516 |  0:07:08s
epoch 61 | loss: 0.58667 | val_0_rmse: 0.88774 | val_1_rmse: 1.16945 |  0:07:15s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.87972
Best weights from best epoch are automatically used!
ended training at: 04:18:51
Feature importance:
Mean squared error is of 0.06251039441462533
Mean absolute error:0.18362849965464084
MAPE:0.20311790948564998
R2 score:0.1996860452603746
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:18:53
epoch 0  | loss: 1.17608 | val_0_rmse: 0.98951 | val_1_rmse: 1.00355 |  0:00:07s
epoch 1  | loss: 0.97932 | val_0_rmse: 0.96264 | val_1_rmse: 0.9742  |  0:00:14s
epoch 2  | loss: 0.93189 | val_0_rmse: 0.94727 | val_1_rmse: 0.96038 |  0:00:21s
epoch 3  | loss: 0.86076 | val_0_rmse: 0.92998 | val_1_rmse: 0.94583 |  0:00:28s
epoch 4  | loss: 0.81862 | val_0_rmse: 0.91481 | val_1_rmse: 0.93198 |  0:00:34s
epoch 5  | loss: 0.79457 | val_0_rmse: 0.91838 | val_1_rmse: 0.93315 |  0:00:42s
epoch 6  | loss: 0.78315 | val_0_rmse: 0.89836 | val_1_rmse: 0.91919 |  0:00:49s
epoch 7  | loss: 0.76968 | val_0_rmse: 0.89319 | val_1_rmse: 0.91618 |  0:00:56s
epoch 8  | loss: 0.75644 | val_0_rmse: 0.87683 | val_1_rmse: 0.90444 |  0:01:03s
epoch 9  | loss: 0.74926 | val_0_rmse: 0.86744 | val_1_rmse: 0.90219 |  0:01:10s
epoch 10 | loss: 0.7385  | val_0_rmse: 0.85328 | val_1_rmse: 0.89462 |  0:01:17s
epoch 11 | loss: 0.72637 | val_0_rmse: 0.859   | val_1_rmse: 0.91674 |  0:01:24s
epoch 12 | loss: 0.72273 | val_0_rmse: 0.83649 | val_1_rmse: 0.89114 |  0:01:31s
epoch 13 | loss: 0.71431 | val_0_rmse: 0.82994 | val_1_rmse: 0.88769 |  0:01:38s
epoch 14 | loss: 0.70647 | val_0_rmse: 0.83207 | val_1_rmse: 0.88687 |  0:01:45s
epoch 15 | loss: 0.70101 | val_0_rmse: 0.82778 | val_1_rmse: 0.90173 |  0:01:52s
epoch 16 | loss: 0.69412 | val_0_rmse: 0.83103 | val_1_rmse: 0.88925 |  0:01:59s
epoch 17 | loss: 0.68644 | val_0_rmse: 0.8183  | val_1_rmse: 0.91385 |  0:02:06s
epoch 18 | loss: 0.68281 | val_0_rmse: 0.83624 | val_1_rmse: 0.91504 |  0:02:13s
epoch 19 | loss: 0.67806 | val_0_rmse: 0.8138  | val_1_rmse: 0.9026  |  0:02:20s
epoch 20 | loss: 0.67067 | val_0_rmse: 0.82983 | val_1_rmse: 0.91265 |  0:02:27s
epoch 21 | loss: 0.67047 | val_0_rmse: 0.81467 | val_1_rmse: 0.89579 |  0:02:34s
epoch 22 | loss: 0.65918 | val_0_rmse: 0.80298 | val_1_rmse: 0.895   |  0:02:41s
epoch 23 | loss: 0.65312 | val_0_rmse: 0.80513 | val_1_rmse: 0.91127 |  0:02:48s
epoch 24 | loss: 0.6535  | val_0_rmse: 0.82996 | val_1_rmse: 0.96574 |  0:02:55s
epoch 25 | loss: 0.6474  | val_0_rmse: 0.90173 | val_1_rmse: 1.05888 |  0:03:02s
epoch 26 | loss: 0.64227 | val_0_rmse: 0.80334 | val_1_rmse: 0.9226  |  0:03:09s
epoch 27 | loss: 0.6384  | val_0_rmse: 0.79347 | val_1_rmse: 0.93395 |  0:03:16s
epoch 28 | loss: 0.62849 | val_0_rmse: 0.80377 | val_1_rmse: 0.92312 |  0:03:23s
epoch 29 | loss: 0.62455 | val_0_rmse: 0.80225 | val_1_rmse: 0.9213  |  0:03:30s
epoch 30 | loss: 0.62354 | val_0_rmse: 0.78979 | val_1_rmse: 0.91637 |  0:03:37s
epoch 31 | loss: 0.65603 | val_0_rmse: 0.82076 | val_1_rmse: 0.91956 |  0:03:44s
epoch 32 | loss: 0.63532 | val_0_rmse: 0.79294 | val_1_rmse: 0.90892 |  0:03:51s
epoch 33 | loss: 0.62571 | val_0_rmse: 0.79536 | val_1_rmse: 0.91752 |  0:03:58s
epoch 34 | loss: 0.62034 | val_0_rmse: 0.78837 | val_1_rmse: 0.90257 |  0:04:05s
epoch 35 | loss: 0.61304 | val_0_rmse: 2.44175 | val_1_rmse: 0.92885 |  0:04:12s
epoch 36 | loss: 0.61094 | val_0_rmse: 0.79167 | val_1_rmse: 0.9159  |  0:04:19s
epoch 37 | loss: 0.60959 | val_0_rmse: 0.77663 | val_1_rmse: 0.90578 |  0:04:26s
epoch 38 | loss: 0.60049 | val_0_rmse: 0.77474 | val_1_rmse: 0.89936 |  0:04:33s
epoch 39 | loss: 0.59558 | val_0_rmse: 0.77944 | val_1_rmse: 0.8938  |  0:04:40s
epoch 40 | loss: 0.59556 | val_0_rmse: 0.77254 | val_1_rmse: 0.91444 |  0:04:47s
epoch 41 | loss: 0.59454 | val_0_rmse: 0.77346 | val_1_rmse: 0.96935 |  0:04:54s
epoch 42 | loss: 0.58779 | val_0_rmse: 0.78576 | val_1_rmse: 0.9464  |  0:05:01s
epoch 43 | loss: 0.59014 | val_0_rmse: 0.77016 | val_1_rmse: 0.90865 |  0:05:08s
epoch 44 | loss: 0.58225 | val_0_rmse: 0.75824 | val_1_rmse: 0.90289 |  0:05:15s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.88687
Best weights from best epoch are automatically used!
ended training at: 04:24:12
Feature importance:
Mean squared error is of 0.06389683857204419
Mean absolute error:0.18636823654442797
MAPE:0.20503997225756126
R2 score:0.19718459547798683
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:24:15
epoch 0  | loss: 1.1916  | val_0_rmse: 0.98441 | val_1_rmse: 0.99375 |  0:00:07s
epoch 1  | loss: 0.96996 | val_0_rmse: 0.94997 | val_1_rmse: 0.95408 |  0:00:13s
epoch 2  | loss: 0.92236 | val_0_rmse: 0.94156 | val_1_rmse: 0.94708 |  0:00:20s
epoch 3  | loss: 0.87411 | val_0_rmse: 0.92454 | val_1_rmse: 0.92917 |  0:00:27s
epoch 4  | loss: 0.82988 | val_0_rmse: 0.914   | val_1_rmse: 0.91659 |  0:00:35s
epoch 5  | loss: 0.80655 | val_0_rmse: 0.90762 | val_1_rmse: 0.91251 |  0:00:42s
epoch 6  | loss: 0.78865 | val_0_rmse: 0.89781 | val_1_rmse: 0.90671 |  0:00:48s
epoch 7  | loss: 0.78099 | val_0_rmse: 0.8912  | val_1_rmse: 0.90061 |  0:00:56s
epoch 8  | loss: 0.76669 | val_0_rmse: 0.87903 | val_1_rmse: 0.89264 |  0:01:03s
epoch 9  | loss: 0.7542  | val_0_rmse: 0.87159 | val_1_rmse: 0.89016 |  0:01:09s
epoch 10 | loss: 0.74243 | val_0_rmse: 0.85674 | val_1_rmse: 0.8811  |  0:01:17s
epoch 11 | loss: 0.73111 | val_0_rmse: 0.84347 | val_1_rmse: 0.87802 |  0:01:24s
epoch 12 | loss: 0.73018 | val_0_rmse: 0.86789 | val_1_rmse: 0.91261 |  0:01:31s
epoch 13 | loss: 0.72665 | val_0_rmse: 0.83973 | val_1_rmse: 0.88768 |  0:01:38s
epoch 14 | loss: 0.71593 | val_0_rmse: 0.83389 | val_1_rmse: 0.89423 |  0:01:45s
epoch 15 | loss: 0.71405 | val_0_rmse: 0.8661  | val_1_rmse: 0.92639 |  0:01:52s
epoch 16 | loss: 0.70923 | val_0_rmse: 0.8296  | val_1_rmse: 0.88785 |  0:01:59s
epoch 17 | loss: 0.6957  | val_0_rmse: 0.83016 | val_1_rmse: 0.8955  |  0:02:06s
epoch 18 | loss: 0.68585 | val_0_rmse: 0.82321 | val_1_rmse: 0.88515 |  0:02:13s
epoch 19 | loss: 0.68023 | val_0_rmse: 0.81408 | val_1_rmse: 0.88778 |  0:02:20s
epoch 20 | loss: 0.67528 | val_0_rmse: 0.84753 | val_1_rmse: 0.89406 |  0:02:27s
epoch 21 | loss: 0.66575 | val_0_rmse: 0.86698 | val_1_rmse: 0.8935  |  0:02:34s
epoch 22 | loss: 0.65691 | val_0_rmse: 0.81219 | val_1_rmse: 0.89906 |  0:02:41s
epoch 23 | loss: 0.6545  | val_0_rmse: 0.84475 | val_1_rmse: 0.90203 |  0:02:48s
epoch 24 | loss: 0.64657 | val_0_rmse: 0.81426 | val_1_rmse: 0.90286 |  0:02:55s
epoch 25 | loss: 0.64455 | val_0_rmse: 0.8509  | val_1_rmse: 0.91964 |  0:03:02s
epoch 26 | loss: 0.64154 | val_0_rmse: 0.79829 | val_1_rmse: 0.90153 |  0:03:09s
epoch 27 | loss: 0.63458 | val_0_rmse: 0.82409 | val_1_rmse: 0.90943 |  0:03:16s
epoch 28 | loss: 0.63033 | val_0_rmse: 0.8417  | val_1_rmse: 0.96112 |  0:03:23s
epoch 29 | loss: 0.62341 | val_0_rmse: 0.79179 | val_1_rmse: 0.90907 |  0:03:30s
epoch 30 | loss: 0.61934 | val_0_rmse: 0.82185 | val_1_rmse: 0.94745 |  0:03:37s
epoch 31 | loss: 0.61582 | val_0_rmse: 0.78502 | val_1_rmse: 0.90376 |  0:03:44s
epoch 32 | loss: 0.61309 | val_0_rmse: 0.90056 | val_1_rmse: 0.92737 |  0:03:51s
epoch 33 | loss: 0.60917 | val_0_rmse: 1.1829  | val_1_rmse: 0.8978  |  0:03:58s
epoch 34 | loss: 0.60227 | val_0_rmse: 1.0823  | val_1_rmse: 0.91046 |  0:04:05s
epoch 35 | loss: 0.60156 | val_0_rmse: 0.89709 | val_1_rmse: 0.91117 |  0:04:13s
epoch 36 | loss: 0.59593 | val_0_rmse: 0.83327 | val_1_rmse: 0.89677 |  0:04:19s
epoch 37 | loss: 0.59201 | val_0_rmse: 1.02968 | val_1_rmse: 0.9014  |  0:04:27s
epoch 38 | loss: 0.58707 | val_0_rmse: 0.99078 | val_1_rmse: 0.94909 |  0:04:34s
epoch 39 | loss: 0.58191 | val_0_rmse: 1.10862 | val_1_rmse: 0.95725 |  0:04:41s
epoch 40 | loss: 0.57996 | val_0_rmse: 0.76667 | val_1_rmse: 0.90222 |  0:04:48s
epoch 41 | loss: 0.57779 | val_0_rmse: 1.09908 | val_1_rmse: 0.90922 |  0:04:55s

Early stopping occured at epoch 41 with best_epoch = 11 and best_val_1_rmse = 0.87802
Best weights from best epoch are automatically used!
ended training at: 04:29:13
Feature importance:
Mean squared error is of 0.06380389072242054
Mean absolute error:0.18573134529382257
MAPE:0.19783060687817422
R2 score:0.19992769601630322
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:29:15
epoch 0  | loss: 1.16663 | val_0_rmse: 1.00118 | val_1_rmse: 1.00187 |  0:00:07s
epoch 1  | loss: 1.00358 | val_0_rmse: 1.00069 | val_1_rmse: 1.00117 |  0:00:14s
epoch 2  | loss: 0.99936 | val_0_rmse: 0.99269 | val_1_rmse: 0.99348 |  0:00:21s
epoch 3  | loss: 0.97446 | val_0_rmse: 0.95683 | val_1_rmse: 0.95609 |  0:00:28s
epoch 4  | loss: 0.90265 | val_0_rmse: 0.94393 | val_1_rmse: 0.94634 |  0:00:35s
epoch 5  | loss: 0.8481  | val_0_rmse: 0.93222 | val_1_rmse: 0.93428 |  0:00:42s
epoch 6  | loss: 0.82296 | val_0_rmse: 0.92413 | val_1_rmse: 0.92642 |  0:00:49s
epoch 7  | loss: 0.8045  | val_0_rmse: 0.90574 | val_1_rmse: 0.91001 |  0:00:56s
epoch 8  | loss: 0.78546 | val_0_rmse: 0.89293 | val_1_rmse: 0.90436 |  0:01:03s
epoch 9  | loss: 0.77871 | val_0_rmse: 0.87906 | val_1_rmse: 0.89436 |  0:01:10s
epoch 10 | loss: 0.76952 | val_0_rmse: 0.8785  | val_1_rmse: 0.89846 |  0:01:17s
epoch 11 | loss: 0.75514 | val_0_rmse: 0.87028 | val_1_rmse: 0.89092 |  0:01:24s
epoch 12 | loss: 0.74928 | val_0_rmse: 0.86831 | val_1_rmse: 0.89989 |  0:01:31s
epoch 13 | loss: 0.74413 | val_0_rmse: 0.84658 | val_1_rmse: 0.87713 |  0:01:38s
epoch 14 | loss: 0.7309  | val_0_rmse: 0.84327 | val_1_rmse: 0.88016 |  0:01:45s
epoch 15 | loss: 0.72535 | val_0_rmse: 0.83309 | val_1_rmse: 0.88077 |  0:01:52s
epoch 16 | loss: 0.71507 | val_0_rmse: 0.82723 | val_1_rmse: 0.88032 |  0:01:59s
epoch 17 | loss: 0.70201 | val_0_rmse: 0.82426 | val_1_rmse: 0.87932 |  0:02:06s
epoch 18 | loss: 0.69514 | val_0_rmse: 0.82441 | val_1_rmse: 0.89532 |  0:02:13s
epoch 19 | loss: 0.69106 | val_0_rmse: 0.82414 | val_1_rmse: 0.89326 |  0:02:20s
epoch 20 | loss: 0.68283 | val_0_rmse: 0.81972 | val_1_rmse: 0.89019 |  0:02:27s
epoch 21 | loss: 0.67927 | val_0_rmse: 0.91162 | val_1_rmse: 0.89799 |  0:02:34s
epoch 22 | loss: 0.6654  | val_0_rmse: 0.81071 | val_1_rmse: 0.90389 |  0:02:41s
epoch 23 | loss: 0.66425 | val_0_rmse: 0.81795 | val_1_rmse: 0.94324 |  0:02:48s
epoch 24 | loss: 0.65499 | val_0_rmse: 0.80524 | val_1_rmse: 0.92305 |  0:02:55s
epoch 25 | loss: 0.64888 | val_0_rmse: 0.81621 | val_1_rmse: 0.90384 |  0:03:02s
epoch 26 | loss: 0.6426  | val_0_rmse: 0.80082 | val_1_rmse: 0.90497 |  0:03:09s
epoch 27 | loss: 0.63796 | val_0_rmse: 0.81886 | val_1_rmse: 0.92581 |  0:03:16s
epoch 28 | loss: 0.62923 | val_0_rmse: 0.78951 | val_1_rmse: 0.89768 |  0:03:23s
epoch 29 | loss: 0.62746 | val_0_rmse: 0.7965  | val_1_rmse: 0.89786 |  0:03:30s
epoch 30 | loss: 0.62119 | val_0_rmse: 0.78725 | val_1_rmse: 0.90569 |  0:03:37s
epoch 31 | loss: 0.61691 | val_0_rmse: 0.79417 | val_1_rmse: 0.91876 |  0:03:44s
epoch 32 | loss: 0.61547 | val_0_rmse: 0.78921 | val_1_rmse: 0.9059  |  0:03:51s
epoch 33 | loss: 0.61355 | val_0_rmse: 0.78686 | val_1_rmse: 0.92    |  0:03:58s
epoch 34 | loss: 0.60649 | val_0_rmse: 0.83828 | val_1_rmse: 0.94159 |  0:04:05s
epoch 35 | loss: 0.60846 | val_0_rmse: 0.79793 | val_1_rmse: 0.92769 |  0:04:12s
epoch 36 | loss: 0.60366 | val_0_rmse: 0.79637 | val_1_rmse: 0.90925 |  0:04:19s
epoch 37 | loss: 0.59898 | val_0_rmse: 0.7744  | val_1_rmse: 0.91982 |  0:04:27s
epoch 38 | loss: 0.58876 | val_0_rmse: 0.98628 | val_1_rmse: 0.90761 |  0:04:34s
epoch 39 | loss: 0.58752 | val_0_rmse: 0.77092 | val_1_rmse: 0.90906 |  0:04:41s
epoch 40 | loss: 0.58656 | val_0_rmse: 0.76624 | val_1_rmse: 0.89538 |  0:04:48s
epoch 41 | loss: 0.5824  | val_0_rmse: 0.76862 | val_1_rmse: 0.89889 |  0:04:55s
epoch 42 | loss: 0.58214 | val_0_rmse: 0.76514 | val_1_rmse: 0.91555 |  0:05:02s
epoch 43 | loss: 0.5763  | val_0_rmse: 0.76723 | val_1_rmse: 0.91179 |  0:05:09s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.87713
Best weights from best epoch are automatically used!
ended training at: 04:34:28
Feature importance:
Mean squared error is of 0.05896788442103243
Mean absolute error:0.18369085655388326
MAPE:0.20011534148868138
R2 score:0.20321572374033048
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:34:30
epoch 0  | loss: 1.15718 | val_0_rmse: 0.98989 | val_1_rmse: 0.97712 |  0:00:07s
epoch 1  | loss: 0.92661 | val_0_rmse: 1.00343 | val_1_rmse: 0.99052 |  0:00:14s
epoch 2  | loss: 0.91236 | val_0_rmse: 0.95548 | val_1_rmse: 0.94048 |  0:00:21s
epoch 3  | loss: 0.92987 | val_0_rmse: 0.95145 | val_1_rmse: 0.93515 |  0:00:28s
epoch 4  | loss: 0.91932 | val_0_rmse: 0.95126 | val_1_rmse: 0.93514 |  0:00:35s
epoch 5  | loss: 0.91696 | val_0_rmse: 0.95414 | val_1_rmse: 0.93832 |  0:00:42s
epoch 6  | loss: 0.91764 | val_0_rmse: 0.95269 | val_1_rmse: 0.9382  |  0:00:49s
epoch 7  | loss: 0.91798 | val_0_rmse: 0.95535 | val_1_rmse: 0.93942 |  0:00:56s
epoch 8  | loss: 0.91898 | val_0_rmse: 0.95551 | val_1_rmse: 0.9398  |  0:01:03s
epoch 9  | loss: 0.9193  | val_0_rmse: 0.95283 | val_1_rmse: 0.93676 |  0:01:10s
epoch 10 | loss: 0.91412 | val_0_rmse: 0.95671 | val_1_rmse: 0.94161 |  0:01:17s
epoch 11 | loss: 0.91203 | val_0_rmse: 0.95098 | val_1_rmse: 0.93614 |  0:01:24s
epoch 12 | loss: 0.9063  | val_0_rmse: 0.94811 | val_1_rmse: 0.93248 |  0:01:31s
epoch 13 | loss: 0.92026 | val_0_rmse: 0.96142 | val_1_rmse: 0.94853 |  0:01:38s
epoch 14 | loss: 0.92155 | val_0_rmse: 0.94848 | val_1_rmse: 0.93454 |  0:01:45s
epoch 15 | loss: 0.90151 | val_0_rmse: 0.94536 | val_1_rmse: 0.93118 |  0:01:52s
epoch 16 | loss: 0.89334 | val_0_rmse: 0.94027 | val_1_rmse: 0.92672 |  0:01:59s
epoch 17 | loss: 0.88738 | val_0_rmse: 0.93821 | val_1_rmse: 0.92455 |  0:02:06s
epoch 18 | loss: 0.88073 | val_0_rmse: 0.935   | val_1_rmse: 0.92464 |  0:02:13s
epoch 19 | loss: 0.87462 | val_0_rmse: 0.92903 | val_1_rmse: 0.92118 |  0:02:20s
epoch 20 | loss: 0.86966 | val_0_rmse: 0.935   | val_1_rmse: 0.92681 |  0:02:27s
epoch 21 | loss: 0.86793 | val_0_rmse: 0.92793 | val_1_rmse: 0.91965 |  0:02:34s
epoch 22 | loss: 0.85944 | val_0_rmse: 0.92083 | val_1_rmse: 0.91808 |  0:02:41s
epoch 23 | loss: 0.85287 | val_0_rmse: 0.92403 | val_1_rmse: 0.92163 |  0:02:48s
epoch 24 | loss: 0.85099 | val_0_rmse: 0.91776 | val_1_rmse: 0.91658 |  0:02:56s
epoch 25 | loss: 0.84566 | val_0_rmse: 0.92224 | val_1_rmse: 0.92155 |  0:03:03s
epoch 26 | loss: 0.84037 | val_0_rmse: 0.91057 | val_1_rmse: 0.91133 |  0:03:10s
epoch 27 | loss: 0.83298 | val_0_rmse: 0.9117  | val_1_rmse: 0.90976 |  0:03:17s
epoch 28 | loss: 0.83296 | val_0_rmse: 0.91167 | val_1_rmse: 0.90875 |  0:03:24s
epoch 29 | loss: 0.83073 | val_0_rmse: 0.9066  | val_1_rmse: 0.90809 |  0:03:31s
epoch 30 | loss: 0.82837 | val_0_rmse: 0.90425 | val_1_rmse: 0.90521 |  0:03:38s
epoch 31 | loss: 0.82677 | val_0_rmse: 0.90638 | val_1_rmse: 0.90644 |  0:03:45s
epoch 32 | loss: 0.82145 | val_0_rmse: 0.94028 | val_1_rmse: 0.94763 |  0:03:52s
epoch 33 | loss: 0.81612 | val_0_rmse: 0.90226 | val_1_rmse: 0.90142 |  0:03:59s
epoch 34 | loss: 0.81292 | val_0_rmse: 0.89711 | val_1_rmse: 0.90113 |  0:04:06s
epoch 35 | loss: 0.80887 | val_0_rmse: 0.91228 | val_1_rmse: 0.91958 |  0:04:13s
epoch 36 | loss: 0.80619 | val_0_rmse: 0.89366 | val_1_rmse: 0.90065 |  0:04:20s
epoch 37 | loss: 0.80116 | val_0_rmse: 0.89641 | val_1_rmse: 0.90433 |  0:04:27s
epoch 38 | loss: 0.80196 | val_0_rmse: 0.89475 | val_1_rmse: 0.90622 |  0:04:34s
epoch 39 | loss: 0.79545 | val_0_rmse: 0.88532 | val_1_rmse: 0.89722 |  0:04:41s
epoch 40 | loss: 0.79019 | val_0_rmse: 0.8922  | val_1_rmse: 0.90075 |  0:04:48s
epoch 41 | loss: 0.78777 | val_0_rmse: 0.8822  | val_1_rmse: 0.89578 |  0:04:56s
epoch 42 | loss: 0.78262 | val_0_rmse: 0.87936 | val_1_rmse: 0.89588 |  0:05:03s
epoch 43 | loss: 0.78099 | val_0_rmse: 0.8813  | val_1_rmse: 0.89515 |  0:05:10s
epoch 44 | loss: 0.77453 | val_0_rmse: 0.87452 | val_1_rmse: 0.89488 |  0:05:17s
epoch 45 | loss: 0.77122 | val_0_rmse: 0.87592 | val_1_rmse: 0.8923  |  0:05:24s
epoch 46 | loss: 0.76452 | val_0_rmse: 0.87096 | val_1_rmse: 0.89563 |  0:05:31s
epoch 47 | loss: 0.76375 | val_0_rmse: 0.86516 | val_1_rmse: 0.8915  |  0:05:38s
epoch 48 | loss: 0.75748 | val_0_rmse: 0.86637 | val_1_rmse: 0.89386 |  0:05:45s
epoch 49 | loss: 0.75115 | val_0_rmse: 0.86867 | val_1_rmse: 0.89181 |  0:05:52s
epoch 50 | loss: 0.74574 | val_0_rmse: 0.86395 | val_1_rmse: 0.89707 |  0:05:59s
epoch 51 | loss: 0.73916 | val_0_rmse: 0.86043 | val_1_rmse: 0.88973 |  0:06:06s
epoch 52 | loss: 0.73791 | val_0_rmse: 0.85809 | val_1_rmse: 0.8889  |  0:06:13s
epoch 53 | loss: 0.73015 | val_0_rmse: 0.84958 | val_1_rmse: 0.892   |  0:06:20s
epoch 54 | loss: 0.72411 | val_0_rmse: 0.8735  | val_1_rmse: 0.91492 |  0:06:27s
epoch 55 | loss: 0.71692 | val_0_rmse: 0.90901 | val_1_rmse: 0.87871 |  0:06:34s
epoch 56 | loss: 0.70919 | val_0_rmse: 0.8414  | val_1_rmse: 0.88475 |  0:06:41s
epoch 57 | loss: 0.70268 | val_0_rmse: 0.82711 | val_1_rmse: 0.87618 |  0:06:48s
epoch 58 | loss: 0.6964  | val_0_rmse: 0.82559 | val_1_rmse: 0.87769 |  0:06:55s
epoch 59 | loss: 0.69131 | val_0_rmse: 0.83131 | val_1_rmse: 0.884   |  0:07:03s
epoch 60 | loss: 0.68204 | val_0_rmse: 0.81785 | val_1_rmse: 0.87639 |  0:07:10s
epoch 61 | loss: 0.67998 | val_0_rmse: 0.8292  | val_1_rmse: 0.89114 |  0:07:17s
epoch 62 | loss: 0.67227 | val_0_rmse: 0.82667 | val_1_rmse: 0.88549 |  0:07:24s
epoch 63 | loss: 0.67037 | val_0_rmse: 0.81345 | val_1_rmse: 0.88567 |  0:07:31s
epoch 64 | loss: 0.66826 | val_0_rmse: 0.82265 | val_1_rmse: 0.8884  |  0:07:38s
epoch 65 | loss: 0.66269 | val_0_rmse: 0.80364 | val_1_rmse: 0.88433 |  0:07:45s
epoch 66 | loss: 0.6563  | val_0_rmse: 0.80679 | val_1_rmse: 0.87605 |  0:07:52s
epoch 67 | loss: 0.65387 | val_0_rmse: 0.82868 | val_1_rmse: 0.88228 |  0:07:59s
epoch 68 | loss: 0.64832 | val_0_rmse: 0.82483 | val_1_rmse: 0.90513 |  0:08:06s
epoch 69 | loss: 0.64191 | val_0_rmse: 0.79401 | val_1_rmse: 0.88228 |  0:08:13s
epoch 70 | loss: 0.63997 | val_0_rmse: 0.79595 | val_1_rmse: 0.89215 |  0:08:20s
epoch 71 | loss: 0.63238 | val_0_rmse: 0.79175 | val_1_rmse: 0.88682 |  0:08:28s
epoch 72 | loss: 0.62966 | val_0_rmse: 0.7877  | val_1_rmse: 0.87252 |  0:08:35s
epoch 73 | loss: 0.65247 | val_0_rmse: 0.88527 | val_1_rmse: 0.93087 |  0:08:42s
epoch 74 | loss: 0.63243 | val_0_rmse: 0.81641 | val_1_rmse: 0.8867  |  0:08:49s
epoch 75 | loss: 0.63368 | val_0_rmse: 0.80686 | val_1_rmse: 0.87274 |  0:08:56s
epoch 76 | loss: 0.61693 | val_0_rmse: 0.80687 | val_1_rmse: 0.88371 |  0:09:03s
epoch 77 | loss: 0.61597 | val_0_rmse: 0.81344 | val_1_rmse: 0.91042 |  0:09:10s
epoch 78 | loss: 0.61514 | val_0_rmse: 0.79189 | val_1_rmse: 0.89172 |  0:09:17s
epoch 79 | loss: 0.60974 | val_0_rmse: 0.78732 | val_1_rmse: 0.88376 |  0:09:24s
epoch 80 | loss: 0.60504 | val_0_rmse: 0.79058 | val_1_rmse: 0.8897  |  0:09:31s
epoch 81 | loss: 0.60395 | val_0_rmse: 0.8113  | val_1_rmse: 0.91979 |  0:09:38s
epoch 82 | loss: 0.60116 | val_0_rmse: 0.78302 | val_1_rmse: 0.88468 |  0:09:45s
epoch 83 | loss: 0.59877 | val_0_rmse: 0.79235 | val_1_rmse: 0.89673 |  0:09:52s
epoch 84 | loss: 0.59132 | val_0_rmse: 0.78844 | val_1_rmse: 0.91268 |  0:09:59s
epoch 85 | loss: 0.59236 | val_0_rmse: 0.77014 | val_1_rmse: 0.88879 |  0:10:06s
epoch 86 | loss: 0.58888 | val_0_rmse: 0.77263 | val_1_rmse: 0.88727 |  0:10:14s
epoch 87 | loss: 0.5865  | val_0_rmse: 0.773   | val_1_rmse: 0.89719 |  0:10:21s
epoch 88 | loss: 0.58639 | val_0_rmse: 0.81158 | val_1_rmse: 0.9521  |  0:10:28s
epoch 89 | loss: 0.58383 | val_0_rmse: 0.77109 | val_1_rmse: 0.90765 |  0:10:35s
epoch 90 | loss: 0.57875 | val_0_rmse: 0.77062 | val_1_rmse: 0.88794 |  0:10:42s
epoch 91 | loss: 0.57529 | val_0_rmse: 0.77165 | val_1_rmse: 0.89932 |  0:10:49s
epoch 92 | loss: 0.57814 | val_0_rmse: 0.783   | val_1_rmse: 0.90448 |  0:10:56s
epoch 93 | loss: 0.57245 | val_0_rmse: 0.78121 | val_1_rmse: 0.90645 |  0:11:03s
epoch 94 | loss: 0.56676 | val_0_rmse: 0.76352 | val_1_rmse: 0.90021 |  0:11:10s
epoch 95 | loss: 0.56794 | val_0_rmse: 0.77956 | val_1_rmse: 0.91756 |  0:11:17s
epoch 96 | loss: 0.56365 | val_0_rmse: 0.77418 | val_1_rmse: 0.92275 |  0:11:24s
epoch 97 | loss: 0.56309 | val_0_rmse: 0.75386 | val_1_rmse: 0.88949 |  0:11:32s
epoch 98 | loss: 0.56107 | val_0_rmse: 0.765   | val_1_rmse: 0.90579 |  0:11:39s
epoch 99 | loss: 0.56518 | val_0_rmse: 0.75406 | val_1_rmse: 0.89602 |  0:11:46s
epoch 100| loss: 0.5626  | val_0_rmse: 0.76533 | val_1_rmse: 0.9078  |  0:11:53s
epoch 101| loss: 0.55574 | val_0_rmse: 0.75732 | val_1_rmse: 0.90956 |  0:12:00s
epoch 102| loss: 0.55836 | val_0_rmse: 0.76065 | val_1_rmse: 0.90136 |  0:12:07s

Early stopping occured at epoch 102 with best_epoch = 72 and best_val_1_rmse = 0.87252
Best weights from best epoch are automatically used!
ended training at: 04:46:41
Feature importance:
Mean squared error is of 0.06082862237719951
Mean absolute error:0.1815906392711737
MAPE:0.1939683712588928
R2 score:0.19444465982482684
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:46:45
epoch 0  | loss: 2.63112 | val_0_rmse: 1.0188  | val_1_rmse: 0.98353 |  0:00:01s
epoch 1  | loss: 1.12864 | val_0_rmse: 1.00593 | val_1_rmse: 0.9716  |  0:00:02s
epoch 2  | loss: 1.0119  | val_0_rmse: 1.00433 | val_1_rmse: 0.97091 |  0:00:03s
epoch 3  | loss: 0.99027 | val_0_rmse: 0.98899 | val_1_rmse: 0.9532  |  0:00:04s
epoch 4  | loss: 0.9481  | val_0_rmse: 0.98583 | val_1_rmse: 0.96357 |  0:00:05s
epoch 5  | loss: 0.89673 | val_0_rmse: 0.95024 | val_1_rmse: 0.93404 |  0:00:06s
epoch 6  | loss: 0.86527 | val_0_rmse: 0.94099 | val_1_rmse: 0.91962 |  0:00:07s
epoch 7  | loss: 0.8432  | val_0_rmse: 0.93226 | val_1_rmse: 0.90581 |  0:00:08s
epoch 8  | loss: 0.84631 | val_0_rmse: 0.93802 | val_1_rmse: 0.90986 |  0:00:10s
epoch 9  | loss: 0.8289  | val_0_rmse: 0.9282  | val_1_rmse: 0.899   |  0:00:11s
epoch 10 | loss: 0.81828 | val_0_rmse: 0.92523 | val_1_rmse: 0.89835 |  0:00:12s
epoch 11 | loss: 0.8104  | val_0_rmse: 0.90579 | val_1_rmse: 0.87933 |  0:00:13s
epoch 12 | loss: 0.80634 | val_0_rmse: 0.9004  | val_1_rmse: 0.878   |  0:00:14s
epoch 13 | loss: 0.78708 | val_0_rmse: 0.89994 | val_1_rmse: 0.88382 |  0:00:15s
epoch 14 | loss: 0.78833 | val_0_rmse: 0.8905  | val_1_rmse: 0.87494 |  0:00:16s
epoch 15 | loss: 0.77926 | val_0_rmse: 0.88946 | val_1_rmse: 0.873   |  0:00:17s
epoch 16 | loss: 0.78424 | val_0_rmse: 0.89631 | val_1_rmse: 0.8869  |  0:00:18s
epoch 17 | loss: 0.77359 | val_0_rmse: 0.88841 | val_1_rmse: 0.86834 |  0:00:19s
epoch 18 | loss: 0.77256 | val_0_rmse: 0.8819  | val_1_rmse: 0.86713 |  0:00:20s
epoch 19 | loss: 0.76057 | val_0_rmse: 0.88188 | val_1_rmse: 0.8681  |  0:00:21s
epoch 20 | loss: 0.75842 | val_0_rmse: 0.87548 | val_1_rmse: 0.86567 |  0:00:22s
epoch 21 | loss: 0.75266 | val_0_rmse: 0.86802 | val_1_rmse: 0.85772 |  0:00:23s
epoch 22 | loss: 0.74714 | val_0_rmse: 0.86819 | val_1_rmse: 0.85295 |  0:00:24s
epoch 23 | loss: 0.73693 | val_0_rmse: 0.87931 | val_1_rmse: 0.86132 |  0:00:25s
epoch 24 | loss: 0.74812 | val_0_rmse: 0.86805 | val_1_rmse: 0.85066 |  0:00:26s
epoch 25 | loss: 0.74224 | val_0_rmse: 0.87479 | val_1_rmse: 0.85697 |  0:00:27s
epoch 26 | loss: 0.73457 | val_0_rmse: 0.86575 | val_1_rmse: 0.85478 |  0:00:28s
epoch 27 | loss: 0.72702 | val_0_rmse: 0.86196 | val_1_rmse: 0.85011 |  0:00:29s
epoch 28 | loss: 0.71591 | val_0_rmse: 0.85567 | val_1_rmse: 0.84545 |  0:00:30s
epoch 29 | loss: 0.71333 | val_0_rmse: 0.85502 | val_1_rmse: 0.84347 |  0:00:31s
epoch 30 | loss: 0.71892 | val_0_rmse: 0.85596 | val_1_rmse: 0.84601 |  0:00:32s
epoch 31 | loss: 0.72143 | val_0_rmse: 0.85221 | val_1_rmse: 0.84759 |  0:00:34s
epoch 32 | loss: 0.71706 | val_0_rmse: 0.84316 | val_1_rmse: 0.839   |  0:00:35s
epoch 33 | loss: 0.71482 | val_0_rmse: 0.84859 | val_1_rmse: 0.84978 |  0:00:36s
epoch 34 | loss: 0.70384 | val_0_rmse: 0.84037 | val_1_rmse: 0.83334 |  0:00:37s
epoch 35 | loss: 0.70835 | val_0_rmse: 0.84457 | val_1_rmse: 0.83967 |  0:00:38s
epoch 36 | loss: 0.70321 | val_0_rmse: 0.84782 | val_1_rmse: 0.84005 |  0:00:39s
epoch 37 | loss: 0.70154 | val_0_rmse: 0.84177 | val_1_rmse: 0.83824 |  0:00:40s
epoch 38 | loss: 0.70102 | val_0_rmse: 0.83508 | val_1_rmse: 0.83575 |  0:00:41s
epoch 39 | loss: 0.69292 | val_0_rmse: 0.8239  | val_1_rmse: 0.83721 |  0:00:42s
epoch 40 | loss: 0.69459 | val_0_rmse: 0.82878 | val_1_rmse: 0.8448  |  0:00:43s
epoch 41 | loss: 0.68088 | val_0_rmse: 0.82522 | val_1_rmse: 0.83428 |  0:00:44s
epoch 42 | loss: 0.68561 | val_0_rmse: 0.83    | val_1_rmse: 0.83452 |  0:00:45s
epoch 43 | loss: 0.69282 | val_0_rmse: 0.8244  | val_1_rmse: 0.83739 |  0:00:46s
epoch 44 | loss: 0.68593 | val_0_rmse: 0.81837 | val_1_rmse: 0.83287 |  0:00:47s
epoch 45 | loss: 0.6899  | val_0_rmse: 0.82366 | val_1_rmse: 0.83971 |  0:00:48s
epoch 46 | loss: 0.687   | val_0_rmse: 0.82768 | val_1_rmse: 0.83463 |  0:00:49s
epoch 47 | loss: 0.68228 | val_0_rmse: 0.83785 | val_1_rmse: 0.84689 |  0:00:50s
epoch 48 | loss: 0.71184 | val_0_rmse: 0.84961 | val_1_rmse: 0.85929 |  0:00:51s
epoch 49 | loss: 0.71095 | val_0_rmse: 0.8278  | val_1_rmse: 0.83723 |  0:00:52s
epoch 50 | loss: 0.68928 | val_0_rmse: 0.82157 | val_1_rmse: 0.83683 |  0:00:53s
epoch 51 | loss: 0.6894  | val_0_rmse: 0.8195  | val_1_rmse: 0.83775 |  0:00:54s
epoch 52 | loss: 0.684   | val_0_rmse: 0.81497 | val_1_rmse: 0.83272 |  0:00:55s
epoch 53 | loss: 0.67332 | val_0_rmse: 0.81131 | val_1_rmse: 0.82613 |  0:00:56s
epoch 54 | loss: 0.6682  | val_0_rmse: 0.81037 | val_1_rmse: 0.84078 |  0:00:57s
epoch 55 | loss: 0.66408 | val_0_rmse: 0.81107 | val_1_rmse: 0.83219 |  0:00:58s
epoch 56 | loss: 0.66715 | val_0_rmse: 0.80137 | val_1_rmse: 0.82888 |  0:00:59s
epoch 57 | loss: 0.67849 | val_0_rmse: 0.82411 | val_1_rmse: 0.85409 |  0:01:00s
epoch 58 | loss: 0.69326 | val_0_rmse: 0.81758 | val_1_rmse: 0.83355 |  0:01:02s
epoch 59 | loss: 0.68132 | val_0_rmse: 0.82769 | val_1_rmse: 0.83852 |  0:01:03s
epoch 60 | loss: 0.69293 | val_0_rmse: 0.81803 | val_1_rmse: 0.83769 |  0:01:04s
epoch 61 | loss: 0.68403 | val_0_rmse: 0.80574 | val_1_rmse: 0.83293 |  0:01:05s
epoch 62 | loss: 0.6724  | val_0_rmse: 0.80975 | val_1_rmse: 0.83555 |  0:01:06s
epoch 63 | loss: 0.66963 | val_0_rmse: 0.80398 | val_1_rmse: 0.8271  |  0:01:07s
epoch 64 | loss: 0.65702 | val_0_rmse: 0.82036 | val_1_rmse: 0.85379 |  0:01:08s
epoch 65 | loss: 0.68885 | val_0_rmse: 0.82156 | val_1_rmse: 0.84191 |  0:01:09s
epoch 66 | loss: 0.69454 | val_0_rmse: 0.82527 | val_1_rmse: 0.84369 |  0:01:10s
epoch 67 | loss: 0.68916 | val_0_rmse: 0.81338 | val_1_rmse: 0.84035 |  0:01:11s
epoch 68 | loss: 0.66777 | val_0_rmse: 0.82627 | val_1_rmse: 0.86723 |  0:01:12s
epoch 69 | loss: 0.66067 | val_0_rmse: 0.81068 | val_1_rmse: 0.83503 |  0:01:13s
epoch 70 | loss: 0.66223 | val_0_rmse: 0.80465 | val_1_rmse: 0.83786 |  0:01:14s
epoch 71 | loss: 0.66137 | val_0_rmse: 0.80455 | val_1_rmse: 0.84499 |  0:01:15s
epoch 72 | loss: 0.66125 | val_0_rmse: 0.79906 | val_1_rmse: 0.83351 |  0:01:16s
epoch 73 | loss: 0.65116 | val_0_rmse: 0.79343 | val_1_rmse: 0.83459 |  0:01:17s
epoch 74 | loss: 0.64011 | val_0_rmse: 0.78564 | val_1_rmse: 0.83113 |  0:01:18s
epoch 75 | loss: 0.635   | val_0_rmse: 0.78097 | val_1_rmse: 0.8247  |  0:01:19s
epoch 76 | loss: 0.62903 | val_0_rmse: 0.78759 | val_1_rmse: 0.82014 |  0:01:20s
epoch 77 | loss: 0.63235 | val_0_rmse: 0.77699 | val_1_rmse: 0.81716 |  0:01:21s
epoch 78 | loss: 0.63637 | val_0_rmse: 0.78622 | val_1_rmse: 0.84556 |  0:01:22s
epoch 79 | loss: 0.63094 | val_0_rmse: 0.79859 | val_1_rmse: 0.84884 |  0:01:23s
epoch 80 | loss: 0.64935 | val_0_rmse: 0.80494 | val_1_rmse: 0.84267 |  0:01:24s
epoch 81 | loss: 0.65342 | val_0_rmse: 0.79999 | val_1_rmse: 0.86701 |  0:01:26s
epoch 82 | loss: 0.63811 | val_0_rmse: 0.77586 | val_1_rmse: 0.83768 |  0:01:27s
epoch 83 | loss: 0.6321  | val_0_rmse: 0.79242 | val_1_rmse: 0.86364 |  0:01:28s
epoch 84 | loss: 0.63769 | val_0_rmse: 0.77534 | val_1_rmse: 0.85768 |  0:01:29s
epoch 85 | loss: 0.62637 | val_0_rmse: 0.78168 | val_1_rmse: 0.84532 |  0:01:30s
epoch 86 | loss: 0.62566 | val_0_rmse: 0.78011 | val_1_rmse: 0.8417  |  0:01:31s
epoch 87 | loss: 0.62506 | val_0_rmse: 0.77767 | val_1_rmse: 0.86392 |  0:01:32s
epoch 88 | loss: 0.60823 | val_0_rmse: 0.76585 | val_1_rmse: 0.83378 |  0:01:33s
epoch 89 | loss: 0.60523 | val_0_rmse: 0.76765 | val_1_rmse: 0.82761 |  0:01:34s
epoch 90 | loss: 0.60187 | val_0_rmse: 0.75956 | val_1_rmse: 0.82752 |  0:01:35s
epoch 91 | loss: 0.59938 | val_0_rmse: 0.7591  | val_1_rmse: 0.83081 |  0:01:36s
epoch 92 | loss: 0.60162 | val_0_rmse: 0.76642 | val_1_rmse: 0.84844 |  0:01:37s
epoch 93 | loss: 0.59257 | val_0_rmse: 0.75879 | val_1_rmse: 0.84625 |  0:01:38s
epoch 94 | loss: 0.58981 | val_0_rmse: 0.75843 | val_1_rmse: 0.84281 |  0:01:39s
epoch 95 | loss: 0.61904 | val_0_rmse: 0.78509 | val_1_rmse: 0.85481 |  0:01:40s
epoch 96 | loss: 0.61841 | val_0_rmse: 0.77265 | val_1_rmse: 0.8406  |  0:01:41s
epoch 97 | loss: 0.61498 | val_0_rmse: 0.77143 | val_1_rmse: 0.83341 |  0:01:42s
epoch 98 | loss: 0.61784 | val_0_rmse: 0.77712 | val_1_rmse: 0.85509 |  0:01:43s
epoch 99 | loss: 0.61392 | val_0_rmse: 0.76519 | val_1_rmse: 0.84214 |  0:01:44s
epoch 100| loss: 0.60953 | val_0_rmse: 0.7773  | val_1_rmse: 0.84385 |  0:01:45s
epoch 101| loss: 0.61834 | val_0_rmse: 0.76556 | val_1_rmse: 0.85201 |  0:01:46s
epoch 102| loss: 0.59958 | val_0_rmse: 0.77334 | val_1_rmse: 0.83296 |  0:01:47s
epoch 103| loss: 0.60593 | val_0_rmse: 0.76733 | val_1_rmse: 0.85175 |  0:01:48s
epoch 104| loss: 0.60466 | val_0_rmse: 0.76044 | val_1_rmse: 0.83967 |  0:01:49s
epoch 105| loss: 0.59564 | val_0_rmse: 0.76102 | val_1_rmse: 0.82762 |  0:01:50s
epoch 106| loss: 0.58738 | val_0_rmse: 0.75299 | val_1_rmse: 0.83371 |  0:01:51s
epoch 107| loss: 0.58474 | val_0_rmse: 0.75071 | val_1_rmse: 0.82915 |  0:01:53s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.81716
Best weights from best epoch are automatically used!
ended training at: 04:48:38
Feature importance:
Mean squared error is of 0.06610291361497564
Mean absolute error:0.18053091073085606
MAPE:0.19795470840237692
R2 score:0.2895094613675361
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:48:39
epoch 0  | loss: 2.84901 | val_0_rmse: 1.01585 | val_1_rmse: 0.96547 |  0:00:01s
epoch 1  | loss: 1.06273 | val_0_rmse: 0.99421 | val_1_rmse: 0.94542 |  0:00:02s
epoch 2  | loss: 0.9755  | val_0_rmse: 0.97308 | val_1_rmse: 0.91755 |  0:00:03s
epoch 3  | loss: 0.91943 | val_0_rmse: 0.95139 | val_1_rmse: 0.9054  |  0:00:04s
epoch 4  | loss: 0.90097 | val_0_rmse: 0.95476 | val_1_rmse: 0.91339 |  0:00:05s
epoch 5  | loss: 0.88667 | val_0_rmse: 0.95156 | val_1_rmse: 0.90153 |  0:00:06s
epoch 6  | loss: 0.87032 | val_0_rmse: 0.94654 | val_1_rmse: 0.89761 |  0:00:07s
epoch 7  | loss: 0.86609 | val_0_rmse: 0.95369 | val_1_rmse: 0.90693 |  0:00:08s
epoch 8  | loss: 0.85855 | val_0_rmse: 0.94877 | val_1_rmse: 0.89945 |  0:00:09s
epoch 9  | loss: 0.86175 | val_0_rmse: 0.95573 | val_1_rmse: 0.91012 |  0:00:10s
epoch 10 | loss: 0.85376 | val_0_rmse: 0.94754 | val_1_rmse: 0.90124 |  0:00:11s
epoch 11 | loss: 0.83655 | val_0_rmse: 0.92981 | val_1_rmse: 0.88301 |  0:00:12s
epoch 12 | loss: 0.83914 | val_0_rmse: 0.93206 | val_1_rmse: 0.88509 |  0:00:13s
epoch 13 | loss: 0.82473 | val_0_rmse: 0.92116 | val_1_rmse: 0.87737 |  0:00:14s
epoch 14 | loss: 0.82129 | val_0_rmse: 0.93323 | val_1_rmse: 0.88849 |  0:00:15s
epoch 15 | loss: 0.83094 | val_0_rmse: 0.92945 | val_1_rmse: 0.88324 |  0:00:16s
epoch 16 | loss: 0.82604 | val_0_rmse: 0.92089 | val_1_rmse: 0.8757  |  0:00:17s
epoch 17 | loss: 0.81103 | val_0_rmse: 0.91572 | val_1_rmse: 0.87141 |  0:00:18s
epoch 18 | loss: 0.80149 | val_0_rmse: 0.92556 | val_1_rmse: 0.88012 |  0:00:19s
epoch 19 | loss: 0.79911 | val_0_rmse: 0.93179 | val_1_rmse: 0.88728 |  0:00:20s
epoch 20 | loss: 0.79353 | val_0_rmse: 0.9303  | val_1_rmse: 0.88825 |  0:00:21s
epoch 21 | loss: 0.79054 | val_0_rmse: 0.91644 | val_1_rmse: 0.87199 |  0:00:22s
epoch 22 | loss: 0.78041 | val_0_rmse: 0.91829 | val_1_rmse: 0.87939 |  0:00:23s
epoch 23 | loss: 0.77698 | val_0_rmse: 0.91324 | val_1_rmse: 0.87265 |  0:00:24s
epoch 24 | loss: 0.77115 | val_0_rmse: 0.90161 | val_1_rmse: 0.86477 |  0:00:26s
epoch 25 | loss: 0.78727 | val_0_rmse: 0.91573 | val_1_rmse: 0.88059 |  0:00:27s
epoch 26 | loss: 0.78685 | val_0_rmse: 0.90569 | val_1_rmse: 0.86681 |  0:00:28s
epoch 27 | loss: 0.78182 | val_0_rmse: 0.89916 | val_1_rmse: 0.85881 |  0:00:29s
epoch 28 | loss: 0.77564 | val_0_rmse: 0.88516 | val_1_rmse: 0.85006 |  0:00:30s
epoch 29 | loss: 0.76655 | val_0_rmse: 0.8808  | val_1_rmse: 0.84537 |  0:00:31s
epoch 30 | loss: 0.7625  | val_0_rmse: 0.87386 | val_1_rmse: 0.84008 |  0:00:32s
epoch 31 | loss: 0.75674 | val_0_rmse: 0.87317 | val_1_rmse: 0.83982 |  0:00:33s
epoch 32 | loss: 0.75015 | val_0_rmse: 0.87431 | val_1_rmse: 0.84162 |  0:00:34s
epoch 33 | loss: 0.74566 | val_0_rmse: 0.86335 | val_1_rmse: 0.83676 |  0:00:35s
epoch 34 | loss: 0.74802 | val_0_rmse: 0.86556 | val_1_rmse: 0.83924 |  0:00:36s
epoch 35 | loss: 0.74674 | val_0_rmse: 0.86132 | val_1_rmse: 0.83262 |  0:00:37s
epoch 36 | loss: 0.74664 | val_0_rmse: 0.86068 | val_1_rmse: 0.84023 |  0:00:38s
epoch 37 | loss: 0.7396  | val_0_rmse: 0.85473 | val_1_rmse: 0.82363 |  0:00:39s
epoch 38 | loss: 0.72863 | val_0_rmse: 0.84774 | val_1_rmse: 0.8194  |  0:00:40s
epoch 39 | loss: 0.72727 | val_0_rmse: 0.86351 | val_1_rmse: 0.83341 |  0:00:41s
epoch 40 | loss: 0.72058 | val_0_rmse: 0.86968 | val_1_rmse: 0.84118 |  0:00:42s
epoch 41 | loss: 0.73309 | val_0_rmse: 0.85741 | val_1_rmse: 0.81503 |  0:00:43s
epoch 42 | loss: 0.72675 | val_0_rmse: 0.84973 | val_1_rmse: 0.81401 |  0:00:44s
epoch 43 | loss: 0.71882 | val_0_rmse: 0.84194 | val_1_rmse: 0.81134 |  0:00:45s
epoch 44 | loss: 0.71719 | val_0_rmse: 0.83696 | val_1_rmse: 0.80794 |  0:00:46s
epoch 45 | loss: 0.7112  | val_0_rmse: 0.83815 | val_1_rmse: 0.81301 |  0:00:47s
epoch 46 | loss: 0.70526 | val_0_rmse: 0.84107 | val_1_rmse: 0.81863 |  0:00:48s
epoch 47 | loss: 0.71703 | val_0_rmse: 0.86088 | val_1_rmse: 0.83822 |  0:00:49s
epoch 48 | loss: 0.70855 | val_0_rmse: 0.82874 | val_1_rmse: 0.80627 |  0:00:50s
epoch 49 | loss: 0.69471 | val_0_rmse: 0.83791 | val_1_rmse: 0.81334 |  0:00:52s
epoch 50 | loss: 0.70373 | val_0_rmse: 0.82747 | val_1_rmse: 0.8051  |  0:00:53s
epoch 51 | loss: 0.69841 | val_0_rmse: 0.83808 | val_1_rmse: 0.82063 |  0:00:54s
epoch 52 | loss: 0.69911 | val_0_rmse: 0.84421 | val_1_rmse: 0.82207 |  0:00:55s
epoch 53 | loss: 0.70274 | val_0_rmse: 0.83234 | val_1_rmse: 0.81835 |  0:00:56s
epoch 54 | loss: 0.69997 | val_0_rmse: 0.83626 | val_1_rmse: 0.81931 |  0:00:57s
epoch 55 | loss: 0.70118 | val_0_rmse: 0.82499 | val_1_rmse: 0.80945 |  0:00:58s
epoch 56 | loss: 0.69411 | val_0_rmse: 0.82929 | val_1_rmse: 0.82289 |  0:00:59s
epoch 57 | loss: 0.68936 | val_0_rmse: 0.81858 | val_1_rmse: 0.80842 |  0:01:00s
epoch 58 | loss: 0.68601 | val_0_rmse: 0.81857 | val_1_rmse: 0.80997 |  0:01:01s
epoch 59 | loss: 0.6866  | val_0_rmse: 0.82083 | val_1_rmse: 0.81269 |  0:01:02s
epoch 60 | loss: 0.68455 | val_0_rmse: 0.81993 | val_1_rmse: 0.80786 |  0:01:03s
epoch 61 | loss: 0.69061 | val_0_rmse: 0.84367 | val_1_rmse: 0.83171 |  0:01:04s
epoch 62 | loss: 0.69519 | val_0_rmse: 0.81642 | val_1_rmse: 0.80082 |  0:01:05s
epoch 63 | loss: 0.68119 | val_0_rmse: 0.83412 | val_1_rmse: 0.81833 |  0:01:06s
epoch 64 | loss: 0.69044 | val_0_rmse: 0.81963 | val_1_rmse: 0.8104  |  0:01:07s
epoch 65 | loss: 0.69556 | val_0_rmse: 0.82354 | val_1_rmse: 0.80508 |  0:01:08s
epoch 66 | loss: 0.69049 | val_0_rmse: 0.81848 | val_1_rmse: 0.80411 |  0:01:09s
epoch 67 | loss: 0.68265 | val_0_rmse: 0.82004 | val_1_rmse: 0.80708 |  0:01:10s
epoch 68 | loss: 0.66895 | val_0_rmse: 0.81839 | val_1_rmse: 0.81005 |  0:01:11s
epoch 69 | loss: 0.67751 | val_0_rmse: 0.80468 | val_1_rmse: 0.79312 |  0:01:12s
epoch 70 | loss: 0.66274 | val_0_rmse: 0.80227 | val_1_rmse: 0.80775 |  0:01:13s
epoch 71 | loss: 0.66469 | val_0_rmse: 0.84699 | val_1_rmse: 0.85077 |  0:01:14s
epoch 72 | loss: 0.67085 | val_0_rmse: 0.81534 | val_1_rmse: 0.8186  |  0:01:15s
epoch 73 | loss: 0.66285 | val_0_rmse: 0.80732 | val_1_rmse: 0.80377 |  0:01:16s
epoch 74 | loss: 0.65763 | val_0_rmse: 0.79561 | val_1_rmse: 0.80666 |  0:01:17s
epoch 75 | loss: 0.6664  | val_0_rmse: 0.80649 | val_1_rmse: 0.80345 |  0:01:19s
epoch 76 | loss: 0.65967 | val_0_rmse: 0.80717 | val_1_rmse: 0.80597 |  0:01:20s
epoch 77 | loss: 0.66261 | val_0_rmse: 0.80614 | val_1_rmse: 0.80538 |  0:01:21s
epoch 78 | loss: 0.65872 | val_0_rmse: 0.81319 | val_1_rmse: 0.82293 |  0:01:22s
epoch 79 | loss: 0.67125 | val_0_rmse: 0.80477 | val_1_rmse: 0.80359 |  0:01:23s
epoch 80 | loss: 0.66366 | val_0_rmse: 0.79738 | val_1_rmse: 0.8024  |  0:01:24s
epoch 81 | loss: 0.65328 | val_0_rmse: 0.80808 | val_1_rmse: 0.8128  |  0:01:25s
epoch 82 | loss: 0.65285 | val_0_rmse: 0.82623 | val_1_rmse: 0.83216 |  0:01:26s
epoch 83 | loss: 0.66012 | val_0_rmse: 0.79874 | val_1_rmse: 0.81082 |  0:01:27s
epoch 84 | loss: 0.6465  | val_0_rmse: 0.78726 | val_1_rmse: 0.80125 |  0:01:28s
epoch 85 | loss: 0.6466  | val_0_rmse: 0.78452 | val_1_rmse: 0.80067 |  0:01:29s
epoch 86 | loss: 0.64247 | val_0_rmse: 0.81982 | val_1_rmse: 0.82926 |  0:01:30s
epoch 87 | loss: 0.64161 | val_0_rmse: 0.7913  | val_1_rmse: 0.79402 |  0:01:31s
epoch 88 | loss: 0.65229 | val_0_rmse: 0.83006 | val_1_rmse: 0.84086 |  0:01:32s
epoch 89 | loss: 0.65712 | val_0_rmse: 0.79659 | val_1_rmse: 0.80296 |  0:01:33s
epoch 90 | loss: 0.6588  | val_0_rmse: 0.79978 | val_1_rmse: 0.81627 |  0:01:34s
epoch 91 | loss: 0.66008 | val_0_rmse: 0.89866 | val_1_rmse: 0.90987 |  0:01:35s
epoch 92 | loss: 0.70327 | val_0_rmse: 0.82393 | val_1_rmse: 0.82835 |  0:01:36s
epoch 93 | loss: 0.71013 | val_0_rmse: 0.88629 | val_1_rmse: 0.86702 |  0:01:37s
epoch 94 | loss: 0.7135  | val_0_rmse: 0.84217 | val_1_rmse: 0.84085 |  0:01:38s
epoch 95 | loss: 0.70552 | val_0_rmse: 0.91155 | val_1_rmse: 0.89963 |  0:01:39s
epoch 96 | loss: 0.70655 | val_0_rmse: 0.82208 | val_1_rmse: 0.81825 |  0:01:40s
epoch 97 | loss: 0.68983 | val_0_rmse: 0.81216 | val_1_rmse: 0.80866 |  0:01:41s
epoch 98 | loss: 0.67091 | val_0_rmse: 0.82113 | val_1_rmse: 0.81536 |  0:01:42s
epoch 99 | loss: 0.69397 | val_0_rmse: 0.84003 | val_1_rmse: 0.82775 |  0:01:43s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.79312
Best weights from best epoch are automatically used!
ended training at: 04:50:23
Feature importance:
Mean squared error is of 0.061511518210787944
Mean absolute error:0.18166788802334327
MAPE:0.19768493571820034
R2 score:0.26621922393826003
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:50:23
epoch 0  | loss: 2.23422 | val_0_rmse: 0.99047 | val_1_rmse: 1.04692 |  0:00:01s
epoch 1  | loss: 1.01001 | val_0_rmse: 0.99203 | val_1_rmse: 1.04727 |  0:00:02s
epoch 2  | loss: 0.98361 | val_0_rmse: 0.98839 | val_1_rmse: 1.04544 |  0:00:03s
epoch 3  | loss: 0.96202 | val_0_rmse: 0.98597 | val_1_rmse: 1.04175 |  0:00:04s
epoch 4  | loss: 0.94551 | val_0_rmse: 0.97179 | val_1_rmse: 1.02435 |  0:00:05s
epoch 5  | loss: 0.92404 | val_0_rmse: 0.96088 | val_1_rmse: 1.01485 |  0:00:06s
epoch 6  | loss: 0.90484 | val_0_rmse: 0.95771 | val_1_rmse: 1.00709 |  0:00:07s
epoch 7  | loss: 0.8759  | val_0_rmse: 0.94546 | val_1_rmse: 0.99587 |  0:00:08s
epoch 8  | loss: 0.85135 | val_0_rmse: 0.93039 | val_1_rmse: 0.98186 |  0:00:09s
epoch 9  | loss: 0.83473 | val_0_rmse: 0.97466 | val_1_rmse: 1.01846 |  0:00:10s
epoch 10 | loss: 0.83061 | val_0_rmse: 0.94584 | val_1_rmse: 0.99654 |  0:00:11s
epoch 11 | loss: 0.81359 | val_0_rmse: 0.92809 | val_1_rmse: 0.98274 |  0:00:12s
epoch 12 | loss: 0.79036 | val_0_rmse: 0.92033 | val_1_rmse: 0.97467 |  0:00:13s
epoch 13 | loss: 0.77209 | val_0_rmse: 0.91919 | val_1_rmse: 0.9697  |  0:00:14s
epoch 14 | loss: 0.752   | val_0_rmse: 0.92346 | val_1_rmse: 0.9754  |  0:00:15s
epoch 15 | loss: 0.73946 | val_0_rmse: 0.89694 | val_1_rmse: 0.95477 |  0:00:16s
epoch 16 | loss: 0.72143 | val_0_rmse: 0.89017 | val_1_rmse: 0.94463 |  0:00:17s
epoch 17 | loss: 0.73026 | val_0_rmse: 0.87346 | val_1_rmse: 0.93307 |  0:00:18s
epoch 18 | loss: 0.7154  | val_0_rmse: 0.85885 | val_1_rmse: 0.92214 |  0:00:19s
epoch 19 | loss: 0.69372 | val_0_rmse: 0.86517 | val_1_rmse: 0.93092 |  0:00:20s
epoch 20 | loss: 0.69115 | val_0_rmse: 0.84719 | val_1_rmse: 0.91106 |  0:00:21s
epoch 21 | loss: 0.68865 | val_0_rmse: 0.8511  | val_1_rmse: 0.91525 |  0:00:22s
epoch 22 | loss: 0.67722 | val_0_rmse: 0.84523 | val_1_rmse: 0.91165 |  0:00:23s
epoch 23 | loss: 0.66564 | val_0_rmse: 0.83562 | val_1_rmse: 0.89886 |  0:00:25s
epoch 24 | loss: 0.66655 | val_0_rmse: 0.8348  | val_1_rmse: 0.8979  |  0:00:26s
epoch 25 | loss: 0.661   | val_0_rmse: 0.84079 | val_1_rmse: 0.90382 |  0:00:27s
epoch 26 | loss: 0.65264 | val_0_rmse: 0.84832 | val_1_rmse: 0.92056 |  0:00:28s
epoch 27 | loss: 0.64874 | val_0_rmse: 0.82667 | val_1_rmse: 0.88823 |  0:00:29s
epoch 28 | loss: 0.65704 | val_0_rmse: 0.8644  | val_1_rmse: 0.94005 |  0:00:30s
epoch 29 | loss: 0.69111 | val_0_rmse: 0.84493 | val_1_rmse: 0.91252 |  0:00:31s
epoch 30 | loss: 0.66651 | val_0_rmse: 0.82509 | val_1_rmse: 0.89537 |  0:00:32s
epoch 31 | loss: 0.65857 | val_0_rmse: 0.81667 | val_1_rmse: 0.89156 |  0:00:33s
epoch 32 | loss: 0.65033 | val_0_rmse: 0.81926 | val_1_rmse: 0.89418 |  0:00:34s
epoch 33 | loss: 0.64234 | val_0_rmse: 0.82846 | val_1_rmse: 0.90987 |  0:00:35s
epoch 34 | loss: 0.64831 | val_0_rmse: 0.81651 | val_1_rmse: 0.89163 |  0:00:36s
epoch 35 | loss: 0.63531 | val_0_rmse: 0.79906 | val_1_rmse: 0.88092 |  0:00:37s
epoch 36 | loss: 0.6345  | val_0_rmse: 0.79562 | val_1_rmse: 0.88225 |  0:00:38s
epoch 37 | loss: 0.62229 | val_0_rmse: 0.79171 | val_1_rmse: 0.87722 |  0:00:39s
epoch 38 | loss: 0.61774 | val_0_rmse: 0.78831 | val_1_rmse: 0.8762  |  0:00:40s
epoch 39 | loss: 0.61653 | val_0_rmse: 0.78925 | val_1_rmse: 0.88272 |  0:00:41s
epoch 40 | loss: 0.61317 | val_0_rmse: 0.79627 | val_1_rmse: 0.88554 |  0:00:42s
epoch 41 | loss: 0.61331 | val_0_rmse: 0.78038 | val_1_rmse: 0.8737  |  0:00:43s
epoch 42 | loss: 0.60887 | val_0_rmse: 0.77826 | val_1_rmse: 0.88404 |  0:00:44s
epoch 43 | loss: 0.60057 | val_0_rmse: 0.77362 | val_1_rmse: 0.88078 |  0:00:45s
epoch 44 | loss: 0.6022  | val_0_rmse: 0.77762 | val_1_rmse: 0.88085 |  0:00:46s
epoch 45 | loss: 0.59699 | val_0_rmse: 0.76942 | val_1_rmse: 0.87658 |  0:00:47s
epoch 46 | loss: 0.59763 | val_0_rmse: 0.7732  | val_1_rmse: 0.89206 |  0:00:48s
epoch 47 | loss: 0.59688 | val_0_rmse: 0.76632 | val_1_rmse: 0.88306 |  0:00:49s
epoch 48 | loss: 0.60738 | val_0_rmse: 0.78412 | val_1_rmse: 0.878   |  0:00:51s
epoch 49 | loss: 0.61601 | val_0_rmse: 0.77192 | val_1_rmse: 0.87919 |  0:00:52s
epoch 50 | loss: 0.60961 | val_0_rmse: 0.767   | val_1_rmse: 0.87376 |  0:00:53s
epoch 51 | loss: 0.60272 | val_0_rmse: 0.7625  | val_1_rmse: 0.87164 |  0:00:54s
epoch 52 | loss: 0.59226 | val_0_rmse: 0.76147 | val_1_rmse: 0.87564 |  0:00:55s
epoch 53 | loss: 0.58626 | val_0_rmse: 0.75316 | val_1_rmse: 0.87414 |  0:00:56s
epoch 54 | loss: 0.58554 | val_0_rmse: 0.75751 | val_1_rmse: 0.8734  |  0:00:57s
epoch 55 | loss: 0.57882 | val_0_rmse: 0.75572 | val_1_rmse: 0.88874 |  0:00:58s
epoch 56 | loss: 0.57909 | val_0_rmse: 0.74237 | val_1_rmse: 0.86983 |  0:00:59s
epoch 57 | loss: 0.57387 | val_0_rmse: 0.74023 | val_1_rmse: 0.87112 |  0:01:00s
epoch 58 | loss: 0.57195 | val_0_rmse: 0.74036 | val_1_rmse: 0.87904 |  0:01:01s
epoch 59 | loss: 0.56813 | val_0_rmse: 0.73019 | val_1_rmse: 0.87468 |  0:01:02s
epoch 60 | loss: 0.56166 | val_0_rmse: 0.72929 | val_1_rmse: 0.88436 |  0:01:03s
epoch 61 | loss: 0.56042 | val_0_rmse: 0.73637 | val_1_rmse: 0.8711  |  0:01:04s
epoch 62 | loss: 0.56716 | val_0_rmse: 0.73088 | val_1_rmse: 0.88046 |  0:01:05s
epoch 63 | loss: 0.56362 | val_0_rmse: 0.72906 | val_1_rmse: 0.88758 |  0:01:06s
epoch 64 | loss: 0.55371 | val_0_rmse: 0.73173 | val_1_rmse: 0.8967  |  0:01:07s
epoch 65 | loss: 0.55125 | val_0_rmse: 0.72929 | val_1_rmse: 0.88183 |  0:01:08s
epoch 66 | loss: 0.55295 | val_0_rmse: 0.72883 | val_1_rmse: 0.88894 |  0:01:09s
epoch 67 | loss: 0.54878 | val_0_rmse: 0.73113 | val_1_rmse: 0.90965 |  0:01:10s
epoch 68 | loss: 0.54501 | val_0_rmse: 0.71554 | val_1_rmse: 0.88786 |  0:01:11s
epoch 69 | loss: 0.54907 | val_0_rmse: 0.71714 | val_1_rmse: 0.88268 |  0:01:12s
epoch 70 | loss: 0.54194 | val_0_rmse: 0.7329  | val_1_rmse: 0.89828 |  0:01:13s
epoch 71 | loss: 0.56134 | val_0_rmse: 0.7309  | val_1_rmse: 0.89582 |  0:01:14s
epoch 72 | loss: 0.55326 | val_0_rmse: 0.72346 | val_1_rmse: 0.88724 |  0:01:15s
epoch 73 | loss: 0.53512 | val_0_rmse: 0.71833 | val_1_rmse: 0.89687 |  0:01:16s
epoch 74 | loss: 0.54414 | val_0_rmse: 0.71919 | val_1_rmse: 0.88847 |  0:01:18s
epoch 75 | loss: 0.54362 | val_0_rmse: 0.71981 | val_1_rmse: 0.89065 |  0:01:19s
epoch 76 | loss: 0.54006 | val_0_rmse: 0.71473 | val_1_rmse: 0.88866 |  0:01:20s
epoch 77 | loss: 0.53178 | val_0_rmse: 0.70889 | val_1_rmse: 0.88553 |  0:01:21s
epoch 78 | loss: 0.53267 | val_0_rmse: 0.72246 | val_1_rmse: 0.90919 |  0:01:22s
epoch 79 | loss: 0.53327 | val_0_rmse: 0.70424 | val_1_rmse: 0.89041 |  0:01:23s
epoch 80 | loss: 0.52776 | val_0_rmse: 0.7081  | val_1_rmse: 0.88294 |  0:01:24s
epoch 81 | loss: 0.52677 | val_0_rmse: 0.70317 | val_1_rmse: 0.882   |  0:01:25s
epoch 82 | loss: 0.52076 | val_0_rmse: 0.7032  | val_1_rmse: 0.88472 |  0:01:26s
epoch 83 | loss: 0.53207 | val_0_rmse: 0.71159 | val_1_rmse: 0.89252 |  0:01:27s
epoch 84 | loss: 0.52997 | val_0_rmse: 0.71557 | val_1_rmse: 0.88901 |  0:01:28s
epoch 85 | loss: 0.51818 | val_0_rmse: 0.70737 | val_1_rmse: 0.89426 |  0:01:29s
epoch 86 | loss: 0.52151 | val_0_rmse: 0.70231 | val_1_rmse: 0.89731 |  0:01:30s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.86983
Best weights from best epoch are automatically used!
ended training at: 04:51:54
Feature importance:
Mean squared error is of 0.062215026358812646
Mean absolute error:0.18202095054954553
MAPE:0.19862234053950995
R2 score:0.28398336761306786
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:51:55
epoch 0  | loss: 2.37481 | val_0_rmse: 1.00921 | val_1_rmse: 1.00325 |  0:00:01s
epoch 1  | loss: 1.05356 | val_0_rmse: 1.00494 | val_1_rmse: 0.99807 |  0:00:02s
epoch 2  | loss: 0.98435 | val_0_rmse: 0.98195 | val_1_rmse: 0.97762 |  0:00:03s
epoch 3  | loss: 0.94418 | val_0_rmse: 0.96554 | val_1_rmse: 0.96393 |  0:00:04s
epoch 4  | loss: 0.9166  | val_0_rmse: 0.93841 | val_1_rmse: 0.94305 |  0:00:05s
epoch 5  | loss: 0.89874 | val_0_rmse: 0.93908 | val_1_rmse: 0.93343 |  0:00:06s
epoch 6  | loss: 0.87455 | val_0_rmse: 0.92458 | val_1_rmse: 0.91457 |  0:00:07s
epoch 7  | loss: 0.86206 | val_0_rmse: 0.94104 | val_1_rmse: 0.93255 |  0:00:08s
epoch 8  | loss: 0.83142 | val_0_rmse: 0.91437 | val_1_rmse: 0.91052 |  0:00:09s
epoch 9  | loss: 0.80039 | val_0_rmse: 0.89527 | val_1_rmse: 0.90698 |  0:00:10s
epoch 10 | loss: 0.77718 | val_0_rmse: 0.89214 | val_1_rmse: 0.90333 |  0:00:11s
epoch 11 | loss: 0.75969 | val_0_rmse: 0.89055 | val_1_rmse: 0.89722 |  0:00:12s
epoch 12 | loss: 0.74969 | val_0_rmse: 0.88843 | val_1_rmse: 0.8963  |  0:00:13s
epoch 13 | loss: 0.75205 | val_0_rmse: 0.87561 | val_1_rmse: 0.88396 |  0:00:14s
epoch 14 | loss: 0.7289  | val_0_rmse: 0.88003 | val_1_rmse: 0.8898  |  0:00:15s
epoch 15 | loss: 0.72262 | val_0_rmse: 0.87491 | val_1_rmse: 0.88722 |  0:00:16s
epoch 16 | loss: 0.71685 | val_0_rmse: 0.87168 | val_1_rmse: 0.88532 |  0:00:17s
epoch 17 | loss: 0.70655 | val_0_rmse: 0.87121 | val_1_rmse: 0.88129 |  0:00:18s
epoch 18 | loss: 0.70251 | val_0_rmse: 0.87123 | val_1_rmse: 0.88805 |  0:00:19s
epoch 19 | loss: 0.6935  | val_0_rmse: 0.85947 | val_1_rmse: 0.87342 |  0:00:20s
epoch 20 | loss: 0.68814 | val_0_rmse: 0.85898 | val_1_rmse: 0.87312 |  0:00:21s
epoch 21 | loss: 0.68703 | val_0_rmse: 0.85543 | val_1_rmse: 0.86665 |  0:00:22s
epoch 22 | loss: 0.6873  | val_0_rmse: 0.85475 | val_1_rmse: 0.86945 |  0:00:24s
epoch 23 | loss: 0.68151 | val_0_rmse: 0.85315 | val_1_rmse: 0.87404 |  0:00:25s
epoch 24 | loss: 0.68006 | val_0_rmse: 0.8456  | val_1_rmse: 0.86418 |  0:00:26s
epoch 25 | loss: 0.6739  | val_0_rmse: 0.8538  | val_1_rmse: 0.86943 |  0:00:27s
epoch 26 | loss: 0.67685 | val_0_rmse: 0.84655 | val_1_rmse: 0.86425 |  0:00:28s
epoch 27 | loss: 0.67928 | val_0_rmse: 0.84516 | val_1_rmse: 0.86937 |  0:00:29s
epoch 28 | loss: 0.66589 | val_0_rmse: 0.83509 | val_1_rmse: 0.86344 |  0:00:30s
epoch 29 | loss: 0.65928 | val_0_rmse: 0.84143 | val_1_rmse: 0.86098 |  0:00:31s
epoch 30 | loss: 0.65969 | val_0_rmse: 0.82357 | val_1_rmse: 0.8521  |  0:00:32s
epoch 31 | loss: 0.65316 | val_0_rmse: 0.82747 | val_1_rmse: 0.86629 |  0:00:33s
epoch 32 | loss: 0.65428 | val_0_rmse: 0.82129 | val_1_rmse: 0.85547 |  0:00:34s
epoch 33 | loss: 0.64794 | val_0_rmse: 0.83527 | val_1_rmse: 0.86624 |  0:00:35s
epoch 34 | loss: 0.6396  | val_0_rmse: 0.81488 | val_1_rmse: 0.8509  |  0:00:36s
epoch 35 | loss: 0.63916 | val_0_rmse: 0.80986 | val_1_rmse: 0.8557  |  0:00:37s
epoch 36 | loss: 0.64023 | val_0_rmse: 0.81244 | val_1_rmse: 0.85832 |  0:00:38s
epoch 37 | loss: 0.63354 | val_0_rmse: 0.79801 | val_1_rmse: 0.84476 |  0:00:39s
epoch 38 | loss: 0.64041 | val_0_rmse: 0.80025 | val_1_rmse: 0.84326 |  0:00:40s
epoch 39 | loss: 0.6294  | val_0_rmse: 0.79461 | val_1_rmse: 0.84678 |  0:00:41s
epoch 40 | loss: 0.63361 | val_0_rmse: 0.79109 | val_1_rmse: 0.83565 |  0:00:42s
epoch 41 | loss: 0.62446 | val_0_rmse: 0.78997 | val_1_rmse: 0.84783 |  0:00:43s
epoch 42 | loss: 0.63537 | val_0_rmse: 0.78742 | val_1_rmse: 0.83764 |  0:00:44s
epoch 43 | loss: 0.628   | val_0_rmse: 0.78255 | val_1_rmse: 0.84165 |  0:00:45s
epoch 44 | loss: 0.6238  | val_0_rmse: 0.82518 | val_1_rmse: 0.86419 |  0:00:46s
epoch 45 | loss: 0.62673 | val_0_rmse: 0.77842 | val_1_rmse: 0.83746 |  0:00:47s
epoch 46 | loss: 0.61934 | val_0_rmse: 0.7752  | val_1_rmse: 0.83294 |  0:00:49s
epoch 47 | loss: 0.61057 | val_0_rmse: 0.77556 | val_1_rmse: 0.82309 |  0:00:50s
epoch 48 | loss: 0.61137 | val_0_rmse: 0.77142 | val_1_rmse: 0.82722 |  0:00:51s
epoch 49 | loss: 0.61363 | val_0_rmse: 0.77665 | val_1_rmse: 0.8278  |  0:00:52s
epoch 50 | loss: 0.60927 | val_0_rmse: 0.78254 | val_1_rmse: 0.84097 |  0:00:53s
epoch 51 | loss: 0.59387 | val_0_rmse: 0.7555  | val_1_rmse: 0.82616 |  0:00:54s
epoch 52 | loss: 0.60078 | val_0_rmse: 0.7603  | val_1_rmse: 0.82966 |  0:00:55s
epoch 53 | loss: 0.60313 | val_0_rmse: 0.76487 | val_1_rmse: 0.84054 |  0:00:56s
epoch 54 | loss: 0.60831 | val_0_rmse: 0.76278 | val_1_rmse: 0.8495  |  0:00:57s
epoch 55 | loss: 0.5927  | val_0_rmse: 0.75026 | val_1_rmse: 0.82457 |  0:00:58s
epoch 56 | loss: 0.58468 | val_0_rmse: 0.75135 | val_1_rmse: 0.82104 |  0:00:59s
epoch 57 | loss: 0.59444 | val_0_rmse: 0.74454 | val_1_rmse: 0.83457 |  0:01:00s
epoch 58 | loss: 0.58387 | val_0_rmse: 0.77935 | val_1_rmse: 0.86977 |  0:01:01s
epoch 59 | loss: 0.60633 | val_0_rmse: 0.7543  | val_1_rmse: 0.82873 |  0:01:02s
epoch 60 | loss: 0.60175 | val_0_rmse: 0.75088 | val_1_rmse: 0.82268 |  0:01:03s
epoch 61 | loss: 0.58878 | val_0_rmse: 0.74126 | val_1_rmse: 0.82948 |  0:01:04s
epoch 62 | loss: 0.58194 | val_0_rmse: 0.74219 | val_1_rmse: 0.83912 |  0:01:05s
epoch 63 | loss: 0.57804 | val_0_rmse: 0.73612 | val_1_rmse: 0.8365  |  0:01:06s
epoch 64 | loss: 0.57105 | val_0_rmse: 0.74193 | val_1_rmse: 0.84196 |  0:01:07s
epoch 65 | loss: 0.57735 | val_0_rmse: 0.73229 | val_1_rmse: 0.82947 |  0:01:08s
epoch 66 | loss: 0.56647 | val_0_rmse: 0.72965 | val_1_rmse: 0.84229 |  0:01:09s
epoch 67 | loss: 0.56244 | val_0_rmse: 0.73568 | val_1_rmse: 0.84758 |  0:01:10s
epoch 68 | loss: 0.5632  | val_0_rmse: 0.73317 | val_1_rmse: 0.84213 |  0:01:11s
epoch 69 | loss: 0.56477 | val_0_rmse: 0.72921 | val_1_rmse: 0.84007 |  0:01:12s
epoch 70 | loss: 0.55821 | val_0_rmse: 0.72531 | val_1_rmse: 0.83222 |  0:01:13s
epoch 71 | loss: 0.55471 | val_0_rmse: 0.72477 | val_1_rmse: 0.83407 |  0:01:14s
epoch 72 | loss: 0.54764 | val_0_rmse: 0.73634 | val_1_rmse: 0.8416  |  0:01:16s
epoch 73 | loss: 0.55398 | val_0_rmse: 0.71733 | val_1_rmse: 0.83752 |  0:01:17s
epoch 74 | loss: 0.55996 | val_0_rmse: 0.72378 | val_1_rmse: 0.83036 |  0:01:18s
epoch 75 | loss: 0.56096 | val_0_rmse: 0.72243 | val_1_rmse: 0.84094 |  0:01:19s
epoch 76 | loss: 0.55926 | val_0_rmse: 0.70914 | val_1_rmse: 0.8546  |  0:01:20s
epoch 77 | loss: 0.54464 | val_0_rmse: 0.71345 | val_1_rmse: 0.86063 |  0:01:21s
epoch 78 | loss: 0.53765 | val_0_rmse: 0.71575 | val_1_rmse: 0.85608 |  0:01:22s
epoch 79 | loss: 0.54464 | val_0_rmse: 0.72613 | val_1_rmse: 0.86534 |  0:01:23s
epoch 80 | loss: 0.54248 | val_0_rmse: 0.71333 | val_1_rmse: 0.85094 |  0:01:24s
epoch 81 | loss: 0.53277 | val_0_rmse: 0.71513 | val_1_rmse: 0.84667 |  0:01:25s
epoch 82 | loss: 0.53614 | val_0_rmse: 0.71533 | val_1_rmse: 0.86735 |  0:01:26s
epoch 83 | loss: 0.53999 | val_0_rmse: 0.70683 | val_1_rmse: 0.83624 |  0:01:27s
epoch 84 | loss: 0.53347 | val_0_rmse: 0.71072 | val_1_rmse: 0.83194 |  0:01:28s
epoch 85 | loss: 0.53486 | val_0_rmse: 0.70985 | val_1_rmse: 0.83162 |  0:01:29s
epoch 86 | loss: 0.53325 | val_0_rmse: 0.70737 | val_1_rmse: 0.84529 |  0:01:30s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.82104
Best weights from best epoch are automatically used!
ended training at: 04:53:26
Feature importance:
Mean squared error is of 0.06019366183132178
Mean absolute error:0.17821574244384708
MAPE:0.1927271788656541
R2 score:0.2635607542917755
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:53:26
epoch 0  | loss: 2.68529 | val_0_rmse: 0.99278 | val_1_rmse: 0.98932 |  0:00:01s
epoch 1  | loss: 1.01328 | val_0_rmse: 0.99189 | val_1_rmse: 0.98952 |  0:00:02s
epoch 2  | loss: 0.98587 | val_0_rmse: 0.98752 | val_1_rmse: 0.98653 |  0:00:03s
epoch 3  | loss: 0.96256 | val_0_rmse: 0.98397 | val_1_rmse: 0.9795  |  0:00:04s
epoch 4  | loss: 0.93174 | val_0_rmse: 0.96731 | val_1_rmse: 0.95975 |  0:00:05s
epoch 5  | loss: 0.90778 | val_0_rmse: 0.94799 | val_1_rmse: 0.93838 |  0:00:06s
epoch 6  | loss: 0.8996  | val_0_rmse: 0.92913 | val_1_rmse: 0.91908 |  0:00:07s
epoch 7  | loss: 0.8625  | val_0_rmse: 0.95488 | val_1_rmse: 0.92148 |  0:00:08s
epoch 8  | loss: 0.83357 | val_0_rmse: 0.92757 | val_1_rmse: 0.89759 |  0:00:09s
epoch 9  | loss: 0.79306 | val_0_rmse: 0.90914 | val_1_rmse: 0.88227 |  0:00:10s
epoch 10 | loss: 0.77522 | val_0_rmse: 0.8857  | val_1_rmse: 0.86124 |  0:00:11s
epoch 11 | loss: 0.76697 | val_0_rmse: 0.88339 | val_1_rmse: 0.86278 |  0:00:12s
epoch 12 | loss: 0.75036 | val_0_rmse: 0.877   | val_1_rmse: 0.85819 |  0:00:13s
epoch 13 | loss: 0.7385  | val_0_rmse: 0.87744 | val_1_rmse: 0.85641 |  0:00:14s
epoch 14 | loss: 0.72991 | val_0_rmse: 0.87061 | val_1_rmse: 0.84811 |  0:00:15s
epoch 15 | loss: 0.72755 | val_0_rmse: 0.88105 | val_1_rmse: 0.85716 |  0:00:16s
epoch 16 | loss: 0.72279 | val_0_rmse: 0.86297 | val_1_rmse: 0.84363 |  0:00:17s
epoch 17 | loss: 0.72232 | val_0_rmse: 0.86595 | val_1_rmse: 0.84561 |  0:00:18s
epoch 18 | loss: 0.71319 | val_0_rmse: 0.85564 | val_1_rmse: 0.83874 |  0:00:19s
epoch 19 | loss: 0.70101 | val_0_rmse: 0.85884 | val_1_rmse: 0.84165 |  0:00:20s
epoch 20 | loss: 0.7001  | val_0_rmse: 0.85401 | val_1_rmse: 0.83726 |  0:00:21s
epoch 21 | loss: 0.69559 | val_0_rmse: 0.85455 | val_1_rmse: 0.83761 |  0:00:23s
epoch 22 | loss: 0.68521 | val_0_rmse: 0.85698 | val_1_rmse: 0.84315 |  0:00:24s
epoch 23 | loss: 0.67691 | val_0_rmse: 0.84217 | val_1_rmse: 0.82995 |  0:00:25s
epoch 24 | loss: 0.67997 | val_0_rmse: 0.84021 | val_1_rmse: 0.82968 |  0:00:26s
epoch 25 | loss: 0.67581 | val_0_rmse: 0.84156 | val_1_rmse: 0.83165 |  0:00:27s
epoch 26 | loss: 0.67074 | val_0_rmse: 0.84169 | val_1_rmse: 0.82636 |  0:00:28s
epoch 27 | loss: 0.66338 | val_0_rmse: 0.83481 | val_1_rmse: 0.81915 |  0:00:29s
epoch 28 | loss: 0.65988 | val_0_rmse: 0.83007 | val_1_rmse: 0.81638 |  0:00:30s
epoch 29 | loss: 0.6559  | val_0_rmse: 0.8357  | val_1_rmse: 0.82835 |  0:00:31s
epoch 30 | loss: 0.6584  | val_0_rmse: 0.83775 | val_1_rmse: 0.81772 |  0:00:32s
epoch 31 | loss: 0.65727 | val_0_rmse: 0.82627 | val_1_rmse: 0.81335 |  0:00:33s
epoch 32 | loss: 0.64529 | val_0_rmse: 0.82743 | val_1_rmse: 0.81391 |  0:00:34s
epoch 33 | loss: 0.64849 | val_0_rmse: 0.81803 | val_1_rmse: 0.81642 |  0:00:35s
epoch 34 | loss: 0.64498 | val_0_rmse: 0.81431 | val_1_rmse: 0.81381 |  0:00:36s
epoch 35 | loss: 0.6464  | val_0_rmse: 0.81079 | val_1_rmse: 0.81213 |  0:00:37s
epoch 36 | loss: 0.64213 | val_0_rmse: 0.80608 | val_1_rmse: 0.8031  |  0:00:38s
epoch 37 | loss: 0.63243 | val_0_rmse: 0.81091 | val_1_rmse: 0.80911 |  0:00:39s
epoch 38 | loss: 0.63222 | val_0_rmse: 0.80587 | val_1_rmse: 0.80893 |  0:00:40s
epoch 39 | loss: 0.63298 | val_0_rmse: 0.82394 | val_1_rmse: 0.82719 |  0:00:41s
epoch 40 | loss: 0.62208 | val_0_rmse: 0.78302 | val_1_rmse: 0.80013 |  0:00:42s
epoch 41 | loss: 0.62228 | val_0_rmse: 0.8224  | val_1_rmse: 0.83024 |  0:00:43s
epoch 42 | loss: 0.62332 | val_0_rmse: 0.77684 | val_1_rmse: 0.80112 |  0:00:44s
epoch 43 | loss: 0.61979 | val_0_rmse: 0.77775 | val_1_rmse: 0.79754 |  0:00:45s
epoch 44 | loss: 0.60649 | val_0_rmse: 0.7738  | val_1_rmse: 0.80834 |  0:00:46s
epoch 45 | loss: 0.62447 | val_0_rmse: 0.82068 | val_1_rmse: 0.83219 |  0:00:47s
epoch 46 | loss: 0.61488 | val_0_rmse: 0.77476 | val_1_rmse: 0.8071  |  0:00:49s
epoch 47 | loss: 0.60411 | val_0_rmse: 0.76663 | val_1_rmse: 0.80599 |  0:00:50s
epoch 48 | loss: 0.59528 | val_0_rmse: 0.7662  | val_1_rmse: 0.80601 |  0:00:51s
epoch 49 | loss: 0.59545 | val_0_rmse: 0.75571 | val_1_rmse: 0.80484 |  0:00:52s
epoch 50 | loss: 0.59004 | val_0_rmse: 0.75757 | val_1_rmse: 0.80847 |  0:00:53s
epoch 51 | loss: 0.58714 | val_0_rmse: 0.75147 | val_1_rmse: 0.79944 |  0:00:54s
epoch 52 | loss: 0.58588 | val_0_rmse: 0.75305 | val_1_rmse: 0.79956 |  0:00:55s
epoch 53 | loss: 0.58801 | val_0_rmse: 0.77035 | val_1_rmse: 0.81749 |  0:00:56s
epoch 54 | loss: 0.58885 | val_0_rmse: 0.75694 | val_1_rmse: 0.80445 |  0:00:57s
epoch 55 | loss: 0.57998 | val_0_rmse: 0.73949 | val_1_rmse: 0.79966 |  0:00:58s
epoch 56 | loss: 0.57752 | val_0_rmse: 0.73661 | val_1_rmse: 0.80475 |  0:00:59s
epoch 57 | loss: 0.57002 | val_0_rmse: 0.73648 | val_1_rmse: 0.80283 |  0:01:00s
epoch 58 | loss: 0.56533 | val_0_rmse: 0.73335 | val_1_rmse: 0.82323 |  0:01:01s
epoch 59 | loss: 0.56037 | val_0_rmse: 0.73216 | val_1_rmse: 0.81398 |  0:01:02s
epoch 60 | loss: 0.55654 | val_0_rmse: 0.73793 | val_1_rmse: 0.82336 |  0:01:03s
epoch 61 | loss: 0.55028 | val_0_rmse: 0.72944 | val_1_rmse: 0.83191 |  0:01:04s
epoch 62 | loss: 0.56139 | val_0_rmse: 0.73853 | val_1_rmse: 0.82234 |  0:01:05s
epoch 63 | loss: 0.56773 | val_0_rmse: 0.74156 | val_1_rmse: 0.82739 |  0:01:06s
epoch 64 | loss: 0.56898 | val_0_rmse: 0.73707 | val_1_rmse: 0.8299  |  0:01:07s
epoch 65 | loss: 0.56422 | val_0_rmse: 0.72597 | val_1_rmse: 0.82453 |  0:01:08s
epoch 66 | loss: 0.55773 | val_0_rmse: 0.72573 | val_1_rmse: 0.88275 |  0:01:09s
epoch 67 | loss: 0.54855 | val_0_rmse: 0.71928 | val_1_rmse: 0.84515 |  0:01:10s
epoch 68 | loss: 0.54955 | val_0_rmse: 0.7196  | val_1_rmse: 0.8437  |  0:01:11s
epoch 69 | loss: 0.54091 | val_0_rmse: 0.72357 | val_1_rmse: 0.84657 |  0:01:12s
epoch 70 | loss: 0.54536 | val_0_rmse: 0.70928 | val_1_rmse: 0.85843 |  0:01:13s
epoch 71 | loss: 0.53074 | val_0_rmse: 0.70949 | val_1_rmse: 0.82474 |  0:01:15s
epoch 72 | loss: 0.53619 | val_0_rmse: 0.72059 | val_1_rmse: 0.83549 |  0:01:16s
epoch 73 | loss: 0.53404 | val_0_rmse: 0.70999 | val_1_rmse: 0.85436 |  0:01:17s

Early stopping occured at epoch 73 with best_epoch = 43 and best_val_1_rmse = 0.79754
Best weights from best epoch are automatically used!
ended training at: 04:54:44
Feature importance:
Mean squared error is of 0.09855888678419505
Mean absolute error:0.18891352018449553
MAPE:0.20188794545638214
R2 score:0.22133420753295052
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:54:46
epoch 0  | loss: 1.2727  | val_0_rmse: 0.91693 | val_1_rmse: 0.88589 |  0:00:00s
epoch 1  | loss: 0.71369 | val_0_rmse: 0.78786 | val_1_rmse: 0.78619 |  0:00:01s
epoch 2  | loss: 0.54609 | val_0_rmse: 0.88948 | val_1_rmse: 0.86908 |  0:00:02s
epoch 3  | loss: 0.46547 | val_0_rmse: 0.7607  | val_1_rmse: 0.74117 |  0:00:03s
epoch 4  | loss: 0.42529 | val_0_rmse: 0.69106 | val_1_rmse: 0.67574 |  0:00:04s
epoch 5  | loss: 0.34261 | val_0_rmse: 0.72248 | val_1_rmse: 0.69064 |  0:00:05s
epoch 6  | loss: 0.2849  | val_0_rmse: 0.73782 | val_1_rmse: 0.71563 |  0:00:06s
epoch 7  | loss: 0.24863 | val_0_rmse: 0.60534 | val_1_rmse: 0.59661 |  0:00:07s
epoch 8  | loss: 0.22839 | val_0_rmse: 0.63083 | val_1_rmse: 0.61886 |  0:00:08s
epoch 9  | loss: 0.22255 | val_0_rmse: 0.58951 | val_1_rmse: 0.57688 |  0:00:09s
epoch 10 | loss: 0.21638 | val_0_rmse: 0.54547 | val_1_rmse: 0.53836 |  0:00:10s
epoch 11 | loss: 0.20783 | val_0_rmse: 0.58763 | val_1_rmse: 0.57447 |  0:00:11s
epoch 12 | loss: 0.19913 | val_0_rmse: 0.60378 | val_1_rmse: 0.59023 |  0:00:12s
epoch 13 | loss: 0.19174 | val_0_rmse: 0.57294 | val_1_rmse: 0.55993 |  0:00:13s
epoch 14 | loss: 0.19694 | val_0_rmse: 0.51822 | val_1_rmse: 0.51269 |  0:00:14s
epoch 15 | loss: 0.19177 | val_0_rmse: 0.47405 | val_1_rmse: 0.47183 |  0:00:15s
epoch 16 | loss: 0.18578 | val_0_rmse: 0.56314 | val_1_rmse: 0.55548 |  0:00:16s
epoch 17 | loss: 0.18654 | val_0_rmse: 0.48845 | val_1_rmse: 0.48464 |  0:00:17s
epoch 18 | loss: 0.18407 | val_0_rmse: 0.55566 | val_1_rmse: 0.54781 |  0:00:18s
epoch 19 | loss: 0.18201 | val_0_rmse: 0.46787 | val_1_rmse: 0.4651  |  0:00:19s
epoch 20 | loss: 0.18238 | val_0_rmse: 0.48756 | val_1_rmse: 0.48302 |  0:00:20s
epoch 21 | loss: 0.18331 | val_0_rmse: 0.46073 | val_1_rmse: 0.46035 |  0:00:21s
epoch 22 | loss: 0.17377 | val_0_rmse: 0.44909 | val_1_rmse: 0.44913 |  0:00:22s
epoch 23 | loss: 0.16951 | val_0_rmse: 0.42727 | val_1_rmse: 0.42862 |  0:00:22s
epoch 24 | loss: 0.16554 | val_0_rmse: 0.43819 | val_1_rmse: 0.43489 |  0:00:23s
epoch 25 | loss: 0.16245 | val_0_rmse: 0.4184  | val_1_rmse: 0.41918 |  0:00:24s
epoch 26 | loss: 0.16298 | val_0_rmse: 0.42962 | val_1_rmse: 0.42491 |  0:00:25s
epoch 27 | loss: 0.15834 | val_0_rmse: 0.41188 | val_1_rmse: 0.41142 |  0:00:26s
epoch 28 | loss: 0.16106 | val_0_rmse: 0.44797 | val_1_rmse: 0.43997 |  0:00:27s
epoch 29 | loss: 0.16077 | val_0_rmse: 0.40635 | val_1_rmse: 0.40612 |  0:00:28s
epoch 30 | loss: 0.15662 | val_0_rmse: 0.39471 | val_1_rmse: 0.39698 |  0:00:29s
epoch 31 | loss: 0.1569  | val_0_rmse: 0.40245 | val_1_rmse: 0.41005 |  0:00:30s
epoch 32 | loss: 0.15792 | val_0_rmse: 0.43232 | val_1_rmse: 0.43526 |  0:00:31s
epoch 33 | loss: 0.16547 | val_0_rmse: 0.43664 | val_1_rmse: 0.43412 |  0:00:32s
epoch 34 | loss: 0.16096 | val_0_rmse: 0.39082 | val_1_rmse: 0.3996  |  0:00:33s
epoch 35 | loss: 0.15174 | val_0_rmse: 0.38854 | val_1_rmse: 0.39105 |  0:00:34s
epoch 36 | loss: 0.15098 | val_0_rmse: 0.38895 | val_1_rmse: 0.38832 |  0:00:35s
epoch 37 | loss: 0.14616 | val_0_rmse: 0.38527 | val_1_rmse: 0.38889 |  0:00:36s
epoch 38 | loss: 0.15181 | val_0_rmse: 0.36985 | val_1_rmse: 0.37273 |  0:00:37s
epoch 39 | loss: 0.1533  | val_0_rmse: 0.3723  | val_1_rmse: 0.37891 |  0:00:38s
epoch 40 | loss: 0.15151 | val_0_rmse: 0.37927 | val_1_rmse: 0.38436 |  0:00:39s
epoch 41 | loss: 0.1487  | val_0_rmse: 0.37173 | val_1_rmse: 0.37925 |  0:00:40s
epoch 42 | loss: 0.14437 | val_0_rmse: 0.36477 | val_1_rmse: 0.37055 |  0:00:41s
epoch 43 | loss: 0.14278 | val_0_rmse: 0.4019  | val_1_rmse: 0.41004 |  0:00:42s
epoch 44 | loss: 0.14589 | val_0_rmse: 0.37369 | val_1_rmse: 0.37692 |  0:00:43s
epoch 45 | loss: 0.14303 | val_0_rmse: 0.41178 | val_1_rmse: 0.41045 |  0:00:44s
epoch 46 | loss: 0.1494  | val_0_rmse: 0.41682 | val_1_rmse: 0.42057 |  0:00:45s
epoch 47 | loss: 0.15372 | val_0_rmse: 0.37438 | val_1_rmse: 0.38043 |  0:00:46s
epoch 48 | loss: 0.15425 | val_0_rmse: 0.38767 | val_1_rmse: 0.39646 |  0:00:46s
epoch 49 | loss: 0.14533 | val_0_rmse: 0.352   | val_1_rmse: 0.35677 |  0:00:47s
epoch 50 | loss: 0.14815 | val_0_rmse: 0.36071 | val_1_rmse: 0.36844 |  0:00:48s
epoch 51 | loss: 0.146   | val_0_rmse: 0.35116 | val_1_rmse: 0.36389 |  0:00:49s
epoch 52 | loss: 0.14136 | val_0_rmse: 0.34981 | val_1_rmse: 0.35937 |  0:00:50s
epoch 53 | loss: 0.1388  | val_0_rmse: 0.35282 | val_1_rmse: 0.35924 |  0:00:51s
epoch 54 | loss: 0.14449 | val_0_rmse: 0.3555  | val_1_rmse: 0.36629 |  0:00:52s
epoch 55 | loss: 0.14904 | val_0_rmse: 0.36203 | val_1_rmse: 0.36742 |  0:00:53s
epoch 56 | loss: 0.14583 | val_0_rmse: 0.36208 | val_1_rmse: 0.37018 |  0:00:54s
epoch 57 | loss: 0.14102 | val_0_rmse: 0.35243 | val_1_rmse: 0.36171 |  0:00:55s
epoch 58 | loss: 0.13615 | val_0_rmse: 0.36638 | val_1_rmse: 0.37756 |  0:00:56s
epoch 59 | loss: 0.13872 | val_0_rmse: 0.36076 | val_1_rmse: 0.3698  |  0:00:57s
epoch 60 | loss: 0.13513 | val_0_rmse: 0.34528 | val_1_rmse: 0.35549 |  0:00:58s
epoch 61 | loss: 0.1342  | val_0_rmse: 0.34861 | val_1_rmse: 0.35691 |  0:00:59s
epoch 62 | loss: 0.13829 | val_0_rmse: 0.34938 | val_1_rmse: 0.35822 |  0:01:00s
epoch 63 | loss: 0.14077 | val_0_rmse: 0.38953 | val_1_rmse: 0.39319 |  0:01:01s
epoch 64 | loss: 0.13653 | val_0_rmse: 0.3542  | val_1_rmse: 0.36442 |  0:01:02s
epoch 65 | loss: 0.13601 | val_0_rmse: 0.33801 | val_1_rmse: 0.34777 |  0:01:03s
epoch 66 | loss: 0.13121 | val_0_rmse: 0.33603 | val_1_rmse: 0.34545 |  0:01:04s
epoch 67 | loss: 0.13335 | val_0_rmse: 0.39218 | val_1_rmse: 0.40274 |  0:01:05s
epoch 68 | loss: 0.13227 | val_0_rmse: 0.35596 | val_1_rmse: 0.3673  |  0:01:06s
epoch 69 | loss: 0.13431 | val_0_rmse: 0.3422  | val_1_rmse: 0.35457 |  0:01:07s
epoch 70 | loss: 0.13147 | val_0_rmse: 0.3452  | val_1_rmse: 0.35663 |  0:01:08s
epoch 71 | loss: 0.13122 | val_0_rmse: 0.35348 | val_1_rmse: 0.36534 |  0:01:09s
epoch 72 | loss: 0.13301 | val_0_rmse: 0.34387 | val_1_rmse: 0.35802 |  0:01:10s
epoch 73 | loss: 0.13431 | val_0_rmse: 0.35056 | val_1_rmse: 0.36257 |  0:01:10s
epoch 74 | loss: 0.13006 | val_0_rmse: 0.33676 | val_1_rmse: 0.35275 |  0:01:11s
epoch 75 | loss: 0.13008 | val_0_rmse: 0.35942 | val_1_rmse: 0.37054 |  0:01:12s
epoch 76 | loss: 0.13688 | val_0_rmse: 0.36085 | val_1_rmse: 0.36635 |  0:01:13s
epoch 77 | loss: 0.13724 | val_0_rmse: 0.3409  | val_1_rmse: 0.35404 |  0:01:14s
epoch 78 | loss: 0.14012 | val_0_rmse: 0.34946 | val_1_rmse: 0.3569  |  0:01:15s
epoch 79 | loss: 0.13341 | val_0_rmse: 0.3471  | val_1_rmse: 0.36333 |  0:01:16s
epoch 80 | loss: 0.13403 | val_0_rmse: 0.35671 | val_1_rmse: 0.37271 |  0:01:17s
epoch 81 | loss: 0.13488 | val_0_rmse: 0.34498 | val_1_rmse: 0.35613 |  0:01:18s
epoch 82 | loss: 0.12859 | val_0_rmse: 0.34817 | val_1_rmse: 0.36114 |  0:01:19s
epoch 83 | loss: 0.12838 | val_0_rmse: 0.33544 | val_1_rmse: 0.35093 |  0:01:20s
epoch 84 | loss: 0.1288  | val_0_rmse: 0.36816 | val_1_rmse: 0.37796 |  0:01:21s
epoch 85 | loss: 0.13184 | val_0_rmse: 0.33029 | val_1_rmse: 0.34837 |  0:01:22s
epoch 86 | loss: 0.1272  | val_0_rmse: 0.34057 | val_1_rmse: 0.35858 |  0:01:23s
epoch 87 | loss: 0.12968 | val_0_rmse: 0.34362 | val_1_rmse: 0.35937 |  0:01:24s
epoch 88 | loss: 0.12849 | val_0_rmse: 0.34031 | val_1_rmse: 0.35526 |  0:01:25s
epoch 89 | loss: 0.12568 | val_0_rmse: 0.32944 | val_1_rmse: 0.34596 |  0:01:26s
epoch 90 | loss: 0.12779 | val_0_rmse: 0.33771 | val_1_rmse: 0.35496 |  0:01:27s
epoch 91 | loss: 0.12505 | val_0_rmse: 0.33176 | val_1_rmse: 0.35005 |  0:01:28s
epoch 92 | loss: 0.12357 | val_0_rmse: 0.34346 | val_1_rmse: 0.35974 |  0:01:29s
epoch 93 | loss: 0.12754 | val_0_rmse: 0.34059 | val_1_rmse: 0.35509 |  0:01:30s
epoch 94 | loss: 0.12642 | val_0_rmse: 0.34666 | val_1_rmse: 0.35893 |  0:01:31s
epoch 95 | loss: 0.12704 | val_0_rmse: 0.34756 | val_1_rmse: 0.36582 |  0:01:32s
epoch 96 | loss: 0.13601 | val_0_rmse: 0.36893 | val_1_rmse: 0.37972 |  0:01:33s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.34545
Best weights from best epoch are automatically used!
ended training at: 04:56:20
Feature importance:
Mean squared error is of 0.06479000159047794
Mean absolute error:0.16374035906540896
MAPE:0.18572577340654117
R2 score:0.8654881063486888
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:56:20
epoch 0  | loss: 1.41811 | val_0_rmse: 0.97426 | val_1_rmse: 0.97246 |  0:00:00s
epoch 1  | loss: 0.70786 | val_0_rmse: 0.87472 | val_1_rmse: 0.88568 |  0:00:01s
epoch 2  | loss: 0.55475 | val_0_rmse: 0.80212 | val_1_rmse: 0.80123 |  0:00:02s
epoch 3  | loss: 0.41722 | val_0_rmse: 0.7833  | val_1_rmse: 0.76287 |  0:00:03s
epoch 4  | loss: 0.32378 | val_0_rmse: 0.76133 | val_1_rmse: 0.74667 |  0:00:04s
epoch 5  | loss: 0.31325 | val_0_rmse: 0.69728 | val_1_rmse: 0.6981  |  0:00:05s
epoch 6  | loss: 0.26864 | val_0_rmse: 0.66801 | val_1_rmse: 0.66112 |  0:00:06s
epoch 7  | loss: 0.25824 | val_0_rmse: 0.65319 | val_1_rmse: 0.65125 |  0:00:07s
epoch 8  | loss: 0.23697 | val_0_rmse: 0.65364 | val_1_rmse: 0.65365 |  0:00:08s
epoch 9  | loss: 0.24559 | val_0_rmse: 0.62459 | val_1_rmse: 0.6227  |  0:00:09s
epoch 10 | loss: 0.22848 | val_0_rmse: 0.57044 | val_1_rmse: 0.56311 |  0:00:10s
epoch 11 | loss: 0.21045 | val_0_rmse: 0.58092 | val_1_rmse: 0.59078 |  0:00:11s
epoch 12 | loss: 0.21527 | val_0_rmse: 0.55255 | val_1_rmse: 0.55039 |  0:00:12s
epoch 13 | loss: 0.21442 | val_0_rmse: 0.55077 | val_1_rmse: 0.54425 |  0:00:13s
epoch 14 | loss: 0.21683 | val_0_rmse: 0.54287 | val_1_rmse: 0.53746 |  0:00:14s
epoch 15 | loss: 0.19502 | val_0_rmse: 0.51665 | val_1_rmse: 0.52041 |  0:00:15s
epoch 16 | loss: 0.18699 | val_0_rmse: 0.51594 | val_1_rmse: 0.52277 |  0:00:16s
epoch 17 | loss: 0.18179 | val_0_rmse: 0.49716 | val_1_rmse: 0.4923  |  0:00:17s
epoch 18 | loss: 0.18098 | val_0_rmse: 0.49627 | val_1_rmse: 0.49245 |  0:00:18s
epoch 19 | loss: 0.18446 | val_0_rmse: 0.48202 | val_1_rmse: 0.48461 |  0:00:19s
epoch 20 | loss: 0.18442 | val_0_rmse: 0.49175 | val_1_rmse: 0.49723 |  0:00:20s
epoch 21 | loss: 0.1787  | val_0_rmse: 0.47598 | val_1_rmse: 0.48482 |  0:00:21s
epoch 22 | loss: 0.17251 | val_0_rmse: 0.44986 | val_1_rmse: 0.45755 |  0:00:22s
epoch 23 | loss: 0.17102 | val_0_rmse: 0.44878 | val_1_rmse: 0.45788 |  0:00:23s
epoch 24 | loss: 0.17023 | val_0_rmse: 0.44587 | val_1_rmse: 0.45389 |  0:00:24s
epoch 25 | loss: 0.16103 | val_0_rmse: 0.43115 | val_1_rmse: 0.44241 |  0:00:25s
epoch 26 | loss: 0.1706  | val_0_rmse: 0.43632 | val_1_rmse: 0.4474  |  0:00:26s
epoch 27 | loss: 0.16732 | val_0_rmse: 0.4206  | val_1_rmse: 0.43376 |  0:00:26s
epoch 28 | loss: 0.16886 | val_0_rmse: 0.4164  | val_1_rmse: 0.42767 |  0:00:27s
epoch 29 | loss: 0.16266 | val_0_rmse: 0.40215 | val_1_rmse: 0.41828 |  0:00:28s
epoch 30 | loss: 0.16819 | val_0_rmse: 0.40273 | val_1_rmse: 0.42581 |  0:00:29s
epoch 31 | loss: 0.16014 | val_0_rmse: 0.43007 | val_1_rmse: 0.44392 |  0:00:30s
epoch 32 | loss: 0.15903 | val_0_rmse: 0.39922 | val_1_rmse: 0.41437 |  0:00:31s
epoch 33 | loss: 0.1607  | val_0_rmse: 0.38695 | val_1_rmse: 0.40467 |  0:00:32s
epoch 34 | loss: 0.15767 | val_0_rmse: 0.39455 | val_1_rmse: 0.4085  |  0:00:33s
epoch 35 | loss: 0.15027 | val_0_rmse: 0.38851 | val_1_rmse: 0.40754 |  0:00:34s
epoch 36 | loss: 0.15503 | val_0_rmse: 0.3724  | val_1_rmse: 0.39197 |  0:00:35s
epoch 37 | loss: 0.14858 | val_0_rmse: 0.37641 | val_1_rmse: 0.39657 |  0:00:36s
epoch 38 | loss: 0.14978 | val_0_rmse: 0.39032 | val_1_rmse: 0.40627 |  0:00:37s
epoch 39 | loss: 0.14937 | val_0_rmse: 0.3768  | val_1_rmse: 0.39712 |  0:00:38s
epoch 40 | loss: 0.15096 | val_0_rmse: 0.36842 | val_1_rmse: 0.3962  |  0:00:39s
epoch 41 | loss: 0.14328 | val_0_rmse: 0.36567 | val_1_rmse: 0.39365 |  0:00:40s
epoch 42 | loss: 0.14592 | val_0_rmse: 0.36613 | val_1_rmse: 0.39197 |  0:00:41s
epoch 43 | loss: 0.14085 | val_0_rmse: 0.35354 | val_1_rmse: 0.38481 |  0:00:42s
epoch 44 | loss: 0.14367 | val_0_rmse: 0.35444 | val_1_rmse: 0.38626 |  0:00:43s
epoch 45 | loss: 0.14226 | val_0_rmse: 0.35077 | val_1_rmse: 0.3815  |  0:00:44s
epoch 46 | loss: 0.14721 | val_0_rmse: 0.35561 | val_1_rmse: 0.3917  |  0:00:45s
epoch 47 | loss: 0.14411 | val_0_rmse: 0.35407 | val_1_rmse: 0.3889  |  0:00:46s
epoch 48 | loss: 0.14435 | val_0_rmse: 0.36599 | val_1_rmse: 0.39529 |  0:00:47s
epoch 49 | loss: 0.14042 | val_0_rmse: 0.35468 | val_1_rmse: 0.39283 |  0:00:48s
epoch 50 | loss: 0.14722 | val_0_rmse: 0.36492 | val_1_rmse: 0.39489 |  0:00:49s
epoch 51 | loss: 0.14629 | val_0_rmse: 0.40142 | val_1_rmse: 0.43808 |  0:00:49s
epoch 52 | loss: 0.1486  | val_0_rmse: 0.35218 | val_1_rmse: 0.38944 |  0:00:50s
epoch 53 | loss: 0.13824 | val_0_rmse: 0.36179 | val_1_rmse: 0.39972 |  0:00:51s
epoch 54 | loss: 0.14013 | val_0_rmse: 0.35016 | val_1_rmse: 0.38595 |  0:00:52s
epoch 55 | loss: 0.14163 | val_0_rmse: 0.36877 | val_1_rmse: 0.40652 |  0:00:53s
epoch 56 | loss: 0.1365  | val_0_rmse: 0.34817 | val_1_rmse: 0.38861 |  0:00:54s
epoch 57 | loss: 0.13474 | val_0_rmse: 0.35449 | val_1_rmse: 0.39748 |  0:00:55s
epoch 58 | loss: 0.13435 | val_0_rmse: 0.34763 | val_1_rmse: 0.39127 |  0:00:56s
epoch 59 | loss: 0.13072 | val_0_rmse: 0.34611 | val_1_rmse: 0.39093 |  0:00:57s
epoch 60 | loss: 0.13224 | val_0_rmse: 0.35372 | val_1_rmse: 0.40351 |  0:00:58s
epoch 61 | loss: 0.13426 | val_0_rmse: 0.36304 | val_1_rmse: 0.40356 |  0:00:59s
epoch 62 | loss: 0.14316 | val_0_rmse: 0.36702 | val_1_rmse: 0.40684 |  0:01:00s
epoch 63 | loss: 0.13136 | val_0_rmse: 0.33944 | val_1_rmse: 0.37982 |  0:01:01s
epoch 64 | loss: 0.13137 | val_0_rmse: 0.34283 | val_1_rmse: 0.38605 |  0:01:02s
epoch 65 | loss: 0.13208 | val_0_rmse: 0.34603 | val_1_rmse: 0.38177 |  0:01:03s
epoch 66 | loss: 0.1318  | val_0_rmse: 0.36543 | val_1_rmse: 0.40104 |  0:01:04s
epoch 67 | loss: 0.13393 | val_0_rmse: 0.35712 | val_1_rmse: 0.3957  |  0:01:05s
epoch 68 | loss: 0.1327  | val_0_rmse: 0.35804 | val_1_rmse: 0.39636 |  0:01:06s
epoch 69 | loss: 0.15287 | val_0_rmse: 0.37433 | val_1_rmse: 0.41764 |  0:01:07s
epoch 70 | loss: 0.14348 | val_0_rmse: 0.36512 | val_1_rmse: 0.402   |  0:01:08s
epoch 71 | loss: 0.14239 | val_0_rmse: 0.36673 | val_1_rmse: 0.4066  |  0:01:09s
epoch 72 | loss: 0.13946 | val_0_rmse: 0.3879  | val_1_rmse: 0.43754 |  0:01:09s
epoch 73 | loss: 0.13706 | val_0_rmse: 0.35364 | val_1_rmse: 0.39583 |  0:01:10s
epoch 74 | loss: 0.13502 | val_0_rmse: 0.36684 | val_1_rmse: 0.40662 |  0:01:11s
epoch 75 | loss: 0.13761 | val_0_rmse: 0.35519 | val_1_rmse: 0.39679 |  0:01:12s
epoch 76 | loss: 0.14453 | val_0_rmse: 0.36398 | val_1_rmse: 0.41116 |  0:01:13s
epoch 77 | loss: 0.13658 | val_0_rmse: 0.35354 | val_1_rmse: 0.39878 |  0:01:14s
epoch 78 | loss: 0.13677 | val_0_rmse: 0.3751  | val_1_rmse: 0.41466 |  0:01:15s
epoch 79 | loss: 0.13757 | val_0_rmse: 0.36085 | val_1_rmse: 0.40587 |  0:01:16s
epoch 80 | loss: 0.14259 | val_0_rmse: 0.37681 | val_1_rmse: 0.41453 |  0:01:17s
epoch 81 | loss: 0.13983 | val_0_rmse: 0.38347 | val_1_rmse: 0.41161 |  0:01:18s
epoch 82 | loss: 0.14222 | val_0_rmse: 0.39526 | val_1_rmse: 0.43214 |  0:01:19s
epoch 83 | loss: 0.16071 | val_0_rmse: 0.37462 | val_1_rmse: 0.41264 |  0:01:20s
epoch 84 | loss: 0.15762 | val_0_rmse: 0.40972 | val_1_rmse: 0.45704 |  0:01:21s
epoch 85 | loss: 0.14298 | val_0_rmse: 0.38129 | val_1_rmse: 0.4248  |  0:01:22s
epoch 86 | loss: 0.14178 | val_0_rmse: 0.36611 | val_1_rmse: 0.4101  |  0:01:23s
epoch 87 | loss: 0.13927 | val_0_rmse: 0.3681  | val_1_rmse: 0.40958 |  0:01:24s
epoch 88 | loss: 0.13003 | val_0_rmse: 0.3504  | val_1_rmse: 0.39183 |  0:01:25s
epoch 89 | loss: 0.13246 | val_0_rmse: 0.36681 | val_1_rmse: 0.40272 |  0:01:26s
epoch 90 | loss: 0.12837 | val_0_rmse: 0.35108 | val_1_rmse: 0.39332 |  0:01:27s
epoch 91 | loss: 0.12669 | val_0_rmse: 0.34303 | val_1_rmse: 0.38777 |  0:01:28s
epoch 92 | loss: 0.12793 | val_0_rmse: 0.33839 | val_1_rmse: 0.37759 |  0:01:29s
epoch 93 | loss: 0.12429 | val_0_rmse: 0.34686 | val_1_rmse: 0.3903  |  0:01:29s
epoch 94 | loss: 0.12718 | val_0_rmse: 0.33841 | val_1_rmse: 0.3814  |  0:01:30s
epoch 95 | loss: 0.12738 | val_0_rmse: 0.35097 | val_1_rmse: 0.39504 |  0:01:31s
epoch 96 | loss: 0.12581 | val_0_rmse: 0.3602  | val_1_rmse: 0.40406 |  0:01:32s
epoch 97 | loss: 0.12749 | val_0_rmse: 0.33665 | val_1_rmse: 0.38017 |  0:01:33s
epoch 98 | loss: 0.12703 | val_0_rmse: 0.34233 | val_1_rmse: 0.38307 |  0:01:34s
epoch 99 | loss: 0.12175 | val_0_rmse: 0.33304 | val_1_rmse: 0.37561 |  0:01:35s
epoch 100| loss: 0.12401 | val_0_rmse: 0.33438 | val_1_rmse: 0.37875 |  0:01:36s
epoch 101| loss: 0.12243 | val_0_rmse: 0.3529  | val_1_rmse: 0.40838 |  0:01:37s
epoch 102| loss: 0.12049 | val_0_rmse: 0.33887 | val_1_rmse: 0.38353 |  0:01:38s
epoch 103| loss: 0.12235 | val_0_rmse: 0.34363 | val_1_rmse: 0.38718 |  0:01:39s
epoch 104| loss: 0.12414 | val_0_rmse: 0.34993 | val_1_rmse: 0.38516 |  0:01:40s
epoch 105| loss: 0.12703 | val_0_rmse: 0.34017 | val_1_rmse: 0.38788 |  0:01:41s
epoch 106| loss: 0.11994 | val_0_rmse: 0.32892 | val_1_rmse: 0.37468 |  0:01:42s
epoch 107| loss: 0.11686 | val_0_rmse: 0.33976 | val_1_rmse: 0.38619 |  0:01:43s
epoch 108| loss: 0.1218  | val_0_rmse: 0.33056 | val_1_rmse: 0.3828  |  0:01:44s
epoch 109| loss: 0.1222  | val_0_rmse: 0.34119 | val_1_rmse: 0.39077 |  0:01:45s
epoch 110| loss: 0.12023 | val_0_rmse: 0.33047 | val_1_rmse: 0.37854 |  0:01:46s
epoch 111| loss: 0.12259 | val_0_rmse: 0.33805 | val_1_rmse: 0.38809 |  0:01:47s
epoch 112| loss: 0.11916 | val_0_rmse: 0.31789 | val_1_rmse: 0.36041 |  0:01:48s
epoch 113| loss: 0.11654 | val_0_rmse: 0.33418 | val_1_rmse: 0.38927 |  0:01:49s
epoch 114| loss: 0.11762 | val_0_rmse: 0.32595 | val_1_rmse: 0.37571 |  0:01:50s
epoch 115| loss: 0.12104 | val_0_rmse: 0.33297 | val_1_rmse: 0.38089 |  0:01:51s
epoch 116| loss: 0.12133 | val_0_rmse: 0.32589 | val_1_rmse: 0.37814 |  0:01:52s
epoch 117| loss: 0.11448 | val_0_rmse: 0.33487 | val_1_rmse: 0.38194 |  0:01:53s
epoch 118| loss: 0.11652 | val_0_rmse: 0.33174 | val_1_rmse: 0.37682 |  0:01:53s
epoch 119| loss: 0.1177  | val_0_rmse: 0.3241  | val_1_rmse: 0.37636 |  0:01:54s
epoch 120| loss: 0.12025 | val_0_rmse: 0.3421  | val_1_rmse: 0.39111 |  0:01:55s
epoch 121| loss: 0.11991 | val_0_rmse: 0.32814 | val_1_rmse: 0.38179 |  0:01:56s
epoch 122| loss: 0.11888 | val_0_rmse: 0.34151 | val_1_rmse: 0.39658 |  0:01:57s
epoch 123| loss: 0.11773 | val_0_rmse: 0.34165 | val_1_rmse: 0.38581 |  0:01:58s
epoch 124| loss: 0.12393 | val_0_rmse: 0.34073 | val_1_rmse: 0.38578 |  0:01:59s
epoch 125| loss: 0.11754 | val_0_rmse: 0.3501  | val_1_rmse: 0.40698 |  0:02:00s
epoch 126| loss: 0.12766 | val_0_rmse: 0.4084  | val_1_rmse: 0.46639 |  0:02:01s
epoch 127| loss: 0.13083 | val_0_rmse: 0.38281 | val_1_rmse: 0.44064 |  0:02:02s
epoch 128| loss: 0.12444 | val_0_rmse: 0.36482 | val_1_rmse: 0.41544 |  0:02:03s
epoch 129| loss: 0.12046 | val_0_rmse: 0.33413 | val_1_rmse: 0.38474 |  0:02:04s
epoch 130| loss: 0.12089 | val_0_rmse: 0.33625 | val_1_rmse: 0.38637 |  0:02:05s
epoch 131| loss: 0.12148 | val_0_rmse: 0.36254 | val_1_rmse: 0.4089  |  0:02:06s
epoch 132| loss: 0.11946 | val_0_rmse: 0.3431  | val_1_rmse: 0.40357 |  0:02:07s
epoch 133| loss: 0.11406 | val_0_rmse: 0.32662 | val_1_rmse: 0.37634 |  0:02:08s
epoch 134| loss: 0.11953 | val_0_rmse: 0.33363 | val_1_rmse: 0.38946 |  0:02:09s
epoch 135| loss: 0.12156 | val_0_rmse: 0.32127 | val_1_rmse: 0.37564 |  0:02:10s
epoch 136| loss: 0.1192  | val_0_rmse: 0.32053 | val_1_rmse: 0.37687 |  0:02:11s
epoch 137| loss: 0.11659 | val_0_rmse: 0.32974 | val_1_rmse: 0.38785 |  0:02:12s
epoch 138| loss: 0.11552 | val_0_rmse: 0.32067 | val_1_rmse: 0.37192 |  0:02:13s
epoch 139| loss: 0.11235 | val_0_rmse: 0.32385 | val_1_rmse: 0.37936 |  0:02:13s
epoch 140| loss: 0.11005 | val_0_rmse: 0.32535 | val_1_rmse: 0.38057 |  0:02:15s
epoch 141| loss: 0.11631 | val_0_rmse: 0.32293 | val_1_rmse: 0.37964 |  0:02:15s
epoch 142| loss: 0.11165 | val_0_rmse: 0.32278 | val_1_rmse: 0.37262 |  0:02:16s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.36041
Best weights from best epoch are automatically used!
ended training at: 04:58:37
Feature importance:
Mean squared error is of 0.07174104927618072
Mean absolute error:0.16573938237435515
MAPE:0.185944263307074
R2 score:0.8505430715414444
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:58:38
epoch 0  | loss: 1.31363 | val_0_rmse: 0.84349 | val_1_rmse: 0.8538  |  0:00:00s
epoch 1  | loss: 0.54748 | val_0_rmse: 0.96349 | val_1_rmse: 0.95853 |  0:00:01s
epoch 2  | loss: 0.40442 | val_0_rmse: 0.71058 | val_1_rmse: 0.71446 |  0:00:02s
epoch 3  | loss: 0.33742 | val_0_rmse: 0.80435 | val_1_rmse: 0.82588 |  0:00:03s
epoch 4  | loss: 0.28531 | val_0_rmse: 0.7712  | val_1_rmse: 0.77872 |  0:00:04s
epoch 5  | loss: 0.25677 | val_0_rmse: 0.60882 | val_1_rmse: 0.60983 |  0:00:05s
epoch 6  | loss: 0.23828 | val_0_rmse: 0.54906 | val_1_rmse: 0.54845 |  0:00:06s
epoch 7  | loss: 0.24107 | val_0_rmse: 0.57307 | val_1_rmse: 0.57341 |  0:00:07s
epoch 8  | loss: 0.24406 | val_0_rmse: 0.64165 | val_1_rmse: 0.64948 |  0:00:08s
epoch 9  | loss: 0.22061 | val_0_rmse: 0.58224 | val_1_rmse: 0.57951 |  0:00:09s
epoch 10 | loss: 0.25853 | val_0_rmse: 0.59918 | val_1_rmse: 0.60079 |  0:00:10s
epoch 11 | loss: 0.22826 | val_0_rmse: 0.52838 | val_1_rmse: 0.53055 |  0:00:11s
epoch 12 | loss: 0.21191 | val_0_rmse: 0.5194  | val_1_rmse: 0.51461 |  0:00:12s
epoch 13 | loss: 0.20434 | val_0_rmse: 0.5069  | val_1_rmse: 0.50496 |  0:00:13s
epoch 14 | loss: 0.20172 | val_0_rmse: 0.48445 | val_1_rmse: 0.4864  |  0:00:14s
epoch 15 | loss: 0.19518 | val_0_rmse: 0.56326 | val_1_rmse: 0.56275 |  0:00:15s
epoch 16 | loss: 0.19528 | val_0_rmse: 0.49974 | val_1_rmse: 0.49294 |  0:00:16s
epoch 17 | loss: 0.18665 | val_0_rmse: 0.47949 | val_1_rmse: 0.48527 |  0:00:17s
epoch 18 | loss: 0.18038 | val_0_rmse: 0.44421 | val_1_rmse: 0.44629 |  0:00:18s
epoch 19 | loss: 0.18427 | val_0_rmse: 0.44055 | val_1_rmse: 0.4512  |  0:00:19s
epoch 20 | loss: 0.18479 | val_0_rmse: 0.42696 | val_1_rmse: 0.4397  |  0:00:20s
epoch 21 | loss: 0.18284 | val_0_rmse: 0.46776 | val_1_rmse: 0.4794  |  0:00:21s
epoch 22 | loss: 0.18118 | val_0_rmse: 0.49339 | val_1_rmse: 0.50261 |  0:00:22s
epoch 23 | loss: 0.17426 | val_0_rmse: 0.42574 | val_1_rmse: 0.43332 |  0:00:23s
epoch 24 | loss: 0.17424 | val_0_rmse: 0.43958 | val_1_rmse: 0.44552 |  0:00:24s
epoch 25 | loss: 0.17461 | val_0_rmse: 0.43374 | val_1_rmse: 0.44278 |  0:00:24s
epoch 26 | loss: 0.1671  | val_0_rmse: 0.40726 | val_1_rmse: 0.41588 |  0:00:25s
epoch 27 | loss: 0.16508 | val_0_rmse: 0.42598 | val_1_rmse: 0.43513 |  0:00:26s
epoch 28 | loss: 0.16598 | val_0_rmse: 0.39685 | val_1_rmse: 0.41068 |  0:00:27s
epoch 29 | loss: 0.16434 | val_0_rmse: 0.38969 | val_1_rmse: 0.40275 |  0:00:28s
epoch 30 | loss: 0.16124 | val_0_rmse: 0.39398 | val_1_rmse: 0.40906 |  0:00:29s
epoch 31 | loss: 0.15785 | val_0_rmse: 0.39192 | val_1_rmse: 0.40502 |  0:00:30s
epoch 32 | loss: 0.16674 | val_0_rmse: 0.44732 | val_1_rmse: 0.4596  |  0:00:31s
epoch 33 | loss: 0.16232 | val_0_rmse: 0.38682 | val_1_rmse: 0.40062 |  0:00:32s
epoch 34 | loss: 0.1594  | val_0_rmse: 0.38807 | val_1_rmse: 0.40237 |  0:00:33s
epoch 35 | loss: 0.1564  | val_0_rmse: 0.45471 | val_1_rmse: 0.46697 |  0:00:34s
epoch 36 | loss: 0.15792 | val_0_rmse: 0.37632 | val_1_rmse: 0.3918  |  0:00:35s
epoch 37 | loss: 0.15316 | val_0_rmse: 0.42132 | val_1_rmse: 0.43203 |  0:00:36s
epoch 38 | loss: 0.16527 | val_0_rmse: 0.4114  | val_1_rmse: 0.42662 |  0:00:37s
epoch 39 | loss: 0.15984 | val_0_rmse: 0.39784 | val_1_rmse: 0.41304 |  0:00:38s
epoch 40 | loss: 0.15516 | val_0_rmse: 0.38729 | val_1_rmse: 0.40328 |  0:00:39s
epoch 41 | loss: 0.14685 | val_0_rmse: 0.39323 | val_1_rmse: 0.41116 |  0:00:40s
epoch 42 | loss: 0.1561  | val_0_rmse: 0.37192 | val_1_rmse: 0.39324 |  0:00:41s
epoch 43 | loss: 0.15144 | val_0_rmse: 0.36165 | val_1_rmse: 0.38553 |  0:00:42s
epoch 44 | loss: 0.1449  | val_0_rmse: 0.36727 | val_1_rmse: 0.38808 |  0:00:43s
epoch 45 | loss: 0.14607 | val_0_rmse: 0.38245 | val_1_rmse: 0.40476 |  0:00:44s
epoch 46 | loss: 0.147   | val_0_rmse: 0.38663 | val_1_rmse: 0.40598 |  0:00:45s
epoch 47 | loss: 0.14866 | val_0_rmse: 0.38178 | val_1_rmse: 0.40094 |  0:00:45s
epoch 48 | loss: 0.14638 | val_0_rmse: 0.37058 | val_1_rmse: 0.39193 |  0:00:46s
epoch 49 | loss: 0.14354 | val_0_rmse: 0.35754 | val_1_rmse: 0.38319 |  0:00:47s
epoch 50 | loss: 0.14115 | val_0_rmse: 0.36079 | val_1_rmse: 0.38565 |  0:00:48s
epoch 51 | loss: 0.13747 | val_0_rmse: 0.34762 | val_1_rmse: 0.37275 |  0:00:49s
epoch 52 | loss: 0.1407  | val_0_rmse: 0.39147 | val_1_rmse: 0.41658 |  0:00:50s
epoch 53 | loss: 0.14329 | val_0_rmse: 0.35292 | val_1_rmse: 0.37333 |  0:00:51s
epoch 54 | loss: 0.13678 | val_0_rmse: 0.34792 | val_1_rmse: 0.37136 |  0:00:52s
epoch 55 | loss: 0.14626 | val_0_rmse: 0.35249 | val_1_rmse: 0.37268 |  0:00:53s
epoch 56 | loss: 0.13992 | val_0_rmse: 0.38522 | val_1_rmse: 0.40782 |  0:00:54s
epoch 57 | loss: 0.14845 | val_0_rmse: 0.35397 | val_1_rmse: 0.38033 |  0:00:55s
epoch 58 | loss: 0.14165 | val_0_rmse: 0.36786 | val_1_rmse: 0.39305 |  0:00:56s
epoch 59 | loss: 0.14732 | val_0_rmse: 0.39704 | val_1_rmse: 0.4168  |  0:00:57s
epoch 60 | loss: 0.14713 | val_0_rmse: 0.4056  | val_1_rmse: 0.42522 |  0:00:58s
epoch 61 | loss: 0.14214 | val_0_rmse: 0.36242 | val_1_rmse: 0.38673 |  0:00:59s
epoch 62 | loss: 0.13865 | val_0_rmse: 0.36546 | val_1_rmse: 0.39273 |  0:01:00s
epoch 63 | loss: 0.13992 | val_0_rmse: 0.35974 | val_1_rmse: 0.38738 |  0:01:01s
epoch 64 | loss: 0.14493 | val_0_rmse: 0.36853 | val_1_rmse: 0.39111 |  0:01:02s
epoch 65 | loss: 0.14364 | val_0_rmse: 0.38067 | val_1_rmse: 0.40166 |  0:01:03s
epoch 66 | loss: 0.13878 | val_0_rmse: 0.35839 | val_1_rmse: 0.38637 |  0:01:04s
epoch 67 | loss: 0.13814 | val_0_rmse: 0.35389 | val_1_rmse: 0.38239 |  0:01:05s
epoch 68 | loss: 0.14592 | val_0_rmse: 0.36706 | val_1_rmse: 0.3923  |  0:01:06s
epoch 69 | loss: 0.14018 | val_0_rmse: 0.34121 | val_1_rmse: 0.37072 |  0:01:07s
epoch 70 | loss: 0.13679 | val_0_rmse: 0.40097 | val_1_rmse: 0.43307 |  0:01:08s
epoch 71 | loss: 0.14311 | val_0_rmse: 0.3467  | val_1_rmse: 0.3741  |  0:01:08s
epoch 72 | loss: 0.13863 | val_0_rmse: 0.3443  | val_1_rmse: 0.37415 |  0:01:09s
epoch 73 | loss: 0.13656 | val_0_rmse: 0.35382 | val_1_rmse: 0.38589 |  0:01:10s
epoch 74 | loss: 0.1306  | val_0_rmse: 0.34748 | val_1_rmse: 0.3785  |  0:01:11s
epoch 75 | loss: 0.13002 | val_0_rmse: 0.34643 | val_1_rmse: 0.3746  |  0:01:12s
epoch 76 | loss: 0.13521 | val_0_rmse: 0.33814 | val_1_rmse: 0.36694 |  0:01:13s
epoch 77 | loss: 0.13024 | val_0_rmse: 0.35926 | val_1_rmse: 0.38798 |  0:01:14s
epoch 78 | loss: 0.13285 | val_0_rmse: 0.34414 | val_1_rmse: 0.37889 |  0:01:15s
epoch 79 | loss: 0.1318  | val_0_rmse: 0.34163 | val_1_rmse: 0.37629 |  0:01:16s
epoch 80 | loss: 0.1356  | val_0_rmse: 0.33889 | val_1_rmse: 0.37476 |  0:01:17s
epoch 81 | loss: 0.13035 | val_0_rmse: 0.35658 | val_1_rmse: 0.38751 |  0:01:18s
epoch 82 | loss: 0.13479 | val_0_rmse: 0.36184 | val_1_rmse: 0.38891 |  0:01:19s
epoch 83 | loss: 0.13293 | val_0_rmse: 0.34072 | val_1_rmse: 0.37372 |  0:01:20s
epoch 84 | loss: 0.13183 | val_0_rmse: 0.35048 | val_1_rmse: 0.38786 |  0:01:21s
epoch 85 | loss: 0.12787 | val_0_rmse: 0.35968 | val_1_rmse: 0.38398 |  0:01:22s
epoch 86 | loss: 0.1285  | val_0_rmse: 0.35296 | val_1_rmse: 0.38311 |  0:01:23s
epoch 87 | loss: 0.12831 | val_0_rmse: 0.33492 | val_1_rmse: 0.37194 |  0:01:24s
epoch 88 | loss: 0.12375 | val_0_rmse: 0.32578 | val_1_rmse: 0.36381 |  0:01:25s
epoch 89 | loss: 0.12641 | val_0_rmse: 0.33122 | val_1_rmse: 0.36631 |  0:01:26s
epoch 90 | loss: 0.12398 | val_0_rmse: 0.32532 | val_1_rmse: 0.36087 |  0:01:27s
epoch 91 | loss: 0.12636 | val_0_rmse: 0.34105 | val_1_rmse: 0.37464 |  0:01:28s
epoch 92 | loss: 0.12629 | val_0_rmse: 0.32941 | val_1_rmse: 0.36546 |  0:01:28s
epoch 93 | loss: 0.14032 | val_0_rmse: 0.36513 | val_1_rmse: 0.40184 |  0:01:29s
epoch 94 | loss: 0.14929 | val_0_rmse: 0.37224 | val_1_rmse: 0.4067  |  0:01:30s
epoch 95 | loss: 0.14724 | val_0_rmse: 0.39493 | val_1_rmse: 0.42499 |  0:01:31s
epoch 96 | loss: 0.17895 | val_0_rmse: 0.37638 | val_1_rmse: 0.40311 |  0:01:32s
epoch 97 | loss: 0.1711  | val_0_rmse: 0.40771 | val_1_rmse: 0.42941 |  0:01:33s
epoch 98 | loss: 0.17983 | val_0_rmse: 0.43657 | val_1_rmse: 0.46776 |  0:01:34s
epoch 99 | loss: 0.16946 | val_0_rmse: 0.39035 | val_1_rmse: 0.40798 |  0:01:35s
epoch 100| loss: 0.15754 | val_0_rmse: 0.404   | val_1_rmse: 0.4257  |  0:01:36s
epoch 101| loss: 0.15664 | val_0_rmse: 0.35936 | val_1_rmse: 0.38628 |  0:01:37s
epoch 102| loss: 0.14435 | val_0_rmse: 0.39144 | val_1_rmse: 0.41342 |  0:01:38s
epoch 103| loss: 0.14387 | val_0_rmse: 0.36513 | val_1_rmse: 0.3949  |  0:01:39s
epoch 104| loss: 0.14114 | val_0_rmse: 0.36107 | val_1_rmse: 0.38901 |  0:01:40s
epoch 105| loss: 0.13957 | val_0_rmse: 0.36895 | val_1_rmse: 0.39654 |  0:01:41s
epoch 106| loss: 0.13997 | val_0_rmse: 0.35918 | val_1_rmse: 0.38555 |  0:01:42s
epoch 107| loss: 0.13661 | val_0_rmse: 0.41389 | val_1_rmse: 0.4329  |  0:01:43s
epoch 108| loss: 0.13859 | val_0_rmse: 0.3557  | val_1_rmse: 0.3799  |  0:01:44s
epoch 109| loss: 0.15549 | val_0_rmse: 0.408   | val_1_rmse: 0.4431  |  0:01:45s
epoch 110| loss: 0.15034 | val_0_rmse: 0.35769 | val_1_rmse: 0.38827 |  0:01:46s
epoch 111| loss: 0.18991 | val_0_rmse: 0.43651 | val_1_rmse: 0.45731 |  0:01:47s
epoch 112| loss: 0.19531 | val_0_rmse: 0.44675 | val_1_rmse: 0.4549  |  0:01:48s
epoch 113| loss: 0.19208 | val_0_rmse: 0.41935 | val_1_rmse: 0.4327  |  0:01:48s
epoch 114| loss: 0.18251 | val_0_rmse: 0.45622 | val_1_rmse: 0.47442 |  0:01:49s
epoch 115| loss: 0.18773 | val_0_rmse: 0.42517 | val_1_rmse: 0.44461 |  0:01:50s
epoch 116| loss: 0.169   | val_0_rmse: 0.38552 | val_1_rmse: 0.40482 |  0:01:51s
epoch 117| loss: 0.16116 | val_0_rmse: 0.37473 | val_1_rmse: 0.39775 |  0:01:52s
epoch 118| loss: 0.18403 | val_0_rmse: 0.52606 | val_1_rmse: 0.53871 |  0:01:53s
epoch 119| loss: 0.18966 | val_0_rmse: 0.40736 | val_1_rmse: 0.42296 |  0:01:54s
epoch 120| loss: 0.17053 | val_0_rmse: 0.39451 | val_1_rmse: 0.41536 |  0:01:55s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.36087
Best weights from best epoch are automatically used!
ended training at: 05:00:34
Feature importance:
Mean squared error is of 0.05913214720670859
Mean absolute error:0.15495338598876576
MAPE:0.1923656043448058
R2 score:0.8693590556660779
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:00:35
epoch 0  | loss: 1.36442 | val_0_rmse: 0.91884 | val_1_rmse: 0.91131 |  0:00:00s
epoch 1  | loss: 0.63109 | val_0_rmse: 0.75437 | val_1_rmse: 0.75492 |  0:00:01s
epoch 2  | loss: 0.47955 | val_0_rmse: 0.66468 | val_1_rmse: 0.66331 |  0:00:02s
epoch 3  | loss: 0.41297 | val_0_rmse: 0.76375 | val_1_rmse: 0.76051 |  0:00:03s
epoch 4  | loss: 0.36137 | val_0_rmse: 0.6886  | val_1_rmse: 0.6839  |  0:00:04s
epoch 5  | loss: 0.32966 | val_0_rmse: 0.60596 | val_1_rmse: 0.6055  |  0:00:05s
epoch 6  | loss: 0.30043 | val_0_rmse: 0.69539 | val_1_rmse: 0.69649 |  0:00:06s
epoch 7  | loss: 0.29831 | val_0_rmse: 0.60222 | val_1_rmse: 0.60434 |  0:00:07s
epoch 8  | loss: 0.28309 | val_0_rmse: 0.58282 | val_1_rmse: 0.57718 |  0:00:08s
epoch 9  | loss: 0.27496 | val_0_rmse: 0.59166 | val_1_rmse: 0.5833  |  0:00:09s
epoch 10 | loss: 0.26664 | val_0_rmse: 0.61342 | val_1_rmse: 0.60331 |  0:00:10s
epoch 11 | loss: 0.25262 | val_0_rmse: 0.55822 | val_1_rmse: 0.54852 |  0:00:11s
epoch 12 | loss: 0.24073 | val_0_rmse: 0.57797 | val_1_rmse: 0.56876 |  0:00:12s
epoch 13 | loss: 0.23966 | val_0_rmse: 0.56894 | val_1_rmse: 0.56809 |  0:00:13s
epoch 14 | loss: 0.25341 | val_0_rmse: 0.63497 | val_1_rmse: 0.62496 |  0:00:14s
epoch 15 | loss: 0.24322 | val_0_rmse: 0.55415 | val_1_rmse: 0.53943 |  0:00:15s
epoch 16 | loss: 0.23747 | val_0_rmse: 0.5831  | val_1_rmse: 0.56976 |  0:00:16s
epoch 17 | loss: 0.23408 | val_0_rmse: 0.50521 | val_1_rmse: 0.49934 |  0:00:17s
epoch 18 | loss: 0.23979 | val_0_rmse: 0.52884 | val_1_rmse: 0.52545 |  0:00:18s
epoch 19 | loss: 0.23149 | val_0_rmse: 0.50505 | val_1_rmse: 0.49411 |  0:00:19s
epoch 20 | loss: 0.21308 | val_0_rmse: 0.52702 | val_1_rmse: 0.51636 |  0:00:20s
epoch 21 | loss: 0.20881 | val_0_rmse: 0.50909 | val_1_rmse: 0.49924 |  0:00:21s
epoch 22 | loss: 0.19684 | val_0_rmse: 0.55256 | val_1_rmse: 0.54232 |  0:00:21s
epoch 23 | loss: 0.18731 | val_0_rmse: 0.46153 | val_1_rmse: 0.45573 |  0:00:23s
epoch 24 | loss: 0.1974  | val_0_rmse: 0.49888 | val_1_rmse: 0.49661 |  0:00:23s
epoch 25 | loss: 0.20226 | val_0_rmse: 0.46338 | val_1_rmse: 0.47034 |  0:00:24s
epoch 26 | loss: 0.18957 | val_0_rmse: 0.42837 | val_1_rmse: 0.41912 |  0:00:25s
epoch 27 | loss: 0.18323 | val_0_rmse: 0.44691 | val_1_rmse: 0.43255 |  0:00:26s
epoch 28 | loss: 0.20347 | val_0_rmse: 0.55711 | val_1_rmse: 0.55802 |  0:00:27s
epoch 29 | loss: 0.24786 | val_0_rmse: 0.4556  | val_1_rmse: 0.46677 |  0:00:28s
epoch 30 | loss: 0.21146 | val_0_rmse: 0.43345 | val_1_rmse: 0.43112 |  0:00:29s
epoch 31 | loss: 0.19348 | val_0_rmse: 0.42518 | val_1_rmse: 0.42675 |  0:00:30s
epoch 32 | loss: 0.20496 | val_0_rmse: 0.49895 | val_1_rmse: 0.48668 |  0:00:31s
epoch 33 | loss: 0.22263 | val_0_rmse: 0.45059 | val_1_rmse: 0.44358 |  0:00:32s
epoch 34 | loss: 0.2986  | val_0_rmse: 0.51336 | val_1_rmse: 0.51686 |  0:00:33s
epoch 35 | loss: 0.28584 | val_0_rmse: 0.55153 | val_1_rmse: 0.54698 |  0:00:34s
epoch 36 | loss: 0.22963 | val_0_rmse: 0.47081 | val_1_rmse: 0.46068 |  0:00:35s
epoch 37 | loss: 0.22266 | val_0_rmse: 0.47608 | val_1_rmse: 0.48701 |  0:00:36s
epoch 38 | loss: 0.28442 | val_0_rmse: 0.46256 | val_1_rmse: 0.46615 |  0:00:37s
epoch 39 | loss: 0.22634 | val_0_rmse: 0.46003 | val_1_rmse: 0.45462 |  0:00:38s
epoch 40 | loss: 0.20725 | val_0_rmse: 0.43008 | val_1_rmse: 0.42774 |  0:00:39s
epoch 41 | loss: 0.19473 | val_0_rmse: 0.445   | val_1_rmse: 0.4448  |  0:00:40s
epoch 42 | loss: 0.19407 | val_0_rmse: 0.42225 | val_1_rmse: 0.41796 |  0:00:41s
epoch 43 | loss: 0.18891 | val_0_rmse: 0.43949 | val_1_rmse: 0.43258 |  0:00:42s
epoch 44 | loss: 0.18589 | val_0_rmse: 0.40216 | val_1_rmse: 0.40157 |  0:00:43s
epoch 45 | loss: 0.18239 | val_0_rmse: 0.39445 | val_1_rmse: 0.392   |  0:00:44s
epoch 46 | loss: 0.19271 | val_0_rmse: 0.58059 | val_1_rmse: 0.57245 |  0:00:45s
epoch 47 | loss: 0.20344 | val_0_rmse: 0.41622 | val_1_rmse: 0.41178 |  0:00:45s
epoch 48 | loss: 0.19058 | val_0_rmse: 0.4207  | val_1_rmse: 0.41428 |  0:00:46s
epoch 49 | loss: 0.17981 | val_0_rmse: 0.42189 | val_1_rmse: 0.41814 |  0:00:47s
epoch 50 | loss: 0.18145 | val_0_rmse: 0.40894 | val_1_rmse: 0.40249 |  0:00:48s
epoch 51 | loss: 0.17455 | val_0_rmse: 0.4054  | val_1_rmse: 0.40436 |  0:00:49s
epoch 52 | loss: 0.17007 | val_0_rmse: 0.38541 | val_1_rmse: 0.37813 |  0:00:50s
epoch 53 | loss: 0.16899 | val_0_rmse: 0.40471 | val_1_rmse: 0.40245 |  0:00:51s
epoch 54 | loss: 0.17187 | val_0_rmse: 0.38136 | val_1_rmse: 0.37484 |  0:00:52s
epoch 55 | loss: 0.17108 | val_0_rmse: 0.38168 | val_1_rmse: 0.37857 |  0:00:53s
epoch 56 | loss: 0.1705  | val_0_rmse: 0.40919 | val_1_rmse: 0.40471 |  0:00:54s
epoch 57 | loss: 0.17154 | val_0_rmse: 0.4314  | val_1_rmse: 0.43271 |  0:00:55s
epoch 58 | loss: 0.1634  | val_0_rmse: 0.37464 | val_1_rmse: 0.37079 |  0:00:56s
epoch 59 | loss: 0.15501 | val_0_rmse: 0.37994 | val_1_rmse: 0.37904 |  0:00:57s
epoch 60 | loss: 0.15777 | val_0_rmse: 0.37849 | val_1_rmse: 0.37707 |  0:00:58s
epoch 61 | loss: 0.15258 | val_0_rmse: 0.37813 | val_1_rmse: 0.37634 |  0:00:59s
epoch 62 | loss: 0.15733 | val_0_rmse: 0.36949 | val_1_rmse: 0.37019 |  0:01:00s
epoch 63 | loss: 0.15679 | val_0_rmse: 0.37207 | val_1_rmse: 0.37297 |  0:01:01s
epoch 64 | loss: 0.1538  | val_0_rmse: 0.37579 | val_1_rmse: 0.37463 |  0:01:02s
epoch 65 | loss: 0.15352 | val_0_rmse: 0.37396 | val_1_rmse: 0.37656 |  0:01:03s
epoch 66 | loss: 0.16457 | val_0_rmse: 0.3716  | val_1_rmse: 0.37053 |  0:01:04s
epoch 67 | loss: 0.16016 | val_0_rmse: 0.36825 | val_1_rmse: 0.37006 |  0:01:05s
epoch 68 | loss: 0.15524 | val_0_rmse: 0.36936 | val_1_rmse: 0.36778 |  0:01:06s
epoch 69 | loss: 0.15645 | val_0_rmse: 0.38685 | val_1_rmse: 0.38994 |  0:01:06s
epoch 70 | loss: 0.15168 | val_0_rmse: 0.35859 | val_1_rmse: 0.35729 |  0:01:07s
epoch 71 | loss: 0.14988 | val_0_rmse: 0.4112  | val_1_rmse: 0.41417 |  0:01:08s
epoch 72 | loss: 0.15278 | val_0_rmse: 0.36766 | val_1_rmse: 0.3701  |  0:01:09s
epoch 73 | loss: 0.15181 | val_0_rmse: 0.374   | val_1_rmse: 0.37452 |  0:01:10s
epoch 74 | loss: 0.14909 | val_0_rmse: 0.36416 | val_1_rmse: 0.36453 |  0:01:11s
epoch 75 | loss: 0.14683 | val_0_rmse: 0.37006 | val_1_rmse: 0.36591 |  0:01:12s
epoch 76 | loss: 0.14915 | val_0_rmse: 0.38677 | val_1_rmse: 0.38751 |  0:01:13s
epoch 77 | loss: 0.15161 | val_0_rmse: 0.36449 | val_1_rmse: 0.36408 |  0:01:14s
epoch 78 | loss: 0.14467 | val_0_rmse: 0.37312 | val_1_rmse: 0.37701 |  0:01:15s
epoch 79 | loss: 0.14567 | val_0_rmse: 0.35582 | val_1_rmse: 0.35696 |  0:01:16s
epoch 80 | loss: 0.14871 | val_0_rmse: 0.36256 | val_1_rmse: 0.37053 |  0:01:17s
epoch 81 | loss: 0.14831 | val_0_rmse: 0.36113 | val_1_rmse: 0.36142 |  0:01:18s
epoch 82 | loss: 0.14644 | val_0_rmse: 0.37772 | val_1_rmse: 0.3811  |  0:01:19s
epoch 83 | loss: 0.14286 | val_0_rmse: 0.36257 | val_1_rmse: 0.3675  |  0:01:20s
epoch 84 | loss: 0.14684 | val_0_rmse: 0.36864 | val_1_rmse: 0.3701  |  0:01:21s
epoch 85 | loss: 0.14383 | val_0_rmse: 0.37825 | val_1_rmse: 0.38316 |  0:01:22s
epoch 86 | loss: 0.14205 | val_0_rmse: 0.35631 | val_1_rmse: 0.35795 |  0:01:23s
epoch 87 | loss: 0.14211 | val_0_rmse: 0.36884 | val_1_rmse: 0.36954 |  0:01:24s
epoch 88 | loss: 0.14276 | val_0_rmse: 0.36331 | val_1_rmse: 0.36681 |  0:01:25s
epoch 89 | loss: 0.14729 | val_0_rmse: 0.35288 | val_1_rmse: 0.35407 |  0:01:26s
epoch 90 | loss: 0.13766 | val_0_rmse: 0.36314 | val_1_rmse: 0.36899 |  0:01:27s
epoch 91 | loss: 0.14216 | val_0_rmse: 0.36656 | val_1_rmse: 0.37133 |  0:01:28s
epoch 92 | loss: 0.15164 | val_0_rmse: 0.38089 | val_1_rmse: 0.3838  |  0:01:29s
epoch 93 | loss: 0.14888 | val_0_rmse: 0.36236 | val_1_rmse: 0.37095 |  0:01:29s
epoch 94 | loss: 0.14002 | val_0_rmse: 0.34962 | val_1_rmse: 0.35267 |  0:01:30s
epoch 95 | loss: 0.13962 | val_0_rmse: 0.36687 | val_1_rmse: 0.36873 |  0:01:31s
epoch 96 | loss: 0.14009 | val_0_rmse: 0.35483 | val_1_rmse: 0.35822 |  0:01:32s
epoch 97 | loss: 0.13625 | val_0_rmse: 0.35836 | val_1_rmse: 0.3651  |  0:01:33s
epoch 98 | loss: 0.13731 | val_0_rmse: 0.34499 | val_1_rmse: 0.35476 |  0:01:34s
epoch 99 | loss: 0.13631 | val_0_rmse: 0.34405 | val_1_rmse: 0.35081 |  0:01:35s
epoch 100| loss: 0.13604 | val_0_rmse: 0.37343 | val_1_rmse: 0.38118 |  0:01:36s
epoch 101| loss: 0.13784 | val_0_rmse: 0.36463 | val_1_rmse: 0.37383 |  0:01:37s
epoch 102| loss: 0.13919 | val_0_rmse: 0.35901 | val_1_rmse: 0.36911 |  0:01:38s
epoch 103| loss: 0.13987 | val_0_rmse: 0.35076 | val_1_rmse: 0.35856 |  0:01:39s
epoch 104| loss: 0.1416  | val_0_rmse: 0.35067 | val_1_rmse: 0.3598  |  0:01:40s
epoch 105| loss: 0.13529 | val_0_rmse: 0.3439  | val_1_rmse: 0.35172 |  0:01:41s
epoch 106| loss: 0.13262 | val_0_rmse: 0.34975 | val_1_rmse: 0.36017 |  0:01:42s
epoch 107| loss: 0.13307 | val_0_rmse: 0.33856 | val_1_rmse: 0.34788 |  0:01:43s
epoch 108| loss: 0.13638 | val_0_rmse: 0.35639 | val_1_rmse: 0.36644 |  0:01:44s
epoch 109| loss: 0.13399 | val_0_rmse: 0.34062 | val_1_rmse: 0.34664 |  0:01:45s
epoch 110| loss: 0.13307 | val_0_rmse: 0.34976 | val_1_rmse: 0.35778 |  0:01:46s
epoch 111| loss: 0.13056 | val_0_rmse: 0.35621 | val_1_rmse: 0.36766 |  0:01:47s
epoch 112| loss: 0.14069 | val_0_rmse: 0.34879 | val_1_rmse: 0.36069 |  0:01:48s
epoch 113| loss: 0.13054 | val_0_rmse: 0.3416  | val_1_rmse: 0.35123 |  0:01:49s
epoch 114| loss: 0.13142 | val_0_rmse: 0.35356 | val_1_rmse: 0.36775 |  0:01:50s
epoch 115| loss: 0.12828 | val_0_rmse: 0.34414 | val_1_rmse: 0.35656 |  0:01:50s
epoch 116| loss: 0.13447 | val_0_rmse: 0.33667 | val_1_rmse: 0.34817 |  0:01:51s
epoch 117| loss: 0.13463 | val_0_rmse: 0.34875 | val_1_rmse: 0.36247 |  0:01:52s
epoch 118| loss: 0.13375 | val_0_rmse: 0.3427  | val_1_rmse: 0.35617 |  0:01:53s
epoch 119| loss: 0.12936 | val_0_rmse: 0.34816 | val_1_rmse: 0.35869 |  0:01:54s
epoch 120| loss: 0.13432 | val_0_rmse: 0.37263 | val_1_rmse: 0.37789 |  0:01:55s
epoch 121| loss: 0.13993 | val_0_rmse: 0.34335 | val_1_rmse: 0.35243 |  0:01:56s
epoch 122| loss: 0.1384  | val_0_rmse: 0.36644 | val_1_rmse: 0.37934 |  0:01:57s
epoch 123| loss: 0.13324 | val_0_rmse: 0.35622 | val_1_rmse: 0.36506 |  0:01:58s
epoch 124| loss: 0.13157 | val_0_rmse: 0.33848 | val_1_rmse: 0.351   |  0:01:59s
epoch 125| loss: 0.13008 | val_0_rmse: 0.35681 | val_1_rmse: 0.37152 |  0:02:00s
epoch 126| loss: 0.13496 | val_0_rmse: 0.35756 | val_1_rmse: 0.37087 |  0:02:01s
epoch 127| loss: 0.13215 | val_0_rmse: 0.33924 | val_1_rmse: 0.35034 |  0:02:02s
epoch 128| loss: 0.12839 | val_0_rmse: 0.33377 | val_1_rmse: 0.35063 |  0:02:03s
epoch 129| loss: 0.12863 | val_0_rmse: 0.34888 | val_1_rmse: 0.36386 |  0:02:04s
epoch 130| loss: 0.1281  | val_0_rmse: 0.34029 | val_1_rmse: 0.35593 |  0:02:05s
epoch 131| loss: 0.12497 | val_0_rmse: 0.3318  | val_1_rmse: 0.34976 |  0:02:06s
epoch 132| loss: 0.12818 | val_0_rmse: 0.34021 | val_1_rmse: 0.35788 |  0:02:07s
epoch 133| loss: 0.12884 | val_0_rmse: 0.3473  | val_1_rmse: 0.36378 |  0:02:08s
epoch 134| loss: 0.12674 | val_0_rmse: 0.33583 | val_1_rmse: 0.35349 |  0:02:09s
epoch 135| loss: 0.12966 | val_0_rmse: 0.35365 | val_1_rmse: 0.37044 |  0:02:10s
epoch 136| loss: 0.12593 | val_0_rmse: 0.34074 | val_1_rmse: 0.35498 |  0:02:11s
epoch 137| loss: 0.14528 | val_0_rmse: 0.38183 | val_1_rmse: 0.39923 |  0:02:11s
epoch 138| loss: 0.15546 | val_0_rmse: 0.38848 | val_1_rmse: 0.40274 |  0:02:12s
epoch 139| loss: 0.14172 | val_0_rmse: 0.35392 | val_1_rmse: 0.36858 |  0:02:13s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.34664
Best weights from best epoch are automatically used!
ended training at: 05:02:49
Feature importance:
Mean squared error is of 0.06554147261187729
Mean absolute error:0.16168819892947622
MAPE:0.18478271937621324
R2 score:0.8553734747206008
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:02:49
epoch 0  | loss: 1.44455 | val_0_rmse: 0.99338 | val_1_rmse: 0.97484 |  0:00:00s
epoch 1  | loss: 0.70701 | val_0_rmse: 0.79562 | val_1_rmse: 0.80123 |  0:00:01s
epoch 2  | loss: 0.52768 | val_0_rmse: 0.73316 | val_1_rmse: 0.7282  |  0:00:02s
epoch 3  | loss: 0.4031  | val_0_rmse: 0.88763 | val_1_rmse: 0.88793 |  0:00:03s
epoch 4  | loss: 0.32698 | val_0_rmse: 0.82253 | val_1_rmse: 0.83888 |  0:00:04s
epoch 5  | loss: 0.28825 | val_0_rmse: 0.75469 | val_1_rmse: 0.76893 |  0:00:05s
epoch 6  | loss: 0.27674 | val_0_rmse: 0.71061 | val_1_rmse: 0.72385 |  0:00:06s
epoch 7  | loss: 0.25276 | val_0_rmse: 0.64728 | val_1_rmse: 0.66259 |  0:00:07s
epoch 8  | loss: 0.22837 | val_0_rmse: 0.63936 | val_1_rmse: 0.65583 |  0:00:08s
epoch 9  | loss: 0.21335 | val_0_rmse: 0.64439 | val_1_rmse: 0.66299 |  0:00:09s
epoch 10 | loss: 0.21062 | val_0_rmse: 0.60387 | val_1_rmse: 0.6287  |  0:00:10s
epoch 11 | loss: 0.23168 | val_0_rmse: 0.55607 | val_1_rmse: 0.57369 |  0:00:11s
epoch 12 | loss: 0.22354 | val_0_rmse: 0.49319 | val_1_rmse: 0.50795 |  0:00:12s
epoch 13 | loss: 0.22531 | val_0_rmse: 0.58583 | val_1_rmse: 0.60149 |  0:00:13s
epoch 14 | loss: 0.21765 | val_0_rmse: 0.54268 | val_1_rmse: 0.56322 |  0:00:14s
epoch 15 | loss: 0.20838 | val_0_rmse: 0.47726 | val_1_rmse: 0.48426 |  0:00:15s
epoch 16 | loss: 0.21744 | val_0_rmse: 0.503   | val_1_rmse: 0.52165 |  0:00:16s
epoch 17 | loss: 0.20844 | val_0_rmse: 0.54783 | val_1_rmse: 0.56936 |  0:00:17s
epoch 18 | loss: 0.19205 | val_0_rmse: 0.45139 | val_1_rmse: 0.47117 |  0:00:18s
epoch 19 | loss: 0.17928 | val_0_rmse: 0.47871 | val_1_rmse: 0.50476 |  0:00:19s
epoch 20 | loss: 0.17986 | val_0_rmse: 0.44054 | val_1_rmse: 0.46087 |  0:00:20s
epoch 21 | loss: 0.17063 | val_0_rmse: 0.44207 | val_1_rmse: 0.46886 |  0:00:21s
epoch 22 | loss: 0.17158 | val_0_rmse: 0.45726 | val_1_rmse: 0.4863  |  0:00:21s
epoch 23 | loss: 0.17266 | val_0_rmse: 0.4343  | val_1_rmse: 0.45734 |  0:00:22s
epoch 24 | loss: 0.19037 | val_0_rmse: 0.46954 | val_1_rmse: 0.47586 |  0:00:23s
epoch 25 | loss: 0.20523 | val_0_rmse: 0.48233 | val_1_rmse: 0.50456 |  0:00:24s
epoch 26 | loss: 0.19761 | val_0_rmse: 0.46442 | val_1_rmse: 0.48047 |  0:00:25s
epoch 27 | loss: 0.18624 | val_0_rmse: 0.48024 | val_1_rmse: 0.49511 |  0:00:26s
epoch 28 | loss: 0.18597 | val_0_rmse: 0.44663 | val_1_rmse: 0.4637  |  0:00:27s
epoch 29 | loss: 0.17906 | val_0_rmse: 0.44937 | val_1_rmse: 0.45423 |  0:00:28s
epoch 30 | loss: 0.18008 | val_0_rmse: 0.44298 | val_1_rmse: 0.45752 |  0:00:29s
epoch 31 | loss: 0.18788 | val_0_rmse: 0.59232 | val_1_rmse: 0.61375 |  0:00:30s
epoch 32 | loss: 0.17566 | val_0_rmse: 0.424   | val_1_rmse: 0.44265 |  0:00:31s
epoch 33 | loss: 0.18968 | val_0_rmse: 0.47456 | val_1_rmse: 0.48862 |  0:00:32s
epoch 34 | loss: 0.20559 | val_0_rmse: 0.46587 | val_1_rmse: 0.47448 |  0:00:33s
epoch 35 | loss: 0.2076  | val_0_rmse: 0.47525 | val_1_rmse: 0.4919  |  0:00:34s
epoch 36 | loss: 0.21395 | val_0_rmse: 0.45483 | val_1_rmse: 0.46563 |  0:00:35s
epoch 37 | loss: 0.19492 | val_0_rmse: 0.41896 | val_1_rmse: 0.434   |  0:00:36s
epoch 38 | loss: 0.18542 | val_0_rmse: 0.42474 | val_1_rmse: 0.44566 |  0:00:37s
epoch 39 | loss: 0.19209 | val_0_rmse: 0.4377  | val_1_rmse: 0.44193 |  0:00:38s
epoch 40 | loss: 0.21204 | val_0_rmse: 0.52281 | val_1_rmse: 0.53294 |  0:00:39s
epoch 41 | loss: 0.21708 | val_0_rmse: 0.47282 | val_1_rmse: 0.47865 |  0:00:40s
epoch 42 | loss: 0.21994 | val_0_rmse: 0.44468 | val_1_rmse: 0.46136 |  0:00:41s
epoch 43 | loss: 0.20006 | val_0_rmse: 0.48636 | val_1_rmse: 0.48604 |  0:00:42s
epoch 44 | loss: 0.19993 | val_0_rmse: 0.41179 | val_1_rmse: 0.42421 |  0:00:43s
epoch 45 | loss: 0.19372 | val_0_rmse: 0.43731 | val_1_rmse: 0.44609 |  0:00:43s
epoch 46 | loss: 0.20175 | val_0_rmse: 0.45439 | val_1_rmse: 0.47026 |  0:00:44s
epoch 47 | loss: 0.18577 | val_0_rmse: 0.4123  | val_1_rmse: 0.42457 |  0:00:45s
epoch 48 | loss: 0.17132 | val_0_rmse: 0.40931 | val_1_rmse: 0.42778 |  0:00:46s
epoch 49 | loss: 0.17495 | val_0_rmse: 0.39156 | val_1_rmse: 0.40444 |  0:00:47s
epoch 50 | loss: 0.16845 | val_0_rmse: 0.40748 | val_1_rmse: 0.40733 |  0:00:48s
epoch 51 | loss: 0.16597 | val_0_rmse: 0.41148 | val_1_rmse: 0.43199 |  0:00:49s
epoch 52 | loss: 0.16686 | val_0_rmse: 0.40153 | val_1_rmse: 0.41182 |  0:00:50s
epoch 53 | loss: 0.18022 | val_0_rmse: 0.3952  | val_1_rmse: 0.40542 |  0:00:51s
epoch 54 | loss: 0.1642  | val_0_rmse: 0.39595 | val_1_rmse: 0.40992 |  0:00:52s
epoch 55 | loss: 0.17049 | val_0_rmse: 0.3971  | val_1_rmse: 0.40685 |  0:00:53s
epoch 56 | loss: 0.17591 | val_0_rmse: 0.46718 | val_1_rmse: 0.47983 |  0:00:54s
epoch 57 | loss: 0.213   | val_0_rmse: 0.4424  | val_1_rmse: 0.45128 |  0:00:55s
epoch 58 | loss: 0.20098 | val_0_rmse: 0.42127 | val_1_rmse: 0.42674 |  0:00:56s
epoch 59 | loss: 0.19066 | val_0_rmse: 0.43823 | val_1_rmse: 0.44501 |  0:00:57s
epoch 60 | loss: 0.18216 | val_0_rmse: 0.45079 | val_1_rmse: 0.45633 |  0:00:58s
epoch 61 | loss: 0.18794 | val_0_rmse: 0.40911 | val_1_rmse: 0.4167  |  0:00:59s
epoch 62 | loss: 0.1857  | val_0_rmse: 0.41659 | val_1_rmse: 0.42931 |  0:01:00s
epoch 63 | loss: 0.18104 | val_0_rmse: 0.41324 | val_1_rmse: 0.43051 |  0:01:01s
epoch 64 | loss: 0.17018 | val_0_rmse: 0.38793 | val_1_rmse: 0.40211 |  0:01:02s
epoch 65 | loss: 0.16506 | val_0_rmse: 0.38203 | val_1_rmse: 0.394   |  0:01:03s
epoch 66 | loss: 0.16098 | val_0_rmse: 0.3988  | val_1_rmse: 0.40522 |  0:01:04s
epoch 67 | loss: 0.16386 | val_0_rmse: 0.39028 | val_1_rmse: 0.40211 |  0:01:04s
epoch 68 | loss: 0.17832 | val_0_rmse: 0.3866  | val_1_rmse: 0.40194 |  0:01:05s
epoch 69 | loss: 0.17335 | val_0_rmse: 0.42157 | val_1_rmse: 0.42934 |  0:01:06s
epoch 70 | loss: 0.1622  | val_0_rmse: 0.38077 | val_1_rmse: 0.39396 |  0:01:07s
epoch 71 | loss: 0.16422 | val_0_rmse: 0.39919 | val_1_rmse: 0.41054 |  0:01:08s
epoch 72 | loss: 0.16108 | val_0_rmse: 0.37976 | val_1_rmse: 0.39102 |  0:01:09s
epoch 73 | loss: 0.15844 | val_0_rmse: 0.37154 | val_1_rmse: 0.38515 |  0:01:10s
epoch 74 | loss: 0.15576 | val_0_rmse: 0.36882 | val_1_rmse: 0.38452 |  0:01:11s
epoch 75 | loss: 0.15515 | val_0_rmse: 0.39569 | val_1_rmse: 0.40758 |  0:01:12s
epoch 76 | loss: 0.15662 | val_0_rmse: 0.36727 | val_1_rmse: 0.37909 |  0:01:13s
epoch 77 | loss: 0.15493 | val_0_rmse: 0.37557 | val_1_rmse: 0.39094 |  0:01:14s
epoch 78 | loss: 0.14935 | val_0_rmse: 0.38677 | val_1_rmse: 0.39189 |  0:01:15s
epoch 79 | loss: 0.15367 | val_0_rmse: 0.36489 | val_1_rmse: 0.37872 |  0:01:16s
epoch 80 | loss: 0.15137 | val_0_rmse: 0.36432 | val_1_rmse: 0.37695 |  0:01:17s
epoch 81 | loss: 0.14853 | val_0_rmse: 0.36505 | val_1_rmse: 0.3771  |  0:01:18s
epoch 82 | loss: 0.14772 | val_0_rmse: 0.37734 | val_1_rmse: 0.38965 |  0:01:19s
epoch 83 | loss: 0.14892 | val_0_rmse: 0.36366 | val_1_rmse: 0.37639 |  0:01:20s
epoch 84 | loss: 0.14944 | val_0_rmse: 0.35891 | val_1_rmse: 0.37879 |  0:01:21s
epoch 85 | loss: 0.15089 | val_0_rmse: 0.35569 | val_1_rmse: 0.37125 |  0:01:22s
epoch 86 | loss: 0.14457 | val_0_rmse: 0.39343 | val_1_rmse: 0.41174 |  0:01:23s
epoch 87 | loss: 0.15146 | val_0_rmse: 0.37037 | val_1_rmse: 0.38673 |  0:01:24s
epoch 88 | loss: 0.14413 | val_0_rmse: 0.35576 | val_1_rmse: 0.37142 |  0:01:25s
epoch 89 | loss: 0.14715 | val_0_rmse: 0.37065 | val_1_rmse: 0.38609 |  0:01:26s
epoch 90 | loss: 0.14529 | val_0_rmse: 0.3691  | val_1_rmse: 0.37823 |  0:01:27s
epoch 91 | loss: 0.15728 | val_0_rmse: 0.36867 | val_1_rmse: 0.37685 |  0:01:28s
epoch 92 | loss: 0.14739 | val_0_rmse: 0.35504 | val_1_rmse: 0.36745 |  0:01:28s
epoch 93 | loss: 0.14406 | val_0_rmse: 0.35903 | val_1_rmse: 0.37337 |  0:01:29s
epoch 94 | loss: 0.14336 | val_0_rmse: 0.35629 | val_1_rmse: 0.36585 |  0:01:30s
epoch 95 | loss: 0.14517 | val_0_rmse: 0.35577 | val_1_rmse: 0.3685  |  0:01:31s
epoch 96 | loss: 0.13926 | val_0_rmse: 0.34534 | val_1_rmse: 0.35892 |  0:01:32s
epoch 97 | loss: 0.137   | val_0_rmse: 0.36458 | val_1_rmse: 0.38582 |  0:01:33s
epoch 98 | loss: 0.14425 | val_0_rmse: 0.34938 | val_1_rmse: 0.36324 |  0:01:34s
epoch 99 | loss: 0.13809 | val_0_rmse: 0.34827 | val_1_rmse: 0.36102 |  0:01:35s
epoch 100| loss: 0.1365  | val_0_rmse: 0.3474  | val_1_rmse: 0.3596  |  0:01:36s
epoch 101| loss: 0.13366 | val_0_rmse: 0.34756 | val_1_rmse: 0.36264 |  0:01:37s
epoch 102| loss: 0.13467 | val_0_rmse: 0.34881 | val_1_rmse: 0.36425 |  0:01:38s
epoch 103| loss: 0.13769 | val_0_rmse: 0.34888 | val_1_rmse: 0.36267 |  0:01:39s
epoch 104| loss: 0.13902 | val_0_rmse: 0.35127 | val_1_rmse: 0.36332 |  0:01:40s
epoch 105| loss: 0.13994 | val_0_rmse: 0.35004 | val_1_rmse: 0.36378 |  0:01:41s
epoch 106| loss: 0.13592 | val_0_rmse: 0.35884 | val_1_rmse: 0.37034 |  0:01:42s
epoch 107| loss: 0.13466 | val_0_rmse: 0.35247 | val_1_rmse: 0.36621 |  0:01:43s
epoch 108| loss: 0.13871 | val_0_rmse: 0.39623 | val_1_rmse: 0.39881 |  0:01:44s
epoch 109| loss: 0.1445  | val_0_rmse: 0.38164 | val_1_rmse: 0.38376 |  0:01:45s
epoch 110| loss: 0.14673 | val_0_rmse: 0.36749 | val_1_rmse: 0.37925 |  0:01:46s
epoch 111| loss: 0.13694 | val_0_rmse: 0.35376 | val_1_rmse: 0.37549 |  0:01:47s
epoch 112| loss: 0.13669 | val_0_rmse: 0.35426 | val_1_rmse: 0.3746  |  0:01:48s
epoch 113| loss: 0.13398 | val_0_rmse: 0.33337 | val_1_rmse: 0.35175 |  0:01:48s
epoch 114| loss: 0.12723 | val_0_rmse: 0.34571 | val_1_rmse: 0.36102 |  0:01:49s
epoch 115| loss: 0.13277 | val_0_rmse: 0.35047 | val_1_rmse: 0.36535 |  0:01:50s
epoch 116| loss: 0.13541 | val_0_rmse: 0.33928 | val_1_rmse: 0.35482 |  0:01:51s
epoch 117| loss: 0.13305 | val_0_rmse: 0.34342 | val_1_rmse: 0.35813 |  0:01:52s
epoch 118| loss: 0.12973 | val_0_rmse: 0.35689 | val_1_rmse: 0.36785 |  0:01:53s
epoch 119| loss: 0.1323  | val_0_rmse: 0.37253 | val_1_rmse: 0.39298 |  0:01:54s
epoch 120| loss: 0.13312 | val_0_rmse: 0.36261 | val_1_rmse: 0.38228 |  0:01:55s
epoch 121| loss: 0.13172 | val_0_rmse: 0.35001 | val_1_rmse: 0.36622 |  0:01:56s
epoch 122| loss: 0.13024 | val_0_rmse: 0.34412 | val_1_rmse: 0.35684 |  0:01:57s
epoch 123| loss: 0.12555 | val_0_rmse: 0.33576 | val_1_rmse: 0.34776 |  0:01:58s
epoch 124| loss: 0.12956 | val_0_rmse: 0.33552 | val_1_rmse: 0.35642 |  0:01:59s
epoch 125| loss: 0.13041 | val_0_rmse: 0.3304  | val_1_rmse: 0.35301 |  0:02:00s
epoch 126| loss: 0.12657 | val_0_rmse: 0.35549 | val_1_rmse: 0.37505 |  0:02:01s
epoch 127| loss: 0.1313  | val_0_rmse: 0.33327 | val_1_rmse: 0.35115 |  0:02:02s
epoch 128| loss: 0.12567 | val_0_rmse: 0.35018 | val_1_rmse: 0.37117 |  0:02:03s
epoch 129| loss: 0.134   | val_0_rmse: 0.35468 | val_1_rmse: 0.37454 |  0:02:04s
epoch 130| loss: 0.13649 | val_0_rmse: 0.33913 | val_1_rmse: 0.35417 |  0:02:05s
epoch 131| loss: 0.13835 | val_0_rmse: 0.34496 | val_1_rmse: 0.36447 |  0:02:06s
epoch 132| loss: 0.14141 | val_0_rmse: 0.35374 | val_1_rmse: 0.37663 |  0:02:07s
epoch 133| loss: 0.1526  | val_0_rmse: 0.4008  | val_1_rmse: 0.41323 |  0:02:08s
epoch 134| loss: 0.14529 | val_0_rmse: 0.35753 | val_1_rmse: 0.37327 |  0:02:09s
epoch 135| loss: 0.13681 | val_0_rmse: 0.34443 | val_1_rmse: 0.35846 |  0:02:09s
epoch 136| loss: 0.14003 | val_0_rmse: 0.34665 | val_1_rmse: 0.36466 |  0:02:10s
epoch 137| loss: 0.13658 | val_0_rmse: 0.34442 | val_1_rmse: 0.35416 |  0:02:11s
epoch 138| loss: 0.14059 | val_0_rmse: 0.35151 | val_1_rmse: 0.36282 |  0:02:12s
epoch 139| loss: 0.13308 | val_0_rmse: 0.34495 | val_1_rmse: 0.3622  |  0:02:13s
epoch 140| loss: 0.13491 | val_0_rmse: 0.33824 | val_1_rmse: 0.35278 |  0:02:14s
epoch 141| loss: 0.13931 | val_0_rmse: 0.36077 | val_1_rmse: 0.37629 |  0:02:15s
epoch 142| loss: 0.13812 | val_0_rmse: 0.36895 | val_1_rmse: 0.38151 |  0:02:16s
epoch 143| loss: 0.13732 | val_0_rmse: 0.35006 | val_1_rmse: 0.3666  |  0:02:17s
epoch 144| loss: 0.13275 | val_0_rmse: 0.36191 | val_1_rmse: 0.37052 |  0:02:18s
epoch 145| loss: 0.13158 | val_0_rmse: 0.35282 | val_1_rmse: 0.36907 |  0:02:19s
epoch 146| loss: 0.13385 | val_0_rmse: 0.38112 | val_1_rmse: 0.39667 |  0:02:20s
epoch 147| loss: 0.13093 | val_0_rmse: 0.34062 | val_1_rmse: 0.35422 |  0:02:21s
epoch 148| loss: 0.12881 | val_0_rmse: 0.34953 | val_1_rmse: 0.37335 |  0:02:22s
epoch 149| loss: 0.13421 | val_0_rmse: 0.34648 | val_1_rmse: 0.36511 |  0:02:23s
Stop training because you reached max_epochs = 150 with best_epoch = 123 and best_val_1_rmse = 0.34776
Best weights from best epoch are automatically used!
ended training at: 05:05:13
Feature importance:
Mean squared error is of 0.06575606472390862
Mean absolute error:0.16182187563629374
MAPE:0.17625965704149107
R2 score:0.8642869156147974
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:05:14
epoch 0  | loss: 1.63958 | val_0_rmse: 0.99779 | val_1_rmse: 1.0332  |  0:00:00s
epoch 1  | loss: 1.07354 | val_0_rmse: 0.96718 | val_1_rmse: 0.9928  |  0:00:00s
epoch 2  | loss: 0.86807 | val_0_rmse: 0.82819 | val_1_rmse: 0.85678 |  0:00:00s
epoch 3  | loss: 0.73232 | val_0_rmse: 0.9461  | val_1_rmse: 0.98263 |  0:00:01s
epoch 4  | loss: 0.68502 | val_0_rmse: 0.88429 | val_1_rmse: 0.91302 |  0:00:01s
epoch 5  | loss: 0.67319 | val_0_rmse: 0.84054 | val_1_rmse: 0.86375 |  0:00:01s
epoch 6  | loss: 0.66774 | val_0_rmse: 0.86514 | val_1_rmse: 0.88612 |  0:00:02s
epoch 7  | loss: 0.67052 | val_0_rmse: 0.81803 | val_1_rmse: 0.84601 |  0:00:02s
epoch 8  | loss: 0.66446 | val_0_rmse: 0.80849 | val_1_rmse: 0.83694 |  0:00:02s
epoch 9  | loss: 0.65496 | val_0_rmse: 0.82163 | val_1_rmse: 0.84905 |  0:00:03s
epoch 10 | loss: 0.6577  | val_0_rmse: 0.80984 | val_1_rmse: 0.83583 |  0:00:03s
epoch 11 | loss: 0.64461 | val_0_rmse: 0.80808 | val_1_rmse: 0.84088 |  0:00:03s
epoch 12 | loss: 0.63952 | val_0_rmse: 0.80955 | val_1_rmse: 0.84456 |  0:00:04s
epoch 13 | loss: 0.641   | val_0_rmse: 0.80131 | val_1_rmse: 0.83938 |  0:00:04s
epoch 14 | loss: 0.64227 | val_0_rmse: 0.80561 | val_1_rmse: 0.8401  |  0:00:04s
epoch 15 | loss: 0.64627 | val_0_rmse: 0.80155 | val_1_rmse: 0.83273 |  0:00:04s
epoch 16 | loss: 0.63512 | val_0_rmse: 0.79691 | val_1_rmse: 0.83109 |  0:00:05s
epoch 17 | loss: 0.63235 | val_0_rmse: 0.79622 | val_1_rmse: 0.82966 |  0:00:05s
epoch 18 | loss: 0.63602 | val_0_rmse: 0.8028  | val_1_rmse: 0.82836 |  0:00:05s
epoch 19 | loss: 0.64072 | val_0_rmse: 0.80259 | val_1_rmse: 0.83339 |  0:00:06s
epoch 20 | loss: 0.63329 | val_0_rmse: 0.80081 | val_1_rmse: 0.82886 |  0:00:06s
epoch 21 | loss: 0.6252  | val_0_rmse: 0.79698 | val_1_rmse: 0.82436 |  0:00:06s
epoch 22 | loss: 0.62165 | val_0_rmse: 0.79749 | val_1_rmse: 0.82399 |  0:00:07s
epoch 23 | loss: 0.62538 | val_0_rmse: 0.79566 | val_1_rmse: 0.82049 |  0:00:07s
epoch 24 | loss: 0.62459 | val_0_rmse: 0.8037  | val_1_rmse: 0.82886 |  0:00:07s
epoch 25 | loss: 0.63407 | val_0_rmse: 0.79888 | val_1_rmse: 0.82552 |  0:00:08s
epoch 26 | loss: 0.62649 | val_0_rmse: 0.8087  | val_1_rmse: 0.83695 |  0:00:08s
epoch 27 | loss: 0.62772 | val_0_rmse: 0.79793 | val_1_rmse: 0.82526 |  0:00:08s
epoch 28 | loss: 0.62465 | val_0_rmse: 0.79932 | val_1_rmse: 0.83076 |  0:00:09s
epoch 29 | loss: 0.62978 | val_0_rmse: 0.80161 | val_1_rmse: 0.82928 |  0:00:09s
epoch 30 | loss: 0.62161 | val_0_rmse: 0.80802 | val_1_rmse: 0.83807 |  0:00:09s
epoch 31 | loss: 0.63064 | val_0_rmse: 0.80463 | val_1_rmse: 0.836   |  0:00:10s
epoch 32 | loss: 0.63445 | val_0_rmse: 0.80358 | val_1_rmse: 0.83881 |  0:00:10s
epoch 33 | loss: 0.63547 | val_0_rmse: 0.79888 | val_1_rmse: 0.83276 |  0:00:10s
epoch 34 | loss: 0.62446 | val_0_rmse: 0.81052 | val_1_rmse: 0.84082 |  0:00:11s
epoch 35 | loss: 0.62266 | val_0_rmse: 0.80951 | val_1_rmse: 0.83842 |  0:00:11s
epoch 36 | loss: 0.63132 | val_0_rmse: 0.79783 | val_1_rmse: 0.82984 |  0:00:11s
epoch 37 | loss: 0.63181 | val_0_rmse: 0.79607 | val_1_rmse: 0.82954 |  0:00:11s
epoch 38 | loss: 0.63047 | val_0_rmse: 0.79414 | val_1_rmse: 0.82743 |  0:00:12s
epoch 39 | loss: 0.61119 | val_0_rmse: 0.7954  | val_1_rmse: 0.83132 |  0:00:12s
epoch 40 | loss: 0.61939 | val_0_rmse: 0.79012 | val_1_rmse: 0.82603 |  0:00:12s
epoch 41 | loss: 0.60869 | val_0_rmse: 0.7932  | val_1_rmse: 0.83228 |  0:00:13s
epoch 42 | loss: 0.59976 | val_0_rmse: 0.79815 | val_1_rmse: 0.83921 |  0:00:13s
epoch 43 | loss: 0.58881 | val_0_rmse: 0.78627 | val_1_rmse: 0.82504 |  0:00:13s
epoch 44 | loss: 0.59973 | val_0_rmse: 0.78335 | val_1_rmse: 0.82593 |  0:00:14s
epoch 45 | loss: 0.59436 | val_0_rmse: 0.78163 | val_1_rmse: 0.82625 |  0:00:14s
epoch 46 | loss: 0.59418 | val_0_rmse: 0.7787  | val_1_rmse: 0.81995 |  0:00:14s
epoch 47 | loss: 0.58528 | val_0_rmse: 0.78327 | val_1_rmse: 0.822   |  0:00:15s
epoch 48 | loss: 0.59199 | val_0_rmse: 0.77562 | val_1_rmse: 0.81702 |  0:00:15s
epoch 49 | loss: 0.58798 | val_0_rmse: 0.77384 | val_1_rmse: 0.8145  |  0:00:15s
epoch 50 | loss: 0.59609 | val_0_rmse: 0.79468 | val_1_rmse: 0.8344  |  0:00:16s
epoch 51 | loss: 0.58578 | val_0_rmse: 0.77546 | val_1_rmse: 0.81777 |  0:00:16s
epoch 52 | loss: 0.58696 | val_0_rmse: 0.77686 | val_1_rmse: 0.81953 |  0:00:16s
epoch 53 | loss: 0.5822  | val_0_rmse: 0.77338 | val_1_rmse: 0.81649 |  0:00:16s
epoch 54 | loss: 0.57824 | val_0_rmse: 0.77388 | val_1_rmse: 0.81325 |  0:00:17s
epoch 55 | loss: 0.5785  | val_0_rmse: 0.77416 | val_1_rmse: 0.81381 |  0:00:17s
epoch 56 | loss: 0.58118 | val_0_rmse: 0.77486 | val_1_rmse: 0.81358 |  0:00:17s
epoch 57 | loss: 0.57951 | val_0_rmse: 0.77331 | val_1_rmse: 0.81259 |  0:00:18s
epoch 58 | loss: 0.58167 | val_0_rmse: 0.78613 | val_1_rmse: 0.82745 |  0:00:18s
epoch 59 | loss: 0.57839 | val_0_rmse: 0.78    | val_1_rmse: 0.81978 |  0:00:18s
epoch 60 | loss: 0.59248 | val_0_rmse: 0.78194 | val_1_rmse: 0.81796 |  0:00:19s
epoch 61 | loss: 0.59359 | val_0_rmse: 0.78179 | val_1_rmse: 0.81949 |  0:00:19s
epoch 62 | loss: 0.59986 | val_0_rmse: 0.78841 | val_1_rmse: 0.81379 |  0:00:19s
epoch 63 | loss: 0.63108 | val_0_rmse: 0.78443 | val_1_rmse: 0.82334 |  0:00:20s
epoch 64 | loss: 0.61716 | val_0_rmse: 0.79472 | val_1_rmse: 0.83582 |  0:00:20s
epoch 65 | loss: 0.62177 | val_0_rmse: 0.7886  | val_1_rmse: 0.82987 |  0:00:20s
epoch 66 | loss: 0.61995 | val_0_rmse: 0.78827 | val_1_rmse: 0.82101 |  0:00:20s
epoch 67 | loss: 0.61874 | val_0_rmse: 0.78757 | val_1_rmse: 0.81795 |  0:00:21s
epoch 68 | loss: 0.62645 | val_0_rmse: 0.78633 | val_1_rmse: 0.81456 |  0:00:21s
epoch 69 | loss: 0.61668 | val_0_rmse: 0.78636 | val_1_rmse: 0.81241 |  0:00:22s
epoch 70 | loss: 0.61292 | val_0_rmse: 0.78281 | val_1_rmse: 0.80994 |  0:00:22s
epoch 71 | loss: 0.61191 | val_0_rmse: 0.78253 | val_1_rmse: 0.81307 |  0:00:22s
epoch 72 | loss: 0.60496 | val_0_rmse: 0.79113 | val_1_rmse: 0.82479 |  0:00:22s
epoch 73 | loss: 0.61119 | val_0_rmse: 0.78713 | val_1_rmse: 0.8221  |  0:00:23s
epoch 74 | loss: 0.61076 | val_0_rmse: 0.78927 | val_1_rmse: 0.82187 |  0:00:23s
epoch 75 | loss: 0.61292 | val_0_rmse: 0.78754 | val_1_rmse: 0.81918 |  0:00:23s
epoch 76 | loss: 0.62331 | val_0_rmse: 0.77945 | val_1_rmse: 0.80538 |  0:00:24s
epoch 77 | loss: 0.6037  | val_0_rmse: 0.77716 | val_1_rmse: 0.8073  |  0:00:24s
epoch 78 | loss: 0.59919 | val_0_rmse: 0.77084 | val_1_rmse: 0.80522 |  0:00:24s
epoch 79 | loss: 0.59733 | val_0_rmse: 0.76805 | val_1_rmse: 0.8032  |  0:00:25s
epoch 80 | loss: 0.59241 | val_0_rmse: 0.76996 | val_1_rmse: 0.80406 |  0:00:25s
epoch 81 | loss: 0.59335 | val_0_rmse: 0.76831 | val_1_rmse: 0.80256 |  0:00:25s
epoch 82 | loss: 0.58283 | val_0_rmse: 0.76428 | val_1_rmse: 0.79959 |  0:00:26s
epoch 83 | loss: 0.59126 | val_0_rmse: 0.76493 | val_1_rmse: 0.80344 |  0:00:26s
epoch 84 | loss: 0.58816 | val_0_rmse: 0.76523 | val_1_rmse: 0.80381 |  0:00:26s
epoch 85 | loss: 0.58564 | val_0_rmse: 0.76253 | val_1_rmse: 0.8049  |  0:00:27s
epoch 86 | loss: 0.57339 | val_0_rmse: 0.75897 | val_1_rmse: 0.80554 |  0:00:27s
epoch 87 | loss: 0.59012 | val_0_rmse: 0.76057 | val_1_rmse: 0.80432 |  0:00:27s
epoch 88 | loss: 0.58273 | val_0_rmse: 0.759   | val_1_rmse: 0.80035 |  0:00:28s
epoch 89 | loss: 0.58829 | val_0_rmse: 0.76163 | val_1_rmse: 0.79968 |  0:00:28s
epoch 90 | loss: 0.58157 | val_0_rmse: 0.75947 | val_1_rmse: 0.79629 |  0:00:28s
epoch 91 | loss: 0.57923 | val_0_rmse: 0.7598  | val_1_rmse: 0.79702 |  0:00:28s
epoch 92 | loss: 0.57901 | val_0_rmse: 0.75904 | val_1_rmse: 0.79381 |  0:00:29s
epoch 93 | loss: 0.5829  | val_0_rmse: 0.76344 | val_1_rmse: 0.79914 |  0:00:29s
epoch 94 | loss: 0.57881 | val_0_rmse: 0.75988 | val_1_rmse: 0.79523 |  0:00:29s
epoch 95 | loss: 0.57485 | val_0_rmse: 0.75813 | val_1_rmse: 0.7925  |  0:00:30s
epoch 96 | loss: 0.57285 | val_0_rmse: 0.75533 | val_1_rmse: 0.79161 |  0:00:30s
epoch 97 | loss: 0.57988 | val_0_rmse: 0.75529 | val_1_rmse: 0.79718 |  0:00:30s
epoch 98 | loss: 0.57495 | val_0_rmse: 0.75502 | val_1_rmse: 0.79857 |  0:00:31s
epoch 99 | loss: 0.58386 | val_0_rmse: 0.76196 | val_1_rmse: 0.80917 |  0:00:31s
epoch 100| loss: 0.59038 | val_0_rmse: 0.77807 | val_1_rmse: 0.81869 |  0:00:31s
epoch 101| loss: 0.5911  | val_0_rmse: 0.77028 | val_1_rmse: 0.81034 |  0:00:32s
epoch 102| loss: 0.5905  | val_0_rmse: 0.77045 | val_1_rmse: 0.80586 |  0:00:32s
epoch 103| loss: 0.59257 | val_0_rmse: 0.77161 | val_1_rmse: 0.811   |  0:00:32s
epoch 104| loss: 0.58327 | val_0_rmse: 0.76875 | val_1_rmse: 0.81218 |  0:00:33s
epoch 105| loss: 0.58646 | val_0_rmse: 0.77054 | val_1_rmse: 0.81847 |  0:00:33s
epoch 106| loss: 0.59336 | val_0_rmse: 0.7711  | val_1_rmse: 0.80991 |  0:00:33s
epoch 107| loss: 0.58852 | val_0_rmse: 0.76981 | val_1_rmse: 0.80999 |  0:00:34s
epoch 108| loss: 0.58982 | val_0_rmse: 0.76185 | val_1_rmse: 0.79814 |  0:00:34s
epoch 109| loss: 0.58469 | val_0_rmse: 0.75804 | val_1_rmse: 0.79814 |  0:00:34s
epoch 110| loss: 0.58119 | val_0_rmse: 0.75813 | val_1_rmse: 0.80933 |  0:00:34s
epoch 111| loss: 0.57823 | val_0_rmse: 0.76204 | val_1_rmse: 0.80997 |  0:00:35s
epoch 112| loss: 0.59064 | val_0_rmse: 0.77696 | val_1_rmse: 0.81972 |  0:00:35s
epoch 113| loss: 0.59137 | val_0_rmse: 0.77901 | val_1_rmse: 0.81917 |  0:00:35s
epoch 114| loss: 0.59587 | val_0_rmse: 0.76393 | val_1_rmse: 0.80475 |  0:00:36s
epoch 115| loss: 0.58847 | val_0_rmse: 0.764   | val_1_rmse: 0.81091 |  0:00:36s
epoch 116| loss: 0.59469 | val_0_rmse: 0.76499 | val_1_rmse: 0.81151 |  0:00:36s
epoch 117| loss: 0.58424 | val_0_rmse: 0.76781 | val_1_rmse: 0.81219 |  0:00:37s
epoch 118| loss: 0.59272 | val_0_rmse: 0.76507 | val_1_rmse: 0.80806 |  0:00:37s
epoch 119| loss: 0.59079 | val_0_rmse: 0.75771 | val_1_rmse: 0.80166 |  0:00:37s
epoch 120| loss: 0.57897 | val_0_rmse: 0.76478 | val_1_rmse: 0.81005 |  0:00:38s
epoch 121| loss: 0.57448 | val_0_rmse: 0.75265 | val_1_rmse: 0.79408 |  0:00:38s
epoch 122| loss: 0.57211 | val_0_rmse: 0.75447 | val_1_rmse: 0.79523 |  0:00:38s
epoch 123| loss: 0.57124 | val_0_rmse: 0.75019 | val_1_rmse: 0.79159 |  0:00:39s
epoch 124| loss: 0.57001 | val_0_rmse: 0.7486  | val_1_rmse: 0.79072 |  0:00:39s
epoch 125| loss: 0.56993 | val_0_rmse: 0.74732 | val_1_rmse: 0.79207 |  0:00:39s
epoch 126| loss: 0.57463 | val_0_rmse: 0.74397 | val_1_rmse: 0.79374 |  0:00:39s
epoch 127| loss: 0.56608 | val_0_rmse: 0.74189 | val_1_rmse: 0.79169 |  0:00:40s
epoch 128| loss: 0.56481 | val_0_rmse: 0.74053 | val_1_rmse: 0.78967 |  0:00:40s
epoch 129| loss: 0.55824 | val_0_rmse: 0.7433  | val_1_rmse: 0.78959 |  0:00:40s
epoch 130| loss: 0.55509 | val_0_rmse: 0.73746 | val_1_rmse: 0.79051 |  0:00:41s
epoch 131| loss: 0.56131 | val_0_rmse: 0.7378  | val_1_rmse: 0.79107 |  0:00:41s
epoch 132| loss: 0.5543  | val_0_rmse: 0.73838 | val_1_rmse: 0.79017 |  0:00:41s
epoch 133| loss: 0.55809 | val_0_rmse: 0.73601 | val_1_rmse: 0.78987 |  0:00:42s
epoch 134| loss: 0.5627  | val_0_rmse: 0.73495 | val_1_rmse: 0.79309 |  0:00:42s
epoch 135| loss: 0.55192 | val_0_rmse: 0.73356 | val_1_rmse: 0.79173 |  0:00:42s
epoch 136| loss: 0.55573 | val_0_rmse: 0.73392 | val_1_rmse: 0.79511 |  0:00:43s
epoch 137| loss: 0.55166 | val_0_rmse: 0.7335  | val_1_rmse: 0.79545 |  0:00:43s
epoch 138| loss: 0.54134 | val_0_rmse: 0.73349 | val_1_rmse: 0.79052 |  0:00:43s
epoch 139| loss: 0.54657 | val_0_rmse: 0.7303  | val_1_rmse: 0.79381 |  0:00:44s
epoch 140| loss: 0.54483 | val_0_rmse: 0.72812 | val_1_rmse: 0.79474 |  0:00:44s
epoch 141| loss: 0.54744 | val_0_rmse: 0.72716 | val_1_rmse: 0.78923 |  0:00:44s
epoch 142| loss: 0.55092 | val_0_rmse: 0.72906 | val_1_rmse: 0.79044 |  0:00:45s
epoch 143| loss: 0.54803 | val_0_rmse: 0.73392 | val_1_rmse: 0.79912 |  0:00:45s
epoch 144| loss: 0.54805 | val_0_rmse: 0.73066 | val_1_rmse: 0.7973  |  0:00:45s
epoch 145| loss: 0.54298 | val_0_rmse: 0.73772 | val_1_rmse: 0.80516 |  0:00:46s
epoch 146| loss: 0.55135 | val_0_rmse: 0.73215 | val_1_rmse: 0.79249 |  0:00:46s
epoch 147| loss: 0.54309 | val_0_rmse: 0.73027 | val_1_rmse: 0.79967 |  0:00:46s
epoch 148| loss: 0.54843 | val_0_rmse: 0.73143 | val_1_rmse: 0.79457 |  0:00:46s
epoch 149| loss: 0.54285 | val_0_rmse: 0.74181 | val_1_rmse: 0.80511 |  0:00:47s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.78923
Best weights from best epoch are automatically used!
ended training at: 05:06:01
Feature importance:
Mean squared error is of 0.036962025007617166
Mean absolute error:0.14324387738311564
MAPE:0.14480409813367054
R2 score:0.42314667440030707
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:06:02
epoch 0  | loss: 1.81457 | val_0_rmse: 0.99793 | val_1_rmse: 0.96387 |  0:00:00s
epoch 1  | loss: 1.06671 | val_0_rmse: 0.99531 | val_1_rmse: 0.96493 |  0:00:00s
epoch 2  | loss: 0.87914 | val_0_rmse: 0.82051 | val_1_rmse: 0.8081  |  0:00:00s
epoch 3  | loss: 0.74079 | val_0_rmse: 0.89348 | val_1_rmse: 0.87139 |  0:00:01s
epoch 4  | loss: 0.69314 | val_0_rmse: 0.8232  | val_1_rmse: 0.81288 |  0:00:01s
epoch 5  | loss: 0.66547 | val_0_rmse: 0.8166  | val_1_rmse: 0.79739 |  0:00:01s
epoch 6  | loss: 0.65864 | val_0_rmse: 0.80271 | val_1_rmse: 0.78495 |  0:00:02s
epoch 7  | loss: 0.65462 | val_0_rmse: 0.80902 | val_1_rmse: 0.79212 |  0:00:02s
epoch 8  | loss: 0.64327 | val_0_rmse: 0.80721 | val_1_rmse: 0.79169 |  0:00:02s
epoch 9  | loss: 0.64497 | val_0_rmse: 0.80461 | val_1_rmse: 0.78758 |  0:00:03s
epoch 10 | loss: 0.63535 | val_0_rmse: 0.80228 | val_1_rmse: 0.78217 |  0:00:03s
epoch 11 | loss: 0.63527 | val_0_rmse: 0.7987  | val_1_rmse: 0.7821  |  0:00:03s
epoch 12 | loss: 0.63325 | val_0_rmse: 0.79572 | val_1_rmse: 0.7787  |  0:00:04s
epoch 13 | loss: 0.63502 | val_0_rmse: 0.80774 | val_1_rmse: 0.79145 |  0:00:04s
epoch 14 | loss: 0.62241 | val_0_rmse: 0.79983 | val_1_rmse: 0.77755 |  0:00:04s
epoch 15 | loss: 0.61675 | val_0_rmse: 0.79823 | val_1_rmse: 0.7793  |  0:00:05s
epoch 16 | loss: 0.62019 | val_0_rmse: 0.79722 | val_1_rmse: 0.77683 |  0:00:05s
epoch 17 | loss: 0.61244 | val_0_rmse: 0.79577 | val_1_rmse: 0.77805 |  0:00:05s
epoch 18 | loss: 0.61052 | val_0_rmse: 0.79505 | val_1_rmse: 0.77919 |  0:00:05s
epoch 19 | loss: 0.60419 | val_0_rmse: 0.7916  | val_1_rmse: 0.77866 |  0:00:06s
epoch 20 | loss: 0.59894 | val_0_rmse: 0.79167 | val_1_rmse: 0.78005 |  0:00:06s
epoch 21 | loss: 0.59895 | val_0_rmse: 0.7889  | val_1_rmse: 0.78261 |  0:00:07s
epoch 22 | loss: 0.59765 | val_0_rmse: 0.78861 | val_1_rmse: 0.78196 |  0:00:07s
epoch 23 | loss: 0.59672 | val_0_rmse: 0.7876  | val_1_rmse: 0.77617 |  0:00:07s
epoch 24 | loss: 0.59063 | val_0_rmse: 0.78487 | val_1_rmse: 0.77208 |  0:00:07s
epoch 25 | loss: 0.58298 | val_0_rmse: 0.78632 | val_1_rmse: 0.77276 |  0:00:08s
epoch 26 | loss: 0.57661 | val_0_rmse: 0.78978 | val_1_rmse: 0.77966 |  0:00:08s
epoch 27 | loss: 0.58766 | val_0_rmse: 0.79351 | val_1_rmse: 0.78317 |  0:00:08s
epoch 28 | loss: 0.5837  | val_0_rmse: 0.787   | val_1_rmse: 0.77485 |  0:00:09s
epoch 29 | loss: 0.58039 | val_0_rmse: 0.78931 | val_1_rmse: 0.77769 |  0:00:09s
epoch 30 | loss: 0.57726 | val_0_rmse: 0.78699 | val_1_rmse: 0.77486 |  0:00:09s
epoch 31 | loss: 0.58024 | val_0_rmse: 0.78728 | val_1_rmse: 0.77758 |  0:00:10s
epoch 32 | loss: 0.60254 | val_0_rmse: 0.79053 | val_1_rmse: 0.78005 |  0:00:10s
epoch 33 | loss: 0.57897 | val_0_rmse: 0.79563 | val_1_rmse: 0.79184 |  0:00:10s
epoch 34 | loss: 0.58079 | val_0_rmse: 0.77735 | val_1_rmse: 0.77131 |  0:00:11s
epoch 35 | loss: 0.56759 | val_0_rmse: 0.77828 | val_1_rmse: 0.77516 |  0:00:11s
epoch 36 | loss: 0.56987 | val_0_rmse: 0.77862 | val_1_rmse: 0.77788 |  0:00:11s
epoch 37 | loss: 0.56615 | val_0_rmse: 0.77689 | val_1_rmse: 0.77844 |  0:00:12s
epoch 38 | loss: 0.57031 | val_0_rmse: 0.78812 | val_1_rmse: 0.78929 |  0:00:12s
epoch 39 | loss: 0.57538 | val_0_rmse: 0.76446 | val_1_rmse: 0.766   |  0:00:12s
epoch 40 | loss: 0.57528 | val_0_rmse: 0.76381 | val_1_rmse: 0.76705 |  0:00:12s
epoch 41 | loss: 0.56546 | val_0_rmse: 0.7702  | val_1_rmse: 0.77244 |  0:00:13s
epoch 42 | loss: 0.57124 | val_0_rmse: 0.76265 | val_1_rmse: 0.76648 |  0:00:13s
epoch 43 | loss: 0.55835 | val_0_rmse: 0.78099 | val_1_rmse: 0.78594 |  0:00:13s
epoch 44 | loss: 0.56195 | val_0_rmse: 0.76769 | val_1_rmse: 0.76916 |  0:00:14s
epoch 45 | loss: 0.54744 | val_0_rmse: 0.76457 | val_1_rmse: 0.76968 |  0:00:14s
epoch 46 | loss: 0.55718 | val_0_rmse: 0.76671 | val_1_rmse: 0.77586 |  0:00:14s
epoch 47 | loss: 0.5502  | val_0_rmse: 0.76402 | val_1_rmse: 0.77662 |  0:00:15s
epoch 48 | loss: 0.53994 | val_0_rmse: 0.76427 | val_1_rmse: 0.7814  |  0:00:15s
epoch 49 | loss: 0.53216 | val_0_rmse: 0.755   | val_1_rmse: 0.78332 |  0:00:15s
epoch 50 | loss: 0.53346 | val_0_rmse: 0.75225 | val_1_rmse: 0.77919 |  0:00:16s
epoch 51 | loss: 0.5406  | val_0_rmse: 0.77016 | val_1_rmse: 0.78965 |  0:00:16s
epoch 52 | loss: 0.53915 | val_0_rmse: 0.75668 | val_1_rmse: 0.77066 |  0:00:16s
epoch 53 | loss: 0.53945 | val_0_rmse: 0.75303 | val_1_rmse: 0.77302 |  0:00:16s
epoch 54 | loss: 0.53283 | val_0_rmse: 0.75184 | val_1_rmse: 0.77636 |  0:00:17s
epoch 55 | loss: 0.53586 | val_0_rmse: 0.75243 | val_1_rmse: 0.77302 |  0:00:17s
epoch 56 | loss: 0.5479  | val_0_rmse: 0.75563 | val_1_rmse: 0.7659  |  0:00:18s
epoch 57 | loss: 0.55349 | val_0_rmse: 0.75798 | val_1_rmse: 0.77278 |  0:00:18s
epoch 58 | loss: 0.54607 | val_0_rmse: 0.75436 | val_1_rmse: 0.77664 |  0:00:18s
epoch 59 | loss: 0.5427  | val_0_rmse: 0.74184 | val_1_rmse: 0.76509 |  0:00:18s
epoch 60 | loss: 0.53635 | val_0_rmse: 0.74681 | val_1_rmse: 0.76139 |  0:00:19s
epoch 61 | loss: 0.53753 | val_0_rmse: 0.74935 | val_1_rmse: 0.75907 |  0:00:19s
epoch 62 | loss: 0.54239 | val_0_rmse: 0.74808 | val_1_rmse: 0.76764 |  0:00:19s
epoch 63 | loss: 0.53084 | val_0_rmse: 0.74483 | val_1_rmse: 0.77327 |  0:00:20s
epoch 64 | loss: 0.52528 | val_0_rmse: 0.7427  | val_1_rmse: 0.76435 |  0:00:20s
epoch 65 | loss: 0.52651 | val_0_rmse: 0.74341 | val_1_rmse: 0.76005 |  0:00:20s
epoch 66 | loss: 0.52546 | val_0_rmse: 0.73713 | val_1_rmse: 0.75741 |  0:00:21s
epoch 67 | loss: 0.53188 | val_0_rmse: 0.72804 | val_1_rmse: 0.75811 |  0:00:21s
epoch 68 | loss: 0.53188 | val_0_rmse: 0.74094 | val_1_rmse: 0.76538 |  0:00:21s
epoch 69 | loss: 0.52405 | val_0_rmse: 0.72976 | val_1_rmse: 0.75212 |  0:00:22s
epoch 70 | loss: 0.5096  | val_0_rmse: 0.73131 | val_1_rmse: 0.76556 |  0:00:22s
epoch 71 | loss: 0.51841 | val_0_rmse: 0.72877 | val_1_rmse: 0.75855 |  0:00:22s
epoch 72 | loss: 0.51636 | val_0_rmse: 0.72481 | val_1_rmse: 0.75267 |  0:00:23s
epoch 73 | loss: 0.51531 | val_0_rmse: 0.72947 | val_1_rmse: 0.76135 |  0:00:23s
epoch 74 | loss: 0.51352 | val_0_rmse: 0.72507 | val_1_rmse: 0.76215 |  0:00:23s
epoch 75 | loss: 0.50741 | val_0_rmse: 0.73795 | val_1_rmse: 0.77977 |  0:00:23s
epoch 76 | loss: 0.49709 | val_0_rmse: 0.72225 | val_1_rmse: 0.75863 |  0:00:24s
epoch 77 | loss: 0.50204 | val_0_rmse: 0.70992 | val_1_rmse: 0.76196 |  0:00:24s
epoch 78 | loss: 0.4957  | val_0_rmse: 0.7078  | val_1_rmse: 0.76471 |  0:00:24s
epoch 79 | loss: 0.48864 | val_0_rmse: 0.70567 | val_1_rmse: 0.76526 |  0:00:25s
epoch 80 | loss: 0.4965  | val_0_rmse: 0.70591 | val_1_rmse: 0.75895 |  0:00:25s
epoch 81 | loss: 0.49654 | val_0_rmse: 0.7049  | val_1_rmse: 0.76488 |  0:00:25s
epoch 82 | loss: 0.48186 | val_0_rmse: 0.70734 | val_1_rmse: 0.77984 |  0:00:26s
epoch 83 | loss: 0.4916  | val_0_rmse: 0.70559 | val_1_rmse: 0.77771 |  0:00:26s
epoch 84 | loss: 0.49762 | val_0_rmse: 0.71019 | val_1_rmse: 0.77756 |  0:00:26s
epoch 85 | loss: 0.4971  | val_0_rmse: 0.71551 | val_1_rmse: 0.76301 |  0:00:27s
epoch 86 | loss: 0.50194 | val_0_rmse: 0.71196 | val_1_rmse: 0.77571 |  0:00:27s
epoch 87 | loss: 0.50206 | val_0_rmse: 0.70806 | val_1_rmse: 0.78071 |  0:00:27s
epoch 88 | loss: 0.50651 | val_0_rmse: 0.71005 | val_1_rmse: 0.767   |  0:00:28s
epoch 89 | loss: 0.49554 | val_0_rmse: 0.69873 | val_1_rmse: 0.77199 |  0:00:28s
epoch 90 | loss: 0.48607 | val_0_rmse: 0.70541 | val_1_rmse: 0.76754 |  0:00:28s
epoch 91 | loss: 0.48811 | val_0_rmse: 0.70091 | val_1_rmse: 0.77839 |  0:00:28s
epoch 92 | loss: 0.49266 | val_0_rmse: 0.69815 | val_1_rmse: 0.78142 |  0:00:29s
epoch 93 | loss: 0.47655 | val_0_rmse: 0.68629 | val_1_rmse: 0.77328 |  0:00:29s
epoch 94 | loss: 0.47051 | val_0_rmse: 0.68154 | val_1_rmse: 0.78028 |  0:00:29s
epoch 95 | loss: 0.47785 | val_0_rmse: 0.68193 | val_1_rmse: 0.77382 |  0:00:30s
epoch 96 | loss: 0.46977 | val_0_rmse: 0.68491 | val_1_rmse: 0.77863 |  0:00:30s
epoch 97 | loss: 0.47928 | val_0_rmse: 0.69309 | val_1_rmse: 0.78746 |  0:00:30s
epoch 98 | loss: 0.48769 | val_0_rmse: 0.68962 | val_1_rmse: 0.76966 |  0:00:31s
epoch 99 | loss: 0.47425 | val_0_rmse: 0.69702 | val_1_rmse: 0.77989 |  0:00:31s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.75212
Best weights from best epoch are automatically used!
ended training at: 05:06:33
Feature importance:
Mean squared error is of 0.042305804071428844
Mean absolute error:0.14604149663465818
MAPE:0.14922416976571645
R2 score:0.3940366875311355
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:06:34
epoch 0  | loss: 1.61964 | val_0_rmse: 1.00331 | val_1_rmse: 0.99564 |  0:00:00s
epoch 1  | loss: 0.97257 | val_0_rmse: 0.94944 | val_1_rmse: 0.94913 |  0:00:00s
epoch 2  | loss: 0.77809 | val_0_rmse: 0.82114 | val_1_rmse: 0.81414 |  0:00:00s
epoch 3  | loss: 0.72115 | val_0_rmse: 0.83474 | val_1_rmse: 0.80532 |  0:00:01s
epoch 4  | loss: 0.70065 | val_0_rmse: 0.84468 | val_1_rmse: 0.84022 |  0:00:01s
epoch 5  | loss: 0.68308 | val_0_rmse: 0.83272 | val_1_rmse: 0.81831 |  0:00:01s
epoch 6  | loss: 0.68444 | val_0_rmse: 0.81818 | val_1_rmse: 0.78996 |  0:00:02s
epoch 7  | loss: 0.66774 | val_0_rmse: 0.81406 | val_1_rmse: 0.79212 |  0:00:02s
epoch 8  | loss: 0.66654 | val_0_rmse: 0.81384 | val_1_rmse: 0.78656 |  0:00:02s
epoch 9  | loss: 0.6603  | val_0_rmse: 0.81368 | val_1_rmse: 0.78485 |  0:00:03s
epoch 10 | loss: 0.65    | val_0_rmse: 0.80805 | val_1_rmse: 0.77462 |  0:00:03s
epoch 11 | loss: 0.67693 | val_0_rmse: 0.81512 | val_1_rmse: 0.78232 |  0:00:03s
epoch 12 | loss: 0.65169 | val_0_rmse: 0.81115 | val_1_rmse: 0.78339 |  0:00:04s
epoch 13 | loss: 0.65061 | val_0_rmse: 0.81909 | val_1_rmse: 0.78791 |  0:00:04s
epoch 14 | loss: 0.65525 | val_0_rmse: 0.80905 | val_1_rmse: 0.78073 |  0:00:04s
epoch 15 | loss: 0.64462 | val_0_rmse: 0.81048 | val_1_rmse: 0.78511 |  0:00:05s
epoch 16 | loss: 0.64138 | val_0_rmse: 0.80988 | val_1_rmse: 0.78382 |  0:00:05s
epoch 17 | loss: 0.64609 | val_0_rmse: 0.80609 | val_1_rmse: 0.78061 |  0:00:05s
epoch 18 | loss: 0.63295 | val_0_rmse: 0.81288 | val_1_rmse: 0.78632 |  0:00:05s
epoch 19 | loss: 0.63322 | val_0_rmse: 0.80389 | val_1_rmse: 0.78352 |  0:00:06s
epoch 20 | loss: 0.63854 | val_0_rmse: 0.80058 | val_1_rmse: 0.78259 |  0:00:06s
epoch 21 | loss: 0.63609 | val_0_rmse: 0.81121 | val_1_rmse: 0.78706 |  0:00:06s
epoch 22 | loss: 0.63904 | val_0_rmse: 0.80185 | val_1_rmse: 0.78512 |  0:00:07s
epoch 23 | loss: 0.63066 | val_0_rmse: 0.80894 | val_1_rmse: 0.79195 |  0:00:07s
epoch 24 | loss: 0.63332 | val_0_rmse: 0.81331 | val_1_rmse: 0.7943  |  0:00:07s
epoch 25 | loss: 0.62434 | val_0_rmse: 0.81196 | val_1_rmse: 0.78909 |  0:00:08s
epoch 26 | loss: 0.62962 | val_0_rmse: 0.83015 | val_1_rmse: 0.79856 |  0:00:08s
epoch 27 | loss: 0.62054 | val_0_rmse: 0.82206 | val_1_rmse: 0.78946 |  0:00:08s
epoch 28 | loss: 0.61954 | val_0_rmse: 0.80795 | val_1_rmse: 0.77492 |  0:00:09s
epoch 29 | loss: 0.61982 | val_0_rmse: 0.83491 | val_1_rmse: 0.80053 |  0:00:09s
epoch 30 | loss: 0.62091 | val_0_rmse: 0.80742 | val_1_rmse: 0.77877 |  0:00:09s
epoch 31 | loss: 0.61916 | val_0_rmse: 0.8184  | val_1_rmse: 0.78821 |  0:00:10s
epoch 32 | loss: 0.61642 | val_0_rmse: 0.8208  | val_1_rmse: 0.79062 |  0:00:10s
epoch 33 | loss: 0.61718 | val_0_rmse: 0.8112  | val_1_rmse: 0.78367 |  0:00:10s
epoch 34 | loss: 0.61816 | val_0_rmse: 0.81564 | val_1_rmse: 0.79036 |  0:00:11s
epoch 35 | loss: 0.60409 | val_0_rmse: 0.80079 | val_1_rmse: 0.77869 |  0:00:11s
epoch 36 | loss: 0.59116 | val_0_rmse: 0.80656 | val_1_rmse: 0.78079 |  0:00:11s
epoch 37 | loss: 0.59218 | val_0_rmse: 0.80022 | val_1_rmse: 0.77822 |  0:00:11s
epoch 38 | loss: 0.59282 | val_0_rmse: 0.80024 | val_1_rmse: 0.77536 |  0:00:12s
epoch 39 | loss: 0.59498 | val_0_rmse: 0.78784 | val_1_rmse: 0.77312 |  0:00:12s
epoch 40 | loss: 0.57928 | val_0_rmse: 0.79907 | val_1_rmse: 0.77714 |  0:00:12s
epoch 41 | loss: 0.57932 | val_0_rmse: 0.7847  | val_1_rmse: 0.76782 |  0:00:13s
epoch 42 | loss: 0.57572 | val_0_rmse: 0.78757 | val_1_rmse: 0.76265 |  0:00:13s
epoch 43 | loss: 0.57488 | val_0_rmse: 0.77615 | val_1_rmse: 0.7568  |  0:00:13s
epoch 44 | loss: 0.57924 | val_0_rmse: 0.78221 | val_1_rmse: 0.76202 |  0:00:14s
epoch 45 | loss: 0.58002 | val_0_rmse: 0.77652 | val_1_rmse: 0.75985 |  0:00:14s
epoch 46 | loss: 0.58047 | val_0_rmse: 0.77996 | val_1_rmse: 0.76237 |  0:00:14s
epoch 47 | loss: 0.57859 | val_0_rmse: 0.7778  | val_1_rmse: 0.76172 |  0:00:15s
epoch 48 | loss: 0.57868 | val_0_rmse: 0.78048 | val_1_rmse: 0.76393 |  0:00:15s
epoch 49 | loss: 0.56019 | val_0_rmse: 0.78738 | val_1_rmse: 0.76849 |  0:00:15s
epoch 50 | loss: 0.56205 | val_0_rmse: 0.77206 | val_1_rmse: 0.75432 |  0:00:16s
epoch 51 | loss: 0.55047 | val_0_rmse: 0.77704 | val_1_rmse: 0.75739 |  0:00:16s
epoch 52 | loss: 0.55524 | val_0_rmse: 0.76723 | val_1_rmse: 0.755   |  0:00:16s
epoch 53 | loss: 0.55514 | val_0_rmse: 0.77486 | val_1_rmse: 0.76079 |  0:00:16s
epoch 54 | loss: 0.55623 | val_0_rmse: 0.77187 | val_1_rmse: 0.7579  |  0:00:17s
epoch 55 | loss: 0.55138 | val_0_rmse: 0.77029 | val_1_rmse: 0.7566  |  0:00:17s
epoch 56 | loss: 0.5506  | val_0_rmse: 0.76467 | val_1_rmse: 0.75529 |  0:00:17s
epoch 57 | loss: 0.55509 | val_0_rmse: 0.76381 | val_1_rmse: 0.75749 |  0:00:18s
epoch 58 | loss: 0.54661 | val_0_rmse: 0.75945 | val_1_rmse: 0.7563  |  0:00:18s
epoch 59 | loss: 0.5456  | val_0_rmse: 0.76389 | val_1_rmse: 0.76304 |  0:00:18s
epoch 60 | loss: 0.53858 | val_0_rmse: 0.75596 | val_1_rmse: 0.75659 |  0:00:19s
epoch 61 | loss: 0.54213 | val_0_rmse: 0.75969 | val_1_rmse: 0.7596  |  0:00:19s
epoch 62 | loss: 0.54871 | val_0_rmse: 0.75385 | val_1_rmse: 0.75405 |  0:00:19s
epoch 63 | loss: 0.535   | val_0_rmse: 0.75235 | val_1_rmse: 0.75554 |  0:00:20s
epoch 64 | loss: 0.54626 | val_0_rmse: 0.75695 | val_1_rmse: 0.75801 |  0:00:20s
epoch 65 | loss: 0.53614 | val_0_rmse: 0.75112 | val_1_rmse: 0.75575 |  0:00:20s
epoch 66 | loss: 0.53544 | val_0_rmse: 0.75025 | val_1_rmse: 0.75878 |  0:00:21s
epoch 67 | loss: 0.53371 | val_0_rmse: 0.74662 | val_1_rmse: 0.75314 |  0:00:21s
epoch 68 | loss: 0.53366 | val_0_rmse: 0.74539 | val_1_rmse: 0.75317 |  0:00:21s
epoch 69 | loss: 0.53783 | val_0_rmse: 0.75716 | val_1_rmse: 0.75858 |  0:00:22s
epoch 70 | loss: 0.53452 | val_0_rmse: 0.75054 | val_1_rmse: 0.76215 |  0:00:22s
epoch 71 | loss: 0.52569 | val_0_rmse: 0.75123 | val_1_rmse: 0.75957 |  0:00:22s
epoch 72 | loss: 0.52675 | val_0_rmse: 0.74678 | val_1_rmse: 0.75831 |  0:00:23s
epoch 73 | loss: 0.52346 | val_0_rmse: 0.73895 | val_1_rmse: 0.75285 |  0:00:23s
epoch 74 | loss: 0.52294 | val_0_rmse: 0.74874 | val_1_rmse: 0.75299 |  0:00:23s
epoch 75 | loss: 0.52718 | val_0_rmse: 0.75013 | val_1_rmse: 0.7545  |  0:00:23s
epoch 76 | loss: 0.53148 | val_0_rmse: 0.75785 | val_1_rmse: 0.75978 |  0:00:24s
epoch 77 | loss: 0.53165 | val_0_rmse: 0.73958 | val_1_rmse: 0.75247 |  0:00:24s
epoch 78 | loss: 0.52629 | val_0_rmse: 0.73134 | val_1_rmse: 0.75468 |  0:00:24s
epoch 79 | loss: 0.51707 | val_0_rmse: 0.72696 | val_1_rmse: 0.7537  |  0:00:25s
epoch 80 | loss: 0.51436 | val_0_rmse: 0.73129 | val_1_rmse: 0.76209 |  0:00:25s
epoch 81 | loss: 0.51234 | val_0_rmse: 0.73188 | val_1_rmse: 0.75891 |  0:00:25s
epoch 82 | loss: 0.51104 | val_0_rmse: 0.7293  | val_1_rmse: 0.75498 |  0:00:26s
epoch 83 | loss: 0.51331 | val_0_rmse: 0.72497 | val_1_rmse: 0.75092 |  0:00:26s
epoch 84 | loss: 0.51049 | val_0_rmse: 0.72571 | val_1_rmse: 0.75179 |  0:00:26s
epoch 85 | loss: 0.51011 | val_0_rmse: 0.72499 | val_1_rmse: 0.75374 |  0:00:27s
epoch 86 | loss: 0.52005 | val_0_rmse: 0.72286 | val_1_rmse: 0.75175 |  0:00:27s
epoch 87 | loss: 0.506   | val_0_rmse: 0.72054 | val_1_rmse: 0.75194 |  0:00:27s
epoch 88 | loss: 0.50508 | val_0_rmse: 0.71402 | val_1_rmse: 0.75229 |  0:00:28s
epoch 89 | loss: 0.49642 | val_0_rmse: 0.71832 | val_1_rmse: 0.75578 |  0:00:28s
epoch 90 | loss: 0.51035 | val_0_rmse: 0.71865 | val_1_rmse: 0.74564 |  0:00:28s
epoch 91 | loss: 0.49817 | val_0_rmse: 0.71288 | val_1_rmse: 0.74975 |  0:00:28s
epoch 92 | loss: 0.50825 | val_0_rmse: 0.70992 | val_1_rmse: 0.75526 |  0:00:29s
epoch 93 | loss: 0.50206 | val_0_rmse: 0.7001  | val_1_rmse: 0.75329 |  0:00:29s
epoch 94 | loss: 0.51437 | val_0_rmse: 0.70936 | val_1_rmse: 0.74994 |  0:00:29s
epoch 95 | loss: 0.51519 | val_0_rmse: 0.71513 | val_1_rmse: 0.74934 |  0:00:30s
epoch 96 | loss: 0.50405 | val_0_rmse: 0.70095 | val_1_rmse: 0.74404 |  0:00:30s
epoch 97 | loss: 0.50438 | val_0_rmse: 0.69069 | val_1_rmse: 0.74201 |  0:00:30s
epoch 98 | loss: 0.49742 | val_0_rmse: 0.69423 | val_1_rmse: 0.74573 |  0:00:31s
epoch 99 | loss: 0.51408 | val_0_rmse: 0.69909 | val_1_rmse: 0.75391 |  0:00:31s
epoch 100| loss: 0.49139 | val_0_rmse: 0.69399 | val_1_rmse: 0.75727 |  0:00:31s
epoch 101| loss: 0.47988 | val_0_rmse: 0.69277 | val_1_rmse: 0.75429 |  0:00:32s
epoch 102| loss: 0.49511 | val_0_rmse: 0.7093  | val_1_rmse: 0.7695  |  0:00:32s
epoch 103| loss: 0.5178  | val_0_rmse: 0.71931 | val_1_rmse: 0.7552  |  0:00:32s
epoch 104| loss: 0.54175 | val_0_rmse: 0.72473 | val_1_rmse: 0.76442 |  0:00:33s
epoch 105| loss: 0.52027 | val_0_rmse: 0.71341 | val_1_rmse: 0.76871 |  0:00:33s
epoch 106| loss: 0.5222  | val_0_rmse: 0.70578 | val_1_rmse: 0.74383 |  0:00:33s
epoch 107| loss: 0.50511 | val_0_rmse: 0.70375 | val_1_rmse: 0.7363  |  0:00:34s
epoch 108| loss: 0.49474 | val_0_rmse: 0.70119 | val_1_rmse: 0.73674 |  0:00:34s
epoch 109| loss: 0.49567 | val_0_rmse: 0.69052 | val_1_rmse: 0.73952 |  0:00:34s
epoch 110| loss: 0.49462 | val_0_rmse: 0.69334 | val_1_rmse: 0.75112 |  0:00:35s
epoch 111| loss: 0.49649 | val_0_rmse: 0.69136 | val_1_rmse: 0.74475 |  0:00:35s
epoch 112| loss: 0.49115 | val_0_rmse: 0.69302 | val_1_rmse: 0.75829 |  0:00:35s
epoch 113| loss: 0.48765 | val_0_rmse: 0.68884 | val_1_rmse: 0.75207 |  0:00:35s
epoch 114| loss: 0.4842  | val_0_rmse: 0.69487 | val_1_rmse: 0.7557  |  0:00:36s
epoch 115| loss: 0.48778 | val_0_rmse: 0.68391 | val_1_rmse: 0.75592 |  0:00:36s
epoch 116| loss: 0.48188 | val_0_rmse: 0.67828 | val_1_rmse: 0.75716 |  0:00:36s
epoch 117| loss: 0.47555 | val_0_rmse: 0.67467 | val_1_rmse: 0.75985 |  0:00:37s
epoch 118| loss: 0.46357 | val_0_rmse: 0.67655 | val_1_rmse: 0.76096 |  0:00:37s
epoch 119| loss: 0.45433 | val_0_rmse: 0.66941 | val_1_rmse: 0.76286 |  0:00:37s
epoch 120| loss: 0.45904 | val_0_rmse: 0.66233 | val_1_rmse: 0.7559  |  0:00:38s
epoch 121| loss: 0.46244 | val_0_rmse: 0.66946 | val_1_rmse: 0.7686  |  0:00:38s
epoch 122| loss: 0.46919 | val_0_rmse: 0.68526 | val_1_rmse: 0.78015 |  0:00:38s
epoch 123| loss: 0.47417 | val_0_rmse: 0.67638 | val_1_rmse: 0.76021 |  0:00:39s
epoch 124| loss: 0.47719 | val_0_rmse: 0.66925 | val_1_rmse: 0.7593  |  0:00:39s
epoch 125| loss: 0.46481 | val_0_rmse: 0.68631 | val_1_rmse: 0.76096 |  0:00:39s
epoch 126| loss: 0.483   | val_0_rmse: 0.6901  | val_1_rmse: 0.76987 |  0:00:39s
epoch 127| loss: 0.48465 | val_0_rmse: 0.6794  | val_1_rmse: 0.77324 |  0:00:40s
epoch 128| loss: 0.48388 | val_0_rmse: 0.68449 | val_1_rmse: 0.77397 |  0:00:40s
epoch 129| loss: 0.4773  | val_0_rmse: 0.67891 | val_1_rmse: 0.78068 |  0:00:40s
epoch 130| loss: 0.47406 | val_0_rmse: 0.67535 | val_1_rmse: 0.76844 |  0:00:41s
epoch 131| loss: 0.47362 | val_0_rmse: 0.66953 | val_1_rmse: 0.76663 |  0:00:41s
epoch 132| loss: 0.47181 | val_0_rmse: 0.66387 | val_1_rmse: 0.76685 |  0:00:41s
epoch 133| loss: 0.47592 | val_0_rmse: 0.6685  | val_1_rmse: 0.772   |  0:00:42s
epoch 134| loss: 0.46232 | val_0_rmse: 0.66402 | val_1_rmse: 0.77263 |  0:00:42s
epoch 135| loss: 0.45893 | val_0_rmse: 0.66137 | val_1_rmse: 0.7814  |  0:00:42s
epoch 136| loss: 0.46578 | val_0_rmse: 0.66789 | val_1_rmse: 0.77196 |  0:00:43s
epoch 137| loss: 0.46469 | val_0_rmse: 0.6633  | val_1_rmse: 0.76155 |  0:00:43s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.7363
Best weights from best epoch are automatically used!
ended training at: 05:07:18
Feature importance:
Mean squared error is of 0.03770518585859295
Mean absolute error:0.14608683334051265
MAPE:0.1521364341328738
R2 score:0.43407760749637225
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:07:18
epoch 0  | loss: 1.66684 | val_0_rmse: 0.98602 | val_1_rmse: 0.98696 |  0:00:00s
epoch 1  | loss: 1.00945 | val_0_rmse: 0.94792 | val_1_rmse: 0.95618 |  0:00:00s
epoch 2  | loss: 0.82134 | val_0_rmse: 0.84179 | val_1_rmse: 0.86811 |  0:00:00s
epoch 3  | loss: 0.7332  | val_0_rmse: 0.85433 | val_1_rmse: 0.88956 |  0:00:01s
epoch 4  | loss: 0.68657 | val_0_rmse: 0.86487 | val_1_rmse: 0.90115 |  0:00:01s
epoch 5  | loss: 0.68266 | val_0_rmse: 0.81902 | val_1_rmse: 0.85729 |  0:00:01s
epoch 6  | loss: 0.67307 | val_0_rmse: 0.85657 | val_1_rmse: 0.89381 |  0:00:02s
epoch 7  | loss: 0.65992 | val_0_rmse: 0.79411 | val_1_rmse: 0.82996 |  0:00:02s
epoch 8  | loss: 0.66416 | val_0_rmse: 0.79259 | val_1_rmse: 0.82735 |  0:00:02s
epoch 9  | loss: 0.65697 | val_0_rmse: 0.79991 | val_1_rmse: 0.83324 |  0:00:03s
epoch 10 | loss: 0.64613 | val_0_rmse: 0.79108 | val_1_rmse: 0.82087 |  0:00:03s
epoch 11 | loss: 0.64374 | val_0_rmse: 0.79691 | val_1_rmse: 0.82476 |  0:00:03s
epoch 12 | loss: 0.62948 | val_0_rmse: 0.80358 | val_1_rmse: 0.83309 |  0:00:04s
epoch 13 | loss: 0.63487 | val_0_rmse: 0.79155 | val_1_rmse: 0.81692 |  0:00:04s
epoch 14 | loss: 0.62771 | val_0_rmse: 0.80761 | val_1_rmse: 0.82937 |  0:00:04s
epoch 15 | loss: 0.62984 | val_0_rmse: 0.78384 | val_1_rmse: 0.8102  |  0:00:05s
epoch 16 | loss: 0.63057 | val_0_rmse: 0.78871 | val_1_rmse: 0.81313 |  0:00:05s
epoch 17 | loss: 0.62207 | val_0_rmse: 0.78659 | val_1_rmse: 0.81778 |  0:00:05s
epoch 18 | loss: 0.61416 | val_0_rmse: 0.78274 | val_1_rmse: 0.81582 |  0:00:05s
epoch 19 | loss: 0.59969 | val_0_rmse: 0.79186 | val_1_rmse: 0.8213  |  0:00:06s
epoch 20 | loss: 0.59948 | val_0_rmse: 0.78596 | val_1_rmse: 0.81917 |  0:00:06s
epoch 21 | loss: 0.58397 | val_0_rmse: 0.79307 | val_1_rmse: 0.82722 |  0:00:06s
epoch 22 | loss: 0.58058 | val_0_rmse: 0.78231 | val_1_rmse: 0.82445 |  0:00:07s
epoch 23 | loss: 0.56791 | val_0_rmse: 0.78828 | val_1_rmse: 0.82781 |  0:00:07s
epoch 24 | loss: 0.56304 | val_0_rmse: 0.77823 | val_1_rmse: 0.80938 |  0:00:07s
epoch 25 | loss: 0.562   | val_0_rmse: 0.78221 | val_1_rmse: 0.80895 |  0:00:08s
epoch 26 | loss: 0.56225 | val_0_rmse: 0.78038 | val_1_rmse: 0.81216 |  0:00:08s
epoch 27 | loss: 0.55873 | val_0_rmse: 0.77428 | val_1_rmse: 0.80849 |  0:00:08s
epoch 28 | loss: 0.55919 | val_0_rmse: 0.79658 | val_1_rmse: 0.81987 |  0:00:09s
epoch 29 | loss: 0.55542 | val_0_rmse: 0.77709 | val_1_rmse: 0.81204 |  0:00:09s
epoch 30 | loss: 0.54731 | val_0_rmse: 0.79541 | val_1_rmse: 0.83009 |  0:00:09s
epoch 31 | loss: 0.54311 | val_0_rmse: 0.76923 | val_1_rmse: 0.80934 |  0:00:10s
epoch 32 | loss: 0.53643 | val_0_rmse: 0.77458 | val_1_rmse: 0.81443 |  0:00:10s
epoch 33 | loss: 0.54705 | val_0_rmse: 0.76232 | val_1_rmse: 0.81093 |  0:00:10s
epoch 34 | loss: 0.53916 | val_0_rmse: 0.75945 | val_1_rmse: 0.81098 |  0:00:11s
epoch 35 | loss: 0.55476 | val_0_rmse: 0.76942 | val_1_rmse: 0.82312 |  0:00:11s
epoch 36 | loss: 0.54061 | val_0_rmse: 0.78576 | val_1_rmse: 0.83904 |  0:00:11s
epoch 37 | loss: 0.54583 | val_0_rmse: 0.80112 | val_1_rmse: 0.84688 |  0:00:11s
epoch 38 | loss: 0.53891 | val_0_rmse: 0.76494 | val_1_rmse: 0.81539 |  0:00:12s
epoch 39 | loss: 0.55278 | val_0_rmse: 0.774   | val_1_rmse: 0.82063 |  0:00:12s
epoch 40 | loss: 0.53912 | val_0_rmse: 0.77534 | val_1_rmse: 0.83707 |  0:00:12s
epoch 41 | loss: 0.54686 | val_0_rmse: 0.76956 | val_1_rmse: 0.82598 |  0:00:13s
epoch 42 | loss: 0.53744 | val_0_rmse: 0.76568 | val_1_rmse: 0.80908 |  0:00:13s
epoch 43 | loss: 0.52668 | val_0_rmse: 0.77386 | val_1_rmse: 0.81875 |  0:00:13s
epoch 44 | loss: 0.53304 | val_0_rmse: 0.77424 | val_1_rmse: 0.84322 |  0:00:14s
epoch 45 | loss: 0.54372 | val_0_rmse: 0.75319 | val_1_rmse: 0.80509 |  0:00:14s
epoch 46 | loss: 0.53451 | val_0_rmse: 0.74734 | val_1_rmse: 0.80836 |  0:00:14s
epoch 47 | loss: 0.52828 | val_0_rmse: 0.75898 | val_1_rmse: 0.82549 |  0:00:15s
epoch 48 | loss: 0.51991 | val_0_rmse: 0.76133 | val_1_rmse: 0.81692 |  0:00:15s
epoch 49 | loss: 0.52208 | val_0_rmse: 0.75865 | val_1_rmse: 0.80762 |  0:00:15s
epoch 50 | loss: 0.51728 | val_0_rmse: 0.76243 | val_1_rmse: 0.81597 |  0:00:16s
epoch 51 | loss: 0.51865 | val_0_rmse: 0.74557 | val_1_rmse: 0.80757 |  0:00:16s
epoch 52 | loss: 0.50931 | val_0_rmse: 0.74347 | val_1_rmse: 0.80608 |  0:00:16s
epoch 53 | loss: 0.51278 | val_0_rmse: 0.75344 | val_1_rmse: 0.82192 |  0:00:16s
epoch 54 | loss: 0.53156 | val_0_rmse: 0.75592 | val_1_rmse: 0.81753 |  0:00:17s
epoch 55 | loss: 0.53509 | val_0_rmse: 0.75278 | val_1_rmse: 0.81847 |  0:00:17s
epoch 56 | loss: 0.53169 | val_0_rmse: 0.75405 | val_1_rmse: 0.82574 |  0:00:17s
epoch 57 | loss: 0.53015 | val_0_rmse: 0.74359 | val_1_rmse: 0.82436 |  0:00:18s
epoch 58 | loss: 0.51703 | val_0_rmse: 0.75624 | val_1_rmse: 0.81519 |  0:00:18s
epoch 59 | loss: 0.52185 | val_0_rmse: 0.74412 | val_1_rmse: 0.80503 |  0:00:18s
epoch 60 | loss: 0.51823 | val_0_rmse: 0.73592 | val_1_rmse: 0.80931 |  0:00:19s
epoch 61 | loss: 0.50306 | val_0_rmse: 0.75945 | val_1_rmse: 0.8198  |  0:00:19s
epoch 62 | loss: 0.50347 | val_0_rmse: 0.74314 | val_1_rmse: 0.80085 |  0:00:19s
epoch 63 | loss: 0.5028  | val_0_rmse: 0.75372 | val_1_rmse: 0.81568 |  0:00:20s
epoch 64 | loss: 0.50218 | val_0_rmse: 0.7412  | val_1_rmse: 0.8129  |  0:00:20s
epoch 65 | loss: 0.50317 | val_0_rmse: 0.73003 | val_1_rmse: 0.80813 |  0:00:20s
epoch 66 | loss: 0.50259 | val_0_rmse: 0.7322  | val_1_rmse: 0.81954 |  0:00:21s
epoch 67 | loss: 0.50141 | val_0_rmse: 0.71887 | val_1_rmse: 0.80675 |  0:00:21s
epoch 68 | loss: 0.50003 | val_0_rmse: 0.72956 | val_1_rmse: 0.7958  |  0:00:21s
epoch 69 | loss: 0.50932 | val_0_rmse: 0.73566 | val_1_rmse: 0.82004 |  0:00:21s
epoch 70 | loss: 0.50905 | val_0_rmse: 0.73759 | val_1_rmse: 0.82888 |  0:00:22s
epoch 71 | loss: 0.50895 | val_0_rmse: 0.73211 | val_1_rmse: 0.81243 |  0:00:22s
epoch 72 | loss: 0.51854 | val_0_rmse: 0.73745 | val_1_rmse: 0.80513 |  0:00:23s
epoch 73 | loss: 0.51271 | val_0_rmse: 0.73039 | val_1_rmse: 0.80265 |  0:00:23s
epoch 74 | loss: 0.50094 | val_0_rmse: 0.72107 | val_1_rmse: 0.80702 |  0:00:23s
epoch 75 | loss: 0.49779 | val_0_rmse: 0.72    | val_1_rmse: 0.81131 |  0:00:23s
epoch 76 | loss: 0.48942 | val_0_rmse: 0.71742 | val_1_rmse: 0.80656 |  0:00:24s
epoch 77 | loss: 0.50185 | val_0_rmse: 0.71855 | val_1_rmse: 0.80653 |  0:00:24s
epoch 78 | loss: 0.49387 | val_0_rmse: 0.71279 | val_1_rmse: 0.81135 |  0:00:24s
epoch 79 | loss: 0.48549 | val_0_rmse: 0.71373 | val_1_rmse: 0.81629 |  0:00:25s
epoch 80 | loss: 0.48653 | val_0_rmse: 0.70714 | val_1_rmse: 0.80987 |  0:00:25s
epoch 81 | loss: 0.48278 | val_0_rmse: 0.70333 | val_1_rmse: 0.81052 |  0:00:25s
epoch 82 | loss: 0.48033 | val_0_rmse: 0.7003  | val_1_rmse: 0.81426 |  0:00:26s
epoch 83 | loss: 0.48098 | val_0_rmse: 0.70659 | val_1_rmse: 0.79906 |  0:00:26s
epoch 84 | loss: 0.48093 | val_0_rmse: 0.70611 | val_1_rmse: 0.8061  |  0:00:26s
epoch 85 | loss: 0.49241 | val_0_rmse: 0.70781 | val_1_rmse: 0.81333 |  0:00:27s
epoch 86 | loss: 0.49067 | val_0_rmse: 0.70283 | val_1_rmse: 0.80927 |  0:00:27s
epoch 87 | loss: 0.48786 | val_0_rmse: 0.70505 | val_1_rmse: 0.82278 |  0:00:27s
epoch 88 | loss: 0.48646 | val_0_rmse: 0.70294 | val_1_rmse: 0.81187 |  0:00:27s
epoch 89 | loss: 0.47932 | val_0_rmse: 0.69075 | val_1_rmse: 0.80252 |  0:00:28s
epoch 90 | loss: 0.47534 | val_0_rmse: 0.69053 | val_1_rmse: 0.80285 |  0:00:28s
epoch 91 | loss: 0.47274 | val_0_rmse: 0.69244 | val_1_rmse: 0.81453 |  0:00:28s
epoch 92 | loss: 0.47159 | val_0_rmse: 0.68894 | val_1_rmse: 0.80792 |  0:00:29s
epoch 93 | loss: 0.4811  | val_0_rmse: 0.69056 | val_1_rmse: 0.80414 |  0:00:29s
epoch 94 | loss: 0.46925 | val_0_rmse: 0.68212 | val_1_rmse: 0.81474 |  0:00:29s
epoch 95 | loss: 0.4676  | val_0_rmse: 0.6813  | val_1_rmse: 0.8139  |  0:00:30s
epoch 96 | loss: 0.46849 | val_0_rmse: 0.67737 | val_1_rmse: 0.80509 |  0:00:30s
epoch 97 | loss: 0.46656 | val_0_rmse: 0.68335 | val_1_rmse: 0.81456 |  0:00:30s
epoch 98 | loss: 0.47618 | val_0_rmse: 0.6883  | val_1_rmse: 0.82731 |  0:00:31s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.7958
Best weights from best epoch are automatically used!
ended training at: 05:07:49
Feature importance:
Mean squared error is of 0.044240461075962495
Mean absolute error:0.1496107892176251
MAPE:0.15852473335779654
R2 score:0.3775304448133373
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:07:50
epoch 0  | loss: 1.61976 | val_0_rmse: 0.99114 | val_1_rmse: 0.98467 |  0:00:00s
epoch 1  | loss: 1.17205 | val_0_rmse: 0.98475 | val_1_rmse: 0.97375 |  0:00:00s
epoch 2  | loss: 0.94287 | val_0_rmse: 0.94748 | val_1_rmse: 0.94187 |  0:00:00s
epoch 3  | loss: 0.79639 | val_0_rmse: 0.90578 | val_1_rmse: 0.91056 |  0:00:01s
epoch 4  | loss: 0.73146 | val_0_rmse: 0.92915 | val_1_rmse: 0.92654 |  0:00:01s
epoch 5  | loss: 0.69197 | val_0_rmse: 0.86565 | val_1_rmse: 0.89429 |  0:00:02s
epoch 6  | loss: 0.68882 | val_0_rmse: 0.82294 | val_1_rmse: 0.84335 |  0:00:02s
epoch 7  | loss: 0.66074 | val_0_rmse: 0.83472 | val_1_rmse: 0.86536 |  0:00:02s
epoch 8  | loss: 0.65579 | val_0_rmse: 0.79553 | val_1_rmse: 0.81651 |  0:00:02s
epoch 9  | loss: 0.65596 | val_0_rmse: 0.80262 | val_1_rmse: 0.827   |  0:00:03s
epoch 10 | loss: 0.64894 | val_0_rmse: 0.81178 | val_1_rmse: 0.8404  |  0:00:03s
epoch 11 | loss: 0.6491  | val_0_rmse: 0.79896 | val_1_rmse: 0.82462 |  0:00:03s
epoch 12 | loss: 0.65079 | val_0_rmse: 0.79231 | val_1_rmse: 0.81633 |  0:00:04s
epoch 13 | loss: 0.63749 | val_0_rmse: 0.79258 | val_1_rmse: 0.81543 |  0:00:04s
epoch 14 | loss: 0.6413  | val_0_rmse: 0.80503 | val_1_rmse: 0.81243 |  0:00:04s
epoch 15 | loss: 0.63133 | val_0_rmse: 0.79361 | val_1_rmse: 0.81356 |  0:00:05s
epoch 16 | loss: 0.63747 | val_0_rmse: 0.80764 | val_1_rmse: 0.82803 |  0:00:05s
epoch 17 | loss: 0.63169 | val_0_rmse: 0.80287 | val_1_rmse: 0.82548 |  0:00:05s
epoch 18 | loss: 0.62096 | val_0_rmse: 0.81569 | val_1_rmse: 0.83608 |  0:00:06s
epoch 19 | loss: 0.61295 | val_0_rmse: 0.80444 | val_1_rmse: 0.82445 |  0:00:06s
epoch 20 | loss: 0.61274 | val_0_rmse: 0.82862 | val_1_rmse: 0.83537 |  0:00:06s
epoch 21 | loss: 0.6154  | val_0_rmse: 0.80776 | val_1_rmse: 0.81959 |  0:00:07s
epoch 22 | loss: 0.6068  | val_0_rmse: 0.81763 | val_1_rmse: 0.82979 |  0:00:07s
epoch 23 | loss: 0.61626 | val_0_rmse: 0.79509 | val_1_rmse: 0.81227 |  0:00:07s
epoch 24 | loss: 0.59358 | val_0_rmse: 0.79484 | val_1_rmse: 0.82963 |  0:00:07s
epoch 25 | loss: 0.59989 | val_0_rmse: 0.7926  | val_1_rmse: 0.81547 |  0:00:08s
epoch 26 | loss: 0.60814 | val_0_rmse: 0.78801 | val_1_rmse: 0.80654 |  0:00:08s
epoch 27 | loss: 0.60866 | val_0_rmse: 0.79105 | val_1_rmse: 0.81589 |  0:00:08s
epoch 28 | loss: 0.60642 | val_0_rmse: 0.78603 | val_1_rmse: 0.81103 |  0:00:09s
epoch 29 | loss: 0.608   | val_0_rmse: 0.79156 | val_1_rmse: 0.81617 |  0:00:09s
epoch 30 | loss: 0.6006  | val_0_rmse: 0.78559 | val_1_rmse: 0.81523 |  0:00:09s
epoch 31 | loss: 0.59772 | val_0_rmse: 0.78854 | val_1_rmse: 0.8162  |  0:00:10s
epoch 32 | loss: 0.58948 | val_0_rmse: 0.7846  | val_1_rmse: 0.81292 |  0:00:10s
epoch 33 | loss: 0.59116 | val_0_rmse: 0.78236 | val_1_rmse: 0.81194 |  0:00:10s
epoch 34 | loss: 0.59677 | val_0_rmse: 0.78486 | val_1_rmse: 0.81388 |  0:00:11s
epoch 35 | loss: 0.59964 | val_0_rmse: 0.78027 | val_1_rmse: 0.8087  |  0:00:11s
epoch 36 | loss: 0.59389 | val_0_rmse: 0.79684 | val_1_rmse: 0.82516 |  0:00:11s
epoch 37 | loss: 0.58707 | val_0_rmse: 0.78028 | val_1_rmse: 0.81126 |  0:00:12s
epoch 38 | loss: 0.58474 | val_0_rmse: 0.78017 | val_1_rmse: 0.81228 |  0:00:12s
epoch 39 | loss: 0.58284 | val_0_rmse: 0.77566 | val_1_rmse: 0.80845 |  0:00:12s
epoch 40 | loss: 0.58038 | val_0_rmse: 0.77662 | val_1_rmse: 0.80726 |  0:00:12s
epoch 41 | loss: 0.58162 | val_0_rmse: 0.77994 | val_1_rmse: 0.81341 |  0:00:13s
epoch 42 | loss: 0.59481 | val_0_rmse: 0.78303 | val_1_rmse: 0.81545 |  0:00:13s
epoch 43 | loss: 0.59246 | val_0_rmse: 0.78226 | val_1_rmse: 0.81097 |  0:00:13s
epoch 44 | loss: 0.5938  | val_0_rmse: 0.78375 | val_1_rmse: 0.81109 |  0:00:14s
epoch 45 | loss: 0.59717 | val_0_rmse: 0.77829 | val_1_rmse: 0.80798 |  0:00:14s
epoch 46 | loss: 0.59726 | val_0_rmse: 0.78258 | val_1_rmse: 0.81286 |  0:00:14s
epoch 47 | loss: 0.58564 | val_0_rmse: 0.78077 | val_1_rmse: 0.81185 |  0:00:15s
epoch 48 | loss: 0.58308 | val_0_rmse: 0.79008 | val_1_rmse: 0.81931 |  0:00:15s
epoch 49 | loss: 0.59177 | val_0_rmse: 0.7833  | val_1_rmse: 0.81027 |  0:00:15s
epoch 50 | loss: 0.58146 | val_0_rmse: 0.78178 | val_1_rmse: 0.80898 |  0:00:16s
epoch 51 | loss: 0.58255 | val_0_rmse: 0.7789  | val_1_rmse: 0.80453 |  0:00:16s
epoch 52 | loss: 0.57551 | val_0_rmse: 0.774   | val_1_rmse: 0.80518 |  0:00:16s
epoch 53 | loss: 0.5762  | val_0_rmse: 0.77479 | val_1_rmse: 0.80393 |  0:00:17s
epoch 54 | loss: 0.57578 | val_0_rmse: 0.77547 | val_1_rmse: 0.80307 |  0:00:17s
epoch 55 | loss: 0.58223 | val_0_rmse: 0.77653 | val_1_rmse: 0.80786 |  0:00:17s
epoch 56 | loss: 0.57363 | val_0_rmse: 0.77819 | val_1_rmse: 0.80823 |  0:00:18s
epoch 57 | loss: 0.57025 | val_0_rmse: 0.77141 | val_1_rmse: 0.79945 |  0:00:18s
epoch 58 | loss: 0.57243 | val_0_rmse: 0.7695  | val_1_rmse: 0.79472 |  0:00:18s
epoch 59 | loss: 0.56628 | val_0_rmse: 0.76654 | val_1_rmse: 0.79338 |  0:00:19s
epoch 60 | loss: 0.5769  | val_0_rmse: 0.7721  | val_1_rmse: 0.80377 |  0:00:19s
epoch 61 | loss: 0.57254 | val_0_rmse: 0.76887 | val_1_rmse: 0.80431 |  0:00:19s
epoch 62 | loss: 0.57701 | val_0_rmse: 0.77345 | val_1_rmse: 0.80866 |  0:00:19s
epoch 63 | loss: 0.57073 | val_0_rmse: 0.77186 | val_1_rmse: 0.80466 |  0:00:20s
epoch 64 | loss: 0.57832 | val_0_rmse: 0.77519 | val_1_rmse: 0.81019 |  0:00:20s
epoch 65 | loss: 0.57416 | val_0_rmse: 0.77199 | val_1_rmse: 0.80755 |  0:00:20s
epoch 66 | loss: 0.57125 | val_0_rmse: 0.76831 | val_1_rmse: 0.80387 |  0:00:21s
epoch 67 | loss: 0.5749  | val_0_rmse: 0.76213 | val_1_rmse: 0.79492 |  0:00:21s
epoch 68 | loss: 0.57646 | val_0_rmse: 0.77295 | val_1_rmse: 0.80115 |  0:00:21s
epoch 69 | loss: 0.57828 | val_0_rmse: 0.76357 | val_1_rmse: 0.80155 |  0:00:22s
epoch 70 | loss: 0.57261 | val_0_rmse: 0.76933 | val_1_rmse: 0.80339 |  0:00:22s
epoch 71 | loss: 0.5716  | val_0_rmse: 0.76141 | val_1_rmse: 0.79855 |  0:00:22s
epoch 72 | loss: 0.57341 | val_0_rmse: 0.78439 | val_1_rmse: 0.82193 |  0:00:23s
epoch 73 | loss: 0.58379 | val_0_rmse: 0.77138 | val_1_rmse: 0.81017 |  0:00:23s
epoch 74 | loss: 0.59113 | val_0_rmse: 0.77973 | val_1_rmse: 0.81659 |  0:00:23s
epoch 75 | loss: 0.58337 | val_0_rmse: 0.77339 | val_1_rmse: 0.80396 |  0:00:24s
epoch 76 | loss: 0.58717 | val_0_rmse: 0.77208 | val_1_rmse: 0.80396 |  0:00:24s
epoch 77 | loss: 0.57587 | val_0_rmse: 0.77441 | val_1_rmse: 0.81826 |  0:00:24s
epoch 78 | loss: 0.5767  | val_0_rmse: 0.76963 | val_1_rmse: 0.80452 |  0:00:24s
epoch 79 | loss: 0.57943 | val_0_rmse: 0.75955 | val_1_rmse: 0.7955  |  0:00:25s
epoch 80 | loss: 0.5704  | val_0_rmse: 0.76182 | val_1_rmse: 0.7946  |  0:00:25s
epoch 81 | loss: 0.5671  | val_0_rmse: 0.76085 | val_1_rmse: 0.78913 |  0:00:25s
epoch 82 | loss: 0.57289 | val_0_rmse: 0.7585  | val_1_rmse: 0.78772 |  0:00:26s
epoch 83 | loss: 0.56781 | val_0_rmse: 0.75654 | val_1_rmse: 0.78629 |  0:00:26s
epoch 84 | loss: 0.56068 | val_0_rmse: 0.75494 | val_1_rmse: 0.78252 |  0:00:26s
epoch 85 | loss: 0.56921 | val_0_rmse: 0.75027 | val_1_rmse: 0.78303 |  0:00:27s
epoch 86 | loss: 0.56066 | val_0_rmse: 0.75603 | val_1_rmse: 0.78955 |  0:00:27s
epoch 87 | loss: 0.56202 | val_0_rmse: 0.76196 | val_1_rmse: 0.79138 |  0:00:27s
epoch 88 | loss: 0.56005 | val_0_rmse: 0.7524  | val_1_rmse: 0.7958  |  0:00:28s
epoch 89 | loss: 0.56347 | val_0_rmse: 0.77    | val_1_rmse: 0.79916 |  0:00:28s
epoch 90 | loss: 0.57791 | val_0_rmse: 0.75456 | val_1_rmse: 0.78977 |  0:00:28s
epoch 91 | loss: 0.57402 | val_0_rmse: 0.75507 | val_1_rmse: 0.78949 |  0:00:29s
epoch 92 | loss: 0.56985 | val_0_rmse: 0.75859 | val_1_rmse: 0.79356 |  0:00:29s
epoch 93 | loss: 0.56436 | val_0_rmse: 0.75727 | val_1_rmse: 0.7942  |  0:00:29s
epoch 94 | loss: 0.56506 | val_0_rmse: 0.75476 | val_1_rmse: 0.79323 |  0:00:30s
epoch 95 | loss: 0.56557 | val_0_rmse: 0.75714 | val_1_rmse: 0.79727 |  0:00:30s
epoch 96 | loss: 0.56363 | val_0_rmse: 0.75194 | val_1_rmse: 0.79284 |  0:00:30s
epoch 97 | loss: 0.56093 | val_0_rmse: 0.75292 | val_1_rmse: 0.79015 |  0:00:31s
epoch 98 | loss: 0.55888 | val_0_rmse: 0.75113 | val_1_rmse: 0.78578 |  0:00:31s
epoch 99 | loss: 0.55933 | val_0_rmse: 0.76305 | val_1_rmse: 0.79781 |  0:00:31s
epoch 100| loss: 0.56542 | val_0_rmse: 0.76496 | val_1_rmse: 0.80399 |  0:00:31s
epoch 101| loss: 0.57    | val_0_rmse: 0.76407 | val_1_rmse: 0.79801 |  0:00:32s
epoch 102| loss: 0.56776 | val_0_rmse: 0.76295 | val_1_rmse: 0.7946  |  0:00:32s
epoch 103| loss: 0.57911 | val_0_rmse: 0.76348 | val_1_rmse: 0.7948  |  0:00:32s
epoch 104| loss: 0.57356 | val_0_rmse: 0.75798 | val_1_rmse: 0.78698 |  0:00:33s
epoch 105| loss: 0.57316 | val_0_rmse: 0.75706 | val_1_rmse: 0.78247 |  0:00:33s
epoch 106| loss: 0.58723 | val_0_rmse: 0.77472 | val_1_rmse: 0.79133 |  0:00:33s
epoch 107| loss: 0.59588 | val_0_rmse: 0.76781 | val_1_rmse: 0.78854 |  0:00:34s
epoch 108| loss: 0.59552 | val_0_rmse: 0.77442 | val_1_rmse: 0.80645 |  0:00:34s
epoch 109| loss: 0.58302 | val_0_rmse: 0.76017 | val_1_rmse: 0.79415 |  0:00:34s
epoch 110| loss: 0.58582 | val_0_rmse: 0.76003 | val_1_rmse: 0.81093 |  0:00:35s
epoch 111| loss: 0.58338 | val_0_rmse: 0.75488 | val_1_rmse: 0.79801 |  0:00:35s
epoch 112| loss: 0.57883 | val_0_rmse: 0.75054 | val_1_rmse: 0.80364 |  0:00:35s
epoch 113| loss: 0.57322 | val_0_rmse: 0.7459  | val_1_rmse: 0.79948 |  0:00:36s
epoch 114| loss: 0.56627 | val_0_rmse: 0.74449 | val_1_rmse: 0.79511 |  0:00:36s
epoch 115| loss: 0.56589 | val_0_rmse: 0.74283 | val_1_rmse: 0.79909 |  0:00:36s
epoch 116| loss: 0.5581  | val_0_rmse: 0.74138 | val_1_rmse: 0.79132 |  0:00:36s
epoch 117| loss: 0.5624  | val_0_rmse: 0.74067 | val_1_rmse: 0.79479 |  0:00:37s
epoch 118| loss: 0.55891 | val_0_rmse: 0.74166 | val_1_rmse: 0.80136 |  0:00:37s
epoch 119| loss: 0.55706 | val_0_rmse: 0.74111 | val_1_rmse: 0.80859 |  0:00:37s
epoch 120| loss: 0.55789 | val_0_rmse: 0.74456 | val_1_rmse: 0.80275 |  0:00:38s
epoch 121| loss: 0.55899 | val_0_rmse: 0.73833 | val_1_rmse: 0.79091 |  0:00:38s
epoch 122| loss: 0.56827 | val_0_rmse: 0.73933 | val_1_rmse: 0.79311 |  0:00:38s
epoch 123| loss: 0.5579  | val_0_rmse: 0.74177 | val_1_rmse: 0.79342 |  0:00:39s
epoch 124| loss: 0.55521 | val_0_rmse: 0.74045 | val_1_rmse: 0.79578 |  0:00:39s
epoch 125| loss: 0.55326 | val_0_rmse: 0.74277 | val_1_rmse: 0.79386 |  0:00:39s
epoch 126| loss: 0.55921 | val_0_rmse: 0.74268 | val_1_rmse: 0.81006 |  0:00:40s
epoch 127| loss: 0.5563  | val_0_rmse: 0.74625 | val_1_rmse: 0.82034 |  0:00:40s
epoch 128| loss: 0.5678  | val_0_rmse: 0.74587 | val_1_rmse: 0.81257 |  0:00:40s
epoch 129| loss: 0.55974 | val_0_rmse: 0.75606 | val_1_rmse: 0.81451 |  0:00:41s
epoch 130| loss: 0.56709 | val_0_rmse: 0.76071 | val_1_rmse: 0.81488 |  0:00:41s
epoch 131| loss: 0.56222 | val_0_rmse: 0.74055 | val_1_rmse: 0.80004 |  0:00:41s
epoch 132| loss: 0.55525 | val_0_rmse: 0.74359 | val_1_rmse: 0.79727 |  0:00:42s
epoch 133| loss: 0.56249 | val_0_rmse: 0.74479 | val_1_rmse: 0.79231 |  0:00:42s
epoch 134| loss: 0.56617 | val_0_rmse: 0.74485 | val_1_rmse: 0.78694 |  0:00:42s
epoch 135| loss: 0.56548 | val_0_rmse: 0.74622 | val_1_rmse: 0.78518 |  0:00:43s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.78247
Best weights from best epoch are automatically used!
ended training at: 05:08:33
Feature importance:
Mean squared error is of 0.04055712286053014
Mean absolute error:0.14863709772779474
MAPE:0.15348035698941967
R2 score:0.3619509874770548
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:08:34
epoch 0  | loss: 2.55912 | val_0_rmse: 1.00505 | val_1_rmse: 0.96004 |  0:00:00s
epoch 1  | loss: 1.25223 | val_0_rmse: 1.00352 | val_1_rmse: 0.95893 |  0:00:01s
epoch 2  | loss: 1.05501 | val_0_rmse: 1.00281 | val_1_rmse: 0.95815 |  0:00:01s
epoch 3  | loss: 1.02324 | val_0_rmse: 1.00415 | val_1_rmse: 0.96023 |  0:00:02s
epoch 4  | loss: 1.02667 | val_0_rmse: 1.00232 | val_1_rmse: 0.95728 |  0:00:03s
epoch 5  | loss: 1.01083 | val_0_rmse: 1.00249 | val_1_rmse: 0.95747 |  0:00:03s
epoch 6  | loss: 1.00674 | val_0_rmse: 1.00241 | val_1_rmse: 0.95752 |  0:00:04s
epoch 7  | loss: 1.00726 | val_0_rmse: 1.00241 | val_1_rmse: 0.95756 |  0:00:05s
epoch 8  | loss: 1.00591 | val_0_rmse: 1.00239 | val_1_rmse: 0.95749 |  0:00:05s
epoch 9  | loss: 1.00597 | val_0_rmse: 1.00243 | val_1_rmse: 0.95756 |  0:00:06s
epoch 10 | loss: 1.00634 | val_0_rmse: 1.00241 | val_1_rmse: 0.95753 |  0:00:06s
epoch 11 | loss: 1.00533 | val_0_rmse: 1.00239 | val_1_rmse: 0.95747 |  0:00:07s
epoch 12 | loss: 1.00523 | val_0_rmse: 1.0024  | val_1_rmse: 0.95751 |  0:00:08s
epoch 13 | loss: 1.00501 | val_0_rmse: 1.00235 | val_1_rmse: 0.95747 |  0:00:08s
epoch 14 | loss: 1.00502 | val_0_rmse: 1.00232 | val_1_rmse: 0.95748 |  0:00:09s
epoch 15 | loss: 1.00563 | val_0_rmse: 1.0023  | val_1_rmse: 0.95744 |  0:00:10s
epoch 16 | loss: 1.00541 | val_0_rmse: 1.00217 | val_1_rmse: 0.9573  |  0:00:10s
epoch 17 | loss: 1.00366 | val_0_rmse: 1.00191 | val_1_rmse: 0.95701 |  0:00:11s
epoch 18 | loss: 1.00343 | val_0_rmse: 1.00146 | val_1_rmse: 0.9567  |  0:00:12s
epoch 19 | loss: 1.00365 | val_0_rmse: 1.00094 | val_1_rmse: 0.95649 |  0:00:12s
epoch 20 | loss: 0.99968 | val_0_rmse: 1.00015 | val_1_rmse: 0.95628 |  0:00:13s
epoch 21 | loss: 0.99778 | val_0_rmse: 0.99916 | val_1_rmse: 0.95537 |  0:00:14s
epoch 22 | loss: 0.99309 | val_0_rmse: 0.99327 | val_1_rmse: 0.95119 |  0:00:14s
epoch 23 | loss: 0.98203 | val_0_rmse: 0.98021 | val_1_rmse: 0.93825 |  0:00:15s
epoch 24 | loss: 0.96287 | val_0_rmse: 0.9829  | val_1_rmse: 0.94397 |  0:00:15s
epoch 25 | loss: 0.93651 | val_0_rmse: 0.97879 | val_1_rmse: 0.93903 |  0:00:16s
epoch 26 | loss: 0.91029 | val_0_rmse: 0.96363 | val_1_rmse: 0.92748 |  0:00:17s
epoch 27 | loss: 0.88953 | val_0_rmse: 1.01574 | val_1_rmse: 0.97705 |  0:00:17s
epoch 28 | loss: 0.88016 | val_0_rmse: 0.94835 | val_1_rmse: 0.90924 |  0:00:18s
epoch 29 | loss: 0.84918 | val_0_rmse: 0.96286 | val_1_rmse: 0.92576 |  0:00:19s
epoch 30 | loss: 0.83039 | val_0_rmse: 0.93705 | val_1_rmse: 0.90055 |  0:00:19s
epoch 31 | loss: 0.8181  | val_0_rmse: 0.93455 | val_1_rmse: 0.89783 |  0:00:20s
epoch 32 | loss: 0.79594 | val_0_rmse: 0.93704 | val_1_rmse: 0.90423 |  0:00:20s
epoch 33 | loss: 0.77584 | val_0_rmse: 0.90383 | val_1_rmse: 0.86986 |  0:00:21s
epoch 34 | loss: 0.74554 | val_0_rmse: 0.89485 | val_1_rmse: 0.86646 |  0:00:22s
epoch 35 | loss: 0.726   | val_0_rmse: 0.87874 | val_1_rmse: 0.84151 |  0:00:23s
epoch 36 | loss: 0.69353 | val_0_rmse: 0.83937 | val_1_rmse: 0.80924 |  0:00:23s
epoch 37 | loss: 0.66442 | val_0_rmse: 0.82668 | val_1_rmse: 0.8018  |  0:00:24s
epoch 38 | loss: 0.68106 | val_0_rmse: 0.80815 | val_1_rmse: 0.77433 |  0:00:24s
epoch 39 | loss: 0.67495 | val_0_rmse: 0.83608 | val_1_rmse: 0.80795 |  0:00:25s
epoch 40 | loss: 0.67993 | val_0_rmse: 0.83516 | val_1_rmse: 0.82149 |  0:00:26s
epoch 41 | loss: 0.63906 | val_0_rmse: 0.80583 | val_1_rmse: 0.77339 |  0:00:26s
epoch 42 | loss: 0.61275 | val_0_rmse: 0.84131 | val_1_rmse: 0.82249 |  0:00:27s
epoch 43 | loss: 0.6229  | val_0_rmse: 0.82674 | val_1_rmse: 0.81375 |  0:00:28s
epoch 44 | loss: 0.59195 | val_0_rmse: 0.80052 | val_1_rmse: 0.78988 |  0:00:28s
epoch 45 | loss: 0.5742  | val_0_rmse: 0.80595 | val_1_rmse: 0.79005 |  0:00:29s
epoch 46 | loss: 0.56379 | val_0_rmse: 0.79072 | val_1_rmse: 0.77106 |  0:00:29s
epoch 47 | loss: 0.56945 | val_0_rmse: 0.78995 | val_1_rmse: 0.78204 |  0:00:30s
epoch 48 | loss: 0.55624 | val_0_rmse: 0.78035 | val_1_rmse: 0.77324 |  0:00:31s
epoch 49 | loss: 0.53984 | val_0_rmse: 0.80491 | val_1_rmse: 0.79653 |  0:00:31s
epoch 50 | loss: 0.52982 | val_0_rmse: 0.7662  | val_1_rmse: 0.75821 |  0:00:32s
epoch 51 | loss: 0.53876 | val_0_rmse: 0.76294 | val_1_rmse: 0.76332 |  0:00:33s
epoch 52 | loss: 0.53169 | val_0_rmse: 0.77127 | val_1_rmse: 0.76003 |  0:00:33s
epoch 53 | loss: 0.51426 | val_0_rmse: 0.78587 | val_1_rmse: 0.7826  |  0:00:34s
epoch 54 | loss: 0.5243  | val_0_rmse: 0.77181 | val_1_rmse: 0.76858 |  0:00:34s
epoch 55 | loss: 0.51476 | val_0_rmse: 0.7621  | val_1_rmse: 0.76742 |  0:00:35s
epoch 56 | loss: 0.50813 | val_0_rmse: 0.74924 | val_1_rmse: 0.76582 |  0:00:36s
epoch 57 | loss: 0.50293 | val_0_rmse: 0.74887 | val_1_rmse: 0.76219 |  0:00:36s
epoch 58 | loss: 0.49379 | val_0_rmse: 0.76588 | val_1_rmse: 0.79269 |  0:00:37s
epoch 59 | loss: 0.49403 | val_0_rmse: 0.73885 | val_1_rmse: 0.76185 |  0:00:38s
epoch 60 | loss: 0.49607 | val_0_rmse: 0.73258 | val_1_rmse: 0.7682  |  0:00:38s
epoch 61 | loss: 0.485   | val_0_rmse: 0.7259  | val_1_rmse: 0.76417 |  0:00:39s
epoch 62 | loss: 0.47777 | val_0_rmse: 0.72524 | val_1_rmse: 0.77276 |  0:00:40s
epoch 63 | loss: 0.47893 | val_0_rmse: 0.71976 | val_1_rmse: 0.76893 |  0:00:40s
epoch 64 | loss: 0.47358 | val_0_rmse: 0.72564 | val_1_rmse: 0.77172 |  0:00:41s
epoch 65 | loss: 0.46534 | val_0_rmse: 0.71396 | val_1_rmse: 0.76595 |  0:00:42s
epoch 66 | loss: 0.45913 | val_0_rmse: 0.70863 | val_1_rmse: 0.76923 |  0:00:42s
epoch 67 | loss: 0.45887 | val_0_rmse: 0.71057 | val_1_rmse: 0.78651 |  0:00:43s
epoch 68 | loss: 0.45437 | val_0_rmse: 0.70027 | val_1_rmse: 0.7629  |  0:00:43s
epoch 69 | loss: 0.44421 | val_0_rmse: 0.7035  | val_1_rmse: 0.78522 |  0:00:44s
epoch 70 | loss: 0.4427  | val_0_rmse: 0.69014 | val_1_rmse: 0.77137 |  0:00:45s
epoch 71 | loss: 0.44176 | val_0_rmse: 0.68995 | val_1_rmse: 0.7861  |  0:00:45s
epoch 72 | loss: 0.44681 | val_0_rmse: 0.68419 | val_1_rmse: 0.77346 |  0:00:46s
epoch 73 | loss: 0.42674 | val_0_rmse: 0.67617 | val_1_rmse: 0.77819 |  0:00:47s
epoch 74 | loss: 0.42447 | val_0_rmse: 0.68137 | val_1_rmse: 0.76732 |  0:00:47s
epoch 75 | loss: 0.44868 | val_0_rmse: 0.67232 | val_1_rmse: 0.78369 |  0:00:48s
epoch 76 | loss: 0.42728 | val_0_rmse: 0.66706 | val_1_rmse: 0.77879 |  0:00:48s
epoch 77 | loss: 0.4241  | val_0_rmse: 0.66773 | val_1_rmse: 0.78483 |  0:00:49s
epoch 78 | loss: 0.42314 | val_0_rmse: 0.6637  | val_1_rmse: 0.79714 |  0:00:50s
epoch 79 | loss: 0.42824 | val_0_rmse: 0.65212 | val_1_rmse: 0.78508 |  0:00:50s
epoch 80 | loss: 0.4174  | val_0_rmse: 0.65372 | val_1_rmse: 0.78959 |  0:00:51s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.75821
Best weights from best epoch are automatically used!
ended training at: 05:09:26
Feature importance:
Mean squared error is of 0.04751958870778473
Mean absolute error:0.1652567838626116
MAPE:0.17327878149139203
R2 score:0.5425080553130965
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:09:26
epoch 0  | loss: 2.32393 | val_0_rmse: 0.99986 | val_1_rmse: 1.02802 |  0:00:00s
epoch 1  | loss: 1.30508 | val_0_rmse: 1.00184 | val_1_rmse: 1.0286  |  0:00:01s
epoch 2  | loss: 1.05463 | val_0_rmse: 0.99985 | val_1_rmse: 1.02751 |  0:00:01s
epoch 3  | loss: 1.01393 | val_0_rmse: 0.99918 | val_1_rmse: 1.02692 |  0:00:02s
epoch 4  | loss: 1.00898 | val_0_rmse: 0.99946 | val_1_rmse: 1.02696 |  0:00:03s
epoch 5  | loss: 1.01301 | val_0_rmse: 0.99887 | val_1_rmse: 1.02664 |  0:00:03s
epoch 6  | loss: 1.00196 | val_0_rmse: 0.999   | val_1_rmse: 1.02659 |  0:00:04s
epoch 7  | loss: 1.00189 | val_0_rmse: 0.99889 | val_1_rmse: 1.02653 |  0:00:05s
epoch 8  | loss: 1.00053 | val_0_rmse: 0.99901 | val_1_rmse: 1.02645 |  0:00:05s
epoch 9  | loss: 0.99963 | val_0_rmse: 0.99898 | val_1_rmse: 1.02658 |  0:00:06s
epoch 10 | loss: 0.99871 | val_0_rmse: 0.9989  | val_1_rmse: 1.02657 |  0:00:06s
epoch 11 | loss: 0.99836 | val_0_rmse: 0.99888 | val_1_rmse: 1.02658 |  0:00:07s
epoch 12 | loss: 0.99879 | val_0_rmse: 0.99895 | val_1_rmse: 1.02658 |  0:00:08s
epoch 13 | loss: 0.9984  | val_0_rmse: 0.99898 | val_1_rmse: 1.02659 |  0:00:08s
epoch 14 | loss: 0.99819 | val_0_rmse: 0.99897 | val_1_rmse: 1.0266  |  0:00:09s
epoch 15 | loss: 0.99831 | val_0_rmse: 0.99888 | val_1_rmse: 1.02657 |  0:00:10s
epoch 16 | loss: 0.99853 | val_0_rmse: 0.99891 | val_1_rmse: 1.02661 |  0:00:10s
epoch 17 | loss: 0.99802 | val_0_rmse: 0.99896 | val_1_rmse: 1.02659 |  0:00:11s
epoch 18 | loss: 0.99726 | val_0_rmse: 0.99893 | val_1_rmse: 1.02658 |  0:00:12s
epoch 19 | loss: 0.99763 | val_0_rmse: 0.99868 | val_1_rmse: 1.02654 |  0:00:12s
epoch 20 | loss: 0.99727 | val_0_rmse: 0.99858 | val_1_rmse: 1.02643 |  0:00:13s
epoch 21 | loss: 0.99689 | val_0_rmse: 0.99854 | val_1_rmse: 1.02631 |  0:00:13s
epoch 22 | loss: 0.99642 | val_0_rmse: 0.99771 | val_1_rmse: 1.02576 |  0:00:14s
epoch 23 | loss: 0.99571 | val_0_rmse: 0.99615 | val_1_rmse: 1.02474 |  0:00:15s
epoch 24 | loss: 0.99532 | val_0_rmse: 0.99466 | val_1_rmse: 1.02341 |  0:00:15s
epoch 25 | loss: 0.99455 | val_0_rmse: 0.98918 | val_1_rmse: 1.01852 |  0:00:16s
epoch 26 | loss: 0.98627 | val_0_rmse: 0.96994 | val_1_rmse: 1.0027  |  0:00:17s
epoch 27 | loss: 0.97469 | val_0_rmse: 0.96248 | val_1_rmse: 0.99685 |  0:00:17s
epoch 28 | loss: 0.94771 | val_0_rmse: 0.96068 | val_1_rmse: 1.00119 |  0:00:18s
epoch 29 | loss: 0.93684 | val_0_rmse: 0.95771 | val_1_rmse: 0.99717 |  0:00:19s
epoch 30 | loss: 0.91908 | val_0_rmse: 0.94966 | val_1_rmse: 0.99258 |  0:00:19s
epoch 31 | loss: 0.89237 | val_0_rmse: 0.94939 | val_1_rmse: 0.99515 |  0:00:20s
epoch 32 | loss: 0.88173 | val_0_rmse: 0.9345  | val_1_rmse: 0.98039 |  0:00:20s
epoch 33 | loss: 0.85612 | val_0_rmse: 0.91502 | val_1_rmse: 0.95825 |  0:00:21s
epoch 34 | loss: 0.84266 | val_0_rmse: 0.90844 | val_1_rmse: 0.95439 |  0:00:22s
epoch 35 | loss: 0.82419 | val_0_rmse: 0.91318 | val_1_rmse: 0.95521 |  0:00:22s
epoch 36 | loss: 0.81516 | val_0_rmse: 0.9037  | val_1_rmse: 0.95046 |  0:00:23s
epoch 37 | loss: 0.80152 | val_0_rmse: 0.89466 | val_1_rmse: 0.95641 |  0:00:24s
epoch 38 | loss: 0.78026 | val_0_rmse: 0.88756 | val_1_rmse: 0.94373 |  0:00:24s
epoch 39 | loss: 0.76012 | val_0_rmse: 0.87841 | val_1_rmse: 0.93375 |  0:00:25s
epoch 40 | loss: 0.73812 | val_0_rmse: 0.86423 | val_1_rmse: 0.9286  |  0:00:26s
epoch 41 | loss: 0.71992 | val_0_rmse: 0.85929 | val_1_rmse: 0.91641 |  0:00:26s
epoch 42 | loss: 0.70262 | val_0_rmse: 0.87273 | val_1_rmse: 0.93622 |  0:00:27s
epoch 43 | loss: 0.70026 | val_0_rmse: 0.83759 | val_1_rmse: 0.8843  |  0:00:28s
epoch 44 | loss: 0.67457 | val_0_rmse: 0.84865 | val_1_rmse: 0.88135 |  0:00:28s
epoch 45 | loss: 0.65422 | val_0_rmse: 0.80767 | val_1_rmse: 0.8493  |  0:00:29s
epoch 46 | loss: 0.62192 | val_0_rmse: 0.85978 | val_1_rmse: 0.87687 |  0:00:29s
epoch 47 | loss: 0.67724 | val_0_rmse: 0.84502 | val_1_rmse: 0.89179 |  0:00:30s
epoch 48 | loss: 0.65212 | val_0_rmse: 0.84796 | val_1_rmse: 0.89765 |  0:00:31s
epoch 49 | loss: 0.63048 | val_0_rmse: 0.83109 | val_1_rmse: 0.86926 |  0:00:31s
epoch 50 | loss: 0.57678 | val_0_rmse: 0.81705 | val_1_rmse: 0.85415 |  0:00:32s
epoch 51 | loss: 0.57494 | val_0_rmse: 0.7681  | val_1_rmse: 0.80283 |  0:00:33s
epoch 52 | loss: 0.58951 | val_0_rmse: 0.76652 | val_1_rmse: 0.80774 |  0:00:33s
epoch 53 | loss: 0.56744 | val_0_rmse: 0.79502 | val_1_rmse: 0.84587 |  0:00:34s
epoch 54 | loss: 0.55014 | val_0_rmse: 0.76272 | val_1_rmse: 0.80912 |  0:00:35s
epoch 55 | loss: 0.54664 | val_0_rmse: 0.75632 | val_1_rmse: 0.8117  |  0:00:35s
epoch 56 | loss: 0.56238 | val_0_rmse: 0.82786 | val_1_rmse: 0.89821 |  0:00:36s
epoch 57 | loss: 0.54798 | val_0_rmse: 0.79585 | val_1_rmse: 0.86177 |  0:00:36s
epoch 58 | loss: 0.54986 | val_0_rmse: 0.74243 | val_1_rmse: 0.7956  |  0:00:37s
epoch 59 | loss: 0.53222 | val_0_rmse: 0.77736 | val_1_rmse: 0.83911 |  0:00:38s
epoch 60 | loss: 0.52767 | val_0_rmse: 0.75115 | val_1_rmse: 0.8067  |  0:00:38s
epoch 61 | loss: 0.52003 | val_0_rmse: 0.74914 | val_1_rmse: 0.80099 |  0:00:39s
epoch 62 | loss: 0.52201 | val_0_rmse: 0.74016 | val_1_rmse: 0.80372 |  0:00:40s
epoch 63 | loss: 0.51389 | val_0_rmse: 0.74257 | val_1_rmse: 0.79257 |  0:00:40s
epoch 64 | loss: 0.49949 | val_0_rmse: 0.7315  | val_1_rmse: 0.791   |  0:00:41s
epoch 65 | loss: 0.48758 | val_0_rmse: 0.71961 | val_1_rmse: 0.78637 |  0:00:42s
epoch 66 | loss: 0.48427 | val_0_rmse: 0.71388 | val_1_rmse: 0.77664 |  0:00:42s
epoch 67 | loss: 0.48289 | val_0_rmse: 0.71239 | val_1_rmse: 0.79967 |  0:00:43s
epoch 68 | loss: 0.4772  | val_0_rmse: 0.70662 | val_1_rmse: 0.7961  |  0:00:43s
epoch 69 | loss: 0.46653 | val_0_rmse: 0.7081  | val_1_rmse: 0.78483 |  0:00:44s
epoch 70 | loss: 0.46655 | val_0_rmse: 0.71361 | val_1_rmse: 0.80907 |  0:00:45s
epoch 71 | loss: 0.45574 | val_0_rmse: 0.69365 | val_1_rmse: 0.79152 |  0:00:45s
epoch 72 | loss: 0.44449 | val_0_rmse: 0.68211 | val_1_rmse: 0.80714 |  0:00:46s
epoch 73 | loss: 0.44301 | val_0_rmse: 0.6769  | val_1_rmse: 0.79793 |  0:00:47s
epoch 74 | loss: 0.44468 | val_0_rmse: 0.6796  | val_1_rmse: 0.79775 |  0:00:47s
epoch 75 | loss: 0.4367  | val_0_rmse: 0.67626 | val_1_rmse: 0.81115 |  0:00:48s
epoch 76 | loss: 0.42972 | val_0_rmse: 0.68211 | val_1_rmse: 0.79615 |  0:00:48s
epoch 77 | loss: 0.42674 | val_0_rmse: 0.66569 | val_1_rmse: 0.80562 |  0:00:49s
epoch 78 | loss: 0.42653 | val_0_rmse: 0.65702 | val_1_rmse: 0.78554 |  0:00:50s
epoch 79 | loss: 0.4227  | val_0_rmse: 0.64902 | val_1_rmse: 0.79694 |  0:00:50s
epoch 80 | loss: 0.41801 | val_0_rmse: 0.65188 | val_1_rmse: 0.82426 |  0:00:51s
epoch 81 | loss: 0.43002 | val_0_rmse: 0.66339 | val_1_rmse: 0.8123  |  0:00:52s
epoch 82 | loss: 0.42156 | val_0_rmse: 0.65195 | val_1_rmse: 0.82257 |  0:00:52s
epoch 83 | loss: 0.4198  | val_0_rmse: 0.64326 | val_1_rmse: 0.81539 |  0:00:53s
epoch 84 | loss: 0.41484 | val_0_rmse: 0.646   | val_1_rmse: 0.79792 |  0:00:53s
epoch 85 | loss: 0.405   | val_0_rmse: 0.63594 | val_1_rmse: 0.80478 |  0:00:54s
epoch 86 | loss: 0.41334 | val_0_rmse: 0.63334 | val_1_rmse: 0.82937 |  0:00:55s
epoch 87 | loss: 0.41119 | val_0_rmse: 0.75789 | val_1_rmse: 0.86602 |  0:00:55s
epoch 88 | loss: 0.50963 | val_0_rmse: 0.72904 | val_1_rmse: 0.84483 |  0:00:56s
epoch 89 | loss: 0.48162 | val_0_rmse: 0.68922 | val_1_rmse: 0.80761 |  0:00:57s
epoch 90 | loss: 0.46741 | val_0_rmse: 0.67515 | val_1_rmse: 0.81465 |  0:00:57s
epoch 91 | loss: 0.445   | val_0_rmse: 0.65267 | val_1_rmse: 0.81678 |  0:00:58s
epoch 92 | loss: 0.43863 | val_0_rmse: 0.64664 | val_1_rmse: 0.83286 |  0:00:59s
epoch 93 | loss: 0.42668 | val_0_rmse: 0.63882 | val_1_rmse: 0.81546 |  0:00:59s
epoch 94 | loss: 0.42238 | val_0_rmse: 0.624   | val_1_rmse: 0.81477 |  0:01:00s
epoch 95 | loss: 0.40488 | val_0_rmse: 0.61637 | val_1_rmse: 0.80671 |  0:01:01s
epoch 96 | loss: 0.40634 | val_0_rmse: 0.61086 | val_1_rmse: 0.81314 |  0:01:01s

Early stopping occured at epoch 96 with best_epoch = 66 and best_val_1_rmse = 0.77664
Best weights from best epoch are automatically used!
ended training at: 05:10:28
Feature importance:
Mean squared error is of 0.05493497686136171
Mean absolute error:0.16288934491492665
MAPE:0.16502018536347987
R2 score:0.2960963613883957
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:10:28
epoch 0  | loss: 2.28189 | val_0_rmse: 1.00706 | val_1_rmse: 1.02145 |  0:00:00s
epoch 1  | loss: 1.31925 | val_0_rmse: 1.00702 | val_1_rmse: 1.02164 |  0:00:01s
epoch 2  | loss: 1.12551 | val_0_rmse: 1.00876 | val_1_rmse: 1.02387 |  0:00:01s
epoch 3  | loss: 1.03499 | val_0_rmse: 1.00858 | val_1_rmse: 1.02295 |  0:00:02s
epoch 4  | loss: 1.02848 | val_0_rmse: 1.00828 | val_1_rmse: 1.02256 |  0:00:03s
epoch 5  | loss: 1.03054 | val_0_rmse: 1.00946 | val_1_rmse: 1.02421 |  0:00:03s
epoch 6  | loss: 1.01842 | val_0_rmse: 1.00881 | val_1_rmse: 1.02323 |  0:00:04s
epoch 7  | loss: 1.02018 | val_0_rmse: 1.00887 | val_1_rmse: 1.02339 |  0:00:05s
epoch 8  | loss: 1.01896 | val_0_rmse: 1.00874 | val_1_rmse: 1.0233  |  0:00:05s
epoch 9  | loss: 1.0187  | val_0_rmse: 1.00885 | val_1_rmse: 1.02333 |  0:00:06s
epoch 10 | loss: 1.01961 | val_0_rmse: 1.00878 | val_1_rmse: 1.02322 |  0:00:06s
epoch 11 | loss: 1.01765 | val_0_rmse: 1.00878 | val_1_rmse: 1.02319 |  0:00:07s
epoch 12 | loss: 1.01855 | val_0_rmse: 1.00877 | val_1_rmse: 1.02324 |  0:00:08s
epoch 13 | loss: 1.01851 | val_0_rmse: 1.00878 | val_1_rmse: 1.02326 |  0:00:08s
epoch 14 | loss: 1.01902 | val_0_rmse: 1.00879 | val_1_rmse: 1.02326 |  0:00:09s
epoch 15 | loss: 1.01936 | val_0_rmse: 1.00881 | val_1_rmse: 1.02326 |  0:00:10s
epoch 16 | loss: 1.01809 | val_0_rmse: 1.0088  | val_1_rmse: 1.02324 |  0:00:10s
epoch 17 | loss: 1.01741 | val_0_rmse: 1.00881 | val_1_rmse: 1.02325 |  0:00:11s
epoch 18 | loss: 1.01918 | val_0_rmse: 1.0088  | val_1_rmse: 1.02326 |  0:00:12s
epoch 19 | loss: 1.01721 | val_0_rmse: 1.00878 | val_1_rmse: 1.02324 |  0:00:12s
epoch 20 | loss: 1.0175  | val_0_rmse: 1.00876 | val_1_rmse: 1.02322 |  0:00:13s
epoch 21 | loss: 1.01784 | val_0_rmse: 1.00874 | val_1_rmse: 1.02321 |  0:00:13s
epoch 22 | loss: 1.01735 | val_0_rmse: 1.00888 | val_1_rmse: 1.0233  |  0:00:14s
epoch 23 | loss: 1.0174  | val_0_rmse: 1.00892 | val_1_rmse: 1.02338 |  0:00:15s
epoch 24 | loss: 1.01781 | val_0_rmse: 1.00875 | val_1_rmse: 1.02319 |  0:00:15s
epoch 25 | loss: 1.01811 | val_0_rmse: 1.00883 | val_1_rmse: 1.02324 |  0:00:16s
epoch 26 | loss: 1.01745 | val_0_rmse: 1.00886 | val_1_rmse: 1.02329 |  0:00:17s
epoch 27 | loss: 1.01726 | val_0_rmse: 1.00887 | val_1_rmse: 1.02331 |  0:00:17s
epoch 28 | loss: 1.01708 | val_0_rmse: 1.00882 | val_1_rmse: 1.02328 |  0:00:18s
epoch 29 | loss: 1.01758 | val_0_rmse: 1.00884 | val_1_rmse: 1.02329 |  0:00:18s
epoch 30 | loss: 1.01643 | val_0_rmse: 1.00887 | val_1_rmse: 1.02341 |  0:00:19s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 1.02145
Best weights from best epoch are automatically used!
ended training at: 05:10:48
Feature importance:
Mean squared error is of 0.07426710979284736
Mean absolute error:0.19244486395552388
MAPE:0.20351446372106444
R2 score:0.0022301161796529634
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:10:49
epoch 0  | loss: 2.50408 | val_0_rmse: 1.01128 | val_1_rmse: 0.96846 |  0:00:00s
epoch 1  | loss: 1.30632 | val_0_rmse: 1.00713 | val_1_rmse: 0.96461 |  0:00:01s
epoch 2  | loss: 1.0786  | val_0_rmse: 1.00299 | val_1_rmse: 0.96175 |  0:00:02s
epoch 3  | loss: 1.02821 | val_0_rmse: 1.00207 | val_1_rmse: 0.96099 |  0:00:02s
epoch 4  | loss: 1.01526 | val_0_rmse: 1.00319 | val_1_rmse: 0.96298 |  0:00:03s
epoch 5  | loss: 1.01468 | val_0_rmse: 1.00357 | val_1_rmse: 0.96258 |  0:00:03s
epoch 6  | loss: 1.00969 | val_0_rmse: 1.00384 | val_1_rmse: 0.96325 |  0:00:04s
epoch 7  | loss: 1.00821 | val_0_rmse: 1.003   | val_1_rmse: 0.962   |  0:00:05s
epoch 8  | loss: 1.00809 | val_0_rmse: 1.00296 | val_1_rmse: 0.96188 |  0:00:05s
epoch 9  | loss: 1.00618 | val_0_rmse: 1.00261 | val_1_rmse: 0.96219 |  0:00:06s
epoch 10 | loss: 1.00806 | val_0_rmse: 1.0024  | val_1_rmse: 0.96183 |  0:00:07s
epoch 11 | loss: 1.00434 | val_0_rmse: 1.00049 | val_1_rmse: 0.95922 |  0:00:07s
epoch 12 | loss: 1.00294 | val_0_rmse: 0.99946 | val_1_rmse: 0.95848 |  0:00:08s
epoch 13 | loss: 1.00117 | val_0_rmse: 0.99745 | val_1_rmse: 0.95778 |  0:00:09s
epoch 14 | loss: 1.00055 | val_0_rmse: 0.99582 | val_1_rmse: 0.9566  |  0:00:09s
epoch 15 | loss: 0.99593 | val_0_rmse: 0.99103 | val_1_rmse: 0.95355 |  0:00:10s
epoch 16 | loss: 0.99343 | val_0_rmse: 0.98304 | val_1_rmse: 0.94722 |  0:00:10s
epoch 17 | loss: 0.98017 | val_0_rmse: 0.97012 | val_1_rmse: 0.94124 |  0:00:11s
epoch 18 | loss: 0.97223 | val_0_rmse: 0.9602  | val_1_rmse: 0.92858 |  0:00:12s
epoch 19 | loss: 0.9649  | val_0_rmse: 0.96798 | val_1_rmse: 0.94588 |  0:00:12s
epoch 20 | loss: 0.94532 | val_0_rmse: 0.96674 | val_1_rmse: 0.9403  |  0:00:13s
epoch 21 | loss: 0.92553 | val_0_rmse: 0.9523  | val_1_rmse: 0.91842 |  0:00:14s
epoch 22 | loss: 0.9082  | val_0_rmse: 0.96386 | val_1_rmse: 0.92847 |  0:00:14s
epoch 23 | loss: 0.88881 | val_0_rmse: 0.94949 | val_1_rmse: 0.91239 |  0:00:15s
epoch 24 | loss: 0.87331 | val_0_rmse: 0.93462 | val_1_rmse: 0.89667 |  0:00:16s
epoch 25 | loss: 0.85269 | val_0_rmse: 0.93245 | val_1_rmse: 0.89851 |  0:00:16s
epoch 26 | loss: 0.83508 | val_0_rmse: 0.93092 | val_1_rmse: 0.90041 |  0:00:17s
epoch 27 | loss: 0.81547 | val_0_rmse: 0.91909 | val_1_rmse: 0.89134 |  0:00:17s
epoch 28 | loss: 0.79197 | val_0_rmse: 0.88297 | val_1_rmse: 0.87684 |  0:00:18s
epoch 29 | loss: 0.79613 | val_0_rmse: 0.88762 | val_1_rmse: 0.87411 |  0:00:19s
epoch 30 | loss: 0.77923 | val_0_rmse: 0.86088 | val_1_rmse: 0.838   |  0:00:19s
epoch 31 | loss: 0.73714 | val_0_rmse: 0.81984 | val_1_rmse: 0.80043 |  0:00:20s
epoch 32 | loss: 0.71656 | val_0_rmse: 0.85082 | val_1_rmse: 0.83685 |  0:00:21s
epoch 33 | loss: 0.6956  | val_0_rmse: 0.82541 | val_1_rmse: 0.7912  |  0:00:21s
epoch 34 | loss: 0.69353 | val_0_rmse: 0.81142 | val_1_rmse: 0.78496 |  0:00:22s
epoch 35 | loss: 0.658   | val_0_rmse: 0.81146 | val_1_rmse: 0.78303 |  0:00:23s
epoch 36 | loss: 0.62852 | val_0_rmse: 0.83211 | val_1_rmse: 0.81618 |  0:00:23s
epoch 37 | loss: 0.60667 | val_0_rmse: 0.82596 | val_1_rmse: 0.79838 |  0:00:24s
epoch 38 | loss: 0.59538 | val_0_rmse: 0.79817 | val_1_rmse: 0.7854  |  0:00:25s
epoch 39 | loss: 0.58111 | val_0_rmse: 0.7858  | val_1_rmse: 0.77657 |  0:00:25s
epoch 40 | loss: 0.57182 | val_0_rmse: 0.79022 | val_1_rmse: 0.78337 |  0:00:26s
epoch 41 | loss: 0.56834 | val_0_rmse: 0.77315 | val_1_rmse: 0.77154 |  0:00:27s
epoch 42 | loss: 0.56539 | val_0_rmse: 0.78362 | val_1_rmse: 0.77624 |  0:00:27s
epoch 43 | loss: 0.56007 | val_0_rmse: 0.77713 | val_1_rmse: 0.76936 |  0:00:28s
epoch 44 | loss: 0.55895 | val_0_rmse: 0.77771 | val_1_rmse: 0.77243 |  0:00:28s
epoch 45 | loss: 0.54801 | val_0_rmse: 0.77585 | val_1_rmse: 0.77728 |  0:00:29s
epoch 46 | loss: 0.53495 | val_0_rmse: 0.77009 | val_1_rmse: 0.77991 |  0:00:30s
epoch 47 | loss: 0.52441 | val_0_rmse: 0.76189 | val_1_rmse: 0.78399 |  0:00:30s
epoch 48 | loss: 0.52115 | val_0_rmse: 0.76009 | val_1_rmse: 0.77686 |  0:00:31s
epoch 49 | loss: 0.53938 | val_0_rmse: 0.76214 | val_1_rmse: 0.7733  |  0:00:32s
epoch 50 | loss: 0.52469 | val_0_rmse: 0.75858 | val_1_rmse: 0.77346 |  0:00:32s
epoch 51 | loss: 0.52933 | val_0_rmse: 0.75915 | val_1_rmse: 0.78514 |  0:00:33s
epoch 52 | loss: 0.5273  | val_0_rmse: 0.75032 | val_1_rmse: 0.76845 |  0:00:34s
epoch 53 | loss: 0.51037 | val_0_rmse: 0.7567  | val_1_rmse: 0.77151 |  0:00:34s
epoch 54 | loss: 0.50509 | val_0_rmse: 0.75752 | val_1_rmse: 0.77727 |  0:00:35s
epoch 55 | loss: 0.49911 | val_0_rmse: 0.74103 | val_1_rmse: 0.76541 |  0:00:35s
epoch 56 | loss: 0.48674 | val_0_rmse: 0.74236 | val_1_rmse: 0.76714 |  0:00:36s
epoch 57 | loss: 0.48151 | val_0_rmse: 0.73296 | val_1_rmse: 0.76964 |  0:00:37s
epoch 58 | loss: 0.48499 | val_0_rmse: 0.72962 | val_1_rmse: 0.75682 |  0:00:37s
epoch 59 | loss: 0.48121 | val_0_rmse: 0.73301 | val_1_rmse: 0.7837  |  0:00:38s
epoch 60 | loss: 0.47223 | val_0_rmse: 0.72621 | val_1_rmse: 0.7653  |  0:00:39s
epoch 61 | loss: 0.47194 | val_0_rmse: 0.73561 | val_1_rmse: 0.79203 |  0:00:39s
epoch 62 | loss: 0.46875 | val_0_rmse: 0.72677 | val_1_rmse: 0.77042 |  0:00:40s
epoch 63 | loss: 0.46961 | val_0_rmse: 0.72478 | val_1_rmse: 0.79255 |  0:00:41s
epoch 64 | loss: 0.46314 | val_0_rmse: 0.7118  | val_1_rmse: 0.76332 |  0:00:41s
epoch 65 | loss: 0.44862 | val_0_rmse: 0.71396 | val_1_rmse: 0.77474 |  0:00:42s
epoch 66 | loss: 0.4543  | val_0_rmse: 0.6997  | val_1_rmse: 0.76524 |  0:00:42s
epoch 67 | loss: 0.45344 | val_0_rmse: 0.70472 | val_1_rmse: 0.7681  |  0:00:43s
epoch 68 | loss: 0.44971 | val_0_rmse: 0.69255 | val_1_rmse: 0.77036 |  0:00:44s
epoch 69 | loss: 0.44872 | val_0_rmse: 0.68972 | val_1_rmse: 0.76931 |  0:00:44s
epoch 70 | loss: 0.44665 | val_0_rmse: 0.69228 | val_1_rmse: 0.77626 |  0:00:45s
epoch 71 | loss: 0.44869 | val_0_rmse: 0.6848  | val_1_rmse: 0.78526 |  0:00:46s
epoch 72 | loss: 0.43595 | val_0_rmse: 0.67573 | val_1_rmse: 0.78921 |  0:00:46s
epoch 73 | loss: 0.43087 | val_0_rmse: 0.6735  | val_1_rmse: 0.78041 |  0:00:47s
epoch 74 | loss: 0.42379 | val_0_rmse: 0.66207 | val_1_rmse: 0.77783 |  0:00:47s
epoch 75 | loss: 0.43145 | val_0_rmse: 0.66182 | val_1_rmse: 0.7686  |  0:00:48s
epoch 76 | loss: 0.41965 | val_0_rmse: 0.65413 | val_1_rmse: 0.77314 |  0:00:49s
epoch 77 | loss: 0.42585 | val_0_rmse: 0.65203 | val_1_rmse: 0.77956 |  0:00:49s
epoch 78 | loss: 0.41214 | val_0_rmse: 0.65207 | val_1_rmse: 0.78065 |  0:00:50s
epoch 79 | loss: 0.41536 | val_0_rmse: 0.64652 | val_1_rmse: 0.78848 |  0:00:51s
epoch 80 | loss: 0.41517 | val_0_rmse: 0.64416 | val_1_rmse: 0.78305 |  0:00:51s
epoch 81 | loss: 0.418   | val_0_rmse: 0.64065 | val_1_rmse: 0.77279 |  0:00:52s
epoch 82 | loss: 0.40954 | val_0_rmse: 0.63157 | val_1_rmse: 0.78437 |  0:00:53s
epoch 83 | loss: 0.41212 | val_0_rmse: 0.63274 | val_1_rmse: 0.80496 |  0:00:53s
epoch 84 | loss: 0.40584 | val_0_rmse: 0.6323  | val_1_rmse: 0.78579 |  0:00:54s
epoch 85 | loss: 0.39873 | val_0_rmse: 0.62931 | val_1_rmse: 0.80543 |  0:00:54s
epoch 86 | loss: 0.39657 | val_0_rmse: 0.62179 | val_1_rmse: 0.7865  |  0:00:55s
epoch 87 | loss: 0.39085 | val_0_rmse: 0.61513 | val_1_rmse: 0.79045 |  0:00:56s
epoch 88 | loss: 0.38965 | val_0_rmse: 0.61179 | val_1_rmse: 0.78134 |  0:00:56s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.75682
Best weights from best epoch are automatically used!
ended training at: 05:11:46
Feature importance:
Mean squared error is of 0.04820712720121474
Mean absolute error:0.15989296976304718
MAPE:0.16680095406644993
R2 score:0.5193517824989722
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:11:46
epoch 0  | loss: 2.13476 | val_0_rmse: 0.99305 | val_1_rmse: 0.99365 |  0:00:00s
epoch 1  | loss: 1.33805 | val_0_rmse: 0.99322 | val_1_rmse: 0.99328 |  0:00:01s
epoch 2  | loss: 1.05709 | val_0_rmse: 0.99498 | val_1_rmse: 0.99515 |  0:00:01s
epoch 3  | loss: 1.00458 | val_0_rmse: 0.99086 | val_1_rmse: 0.99196 |  0:00:02s
epoch 4  | loss: 0.9772  | val_0_rmse: 0.95271 | val_1_rmse: 0.96172 |  0:00:03s
epoch 5  | loss: 0.9455  | val_0_rmse: 0.94114 | val_1_rmse: 0.95814 |  0:00:03s
epoch 6  | loss: 0.92684 | val_0_rmse: 0.94101 | val_1_rmse: 0.96117 |  0:00:04s
epoch 7  | loss: 0.90973 | val_0_rmse: 0.94516 | val_1_rmse: 0.9639  |  0:00:05s
epoch 8  | loss: 0.89852 | val_0_rmse: 0.93724 | val_1_rmse: 0.95987 |  0:00:05s
epoch 9  | loss: 0.88281 | val_0_rmse: 0.93416 | val_1_rmse: 0.95291 |  0:00:06s
epoch 10 | loss: 0.86079 | val_0_rmse: 0.92657 | val_1_rmse: 0.95407 |  0:00:07s
epoch 11 | loss: 0.85387 | val_0_rmse: 0.91809 | val_1_rmse: 0.93787 |  0:00:07s
epoch 12 | loss: 0.8571  | val_0_rmse: 0.91996 | val_1_rmse: 0.94267 |  0:00:08s
epoch 13 | loss: 0.85453 | val_0_rmse: 0.91995 | val_1_rmse: 0.94065 |  0:00:08s
epoch 14 | loss: 0.85224 | val_0_rmse: 0.91961 | val_1_rmse: 0.94118 |  0:00:09s
epoch 15 | loss: 0.85157 | val_0_rmse: 0.9214  | val_1_rmse: 0.94384 |  0:00:10s
epoch 16 | loss: 0.85682 | val_0_rmse: 0.91564 | val_1_rmse: 0.9368  |  0:00:10s
epoch 17 | loss: 0.84561 | val_0_rmse: 0.9175  | val_1_rmse: 0.94312 |  0:00:11s
epoch 18 | loss: 0.84776 | val_0_rmse: 0.91095 | val_1_rmse: 0.93497 |  0:00:12s
epoch 19 | loss: 0.83748 | val_0_rmse: 0.91029 | val_1_rmse: 0.93563 |  0:00:12s
epoch 20 | loss: 0.83949 | val_0_rmse: 0.91355 | val_1_rmse: 0.93913 |  0:00:13s
epoch 21 | loss: 0.84132 | val_0_rmse: 0.9066  | val_1_rmse: 0.93324 |  0:00:14s
epoch 22 | loss: 0.83321 | val_0_rmse: 0.90123 | val_1_rmse: 0.92964 |  0:00:14s
epoch 23 | loss: 0.82829 | val_0_rmse: 0.90893 | val_1_rmse: 0.93805 |  0:00:15s
epoch 24 | loss: 0.8196  | val_0_rmse: 0.91591 | val_1_rmse: 0.95571 |  0:00:15s
epoch 25 | loss: 0.82601 | val_0_rmse: 0.88858 | val_1_rmse: 0.924   |  0:00:16s
epoch 26 | loss: 0.81831 | val_0_rmse: 0.89487 | val_1_rmse: 0.92786 |  0:00:17s
epoch 27 | loss: 0.81208 | val_0_rmse: 0.89154 | val_1_rmse: 0.91715 |  0:00:17s
epoch 28 | loss: 0.80438 | val_0_rmse: 0.884   | val_1_rmse: 0.9198  |  0:00:18s
epoch 29 | loss: 0.8009  | val_0_rmse: 0.88744 | val_1_rmse: 0.92137 |  0:00:19s
epoch 30 | loss: 0.80144 | val_0_rmse: 0.87692 | val_1_rmse: 0.9155  |  0:00:19s
epoch 31 | loss: 0.79782 | val_0_rmse: 0.88216 | val_1_rmse: 0.91567 |  0:00:20s
epoch 32 | loss: 0.80826 | val_0_rmse: 0.89441 | val_1_rmse: 0.91703 |  0:00:21s
epoch 33 | loss: 0.80715 | val_0_rmse: 0.89143 | val_1_rmse: 0.91568 |  0:00:21s
epoch 34 | loss: 0.80771 | val_0_rmse: 0.88542 | val_1_rmse: 0.90779 |  0:00:22s
epoch 35 | loss: 0.78542 | val_0_rmse: 0.89469 | val_1_rmse: 0.92222 |  0:00:23s
epoch 36 | loss: 0.77987 | val_0_rmse: 0.87957 | val_1_rmse: 0.90611 |  0:00:23s
epoch 37 | loss: 0.77419 | val_0_rmse: 0.87261 | val_1_rmse: 0.89668 |  0:00:24s
epoch 38 | loss: 0.76814 | val_0_rmse: 0.87206 | val_1_rmse: 0.8992  |  0:00:24s
epoch 39 | loss: 0.7544  | val_0_rmse: 0.86594 | val_1_rmse: 0.89233 |  0:00:25s
epoch 40 | loss: 0.75197 | val_0_rmse: 0.86542 | val_1_rmse: 0.89624 |  0:00:26s
epoch 41 | loss: 0.73729 | val_0_rmse: 0.8714  | val_1_rmse: 0.90131 |  0:00:26s
epoch 42 | loss: 0.73622 | val_0_rmse: 0.86049 | val_1_rmse: 0.89656 |  0:00:27s
epoch 43 | loss: 0.72979 | val_0_rmse: 0.85169 | val_1_rmse: 0.89321 |  0:00:28s
epoch 44 | loss: 0.72595 | val_0_rmse: 0.85335 | val_1_rmse: 0.89519 |  0:00:28s
epoch 45 | loss: 0.71587 | val_0_rmse: 0.84482 | val_1_rmse: 0.89328 |  0:00:29s
epoch 46 | loss: 0.71314 | val_0_rmse: 0.84201 | val_1_rmse: 0.89328 |  0:00:30s
epoch 47 | loss: 0.70238 | val_0_rmse: 0.85383 | val_1_rmse: 0.91156 |  0:00:30s
epoch 48 | loss: 0.69759 | val_0_rmse: 0.8354  | val_1_rmse: 0.89459 |  0:00:31s
epoch 49 | loss: 0.69147 | val_0_rmse: 0.84158 | val_1_rmse: 0.90297 |  0:00:31s
epoch 50 | loss: 0.69309 | val_0_rmse: 0.83049 | val_1_rmse: 0.90033 |  0:00:32s
epoch 51 | loss: 0.67696 | val_0_rmse: 0.85657 | val_1_rmse: 0.9196  |  0:00:33s
epoch 52 | loss: 0.69292 | val_0_rmse: 0.83345 | val_1_rmse: 0.89197 |  0:00:33s
epoch 53 | loss: 0.66955 | val_0_rmse: 0.82794 | val_1_rmse: 0.89037 |  0:00:34s
epoch 54 | loss: 0.66389 | val_0_rmse: 0.82491 | val_1_rmse: 0.89211 |  0:00:35s
epoch 55 | loss: 0.66867 | val_0_rmse: 0.81879 | val_1_rmse: 0.88381 |  0:00:35s
epoch 56 | loss: 0.66375 | val_0_rmse: 0.82929 | val_1_rmse: 0.8984  |  0:00:36s
epoch 57 | loss: 0.66391 | val_0_rmse: 0.83465 | val_1_rmse: 0.90869 |  0:00:37s
epoch 58 | loss: 0.67397 | val_0_rmse: 0.83148 | val_1_rmse: 0.89844 |  0:00:37s
epoch 59 | loss: 0.66814 | val_0_rmse: 0.82588 | val_1_rmse: 0.8899  |  0:00:38s
epoch 60 | loss: 0.64907 | val_0_rmse: 0.81099 | val_1_rmse: 0.87554 |  0:00:38s
epoch 61 | loss: 0.63748 | val_0_rmse: 0.80151 | val_1_rmse: 0.86986 |  0:00:39s
epoch 62 | loss: 0.64728 | val_0_rmse: 0.81099 | val_1_rmse: 0.87448 |  0:00:40s
epoch 63 | loss: 0.65378 | val_0_rmse: 0.81225 | val_1_rmse: 0.88207 |  0:00:40s
epoch 64 | loss: 0.63509 | val_0_rmse: 0.82766 | val_1_rmse: 0.89991 |  0:00:41s
epoch 65 | loss: 0.61707 | val_0_rmse: 0.79981 | val_1_rmse: 0.86901 |  0:00:42s
epoch 66 | loss: 0.61581 | val_0_rmse: 0.79431 | val_1_rmse: 0.86972 |  0:00:42s
epoch 67 | loss: 0.6339  | val_0_rmse: 0.78853 | val_1_rmse: 0.86645 |  0:00:43s
epoch 68 | loss: 0.62023 | val_0_rmse: 0.79855 | val_1_rmse: 0.87495 |  0:00:44s
epoch 69 | loss: 0.62505 | val_0_rmse: 0.84077 | val_1_rmse: 0.91162 |  0:00:44s
epoch 70 | loss: 0.63173 | val_0_rmse: 0.81033 | val_1_rmse: 0.88392 |  0:00:45s
epoch 71 | loss: 0.62688 | val_0_rmse: 0.78553 | val_1_rmse: 0.8675  |  0:00:45s
epoch 72 | loss: 0.60268 | val_0_rmse: 0.78056 | val_1_rmse: 0.87806 |  0:00:46s
epoch 73 | loss: 0.58589 | val_0_rmse: 0.76479 | val_1_rmse: 0.86356 |  0:00:47s
epoch 74 | loss: 0.59347 | val_0_rmse: 0.80106 | val_1_rmse: 0.87204 |  0:00:47s
epoch 75 | loss: 0.61115 | val_0_rmse: 0.76884 | val_1_rmse: 0.85333 |  0:00:48s
epoch 76 | loss: 0.59556 | val_0_rmse: 0.7533  | val_1_rmse: 0.85344 |  0:00:49s
epoch 77 | loss: 0.57976 | val_0_rmse: 0.75107 | val_1_rmse: 0.85286 |  0:00:49s
epoch 78 | loss: 0.57118 | val_0_rmse: 0.74996 | val_1_rmse: 0.84725 |  0:00:50s
epoch 79 | loss: 0.55222 | val_0_rmse: 0.76149 | val_1_rmse: 0.86537 |  0:00:51s
epoch 80 | loss: 0.55155 | val_0_rmse: 0.73051 | val_1_rmse: 0.85145 |  0:00:51s
epoch 81 | loss: 0.5616  | val_0_rmse: 0.74467 | val_1_rmse: 0.85287 |  0:00:52s
epoch 82 | loss: 0.55318 | val_0_rmse: 0.72945 | val_1_rmse: 0.82893 |  0:00:52s
epoch 83 | loss: 0.54597 | val_0_rmse: 0.71817 | val_1_rmse: 0.83335 |  0:00:53s
epoch 84 | loss: 0.53744 | val_0_rmse: 0.72229 | val_1_rmse: 0.8217  |  0:00:54s
epoch 85 | loss: 0.53744 | val_0_rmse: 0.72287 | val_1_rmse: 0.8263  |  0:00:54s
epoch 86 | loss: 0.52371 | val_0_rmse: 0.72048 | val_1_rmse: 0.8277  |  0:00:55s
epoch 87 | loss: 0.51743 | val_0_rmse: 0.71529 | val_1_rmse: 0.84053 |  0:00:56s
epoch 88 | loss: 0.5133  | val_0_rmse: 0.70081 | val_1_rmse: 0.81624 |  0:00:56s
epoch 89 | loss: 0.5043  | val_0_rmse: 0.69262 | val_1_rmse: 0.8086  |  0:00:57s
epoch 90 | loss: 0.48936 | val_0_rmse: 0.682   | val_1_rmse: 0.81125 |  0:00:58s
epoch 91 | loss: 0.48658 | val_0_rmse: 0.6771  | val_1_rmse: 0.82247 |  0:00:58s
epoch 92 | loss: 0.4858  | val_0_rmse: 0.67526 | val_1_rmse: 0.81102 |  0:00:59s
epoch 93 | loss: 0.4761  | val_0_rmse: 0.67309 | val_1_rmse: 0.8361  |  0:01:00s
epoch 94 | loss: 0.46134 | val_0_rmse: 0.67089 | val_1_rmse: 0.82653 |  0:01:00s
epoch 95 | loss: 0.46821 | val_0_rmse: 0.67006 | val_1_rmse: 0.81414 |  0:01:01s
epoch 96 | loss: 0.46141 | val_0_rmse: 0.66456 | val_1_rmse: 0.84134 |  0:01:01s
epoch 97 | loss: 0.45557 | val_0_rmse: 0.67295 | val_1_rmse: 0.82511 |  0:01:02s
epoch 98 | loss: 0.4561  | val_0_rmse: 0.65671 | val_1_rmse: 0.83155 |  0:01:03s
epoch 99 | loss: 0.44657 | val_0_rmse: 0.65401 | val_1_rmse: 0.84618 |  0:01:03s
epoch 100| loss: 0.46199 | val_0_rmse: 0.65936 | val_1_rmse: 0.8096  |  0:01:04s
epoch 101| loss: 0.44812 | val_0_rmse: 0.64491 | val_1_rmse: 0.82186 |  0:01:05s
epoch 102| loss: 0.4417  | val_0_rmse: 0.6464  | val_1_rmse: 0.83259 |  0:01:05s
epoch 103| loss: 0.44259 | val_0_rmse: 0.65267 | val_1_rmse: 0.83454 |  0:01:06s
epoch 104| loss: 0.45048 | val_0_rmse: 0.64586 | val_1_rmse: 0.83264 |  0:01:06s
epoch 105| loss: 0.44866 | val_0_rmse: 0.6416  | val_1_rmse: 0.8268  |  0:01:07s
epoch 106| loss: 0.43455 | val_0_rmse: 0.63414 | val_1_rmse: 0.84205 |  0:01:08s
epoch 107| loss: 0.43951 | val_0_rmse: 0.62997 | val_1_rmse: 0.85529 |  0:01:08s
epoch 108| loss: 0.42965 | val_0_rmse: 0.65495 | val_1_rmse: 0.83636 |  0:01:09s
epoch 109| loss: 0.43127 | val_0_rmse: 0.63879 | val_1_rmse: 0.86453 |  0:01:10s
epoch 110| loss: 0.41254 | val_0_rmse: 0.62202 | val_1_rmse: 0.84675 |  0:01:10s
epoch 111| loss: 0.40316 | val_0_rmse: 0.61019 | val_1_rmse: 0.83865 |  0:01:11s
epoch 112| loss: 0.40339 | val_0_rmse: 0.62997 | val_1_rmse: 0.85698 |  0:01:11s
epoch 113| loss: 0.39863 | val_0_rmse: 0.60232 | val_1_rmse: 0.84243 |  0:01:12s
epoch 114| loss: 0.39698 | val_0_rmse: 0.61125 | val_1_rmse: 0.85896 |  0:01:13s
epoch 115| loss: 0.40258 | val_0_rmse: 0.59059 | val_1_rmse: 0.83873 |  0:01:13s
epoch 116| loss: 0.38882 | val_0_rmse: 0.60129 | val_1_rmse: 0.83185 |  0:01:14s
epoch 117| loss: 0.38768 | val_0_rmse: 0.62719 | val_1_rmse: 0.89988 |  0:01:15s
epoch 118| loss: 0.39517 | val_0_rmse: 0.60393 | val_1_rmse: 0.84161 |  0:01:16s
epoch 119| loss: 0.3856  | val_0_rmse: 0.58923 | val_1_rmse: 0.85122 |  0:01:16s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.8086
Best weights from best epoch are automatically used!
ended training at: 05:13:03
Feature importance:
Mean squared error is of 0.05943099714101964
Mean absolute error:0.16598165336025772
MAPE:0.1719672864404539
R2 score:0.41440610360142494
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:04
epoch 0  | loss: 3.5266  | val_0_rmse: 1.00005 | val_1_rmse: 0.95879 |  0:00:00s
epoch 1  | loss: 2.59026 | val_0_rmse: 1.00061 | val_1_rmse: 0.95763 |  0:00:00s
epoch 2  | loss: 1.7733  | val_0_rmse: 0.99982 | val_1_rmse: 0.95237 |  0:00:00s
epoch 3  | loss: 1.58169 | val_0_rmse: 1.00088 | val_1_rmse: 0.94896 |  0:00:00s
epoch 4  | loss: 1.81964 | val_0_rmse: 1.0035  | val_1_rmse: 0.94639 |  0:00:00s
epoch 5  | loss: 1.31593 | val_0_rmse: 1.00364 | val_1_rmse: 0.94848 |  0:00:00s
epoch 6  | loss: 1.32633 | val_0_rmse: 1.00457 | val_1_rmse: 0.94793 |  0:00:00s
epoch 7  | loss: 1.26131 | val_0_rmse: 1.00515 | val_1_rmse: 0.94766 |  0:00:00s
epoch 8  | loss: 1.13344 | val_0_rmse: 1.0013  | val_1_rmse: 0.95082 |  0:00:00s
epoch 9  | loss: 1.06473 | val_0_rmse: 1.00034 | val_1_rmse: 0.95223 |  0:00:00s
epoch 10 | loss: 1.12873 | val_0_rmse: 1.00023 | val_1_rmse: 0.95349 |  0:00:00s
epoch 11 | loss: 1.03171 | val_0_rmse: 0.99967 | val_1_rmse: 0.95582 |  0:00:01s
epoch 12 | loss: 1.0536  | val_0_rmse: 0.99952 | val_1_rmse: 0.95415 |  0:00:01s
epoch 13 | loss: 1.05434 | val_0_rmse: 0.9996  | val_1_rmse: 0.95521 |  0:00:01s
epoch 14 | loss: 1.01878 | val_0_rmse: 1.00002 | val_1_rmse: 0.95498 |  0:00:01s
epoch 15 | loss: 1.01151 | val_0_rmse: 1.00024 | val_1_rmse: 0.95502 |  0:00:01s
epoch 16 | loss: 0.97829 | val_0_rmse: 0.99893 | val_1_rmse: 0.95186 |  0:00:01s
epoch 17 | loss: 0.94313 | val_0_rmse: 0.99869 | val_1_rmse: 0.94905 |  0:00:01s
epoch 18 | loss: 0.99641 | val_0_rmse: 0.99895 | val_1_rmse: 0.9473  |  0:00:01s
epoch 19 | loss: 0.99987 | val_0_rmse: 0.99997 | val_1_rmse: 0.94689 |  0:00:01s
epoch 20 | loss: 0.99335 | val_0_rmse: 1.00053 | val_1_rmse: 0.94838 |  0:00:01s
epoch 21 | loss: 0.97811 | val_0_rmse: 1.00083 | val_1_rmse: 0.94855 |  0:00:01s
epoch 22 | loss: 0.94362 | val_0_rmse: 1.00059 | val_1_rmse: 0.94397 |  0:00:01s
epoch 23 | loss: 0.95123 | val_0_rmse: 0.99945 | val_1_rmse: 0.93936 |  0:00:02s
epoch 24 | loss: 0.9627  | val_0_rmse: 0.99761 | val_1_rmse: 0.93698 |  0:00:02s
epoch 25 | loss: 0.9576  | val_0_rmse: 0.99438 | val_1_rmse: 0.93477 |  0:00:02s
epoch 26 | loss: 0.93825 | val_0_rmse: 0.99287 | val_1_rmse: 0.93555 |  0:00:02s
epoch 27 | loss: 0.94712 | val_0_rmse: 0.99262 | val_1_rmse: 0.93355 |  0:00:02s
epoch 28 | loss: 0.92123 | val_0_rmse: 0.99296 | val_1_rmse: 0.92761 |  0:00:02s
epoch 29 | loss: 0.92715 | val_0_rmse: 0.99746 | val_1_rmse: 0.91728 |  0:00:02s
epoch 30 | loss: 0.90853 | val_0_rmse: 1.00257 | val_1_rmse: 0.90934 |  0:00:02s
epoch 31 | loss: 0.89945 | val_0_rmse: 1.0094  | val_1_rmse: 0.91141 |  0:00:02s
epoch 32 | loss: 0.88126 | val_0_rmse: 1.0139  | val_1_rmse: 0.91606 |  0:00:02s
epoch 33 | loss: 0.86121 | val_0_rmse: 1.01869 | val_1_rmse: 0.92468 |  0:00:02s
epoch 34 | loss: 0.89371 | val_0_rmse: 1.02003 | val_1_rmse: 0.92153 |  0:00:03s
epoch 35 | loss: 0.83426 | val_0_rmse: 1.02128 | val_1_rmse: 0.92304 |  0:00:03s
epoch 36 | loss: 0.84937 | val_0_rmse: 1.01822 | val_1_rmse: 0.92095 |  0:00:03s
epoch 37 | loss: 0.85473 | val_0_rmse: 1.00534 | val_1_rmse: 0.91892 |  0:00:03s
epoch 38 | loss: 0.85307 | val_0_rmse: 1.00269 | val_1_rmse: 0.93053 |  0:00:03s
epoch 39 | loss: 0.84368 | val_0_rmse: 1.01387 | val_1_rmse: 0.95045 |  0:00:03s
epoch 40 | loss: 0.82488 | val_0_rmse: 1.02446 | val_1_rmse: 0.95994 |  0:00:03s
epoch 41 | loss: 0.80363 | val_0_rmse: 1.04405 | val_1_rmse: 0.96137 |  0:00:03s
epoch 42 | loss: 0.83924 | val_0_rmse: 1.05375 | val_1_rmse: 0.96197 |  0:00:03s
epoch 43 | loss: 0.83215 | val_0_rmse: 1.06377 | val_1_rmse: 0.96799 |  0:00:03s
epoch 44 | loss: 0.81936 | val_0_rmse: 1.07512 | val_1_rmse: 0.96895 |  0:00:03s
epoch 45 | loss: 0.81942 | val_0_rmse: 1.08952 | val_1_rmse: 0.96375 |  0:00:03s
epoch 46 | loss: 0.83148 | val_0_rmse: 1.09528 | val_1_rmse: 0.95841 |  0:00:04s
epoch 47 | loss: 0.82203 | val_0_rmse: 1.06694 | val_1_rmse: 0.94514 |  0:00:04s
epoch 48 | loss: 0.83833 | val_0_rmse: 1.05896 | val_1_rmse: 0.93696 |  0:00:04s
epoch 49 | loss: 0.83087 | val_0_rmse: 1.06147 | val_1_rmse: 0.95685 |  0:00:04s
epoch 50 | loss: 0.8289  | val_0_rmse: 1.07308 | val_1_rmse: 0.96277 |  0:00:04s
epoch 51 | loss: 0.85478 | val_0_rmse: 1.05919 | val_1_rmse: 0.9639  |  0:00:04s
epoch 52 | loss: 0.82917 | val_0_rmse: 1.02752 | val_1_rmse: 0.96267 |  0:00:04s
epoch 53 | loss: 0.8286  | val_0_rmse: 1.02485 | val_1_rmse: 0.97902 |  0:00:04s
epoch 54 | loss: 0.84538 | val_0_rmse: 1.02476 | val_1_rmse: 0.98452 |  0:00:04s
epoch 55 | loss: 0.81967 | val_0_rmse: 1.02306 | val_1_rmse: 0.97908 |  0:00:04s
epoch 56 | loss: 0.81648 | val_0_rmse: 1.01695 | val_1_rmse: 0.96439 |  0:00:04s
epoch 57 | loss: 0.81445 | val_0_rmse: 1.00996 | val_1_rmse: 0.95638 |  0:00:05s
epoch 58 | loss: 0.79727 | val_0_rmse: 1.00415 | val_1_rmse: 0.95476 |  0:00:05s
epoch 59 | loss: 0.78721 | val_0_rmse: 1.00146 | val_1_rmse: 0.94964 |  0:00:05s
epoch 60 | loss: 0.82697 | val_0_rmse: 0.99936 | val_1_rmse: 0.95337 |  0:00:05s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.90934
Best weights from best epoch are automatically used!
ended training at: 05:13:09
Feature importance:
Mean squared error is of 0.09732849366833016
Mean absolute error:0.20479948860259867
MAPE:0.21738885193695853
R2 score:0.047292791517837496
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:10
epoch 0  | loss: 3.37034 | val_0_rmse: 1.03123 | val_1_rmse: 1.04473 |  0:00:00s
epoch 1  | loss: 3.24576 | val_0_rmse: 1.01762 | val_1_rmse: 1.01626 |  0:00:00s
epoch 2  | loss: 1.96375 | val_0_rmse: 1.01136 | val_1_rmse: 0.99364 |  0:00:00s
epoch 3  | loss: 1.88984 | val_0_rmse: 1.01262 | val_1_rmse: 0.98736 |  0:00:00s
epoch 4  | loss: 1.70344 | val_0_rmse: 1.01281 | val_1_rmse: 0.99681 |  0:00:00s
epoch 5  | loss: 1.49294 | val_0_rmse: 1.01537 | val_1_rmse: 1.00725 |  0:00:00s
epoch 6  | loss: 1.25725 | val_0_rmse: 1.01667 | val_1_rmse: 1.01036 |  0:00:00s
epoch 7  | loss: 1.31142 | val_0_rmse: 1.01692 | val_1_rmse: 1.01284 |  0:00:00s
epoch 8  | loss: 1.14708 | val_0_rmse: 1.01215 | val_1_rmse: 0.99419 |  0:00:00s
epoch 9  | loss: 1.21532 | val_0_rmse: 1.0129  | val_1_rmse: 0.98918 |  0:00:00s
epoch 10 | loss: 1.11319 | val_0_rmse: 1.01278 | val_1_rmse: 0.98969 |  0:00:00s
epoch 11 | loss: 1.1204  | val_0_rmse: 1.01117 | val_1_rmse: 0.99365 |  0:00:01s
epoch 12 | loss: 1.06781 | val_0_rmse: 1.01048 | val_1_rmse: 0.99442 |  0:00:01s
epoch 13 | loss: 1.08454 | val_0_rmse: 1.01026 | val_1_rmse: 0.99053 |  0:00:01s
epoch 14 | loss: 1.06069 | val_0_rmse: 1.01086 | val_1_rmse: 0.98617 |  0:00:01s
epoch 15 | loss: 1.01609 | val_0_rmse: 1.01049 | val_1_rmse: 0.98631 |  0:00:01s
epoch 16 | loss: 1.0154  | val_0_rmse: 1.00958 | val_1_rmse: 0.98776 |  0:00:01s
epoch 17 | loss: 1.04854 | val_0_rmse: 1.00956 | val_1_rmse: 0.99221 |  0:00:01s
epoch 18 | loss: 1.03663 | val_0_rmse: 1.01039 | val_1_rmse: 0.99566 |  0:00:01s
epoch 19 | loss: 1.01484 | val_0_rmse: 1.00987 | val_1_rmse: 0.99504 |  0:00:01s
epoch 20 | loss: 1.01999 | val_0_rmse: 1.00814 | val_1_rmse: 0.98949 |  0:00:01s
epoch 21 | loss: 0.9983  | val_0_rmse: 1.00346 | val_1_rmse: 0.98226 |  0:00:01s
epoch 22 | loss: 1.03307 | val_0_rmse: 1.00284 | val_1_rmse: 0.98357 |  0:00:01s
epoch 23 | loss: 0.99982 | val_0_rmse: 1.00183 | val_1_rmse: 0.98527 |  0:00:02s
epoch 24 | loss: 0.97489 | val_0_rmse: 0.99912 | val_1_rmse: 0.98582 |  0:00:02s
epoch 25 | loss: 0.96494 | val_0_rmse: 0.99704 | val_1_rmse: 0.9871  |  0:00:02s
epoch 26 | loss: 0.95857 | val_0_rmse: 0.99586 | val_1_rmse: 0.98956 |  0:00:02s
epoch 27 | loss: 0.96904 | val_0_rmse: 0.9951  | val_1_rmse: 0.98894 |  0:00:02s
epoch 28 | loss: 0.96205 | val_0_rmse: 0.99385 | val_1_rmse: 0.98488 |  0:00:02s
epoch 29 | loss: 0.95003 | val_0_rmse: 0.99333 | val_1_rmse: 0.97985 |  0:00:02s
epoch 30 | loss: 0.9511  | val_0_rmse: 0.99291 | val_1_rmse: 0.97619 |  0:00:02s
epoch 31 | loss: 0.94153 | val_0_rmse: 0.99115 | val_1_rmse: 0.9745  |  0:00:02s
epoch 32 | loss: 0.93538 | val_0_rmse: 0.9877  | val_1_rmse: 0.97549 |  0:00:02s
epoch 33 | loss: 0.93596 | val_0_rmse: 0.98429 | val_1_rmse: 0.97743 |  0:00:02s
epoch 34 | loss: 0.93354 | val_0_rmse: 0.98024 | val_1_rmse: 0.98005 |  0:00:03s
epoch 35 | loss: 0.92036 | val_0_rmse: 0.97584 | val_1_rmse: 0.97961 |  0:00:03s
epoch 36 | loss: 0.906   | val_0_rmse: 0.97311 | val_1_rmse: 0.97979 |  0:00:03s
epoch 37 | loss: 0.90078 | val_0_rmse: 0.97169 | val_1_rmse: 0.98077 |  0:00:03s
epoch 38 | loss: 0.90067 | val_0_rmse: 0.96828 | val_1_rmse: 0.98161 |  0:00:03s
epoch 39 | loss: 0.90138 | val_0_rmse: 0.96754 | val_1_rmse: 0.97993 |  0:00:03s
epoch 40 | loss: 0.9067  | val_0_rmse: 0.96799 | val_1_rmse: 0.97381 |  0:00:03s
epoch 41 | loss: 0.9104  | val_0_rmse: 0.96904 | val_1_rmse: 0.97384 |  0:00:03s
epoch 42 | loss: 0.87325 | val_0_rmse: 0.96913 | val_1_rmse: 0.97418 |  0:00:03s
epoch 43 | loss: 0.89446 | val_0_rmse: 0.9705  | val_1_rmse: 0.97238 |  0:00:03s
epoch 44 | loss: 0.90103 | val_0_rmse: 0.97227 | val_1_rmse: 0.9697  |  0:00:03s
epoch 45 | loss: 0.88165 | val_0_rmse: 0.97604 | val_1_rmse: 0.96779 |  0:00:03s
epoch 46 | loss: 0.89877 | val_0_rmse: 0.97455 | val_1_rmse: 0.96873 |  0:00:04s
epoch 47 | loss: 0.86225 | val_0_rmse: 0.97432 | val_1_rmse: 0.97054 |  0:00:04s
epoch 48 | loss: 0.85692 | val_0_rmse: 0.97617 | val_1_rmse: 0.97388 |  0:00:04s
epoch 49 | loss: 0.85036 | val_0_rmse: 0.98277 | val_1_rmse: 0.9768  |  0:00:04s
epoch 50 | loss: 0.86455 | val_0_rmse: 0.98644 | val_1_rmse: 0.97326 |  0:00:04s
epoch 51 | loss: 0.85984 | val_0_rmse: 0.98394 | val_1_rmse: 0.96848 |  0:00:04s
epoch 52 | loss: 0.8527  | val_0_rmse: 0.97636 | val_1_rmse: 0.96448 |  0:00:04s
epoch 53 | loss: 0.83711 | val_0_rmse: 0.96879 | val_1_rmse: 0.96453 |  0:00:04s
epoch 54 | loss: 0.84891 | val_0_rmse: 0.96764 | val_1_rmse: 0.9641  |  0:00:04s
epoch 55 | loss: 0.82667 | val_0_rmse: 0.96816 | val_1_rmse: 0.96369 |  0:00:04s
epoch 56 | loss: 0.82487 | val_0_rmse: 0.97152 | val_1_rmse: 0.96647 |  0:00:04s
epoch 57 | loss: 0.82599 | val_0_rmse: 0.97214 | val_1_rmse: 0.97147 |  0:00:05s
epoch 58 | loss: 0.80674 | val_0_rmse: 0.96817 | val_1_rmse: 0.97461 |  0:00:05s
epoch 59 | loss: 0.83547 | val_0_rmse: 0.96801 | val_1_rmse: 0.97927 |  0:00:05s
epoch 60 | loss: 0.81844 | val_0_rmse: 0.96848 | val_1_rmse: 0.97406 |  0:00:05s
epoch 61 | loss: 0.80928 | val_0_rmse: 0.97466 | val_1_rmse: 0.97229 |  0:00:05s
epoch 62 | loss: 0.7936  | val_0_rmse: 0.98134 | val_1_rmse: 0.97241 |  0:00:05s
epoch 63 | loss: 0.79122 | val_0_rmse: 0.97824 | val_1_rmse: 0.97023 |  0:00:05s
epoch 64 | loss: 0.79289 | val_0_rmse: 0.97407 | val_1_rmse: 0.97174 |  0:00:05s
epoch 65 | loss: 0.785   | val_0_rmse: 0.97603 | val_1_rmse: 0.97798 |  0:00:05s
epoch 66 | loss: 0.77956 | val_0_rmse: 0.9791  | val_1_rmse: 0.98156 |  0:00:05s
epoch 67 | loss: 0.75921 | val_0_rmse: 0.98719 | val_1_rmse: 0.98342 |  0:00:05s
epoch 68 | loss: 0.74775 | val_0_rmse: 0.98263 | val_1_rmse: 0.97608 |  0:00:05s
epoch 69 | loss: 0.72636 | val_0_rmse: 0.97649 | val_1_rmse: 0.97222 |  0:00:06s
epoch 70 | loss: 0.73697 | val_0_rmse: 0.97528 | val_1_rmse: 0.96545 |  0:00:06s
epoch 71 | loss: 0.72189 | val_0_rmse: 0.9709  | val_1_rmse: 0.95215 |  0:00:06s
epoch 72 | loss: 0.68695 | val_0_rmse: 0.96577 | val_1_rmse: 0.94437 |  0:00:06s
epoch 73 | loss: 0.71176 | val_0_rmse: 0.96802 | val_1_rmse: 0.94825 |  0:00:06s
epoch 74 | loss: 0.72756 | val_0_rmse: 0.97266 | val_1_rmse: 0.9668  |  0:00:06s
epoch 75 | loss: 0.68892 | val_0_rmse: 0.97073 | val_1_rmse: 0.97904 |  0:00:06s
epoch 76 | loss: 0.71329 | val_0_rmse: 0.9709  | val_1_rmse: 0.98561 |  0:00:06s
epoch 77 | loss: 0.68295 | val_0_rmse: 0.97239 | val_1_rmse: 0.99822 |  0:00:06s
epoch 78 | loss: 0.68574 | val_0_rmse: 0.97612 | val_1_rmse: 1.00456 |  0:00:06s
epoch 79 | loss: 0.68436 | val_0_rmse: 0.97253 | val_1_rmse: 1.00088 |  0:00:06s
epoch 80 | loss: 0.66286 | val_0_rmse: 0.97636 | val_1_rmse: 1.00427 |  0:00:06s
epoch 81 | loss: 0.67854 | val_0_rmse: 0.96841 | val_1_rmse: 0.98317 |  0:00:07s
epoch 82 | loss: 0.67641 | val_0_rmse: 0.95963 | val_1_rmse: 0.9657  |  0:00:07s
epoch 83 | loss: 0.66849 | val_0_rmse: 0.95054 | val_1_rmse: 0.95475 |  0:00:07s
epoch 84 | loss: 0.66037 | val_0_rmse: 0.94963 | val_1_rmse: 0.9534  |  0:00:07s
epoch 85 | loss: 0.64404 | val_0_rmse: 0.95268 | val_1_rmse: 0.96087 |  0:00:07s
epoch 86 | loss: 0.64601 | val_0_rmse: 0.95542 | val_1_rmse: 0.97185 |  0:00:07s
epoch 87 | loss: 0.64149 | val_0_rmse: 0.94929 | val_1_rmse: 0.96917 |  0:00:07s
epoch 88 | loss: 0.61893 | val_0_rmse: 0.94053 | val_1_rmse: 0.95468 |  0:00:07s
epoch 89 | loss: 0.60032 | val_0_rmse: 0.92869 | val_1_rmse: 0.94698 |  0:00:07s
epoch 90 | loss: 0.5907  | val_0_rmse: 0.91935 | val_1_rmse: 0.94047 |  0:00:07s
epoch 91 | loss: 0.58687 | val_0_rmse: 0.90855 | val_1_rmse: 0.93342 |  0:00:07s
epoch 92 | loss: 0.56791 | val_0_rmse: 0.90825 | val_1_rmse: 0.93504 |  0:00:07s
epoch 93 | loss: 0.57479 | val_0_rmse: 0.90987 | val_1_rmse: 0.93892 |  0:00:08s
epoch 94 | loss: 0.55504 | val_0_rmse: 0.91509 | val_1_rmse: 0.93977 |  0:00:08s
epoch 95 | loss: 0.55465 | val_0_rmse: 0.91973 | val_1_rmse: 0.94478 |  0:00:08s
epoch 96 | loss: 0.5301  | val_0_rmse: 0.9366  | val_1_rmse: 0.95456 |  0:00:08s
epoch 97 | loss: 0.5199  | val_0_rmse: 0.94086 | val_1_rmse: 0.95872 |  0:00:08s
epoch 98 | loss: 0.51403 | val_0_rmse: 0.93093 | val_1_rmse: 0.95335 |  0:00:08s
epoch 99 | loss: 0.50998 | val_0_rmse: 0.92231 | val_1_rmse: 0.94691 |  0:00:08s
epoch 100| loss: 0.5212  | val_0_rmse: 0.92446 | val_1_rmse: 0.95335 |  0:00:08s
epoch 101| loss: 0.54076 | val_0_rmse: 0.92681 | val_1_rmse: 0.962   |  0:00:08s
epoch 102| loss: 0.52274 | val_0_rmse: 0.93059 | val_1_rmse: 0.97182 |  0:00:08s
epoch 103| loss: 0.52269 | val_0_rmse: 0.93628 | val_1_rmse: 0.98283 |  0:00:08s
epoch 104| loss: 0.4916  | val_0_rmse: 0.95076 | val_1_rmse: 1.00038 |  0:00:08s
epoch 105| loss: 0.52453 | val_0_rmse: 0.95084 | val_1_rmse: 1.00188 |  0:00:09s
epoch 106| loss: 0.49198 | val_0_rmse: 0.94147 | val_1_rmse: 0.99971 |  0:00:09s
epoch 107| loss: 0.49764 | val_0_rmse: 0.93788 | val_1_rmse: 1.0029  |  0:00:09s
epoch 108| loss: 0.51728 | val_0_rmse: 0.94354 | val_1_rmse: 1.01392 |  0:00:09s
epoch 109| loss: 0.49107 | val_0_rmse: 0.93924 | val_1_rmse: 1.01164 |  0:00:09s
epoch 110| loss: 0.49069 | val_0_rmse: 0.93383 | val_1_rmse: 1.00819 |  0:00:09s
epoch 111| loss: 0.49217 | val_0_rmse: 0.94009 | val_1_rmse: 1.00796 |  0:00:09s
epoch 112| loss: 0.47053 | val_0_rmse: 0.95656 | val_1_rmse: 1.00452 |  0:00:09s
epoch 113| loss: 0.48916 | val_0_rmse: 0.95012 | val_1_rmse: 0.99448 |  0:00:09s
epoch 114| loss: 0.50044 | val_0_rmse: 0.94464 | val_1_rmse: 0.99111 |  0:00:09s
epoch 115| loss: 0.4882  | val_0_rmse: 0.95379 | val_1_rmse: 1.00228 |  0:00:09s
epoch 116| loss: 0.49167 | val_0_rmse: 0.96788 | val_1_rmse: 1.0199  |  0:00:10s
epoch 117| loss: 0.49327 | val_0_rmse: 0.96116 | val_1_rmse: 1.01957 |  0:00:10s
epoch 118| loss: 0.48317 | val_0_rmse: 0.96145 | val_1_rmse: 1.02323 |  0:00:10s
epoch 119| loss: 0.48422 | val_0_rmse: 0.95913 | val_1_rmse: 1.02121 |  0:00:10s
epoch 120| loss: 0.46444 | val_0_rmse: 0.95762 | val_1_rmse: 1.0104  |  0:00:10s
epoch 121| loss: 0.45621 | val_0_rmse: 0.96924 | val_1_rmse: 1.0093  |  0:00:10s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.93342
Best weights from best epoch are automatically used!
ended training at: 05:13:20
Feature importance:
Mean squared error is of 0.0829957489424992
Mean absolute error:0.21175849938953717
MAPE:0.2346859301564452
R2 score:-0.054283028347735574
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:20
epoch 0  | loss: 3.78611 | val_0_rmse: 1.03509 | val_1_rmse: 0.91706 |  0:00:00s
epoch 1  | loss: 2.67876 | val_0_rmse: 1.0299  | val_1_rmse: 0.91923 |  0:00:00s
epoch 2  | loss: 2.49098 | val_0_rmse: 1.02602 | val_1_rmse: 0.93204 |  0:00:00s
epoch 3  | loss: 1.82681 | val_0_rmse: 1.02607 | val_1_rmse: 0.93513 |  0:00:00s
epoch 4  | loss: 1.66298 | val_0_rmse: 1.02143 | val_1_rmse: 0.92272 |  0:00:00s
epoch 5  | loss: 1.591   | val_0_rmse: 1.02109 | val_1_rmse: 0.92469 |  0:00:00s
epoch 6  | loss: 1.43669 | val_0_rmse: 1.02196 | val_1_rmse: 0.92244 |  0:00:00s
epoch 7  | loss: 1.31591 | val_0_rmse: 1.02179 | val_1_rmse: 0.92181 |  0:00:00s
epoch 8  | loss: 1.23578 | val_0_rmse: 1.02243 | val_1_rmse: 0.92014 |  0:00:00s
epoch 9  | loss: 1.16169 | val_0_rmse: 1.0225  | val_1_rmse: 0.9197  |  0:00:00s
epoch 10 | loss: 1.16267 | val_0_rmse: 1.02122 | val_1_rmse: 0.92021 |  0:00:00s
epoch 11 | loss: 1.12768 | val_0_rmse: 1.02042 | val_1_rmse: 0.92123 |  0:00:01s
epoch 12 | loss: 1.0596  | val_0_rmse: 1.02028 | val_1_rmse: 0.92558 |  0:00:01s
epoch 13 | loss: 1.06528 | val_0_rmse: 1.02253 | val_1_rmse: 0.93302 |  0:00:01s
epoch 14 | loss: 1.11434 | val_0_rmse: 1.02242 | val_1_rmse: 0.93159 |  0:00:01s
epoch 15 | loss: 1.02948 | val_0_rmse: 1.02146 | val_1_rmse: 0.92643 |  0:00:01s
epoch 16 | loss: 1.05477 | val_0_rmse: 1.02126 | val_1_rmse: 0.92624 |  0:00:01s
epoch 17 | loss: 1.03215 | val_0_rmse: 1.02146 | val_1_rmse: 0.92642 |  0:00:01s
epoch 18 | loss: 1.0379  | val_0_rmse: 1.02235 | val_1_rmse: 0.92963 |  0:00:01s
epoch 19 | loss: 1.04929 | val_0_rmse: 1.02169 | val_1_rmse: 0.92661 |  0:00:01s
epoch 20 | loss: 1.01353 | val_0_rmse: 1.02021 | val_1_rmse: 0.9245  |  0:00:01s
epoch 21 | loss: 1.06605 | val_0_rmse: 1.01984 | val_1_rmse: 0.92293 |  0:00:01s
epoch 22 | loss: 1.01336 | val_0_rmse: 1.02018 | val_1_rmse: 0.92362 |  0:00:01s
epoch 23 | loss: 1.01111 | val_0_rmse: 1.01997 | val_1_rmse: 0.92395 |  0:00:02s
epoch 24 | loss: 1.0462  | val_0_rmse: 1.02001 | val_1_rmse: 0.92548 |  0:00:02s
epoch 25 | loss: 0.99594 | val_0_rmse: 1.0198  | val_1_rmse: 0.92719 |  0:00:02s
epoch 26 | loss: 1.03314 | val_0_rmse: 1.01937 | val_1_rmse: 0.92676 |  0:00:02s
epoch 27 | loss: 1.02849 | val_0_rmse: 1.01967 | val_1_rmse: 0.92415 |  0:00:02s
epoch 28 | loss: 1.01009 | val_0_rmse: 1.02098 | val_1_rmse: 0.92189 |  0:00:02s
epoch 29 | loss: 1.02279 | val_0_rmse: 1.02171 | val_1_rmse: 0.92233 |  0:00:02s
epoch 30 | loss: 1.03323 | val_0_rmse: 1.02258 | val_1_rmse: 0.92631 |  0:00:02s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 0.91706
Best weights from best epoch are automatically used!
ended training at: 05:13:23
Feature importance:
Mean squared error is of 0.07800588377196585
Mean absolute error:0.20933853203815397
MAPE:0.2668154522699261
R2 score:-0.013280795007240442
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:24
epoch 0  | loss: 4.11758 | val_0_rmse: 1.04285 | val_1_rmse: 0.95239 |  0:00:00s
epoch 1  | loss: 2.50965 | val_0_rmse: 1.02727 | val_1_rmse: 0.93283 |  0:00:00s
epoch 2  | loss: 1.9332  | val_0_rmse: 1.01906 | val_1_rmse: 0.91488 |  0:00:00s
epoch 3  | loss: 1.46014 | val_0_rmse: 1.02115 | val_1_rmse: 0.91149 |  0:00:00s
epoch 4  | loss: 1.62319 | val_0_rmse: 1.01998 | val_1_rmse: 0.91468 |  0:00:00s
epoch 5  | loss: 1.27    | val_0_rmse: 1.02011 | val_1_rmse: 0.9172  |  0:00:00s
epoch 6  | loss: 1.45972 | val_0_rmse: 1.02    | val_1_rmse: 0.91483 |  0:00:00s
epoch 7  | loss: 1.20563 | val_0_rmse: 1.02195 | val_1_rmse: 0.91302 |  0:00:00s
epoch 8  | loss: 1.12644 | val_0_rmse: 1.02135 | val_1_rmse: 0.91101 |  0:00:00s
epoch 9  | loss: 1.1361  | val_0_rmse: 1.02069 | val_1_rmse: 0.91012 |  0:00:00s
epoch 10 | loss: 1.11516 | val_0_rmse: 1.01936 | val_1_rmse: 0.91148 |  0:00:00s
epoch 11 | loss: 1.05231 | val_0_rmse: 1.01907 | val_1_rmse: 0.91299 |  0:00:01s
epoch 12 | loss: 1.06995 | val_0_rmse: 1.0188  | val_1_rmse: 0.9132  |  0:00:01s
epoch 13 | loss: 1.02857 | val_0_rmse: 1.01871 | val_1_rmse: 0.91283 |  0:00:01s
epoch 14 | loss: 1.06854 | val_0_rmse: 1.0182  | val_1_rmse: 0.91282 |  0:00:01s
epoch 15 | loss: 1.00702 | val_0_rmse: 1.01789 | val_1_rmse: 0.91337 |  0:00:01s
epoch 16 | loss: 1.022   | val_0_rmse: 1.0183  | val_1_rmse: 0.91441 |  0:00:01s
epoch 17 | loss: 1.03147 | val_0_rmse: 1.01878 | val_1_rmse: 0.91547 |  0:00:01s
epoch 18 | loss: 1.03803 | val_0_rmse: 1.01883 | val_1_rmse: 0.91575 |  0:00:01s
epoch 19 | loss: 1.03632 | val_0_rmse: 1.01847 | val_1_rmse: 0.91601 |  0:00:01s
epoch 20 | loss: 1.05097 | val_0_rmse: 1.01816 | val_1_rmse: 0.91579 |  0:00:01s
epoch 21 | loss: 1.04509 | val_0_rmse: 1.018   | val_1_rmse: 0.91542 |  0:00:01s
epoch 22 | loss: 1.05239 | val_0_rmse: 1.01802 | val_1_rmse: 0.91558 |  0:00:01s
epoch 23 | loss: 1.02848 | val_0_rmse: 1.01775 | val_1_rmse: 0.9152  |  0:00:02s
epoch 24 | loss: 1.04705 | val_0_rmse: 1.01669 | val_1_rmse: 0.91439 |  0:00:02s
epoch 25 | loss: 1.03374 | val_0_rmse: 1.01618 | val_1_rmse: 0.9136  |  0:00:02s
epoch 26 | loss: 1.01322 | val_0_rmse: 1.0157  | val_1_rmse: 0.91316 |  0:00:02s
epoch 27 | loss: 1.0175  | val_0_rmse: 1.01509 | val_1_rmse: 0.9127  |  0:00:02s
epoch 28 | loss: 1.02599 | val_0_rmse: 1.01463 | val_1_rmse: 0.91232 |  0:00:02s
epoch 29 | loss: 1.02133 | val_0_rmse: 1.01443 | val_1_rmse: 0.91259 |  0:00:02s
epoch 30 | loss: 1.02555 | val_0_rmse: 1.01442 | val_1_rmse: 0.91389 |  0:00:02s
epoch 31 | loss: 1.03211 | val_0_rmse: 1.01451 | val_1_rmse: 0.91382 |  0:00:02s
epoch 32 | loss: 1.01644 | val_0_rmse: 1.01451 | val_1_rmse: 0.91364 |  0:00:02s
epoch 33 | loss: 1.00779 | val_0_rmse: 1.01421 | val_1_rmse: 0.91153 |  0:00:02s
epoch 34 | loss: 0.99265 | val_0_rmse: 1.01331 | val_1_rmse: 0.90979 |  0:00:03s
epoch 35 | loss: 0.99113 | val_0_rmse: 1.01243 | val_1_rmse: 0.909   |  0:00:03s
epoch 36 | loss: 0.98956 | val_0_rmse: 1.01128 | val_1_rmse: 0.90806 |  0:00:03s
epoch 37 | loss: 0.99266 | val_0_rmse: 1.01024 | val_1_rmse: 0.90704 |  0:00:03s
epoch 38 | loss: 0.9949  | val_0_rmse: 1.0095  | val_1_rmse: 0.90782 |  0:00:03s
epoch 39 | loss: 0.96066 | val_0_rmse: 1.0089  | val_1_rmse: 0.90755 |  0:00:03s
epoch 40 | loss: 0.97281 | val_0_rmse: 1.00846 | val_1_rmse: 0.90757 |  0:00:03s
epoch 41 | loss: 0.95281 | val_0_rmse: 1.00786 | val_1_rmse: 0.90733 |  0:00:03s
epoch 42 | loss: 0.96935 | val_0_rmse: 1.00689 | val_1_rmse: 0.90566 |  0:00:03s
epoch 43 | loss: 0.96472 | val_0_rmse: 1.00661 | val_1_rmse: 0.90511 |  0:00:03s
epoch 44 | loss: 0.94187 | val_0_rmse: 1.00602 | val_1_rmse: 0.90454 |  0:00:03s
epoch 45 | loss: 0.92037 | val_0_rmse: 1.00529 | val_1_rmse: 0.90396 |  0:00:03s
epoch 46 | loss: 0.95997 | val_0_rmse: 1.00457 | val_1_rmse: 0.90398 |  0:00:04s
epoch 47 | loss: 0.94096 | val_0_rmse: 1.0047  | val_1_rmse: 0.9054  |  0:00:04s
epoch 48 | loss: 0.95114 | val_0_rmse: 1.00608 | val_1_rmse: 0.90933 |  0:00:04s
epoch 49 | loss: 0.92637 | val_0_rmse: 1.00791 | val_1_rmse: 0.91479 |  0:00:04s
epoch 50 | loss: 0.94004 | val_0_rmse: 1.00751 | val_1_rmse: 0.91786 |  0:00:04s
epoch 51 | loss: 0.90528 | val_0_rmse: 1.00581 | val_1_rmse: 0.91869 |  0:00:04s
epoch 52 | loss: 0.9046  | val_0_rmse: 1.00357 | val_1_rmse: 0.91749 |  0:00:04s
epoch 53 | loss: 0.90162 | val_0_rmse: 1.00103 | val_1_rmse: 0.91461 |  0:00:04s
epoch 54 | loss: 0.87955 | val_0_rmse: 0.99868 | val_1_rmse: 0.91247 |  0:00:04s
epoch 55 | loss: 0.87101 | val_0_rmse: 0.99524 | val_1_rmse: 0.90884 |  0:00:04s
epoch 56 | loss: 0.87343 | val_0_rmse: 0.9923  | val_1_rmse: 0.90584 |  0:00:04s
epoch 57 | loss: 0.864   | val_0_rmse: 0.98887 | val_1_rmse: 0.90316 |  0:00:05s
epoch 58 | loss: 0.86321 | val_0_rmse: 0.98727 | val_1_rmse: 0.90092 |  0:00:05s
epoch 59 | loss: 0.8281  | val_0_rmse: 0.98575 | val_1_rmse: 0.89771 |  0:00:05s
epoch 60 | loss: 0.865   | val_0_rmse: 0.9855  | val_1_rmse: 0.89517 |  0:00:05s
epoch 61 | loss: 0.8298  | val_0_rmse: 0.98692 | val_1_rmse: 0.89388 |  0:00:05s
epoch 62 | loss: 0.82319 | val_0_rmse: 0.98846 | val_1_rmse: 0.8953  |  0:00:05s
epoch 63 | loss: 0.83555 | val_0_rmse: 0.99047 | val_1_rmse: 0.89743 |  0:00:05s
epoch 64 | loss: 0.82747 | val_0_rmse: 0.99058 | val_1_rmse: 0.89776 |  0:00:05s
epoch 65 | loss: 0.80443 | val_0_rmse: 0.99054 | val_1_rmse: 0.89777 |  0:00:05s
epoch 66 | loss: 0.83202 | val_0_rmse: 0.99015 | val_1_rmse: 0.897   |  0:00:05s
epoch 67 | loss: 0.79794 | val_0_rmse: 0.98959 | val_1_rmse: 0.89682 |  0:00:05s
epoch 68 | loss: 0.79621 | val_0_rmse: 0.99207 | val_1_rmse: 0.89743 |  0:00:05s
epoch 69 | loss: 0.80413 | val_0_rmse: 0.99429 | val_1_rmse: 0.89845 |  0:00:06s
epoch 70 | loss: 0.8094  | val_0_rmse: 0.99075 | val_1_rmse: 0.8962  |  0:00:06s
epoch 71 | loss: 0.77082 | val_0_rmse: 0.9863  | val_1_rmse: 0.89417 |  0:00:06s
epoch 72 | loss: 0.77962 | val_0_rmse: 0.9835  | val_1_rmse: 0.89402 |  0:00:06s
epoch 73 | loss: 0.80387 | val_0_rmse: 0.9815  | val_1_rmse: 0.89472 |  0:00:06s
epoch 74 | loss: 0.80348 | val_0_rmse: 0.98111 | val_1_rmse: 0.89532 |  0:00:06s
epoch 75 | loss: 0.78609 | val_0_rmse: 0.98098 | val_1_rmse: 0.8969  |  0:00:06s
epoch 76 | loss: 0.76759 | val_0_rmse: 0.98186 | val_1_rmse: 0.89904 |  0:00:06s
epoch 77 | loss: 0.77234 | val_0_rmse: 0.98168 | val_1_rmse: 0.90153 |  0:00:06s
epoch 78 | loss: 0.76748 | val_0_rmse: 0.98202 | val_1_rmse: 0.90532 |  0:00:06s
epoch 79 | loss: 0.75365 | val_0_rmse: 0.98395 | val_1_rmse: 0.91018 |  0:00:06s
epoch 80 | loss: 0.75562 | val_0_rmse: 0.98547 | val_1_rmse: 0.9114  |  0:00:07s
epoch 81 | loss: 0.73278 | val_0_rmse: 0.98742 | val_1_rmse: 0.91164 |  0:00:07s
epoch 82 | loss: 0.73926 | val_0_rmse: 0.98785 | val_1_rmse: 0.91152 |  0:00:07s
epoch 83 | loss: 0.73329 | val_0_rmse: 0.98819 | val_1_rmse: 0.90996 |  0:00:07s
epoch 84 | loss: 0.71317 | val_0_rmse: 0.98906 | val_1_rmse: 0.90919 |  0:00:07s
epoch 85 | loss: 0.69563 | val_0_rmse: 0.99133 | val_1_rmse: 0.9101  |  0:00:07s
epoch 86 | loss: 0.71393 | val_0_rmse: 0.99174 | val_1_rmse: 0.91318 |  0:00:07s
epoch 87 | loss: 0.67259 | val_0_rmse: 0.99113 | val_1_rmse: 0.91715 |  0:00:07s
epoch 88 | loss: 0.67848 | val_0_rmse: 0.99174 | val_1_rmse: 0.92298 |  0:00:07s
epoch 89 | loss: 0.6646  | val_0_rmse: 0.9908  | val_1_rmse: 0.92591 |  0:00:07s
epoch 90 | loss: 0.67783 | val_0_rmse: 0.9892  | val_1_rmse: 0.92627 |  0:00:07s
epoch 91 | loss: 0.65623 | val_0_rmse: 0.98804 | val_1_rmse: 0.92566 |  0:00:07s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.89388
Best weights from best epoch are automatically used!
ended training at: 05:13:32
Feature importance:
Mean squared error is of 0.07709927266156805
Mean absolute error:0.21022265744057253
MAPE:0.24857602363892856
R2 score:0.029769636825663914
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:32
epoch 0  | loss: 2.92883 | val_0_rmse: 1.01582 | val_1_rmse: 1.00009 |  0:00:00s
epoch 1  | loss: 1.76315 | val_0_rmse: 0.99724 | val_1_rmse: 0.99436 |  0:00:00s
epoch 2  | loss: 2.02358 | val_0_rmse: 0.99654 | val_1_rmse: 0.99322 |  0:00:00s
epoch 3  | loss: 1.4356  | val_0_rmse: 0.99588 | val_1_rmse: 0.99945 |  0:00:00s
epoch 4  | loss: 1.45119 | val_0_rmse: 1.00021 | val_1_rmse: 1.00626 |  0:00:00s
epoch 5  | loss: 1.30777 | val_0_rmse: 1.00422 | val_1_rmse: 1.00918 |  0:00:00s
epoch 6  | loss: 1.29    | val_0_rmse: 1.00009 | val_1_rmse: 1.00792 |  0:00:00s
epoch 7  | loss: 1.15312 | val_0_rmse: 0.99415 | val_1_rmse: 0.99954 |  0:00:00s
epoch 8  | loss: 1.04628 | val_0_rmse: 0.99215 | val_1_rmse: 0.99422 |  0:00:00s
epoch 9  | loss: 1.10554 | val_0_rmse: 0.99191 | val_1_rmse: 0.99291 |  0:00:00s
epoch 10 | loss: 1.00962 | val_0_rmse: 0.99232 | val_1_rmse: 0.99195 |  0:00:00s
epoch 11 | loss: 1.09373 | val_0_rmse: 0.99213 | val_1_rmse: 0.99195 |  0:00:01s
epoch 12 | loss: 1.04891 | val_0_rmse: 0.99371 | val_1_rmse: 0.99625 |  0:00:01s
epoch 13 | loss: 1.03869 | val_0_rmse: 0.99491 | val_1_rmse: 0.99831 |  0:00:01s
epoch 14 | loss: 1.01776 | val_0_rmse: 0.9943  | val_1_rmse: 0.99771 |  0:00:01s
epoch 15 | loss: 1.01416 | val_0_rmse: 0.99234 | val_1_rmse: 0.99636 |  0:00:01s
epoch 16 | loss: 1.00525 | val_0_rmse: 0.99137 | val_1_rmse: 0.99527 |  0:00:01s
epoch 17 | loss: 1.00142 | val_0_rmse: 0.99138 | val_1_rmse: 0.99527 |  0:00:01s
epoch 18 | loss: 0.99184 | val_0_rmse: 0.99077 | val_1_rmse: 0.99607 |  0:00:01s
epoch 19 | loss: 0.97489 | val_0_rmse: 0.99058 | val_1_rmse: 0.99631 |  0:00:01s
epoch 20 | loss: 0.97492 | val_0_rmse: 0.99058 | val_1_rmse: 0.99628 |  0:00:01s
epoch 21 | loss: 0.97952 | val_0_rmse: 0.99042 | val_1_rmse: 0.99581 |  0:00:01s
epoch 22 | loss: 0.98075 | val_0_rmse: 0.99008 | val_1_rmse: 0.99529 |  0:00:01s
epoch 23 | loss: 0.983   | val_0_rmse: 0.9896  | val_1_rmse: 0.99488 |  0:00:02s
epoch 24 | loss: 0.97585 | val_0_rmse: 0.98922 | val_1_rmse: 0.99458 |  0:00:02s
epoch 25 | loss: 0.96956 | val_0_rmse: 0.98855 | val_1_rmse: 0.99506 |  0:00:02s
epoch 26 | loss: 0.96084 | val_0_rmse: 0.98767 | val_1_rmse: 0.99555 |  0:00:02s
epoch 27 | loss: 0.95434 | val_0_rmse: 0.98697 | val_1_rmse: 0.99559 |  0:00:02s
epoch 28 | loss: 0.95814 | val_0_rmse: 0.9876  | val_1_rmse: 0.99605 |  0:00:02s
epoch 29 | loss: 0.95472 | val_0_rmse: 0.9885  | val_1_rmse: 0.99663 |  0:00:02s
epoch 30 | loss: 0.93814 | val_0_rmse: 0.99029 | val_1_rmse: 0.99763 |  0:00:02s
epoch 31 | loss: 0.92878 | val_0_rmse: 0.99196 | val_1_rmse: 0.99967 |  0:00:02s
epoch 32 | loss: 0.91314 | val_0_rmse: 0.99223 | val_1_rmse: 1.00077 |  0:00:02s
epoch 33 | loss: 0.93169 | val_0_rmse: 0.99106 | val_1_rmse: 0.99807 |  0:00:02s
epoch 34 | loss: 0.89511 | val_0_rmse: 0.99317 | val_1_rmse: 1.00779 |  0:00:02s
epoch 35 | loss: 0.89727 | val_0_rmse: 1.00801 | val_1_rmse: 1.0134  |  0:00:03s
epoch 36 | loss: 0.87872 | val_0_rmse: 1.05863 | val_1_rmse: 1.05214 |  0:00:03s
epoch 37 | loss: 0.86399 | val_0_rmse: 1.06511 | val_1_rmse: 1.05536 |  0:00:03s
epoch 38 | loss: 0.88307 | val_0_rmse: 1.04307 | val_1_rmse: 1.03712 |  0:00:03s
epoch 39 | loss: 0.87029 | val_0_rmse: 1.00348 | val_1_rmse: 1.01588 |  0:00:03s
epoch 40 | loss: 0.86825 | val_0_rmse: 0.98843 | val_1_rmse: 1.0137  |  0:00:03s
epoch 41 | loss: 0.85059 | val_0_rmse: 0.9951  | val_1_rmse: 1.01519 |  0:00:03s

Early stopping occured at epoch 41 with best_epoch = 11 and best_val_1_rmse = 0.99195
Best weights from best epoch are automatically used!
ended training at: 05:13:36
Feature importance:
Mean squared error is of 0.09304384545735175
Mean absolute error:0.21195204209554974
MAPE:0.24072177914594178
R2 score:-0.00401624302732051
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:13:36
epoch 0  | loss: 2.05053 | val_0_rmse: 1.0174  | val_1_rmse: 1.01077 |  0:00:00s
epoch 1  | loss: 1.2583  | val_0_rmse: 1.00182 | val_1_rmse: 0.99423 |  0:00:00s
epoch 2  | loss: 1.05319 | val_0_rmse: 1.00159 | val_1_rmse: 0.99328 |  0:00:00s
epoch 3  | loss: 1.01119 | val_0_rmse: 1.00096 | val_1_rmse: 0.99215 |  0:00:01s
epoch 4  | loss: 1.003   | val_0_rmse: 1.0005  | val_1_rmse: 0.99322 |  0:00:01s
epoch 5  | loss: 0.99981 | val_0_rmse: 0.99916 | val_1_rmse: 0.99464 |  0:00:01s
epoch 6  | loss: 0.99702 | val_0_rmse: 0.99954 | val_1_rmse: 0.99304 |  0:00:02s
epoch 7  | loss: 0.97836 | val_0_rmse: 0.99009 | val_1_rmse: 0.98829 |  0:00:02s
epoch 8  | loss: 0.97507 | val_0_rmse: 0.97957 | val_1_rmse: 0.98109 |  0:00:02s
epoch 9  | loss: 0.97822 | val_0_rmse: 0.97814 | val_1_rmse: 0.97122 |  0:00:02s
epoch 10 | loss: 0.96776 | val_0_rmse: 0.97485 | val_1_rmse: 0.96834 |  0:00:03s
epoch 11 | loss: 0.95822 | val_0_rmse: 0.97335 | val_1_rmse: 0.96219 |  0:00:03s
epoch 12 | loss: 0.94953 | val_0_rmse: 0.96322 | val_1_rmse: 0.94861 |  0:00:03s
epoch 13 | loss: 0.94401 | val_0_rmse: 0.96439 | val_1_rmse: 0.95059 |  0:00:04s
epoch 14 | loss: 0.9381  | val_0_rmse: 0.96481 | val_1_rmse: 0.94761 |  0:00:04s
epoch 15 | loss: 0.92247 | val_0_rmse: 0.95219 | val_1_rmse: 0.93657 |  0:00:04s
epoch 16 | loss: 0.91301 | val_0_rmse: 0.95919 | val_1_rmse: 0.93537 |  0:00:05s
epoch 17 | loss: 0.87994 | val_0_rmse: 0.93133 | val_1_rmse: 0.90649 |  0:00:05s
epoch 18 | loss: 0.85682 | val_0_rmse: 0.91679 | val_1_rmse: 0.89592 |  0:00:05s
epoch 19 | loss: 0.8307  | val_0_rmse: 0.91092 | val_1_rmse: 0.88236 |  0:00:06s
epoch 20 | loss: 0.81054 | val_0_rmse: 0.92252 | val_1_rmse: 0.88352 |  0:00:06s
epoch 21 | loss: 0.78786 | val_0_rmse: 0.90279 | val_1_rmse: 0.86494 |  0:00:06s
epoch 22 | loss: 0.76064 | val_0_rmse: 0.88612 | val_1_rmse: 0.85276 |  0:00:07s
epoch 23 | loss: 0.75579 | val_0_rmse: 0.87085 | val_1_rmse: 0.84342 |  0:00:07s
epoch 24 | loss: 0.73021 | val_0_rmse: 0.86851 | val_1_rmse: 0.84348 |  0:00:07s
epoch 25 | loss: 0.7106  | val_0_rmse: 0.87758 | val_1_rmse: 0.84887 |  0:00:07s
epoch 26 | loss: 0.68435 | val_0_rmse: 0.85131 | val_1_rmse: 0.81745 |  0:00:08s
epoch 27 | loss: 0.68461 | val_0_rmse: 0.8289  | val_1_rmse: 0.79665 |  0:00:08s
epoch 28 | loss: 0.65801 | val_0_rmse: 0.82513 | val_1_rmse: 0.78581 |  0:00:08s
epoch 29 | loss: 0.64115 | val_0_rmse: 0.8206  | val_1_rmse: 0.7829  |  0:00:09s
epoch 30 | loss: 0.63178 | val_0_rmse: 0.82388 | val_1_rmse: 0.78914 |  0:00:09s
epoch 31 | loss: 0.62502 | val_0_rmse: 0.82366 | val_1_rmse: 0.78774 |  0:00:09s
epoch 32 | loss: 0.62701 | val_0_rmse: 0.82059 | val_1_rmse: 0.79426 |  0:00:09s
epoch 33 | loss: 0.61285 | val_0_rmse: 0.8359  | val_1_rmse: 0.80505 |  0:00:10s
epoch 34 | loss: 0.60297 | val_0_rmse: 0.86092 | val_1_rmse: 0.83221 |  0:00:10s
epoch 35 | loss: 0.60473 | val_0_rmse: 0.84612 | val_1_rmse: 0.80305 |  0:00:10s
epoch 36 | loss: 0.59844 | val_0_rmse: 0.82401 | val_1_rmse: 0.78032 |  0:00:11s
epoch 37 | loss: 0.58735 | val_0_rmse: 0.81205 | val_1_rmse: 0.78398 |  0:00:11s
epoch 38 | loss: 0.58415 | val_0_rmse: 0.80695 | val_1_rmse: 0.77948 |  0:00:11s
epoch 39 | loss: 0.58005 | val_0_rmse: 0.80929 | val_1_rmse: 0.78259 |  0:00:12s
epoch 40 | loss: 0.56482 | val_0_rmse: 0.79924 | val_1_rmse: 0.7803  |  0:00:12s
epoch 41 | loss: 0.56586 | val_0_rmse: 0.80257 | val_1_rmse: 0.77812 |  0:00:12s
epoch 42 | loss: 0.57458 | val_0_rmse: 0.81044 | val_1_rmse: 0.79456 |  0:00:12s
epoch 43 | loss: 0.5621  | val_0_rmse: 0.80309 | val_1_rmse: 0.7806  |  0:00:13s
epoch 44 | loss: 0.55035 | val_0_rmse: 0.79122 | val_1_rmse: 0.76483 |  0:00:13s
epoch 45 | loss: 0.55518 | val_0_rmse: 0.79371 | val_1_rmse: 0.77546 |  0:00:13s
epoch 46 | loss: 0.55456 | val_0_rmse: 0.80601 | val_1_rmse: 0.79925 |  0:00:14s
epoch 47 | loss: 0.5443  | val_0_rmse: 0.80912 | val_1_rmse: 0.80283 |  0:00:14s
epoch 48 | loss: 0.5338  | val_0_rmse: 0.80946 | val_1_rmse: 0.81143 |  0:00:14s
epoch 49 | loss: 0.53477 | val_0_rmse: 0.77606 | val_1_rmse: 0.77109 |  0:00:14s
epoch 50 | loss: 0.53326 | val_0_rmse: 0.7786  | val_1_rmse: 0.76878 |  0:00:15s
epoch 51 | loss: 0.53936 | val_0_rmse: 0.77516 | val_1_rmse: 0.76649 |  0:00:15s
epoch 52 | loss: 0.52564 | val_0_rmse: 0.78537 | val_1_rmse: 0.78086 |  0:00:15s
epoch 53 | loss: 0.52225 | val_0_rmse: 0.79665 | val_1_rmse: 0.79825 |  0:00:16s
epoch 54 | loss: 0.50555 | val_0_rmse: 0.78009 | val_1_rmse: 0.77371 |  0:00:16s
epoch 55 | loss: 0.51748 | val_0_rmse: 0.7891  | val_1_rmse: 0.78122 |  0:00:16s
epoch 56 | loss: 0.52157 | val_0_rmse: 0.77357 | val_1_rmse: 0.76668 |  0:00:16s
epoch 57 | loss: 0.5068  | val_0_rmse: 0.81586 | val_1_rmse: 0.81872 |  0:00:17s
epoch 58 | loss: 0.50501 | val_0_rmse: 0.77397 | val_1_rmse: 0.78324 |  0:00:17s
epoch 59 | loss: 0.49766 | val_0_rmse: 0.77045 | val_1_rmse: 0.77686 |  0:00:17s
epoch 60 | loss: 0.50224 | val_0_rmse: 0.7661  | val_1_rmse: 0.77004 |  0:00:18s
epoch 61 | loss: 0.49184 | val_0_rmse: 0.76865 | val_1_rmse: 0.79177 |  0:00:18s
epoch 62 | loss: 0.49311 | val_0_rmse: 0.79021 | val_1_rmse: 0.81041 |  0:00:18s
epoch 63 | loss: 0.50429 | val_0_rmse: 0.7613  | val_1_rmse: 0.76867 |  0:00:19s
epoch 64 | loss: 0.49097 | val_0_rmse: 0.75129 | val_1_rmse: 0.76568 |  0:00:19s
epoch 65 | loss: 0.49025 | val_0_rmse: 0.75744 | val_1_rmse: 0.78621 |  0:00:19s
epoch 66 | loss: 0.48842 | val_0_rmse: 0.76694 | val_1_rmse: 0.79708 |  0:00:19s
epoch 67 | loss: 0.47841 | val_0_rmse: 0.75805 | val_1_rmse: 0.76784 |  0:00:20s
epoch 68 | loss: 0.48354 | val_0_rmse: 0.74662 | val_1_rmse: 0.76288 |  0:00:20s
epoch 69 | loss: 0.48405 | val_0_rmse: 0.75609 | val_1_rmse: 0.77748 |  0:00:20s
epoch 70 | loss: 0.48621 | val_0_rmse: 0.74291 | val_1_rmse: 0.76786 |  0:00:21s
epoch 71 | loss: 0.47908 | val_0_rmse: 0.74591 | val_1_rmse: 0.77768 |  0:00:21s
epoch 72 | loss: 0.47644 | val_0_rmse: 0.74103 | val_1_rmse: 0.75832 |  0:00:21s
epoch 73 | loss: 0.45994 | val_0_rmse: 0.73543 | val_1_rmse: 0.77208 |  0:00:21s
epoch 74 | loss: 0.46651 | val_0_rmse: 0.7312  | val_1_rmse: 0.77245 |  0:00:22s
epoch 75 | loss: 0.46582 | val_0_rmse: 0.73995 | val_1_rmse: 0.77754 |  0:00:22s
epoch 76 | loss: 0.47756 | val_0_rmse: 0.73148 | val_1_rmse: 0.7799  |  0:00:22s
epoch 77 | loss: 0.48062 | val_0_rmse: 0.74122 | val_1_rmse: 0.80642 |  0:00:23s
epoch 78 | loss: 0.46858 | val_0_rmse: 0.7314  | val_1_rmse: 0.78236 |  0:00:23s
epoch 79 | loss: 0.45588 | val_0_rmse: 0.72322 | val_1_rmse: 0.78834 |  0:00:23s
epoch 80 | loss: 0.46752 | val_0_rmse: 0.723   | val_1_rmse: 0.78231 |  0:00:24s
epoch 81 | loss: 0.46359 | val_0_rmse: 0.71512 | val_1_rmse: 0.78654 |  0:00:24s
epoch 82 | loss: 0.47522 | val_0_rmse: 0.72155 | val_1_rmse: 0.79182 |  0:00:24s
epoch 83 | loss: 0.45015 | val_0_rmse: 0.70562 | val_1_rmse: 0.7709  |  0:00:25s
epoch 84 | loss: 0.46082 | val_0_rmse: 0.7023  | val_1_rmse: 0.77842 |  0:00:25s
epoch 85 | loss: 0.45563 | val_0_rmse: 0.69283 | val_1_rmse: 0.76885 |  0:00:25s
epoch 86 | loss: 0.46044 | val_0_rmse: 0.69755 | val_1_rmse: 0.78408 |  0:00:25s
epoch 87 | loss: 0.4471  | val_0_rmse: 0.69883 | val_1_rmse: 0.79438 |  0:00:26s
epoch 88 | loss: 0.44516 | val_0_rmse: 0.69675 | val_1_rmse: 0.76692 |  0:00:26s
epoch 89 | loss: 0.45    | val_0_rmse: 0.6903  | val_1_rmse: 0.76342 |  0:00:26s
epoch 90 | loss: 0.43532 | val_0_rmse: 0.68173 | val_1_rmse: 0.75754 |  0:00:27s
epoch 91 | loss: 0.43612 | val_0_rmse: 0.6832  | val_1_rmse: 0.76274 |  0:00:27s
epoch 92 | loss: 0.44234 | val_0_rmse: 0.68897 | val_1_rmse: 0.76538 |  0:00:27s
epoch 93 | loss: 0.43385 | val_0_rmse: 0.66476 | val_1_rmse: 0.78582 |  0:00:27s
epoch 94 | loss: 0.42025 | val_0_rmse: 0.67333 | val_1_rmse: 0.81884 |  0:00:28s
epoch 95 | loss: 0.41923 | val_0_rmse: 0.66495 | val_1_rmse: 0.78471 |  0:00:28s
epoch 96 | loss: 0.43005 | val_0_rmse: 0.66703 | val_1_rmse: 0.77429 |  0:00:28s
epoch 97 | loss: 0.43101 | val_0_rmse: 0.66108 | val_1_rmse: 0.77628 |  0:00:29s
epoch 98 | loss: 0.41729 | val_0_rmse: 0.67191 | val_1_rmse: 0.80801 |  0:00:29s
epoch 99 | loss: 0.42442 | val_0_rmse: 0.6653  | val_1_rmse: 0.76886 |  0:00:29s
epoch 100| loss: 0.41572 | val_0_rmse: 0.64599 | val_1_rmse: 0.79829 |  0:00:30s
epoch 101| loss: 0.41563 | val_0_rmse: 0.65047 | val_1_rmse: 0.78776 |  0:00:30s
epoch 102| loss: 0.41822 | val_0_rmse: 0.6537  | val_1_rmse: 0.7805  |  0:00:30s
epoch 103| loss: 0.41142 | val_0_rmse: 0.67216 | val_1_rmse: 0.81774 |  0:00:30s
epoch 104| loss: 0.41079 | val_0_rmse: 0.65829 | val_1_rmse: 0.8011  |  0:00:31s
epoch 105| loss: 0.41864 | val_0_rmse: 0.6497  | val_1_rmse: 0.77983 |  0:00:31s
epoch 106| loss: 0.41441 | val_0_rmse: 0.64444 | val_1_rmse: 0.79387 |  0:00:31s
epoch 107| loss: 0.41053 | val_0_rmse: 0.63325 | val_1_rmse: 0.7914  |  0:00:32s
epoch 108| loss: 0.4086  | val_0_rmse: 0.64584 | val_1_rmse: 0.79195 |  0:00:32s
epoch 109| loss: 0.41609 | val_0_rmse: 0.63929 | val_1_rmse: 0.82258 |  0:00:32s
epoch 110| loss: 0.41306 | val_0_rmse: 0.63704 | val_1_rmse: 0.81082 |  0:00:32s
epoch 111| loss: 0.4097  | val_0_rmse: 0.62973 | val_1_rmse: 0.79515 |  0:00:33s
epoch 112| loss: 0.41056 | val_0_rmse: 0.63006 | val_1_rmse: 0.82368 |  0:00:33s
epoch 113| loss: 0.40508 | val_0_rmse: 0.6334  | val_1_rmse: 0.79319 |  0:00:33s
epoch 114| loss: 0.40274 | val_0_rmse: 0.63436 | val_1_rmse: 0.80585 |  0:00:34s
epoch 115| loss: 0.3951  | val_0_rmse: 0.61221 | val_1_rmse: 0.80059 |  0:00:34s
epoch 116| loss: 0.39146 | val_0_rmse: 0.62729 | val_1_rmse: 0.77577 |  0:00:34s
epoch 117| loss: 0.40466 | val_0_rmse: 0.62166 | val_1_rmse: 0.77281 |  0:00:34s
epoch 118| loss: 0.39202 | val_0_rmse: 0.61208 | val_1_rmse: 0.78014 |  0:00:35s
epoch 119| loss: 0.38632 | val_0_rmse: 0.61773 | val_1_rmse: 0.8025  |  0:00:35s
epoch 120| loss: 0.3712  | val_0_rmse: 0.60338 | val_1_rmse: 0.78871 |  0:00:35s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.75754
Best weights from best epoch are automatically used!
ended training at: 05:14:12
Feature importance:
Mean squared error is of 0.06029764868198406
Mean absolute error:0.16729461513893873
MAPE:0.1793494573674896
R2 score:0.3299482375471926
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:14:13
epoch 0  | loss: 2.30713 | val_0_rmse: 1.00388 | val_1_rmse: 0.95423 |  0:00:00s
epoch 1  | loss: 1.1477  | val_0_rmse: 1.00267 | val_1_rmse: 0.95537 |  0:00:00s
epoch 2  | loss: 1.06604 | val_0_rmse: 1.00189 | val_1_rmse: 0.95389 |  0:00:00s
epoch 3  | loss: 1.03251 | val_0_rmse: 1.00235 | val_1_rmse: 0.95448 |  0:00:01s
epoch 4  | loss: 1.01069 | val_0_rmse: 1.00187 | val_1_rmse: 0.95336 |  0:00:01s
epoch 5  | loss: 1.00706 | val_0_rmse: 1.00168 | val_1_rmse: 0.95402 |  0:00:01s
epoch 6  | loss: 1.00263 | val_0_rmse: 0.99668 | val_1_rmse: 0.95001 |  0:00:02s
epoch 7  | loss: 0.9955  | val_0_rmse: 0.99437 | val_1_rmse: 0.94869 |  0:00:02s
epoch 8  | loss: 0.98688 | val_0_rmse: 0.97483 | val_1_rmse: 0.92582 |  0:00:02s
epoch 9  | loss: 0.98039 | val_0_rmse: 0.97654 | val_1_rmse: 0.92668 |  0:00:02s
epoch 10 | loss: 0.96774 | val_0_rmse: 0.96892 | val_1_rmse: 0.91203 |  0:00:03s
epoch 11 | loss: 0.93988 | val_0_rmse: 0.95655 | val_1_rmse: 0.90962 |  0:00:03s
epoch 12 | loss: 0.9329  | val_0_rmse: 0.95627 | val_1_rmse: 0.91333 |  0:00:03s
epoch 13 | loss: 0.92428 | val_0_rmse: 0.95275 | val_1_rmse: 0.91033 |  0:00:04s
epoch 14 | loss: 0.91545 | val_0_rmse: 0.93852 | val_1_rmse: 0.89394 |  0:00:04s
epoch 15 | loss: 0.89173 | val_0_rmse: 0.93145 | val_1_rmse: 0.88194 |  0:00:04s
epoch 16 | loss: 0.85867 | val_0_rmse: 0.92106 | val_1_rmse: 0.87858 |  0:00:05s
epoch 17 | loss: 0.82915 | val_0_rmse: 0.91547 | val_1_rmse: 0.87327 |  0:00:05s
epoch 18 | loss: 0.78468 | val_0_rmse: 0.94896 | val_1_rmse: 0.90556 |  0:00:05s
epoch 19 | loss: 0.75692 | val_0_rmse: 0.97175 | val_1_rmse: 0.94076 |  0:00:06s
epoch 20 | loss: 0.72582 | val_0_rmse: 0.89211 | val_1_rmse: 0.87112 |  0:00:06s
epoch 21 | loss: 0.72668 | val_0_rmse: 0.94464 | val_1_rmse: 0.92845 |  0:00:06s
epoch 22 | loss: 0.70881 | val_0_rmse: 1.02657 | val_1_rmse: 1.00743 |  0:00:06s
epoch 23 | loss: 0.70186 | val_0_rmse: 0.86373 | val_1_rmse: 0.83914 |  0:00:07s
epoch 24 | loss: 0.70291 | val_0_rmse: 0.85701 | val_1_rmse: 0.84284 |  0:00:07s
epoch 25 | loss: 0.67576 | val_0_rmse: 0.90815 | val_1_rmse: 0.90294 |  0:00:07s
epoch 26 | loss: 0.66409 | val_0_rmse: 0.83832 | val_1_rmse: 0.82333 |  0:00:08s
epoch 27 | loss: 0.65275 | val_0_rmse: 0.85597 | val_1_rmse: 0.84775 |  0:00:08s
epoch 28 | loss: 0.64756 | val_0_rmse: 0.87677 | val_1_rmse: 0.87777 |  0:00:08s
epoch 29 | loss: 0.64202 | val_0_rmse: 0.83495 | val_1_rmse: 0.83356 |  0:00:09s
epoch 30 | loss: 0.62865 | val_0_rmse: 0.82797 | val_1_rmse: 0.82551 |  0:00:09s
epoch 31 | loss: 0.63249 | val_0_rmse: 0.83222 | val_1_rmse: 0.83117 |  0:00:09s
epoch 32 | loss: 0.60658 | val_0_rmse: 0.82291 | val_1_rmse: 0.83079 |  0:00:09s
epoch 33 | loss: 0.60778 | val_0_rmse: 0.81874 | val_1_rmse: 0.81903 |  0:00:10s
epoch 34 | loss: 0.60194 | val_0_rmse: 0.84065 | val_1_rmse: 0.83339 |  0:00:10s
epoch 35 | loss: 0.5979  | val_0_rmse: 0.81083 | val_1_rmse: 0.80542 |  0:00:10s
epoch 36 | loss: 0.59238 | val_0_rmse: 0.82203 | val_1_rmse: 0.82283 |  0:00:11s
epoch 37 | loss: 0.59941 | val_0_rmse: 0.80971 | val_1_rmse: 0.80808 |  0:00:11s
epoch 38 | loss: 0.58319 | val_0_rmse: 0.8044  | val_1_rmse: 0.80249 |  0:00:11s
epoch 39 | loss: 0.58204 | val_0_rmse: 0.83756 | val_1_rmse: 0.83604 |  0:00:11s
epoch 40 | loss: 0.58319 | val_0_rmse: 0.7971  | val_1_rmse: 0.80015 |  0:00:12s
epoch 41 | loss: 0.58446 | val_0_rmse: 0.79806 | val_1_rmse: 0.79727 |  0:00:12s
epoch 42 | loss: 0.56581 | val_0_rmse: 0.83415 | val_1_rmse: 0.83391 |  0:00:12s
epoch 43 | loss: 0.56897 | val_0_rmse: 0.79227 | val_1_rmse: 0.78997 |  0:00:13s
epoch 44 | loss: 0.57855 | val_0_rmse: 0.79621 | val_1_rmse: 0.80149 |  0:00:13s
epoch 45 | loss: 0.55768 | val_0_rmse: 0.83276 | val_1_rmse: 0.83986 |  0:00:13s
epoch 46 | loss: 0.55605 | val_0_rmse: 0.79272 | val_1_rmse: 0.79687 |  0:00:13s
epoch 47 | loss: 0.55509 | val_0_rmse: 0.79823 | val_1_rmse: 0.80205 |  0:00:14s
epoch 48 | loss: 0.54751 | val_0_rmse: 0.7975  | val_1_rmse: 0.80746 |  0:00:14s
epoch 49 | loss: 0.55464 | val_0_rmse: 0.79819 | val_1_rmse: 0.81242 |  0:00:14s
epoch 50 | loss: 0.54945 | val_0_rmse: 0.8209  | val_1_rmse: 0.82603 |  0:00:15s
epoch 51 | loss: 0.55174 | val_0_rmse: 0.81564 | val_1_rmse: 0.81625 |  0:00:15s
epoch 52 | loss: 0.54301 | val_0_rmse: 0.7968  | val_1_rmse: 0.79709 |  0:00:15s
epoch 53 | loss: 0.53603 | val_0_rmse: 0.78139 | val_1_rmse: 0.79259 |  0:00:16s
epoch 54 | loss: 0.53992 | val_0_rmse: 0.76641 | val_1_rmse: 0.78651 |  0:00:16s
epoch 55 | loss: 0.54413 | val_0_rmse: 0.78023 | val_1_rmse: 0.80375 |  0:00:16s
epoch 56 | loss: 0.53775 | val_0_rmse: 0.80086 | val_1_rmse: 0.82457 |  0:00:16s
epoch 57 | loss: 0.53713 | val_0_rmse: 0.78432 | val_1_rmse: 0.80406 |  0:00:17s
epoch 58 | loss: 0.52812 | val_0_rmse: 0.81312 | val_1_rmse: 0.83355 |  0:00:17s
epoch 59 | loss: 0.5386  | val_0_rmse: 0.77358 | val_1_rmse: 0.79057 |  0:00:17s
epoch 60 | loss: 0.52809 | val_0_rmse: 0.77261 | val_1_rmse: 0.79747 |  0:00:18s
epoch 61 | loss: 0.5199  | val_0_rmse: 0.78867 | val_1_rmse: 0.82402 |  0:00:18s
epoch 62 | loss: 0.5335  | val_0_rmse: 0.76862 | val_1_rmse: 0.80892 |  0:00:18s
epoch 63 | loss: 0.52577 | val_0_rmse: 0.75929 | val_1_rmse: 0.79453 |  0:00:18s
epoch 64 | loss: 0.52113 | val_0_rmse: 0.75396 | val_1_rmse: 0.78698 |  0:00:19s
epoch 65 | loss: 0.51664 | val_0_rmse: 0.75861 | val_1_rmse: 0.802   |  0:00:19s
epoch 66 | loss: 0.5076  | val_0_rmse: 0.74695 | val_1_rmse: 0.792   |  0:00:19s
epoch 67 | loss: 0.5092  | val_0_rmse: 0.79767 | val_1_rmse: 0.84188 |  0:00:20s
epoch 68 | loss: 0.51878 | val_0_rmse: 0.81102 | val_1_rmse: 0.86042 |  0:00:20s
epoch 69 | loss: 0.50914 | val_0_rmse: 0.76193 | val_1_rmse: 0.80706 |  0:00:20s
epoch 70 | loss: 0.5146  | val_0_rmse: 0.77476 | val_1_rmse: 0.81944 |  0:00:20s
epoch 71 | loss: 0.5156  | val_0_rmse: 0.78173 | val_1_rmse: 0.8372  |  0:00:21s
epoch 72 | loss: 0.51354 | val_0_rmse: 0.74822 | val_1_rmse: 0.81724 |  0:00:21s
epoch 73 | loss: 0.51235 | val_0_rmse: 0.74521 | val_1_rmse: 0.80034 |  0:00:21s
epoch 74 | loss: 0.50726 | val_0_rmse: 0.74163 | val_1_rmse: 0.79593 |  0:00:22s
epoch 75 | loss: 0.50067 | val_0_rmse: 0.73009 | val_1_rmse: 0.79931 |  0:00:22s
epoch 76 | loss: 0.50092 | val_0_rmse: 0.74186 | val_1_rmse: 0.79923 |  0:00:22s
epoch 77 | loss: 0.49022 | val_0_rmse: 0.72621 | val_1_rmse: 0.78515 |  0:00:23s
epoch 78 | loss: 0.48414 | val_0_rmse: 0.73189 | val_1_rmse: 0.78985 |  0:00:23s
epoch 79 | loss: 0.48986 | val_0_rmse: 0.73928 | val_1_rmse: 0.80239 |  0:00:23s
epoch 80 | loss: 0.47765 | val_0_rmse: 0.72733 | val_1_rmse: 0.7879  |  0:00:23s
epoch 81 | loss: 0.47974 | val_0_rmse: 0.72366 | val_1_rmse: 0.78261 |  0:00:24s
epoch 82 | loss: 0.48171 | val_0_rmse: 0.72783 | val_1_rmse: 0.79249 |  0:00:24s
epoch 83 | loss: 0.47499 | val_0_rmse: 0.72115 | val_1_rmse: 0.78324 |  0:00:24s
epoch 84 | loss: 0.47614 | val_0_rmse: 0.71228 | val_1_rmse: 0.78564 |  0:00:25s
epoch 85 | loss: 0.47156 | val_0_rmse: 0.72864 | val_1_rmse: 0.80572 |  0:00:25s
epoch 86 | loss: 0.47825 | val_0_rmse: 0.71848 | val_1_rmse: 0.79251 |  0:00:25s
epoch 87 | loss: 0.47625 | val_0_rmse: 0.71499 | val_1_rmse: 0.79026 |  0:00:26s
epoch 88 | loss: 0.46607 | val_0_rmse: 0.71187 | val_1_rmse: 0.77294 |  0:00:26s
epoch 89 | loss: 0.45984 | val_0_rmse: 0.69816 | val_1_rmse: 0.76691 |  0:00:26s
epoch 90 | loss: 0.45123 | val_0_rmse: 0.7065  | val_1_rmse: 0.78709 |  0:00:26s
epoch 91 | loss: 0.45877 | val_0_rmse: 0.70685 | val_1_rmse: 0.78413 |  0:00:27s
epoch 92 | loss: 0.44951 | val_0_rmse: 0.71282 | val_1_rmse: 0.78959 |  0:00:27s
epoch 93 | loss: 0.45922 | val_0_rmse: 0.70924 | val_1_rmse: 0.80163 |  0:00:27s
epoch 94 | loss: 0.45743 | val_0_rmse: 0.6883  | val_1_rmse: 0.78825 |  0:00:28s
epoch 95 | loss: 0.45245 | val_0_rmse: 0.68317 | val_1_rmse: 0.77485 |  0:00:28s
epoch 96 | loss: 0.45281 | val_0_rmse: 0.70196 | val_1_rmse: 0.8042  |  0:00:28s
epoch 97 | loss: 0.46529 | val_0_rmse: 0.67974 | val_1_rmse: 0.77402 |  0:00:29s
epoch 98 | loss: 0.46357 | val_0_rmse: 0.68365 | val_1_rmse: 0.77108 |  0:00:29s
epoch 99 | loss: 0.44789 | val_0_rmse: 0.70495 | val_1_rmse: 0.81991 |  0:00:29s
epoch 100| loss: 0.45659 | val_0_rmse: 0.67562 | val_1_rmse: 0.78771 |  0:00:29s
epoch 101| loss: 0.44261 | val_0_rmse: 0.66542 | val_1_rmse: 0.77634 |  0:00:30s
epoch 102| loss: 0.44773 | val_0_rmse: 0.65987 | val_1_rmse: 0.78386 |  0:00:30s
epoch 103| loss: 0.43929 | val_0_rmse: 0.65856 | val_1_rmse: 0.79098 |  0:00:30s
epoch 104| loss: 0.43545 | val_0_rmse: 0.66746 | val_1_rmse: 0.77752 |  0:00:31s
epoch 105| loss: 0.43484 | val_0_rmse: 0.6627  | val_1_rmse: 0.78983 |  0:00:31s
epoch 106| loss: 0.43687 | val_0_rmse: 0.66071 | val_1_rmse: 0.7838  |  0:00:31s
epoch 107| loss: 0.44624 | val_0_rmse: 0.65672 | val_1_rmse: 0.78865 |  0:00:31s
epoch 108| loss: 0.44683 | val_0_rmse: 0.68123 | val_1_rmse: 0.83513 |  0:00:32s
epoch 109| loss: 0.4384  | val_0_rmse: 0.65336 | val_1_rmse: 0.79417 |  0:00:32s
epoch 110| loss: 0.44209 | val_0_rmse: 0.65447 | val_1_rmse: 0.80498 |  0:00:32s
epoch 111| loss: 0.44125 | val_0_rmse: 0.70552 | val_1_rmse: 0.84488 |  0:00:33s
epoch 112| loss: 0.45722 | val_0_rmse: 0.66463 | val_1_rmse: 0.79508 |  0:00:33s
epoch 113| loss: 0.44027 | val_0_rmse: 0.65422 | val_1_rmse: 0.79806 |  0:00:33s
epoch 114| loss: 0.43396 | val_0_rmse: 0.65128 | val_1_rmse: 0.80068 |  0:00:33s
epoch 115| loss: 0.43597 | val_0_rmse: 0.64734 | val_1_rmse: 0.79464 |  0:00:34s
epoch 116| loss: 0.44018 | val_0_rmse: 0.64219 | val_1_rmse: 0.7971  |  0:00:34s
epoch 117| loss: 0.42139 | val_0_rmse: 0.64923 | val_1_rmse: 0.81434 |  0:00:34s
epoch 118| loss: 0.42311 | val_0_rmse: 0.63641 | val_1_rmse: 0.80864 |  0:00:35s
epoch 119| loss: 0.42126 | val_0_rmse: 0.63555 | val_1_rmse: 0.81122 |  0:00:35s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.76691
Best weights from best epoch are automatically used!
ended training at: 05:14:48
Feature importance:
Mean squared error is of 0.06287838836813826
Mean absolute error:0.18000025169509323
MAPE:0.19642683736197594
R2 score:0.31469921307239845
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:14:49
epoch 0  | loss: 2.11695 | val_0_rmse: 1.01027 | val_1_rmse: 0.97444 |  0:00:00s
epoch 1  | loss: 1.19329 | val_0_rmse: 1.00511 | val_1_rmse: 0.97221 |  0:00:00s
epoch 2  | loss: 1.0548  | val_0_rmse: 1.00679 | val_1_rmse: 0.97298 |  0:00:00s
epoch 3  | loss: 1.03384 | val_0_rmse: 1.00693 | val_1_rmse: 0.97324 |  0:00:01s
epoch 4  | loss: 1.01969 | val_0_rmse: 1.00694 | val_1_rmse: 0.97182 |  0:00:01s
epoch 5  | loss: 1.02312 | val_0_rmse: 1.0068  | val_1_rmse: 0.97194 |  0:00:01s
epoch 6  | loss: 1.01742 | val_0_rmse: 1.00695 | val_1_rmse: 0.97219 |  0:00:02s
epoch 7  | loss: 1.0147  | val_0_rmse: 1.00616 | val_1_rmse: 0.97224 |  0:00:02s
epoch 8  | loss: 1.0185  | val_0_rmse: 1.00577 | val_1_rmse: 0.97317 |  0:00:02s
epoch 9  | loss: 1.01144 | val_0_rmse: 1.00572 | val_1_rmse: 0.9721  |  0:00:02s
epoch 10 | loss: 1.00768 | val_0_rmse: 1.00448 | val_1_rmse: 0.9716  |  0:00:03s
epoch 11 | loss: 1.00959 | val_0_rmse: 1.00304 | val_1_rmse: 0.97021 |  0:00:03s
epoch 12 | loss: 1.00723 | val_0_rmse: 1.00559 | val_1_rmse: 0.9718  |  0:00:03s
epoch 13 | loss: 1.00681 | val_0_rmse: 1.00705 | val_1_rmse: 0.9707  |  0:00:04s
epoch 14 | loss: 1.00794 | val_0_rmse: 1.00615 | val_1_rmse: 0.96994 |  0:00:04s
epoch 15 | loss: 0.999   | val_0_rmse: 1.00314 | val_1_rmse: 0.96865 |  0:00:04s
epoch 16 | loss: 1.00729 | val_0_rmse: 1.00224 | val_1_rmse: 0.96686 |  0:00:05s
epoch 17 | loss: 0.99592 | val_0_rmse: 1.00376 | val_1_rmse: 0.96727 |  0:00:05s
epoch 18 | loss: 0.99583 | val_0_rmse: 1.00043 | val_1_rmse: 0.9641  |  0:00:05s
epoch 19 | loss: 0.98857 | val_0_rmse: 0.9957  | val_1_rmse: 0.95901 |  0:00:05s
epoch 20 | loss: 0.98367 | val_0_rmse: 0.99104 | val_1_rmse: 0.95248 |  0:00:06s
epoch 21 | loss: 0.98937 | val_0_rmse: 0.98484 | val_1_rmse: 0.94973 |  0:00:06s
epoch 22 | loss: 0.9856  | val_0_rmse: 0.98029 | val_1_rmse: 0.94966 |  0:00:06s
epoch 23 | loss: 0.96644 | val_0_rmse: 0.96916 | val_1_rmse: 0.94134 |  0:00:07s
epoch 24 | loss: 0.94434 | val_0_rmse: 0.96274 | val_1_rmse: 0.94184 |  0:00:07s
epoch 25 | loss: 0.92781 | val_0_rmse: 0.9463  | val_1_rmse: 0.91794 |  0:00:07s
epoch 26 | loss: 0.89996 | val_0_rmse: 0.92602 | val_1_rmse: 0.89787 |  0:00:08s
epoch 27 | loss: 0.86217 | val_0_rmse: 0.90376 | val_1_rmse: 0.87454 |  0:00:08s
epoch 28 | loss: 0.82457 | val_0_rmse: 0.88356 | val_1_rmse: 0.85799 |  0:00:08s
epoch 29 | loss: 0.79356 | val_0_rmse: 0.89189 | val_1_rmse: 0.85336 |  0:00:09s
epoch 30 | loss: 0.78573 | val_0_rmse: 0.84882 | val_1_rmse: 0.81719 |  0:00:09s
epoch 31 | loss: 0.76869 | val_0_rmse: 0.85218 | val_1_rmse: 0.81494 |  0:00:09s
epoch 32 | loss: 0.7395  | val_0_rmse: 0.87573 | val_1_rmse: 0.83982 |  0:00:09s
epoch 33 | loss: 0.73316 | val_0_rmse: 0.88871 | val_1_rmse: 0.86118 |  0:00:10s
epoch 34 | loss: 0.73548 | val_0_rmse: 0.8456  | val_1_rmse: 0.81907 |  0:00:10s
epoch 35 | loss: 0.7365  | val_0_rmse: 0.83489 | val_1_rmse: 0.81115 |  0:00:10s
epoch 36 | loss: 0.716   | val_0_rmse: 0.87964 | val_1_rmse: 0.86082 |  0:00:11s
epoch 37 | loss: 0.69352 | val_0_rmse: 0.86539 | val_1_rmse: 0.84067 |  0:00:11s
epoch 38 | loss: 0.6883  | val_0_rmse: 0.84802 | val_1_rmse: 0.82069 |  0:00:11s
epoch 39 | loss: 0.66135 | val_0_rmse: 0.82947 | val_1_rmse: 0.80342 |  0:00:12s
epoch 40 | loss: 0.66326 | val_0_rmse: 0.8245  | val_1_rmse: 0.79764 |  0:00:12s
epoch 41 | loss: 0.65261 | val_0_rmse: 0.82572 | val_1_rmse: 0.79304 |  0:00:12s
epoch 42 | loss: 0.64004 | val_0_rmse: 0.81797 | val_1_rmse: 0.7906  |  0:00:12s
epoch 43 | loss: 0.63421 | val_0_rmse: 0.80693 | val_1_rmse: 0.77912 |  0:00:13s
epoch 44 | loss: 0.62391 | val_0_rmse: 0.80291 | val_1_rmse: 0.77899 |  0:00:13s
epoch 45 | loss: 0.60358 | val_0_rmse: 0.80428 | val_1_rmse: 0.78169 |  0:00:13s
epoch 46 | loss: 0.5987  | val_0_rmse: 0.80125 | val_1_rmse: 0.78023 |  0:00:14s
epoch 47 | loss: 0.59223 | val_0_rmse: 0.80672 | val_1_rmse: 0.78581 |  0:00:14s
epoch 48 | loss: 0.58898 | val_0_rmse: 0.80148 | val_1_rmse: 0.78264 |  0:00:14s
epoch 49 | loss: 0.5881  | val_0_rmse: 0.79437 | val_1_rmse: 0.77418 |  0:00:15s
epoch 50 | loss: 0.58211 | val_0_rmse: 0.79186 | val_1_rmse: 0.77262 |  0:00:15s
epoch 51 | loss: 0.57751 | val_0_rmse: 0.79492 | val_1_rmse: 0.78342 |  0:00:15s
epoch 52 | loss: 0.57845 | val_0_rmse: 0.79456 | val_1_rmse: 0.78525 |  0:00:15s
epoch 53 | loss: 0.56441 | val_0_rmse: 0.78391 | val_1_rmse: 0.76852 |  0:00:16s
epoch 54 | loss: 0.56834 | val_0_rmse: 0.78063 | val_1_rmse: 0.76377 |  0:00:16s
epoch 55 | loss: 0.55538 | val_0_rmse: 0.7861  | val_1_rmse: 0.77602 |  0:00:16s
epoch 56 | loss: 0.56441 | val_0_rmse: 0.78045 | val_1_rmse: 0.77309 |  0:00:17s
epoch 57 | loss: 0.55316 | val_0_rmse: 0.78887 | val_1_rmse: 0.77908 |  0:00:17s
epoch 58 | loss: 0.55998 | val_0_rmse: 0.78204 | val_1_rmse: 0.76865 |  0:00:17s
epoch 59 | loss: 0.56404 | val_0_rmse: 0.78287 | val_1_rmse: 0.77311 |  0:00:17s
epoch 60 | loss: 0.55785 | val_0_rmse: 0.79378 | val_1_rmse: 0.79104 |  0:00:18s
epoch 61 | loss: 0.55382 | val_0_rmse: 0.78069 | val_1_rmse: 0.77772 |  0:00:18s
epoch 62 | loss: 0.55753 | val_0_rmse: 0.79051 | val_1_rmse: 0.7855  |  0:00:18s
epoch 63 | loss: 0.55685 | val_0_rmse: 0.78094 | val_1_rmse: 0.77658 |  0:00:19s
epoch 64 | loss: 0.55524 | val_0_rmse: 0.77681 | val_1_rmse: 0.77237 |  0:00:19s
epoch 65 | loss: 0.55077 | val_0_rmse: 0.77021 | val_1_rmse: 0.77208 |  0:00:19s
epoch 66 | loss: 0.54922 | val_0_rmse: 0.7706  | val_1_rmse: 0.77375 |  0:00:20s
epoch 67 | loss: 0.55312 | val_0_rmse: 0.76954 | val_1_rmse: 0.76597 |  0:00:20s
epoch 68 | loss: 0.54675 | val_0_rmse: 0.76735 | val_1_rmse: 0.76808 |  0:00:20s
epoch 69 | loss: 0.54135 | val_0_rmse: 0.76176 | val_1_rmse: 0.76917 |  0:00:20s
epoch 70 | loss: 0.54221 | val_0_rmse: 0.77504 | val_1_rmse: 0.7808  |  0:00:21s
epoch 71 | loss: 0.54204 | val_0_rmse: 0.76328 | val_1_rmse: 0.76584 |  0:00:21s
epoch 72 | loss: 0.53688 | val_0_rmse: 0.77626 | val_1_rmse: 0.77679 |  0:00:21s
epoch 73 | loss: 0.5432  | val_0_rmse: 0.75661 | val_1_rmse: 0.75899 |  0:00:22s
epoch 74 | loss: 0.53883 | val_0_rmse: 0.75939 | val_1_rmse: 0.76485 |  0:00:22s
epoch 75 | loss: 0.52968 | val_0_rmse: 0.77227 | val_1_rmse: 0.78021 |  0:00:22s
epoch 76 | loss: 0.5374  | val_0_rmse: 0.76634 | val_1_rmse: 0.77097 |  0:00:22s
epoch 77 | loss: 0.53135 | val_0_rmse: 0.76858 | val_1_rmse: 0.77663 |  0:00:23s
epoch 78 | loss: 0.53584 | val_0_rmse: 0.76087 | val_1_rmse: 0.76758 |  0:00:23s
epoch 79 | loss: 0.53361 | val_0_rmse: 0.74471 | val_1_rmse: 0.75297 |  0:00:23s
epoch 80 | loss: 0.52569 | val_0_rmse: 0.7491  | val_1_rmse: 0.76425 |  0:00:24s
epoch 81 | loss: 0.52171 | val_0_rmse: 0.74444 | val_1_rmse: 0.76471 |  0:00:24s
epoch 82 | loss: 0.52704 | val_0_rmse: 0.74348 | val_1_rmse: 0.76171 |  0:00:24s
epoch 83 | loss: 0.5159  | val_0_rmse: 0.74675 | val_1_rmse: 0.77027 |  0:00:25s
epoch 84 | loss: 0.53438 | val_0_rmse: 0.7513  | val_1_rmse: 0.77226 |  0:00:25s
epoch 85 | loss: 0.52137 | val_0_rmse: 0.75161 | val_1_rmse: 0.76495 |  0:00:25s
epoch 86 | loss: 0.52728 | val_0_rmse: 0.73647 | val_1_rmse: 0.75654 |  0:00:25s
epoch 87 | loss: 0.51197 | val_0_rmse: 0.73792 | val_1_rmse: 0.76175 |  0:00:26s
epoch 88 | loss: 0.51032 | val_0_rmse: 0.75341 | val_1_rmse: 0.78231 |  0:00:26s
epoch 89 | loss: 0.51267 | val_0_rmse: 0.7357  | val_1_rmse: 0.76592 |  0:00:26s
epoch 90 | loss: 0.51526 | val_0_rmse: 0.73835 | val_1_rmse: 0.76515 |  0:00:27s
epoch 91 | loss: 0.51188 | val_0_rmse: 0.75306 | val_1_rmse: 0.77575 |  0:00:27s
epoch 92 | loss: 0.50785 | val_0_rmse: 0.75118 | val_1_rmse: 0.76983 |  0:00:27s
epoch 93 | loss: 0.50528 | val_0_rmse: 0.73643 | val_1_rmse: 0.7651  |  0:00:27s
epoch 94 | loss: 0.51815 | val_0_rmse: 0.72824 | val_1_rmse: 0.75955 |  0:00:28s
epoch 95 | loss: 0.5036  | val_0_rmse: 0.71355 | val_1_rmse: 0.7529  |  0:00:28s
epoch 96 | loss: 0.48982 | val_0_rmse: 0.70645 | val_1_rmse: 0.74996 |  0:00:28s
epoch 97 | loss: 0.4934  | val_0_rmse: 0.71266 | val_1_rmse: 0.74839 |  0:00:29s
epoch 98 | loss: 0.49195 | val_0_rmse: 0.71666 | val_1_rmse: 0.76478 |  0:00:29s
epoch 99 | loss: 0.48176 | val_0_rmse: 0.69974 | val_1_rmse: 0.77023 |  0:00:29s
epoch 100| loss: 0.47942 | val_0_rmse: 0.70158 | val_1_rmse: 0.76278 |  0:00:30s
epoch 101| loss: 0.48372 | val_0_rmse: 0.73317 | val_1_rmse: 0.78546 |  0:00:30s
epoch 102| loss: 0.49687 | val_0_rmse: 0.71106 | val_1_rmse: 0.78159 |  0:00:30s
epoch 103| loss: 0.49207 | val_0_rmse: 0.76646 | val_1_rmse: 0.83913 |  0:00:31s
epoch 104| loss: 0.49375 | val_0_rmse: 0.69587 | val_1_rmse: 0.76717 |  0:00:31s
epoch 105| loss: 0.48487 | val_0_rmse: 0.69432 | val_1_rmse: 0.77172 |  0:00:31s
epoch 106| loss: 0.48803 | val_0_rmse: 0.70005 | val_1_rmse: 0.79352 |  0:00:31s
epoch 107| loss: 0.47254 | val_0_rmse: 0.69296 | val_1_rmse: 0.77239 |  0:00:32s
epoch 108| loss: 0.48799 | val_0_rmse: 0.69937 | val_1_rmse: 0.7795  |  0:00:32s
epoch 109| loss: 0.47363 | val_0_rmse: 0.68928 | val_1_rmse: 0.77946 |  0:00:32s
epoch 110| loss: 0.45837 | val_0_rmse: 0.6806  | val_1_rmse: 0.75851 |  0:00:33s
epoch 111| loss: 0.45914 | val_0_rmse: 0.67983 | val_1_rmse: 0.76247 |  0:00:33s
epoch 112| loss: 0.46473 | val_0_rmse: 0.67783 | val_1_rmse: 0.76764 |  0:00:33s
epoch 113| loss: 0.46671 | val_0_rmse: 0.6891  | val_1_rmse: 0.76149 |  0:00:33s
epoch 114| loss: 0.46416 | val_0_rmse: 0.67677 | val_1_rmse: 0.76567 |  0:00:34s
epoch 115| loss: 0.45838 | val_0_rmse: 0.67834 | val_1_rmse: 0.77802 |  0:00:34s
epoch 116| loss: 0.46002 | val_0_rmse: 0.66804 | val_1_rmse: 0.76399 |  0:00:34s
epoch 117| loss: 0.44808 | val_0_rmse: 0.6548  | val_1_rmse: 0.7619  |  0:00:35s
epoch 118| loss: 0.45577 | val_0_rmse: 0.66348 | val_1_rmse: 0.76525 |  0:00:35s
epoch 119| loss: 0.4439  | val_0_rmse: 0.66243 | val_1_rmse: 0.75432 |  0:00:35s
epoch 120| loss: 0.45537 | val_0_rmse: 0.65626 | val_1_rmse: 0.75677 |  0:00:36s
epoch 121| loss: 0.43938 | val_0_rmse: 0.65176 | val_1_rmse: 0.76506 |  0:00:36s
epoch 122| loss: 0.44452 | val_0_rmse: 0.66101 | val_1_rmse: 0.77735 |  0:00:36s
epoch 123| loss: 0.43615 | val_0_rmse: 0.65928 | val_1_rmse: 0.77615 |  0:00:36s
epoch 124| loss: 0.44181 | val_0_rmse: 0.64686 | val_1_rmse: 0.7752  |  0:00:37s
epoch 125| loss: 0.4309  | val_0_rmse: 0.63831 | val_1_rmse: 0.78745 |  0:00:37s
epoch 126| loss: 0.4371  | val_0_rmse: 0.6577  | val_1_rmse: 0.8017  |  0:00:37s
epoch 127| loss: 0.43415 | val_0_rmse: 0.64338 | val_1_rmse: 0.77778 |  0:00:38s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.74839
Best weights from best epoch are automatically used!
ended training at: 05:15:27
Feature importance:
Mean squared error is of 0.0626169937049303
Mean absolute error:0.17247537473370114
MAPE:0.1824222624384827
R2 score:0.3213734540533044
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:15:27
epoch 0  | loss: 1.82278 | val_0_rmse: 1.02153 | val_1_rmse: 1.04246 |  0:00:00s
epoch 1  | loss: 1.1715  | val_0_rmse: 1.00095 | val_1_rmse: 1.02546 |  0:00:00s
epoch 2  | loss: 1.03785 | val_0_rmse: 0.99952 | val_1_rmse: 1.02341 |  0:00:00s
epoch 3  | loss: 1.02281 | val_0_rmse: 0.99725 | val_1_rmse: 1.02155 |  0:00:01s
epoch 4  | loss: 0.98971 | val_0_rmse: 0.99435 | val_1_rmse: 1.01898 |  0:00:01s
epoch 5  | loss: 0.98306 | val_0_rmse: 0.99313 | val_1_rmse: 1.01838 |  0:00:01s
epoch 6  | loss: 0.96519 | val_0_rmse: 0.98879 | val_1_rmse: 1.01658 |  0:00:02s
epoch 7  | loss: 0.94879 | val_0_rmse: 0.97381 | val_1_rmse: 1.00236 |  0:00:02s
epoch 8  | loss: 0.92574 | val_0_rmse: 0.95596 | val_1_rmse: 0.98047 |  0:00:02s
epoch 9  | loss: 0.88181 | val_0_rmse: 0.90903 | val_1_rmse: 0.89912 |  0:00:02s
epoch 10 | loss: 0.85933 | val_0_rmse: 0.91728 | val_1_rmse: 0.91204 |  0:00:03s
epoch 11 | loss: 0.82517 | val_0_rmse: 0.94668 | val_1_rmse: 0.92479 |  0:00:03s
epoch 12 | loss: 0.78881 | val_0_rmse: 0.93316 | val_1_rmse: 0.91633 |  0:00:03s
epoch 13 | loss: 0.7633  | val_0_rmse: 0.88244 | val_1_rmse: 0.88498 |  0:00:04s
epoch 14 | loss: 0.74614 | val_0_rmse: 0.86138 | val_1_rmse: 0.87104 |  0:00:04s
epoch 15 | loss: 0.75067 | val_0_rmse: 0.85857 | val_1_rmse: 0.86919 |  0:00:04s
epoch 16 | loss: 0.72957 | val_0_rmse: 0.85648 | val_1_rmse: 0.86106 |  0:00:05s
epoch 17 | loss: 0.71021 | val_0_rmse: 0.84887 | val_1_rmse: 0.86942 |  0:00:05s
epoch 18 | loss: 0.70779 | val_0_rmse: 0.84877 | val_1_rmse: 0.8663  |  0:00:05s
epoch 19 | loss: 0.71215 | val_0_rmse: 0.84849 | val_1_rmse: 0.86713 |  0:00:05s
epoch 20 | loss: 0.70649 | val_0_rmse: 0.84117 | val_1_rmse: 0.85593 |  0:00:06s
epoch 21 | loss: 0.70955 | val_0_rmse: 0.85863 | val_1_rmse: 0.8784  |  0:00:06s
epoch 22 | loss: 0.69909 | val_0_rmse: 0.83098 | val_1_rmse: 0.84813 |  0:00:06s
epoch 23 | loss: 0.67691 | val_0_rmse: 0.85307 | val_1_rmse: 0.85817 |  0:00:07s
epoch 24 | loss: 0.66831 | val_0_rmse: 0.82442 | val_1_rmse: 0.83278 |  0:00:07s
epoch 25 | loss: 0.66038 | val_0_rmse: 0.85325 | val_1_rmse: 0.8606  |  0:00:07s
epoch 26 | loss: 0.65747 | val_0_rmse: 0.8297  | val_1_rmse: 0.84697 |  0:00:08s
epoch 27 | loss: 0.66769 | val_0_rmse: 0.8554  | val_1_rmse: 0.87017 |  0:00:08s
epoch 28 | loss: 0.66218 | val_0_rmse: 0.82036 | val_1_rmse: 0.84134 |  0:00:08s
epoch 29 | loss: 0.66288 | val_0_rmse: 0.84628 | val_1_rmse: 0.87154 |  0:00:08s
epoch 30 | loss: 0.65004 | val_0_rmse: 0.8327  | val_1_rmse: 0.84079 |  0:00:09s
epoch 31 | loss: 0.64257 | val_0_rmse: 0.82591 | val_1_rmse: 0.81937 |  0:00:09s
epoch 32 | loss: 0.63512 | val_0_rmse: 0.82131 | val_1_rmse: 0.82191 |  0:00:09s
epoch 33 | loss: 0.64971 | val_0_rmse: 0.84974 | val_1_rmse: 0.84686 |  0:00:10s
epoch 34 | loss: 0.65841 | val_0_rmse: 0.80931 | val_1_rmse: 0.81797 |  0:00:10s
epoch 35 | loss: 0.63371 | val_0_rmse: 0.81808 | val_1_rmse: 0.83103 |  0:00:10s
epoch 36 | loss: 0.63915 | val_0_rmse: 0.81957 | val_1_rmse: 0.82668 |  0:00:11s
epoch 37 | loss: 0.64774 | val_0_rmse: 0.81397 | val_1_rmse: 0.82495 |  0:00:11s
epoch 38 | loss: 0.63771 | val_0_rmse: 0.8182  | val_1_rmse: 0.83738 |  0:00:11s
epoch 39 | loss: 0.63782 | val_0_rmse: 0.81212 | val_1_rmse: 0.83519 |  0:00:12s
epoch 40 | loss: 0.63443 | val_0_rmse: 0.82192 | val_1_rmse: 0.84233 |  0:00:12s
epoch 41 | loss: 0.63296 | val_0_rmse: 0.80933 | val_1_rmse: 0.82871 |  0:00:12s
epoch 42 | loss: 0.62499 | val_0_rmse: 0.81721 | val_1_rmse: 0.83279 |  0:00:12s
epoch 43 | loss: 0.62094 | val_0_rmse: 0.81533 | val_1_rmse: 0.82949 |  0:00:13s
epoch 44 | loss: 0.62361 | val_0_rmse: 0.8208  | val_1_rmse: 0.82645 |  0:00:13s
epoch 45 | loss: 0.62035 | val_0_rmse: 0.81646 | val_1_rmse: 0.81913 |  0:00:13s
epoch 46 | loss: 0.6197  | val_0_rmse: 0.8106  | val_1_rmse: 0.81878 |  0:00:14s
epoch 47 | loss: 0.61939 | val_0_rmse: 0.80311 | val_1_rmse: 0.81837 |  0:00:14s
epoch 48 | loss: 0.62826 | val_0_rmse: 0.80604 | val_1_rmse: 0.82393 |  0:00:14s
epoch 49 | loss: 0.61438 | val_0_rmse: 0.80137 | val_1_rmse: 0.81872 |  0:00:14s
epoch 50 | loss: 0.61091 | val_0_rmse: 0.81049 | val_1_rmse: 0.81633 |  0:00:15s
epoch 51 | loss: 0.62239 | val_0_rmse: 0.80391 | val_1_rmse: 0.81415 |  0:00:15s
epoch 52 | loss: 0.62967 | val_0_rmse: 0.80809 | val_1_rmse: 0.82593 |  0:00:15s
epoch 53 | loss: 0.61717 | val_0_rmse: 0.8018  | val_1_rmse: 0.81885 |  0:00:16s
epoch 54 | loss: 0.61277 | val_0_rmse: 0.79935 | val_1_rmse: 0.81403 |  0:00:16s
epoch 55 | loss: 0.61098 | val_0_rmse: 0.80067 | val_1_rmse: 0.81608 |  0:00:16s
epoch 56 | loss: 0.6067  | val_0_rmse: 0.80119 | val_1_rmse: 0.82131 |  0:00:17s
epoch 57 | loss: 0.617   | val_0_rmse: 0.79281 | val_1_rmse: 0.81114 |  0:00:17s
epoch 58 | loss: 0.61186 | val_0_rmse: 0.79698 | val_1_rmse: 0.8223  |  0:00:17s
epoch 59 | loss: 0.62193 | val_0_rmse: 0.80593 | val_1_rmse: 0.83669 |  0:00:17s
epoch 60 | loss: 0.61345 | val_0_rmse: 0.81473 | val_1_rmse: 0.83302 |  0:00:18s
epoch 61 | loss: 0.61601 | val_0_rmse: 0.79832 | val_1_rmse: 0.81931 |  0:00:18s
epoch 62 | loss: 0.62707 | val_0_rmse: 0.79118 | val_1_rmse: 0.81569 |  0:00:18s
epoch 63 | loss: 0.60928 | val_0_rmse: 0.78702 | val_1_rmse: 0.81499 |  0:00:19s
epoch 64 | loss: 0.61173 | val_0_rmse: 0.78756 | val_1_rmse: 0.81362 |  0:00:19s
epoch 65 | loss: 0.60735 | val_0_rmse: 0.78851 | val_1_rmse: 0.81322 |  0:00:19s
epoch 66 | loss: 0.60797 | val_0_rmse: 0.79455 | val_1_rmse: 0.82318 |  0:00:19s
epoch 67 | loss: 0.60212 | val_0_rmse: 0.79054 | val_1_rmse: 0.81566 |  0:00:20s
epoch 68 | loss: 0.60322 | val_0_rmse: 0.78925 | val_1_rmse: 0.81419 |  0:00:20s
epoch 69 | loss: 0.60492 | val_0_rmse: 0.78155 | val_1_rmse: 0.80423 |  0:00:20s
epoch 70 | loss: 0.60352 | val_0_rmse: 0.79188 | val_1_rmse: 0.805   |  0:00:21s
epoch 71 | loss: 0.60625 | val_0_rmse: 0.79407 | val_1_rmse: 0.82119 |  0:00:21s
epoch 72 | loss: 0.60149 | val_0_rmse: 0.78014 | val_1_rmse: 0.81273 |  0:00:21s
epoch 73 | loss: 0.59622 | val_0_rmse: 0.78197 | val_1_rmse: 0.81364 |  0:00:21s
epoch 74 | loss: 0.60327 | val_0_rmse: 0.79389 | val_1_rmse: 0.82148 |  0:00:22s
epoch 75 | loss: 0.5985  | val_0_rmse: 0.78257 | val_1_rmse: 0.81128 |  0:00:22s
epoch 76 | loss: 0.59774 | val_0_rmse: 0.77676 | val_1_rmse: 0.80869 |  0:00:22s
epoch 77 | loss: 0.60711 | val_0_rmse: 0.7912  | val_1_rmse: 0.82149 |  0:00:23s
epoch 78 | loss: 0.59471 | val_0_rmse: 0.78659 | val_1_rmse: 0.8199  |  0:00:23s
epoch 79 | loss: 0.60336 | val_0_rmse: 0.77811 | val_1_rmse: 0.81087 |  0:00:23s
epoch 80 | loss: 0.59376 | val_0_rmse: 0.77848 | val_1_rmse: 0.80444 |  0:00:24s
epoch 81 | loss: 0.5937  | val_0_rmse: 0.77419 | val_1_rmse: 0.79988 |  0:00:24s
epoch 82 | loss: 0.59209 | val_0_rmse: 0.77198 | val_1_rmse: 0.80632 |  0:00:24s
epoch 83 | loss: 0.58831 | val_0_rmse: 0.77386 | val_1_rmse: 0.80544 |  0:00:24s
epoch 84 | loss: 0.58856 | val_0_rmse: 0.76894 | val_1_rmse: 0.79883 |  0:00:25s
epoch 85 | loss: 0.58912 | val_0_rmse: 0.77304 | val_1_rmse: 0.80758 |  0:00:25s
epoch 86 | loss: 0.58543 | val_0_rmse: 0.77388 | val_1_rmse: 0.81104 |  0:00:25s
epoch 87 | loss: 0.58404 | val_0_rmse: 0.77238 | val_1_rmse: 0.80656 |  0:00:26s
epoch 88 | loss: 0.59425 | val_0_rmse: 0.76869 | val_1_rmse: 0.8009  |  0:00:26s
epoch 89 | loss: 0.59032 | val_0_rmse: 0.77741 | val_1_rmse: 0.8134  |  0:00:26s
epoch 90 | loss: 0.59347 | val_0_rmse: 0.76735 | val_1_rmse: 0.80676 |  0:00:26s
epoch 91 | loss: 0.58314 | val_0_rmse: 0.78063 | val_1_rmse: 0.82086 |  0:00:27s
epoch 92 | loss: 0.59701 | val_0_rmse: 0.76743 | val_1_rmse: 0.81288 |  0:00:27s
epoch 93 | loss: 0.59603 | val_0_rmse: 0.77028 | val_1_rmse: 0.81322 |  0:00:27s
epoch 94 | loss: 0.58866 | val_0_rmse: 0.76453 | val_1_rmse: 0.81389 |  0:00:28s
epoch 95 | loss: 0.58789 | val_0_rmse: 0.76853 | val_1_rmse: 0.81587 |  0:00:28s
epoch 96 | loss: 0.58666 | val_0_rmse: 0.76281 | val_1_rmse: 0.80754 |  0:00:28s
epoch 97 | loss: 0.5826  | val_0_rmse: 0.76451 | val_1_rmse: 0.80163 |  0:00:28s
epoch 98 | loss: 0.58014 | val_0_rmse: 0.76707 | val_1_rmse: 0.80823 |  0:00:29s
epoch 99 | loss: 0.57606 | val_0_rmse: 0.7661  | val_1_rmse: 0.80977 |  0:00:29s
epoch 100| loss: 0.58646 | val_0_rmse: 0.76506 | val_1_rmse: 0.80138 |  0:00:29s
epoch 101| loss: 0.58039 | val_0_rmse: 0.7675  | val_1_rmse: 0.80647 |  0:00:30s
epoch 102| loss: 0.58481 | val_0_rmse: 0.76151 | val_1_rmse: 0.80946 |  0:00:30s
epoch 103| loss: 0.58618 | val_0_rmse: 0.7624  | val_1_rmse: 0.81494 |  0:00:30s
epoch 104| loss: 0.57809 | val_0_rmse: 0.76123 | val_1_rmse: 0.80163 |  0:00:31s
epoch 105| loss: 0.59479 | val_0_rmse: 0.76737 | val_1_rmse: 0.80287 |  0:00:31s
epoch 106| loss: 0.58681 | val_0_rmse: 0.76492 | val_1_rmse: 0.80802 |  0:00:31s
epoch 107| loss: 0.57842 | val_0_rmse: 0.76181 | val_1_rmse: 0.82133 |  0:00:32s
epoch 108| loss: 0.57814 | val_0_rmse: 0.7697  | val_1_rmse: 0.83139 |  0:00:32s
epoch 109| loss: 0.58195 | val_0_rmse: 0.75132 | val_1_rmse: 0.80691 |  0:00:32s
epoch 110| loss: 0.564   | val_0_rmse: 0.74954 | val_1_rmse: 0.80127 |  0:00:32s
epoch 111| loss: 0.57121 | val_0_rmse: 0.75191 | val_1_rmse: 0.7978  |  0:00:33s
epoch 112| loss: 0.56879 | val_0_rmse: 0.7554  | val_1_rmse: 0.79649 |  0:00:33s
epoch 113| loss: 0.5755  | val_0_rmse: 0.7551  | val_1_rmse: 0.7973  |  0:00:33s
epoch 114| loss: 0.56719 | val_0_rmse: 0.75497 | val_1_rmse: 0.8038  |  0:00:34s
epoch 115| loss: 0.5674  | val_0_rmse: 0.75416 | val_1_rmse: 0.80689 |  0:00:34s
epoch 116| loss: 0.56148 | val_0_rmse: 0.74862 | val_1_rmse: 0.80316 |  0:00:34s
epoch 117| loss: 0.57091 | val_0_rmse: 0.75255 | val_1_rmse: 0.80752 |  0:00:34s
epoch 118| loss: 0.57533 | val_0_rmse: 0.76084 | val_1_rmse: 0.80545 |  0:00:35s
epoch 119| loss: 0.56981 | val_0_rmse: 0.75693 | val_1_rmse: 0.80735 |  0:00:35s
epoch 120| loss: 0.58506 | val_0_rmse: 0.7534  | val_1_rmse: 0.79807 |  0:00:35s
epoch 121| loss: 0.57658 | val_0_rmse: 0.74779 | val_1_rmse: 0.79053 |  0:00:36s
epoch 122| loss: 0.5681  | val_0_rmse: 0.74978 | val_1_rmse: 0.79712 |  0:00:36s
epoch 123| loss: 0.58241 | val_0_rmse: 0.75302 | val_1_rmse: 0.80165 |  0:00:36s
epoch 124| loss: 0.58355 | val_0_rmse: 0.75249 | val_1_rmse: 0.79469 |  0:00:37s
epoch 125| loss: 0.57517 | val_0_rmse: 0.75441 | val_1_rmse: 0.79639 |  0:00:37s
epoch 126| loss: 0.57083 | val_0_rmse: 0.74622 | val_1_rmse: 0.80155 |  0:00:37s
epoch 127| loss: 0.57806 | val_0_rmse: 0.75389 | val_1_rmse: 0.8051  |  0:00:37s
epoch 128| loss: 0.57722 | val_0_rmse: 0.75731 | val_1_rmse: 0.80921 |  0:00:38s
epoch 129| loss: 0.57526 | val_0_rmse: 0.75259 | val_1_rmse: 0.79752 |  0:00:38s
epoch 130| loss: 0.57764 | val_0_rmse: 0.74718 | val_1_rmse: 0.79273 |  0:00:38s
epoch 131| loss: 0.56758 | val_0_rmse: 0.74404 | val_1_rmse: 0.79916 |  0:00:39s
epoch 132| loss: 0.56433 | val_0_rmse: 0.74231 | val_1_rmse: 0.8041  |  0:00:39s
epoch 133| loss: 0.55933 | val_0_rmse: 0.74606 | val_1_rmse: 0.80351 |  0:00:39s
epoch 134| loss: 0.57076 | val_0_rmse: 0.74691 | val_1_rmse: 0.8028  |  0:00:39s
epoch 135| loss: 0.55822 | val_0_rmse: 0.74006 | val_1_rmse: 0.80922 |  0:00:40s
epoch 136| loss: 0.55519 | val_0_rmse: 0.74227 | val_1_rmse: 0.80929 |  0:00:40s
epoch 137| loss: 0.55665 | val_0_rmse: 0.74125 | val_1_rmse: 0.79047 |  0:00:40s
epoch 138| loss: 0.56089 | val_0_rmse: 0.7352  | val_1_rmse: 0.79312 |  0:00:41s
epoch 139| loss: 0.55392 | val_0_rmse: 0.73516 | val_1_rmse: 0.80681 |  0:00:41s
epoch 140| loss: 0.55662 | val_0_rmse: 0.73616 | val_1_rmse: 0.79495 |  0:00:41s
epoch 141| loss: 0.56541 | val_0_rmse: 0.73748 | val_1_rmse: 0.79107 |  0:00:41s
epoch 142| loss: 0.5728  | val_0_rmse: 0.74116 | val_1_rmse: 0.79781 |  0:00:42s
epoch 143| loss: 0.56053 | val_0_rmse: 0.74956 | val_1_rmse: 0.80194 |  0:00:42s
epoch 144| loss: 0.55829 | val_0_rmse: 0.7335  | val_1_rmse: 0.79327 |  0:00:42s
epoch 145| loss: 0.5528  | val_0_rmse: 0.73017 | val_1_rmse: 0.79404 |  0:00:43s
epoch 146| loss: 0.55083 | val_0_rmse: 0.7278  | val_1_rmse: 0.78951 |  0:00:43s
epoch 147| loss: 0.54294 | val_0_rmse: 0.73111 | val_1_rmse: 0.79599 |  0:00:43s
epoch 148| loss: 0.54775 | val_0_rmse: 0.73157 | val_1_rmse: 0.79868 |  0:00:44s
epoch 149| loss: 0.55536 | val_0_rmse: 0.73678 | val_1_rmse: 0.79624 |  0:00:44s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.78951
Best weights from best epoch are automatically used!
ended training at: 05:16:12
Feature importance:
Mean squared error is of 0.05364454161736634
Mean absolute error:0.17098950837612095
MAPE:0.19105276414657685
R2 score:0.3541622675111231
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:16:12
epoch 0  | loss: 2.0638  | val_0_rmse: 1.02619 | val_1_rmse: 1.00602 |  0:00:00s
epoch 1  | loss: 1.22691 | val_0_rmse: 1.01805 | val_1_rmse: 0.99136 |  0:00:00s
epoch 2  | loss: 1.06043 | val_0_rmse: 1.0076  | val_1_rmse: 0.97962 |  0:00:00s
epoch 3  | loss: 1.02853 | val_0_rmse: 1.00346 | val_1_rmse: 0.97622 |  0:00:01s
epoch 4  | loss: 1.00625 | val_0_rmse: 0.97437 | val_1_rmse: 0.95087 |  0:00:01s
epoch 5  | loss: 0.9643  | val_0_rmse: 0.92375 | val_1_rmse: 0.92151 |  0:00:01s
epoch 6  | loss: 0.92726 | val_0_rmse: 0.89142 | val_1_rmse: 0.87161 |  0:00:02s
epoch 7  | loss: 0.87317 | val_0_rmse: 0.94754 | val_1_rmse: 0.93615 |  0:00:02s
epoch 8  | loss: 0.85771 | val_0_rmse: 0.91052 | val_1_rmse: 0.89506 |  0:00:02s
epoch 9  | loss: 0.82473 | val_0_rmse: 0.86027 | val_1_rmse: 0.84335 |  0:00:02s
epoch 10 | loss: 0.80228 | val_0_rmse: 0.85561 | val_1_rmse: 0.83972 |  0:00:03s
epoch 11 | loss: 0.75902 | val_0_rmse: 0.88715 | val_1_rmse: 0.88004 |  0:00:03s
epoch 12 | loss: 0.75079 | val_0_rmse: 0.85253 | val_1_rmse: 0.84132 |  0:00:03s
epoch 13 | loss: 0.72633 | val_0_rmse: 0.88707 | val_1_rmse: 0.88455 |  0:00:04s
epoch 14 | loss: 0.73101 | val_0_rmse: 0.86141 | val_1_rmse: 0.85807 |  0:00:04s
epoch 15 | loss: 0.70836 | val_0_rmse: 0.84334 | val_1_rmse: 0.84605 |  0:00:04s
epoch 16 | loss: 0.6983  | val_0_rmse: 0.8504  | val_1_rmse: 0.85876 |  0:00:05s
epoch 17 | loss: 0.67281 | val_0_rmse: 0.84702 | val_1_rmse: 0.84877 |  0:00:05s
epoch 18 | loss: 0.68248 | val_0_rmse: 0.84896 | val_1_rmse: 0.85098 |  0:00:05s
epoch 19 | loss: 0.67064 | val_0_rmse: 0.84828 | val_1_rmse: 0.85085 |  0:00:06s
epoch 20 | loss: 0.68111 | val_0_rmse: 0.82501 | val_1_rmse: 0.83087 |  0:00:06s
epoch 21 | loss: 0.66959 | val_0_rmse: 0.81714 | val_1_rmse: 0.8241  |  0:00:06s
epoch 22 | loss: 0.65758 | val_0_rmse: 0.82922 | val_1_rmse: 0.82935 |  0:00:06s
epoch 23 | loss: 0.66116 | val_0_rmse: 0.81185 | val_1_rmse: 0.80885 |  0:00:07s
epoch 24 | loss: 0.65082 | val_0_rmse: 0.82242 | val_1_rmse: 0.81859 |  0:00:07s
epoch 25 | loss: 0.6532  | val_0_rmse: 0.82487 | val_1_rmse: 0.82264 |  0:00:07s
epoch 26 | loss: 0.64657 | val_0_rmse: 0.80971 | val_1_rmse: 0.80544 |  0:00:08s
epoch 27 | loss: 0.63945 | val_0_rmse: 0.81875 | val_1_rmse: 0.80873 |  0:00:08s
epoch 28 | loss: 0.642   | val_0_rmse: 0.82284 | val_1_rmse: 0.81041 |  0:00:08s
epoch 29 | loss: 0.63592 | val_0_rmse: 0.83107 | val_1_rmse: 0.81785 |  0:00:09s
epoch 30 | loss: 0.64738 | val_0_rmse: 0.81745 | val_1_rmse: 0.80428 |  0:00:09s
epoch 31 | loss: 0.62817 | val_0_rmse: 0.82486 | val_1_rmse: 0.81356 |  0:00:09s
epoch 32 | loss: 0.6351  | val_0_rmse: 0.8193  | val_1_rmse: 0.81    |  0:00:09s
epoch 33 | loss: 0.62308 | val_0_rmse: 0.86764 | val_1_rmse: 0.86987 |  0:00:10s
epoch 34 | loss: 0.64179 | val_0_rmse: 0.81929 | val_1_rmse: 0.81736 |  0:00:10s
epoch 35 | loss: 0.60646 | val_0_rmse: 0.81712 | val_1_rmse: 0.80614 |  0:00:10s
epoch 36 | loss: 0.61486 | val_0_rmse: 0.80134 | val_1_rmse: 0.79464 |  0:00:11s
epoch 37 | loss: 0.59652 | val_0_rmse: 0.79499 | val_1_rmse: 0.78976 |  0:00:11s
epoch 38 | loss: 0.59557 | val_0_rmse: 0.80366 | val_1_rmse: 0.80724 |  0:00:11s
epoch 39 | loss: 0.60574 | val_0_rmse: 0.79454 | val_1_rmse: 0.78991 |  0:00:11s
epoch 40 | loss: 0.59993 | val_0_rmse: 0.80112 | val_1_rmse: 0.79086 |  0:00:12s
epoch 41 | loss: 0.58737 | val_0_rmse: 0.80341 | val_1_rmse: 0.79768 |  0:00:12s
epoch 42 | loss: 0.60206 | val_0_rmse: 0.79534 | val_1_rmse: 0.79128 |  0:00:12s
epoch 43 | loss: 0.59257 | val_0_rmse: 0.79049 | val_1_rmse: 0.78515 |  0:00:13s
epoch 44 | loss: 0.58338 | val_0_rmse: 0.79545 | val_1_rmse: 0.78884 |  0:00:13s
epoch 45 | loss: 0.58461 | val_0_rmse: 0.79742 | val_1_rmse: 0.78837 |  0:00:13s
epoch 46 | loss: 0.59468 | val_0_rmse: 0.80104 | val_1_rmse: 0.79843 |  0:00:14s
epoch 47 | loss: 0.58333 | val_0_rmse: 0.7851  | val_1_rmse: 0.7825  |  0:00:14s
epoch 48 | loss: 0.58268 | val_0_rmse: 0.79011 | val_1_rmse: 0.79182 |  0:00:14s
epoch 49 | loss: 0.57882 | val_0_rmse: 0.79359 | val_1_rmse: 0.79505 |  0:00:14s
epoch 50 | loss: 0.59077 | val_0_rmse: 0.78912 | val_1_rmse: 0.78673 |  0:00:15s
epoch 51 | loss: 0.57892 | val_0_rmse: 0.79678 | val_1_rmse: 0.79931 |  0:00:15s
epoch 52 | loss: 0.57658 | val_0_rmse: 0.78261 | val_1_rmse: 0.77742 |  0:00:15s
epoch 53 | loss: 0.58137 | val_0_rmse: 0.79341 | val_1_rmse: 0.7992  |  0:00:16s
epoch 54 | loss: 0.58281 | val_0_rmse: 0.7869  | val_1_rmse: 0.7835  |  0:00:16s
epoch 55 | loss: 0.59593 | val_0_rmse: 0.7853  | val_1_rmse: 0.7748  |  0:00:16s
epoch 56 | loss: 0.57153 | val_0_rmse: 0.8168  | val_1_rmse: 0.82629 |  0:00:16s
epoch 57 | loss: 0.59089 | val_0_rmse: 0.79067 | val_1_rmse: 0.78692 |  0:00:17s
epoch 58 | loss: 0.57991 | val_0_rmse: 0.78371 | val_1_rmse: 0.7742  |  0:00:17s
epoch 59 | loss: 0.5707  | val_0_rmse: 0.78418 | val_1_rmse: 0.79152 |  0:00:17s
epoch 60 | loss: 0.57976 | val_0_rmse: 0.7758  | val_1_rmse: 0.78828 |  0:00:18s
epoch 61 | loss: 0.56173 | val_0_rmse: 0.78196 | val_1_rmse: 0.79418 |  0:00:18s
epoch 62 | loss: 0.57528 | val_0_rmse: 0.77464 | val_1_rmse: 0.78641 |  0:00:18s
epoch 63 | loss: 0.56162 | val_0_rmse: 0.77684 | val_1_rmse: 0.79534 |  0:00:19s
epoch 64 | loss: 0.55608 | val_0_rmse: 0.78232 | val_1_rmse: 0.80108 |  0:00:19s
epoch 65 | loss: 0.56137 | val_0_rmse: 0.76803 | val_1_rmse: 0.78556 |  0:00:19s
epoch 66 | loss: 0.56149 | val_0_rmse: 0.77188 | val_1_rmse: 0.79198 |  0:00:19s
epoch 67 | loss: 0.57136 | val_0_rmse: 0.7759  | val_1_rmse: 0.80258 |  0:00:20s
epoch 68 | loss: 0.56576 | val_0_rmse: 0.77581 | val_1_rmse: 0.7951  |  0:00:20s
epoch 69 | loss: 0.55644 | val_0_rmse: 0.77536 | val_1_rmse: 0.79922 |  0:00:20s
epoch 70 | loss: 0.56136 | val_0_rmse: 0.76931 | val_1_rmse: 0.79386 |  0:00:21s
epoch 71 | loss: 0.54699 | val_0_rmse: 0.7835  | val_1_rmse: 0.81004 |  0:00:21s
epoch 72 | loss: 0.55854 | val_0_rmse: 0.76419 | val_1_rmse: 0.79327 |  0:00:21s
epoch 73 | loss: 0.54034 | val_0_rmse: 0.7639  | val_1_rmse: 0.79581 |  0:00:21s
epoch 74 | loss: 0.54126 | val_0_rmse: 0.75313 | val_1_rmse: 0.79332 |  0:00:22s
epoch 75 | loss: 0.53394 | val_0_rmse: 0.75652 | val_1_rmse: 0.79151 |  0:00:22s
epoch 76 | loss: 0.5399  | val_0_rmse: 0.75393 | val_1_rmse: 0.79491 |  0:00:22s
epoch 77 | loss: 0.53791 | val_0_rmse: 0.74941 | val_1_rmse: 0.79044 |  0:00:23s
epoch 78 | loss: 0.53516 | val_0_rmse: 0.74857 | val_1_rmse: 0.77969 |  0:00:23s
epoch 79 | loss: 0.52916 | val_0_rmse: 0.75107 | val_1_rmse: 0.78987 |  0:00:23s
epoch 80 | loss: 0.52668 | val_0_rmse: 0.75066 | val_1_rmse: 0.78727 |  0:00:24s
epoch 81 | loss: 0.53429 | val_0_rmse: 0.74778 | val_1_rmse: 0.77194 |  0:00:24s
epoch 82 | loss: 0.53352 | val_0_rmse: 0.73922 | val_1_rmse: 0.7861  |  0:00:24s
epoch 83 | loss: 0.53133 | val_0_rmse: 0.74985 | val_1_rmse: 0.8067  |  0:00:24s
epoch 84 | loss: 0.5258  | val_0_rmse: 0.7475  | val_1_rmse: 0.79679 |  0:00:25s
epoch 85 | loss: 0.51836 | val_0_rmse: 0.73789 | val_1_rmse: 0.79692 |  0:00:25s
epoch 86 | loss: 0.51975 | val_0_rmse: 0.73866 | val_1_rmse: 0.80425 |  0:00:25s
epoch 87 | loss: 0.51054 | val_0_rmse: 0.73215 | val_1_rmse: 0.78433 |  0:00:26s
epoch 88 | loss: 0.51273 | val_0_rmse: 0.72436 | val_1_rmse: 0.77748 |  0:00:26s
epoch 89 | loss: 0.51899 | val_0_rmse: 0.73714 | val_1_rmse: 0.78817 |  0:00:26s
epoch 90 | loss: 0.52581 | val_0_rmse: 0.73221 | val_1_rmse: 0.77047 |  0:00:27s
epoch 91 | loss: 0.52767 | val_0_rmse: 0.72744 | val_1_rmse: 0.7773  |  0:00:27s
epoch 92 | loss: 0.52502 | val_0_rmse: 0.73398 | val_1_rmse: 0.76773 |  0:00:27s
epoch 93 | loss: 0.52598 | val_0_rmse: 0.73354 | val_1_rmse: 0.76853 |  0:00:28s
epoch 94 | loss: 0.52493 | val_0_rmse: 0.73057 | val_1_rmse: 0.77267 |  0:00:28s
epoch 95 | loss: 0.52132 | val_0_rmse: 0.72665 | val_1_rmse: 0.78951 |  0:00:28s
epoch 96 | loss: 0.51921 | val_0_rmse: 0.72247 | val_1_rmse: 0.79414 |  0:00:28s
epoch 97 | loss: 0.50152 | val_0_rmse: 0.71686 | val_1_rmse: 0.79085 |  0:00:29s
epoch 98 | loss: 0.50165 | val_0_rmse: 0.71129 | val_1_rmse: 0.78836 |  0:00:29s
epoch 99 | loss: 0.51384 | val_0_rmse: 0.7118  | val_1_rmse: 0.7749  |  0:00:29s
epoch 100| loss: 0.50269 | val_0_rmse: 0.71395 | val_1_rmse: 0.78333 |  0:00:30s
epoch 101| loss: 0.50766 | val_0_rmse: 0.72568 | val_1_rmse: 0.80261 |  0:00:30s
epoch 102| loss: 0.50722 | val_0_rmse: 0.71023 | val_1_rmse: 0.79151 |  0:00:30s
epoch 103| loss: 0.50426 | val_0_rmse: 0.71856 | val_1_rmse: 0.79206 |  0:00:30s
epoch 104| loss: 0.50163 | val_0_rmse: 0.70114 | val_1_rmse: 0.7827  |  0:00:31s
epoch 105| loss: 0.51209 | val_0_rmse: 0.72379 | val_1_rmse: 0.82685 |  0:00:31s
epoch 106| loss: 0.50836 | val_0_rmse: 0.715   | val_1_rmse: 0.81388 |  0:00:31s
epoch 107| loss: 0.5088  | val_0_rmse: 0.69768 | val_1_rmse: 0.77938 |  0:00:32s
epoch 108| loss: 0.49857 | val_0_rmse: 0.69785 | val_1_rmse: 0.80696 |  0:00:32s
epoch 109| loss: 0.49112 | val_0_rmse: 0.69854 | val_1_rmse: 0.78085 |  0:00:32s
epoch 110| loss: 0.49503 | val_0_rmse: 0.70313 | val_1_rmse: 0.77239 |  0:00:33s
epoch 111| loss: 0.47797 | val_0_rmse: 0.68539 | val_1_rmse: 0.78802 |  0:00:33s
epoch 112| loss: 0.4917  | val_0_rmse: 0.68679 | val_1_rmse: 0.7834  |  0:00:33s
epoch 113| loss: 0.48096 | val_0_rmse: 0.7133  | val_1_rmse: 0.80626 |  0:00:33s
epoch 114| loss: 0.49351 | val_0_rmse: 0.68588 | val_1_rmse: 0.77872 |  0:00:34s
epoch 115| loss: 0.48308 | val_0_rmse: 0.68204 | val_1_rmse: 0.79528 |  0:00:34s
epoch 116| loss: 0.47588 | val_0_rmse: 0.69682 | val_1_rmse: 0.79292 |  0:00:34s
epoch 117| loss: 0.47031 | val_0_rmse: 0.6767  | val_1_rmse: 0.77194 |  0:00:35s
epoch 118| loss: 0.471   | val_0_rmse: 0.67238 | val_1_rmse: 0.76248 |  0:00:35s
epoch 119| loss: 0.47275 | val_0_rmse: 0.67838 | val_1_rmse: 0.75406 |  0:00:35s
epoch 120| loss: 0.47863 | val_0_rmse: 0.69134 | val_1_rmse: 0.77512 |  0:00:35s
epoch 121| loss: 0.48125 | val_0_rmse: 0.69987 | val_1_rmse: 0.79335 |  0:00:36s
epoch 122| loss: 0.4831  | val_0_rmse: 0.68609 | val_1_rmse: 0.78226 |  0:00:36s
epoch 123| loss: 0.4904  | val_0_rmse: 0.68335 | val_1_rmse: 0.77727 |  0:00:36s
epoch 124| loss: 0.47764 | val_0_rmse: 0.68423 | val_1_rmse: 0.7807  |  0:00:37s
epoch 125| loss: 0.48275 | val_0_rmse: 0.67167 | val_1_rmse: 0.77569 |  0:00:37s
epoch 126| loss: 0.47922 | val_0_rmse: 0.66768 | val_1_rmse: 0.76112 |  0:00:37s
epoch 127| loss: 0.46603 | val_0_rmse: 0.67191 | val_1_rmse: 0.77283 |  0:00:38s
epoch 128| loss: 0.47121 | val_0_rmse: 0.66544 | val_1_rmse: 0.77185 |  0:00:38s
epoch 129| loss: 0.4902  | val_0_rmse: 0.67673 | val_1_rmse: 0.77081 |  0:00:38s
epoch 130| loss: 0.4729  | val_0_rmse: 0.68208 | val_1_rmse: 0.78322 |  0:00:38s
epoch 131| loss: 0.4731  | val_0_rmse: 0.67093 | val_1_rmse: 0.77762 |  0:00:39s
epoch 132| loss: 0.46637 | val_0_rmse: 0.66541 | val_1_rmse: 0.77109 |  0:00:39s
epoch 133| loss: 0.45818 | val_0_rmse: 0.67586 | val_1_rmse: 0.78142 |  0:00:39s
epoch 134| loss: 0.46677 | val_0_rmse: 0.65766 | val_1_rmse: 0.77913 |  0:00:40s
epoch 135| loss: 0.4477  | val_0_rmse: 0.66228 | val_1_rmse: 0.76496 |  0:00:40s
epoch 136| loss: 0.45614 | val_0_rmse: 0.65901 | val_1_rmse: 0.77148 |  0:00:40s
epoch 137| loss: 0.45059 | val_0_rmse: 0.65218 | val_1_rmse: 0.77661 |  0:00:40s
epoch 138| loss: 0.44224 | val_0_rmse: 0.65766 | val_1_rmse: 0.76506 |  0:00:41s
epoch 139| loss: 0.45058 | val_0_rmse: 0.65926 | val_1_rmse: 0.77656 |  0:00:41s
epoch 140| loss: 0.44287 | val_0_rmse: 0.65492 | val_1_rmse: 0.78874 |  0:00:41s
epoch 141| loss: 0.4481  | val_0_rmse: 0.65507 | val_1_rmse: 0.79929 |  0:00:42s
epoch 142| loss: 0.44963 | val_0_rmse: 0.65121 | val_1_rmse: 0.77013 |  0:00:42s
epoch 143| loss: 0.44198 | val_0_rmse: 0.64788 | val_1_rmse: 0.77864 |  0:00:42s
epoch 144| loss: 0.44862 | val_0_rmse: 0.64687 | val_1_rmse: 0.77383 |  0:00:43s
epoch 145| loss: 0.4448  | val_0_rmse: 0.64691 | val_1_rmse: 0.76443 |  0:00:43s
epoch 146| loss: 0.43823 | val_0_rmse: 0.64929 | val_1_rmse: 0.77695 |  0:00:43s
epoch 147| loss: 0.43209 | val_0_rmse: 0.65675 | val_1_rmse: 0.79953 |  0:00:43s
epoch 148| loss: 0.44355 | val_0_rmse: 0.64559 | val_1_rmse: 0.80864 |  0:00:44s
epoch 149| loss: 0.43581 | val_0_rmse: 0.63288 | val_1_rmse: 0.8176  |  0:00:44s

Early stopping occured at epoch 149 with best_epoch = 119 and best_val_1_rmse = 0.75406
Best weights from best epoch are automatically used!
ended training at: 05:16:57
Feature importance:
Mean squared error is of 0.0610558961748111
Mean absolute error:0.1728599520489453
MAPE:0.18625625217831643
R2 score:0.2853415267226007
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:21:31
epoch 0  | loss: 1.03828 | val_0_rmse: 0.91973 | val_1_rmse: 0.91805 |  0:00:22s
epoch 1  | loss: 0.72352 | val_0_rmse: 0.83796 | val_1_rmse: 0.83654 |  0:00:44s
epoch 2  | loss: 0.62456 | val_0_rmse: 0.79966 | val_1_rmse: 0.7981  |  0:01:06s
epoch 3  | loss: 0.5864  | val_0_rmse: 0.79823 | val_1_rmse: 0.79987 |  0:01:28s
epoch 4  | loss: 0.56516 | val_0_rmse: 0.74822 | val_1_rmse: 0.75641 |  0:01:50s
epoch 5  | loss: 0.54671 | val_0_rmse: 0.71989 | val_1_rmse: 0.73718 |  0:02:12s
epoch 6  | loss: 0.52857 | val_0_rmse: 0.72412 | val_1_rmse: 0.74754 |  0:02:34s
epoch 7  | loss: 0.51902 | val_0_rmse: 0.70016 | val_1_rmse: 0.73407 |  0:02:56s
epoch 8  | loss: 0.50601 | val_0_rmse: 0.74679 | val_1_rmse: 0.77313 |  0:03:18s
epoch 9  | loss: 0.49777 | val_0_rmse: 0.70702 | val_1_rmse: 0.74864 |  0:03:42s
epoch 10 | loss: 0.48632 | val_0_rmse: 0.7043  | val_1_rmse: 0.74825 |  0:04:04s
epoch 11 | loss: 0.48161 | val_0_rmse: 1.02869 | val_1_rmse: 0.86011 |  0:04:26s
epoch 12 | loss: 0.47396 | val_0_rmse: 1.75618 | val_1_rmse: 0.77488 |  0:04:48s
epoch 13 | loss: 0.47232 | val_0_rmse: 0.8342  | val_1_rmse: 0.74853 |  0:05:10s
epoch 14 | loss: 0.46642 | val_0_rmse: 0.85179 | val_1_rmse: 0.76249 |  0:05:32s
epoch 15 | loss: 0.45942 | val_0_rmse: 0.68737 | val_1_rmse: 0.75203 |  0:05:54s
epoch 16 | loss: 0.45221 | val_0_rmse: 1.0079  | val_1_rmse: 0.75963 |  0:06:16s
epoch 17 | loss: 0.44895 | val_0_rmse: 0.68756 | val_1_rmse: 0.74402 |  0:06:38s
epoch 18 | loss: 0.44513 | val_0_rmse: 1.43177 | val_1_rmse: 0.74469 |  0:07:00s
epoch 19 | loss: 0.44031 | val_0_rmse: 0.68292 | val_1_rmse: 0.76386 |  0:07:22s
epoch 20 | loss: 0.43638 | val_0_rmse: 0.68062 | val_1_rmse: 0.80681 |  0:07:44s
epoch 21 | loss: 0.43135 | val_0_rmse: 0.68007 | val_1_rmse: 0.84156 |  0:08:06s
epoch 22 | loss: 0.42961 | val_0_rmse: 0.68228 | val_1_rmse: 0.91432 |  0:08:29s
epoch 23 | loss: 0.42787 | val_0_rmse: 0.68037 | val_1_rmse: 0.8772  |  0:08:51s
epoch 24 | loss: 0.4266  | val_0_rmse: 0.68067 | val_1_rmse: 0.93772 |  0:09:13s
epoch 25 | loss: 0.42044 | val_0_rmse: 0.70725 | val_1_rmse: 1.03178 |  0:09:35s
epoch 26 | loss: 0.4487  | val_0_rmse: 0.70081 | val_1_rmse: 0.80202 |  0:09:56s
epoch 27 | loss: 0.42675 | val_0_rmse: 0.68013 | val_1_rmse: 0.75163 |  0:10:18s
epoch 28 | loss: 0.41952 | val_0_rmse: 0.66446 | val_1_rmse: 0.75582 |  0:10:40s
epoch 29 | loss: 0.41186 | val_0_rmse: 0.66805 | val_1_rmse: 0.75864 |  0:11:02s
epoch 30 | loss: 0.4089  | val_0_rmse: 0.66058 | val_1_rmse: 0.75721 |  0:11:23s
epoch 31 | loss: 0.40461 | val_0_rmse: 0.65546 | val_1_rmse: 0.7402  |  0:11:45s
epoch 32 | loss: 0.40278 | val_0_rmse: 0.65204 | val_1_rmse: 0.73865 |  0:12:07s
epoch 33 | loss: 0.40472 | val_0_rmse: 0.84504 | val_1_rmse: 0.8689  |  0:12:29s
epoch 34 | loss: 0.40058 | val_0_rmse: 0.65997 | val_1_rmse: 0.74588 |  0:12:50s
epoch 35 | loss: 0.39785 | val_0_rmse: 0.65315 | val_1_rmse: 0.75535 |  0:13:12s
epoch 36 | loss: 0.39636 | val_0_rmse: 0.65088 | val_1_rmse: 0.7484  |  0:13:34s
epoch 37 | loss: 0.39397 | val_0_rmse: 0.64881 | val_1_rmse: 0.75696 |  0:13:56s

Early stopping occured at epoch 37 with best_epoch = 7 and best_val_1_rmse = 0.73407
Best weights from best epoch are automatically used!
ended training at: 05:35:39
Feature importance:
Mean squared error is of 0.06569101536357728
Mean absolute error:0.17854003459097337
MAPE:0.1985321536033641
R2 score:0.45437384344430554
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:35:54
epoch 0  | loss: 0.98768 | val_0_rmse: 0.86941 | val_1_rmse: 0.87797 |  0:00:21s
epoch 1  | loss: 0.71734 | val_0_rmse: 0.82002 | val_1_rmse: 0.82891 |  0:00:43s
epoch 2  | loss: 0.63607 | val_0_rmse: 0.79958 | val_1_rmse: 0.81471 |  0:01:05s
epoch 3  | loss: 0.59955 | val_0_rmse: 0.76585 | val_1_rmse: 0.77822 |  0:01:26s
epoch 4  | loss: 0.57034 | val_0_rmse: 0.76596 | val_1_rmse: 0.7869  |  0:01:49s
epoch 5  | loss: 0.55544 | val_0_rmse: 0.73796 | val_1_rmse: 0.76075 |  0:02:11s
epoch 6  | loss: 0.5377  | val_0_rmse: 0.70507 | val_1_rmse: 0.73172 |  0:02:32s
epoch 7  | loss: 0.52498 | val_0_rmse: 0.70459 | val_1_rmse: 0.73837 |  0:02:54s
epoch 8  | loss: 0.51366 | val_0_rmse: 0.71134 | val_1_rmse: 0.74764 |  0:03:16s
epoch 9  | loss: 0.50389 | val_0_rmse: 0.70294 | val_1_rmse: 0.75211 |  0:03:38s
epoch 10 | loss: 0.49657 | val_0_rmse: 0.71137 | val_1_rmse: 0.7481  |  0:04:00s
epoch 11 | loss: 0.4991  | val_0_rmse: 0.7282  | val_1_rmse: 0.76768 |  0:04:21s
epoch 12 | loss: 0.48577 | val_0_rmse: 0.68909 | val_1_rmse: 0.74537 |  0:04:43s
epoch 13 | loss: 0.4835  | val_0_rmse: 0.75386 | val_1_rmse: 0.86148 |  0:05:05s
epoch 14 | loss: 0.47387 | val_0_rmse: 0.68055 | val_1_rmse: 0.74218 |  0:05:26s
epoch 15 | loss: 0.47177 | val_0_rmse: 0.68275 | val_1_rmse: 0.74826 |  0:05:48s
epoch 16 | loss: 0.46528 | val_0_rmse: 0.67776 | val_1_rmse: 0.74277 |  0:06:10s
epoch 17 | loss: 0.46297 | val_0_rmse: 0.71274 | val_1_rmse: 0.7785  |  0:06:32s
epoch 18 | loss: 0.4588  | val_0_rmse: 0.68014 | val_1_rmse: 0.749   |  0:06:54s
epoch 19 | loss: 0.44987 | val_0_rmse: 0.68365 | val_1_rmse: 0.75199 |  0:07:15s
epoch 20 | loss: 0.4472  | val_0_rmse: 0.67184 | val_1_rmse: 0.74916 |  0:07:38s
epoch 21 | loss: 0.44209 | val_0_rmse: 0.67793 | val_1_rmse: 0.75779 |  0:08:02s
epoch 22 | loss: 0.4388  | val_0_rmse: 0.6663  | val_1_rmse: 0.73903 |  0:08:26s
epoch 23 | loss: 0.4319  | val_0_rmse: 0.73922 | val_1_rmse: 0.94598 |  0:08:51s
epoch 24 | loss: 0.42818 | val_0_rmse: 0.66598 | val_1_rmse: 0.75047 |  0:09:17s
epoch 25 | loss: 0.42686 | val_0_rmse: 0.69322 | val_1_rmse: 0.7636  |  0:09:41s
epoch 26 | loss: 0.42261 | val_0_rmse: 0.65875 | val_1_rmse: 0.74166 |  0:10:05s
epoch 27 | loss: 0.42121 | val_0_rmse: 0.70642 | val_1_rmse: 0.79119 |  0:10:29s
epoch 28 | loss: 0.485   | val_0_rmse: 0.70581 | val_1_rmse: 0.76285 |  0:10:53s
epoch 29 | loss: 0.44496 | val_0_rmse: 0.68493 | val_1_rmse: 0.74232 |  0:11:17s
epoch 30 | loss: 0.42641 | val_0_rmse: 0.66659 | val_1_rmse: 0.7619  |  0:11:41s
epoch 31 | loss: 0.41758 | val_0_rmse: 0.66957 | val_1_rmse: 0.76943 |  0:12:03s
epoch 32 | loss: 0.40969 | val_0_rmse: 0.68079 | val_1_rmse: 0.78805 |  0:12:26s
epoch 33 | loss: 0.40912 | val_0_rmse: 0.64822 | val_1_rmse: 0.73762 |  0:12:48s
epoch 34 | loss: 0.40549 | val_0_rmse: 0.64921 | val_1_rmse: 0.74251 |  0:13:10s
epoch 35 | loss: 0.40342 | val_0_rmse: 0.65462 | val_1_rmse: 0.75613 |  0:13:32s
epoch 36 | loss: 0.4025  | val_0_rmse: 0.64866 | val_1_rmse: 0.7561  |  0:13:53s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.73172
Best weights from best epoch are automatically used!
ended training at: 05:50:00
Feature importance:
Mean squared error is of 0.06519793413460542
Mean absolute error:0.17848924470073108
MAPE:0.19594892480067985
R2 score:0.4549950576111096
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:50:15
epoch 0  | loss: 1.0583  | val_0_rmse: 0.95833 | val_1_rmse: 0.95001 |  0:00:22s
epoch 1  | loss: 0.78748 | val_0_rmse: 0.86282 | val_1_rmse: 0.85556 |  0:00:44s
epoch 2  | loss: 0.63357 | val_0_rmse: 0.90561 | val_1_rmse: 0.89898 |  0:01:06s
epoch 3  | loss: 0.59276 | val_0_rmse: 0.821   | val_1_rmse: 0.81996 |  0:01:27s
epoch 4  | loss: 0.57058 | val_0_rmse: 0.75294 | val_1_rmse: 0.75568 |  0:01:49s
epoch 5  | loss: 0.54937 | val_0_rmse: 0.72626 | val_1_rmse: 0.7363  |  0:02:11s
epoch 6  | loss: 0.53818 | val_0_rmse: 0.71096 | val_1_rmse: 0.72676 |  0:02:33s
epoch 7  | loss: 0.52051 | val_0_rmse: 0.70832 | val_1_rmse: 0.73212 |  0:02:54s
epoch 8  | loss: 0.50806 | val_0_rmse: 0.71974 | val_1_rmse: 0.75691 |  0:03:16s
epoch 9  | loss: 0.50228 | val_0_rmse: 0.70411 | val_1_rmse: 0.74318 |  0:03:38s
epoch 10 | loss: 0.49349 | val_0_rmse: 0.69437 | val_1_rmse: 0.73731 |  0:04:00s
epoch 11 | loss: 0.48245 | val_0_rmse: 0.72559 | val_1_rmse: 0.78257 |  0:04:21s
epoch 12 | loss: 0.48154 | val_0_rmse: 0.75377 | val_1_rmse: 0.80024 |  0:04:43s
epoch 13 | loss: 0.47259 | val_0_rmse: 0.696   | val_1_rmse: 0.75747 |  0:05:05s
epoch 14 | loss: 0.46843 | val_0_rmse: 0.72154 | val_1_rmse: 0.79498 |  0:05:27s
epoch 15 | loss: 0.46238 | val_0_rmse: 0.69175 | val_1_rmse: 0.76938 |  0:05:49s
epoch 16 | loss: 0.454   | val_0_rmse: 0.69777 | val_1_rmse: 0.77087 |  0:06:11s
epoch 17 | loss: 0.44688 | val_0_rmse: 0.68273 | val_1_rmse: 0.7566  |  0:06:32s
epoch 18 | loss: 0.44652 | val_0_rmse: 0.68595 | val_1_rmse: 0.76422 |  0:06:54s
epoch 19 | loss: 0.43806 | val_0_rmse: 0.68075 | val_1_rmse: 0.75115 |  0:07:16s
epoch 20 | loss: 0.43544 | val_0_rmse: 0.6768  | val_1_rmse: 0.74891 |  0:07:38s
epoch 21 | loss: 0.43313 | val_0_rmse: 0.67609 | val_1_rmse: 0.74527 |  0:08:00s
epoch 22 | loss: 0.43362 | val_0_rmse: 0.66556 | val_1_rmse: 0.73837 |  0:08:22s
epoch 23 | loss: 0.42881 | val_0_rmse: 0.73028 | val_1_rmse: 0.82214 |  0:08:44s
epoch 24 | loss: 0.42359 | val_0_rmse: 0.70222 | val_1_rmse: 0.79963 |  0:09:06s
epoch 25 | loss: 0.41737 | val_0_rmse: 0.66327 | val_1_rmse: 0.73327 |  0:09:28s
epoch 26 | loss: 0.41922 | val_0_rmse: 0.67138 | val_1_rmse: 0.75575 |  0:09:50s
epoch 27 | loss: 0.41494 | val_0_rmse: 0.68077 | val_1_rmse: 0.77783 |  0:10:11s
epoch 28 | loss: 0.41361 | val_0_rmse: 0.66374 | val_1_rmse: 0.74406 |  0:10:33s
epoch 29 | loss: 0.41132 | val_0_rmse: 0.68237 | val_1_rmse: 0.79856 |  0:10:55s
epoch 30 | loss: 0.40934 | val_0_rmse: 0.78519 | val_1_rmse: 0.84266 |  0:11:17s
epoch 31 | loss: 0.40707 | val_0_rmse: 0.6648  | val_1_rmse: 0.77584 |  0:11:39s
epoch 32 | loss: 0.40345 | val_0_rmse: 0.81779 | val_1_rmse: 0.87225 |  0:12:00s
epoch 33 | loss: 0.39839 | val_0_rmse: 0.64902 | val_1_rmse: 0.74379 |  0:12:22s
epoch 34 | loss: 0.40039 | val_0_rmse: 0.81094 | val_1_rmse: 0.85811 |  0:12:44s
epoch 35 | loss: 0.39672 | val_0_rmse: 0.65209 | val_1_rmse: 0.73946 |  0:13:06s
epoch 36 | loss: 0.39401 | val_0_rmse: 0.6632  | val_1_rmse: 0.75678 |  0:13:28s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.72676
Best weights from best epoch are automatically used!
ended training at: 06:03:55
Feature importance:
Mean squared error is of 0.06608575079356632
Mean absolute error:0.1791928832822919
MAPE:0.19685674305279383
R2 score:0.44681429930595395
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:04:07
epoch 0  | loss: 0.98017 | val_0_rmse: 0.91911 | val_1_rmse: 0.9032  |  0:00:22s
epoch 1  | loss: 0.72945 | val_0_rmse: 0.84305 | val_1_rmse: 0.83443 |  0:00:43s
epoch 2  | loss: 0.65238 | val_0_rmse: 0.82011 | val_1_rmse: 0.8131  |  0:01:05s
epoch 3  | loss: 0.61658 | val_0_rmse: 0.7828  | val_1_rmse: 0.77641 |  0:01:27s
epoch 4  | loss: 0.60502 | val_0_rmse: 0.76473 | val_1_rmse: 0.76466 |  0:01:49s
epoch 5  | loss: 0.57982 | val_0_rmse: 0.75004 | val_1_rmse: 0.75647 |  0:02:10s
epoch 6  | loss: 0.55691 | val_0_rmse: 0.73517 | val_1_rmse: 0.74438 |  0:02:32s
epoch 7  | loss: 0.54699 | val_0_rmse: 0.7185  | val_1_rmse: 0.73965 |  0:02:54s
epoch 8  | loss: 0.53586 | val_0_rmse: 0.71767 | val_1_rmse: 0.74043 |  0:03:16s
epoch 9  | loss: 0.52931 | val_0_rmse: 0.72275 | val_1_rmse: 0.75185 |  0:03:37s
epoch 10 | loss: 0.52159 | val_0_rmse: 0.86336 | val_1_rmse: 0.7751  |  0:03:59s
epoch 11 | loss: 0.53025 | val_0_rmse: 0.86757 | val_1_rmse: 0.87438 |  0:04:21s
epoch 12 | loss: 0.52286 | val_0_rmse: 0.73161 | val_1_rmse: 0.74453 |  0:04:43s
epoch 13 | loss: 0.50253 | val_0_rmse: 0.71363 | val_1_rmse: 1.41317 |  0:05:05s
epoch 14 | loss: 0.49318 | val_0_rmse: 0.71073 | val_1_rmse: 0.73796 |  0:05:27s
epoch 15 | loss: 0.48808 | val_0_rmse: 0.72436 | val_1_rmse: 0.75911 |  0:05:48s
epoch 16 | loss: 0.48024 | val_0_rmse: 0.70312 | val_1_rmse: 0.7477  |  0:06:10s
epoch 17 | loss: 0.47438 | val_0_rmse: 0.6958  | val_1_rmse: 0.74557 |  0:06:32s
epoch 18 | loss: 0.47042 | val_0_rmse: 0.68785 | val_1_rmse: 0.73548 |  0:06:53s
epoch 19 | loss: 0.46534 | val_0_rmse: 0.6875  | val_1_rmse: 0.73477 |  0:07:15s
epoch 20 | loss: 0.45927 | val_0_rmse: 0.68386 | val_1_rmse: 0.73857 |  0:07:37s
epoch 21 | loss: 0.45499 | val_0_rmse: 0.9026  | val_1_rmse: 0.93461 |  0:07:58s
epoch 22 | loss: 0.45484 | val_0_rmse: 0.80698 | val_1_rmse: 0.95949 |  0:08:20s
epoch 23 | loss: 0.45302 | val_0_rmse: 0.93373 | val_1_rmse: 0.90705 |  0:08:42s
epoch 24 | loss: 0.45165 | val_0_rmse: 0.6946  | val_1_rmse: 0.74362 |  0:09:04s
epoch 25 | loss: 0.44442 | val_0_rmse: 0.70265 | val_1_rmse: 0.76426 |  0:09:26s
epoch 26 | loss: 0.44048 | val_0_rmse: 0.77051 | val_1_rmse: 0.80467 |  0:09:48s
epoch 27 | loss: 0.43877 | val_0_rmse: 0.67261 | val_1_rmse: 0.74458 |  0:10:10s
epoch 28 | loss: 0.4377  | val_0_rmse: 0.71733 | val_1_rmse: 0.76718 |  0:10:32s
epoch 29 | loss: 0.43493 | val_0_rmse: 0.67392 | val_1_rmse: 0.74295 |  0:10:54s
epoch 30 | loss: 0.43139 | val_0_rmse: 0.69308 | val_1_rmse: 0.77833 |  0:11:15s
epoch 31 | loss: 0.42595 | val_0_rmse: 0.66532 | val_1_rmse: 0.74723 |  0:11:37s
epoch 32 | loss: 0.42486 | val_0_rmse: 0.67397 | val_1_rmse: 0.74466 |  0:11:59s
epoch 33 | loss: 0.42347 | val_0_rmse: 0.81791 | val_1_rmse: 0.90619 |  0:12:20s
epoch 34 | loss: 0.42591 | val_0_rmse: 0.95008 | val_1_rmse: 0.89675 |  0:12:42s
epoch 35 | loss: 0.45658 | val_0_rmse: 2.38907 | val_1_rmse: 2.41459 |  0:13:04s
epoch 36 | loss: 0.43937 | val_0_rmse: 0.68674 | val_1_rmse: 0.76017 |  0:13:26s
epoch 37 | loss: 0.42334 | val_0_rmse: 0.68991 | val_1_rmse: 1.05245 |  0:13:47s
epoch 38 | loss: 0.41665 | val_0_rmse: 0.68113 | val_1_rmse: 0.83785 |  0:14:10s
epoch 39 | loss: 0.41879 | val_0_rmse: 0.69578 | val_1_rmse: 0.77521 |  0:14:32s
epoch 40 | loss: 0.41147 | val_0_rmse: 0.68176 | val_1_rmse: 0.74442 |  0:14:53s
epoch 41 | loss: 0.41292 | val_0_rmse: 0.6715  | val_1_rmse: 0.75887 |  0:15:15s
epoch 42 | loss: 0.42404 | val_0_rmse: 0.68094 | val_1_rmse: 0.76376 |  0:15:37s
epoch 43 | loss: 0.41974 | val_0_rmse: 0.66134 | val_1_rmse: 0.74491 |  0:15:58s
epoch 44 | loss: 0.40627 | val_0_rmse: 0.66762 | val_1_rmse: 0.75635 |  0:16:20s
epoch 45 | loss: 0.4018  | val_0_rmse: 0.68913 | val_1_rmse: 0.78078 |  0:16:42s
epoch 46 | loss: 0.40306 | val_0_rmse: 0.65865 | val_1_rmse: 0.74267 |  0:17:04s
epoch 47 | loss: 0.39652 | val_0_rmse: 0.65482 | val_1_rmse: 0.75793 |  0:17:26s
epoch 48 | loss: 0.39718 | val_0_rmse: 0.68883 | val_1_rmse: 0.75639 |  0:17:47s
epoch 49 | loss: 0.39486 | val_0_rmse: 0.67364 | val_1_rmse: 0.76242 |  0:18:09s

Early stopping occured at epoch 49 with best_epoch = 19 and best_val_1_rmse = 0.73477
Best weights from best epoch are automatically used!
ended training at: 06:22:28
Feature importance:
Mean squared error is of 0.07040728206705736
Mean absolute error:0.17840893868052565
MAPE:0.19348470459688208
R2 score:0.4122756942785277
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:22:40
epoch 0  | loss: 0.97975 | val_0_rmse: 0.90121 | val_1_rmse: 0.89685 |  0:00:21s
epoch 1  | loss: 0.70515 | val_0_rmse: 0.84418 | val_1_rmse: 0.84374 |  0:00:43s
epoch 2  | loss: 0.61409 | val_0_rmse: 0.79898 | val_1_rmse: 0.7973  |  0:01:05s
epoch 3  | loss: 0.5858  | val_0_rmse: 0.77596 | val_1_rmse: 0.77739 |  0:01:27s
epoch 4  | loss: 0.56496 | val_0_rmse: 0.74947 | val_1_rmse: 0.75643 |  0:01:48s
epoch 5  | loss: 0.54817 | val_0_rmse: 0.73474 | val_1_rmse: 0.74857 |  0:02:10s
epoch 6  | loss: 0.5239  | val_0_rmse: 0.7341  | val_1_rmse: 0.75236 |  0:02:32s
epoch 7  | loss: 0.51423 | val_0_rmse: 0.74062 | val_1_rmse: 0.76412 |  0:02:54s
epoch 8  | loss: 0.50525 | val_0_rmse: 0.70441 | val_1_rmse: 0.75005 |  0:03:16s
epoch 9  | loss: 0.49663 | val_0_rmse: 0.75935 | val_1_rmse: 0.80064 |  0:03:38s
epoch 10 | loss: 0.49171 | val_0_rmse: 0.69632 | val_1_rmse: 0.74209 |  0:03:59s
epoch 11 | loss: 0.48126 | val_0_rmse: 0.69453 | val_1_rmse: 0.74602 |  0:04:21s
epoch 12 | loss: 0.47285 | val_0_rmse: 0.70138 | val_1_rmse: 0.74355 |  0:04:43s
epoch 13 | loss: 0.46634 | val_0_rmse: 0.94378 | val_1_rmse: 0.96972 |  0:05:04s
epoch 14 | loss: 0.45861 | val_0_rmse: 0.70475 | val_1_rmse: 0.76814 |  0:05:26s
epoch 15 | loss: 0.45279 | val_0_rmse: 0.75641 | val_1_rmse: 0.75809 |  0:05:48s
epoch 16 | loss: 0.4525  | val_0_rmse: 0.85895 | val_1_rmse: 0.89692 |  0:06:10s
epoch 17 | loss: 0.44605 | val_0_rmse: 0.75162 | val_1_rmse: 0.78815 |  0:06:32s
epoch 18 | loss: 0.43942 | val_0_rmse: 0.88489 | val_1_rmse: 0.92225 |  0:06:54s
epoch 19 | loss: 0.43331 | val_0_rmse: 0.671   | val_1_rmse: 0.74604 |  0:07:16s
epoch 20 | loss: 0.4344  | val_0_rmse: 0.7147  | val_1_rmse: 0.76991 |  0:07:37s
epoch 21 | loss: 0.42902 | val_0_rmse: 0.86701 | val_1_rmse: 0.9014  |  0:07:59s
epoch 22 | loss: 0.42364 | val_0_rmse: 0.67374 | val_1_rmse: 0.75305 |  0:08:21s
epoch 23 | loss: 0.42161 | val_0_rmse: 0.66354 | val_1_rmse: 0.74617 |  0:08:42s
epoch 24 | loss: 0.41756 | val_0_rmse: 0.65686 | val_1_rmse: 0.73573 |  0:09:04s
epoch 25 | loss: 0.41515 | val_0_rmse: 0.72166 | val_1_rmse: 0.78666 |  0:09:26s
epoch 26 | loss: 0.41321 | val_0_rmse: 0.66459 | val_1_rmse: 0.75432 |  0:09:48s
epoch 27 | loss: 0.40666 | val_0_rmse: 0.65724 | val_1_rmse: 0.74016 |  0:10:09s
epoch 28 | loss: 0.40447 | val_0_rmse: 0.65493 | val_1_rmse: 0.74824 |  0:10:31s
epoch 29 | loss: 0.40104 | val_0_rmse: 0.65399 | val_1_rmse: 0.74535 |  0:10:53s
epoch 30 | loss: 0.40098 | val_0_rmse: 0.67172 | val_1_rmse: 0.75965 |  0:11:15s
epoch 31 | loss: 0.40304 | val_0_rmse: 0.75814 | val_1_rmse: 0.83484 |  0:11:37s
epoch 32 | loss: 0.3996  | val_0_rmse: 0.68762 | val_1_rmse: 0.76546 |  0:11:59s
epoch 33 | loss: 0.39589 | val_0_rmse: 0.65535 | val_1_rmse: 0.75501 |  0:12:21s
epoch 34 | loss: 0.39254 | val_0_rmse: 0.65399 | val_1_rmse: 0.76226 |  0:12:42s
epoch 35 | loss: 0.38982 | val_0_rmse: 0.66945 | val_1_rmse: 0.75208 |  0:13:04s
epoch 36 | loss: 0.38815 | val_0_rmse: 0.6691  | val_1_rmse: 0.75739 |  0:13:26s
epoch 37 | loss: 0.38678 | val_0_rmse: 0.64759 | val_1_rmse: 0.74707 |  0:13:48s
epoch 38 | loss: 0.38474 | val_0_rmse: 0.6685  | val_1_rmse: 0.78525 |  0:14:10s
epoch 39 | loss: 0.38206 | val_0_rmse: 0.64749 | val_1_rmse: 0.75348 |  0:14:32s
epoch 40 | loss: 0.38334 | val_0_rmse: 0.65377 | val_1_rmse: 0.76548 |  0:14:54s
epoch 41 | loss: 0.38128 | val_0_rmse: 0.66487 | val_1_rmse: 0.77463 |  0:15:16s
epoch 42 | loss: 0.37887 | val_0_rmse: 0.85035 | val_1_rmse: 0.93699 |  0:15:38s
epoch 43 | loss: 0.37805 | val_0_rmse: 0.67538 | val_1_rmse: 0.75172 |  0:16:00s
epoch 44 | loss: 0.37858 | val_0_rmse: 0.66171 | val_1_rmse: 0.76139 |  0:16:22s
epoch 45 | loss: 0.37403 | val_0_rmse: 0.63973 | val_1_rmse: 0.76052 |  0:16:44s
epoch 46 | loss: 0.3724  | val_0_rmse: 0.66808 | val_1_rmse: 0.75237 |  0:17:06s
epoch 47 | loss: 0.37127 | val_0_rmse: 0.64592 | val_1_rmse: 0.75858 |  0:17:27s
epoch 48 | loss: 0.37281 | val_0_rmse: 2.76726 | val_1_rmse: 0.80074 |  0:17:49s
epoch 49 | loss: 0.36858 | val_0_rmse: 0.67068 | val_1_rmse: 0.76349 |  0:18:12s
epoch 50 | loss: 0.3676  | val_0_rmse: 0.63992 | val_1_rmse: 0.77284 |  0:18:34s
epoch 51 | loss: 0.36707 | val_0_rmse: 0.64928 | val_1_rmse: 0.7738  |  0:18:57s
epoch 52 | loss: 0.36575 | val_0_rmse: 0.64731 | val_1_rmse: 0.76154 |  0:19:19s
epoch 53 | loss: 0.36405 | val_0_rmse: 0.646   | val_1_rmse: 0.77647 |  0:19:41s
epoch 54 | loss: 0.36027 | val_0_rmse: 0.6565  | val_1_rmse: 0.77954 |  0:20:04s

Early stopping occured at epoch 54 with best_epoch = 24 and best_val_1_rmse = 0.73573
Best weights from best epoch are automatically used!
ended training at: 06:42:56
Feature importance:
Mean squared error is of 0.07500049016761921
Mean absolute error:0.1762205987460896
MAPE:0.18690651557235097
R2 score:0.41394612326539293
------------------------------------------------------------------
