TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:06:27
epoch 0  | loss: 1.44127 | val_0_rmse: 0.9877  | val_1_rmse: 1.02428 |  0:00:05s
epoch 1  | loss: 0.95539 | val_0_rmse: 0.91074 | val_1_rmse: 0.94912 |  0:00:07s
epoch 2  | loss: 0.85205 | val_0_rmse: 0.8864  | val_1_rmse: 0.93291 |  0:00:09s
epoch 3  | loss: 0.80169 | val_0_rmse: 0.8993  | val_1_rmse: 0.93904 |  0:00:11s
epoch 4  | loss: 0.79715 | val_0_rmse: 0.9012  | val_1_rmse: 0.9434  |  0:00:13s
epoch 5  | loss: 0.79708 | val_0_rmse: 0.8794  | val_1_rmse: 0.92159 |  0:00:15s
epoch 6  | loss: 0.79062 | val_0_rmse: 0.86501 | val_1_rmse: 0.91028 |  0:00:16s
epoch 7  | loss: 0.78107 | val_0_rmse: 0.86447 | val_1_rmse: 0.91124 |  0:00:18s
epoch 8  | loss: 0.77591 | val_0_rmse: 0.8668  | val_1_rmse: 0.91168 |  0:00:20s
epoch 9  | loss: 0.76743 | val_0_rmse: 0.86495 | val_1_rmse: 0.90875 |  0:00:22s
epoch 10 | loss: 0.76005 | val_0_rmse: 0.86972 | val_1_rmse: 0.91796 |  0:00:24s
epoch 11 | loss: 0.75635 | val_0_rmse: 0.86089 | val_1_rmse: 0.9045  |  0:00:26s
epoch 12 | loss: 0.74645 | val_0_rmse: 0.86437 | val_1_rmse: 0.91219 |  0:00:28s
epoch 13 | loss: 0.74724 | val_0_rmse: 0.86703 | val_1_rmse: 0.91245 |  0:00:29s
epoch 14 | loss: 0.73947 | val_0_rmse: 0.86124 | val_1_rmse: 0.90428 |  0:00:31s
epoch 15 | loss: 0.73299 | val_0_rmse: 0.85224 | val_1_rmse: 0.89877 |  0:00:33s
epoch 16 | loss: 0.72553 | val_0_rmse: 0.85722 | val_1_rmse: 0.90071 |  0:00:35s
epoch 17 | loss: 0.72449 | val_0_rmse: 0.8497  | val_1_rmse: 0.89712 |  0:00:37s
epoch 18 | loss: 0.72964 | val_0_rmse: 0.85093 | val_1_rmse: 0.90475 |  0:00:39s
epoch 19 | loss: 0.72591 | val_0_rmse: 0.84716 | val_1_rmse: 0.89927 |  0:00:41s
epoch 20 | loss: 0.72611 | val_0_rmse: 0.84564 | val_1_rmse: 0.89483 |  0:00:43s
epoch 21 | loss: 0.71587 | val_0_rmse: 0.84517 | val_1_rmse: 0.8999  |  0:00:44s
epoch 22 | loss: 0.71307 | val_0_rmse: 0.84093 | val_1_rmse: 0.89528 |  0:00:46s
epoch 23 | loss: 0.71558 | val_0_rmse: 0.84662 | val_1_rmse: 0.90291 |  0:00:48s
epoch 24 | loss: 0.71223 | val_0_rmse: 0.84272 | val_1_rmse: 0.89444 |  0:00:50s
epoch 25 | loss: 0.71413 | val_0_rmse: 0.84002 | val_1_rmse: 0.8982  |  0:00:52s
epoch 26 | loss: 0.71809 | val_0_rmse: 0.83934 | val_1_rmse: 0.89105 |  0:00:54s
epoch 27 | loss: 0.70563 | val_0_rmse: 0.84714 | val_1_rmse: 0.90137 |  0:00:56s
epoch 28 | loss: 0.70503 | val_0_rmse: 0.8385  | val_1_rmse: 0.89267 |  0:00:57s
epoch 29 | loss: 0.70749 | val_0_rmse: 0.83611 | val_1_rmse: 0.88856 |  0:00:59s
epoch 30 | loss: 0.70204 | val_0_rmse: 0.83141 | val_1_rmse: 0.88839 |  0:01:01s
epoch 31 | loss: 0.70014 | val_0_rmse: 0.83607 | val_1_rmse: 0.89674 |  0:01:03s
epoch 32 | loss: 0.69831 | val_0_rmse: 0.83014 | val_1_rmse: 0.88288 |  0:01:05s
epoch 33 | loss: 0.70364 | val_0_rmse: 0.83183 | val_1_rmse: 0.88766 |  0:01:07s
epoch 34 | loss: 0.70695 | val_0_rmse: 0.83025 | val_1_rmse: 0.88922 |  0:01:09s
epoch 35 | loss: 0.69915 | val_0_rmse: 0.8295  | val_1_rmse: 0.89073 |  0:01:11s
epoch 36 | loss: 0.69816 | val_0_rmse: 0.82827 | val_1_rmse: 0.89353 |  0:01:12s
epoch 37 | loss: 0.69038 | val_0_rmse: 0.82376 | val_1_rmse: 0.8876  |  0:01:14s
epoch 38 | loss: 0.69042 | val_0_rmse: 0.82358 | val_1_rmse: 0.88293 |  0:01:16s
epoch 39 | loss: 0.68796 | val_0_rmse: 0.82346 | val_1_rmse: 0.88507 |  0:01:18s
epoch 40 | loss: 0.70399 | val_0_rmse: 0.83391 | val_1_rmse: 0.88703 |  0:01:20s
epoch 41 | loss: 0.72022 | val_0_rmse: 0.84582 | val_1_rmse: 0.90379 |  0:01:22s
epoch 42 | loss: 0.7132  | val_0_rmse: 0.83701 | val_1_rmse: 0.89208 |  0:01:24s
epoch 43 | loss: 0.71383 | val_0_rmse: 0.91478 | val_1_rmse: 0.94762 |  0:01:25s
epoch 44 | loss: 0.74102 | val_0_rmse: 0.88277 | val_1_rmse: 0.92783 |  0:01:27s
epoch 45 | loss: 0.72074 | val_0_rmse: 0.84397 | val_1_rmse: 0.90112 |  0:01:29s
epoch 46 | loss: 0.71766 | val_0_rmse: 0.84968 | val_1_rmse: 0.89648 |  0:01:31s
epoch 47 | loss: 0.72727 | val_0_rmse: 0.85752 | val_1_rmse: 0.89453 |  0:01:33s
epoch 48 | loss: 0.72314 | val_0_rmse: 0.84476 | val_1_rmse: 0.89237 |  0:01:35s
epoch 49 | loss: 0.71617 | val_0_rmse: 0.84256 | val_1_rmse: 0.89031 |  0:01:37s
epoch 50 | loss: 0.71339 | val_0_rmse: 0.84312 | val_1_rmse: 0.88755 |  0:01:39s
epoch 51 | loss: 0.71722 | val_0_rmse: 0.84294 | val_1_rmse: 0.89099 |  0:01:40s
epoch 52 | loss: 0.71665 | val_0_rmse: 0.84235 | val_1_rmse: 0.89391 |  0:01:42s
epoch 53 | loss: 0.71565 | val_0_rmse: 0.84176 | val_1_rmse: 0.89718 |  0:01:44s
epoch 54 | loss: 0.71865 | val_0_rmse: 0.84132 | val_1_rmse: 0.89235 |  0:01:46s
epoch 55 | loss: 0.71832 | val_0_rmse: 0.84662 | val_1_rmse: 0.90715 |  0:01:48s
epoch 56 | loss: 0.71497 | val_0_rmse: 0.83686 | val_1_rmse: 0.88888 |  0:01:50s
epoch 57 | loss: 0.70698 | val_0_rmse: 0.83386 | val_1_rmse: 0.88672 |  0:01:52s
epoch 58 | loss: 0.704   | val_0_rmse: 0.83525 | val_1_rmse: 0.89307 |  0:01:53s
epoch 59 | loss: 0.70149 | val_0_rmse: 0.83407 | val_1_rmse: 0.89304 |  0:01:55s
epoch 60 | loss: 0.69906 | val_0_rmse: 0.8318  | val_1_rmse: 0.88657 |  0:01:57s
epoch 61 | loss: 0.69684 | val_0_rmse: 0.82775 | val_1_rmse: 0.88889 |  0:01:59s
epoch 62 | loss: 0.69122 | val_0_rmse: 0.82613 | val_1_rmse: 0.88508 |  0:02:01s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.88288
Best weights from best epoch are automatically used!
ended training at: 13:08:30
Feature importance:
Mean squared error is of 0.055371701884602983
Mean absolute error:0.15552390091440305
MAPE:0.16969989246151723
R2 score:0.25163463021100363
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:08:31
epoch 0  | loss: 1.42897 | val_0_rmse: 1.00646 | val_1_rmse: 1.01385 |  0:00:01s
epoch 1  | loss: 1.02079 | val_0_rmse: 1.00639 | val_1_rmse: 1.01384 |  0:00:03s
epoch 2  | loss: 1.01653 | val_0_rmse: 1.00625 | val_1_rmse: 1.01365 |  0:00:05s
epoch 3  | loss: 1.01548 | val_0_rmse: 1.00636 | val_1_rmse: 1.01375 |  0:00:07s
epoch 4  | loss: 1.01567 | val_0_rmse: 1.00634 | val_1_rmse: 1.01366 |  0:00:09s
epoch 5  | loss: 1.01509 | val_0_rmse: 1.00628 | val_1_rmse: 1.01383 |  0:00:11s
epoch 6  | loss: 1.01481 | val_0_rmse: 1.0063  | val_1_rmse: 1.01378 |  0:00:13s
epoch 7  | loss: 1.01395 | val_0_rmse: 1.00629 | val_1_rmse: 1.01374 |  0:00:14s
epoch 8  | loss: 1.0144  | val_0_rmse: 1.0068  | val_1_rmse: 1.01425 |  0:00:16s
epoch 9  | loss: 1.01339 | val_0_rmse: 1.0062  | val_1_rmse: 1.01361 |  0:00:18s
epoch 10 | loss: 1.01277 | val_0_rmse: 1.00542 | val_1_rmse: 1.01301 |  0:00:20s
epoch 11 | loss: 1.01024 | val_0_rmse: 0.9978  | val_1_rmse: 1.006   |  0:00:22s
epoch 12 | loss: 0.99144 | val_0_rmse: 0.9472  | val_1_rmse: 0.95629 |  0:00:24s
epoch 13 | loss: 0.92204 | val_0_rmse: 0.9605  | val_1_rmse: 0.9678  |  0:00:26s
epoch 14 | loss: 0.87846 | val_0_rmse: 0.92629 | val_1_rmse: 0.93555 |  0:00:28s
epoch 15 | loss: 0.83357 | val_0_rmse: 0.88876 | val_1_rmse: 0.89254 |  0:00:29s
epoch 16 | loss: 0.8046  | val_0_rmse: 0.88464 | val_1_rmse: 0.88944 |  0:00:31s
epoch 17 | loss: 0.78357 | val_0_rmse: 0.87883 | val_1_rmse: 0.88629 |  0:00:33s
epoch 18 | loss: 0.77004 | val_0_rmse: 0.87179 | val_1_rmse: 0.88047 |  0:00:35s
epoch 19 | loss: 0.75031 | val_0_rmse: 0.87313 | val_1_rmse: 0.88709 |  0:00:37s
epoch 20 | loss: 0.74021 | val_0_rmse: 0.86894 | val_1_rmse: 0.886   |  0:00:39s
epoch 21 | loss: 0.73904 | val_0_rmse: 0.85571 | val_1_rmse: 0.87267 |  0:00:41s
epoch 22 | loss: 0.73033 | val_0_rmse: 0.88569 | val_1_rmse: 0.89979 |  0:00:43s
epoch 23 | loss: 0.72358 | val_0_rmse: 0.85411 | val_1_rmse: 0.8761  |  0:00:44s
epoch 24 | loss: 0.71463 | val_0_rmse: 0.85303 | val_1_rmse: 0.8803  |  0:00:46s
epoch 25 | loss: 0.7161  | val_0_rmse: 0.84603 | val_1_rmse: 0.87176 |  0:00:48s
epoch 26 | loss: 0.69954 | val_0_rmse: 0.82959 | val_1_rmse: 0.86379 |  0:00:50s
epoch 27 | loss: 0.68979 | val_0_rmse: 0.83744 | val_1_rmse: 0.86441 |  0:00:52s
epoch 28 | loss: 0.69704 | val_0_rmse: 0.86727 | val_1_rmse: 0.86249 |  0:00:54s
epoch 29 | loss: 0.70114 | val_0_rmse: 0.85006 | val_1_rmse: 0.87982 |  0:00:56s
epoch 30 | loss: 0.70928 | val_0_rmse: 0.83529 | val_1_rmse: 0.87741 |  0:00:58s
epoch 31 | loss: 0.69637 | val_0_rmse: 0.81761 | val_1_rmse: 0.86852 |  0:00:59s
epoch 32 | loss: 0.68502 | val_0_rmse: 0.81449 | val_1_rmse: 0.8604  |  0:01:01s
epoch 33 | loss: 0.67393 | val_0_rmse: 0.80699 | val_1_rmse: 0.85995 |  0:01:03s
epoch 34 | loss: 0.66754 | val_0_rmse: 0.80381 | val_1_rmse: 0.85771 |  0:01:05s
epoch 35 | loss: 0.66129 | val_0_rmse: 0.83284 | val_1_rmse: 0.89292 |  0:01:07s
epoch 36 | loss: 0.65515 | val_0_rmse: 0.80293 | val_1_rmse: 0.86247 |  0:01:09s
epoch 37 | loss: 0.65091 | val_0_rmse: 0.80526 | val_1_rmse: 0.87804 |  0:01:11s
epoch 38 | loss: 0.66397 | val_0_rmse: 0.80691 | val_1_rmse: 0.8636  |  0:01:13s
epoch 39 | loss: 0.65054 | val_0_rmse: 0.86819 | val_1_rmse: 0.92041 |  0:01:14s
epoch 40 | loss: 0.64017 | val_0_rmse: 0.78377 | val_1_rmse: 0.8739  |  0:01:16s
epoch 41 | loss: 0.63334 | val_0_rmse: 0.7776  | val_1_rmse: 0.85911 |  0:01:18s
epoch 42 | loss: 0.62176 | val_0_rmse: 0.76772 | val_1_rmse: 0.85861 |  0:01:20s
epoch 43 | loss: 0.62406 | val_0_rmse: 0.7729  | val_1_rmse: 0.85521 |  0:01:22s
epoch 44 | loss: 0.63063 | val_0_rmse: 0.77413 | val_1_rmse: 0.86033 |  0:01:24s
epoch 45 | loss: 0.61793 | val_0_rmse: 0.76113 | val_1_rmse: 0.85488 |  0:01:26s
epoch 46 | loss: 0.61883 | val_0_rmse: 0.76966 | val_1_rmse: 0.85788 |  0:01:27s
epoch 47 | loss: 0.61289 | val_0_rmse: 0.82107 | val_1_rmse: 0.86351 |  0:01:29s
epoch 48 | loss: 0.61103 | val_0_rmse: 0.78363 | val_1_rmse: 0.86171 |  0:01:31s
epoch 49 | loss: 0.60793 | val_0_rmse: 0.76625 | val_1_rmse: 0.87108 |  0:01:33s
epoch 50 | loss: 0.60889 | val_0_rmse: 0.75631 | val_1_rmse: 0.86399 |  0:01:35s
epoch 51 | loss: 0.6073  | val_0_rmse: 0.82366 | val_1_rmse: 0.90992 |  0:01:37s
epoch 52 | loss: 0.60222 | val_0_rmse: 0.7603  | val_1_rmse: 0.87739 |  0:01:39s
epoch 53 | loss: 0.59495 | val_0_rmse: 0.79808 | val_1_rmse: 0.92497 |  0:01:41s
epoch 54 | loss: 0.58559 | val_0_rmse: 0.75029 | val_1_rmse: 0.87486 |  0:01:42s
epoch 55 | loss: 0.58665 | val_0_rmse: 0.74719 | val_1_rmse: 0.87028 |  0:01:44s
epoch 56 | loss: 0.58537 | val_0_rmse: 0.7445  | val_1_rmse: 0.87646 |  0:01:46s
epoch 57 | loss: 0.56972 | val_0_rmse: 0.746   | val_1_rmse: 0.86069 |  0:01:48s
epoch 58 | loss: 0.5777  | val_0_rmse: 0.82031 | val_1_rmse: 0.96557 |  0:01:50s
epoch 59 | loss: 0.59169 | val_0_rmse: 0.85746 | val_1_rmse: 0.936   |  0:01:52s
epoch 60 | loss: 0.58116 | val_0_rmse: 0.74624 | val_1_rmse: 0.89045 |  0:01:54s
epoch 61 | loss: 0.56612 | val_0_rmse: 0.76918 | val_1_rmse: 0.92971 |  0:01:55s
epoch 62 | loss: 0.56257 | val_0_rmse: 0.71939 | val_1_rmse: 0.87806 |  0:01:57s
epoch 63 | loss: 0.55171 | val_0_rmse: 0.78319 | val_1_rmse: 0.94136 |  0:01:59s
epoch 64 | loss: 0.55182 | val_0_rmse: 0.84347 | val_1_rmse: 0.94634 |  0:02:01s
epoch 65 | loss: 0.55081 | val_0_rmse: 0.76433 | val_1_rmse: 0.91291 |  0:02:03s
epoch 66 | loss: 0.55411 | val_0_rmse: 0.74919 | val_1_rmse: 0.87732 |  0:02:05s
epoch 67 | loss: 0.55925 | val_0_rmse: 0.76767 | val_1_rmse: 0.89608 |  0:02:07s
epoch 68 | loss: 0.5594  | val_0_rmse: 0.72412 | val_1_rmse: 0.87843 |  0:02:09s
epoch 69 | loss: 0.54913 | val_0_rmse: 0.72125 | val_1_rmse: 0.88168 |  0:02:10s
epoch 70 | loss: 0.53511 | val_0_rmse: 0.72365 | val_1_rmse: 0.90756 |  0:02:12s
epoch 71 | loss: 0.55102 | val_0_rmse: 0.74029 | val_1_rmse: 0.90037 |  0:02:14s
epoch 72 | loss: 0.5338  | val_0_rmse: 0.74298 | val_1_rmse: 0.88775 |  0:02:16s
epoch 73 | loss: 0.54482 | val_0_rmse: 0.83398 | val_1_rmse: 0.94642 |  0:02:18s
epoch 74 | loss: 0.5297  | val_0_rmse: 0.83769 | val_1_rmse: 0.95376 |  0:02:20s
epoch 75 | loss: 0.5321  | val_0_rmse: 0.83981 | val_1_rmse: 0.95631 |  0:02:22s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.85488
Best weights from best epoch are automatically used!
ended training at: 13:10:53
Feature importance:
Mean squared error is of 0.04403652431329308
Mean absolute error:0.14573537592711996
MAPE:0.15713878971652254
R2 score:0.2700297617060613
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:10:54
epoch 0  | loss: 1.4243  | val_0_rmse: 0.99988 | val_1_rmse: 0.9827  |  0:00:01s
epoch 1  | loss: 1.00848 | val_0_rmse: 1.00088 | val_1_rmse: 0.984   |  0:00:03s
epoch 2  | loss: 1.00471 | val_0_rmse: 1.00072 | val_1_rmse: 0.98352 |  0:00:05s
epoch 3  | loss: 1.00581 | val_0_rmse: 1.0006  | val_1_rmse: 0.98346 |  0:00:07s
epoch 4  | loss: 1.0     | val_0_rmse: 1.00031 | val_1_rmse: 0.98361 |  0:00:09s
epoch 5  | loss: 0.99261 | val_0_rmse: 1.00036 | val_1_rmse: 0.98262 |  0:00:11s
epoch 6  | loss: 0.98515 | val_0_rmse: 0.99564 | val_1_rmse: 0.98056 |  0:00:13s
epoch 7  | loss: 0.97846 | val_0_rmse: 0.96565 | val_1_rmse: 0.95543 |  0:00:14s
epoch 8  | loss: 0.96112 | val_0_rmse: 0.96155 | val_1_rmse: 0.94781 |  0:00:16s
epoch 9  | loss: 0.954   | val_0_rmse: 0.97504 | val_1_rmse: 0.95905 |  0:00:18s
epoch 10 | loss: 0.96014 | val_0_rmse: 0.96462 | val_1_rmse: 0.9502  |  0:00:20s
epoch 11 | loss: 0.9563  | val_0_rmse: 0.96489 | val_1_rmse: 0.95202 |  0:00:22s
epoch 12 | loss: 0.94955 | val_0_rmse: 0.97856 | val_1_rmse: 0.9622  |  0:00:24s
epoch 13 | loss: 0.93825 | val_0_rmse: 0.96828 | val_1_rmse: 0.95638 |  0:00:26s
epoch 14 | loss: 0.93928 | val_0_rmse: 0.98877 | val_1_rmse: 0.99051 |  0:00:28s
epoch 15 | loss: 0.93702 | val_0_rmse: 0.99444 | val_1_rmse: 0.99514 |  0:00:30s
epoch 16 | loss: 0.94259 | val_0_rmse: 0.99196 | val_1_rmse: 0.99337 |  0:00:31s
epoch 17 | loss: 0.93747 | val_0_rmse: 0.96344 | val_1_rmse: 0.94498 |  0:00:33s
epoch 18 | loss: 0.93635 | val_0_rmse: 0.96446 | val_1_rmse: 0.94929 |  0:00:35s
epoch 19 | loss: 0.94492 | val_0_rmse: 0.96518 | val_1_rmse: 0.94767 |  0:00:37s
epoch 20 | loss: 0.9352  | val_0_rmse: 0.95926 | val_1_rmse: 0.94621 |  0:00:39s
epoch 21 | loss: 0.93125 | val_0_rmse: 0.96074 | val_1_rmse: 0.95157 |  0:00:41s
epoch 22 | loss: 0.93076 | val_0_rmse: 0.96563 | val_1_rmse: 0.95302 |  0:00:43s
epoch 23 | loss: 0.93541 | val_0_rmse: 0.96693 | val_1_rmse: 0.95325 |  0:00:45s
epoch 24 | loss: 0.93481 | val_0_rmse: 0.96832 | val_1_rmse: 0.95643 |  0:00:46s
epoch 25 | loss: 0.9402  | val_0_rmse: 0.96855 | val_1_rmse: 0.95601 |  0:00:48s
epoch 26 | loss: 0.93609 | val_0_rmse: 0.96892 | val_1_rmse: 0.95568 |  0:00:50s
epoch 27 | loss: 0.93397 | val_0_rmse: 0.96625 | val_1_rmse: 0.95389 |  0:00:52s
epoch 28 | loss: 0.93893 | val_0_rmse: 0.96599 | val_1_rmse: 0.95304 |  0:00:54s
epoch 29 | loss: 0.93887 | val_0_rmse: 0.96778 | val_1_rmse: 0.95527 |  0:00:56s
epoch 30 | loss: 0.94081 | val_0_rmse: 0.96624 | val_1_rmse: 0.95369 |  0:00:58s
epoch 31 | loss: 0.93469 | val_0_rmse: 0.96426 | val_1_rmse: 0.95068 |  0:01:00s
epoch 32 | loss: 0.92931 | val_0_rmse: 0.96463 | val_1_rmse: 0.95189 |  0:01:01s
epoch 33 | loss: 0.92468 | val_0_rmse: 0.95687 | val_1_rmse: 0.94537 |  0:01:03s
epoch 34 | loss: 0.89502 | val_0_rmse: 0.92872 | val_1_rmse: 0.91276 |  0:01:05s
epoch 35 | loss: 0.83752 | val_0_rmse: 0.91649 | val_1_rmse: 0.90418 |  0:01:07s
epoch 36 | loss: 0.81194 | val_0_rmse: 0.89334 | val_1_rmse: 0.87947 |  0:01:09s
epoch 37 | loss: 0.81244 | val_0_rmse: 0.8935  | val_1_rmse: 0.88778 |  0:01:11s
epoch 38 | loss: 0.80993 | val_0_rmse: 0.91011 | val_1_rmse: 0.90404 |  0:01:13s
epoch 39 | loss: 0.81463 | val_0_rmse: 0.90329 | val_1_rmse: 0.89241 |  0:01:15s
epoch 40 | loss: 0.78817 | val_0_rmse: 0.87885 | val_1_rmse: 0.87508 |  0:01:17s
epoch 41 | loss: 0.75458 | val_0_rmse: 0.87594 | val_1_rmse: 0.87587 |  0:01:18s
epoch 42 | loss: 0.75063 | val_0_rmse: 0.86079 | val_1_rmse: 0.86418 |  0:01:20s
epoch 43 | loss: 0.74593 | val_0_rmse: 0.88576 | val_1_rmse: 0.89241 |  0:01:22s
epoch 44 | loss: 0.74631 | val_0_rmse: 0.84604 | val_1_rmse: 0.86134 |  0:01:24s
epoch 45 | loss: 0.73632 | val_0_rmse: 0.84421 | val_1_rmse: 0.8588  |  0:01:26s
epoch 46 | loss: 0.7226  | val_0_rmse: 0.83917 | val_1_rmse: 0.8575  |  0:01:28s
epoch 47 | loss: 0.71937 | val_0_rmse: 0.84992 | val_1_rmse: 0.86832 |  0:01:30s
epoch 48 | loss: 0.71756 | val_0_rmse: 0.83829 | val_1_rmse: 0.85571 |  0:01:32s
epoch 49 | loss: 0.71521 | val_0_rmse: 0.84637 | val_1_rmse: 0.87257 |  0:01:33s
epoch 50 | loss: 0.70699 | val_0_rmse: 0.84905 | val_1_rmse: 0.8594  |  0:01:35s
epoch 51 | loss: 0.70169 | val_0_rmse: 0.83697 | val_1_rmse: 0.86688 |  0:01:37s
epoch 52 | loss: 0.71239 | val_0_rmse: 0.84425 | val_1_rmse: 0.87101 |  0:01:39s
epoch 53 | loss: 0.70676 | val_0_rmse: 0.83135 | val_1_rmse: 0.86537 |  0:01:41s
epoch 54 | loss: 0.70416 | val_0_rmse: 0.86863 | val_1_rmse: 0.88069 |  0:01:43s
epoch 55 | loss: 0.70499 | val_0_rmse: 0.89939 | val_1_rmse: 0.93193 |  0:01:45s
epoch 56 | loss: 0.72703 | val_0_rmse: 0.86657 | val_1_rmse: 0.89242 |  0:01:47s
epoch 57 | loss: 0.72999 | val_0_rmse: 0.84634 | val_1_rmse: 0.86772 |  0:01:48s
epoch 58 | loss: 0.72414 | val_0_rmse: 0.864   | val_1_rmse: 0.8939  |  0:01:50s
epoch 59 | loss: 0.72712 | val_0_rmse: 0.84099 | val_1_rmse: 0.85518 |  0:01:52s
epoch 60 | loss: 0.71782 | val_0_rmse: 0.84536 | val_1_rmse: 0.85314 |  0:01:54s
epoch 61 | loss: 0.71533 | val_0_rmse: 0.8364  | val_1_rmse: 0.84786 |  0:01:56s
epoch 62 | loss: 0.70962 | val_0_rmse: 0.83164 | val_1_rmse: 0.8424  |  0:01:58s
epoch 63 | loss: 0.70535 | val_0_rmse: 0.8402  | val_1_rmse: 0.86078 |  0:02:00s
epoch 64 | loss: 0.70207 | val_0_rmse: 0.84725 | val_1_rmse: 0.87366 |  0:02:02s
epoch 65 | loss: 0.69965 | val_0_rmse: 0.8343  | val_1_rmse: 0.84997 |  0:02:03s
epoch 66 | loss: 0.70236 | val_0_rmse: 0.83181 | val_1_rmse: 0.86254 |  0:02:05s
epoch 67 | loss: 0.70243 | val_0_rmse: 0.99496 | val_1_rmse: 1.0472  |  0:02:07s
epoch 68 | loss: 0.73176 | val_0_rmse: 0.83902 | val_1_rmse: 0.84956 |  0:02:09s
epoch 69 | loss: 0.70792 | val_0_rmse: 0.90103 | val_1_rmse: 0.90202 |  0:02:11s
epoch 70 | loss: 0.73052 | val_0_rmse: 0.83979 | val_1_rmse: 0.86721 |  0:02:13s
epoch 71 | loss: 0.72045 | val_0_rmse: 0.84827 | val_1_rmse: 0.86426 |  0:02:15s
epoch 72 | loss: 0.71188 | val_0_rmse: 0.82747 | val_1_rmse: 0.8515  |  0:02:17s
epoch 73 | loss: 0.70192 | val_0_rmse: 0.89609 | val_1_rmse: 0.92819 |  0:02:19s
epoch 74 | loss: 0.73854 | val_0_rmse: 0.85571 | val_1_rmse: 0.86315 |  0:02:21s
epoch 75 | loss: 0.72229 | val_0_rmse: 0.85106 | val_1_rmse: 0.87158 |  0:02:23s
epoch 76 | loss: 0.72878 | val_0_rmse: 0.83576 | val_1_rmse: 0.86791 |  0:02:25s
epoch 77 | loss: 0.69959 | val_0_rmse: 0.83333 | val_1_rmse: 0.85932 |  0:02:27s
epoch 78 | loss: 0.68748 | val_0_rmse: 0.8372  | val_1_rmse: 0.88258 |  0:02:29s
epoch 79 | loss: 0.69552 | val_0_rmse: 0.82447 | val_1_rmse: 0.85631 |  0:02:31s
epoch 80 | loss: 0.68998 | val_0_rmse: 0.81489 | val_1_rmse: 0.85677 |  0:02:32s
epoch 81 | loss: 0.67711 | val_0_rmse: 0.81164 | val_1_rmse: 0.85727 |  0:02:34s
epoch 82 | loss: 0.68076 | val_0_rmse: 0.83632 | val_1_rmse: 0.85376 |  0:02:36s
epoch 83 | loss: 0.67968 | val_0_rmse: 0.83361 | val_1_rmse: 0.88788 |  0:02:38s
epoch 84 | loss: 0.68322 | val_0_rmse: 0.82047 | val_1_rmse: 0.85041 |  0:02:40s
epoch 85 | loss: 0.68255 | val_0_rmse: 0.82015 | val_1_rmse: 0.85743 |  0:02:42s
epoch 86 | loss: 0.6848  | val_0_rmse: 0.81641 | val_1_rmse: 0.85702 |  0:02:44s
epoch 87 | loss: 0.67747 | val_0_rmse: 0.82978 | val_1_rmse: 0.85692 |  0:02:46s
epoch 88 | loss: 0.67755 | val_0_rmse: 0.81574 | val_1_rmse: 0.85257 |  0:02:48s
epoch 89 | loss: 0.68115 | val_0_rmse: 0.81628 | val_1_rmse: 0.85935 |  0:02:50s
epoch 90 | loss: 0.67893 | val_0_rmse: 0.83294 | val_1_rmse: 0.8717  |  0:02:52s
epoch 91 | loss: 0.69053 | val_0_rmse: 0.83243 | val_1_rmse: 0.85447 |  0:02:54s
epoch 92 | loss: 0.69223 | val_0_rmse: 0.83462 | val_1_rmse: 0.85609 |  0:02:55s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.8424
Best weights from best epoch are automatically used!
ended training at: 13:13:51
Feature importance:
Mean squared error is of 0.06410648885109864
Mean absolute error:0.15527160883470187
MAPE:0.16739012390186117
R2 score:0.21606231188142455
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:13:51
epoch 0  | loss: 1.34086 | val_0_rmse: 1.00337 | val_1_rmse: 0.991   |  0:00:01s
epoch 1  | loss: 0.95284 | val_0_rmse: 0.96323 | val_1_rmse: 0.94798 |  0:00:03s
epoch 2  | loss: 0.85982 | val_0_rmse: 0.91414 | val_1_rmse: 0.9002  |  0:00:06s
epoch 3  | loss: 0.83904 | val_0_rmse: 0.91315 | val_1_rmse: 0.89989 |  0:00:08s
epoch 4  | loss: 0.83476 | val_0_rmse: 0.914   | val_1_rmse: 0.90319 |  0:00:09s
epoch 5  | loss: 0.83614 | val_0_rmse: 0.91414 | val_1_rmse: 0.90035 |  0:00:11s
epoch 6  | loss: 0.83437 | val_0_rmse: 0.91024 | val_1_rmse: 0.89682 |  0:00:13s
epoch 7  | loss: 0.84504 | val_0_rmse: 0.91011 | val_1_rmse: 0.8984  |  0:00:15s
epoch 8  | loss: 0.83511 | val_0_rmse: 0.91077 | val_1_rmse: 0.89945 |  0:00:17s
epoch 9  | loss: 0.82788 | val_0_rmse: 0.90754 | val_1_rmse: 0.89349 |  0:00:19s
epoch 10 | loss: 0.82665 | val_0_rmse: 0.90801 | val_1_rmse: 0.895   |  0:00:21s
epoch 11 | loss: 0.83495 | val_0_rmse: 0.91099 | val_1_rmse: 0.89643 |  0:00:23s
epoch 12 | loss: 0.82894 | val_0_rmse: 0.91126 | val_1_rmse: 0.89994 |  0:00:25s
epoch 13 | loss: 0.83075 | val_0_rmse: 0.90742 | val_1_rmse: 0.89721 |  0:00:26s
epoch 14 | loss: 0.83191 | val_0_rmse: 0.91187 | val_1_rmse: 0.90171 |  0:00:28s
epoch 15 | loss: 0.82753 | val_0_rmse: 0.90847 | val_1_rmse: 0.89805 |  0:00:30s
epoch 16 | loss: 0.82412 | val_0_rmse: 0.90999 | val_1_rmse: 0.8976  |  0:00:32s
epoch 17 | loss: 0.82633 | val_0_rmse: 0.91189 | val_1_rmse: 0.89823 |  0:00:34s
epoch 18 | loss: 0.82636 | val_0_rmse: 0.9067  | val_1_rmse: 0.89345 |  0:00:36s
epoch 19 | loss: 0.82386 | val_0_rmse: 0.90508 | val_1_rmse: 0.89331 |  0:00:38s
epoch 20 | loss: 0.82383 | val_0_rmse: 0.90581 | val_1_rmse: 0.89259 |  0:00:40s
epoch 21 | loss: 0.82139 | val_0_rmse: 0.90471 | val_1_rmse: 0.88994 |  0:00:42s
epoch 22 | loss: 0.82204 | val_0_rmse: 0.90504 | val_1_rmse: 0.89068 |  0:00:44s
epoch 23 | loss: 0.82362 | val_0_rmse: 0.90578 | val_1_rmse: 0.8923  |  0:00:46s
epoch 24 | loss: 0.82341 | val_0_rmse: 0.90484 | val_1_rmse: 0.88916 |  0:00:48s
epoch 25 | loss: 0.82202 | val_0_rmse: 0.90436 | val_1_rmse: 0.88817 |  0:00:50s
epoch 26 | loss: 0.82159 | val_0_rmse: 0.90477 | val_1_rmse: 0.89182 |  0:00:52s
epoch 27 | loss: 0.82278 | val_0_rmse: 0.90472 | val_1_rmse: 0.89256 |  0:00:53s
epoch 28 | loss: 0.82151 | val_0_rmse: 0.90427 | val_1_rmse: 0.89219 |  0:00:55s
epoch 29 | loss: 0.82199 | val_0_rmse: 0.90334 | val_1_rmse: 0.88993 |  0:00:57s
epoch 30 | loss: 0.81995 | val_0_rmse: 0.9105  | val_1_rmse: 0.89587 |  0:00:59s
epoch 31 | loss: 0.82488 | val_0_rmse: 0.90997 | val_1_rmse: 0.89625 |  0:01:01s
epoch 32 | loss: 0.82285 | val_0_rmse: 0.90476 | val_1_rmse: 0.89047 |  0:01:03s
epoch 33 | loss: 0.82522 | val_0_rmse: 0.90731 | val_1_rmse: 0.8945  |  0:01:05s
epoch 34 | loss: 0.82602 | val_0_rmse: 0.90807 | val_1_rmse: 0.89569 |  0:01:07s
epoch 35 | loss: 0.82604 | val_0_rmse: 0.92423 | val_1_rmse: 0.9061  |  0:01:08s
epoch 36 | loss: 0.82574 | val_0_rmse: 0.90533 | val_1_rmse: 0.89217 |  0:01:12s
epoch 37 | loss: 0.82144 | val_0_rmse: 0.9042  | val_1_rmse: 0.89121 |  0:01:14s
epoch 38 | loss: 0.8227  | val_0_rmse: 0.90513 | val_1_rmse: 0.89218 |  0:01:16s
epoch 39 | loss: 0.82052 | val_0_rmse: 0.90312 | val_1_rmse: 0.89087 |  0:01:18s
epoch 40 | loss: 0.81987 | val_0_rmse: 0.90725 | val_1_rmse: 0.8959  |  0:01:20s
epoch 41 | loss: 0.83105 | val_0_rmse: 0.90865 | val_1_rmse: 0.89775 |  0:01:22s
epoch 42 | loss: 0.82731 | val_0_rmse: 0.90467 | val_1_rmse: 0.89372 |  0:01:23s
epoch 43 | loss: 0.82133 | val_0_rmse: 0.90561 | val_1_rmse: 0.89282 |  0:01:25s
epoch 44 | loss: 0.82477 | val_0_rmse: 0.90538 | val_1_rmse: 0.89193 |  0:01:27s
epoch 45 | loss: 0.82115 | val_0_rmse: 0.91221 | val_1_rmse: 0.90057 |  0:01:29s
epoch 46 | loss: 0.81608 | val_0_rmse: 0.90287 | val_1_rmse: 0.89461 |  0:01:31s
epoch 47 | loss: 0.81605 | val_0_rmse: 0.90296 | val_1_rmse: 0.89228 |  0:01:33s
epoch 48 | loss: 0.81569 | val_0_rmse: 0.89812 | val_1_rmse: 0.8877  |  0:01:35s
epoch 49 | loss: 0.81131 | val_0_rmse: 0.90055 | val_1_rmse: 0.8882  |  0:01:37s
epoch 50 | loss: 0.81392 | val_0_rmse: 0.8979  | val_1_rmse: 0.88707 |  0:01:39s
epoch 51 | loss: 0.81112 | val_0_rmse: 0.89996 | val_1_rmse: 0.89062 |  0:01:41s
epoch 52 | loss: 0.81133 | val_0_rmse: 0.89809 | val_1_rmse: 0.88702 |  0:01:43s
epoch 53 | loss: 0.80811 | val_0_rmse: 0.89604 | val_1_rmse: 0.8873  |  0:01:45s
epoch 54 | loss: 0.81057 | val_0_rmse: 0.89692 | val_1_rmse: 0.89089 |  0:01:46s
epoch 55 | loss: 0.80508 | val_0_rmse: 0.89235 | val_1_rmse: 0.88621 |  0:01:48s
epoch 56 | loss: 0.80086 | val_0_rmse: 0.8919  | val_1_rmse: 0.88601 |  0:01:50s
epoch 57 | loss: 0.80212 | val_0_rmse: 0.89351 | val_1_rmse: 0.88995 |  0:01:52s
epoch 58 | loss: 0.80267 | val_0_rmse: 0.89641 | val_1_rmse: 0.8888  |  0:01:54s
epoch 59 | loss: 0.80546 | val_0_rmse: 0.89375 | val_1_rmse: 0.89064 |  0:01:56s
epoch 60 | loss: 0.8018  | val_0_rmse: 0.89179 | val_1_rmse: 0.8904  |  0:01:58s
epoch 61 | loss: 0.80213 | val_0_rmse: 0.89347 | val_1_rmse: 0.89465 |  0:02:00s
epoch 62 | loss: 0.80429 | val_0_rmse: 0.89586 | val_1_rmse: 0.90321 |  0:02:02s
epoch 63 | loss: 0.80335 | val_0_rmse: 0.89207 | val_1_rmse: 0.88662 |  0:02:04s
epoch 64 | loss: 0.80072 | val_0_rmse: 0.89861 | val_1_rmse: 0.89241 |  0:02:05s
epoch 65 | loss: 0.8035  | val_0_rmse: 0.8886  | val_1_rmse: 0.88412 |  0:02:07s
epoch 66 | loss: 0.7946  | val_0_rmse: 0.8891  | val_1_rmse: 0.88953 |  0:02:09s
epoch 67 | loss: 0.79817 | val_0_rmse: 0.89003 | val_1_rmse: 0.87973 |  0:02:11s
epoch 68 | loss: 0.79525 | val_0_rmse: 0.88722 | val_1_rmse: 0.88089 |  0:02:13s
epoch 69 | loss: 0.79984 | val_0_rmse: 0.88935 | val_1_rmse: 0.88506 |  0:02:15s
epoch 70 | loss: 0.79663 | val_0_rmse: 0.89293 | val_1_rmse: 0.88656 |  0:02:17s
epoch 71 | loss: 0.79979 | val_0_rmse: 0.88971 | val_1_rmse: 0.88141 |  0:02:19s
epoch 72 | loss: 0.79502 | val_0_rmse: 0.88941 | val_1_rmse: 0.88394 |  0:02:21s
epoch 73 | loss: 0.79095 | val_0_rmse: 0.88508 | val_1_rmse: 0.88526 |  0:02:22s
epoch 74 | loss: 0.78956 | val_0_rmse: 0.88479 | val_1_rmse: 0.88403 |  0:02:24s
epoch 75 | loss: 0.78595 | val_0_rmse: 0.88426 | val_1_rmse: 0.8828  |  0:02:26s
epoch 76 | loss: 0.78458 | val_0_rmse: 0.88384 | val_1_rmse: 0.88415 |  0:02:28s
epoch 77 | loss: 0.78756 | val_0_rmse: 0.88592 | val_1_rmse: 0.88435 |  0:02:30s
epoch 78 | loss: 0.79267 | val_0_rmse: 0.89087 | val_1_rmse: 0.88749 |  0:02:32s
epoch 79 | loss: 0.79256 | val_0_rmse: 0.88593 | val_1_rmse: 0.88289 |  0:02:34s
epoch 80 | loss: 0.78868 | val_0_rmse: 0.89107 | val_1_rmse: 0.88394 |  0:02:36s
epoch 81 | loss: 0.79183 | val_0_rmse: 0.88532 | val_1_rmse: 0.88168 |  0:02:37s
epoch 82 | loss: 0.79395 | val_0_rmse: 0.88856 | val_1_rmse: 0.88445 |  0:02:39s
epoch 83 | loss: 0.79286 | val_0_rmse: 0.88497 | val_1_rmse: 0.88029 |  0:02:41s
epoch 84 | loss: 0.78621 | val_0_rmse: 0.88371 | val_1_rmse: 0.8788  |  0:02:43s
epoch 85 | loss: 0.78615 | val_0_rmse: 0.88279 | val_1_rmse: 0.88145 |  0:02:45s
epoch 86 | loss: 0.78505 | val_0_rmse: 0.88361 | val_1_rmse: 0.88233 |  0:02:47s
epoch 87 | loss: 0.78353 | val_0_rmse: 0.88325 | val_1_rmse: 0.88252 |  0:02:49s
epoch 88 | loss: 0.78751 | val_0_rmse: 0.88665 | val_1_rmse: 0.88727 |  0:02:51s
epoch 89 | loss: 0.78312 | val_0_rmse: 0.88229 | val_1_rmse: 0.88167 |  0:02:52s
epoch 90 | loss: 0.78264 | val_0_rmse: 0.8812  | val_1_rmse: 0.88311 |  0:02:54s
epoch 91 | loss: 0.78152 | val_0_rmse: 0.87985 | val_1_rmse: 0.88414 |  0:02:56s
epoch 92 | loss: 0.77976 | val_0_rmse: 0.87969 | val_1_rmse: 0.88527 |  0:02:58s
epoch 93 | loss: 0.78006 | val_0_rmse: 0.88402 | val_1_rmse: 0.88626 |  0:03:00s
epoch 94 | loss: 0.78685 | val_0_rmse: 0.88332 | val_1_rmse: 0.88477 |  0:03:02s
epoch 95 | loss: 0.78397 | val_0_rmse: 0.88114 | val_1_rmse: 0.88476 |  0:03:04s
epoch 96 | loss: 0.78283 | val_0_rmse: 0.8809  | val_1_rmse: 0.88439 |  0:03:06s
epoch 97 | loss: 0.78436 | val_0_rmse: 0.88168 | val_1_rmse: 0.88124 |  0:03:07s
epoch 98 | loss: 0.78371 | val_0_rmse: 0.88124 | val_1_rmse: 0.88105 |  0:03:09s
epoch 99 | loss: 0.78152 | val_0_rmse: 0.88011 | val_1_rmse: 0.87916 |  0:03:11s
epoch 100| loss: 0.7794  | val_0_rmse: 0.88144 | val_1_rmse: 0.88059 |  0:03:13s
epoch 101| loss: 0.77673 | val_0_rmse: 0.87825 | val_1_rmse: 0.87704 |  0:03:15s
epoch 102| loss: 0.77521 | val_0_rmse: 0.87772 | val_1_rmse: 0.87625 |  0:03:17s
epoch 103| loss: 0.77552 | val_0_rmse: 0.88069 | val_1_rmse: 0.8754  |  0:03:19s
epoch 104| loss: 0.77585 | val_0_rmse: 0.87903 | val_1_rmse: 0.87701 |  0:03:21s
epoch 105| loss: 0.77556 | val_0_rmse: 0.87519 | val_1_rmse: 0.87398 |  0:03:23s
epoch 106| loss: 0.76898 | val_0_rmse: 0.86817 | val_1_rmse: 0.86657 |  0:03:24s
epoch 107| loss: 0.76449 | val_0_rmse: 0.86874 | val_1_rmse: 0.86553 |  0:03:26s
epoch 108| loss: 0.76591 | val_0_rmse: 0.87171 | val_1_rmse: 0.86858 |  0:03:28s
epoch 109| loss: 0.75704 | val_0_rmse: 0.86385 | val_1_rmse: 0.87109 |  0:03:30s
epoch 110| loss: 0.75582 | val_0_rmse: 0.86242 | val_1_rmse: 0.86405 |  0:03:32s
epoch 111| loss: 0.75629 | val_0_rmse: 0.86841 | val_1_rmse: 0.87716 |  0:03:34s
epoch 112| loss: 0.76754 | val_0_rmse: 0.86232 | val_1_rmse: 0.86527 |  0:03:36s
epoch 113| loss: 0.75681 | val_0_rmse: 0.88459 | val_1_rmse: 0.88227 |  0:03:38s
epoch 114| loss: 0.75723 | val_0_rmse: 0.86866 | val_1_rmse: 0.87215 |  0:03:39s
epoch 115| loss: 0.75799 | val_0_rmse: 0.86862 | val_1_rmse: 0.86577 |  0:03:41s
epoch 116| loss: 0.75482 | val_0_rmse: 0.86128 | val_1_rmse: 0.86257 |  0:03:43s
epoch 117| loss: 0.75423 | val_0_rmse: 0.88166 | val_1_rmse: 0.8896  |  0:03:45s
epoch 118| loss: 0.75512 | val_0_rmse: 0.87309 | val_1_rmse: 0.87    |  0:03:47s
epoch 119| loss: 0.76615 | val_0_rmse: 0.8698  | val_1_rmse: 0.87574 |  0:03:49s
epoch 120| loss: 0.75288 | val_0_rmse: 0.85691 | val_1_rmse: 0.85879 |  0:03:51s
epoch 121| loss: 0.74865 | val_0_rmse: 0.85886 | val_1_rmse: 0.86029 |  0:03:53s
epoch 122| loss: 0.74483 | val_0_rmse: 0.85912 | val_1_rmse: 0.85978 |  0:03:55s
epoch 123| loss: 0.7439  | val_0_rmse: 0.86223 | val_1_rmse: 0.86154 |  0:03:56s
epoch 124| loss: 0.75013 | val_0_rmse: 0.86338 | val_1_rmse: 0.86082 |  0:03:58s
epoch 125| loss: 0.74811 | val_0_rmse: 0.85752 | val_1_rmse: 0.86037 |  0:04:00s
epoch 126| loss: 0.74698 | val_0_rmse: 0.86152 | val_1_rmse: 0.86354 |  0:04:02s
epoch 127| loss: 0.73922 | val_0_rmse: 0.85363 | val_1_rmse: 0.85381 |  0:04:04s
epoch 128| loss: 0.73796 | val_0_rmse: 0.85619 | val_1_rmse: 0.85201 |  0:04:06s
epoch 129| loss: 0.74184 | val_0_rmse: 0.85883 | val_1_rmse: 0.8646  |  0:04:08s
epoch 130| loss: 0.75242 | val_0_rmse: 0.86404 | val_1_rmse: 0.86498 |  0:04:10s
epoch 131| loss: 0.74628 | val_0_rmse: 0.84966 | val_1_rmse: 0.85396 |  0:04:11s
epoch 132| loss: 0.73298 | val_0_rmse: 0.86643 | val_1_rmse: 0.86598 |  0:04:13s
epoch 133| loss: 0.73498 | val_0_rmse: 0.85697 | val_1_rmse: 0.85694 |  0:04:15s
epoch 134| loss: 0.7313  | val_0_rmse: 0.84665 | val_1_rmse: 0.84922 |  0:04:17s
epoch 135| loss: 0.72708 | val_0_rmse: 0.85448 | val_1_rmse: 0.87213 |  0:04:19s
epoch 136| loss: 0.73006 | val_0_rmse: 0.84718 | val_1_rmse: 0.85649 |  0:04:21s
epoch 137| loss: 0.72829 | val_0_rmse: 0.84537 | val_1_rmse: 0.85502 |  0:04:23s
epoch 138| loss: 0.72424 | val_0_rmse: 0.84416 | val_1_rmse: 0.85427 |  0:04:25s
epoch 139| loss: 0.7208  | val_0_rmse: 0.84405 | val_1_rmse: 0.85115 |  0:04:27s
epoch 140| loss: 0.73169 | val_0_rmse: 0.84698 | val_1_rmse: 0.85584 |  0:04:28s
epoch 141| loss: 0.73212 | val_0_rmse: 0.84897 | val_1_rmse: 0.8564  |  0:04:30s
epoch 142| loss: 0.72603 | val_0_rmse: 0.8455  | val_1_rmse: 0.85453 |  0:04:32s
epoch 143| loss: 0.72291 | val_0_rmse: 0.84643 | val_1_rmse: 0.85405 |  0:04:34s
epoch 144| loss: 0.71959 | val_0_rmse: 0.84858 | val_1_rmse: 0.8492  |  0:04:36s
epoch 145| loss: 0.72541 | val_0_rmse: 0.84553 | val_1_rmse: 0.84942 |  0:04:38s
epoch 146| loss: 0.73628 | val_0_rmse: 0.91145 | val_1_rmse: 0.92104 |  0:04:40s
epoch 147| loss: 0.73781 | val_0_rmse: 0.85172 | val_1_rmse: 0.85348 |  0:04:42s
epoch 148| loss: 0.73082 | val_0_rmse: 0.86295 | val_1_rmse: 0.8628  |  0:04:43s
epoch 149| loss: 0.73511 | val_0_rmse: 0.85336 | val_1_rmse: 0.85419 |  0:04:45s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.8492
Best weights from best epoch are automatically used!
ended training at: 13:18:38
Feature importance:
Mean squared error is of 0.05121309319185259
Mean absolute error:0.15359336693982534
MAPE:0.1654268611733328
R2 score:0.2434040949309515
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:18:38
epoch 0  | loss: 1.39843 | val_0_rmse: 1.00229 | val_1_rmse: 1.01272 |  0:00:01s
epoch 1  | loss: 1.00718 | val_0_rmse: 1.00274 | val_1_rmse: 1.01262 |  0:00:03s
epoch 2  | loss: 1.00159 | val_0_rmse: 1.00229 | val_1_rmse: 1.0127  |  0:00:05s
epoch 3  | loss: 0.99754 | val_0_rmse: 1.00237 | val_1_rmse: 1.01261 |  0:00:07s
epoch 4  | loss: 1.00271 | val_0_rmse: 1.00227 | val_1_rmse: 1.01231 |  0:00:09s
epoch 5  | loss: 1.00058 | val_0_rmse: 0.99898 | val_1_rmse: 1.00939 |  0:00:11s
epoch 6  | loss: 0.99116 | val_0_rmse: 0.97711 | val_1_rmse: 0.98805 |  0:00:13s
epoch 7  | loss: 0.9499  | val_0_rmse: 0.9479  | val_1_rmse: 0.95468 |  0:00:15s
epoch 8  | loss: 0.88487 | val_0_rmse: 0.91815 | val_1_rmse: 0.92347 |  0:00:16s
epoch 9  | loss: 0.8541  | val_0_rmse: 0.92796 | val_1_rmse: 0.93016 |  0:00:18s
epoch 10 | loss: 0.82878 | val_0_rmse: 0.91175 | val_1_rmse: 0.92112 |  0:00:20s
epoch 11 | loss: 0.81854 | val_0_rmse: 0.90553 | val_1_rmse: 0.91152 |  0:00:22s
epoch 12 | loss: 0.8056  | val_0_rmse: 0.89169 | val_1_rmse: 0.8966  |  0:00:24s
epoch 13 | loss: 0.77059 | val_0_rmse: 0.88375 | val_1_rmse: 0.88841 |  0:00:26s
epoch 14 | loss: 0.75592 | val_0_rmse: 0.88933 | val_1_rmse: 0.89501 |  0:00:28s
epoch 15 | loss: 0.74898 | val_0_rmse: 0.86919 | val_1_rmse: 0.87812 |  0:00:30s
epoch 16 | loss: 0.73685 | val_0_rmse: 0.8651  | val_1_rmse: 0.87319 |  0:00:31s
epoch 17 | loss: 0.71878 | val_0_rmse: 0.8654  | val_1_rmse: 0.87622 |  0:00:33s
epoch 18 | loss: 0.7126  | val_0_rmse: 0.85342 | val_1_rmse: 0.86752 |  0:00:35s
epoch 19 | loss: 0.72024 | val_0_rmse: 0.86039 | val_1_rmse: 0.87364 |  0:00:37s
epoch 20 | loss: 0.70545 | val_0_rmse: 0.85137 | val_1_rmse: 0.86757 |  0:00:39s
epoch 21 | loss: 0.69686 | val_0_rmse: 0.83699 | val_1_rmse: 0.86246 |  0:00:41s
epoch 22 | loss: 0.6943  | val_0_rmse: 0.87192 | val_1_rmse: 0.88547 |  0:00:43s
epoch 23 | loss: 0.69734 | val_0_rmse: 0.8338  | val_1_rmse: 0.86267 |  0:00:44s
epoch 24 | loss: 0.69385 | val_0_rmse: 0.85452 | val_1_rmse: 0.87737 |  0:00:46s
epoch 25 | loss: 0.69031 | val_0_rmse: 0.82005 | val_1_rmse: 0.84839 |  0:00:48s
epoch 26 | loss: 0.68594 | val_0_rmse: 0.99629 | val_1_rmse: 1.00754 |  0:00:50s
epoch 27 | loss: 0.69941 | val_0_rmse: 0.82777 | val_1_rmse: 0.85475 |  0:00:52s
epoch 28 | loss: 0.67479 | val_0_rmse: 0.80889 | val_1_rmse: 0.84956 |  0:00:54s
epoch 29 | loss: 0.66904 | val_0_rmse: 0.8099  | val_1_rmse: 0.84325 |  0:00:56s
epoch 30 | loss: 0.66348 | val_0_rmse: 0.81029 | val_1_rmse: 0.85065 |  0:00:58s
epoch 31 | loss: 0.6646  | val_0_rmse: 0.81175 | val_1_rmse: 0.85134 |  0:00:59s
epoch 32 | loss: 0.66982 | val_0_rmse: 0.80182 | val_1_rmse: 0.86417 |  0:01:01s
epoch 33 | loss: 0.65688 | val_0_rmse: 0.79602 | val_1_rmse: 0.84549 |  0:01:03s
epoch 34 | loss: 0.65717 | val_0_rmse: 0.81014 | val_1_rmse: 0.87512 |  0:01:05s
epoch 35 | loss: 0.65251 | val_0_rmse: 0.80793 | val_1_rmse: 0.86246 |  0:01:07s
epoch 36 | loss: 0.64789 | val_0_rmse: 0.78712 | val_1_rmse: 0.86314 |  0:01:09s
epoch 37 | loss: 0.64806 | val_0_rmse: 0.79928 | val_1_rmse: 0.87149 |  0:01:11s
epoch 38 | loss: 0.64606 | val_0_rmse: 0.80257 | val_1_rmse: 0.86366 |  0:01:13s
epoch 39 | loss: 0.63174 | val_0_rmse: 0.78207 | val_1_rmse: 0.8574  |  0:01:14s
epoch 40 | loss: 0.62907 | val_0_rmse: 0.77913 | val_1_rmse: 0.84951 |  0:01:16s
epoch 41 | loss: 0.64171 | val_0_rmse: 0.79047 | val_1_rmse: 0.86771 |  0:01:18s
epoch 42 | loss: 0.63614 | val_0_rmse: 0.81332 | val_1_rmse: 0.87768 |  0:01:20s
epoch 43 | loss: 0.63095 | val_0_rmse: 0.77781 | val_1_rmse: 0.86373 |  0:01:22s
epoch 44 | loss: 0.62264 | val_0_rmse: 0.7701  | val_1_rmse: 0.8665  |  0:01:24s
epoch 45 | loss: 0.62194 | val_0_rmse: 0.77443 | val_1_rmse: 0.8619  |  0:01:26s
epoch 46 | loss: 0.61959 | val_0_rmse: 0.7904  | val_1_rmse: 0.88382 |  0:01:27s
epoch 47 | loss: 0.61498 | val_0_rmse: 0.76651 | val_1_rmse: 0.86655 |  0:01:29s
epoch 48 | loss: 0.60807 | val_0_rmse: 0.77173 | val_1_rmse: 0.87085 |  0:01:31s
epoch 49 | loss: 0.60785 | val_0_rmse: 0.76169 | val_1_rmse: 0.86098 |  0:01:33s
epoch 50 | loss: 0.61982 | val_0_rmse: 0.77175 | val_1_rmse: 0.87082 |  0:01:35s
epoch 51 | loss: 0.62154 | val_0_rmse: 0.7625  | val_1_rmse: 0.88169 |  0:01:37s
epoch 52 | loss: 0.61517 | val_0_rmse: 0.784   | val_1_rmse: 0.8703  |  0:01:39s
epoch 53 | loss: 0.61832 | val_0_rmse: 0.84717 | val_1_rmse: 0.95904 |  0:01:41s
epoch 54 | loss: 0.60812 | val_0_rmse: 0.77933 | val_1_rmse: 0.8706  |  0:01:42s
epoch 55 | loss: 0.60714 | val_0_rmse: 0.75643 | val_1_rmse: 0.88235 |  0:01:44s
epoch 56 | loss: 0.60751 | val_0_rmse: 0.75875 | val_1_rmse: 0.88796 |  0:01:46s
epoch 57 | loss: 0.60997 | val_0_rmse: 0.82715 | val_1_rmse: 0.97832 |  0:01:48s
epoch 58 | loss: 0.60498 | val_0_rmse: 0.76823 | val_1_rmse: 0.88222 |  0:01:50s
epoch 59 | loss: 0.60182 | val_0_rmse: 0.76735 | val_1_rmse: 0.87939 |  0:01:52s

Early stopping occured at epoch 59 with best_epoch = 29 and best_val_1_rmse = 0.84325
Best weights from best epoch are automatically used!
ended training at: 13:20:31
Feature importance:
Mean squared error is of 0.04857569787621537
Mean absolute error:0.14949506884002176
MAPE:0.1619174441902439
R2 score:0.2517079848527064
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:21:35
epoch 0  | loss: 1.19737 | val_0_rmse: 0.99422 | val_1_rmse: 1.00468 |  0:00:07s
epoch 1  | loss: 0.99349 | val_0_rmse: 0.99572 | val_1_rmse: 1.00661 |  0:00:14s
epoch 2  | loss: 0.99186 | val_0_rmse: 0.9948  | val_1_rmse: 1.0054  |  0:00:21s
epoch 3  | loss: 0.98387 | val_0_rmse: 0.95626 | val_1_rmse: 0.9664  |  0:00:28s
epoch 4  | loss: 0.92887 | val_0_rmse: 0.94745 | val_1_rmse: 0.9541  |  0:00:35s
epoch 5  | loss: 0.88072 | val_0_rmse: 0.94195 | val_1_rmse: 0.9498  |  0:00:42s
epoch 6  | loss: 0.85342 | val_0_rmse: 0.93652 | val_1_rmse: 0.94824 |  0:00:49s
epoch 7  | loss: 0.83698 | val_0_rmse: 0.92553 | val_1_rmse: 0.93872 |  0:00:56s
epoch 8  | loss: 0.82376 | val_0_rmse: 0.90504 | val_1_rmse: 0.92196 |  0:01:03s
epoch 9  | loss: 0.80613 | val_0_rmse: 0.90135 | val_1_rmse: 0.92113 |  0:01:10s
epoch 10 | loss: 0.79249 | val_0_rmse: 0.88461 | val_1_rmse: 0.91199 |  0:01:17s
epoch 11 | loss: 0.7838  | val_0_rmse: 0.8758  | val_1_rmse: 0.91266 |  0:01:24s
epoch 12 | loss: 0.77767 | val_0_rmse: 0.87122 | val_1_rmse: 0.90868 |  0:01:31s
epoch 13 | loss: 0.76779 | val_0_rmse: 0.86187 | val_1_rmse: 0.91514 |  0:01:38s
epoch 14 | loss: 0.76009 | val_0_rmse: 0.86944 | val_1_rmse: 0.90585 |  0:01:45s
epoch 15 | loss: 0.75083 | val_0_rmse: 0.85225 | val_1_rmse: 0.90559 |  0:01:52s
epoch 16 | loss: 0.74366 | val_0_rmse: 0.86342 | val_1_rmse: 0.91404 |  0:01:59s
epoch 17 | loss: 0.73578 | val_0_rmse: 0.85613 | val_1_rmse: 0.93188 |  0:02:06s
epoch 18 | loss: 0.72942 | val_0_rmse: 0.85348 | val_1_rmse: 0.9266  |  0:02:13s
epoch 19 | loss: 0.72172 | val_0_rmse: 0.85802 | val_1_rmse: 0.92036 |  0:02:20s
epoch 20 | loss: 0.71823 | val_0_rmse: 0.85751 | val_1_rmse: 0.93323 |  0:02:27s
epoch 21 | loss: 0.71122 | val_0_rmse: 0.83994 | val_1_rmse: 0.92211 |  0:02:34s
epoch 22 | loss: 0.71318 | val_0_rmse: 0.84014 | val_1_rmse: 0.93271 |  0:02:41s
epoch 23 | loss: 0.70575 | val_0_rmse: 0.83698 | val_1_rmse: 0.92712 |  0:02:48s
epoch 24 | loss: 0.69892 | val_0_rmse: 0.83574 | val_1_rmse: 0.91914 |  0:02:55s
epoch 25 | loss: 0.69387 | val_0_rmse: 0.83937 | val_1_rmse: 0.92813 |  0:03:02s
epoch 26 | loss: 0.68904 | val_0_rmse: 0.83549 | val_1_rmse: 0.92933 |  0:03:09s
epoch 27 | loss: 0.68228 | val_0_rmse: 0.82982 | val_1_rmse: 0.93048 |  0:03:16s
epoch 28 | loss: 0.67942 | val_0_rmse: 0.83444 | val_1_rmse: 0.92061 |  0:03:23s
epoch 29 | loss: 0.67302 | val_0_rmse: 0.82804 | val_1_rmse: 0.94665 |  0:03:30s
epoch 30 | loss: 0.67065 | val_0_rmse: 0.82119 | val_1_rmse: 0.93816 |  0:03:37s
epoch 31 | loss: 0.66438 | val_0_rmse: 0.81998 | val_1_rmse: 0.94051 |  0:03:44s
epoch 32 | loss: 0.66115 | val_0_rmse: 0.95683 | val_1_rmse: 0.93529 |  0:03:51s
epoch 33 | loss: 0.65968 | val_0_rmse: 1.31103 | val_1_rmse: 0.94136 |  0:03:58s
epoch 34 | loss: 0.65142 | val_0_rmse: 0.8219  | val_1_rmse: 0.91831 |  0:04:05s
epoch 35 | loss: 0.65631 | val_0_rmse: 0.8123  | val_1_rmse: 0.92375 |  0:04:12s
epoch 36 | loss: 0.64761 | val_0_rmse: 0.81614 | val_1_rmse: 0.92433 |  0:04:19s
epoch 37 | loss: 0.64272 | val_0_rmse: 0.82279 | val_1_rmse: 0.94843 |  0:04:26s
epoch 38 | loss: 0.63801 | val_0_rmse: 0.81467 | val_1_rmse: 0.95059 |  0:04:33s
epoch 39 | loss: 0.63168 | val_0_rmse: 0.80829 | val_1_rmse: 0.94161 |  0:04:40s
epoch 40 | loss: 0.6314  | val_0_rmse: 0.8017  | val_1_rmse: 0.9458  |  0:04:47s
epoch 41 | loss: 0.63059 | val_0_rmse: 0.79853 | val_1_rmse: 0.93475 |  0:04:54s
epoch 42 | loss: 0.62645 | val_0_rmse: 0.80549 | val_1_rmse: 0.95472 |  0:05:01s
epoch 43 | loss: 0.62791 | val_0_rmse: 0.83563 | val_1_rmse: 0.9595  |  0:05:08s
epoch 44 | loss: 0.62387 | val_0_rmse: 0.83618 | val_1_rmse: 0.95877 |  0:05:15s
epoch 45 | loss: 0.62233 | val_0_rmse: 0.80415 | val_1_rmse: 0.92176 |  0:05:22s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.90559
Best weights from best epoch are automatically used!
ended training at: 13:27:00
Feature importance:
Mean squared error is of 0.06582927381607838
Mean absolute error:0.1892308128611055
MAPE:0.20063794324596215
R2 score:0.1540827428307434
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:27:02
epoch 0  | loss: 1.16733 | val_0_rmse: 0.99724 | val_1_rmse: 1.00532 |  0:00:06s
epoch 1  | loss: 0.98356 | val_0_rmse: 0.95803 | val_1_rmse: 0.96299 |  0:00:13s
epoch 2  | loss: 0.92597 | val_0_rmse: 0.94358 | val_1_rmse: 0.94844 |  0:00:20s
epoch 3  | loss: 0.86705 | val_0_rmse: 0.93027 | val_1_rmse: 0.93358 |  0:00:27s
epoch 4  | loss: 0.84867 | val_0_rmse: 0.92686 | val_1_rmse: 0.93148 |  0:00:34s
epoch 5  | loss: 0.82671 | val_0_rmse: 0.91895 | val_1_rmse: 0.9253  |  0:00:41s
epoch 6  | loss: 0.80924 | val_0_rmse: 0.92359 | val_1_rmse: 0.93035 |  0:00:48s
epoch 7  | loss: 0.79439 | val_0_rmse: 0.9069  | val_1_rmse: 0.9188  |  0:00:54s
epoch 8  | loss: 0.79443 | val_0_rmse: 0.90267 | val_1_rmse: 0.91483 |  0:01:02s
epoch 9  | loss: 0.78626 | val_0_rmse: 0.88486 | val_1_rmse: 0.90501 |  0:01:09s
epoch 10 | loss: 0.77455 | val_0_rmse: 0.87432 | val_1_rmse: 0.89861 |  0:01:16s
epoch 11 | loss: 0.76438 | val_0_rmse: 0.86764 | val_1_rmse: 0.89965 |  0:01:23s
epoch 12 | loss: 0.76324 | val_0_rmse: 0.91662 | val_1_rmse: 0.93814 |  0:01:30s
epoch 13 | loss: 0.76262 | val_0_rmse: 0.95439 | val_1_rmse: 0.97214 |  0:01:37s
epoch 14 | loss: 0.75905 | val_0_rmse: 0.85239 | val_1_rmse: 0.89206 |  0:01:44s
epoch 15 | loss: 0.75881 | val_0_rmse: 0.85901 | val_1_rmse: 0.90649 |  0:01:51s
epoch 16 | loss: 0.74652 | val_0_rmse: 0.85176 | val_1_rmse: 0.91858 |  0:01:58s
epoch 17 | loss: 0.74033 | val_0_rmse: 0.85872 | val_1_rmse: 0.9057  |  0:02:05s
epoch 18 | loss: 0.73702 | val_0_rmse: 0.84651 | val_1_rmse: 0.90394 |  0:02:12s
epoch 19 | loss: 0.73149 | val_0_rmse: 0.86126 | val_1_rmse: 0.91026 |  0:02:19s
epoch 20 | loss: 0.73477 | val_0_rmse: 0.8447  | val_1_rmse: 0.90239 |  0:02:26s
epoch 21 | loss: 0.72094 | val_0_rmse: 0.84063 | val_1_rmse: 0.90562 |  0:02:33s
epoch 22 | loss: 0.71414 | val_0_rmse: 0.84348 | val_1_rmse: 0.90641 |  0:02:40s
epoch 23 | loss: 0.71231 | val_0_rmse: 0.84916 | val_1_rmse: 0.92274 |  0:02:47s
epoch 24 | loss: 0.70802 | val_0_rmse: 0.86236 | val_1_rmse: 0.9447  |  0:02:54s
epoch 25 | loss: 0.70208 | val_0_rmse: 0.86721 | val_1_rmse: 0.98625 |  0:03:01s
epoch 26 | loss: 0.69779 | val_0_rmse: 0.89982 | val_1_rmse: 1.03307 |  0:03:08s
epoch 27 | loss: 0.69384 | val_0_rmse: 0.83757 | val_1_rmse: 0.91266 |  0:03:15s
epoch 28 | loss: 0.69289 | val_0_rmse: 0.88289 | val_1_rmse: 1.00258 |  0:03:22s
epoch 29 | loss: 0.69069 | val_0_rmse: 0.8787  | val_1_rmse: 1.16253 |  0:03:29s
epoch 30 | loss: 0.68444 | val_0_rmse: 0.8386  | val_1_rmse: 0.91502 |  0:03:37s
epoch 31 | loss: 0.68093 | val_0_rmse: 0.8282  | val_1_rmse: 0.90761 |  0:03:44s
epoch 32 | loss: 0.679   | val_0_rmse: 0.89378 | val_1_rmse: 0.90671 |  0:03:51s
epoch 33 | loss: 0.67409 | val_0_rmse: 0.83175 | val_1_rmse: 0.92916 |  0:03:58s
epoch 34 | loss: 0.67163 | val_0_rmse: 0.82038 | val_1_rmse: 0.90117 |  0:04:05s
epoch 35 | loss: 0.66679 | val_0_rmse: 0.82087 | val_1_rmse: 0.90537 |  0:04:12s
epoch 36 | loss: 0.66171 | val_0_rmse: 0.82116 | val_1_rmse: 0.91615 |  0:04:19s
epoch 37 | loss: 0.6606  | val_0_rmse: 0.81342 | val_1_rmse: 0.90922 |  0:04:26s
epoch 38 | loss: 0.65706 | val_0_rmse: 0.81954 | val_1_rmse: 0.91647 |  0:04:33s
epoch 39 | loss: 0.65887 | val_0_rmse: 0.83439 | val_1_rmse: 0.90461 |  0:04:40s
epoch 40 | loss: 0.66681 | val_0_rmse: 1.2209  | val_1_rmse: 0.91459 |  0:04:47s
epoch 41 | loss: 0.66024 | val_0_rmse: 0.89915 | val_1_rmse: 0.95054 |  0:04:54s
epoch 42 | loss: 0.66805 | val_0_rmse: 0.83502 | val_1_rmse: 0.91193 |  0:05:01s
epoch 43 | loss: 0.65611 | val_0_rmse: 0.9087  | val_1_rmse: 0.98316 |  0:05:08s
epoch 44 | loss: 0.64742 | val_0_rmse: 0.81147 | val_1_rmse: 0.92728 |  0:05:15s

Early stopping occured at epoch 44 with best_epoch = 14 and best_val_1_rmse = 0.89206
Best weights from best epoch are automatically used!
ended training at: 13:32:20
Feature importance:
Mean squared error is of 0.06937206905936716
Mean absolute error:0.1871689429730395
MAPE:0.20132724223734066
R2 score:0.16629179925921866
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:32:22
epoch 0  | loss: 1.14533 | val_0_rmse: 0.99792 | val_1_rmse: 0.99718 |  0:00:07s
epoch 1  | loss: 0.99789 | val_0_rmse: 0.99606 | val_1_rmse: 0.99466 |  0:00:14s
epoch 2  | loss: 0.9817  | val_0_rmse: 0.96402 | val_1_rmse: 0.96208 |  0:00:21s
epoch 3  | loss: 0.90524 | val_0_rmse: 0.93479 | val_1_rmse: 0.93309 |  0:00:28s
epoch 4  | loss: 0.86167 | val_0_rmse: 0.93065 | val_1_rmse: 0.93112 |  0:00:35s
epoch 5  | loss: 0.8414  | val_0_rmse: 0.92545 | val_1_rmse: 0.93076 |  0:00:42s
epoch 6  | loss: 0.82622 | val_0_rmse: 0.91758 | val_1_rmse: 0.9234  |  0:00:49s
epoch 7  | loss: 0.80407 | val_0_rmse: 0.90717 | val_1_rmse: 0.91605 |  0:00:56s
epoch 8  | loss: 0.79949 | val_0_rmse: 0.9079  | val_1_rmse: 0.91936 |  0:01:03s
epoch 9  | loss: 0.78806 | val_0_rmse: 0.89412 | val_1_rmse: 0.91318 |  0:01:10s
epoch 10 | loss: 0.78549 | val_0_rmse: 0.88328 | val_1_rmse: 0.903   |  0:01:17s
epoch 11 | loss: 0.77457 | val_0_rmse: 0.87423 | val_1_rmse: 0.90972 |  0:01:25s
epoch 12 | loss: 0.7635  | val_0_rmse: 0.86352 | val_1_rmse: 0.90233 |  0:01:32s
epoch 13 | loss: 0.75911 | val_0_rmse: 0.8564  | val_1_rmse: 0.89933 |  0:01:39s
epoch 14 | loss: 0.75091 | val_0_rmse: 0.85357 | val_1_rmse: 0.90228 |  0:01:46s
epoch 15 | loss: 0.74428 | val_0_rmse: 0.85777 | val_1_rmse: 0.90407 |  0:01:53s
epoch 16 | loss: 0.73676 | val_0_rmse: 0.84563 | val_1_rmse: 0.90703 |  0:02:00s
epoch 17 | loss: 0.73198 | val_0_rmse: 0.85089 | val_1_rmse: 0.9032  |  0:02:07s
epoch 18 | loss: 0.72582 | val_0_rmse: 0.93946 | val_1_rmse: 1.20996 |  0:02:14s
epoch 19 | loss: 0.72186 | val_0_rmse: 1.09374 | val_1_rmse: 1.29112 |  0:02:21s
epoch 20 | loss: 0.72323 | val_0_rmse: 0.84883 | val_1_rmse: 0.91334 |  0:02:28s
epoch 21 | loss: 0.71563 | val_0_rmse: 0.84306 | val_1_rmse: 0.91309 |  0:02:35s
epoch 22 | loss: 0.71183 | val_0_rmse: 0.83725 | val_1_rmse: 0.90854 |  0:02:43s
epoch 23 | loss: 0.70457 | val_0_rmse: 0.91158 | val_1_rmse: 0.94524 |  0:02:50s
epoch 24 | loss: 0.69895 | val_0_rmse: 0.84086 | val_1_rmse: 0.94315 |  0:02:57s
epoch 25 | loss: 0.69613 | val_0_rmse: 0.86551 | val_1_rmse: 0.94608 |  0:03:04s
epoch 26 | loss: 0.69228 | val_0_rmse: 0.9228  | val_1_rmse: 1.21299 |  0:03:11s
epoch 27 | loss: 0.69032 | val_0_rmse: 0.96084 | val_1_rmse: 1.29054 |  0:03:18s
epoch 28 | loss: 0.68911 | val_0_rmse: 0.87005 | val_1_rmse: 0.92693 |  0:03:25s
epoch 29 | loss: 0.68523 | val_0_rmse: 0.87771 | val_1_rmse: 0.90548 |  0:03:32s
epoch 30 | loss: 0.6805  | val_0_rmse: 0.82162 | val_1_rmse: 0.92159 |  0:03:39s
epoch 31 | loss: 0.67846 | val_0_rmse: 0.82224 | val_1_rmse: 0.90414 |  0:03:46s
epoch 32 | loss: 0.67736 | val_0_rmse: 0.82236 | val_1_rmse: 0.90977 |  0:03:53s
epoch 33 | loss: 0.67253 | val_0_rmse: 0.82199 | val_1_rmse: 0.92492 |  0:04:01s
epoch 34 | loss: 0.66933 | val_0_rmse: 0.82726 | val_1_rmse: 0.90775 |  0:04:08s
epoch 35 | loss: 0.66857 | val_0_rmse: 0.8168  | val_1_rmse: 0.92315 |  0:04:15s
epoch 36 | loss: 0.6652  | val_0_rmse: 0.81449 | val_1_rmse: 0.91124 |  0:04:22s
epoch 37 | loss: 0.65849 | val_0_rmse: 0.81565 | val_1_rmse: 0.91607 |  0:04:29s
epoch 38 | loss: 0.65442 | val_0_rmse: 0.8242  | val_1_rmse: 0.91557 |  0:04:36s
epoch 39 | loss: 0.65407 | val_0_rmse: 0.81251 | val_1_rmse: 0.90817 |  0:04:43s
epoch 40 | loss: 0.65363 | val_0_rmse: 0.80489 | val_1_rmse: 0.90742 |  0:04:50s
epoch 41 | loss: 0.65298 | val_0_rmse: 0.82548 | val_1_rmse: 0.95379 |  0:04:57s
epoch 42 | loss: 0.64867 | val_0_rmse: 0.81454 | val_1_rmse: 0.94557 |  0:05:04s
epoch 43 | loss: 0.64812 | val_0_rmse: 0.80351 | val_1_rmse: 0.92739 |  0:05:11s

Early stopping occured at epoch 43 with best_epoch = 13 and best_val_1_rmse = 0.89933
Best weights from best epoch are automatically used!
ended training at: 13:37:37
Feature importance:
Mean squared error is of 0.06690537254030388
Mean absolute error:0.18878073470321494
MAPE:0.2049390671391042
R2 score:0.1621385661079222
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:37:39
epoch 0  | loss: 1.19345 | val_0_rmse: 1.00041 | val_1_rmse: 0.9966  |  0:00:06s
epoch 1  | loss: 1.0034  | val_0_rmse: 1.0007  | val_1_rmse: 0.99678 |  0:00:13s
epoch 2  | loss: 0.98929 | val_0_rmse: 0.9898  | val_1_rmse: 0.99259 |  0:00:20s
epoch 3  | loss: 0.94302 | val_0_rmse: 0.96212 | val_1_rmse: 0.96088 |  0:00:27s
epoch 4  | loss: 0.90635 | val_0_rmse: 0.95077 | val_1_rmse: 0.95107 |  0:00:34s
epoch 5  | loss: 0.87864 | val_0_rmse: 0.93117 | val_1_rmse: 0.93389 |  0:00:41s
epoch 6  | loss: 0.8493  | val_0_rmse: 0.92252 | val_1_rmse: 0.92735 |  0:00:48s
epoch 7  | loss: 0.83128 | val_0_rmse: 0.91477 | val_1_rmse: 0.92242 |  0:00:55s
epoch 8  | loss: 0.81704 | val_0_rmse: 0.90789 | val_1_rmse: 0.92    |  0:01:01s
epoch 9  | loss: 0.80551 | val_0_rmse: 0.90008 | val_1_rmse: 0.91338 |  0:01:08s
epoch 10 | loss: 0.7965  | val_0_rmse: 0.88654 | val_1_rmse: 0.90616 |  0:01:15s
epoch 11 | loss: 0.79016 | val_0_rmse: 0.881   | val_1_rmse: 0.90442 |  0:01:22s
epoch 12 | loss: 0.7789  | val_0_rmse: 0.87934 | val_1_rmse: 0.90484 |  0:01:29s
epoch 13 | loss: 0.76901 | val_0_rmse: 0.86377 | val_1_rmse: 0.90545 |  0:01:36s
epoch 14 | loss: 0.76716 | val_0_rmse: 0.87011 | val_1_rmse: 0.90697 |  0:01:43s
epoch 15 | loss: 0.76151 | val_0_rmse: 0.87822 | val_1_rmse: 0.91313 |  0:01:49s
epoch 16 | loss: 0.75681 | val_0_rmse: 0.86335 | val_1_rmse: 0.90309 |  0:01:56s
epoch 17 | loss: 0.74744 | val_0_rmse: 0.86734 | val_1_rmse: 0.91599 |  0:02:03s
epoch 18 | loss: 0.74307 | val_0_rmse: 0.85067 | val_1_rmse: 0.90864 |  0:02:10s
epoch 19 | loss: 0.73891 | val_0_rmse: 0.85046 | val_1_rmse: 0.90884 |  0:02:17s
epoch 20 | loss: 0.72985 | val_0_rmse: 0.85694 | val_1_rmse: 0.92303 |  0:02:24s
epoch 21 | loss: 0.72462 | val_0_rmse: 0.84364 | val_1_rmse: 0.9067  |  0:02:31s
epoch 22 | loss: 0.71805 | val_0_rmse: 0.84724 | val_1_rmse: 0.91781 |  0:02:38s
epoch 23 | loss: 0.71797 | val_0_rmse: 0.84684 | val_1_rmse: 0.91451 |  0:02:44s
epoch 24 | loss: 0.71348 | val_0_rmse: 0.85272 | val_1_rmse: 0.92271 |  0:02:51s
epoch 25 | loss: 0.70953 | val_0_rmse: 0.84239 | val_1_rmse: 0.90276 |  0:02:58s
epoch 26 | loss: 0.70592 | val_0_rmse: 0.83485 | val_1_rmse: 0.90266 |  0:03:05s
epoch 27 | loss: 0.70371 | val_0_rmse: 0.83728 | val_1_rmse: 0.90198 |  0:03:12s
epoch 28 | loss: 0.69856 | val_0_rmse: 0.83511 | val_1_rmse: 0.91268 |  0:03:19s
epoch 29 | loss: 0.69148 | val_0_rmse: 0.83041 | val_1_rmse: 0.90327 |  0:03:26s
epoch 30 | loss: 0.68396 | val_0_rmse: 0.83054 | val_1_rmse: 0.90407 |  0:03:33s
epoch 31 | loss: 0.68474 | val_0_rmse: 0.82579 | val_1_rmse: 0.90572 |  0:03:39s
epoch 32 | loss: 0.67723 | val_0_rmse: 0.82649 | val_1_rmse: 0.90916 |  0:03:46s
epoch 33 | loss: 0.67693 | val_0_rmse: 0.82946 | val_1_rmse: 0.91233 |  0:03:53s
epoch 34 | loss: 0.67337 | val_0_rmse: 0.83769 | val_1_rmse: 0.93868 |  0:04:00s
epoch 35 | loss: 0.66767 | val_0_rmse: 0.82748 | val_1_rmse: 0.91035 |  0:04:07s
epoch 36 | loss: 0.66239 | val_0_rmse: 0.8169  | val_1_rmse: 0.90945 |  0:04:14s
epoch 37 | loss: 0.66406 | val_0_rmse: 0.8601  | val_1_rmse: 0.96739 |  0:04:21s
epoch 38 | loss: 0.66227 | val_0_rmse: 0.91848 | val_1_rmse: 0.90547 |  0:04:27s
epoch 39 | loss: 0.65781 | val_0_rmse: 0.81874 | val_1_rmse: 0.93131 |  0:04:34s
epoch 40 | loss: 0.6522  | val_0_rmse: 0.8146  | val_1_rmse: 0.93064 |  0:04:41s
epoch 41 | loss: 0.64889 | val_0_rmse: 0.81121 | val_1_rmse: 0.91874 |  0:04:48s
epoch 42 | loss: 0.64301 | val_0_rmse: 0.8078  | val_1_rmse: 0.91966 |  0:04:55s
epoch 43 | loss: 0.6421  | val_0_rmse: 0.81905 | val_1_rmse: 0.94715 |  0:05:02s
epoch 44 | loss: 0.6402  | val_0_rmse: 0.80491 | val_1_rmse: 0.91358 |  0:05:09s
epoch 45 | loss: 0.63627 | val_0_rmse: 0.84515 | val_1_rmse: 0.97313 |  0:05:16s
epoch 46 | loss: 0.62995 | val_0_rmse: 0.81085 | val_1_rmse: 0.94282 |  0:05:23s
epoch 47 | loss: 0.63088 | val_0_rmse: 0.80311 | val_1_rmse: 0.92933 |  0:05:30s
epoch 48 | loss: 0.62485 | val_0_rmse: 0.90077 | val_1_rmse: 0.9749  |  0:05:37s
epoch 49 | loss: 0.62611 | val_0_rmse: 0.82925 | val_1_rmse: 0.97027 |  0:05:44s
epoch 50 | loss: 0.62685 | val_0_rmse: 0.79049 | val_1_rmse: 0.91802 |  0:05:51s
epoch 51 | loss: 0.62077 | val_0_rmse: 0.7946  | val_1_rmse: 0.95657 |  0:05:58s
epoch 52 | loss: 0.61812 | val_0_rmse: 0.79992 | val_1_rmse: 0.95777 |  0:06:05s
epoch 53 | loss: 0.618   | val_0_rmse: 0.79448 | val_1_rmse: 1.34527 |  0:06:12s
epoch 54 | loss: 0.61459 | val_0_rmse: 0.79014 | val_1_rmse: 0.97802 |  0:06:19s
epoch 55 | loss: 0.61329 | val_0_rmse: 0.83825 | val_1_rmse: 0.94043 |  0:06:26s
epoch 56 | loss: 0.61404 | val_0_rmse: 0.89358 | val_1_rmse: 0.93766 |  0:06:33s
epoch 57 | loss: 0.61066 | val_0_rmse: 1.05658 | val_1_rmse: 0.95617 |  0:06:40s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.90198
Best weights from best epoch are automatically used!
ended training at: 13:44:23
Feature importance:
Mean squared error is of 0.06348854727472798
Mean absolute error:0.1880314634907767
MAPE:0.2014642160905685
R2 score:0.16097747863959733
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:44:25
epoch 0  | loss: 1.22867 | val_0_rmse: 1.00464 | val_1_rmse: 0.98797 |  0:00:07s
epoch 1  | loss: 1.01296 | val_0_rmse: 1.00553 | val_1_rmse: 0.9879  |  0:00:14s
epoch 2  | loss: 1.00362 | val_0_rmse: 0.98335 | val_1_rmse: 0.96535 |  0:00:21s
epoch 3  | loss: 0.94664 | val_0_rmse: 0.96601 | val_1_rmse: 0.95336 |  0:00:28s
epoch 4  | loss: 0.88752 | val_0_rmse: 0.93788 | val_1_rmse: 0.92367 |  0:00:35s
epoch 5  | loss: 0.85934 | val_0_rmse: 0.93762 | val_1_rmse: 0.92543 |  0:00:42s
epoch 6  | loss: 0.83855 | val_0_rmse: 0.93808 | val_1_rmse: 0.92982 |  0:00:49s
epoch 7  | loss: 0.82138 | val_0_rmse: 0.91632 | val_1_rmse: 0.90912 |  0:00:56s
epoch 8  | loss: 0.81031 | val_0_rmse: 0.91091 | val_1_rmse: 0.90663 |  0:01:03s
epoch 9  | loss: 0.80205 | val_0_rmse: 0.89629 | val_1_rmse: 0.89958 |  0:01:10s
epoch 10 | loss: 0.79166 | val_0_rmse: 0.9572  | val_1_rmse: 0.95179 |  0:01:17s
epoch 11 | loss: 0.7826  | val_0_rmse: 0.87355 | val_1_rmse: 0.89494 |  0:01:24s
epoch 12 | loss: 0.76984 | val_0_rmse: 0.86428 | val_1_rmse: 0.88617 |  0:01:31s
epoch 13 | loss: 0.76063 | val_0_rmse: 0.85763 | val_1_rmse: 0.89183 |  0:01:38s
epoch 14 | loss: 0.75692 | val_0_rmse: 0.87124 | val_1_rmse: 0.89561 |  0:01:45s
epoch 15 | loss: 0.75147 | val_0_rmse: 0.85295 | val_1_rmse: 0.88623 |  0:01:52s
epoch 16 | loss: 0.74472 | val_0_rmse: 0.8509  | val_1_rmse: 0.90811 |  0:01:59s
epoch 17 | loss: 0.73832 | val_0_rmse: 0.85116 | val_1_rmse: 0.90378 |  0:02:06s
epoch 18 | loss: 0.73194 | val_0_rmse: 0.86523 | val_1_rmse: 0.96729 |  0:02:13s
epoch 19 | loss: 0.72909 | val_0_rmse: 0.86431 | val_1_rmse: 0.89877 |  0:02:21s
epoch 20 | loss: 0.72296 | val_0_rmse: 0.89422 | val_1_rmse: 1.05731 |  0:02:28s
epoch 21 | loss: 0.71966 | val_0_rmse: 0.86197 | val_1_rmse: 0.97591 |  0:02:35s
epoch 22 | loss: 0.71455 | val_0_rmse: 0.87869 | val_1_rmse: 1.02265 |  0:02:42s
epoch 23 | loss: 0.71315 | val_0_rmse: 0.88543 | val_1_rmse: 1.04842 |  0:02:49s
epoch 24 | loss: 0.70992 | val_0_rmse: 0.9084  | val_1_rmse: 1.08058 |  0:02:56s
epoch 25 | loss: 0.69916 | val_0_rmse: 0.83473 | val_1_rmse: 0.91681 |  0:03:03s
epoch 26 | loss: 0.69727 | val_0_rmse: 0.83238 | val_1_rmse: 0.89099 |  0:03:10s
epoch 27 | loss: 0.69404 | val_0_rmse: 0.85025 | val_1_rmse: 0.90141 |  0:03:17s
epoch 28 | loss: 0.69165 | val_0_rmse: 0.83312 | val_1_rmse: 0.92665 |  0:03:24s
epoch 29 | loss: 0.6876  | val_0_rmse: 0.83074 | val_1_rmse: 0.89844 |  0:03:31s
epoch 30 | loss: 0.6842  | val_0_rmse: 0.83303 | val_1_rmse: 0.90019 |  0:03:38s
epoch 31 | loss: 0.67743 | val_0_rmse: 0.82121 | val_1_rmse: 0.91035 |  0:03:46s
epoch 32 | loss: 0.67135 | val_0_rmse: 0.81742 | val_1_rmse: 0.90213 |  0:03:53s
epoch 33 | loss: 0.67151 | val_0_rmse: 0.81604 | val_1_rmse: 0.98027 |  0:04:00s
epoch 34 | loss: 0.66632 | val_0_rmse: 0.81639 | val_1_rmse: 0.90533 |  0:04:07s
epoch 35 | loss: 0.66049 | val_0_rmse: 0.81354 | val_1_rmse: 0.90859 |  0:04:14s
epoch 36 | loss: 0.66034 | val_0_rmse: 0.81234 | val_1_rmse: 0.90844 |  0:04:21s
epoch 37 | loss: 0.65629 | val_0_rmse: 0.82451 | val_1_rmse: 0.90297 |  0:04:28s
epoch 38 | loss: 0.65323 | val_0_rmse: 0.81522 | val_1_rmse: 1.05728 |  0:04:35s
epoch 39 | loss: 0.65287 | val_0_rmse: 0.82937 | val_1_rmse: 0.92109 |  0:04:42s
epoch 40 | loss: 0.65005 | val_0_rmse: 0.80415 | val_1_rmse: 0.90593 |  0:04:49s
epoch 41 | loss: 0.64643 | val_0_rmse: 0.80313 | val_1_rmse: 0.89605 |  0:04:56s
epoch 42 | loss: 0.6419  | val_0_rmse: 0.80481 | val_1_rmse: 0.92248 |  0:05:03s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.88617
Best weights from best epoch are automatically used!
ended training at: 13:49:32
Feature importance:
Mean squared error is of 0.06508883307816936
Mean absolute error:0.18506060545558836
MAPE:0.2000340699976219
R2 score:0.17000777240490972
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:49:35
epoch 0  | loss: 2.65027 | val_0_rmse: 0.99737 | val_1_rmse: 1.02086 |  0:00:01s
epoch 1  | loss: 1.03767 | val_0_rmse: 0.99527 | val_1_rmse: 1.01951 |  0:00:02s
epoch 2  | loss: 0.9709  | val_0_rmse: 0.98228 | val_1_rmse: 1.00836 |  0:00:03s
epoch 3  | loss: 0.94711 | val_0_rmse: 0.98685 | val_1_rmse: 1.01229 |  0:00:04s
epoch 4  | loss: 0.9313  | val_0_rmse: 0.99006 | val_1_rmse: 1.01376 |  0:00:05s
epoch 5  | loss: 0.93118 | val_0_rmse: 0.98695 | val_1_rmse: 1.01302 |  0:00:06s
epoch 6  | loss: 0.91392 | val_0_rmse: 0.98222 | val_1_rmse: 1.00686 |  0:00:07s
epoch 7  | loss: 0.9084  | val_0_rmse: 0.96831 | val_1_rmse: 0.99227 |  0:00:08s
epoch 8  | loss: 0.89813 | val_0_rmse: 0.96691 | val_1_rmse: 0.98997 |  0:00:09s
epoch 9  | loss: 0.87308 | val_0_rmse: 0.95477 | val_1_rmse: 0.97723 |  0:00:10s
epoch 10 | loss: 0.8565  | val_0_rmse: 0.94054 | val_1_rmse: 0.96048 |  0:00:11s
epoch 11 | loss: 0.84176 | val_0_rmse: 0.9423  | val_1_rmse: 0.96193 |  0:00:12s
epoch 12 | loss: 0.83266 | val_0_rmse: 0.92791 | val_1_rmse: 0.94733 |  0:00:13s
epoch 13 | loss: 0.81633 | val_0_rmse: 0.9034  | val_1_rmse: 0.92154 |  0:00:14s
epoch 14 | loss: 0.80306 | val_0_rmse: 0.90158 | val_1_rmse: 0.92133 |  0:00:15s
epoch 15 | loss: 0.78999 | val_0_rmse: 0.9011  | val_1_rmse: 0.9218  |  0:00:16s
epoch 16 | loss: 0.78133 | val_0_rmse: 0.90363 | val_1_rmse: 0.92533 |  0:00:17s
epoch 17 | loss: 0.76982 | val_0_rmse: 0.9051  | val_1_rmse: 0.92772 |  0:00:18s
epoch 18 | loss: 0.76224 | val_0_rmse: 0.88928 | val_1_rmse: 0.91142 |  0:00:19s
epoch 19 | loss: 0.75906 | val_0_rmse: 0.88752 | val_1_rmse: 0.90983 |  0:00:20s
epoch 20 | loss: 0.75407 | val_0_rmse: 0.8822  | val_1_rmse: 0.9052  |  0:00:21s
epoch 21 | loss: 0.76305 | val_0_rmse: 0.88999 | val_1_rmse: 0.91324 |  0:00:22s
epoch 22 | loss: 0.74928 | val_0_rmse: 0.88063 | val_1_rmse: 0.90594 |  0:00:23s
epoch 23 | loss: 0.74287 | val_0_rmse: 0.87809 | val_1_rmse: 0.90399 |  0:00:24s
epoch 24 | loss: 0.73755 | val_0_rmse: 0.87151 | val_1_rmse: 0.89651 |  0:00:26s
epoch 25 | loss: 0.73294 | val_0_rmse: 0.87581 | val_1_rmse: 0.90035 |  0:00:27s
epoch 26 | loss: 0.73013 | val_0_rmse: 0.86926 | val_1_rmse: 0.89737 |  0:00:28s
epoch 27 | loss: 0.72458 | val_0_rmse: 0.86503 | val_1_rmse: 0.89541 |  0:00:29s
epoch 28 | loss: 0.71807 | val_0_rmse: 0.8599  | val_1_rmse: 0.89221 |  0:00:30s
epoch 29 | loss: 0.71698 | val_0_rmse: 0.85892 | val_1_rmse: 0.89064 |  0:00:31s
epoch 30 | loss: 0.71556 | val_0_rmse: 0.85824 | val_1_rmse: 0.89173 |  0:00:32s
epoch 31 | loss: 0.70885 | val_0_rmse: 0.85936 | val_1_rmse: 0.89116 |  0:00:33s
epoch 32 | loss: 0.70697 | val_0_rmse: 0.8482  | val_1_rmse: 0.88288 |  0:00:34s
epoch 33 | loss: 0.69843 | val_0_rmse: 0.83946 | val_1_rmse: 0.88033 |  0:00:35s
epoch 34 | loss: 0.70055 | val_0_rmse: 0.85153 | val_1_rmse: 0.89391 |  0:00:36s
epoch 35 | loss: 0.70615 | val_0_rmse: 0.83404 | val_1_rmse: 0.8775  |  0:00:37s
epoch 36 | loss: 0.69405 | val_0_rmse: 0.83612 | val_1_rmse: 0.87811 |  0:00:38s
epoch 37 | loss: 0.68891 | val_0_rmse: 0.83176 | val_1_rmse: 0.87582 |  0:00:39s
epoch 38 | loss: 0.68509 | val_0_rmse: 0.83206 | val_1_rmse: 0.88175 |  0:00:40s
epoch 39 | loss: 0.68448 | val_0_rmse: 0.81782 | val_1_rmse: 0.8747  |  0:00:41s
epoch 40 | loss: 0.6698  | val_0_rmse: 0.81528 | val_1_rmse: 0.87378 |  0:00:42s
epoch 41 | loss: 0.67122 | val_0_rmse: 0.8166  | val_1_rmse: 0.87536 |  0:00:43s
epoch 42 | loss: 0.66334 | val_0_rmse: 0.81102 | val_1_rmse: 0.87228 |  0:00:44s
epoch 43 | loss: 0.66412 | val_0_rmse: 0.81262 | val_1_rmse: 0.88249 |  0:00:45s
epoch 44 | loss: 0.65931 | val_0_rmse: 0.81021 | val_1_rmse: 0.88402 |  0:00:46s
epoch 45 | loss: 0.66473 | val_0_rmse: 0.80806 | val_1_rmse: 0.88375 |  0:00:47s
epoch 46 | loss: 0.65965 | val_0_rmse: 0.80536 | val_1_rmse: 0.88157 |  0:00:48s
epoch 47 | loss: 0.65521 | val_0_rmse: 0.79287 | val_1_rmse: 0.87575 |  0:00:49s
epoch 48 | loss: 0.64773 | val_0_rmse: 0.79633 | val_1_rmse: 0.87422 |  0:00:50s
epoch 49 | loss: 0.64872 | val_0_rmse: 0.78867 | val_1_rmse: 0.87858 |  0:00:52s
epoch 50 | loss: 0.64531 | val_0_rmse: 0.78991 | val_1_rmse: 0.87768 |  0:00:53s
epoch 51 | loss: 0.64938 | val_0_rmse: 0.7869  | val_1_rmse: 0.87926 |  0:00:54s
epoch 52 | loss: 0.64477 | val_0_rmse: 0.7929  | val_1_rmse: 0.87601 |  0:00:55s
epoch 53 | loss: 0.64245 | val_0_rmse: 0.78699 | val_1_rmse: 0.89257 |  0:00:56s
epoch 54 | loss: 0.63245 | val_0_rmse: 0.78223 | val_1_rmse: 0.87681 |  0:00:57s
epoch 55 | loss: 0.632   | val_0_rmse: 0.77758 | val_1_rmse: 0.87647 |  0:00:58s
epoch 56 | loss: 0.63479 | val_0_rmse: 0.78615 | val_1_rmse: 0.88688 |  0:00:59s
epoch 57 | loss: 0.63334 | val_0_rmse: 0.78452 | val_1_rmse: 0.87667 |  0:01:00s
epoch 58 | loss: 0.62986 | val_0_rmse: 0.77659 | val_1_rmse: 0.88109 |  0:01:01s
epoch 59 | loss: 0.62652 | val_0_rmse: 0.77914 | val_1_rmse: 0.88067 |  0:01:02s
epoch 60 | loss: 0.62312 | val_0_rmse: 0.77923 | val_1_rmse: 0.88891 |  0:01:03s
epoch 61 | loss: 0.62346 | val_0_rmse: 0.7658  | val_1_rmse: 0.88272 |  0:01:04s
epoch 62 | loss: 0.62292 | val_0_rmse: 0.7725  | val_1_rmse: 0.88418 |  0:01:05s
epoch 63 | loss: 0.61582 | val_0_rmse: 0.76636 | val_1_rmse: 0.88292 |  0:01:06s
epoch 64 | loss: 0.62135 | val_0_rmse: 0.76894 | val_1_rmse: 0.88819 |  0:01:07s
epoch 65 | loss: 0.6147  | val_0_rmse: 0.7748  | val_1_rmse: 0.88245 |  0:01:08s
epoch 66 | loss: 0.61903 | val_0_rmse: 0.77131 | val_1_rmse: 0.89301 |  0:01:09s
epoch 67 | loss: 0.61621 | val_0_rmse: 0.7708  | val_1_rmse: 0.89692 |  0:01:10s
epoch 68 | loss: 0.61132 | val_0_rmse: 0.76383 | val_1_rmse: 0.89303 |  0:01:11s
epoch 69 | loss: 0.60852 | val_0_rmse: 0.77023 | val_1_rmse: 0.888   |  0:01:12s
epoch 70 | loss: 0.61213 | val_0_rmse: 0.76642 | val_1_rmse: 0.89221 |  0:01:13s
epoch 71 | loss: 0.60196 | val_0_rmse: 0.7936  | val_1_rmse: 0.91325 |  0:01:14s
epoch 72 | loss: 0.6109  | val_0_rmse: 0.77185 | val_1_rmse: 0.91014 |  0:01:15s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.87228
Best weights from best epoch are automatically used!
ended training at: 13:50:51
Feature importance:
Mean squared error is of 0.09087224155672927
Mean absolute error:0.18909276246077802
MAPE:0.20409777133921067
R2 score:0.20187006829140863
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:50:52
epoch 0  | loss: 2.70685 | val_0_rmse: 1.0069  | val_1_rmse: 0.97666 |  0:00:01s
epoch 1  | loss: 1.02873 | val_0_rmse: 1.00471 | val_1_rmse: 0.97392 |  0:00:02s
epoch 2  | loss: 1.00251 | val_0_rmse: 1.00262 | val_1_rmse: 0.97127 |  0:00:03s
epoch 3  | loss: 0.9802  | val_0_rmse: 0.99542 | val_1_rmse: 0.96482 |  0:00:04s
epoch 4  | loss: 0.95697 | val_0_rmse: 0.98466 | val_1_rmse: 0.95394 |  0:00:05s
epoch 5  | loss: 0.94717 | val_0_rmse: 0.97619 | val_1_rmse: 0.94478 |  0:00:06s
epoch 6  | loss: 0.92974 | val_0_rmse: 0.97045 | val_1_rmse: 0.93804 |  0:00:07s
epoch 7  | loss: 0.90496 | val_0_rmse: 0.95049 | val_1_rmse: 0.91829 |  0:00:08s
epoch 8  | loss: 0.87698 | val_0_rmse: 0.93781 | val_1_rmse: 0.90256 |  0:00:09s
epoch 9  | loss: 0.85362 | val_0_rmse: 0.94231 | val_1_rmse: 0.90856 |  0:00:10s
epoch 10 | loss: 0.85359 | val_0_rmse: 0.92506 | val_1_rmse: 0.89396 |  0:00:11s
epoch 11 | loss: 0.83141 | val_0_rmse: 0.93193 | val_1_rmse: 0.89582 |  0:00:12s
epoch 12 | loss: 0.81769 | val_0_rmse: 0.91375 | val_1_rmse: 0.87734 |  0:00:13s
epoch 13 | loss: 0.80192 | val_0_rmse: 0.91028 | val_1_rmse: 0.87669 |  0:00:14s
epoch 14 | loss: 0.79618 | val_0_rmse: 0.90701 | val_1_rmse: 0.87468 |  0:00:15s
epoch 15 | loss: 0.78908 | val_0_rmse: 0.90407 | val_1_rmse: 0.87676 |  0:00:16s
epoch 16 | loss: 0.77902 | val_0_rmse: 0.89558 | val_1_rmse: 0.86598 |  0:00:17s
epoch 17 | loss: 0.78108 | val_0_rmse: 0.91132 | val_1_rmse: 0.88131 |  0:00:18s
epoch 18 | loss: 0.77122 | val_0_rmse: 0.89619 | val_1_rmse: 0.86734 |  0:00:19s
epoch 19 | loss: 0.76602 | val_0_rmse: 0.89545 | val_1_rmse: 0.86779 |  0:00:20s
epoch 20 | loss: 0.7679  | val_0_rmse: 0.89083 | val_1_rmse: 0.86449 |  0:00:21s
epoch 21 | loss: 0.76616 | val_0_rmse: 0.88503 | val_1_rmse: 0.85869 |  0:00:22s
epoch 22 | loss: 0.75735 | val_0_rmse: 0.88883 | val_1_rmse: 0.86928 |  0:00:23s
epoch 23 | loss: 0.75445 | val_0_rmse: 0.88469 | val_1_rmse: 0.85964 |  0:00:24s
epoch 24 | loss: 0.75001 | val_0_rmse: 0.88366 | val_1_rmse: 0.86287 |  0:00:25s
epoch 25 | loss: 0.74875 | val_0_rmse: 0.88222 | val_1_rmse: 0.8651  |  0:00:26s
epoch 26 | loss: 0.74803 | val_0_rmse: 0.87683 | val_1_rmse: 0.85775 |  0:00:27s
epoch 27 | loss: 0.73229 | val_0_rmse: 0.87393 | val_1_rmse: 0.85979 |  0:00:28s
epoch 28 | loss: 0.72796 | val_0_rmse: 0.87248 | val_1_rmse: 0.86347 |  0:00:30s
epoch 29 | loss: 0.73149 | val_0_rmse: 0.8716  | val_1_rmse: 0.86532 |  0:00:31s
epoch 30 | loss: 0.7288  | val_0_rmse: 0.86769 | val_1_rmse: 0.85793 |  0:00:32s
epoch 31 | loss: 0.72585 | val_0_rmse: 0.87986 | val_1_rmse: 0.87184 |  0:00:33s
epoch 32 | loss: 0.7266  | val_0_rmse: 0.85703 | val_1_rmse: 0.84928 |  0:00:34s
epoch 33 | loss: 0.7186  | val_0_rmse: 0.85327 | val_1_rmse: 0.84982 |  0:00:35s
epoch 34 | loss: 0.71103 | val_0_rmse: 0.86087 | val_1_rmse: 0.86179 |  0:00:36s
epoch 35 | loss: 0.70955 | val_0_rmse: 0.84751 | val_1_rmse: 0.85006 |  0:00:37s
epoch 36 | loss: 0.70962 | val_0_rmse: 0.84016 | val_1_rmse: 0.84669 |  0:00:38s
epoch 37 | loss: 0.69426 | val_0_rmse: 0.83369 | val_1_rmse: 0.84533 |  0:00:39s
epoch 38 | loss: 0.6895  | val_0_rmse: 0.82689 | val_1_rmse: 0.84578 |  0:00:40s
epoch 39 | loss: 0.68853 | val_0_rmse: 0.82864 | val_1_rmse: 0.84659 |  0:00:41s
epoch 40 | loss: 0.68626 | val_0_rmse: 0.83677 | val_1_rmse: 0.85154 |  0:00:42s
epoch 41 | loss: 0.68658 | val_0_rmse: 0.82418 | val_1_rmse: 0.84969 |  0:00:43s
epoch 42 | loss: 0.67696 | val_0_rmse: 0.82136 | val_1_rmse: 0.84657 |  0:00:44s
epoch 43 | loss: 0.67752 | val_0_rmse: 0.81806 | val_1_rmse: 0.85028 |  0:00:45s
epoch 44 | loss: 0.67142 | val_0_rmse: 0.81175 | val_1_rmse: 0.84691 |  0:00:46s
epoch 45 | loss: 0.66909 | val_0_rmse: 0.81262 | val_1_rmse: 0.84828 |  0:00:47s
epoch 46 | loss: 0.66734 | val_0_rmse: 0.80322 | val_1_rmse: 0.8549  |  0:00:48s
epoch 47 | loss: 0.66367 | val_0_rmse: 0.8024  | val_1_rmse: 0.8476  |  0:00:49s
epoch 48 | loss: 0.66792 | val_0_rmse: 0.80414 | val_1_rmse: 0.8531  |  0:00:50s
epoch 49 | loss: 0.66989 | val_0_rmse: 0.80373 | val_1_rmse: 0.8482  |  0:00:51s
epoch 50 | loss: 0.66274 | val_0_rmse: 0.81279 | val_1_rmse: 0.85493 |  0:00:52s
epoch 51 | loss: 0.65666 | val_0_rmse: 0.79406 | val_1_rmse: 0.8453  |  0:00:53s
epoch 52 | loss: 0.65034 | val_0_rmse: 0.78756 | val_1_rmse: 0.85379 |  0:00:54s
epoch 53 | loss: 0.64781 | val_0_rmse: 0.79703 | val_1_rmse: 0.86283 |  0:00:55s
epoch 54 | loss: 0.66109 | val_0_rmse: 0.79617 | val_1_rmse: 0.85936 |  0:00:56s
epoch 55 | loss: 0.65597 | val_0_rmse: 0.79302 | val_1_rmse: 0.85215 |  0:00:57s
epoch 56 | loss: 0.64956 | val_0_rmse: 0.79162 | val_1_rmse: 0.8487  |  0:00:58s
epoch 57 | loss: 0.63818 | val_0_rmse: 0.78115 | val_1_rmse: 0.84301 |  0:00:59s
epoch 58 | loss: 0.63959 | val_0_rmse: 0.78192 | val_1_rmse: 0.85004 |  0:01:01s
epoch 59 | loss: 0.637   | val_0_rmse: 0.78337 | val_1_rmse: 0.86929 |  0:01:02s
epoch 60 | loss: 0.63222 | val_0_rmse: 0.78334 | val_1_rmse: 0.85155 |  0:01:03s
epoch 61 | loss: 0.6351  | val_0_rmse: 0.79506 | val_1_rmse: 0.85819 |  0:01:04s
epoch 62 | loss: 0.63633 | val_0_rmse: 0.78866 | val_1_rmse: 0.85092 |  0:01:05s
epoch 63 | loss: 0.63754 | val_0_rmse: 0.78274 | val_1_rmse: 0.86142 |  0:01:06s
epoch 64 | loss: 0.6355  | val_0_rmse: 0.7871  | val_1_rmse: 0.86219 |  0:01:07s
epoch 65 | loss: 0.63359 | val_0_rmse: 0.77887 | val_1_rmse: 0.87231 |  0:01:08s
epoch 66 | loss: 0.62126 | val_0_rmse: 0.77494 | val_1_rmse: 0.867   |  0:01:09s
epoch 67 | loss: 0.61707 | val_0_rmse: 0.77895 | val_1_rmse: 0.86697 |  0:01:10s
epoch 68 | loss: 0.6235  | val_0_rmse: 0.773   | val_1_rmse: 0.86785 |  0:01:11s
epoch 69 | loss: 0.61651 | val_0_rmse: 0.77061 | val_1_rmse: 0.86451 |  0:01:12s
epoch 70 | loss: 0.62366 | val_0_rmse: 0.78036 | val_1_rmse: 0.86373 |  0:01:13s
epoch 71 | loss: 0.62336 | val_0_rmse: 0.78454 | val_1_rmse: 0.85841 |  0:01:14s
epoch 72 | loss: 0.62504 | val_0_rmse: 0.77391 | val_1_rmse: 0.86477 |  0:01:15s
epoch 73 | loss: 0.62222 | val_0_rmse: 0.77449 | val_1_rmse: 0.8703  |  0:01:16s
epoch 74 | loss: 0.615   | val_0_rmse: 0.7686  | val_1_rmse: 0.86671 |  0:01:17s
epoch 75 | loss: 0.61326 | val_0_rmse: 0.76926 | val_1_rmse: 0.86622 |  0:01:18s
epoch 76 | loss: 0.61403 | val_0_rmse: 0.76616 | val_1_rmse: 0.85845 |  0:01:19s
epoch 77 | loss: 0.60588 | val_0_rmse: 0.76896 | val_1_rmse: 0.86855 |  0:01:20s
epoch 78 | loss: 0.61245 | val_0_rmse: 0.76316 | val_1_rmse: 0.86411 |  0:01:21s
epoch 79 | loss: 0.61662 | val_0_rmse: 0.77277 | val_1_rmse: 0.8735  |  0:01:22s
epoch 80 | loss: 0.60851 | val_0_rmse: 0.77648 | val_1_rmse: 0.85328 |  0:01:23s
epoch 81 | loss: 0.60912 | val_0_rmse: 0.77839 | val_1_rmse: 0.87974 |  0:01:24s
epoch 82 | loss: 0.61601 | val_0_rmse: 0.76659 | val_1_rmse: 0.86119 |  0:01:25s
epoch 83 | loss: 0.60809 | val_0_rmse: 0.76168 | val_1_rmse: 0.86116 |  0:01:26s
epoch 84 | loss: 0.6075  | val_0_rmse: 0.76166 | val_1_rmse: 0.88656 |  0:01:27s
epoch 85 | loss: 0.60357 | val_0_rmse: 0.76352 | val_1_rmse: 0.87033 |  0:01:28s
epoch 86 | loss: 0.59816 | val_0_rmse: 0.76205 | val_1_rmse: 0.8718  |  0:01:29s
epoch 87 | loss: 0.60344 | val_0_rmse: 0.79334 | val_1_rmse: 0.91652 |  0:01:30s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.84301
Best weights from best epoch are automatically used!
ended training at: 13:52:23
Feature importance:
Mean squared error is of 0.07125132904256519
Mean absolute error:0.19155703900001322
MAPE:0.2058184753263353
R2 score:0.2263619247139831
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:52:24
epoch 0  | loss: 2.60013 | val_0_rmse: 1.00131 | val_1_rmse: 0.99977 |  0:00:01s
epoch 1  | loss: 1.0402  | val_0_rmse: 1.0013  | val_1_rmse: 1.00029 |  0:00:02s
epoch 2  | loss: 0.98176 | val_0_rmse: 0.99841 | val_1_rmse: 0.99731 |  0:00:03s
epoch 3  | loss: 0.94778 | val_0_rmse: 0.99636 | val_1_rmse: 0.99525 |  0:00:04s
epoch 4  | loss: 0.93064 | val_0_rmse: 0.98096 | val_1_rmse: 0.9837  |  0:00:05s
epoch 5  | loss: 0.90753 | val_0_rmse: 0.96379 | val_1_rmse: 0.96873 |  0:00:06s
epoch 6  | loss: 0.88852 | val_0_rmse: 0.94207 | val_1_rmse: 0.94979 |  0:00:07s
epoch 7  | loss: 0.86608 | val_0_rmse: 0.93846 | val_1_rmse: 0.94656 |  0:00:08s
epoch 8  | loss: 0.84651 | val_0_rmse: 0.92482 | val_1_rmse: 0.93723 |  0:00:09s
epoch 9  | loss: 0.82237 | val_0_rmse: 0.92098 | val_1_rmse: 0.92885 |  0:00:10s
epoch 10 | loss: 0.80502 | val_0_rmse: 0.91008 | val_1_rmse: 0.92091 |  0:00:11s
epoch 11 | loss: 0.79456 | val_0_rmse: 0.90917 | val_1_rmse: 0.92222 |  0:00:12s
epoch 12 | loss: 0.78842 | val_0_rmse: 0.9056  | val_1_rmse: 0.91864 |  0:00:13s
epoch 13 | loss: 0.77821 | val_0_rmse: 0.90234 | val_1_rmse: 0.9175  |  0:00:14s
epoch 14 | loss: 0.77737 | val_0_rmse: 0.89447 | val_1_rmse: 0.90968 |  0:00:15s
epoch 15 | loss: 0.76297 | val_0_rmse: 0.88724 | val_1_rmse: 0.90488 |  0:00:16s
epoch 16 | loss: 0.75276 | val_0_rmse: 0.88601 | val_1_rmse: 0.90403 |  0:00:17s
epoch 17 | loss: 0.7471  | val_0_rmse: 0.88113 | val_1_rmse: 0.90505 |  0:00:18s
epoch 18 | loss: 0.74613 | val_0_rmse: 0.88248 | val_1_rmse: 0.90112 |  0:00:19s
epoch 19 | loss: 0.74431 | val_0_rmse: 0.87562 | val_1_rmse: 0.89757 |  0:00:20s
epoch 20 | loss: 0.75149 | val_0_rmse: 0.87179 | val_1_rmse: 0.89545 |  0:00:21s
epoch 21 | loss: 0.73338 | val_0_rmse: 0.87959 | val_1_rmse: 0.89977 |  0:00:22s
epoch 22 | loss: 0.73089 | val_0_rmse: 0.87107 | val_1_rmse: 0.89845 |  0:00:23s
epoch 23 | loss: 0.72555 | val_0_rmse: 0.86931 | val_1_rmse: 0.89166 |  0:00:25s
epoch 24 | loss: 0.72085 | val_0_rmse: 0.86903 | val_1_rmse: 0.89723 |  0:00:26s
epoch 25 | loss: 0.71532 | val_0_rmse: 0.86526 | val_1_rmse: 0.89301 |  0:00:27s
epoch 26 | loss: 0.71354 | val_0_rmse: 0.8569  | val_1_rmse: 0.88825 |  0:00:28s
epoch 27 | loss: 0.71872 | val_0_rmse: 0.86219 | val_1_rmse: 0.8961  |  0:00:29s
epoch 28 | loss: 0.72047 | val_0_rmse: 0.85501 | val_1_rmse: 0.88218 |  0:00:30s
epoch 29 | loss: 0.71549 | val_0_rmse: 0.85465 | val_1_rmse: 0.88649 |  0:00:31s
epoch 30 | loss: 0.71259 | val_0_rmse: 0.85861 | val_1_rmse: 0.8898  |  0:00:32s
epoch 31 | loss: 0.70978 | val_0_rmse: 0.84768 | val_1_rmse: 0.88731 |  0:00:33s
epoch 32 | loss: 0.70103 | val_0_rmse: 0.8474  | val_1_rmse: 0.87926 |  0:00:34s
epoch 33 | loss: 0.69743 | val_0_rmse: 0.83944 | val_1_rmse: 0.87934 |  0:00:35s
epoch 34 | loss: 0.69045 | val_0_rmse: 0.84119 | val_1_rmse: 0.88159 |  0:00:36s
epoch 35 | loss: 0.68992 | val_0_rmse: 0.84545 | val_1_rmse: 0.88077 |  0:00:37s
epoch 36 | loss: 0.70396 | val_0_rmse: 0.84438 | val_1_rmse: 0.89208 |  0:00:38s
epoch 37 | loss: 0.70142 | val_0_rmse: 0.83679 | val_1_rmse: 0.89285 |  0:00:39s
epoch 38 | loss: 0.68747 | val_0_rmse: 0.83755 | val_1_rmse: 0.89527 |  0:00:40s
epoch 39 | loss: 0.68155 | val_0_rmse: 0.82162 | val_1_rmse: 0.87034 |  0:00:41s
epoch 40 | loss: 0.67682 | val_0_rmse: 0.82189 | val_1_rmse: 0.87296 |  0:00:42s
epoch 41 | loss: 0.67477 | val_0_rmse: 0.82227 | val_1_rmse: 0.87297 |  0:00:43s
epoch 42 | loss: 0.67668 | val_0_rmse: 0.86114 | val_1_rmse: 0.8985  |  0:00:44s
epoch 43 | loss: 0.68784 | val_0_rmse: 0.82172 | val_1_rmse: 0.8906  |  0:00:45s
epoch 44 | loss: 0.66339 | val_0_rmse: 0.81025 | val_1_rmse: 0.88054 |  0:00:46s
epoch 45 | loss: 0.66073 | val_0_rmse: 0.80995 | val_1_rmse: 0.87665 |  0:00:47s
epoch 46 | loss: 0.66284 | val_0_rmse: 0.80765 | val_1_rmse: 0.88157 |  0:00:48s
epoch 47 | loss: 0.66471 | val_0_rmse: 0.80218 | val_1_rmse: 0.87622 |  0:00:49s
epoch 48 | loss: 0.66233 | val_0_rmse: 0.80294 | val_1_rmse: 0.87524 |  0:00:50s
epoch 49 | loss: 0.65531 | val_0_rmse: 0.80319 | val_1_rmse: 0.88288 |  0:00:52s
epoch 50 | loss: 0.65898 | val_0_rmse: 0.80774 | val_1_rmse: 0.87566 |  0:00:53s
epoch 51 | loss: 0.66534 | val_0_rmse: 0.79885 | val_1_rmse: 0.88904 |  0:00:54s
epoch 52 | loss: 0.66046 | val_0_rmse: 0.80656 | val_1_rmse: 0.87796 |  0:00:55s
epoch 53 | loss: 0.66471 | val_0_rmse: 0.80706 | val_1_rmse: 0.87999 |  0:00:56s
epoch 54 | loss: 0.67916 | val_0_rmse: 0.81925 | val_1_rmse: 0.89915 |  0:00:57s
epoch 55 | loss: 0.6747  | val_0_rmse: 0.8075  | val_1_rmse: 0.87368 |  0:00:58s
epoch 56 | loss: 0.66757 | val_0_rmse: 0.80562 | val_1_rmse: 0.87697 |  0:00:59s
epoch 57 | loss: 0.65952 | val_0_rmse: 0.80127 | val_1_rmse: 0.88004 |  0:01:00s
epoch 58 | loss: 0.6708  | val_0_rmse: 0.80296 | val_1_rmse: 0.86702 |  0:01:01s
epoch 59 | loss: 0.65551 | val_0_rmse: 0.79408 | val_1_rmse: 0.87096 |  0:01:02s
epoch 60 | loss: 0.64997 | val_0_rmse: 0.8064  | val_1_rmse: 0.89563 |  0:01:03s
epoch 61 | loss: 0.64954 | val_0_rmse: 0.79799 | val_1_rmse: 0.89518 |  0:01:04s
epoch 62 | loss: 0.65068 | val_0_rmse: 0.79538 | val_1_rmse: 0.88852 |  0:01:05s
epoch 63 | loss: 0.65374 | val_0_rmse: 0.79547 | val_1_rmse: 0.89825 |  0:01:06s
epoch 64 | loss: 0.6511  | val_0_rmse: 0.78885 | val_1_rmse: 0.86788 |  0:01:07s
epoch 65 | loss: 0.6493  | val_0_rmse: 0.79459 | val_1_rmse: 0.86638 |  0:01:08s
epoch 66 | loss: 0.64316 | val_0_rmse: 0.78668 | val_1_rmse: 0.87302 |  0:01:09s
epoch 67 | loss: 0.64375 | val_0_rmse: 0.79657 | val_1_rmse: 0.87617 |  0:01:10s
epoch 68 | loss: 0.65506 | val_0_rmse: 0.79233 | val_1_rmse: 0.86554 |  0:01:11s
epoch 69 | loss: 0.65264 | val_0_rmse: 0.78978 | val_1_rmse: 0.8726  |  0:01:12s
epoch 70 | loss: 0.64243 | val_0_rmse: 0.79033 | val_1_rmse: 0.87691 |  0:01:13s
epoch 71 | loss: 0.63964 | val_0_rmse: 0.783   | val_1_rmse: 0.86386 |  0:01:14s
epoch 72 | loss: 0.63869 | val_0_rmse: 0.79118 | val_1_rmse: 0.86922 |  0:01:15s
epoch 73 | loss: 0.63502 | val_0_rmse: 0.78628 | val_1_rmse: 0.8652  |  0:01:16s
epoch 74 | loss: 0.63765 | val_0_rmse: 0.78748 | val_1_rmse: 0.87603 |  0:01:17s
epoch 75 | loss: 0.63419 | val_0_rmse: 0.78856 | val_1_rmse: 0.8862  |  0:01:18s
epoch 76 | loss: 0.63023 | val_0_rmse: 0.81428 | val_1_rmse: 0.88576 |  0:01:20s
epoch 77 | loss: 0.63656 | val_0_rmse: 0.84335 | val_1_rmse: 0.87298 |  0:01:21s
epoch 78 | loss: 0.64301 | val_0_rmse: 0.79589 | val_1_rmse: 0.87763 |  0:01:22s
epoch 79 | loss: 0.63709 | val_0_rmse: 0.80503 | val_1_rmse: 0.87353 |  0:01:23s
epoch 80 | loss: 0.62249 | val_0_rmse: 0.79769 | val_1_rmse: 0.89006 |  0:01:24s
epoch 81 | loss: 0.61886 | val_0_rmse: 0.78698 | val_1_rmse: 0.87252 |  0:01:25s
epoch 82 | loss: 0.62562 | val_0_rmse: 0.78769 | val_1_rmse: 0.8688  |  0:01:26s
epoch 83 | loss: 0.62664 | val_0_rmse: 0.7875  | val_1_rmse: 0.87602 |  0:01:27s
epoch 84 | loss: 0.6316  | val_0_rmse: 0.78249 | val_1_rmse: 0.87515 |  0:01:28s
epoch 85 | loss: 0.62289 | val_0_rmse: 0.78229 | val_1_rmse: 0.8754  |  0:01:29s
epoch 86 | loss: 0.62665 | val_0_rmse: 0.78611 | val_1_rmse: 0.86806 |  0:01:30s
epoch 87 | loss: 0.62455 | val_0_rmse: 0.77899 | val_1_rmse: 0.8881  |  0:01:31s
epoch 88 | loss: 0.62555 | val_0_rmse: 0.77646 | val_1_rmse: 0.88033 |  0:01:32s
epoch 89 | loss: 0.61435 | val_0_rmse: 0.78024 | val_1_rmse: 0.8842  |  0:01:33s
epoch 90 | loss: 0.61756 | val_0_rmse: 0.77333 | val_1_rmse: 0.86743 |  0:01:34s
epoch 91 | loss: 0.61229 | val_0_rmse: 0.77531 | val_1_rmse: 0.87018 |  0:01:35s
epoch 92 | loss: 0.62104 | val_0_rmse: 0.77345 | val_1_rmse: 0.87683 |  0:01:36s
epoch 93 | loss: 0.6146  | val_0_rmse: 0.77558 | val_1_rmse: 0.88861 |  0:01:37s
epoch 94 | loss: 0.62481 | val_0_rmse: 0.77246 | val_1_rmse: 0.87206 |  0:01:38s
epoch 95 | loss: 0.61798 | val_0_rmse: 0.77409 | val_1_rmse: 0.88806 |  0:01:39s
epoch 96 | loss: 0.61854 | val_0_rmse: 0.77839 | val_1_rmse: 0.87565 |  0:01:40s
epoch 97 | loss: 0.61761 | val_0_rmse: 0.78081 | val_1_rmse: 0.87591 |  0:01:41s
epoch 98 | loss: 0.627   | val_0_rmse: 0.77823 | val_1_rmse: 0.89011 |  0:01:42s
epoch 99 | loss: 0.61617 | val_0_rmse: 0.76799 | val_1_rmse: 0.88408 |  0:01:43s
epoch 100| loss: 0.60723 | val_0_rmse: 0.77296 | val_1_rmse: 0.88846 |  0:01:44s
epoch 101| loss: 0.60258 | val_0_rmse: 0.76966 | val_1_rmse: 0.89324 |  0:01:45s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.86386
Best weights from best epoch are automatically used!
ended training at: 13:54:10
Feature importance:
Mean squared error is of 0.06610243573176543
Mean absolute error:0.19008524690869547
MAPE:0.2051878293555601
R2 score:0.23219569664978545
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:54:10
epoch 0  | loss: 2.63872 | val_0_rmse: 1.00378 | val_1_rmse: 0.9994  |  0:00:01s
epoch 1  | loss: 1.05429 | val_0_rmse: 1.00075 | val_1_rmse: 0.99554 |  0:00:02s
epoch 2  | loss: 0.98986 | val_0_rmse: 0.99692 | val_1_rmse: 0.9918  |  0:00:03s
epoch 3  | loss: 0.97923 | val_0_rmse: 0.99085 | val_1_rmse: 0.98835 |  0:00:04s
epoch 4  | loss: 0.95967 | val_0_rmse: 0.9829  | val_1_rmse: 0.98222 |  0:00:05s
epoch 5  | loss: 0.95171 | val_0_rmse: 0.97526 | val_1_rmse: 0.96864 |  0:00:06s
epoch 6  | loss: 0.93746 | val_0_rmse: 0.96908 | val_1_rmse: 0.96927 |  0:00:07s
epoch 7  | loss: 0.90918 | val_0_rmse: 0.97505 | val_1_rmse: 0.97481 |  0:00:08s
epoch 8  | loss: 0.89222 | val_0_rmse: 0.95375 | val_1_rmse: 0.95183 |  0:00:09s
epoch 9  | loss: 0.86992 | val_0_rmse: 0.93514 | val_1_rmse: 0.93123 |  0:00:10s
epoch 10 | loss: 0.84926 | val_0_rmse: 0.92834 | val_1_rmse: 0.92326 |  0:00:11s
epoch 11 | loss: 0.84027 | val_0_rmse: 0.92065 | val_1_rmse: 0.91573 |  0:00:12s
epoch 12 | loss: 0.81583 | val_0_rmse: 0.92076 | val_1_rmse: 0.91634 |  0:00:13s
epoch 13 | loss: 0.80403 | val_0_rmse: 0.91008 | val_1_rmse: 0.9055  |  0:00:14s
epoch 14 | loss: 0.80027 | val_0_rmse: 0.91085 | val_1_rmse: 0.90482 |  0:00:15s
epoch 15 | loss: 0.7882  | val_0_rmse: 0.91032 | val_1_rmse: 0.90581 |  0:00:16s
epoch 16 | loss: 0.77567 | val_0_rmse: 0.89117 | val_1_rmse: 0.88476 |  0:00:17s
epoch 17 | loss: 0.76767 | val_0_rmse: 0.88924 | val_1_rmse: 0.88163 |  0:00:18s
epoch 18 | loss: 0.76426 | val_0_rmse: 0.89108 | val_1_rmse: 0.88043 |  0:00:19s
epoch 19 | loss: 0.75875 | val_0_rmse: 0.88816 | val_1_rmse: 0.87858 |  0:00:20s
epoch 20 | loss: 0.75907 | val_0_rmse: 0.88394 | val_1_rmse: 0.87489 |  0:00:21s
epoch 21 | loss: 0.74757 | val_0_rmse: 0.88552 | val_1_rmse: 0.87599 |  0:00:22s
epoch 22 | loss: 0.74269 | val_0_rmse: 0.88629 | val_1_rmse: 0.87983 |  0:00:24s
epoch 23 | loss: 0.74346 | val_0_rmse: 0.88071 | val_1_rmse: 0.8731  |  0:00:25s
epoch 24 | loss: 0.73871 | val_0_rmse: 0.87905 | val_1_rmse: 0.87253 |  0:00:26s
epoch 25 | loss: 0.73186 | val_0_rmse: 0.88144 | val_1_rmse: 0.88047 |  0:00:27s
epoch 26 | loss: 0.72863 | val_0_rmse: 0.86971 | val_1_rmse: 0.8629  |  0:00:28s
epoch 27 | loss: 0.72594 | val_0_rmse: 0.86728 | val_1_rmse: 0.86589 |  0:00:29s
epoch 28 | loss: 0.72418 | val_0_rmse: 0.86612 | val_1_rmse: 0.86575 |  0:00:30s
epoch 29 | loss: 0.72373 | val_0_rmse: 0.86223 | val_1_rmse: 0.86398 |  0:00:31s
epoch 30 | loss: 0.72088 | val_0_rmse: 0.86253 | val_1_rmse: 0.867   |  0:00:32s
epoch 31 | loss: 0.72332 | val_0_rmse: 0.87224 | val_1_rmse: 0.87705 |  0:00:33s
epoch 32 | loss: 0.7282  | val_0_rmse: 0.85587 | val_1_rmse: 0.86145 |  0:00:34s
epoch 33 | loss: 0.71772 | val_0_rmse: 0.85428 | val_1_rmse: 0.86289 |  0:00:35s
epoch 34 | loss: 0.71324 | val_0_rmse: 0.85231 | val_1_rmse: 0.86244 |  0:00:36s
epoch 35 | loss: 0.71188 | val_0_rmse: 0.84924 | val_1_rmse: 0.85782 |  0:00:37s
epoch 36 | loss: 0.71067 | val_0_rmse: 0.85391 | val_1_rmse: 0.86274 |  0:00:38s
epoch 37 | loss: 0.70611 | val_0_rmse: 0.84143 | val_1_rmse: 0.85466 |  0:00:39s
epoch 38 | loss: 0.70616 | val_0_rmse: 0.84707 | val_1_rmse: 0.86142 |  0:00:40s
epoch 39 | loss: 0.70211 | val_0_rmse: 0.84584 | val_1_rmse: 0.86854 |  0:00:41s
epoch 40 | loss: 0.71246 | val_0_rmse: 0.83433 | val_1_rmse: 0.85395 |  0:00:42s
epoch 41 | loss: 0.70068 | val_0_rmse: 0.82725 | val_1_rmse: 0.84117 |  0:00:43s
epoch 42 | loss: 0.69905 | val_0_rmse: 0.83213 | val_1_rmse: 0.84597 |  0:00:44s
epoch 43 | loss: 0.69176 | val_0_rmse: 0.82886 | val_1_rmse: 0.84562 |  0:00:45s
epoch 44 | loss: 0.69189 | val_0_rmse: 0.8256  | val_1_rmse: 0.83903 |  0:00:46s
epoch 45 | loss: 0.68757 | val_0_rmse: 0.82183 | val_1_rmse: 0.84407 |  0:00:47s
epoch 46 | loss: 0.68444 | val_0_rmse: 0.8233  | val_1_rmse: 0.84352 |  0:00:48s
epoch 47 | loss: 0.68703 | val_0_rmse: 0.81656 | val_1_rmse: 0.84398 |  0:00:50s
epoch 48 | loss: 0.68681 | val_0_rmse: 0.82052 | val_1_rmse: 0.85213 |  0:00:51s
epoch 49 | loss: 0.68555 | val_0_rmse: 0.81916 | val_1_rmse: 0.85301 |  0:00:52s
epoch 50 | loss: 0.69197 | val_0_rmse: 0.81947 | val_1_rmse: 0.84745 |  0:00:53s
epoch 51 | loss: 0.68087 | val_0_rmse: 0.81232 | val_1_rmse: 0.84436 |  0:00:54s
epoch 52 | loss: 0.67872 | val_0_rmse: 0.81111 | val_1_rmse: 0.8413  |  0:00:55s
epoch 53 | loss: 0.6726  | val_0_rmse: 0.81213 | val_1_rmse: 0.8568  |  0:00:56s
epoch 54 | loss: 0.66696 | val_0_rmse: 0.80473 | val_1_rmse: 0.84888 |  0:00:57s
epoch 55 | loss: 0.66515 | val_0_rmse: 0.80345 | val_1_rmse: 0.85347 |  0:00:58s
epoch 56 | loss: 0.66353 | val_0_rmse: 0.79707 | val_1_rmse: 0.84881 |  0:00:59s
epoch 57 | loss: 0.66586 | val_0_rmse: 0.80048 | val_1_rmse: 0.85439 |  0:01:00s
epoch 58 | loss: 0.66393 | val_0_rmse: 0.79703 | val_1_rmse: 0.84994 |  0:01:01s
epoch 59 | loss: 0.65657 | val_0_rmse: 0.79591 | val_1_rmse: 0.85086 |  0:01:02s
epoch 60 | loss: 0.65348 | val_0_rmse: 0.79827 | val_1_rmse: 0.85095 |  0:01:03s
epoch 61 | loss: 0.6519  | val_0_rmse: 0.7879  | val_1_rmse: 0.8555  |  0:01:04s
epoch 62 | loss: 0.64727 | val_0_rmse: 0.79849 | val_1_rmse: 0.85277 |  0:01:05s
epoch 63 | loss: 0.66925 | val_0_rmse: 0.79861 | val_1_rmse: 0.85197 |  0:01:06s
epoch 64 | loss: 0.66388 | val_0_rmse: 0.79459 | val_1_rmse: 0.85868 |  0:01:07s
epoch 65 | loss: 0.6536  | val_0_rmse: 0.80969 | val_1_rmse: 0.86101 |  0:01:08s
epoch 66 | loss: 0.65645 | val_0_rmse: 0.79258 | val_1_rmse: 0.84366 |  0:01:09s
epoch 67 | loss: 0.65103 | val_0_rmse: 0.80086 | val_1_rmse: 0.85129 |  0:01:10s
epoch 68 | loss: 0.65958 | val_0_rmse: 0.8029  | val_1_rmse: 0.85423 |  0:01:11s
epoch 69 | loss: 0.65226 | val_0_rmse: 0.79035 | val_1_rmse: 0.85493 |  0:01:12s
epoch 70 | loss: 0.65923 | val_0_rmse: 0.81539 | val_1_rmse: 0.89459 |  0:01:13s
epoch 71 | loss: 0.65083 | val_0_rmse: 0.80865 | val_1_rmse: 0.88592 |  0:01:14s
epoch 72 | loss: 0.64514 | val_0_rmse: 0.79075 | val_1_rmse: 0.8803  |  0:01:15s
epoch 73 | loss: 0.6398  | val_0_rmse: 0.78521 | val_1_rmse: 0.86641 |  0:01:16s
epoch 74 | loss: 0.64321 | val_0_rmse: 0.78777 | val_1_rmse: 0.87197 |  0:01:18s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.83903
Best weights from best epoch are automatically used!
ended training at: 13:55:29
Feature importance:
Mean squared error is of 0.07114395335235897
Mean absolute error:0.18967632941474194
MAPE:0.2030735946699471
R2 score:0.24385080406733173
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:55:29
epoch 0  | loss: 2.56563 | val_0_rmse: 0.99167 | val_1_rmse: 1.00861 |  0:00:01s
epoch 1  | loss: 1.00756 | val_0_rmse: 0.98945 | val_1_rmse: 1.00636 |  0:00:02s
epoch 2  | loss: 0.95852 | val_0_rmse: 0.97165 | val_1_rmse: 0.99446 |  0:00:03s
epoch 3  | loss: 0.90333 | val_0_rmse: 0.93697 | val_1_rmse: 0.96193 |  0:00:04s
epoch 4  | loss: 0.85504 | val_0_rmse: 0.941   | val_1_rmse: 0.96279 |  0:00:05s
epoch 5  | loss: 0.84395 | val_0_rmse: 0.92966 | val_1_rmse: 0.94753 |  0:00:06s
epoch 6  | loss: 0.82998 | val_0_rmse: 0.93799 | val_1_rmse: 0.95599 |  0:00:07s
epoch 7  | loss: 0.83343 | val_0_rmse: 0.94316 | val_1_rmse: 0.95764 |  0:00:08s
epoch 8  | loss: 0.81672 | val_0_rmse: 0.93087 | val_1_rmse: 0.9449  |  0:00:09s
epoch 9  | loss: 0.81116 | val_0_rmse: 0.9206  | val_1_rmse: 0.93883 |  0:00:10s
epoch 10 | loss: 0.81017 | val_0_rmse: 0.9315  | val_1_rmse: 0.93952 |  0:00:11s
epoch 11 | loss: 0.80578 | val_0_rmse: 0.91827 | val_1_rmse: 0.93514 |  0:00:12s
epoch 12 | loss: 0.80229 | val_0_rmse: 0.91436 | val_1_rmse: 0.92858 |  0:00:13s
epoch 13 | loss: 0.78751 | val_0_rmse: 0.91232 | val_1_rmse: 0.92902 |  0:00:14s
epoch 14 | loss: 0.78376 | val_0_rmse: 0.92583 | val_1_rmse: 0.93658 |  0:00:15s
epoch 15 | loss: 0.78289 | val_0_rmse: 0.90583 | val_1_rmse: 0.92588 |  0:00:16s
epoch 16 | loss: 0.77747 | val_0_rmse: 0.90032 | val_1_rmse: 0.92097 |  0:00:17s
epoch 17 | loss: 0.77395 | val_0_rmse: 0.89896 | val_1_rmse: 0.922   |  0:00:18s
epoch 18 | loss: 0.77405 | val_0_rmse: 0.9075  | val_1_rmse: 0.92148 |  0:00:19s
epoch 19 | loss: 0.7731  | val_0_rmse: 0.90084 | val_1_rmse: 0.92085 |  0:00:20s
epoch 20 | loss: 0.7675  | val_0_rmse: 0.90626 | val_1_rmse: 0.93323 |  0:00:21s
epoch 21 | loss: 0.76627 | val_0_rmse: 0.90125 | val_1_rmse: 0.92041 |  0:00:22s
epoch 22 | loss: 0.76467 | val_0_rmse: 0.89635 | val_1_rmse: 0.91338 |  0:00:23s
epoch 23 | loss: 0.76081 | val_0_rmse: 0.90573 | val_1_rmse: 0.92307 |  0:00:24s
epoch 24 | loss: 0.75933 | val_0_rmse: 0.89967 | val_1_rmse: 0.92074 |  0:00:26s
epoch 25 | loss: 0.75994 | val_0_rmse: 0.8887  | val_1_rmse: 0.91712 |  0:00:27s
epoch 26 | loss: 0.75279 | val_0_rmse: 0.89021 | val_1_rmse: 0.92408 |  0:00:28s
epoch 27 | loss: 0.75803 | val_0_rmse: 0.87988 | val_1_rmse: 0.90868 |  0:00:29s
epoch 28 | loss: 0.75492 | val_0_rmse: 0.87742 | val_1_rmse: 0.90655 |  0:00:30s
epoch 29 | loss: 0.74898 | val_0_rmse: 0.87556 | val_1_rmse: 0.90567 |  0:00:31s
epoch 30 | loss: 0.75251 | val_0_rmse: 0.88636 | val_1_rmse: 0.91685 |  0:00:32s
epoch 31 | loss: 0.75208 | val_0_rmse: 0.88071 | val_1_rmse: 0.91074 |  0:00:33s
epoch 32 | loss: 0.75523 | val_0_rmse: 0.88698 | val_1_rmse: 0.9178  |  0:00:34s
epoch 33 | loss: 0.76006 | val_0_rmse: 0.87211 | val_1_rmse: 0.90406 |  0:00:35s
epoch 34 | loss: 0.75684 | val_0_rmse: 0.8746  | val_1_rmse: 0.91286 |  0:00:36s
epoch 35 | loss: 0.74974 | val_0_rmse: 0.8722  | val_1_rmse: 0.9094  |  0:00:37s
epoch 36 | loss: 0.75286 | val_0_rmse: 0.86119 | val_1_rmse: 0.89868 |  0:00:38s
epoch 37 | loss: 0.7394  | val_0_rmse: 0.85627 | val_1_rmse: 0.89687 |  0:00:39s
epoch 38 | loss: 0.73957 | val_0_rmse: 0.85795 | val_1_rmse: 0.89819 |  0:00:40s
epoch 39 | loss: 0.73691 | val_0_rmse: 0.85998 | val_1_rmse: 0.89339 |  0:00:41s
epoch 40 | loss: 0.74093 | val_0_rmse: 0.86065 | val_1_rmse: 0.89605 |  0:00:42s
epoch 41 | loss: 0.75358 | val_0_rmse: 0.8861  | val_1_rmse: 0.91748 |  0:00:43s
epoch 42 | loss: 0.7663  | val_0_rmse: 0.89105 | val_1_rmse: 0.91985 |  0:00:44s
epoch 43 | loss: 0.75992 | val_0_rmse: 0.87987 | val_1_rmse: 0.91036 |  0:00:45s
epoch 44 | loss: 0.76139 | val_0_rmse: 0.86642 | val_1_rmse: 0.8971  |  0:00:46s
epoch 45 | loss: 0.74845 | val_0_rmse: 0.85942 | val_1_rmse: 0.89978 |  0:00:47s
epoch 46 | loss: 0.75248 | val_0_rmse: 0.86818 | val_1_rmse: 0.89913 |  0:00:48s
epoch 47 | loss: 0.7571  | val_0_rmse: 0.85624 | val_1_rmse: 0.89572 |  0:00:49s
epoch 48 | loss: 0.73849 | val_0_rmse: 0.85598 | val_1_rmse: 0.89727 |  0:00:50s
epoch 49 | loss: 0.7435  | val_0_rmse: 0.86134 | val_1_rmse: 0.90424 |  0:00:51s
epoch 50 | loss: 0.76362 | val_0_rmse: 0.87892 | val_1_rmse: 0.91998 |  0:00:53s
epoch 51 | loss: 0.80803 | val_0_rmse: 0.89817 | val_1_rmse: 0.93367 |  0:00:54s
epoch 52 | loss: 0.82268 | val_0_rmse: 0.91085 | val_1_rmse: 0.93797 |  0:00:55s
epoch 53 | loss: 0.82361 | val_0_rmse: 0.90832 | val_1_rmse: 0.9359  |  0:00:56s
epoch 54 | loss: 0.81781 | val_0_rmse: 0.90638 | val_1_rmse: 0.93685 |  0:00:57s
epoch 55 | loss: 0.80224 | val_0_rmse: 0.88889 | val_1_rmse: 0.92147 |  0:00:58s
epoch 56 | loss: 0.78919 | val_0_rmse: 0.87996 | val_1_rmse: 0.91541 |  0:00:59s
epoch 57 | loss: 0.77463 | val_0_rmse: 0.87531 | val_1_rmse: 0.912   |  0:01:00s
epoch 58 | loss: 0.77593 | val_0_rmse: 0.88423 | val_1_rmse: 0.91685 |  0:01:01s
epoch 59 | loss: 0.7714  | val_0_rmse: 0.86839 | val_1_rmse: 0.90716 |  0:01:02s
epoch 60 | loss: 0.75979 | val_0_rmse: 0.86663 | val_1_rmse: 0.90831 |  0:01:03s
epoch 61 | loss: 0.75577 | val_0_rmse: 0.87348 | val_1_rmse: 0.91317 |  0:01:04s
epoch 62 | loss: 0.77841 | val_0_rmse: 0.89078 | val_1_rmse: 0.92472 |  0:01:05s
epoch 63 | loss: 0.79858 | val_0_rmse: 0.89005 | val_1_rmse: 0.91763 |  0:01:06s
epoch 64 | loss: 0.79722 | val_0_rmse: 0.88305 | val_1_rmse: 0.91321 |  0:01:07s
epoch 65 | loss: 0.79045 | val_0_rmse: 0.88432 | val_1_rmse: 0.91776 |  0:01:08s
epoch 66 | loss: 0.77857 | val_0_rmse: 0.87557 | val_1_rmse: 0.91212 |  0:01:09s
epoch 67 | loss: 0.77166 | val_0_rmse: 0.87222 | val_1_rmse: 0.91001 |  0:01:10s
epoch 68 | loss: 0.76699 | val_0_rmse: 0.86638 | val_1_rmse: 0.90691 |  0:01:11s
epoch 69 | loss: 0.77585 | val_0_rmse: 0.88101 | val_1_rmse: 0.9124  |  0:01:12s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.89339
Best weights from best epoch are automatically used!
ended training at: 13:56:42
Feature importance:
Mean squared error is of 0.09748118923382945
Mean absolute error:0.19591081262956841
MAPE:0.21236293228896605
R2 score:0.17179456221786793
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:56:45
epoch 0  | loss: 1.37985 | val_0_rmse: 0.9695  | val_1_rmse: 0.96908 |  0:00:00s
epoch 1  | loss: 0.74392 | val_0_rmse: 0.84759 | val_1_rmse: 0.84778 |  0:00:01s
epoch 2  | loss: 0.66665 | val_0_rmse: 0.76355 | val_1_rmse: 0.7572  |  0:00:02s
epoch 3  | loss: 0.57834 | val_0_rmse: 0.75127 | val_1_rmse: 0.74433 |  0:00:03s
epoch 4  | loss: 0.5311  | val_0_rmse: 0.75034 | val_1_rmse: 0.74241 |  0:00:04s
epoch 5  | loss: 0.49441 | val_0_rmse: 0.7779  | val_1_rmse: 0.76999 |  0:00:05s
epoch 6  | loss: 0.47556 | val_0_rmse: 0.77685 | val_1_rmse: 0.76683 |  0:00:06s
epoch 7  | loss: 0.46158 | val_0_rmse: 0.74597 | val_1_rmse: 0.73766 |  0:00:07s
epoch 8  | loss: 0.42978 | val_0_rmse: 0.72844 | val_1_rmse: 0.72151 |  0:00:08s
epoch 9  | loss: 0.40843 | val_0_rmse: 0.72146 | val_1_rmse: 0.71455 |  0:00:09s
epoch 10 | loss: 0.40496 | val_0_rmse: 0.71259 | val_1_rmse: 0.70472 |  0:00:10s
epoch 11 | loss: 0.3877  | val_0_rmse: 0.71271 | val_1_rmse: 0.70515 |  0:00:11s
epoch 12 | loss: 0.37365 | val_0_rmse: 0.69521 | val_1_rmse: 0.68752 |  0:00:12s
epoch 13 | loss: 0.37204 | val_0_rmse: 0.68415 | val_1_rmse: 0.6801  |  0:00:13s
epoch 14 | loss: 0.36215 | val_0_rmse: 0.68158 | val_1_rmse: 0.67607 |  0:00:14s
epoch 15 | loss: 0.35675 | val_0_rmse: 0.65307 | val_1_rmse: 0.65172 |  0:00:15s
epoch 16 | loss: 0.35014 | val_0_rmse: 0.65156 | val_1_rmse: 0.65099 |  0:00:16s
epoch 17 | loss: 0.3419  | val_0_rmse: 0.63481 | val_1_rmse: 0.63827 |  0:00:17s
epoch 18 | loss: 0.3425  | val_0_rmse: 0.62455 | val_1_rmse: 0.63204 |  0:00:18s
epoch 19 | loss: 0.34129 | val_0_rmse: 0.64596 | val_1_rmse: 0.64993 |  0:00:19s
epoch 20 | loss: 0.33349 | val_0_rmse: 0.60703 | val_1_rmse: 0.61328 |  0:00:20s
epoch 21 | loss: 0.33425 | val_0_rmse: 0.60906 | val_1_rmse: 0.60847 |  0:00:21s
epoch 22 | loss: 0.33171 | val_0_rmse: 0.60082 | val_1_rmse: 0.60825 |  0:00:22s
epoch 23 | loss: 0.32637 | val_0_rmse: 0.58978 | val_1_rmse: 0.60075 |  0:00:22s
epoch 24 | loss: 0.32681 | val_0_rmse: 0.59631 | val_1_rmse: 0.60317 |  0:00:24s
epoch 25 | loss: 0.34072 | val_0_rmse: 0.60099 | val_1_rmse: 0.61208 |  0:00:24s
epoch 26 | loss: 0.33355 | val_0_rmse: 0.58572 | val_1_rmse: 0.59672 |  0:00:25s
epoch 27 | loss: 0.3242  | val_0_rmse: 0.57818 | val_1_rmse: 0.59178 |  0:00:26s
epoch 28 | loss: 0.32305 | val_0_rmse: 0.58114 | val_1_rmse: 0.59246 |  0:00:27s
epoch 29 | loss: 0.32111 | val_0_rmse: 0.57059 | val_1_rmse: 0.58474 |  0:00:28s
epoch 30 | loss: 0.3137  | val_0_rmse: 0.55191 | val_1_rmse: 0.56438 |  0:00:29s
epoch 31 | loss: 0.3166  | val_0_rmse: 0.57102 | val_1_rmse: 0.58549 |  0:00:30s
epoch 32 | loss: 0.32803 | val_0_rmse: 0.59354 | val_1_rmse: 0.59799 |  0:00:31s
epoch 33 | loss: 0.33067 | val_0_rmse: 0.57351 | val_1_rmse: 0.58678 |  0:00:32s
epoch 34 | loss: 0.32936 | val_0_rmse: 0.56802 | val_1_rmse: 0.57649 |  0:00:33s
epoch 35 | loss: 0.32349 | val_0_rmse: 0.56223 | val_1_rmse: 0.57401 |  0:00:34s
epoch 36 | loss: 0.31311 | val_0_rmse: 0.5644  | val_1_rmse: 0.5749  |  0:00:35s
epoch 37 | loss: 0.30848 | val_0_rmse: 0.54059 | val_1_rmse: 0.55322 |  0:00:36s
epoch 38 | loss: 0.31387 | val_0_rmse: 0.54845 | val_1_rmse: 0.5592  |  0:00:37s
epoch 39 | loss: 0.31214 | val_0_rmse: 0.53817 | val_1_rmse: 0.55391 |  0:00:38s
epoch 40 | loss: 0.30979 | val_0_rmse: 0.54762 | val_1_rmse: 0.56581 |  0:00:39s
epoch 41 | loss: 0.30734 | val_0_rmse: 0.5435  | val_1_rmse: 0.55939 |  0:00:40s
epoch 42 | loss: 0.30519 | val_0_rmse: 0.55075 | val_1_rmse: 0.56799 |  0:00:41s
epoch 43 | loss: 0.31281 | val_0_rmse: 0.54055 | val_1_rmse: 0.56054 |  0:00:42s
epoch 44 | loss: 0.30976 | val_0_rmse: 0.55238 | val_1_rmse: 0.56817 |  0:00:43s
epoch 45 | loss: 0.31345 | val_0_rmse: 0.54906 | val_1_rmse: 0.56295 |  0:00:44s
epoch 46 | loss: 0.30707 | val_0_rmse: 0.53907 | val_1_rmse: 0.55959 |  0:00:45s
epoch 47 | loss: 0.30916 | val_0_rmse: 0.54473 | val_1_rmse: 0.56051 |  0:00:45s
epoch 48 | loss: 0.3097  | val_0_rmse: 0.53155 | val_1_rmse: 0.54598 |  0:00:46s
epoch 49 | loss: 0.30276 | val_0_rmse: 0.53612 | val_1_rmse: 0.55412 |  0:00:47s
epoch 50 | loss: 0.30583 | val_0_rmse: 0.55545 | val_1_rmse: 0.56958 |  0:00:48s
epoch 51 | loss: 0.30927 | val_0_rmse: 0.55274 | val_1_rmse: 0.57236 |  0:00:49s
epoch 52 | loss: 0.30833 | val_0_rmse: 0.53858 | val_1_rmse: 0.5545  |  0:00:50s
epoch 53 | loss: 0.29472 | val_0_rmse: 0.53365 | val_1_rmse: 0.55465 |  0:00:51s
epoch 54 | loss: 0.29366 | val_0_rmse: 0.52857 | val_1_rmse: 0.54727 |  0:00:52s
epoch 55 | loss: 0.29937 | val_0_rmse: 0.53299 | val_1_rmse: 0.55383 |  0:00:53s
epoch 56 | loss: 0.29636 | val_0_rmse: 0.53054 | val_1_rmse: 0.55227 |  0:00:54s
epoch 57 | loss: 0.29252 | val_0_rmse: 0.52867 | val_1_rmse: 0.54867 |  0:00:55s
epoch 58 | loss: 0.29508 | val_0_rmse: 0.55322 | val_1_rmse: 0.57701 |  0:00:56s
epoch 59 | loss: 0.30591 | val_0_rmse: 0.54923 | val_1_rmse: 0.56553 |  0:00:57s
epoch 60 | loss: 0.305   | val_0_rmse: 0.53307 | val_1_rmse: 0.55476 |  0:00:58s
epoch 61 | loss: 0.29662 | val_0_rmse: 0.52263 | val_1_rmse: 0.54403 |  0:00:59s
epoch 62 | loss: 0.28707 | val_0_rmse: 0.52164 | val_1_rmse: 0.54519 |  0:01:00s
epoch 63 | loss: 0.28691 | val_0_rmse: 0.52115 | val_1_rmse: 0.54322 |  0:01:01s
epoch 64 | loss: 0.28356 | val_0_rmse: 0.5212  | val_1_rmse: 0.54583 |  0:01:02s
epoch 65 | loss: 0.2833  | val_0_rmse: 0.5254  | val_1_rmse: 0.54843 |  0:01:03s
epoch 66 | loss: 0.28549 | val_0_rmse: 0.53017 | val_1_rmse: 0.55082 |  0:01:04s
epoch 67 | loss: 0.29275 | val_0_rmse: 0.5321  | val_1_rmse: 0.55376 |  0:01:05s
epoch 68 | loss: 0.28639 | val_0_rmse: 0.51824 | val_1_rmse: 0.54273 |  0:01:06s
epoch 69 | loss: 0.2913  | val_0_rmse: 0.52597 | val_1_rmse: 0.54893 |  0:01:07s
epoch 70 | loss: 0.29093 | val_0_rmse: 0.51736 | val_1_rmse: 0.54592 |  0:01:07s
epoch 71 | loss: 0.29159 | val_0_rmse: 0.54027 | val_1_rmse: 0.56398 |  0:01:08s
epoch 72 | loss: 0.29486 | val_0_rmse: 0.51934 | val_1_rmse: 0.54961 |  0:01:09s
epoch 73 | loss: 0.28427 | val_0_rmse: 0.52044 | val_1_rmse: 0.54451 |  0:01:10s
epoch 74 | loss: 0.28584 | val_0_rmse: 0.51554 | val_1_rmse: 0.54003 |  0:01:11s
epoch 75 | loss: 0.2873  | val_0_rmse: 0.51965 | val_1_rmse: 0.54459 |  0:01:12s
epoch 76 | loss: 0.28313 | val_0_rmse: 0.52523 | val_1_rmse: 0.55219 |  0:01:13s
epoch 77 | loss: 0.29661 | val_0_rmse: 0.53138 | val_1_rmse: 0.55859 |  0:01:14s
epoch 78 | loss: 0.29256 | val_0_rmse: 0.52442 | val_1_rmse: 0.55532 |  0:01:15s
epoch 79 | loss: 0.28725 | val_0_rmse: 0.52545 | val_1_rmse: 0.55399 |  0:01:16s
epoch 80 | loss: 0.28904 | val_0_rmse: 0.52284 | val_1_rmse: 0.55204 |  0:01:17s
epoch 81 | loss: 0.28505 | val_0_rmse: 0.51817 | val_1_rmse: 0.55051 |  0:01:18s
epoch 82 | loss: 0.28087 | val_0_rmse: 0.5192  | val_1_rmse: 0.55006 |  0:01:19s
epoch 83 | loss: 0.2836  | val_0_rmse: 0.5222  | val_1_rmse: 0.55149 |  0:01:20s
epoch 84 | loss: 0.27991 | val_0_rmse: 0.5149  | val_1_rmse: 0.54093 |  0:01:21s
epoch 85 | loss: 0.27974 | val_0_rmse: 0.51827 | val_1_rmse: 0.54402 |  0:01:22s
epoch 86 | loss: 0.27522 | val_0_rmse: 0.51689 | val_1_rmse: 0.54703 |  0:01:23s
epoch 87 | loss: 0.27744 | val_0_rmse: 0.51719 | val_1_rmse: 0.54716 |  0:01:24s
epoch 88 | loss: 0.27968 | val_0_rmse: 0.51393 | val_1_rmse: 0.54222 |  0:01:25s
epoch 89 | loss: 0.27911 | val_0_rmse: 0.51848 | val_1_rmse: 0.55565 |  0:01:26s
epoch 90 | loss: 0.28012 | val_0_rmse: 0.51648 | val_1_rmse: 0.5535  |  0:01:27s
epoch 91 | loss: 0.27853 | val_0_rmse: 0.51214 | val_1_rmse: 0.54436 |  0:01:28s
epoch 92 | loss: 0.27449 | val_0_rmse: 0.51506 | val_1_rmse: 0.54343 |  0:01:29s
epoch 93 | loss: 0.28248 | val_0_rmse: 0.5713  | val_1_rmse: 0.60346 |  0:01:30s
epoch 94 | loss: 0.32807 | val_0_rmse: 0.55009 | val_1_rmse: 0.58003 |  0:01:31s
epoch 95 | loss: 0.31248 | val_0_rmse: 0.54816 | val_1_rmse: 0.56304 |  0:01:32s
epoch 96 | loss: 0.30121 | val_0_rmse: 0.52838 | val_1_rmse: 0.55386 |  0:01:33s
epoch 97 | loss: 0.29078 | val_0_rmse: 0.52283 | val_1_rmse: 0.54516 |  0:01:33s
epoch 98 | loss: 0.28603 | val_0_rmse: 0.51997 | val_1_rmse: 0.54302 |  0:01:34s
epoch 99 | loss: 0.28441 | val_0_rmse: 0.52442 | val_1_rmse: 0.54685 |  0:01:35s
epoch 100| loss: 0.28514 | val_0_rmse: 0.51566 | val_1_rmse: 0.54042 |  0:01:36s
epoch 101| loss: 0.27861 | val_0_rmse: 0.51579 | val_1_rmse: 0.54091 |  0:01:37s
epoch 102| loss: 0.27788 | val_0_rmse: 0.51256 | val_1_rmse: 0.54095 |  0:01:38s
epoch 103| loss: 0.27837 | val_0_rmse: 0.5126  | val_1_rmse: 0.54137 |  0:01:39s
epoch 104| loss: 0.28008 | val_0_rmse: 0.51706 | val_1_rmse: 0.54388 |  0:01:40s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.54003
Best weights from best epoch are automatically used!
ended training at: 13:58:26
Feature importance:
Mean squared error is of 0.160615990549064
Mean absolute error:0.2604621469222252
MAPE:0.31322172945299226
R2 score:0.6928886744832108
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:58:27
epoch 0  | loss: 1.30905 | val_0_rmse: 0.96346 | val_1_rmse: 1.00016 |  0:00:00s
epoch 1  | loss: 0.71007 | val_0_rmse: 0.83301 | val_1_rmse: 0.86423 |  0:00:01s
epoch 2  | loss: 0.59143 | val_0_rmse: 0.78772 | val_1_rmse: 0.81332 |  0:00:02s
epoch 3  | loss: 0.50326 | val_0_rmse: 0.73117 | val_1_rmse: 0.76604 |  0:00:03s
epoch 4  | loss: 0.47074 | val_0_rmse: 0.70887 | val_1_rmse: 0.74826 |  0:00:04s
epoch 5  | loss: 0.44153 | val_0_rmse: 0.70207 | val_1_rmse: 0.73971 |  0:00:05s
epoch 6  | loss: 0.42219 | val_0_rmse: 0.68075 | val_1_rmse: 0.71719 |  0:00:06s
epoch 7  | loss: 0.41046 | val_0_rmse: 0.68522 | val_1_rmse: 0.7236  |  0:00:07s
epoch 8  | loss: 0.41451 | val_0_rmse: 0.71161 | val_1_rmse: 0.74948 |  0:00:08s
epoch 9  | loss: 0.422   | val_0_rmse: 0.69523 | val_1_rmse: 0.73487 |  0:00:09s
epoch 10 | loss: 0.40907 | val_0_rmse: 0.69167 | val_1_rmse: 0.73026 |  0:00:10s
epoch 11 | loss: 0.406   | val_0_rmse: 0.70371 | val_1_rmse: 0.74551 |  0:00:11s
epoch 12 | loss: 0.40135 | val_0_rmse: 0.6981  | val_1_rmse: 0.72768 |  0:00:12s
epoch 13 | loss: 0.39663 | val_0_rmse: 0.65466 | val_1_rmse: 0.69014 |  0:00:13s
epoch 14 | loss: 0.39043 | val_0_rmse: 0.65959 | val_1_rmse: 0.6959  |  0:00:14s
epoch 15 | loss: 0.38814 | val_0_rmse: 0.65086 | val_1_rmse: 0.68712 |  0:00:15s
epoch 16 | loss: 0.38975 | val_0_rmse: 0.6509  | val_1_rmse: 0.68919 |  0:00:16s
epoch 17 | loss: 0.387   | val_0_rmse: 0.65564 | val_1_rmse: 0.69833 |  0:00:17s
epoch 18 | loss: 0.3823  | val_0_rmse: 0.65706 | val_1_rmse: 0.69324 |  0:00:18s
epoch 19 | loss: 0.38481 | val_0_rmse: 0.65353 | val_1_rmse: 0.69725 |  0:00:19s
epoch 20 | loss: 0.38133 | val_0_rmse: 0.63543 | val_1_rmse: 0.66416 |  0:00:20s
epoch 21 | loss: 0.37624 | val_0_rmse: 0.62199 | val_1_rmse: 0.65981 |  0:00:21s
epoch 22 | loss: 0.36674 | val_0_rmse: 0.60818 | val_1_rmse: 0.64365 |  0:00:22s
epoch 23 | loss: 0.36091 | val_0_rmse: 0.60207 | val_1_rmse: 0.63724 |  0:00:23s
epoch 24 | loss: 0.35209 | val_0_rmse: 0.59847 | val_1_rmse: 0.63866 |  0:00:23s
epoch 25 | loss: 0.34982 | val_0_rmse: 0.62619 | val_1_rmse: 0.65476 |  0:00:24s
epoch 26 | loss: 0.34887 | val_0_rmse: 0.5942  | val_1_rmse: 0.6264  |  0:00:25s
epoch 27 | loss: 0.341   | val_0_rmse: 0.59174 | val_1_rmse: 0.62914 |  0:00:26s
epoch 28 | loss: 0.34459 | val_0_rmse: 0.59395 | val_1_rmse: 0.62939 |  0:00:27s
epoch 29 | loss: 0.34841 | val_0_rmse: 0.59725 | val_1_rmse: 0.62633 |  0:00:28s
epoch 30 | loss: 0.33932 | val_0_rmse: 0.58524 | val_1_rmse: 0.62019 |  0:00:29s
epoch 31 | loss: 0.34102 | val_0_rmse: 0.58004 | val_1_rmse: 0.61233 |  0:00:30s
epoch 32 | loss: 0.34325 | val_0_rmse: 0.5754  | val_1_rmse: 0.60776 |  0:00:31s
epoch 33 | loss: 0.34236 | val_0_rmse: 0.59843 | val_1_rmse: 0.63316 |  0:00:32s
epoch 34 | loss: 0.34255 | val_0_rmse: 0.58331 | val_1_rmse: 0.61668 |  0:00:33s
epoch 35 | loss: 0.34421 | val_0_rmse: 0.57834 | val_1_rmse: 0.60569 |  0:00:34s
epoch 36 | loss: 0.34474 | val_0_rmse: 0.58697 | val_1_rmse: 0.62038 |  0:00:35s
epoch 37 | loss: 0.34712 | val_0_rmse: 0.5751  | val_1_rmse: 0.60909 |  0:00:36s
epoch 38 | loss: 0.34006 | val_0_rmse: 0.59224 | val_1_rmse: 0.62237 |  0:00:37s
epoch 39 | loss: 0.35544 | val_0_rmse: 0.5916  | val_1_rmse: 0.62208 |  0:00:38s
epoch 40 | loss: 0.34992 | val_0_rmse: 0.57964 | val_1_rmse: 0.60686 |  0:00:39s
epoch 41 | loss: 0.35168 | val_0_rmse: 0.58765 | val_1_rmse: 0.61416 |  0:00:40s
epoch 42 | loss: 0.37313 | val_0_rmse: 0.6479  | val_1_rmse: 0.68054 |  0:00:41s
epoch 43 | loss: 0.39982 | val_0_rmse: 0.63486 | val_1_rmse: 0.66101 |  0:00:42s
epoch 44 | loss: 0.38963 | val_0_rmse: 0.62906 | val_1_rmse: 0.65902 |  0:00:43s
epoch 45 | loss: 0.37764 | val_0_rmse: 0.6072  | val_1_rmse: 0.6355  |  0:00:44s
epoch 46 | loss: 0.39795 | val_0_rmse: 0.65085 | val_1_rmse: 0.68534 |  0:00:45s
epoch 47 | loss: 0.41488 | val_0_rmse: 0.61936 | val_1_rmse: 0.65095 |  0:00:45s
epoch 48 | loss: 0.38463 | val_0_rmse: 0.61687 | val_1_rmse: 0.63859 |  0:00:46s
epoch 49 | loss: 0.38097 | val_0_rmse: 0.61013 | val_1_rmse: 0.62048 |  0:00:47s
epoch 50 | loss: 0.37445 | val_0_rmse: 0.60779 | val_1_rmse: 0.63417 |  0:00:48s
epoch 51 | loss: 0.38145 | val_0_rmse: 0.60829 | val_1_rmse: 0.62997 |  0:00:49s
epoch 52 | loss: 0.37106 | val_0_rmse: 0.64425 | val_1_rmse: 0.66529 |  0:00:50s
epoch 53 | loss: 0.38537 | val_0_rmse: 0.61268 | val_1_rmse: 0.6505  |  0:00:51s
epoch 54 | loss: 0.37741 | val_0_rmse: 0.60323 | val_1_rmse: 0.6395  |  0:00:52s
epoch 55 | loss: 0.36985 | val_0_rmse: 0.60754 | val_1_rmse: 0.63114 |  0:00:53s
epoch 56 | loss: 0.36634 | val_0_rmse: 0.6188  | val_1_rmse: 0.64458 |  0:00:54s
epoch 57 | loss: 0.38503 | val_0_rmse: 0.60585 | val_1_rmse: 0.64076 |  0:00:55s
epoch 58 | loss: 0.36953 | val_0_rmse: 0.59822 | val_1_rmse: 0.63514 |  0:00:56s
epoch 59 | loss: 0.36108 | val_0_rmse: 0.59547 | val_1_rmse: 0.62377 |  0:00:57s
epoch 60 | loss: 0.36118 | val_0_rmse: 0.58889 | val_1_rmse: 0.62235 |  0:00:58s
epoch 61 | loss: 0.35795 | val_0_rmse: 0.59571 | val_1_rmse: 0.63555 |  0:00:59s
epoch 62 | loss: 0.37201 | val_0_rmse: 0.6136  | val_1_rmse: 0.64558 |  0:01:00s
epoch 63 | loss: 0.38159 | val_0_rmse: 0.60682 | val_1_rmse: 0.63993 |  0:01:01s
epoch 64 | loss: 0.36386 | val_0_rmse: 0.59856 | val_1_rmse: 0.6287  |  0:01:02s
epoch 65 | loss: 0.35938 | val_0_rmse: 0.58986 | val_1_rmse: 0.62276 |  0:01:03s

Early stopping occured at epoch 65 with best_epoch = 35 and best_val_1_rmse = 0.60569
Best weights from best epoch are automatically used!
ended training at: 13:59:30
Feature importance:
Mean squared error is of 0.17018192621113443
Mean absolute error:0.28018701936708257
MAPE:0.343950126326495
R2 score:0.6392497661700274
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:59:31
epoch 0  | loss: 1.34268 | val_0_rmse: 1.02494 | val_1_rmse: 1.01393 |  0:00:01s
epoch 1  | loss: 0.62145 | val_0_rmse: 0.83314 | val_1_rmse: 0.80414 |  0:00:02s
epoch 2  | loss: 0.533   | val_0_rmse: 0.78286 | val_1_rmse: 0.76826 |  0:00:02s
epoch 3  | loss: 0.49153 | val_0_rmse: 0.71064 | val_1_rmse: 0.70067 |  0:00:03s
epoch 4  | loss: 0.46991 | val_0_rmse: 0.70488 | val_1_rmse: 0.69611 |  0:00:04s
epoch 5  | loss: 0.45075 | val_0_rmse: 0.71963 | val_1_rmse: 0.70472 |  0:00:05s
epoch 6  | loss: 0.45018 | val_0_rmse: 0.72155 | val_1_rmse: 0.7119  |  0:00:06s
epoch 7  | loss: 0.43681 | val_0_rmse: 0.73674 | val_1_rmse: 0.74162 |  0:00:07s
epoch 8  | loss: 0.42193 | val_0_rmse: 0.73226 | val_1_rmse: 0.73561 |  0:00:08s
epoch 9  | loss: 0.4376  | val_0_rmse: 0.75638 | val_1_rmse: 0.75596 |  0:00:09s
epoch 10 | loss: 0.44079 | val_0_rmse: 0.76114 | val_1_rmse: 0.76666 |  0:00:10s
epoch 11 | loss: 0.43528 | val_0_rmse: 0.75118 | val_1_rmse: 0.74724 |  0:00:11s
epoch 12 | loss: 0.41652 | val_0_rmse: 0.71138 | val_1_rmse: 0.70914 |  0:00:12s
epoch 13 | loss: 0.39857 | val_0_rmse: 0.69001 | val_1_rmse: 0.68253 |  0:00:13s
epoch 14 | loss: 0.40412 | val_0_rmse: 0.7363  | val_1_rmse: 0.73807 |  0:00:14s
epoch 15 | loss: 0.44261 | val_0_rmse: 0.70852 | val_1_rmse: 0.70708 |  0:00:15s
epoch 16 | loss: 0.41533 | val_0_rmse: 0.67767 | val_1_rmse: 0.68267 |  0:00:16s
epoch 17 | loss: 0.40904 | val_0_rmse: 0.67635 | val_1_rmse: 0.68092 |  0:00:17s
epoch 18 | loss: 0.40059 | val_0_rmse: 0.65304 | val_1_rmse: 0.65412 |  0:00:18s
epoch 19 | loss: 0.38724 | val_0_rmse: 0.64814 | val_1_rmse: 0.64463 |  0:00:19s
epoch 20 | loss: 0.38688 | val_0_rmse: 0.64328 | val_1_rmse: 0.63919 |  0:00:20s
epoch 21 | loss: 0.37861 | val_0_rmse: 0.64439 | val_1_rmse: 0.65035 |  0:00:21s
epoch 22 | loss: 0.38242 | val_0_rmse: 0.63582 | val_1_rmse: 0.63372 |  0:00:22s
epoch 23 | loss: 0.38296 | val_0_rmse: 0.62977 | val_1_rmse: 0.62608 |  0:00:23s
epoch 24 | loss: 0.37909 | val_0_rmse: 0.6447  | val_1_rmse: 0.63571 |  0:00:24s
epoch 25 | loss: 0.38834 | val_0_rmse: 0.63421 | val_1_rmse: 0.63527 |  0:00:25s
epoch 26 | loss: 0.37147 | val_0_rmse: 0.61205 | val_1_rmse: 0.60789 |  0:00:25s
epoch 27 | loss: 0.36808 | val_0_rmse: 0.60663 | val_1_rmse: 0.60664 |  0:00:26s
epoch 28 | loss: 0.36114 | val_0_rmse: 0.59705 | val_1_rmse: 0.59414 |  0:00:27s
epoch 29 | loss: 0.35618 | val_0_rmse: 0.63867 | val_1_rmse: 0.63533 |  0:00:28s
epoch 30 | loss: 0.36087 | val_0_rmse: 0.64547 | val_1_rmse: 0.64333 |  0:00:29s
epoch 31 | loss: 0.34791 | val_0_rmse: 0.58423 | val_1_rmse: 0.5835  |  0:00:30s
epoch 32 | loss: 0.34702 | val_0_rmse: 0.58225 | val_1_rmse: 0.57832 |  0:00:31s
epoch 33 | loss: 0.34267 | val_0_rmse: 0.57947 | val_1_rmse: 0.57672 |  0:00:32s
epoch 34 | loss: 0.34092 | val_0_rmse: 0.57291 | val_1_rmse: 0.5681  |  0:00:33s
epoch 35 | loss: 0.35266 | val_0_rmse: 0.58439 | val_1_rmse: 0.58309 |  0:00:34s
epoch 36 | loss: 0.34366 | val_0_rmse: 0.57596 | val_1_rmse: 0.5691  |  0:00:35s
epoch 37 | loss: 0.3351  | val_0_rmse: 0.59313 | val_1_rmse: 0.59422 |  0:00:36s
epoch 38 | loss: 0.34101 | val_0_rmse: 0.59318 | val_1_rmse: 0.59259 |  0:00:37s
epoch 39 | loss: 0.33961 | val_0_rmse: 0.56942 | val_1_rmse: 0.56853 |  0:00:38s
epoch 40 | loss: 0.33666 | val_0_rmse: 0.56753 | val_1_rmse: 0.56106 |  0:00:39s
epoch 41 | loss: 0.33227 | val_0_rmse: 0.56514 | val_1_rmse: 0.56374 |  0:00:40s
epoch 42 | loss: 0.32749 | val_0_rmse: 0.56193 | val_1_rmse: 0.55901 |  0:00:41s
epoch 43 | loss: 0.32938 | val_0_rmse: 0.56795 | val_1_rmse: 0.56447 |  0:00:42s
epoch 44 | loss: 0.33247 | val_0_rmse: 0.56824 | val_1_rmse: 0.57044 |  0:00:43s
epoch 45 | loss: 0.32854 | val_0_rmse: 0.56539 | val_1_rmse: 0.56794 |  0:00:44s
epoch 46 | loss: 0.32777 | val_0_rmse: 0.56935 | val_1_rmse: 0.57261 |  0:00:45s
epoch 47 | loss: 0.3285  | val_0_rmse: 0.56089 | val_1_rmse: 0.56546 |  0:00:46s
epoch 48 | loss: 0.32673 | val_0_rmse: 0.55944 | val_1_rmse: 0.56516 |  0:00:47s
epoch 49 | loss: 0.32426 | val_0_rmse: 0.55972 | val_1_rmse: 0.56581 |  0:00:48s
epoch 50 | loss: 0.33019 | val_0_rmse: 0.55764 | val_1_rmse: 0.56147 |  0:00:49s
epoch 51 | loss: 0.32381 | val_0_rmse: 0.55565 | val_1_rmse: 0.55978 |  0:00:50s
epoch 52 | loss: 0.31825 | val_0_rmse: 0.56774 | val_1_rmse: 0.57113 |  0:00:50s
epoch 53 | loss: 0.31887 | val_0_rmse: 0.56118 | val_1_rmse: 0.56577 |  0:00:51s
epoch 54 | loss: 0.32257 | val_0_rmse: 0.55208 | val_1_rmse: 0.55458 |  0:00:52s
epoch 55 | loss: 0.3163  | val_0_rmse: 0.55316 | val_1_rmse: 0.5617  |  0:00:53s
epoch 56 | loss: 0.31638 | val_0_rmse: 0.55014 | val_1_rmse: 0.55696 |  0:00:54s
epoch 57 | loss: 0.31387 | val_0_rmse: 0.55046 | val_1_rmse: 0.55587 |  0:00:55s
epoch 58 | loss: 0.31975 | val_0_rmse: 0.55369 | val_1_rmse: 0.55964 |  0:00:56s
epoch 59 | loss: 0.31499 | val_0_rmse: 0.55459 | val_1_rmse: 0.56143 |  0:00:57s
epoch 60 | loss: 0.31124 | val_0_rmse: 0.543   | val_1_rmse: 0.55156 |  0:00:58s
epoch 61 | loss: 0.31258 | val_0_rmse: 0.54866 | val_1_rmse: 0.55882 |  0:00:59s
epoch 62 | loss: 0.30552 | val_0_rmse: 0.54006 | val_1_rmse: 0.54936 |  0:01:00s
epoch 63 | loss: 0.30633 | val_0_rmse: 0.54553 | val_1_rmse: 0.55398 |  0:01:01s
epoch 64 | loss: 0.31438 | val_0_rmse: 0.54924 | val_1_rmse: 0.55933 |  0:01:02s
epoch 65 | loss: 0.30971 | val_0_rmse: 0.54433 | val_1_rmse: 0.55201 |  0:01:03s
epoch 66 | loss: 0.30733 | val_0_rmse: 0.54253 | val_1_rmse: 0.55016 |  0:01:04s
epoch 67 | loss: 0.31088 | val_0_rmse: 0.55327 | val_1_rmse: 0.55996 |  0:01:05s
epoch 68 | loss: 0.31404 | val_0_rmse: 0.54735 | val_1_rmse: 0.55565 |  0:01:06s
epoch 69 | loss: 0.31123 | val_0_rmse: 0.54965 | val_1_rmse: 0.5508  |  0:01:07s
epoch 70 | loss: 0.30819 | val_0_rmse: 0.54787 | val_1_rmse: 0.55969 |  0:01:08s
epoch 71 | loss: 0.30745 | val_0_rmse: 0.556   | val_1_rmse: 0.56626 |  0:01:09s
epoch 72 | loss: 0.32563 | val_0_rmse: 0.54853 | val_1_rmse: 0.56292 |  0:01:10s
epoch 73 | loss: 0.31095 | val_0_rmse: 0.54678 | val_1_rmse: 0.55489 |  0:01:11s
epoch 74 | loss: 0.31111 | val_0_rmse: 0.5416  | val_1_rmse: 0.55531 |  0:01:12s
epoch 75 | loss: 0.31056 | val_0_rmse: 0.54543 | val_1_rmse: 0.56153 |  0:01:13s
epoch 76 | loss: 0.3125  | val_0_rmse: 0.55427 | val_1_rmse: 0.57011 |  0:01:14s
epoch 77 | loss: 0.30683 | val_0_rmse: 0.55133 | val_1_rmse: 0.55953 |  0:01:14s
epoch 78 | loss: 0.30922 | val_0_rmse: 0.54771 | val_1_rmse: 0.55395 |  0:01:15s
epoch 79 | loss: 0.30743 | val_0_rmse: 0.54645 | val_1_rmse: 0.55376 |  0:01:16s
epoch 80 | loss: 0.30869 | val_0_rmse: 0.55349 | val_1_rmse: 0.55605 |  0:01:17s
epoch 81 | loss: 0.31043 | val_0_rmse: 0.53886 | val_1_rmse: 0.55158 |  0:01:18s
epoch 82 | loss: 0.30293 | val_0_rmse: 0.54204 | val_1_rmse: 0.55156 |  0:01:19s
epoch 83 | loss: 0.3031  | val_0_rmse: 0.53908 | val_1_rmse: 0.54968 |  0:01:20s
epoch 84 | loss: 0.29754 | val_0_rmse: 0.53639 | val_1_rmse: 0.54999 |  0:01:21s
epoch 85 | loss: 0.29815 | val_0_rmse: 0.54387 | val_1_rmse: 0.55411 |  0:01:22s
epoch 86 | loss: 0.30088 | val_0_rmse: 0.54035 | val_1_rmse: 0.55084 |  0:01:23s
epoch 87 | loss: 0.30532 | val_0_rmse: 0.54736 | val_1_rmse: 0.5611  |  0:01:24s
epoch 88 | loss: 0.321   | val_0_rmse: 0.55714 | val_1_rmse: 0.56961 |  0:01:25s
epoch 89 | loss: 0.34158 | val_0_rmse: 0.58133 | val_1_rmse: 0.58069 |  0:01:26s
epoch 90 | loss: 0.33043 | val_0_rmse: 0.55779 | val_1_rmse: 0.56648 |  0:01:27s
epoch 91 | loss: 0.32334 | val_0_rmse: 0.55707 | val_1_rmse: 0.56906 |  0:01:28s
epoch 92 | loss: 0.31415 | val_0_rmse: 0.54828 | val_1_rmse: 0.56779 |  0:01:29s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.54936
Best weights from best epoch are automatically used!
ended training at: 14:01:00
Feature importance:
Mean squared error is of 0.1588818267698126
Mean absolute error:0.26651344158661544
MAPE:0.31933310439718066
R2 score:0.6854286157190621
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:01:01
epoch 0  | loss: 1.50088 | val_0_rmse: 0.99927 | val_1_rmse: 0.99563 |  0:00:00s
epoch 1  | loss: 0.93038 | val_0_rmse: 0.97994 | val_1_rmse: 0.9787  |  0:00:01s
epoch 2  | loss: 0.77388 | val_0_rmse: 0.88246 | val_1_rmse: 0.88165 |  0:00:02s
epoch 3  | loss: 0.66917 | val_0_rmse: 0.80565 | val_1_rmse: 0.80554 |  0:00:03s
epoch 4  | loss: 0.58651 | val_0_rmse: 0.77698 | val_1_rmse: 0.78005 |  0:00:04s
epoch 5  | loss: 0.52328 | val_0_rmse: 0.78509 | val_1_rmse: 0.77512 |  0:00:05s
epoch 6  | loss: 0.49868 | val_0_rmse: 0.73633 | val_1_rmse: 0.73611 |  0:00:06s
epoch 7  | loss: 0.47371 | val_0_rmse: 0.71871 | val_1_rmse: 0.71824 |  0:00:07s
epoch 8  | loss: 0.43821 | val_0_rmse: 0.69169 | val_1_rmse: 0.68528 |  0:00:08s
epoch 9  | loss: 0.41023 | val_0_rmse: 0.66381 | val_1_rmse: 0.66211 |  0:00:09s
epoch 10 | loss: 0.38898 | val_0_rmse: 0.67307 | val_1_rmse: 0.66939 |  0:00:10s
epoch 11 | loss: 0.37305 | val_0_rmse: 0.65611 | val_1_rmse: 0.65319 |  0:00:11s
epoch 12 | loss: 0.37348 | val_0_rmse: 0.65115 | val_1_rmse: 0.64999 |  0:00:12s
epoch 13 | loss: 0.36525 | val_0_rmse: 0.6405  | val_1_rmse: 0.63693 |  0:00:13s
epoch 14 | loss: 0.3535  | val_0_rmse: 0.6302  | val_1_rmse: 0.6257  |  0:00:14s
epoch 15 | loss: 0.35289 | val_0_rmse: 0.62559 | val_1_rmse: 0.62722 |  0:00:15s
epoch 16 | loss: 0.34682 | val_0_rmse: 0.61949 | val_1_rmse: 0.61894 |  0:00:16s
epoch 17 | loss: 0.34225 | val_0_rmse: 0.60637 | val_1_rmse: 0.60627 |  0:00:17s
epoch 18 | loss: 0.33412 | val_0_rmse: 0.6065  | val_1_rmse: 0.60815 |  0:00:18s
epoch 19 | loss: 0.33254 | val_0_rmse: 0.59617 | val_1_rmse: 0.59889 |  0:00:19s
epoch 20 | loss: 0.33422 | val_0_rmse: 0.60167 | val_1_rmse: 0.60238 |  0:00:20s
epoch 21 | loss: 0.32605 | val_0_rmse: 0.59075 | val_1_rmse: 0.59456 |  0:00:21s
epoch 22 | loss: 0.32626 | val_0_rmse: 0.58583 | val_1_rmse: 0.58814 |  0:00:22s
epoch 23 | loss: 0.33423 | val_0_rmse: 0.59548 | val_1_rmse: 0.59633 |  0:00:23s
epoch 24 | loss: 0.33754 | val_0_rmse: 0.58164 | val_1_rmse: 0.58808 |  0:00:23s
epoch 25 | loss: 0.3388  | val_0_rmse: 0.60789 | val_1_rmse: 0.61585 |  0:00:24s
epoch 26 | loss: 0.34326 | val_0_rmse: 0.57884 | val_1_rmse: 0.58229 |  0:00:25s
epoch 27 | loss: 0.33018 | val_0_rmse: 0.58265 | val_1_rmse: 0.58644 |  0:00:26s
epoch 28 | loss: 0.33503 | val_0_rmse: 0.57861 | val_1_rmse: 0.58606 |  0:00:27s
epoch 29 | loss: 0.32858 | val_0_rmse: 0.56771 | val_1_rmse: 0.58019 |  0:00:28s
epoch 30 | loss: 0.32302 | val_0_rmse: 0.55719 | val_1_rmse: 0.56923 |  0:00:29s
epoch 31 | loss: 0.32098 | val_0_rmse: 0.5631  | val_1_rmse: 0.57312 |  0:00:30s
epoch 32 | loss: 0.31892 | val_0_rmse: 0.56105 | val_1_rmse: 0.57601 |  0:00:31s
epoch 33 | loss: 0.32099 | val_0_rmse: 0.55786 | val_1_rmse: 0.57338 |  0:00:32s
epoch 34 | loss: 0.31812 | val_0_rmse: 0.55198 | val_1_rmse: 0.5676  |  0:00:33s
epoch 35 | loss: 0.31168 | val_0_rmse: 0.54741 | val_1_rmse: 0.56338 |  0:00:34s
epoch 36 | loss: 0.32058 | val_0_rmse: 0.58444 | val_1_rmse: 0.60118 |  0:00:35s
epoch 37 | loss: 0.31891 | val_0_rmse: 0.54432 | val_1_rmse: 0.55931 |  0:00:36s
epoch 38 | loss: 0.31183 | val_0_rmse: 0.54437 | val_1_rmse: 0.56304 |  0:00:37s
epoch 39 | loss: 0.31096 | val_0_rmse: 0.54512 | val_1_rmse: 0.56496 |  0:00:38s
epoch 40 | loss: 0.30545 | val_0_rmse: 0.53741 | val_1_rmse: 0.55945 |  0:00:39s
epoch 41 | loss: 0.30737 | val_0_rmse: 0.54958 | val_1_rmse: 0.57337 |  0:00:40s
epoch 42 | loss: 0.31567 | val_0_rmse: 0.54458 | val_1_rmse: 0.56241 |  0:00:41s
epoch 43 | loss: 0.30567 | val_0_rmse: 0.54168 | val_1_rmse: 0.56285 |  0:00:42s
epoch 44 | loss: 0.30918 | val_0_rmse: 0.54622 | val_1_rmse: 0.57039 |  0:00:43s
epoch 45 | loss: 0.31003 | val_0_rmse: 0.53974 | val_1_rmse: 0.56788 |  0:00:44s
epoch 46 | loss: 0.30912 | val_0_rmse: 0.54429 | val_1_rmse: 0.56245 |  0:00:45s
epoch 47 | loss: 0.30794 | val_0_rmse: 0.53351 | val_1_rmse: 0.55852 |  0:00:45s
epoch 48 | loss: 0.29973 | val_0_rmse: 0.53139 | val_1_rmse: 0.56075 |  0:00:46s
epoch 49 | loss: 0.30131 | val_0_rmse: 0.53109 | val_1_rmse: 0.56354 |  0:00:47s
epoch 50 | loss: 0.30048 | val_0_rmse: 0.53998 | val_1_rmse: 0.56809 |  0:00:48s
epoch 51 | loss: 0.30249 | val_0_rmse: 0.53118 | val_1_rmse: 0.5574  |  0:00:49s
epoch 52 | loss: 0.30028 | val_0_rmse: 0.53619 | val_1_rmse: 0.5655  |  0:00:50s
epoch 53 | loss: 0.29723 | val_0_rmse: 0.53258 | val_1_rmse: 0.55736 |  0:00:51s
epoch 54 | loss: 0.29734 | val_0_rmse: 0.52947 | val_1_rmse: 0.56273 |  0:00:52s
epoch 55 | loss: 0.29663 | val_0_rmse: 0.52741 | val_1_rmse: 0.55378 |  0:00:53s
epoch 56 | loss: 0.29494 | val_0_rmse: 0.52697 | val_1_rmse: 0.55543 |  0:00:54s
epoch 57 | loss: 0.29251 | val_0_rmse: 0.5281  | val_1_rmse: 0.55714 |  0:00:55s
epoch 58 | loss: 0.29766 | val_0_rmse: 0.52909 | val_1_rmse: 0.55717 |  0:00:56s
epoch 59 | loss: 0.29485 | val_0_rmse: 0.53329 | val_1_rmse: 0.55976 |  0:00:57s
epoch 60 | loss: 0.29442 | val_0_rmse: 0.53068 | val_1_rmse: 0.55743 |  0:00:58s
epoch 61 | loss: 0.29744 | val_0_rmse: 0.53472 | val_1_rmse: 0.56981 |  0:00:59s
epoch 62 | loss: 0.29739 | val_0_rmse: 0.53372 | val_1_rmse: 0.56844 |  0:01:00s
epoch 63 | loss: 0.29867 | val_0_rmse: 0.53441 | val_1_rmse: 0.56195 |  0:01:01s
epoch 64 | loss: 0.29265 | val_0_rmse: 0.53937 | val_1_rmse: 0.56469 |  0:01:02s
epoch 65 | loss: 0.29436 | val_0_rmse: 0.53235 | val_1_rmse: 0.56241 |  0:01:03s
epoch 66 | loss: 0.29528 | val_0_rmse: 0.5191  | val_1_rmse: 0.55445 |  0:01:04s
epoch 67 | loss: 0.2913  | val_0_rmse: 0.52022 | val_1_rmse: 0.5538  |  0:01:05s
epoch 68 | loss: 0.2884  | val_0_rmse: 0.5281  | val_1_rmse: 0.56076 |  0:01:06s
epoch 69 | loss: 0.28689 | val_0_rmse: 0.5239  | val_1_rmse: 0.55687 |  0:01:07s
epoch 70 | loss: 0.28931 | val_0_rmse: 0.53175 | val_1_rmse: 0.56924 |  0:01:08s
epoch 71 | loss: 0.29363 | val_0_rmse: 0.52397 | val_1_rmse: 0.55669 |  0:01:09s
epoch 72 | loss: 0.28794 | val_0_rmse: 0.51978 | val_1_rmse: 0.55399 |  0:01:10s
epoch 73 | loss: 0.29085 | val_0_rmse: 0.52404 | val_1_rmse: 0.56278 |  0:01:10s
epoch 74 | loss: 0.29586 | val_0_rmse: 0.5226  | val_1_rmse: 0.56217 |  0:01:11s
epoch 75 | loss: 0.28638 | val_0_rmse: 0.52079 | val_1_rmse: 0.56278 |  0:01:12s
epoch 76 | loss: 0.2856  | val_0_rmse: 0.52059 | val_1_rmse: 0.55693 |  0:01:13s
epoch 77 | loss: 0.28546 | val_0_rmse: 0.52885 | val_1_rmse: 0.56979 |  0:01:14s
epoch 78 | loss: 0.28414 | val_0_rmse: 0.51724 | val_1_rmse: 0.55876 |  0:01:15s
epoch 79 | loss: 0.28496 | val_0_rmse: 0.5227  | val_1_rmse: 0.55986 |  0:01:16s
epoch 80 | loss: 0.28645 | val_0_rmse: 0.52467 | val_1_rmse: 0.56089 |  0:01:17s
epoch 81 | loss: 0.28438 | val_0_rmse: 0.5291  | val_1_rmse: 0.56837 |  0:01:18s
epoch 82 | loss: 0.28175 | val_0_rmse: 0.51825 | val_1_rmse: 0.5643  |  0:01:19s
epoch 83 | loss: 0.28497 | val_0_rmse: 0.52091 | val_1_rmse: 0.55875 |  0:01:20s
epoch 84 | loss: 0.28555 | val_0_rmse: 0.51865 | val_1_rmse: 0.55737 |  0:01:21s
epoch 85 | loss: 0.28292 | val_0_rmse: 0.5206  | val_1_rmse: 0.55828 |  0:01:22s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.55378
Best weights from best epoch are automatically used!
ended training at: 14:02:23
Feature importance:
Mean squared error is of 0.14895461615414757
Mean absolute error:0.25892936649958376
MAPE:0.30095142027976146
R2 score:0.7033201459978395
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:02:24
epoch 0  | loss: 1.40205 | val_0_rmse: 0.90177 | val_1_rmse: 0.89327 |  0:00:00s
epoch 1  | loss: 0.59905 | val_0_rmse: 0.74504 | val_1_rmse: 0.73943 |  0:00:01s
epoch 2  | loss: 0.51981 | val_0_rmse: 0.7237  | val_1_rmse: 0.71817 |  0:00:02s
epoch 3  | loss: 0.48518 | val_0_rmse: 0.7203  | val_1_rmse: 0.71858 |  0:00:03s
epoch 4  | loss: 0.47336 | val_0_rmse: 0.70972 | val_1_rmse: 0.70694 |  0:00:04s
epoch 5  | loss: 0.44804 | val_0_rmse: 0.71121 | val_1_rmse: 0.70469 |  0:00:05s
epoch 6  | loss: 0.44576 | val_0_rmse: 0.71011 | val_1_rmse: 0.70441 |  0:00:06s
epoch 7  | loss: 0.43963 | val_0_rmse: 0.71455 | val_1_rmse: 0.70564 |  0:00:07s
epoch 8  | loss: 0.44682 | val_0_rmse: 0.70006 | val_1_rmse: 0.69343 |  0:00:08s
epoch 9  | loss: 0.43546 | val_0_rmse: 0.70588 | val_1_rmse: 0.70019 |  0:00:09s
epoch 10 | loss: 0.42726 | val_0_rmse: 0.68645 | val_1_rmse: 0.68064 |  0:00:10s
epoch 11 | loss: 0.4406  | val_0_rmse: 0.69081 | val_1_rmse: 0.68478 |  0:00:11s
epoch 12 | loss: 0.42048 | val_0_rmse: 0.6811  | val_1_rmse: 0.67481 |  0:00:12s
epoch 13 | loss: 0.41235 | val_0_rmse: 0.68444 | val_1_rmse: 0.68165 |  0:00:13s
epoch 14 | loss: 0.40694 | val_0_rmse: 0.65137 | val_1_rmse: 0.64948 |  0:00:14s
epoch 15 | loss: 0.39701 | val_0_rmse: 0.63688 | val_1_rmse: 0.63952 |  0:00:15s
epoch 16 | loss: 0.38186 | val_0_rmse: 0.63819 | val_1_rmse: 0.63987 |  0:00:16s
epoch 17 | loss: 0.38021 | val_0_rmse: 0.63616 | val_1_rmse: 0.63924 |  0:00:17s
epoch 18 | loss: 0.38307 | val_0_rmse: 0.65638 | val_1_rmse: 0.65799 |  0:00:18s
epoch 19 | loss: 0.40849 | val_0_rmse: 0.65674 | val_1_rmse: 0.65309 |  0:00:19s
epoch 20 | loss: 0.38251 | val_0_rmse: 0.634   | val_1_rmse: 0.63684 |  0:00:20s
epoch 21 | loss: 0.37904 | val_0_rmse: 0.62548 | val_1_rmse: 0.62866 |  0:00:21s
epoch 22 | loss: 0.37511 | val_0_rmse: 0.6166  | val_1_rmse: 0.62222 |  0:00:22s
epoch 23 | loss: 0.36378 | val_0_rmse: 0.61651 | val_1_rmse: 0.62336 |  0:00:23s
epoch 24 | loss: 0.35854 | val_0_rmse: 0.62109 | val_1_rmse: 0.62688 |  0:00:24s
epoch 25 | loss: 0.35833 | val_0_rmse: 0.60173 | val_1_rmse: 0.61302 |  0:00:25s
epoch 26 | loss: 0.3507  | val_0_rmse: 0.59907 | val_1_rmse: 0.60742 |  0:00:25s
epoch 27 | loss: 0.35356 | val_0_rmse: 0.58859 | val_1_rmse: 0.59714 |  0:00:26s
epoch 28 | loss: 0.34787 | val_0_rmse: 0.59348 | val_1_rmse: 0.60451 |  0:00:27s
epoch 29 | loss: 0.34619 | val_0_rmse: 0.57633 | val_1_rmse: 0.58686 |  0:00:28s
epoch 30 | loss: 0.34035 | val_0_rmse: 0.58198 | val_1_rmse: 0.59229 |  0:00:29s
epoch 31 | loss: 0.35524 | val_0_rmse: 0.58736 | val_1_rmse: 0.59633 |  0:00:30s
epoch 32 | loss: 0.34441 | val_0_rmse: 0.58159 | val_1_rmse: 0.58895 |  0:00:31s
epoch 33 | loss: 0.34049 | val_0_rmse: 0.59482 | val_1_rmse: 0.60418 |  0:00:32s
epoch 34 | loss: 0.35917 | val_0_rmse: 0.59836 | val_1_rmse: 0.61005 |  0:00:33s
epoch 35 | loss: 0.34681 | val_0_rmse: 0.58011 | val_1_rmse: 0.59326 |  0:00:34s
epoch 36 | loss: 0.33671 | val_0_rmse: 0.56802 | val_1_rmse: 0.58511 |  0:00:35s
epoch 37 | loss: 0.33074 | val_0_rmse: 0.56574 | val_1_rmse: 0.57931 |  0:00:36s
epoch 38 | loss: 0.33259 | val_0_rmse: 0.57183 | val_1_rmse: 0.59212 |  0:00:37s
epoch 39 | loss: 0.33747 | val_0_rmse: 0.59333 | val_1_rmse: 0.60517 |  0:00:38s
epoch 40 | loss: 0.33072 | val_0_rmse: 0.56861 | val_1_rmse: 0.59173 |  0:00:39s
epoch 41 | loss: 0.33087 | val_0_rmse: 0.56199 | val_1_rmse: 0.57895 |  0:00:40s
epoch 42 | loss: 0.32544 | val_0_rmse: 0.56956 | val_1_rmse: 0.58991 |  0:00:41s
epoch 43 | loss: 0.32429 | val_0_rmse: 0.55793 | val_1_rmse: 0.57887 |  0:00:42s
epoch 44 | loss: 0.321   | val_0_rmse: 0.56074 | val_1_rmse: 0.58261 |  0:00:43s
epoch 45 | loss: 0.32209 | val_0_rmse: 0.57802 | val_1_rmse: 0.59288 |  0:00:44s
epoch 46 | loss: 0.32375 | val_0_rmse: 0.5551  | val_1_rmse: 0.57496 |  0:00:45s
epoch 47 | loss: 0.32038 | val_0_rmse: 0.55626 | val_1_rmse: 0.57667 |  0:00:46s
epoch 48 | loss: 0.31867 | val_0_rmse: 0.55592 | val_1_rmse: 0.57568 |  0:00:47s
epoch 49 | loss: 0.31823 | val_0_rmse: 0.55329 | val_1_rmse: 0.57612 |  0:00:48s
epoch 50 | loss: 0.31758 | val_0_rmse: 0.55306 | val_1_rmse: 0.57564 |  0:00:48s
epoch 51 | loss: 0.31524 | val_0_rmse: 0.55958 | val_1_rmse: 0.58196 |  0:00:49s
epoch 52 | loss: 0.31295 | val_0_rmse: 0.55604 | val_1_rmse: 0.57843 |  0:00:50s
epoch 53 | loss: 0.31102 | val_0_rmse: 0.55213 | val_1_rmse: 0.57342 |  0:00:51s
epoch 54 | loss: 0.31786 | val_0_rmse: 0.56148 | val_1_rmse: 0.58645 |  0:00:52s
epoch 55 | loss: 0.30835 | val_0_rmse: 0.54878 | val_1_rmse: 0.56903 |  0:00:53s
epoch 56 | loss: 0.30924 | val_0_rmse: 0.54527 | val_1_rmse: 0.56706 |  0:00:54s
epoch 57 | loss: 0.30725 | val_0_rmse: 0.54299 | val_1_rmse: 0.56492 |  0:00:55s
epoch 58 | loss: 0.30211 | val_0_rmse: 0.54072 | val_1_rmse: 0.56174 |  0:00:56s
epoch 59 | loss: 0.30219 | val_0_rmse: 0.54131 | val_1_rmse: 0.57097 |  0:00:57s
epoch 60 | loss: 0.30367 | val_0_rmse: 0.53592 | val_1_rmse: 0.56273 |  0:00:58s
epoch 61 | loss: 0.30598 | val_0_rmse: 0.53764 | val_1_rmse: 0.55871 |  0:00:59s
epoch 62 | loss: 0.30412 | val_0_rmse: 0.54605 | val_1_rmse: 0.5707  |  0:01:00s
epoch 63 | loss: 0.30099 | val_0_rmse: 0.53312 | val_1_rmse: 0.55796 |  0:01:01s
epoch 64 | loss: 0.30229 | val_0_rmse: 0.53254 | val_1_rmse: 0.56085 |  0:01:02s
epoch 65 | loss: 0.29526 | val_0_rmse: 0.53217 | val_1_rmse: 0.56338 |  0:01:03s
epoch 66 | loss: 0.29677 | val_0_rmse: 0.5429  | val_1_rmse: 0.57105 |  0:01:04s
epoch 67 | loss: 0.30435 | val_0_rmse: 0.56002 | val_1_rmse: 0.59088 |  0:01:05s
epoch 68 | loss: 0.30377 | val_0_rmse: 0.53379 | val_1_rmse: 0.56031 |  0:01:06s
epoch 69 | loss: 0.30048 | val_0_rmse: 0.53981 | val_1_rmse: 0.56379 |  0:01:07s
epoch 70 | loss: 0.30432 | val_0_rmse: 0.55563 | val_1_rmse: 0.57984 |  0:01:08s
epoch 71 | loss: 0.30243 | val_0_rmse: 0.53725 | val_1_rmse: 0.56907 |  0:01:09s
epoch 72 | loss: 0.29834 | val_0_rmse: 0.53839 | val_1_rmse: 0.57319 |  0:01:10s
epoch 73 | loss: 0.30086 | val_0_rmse: 0.53406 | val_1_rmse: 0.56579 |  0:01:11s
epoch 74 | loss: 0.30131 | val_0_rmse: 0.54815 | val_1_rmse: 0.57585 |  0:01:12s
epoch 75 | loss: 0.29873 | val_0_rmse: 0.53145 | val_1_rmse: 0.56198 |  0:01:13s
epoch 76 | loss: 0.2934  | val_0_rmse: 0.53654 | val_1_rmse: 0.56645 |  0:01:13s
epoch 77 | loss: 0.29475 | val_0_rmse: 0.53314 | val_1_rmse: 0.5633  |  0:01:14s
epoch 78 | loss: 0.295   | val_0_rmse: 0.52765 | val_1_rmse: 0.56084 |  0:01:15s
epoch 79 | loss: 0.29052 | val_0_rmse: 0.53495 | val_1_rmse: 0.56278 |  0:01:16s
epoch 80 | loss: 0.29555 | val_0_rmse: 0.53053 | val_1_rmse: 0.56593 |  0:01:17s
epoch 81 | loss: 0.29047 | val_0_rmse: 0.52616 | val_1_rmse: 0.55927 |  0:01:18s
epoch 82 | loss: 0.29012 | val_0_rmse: 0.52421 | val_1_rmse: 0.55981 |  0:01:19s
epoch 83 | loss: 0.2854  | val_0_rmse: 0.53197 | val_1_rmse: 0.5621  |  0:01:20s
epoch 84 | loss: 0.29081 | val_0_rmse: 0.55744 | val_1_rmse: 0.5833  |  0:01:21s
epoch 85 | loss: 0.29096 | val_0_rmse: 0.52453 | val_1_rmse: 0.55624 |  0:01:22s
epoch 86 | loss: 0.29585 | val_0_rmse: 0.54854 | val_1_rmse: 0.58678 |  0:01:23s
epoch 87 | loss: 0.29839 | val_0_rmse: 0.53803 | val_1_rmse: 0.56435 |  0:01:24s
epoch 88 | loss: 0.30119 | val_0_rmse: 0.53169 | val_1_rmse: 0.55552 |  0:01:25s
epoch 89 | loss: 0.30123 | val_0_rmse: 0.53984 | val_1_rmse: 0.56287 |  0:01:26s
epoch 90 | loss: 0.29778 | val_0_rmse: 0.52924 | val_1_rmse: 0.56035 |  0:01:27s
epoch 91 | loss: 0.29066 | val_0_rmse: 0.53963 | val_1_rmse: 0.57586 |  0:01:28s
epoch 92 | loss: 0.29076 | val_0_rmse: 0.53813 | val_1_rmse: 0.56912 |  0:01:29s
epoch 93 | loss: 0.29838 | val_0_rmse: 0.53016 | val_1_rmse: 0.56979 |  0:01:30s
epoch 94 | loss: 0.29233 | val_0_rmse: 0.54476 | val_1_rmse: 0.57807 |  0:01:31s
epoch 95 | loss: 0.28932 | val_0_rmse: 0.53113 | val_1_rmse: 0.56148 |  0:01:32s
epoch 96 | loss: 0.29351 | val_0_rmse: 0.52557 | val_1_rmse: 0.56192 |  0:01:33s
epoch 97 | loss: 0.28678 | val_0_rmse: 0.52332 | val_1_rmse: 0.55983 |  0:01:34s
epoch 98 | loss: 0.28636 | val_0_rmse: 0.52121 | val_1_rmse: 0.55648 |  0:01:35s
epoch 99 | loss: 0.28611 | val_0_rmse: 0.51934 | val_1_rmse: 0.55352 |  0:01:35s
epoch 100| loss: 0.28612 | val_0_rmse: 0.52819 | val_1_rmse: 0.56696 |  0:01:36s
epoch 101| loss: 0.28658 | val_0_rmse: 0.51939 | val_1_rmse: 0.55337 |  0:01:37s
epoch 102| loss: 0.28321 | val_0_rmse: 0.51946 | val_1_rmse: 0.55916 |  0:01:38s
epoch 103| loss: 0.28338 | val_0_rmse: 0.52129 | val_1_rmse: 0.55848 |  0:01:39s
epoch 104| loss: 0.28385 | val_0_rmse: 0.52145 | val_1_rmse: 0.56266 |  0:01:40s
epoch 105| loss: 0.28047 | val_0_rmse: 0.52164 | val_1_rmse: 0.5608  |  0:01:41s
epoch 106| loss: 0.28021 | val_0_rmse: 0.51858 | val_1_rmse: 0.56158 |  0:01:42s
epoch 107| loss: 0.2819  | val_0_rmse: 0.53398 | val_1_rmse: 0.57192 |  0:01:43s
epoch 108| loss: 0.28257 | val_0_rmse: 0.51839 | val_1_rmse: 0.56378 |  0:01:44s
epoch 109| loss: 0.27949 | val_0_rmse: 0.52798 | val_1_rmse: 0.56691 |  0:01:45s
epoch 110| loss: 0.28374 | val_0_rmse: 0.52296 | val_1_rmse: 0.56379 |  0:01:46s
epoch 111| loss: 0.28198 | val_0_rmse: 0.51798 | val_1_rmse: 0.56049 |  0:01:47s
epoch 112| loss: 0.27815 | val_0_rmse: 0.51742 | val_1_rmse: 0.56326 |  0:01:48s
epoch 113| loss: 0.27604 | val_0_rmse: 0.5179  | val_1_rmse: 0.55806 |  0:01:49s
epoch 114| loss: 0.28102 | val_0_rmse: 0.51752 | val_1_rmse: 0.56012 |  0:01:50s
epoch 115| loss: 0.27974 | val_0_rmse: 0.52461 | val_1_rmse: 0.57076 |  0:01:51s
epoch 116| loss: 0.27852 | val_0_rmse: 0.5344  | val_1_rmse: 0.56926 |  0:01:52s
epoch 117| loss: 0.28183 | val_0_rmse: 0.52103 | val_1_rmse: 0.56317 |  0:01:53s
epoch 118| loss: 0.2788  | val_0_rmse: 0.5231  | val_1_rmse: 0.56625 |  0:01:54s
epoch 119| loss: 0.27732 | val_0_rmse: 0.51549 | val_1_rmse: 0.55934 |  0:01:55s
epoch 120| loss: 0.2767  | val_0_rmse: 0.51704 | val_1_rmse: 0.5636  |  0:01:56s
epoch 121| loss: 0.27902 | val_0_rmse: 0.51466 | val_1_rmse: 0.55781 |  0:01:57s
epoch 122| loss: 0.27614 | val_0_rmse: 0.51355 | val_1_rmse: 0.55921 |  0:01:58s
epoch 123| loss: 0.28593 | val_0_rmse: 0.52863 | val_1_rmse: 0.56803 |  0:01:59s
epoch 124| loss: 0.28475 | val_0_rmse: 0.52316 | val_1_rmse: 0.57071 |  0:01:59s
epoch 125| loss: 0.27809 | val_0_rmse: 0.5204  | val_1_rmse: 0.57256 |  0:02:01s
epoch 126| loss: 0.27888 | val_0_rmse: 0.52485 | val_1_rmse: 0.56805 |  0:02:02s
epoch 127| loss: 0.27922 | val_0_rmse: 0.52287 | val_1_rmse: 0.57067 |  0:02:02s
epoch 128| loss: 0.27848 | val_0_rmse: 0.51572 | val_1_rmse: 0.56445 |  0:02:03s
epoch 129| loss: 0.27666 | val_0_rmse: 0.51533 | val_1_rmse: 0.55719 |  0:02:04s
epoch 130| loss: 0.27568 | val_0_rmse: 0.51985 | val_1_rmse: 0.56472 |  0:02:05s
epoch 131| loss: 0.27421 | val_0_rmse: 0.53455 | val_1_rmse: 0.57801 |  0:02:06s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.55337
Best weights from best epoch are automatically used!
ended training at: 14:04:31
Feature importance:
Mean squared error is of 0.12954468221245471
Mean absolute error:0.24985762826133573
MAPE:0.31016154595265866
R2 score:0.7306016274046514
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:04:32
epoch 0  | loss: 1.73338 | val_0_rmse: 0.99267 | val_1_rmse: 1.04145 |  0:00:00s
epoch 1  | loss: 1.1268  | val_0_rmse: 0.97145 | val_1_rmse: 1.02337 |  0:00:00s
epoch 2  | loss: 0.85822 | val_0_rmse: 0.94123 | val_1_rmse: 0.99241 |  0:00:00s
epoch 3  | loss: 0.78487 | val_0_rmse: 0.91182 | val_1_rmse: 0.96695 |  0:00:01s
epoch 4  | loss: 0.75432 | val_0_rmse: 0.86648 | val_1_rmse: 0.92056 |  0:00:01s
epoch 5  | loss: 0.71687 | val_0_rmse: 0.85256 | val_1_rmse: 0.90088 |  0:00:02s
epoch 6  | loss: 0.72176 | val_0_rmse: 0.84499 | val_1_rmse: 0.88147 |  0:00:02s
epoch 7  | loss: 0.70342 | val_0_rmse: 0.85009 | val_1_rmse: 0.885   |  0:00:02s
epoch 8  | loss: 0.69985 | val_0_rmse: 0.83218 | val_1_rmse: 0.8769  |  0:00:02s
epoch 9  | loss: 0.69902 | val_0_rmse: 0.83855 | val_1_rmse: 0.8743  |  0:00:03s
epoch 10 | loss: 0.69194 | val_0_rmse: 0.83538 | val_1_rmse: 0.872   |  0:00:03s
epoch 11 | loss: 0.7012  | val_0_rmse: 0.85798 | val_1_rmse: 0.90127 |  0:00:03s
epoch 12 | loss: 0.72641 | val_0_rmse: 0.84045 | val_1_rmse: 0.88362 |  0:00:04s
epoch 13 | loss: 0.72024 | val_0_rmse: 0.83998 | val_1_rmse: 0.87883 |  0:00:04s
epoch 14 | loss: 0.70631 | val_0_rmse: 0.83283 | val_1_rmse: 0.87748 |  0:00:04s
epoch 15 | loss: 0.69709 | val_0_rmse: 0.83111 | val_1_rmse: 0.87585 |  0:00:05s
epoch 16 | loss: 0.69311 | val_0_rmse: 0.83603 | val_1_rmse: 0.88169 |  0:00:05s
epoch 17 | loss: 0.69003 | val_0_rmse: 0.83182 | val_1_rmse: 0.87895 |  0:00:05s
epoch 18 | loss: 0.67786 | val_0_rmse: 0.83114 | val_1_rmse: 0.87727 |  0:00:06s
epoch 19 | loss: 0.68238 | val_0_rmse: 0.83272 | val_1_rmse: 0.8834  |  0:00:06s
epoch 20 | loss: 0.68309 | val_0_rmse: 0.82712 | val_1_rmse: 0.87826 |  0:00:06s
epoch 21 | loss: 0.68362 | val_0_rmse: 0.82797 | val_1_rmse: 0.8762  |  0:00:07s
epoch 22 | loss: 0.68589 | val_0_rmse: 0.82627 | val_1_rmse: 0.87509 |  0:00:07s
epoch 23 | loss: 0.68396 | val_0_rmse: 0.82725 | val_1_rmse: 0.87739 |  0:00:07s
epoch 24 | loss: 0.67532 | val_0_rmse: 0.825   | val_1_rmse: 0.87803 |  0:00:07s
epoch 25 | loss: 0.66207 | val_0_rmse: 0.82346 | val_1_rmse: 0.87122 |  0:00:08s
epoch 26 | loss: 0.67392 | val_0_rmse: 0.82146 | val_1_rmse: 0.87094 |  0:00:08s
epoch 27 | loss: 0.66645 | val_0_rmse: 0.81998 | val_1_rmse: 0.87174 |  0:00:08s
epoch 28 | loss: 0.65984 | val_0_rmse: 0.81716 | val_1_rmse: 0.86626 |  0:00:09s
epoch 29 | loss: 0.65827 | val_0_rmse: 0.82166 | val_1_rmse: 0.87303 |  0:00:09s
epoch 30 | loss: 0.65842 | val_0_rmse: 0.81473 | val_1_rmse: 0.86581 |  0:00:09s
epoch 31 | loss: 0.65849 | val_0_rmse: 0.81358 | val_1_rmse: 0.86379 |  0:00:10s
epoch 32 | loss: 0.6595  | val_0_rmse: 0.81231 | val_1_rmse: 0.86167 |  0:00:10s
epoch 33 | loss: 0.65495 | val_0_rmse: 0.81199 | val_1_rmse: 0.86237 |  0:00:10s
epoch 34 | loss: 0.64877 | val_0_rmse: 0.818   | val_1_rmse: 0.86724 |  0:00:11s
epoch 35 | loss: 0.64651 | val_0_rmse: 0.81348 | val_1_rmse: 0.8618  |  0:00:11s
epoch 36 | loss: 0.64014 | val_0_rmse: 0.80916 | val_1_rmse: 0.85731 |  0:00:11s
epoch 37 | loss: 0.64593 | val_0_rmse: 0.81042 | val_1_rmse: 0.85813 |  0:00:12s
epoch 38 | loss: 0.63253 | val_0_rmse: 0.80796 | val_1_rmse: 0.85795 |  0:00:12s
epoch 39 | loss: 0.63467 | val_0_rmse: 0.80863 | val_1_rmse: 0.86092 |  0:00:12s
epoch 40 | loss: 0.63582 | val_0_rmse: 0.80215 | val_1_rmse: 0.85229 |  0:00:13s
epoch 41 | loss: 0.62538 | val_0_rmse: 0.80216 | val_1_rmse: 0.85355 |  0:00:13s
epoch 42 | loss: 0.61609 | val_0_rmse: 0.80252 | val_1_rmse: 0.85642 |  0:00:13s
epoch 43 | loss: 0.62353 | val_0_rmse: 0.80062 | val_1_rmse: 0.85959 |  0:00:14s
epoch 44 | loss: 0.61638 | val_0_rmse: 0.80077 | val_1_rmse: 0.85856 |  0:00:14s
epoch 45 | loss: 0.61799 | val_0_rmse: 0.80764 | val_1_rmse: 0.8706  |  0:00:14s
epoch 46 | loss: 0.61462 | val_0_rmse: 0.8058  | val_1_rmse: 0.86584 |  0:00:15s
epoch 47 | loss: 0.61742 | val_0_rmse: 0.80858 | val_1_rmse: 0.87196 |  0:00:15s
epoch 48 | loss: 0.61371 | val_0_rmse: 0.79722 | val_1_rmse: 0.86095 |  0:00:15s
epoch 49 | loss: 0.60116 | val_0_rmse: 0.80041 | val_1_rmse: 0.86383 |  0:00:15s
epoch 50 | loss: 0.59612 | val_0_rmse: 0.79661 | val_1_rmse: 0.86103 |  0:00:16s
epoch 51 | loss: 0.59151 | val_0_rmse: 0.79511 | val_1_rmse: 0.86429 |  0:00:16s
epoch 52 | loss: 0.59078 | val_0_rmse: 0.78913 | val_1_rmse: 0.86829 |  0:00:16s
epoch 53 | loss: 0.58748 | val_0_rmse: 0.78848 | val_1_rmse: 0.87036 |  0:00:17s
epoch 54 | loss: 0.59306 | val_0_rmse: 0.78627 | val_1_rmse: 0.86709 |  0:00:17s
epoch 55 | loss: 0.58154 | val_0_rmse: 0.78095 | val_1_rmse: 0.86395 |  0:00:17s
epoch 56 | loss: 0.58765 | val_0_rmse: 0.78295 | val_1_rmse: 0.86148 |  0:00:18s
epoch 57 | loss: 0.58543 | val_0_rmse: 0.78119 | val_1_rmse: 0.86095 |  0:00:18s
epoch 58 | loss: 0.57947 | val_0_rmse: 0.77755 | val_1_rmse: 0.86504 |  0:00:18s
epoch 59 | loss: 0.57742 | val_0_rmse: 0.77994 | val_1_rmse: 0.86501 |  0:00:19s
epoch 60 | loss: 0.57202 | val_0_rmse: 0.77768 | val_1_rmse: 0.85376 |  0:00:19s
epoch 61 | loss: 0.57151 | val_0_rmse: 0.78632 | val_1_rmse: 0.86186 |  0:00:19s
epoch 62 | loss: 0.57929 | val_0_rmse: 0.78769 | val_1_rmse: 0.86747 |  0:00:20s
epoch 63 | loss: 0.56645 | val_0_rmse: 0.77293 | val_1_rmse: 0.85203 |  0:00:20s
epoch 64 | loss: 0.57275 | val_0_rmse: 0.77452 | val_1_rmse: 0.85663 |  0:00:20s
epoch 65 | loss: 0.56623 | val_0_rmse: 0.77341 | val_1_rmse: 0.85908 |  0:00:20s
epoch 66 | loss: 0.56534 | val_0_rmse: 0.76485 | val_1_rmse: 0.85252 |  0:00:21s
epoch 67 | loss: 0.56639 | val_0_rmse: 0.76425 | val_1_rmse: 0.85628 |  0:00:21s
epoch 68 | loss: 0.55196 | val_0_rmse: 0.75852 | val_1_rmse: 0.85767 |  0:00:21s
epoch 69 | loss: 0.55183 | val_0_rmse: 0.75818 | val_1_rmse: 0.86421 |  0:00:22s
epoch 70 | loss: 0.55455 | val_0_rmse: 0.75838 | val_1_rmse: 0.86461 |  0:00:22s
epoch 71 | loss: 0.55842 | val_0_rmse: 0.76225 | val_1_rmse: 0.85621 |  0:00:22s
epoch 72 | loss: 0.55228 | val_0_rmse: 0.75568 | val_1_rmse: 0.8586  |  0:00:23s
epoch 73 | loss: 0.55275 | val_0_rmse: 0.749   | val_1_rmse: 0.86931 |  0:00:23s
epoch 74 | loss: 0.53965 | val_0_rmse: 0.74708 | val_1_rmse: 0.85598 |  0:00:23s
epoch 75 | loss: 0.53978 | val_0_rmse: 0.74973 | val_1_rmse: 0.86934 |  0:00:24s
epoch 76 | loss: 0.54934 | val_0_rmse: 0.75431 | val_1_rmse: 0.86751 |  0:00:24s
epoch 77 | loss: 0.54523 | val_0_rmse: 0.74929 | val_1_rmse: 0.86686 |  0:00:24s
epoch 78 | loss: 0.54383 | val_0_rmse: 0.75532 | val_1_rmse: 0.86856 |  0:00:25s
epoch 79 | loss: 0.54344 | val_0_rmse: 0.74316 | val_1_rmse: 0.85644 |  0:00:25s
epoch 80 | loss: 0.53425 | val_0_rmse: 0.73891 | val_1_rmse: 0.87851 |  0:00:25s
epoch 81 | loss: 0.54453 | val_0_rmse: 0.73605 | val_1_rmse: 0.86302 |  0:00:26s
epoch 82 | loss: 0.53133 | val_0_rmse: 0.73968 | val_1_rmse: 0.85366 |  0:00:26s
epoch 83 | loss: 0.53139 | val_0_rmse: 0.73231 | val_1_rmse: 0.85643 |  0:00:26s
epoch 84 | loss: 0.52673 | val_0_rmse: 0.73191 | val_1_rmse: 0.87083 |  0:00:26s
epoch 85 | loss: 0.52531 | val_0_rmse: 0.73694 | val_1_rmse: 0.86746 |  0:00:27s
epoch 86 | loss: 0.53531 | val_0_rmse: 0.73219 | val_1_rmse: 0.86292 |  0:00:27s
epoch 87 | loss: 0.53076 | val_0_rmse: 0.7447  | val_1_rmse: 0.87454 |  0:00:27s
epoch 88 | loss: 0.54331 | val_0_rmse: 0.74048 | val_1_rmse: 0.86624 |  0:00:28s
epoch 89 | loss: 0.53942 | val_0_rmse: 0.74072 | val_1_rmse: 0.87351 |  0:00:28s
epoch 90 | loss: 0.54464 | val_0_rmse: 0.73677 | val_1_rmse: 0.87083 |  0:00:28s
epoch 91 | loss: 0.53494 | val_0_rmse: 0.73099 | val_1_rmse: 0.8738  |  0:00:29s
epoch 92 | loss: 0.52354 | val_0_rmse: 0.72815 | val_1_rmse: 0.86878 |  0:00:29s
epoch 93 | loss: 0.52872 | val_0_rmse: 0.7265  | val_1_rmse: 0.8678  |  0:00:29s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.85203
Best weights from best epoch are automatically used!
ended training at: 14:05:02
Feature importance:
Mean squared error is of 0.03943813724379788
Mean absolute error:0.14931388333484122
MAPE:0.15959415676538982
R2 score:0.3479447415007093
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:05:02
epoch 0  | loss: 1.64924 | val_0_rmse: 0.98905 | val_1_rmse: 1.0373  |  0:00:00s
epoch 1  | loss: 1.0751  | val_0_rmse: 0.98779 | val_1_rmse: 1.03493 |  0:00:00s
epoch 2  | loss: 0.95052 | val_0_rmse: 0.96471 | val_1_rmse: 1.011   |  0:00:00s
epoch 3  | loss: 0.87845 | val_0_rmse: 0.92103 | val_1_rmse: 0.96677 |  0:00:01s
epoch 4  | loss: 0.83231 | val_0_rmse: 0.87402 | val_1_rmse: 0.90978 |  0:00:01s
epoch 5  | loss: 0.79113 | val_0_rmse: 0.85572 | val_1_rmse: 0.88919 |  0:00:01s
epoch 6  | loss: 0.77818 | val_0_rmse: 0.85126 | val_1_rmse: 0.88405 |  0:00:02s
epoch 7  | loss: 0.76041 | val_0_rmse: 0.85217 | val_1_rmse: 0.89264 |  0:00:02s
epoch 8  | loss: 0.74446 | val_0_rmse: 0.84184 | val_1_rmse: 0.88234 |  0:00:02s
epoch 9  | loss: 0.74152 | val_0_rmse: 0.8375  | val_1_rmse: 0.87191 |  0:00:03s
epoch 10 | loss: 0.73165 | val_0_rmse: 0.8357  | val_1_rmse: 0.87207 |  0:00:03s
epoch 11 | loss: 0.72739 | val_0_rmse: 0.83011 | val_1_rmse: 0.86162 |  0:00:03s
epoch 12 | loss: 0.7073  | val_0_rmse: 0.83311 | val_1_rmse: 0.86718 |  0:00:04s
epoch 13 | loss: 0.70536 | val_0_rmse: 0.83509 | val_1_rmse: 0.86652 |  0:00:04s
epoch 14 | loss: 0.70205 | val_0_rmse: 0.8322  | val_1_rmse: 0.86198 |  0:00:04s
epoch 15 | loss: 0.70285 | val_0_rmse: 0.83643 | val_1_rmse: 0.86648 |  0:00:05s
epoch 16 | loss: 0.70243 | val_0_rmse: 0.83357 | val_1_rmse: 0.86461 |  0:00:05s
epoch 17 | loss: 0.69378 | val_0_rmse: 0.83341 | val_1_rmse: 0.86265 |  0:00:05s
epoch 18 | loss: 0.68797 | val_0_rmse: 0.83424 | val_1_rmse: 0.86281 |  0:00:06s
epoch 19 | loss: 0.69153 | val_0_rmse: 0.83021 | val_1_rmse: 0.85987 |  0:00:06s
epoch 20 | loss: 0.69096 | val_0_rmse: 0.83239 | val_1_rmse: 0.86726 |  0:00:06s
epoch 21 | loss: 0.68409 | val_0_rmse: 0.8291  | val_1_rmse: 0.86258 |  0:00:07s
epoch 22 | loss: 0.68034 | val_0_rmse: 0.82582 | val_1_rmse: 0.85726 |  0:00:07s
epoch 23 | loss: 0.68124 | val_0_rmse: 0.83229 | val_1_rmse: 0.866   |  0:00:07s
epoch 24 | loss: 0.68284 | val_0_rmse: 0.83007 | val_1_rmse: 0.86398 |  0:00:07s
epoch 25 | loss: 0.67846 | val_0_rmse: 0.82467 | val_1_rmse: 0.85602 |  0:00:08s
epoch 26 | loss: 0.67649 | val_0_rmse: 0.82556 | val_1_rmse: 0.8569  |  0:00:08s
epoch 27 | loss: 0.67448 | val_0_rmse: 0.82533 | val_1_rmse: 0.85479 |  0:00:08s
epoch 28 | loss: 0.67595 | val_0_rmse: 0.82285 | val_1_rmse: 0.85252 |  0:00:09s
epoch 29 | loss: 0.68323 | val_0_rmse: 0.8259  | val_1_rmse: 0.8584  |  0:00:09s
epoch 30 | loss: 0.67369 | val_0_rmse: 0.8233  | val_1_rmse: 0.85154 |  0:00:09s
epoch 31 | loss: 0.67048 | val_0_rmse: 0.82372 | val_1_rmse: 0.85419 |  0:00:10s
epoch 32 | loss: 0.66941 | val_0_rmse: 0.82472 | val_1_rmse: 0.85363 |  0:00:10s
epoch 33 | loss: 0.66701 | val_0_rmse: 0.82506 | val_1_rmse: 0.85723 |  0:00:10s
epoch 34 | loss: 0.66226 | val_0_rmse: 0.82112 | val_1_rmse: 0.85064 |  0:00:11s
epoch 35 | loss: 0.65911 | val_0_rmse: 0.82249 | val_1_rmse: 0.85746 |  0:00:11s
epoch 36 | loss: 0.66223 | val_0_rmse: 0.82127 | val_1_rmse: 0.85434 |  0:00:11s
epoch 37 | loss: 0.6678  | val_0_rmse: 0.82048 | val_1_rmse: 0.85114 |  0:00:12s
epoch 38 | loss: 0.67012 | val_0_rmse: 0.82054 | val_1_rmse: 0.85148 |  0:00:12s
epoch 39 | loss: 0.67076 | val_0_rmse: 0.82192 | val_1_rmse: 0.85007 |  0:00:12s
epoch 40 | loss: 0.66155 | val_0_rmse: 0.8224  | val_1_rmse: 0.8517  |  0:00:13s
epoch 41 | loss: 0.66331 | val_0_rmse: 0.82264 | val_1_rmse: 0.85107 |  0:00:13s
epoch 42 | loss: 0.65823 | val_0_rmse: 0.82352 | val_1_rmse: 0.85404 |  0:00:13s
epoch 43 | loss: 0.65696 | val_0_rmse: 0.81836 | val_1_rmse: 0.85353 |  0:00:13s
epoch 44 | loss: 0.65578 | val_0_rmse: 0.8181  | val_1_rmse: 0.84818 |  0:00:14s
epoch 45 | loss: 0.64977 | val_0_rmse: 0.81699 | val_1_rmse: 0.84867 |  0:00:14s
epoch 46 | loss: 0.64853 | val_0_rmse: 0.81523 | val_1_rmse: 0.8462  |  0:00:14s
epoch 47 | loss: 0.6453  | val_0_rmse: 0.81907 | val_1_rmse: 0.84844 |  0:00:15s
epoch 48 | loss: 0.64235 | val_0_rmse: 0.81593 | val_1_rmse: 0.84962 |  0:00:15s
epoch 49 | loss: 0.64174 | val_0_rmse: 0.8166  | val_1_rmse: 0.8538  |  0:00:15s
epoch 50 | loss: 0.63867 | val_0_rmse: 0.81737 | val_1_rmse: 0.84618 |  0:00:16s
epoch 51 | loss: 0.64122 | val_0_rmse: 0.81753 | val_1_rmse: 0.84521 |  0:00:16s
epoch 52 | loss: 0.6438  | val_0_rmse: 0.81381 | val_1_rmse: 0.84608 |  0:00:16s
epoch 53 | loss: 0.63403 | val_0_rmse: 0.82237 | val_1_rmse: 0.85334 |  0:00:17s
epoch 54 | loss: 0.63657 | val_0_rmse: 0.8215  | val_1_rmse: 0.85211 |  0:00:17s
epoch 55 | loss: 0.63639 | val_0_rmse: 0.81559 | val_1_rmse: 0.8512  |  0:00:17s
epoch 56 | loss: 0.63476 | val_0_rmse: 0.81341 | val_1_rmse: 0.84731 |  0:00:18s
epoch 57 | loss: 0.63051 | val_0_rmse: 0.81476 | val_1_rmse: 0.84459 |  0:00:18s
epoch 58 | loss: 0.62832 | val_0_rmse: 0.80836 | val_1_rmse: 0.8416  |  0:00:18s
epoch 59 | loss: 0.6249  | val_0_rmse: 0.8055  | val_1_rmse: 0.84356 |  0:00:19s
epoch 60 | loss: 0.62269 | val_0_rmse: 0.8042  | val_1_rmse: 0.8438  |  0:00:19s
epoch 61 | loss: 0.61442 | val_0_rmse: 0.80185 | val_1_rmse: 0.84741 |  0:00:19s
epoch 62 | loss: 0.62136 | val_0_rmse: 0.8035  | val_1_rmse: 0.85033 |  0:00:20s
epoch 63 | loss: 0.61851 | val_0_rmse: 0.80252 | val_1_rmse: 0.84971 |  0:00:20s
epoch 64 | loss: 0.61623 | val_0_rmse: 0.79956 | val_1_rmse: 0.84508 |  0:00:20s
epoch 65 | loss: 0.61965 | val_0_rmse: 0.80121 | val_1_rmse: 0.84462 |  0:00:20s
epoch 66 | loss: 0.60932 | val_0_rmse: 0.80403 | val_1_rmse: 0.84644 |  0:00:21s
epoch 67 | loss: 0.61705 | val_0_rmse: 0.80109 | val_1_rmse: 0.84716 |  0:00:21s
epoch 68 | loss: 0.61896 | val_0_rmse: 0.79718 | val_1_rmse: 0.84368 |  0:00:21s
epoch 69 | loss: 0.60862 | val_0_rmse: 0.79584 | val_1_rmse: 0.83732 |  0:00:22s
epoch 70 | loss: 0.60787 | val_0_rmse: 0.79283 | val_1_rmse: 0.83973 |  0:00:22s
epoch 71 | loss: 0.60998 | val_0_rmse: 0.79408 | val_1_rmse: 0.84676 |  0:00:22s
epoch 72 | loss: 0.61051 | val_0_rmse: 0.78938 | val_1_rmse: 0.84504 |  0:00:23s
epoch 73 | loss: 0.60278 | val_0_rmse: 0.78921 | val_1_rmse: 0.84504 |  0:00:23s
epoch 74 | loss: 0.6094  | val_0_rmse: 0.78752 | val_1_rmse: 0.84368 |  0:00:23s
epoch 75 | loss: 0.61193 | val_0_rmse: 0.7928  | val_1_rmse: 0.85107 |  0:00:24s
epoch 76 | loss: 0.60811 | val_0_rmse: 0.79463 | val_1_rmse: 0.85039 |  0:00:24s
epoch 77 | loss: 0.61021 | val_0_rmse: 0.78532 | val_1_rmse: 0.84068 |  0:00:24s
epoch 78 | loss: 0.6018  | val_0_rmse: 0.7853  | val_1_rmse: 0.83936 |  0:00:25s
epoch 79 | loss: 0.60131 | val_0_rmse: 0.78328 | val_1_rmse: 0.83716 |  0:00:25s
epoch 80 | loss: 0.59793 | val_0_rmse: 0.78069 | val_1_rmse: 0.83966 |  0:00:25s
epoch 81 | loss: 0.60083 | val_0_rmse: 0.77454 | val_1_rmse: 0.83984 |  0:00:25s
epoch 82 | loss: 0.58864 | val_0_rmse: 0.77195 | val_1_rmse: 0.8401  |  0:00:26s
epoch 83 | loss: 0.58794 | val_0_rmse: 0.77628 | val_1_rmse: 0.84755 |  0:00:26s
epoch 84 | loss: 0.58811 | val_0_rmse: 0.77223 | val_1_rmse: 0.8391  |  0:00:26s
epoch 85 | loss: 0.58268 | val_0_rmse: 0.76769 | val_1_rmse: 0.83496 |  0:00:27s
epoch 86 | loss: 0.58012 | val_0_rmse: 0.76307 | val_1_rmse: 0.83202 |  0:00:27s
epoch 87 | loss: 0.58521 | val_0_rmse: 0.76637 | val_1_rmse: 0.83186 |  0:00:27s
epoch 88 | loss: 0.57695 | val_0_rmse: 0.76454 | val_1_rmse: 0.83591 |  0:00:28s
epoch 89 | loss: 0.5851  | val_0_rmse: 0.7657  | val_1_rmse: 0.83296 |  0:00:28s
epoch 90 | loss: 0.5832  | val_0_rmse: 0.763   | val_1_rmse: 0.82947 |  0:00:28s
epoch 91 | loss: 0.5833  | val_0_rmse: 0.76412 | val_1_rmse: 0.83396 |  0:00:29s
epoch 92 | loss: 0.57117 | val_0_rmse: 0.75762 | val_1_rmse: 0.83848 |  0:00:29s
epoch 93 | loss: 0.57268 | val_0_rmse: 0.75726 | val_1_rmse: 0.83465 |  0:00:29s
epoch 94 | loss: 0.57122 | val_0_rmse: 0.75228 | val_1_rmse: 0.83381 |  0:00:30s
epoch 95 | loss: 0.566   | val_0_rmse: 0.75406 | val_1_rmse: 0.83104 |  0:00:30s
epoch 96 | loss: 0.5651  | val_0_rmse: 0.75376 | val_1_rmse: 0.8277  |  0:00:30s
epoch 97 | loss: 0.56093 | val_0_rmse: 0.75631 | val_1_rmse: 0.83416 |  0:00:31s
epoch 98 | loss: 0.56678 | val_0_rmse: 0.74776 | val_1_rmse: 0.84233 |  0:00:31s
epoch 99 | loss: 0.57277 | val_0_rmse: 0.75525 | val_1_rmse: 0.84639 |  0:00:31s
epoch 100| loss: 0.56936 | val_0_rmse: 0.75007 | val_1_rmse: 0.84595 |  0:00:32s
epoch 101| loss: 0.56481 | val_0_rmse: 0.74351 | val_1_rmse: 0.84776 |  0:00:32s
epoch 102| loss: 0.55791 | val_0_rmse: 0.74327 | val_1_rmse: 0.84849 |  0:00:32s
epoch 103| loss: 0.5627  | val_0_rmse: 0.74807 | val_1_rmse: 0.85815 |  0:00:32s
epoch 104| loss: 0.55881 | val_0_rmse: 0.7395  | val_1_rmse: 0.84953 |  0:00:33s
epoch 105| loss: 0.55224 | val_0_rmse: 0.73725 | val_1_rmse: 0.84086 |  0:00:33s
epoch 106| loss: 0.55271 | val_0_rmse: 0.73918 | val_1_rmse: 0.84258 |  0:00:33s
epoch 107| loss: 0.54854 | val_0_rmse: 0.73697 | val_1_rmse: 0.84357 |  0:00:34s
epoch 108| loss: 0.54333 | val_0_rmse: 0.73186 | val_1_rmse: 0.86139 |  0:00:34s
epoch 109| loss: 0.54879 | val_0_rmse: 0.72807 | val_1_rmse: 0.86649 |  0:00:34s
epoch 110| loss: 0.54589 | val_0_rmse: 0.7287  | val_1_rmse: 0.87615 |  0:00:35s
epoch 111| loss: 0.5396  | val_0_rmse: 0.72586 | val_1_rmse: 0.87238 |  0:00:35s
epoch 112| loss: 0.54756 | val_0_rmse: 0.72291 | val_1_rmse: 0.85797 |  0:00:35s
epoch 113| loss: 0.54191 | val_0_rmse: 0.7278  | val_1_rmse: 0.86456 |  0:00:36s
epoch 114| loss: 0.54135 | val_0_rmse: 0.72268 | val_1_rmse: 0.86642 |  0:00:36s
epoch 115| loss: 0.55297 | val_0_rmse: 0.7251  | val_1_rmse: 0.8488  |  0:00:36s
epoch 116| loss: 0.54147 | val_0_rmse: 0.72171 | val_1_rmse: 0.85756 |  0:00:37s
epoch 117| loss: 0.53675 | val_0_rmse: 0.71939 | val_1_rmse: 0.86329 |  0:00:37s
epoch 118| loss: 0.52921 | val_0_rmse: 0.71673 | val_1_rmse: 0.8537  |  0:00:37s
epoch 119| loss: 0.53283 | val_0_rmse: 0.71459 | val_1_rmse: 0.86082 |  0:00:37s
epoch 120| loss: 0.52872 | val_0_rmse: 0.71235 | val_1_rmse: 0.86978 |  0:00:38s
epoch 121| loss: 0.52483 | val_0_rmse: 0.71971 | val_1_rmse: 0.87125 |  0:00:38s
epoch 122| loss: 0.52347 | val_0_rmse: 0.71615 | val_1_rmse: 0.85919 |  0:00:39s
epoch 123| loss: 0.5247  | val_0_rmse: 0.71319 | val_1_rmse: 0.86186 |  0:00:39s
epoch 124| loss: 0.51241 | val_0_rmse: 0.70201 | val_1_rmse: 0.8637  |  0:00:39s
epoch 125| loss: 0.51668 | val_0_rmse: 0.70097 | val_1_rmse: 0.84664 |  0:00:39s
epoch 126| loss: 0.50711 | val_0_rmse: 0.69881 | val_1_rmse: 0.85875 |  0:00:40s

Early stopping occured at epoch 126 with best_epoch = 96 and best_val_1_rmse = 0.8277
Best weights from best epoch are automatically used!
ended training at: 14:05:43
Feature importance:
Mean squared error is of 0.041743840202620904
Mean absolute error:0.1484139420432154
MAPE:0.1542636195867039
R2 score:0.3660664292354293
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:05:43
epoch 0  | loss: 1.75279 | val_0_rmse: 0.99129 | val_1_rmse: 0.97167 |  0:00:00s
epoch 1  | loss: 1.03015 | val_0_rmse: 0.99179 | val_1_rmse: 0.97556 |  0:00:00s
epoch 2  | loss: 0.92895 | val_0_rmse: 0.93819 | val_1_rmse: 0.91088 |  0:00:00s
epoch 3  | loss: 0.8311  | val_0_rmse: 0.88219 | val_1_rmse: 0.85335 |  0:00:01s
epoch 4  | loss: 0.76407 | val_0_rmse: 0.8686  | val_1_rmse: 0.84266 |  0:00:01s
epoch 5  | loss: 0.7679  | val_0_rmse: 0.865   | val_1_rmse: 0.835   |  0:00:01s
epoch 6  | loss: 0.74095 | val_0_rmse: 0.85151 | val_1_rmse: 0.81521 |  0:00:02s
epoch 7  | loss: 0.72051 | val_0_rmse: 0.84629 | val_1_rmse: 0.81973 |  0:00:02s
epoch 8  | loss: 0.70976 | val_0_rmse: 0.84238 | val_1_rmse: 0.80838 |  0:00:02s
epoch 9  | loss: 0.7032  | val_0_rmse: 0.84237 | val_1_rmse: 0.81192 |  0:00:03s
epoch 10 | loss: 0.6968  | val_0_rmse: 0.83734 | val_1_rmse: 0.80019 |  0:00:03s
epoch 11 | loss: 0.70137 | val_0_rmse: 0.84686 | val_1_rmse: 0.80913 |  0:00:03s
epoch 12 | loss: 0.70441 | val_0_rmse: 0.84868 | val_1_rmse: 0.81383 |  0:00:04s
epoch 13 | loss: 0.69921 | val_0_rmse: 0.85538 | val_1_rmse: 0.81806 |  0:00:04s
epoch 14 | loss: 0.6901  | val_0_rmse: 0.84953 | val_1_rmse: 0.81462 |  0:00:04s
epoch 15 | loss: 0.68699 | val_0_rmse: 0.84551 | val_1_rmse: 0.80472 |  0:00:05s
epoch 16 | loss: 0.69417 | val_0_rmse: 0.83916 | val_1_rmse: 0.80265 |  0:00:05s
epoch 17 | loss: 0.68322 | val_0_rmse: 0.84232 | val_1_rmse: 0.80528 |  0:00:05s
epoch 18 | loss: 0.68903 | val_0_rmse: 0.84028 | val_1_rmse: 0.81006 |  0:00:06s
epoch 19 | loss: 0.69284 | val_0_rmse: 0.84194 | val_1_rmse: 0.80338 |  0:00:06s
epoch 20 | loss: 0.68564 | val_0_rmse: 0.83582 | val_1_rmse: 0.81099 |  0:00:06s
epoch 21 | loss: 0.68746 | val_0_rmse: 0.83793 | val_1_rmse: 0.80571 |  0:00:06s
epoch 22 | loss: 0.68489 | val_0_rmse: 0.84161 | val_1_rmse: 0.81413 |  0:00:07s
epoch 23 | loss: 0.68728 | val_0_rmse: 0.83361 | val_1_rmse: 0.79982 |  0:00:07s
epoch 24 | loss: 0.67468 | val_0_rmse: 0.82988 | val_1_rmse: 0.79983 |  0:00:07s
epoch 25 | loss: 0.67649 | val_0_rmse: 0.83534 | val_1_rmse: 0.81041 |  0:00:08s
epoch 26 | loss: 0.67062 | val_0_rmse: 0.8365  | val_1_rmse: 0.81147 |  0:00:08s
epoch 27 | loss: 0.67359 | val_0_rmse: 0.84649 | val_1_rmse: 0.8194  |  0:00:08s
epoch 28 | loss: 0.66527 | val_0_rmse: 0.8399  | val_1_rmse: 0.81555 |  0:00:09s
epoch 29 | loss: 0.65917 | val_0_rmse: 0.84809 | val_1_rmse: 0.82187 |  0:00:09s
epoch 30 | loss: 0.65947 | val_0_rmse: 0.84181 | val_1_rmse: 0.81838 |  0:00:09s
epoch 31 | loss: 0.65756 | val_0_rmse: 0.84666 | val_1_rmse: 0.81947 |  0:00:10s
epoch 32 | loss: 0.65396 | val_0_rmse: 0.85116 | val_1_rmse: 0.81843 |  0:00:10s
epoch 33 | loss: 0.65729 | val_0_rmse: 0.8401  | val_1_rmse: 0.81321 |  0:00:10s
epoch 34 | loss: 0.6537  | val_0_rmse: 0.86065 | val_1_rmse: 0.83095 |  0:00:11s
epoch 35 | loss: 0.64836 | val_0_rmse: 0.84571 | val_1_rmse: 0.8137  |  0:00:11s
epoch 36 | loss: 0.65084 | val_0_rmse: 0.83192 | val_1_rmse: 0.80333 |  0:00:11s
epoch 37 | loss: 0.64072 | val_0_rmse: 0.835   | val_1_rmse: 0.80343 |  0:00:12s
epoch 38 | loss: 0.64365 | val_0_rmse: 0.83698 | val_1_rmse: 0.80681 |  0:00:12s
epoch 39 | loss: 0.63743 | val_0_rmse: 0.83488 | val_1_rmse: 0.80925 |  0:00:12s
epoch 40 | loss: 0.63302 | val_0_rmse: 0.84969 | val_1_rmse: 0.81863 |  0:00:13s
epoch 41 | loss: 0.62945 | val_0_rmse: 0.8349  | val_1_rmse: 0.81129 |  0:00:13s
epoch 42 | loss: 0.63198 | val_0_rmse: 0.86269 | val_1_rmse: 0.84102 |  0:00:13s
epoch 43 | loss: 0.63564 | val_0_rmse: 0.83995 | val_1_rmse: 0.80563 |  0:00:13s
epoch 44 | loss: 0.64462 | val_0_rmse: 0.82674 | val_1_rmse: 0.80365 |  0:00:14s
epoch 45 | loss: 0.64384 | val_0_rmse: 0.84317 | val_1_rmse: 0.81651 |  0:00:14s
epoch 46 | loss: 0.64147 | val_0_rmse: 0.82361 | val_1_rmse: 0.79999 |  0:00:14s
epoch 47 | loss: 0.64746 | val_0_rmse: 0.83202 | val_1_rmse: 0.80351 |  0:00:15s
epoch 48 | loss: 0.65112 | val_0_rmse: 0.81878 | val_1_rmse: 0.8003  |  0:00:15s
epoch 49 | loss: 0.64742 | val_0_rmse: 0.82604 | val_1_rmse: 0.80936 |  0:00:15s
epoch 50 | loss: 0.63028 | val_0_rmse: 0.81858 | val_1_rmse: 0.80837 |  0:00:16s
epoch 51 | loss: 0.63719 | val_0_rmse: 0.83037 | val_1_rmse: 0.82073 |  0:00:16s
epoch 52 | loss: 0.62706 | val_0_rmse: 0.81459 | val_1_rmse: 0.80307 |  0:00:16s
epoch 53 | loss: 0.62897 | val_0_rmse: 0.81564 | val_1_rmse: 0.79863 |  0:00:17s
epoch 54 | loss: 0.62816 | val_0_rmse: 0.81338 | val_1_rmse: 0.80027 |  0:00:17s
epoch 55 | loss: 0.62952 | val_0_rmse: 0.8127  | val_1_rmse: 0.81309 |  0:00:17s
epoch 56 | loss: 0.63124 | val_0_rmse: 0.82271 | val_1_rmse: 0.81396 |  0:00:18s
epoch 57 | loss: 0.63267 | val_0_rmse: 0.80692 | val_1_rmse: 0.80282 |  0:00:18s
epoch 58 | loss: 0.63131 | val_0_rmse: 0.81733 | val_1_rmse: 0.8026  |  0:00:18s
epoch 59 | loss: 0.63416 | val_0_rmse: 0.80589 | val_1_rmse: 0.80441 |  0:00:19s
epoch 60 | loss: 0.63473 | val_0_rmse: 0.80485 | val_1_rmse: 0.79119 |  0:00:19s
epoch 61 | loss: 0.63271 | val_0_rmse: 0.80151 | val_1_rmse: 0.78867 |  0:00:19s
epoch 62 | loss: 0.62864 | val_0_rmse: 0.80049 | val_1_rmse: 0.78537 |  0:00:19s
epoch 63 | loss: 0.63983 | val_0_rmse: 0.80234 | val_1_rmse: 0.77808 |  0:00:20s
epoch 64 | loss: 0.63915 | val_0_rmse: 0.79643 | val_1_rmse: 0.77251 |  0:00:20s
epoch 65 | loss: 0.63768 | val_0_rmse: 0.80177 | val_1_rmse: 0.77537 |  0:00:21s
epoch 66 | loss: 0.6591  | val_0_rmse: 0.80497 | val_1_rmse: 0.7776  |  0:00:21s
epoch 67 | loss: 0.65783 | val_0_rmse: 0.80962 | val_1_rmse: 0.78261 |  0:00:21s
epoch 68 | loss: 0.65851 | val_0_rmse: 0.80663 | val_1_rmse: 0.79118 |  0:00:21s
epoch 69 | loss: 0.65555 | val_0_rmse: 0.84094 | val_1_rmse: 0.82361 |  0:00:22s
epoch 70 | loss: 0.6611  | val_0_rmse: 0.82206 | val_1_rmse: 0.80041 |  0:00:22s
epoch 71 | loss: 0.65901 | val_0_rmse: 0.82515 | val_1_rmse: 0.79394 |  0:00:22s
epoch 72 | loss: 0.6628  | val_0_rmse: 0.81202 | val_1_rmse: 0.78792 |  0:00:23s
epoch 73 | loss: 0.66439 | val_0_rmse: 0.83314 | val_1_rmse: 0.81304 |  0:00:23s
epoch 74 | loss: 0.66739 | val_0_rmse: 0.81741 | val_1_rmse: 0.79863 |  0:00:23s
epoch 75 | loss: 0.67609 | val_0_rmse: 0.81555 | val_1_rmse: 0.79469 |  0:00:24s
epoch 76 | loss: 0.66326 | val_0_rmse: 0.8282  | val_1_rmse: 0.79896 |  0:00:24s
epoch 77 | loss: 0.6702  | val_0_rmse: 0.83098 | val_1_rmse: 0.8014  |  0:00:24s
epoch 78 | loss: 0.67622 | val_0_rmse: 0.8363  | val_1_rmse: 0.80487 |  0:00:25s
epoch 79 | loss: 0.68963 | val_0_rmse: 0.83422 | val_1_rmse: 0.80788 |  0:00:25s
epoch 80 | loss: 0.68034 | val_0_rmse: 0.82924 | val_1_rmse: 0.80314 |  0:00:25s
epoch 81 | loss: 0.6741  | val_0_rmse: 0.83255 | val_1_rmse: 0.80717 |  0:00:26s
epoch 82 | loss: 0.67553 | val_0_rmse: 0.83811 | val_1_rmse: 0.81661 |  0:00:26s
epoch 83 | loss: 0.67345 | val_0_rmse: 0.82362 | val_1_rmse: 0.79439 |  0:00:26s
epoch 84 | loss: 0.66775 | val_0_rmse: 0.81885 | val_1_rmse: 0.79466 |  0:00:26s
epoch 85 | loss: 0.66836 | val_0_rmse: 0.82098 | val_1_rmse: 0.79235 |  0:00:27s
epoch 86 | loss: 0.67056 | val_0_rmse: 0.82067 | val_1_rmse: 0.78986 |  0:00:27s
epoch 87 | loss: 0.66859 | val_0_rmse: 0.82013 | val_1_rmse: 0.79133 |  0:00:27s
epoch 88 | loss: 0.66283 | val_0_rmse: 0.81343 | val_1_rmse: 0.78844 |  0:00:28s
epoch 89 | loss: 0.67562 | val_0_rmse: 0.81324 | val_1_rmse: 0.78643 |  0:00:28s
epoch 90 | loss: 0.66961 | val_0_rmse: 0.81202 | val_1_rmse: 0.78871 |  0:00:28s
epoch 91 | loss: 0.66521 | val_0_rmse: 0.81366 | val_1_rmse: 0.78579 |  0:00:29s
epoch 92 | loss: 0.66186 | val_0_rmse: 0.8099  | val_1_rmse: 0.78576 |  0:00:29s
epoch 93 | loss: 0.65836 | val_0_rmse: 0.80486 | val_1_rmse: 0.79073 |  0:00:29s
epoch 94 | loss: 0.66259 | val_0_rmse: 0.80278 | val_1_rmse: 0.78607 |  0:00:30s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.77251
Best weights from best epoch are automatically used!
ended training at: 14:06:13
Feature importance:
Mean squared error is of 0.04435544823674838
Mean absolute error:0.15503669532538103
MAPE:0.15829187203925452
R2 score:0.32587373312376333
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:06:14
epoch 0  | loss: 1.66189 | val_0_rmse: 0.94275 | val_1_rmse: 0.95431 |  0:00:00s
epoch 1  | loss: 0.9834  | val_0_rmse: 0.9454  | val_1_rmse: 0.95644 |  0:00:00s
epoch 2  | loss: 0.8545  | val_0_rmse: 0.89608 | val_1_rmse: 0.90137 |  0:00:01s
epoch 3  | loss: 0.77584 | val_0_rmse: 0.8608  | val_1_rmse: 0.85444 |  0:00:01s
epoch 4  | loss: 0.73334 | val_0_rmse: 0.85411 | val_1_rmse: 0.83722 |  0:00:01s
epoch 5  | loss: 0.73593 | val_0_rmse: 0.85465 | val_1_rmse: 0.83844 |  0:00:02s
epoch 6  | loss: 0.72546 | val_0_rmse: 0.87213 | val_1_rmse: 0.86421 |  0:00:02s
epoch 7  | loss: 0.72012 | val_0_rmse: 0.87884 | val_1_rmse: 0.88548 |  0:00:02s
epoch 8  | loss: 0.71022 | val_0_rmse: 0.84075 | val_1_rmse: 0.83199 |  0:00:02s
epoch 9  | loss: 0.69598 | val_0_rmse: 0.834   | val_1_rmse: 0.81862 |  0:00:03s
epoch 10 | loss: 0.70056 | val_0_rmse: 0.83417 | val_1_rmse: 0.81434 |  0:00:03s
epoch 11 | loss: 0.68667 | val_0_rmse: 0.82945 | val_1_rmse: 0.81107 |  0:00:03s
epoch 12 | loss: 0.69056 | val_0_rmse: 0.82908 | val_1_rmse: 0.80983 |  0:00:04s
epoch 13 | loss: 0.68831 | val_0_rmse: 0.83101 | val_1_rmse: 0.81494 |  0:00:04s
epoch 14 | loss: 0.68828 | val_0_rmse: 0.82992 | val_1_rmse: 0.81418 |  0:00:04s
epoch 15 | loss: 0.67958 | val_0_rmse: 0.8244  | val_1_rmse: 0.80428 |  0:00:05s
epoch 16 | loss: 0.68488 | val_0_rmse: 0.82473 | val_1_rmse: 0.80634 |  0:00:05s
epoch 17 | loss: 0.68861 | val_0_rmse: 0.82665 | val_1_rmse: 0.80383 |  0:00:05s
epoch 18 | loss: 0.67661 | val_0_rmse: 0.83597 | val_1_rmse: 0.81383 |  0:00:06s
epoch 19 | loss: 0.67046 | val_0_rmse: 0.83492 | val_1_rmse: 0.80766 |  0:00:06s
epoch 20 | loss: 0.66873 | val_0_rmse: 0.84051 | val_1_rmse: 0.82248 |  0:00:06s
epoch 21 | loss: 0.66427 | val_0_rmse: 0.83404 | val_1_rmse: 0.81284 |  0:00:07s
epoch 22 | loss: 0.67073 | val_0_rmse: 0.84171 | val_1_rmse: 0.831   |  0:00:07s
epoch 23 | loss: 0.66173 | val_0_rmse: 0.83308 | val_1_rmse: 0.81772 |  0:00:07s
epoch 24 | loss: 0.66839 | val_0_rmse: 0.83838 | val_1_rmse: 0.82688 |  0:00:08s
epoch 25 | loss: 0.66207 | val_0_rmse: 0.82498 | val_1_rmse: 0.80337 |  0:00:08s
epoch 26 | loss: 0.65715 | val_0_rmse: 0.83151 | val_1_rmse: 0.81886 |  0:00:08s
epoch 27 | loss: 0.65349 | val_0_rmse: 0.8234  | val_1_rmse: 0.8084  |  0:00:09s
epoch 28 | loss: 0.65827 | val_0_rmse: 0.83501 | val_1_rmse: 0.83142 |  0:00:09s
epoch 29 | loss: 0.64828 | val_0_rmse: 0.82543 | val_1_rmse: 0.81532 |  0:00:09s
epoch 30 | loss: 0.65558 | val_0_rmse: 0.82551 | val_1_rmse: 0.8152  |  0:00:09s
epoch 31 | loss: 0.64416 | val_0_rmse: 0.83318 | val_1_rmse: 0.8303  |  0:00:10s
epoch 32 | loss: 0.65448 | val_0_rmse: 0.81772 | val_1_rmse: 0.80811 |  0:00:10s
epoch 33 | loss: 0.64098 | val_0_rmse: 0.83215 | val_1_rmse: 0.82739 |  0:00:10s
epoch 34 | loss: 0.66143 | val_0_rmse: 0.82171 | val_1_rmse: 0.81532 |  0:00:11s
epoch 35 | loss: 0.64622 | val_0_rmse: 0.81725 | val_1_rmse: 0.7969  |  0:00:11s
epoch 36 | loss: 0.64099 | val_0_rmse: 0.82796 | val_1_rmse: 0.81421 |  0:00:11s
epoch 37 | loss: 0.64571 | val_0_rmse: 0.81884 | val_1_rmse: 0.80148 |  0:00:12s
epoch 38 | loss: 0.64242 | val_0_rmse: 0.82456 | val_1_rmse: 0.80721 |  0:00:12s
epoch 39 | loss: 0.63788 | val_0_rmse: 0.82503 | val_1_rmse: 0.81575 |  0:00:12s
epoch 40 | loss: 0.63142 | val_0_rmse: 0.81305 | val_1_rmse: 0.80461 |  0:00:13s
epoch 41 | loss: 0.62592 | val_0_rmse: 0.81048 | val_1_rmse: 0.79969 |  0:00:13s
epoch 42 | loss: 0.62823 | val_0_rmse: 0.8095  | val_1_rmse: 0.79875 |  0:00:13s
epoch 43 | loss: 0.62207 | val_0_rmse: 0.81006 | val_1_rmse: 0.80501 |  0:00:14s
epoch 44 | loss: 0.62398 | val_0_rmse: 0.80512 | val_1_rmse: 0.79166 |  0:00:14s
epoch 45 | loss: 0.62132 | val_0_rmse: 0.80913 | val_1_rmse: 0.79494 |  0:00:14s
epoch 46 | loss: 0.61984 | val_0_rmse: 0.80895 | val_1_rmse: 0.7945  |  0:00:15s
epoch 47 | loss: 0.61857 | val_0_rmse: 0.82361 | val_1_rmse: 0.81284 |  0:00:15s
epoch 48 | loss: 0.62439 | val_0_rmse: 0.80922 | val_1_rmse: 0.80504 |  0:00:15s
epoch 49 | loss: 0.61969 | val_0_rmse: 0.80858 | val_1_rmse: 0.80274 |  0:00:16s
epoch 50 | loss: 0.61296 | val_0_rmse: 0.80886 | val_1_rmse: 0.79943 |  0:00:16s
epoch 51 | loss: 0.61135 | val_0_rmse: 0.80213 | val_1_rmse: 0.79011 |  0:00:16s
epoch 52 | loss: 0.61219 | val_0_rmse: 0.79632 | val_1_rmse: 0.78874 |  0:00:17s
epoch 53 | loss: 0.6133  | val_0_rmse: 0.79591 | val_1_rmse: 0.78522 |  0:00:17s
epoch 54 | loss: 0.61231 | val_0_rmse: 0.79803 | val_1_rmse: 0.79435 |  0:00:17s
epoch 55 | loss: 0.61875 | val_0_rmse: 0.80049 | val_1_rmse: 0.79302 |  0:00:18s
epoch 56 | loss: 0.60768 | val_0_rmse: 0.80246 | val_1_rmse: 0.79587 |  0:00:18s
epoch 57 | loss: 0.59972 | val_0_rmse: 0.79917 | val_1_rmse: 0.80054 |  0:00:18s
epoch 58 | loss: 0.6012  | val_0_rmse: 0.79415 | val_1_rmse: 0.79654 |  0:00:18s
epoch 59 | loss: 0.59981 | val_0_rmse: 0.78916 | val_1_rmse: 0.79485 |  0:00:19s
epoch 60 | loss: 0.59606 | val_0_rmse: 0.7856  | val_1_rmse: 0.79069 |  0:00:19s
epoch 61 | loss: 0.59564 | val_0_rmse: 0.78797 | val_1_rmse: 0.78649 |  0:00:19s
epoch 62 | loss: 0.59823 | val_0_rmse: 0.78536 | val_1_rmse: 0.78616 |  0:00:20s
epoch 63 | loss: 0.59863 | val_0_rmse: 0.78372 | val_1_rmse: 0.78583 |  0:00:20s
epoch 64 | loss: 0.60086 | val_0_rmse: 0.78907 | val_1_rmse: 0.79421 |  0:00:20s
epoch 65 | loss: 0.60134 | val_0_rmse: 0.78855 | val_1_rmse: 0.79523 |  0:00:21s
epoch 66 | loss: 0.60499 | val_0_rmse: 0.78598 | val_1_rmse: 0.79278 |  0:00:21s
epoch 67 | loss: 0.60022 | val_0_rmse: 0.78461 | val_1_rmse: 0.80407 |  0:00:21s
epoch 68 | loss: 0.60081 | val_0_rmse: 0.78049 | val_1_rmse: 0.79497 |  0:00:22s
epoch 69 | loss: 0.59585 | val_0_rmse: 0.77991 | val_1_rmse: 0.79081 |  0:00:22s
epoch 70 | loss: 0.59785 | val_0_rmse: 0.78319 | val_1_rmse: 0.78915 |  0:00:22s
epoch 71 | loss: 0.59684 | val_0_rmse: 0.77765 | val_1_rmse: 0.80016 |  0:00:23s
epoch 72 | loss: 0.5954  | val_0_rmse: 0.77617 | val_1_rmse: 0.80204 |  0:00:23s
epoch 73 | loss: 0.59589 | val_0_rmse: 0.77847 | val_1_rmse: 0.80238 |  0:00:23s
epoch 74 | loss: 0.59702 | val_0_rmse: 0.7747  | val_1_rmse: 0.79683 |  0:00:23s
epoch 75 | loss: 0.58973 | val_0_rmse: 0.77337 | val_1_rmse: 0.7978  |  0:00:24s
epoch 76 | loss: 0.58683 | val_0_rmse: 0.76816 | val_1_rmse: 0.79552 |  0:00:24s
epoch 77 | loss: 0.58695 | val_0_rmse: 0.76976 | val_1_rmse: 0.79474 |  0:00:25s
epoch 78 | loss: 0.58891 | val_0_rmse: 0.76925 | val_1_rmse: 0.80071 |  0:00:25s
epoch 79 | loss: 0.58167 | val_0_rmse: 0.76855 | val_1_rmse: 0.79641 |  0:00:25s
epoch 80 | loss: 0.58522 | val_0_rmse: 0.76968 | val_1_rmse: 0.78866 |  0:00:26s
epoch 81 | loss: 0.58266 | val_0_rmse: 0.7679  | val_1_rmse: 0.79153 |  0:00:26s
epoch 82 | loss: 0.58235 | val_0_rmse: 0.7682  | val_1_rmse: 0.79614 |  0:00:26s
epoch 83 | loss: 0.58524 | val_0_rmse: 0.77262 | val_1_rmse: 0.79418 |  0:00:26s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.78522
Best weights from best epoch are automatically used!
ended training at: 14:06:41
Feature importance:
Mean squared error is of 0.051775689480282855
Mean absolute error:0.15807861836402887
MAPE:0.1542507140782094
R2 score:0.32481230456447807
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:06:41
epoch 0  | loss: 1.6937  | val_0_rmse: 1.00685 | val_1_rmse: 0.99376 |  0:00:00s
epoch 1  | loss: 1.1237  | val_0_rmse: 1.00199 | val_1_rmse: 0.9881  |  0:00:00s
epoch 2  | loss: 0.97441 | val_0_rmse: 0.9604  | val_1_rmse: 0.94922 |  0:00:00s
epoch 3  | loss: 0.90205 | val_0_rmse: 0.90715 | val_1_rmse: 0.90468 |  0:00:01s
epoch 4  | loss: 0.78356 | val_0_rmse: 0.86084 | val_1_rmse: 0.85493 |  0:00:01s
epoch 5  | loss: 0.76311 | val_0_rmse: 0.86327 | val_1_rmse: 0.85378 |  0:00:01s
epoch 6  | loss: 0.74876 | val_0_rmse: 0.85154 | val_1_rmse: 0.83085 |  0:00:02s
epoch 7  | loss: 0.72903 | val_0_rmse: 0.8473  | val_1_rmse: 0.82444 |  0:00:02s
epoch 8  | loss: 0.73255 | val_0_rmse: 0.84617 | val_1_rmse: 0.82399 |  0:00:02s
epoch 9  | loss: 0.72118 | val_0_rmse: 0.84877 | val_1_rmse: 0.82503 |  0:00:03s
epoch 10 | loss: 0.71244 | val_0_rmse: 0.84671 | val_1_rmse: 0.82852 |  0:00:03s
epoch 11 | loss: 0.7191  | val_0_rmse: 0.84922 | val_1_rmse: 0.83039 |  0:00:03s
epoch 12 | loss: 0.70345 | val_0_rmse: 0.84157 | val_1_rmse: 0.82363 |  0:00:04s
epoch 13 | loss: 0.70629 | val_0_rmse: 0.85925 | val_1_rmse: 0.83411 |  0:00:04s
epoch 14 | loss: 0.70862 | val_0_rmse: 0.84986 | val_1_rmse: 0.8378  |  0:00:04s
epoch 15 | loss: 0.71158 | val_0_rmse: 0.86135 | val_1_rmse: 0.84988 |  0:00:05s
epoch 16 | loss: 0.70111 | val_0_rmse: 0.83865 | val_1_rmse: 0.83339 |  0:00:05s
epoch 17 | loss: 0.69331 | val_0_rmse: 0.83776 | val_1_rmse: 0.83309 |  0:00:05s
epoch 18 | loss: 0.68787 | val_0_rmse: 0.83846 | val_1_rmse: 0.83333 |  0:00:06s
epoch 19 | loss: 0.68508 | val_0_rmse: 0.83603 | val_1_rmse: 0.83415 |  0:00:06s
epoch 20 | loss: 0.68387 | val_0_rmse: 0.8446  | val_1_rmse: 0.84102 |  0:00:06s
epoch 21 | loss: 0.6756  | val_0_rmse: 0.83556 | val_1_rmse: 0.83483 |  0:00:06s
epoch 22 | loss: 0.6686  | val_0_rmse: 0.85522 | val_1_rmse: 0.85637 |  0:00:07s
epoch 23 | loss: 0.66732 | val_0_rmse: 0.83679 | val_1_rmse: 0.83947 |  0:00:07s
epoch 24 | loss: 0.67581 | val_0_rmse: 0.85365 | val_1_rmse: 0.85719 |  0:00:07s
epoch 25 | loss: 0.67302 | val_0_rmse: 0.84197 | val_1_rmse: 0.846   |  0:00:08s
epoch 26 | loss: 0.67743 | val_0_rmse: 0.85077 | val_1_rmse: 0.85515 |  0:00:08s
epoch 27 | loss: 0.67027 | val_0_rmse: 0.84259 | val_1_rmse: 0.84077 |  0:00:08s
epoch 28 | loss: 0.65871 | val_0_rmse: 0.84867 | val_1_rmse: 0.84666 |  0:00:09s
epoch 29 | loss: 0.65862 | val_0_rmse: 0.84209 | val_1_rmse: 0.84032 |  0:00:09s
epoch 30 | loss: 0.67094 | val_0_rmse: 0.8498  | val_1_rmse: 0.84879 |  0:00:09s
epoch 31 | loss: 0.66422 | val_0_rmse: 0.84486 | val_1_rmse: 0.84037 |  0:00:10s
epoch 32 | loss: 0.65765 | val_0_rmse: 0.84195 | val_1_rmse: 0.83818 |  0:00:10s
epoch 33 | loss: 0.65345 | val_0_rmse: 0.83466 | val_1_rmse: 0.83577 |  0:00:10s
epoch 34 | loss: 0.64486 | val_0_rmse: 0.83977 | val_1_rmse: 0.84255 |  0:00:11s
epoch 35 | loss: 0.64705 | val_0_rmse: 0.83853 | val_1_rmse: 0.8402  |  0:00:11s
epoch 36 | loss: 0.64997 | val_0_rmse: 0.83571 | val_1_rmse: 0.83221 |  0:00:11s
epoch 37 | loss: 0.65118 | val_0_rmse: 0.83805 | val_1_rmse: 0.83643 |  0:00:12s
epoch 38 | loss: 0.65222 | val_0_rmse: 0.85573 | val_1_rmse: 0.8438  |  0:00:12s
epoch 39 | loss: 0.6616  | val_0_rmse: 0.83732 | val_1_rmse: 0.83513 |  0:00:12s
epoch 40 | loss: 0.65899 | val_0_rmse: 0.8285  | val_1_rmse: 0.84177 |  0:00:13s
epoch 41 | loss: 0.65953 | val_0_rmse: 0.82937 | val_1_rmse: 0.83856 |  0:00:13s
epoch 42 | loss: 0.64981 | val_0_rmse: 0.82945 | val_1_rmse: 0.83589 |  0:00:13s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.82363
Best weights from best epoch are automatically used!
ended training at: 14:06:55
Feature importance:
Mean squared error is of 0.04169700422568471
Mean absolute error:0.15569244345782365
MAPE:0.1649173770132018
R2 score:0.29174969496907177
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:06:56
epoch 0  | loss: 2.53363 | val_0_rmse: 1.01312 | val_1_rmse: 0.96912 |  0:00:00s
epoch 1  | loss: 1.28398 | val_0_rmse: 1.01316 | val_1_rmse: 0.96969 |  0:00:01s
epoch 2  | loss: 1.08935 | val_0_rmse: 1.01324 | val_1_rmse: 0.96942 |  0:00:01s
epoch 3  | loss: 1.04008 | val_0_rmse: 1.0131  | val_1_rmse: 0.96895 |  0:00:02s
epoch 4  | loss: 1.03468 | val_0_rmse: 1.01334 | val_1_rmse: 0.96946 |  0:00:03s
epoch 5  | loss: 1.02903 | val_0_rmse: 1.01334 | val_1_rmse: 0.96923 |  0:00:03s
epoch 6  | loss: 1.03442 | val_0_rmse: 1.01332 | val_1_rmse: 0.96948 |  0:00:04s
epoch 7  | loss: 1.03132 | val_0_rmse: 1.01317 | val_1_rmse: 0.96921 |  0:00:05s
epoch 8  | loss: 1.02858 | val_0_rmse: 1.0133  | val_1_rmse: 0.9694  |  0:00:05s
epoch 9  | loss: 1.02959 | val_0_rmse: 1.01308 | val_1_rmse: 0.96958 |  0:00:06s
epoch 10 | loss: 1.02828 | val_0_rmse: 1.01311 | val_1_rmse: 0.96941 |  0:00:07s
epoch 11 | loss: 1.03006 | val_0_rmse: 1.01324 | val_1_rmse: 0.96935 |  0:00:07s
epoch 12 | loss: 1.02758 | val_0_rmse: 1.01328 | val_1_rmse: 0.9694  |  0:00:08s
epoch 13 | loss: 1.02832 | val_0_rmse: 1.01328 | val_1_rmse: 0.96938 |  0:00:08s
epoch 14 | loss: 1.02859 | val_0_rmse: 1.01332 | val_1_rmse: 0.9698  |  0:00:09s
epoch 15 | loss: 1.02632 | val_0_rmse: 1.01322 | val_1_rmse: 0.96938 |  0:00:10s
epoch 16 | loss: 1.02804 | val_0_rmse: 1.01317 | val_1_rmse: 0.96966 |  0:00:10s
epoch 17 | loss: 1.02152 | val_0_rmse: 1.01356 | val_1_rmse: 0.97058 |  0:00:11s
epoch 18 | loss: 1.02576 | val_0_rmse: 1.0132  | val_1_rmse: 0.9698  |  0:00:12s
epoch 19 | loss: 1.02345 | val_0_rmse: 1.01325 | val_1_rmse: 0.96985 |  0:00:12s
epoch 20 | loss: 1.02562 | val_0_rmse: 1.01304 | val_1_rmse: 0.96925 |  0:00:13s
epoch 21 | loss: 1.02274 | val_0_rmse: 1.01307 | val_1_rmse: 0.96949 |  0:00:13s
epoch 22 | loss: 1.02484 | val_0_rmse: 1.01275 | val_1_rmse: 0.96989 |  0:00:14s
epoch 23 | loss: 1.02169 | val_0_rmse: 1.01281 | val_1_rmse: 0.96984 |  0:00:15s
epoch 24 | loss: 1.02314 | val_0_rmse: 1.01286 | val_1_rmse: 0.96953 |  0:00:15s
epoch 25 | loss: 1.02925 | val_0_rmse: 1.01349 | val_1_rmse: 0.97    |  0:00:16s
epoch 26 | loss: 1.02284 | val_0_rmse: 1.01294 | val_1_rmse: 0.96991 |  0:00:17s
epoch 27 | loss: 1.01961 | val_0_rmse: 1.01245 | val_1_rmse: 0.97129 |  0:00:17s
epoch 28 | loss: 1.02095 | val_0_rmse: 1.01508 | val_1_rmse: 0.97097 |  0:00:18s
epoch 29 | loss: 1.0222  | val_0_rmse: 1.01433 | val_1_rmse: 0.97137 |  0:00:18s
epoch 30 | loss: 1.02204 | val_0_rmse: 1.01093 | val_1_rmse: 0.97016 |  0:00:19s
epoch 31 | loss: 1.02202 | val_0_rmse: 1.00874 | val_1_rmse: 0.96893 |  0:00:20s
epoch 32 | loss: 1.02232 | val_0_rmse: 1.0104  | val_1_rmse: 0.96842 |  0:00:20s
epoch 33 | loss: 1.01744 | val_0_rmse: 1.00496 | val_1_rmse: 0.96548 |  0:00:21s
epoch 34 | loss: 1.01572 | val_0_rmse: 1.0032  | val_1_rmse: 0.96164 |  0:00:22s
epoch 35 | loss: 1.01037 | val_0_rmse: 1.00001 | val_1_rmse: 0.96079 |  0:00:22s
epoch 36 | loss: 1.0067  | val_0_rmse: 0.99725 | val_1_rmse: 0.95941 |  0:00:23s
epoch 37 | loss: 1.00739 | val_0_rmse: 0.99531 | val_1_rmse: 0.9612  |  0:00:24s
epoch 38 | loss: 1.00377 | val_0_rmse: 0.99457 | val_1_rmse: 0.95604 |  0:00:24s
epoch 39 | loss: 0.99464 | val_0_rmse: 0.98858 | val_1_rmse: 0.95414 |  0:00:25s
epoch 40 | loss: 0.98583 | val_0_rmse: 0.98596 | val_1_rmse: 0.95093 |  0:00:26s
epoch 41 | loss: 0.97479 | val_0_rmse: 0.98073 | val_1_rmse: 0.94838 |  0:00:26s
epoch 42 | loss: 0.96907 | val_0_rmse: 0.97902 | val_1_rmse: 0.94721 |  0:00:27s
epoch 43 | loss: 0.95259 | val_0_rmse: 0.97065 | val_1_rmse: 0.94193 |  0:00:27s
epoch 44 | loss: 0.93533 | val_0_rmse: 0.96444 | val_1_rmse: 0.93103 |  0:00:28s
epoch 45 | loss: 0.92334 | val_0_rmse: 0.94444 | val_1_rmse: 0.9253  |  0:00:29s
epoch 46 | loss: 0.89561 | val_0_rmse: 0.95701 | val_1_rmse: 0.93155 |  0:00:29s
epoch 47 | loss: 0.87425 | val_0_rmse: 0.9248  | val_1_rmse: 0.93578 |  0:00:30s
epoch 48 | loss: 0.85604 | val_0_rmse: 0.87492 | val_1_rmse: 0.91304 |  0:00:31s
epoch 49 | loss: 0.80487 | val_0_rmse: 0.93516 | val_1_rmse: 0.90177 |  0:00:31s
epoch 50 | loss: 0.81465 | val_0_rmse: 0.90155 | val_1_rmse: 0.92305 |  0:00:32s
epoch 51 | loss: 0.80619 | val_0_rmse: 0.90724 | val_1_rmse: 0.93155 |  0:00:33s
epoch 52 | loss: 0.7946  | val_0_rmse: 0.89878 | val_1_rmse: 0.91771 |  0:00:33s
epoch 53 | loss: 0.77717 | val_0_rmse: 0.88919 | val_1_rmse: 0.92207 |  0:00:34s
epoch 54 | loss: 0.77062 | val_0_rmse: 0.88488 | val_1_rmse: 0.89857 |  0:00:35s
epoch 55 | loss: 0.75348 | val_0_rmse: 0.87576 | val_1_rmse: 0.88344 |  0:00:35s
epoch 56 | loss: 0.7411  | val_0_rmse: 0.8725  | val_1_rmse: 0.8814  |  0:00:36s
epoch 57 | loss: 0.73302 | val_0_rmse: 0.8589  | val_1_rmse: 0.88465 |  0:00:36s
epoch 58 | loss: 0.721   | val_0_rmse: 0.85814 | val_1_rmse: 0.8828  |  0:00:37s
epoch 59 | loss: 0.717   | val_0_rmse: 0.86431 | val_1_rmse: 0.88116 |  0:00:38s
epoch 60 | loss: 0.71401 | val_0_rmse: 0.85167 | val_1_rmse: 0.8855  |  0:00:38s
epoch 61 | loss: 0.70563 | val_0_rmse: 0.84528 | val_1_rmse: 0.88822 |  0:00:39s
epoch 62 | loss: 0.70127 | val_0_rmse: 0.84146 | val_1_rmse: 0.88595 |  0:00:40s
epoch 63 | loss: 0.70841 | val_0_rmse: 0.83878 | val_1_rmse: 0.88893 |  0:00:40s
epoch 64 | loss: 0.71322 | val_0_rmse: 0.85414 | val_1_rmse: 0.90274 |  0:00:41s
epoch 65 | loss: 0.70329 | val_0_rmse: 0.8527  | val_1_rmse: 0.88803 |  0:00:42s
epoch 66 | loss: 0.72477 | val_0_rmse: 0.84341 | val_1_rmse: 0.88241 |  0:00:42s
epoch 67 | loss: 0.69502 | val_0_rmse: 0.83837 | val_1_rmse: 0.88303 |  0:00:43s
epoch 68 | loss: 0.69566 | val_0_rmse: 0.83552 | val_1_rmse: 0.89187 |  0:00:43s
epoch 69 | loss: 0.6775  | val_0_rmse: 0.82926 | val_1_rmse: 0.88215 |  0:00:44s
epoch 70 | loss: 0.66685 | val_0_rmse: 0.82902 | val_1_rmse: 0.87832 |  0:00:45s
epoch 71 | loss: 0.6621  | val_0_rmse: 0.83008 | val_1_rmse: 0.87316 |  0:00:45s
epoch 72 | loss: 0.65832 | val_0_rmse: 0.81634 | val_1_rmse: 0.87327 |  0:00:46s
epoch 73 | loss: 0.65384 | val_0_rmse: 0.81339 | val_1_rmse: 0.87821 |  0:00:47s
epoch 74 | loss: 0.64486 | val_0_rmse: 0.80745 | val_1_rmse: 0.87799 |  0:00:47s
epoch 75 | loss: 0.64091 | val_0_rmse: 0.8036  | val_1_rmse: 0.87703 |  0:00:48s
epoch 76 | loss: 0.6362  | val_0_rmse: 0.80205 | val_1_rmse: 0.88033 |  0:00:48s
epoch 77 | loss: 0.63543 | val_0_rmse: 0.79888 | val_1_rmse: 0.87378 |  0:00:49s
epoch 78 | loss: 0.63069 | val_0_rmse: 0.80032 | val_1_rmse: 0.89156 |  0:00:50s
epoch 79 | loss: 0.63382 | val_0_rmse: 0.79169 | val_1_rmse: 0.87854 |  0:00:50s
epoch 80 | loss: 0.62484 | val_0_rmse: 0.79111 | val_1_rmse: 0.88357 |  0:00:51s
epoch 81 | loss: 0.62769 | val_0_rmse: 0.78353 | val_1_rmse: 0.88163 |  0:00:52s
epoch 82 | loss: 0.61767 | val_0_rmse: 0.78673 | val_1_rmse: 0.87839 |  0:00:52s
epoch 83 | loss: 0.61513 | val_0_rmse: 0.78097 | val_1_rmse: 0.88749 |  0:00:53s
epoch 84 | loss: 0.61763 | val_0_rmse: 0.7768  | val_1_rmse: 0.87712 |  0:00:54s
epoch 85 | loss: 0.61967 | val_0_rmse: 0.77729 | val_1_rmse: 0.88402 |  0:00:54s
epoch 86 | loss: 0.60605 | val_0_rmse: 0.76957 | val_1_rmse: 0.88596 |  0:00:55s
epoch 87 | loss: 0.60564 | val_0_rmse: 0.7685  | val_1_rmse: 0.88347 |  0:00:56s
epoch 88 | loss: 0.60321 | val_0_rmse: 0.76831 | val_1_rmse: 0.89916 |  0:00:56s
epoch 89 | loss: 0.59052 | val_0_rmse: 0.7707  | val_1_rmse: 0.89289 |  0:00:57s
epoch 90 | loss: 0.59317 | val_0_rmse: 0.75981 | val_1_rmse: 0.89997 |  0:00:57s
epoch 91 | loss: 0.59021 | val_0_rmse: 0.75863 | val_1_rmse: 0.90109 |  0:00:58s
epoch 92 | loss: 0.58037 | val_0_rmse: 0.75662 | val_1_rmse: 0.89698 |  0:00:59s
epoch 93 | loss: 0.59096 | val_0_rmse: 0.75992 | val_1_rmse: 0.91359 |  0:00:59s
epoch 94 | loss: 0.59185 | val_0_rmse: 0.76571 | val_1_rmse: 0.91825 |  0:01:00s
epoch 95 | loss: 0.58565 | val_0_rmse: 0.75092 | val_1_rmse: 0.89944 |  0:01:01s
epoch 96 | loss: 0.5695  | val_0_rmse: 0.74696 | val_1_rmse: 0.89846 |  0:01:01s
epoch 97 | loss: 0.57199 | val_0_rmse: 0.74189 | val_1_rmse: 0.89443 |  0:01:02s
epoch 98 | loss: 0.56858 | val_0_rmse: 0.73718 | val_1_rmse: 0.92102 |  0:01:02s
epoch 99 | loss: 0.56602 | val_0_rmse: 0.7371  | val_1_rmse: 0.90673 |  0:01:03s
epoch 100| loss: 0.56199 | val_0_rmse: 0.74428 | val_1_rmse: 0.92528 |  0:01:04s
epoch 101| loss: 0.55734 | val_0_rmse: 0.7496  | val_1_rmse: 0.92954 |  0:01:04s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.87316
Best weights from best epoch are automatically used!
ended training at: 14:08:01
Feature importance:
Mean squared error is of 0.0551442918914331
Mean absolute error:0.17302021668546388
MAPE:0.1802921403633217
R2 score:0.33274887323407265
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:08:02
epoch 0  | loss: 2.27082 | val_0_rmse: 1.00499 | val_1_rmse: 1.00404 |  0:00:00s
epoch 1  | loss: 1.31321 | val_0_rmse: 1.00492 | val_1_rmse: 1.00472 |  0:00:01s
epoch 2  | loss: 1.1236  | val_0_rmse: 1.00404 | val_1_rmse: 1.00239 |  0:00:01s
epoch 3  | loss: 1.0474  | val_0_rmse: 1.00412 | val_1_rmse: 1.00206 |  0:00:02s
epoch 4  | loss: 1.01716 | val_0_rmse: 1.00421 | val_1_rmse: 1.00248 |  0:00:03s
epoch 5  | loss: 1.01394 | val_0_rmse: 1.00413 | val_1_rmse: 1.00181 |  0:00:03s
epoch 6  | loss: 1.01182 | val_0_rmse: 1.00381 | val_1_rmse: 1.00165 |  0:00:04s
epoch 7  | loss: 1.01275 | val_0_rmse: 1.00382 | val_1_rmse: 1.0015  |  0:00:05s
epoch 8  | loss: 1.01185 | val_0_rmse: 1.00425 | val_1_rmse: 1.00273 |  0:00:05s
epoch 9  | loss: 1.00906 | val_0_rmse: 1.00436 | val_1_rmse: 1.00242 |  0:00:06s
epoch 10 | loss: 1.00883 | val_0_rmse: 1.00403 | val_1_rmse: 1.00189 |  0:00:07s
epoch 11 | loss: 1.00943 | val_0_rmse: 1.0041  | val_1_rmse: 1.00244 |  0:00:07s
epoch 12 | loss: 1.00842 | val_0_rmse: 1.00391 | val_1_rmse: 1.00156 |  0:00:08s
epoch 13 | loss: 1.00694 | val_0_rmse: 1.0041  | val_1_rmse: 1.00198 |  0:00:08s
epoch 14 | loss: 1.00856 | val_0_rmse: 1.00425 | val_1_rmse: 1.00236 |  0:00:09s
epoch 15 | loss: 1.00843 | val_0_rmse: 1.00397 | val_1_rmse: 1.00176 |  0:00:10s
epoch 16 | loss: 1.00759 | val_0_rmse: 1.00395 | val_1_rmse: 1.00168 |  0:00:10s
epoch 17 | loss: 1.00835 | val_0_rmse: 1.00393 | val_1_rmse: 1.00159 |  0:00:11s
epoch 18 | loss: 1.00828 | val_0_rmse: 1.00403 | val_1_rmse: 1.00226 |  0:00:12s
epoch 19 | loss: 1.00775 | val_0_rmse: 1.00382 | val_1_rmse: 1.00156 |  0:00:12s
epoch 20 | loss: 1.00751 | val_0_rmse: 1.00385 | val_1_rmse: 1.00213 |  0:00:13s
epoch 21 | loss: 1.00858 | val_0_rmse: 1.00405 | val_1_rmse: 1.00234 |  0:00:14s
epoch 22 | loss: 1.00902 | val_0_rmse: 1.00416 | val_1_rmse: 1.0018  |  0:00:14s
epoch 23 | loss: 1.00719 | val_0_rmse: 1.00442 | val_1_rmse: 1.00227 |  0:00:15s
epoch 24 | loss: 1.00706 | val_0_rmse: 1.00404 | val_1_rmse: 1.00202 |  0:00:16s
epoch 25 | loss: 1.00601 | val_0_rmse: 1.00401 | val_1_rmse: 1.00238 |  0:00:16s
epoch 26 | loss: 1.00714 | val_0_rmse: 1.00413 | val_1_rmse: 1.0027  |  0:00:17s
epoch 27 | loss: 1.00537 | val_0_rmse: 1.00366 | val_1_rmse: 1.00169 |  0:00:17s
epoch 28 | loss: 1.00677 | val_0_rmse: 1.0027  | val_1_rmse: 1.00136 |  0:00:18s
epoch 29 | loss: 1.00709 | val_0_rmse: 1.00347 | val_1_rmse: 1.00149 |  0:00:19s
epoch 30 | loss: 1.00763 | val_0_rmse: 1.00342 | val_1_rmse: 1.00232 |  0:00:19s
epoch 31 | loss: 1.00623 | val_0_rmse: 1.00358 | val_1_rmse: 1.00307 |  0:00:20s
epoch 32 | loss: 1.00552 | val_0_rmse: 1.00333 | val_1_rmse: 1.00319 |  0:00:21s
epoch 33 | loss: 1.00416 | val_0_rmse: 1.00315 | val_1_rmse: 1.00244 |  0:00:21s
epoch 34 | loss: 1.00339 | val_0_rmse: 1.0029  | val_1_rmse: 1.00219 |  0:00:22s
epoch 35 | loss: 1.00478 | val_0_rmse: 1.00299 | val_1_rmse: 1.00175 |  0:00:22s
epoch 36 | loss: 1.00556 | val_0_rmse: 1.0032  | val_1_rmse: 1.00162 |  0:00:23s
epoch 37 | loss: 1.00236 | val_0_rmse: 1.00252 | val_1_rmse: 1.00351 |  0:00:24s
epoch 38 | loss: 1.00456 | val_0_rmse: 1.00239 | val_1_rmse: 1.00163 |  0:00:24s
epoch 39 | loss: 1.00362 | val_0_rmse: 0.99763 | val_1_rmse: 0.99969 |  0:00:25s
epoch 40 | loss: 1.00179 | val_0_rmse: 0.99426 | val_1_rmse: 1.00226 |  0:00:26s
epoch 41 | loss: 0.99904 | val_0_rmse: 1.00325 | val_1_rmse: 1.0028  |  0:00:26s
epoch 42 | loss: 0.99893 | val_0_rmse: 0.99883 | val_1_rmse: 0.99946 |  0:00:27s
epoch 43 | loss: 1.0001  | val_0_rmse: 0.99389 | val_1_rmse: 1.00149 |  0:00:28s
epoch 44 | loss: 0.99254 | val_0_rmse: 0.98561 | val_1_rmse: 0.99638 |  0:00:28s
epoch 45 | loss: 0.98532 | val_0_rmse: 0.97834 | val_1_rmse: 0.99807 |  0:00:29s
epoch 46 | loss: 0.97312 | val_0_rmse: 0.97546 | val_1_rmse: 0.99375 |  0:00:30s
epoch 47 | loss: 0.95831 | val_0_rmse: 0.96334 | val_1_rmse: 0.97997 |  0:00:30s
epoch 48 | loss: 0.9382  | val_0_rmse: 0.95071 | val_1_rmse: 0.96811 |  0:00:31s
epoch 49 | loss: 0.90541 | val_0_rmse: 0.92036 | val_1_rmse: 0.94036 |  0:00:31s
epoch 50 | loss: 0.89645 | val_0_rmse: 0.96117 | val_1_rmse: 0.96723 |  0:00:32s
epoch 51 | loss: 0.9291  | val_0_rmse: 0.95939 | val_1_rmse: 0.96744 |  0:00:33s
epoch 52 | loss: 0.91959 | val_0_rmse: 0.95491 | val_1_rmse: 0.96915 |  0:00:33s
epoch 53 | loss: 0.90906 | val_0_rmse: 0.94654 | val_1_rmse: 0.95772 |  0:00:34s
epoch 54 | loss: 0.89429 | val_0_rmse: 0.94395 | val_1_rmse: 0.95567 |  0:00:35s
epoch 55 | loss: 0.88819 | val_0_rmse: 0.93841 | val_1_rmse: 0.95277 |  0:00:35s
epoch 56 | loss: 0.87373 | val_0_rmse: 0.93472 | val_1_rmse: 0.95269 |  0:00:36s
epoch 57 | loss: 0.87482 | val_0_rmse: 0.92685 | val_1_rmse: 0.94578 |  0:00:37s
epoch 58 | loss: 0.86436 | val_0_rmse: 0.90899 | val_1_rmse: 0.92745 |  0:00:37s
epoch 59 | loss: 0.82276 | val_0_rmse: 0.89278 | val_1_rmse: 0.93395 |  0:00:38s
epoch 60 | loss: 0.81348 | val_0_rmse: 0.87614 | val_1_rmse: 0.90529 |  0:00:38s
epoch 61 | loss: 0.82948 | val_0_rmse: 0.94317 | val_1_rmse: 0.95063 |  0:00:39s
epoch 62 | loss: 0.87839 | val_0_rmse: 0.94313 | val_1_rmse: 0.95204 |  0:00:40s
epoch 63 | loss: 0.87065 | val_0_rmse: 0.93552 | val_1_rmse: 0.94205 |  0:00:40s
epoch 64 | loss: 0.86156 | val_0_rmse: 0.9332  | val_1_rmse: 0.94277 |  0:00:41s
epoch 65 | loss: 0.85109 | val_0_rmse: 0.92815 | val_1_rmse: 0.94079 |  0:00:42s
epoch 66 | loss: 0.84644 | val_0_rmse: 0.92464 | val_1_rmse: 0.93784 |  0:00:42s
epoch 67 | loss: 0.8327  | val_0_rmse: 0.921   | val_1_rmse: 0.94627 |  0:00:43s
epoch 68 | loss: 0.83273 | val_0_rmse: 0.91582 | val_1_rmse: 0.93704 |  0:00:44s
epoch 69 | loss: 0.81851 | val_0_rmse: 0.91752 | val_1_rmse: 0.94867 |  0:00:44s
epoch 70 | loss: 0.81491 | val_0_rmse: 0.90975 | val_1_rmse: 0.93911 |  0:00:45s
epoch 71 | loss: 0.8066  | val_0_rmse: 0.90619 | val_1_rmse: 0.95161 |  0:00:46s
epoch 72 | loss: 0.80004 | val_0_rmse: 0.89784 | val_1_rmse: 0.93597 |  0:00:46s
epoch 73 | loss: 0.79016 | val_0_rmse: 0.89101 | val_1_rmse: 0.94775 |  0:00:47s
epoch 74 | loss: 0.78414 | val_0_rmse: 0.88487 | val_1_rmse: 0.93141 |  0:00:47s
epoch 75 | loss: 0.76946 | val_0_rmse: 0.87677 | val_1_rmse: 0.93869 |  0:00:48s
epoch 76 | loss: 0.76912 | val_0_rmse: 0.86189 | val_1_rmse: 0.91384 |  0:00:49s
epoch 77 | loss: 0.75889 | val_0_rmse: 0.83201 | val_1_rmse: 0.90543 |  0:00:49s
epoch 78 | loss: 0.70683 | val_0_rmse: 0.88949 | val_1_rmse: 0.93896 |  0:00:50s
epoch 79 | loss: 0.72909 | val_0_rmse: 0.85225 | val_1_rmse: 0.91932 |  0:00:51s
epoch 80 | loss: 0.72556 | val_0_rmse: 0.82829 | val_1_rmse: 0.8877  |  0:00:51s
epoch 81 | loss: 0.70922 | val_0_rmse: 0.83909 | val_1_rmse: 0.9128  |  0:00:52s
epoch 82 | loss: 0.6917  | val_0_rmse: 0.81457 | val_1_rmse: 0.88392 |  0:00:52s
epoch 83 | loss: 0.67861 | val_0_rmse: 0.82444 | val_1_rmse: 0.88682 |  0:00:53s
epoch 84 | loss: 0.65817 | val_0_rmse: 0.80139 | val_1_rmse: 0.89769 |  0:00:54s
epoch 85 | loss: 0.63685 | val_0_rmse: 0.78759 | val_1_rmse: 0.88894 |  0:00:54s
epoch 86 | loss: 0.62041 | val_0_rmse: 0.78497 | val_1_rmse: 0.8884  |  0:00:55s
epoch 87 | loss: 0.62197 | val_0_rmse: 0.78406 | val_1_rmse: 0.88991 |  0:00:56s
epoch 88 | loss: 0.61906 | val_0_rmse: 0.78306 | val_1_rmse: 0.91715 |  0:00:56s
epoch 89 | loss: 0.60952 | val_0_rmse: 0.78107 | val_1_rmse: 0.89578 |  0:00:57s
epoch 90 | loss: 0.60537 | val_0_rmse: 0.77931 | val_1_rmse: 0.92055 |  0:00:58s
epoch 91 | loss: 0.60908 | val_0_rmse: 0.77686 | val_1_rmse: 0.91063 |  0:00:58s
epoch 92 | loss: 0.59897 | val_0_rmse: 0.75888 | val_1_rmse: 0.90333 |  0:00:59s
epoch 93 | loss: 0.60307 | val_0_rmse: 0.76485 | val_1_rmse: 0.89411 |  0:00:59s
epoch 94 | loss: 0.60434 | val_0_rmse: 0.75095 | val_1_rmse: 0.90772 |  0:01:00s
epoch 95 | loss: 0.58789 | val_0_rmse: 0.76613 | val_1_rmse: 0.89417 |  0:01:01s
epoch 96 | loss: 0.5938  | val_0_rmse: 0.7521  | val_1_rmse: 0.89978 |  0:01:01s
epoch 97 | loss: 0.59304 | val_0_rmse: 0.7503  | val_1_rmse: 0.91447 |  0:01:02s
epoch 98 | loss: 0.57562 | val_0_rmse: 0.73797 | val_1_rmse: 0.90373 |  0:01:03s
epoch 99 | loss: 0.57253 | val_0_rmse: 0.73481 | val_1_rmse: 0.9005  |  0:01:03s
epoch 100| loss: 0.57003 | val_0_rmse: 0.73223 | val_1_rmse: 0.93139 |  0:01:04s
epoch 101| loss: 0.56262 | val_0_rmse: 0.74582 | val_1_rmse: 0.89971 |  0:01:05s
epoch 102| loss: 0.56762 | val_0_rmse: 0.73231 | val_1_rmse: 0.921   |  0:01:05s
epoch 103| loss: 0.55249 | val_0_rmse: 0.7239  | val_1_rmse: 0.91548 |  0:01:06s
epoch 104| loss: 0.54881 | val_0_rmse: 0.7276  | val_1_rmse: 0.91092 |  0:01:06s
epoch 105| loss: 0.55181 | val_0_rmse: 0.75462 | val_1_rmse: 0.97194 |  0:01:07s
epoch 106| loss: 0.5513  | val_0_rmse: 0.7272  | val_1_rmse: 0.93016 |  0:01:08s
epoch 107| loss: 0.5486  | val_0_rmse: 0.73074 | val_1_rmse: 0.93913 |  0:01:08s
epoch 108| loss: 0.55916 | val_0_rmse: 0.74773 | val_1_rmse: 0.94071 |  0:01:09s
epoch 109| loss: 0.58891 | val_0_rmse: 0.7615  | val_1_rmse: 0.95748 |  0:01:10s
epoch 110| loss: 0.5803  | val_0_rmse: 0.74988 | val_1_rmse: 0.91112 |  0:01:10s
epoch 111| loss: 0.57114 | val_0_rmse: 0.74297 | val_1_rmse: 0.90424 |  0:01:11s
epoch 112| loss: 0.55625 | val_0_rmse: 0.73242 | val_1_rmse: 0.91411 |  0:01:11s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.88392
Best weights from best epoch are automatically used!
ended training at: 14:09:14
Feature importance:
Mean squared error is of 0.058664316613788216
Mean absolute error:0.17885754599030168
MAPE:0.18163310431025875
R2 score:0.31973523873348875
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:09:14
epoch 0  | loss: 2.18962 | val_0_rmse: 0.97892 | val_1_rmse: 0.97729 |  0:00:00s
epoch 1  | loss: 1.16624 | val_0_rmse: 0.97759 | val_1_rmse: 0.97484 |  0:00:01s
epoch 2  | loss: 1.01976 | val_0_rmse: 0.97833 | val_1_rmse: 0.97509 |  0:00:02s
epoch 3  | loss: 0.97472 | val_0_rmse: 0.97738 | val_1_rmse: 0.97443 |  0:00:02s
epoch 4  | loss: 0.96528 | val_0_rmse: 0.97777 | val_1_rmse: 0.97502 |  0:00:03s
epoch 5  | loss: 0.96347 | val_0_rmse: 0.97792 | val_1_rmse: 0.97543 |  0:00:03s
epoch 6  | loss: 0.96108 | val_0_rmse: 0.978   | val_1_rmse: 0.97456 |  0:00:04s
epoch 7  | loss: 0.96186 | val_0_rmse: 0.97805 | val_1_rmse: 0.97495 |  0:00:05s
epoch 8  | loss: 0.95877 | val_0_rmse: 0.97827 | val_1_rmse: 0.97568 |  0:00:05s
epoch 9  | loss: 0.95798 | val_0_rmse: 0.97799 | val_1_rmse: 0.97459 |  0:00:06s
epoch 10 | loss: 0.95777 | val_0_rmse: 0.97775 | val_1_rmse: 0.97544 |  0:00:07s
epoch 11 | loss: 0.9592  | val_0_rmse: 0.97775 | val_1_rmse: 0.97476 |  0:00:07s
epoch 12 | loss: 0.95683 | val_0_rmse: 0.97787 | val_1_rmse: 0.97487 |  0:00:08s
epoch 13 | loss: 0.95691 | val_0_rmse: 0.97774 | val_1_rmse: 0.9747  |  0:00:08s
epoch 14 | loss: 0.9554  | val_0_rmse: 0.97767 | val_1_rmse: 0.9743  |  0:00:09s
epoch 15 | loss: 0.95717 | val_0_rmse: 0.97795 | val_1_rmse: 0.97496 |  0:00:10s
epoch 16 | loss: 0.95689 | val_0_rmse: 0.97796 | val_1_rmse: 0.97501 |  0:00:10s
epoch 17 | loss: 0.95371 | val_0_rmse: 0.97802 | val_1_rmse: 0.97498 |  0:00:11s
epoch 18 | loss: 0.9571  | val_0_rmse: 0.97768 | val_1_rmse: 0.97456 |  0:00:12s
epoch 19 | loss: 0.96116 | val_0_rmse: 0.97758 | val_1_rmse: 0.97444 |  0:00:12s
epoch 20 | loss: 0.96215 | val_0_rmse: 0.98011 | val_1_rmse: 0.9774  |  0:00:13s
epoch 21 | loss: 0.95504 | val_0_rmse: 1.00415 | val_1_rmse: 0.99373 |  0:00:13s
epoch 22 | loss: 0.95906 | val_0_rmse: 0.98367 | val_1_rmse: 0.98061 |  0:00:14s
epoch 23 | loss: 0.95651 | val_0_rmse: 0.98589 | val_1_rmse: 0.98312 |  0:00:15s
epoch 24 | loss: 0.95577 | val_0_rmse: 0.97643 | val_1_rmse: 0.97287 |  0:00:15s
epoch 25 | loss: 0.95346 | val_0_rmse: 0.97553 | val_1_rmse: 0.97197 |  0:00:16s
epoch 26 | loss: 0.95265 | val_0_rmse: 0.97864 | val_1_rmse: 0.97512 |  0:00:17s
epoch 27 | loss: 0.95738 | val_0_rmse: 0.98107 | val_1_rmse: 0.97623 |  0:00:17s
epoch 28 | loss: 0.9539  | val_0_rmse: 0.97909 | val_1_rmse: 0.97501 |  0:00:18s
epoch 29 | loss: 0.95354 | val_0_rmse: 0.97715 | val_1_rmse: 0.97397 |  0:00:19s
epoch 30 | loss: 0.95038 | val_0_rmse: 0.97595 | val_1_rmse: 0.97248 |  0:00:19s
epoch 31 | loss: 0.95012 | val_0_rmse: 0.97416 | val_1_rmse: 0.97118 |  0:00:20s
epoch 32 | loss: 0.94496 | val_0_rmse: 0.96765 | val_1_rmse: 0.96413 |  0:00:21s
epoch 33 | loss: 0.93965 | val_0_rmse: 0.96148 | val_1_rmse: 0.95922 |  0:00:21s
epoch 34 | loss: 0.93229 | val_0_rmse: 0.94388 | val_1_rmse: 0.94411 |  0:00:22s
epoch 35 | loss: 0.9188  | val_0_rmse: 0.94218 | val_1_rmse: 0.94013 |  0:00:22s
epoch 36 | loss: 0.90828 | val_0_rmse: 0.93756 | val_1_rmse: 0.93525 |  0:00:23s
epoch 37 | loss: 0.88858 | val_0_rmse: 0.93668 | val_1_rmse: 0.93298 |  0:00:24s
epoch 38 | loss: 0.87518 | val_0_rmse: 0.93378 | val_1_rmse: 0.92942 |  0:00:24s
epoch 39 | loss: 0.87177 | val_0_rmse: 0.93515 | val_1_rmse: 0.92909 |  0:00:25s
epoch 40 | loss: 0.8615  | val_0_rmse: 0.93314 | val_1_rmse: 0.92523 |  0:00:26s
epoch 41 | loss: 0.86694 | val_0_rmse: 0.93381 | val_1_rmse: 0.92803 |  0:00:26s
epoch 42 | loss: 0.86079 | val_0_rmse: 0.93755 | val_1_rmse: 0.93305 |  0:00:27s
epoch 43 | loss: 0.85567 | val_0_rmse: 0.93015 | val_1_rmse: 0.92766 |  0:00:28s
epoch 44 | loss: 0.85061 | val_0_rmse: 0.92579 | val_1_rmse: 0.92345 |  0:00:28s
epoch 45 | loss: 0.8369  | val_0_rmse: 0.93186 | val_1_rmse: 0.92965 |  0:00:29s
epoch 46 | loss: 0.83679 | val_0_rmse: 0.92201 | val_1_rmse: 0.91882 |  0:00:29s
epoch 47 | loss: 0.84082 | val_0_rmse: 0.92486 | val_1_rmse: 0.9196  |  0:00:30s
epoch 48 | loss: 0.82151 | val_0_rmse: 0.9207  | val_1_rmse: 0.91842 |  0:00:31s
epoch 49 | loss: 0.81849 | val_0_rmse: 0.90102 | val_1_rmse: 0.90769 |  0:00:31s
epoch 50 | loss: 0.81439 | val_0_rmse: 0.89429 | val_1_rmse: 0.90036 |  0:00:32s
epoch 51 | loss: 0.80695 | val_0_rmse: 0.89145 | val_1_rmse: 0.89916 |  0:00:33s
epoch 52 | loss: 0.78007 | val_0_rmse: 0.89137 | val_1_rmse: 0.91647 |  0:00:33s
epoch 53 | loss: 0.76928 | val_0_rmse: 0.87627 | val_1_rmse: 0.92247 |  0:00:34s
epoch 54 | loss: 0.7371  | val_0_rmse: 0.85914 | val_1_rmse: 0.89271 |  0:00:35s
epoch 55 | loss: 0.72323 | val_0_rmse: 0.92194 | val_1_rmse: 0.94171 |  0:00:35s
epoch 56 | loss: 0.79148 | val_0_rmse: 0.88005 | val_1_rmse: 0.90133 |  0:00:36s
epoch 57 | loss: 0.74616 | val_0_rmse: 0.84765 | val_1_rmse: 0.89283 |  0:00:37s
epoch 58 | loss: 0.74662 | val_0_rmse: 0.8449  | val_1_rmse: 0.89189 |  0:00:37s
epoch 59 | loss: 0.7204  | val_0_rmse: 0.84413 | val_1_rmse: 0.88285 |  0:00:38s
epoch 60 | loss: 0.68747 | val_0_rmse: 0.86416 | val_1_rmse: 0.89399 |  0:00:38s
epoch 61 | loss: 0.73313 | val_0_rmse: 0.86694 | val_1_rmse: 0.88825 |  0:00:39s
epoch 62 | loss: 0.69797 | val_0_rmse: 0.85614 | val_1_rmse: 0.89401 |  0:00:40s
epoch 63 | loss: 0.68612 | val_0_rmse: 0.83526 | val_1_rmse: 0.87884 |  0:00:40s
epoch 64 | loss: 0.67282 | val_0_rmse: 0.83195 | val_1_rmse: 0.87752 |  0:00:41s
epoch 65 | loss: 0.66424 | val_0_rmse: 0.83343 | val_1_rmse: 0.88231 |  0:00:42s
epoch 66 | loss: 0.65378 | val_0_rmse: 0.8274  | val_1_rmse: 0.88704 |  0:00:42s
epoch 67 | loss: 0.65862 | val_0_rmse: 0.81486 | val_1_rmse: 0.87462 |  0:00:43s
epoch 68 | loss: 0.64112 | val_0_rmse: 0.80881 | val_1_rmse: 0.87577 |  0:00:44s
epoch 69 | loss: 0.63789 | val_0_rmse: 0.81506 | val_1_rmse: 0.88026 |  0:00:44s
epoch 70 | loss: 0.62881 | val_0_rmse: 0.81604 | val_1_rmse: 0.88544 |  0:00:45s
epoch 71 | loss: 0.63327 | val_0_rmse: 0.79839 | val_1_rmse: 0.87462 |  0:00:45s
epoch 72 | loss: 0.62541 | val_0_rmse: 0.80284 | val_1_rmse: 0.88036 |  0:00:46s
epoch 73 | loss: 0.62876 | val_0_rmse: 0.79684 | val_1_rmse: 0.8729  |  0:00:47s
epoch 74 | loss: 0.6182  | val_0_rmse: 0.79192 | val_1_rmse: 0.86984 |  0:00:47s
epoch 75 | loss: 0.61479 | val_0_rmse: 0.78355 | val_1_rmse: 0.86895 |  0:00:48s
epoch 76 | loss: 0.60936 | val_0_rmse: 0.78702 | val_1_rmse: 0.87931 |  0:00:49s
epoch 77 | loss: 0.59978 | val_0_rmse: 0.78375 | val_1_rmse: 0.8719  |  0:00:49s
epoch 78 | loss: 0.59703 | val_0_rmse: 0.78324 | val_1_rmse: 0.87998 |  0:00:50s
epoch 79 | loss: 0.596   | val_0_rmse: 0.77407 | val_1_rmse: 0.87865 |  0:00:51s
epoch 80 | loss: 0.59112 | val_0_rmse: 0.77248 | val_1_rmse: 0.88525 |  0:00:51s
epoch 81 | loss: 0.59049 | val_0_rmse: 0.771   | val_1_rmse: 0.89292 |  0:00:52s
epoch 82 | loss: 0.57656 | val_0_rmse: 0.76515 | val_1_rmse: 0.87361 |  0:00:52s
epoch 83 | loss: 0.58696 | val_0_rmse: 0.76443 | val_1_rmse: 0.8852  |  0:00:53s
epoch 84 | loss: 0.57114 | val_0_rmse: 0.76087 | val_1_rmse: 0.87903 |  0:00:54s
epoch 85 | loss: 0.57005 | val_0_rmse: 0.74731 | val_1_rmse: 0.88388 |  0:00:54s
epoch 86 | loss: 0.57213 | val_0_rmse: 0.75268 | val_1_rmse: 0.88194 |  0:00:55s
epoch 87 | loss: 0.5669  | val_0_rmse: 0.74316 | val_1_rmse: 0.87667 |  0:00:56s
epoch 88 | loss: 0.55548 | val_0_rmse: 0.73611 | val_1_rmse: 0.8889  |  0:00:56s
epoch 89 | loss: 0.55777 | val_0_rmse: 0.73511 | val_1_rmse: 0.88437 |  0:00:57s
epoch 90 | loss: 0.55962 | val_0_rmse: 0.73965 | val_1_rmse: 0.8887  |  0:00:57s
epoch 91 | loss: 0.54848 | val_0_rmse: 0.73749 | val_1_rmse: 0.88808 |  0:00:58s
epoch 92 | loss: 0.55464 | val_0_rmse: 0.75951 | val_1_rmse: 0.91212 |  0:00:59s
epoch 93 | loss: 0.58487 | val_0_rmse: 0.75224 | val_1_rmse: 0.90693 |  0:00:59s
epoch 94 | loss: 0.57809 | val_0_rmse: 0.7402  | val_1_rmse: 0.89975 |  0:01:00s
epoch 95 | loss: 0.56634 | val_0_rmse: 0.73772 | val_1_rmse: 0.90266 |  0:01:01s
epoch 96 | loss: 0.56186 | val_0_rmse: 0.74033 | val_1_rmse: 0.91367 |  0:01:01s
epoch 97 | loss: 0.55658 | val_0_rmse: 0.73014 | val_1_rmse: 0.89601 |  0:01:02s
epoch 98 | loss: 0.55896 | val_0_rmse: 0.72095 | val_1_rmse: 0.89485 |  0:01:02s
epoch 99 | loss: 0.55367 | val_0_rmse: 0.71859 | val_1_rmse: 0.89662 |  0:01:03s
epoch 100| loss: 0.56326 | val_0_rmse: 0.72722 | val_1_rmse: 0.91451 |  0:01:04s
epoch 101| loss: 0.53855 | val_0_rmse: 0.72676 | val_1_rmse: 0.90148 |  0:01:04s
epoch 102| loss: 0.53098 | val_0_rmse: 0.71288 | val_1_rmse: 0.90539 |  0:01:05s
epoch 103| loss: 0.54525 | val_0_rmse: 0.70766 | val_1_rmse: 0.89073 |  0:01:06s
epoch 104| loss: 0.53452 | val_0_rmse: 0.70808 | val_1_rmse: 0.90056 |  0:01:06s
epoch 105| loss: 0.53429 | val_0_rmse: 0.71428 | val_1_rmse: 0.89507 |  0:01:07s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.86895
Best weights from best epoch are automatically used!
ended training at: 14:10:22
Feature importance:
Mean squared error is of 0.06741379401891211
Mean absolute error:0.18009550979570707
MAPE:0.184808402866151
R2 score:0.493783783648007
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:10:23
epoch 0  | loss: 2.31809 | val_0_rmse: 1.01693 | val_1_rmse: 0.96847 |  0:00:00s
epoch 1  | loss: 1.27011 | val_0_rmse: 1.01746 | val_1_rmse: 0.96805 |  0:00:01s
epoch 2  | loss: 1.11329 | val_0_rmse: 1.01645 | val_1_rmse: 0.96632 |  0:00:01s
epoch 3  | loss: 1.05578 | val_0_rmse: 1.01662 | val_1_rmse: 0.96653 |  0:00:02s
epoch 4  | loss: 1.04152 | val_0_rmse: 1.01662 | val_1_rmse: 0.96653 |  0:00:03s
epoch 5  | loss: 1.03738 | val_0_rmse: 1.01647 | val_1_rmse: 0.96651 |  0:00:03s
epoch 6  | loss: 1.03613 | val_0_rmse: 1.01649 | val_1_rmse: 0.96688 |  0:00:04s
epoch 7  | loss: 1.03942 | val_0_rmse: 1.01671 | val_1_rmse: 0.96705 |  0:00:05s
epoch 8  | loss: 1.03946 | val_0_rmse: 1.01709 | val_1_rmse: 0.96773 |  0:00:05s
epoch 9  | loss: 1.03707 | val_0_rmse: 1.01682 | val_1_rmse: 0.96674 |  0:00:06s
epoch 10 | loss: 1.03469 | val_0_rmse: 1.01682 | val_1_rmse: 0.96627 |  0:00:06s
epoch 11 | loss: 1.0352  | val_0_rmse: 1.01685 | val_1_rmse: 0.96649 |  0:00:07s
epoch 12 | loss: 1.03514 | val_0_rmse: 1.01685 | val_1_rmse: 0.9673  |  0:00:08s
epoch 13 | loss: 1.03382 | val_0_rmse: 1.01672 | val_1_rmse: 0.96758 |  0:00:08s
epoch 14 | loss: 1.03645 | val_0_rmse: 1.01694 | val_1_rmse: 0.96689 |  0:00:09s
epoch 15 | loss: 1.03368 | val_0_rmse: 1.01787 | val_1_rmse: 0.96919 |  0:00:10s
epoch 16 | loss: 1.03478 | val_0_rmse: 1.01704 | val_1_rmse: 0.96761 |  0:00:10s
epoch 17 | loss: 1.03341 | val_0_rmse: 1.01656 | val_1_rmse: 0.9669  |  0:00:11s
epoch 18 | loss: 1.03293 | val_0_rmse: 1.01651 | val_1_rmse: 0.96698 |  0:00:12s
epoch 19 | loss: 1.03376 | val_0_rmse: 1.01652 | val_1_rmse: 0.96703 |  0:00:12s
epoch 20 | loss: 1.03473 | val_0_rmse: 1.01659 | val_1_rmse: 0.96732 |  0:00:13s
epoch 21 | loss: 1.03335 | val_0_rmse: 1.01679 | val_1_rmse: 0.96692 |  0:00:14s
epoch 22 | loss: 1.03388 | val_0_rmse: 1.0164  | val_1_rmse: 0.96805 |  0:00:14s
epoch 23 | loss: 1.0334  | val_0_rmse: 1.01646 | val_1_rmse: 0.96675 |  0:00:15s
epoch 24 | loss: 1.03279 | val_0_rmse: 1.01608 | val_1_rmse: 0.96705 |  0:00:15s
epoch 25 | loss: 1.03253 | val_0_rmse: 1.01621 | val_1_rmse: 0.96689 |  0:00:16s
epoch 26 | loss: 1.03106 | val_0_rmse: 1.01628 | val_1_rmse: 0.96682 |  0:00:17s
epoch 27 | loss: 1.03313 | val_0_rmse: 1.01606 | val_1_rmse: 0.96695 |  0:00:17s
epoch 28 | loss: 1.03163 | val_0_rmse: 1.01545 | val_1_rmse: 0.96682 |  0:00:18s
epoch 29 | loss: 1.03172 | val_0_rmse: 1.01504 | val_1_rmse: 0.96645 |  0:00:19s
epoch 30 | loss: 1.02964 | val_0_rmse: 1.01273 | val_1_rmse: 0.96568 |  0:00:19s
epoch 31 | loss: 1.02967 | val_0_rmse: 1.01355 | val_1_rmse: 0.96523 |  0:00:20s
epoch 32 | loss: 1.02974 | val_0_rmse: 1.01233 | val_1_rmse: 0.96658 |  0:00:20s
epoch 33 | loss: 1.0299  | val_0_rmse: 1.01045 | val_1_rmse: 0.96724 |  0:00:21s
epoch 34 | loss: 1.02655 | val_0_rmse: 1.00878 | val_1_rmse: 0.96373 |  0:00:22s
epoch 35 | loss: 1.02352 | val_0_rmse: 1.00316 | val_1_rmse: 0.96095 |  0:00:22s
epoch 36 | loss: 1.02022 | val_0_rmse: 1.00009 | val_1_rmse: 0.95738 |  0:00:23s
epoch 37 | loss: 1.00941 | val_0_rmse: 0.99083 | val_1_rmse: 0.94656 |  0:00:24s
epoch 38 | loss: 1.00156 | val_0_rmse: 0.98188 | val_1_rmse: 0.94808 |  0:00:24s
epoch 39 | loss: 0.98183 | val_0_rmse: 0.97738 | val_1_rmse: 0.93628 |  0:00:25s
epoch 40 | loss: 0.96469 | val_0_rmse: 0.98778 | val_1_rmse: 0.95599 |  0:00:26s
epoch 41 | loss: 0.95495 | val_0_rmse: 0.96676 | val_1_rmse: 0.92741 |  0:00:26s
epoch 42 | loss: 0.93907 | val_0_rmse: 0.95761 | val_1_rmse: 0.93451 |  0:00:27s
epoch 43 | loss: 0.92439 | val_0_rmse: 0.97241 | val_1_rmse: 0.96996 |  0:00:28s
epoch 44 | loss: 0.90499 | val_0_rmse: 0.94863 | val_1_rmse: 0.93073 |  0:00:28s
epoch 45 | loss: 0.90962 | val_0_rmse: 0.94484 | val_1_rmse: 0.93124 |  0:00:29s
epoch 46 | loss: 0.8742  | val_0_rmse: 0.97425 | val_1_rmse: 0.98823 |  0:00:29s
epoch 47 | loss: 0.84776 | val_0_rmse: 0.92743 | val_1_rmse: 0.89456 |  0:00:30s
epoch 48 | loss: 0.84419 | val_0_rmse: 0.90341 | val_1_rmse: 0.92739 |  0:00:31s
epoch 49 | loss: 0.81859 | val_0_rmse: 0.88863 | val_1_rmse: 0.91352 |  0:00:31s
epoch 50 | loss: 0.80331 | val_0_rmse: 0.89515 | val_1_rmse: 0.92351 |  0:00:32s
epoch 51 | loss: 0.82716 | val_0_rmse: 0.91262 | val_1_rmse: 0.88834 |  0:00:33s
epoch 52 | loss: 0.82942 | val_0_rmse: 0.87353 | val_1_rmse: 0.8754  |  0:00:33s
epoch 53 | loss: 0.75892 | val_0_rmse: 0.91079 | val_1_rmse: 0.88407 |  0:00:34s
epoch 54 | loss: 0.79519 | val_0_rmse: 1.07136 | val_1_rmse: 0.98172 |  0:00:35s
epoch 55 | loss: 0.79328 | val_0_rmse: 0.88393 | val_1_rmse: 0.8679  |  0:00:35s
epoch 56 | loss: 0.7518  | val_0_rmse: 0.90206 | val_1_rmse: 0.88046 |  0:00:36s
epoch 57 | loss: 0.76578 | val_0_rmse: 0.92106 | val_1_rmse: 0.91728 |  0:00:36s
epoch 58 | loss: 0.7824  | val_0_rmse: 0.86716 | val_1_rmse: 0.86599 |  0:00:37s
epoch 59 | loss: 0.78087 | val_0_rmse: 0.8573  | val_1_rmse: 0.86914 |  0:00:38s
epoch 60 | loss: 0.75182 | val_0_rmse: 0.86643 | val_1_rmse: 0.9107  |  0:00:38s
epoch 61 | loss: 0.71571 | val_0_rmse: 0.87792 | val_1_rmse: 0.88211 |  0:00:39s
epoch 62 | loss: 0.7362  | val_0_rmse: 0.88078 | val_1_rmse: 0.92867 |  0:00:40s
epoch 63 | loss: 0.71684 | val_0_rmse: 0.84159 | val_1_rmse: 0.8648  |  0:00:40s
epoch 64 | loss: 0.73388 | val_0_rmse: 0.84381 | val_1_rmse: 0.86692 |  0:00:41s
epoch 65 | loss: 0.71195 | val_0_rmse: 0.86002 | val_1_rmse: 0.86954 |  0:00:42s
epoch 66 | loss: 0.72129 | val_0_rmse: 0.85006 | val_1_rmse: 0.89172 |  0:00:42s
epoch 67 | loss: 0.69941 | val_0_rmse: 0.84613 | val_1_rmse: 0.89524 |  0:00:43s
epoch 68 | loss: 0.67336 | val_0_rmse: 0.83043 | val_1_rmse: 0.88099 |  0:00:44s
epoch 69 | loss: 0.68232 | val_0_rmse: 0.82922 | val_1_rmse: 0.88564 |  0:00:44s
epoch 70 | loss: 0.67432 | val_0_rmse: 0.825   | val_1_rmse: 0.87848 |  0:00:45s
epoch 71 | loss: 0.67632 | val_0_rmse: 0.83387 | val_1_rmse: 0.88006 |  0:00:45s
epoch 72 | loss: 0.65741 | val_0_rmse: 0.81084 | val_1_rmse: 0.89269 |  0:00:46s
epoch 73 | loss: 0.65238 | val_0_rmse: 0.81173 | val_1_rmse: 0.89965 |  0:00:47s
epoch 74 | loss: 0.63762 | val_0_rmse: 0.81166 | val_1_rmse: 0.89466 |  0:00:47s
epoch 75 | loss: 0.6473  | val_0_rmse: 0.80788 | val_1_rmse: 0.89711 |  0:00:48s
epoch 76 | loss: 0.63608 | val_0_rmse: 0.80379 | val_1_rmse: 0.88979 |  0:00:49s
epoch 77 | loss: 0.63924 | val_0_rmse: 0.80683 | val_1_rmse: 0.91006 |  0:00:49s
epoch 78 | loss: 0.63068 | val_0_rmse: 0.79471 | val_1_rmse: 0.89847 |  0:00:50s
epoch 79 | loss: 0.62536 | val_0_rmse: 0.79998 | val_1_rmse: 0.91005 |  0:00:50s
epoch 80 | loss: 0.61946 | val_0_rmse: 0.79486 | val_1_rmse: 0.90843 |  0:00:51s
epoch 81 | loss: 0.62151 | val_0_rmse: 0.78464 | val_1_rmse: 0.89123 |  0:00:52s
epoch 82 | loss: 0.61972 | val_0_rmse: 0.80042 | val_1_rmse: 0.90516 |  0:00:52s
epoch 83 | loss: 0.62011 | val_0_rmse: 0.7834  | val_1_rmse: 0.91971 |  0:00:53s
epoch 84 | loss: 0.61805 | val_0_rmse: 0.77796 | val_1_rmse: 0.91367 |  0:00:54s
epoch 85 | loss: 0.6096  | val_0_rmse: 0.78167 | val_1_rmse: 0.9342  |  0:00:54s
epoch 86 | loss: 0.60362 | val_0_rmse: 0.77312 | val_1_rmse: 0.93337 |  0:00:55s
epoch 87 | loss: 0.60443 | val_0_rmse: 0.77219 | val_1_rmse: 0.92627 |  0:00:56s
epoch 88 | loss: 0.59711 | val_0_rmse: 0.76661 | val_1_rmse: 0.92139 |  0:00:56s
epoch 89 | loss: 0.61138 | val_0_rmse: 0.77129 | val_1_rmse: 0.92309 |  0:00:57s
epoch 90 | loss: 0.60022 | val_0_rmse: 0.76634 | val_1_rmse: 0.92391 |  0:00:58s
epoch 91 | loss: 0.5967  | val_0_rmse: 0.76479 | val_1_rmse: 0.91987 |  0:00:58s
epoch 92 | loss: 0.59355 | val_0_rmse: 0.76074 | val_1_rmse: 0.90896 |  0:00:59s
epoch 93 | loss: 0.58866 | val_0_rmse: 0.75563 | val_1_rmse: 0.90988 |  0:00:59s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.8648
Best weights from best epoch are automatically used!
ended training at: 14:11:23
Feature importance:
Mean squared error is of 0.057131975135669776
Mean absolute error:0.17727663599976262
MAPE:0.18423252308426238
R2 score:0.2527347638256343
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:23
epoch 0  | loss: 2.37173 | val_0_rmse: 1.0004  | val_1_rmse: 1.00818 |  0:00:00s
epoch 1  | loss: 1.30739 | val_0_rmse: 0.99745 | val_1_rmse: 1.00607 |  0:00:01s
epoch 2  | loss: 1.04243 | val_0_rmse: 0.99446 | val_1_rmse: 1.00447 |  0:00:01s
epoch 3  | loss: 1.0008  | val_0_rmse: 0.99507 | val_1_rmse: 1.00465 |  0:00:02s
epoch 4  | loss: 0.99387 | val_0_rmse: 0.9947  | val_1_rmse: 1.00495 |  0:00:03s
epoch 5  | loss: 0.99587 | val_0_rmse: 0.99492 | val_1_rmse: 1.0049  |  0:00:03s
epoch 6  | loss: 0.99231 | val_0_rmse: 0.99452 | val_1_rmse: 1.00405 |  0:00:04s
epoch 7  | loss: 0.99023 | val_0_rmse: 0.99453 | val_1_rmse: 1.00391 |  0:00:05s
epoch 8  | loss: 0.99215 | val_0_rmse: 0.99471 | val_1_rmse: 1.00411 |  0:00:05s
epoch 9  | loss: 0.99256 | val_0_rmse: 0.99472 | val_1_rmse: 1.00397 |  0:00:06s
epoch 10 | loss: 0.99176 | val_0_rmse: 0.99535 | val_1_rmse: 1.00414 |  0:00:06s
epoch 11 | loss: 0.98902 | val_0_rmse: 0.99516 | val_1_rmse: 1.0048  |  0:00:07s
epoch 12 | loss: 0.98892 | val_0_rmse: 0.9953  | val_1_rmse: 1.00496 |  0:00:08s
epoch 13 | loss: 0.98855 | val_0_rmse: 0.99572 | val_1_rmse: 1.00452 |  0:00:08s
epoch 14 | loss: 0.98877 | val_0_rmse: 0.99508 | val_1_rmse: 1.00404 |  0:00:09s
epoch 15 | loss: 0.98896 | val_0_rmse: 0.99439 | val_1_rmse: 1.00364 |  0:00:10s
epoch 16 | loss: 0.98797 | val_0_rmse: 0.99404 | val_1_rmse: 1.00338 |  0:00:10s
epoch 17 | loss: 0.98524 | val_0_rmse: 0.99341 | val_1_rmse: 1.00316 |  0:00:11s
epoch 18 | loss: 0.9844  | val_0_rmse: 0.99177 | val_1_rmse: 1.0021  |  0:00:12s
epoch 19 | loss: 0.98368 | val_0_rmse: 0.99221 | val_1_rmse: 1.00178 |  0:00:12s
epoch 20 | loss: 0.98349 | val_0_rmse: 0.98877 | val_1_rmse: 1.00019 |  0:00:13s
epoch 21 | loss: 0.98031 | val_0_rmse: 0.98394 | val_1_rmse: 0.99721 |  0:00:14s
epoch 22 | loss: 0.97245 | val_0_rmse: 0.98318 | val_1_rmse: 0.99682 |  0:00:14s
epoch 23 | loss: 0.96715 | val_0_rmse: 0.97562 | val_1_rmse: 0.99065 |  0:00:15s
epoch 24 | loss: 0.95755 | val_0_rmse: 0.97108 | val_1_rmse: 0.99117 |  0:00:15s
epoch 25 | loss: 0.95402 | val_0_rmse: 0.96864 | val_1_rmse: 0.98674 |  0:00:16s
epoch 26 | loss: 0.94557 | val_0_rmse: 0.96039 | val_1_rmse: 0.98323 |  0:00:17s
epoch 27 | loss: 0.92993 | val_0_rmse: 0.96365 | val_1_rmse: 0.99201 |  0:00:17s
epoch 28 | loss: 0.92288 | val_0_rmse: 0.96409 | val_1_rmse: 0.9855  |  0:00:18s
epoch 29 | loss: 0.91715 | val_0_rmse: 0.95636 | val_1_rmse: 0.98134 |  0:00:19s
epoch 30 | loss: 0.89197 | val_0_rmse: 0.94932 | val_1_rmse: 0.9717  |  0:00:19s
epoch 31 | loss: 0.88081 | val_0_rmse: 0.9422  | val_1_rmse: 0.96787 |  0:00:20s
epoch 32 | loss: 0.87681 | val_0_rmse: 0.94828 | val_1_rmse: 0.97742 |  0:00:21s
epoch 33 | loss: 0.86723 | val_0_rmse: 0.93569 | val_1_rmse: 0.96066 |  0:00:21s
epoch 34 | loss: 0.86091 | val_0_rmse: 0.94761 | val_1_rmse: 0.97134 |  0:00:22s
epoch 35 | loss: 0.85913 | val_0_rmse: 0.93849 | val_1_rmse: 0.95885 |  0:00:22s
epoch 36 | loss: 0.84787 | val_0_rmse: 0.92617 | val_1_rmse: 0.95204 |  0:00:23s
epoch 37 | loss: 0.84044 | val_0_rmse: 0.92203 | val_1_rmse: 0.94611 |  0:00:24s
epoch 38 | loss: 0.83363 | val_0_rmse: 0.91676 | val_1_rmse: 0.94818 |  0:00:25s
epoch 39 | loss: 0.81966 | val_0_rmse: 0.89946 | val_1_rmse: 0.92218 |  0:00:25s
epoch 40 | loss: 0.78546 | val_0_rmse: 0.87752 | val_1_rmse: 0.92888 |  0:00:26s
epoch 41 | loss: 0.78228 | val_0_rmse: 1.03579 | val_1_rmse: 1.05877 |  0:00:26s
epoch 42 | loss: 0.78156 | val_0_rmse: 0.86446 | val_1_rmse: 0.89416 |  0:00:27s
epoch 43 | loss: 0.74282 | val_0_rmse: 0.88793 | val_1_rmse: 0.92381 |  0:00:28s
epoch 44 | loss: 0.80557 | val_0_rmse: 0.85705 | val_1_rmse: 0.89382 |  0:00:28s
epoch 45 | loss: 0.7566  | val_0_rmse: 0.85977 | val_1_rmse: 0.91377 |  0:00:29s
epoch 46 | loss: 0.7662  | val_0_rmse: 0.92573 | val_1_rmse: 0.96511 |  0:00:30s
epoch 47 | loss: 0.78936 | val_0_rmse: 0.89418 | val_1_rmse: 0.93556 |  0:00:30s
epoch 48 | loss: 0.73377 | val_0_rmse: 0.88158 | val_1_rmse: 0.92384 |  0:00:31s
epoch 49 | loss: 0.68933 | val_0_rmse: 0.84817 | val_1_rmse: 0.88889 |  0:00:31s
epoch 50 | loss: 0.67546 | val_0_rmse: 0.85999 | val_1_rmse: 0.89892 |  0:00:32s
epoch 51 | loss: 0.67638 | val_0_rmse: 0.84784 | val_1_rmse: 0.89146 |  0:00:33s
epoch 52 | loss: 0.6759  | val_0_rmse: 0.83666 | val_1_rmse: 0.88674 |  0:00:33s
epoch 53 | loss: 0.66745 | val_0_rmse: 0.84988 | val_1_rmse: 0.90542 |  0:00:34s
epoch 54 | loss: 0.67173 | val_0_rmse: 0.83484 | val_1_rmse: 0.88529 |  0:00:35s
epoch 55 | loss: 0.66367 | val_0_rmse: 0.82372 | val_1_rmse: 0.87449 |  0:00:35s
epoch 56 | loss: 0.63235 | val_0_rmse: 0.8277  | val_1_rmse: 0.87937 |  0:00:36s
epoch 57 | loss: 0.64474 | val_0_rmse: 0.83401 | val_1_rmse: 0.9002  |  0:00:37s
epoch 58 | loss: 0.64951 | val_0_rmse: 0.82713 | val_1_rmse: 0.88114 |  0:00:37s
epoch 59 | loss: 0.63161 | val_0_rmse: 0.81577 | val_1_rmse: 0.87915 |  0:00:38s
epoch 60 | loss: 0.62515 | val_0_rmse: 0.81023 | val_1_rmse: 0.86829 |  0:00:38s
epoch 61 | loss: 0.6195  | val_0_rmse: 0.81397 | val_1_rmse: 0.88087 |  0:00:39s
epoch 62 | loss: 0.61199 | val_0_rmse: 0.8043  | val_1_rmse: 0.87755 |  0:00:40s
epoch 63 | loss: 0.60927 | val_0_rmse: 0.79702 | val_1_rmse: 0.86823 |  0:00:40s
epoch 64 | loss: 0.6027  | val_0_rmse: 0.80052 | val_1_rmse: 0.88339 |  0:00:41s
epoch 65 | loss: 0.59971 | val_0_rmse: 0.79235 | val_1_rmse: 0.87436 |  0:00:42s
epoch 66 | loss: 0.59812 | val_0_rmse: 0.79375 | val_1_rmse: 0.8837  |  0:00:42s
epoch 67 | loss: 0.58453 | val_0_rmse: 0.79389 | val_1_rmse: 0.88849 |  0:00:43s
epoch 68 | loss: 0.58739 | val_0_rmse: 0.77849 | val_1_rmse: 0.8737  |  0:00:44s
epoch 69 | loss: 0.5938  | val_0_rmse: 0.78089 | val_1_rmse: 0.87936 |  0:00:44s
epoch 70 | loss: 0.58551 | val_0_rmse: 0.77714 | val_1_rmse: 0.89692 |  0:00:45s
epoch 71 | loss: 0.58323 | val_0_rmse: 0.7704  | val_1_rmse: 0.88955 |  0:00:45s
epoch 72 | loss: 0.57786 | val_0_rmse: 0.77591 | val_1_rmse: 0.87913 |  0:00:46s
epoch 73 | loss: 0.56581 | val_0_rmse: 0.77009 | val_1_rmse: 0.884   |  0:00:47s
epoch 74 | loss: 0.56306 | val_0_rmse: 0.76096 | val_1_rmse: 0.88259 |  0:00:47s
epoch 75 | loss: 0.55733 | val_0_rmse: 0.75641 | val_1_rmse: 0.88283 |  0:00:48s
epoch 76 | loss: 0.54717 | val_0_rmse: 0.7561  | val_1_rmse: 0.89969 |  0:00:49s
epoch 77 | loss: 0.55945 | val_0_rmse: 0.75174 | val_1_rmse: 0.88378 |  0:00:49s
epoch 78 | loss: 0.55336 | val_0_rmse: 0.74206 | val_1_rmse: 0.90226 |  0:00:50s
epoch 79 | loss: 0.5517  | val_0_rmse: 0.73909 | val_1_rmse: 0.8957  |  0:00:50s
epoch 80 | loss: 0.54969 | val_0_rmse: 0.73987 | val_1_rmse: 0.89696 |  0:00:51s
epoch 81 | loss: 0.53427 | val_0_rmse: 0.73582 | val_1_rmse: 0.88387 |  0:00:52s
epoch 82 | loss: 0.53983 | val_0_rmse: 0.74124 | val_1_rmse: 0.88429 |  0:00:52s
epoch 83 | loss: 0.55115 | val_0_rmse: 0.75582 | val_1_rmse: 0.88998 |  0:00:53s
epoch 84 | loss: 0.55127 | val_0_rmse: 0.73008 | val_1_rmse: 0.90049 |  0:00:54s
epoch 85 | loss: 0.54571 | val_0_rmse: 0.7238  | val_1_rmse: 0.8859  |  0:00:54s
epoch 86 | loss: 0.53559 | val_0_rmse: 0.71861 | val_1_rmse: 0.9056  |  0:00:55s
epoch 87 | loss: 0.53805 | val_0_rmse: 0.7181  | val_1_rmse: 0.90283 |  0:00:55s
epoch 88 | loss: 0.53924 | val_0_rmse: 0.71706 | val_1_rmse: 0.90333 |  0:00:56s
epoch 89 | loss: 0.53134 | val_0_rmse: 0.71659 | val_1_rmse: 0.9234  |  0:00:57s
epoch 90 | loss: 0.52149 | val_0_rmse: 0.70555 | val_1_rmse: 0.90762 |  0:00:57s
epoch 91 | loss: 0.51734 | val_0_rmse: 0.71125 | val_1_rmse: 0.89984 |  0:00:58s
epoch 92 | loss: 0.51963 | val_0_rmse: 0.70806 | val_1_rmse: 0.90267 |  0:00:59s
epoch 93 | loss: 0.50776 | val_0_rmse: 0.69453 | val_1_rmse: 0.92573 |  0:00:59s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.86823
Best weights from best epoch are automatically used!
ended training at: 14:12:23
Feature importance:
Mean squared error is of 0.0628417846651544
Mean absolute error:0.1773772445593046
MAPE:0.18158943248520992
R2 score:0.3828935250958644
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:24
epoch 0  | loss: 3.38474 | val_0_rmse: 0.99856 | val_1_rmse: 1.0603  |  0:00:00s
epoch 1  | loss: 2.91831 | val_0_rmse: 1.00187 | val_1_rmse: 1.06198 |  0:00:00s
epoch 2  | loss: 1.60918 | val_0_rmse: 0.99812 | val_1_rmse: 1.05633 |  0:00:00s
epoch 3  | loss: 1.48818 | val_0_rmse: 1.00115 | val_1_rmse: 1.04937 |  0:00:00s
epoch 4  | loss: 1.4204  | val_0_rmse: 1.00302 | val_1_rmse: 1.04878 |  0:00:00s
epoch 5  | loss: 1.30575 | val_0_rmse: 1.00007 | val_1_rmse: 1.04998 |  0:00:00s
epoch 6  | loss: 1.20651 | val_0_rmse: 0.99725 | val_1_rmse: 1.05446 |  0:00:00s
epoch 7  | loss: 1.21226 | val_0_rmse: 0.99741 | val_1_rmse: 1.05007 |  0:00:00s
epoch 8  | loss: 1.26832 | val_0_rmse: 0.99813 | val_1_rmse: 1.04659 |  0:00:00s
epoch 9  | loss: 1.12794 | val_0_rmse: 0.99866 | val_1_rmse: 1.04526 |  0:00:00s
epoch 10 | loss: 1.0752  | val_0_rmse: 0.99787 | val_1_rmse: 1.04608 |  0:00:00s
epoch 11 | loss: 1.0621  | val_0_rmse: 0.99858 | val_1_rmse: 1.0455  |  0:00:01s
epoch 12 | loss: 1.01533 | val_0_rmse: 0.99888 | val_1_rmse: 1.04443 |  0:00:01s
epoch 13 | loss: 1.06685 | val_0_rmse: 0.99762 | val_1_rmse: 1.04576 |  0:00:01s
epoch 14 | loss: 1.03034 | val_0_rmse: 0.99722 | val_1_rmse: 1.04912 |  0:00:01s
epoch 15 | loss: 1.02432 | val_0_rmse: 0.99784 | val_1_rmse: 1.05232 |  0:00:01s
epoch 16 | loss: 1.02168 | val_0_rmse: 0.99797 | val_1_rmse: 1.05153 |  0:00:01s
epoch 17 | loss: 1.02167 | val_0_rmse: 0.9976  | val_1_rmse: 1.04988 |  0:00:01s
epoch 18 | loss: 1.01122 | val_0_rmse: 0.99839 | val_1_rmse: 1.0474  |  0:00:01s
epoch 19 | loss: 1.0008  | val_0_rmse: 0.99869 | val_1_rmse: 1.04798 |  0:00:01s
epoch 20 | loss: 0.99608 | val_0_rmse: 0.9984  | val_1_rmse: 1.04875 |  0:00:01s
epoch 21 | loss: 1.00525 | val_0_rmse: 0.99777 | val_1_rmse: 1.04905 |  0:00:01s
epoch 22 | loss: 0.98668 | val_0_rmse: 0.9977  | val_1_rmse: 1.05178 |  0:00:01s
epoch 23 | loss: 0.98406 | val_0_rmse: 0.99869 | val_1_rmse: 1.05627 |  0:00:02s
epoch 24 | loss: 0.98276 | val_0_rmse: 0.99784 | val_1_rmse: 1.05545 |  0:00:02s
epoch 25 | loss: 0.98142 | val_0_rmse: 0.99413 | val_1_rmse: 1.04841 |  0:00:02s
epoch 26 | loss: 0.96833 | val_0_rmse: 0.99053 | val_1_rmse: 1.04199 |  0:00:02s
epoch 27 | loss: 0.96152 | val_0_rmse: 0.98686 | val_1_rmse: 1.04015 |  0:00:02s
epoch 28 | loss: 0.97576 | val_0_rmse: 0.98534 | val_1_rmse: 1.03849 |  0:00:02s
epoch 29 | loss: 0.9563  | val_0_rmse: 0.98345 | val_1_rmse: 1.03714 |  0:00:02s
epoch 30 | loss: 0.95843 | val_0_rmse: 0.98389 | val_1_rmse: 1.04236 |  0:00:02s
epoch 31 | loss: 0.95341 | val_0_rmse: 0.98632 | val_1_rmse: 1.04738 |  0:00:02s
epoch 32 | loss: 0.95447 | val_0_rmse: 0.9884  | val_1_rmse: 1.04995 |  0:00:02s
epoch 33 | loss: 0.96388 | val_0_rmse: 0.98561 | val_1_rmse: 1.04251 |  0:00:02s
epoch 34 | loss: 0.95859 | val_0_rmse: 0.98329 | val_1_rmse: 1.03622 |  0:00:03s
epoch 35 | loss: 0.9124  | val_0_rmse: 0.98172 | val_1_rmse: 1.03429 |  0:00:03s
epoch 36 | loss: 0.9304  | val_0_rmse: 0.9793  | val_1_rmse: 1.03409 |  0:00:03s
epoch 37 | loss: 0.90269 | val_0_rmse: 0.97903 | val_1_rmse: 1.03399 |  0:00:03s
epoch 38 | loss: 0.92845 | val_0_rmse: 0.97895 | val_1_rmse: 1.03405 |  0:00:03s
epoch 39 | loss: 0.9142  | val_0_rmse: 0.9792  | val_1_rmse: 1.03346 |  0:00:03s
epoch 40 | loss: 0.91519 | val_0_rmse: 0.97946 | val_1_rmse: 1.03202 |  0:00:03s
epoch 41 | loss: 0.90932 | val_0_rmse: 0.98054 | val_1_rmse: 1.03151 |  0:00:03s
epoch 42 | loss: 0.90778 | val_0_rmse: 0.98078 | val_1_rmse: 1.03192 |  0:00:03s
epoch 43 | loss: 0.892   | val_0_rmse: 0.98085 | val_1_rmse: 1.03432 |  0:00:03s
epoch 44 | loss: 0.8988  | val_0_rmse: 0.98053 | val_1_rmse: 1.03466 |  0:00:03s
epoch 45 | loss: 0.88736 | val_0_rmse: 0.98194 | val_1_rmse: 1.03771 |  0:00:04s
epoch 46 | loss: 0.88724 | val_0_rmse: 0.98477 | val_1_rmse: 1.04164 |  0:00:04s
epoch 47 | loss: 0.89395 | val_0_rmse: 0.98746 | val_1_rmse: 1.04719 |  0:00:04s
epoch 48 | loss: 0.87949 | val_0_rmse: 0.98716 | val_1_rmse: 1.04601 |  0:00:04s
epoch 49 | loss: 0.873   | val_0_rmse: 0.98623 | val_1_rmse: 1.04253 |  0:00:04s
epoch 50 | loss: 0.85912 | val_0_rmse: 0.98463 | val_1_rmse: 1.04154 |  0:00:04s
epoch 51 | loss: 0.85167 | val_0_rmse: 0.98437 | val_1_rmse: 1.04362 |  0:00:04s
epoch 52 | loss: 0.85137 | val_0_rmse: 0.98367 | val_1_rmse: 1.04506 |  0:00:04s
epoch 53 | loss: 0.84806 | val_0_rmse: 0.98341 | val_1_rmse: 1.04379 |  0:00:04s
epoch 54 | loss: 0.86413 | val_0_rmse: 0.9828  | val_1_rmse: 1.04029 |  0:00:04s
epoch 55 | loss: 0.8539  | val_0_rmse: 0.98219 | val_1_rmse: 1.03841 |  0:00:04s
epoch 56 | loss: 0.8276  | val_0_rmse: 0.98157 | val_1_rmse: 1.03797 |  0:00:04s
epoch 57 | loss: 0.82867 | val_0_rmse: 0.97936 | val_1_rmse: 1.03555 |  0:00:05s
epoch 58 | loss: 0.81208 | val_0_rmse: 0.9756  | val_1_rmse: 1.03284 |  0:00:05s
epoch 59 | loss: 0.80017 | val_0_rmse: 0.97333 | val_1_rmse: 1.03102 |  0:00:05s
epoch 60 | loss: 0.82115 | val_0_rmse: 0.97384 | val_1_rmse: 1.0292  |  0:00:05s
epoch 61 | loss: 0.80747 | val_0_rmse: 0.9755  | val_1_rmse: 1.02736 |  0:00:05s
epoch 62 | loss: 0.80928 | val_0_rmse: 0.97564 | val_1_rmse: 1.02746 |  0:00:05s
epoch 63 | loss: 0.7937  | val_0_rmse: 0.9744  | val_1_rmse: 1.0287  |  0:00:05s
epoch 64 | loss: 0.78229 | val_0_rmse: 0.97338 | val_1_rmse: 1.0322  |  0:00:05s
epoch 65 | loss: 0.77707 | val_0_rmse: 0.97466 | val_1_rmse: 1.03913 |  0:00:05s
epoch 66 | loss: 0.77898 | val_0_rmse: 0.97704 | val_1_rmse: 1.04343 |  0:00:05s
epoch 67 | loss: 0.78783 | val_0_rmse: 0.98047 | val_1_rmse: 1.04499 |  0:00:05s
epoch 68 | loss: 0.77563 | val_0_rmse: 0.97927 | val_1_rmse: 1.04063 |  0:00:05s
epoch 69 | loss: 0.78024 | val_0_rmse: 0.9749  | val_1_rmse: 1.03378 |  0:00:06s
epoch 70 | loss: 0.78897 | val_0_rmse: 0.97137 | val_1_rmse: 1.03448 |  0:00:06s
epoch 71 | loss: 0.79186 | val_0_rmse: 0.97051 | val_1_rmse: 1.0364  |  0:00:06s
epoch 72 | loss: 0.7764  | val_0_rmse: 0.97044 | val_1_rmse: 1.03663 |  0:00:06s
epoch 73 | loss: 0.78969 | val_0_rmse: 0.96839 | val_1_rmse: 1.03321 |  0:00:06s
epoch 74 | loss: 0.77338 | val_0_rmse: 0.96253 | val_1_rmse: 1.02788 |  0:00:06s
epoch 75 | loss: 0.78462 | val_0_rmse: 0.96134 | val_1_rmse: 1.02687 |  0:00:06s
epoch 76 | loss: 0.77008 | val_0_rmse: 0.96145 | val_1_rmse: 1.02931 |  0:00:06s
epoch 77 | loss: 0.75615 | val_0_rmse: 0.96336 | val_1_rmse: 1.03313 |  0:00:06s
epoch 78 | loss: 0.7575  | val_0_rmse: 0.96419 | val_1_rmse: 1.03368 |  0:00:06s
epoch 79 | loss: 0.73697 | val_0_rmse: 0.96212 | val_1_rmse: 1.03292 |  0:00:06s
epoch 80 | loss: 0.7496  | val_0_rmse: 0.95973 | val_1_rmse: 1.02933 |  0:00:07s
epoch 81 | loss: 0.71146 | val_0_rmse: 0.9567  | val_1_rmse: 1.02544 |  0:00:07s
epoch 82 | loss: 0.72603 | val_0_rmse: 0.95353 | val_1_rmse: 1.02261 |  0:00:07s
epoch 83 | loss: 0.7123  | val_0_rmse: 0.95064 | val_1_rmse: 1.02398 |  0:00:07s
epoch 84 | loss: 0.72521 | val_0_rmse: 0.94883 | val_1_rmse: 1.02538 |  0:00:07s
epoch 85 | loss: 0.6968  | val_0_rmse: 0.9495  | val_1_rmse: 1.02989 |  0:00:07s
epoch 86 | loss: 0.70613 | val_0_rmse: 0.95299 | val_1_rmse: 1.0372  |  0:00:07s
epoch 87 | loss: 0.69817 | val_0_rmse: 0.95839 | val_1_rmse: 1.05075 |  0:00:07s
epoch 88 | loss: 0.69714 | val_0_rmse: 0.96362 | val_1_rmse: 1.06181 |  0:00:07s
epoch 89 | loss: 0.70061 | val_0_rmse: 0.96591 | val_1_rmse: 1.06609 |  0:00:07s
epoch 90 | loss: 0.67435 | val_0_rmse: 0.96697 | val_1_rmse: 1.06695 |  0:00:07s
epoch 91 | loss: 0.68414 | val_0_rmse: 0.96682 | val_1_rmse: 1.06411 |  0:00:07s
epoch 92 | loss: 0.67133 | val_0_rmse: 0.96662 | val_1_rmse: 1.06239 |  0:00:08s
epoch 93 | loss: 0.67939 | val_0_rmse: 0.96677 | val_1_rmse: 1.06322 |  0:00:08s
epoch 94 | loss: 0.66025 | val_0_rmse: 0.96764 | val_1_rmse: 1.06499 |  0:00:08s
epoch 95 | loss: 0.6666  | val_0_rmse: 0.96633 | val_1_rmse: 1.06432 |  0:00:08s
epoch 96 | loss: 0.6589  | val_0_rmse: 0.96448 | val_1_rmse: 1.06151 |  0:00:08s
epoch 97 | loss: 0.65015 | val_0_rmse: 0.96522 | val_1_rmse: 1.06197 |  0:00:08s
epoch 98 | loss: 0.64507 | val_0_rmse: 0.96532 | val_1_rmse: 1.06671 |  0:00:08s
epoch 99 | loss: 0.64184 | val_0_rmse: 0.9654  | val_1_rmse: 1.07109 |  0:00:08s
epoch 100| loss: 0.67387 | val_0_rmse: 0.96516 | val_1_rmse: 1.0659  |  0:00:08s
epoch 101| loss: 0.66351 | val_0_rmse: 0.96493 | val_1_rmse: 1.05797 |  0:00:08s
epoch 102| loss: 0.66074 | val_0_rmse: 0.96429 | val_1_rmse: 1.05131 |  0:00:08s
epoch 103| loss: 0.67867 | val_0_rmse: 0.96367 | val_1_rmse: 1.0509  |  0:00:08s
epoch 104| loss: 0.64614 | val_0_rmse: 0.96761 | val_1_rmse: 1.05625 |  0:00:09s
epoch 105| loss: 0.66667 | val_0_rmse: 0.96285 | val_1_rmse: 1.05317 |  0:00:09s
epoch 106| loss: 0.67769 | val_0_rmse: 0.95442 | val_1_rmse: 1.04463 |  0:00:09s
epoch 107| loss: 0.6514  | val_0_rmse: 0.95088 | val_1_rmse: 1.0368  |  0:00:09s
epoch 108| loss: 0.67385 | val_0_rmse: 0.94782 | val_1_rmse: 1.03441 |  0:00:09s
epoch 109| loss: 0.67068 | val_0_rmse: 0.94939 | val_1_rmse: 1.03968 |  0:00:09s
epoch 110| loss: 0.67637 | val_0_rmse: 0.95225 | val_1_rmse: 1.0481  |  0:00:09s
epoch 111| loss: 0.66229 | val_0_rmse: 0.95491 | val_1_rmse: 1.05807 |  0:00:09s
epoch 112| loss: 0.65921 | val_0_rmse: 0.9565  | val_1_rmse: 1.0529  |  0:00:09s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 1.02261
Best weights from best epoch are automatically used!
ended training at: 14:12:34
Feature importance:
Mean squared error is of 0.07790328252751734
Mean absolute error:0.19147631368382828
MAPE:0.2230981847631175
R2 score:0.05748234304517541
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:34
epoch 0  | loss: 3.63477 | val_0_rmse: 1.03764 | val_1_rmse: 1.02589 |  0:00:00s
epoch 1  | loss: 2.69994 | val_0_rmse: 1.02369 | val_1_rmse: 1.01962 |  0:00:00s
epoch 2  | loss: 2.10944 | val_0_rmse: 1.01098 | val_1_rmse: 1.01666 |  0:00:00s
epoch 3  | loss: 1.60412 | val_0_rmse: 1.00333 | val_1_rmse: 1.01791 |  0:00:00s
epoch 4  | loss: 1.50611 | val_0_rmse: 0.99986 | val_1_rmse: 1.0219  |  0:00:00s
epoch 5  | loss: 1.37322 | val_0_rmse: 0.99401 | val_1_rmse: 1.02742 |  0:00:00s
epoch 6  | loss: 1.28623 | val_0_rmse: 0.99331 | val_1_rmse: 1.02611 |  0:00:00s
epoch 7  | loss: 1.22984 | val_0_rmse: 1.00236 | val_1_rmse: 1.01877 |  0:00:00s
epoch 8  | loss: 1.18622 | val_0_rmse: 1.00941 | val_1_rmse: 1.02339 |  0:00:00s
epoch 9  | loss: 1.27883 | val_0_rmse: 1.00098 | val_1_rmse: 1.028   |  0:00:00s
epoch 10 | loss: 1.15172 | val_0_rmse: 0.9958  | val_1_rmse: 1.02311 |  0:00:00s
epoch 11 | loss: 1.0626  | val_0_rmse: 0.99698 | val_1_rmse: 1.01932 |  0:00:01s
epoch 12 | loss: 1.06625 | val_0_rmse: 0.99814 | val_1_rmse: 1.01739 |  0:00:01s
epoch 13 | loss: 1.07057 | val_0_rmse: 0.99828 | val_1_rmse: 1.01684 |  0:00:01s
epoch 14 | loss: 0.98943 | val_0_rmse: 1.00231 | val_1_rmse: 1.01807 |  0:00:01s
epoch 15 | loss: 1.00463 | val_0_rmse: 1.00369 | val_1_rmse: 1.01785 |  0:00:01s
epoch 16 | loss: 1.01246 | val_0_rmse: 1.00024 | val_1_rmse: 1.01599 |  0:00:01s
epoch 17 | loss: 1.00486 | val_0_rmse: 0.9967  | val_1_rmse: 1.0131  |  0:00:01s
epoch 18 | loss: 0.97785 | val_0_rmse: 0.99626 | val_1_rmse: 1.01536 |  0:00:01s
epoch 19 | loss: 0.99998 | val_0_rmse: 0.99328 | val_1_rmse: 1.01389 |  0:00:01s
epoch 20 | loss: 0.96961 | val_0_rmse: 0.99582 | val_1_rmse: 1.01612 |  0:00:01s
epoch 21 | loss: 0.95514 | val_0_rmse: 0.99595 | val_1_rmse: 1.01786 |  0:00:01s
epoch 22 | loss: 0.95999 | val_0_rmse: 0.99515 | val_1_rmse: 1.0191  |  0:00:01s
epoch 23 | loss: 0.96297 | val_0_rmse: 0.99452 | val_1_rmse: 1.01918 |  0:00:02s
epoch 24 | loss: 0.9215  | val_0_rmse: 0.99177 | val_1_rmse: 1.01759 |  0:00:02s
epoch 25 | loss: 0.94087 | val_0_rmse: 0.98637 | val_1_rmse: 1.01259 |  0:00:02s
epoch 26 | loss: 0.9689  | val_0_rmse: 0.98531 | val_1_rmse: 1.01258 |  0:00:02s
epoch 27 | loss: 0.92008 | val_0_rmse: 0.9866  | val_1_rmse: 1.01589 |  0:00:02s
epoch 28 | loss: 0.9326  | val_0_rmse: 0.99146 | val_1_rmse: 1.02582 |  0:00:02s
epoch 29 | loss: 0.9417  | val_0_rmse: 0.99399 | val_1_rmse: 1.03201 |  0:00:02s
epoch 30 | loss: 0.9425  | val_0_rmse: 0.98907 | val_1_rmse: 1.02372 |  0:00:02s
epoch 31 | loss: 0.9298  | val_0_rmse: 0.9844  | val_1_rmse: 1.01363 |  0:00:02s
epoch 32 | loss: 0.91092 | val_0_rmse: 0.98264 | val_1_rmse: 1.00771 |  0:00:02s
epoch 33 | loss: 0.93185 | val_0_rmse: 0.97962 | val_1_rmse: 1.00952 |  0:00:02s
epoch 34 | loss: 0.89034 | val_0_rmse: 0.97266 | val_1_rmse: 1.01698 |  0:00:03s
epoch 35 | loss: 0.90434 | val_0_rmse: 0.97364 | val_1_rmse: 1.0296  |  0:00:03s
epoch 36 | loss: 0.88086 | val_0_rmse: 0.97344 | val_1_rmse: 1.03602 |  0:00:03s
epoch 37 | loss: 0.88384 | val_0_rmse: 0.96836 | val_1_rmse: 1.02786 |  0:00:03s
epoch 38 | loss: 0.88443 | val_0_rmse: 0.96639 | val_1_rmse: 1.02011 |  0:00:03s
epoch 39 | loss: 0.85909 | val_0_rmse: 0.96742 | val_1_rmse: 1.00443 |  0:00:03s
epoch 40 | loss: 0.86988 | val_0_rmse: 0.97218 | val_1_rmse: 1.00296 |  0:00:03s
epoch 41 | loss: 0.88676 | val_0_rmse: 0.97357 | val_1_rmse: 1.00177 |  0:00:03s
epoch 42 | loss: 0.85043 | val_0_rmse: 0.96949 | val_1_rmse: 0.99971 |  0:00:03s
epoch 43 | loss: 0.82666 | val_0_rmse: 0.9632  | val_1_rmse: 1.00196 |  0:00:03s
epoch 44 | loss: 0.85068 | val_0_rmse: 0.96042 | val_1_rmse: 1.00565 |  0:00:03s
epoch 45 | loss: 0.85764 | val_0_rmse: 0.9611  | val_1_rmse: 1.00511 |  0:00:03s
epoch 46 | loss: 0.83571 | val_0_rmse: 0.96324 | val_1_rmse: 1.00373 |  0:00:04s
epoch 47 | loss: 0.83481 | val_0_rmse: 0.96978 | val_1_rmse: 1.00233 |  0:00:04s
epoch 48 | loss: 0.82867 | val_0_rmse: 0.9747  | val_1_rmse: 1.00334 |  0:00:04s
epoch 49 | loss: 0.83076 | val_0_rmse: 0.97165 | val_1_rmse: 1.00084 |  0:00:04s
epoch 50 | loss: 0.8156  | val_0_rmse: 0.96631 | val_1_rmse: 0.99903 |  0:00:04s
epoch 51 | loss: 0.82907 | val_0_rmse: 0.96234 | val_1_rmse: 0.99697 |  0:00:04s
epoch 52 | loss: 0.82261 | val_0_rmse: 0.96298 | val_1_rmse: 0.99573 |  0:00:04s
epoch 53 | loss: 0.82277 | val_0_rmse: 0.96377 | val_1_rmse: 0.99385 |  0:00:04s
epoch 54 | loss: 0.80504 | val_0_rmse: 0.96382 | val_1_rmse: 0.99242 |  0:00:04s
epoch 55 | loss: 0.7969  | val_0_rmse: 0.96128 | val_1_rmse: 0.99417 |  0:00:04s
epoch 56 | loss: 0.7801  | val_0_rmse: 0.9578  | val_1_rmse: 0.99745 |  0:00:04s
epoch 57 | loss: 0.77815 | val_0_rmse: 0.95598 | val_1_rmse: 1.00102 |  0:00:05s
epoch 58 | loss: 0.79356 | val_0_rmse: 0.9561  | val_1_rmse: 1.00286 |  0:00:05s
epoch 59 | loss: 0.77702 | val_0_rmse: 0.95693 | val_1_rmse: 1.00375 |  0:00:05s
epoch 60 | loss: 0.7866  | val_0_rmse: 0.95833 | val_1_rmse: 1.00458 |  0:00:05s
epoch 61 | loss: 0.79004 | val_0_rmse: 0.95939 | val_1_rmse: 1.00453 |  0:00:05s
epoch 62 | loss: 0.78921 | val_0_rmse: 0.96021 | val_1_rmse: 1.00374 |  0:00:05s
epoch 63 | loss: 0.78765 | val_0_rmse: 0.95974 | val_1_rmse: 1.00401 |  0:00:05s
epoch 64 | loss: 0.77853 | val_0_rmse: 0.95923 | val_1_rmse: 1.0038  |  0:00:05s
epoch 65 | loss: 0.75268 | val_0_rmse: 0.95859 | val_1_rmse: 1.00322 |  0:00:05s
epoch 66 | loss: 0.75606 | val_0_rmse: 0.95757 | val_1_rmse: 1.00404 |  0:00:05s
epoch 67 | loss: 0.7528  | val_0_rmse: 0.95579 | val_1_rmse: 1.0061  |  0:00:05s
epoch 68 | loss: 0.77352 | val_0_rmse: 0.95445 | val_1_rmse: 1.00856 |  0:00:05s
epoch 69 | loss: 0.75331 | val_0_rmse: 0.95348 | val_1_rmse: 1.00995 |  0:00:06s
epoch 70 | loss: 0.71899 | val_0_rmse: 0.95357 | val_1_rmse: 1.00931 |  0:00:06s
epoch 71 | loss: 0.7429  | val_0_rmse: 0.95423 | val_1_rmse: 1.00555 |  0:00:06s
epoch 72 | loss: 0.75847 | val_0_rmse: 0.95509 | val_1_rmse: 1.0058  |  0:00:06s
epoch 73 | loss: 0.72721 | val_0_rmse: 0.95661 | val_1_rmse: 1.00536 |  0:00:06s
epoch 74 | loss: 0.73208 | val_0_rmse: 0.95666 | val_1_rmse: 1.00591 |  0:00:06s
epoch 75 | loss: 0.73258 | val_0_rmse: 0.95638 | val_1_rmse: 1.00664 |  0:00:06s
epoch 76 | loss: 0.73453 | val_0_rmse: 0.95675 | val_1_rmse: 1.00599 |  0:00:06s
epoch 77 | loss: 0.73421 | val_0_rmse: 0.95572 | val_1_rmse: 1.00498 |  0:00:06s
epoch 78 | loss: 0.70425 | val_0_rmse: 0.95597 | val_1_rmse: 1.00381 |  0:00:06s
epoch 79 | loss: 0.68696 | val_0_rmse: 0.95562 | val_1_rmse: 1.00348 |  0:00:06s
epoch 80 | loss: 0.69589 | val_0_rmse: 0.95797 | val_1_rmse: 1.00312 |  0:00:06s
epoch 81 | loss: 0.69918 | val_0_rmse: 0.96359 | val_1_rmse: 1.00311 |  0:00:07s
epoch 82 | loss: 0.70066 | val_0_rmse: 0.96701 | val_1_rmse: 1.01024 |  0:00:07s
epoch 83 | loss: 0.71619 | val_0_rmse: 0.97152 | val_1_rmse: 1.01219 |  0:00:07s
epoch 84 | loss: 0.71407 | val_0_rmse: 0.97059 | val_1_rmse: 1.01098 |  0:00:07s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.99242
Best weights from best epoch are automatically used!
ended training at: 14:12:41
Feature importance:
Mean squared error is of 0.08911110839088417
Mean absolute error:0.21278504228606904
MAPE:0.21696015338806576
R2 score:-0.021693068086172662
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:42
epoch 0  | loss: 3.2911  | val_0_rmse: 0.99845 | val_1_rmse: 0.9276  |  0:00:00s
epoch 1  | loss: 3.0625  | val_0_rmse: 1.00121 | val_1_rmse: 0.92404 |  0:00:00s
epoch 2  | loss: 2.23402 | val_0_rmse: 1.00105 | val_1_rmse: 0.92369 |  0:00:00s
epoch 3  | loss: 1.92247 | val_0_rmse: 0.99616 | val_1_rmse: 0.92458 |  0:00:00s
epoch 4  | loss: 1.33695 | val_0_rmse: 0.9998  | val_1_rmse: 0.9338  |  0:00:00s
epoch 5  | loss: 1.37431 | val_0_rmse: 1.00472 | val_1_rmse: 0.93867 |  0:00:00s
epoch 6  | loss: 1.50898 | val_0_rmse: 1.00318 | val_1_rmse: 0.93442 |  0:00:00s
epoch 7  | loss: 1.36942 | val_0_rmse: 1.00085 | val_1_rmse: 0.92933 |  0:00:00s
epoch 8  | loss: 1.17082 | val_0_rmse: 0.9981  | val_1_rmse: 0.92651 |  0:00:00s
epoch 9  | loss: 1.14352 | val_0_rmse: 0.99741 | val_1_rmse: 0.92463 |  0:00:01s
epoch 10 | loss: 1.11092 | val_0_rmse: 0.9953  | val_1_rmse: 0.92477 |  0:00:01s
epoch 11 | loss: 1.0522  | val_0_rmse: 0.99479 | val_1_rmse: 0.9256  |  0:00:01s
epoch 12 | loss: 1.06656 | val_0_rmse: 0.99373 | val_1_rmse: 0.92529 |  0:00:01s
epoch 13 | loss: 1.05481 | val_0_rmse: 0.99393 | val_1_rmse: 0.92513 |  0:00:01s
epoch 14 | loss: 1.03321 | val_0_rmse: 0.99496 | val_1_rmse: 0.92472 |  0:00:01s
epoch 15 | loss: 1.06623 | val_0_rmse: 0.99428 | val_1_rmse: 0.92509 |  0:00:01s
epoch 16 | loss: 1.01862 | val_0_rmse: 0.99279 | val_1_rmse: 0.92679 |  0:00:01s
epoch 17 | loss: 1.02238 | val_0_rmse: 0.99233 | val_1_rmse: 0.92655 |  0:00:01s
epoch 18 | loss: 1.0139  | val_0_rmse: 0.99423 | val_1_rmse: 0.92578 |  0:00:01s
epoch 19 | loss: 0.991   | val_0_rmse: 0.99759 | val_1_rmse: 0.92654 |  0:00:01s
epoch 20 | loss: 0.99947 | val_0_rmse: 0.99625 | val_1_rmse: 0.92643 |  0:00:01s
epoch 21 | loss: 0.98408 | val_0_rmse: 0.99537 | val_1_rmse: 0.92607 |  0:00:02s
epoch 22 | loss: 0.98037 | val_0_rmse: 0.9942  | val_1_rmse: 0.92576 |  0:00:02s
epoch 23 | loss: 1.01815 | val_0_rmse: 0.99329 | val_1_rmse: 0.92543 |  0:00:02s
epoch 24 | loss: 0.98933 | val_0_rmse: 0.99323 | val_1_rmse: 0.92542 |  0:00:02s
epoch 25 | loss: 0.99577 | val_0_rmse: 0.99249 | val_1_rmse: 0.92561 |  0:00:02s
epoch 26 | loss: 0.99331 | val_0_rmse: 0.99317 | val_1_rmse: 0.92617 |  0:00:02s
epoch 27 | loss: 0.99311 | val_0_rmse: 0.99537 | val_1_rmse: 0.92789 |  0:00:02s
epoch 28 | loss: 0.99779 | val_0_rmse: 0.9994  | val_1_rmse: 0.93128 |  0:00:02s
epoch 29 | loss: 0.99778 | val_0_rmse: 1.00246 | val_1_rmse: 0.93364 |  0:00:02s
epoch 30 | loss: 0.9931  | val_0_rmse: 1.00076 | val_1_rmse: 0.93235 |  0:00:02s
epoch 31 | loss: 1.00162 | val_0_rmse: 0.99683 | val_1_rmse: 0.92936 |  0:00:02s
epoch 32 | loss: 0.99066 | val_0_rmse: 0.99496 | val_1_rmse: 0.92748 |  0:00:02s

Early stopping occured at epoch 32 with best_epoch = 2 and best_val_1_rmse = 0.92369
Best weights from best epoch are automatically used!
ended training at: 14:12:45
Feature importance:
Mean squared error is of 0.10178614626459125
Mean absolute error:0.21766580371814187
MAPE:0.25721342333945
R2 score:-0.0039751217196843935
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:45
epoch 0  | loss: 2.87039 | val_0_rmse: 1.03262 | val_1_rmse: 0.89055 |  0:00:00s
epoch 1  | loss: 2.60129 | val_0_rmse: 1.03788 | val_1_rmse: 0.90132 |  0:00:00s
epoch 2  | loss: 2.47828 | val_0_rmse: 1.02575 | val_1_rmse: 0.87719 |  0:00:00s
epoch 3  | loss: 1.7062  | val_0_rmse: 1.03739 | val_1_rmse: 0.87934 |  0:00:00s
epoch 4  | loss: 1.8656  | val_0_rmse: 1.03496 | val_1_rmse: 0.8776  |  0:00:00s
epoch 5  | loss: 1.64137 | val_0_rmse: 1.02616 | val_1_rmse: 0.87514 |  0:00:00s
epoch 6  | loss: 1.47772 | val_0_rmse: 1.02433 | val_1_rmse: 0.87978 |  0:00:00s
epoch 7  | loss: 1.56763 | val_0_rmse: 1.02526 | val_1_rmse: 0.88283 |  0:00:00s
epoch 8  | loss: 1.40216 | val_0_rmse: 1.02596 | val_1_rmse: 0.88296 |  0:00:00s
epoch 9  | loss: 1.32077 | val_0_rmse: 1.02689 | val_1_rmse: 0.88152 |  0:00:00s
epoch 10 | loss: 1.26864 | val_0_rmse: 1.02699 | val_1_rmse: 0.88067 |  0:00:00s
epoch 11 | loss: 1.26475 | val_0_rmse: 1.02674 | val_1_rmse: 0.8793  |  0:00:01s
epoch 12 | loss: 1.10081 | val_0_rmse: 1.02639 | val_1_rmse: 0.8794  |  0:00:01s
epoch 13 | loss: 1.17062 | val_0_rmse: 1.02637 | val_1_rmse: 0.8783  |  0:00:01s
epoch 14 | loss: 1.12667 | val_0_rmse: 1.02637 | val_1_rmse: 0.87763 |  0:00:01s
epoch 15 | loss: 1.0687  | val_0_rmse: 1.02616 | val_1_rmse: 0.87717 |  0:00:01s
epoch 16 | loss: 1.12188 | val_0_rmse: 1.02678 | val_1_rmse: 0.87748 |  0:00:01s
epoch 17 | loss: 1.09968 | val_0_rmse: 1.0266  | val_1_rmse: 0.8777  |  0:00:01s
epoch 18 | loss: 1.06857 | val_0_rmse: 1.02662 | val_1_rmse: 0.8779  |  0:00:01s
epoch 19 | loss: 1.06719 | val_0_rmse: 1.02692 | val_1_rmse: 0.87759 |  0:00:01s
epoch 20 | loss: 1.03852 | val_0_rmse: 1.02725 | val_1_rmse: 0.87727 |  0:00:01s
epoch 21 | loss: 1.06952 | val_0_rmse: 1.02718 | val_1_rmse: 0.87691 |  0:00:01s
epoch 22 | loss: 1.06765 | val_0_rmse: 1.02683 | val_1_rmse: 0.87688 |  0:00:01s
epoch 23 | loss: 1.04918 | val_0_rmse: 1.02657 | val_1_rmse: 0.87667 |  0:00:02s
epoch 24 | loss: 1.04682 | val_0_rmse: 1.02661 | val_1_rmse: 0.8764  |  0:00:02s
epoch 25 | loss: 1.05249 | val_0_rmse: 1.02698 | val_1_rmse: 0.87561 |  0:00:02s
epoch 26 | loss: 1.04874 | val_0_rmse: 1.02746 | val_1_rmse: 0.87587 |  0:00:02s
epoch 27 | loss: 1.04918 | val_0_rmse: 1.02585 | val_1_rmse: 0.87552 |  0:00:02s
epoch 28 | loss: 1.02907 | val_0_rmse: 1.02304 | val_1_rmse: 0.87433 |  0:00:02s
epoch 29 | loss: 1.02329 | val_0_rmse: 1.02079 | val_1_rmse: 0.87429 |  0:00:02s
epoch 30 | loss: 1.0253  | val_0_rmse: 1.02043 | val_1_rmse: 0.87696 |  0:00:02s
epoch 31 | loss: 1.0056  | val_0_rmse: 1.02361 | val_1_rmse: 0.88436 |  0:00:02s
epoch 32 | loss: 0.99702 | val_0_rmse: 1.02653 | val_1_rmse: 0.88892 |  0:00:02s
epoch 33 | loss: 1.03273 | val_0_rmse: 1.03773 | val_1_rmse: 0.90465 |  0:00:02s
epoch 34 | loss: 1.02572 | val_0_rmse: 1.056   | val_1_rmse: 0.9119  |  0:00:03s
epoch 35 | loss: 1.01128 | val_0_rmse: 1.04529 | val_1_rmse: 0.89718 |  0:00:03s
epoch 36 | loss: 0.99517 | val_0_rmse: 1.04102 | val_1_rmse: 0.8929  |  0:00:03s
epoch 37 | loss: 0.99161 | val_0_rmse: 1.04528 | val_1_rmse: 0.89988 |  0:00:03s
epoch 38 | loss: 0.98802 | val_0_rmse: 1.04983 | val_1_rmse: 0.90911 |  0:00:03s
epoch 39 | loss: 0.97745 | val_0_rmse: 1.03971 | val_1_rmse: 0.90778 |  0:00:03s
epoch 40 | loss: 0.96547 | val_0_rmse: 1.02804 | val_1_rmse: 0.89673 |  0:00:03s
epoch 41 | loss: 0.93897 | val_0_rmse: 1.0252  | val_1_rmse: 0.89408 |  0:00:03s
epoch 42 | loss: 0.95535 | val_0_rmse: 1.03043 | val_1_rmse: 0.89953 |  0:00:03s
epoch 43 | loss: 0.96974 | val_0_rmse: 1.03661 | val_1_rmse: 0.90994 |  0:00:03s
epoch 44 | loss: 0.95873 | val_0_rmse: 1.03501 | val_1_rmse: 0.93275 |  0:00:03s
epoch 45 | loss: 0.9577  | val_0_rmse: 1.02691 | val_1_rmse: 0.93903 |  0:00:03s
epoch 46 | loss: 0.98049 | val_0_rmse: 1.01691 | val_1_rmse: 0.9306  |  0:00:04s
epoch 47 | loss: 0.96787 | val_0_rmse: 1.0072  | val_1_rmse: 0.93424 |  0:00:04s
epoch 48 | loss: 0.95299 | val_0_rmse: 1.00244 | val_1_rmse: 0.92818 |  0:00:04s
epoch 49 | loss: 0.96787 | val_0_rmse: 0.99768 | val_1_rmse: 0.90984 |  0:00:04s
epoch 50 | loss: 0.94797 | val_0_rmse: 0.993   | val_1_rmse: 0.89941 |  0:00:04s
epoch 51 | loss: 0.93688 | val_0_rmse: 0.99176 | val_1_rmse: 0.89435 |  0:00:04s
epoch 52 | loss: 0.94788 | val_0_rmse: 0.99361 | val_1_rmse: 0.89121 |  0:00:04s
epoch 53 | loss: 0.94559 | val_0_rmse: 0.9958  | val_1_rmse: 0.90559 |  0:00:04s
epoch 54 | loss: 0.95291 | val_0_rmse: 1.00202 | val_1_rmse: 0.89209 |  0:00:04s
epoch 55 | loss: 0.94727 | val_0_rmse: 0.99827 | val_1_rmse: 0.88355 |  0:00:04s
epoch 56 | loss: 0.93953 | val_0_rmse: 0.99671 | val_1_rmse: 0.86979 |  0:00:04s
epoch 57 | loss: 0.9201  | val_0_rmse: 0.99568 | val_1_rmse: 0.86301 |  0:00:04s
epoch 58 | loss: 0.91721 | val_0_rmse: 0.99481 | val_1_rmse: 0.85857 |  0:00:05s
epoch 59 | loss: 0.92146 | val_0_rmse: 0.99524 | val_1_rmse: 0.85701 |  0:00:05s
epoch 60 | loss: 0.88638 | val_0_rmse: 0.99785 | val_1_rmse: 0.85638 |  0:00:05s
epoch 61 | loss: 0.87256 | val_0_rmse: 1.00241 | val_1_rmse: 0.85631 |  0:00:05s
epoch 62 | loss: 0.8919  | val_0_rmse: 0.99983 | val_1_rmse: 0.85816 |  0:00:05s
epoch 63 | loss: 0.88425 | val_0_rmse: 0.99431 | val_1_rmse: 0.86275 |  0:00:05s
epoch 64 | loss: 0.87006 | val_0_rmse: 0.99043 | val_1_rmse: 0.86545 |  0:00:05s
epoch 65 | loss: 0.8777  | val_0_rmse: 0.99255 | val_1_rmse: 0.86635 |  0:00:05s
epoch 66 | loss: 0.87229 | val_0_rmse: 0.99793 | val_1_rmse: 0.86709 |  0:00:05s
epoch 67 | loss: 0.86643 | val_0_rmse: 0.99326 | val_1_rmse: 0.86902 |  0:00:05s
epoch 68 | loss: 0.89971 | val_0_rmse: 0.98832 | val_1_rmse: 0.86394 |  0:00:05s
epoch 69 | loss: 0.85267 | val_0_rmse: 0.98521 | val_1_rmse: 0.86157 |  0:00:06s
epoch 70 | loss: 0.85134 | val_0_rmse: 0.98846 | val_1_rmse: 0.86111 |  0:00:06s
epoch 71 | loss: 0.84789 | val_0_rmse: 0.99661 | val_1_rmse: 0.86172 |  0:00:06s
epoch 72 | loss: 0.8126  | val_0_rmse: 1.00497 | val_1_rmse: 0.87506 |  0:00:06s
epoch 73 | loss: 0.82928 | val_0_rmse: 1.003   | val_1_rmse: 0.87861 |  0:00:06s
epoch 74 | loss: 0.80671 | val_0_rmse: 0.99253 | val_1_rmse: 0.86801 |  0:00:06s
epoch 75 | loss: 0.80279 | val_0_rmse: 0.99373 | val_1_rmse: 0.85165 |  0:00:06s
epoch 76 | loss: 0.76397 | val_0_rmse: 0.99964 | val_1_rmse: 0.85034 |  0:00:06s
epoch 77 | loss: 0.79351 | val_0_rmse: 1.00359 | val_1_rmse: 0.85744 |  0:00:06s
epoch 78 | loss: 0.78763 | val_0_rmse: 0.99449 | val_1_rmse: 0.85428 |  0:00:06s
epoch 79 | loss: 0.7677  | val_0_rmse: 0.98545 | val_1_rmse: 0.85897 |  0:00:06s
epoch 80 | loss: 0.76775 | val_0_rmse: 0.98731 | val_1_rmse: 0.87101 |  0:00:06s
epoch 81 | loss: 0.78057 | val_0_rmse: 0.99814 | val_1_rmse: 0.88391 |  0:00:07s
epoch 82 | loss: 0.76668 | val_0_rmse: 1.00024 | val_1_rmse: 0.88117 |  0:00:07s
epoch 83 | loss: 0.8288  | val_0_rmse: 0.99586 | val_1_rmse: 0.86962 |  0:00:07s
epoch 84 | loss: 0.77089 | val_0_rmse: 0.98274 | val_1_rmse: 0.85612 |  0:00:07s
epoch 85 | loss: 0.7762  | val_0_rmse: 0.97634 | val_1_rmse: 0.84981 |  0:00:07s
epoch 86 | loss: 0.77533 | val_0_rmse: 0.97353 | val_1_rmse: 0.84745 |  0:00:07s
epoch 87 | loss: 0.75474 | val_0_rmse: 0.9768  | val_1_rmse: 0.85485 |  0:00:07s
epoch 88 | loss: 0.75307 | val_0_rmse: 0.97969 | val_1_rmse: 0.86119 |  0:00:07s
epoch 89 | loss: 0.75656 | val_0_rmse: 0.98543 | val_1_rmse: 0.86206 |  0:00:07s
epoch 90 | loss: 0.71854 | val_0_rmse: 0.99603 | val_1_rmse: 0.88076 |  0:00:07s
epoch 91 | loss: 0.73841 | val_0_rmse: 0.99878 | val_1_rmse: 0.87834 |  0:00:07s
epoch 92 | loss: 0.71151 | val_0_rmse: 0.9963  | val_1_rmse: 0.87075 |  0:00:08s
epoch 93 | loss: 0.71666 | val_0_rmse: 1.00357 | val_1_rmse: 0.85877 |  0:00:08s
epoch 94 | loss: 0.76475 | val_0_rmse: 1.0118  | val_1_rmse: 0.88867 |  0:00:08s
epoch 95 | loss: 0.73544 | val_0_rmse: 1.02182 | val_1_rmse: 0.92    |  0:00:08s
epoch 96 | loss: 0.75921 | val_0_rmse: 1.01338 | val_1_rmse: 0.92241 |  0:00:08s
epoch 97 | loss: 0.73057 | val_0_rmse: 1.00924 | val_1_rmse: 0.91631 |  0:00:08s
epoch 98 | loss: 0.70294 | val_0_rmse: 1.02228 | val_1_rmse: 0.9297  |  0:00:08s
epoch 99 | loss: 0.72871 | val_0_rmse: 1.03111 | val_1_rmse: 0.93487 |  0:00:08s
epoch 100| loss: 0.69366 | val_0_rmse: 1.04092 | val_1_rmse: 0.95484 |  0:00:08s
epoch 101| loss: 0.67975 | val_0_rmse: 1.04509 | val_1_rmse: 0.98097 |  0:00:08s
epoch 102| loss: 0.73294 | val_0_rmse: 1.04413 | val_1_rmse: 0.97165 |  0:00:08s
epoch 103| loss: 0.6872  | val_0_rmse: 1.03368 | val_1_rmse: 0.95733 |  0:00:08s
epoch 104| loss: 0.67984 | val_0_rmse: 1.02331 | val_1_rmse: 0.95596 |  0:00:09s
epoch 105| loss: 0.66996 | val_0_rmse: 1.02247 | val_1_rmse: 0.95349 |  0:00:09s
epoch 106| loss: 0.68889 | val_0_rmse: 1.0213  | val_1_rmse: 0.96288 |  0:00:09s
epoch 107| loss: 0.65883 | val_0_rmse: 1.01967 | val_1_rmse: 0.96951 |  0:00:09s
epoch 108| loss: 0.66902 | val_0_rmse: 1.01762 | val_1_rmse: 0.97402 |  0:00:09s
epoch 109| loss: 0.66951 | val_0_rmse: 1.01441 | val_1_rmse: 0.97818 |  0:00:09s
epoch 110| loss: 0.68401 | val_0_rmse: 1.00375 | val_1_rmse: 0.97127 |  0:00:09s
epoch 111| loss: 0.66712 | val_0_rmse: 0.99309 | val_1_rmse: 0.958   |  0:00:09s
epoch 112| loss: 0.65223 | val_0_rmse: 0.98693 | val_1_rmse: 0.93546 |  0:00:09s
epoch 113| loss: 0.63012 | val_0_rmse: 0.98623 | val_1_rmse: 0.91987 |  0:00:09s
epoch 114| loss: 0.6419  | val_0_rmse: 0.99535 | val_1_rmse: 0.91726 |  0:00:09s
epoch 115| loss: 0.62724 | val_0_rmse: 0.99286 | val_1_rmse: 0.91231 |  0:00:09s
epoch 116| loss: 0.60827 | val_0_rmse: 0.98118 | val_1_rmse: 0.90723 |  0:00:10s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.84745
Best weights from best epoch are automatically used!
ended training at: 14:12:55
Feature importance:
Mean squared error is of 0.1033661864440522
Mean absolute error:0.21263829556905034
MAPE:0.20812922803423647
R2 score:-0.07933096534736817
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:12:56
epoch 0  | loss: 3.07353 | val_0_rmse: 0.99532 | val_1_rmse: 1.0518  |  0:00:00s
epoch 1  | loss: 2.37953 | val_0_rmse: 0.9983  | val_1_rmse: 1.04783 |  0:00:00s
epoch 2  | loss: 2.09419 | val_0_rmse: 0.99305 | val_1_rmse: 1.04287 |  0:00:00s
epoch 3  | loss: 1.56401 | val_0_rmse: 0.98793 | val_1_rmse: 1.03813 |  0:00:00s
epoch 4  | loss: 1.60921 | val_0_rmse: 0.99009 | val_1_rmse: 1.04175 |  0:00:00s
epoch 5  | loss: 1.44479 | val_0_rmse: 0.99162 | val_1_rmse: 1.03938 |  0:00:00s
epoch 6  | loss: 1.30298 | val_0_rmse: 0.99185 | val_1_rmse: 1.03938 |  0:00:00s
epoch 7  | loss: 1.4679  | val_0_rmse: 0.99241 | val_1_rmse: 1.0411  |  0:00:00s
epoch 8  | loss: 1.28897 | val_0_rmse: 0.99354 | val_1_rmse: 1.04288 |  0:00:00s
epoch 9  | loss: 1.23797 | val_0_rmse: 0.99464 | val_1_rmse: 1.04287 |  0:00:00s
epoch 10 | loss: 1.2104  | val_0_rmse: 0.99216 | val_1_rmse: 1.04057 |  0:00:00s
epoch 11 | loss: 1.08294 | val_0_rmse: 0.98991 | val_1_rmse: 1.03863 |  0:00:01s
epoch 12 | loss: 1.04528 | val_0_rmse: 0.98845 | val_1_rmse: 1.03803 |  0:00:01s
epoch 13 | loss: 1.02764 | val_0_rmse: 0.98771 | val_1_rmse: 1.03697 |  0:00:01s
epoch 14 | loss: 0.98835 | val_0_rmse: 0.98765 | val_1_rmse: 1.03659 |  0:00:01s
epoch 15 | loss: 1.0035  | val_0_rmse: 0.98827 | val_1_rmse: 1.03743 |  0:00:01s
epoch 16 | loss: 1.04842 | val_0_rmse: 0.98867 | val_1_rmse: 1.03832 |  0:00:01s
epoch 17 | loss: 0.98047 | val_0_rmse: 0.98883 | val_1_rmse: 1.03798 |  0:00:01s
epoch 18 | loss: 0.99625 | val_0_rmse: 0.98825 | val_1_rmse: 1.0368  |  0:00:01s
epoch 19 | loss: 0.96676 | val_0_rmse: 0.98738 | val_1_rmse: 1.03553 |  0:00:01s
epoch 20 | loss: 0.93726 | val_0_rmse: 0.98652 | val_1_rmse: 1.03478 |  0:00:01s
epoch 21 | loss: 0.95824 | val_0_rmse: 0.98598 | val_1_rmse: 1.03168 |  0:00:01s
epoch 22 | loss: 0.94159 | val_0_rmse: 0.98643 | val_1_rmse: 1.02983 |  0:00:02s
epoch 23 | loss: 0.95245 | val_0_rmse: 0.98829 | val_1_rmse: 1.02881 |  0:00:02s
epoch 24 | loss: 0.95397 | val_0_rmse: 0.98856 | val_1_rmse: 1.02935 |  0:00:02s
epoch 25 | loss: 0.92735 | val_0_rmse: 0.98772 | val_1_rmse: 1.0301  |  0:00:02s
epoch 26 | loss: 0.91462 | val_0_rmse: 0.98781 | val_1_rmse: 1.03352 |  0:00:02s
epoch 27 | loss: 0.93521 | val_0_rmse: 0.98764 | val_1_rmse: 1.03254 |  0:00:02s
epoch 28 | loss: 0.93402 | val_0_rmse: 0.98752 | val_1_rmse: 1.03253 |  0:00:02s
epoch 29 | loss: 0.9341  | val_0_rmse: 0.98828 | val_1_rmse: 1.03543 |  0:00:02s
epoch 30 | loss: 0.93155 | val_0_rmse: 0.98704 | val_1_rmse: 1.03454 |  0:00:02s
epoch 31 | loss: 0.9372  | val_0_rmse: 0.98515 | val_1_rmse: 1.03135 |  0:00:02s
epoch 32 | loss: 0.92395 | val_0_rmse: 0.98364 | val_1_rmse: 1.02803 |  0:00:02s
epoch 33 | loss: 0.93196 | val_0_rmse: 0.98463 | val_1_rmse: 1.02677 |  0:00:02s
epoch 34 | loss: 0.91782 | val_0_rmse: 0.98452 | val_1_rmse: 1.02677 |  0:00:03s
epoch 35 | loss: 0.93087 | val_0_rmse: 0.98332 | val_1_rmse: 1.02694 |  0:00:03s
epoch 36 | loss: 0.93874 | val_0_rmse: 0.9819  | val_1_rmse: 1.02415 |  0:00:03s
epoch 37 | loss: 0.91352 | val_0_rmse: 0.98214 | val_1_rmse: 1.02424 |  0:00:03s
epoch 38 | loss: 0.90398 | val_0_rmse: 0.98351 | val_1_rmse: 1.02589 |  0:00:03s
epoch 39 | loss: 0.91623 | val_0_rmse: 0.98358 | val_1_rmse: 1.02791 |  0:00:03s
epoch 40 | loss: 0.90807 | val_0_rmse: 0.98349 | val_1_rmse: 1.02537 |  0:00:03s
epoch 41 | loss: 0.90061 | val_0_rmse: 0.98312 | val_1_rmse: 1.02485 |  0:00:03s
epoch 42 | loss: 0.91255 | val_0_rmse: 0.98225 | val_1_rmse: 1.02179 |  0:00:03s
epoch 43 | loss: 0.88855 | val_0_rmse: 0.98181 | val_1_rmse: 1.02092 |  0:00:03s
epoch 44 | loss: 0.8907  | val_0_rmse: 0.98166 | val_1_rmse: 1.0201  |  0:00:03s
epoch 45 | loss: 0.88646 | val_0_rmse: 0.98138 | val_1_rmse: 1.01999 |  0:00:04s
epoch 46 | loss: 0.8955  | val_0_rmse: 0.9804  | val_1_rmse: 1.01919 |  0:00:04s
epoch 47 | loss: 0.88033 | val_0_rmse: 0.97832 | val_1_rmse: 1.01892 |  0:00:04s
epoch 48 | loss: 0.85695 | val_0_rmse: 0.97427 | val_1_rmse: 1.01676 |  0:00:04s
epoch 49 | loss: 0.86641 | val_0_rmse: 0.97331 | val_1_rmse: 1.00869 |  0:00:04s
epoch 50 | loss: 0.86664 | val_0_rmse: 0.97359 | val_1_rmse: 1.00666 |  0:00:04s
epoch 51 | loss: 0.85963 | val_0_rmse: 0.9726  | val_1_rmse: 1.00906 |  0:00:04s
epoch 52 | loss: 0.8463  | val_0_rmse: 0.97394 | val_1_rmse: 1.01195 |  0:00:04s
epoch 53 | loss: 0.86083 | val_0_rmse: 0.97704 | val_1_rmse: 1.01703 |  0:00:04s
epoch 54 | loss: 0.84197 | val_0_rmse: 0.98003 | val_1_rmse: 1.02428 |  0:00:04s
epoch 55 | loss: 0.82609 | val_0_rmse: 0.98045 | val_1_rmse: 1.02571 |  0:00:04s
epoch 56 | loss: 0.83483 | val_0_rmse: 0.98164 | val_1_rmse: 1.03237 |  0:00:05s
epoch 57 | loss: 0.82985 | val_0_rmse: 0.98367 | val_1_rmse: 1.03687 |  0:00:05s
epoch 58 | loss: 0.81412 | val_0_rmse: 0.98606 | val_1_rmse: 1.03681 |  0:00:05s
epoch 59 | loss: 0.8182  | val_0_rmse: 0.98491 | val_1_rmse: 1.03226 |  0:00:05s
epoch 60 | loss: 0.80783 | val_0_rmse: 0.98255 | val_1_rmse: 1.02855 |  0:00:05s
epoch 61 | loss: 0.8026  | val_0_rmse: 0.98151 | val_1_rmse: 1.02205 |  0:00:05s
epoch 62 | loss: 0.80094 | val_0_rmse: 0.98191 | val_1_rmse: 1.01844 |  0:00:05s
epoch 63 | loss: 0.79275 | val_0_rmse: 0.98748 | val_1_rmse: 1.02742 |  0:00:05s
epoch 64 | loss: 0.80514 | val_0_rmse: 0.99194 | val_1_rmse: 1.0321  |  0:00:05s
epoch 65 | loss: 0.77976 | val_0_rmse: 0.99068 | val_1_rmse: 1.02728 |  0:00:05s
epoch 66 | loss: 0.8015  | val_0_rmse: 0.98968 | val_1_rmse: 1.02513 |  0:00:05s
epoch 67 | loss: 0.79561 | val_0_rmse: 0.98911 | val_1_rmse: 1.02448 |  0:00:05s
epoch 68 | loss: 0.7769  | val_0_rmse: 0.99429 | val_1_rmse: 1.03067 |  0:00:06s
epoch 69 | loss: 0.80316 | val_0_rmse: 1.00274 | val_1_rmse: 1.04966 |  0:00:06s
epoch 70 | loss: 0.7692  | val_0_rmse: 1.00346 | val_1_rmse: 1.05421 |  0:00:06s
epoch 71 | loss: 0.7775  | val_0_rmse: 1.00047 | val_1_rmse: 1.05451 |  0:00:06s
epoch 72 | loss: 0.77283 | val_0_rmse: 0.99991 | val_1_rmse: 1.06112 |  0:00:06s
epoch 73 | loss: 0.78244 | val_0_rmse: 1.0007  | val_1_rmse: 1.06348 |  0:00:06s
epoch 74 | loss: 0.78983 | val_0_rmse: 1.00289 | val_1_rmse: 1.06568 |  0:00:06s
epoch 75 | loss: 0.75452 | val_0_rmse: 1.00539 | val_1_rmse: 1.06459 |  0:00:06s
epoch 76 | loss: 0.74852 | val_0_rmse: 1.00071 | val_1_rmse: 1.05905 |  0:00:06s
epoch 77 | loss: 0.76779 | val_0_rmse: 0.99217 | val_1_rmse: 1.0481  |  0:00:06s
epoch 78 | loss: 0.74753 | val_0_rmse: 0.98689 | val_1_rmse: 1.04161 |  0:00:06s
epoch 79 | loss: 0.74658 | val_0_rmse: 0.98709 | val_1_rmse: 1.04247 |  0:00:06s
epoch 80 | loss: 0.74925 | val_0_rmse: 0.99212 | val_1_rmse: 1.05031 |  0:00:07s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 1.00666
Best weights from best epoch are automatically used!
ended training at: 14:13:03
Feature importance:
Mean squared error is of 0.09387533009443846
Mean absolute error:0.20968901821210148
MAPE:0.2214942627538175
R2 score:-0.00024994517865084553
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:13:04
epoch 0  | loss: 2.05815 | val_0_rmse: 1.02835 | val_1_rmse: 0.93928 |  0:00:00s
epoch 1  | loss: 1.20089 | val_0_rmse: 1.02745 | val_1_rmse: 0.93705 |  0:00:00s
epoch 2  | loss: 1.10776 | val_0_rmse: 1.02663 | val_1_rmse: 0.93737 |  0:00:01s
epoch 3  | loss: 1.08064 | val_0_rmse: 1.02708 | val_1_rmse: 0.93915 |  0:00:01s
epoch 4  | loss: 1.05861 | val_0_rmse: 1.02685 | val_1_rmse: 0.93797 |  0:00:01s
epoch 5  | loss: 1.05523 | val_0_rmse: 1.02818 | val_1_rmse: 0.94212 |  0:00:01s
epoch 6  | loss: 1.05185 | val_0_rmse: 1.02598 | val_1_rmse: 0.93768 |  0:00:02s
epoch 7  | loss: 1.0393  | val_0_rmse: 1.01956 | val_1_rmse: 0.93253 |  0:00:02s
epoch 8  | loss: 1.0141  | val_0_rmse: 0.97784 | val_1_rmse: 0.8967  |  0:00:02s
epoch 9  | loss: 1.0107  | val_0_rmse: 0.93907 | val_1_rmse: 0.84867 |  0:00:03s
epoch 10 | loss: 0.97342 | val_0_rmse: 0.92082 | val_1_rmse: 0.83556 |  0:00:03s
epoch 11 | loss: 0.95459 | val_0_rmse: 0.94274 | val_1_rmse: 0.88301 |  0:00:03s
epoch 12 | loss: 0.92001 | val_0_rmse: 0.94576 | val_1_rmse: 0.88886 |  0:00:03s
epoch 13 | loss: 0.88535 | val_0_rmse: 0.92244 | val_1_rmse: 0.85504 |  0:00:04s
epoch 14 | loss: 0.83771 | val_0_rmse: 0.929   | val_1_rmse: 0.86253 |  0:00:04s
epoch 15 | loss: 0.8448  | val_0_rmse: 0.88932 | val_1_rmse: 0.82282 |  0:00:04s
epoch 16 | loss: 0.83579 | val_0_rmse: 0.89219 | val_1_rmse: 0.81797 |  0:00:05s
epoch 17 | loss: 0.82716 | val_0_rmse: 0.88421 | val_1_rmse: 0.81909 |  0:00:05s
epoch 18 | loss: 0.83046 | val_0_rmse: 0.90082 | val_1_rmse: 0.84239 |  0:00:05s
epoch 19 | loss: 0.81807 | val_0_rmse: 0.88974 | val_1_rmse: 0.81895 |  0:00:06s
epoch 20 | loss: 0.80311 | val_0_rmse: 0.8883  | val_1_rmse: 0.8195  |  0:00:06s
epoch 21 | loss: 0.79419 | val_0_rmse: 0.88106 | val_1_rmse: 0.81145 |  0:00:06s
epoch 22 | loss: 0.79383 | val_0_rmse: 0.88136 | val_1_rmse: 0.81154 |  0:00:06s
epoch 23 | loss: 0.77905 | val_0_rmse: 0.87912 | val_1_rmse: 0.81302 |  0:00:07s
epoch 24 | loss: 0.77208 | val_0_rmse: 0.88199 | val_1_rmse: 0.81127 |  0:00:07s
epoch 25 | loss: 0.78019 | val_0_rmse: 0.88348 | val_1_rmse: 0.81555 |  0:00:07s
epoch 26 | loss: 0.78008 | val_0_rmse: 0.88106 | val_1_rmse: 0.82439 |  0:00:08s
epoch 27 | loss: 0.78293 | val_0_rmse: 0.88355 | val_1_rmse: 0.81733 |  0:00:08s
epoch 28 | loss: 0.77874 | val_0_rmse: 0.87549 | val_1_rmse: 0.81478 |  0:00:08s
epoch 29 | loss: 0.76977 | val_0_rmse: 0.87538 | val_1_rmse: 0.82168 |  0:00:09s
epoch 30 | loss: 0.77736 | val_0_rmse: 0.87711 | val_1_rmse: 0.81595 |  0:00:09s
epoch 31 | loss: 0.77007 | val_0_rmse: 0.87564 | val_1_rmse: 0.80937 |  0:00:09s
epoch 32 | loss: 0.76191 | val_0_rmse: 0.86708 | val_1_rmse: 0.80281 |  0:00:09s
epoch 33 | loss: 0.75055 | val_0_rmse: 0.87382 | val_1_rmse: 0.80482 |  0:00:10s
epoch 34 | loss: 0.75528 | val_0_rmse: 0.8733  | val_1_rmse: 0.80572 |  0:00:10s
epoch 35 | loss: 0.74865 | val_0_rmse: 0.86232 | val_1_rmse: 0.79164 |  0:00:10s
epoch 36 | loss: 0.75941 | val_0_rmse: 0.87578 | val_1_rmse: 0.80554 |  0:00:11s
epoch 37 | loss: 0.75217 | val_0_rmse: 0.86282 | val_1_rmse: 0.79376 |  0:00:11s
epoch 38 | loss: 0.75485 | val_0_rmse: 0.86295 | val_1_rmse: 0.7996  |  0:00:11s
epoch 39 | loss: 0.74911 | val_0_rmse: 0.87035 | val_1_rmse: 0.81018 |  0:00:12s
epoch 40 | loss: 0.7472  | val_0_rmse: 0.86161 | val_1_rmse: 0.79504 |  0:00:12s
epoch 41 | loss: 0.74545 | val_0_rmse: 0.85613 | val_1_rmse: 0.79112 |  0:00:12s
epoch 42 | loss: 0.7384  | val_0_rmse: 0.86033 | val_1_rmse: 0.80147 |  0:00:13s
epoch 43 | loss: 0.73731 | val_0_rmse: 0.866   | val_1_rmse: 0.79883 |  0:00:13s
epoch 44 | loss: 0.73316 | val_0_rmse: 0.85709 | val_1_rmse: 0.79216 |  0:00:13s
epoch 45 | loss: 0.73895 | val_0_rmse: 0.85747 | val_1_rmse: 0.79701 |  0:00:13s
epoch 46 | loss: 0.73174 | val_0_rmse: 0.85688 | val_1_rmse: 0.79812 |  0:00:14s
epoch 47 | loss: 0.73452 | val_0_rmse: 0.86386 | val_1_rmse: 0.8078  |  0:00:14s
epoch 48 | loss: 0.73228 | val_0_rmse: 0.86401 | val_1_rmse: 0.79808 |  0:00:14s
epoch 49 | loss: 0.73496 | val_0_rmse: 0.85947 | val_1_rmse: 0.79821 |  0:00:15s
epoch 50 | loss: 0.72307 | val_0_rmse: 0.87128 | val_1_rmse: 0.81791 |  0:00:15s
epoch 51 | loss: 0.72735 | val_0_rmse: 0.85947 | val_1_rmse: 0.80428 |  0:00:15s
epoch 52 | loss: 0.71716 | val_0_rmse: 0.8631  | val_1_rmse: 0.80757 |  0:00:15s
epoch 53 | loss: 0.72157 | val_0_rmse: 0.85422 | val_1_rmse: 0.79883 |  0:00:16s
epoch 54 | loss: 0.71475 | val_0_rmse: 0.86562 | val_1_rmse: 0.81295 |  0:00:16s
epoch 55 | loss: 0.72043 | val_0_rmse: 0.85393 | val_1_rmse: 0.79709 |  0:00:16s
epoch 56 | loss: 0.70768 | val_0_rmse: 0.85722 | val_1_rmse: 0.81129 |  0:00:17s
epoch 57 | loss: 0.70858 | val_0_rmse: 0.85774 | val_1_rmse: 0.81396 |  0:00:17s
epoch 58 | loss: 0.71283 | val_0_rmse: 0.85228 | val_1_rmse: 0.80007 |  0:00:17s
epoch 59 | loss: 0.70481 | val_0_rmse: 0.8551  | val_1_rmse: 0.79619 |  0:00:18s
epoch 60 | loss: 0.69679 | val_0_rmse: 0.84605 | val_1_rmse: 0.79959 |  0:00:18s
epoch 61 | loss: 0.68508 | val_0_rmse: 0.84682 | val_1_rmse: 0.80411 |  0:00:18s
epoch 62 | loss: 0.69028 | val_0_rmse: 0.84111 | val_1_rmse: 0.79708 |  0:00:18s
epoch 63 | loss: 0.69004 | val_0_rmse: 0.84339 | val_1_rmse: 0.80565 |  0:00:19s
epoch 64 | loss: 0.68541 | val_0_rmse: 0.84069 | val_1_rmse: 0.81098 |  0:00:19s
epoch 65 | loss: 0.69617 | val_0_rmse: 0.84064 | val_1_rmse: 0.80566 |  0:00:19s
epoch 66 | loss: 0.698   | val_0_rmse: 0.83954 | val_1_rmse: 0.81094 |  0:00:20s
epoch 67 | loss: 0.69141 | val_0_rmse: 0.83742 | val_1_rmse: 0.80156 |  0:00:20s
epoch 68 | loss: 0.68271 | val_0_rmse: 0.85228 | val_1_rmse: 0.81158 |  0:00:20s
epoch 69 | loss: 0.67558 | val_0_rmse: 0.83705 | val_1_rmse: 0.81683 |  0:00:21s
epoch 70 | loss: 0.6721  | val_0_rmse: 0.84308 | val_1_rmse: 0.82715 |  0:00:21s
epoch 71 | loss: 0.67398 | val_0_rmse: 0.83799 | val_1_rmse: 0.80841 |  0:00:21s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.79112
Best weights from best epoch are automatically used!
ended training at: 14:13:25
Feature importance:
Mean squared error is of 0.06671023641671049
Mean absolute error:0.19200741477962663
MAPE:0.21288782109808518
R2 score:0.18171396962552033
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:13:26
epoch 0  | loss: 1.97112 | val_0_rmse: 1.00146 | val_1_rmse: 1.03233 |  0:00:00s
epoch 1  | loss: 1.13596 | val_0_rmse: 0.98992 | val_1_rmse: 1.0186  |  0:00:00s
epoch 2  | loss: 1.00072 | val_0_rmse: 0.98909 | val_1_rmse: 1.01918 |  0:00:00s
epoch 3  | loss: 0.97059 | val_0_rmse: 0.97679 | val_1_rmse: 1.01432 |  0:00:01s
epoch 4  | loss: 0.96015 | val_0_rmse: 0.97675 | val_1_rmse: 1.02031 |  0:00:01s
epoch 5  | loss: 0.95841 | val_0_rmse: 0.98499 | val_1_rmse: 1.03482 |  0:00:01s
epoch 6  | loss: 0.95092 | val_0_rmse: 0.97108 | val_1_rmse: 1.01443 |  0:00:02s
epoch 7  | loss: 0.93963 | val_0_rmse: 0.96869 | val_1_rmse: 1.00842 |  0:00:02s
epoch 8  | loss: 0.93411 | val_0_rmse: 0.96687 | val_1_rmse: 1.01051 |  0:00:02s
epoch 9  | loss: 0.93148 | val_0_rmse: 0.96247 | val_1_rmse: 1.00383 |  0:00:02s
epoch 10 | loss: 0.9251  | val_0_rmse: 0.96532 | val_1_rmse: 1.00402 |  0:00:03s
epoch 11 | loss: 0.92957 | val_0_rmse: 0.96382 | val_1_rmse: 1.00473 |  0:00:03s
epoch 12 | loss: 0.92345 | val_0_rmse: 0.9669  | val_1_rmse: 1.00229 |  0:00:03s
epoch 13 | loss: 0.92679 | val_0_rmse: 0.96434 | val_1_rmse: 1.00437 |  0:00:04s
epoch 14 | loss: 0.92107 | val_0_rmse: 0.95967 | val_1_rmse: 0.99925 |  0:00:04s
epoch 15 | loss: 0.92008 | val_0_rmse: 0.95884 | val_1_rmse: 0.99807 |  0:00:04s
epoch 16 | loss: 0.911   | val_0_rmse: 0.95904 | val_1_rmse: 0.99557 |  0:00:05s
epoch 17 | loss: 0.90991 | val_0_rmse: 0.96482 | val_1_rmse: 0.99718 |  0:00:05s
epoch 18 | loss: 0.9203  | val_0_rmse: 0.95632 | val_1_rmse: 0.9919  |  0:00:05s
epoch 19 | loss: 0.90612 | val_0_rmse: 0.95264 | val_1_rmse: 0.98698 |  0:00:05s
epoch 20 | loss: 0.90221 | val_0_rmse: 0.94708 | val_1_rmse: 0.98095 |  0:00:06s
epoch 21 | loss: 0.88165 | val_0_rmse: 0.93361 | val_1_rmse: 0.96781 |  0:00:06s
epoch 22 | loss: 0.8579  | val_0_rmse: 0.90891 | val_1_rmse: 0.93987 |  0:00:06s
epoch 23 | loss: 0.85545 | val_0_rmse: 0.90384 | val_1_rmse: 0.93116 |  0:00:07s
epoch 24 | loss: 0.84089 | val_0_rmse: 0.89334 | val_1_rmse: 0.91921 |  0:00:07s
epoch 25 | loss: 0.82334 | val_0_rmse: 0.88041 | val_1_rmse: 0.89685 |  0:00:07s
epoch 26 | loss: 0.80279 | val_0_rmse: 0.88336 | val_1_rmse: 0.90175 |  0:00:08s
epoch 27 | loss: 0.80886 | val_0_rmse: 0.88104 | val_1_rmse: 0.89899 |  0:00:08s
epoch 28 | loss: 0.79324 | val_0_rmse: 0.88527 | val_1_rmse: 0.9016  |  0:00:08s
epoch 29 | loss: 0.772   | val_0_rmse: 0.8791  | val_1_rmse: 0.90443 |  0:00:08s
epoch 30 | loss: 0.75609 | val_0_rmse: 0.87953 | val_1_rmse: 0.89609 |  0:00:09s
epoch 31 | loss: 0.7444  | val_0_rmse: 0.87576 | val_1_rmse: 0.89693 |  0:00:09s
epoch 32 | loss: 0.7439  | val_0_rmse: 0.85836 | val_1_rmse: 0.87737 |  0:00:09s
epoch 33 | loss: 0.73387 | val_0_rmse: 0.85797 | val_1_rmse: 0.87817 |  0:00:10s
epoch 34 | loss: 0.73115 | val_0_rmse: 0.864   | val_1_rmse: 0.88904 |  0:00:10s
epoch 35 | loss: 0.71675 | val_0_rmse: 0.8523  | val_1_rmse: 0.87697 |  0:00:10s
epoch 36 | loss: 0.71645 | val_0_rmse: 0.86098 | val_1_rmse: 0.88507 |  0:00:11s
epoch 37 | loss: 0.70763 | val_0_rmse: 0.86598 | val_1_rmse: 0.889   |  0:00:11s
epoch 38 | loss: 0.7092  | val_0_rmse: 0.85876 | val_1_rmse: 0.87599 |  0:00:11s
epoch 39 | loss: 0.70673 | val_0_rmse: 0.86308 | val_1_rmse: 0.88166 |  0:00:11s
epoch 40 | loss: 0.69552 | val_0_rmse: 0.84689 | val_1_rmse: 0.86604 |  0:00:12s
epoch 41 | loss: 0.69027 | val_0_rmse: 0.84313 | val_1_rmse: 0.86601 |  0:00:12s
epoch 42 | loss: 0.68319 | val_0_rmse: 0.84869 | val_1_rmse: 0.87755 |  0:00:12s
epoch 43 | loss: 0.68549 | val_0_rmse: 0.84196 | val_1_rmse: 0.87054 |  0:00:13s
epoch 44 | loss: 0.68192 | val_0_rmse: 0.84049 | val_1_rmse: 0.86781 |  0:00:13s
epoch 45 | loss: 0.67602 | val_0_rmse: 0.84728 | val_1_rmse: 0.87211 |  0:00:13s
epoch 46 | loss: 0.6931  | val_0_rmse: 0.84606 | val_1_rmse: 0.86922 |  0:00:13s
epoch 47 | loss: 0.68993 | val_0_rmse: 0.84541 | val_1_rmse: 0.86734 |  0:00:14s
epoch 48 | loss: 0.68422 | val_0_rmse: 0.84296 | val_1_rmse: 0.86749 |  0:00:14s
epoch 49 | loss: 0.66575 | val_0_rmse: 0.90197 | val_1_rmse: 0.93948 |  0:00:14s
epoch 50 | loss: 0.69904 | val_0_rmse: 0.83801 | val_1_rmse: 0.86482 |  0:00:15s
epoch 51 | loss: 0.68736 | val_0_rmse: 0.836   | val_1_rmse: 0.86847 |  0:00:15s
epoch 52 | loss: 0.67957 | val_0_rmse: 0.83487 | val_1_rmse: 0.8689  |  0:00:15s
epoch 53 | loss: 0.67526 | val_0_rmse: 0.84063 | val_1_rmse: 0.87295 |  0:00:16s
epoch 54 | loss: 0.6709  | val_0_rmse: 0.83317 | val_1_rmse: 0.8671  |  0:00:16s
epoch 55 | loss: 0.67333 | val_0_rmse: 0.8285  | val_1_rmse: 0.8665  |  0:00:16s
epoch 56 | loss: 0.66574 | val_0_rmse: 0.82988 | val_1_rmse: 0.86585 |  0:00:17s
epoch 57 | loss: 0.65999 | val_0_rmse: 0.82665 | val_1_rmse: 0.87027 |  0:00:17s
epoch 58 | loss: 0.66188 | val_0_rmse: 0.82916 | val_1_rmse: 0.86566 |  0:00:17s
epoch 59 | loss: 0.64971 | val_0_rmse: 0.83153 | val_1_rmse: 0.86811 |  0:00:17s
epoch 60 | loss: 0.64915 | val_0_rmse: 0.82182 | val_1_rmse: 0.86056 |  0:00:18s
epoch 61 | loss: 0.65872 | val_0_rmse: 0.81662 | val_1_rmse: 0.85914 |  0:00:18s
epoch 62 | loss: 0.65632 | val_0_rmse: 0.8231  | val_1_rmse: 0.87189 |  0:00:18s
epoch 63 | loss: 0.65225 | val_0_rmse: 0.81789 | val_1_rmse: 0.86726 |  0:00:19s
epoch 64 | loss: 0.66689 | val_0_rmse: 0.83194 | val_1_rmse: 0.8694  |  0:00:19s
epoch 65 | loss: 0.66556 | val_0_rmse: 0.83078 | val_1_rmse: 0.86405 |  0:00:19s
epoch 66 | loss: 0.67259 | val_0_rmse: 0.84683 | val_1_rmse: 0.86912 |  0:00:20s
epoch 67 | loss: 0.67855 | val_0_rmse: 0.83995 | val_1_rmse: 0.86252 |  0:00:20s
epoch 68 | loss: 0.67076 | val_0_rmse: 0.828   | val_1_rmse: 0.85518 |  0:00:20s
epoch 69 | loss: 0.66486 | val_0_rmse: 0.82291 | val_1_rmse: 0.85681 |  0:00:20s
epoch 70 | loss: 0.65759 | val_0_rmse: 0.82339 | val_1_rmse: 0.85424 |  0:00:21s
epoch 71 | loss: 0.65627 | val_0_rmse: 0.82094 | val_1_rmse: 0.8522  |  0:00:21s
epoch 72 | loss: 0.64678 | val_0_rmse: 0.82042 | val_1_rmse: 0.85379 |  0:00:21s
epoch 73 | loss: 0.64952 | val_0_rmse: 0.816   | val_1_rmse: 0.85241 |  0:00:22s
epoch 74 | loss: 0.65548 | val_0_rmse: 0.83803 | val_1_rmse: 0.87145 |  0:00:22s
epoch 75 | loss: 0.65545 | val_0_rmse: 0.81875 | val_1_rmse: 0.85283 |  0:00:22s
epoch 76 | loss: 0.65131 | val_0_rmse: 0.81725 | val_1_rmse: 0.86051 |  0:00:23s
epoch 77 | loss: 0.64116 | val_0_rmse: 0.80523 | val_1_rmse: 0.84977 |  0:00:23s
epoch 78 | loss: 0.64868 | val_0_rmse: 0.80926 | val_1_rmse: 0.85652 |  0:00:23s
epoch 79 | loss: 0.62682 | val_0_rmse: 0.80089 | val_1_rmse: 0.84806 |  0:00:23s
epoch 80 | loss: 0.63847 | val_0_rmse: 0.8012  | val_1_rmse: 0.8487  |  0:00:24s
epoch 81 | loss: 0.63479 | val_0_rmse: 0.81508 | val_1_rmse: 0.86378 |  0:00:24s
epoch 82 | loss: 0.64174 | val_0_rmse: 0.81039 | val_1_rmse: 0.85364 |  0:00:24s
epoch 83 | loss: 0.64906 | val_0_rmse: 0.80804 | val_1_rmse: 0.84936 |  0:00:25s
epoch 84 | loss: 0.63633 | val_0_rmse: 0.81327 | val_1_rmse: 0.85806 |  0:00:25s
epoch 85 | loss: 0.63567 | val_0_rmse: 0.8041  | val_1_rmse: 0.85525 |  0:00:25s
epoch 86 | loss: 0.6363  | val_0_rmse: 0.7986  | val_1_rmse: 0.84825 |  0:00:25s
epoch 87 | loss: 0.62996 | val_0_rmse: 0.81062 | val_1_rmse: 0.86011 |  0:00:26s
epoch 88 | loss: 0.62525 | val_0_rmse: 0.79618 | val_1_rmse: 0.85047 |  0:00:26s
epoch 89 | loss: 0.63563 | val_0_rmse: 0.80258 | val_1_rmse: 0.85357 |  0:00:26s
epoch 90 | loss: 0.64241 | val_0_rmse: 0.80443 | val_1_rmse: 0.84917 |  0:00:27s
epoch 91 | loss: 0.6415  | val_0_rmse: 0.80346 | val_1_rmse: 0.85196 |  0:00:27s
epoch 92 | loss: 0.63162 | val_0_rmse: 0.79856 | val_1_rmse: 0.84379 |  0:00:27s
epoch 93 | loss: 0.62971 | val_0_rmse: 0.79358 | val_1_rmse: 0.84793 |  0:00:28s
epoch 94 | loss: 0.61815 | val_0_rmse: 0.78848 | val_1_rmse: 0.85399 |  0:00:28s
epoch 95 | loss: 0.62507 | val_0_rmse: 0.78751 | val_1_rmse: 0.85398 |  0:00:28s
epoch 96 | loss: 0.61554 | val_0_rmse: 0.78603 | val_1_rmse: 0.85606 |  0:00:28s
epoch 97 | loss: 0.61041 | val_0_rmse: 0.78052 | val_1_rmse: 0.86156 |  0:00:29s
epoch 98 | loss: 0.62832 | val_0_rmse: 0.78372 | val_1_rmse: 0.85006 |  0:00:29s
epoch 99 | loss: 0.61393 | val_0_rmse: 0.78331 | val_1_rmse: 0.84743 |  0:00:29s
epoch 100| loss: 0.61499 | val_0_rmse: 0.77661 | val_1_rmse: 0.85107 |  0:00:30s
epoch 101| loss: 0.6031  | val_0_rmse: 0.79233 | val_1_rmse: 0.86429 |  0:00:30s
epoch 102| loss: 0.60372 | val_0_rmse: 0.78692 | val_1_rmse: 0.85233 |  0:00:30s
epoch 103| loss: 0.61274 | val_0_rmse: 0.79346 | val_1_rmse: 0.85905 |  0:00:30s
epoch 104| loss: 0.61901 | val_0_rmse: 0.82302 | val_1_rmse: 0.89242 |  0:00:31s
epoch 105| loss: 0.62771 | val_0_rmse: 0.78462 | val_1_rmse: 0.86362 |  0:00:31s
epoch 106| loss: 0.61593 | val_0_rmse: 0.77955 | val_1_rmse: 0.8643  |  0:00:31s
epoch 107| loss: 0.61062 | val_0_rmse: 0.77814 | val_1_rmse: 0.8604  |  0:00:32s
epoch 108| loss: 0.60567 | val_0_rmse: 0.77738 | val_1_rmse: 0.85729 |  0:00:32s
epoch 109| loss: 0.60919 | val_0_rmse: 0.78517 | val_1_rmse: 0.86966 |  0:00:32s
epoch 110| loss: 0.60909 | val_0_rmse: 0.77365 | val_1_rmse: 0.85971 |  0:00:33s
epoch 111| loss: 0.5999  | val_0_rmse: 0.78014 | val_1_rmse: 0.8562  |  0:00:33s
epoch 112| loss: 0.6045  | val_0_rmse: 0.77885 | val_1_rmse: 0.85833 |  0:00:33s
epoch 113| loss: 0.60142 | val_0_rmse: 0.77704 | val_1_rmse: 0.8647  |  0:00:33s
epoch 114| loss: 0.6036  | val_0_rmse: 0.78279 | val_1_rmse: 0.86426 |  0:00:34s
epoch 115| loss: 0.59726 | val_0_rmse: 0.77908 | val_1_rmse: 0.86293 |  0:00:34s
epoch 116| loss: 0.59343 | val_0_rmse: 0.76643 | val_1_rmse: 0.86127 |  0:00:34s
epoch 117| loss: 0.59479 | val_0_rmse: 0.76436 | val_1_rmse: 0.85222 |  0:00:35s
epoch 118| loss: 0.58744 | val_0_rmse: 0.7643  | val_1_rmse: 0.85267 |  0:00:35s
epoch 119| loss: 0.59102 | val_0_rmse: 0.76736 | val_1_rmse: 0.85894 |  0:00:35s
epoch 120| loss: 0.58661 | val_0_rmse: 0.76469 | val_1_rmse: 0.85989 |  0:00:36s
epoch 121| loss: 0.59044 | val_0_rmse: 0.76253 | val_1_rmse: 0.8573  |  0:00:36s
epoch 122| loss: 0.59078 | val_0_rmse: 0.75874 | val_1_rmse: 0.85639 |  0:00:36s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.84379
Best weights from best epoch are automatically used!
ended training at: 14:14:03
Feature importance:
Mean squared error is of 0.07362728634975203
Mean absolute error:0.19247313976318156
MAPE:0.2026527396022188
R2 score:0.23156747945915168
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:14:03
epoch 0  | loss: 2.03754 | val_0_rmse: 1.04639 | val_1_rmse: 1.05727 |  0:00:00s
epoch 1  | loss: 1.34855 | val_0_rmse: 1.01107 | val_1_rmse: 1.02419 |  0:00:00s
epoch 2  | loss: 1.09116 | val_0_rmse: 1.00985 | val_1_rmse: 1.02116 |  0:00:00s
epoch 3  | loss: 1.02322 | val_0_rmse: 1.00805 | val_1_rmse: 1.01895 |  0:00:01s
epoch 4  | loss: 1.01066 | val_0_rmse: 1.00203 | val_1_rmse: 1.01333 |  0:00:01s
epoch 5  | loss: 0.9945  | val_0_rmse: 0.98155 | val_1_rmse: 0.99511 |  0:00:01s
epoch 6  | loss: 0.91798 | val_0_rmse: 0.96064 | val_1_rmse: 0.97828 |  0:00:02s
epoch 7  | loss: 0.87626 | val_0_rmse: 0.93607 | val_1_rmse: 0.95215 |  0:00:02s
epoch 8  | loss: 0.86028 | val_0_rmse: 0.92653 | val_1_rmse: 0.9436  |  0:00:02s
epoch 9  | loss: 0.84631 | val_0_rmse: 0.90896 | val_1_rmse: 0.92379 |  0:00:03s
epoch 10 | loss: 0.82558 | val_0_rmse: 0.90867 | val_1_rmse: 0.92555 |  0:00:03s
epoch 11 | loss: 0.83097 | val_0_rmse: 0.9104  | val_1_rmse: 0.92782 |  0:00:03s
epoch 12 | loss: 0.82297 | val_0_rmse: 0.90355 | val_1_rmse: 0.92201 |  0:00:03s
epoch 13 | loss: 0.80718 | val_0_rmse: 0.89393 | val_1_rmse: 0.91266 |  0:00:04s
epoch 14 | loss: 0.80747 | val_0_rmse: 0.90005 | val_1_rmse: 0.90544 |  0:00:04s
epoch 15 | loss: 0.80802 | val_0_rmse: 0.90562 | val_1_rmse: 0.91696 |  0:00:04s
epoch 16 | loss: 0.81568 | val_0_rmse: 0.90693 | val_1_rmse: 0.92432 |  0:00:05s
epoch 17 | loss: 0.80537 | val_0_rmse: 0.90438 | val_1_rmse: 0.93152 |  0:00:05s
epoch 18 | loss: 0.80319 | val_0_rmse: 0.88478 | val_1_rmse: 0.90697 |  0:00:05s
epoch 19 | loss: 0.80177 | val_0_rmse: 0.88521 | val_1_rmse: 0.90593 |  0:00:05s
epoch 20 | loss: 0.79651 | val_0_rmse: 0.90293 | val_1_rmse: 0.9146  |  0:00:06s
epoch 21 | loss: 0.79927 | val_0_rmse: 0.896   | val_1_rmse: 0.90388 |  0:00:06s
epoch 22 | loss: 0.79187 | val_0_rmse: 0.89174 | val_1_rmse: 0.91269 |  0:00:06s
epoch 23 | loss: 0.79191 | val_0_rmse: 0.90244 | val_1_rmse: 0.91708 |  0:00:07s
epoch 24 | loss: 0.78736 | val_0_rmse: 0.89512 | val_1_rmse: 0.91715 |  0:00:07s
epoch 25 | loss: 0.79206 | val_0_rmse: 0.89215 | val_1_rmse: 0.91398 |  0:00:07s
epoch 26 | loss: 0.78724 | val_0_rmse: 0.90113 | val_1_rmse: 0.91171 |  0:00:08s
epoch 27 | loss: 0.78702 | val_0_rmse: 0.88716 | val_1_rmse: 0.89954 |  0:00:08s
epoch 28 | loss: 0.78069 | val_0_rmse: 0.88947 | val_1_rmse: 0.90241 |  0:00:08s
epoch 29 | loss: 0.78107 | val_0_rmse: 0.8858  | val_1_rmse: 0.90282 |  0:00:08s
epoch 30 | loss: 0.77504 | val_0_rmse: 0.88457 | val_1_rmse: 0.90102 |  0:00:09s
epoch 31 | loss: 0.77461 | val_0_rmse: 0.88612 | val_1_rmse: 0.89837 |  0:00:09s
epoch 32 | loss: 0.76937 | val_0_rmse: 0.88617 | val_1_rmse: 0.89915 |  0:00:09s
epoch 33 | loss: 0.76633 | val_0_rmse: 0.88272 | val_1_rmse: 0.89685 |  0:00:10s
epoch 34 | loss: 0.78666 | val_0_rmse: 0.89076 | val_1_rmse: 0.90406 |  0:00:10s
epoch 35 | loss: 0.77737 | val_0_rmse: 0.89512 | val_1_rmse: 0.91304 |  0:00:10s
epoch 36 | loss: 0.78223 | val_0_rmse: 0.8888  | val_1_rmse: 0.89861 |  0:00:11s
epoch 37 | loss: 0.78349 | val_0_rmse: 0.89036 | val_1_rmse: 0.9011  |  0:00:11s
epoch 38 | loss: 0.79217 | val_0_rmse: 0.89302 | val_1_rmse: 0.90874 |  0:00:11s
epoch 39 | loss: 0.79405 | val_0_rmse: 0.89104 | val_1_rmse: 0.90791 |  0:00:11s
epoch 40 | loss: 0.78686 | val_0_rmse: 0.89023 | val_1_rmse: 0.90868 |  0:00:12s
epoch 41 | loss: 0.79656 | val_0_rmse: 0.88734 | val_1_rmse: 0.90588 |  0:00:12s
epoch 42 | loss: 0.79059 | val_0_rmse: 0.88819 | val_1_rmse: 0.90844 |  0:00:12s
epoch 43 | loss: 0.78818 | val_0_rmse: 0.88406 | val_1_rmse: 0.90635 |  0:00:13s
epoch 44 | loss: 0.78126 | val_0_rmse: 0.88495 | val_1_rmse: 0.91158 |  0:00:13s
epoch 45 | loss: 0.78456 | val_0_rmse: 0.87916 | val_1_rmse: 0.89937 |  0:00:13s
epoch 46 | loss: 0.78325 | val_0_rmse: 0.87998 | val_1_rmse: 0.89977 |  0:00:13s
epoch 47 | loss: 0.77926 | val_0_rmse: 0.88379 | val_1_rmse: 0.90576 |  0:00:14s
epoch 48 | loss: 0.78144 | val_0_rmse: 0.88828 | val_1_rmse: 0.91103 |  0:00:14s
epoch 49 | loss: 0.77819 | val_0_rmse: 0.88988 | val_1_rmse: 0.91097 |  0:00:14s
epoch 50 | loss: 0.77985 | val_0_rmse: 0.89226 | val_1_rmse: 0.90716 |  0:00:15s
epoch 51 | loss: 0.78233 | val_0_rmse: 0.90379 | val_1_rmse: 0.92702 |  0:00:15s
epoch 52 | loss: 0.77646 | val_0_rmse: 0.88582 | val_1_rmse: 0.91045 |  0:00:15s
epoch 53 | loss: 0.76983 | val_0_rmse: 0.88764 | val_1_rmse: 0.91379 |  0:00:16s
epoch 54 | loss: 0.77567 | val_0_rmse: 0.88838 | val_1_rmse: 0.92082 |  0:00:16s
epoch 55 | loss: 0.76987 | val_0_rmse: 0.87906 | val_1_rmse: 0.90639 |  0:00:16s
epoch 56 | loss: 0.76667 | val_0_rmse: 0.87819 | val_1_rmse: 0.90591 |  0:00:16s
epoch 57 | loss: 0.76707 | val_0_rmse: 0.87717 | val_1_rmse: 0.90491 |  0:00:17s
epoch 58 | loss: 0.76139 | val_0_rmse: 0.87698 | val_1_rmse: 0.90033 |  0:00:17s
epoch 59 | loss: 0.77159 | val_0_rmse: 0.87841 | val_1_rmse: 0.90383 |  0:00:17s
epoch 60 | loss: 0.76472 | val_0_rmse: 0.87733 | val_1_rmse: 0.90954 |  0:00:18s
epoch 61 | loss: 0.76343 | val_0_rmse: 0.87486 | val_1_rmse: 0.90171 |  0:00:18s
epoch 62 | loss: 0.76514 | val_0_rmse: 0.88014 | val_1_rmse: 0.9007  |  0:00:18s
epoch 63 | loss: 0.7585  | val_0_rmse: 0.87642 | val_1_rmse: 0.89765 |  0:00:19s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.89685
Best weights from best epoch are automatically used!
ended training at: 14:14:22
Feature importance:
Mean squared error is of 0.06284128416438438
Mean absolute error:0.18565171673574313
MAPE:0.210922083767166
R2 score:0.19153819946062978
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:14:23
epoch 0  | loss: 1.93621 | val_0_rmse: 1.007   | val_1_rmse: 1.01507 |  0:00:00s
epoch 1  | loss: 1.22443 | val_0_rmse: 0.98768 | val_1_rmse: 0.99039 |  0:00:00s
epoch 2  | loss: 1.02806 | val_0_rmse: 0.98676 | val_1_rmse: 0.99381 |  0:00:00s
epoch 3  | loss: 0.99851 | val_0_rmse: 0.98635 | val_1_rmse: 0.99095 |  0:00:01s
epoch 4  | loss: 0.97861 | val_0_rmse: 0.98508 | val_1_rmse: 0.99023 |  0:00:01s
epoch 5  | loss: 0.97138 | val_0_rmse: 0.98383 | val_1_rmse: 0.98996 |  0:00:01s
epoch 6  | loss: 0.9685  | val_0_rmse: 0.98318 | val_1_rmse: 0.99108 |  0:00:02s
epoch 7  | loss: 0.97037 | val_0_rmse: 0.9821  | val_1_rmse: 0.99035 |  0:00:02s
epoch 8  | loss: 0.95925 | val_0_rmse: 0.97473 | val_1_rmse: 0.98158 |  0:00:02s
epoch 9  | loss: 0.95313 | val_0_rmse: 0.96845 | val_1_rmse: 0.97146 |  0:00:02s
epoch 10 | loss: 0.92709 | val_0_rmse: 0.96031 | val_1_rmse: 0.96362 |  0:00:03s
epoch 11 | loss: 0.90576 | val_0_rmse: 0.94018 | val_1_rmse: 0.95087 |  0:00:03s
epoch 12 | loss: 0.89745 | val_0_rmse: 0.93539 | val_1_rmse: 0.94774 |  0:00:03s
epoch 13 | loss: 0.88266 | val_0_rmse: 0.93627 | val_1_rmse: 0.9439  |  0:00:04s
epoch 14 | loss: 0.86316 | val_0_rmse: 0.92063 | val_1_rmse: 0.93101 |  0:00:04s
epoch 15 | loss: 0.84807 | val_0_rmse: 0.90939 | val_1_rmse: 0.93048 |  0:00:04s
epoch 16 | loss: 0.84485 | val_0_rmse: 0.90265 | val_1_rmse: 0.92291 |  0:00:05s
epoch 17 | loss: 0.83298 | val_0_rmse: 0.89062 | val_1_rmse: 0.90528 |  0:00:05s
epoch 18 | loss: 0.82019 | val_0_rmse: 0.88275 | val_1_rmse: 0.89958 |  0:00:05s
epoch 19 | loss: 0.80365 | val_0_rmse: 0.89347 | val_1_rmse: 0.90012 |  0:00:05s
epoch 20 | loss: 0.8035  | val_0_rmse: 0.88465 | val_1_rmse: 0.89477 |  0:00:06s
epoch 21 | loss: 0.80489 | val_0_rmse: 0.88075 | val_1_rmse: 0.88938 |  0:00:06s
epoch 22 | loss: 0.79652 | val_0_rmse: 0.88047 | val_1_rmse: 0.88603 |  0:00:06s
epoch 23 | loss: 0.78312 | val_0_rmse: 0.87756 | val_1_rmse: 0.88452 |  0:00:07s
epoch 24 | loss: 0.7832  | val_0_rmse: 0.87853 | val_1_rmse: 0.88535 |  0:00:07s
epoch 25 | loss: 0.77715 | val_0_rmse: 0.8736  | val_1_rmse: 0.88635 |  0:00:07s
epoch 26 | loss: 0.75481 | val_0_rmse: 0.87479 | val_1_rmse: 0.89887 |  0:00:08s
epoch 27 | loss: 0.74959 | val_0_rmse: 0.86618 | val_1_rmse: 0.88476 |  0:00:08s
epoch 28 | loss: 0.73812 | val_0_rmse: 0.86553 | val_1_rmse: 0.87578 |  0:00:08s
epoch 29 | loss: 0.74228 | val_0_rmse: 0.86338 | val_1_rmse: 0.87465 |  0:00:08s
epoch 30 | loss: 0.7364  | val_0_rmse: 0.85864 | val_1_rmse: 0.87636 |  0:00:09s
epoch 31 | loss: 0.72776 | val_0_rmse: 0.8593  | val_1_rmse: 0.8805  |  0:00:09s
epoch 32 | loss: 0.72605 | val_0_rmse: 0.85793 | val_1_rmse: 0.87393 |  0:00:09s
epoch 33 | loss: 0.71901 | val_0_rmse: 0.85929 | val_1_rmse: 0.86933 |  0:00:10s
epoch 34 | loss: 0.70557 | val_0_rmse: 0.85748 | val_1_rmse: 0.8732  |  0:00:10s
epoch 35 | loss: 0.70745 | val_0_rmse: 0.85224 | val_1_rmse: 0.86805 |  0:00:10s
epoch 36 | loss: 0.70761 | val_0_rmse: 0.85161 | val_1_rmse: 0.86537 |  0:00:11s
epoch 37 | loss: 0.6958  | val_0_rmse: 0.8558  | val_1_rmse: 0.87115 |  0:00:11s
epoch 38 | loss: 0.7013  | val_0_rmse: 0.85437 | val_1_rmse: 0.86784 |  0:00:11s
epoch 39 | loss: 0.69289 | val_0_rmse: 0.85174 | val_1_rmse: 0.86929 |  0:00:11s
epoch 40 | loss: 0.69755 | val_0_rmse: 0.85628 | val_1_rmse: 0.87806 |  0:00:12s
epoch 41 | loss: 0.70283 | val_0_rmse: 0.85835 | val_1_rmse: 0.87604 |  0:00:12s
epoch 42 | loss: 0.69798 | val_0_rmse: 0.85608 | val_1_rmse: 0.87622 |  0:00:12s
epoch 43 | loss: 0.69894 | val_0_rmse: 0.84646 | val_1_rmse: 0.86978 |  0:00:13s
epoch 44 | loss: 0.69602 | val_0_rmse: 0.84685 | val_1_rmse: 0.86031 |  0:00:13s
epoch 45 | loss: 0.68641 | val_0_rmse: 0.84465 | val_1_rmse: 0.86329 |  0:00:13s
epoch 46 | loss: 0.68514 | val_0_rmse: 0.84824 | val_1_rmse: 0.86617 |  0:00:14s
epoch 47 | loss: 0.70534 | val_0_rmse: 0.8456  | val_1_rmse: 0.86482 |  0:00:14s
epoch 48 | loss: 0.68198 | val_0_rmse: 0.84852 | val_1_rmse: 0.86475 |  0:00:14s
epoch 49 | loss: 0.67956 | val_0_rmse: 0.84786 | val_1_rmse: 0.8621  |  0:00:14s
epoch 50 | loss: 0.67628 | val_0_rmse: 0.84439 | val_1_rmse: 0.86067 |  0:00:15s
epoch 51 | loss: 0.67445 | val_0_rmse: 0.85282 | val_1_rmse: 0.8757  |  0:00:15s
epoch 52 | loss: 0.67472 | val_0_rmse: 0.84866 | val_1_rmse: 0.87235 |  0:00:16s
epoch 53 | loss: 0.67232 | val_0_rmse: 0.85266 | val_1_rmse: 0.87998 |  0:00:16s
epoch 54 | loss: 0.67131 | val_0_rmse: 0.85091 | val_1_rmse: 0.87907 |  0:00:16s
epoch 55 | loss: 0.66836 | val_0_rmse: 0.84839 | val_1_rmse: 0.87545 |  0:00:16s
epoch 56 | loss: 0.67365 | val_0_rmse: 0.84    | val_1_rmse: 0.8578  |  0:00:17s
epoch 57 | loss: 0.67427 | val_0_rmse: 0.8379  | val_1_rmse: 0.85499 |  0:00:17s
epoch 58 | loss: 0.683   | val_0_rmse: 0.83278 | val_1_rmse: 0.84714 |  0:00:17s
epoch 59 | loss: 0.65635 | val_0_rmse: 0.83476 | val_1_rmse: 0.84904 |  0:00:18s
epoch 60 | loss: 0.65535 | val_0_rmse: 0.83678 | val_1_rmse: 0.84988 |  0:00:18s
epoch 61 | loss: 0.66482 | val_0_rmse: 0.84255 | val_1_rmse: 0.86207 |  0:00:18s
epoch 62 | loss: 0.6686  | val_0_rmse: 0.83657 | val_1_rmse: 0.85228 |  0:00:18s
epoch 63 | loss: 0.66902 | val_0_rmse: 0.83597 | val_1_rmse: 0.84866 |  0:00:19s
epoch 64 | loss: 0.66724 | val_0_rmse: 0.85725 | val_1_rmse: 0.87824 |  0:00:19s
epoch 65 | loss: 0.67629 | val_0_rmse: 0.83343 | val_1_rmse: 0.8577  |  0:00:19s
epoch 66 | loss: 0.66477 | val_0_rmse: 0.8398  | val_1_rmse: 0.86409 |  0:00:20s
epoch 67 | loss: 0.67028 | val_0_rmse: 0.8224  | val_1_rmse: 0.85581 |  0:00:20s
epoch 68 | loss: 0.66411 | val_0_rmse: 0.83311 | val_1_rmse: 0.86267 |  0:00:20s
epoch 69 | loss: 0.66011 | val_0_rmse: 0.83005 | val_1_rmse: 0.85666 |  0:00:21s
epoch 70 | loss: 0.6435  | val_0_rmse: 0.82574 | val_1_rmse: 0.85365 |  0:00:21s
epoch 71 | loss: 0.65907 | val_0_rmse: 0.82318 | val_1_rmse: 0.85488 |  0:00:21s
epoch 72 | loss: 0.64798 | val_0_rmse: 0.82219 | val_1_rmse: 0.85575 |  0:00:21s
epoch 73 | loss: 0.64821 | val_0_rmse: 0.81657 | val_1_rmse: 0.84788 |  0:00:22s
epoch 74 | loss: 0.63547 | val_0_rmse: 0.81379 | val_1_rmse: 0.84786 |  0:00:22s
epoch 75 | loss: 0.64943 | val_0_rmse: 0.81426 | val_1_rmse: 0.85091 |  0:00:22s
epoch 76 | loss: 0.64637 | val_0_rmse: 0.81271 | val_1_rmse: 0.84654 |  0:00:23s
epoch 77 | loss: 0.64759 | val_0_rmse: 0.81867 | val_1_rmse: 0.85094 |  0:00:23s
epoch 78 | loss: 0.64671 | val_0_rmse: 0.82115 | val_1_rmse: 0.85504 |  0:00:23s
epoch 79 | loss: 0.65575 | val_0_rmse: 0.81827 | val_1_rmse: 0.85737 |  0:00:23s
epoch 80 | loss: 0.66168 | val_0_rmse: 0.82611 | val_1_rmse: 0.86845 |  0:00:24s
epoch 81 | loss: 0.66835 | val_0_rmse: 0.84007 | val_1_rmse: 0.87538 |  0:00:24s
epoch 82 | loss: 0.67693 | val_0_rmse: 0.84228 | val_1_rmse: 0.87061 |  0:00:24s
epoch 83 | loss: 0.68473 | val_0_rmse: 0.82256 | val_1_rmse: 0.85057 |  0:00:25s
epoch 84 | loss: 0.66532 | val_0_rmse: 0.81785 | val_1_rmse: 0.84888 |  0:00:25s
epoch 85 | loss: 0.65592 | val_0_rmse: 0.81295 | val_1_rmse: 0.84776 |  0:00:25s
epoch 86 | loss: 0.6539  | val_0_rmse: 0.80898 | val_1_rmse: 0.84472 |  0:00:26s
epoch 87 | loss: 0.64861 | val_0_rmse: 0.80763 | val_1_rmse: 0.84728 |  0:00:26s
epoch 88 | loss: 0.63756 | val_0_rmse: 0.80059 | val_1_rmse: 0.85257 |  0:00:26s
epoch 89 | loss: 0.64679 | val_0_rmse: 0.80096 | val_1_rmse: 0.84671 |  0:00:26s
epoch 90 | loss: 0.63372 | val_0_rmse: 0.80302 | val_1_rmse: 0.84229 |  0:00:27s
epoch 91 | loss: 0.63799 | val_0_rmse: 0.80233 | val_1_rmse: 0.84344 |  0:00:27s
epoch 92 | loss: 0.6407  | val_0_rmse: 0.80236 | val_1_rmse: 0.84278 |  0:00:27s
epoch 93 | loss: 0.63645 | val_0_rmse: 0.79964 | val_1_rmse: 0.85112 |  0:00:28s
epoch 94 | loss: 0.6364  | val_0_rmse: 0.79409 | val_1_rmse: 0.84626 |  0:00:28s
epoch 95 | loss: 0.63476 | val_0_rmse: 0.80059 | val_1_rmse: 0.85081 |  0:00:28s
epoch 96 | loss: 0.63425 | val_0_rmse: 0.79492 | val_1_rmse: 0.85023 |  0:00:29s
epoch 97 | loss: 0.6284  | val_0_rmse: 0.79005 | val_1_rmse: 0.84295 |  0:00:29s
epoch 98 | loss: 0.61782 | val_0_rmse: 0.78878 | val_1_rmse: 0.84055 |  0:00:29s
epoch 99 | loss: 0.62955 | val_0_rmse: 0.78986 | val_1_rmse: 0.8445  |  0:00:29s
epoch 100| loss: 0.61864 | val_0_rmse: 0.79217 | val_1_rmse: 0.85461 |  0:00:30s
epoch 101| loss: 0.63264 | val_0_rmse: 0.7884  | val_1_rmse: 0.84989 |  0:00:30s
epoch 102| loss: 0.63042 | val_0_rmse: 0.78471 | val_1_rmse: 0.84775 |  0:00:30s
epoch 103| loss: 0.62254 | val_0_rmse: 0.78845 | val_1_rmse: 0.85421 |  0:00:31s
epoch 104| loss: 0.61732 | val_0_rmse: 0.79392 | val_1_rmse: 0.85342 |  0:00:31s
epoch 105| loss: 0.628   | val_0_rmse: 0.79937 | val_1_rmse: 0.84444 |  0:00:31s
epoch 106| loss: 0.6419  | val_0_rmse: 0.79775 | val_1_rmse: 0.84861 |  0:00:31s
epoch 107| loss: 0.63884 | val_0_rmse: 0.79386 | val_1_rmse: 0.84853 |  0:00:32s
epoch 108| loss: 0.6341  | val_0_rmse: 0.81604 | val_1_rmse: 0.87237 |  0:00:32s
epoch 109| loss: 0.66184 | val_0_rmse: 0.82676 | val_1_rmse: 0.88001 |  0:00:32s
epoch 110| loss: 0.65178 | val_0_rmse: 0.81609 | val_1_rmse: 0.86139 |  0:00:33s
epoch 111| loss: 0.64067 | val_0_rmse: 0.80456 | val_1_rmse: 0.85113 |  0:00:33s
epoch 112| loss: 0.64193 | val_0_rmse: 0.79831 | val_1_rmse: 0.85215 |  0:00:33s
epoch 113| loss: 0.63047 | val_0_rmse: 0.79506 | val_1_rmse: 0.85473 |  0:00:34s
epoch 114| loss: 0.62265 | val_0_rmse: 0.79483 | val_1_rmse: 0.85544 |  0:00:34s
epoch 115| loss: 0.63028 | val_0_rmse: 0.79398 | val_1_rmse: 0.84804 |  0:00:34s
epoch 116| loss: 0.62902 | val_0_rmse: 0.78955 | val_1_rmse: 0.83896 |  0:00:34s
epoch 117| loss: 0.63983 | val_0_rmse: 0.79686 | val_1_rmse: 0.84787 |  0:00:35s
epoch 118| loss: 0.63725 | val_0_rmse: 0.79179 | val_1_rmse: 0.85412 |  0:00:35s
epoch 119| loss: 0.6622  | val_0_rmse: 0.80814 | val_1_rmse: 0.87709 |  0:00:35s
epoch 120| loss: 0.6503  | val_0_rmse: 0.82441 | val_1_rmse: 0.88833 |  0:00:36s
epoch 121| loss: 0.65375 | val_0_rmse: 0.81173 | val_1_rmse: 0.87677 |  0:00:36s
epoch 122| loss: 0.65425 | val_0_rmse: 0.80621 | val_1_rmse: 0.87854 |  0:00:36s
epoch 123| loss: 0.64553 | val_0_rmse: 0.8017  | val_1_rmse: 0.86863 |  0:00:37s
epoch 124| loss: 0.63159 | val_0_rmse: 0.7903  | val_1_rmse: 0.85968 |  0:00:37s
epoch 125| loss: 0.63397 | val_0_rmse: 0.78724 | val_1_rmse: 0.87181 |  0:00:37s
epoch 126| loss: 0.63017 | val_0_rmse: 0.78125 | val_1_rmse: 0.86157 |  0:00:38s
epoch 127| loss: 0.63704 | val_0_rmse: 0.77726 | val_1_rmse: 0.84215 |  0:00:38s
epoch 128| loss: 0.61562 | val_0_rmse: 0.77451 | val_1_rmse: 0.84681 |  0:00:38s
epoch 129| loss: 0.61555 | val_0_rmse: 0.7719  | val_1_rmse: 0.85376 |  0:00:38s
epoch 130| loss: 0.61538 | val_0_rmse: 0.77673 | val_1_rmse: 0.85504 |  0:00:39s
epoch 131| loss: 0.61711 | val_0_rmse: 0.7833  | val_1_rmse: 0.86571 |  0:00:39s
epoch 132| loss: 0.61047 | val_0_rmse: 0.78808 | val_1_rmse: 0.87402 |  0:00:39s
epoch 133| loss: 0.6159  | val_0_rmse: 0.77582 | val_1_rmse: 0.86002 |  0:00:40s
epoch 134| loss: 0.61192 | val_0_rmse: 0.77232 | val_1_rmse: 0.85699 |  0:00:40s
epoch 135| loss: 0.61049 | val_0_rmse: 0.76415 | val_1_rmse: 0.86292 |  0:00:40s
epoch 136| loss: 0.60681 | val_0_rmse: 0.76274 | val_1_rmse: 0.87222 |  0:00:40s
epoch 137| loss: 0.5985  | val_0_rmse: 0.75984 | val_1_rmse: 0.86337 |  0:00:41s
epoch 138| loss: 0.59575 | val_0_rmse: 0.76391 | val_1_rmse: 0.85324 |  0:00:41s
epoch 139| loss: 0.60235 | val_0_rmse: 0.7622  | val_1_rmse: 0.85773 |  0:00:41s
epoch 140| loss: 0.59524 | val_0_rmse: 0.75737 | val_1_rmse: 0.86488 |  0:00:42s
epoch 141| loss: 0.59469 | val_0_rmse: 0.75434 | val_1_rmse: 0.85753 |  0:00:42s
epoch 142| loss: 0.57781 | val_0_rmse: 0.74796 | val_1_rmse: 0.86185 |  0:00:42s
epoch 143| loss: 0.58171 | val_0_rmse: 0.74769 | val_1_rmse: 0.86081 |  0:00:43s
epoch 144| loss: 0.57944 | val_0_rmse: 0.74727 | val_1_rmse: 0.85113 |  0:00:43s
epoch 145| loss: 0.57925 | val_0_rmse: 0.74386 | val_1_rmse: 0.84756 |  0:00:43s
epoch 146| loss: 0.57486 | val_0_rmse: 0.74583 | val_1_rmse: 0.85443 |  0:00:43s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.83896
Best weights from best epoch are automatically used!
ended training at: 14:15:07
Feature importance:
Mean squared error is of 0.08218492552134071
Mean absolute error:0.20277435542809993
MAPE:0.21970854907986861
R2 score:0.1703380505594415
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:15:07
epoch 0  | loss: 1.92584 | val_0_rmse: 1.02271 | val_1_rmse: 1.01316 |  0:00:00s
epoch 1  | loss: 1.16992 | val_0_rmse: 1.00073 | val_1_rmse: 0.98691 |  0:00:00s
epoch 2  | loss: 1.05313 | val_0_rmse: 0.99819 | val_1_rmse: 0.98476 |  0:00:00s
epoch 3  | loss: 1.01072 | val_0_rmse: 0.9949  | val_1_rmse: 0.98134 |  0:00:01s
epoch 4  | loss: 0.99655 | val_0_rmse: 0.98847 | val_1_rmse: 0.97552 |  0:00:01s
epoch 5  | loss: 0.93836 | val_0_rmse: 0.97333 | val_1_rmse: 0.95773 |  0:00:01s
epoch 6  | loss: 0.87215 | val_0_rmse: 0.91116 | val_1_rmse: 0.88807 |  0:00:02s
epoch 7  | loss: 0.81736 | val_0_rmse: 0.89779 | val_1_rmse: 0.88153 |  0:00:02s
epoch 8  | loss: 0.80606 | val_0_rmse: 0.88658 | val_1_rmse: 0.87187 |  0:00:02s
epoch 9  | loss: 0.78095 | val_0_rmse: 0.88144 | val_1_rmse: 0.86524 |  0:00:03s
epoch 10 | loss: 0.7818  | val_0_rmse: 0.87953 | val_1_rmse: 0.86251 |  0:00:03s
epoch 11 | loss: 0.76818 | val_0_rmse: 0.87223 | val_1_rmse: 0.853   |  0:00:03s
epoch 12 | loss: 0.76476 | val_0_rmse: 0.87132 | val_1_rmse: 0.85464 |  0:00:03s
epoch 13 | loss: 0.76097 | val_0_rmse: 0.87187 | val_1_rmse: 0.86215 |  0:00:04s
epoch 14 | loss: 0.76324 | val_0_rmse: 0.86865 | val_1_rmse: 0.85494 |  0:00:04s
epoch 15 | loss: 0.75979 | val_0_rmse: 0.86903 | val_1_rmse: 0.85605 |  0:00:04s
epoch 16 | loss: 0.74773 | val_0_rmse: 0.87091 | val_1_rmse: 0.85774 |  0:00:05s
epoch 17 | loss: 0.75424 | val_0_rmse: 0.87222 | val_1_rmse: 0.8628  |  0:00:05s
epoch 18 | loss: 0.74883 | val_0_rmse: 0.86799 | val_1_rmse: 0.85141 |  0:00:05s
epoch 19 | loss: 0.75282 | val_0_rmse: 0.87743 | val_1_rmse: 0.86502 |  0:00:05s
epoch 20 | loss: 0.75741 | val_0_rmse: 0.87558 | val_1_rmse: 0.86101 |  0:00:06s
epoch 21 | loss: 0.74934 | val_0_rmse: 0.87044 | val_1_rmse: 0.85795 |  0:00:06s
epoch 22 | loss: 0.74656 | val_0_rmse: 0.87808 | val_1_rmse: 0.86265 |  0:00:06s
epoch 23 | loss: 0.75094 | val_0_rmse: 0.87272 | val_1_rmse: 0.85632 |  0:00:07s
epoch 24 | loss: 0.73659 | val_0_rmse: 0.86883 | val_1_rmse: 0.85881 |  0:00:07s
epoch 25 | loss: 0.74216 | val_0_rmse: 0.87061 | val_1_rmse: 0.85983 |  0:00:07s
epoch 26 | loss: 0.74302 | val_0_rmse: 0.86219 | val_1_rmse: 0.85435 |  0:00:08s
epoch 27 | loss: 0.73114 | val_0_rmse: 0.86336 | val_1_rmse: 0.85496 |  0:00:08s
epoch 28 | loss: 0.72996 | val_0_rmse: 0.85949 | val_1_rmse: 0.85289 |  0:00:08s
epoch 29 | loss: 0.72641 | val_0_rmse: 0.85689 | val_1_rmse: 0.85328 |  0:00:09s
epoch 30 | loss: 0.73194 | val_0_rmse: 0.85455 | val_1_rmse: 0.8457  |  0:00:09s
epoch 31 | loss: 0.73458 | val_0_rmse: 0.85742 | val_1_rmse: 0.85029 |  0:00:09s
epoch 32 | loss: 0.72906 | val_0_rmse: 0.85772 | val_1_rmse: 0.84888 |  0:00:09s
epoch 33 | loss: 0.74128 | val_0_rmse: 0.85905 | val_1_rmse: 0.85025 |  0:00:10s
epoch 34 | loss: 0.73185 | val_0_rmse: 0.87253 | val_1_rmse: 0.85666 |  0:00:10s
epoch 35 | loss: 0.73147 | val_0_rmse: 0.87471 | val_1_rmse: 0.85926 |  0:00:10s
epoch 36 | loss: 0.72338 | val_0_rmse: 0.86164 | val_1_rmse: 0.8539  |  0:00:11s
epoch 37 | loss: 0.72774 | val_0_rmse: 0.85886 | val_1_rmse: 0.8504  |  0:00:11s
epoch 38 | loss: 0.72866 | val_0_rmse: 0.85857 | val_1_rmse: 0.85056 |  0:00:11s
epoch 39 | loss: 0.73585 | val_0_rmse: 0.85803 | val_1_rmse: 0.85113 |  0:00:12s
epoch 40 | loss: 0.72597 | val_0_rmse: 0.86512 | val_1_rmse: 0.85672 |  0:00:12s
epoch 41 | loss: 0.72939 | val_0_rmse: 0.8623  | val_1_rmse: 0.8527  |  0:00:12s
epoch 42 | loss: 0.72847 | val_0_rmse: 0.8554  | val_1_rmse: 0.85277 |  0:00:12s
epoch 43 | loss: 0.72286 | val_0_rmse: 0.86272 | val_1_rmse: 0.85869 |  0:00:13s
epoch 44 | loss: 0.74452 | val_0_rmse: 0.86688 | val_1_rmse: 0.85491 |  0:00:13s
epoch 45 | loss: 0.74675 | val_0_rmse: 0.86399 | val_1_rmse: 0.85706 |  0:00:13s
epoch 46 | loss: 0.741   | val_0_rmse: 0.85976 | val_1_rmse: 0.8546  |  0:00:14s
epoch 47 | loss: 0.73245 | val_0_rmse: 0.8676  | val_1_rmse: 0.86148 |  0:00:14s
epoch 48 | loss: 0.72699 | val_0_rmse: 0.85393 | val_1_rmse: 0.85141 |  0:00:14s
epoch 49 | loss: 0.72375 | val_0_rmse: 0.85387 | val_1_rmse: 0.8529  |  0:00:14s
epoch 50 | loss: 0.72347 | val_0_rmse: 0.85292 | val_1_rmse: 0.85227 |  0:00:15s
epoch 51 | loss: 0.70995 | val_0_rmse: 0.85116 | val_1_rmse: 0.85166 |  0:00:15s
epoch 52 | loss: 0.71346 | val_0_rmse: 0.84995 | val_1_rmse: 0.85142 |  0:00:15s
epoch 53 | loss: 0.71117 | val_0_rmse: 0.85098 | val_1_rmse: 0.85158 |  0:00:16s
epoch 54 | loss: 0.70847 | val_0_rmse: 0.85458 | val_1_rmse: 0.85545 |  0:00:16s
epoch 55 | loss: 0.71034 | val_0_rmse: 0.85137 | val_1_rmse: 0.85142 |  0:00:16s
epoch 56 | loss: 0.70907 | val_0_rmse: 0.84543 | val_1_rmse: 0.85244 |  0:00:17s
epoch 57 | loss: 0.70612 | val_0_rmse: 0.84523 | val_1_rmse: 0.84946 |  0:00:17s
epoch 58 | loss: 0.70127 | val_0_rmse: 0.84041 | val_1_rmse: 0.84948 |  0:00:17s
epoch 59 | loss: 0.71175 | val_0_rmse: 0.84126 | val_1_rmse: 0.85217 |  0:00:17s
epoch 60 | loss: 0.70471 | val_0_rmse: 0.84738 | val_1_rmse: 0.85419 |  0:00:18s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.8457
Best weights from best epoch are automatically used!
ended training at: 14:15:26
Feature importance:
Mean squared error is of 0.06939939218543013
Mean absolute error:0.19752357871001827
MAPE:0.224300726923627
R2 score:0.18329012075840623
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:19:54
epoch 0  | loss: 1.03715 | val_0_rmse: 0.93881 | val_1_rmse: 0.93868 |  0:00:22s
epoch 1  | loss: 0.82289 | val_0_rmse: 0.88587 | val_1_rmse: 0.88748 |  0:00:44s
epoch 2  | loss: 0.76454 | val_0_rmse: 0.89021 | val_1_rmse: 0.89012 |  0:01:06s
epoch 3  | loss: 0.7347  | val_0_rmse: 0.90122 | val_1_rmse: 0.90371 |  0:01:27s
epoch 4  | loss: 0.75621 | val_0_rmse: 0.8689  | val_1_rmse: 0.87275 |  0:01:49s
epoch 5  | loss: 0.73674 | val_0_rmse: 0.85054 | val_1_rmse: 0.85705 |  0:02:11s
epoch 6  | loss: 0.72284 | val_0_rmse: 0.85951 | val_1_rmse: 0.86893 |  0:02:33s
epoch 7  | loss: 0.6964  | val_0_rmse: 0.82296 | val_1_rmse: 0.84322 |  0:02:55s
epoch 8  | loss: 0.69492 | val_0_rmse: 0.82596 | val_1_rmse: 0.8499  |  0:03:17s
epoch 9  | loss: 0.68379 | val_0_rmse: 0.85106 | val_1_rmse: 0.87913 |  0:03:38s
epoch 10 | loss: 0.66131 | val_0_rmse: 0.80565 | val_1_rmse: 0.83689 |  0:04:00s
epoch 11 | loss: 0.64091 | val_0_rmse: 0.80528 | val_1_rmse: 0.83359 |  0:04:22s
epoch 12 | loss: 0.63497 | val_0_rmse: 0.80535 | val_1_rmse: 0.83014 |  0:04:44s
epoch 13 | loss: 0.62596 | val_0_rmse: 0.82513 | val_1_rmse: 0.86559 |  0:05:05s
epoch 14 | loss: 0.61974 | val_0_rmse: 0.82757 | val_1_rmse: 0.88466 |  0:05:27s
epoch 15 | loss: 0.60923 | val_0_rmse: 0.83387 | val_1_rmse: 0.89886 |  0:05:49s
epoch 16 | loss: 0.60923 | val_0_rmse: 0.85228 | val_1_rmse: 0.91162 |  0:06:11s
epoch 17 | loss: 0.60576 | val_0_rmse: 0.79465 | val_1_rmse: 0.82907 |  0:06:33s
epoch 18 | loss: 0.6005  | val_0_rmse: 0.78316 | val_1_rmse: 0.83342 |  0:06:54s
epoch 19 | loss: 0.59328 | val_0_rmse: 0.78034 | val_1_rmse: 0.83493 |  0:07:16s
epoch 20 | loss: 0.58546 | val_0_rmse: 0.79124 | val_1_rmse: 0.84601 |  0:07:38s
epoch 21 | loss: 0.58286 | val_0_rmse: 0.7792  | val_1_rmse: 0.83754 |  0:08:00s
epoch 22 | loss: 0.58082 | val_0_rmse: 0.80038 | val_1_rmse: 0.82591 |  0:08:22s
epoch 23 | loss: 0.57634 | val_0_rmse: 0.77038 | val_1_rmse: 0.82668 |  0:08:44s
epoch 24 | loss: 0.57336 | val_0_rmse: 0.79226 | val_1_rmse: 0.83747 |  0:09:06s
epoch 25 | loss: 0.57131 | val_0_rmse: 0.77343 | val_1_rmse: 0.83351 |  0:09:28s
epoch 26 | loss: 0.56646 | val_0_rmse: 0.77628 | val_1_rmse: 0.83673 |  0:09:50s
epoch 27 | loss: 0.56737 | val_0_rmse: 0.76157 | val_1_rmse: 0.82062 |  0:10:12s
epoch 28 | loss: 0.56419 | val_0_rmse: 0.78072 | val_1_rmse: 0.82005 |  0:10:34s
epoch 29 | loss: 0.55859 | val_0_rmse: 0.77317 | val_1_rmse: 0.86619 |  0:10:56s
epoch 30 | loss: 0.56468 | val_0_rmse: 0.77918 | val_1_rmse: 0.84937 |  0:11:18s
epoch 31 | loss: 0.56288 | val_0_rmse: 0.78255 | val_1_rmse: 0.86346 |  0:11:39s
epoch 32 | loss: 0.55397 | val_0_rmse: 0.7874  | val_1_rmse: 0.86236 |  0:12:01s
epoch 33 | loss: 0.55218 | val_0_rmse: 0.77584 | val_1_rmse: 0.86757 |  0:12:23s
epoch 34 | loss: 0.54623 | val_0_rmse: 0.75858 | val_1_rmse: 0.83415 |  0:12:44s
epoch 35 | loss: 0.54582 | val_0_rmse: 0.77939 | val_1_rmse: 0.84266 |  0:13:06s
epoch 36 | loss: 0.54048 | val_0_rmse: 0.784   | val_1_rmse: 0.85724 |  0:13:28s
epoch 37 | loss: 0.54948 | val_0_rmse: 0.81326 | val_1_rmse: 0.91854 |  0:13:49s
epoch 38 | loss: 0.54103 | val_0_rmse: 0.84232 | val_1_rmse: 0.89712 |  0:14:11s
epoch 39 | loss: 0.54055 | val_0_rmse: 0.77312 | val_1_rmse: 0.85393 |  0:14:33s
epoch 40 | loss: 0.54017 | val_0_rmse: 0.78368 | val_1_rmse: 0.88008 |  0:14:55s
epoch 41 | loss: 0.53824 | val_0_rmse: 0.80075 | val_1_rmse: 0.91751 |  0:15:17s
epoch 42 | loss: 0.53626 | val_0_rmse: 0.8932  | val_1_rmse: 1.0653  |  0:15:38s
epoch 43 | loss: 0.53508 | val_0_rmse: 0.78834 | val_1_rmse: 0.88112 |  0:16:00s
epoch 44 | loss: 0.53458 | val_0_rmse: 0.7761  | val_1_rmse: 0.86008 |  0:16:22s
epoch 45 | loss: 0.53285 | val_0_rmse: 0.75079 | val_1_rmse: 0.82464 |  0:16:44s
epoch 46 | loss: 0.5315  | val_0_rmse: 0.77125 | val_1_rmse: 0.83803 |  0:17:06s
epoch 47 | loss: 0.5297  | val_0_rmse: 0.76055 | val_1_rmse: 0.84687 |  0:17:28s
epoch 48 | loss: 0.52888 | val_0_rmse: 0.75902 | val_1_rmse: 0.84702 |  0:17:50s
epoch 49 | loss: 0.52976 | val_0_rmse: 0.76035 | val_1_rmse: 0.84159 |  0:18:12s
epoch 50 | loss: 0.52916 | val_0_rmse: 0.9839  | val_1_rmse: 1.03084 |  0:18:33s
epoch 51 | loss: 0.52796 | val_0_rmse: 0.76569 | val_1_rmse: 0.83573 |  0:18:55s
epoch 52 | loss: 0.52785 | val_0_rmse: 0.79518 | val_1_rmse: 0.87687 |  0:19:17s
epoch 53 | loss: 0.52637 | val_0_rmse: 0.75976 | val_1_rmse: 0.84339 |  0:19:39s
epoch 54 | loss: 0.52649 | val_0_rmse: 0.77936 | val_1_rmse: 0.87653 |  0:20:01s
epoch 55 | loss: 0.52297 | val_0_rmse: 0.76626 | val_1_rmse: 0.85508 |  0:20:23s
epoch 56 | loss: 0.52398 | val_0_rmse: 0.78712 | val_1_rmse: 0.88945 |  0:20:45s
epoch 57 | loss: 0.52265 | val_0_rmse: 0.7984  | val_1_rmse: 0.89418 |  0:21:07s
epoch 58 | loss: 0.52045 | val_0_rmse: 0.78723 | val_1_rmse: 0.87827 |  0:21:28s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.82005
Best weights from best epoch are automatically used!
ended training at: 14:41:34
Feature importance:
Mean squared error is of 0.3051185551577621
Mean absolute error:0.20209967218608615
MAPE:0.22035546616856944
R2 score:-1.4118018125596397
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:41:40
epoch 0  | loss: 1.00625 | val_0_rmse: 0.90237 | val_1_rmse: 0.90757 |  0:00:21s
epoch 1  | loss: 0.75916 | val_0_rmse: 0.85437 | val_1_rmse: 0.86081 |  0:00:43s
epoch 2  | loss: 0.71353 | val_0_rmse: 0.83995 | val_1_rmse: 0.84663 |  0:01:05s
epoch 3  | loss: 0.67771 | val_0_rmse: 0.82256 | val_1_rmse: 0.83366 |  0:01:26s
epoch 4  | loss: 0.66263 | val_0_rmse: 0.81255 | val_1_rmse: 0.82483 |  0:01:48s
epoch 5  | loss: 0.65243 | val_0_rmse: 0.78835 | val_1_rmse: 0.80735 |  0:02:10s
epoch 6  | loss: 0.64422 | val_0_rmse: 0.80648 | val_1_rmse: 0.81892 |  0:02:32s
epoch 7  | loss: 0.6323  | val_0_rmse: 0.79437 | val_1_rmse: 0.8142  |  0:02:54s
epoch 8  | loss: 0.623   | val_0_rmse: 0.85544 | val_1_rmse: 0.88216 |  0:03:15s
epoch 9  | loss: 0.61826 | val_0_rmse: 0.78915 | val_1_rmse: 0.82416 |  0:03:37s
epoch 10 | loss: 0.60745 | val_0_rmse: 0.79361 | val_1_rmse: 0.82139 |  0:03:58s
epoch 11 | loss: 0.60198 | val_0_rmse: 0.77531 | val_1_rmse: 0.808   |  0:04:20s
epoch 12 | loss: 0.59305 | val_0_rmse: 0.7759  | val_1_rmse: 0.81287 |  0:04:42s
epoch 13 | loss: 0.5929  | val_0_rmse: 0.81978 | val_1_rmse: 0.84533 |  0:05:03s
epoch 14 | loss: 0.58454 | val_0_rmse: 0.77795 | val_1_rmse: 0.82417 |  0:05:25s
epoch 15 | loss: 0.58424 | val_0_rmse: 0.76122 | val_1_rmse: 0.81007 |  0:05:47s
epoch 16 | loss: 0.5766  | val_0_rmse: 0.83953 | val_1_rmse: 0.83946 |  0:06:08s
epoch 17 | loss: 0.57142 | val_0_rmse: 0.75376 | val_1_rmse: 0.7995  |  0:06:30s
epoch 18 | loss: 0.56975 | val_0_rmse: 0.76565 | val_1_rmse: 0.80982 |  0:06:51s
epoch 19 | loss: 0.57173 | val_0_rmse: 0.75883 | val_1_rmse: 0.80996 |  0:07:13s
epoch 20 | loss: 0.56301 | val_0_rmse: 0.751   | val_1_rmse: 0.80476 |  0:07:34s
epoch 21 | loss: 0.55577 | val_0_rmse: 0.75681 | val_1_rmse: 0.82516 |  0:07:56s
epoch 22 | loss: 0.55801 | val_0_rmse: 0.75376 | val_1_rmse: 0.81758 |  0:08:18s
epoch 23 | loss: 0.56047 | val_0_rmse: 0.74815 | val_1_rmse: 0.80609 |  0:08:39s
epoch 24 | loss: 0.55094 | val_0_rmse: 0.75916 | val_1_rmse: 0.84292 |  0:09:01s
epoch 25 | loss: 0.54991 | val_0_rmse: 0.74188 | val_1_rmse: 0.80605 |  0:09:23s
epoch 26 | loss: 0.54467 | val_0_rmse: 0.74459 | val_1_rmse: 0.80529 |  0:09:44s
epoch 27 | loss: 0.53979 | val_0_rmse: 0.7371  | val_1_rmse: 0.80576 |  0:10:06s
epoch 28 | loss: 0.53804 | val_0_rmse: 0.73923 | val_1_rmse: 0.80201 |  0:10:28s
epoch 29 | loss: 0.53599 | val_0_rmse: 0.73943 | val_1_rmse: 0.80802 |  0:10:49s
epoch 30 | loss: 0.53222 | val_0_rmse: 0.74377 | val_1_rmse: 0.81074 |  0:11:11s
epoch 31 | loss: 0.53106 | val_0_rmse: 0.7365  | val_1_rmse: 0.80925 |  0:11:33s
epoch 32 | loss: 0.52861 | val_0_rmse: 0.73059 | val_1_rmse: 0.81488 |  0:11:55s
epoch 33 | loss: 0.52477 | val_0_rmse: 0.73489 | val_1_rmse: 0.81026 |  0:12:16s
epoch 34 | loss: 0.52484 | val_0_rmse: 0.73832 | val_1_rmse: 0.82355 |  0:12:38s
epoch 35 | loss: 0.52104 | val_0_rmse: 0.73856 | val_1_rmse: 0.79954 |  0:13:00s
epoch 36 | loss: 0.51803 | val_0_rmse: 0.74442 | val_1_rmse: 0.81737 |  0:13:22s
epoch 37 | loss: 0.52184 | val_0_rmse: 0.73012 | val_1_rmse: 0.81193 |  0:13:43s
epoch 38 | loss: 0.51772 | val_0_rmse: 0.73594 | val_1_rmse: 0.81423 |  0:14:05s
epoch 39 | loss: 0.51515 | val_0_rmse: 0.73535 | val_1_rmse: 0.82413 |  0:14:27s
epoch 40 | loss: 0.55147 | val_0_rmse: 0.76588 | val_1_rmse: 0.85245 |  0:14:49s
epoch 41 | loss: 0.54271 | val_0_rmse: 0.76931 | val_1_rmse: 0.86545 |  0:15:11s
epoch 42 | loss: 0.53254 | val_0_rmse: 0.75374 | val_1_rmse: 0.82613 |  0:15:33s
epoch 43 | loss: 0.53546 | val_0_rmse: 0.73204 | val_1_rmse: 0.81208 |  0:15:54s
epoch 44 | loss: 0.51926 | val_0_rmse: 0.72831 | val_1_rmse: 0.82006 |  0:16:16s
epoch 45 | loss: 0.51091 | val_0_rmse: 0.72307 | val_1_rmse: 0.81793 |  0:16:38s
epoch 46 | loss: 0.51028 | val_0_rmse: 0.72724 | val_1_rmse: 0.8245  |  0:17:00s
epoch 47 | loss: 0.50526 | val_0_rmse: 0.73171 | val_1_rmse: 0.8306  |  0:17:22s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.7995
Best weights from best epoch are automatically used!
ended training at: 14:59:13
Feature importance:
Mean squared error is of 0.08601001318633193
Mean absolute error:0.1945869057958393
MAPE:0.21502485127128046
R2 score:0.3402315263694927
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:59:18
epoch 0  | loss: 0.9721  | val_0_rmse: 0.876   | val_1_rmse: 0.88684 |  0:00:21s
epoch 1  | loss: 0.7476  | val_0_rmse: 0.87517 | val_1_rmse: 0.88306 |  0:00:43s
epoch 2  | loss: 0.71155 | val_0_rmse: 0.85767 | val_1_rmse: 0.86702 |  0:01:04s
epoch 3  | loss: 0.67583 | val_0_rmse: 0.8214  | val_1_rmse: 0.83248 |  0:01:26s
epoch 4  | loss: 0.66032 | val_0_rmse: 0.84606 | val_1_rmse: 0.86531 |  0:01:47s
epoch 5  | loss: 0.65543 | val_0_rmse: 0.80958 | val_1_rmse: 0.82942 |  0:02:09s
epoch 6  | loss: 0.64212 | val_0_rmse: 0.79042 | val_1_rmse: 0.81579 |  0:02:30s
epoch 7  | loss: 0.63015 | val_0_rmse: 0.84939 | val_1_rmse: 0.87283 |  0:02:52s
epoch 8  | loss: 0.61746 | val_0_rmse: 0.77998 | val_1_rmse: 0.81401 |  0:03:13s
epoch 9  | loss: 0.60796 | val_0_rmse: 0.78226 | val_1_rmse: 0.81837 |  0:03:35s
epoch 10 | loss: 0.60192 | val_0_rmse: 0.77185 | val_1_rmse: 0.81787 |  0:03:56s
epoch 11 | loss: 0.6029  | val_0_rmse: 0.77709 | val_1_rmse: 0.83711 |  0:04:18s
epoch 12 | loss: 0.59778 | val_0_rmse: 0.78351 | val_1_rmse: 0.83392 |  0:04:39s
epoch 13 | loss: 0.59195 | val_0_rmse: 0.76908 | val_1_rmse: 0.81423 |  0:05:01s
epoch 14 | loss: 0.58747 | val_0_rmse: 0.7747  | val_1_rmse: 0.81681 |  0:05:22s
epoch 15 | loss: 0.58299 | val_0_rmse: 0.76303 | val_1_rmse: 0.81985 |  0:05:44s
epoch 16 | loss: 0.58087 | val_0_rmse: 0.76486 | val_1_rmse: 0.81211 |  0:06:06s
epoch 17 | loss: 0.57344 | val_0_rmse: 0.75769 | val_1_rmse: 0.81333 |  0:06:27s
epoch 18 | loss: 0.56974 | val_0_rmse: 0.75334 | val_1_rmse: 0.81666 |  0:06:49s
epoch 19 | loss: 0.56562 | val_0_rmse: 0.77447 | val_1_rmse: 0.83048 |  0:07:10s
epoch 20 | loss: 0.56246 | val_0_rmse: 0.76488 | val_1_rmse: 0.83187 |  0:07:32s
epoch 21 | loss: 0.56138 | val_0_rmse: 0.75338 | val_1_rmse: 0.81293 |  0:07:53s
epoch 22 | loss: 0.56193 | val_0_rmse: 0.75299 | val_1_rmse: 0.82079 |  0:08:15s
epoch 23 | loss: 0.55573 | val_0_rmse: 0.75294 | val_1_rmse: 0.81495 |  0:08:36s
epoch 24 | loss: 0.55536 | val_0_rmse: 0.74835 | val_1_rmse: 0.81826 |  0:08:58s
epoch 25 | loss: 0.55347 | val_0_rmse: 0.7478  | val_1_rmse: 0.81488 |  0:09:19s
epoch 26 | loss: 0.5523  | val_0_rmse: 0.7809  | val_1_rmse: 0.82674 |  0:09:41s
epoch 27 | loss: 0.58391 | val_0_rmse: 0.76395 | val_1_rmse: 0.8257  |  0:10:02s
epoch 28 | loss: 0.56633 | val_0_rmse: 0.76242 | val_1_rmse: 0.82446 |  0:10:24s
epoch 29 | loss: 0.55032 | val_0_rmse: 0.75622 | val_1_rmse: 0.82779 |  0:10:46s
epoch 30 | loss: 0.54699 | val_0_rmse: 0.75445 | val_1_rmse: 0.83687 |  0:11:07s
epoch 31 | loss: 0.54175 | val_0_rmse: 0.74614 | val_1_rmse: 0.82923 |  0:11:29s
epoch 32 | loss: 0.53657 | val_0_rmse: 0.74402 | val_1_rmse: 0.83241 |  0:11:50s
epoch 33 | loss: 0.53777 | val_0_rmse: 0.73699 | val_1_rmse: 0.82501 |  0:12:12s
epoch 34 | loss: 0.53281 | val_0_rmse: 0.73622 | val_1_rmse: 0.81479 |  0:12:33s
epoch 35 | loss: 0.52979 | val_0_rmse: 0.73417 | val_1_rmse: 0.82047 |  0:12:55s
epoch 36 | loss: 0.52839 | val_0_rmse: 0.73581 | val_1_rmse: 0.81479 |  0:13:17s
epoch 37 | loss: 0.52824 | val_0_rmse: 1.39411 | val_1_rmse: 2.48816 |  0:13:38s
epoch 38 | loss: 0.52494 | val_0_rmse: 0.78364 | val_1_rmse: 1.00739 |  0:14:00s
epoch 39 | loss: 0.52183 | val_0_rmse: 0.74714 | val_1_rmse: 0.84119 |  0:14:21s
epoch 40 | loss: 0.5209  | val_0_rmse: 0.72804 | val_1_rmse: 0.82619 |  0:14:44s
epoch 41 | loss: 0.51912 | val_0_rmse: 0.75682 | val_1_rmse: 0.86069 |  0:15:05s
epoch 42 | loss: 0.51862 | val_0_rmse: 0.72987 | val_1_rmse: 0.83809 |  0:15:27s
epoch 43 | loss: 0.51406 | val_0_rmse: 0.72181 | val_1_rmse: 0.83141 |  0:15:49s
epoch 44 | loss: 0.51523 | val_0_rmse: 0.75399 | val_1_rmse: 0.87031 |  0:16:10s
epoch 45 | loss: 0.51174 | val_0_rmse: 0.72052 | val_1_rmse: 0.82401 |  0:16:31s
epoch 46 | loss: 0.51022 | val_0_rmse: 0.71825 | val_1_rmse: 0.81957 |  0:16:53s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.81211
Best weights from best epoch are automatically used!
ended training at: 15:16:22
Feature importance:
Mean squared error is of 0.07982444525191773
Mean absolute error:0.19689034151461482
MAPE:0.22022017926989654
R2 score:0.3217863956941135
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:16:27
epoch 0  | loss: 1.01411 | val_0_rmse: 0.93204 | val_1_rmse: 0.9339  |  0:00:21s
epoch 1  | loss: 0.78232 | val_0_rmse: 0.89649 | val_1_rmse: 0.90076 |  0:00:43s
epoch 2  | loss: 0.71398 | val_0_rmse: 0.84362 | val_1_rmse: 0.84337 |  0:01:04s
epoch 3  | loss: 0.69291 | val_0_rmse: 0.83379 | val_1_rmse: 0.83737 |  0:01:26s
epoch 4  | loss: 0.66996 | val_0_rmse: 0.86533 | val_1_rmse: 0.87561 |  0:01:47s
epoch 5  | loss: 0.65421 | val_0_rmse: 0.79361 | val_1_rmse: 0.80595 |  0:02:09s
epoch 6  | loss: 0.6446  | val_0_rmse: 0.77949 | val_1_rmse: 0.80314 |  0:02:31s
epoch 7  | loss: 0.63174 | val_0_rmse: 0.7841  | val_1_rmse: 0.80877 |  0:02:52s
epoch 8  | loss: 0.6264  | val_0_rmse: 0.77588 | val_1_rmse: 0.80597 |  0:03:14s
epoch 9  | loss: 0.61015 | val_0_rmse: 0.77197 | val_1_rmse: 0.80202 |  0:03:36s
epoch 10 | loss: 0.60601 | val_0_rmse: 0.76953 | val_1_rmse: 0.8021  |  0:03:58s
epoch 11 | loss: 0.5966  | val_0_rmse: 0.76736 | val_1_rmse: 0.80277 |  0:04:19s
epoch 12 | loss: 0.59495 | val_0_rmse: 0.76533 | val_1_rmse: 0.80426 |  0:04:41s
epoch 13 | loss: 0.58905 | val_0_rmse: 0.76849 | val_1_rmse: 0.80879 |  0:05:03s
epoch 14 | loss: 0.58502 | val_0_rmse: 0.8522  | val_1_rmse: 0.89226 |  0:05:25s
epoch 15 | loss: 0.58212 | val_0_rmse: 0.82676 | val_1_rmse: 1.02079 |  0:05:47s
epoch 16 | loss: 0.57611 | val_0_rmse: 0.75259 | val_1_rmse: 0.79919 |  0:06:08s
epoch 17 | loss: 0.5743  | val_0_rmse: 0.75308 | val_1_rmse: 0.80264 |  0:06:30s
epoch 18 | loss: 0.57252 | val_0_rmse: 0.79121 | val_1_rmse: 0.84582 |  0:06:52s
epoch 19 | loss: 0.59325 | val_0_rmse: 0.78382 | val_1_rmse: 0.8237  |  0:07:14s
epoch 20 | loss: 0.62545 | val_0_rmse: 0.82476 | val_1_rmse: 0.84942 |  0:07:35s
epoch 21 | loss: 0.63434 | val_0_rmse: 0.77974 | val_1_rmse: 0.81426 |  0:07:57s
epoch 22 | loss: 0.59421 | val_0_rmse: 0.80288 | val_1_rmse: 0.84662 |  0:08:19s
epoch 23 | loss: 0.5807  | val_0_rmse: 0.76489 | val_1_rmse: 0.82114 |  0:08:41s
epoch 24 | loss: 0.57755 | val_0_rmse: 0.75714 | val_1_rmse: 0.81585 |  0:09:02s
epoch 25 | loss: 0.56813 | val_0_rmse: 0.76083 | val_1_rmse: 0.82037 |  0:09:24s
epoch 26 | loss: 0.56468 | val_0_rmse: 0.7601  | val_1_rmse: 0.82699 |  0:09:46s
epoch 27 | loss: 0.5575  | val_0_rmse: 0.74981 | val_1_rmse: 0.81159 |  0:10:08s
epoch 28 | loss: 0.55072 | val_0_rmse: 0.83323 | val_1_rmse: 0.86452 |  0:10:29s
epoch 29 | loss: 0.54959 | val_0_rmse: 0.74649 | val_1_rmse: 0.81601 |  0:10:51s
epoch 30 | loss: 0.54645 | val_0_rmse: 0.7517  | val_1_rmse: 0.82398 |  0:11:13s
epoch 31 | loss: 0.54405 | val_0_rmse: 0.74294 | val_1_rmse: 0.80699 |  0:11:35s
epoch 32 | loss: 0.54133 | val_0_rmse: 0.74048 | val_1_rmse: 0.81501 |  0:11:57s
epoch 33 | loss: 0.53796 | val_0_rmse: 0.76015 | val_1_rmse: 0.82531 |  0:12:18s
epoch 34 | loss: 0.53852 | val_0_rmse: 0.74175 | val_1_rmse: 0.81081 |  0:12:41s
epoch 35 | loss: 0.53393 | val_0_rmse: 0.74187 | val_1_rmse: 0.82025 |  0:13:03s
epoch 36 | loss: 0.5357  | val_0_rmse: 0.74834 | val_1_rmse: 0.83397 |  0:13:25s
epoch 37 | loss: 0.53323 | val_0_rmse: 0.74574 | val_1_rmse: 0.81627 |  0:13:46s
epoch 38 | loss: 0.53013 | val_0_rmse: 0.74124 | val_1_rmse: 0.81982 |  0:14:08s
epoch 39 | loss: 0.52958 | val_0_rmse: 0.73765 | val_1_rmse: 0.82762 |  0:14:29s
epoch 40 | loss: 0.52607 | val_0_rmse: 0.76293 | val_1_rmse: 0.84633 |  0:14:51s
epoch 41 | loss: 0.52538 | val_0_rmse: 0.73536 | val_1_rmse: 0.85672 |  0:15:12s
epoch 42 | loss: 0.52211 | val_0_rmse: 0.73536 | val_1_rmse: 0.81858 |  0:15:34s
epoch 43 | loss: 0.52434 | val_0_rmse: 0.73501 | val_1_rmse: 0.83288 |  0:15:56s
epoch 44 | loss: 0.51994 | val_0_rmse: 0.73489 | val_1_rmse: 0.83275 |  0:16:17s
epoch 45 | loss: 0.51584 | val_0_rmse: 0.73717 | val_1_rmse: 0.83223 |  0:16:39s
epoch 46 | loss: 0.51577 | val_0_rmse: 0.73155 | val_1_rmse: 0.82554 |  0:17:01s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.79919
Best weights from best epoch are automatically used!
ended training at: 15:33:39
Feature importance:
Mean squared error is of 0.08061468094830124
Mean absolute error:0.19514663181477096
MAPE:0.21345997380474221
R2 score:0.3463249356005542
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
TabNet params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:33:44
epoch 0  | loss: 0.98671 | val_0_rmse: 0.89446 | val_1_rmse: 0.91096 |  0:00:21s
epoch 1  | loss: 0.73847 | val_0_rmse: 0.95546 | val_1_rmse: 0.969   |  0:00:43s
epoch 2  | loss: 0.69809 | val_0_rmse: 0.84355 | val_1_rmse: 0.85688 |  0:01:04s
epoch 3  | loss: 0.67309 | val_0_rmse: 0.81609 | val_1_rmse: 0.8336  |  0:01:26s
epoch 4  | loss: 0.65509 | val_0_rmse: 0.80063 | val_1_rmse: 0.82157 |  0:01:47s
epoch 5  | loss: 0.63372 | val_0_rmse: 0.79742 | val_1_rmse: 0.82518 |  0:02:09s
epoch 6  | loss: 0.62788 | val_0_rmse: 0.78331 | val_1_rmse: 0.81637 |  0:02:30s
epoch 7  | loss: 0.61499 | val_0_rmse: 0.77347 | val_1_rmse: 0.81325 |  0:02:52s
epoch 8  | loss: 0.61078 | val_0_rmse: 0.77751 | val_1_rmse: 0.82169 |  0:03:14s
epoch 9  | loss: 0.6008  | val_0_rmse: 0.78763 | val_1_rmse: 0.84306 |  0:03:35s
epoch 10 | loss: 0.5957  | val_0_rmse: 0.80871 | val_1_rmse: 0.85496 |  0:03:57s
epoch 11 | loss: 0.58992 | val_0_rmse: 0.78938 | val_1_rmse: 0.84366 |  0:04:18s
epoch 12 | loss: 0.58888 | val_0_rmse: 0.80046 | val_1_rmse: 0.83473 |  0:04:40s
epoch 13 | loss: 0.58306 | val_0_rmse: 0.83183 | val_1_rmse: 0.8717  |  0:05:01s
epoch 14 | loss: 0.57765 | val_0_rmse: 0.76377 | val_1_rmse: 0.82469 |  0:05:23s
epoch 15 | loss: 0.57538 | val_0_rmse: 0.75427 | val_1_rmse: 0.81257 |  0:05:44s
epoch 16 | loss: 0.57343 | val_0_rmse: 0.75779 | val_1_rmse: 0.81999 |  0:06:06s
epoch 17 | loss: 0.56831 | val_0_rmse: 0.75437 | val_1_rmse: 0.82466 |  0:06:27s
epoch 18 | loss: 0.57123 | val_0_rmse: 0.86993 | val_1_rmse: 0.91212 |  0:06:49s
epoch 19 | loss: 0.56163 | val_0_rmse: 1.01658 | val_1_rmse: 1.1827  |  0:07:10s
epoch 20 | loss: 0.56051 | val_0_rmse: 0.79357 | val_1_rmse: 0.8567  |  0:07:32s
epoch 21 | loss: 0.55544 | val_0_rmse: 1.43657 | val_1_rmse: 1.90587 |  0:07:54s
epoch 22 | loss: 0.55184 | val_0_rmse: 0.74994 | val_1_rmse: 0.82222 |  0:08:15s
epoch 23 | loss: 0.55039 | val_0_rmse: 0.75226 | val_1_rmse: 0.83217 |  0:08:37s
epoch 24 | loss: 0.54755 | val_0_rmse: 0.75197 | val_1_rmse: 0.82558 |  0:08:58s
epoch 25 | loss: 0.5451  | val_0_rmse: 0.74766 | val_1_rmse: 0.82529 |  0:09:20s
epoch 26 | loss: 0.5463  | val_0_rmse: 0.7492  | val_1_rmse: 0.81689 |  0:09:41s
epoch 27 | loss: 0.54806 | val_0_rmse: 0.74005 | val_1_rmse: 0.81721 |  0:10:03s
epoch 28 | loss: 0.53907 | val_0_rmse: 0.73733 | val_1_rmse: 0.81552 |  0:10:25s
epoch 29 | loss: 0.53653 | val_0_rmse: 0.74748 | val_1_rmse: 0.83382 |  0:10:47s
epoch 30 | loss: 0.53587 | val_0_rmse: 0.73967 | val_1_rmse: 0.81304 |  0:11:09s
epoch 31 | loss: 0.53571 | val_0_rmse: 0.77198 | val_1_rmse: 0.86221 |  0:11:30s
epoch 32 | loss: 0.53254 | val_0_rmse: 0.75311 | val_1_rmse: 0.83543 |  0:11:52s
epoch 33 | loss: 0.53422 | val_0_rmse: 0.88807 | val_1_rmse: 0.93981 |  0:12:13s
epoch 34 | loss: 0.52792 | val_0_rmse: 0.73674 | val_1_rmse: 0.82549 |  0:12:35s
epoch 35 | loss: 0.52872 | val_0_rmse: 0.73568 | val_1_rmse: 0.82473 |  0:12:56s
epoch 36 | loss: 0.52583 | val_0_rmse: 0.84076 | val_1_rmse: 0.89998 |  0:13:18s
epoch 37 | loss: 0.52498 | val_0_rmse: 0.74209 | val_1_rmse: 0.82492 |  0:13:40s
epoch 38 | loss: 0.52252 | val_0_rmse: 0.75157 | val_1_rmse: 0.85102 |  0:14:01s
epoch 39 | loss: 0.52318 | val_0_rmse: 0.75896 | val_1_rmse: 0.85809 |  0:14:23s
epoch 40 | loss: 0.5211  | val_0_rmse: 0.72783 | val_1_rmse: 0.82394 |  0:14:44s
epoch 41 | loss: 0.51794 | val_0_rmse: 0.87732 | val_1_rmse: 0.94292 |  0:15:06s
epoch 42 | loss: 0.51672 | val_0_rmse: 0.73623 | val_1_rmse: 0.83093 |  0:15:27s
epoch 43 | loss: 0.51843 | val_0_rmse: 0.73003 | val_1_rmse: 0.8348  |  0:15:49s
epoch 44 | loss: 0.51311 | val_0_rmse: 0.73026 | val_1_rmse: 0.82926 |  0:16:11s
epoch 45 | loss: 0.51112 | val_0_rmse: 0.73898 | val_1_rmse: 0.84399 |  0:16:32s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.81257
Best weights from best epoch are automatically used!
ended training at: 15:50:27
Feature importance:
Mean squared error is of 0.08341274250065309
Mean absolute error:0.19485624131282503
MAPE:0.21256953064874484
R2 score:0.3481575408315555
------------------------------------------------------------------
