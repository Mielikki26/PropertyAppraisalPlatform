TabNet Logs:

Saving copy of script...
In this script all datasets are decreased in size down to the size of the smallest dataset by sampling random rows and deleting them from the dataThis is done to test the possibility that the variance in datasets sizes is decreasing performanceBy evening out the sizes its excepted that the model achieves better performance
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:27:08
epoch 0  | loss: 0.80429 | val_0_rmse: 0.81513 | val_1_rmse: 0.81911 |  0:00:04s
epoch 1  | loss: 0.55886 | val_0_rmse: 0.73872 | val_1_rmse: 0.76389 |  0:00:05s
epoch 2  | loss: 0.47478 | val_0_rmse: 0.70438 | val_1_rmse: 0.71614 |  0:00:07s
epoch 3  | loss: 0.41959 | val_0_rmse: 0.63344 | val_1_rmse: 0.65075 |  0:00:08s
epoch 4  | loss: 0.39013 | val_0_rmse: 0.61021 | val_1_rmse: 0.63299 |  0:00:09s
epoch 5  | loss: 0.3807  | val_0_rmse: 0.60545 | val_1_rmse: 0.62302 |  0:00:10s
epoch 6  | loss: 0.36117 | val_0_rmse: 0.58285 | val_1_rmse: 0.59629 |  0:00:11s
epoch 7  | loss: 0.3622  | val_0_rmse: 0.57875 | val_1_rmse: 0.59515 |  0:00:12s
epoch 8  | loss: 0.36048 | val_0_rmse: 0.63251 | val_1_rmse: 0.64422 |  0:00:13s
epoch 9  | loss: 0.3567  | val_0_rmse: 0.6054  | val_1_rmse: 0.62194 |  0:00:14s
epoch 10 | loss: 0.34307 | val_0_rmse: 0.55949 | val_1_rmse: 0.57747 |  0:00:15s
epoch 11 | loss: 0.3358  | val_0_rmse: 0.55104 | val_1_rmse: 0.57257 |  0:00:16s
epoch 12 | loss: 0.34101 | val_0_rmse: 0.58714 | val_1_rmse: 0.60297 |  0:00:18s
epoch 13 | loss: 0.33659 | val_0_rmse: 0.56139 | val_1_rmse: 0.57861 |  0:00:19s
epoch 14 | loss: 0.34615 | val_0_rmse: 0.56716 | val_1_rmse: 0.58372 |  0:00:20s
epoch 15 | loss: 0.33518 | val_0_rmse: 0.55406 | val_1_rmse: 0.57199 |  0:00:21s
epoch 16 | loss: 0.32761 | val_0_rmse: 0.556   | val_1_rmse: 0.57253 |  0:00:22s
epoch 17 | loss: 0.34078 | val_0_rmse: 0.54783 | val_1_rmse: 0.57121 |  0:00:23s
epoch 18 | loss: 0.32096 | val_0_rmse: 0.55533 | val_1_rmse: 0.57318 |  0:00:24s
epoch 19 | loss: 0.33191 | val_0_rmse: 0.55466 | val_1_rmse: 0.57549 |  0:00:25s
epoch 20 | loss: 0.33559 | val_0_rmse: 0.54595 | val_1_rmse: 0.56744 |  0:00:26s
epoch 21 | loss: 0.33477 | val_0_rmse: 0.55251 | val_1_rmse: 0.57841 |  0:00:27s
epoch 22 | loss: 0.32943 | val_0_rmse: 0.55848 | val_1_rmse: 0.57723 |  0:00:29s
epoch 23 | loss: 0.32098 | val_0_rmse: 0.53317 | val_1_rmse: 0.55643 |  0:00:30s
epoch 24 | loss: 0.31845 | val_0_rmse: 0.54271 | val_1_rmse: 0.56022 |  0:00:31s
epoch 25 | loss: 0.32152 | val_0_rmse: 0.55554 | val_1_rmse: 0.57948 |  0:00:32s
epoch 26 | loss: 0.31957 | val_0_rmse: 0.53679 | val_1_rmse: 0.56311 |  0:00:33s
epoch 27 | loss: 0.31062 | val_0_rmse: 0.53733 | val_1_rmse: 0.55962 |  0:00:34s
epoch 28 | loss: 0.30941 | val_0_rmse: 0.53445 | val_1_rmse: 0.55553 |  0:00:35s
epoch 29 | loss: 0.32115 | val_0_rmse: 0.53904 | val_1_rmse: 0.5585  |  0:00:36s
epoch 30 | loss: 0.3202  | val_0_rmse: 0.54337 | val_1_rmse: 0.5638  |  0:00:37s
epoch 31 | loss: 0.32524 | val_0_rmse: 0.56808 | val_1_rmse: 0.58383 |  0:00:39s
epoch 32 | loss: 0.32608 | val_0_rmse: 0.54855 | val_1_rmse: 0.57345 |  0:00:40s
epoch 33 | loss: 0.32605 | val_0_rmse: 0.55582 | val_1_rmse: 0.57428 |  0:00:41s
epoch 34 | loss: 0.31374 | val_0_rmse: 0.54154 | val_1_rmse: 0.56099 |  0:00:42s
epoch 35 | loss: 0.31163 | val_0_rmse: 0.54362 | val_1_rmse: 0.56404 |  0:00:43s
epoch 36 | loss: 0.31555 | val_0_rmse: 0.53986 | val_1_rmse: 0.55929 |  0:00:44s
epoch 37 | loss: 0.31101 | val_0_rmse: 0.54024 | val_1_rmse: 0.56169 |  0:00:45s
epoch 38 | loss: 0.31852 | val_0_rmse: 0.5595  | val_1_rmse: 0.57713 |  0:00:46s
epoch 39 | loss: 0.31562 | val_0_rmse: 0.55063 | val_1_rmse: 0.56911 |  0:00:47s
epoch 40 | loss: 0.32727 | val_0_rmse: 0.55377 | val_1_rmse: 0.57231 |  0:00:48s
epoch 41 | loss: 0.31493 | val_0_rmse: 0.54365 | val_1_rmse: 0.56352 |  0:00:50s
epoch 42 | loss: 0.31022 | val_0_rmse: 0.53014 | val_1_rmse: 0.55094 |  0:00:51s
epoch 43 | loss: 0.30793 | val_0_rmse: 0.53808 | val_1_rmse: 0.5598  |  0:00:52s
epoch 44 | loss: 0.31291 | val_0_rmse: 0.5357  | val_1_rmse: 0.55725 |  0:00:53s
epoch 45 | loss: 0.3166  | val_0_rmse: 0.54441 | val_1_rmse: 0.56905 |  0:00:54s
epoch 46 | loss: 0.30781 | val_0_rmse: 0.53302 | val_1_rmse: 0.5552  |  0:00:55s
epoch 47 | loss: 0.3209  | val_0_rmse: 0.55485 | val_1_rmse: 0.56965 |  0:00:56s
epoch 48 | loss: 0.31488 | val_0_rmse: 0.52623 | val_1_rmse: 0.55122 |  0:00:57s
epoch 49 | loss: 0.30249 | val_0_rmse: 0.52919 | val_1_rmse: 0.54996 |  0:00:58s
epoch 50 | loss: 0.30551 | val_0_rmse: 0.53174 | val_1_rmse: 0.55773 |  0:00:59s
epoch 51 | loss: 0.30627 | val_0_rmse: 0.54407 | val_1_rmse: 0.57033 |  0:01:01s
epoch 52 | loss: 0.3141  | val_0_rmse: 0.54854 | val_1_rmse: 0.56989 |  0:01:02s
epoch 53 | loss: 0.31043 | val_0_rmse: 0.54666 | val_1_rmse: 0.57614 |  0:01:03s
epoch 54 | loss: 0.3073  | val_0_rmse: 0.53406 | val_1_rmse: 0.55938 |  0:01:04s
epoch 55 | loss: 0.30221 | val_0_rmse: 0.52641 | val_1_rmse: 0.55005 |  0:01:05s
epoch 56 | loss: 0.29864 | val_0_rmse: 0.54312 | val_1_rmse: 0.56365 |  0:01:06s
epoch 57 | loss: 0.31947 | val_0_rmse: 0.55199 | val_1_rmse: 0.56987 |  0:01:07s
epoch 58 | loss: 0.32301 | val_0_rmse: 0.57918 | val_1_rmse: 0.59598 |  0:01:08s
epoch 59 | loss: 0.31509 | val_0_rmse: 0.55357 | val_1_rmse: 0.57328 |  0:01:09s
epoch 60 | loss: 0.31074 | val_0_rmse: 0.55198 | val_1_rmse: 0.57126 |  0:01:10s
epoch 61 | loss: 0.30567 | val_0_rmse: 0.52828 | val_1_rmse: 0.54931 |  0:01:12s
epoch 62 | loss: 0.3093  | val_0_rmse: 0.534   | val_1_rmse: 0.55402 |  0:01:13s
epoch 63 | loss: 0.31444 | val_0_rmse: 0.54092 | val_1_rmse: 0.56462 |  0:01:14s
epoch 64 | loss: 0.32181 | val_0_rmse: 0.54232 | val_1_rmse: 0.56598 |  0:01:15s
epoch 65 | loss: 0.31559 | val_0_rmse: 0.52483 | val_1_rmse: 0.54779 |  0:01:16s
epoch 66 | loss: 0.30916 | val_0_rmse: 0.56889 | val_1_rmse: 0.58907 |  0:01:17s
epoch 67 | loss: 0.30257 | val_0_rmse: 0.54591 | val_1_rmse: 0.56754 |  0:01:18s
epoch 68 | loss: 0.30705 | val_0_rmse: 0.53853 | val_1_rmse: 0.55822 |  0:01:19s
epoch 69 | loss: 0.29701 | val_0_rmse: 0.52461 | val_1_rmse: 0.55006 |  0:01:20s
epoch 70 | loss: 0.29804 | val_0_rmse: 0.53203 | val_1_rmse: 0.55117 |  0:01:21s
epoch 71 | loss: 0.2935  | val_0_rmse: 0.54307 | val_1_rmse: 0.5668  |  0:01:23s
epoch 72 | loss: 0.29582 | val_0_rmse: 0.53368 | val_1_rmse: 0.5566  |  0:01:24s
epoch 73 | loss: 0.29338 | val_0_rmse: 0.52677 | val_1_rmse: 0.54883 |  0:01:25s
epoch 74 | loss: 0.29333 | val_0_rmse: 0.52405 | val_1_rmse: 0.55016 |  0:01:26s
epoch 75 | loss: 0.29611 | val_0_rmse: 0.5613  | val_1_rmse: 0.58888 |  0:01:27s
epoch 76 | loss: 0.30112 | val_0_rmse: 0.52221 | val_1_rmse: 0.54661 |  0:01:28s
epoch 77 | loss: 0.30484 | val_0_rmse: 0.52283 | val_1_rmse: 0.54679 |  0:01:29s
epoch 78 | loss: 0.29827 | val_0_rmse: 0.5275  | val_1_rmse: 0.54939 |  0:01:30s
epoch 79 | loss: 0.30053 | val_0_rmse: 0.52603 | val_1_rmse: 0.54655 |  0:01:31s
epoch 80 | loss: 0.29937 | val_0_rmse: 0.5209  | val_1_rmse: 0.547   |  0:01:32s
epoch 81 | loss: 0.29543 | val_0_rmse: 0.56641 | val_1_rmse: 0.59006 |  0:01:34s
epoch 82 | loss: 0.30296 | val_0_rmse: 0.53799 | val_1_rmse: 0.55828 |  0:01:35s
epoch 83 | loss: 0.29755 | val_0_rmse: 0.59241 | val_1_rmse: 0.61191 |  0:01:36s
epoch 84 | loss: 0.2976  | val_0_rmse: 0.52912 | val_1_rmse: 0.55429 |  0:01:37s
epoch 85 | loss: 0.29954 | val_0_rmse: 0.5577  | val_1_rmse: 0.58331 |  0:01:38s
epoch 86 | loss: 0.30222 | val_0_rmse: 0.51905 | val_1_rmse: 0.54418 |  0:01:39s
epoch 87 | loss: 0.29613 | val_0_rmse: 0.51318 | val_1_rmse: 0.54033 |  0:01:40s
epoch 88 | loss: 0.30291 | val_0_rmse: 0.51667 | val_1_rmse: 0.54713 |  0:01:41s
epoch 89 | loss: 0.29553 | val_0_rmse: 0.51729 | val_1_rmse: 0.54527 |  0:01:42s
epoch 90 | loss: 0.2986  | val_0_rmse: 0.5264  | val_1_rmse: 0.55013 |  0:01:43s
epoch 91 | loss: 0.30224 | val_0_rmse: 0.55782 | val_1_rmse: 0.58321 |  0:01:45s
epoch 92 | loss: 0.30484 | val_0_rmse: 0.52719 | val_1_rmse: 0.55127 |  0:01:46s
epoch 93 | loss: 0.29796 | val_0_rmse: 0.51095 | val_1_rmse: 0.53775 |  0:01:47s
epoch 94 | loss: 0.29183 | val_0_rmse: 0.51539 | val_1_rmse: 0.54142 |  0:01:48s
epoch 95 | loss: 0.29262 | val_0_rmse: 0.53792 | val_1_rmse: 0.5653  |  0:01:49s
epoch 96 | loss: 0.29232 | val_0_rmse: 0.51591 | val_1_rmse: 0.53963 |  0:01:50s
epoch 97 | loss: 0.28919 | val_0_rmse: 0.51812 | val_1_rmse: 0.5442  |  0:01:51s
epoch 98 | loss: 0.2929  | val_0_rmse: 0.5088  | val_1_rmse: 0.53806 |  0:01:52s
epoch 99 | loss: 0.29455 | val_0_rmse: 0.61686 | val_1_rmse: 0.63907 |  0:01:53s
epoch 100| loss: 0.31026 | val_0_rmse: 0.53554 | val_1_rmse: 0.55675 |  0:01:54s
epoch 101| loss: 0.29523 | val_0_rmse: 0.55071 | val_1_rmse: 0.57096 |  0:01:56s
epoch 102| loss: 0.30269 | val_0_rmse: 0.52488 | val_1_rmse: 0.54695 |  0:01:57s
epoch 103| loss: 0.31009 | val_0_rmse: 0.54585 | val_1_rmse: 0.56436 |  0:01:58s
epoch 104| loss: 0.29937 | val_0_rmse: 0.51903 | val_1_rmse: 0.5401  |  0:01:59s
epoch 105| loss: 0.28866 | val_0_rmse: 0.56396 | val_1_rmse: 0.58626 |  0:02:00s
epoch 106| loss: 0.29195 | val_0_rmse: 0.52333 | val_1_rmse: 0.54343 |  0:02:01s
epoch 107| loss: 0.29103 | val_0_rmse: 0.5088  | val_1_rmse: 0.53438 |  0:02:02s
epoch 108| loss: 0.29352 | val_0_rmse: 0.5651  | val_1_rmse: 0.57826 |  0:02:03s
epoch 109| loss: 0.29505 | val_0_rmse: 0.53723 | val_1_rmse: 0.5621  |  0:02:04s
epoch 110| loss: 0.29075 | val_0_rmse: 0.5638  | val_1_rmse: 0.59111 |  0:02:05s
epoch 111| loss: 0.31069 | val_0_rmse: 0.54333 | val_1_rmse: 0.56757 |  0:02:07s
epoch 112| loss: 0.29475 | val_0_rmse: 0.54303 | val_1_rmse: 0.56504 |  0:02:08s
epoch 113| loss: 0.30063 | val_0_rmse: 0.52746 | val_1_rmse: 0.55173 |  0:02:09s
epoch 114| loss: 0.30314 | val_0_rmse: 0.52378 | val_1_rmse: 0.55235 |  0:02:10s
epoch 115| loss: 0.29372 | val_0_rmse: 0.51108 | val_1_rmse: 0.53927 |  0:02:11s
epoch 116| loss: 0.28874 | val_0_rmse: 0.51263 | val_1_rmse: 0.53803 |  0:02:12s
epoch 117| loss: 0.28299 | val_0_rmse: 0.51339 | val_1_rmse: 0.53475 |  0:02:13s
epoch 118| loss: 0.29135 | val_0_rmse: 0.50365 | val_1_rmse: 0.53201 |  0:02:14s
epoch 119| loss: 0.29226 | val_0_rmse: 0.50879 | val_1_rmse: 0.53507 |  0:02:15s
epoch 120| loss: 0.28653 | val_0_rmse: 0.52079 | val_1_rmse: 0.54417 |  0:02:16s
epoch 121| loss: 0.29713 | val_0_rmse: 0.5192  | val_1_rmse: 0.54384 |  0:02:18s
epoch 122| loss: 0.29314 | val_0_rmse: 0.51356 | val_1_rmse: 0.53849 |  0:02:19s
epoch 123| loss: 0.28602 | val_0_rmse: 0.5136  | val_1_rmse: 0.53904 |  0:02:20s
epoch 124| loss: 0.28068 | val_0_rmse: 0.50841 | val_1_rmse: 0.53377 |  0:02:21s
epoch 125| loss: 0.28626 | val_0_rmse: 0.52336 | val_1_rmse: 0.54639 |  0:02:22s
epoch 126| loss: 0.28077 | val_0_rmse: 0.5522  | val_1_rmse: 0.57126 |  0:02:23s
epoch 127| loss: 0.28267 | val_0_rmse: 0.50974 | val_1_rmse: 0.53303 |  0:02:24s
epoch 128| loss: 0.28856 | val_0_rmse: 0.52068 | val_1_rmse: 0.54446 |  0:02:25s
epoch 129| loss: 0.28837 | val_0_rmse: 0.50902 | val_1_rmse: 0.53971 |  0:02:26s
epoch 130| loss: 0.283   | val_0_rmse: 0.53724 | val_1_rmse: 0.56136 |  0:02:27s
epoch 131| loss: 0.2758  | val_0_rmse: 0.52497 | val_1_rmse: 0.55202 |  0:02:29s
epoch 132| loss: 0.28386 | val_0_rmse: 0.50016 | val_1_rmse: 0.52434 |  0:02:30s
epoch 133| loss: 0.28198 | val_0_rmse: 0.51357 | val_1_rmse: 0.54046 |  0:02:31s
epoch 134| loss: 0.28204 | val_0_rmse: 0.50177 | val_1_rmse: 0.52793 |  0:02:32s
epoch 135| loss: 0.28055 | val_0_rmse: 0.50588 | val_1_rmse: 0.53699 |  0:02:33s
epoch 136| loss: 0.28129 | val_0_rmse: 0.5066  | val_1_rmse: 0.53444 |  0:02:34s
epoch 137| loss: 0.28267 | val_0_rmse: 0.51474 | val_1_rmse: 0.54508 |  0:02:35s
epoch 138| loss: 0.28178 | val_0_rmse: 0.50745 | val_1_rmse: 0.53103 |  0:02:36s
epoch 139| loss: 0.28039 | val_0_rmse: 0.50454 | val_1_rmse: 0.533   |  0:02:37s
epoch 140| loss: 0.27498 | val_0_rmse: 0.54246 | val_1_rmse: 0.57374 |  0:02:39s
epoch 141| loss: 0.28314 | val_0_rmse: 0.50113 | val_1_rmse: 0.52831 |  0:02:40s
epoch 142| loss: 0.27868 | val_0_rmse: 0.54222 | val_1_rmse: 0.57046 |  0:02:41s
epoch 143| loss: 0.28058 | val_0_rmse: 0.51202 | val_1_rmse: 0.54175 |  0:02:42s
epoch 144| loss: 0.28066 | val_0_rmse: 0.5067  | val_1_rmse: 0.52886 |  0:02:43s
epoch 145| loss: 0.27597 | val_0_rmse: 0.48646 | val_1_rmse: 0.51777 |  0:02:44s
epoch 146| loss: 0.29023 | val_0_rmse: 0.65622 | val_1_rmse: 0.68395 |  0:02:45s
epoch 147| loss: 0.31286 | val_0_rmse: 0.59097 | val_1_rmse: 0.61745 |  0:02:46s
epoch 148| loss: 0.29572 | val_0_rmse: 0.52508 | val_1_rmse: 0.54598 |  0:02:48s
epoch 149| loss: 0.30071 | val_0_rmse: 0.59727 | val_1_rmse: 0.62701 |  0:02:49s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.51777
Best weights from best epoch are automatically used!
ended training at: 09:29:58
Feature importance:
[('Area', 0.3095237359841039), ('Baths', 0.042217002783213914), ('Beds', 0.0), ('Latitude', 0.29341719736544053), ('Longitude', 0.2763609239526778), ('Month', 8.514695918282255e-06), ('Year', 0.0784726252186456)]
Mean squared error is of 5865611663.892581
Mean absolute error:52236.89587805247
MAPE:0.17045964152850684
R2 score:0.7355155512283018
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:29:59
epoch 0  | loss: 0.79821 | val_0_rmse: 0.88335 | val_1_rmse: 0.90853 |  0:00:01s
epoch 1  | loss: 0.51829 | val_0_rmse: 0.70441 | val_1_rmse: 0.73735 |  0:00:02s
epoch 2  | loss: 0.45042 | val_0_rmse: 0.65473 | val_1_rmse: 0.68444 |  0:00:03s
epoch 3  | loss: 0.41448 | val_0_rmse: 0.63565 | val_1_rmse: 0.6648  |  0:00:04s
epoch 4  | loss: 0.40058 | val_0_rmse: 0.66511 | val_1_rmse: 0.69245 |  0:00:05s
epoch 5  | loss: 0.3838  | val_0_rmse: 0.59243 | val_1_rmse: 0.62701 |  0:00:06s
epoch 6  | loss: 0.37361 | val_0_rmse: 0.60283 | val_1_rmse: 0.64481 |  0:00:07s
epoch 7  | loss: 0.36974 | val_0_rmse: 0.58697 | val_1_rmse: 0.62684 |  0:00:08s
epoch 8  | loss: 0.36297 | val_0_rmse: 0.62821 | val_1_rmse: 0.65893 |  0:00:10s
epoch 9  | loss: 0.35624 | val_0_rmse: 0.56935 | val_1_rmse: 0.61336 |  0:00:11s
epoch 10 | loss: 0.34743 | val_0_rmse: 0.56385 | val_1_rmse: 0.60422 |  0:00:12s
epoch 11 | loss: 0.3543  | val_0_rmse: 0.6236  | val_1_rmse: 0.65849 |  0:00:13s
epoch 12 | loss: 0.35292 | val_0_rmse: 0.57503 | val_1_rmse: 0.60919 |  0:00:14s
epoch 13 | loss: 0.34592 | val_0_rmse: 0.57427 | val_1_rmse: 0.60895 |  0:00:15s
epoch 14 | loss: 0.34705 | val_0_rmse: 0.58429 | val_1_rmse: 0.61765 |  0:00:16s
epoch 15 | loss: 0.34074 | val_0_rmse: 0.57626 | val_1_rmse: 0.61098 |  0:00:17s
epoch 16 | loss: 0.32979 | val_0_rmse: 0.56615 | val_1_rmse: 0.60925 |  0:00:18s
epoch 17 | loss: 0.33134 | val_0_rmse: 0.55423 | val_1_rmse: 0.59672 |  0:00:19s
epoch 18 | loss: 0.33156 | val_0_rmse: 0.59426 | val_1_rmse: 0.63165 |  0:00:21s
epoch 19 | loss: 0.32094 | val_0_rmse: 0.54805 | val_1_rmse: 0.59014 |  0:00:22s
epoch 20 | loss: 0.32344 | val_0_rmse: 0.5587  | val_1_rmse: 0.59415 |  0:00:23s
epoch 21 | loss: 0.32491 | val_0_rmse: 0.5614  | val_1_rmse: 0.59908 |  0:00:24s
epoch 22 | loss: 0.3355  | val_0_rmse: 0.55983 | val_1_rmse: 0.59567 |  0:00:25s
epoch 23 | loss: 0.32367 | val_0_rmse: 0.56269 | val_1_rmse: 0.60079 |  0:00:26s
epoch 24 | loss: 0.33525 | val_0_rmse: 0.54711 | val_1_rmse: 0.58747 |  0:00:27s
epoch 25 | loss: 0.32023 | val_0_rmse: 0.53979 | val_1_rmse: 0.58059 |  0:00:28s
epoch 26 | loss: 0.3169  | val_0_rmse: 0.54531 | val_1_rmse: 0.58508 |  0:00:29s
epoch 27 | loss: 0.31316 | val_0_rmse: 0.53311 | val_1_rmse: 0.57589 |  0:00:31s
epoch 28 | loss: 0.30446 | val_0_rmse: 0.53239 | val_1_rmse: 0.5736  |  0:00:32s
epoch 29 | loss: 0.31227 | val_0_rmse: 0.53677 | val_1_rmse: 0.57303 |  0:00:33s
epoch 30 | loss: 0.30804 | val_0_rmse: 0.53617 | val_1_rmse: 0.5784  |  0:00:34s
epoch 31 | loss: 0.30887 | val_0_rmse: 0.54214 | val_1_rmse: 0.58238 |  0:00:35s
epoch 32 | loss: 0.32001 | val_0_rmse: 0.53839 | val_1_rmse: 0.5797  |  0:00:36s
epoch 33 | loss: 0.30192 | val_0_rmse: 0.53184 | val_1_rmse: 0.57141 |  0:00:37s
epoch 34 | loss: 0.3093  | val_0_rmse: 0.52569 | val_1_rmse: 0.56675 |  0:00:38s
epoch 35 | loss: 0.30763 | val_0_rmse: 0.53034 | val_1_rmse: 0.56651 |  0:00:39s
epoch 36 | loss: 0.30571 | val_0_rmse: 0.54932 | val_1_rmse: 0.5844  |  0:00:41s
epoch 37 | loss: 0.30502 | val_0_rmse: 0.53334 | val_1_rmse: 0.57165 |  0:00:42s
epoch 38 | loss: 0.30237 | val_0_rmse: 0.55656 | val_1_rmse: 0.59609 |  0:00:43s
epoch 39 | loss: 0.30433 | val_0_rmse: 0.52381 | val_1_rmse: 0.56166 |  0:00:44s
epoch 40 | loss: 0.30195 | val_0_rmse: 0.52511 | val_1_rmse: 0.56552 |  0:00:45s
epoch 41 | loss: 0.30111 | val_0_rmse: 0.51814 | val_1_rmse: 0.55705 |  0:00:46s
epoch 42 | loss: 0.29851 | val_0_rmse: 0.5272  | val_1_rmse: 0.56322 |  0:00:47s
epoch 43 | loss: 0.30309 | val_0_rmse: 0.5287  | val_1_rmse: 0.56648 |  0:00:48s
epoch 44 | loss: 0.30484 | val_0_rmse: 0.54645 | val_1_rmse: 0.58798 |  0:00:49s
epoch 45 | loss: 0.30504 | val_0_rmse: 0.52391 | val_1_rmse: 0.56227 |  0:00:50s
epoch 46 | loss: 0.2979  | val_0_rmse: 0.54553 | val_1_rmse: 0.58982 |  0:00:52s
epoch 47 | loss: 0.29669 | val_0_rmse: 0.53385 | val_1_rmse: 0.57465 |  0:00:53s
epoch 48 | loss: 0.30184 | val_0_rmse: 0.52682 | val_1_rmse: 0.57141 |  0:00:54s
epoch 49 | loss: 0.29252 | val_0_rmse: 0.51543 | val_1_rmse: 0.55731 |  0:00:55s
epoch 50 | loss: 0.29061 | val_0_rmse: 0.5283  | val_1_rmse: 0.57144 |  0:00:56s
epoch 51 | loss: 0.29486 | val_0_rmse: 0.5241  | val_1_rmse: 0.56536 |  0:00:57s
epoch 52 | loss: 0.29842 | val_0_rmse: 0.51496 | val_1_rmse: 0.56066 |  0:00:58s
epoch 53 | loss: 0.29471 | val_0_rmse: 0.54957 | val_1_rmse: 0.58905 |  0:00:59s
epoch 54 | loss: 0.30285 | val_0_rmse: 0.53043 | val_1_rmse: 0.57316 |  0:01:00s
epoch 55 | loss: 0.29227 | val_0_rmse: 0.51078 | val_1_rmse: 0.55272 |  0:01:01s
epoch 56 | loss: 0.28539 | val_0_rmse: 0.5153  | val_1_rmse: 0.55939 |  0:01:03s
epoch 57 | loss: 0.28245 | val_0_rmse: 0.51037 | val_1_rmse: 0.54971 |  0:01:04s
epoch 58 | loss: 0.28986 | val_0_rmse: 0.50428 | val_1_rmse: 0.54939 |  0:01:05s
epoch 59 | loss: 0.28341 | val_0_rmse: 0.50569 | val_1_rmse: 0.55114 |  0:01:06s
epoch 60 | loss: 0.28488 | val_0_rmse: 0.53643 | val_1_rmse: 0.5799  |  0:01:07s
epoch 61 | loss: 0.2945  | val_0_rmse: 0.50917 | val_1_rmse: 0.55485 |  0:01:08s
epoch 62 | loss: 0.28723 | val_0_rmse: 0.50691 | val_1_rmse: 0.55399 |  0:01:09s
epoch 63 | loss: 0.28451 | val_0_rmse: 0.52502 | val_1_rmse: 0.57026 |  0:01:10s
epoch 64 | loss: 0.28531 | val_0_rmse: 0.52824 | val_1_rmse: 0.57198 |  0:01:11s
epoch 65 | loss: 0.28671 | val_0_rmse: 0.52634 | val_1_rmse: 0.56776 |  0:01:13s
epoch 66 | loss: 0.29031 | val_0_rmse: 0.52607 | val_1_rmse: 0.5698  |  0:01:14s
epoch 67 | loss: 0.28733 | val_0_rmse: 0.52655 | val_1_rmse: 0.56651 |  0:01:15s
epoch 68 | loss: 0.28338 | val_0_rmse: 0.51818 | val_1_rmse: 0.56577 |  0:01:16s
epoch 69 | loss: 0.29107 | val_0_rmse: 0.53662 | val_1_rmse: 0.57544 |  0:01:17s
epoch 70 | loss: 0.28889 | val_0_rmse: 0.50803 | val_1_rmse: 0.55345 |  0:01:18s
epoch 71 | loss: 0.27888 | val_0_rmse: 0.53144 | val_1_rmse: 0.57358 |  0:01:19s
epoch 72 | loss: 0.28525 | val_0_rmse: 0.54392 | val_1_rmse: 0.58832 |  0:01:20s
epoch 73 | loss: 0.28744 | val_0_rmse: 0.52084 | val_1_rmse: 0.56723 |  0:01:21s
epoch 74 | loss: 0.28952 | val_0_rmse: 0.50827 | val_1_rmse: 0.55551 |  0:01:22s
epoch 75 | loss: 0.27979 | val_0_rmse: 0.52634 | val_1_rmse: 0.5736  |  0:01:24s
epoch 76 | loss: 0.27277 | val_0_rmse: 0.50614 | val_1_rmse: 0.55886 |  0:01:25s
epoch 77 | loss: 0.28562 | val_0_rmse: 0.51591 | val_1_rmse: 0.55903 |  0:01:26s
epoch 78 | loss: 0.28232 | val_0_rmse: 0.49829 | val_1_rmse: 0.54662 |  0:01:27s
epoch 79 | loss: 0.28061 | val_0_rmse: 0.51126 | val_1_rmse: 0.56189 |  0:01:28s
epoch 80 | loss: 0.27199 | val_0_rmse: 0.52505 | val_1_rmse: 0.57071 |  0:01:29s
epoch 81 | loss: 0.27577 | val_0_rmse: 0.51612 | val_1_rmse: 0.56197 |  0:01:30s
epoch 82 | loss: 0.27723 | val_0_rmse: 0.51554 | val_1_rmse: 0.56538 |  0:01:31s
epoch 83 | loss: 0.2759  | val_0_rmse: 0.5326  | val_1_rmse: 0.5746  |  0:01:32s
epoch 84 | loss: 0.28024 | val_0_rmse: 0.51118 | val_1_rmse: 0.55671 |  0:01:34s
epoch 85 | loss: 0.27595 | val_0_rmse: 0.50346 | val_1_rmse: 0.55401 |  0:01:35s
epoch 86 | loss: 0.27781 | val_0_rmse: 0.51302 | val_1_rmse: 0.56631 |  0:01:36s
epoch 87 | loss: 0.27959 | val_0_rmse: 0.50345 | val_1_rmse: 0.55152 |  0:01:37s
epoch 88 | loss: 0.27996 | val_0_rmse: 0.50271 | val_1_rmse: 0.5494  |  0:01:38s
epoch 89 | loss: 0.28031 | val_0_rmse: 0.50071 | val_1_rmse: 0.55484 |  0:01:39s
epoch 90 | loss: 0.27638 | val_0_rmse: 0.51185 | val_1_rmse: 0.55427 |  0:01:40s
epoch 91 | loss: 0.28059 | val_0_rmse: 0.52297 | val_1_rmse: 0.57145 |  0:01:41s
epoch 92 | loss: 0.274   | val_0_rmse: 0.49955 | val_1_rmse: 0.54829 |  0:01:42s
epoch 93 | loss: 0.27067 | val_0_rmse: 0.50641 | val_1_rmse: 0.55532 |  0:01:43s
epoch 94 | loss: 0.27275 | val_0_rmse: 0.50488 | val_1_rmse: 0.55356 |  0:01:45s
epoch 95 | loss: 0.27778 | val_0_rmse: 0.51812 | val_1_rmse: 0.5658  |  0:01:46s
epoch 96 | loss: 0.28777 | val_0_rmse: 0.51066 | val_1_rmse: 0.55285 |  0:01:47s
epoch 97 | loss: 0.27402 | val_0_rmse: 0.49417 | val_1_rmse: 0.54531 |  0:01:48s
epoch 98 | loss: 0.27718 | val_0_rmse: 0.51948 | val_1_rmse: 0.56585 |  0:01:49s
epoch 99 | loss: 0.27396 | val_0_rmse: 0.49503 | val_1_rmse: 0.54354 |  0:01:50s
epoch 100| loss: 0.26763 | val_0_rmse: 0.50626 | val_1_rmse: 0.55313 |  0:01:51s
epoch 101| loss: 0.27009 | val_0_rmse: 0.49461 | val_1_rmse: 0.54019 |  0:01:52s
epoch 102| loss: 0.28157 | val_0_rmse: 0.49666 | val_1_rmse: 0.54611 |  0:01:53s
epoch 103| loss: 0.27016 | val_0_rmse: 0.50403 | val_1_rmse: 0.55412 |  0:01:55s
epoch 104| loss: 0.26621 | val_0_rmse: 0.50055 | val_1_rmse: 0.55315 |  0:01:56s
epoch 105| loss: 0.27355 | val_0_rmse: 0.51339 | val_1_rmse: 0.56123 |  0:01:57s
epoch 106| loss: 0.2703  | val_0_rmse: 0.50843 | val_1_rmse: 0.55773 |  0:01:58s
epoch 107| loss: 0.27433 | val_0_rmse: 0.50252 | val_1_rmse: 0.55674 |  0:01:59s
epoch 108| loss: 0.26987 | val_0_rmse: 0.49881 | val_1_rmse: 0.54993 |  0:02:00s
epoch 109| loss: 0.27645 | val_0_rmse: 0.50269 | val_1_rmse: 0.55719 |  0:02:01s
epoch 110| loss: 0.27688 | val_0_rmse: 0.51416 | val_1_rmse: 0.5635  |  0:02:02s
epoch 111| loss: 0.2663  | val_0_rmse: 0.5074  | val_1_rmse: 0.55918 |  0:02:03s
epoch 112| loss: 0.27006 | val_0_rmse: 0.50527 | val_1_rmse: 0.56221 |  0:02:04s
epoch 113| loss: 0.26876 | val_0_rmse: 0.50565 | val_1_rmse: 0.56037 |  0:02:06s
epoch 114| loss: 0.27194 | val_0_rmse: 0.55326 | val_1_rmse: 0.60007 |  0:02:07s
epoch 115| loss: 0.26825 | val_0_rmse: 0.50934 | val_1_rmse: 0.56553 |  0:02:08s
epoch 116| loss: 0.27168 | val_0_rmse: 0.51372 | val_1_rmse: 0.56912 |  0:02:09s
epoch 117| loss: 0.26868 | val_0_rmse: 0.48936 | val_1_rmse: 0.54527 |  0:02:10s
epoch 118| loss: 0.26519 | val_0_rmse: 0.49853 | val_1_rmse: 0.54835 |  0:02:11s
epoch 119| loss: 0.27209 | val_0_rmse: 0.50255 | val_1_rmse: 0.55923 |  0:02:12s
epoch 120| loss: 0.27176 | val_0_rmse: 0.50154 | val_1_rmse: 0.55448 |  0:02:13s
epoch 121| loss: 0.26531 | val_0_rmse: 0.51512 | val_1_rmse: 0.56954 |  0:02:14s
epoch 122| loss: 0.27997 | val_0_rmse: 0.5392  | val_1_rmse: 0.58834 |  0:02:15s
epoch 123| loss: 0.27273 | val_0_rmse: 0.5194  | val_1_rmse: 0.57243 |  0:02:17s
epoch 124| loss: 0.2775  | val_0_rmse: 0.51816 | val_1_rmse: 0.56752 |  0:02:18s
epoch 125| loss: 0.27821 | val_0_rmse: 0.51189 | val_1_rmse: 0.55618 |  0:02:19s
epoch 126| loss: 0.27674 | val_0_rmse: 0.49715 | val_1_rmse: 0.54522 |  0:02:20s
epoch 127| loss: 0.26941 | val_0_rmse: 0.49369 | val_1_rmse: 0.54189 |  0:02:21s
epoch 128| loss: 0.27369 | val_0_rmse: 0.51321 | val_1_rmse: 0.56644 |  0:02:22s
epoch 129| loss: 0.26933 | val_0_rmse: 0.49272 | val_1_rmse: 0.54641 |  0:02:23s
epoch 130| loss: 0.26559 | val_0_rmse: 0.52514 | val_1_rmse: 0.57371 |  0:02:24s
epoch 131| loss: 0.27594 | val_0_rmse: 0.55953 | val_1_rmse: 0.60785 |  0:02:25s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.54019
Best weights from best epoch are automatically used!
ended training at: 09:32:25
Feature importance:
[('Area', 0.2535307420495603), ('Baths', 0.04050037102634278), ('Beds', 0.0), ('Latitude', 0.3259830776596117), ('Longitude', 0.2872253956234068), ('Month', 0.0), ('Year', 0.09276041364107838)]
Mean squared error is of 5947312378.177388
Mean absolute error:53889.53278701343
MAPE:0.18416012536758206
R2 score:0.7289261094027126
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:32:25
epoch 0  | loss: 0.82906 | val_0_rmse: 0.79105 | val_1_rmse: 0.79886 |  0:00:01s
epoch 1  | loss: 0.51323 | val_0_rmse: 0.67223 | val_1_rmse: 0.67478 |  0:00:02s
epoch 2  | loss: 0.43316 | val_0_rmse: 0.6325  | val_1_rmse: 0.64518 |  0:00:03s
epoch 3  | loss: 0.40049 | val_0_rmse: 0.61859 | val_1_rmse: 0.63278 |  0:00:04s
epoch 4  | loss: 0.41154 | val_0_rmse: 0.64677 | val_1_rmse: 0.64903 |  0:00:05s
epoch 5  | loss: 0.40585 | val_0_rmse: 0.60904 | val_1_rmse: 0.62078 |  0:00:06s
epoch 6  | loss: 0.39469 | val_0_rmse: 0.59582 | val_1_rmse: 0.60963 |  0:00:07s
epoch 7  | loss: 0.38648 | val_0_rmse: 0.60069 | val_1_rmse: 0.61616 |  0:00:08s
epoch 8  | loss: 0.39778 | val_0_rmse: 0.62095 | val_1_rmse: 0.6331  |  0:00:10s
epoch 9  | loss: 0.39425 | val_0_rmse: 0.60608 | val_1_rmse: 0.61412 |  0:00:11s
epoch 10 | loss: 0.39123 | val_0_rmse: 0.60501 | val_1_rmse: 0.61457 |  0:00:12s
epoch 11 | loss: 0.38195 | val_0_rmse: 0.59645 | val_1_rmse: 0.60527 |  0:00:13s
epoch 12 | loss: 0.37049 | val_0_rmse: 0.58834 | val_1_rmse: 0.6059  |  0:00:14s
epoch 13 | loss: 0.37266 | val_0_rmse: 0.57842 | val_1_rmse: 0.59181 |  0:00:15s
epoch 14 | loss: 0.36258 | val_0_rmse: 0.57392 | val_1_rmse: 0.58588 |  0:00:16s
epoch 15 | loss: 0.3541  | val_0_rmse: 0.57629 | val_1_rmse: 0.58616 |  0:00:17s
epoch 16 | loss: 0.38426 | val_0_rmse: 0.65009 | val_1_rmse: 0.66328 |  0:00:18s
epoch 17 | loss: 0.37592 | val_0_rmse: 0.59035 | val_1_rmse: 0.60795 |  0:00:19s
epoch 18 | loss: 0.36144 | val_0_rmse: 0.5791  | val_1_rmse: 0.59421 |  0:00:21s
epoch 19 | loss: 0.35563 | val_0_rmse: 0.5758  | val_1_rmse: 0.59192 |  0:00:22s
epoch 20 | loss: 0.34933 | val_0_rmse: 0.55874 | val_1_rmse: 0.57877 |  0:00:23s
epoch 21 | loss: 0.35002 | val_0_rmse: 0.56222 | val_1_rmse: 0.57642 |  0:00:24s
epoch 22 | loss: 0.34743 | val_0_rmse: 0.567   | val_1_rmse: 0.58308 |  0:00:25s
epoch 23 | loss: 0.34505 | val_0_rmse: 0.56941 | val_1_rmse: 0.58303 |  0:00:26s
epoch 24 | loss: 0.34873 | val_0_rmse: 0.56238 | val_1_rmse: 0.57452 |  0:00:27s
epoch 25 | loss: 0.34638 | val_0_rmse: 0.56759 | val_1_rmse: 0.58673 |  0:00:28s
epoch 26 | loss: 0.3384  | val_0_rmse: 0.56415 | val_1_rmse: 0.58283 |  0:00:29s
epoch 27 | loss: 0.33723 | val_0_rmse: 0.57029 | val_1_rmse: 0.58573 |  0:00:31s
epoch 28 | loss: 0.34097 | val_0_rmse: 0.56168 | val_1_rmse: 0.57574 |  0:00:32s
epoch 29 | loss: 0.34055 | val_0_rmse: 0.58347 | val_1_rmse: 0.59079 |  0:00:33s
epoch 30 | loss: 0.34243 | val_0_rmse: 0.56768 | val_1_rmse: 0.58422 |  0:00:34s
epoch 31 | loss: 0.33887 | val_0_rmse: 0.57738 | val_1_rmse: 0.59381 |  0:00:35s
epoch 32 | loss: 0.34591 | val_0_rmse: 0.55785 | val_1_rmse: 0.57351 |  0:00:36s
epoch 33 | loss: 0.3367  | val_0_rmse: 0.56223 | val_1_rmse: 0.57861 |  0:00:37s
epoch 34 | loss: 0.33203 | val_0_rmse: 0.56082 | val_1_rmse: 0.57818 |  0:00:38s
epoch 35 | loss: 0.32306 | val_0_rmse: 0.55414 | val_1_rmse: 0.57201 |  0:00:39s
epoch 36 | loss: 0.32959 | val_0_rmse: 0.54543 | val_1_rmse: 0.56564 |  0:00:40s
epoch 37 | loss: 0.33356 | val_0_rmse: 0.56861 | val_1_rmse: 0.58972 |  0:00:42s
epoch 38 | loss: 0.32568 | val_0_rmse: 0.54728 | val_1_rmse: 0.56604 |  0:00:43s
epoch 39 | loss: 0.32935 | val_0_rmse: 0.57053 | val_1_rmse: 0.58778 |  0:00:44s
epoch 40 | loss: 0.32286 | val_0_rmse: 0.55379 | val_1_rmse: 0.57049 |  0:00:45s
epoch 41 | loss: 0.32962 | val_0_rmse: 0.58905 | val_1_rmse: 0.60791 |  0:00:46s
epoch 42 | loss: 0.32753 | val_0_rmse: 0.54453 | val_1_rmse: 0.56301 |  0:00:47s
epoch 43 | loss: 0.33563 | val_0_rmse: 0.54799 | val_1_rmse: 0.5702  |  0:00:48s
epoch 44 | loss: 0.32914 | val_0_rmse: 0.54624 | val_1_rmse: 0.56366 |  0:00:49s
epoch 45 | loss: 0.32195 | val_0_rmse: 0.54015 | val_1_rmse: 0.56068 |  0:00:50s
epoch 46 | loss: 0.31513 | val_0_rmse: 0.55702 | val_1_rmse: 0.57417 |  0:00:52s
epoch 47 | loss: 0.33153 | val_0_rmse: 0.55253 | val_1_rmse: 0.57116 |  0:00:53s
epoch 48 | loss: 0.33539 | val_0_rmse: 0.59003 | val_1_rmse: 0.61369 |  0:00:54s
epoch 49 | loss: 0.34137 | val_0_rmse: 0.55885 | val_1_rmse: 0.57917 |  0:00:55s
epoch 50 | loss: 0.32892 | val_0_rmse: 0.552   | val_1_rmse: 0.56693 |  0:00:56s
epoch 51 | loss: 0.34393 | val_0_rmse: 0.56664 | val_1_rmse: 0.58429 |  0:00:57s
epoch 52 | loss: 0.34038 | val_0_rmse: 0.5665  | val_1_rmse: 0.58159 |  0:00:58s
epoch 53 | loss: 0.34394 | val_0_rmse: 0.57708 | val_1_rmse: 0.59493 |  0:00:59s
epoch 54 | loss: 0.33174 | val_0_rmse: 0.54076 | val_1_rmse: 0.55916 |  0:01:00s
epoch 55 | loss: 0.31892 | val_0_rmse: 0.54903 | val_1_rmse: 0.56886 |  0:01:01s
epoch 56 | loss: 0.32511 | val_0_rmse: 0.54871 | val_1_rmse: 0.56794 |  0:01:03s
epoch 57 | loss: 0.32652 | val_0_rmse: 0.56443 | val_1_rmse: 0.58951 |  0:01:04s
epoch 58 | loss: 0.32317 | val_0_rmse: 0.54229 | val_1_rmse: 0.56894 |  0:01:05s
epoch 59 | loss: 0.31592 | val_0_rmse: 0.54735 | val_1_rmse: 0.5729  |  0:01:06s
epoch 60 | loss: 0.32465 | val_0_rmse: 0.54288 | val_1_rmse: 0.56704 |  0:01:07s
epoch 61 | loss: 0.3247  | val_0_rmse: 0.54086 | val_1_rmse: 0.56546 |  0:01:08s
epoch 62 | loss: 0.32707 | val_0_rmse: 0.55134 | val_1_rmse: 0.57487 |  0:01:09s
epoch 63 | loss: 0.32487 | val_0_rmse: 0.54656 | val_1_rmse: 0.56562 |  0:01:10s
epoch 64 | loss: 0.32003 | val_0_rmse: 0.53233 | val_1_rmse: 0.55674 |  0:01:11s
epoch 65 | loss: 0.31707 | val_0_rmse: 0.53392 | val_1_rmse: 0.55876 |  0:01:13s
epoch 66 | loss: 0.32259 | val_0_rmse: 0.53732 | val_1_rmse: 0.56201 |  0:01:14s
epoch 67 | loss: 0.31548 | val_0_rmse: 0.5391  | val_1_rmse: 0.56235 |  0:01:15s
epoch 68 | loss: 0.32193 | val_0_rmse: 0.54306 | val_1_rmse: 0.56585 |  0:01:16s
epoch 69 | loss: 0.32188 | val_0_rmse: 0.53826 | val_1_rmse: 0.56404 |  0:01:17s
epoch 70 | loss: 0.31253 | val_0_rmse: 0.53282 | val_1_rmse: 0.55414 |  0:01:18s
epoch 71 | loss: 0.31138 | val_0_rmse: 0.53483 | val_1_rmse: 0.55285 |  0:01:19s
epoch 72 | loss: 0.31307 | val_0_rmse: 0.54863 | val_1_rmse: 0.56582 |  0:01:20s
epoch 73 | loss: 0.32192 | val_0_rmse: 0.54077 | val_1_rmse: 0.55762 |  0:01:21s
epoch 74 | loss: 0.31012 | val_0_rmse: 0.53052 | val_1_rmse: 0.55225 |  0:01:22s
epoch 75 | loss: 0.3098  | val_0_rmse: 0.53439 | val_1_rmse: 0.55089 |  0:01:24s
epoch 76 | loss: 0.31125 | val_0_rmse: 0.56442 | val_1_rmse: 0.58774 |  0:01:25s
epoch 77 | loss: 0.31968 | val_0_rmse: 0.55248 | val_1_rmse: 0.57008 |  0:01:26s
epoch 78 | loss: 0.31159 | val_0_rmse: 0.52694 | val_1_rmse: 0.54948 |  0:01:27s
epoch 79 | loss: 0.30454 | val_0_rmse: 0.52824 | val_1_rmse: 0.55401 |  0:01:28s
epoch 80 | loss: 0.30862 | val_0_rmse: 0.53441 | val_1_rmse: 0.55212 |  0:01:29s
epoch 81 | loss: 0.30533 | val_0_rmse: 0.55233 | val_1_rmse: 0.57607 |  0:01:30s
epoch 82 | loss: 0.31448 | val_0_rmse: 0.55108 | val_1_rmse: 0.56987 |  0:01:31s
epoch 83 | loss: 0.31804 | val_0_rmse: 0.53021 | val_1_rmse: 0.55449 |  0:01:32s
epoch 84 | loss: 0.31082 | val_0_rmse: 0.54458 | val_1_rmse: 0.56378 |  0:01:34s
epoch 85 | loss: 0.30515 | val_0_rmse: 0.53066 | val_1_rmse: 0.55627 |  0:01:35s
epoch 86 | loss: 0.3085  | val_0_rmse: 0.52272 | val_1_rmse: 0.54757 |  0:01:36s
epoch 87 | loss: 0.3035  | val_0_rmse: 0.53912 | val_1_rmse: 0.56042 |  0:01:37s
epoch 88 | loss: 0.31462 | val_0_rmse: 0.54137 | val_1_rmse: 0.56099 |  0:01:38s
epoch 89 | loss: 0.3133  | val_0_rmse: 0.54385 | val_1_rmse: 0.56509 |  0:01:39s
epoch 90 | loss: 0.31198 | val_0_rmse: 0.53147 | val_1_rmse: 0.55244 |  0:01:40s
epoch 91 | loss: 0.31033 | val_0_rmse: 0.53281 | val_1_rmse: 0.55577 |  0:01:41s
epoch 92 | loss: 0.31712 | val_0_rmse: 0.52693 | val_1_rmse: 0.55088 |  0:01:42s
epoch 93 | loss: 0.31601 | val_0_rmse: 0.55334 | val_1_rmse: 0.5764  |  0:01:44s
epoch 94 | loss: 0.32271 | val_0_rmse: 0.53303 | val_1_rmse: 0.55921 |  0:01:45s
epoch 95 | loss: 0.31605 | val_0_rmse: 0.54405 | val_1_rmse: 0.5652  |  0:01:46s
epoch 96 | loss: 0.33095 | val_0_rmse: 0.55916 | val_1_rmse: 0.57363 |  0:01:47s
epoch 97 | loss: 0.34071 | val_0_rmse: 0.55604 | val_1_rmse: 0.57209 |  0:01:48s
epoch 98 | loss: 0.3213  | val_0_rmse: 0.53909 | val_1_rmse: 0.55974 |  0:01:49s
epoch 99 | loss: 0.30949 | val_0_rmse: 0.53371 | val_1_rmse: 0.55743 |  0:01:50s
epoch 100| loss: 0.31189 | val_0_rmse: 0.5343  | val_1_rmse: 0.55519 |  0:01:51s
epoch 101| loss: 0.31591 | val_0_rmse: 0.55432 | val_1_rmse: 0.57806 |  0:01:52s
epoch 102| loss: 0.32085 | val_0_rmse: 0.55534 | val_1_rmse: 0.57504 |  0:01:53s
epoch 103| loss: 0.31385 | val_0_rmse: 0.55552 | val_1_rmse: 0.58051 |  0:01:55s
epoch 104| loss: 0.31482 | val_0_rmse: 0.52109 | val_1_rmse: 0.54615 |  0:01:56s
epoch 105| loss: 0.3087  | val_0_rmse: 0.52561 | val_1_rmse: 0.55033 |  0:01:57s
epoch 106| loss: 0.30125 | val_0_rmse: 0.52236 | val_1_rmse: 0.54366 |  0:01:58s
epoch 107| loss: 0.29415 | val_0_rmse: 0.52103 | val_1_rmse: 0.54502 |  0:01:59s
epoch 108| loss: 0.29663 | val_0_rmse: 0.51913 | val_1_rmse: 0.54771 |  0:02:00s
epoch 109| loss: 0.30265 | val_0_rmse: 0.52469 | val_1_rmse: 0.5448  |  0:02:01s
epoch 110| loss: 0.3034  | val_0_rmse: 0.52078 | val_1_rmse: 0.54075 |  0:02:02s
epoch 111| loss: 0.30685 | val_0_rmse: 0.52297 | val_1_rmse: 0.54652 |  0:02:03s
epoch 112| loss: 0.29951 | val_0_rmse: 0.52537 | val_1_rmse: 0.54876 |  0:02:05s
epoch 113| loss: 0.30989 | val_0_rmse: 0.54006 | val_1_rmse: 0.56489 |  0:02:06s
epoch 114| loss: 0.30083 | val_0_rmse: 0.52084 | val_1_rmse: 0.54742 |  0:02:07s
epoch 115| loss: 0.29963 | val_0_rmse: 0.5318  | val_1_rmse: 0.55793 |  0:02:08s
epoch 116| loss: 0.30396 | val_0_rmse: 0.53942 | val_1_rmse: 0.56394 |  0:02:09s
epoch 117| loss: 0.30848 | val_0_rmse: 0.52869 | val_1_rmse: 0.55602 |  0:02:10s
epoch 118| loss: 0.2961  | val_0_rmse: 0.52779 | val_1_rmse: 0.55283 |  0:02:11s
epoch 119| loss: 0.3159  | val_0_rmse: 0.52729 | val_1_rmse: 0.55069 |  0:02:12s
epoch 120| loss: 0.31967 | val_0_rmse: 0.57093 | val_1_rmse: 0.5878  |  0:02:13s
epoch 121| loss: 0.32734 | val_0_rmse: 0.5664  | val_1_rmse: 0.58581 |  0:02:14s
epoch 122| loss: 0.31467 | val_0_rmse: 0.52827 | val_1_rmse: 0.5525  |  0:02:16s
epoch 123| loss: 0.3023  | val_0_rmse: 0.5331  | val_1_rmse: 0.55728 |  0:02:17s
epoch 124| loss: 0.30557 | val_0_rmse: 0.52806 | val_1_rmse: 0.5555  |  0:02:18s
epoch 125| loss: 0.30217 | val_0_rmse: 0.52563 | val_1_rmse: 0.54657 |  0:02:19s
epoch 126| loss: 0.29371 | val_0_rmse: 0.51195 | val_1_rmse: 0.53824 |  0:02:20s
epoch 127| loss: 0.28851 | val_0_rmse: 0.51368 | val_1_rmse: 0.5405  |  0:02:21s
epoch 128| loss: 0.29577 | val_0_rmse: 0.51417 | val_1_rmse: 0.54242 |  0:02:22s
epoch 129| loss: 0.29513 | val_0_rmse: 0.52756 | val_1_rmse: 0.55424 |  0:02:23s
epoch 130| loss: 0.29107 | val_0_rmse: 0.50813 | val_1_rmse: 0.53547 |  0:02:24s
epoch 131| loss: 0.28607 | val_0_rmse: 0.51219 | val_1_rmse: 0.53513 |  0:02:25s
epoch 132| loss: 0.28865 | val_0_rmse: 0.52263 | val_1_rmse: 0.55403 |  0:02:27s
epoch 133| loss: 0.29614 | val_0_rmse: 0.52226 | val_1_rmse: 0.54506 |  0:02:28s
epoch 134| loss: 0.30265 | val_0_rmse: 0.52799 | val_1_rmse: 0.55221 |  0:02:29s
epoch 135| loss: 0.30981 | val_0_rmse: 0.52606 | val_1_rmse: 0.55253 |  0:02:30s
epoch 136| loss: 0.30518 | val_0_rmse: 0.54124 | val_1_rmse: 0.55775 |  0:02:31s
epoch 137| loss: 0.30537 | val_0_rmse: 0.53326 | val_1_rmse: 0.55703 |  0:02:32s
epoch 138| loss: 0.31116 | val_0_rmse: 0.53372 | val_1_rmse: 0.56389 |  0:02:33s
epoch 139| loss: 0.30562 | val_0_rmse: 0.52755 | val_1_rmse: 0.55247 |  0:02:34s
epoch 140| loss: 0.30732 | val_0_rmse: 0.51967 | val_1_rmse: 0.54461 |  0:02:35s
epoch 141| loss: 0.29288 | val_0_rmse: 0.51369 | val_1_rmse: 0.54237 |  0:02:37s
epoch 142| loss: 0.28958 | val_0_rmse: 0.51573 | val_1_rmse: 0.54228 |  0:02:38s
epoch 143| loss: 0.29685 | val_0_rmse: 0.52022 | val_1_rmse: 0.54783 |  0:02:39s
epoch 144| loss: 0.28957 | val_0_rmse: 0.52145 | val_1_rmse: 0.55012 |  0:02:40s
epoch 145| loss: 0.29204 | val_0_rmse: 0.51826 | val_1_rmse: 0.54135 |  0:02:41s
epoch 146| loss: 0.29291 | val_0_rmse: 0.52082 | val_1_rmse: 0.55072 |  0:02:42s
epoch 147| loss: 0.29695 | val_0_rmse: 0.52658 | val_1_rmse: 0.55617 |  0:02:43s
epoch 148| loss: 0.29064 | val_0_rmse: 0.51468 | val_1_rmse: 0.54409 |  0:02:44s
epoch 149| loss: 0.29009 | val_0_rmse: 0.51506 | val_1_rmse: 0.54573 |  0:02:45s
Stop training because you reached max_epochs = 150 with best_epoch = 131 and best_val_1_rmse = 0.53513
Best weights from best epoch are automatically used!
ended training at: 09:35:11
Feature importance:
[('Area', 0.25230926096177525), ('Baths', 0.04744403736637599), ('Beds', 0.03615724288514437), ('Latitude', 0.2578246728420448), ('Longitude', 0.37854919279345095), ('Month', 0.0), ('Year', 0.02771559315120861)]
Mean squared error is of 6185686197.228802
Mean absolute error:53998.62166995463
MAPE:0.1799814740298007
R2 score:0.721565469178268
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:35:11
epoch 0  | loss: 0.79781 | val_0_rmse: 0.79256 | val_1_rmse: 0.83332 |  0:00:01s
epoch 1  | loss: 0.49086 | val_0_rmse: 0.69203 | val_1_rmse: 0.71192 |  0:00:02s
epoch 2  | loss: 0.4426  | val_0_rmse: 0.6861  | val_1_rmse: 0.70484 |  0:00:03s
epoch 3  | loss: 0.40762 | val_0_rmse: 0.59978 | val_1_rmse: 0.61408 |  0:00:04s
epoch 4  | loss: 0.38745 | val_0_rmse: 0.58955 | val_1_rmse: 0.59991 |  0:00:05s
epoch 5  | loss: 0.36831 | val_0_rmse: 0.58891 | val_1_rmse: 0.60051 |  0:00:06s
epoch 6  | loss: 0.36378 | val_0_rmse: 0.64448 | val_1_rmse: 0.65687 |  0:00:07s
epoch 7  | loss: 0.37234 | val_0_rmse: 0.64146 | val_1_rmse: 0.6541  |  0:00:08s
epoch 8  | loss: 0.37182 | val_0_rmse: 0.57772 | val_1_rmse: 0.58671 |  0:00:10s
epoch 9  | loss: 0.36666 | val_0_rmse: 0.58986 | val_1_rmse: 0.59997 |  0:00:11s
epoch 10 | loss: 0.35579 | val_0_rmse: 0.57651 | val_1_rmse: 0.58291 |  0:00:12s
epoch 11 | loss: 0.34952 | val_0_rmse: 0.56526 | val_1_rmse: 0.57794 |  0:00:13s
epoch 12 | loss: 0.34536 | val_0_rmse: 0.56282 | val_1_rmse: 0.57343 |  0:00:14s
epoch 13 | loss: 0.35098 | val_0_rmse: 0.58472 | val_1_rmse: 0.595   |  0:00:15s
epoch 14 | loss: 0.34219 | val_0_rmse: 0.58203 | val_1_rmse: 0.59728 |  0:00:16s
epoch 15 | loss: 0.34099 | val_0_rmse: 0.56425 | val_1_rmse: 0.58191 |  0:00:17s
epoch 16 | loss: 0.33349 | val_0_rmse: 0.55782 | val_1_rmse: 0.57087 |  0:00:18s
epoch 17 | loss: 0.33655 | val_0_rmse: 0.56175 | val_1_rmse: 0.57896 |  0:00:20s
epoch 18 | loss: 0.34021 | val_0_rmse: 0.55964 | val_1_rmse: 0.57612 |  0:00:21s
epoch 19 | loss: 0.33196 | val_0_rmse: 0.55838 | val_1_rmse: 0.57263 |  0:00:22s
epoch 20 | loss: 0.33822 | val_0_rmse: 0.56995 | val_1_rmse: 0.58427 |  0:00:23s
epoch 21 | loss: 0.32664 | val_0_rmse: 0.56036 | val_1_rmse: 0.57596 |  0:00:24s
epoch 22 | loss: 0.32744 | val_0_rmse: 0.54851 | val_1_rmse: 0.56888 |  0:00:25s
epoch 23 | loss: 0.33625 | val_0_rmse: 0.55661 | val_1_rmse: 0.57218 |  0:00:26s
epoch 24 | loss: 0.32361 | val_0_rmse: 0.55455 | val_1_rmse: 0.57095 |  0:00:27s
epoch 25 | loss: 0.31919 | val_0_rmse: 0.53996 | val_1_rmse: 0.55804 |  0:00:28s
epoch 26 | loss: 0.3163  | val_0_rmse: 0.555   | val_1_rmse: 0.57357 |  0:00:30s
epoch 27 | loss: 0.32566 | val_0_rmse: 0.55038 | val_1_rmse: 0.57046 |  0:00:31s
epoch 28 | loss: 0.31979 | val_0_rmse: 0.54328 | val_1_rmse: 0.56475 |  0:00:32s
epoch 29 | loss: 0.31992 | val_0_rmse: 0.54662 | val_1_rmse: 0.55683 |  0:00:33s
epoch 30 | loss: 0.31895 | val_0_rmse: 0.53911 | val_1_rmse: 0.55881 |  0:00:34s
epoch 31 | loss: 0.31208 | val_0_rmse: 0.53797 | val_1_rmse: 0.55943 |  0:00:35s
epoch 32 | loss: 0.31732 | val_0_rmse: 0.54865 | val_1_rmse: 0.56603 |  0:00:36s
epoch 33 | loss: 0.32016 | val_0_rmse: 0.54716 | val_1_rmse: 0.56323 |  0:00:37s
epoch 34 | loss: 0.31353 | val_0_rmse: 0.53208 | val_1_rmse: 0.55411 |  0:00:38s
epoch 35 | loss: 0.31555 | val_0_rmse: 0.55304 | val_1_rmse: 0.57463 |  0:00:40s
epoch 36 | loss: 0.31919 | val_0_rmse: 0.5454  | val_1_rmse: 0.56401 |  0:00:41s
epoch 37 | loss: 0.319   | val_0_rmse: 0.54541 | val_1_rmse: 0.55957 |  0:00:42s
epoch 38 | loss: 0.31594 | val_0_rmse: 0.53496 | val_1_rmse: 0.55678 |  0:00:43s
epoch 39 | loss: 0.30651 | val_0_rmse: 0.52998 | val_1_rmse: 0.54477 |  0:00:44s
epoch 40 | loss: 0.30829 | val_0_rmse: 0.53796 | val_1_rmse: 0.55641 |  0:00:45s
epoch 41 | loss: 0.31566 | val_0_rmse: 0.52515 | val_1_rmse: 0.54538 |  0:00:46s
epoch 42 | loss: 0.30675 | val_0_rmse: 0.54659 | val_1_rmse: 0.56313 |  0:00:47s
epoch 43 | loss: 0.30743 | val_0_rmse: 0.54917 | val_1_rmse: 0.56447 |  0:00:48s
epoch 44 | loss: 0.30191 | val_0_rmse: 0.52673 | val_1_rmse: 0.549   |  0:00:50s
epoch 45 | loss: 0.31456 | val_0_rmse: 0.55256 | val_1_rmse: 0.57598 |  0:00:51s
epoch 46 | loss: 0.31739 | val_0_rmse: 0.53164 | val_1_rmse: 0.55195 |  0:00:52s
epoch 47 | loss: 0.30867 | val_0_rmse: 0.53034 | val_1_rmse: 0.54859 |  0:00:53s
epoch 48 | loss: 0.30405 | val_0_rmse: 0.53628 | val_1_rmse: 0.55398 |  0:00:54s
epoch 49 | loss: 0.30182 | val_0_rmse: 0.5272  | val_1_rmse: 0.54462 |  0:00:55s
epoch 50 | loss: 0.2984  | val_0_rmse: 0.54063 | val_1_rmse: 0.55736 |  0:00:56s
epoch 51 | loss: 0.30361 | val_0_rmse: 0.52882 | val_1_rmse: 0.55095 |  0:00:57s
epoch 52 | loss: 0.30541 | val_0_rmse: 0.51914 | val_1_rmse: 0.53977 |  0:00:58s
epoch 53 | loss: 0.30953 | val_0_rmse: 0.53956 | val_1_rmse: 0.56177 |  0:01:00s
epoch 54 | loss: 0.31075 | val_0_rmse: 0.53834 | val_1_rmse: 0.55944 |  0:01:01s
epoch 55 | loss: 0.31556 | val_0_rmse: 0.51588 | val_1_rmse: 0.53715 |  0:01:02s
epoch 56 | loss: 0.29683 | val_0_rmse: 0.53511 | val_1_rmse: 0.55568 |  0:01:03s
epoch 57 | loss: 0.29978 | val_0_rmse: 0.52624 | val_1_rmse: 0.54629 |  0:01:04s
epoch 58 | loss: 0.29585 | val_0_rmse: 0.51033 | val_1_rmse: 0.53686 |  0:01:05s
epoch 59 | loss: 0.29582 | val_0_rmse: 0.52176 | val_1_rmse: 0.54259 |  0:01:06s
epoch 60 | loss: 0.30175 | val_0_rmse: 0.5217  | val_1_rmse: 0.545   |  0:01:07s
epoch 61 | loss: 0.29423 | val_0_rmse: 0.52207 | val_1_rmse: 0.54078 |  0:01:08s
epoch 62 | loss: 0.29337 | val_0_rmse: 0.52703 | val_1_rmse: 0.55091 |  0:01:10s
epoch 63 | loss: 0.29575 | val_0_rmse: 0.51802 | val_1_rmse: 0.54274 |  0:01:11s
epoch 64 | loss: 0.29455 | val_0_rmse: 0.51437 | val_1_rmse: 0.53717 |  0:01:12s
epoch 65 | loss: 0.28626 | val_0_rmse: 0.5084  | val_1_rmse: 0.52832 |  0:01:13s
epoch 66 | loss: 0.29825 | val_0_rmse: 0.51862 | val_1_rmse: 0.54138 |  0:01:14s
epoch 67 | loss: 0.29885 | val_0_rmse: 0.52063 | val_1_rmse: 0.54601 |  0:01:15s
epoch 68 | loss: 0.30292 | val_0_rmse: 0.56158 | val_1_rmse: 0.57809 |  0:01:16s
epoch 69 | loss: 0.32003 | val_0_rmse: 0.54045 | val_1_rmse: 0.56002 |  0:01:17s
epoch 70 | loss: 0.3164  | val_0_rmse: 0.53007 | val_1_rmse: 0.54898 |  0:01:19s
epoch 71 | loss: 0.30751 | val_0_rmse: 0.53509 | val_1_rmse: 0.55634 |  0:01:20s
epoch 72 | loss: 0.31832 | val_0_rmse: 0.5456  | val_1_rmse: 0.56776 |  0:01:21s
epoch 73 | loss: 0.31279 | val_0_rmse: 0.53768 | val_1_rmse: 0.55314 |  0:01:22s
epoch 74 | loss: 0.30454 | val_0_rmse: 0.52957 | val_1_rmse: 0.55214 |  0:01:23s
epoch 75 | loss: 0.29528 | val_0_rmse: 0.5156  | val_1_rmse: 0.53988 |  0:01:24s
epoch 76 | loss: 0.29212 | val_0_rmse: 0.54168 | val_1_rmse: 0.56649 |  0:01:25s
epoch 77 | loss: 0.29332 | val_0_rmse: 0.51604 | val_1_rmse: 0.54164 |  0:01:26s
epoch 78 | loss: 0.29276 | val_0_rmse: 0.51583 | val_1_rmse: 0.53617 |  0:01:27s
epoch 79 | loss: 0.28979 | val_0_rmse: 0.506   | val_1_rmse: 0.53228 |  0:01:28s
epoch 80 | loss: 0.28359 | val_0_rmse: 0.51791 | val_1_rmse: 0.54002 |  0:01:30s
epoch 81 | loss: 0.29179 | val_0_rmse: 0.51524 | val_1_rmse: 0.54107 |  0:01:31s
epoch 82 | loss: 0.28243 | val_0_rmse: 0.51185 | val_1_rmse: 0.53586 |  0:01:32s
epoch 83 | loss: 0.28588 | val_0_rmse: 0.51679 | val_1_rmse: 0.54249 |  0:01:33s
epoch 84 | loss: 0.28979 | val_0_rmse: 0.53073 | val_1_rmse: 0.55632 |  0:01:34s
epoch 85 | loss: 0.29688 | val_0_rmse: 0.51336 | val_1_rmse: 0.53849 |  0:01:35s
epoch 86 | loss: 0.29167 | val_0_rmse: 0.53762 | val_1_rmse: 0.55896 |  0:01:36s
epoch 87 | loss: 0.29818 | val_0_rmse: 0.51003 | val_1_rmse: 0.53269 |  0:01:37s
epoch 88 | loss: 0.28547 | val_0_rmse: 0.50701 | val_1_rmse: 0.52978 |  0:01:38s
epoch 89 | loss: 0.28083 | val_0_rmse: 0.50718 | val_1_rmse: 0.53283 |  0:01:39s
epoch 90 | loss: 0.29065 | val_0_rmse: 0.50862 | val_1_rmse: 0.53312 |  0:01:41s
epoch 91 | loss: 0.29396 | val_0_rmse: 0.52946 | val_1_rmse: 0.54454 |  0:01:42s
epoch 92 | loss: 0.29922 | val_0_rmse: 0.53739 | val_1_rmse: 0.56496 |  0:01:43s
epoch 93 | loss: 0.29074 | val_0_rmse: 0.51049 | val_1_rmse: 0.53048 |  0:01:44s
epoch 94 | loss: 0.28933 | val_0_rmse: 0.51595 | val_1_rmse: 0.53803 |  0:01:45s
epoch 95 | loss: 0.28489 | val_0_rmse: 0.51011 | val_1_rmse: 0.53336 |  0:01:46s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.52832
Best weights from best epoch are automatically used!
ended training at: 09:36:58
Feature importance:
[('Area', 0.24330391119129524), ('Baths', 0.06352163902721591), ('Beds', 0.011398565884651216), ('Latitude', 0.2989525458850161), ('Longitude', 0.27095284217324894), ('Month', 0.0), ('Year', 0.11187049583857256)]
Mean squared error is of 6117562832.289822
Mean absolute error:54432.12714395305
MAPE:0.1840704620779189
R2 score:0.7258389458272179
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:36:58
epoch 0  | loss: 0.81819 | val_0_rmse: 0.84363 | val_1_rmse: 0.85163 |  0:00:01s
epoch 1  | loss: 0.55093 | val_0_rmse: 0.73979 | val_1_rmse: 0.74504 |  0:00:02s
epoch 2  | loss: 0.51226 | val_0_rmse: 0.68594 | val_1_rmse: 0.68795 |  0:00:03s
epoch 3  | loss: 0.47153 | val_0_rmse: 0.65476 | val_1_rmse: 0.65384 |  0:00:04s
epoch 4  | loss: 0.43525 | val_0_rmse: 0.63836 | val_1_rmse: 0.63548 |  0:00:05s
epoch 5  | loss: 0.4273  | val_0_rmse: 0.63144 | val_1_rmse: 0.63034 |  0:00:06s
epoch 6  | loss: 0.42342 | val_0_rmse: 0.62657 | val_1_rmse: 0.61946 |  0:00:07s
epoch 7  | loss: 0.41198 | val_0_rmse: 0.61429 | val_1_rmse: 0.61314 |  0:00:08s
epoch 8  | loss: 0.39156 | val_0_rmse: 0.62017 | val_1_rmse: 0.61454 |  0:00:10s
epoch 9  | loss: 0.40015 | val_0_rmse: 0.6056  | val_1_rmse: 0.59918 |  0:00:11s
epoch 10 | loss: 0.39601 | val_0_rmse: 0.61802 | val_1_rmse: 0.60606 |  0:00:12s
epoch 11 | loss: 0.37939 | val_0_rmse: 0.59187 | val_1_rmse: 0.58722 |  0:00:13s
epoch 12 | loss: 0.37261 | val_0_rmse: 0.5917  | val_1_rmse: 0.57985 |  0:00:14s
epoch 13 | loss: 0.36935 | val_0_rmse: 0.58273 | val_1_rmse: 0.57726 |  0:00:15s
epoch 14 | loss: 0.36547 | val_0_rmse: 0.59199 | val_1_rmse: 0.58422 |  0:00:16s
epoch 15 | loss: 0.36255 | val_0_rmse: 0.58022 | val_1_rmse: 0.57205 |  0:00:17s
epoch 16 | loss: 0.36087 | val_0_rmse: 0.5738  | val_1_rmse: 0.56819 |  0:00:18s
epoch 17 | loss: 0.35477 | val_0_rmse: 0.57227 | val_1_rmse: 0.56831 |  0:00:20s
epoch 18 | loss: 0.35725 | val_0_rmse: 0.57967 | val_1_rmse: 0.57079 |  0:00:21s
epoch 19 | loss: 0.36131 | val_0_rmse: 0.56722 | val_1_rmse: 0.55409 |  0:00:22s
epoch 20 | loss: 0.34833 | val_0_rmse: 0.56896 | val_1_rmse: 0.5518  |  0:00:23s
epoch 21 | loss: 0.34366 | val_0_rmse: 0.5601  | val_1_rmse: 0.54771 |  0:00:24s
epoch 22 | loss: 0.34586 | val_0_rmse: 0.55671 | val_1_rmse: 0.54504 |  0:00:25s
epoch 23 | loss: 0.34708 | val_0_rmse: 0.56036 | val_1_rmse: 0.55384 |  0:00:26s
epoch 24 | loss: 0.33929 | val_0_rmse: 0.55705 | val_1_rmse: 0.54831 |  0:00:27s
epoch 25 | loss: 0.33639 | val_0_rmse: 0.5552  | val_1_rmse: 0.54166 |  0:00:28s
epoch 26 | loss: 0.33483 | val_0_rmse: 0.56431 | val_1_rmse: 0.55509 |  0:00:30s
epoch 27 | loss: 0.34012 | val_0_rmse: 0.55201 | val_1_rmse: 0.54243 |  0:00:31s
epoch 28 | loss: 0.33181 | val_0_rmse: 0.56352 | val_1_rmse: 0.54789 |  0:00:32s
epoch 29 | loss: 0.33996 | val_0_rmse: 0.55399 | val_1_rmse: 0.54646 |  0:00:33s
epoch 30 | loss: 0.33025 | val_0_rmse: 0.54969 | val_1_rmse: 0.53555 |  0:00:34s
epoch 31 | loss: 0.32109 | val_0_rmse: 0.55148 | val_1_rmse: 0.54452 |  0:00:35s
epoch 32 | loss: 0.32694 | val_0_rmse: 0.62006 | val_1_rmse: 0.61199 |  0:00:36s
epoch 33 | loss: 0.33452 | val_0_rmse: 0.54719 | val_1_rmse: 0.53583 |  0:00:37s
epoch 34 | loss: 0.32489 | val_0_rmse: 0.59402 | val_1_rmse: 0.58215 |  0:00:38s
epoch 35 | loss: 0.33211 | val_0_rmse: 0.58977 | val_1_rmse: 0.58543 |  0:00:40s
epoch 36 | loss: 0.33138 | val_0_rmse: 0.59662 | val_1_rmse: 0.59049 |  0:00:41s
epoch 37 | loss: 0.32657 | val_0_rmse: 0.54515 | val_1_rmse: 0.53494 |  0:00:42s
epoch 38 | loss: 0.32649 | val_0_rmse: 0.54231 | val_1_rmse: 0.53367 |  0:00:43s
epoch 39 | loss: 0.31154 | val_0_rmse: 0.57142 | val_1_rmse: 0.56052 |  0:00:44s
epoch 40 | loss: 0.31109 | val_0_rmse: 0.53597 | val_1_rmse: 0.52342 |  0:00:45s
epoch 41 | loss: 0.31667 | val_0_rmse: 0.5404  | val_1_rmse: 0.52556 |  0:00:46s
epoch 42 | loss: 0.3193  | val_0_rmse: 0.55374 | val_1_rmse: 0.54549 |  0:00:47s
epoch 43 | loss: 0.32365 | val_0_rmse: 0.59148 | val_1_rmse: 0.58116 |  0:00:48s
epoch 44 | loss: 0.31929 | val_0_rmse: 0.54579 | val_1_rmse: 0.53934 |  0:00:50s
epoch 45 | loss: 0.30837 | val_0_rmse: 0.53271 | val_1_rmse: 0.52419 |  0:00:51s
epoch 46 | loss: 0.30766 | val_0_rmse: 0.53922 | val_1_rmse: 0.52698 |  0:00:52s
epoch 47 | loss: 0.30662 | val_0_rmse: 0.53909 | val_1_rmse: 0.53292 |  0:00:53s
epoch 48 | loss: 0.31176 | val_0_rmse: 0.55759 | val_1_rmse: 0.54882 |  0:00:54s
epoch 49 | loss: 0.31238 | val_0_rmse: 0.55264 | val_1_rmse: 0.54639 |  0:00:55s
epoch 50 | loss: 0.32065 | val_0_rmse: 0.53894 | val_1_rmse: 0.53417 |  0:00:56s
epoch 51 | loss: 0.30726 | val_0_rmse: 0.54167 | val_1_rmse: 0.53713 |  0:00:57s
epoch 52 | loss: 0.30376 | val_0_rmse: 0.54814 | val_1_rmse: 0.5334  |  0:00:58s
epoch 53 | loss: 0.30631 | val_0_rmse: 0.53669 | val_1_rmse: 0.53308 |  0:00:59s
epoch 54 | loss: 0.31042 | val_0_rmse: 0.53732 | val_1_rmse: 0.52632 |  0:01:01s
epoch 55 | loss: 0.30448 | val_0_rmse: 0.56619 | val_1_rmse: 0.55669 |  0:01:02s
epoch 56 | loss: 0.31353 | val_0_rmse: 0.55674 | val_1_rmse: 0.54724 |  0:01:03s
epoch 57 | loss: 0.31058 | val_0_rmse: 0.52266 | val_1_rmse: 0.5084  |  0:01:04s
epoch 58 | loss: 0.30601 | val_0_rmse: 0.53265 | val_1_rmse: 0.52679 |  0:01:05s
epoch 59 | loss: 0.30559 | val_0_rmse: 0.52463 | val_1_rmse: 0.51534 |  0:01:06s
epoch 60 | loss: 0.30544 | val_0_rmse: 0.53867 | val_1_rmse: 0.53314 |  0:01:07s
epoch 61 | loss: 0.31104 | val_0_rmse: 0.54156 | val_1_rmse: 0.53389 |  0:01:08s
epoch 62 | loss: 0.30662 | val_0_rmse: 0.52362 | val_1_rmse: 0.52044 |  0:01:09s
epoch 63 | loss: 0.29887 | val_0_rmse: 0.55454 | val_1_rmse: 0.55596 |  0:01:10s
epoch 64 | loss: 0.30222 | val_0_rmse: 0.5289  | val_1_rmse: 0.52535 |  0:01:12s
epoch 65 | loss: 0.29721 | val_0_rmse: 0.58697 | val_1_rmse: 0.58591 |  0:01:13s
epoch 66 | loss: 0.29754 | val_0_rmse: 0.52868 | val_1_rmse: 0.52084 |  0:01:14s
epoch 67 | loss: 0.30864 | val_0_rmse: 0.54459 | val_1_rmse: 0.54178 |  0:01:15s
epoch 68 | loss: 0.30551 | val_0_rmse: 0.5772  | val_1_rmse: 0.58033 |  0:01:16s
epoch 69 | loss: 0.30115 | val_0_rmse: 0.51513 | val_1_rmse: 0.50825 |  0:01:17s
epoch 70 | loss: 0.29865 | val_0_rmse: 0.51509 | val_1_rmse: 0.51027 |  0:01:18s
epoch 71 | loss: 0.29242 | val_0_rmse: 0.52787 | val_1_rmse: 0.52769 |  0:01:19s
epoch 72 | loss: 0.29541 | val_0_rmse: 0.52506 | val_1_rmse: 0.52097 |  0:01:20s
epoch 73 | loss: 0.29975 | val_0_rmse: 0.52693 | val_1_rmse: 0.52516 |  0:01:22s
epoch 74 | loss: 0.29147 | val_0_rmse: 0.54665 | val_1_rmse: 0.54483 |  0:01:23s
epoch 75 | loss: 0.29867 | val_0_rmse: 0.53689 | val_1_rmse: 0.53791 |  0:01:24s
epoch 76 | loss: 0.28687 | val_0_rmse: 0.51335 | val_1_rmse: 0.51469 |  0:01:25s
epoch 77 | loss: 0.28763 | val_0_rmse: 0.53949 | val_1_rmse: 0.5353  |  0:01:26s
epoch 78 | loss: 0.28961 | val_0_rmse: 0.51221 | val_1_rmse: 0.5078  |  0:01:27s
epoch 79 | loss: 0.29673 | val_0_rmse: 0.53964 | val_1_rmse: 0.54074 |  0:01:28s
epoch 80 | loss: 0.28779 | val_0_rmse: 0.51844 | val_1_rmse: 0.51722 |  0:01:29s
epoch 81 | loss: 0.30073 | val_0_rmse: 0.52648 | val_1_rmse: 0.52237 |  0:01:30s
epoch 82 | loss: 0.29101 | val_0_rmse: 0.51239 | val_1_rmse: 0.51316 |  0:01:32s
epoch 83 | loss: 0.29403 | val_0_rmse: 0.53054 | val_1_rmse: 0.53337 |  0:01:33s
epoch 84 | loss: 0.28751 | val_0_rmse: 0.55023 | val_1_rmse: 0.55398 |  0:01:34s
epoch 85 | loss: 0.29059 | val_0_rmse: 0.51437 | val_1_rmse: 0.5178  |  0:01:35s
epoch 86 | loss: 0.28784 | val_0_rmse: 0.51797 | val_1_rmse: 0.51122 |  0:01:36s
epoch 87 | loss: 0.28484 | val_0_rmse: 0.52096 | val_1_rmse: 0.51963 |  0:01:37s
epoch 88 | loss: 0.2862  | val_0_rmse: 0.51203 | val_1_rmse: 0.50703 |  0:01:38s
epoch 89 | loss: 0.29485 | val_0_rmse: 0.51464 | val_1_rmse: 0.51932 |  0:01:39s
epoch 90 | loss: 0.28598 | val_0_rmse: 0.56188 | val_1_rmse: 0.55944 |  0:01:40s
epoch 91 | loss: 0.28668 | val_0_rmse: 0.53465 | val_1_rmse: 0.53248 |  0:01:41s
epoch 92 | loss: 0.27964 | val_0_rmse: 0.52581 | val_1_rmse: 0.52799 |  0:01:43s
epoch 93 | loss: 0.2823  | val_0_rmse: 0.51736 | val_1_rmse: 0.5202  |  0:01:44s
epoch 94 | loss: 0.28275 | val_0_rmse: 0.51268 | val_1_rmse: 0.51824 |  0:01:45s
epoch 95 | loss: 0.28315 | val_0_rmse: 0.54048 | val_1_rmse: 0.5444  |  0:01:46s
epoch 96 | loss: 0.29743 | val_0_rmse: 0.56511 | val_1_rmse: 0.56975 |  0:01:47s
epoch 97 | loss: 0.28626 | val_0_rmse: 0.50358 | val_1_rmse: 0.50013 |  0:01:48s
epoch 98 | loss: 0.27468 | val_0_rmse: 0.50829 | val_1_rmse: 0.5042  |  0:01:49s
epoch 99 | loss: 0.282   | val_0_rmse: 0.49689 | val_1_rmse: 0.49329 |  0:01:50s
epoch 100| loss: 0.28038 | val_0_rmse: 0.50567 | val_1_rmse: 0.50547 |  0:01:51s
epoch 101| loss: 0.27414 | val_0_rmse: 0.51231 | val_1_rmse: 0.52088 |  0:01:53s
epoch 102| loss: 0.2805  | val_0_rmse: 0.52453 | val_1_rmse: 0.53144 |  0:01:54s
epoch 103| loss: 0.28114 | val_0_rmse: 0.5359  | val_1_rmse: 0.53577 |  0:01:55s
epoch 104| loss: 0.2851  | val_0_rmse: 0.52597 | val_1_rmse: 0.52561 |  0:01:56s
epoch 105| loss: 0.27986 | val_0_rmse: 0.54141 | val_1_rmse: 0.5395  |  0:01:57s
epoch 106| loss: 0.2749  | val_0_rmse: 0.53864 | val_1_rmse: 0.54338 |  0:01:58s
epoch 107| loss: 0.29145 | val_0_rmse: 0.54283 | val_1_rmse: 0.54392 |  0:01:59s
epoch 108| loss: 0.28423 | val_0_rmse: 0.51156 | val_1_rmse: 0.51006 |  0:02:00s
epoch 109| loss: 0.27399 | val_0_rmse: 0.49852 | val_1_rmse: 0.5013  |  0:02:01s
epoch 110| loss: 0.27474 | val_0_rmse: 0.51249 | val_1_rmse: 0.51758 |  0:02:03s
epoch 111| loss: 0.28039 | val_0_rmse: 0.51347 | val_1_rmse: 0.51488 |  0:02:04s
epoch 112| loss: 0.27975 | val_0_rmse: 0.52796 | val_1_rmse: 0.52988 |  0:02:05s
epoch 113| loss: 0.27493 | val_0_rmse: 0.50863 | val_1_rmse: 0.51111 |  0:02:06s
epoch 114| loss: 0.27154 | val_0_rmse: 0.49569 | val_1_rmse: 0.50027 |  0:02:07s
epoch 115| loss: 0.27443 | val_0_rmse: 0.52847 | val_1_rmse: 0.52696 |  0:02:08s
epoch 116| loss: 0.26981 | val_0_rmse: 0.49635 | val_1_rmse: 0.50285 |  0:02:09s
epoch 117| loss: 0.27858 | val_0_rmse: 0.5131  | val_1_rmse: 0.51582 |  0:02:10s
epoch 118| loss: 0.28955 | val_0_rmse: 0.52475 | val_1_rmse: 0.53127 |  0:02:11s
epoch 119| loss: 0.2755  | val_0_rmse: 0.49803 | val_1_rmse: 0.50029 |  0:02:12s
epoch 120| loss: 0.27775 | val_0_rmse: 0.5112  | val_1_rmse: 0.51204 |  0:02:14s
epoch 121| loss: 0.27903 | val_0_rmse: 0.50351 | val_1_rmse: 0.51059 |  0:02:15s
epoch 122| loss: 0.28334 | val_0_rmse: 0.54327 | val_1_rmse: 0.5432  |  0:02:16s
epoch 123| loss: 0.2786  | val_0_rmse: 0.5791  | val_1_rmse: 0.57765 |  0:02:17s
epoch 124| loss: 0.27761 | val_0_rmse: 0.50763 | val_1_rmse: 0.51126 |  0:02:18s
epoch 125| loss: 0.2733  | val_0_rmse: 0.51576 | val_1_rmse: 0.51857 |  0:02:19s
epoch 126| loss: 0.27646 | val_0_rmse: 0.49314 | val_1_rmse: 0.49923 |  0:02:20s
epoch 127| loss: 0.2742  | val_0_rmse: 0.51574 | val_1_rmse: 0.52175 |  0:02:21s
epoch 128| loss: 0.27921 | val_0_rmse: 0.50701 | val_1_rmse: 0.51336 |  0:02:22s
epoch 129| loss: 0.2852  | val_0_rmse: 0.55671 | val_1_rmse: 0.56694 |  0:02:24s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.49329
Best weights from best epoch are automatically used!
ended training at: 09:39:23
Feature importance:
[('Area', 0.3188404721854017), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.31277641963120667), ('Longitude', 0.22656876154704156), ('Month', 0.038174125235106916), ('Year', 0.10364022140124315)]
Mean squared error is of 5962393400.6102495
Mean absolute error:53252.301857484854
MAPE:0.17895339650314931
R2 score:0.733220977347079
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:39:23
epoch 0  | loss: 0.60339 | val_0_rmse: 0.72868 | val_1_rmse: 0.72539 |  0:00:04s
epoch 1  | loss: 0.46748 | val_0_rmse: 0.70503 | val_1_rmse: 0.70235 |  0:00:08s
epoch 2  | loss: 0.4158  | val_0_rmse: 0.64537 | val_1_rmse: 0.64589 |  0:00:12s
epoch 3  | loss: 0.39469 | val_0_rmse: 0.61197 | val_1_rmse: 0.61479 |  0:00:16s
epoch 4  | loss: 0.38186 | val_0_rmse: 0.71268 | val_1_rmse: 0.71463 |  0:00:20s
epoch 5  | loss: 0.3821  | val_0_rmse: 0.60546 | val_1_rmse: 0.61066 |  0:00:25s
epoch 6  | loss: 0.37577 | val_0_rmse: 0.6783  | val_1_rmse: 0.68444 |  0:00:29s
epoch 7  | loss: 0.37325 | val_0_rmse: 0.63771 | val_1_rmse: 0.64112 |  0:00:33s
epoch 8  | loss: 0.37567 | val_0_rmse: 0.77595 | val_1_rmse: 0.77689 |  0:00:37s
epoch 9  | loss: 0.36935 | val_0_rmse: 0.66364 | val_1_rmse: 0.66661 |  0:00:41s
epoch 10 | loss: 0.37093 | val_0_rmse: 0.69019 | val_1_rmse: 0.69284 |  0:00:45s
epoch 11 | loss: 0.37491 | val_0_rmse: 0.59381 | val_1_rmse: 0.59606 |  0:00:50s
epoch 12 | loss: 0.36999 | val_0_rmse: 0.62793 | val_1_rmse: 0.63027 |  0:00:54s
epoch 13 | loss: 0.36497 | val_0_rmse: 0.70971 | val_1_rmse: 0.71319 |  0:00:58s
epoch 14 | loss: 0.36769 | val_0_rmse: 0.60507 | val_1_rmse: 0.60601 |  0:01:02s
epoch 15 | loss: 0.36815 | val_0_rmse: 0.66682 | val_1_rmse: 0.67092 |  0:01:06s
epoch 16 | loss: 0.36723 | val_0_rmse: 0.62694 | val_1_rmse: 0.62804 |  0:01:11s
epoch 17 | loss: 0.36559 | val_0_rmse: 0.62004 | val_1_rmse: 0.62239 |  0:01:15s
epoch 18 | loss: 0.36563 | val_0_rmse: 0.6885  | val_1_rmse: 0.69061 |  0:01:19s
epoch 19 | loss: 0.36487 | val_0_rmse: 0.64157 | val_1_rmse: 0.64256 |  0:01:23s
epoch 20 | loss: 0.36316 | val_0_rmse: 0.63387 | val_1_rmse: 0.63466 |  0:01:27s
epoch 21 | loss: 0.36087 | val_0_rmse: 0.60043 | val_1_rmse: 0.60241 |  0:01:32s
epoch 22 | loss: 0.3671  | val_0_rmse: 0.69085 | val_1_rmse: 0.69524 |  0:01:36s
epoch 23 | loss: 0.36445 | val_0_rmse: 0.69853 | val_1_rmse: 0.69726 |  0:01:40s
epoch 24 | loss: 0.36701 | val_0_rmse: 0.60247 | val_1_rmse: 0.60384 |  0:01:44s
epoch 25 | loss: 0.36352 | val_0_rmse: 0.60981 | val_1_rmse: 0.61205 |  0:01:48s
epoch 26 | loss: 0.36    | val_0_rmse: 0.67393 | val_1_rmse: 0.67785 |  0:01:52s
epoch 27 | loss: 0.36024 | val_0_rmse: 0.73296 | val_1_rmse: 0.73692 |  0:01:57s
epoch 28 | loss: 0.36555 | val_0_rmse: 0.76548 | val_1_rmse: 0.76485 |  0:02:01s
epoch 29 | loss: 0.36258 | val_0_rmse: 0.68295 | val_1_rmse: 0.68593 |  0:02:05s
epoch 30 | loss: 0.363   | val_0_rmse: 0.66256 | val_1_rmse: 0.66319 |  0:02:09s
epoch 31 | loss: 0.36212 | val_0_rmse: 0.69706 | val_1_rmse: 0.70092 |  0:02:13s
epoch 32 | loss: 0.36248 | val_0_rmse: 0.6982  | val_1_rmse: 0.70137 |  0:02:18s
epoch 33 | loss: 0.36154 | val_0_rmse: 0.61612 | val_1_rmse: 0.61726 |  0:02:22s
epoch 34 | loss: 0.3613  | val_0_rmse: 0.60162 | val_1_rmse: 0.60419 |  0:02:26s
epoch 35 | loss: 0.3608  | val_0_rmse: 0.62117 | val_1_rmse: 0.62377 |  0:02:30s
epoch 36 | loss: 0.36296 | val_0_rmse: 0.62231 | val_1_rmse: 0.62557 |  0:02:34s
epoch 37 | loss: 0.35458 | val_0_rmse: 0.60699 | val_1_rmse: 0.61014 |  0:02:39s
epoch 38 | loss: 0.35588 | val_0_rmse: 0.69643 | val_1_rmse: 0.69435 |  0:02:43s
epoch 39 | loss: 0.35963 | val_0_rmse: 0.71089 | val_1_rmse: 0.71375 |  0:02:47s
epoch 40 | loss: 0.35989 | val_0_rmse: 0.59202 | val_1_rmse: 0.59235 |  0:02:51s
epoch 41 | loss: 0.35913 | val_0_rmse: 0.66264 | val_1_rmse: 0.66471 |  0:02:55s
epoch 42 | loss: 0.36298 | val_0_rmse: 0.61129 | val_1_rmse: 0.61083 |  0:02:59s
epoch 43 | loss: 0.35583 | val_0_rmse: 0.59295 | val_1_rmse: 0.59429 |  0:03:04s
epoch 44 | loss: 0.35678 | val_0_rmse: 0.6645  | val_1_rmse: 0.66647 |  0:03:08s
epoch 45 | loss: 0.34721 | val_0_rmse: 0.58313 | val_1_rmse: 0.58298 |  0:03:12s
epoch 46 | loss: 0.35201 | val_0_rmse: 0.70775 | val_1_rmse: 0.71502 |  0:03:16s
epoch 47 | loss: 0.36023 | val_0_rmse: 0.59172 | val_1_rmse: 0.59338 |  0:03:20s
epoch 48 | loss: 0.36135 | val_0_rmse: 0.64421 | val_1_rmse: 0.64277 |  0:03:25s
epoch 49 | loss: 0.35273 | val_0_rmse: 0.6188  | val_1_rmse: 0.62074 |  0:03:29s
epoch 50 | loss: 0.35004 | val_0_rmse: 0.6803  | val_1_rmse: 0.68001 |  0:03:33s
epoch 51 | loss: 0.34649 | val_0_rmse: 0.58842 | val_1_rmse: 0.58677 |  0:03:37s
epoch 52 | loss: 0.35205 | val_0_rmse: 0.58467 | val_1_rmse: 0.58784 |  0:03:41s
epoch 53 | loss: 0.34989 | val_0_rmse: 0.61146 | val_1_rmse: 0.61246 |  0:03:45s
epoch 54 | loss: 0.34438 | val_0_rmse: 0.61254 | val_1_rmse: 0.61321 |  0:03:50s
epoch 55 | loss: 0.34965 | val_0_rmse: 0.67375 | val_1_rmse: 0.67612 |  0:03:54s
epoch 56 | loss: 0.35534 | val_0_rmse: 0.67503 | val_1_rmse: 0.67781 |  0:03:58s
epoch 57 | loss: 0.35012 | val_0_rmse: 0.62414 | val_1_rmse: 0.62571 |  0:04:02s
epoch 58 | loss: 0.35106 | val_0_rmse: 0.64808 | val_1_rmse: 0.64844 |  0:04:06s
epoch 59 | loss: 0.34821 | val_0_rmse: 0.68457 | val_1_rmse: 0.68766 |  0:04:11s
epoch 60 | loss: 0.35114 | val_0_rmse: 0.62406 | val_1_rmse: 0.62535 |  0:04:15s
epoch 61 | loss: 0.3484  | val_0_rmse: 0.58346 | val_1_rmse: 0.58316 |  0:04:19s
epoch 62 | loss: 0.34592 | val_0_rmse: 0.63855 | val_1_rmse: 0.63951 |  0:04:23s
epoch 63 | loss: 0.34388 | val_0_rmse: 0.61443 | val_1_rmse: 0.61565 |  0:04:27s
epoch 64 | loss: 0.34483 | val_0_rmse: 0.67749 | val_1_rmse: 0.67886 |  0:04:32s
epoch 65 | loss: 0.34575 | val_0_rmse: 0.62458 | val_1_rmse: 0.62483 |  0:04:36s
epoch 66 | loss: 0.34481 | val_0_rmse: 0.68095 | val_1_rmse: 0.6809  |  0:04:40s
epoch 67 | loss: 0.34458 | val_0_rmse: 0.69147 | val_1_rmse: 0.6938  |  0:04:44s
epoch 68 | loss: 0.34036 | val_0_rmse: 0.65336 | val_1_rmse: 0.65439 |  0:04:48s
epoch 69 | loss: 0.34732 | val_0_rmse: 0.68263 | val_1_rmse: 0.68369 |  0:04:52s
epoch 70 | loss: 0.34397 | val_0_rmse: 0.64004 | val_1_rmse: 0.64711 |  0:04:57s
epoch 71 | loss: 0.33971 | val_0_rmse: 0.60518 | val_1_rmse: 0.60544 |  0:05:01s
epoch 72 | loss: 0.34025 | val_0_rmse: 0.67193 | val_1_rmse: 0.67347 |  0:05:05s
epoch 73 | loss: 0.34447 | val_0_rmse: 0.60108 | val_1_rmse: 0.59837 |  0:05:09s
epoch 74 | loss: 0.33884 | val_0_rmse: 0.60831 | val_1_rmse: 0.60874 |  0:05:13s
epoch 75 | loss: 0.34223 | val_0_rmse: 0.63923 | val_1_rmse: 0.64249 |  0:05:17s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.58298
Best weights from best epoch are automatically used!
ended training at: 09:44:42
Feature importance:
[('Area', 0.286307499385818), ('Baths', 0.15202298405649642), ('Beds', 0.154080213110839), ('Latitude', 0.17720531876606302), ('Longitude', 0.23038398468078355), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2297185001.470137
Mean absolute error:35607.6507787718
MAPE:0.35391657914892716
R2 score:0.664089938183517
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:44:43
epoch 0  | loss: 0.60694 | val_0_rmse: 0.73187 | val_1_rmse: 0.73622 |  0:00:04s
epoch 1  | loss: 0.51408 | val_0_rmse: 0.78297 | val_1_rmse: 0.78551 |  0:00:08s
epoch 2  | loss: 0.48854 | val_0_rmse: 0.71648 | val_1_rmse: 0.72025 |  0:00:12s
epoch 3  | loss: 0.45874 | val_0_rmse: 0.70732 | val_1_rmse: 0.71695 |  0:00:16s
epoch 4  | loss: 0.42652 | val_0_rmse: 0.77295 | val_1_rmse: 0.7788  |  0:00:21s
epoch 5  | loss: 0.42398 | val_0_rmse: 0.6819  | val_1_rmse: 0.68734 |  0:00:25s
epoch 6  | loss: 0.4157  | val_0_rmse: 0.77176 | val_1_rmse: 0.77856 |  0:00:29s
epoch 7  | loss: 0.41981 | val_0_rmse: 0.67724 | val_1_rmse: 0.68501 |  0:00:33s
epoch 8  | loss: 0.40852 | val_0_rmse: 0.64478 | val_1_rmse: 0.65568 |  0:00:37s
epoch 9  | loss: 0.39992 | val_0_rmse: 0.67234 | val_1_rmse: 0.67954 |  0:00:42s
epoch 10 | loss: 0.39929 | val_0_rmse: 0.66853 | val_1_rmse: 0.67477 |  0:00:46s
epoch 11 | loss: 0.39282 | val_0_rmse: 0.62848 | val_1_rmse: 0.63851 |  0:00:50s
epoch 12 | loss: 0.39034 | val_0_rmse: 0.62991 | val_1_rmse: 0.63862 |  0:00:54s
epoch 13 | loss: 0.38892 | val_0_rmse: 0.60824 | val_1_rmse: 0.61967 |  0:00:58s
epoch 14 | loss: 0.38979 | val_0_rmse: 0.64142 | val_1_rmse: 0.65191 |  0:01:03s
epoch 15 | loss: 0.38931 | val_0_rmse: 0.66918 | val_1_rmse: 0.67649 |  0:01:07s
epoch 16 | loss: 0.38075 | val_0_rmse: 0.60962 | val_1_rmse: 0.61646 |  0:01:11s
epoch 17 | loss: 0.38575 | val_0_rmse: 0.6134  | val_1_rmse: 0.62312 |  0:01:15s
epoch 18 | loss: 0.38518 | val_0_rmse: 0.61449 | val_1_rmse: 0.62318 |  0:01:19s
epoch 19 | loss: 0.38112 | val_0_rmse: 0.60751 | val_1_rmse: 0.61541 |  0:01:24s
epoch 20 | loss: 0.3844  | val_0_rmse: 0.61168 | val_1_rmse: 0.61851 |  0:01:28s
epoch 21 | loss: 0.37448 | val_0_rmse: 0.66589 | val_1_rmse: 0.67634 |  0:01:32s
epoch 22 | loss: 0.38181 | val_0_rmse: 0.62237 | val_1_rmse: 0.63037 |  0:01:36s
epoch 23 | loss: 0.37752 | val_0_rmse: 0.67369 | val_1_rmse: 0.68087 |  0:01:40s
epoch 24 | loss: 0.37011 | val_0_rmse: 0.65179 | val_1_rmse: 0.65967 |  0:01:45s
epoch 25 | loss: 0.36806 | val_0_rmse: 0.71123 | val_1_rmse: 0.71923 |  0:01:49s
epoch 26 | loss: 0.37269 | val_0_rmse: 0.59223 | val_1_rmse: 0.6015  |  0:01:53s
epoch 27 | loss: 0.3703  | val_0_rmse: 0.61496 | val_1_rmse: 0.62399 |  0:01:57s
epoch 28 | loss: 0.37109 | val_0_rmse: 0.60986 | val_1_rmse: 0.61797 |  0:02:01s
epoch 29 | loss: 0.36402 | val_0_rmse: 0.58861 | val_1_rmse: 0.59925 |  0:02:06s
epoch 30 | loss: 0.36271 | val_0_rmse: 0.59501 | val_1_rmse: 0.60399 |  0:02:10s
epoch 31 | loss: 0.36695 | val_0_rmse: 0.60747 | val_1_rmse: 0.61363 |  0:02:14s
epoch 32 | loss: 0.36725 | val_0_rmse: 0.61465 | val_1_rmse: 0.62318 |  0:02:18s
epoch 33 | loss: 0.37163 | val_0_rmse: 0.69226 | val_1_rmse: 0.70031 |  0:02:22s
epoch 34 | loss: 0.37644 | val_0_rmse: 0.65657 | val_1_rmse: 0.66456 |  0:02:27s
epoch 35 | loss: 0.36652 | val_0_rmse: 0.62299 | val_1_rmse: 0.6289  |  0:02:31s
epoch 36 | loss: 0.37016 | val_0_rmse: 0.59783 | val_1_rmse: 0.60674 |  0:02:35s
epoch 37 | loss: 0.36424 | val_0_rmse: 0.58996 | val_1_rmse: 0.59855 |  0:02:39s
epoch 38 | loss: 0.36494 | val_0_rmse: 0.6102  | val_1_rmse: 0.61748 |  0:02:43s
epoch 39 | loss: 0.36556 | val_0_rmse: 0.67726 | val_1_rmse: 0.68682 |  0:02:48s
epoch 40 | loss: 0.35931 | val_0_rmse: 0.5894  | val_1_rmse: 0.59775 |  0:02:52s
epoch 41 | loss: 0.36104 | val_0_rmse: 0.6269  | val_1_rmse: 0.63559 |  0:02:56s
epoch 42 | loss: 0.35972 | val_0_rmse: 0.60412 | val_1_rmse: 0.60962 |  0:03:00s
epoch 43 | loss: 0.36715 | val_0_rmse: 0.591   | val_1_rmse: 0.5983  |  0:03:04s
epoch 44 | loss: 0.36316 | val_0_rmse: 0.65468 | val_1_rmse: 0.66287 |  0:03:08s
epoch 45 | loss: 0.36147 | val_0_rmse: 0.5896  | val_1_rmse: 0.59701 |  0:03:13s
epoch 46 | loss: 0.35558 | val_0_rmse: 0.65165 | val_1_rmse: 0.65803 |  0:03:17s
epoch 47 | loss: 0.35937 | val_0_rmse: 0.68798 | val_1_rmse: 0.69732 |  0:03:21s
epoch 48 | loss: 0.35364 | val_0_rmse: 0.64136 | val_1_rmse: 0.64853 |  0:03:25s
epoch 49 | loss: 0.35692 | val_0_rmse: 0.59279 | val_1_rmse: 0.60066 |  0:03:29s
epoch 50 | loss: 0.36429 | val_0_rmse: 0.76444 | val_1_rmse: 0.767   |  0:03:34s
epoch 51 | loss: 0.36486 | val_0_rmse: 0.62355 | val_1_rmse: 0.62915 |  0:03:38s
epoch 52 | loss: 0.36058 | val_0_rmse: 0.67028 | val_1_rmse: 0.67512 |  0:03:42s
epoch 53 | loss: 0.36763 | val_0_rmse: 0.60913 | val_1_rmse: 0.61705 |  0:03:46s
epoch 54 | loss: 0.36392 | val_0_rmse: 0.59694 | val_1_rmse: 0.60396 |  0:03:50s
epoch 55 | loss: 0.36702 | val_0_rmse: 0.60654 | val_1_rmse: 0.61393 |  0:03:54s
epoch 56 | loss: 0.36109 | val_0_rmse: 0.73088 | val_1_rmse: 0.7388  |  0:03:59s
epoch 57 | loss: 0.36425 | val_0_rmse: 0.60217 | val_1_rmse: 0.60817 |  0:04:03s
epoch 58 | loss: 0.35457 | val_0_rmse: 0.63478 | val_1_rmse: 0.64418 |  0:04:07s
epoch 59 | loss: 0.36086 | val_0_rmse: 0.62752 | val_1_rmse: 0.63313 |  0:04:11s
epoch 60 | loss: 0.35531 | val_0_rmse: 0.6312  | val_1_rmse: 0.63724 |  0:04:15s
epoch 61 | loss: 0.37068 | val_0_rmse: 0.7333  | val_1_rmse: 0.74058 |  0:04:20s
epoch 62 | loss: 0.36248 | val_0_rmse: 0.61571 | val_1_rmse: 0.62274 |  0:04:24s
epoch 63 | loss: 0.35201 | val_0_rmse: 0.61555 | val_1_rmse: 0.62215 |  0:04:28s
epoch 64 | loss: 0.35265 | val_0_rmse: 0.59215 | val_1_rmse: 0.60141 |  0:04:32s
epoch 65 | loss: 0.34928 | val_0_rmse: 0.59577 | val_1_rmse: 0.60211 |  0:04:36s
epoch 66 | loss: 0.35176 | val_0_rmse: 0.62557 | val_1_rmse: 0.63284 |  0:04:40s
epoch 67 | loss: 0.34918 | val_0_rmse: 0.61088 | val_1_rmse: 0.61865 |  0:04:45s
epoch 68 | loss: 0.35632 | val_0_rmse: 0.63885 | val_1_rmse: 0.64628 |  0:04:49s
epoch 69 | loss: 0.36239 | val_0_rmse: 0.59489 | val_1_rmse: 0.60246 |  0:04:53s
epoch 70 | loss: 0.35052 | val_0_rmse: 0.62058 | val_1_rmse: 0.62905 |  0:04:57s
epoch 71 | loss: 0.35696 | val_0_rmse: 0.62347 | val_1_rmse: 0.63352 |  0:05:01s
epoch 72 | loss: 0.35054 | val_0_rmse: 0.60185 | val_1_rmse: 0.60765 |  0:05:06s
epoch 73 | loss: 0.34623 | val_0_rmse: 0.65321 | val_1_rmse: 0.66311 |  0:05:10s
epoch 74 | loss: 0.35151 | val_0_rmse: 0.59683 | val_1_rmse: 0.60626 |  0:05:14s
epoch 75 | loss: 0.34811 | val_0_rmse: 0.60505 | val_1_rmse: 0.61215 |  0:05:18s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.59701
Best weights from best epoch are automatically used!
ended training at: 09:50:03
Feature importance:
[('Area', 0.23740713710362338), ('Baths', 0.15696942057026297), ('Beds', 9.844642715484093e-06), ('Latitude', 0.3059267385765676), ('Longitude', 0.29186100137174575), ('Month', 0.0), ('Year', 0.007825857735084804)]
Mean squared error is of 2333680063.584555
Mean absolute error:34950.27703305798
MAPE:0.3333041537827687
R2 score:0.6546071396899591
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:50:03
epoch 0  | loss: 0.6326  | val_0_rmse: 0.72871 | val_1_rmse: 0.72851 |  0:00:04s
epoch 1  | loss: 0.52479 | val_0_rmse: 0.72334 | val_1_rmse: 0.72138 |  0:00:08s
epoch 2  | loss: 0.46431 | val_0_rmse: 0.72516 | val_1_rmse: 0.72751 |  0:00:12s
epoch 3  | loss: 0.43379 | val_0_rmse: 0.68323 | val_1_rmse: 0.67615 |  0:00:16s
epoch 4  | loss: 0.41653 | val_0_rmse: 0.63538 | val_1_rmse: 0.63509 |  0:00:21s
epoch 5  | loss: 0.40609 | val_0_rmse: 0.70821 | val_1_rmse: 0.70021 |  0:00:25s
epoch 6  | loss: 0.39789 | val_0_rmse: 0.67004 | val_1_rmse: 0.66595 |  0:00:29s
epoch 7  | loss: 0.38793 | val_0_rmse: 0.63568 | val_1_rmse: 0.63405 |  0:00:33s
epoch 8  | loss: 0.38508 | val_0_rmse: 0.63669 | val_1_rmse: 0.63582 |  0:00:37s
epoch 9  | loss: 0.38613 | val_0_rmse: 0.61717 | val_1_rmse: 0.61511 |  0:00:42s
epoch 10 | loss: 0.38007 | val_0_rmse: 0.63046 | val_1_rmse: 0.62951 |  0:00:46s
epoch 11 | loss: 0.38627 | val_0_rmse: 0.63939 | val_1_rmse: 0.64023 |  0:00:50s
epoch 12 | loss: 0.37716 | val_0_rmse: 0.67029 | val_1_rmse: 0.67008 |  0:00:54s
epoch 13 | loss: 0.37821 | val_0_rmse: 0.6816  | val_1_rmse: 0.68233 |  0:00:58s
epoch 14 | loss: 0.37987 | val_0_rmse: 0.62858 | val_1_rmse: 0.62626 |  0:01:03s
epoch 15 | loss: 0.3726  | val_0_rmse: 0.71737 | val_1_rmse: 0.71945 |  0:01:07s
epoch 16 | loss: 0.37247 | val_0_rmse: 0.62302 | val_1_rmse: 0.62214 |  0:01:11s
epoch 17 | loss: 0.36671 | val_0_rmse: 0.59897 | val_1_rmse: 0.59701 |  0:01:15s
epoch 18 | loss: 0.38217 | val_0_rmse: 0.73214 | val_1_rmse: 0.73317 |  0:01:19s
epoch 19 | loss: 0.37753 | val_0_rmse: 0.60827 | val_1_rmse: 0.60615 |  0:01:24s
epoch 20 | loss: 0.37325 | val_0_rmse: 0.76811 | val_1_rmse: 0.76869 |  0:01:28s
epoch 21 | loss: 0.37921 | val_0_rmse: 0.73695 | val_1_rmse: 0.73299 |  0:01:32s
epoch 22 | loss: 0.37181 | val_0_rmse: 0.59746 | val_1_rmse: 0.59428 |  0:01:36s
epoch 23 | loss: 0.37195 | val_0_rmse: 0.60234 | val_1_rmse: 0.60026 |  0:01:40s
epoch 24 | loss: 0.36677 | val_0_rmse: 0.60581 | val_1_rmse: 0.60339 |  0:01:45s
epoch 25 | loss: 0.36732 | val_0_rmse: 0.67507 | val_1_rmse: 0.67563 |  0:01:49s
epoch 26 | loss: 0.3671  | val_0_rmse: 0.61658 | val_1_rmse: 0.61433 |  0:01:53s
epoch 27 | loss: 0.37001 | val_0_rmse: 0.67655 | val_1_rmse: 0.6759  |  0:01:57s
epoch 28 | loss: 0.36444 | val_0_rmse: 0.67664 | val_1_rmse: 0.6761  |  0:02:01s
epoch 29 | loss: 0.36241 | val_0_rmse: 0.63335 | val_1_rmse: 0.62554 |  0:02:05s
epoch 30 | loss: 0.36908 | val_0_rmse: 0.77839 | val_1_rmse: 0.77142 |  0:02:10s
epoch 31 | loss: 0.37081 | val_0_rmse: 0.78341 | val_1_rmse: 0.78542 |  0:02:14s
epoch 32 | loss: 0.36132 | val_0_rmse: 0.84737 | val_1_rmse: 0.84362 |  0:02:18s
epoch 33 | loss: 0.36106 | val_0_rmse: 0.7673  | val_1_rmse: 0.76108 |  0:02:22s
epoch 34 | loss: 0.35876 | val_0_rmse: 0.61372 | val_1_rmse: 0.61235 |  0:02:26s
epoch 35 | loss: 0.35919 | val_0_rmse: 0.68085 | val_1_rmse: 0.67969 |  0:02:31s
epoch 36 | loss: 0.36474 | val_0_rmse: 0.6918  | val_1_rmse: 0.69243 |  0:02:35s
epoch 37 | loss: 0.36745 | val_0_rmse: 0.679   | val_1_rmse: 0.67765 |  0:02:39s
epoch 38 | loss: 0.36726 | val_0_rmse: 0.5958  | val_1_rmse: 0.593   |  0:02:43s
epoch 39 | loss: 0.35431 | val_0_rmse: 0.60351 | val_1_rmse: 0.60185 |  0:02:47s
epoch 40 | loss: 0.35428 | val_0_rmse: 0.60281 | val_1_rmse: 0.60144 |  0:02:52s
epoch 41 | loss: 0.35714 | val_0_rmse: 0.62602 | val_1_rmse: 0.62762 |  0:02:56s
epoch 42 | loss: 0.35173 | val_0_rmse: 0.70099 | val_1_rmse: 0.70409 |  0:03:00s
epoch 43 | loss: 0.35406 | val_0_rmse: 0.63462 | val_1_rmse: 0.63355 |  0:03:04s
epoch 44 | loss: 0.35262 | val_0_rmse: 0.68326 | val_1_rmse: 0.68712 |  0:03:08s
epoch 45 | loss: 0.35498 | val_0_rmse: 0.59053 | val_1_rmse: 0.58799 |  0:03:13s
epoch 46 | loss: 0.3568  | val_0_rmse: 0.59109 | val_1_rmse: 0.59121 |  0:03:17s
epoch 47 | loss: 0.35149 | val_0_rmse: 0.64612 | val_1_rmse: 0.64371 |  0:03:21s
epoch 48 | loss: 0.35472 | val_0_rmse: 0.73012 | val_1_rmse: 0.7329  |  0:03:25s
epoch 49 | loss: 0.35346 | val_0_rmse: 0.5943  | val_1_rmse: 0.59213 |  0:03:29s
epoch 50 | loss: 0.35748 | val_0_rmse: 0.5979  | val_1_rmse: 0.59564 |  0:03:34s
epoch 51 | loss: 0.35211 | val_0_rmse: 0.71964 | val_1_rmse: 0.71459 |  0:03:38s
epoch 52 | loss: 0.35109 | val_0_rmse: 0.67823 | val_1_rmse: 0.67564 |  0:03:42s
epoch 53 | loss: 0.35443 | val_0_rmse: 0.70527 | val_1_rmse: 0.70781 |  0:03:46s
epoch 54 | loss: 0.35413 | val_0_rmse: 0.80076 | val_1_rmse: 0.79639 |  0:03:50s
epoch 55 | loss: 0.35624 | val_0_rmse: 0.59956 | val_1_rmse: 0.60078 |  0:03:55s
epoch 56 | loss: 0.36112 | val_0_rmse: 0.7699  | val_1_rmse: 0.76778 |  0:03:59s
epoch 57 | loss: 0.35233 | val_0_rmse: 0.71153 | val_1_rmse: 0.7066  |  0:04:03s
epoch 58 | loss: 0.35109 | val_0_rmse: 0.59403 | val_1_rmse: 0.59514 |  0:04:07s
epoch 59 | loss: 0.34992 | val_0_rmse: 0.58462 | val_1_rmse: 0.58486 |  0:04:11s
epoch 60 | loss: 0.35338 | val_0_rmse: 0.62827 | val_1_rmse: 0.62641 |  0:04:15s
epoch 61 | loss: 0.35497 | val_0_rmse: 0.71199 | val_1_rmse: 0.71449 |  0:04:20s
epoch 62 | loss: 0.35183 | val_0_rmse: 0.73242 | val_1_rmse: 0.73058 |  0:04:24s
epoch 63 | loss: 0.35121 | val_0_rmse: 0.59482 | val_1_rmse: 0.59591 |  0:04:28s
epoch 64 | loss: 0.35202 | val_0_rmse: 0.61114 | val_1_rmse: 0.61305 |  0:04:32s
epoch 65 | loss: 0.34817 | val_0_rmse: 0.73251 | val_1_rmse: 0.73155 |  0:04:36s
epoch 66 | loss: 0.3601  | val_0_rmse: 0.59415 | val_1_rmse: 0.59125 |  0:04:41s
epoch 67 | loss: 0.35769 | val_0_rmse: 0.58167 | val_1_rmse: 0.58014 |  0:04:45s
epoch 68 | loss: 0.3519  | val_0_rmse: 0.64018 | val_1_rmse: 0.6404  |  0:04:49s
epoch 69 | loss: 0.35451 | val_0_rmse: 0.65088 | val_1_rmse: 0.64741 |  0:04:53s
epoch 70 | loss: 0.35309 | val_0_rmse: 0.64071 | val_1_rmse: 0.64104 |  0:04:57s
epoch 71 | loss: 0.35036 | val_0_rmse: 0.63769 | val_1_rmse: 0.63981 |  0:05:02s
epoch 72 | loss: 0.3549  | val_0_rmse: 0.62307 | val_1_rmse: 0.62108 |  0:05:06s
epoch 73 | loss: 0.36143 | val_0_rmse: 0.60403 | val_1_rmse: 0.60357 |  0:05:10s
epoch 74 | loss: 0.35385 | val_0_rmse: 0.66397 | val_1_rmse: 0.66619 |  0:05:14s
epoch 75 | loss: 0.35219 | val_0_rmse: 0.71796 | val_1_rmse: 0.71175 |  0:05:18s
epoch 76 | loss: 0.35212 | val_0_rmse: 0.68907 | val_1_rmse: 0.69049 |  0:05:23s
epoch 77 | loss: 0.35142 | val_0_rmse: 0.66822 | val_1_rmse: 0.66818 |  0:05:27s
epoch 78 | loss: 0.35117 | val_0_rmse: 0.59853 | val_1_rmse: 0.59557 |  0:05:31s
epoch 79 | loss: 0.35281 | val_0_rmse: 0.61684 | val_1_rmse: 0.61624 |  0:05:35s
epoch 80 | loss: 0.35262 | val_0_rmse: 0.59529 | val_1_rmse: 0.59331 |  0:05:39s
epoch 81 | loss: 0.34886 | val_0_rmse: 0.59065 | val_1_rmse: 0.58634 |  0:05:43s
epoch 82 | loss: 0.35505 | val_0_rmse: 0.60851 | val_1_rmse: 0.60397 |  0:05:48s
epoch 83 | loss: 0.35248 | val_0_rmse: 0.5938  | val_1_rmse: 0.59113 |  0:05:52s
epoch 84 | loss: 0.35567 | val_0_rmse: 0.77852 | val_1_rmse: 0.77927 |  0:05:56s
epoch 85 | loss: 0.3583  | val_0_rmse: 0.58535 | val_1_rmse: 0.58309 |  0:06:00s
epoch 86 | loss: 0.35156 | val_0_rmse: 0.65348 | val_1_rmse: 0.65182 |  0:06:04s
epoch 87 | loss: 0.35119 | val_0_rmse: 0.63656 | val_1_rmse: 0.63263 |  0:06:09s
epoch 88 | loss: 0.35297 | val_0_rmse: 0.67831 | val_1_rmse: 0.67918 |  0:06:13s
epoch 89 | loss: 0.35573 | val_0_rmse: 0.61307 | val_1_rmse: 0.6116  |  0:06:17s
epoch 90 | loss: 0.35131 | val_0_rmse: 0.67887 | val_1_rmse: 0.6775  |  0:06:21s
epoch 91 | loss: 0.35399 | val_0_rmse: 0.61325 | val_1_rmse: 0.61371 |  0:06:25s
epoch 92 | loss: 0.34832 | val_0_rmse: 0.61962 | val_1_rmse: 0.6175  |  0:06:30s
epoch 93 | loss: 0.3509  | val_0_rmse: 0.62361 | val_1_rmse: 0.62064 |  0:06:34s
epoch 94 | loss: 0.34843 | val_0_rmse: 0.62635 | val_1_rmse: 0.62293 |  0:06:38s
epoch 95 | loss: 0.34611 | val_0_rmse: 0.70825 | val_1_rmse: 0.7038  |  0:06:42s
epoch 96 | loss: 0.35311 | val_0_rmse: 0.69409 | val_1_rmse: 0.69669 |  0:06:46s
epoch 97 | loss: 0.35597 | val_0_rmse: 0.62172 | val_1_rmse: 0.62017 |  0:06:51s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.58014
Best weights from best epoch are automatically used!
ended training at: 09:56:56
Feature importance:
[('Area', 0.3070308639422249), ('Baths', 0.28127362692075286), ('Beds', 0.017541014308434705), ('Latitude', 0.061230336905865795), ('Longitude', 0.3329240129673587), ('Month', 1.449553630681583e-07), ('Year', 0.0)]
Mean squared error is of 2365929451.766732
Mean absolute error:35068.55879568078
MAPE:0.32319282849680664
R2 score:0.6567466547269794
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 09:56:56
epoch 0  | loss: 0.63357 | val_0_rmse: 0.73269 | val_1_rmse: 0.74901 |  0:00:04s
epoch 1  | loss: 0.52764 | val_0_rmse: 0.70636 | val_1_rmse: 0.71956 |  0:00:08s
epoch 2  | loss: 0.49826 | val_0_rmse: 0.69221 | val_1_rmse: 0.70595 |  0:00:12s
epoch 3  | loss: 0.46874 | val_0_rmse: 0.68618 | val_1_rmse: 0.70043 |  0:00:16s
epoch 4  | loss: 0.45895 | val_0_rmse: 0.66143 | val_1_rmse: 0.67608 |  0:00:20s
epoch 5  | loss: 0.44911 | val_0_rmse: 0.65898 | val_1_rmse: 0.6749  |  0:00:25s
epoch 6  | loss: 0.44654 | val_0_rmse: 0.72136 | val_1_rmse: 0.73471 |  0:00:29s
epoch 7  | loss: 0.42745 | val_0_rmse: 0.65422 | val_1_rmse: 0.66726 |  0:00:33s
epoch 8  | loss: 0.39855 | val_0_rmse: 0.62351 | val_1_rmse: 0.63711 |  0:00:37s
epoch 9  | loss: 0.39019 | val_0_rmse: 0.70952 | val_1_rmse: 0.71721 |  0:00:41s
epoch 10 | loss: 0.39064 | val_0_rmse: 0.61524 | val_1_rmse: 0.62289 |  0:00:46s
epoch 11 | loss: 0.41348 | val_0_rmse: 0.72159 | val_1_rmse: 0.72828 |  0:00:50s
epoch 12 | loss: 0.39831 | val_0_rmse: 0.63036 | val_1_rmse: 0.64204 |  0:00:54s
epoch 13 | loss: 0.38657 | val_0_rmse: 0.86917 | val_1_rmse: 0.88391 |  0:00:58s
epoch 14 | loss: 0.39027 | val_0_rmse: 0.63423 | val_1_rmse: 0.64805 |  0:01:02s
epoch 15 | loss: 0.38063 | val_0_rmse: 0.60424 | val_1_rmse: 0.62011 |  0:01:07s
epoch 16 | loss: 0.37655 | val_0_rmse: 0.81469 | val_1_rmse: 0.83139 |  0:01:11s
epoch 17 | loss: 0.3666  | val_0_rmse: 0.67157 | val_1_rmse: 0.67296 |  0:01:15s
epoch 18 | loss: 0.36234 | val_0_rmse: 0.60168 | val_1_rmse: 0.61545 |  0:01:19s
epoch 19 | loss: 0.36412 | val_0_rmse: 0.76935 | val_1_rmse: 0.78441 |  0:01:23s
epoch 20 | loss: 0.36138 | val_0_rmse: 0.62609 | val_1_rmse: 0.62991 |  0:01:27s
epoch 21 | loss: 0.36553 | val_0_rmse: 0.63132 | val_1_rmse: 0.64184 |  0:01:32s
epoch 22 | loss: 0.36668 | val_0_rmse: 0.60387 | val_1_rmse: 0.61028 |  0:01:36s
epoch 23 | loss: 0.35834 | val_0_rmse: 0.74877 | val_1_rmse: 0.75524 |  0:01:40s
epoch 24 | loss: 0.36629 | val_0_rmse: 0.70374 | val_1_rmse: 0.72032 |  0:01:44s
epoch 25 | loss: 0.36176 | val_0_rmse: 0.73623 | val_1_rmse: 0.74999 |  0:01:48s
epoch 26 | loss: 0.36242 | val_0_rmse: 0.65877 | val_1_rmse: 0.67349 |  0:01:53s
epoch 27 | loss: 0.35649 | val_0_rmse: 0.58229 | val_1_rmse: 0.5941  |  0:01:57s
epoch 28 | loss: 0.35219 | val_0_rmse: 0.75507 | val_1_rmse: 0.76953 |  0:02:01s
epoch 29 | loss: 0.35836 | val_0_rmse: 0.62982 | val_1_rmse: 0.64357 |  0:02:05s
epoch 30 | loss: 0.36376 | val_0_rmse: 0.73197 | val_1_rmse: 0.73001 |  0:02:09s
epoch 31 | loss: 0.35328 | val_0_rmse: 0.61923 | val_1_rmse: 0.63048 |  0:02:14s
epoch 32 | loss: 0.34968 | val_0_rmse: 0.58084 | val_1_rmse: 0.59422 |  0:02:18s
epoch 33 | loss: 0.35558 | val_0_rmse: 0.83254 | val_1_rmse: 0.84376 |  0:02:22s
epoch 34 | loss: 0.35183 | val_0_rmse: 0.61994 | val_1_rmse: 0.63426 |  0:02:26s
epoch 35 | loss: 0.34789 | val_0_rmse: 0.67299 | val_1_rmse: 0.68479 |  0:02:30s
epoch 36 | loss: 0.34503 | val_0_rmse: 0.68399 | val_1_rmse: 0.69329 |  0:02:35s
epoch 37 | loss: 0.36034 | val_0_rmse: 0.65389 | val_1_rmse: 0.65622 |  0:02:39s
epoch 38 | loss: 0.35069 | val_0_rmse: 0.74229 | val_1_rmse: 0.73737 |  0:02:43s
epoch 39 | loss: 0.34456 | val_0_rmse: 0.606   | val_1_rmse: 0.61661 |  0:02:47s
epoch 40 | loss: 0.34238 | val_0_rmse: 0.68923 | val_1_rmse: 0.70035 |  0:02:51s
epoch 41 | loss: 0.34273 | val_0_rmse: 0.61304 | val_1_rmse: 0.6285  |  0:02:56s
epoch 42 | loss: 0.35567 | val_0_rmse: 0.70376 | val_1_rmse: 0.706   |  0:03:00s
epoch 43 | loss: 0.35901 | val_0_rmse: 0.60011 | val_1_rmse: 0.60872 |  0:03:04s
epoch 44 | loss: 0.34793 | val_0_rmse: 0.72481 | val_1_rmse: 0.74167 |  0:03:08s
epoch 45 | loss: 0.34582 | val_0_rmse: 0.63195 | val_1_rmse: 0.64171 |  0:03:12s
epoch 46 | loss: 0.34469 | val_0_rmse: 0.6252  | val_1_rmse: 0.63297 |  0:03:17s
epoch 47 | loss: 0.34094 | val_0_rmse: 0.63638 | val_1_rmse: 0.63865 |  0:03:21s
epoch 48 | loss: 0.34011 | val_0_rmse: 0.60624 | val_1_rmse: 0.61373 |  0:03:25s
epoch 49 | loss: 0.33867 | val_0_rmse: 0.66897 | val_1_rmse: 0.67434 |  0:03:29s
epoch 50 | loss: 0.33762 | val_0_rmse: 0.62789 | val_1_rmse: 0.63411 |  0:03:33s
epoch 51 | loss: 0.34496 | val_0_rmse: 0.60329 | val_1_rmse: 0.61812 |  0:03:37s
epoch 52 | loss: 0.34031 | val_0_rmse: 0.62287 | val_1_rmse: 0.62915 |  0:03:42s
epoch 53 | loss: 0.33871 | val_0_rmse: 0.77281 | val_1_rmse: 0.76773 |  0:03:46s
epoch 54 | loss: 0.33983 | val_0_rmse: 0.7006  | val_1_rmse: 0.70999 |  0:03:50s
epoch 55 | loss: 0.33836 | val_0_rmse: 0.70262 | val_1_rmse: 0.71112 |  0:03:54s
epoch 56 | loss: 0.33579 | val_0_rmse: 0.72123 | val_1_rmse: 0.72889 |  0:03:58s
epoch 57 | loss: 0.33596 | val_0_rmse: 0.58505 | val_1_rmse: 0.59717 |  0:04:03s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.5941
Best weights from best epoch are automatically used!
ended training at: 10:01:00
Feature importance:
[('Area', 0.34467918027640493), ('Baths', 0.21480056909948386), ('Beds', 0.0), ('Latitude', 0.35030715729626144), ('Longitude', 0.027589033588384267), ('Month', 0.0), ('Year', 0.06262405973946554)]
Mean squared error is of 2353555634.3935294
Mean absolute error:35490.43759030419
MAPE:0.3622516606078573
R2 score:0.6561245278793049
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:01:01
epoch 0  | loss: 0.65081 | val_0_rmse: 0.74026 | val_1_rmse: 0.72984 |  0:00:04s
epoch 1  | loss: 0.51612 | val_0_rmse: 0.66651 | val_1_rmse: 0.66074 |  0:00:08s
epoch 2  | loss: 0.43946 | val_0_rmse: 0.83575 | val_1_rmse: 0.82073 |  0:00:12s
epoch 3  | loss: 0.40689 | val_0_rmse: 0.67972 | val_1_rmse: 0.66619 |  0:00:16s
epoch 4  | loss: 0.39908 | val_0_rmse: 0.64738 | val_1_rmse: 0.63914 |  0:00:21s
epoch 5  | loss: 0.39102 | val_0_rmse: 0.60669 | val_1_rmse: 0.59932 |  0:00:25s
epoch 6  | loss: 0.38942 | val_0_rmse: 0.61587 | val_1_rmse: 0.60894 |  0:00:29s
epoch 7  | loss: 0.38248 | val_0_rmse: 0.64824 | val_1_rmse: 0.63786 |  0:00:33s
epoch 8  | loss: 0.38214 | val_0_rmse: 0.66739 | val_1_rmse: 0.65714 |  0:00:37s
epoch 9  | loss: 0.3861  | val_0_rmse: 0.61528 | val_1_rmse: 0.61016 |  0:00:42s
epoch 10 | loss: 0.37769 | val_0_rmse: 0.69816 | val_1_rmse: 0.68287 |  0:00:46s
epoch 11 | loss: 0.37036 | val_0_rmse: 0.68808 | val_1_rmse: 0.6763  |  0:00:50s
epoch 12 | loss: 0.37039 | val_0_rmse: 0.66559 | val_1_rmse: 0.65372 |  0:00:54s
epoch 13 | loss: 0.369   | val_0_rmse: 0.67597 | val_1_rmse: 0.66165 |  0:00:58s
epoch 14 | loss: 0.36753 | val_0_rmse: 0.59168 | val_1_rmse: 0.58312 |  0:01:02s
epoch 15 | loss: 0.36278 | val_0_rmse: 0.6214  | val_1_rmse: 0.6096  |  0:01:07s
epoch 16 | loss: 0.36428 | val_0_rmse: 0.61603 | val_1_rmse: 0.60617 |  0:01:11s
epoch 17 | loss: 0.36749 | val_0_rmse: 0.67768 | val_1_rmse: 0.66681 |  0:01:15s
epoch 18 | loss: 0.36397 | val_0_rmse: 0.59587 | val_1_rmse: 0.58724 |  0:01:19s
epoch 19 | loss: 0.36372 | val_0_rmse: 0.61936 | val_1_rmse: 0.60834 |  0:01:23s
epoch 20 | loss: 0.36089 | val_0_rmse: 0.66419 | val_1_rmse: 0.65311 |  0:01:28s
epoch 21 | loss: 0.36316 | val_0_rmse: 0.72605 | val_1_rmse: 0.71246 |  0:01:32s
epoch 22 | loss: 0.36078 | val_0_rmse: 0.65706 | val_1_rmse: 0.64314 |  0:01:36s
epoch 23 | loss: 0.37605 | val_0_rmse: 0.60706 | val_1_rmse: 0.59862 |  0:01:40s
epoch 24 | loss: 0.36405 | val_0_rmse: 0.63336 | val_1_rmse: 0.62895 |  0:01:44s
epoch 25 | loss: 0.36561 | val_0_rmse: 0.62349 | val_1_rmse: 0.61133 |  0:01:49s
epoch 26 | loss: 0.36024 | val_0_rmse: 0.62056 | val_1_rmse: 0.61163 |  0:01:53s
epoch 27 | loss: 0.36165 | val_0_rmse: 0.60203 | val_1_rmse: 0.59173 |  0:01:57s
epoch 28 | loss: 0.35906 | val_0_rmse: 0.59465 | val_1_rmse: 0.58715 |  0:02:01s
epoch 29 | loss: 0.35899 | val_0_rmse: 0.59743 | val_1_rmse: 0.59156 |  0:02:05s
epoch 30 | loss: 0.35451 | val_0_rmse: 0.73509 | val_1_rmse: 0.72311 |  0:02:10s
epoch 31 | loss: 0.35464 | val_0_rmse: 0.64396 | val_1_rmse: 0.63387 |  0:02:14s
epoch 32 | loss: 0.36049 | val_0_rmse: 0.62135 | val_1_rmse: 0.60912 |  0:02:18s
epoch 33 | loss: 0.35289 | val_0_rmse: 0.5955  | val_1_rmse: 0.58629 |  0:02:22s
epoch 34 | loss: 0.3537  | val_0_rmse: 0.64303 | val_1_rmse: 0.62786 |  0:02:26s
epoch 35 | loss: 0.35088 | val_0_rmse: 0.607   | val_1_rmse: 0.59681 |  0:02:31s
epoch 36 | loss: 0.35352 | val_0_rmse: 0.64921 | val_1_rmse: 0.63925 |  0:02:35s
epoch 37 | loss: 0.34709 | val_0_rmse: 0.61072 | val_1_rmse: 0.60067 |  0:02:39s
epoch 38 | loss: 0.34623 | val_0_rmse: 0.66448 | val_1_rmse: 0.64815 |  0:02:43s
epoch 39 | loss: 0.34947 | val_0_rmse: 0.66732 | val_1_rmse: 0.65184 |  0:02:47s
epoch 40 | loss: 0.34302 | val_0_rmse: 0.71103 | val_1_rmse: 0.70887 |  0:02:52s
epoch 41 | loss: 0.34476 | val_0_rmse: 0.60765 | val_1_rmse: 0.59833 |  0:02:56s
epoch 42 | loss: 0.34327 | val_0_rmse: 0.75733 | val_1_rmse: 0.74292 |  0:03:00s
epoch 43 | loss: 0.34609 | val_0_rmse: 0.66019 | val_1_rmse: 0.65541 |  0:03:04s
epoch 44 | loss: 0.34507 | val_0_rmse: 0.57743 | val_1_rmse: 0.5699  |  0:03:08s
epoch 45 | loss: 0.34488 | val_0_rmse: 0.58705 | val_1_rmse: 0.58285 |  0:03:13s
epoch 46 | loss: 0.34376 | val_0_rmse: 0.60597 | val_1_rmse: 0.59626 |  0:03:17s
epoch 47 | loss: 0.33901 | val_0_rmse: 0.57511 | val_1_rmse: 0.5696  |  0:03:21s
epoch 48 | loss: 0.33765 | val_0_rmse: 0.66649 | val_1_rmse: 0.65434 |  0:03:25s
epoch 49 | loss: 0.34152 | val_0_rmse: 0.58817 | val_1_rmse: 0.58454 |  0:03:29s
epoch 50 | loss: 0.34102 | val_0_rmse: 0.58711 | val_1_rmse: 0.58395 |  0:03:34s
epoch 51 | loss: 0.34089 | val_0_rmse: 0.6206  | val_1_rmse: 0.61087 |  0:03:38s
epoch 52 | loss: 0.33773 | val_0_rmse: 0.64001 | val_1_rmse: 0.6282  |  0:03:42s
epoch 53 | loss: 0.3371  | val_0_rmse: 0.60091 | val_1_rmse: 0.59541 |  0:03:46s
epoch 54 | loss: 0.34033 | val_0_rmse: 0.61487 | val_1_rmse: 0.6117  |  0:03:50s
epoch 55 | loss: 0.34186 | val_0_rmse: 0.60972 | val_1_rmse: 0.59966 |  0:03:54s
epoch 56 | loss: 0.33794 | val_0_rmse: 0.72216 | val_1_rmse: 0.70534 |  0:03:59s
epoch 57 | loss: 0.33977 | val_0_rmse: 0.64189 | val_1_rmse: 0.63539 |  0:04:03s
epoch 58 | loss: 0.33845 | val_0_rmse: 0.66325 | val_1_rmse: 0.64956 |  0:04:07s
epoch 59 | loss: 0.3355  | val_0_rmse: 0.57032 | val_1_rmse: 0.5642  |  0:04:11s
epoch 60 | loss: 0.33784 | val_0_rmse: 0.67094 | val_1_rmse: 0.65866 |  0:04:16s
epoch 61 | loss: 0.33628 | val_0_rmse: 0.68435 | val_1_rmse: 0.67056 |  0:04:20s
epoch 62 | loss: 0.33786 | val_0_rmse: 0.60053 | val_1_rmse: 0.58915 |  0:04:24s
epoch 63 | loss: 0.34157 | val_0_rmse: 0.60702 | val_1_rmse: 0.60074 |  0:04:28s
epoch 64 | loss: 0.33771 | val_0_rmse: 0.59064 | val_1_rmse: 0.58692 |  0:04:32s
epoch 65 | loss: 0.34054 | val_0_rmse: 0.6338  | val_1_rmse: 0.62488 |  0:04:37s
epoch 66 | loss: 0.33558 | val_0_rmse: 0.68257 | val_1_rmse: 0.66681 |  0:04:41s
epoch 67 | loss: 0.33731 | val_0_rmse: 0.61062 | val_1_rmse: 0.60889 |  0:04:45s
epoch 68 | loss: 0.33867 | val_0_rmse: 0.87585 | val_1_rmse: 0.85857 |  0:04:49s
epoch 69 | loss: 0.33737 | val_0_rmse: 0.62895 | val_1_rmse: 0.62015 |  0:04:53s
epoch 70 | loss: 0.3391  | val_0_rmse: 0.64226 | val_1_rmse: 0.64257 |  0:04:57s
epoch 71 | loss: 0.33541 | val_0_rmse: 0.67988 | val_1_rmse: 0.66732 |  0:05:02s
epoch 72 | loss: 0.33582 | val_0_rmse: 0.62706 | val_1_rmse: 0.62232 |  0:05:06s
epoch 73 | loss: 0.33804 | val_0_rmse: 0.7093  | val_1_rmse: 0.69686 |  0:05:10s
epoch 74 | loss: 0.34238 | val_0_rmse: 0.61156 | val_1_rmse: 0.60751 |  0:05:14s
epoch 75 | loss: 0.33714 | val_0_rmse: 0.75792 | val_1_rmse: 0.74075 |  0:05:18s
epoch 76 | loss: 0.33521 | val_0_rmse: 0.60789 | val_1_rmse: 0.60151 |  0:05:23s
epoch 77 | loss: 0.33632 | val_0_rmse: 0.65772 | val_1_rmse: 0.65661 |  0:05:27s
epoch 78 | loss: 0.33826 | val_0_rmse: 0.74931 | val_1_rmse: 0.74015 |  0:05:31s
epoch 79 | loss: 0.33381 | val_0_rmse: 0.67755 | val_1_rmse: 0.66222 |  0:05:35s
epoch 80 | loss: 0.33426 | val_0_rmse: 0.72622 | val_1_rmse: 0.71769 |  0:05:39s
epoch 81 | loss: 0.3433  | val_0_rmse: 0.64187 | val_1_rmse: 0.64189 |  0:05:44s
epoch 82 | loss: 0.33901 | val_0_rmse: 0.6204  | val_1_rmse: 0.60821 |  0:05:48s
epoch 83 | loss: 0.33524 | val_0_rmse: 0.60443 | val_1_rmse: 0.59402 |  0:05:52s
epoch 84 | loss: 0.33692 | val_0_rmse: 0.59657 | val_1_rmse: 0.59426 |  0:05:56s
epoch 85 | loss: 0.33184 | val_0_rmse: 0.62809 | val_1_rmse: 0.62839 |  0:06:00s
epoch 86 | loss: 0.33128 | val_0_rmse: 0.62706 | val_1_rmse: 0.62136 |  0:06:05s
epoch 87 | loss: 0.33382 | val_0_rmse: 0.58255 | val_1_rmse: 0.57376 |  0:06:09s
epoch 88 | loss: 0.33786 | val_0_rmse: 0.73794 | val_1_rmse: 0.72322 |  0:06:13s
epoch 89 | loss: 0.34082 | val_0_rmse: 0.67465 | val_1_rmse: 0.65874 |  0:06:17s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.5642
Best weights from best epoch are automatically used!
ended training at: 10:07:20
Feature importance:
[('Area', 0.34163136676283334), ('Baths', 0.15870740603643962), ('Beds', 0.05902847325883362), ('Latitude', 0.18337032050146587), ('Longitude', 0.18060843783200475), ('Month', 0.036500793729515896), ('Year', 0.04015320187890692)]
Mean squared error is of 2200228787.562499
Mean absolute error:33916.5918234782
MAPE:0.3202417247345658
R2 score:0.6805138344134836
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:07:20
epoch 0  | loss: 0.58528 | val_0_rmse: 0.65881 | val_1_rmse: 0.64882 |  0:00:01s
epoch 1  | loss: 0.36262 | val_0_rmse: 0.58202 | val_1_rmse: 0.58372 |  0:00:03s
epoch 2  | loss: 0.33915 | val_0_rmse: 0.60453 | val_1_rmse: 0.60834 |  0:00:04s
epoch 3  | loss: 0.35772 | val_0_rmse: 0.57231 | val_1_rmse: 0.58032 |  0:00:06s
epoch 4  | loss: 0.33054 | val_0_rmse: 0.56113 | val_1_rmse: 0.56739 |  0:00:07s
epoch 5  | loss: 0.31497 | val_0_rmse: 0.58994 | val_1_rmse: 0.59244 |  0:00:09s
epoch 6  | loss: 0.32369 | val_0_rmse: 0.57742 | val_1_rmse: 0.587   |  0:00:10s
epoch 7  | loss: 0.30319 | val_0_rmse: 0.57191 | val_1_rmse: 0.57924 |  0:00:12s
epoch 8  | loss: 0.28941 | val_0_rmse: 0.54855 | val_1_rmse: 0.55201 |  0:00:13s
epoch 9  | loss: 0.29831 | val_0_rmse: 0.61225 | val_1_rmse: 0.61853 |  0:00:15s
epoch 10 | loss: 0.29092 | val_0_rmse: 0.56148 | val_1_rmse: 0.56429 |  0:00:17s
epoch 11 | loss: 0.28522 | val_0_rmse: 0.63047 | val_1_rmse: 0.63363 |  0:00:18s
epoch 12 | loss: 0.28038 | val_0_rmse: 0.50889 | val_1_rmse: 0.51694 |  0:00:20s
epoch 13 | loss: 0.27356 | val_0_rmse: 0.54075 | val_1_rmse: 0.54931 |  0:00:21s
epoch 14 | loss: 0.28743 | val_0_rmse: 0.68345 | val_1_rmse: 0.6845  |  0:00:23s
epoch 15 | loss: 0.27643 | val_0_rmse: 0.65069 | val_1_rmse: 0.65782 |  0:00:24s
epoch 16 | loss: 0.27693 | val_0_rmse: 0.53286 | val_1_rmse: 0.5426  |  0:00:26s
epoch 17 | loss: 0.28358 | val_0_rmse: 0.66664 | val_1_rmse: 0.67075 |  0:00:27s
epoch 18 | loss: 0.28293 | val_0_rmse: 0.68161 | val_1_rmse: 0.69145 |  0:00:29s
epoch 19 | loss: 0.27405 | val_0_rmse: 0.52655 | val_1_rmse: 0.53304 |  0:00:30s
epoch 20 | loss: 0.27094 | val_0_rmse: 0.53977 | val_1_rmse: 0.54441 |  0:00:32s
epoch 21 | loss: 0.27738 | val_0_rmse: 0.54695 | val_1_rmse: 0.55297 |  0:00:34s
epoch 22 | loss: 0.28102 | val_0_rmse: 0.53011 | val_1_rmse: 0.538   |  0:00:35s
epoch 23 | loss: 0.28899 | val_0_rmse: 0.53155 | val_1_rmse: 0.53482 |  0:00:37s
epoch 24 | loss: 0.27573 | val_0_rmse: 0.53867 | val_1_rmse: 0.5454  |  0:00:38s
epoch 25 | loss: 0.2716  | val_0_rmse: 0.53134 | val_1_rmse: 0.53747 |  0:00:40s
epoch 26 | loss: 0.27498 | val_0_rmse: 0.5292  | val_1_rmse: 0.5347  |  0:00:41s
epoch 27 | loss: 0.28677 | val_0_rmse: 0.61688 | val_1_rmse: 0.62609 |  0:00:43s
epoch 28 | loss: 0.27597 | val_0_rmse: 0.65117 | val_1_rmse: 0.6538  |  0:00:44s
epoch 29 | loss: 0.27217 | val_0_rmse: 0.56487 | val_1_rmse: 0.57218 |  0:00:46s
epoch 30 | loss: 0.28949 | val_0_rmse: 0.59014 | val_1_rmse: 0.59235 |  0:00:47s
epoch 31 | loss: 0.29479 | val_0_rmse: 0.53765 | val_1_rmse: 0.54808 |  0:00:49s
epoch 32 | loss: 0.29199 | val_0_rmse: 0.59183 | val_1_rmse: 0.59894 |  0:00:51s
epoch 33 | loss: 0.29766 | val_0_rmse: 0.54246 | val_1_rmse: 0.55177 |  0:00:52s
epoch 34 | loss: 0.2852  | val_0_rmse: 0.54957 | val_1_rmse: 0.5543  |  0:00:54s
epoch 35 | loss: 0.28606 | val_0_rmse: 0.69494 | val_1_rmse: 0.69424 |  0:00:55s
epoch 36 | loss: 0.30514 | val_0_rmse: 0.59364 | val_1_rmse: 0.60192 |  0:00:57s
epoch 37 | loss: 0.29183 | val_0_rmse: 0.51501 | val_1_rmse: 0.52121 |  0:00:58s
epoch 38 | loss: 0.27534 | val_0_rmse: 0.52636 | val_1_rmse: 0.53123 |  0:01:00s
epoch 39 | loss: 0.26879 | val_0_rmse: 0.53865 | val_1_rmse: 0.54508 |  0:01:01s
epoch 40 | loss: 0.28477 | val_0_rmse: 0.63463 | val_1_rmse: 0.63826 |  0:01:03s
epoch 41 | loss: 0.27031 | val_0_rmse: 0.51968 | val_1_rmse: 0.52766 |  0:01:04s
epoch 42 | loss: 0.27476 | val_0_rmse: 0.55496 | val_1_rmse: 0.56087 |  0:01:06s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.51694
Best weights from best epoch are automatically used!
ended training at: 10:08:27
Feature importance:
[('Area', 0.34396220591928406), ('Baths', 0.023801212557890552), ('Beds', 0.025229452191410855), ('Latitude', 0.2470935037873238), ('Longitude', 0.28501544036745524), ('Month', 0.001151303991242843), ('Year', 0.07374688118539267)]
Mean squared error is of 1060503659.2746103
Mean absolute error:22954.234936532528
MAPE:0.3018811916227329
R2 score:0.7237326117630707
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:08:28
epoch 0  | loss: 0.63744 | val_0_rmse: 0.70071 | val_1_rmse: 0.69691 |  0:00:01s
epoch 1  | loss: 0.355   | val_0_rmse: 0.65999 | val_1_rmse: 0.66594 |  0:00:03s
epoch 2  | loss: 0.34917 | val_0_rmse: 0.64002 | val_1_rmse: 0.66017 |  0:00:04s
epoch 3  | loss: 0.3449  | val_0_rmse: 0.56952 | val_1_rmse: 0.57176 |  0:00:06s
epoch 4  | loss: 0.3326  | val_0_rmse: 0.55706 | val_1_rmse: 0.56242 |  0:00:07s
epoch 5  | loss: 0.32674 | val_0_rmse: 0.5673  | val_1_rmse: 0.57002 |  0:00:09s
epoch 6  | loss: 0.33309 | val_0_rmse: 0.55856 | val_1_rmse: 0.56494 |  0:00:10s
epoch 7  | loss: 0.32115 | val_0_rmse: 0.56881 | val_1_rmse: 0.57458 |  0:00:12s
epoch 8  | loss: 0.31503 | val_0_rmse: 0.56268 | val_1_rmse: 0.56666 |  0:00:13s
epoch 9  | loss: 0.31246 | val_0_rmse: 0.55083 | val_1_rmse: 0.55912 |  0:00:15s
epoch 10 | loss: 0.31541 | val_0_rmse: 0.55718 | val_1_rmse: 0.5679  |  0:00:17s
epoch 11 | loss: 0.31403 | val_0_rmse: 0.54624 | val_1_rmse: 0.55112 |  0:00:18s
epoch 12 | loss: 0.30751 | val_0_rmse: 0.56298 | val_1_rmse: 0.56607 |  0:00:20s
epoch 13 | loss: 0.31018 | val_0_rmse: 0.53736 | val_1_rmse: 0.54423 |  0:00:21s
epoch 14 | loss: 0.29713 | val_0_rmse: 0.53056 | val_1_rmse: 0.53759 |  0:00:23s
epoch 15 | loss: 0.29751 | val_0_rmse: 0.53789 | val_1_rmse: 0.55056 |  0:00:24s
epoch 16 | loss: 0.31431 | val_0_rmse: 0.57387 | val_1_rmse: 0.57871 |  0:00:26s
epoch 17 | loss: 0.3151  | val_0_rmse: 0.56461 | val_1_rmse: 0.56687 |  0:00:27s
epoch 18 | loss: 0.30416 | val_0_rmse: 0.53586 | val_1_rmse: 0.54242 |  0:00:29s
epoch 19 | loss: 0.31035 | val_0_rmse: 0.54777 | val_1_rmse: 0.55308 |  0:00:31s
epoch 20 | loss: 0.30415 | val_0_rmse: 0.56427 | val_1_rmse: 0.57201 |  0:00:32s
epoch 21 | loss: 0.30763 | val_0_rmse: 0.53127 | val_1_rmse: 0.54223 |  0:00:34s
epoch 22 | loss: 0.29517 | val_0_rmse: 0.53424 | val_1_rmse: 0.54098 |  0:00:35s
epoch 23 | loss: 0.32318 | val_0_rmse: 0.5655  | val_1_rmse: 0.57122 |  0:00:37s
epoch 24 | loss: 0.31403 | val_0_rmse: 0.54461 | val_1_rmse: 0.55289 |  0:00:38s
epoch 25 | loss: 0.30854 | val_0_rmse: 0.53523 | val_1_rmse: 0.54494 |  0:00:40s
epoch 26 | loss: 0.29785 | val_0_rmse: 0.54681 | val_1_rmse: 0.55139 |  0:00:41s
epoch 27 | loss: 0.30585 | val_0_rmse: 0.55946 | val_1_rmse: 0.56476 |  0:00:43s
epoch 28 | loss: 0.3192  | val_0_rmse: 0.55449 | val_1_rmse: 0.56153 |  0:00:44s
epoch 29 | loss: 0.30581 | val_0_rmse: 0.53439 | val_1_rmse: 0.54485 |  0:00:46s
epoch 30 | loss: 0.29741 | val_0_rmse: 0.53961 | val_1_rmse: 0.54462 |  0:00:48s
epoch 31 | loss: 0.29193 | val_0_rmse: 0.55189 | val_1_rmse: 0.56078 |  0:00:49s
epoch 32 | loss: 0.29936 | val_0_rmse: 0.52477 | val_1_rmse: 0.53462 |  0:00:51s
epoch 33 | loss: 0.29485 | val_0_rmse: 0.56346 | val_1_rmse: 0.57323 |  0:00:52s
epoch 34 | loss: 0.30343 | val_0_rmse: 0.53736 | val_1_rmse: 0.54958 |  0:00:54s
epoch 35 | loss: 0.31455 | val_0_rmse: 0.54605 | val_1_rmse: 0.55128 |  0:00:55s
epoch 36 | loss: 0.32049 | val_0_rmse: 0.5779  | val_1_rmse: 0.58089 |  0:00:57s
epoch 37 | loss: 0.32091 | val_0_rmse: 0.56627 | val_1_rmse: 0.56812 |  0:00:58s
epoch 38 | loss: 0.32041 | val_0_rmse: 0.55748 | val_1_rmse: 0.55635 |  0:01:00s
epoch 39 | loss: 0.32802 | val_0_rmse: 0.55242 | val_1_rmse: 0.55782 |  0:01:02s
epoch 40 | loss: 0.31459 | val_0_rmse: 0.5607  | val_1_rmse: 0.56797 |  0:01:03s
epoch 41 | loss: 0.3129  | val_0_rmse: 0.55046 | val_1_rmse: 0.55349 |  0:01:05s
epoch 42 | loss: 0.3091  | val_0_rmse: 0.54985 | val_1_rmse: 0.55859 |  0:01:06s
epoch 43 | loss: 0.3019  | val_0_rmse: 0.5359  | val_1_rmse: 0.5457  |  0:01:08s
epoch 44 | loss: 0.29659 | val_0_rmse: 0.55812 | val_1_rmse: 0.56303 |  0:01:09s
epoch 45 | loss: 0.30389 | val_0_rmse: 0.53178 | val_1_rmse: 0.53827 |  0:01:11s
epoch 46 | loss: 0.29181 | val_0_rmse: 0.5361  | val_1_rmse: 0.54366 |  0:01:12s
epoch 47 | loss: 0.29615 | val_0_rmse: 0.53232 | val_1_rmse: 0.53951 |  0:01:14s
epoch 48 | loss: 0.30197 | val_0_rmse: 0.53474 | val_1_rmse: 0.54294 |  0:01:15s
epoch 49 | loss: 0.3071  | val_0_rmse: 0.56651 | val_1_rmse: 0.57179 |  0:01:17s
epoch 50 | loss: 0.32148 | val_0_rmse: 0.54096 | val_1_rmse: 0.54773 |  0:01:19s
epoch 51 | loss: 0.2938  | val_0_rmse: 0.53164 | val_1_rmse: 0.54168 |  0:01:20s
epoch 52 | loss: 0.2951  | val_0_rmse: 0.56427 | val_1_rmse: 0.56925 |  0:01:22s
epoch 53 | loss: 0.30438 | val_0_rmse: 0.53681 | val_1_rmse: 0.54269 |  0:01:23s
epoch 54 | loss: 0.29584 | val_0_rmse: 0.54876 | val_1_rmse: 0.55359 |  0:01:25s
epoch 55 | loss: 0.2992  | val_0_rmse: 0.53887 | val_1_rmse: 0.54413 |  0:01:26s
epoch 56 | loss: 0.2936  | val_0_rmse: 0.53817 | val_1_rmse: 0.54689 |  0:01:28s
epoch 57 | loss: 0.28961 | val_0_rmse: 0.53156 | val_1_rmse: 0.54158 |  0:01:29s
epoch 58 | loss: 0.29739 | val_0_rmse: 0.53421 | val_1_rmse: 0.54596 |  0:01:31s
epoch 59 | loss: 0.29415 | val_0_rmse: 0.53972 | val_1_rmse: 0.54953 |  0:01:33s
epoch 60 | loss: 0.29967 | val_0_rmse: 0.53896 | val_1_rmse: 0.54791 |  0:01:34s
epoch 61 | loss: 0.29736 | val_0_rmse: 0.53181 | val_1_rmse: 0.54074 |  0:01:36s
epoch 62 | loss: 0.29429 | val_0_rmse: 0.54651 | val_1_rmse: 0.55539 |  0:01:37s

Early stopping occured at epoch 62 with best_epoch = 32 and best_val_1_rmse = 0.53462
Best weights from best epoch are automatically used!
ended training at: 10:10:06
Feature importance:
[('Area', 0.45700999220288474), ('Baths', 0.05717709919482608), ('Beds', 0.11012925006763243), ('Latitude', 0.11080180775725747), ('Longitude', 0.2036750655915462), ('Month', 1.5463106545834236e-05), ('Year', 0.06119132207930725)]
Mean squared error is of 1116523380.0629346
Mean absolute error:23041.59647652394
MAPE:0.2774402534303705
R2 score:0.7299995833346777
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:10:06
epoch 0  | loss: 0.58943 | val_0_rmse: 0.64199 | val_1_rmse: 0.63496 |  0:00:01s
epoch 1  | loss: 0.37192 | val_0_rmse: 0.65055 | val_1_rmse: 0.6421  |  0:00:03s
epoch 2  | loss: 0.36159 | val_0_rmse: 0.59908 | val_1_rmse: 0.59442 |  0:00:04s
epoch 3  | loss: 0.34319 | val_0_rmse: 0.63328 | val_1_rmse: 0.6348  |  0:00:06s
epoch 4  | loss: 0.34498 | val_0_rmse: 0.63079 | val_1_rmse: 0.61952 |  0:00:07s
epoch 5  | loss: 0.33302 | val_0_rmse: 0.56331 | val_1_rmse: 0.56066 |  0:00:09s
epoch 6  | loss: 0.34263 | val_0_rmse: 0.59064 | val_1_rmse: 0.58996 |  0:00:10s
epoch 7  | loss: 0.32244 | val_0_rmse: 0.56292 | val_1_rmse: 0.55999 |  0:00:12s
epoch 8  | loss: 0.31703 | val_0_rmse: 0.58417 | val_1_rmse: 0.57992 |  0:00:13s
epoch 9  | loss: 0.33667 | val_0_rmse: 0.54807 | val_1_rmse: 0.54813 |  0:00:15s
epoch 10 | loss: 0.3137  | val_0_rmse: 0.54414 | val_1_rmse: 0.54045 |  0:00:17s
epoch 11 | loss: 0.31186 | val_0_rmse: 0.54611 | val_1_rmse: 0.54107 |  0:00:18s
epoch 12 | loss: 0.3133  | val_0_rmse: 0.56147 | val_1_rmse: 0.55659 |  0:00:20s
epoch 13 | loss: 0.31723 | val_0_rmse: 0.5473  | val_1_rmse: 0.5437  |  0:00:21s
epoch 14 | loss: 0.33363 | val_0_rmse: 0.56642 | val_1_rmse: 0.56091 |  0:00:23s
epoch 15 | loss: 0.32278 | val_0_rmse: 0.56838 | val_1_rmse: 0.56576 |  0:00:24s
epoch 16 | loss: 0.32469 | val_0_rmse: 0.55749 | val_1_rmse: 0.55111 |  0:00:26s
epoch 17 | loss: 0.32006 | val_0_rmse: 0.54338 | val_1_rmse: 0.53783 |  0:00:27s
epoch 18 | loss: 0.30749 | val_0_rmse: 0.56055 | val_1_rmse: 0.55029 |  0:00:29s
epoch 19 | loss: 0.31582 | val_0_rmse: 0.54915 | val_1_rmse: 0.54796 |  0:00:30s
epoch 20 | loss: 0.31613 | val_0_rmse: 0.55795 | val_1_rmse: 0.55238 |  0:00:32s
epoch 21 | loss: 0.31541 | val_0_rmse: 0.54094 | val_1_rmse: 0.53621 |  0:00:34s
epoch 22 | loss: 0.30958 | val_0_rmse: 0.55532 | val_1_rmse: 0.54919 |  0:00:35s
epoch 23 | loss: 0.3075  | val_0_rmse: 0.54646 | val_1_rmse: 0.5388  |  0:00:37s
epoch 24 | loss: 0.31821 | val_0_rmse: 0.56645 | val_1_rmse: 0.56192 |  0:00:38s
epoch 25 | loss: 0.32228 | val_0_rmse: 0.55907 | val_1_rmse: 0.55585 |  0:00:40s
epoch 26 | loss: 0.30862 | val_0_rmse: 0.56849 | val_1_rmse: 0.56594 |  0:00:41s
epoch 27 | loss: 0.3125  | val_0_rmse: 0.57776 | val_1_rmse: 0.56905 |  0:00:43s
epoch 28 | loss: 0.31098 | val_0_rmse: 0.5423  | val_1_rmse: 0.5401  |  0:00:44s
epoch 29 | loss: 0.3017  | val_0_rmse: 0.57394 | val_1_rmse: 0.56924 |  0:00:46s
epoch 30 | loss: 0.32358 | val_0_rmse: 0.54687 | val_1_rmse: 0.5397  |  0:00:47s
epoch 31 | loss: 0.30898 | val_0_rmse: 0.5702  | val_1_rmse: 0.56348 |  0:00:49s
epoch 32 | loss: 0.31743 | val_0_rmse: 0.5417  | val_1_rmse: 0.53857 |  0:00:51s
epoch 33 | loss: 0.30389 | val_0_rmse: 0.55129 | val_1_rmse: 0.54871 |  0:00:52s
epoch 34 | loss: 0.31098 | val_0_rmse: 0.54942 | val_1_rmse: 0.54025 |  0:00:54s
epoch 35 | loss: 0.30974 | val_0_rmse: 0.55283 | val_1_rmse: 0.54821 |  0:00:55s
epoch 36 | loss: 0.31161 | val_0_rmse: 0.60591 | val_1_rmse: 0.59866 |  0:00:57s
epoch 37 | loss: 0.31046 | val_0_rmse: 0.53462 | val_1_rmse: 0.52596 |  0:00:58s
epoch 38 | loss: 0.30355 | val_0_rmse: 0.53632 | val_1_rmse: 0.52788 |  0:01:00s
epoch 39 | loss: 0.30813 | val_0_rmse: 0.54254 | val_1_rmse: 0.53663 |  0:01:01s
epoch 40 | loss: 0.30604 | val_0_rmse: 0.55421 | val_1_rmse: 0.54739 |  0:01:03s
epoch 41 | loss: 0.32142 | val_0_rmse: 0.55377 | val_1_rmse: 0.54874 |  0:01:05s
epoch 42 | loss: 0.31267 | val_0_rmse: 0.57269 | val_1_rmse: 0.56695 |  0:01:06s
epoch 43 | loss: 0.30825 | val_0_rmse: 0.53984 | val_1_rmse: 0.53249 |  0:01:08s
epoch 44 | loss: 0.30363 | val_0_rmse: 0.55067 | val_1_rmse: 0.54778 |  0:01:09s
epoch 45 | loss: 0.30935 | val_0_rmse: 0.53924 | val_1_rmse: 0.53182 |  0:01:11s
epoch 46 | loss: 0.31286 | val_0_rmse: 0.54521 | val_1_rmse: 0.53884 |  0:01:12s
epoch 47 | loss: 0.30919 | val_0_rmse: 0.54532 | val_1_rmse: 0.53968 |  0:01:14s
epoch 48 | loss: 0.30802 | val_0_rmse: 0.54833 | val_1_rmse: 0.54065 |  0:01:15s
epoch 49 | loss: 0.30116 | val_0_rmse: 0.54459 | val_1_rmse: 0.54012 |  0:01:17s
epoch 50 | loss: 0.30068 | val_0_rmse: 0.56406 | val_1_rmse: 0.55572 |  0:01:18s
epoch 51 | loss: 0.31366 | val_0_rmse: 0.56788 | val_1_rmse: 0.56222 |  0:01:20s
epoch 52 | loss: 0.31237 | val_0_rmse: 0.54107 | val_1_rmse: 0.5378  |  0:01:21s
epoch 53 | loss: 0.30382 | val_0_rmse: 0.55032 | val_1_rmse: 0.54772 |  0:01:23s
epoch 54 | loss: 0.30681 | val_0_rmse: 0.54582 | val_1_rmse: 0.54053 |  0:01:25s
epoch 55 | loss: 0.31272 | val_0_rmse: 0.6492  | val_1_rmse: 0.652   |  0:01:26s
epoch 56 | loss: 0.31525 | val_0_rmse: 0.53688 | val_1_rmse: 0.531   |  0:01:28s
epoch 57 | loss: 0.30015 | val_0_rmse: 0.53633 | val_1_rmse: 0.53341 |  0:01:29s
epoch 58 | loss: 0.30289 | val_0_rmse: 0.53682 | val_1_rmse: 0.53435 |  0:01:31s
epoch 59 | loss: 0.29143 | val_0_rmse: 0.54095 | val_1_rmse: 0.53813 |  0:01:32s
epoch 60 | loss: 0.29466 | val_0_rmse: 0.52765 | val_1_rmse: 0.52342 |  0:01:34s
epoch 61 | loss: 0.30141 | val_0_rmse: 0.56034 | val_1_rmse: 0.55688 |  0:01:35s
epoch 62 | loss: 0.30342 | val_0_rmse: 0.53863 | val_1_rmse: 0.533   |  0:01:37s
epoch 63 | loss: 0.2998  | val_0_rmse: 0.55071 | val_1_rmse: 0.54151 |  0:01:38s
epoch 64 | loss: 0.29288 | val_0_rmse: 0.54437 | val_1_rmse: 0.54015 |  0:01:40s
epoch 65 | loss: 0.29931 | val_0_rmse: 0.5328  | val_1_rmse: 0.52983 |  0:01:42s
epoch 66 | loss: 0.29852 | val_0_rmse: 0.54512 | val_1_rmse: 0.53782 |  0:01:43s
epoch 67 | loss: 0.3117  | val_0_rmse: 0.57013 | val_1_rmse: 0.56382 |  0:01:45s
epoch 68 | loss: 0.3081  | val_0_rmse: 0.55071 | val_1_rmse: 0.54547 |  0:01:46s
epoch 69 | loss: 0.3107  | val_0_rmse: 0.53802 | val_1_rmse: 0.53571 |  0:01:48s
epoch 70 | loss: 0.30633 | val_0_rmse: 0.54555 | val_1_rmse: 0.54052 |  0:01:49s
epoch 71 | loss: 0.30214 | val_0_rmse: 0.53745 | val_1_rmse: 0.53251 |  0:01:51s
epoch 72 | loss: 0.29632 | val_0_rmse: 0.54481 | val_1_rmse: 0.53959 |  0:01:52s
epoch 73 | loss: 0.29204 | val_0_rmse: 0.56023 | val_1_rmse: 0.54983 |  0:01:54s
epoch 74 | loss: 0.29191 | val_0_rmse: 0.52869 | val_1_rmse: 0.52283 |  0:01:56s
epoch 75 | loss: 0.28792 | val_0_rmse: 0.52925 | val_1_rmse: 0.52412 |  0:01:57s
epoch 76 | loss: 0.28849 | val_0_rmse: 0.53141 | val_1_rmse: 0.5249  |  0:01:59s
epoch 77 | loss: 0.29998 | val_0_rmse: 0.55382 | val_1_rmse: 0.55234 |  0:02:00s
epoch 78 | loss: 0.29462 | val_0_rmse: 0.56007 | val_1_rmse: 0.55476 |  0:02:02s
epoch 79 | loss: 0.28979 | val_0_rmse: 0.53456 | val_1_rmse: 0.53099 |  0:02:03s
epoch 80 | loss: 0.30196 | val_0_rmse: 0.5337  | val_1_rmse: 0.53082 |  0:02:05s
epoch 81 | loss: 0.28726 | val_0_rmse: 0.52763 | val_1_rmse: 0.52459 |  0:02:06s
epoch 82 | loss: 0.29182 | val_0_rmse: 0.53038 | val_1_rmse: 0.52639 |  0:02:08s
epoch 83 | loss: 0.29918 | val_0_rmse: 0.53765 | val_1_rmse: 0.53149 |  0:02:09s
epoch 84 | loss: 0.29745 | val_0_rmse: 0.55382 | val_1_rmse: 0.5536  |  0:02:11s
epoch 85 | loss: 0.31763 | val_0_rmse: 0.54278 | val_1_rmse: 0.53467 |  0:02:13s
epoch 86 | loss: 0.31091 | val_0_rmse: 0.55818 | val_1_rmse: 0.55896 |  0:02:14s
epoch 87 | loss: 0.2964  | val_0_rmse: 0.54728 | val_1_rmse: 0.54218 |  0:02:16s
epoch 88 | loss: 0.30736 | val_0_rmse: 0.55135 | val_1_rmse: 0.54952 |  0:02:17s
epoch 89 | loss: 0.29771 | val_0_rmse: 0.53741 | val_1_rmse: 0.53622 |  0:02:19s
epoch 90 | loss: 0.29425 | val_0_rmse: 0.52998 | val_1_rmse: 0.52757 |  0:02:20s
epoch 91 | loss: 0.29261 | val_0_rmse: 0.53154 | val_1_rmse: 0.52714 |  0:02:22s
epoch 92 | loss: 0.30312 | val_0_rmse: 0.52764 | val_1_rmse: 0.52382 |  0:02:23s
epoch 93 | loss: 0.2889  | val_0_rmse: 0.53995 | val_1_rmse: 0.53608 |  0:02:25s
epoch 94 | loss: 0.30274 | val_0_rmse: 0.54533 | val_1_rmse: 0.54316 |  0:02:26s
epoch 95 | loss: 0.30303 | val_0_rmse: 0.55886 | val_1_rmse: 0.55971 |  0:02:28s
epoch 96 | loss: 0.28953 | val_0_rmse: 0.52722 | val_1_rmse: 0.52641 |  0:02:30s
epoch 97 | loss: 0.29139 | val_0_rmse: 0.54626 | val_1_rmse: 0.54341 |  0:02:31s
epoch 98 | loss: 0.29304 | val_0_rmse: 0.53922 | val_1_rmse: 0.53852 |  0:02:33s
epoch 99 | loss: 0.29361 | val_0_rmse: 0.5259  | val_1_rmse: 0.52396 |  0:02:34s
epoch 100| loss: 0.29365 | val_0_rmse: 0.52609 | val_1_rmse: 0.52503 |  0:02:36s
epoch 101| loss: 0.29698 | val_0_rmse: 0.57881 | val_1_rmse: 0.57345 |  0:02:37s
epoch 102| loss: 0.29419 | val_0_rmse: 0.53295 | val_1_rmse: 0.53036 |  0:02:39s
epoch 103| loss: 0.28038 | val_0_rmse: 0.52063 | val_1_rmse: 0.51953 |  0:02:40s
epoch 104| loss: 0.28059 | val_0_rmse: 0.51563 | val_1_rmse: 0.51408 |  0:02:42s
epoch 105| loss: 0.28635 | val_0_rmse: 0.53613 | val_1_rmse: 0.53478 |  0:02:43s
epoch 106| loss: 0.30693 | val_0_rmse: 0.55402 | val_1_rmse: 0.55377 |  0:02:45s
epoch 107| loss: 0.29259 | val_0_rmse: 0.52643 | val_1_rmse: 0.52337 |  0:02:47s
epoch 108| loss: 0.29452 | val_0_rmse: 0.53337 | val_1_rmse: 0.5353  |  0:02:48s
epoch 109| loss: 0.28896 | val_0_rmse: 0.54801 | val_1_rmse: 0.54375 |  0:02:50s
epoch 110| loss: 0.3045  | val_0_rmse: 0.54041 | val_1_rmse: 0.53835 |  0:02:51s
epoch 111| loss: 0.30104 | val_0_rmse: 0.53001 | val_1_rmse: 0.52487 |  0:02:53s
epoch 112| loss: 0.29587 | val_0_rmse: 0.52878 | val_1_rmse: 0.52365 |  0:02:54s
epoch 113| loss: 0.29034 | val_0_rmse: 0.56666 | val_1_rmse: 0.56889 |  0:02:56s
epoch 114| loss: 0.29771 | val_0_rmse: 0.52641 | val_1_rmse: 0.52395 |  0:02:57s
epoch 115| loss: 0.29057 | val_0_rmse: 0.53605 | val_1_rmse: 0.53194 |  0:02:59s
epoch 116| loss: 0.29135 | val_0_rmse: 0.5373  | val_1_rmse: 0.53477 |  0:03:00s
epoch 117| loss: 0.29382 | val_0_rmse: 0.53603 | val_1_rmse: 0.53473 |  0:03:02s
epoch 118| loss: 0.30083 | val_0_rmse: 0.53378 | val_1_rmse: 0.52689 |  0:03:04s
epoch 119| loss: 0.28597 | val_0_rmse: 0.52319 | val_1_rmse: 0.51945 |  0:03:05s
epoch 120| loss: 0.28135 | val_0_rmse: 0.53252 | val_1_rmse: 0.5305  |  0:03:07s
epoch 121| loss: 0.29234 | val_0_rmse: 0.52821 | val_1_rmse: 0.52408 |  0:03:08s
epoch 122| loss: 0.28858 | val_0_rmse: 0.52486 | val_1_rmse: 0.52314 |  0:03:10s
epoch 123| loss: 0.28897 | val_0_rmse: 0.53623 | val_1_rmse: 0.53367 |  0:03:11s
epoch 124| loss: 0.29277 | val_0_rmse: 0.53266 | val_1_rmse: 0.52976 |  0:03:13s
epoch 125| loss: 0.30508 | val_0_rmse: 0.54492 | val_1_rmse: 0.5408  |  0:03:14s
epoch 126| loss: 0.29906 | val_0_rmse: 0.54981 | val_1_rmse: 0.54806 |  0:03:16s
epoch 127| loss: 0.29958 | val_0_rmse: 0.54057 | val_1_rmse: 0.53921 |  0:03:17s
epoch 128| loss: 0.30844 | val_0_rmse: 0.53785 | val_1_rmse: 0.5331  |  0:03:19s
epoch 129| loss: 0.30757 | val_0_rmse: 0.54384 | val_1_rmse: 0.54189 |  0:03:21s
epoch 130| loss: 0.30097 | val_0_rmse: 0.52765 | val_1_rmse: 0.52649 |  0:03:22s
epoch 131| loss: 0.29182 | val_0_rmse: 0.52871 | val_1_rmse: 0.52519 |  0:03:24s
epoch 132| loss: 0.28279 | val_0_rmse: 0.53279 | val_1_rmse: 0.52963 |  0:03:25s
epoch 133| loss: 0.29015 | val_0_rmse: 0.52497 | val_1_rmse: 0.52095 |  0:03:27s
epoch 134| loss: 0.29452 | val_0_rmse: 0.53711 | val_1_rmse: 0.53672 |  0:03:28s

Early stopping occured at epoch 134 with best_epoch = 104 and best_val_1_rmse = 0.51408
Best weights from best epoch are automatically used!
ended training at: 10:13:35
Feature importance:
[('Area', 0.41688453672425085), ('Baths', 0.3356719821060862), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.23016378464288972), ('Month', 0.0), ('Year', 0.017279696526773216)]
Mean squared error is of 1061679646.7715847
Mean absolute error:22702.864740134915
MAPE:0.2846563286936331
R2 score:0.7394188912720967
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:13:35
epoch 0  | loss: 0.60335 | val_0_rmse: 0.76072 | val_1_rmse: 0.76899 |  0:00:01s
epoch 1  | loss: 0.36639 | val_0_rmse: 0.60182 | val_1_rmse: 0.61143 |  0:00:03s
epoch 2  | loss: 0.34866 | val_0_rmse: 0.58726 | val_1_rmse: 0.60433 |  0:00:04s
epoch 3  | loss: 0.33765 | val_0_rmse: 0.56527 | val_1_rmse: 0.57807 |  0:00:06s
epoch 4  | loss: 0.33904 | val_0_rmse: 0.57203 | val_1_rmse: 0.58471 |  0:00:07s
epoch 5  | loss: 0.31724 | val_0_rmse: 0.57299 | val_1_rmse: 0.5911  |  0:00:09s
epoch 6  | loss: 0.31856 | val_0_rmse: 0.55171 | val_1_rmse: 0.56658 |  0:00:10s
epoch 7  | loss: 0.31673 | val_0_rmse: 0.55166 | val_1_rmse: 0.56772 |  0:00:12s
epoch 8  | loss: 0.311   | val_0_rmse: 0.55133 | val_1_rmse: 0.56551 |  0:00:14s
epoch 9  | loss: 0.30486 | val_0_rmse: 0.5563  | val_1_rmse: 0.56437 |  0:00:15s
epoch 10 | loss: 0.30532 | val_0_rmse: 0.56663 | val_1_rmse: 0.57859 |  0:00:17s
epoch 11 | loss: 0.31072 | val_0_rmse: 0.55333 | val_1_rmse: 0.56445 |  0:00:18s
epoch 12 | loss: 0.29914 | val_0_rmse: 0.57331 | val_1_rmse: 0.58862 |  0:00:20s
epoch 13 | loss: 0.29984 | val_0_rmse: 0.53643 | val_1_rmse: 0.54462 |  0:00:21s
epoch 14 | loss: 0.30307 | val_0_rmse: 0.54866 | val_1_rmse: 0.56409 |  0:00:23s
epoch 15 | loss: 0.30307 | val_0_rmse: 0.54788 | val_1_rmse: 0.55967 |  0:00:24s
epoch 16 | loss: 0.29264 | val_0_rmse: 0.56015 | val_1_rmse: 0.57386 |  0:00:26s
epoch 17 | loss: 0.30236 | val_0_rmse: 0.5836  | val_1_rmse: 0.59632 |  0:00:27s
epoch 18 | loss: 0.29432 | val_0_rmse: 0.54003 | val_1_rmse: 0.55197 |  0:00:29s
epoch 19 | loss: 0.28515 | val_0_rmse: 0.52058 | val_1_rmse: 0.53377 |  0:00:31s
epoch 20 | loss: 0.30549 | val_0_rmse: 0.6743  | val_1_rmse: 0.6781  |  0:00:32s
epoch 21 | loss: 0.31077 | val_0_rmse: 0.53895 | val_1_rmse: 0.54835 |  0:00:34s
epoch 22 | loss: 0.29256 | val_0_rmse: 0.54692 | val_1_rmse: 0.56034 |  0:00:35s
epoch 23 | loss: 0.28998 | val_0_rmse: 0.61326 | val_1_rmse: 0.62174 |  0:00:37s
epoch 24 | loss: 0.31651 | val_0_rmse: 0.57099 | val_1_rmse: 0.58045 |  0:00:38s
epoch 25 | loss: 0.30462 | val_0_rmse: 0.55808 | val_1_rmse: 0.56991 |  0:00:40s
epoch 26 | loss: 0.29964 | val_0_rmse: 0.54529 | val_1_rmse: 0.55748 |  0:00:41s
epoch 27 | loss: 0.29743 | val_0_rmse: 0.54212 | val_1_rmse: 0.55316 |  0:00:43s
epoch 28 | loss: 0.30831 | val_0_rmse: 0.534   | val_1_rmse: 0.54073 |  0:00:44s
epoch 29 | loss: 0.29403 | val_0_rmse: 0.54391 | val_1_rmse: 0.54385 |  0:00:46s
epoch 30 | loss: 0.30255 | val_0_rmse: 0.59628 | val_1_rmse: 0.59672 |  0:00:48s
epoch 31 | loss: 0.30105 | val_0_rmse: 0.76665 | val_1_rmse: 0.7651  |  0:00:49s
epoch 32 | loss: 0.31318 | val_0_rmse: 0.63911 | val_1_rmse: 0.63451 |  0:00:51s
epoch 33 | loss: 0.28367 | val_0_rmse: 0.53291 | val_1_rmse: 0.5445  |  0:00:52s
epoch 34 | loss: 0.28579 | val_0_rmse: 0.57207 | val_1_rmse: 0.57462 |  0:00:54s
epoch 35 | loss: 0.27411 | val_0_rmse: 0.52446 | val_1_rmse: 0.53126 |  0:00:55s
epoch 36 | loss: 0.28013 | val_0_rmse: 0.54189 | val_1_rmse: 0.55368 |  0:00:57s
epoch 37 | loss: 0.27818 | val_0_rmse: 0.51278 | val_1_rmse: 0.52268 |  0:00:58s
epoch 38 | loss: 0.27742 | val_0_rmse: 0.68948 | val_1_rmse: 0.69874 |  0:01:00s
epoch 39 | loss: 0.27628 | val_0_rmse: 0.51356 | val_1_rmse: 0.51835 |  0:01:01s
epoch 40 | loss: 0.28713 | val_0_rmse: 0.58062 | val_1_rmse: 0.58215 |  0:01:03s
epoch 41 | loss: 0.2931  | val_0_rmse: 0.54234 | val_1_rmse: 0.55683 |  0:01:05s
epoch 42 | loss: 0.28736 | val_0_rmse: 0.68472 | val_1_rmse: 0.69394 |  0:01:06s
epoch 43 | loss: 0.28579 | val_0_rmse: 0.52702 | val_1_rmse: 0.54202 |  0:01:08s
epoch 44 | loss: 0.27968 | val_0_rmse: 0.56185 | val_1_rmse: 0.57408 |  0:01:09s
epoch 45 | loss: 0.27852 | val_0_rmse: 0.53345 | val_1_rmse: 0.54544 |  0:01:11s
epoch 46 | loss: 0.27628 | val_0_rmse: 0.5339  | val_1_rmse: 0.54618 |  0:01:12s
epoch 47 | loss: 0.27439 | val_0_rmse: 0.506   | val_1_rmse: 0.51704 |  0:01:14s
epoch 48 | loss: 0.2797  | val_0_rmse: 0.52043 | val_1_rmse: 0.5322  |  0:01:15s
epoch 49 | loss: 0.28121 | val_0_rmse: 0.52286 | val_1_rmse: 0.5322  |  0:01:17s
epoch 50 | loss: 0.27769 | val_0_rmse: 0.61122 | val_1_rmse: 0.60654 |  0:01:19s
epoch 51 | loss: 0.29473 | val_0_rmse: 0.54633 | val_1_rmse: 0.55434 |  0:01:20s
epoch 52 | loss: 0.27958 | val_0_rmse: 0.51747 | val_1_rmse: 0.53013 |  0:01:22s
epoch 53 | loss: 0.2708  | val_0_rmse: 0.51083 | val_1_rmse: 0.52384 |  0:01:23s
epoch 54 | loss: 0.26641 | val_0_rmse: 0.56467 | val_1_rmse: 0.57842 |  0:01:25s
epoch 55 | loss: 0.27737 | val_0_rmse: 0.51588 | val_1_rmse: 0.52572 |  0:01:26s
epoch 56 | loss: 0.27413 | val_0_rmse: 0.65467 | val_1_rmse: 0.65775 |  0:01:28s
epoch 57 | loss: 0.27508 | val_0_rmse: 0.54641 | val_1_rmse: 0.5623  |  0:01:29s
epoch 58 | loss: 0.2733  | val_0_rmse: 0.49764 | val_1_rmse: 0.50622 |  0:01:31s
epoch 59 | loss: 0.2734  | val_0_rmse: 0.52647 | val_1_rmse: 0.53102 |  0:01:33s
epoch 60 | loss: 0.26451 | val_0_rmse: 0.51725 | val_1_rmse: 0.52849 |  0:01:34s
epoch 61 | loss: 0.26709 | val_0_rmse: 0.52846 | val_1_rmse: 0.54139 |  0:01:36s
epoch 62 | loss: 0.27429 | val_0_rmse: 0.50715 | val_1_rmse: 0.52146 |  0:01:37s
epoch 63 | loss: 0.27538 | val_0_rmse: 0.58225 | val_1_rmse: 0.59055 |  0:01:39s
epoch 64 | loss: 0.26741 | val_0_rmse: 0.57766 | val_1_rmse: 0.58993 |  0:01:40s
epoch 65 | loss: 0.27732 | val_0_rmse: 0.52811 | val_1_rmse: 0.53364 |  0:01:42s
epoch 66 | loss: 0.26875 | val_0_rmse: 0.56755 | val_1_rmse: 0.58446 |  0:01:43s
epoch 67 | loss: 0.27484 | val_0_rmse: 0.50414 | val_1_rmse: 0.51094 |  0:01:45s
epoch 68 | loss: 0.26669 | val_0_rmse: 0.52197 | val_1_rmse: 0.53307 |  0:01:46s
epoch 69 | loss: 0.2602  | val_0_rmse: 0.49621 | val_1_rmse: 0.50712 |  0:01:48s
epoch 70 | loss: 0.26053 | val_0_rmse: 0.53443 | val_1_rmse: 0.55136 |  0:01:50s
epoch 71 | loss: 0.28907 | val_0_rmse: 0.59898 | val_1_rmse: 0.61158 |  0:01:51s
epoch 72 | loss: 0.27674 | val_0_rmse: 0.51657 | val_1_rmse: 0.53174 |  0:01:53s
epoch 73 | loss: 0.27971 | val_0_rmse: 0.55075 | val_1_rmse: 0.55806 |  0:01:54s
epoch 74 | loss: 0.27745 | val_0_rmse: 0.52466 | val_1_rmse: 0.53076 |  0:01:56s
epoch 75 | loss: 0.27197 | val_0_rmse: 0.58454 | val_1_rmse: 0.59798 |  0:01:57s
epoch 76 | loss: 0.2784  | val_0_rmse: 0.50159 | val_1_rmse: 0.51117 |  0:01:59s
epoch 77 | loss: 0.26969 | val_0_rmse: 0.56833 | val_1_rmse: 0.58028 |  0:02:00s
epoch 78 | loss: 0.27776 | val_0_rmse: 0.51066 | val_1_rmse: 0.52114 |  0:02:02s
epoch 79 | loss: 0.26843 | val_0_rmse: 0.66125 | val_1_rmse: 0.6767  |  0:02:03s
epoch 80 | loss: 0.28366 | val_0_rmse: 0.50729 | val_1_rmse: 0.51781 |  0:02:05s
epoch 81 | loss: 0.27483 | val_0_rmse: 0.57258 | val_1_rmse: 0.57625 |  0:02:07s
epoch 82 | loss: 0.27588 | val_0_rmse: 0.50202 | val_1_rmse: 0.51414 |  0:02:08s
epoch 83 | loss: 0.26995 | val_0_rmse: 0.51802 | val_1_rmse: 0.525   |  0:02:10s
epoch 84 | loss: 0.27162 | val_0_rmse: 0.50148 | val_1_rmse: 0.50966 |  0:02:11s
epoch 85 | loss: 0.26362 | val_0_rmse: 0.56096 | val_1_rmse: 0.57276 |  0:02:13s
epoch 86 | loss: 0.2661  | val_0_rmse: 0.60514 | val_1_rmse: 0.61547 |  0:02:14s
epoch 87 | loss: 0.26967 | val_0_rmse: 0.62678 | val_1_rmse: 0.63157 |  0:02:16s
epoch 88 | loss: 0.27638 | val_0_rmse: 0.53735 | val_1_rmse: 0.55062 |  0:02:17s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.50622
Best weights from best epoch are automatically used!
ended training at: 10:15:54
Feature importance:
[('Area', 0.48867619569123283), ('Baths', 0.26848685051808696), ('Beds', 0.05382540065639798), ('Latitude', 0.018978206006233875), ('Longitude', 0.16665922118731558), ('Month', 0.0), ('Year', 0.003374125940732806)]
Mean squared error is of 1042689808.5293117
Mean absolute error:22151.65331651925
MAPE:0.28458290177779555
R2 score:0.7478172386508543
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:15:54
epoch 0  | loss: 0.62669 | val_0_rmse: 0.70638 | val_1_rmse: 0.6949  |  0:00:01s
epoch 1  | loss: 0.37259 | val_0_rmse: 0.62781 | val_1_rmse: 0.63096 |  0:00:03s
epoch 2  | loss: 0.35246 | val_0_rmse: 0.59486 | val_1_rmse: 0.59148 |  0:00:04s
epoch 3  | loss: 0.33765 | val_0_rmse: 0.5665  | val_1_rmse: 0.56582 |  0:00:06s
epoch 4  | loss: 0.32733 | val_0_rmse: 0.56054 | val_1_rmse: 0.55943 |  0:00:07s
epoch 5  | loss: 0.32445 | val_0_rmse: 0.60212 | val_1_rmse: 0.60093 |  0:00:09s
epoch 6  | loss: 0.32281 | val_0_rmse: 0.5511  | val_1_rmse: 0.54971 |  0:00:10s
epoch 7  | loss: 0.31316 | val_0_rmse: 0.56135 | val_1_rmse: 0.56635 |  0:00:12s
epoch 8  | loss: 0.30793 | val_0_rmse: 0.58921 | val_1_rmse: 0.58035 |  0:00:13s
epoch 9  | loss: 0.3191  | val_0_rmse: 0.5646  | val_1_rmse: 0.55678 |  0:00:15s
epoch 10 | loss: 0.30734 | val_0_rmse: 0.57155 | val_1_rmse: 0.56727 |  0:00:17s
epoch 11 | loss: 0.3063  | val_0_rmse: 0.54685 | val_1_rmse: 0.54401 |  0:00:18s
epoch 12 | loss: 0.30543 | val_0_rmse: 0.55244 | val_1_rmse: 0.54685 |  0:00:20s
epoch 13 | loss: 0.31402 | val_0_rmse: 0.55747 | val_1_rmse: 0.55132 |  0:00:21s
epoch 14 | loss: 0.31599 | val_0_rmse: 0.55835 | val_1_rmse: 0.54993 |  0:00:23s
epoch 15 | loss: 0.30247 | val_0_rmse: 0.53823 | val_1_rmse: 0.53117 |  0:00:24s
epoch 16 | loss: 0.3076  | val_0_rmse: 0.54344 | val_1_rmse: 0.54323 |  0:00:26s
epoch 17 | loss: 0.2987  | val_0_rmse: 0.53953 | val_1_rmse: 0.53272 |  0:00:27s
epoch 18 | loss: 0.30359 | val_0_rmse: 0.54104 | val_1_rmse: 0.53751 |  0:00:29s
epoch 19 | loss: 0.29644 | val_0_rmse: 0.53217 | val_1_rmse: 0.53098 |  0:00:30s
epoch 20 | loss: 0.30269 | val_0_rmse: 0.55625 | val_1_rmse: 0.5499  |  0:00:32s
epoch 21 | loss: 0.29706 | val_0_rmse: 0.5499  | val_1_rmse: 0.54161 |  0:00:34s
epoch 22 | loss: 0.30031 | val_0_rmse: 0.53967 | val_1_rmse: 0.53848 |  0:00:35s
epoch 23 | loss: 0.29398 | val_0_rmse: 0.55083 | val_1_rmse: 0.55169 |  0:00:37s
epoch 24 | loss: 0.30111 | val_0_rmse: 0.54275 | val_1_rmse: 0.53855 |  0:00:38s
epoch 25 | loss: 0.30891 | val_0_rmse: 0.54621 | val_1_rmse: 0.54696 |  0:00:40s
epoch 26 | loss: 0.30729 | val_0_rmse: 0.55764 | val_1_rmse: 0.5537  |  0:00:41s
epoch 27 | loss: 0.31126 | val_0_rmse: 0.54429 | val_1_rmse: 0.53866 |  0:00:43s
epoch 28 | loss: 0.2984  | val_0_rmse: 0.55368 | val_1_rmse: 0.55205 |  0:00:44s
epoch 29 | loss: 0.29823 | val_0_rmse: 0.53028 | val_1_rmse: 0.52781 |  0:00:46s
epoch 30 | loss: 0.29836 | val_0_rmse: 0.53324 | val_1_rmse: 0.53058 |  0:00:47s
epoch 31 | loss: 0.29412 | val_0_rmse: 0.57011 | val_1_rmse: 0.56414 |  0:00:49s
epoch 32 | loss: 0.29766 | val_0_rmse: 0.58596 | val_1_rmse: 0.58853 |  0:00:51s
epoch 33 | loss: 0.30995 | val_0_rmse: 0.55218 | val_1_rmse: 0.55308 |  0:00:52s
epoch 34 | loss: 0.31519 | val_0_rmse: 0.54685 | val_1_rmse: 0.54958 |  0:00:54s
epoch 35 | loss: 0.31645 | val_0_rmse: 0.52992 | val_1_rmse: 0.52876 |  0:00:55s
epoch 36 | loss: 0.29347 | val_0_rmse: 0.5341  | val_1_rmse: 0.53321 |  0:00:57s
epoch 37 | loss: 0.298   | val_0_rmse: 0.53206 | val_1_rmse: 0.53003 |  0:00:58s
epoch 38 | loss: 0.29902 | val_0_rmse: 0.53989 | val_1_rmse: 0.53282 |  0:01:00s
epoch 39 | loss: 0.29203 | val_0_rmse: 0.57514 | val_1_rmse: 0.56865 |  0:01:01s
epoch 40 | loss: 0.3017  | val_0_rmse: 0.62511 | val_1_rmse: 0.61423 |  0:01:03s
epoch 41 | loss: 0.30088 | val_0_rmse: 0.58358 | val_1_rmse: 0.58228 |  0:01:05s
epoch 42 | loss: 0.31221 | val_0_rmse: 0.55464 | val_1_rmse: 0.54973 |  0:01:06s
epoch 43 | loss: 0.29456 | val_0_rmse: 0.57904 | val_1_rmse: 0.58795 |  0:01:08s
epoch 44 | loss: 0.30413 | val_0_rmse: 0.55161 | val_1_rmse: 0.55265 |  0:01:09s
epoch 45 | loss: 0.30021 | val_0_rmse: 0.5457  | val_1_rmse: 0.53892 |  0:01:11s
epoch 46 | loss: 0.29753 | val_0_rmse: 0.54171 | val_1_rmse: 0.54248 |  0:01:12s
epoch 47 | loss: 0.30068 | val_0_rmse: 0.54518 | val_1_rmse: 0.5434  |  0:01:14s
epoch 48 | loss: 0.30443 | val_0_rmse: 0.58862 | val_1_rmse: 0.59008 |  0:01:15s
epoch 49 | loss: 0.30267 | val_0_rmse: 0.52194 | val_1_rmse: 0.52014 |  0:01:17s
epoch 50 | loss: 0.28578 | val_0_rmse: 0.52783 | val_1_rmse: 0.52613 |  0:01:18s
epoch 51 | loss: 0.29208 | val_0_rmse: 0.56892 | val_1_rmse: 0.56732 |  0:01:20s
epoch 52 | loss: 0.30485 | val_0_rmse: 0.52836 | val_1_rmse: 0.52784 |  0:01:22s
epoch 53 | loss: 0.31723 | val_0_rmse: 0.65103 | val_1_rmse: 0.65158 |  0:01:23s
epoch 54 | loss: 0.31504 | val_0_rmse: 0.57636 | val_1_rmse: 0.56525 |  0:01:25s
epoch 55 | loss: 0.29465 | val_0_rmse: 0.57364 | val_1_rmse: 0.58123 |  0:01:26s
epoch 56 | loss: 0.30132 | val_0_rmse: 0.6092  | val_1_rmse: 0.60185 |  0:01:28s
epoch 57 | loss: 0.28945 | val_0_rmse: 0.58637 | val_1_rmse: 0.58038 |  0:01:29s
epoch 58 | loss: 0.28106 | val_0_rmse: 0.51488 | val_1_rmse: 0.51599 |  0:01:31s
epoch 59 | loss: 0.28514 | val_0_rmse: 0.52383 | val_1_rmse: 0.52739 |  0:01:32s
epoch 60 | loss: 0.28717 | val_0_rmse: 0.52745 | val_1_rmse: 0.52646 |  0:01:34s
epoch 61 | loss: 0.29844 | val_0_rmse: 0.53853 | val_1_rmse: 0.53788 |  0:01:35s
epoch 62 | loss: 0.29722 | val_0_rmse: 0.56206 | val_1_rmse: 0.55624 |  0:01:37s
epoch 63 | loss: 0.28267 | val_0_rmse: 0.56884 | val_1_rmse: 0.56329 |  0:01:39s
epoch 64 | loss: 0.28077 | val_0_rmse: 0.52148 | val_1_rmse: 0.5221  |  0:01:40s
epoch 65 | loss: 0.2701  | val_0_rmse: 0.52878 | val_1_rmse: 0.53358 |  0:01:42s
epoch 66 | loss: 0.26985 | val_0_rmse: 0.51858 | val_1_rmse: 0.5219  |  0:01:43s
epoch 67 | loss: 0.28693 | val_0_rmse: 0.53712 | val_1_rmse: 0.53355 |  0:01:45s
epoch 68 | loss: 0.27923 | val_0_rmse: 0.5095  | val_1_rmse: 0.51203 |  0:01:46s
epoch 69 | loss: 0.28086 | val_0_rmse: 0.81442 | val_1_rmse: 0.81265 |  0:01:48s
epoch 70 | loss: 0.2987  | val_0_rmse: 0.60887 | val_1_rmse: 0.59837 |  0:01:49s
epoch 71 | loss: 0.30029 | val_0_rmse: 0.98399 | val_1_rmse: 0.99091 |  0:01:51s
epoch 72 | loss: 0.31935 | val_0_rmse: 0.55893 | val_1_rmse: 0.5553  |  0:01:52s
epoch 73 | loss: 0.30388 | val_0_rmse: 0.53413 | val_1_rmse: 0.52691 |  0:01:54s
epoch 74 | loss: 0.2907  | val_0_rmse: 0.59357 | val_1_rmse: 0.58392 |  0:01:56s
epoch 75 | loss: 0.29348 | val_0_rmse: 0.52141 | val_1_rmse: 0.52058 |  0:01:57s
epoch 76 | loss: 0.27929 | val_0_rmse: 0.51108 | val_1_rmse: 0.50589 |  0:01:59s
epoch 77 | loss: 0.28214 | val_0_rmse: 0.52173 | val_1_rmse: 0.51607 |  0:02:00s
epoch 78 | loss: 0.28107 | val_0_rmse: 0.53203 | val_1_rmse: 0.53354 |  0:02:02s
epoch 79 | loss: 0.27787 | val_0_rmse: 0.56979 | val_1_rmse: 0.56098 |  0:02:03s
epoch 80 | loss: 0.27455 | val_0_rmse: 0.51009 | val_1_rmse: 0.50428 |  0:02:05s
epoch 81 | loss: 0.27745 | val_0_rmse: 0.52395 | val_1_rmse: 0.52163 |  0:02:06s
epoch 82 | loss: 0.28652 | val_0_rmse: 0.51326 | val_1_rmse: 0.51283 |  0:02:08s
epoch 83 | loss: 0.28017 | val_0_rmse: 0.53047 | val_1_rmse: 0.5277  |  0:02:09s
epoch 84 | loss: 0.28754 | val_0_rmse: 0.58961 | val_1_rmse: 0.58267 |  0:02:11s
epoch 85 | loss: 0.27616 | val_0_rmse: 0.54516 | val_1_rmse: 0.53545 |  0:02:13s
epoch 86 | loss: 0.26821 | val_0_rmse: 0.52249 | val_1_rmse: 0.51673 |  0:02:14s
epoch 87 | loss: 0.26807 | val_0_rmse: 0.51559 | val_1_rmse: 0.51113 |  0:02:16s
epoch 88 | loss: 0.26989 | val_0_rmse: 0.52024 | val_1_rmse: 0.51245 |  0:02:17s
epoch 89 | loss: 0.2762  | val_0_rmse: 0.50838 | val_1_rmse: 0.50527 |  0:02:19s
epoch 90 | loss: 0.2818  | val_0_rmse: 1.02886 | val_1_rmse: 1.03181 |  0:02:20s
epoch 91 | loss: 0.2786  | val_0_rmse: 0.57901 | val_1_rmse: 0.58428 |  0:02:22s
epoch 92 | loss: 0.28211 | val_0_rmse: 0.56018 | val_1_rmse: 0.54946 |  0:02:23s
epoch 93 | loss: 0.26844 | val_0_rmse: 0.53556 | val_1_rmse: 0.53633 |  0:02:25s
epoch 94 | loss: 0.28335 | val_0_rmse: 0.60474 | val_1_rmse: 0.61333 |  0:02:26s
epoch 95 | loss: 0.2657  | val_0_rmse: 0.53383 | val_1_rmse: 0.53754 |  0:02:28s
epoch 96 | loss: 0.28183 | val_0_rmse: 0.51351 | val_1_rmse: 0.5106  |  0:02:30s
epoch 97 | loss: 0.27686 | val_0_rmse: 0.60929 | val_1_rmse: 0.61342 |  0:02:31s
epoch 98 | loss: 0.27593 | val_0_rmse: 0.61064 | val_1_rmse: 0.61332 |  0:02:33s
epoch 99 | loss: 0.27084 | val_0_rmse: 0.60066 | val_1_rmse: 0.58959 |  0:02:34s
epoch 100| loss: 0.26588 | val_0_rmse: 0.50536 | val_1_rmse: 0.50037 |  0:02:36s
epoch 101| loss: 0.26367 | val_0_rmse: 0.55376 | val_1_rmse: 0.54839 |  0:02:37s
epoch 102| loss: 0.28235 | val_0_rmse: 0.54335 | val_1_rmse: 0.54748 |  0:02:39s
epoch 103| loss: 0.27684 | val_0_rmse: 0.59876 | val_1_rmse: 0.59061 |  0:02:40s
epoch 104| loss: 0.28686 | val_0_rmse: 0.60113 | val_1_rmse: 0.60687 |  0:02:42s
epoch 105| loss: 0.2837  | val_0_rmse: 0.51037 | val_1_rmse: 0.50616 |  0:02:43s
epoch 106| loss: 0.27274 | val_0_rmse: 0.5037  | val_1_rmse: 0.50239 |  0:02:45s
epoch 107| loss: 0.27965 | val_0_rmse: 0.58159 | val_1_rmse: 0.57981 |  0:02:47s
epoch 108| loss: 0.26322 | val_0_rmse: 0.50883 | val_1_rmse: 0.50506 |  0:02:48s
epoch 109| loss: 0.27894 | val_0_rmse: 0.542   | val_1_rmse: 0.53337 |  0:02:50s
epoch 110| loss: 0.27019 | val_0_rmse: 0.65862 | val_1_rmse: 0.65061 |  0:02:51s
epoch 111| loss: 0.27664 | val_0_rmse: 0.51823 | val_1_rmse: 0.51699 |  0:02:53s
epoch 112| loss: 0.28138 | val_0_rmse: 0.5258  | val_1_rmse: 0.52396 |  0:02:54s
epoch 113| loss: 0.27492 | val_0_rmse: 0.53322 | val_1_rmse: 0.52916 |  0:02:56s
epoch 114| loss: 0.27314 | val_0_rmse: 0.51588 | val_1_rmse: 0.5132  |  0:02:57s
epoch 115| loss: 0.27815 | val_0_rmse: 0.51314 | val_1_rmse: 0.50832 |  0:02:59s
epoch 116| loss: 0.26965 | val_0_rmse: 0.50415 | val_1_rmse: 0.50231 |  0:03:00s
epoch 117| loss: 0.27585 | val_0_rmse: 0.50639 | val_1_rmse: 0.50753 |  0:03:02s
epoch 118| loss: 0.26805 | val_0_rmse: 0.5096  | val_1_rmse: 0.50691 |  0:03:03s
epoch 119| loss: 0.28496 | val_0_rmse: 0.60715 | val_1_rmse: 0.60967 |  0:03:05s
epoch 120| loss: 0.26851 | val_0_rmse: 0.50751 | val_1_rmse: 0.50961 |  0:03:07s
epoch 121| loss: 0.2735  | val_0_rmse: 0.5178  | val_1_rmse: 0.5195  |  0:03:08s
epoch 122| loss: 0.27356 | val_0_rmse: 0.52763 | val_1_rmse: 0.5238  |  0:03:10s
epoch 123| loss: 0.27634 | val_0_rmse: 0.51328 | val_1_rmse: 0.5062  |  0:03:11s
epoch 124| loss: 0.27953 | val_0_rmse: 0.52564 | val_1_rmse: 0.52188 |  0:03:13s
epoch 125| loss: 0.27857 | val_0_rmse: 0.54277 | val_1_rmse: 0.54748 |  0:03:14s
epoch 126| loss: 0.26623 | val_0_rmse: 0.53114 | val_1_rmse: 0.52565 |  0:03:16s
epoch 127| loss: 0.27821 | val_0_rmse: 0.52578 | val_1_rmse: 0.52611 |  0:03:17s
epoch 128| loss: 0.26818 | val_0_rmse: 0.51253 | val_1_rmse: 0.51053 |  0:03:19s
epoch 129| loss: 0.27456 | val_0_rmse: 0.50942 | val_1_rmse: 0.50983 |  0:03:21s
epoch 130| loss: 0.26041 | val_0_rmse: 0.49537 | val_1_rmse: 0.49563 |  0:03:22s
epoch 131| loss: 0.26336 | val_0_rmse: 0.54559 | val_1_rmse: 0.54855 |  0:03:24s
epoch 132| loss: 0.27934 | val_0_rmse: 0.51679 | val_1_rmse: 0.51283 |  0:03:25s
epoch 133| loss: 0.2624  | val_0_rmse: 0.50477 | val_1_rmse: 0.50176 |  0:03:27s
epoch 134| loss: 0.26375 | val_0_rmse: 0.50705 | val_1_rmse: 0.5057  |  0:03:28s
epoch 135| loss: 0.27009 | val_0_rmse: 0.50921 | val_1_rmse: 0.51057 |  0:03:30s
epoch 136| loss: 0.26684 | val_0_rmse: 0.51347 | val_1_rmse: 0.51305 |  0:03:31s
epoch 137| loss: 0.26456 | val_0_rmse: 0.51885 | val_1_rmse: 0.51793 |  0:03:33s
epoch 138| loss: 0.27172 | val_0_rmse: 0.52039 | val_1_rmse: 0.52194 |  0:03:34s
epoch 139| loss: 0.26533 | val_0_rmse: 0.52904 | val_1_rmse: 0.5245  |  0:03:36s
epoch 140| loss: 0.26542 | val_0_rmse: 0.51798 | val_1_rmse: 0.52151 |  0:03:37s
epoch 141| loss: 0.26773 | val_0_rmse: 0.50406 | val_1_rmse: 0.50306 |  0:03:39s
epoch 142| loss: 0.2571  | val_0_rmse: 0.5247  | val_1_rmse: 0.52707 |  0:03:41s
epoch 143| loss: 0.26923 | val_0_rmse: 0.54408 | val_1_rmse: 0.54103 |  0:03:42s
epoch 144| loss: 0.27443 | val_0_rmse: 0.51497 | val_1_rmse: 0.51144 |  0:03:44s
epoch 145| loss: 0.26377 | val_0_rmse: 0.54604 | val_1_rmse: 0.54064 |  0:03:45s
epoch 146| loss: 0.27125 | val_0_rmse: 0.57005 | val_1_rmse: 0.56441 |  0:03:47s
epoch 147| loss: 0.28625 | val_0_rmse: 0.65014 | val_1_rmse: 0.64422 |  0:03:48s
epoch 148| loss: 0.27186 | val_0_rmse: 0.53622 | val_1_rmse: 0.53111 |  0:03:50s
epoch 149| loss: 0.26837 | val_0_rmse: 0.5344  | val_1_rmse: 0.53447 |  0:03:51s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.49563
Best weights from best epoch are automatically used!
ended training at: 10:19:46
Feature importance:
[('Area', 0.3677211622637033), ('Baths', 0.10654500127050033), ('Beds', 0.0), ('Latitude', 0.1156019414808336), ('Longitude', 0.22033364972348174), ('Month', 0.0), ('Year', 0.18979824526148098)]
Mean squared error is of 981741605.2569836
Mean absolute error:21235.20910054304
MAPE:0.26201768181405266
R2 score:0.7492230925647944
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:19:47
epoch 0  | loss: 0.45695 | val_0_rmse: 0.60483 | val_1_rmse: 0.61305 |  0:00:02s
epoch 1  | loss: 0.30638 | val_0_rmse: 0.51773 | val_1_rmse: 0.52094 |  0:00:04s
epoch 2  | loss: 0.27359 | val_0_rmse: 0.49507 | val_1_rmse: 0.5019  |  0:00:07s
epoch 3  | loss: 0.25647 | val_0_rmse: 0.48516 | val_1_rmse: 0.49066 |  0:00:09s
epoch 4  | loss: 0.24762 | val_0_rmse: 0.48456 | val_1_rmse: 0.49056 |  0:00:12s
epoch 5  | loss: 0.24501 | val_0_rmse: 0.47761 | val_1_rmse: 0.48169 |  0:00:14s
epoch 6  | loss: 0.24284 | val_0_rmse: 0.46463 | val_1_rmse: 0.46885 |  0:00:17s
epoch 7  | loss: 0.23518 | val_0_rmse: 0.46478 | val_1_rmse: 0.47027 |  0:00:19s
epoch 8  | loss: 0.22694 | val_0_rmse: 0.46575 | val_1_rmse: 0.46832 |  0:00:22s
epoch 9  | loss: 0.22716 | val_0_rmse: 0.45224 | val_1_rmse: 0.45698 |  0:00:24s
epoch 10 | loss: 0.21639 | val_0_rmse: 0.46181 | val_1_rmse: 0.46796 |  0:00:27s
epoch 11 | loss: 0.221   | val_0_rmse: 0.45293 | val_1_rmse: 0.4573  |  0:00:29s
epoch 12 | loss: 0.22152 | val_0_rmse: 0.45147 | val_1_rmse: 0.45513 |  0:00:32s
epoch 13 | loss: 0.22059 | val_0_rmse: 0.4522  | val_1_rmse: 0.45796 |  0:00:34s
epoch 14 | loss: 0.22378 | val_0_rmse: 0.44115 | val_1_rmse: 0.4468  |  0:00:37s
epoch 15 | loss: 0.21323 | val_0_rmse: 0.44475 | val_1_rmse: 0.45226 |  0:00:39s
epoch 16 | loss: 0.21252 | val_0_rmse: 0.47905 | val_1_rmse: 0.48656 |  0:00:41s
epoch 17 | loss: 0.21121 | val_0_rmse: 0.45184 | val_1_rmse: 0.45557 |  0:00:44s
epoch 18 | loss: 0.20947 | val_0_rmse: 0.45228 | val_1_rmse: 0.46147 |  0:00:46s
epoch 19 | loss: 0.20865 | val_0_rmse: 0.43613 | val_1_rmse: 0.44357 |  0:00:49s
epoch 20 | loss: 0.21298 | val_0_rmse: 0.44021 | val_1_rmse: 0.44886 |  0:00:51s
epoch 21 | loss: 0.20948 | val_0_rmse: 0.44221 | val_1_rmse: 0.44819 |  0:00:54s
epoch 22 | loss: 0.20745 | val_0_rmse: 0.43581 | val_1_rmse: 0.44374 |  0:00:56s
epoch 23 | loss: 0.20626 | val_0_rmse: 0.44424 | val_1_rmse: 0.45203 |  0:00:59s
epoch 24 | loss: 0.20822 | val_0_rmse: 0.44195 | val_1_rmse: 0.44985 |  0:01:01s
epoch 25 | loss: 0.20922 | val_0_rmse: 0.47258 | val_1_rmse: 0.47831 |  0:01:04s
epoch 26 | loss: 0.20101 | val_0_rmse: 0.4347  | val_1_rmse: 0.44326 |  0:01:06s
epoch 27 | loss: 0.21176 | val_0_rmse: 0.43753 | val_1_rmse: 0.44851 |  0:01:09s
epoch 28 | loss: 0.20745 | val_0_rmse: 0.43493 | val_1_rmse: 0.44443 |  0:01:11s
epoch 29 | loss: 0.20245 | val_0_rmse: 0.43708 | val_1_rmse: 0.44681 |  0:01:13s
epoch 30 | loss: 0.20071 | val_0_rmse: 0.43738 | val_1_rmse: 0.44704 |  0:01:16s
epoch 31 | loss: 0.20624 | val_0_rmse: 0.4356  | val_1_rmse: 0.44442 |  0:01:18s
epoch 32 | loss: 0.208   | val_0_rmse: 0.42686 | val_1_rmse: 0.43505 |  0:01:21s
epoch 33 | loss: 0.20349 | val_0_rmse: 0.44415 | val_1_rmse: 0.45424 |  0:01:23s
epoch 34 | loss: 0.20674 | val_0_rmse: 0.45734 | val_1_rmse: 0.46261 |  0:01:26s
epoch 35 | loss: 0.19789 | val_0_rmse: 0.44645 | val_1_rmse: 0.4579  |  0:01:28s
epoch 36 | loss: 0.19846 | val_0_rmse: 0.4438  | val_1_rmse: 0.45161 |  0:01:31s
epoch 37 | loss: 0.19987 | val_0_rmse: 0.46089 | val_1_rmse: 0.467   |  0:01:33s
epoch 38 | loss: 0.19952 | val_0_rmse: 0.42903 | val_1_rmse: 0.44085 |  0:01:36s
epoch 39 | loss: 0.20443 | val_0_rmse: 0.42859 | val_1_rmse: 0.43942 |  0:01:38s
epoch 40 | loss: 0.19982 | val_0_rmse: 0.43845 | val_1_rmse: 0.44785 |  0:01:41s
epoch 41 | loss: 0.20084 | val_0_rmse: 0.44919 | val_1_rmse: 0.45997 |  0:01:43s
epoch 42 | loss: 0.19832 | val_0_rmse: 0.43882 | val_1_rmse: 0.45264 |  0:01:45s
epoch 43 | loss: 0.19815 | val_0_rmse: 0.45814 | val_1_rmse: 0.46686 |  0:01:48s
epoch 44 | loss: 0.19946 | val_0_rmse: 0.43098 | val_1_rmse: 0.43985 |  0:01:50s
epoch 45 | loss: 0.19708 | val_0_rmse: 0.42376 | val_1_rmse: 0.43756 |  0:01:53s
epoch 46 | loss: 0.1952  | val_0_rmse: 0.42424 | val_1_rmse: 0.43496 |  0:01:55s
epoch 47 | loss: 0.19606 | val_0_rmse: 0.42815 | val_1_rmse: 0.43871 |  0:01:58s
epoch 48 | loss: 0.19398 | val_0_rmse: 0.42158 | val_1_rmse: 0.43323 |  0:02:00s
epoch 49 | loss: 0.1935  | val_0_rmse: 0.41875 | val_1_rmse: 0.42944 |  0:02:03s
epoch 50 | loss: 0.19107 | val_0_rmse: 0.41944 | val_1_rmse: 0.43352 |  0:02:05s
epoch 51 | loss: 0.19626 | val_0_rmse: 0.44183 | val_1_rmse: 0.45171 |  0:02:08s
epoch 52 | loss: 0.19793 | val_0_rmse: 0.43945 | val_1_rmse: 0.45111 |  0:02:10s
epoch 53 | loss: 0.20227 | val_0_rmse: 0.43672 | val_1_rmse: 0.44694 |  0:02:13s
epoch 54 | loss: 0.19064 | val_0_rmse: 0.41835 | val_1_rmse: 0.43018 |  0:02:15s
epoch 55 | loss: 0.19243 | val_0_rmse: 0.42326 | val_1_rmse: 0.43527 |  0:02:18s
epoch 56 | loss: 0.19426 | val_0_rmse: 0.43147 | val_1_rmse: 0.43839 |  0:02:20s
epoch 57 | loss: 0.19457 | val_0_rmse: 0.43891 | val_1_rmse: 0.45081 |  0:02:22s
epoch 58 | loss: 0.19125 | val_0_rmse: 0.41933 | val_1_rmse: 0.43563 |  0:02:25s
epoch 59 | loss: 0.19294 | val_0_rmse: 0.42414 | val_1_rmse: 0.43665 |  0:02:27s
epoch 60 | loss: 0.19406 | val_0_rmse: 0.42252 | val_1_rmse: 0.43502 |  0:02:30s
epoch 61 | loss: 0.19334 | val_0_rmse: 0.42384 | val_1_rmse: 0.43656 |  0:02:32s
epoch 62 | loss: 0.19337 | val_0_rmse: 0.42676 | val_1_rmse: 0.43951 |  0:02:35s
epoch 63 | loss: 0.19518 | val_0_rmse: 0.44582 | val_1_rmse: 0.46055 |  0:02:37s
epoch 64 | loss: 0.19226 | val_0_rmse: 0.4135  | val_1_rmse: 0.42942 |  0:02:40s
epoch 65 | loss: 0.18895 | val_0_rmse: 0.41397 | val_1_rmse: 0.42955 |  0:02:42s
epoch 66 | loss: 0.18906 | val_0_rmse: 0.42575 | val_1_rmse: 0.43631 |  0:02:45s
epoch 67 | loss: 0.19674 | val_0_rmse: 0.42121 | val_1_rmse: 0.4356  |  0:02:47s
epoch 68 | loss: 0.19155 | val_0_rmse: 0.4242  | val_1_rmse: 0.4394  |  0:02:50s
epoch 69 | loss: 0.19024 | val_0_rmse: 0.41579 | val_1_rmse: 0.43015 |  0:02:52s
epoch 70 | loss: 0.1925  | val_0_rmse: 0.42187 | val_1_rmse: 0.43867 |  0:02:54s
epoch 71 | loss: 0.19416 | val_0_rmse: 0.43771 | val_1_rmse: 0.45194 |  0:02:57s
epoch 72 | loss: 0.19159 | val_0_rmse: 0.42506 | val_1_rmse: 0.43886 |  0:02:59s
epoch 73 | loss: 0.19509 | val_0_rmse: 0.42418 | val_1_rmse: 0.43814 |  0:03:02s
epoch 74 | loss: 0.18526 | val_0_rmse: 0.41243 | val_1_rmse: 0.42685 |  0:03:04s
epoch 75 | loss: 0.18458 | val_0_rmse: 0.42118 | val_1_rmse: 0.43527 |  0:03:07s
epoch 76 | loss: 0.18643 | val_0_rmse: 0.42383 | val_1_rmse: 0.43841 |  0:03:09s
epoch 77 | loss: 0.18883 | val_0_rmse: 0.41148 | val_1_rmse: 0.42649 |  0:03:12s
epoch 78 | loss: 0.18538 | val_0_rmse: 0.41111 | val_1_rmse: 0.42893 |  0:03:14s
epoch 79 | loss: 0.18652 | val_0_rmse: 0.42015 | val_1_rmse: 0.43321 |  0:03:17s
epoch 80 | loss: 0.19216 | val_0_rmse: 0.43458 | val_1_rmse: 0.44814 |  0:03:19s
epoch 81 | loss: 0.19259 | val_0_rmse: 0.42914 | val_1_rmse: 0.44297 |  0:03:22s
epoch 82 | loss: 0.19691 | val_0_rmse: 0.41802 | val_1_rmse: 0.433   |  0:03:24s
epoch 83 | loss: 0.189   | val_0_rmse: 0.42098 | val_1_rmse: 0.43698 |  0:03:27s
epoch 84 | loss: 0.19201 | val_0_rmse: 0.42123 | val_1_rmse: 0.43646 |  0:03:29s
epoch 85 | loss: 0.19102 | val_0_rmse: 0.41269 | val_1_rmse: 0.4304  |  0:03:31s
epoch 86 | loss: 0.19461 | val_0_rmse: 0.41954 | val_1_rmse: 0.43738 |  0:03:34s
epoch 87 | loss: 0.18492 | val_0_rmse: 0.43916 | val_1_rmse: 0.4535  |  0:03:36s
epoch 88 | loss: 0.19016 | val_0_rmse: 0.42309 | val_1_rmse: 0.43786 |  0:03:39s
epoch 89 | loss: 0.18763 | val_0_rmse: 0.42139 | val_1_rmse: 0.436   |  0:03:41s
epoch 90 | loss: 0.18376 | val_0_rmse: 0.40796 | val_1_rmse: 0.42346 |  0:03:44s
epoch 91 | loss: 0.18399 | val_0_rmse: 0.4228  | val_1_rmse: 0.43724 |  0:03:46s
epoch 92 | loss: 0.19022 | val_0_rmse: 0.41658 | val_1_rmse: 0.4339  |  0:03:49s
epoch 93 | loss: 0.18807 | val_0_rmse: 0.41528 | val_1_rmse: 0.43437 |  0:03:51s
epoch 94 | loss: 0.18442 | val_0_rmse: 0.40941 | val_1_rmse: 0.42813 |  0:03:54s
epoch 95 | loss: 0.18215 | val_0_rmse: 0.45094 | val_1_rmse: 0.46521 |  0:03:56s
epoch 96 | loss: 0.18723 | val_0_rmse: 0.41674 | val_1_rmse: 0.43682 |  0:03:59s
epoch 97 | loss: 0.18453 | val_0_rmse: 0.42294 | val_1_rmse: 0.44012 |  0:04:01s
epoch 98 | loss: 0.18557 | val_0_rmse: 0.44338 | val_1_rmse: 0.45902 |  0:04:04s
epoch 99 | loss: 0.18833 | val_0_rmse: 0.41524 | val_1_rmse: 0.4279  |  0:04:06s
epoch 100| loss: 0.18793 | val_0_rmse: 0.41945 | val_1_rmse: 0.43566 |  0:04:08s
epoch 101| loss: 0.18615 | val_0_rmse: 0.40694 | val_1_rmse: 0.42425 |  0:04:11s
epoch 102| loss: 0.18698 | val_0_rmse: 0.41894 | val_1_rmse: 0.43616 |  0:04:13s
epoch 103| loss: 0.18664 | val_0_rmse: 0.44572 | val_1_rmse: 0.45825 |  0:04:16s
epoch 104| loss: 0.18858 | val_0_rmse: 0.45537 | val_1_rmse: 0.46854 |  0:04:18s
epoch 105| loss: 0.18991 | val_0_rmse: 0.41342 | val_1_rmse: 0.42653 |  0:04:21s
epoch 106| loss: 0.18574 | val_0_rmse: 0.41153 | val_1_rmse: 0.42708 |  0:04:23s
epoch 107| loss: 0.17852 | val_0_rmse: 0.40516 | val_1_rmse: 0.42087 |  0:04:26s
epoch 108| loss: 0.18505 | val_0_rmse: 0.44471 | val_1_rmse: 0.46076 |  0:04:28s
epoch 109| loss: 0.18666 | val_0_rmse: 0.40482 | val_1_rmse: 0.42271 |  0:04:31s
epoch 110| loss: 0.18455 | val_0_rmse: 0.50663 | val_1_rmse: 0.52321 |  0:04:33s
epoch 111| loss: 0.19072 | val_0_rmse: 0.42044 | val_1_rmse: 0.43592 |  0:04:36s
epoch 112| loss: 0.19286 | val_0_rmse: 0.41105 | val_1_rmse: 0.43151 |  0:04:38s
epoch 113| loss: 0.18425 | val_0_rmse: 0.40715 | val_1_rmse: 0.42614 |  0:04:40s
epoch 114| loss: 0.18284 | val_0_rmse: 0.4055  | val_1_rmse: 0.42283 |  0:04:43s
epoch 115| loss: 0.18093 | val_0_rmse: 0.41103 | val_1_rmse: 0.42855 |  0:04:45s
epoch 116| loss: 0.18258 | val_0_rmse: 0.40878 | val_1_rmse: 0.42767 |  0:04:48s
epoch 117| loss: 0.18237 | val_0_rmse: 0.42887 | val_1_rmse: 0.44475 |  0:04:50s
epoch 118| loss: 0.18033 | val_0_rmse: 0.4083  | val_1_rmse: 0.42704 |  0:04:53s
epoch 119| loss: 0.18438 | val_0_rmse: 0.40742 | val_1_rmse: 0.42506 |  0:04:55s
epoch 120| loss: 0.18723 | val_0_rmse: 0.41581 | val_1_rmse: 0.43539 |  0:04:58s
epoch 121| loss: 0.17966 | val_0_rmse: 0.41197 | val_1_rmse: 0.43116 |  0:05:00s
epoch 122| loss: 0.18413 | val_0_rmse: 0.40707 | val_1_rmse: 0.42411 |  0:05:03s
epoch 123| loss: 0.19041 | val_0_rmse: 0.40953 | val_1_rmse: 0.42812 |  0:05:05s
epoch 124| loss: 0.18389 | val_0_rmse: 0.40269 | val_1_rmse: 0.42349 |  0:05:08s
epoch 125| loss: 0.18095 | val_0_rmse: 0.40432 | val_1_rmse: 0.4261  |  0:05:10s
epoch 126| loss: 0.18248 | val_0_rmse: 0.41643 | val_1_rmse: 0.43641 |  0:05:12s
epoch 127| loss: 0.1844  | val_0_rmse: 0.41571 | val_1_rmse: 0.43331 |  0:05:15s
epoch 128| loss: 0.18644 | val_0_rmse: 0.42165 | val_1_rmse: 0.44054 |  0:05:17s
epoch 129| loss: 0.18441 | val_0_rmse: 0.41346 | val_1_rmse: 0.42907 |  0:05:20s
epoch 130| loss: 0.18817 | val_0_rmse: 0.41217 | val_1_rmse: 0.43531 |  0:05:22s
epoch 131| loss: 0.18262 | val_0_rmse: 0.41271 | val_1_rmse: 0.43111 |  0:05:25s
epoch 132| loss: 0.18516 | val_0_rmse: 0.40938 | val_1_rmse: 0.42774 |  0:05:27s
epoch 133| loss: 0.18603 | val_0_rmse: 0.41738 | val_1_rmse: 0.4346  |  0:05:30s
epoch 134| loss: 0.17964 | val_0_rmse: 0.40145 | val_1_rmse: 0.42119 |  0:05:32s
epoch 135| loss: 0.17809 | val_0_rmse: 0.42605 | val_1_rmse: 0.44188 |  0:05:35s
epoch 136| loss: 0.17753 | val_0_rmse: 0.41379 | val_1_rmse: 0.43036 |  0:05:37s
epoch 137| loss: 0.1794  | val_0_rmse: 0.41583 | val_1_rmse: 0.43444 |  0:05:39s

Early stopping occured at epoch 137 with best_epoch = 107 and best_val_1_rmse = 0.42087
Best weights from best epoch are automatically used!
ended training at: 10:25:27
Feature importance:
[('Area', 0.03191843381475019), ('Baths', 0.25742437876471275), ('Beds', 0.09520717785787583), ('Latitude', 0.0699798903774249), ('Longitude', 0.19922532465118362), ('Month', 0.037220226990770704), ('Year', 0.309024567543282)]
Mean squared error is of 9816027368.60562
Mean absolute error:68960.83847306365
MAPE:0.2893135231602705
R2 score:0.8284706409900029
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:25:28
epoch 0  | loss: 0.47122 | val_0_rmse: 0.56688 | val_1_rmse: 0.58235 |  0:00:02s
epoch 1  | loss: 0.30795 | val_0_rmse: 0.52326 | val_1_rmse: 0.53166 |  0:00:04s
epoch 2  | loss: 0.27345 | val_0_rmse: 0.495   | val_1_rmse: 0.50239 |  0:00:07s
epoch 3  | loss: 0.25472 | val_0_rmse: 0.49116 | val_1_rmse: 0.50002 |  0:00:09s
epoch 4  | loss: 0.24862 | val_0_rmse: 0.51098 | val_1_rmse: 0.51901 |  0:00:12s
epoch 5  | loss: 0.24872 | val_0_rmse: 0.47515 | val_1_rmse: 0.48487 |  0:00:14s
epoch 6  | loss: 0.24023 | val_0_rmse: 0.45448 | val_1_rmse: 0.46557 |  0:00:17s
epoch 7  | loss: 0.23378 | val_0_rmse: 0.48685 | val_1_rmse: 0.49676 |  0:00:19s
epoch 8  | loss: 0.24363 | val_0_rmse: 0.45419 | val_1_rmse: 0.46318 |  0:00:22s
epoch 9  | loss: 0.2249  | val_0_rmse: 0.46056 | val_1_rmse: 0.46705 |  0:00:24s
epoch 10 | loss: 0.22743 | val_0_rmse: 0.46182 | val_1_rmse: 0.47529 |  0:00:27s
epoch 11 | loss: 0.22292 | val_0_rmse: 0.44922 | val_1_rmse: 0.45713 |  0:00:29s
epoch 12 | loss: 0.22018 | val_0_rmse: 0.48714 | val_1_rmse: 0.49471 |  0:00:32s
epoch 13 | loss: 0.22948 | val_0_rmse: 0.44939 | val_1_rmse: 0.45963 |  0:00:34s
epoch 14 | loss: 0.21872 | val_0_rmse: 0.43555 | val_1_rmse: 0.44562 |  0:00:36s
epoch 15 | loss: 0.21035 | val_0_rmse: 0.43303 | val_1_rmse: 0.44368 |  0:00:39s
epoch 16 | loss: 0.21098 | val_0_rmse: 0.43965 | val_1_rmse: 0.45182 |  0:00:41s
epoch 17 | loss: 0.2139  | val_0_rmse: 0.45274 | val_1_rmse: 0.4611  |  0:00:44s
epoch 18 | loss: 0.21234 | val_0_rmse: 0.46624 | val_1_rmse: 0.47454 |  0:00:46s
epoch 19 | loss: 0.21019 | val_0_rmse: 0.45346 | val_1_rmse: 0.46779 |  0:00:49s
epoch 20 | loss: 0.20422 | val_0_rmse: 0.46115 | val_1_rmse: 0.47147 |  0:00:51s
epoch 21 | loss: 0.2069  | val_0_rmse: 0.42935 | val_1_rmse: 0.44305 |  0:00:54s
epoch 22 | loss: 0.20645 | val_0_rmse: 0.44917 | val_1_rmse: 0.46003 |  0:00:56s
epoch 23 | loss: 0.20485 | val_0_rmse: 0.44951 | val_1_rmse: 0.4648  |  0:00:59s
epoch 24 | loss: 0.20391 | val_0_rmse: 0.43943 | val_1_rmse: 0.44872 |  0:01:01s
epoch 25 | loss: 0.20553 | val_0_rmse: 0.43123 | val_1_rmse: 0.44541 |  0:01:04s
epoch 26 | loss: 0.20384 | val_0_rmse: 0.42317 | val_1_rmse: 0.43346 |  0:01:06s
epoch 27 | loss: 0.1973  | val_0_rmse: 0.44082 | val_1_rmse: 0.45389 |  0:01:09s
epoch 28 | loss: 0.2014  | val_0_rmse: 0.4267  | val_1_rmse: 0.44026 |  0:01:11s
epoch 29 | loss: 0.20264 | val_0_rmse: 0.43285 | val_1_rmse: 0.44474 |  0:01:14s
epoch 30 | loss: 0.19755 | val_0_rmse: 0.42143 | val_1_rmse: 0.43523 |  0:01:16s
epoch 31 | loss: 0.20039 | val_0_rmse: 0.43847 | val_1_rmse: 0.452   |  0:01:18s
epoch 32 | loss: 0.20298 | val_0_rmse: 0.44549 | val_1_rmse: 0.45835 |  0:01:21s
epoch 33 | loss: 0.20083 | val_0_rmse: 0.42401 | val_1_rmse: 0.43536 |  0:01:23s
epoch 34 | loss: 0.19659 | val_0_rmse: 0.42223 | val_1_rmse: 0.43598 |  0:01:26s
epoch 35 | loss: 0.19724 | val_0_rmse: 0.4326  | val_1_rmse: 0.44643 |  0:01:28s
epoch 36 | loss: 0.19949 | val_0_rmse: 0.42178 | val_1_rmse: 0.43653 |  0:01:31s
epoch 37 | loss: 0.19564 | val_0_rmse: 0.42016 | val_1_rmse: 0.43429 |  0:01:33s
epoch 38 | loss: 0.19799 | val_0_rmse: 0.4207  | val_1_rmse: 0.43442 |  0:01:36s
epoch 39 | loss: 0.20153 | val_0_rmse: 0.41797 | val_1_rmse: 0.43101 |  0:01:38s
epoch 40 | loss: 0.19621 | val_0_rmse: 0.43268 | val_1_rmse: 0.44792 |  0:01:41s
epoch 41 | loss: 0.19219 | val_0_rmse: 0.41653 | val_1_rmse: 0.43305 |  0:01:43s
epoch 42 | loss: 0.19    | val_0_rmse: 0.43212 | val_1_rmse: 0.44536 |  0:01:45s
epoch 43 | loss: 0.19085 | val_0_rmse: 0.42246 | val_1_rmse: 0.43651 |  0:01:48s
epoch 44 | loss: 0.19368 | val_0_rmse: 0.41715 | val_1_rmse: 0.43164 |  0:01:50s
epoch 45 | loss: 0.19206 | val_0_rmse: 0.41921 | val_1_rmse: 0.4352  |  0:01:53s
epoch 46 | loss: 0.19356 | val_0_rmse: 0.42733 | val_1_rmse: 0.44297 |  0:01:55s
epoch 47 | loss: 0.1951  | val_0_rmse: 0.43277 | val_1_rmse: 0.44623 |  0:01:58s
epoch 48 | loss: 0.19304 | val_0_rmse: 0.41362 | val_1_rmse: 0.42894 |  0:02:00s
epoch 49 | loss: 0.18732 | val_0_rmse: 0.4181  | val_1_rmse: 0.43646 |  0:02:03s
epoch 50 | loss: 0.19144 | val_0_rmse: 0.42311 | val_1_rmse: 0.44046 |  0:02:05s
epoch 51 | loss: 0.1917  | val_0_rmse: 0.42751 | val_1_rmse: 0.44312 |  0:02:08s
epoch 52 | loss: 0.1884  | val_0_rmse: 0.41285 | val_1_rmse: 0.42928 |  0:02:10s
epoch 53 | loss: 0.19439 | val_0_rmse: 0.41565 | val_1_rmse: 0.43133 |  0:02:13s
epoch 54 | loss: 0.20014 | val_0_rmse: 0.4377  | val_1_rmse: 0.45614 |  0:02:15s
epoch 55 | loss: 0.19422 | val_0_rmse: 0.43106 | val_1_rmse: 0.44775 |  0:02:17s
epoch 56 | loss: 0.19301 | val_0_rmse: 0.42612 | val_1_rmse: 0.44605 |  0:02:20s
epoch 57 | loss: 0.19315 | val_0_rmse: 0.43029 | val_1_rmse: 0.44524 |  0:02:22s
epoch 58 | loss: 0.19036 | val_0_rmse: 0.42126 | val_1_rmse: 0.439   |  0:02:25s
epoch 59 | loss: 0.19203 | val_0_rmse: 0.41367 | val_1_rmse: 0.43078 |  0:02:27s
epoch 60 | loss: 0.18758 | val_0_rmse: 0.42715 | val_1_rmse: 0.44425 |  0:02:30s
epoch 61 | loss: 0.19467 | val_0_rmse: 0.45032 | val_1_rmse: 0.46567 |  0:02:32s
epoch 62 | loss: 0.21355 | val_0_rmse: 0.43359 | val_1_rmse: 0.4455  |  0:02:35s
epoch 63 | loss: 0.1997  | val_0_rmse: 0.42264 | val_1_rmse: 0.43782 |  0:02:37s
epoch 64 | loss: 0.19412 | val_0_rmse: 0.43512 | val_1_rmse: 0.44745 |  0:02:40s
epoch 65 | loss: 0.19298 | val_0_rmse: 0.42676 | val_1_rmse: 0.4438  |  0:02:42s
epoch 66 | loss: 0.18946 | val_0_rmse: 0.41172 | val_1_rmse: 0.43128 |  0:02:45s
epoch 67 | loss: 0.18747 | val_0_rmse: 0.41143 | val_1_rmse: 0.43038 |  0:02:47s
epoch 68 | loss: 0.18951 | val_0_rmse: 0.44875 | val_1_rmse: 0.46408 |  0:02:49s
epoch 69 | loss: 0.1943  | val_0_rmse: 0.41201 | val_1_rmse: 0.42648 |  0:02:52s
epoch 70 | loss: 0.18865 | val_0_rmse: 0.41787 | val_1_rmse: 0.4358  |  0:02:54s
epoch 71 | loss: 0.19254 | val_0_rmse: 0.42408 | val_1_rmse: 0.44314 |  0:02:57s
epoch 72 | loss: 0.18899 | val_0_rmse: 0.42086 | val_1_rmse: 0.44155 |  0:02:59s
epoch 73 | loss: 0.18428 | val_0_rmse: 0.41164 | val_1_rmse: 0.42991 |  0:03:02s
epoch 74 | loss: 0.18811 | val_0_rmse: 0.42749 | val_1_rmse: 0.44703 |  0:03:04s
epoch 75 | loss: 0.19211 | val_0_rmse: 0.42643 | val_1_rmse: 0.44335 |  0:03:07s
epoch 76 | loss: 0.18575 | val_0_rmse: 0.40925 | val_1_rmse: 0.42895 |  0:03:09s
epoch 77 | loss: 0.19466 | val_0_rmse: 0.40972 | val_1_rmse: 0.42775 |  0:03:12s
epoch 78 | loss: 0.18967 | val_0_rmse: 0.41499 | val_1_rmse: 0.43139 |  0:03:14s
epoch 79 | loss: 0.1867  | val_0_rmse: 0.41774 | val_1_rmse: 0.43378 |  0:03:16s
epoch 80 | loss: 0.18964 | val_0_rmse: 0.40437 | val_1_rmse: 0.42583 |  0:03:19s
epoch 81 | loss: 0.18614 | val_0_rmse: 0.44749 | val_1_rmse: 0.46208 |  0:03:21s
epoch 82 | loss: 0.18707 | val_0_rmse: 0.41961 | val_1_rmse: 0.43811 |  0:03:24s
epoch 83 | loss: 0.18367 | val_0_rmse: 0.4226  | val_1_rmse: 0.43951 |  0:03:26s
epoch 84 | loss: 0.18395 | val_0_rmse: 0.42461 | val_1_rmse: 0.44334 |  0:03:29s
epoch 85 | loss: 0.18323 | val_0_rmse: 0.42427 | val_1_rmse: 0.44358 |  0:03:31s
epoch 86 | loss: 0.18312 | val_0_rmse: 0.4215  | val_1_rmse: 0.44093 |  0:03:34s
epoch 87 | loss: 0.18454 | val_0_rmse: 0.40687 | val_1_rmse: 0.42812 |  0:03:36s
epoch 88 | loss: 0.18356 | val_0_rmse: 0.41427 | val_1_rmse: 0.43396 |  0:03:39s
epoch 89 | loss: 0.19277 | val_0_rmse: 0.40109 | val_1_rmse: 0.42195 |  0:03:41s
epoch 90 | loss: 0.18141 | val_0_rmse: 0.40357 | val_1_rmse: 0.42194 |  0:03:43s
epoch 91 | loss: 0.18035 | val_0_rmse: 0.40576 | val_1_rmse: 0.42511 |  0:03:46s
epoch 92 | loss: 0.18037 | val_0_rmse: 0.4209  | val_1_rmse: 0.43989 |  0:03:48s
epoch 93 | loss: 0.18217 | val_0_rmse: 0.39953 | val_1_rmse: 0.42229 |  0:03:51s
epoch 94 | loss: 0.18253 | val_0_rmse: 0.41293 | val_1_rmse: 0.43246 |  0:03:53s
epoch 95 | loss: 0.18113 | val_0_rmse: 0.4139  | val_1_rmse: 0.43799 |  0:03:56s
epoch 96 | loss: 0.22915 | val_0_rmse: 0.44598 | val_1_rmse: 0.45748 |  0:03:58s
epoch 97 | loss: 0.21513 | val_0_rmse: 0.44012 | val_1_rmse: 0.45558 |  0:04:01s
epoch 98 | loss: 0.20442 | val_0_rmse: 0.43002 | val_1_rmse: 0.44801 |  0:04:03s
epoch 99 | loss: 0.19869 | val_0_rmse: 0.42832 | val_1_rmse: 0.44344 |  0:04:06s
epoch 100| loss: 0.19639 | val_0_rmse: 0.41821 | val_1_rmse: 0.43268 |  0:04:08s
epoch 101| loss: 0.19508 | val_0_rmse: 0.44436 | val_1_rmse: 0.46067 |  0:04:11s
epoch 102| loss: 0.19373 | val_0_rmse: 0.42634 | val_1_rmse: 0.44079 |  0:04:13s
epoch 103| loss: 0.19166 | val_0_rmse: 0.40854 | val_1_rmse: 0.42817 |  0:04:15s
epoch 104| loss: 0.18881 | val_0_rmse: 0.4196  | val_1_rmse: 0.4381  |  0:04:18s
epoch 105| loss: 0.19202 | val_0_rmse: 0.41536 | val_1_rmse: 0.43416 |  0:04:20s
epoch 106| loss: 0.18563 | val_0_rmse: 0.4067  | val_1_rmse: 0.42668 |  0:04:23s
epoch 107| loss: 0.18478 | val_0_rmse: 0.41848 | val_1_rmse: 0.43623 |  0:04:25s
epoch 108| loss: 0.19376 | val_0_rmse: 0.42021 | val_1_rmse: 0.43689 |  0:04:28s
epoch 109| loss: 0.19065 | val_0_rmse: 0.41699 | val_1_rmse: 0.43591 |  0:04:30s
epoch 110| loss: 0.19238 | val_0_rmse: 0.40868 | val_1_rmse: 0.42926 |  0:04:33s
epoch 111| loss: 0.18636 | val_0_rmse: 0.41747 | val_1_rmse: 0.43517 |  0:04:35s
epoch 112| loss: 0.18783 | val_0_rmse: 0.4295  | val_1_rmse: 0.45075 |  0:04:38s
epoch 113| loss: 0.18668 | val_0_rmse: 0.41126 | val_1_rmse: 0.42994 |  0:04:40s
epoch 114| loss: 0.19118 | val_0_rmse: 0.4171  | val_1_rmse: 0.4367  |  0:04:43s
epoch 115| loss: 0.18616 | val_0_rmse: 0.40997 | val_1_rmse: 0.43135 |  0:04:45s
epoch 116| loss: 0.18748 | val_0_rmse: 0.4175  | val_1_rmse: 0.43713 |  0:04:48s
epoch 117| loss: 0.18822 | val_0_rmse: 0.41896 | val_1_rmse: 0.43859 |  0:04:50s
epoch 118| loss: 0.18364 | val_0_rmse: 0.40177 | val_1_rmse: 0.4234  |  0:04:52s
epoch 119| loss: 0.18478 | val_0_rmse: 0.41089 | val_1_rmse: 0.43537 |  0:04:55s
epoch 120| loss: 0.18632 | val_0_rmse: 0.41222 | val_1_rmse: 0.43141 |  0:04:57s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.42194
Best weights from best epoch are automatically used!
ended training at: 10:30:27
Feature importance:
[('Area', 0.0), ('Baths', 0.1452135315309903), ('Beds', 0.19238754950293746), ('Latitude', 0.16593502009134148), ('Longitude', 0.24115943597692943), ('Month', 0.0), ('Year', 0.25530446289780134)]
Mean squared error is of 10114280885.702118
Mean absolute error:69452.63185149973
MAPE:0.28767603458667285
R2 score:0.8209704286219627
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:30:27
epoch 0  | loss: 0.47009 | val_0_rmse: 0.62675 | val_1_rmse: 0.62815 |  0:00:02s
epoch 1  | loss: 0.31835 | val_0_rmse: 0.53952 | val_1_rmse: 0.54331 |  0:00:04s
epoch 2  | loss: 0.28723 | val_0_rmse: 0.5294  | val_1_rmse: 0.53105 |  0:00:07s
epoch 3  | loss: 0.27396 | val_0_rmse: 0.52524 | val_1_rmse: 0.52602 |  0:00:09s
epoch 4  | loss: 0.27618 | val_0_rmse: 0.50051 | val_1_rmse: 0.50288 |  0:00:12s
epoch 5  | loss: 0.26656 | val_0_rmse: 0.52829 | val_1_rmse: 0.52276 |  0:00:14s
epoch 6  | loss: 0.2476  | val_0_rmse: 0.4902  | val_1_rmse: 0.49196 |  0:00:17s
epoch 7  | loss: 0.24366 | val_0_rmse: 0.46079 | val_1_rmse: 0.46023 |  0:00:19s
epoch 8  | loss: 0.23    | val_0_rmse: 0.46324 | val_1_rmse: 0.46291 |  0:00:22s
epoch 9  | loss: 0.23362 | val_0_rmse: 0.44817 | val_1_rmse: 0.4464  |  0:00:24s
epoch 10 | loss: 0.22364 | val_0_rmse: 0.44993 | val_1_rmse: 0.45001 |  0:00:27s
epoch 11 | loss: 0.21901 | val_0_rmse: 0.45926 | val_1_rmse: 0.45699 |  0:00:29s
epoch 12 | loss: 0.22266 | val_0_rmse: 0.45001 | val_1_rmse: 0.45078 |  0:00:32s
epoch 13 | loss: 0.2172  | val_0_rmse: 0.45523 | val_1_rmse: 0.45308 |  0:00:34s
epoch 14 | loss: 0.21662 | val_0_rmse: 0.45211 | val_1_rmse: 0.45231 |  0:00:37s
epoch 15 | loss: 0.21416 | val_0_rmse: 0.46805 | val_1_rmse: 0.46499 |  0:00:39s
epoch 16 | loss: 0.22196 | val_0_rmse: 0.43953 | val_1_rmse: 0.43954 |  0:00:41s
epoch 17 | loss: 0.21134 | val_0_rmse: 0.43895 | val_1_rmse: 0.44114 |  0:00:44s
epoch 18 | loss: 0.21376 | val_0_rmse: 0.44201 | val_1_rmse: 0.44198 |  0:00:46s
epoch 19 | loss: 0.2128  | val_0_rmse: 0.43989 | val_1_rmse: 0.44198 |  0:00:49s
epoch 20 | loss: 0.20475 | val_0_rmse: 0.46461 | val_1_rmse: 0.46753 |  0:00:51s
epoch 21 | loss: 0.20962 | val_0_rmse: 0.45204 | val_1_rmse: 0.45516 |  0:00:54s
epoch 22 | loss: 0.21219 | val_0_rmse: 0.43784 | val_1_rmse: 0.44022 |  0:00:56s
epoch 23 | loss: 0.20549 | val_0_rmse: 0.45972 | val_1_rmse: 0.4597  |  0:00:59s
epoch 24 | loss: 0.20997 | val_0_rmse: 0.44421 | val_1_rmse: 0.44602 |  0:01:01s
epoch 25 | loss: 0.21147 | val_0_rmse: 0.44372 | val_1_rmse: 0.44352 |  0:01:04s
epoch 26 | loss: 0.21231 | val_0_rmse: 0.45147 | val_1_rmse: 0.45388 |  0:01:06s
epoch 27 | loss: 0.20627 | val_0_rmse: 0.43454 | val_1_rmse: 0.437   |  0:01:09s
epoch 28 | loss: 0.20834 | val_0_rmse: 0.43655 | val_1_rmse: 0.43945 |  0:01:11s
epoch 29 | loss: 0.20914 | val_0_rmse: 0.44778 | val_1_rmse: 0.4491  |  0:01:13s
epoch 30 | loss: 0.20335 | val_0_rmse: 0.43509 | val_1_rmse: 0.43968 |  0:01:16s
epoch 31 | loss: 0.20627 | val_0_rmse: 0.43459 | val_1_rmse: 0.43924 |  0:01:18s
epoch 32 | loss: 0.2087  | val_0_rmse: 0.45571 | val_1_rmse: 0.45839 |  0:01:21s
epoch 33 | loss: 0.20257 | val_0_rmse: 0.43271 | val_1_rmse: 0.43518 |  0:01:23s
epoch 34 | loss: 0.20228 | val_0_rmse: 0.44279 | val_1_rmse: 0.44544 |  0:01:26s
epoch 35 | loss: 0.20144 | val_0_rmse: 0.42655 | val_1_rmse: 0.42986 |  0:01:28s
epoch 36 | loss: 0.20193 | val_0_rmse: 0.4312  | val_1_rmse: 0.43142 |  0:01:31s
epoch 37 | loss: 0.19721 | val_0_rmse: 0.43068 | val_1_rmse: 0.4339  |  0:01:33s
epoch 38 | loss: 0.20068 | val_0_rmse: 0.44734 | val_1_rmse: 0.44992 |  0:01:36s
epoch 39 | loss: 0.21019 | val_0_rmse: 0.47042 | val_1_rmse: 0.47336 |  0:01:38s
epoch 40 | loss: 0.20736 | val_0_rmse: 0.4328  | val_1_rmse: 0.43628 |  0:01:41s
epoch 41 | loss: 0.20363 | val_0_rmse: 0.44093 | val_1_rmse: 0.44525 |  0:01:43s
epoch 42 | loss: 0.19842 | val_0_rmse: 0.43661 | val_1_rmse: 0.44269 |  0:01:45s
epoch 43 | loss: 0.19901 | val_0_rmse: 0.44887 | val_1_rmse: 0.4501  |  0:01:48s
epoch 44 | loss: 0.20094 | val_0_rmse: 0.42131 | val_1_rmse: 0.42625 |  0:01:50s
epoch 45 | loss: 0.20254 | val_0_rmse: 0.42829 | val_1_rmse: 0.42977 |  0:01:53s
epoch 46 | loss: 0.20009 | val_0_rmse: 0.46028 | val_1_rmse: 0.46256 |  0:01:55s
epoch 47 | loss: 0.20634 | val_0_rmse: 0.42813 | val_1_rmse: 0.43169 |  0:01:58s
epoch 48 | loss: 0.19689 | val_0_rmse: 0.43209 | val_1_rmse: 0.43425 |  0:02:00s
epoch 49 | loss: 0.19959 | val_0_rmse: 0.43027 | val_1_rmse: 0.43243 |  0:02:03s
epoch 50 | loss: 0.19776 | val_0_rmse: 0.45115 | val_1_rmse: 0.45309 |  0:02:05s
epoch 51 | loss: 0.20043 | val_0_rmse: 0.4274  | val_1_rmse: 0.43251 |  0:02:08s
epoch 52 | loss: 0.19904 | val_0_rmse: 0.43501 | val_1_rmse: 0.43739 |  0:02:10s
epoch 53 | loss: 0.19977 | val_0_rmse: 0.42736 | val_1_rmse: 0.42976 |  0:02:13s
epoch 54 | loss: 0.20238 | val_0_rmse: 0.43267 | val_1_rmse: 0.43713 |  0:02:15s
epoch 55 | loss: 0.19769 | val_0_rmse: 0.42629 | val_1_rmse: 0.42831 |  0:02:17s
epoch 56 | loss: 0.19464 | val_0_rmse: 0.43914 | val_1_rmse: 0.44333 |  0:02:20s
epoch 57 | loss: 0.20431 | val_0_rmse: 0.42112 | val_1_rmse: 0.42581 |  0:02:22s
epoch 58 | loss: 0.19616 | val_0_rmse: 0.42789 | val_1_rmse: 0.43388 |  0:02:25s
epoch 59 | loss: 0.19996 | val_0_rmse: 0.4352  | val_1_rmse: 0.43713 |  0:02:27s
epoch 60 | loss: 0.19641 | val_0_rmse: 0.42204 | val_1_rmse: 0.42537 |  0:02:30s
epoch 61 | loss: 0.19784 | val_0_rmse: 0.4263  | val_1_rmse: 0.43184 |  0:02:32s
epoch 62 | loss: 0.19428 | val_0_rmse: 0.43176 | val_1_rmse: 0.43638 |  0:02:35s
epoch 63 | loss: 0.19282 | val_0_rmse: 0.42044 | val_1_rmse: 0.42733 |  0:02:37s
epoch 64 | loss: 0.19059 | val_0_rmse: 0.42594 | val_1_rmse: 0.43076 |  0:02:40s
epoch 65 | loss: 0.19646 | val_0_rmse: 0.42201 | val_1_rmse: 0.42682 |  0:02:42s
epoch 66 | loss: 0.19424 | val_0_rmse: 0.43749 | val_1_rmse: 0.44408 |  0:02:44s
epoch 67 | loss: 0.19442 | val_0_rmse: 0.42189 | val_1_rmse: 0.4277  |  0:02:47s
epoch 68 | loss: 0.18851 | val_0_rmse: 0.42572 | val_1_rmse: 0.42979 |  0:02:49s
epoch 69 | loss: 0.19764 | val_0_rmse: 0.41739 | val_1_rmse: 0.42592 |  0:02:52s
epoch 70 | loss: 0.19545 | val_0_rmse: 0.43674 | val_1_rmse: 0.44406 |  0:02:54s
epoch 71 | loss: 0.1937  | val_0_rmse: 0.41788 | val_1_rmse: 0.4229  |  0:02:57s
epoch 72 | loss: 0.19187 | val_0_rmse: 0.42346 | val_1_rmse: 0.43026 |  0:02:59s
epoch 73 | loss: 0.19372 | val_0_rmse: 0.45859 | val_1_rmse: 0.46132 |  0:03:02s
epoch 74 | loss: 0.19415 | val_0_rmse: 0.44114 | val_1_rmse: 0.44584 |  0:03:04s
epoch 75 | loss: 0.19302 | val_0_rmse: 0.42102 | val_1_rmse: 0.42355 |  0:03:07s
epoch 76 | loss: 0.19549 | val_0_rmse: 0.46548 | val_1_rmse: 0.46858 |  0:03:09s
epoch 77 | loss: 0.20059 | val_0_rmse: 0.42548 | val_1_rmse: 0.43044 |  0:03:12s
epoch 78 | loss: 0.19333 | val_0_rmse: 0.42051 | val_1_rmse: 0.42613 |  0:03:14s
epoch 79 | loss: 0.19568 | val_0_rmse: 0.42241 | val_1_rmse: 0.42983 |  0:03:17s
epoch 80 | loss: 0.19399 | val_0_rmse: 0.42461 | val_1_rmse: 0.43021 |  0:03:19s
epoch 81 | loss: 0.18955 | val_0_rmse: 0.4397  | val_1_rmse: 0.44818 |  0:03:22s
epoch 82 | loss: 0.19247 | val_0_rmse: 0.4119  | val_1_rmse: 0.4192  |  0:03:24s
epoch 83 | loss: 0.19104 | val_0_rmse: 0.4257  | val_1_rmse: 0.43207 |  0:03:26s
epoch 84 | loss: 0.19001 | val_0_rmse: 0.41771 | val_1_rmse: 0.42533 |  0:03:29s
epoch 85 | loss: 0.19005 | val_0_rmse: 0.41354 | val_1_rmse: 0.42233 |  0:03:31s
epoch 86 | loss: 0.18907 | val_0_rmse: 0.45345 | val_1_rmse: 0.45754 |  0:03:34s
epoch 87 | loss: 0.19109 | val_0_rmse: 0.41757 | val_1_rmse: 0.42274 |  0:03:36s
epoch 88 | loss: 0.18879 | val_0_rmse: 0.42806 | val_1_rmse: 0.43475 |  0:03:39s
epoch 89 | loss: 0.18954 | val_0_rmse: 0.41529 | val_1_rmse: 0.41949 |  0:03:41s
epoch 90 | loss: 0.19076 | val_0_rmse: 0.41616 | val_1_rmse: 0.42333 |  0:03:44s
epoch 91 | loss: 0.18776 | val_0_rmse: 0.42955 | val_1_rmse: 0.43336 |  0:03:46s
epoch 92 | loss: 0.1978  | val_0_rmse: 0.42386 | val_1_rmse: 0.42824 |  0:03:49s
epoch 93 | loss: 0.19252 | val_0_rmse: 0.43043 | val_1_rmse: 0.43629 |  0:03:51s
epoch 94 | loss: 0.18836 | val_0_rmse: 0.41535 | val_1_rmse: 0.42261 |  0:03:54s
epoch 95 | loss: 0.18935 | val_0_rmse: 0.42134 | val_1_rmse: 0.42593 |  0:03:56s
epoch 96 | loss: 0.18622 | val_0_rmse: 0.42328 | val_1_rmse: 0.42972 |  0:03:58s
epoch 97 | loss: 0.19245 | val_0_rmse: 0.42386 | val_1_rmse: 0.42946 |  0:04:01s
epoch 98 | loss: 0.19149 | val_0_rmse: 0.4275  | val_1_rmse: 0.43428 |  0:04:03s
epoch 99 | loss: 0.19009 | val_0_rmse: 0.42498 | val_1_rmse: 0.43159 |  0:04:06s
epoch 100| loss: 0.18861 | val_0_rmse: 0.41767 | val_1_rmse: 0.42286 |  0:04:08s
epoch 101| loss: 0.19064 | val_0_rmse: 0.42067 | val_1_rmse: 0.42537 |  0:04:11s
epoch 102| loss: 0.18649 | val_0_rmse: 0.41041 | val_1_rmse: 0.41584 |  0:04:13s
epoch 103| loss: 0.19529 | val_0_rmse: 0.43953 | val_1_rmse: 0.44468 |  0:04:16s
epoch 104| loss: 0.19105 | val_0_rmse: 0.41356 | val_1_rmse: 0.41878 |  0:04:18s
epoch 105| loss: 0.18439 | val_0_rmse: 0.44817 | val_1_rmse: 0.45429 |  0:04:21s
epoch 106| loss: 0.18627 | val_0_rmse: 0.42187 | val_1_rmse: 0.42724 |  0:04:23s
epoch 107| loss: 0.18703 | val_0_rmse: 0.41602 | val_1_rmse: 0.42348 |  0:04:26s
epoch 108| loss: 0.18327 | val_0_rmse: 0.41046 | val_1_rmse: 0.41819 |  0:04:28s
epoch 109| loss: 0.18464 | val_0_rmse: 0.43637 | val_1_rmse: 0.44383 |  0:04:31s
epoch 110| loss: 0.19087 | val_0_rmse: 0.42078 | val_1_rmse: 0.42886 |  0:04:33s
epoch 111| loss: 0.18722 | val_0_rmse: 0.42558 | val_1_rmse: 0.42985 |  0:04:35s
epoch 112| loss: 0.18759 | val_0_rmse: 0.44132 | val_1_rmse: 0.44822 |  0:04:38s
epoch 113| loss: 0.19231 | val_0_rmse: 0.44061 | val_1_rmse: 0.44932 |  0:04:40s
epoch 114| loss: 0.19222 | val_0_rmse: 0.41076 | val_1_rmse: 0.42127 |  0:04:43s
epoch 115| loss: 0.18425 | val_0_rmse: 0.4138  | val_1_rmse: 0.42096 |  0:04:45s
epoch 116| loss: 0.18446 | val_0_rmse: 0.41177 | val_1_rmse: 0.42029 |  0:04:48s
epoch 117| loss: 0.18492 | val_0_rmse: 0.4084  | val_1_rmse: 0.4177  |  0:04:50s
epoch 118| loss: 0.1834  | val_0_rmse: 0.4198  | val_1_rmse: 0.42651 |  0:04:53s
epoch 119| loss: 0.18421 | val_0_rmse: 0.41887 | val_1_rmse: 0.42869 |  0:04:55s
epoch 120| loss: 0.18296 | val_0_rmse: 0.41188 | val_1_rmse: 0.42114 |  0:04:58s
epoch 121| loss: 0.18351 | val_0_rmse: 0.41991 | val_1_rmse: 0.42927 |  0:05:00s
epoch 122| loss: 0.18048 | val_0_rmse: 0.43042 | val_1_rmse: 0.43859 |  0:05:02s
epoch 123| loss: 0.18246 | val_0_rmse: 0.41    | val_1_rmse: 0.41965 |  0:05:05s
epoch 124| loss: 0.18573 | val_0_rmse: 0.42161 | val_1_rmse: 0.43296 |  0:05:07s
epoch 125| loss: 0.18645 | val_0_rmse: 0.41886 | val_1_rmse: 0.42792 |  0:05:10s
epoch 126| loss: 0.18518 | val_0_rmse: 0.41634 | val_1_rmse: 0.42466 |  0:05:12s
epoch 127| loss: 0.18729 | val_0_rmse: 0.40866 | val_1_rmse: 0.41964 |  0:05:15s
epoch 128| loss: 0.18233 | val_0_rmse: 0.4129  | val_1_rmse: 0.42248 |  0:05:17s
epoch 129| loss: 0.18624 | val_0_rmse: 0.41509 | val_1_rmse: 0.42466 |  0:05:20s
epoch 130| loss: 0.18613 | val_0_rmse: 0.43732 | val_1_rmse: 0.44431 |  0:05:22s
epoch 131| loss: 0.18515 | val_0_rmse: 0.41546 | val_1_rmse: 0.42344 |  0:05:25s
epoch 132| loss: 0.18775 | val_0_rmse: 0.40531 | val_1_rmse: 0.41421 |  0:05:27s
epoch 133| loss: 0.1831  | val_0_rmse: 0.42692 | val_1_rmse: 0.43536 |  0:05:30s
epoch 134| loss: 0.18154 | val_0_rmse: 0.41941 | val_1_rmse: 0.42679 |  0:05:32s
epoch 135| loss: 0.18328 | val_0_rmse: 0.40865 | val_1_rmse: 0.41691 |  0:05:34s
epoch 136| loss: 0.18046 | val_0_rmse: 0.40496 | val_1_rmse: 0.4155  |  0:05:37s
epoch 137| loss: 0.18174 | val_0_rmse: 0.41171 | val_1_rmse: 0.42048 |  0:05:39s
epoch 138| loss: 0.18064 | val_0_rmse: 0.42481 | val_1_rmse: 0.43009 |  0:05:42s
epoch 139| loss: 0.1879  | val_0_rmse: 0.42081 | val_1_rmse: 0.43031 |  0:05:44s
epoch 140| loss: 0.18136 | val_0_rmse: 0.4119  | val_1_rmse: 0.42063 |  0:05:47s
epoch 141| loss: 0.18354 | val_0_rmse: 0.41672 | val_1_rmse: 0.42664 |  0:05:49s
epoch 142| loss: 0.17999 | val_0_rmse: 0.41345 | val_1_rmse: 0.42539 |  0:05:52s
epoch 143| loss: 0.17869 | val_0_rmse: 0.44232 | val_1_rmse: 0.44896 |  0:05:54s
epoch 144| loss: 0.18563 | val_0_rmse: 0.42297 | val_1_rmse: 0.43214 |  0:05:57s
epoch 145| loss: 0.18351 | val_0_rmse: 0.41103 | val_1_rmse: 0.42477 |  0:05:59s
epoch 146| loss: 0.17817 | val_0_rmse: 0.40376 | val_1_rmse: 0.41304 |  0:06:02s
epoch 147| loss: 0.17851 | val_0_rmse: 0.41701 | val_1_rmse: 0.42691 |  0:06:04s
epoch 148| loss: 0.1893  | val_0_rmse: 0.4249  | val_1_rmse: 0.43527 |  0:06:06s
epoch 149| loss: 0.18383 | val_0_rmse: 0.40688 | val_1_rmse: 0.41754 |  0:06:09s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.41304
Best weights from best epoch are automatically used!
ended training at: 10:36:37
Feature importance:
[('Area', 0.024376971921329888), ('Baths', 0.14036575883273134), ('Beds', 0.2187220582884603), ('Latitude', 0.1489433007429691), ('Longitude', 0.23470586233431262), ('Month', 0.0), ('Year', 0.23288604788019673)]
Mean squared error is of 9805889979.150883
Mean absolute error:69387.97544632229
MAPE:0.2862837848395895
R2 score:0.8333716806935016
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:36:37
epoch 0  | loss: 0.5027  | val_0_rmse: 0.6083  | val_1_rmse: 0.60224 |  0:00:02s
epoch 1  | loss: 0.32923 | val_0_rmse: 0.54546 | val_1_rmse: 0.54939 |  0:00:04s
epoch 2  | loss: 0.28549 | val_0_rmse: 0.52222 | val_1_rmse: 0.52669 |  0:00:07s
epoch 3  | loss: 0.26601 | val_0_rmse: 0.50509 | val_1_rmse: 0.50903 |  0:00:09s
epoch 4  | loss: 0.25313 | val_0_rmse: 0.50801 | val_1_rmse: 0.51369 |  0:00:12s
epoch 5  | loss: 0.24936 | val_0_rmse: 0.47527 | val_1_rmse: 0.48134 |  0:00:14s
epoch 6  | loss: 0.2364  | val_0_rmse: 0.50941 | val_1_rmse: 0.51786 |  0:00:17s
epoch 7  | loss: 0.23665 | val_0_rmse: 0.46175 | val_1_rmse: 0.46432 |  0:00:19s
epoch 8  | loss: 0.22866 | val_0_rmse: 0.45355 | val_1_rmse: 0.45671 |  0:00:22s
epoch 9  | loss: 0.22055 | val_0_rmse: 0.4474  | val_1_rmse: 0.4508  |  0:00:24s
epoch 10 | loss: 0.22117 | val_0_rmse: 0.46368 | val_1_rmse: 0.46772 |  0:00:27s
epoch 11 | loss: 0.22791 | val_0_rmse: 0.44483 | val_1_rmse: 0.44705 |  0:00:29s
epoch 12 | loss: 0.22026 | val_0_rmse: 0.4928  | val_1_rmse: 0.49186 |  0:00:32s
epoch 13 | loss: 0.22098 | val_0_rmse: 0.45845 | val_1_rmse: 0.46338 |  0:00:34s
epoch 14 | loss: 0.22105 | val_0_rmse: 0.45115 | val_1_rmse: 0.45082 |  0:00:37s
epoch 15 | loss: 0.21703 | val_0_rmse: 0.43282 | val_1_rmse: 0.43648 |  0:00:39s
epoch 16 | loss: 0.2109  | val_0_rmse: 0.44853 | val_1_rmse: 0.45305 |  0:00:41s
epoch 17 | loss: 0.21308 | val_0_rmse: 0.44116 | val_1_rmse: 0.44524 |  0:00:44s
epoch 18 | loss: 0.21169 | val_0_rmse: 0.43326 | val_1_rmse: 0.4379  |  0:00:46s
epoch 19 | loss: 0.21379 | val_0_rmse: 0.45727 | val_1_rmse: 0.45911 |  0:00:49s
epoch 20 | loss: 0.21059 | val_0_rmse: 0.44479 | val_1_rmse: 0.44573 |  0:00:51s
epoch 21 | loss: 0.21561 | val_0_rmse: 0.444   | val_1_rmse: 0.44621 |  0:00:54s
epoch 22 | loss: 0.20904 | val_0_rmse: 0.45802 | val_1_rmse: 0.45957 |  0:00:56s
epoch 23 | loss: 0.21052 | val_0_rmse: 0.43435 | val_1_rmse: 0.43789 |  0:00:59s
epoch 24 | loss: 0.20584 | val_0_rmse: 0.43371 | val_1_rmse: 0.43759 |  0:01:01s
epoch 25 | loss: 0.20372 | val_0_rmse: 0.42709 | val_1_rmse: 0.43172 |  0:01:04s
epoch 26 | loss: 0.19976 | val_0_rmse: 0.43816 | val_1_rmse: 0.44379 |  0:01:06s
epoch 27 | loss: 0.20364 | val_0_rmse: 0.43606 | val_1_rmse: 0.44028 |  0:01:09s
epoch 28 | loss: 0.20345 | val_0_rmse: 0.46978 | val_1_rmse: 0.47193 |  0:01:11s
epoch 29 | loss: 0.20918 | val_0_rmse: 0.4459  | val_1_rmse: 0.45364 |  0:01:13s
epoch 30 | loss: 0.20578 | val_0_rmse: 0.43248 | val_1_rmse: 0.43897 |  0:01:16s
epoch 31 | loss: 0.19946 | val_0_rmse: 0.43714 | val_1_rmse: 0.44222 |  0:01:18s
epoch 32 | loss: 0.20345 | val_0_rmse: 0.43937 | val_1_rmse: 0.4483  |  0:01:21s
epoch 33 | loss: 0.20277 | val_0_rmse: 0.43585 | val_1_rmse: 0.447   |  0:01:23s
epoch 34 | loss: 0.20353 | val_0_rmse: 0.4274  | val_1_rmse: 0.43266 |  0:01:26s
epoch 35 | loss: 0.19557 | val_0_rmse: 0.43606 | val_1_rmse: 0.44266 |  0:01:28s
epoch 36 | loss: 0.19797 | val_0_rmse: 0.43279 | val_1_rmse: 0.44071 |  0:01:31s
epoch 37 | loss: 0.19999 | val_0_rmse: 0.43281 | val_1_rmse: 0.43976 |  0:01:33s
epoch 38 | loss: 0.20295 | val_0_rmse: 0.44665 | val_1_rmse: 0.45218 |  0:01:36s
epoch 39 | loss: 0.19988 | val_0_rmse: 0.43885 | val_1_rmse: 0.44349 |  0:01:38s
epoch 40 | loss: 0.19666 | val_0_rmse: 0.424   | val_1_rmse: 0.43017 |  0:01:40s
epoch 41 | loss: 0.19905 | val_0_rmse: 0.43691 | val_1_rmse: 0.44026 |  0:01:43s
epoch 42 | loss: 0.19738 | val_0_rmse: 0.42424 | val_1_rmse: 0.42877 |  0:01:45s
epoch 43 | loss: 0.19987 | val_0_rmse: 0.43245 | val_1_rmse: 0.43817 |  0:01:48s
epoch 44 | loss: 0.19501 | val_0_rmse: 0.4211  | val_1_rmse: 0.43115 |  0:01:50s
epoch 45 | loss: 0.19412 | val_0_rmse: 0.42841 | val_1_rmse: 0.4351  |  0:01:53s
epoch 46 | loss: 0.20036 | val_0_rmse: 0.42529 | val_1_rmse: 0.43398 |  0:01:55s
epoch 47 | loss: 0.1977  | val_0_rmse: 0.41837 | val_1_rmse: 0.42456 |  0:01:58s
epoch 48 | loss: 0.19289 | val_0_rmse: 0.43038 | val_1_rmse: 0.43603 |  0:02:00s
epoch 49 | loss: 0.1979  | val_0_rmse: 0.42236 | val_1_rmse: 0.42841 |  0:02:03s
epoch 50 | loss: 0.19007 | val_0_rmse: 0.42046 | val_1_rmse: 0.42473 |  0:02:05s
epoch 51 | loss: 0.19642 | val_0_rmse: 0.43052 | val_1_rmse: 0.43548 |  0:02:08s
epoch 52 | loss: 0.20196 | val_0_rmse: 0.4389  | val_1_rmse: 0.44786 |  0:02:10s
epoch 53 | loss: 0.19837 | val_0_rmse: 0.43249 | val_1_rmse: 0.43576 |  0:02:12s
epoch 54 | loss: 0.19513 | val_0_rmse: 0.44056 | val_1_rmse: 0.44465 |  0:02:15s
epoch 55 | loss: 0.19683 | val_0_rmse: 0.44239 | val_1_rmse: 0.44752 |  0:02:17s
epoch 56 | loss: 0.19911 | val_0_rmse: 0.43342 | val_1_rmse: 0.44353 |  0:02:20s
epoch 57 | loss: 0.20078 | val_0_rmse: 0.4562  | val_1_rmse: 0.4657  |  0:02:22s
epoch 58 | loss: 0.20076 | val_0_rmse: 0.46524 | val_1_rmse: 0.46785 |  0:02:25s
epoch 59 | loss: 0.19554 | val_0_rmse: 0.4285  | val_1_rmse: 0.43791 |  0:02:27s
epoch 60 | loss: 0.19414 | val_0_rmse: 0.45527 | val_1_rmse: 0.46472 |  0:02:30s
epoch 61 | loss: 0.20288 | val_0_rmse: 0.42413 | val_1_rmse: 0.42831 |  0:02:32s
epoch 62 | loss: 0.19484 | val_0_rmse: 0.43425 | val_1_rmse: 0.43823 |  0:02:35s
epoch 63 | loss: 0.19509 | val_0_rmse: 0.42955 | val_1_rmse: 0.43594 |  0:02:37s
epoch 64 | loss: 0.19271 | val_0_rmse: 0.4397  | val_1_rmse: 0.4459  |  0:02:40s
epoch 65 | loss: 0.19673 | val_0_rmse: 0.41575 | val_1_rmse: 0.42404 |  0:02:42s
epoch 66 | loss: 0.19485 | val_0_rmse: 0.42444 | val_1_rmse: 0.4337  |  0:02:44s
epoch 67 | loss: 0.19128 | val_0_rmse: 0.42873 | val_1_rmse: 0.43694 |  0:02:47s
epoch 68 | loss: 0.20076 | val_0_rmse: 0.43037 | val_1_rmse: 0.43353 |  0:02:49s
epoch 69 | loss: 0.19456 | val_0_rmse: 0.42298 | val_1_rmse: 0.42808 |  0:02:52s
epoch 70 | loss: 0.19031 | val_0_rmse: 0.42329 | val_1_rmse: 0.43089 |  0:02:54s
epoch 71 | loss: 0.19019 | val_0_rmse: 0.41658 | val_1_rmse: 0.42068 |  0:02:57s
epoch 72 | loss: 0.19804 | val_0_rmse: 0.4218  | val_1_rmse: 0.42958 |  0:02:59s
epoch 73 | loss: 0.19305 | val_0_rmse: 0.42353 | val_1_rmse: 0.43057 |  0:03:02s
epoch 74 | loss: 0.19126 | val_0_rmse: 0.43377 | val_1_rmse: 0.43866 |  0:03:05s
epoch 75 | loss: 0.19545 | val_0_rmse: 0.44267 | val_1_rmse: 0.45024 |  0:03:07s
epoch 76 | loss: 0.19217 | val_0_rmse: 0.42628 | val_1_rmse: 0.43221 |  0:03:10s
epoch 77 | loss: 0.18926 | val_0_rmse: 0.42972 | val_1_rmse: 0.43603 |  0:03:12s
epoch 78 | loss: 0.1945  | val_0_rmse: 0.41935 | val_1_rmse: 0.42961 |  0:03:14s
epoch 79 | loss: 0.18834 | val_0_rmse: 0.42893 | val_1_rmse: 0.43884 |  0:03:17s
epoch 80 | loss: 0.19556 | val_0_rmse: 0.42597 | val_1_rmse: 0.43199 |  0:03:19s
epoch 81 | loss: 0.18896 | val_0_rmse: 0.42797 | val_1_rmse: 0.43494 |  0:03:22s
epoch 82 | loss: 0.19435 | val_0_rmse: 0.43721 | val_1_rmse: 0.445   |  0:03:24s
epoch 83 | loss: 0.20434 | val_0_rmse: 0.42529 | val_1_rmse: 0.43633 |  0:03:27s
epoch 84 | loss: 0.1951  | val_0_rmse: 0.43066 | val_1_rmse: 0.43689 |  0:03:29s
epoch 85 | loss: 0.19247 | val_0_rmse: 0.42034 | val_1_rmse: 0.42897 |  0:03:32s
epoch 86 | loss: 0.18908 | val_0_rmse: 0.42451 | val_1_rmse: 0.43611 |  0:03:34s
epoch 87 | loss: 0.19071 | val_0_rmse: 0.4429  | val_1_rmse: 0.45051 |  0:03:37s
epoch 88 | loss: 0.23723 | val_0_rmse: 0.46757 | val_1_rmse: 0.47097 |  0:03:39s
epoch 89 | loss: 0.20591 | val_0_rmse: 0.43688 | val_1_rmse: 0.44539 |  0:03:41s
epoch 90 | loss: 0.20221 | val_0_rmse: 0.43104 | val_1_rmse: 0.44035 |  0:03:44s
epoch 91 | loss: 0.19925 | val_0_rmse: 0.44028 | val_1_rmse: 0.44899 |  0:03:46s
epoch 92 | loss: 0.19713 | val_0_rmse: 0.42438 | val_1_rmse: 0.42903 |  0:03:49s
epoch 93 | loss: 0.19259 | val_0_rmse: 0.42484 | val_1_rmse: 0.43495 |  0:03:51s
epoch 94 | loss: 0.19853 | val_0_rmse: 0.43333 | val_1_rmse: 0.43978 |  0:03:54s
epoch 95 | loss: 0.19569 | val_0_rmse: 0.4217  | val_1_rmse: 0.42659 |  0:03:56s
epoch 96 | loss: 0.19812 | val_0_rmse: 0.4284  | val_1_rmse: 0.43625 |  0:03:59s
epoch 97 | loss: 0.20165 | val_0_rmse: 0.43934 | val_1_rmse: 0.44517 |  0:04:01s
epoch 98 | loss: 0.20638 | val_0_rmse: 0.43163 | val_1_rmse: 0.4365  |  0:04:04s
epoch 99 | loss: 0.2111  | val_0_rmse: 0.45806 | val_1_rmse: 0.46155 |  0:04:06s
epoch 100| loss: 0.20496 | val_0_rmse: 0.4373  | val_1_rmse: 0.44211 |  0:04:08s
epoch 101| loss: 0.20582 | val_0_rmse: 0.43516 | val_1_rmse: 0.43919 |  0:04:11s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.42068
Best weights from best epoch are automatically used!
ended training at: 10:40:49
Feature importance:
[('Area', 0.012837807869115588), ('Baths', 0.11958767794121401), ('Beds', 0.1366944835326841), ('Latitude', 0.1770192033870061), ('Longitude', 0.2493716556558252), ('Month', 0.0), ('Year', 0.304489171614155)]
Mean squared error is of 10348810429.037687
Mean absolute error:71133.4600240917
MAPE:0.27362751092382986
R2 score:0.823060637204973
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:40:50
epoch 0  | loss: 0.48526 | val_0_rmse: 0.57903 | val_1_rmse: 0.57397 |  0:00:02s
epoch 1  | loss: 0.32337 | val_0_rmse: 0.53249 | val_1_rmse: 0.52931 |  0:00:04s
epoch 2  | loss: 0.28609 | val_0_rmse: 0.51702 | val_1_rmse: 0.51422 |  0:00:07s
epoch 3  | loss: 0.25967 | val_0_rmse: 0.5007  | val_1_rmse: 0.49764 |  0:00:09s
epoch 4  | loss: 0.25411 | val_0_rmse: 0.51194 | val_1_rmse: 0.50684 |  0:00:12s
epoch 5  | loss: 0.2458  | val_0_rmse: 0.49677 | val_1_rmse: 0.49073 |  0:00:14s
epoch 6  | loss: 0.24534 | val_0_rmse: 0.46713 | val_1_rmse: 0.46082 |  0:00:17s
epoch 7  | loss: 0.23855 | val_0_rmse: 0.49752 | val_1_rmse: 0.49316 |  0:00:19s
epoch 8  | loss: 0.23248 | val_0_rmse: 0.46514 | val_1_rmse: 0.46243 |  0:00:22s
epoch 9  | loss: 0.2262  | val_0_rmse: 0.47558 | val_1_rmse: 0.47096 |  0:00:24s
epoch 10 | loss: 0.22911 | val_0_rmse: 0.46004 | val_1_rmse: 0.45528 |  0:00:27s
epoch 11 | loss: 0.22246 | val_0_rmse: 0.44825 | val_1_rmse: 0.44618 |  0:00:29s
epoch 12 | loss: 0.21923 | val_0_rmse: 0.44364 | val_1_rmse: 0.44347 |  0:00:32s
epoch 13 | loss: 0.2145  | val_0_rmse: 0.49026 | val_1_rmse: 0.48888 |  0:00:34s
epoch 14 | loss: 0.21959 | val_0_rmse: 0.455   | val_1_rmse: 0.45151 |  0:00:37s
epoch 15 | loss: 0.21874 | val_0_rmse: 0.43739 | val_1_rmse: 0.43477 |  0:00:39s
epoch 16 | loss: 0.2124  | val_0_rmse: 0.43676 | val_1_rmse: 0.43628 |  0:00:41s
epoch 17 | loss: 0.20596 | val_0_rmse: 0.43429 | val_1_rmse: 0.43503 |  0:00:44s
epoch 18 | loss: 0.20916 | val_0_rmse: 0.43909 | val_1_rmse: 0.43897 |  0:00:46s
epoch 19 | loss: 0.20804 | val_0_rmse: 0.45092 | val_1_rmse: 0.45181 |  0:00:49s
epoch 20 | loss: 0.21044 | val_0_rmse: 0.44304 | val_1_rmse: 0.44082 |  0:00:51s
epoch 21 | loss: 0.20675 | val_0_rmse: 0.4508  | val_1_rmse: 0.44762 |  0:00:54s
epoch 22 | loss: 0.20678 | val_0_rmse: 0.43925 | val_1_rmse: 0.43711 |  0:00:56s
epoch 23 | loss: 0.21126 | val_0_rmse: 0.43675 | val_1_rmse: 0.43309 |  0:00:59s
epoch 24 | loss: 0.20437 | val_0_rmse: 0.43449 | val_1_rmse: 0.43042 |  0:01:01s
epoch 25 | loss: 0.20284 | val_0_rmse: 0.44222 | val_1_rmse: 0.43923 |  0:01:04s
epoch 26 | loss: 0.20348 | val_0_rmse: 0.44206 | val_1_rmse: 0.44042 |  0:01:06s
epoch 27 | loss: 0.20507 | val_0_rmse: 0.43717 | val_1_rmse: 0.43619 |  0:01:09s
epoch 28 | loss: 0.20208 | val_0_rmse: 0.42539 | val_1_rmse: 0.42497 |  0:01:11s
epoch 29 | loss: 0.19802 | val_0_rmse: 0.43131 | val_1_rmse: 0.42825 |  0:01:13s
epoch 30 | loss: 0.20747 | val_0_rmse: 0.43139 | val_1_rmse: 0.43033 |  0:01:16s
epoch 31 | loss: 0.19933 | val_0_rmse: 0.43745 | val_1_rmse: 0.43642 |  0:01:18s
epoch 32 | loss: 0.1973  | val_0_rmse: 0.42267 | val_1_rmse: 0.42076 |  0:01:21s
epoch 33 | loss: 0.19888 | val_0_rmse: 0.42895 | val_1_rmse: 0.42803 |  0:01:23s
epoch 34 | loss: 0.19371 | val_0_rmse: 0.43449 | val_1_rmse: 0.43324 |  0:01:26s
epoch 35 | loss: 0.19999 | val_0_rmse: 0.43417 | val_1_rmse: 0.43514 |  0:01:28s
epoch 36 | loss: 0.20483 | val_0_rmse: 0.45405 | val_1_rmse: 0.45026 |  0:01:31s
epoch 37 | loss: 0.19591 | val_0_rmse: 0.49209 | val_1_rmse: 0.48972 |  0:01:33s
epoch 38 | loss: 0.20046 | val_0_rmse: 0.42202 | val_1_rmse: 0.42216 |  0:01:36s
epoch 39 | loss: 0.19914 | val_0_rmse: 0.41576 | val_1_rmse: 0.41583 |  0:01:38s
epoch 40 | loss: 0.19438 | val_0_rmse: 0.42098 | val_1_rmse: 0.4192  |  0:01:41s
epoch 41 | loss: 0.19276 | val_0_rmse: 0.43149 | val_1_rmse: 0.433   |  0:01:43s
epoch 42 | loss: 0.19208 | val_0_rmse: 0.42727 | val_1_rmse: 0.43    |  0:01:46s
epoch 43 | loss: 0.19296 | val_0_rmse: 0.42136 | val_1_rmse: 0.42117 |  0:01:48s
epoch 44 | loss: 0.19657 | val_0_rmse: 0.42609 | val_1_rmse: 0.42309 |  0:01:50s
epoch 45 | loss: 0.19957 | val_0_rmse: 0.44127 | val_1_rmse: 0.44073 |  0:01:53s
epoch 46 | loss: 0.19413 | val_0_rmse: 0.42654 | val_1_rmse: 0.42817 |  0:01:55s
epoch 47 | loss: 0.19578 | val_0_rmse: 0.42904 | val_1_rmse: 0.42785 |  0:01:58s
epoch 48 | loss: 0.19391 | val_0_rmse: 0.45117 | val_1_rmse: 0.45019 |  0:02:00s
epoch 49 | loss: 0.19858 | val_0_rmse: 0.41881 | val_1_rmse: 0.42172 |  0:02:03s
epoch 50 | loss: 0.19615 | val_0_rmse: 0.42664 | val_1_rmse: 0.42808 |  0:02:05s
epoch 51 | loss: 0.18996 | val_0_rmse: 0.42194 | val_1_rmse: 0.426   |  0:02:08s
epoch 52 | loss: 0.19152 | val_0_rmse: 0.42238 | val_1_rmse: 0.42362 |  0:02:10s
epoch 53 | loss: 0.18896 | val_0_rmse: 0.43563 | val_1_rmse: 0.43602 |  0:02:13s
epoch 54 | loss: 0.19249 | val_0_rmse: 0.42272 | val_1_rmse: 0.42387 |  0:02:15s
epoch 55 | loss: 0.19172 | val_0_rmse: 0.41339 | val_1_rmse: 0.41637 |  0:02:17s
epoch 56 | loss: 0.19134 | val_0_rmse: 0.44921 | val_1_rmse: 0.45108 |  0:02:20s
epoch 57 | loss: 0.19374 | val_0_rmse: 0.44293 | val_1_rmse: 0.4457  |  0:02:22s
epoch 58 | loss: 0.19643 | val_0_rmse: 0.42204 | val_1_rmse: 0.4225  |  0:02:25s
epoch 59 | loss: 0.19116 | val_0_rmse: 0.41868 | val_1_rmse: 0.41748 |  0:02:27s
epoch 60 | loss: 0.18912 | val_0_rmse: 0.43365 | val_1_rmse: 0.43539 |  0:02:30s
epoch 61 | loss: 0.19221 | val_0_rmse: 0.46096 | val_1_rmse: 0.45695 |  0:02:32s
epoch 62 | loss: 0.19233 | val_0_rmse: 0.43117 | val_1_rmse: 0.43258 |  0:02:35s
epoch 63 | loss: 0.191   | val_0_rmse: 0.41733 | val_1_rmse: 0.42026 |  0:02:37s
epoch 64 | loss: 0.18535 | val_0_rmse: 0.40854 | val_1_rmse: 0.4143  |  0:02:40s
epoch 65 | loss: 0.19009 | val_0_rmse: 0.43516 | val_1_rmse: 0.43914 |  0:02:42s
epoch 66 | loss: 0.18841 | val_0_rmse: 0.42044 | val_1_rmse: 0.42565 |  0:02:45s
epoch 67 | loss: 0.18365 | val_0_rmse: 0.42099 | val_1_rmse: 0.42965 |  0:02:47s
epoch 68 | loss: 0.18504 | val_0_rmse: 0.41735 | val_1_rmse: 0.42253 |  0:02:49s
epoch 69 | loss: 0.18527 | val_0_rmse: 0.43169 | val_1_rmse: 0.43415 |  0:02:52s
epoch 70 | loss: 0.18529 | val_0_rmse: 0.41756 | val_1_rmse: 0.42228 |  0:02:54s
epoch 71 | loss: 0.18667 | val_0_rmse: 0.43528 | val_1_rmse: 0.43849 |  0:02:57s
epoch 72 | loss: 0.18615 | val_0_rmse: 0.41053 | val_1_rmse: 0.41665 |  0:02:59s
epoch 73 | loss: 0.18392 | val_0_rmse: 0.40935 | val_1_rmse: 0.41794 |  0:03:02s
epoch 74 | loss: 0.18711 | val_0_rmse: 0.4416  | val_1_rmse: 0.44687 |  0:03:04s
epoch 75 | loss: 0.18837 | val_0_rmse: 0.40912 | val_1_rmse: 0.41604 |  0:03:07s
epoch 76 | loss: 0.18983 | val_0_rmse: 0.40748 | val_1_rmse: 0.41452 |  0:03:09s
epoch 77 | loss: 0.19485 | val_0_rmse: 0.4164  | val_1_rmse: 0.42302 |  0:03:12s
epoch 78 | loss: 0.18402 | val_0_rmse: 0.42142 | val_1_rmse: 0.42691 |  0:03:14s
epoch 79 | loss: 0.18198 | val_0_rmse: 0.41076 | val_1_rmse: 0.41829 |  0:03:17s
epoch 80 | loss: 0.18349 | val_0_rmse: 0.41383 | val_1_rmse: 0.41828 |  0:03:19s
epoch 81 | loss: 0.18698 | val_0_rmse: 0.41837 | val_1_rmse: 0.42664 |  0:03:22s
epoch 82 | loss: 0.18176 | val_0_rmse: 0.40758 | val_1_rmse: 0.41379 |  0:03:24s
epoch 83 | loss: 0.18099 | val_0_rmse: 0.41468 | val_1_rmse: 0.42155 |  0:03:26s
epoch 84 | loss: 0.18458 | val_0_rmse: 0.41562 | val_1_rmse: 0.41697 |  0:03:29s
epoch 85 | loss: 0.18368 | val_0_rmse: 0.41541 | val_1_rmse: 0.4243  |  0:03:31s
epoch 86 | loss: 0.18152 | val_0_rmse: 0.44044 | val_1_rmse: 0.44231 |  0:03:34s
epoch 87 | loss: 0.18089 | val_0_rmse: 0.40991 | val_1_rmse: 0.42039 |  0:03:36s
epoch 88 | loss: 0.17956 | val_0_rmse: 0.41377 | val_1_rmse: 0.42315 |  0:03:39s
epoch 89 | loss: 0.18852 | val_0_rmse: 0.40719 | val_1_rmse: 0.41246 |  0:03:41s
epoch 90 | loss: 0.18344 | val_0_rmse: 0.41568 | val_1_rmse: 0.42201 |  0:03:44s
epoch 91 | loss: 0.18174 | val_0_rmse: 0.42737 | val_1_rmse: 0.43398 |  0:03:46s
epoch 92 | loss: 0.18361 | val_0_rmse: 0.42193 | val_1_rmse: 0.43119 |  0:03:49s
epoch 93 | loss: 0.18471 | val_0_rmse: 0.41668 | val_1_rmse: 0.42474 |  0:03:51s
epoch 94 | loss: 0.18206 | val_0_rmse: 0.43035 | val_1_rmse: 0.43534 |  0:03:53s
epoch 95 | loss: 0.17922 | val_0_rmse: 0.40132 | val_1_rmse: 0.4118  |  0:03:56s
epoch 96 | loss: 0.18231 | val_0_rmse: 0.4046  | val_1_rmse: 0.41445 |  0:03:58s
epoch 97 | loss: 0.18082 | val_0_rmse: 0.41786 | val_1_rmse: 0.42369 |  0:04:01s
epoch 98 | loss: 0.18145 | val_0_rmse: 0.41684 | val_1_rmse: 0.42563 |  0:04:03s
epoch 99 | loss: 0.18101 | val_0_rmse: 0.41514 | val_1_rmse: 0.42317 |  0:04:06s
epoch 100| loss: 0.18005 | val_0_rmse: 0.40728 | val_1_rmse: 0.41547 |  0:04:08s
epoch 101| loss: 0.18328 | val_0_rmse: 0.41077 | val_1_rmse: 0.42074 |  0:04:11s
epoch 102| loss: 0.18079 | val_0_rmse: 0.40759 | val_1_rmse: 0.41997 |  0:04:13s
epoch 103| loss: 0.17836 | val_0_rmse: 0.422   | val_1_rmse: 0.42941 |  0:04:16s
epoch 104| loss: 0.18117 | val_0_rmse: 0.40136 | val_1_rmse: 0.41026 |  0:04:18s
epoch 105| loss: 0.18319 | val_0_rmse: 0.41411 | val_1_rmse: 0.41862 |  0:04:21s
epoch 106| loss: 0.1784  | val_0_rmse: 0.39673 | val_1_rmse: 0.40867 |  0:04:23s
epoch 107| loss: 0.17826 | val_0_rmse: 0.42767 | val_1_rmse: 0.4365  |  0:04:26s
epoch 108| loss: 0.18227 | val_0_rmse: 0.40622 | val_1_rmse: 0.41354 |  0:04:28s
epoch 109| loss: 0.17903 | val_0_rmse: 0.42312 | val_1_rmse: 0.43101 |  0:04:31s
epoch 110| loss: 0.18023 | val_0_rmse: 0.40993 | val_1_rmse: 0.41928 |  0:04:33s
epoch 111| loss: 0.17855 | val_0_rmse: 0.41147 | val_1_rmse: 0.42075 |  0:04:35s
epoch 112| loss: 0.17944 | val_0_rmse: 0.4327  | val_1_rmse: 0.43872 |  0:04:38s
epoch 113| loss: 0.1827  | val_0_rmse: 0.41347 | val_1_rmse: 0.42153 |  0:04:40s
epoch 114| loss: 0.18342 | val_0_rmse: 0.40231 | val_1_rmse: 0.41445 |  0:04:43s
epoch 115| loss: 0.17781 | val_0_rmse: 0.41356 | val_1_rmse: 0.42068 |  0:04:45s
epoch 116| loss: 0.17775 | val_0_rmse: 0.41533 | val_1_rmse: 0.42123 |  0:04:48s
epoch 117| loss: 0.17648 | val_0_rmse: 0.40451 | val_1_rmse: 0.41339 |  0:04:50s
epoch 118| loss: 0.18041 | val_0_rmse: 0.39871 | val_1_rmse: 0.40814 |  0:04:53s
epoch 119| loss: 0.17648 | val_0_rmse: 0.41032 | val_1_rmse: 0.42426 |  0:04:55s
epoch 120| loss: 0.17952 | val_0_rmse: 0.41576 | val_1_rmse: 0.4227  |  0:04:58s
epoch 121| loss: 0.17936 | val_0_rmse: 0.41842 | val_1_rmse: 0.42886 |  0:05:00s
epoch 122| loss: 0.18442 | val_0_rmse: 0.40614 | val_1_rmse: 0.41655 |  0:05:02s
epoch 123| loss: 0.18475 | val_0_rmse: 0.405   | val_1_rmse: 0.41585 |  0:05:05s
epoch 124| loss: 0.18509 | val_0_rmse: 0.42167 | val_1_rmse: 0.43054 |  0:05:07s
epoch 125| loss: 0.18932 | val_0_rmse: 0.43918 | val_1_rmse: 0.44522 |  0:05:10s
epoch 126| loss: 0.19696 | val_0_rmse: 0.4299  | val_1_rmse: 0.43569 |  0:05:12s
epoch 127| loss: 0.18167 | val_0_rmse: 0.4143  | val_1_rmse: 0.4245  |  0:05:15s
epoch 128| loss: 0.18213 | val_0_rmse: 0.41468 | val_1_rmse: 0.42164 |  0:05:17s
epoch 129| loss: 0.18125 | val_0_rmse: 0.41027 | val_1_rmse: 0.42181 |  0:05:20s
epoch 130| loss: 0.18025 | val_0_rmse: 0.40381 | val_1_rmse: 0.41282 |  0:05:22s
epoch 131| loss: 0.1748  | val_0_rmse: 0.40737 | val_1_rmse: 0.41704 |  0:05:25s
epoch 132| loss: 0.17755 | val_0_rmse: 0.44387 | val_1_rmse: 0.4535  |  0:05:27s
epoch 133| loss: 0.17948 | val_0_rmse: 0.40142 | val_1_rmse: 0.41245 |  0:05:30s
epoch 134| loss: 0.17578 | val_0_rmse: 0.414   | val_1_rmse: 0.42657 |  0:05:32s
epoch 135| loss: 0.19034 | val_0_rmse: 0.45137 | val_1_rmse: 0.46121 |  0:05:34s
epoch 136| loss: 0.20446 | val_0_rmse: 0.43345 | val_1_rmse: 0.44066 |  0:05:37s
epoch 137| loss: 0.19512 | val_0_rmse: 0.45862 | val_1_rmse: 0.46704 |  0:05:39s
epoch 138| loss: 0.20987 | val_0_rmse: 0.43078 | val_1_rmse: 0.43425 |  0:05:42s
epoch 139| loss: 0.20073 | val_0_rmse: 0.41586 | val_1_rmse: 0.41713 |  0:05:44s
epoch 140| loss: 0.18789 | val_0_rmse: 0.41338 | val_1_rmse: 0.41954 |  0:05:47s
epoch 141| loss: 0.18389 | val_0_rmse: 0.40979 | val_1_rmse: 0.41755 |  0:05:49s
epoch 142| loss: 0.18489 | val_0_rmse: 0.41493 | val_1_rmse: 0.42271 |  0:05:52s
epoch 143| loss: 0.18547 | val_0_rmse: 0.43347 | val_1_rmse: 0.43978 |  0:05:54s
epoch 144| loss: 0.18395 | val_0_rmse: 0.40961 | val_1_rmse: 0.4217  |  0:05:57s
epoch 145| loss: 0.1773  | val_0_rmse: 0.40134 | val_1_rmse: 0.41417 |  0:05:59s
epoch 146| loss: 0.17638 | val_0_rmse: 0.39668 | val_1_rmse: 0.40872 |  0:06:02s
epoch 147| loss: 0.18054 | val_0_rmse: 0.41769 | val_1_rmse: 0.42586 |  0:06:04s
epoch 148| loss: 0.19598 | val_0_rmse: 0.49339 | val_1_rmse: 0.49584 |  0:06:07s

Early stopping occured at epoch 148 with best_epoch = 118 and best_val_1_rmse = 0.40814
Best weights from best epoch are automatically used!
ended training at: 10:46:57
Feature importance:
[('Area', 0.053113914204704286), ('Baths', 0.11813814323351471), ('Beds', 0.16285460557094877), ('Latitude', 0.07944067596964902), ('Longitude', 0.15558383404442094), ('Month', 0.0528124112404466), ('Year', 0.37805641573631565)]
Mean squared error is of 10141116984.459463
Mean absolute error:69556.54230651562
MAPE:0.30362329554899864
R2 score:0.8242096011974248
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:46:58
epoch 0  | loss: 0.76947 | val_0_rmse: 0.74358 | val_1_rmse: 0.76647 |  0:00:00s
epoch 1  | loss: 0.42187 | val_0_rmse: 0.63114 | val_1_rmse: 0.66489 |  0:00:01s
epoch 2  | loss: 0.36158 | val_0_rmse: 0.59068 | val_1_rmse: 0.62477 |  0:00:02s
epoch 3  | loss: 0.32952 | val_0_rmse: 0.56169 | val_1_rmse: 0.59322 |  0:00:02s
epoch 4  | loss: 0.31191 | val_0_rmse: 0.54427 | val_1_rmse: 0.57573 |  0:00:03s
epoch 5  | loss: 0.30541 | val_0_rmse: 0.5285  | val_1_rmse: 0.55896 |  0:00:04s
epoch 6  | loss: 0.30057 | val_0_rmse: 0.53426 | val_1_rmse: 0.56608 |  0:00:05s
epoch 7  | loss: 0.28761 | val_0_rmse: 0.53094 | val_1_rmse: 0.56891 |  0:00:05s
epoch 8  | loss: 0.28217 | val_0_rmse: 0.50958 | val_1_rmse: 0.54088 |  0:00:06s
epoch 9  | loss: 0.28618 | val_0_rmse: 0.50739 | val_1_rmse: 0.54084 |  0:00:07s
epoch 10 | loss: 0.27587 | val_0_rmse: 0.50417 | val_1_rmse: 0.54219 |  0:00:08s
epoch 11 | loss: 0.27123 | val_0_rmse: 0.51212 | val_1_rmse: 0.54936 |  0:00:08s
epoch 12 | loss: 0.26585 | val_0_rmse: 0.49867 | val_1_rmse: 0.5407  |  0:00:09s
epoch 13 | loss: 0.26173 | val_0_rmse: 0.49527 | val_1_rmse: 0.53636 |  0:00:10s
epoch 14 | loss: 0.26868 | val_0_rmse: 0.49257 | val_1_rmse: 0.52967 |  0:00:11s
epoch 15 | loss: 0.265   | val_0_rmse: 0.50368 | val_1_rmse: 0.53612 |  0:00:11s
epoch 16 | loss: 0.26412 | val_0_rmse: 0.48599 | val_1_rmse: 0.5288  |  0:00:12s
epoch 17 | loss: 0.26103 | val_0_rmse: 0.49189 | val_1_rmse: 0.52964 |  0:00:13s
epoch 18 | loss: 0.26603 | val_0_rmse: 0.5143  | val_1_rmse: 0.5595  |  0:00:13s
epoch 19 | loss: 0.26211 | val_0_rmse: 0.48469 | val_1_rmse: 0.52407 |  0:00:14s
epoch 20 | loss: 0.25716 | val_0_rmse: 0.50053 | val_1_rmse: 0.54367 |  0:00:15s
epoch 21 | loss: 0.25002 | val_0_rmse: 0.47834 | val_1_rmse: 0.52154 |  0:00:16s
epoch 22 | loss: 0.25659 | val_0_rmse: 0.48706 | val_1_rmse: 0.52698 |  0:00:16s
epoch 23 | loss: 0.26147 | val_0_rmse: 0.4829  | val_1_rmse: 0.52622 |  0:00:17s
epoch 24 | loss: 0.24325 | val_0_rmse: 0.47466 | val_1_rmse: 0.52212 |  0:00:18s
epoch 25 | loss: 0.24448 | val_0_rmse: 0.48486 | val_1_rmse: 0.52629 |  0:00:19s
epoch 26 | loss: 0.25705 | val_0_rmse: 0.47931 | val_1_rmse: 0.51979 |  0:00:19s
epoch 27 | loss: 0.25599 | val_0_rmse: 0.49392 | val_1_rmse: 0.54234 |  0:00:20s
epoch 28 | loss: 0.25186 | val_0_rmse: 0.48162 | val_1_rmse: 0.52783 |  0:00:21s
epoch 29 | loss: 0.24393 | val_0_rmse: 0.48282 | val_1_rmse: 0.5222  |  0:00:22s
epoch 30 | loss: 0.25129 | val_0_rmse: 0.52156 | val_1_rmse: 0.57298 |  0:00:22s
epoch 31 | loss: 0.25889 | val_0_rmse: 0.48674 | val_1_rmse: 0.53183 |  0:00:23s
epoch 32 | loss: 0.24957 | val_0_rmse: 0.48212 | val_1_rmse: 0.53013 |  0:00:24s
epoch 33 | loss: 0.24195 | val_0_rmse: 0.47502 | val_1_rmse: 0.52013 |  0:00:24s
epoch 34 | loss: 0.2454  | val_0_rmse: 0.48158 | val_1_rmse: 0.5205  |  0:00:25s
epoch 35 | loss: 0.25444 | val_0_rmse: 0.48566 | val_1_rmse: 0.53119 |  0:00:26s
epoch 36 | loss: 0.24803 | val_0_rmse: 0.49679 | val_1_rmse: 0.54142 |  0:00:27s
epoch 37 | loss: 0.246   | val_0_rmse: 0.47999 | val_1_rmse: 0.52534 |  0:00:27s
epoch 38 | loss: 0.24704 | val_0_rmse: 0.47383 | val_1_rmse: 0.51933 |  0:00:28s
epoch 39 | loss: 0.24013 | val_0_rmse: 0.46672 | val_1_rmse: 0.51096 |  0:00:29s
epoch 40 | loss: 0.24279 | val_0_rmse: 0.47362 | val_1_rmse: 0.51677 |  0:00:30s
epoch 41 | loss: 0.2389  | val_0_rmse: 0.48351 | val_1_rmse: 0.53393 |  0:00:30s
epoch 42 | loss: 0.23933 | val_0_rmse: 0.50531 | val_1_rmse: 0.5513  |  0:00:31s
epoch 43 | loss: 0.24136 | val_0_rmse: 0.48299 | val_1_rmse: 0.53028 |  0:00:32s
epoch 44 | loss: 0.23938 | val_0_rmse: 0.46265 | val_1_rmse: 0.5097  |  0:00:32s
epoch 45 | loss: 0.23676 | val_0_rmse: 0.46148 | val_1_rmse: 0.50677 |  0:00:33s
epoch 46 | loss: 0.23395 | val_0_rmse: 0.47004 | val_1_rmse: 0.51717 |  0:00:34s
epoch 47 | loss: 0.22977 | val_0_rmse: 0.46186 | val_1_rmse: 0.51114 |  0:00:35s
epoch 48 | loss: 0.22713 | val_0_rmse: 0.4901  | val_1_rmse: 0.54125 |  0:00:35s
epoch 49 | loss: 0.24421 | val_0_rmse: 0.4865  | val_1_rmse: 0.53006 |  0:00:36s
epoch 50 | loss: 0.2385  | val_0_rmse: 0.49846 | val_1_rmse: 0.55012 |  0:00:37s
epoch 51 | loss: 0.24608 | val_0_rmse: 0.47925 | val_1_rmse: 0.52719 |  0:00:38s
epoch 52 | loss: 0.23461 | val_0_rmse: 0.48997 | val_1_rmse: 0.52611 |  0:00:38s
epoch 53 | loss: 0.23169 | val_0_rmse: 0.47658 | val_1_rmse: 0.52794 |  0:00:39s
epoch 54 | loss: 0.22844 | val_0_rmse: 0.47092 | val_1_rmse: 0.5156  |  0:00:40s
epoch 55 | loss: 0.23306 | val_0_rmse: 0.45923 | val_1_rmse: 0.50712 |  0:00:41s
epoch 56 | loss: 0.2254  | val_0_rmse: 0.47344 | val_1_rmse: 0.51372 |  0:00:41s
epoch 57 | loss: 0.23311 | val_0_rmse: 0.4597  | val_1_rmse: 0.51006 |  0:00:42s
epoch 58 | loss: 0.24034 | val_0_rmse: 0.46117 | val_1_rmse: 0.51191 |  0:00:43s
epoch 59 | loss: 0.23919 | val_0_rmse: 0.49585 | val_1_rmse: 0.54807 |  0:00:43s
epoch 60 | loss: 0.23655 | val_0_rmse: 0.47488 | val_1_rmse: 0.51861 |  0:00:44s
epoch 61 | loss: 0.22918 | val_0_rmse: 0.46163 | val_1_rmse: 0.51473 |  0:00:45s
epoch 62 | loss: 0.22763 | val_0_rmse: 0.45826 | val_1_rmse: 0.50207 |  0:00:46s
epoch 63 | loss: 0.23203 | val_0_rmse: 0.45377 | val_1_rmse: 0.50487 |  0:00:46s
epoch 64 | loss: 0.22418 | val_0_rmse: 0.46358 | val_1_rmse: 0.51533 |  0:00:47s
epoch 65 | loss: 0.22652 | val_0_rmse: 0.4533  | val_1_rmse: 0.50189 |  0:00:48s
epoch 66 | loss: 0.22515 | val_0_rmse: 0.45405 | val_1_rmse: 0.49786 |  0:00:49s
epoch 67 | loss: 0.23154 | val_0_rmse: 0.46548 | val_1_rmse: 0.50967 |  0:00:49s
epoch 68 | loss: 0.23452 | val_0_rmse: 0.45479 | val_1_rmse: 0.50579 |  0:00:50s
epoch 69 | loss: 0.23191 | val_0_rmse: 0.46903 | val_1_rmse: 0.51829 |  0:00:51s
epoch 70 | loss: 0.23269 | val_0_rmse: 0.46652 | val_1_rmse: 0.51072 |  0:00:51s
epoch 71 | loss: 0.22521 | val_0_rmse: 0.46016 | val_1_rmse: 0.51738 |  0:00:52s
epoch 72 | loss: 0.22316 | val_0_rmse: 0.47074 | val_1_rmse: 0.5205  |  0:00:53s
epoch 73 | loss: 0.23328 | val_0_rmse: 0.46399 | val_1_rmse: 0.51563 |  0:00:54s
epoch 74 | loss: 0.22381 | val_0_rmse: 0.47406 | val_1_rmse: 0.53042 |  0:00:54s
epoch 75 | loss: 0.22804 | val_0_rmse: 0.46245 | val_1_rmse: 0.52196 |  0:00:55s
epoch 76 | loss: 0.2313  | val_0_rmse: 0.47139 | val_1_rmse: 0.51532 |  0:00:56s
epoch 77 | loss: 0.23873 | val_0_rmse: 0.47937 | val_1_rmse: 0.52604 |  0:00:57s
epoch 78 | loss: 0.23715 | val_0_rmse: 0.46819 | val_1_rmse: 0.51817 |  0:00:57s
epoch 79 | loss: 0.22988 | val_0_rmse: 0.47089 | val_1_rmse: 0.51383 |  0:00:58s
epoch 80 | loss: 0.22692 | val_0_rmse: 0.48207 | val_1_rmse: 0.51609 |  0:00:59s
epoch 81 | loss: 0.22309 | val_0_rmse: 0.4539  | val_1_rmse: 0.50762 |  0:00:59s
epoch 82 | loss: 0.22515 | val_0_rmse: 0.45479 | val_1_rmse: 0.50448 |  0:01:00s
epoch 83 | loss: 0.22889 | val_0_rmse: 0.45073 | val_1_rmse: 0.50022 |  0:01:01s
epoch 84 | loss: 0.22067 | val_0_rmse: 0.44676 | val_1_rmse: 0.49451 |  0:01:02s
epoch 85 | loss: 0.22225 | val_0_rmse: 0.45868 | val_1_rmse: 0.5078  |  0:01:02s
epoch 86 | loss: 0.21781 | val_0_rmse: 0.44987 | val_1_rmse: 0.50026 |  0:01:03s
epoch 87 | loss: 0.21627 | val_0_rmse: 0.46209 | val_1_rmse: 0.51076 |  0:01:04s
epoch 88 | loss: 0.22275 | val_0_rmse: 0.45512 | val_1_rmse: 0.50323 |  0:01:05s
epoch 89 | loss: 0.22286 | val_0_rmse: 0.45231 | val_1_rmse: 0.50027 |  0:01:05s
epoch 90 | loss: 0.21773 | val_0_rmse: 0.45154 | val_1_rmse: 0.4997  |  0:01:06s
epoch 91 | loss: 0.21891 | val_0_rmse: 0.45356 | val_1_rmse: 0.50124 |  0:01:07s
epoch 92 | loss: 0.21918 | val_0_rmse: 0.44458 | val_1_rmse: 0.49142 |  0:01:08s
epoch 93 | loss: 0.21865 | val_0_rmse: 0.45309 | val_1_rmse: 0.5144  |  0:01:08s
epoch 94 | loss: 0.21685 | val_0_rmse: 0.45575 | val_1_rmse: 0.51756 |  0:01:09s
epoch 95 | loss: 0.21224 | val_0_rmse: 0.46851 | val_1_rmse: 0.51347 |  0:01:10s
epoch 96 | loss: 0.22844 | val_0_rmse: 0.45541 | val_1_rmse: 0.50981 |  0:01:10s
epoch 97 | loss: 0.22823 | val_0_rmse: 0.47584 | val_1_rmse: 0.52844 |  0:01:11s
epoch 98 | loss: 0.22283 | val_0_rmse: 0.45358 | val_1_rmse: 0.5048  |  0:01:12s
epoch 99 | loss: 0.22146 | val_0_rmse: 0.45863 | val_1_rmse: 0.50744 |  0:01:13s
epoch 100| loss: 0.22016 | val_0_rmse: 0.44542 | val_1_rmse: 0.50486 |  0:01:13s
epoch 101| loss: 0.21551 | val_0_rmse: 0.46075 | val_1_rmse: 0.52499 |  0:01:14s
epoch 102| loss: 0.21804 | val_0_rmse: 0.44913 | val_1_rmse: 0.49944 |  0:01:15s
epoch 103| loss: 0.21618 | val_0_rmse: 0.44929 | val_1_rmse: 0.5     |  0:01:16s
epoch 104| loss: 0.22076 | val_0_rmse: 0.44798 | val_1_rmse: 0.49175 |  0:01:16s
epoch 105| loss: 0.21756 | val_0_rmse: 0.45702 | val_1_rmse: 0.51351 |  0:01:17s
epoch 106| loss: 0.21566 | val_0_rmse: 0.43472 | val_1_rmse: 0.49147 |  0:01:18s
epoch 107| loss: 0.21038 | val_0_rmse: 0.45029 | val_1_rmse: 0.5008  |  0:01:18s
epoch 108| loss: 0.21255 | val_0_rmse: 0.4496  | val_1_rmse: 0.5051  |  0:01:19s
epoch 109| loss: 0.21988 | val_0_rmse: 0.45031 | val_1_rmse: 0.49489 |  0:01:20s
epoch 110| loss: 0.21772 | val_0_rmse: 0.4516  | val_1_rmse: 0.50738 |  0:01:21s
epoch 111| loss: 0.21831 | val_0_rmse: 0.44514 | val_1_rmse: 0.50001 |  0:01:21s
epoch 112| loss: 0.21737 | val_0_rmse: 0.44971 | val_1_rmse: 0.50137 |  0:01:22s
epoch 113| loss: 0.22086 | val_0_rmse: 0.4599  | val_1_rmse: 0.51799 |  0:01:23s
epoch 114| loss: 0.21457 | val_0_rmse: 0.45395 | val_1_rmse: 0.51603 |  0:01:24s
epoch 115| loss: 0.22046 | val_0_rmse: 0.44175 | val_1_rmse: 0.4991  |  0:01:24s
epoch 116| loss: 0.22259 | val_0_rmse: 0.45686 | val_1_rmse: 0.51254 |  0:01:25s
epoch 117| loss: 0.21654 | val_0_rmse: 0.44077 | val_1_rmse: 0.49533 |  0:01:26s
epoch 118| loss: 0.22037 | val_0_rmse: 0.44271 | val_1_rmse: 0.50081 |  0:01:26s
epoch 119| loss: 0.21463 | val_0_rmse: 0.4482  | val_1_rmse: 0.4984  |  0:01:27s
epoch 120| loss: 0.21433 | val_0_rmse: 0.44959 | val_1_rmse: 0.50385 |  0:01:28s
epoch 121| loss: 0.21361 | val_0_rmse: 0.45107 | val_1_rmse: 0.50231 |  0:01:29s
epoch 122| loss: 0.21658 | val_0_rmse: 0.45261 | val_1_rmse: 0.50337 |  0:01:29s

Early stopping occured at epoch 122 with best_epoch = 92 and best_val_1_rmse = 0.49142
Best weights from best epoch are automatically used!
ended training at: 10:48:28
Feature importance:
[('Area', 0.37813842368490214), ('Baths', 5.321936637222351e-05), ('Beds', 0.04225538781753232), ('Latitude', 0.31727977352706005), ('Longitude', 0.1496440855736041), ('Month', 0.03350101422352366), ('Year', 0.07912809580700553)]
Mean squared error is of 6538311006.039549
Mean absolute error:58306.616703271815
MAPE:0.15786926389312697
R2 score:0.7776909320784552
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:48:28
epoch 0  | loss: 0.76292 | val_0_rmse: 0.79123 | val_1_rmse: 0.78503 |  0:00:00s
epoch 1  | loss: 0.39681 | val_0_rmse: 0.65813 | val_1_rmse: 0.67124 |  0:00:01s
epoch 2  | loss: 0.35688 | val_0_rmse: 0.60477 | val_1_rmse: 0.60418 |  0:00:02s
epoch 3  | loss: 0.33086 | val_0_rmse: 0.57649 | val_1_rmse: 0.58017 |  0:00:02s
epoch 4  | loss: 0.31565 | val_0_rmse: 0.54244 | val_1_rmse: 0.54131 |  0:00:03s
epoch 5  | loss: 0.30151 | val_0_rmse: 0.55294 | val_1_rmse: 0.55663 |  0:00:04s
epoch 6  | loss: 0.30155 | val_0_rmse: 0.53366 | val_1_rmse: 0.52575 |  0:00:05s
epoch 7  | loss: 0.30111 | val_0_rmse: 0.53783 | val_1_rmse: 0.53756 |  0:00:05s
epoch 8  | loss: 0.28792 | val_0_rmse: 0.52336 | val_1_rmse: 0.52187 |  0:00:06s
epoch 9  | loss: 0.2812  | val_0_rmse: 0.52763 | val_1_rmse: 0.5186  |  0:00:07s
epoch 10 | loss: 0.28654 | val_0_rmse: 0.51173 | val_1_rmse: 0.50755 |  0:00:08s
epoch 11 | loss: 0.28238 | val_0_rmse: 0.51693 | val_1_rmse: 0.51551 |  0:00:08s
epoch 12 | loss: 0.27902 | val_0_rmse: 0.50709 | val_1_rmse: 0.50357 |  0:00:09s
epoch 13 | loss: 0.27077 | val_0_rmse: 0.50361 | val_1_rmse: 0.50139 |  0:00:10s
epoch 14 | loss: 0.26956 | val_0_rmse: 0.52992 | val_1_rmse: 0.53472 |  0:00:10s
epoch 15 | loss: 0.27281 | val_0_rmse: 0.49599 | val_1_rmse: 0.49953 |  0:00:11s
epoch 16 | loss: 0.26191 | val_0_rmse: 0.49874 | val_1_rmse: 0.50077 |  0:00:12s
epoch 17 | loss: 0.25714 | val_0_rmse: 0.50613 | val_1_rmse: 0.51219 |  0:00:13s
epoch 18 | loss: 0.26056 | val_0_rmse: 0.53399 | val_1_rmse: 0.53339 |  0:00:13s
epoch 19 | loss: 0.27254 | val_0_rmse: 0.5062  | val_1_rmse: 0.50558 |  0:00:14s
epoch 20 | loss: 0.2576  | val_0_rmse: 0.49526 | val_1_rmse: 0.49525 |  0:00:15s
epoch 21 | loss: 0.25721 | val_0_rmse: 0.5066  | val_1_rmse: 0.50695 |  0:00:16s
epoch 22 | loss: 0.26177 | val_0_rmse: 0.49447 | val_1_rmse: 0.49141 |  0:00:16s
epoch 23 | loss: 0.25394 | val_0_rmse: 0.4883  | val_1_rmse: 0.48982 |  0:00:17s
epoch 24 | loss: 0.25278 | val_0_rmse: 0.48288 | val_1_rmse: 0.48443 |  0:00:18s
epoch 25 | loss: 0.24649 | val_0_rmse: 0.47673 | val_1_rmse: 0.47906 |  0:00:19s
epoch 26 | loss: 0.23909 | val_0_rmse: 0.4744  | val_1_rmse: 0.47178 |  0:00:19s
epoch 27 | loss: 0.23727 | val_0_rmse: 0.46864 | val_1_rmse: 0.46681 |  0:00:20s
epoch 28 | loss: 0.24018 | val_0_rmse: 0.48973 | val_1_rmse: 0.48229 |  0:00:21s
epoch 29 | loss: 0.24258 | val_0_rmse: 0.49292 | val_1_rmse: 0.49488 |  0:00:21s
epoch 30 | loss: 0.24376 | val_0_rmse: 0.47957 | val_1_rmse: 0.48531 |  0:00:22s
epoch 31 | loss: 0.23921 | val_0_rmse: 0.47314 | val_1_rmse: 0.47471 |  0:00:23s
epoch 32 | loss: 0.24062 | val_0_rmse: 0.4981  | val_1_rmse: 0.48972 |  0:00:24s
epoch 33 | loss: 0.24264 | val_0_rmse: 0.47024 | val_1_rmse: 0.47254 |  0:00:24s
epoch 34 | loss: 0.23943 | val_0_rmse: 0.50044 | val_1_rmse: 0.50784 |  0:00:25s
epoch 35 | loss: 0.23339 | val_0_rmse: 0.45819 | val_1_rmse: 0.46159 |  0:00:26s
epoch 36 | loss: 0.23556 | val_0_rmse: 0.46333 | val_1_rmse: 0.4653  |  0:00:27s
epoch 37 | loss: 0.23662 | val_0_rmse: 0.46982 | val_1_rmse: 0.46857 |  0:00:27s
epoch 38 | loss: 0.22815 | val_0_rmse: 0.47661 | val_1_rmse: 0.48247 |  0:00:28s
epoch 39 | loss: 0.24833 | val_0_rmse: 0.46874 | val_1_rmse: 0.47418 |  0:00:29s
epoch 40 | loss: 0.24146 | val_0_rmse: 0.51389 | val_1_rmse: 0.51294 |  0:00:30s
epoch 41 | loss: 0.23317 | val_0_rmse: 0.47277 | val_1_rmse: 0.48038 |  0:00:30s
epoch 42 | loss: 0.22928 | val_0_rmse: 0.45684 | val_1_rmse: 0.45865 |  0:00:31s
epoch 43 | loss: 0.22764 | val_0_rmse: 0.47297 | val_1_rmse: 0.47903 |  0:00:32s
epoch 44 | loss: 0.23164 | val_0_rmse: 0.48186 | val_1_rmse: 0.48132 |  0:00:32s
epoch 45 | loss: 0.2416  | val_0_rmse: 0.46734 | val_1_rmse: 0.4704  |  0:00:33s
epoch 46 | loss: 0.23145 | val_0_rmse: 0.48336 | val_1_rmse: 0.48954 |  0:00:34s
epoch 47 | loss: 0.22834 | val_0_rmse: 0.48182 | val_1_rmse: 0.48234 |  0:00:35s
epoch 48 | loss: 0.23121 | val_0_rmse: 0.4744  | val_1_rmse: 0.48119 |  0:00:35s
epoch 49 | loss: 0.23685 | val_0_rmse: 0.47098 | val_1_rmse: 0.47456 |  0:00:36s
epoch 50 | loss: 0.22862 | val_0_rmse: 0.50329 | val_1_rmse: 0.50751 |  0:00:37s
epoch 51 | loss: 0.2312  | val_0_rmse: 0.45982 | val_1_rmse: 0.46262 |  0:00:37s
epoch 52 | loss: 0.2297  | val_0_rmse: 0.47332 | val_1_rmse: 0.4808  |  0:00:38s
epoch 53 | loss: 0.229   | val_0_rmse: 0.47042 | val_1_rmse: 0.46954 |  0:00:39s
epoch 54 | loss: 0.22082 | val_0_rmse: 0.46425 | val_1_rmse: 0.46928 |  0:00:40s
epoch 55 | loss: 0.22498 | val_0_rmse: 0.45955 | val_1_rmse: 0.46187 |  0:00:40s
epoch 56 | loss: 0.2285  | val_0_rmse: 0.45065 | val_1_rmse: 0.45593 |  0:00:41s
epoch 57 | loss: 0.22359 | val_0_rmse: 0.47183 | val_1_rmse: 0.47774 |  0:00:42s
epoch 58 | loss: 0.22125 | val_0_rmse: 0.45577 | val_1_rmse: 0.46361 |  0:00:43s
epoch 59 | loss: 0.23014 | val_0_rmse: 0.47787 | val_1_rmse: 0.48473 |  0:00:43s
epoch 60 | loss: 0.23262 | val_0_rmse: 0.46569 | val_1_rmse: 0.47133 |  0:00:44s
epoch 61 | loss: 0.2381  | val_0_rmse: 0.47075 | val_1_rmse: 0.475   |  0:00:45s
epoch 62 | loss: 0.23717 | val_0_rmse: 0.47364 | val_1_rmse: 0.47954 |  0:00:46s
epoch 63 | loss: 0.23033 | val_0_rmse: 0.45885 | val_1_rmse: 0.46314 |  0:00:46s
epoch 64 | loss: 0.23401 | val_0_rmse: 0.46594 | val_1_rmse: 0.47016 |  0:00:47s
epoch 65 | loss: 0.23194 | val_0_rmse: 0.47312 | val_1_rmse: 0.47986 |  0:00:48s
epoch 66 | loss: 0.22907 | val_0_rmse: 0.45849 | val_1_rmse: 0.4619  |  0:00:48s
epoch 67 | loss: 0.22031 | val_0_rmse: 0.47438 | val_1_rmse: 0.48211 |  0:00:49s
epoch 68 | loss: 0.23071 | val_0_rmse: 0.47027 | val_1_rmse: 0.47657 |  0:00:50s
epoch 69 | loss: 0.22263 | val_0_rmse: 0.47497 | val_1_rmse: 0.48888 |  0:00:51s
epoch 70 | loss: 0.22798 | val_0_rmse: 0.46337 | val_1_rmse: 0.46762 |  0:00:51s
epoch 71 | loss: 0.22414 | val_0_rmse: 0.46305 | val_1_rmse: 0.47259 |  0:00:52s
epoch 72 | loss: 0.2215  | val_0_rmse: 0.4593  | val_1_rmse: 0.46504 |  0:00:53s
epoch 73 | loss: 0.22416 | val_0_rmse: 0.46029 | val_1_rmse: 0.47034 |  0:00:53s
epoch 74 | loss: 0.21873 | val_0_rmse: 0.47406 | val_1_rmse: 0.48613 |  0:00:54s
epoch 75 | loss: 0.22176 | val_0_rmse: 0.45357 | val_1_rmse: 0.46388 |  0:00:55s
epoch 76 | loss: 0.21906 | val_0_rmse: 0.44525 | val_1_rmse: 0.45298 |  0:00:56s
epoch 77 | loss: 0.21583 | val_0_rmse: 0.45553 | val_1_rmse: 0.46567 |  0:00:56s
epoch 78 | loss: 0.22515 | val_0_rmse: 0.45451 | val_1_rmse: 0.46275 |  0:00:57s
epoch 79 | loss: 0.22621 | val_0_rmse: 0.4641  | val_1_rmse: 0.47271 |  0:00:58s
epoch 80 | loss: 0.21988 | val_0_rmse: 0.45367 | val_1_rmse: 0.46164 |  0:00:59s
epoch 81 | loss: 0.22197 | val_0_rmse: 0.45154 | val_1_rmse: 0.46079 |  0:00:59s
epoch 82 | loss: 0.22272 | val_0_rmse: 0.46636 | val_1_rmse: 0.4749  |  0:01:00s
epoch 83 | loss: 0.22354 | val_0_rmse: 0.45706 | val_1_rmse: 0.46629 |  0:01:01s
epoch 84 | loss: 0.22166 | val_0_rmse: 0.49553 | val_1_rmse: 0.50219 |  0:01:02s
epoch 85 | loss: 0.22422 | val_0_rmse: 0.47035 | val_1_rmse: 0.47976 |  0:01:02s
epoch 86 | loss: 0.2225  | val_0_rmse: 0.46086 | val_1_rmse: 0.46623 |  0:01:03s
epoch 87 | loss: 0.22095 | val_0_rmse: 0.4534  | val_1_rmse: 0.45946 |  0:01:04s
epoch 88 | loss: 0.22181 | val_0_rmse: 0.45633 | val_1_rmse: 0.4656  |  0:01:04s
epoch 89 | loss: 0.22377 | val_0_rmse: 0.45803 | val_1_rmse: 0.46188 |  0:01:05s
epoch 90 | loss: 0.22648 | val_0_rmse: 0.45549 | val_1_rmse: 0.46219 |  0:01:06s
epoch 91 | loss: 0.22368 | val_0_rmse: 0.4763  | val_1_rmse: 0.48791 |  0:01:07s
epoch 92 | loss: 0.21934 | val_0_rmse: 0.44876 | val_1_rmse: 0.45835 |  0:01:07s
epoch 93 | loss: 0.22337 | val_0_rmse: 0.46711 | val_1_rmse: 0.47165 |  0:01:08s
epoch 94 | loss: 0.22669 | val_0_rmse: 0.47483 | val_1_rmse: 0.47989 |  0:01:09s
epoch 95 | loss: 0.22626 | val_0_rmse: 0.45382 | val_1_rmse: 0.46736 |  0:01:10s
epoch 96 | loss: 0.21671 | val_0_rmse: 0.44737 | val_1_rmse: 0.45758 |  0:01:10s
epoch 97 | loss: 0.21654 | val_0_rmse: 0.44486 | val_1_rmse: 0.45165 |  0:01:11s
epoch 98 | loss: 0.21804 | val_0_rmse: 0.45525 | val_1_rmse: 0.46374 |  0:01:12s
epoch 99 | loss: 0.21416 | val_0_rmse: 0.45731 | val_1_rmse: 0.46481 |  0:01:13s
epoch 100| loss: 0.21955 | val_0_rmse: 0.45669 | val_1_rmse: 0.46922 |  0:01:13s
epoch 101| loss: 0.215   | val_0_rmse: 0.44659 | val_1_rmse: 0.45299 |  0:01:14s
epoch 102| loss: 0.21374 | val_0_rmse: 0.4493  | val_1_rmse: 0.45831 |  0:01:15s
epoch 103| loss: 0.21189 | val_0_rmse: 0.45689 | val_1_rmse: 0.47197 |  0:01:15s
epoch 104| loss: 0.22376 | val_0_rmse: 0.44374 | val_1_rmse: 0.45402 |  0:01:16s
epoch 105| loss: 0.21674 | val_0_rmse: 0.46359 | val_1_rmse: 0.47697 |  0:01:17s
epoch 106| loss: 0.22028 | val_0_rmse: 0.47    | val_1_rmse: 0.47843 |  0:01:18s
epoch 107| loss: 0.22328 | val_0_rmse: 0.45303 | val_1_rmse: 0.46215 |  0:01:18s
epoch 108| loss: 0.22797 | val_0_rmse: 0.45554 | val_1_rmse: 0.46318 |  0:01:19s
epoch 109| loss: 0.22416 | val_0_rmse: 0.45458 | val_1_rmse: 0.46374 |  0:01:20s
epoch 110| loss: 0.21679 | val_0_rmse: 0.45382 | val_1_rmse: 0.46347 |  0:01:21s
epoch 111| loss: 0.21845 | val_0_rmse: 0.44838 | val_1_rmse: 0.45911 |  0:01:21s
epoch 112| loss: 0.21628 | val_0_rmse: 0.44026 | val_1_rmse: 0.45062 |  0:01:22s
epoch 113| loss: 0.21934 | val_0_rmse: 0.46751 | val_1_rmse: 0.47944 |  0:01:23s
epoch 114| loss: 0.21868 | val_0_rmse: 0.45669 | val_1_rmse: 0.46555 |  0:01:24s
epoch 115| loss: 0.21977 | val_0_rmse: 0.45217 | val_1_rmse: 0.4634  |  0:01:24s
epoch 116| loss: 0.21577 | val_0_rmse: 0.44348 | val_1_rmse: 0.45674 |  0:01:25s
epoch 117| loss: 0.21416 | val_0_rmse: 0.44322 | val_1_rmse: 0.4534  |  0:01:26s
epoch 118| loss: 0.21087 | val_0_rmse: 0.43812 | val_1_rmse: 0.449   |  0:01:26s
epoch 119| loss: 0.21224 | val_0_rmse: 0.44035 | val_1_rmse: 0.44931 |  0:01:27s
epoch 120| loss: 0.21386 | val_0_rmse: 0.44339 | val_1_rmse: 0.45031 |  0:01:28s
epoch 121| loss: 0.21161 | val_0_rmse: 0.46854 | val_1_rmse: 0.47703 |  0:01:29s
epoch 122| loss: 0.21933 | val_0_rmse: 0.44905 | val_1_rmse: 0.4599  |  0:01:29s
epoch 123| loss: 0.21715 | val_0_rmse: 0.4472  | val_1_rmse: 0.45767 |  0:01:30s
epoch 124| loss: 0.22282 | val_0_rmse: 0.45463 | val_1_rmse: 0.46294 |  0:01:31s
epoch 125| loss: 0.21716 | val_0_rmse: 0.43934 | val_1_rmse: 0.44901 |  0:01:32s
epoch 126| loss: 0.21471 | val_0_rmse: 0.44483 | val_1_rmse: 0.45378 |  0:01:32s
epoch 127| loss: 0.22029 | val_0_rmse: 0.43987 | val_1_rmse: 0.44972 |  0:01:33s
epoch 128| loss: 0.21742 | val_0_rmse: 0.45336 | val_1_rmse: 0.46806 |  0:01:34s
epoch 129| loss: 0.21568 | val_0_rmse: 0.49658 | val_1_rmse: 0.51097 |  0:01:34s
epoch 130| loss: 0.22282 | val_0_rmse: 0.4514  | val_1_rmse: 0.46007 |  0:01:35s
epoch 131| loss: 0.21417 | val_0_rmse: 0.43884 | val_1_rmse: 0.4533  |  0:01:36s
epoch 132| loss: 0.21358 | val_0_rmse: 0.4393  | val_1_rmse: 0.45171 |  0:01:37s
epoch 133| loss: 0.2112  | val_0_rmse: 0.45028 | val_1_rmse: 0.46219 |  0:01:37s
epoch 134| loss: 0.22334 | val_0_rmse: 0.44701 | val_1_rmse: 0.45905 |  0:01:38s
epoch 135| loss: 0.20793 | val_0_rmse: 0.44294 | val_1_rmse: 0.4542  |  0:01:39s
epoch 136| loss: 0.20911 | val_0_rmse: 0.44142 | val_1_rmse: 0.45726 |  0:01:40s
epoch 137| loss: 0.20758 | val_0_rmse: 0.43479 | val_1_rmse: 0.44714 |  0:01:40s
epoch 138| loss: 0.20783 | val_0_rmse: 0.44108 | val_1_rmse: 0.45087 |  0:01:41s
epoch 139| loss: 0.21264 | val_0_rmse: 0.44104 | val_1_rmse: 0.45531 |  0:01:42s
epoch 140| loss: 0.22003 | val_0_rmse: 0.45363 | val_1_rmse: 0.45944 |  0:01:43s
epoch 141| loss: 0.21047 | val_0_rmse: 0.43559 | val_1_rmse: 0.45239 |  0:01:43s
epoch 142| loss: 0.20652 | val_0_rmse: 0.44236 | val_1_rmse: 0.45259 |  0:01:44s
epoch 143| loss: 0.20977 | val_0_rmse: 0.44039 | val_1_rmse: 0.45041 |  0:01:45s
epoch 144| loss: 0.21695 | val_0_rmse: 0.44026 | val_1_rmse: 0.45676 |  0:01:45s
epoch 145| loss: 0.20988 | val_0_rmse: 0.44538 | val_1_rmse: 0.4606  |  0:01:46s
epoch 146| loss: 0.21101 | val_0_rmse: 0.46696 | val_1_rmse: 0.4724  |  0:01:47s
epoch 147| loss: 0.21446 | val_0_rmse: 0.44296 | val_1_rmse: 0.4538  |  0:01:48s
epoch 148| loss: 0.21472 | val_0_rmse: 0.44685 | val_1_rmse: 0.45839 |  0:01:48s
epoch 149| loss: 0.21251 | val_0_rmse: 0.43684 | val_1_rmse: 0.4507  |  0:01:49s
Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_val_1_rmse = 0.44714
Best weights from best epoch are automatically used!
ended training at: 10:50:18
Feature importance:
[('Area', 0.3887950642380451), ('Baths', 0.0), ('Beds', 0.033737665258997865), ('Latitude', 0.38858309930413515), ('Longitude', 0.10816021669063577), ('Month', 0.0), ('Year', 0.08072395450818615)]
Mean squared error is of 6191796576.445698
Mean absolute error:55573.600599622485
MAPE:0.14435088061343368
R2 score:0.7905480999319755
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:50:18
epoch 0  | loss: 0.76302 | val_0_rmse: 0.7654  | val_1_rmse: 0.79795 |  0:00:00s
epoch 1  | loss: 0.41232 | val_0_rmse: 0.63985 | val_1_rmse: 0.66339 |  0:00:01s
epoch 2  | loss: 0.3555  | val_0_rmse: 0.59824 | val_1_rmse: 0.61733 |  0:00:02s
epoch 3  | loss: 0.33014 | val_0_rmse: 0.57716 | val_1_rmse: 0.58844 |  0:00:02s
epoch 4  | loss: 0.31452 | val_0_rmse: 0.55061 | val_1_rmse: 0.57143 |  0:00:03s
epoch 5  | loss: 0.30822 | val_0_rmse: 0.55734 | val_1_rmse: 0.5761  |  0:00:04s
epoch 6  | loss: 0.29764 | val_0_rmse: 0.53122 | val_1_rmse: 0.55488 |  0:00:05s
epoch 7  | loss: 0.29603 | val_0_rmse: 0.53784 | val_1_rmse: 0.55609 |  0:00:05s
epoch 8  | loss: 0.30331 | val_0_rmse: 0.53789 | val_1_rmse: 0.55895 |  0:00:06s
epoch 9  | loss: 0.2836  | val_0_rmse: 0.53937 | val_1_rmse: 0.55836 |  0:00:07s
epoch 10 | loss: 0.27318 | val_0_rmse: 0.5081  | val_1_rmse: 0.52433 |  0:00:08s
epoch 11 | loss: 0.26088 | val_0_rmse: 0.49854 | val_1_rmse: 0.51799 |  0:00:08s
epoch 12 | loss: 0.24962 | val_0_rmse: 0.4788  | val_1_rmse: 0.50078 |  0:00:09s
epoch 13 | loss: 0.25362 | val_0_rmse: 0.48091 | val_1_rmse: 0.49629 |  0:00:10s
epoch 14 | loss: 0.24978 | val_0_rmse: 0.48452 | val_1_rmse: 0.50711 |  0:00:11s
epoch 15 | loss: 0.25105 | val_0_rmse: 0.4947  | val_1_rmse: 0.51124 |  0:00:11s
epoch 16 | loss: 0.25305 | val_0_rmse: 0.47546 | val_1_rmse: 0.49478 |  0:00:12s
epoch 17 | loss: 0.2475  | val_0_rmse: 0.47716 | val_1_rmse: 0.49503 |  0:00:13s
epoch 18 | loss: 0.24488 | val_0_rmse: 0.488   | val_1_rmse: 0.5107  |  0:00:14s
epoch 19 | loss: 0.24144 | val_0_rmse: 0.47596 | val_1_rmse: 0.49284 |  0:00:14s
epoch 20 | loss: 0.2449  | val_0_rmse: 0.50203 | val_1_rmse: 0.52259 |  0:00:15s
epoch 21 | loss: 0.2378  | val_0_rmse: 0.47219 | val_1_rmse: 0.4885  |  0:00:16s
epoch 22 | loss: 0.24207 | val_0_rmse: 0.4707  | val_1_rmse: 0.48624 |  0:00:16s
epoch 23 | loss: 0.2359  | val_0_rmse: 0.46837 | val_1_rmse: 0.48485 |  0:00:17s
epoch 24 | loss: 0.23693 | val_0_rmse: 0.46666 | val_1_rmse: 0.48318 |  0:00:18s
epoch 25 | loss: 0.2336  | val_0_rmse: 0.47363 | val_1_rmse: 0.48813 |  0:00:19s
epoch 26 | loss: 0.23996 | val_0_rmse: 0.47911 | val_1_rmse: 0.50465 |  0:00:19s
epoch 27 | loss: 0.24017 | val_0_rmse: 0.46964 | val_1_rmse: 0.48435 |  0:00:20s
epoch 28 | loss: 0.23353 | val_0_rmse: 0.48535 | val_1_rmse: 0.50403 |  0:00:21s
epoch 29 | loss: 0.23567 | val_0_rmse: 0.46306 | val_1_rmse: 0.48021 |  0:00:22s
epoch 30 | loss: 0.23738 | val_0_rmse: 0.46205 | val_1_rmse: 0.48312 |  0:00:22s
epoch 31 | loss: 0.2328  | val_0_rmse: 0.46053 | val_1_rmse: 0.48004 |  0:00:23s
epoch 32 | loss: 0.22825 | val_0_rmse: 0.45935 | val_1_rmse: 0.47585 |  0:00:24s
epoch 33 | loss: 0.22245 | val_0_rmse: 0.47778 | val_1_rmse: 0.49926 |  0:00:25s
epoch 34 | loss: 0.23313 | val_0_rmse: 0.47252 | val_1_rmse: 0.49616 |  0:00:25s
epoch 35 | loss: 0.22555 | val_0_rmse: 0.45703 | val_1_rmse: 0.47712 |  0:00:26s
epoch 36 | loss: 0.22821 | val_0_rmse: 0.46131 | val_1_rmse: 0.48232 |  0:00:27s
epoch 37 | loss: 0.22589 | val_0_rmse: 0.45381 | val_1_rmse: 0.46938 |  0:00:27s
epoch 38 | loss: 0.22838 | val_0_rmse: 0.46442 | val_1_rmse: 0.48479 |  0:00:28s
epoch 39 | loss: 0.22437 | val_0_rmse: 0.46375 | val_1_rmse: 0.47919 |  0:00:29s
epoch 40 | loss: 0.22155 | val_0_rmse: 0.46773 | val_1_rmse: 0.48354 |  0:00:30s
epoch 41 | loss: 0.22648 | val_0_rmse: 0.45592 | val_1_rmse: 0.47679 |  0:00:30s
epoch 42 | loss: 0.22398 | val_0_rmse: 0.4539  | val_1_rmse: 0.47621 |  0:00:31s
epoch 43 | loss: 0.2206  | val_0_rmse: 0.45825 | val_1_rmse: 0.48085 |  0:00:32s
epoch 44 | loss: 0.22392 | val_0_rmse: 0.45442 | val_1_rmse: 0.47203 |  0:00:33s
epoch 45 | loss: 0.2251  | val_0_rmse: 0.46328 | val_1_rmse: 0.48053 |  0:00:33s
epoch 46 | loss: 0.23384 | val_0_rmse: 0.4502  | val_1_rmse: 0.47253 |  0:00:34s
epoch 47 | loss: 0.22334 | val_0_rmse: 0.47376 | val_1_rmse: 0.49588 |  0:00:35s
epoch 48 | loss: 0.22585 | val_0_rmse: 0.46739 | val_1_rmse: 0.4853  |  0:00:36s
epoch 49 | loss: 0.23273 | val_0_rmse: 0.45826 | val_1_rmse: 0.47652 |  0:00:36s
epoch 50 | loss: 0.22157 | val_0_rmse: 0.46048 | val_1_rmse: 0.47636 |  0:00:37s
epoch 51 | loss: 0.21828 | val_0_rmse: 0.45304 | val_1_rmse: 0.47371 |  0:00:38s
epoch 52 | loss: 0.21844 | val_0_rmse: 0.45961 | val_1_rmse: 0.47574 |  0:00:38s
epoch 53 | loss: 0.22676 | val_0_rmse: 0.45575 | val_1_rmse: 0.47291 |  0:00:39s
epoch 54 | loss: 0.21857 | val_0_rmse: 0.45047 | val_1_rmse: 0.4656  |  0:00:40s
epoch 55 | loss: 0.22728 | val_0_rmse: 0.44886 | val_1_rmse: 0.47038 |  0:00:41s
epoch 56 | loss: 0.22313 | val_0_rmse: 0.45823 | val_1_rmse: 0.47694 |  0:00:41s
epoch 57 | loss: 0.21787 | val_0_rmse: 0.45255 | val_1_rmse: 0.47108 |  0:00:42s
epoch 58 | loss: 0.21897 | val_0_rmse: 0.45275 | val_1_rmse: 0.46949 |  0:00:43s
epoch 59 | loss: 0.21599 | val_0_rmse: 0.45537 | val_1_rmse: 0.47249 |  0:00:44s
epoch 60 | loss: 0.21792 | val_0_rmse: 0.45566 | val_1_rmse: 0.47672 |  0:00:44s
epoch 61 | loss: 0.21834 | val_0_rmse: 0.45548 | val_1_rmse: 0.47408 |  0:00:45s
epoch 62 | loss: 0.21812 | val_0_rmse: 0.45716 | val_1_rmse: 0.47363 |  0:00:46s
epoch 63 | loss: 0.21886 | val_0_rmse: 0.44637 | val_1_rmse: 0.46367 |  0:00:46s
epoch 64 | loss: 0.21735 | val_0_rmse: 0.45153 | val_1_rmse: 0.47701 |  0:00:47s
epoch 65 | loss: 0.21976 | val_0_rmse: 0.45414 | val_1_rmse: 0.47243 |  0:00:48s
epoch 66 | loss: 0.21415 | val_0_rmse: 0.44545 | val_1_rmse: 0.46507 |  0:00:49s
epoch 67 | loss: 0.2121  | val_0_rmse: 0.43934 | val_1_rmse: 0.46134 |  0:00:49s
epoch 68 | loss: 0.21571 | val_0_rmse: 0.44821 | val_1_rmse: 0.46658 |  0:00:50s
epoch 69 | loss: 0.21203 | val_0_rmse: 0.4481  | val_1_rmse: 0.46605 |  0:00:51s
epoch 70 | loss: 0.21713 | val_0_rmse: 0.44016 | val_1_rmse: 0.46199 |  0:00:52s
epoch 71 | loss: 0.22468 | val_0_rmse: 0.46764 | val_1_rmse: 0.49073 |  0:00:52s
epoch 72 | loss: 0.22717 | val_0_rmse: 0.45313 | val_1_rmse: 0.47324 |  0:00:53s
epoch 73 | loss: 0.2107  | val_0_rmse: 0.4536  | val_1_rmse: 0.47899 |  0:00:54s
epoch 74 | loss: 0.2223  | val_0_rmse: 0.44901 | val_1_rmse: 0.47086 |  0:00:54s
epoch 75 | loss: 0.21783 | val_0_rmse: 0.44537 | val_1_rmse: 0.46385 |  0:00:55s
epoch 76 | loss: 0.21315 | val_0_rmse: 0.46247 | val_1_rmse: 0.49157 |  0:00:56s
epoch 77 | loss: 0.21776 | val_0_rmse: 0.45056 | val_1_rmse: 0.47595 |  0:00:57s
epoch 78 | loss: 0.20587 | val_0_rmse: 0.44043 | val_1_rmse: 0.46426 |  0:00:57s
epoch 79 | loss: 0.21019 | val_0_rmse: 0.455   | val_1_rmse: 0.48267 |  0:00:58s
epoch 80 | loss: 0.20994 | val_0_rmse: 0.43973 | val_1_rmse: 0.45932 |  0:00:59s
epoch 81 | loss: 0.20901 | val_0_rmse: 0.44456 | val_1_rmse: 0.46507 |  0:01:00s
epoch 82 | loss: 0.20828 | val_0_rmse: 0.45693 | val_1_rmse: 0.4837  |  0:01:00s
epoch 83 | loss: 0.21886 | val_0_rmse: 0.47658 | val_1_rmse: 0.50183 |  0:01:01s
epoch 84 | loss: 0.21866 | val_0_rmse: 0.43846 | val_1_rmse: 0.46689 |  0:01:02s
epoch 85 | loss: 0.21998 | val_0_rmse: 0.4528  | val_1_rmse: 0.47116 |  0:01:02s
epoch 86 | loss: 0.21717 | val_0_rmse: 0.45618 | val_1_rmse: 0.48326 |  0:01:03s
epoch 87 | loss: 0.21833 | val_0_rmse: 0.44075 | val_1_rmse: 0.46098 |  0:01:04s
epoch 88 | loss: 0.21032 | val_0_rmse: 0.44594 | val_1_rmse: 0.47258 |  0:01:05s
epoch 89 | loss: 0.21274 | val_0_rmse: 0.44992 | val_1_rmse: 0.47948 |  0:01:05s
epoch 90 | loss: 0.21402 | val_0_rmse: 0.43919 | val_1_rmse: 0.46456 |  0:01:06s
epoch 91 | loss: 0.21064 | val_0_rmse: 0.43922 | val_1_rmse: 0.46577 |  0:01:07s
epoch 92 | loss: 0.21048 | val_0_rmse: 0.46217 | val_1_rmse: 0.48153 |  0:01:08s
epoch 93 | loss: 0.20892 | val_0_rmse: 0.43674 | val_1_rmse: 0.45775 |  0:01:08s
epoch 94 | loss: 0.21317 | val_0_rmse: 0.44545 | val_1_rmse: 0.46659 |  0:01:09s
epoch 95 | loss: 0.21578 | val_0_rmse: 0.44254 | val_1_rmse: 0.46531 |  0:01:10s
epoch 96 | loss: 0.21003 | val_0_rmse: 0.43565 | val_1_rmse: 0.46545 |  0:01:10s
epoch 97 | loss: 0.21418 | val_0_rmse: 0.43685 | val_1_rmse: 0.46126 |  0:01:11s
epoch 98 | loss: 0.21568 | val_0_rmse: 0.43998 | val_1_rmse: 0.46516 |  0:01:12s
epoch 99 | loss: 0.20757 | val_0_rmse: 0.4369  | val_1_rmse: 0.45925 |  0:01:13s
epoch 100| loss: 0.20954 | val_0_rmse: 0.4582  | val_1_rmse: 0.48058 |  0:01:13s
epoch 101| loss: 0.21185 | val_0_rmse: 0.43964 | val_1_rmse: 0.46556 |  0:01:14s
epoch 102| loss: 0.20817 | val_0_rmse: 0.43766 | val_1_rmse: 0.4622  |  0:01:15s
epoch 103| loss: 0.20522 | val_0_rmse: 0.43898 | val_1_rmse: 0.45428 |  0:01:16s
epoch 104| loss: 0.21447 | val_0_rmse: 0.45584 | val_1_rmse: 0.47639 |  0:01:16s
epoch 105| loss: 0.21153 | val_0_rmse: 0.45137 | val_1_rmse: 0.47266 |  0:01:17s
epoch 106| loss: 0.2118  | val_0_rmse: 0.43436 | val_1_rmse: 0.45593 |  0:01:18s
epoch 107| loss: 0.20261 | val_0_rmse: 0.45211 | val_1_rmse: 0.47805 |  0:01:19s
epoch 108| loss: 0.21061 | val_0_rmse: 0.435   | val_1_rmse: 0.45536 |  0:01:19s
epoch 109| loss: 0.21266 | val_0_rmse: 0.45503 | val_1_rmse: 0.46888 |  0:01:20s
epoch 110| loss: 0.21461 | val_0_rmse: 0.44559 | val_1_rmse: 0.46745 |  0:01:21s
epoch 111| loss: 0.2077  | val_0_rmse: 0.46586 | val_1_rmse: 0.47959 |  0:01:21s
epoch 112| loss: 0.21678 | val_0_rmse: 0.43425 | val_1_rmse: 0.4531  |  0:01:22s
epoch 113| loss: 0.21268 | val_0_rmse: 0.44049 | val_1_rmse: 0.45995 |  0:01:23s
epoch 114| loss: 0.2009  | val_0_rmse: 0.44202 | val_1_rmse: 0.46593 |  0:01:24s
epoch 115| loss: 0.20614 | val_0_rmse: 0.44144 | val_1_rmse: 0.46262 |  0:01:24s
epoch 116| loss: 0.20947 | val_0_rmse: 0.42724 | val_1_rmse: 0.45239 |  0:01:25s
epoch 117| loss: 0.2104  | val_0_rmse: 0.43516 | val_1_rmse: 0.4561  |  0:01:26s
epoch 118| loss: 0.20723 | val_0_rmse: 0.43755 | val_1_rmse: 0.46423 |  0:01:27s
epoch 119| loss: 0.20516 | val_0_rmse: 0.43797 | val_1_rmse: 0.45845 |  0:01:27s
epoch 120| loss: 0.21189 | val_0_rmse: 0.45171 | val_1_rmse: 0.47338 |  0:01:28s
epoch 121| loss: 0.20393 | val_0_rmse: 0.43702 | val_1_rmse: 0.46071 |  0:01:29s
epoch 122| loss: 0.20128 | val_0_rmse: 0.42893 | val_1_rmse: 0.45171 |  0:01:29s
epoch 123| loss: 0.20132 | val_0_rmse: 0.43447 | val_1_rmse: 0.45391 |  0:01:30s
epoch 124| loss: 0.21864 | val_0_rmse: 0.46889 | val_1_rmse: 0.4817  |  0:01:31s
epoch 125| loss: 0.21595 | val_0_rmse: 0.43889 | val_1_rmse: 0.46283 |  0:01:32s
epoch 126| loss: 0.20493 | val_0_rmse: 0.43849 | val_1_rmse: 0.45681 |  0:01:32s
epoch 127| loss: 0.2     | val_0_rmse: 0.43135 | val_1_rmse: 0.45246 |  0:01:33s
epoch 128| loss: 0.19945 | val_0_rmse: 0.43057 | val_1_rmse: 0.45458 |  0:01:34s
epoch 129| loss: 0.20506 | val_0_rmse: 0.42815 | val_1_rmse: 0.44639 |  0:01:35s
epoch 130| loss: 0.20711 | val_0_rmse: 0.44872 | val_1_rmse: 0.47189 |  0:01:35s
epoch 131| loss: 0.2059  | val_0_rmse: 0.43526 | val_1_rmse: 0.45925 |  0:01:36s
epoch 132| loss: 0.2044  | val_0_rmse: 0.43178 | val_1_rmse: 0.45355 |  0:01:37s
epoch 133| loss: 0.20383 | val_0_rmse: 0.43381 | val_1_rmse: 0.45969 |  0:01:38s
epoch 134| loss: 0.20525 | val_0_rmse: 0.43691 | val_1_rmse: 0.45973 |  0:01:38s
epoch 135| loss: 0.20353 | val_0_rmse: 0.43111 | val_1_rmse: 0.45462 |  0:01:39s
epoch 136| loss: 0.20777 | val_0_rmse: 0.44084 | val_1_rmse: 0.46279 |  0:01:40s
epoch 137| loss: 0.20613 | val_0_rmse: 0.4263  | val_1_rmse: 0.45094 |  0:01:40s
epoch 138| loss: 0.20435 | val_0_rmse: 0.44007 | val_1_rmse: 0.4664  |  0:01:41s
epoch 139| loss: 0.20353 | val_0_rmse: 0.43443 | val_1_rmse: 0.45449 |  0:01:42s
epoch 140| loss: 0.20017 | val_0_rmse: 0.42429 | val_1_rmse: 0.44832 |  0:01:43s
epoch 141| loss: 0.20436 | val_0_rmse: 0.43653 | val_1_rmse: 0.45628 |  0:01:43s
epoch 142| loss: 0.20467 | val_0_rmse: 0.43122 | val_1_rmse: 0.46117 |  0:01:44s
epoch 143| loss: 0.20054 | val_0_rmse: 0.46804 | val_1_rmse: 0.48771 |  0:01:45s
epoch 144| loss: 0.2138  | val_0_rmse: 0.44286 | val_1_rmse: 0.46699 |  0:01:46s
epoch 145| loss: 0.20473 | val_0_rmse: 0.45424 | val_1_rmse: 0.46773 |  0:01:46s
epoch 146| loss: 0.20908 | val_0_rmse: 0.44884 | val_1_rmse: 0.46949 |  0:01:47s
epoch 147| loss: 0.20528 | val_0_rmse: 0.44877 | val_1_rmse: 0.46647 |  0:01:48s
epoch 148| loss: 0.20531 | val_0_rmse: 0.44461 | val_1_rmse: 0.46884 |  0:01:48s
epoch 149| loss: 0.20116 | val_0_rmse: 0.4354  | val_1_rmse: 0.46054 |  0:01:49s
Stop training because you reached max_epochs = 150 with best_epoch = 129 and best_val_1_rmse = 0.44639
Best weights from best epoch are automatically used!
ended training at: 10:52:08
Feature importance:
[('Area', 0.40294240117885416), ('Baths', 0.03629116868168648), ('Beds', 0.004547391890864405), ('Latitude', 0.35521235417819463), ('Longitude', 0.20100668407040034), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 6632310619.116421
Mean absolute error:57067.57983175336
MAPE:0.14884511853176532
R2 score:0.7870107161357572
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:52:08
epoch 0  | loss: 0.73764 | val_0_rmse: 0.81965 | val_1_rmse: 0.84136 |  0:00:00s
epoch 1  | loss: 0.38729 | val_0_rmse: 0.61937 | val_1_rmse: 0.60641 |  0:00:01s
epoch 2  | loss: 0.33689 | val_0_rmse: 0.58315 | val_1_rmse: 0.57469 |  0:00:02s
epoch 3  | loss: 0.32587 | val_0_rmse: 0.57704 | val_1_rmse: 0.57335 |  0:00:02s
epoch 4  | loss: 0.30066 | val_0_rmse: 0.53571 | val_1_rmse: 0.5384  |  0:00:03s
epoch 5  | loss: 0.29838 | val_0_rmse: 0.57762 | val_1_rmse: 0.58299 |  0:00:04s
epoch 6  | loss: 0.2998  | val_0_rmse: 0.52031 | val_1_rmse: 0.52085 |  0:00:05s
epoch 7  | loss: 0.29292 | val_0_rmse: 0.52259 | val_1_rmse: 0.53157 |  0:00:05s
epoch 8  | loss: 0.28719 | val_0_rmse: 0.52991 | val_1_rmse: 0.53057 |  0:00:06s
epoch 9  | loss: 0.29358 | val_0_rmse: 0.54464 | val_1_rmse: 0.56014 |  0:00:07s
epoch 10 | loss: 0.29651 | val_0_rmse: 0.54479 | val_1_rmse: 0.55582 |  0:00:08s
epoch 11 | loss: 0.28706 | val_0_rmse: 0.53173 | val_1_rmse: 0.53989 |  0:00:08s
epoch 12 | loss: 0.27872 | val_0_rmse: 0.54161 | val_1_rmse: 0.54759 |  0:00:09s
epoch 13 | loss: 0.27887 | val_0_rmse: 0.50534 | val_1_rmse: 0.50981 |  0:00:10s
epoch 14 | loss: 0.28101 | val_0_rmse: 0.50784 | val_1_rmse: 0.50804 |  0:00:11s
epoch 15 | loss: 0.27643 | val_0_rmse: 0.51516 | val_1_rmse: 0.50949 |  0:00:11s
epoch 16 | loss: 0.27288 | val_0_rmse: 0.50459 | val_1_rmse: 0.50182 |  0:00:12s
epoch 17 | loss: 0.26909 | val_0_rmse: 0.50175 | val_1_rmse: 0.5034  |  0:00:13s
epoch 18 | loss: 0.26914 | val_0_rmse: 0.50031 | val_1_rmse: 0.50397 |  0:00:13s
epoch 19 | loss: 0.26493 | val_0_rmse: 0.504   | val_1_rmse: 0.50792 |  0:00:14s
epoch 20 | loss: 0.28746 | val_0_rmse: 0.51982 | val_1_rmse: 0.52687 |  0:00:15s
epoch 21 | loss: 0.2802  | val_0_rmse: 0.51774 | val_1_rmse: 0.52333 |  0:00:16s
epoch 22 | loss: 0.27335 | val_0_rmse: 0.50079 | val_1_rmse: 0.50814 |  0:00:16s
epoch 23 | loss: 0.27169 | val_0_rmse: 0.50099 | val_1_rmse: 0.50464 |  0:00:17s
epoch 24 | loss: 0.26851 | val_0_rmse: 0.51546 | val_1_rmse: 0.52236 |  0:00:18s
epoch 25 | loss: 0.26487 | val_0_rmse: 0.50732 | val_1_rmse: 0.51545 |  0:00:19s
epoch 26 | loss: 0.26564 | val_0_rmse: 0.49452 | val_1_rmse: 0.50218 |  0:00:19s
epoch 27 | loss: 0.26052 | val_0_rmse: 0.53384 | val_1_rmse: 0.52683 |  0:00:20s
epoch 28 | loss: 0.26798 | val_0_rmse: 0.49653 | val_1_rmse: 0.50522 |  0:00:21s
epoch 29 | loss: 0.25996 | val_0_rmse: 0.49933 | val_1_rmse: 0.50757 |  0:00:21s
epoch 30 | loss: 0.25727 | val_0_rmse: 0.48922 | val_1_rmse: 0.49576 |  0:00:22s
epoch 31 | loss: 0.26225 | val_0_rmse: 0.4974  | val_1_rmse: 0.49865 |  0:00:23s
epoch 32 | loss: 0.2528  | val_0_rmse: 0.49242 | val_1_rmse: 0.49145 |  0:00:24s
epoch 33 | loss: 0.25066 | val_0_rmse: 0.49785 | val_1_rmse: 0.511   |  0:00:24s
epoch 34 | loss: 0.25784 | val_0_rmse: 0.49855 | val_1_rmse: 0.51131 |  0:00:25s
epoch 35 | loss: 0.25814 | val_0_rmse: 0.50057 | val_1_rmse: 0.51392 |  0:00:26s
epoch 36 | loss: 0.25343 | val_0_rmse: 0.49477 | val_1_rmse: 0.50162 |  0:00:27s
epoch 37 | loss: 0.25331 | val_0_rmse: 0.48886 | val_1_rmse: 0.4894  |  0:00:27s
epoch 38 | loss: 0.24962 | val_0_rmse: 0.48657 | val_1_rmse: 0.49116 |  0:00:28s
epoch 39 | loss: 0.24874 | val_0_rmse: 0.48578 | val_1_rmse: 0.48882 |  0:00:29s
epoch 40 | loss: 0.25232 | val_0_rmse: 0.48976 | val_1_rmse: 0.4997  |  0:00:29s
epoch 41 | loss: 0.24855 | val_0_rmse: 0.49359 | val_1_rmse: 0.50545 |  0:00:30s
epoch 42 | loss: 0.25129 | val_0_rmse: 0.49501 | val_1_rmse: 0.50426 |  0:00:31s
epoch 43 | loss: 0.25105 | val_0_rmse: 0.49248 | val_1_rmse: 0.50063 |  0:00:32s
epoch 44 | loss: 0.24808 | val_0_rmse: 0.48173 | val_1_rmse: 0.49096 |  0:00:32s
epoch 45 | loss: 0.2468  | val_0_rmse: 0.47927 | val_1_rmse: 0.48593 |  0:00:33s
epoch 46 | loss: 0.24039 | val_0_rmse: 0.47771 | val_1_rmse: 0.48487 |  0:00:34s
epoch 47 | loss: 0.24768 | val_0_rmse: 0.48532 | val_1_rmse: 0.49075 |  0:00:35s
epoch 48 | loss: 0.24517 | val_0_rmse: 0.48799 | val_1_rmse: 0.49392 |  0:00:35s
epoch 49 | loss: 0.24566 | val_0_rmse: 0.47822 | val_1_rmse: 0.48646 |  0:00:36s
epoch 50 | loss: 0.24081 | val_0_rmse: 0.47595 | val_1_rmse: 0.48323 |  0:00:37s
epoch 51 | loss: 0.2397  | val_0_rmse: 0.48164 | val_1_rmse: 0.48109 |  0:00:38s
epoch 52 | loss: 0.24404 | val_0_rmse: 0.48144 | val_1_rmse: 0.48775 |  0:00:38s
epoch 53 | loss: 0.23761 | val_0_rmse: 0.48316 | val_1_rmse: 0.48419 |  0:00:39s
epoch 54 | loss: 0.24113 | val_0_rmse: 0.46931 | val_1_rmse: 0.47255 |  0:00:40s
epoch 55 | loss: 0.23758 | val_0_rmse: 0.48124 | val_1_rmse: 0.48971 |  0:00:41s
epoch 56 | loss: 0.23919 | val_0_rmse: 0.48316 | val_1_rmse: 0.49311 |  0:00:41s
epoch 57 | loss: 0.24433 | val_0_rmse: 0.48359 | val_1_rmse: 0.49801 |  0:00:42s
epoch 58 | loss: 0.24104 | val_0_rmse: 0.47784 | val_1_rmse: 0.49387 |  0:00:43s
epoch 59 | loss: 0.24221 | val_0_rmse: 0.4749  | val_1_rmse: 0.48755 |  0:00:43s
epoch 60 | loss: 0.24206 | val_0_rmse: 0.47712 | val_1_rmse: 0.48862 |  0:00:44s
epoch 61 | loss: 0.23891 | val_0_rmse: 0.46945 | val_1_rmse: 0.48492 |  0:00:45s
epoch 62 | loss: 0.2396  | val_0_rmse: 0.494   | val_1_rmse: 0.49889 |  0:00:46s
epoch 63 | loss: 0.23821 | val_0_rmse: 0.47348 | val_1_rmse: 0.48487 |  0:00:46s
epoch 64 | loss: 0.23575 | val_0_rmse: 0.48249 | val_1_rmse: 0.49798 |  0:00:47s
epoch 65 | loss: 0.23629 | val_0_rmse: 0.46368 | val_1_rmse: 0.474   |  0:00:48s
epoch 66 | loss: 0.23316 | val_0_rmse: 0.46922 | val_1_rmse: 0.47905 |  0:00:48s
epoch 67 | loss: 0.23346 | val_0_rmse: 0.47016 | val_1_rmse: 0.48141 |  0:00:49s
epoch 68 | loss: 0.23431 | val_0_rmse: 0.46897 | val_1_rmse: 0.48431 |  0:00:50s
epoch 69 | loss: 0.23644 | val_0_rmse: 0.47093 | val_1_rmse: 0.47897 |  0:00:51s
epoch 70 | loss: 0.23378 | val_0_rmse: 0.46215 | val_1_rmse: 0.46696 |  0:00:51s
epoch 71 | loss: 0.23092 | val_0_rmse: 0.46588 | val_1_rmse: 0.48038 |  0:00:52s
epoch 72 | loss: 0.23293 | val_0_rmse: 0.46118 | val_1_rmse: 0.47251 |  0:00:53s
epoch 73 | loss: 0.22759 | val_0_rmse: 0.46464 | val_1_rmse: 0.46832 |  0:00:54s
epoch 74 | loss: 0.23053 | val_0_rmse: 0.47123 | val_1_rmse: 0.48411 |  0:00:54s
epoch 75 | loss: 0.23096 | val_0_rmse: 0.45791 | val_1_rmse: 0.47191 |  0:00:55s
epoch 76 | loss: 0.2263  | val_0_rmse: 0.46802 | val_1_rmse: 0.47745 |  0:00:56s
epoch 77 | loss: 0.22802 | val_0_rmse: 0.46255 | val_1_rmse: 0.48084 |  0:00:56s
epoch 78 | loss: 0.23076 | val_0_rmse: 0.45959 | val_1_rmse: 0.46792 |  0:00:57s
epoch 79 | loss: 0.22799 | val_0_rmse: 0.46087 | val_1_rmse: 0.47092 |  0:00:58s
epoch 80 | loss: 0.22769 | val_0_rmse: 0.47599 | val_1_rmse: 0.48218 |  0:00:59s
epoch 81 | loss: 0.23119 | val_0_rmse: 0.47039 | val_1_rmse: 0.48599 |  0:00:59s
epoch 82 | loss: 0.23054 | val_0_rmse: 0.46449 | val_1_rmse: 0.46999 |  0:01:00s
epoch 83 | loss: 0.23015 | val_0_rmse: 0.45568 | val_1_rmse: 0.4683  |  0:01:01s
epoch 84 | loss: 0.2285  | val_0_rmse: 0.46073 | val_1_rmse: 0.47284 |  0:01:02s
epoch 85 | loss: 0.22874 | val_0_rmse: 0.45746 | val_1_rmse: 0.47144 |  0:01:02s
epoch 86 | loss: 0.22056 | val_0_rmse: 0.46723 | val_1_rmse: 0.48527 |  0:01:03s
epoch 87 | loss: 0.22518 | val_0_rmse: 0.46205 | val_1_rmse: 0.47495 |  0:01:04s
epoch 88 | loss: 0.2259  | val_0_rmse: 0.47934 | val_1_rmse: 0.49715 |  0:01:05s
epoch 89 | loss: 0.22521 | val_0_rmse: 0.46069 | val_1_rmse: 0.47409 |  0:01:05s
epoch 90 | loss: 0.22597 | val_0_rmse: 0.46398 | val_1_rmse: 0.47108 |  0:01:06s
epoch 91 | loss: 0.23174 | val_0_rmse: 0.46387 | val_1_rmse: 0.47833 |  0:01:07s
epoch 92 | loss: 0.22837 | val_0_rmse: 0.4534  | val_1_rmse: 0.46263 |  0:01:07s
epoch 93 | loss: 0.22492 | val_0_rmse: 0.45519 | val_1_rmse: 0.46348 |  0:01:08s
epoch 94 | loss: 0.2265  | val_0_rmse: 0.47674 | val_1_rmse: 0.48953 |  0:01:09s
epoch 95 | loss: 0.22806 | val_0_rmse: 0.47369 | val_1_rmse: 0.49339 |  0:01:10s
epoch 96 | loss: 0.22236 | val_0_rmse: 0.46919 | val_1_rmse: 0.48239 |  0:01:10s
epoch 97 | loss: 0.22844 | val_0_rmse: 0.45575 | val_1_rmse: 0.46675 |  0:01:11s
epoch 98 | loss: 0.2247  | val_0_rmse: 0.45958 | val_1_rmse: 0.46798 |  0:01:12s
epoch 99 | loss: 0.22281 | val_0_rmse: 0.45133 | val_1_rmse: 0.4703  |  0:01:13s
epoch 100| loss: 0.22826 | val_0_rmse: 0.47795 | val_1_rmse: 0.47806 |  0:01:13s
epoch 101| loss: 0.22854 | val_0_rmse: 0.45545 | val_1_rmse: 0.46863 |  0:01:14s
epoch 102| loss: 0.22638 | val_0_rmse: 0.45903 | val_1_rmse: 0.46866 |  0:01:15s
epoch 103| loss: 0.21626 | val_0_rmse: 0.45577 | val_1_rmse: 0.47233 |  0:01:16s
epoch 104| loss: 0.2225  | val_0_rmse: 0.45218 | val_1_rmse: 0.46119 |  0:01:16s
epoch 105| loss: 0.22461 | val_0_rmse: 0.45647 | val_1_rmse: 0.47809 |  0:01:17s
epoch 106| loss: 0.22525 | val_0_rmse: 0.45727 | val_1_rmse: 0.47424 |  0:01:18s
epoch 107| loss: 0.22296 | val_0_rmse: 0.45865 | val_1_rmse: 0.47697 |  0:01:18s
epoch 108| loss: 0.22179 | val_0_rmse: 0.46534 | val_1_rmse: 0.48399 |  0:01:19s
epoch 109| loss: 0.2164  | val_0_rmse: 0.45628 | val_1_rmse: 0.4721  |  0:01:20s
epoch 110| loss: 0.22356 | val_0_rmse: 0.44837 | val_1_rmse: 0.46258 |  0:01:21s
epoch 111| loss: 0.21918 | val_0_rmse: 0.44364 | val_1_rmse: 0.45909 |  0:01:21s
epoch 112| loss: 0.21843 | val_0_rmse: 0.46867 | val_1_rmse: 0.49186 |  0:01:22s
epoch 113| loss: 0.223   | val_0_rmse: 0.47388 | val_1_rmse: 0.48677 |  0:01:23s
epoch 114| loss: 0.23327 | val_0_rmse: 0.45484 | val_1_rmse: 0.46353 |  0:01:24s
epoch 115| loss: 0.22238 | val_0_rmse: 0.44825 | val_1_rmse: 0.45849 |  0:01:24s
epoch 116| loss: 0.21892 | val_0_rmse: 0.46734 | val_1_rmse: 0.48971 |  0:01:25s
epoch 117| loss: 0.2326  | val_0_rmse: 0.45777 | val_1_rmse: 0.47356 |  0:01:26s
epoch 118| loss: 0.2178  | val_0_rmse: 0.44689 | val_1_rmse: 0.46012 |  0:01:26s
epoch 119| loss: 0.2201  | val_0_rmse: 0.47093 | val_1_rmse: 0.47688 |  0:01:27s
epoch 120| loss: 0.21098 | val_0_rmse: 0.43632 | val_1_rmse: 0.4532  |  0:01:28s
epoch 121| loss: 0.21308 | val_0_rmse: 0.45188 | val_1_rmse: 0.46712 |  0:01:29s
epoch 122| loss: 0.2253  | val_0_rmse: 0.46289 | val_1_rmse: 0.48296 |  0:01:29s
epoch 123| loss: 0.22296 | val_0_rmse: 0.46512 | val_1_rmse: 0.48335 |  0:01:30s
epoch 124| loss: 0.22159 | val_0_rmse: 0.45559 | val_1_rmse: 0.47369 |  0:01:31s
epoch 125| loss: 0.21571 | val_0_rmse: 0.44334 | val_1_rmse: 0.46083 |  0:01:32s
epoch 126| loss: 0.21494 | val_0_rmse: 0.44191 | val_1_rmse: 0.45492 |  0:01:32s
epoch 127| loss: 0.21241 | val_0_rmse: 0.45942 | val_1_rmse: 0.46485 |  0:01:33s
epoch 128| loss: 0.21379 | val_0_rmse: 0.44215 | val_1_rmse: 0.46457 |  0:01:34s
epoch 129| loss: 0.21001 | val_0_rmse: 0.4519  | val_1_rmse: 0.46932 |  0:01:34s
epoch 130| loss: 0.21529 | val_0_rmse: 0.43676 | val_1_rmse: 0.4592  |  0:01:35s
epoch 131| loss: 0.21328 | val_0_rmse: 0.43402 | val_1_rmse: 0.45305 |  0:01:36s
epoch 132| loss: 0.20872 | val_0_rmse: 0.45564 | val_1_rmse: 0.4762  |  0:01:37s
epoch 133| loss: 0.21688 | val_0_rmse: 0.43464 | val_1_rmse: 0.44884 |  0:01:37s
epoch 134| loss: 0.20884 | val_0_rmse: 0.45445 | val_1_rmse: 0.46648 |  0:01:38s
epoch 135| loss: 0.20569 | val_0_rmse: 0.43303 | val_1_rmse: 0.45156 |  0:01:39s
epoch 136| loss: 0.21085 | val_0_rmse: 0.44221 | val_1_rmse: 0.45883 |  0:01:40s
epoch 137| loss: 0.21695 | val_0_rmse: 0.44375 | val_1_rmse: 0.46918 |  0:01:40s
epoch 138| loss: 0.21088 | val_0_rmse: 0.43998 | val_1_rmse: 0.46313 |  0:01:41s
epoch 139| loss: 0.2085  | val_0_rmse: 0.44391 | val_1_rmse: 0.45857 |  0:01:42s
epoch 140| loss: 0.21301 | val_0_rmse: 0.44655 | val_1_rmse: 0.468   |  0:01:42s
epoch 141| loss: 0.2107  | val_0_rmse: 0.44651 | val_1_rmse: 0.46776 |  0:01:43s
epoch 142| loss: 0.21231 | val_0_rmse: 0.43663 | val_1_rmse: 0.45408 |  0:01:44s
epoch 143| loss: 0.21754 | val_0_rmse: 0.43074 | val_1_rmse: 0.44915 |  0:01:45s
epoch 144| loss: 0.20827 | val_0_rmse: 0.44743 | val_1_rmse: 0.4758  |  0:01:45s
epoch 145| loss: 0.21212 | val_0_rmse: 0.44619 | val_1_rmse: 0.47    |  0:01:46s
epoch 146| loss: 0.21255 | val_0_rmse: 0.43765 | val_1_rmse: 0.45882 |  0:01:47s
epoch 147| loss: 0.21064 | val_0_rmse: 0.43928 | val_1_rmse: 0.4535  |  0:01:48s
epoch 148| loss: 0.21366 | val_0_rmse: 0.45098 | val_1_rmse: 0.46317 |  0:01:48s
epoch 149| loss: 0.21342 | val_0_rmse: 0.45752 | val_1_rmse: 0.47915 |  0:01:49s
Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_val_1_rmse = 0.44884
Best weights from best epoch are automatically used!
ended training at: 10:53:58
Feature importance:
[('Area', 0.36665089971976794), ('Baths', 0.08953446378246921), ('Beds', 0.08740043205573983), ('Latitude', 0.21689843681486246), ('Longitude', 0.07465238676451848), ('Month', 0.04178312679222321), ('Year', 0.12308025407041888)]
Mean squared error is of 6514333556.179776
Mean absolute error:58093.244979026844
MAPE:0.1547015855305769
R2 score:0.7995382189312761
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:53:58
epoch 0  | loss: 0.73117 | val_0_rmse: 0.72123 | val_1_rmse: 0.72286 |  0:00:00s
epoch 1  | loss: 0.40348 | val_0_rmse: 0.62923 | val_1_rmse: 0.62307 |  0:00:01s
epoch 2  | loss: 0.35166 | val_0_rmse: 0.57083 | val_1_rmse: 0.56483 |  0:00:02s
epoch 3  | loss: 0.32956 | val_0_rmse: 0.56534 | val_1_rmse: 0.54676 |  0:00:02s
epoch 4  | loss: 0.31504 | val_0_rmse: 0.5403  | val_1_rmse: 0.52565 |  0:00:03s
epoch 5  | loss: 0.29749 | val_0_rmse: 0.52963 | val_1_rmse: 0.51803 |  0:00:04s
epoch 6  | loss: 0.29608 | val_0_rmse: 0.52636 | val_1_rmse: 0.51418 |  0:00:05s
epoch 7  | loss: 0.28721 | val_0_rmse: 0.51706 | val_1_rmse: 0.50564 |  0:00:05s
epoch 8  | loss: 0.27975 | val_0_rmse: 0.51155 | val_1_rmse: 0.50184 |  0:00:06s
epoch 9  | loss: 0.27176 | val_0_rmse: 0.51897 | val_1_rmse: 0.50781 |  0:00:07s
epoch 10 | loss: 0.27585 | val_0_rmse: 0.50996 | val_1_rmse: 0.49523 |  0:00:08s
epoch 11 | loss: 0.27045 | val_0_rmse: 0.5092  | val_1_rmse: 0.50102 |  0:00:08s
epoch 12 | loss: 0.27146 | val_0_rmse: 0.52225 | val_1_rmse: 0.50987 |  0:00:09s
epoch 13 | loss: 0.27011 | val_0_rmse: 0.50002 | val_1_rmse: 0.49159 |  0:00:10s
epoch 14 | loss: 0.26803 | val_0_rmse: 0.50858 | val_1_rmse: 0.49508 |  0:00:11s
epoch 15 | loss: 0.27181 | val_0_rmse: 0.50255 | val_1_rmse: 0.4949  |  0:00:11s
epoch 16 | loss: 0.26716 | val_0_rmse: 0.5129  | val_1_rmse: 0.50649 |  0:00:12s
epoch 17 | loss: 0.26738 | val_0_rmse: 0.49536 | val_1_rmse: 0.48729 |  0:00:13s
epoch 18 | loss: 0.25659 | val_0_rmse: 0.50728 | val_1_rmse: 0.49838 |  0:00:13s
epoch 19 | loss: 0.26665 | val_0_rmse: 0.5119  | val_1_rmse: 0.50211 |  0:00:14s
epoch 20 | loss: 0.2651  | val_0_rmse: 0.49866 | val_1_rmse: 0.49091 |  0:00:15s
epoch 21 | loss: 0.25301 | val_0_rmse: 0.48023 | val_1_rmse: 0.46836 |  0:00:16s
epoch 22 | loss: 0.2474  | val_0_rmse: 0.49164 | val_1_rmse: 0.47958 |  0:00:16s
epoch 23 | loss: 0.24949 | val_0_rmse: 0.48351 | val_1_rmse: 0.47728 |  0:00:17s
epoch 24 | loss: 0.25248 | val_0_rmse: 0.48914 | val_1_rmse: 0.47972 |  0:00:18s
epoch 25 | loss: 0.25079 | val_0_rmse: 0.47952 | val_1_rmse: 0.47107 |  0:00:19s
epoch 26 | loss: 0.24624 | val_0_rmse: 0.48853 | val_1_rmse: 0.48599 |  0:00:19s
epoch 27 | loss: 0.24761 | val_0_rmse: 0.48362 | val_1_rmse: 0.47612 |  0:00:20s
epoch 28 | loss: 0.24824 | val_0_rmse: 0.49683 | val_1_rmse: 0.48937 |  0:00:21s
epoch 29 | loss: 0.24593 | val_0_rmse: 0.48723 | val_1_rmse: 0.47513 |  0:00:22s
epoch 30 | loss: 0.24057 | val_0_rmse: 0.4996  | val_1_rmse: 0.48581 |  0:00:22s
epoch 31 | loss: 0.25025 | val_0_rmse: 0.47594 | val_1_rmse: 0.46943 |  0:00:23s
epoch 32 | loss: 0.23754 | val_0_rmse: 0.46973 | val_1_rmse: 0.46215 |  0:00:24s
epoch 33 | loss: 0.2357  | val_0_rmse: 0.48198 | val_1_rmse: 0.47739 |  0:00:24s
epoch 34 | loss: 0.24387 | val_0_rmse: 0.48155 | val_1_rmse: 0.47021 |  0:00:25s
epoch 35 | loss: 0.24128 | val_0_rmse: 0.46718 | val_1_rmse: 0.45644 |  0:00:26s
epoch 36 | loss: 0.23517 | val_0_rmse: 0.46654 | val_1_rmse: 0.45753 |  0:00:27s
epoch 37 | loss: 0.23017 | val_0_rmse: 0.47081 | val_1_rmse: 0.46379 |  0:00:27s
epoch 38 | loss: 0.23354 | val_0_rmse: 0.46676 | val_1_rmse: 0.45637 |  0:00:28s
epoch 39 | loss: 0.23758 | val_0_rmse: 0.46395 | val_1_rmse: 0.45733 |  0:00:29s
epoch 40 | loss: 0.22687 | val_0_rmse: 0.45227 | val_1_rmse: 0.44897 |  0:00:30s
epoch 41 | loss: 0.22795 | val_0_rmse: 0.46061 | val_1_rmse: 0.45626 |  0:00:30s
epoch 42 | loss: 0.22828 | val_0_rmse: 0.46239 | val_1_rmse: 0.46331 |  0:00:31s
epoch 43 | loss: 0.22886 | val_0_rmse: 0.45577 | val_1_rmse: 0.44635 |  0:00:32s
epoch 44 | loss: 0.22052 | val_0_rmse: 0.4672  | val_1_rmse: 0.46016 |  0:00:33s
epoch 45 | loss: 0.22111 | val_0_rmse: 0.45969 | val_1_rmse: 0.45658 |  0:00:33s
epoch 46 | loss: 0.23025 | val_0_rmse: 0.4783  | val_1_rmse: 0.47022 |  0:00:34s
epoch 47 | loss: 0.22692 | val_0_rmse: 0.45513 | val_1_rmse: 0.45237 |  0:00:35s
epoch 48 | loss: 0.21922 | val_0_rmse: 0.44736 | val_1_rmse: 0.44237 |  0:00:35s
epoch 49 | loss: 0.21874 | val_0_rmse: 0.45048 | val_1_rmse: 0.44803 |  0:00:36s
epoch 50 | loss: 0.22273 | val_0_rmse: 0.453   | val_1_rmse: 0.4469  |  0:00:37s
epoch 51 | loss: 0.2224  | val_0_rmse: 0.4498  | val_1_rmse: 0.44782 |  0:00:38s
epoch 52 | loss: 0.21991 | val_0_rmse: 0.44068 | val_1_rmse: 0.43677 |  0:00:38s
epoch 53 | loss: 0.21963 | val_0_rmse: 0.45499 | val_1_rmse: 0.44877 |  0:00:39s
epoch 54 | loss: 0.21947 | val_0_rmse: 0.44037 | val_1_rmse: 0.44058 |  0:00:40s
epoch 55 | loss: 0.2136  | val_0_rmse: 0.44393 | val_1_rmse: 0.43788 |  0:00:40s
epoch 56 | loss: 0.21071 | val_0_rmse: 0.45298 | val_1_rmse: 0.4521  |  0:00:41s
epoch 57 | loss: 0.22765 | val_0_rmse: 0.46123 | val_1_rmse: 0.45709 |  0:00:42s
epoch 58 | loss: 0.22706 | val_0_rmse: 0.47564 | val_1_rmse: 0.47083 |  0:00:43s
epoch 59 | loss: 0.22519 | val_0_rmse: 0.45262 | val_1_rmse: 0.44744 |  0:00:43s
epoch 60 | loss: 0.22778 | val_0_rmse: 0.46516 | val_1_rmse: 0.45477 |  0:00:44s
epoch 61 | loss: 0.22823 | val_0_rmse: 0.50149 | val_1_rmse: 0.48598 |  0:00:45s
epoch 62 | loss: 0.22667 | val_0_rmse: 0.471   | val_1_rmse: 0.46146 |  0:00:46s
epoch 63 | loss: 0.23198 | val_0_rmse: 0.46712 | val_1_rmse: 0.46145 |  0:00:46s
epoch 64 | loss: 0.22708 | val_0_rmse: 0.46025 | val_1_rmse: 0.4508  |  0:00:47s
epoch 65 | loss: 0.21891 | val_0_rmse: 0.47678 | val_1_rmse: 0.46446 |  0:00:48s
epoch 66 | loss: 0.23148 | val_0_rmse: 0.47643 | val_1_rmse: 0.47177 |  0:00:49s
epoch 67 | loss: 0.22839 | val_0_rmse: 0.4643  | val_1_rmse: 0.46344 |  0:00:49s
epoch 68 | loss: 0.22507 | val_0_rmse: 0.46614 | val_1_rmse: 0.45716 |  0:00:50s
epoch 69 | loss: 0.22684 | val_0_rmse: 0.45207 | val_1_rmse: 0.44647 |  0:00:51s
epoch 70 | loss: 0.22108 | val_0_rmse: 0.47455 | val_1_rmse: 0.46798 |  0:00:51s
epoch 71 | loss: 0.23183 | val_0_rmse: 0.45558 | val_1_rmse: 0.44914 |  0:00:52s
epoch 72 | loss: 0.21505 | val_0_rmse: 0.44412 | val_1_rmse: 0.44073 |  0:00:53s
epoch 73 | loss: 0.21096 | val_0_rmse: 0.45331 | val_1_rmse: 0.44863 |  0:00:54s
epoch 74 | loss: 0.22137 | val_0_rmse: 0.4442  | val_1_rmse: 0.44041 |  0:00:54s
epoch 75 | loss: 0.21669 | val_0_rmse: 0.44101 | val_1_rmse: 0.43557 |  0:00:55s
epoch 76 | loss: 0.2089  | val_0_rmse: 0.44388 | val_1_rmse: 0.4421  |  0:00:56s
epoch 77 | loss: 0.21034 | val_0_rmse: 0.46477 | val_1_rmse: 0.46415 |  0:00:57s
epoch 78 | loss: 0.22545 | val_0_rmse: 0.47146 | val_1_rmse: 0.47212 |  0:00:57s
epoch 79 | loss: 0.23034 | val_0_rmse: 0.45562 | val_1_rmse: 0.45572 |  0:00:58s
epoch 80 | loss: 0.22087 | val_0_rmse: 0.47115 | val_1_rmse: 0.46378 |  0:00:59s
epoch 81 | loss: 0.23827 | val_0_rmse: 0.4766  | val_1_rmse: 0.47021 |  0:00:59s
epoch 82 | loss: 0.22637 | val_0_rmse: 0.47    | val_1_rmse: 0.46979 |  0:01:00s
epoch 83 | loss: 0.22203 | val_0_rmse: 0.44843 | val_1_rmse: 0.44556 |  0:01:01s
epoch 84 | loss: 0.21921 | val_0_rmse: 0.45606 | val_1_rmse: 0.45422 |  0:01:02s
epoch 85 | loss: 0.21777 | val_0_rmse: 0.44843 | val_1_rmse: 0.44461 |  0:01:02s
epoch 86 | loss: 0.21902 | val_0_rmse: 0.45233 | val_1_rmse: 0.45017 |  0:01:03s
epoch 87 | loss: 0.22187 | val_0_rmse: 0.46884 | val_1_rmse: 0.46032 |  0:01:04s
epoch 88 | loss: 0.21863 | val_0_rmse: 0.43408 | val_1_rmse: 0.43153 |  0:01:05s
epoch 89 | loss: 0.20731 | val_0_rmse: 0.4421  | val_1_rmse: 0.44404 |  0:01:05s
epoch 90 | loss: 0.20962 | val_0_rmse: 0.43416 | val_1_rmse: 0.43761 |  0:01:06s
epoch 91 | loss: 0.2113  | val_0_rmse: 0.42976 | val_1_rmse: 0.42841 |  0:01:07s
epoch 92 | loss: 0.21564 | val_0_rmse: 0.44383 | val_1_rmse: 0.4457  |  0:01:08s
epoch 93 | loss: 0.20935 | val_0_rmse: 0.438   | val_1_rmse: 0.436   |  0:01:08s
epoch 94 | loss: 0.21112 | val_0_rmse: 0.43954 | val_1_rmse: 0.4386  |  0:01:09s
epoch 95 | loss: 0.2061  | val_0_rmse: 0.47087 | val_1_rmse: 0.47191 |  0:01:10s
epoch 96 | loss: 0.21704 | val_0_rmse: 0.44463 | val_1_rmse: 0.44454 |  0:01:11s
epoch 97 | loss: 0.21773 | val_0_rmse: 0.43087 | val_1_rmse: 0.43259 |  0:01:11s
epoch 98 | loss: 0.20628 | val_0_rmse: 0.43443 | val_1_rmse: 0.43275 |  0:01:12s
epoch 99 | loss: 0.20412 | val_0_rmse: 0.42679 | val_1_rmse: 0.42962 |  0:01:13s
epoch 100| loss: 0.20207 | val_0_rmse: 0.43609 | val_1_rmse: 0.43619 |  0:01:13s
epoch 101| loss: 0.20446 | val_0_rmse: 0.43324 | val_1_rmse: 0.43572 |  0:01:14s
epoch 102| loss: 0.20395 | val_0_rmse: 0.44359 | val_1_rmse: 0.44948 |  0:01:15s
epoch 103| loss: 0.20599 | val_0_rmse: 0.42902 | val_1_rmse: 0.43166 |  0:01:16s
epoch 104| loss: 0.20382 | val_0_rmse: 0.43024 | val_1_rmse: 0.43521 |  0:01:16s
epoch 105| loss: 0.20895 | val_0_rmse: 0.45166 | val_1_rmse: 0.45146 |  0:01:17s
epoch 106| loss: 0.20747 | val_0_rmse: 0.44855 | val_1_rmse: 0.45674 |  0:01:18s
epoch 107| loss: 0.21283 | val_0_rmse: 0.43909 | val_1_rmse: 0.44478 |  0:01:19s
epoch 108| loss: 0.20721 | val_0_rmse: 0.43004 | val_1_rmse: 0.43253 |  0:01:19s
epoch 109| loss: 0.20208 | val_0_rmse: 0.4267  | val_1_rmse: 0.4294  |  0:01:20s
epoch 110| loss: 0.19866 | val_0_rmse: 0.4306  | val_1_rmse: 0.43346 |  0:01:21s
epoch 111| loss: 0.20024 | val_0_rmse: 0.43868 | val_1_rmse: 0.44663 |  0:01:21s
epoch 112| loss: 0.20787 | val_0_rmse: 0.46438 | val_1_rmse: 0.46239 |  0:01:22s
epoch 113| loss: 0.20821 | val_0_rmse: 0.46735 | val_1_rmse: 0.47306 |  0:01:23s
epoch 114| loss: 0.2178  | val_0_rmse: 0.44263 | val_1_rmse: 0.44578 |  0:01:24s
epoch 115| loss: 0.20585 | val_0_rmse: 0.43307 | val_1_rmse: 0.44224 |  0:01:24s
epoch 116| loss: 0.20095 | val_0_rmse: 0.42471 | val_1_rmse: 0.4362  |  0:01:25s
epoch 117| loss: 0.20583 | val_0_rmse: 0.42229 | val_1_rmse: 0.43083 |  0:01:26s
epoch 118| loss: 0.20492 | val_0_rmse: 0.44099 | val_1_rmse: 0.443   |  0:01:27s
epoch 119| loss: 0.20554 | val_0_rmse: 0.42674 | val_1_rmse: 0.43518 |  0:01:27s
epoch 120| loss: 0.20296 | val_0_rmse: 0.43115 | val_1_rmse: 0.43489 |  0:01:28s
epoch 121| loss: 0.19723 | val_0_rmse: 0.43074 | val_1_rmse: 0.44218 |  0:01:29s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.42841
Best weights from best epoch are automatically used!
ended training at: 10:55:27
Feature importance:
[('Area', 0.40345772795708734), ('Baths', 0.0643093542469873), ('Beds', 0.016756621458230744), ('Latitude', 0.30685244170589543), ('Longitude', 0.11713979778050934), ('Month', 0.0), ('Year', 0.09148405685128982)]
Mean squared error is of 6227667456.231134
Mean absolute error:56742.76453863255
MAPE:0.15220192606852223
R2 score:0.7919032598400167
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:55:27
epoch 0  | loss: 1.10179 | val_0_rmse: 1.57821 | val_1_rmse: 1.57932 |  0:00:00s
epoch 1  | loss: 0.59438 | val_0_rmse: 0.99837 | val_1_rmse: 1.00648 |  0:00:00s
epoch 2  | loss: 0.46547 | val_0_rmse: 1.02856 | val_1_rmse: 1.13111 |  0:00:01s
epoch 3  | loss: 0.41026 | val_0_rmse: 0.75605 | val_1_rmse: 0.78445 |  0:00:01s
epoch 4  | loss: 0.3893  | val_0_rmse: 0.64812 | val_1_rmse: 0.68172 |  0:00:01s
epoch 5  | loss: 0.37092 | val_0_rmse: 0.70459 | val_1_rmse: 0.74716 |  0:00:02s
epoch 6  | loss: 0.35649 | val_0_rmse: 0.59163 | val_1_rmse: 0.62253 |  0:00:02s
epoch 7  | loss: 0.3462  | val_0_rmse: 0.56218 | val_1_rmse: 0.59757 |  0:00:02s
epoch 8  | loss: 0.33246 | val_0_rmse: 0.55548 | val_1_rmse: 0.60111 |  0:00:03s
epoch 9  | loss: 0.32539 | val_0_rmse: 0.55246 | val_1_rmse: 0.58798 |  0:00:03s
epoch 10 | loss: 0.31949 | val_0_rmse: 0.55284 | val_1_rmse: 0.59343 |  0:00:03s
epoch 11 | loss: 0.31688 | val_0_rmse: 0.56175 | val_1_rmse: 0.61031 |  0:00:04s
epoch 12 | loss: 0.30986 | val_0_rmse: 0.54085 | val_1_rmse: 0.58777 |  0:00:04s
epoch 13 | loss: 0.29699 | val_0_rmse: 0.52715 | val_1_rmse: 0.5676  |  0:00:05s
epoch 14 | loss: 0.3054  | val_0_rmse: 0.53475 | val_1_rmse: 0.58814 |  0:00:05s
epoch 15 | loss: 0.28867 | val_0_rmse: 0.53528 | val_1_rmse: 0.58943 |  0:00:05s
epoch 16 | loss: 0.29214 | val_0_rmse: 0.51063 | val_1_rmse: 0.56175 |  0:00:06s
epoch 17 | loss: 0.28213 | val_0_rmse: 0.50972 | val_1_rmse: 0.5612  |  0:00:06s
epoch 18 | loss: 0.28484 | val_0_rmse: 0.50753 | val_1_rmse: 0.56219 |  0:00:06s
epoch 19 | loss: 0.27297 | val_0_rmse: 0.50344 | val_1_rmse: 0.55469 |  0:00:07s
epoch 20 | loss: 0.27123 | val_0_rmse: 0.50923 | val_1_rmse: 0.56063 |  0:00:07s
epoch 21 | loss: 0.27556 | val_0_rmse: 0.49344 | val_1_rmse: 0.55263 |  0:00:08s
epoch 22 | loss: 0.27771 | val_0_rmse: 0.50694 | val_1_rmse: 0.56932 |  0:00:08s
epoch 23 | loss: 0.28311 | val_0_rmse: 0.50633 | val_1_rmse: 0.56349 |  0:00:08s
epoch 24 | loss: 0.28239 | val_0_rmse: 0.52796 | val_1_rmse: 0.56804 |  0:00:09s
epoch 25 | loss: 0.29648 | val_0_rmse: 0.49759 | val_1_rmse: 0.55    |  0:00:09s
epoch 26 | loss: 0.28468 | val_0_rmse: 0.51672 | val_1_rmse: 0.57443 |  0:00:09s
epoch 27 | loss: 0.2742  | val_0_rmse: 0.50289 | val_1_rmse: 0.55344 |  0:00:10s
epoch 28 | loss: 0.26346 | val_0_rmse: 0.49753 | val_1_rmse: 0.55855 |  0:00:10s
epoch 29 | loss: 0.26542 | val_0_rmse: 0.4995  | val_1_rmse: 0.54981 |  0:00:10s
epoch 30 | loss: 0.26737 | val_0_rmse: 0.49295 | val_1_rmse: 0.55799 |  0:00:11s
epoch 31 | loss: 0.26644 | val_0_rmse: 0.5005  | val_1_rmse: 0.56055 |  0:00:11s
epoch 32 | loss: 0.27571 | val_0_rmse: 0.49871 | val_1_rmse: 0.55768 |  0:00:11s
epoch 33 | loss: 0.26372 | val_0_rmse: 0.49449 | val_1_rmse: 0.54833 |  0:00:12s
epoch 34 | loss: 0.26476 | val_0_rmse: 0.4948  | val_1_rmse: 0.54661 |  0:00:12s
epoch 35 | loss: 0.26384 | val_0_rmse: 0.49838 | val_1_rmse: 0.55017 |  0:00:13s
epoch 36 | loss: 0.2564  | val_0_rmse: 0.48792 | val_1_rmse: 0.55048 |  0:00:13s
epoch 37 | loss: 0.26272 | val_0_rmse: 0.4949  | val_1_rmse: 0.55923 |  0:00:13s
epoch 38 | loss: 0.25832 | val_0_rmse: 0.48459 | val_1_rmse: 0.54916 |  0:00:14s
epoch 39 | loss: 0.2585  | val_0_rmse: 0.4837  | val_1_rmse: 0.54191 |  0:00:14s
epoch 40 | loss: 0.24881 | val_0_rmse: 0.48334 | val_1_rmse: 0.54411 |  0:00:14s
epoch 41 | loss: 0.25709 | val_0_rmse: 0.48712 | val_1_rmse: 0.55049 |  0:00:15s
epoch 42 | loss: 0.25732 | val_0_rmse: 0.48013 | val_1_rmse: 0.54061 |  0:00:15s
epoch 43 | loss: 0.25544 | val_0_rmse: 0.48607 | val_1_rmse: 0.54821 |  0:00:15s
epoch 44 | loss: 0.25876 | val_0_rmse: 0.4787  | val_1_rmse: 0.54536 |  0:00:16s
epoch 45 | loss: 0.25691 | val_0_rmse: 0.50871 | val_1_rmse: 0.57549 |  0:00:16s
epoch 46 | loss: 0.26314 | val_0_rmse: 0.48651 | val_1_rmse: 0.55004 |  0:00:17s
epoch 47 | loss: 0.25613 | val_0_rmse: 0.47992 | val_1_rmse: 0.54281 |  0:00:17s
epoch 48 | loss: 0.26175 | val_0_rmse: 0.51479 | val_1_rmse: 0.58172 |  0:00:17s
epoch 49 | loss: 0.27401 | val_0_rmse: 0.48942 | val_1_rmse: 0.55863 |  0:00:18s
epoch 50 | loss: 0.25551 | val_0_rmse: 0.48469 | val_1_rmse: 0.54427 |  0:00:18s
epoch 51 | loss: 0.25729 | val_0_rmse: 0.48068 | val_1_rmse: 0.54709 |  0:00:18s
epoch 52 | loss: 0.24431 | val_0_rmse: 0.47925 | val_1_rmse: 0.5431  |  0:00:19s
epoch 53 | loss: 0.26315 | val_0_rmse: 0.48255 | val_1_rmse: 0.54355 |  0:00:19s
epoch 54 | loss: 0.26599 | val_0_rmse: 0.49144 | val_1_rmse: 0.55812 |  0:00:19s
epoch 55 | loss: 0.26808 | val_0_rmse: 0.49199 | val_1_rmse: 0.55466 |  0:00:20s
epoch 56 | loss: 0.25518 | val_0_rmse: 0.4908  | val_1_rmse: 0.55242 |  0:00:20s
epoch 57 | loss: 0.25395 | val_0_rmse: 0.48139 | val_1_rmse: 0.54345 |  0:00:20s
epoch 58 | loss: 0.25226 | val_0_rmse: 0.49257 | val_1_rmse: 0.55821 |  0:00:21s
epoch 59 | loss: 0.25773 | val_0_rmse: 0.50236 | val_1_rmse: 0.55388 |  0:00:21s
epoch 60 | loss: 0.271   | val_0_rmse: 0.49659 | val_1_rmse: 0.56549 |  0:00:22s
epoch 61 | loss: 0.25594 | val_0_rmse: 0.48145 | val_1_rmse: 0.54997 |  0:00:22s
epoch 62 | loss: 0.25149 | val_0_rmse: 0.47844 | val_1_rmse: 0.54447 |  0:00:22s
epoch 63 | loss: 0.24443 | val_0_rmse: 0.47723 | val_1_rmse: 0.55055 |  0:00:23s
epoch 64 | loss: 0.25095 | val_0_rmse: 0.47971 | val_1_rmse: 0.54635 |  0:00:23s
epoch 65 | loss: 0.25201 | val_0_rmse: 0.47133 | val_1_rmse: 0.54597 |  0:00:23s
epoch 66 | loss: 0.24588 | val_0_rmse: 0.47519 | val_1_rmse: 0.55177 |  0:00:24s
epoch 67 | loss: 0.24149 | val_0_rmse: 0.48988 | val_1_rmse: 0.55033 |  0:00:24s
epoch 68 | loss: 0.24814 | val_0_rmse: 0.47861 | val_1_rmse: 0.54682 |  0:00:24s
epoch 69 | loss: 0.23733 | val_0_rmse: 0.46983 | val_1_rmse: 0.54647 |  0:00:25s
epoch 70 | loss: 0.23785 | val_0_rmse: 0.47079 | val_1_rmse: 0.53566 |  0:00:25s
epoch 71 | loss: 0.2404  | val_0_rmse: 0.47154 | val_1_rmse: 0.54473 |  0:00:26s
epoch 72 | loss: 0.24442 | val_0_rmse: 0.48549 | val_1_rmse: 0.5526  |  0:00:26s
epoch 73 | loss: 0.24123 | val_0_rmse: 0.48609 | val_1_rmse: 0.56028 |  0:00:26s
epoch 74 | loss: 0.24826 | val_0_rmse: 0.47315 | val_1_rmse: 0.53808 |  0:00:27s
epoch 75 | loss: 0.25303 | val_0_rmse: 0.47934 | val_1_rmse: 0.55292 |  0:00:27s
epoch 76 | loss: 0.24883 | val_0_rmse: 0.47956 | val_1_rmse: 0.54631 |  0:00:27s
epoch 77 | loss: 0.24524 | val_0_rmse: 0.48158 | val_1_rmse: 0.5585  |  0:00:28s
epoch 78 | loss: 0.24655 | val_0_rmse: 0.46919 | val_1_rmse: 0.53572 |  0:00:28s
epoch 79 | loss: 0.23784 | val_0_rmse: 0.48678 | val_1_rmse: 0.5642  |  0:00:28s
epoch 80 | loss: 0.24358 | val_0_rmse: 0.4668  | val_1_rmse: 0.53844 |  0:00:29s
epoch 81 | loss: 0.24032 | val_0_rmse: 0.48028 | val_1_rmse: 0.54297 |  0:00:29s
epoch 82 | loss: 0.24128 | val_0_rmse: 0.49193 | val_1_rmse: 0.56612 |  0:00:30s
epoch 83 | loss: 0.24336 | val_0_rmse: 0.46763 | val_1_rmse: 0.53945 |  0:00:30s
epoch 84 | loss: 0.23691 | val_0_rmse: 0.50773 | val_1_rmse: 0.56655 |  0:00:30s
epoch 85 | loss: 0.24679 | val_0_rmse: 0.47776 | val_1_rmse: 0.55575 |  0:00:31s
epoch 86 | loss: 0.23516 | val_0_rmse: 0.45891 | val_1_rmse: 0.52812 |  0:00:31s
epoch 87 | loss: 0.23812 | val_0_rmse: 0.46795 | val_1_rmse: 0.53603 |  0:00:31s
epoch 88 | loss: 0.23684 | val_0_rmse: 0.46592 | val_1_rmse: 0.53671 |  0:00:32s
epoch 89 | loss: 0.23608 | val_0_rmse: 0.47968 | val_1_rmse: 0.55096 |  0:00:32s
epoch 90 | loss: 0.24105 | val_0_rmse: 0.48452 | val_1_rmse: 0.5612  |  0:00:32s
epoch 91 | loss: 0.23665 | val_0_rmse: 0.47849 | val_1_rmse: 0.55986 |  0:00:33s
epoch 92 | loss: 0.23903 | val_0_rmse: 0.46874 | val_1_rmse: 0.5337  |  0:00:33s
epoch 93 | loss: 0.24016 | val_0_rmse: 0.4606  | val_1_rmse: 0.53075 |  0:00:34s
epoch 94 | loss: 0.23841 | val_0_rmse: 0.46421 | val_1_rmse: 0.54668 |  0:00:34s
epoch 95 | loss: 0.23216 | val_0_rmse: 0.45974 | val_1_rmse: 0.53802 |  0:00:34s
epoch 96 | loss: 0.23293 | val_0_rmse: 0.46097 | val_1_rmse: 0.53587 |  0:00:35s
epoch 97 | loss: 0.23268 | val_0_rmse: 0.45646 | val_1_rmse: 0.53485 |  0:00:35s
epoch 98 | loss: 0.2296  | val_0_rmse: 0.45728 | val_1_rmse: 0.53778 |  0:00:35s
epoch 99 | loss: 0.2346  | val_0_rmse: 0.46722 | val_1_rmse: 0.54957 |  0:00:36s
epoch 100| loss: 0.23634 | val_0_rmse: 0.45724 | val_1_rmse: 0.54688 |  0:00:36s
epoch 101| loss: 0.22452 | val_0_rmse: 0.46555 | val_1_rmse: 0.54983 |  0:00:36s
epoch 102| loss: 0.22978 | val_0_rmse: 0.4587  | val_1_rmse: 0.53607 |  0:00:37s
epoch 103| loss: 0.23062 | val_0_rmse: 0.46862 | val_1_rmse: 0.54151 |  0:00:37s
epoch 104| loss: 0.23622 | val_0_rmse: 0.47254 | val_1_rmse: 0.55653 |  0:00:37s
epoch 105| loss: 0.23498 | val_0_rmse: 0.47319 | val_1_rmse: 0.55611 |  0:00:38s
epoch 106| loss: 0.23227 | val_0_rmse: 0.47647 | val_1_rmse: 0.53693 |  0:00:38s
epoch 107| loss: 0.2382  | val_0_rmse: 0.46778 | val_1_rmse: 0.54429 |  0:00:39s
epoch 108| loss: 0.24705 | val_0_rmse: 0.46045 | val_1_rmse: 0.53298 |  0:00:39s
epoch 109| loss: 0.2265  | val_0_rmse: 0.46485 | val_1_rmse: 0.53826 |  0:00:39s
epoch 110| loss: 0.233   | val_0_rmse: 0.46471 | val_1_rmse: 0.54397 |  0:00:40s
epoch 111| loss: 0.23874 | val_0_rmse: 0.45916 | val_1_rmse: 0.54181 |  0:00:40s
epoch 112| loss: 0.23243 | val_0_rmse: 0.45263 | val_1_rmse: 0.53832 |  0:00:40s
epoch 113| loss: 0.23054 | val_0_rmse: 0.46065 | val_1_rmse: 0.54671 |  0:00:41s
epoch 114| loss: 0.23083 | val_0_rmse: 0.45194 | val_1_rmse: 0.53155 |  0:00:41s
epoch 115| loss: 0.22839 | val_0_rmse: 0.45467 | val_1_rmse: 0.54659 |  0:00:41s
epoch 116| loss: 0.22337 | val_0_rmse: 0.45698 | val_1_rmse: 0.54186 |  0:00:42s

Early stopping occured at epoch 116 with best_epoch = 86 and best_val_1_rmse = 0.52812
Best weights from best epoch are automatically used!
ended training at: 10:56:10
Feature importance:
[('Area', 0.29104336071758646), ('Baths', 0.0038362556800147695), ('Beds', 0.1265625846549099), ('Latitude', 0.2273442391884824), ('Longitude', 0.20755535714187237), ('Month', 0.006443115828758441), ('Year', 0.13721508678837563)]
Mean squared error is of 22600969313.134296
Mean absolute error:108056.99935971443
MAPE:0.19417312880911095
R2 score:0.716470611987787
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:56:10
epoch 0  | loss: 1.19056 | val_0_rmse: 0.95174 | val_1_rmse: 0.96825 |  0:00:00s
epoch 1  | loss: 0.67196 | val_0_rmse: 0.90821 | val_1_rmse: 0.9152  |  0:00:00s
epoch 2  | loss: 0.52315 | val_0_rmse: 0.81572 | val_1_rmse: 0.81214 |  0:00:01s
epoch 3  | loss: 0.43596 | val_0_rmse: 0.80502 | val_1_rmse: 0.7935  |  0:00:01s
epoch 4  | loss: 0.40075 | val_0_rmse: 0.68861 | val_1_rmse: 0.66847 |  0:00:01s
epoch 5  | loss: 0.38631 | val_0_rmse: 0.6527  | val_1_rmse: 0.65119 |  0:00:02s
epoch 6  | loss: 0.37191 | val_0_rmse: 0.61272 | val_1_rmse: 0.60691 |  0:00:02s
epoch 7  | loss: 0.36386 | val_0_rmse: 0.61478 | val_1_rmse: 0.61068 |  0:00:02s
epoch 8  | loss: 0.36285 | val_0_rmse: 0.59654 | val_1_rmse: 0.59491 |  0:00:03s
epoch 9  | loss: 0.38536 | val_0_rmse: 0.61249 | val_1_rmse: 0.60583 |  0:00:03s
epoch 10 | loss: 0.37237 | val_0_rmse: 0.63123 | val_1_rmse: 0.62517 |  0:00:03s
epoch 11 | loss: 0.37645 | val_0_rmse: 0.57407 | val_1_rmse: 0.5748  |  0:00:04s
epoch 12 | loss: 0.35897 | val_0_rmse: 0.56262 | val_1_rmse: 0.55785 |  0:00:04s
epoch 13 | loss: 0.35022 | val_0_rmse: 0.59219 | val_1_rmse: 0.59241 |  0:00:05s
epoch 14 | loss: 0.33363 | val_0_rmse: 0.55924 | val_1_rmse: 0.55823 |  0:00:05s
epoch 15 | loss: 0.32175 | val_0_rmse: 0.5524  | val_1_rmse: 0.55237 |  0:00:05s
epoch 16 | loss: 0.32301 | val_0_rmse: 0.55485 | val_1_rmse: 0.55449 |  0:00:06s
epoch 17 | loss: 0.31833 | val_0_rmse: 0.54971 | val_1_rmse: 0.5537  |  0:00:06s
epoch 18 | loss: 0.32383 | val_0_rmse: 0.55352 | val_1_rmse: 0.56043 |  0:00:06s
epoch 19 | loss: 0.31979 | val_0_rmse: 0.55161 | val_1_rmse: 0.55857 |  0:00:07s
epoch 20 | loss: 0.32355 | val_0_rmse: 0.54081 | val_1_rmse: 0.54852 |  0:00:07s
epoch 21 | loss: 0.31241 | val_0_rmse: 0.54327 | val_1_rmse: 0.54587 |  0:00:07s
epoch 22 | loss: 0.31849 | val_0_rmse: 0.53446 | val_1_rmse: 0.54294 |  0:00:08s
epoch 23 | loss: 0.30786 | val_0_rmse: 0.54442 | val_1_rmse: 0.55295 |  0:00:08s
epoch 24 | loss: 0.30395 | val_0_rmse: 0.53767 | val_1_rmse: 0.54563 |  0:00:09s
epoch 25 | loss: 0.29862 | val_0_rmse: 0.53595 | val_1_rmse: 0.54664 |  0:00:09s
epoch 26 | loss: 0.30724 | val_0_rmse: 0.53415 | val_1_rmse: 0.54032 |  0:00:09s
epoch 27 | loss: 0.30284 | val_0_rmse: 0.5385  | val_1_rmse: 0.54495 |  0:00:10s
epoch 28 | loss: 0.29947 | val_0_rmse: 0.52871 | val_1_rmse: 0.53297 |  0:00:10s
epoch 29 | loss: 0.30436 | val_0_rmse: 0.531   | val_1_rmse: 0.5318  |  0:00:10s
epoch 30 | loss: 0.30274 | val_0_rmse: 0.5309  | val_1_rmse: 0.53567 |  0:00:11s
epoch 31 | loss: 0.29129 | val_0_rmse: 0.5256  | val_1_rmse: 0.53148 |  0:00:11s
epoch 32 | loss: 0.29439 | val_0_rmse: 0.52051 | val_1_rmse: 0.52277 |  0:00:11s
epoch 33 | loss: 0.29602 | val_0_rmse: 0.54204 | val_1_rmse: 0.54843 |  0:00:12s
epoch 34 | loss: 0.29969 | val_0_rmse: 0.52432 | val_1_rmse: 0.53481 |  0:00:12s
epoch 35 | loss: 0.29907 | val_0_rmse: 0.53837 | val_1_rmse: 0.54618 |  0:00:13s
epoch 36 | loss: 0.2982  | val_0_rmse: 0.53074 | val_1_rmse: 0.53161 |  0:00:13s
epoch 37 | loss: 0.29975 | val_0_rmse: 0.54966 | val_1_rmse: 0.56109 |  0:00:13s
epoch 38 | loss: 0.31305 | val_0_rmse: 0.54388 | val_1_rmse: 0.55081 |  0:00:14s
epoch 39 | loss: 0.30423 | val_0_rmse: 0.52576 | val_1_rmse: 0.53351 |  0:00:14s
epoch 40 | loss: 0.29789 | val_0_rmse: 0.52806 | val_1_rmse: 0.53017 |  0:00:14s
epoch 41 | loss: 0.3065  | val_0_rmse: 0.53132 | val_1_rmse: 0.53513 |  0:00:15s
epoch 42 | loss: 0.30068 | val_0_rmse: 0.52469 | val_1_rmse: 0.53494 |  0:00:15s
epoch 43 | loss: 0.29248 | val_0_rmse: 0.51996 | val_1_rmse: 0.52199 |  0:00:15s
epoch 44 | loss: 0.28887 | val_0_rmse: 0.51549 | val_1_rmse: 0.52633 |  0:00:16s
epoch 45 | loss: 0.28948 | val_0_rmse: 0.5234  | val_1_rmse: 0.5273  |  0:00:16s
epoch 46 | loss: 0.28168 | val_0_rmse: 0.51118 | val_1_rmse: 0.52003 |  0:00:17s
epoch 47 | loss: 0.28712 | val_0_rmse: 0.51612 | val_1_rmse: 0.52215 |  0:00:17s
epoch 48 | loss: 0.27809 | val_0_rmse: 0.50665 | val_1_rmse: 0.51851 |  0:00:17s
epoch 49 | loss: 0.28034 | val_0_rmse: 0.51827 | val_1_rmse: 0.52605 |  0:00:18s
epoch 50 | loss: 0.28938 | val_0_rmse: 0.50979 | val_1_rmse: 0.52115 |  0:00:18s
epoch 51 | loss: 0.28005 | val_0_rmse: 0.51209 | val_1_rmse: 0.52586 |  0:00:18s
epoch 52 | loss: 0.27793 | val_0_rmse: 0.51365 | val_1_rmse: 0.526   |  0:00:19s
epoch 53 | loss: 0.28936 | val_0_rmse: 0.50783 | val_1_rmse: 0.53072 |  0:00:19s
epoch 54 | loss: 0.28181 | val_0_rmse: 0.51027 | val_1_rmse: 0.53043 |  0:00:19s
epoch 55 | loss: 0.27547 | val_0_rmse: 0.51393 | val_1_rmse: 0.52594 |  0:00:20s
epoch 56 | loss: 0.28401 | val_0_rmse: 0.50893 | val_1_rmse: 0.52838 |  0:00:20s
epoch 57 | loss: 0.2753  | val_0_rmse: 0.50407 | val_1_rmse: 0.5235  |  0:00:20s
epoch 58 | loss: 0.28665 | val_0_rmse: 0.51907 | val_1_rmse: 0.5307  |  0:00:21s
epoch 59 | loss: 0.27739 | val_0_rmse: 0.51184 | val_1_rmse: 0.52973 |  0:00:21s
epoch 60 | loss: 0.272   | val_0_rmse: 0.50784 | val_1_rmse: 0.51937 |  0:00:22s
epoch 61 | loss: 0.2758  | val_0_rmse: 0.50837 | val_1_rmse: 0.53499 |  0:00:22s
epoch 62 | loss: 0.27465 | val_0_rmse: 0.50975 | val_1_rmse: 0.52649 |  0:00:22s
epoch 63 | loss: 0.27893 | val_0_rmse: 0.50852 | val_1_rmse: 0.52372 |  0:00:23s
epoch 64 | loss: 0.29602 | val_0_rmse: 0.51765 | val_1_rmse: 0.52623 |  0:00:23s
epoch 65 | loss: 0.2832  | val_0_rmse: 0.51934 | val_1_rmse: 0.53942 |  0:00:23s
epoch 66 | loss: 0.28125 | val_0_rmse: 0.50866 | val_1_rmse: 0.52727 |  0:00:24s
epoch 67 | loss: 0.27674 | val_0_rmse: 0.53786 | val_1_rmse: 0.5586  |  0:00:24s
epoch 68 | loss: 0.28569 | val_0_rmse: 0.51065 | val_1_rmse: 0.51747 |  0:00:24s
epoch 69 | loss: 0.28408 | val_0_rmse: 0.52171 | val_1_rmse: 0.53123 |  0:00:25s
epoch 70 | loss: 0.27869 | val_0_rmse: 0.50835 | val_1_rmse: 0.52911 |  0:00:25s
epoch 71 | loss: 0.27754 | val_0_rmse: 0.49544 | val_1_rmse: 0.51721 |  0:00:26s
epoch 72 | loss: 0.26226 | val_0_rmse: 0.49313 | val_1_rmse: 0.52128 |  0:00:26s
epoch 73 | loss: 0.2661  | val_0_rmse: 0.50182 | val_1_rmse: 0.52298 |  0:00:26s
epoch 74 | loss: 0.26414 | val_0_rmse: 0.49346 | val_1_rmse: 0.51943 |  0:00:27s
epoch 75 | loss: 0.27374 | val_0_rmse: 0.49583 | val_1_rmse: 0.52519 |  0:00:27s
epoch 76 | loss: 0.26934 | val_0_rmse: 0.49622 | val_1_rmse: 0.51807 |  0:00:27s
epoch 77 | loss: 0.26808 | val_0_rmse: 0.50266 | val_1_rmse: 0.52159 |  0:00:28s
epoch 78 | loss: 0.2672  | val_0_rmse: 0.50994 | val_1_rmse: 0.52847 |  0:00:28s
epoch 79 | loss: 0.26415 | val_0_rmse: 0.49779 | val_1_rmse: 0.52809 |  0:00:28s
epoch 80 | loss: 0.26765 | val_0_rmse: 0.49039 | val_1_rmse: 0.52018 |  0:00:29s
epoch 81 | loss: 0.26278 | val_0_rmse: 0.48278 | val_1_rmse: 0.51147 |  0:00:29s
epoch 82 | loss: 0.26243 | val_0_rmse: 0.4958  | val_1_rmse: 0.52602 |  0:00:29s
epoch 83 | loss: 0.26885 | val_0_rmse: 0.48577 | val_1_rmse: 0.51758 |  0:00:30s
epoch 84 | loss: 0.258   | val_0_rmse: 0.48508 | val_1_rmse: 0.5134  |  0:00:30s
epoch 85 | loss: 0.25789 | val_0_rmse: 0.48972 | val_1_rmse: 0.52586 |  0:00:31s
epoch 86 | loss: 0.26346 | val_0_rmse: 0.49707 | val_1_rmse: 0.52407 |  0:00:31s
epoch 87 | loss: 0.27174 | val_0_rmse: 0.51581 | val_1_rmse: 0.53685 |  0:00:31s
epoch 88 | loss: 0.27168 | val_0_rmse: 0.50591 | val_1_rmse: 0.52634 |  0:00:32s
epoch 89 | loss: 0.27289 | val_0_rmse: 0.50387 | val_1_rmse: 0.52474 |  0:00:32s
epoch 90 | loss: 0.26283 | val_0_rmse: 0.49434 | val_1_rmse: 0.52083 |  0:00:32s
epoch 91 | loss: 0.27186 | val_0_rmse: 0.48983 | val_1_rmse: 0.5212  |  0:00:33s
epoch 92 | loss: 0.26854 | val_0_rmse: 0.48574 | val_1_rmse: 0.51938 |  0:00:33s
epoch 93 | loss: 0.26434 | val_0_rmse: 0.49468 | val_1_rmse: 0.52432 |  0:00:33s
epoch 94 | loss: 0.25482 | val_0_rmse: 0.48632 | val_1_rmse: 0.52374 |  0:00:34s
epoch 95 | loss: 0.25789 | val_0_rmse: 0.50176 | val_1_rmse: 0.52716 |  0:00:34s
epoch 96 | loss: 0.25926 | val_0_rmse: 0.48804 | val_1_rmse: 0.51638 |  0:00:34s
epoch 97 | loss: 0.26716 | val_0_rmse: 0.49246 | val_1_rmse: 0.52867 |  0:00:35s
epoch 98 | loss: 0.25807 | val_0_rmse: 0.48099 | val_1_rmse: 0.51166 |  0:00:35s
epoch 99 | loss: 0.2541  | val_0_rmse: 0.48424 | val_1_rmse: 0.51227 |  0:00:36s
epoch 100| loss: 0.25088 | val_0_rmse: 0.48216 | val_1_rmse: 0.51393 |  0:00:36s
epoch 101| loss: 0.25104 | val_0_rmse: 0.47605 | val_1_rmse: 0.51291 |  0:00:36s
epoch 102| loss: 0.25022 | val_0_rmse: 0.48105 | val_1_rmse: 0.51277 |  0:00:37s
epoch 103| loss: 0.25308 | val_0_rmse: 0.48022 | val_1_rmse: 0.50958 |  0:00:37s
epoch 104| loss: 0.25228 | val_0_rmse: 0.48418 | val_1_rmse: 0.51617 |  0:00:37s
epoch 105| loss: 0.25781 | val_0_rmse: 0.47814 | val_1_rmse: 0.51517 |  0:00:38s
epoch 106| loss: 0.2496  | val_0_rmse: 0.4783  | val_1_rmse: 0.5172  |  0:00:38s
epoch 107| loss: 0.24759 | val_0_rmse: 0.48424 | val_1_rmse: 0.52438 |  0:00:38s
epoch 108| loss: 0.25019 | val_0_rmse: 0.48006 | val_1_rmse: 0.51526 |  0:00:39s
epoch 109| loss: 0.26286 | val_0_rmse: 0.50778 | val_1_rmse: 0.53047 |  0:00:39s
epoch 110| loss: 0.26377 | val_0_rmse: 0.49559 | val_1_rmse: 0.52404 |  0:00:40s
epoch 111| loss: 0.25627 | val_0_rmse: 0.49529 | val_1_rmse: 0.52263 |  0:00:40s
epoch 112| loss: 0.25912 | val_0_rmse: 0.48375 | val_1_rmse: 0.51866 |  0:00:40s
epoch 113| loss: 0.25051 | val_0_rmse: 0.49135 | val_1_rmse: 0.52459 |  0:00:41s
epoch 114| loss: 0.25883 | val_0_rmse: 0.4894  | val_1_rmse: 0.52163 |  0:00:41s
epoch 115| loss: 0.25986 | val_0_rmse: 0.4786  | val_1_rmse: 0.51535 |  0:00:41s
epoch 116| loss: 0.25523 | val_0_rmse: 0.4754  | val_1_rmse: 0.51238 |  0:00:42s
epoch 117| loss: 0.24337 | val_0_rmse: 0.49542 | val_1_rmse: 0.52811 |  0:00:42s
epoch 118| loss: 0.26143 | val_0_rmse: 0.4889  | val_1_rmse: 0.52585 |  0:00:42s
epoch 119| loss: 0.25906 | val_0_rmse: 0.47949 | val_1_rmse: 0.51456 |  0:00:43s
epoch 120| loss: 0.24208 | val_0_rmse: 0.48146 | val_1_rmse: 0.51757 |  0:00:43s
epoch 121| loss: 0.24397 | val_0_rmse: 0.47248 | val_1_rmse: 0.51012 |  0:00:43s
epoch 122| loss: 0.25041 | val_0_rmse: 0.47439 | val_1_rmse: 0.5146  |  0:00:44s
epoch 123| loss: 0.24835 | val_0_rmse: 0.4754  | val_1_rmse: 0.51793 |  0:00:44s
epoch 124| loss: 0.25231 | val_0_rmse: 0.47757 | val_1_rmse: 0.52425 |  0:00:45s
epoch 125| loss: 0.25269 | val_0_rmse: 0.48023 | val_1_rmse: 0.5235  |  0:00:45s
epoch 126| loss: 0.24917 | val_0_rmse: 0.49675 | val_1_rmse: 0.52859 |  0:00:45s
epoch 127| loss: 0.26878 | val_0_rmse: 0.50007 | val_1_rmse: 0.52752 |  0:00:46s
epoch 128| loss: 0.25561 | val_0_rmse: 0.48179 | val_1_rmse: 0.51037 |  0:00:46s
epoch 129| loss: 0.26295 | val_0_rmse: 0.48719 | val_1_rmse: 0.52443 |  0:00:46s
epoch 130| loss: 0.26211 | val_0_rmse: 0.47973 | val_1_rmse: 0.51587 |  0:00:47s
epoch 131| loss: 0.25166 | val_0_rmse: 0.48034 | val_1_rmse: 0.51416 |  0:00:47s
epoch 132| loss: 0.25375 | val_0_rmse: 0.48773 | val_1_rmse: 0.52469 |  0:00:47s
epoch 133| loss: 0.25507 | val_0_rmse: 0.5007  | val_1_rmse: 0.53668 |  0:00:48s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.50958
Best weights from best epoch are automatically used!
ended training at: 10:56:59
Feature importance:
[('Area', 0.20850051634531164), ('Baths', 0.014160051226509108), ('Beds', 0.05009074269632662), ('Latitude', 0.3922195230692489), ('Longitude', 0.2818289052917895), ('Month', 0.033366328526920626), ('Year', 0.019833932843893568)]
Mean squared error is of 21345419387.781754
Mean absolute error:107171.09657112823
MAPE:0.18641491322232112
R2 score:0.7247129855424426
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:56:59
epoch 0  | loss: 1.18074 | val_0_rmse: 1.24661 | val_1_rmse: 1.27603 |  0:00:00s
epoch 1  | loss: 0.66993 | val_0_rmse: 0.86016 | val_1_rmse: 0.84752 |  0:00:00s
epoch 2  | loss: 0.52352 | val_0_rmse: 0.73923 | val_1_rmse: 0.74824 |  0:00:01s
epoch 3  | loss: 0.4704  | val_0_rmse: 0.7069  | val_1_rmse: 0.71999 |  0:00:01s
epoch 4  | loss: 0.43172 | val_0_rmse: 0.69761 | val_1_rmse: 0.699   |  0:00:01s
epoch 5  | loss: 0.42144 | val_0_rmse: 0.70263 | val_1_rmse: 0.69964 |  0:00:02s
epoch 6  | loss: 0.38665 | val_0_rmse: 0.68789 | val_1_rmse: 0.68986 |  0:00:02s
epoch 7  | loss: 0.37308 | val_0_rmse: 0.59171 | val_1_rmse: 0.59262 |  0:00:02s
epoch 8  | loss: 0.36088 | val_0_rmse: 0.58225 | val_1_rmse: 0.59056 |  0:00:03s
epoch 9  | loss: 0.34862 | val_0_rmse: 0.55866 | val_1_rmse: 0.56337 |  0:00:03s
epoch 10 | loss: 0.33482 | val_0_rmse: 0.56335 | val_1_rmse: 0.56824 |  0:00:04s
epoch 11 | loss: 0.33682 | val_0_rmse: 0.56104 | val_1_rmse: 0.57339 |  0:00:04s
epoch 12 | loss: 0.32551 | val_0_rmse: 0.54621 | val_1_rmse: 0.55175 |  0:00:04s
epoch 13 | loss: 0.32964 | val_0_rmse: 0.55508 | val_1_rmse: 0.56517 |  0:00:05s
epoch 14 | loss: 0.32081 | val_0_rmse: 0.53315 | val_1_rmse: 0.54663 |  0:00:05s
epoch 15 | loss: 0.31218 | val_0_rmse: 0.54031 | val_1_rmse: 0.54221 |  0:00:05s
epoch 16 | loss: 0.31926 | val_0_rmse: 0.53516 | val_1_rmse: 0.5428  |  0:00:06s
epoch 17 | loss: 0.31594 | val_0_rmse: 0.52681 | val_1_rmse: 0.53803 |  0:00:06s
epoch 18 | loss: 0.30447 | val_0_rmse: 0.53843 | val_1_rmse: 0.53808 |  0:00:06s
epoch 19 | loss: 0.30886 | val_0_rmse: 0.54431 | val_1_rmse: 0.54369 |  0:00:07s
epoch 20 | loss: 0.3071  | val_0_rmse: 0.53541 | val_1_rmse: 0.53395 |  0:00:07s
epoch 21 | loss: 0.29524 | val_0_rmse: 0.51984 | val_1_rmse: 0.53665 |  0:00:08s
epoch 22 | loss: 0.29144 | val_0_rmse: 0.51749 | val_1_rmse: 0.52564 |  0:00:08s
epoch 23 | loss: 0.2875  | val_0_rmse: 0.51145 | val_1_rmse: 0.52822 |  0:00:08s
epoch 24 | loss: 0.29116 | val_0_rmse: 0.51957 | val_1_rmse: 0.5373  |  0:00:09s
epoch 25 | loss: 0.29761 | val_0_rmse: 0.52612 | val_1_rmse: 0.54274 |  0:00:09s
epoch 26 | loss: 0.29659 | val_0_rmse: 0.52235 | val_1_rmse: 0.53532 |  0:00:09s
epoch 27 | loss: 0.29313 | val_0_rmse: 0.51327 | val_1_rmse: 0.51683 |  0:00:10s
epoch 28 | loss: 0.28303 | val_0_rmse: 0.5134  | val_1_rmse: 0.52535 |  0:00:10s
epoch 29 | loss: 0.28403 | val_0_rmse: 0.52095 | val_1_rmse: 0.52623 |  0:00:10s
epoch 30 | loss: 0.27751 | val_0_rmse: 0.51274 | val_1_rmse: 0.52658 |  0:00:11s
epoch 31 | loss: 0.28618 | val_0_rmse: 0.50556 | val_1_rmse: 0.51406 |  0:00:11s
epoch 32 | loss: 0.27695 | val_0_rmse: 0.51117 | val_1_rmse: 0.53109 |  0:00:12s
epoch 33 | loss: 0.28844 | val_0_rmse: 0.5018  | val_1_rmse: 0.5154  |  0:00:12s
epoch 34 | loss: 0.27635 | val_0_rmse: 0.51427 | val_1_rmse: 0.53392 |  0:00:12s
epoch 35 | loss: 0.29059 | val_0_rmse: 0.51114 | val_1_rmse: 0.52846 |  0:00:13s
epoch 36 | loss: 0.27265 | val_0_rmse: 0.49702 | val_1_rmse: 0.51853 |  0:00:13s
epoch 37 | loss: 0.27656 | val_0_rmse: 0.52003 | val_1_rmse: 0.54005 |  0:00:13s
epoch 38 | loss: 0.28011 | val_0_rmse: 0.5023  | val_1_rmse: 0.5234  |  0:00:14s
epoch 39 | loss: 0.28419 | val_0_rmse: 0.50343 | val_1_rmse: 0.5223  |  0:00:14s
epoch 40 | loss: 0.281   | val_0_rmse: 0.49638 | val_1_rmse: 0.5152  |  0:00:14s
epoch 41 | loss: 0.2676  | val_0_rmse: 0.49834 | val_1_rmse: 0.52177 |  0:00:15s
epoch 42 | loss: 0.27327 | val_0_rmse: 0.50506 | val_1_rmse: 0.51433 |  0:00:15s
epoch 43 | loss: 0.26868 | val_0_rmse: 0.49789 | val_1_rmse: 0.52185 |  0:00:15s
epoch 44 | loss: 0.27007 | val_0_rmse: 0.50043 | val_1_rmse: 0.52992 |  0:00:16s
epoch 45 | loss: 0.27199 | val_0_rmse: 0.50135 | val_1_rmse: 0.53508 |  0:00:16s
epoch 46 | loss: 0.26299 | val_0_rmse: 0.49343 | val_1_rmse: 0.51919 |  0:00:17s
epoch 47 | loss: 0.26806 | val_0_rmse: 0.50274 | val_1_rmse: 0.51747 |  0:00:17s
epoch 48 | loss: 0.26918 | val_0_rmse: 0.49897 | val_1_rmse: 0.52169 |  0:00:17s
epoch 49 | loss: 0.26937 | val_0_rmse: 0.49667 | val_1_rmse: 0.51623 |  0:00:18s
epoch 50 | loss: 0.27173 | val_0_rmse: 0.48787 | val_1_rmse: 0.51573 |  0:00:18s
epoch 51 | loss: 0.26007 | val_0_rmse: 0.49593 | val_1_rmse: 0.52006 |  0:00:18s
epoch 52 | loss: 0.25757 | val_0_rmse: 0.49326 | val_1_rmse: 0.51611 |  0:00:19s
epoch 53 | loss: 0.26335 | val_0_rmse: 0.49068 | val_1_rmse: 0.51165 |  0:00:19s
epoch 54 | loss: 0.25955 | val_0_rmse: 0.50361 | val_1_rmse: 0.52182 |  0:00:19s
epoch 55 | loss: 0.26924 | val_0_rmse: 0.50277 | val_1_rmse: 0.52487 |  0:00:20s
epoch 56 | loss: 0.26457 | val_0_rmse: 0.49454 | val_1_rmse: 0.5199  |  0:00:20s
epoch 57 | loss: 0.25586 | val_0_rmse: 0.48961 | val_1_rmse: 0.51221 |  0:00:21s
epoch 58 | loss: 0.25493 | val_0_rmse: 0.48549 | val_1_rmse: 0.51527 |  0:00:21s
epoch 59 | loss: 0.25703 | val_0_rmse: 0.49124 | val_1_rmse: 0.52215 |  0:00:21s
epoch 60 | loss: 0.26372 | val_0_rmse: 0.50982 | val_1_rmse: 0.53292 |  0:00:22s
epoch 61 | loss: 0.27049 | val_0_rmse: 0.48784 | val_1_rmse: 0.51637 |  0:00:22s
epoch 62 | loss: 0.26775 | val_0_rmse: 0.48881 | val_1_rmse: 0.51314 |  0:00:22s
epoch 63 | loss: 0.25695 | val_0_rmse: 0.49621 | val_1_rmse: 0.53167 |  0:00:23s
epoch 64 | loss: 0.25956 | val_0_rmse: 0.48814 | val_1_rmse: 0.51707 |  0:00:23s
epoch 65 | loss: 0.25394 | val_0_rmse: 0.48143 | val_1_rmse: 0.51409 |  0:00:23s
epoch 66 | loss: 0.24897 | val_0_rmse: 0.48077 | val_1_rmse: 0.50731 |  0:00:24s
epoch 67 | loss: 0.2552  | val_0_rmse: 0.48159 | val_1_rmse: 0.52239 |  0:00:24s
epoch 68 | loss: 0.25381 | val_0_rmse: 0.47916 | val_1_rmse: 0.51246 |  0:00:25s
epoch 69 | loss: 0.25948 | val_0_rmse: 0.48036 | val_1_rmse: 0.51524 |  0:00:25s
epoch 70 | loss: 0.26684 | val_0_rmse: 0.49712 | val_1_rmse: 0.52953 |  0:00:25s
epoch 71 | loss: 0.26352 | val_0_rmse: 0.48087 | val_1_rmse: 0.51404 |  0:00:26s
epoch 72 | loss: 0.25739 | val_0_rmse: 0.49708 | val_1_rmse: 0.52563 |  0:00:26s
epoch 73 | loss: 0.26881 | val_0_rmse: 0.49437 | val_1_rmse: 0.53448 |  0:00:26s
epoch 74 | loss: 0.27263 | val_0_rmse: 0.52414 | val_1_rmse: 0.55906 |  0:00:27s
epoch 75 | loss: 0.27642 | val_0_rmse: 0.47652 | val_1_rmse: 0.51331 |  0:00:27s
epoch 76 | loss: 0.25428 | val_0_rmse: 0.48935 | val_1_rmse: 0.52494 |  0:00:27s
epoch 77 | loss: 0.25765 | val_0_rmse: 0.49269 | val_1_rmse: 0.52233 |  0:00:28s
epoch 78 | loss: 0.25691 | val_0_rmse: 0.48133 | val_1_rmse: 0.51743 |  0:00:28s
epoch 79 | loss: 0.24697 | val_0_rmse: 0.48578 | val_1_rmse: 0.51794 |  0:00:28s
epoch 80 | loss: 0.25182 | val_0_rmse: 0.47729 | val_1_rmse: 0.50999 |  0:00:29s
epoch 81 | loss: 0.24771 | val_0_rmse: 0.47809 | val_1_rmse: 0.50508 |  0:00:29s
epoch 82 | loss: 0.25245 | val_0_rmse: 0.47628 | val_1_rmse: 0.50585 |  0:00:30s
epoch 83 | loss: 0.25222 | val_0_rmse: 0.48471 | val_1_rmse: 0.51505 |  0:00:30s
epoch 84 | loss: 0.2472  | val_0_rmse: 0.47758 | val_1_rmse: 0.51147 |  0:00:30s
epoch 85 | loss: 0.24994 | val_0_rmse: 0.48363 | val_1_rmse: 0.51922 |  0:00:31s
epoch 86 | loss: 0.24603 | val_0_rmse: 0.47243 | val_1_rmse: 0.5057  |  0:00:31s
epoch 87 | loss: 0.25339 | val_0_rmse: 0.48998 | val_1_rmse: 0.52326 |  0:00:31s
epoch 88 | loss: 0.24699 | val_0_rmse: 0.47849 | val_1_rmse: 0.5097  |  0:00:32s
epoch 89 | loss: 0.24749 | val_0_rmse: 0.47229 | val_1_rmse: 0.51319 |  0:00:32s
epoch 90 | loss: 0.24905 | val_0_rmse: 0.49345 | val_1_rmse: 0.518   |  0:00:32s
epoch 91 | loss: 0.25714 | val_0_rmse: 0.48633 | val_1_rmse: 0.52904 |  0:00:33s
epoch 92 | loss: 0.24745 | val_0_rmse: 0.48508 | val_1_rmse: 0.51625 |  0:00:33s
epoch 93 | loss: 0.25434 | val_0_rmse: 0.47169 | val_1_rmse: 0.50583 |  0:00:34s
epoch 94 | loss: 0.25509 | val_0_rmse: 0.47519 | val_1_rmse: 0.51383 |  0:00:34s
epoch 95 | loss: 0.2567  | val_0_rmse: 0.47963 | val_1_rmse: 0.51506 |  0:00:34s
epoch 96 | loss: 0.25148 | val_0_rmse: 0.47262 | val_1_rmse: 0.51154 |  0:00:35s
epoch 97 | loss: 0.25257 | val_0_rmse: 0.48159 | val_1_rmse: 0.51811 |  0:00:35s
epoch 98 | loss: 0.24904 | val_0_rmse: 0.4777  | val_1_rmse: 0.51392 |  0:00:35s
epoch 99 | loss: 0.24075 | val_0_rmse: 0.46841 | val_1_rmse: 0.50705 |  0:00:36s
epoch 100| loss: 0.23979 | val_0_rmse: 0.4682  | val_1_rmse: 0.50599 |  0:00:36s
epoch 101| loss: 0.24266 | val_0_rmse: 0.47053 | val_1_rmse: 0.50525 |  0:00:36s
epoch 102| loss: 0.24681 | val_0_rmse: 0.47783 | val_1_rmse: 0.52188 |  0:00:37s
epoch 103| loss: 0.24796 | val_0_rmse: 0.48155 | val_1_rmse: 0.51211 |  0:00:37s
epoch 104| loss: 0.24891 | val_0_rmse: 0.46515 | val_1_rmse: 0.50874 |  0:00:38s
epoch 105| loss: 0.23918 | val_0_rmse: 0.4638  | val_1_rmse: 0.49812 |  0:00:38s
epoch 106| loss: 0.24199 | val_0_rmse: 0.47307 | val_1_rmse: 0.5073  |  0:00:38s
epoch 107| loss: 0.24265 | val_0_rmse: 0.46918 | val_1_rmse: 0.50987 |  0:00:39s
epoch 108| loss: 0.24154 | val_0_rmse: 0.47298 | val_1_rmse: 0.50639 |  0:00:39s
epoch 109| loss: 0.24978 | val_0_rmse: 0.47407 | val_1_rmse: 0.51416 |  0:00:39s
epoch 110| loss: 0.24862 | val_0_rmse: 0.47686 | val_1_rmse: 0.50904 |  0:00:40s
epoch 111| loss: 0.24265 | val_0_rmse: 0.46682 | val_1_rmse: 0.50571 |  0:00:40s
epoch 112| loss: 0.24517 | val_0_rmse: 0.47141 | val_1_rmse: 0.50342 |  0:00:40s
epoch 113| loss: 0.23771 | val_0_rmse: 0.47091 | val_1_rmse: 0.5154  |  0:00:41s
epoch 114| loss: 0.24642 | val_0_rmse: 0.47983 | val_1_rmse: 0.50648 |  0:00:41s
epoch 115| loss: 0.2389  | val_0_rmse: 0.4708  | val_1_rmse: 0.50553 |  0:00:41s
epoch 116| loss: 0.24729 | val_0_rmse: 0.47279 | val_1_rmse: 0.50613 |  0:00:42s
epoch 117| loss: 0.24458 | val_0_rmse: 0.46855 | val_1_rmse: 0.50611 |  0:00:42s
epoch 118| loss: 0.24676 | val_0_rmse: 0.46667 | val_1_rmse: 0.49879 |  0:00:43s
epoch 119| loss: 0.23413 | val_0_rmse: 0.46217 | val_1_rmse: 0.49905 |  0:00:43s
epoch 120| loss: 0.23369 | val_0_rmse: 0.46571 | val_1_rmse: 0.50612 |  0:00:43s
epoch 121| loss: 0.23761 | val_0_rmse: 0.46574 | val_1_rmse: 0.504   |  0:00:44s
epoch 122| loss: 0.23618 | val_0_rmse: 0.46064 | val_1_rmse: 0.49963 |  0:00:44s
epoch 123| loss: 0.24029 | val_0_rmse: 0.47228 | val_1_rmse: 0.51508 |  0:00:44s
epoch 124| loss: 0.25506 | val_0_rmse: 0.4838  | val_1_rmse: 0.50847 |  0:00:45s
epoch 125| loss: 0.24847 | val_0_rmse: 0.47114 | val_1_rmse: 0.50922 |  0:00:45s
epoch 126| loss: 0.2442  | val_0_rmse: 0.4721  | val_1_rmse: 0.50203 |  0:00:45s
epoch 127| loss: 0.23629 | val_0_rmse: 0.46942 | val_1_rmse: 0.5092  |  0:00:46s
epoch 128| loss: 0.24249 | val_0_rmse: 0.47566 | val_1_rmse: 0.50089 |  0:00:46s
epoch 129| loss: 0.24496 | val_0_rmse: 0.47417 | val_1_rmse: 0.51172 |  0:00:47s
epoch 130| loss: 0.24999 | val_0_rmse: 0.47157 | val_1_rmse: 0.49977 |  0:00:47s
epoch 131| loss: 0.24064 | val_0_rmse: 0.46262 | val_1_rmse: 0.50029 |  0:00:47s
epoch 132| loss: 0.24029 | val_0_rmse: 0.46069 | val_1_rmse: 0.49542 |  0:00:48s
epoch 133| loss: 0.23151 | val_0_rmse: 0.45869 | val_1_rmse: 0.50268 |  0:00:48s
epoch 134| loss: 0.24163 | val_0_rmse: 0.46265 | val_1_rmse: 0.50592 |  0:00:48s
epoch 135| loss: 0.24453 | val_0_rmse: 0.48643 | val_1_rmse: 0.52508 |  0:00:49s
epoch 136| loss: 0.24191 | val_0_rmse: 0.47189 | val_1_rmse: 0.51359 |  0:00:49s
epoch 137| loss: 0.24101 | val_0_rmse: 0.47    | val_1_rmse: 0.5063  |  0:00:49s
epoch 138| loss: 0.23066 | val_0_rmse: 0.46215 | val_1_rmse: 0.50363 |  0:00:50s
epoch 139| loss: 0.23535 | val_0_rmse: 0.45585 | val_1_rmse: 0.49895 |  0:00:50s
epoch 140| loss: 0.23622 | val_0_rmse: 0.45774 | val_1_rmse: 0.49927 |  0:00:50s
epoch 141| loss: 0.23216 | val_0_rmse: 0.45863 | val_1_rmse: 0.50423 |  0:00:51s
epoch 142| loss: 0.23419 | val_0_rmse: 0.46458 | val_1_rmse: 0.51315 |  0:00:51s
epoch 143| loss: 0.23261 | val_0_rmse: 0.46686 | val_1_rmse: 0.50866 |  0:00:52s
epoch 144| loss: 0.24036 | val_0_rmse: 0.46468 | val_1_rmse: 0.50467 |  0:00:52s
epoch 145| loss: 0.23641 | val_0_rmse: 0.46314 | val_1_rmse: 0.50962 |  0:00:52s
epoch 146| loss: 0.24003 | val_0_rmse: 0.45495 | val_1_rmse: 0.499   |  0:00:53s
epoch 147| loss: 0.23182 | val_0_rmse: 0.45571 | val_1_rmse: 0.5091  |  0:00:53s
epoch 148| loss: 0.24299 | val_0_rmse: 0.46206 | val_1_rmse: 0.50803 |  0:00:53s
epoch 149| loss: 0.23169 | val_0_rmse: 0.4636  | val_1_rmse: 0.50215 |  0:00:54s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.49542
Best weights from best epoch are automatically used!
ended training at: 10:57:53
Feature importance:
[('Area', 0.1945756471225663), ('Baths', 0.03074229880217497), ('Beds', 0.15871286851909597), ('Latitude', 0.17916162313588607), ('Longitude', 0.3042557705814834), ('Month', 0.0), ('Year', 0.1325517918387933)]
Mean squared error is of 23823921973.996246
Mean absolute error:110612.6254543185
MAPE:0.19562652461317862
R2 score:0.7043143786559378
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:57:53
epoch 0  | loss: 1.1572  | val_0_rmse: 1.11004 | val_1_rmse: 1.05138 |  0:00:00s
epoch 1  | loss: 0.61557 | val_0_rmse: 0.90658 | val_1_rmse: 0.86943 |  0:00:00s
epoch 2  | loss: 0.51426 | val_0_rmse: 0.78427 | val_1_rmse: 0.79137 |  0:00:01s
epoch 3  | loss: 0.47659 | val_0_rmse: 0.70535 | val_1_rmse: 0.70828 |  0:00:01s
epoch 4  | loss: 0.42114 | val_0_rmse: 0.75469 | val_1_rmse: 0.75977 |  0:00:01s
epoch 5  | loss: 0.39647 | val_0_rmse: 0.66261 | val_1_rmse: 0.66579 |  0:00:02s
epoch 6  | loss: 0.37212 | val_0_rmse: 0.62047 | val_1_rmse: 0.61148 |  0:00:02s
epoch 7  | loss: 0.36546 | val_0_rmse: 0.61301 | val_1_rmse: 0.61076 |  0:00:02s
epoch 8  | loss: 0.35094 | val_0_rmse: 0.58504 | val_1_rmse: 0.58443 |  0:00:03s
epoch 9  | loss: 0.34172 | val_0_rmse: 0.57376 | val_1_rmse: 0.58032 |  0:00:03s
epoch 10 | loss: 0.33747 | val_0_rmse: 0.57624 | val_1_rmse: 0.58297 |  0:00:04s
epoch 11 | loss: 0.33893 | val_0_rmse: 0.55858 | val_1_rmse: 0.55744 |  0:00:04s
epoch 12 | loss: 0.31768 | val_0_rmse: 0.55166 | val_1_rmse: 0.55695 |  0:00:04s
epoch 13 | loss: 0.32169 | val_0_rmse: 0.57803 | val_1_rmse: 0.56612 |  0:00:05s
epoch 14 | loss: 0.34055 | val_0_rmse: 0.5613  | val_1_rmse: 0.55437 |  0:00:05s
epoch 15 | loss: 0.31665 | val_0_rmse: 0.55001 | val_1_rmse: 0.55148 |  0:00:05s
epoch 16 | loss: 0.31001 | val_0_rmse: 0.53928 | val_1_rmse: 0.53983 |  0:00:06s
epoch 17 | loss: 0.315   | val_0_rmse: 0.55712 | val_1_rmse: 0.54883 |  0:00:06s
epoch 18 | loss: 0.31349 | val_0_rmse: 0.54929 | val_1_rmse: 0.542   |  0:00:06s
epoch 19 | loss: 0.30024 | val_0_rmse: 0.53058 | val_1_rmse: 0.53578 |  0:00:07s
epoch 20 | loss: 0.30757 | val_0_rmse: 0.55107 | val_1_rmse: 0.54496 |  0:00:07s
epoch 21 | loss: 0.31372 | val_0_rmse: 0.52335 | val_1_rmse: 0.5399  |  0:00:08s
epoch 22 | loss: 0.30742 | val_0_rmse: 0.54954 | val_1_rmse: 0.54379 |  0:00:08s
epoch 23 | loss: 0.30402 | val_0_rmse: 0.53534 | val_1_rmse: 0.54936 |  0:00:08s
epoch 24 | loss: 0.2985  | val_0_rmse: 0.52209 | val_1_rmse: 0.52439 |  0:00:09s
epoch 25 | loss: 0.28616 | val_0_rmse: 0.51842 | val_1_rmse: 0.53234 |  0:00:09s
epoch 26 | loss: 0.29586 | val_0_rmse: 0.52371 | val_1_rmse: 0.53357 |  0:00:09s
epoch 27 | loss: 0.28844 | val_0_rmse: 0.51245 | val_1_rmse: 0.53368 |  0:00:10s
epoch 28 | loss: 0.28588 | val_0_rmse: 0.5211  | val_1_rmse: 0.53131 |  0:00:10s
epoch 29 | loss: 0.29441 | val_0_rmse: 0.51318 | val_1_rmse: 0.53455 |  0:00:10s
epoch 30 | loss: 0.28213 | val_0_rmse: 0.51156 | val_1_rmse: 0.52306 |  0:00:11s
epoch 31 | loss: 0.2827  | val_0_rmse: 0.52378 | val_1_rmse: 0.5336  |  0:00:11s
epoch 32 | loss: 0.28273 | val_0_rmse: 0.50511 | val_1_rmse: 0.52279 |  0:00:12s
epoch 33 | loss: 0.2888  | val_0_rmse: 0.51104 | val_1_rmse: 0.5282  |  0:00:12s
epoch 34 | loss: 0.27681 | val_0_rmse: 0.5138  | val_1_rmse: 0.52523 |  0:00:12s
epoch 35 | loss: 0.28848 | val_0_rmse: 0.51099 | val_1_rmse: 0.52983 |  0:00:13s
epoch 36 | loss: 0.28259 | val_0_rmse: 0.51223 | val_1_rmse: 0.53108 |  0:00:13s
epoch 37 | loss: 0.2702  | val_0_rmse: 0.51303 | val_1_rmse: 0.53481 |  0:00:13s
epoch 38 | loss: 0.27452 | val_0_rmse: 0.4994  | val_1_rmse: 0.51669 |  0:00:14s
epoch 39 | loss: 0.26732 | val_0_rmse: 0.50158 | val_1_rmse: 0.51496 |  0:00:14s
epoch 40 | loss: 0.27923 | val_0_rmse: 0.49908 | val_1_rmse: 0.51326 |  0:00:14s
epoch 41 | loss: 0.26635 | val_0_rmse: 0.49764 | val_1_rmse: 0.51624 |  0:00:15s
epoch 42 | loss: 0.27404 | val_0_rmse: 0.50455 | val_1_rmse: 0.51311 |  0:00:15s
epoch 43 | loss: 0.27137 | val_0_rmse: 0.49483 | val_1_rmse: 0.50949 |  0:00:16s
epoch 44 | loss: 0.2718  | val_0_rmse: 0.50502 | val_1_rmse: 0.52727 |  0:00:16s
epoch 45 | loss: 0.27276 | val_0_rmse: 0.5009  | val_1_rmse: 0.5197  |  0:00:16s
epoch 46 | loss: 0.27338 | val_0_rmse: 0.51896 | val_1_rmse: 0.51865 |  0:00:17s
epoch 47 | loss: 0.27999 | val_0_rmse: 0.50063 | val_1_rmse: 0.51374 |  0:00:17s
epoch 48 | loss: 0.27096 | val_0_rmse: 0.5     | val_1_rmse: 0.51375 |  0:00:17s
epoch 49 | loss: 0.26718 | val_0_rmse: 0.50103 | val_1_rmse: 0.50532 |  0:00:18s
epoch 50 | loss: 0.2741  | val_0_rmse: 0.50322 | val_1_rmse: 0.51165 |  0:00:18s
epoch 51 | loss: 0.27279 | val_0_rmse: 0.49854 | val_1_rmse: 0.50788 |  0:00:18s
epoch 52 | loss: 0.26569 | val_0_rmse: 0.49133 | val_1_rmse: 0.50027 |  0:00:19s
epoch 53 | loss: 0.26515 | val_0_rmse: 0.49837 | val_1_rmse: 0.50667 |  0:00:19s
epoch 54 | loss: 0.26824 | val_0_rmse: 0.5074  | val_1_rmse: 0.50834 |  0:00:19s
epoch 55 | loss: 0.27339 | val_0_rmse: 0.51557 | val_1_rmse: 0.51995 |  0:00:20s
epoch 56 | loss: 0.28205 | val_0_rmse: 0.53131 | val_1_rmse: 0.52388 |  0:00:20s
epoch 57 | loss: 0.2751  | val_0_rmse: 0.50272 | val_1_rmse: 0.51202 |  0:00:21s
epoch 58 | loss: 0.27567 | val_0_rmse: 0.5038  | val_1_rmse: 0.5061  |  0:00:21s
epoch 59 | loss: 0.2623  | val_0_rmse: 0.49567 | val_1_rmse: 0.4898  |  0:00:21s
epoch 60 | loss: 0.25797 | val_0_rmse: 0.50036 | val_1_rmse: 0.5165  |  0:00:22s
epoch 61 | loss: 0.26001 | val_0_rmse: 0.49856 | val_1_rmse: 0.49698 |  0:00:22s
epoch 62 | loss: 0.26332 | val_0_rmse: 0.49803 | val_1_rmse: 0.51232 |  0:00:22s
epoch 63 | loss: 0.26358 | val_0_rmse: 0.49664 | val_1_rmse: 0.4952  |  0:00:23s
epoch 64 | loss: 0.25741 | val_0_rmse: 0.49822 | val_1_rmse: 0.51833 |  0:00:23s
epoch 65 | loss: 0.26583 | val_0_rmse: 0.49062 | val_1_rmse: 0.4937  |  0:00:23s
epoch 66 | loss: 0.25732 | val_0_rmse: 0.48671 | val_1_rmse: 0.49798 |  0:00:24s
epoch 67 | loss: 0.26071 | val_0_rmse: 0.49066 | val_1_rmse: 0.50374 |  0:00:24s
epoch 68 | loss: 0.25834 | val_0_rmse: 0.48686 | val_1_rmse: 0.49893 |  0:00:25s
epoch 69 | loss: 0.25471 | val_0_rmse: 0.48547 | val_1_rmse: 0.49269 |  0:00:25s
epoch 70 | loss: 0.26469 | val_0_rmse: 0.48981 | val_1_rmse: 0.4973  |  0:00:25s
epoch 71 | loss: 0.25629 | val_0_rmse: 0.48266 | val_1_rmse: 0.49584 |  0:00:26s
epoch 72 | loss: 0.25451 | val_0_rmse: 0.49087 | val_1_rmse: 0.49779 |  0:00:26s
epoch 73 | loss: 0.25215 | val_0_rmse: 0.49578 | val_1_rmse: 0.50903 |  0:00:26s
epoch 74 | loss: 0.2503  | val_0_rmse: 0.48145 | val_1_rmse: 0.49147 |  0:00:27s
epoch 75 | loss: 0.25332 | val_0_rmse: 0.48844 | val_1_rmse: 0.49781 |  0:00:27s
epoch 76 | loss: 0.25488 | val_0_rmse: 0.48039 | val_1_rmse: 0.4929  |  0:00:27s
epoch 77 | loss: 0.2619  | val_0_rmse: 0.48459 | val_1_rmse: 0.50036 |  0:00:28s
epoch 78 | loss: 0.25863 | val_0_rmse: 0.49172 | val_1_rmse: 0.49848 |  0:00:28s
epoch 79 | loss: 0.26851 | val_0_rmse: 0.49599 | val_1_rmse: 0.5028  |  0:00:29s
epoch 80 | loss: 0.26429 | val_0_rmse: 0.48215 | val_1_rmse: 0.49019 |  0:00:29s
epoch 81 | loss: 0.25234 | val_0_rmse: 0.48343 | val_1_rmse: 0.50259 |  0:00:29s
epoch 82 | loss: 0.24649 | val_0_rmse: 0.49953 | val_1_rmse: 0.50416 |  0:00:30s
epoch 83 | loss: 0.25325 | val_0_rmse: 0.47662 | val_1_rmse: 0.49488 |  0:00:30s
epoch 84 | loss: 0.25637 | val_0_rmse: 0.48427 | val_1_rmse: 0.50405 |  0:00:30s
epoch 85 | loss: 0.25594 | val_0_rmse: 0.47803 | val_1_rmse: 0.49409 |  0:00:31s
epoch 86 | loss: 0.2467  | val_0_rmse: 0.47379 | val_1_rmse: 0.49767 |  0:00:31s
epoch 87 | loss: 0.24858 | val_0_rmse: 0.48031 | val_1_rmse: 0.50102 |  0:00:31s
epoch 88 | loss: 0.24727 | val_0_rmse: 0.47853 | val_1_rmse: 0.49583 |  0:00:32s
epoch 89 | loss: 0.25413 | val_0_rmse: 0.48585 | val_1_rmse: 0.49539 |  0:00:32s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.4898
Best weights from best epoch are automatically used!
ended training at: 10:58:26
Feature importance:
[('Area', 0.2745558962733232), ('Baths', 0.09035739927476316), ('Beds', 0.05354950680261901), ('Latitude', 0.32294653658857747), ('Longitude', 0.2211618857725353), ('Month', 0.010262566756235908), ('Year', 0.027166208531945985)]
Mean squared error is of 21346340530.663746
Mean absolute error:105367.36408623861
MAPE:0.17534466143174374
R2 score:0.7336340988187793
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:58:26
epoch 0  | loss: 1.15326 | val_0_rmse: 1.00955 | val_1_rmse: 0.95468 |  0:00:00s
epoch 1  | loss: 0.62507 | val_0_rmse: 1.06255 | val_1_rmse: 1.0231  |  0:00:00s
epoch 2  | loss: 0.50479 | val_0_rmse: 0.93847 | val_1_rmse: 0.90981 |  0:00:01s
epoch 3  | loss: 0.44806 | val_0_rmse: 0.69725 | val_1_rmse: 0.68533 |  0:00:01s
epoch 4  | loss: 0.38989 | val_0_rmse: 0.66271 | val_1_rmse: 0.64923 |  0:00:01s
epoch 5  | loss: 0.38148 | val_0_rmse: 0.62063 | val_1_rmse: 0.60014 |  0:00:02s
epoch 6  | loss: 0.38112 | val_0_rmse: 0.61281 | val_1_rmse: 0.61621 |  0:00:02s
epoch 7  | loss: 0.37785 | val_0_rmse: 0.60688 | val_1_rmse: 0.61897 |  0:00:02s
epoch 8  | loss: 0.35644 | val_0_rmse: 0.5738  | val_1_rmse: 0.58539 |  0:00:03s
epoch 9  | loss: 0.34408 | val_0_rmse: 0.57875 | val_1_rmse: 0.57975 |  0:00:03s
epoch 10 | loss: 0.34474 | val_0_rmse: 0.56889 | val_1_rmse: 0.57051 |  0:00:04s
epoch 11 | loss: 0.34842 | val_0_rmse: 0.55683 | val_1_rmse: 0.56049 |  0:00:04s
epoch 12 | loss: 0.33375 | val_0_rmse: 0.5648  | val_1_rmse: 0.56409 |  0:00:04s
epoch 13 | loss: 0.32874 | val_0_rmse: 0.56118 | val_1_rmse: 0.55214 |  0:00:05s
epoch 14 | loss: 0.32708 | val_0_rmse: 0.55155 | val_1_rmse: 0.55245 |  0:00:05s
epoch 15 | loss: 0.32261 | val_0_rmse: 0.54752 | val_1_rmse: 0.53964 |  0:00:05s
epoch 16 | loss: 0.31315 | val_0_rmse: 0.54434 | val_1_rmse: 0.54195 |  0:00:06s
epoch 17 | loss: 0.31266 | val_0_rmse: 0.54998 | val_1_rmse: 0.54788 |  0:00:06s
epoch 18 | loss: 0.31857 | val_0_rmse: 0.5518  | val_1_rmse: 0.55284 |  0:00:06s
epoch 19 | loss: 0.33244 | val_0_rmse: 0.55267 | val_1_rmse: 0.55762 |  0:00:07s
epoch 20 | loss: 0.3145  | val_0_rmse: 0.55477 | val_1_rmse: 0.54475 |  0:00:07s
epoch 21 | loss: 0.3111  | val_0_rmse: 0.54502 | val_1_rmse: 0.55385 |  0:00:08s
epoch 22 | loss: 0.30829 | val_0_rmse: 0.55488 | val_1_rmse: 0.55282 |  0:00:08s
epoch 23 | loss: 0.30137 | val_0_rmse: 0.54891 | val_1_rmse: 0.56777 |  0:00:08s
epoch 24 | loss: 0.30564 | val_0_rmse: 0.53021 | val_1_rmse: 0.539   |  0:00:09s
epoch 25 | loss: 0.29313 | val_0_rmse: 0.52816 | val_1_rmse: 0.53912 |  0:00:09s
epoch 26 | loss: 0.29848 | val_0_rmse: 0.5286  | val_1_rmse: 0.53568 |  0:00:09s
epoch 27 | loss: 0.30485 | val_0_rmse: 0.51888 | val_1_rmse: 0.53367 |  0:00:10s
epoch 28 | loss: 0.28827 | val_0_rmse: 0.52492 | val_1_rmse: 0.52905 |  0:00:10s
epoch 29 | loss: 0.28531 | val_0_rmse: 0.55124 | val_1_rmse: 0.55037 |  0:00:10s
epoch 30 | loss: 0.2978  | val_0_rmse: 0.54896 | val_1_rmse: 0.5426  |  0:00:11s
epoch 31 | loss: 0.29948 | val_0_rmse: 0.53062 | val_1_rmse: 0.53123 |  0:00:11s
epoch 32 | loss: 0.2936  | val_0_rmse: 0.52282 | val_1_rmse: 0.5257  |  0:00:12s
epoch 33 | loss: 0.28715 | val_0_rmse: 0.51064 | val_1_rmse: 0.51894 |  0:00:12s
epoch 34 | loss: 0.27944 | val_0_rmse: 0.51281 | val_1_rmse: 0.51558 |  0:00:12s
epoch 35 | loss: 0.28303 | val_0_rmse: 0.50609 | val_1_rmse: 0.5107  |  0:00:13s
epoch 36 | loss: 0.28003 | val_0_rmse: 0.50767 | val_1_rmse: 0.51081 |  0:00:13s
epoch 37 | loss: 0.28124 | val_0_rmse: 0.50797 | val_1_rmse: 0.51673 |  0:00:13s
epoch 38 | loss: 0.27907 | val_0_rmse: 0.50417 | val_1_rmse: 0.51622 |  0:00:14s
epoch 39 | loss: 0.27831 | val_0_rmse: 0.51063 | val_1_rmse: 0.52249 |  0:00:14s
epoch 40 | loss: 0.2822  | val_0_rmse: 0.51201 | val_1_rmse: 0.51756 |  0:00:14s
epoch 41 | loss: 0.28726 | val_0_rmse: 0.50413 | val_1_rmse: 0.51895 |  0:00:15s
epoch 42 | loss: 0.26923 | val_0_rmse: 0.51401 | val_1_rmse: 0.53325 |  0:00:15s
epoch 43 | loss: 0.27138 | val_0_rmse: 0.50139 | val_1_rmse: 0.51427 |  0:00:16s
epoch 44 | loss: 0.27461 | val_0_rmse: 0.50076 | val_1_rmse: 0.51261 |  0:00:16s
epoch 45 | loss: 0.27445 | val_0_rmse: 0.50028 | val_1_rmse: 0.50809 |  0:00:16s
epoch 46 | loss: 0.26517 | val_0_rmse: 0.49952 | val_1_rmse: 0.51235 |  0:00:17s
epoch 47 | loss: 0.26361 | val_0_rmse: 0.52267 | val_1_rmse: 0.52481 |  0:00:17s
epoch 48 | loss: 0.28505 | val_0_rmse: 0.5058  | val_1_rmse: 0.51165 |  0:00:17s
epoch 49 | loss: 0.27567 | val_0_rmse: 0.51048 | val_1_rmse: 0.52436 |  0:00:18s
epoch 50 | loss: 0.26923 | val_0_rmse: 0.49436 | val_1_rmse: 0.51103 |  0:00:18s
epoch 51 | loss: 0.26266 | val_0_rmse: 0.49375 | val_1_rmse: 0.50975 |  0:00:18s
epoch 52 | loss: 0.26317 | val_0_rmse: 0.49529 | val_1_rmse: 0.51675 |  0:00:19s
epoch 53 | loss: 0.26079 | val_0_rmse: 0.49238 | val_1_rmse: 0.50477 |  0:00:19s
epoch 54 | loss: 0.27013 | val_0_rmse: 0.49551 | val_1_rmse: 0.51188 |  0:00:20s
epoch 55 | loss: 0.26551 | val_0_rmse: 0.49858 | val_1_rmse: 0.51977 |  0:00:20s
epoch 56 | loss: 0.26961 | val_0_rmse: 0.49444 | val_1_rmse: 0.50686 |  0:00:20s
epoch 57 | loss: 0.26082 | val_0_rmse: 0.49481 | val_1_rmse: 0.51343 |  0:00:21s
epoch 58 | loss: 0.26214 | val_0_rmse: 0.48893 | val_1_rmse: 0.50908 |  0:00:21s
epoch 59 | loss: 0.26201 | val_0_rmse: 0.51064 | val_1_rmse: 0.52091 |  0:00:21s
epoch 60 | loss: 0.26398 | val_0_rmse: 0.4935  | val_1_rmse: 0.52147 |  0:00:22s
epoch 61 | loss: 0.26194 | val_0_rmse: 0.50593 | val_1_rmse: 0.51836 |  0:00:22s
epoch 62 | loss: 0.26882 | val_0_rmse: 0.48869 | val_1_rmse: 0.50985 |  0:00:22s
epoch 63 | loss: 0.25774 | val_0_rmse: 0.4913  | val_1_rmse: 0.5048  |  0:00:23s
epoch 64 | loss: 0.25765 | val_0_rmse: 0.48983 | val_1_rmse: 0.50787 |  0:00:23s
epoch 65 | loss: 0.26457 | val_0_rmse: 0.49842 | val_1_rmse: 0.5111  |  0:00:23s
epoch 66 | loss: 0.25911 | val_0_rmse: 0.5058  | val_1_rmse: 0.52084 |  0:00:24s
epoch 67 | loss: 0.27661 | val_0_rmse: 0.49077 | val_1_rmse: 0.5114  |  0:00:24s
epoch 68 | loss: 0.26433 | val_0_rmse: 0.4897  | val_1_rmse: 0.51093 |  0:00:25s
epoch 69 | loss: 0.26612 | val_0_rmse: 0.48783 | val_1_rmse: 0.50687 |  0:00:25s
epoch 70 | loss: 0.25328 | val_0_rmse: 0.485   | val_1_rmse: 0.5052  |  0:00:25s
epoch 71 | loss: 0.25537 | val_0_rmse: 0.48275 | val_1_rmse: 0.49908 |  0:00:26s
epoch 72 | loss: 0.2537  | val_0_rmse: 0.48355 | val_1_rmse: 0.50644 |  0:00:26s
epoch 73 | loss: 0.25347 | val_0_rmse: 0.48893 | val_1_rmse: 0.50939 |  0:00:26s
epoch 74 | loss: 0.25204 | val_0_rmse: 0.48555 | val_1_rmse: 0.51116 |  0:00:27s
epoch 75 | loss: 0.25941 | val_0_rmse: 0.48244 | val_1_rmse: 0.5089  |  0:00:27s
epoch 76 | loss: 0.26193 | val_0_rmse: 0.50192 | val_1_rmse: 0.52753 |  0:00:27s
epoch 77 | loss: 0.26418 | val_0_rmse: 0.48826 | val_1_rmse: 0.51381 |  0:00:28s
epoch 78 | loss: 0.25389 | val_0_rmse: 0.49258 | val_1_rmse: 0.53962 |  0:00:28s
epoch 79 | loss: 0.25361 | val_0_rmse: 0.48129 | val_1_rmse: 0.51438 |  0:00:28s
epoch 80 | loss: 0.25003 | val_0_rmse: 0.47591 | val_1_rmse: 0.51528 |  0:00:29s
epoch 81 | loss: 0.25053 | val_0_rmse: 0.47945 | val_1_rmse: 0.51391 |  0:00:29s
epoch 82 | loss: 0.25187 | val_0_rmse: 0.4785  | val_1_rmse: 0.50889 |  0:00:30s
epoch 83 | loss: 0.24405 | val_0_rmse: 0.48605 | val_1_rmse: 0.5075  |  0:00:30s
epoch 84 | loss: 0.24097 | val_0_rmse: 0.49726 | val_1_rmse: 0.53334 |  0:00:30s
epoch 85 | loss: 0.25022 | val_0_rmse: 0.49669 | val_1_rmse: 0.52552 |  0:00:31s
epoch 86 | loss: 0.25823 | val_0_rmse: 0.47907 | val_1_rmse: 0.51391 |  0:00:31s
epoch 87 | loss: 0.24975 | val_0_rmse: 0.50586 | val_1_rmse: 0.55144 |  0:00:31s
epoch 88 | loss: 0.25765 | val_0_rmse: 0.48187 | val_1_rmse: 0.51732 |  0:00:32s
epoch 89 | loss: 0.24868 | val_0_rmse: 0.48343 | val_1_rmse: 0.51938 |  0:00:32s
epoch 90 | loss: 0.25244 | val_0_rmse: 0.48252 | val_1_rmse: 0.50812 |  0:00:32s
epoch 91 | loss: 0.24899 | val_0_rmse: 0.48578 | val_1_rmse: 0.52107 |  0:00:33s
epoch 92 | loss: 0.25888 | val_0_rmse: 0.49286 | val_1_rmse: 0.52083 |  0:00:33s
epoch 93 | loss: 0.26238 | val_0_rmse: 0.50494 | val_1_rmse: 0.54317 |  0:00:34s
epoch 94 | loss: 0.25787 | val_0_rmse: 0.47785 | val_1_rmse: 0.5194  |  0:00:34s
epoch 95 | loss: 0.25602 | val_0_rmse: 0.4766  | val_1_rmse: 0.51463 |  0:00:34s
epoch 96 | loss: 0.25063 | val_0_rmse: 0.48434 | val_1_rmse: 0.52433 |  0:00:35s
epoch 97 | loss: 0.25182 | val_0_rmse: 0.47266 | val_1_rmse: 0.51053 |  0:00:35s
epoch 98 | loss: 0.23965 | val_0_rmse: 0.4677  | val_1_rmse: 0.51392 |  0:00:35s
epoch 99 | loss: 0.24161 | val_0_rmse: 0.4757  | val_1_rmse: 0.51761 |  0:00:36s
epoch 100| loss: 0.25613 | val_0_rmse: 0.4807  | val_1_rmse: 0.51718 |  0:00:36s
epoch 101| loss: 0.25767 | val_0_rmse: 0.47423 | val_1_rmse: 0.51691 |  0:00:36s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.49908
Best weights from best epoch are automatically used!
ended training at: 10:59:03
Feature importance:
[('Area', 0.2493628296979751), ('Baths', 0.015124047824689341), ('Beds', 0.1812906314976709), ('Latitude', 0.18709601802305575), ('Longitude', 0.2792233362034017), ('Month', 0.007107295555429064), ('Year', 0.08079584119777813)]
Mean squared error is of 20407474415.891857
Mean absolute error:104644.97506854414
MAPE:0.18775509368052412
R2 score:0.7673458284118723
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:59:03
epoch 0  | loss: 1.66546 | val_0_rmse: 1.09746 | val_1_rmse: 1.18938 |  0:00:00s
epoch 1  | loss: 0.85961 | val_0_rmse: 1.12471 | val_1_rmse: 1.11615 |  0:00:00s
epoch 2  | loss: 0.7192  | val_0_rmse: 0.94357 | val_1_rmse: 0.9729  |  0:00:00s
epoch 3  | loss: 0.59093 | val_0_rmse: 0.81565 | val_1_rmse: 0.84634 |  0:00:00s
epoch 4  | loss: 0.52706 | val_0_rmse: 0.82247 | val_1_rmse: 0.86011 |  0:00:00s
epoch 5  | loss: 0.51625 | val_0_rmse: 0.79757 | val_1_rmse: 0.81538 |  0:00:00s
epoch 6  | loss: 0.50097 | val_0_rmse: 0.74387 | val_1_rmse: 0.73792 |  0:00:00s
epoch 7  | loss: 0.4934  | val_0_rmse: 0.72689 | val_1_rmse: 0.69928 |  0:00:01s
epoch 8  | loss: 0.49185 | val_0_rmse: 0.71625 | val_1_rmse: 0.69599 |  0:00:01s
epoch 9  | loss: 0.48431 | val_0_rmse: 0.71145 | val_1_rmse: 0.70899 |  0:00:01s
epoch 10 | loss: 0.48104 | val_0_rmse: 0.69825 | val_1_rmse: 0.71199 |  0:00:01s
epoch 11 | loss: 0.47266 | val_0_rmse: 0.6984  | val_1_rmse: 0.70752 |  0:00:01s
epoch 12 | loss: 0.46319 | val_0_rmse: 0.67795 | val_1_rmse: 0.69685 |  0:00:01s
epoch 13 | loss: 0.44888 | val_0_rmse: 0.68284 | val_1_rmse: 0.71337 |  0:00:01s
epoch 14 | loss: 0.45565 | val_0_rmse: 0.71259 | val_1_rmse: 0.73053 |  0:00:02s
epoch 15 | loss: 0.44947 | val_0_rmse: 0.70052 | val_1_rmse: 0.69637 |  0:00:02s
epoch 16 | loss: 0.43782 | val_0_rmse: 0.67393 | val_1_rmse: 0.69706 |  0:00:02s
epoch 17 | loss: 0.44602 | val_0_rmse: 0.67019 | val_1_rmse: 0.71266 |  0:00:02s
epoch 18 | loss: 0.42387 | val_0_rmse: 0.66406 | val_1_rmse: 0.71054 |  0:00:02s
epoch 19 | loss: 0.43055 | val_0_rmse: 0.68247 | val_1_rmse: 0.72097 |  0:00:02s
epoch 20 | loss: 0.43072 | val_0_rmse: 0.67439 | val_1_rmse: 0.71872 |  0:00:02s
epoch 21 | loss: 0.46875 | val_0_rmse: 0.66489 | val_1_rmse: 0.69321 |  0:00:02s
epoch 22 | loss: 0.44333 | val_0_rmse: 0.66416 | val_1_rmse: 0.68369 |  0:00:03s
epoch 23 | loss: 0.42802 | val_0_rmse: 0.6642  | val_1_rmse: 0.70062 |  0:00:03s
epoch 24 | loss: 0.42574 | val_0_rmse: 0.65788 | val_1_rmse: 0.70956 |  0:00:03s
epoch 25 | loss: 0.4219  | val_0_rmse: 0.63772 | val_1_rmse: 0.67628 |  0:00:03s
epoch 26 | loss: 0.42549 | val_0_rmse: 0.64517 | val_1_rmse: 0.65988 |  0:00:03s
epoch 27 | loss: 0.43211 | val_0_rmse: 0.64799 | val_1_rmse: 0.66634 |  0:00:03s
epoch 28 | loss: 0.41887 | val_0_rmse: 0.64455 | val_1_rmse: 0.67514 |  0:00:03s
epoch 29 | loss: 0.41859 | val_0_rmse: 0.64172 | val_1_rmse: 0.67668 |  0:00:04s
epoch 30 | loss: 0.41215 | val_0_rmse: 0.63957 | val_1_rmse: 0.67134 |  0:00:04s
epoch 31 | loss: 0.4054  | val_0_rmse: 0.63818 | val_1_rmse: 0.68919 |  0:00:04s
epoch 32 | loss: 0.41377 | val_0_rmse: 0.63357 | val_1_rmse: 0.69103 |  0:00:04s
epoch 33 | loss: 0.40481 | val_0_rmse: 0.63939 | val_1_rmse: 0.68775 |  0:00:04s
epoch 34 | loss: 0.40294 | val_0_rmse: 0.63232 | val_1_rmse: 0.6813  |  0:00:04s
epoch 35 | loss: 0.4118  | val_0_rmse: 0.63389 | val_1_rmse: 0.68881 |  0:00:04s
epoch 36 | loss: 0.41131 | val_0_rmse: 0.63212 | val_1_rmse: 0.69317 |  0:00:04s
epoch 37 | loss: 0.4     | val_0_rmse: 0.62945 | val_1_rmse: 0.69603 |  0:00:05s
epoch 38 | loss: 0.41206 | val_0_rmse: 0.63242 | val_1_rmse: 0.68882 |  0:00:05s
epoch 39 | loss: 0.40674 | val_0_rmse: 0.62555 | val_1_rmse: 0.69057 |  0:00:05s
epoch 40 | loss: 0.4013  | val_0_rmse: 0.62536 | val_1_rmse: 0.70638 |  0:00:05s
epoch 41 | loss: 0.41407 | val_0_rmse: 0.62939 | val_1_rmse: 0.70234 |  0:00:05s
epoch 42 | loss: 0.40633 | val_0_rmse: 0.63586 | val_1_rmse: 0.69174 |  0:00:05s
epoch 43 | loss: 0.41339 | val_0_rmse: 0.62992 | val_1_rmse: 0.6778  |  0:00:05s
epoch 44 | loss: 0.41355 | val_0_rmse: 0.62756 | val_1_rmse: 0.68189 |  0:00:06s
epoch 45 | loss: 0.3965  | val_0_rmse: 0.62792 | val_1_rmse: 0.70189 |  0:00:06s
epoch 46 | loss: 0.41865 | val_0_rmse: 0.63403 | val_1_rmse: 0.70309 |  0:00:06s
epoch 47 | loss: 0.40731 | val_0_rmse: 0.63234 | val_1_rmse: 0.68529 |  0:00:06s
epoch 48 | loss: 0.41124 | val_0_rmse: 0.63829 | val_1_rmse: 0.69444 |  0:00:06s
epoch 49 | loss: 0.41989 | val_0_rmse: 0.62959 | val_1_rmse: 0.69341 |  0:00:06s
epoch 50 | loss: 0.40017 | val_0_rmse: 0.64467 | val_1_rmse: 0.70727 |  0:00:06s
epoch 51 | loss: 0.41247 | val_0_rmse: 0.63188 | val_1_rmse: 0.70136 |  0:00:06s
epoch 52 | loss: 0.39846 | val_0_rmse: 0.6237  | val_1_rmse: 0.67517 |  0:00:07s
epoch 53 | loss: 0.39735 | val_0_rmse: 0.6327  | val_1_rmse: 0.66605 |  0:00:07s
epoch 54 | loss: 0.38897 | val_0_rmse: 0.62601 | val_1_rmse: 0.66973 |  0:00:07s
epoch 55 | loss: 0.39433 | val_0_rmse: 0.61735 | val_1_rmse: 0.68902 |  0:00:07s
epoch 56 | loss: 0.3933  | val_0_rmse: 0.61772 | val_1_rmse: 0.69238 |  0:00:07s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.65988
Best weights from best epoch are automatically used!
ended training at: 10:59:11
Feature importance:
[('Area', 0.286507239732549), ('Baths', 0.19351253980683616), ('Beds', 0.16039050740457708), ('Latitude', 0.17130648529136425), ('Longitude', 0.05843761782010903), ('Month', 0.1134199534810443), ('Year', 0.016425656463520177)]
Mean squared error is of 3394381561.0710745
Mean absolute error:42701.00571442308
MAPE:0.419945200479622
R2 score:0.5279617898926348
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:59:11
epoch 0  | loss: 1.64319 | val_0_rmse: 1.16394 | val_1_rmse: 0.93103 |  0:00:00s
epoch 1  | loss: 0.80062 | val_0_rmse: 0.90126 | val_1_rmse: 0.82521 |  0:00:00s
epoch 2  | loss: 0.6697  | val_0_rmse: 1.05445 | val_1_rmse: 1.02934 |  0:00:00s
epoch 3  | loss: 0.6195  | val_0_rmse: 0.84582 | val_1_rmse: 0.81462 |  0:00:00s
epoch 4  | loss: 0.55528 | val_0_rmse: 0.81717 | val_1_rmse: 0.80085 |  0:00:00s
epoch 5  | loss: 0.50677 | val_0_rmse: 0.7704  | val_1_rmse: 0.79888 |  0:00:00s
epoch 6  | loss: 0.49683 | val_0_rmse: 0.73532 | val_1_rmse: 0.74222 |  0:00:00s
epoch 7  | loss: 0.48293 | val_0_rmse: 0.72793 | val_1_rmse: 0.71638 |  0:00:01s
epoch 8  | loss: 0.46775 | val_0_rmse: 0.70271 | val_1_rmse: 0.68634 |  0:00:01s
epoch 9  | loss: 0.45548 | val_0_rmse: 0.67426 | val_1_rmse: 0.65656 |  0:00:01s
epoch 10 | loss: 0.45166 | val_0_rmse: 0.66265 | val_1_rmse: 0.64911 |  0:00:01s
epoch 11 | loss: 0.44087 | val_0_rmse: 0.6598  | val_1_rmse: 0.65067 |  0:00:01s
epoch 12 | loss: 0.44272 | val_0_rmse: 0.65522 | val_1_rmse: 0.65234 |  0:00:01s
epoch 13 | loss: 0.43602 | val_0_rmse: 0.66285 | val_1_rmse: 0.66359 |  0:00:01s
epoch 14 | loss: 0.43929 | val_0_rmse: 0.66304 | val_1_rmse: 0.67137 |  0:00:02s
epoch 15 | loss: 0.4312  | val_0_rmse: 0.65606 | val_1_rmse: 0.66997 |  0:00:02s
epoch 16 | loss: 0.43563 | val_0_rmse: 0.65127 | val_1_rmse: 0.65026 |  0:00:02s
epoch 17 | loss: 0.43821 | val_0_rmse: 0.64528 | val_1_rmse: 0.65799 |  0:00:02s
epoch 18 | loss: 0.41765 | val_0_rmse: 0.64953 | val_1_rmse: 0.68536 |  0:00:02s
epoch 19 | loss: 0.43026 | val_0_rmse: 0.64504 | val_1_rmse: 0.68463 |  0:00:02s
epoch 20 | loss: 0.41319 | val_0_rmse: 0.63745 | val_1_rmse: 0.67721 |  0:00:02s
epoch 21 | loss: 0.42203 | val_0_rmse: 0.63461 | val_1_rmse: 0.67556 |  0:00:02s
epoch 22 | loss: 0.41591 | val_0_rmse: 0.62926 | val_1_rmse: 0.68699 |  0:00:03s
epoch 23 | loss: 0.4205  | val_0_rmse: 0.64133 | val_1_rmse: 0.713   |  0:00:03s
epoch 24 | loss: 0.41437 | val_0_rmse: 0.63631 | val_1_rmse: 0.69146 |  0:00:03s
epoch 25 | loss: 0.40178 | val_0_rmse: 0.62974 | val_1_rmse: 0.65692 |  0:00:03s
epoch 26 | loss: 0.40771 | val_0_rmse: 0.63047 | val_1_rmse: 0.66525 |  0:00:03s
epoch 27 | loss: 0.42972 | val_0_rmse: 0.62923 | val_1_rmse: 0.67262 |  0:00:03s
epoch 28 | loss: 0.40319 | val_0_rmse: 0.63047 | val_1_rmse: 0.66121 |  0:00:03s
epoch 29 | loss: 0.40077 | val_0_rmse: 0.62608 | val_1_rmse: 0.67377 |  0:00:03s
epoch 30 | loss: 0.40321 | val_0_rmse: 0.62597 | val_1_rmse: 0.6953  |  0:00:04s
epoch 31 | loss: 0.40677 | val_0_rmse: 0.61697 | val_1_rmse: 0.66342 |  0:00:04s
epoch 32 | loss: 0.40408 | val_0_rmse: 0.6239  | val_1_rmse: 0.65352 |  0:00:04s
epoch 33 | loss: 0.41112 | val_0_rmse: 0.62581 | val_1_rmse: 0.66427 |  0:00:04s
epoch 34 | loss: 0.39979 | val_0_rmse: 0.6236  | val_1_rmse: 0.68045 |  0:00:04s
epoch 35 | loss: 0.39964 | val_0_rmse: 0.62452 | val_1_rmse: 0.66623 |  0:00:04s
epoch 36 | loss: 0.40131 | val_0_rmse: 0.61987 | val_1_rmse: 0.65281 |  0:00:04s
epoch 37 | loss: 0.40177 | val_0_rmse: 0.61705 | val_1_rmse: 0.66089 |  0:00:05s
epoch 38 | loss: 0.39374 | val_0_rmse: 0.61439 | val_1_rmse: 0.66337 |  0:00:05s
epoch 39 | loss: 0.40721 | val_0_rmse: 0.62001 | val_1_rmse: 0.67132 |  0:00:05s
epoch 40 | loss: 0.38675 | val_0_rmse: 0.61707 | val_1_rmse: 0.68022 |  0:00:05s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.64911
Best weights from best epoch are automatically used!
ended training at: 10:59:17
Feature importance:
[('Area', 0.2701888662774777), ('Baths', 0.2433009137537709), ('Beds', 0.07693538870719914), ('Latitude', 0.15460765001573468), ('Longitude', 0.1433281668394488), ('Month', 0.10542031521736363), ('Year', 0.0062186991890051885)]
Mean squared error is of 3435049075.9165897
Mean absolute error:44752.90250164835
MAPE:0.43395857068691807
R2 score:0.5186879712149257
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:59:17
epoch 0  | loss: 1.73473 | val_0_rmse: 1.21129 | val_1_rmse: 1.17127 |  0:00:00s
epoch 1  | loss: 0.85682 | val_0_rmse: 1.13451 | val_1_rmse: 1.07485 |  0:00:00s
epoch 2  | loss: 0.55672 | val_0_rmse: 0.99219 | val_1_rmse: 0.98961 |  0:00:00s
epoch 3  | loss: 0.53152 | val_0_rmse: 0.78072 | val_1_rmse: 0.80399 |  0:00:00s
epoch 4  | loss: 0.50093 | val_0_rmse: 0.76655 | val_1_rmse: 0.79979 |  0:00:00s
epoch 5  | loss: 0.48115 | val_0_rmse: 0.74603 | val_1_rmse: 0.77826 |  0:00:00s
epoch 6  | loss: 0.47231 | val_0_rmse: 0.76269 | val_1_rmse: 0.78244 |  0:00:00s
epoch 7  | loss: 0.4723  | val_0_rmse: 0.71774 | val_1_rmse: 0.71165 |  0:00:01s
epoch 8  | loss: 0.46043 | val_0_rmse: 0.7017  | val_1_rmse: 0.69839 |  0:00:01s
epoch 9  | loss: 0.45083 | val_0_rmse: 0.68359 | val_1_rmse: 0.696   |  0:00:01s
epoch 10 | loss: 0.44498 | val_0_rmse: 0.6799  | val_1_rmse: 0.69808 |  0:00:01s
epoch 11 | loss: 0.43926 | val_0_rmse: 0.68166 | val_1_rmse: 0.69941 |  0:00:01s
epoch 12 | loss: 0.43856 | val_0_rmse: 0.66318 | val_1_rmse: 0.68419 |  0:00:01s
epoch 13 | loss: 0.42468 | val_0_rmse: 0.65859 | val_1_rmse: 0.67034 |  0:00:01s
epoch 14 | loss: 0.43875 | val_0_rmse: 0.64688 | val_1_rmse: 0.67276 |  0:00:02s
epoch 15 | loss: 0.41988 | val_0_rmse: 0.64482 | val_1_rmse: 0.67692 |  0:00:02s
epoch 16 | loss: 0.4239  | val_0_rmse: 0.64405 | val_1_rmse: 0.67101 |  0:00:02s
epoch 17 | loss: 0.42524 | val_0_rmse: 0.64311 | val_1_rmse: 0.6775  |  0:00:02s
epoch 18 | loss: 0.42437 | val_0_rmse: 0.63534 | val_1_rmse: 0.67278 |  0:00:02s
epoch 19 | loss: 0.41664 | val_0_rmse: 0.63457 | val_1_rmse: 0.66798 |  0:00:02s
epoch 20 | loss: 0.41124 | val_0_rmse: 0.6396  | val_1_rmse: 0.67637 |  0:00:02s
epoch 21 | loss: 0.41807 | val_0_rmse: 0.65177 | val_1_rmse: 0.67263 |  0:00:02s
epoch 22 | loss: 0.42128 | val_0_rmse: 0.65097 | val_1_rmse: 0.68022 |  0:00:03s
epoch 23 | loss: 0.43354 | val_0_rmse: 0.6454  | val_1_rmse: 0.68229 |  0:00:03s
epoch 24 | loss: 0.42753 | val_0_rmse: 0.64057 | val_1_rmse: 0.67811 |  0:00:03s
epoch 25 | loss: 0.4048  | val_0_rmse: 0.64325 | val_1_rmse: 0.67923 |  0:00:03s
epoch 26 | loss: 0.41395 | val_0_rmse: 0.64311 | val_1_rmse: 0.68404 |  0:00:03s
epoch 27 | loss: 0.41609 | val_0_rmse: 0.64042 | val_1_rmse: 0.68272 |  0:00:03s
epoch 28 | loss: 0.4119  | val_0_rmse: 0.63753 | val_1_rmse: 0.67295 |  0:00:03s
epoch 29 | loss: 0.41403 | val_0_rmse: 0.63364 | val_1_rmse: 0.66116 |  0:00:04s
epoch 30 | loss: 0.40621 | val_0_rmse: 0.63483 | val_1_rmse: 0.66256 |  0:00:04s
epoch 31 | loss: 0.39655 | val_0_rmse: 0.62765 | val_1_rmse: 0.66174 |  0:00:04s
epoch 32 | loss: 0.40553 | val_0_rmse: 0.62389 | val_1_rmse: 0.6519  |  0:00:04s
epoch 33 | loss: 0.39146 | val_0_rmse: 0.62185 | val_1_rmse: 0.64896 |  0:00:04s
epoch 34 | loss: 0.38895 | val_0_rmse: 0.60904 | val_1_rmse: 0.64272 |  0:00:04s
epoch 35 | loss: 0.37559 | val_0_rmse: 0.61042 | val_1_rmse: 0.63955 |  0:00:04s
epoch 36 | loss: 0.38077 | val_0_rmse: 0.62207 | val_1_rmse: 0.6531  |  0:00:04s
epoch 37 | loss: 0.40262 | val_0_rmse: 0.61063 | val_1_rmse: 0.64955 |  0:00:05s
epoch 38 | loss: 0.37947 | val_0_rmse: 0.61196 | val_1_rmse: 0.64162 |  0:00:05s
epoch 39 | loss: 0.3904  | val_0_rmse: 0.61834 | val_1_rmse: 0.63478 |  0:00:05s
epoch 40 | loss: 0.38246 | val_0_rmse: 0.62554 | val_1_rmse: 0.65857 |  0:00:05s
epoch 41 | loss: 0.37531 | val_0_rmse: 0.60464 | val_1_rmse: 0.64172 |  0:00:05s
epoch 42 | loss: 0.37608 | val_0_rmse: 0.60208 | val_1_rmse: 0.61964 |  0:00:05s
epoch 43 | loss: 0.36039 | val_0_rmse: 0.60774 | val_1_rmse: 0.62121 |  0:00:05s
epoch 44 | loss: 0.35514 | val_0_rmse: 0.58032 | val_1_rmse: 0.61119 |  0:00:06s
epoch 45 | loss: 0.35752 | val_0_rmse: 0.59982 | val_1_rmse: 0.62538 |  0:00:06s
epoch 46 | loss: 0.35694 | val_0_rmse: 0.60236 | val_1_rmse: 0.60742 |  0:00:06s
epoch 47 | loss: 0.35101 | val_0_rmse: 0.60823 | val_1_rmse: 0.60857 |  0:00:06s
epoch 48 | loss: 0.3567  | val_0_rmse: 0.61543 | val_1_rmse: 0.62003 |  0:00:06s
epoch 49 | loss: 0.3444  | val_0_rmse: 0.61532 | val_1_rmse: 0.62179 |  0:00:06s
epoch 50 | loss: 0.36983 | val_0_rmse: 0.59922 | val_1_rmse: 0.60952 |  0:00:06s
epoch 51 | loss: 0.36101 | val_0_rmse: 0.61147 | val_1_rmse: 0.62057 |  0:00:07s
epoch 52 | loss: 0.3618  | val_0_rmse: 0.60812 | val_1_rmse: 0.61778 |  0:00:07s
epoch 53 | loss: 0.36974 | val_0_rmse: 0.59039 | val_1_rmse: 0.59931 |  0:00:07s
epoch 54 | loss: 0.35582 | val_0_rmse: 0.58637 | val_1_rmse: 0.5842  |  0:00:07s
epoch 55 | loss: 0.35167 | val_0_rmse: 0.58495 | val_1_rmse: 0.59497 |  0:00:07s
epoch 56 | loss: 0.34343 | val_0_rmse: 0.58132 | val_1_rmse: 0.59783 |  0:00:07s
epoch 57 | loss: 0.34148 | val_0_rmse: 0.57492 | val_1_rmse: 0.57874 |  0:00:07s
epoch 58 | loss: 0.33855 | val_0_rmse: 0.64842 | val_1_rmse: 0.64996 |  0:00:07s
epoch 59 | loss: 0.34376 | val_0_rmse: 0.68782 | val_1_rmse: 0.69267 |  0:00:08s
epoch 60 | loss: 0.3464  | val_0_rmse: 0.6881  | val_1_rmse: 0.70379 |  0:00:08s
epoch 61 | loss: 0.33789 | val_0_rmse: 0.63005 | val_1_rmse: 0.6449  |  0:00:08s
epoch 62 | loss: 0.33081 | val_0_rmse: 0.5774  | val_1_rmse: 0.59192 |  0:00:08s
epoch 63 | loss: 0.33187 | val_0_rmse: 0.61856 | val_1_rmse: 0.63451 |  0:00:08s
epoch 64 | loss: 0.32621 | val_0_rmse: 0.71609 | val_1_rmse: 0.73943 |  0:00:08s
epoch 65 | loss: 0.32932 | val_0_rmse: 0.72705 | val_1_rmse: 0.74876 |  0:00:08s
epoch 66 | loss: 0.33104 | val_0_rmse: 0.70079 | val_1_rmse: 0.71492 |  0:00:09s
epoch 67 | loss: 0.32408 | val_0_rmse: 0.74769 | val_1_rmse: 0.7672  |  0:00:09s
epoch 68 | loss: 0.33313 | val_0_rmse: 0.66763 | val_1_rmse: 0.69092 |  0:00:09s
epoch 69 | loss: 0.3332  | val_0_rmse: 0.64976 | val_1_rmse: 0.67269 |  0:00:09s
epoch 70 | loss: 0.316   | val_0_rmse: 0.64645 | val_1_rmse: 0.66698 |  0:00:09s
epoch 71 | loss: 0.32314 | val_0_rmse: 0.59545 | val_1_rmse: 0.61522 |  0:00:09s
epoch 72 | loss: 0.32931 | val_0_rmse: 0.55379 | val_1_rmse: 0.58317 |  0:00:09s
epoch 73 | loss: 0.33295 | val_0_rmse: 0.60595 | val_1_rmse: 0.64462 |  0:00:09s
epoch 74 | loss: 0.33285 | val_0_rmse: 0.60484 | val_1_rmse: 0.62963 |  0:00:10s
epoch 75 | loss: 0.33549 | val_0_rmse: 0.57896 | val_1_rmse: 0.60039 |  0:00:10s
epoch 76 | loss: 0.32881 | val_0_rmse: 0.56315 | val_1_rmse: 0.60188 |  0:00:10s
epoch 77 | loss: 0.329   | val_0_rmse: 0.55025 | val_1_rmse: 0.57177 |  0:00:10s
epoch 78 | loss: 0.3245  | val_0_rmse: 0.56738 | val_1_rmse: 0.58109 |  0:00:10s
epoch 79 | loss: 0.32402 | val_0_rmse: 0.58082 | val_1_rmse: 0.6195  |  0:00:10s
epoch 80 | loss: 0.3307  | val_0_rmse: 0.56384 | val_1_rmse: 0.60091 |  0:00:10s
epoch 81 | loss: 0.30712 | val_0_rmse: 0.56911 | val_1_rmse: 0.57906 |  0:00:10s
epoch 82 | loss: 0.33381 | val_0_rmse: 0.55225 | val_1_rmse: 0.57398 |  0:00:11s
epoch 83 | loss: 0.31128 | val_0_rmse: 0.55277 | val_1_rmse: 0.59039 |  0:00:11s
epoch 84 | loss: 0.32276 | val_0_rmse: 0.55793 | val_1_rmse: 0.57181 |  0:00:11s
epoch 85 | loss: 0.3137  | val_0_rmse: 0.55046 | val_1_rmse: 0.56335 |  0:00:11s
epoch 86 | loss: 0.31293 | val_0_rmse: 0.57177 | val_1_rmse: 0.60076 |  0:00:11s
epoch 87 | loss: 0.32001 | val_0_rmse: 0.57281 | val_1_rmse: 0.59139 |  0:00:11s
epoch 88 | loss: 0.32049 | val_0_rmse: 0.54601 | val_1_rmse: 0.57093 |  0:00:11s
epoch 89 | loss: 0.30645 | val_0_rmse: 0.54994 | val_1_rmse: 0.59303 |  0:00:12s
epoch 90 | loss: 0.32434 | val_0_rmse: 0.54668 | val_1_rmse: 0.57146 |  0:00:12s
epoch 91 | loss: 0.30447 | val_0_rmse: 0.57045 | val_1_rmse: 0.58681 |  0:00:12s
epoch 92 | loss: 0.31654 | val_0_rmse: 0.61597 | val_1_rmse: 0.64595 |  0:00:12s
epoch 93 | loss: 0.30919 | val_0_rmse: 0.65064 | val_1_rmse: 0.68853 |  0:00:12s
epoch 94 | loss: 0.29599 | val_0_rmse: 0.57972 | val_1_rmse: 0.61526 |  0:00:12s
epoch 95 | loss: 0.30925 | val_0_rmse: 0.56693 | val_1_rmse: 0.60071 |  0:00:12s
epoch 96 | loss: 0.29722 | val_0_rmse: 0.59489 | val_1_rmse: 0.63129 |  0:00:12s
epoch 97 | loss: 0.30093 | val_0_rmse: 0.63786 | val_1_rmse: 0.67674 |  0:00:13s
epoch 98 | loss: 0.294   | val_0_rmse: 0.64562 | val_1_rmse: 0.67802 |  0:00:13s
epoch 99 | loss: 0.30366 | val_0_rmse: 0.62137 | val_1_rmse: 0.65473 |  0:00:13s
epoch 100| loss: 0.2938  | val_0_rmse: 0.58874 | val_1_rmse: 0.62436 |  0:00:13s
epoch 101| loss: 0.30301 | val_0_rmse: 0.54405 | val_1_rmse: 0.57702 |  0:00:13s
epoch 102| loss: 0.30294 | val_0_rmse: 0.53566 | val_1_rmse: 0.57281 |  0:00:13s
epoch 103| loss: 0.29763 | val_0_rmse: 0.5434  | val_1_rmse: 0.58396 |  0:00:13s
epoch 104| loss: 0.29248 | val_0_rmse: 0.5369  | val_1_rmse: 0.57451 |  0:00:14s
epoch 105| loss: 0.30791 | val_0_rmse: 0.53056 | val_1_rmse: 0.56985 |  0:00:14s
epoch 106| loss: 0.29649 | val_0_rmse: 0.56626 | val_1_rmse: 0.6211  |  0:00:14s
epoch 107| loss: 0.30764 | val_0_rmse: 0.61705 | val_1_rmse: 0.67079 |  0:00:14s
epoch 108| loss: 0.28709 | val_0_rmse: 0.60515 | val_1_rmse: 0.65503 |  0:00:14s
epoch 109| loss: 0.29328 | val_0_rmse: 0.56686 | val_1_rmse: 0.62343 |  0:00:14s
epoch 110| loss: 0.28654 | val_0_rmse: 0.5351  | val_1_rmse: 0.58238 |  0:00:14s
epoch 111| loss: 0.29698 | val_0_rmse: 0.53393 | val_1_rmse: 0.57024 |  0:00:14s
epoch 112| loss: 0.30161 | val_0_rmse: 0.53459 | val_1_rmse: 0.57405 |  0:00:15s
epoch 113| loss: 0.29946 | val_0_rmse: 0.54937 | val_1_rmse: 0.57995 |  0:00:15s
epoch 114| loss: 0.30397 | val_0_rmse: 0.56705 | val_1_rmse: 0.60045 |  0:00:15s
epoch 115| loss: 0.30171 | val_0_rmse: 0.54662 | val_1_rmse: 0.58344 |  0:00:15s

Early stopping occured at epoch 115 with best_epoch = 85 and best_val_1_rmse = 0.56335
Best weights from best epoch are automatically used!
ended training at: 10:59:32
Feature importance:
[('Area', 0.24578237423550064), ('Baths', 0.16763047944345402), ('Beds', 0.0), ('Latitude', 0.4970742134710622), ('Longitude', 0.04898286580041649), ('Month', 0.040530067049566666), ('Year', 0.0)]
Mean squared error is of 2853201928.804577
Mean absolute error:38588.585726510995
MAPE:0.35157260489220077
R2 score:0.6160523377404286
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:59:32
epoch 0  | loss: 1.64591 | val_0_rmse: 1.1533  | val_1_rmse: 1.15104 |  0:00:00s
epoch 1  | loss: 0.90259 | val_0_rmse: 1.26776 | val_1_rmse: 1.10999 |  0:00:00s
epoch 2  | loss: 0.65423 | val_0_rmse: 1.02323 | val_1_rmse: 1.03736 |  0:00:00s
epoch 3  | loss: 0.53903 | val_0_rmse: 1.00943 | val_1_rmse: 0.99626 |  0:00:00s
epoch 4  | loss: 0.5356  | val_0_rmse: 0.86598 | val_1_rmse: 0.88149 |  0:00:00s
epoch 5  | loss: 0.51511 | val_0_rmse: 0.84697 | val_1_rmse: 0.84205 |  0:00:00s
epoch 6  | loss: 0.49897 | val_0_rmse: 0.76508 | val_1_rmse: 0.74254 |  0:00:00s
epoch 7  | loss: 0.4754  | val_0_rmse: 0.73364 | val_1_rmse: 0.71089 |  0:00:01s
epoch 8  | loss: 0.47074 | val_0_rmse: 0.72523 | val_1_rmse: 0.69376 |  0:00:01s
epoch 9  | loss: 0.45142 | val_0_rmse: 0.72544 | val_1_rmse: 0.69853 |  0:00:01s
epoch 10 | loss: 0.46643 | val_0_rmse: 0.70611 | val_1_rmse: 0.68698 |  0:00:01s
epoch 11 | loss: 0.43418 | val_0_rmse: 0.66986 | val_1_rmse: 0.67204 |  0:00:01s
epoch 12 | loss: 0.44902 | val_0_rmse: 0.64956 | val_1_rmse: 0.64858 |  0:00:01s
epoch 13 | loss: 0.4325  | val_0_rmse: 0.64874 | val_1_rmse: 0.65206 |  0:00:01s
epoch 14 | loss: 0.42957 | val_0_rmse: 0.63628 | val_1_rmse: 0.65216 |  0:00:02s
epoch 15 | loss: 0.41926 | val_0_rmse: 0.63789 | val_1_rmse: 0.65199 |  0:00:02s
epoch 16 | loss: 0.41915 | val_0_rmse: 0.63308 | val_1_rmse: 0.62878 |  0:00:02s
epoch 17 | loss: 0.42324 | val_0_rmse: 0.6487  | val_1_rmse: 0.62468 |  0:00:02s
epoch 18 | loss: 0.41139 | val_0_rmse: 0.63227 | val_1_rmse: 0.62216 |  0:00:02s
epoch 19 | loss: 0.41527 | val_0_rmse: 0.64294 | val_1_rmse: 0.65787 |  0:00:02s
epoch 20 | loss: 0.398   | val_0_rmse: 0.63013 | val_1_rmse: 0.6485  |  0:00:02s
epoch 21 | loss: 0.40568 | val_0_rmse: 0.627   | val_1_rmse: 0.64216 |  0:00:02s
epoch 22 | loss: 0.40435 | val_0_rmse: 0.61692 | val_1_rmse: 0.62961 |  0:00:03s
epoch 23 | loss: 0.39338 | val_0_rmse: 0.61635 | val_1_rmse: 0.6242  |  0:00:03s
epoch 24 | loss: 0.39232 | val_0_rmse: 0.62693 | val_1_rmse: 0.62835 |  0:00:03s
epoch 25 | loss: 0.38751 | val_0_rmse: 0.61729 | val_1_rmse: 0.62119 |  0:00:03s
epoch 26 | loss: 0.38163 | val_0_rmse: 0.61032 | val_1_rmse: 0.62934 |  0:00:03s
epoch 27 | loss: 0.37993 | val_0_rmse: 0.60908 | val_1_rmse: 0.62894 |  0:00:03s
epoch 28 | loss: 0.37593 | val_0_rmse: 0.59835 | val_1_rmse: 0.6083  |  0:00:03s
epoch 29 | loss: 0.37332 | val_0_rmse: 0.60269 | val_1_rmse: 0.60926 |  0:00:04s
epoch 30 | loss: 0.37072 | val_0_rmse: 0.60269 | val_1_rmse: 0.61601 |  0:00:04s
epoch 31 | loss: 0.36866 | val_0_rmse: 0.60367 | val_1_rmse: 0.63133 |  0:00:04s
epoch 32 | loss: 0.36546 | val_0_rmse: 0.60628 | val_1_rmse: 0.63341 |  0:00:04s
epoch 33 | loss: 0.3585  | val_0_rmse: 0.59859 | val_1_rmse: 0.62686 |  0:00:04s
epoch 34 | loss: 0.35099 | val_0_rmse: 0.58705 | val_1_rmse: 0.6049  |  0:00:04s
epoch 35 | loss: 0.36337 | val_0_rmse: 0.5823  | val_1_rmse: 0.60174 |  0:00:04s
epoch 36 | loss: 0.35524 | val_0_rmse: 0.61367 | val_1_rmse: 0.63373 |  0:00:04s
epoch 37 | loss: 0.35585 | val_0_rmse: 0.58673 | val_1_rmse: 0.60024 |  0:00:05s
epoch 38 | loss: 0.34723 | val_0_rmse: 0.59497 | val_1_rmse: 0.59701 |  0:00:05s
epoch 39 | loss: 0.363   | val_0_rmse: 0.58716 | val_1_rmse: 0.59443 |  0:00:05s
epoch 40 | loss: 0.3504  | val_0_rmse: 0.57568 | val_1_rmse: 0.60413 |  0:00:05s
epoch 41 | loss: 0.33847 | val_0_rmse: 0.5794  | val_1_rmse: 0.62368 |  0:00:05s
epoch 42 | loss: 0.33219 | val_0_rmse: 0.57476 | val_1_rmse: 0.60883 |  0:00:05s
epoch 43 | loss: 0.34092 | val_0_rmse: 0.58161 | val_1_rmse: 0.59494 |  0:00:05s
epoch 44 | loss: 0.33221 | val_0_rmse: 0.5775  | val_1_rmse: 0.61546 |  0:00:06s
epoch 45 | loss: 0.33108 | val_0_rmse: 0.5756  | val_1_rmse: 0.62208 |  0:00:06s
epoch 46 | loss: 0.33185 | val_0_rmse: 0.57561 | val_1_rmse: 0.62581 |  0:00:06s
epoch 47 | loss: 0.3391  | val_0_rmse: 0.58644 | val_1_rmse: 0.63575 |  0:00:06s
epoch 48 | loss: 0.34045 | val_0_rmse: 0.58535 | val_1_rmse: 0.62234 |  0:00:06s
epoch 49 | loss: 0.34501 | val_0_rmse: 0.59515 | val_1_rmse: 0.61237 |  0:00:06s
epoch 50 | loss: 0.34329 | val_0_rmse: 0.57772 | val_1_rmse: 0.60705 |  0:00:06s
epoch 51 | loss: 0.33052 | val_0_rmse: 0.58221 | val_1_rmse: 0.59826 |  0:00:07s
epoch 52 | loss: 0.33239 | val_0_rmse: 0.57964 | val_1_rmse: 0.60372 |  0:00:07s
epoch 53 | loss: 0.33671 | val_0_rmse: 0.59321 | val_1_rmse: 0.63325 |  0:00:07s
epoch 54 | loss: 0.33613 | val_0_rmse: 0.59044 | val_1_rmse: 0.63406 |  0:00:07s
epoch 55 | loss: 0.33737 | val_0_rmse: 0.58739 | val_1_rmse: 0.60155 |  0:00:07s
epoch 56 | loss: 0.35129 | val_0_rmse: 0.60768 | val_1_rmse: 0.6029  |  0:00:07s
epoch 57 | loss: 0.34978 | val_0_rmse: 0.62177 | val_1_rmse: 0.63813 |  0:00:07s
epoch 58 | loss: 0.33931 | val_0_rmse: 0.57989 | val_1_rmse: 0.6032  |  0:00:07s
epoch 59 | loss: 0.33659 | val_0_rmse: 0.56739 | val_1_rmse: 0.60098 |  0:00:08s
epoch 60 | loss: 0.33008 | val_0_rmse: 0.56831 | val_1_rmse: 0.61018 |  0:00:08s
epoch 61 | loss: 0.32846 | val_0_rmse: 0.60032 | val_1_rmse: 0.61859 |  0:00:08s
epoch 62 | loss: 0.32678 | val_0_rmse: 0.58982 | val_1_rmse: 0.6093  |  0:00:08s
epoch 63 | loss: 0.32934 | val_0_rmse: 0.5618  | val_1_rmse: 0.60064 |  0:00:08s
epoch 64 | loss: 0.31941 | val_0_rmse: 0.57687 | val_1_rmse: 0.63134 |  0:00:08s
epoch 65 | loss: 0.32788 | val_0_rmse: 0.58    | val_1_rmse: 0.63521 |  0:00:08s
epoch 66 | loss: 0.33782 | val_0_rmse: 0.63    | val_1_rmse: 0.68561 |  0:00:08s
epoch 67 | loss: 0.32815 | val_0_rmse: 0.62676 | val_1_rmse: 0.67748 |  0:00:09s
epoch 68 | loss: 0.32032 | val_0_rmse: 0.5763  | val_1_rmse: 0.61858 |  0:00:09s
epoch 69 | loss: 0.31779 | val_0_rmse: 0.55533 | val_1_rmse: 0.58224 |  0:00:09s
epoch 70 | loss: 0.32287 | val_0_rmse: 0.55495 | val_1_rmse: 0.58525 |  0:00:09s
epoch 71 | loss: 0.32114 | val_0_rmse: 0.55705 | val_1_rmse: 0.59852 |  0:00:09s
epoch 72 | loss: 0.31749 | val_0_rmse: 0.56027 | val_1_rmse: 0.59241 |  0:00:09s
epoch 73 | loss: 0.32423 | val_0_rmse: 0.57304 | val_1_rmse: 0.59358 |  0:00:09s
epoch 74 | loss: 0.32475 | val_0_rmse: 0.57388 | val_1_rmse: 0.60089 |  0:00:10s
epoch 75 | loss: 0.31606 | val_0_rmse: 0.56658 | val_1_rmse: 0.60123 |  0:00:10s
epoch 76 | loss: 0.30807 | val_0_rmse: 0.55552 | val_1_rmse: 0.59031 |  0:00:10s
epoch 77 | loss: 0.30724 | val_0_rmse: 0.55271 | val_1_rmse: 0.59357 |  0:00:10s
epoch 78 | loss: 0.31708 | val_0_rmse: 0.55213 | val_1_rmse: 0.60395 |  0:00:10s
epoch 79 | loss: 0.31055 | val_0_rmse: 0.54694 | val_1_rmse: 0.58882 |  0:00:10s
epoch 80 | loss: 0.30915 | val_0_rmse: 0.55333 | val_1_rmse: 0.59237 |  0:00:10s
epoch 81 | loss: 0.31284 | val_0_rmse: 0.59092 | val_1_rmse: 0.63394 |  0:00:10s
epoch 82 | loss: 0.31159 | val_0_rmse: 0.58513 | val_1_rmse: 0.6204  |  0:00:11s
epoch 83 | loss: 0.31687 | val_0_rmse: 0.56609 | val_1_rmse: 0.60711 |  0:00:11s
epoch 84 | loss: 0.32393 | val_0_rmse: 0.56039 | val_1_rmse: 0.60438 |  0:00:11s
epoch 85 | loss: 0.32014 | val_0_rmse: 0.56223 | val_1_rmse: 0.59724 |  0:00:11s
epoch 86 | loss: 0.30679 | val_0_rmse: 0.54891 | val_1_rmse: 0.58815 |  0:00:11s
epoch 87 | loss: 0.31141 | val_0_rmse: 0.55192 | val_1_rmse: 0.59418 |  0:00:11s
epoch 88 | loss: 0.32154 | val_0_rmse: 0.56027 | val_1_rmse: 0.59008 |  0:00:11s
epoch 89 | loss: 0.31176 | val_0_rmse: 0.5611  | val_1_rmse: 0.59637 |  0:00:12s
epoch 90 | loss: 0.31192 | val_0_rmse: 0.55621 | val_1_rmse: 0.59448 |  0:00:12s
epoch 91 | loss: 0.30682 | val_0_rmse: 0.5431  | val_1_rmse: 0.58448 |  0:00:12s
epoch 92 | loss: 0.30298 | val_0_rmse: 0.54391 | val_1_rmse: 0.59279 |  0:00:12s
epoch 93 | loss: 0.30053 | val_0_rmse: 0.54086 | val_1_rmse: 0.57775 |  0:00:12s
epoch 94 | loss: 0.30722 | val_0_rmse: 0.53995 | val_1_rmse: 0.56794 |  0:00:12s
epoch 95 | loss: 0.31738 | val_0_rmse: 0.54626 | val_1_rmse: 0.59741 |  0:00:12s
epoch 96 | loss: 0.32237 | val_0_rmse: 0.56102 | val_1_rmse: 0.61822 |  0:00:12s
epoch 97 | loss: 0.32305 | val_0_rmse: 0.54049 | val_1_rmse: 0.58945 |  0:00:13s
epoch 98 | loss: 0.30758 | val_0_rmse: 0.56274 | val_1_rmse: 0.60981 |  0:00:13s
epoch 99 | loss: 0.30784 | val_0_rmse: 0.54402 | val_1_rmse: 0.59215 |  0:00:13s
epoch 100| loss: 0.28101 | val_0_rmse: 0.54837 | val_1_rmse: 0.61918 |  0:00:13s
epoch 101| loss: 0.29689 | val_0_rmse: 0.55805 | val_1_rmse: 0.6163  |  0:00:13s
epoch 102| loss: 0.31367 | val_0_rmse: 0.57198 | val_1_rmse: 0.59923 |  0:00:13s
epoch 103| loss: 0.29147 | val_0_rmse: 0.57926 | val_1_rmse: 0.60881 |  0:00:13s
epoch 104| loss: 0.28689 | val_0_rmse: 0.56026 | val_1_rmse: 0.58703 |  0:00:13s
epoch 105| loss: 0.29343 | val_0_rmse: 0.57852 | val_1_rmse: 0.61474 |  0:00:14s
epoch 106| loss: 0.27943 | val_0_rmse: 0.53234 | val_1_rmse: 0.59752 |  0:00:14s
epoch 107| loss: 0.28459 | val_0_rmse: 0.70238 | val_1_rmse: 0.74989 |  0:00:14s
epoch 108| loss: 0.29541 | val_0_rmse: 0.71241 | val_1_rmse: 0.75922 |  0:00:14s
epoch 109| loss: 0.29158 | val_0_rmse: 0.7053  | val_1_rmse: 0.77939 |  0:00:14s
epoch 110| loss: 0.28819 | val_0_rmse: 0.65404 | val_1_rmse: 0.73506 |  0:00:14s
epoch 111| loss: 0.28182 | val_0_rmse: 0.62622 | val_1_rmse: 0.68041 |  0:00:14s
epoch 112| loss: 0.27659 | val_0_rmse: 0.66876 | val_1_rmse: 0.7032  |  0:00:15s
epoch 113| loss: 0.29399 | val_0_rmse: 0.64926 | val_1_rmse: 0.71251 |  0:00:15s
epoch 114| loss: 0.28245 | val_0_rmse: 0.59611 | val_1_rmse: 0.66245 |  0:00:15s
epoch 115| loss: 0.28585 | val_0_rmse: 0.52438 | val_1_rmse: 0.59691 |  0:00:15s
epoch 116| loss: 0.28775 | val_0_rmse: 0.52625 | val_1_rmse: 0.58687 |  0:00:15s
epoch 117| loss: 0.28266 | val_0_rmse: 0.51866 | val_1_rmse: 0.56282 |  0:00:15s
epoch 118| loss: 0.29068 | val_0_rmse: 0.5106  | val_1_rmse: 0.5779  |  0:00:15s
epoch 119| loss: 0.28195 | val_0_rmse: 0.54633 | val_1_rmse: 0.58498 |  0:00:16s
epoch 120| loss: 0.29898 | val_0_rmse: 0.62652 | val_1_rmse: 0.64623 |  0:00:16s
epoch 121| loss: 0.32532 | val_0_rmse: 0.68263 | val_1_rmse: 0.71202 |  0:00:16s
epoch 122| loss: 0.35606 | val_0_rmse: 0.59743 | val_1_rmse: 0.64297 |  0:00:16s
epoch 123| loss: 0.32189 | val_0_rmse: 0.65232 | val_1_rmse: 0.70563 |  0:00:16s
epoch 124| loss: 0.32438 | val_0_rmse: 0.64145 | val_1_rmse: 0.68208 |  0:00:16s
epoch 125| loss: 0.31292 | val_0_rmse: 0.58983 | val_1_rmse: 0.60982 |  0:00:16s
epoch 126| loss: 0.30974 | val_0_rmse: 0.53686 | val_1_rmse: 0.56234 |  0:00:16s
epoch 127| loss: 0.30545 | val_0_rmse: 0.5466  | val_1_rmse: 0.56847 |  0:00:17s
epoch 128| loss: 0.30288 | val_0_rmse: 0.5801  | val_1_rmse: 0.59274 |  0:00:17s
epoch 129| loss: 0.29872 | val_0_rmse: 0.58926 | val_1_rmse: 0.62066 |  0:00:17s
epoch 130| loss: 0.31271 | val_0_rmse: 0.60031 | val_1_rmse: 0.64509 |  0:00:17s
epoch 131| loss: 0.30372 | val_0_rmse: 0.66377 | val_1_rmse: 0.66378 |  0:00:17s
epoch 132| loss: 0.30648 | val_0_rmse: 0.65789 | val_1_rmse: 0.67811 |  0:00:17s
epoch 133| loss: 0.30078 | val_0_rmse: 0.66166 | val_1_rmse: 0.70523 |  0:00:17s
epoch 134| loss: 0.30713 | val_0_rmse: 0.69362 | val_1_rmse: 0.71098 |  0:00:17s
epoch 135| loss: 0.29896 | val_0_rmse: 0.68071 | val_1_rmse: 0.68457 |  0:00:18s
epoch 136| loss: 0.3074  | val_0_rmse: 0.62556 | val_1_rmse: 0.65356 |  0:00:18s
epoch 137| loss: 0.30678 | val_0_rmse: 0.5523  | val_1_rmse: 0.59537 |  0:00:18s
epoch 138| loss: 0.29994 | val_0_rmse: 0.72215 | val_1_rmse: 0.73329 |  0:00:18s
epoch 139| loss: 0.29964 | val_0_rmse: 0.60904 | val_1_rmse: 0.62269 |  0:00:18s
epoch 140| loss: 0.29324 | val_0_rmse: 0.59941 | val_1_rmse: 0.59027 |  0:00:18s
epoch 141| loss: 0.31525 | val_0_rmse: 0.55783 | val_1_rmse: 0.54767 |  0:00:18s
epoch 142| loss: 0.31581 | val_0_rmse: 0.57718 | val_1_rmse: 0.59455 |  0:00:19s
epoch 143| loss: 0.30442 | val_0_rmse: 0.63091 | val_1_rmse: 0.64536 |  0:00:19s
epoch 144| loss: 0.30394 | val_0_rmse: 0.67601 | val_1_rmse: 0.65781 |  0:00:19s
epoch 145| loss: 0.30401 | val_0_rmse: 0.66029 | val_1_rmse: 0.64388 |  0:00:19s
epoch 146| loss: 0.29517 | val_0_rmse: 0.64043 | val_1_rmse: 0.64215 |  0:00:19s
epoch 147| loss: 0.30294 | val_0_rmse: 0.66224 | val_1_rmse: 0.66766 |  0:00:19s
epoch 148| loss: 0.30444 | val_0_rmse: 0.69991 | val_1_rmse: 0.6943  |  0:00:19s
epoch 149| loss: 0.29793 | val_0_rmse: 0.64417 | val_1_rmse: 0.65012 |  0:00:19s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.54767
Best weights from best epoch are automatically used!
ended training at: 10:59:52
Feature importance:
[('Area', 0.13358740660227236), ('Baths', 0.13645162884903503), ('Beds', 0.09545782718759203), ('Latitude', 0.42305714199493605), ('Longitude', 0.16802046749216953), ('Month', 0.03818680511693195), ('Year', 0.005238722757063004)]
Mean squared error is of 3198698151.5909996
Mean absolute error:38694.87753682607
MAPE:0.35100596648510635
R2 score:0.5845247510109248
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 10:59:52
epoch 0  | loss: 1.4478  | val_0_rmse: 1.5167  | val_1_rmse: 4.11553 |  0:00:00s
epoch 1  | loss: 0.89144 | val_0_rmse: 1.18315 | val_1_rmse: 3.45927 |  0:00:00s
epoch 2  | loss: 0.68342 | val_0_rmse: 0.90504 | val_1_rmse: 0.90959 |  0:00:00s
epoch 3  | loss: 0.54006 | val_0_rmse: 0.81049 | val_1_rmse: 0.907   |  0:00:00s
epoch 4  | loss: 0.51315 | val_0_rmse: 0.75973 | val_1_rmse: 1.16923 |  0:00:00s
epoch 5  | loss: 0.49057 | val_0_rmse: 0.7665  | val_1_rmse: 0.75888 |  0:00:00s
epoch 6  | loss: 0.47139 | val_0_rmse: 0.76548 | val_1_rmse: 0.80747 |  0:00:00s
epoch 7  | loss: 0.46071 | val_0_rmse: 0.75855 | val_1_rmse: 0.75976 |  0:00:01s
epoch 8  | loss: 0.46721 | val_0_rmse: 0.72057 | val_1_rmse: 0.72991 |  0:00:01s
epoch 9  | loss: 0.45968 | val_0_rmse: 0.70285 | val_1_rmse: 0.71872 |  0:00:01s
epoch 10 | loss: 0.45391 | val_0_rmse: 0.69948 | val_1_rmse: 0.71283 |  0:00:01s
epoch 11 | loss: 0.45089 | val_0_rmse: 0.67886 | val_1_rmse: 0.69833 |  0:00:01s
epoch 12 | loss: 0.45692 | val_0_rmse: 0.67925 | val_1_rmse: 0.691   |  0:00:01s
epoch 13 | loss: 0.45751 | val_0_rmse: 0.67637 | val_1_rmse: 0.68476 |  0:00:01s
epoch 14 | loss: 0.44586 | val_0_rmse: 0.67809 | val_1_rmse: 0.69112 |  0:00:02s
epoch 15 | loss: 0.44186 | val_0_rmse: 0.67073 | val_1_rmse: 0.67582 |  0:00:02s
epoch 16 | loss: 0.4398  | val_0_rmse: 0.6585  | val_1_rmse: 0.64435 |  0:00:02s
epoch 17 | loss: 0.42869 | val_0_rmse: 0.65651 | val_1_rmse: 0.64176 |  0:00:02s
epoch 18 | loss: 0.43064 | val_0_rmse: 0.64946 | val_1_rmse: 0.63954 |  0:00:02s
epoch 19 | loss: 0.4235  | val_0_rmse: 0.65134 | val_1_rmse: 0.63556 |  0:00:02s
epoch 20 | loss: 0.42214 | val_0_rmse: 0.65992 | val_1_rmse: 0.6356  |  0:00:02s
epoch 21 | loss: 0.42733 | val_0_rmse: 0.66979 | val_1_rmse: 0.65286 |  0:00:03s
epoch 22 | loss: 0.42591 | val_0_rmse: 0.65216 | val_1_rmse: 0.65559 |  0:00:03s
epoch 23 | loss: 0.41116 | val_0_rmse: 0.64549 | val_1_rmse: 0.65992 |  0:00:03s
epoch 24 | loss: 0.41977 | val_0_rmse: 0.65987 | val_1_rmse: 0.67515 |  0:00:03s
epoch 25 | loss: 0.41878 | val_0_rmse: 0.66464 | val_1_rmse: 0.68208 |  0:00:03s
epoch 26 | loss: 0.41668 | val_0_rmse: 0.63433 | val_1_rmse: 0.65063 |  0:00:03s
epoch 27 | loss: 0.40676 | val_0_rmse: 0.6394  | val_1_rmse: 0.65584 |  0:00:03s
epoch 28 | loss: 0.41956 | val_0_rmse: 0.63575 | val_1_rmse: 0.65023 |  0:00:03s
epoch 29 | loss: 0.40552 | val_0_rmse: 0.63819 | val_1_rmse: 0.66136 |  0:00:04s
epoch 30 | loss: 0.40948 | val_0_rmse: 0.63446 | val_1_rmse: 0.66847 |  0:00:04s
epoch 31 | loss: 0.41274 | val_0_rmse: 0.64731 | val_1_rmse: 0.68447 |  0:00:04s
epoch 32 | loss: 0.41217 | val_0_rmse: 0.63303 | val_1_rmse: 0.67484 |  0:00:04s
epoch 33 | loss: 0.4018  | val_0_rmse: 0.63059 | val_1_rmse: 0.67344 |  0:00:04s
epoch 34 | loss: 0.39383 | val_0_rmse: 0.61861 | val_1_rmse: 0.64244 |  0:00:04s
epoch 35 | loss: 0.39813 | val_0_rmse: 0.61936 | val_1_rmse: 0.6336  |  0:00:05s
epoch 36 | loss: 0.39465 | val_0_rmse: 0.61876 | val_1_rmse: 0.65225 |  0:00:05s
epoch 37 | loss: 0.39119 | val_0_rmse: 0.63267 | val_1_rmse: 0.66938 |  0:00:05s
epoch 38 | loss: 0.39766 | val_0_rmse: 0.6362  | val_1_rmse: 0.65312 |  0:00:05s
epoch 39 | loss: 0.40713 | val_0_rmse: 0.64977 | val_1_rmse: 0.65622 |  0:00:05s
epoch 40 | loss: 0.42175 | val_0_rmse: 0.62773 | val_1_rmse: 0.65139 |  0:00:05s
epoch 41 | loss: 0.41109 | val_0_rmse: 0.6286  | val_1_rmse: 0.66624 |  0:00:05s
epoch 42 | loss: 0.4037  | val_0_rmse: 0.62304 | val_1_rmse: 0.65841 |  0:00:05s
epoch 43 | loss: 0.40168 | val_0_rmse: 0.63365 | val_1_rmse: 0.66093 |  0:00:06s
epoch 44 | loss: 0.40508 | val_0_rmse: 0.62794 | val_1_rmse: 0.65423 |  0:00:06s
epoch 45 | loss: 0.39952 | val_0_rmse: 0.62061 | val_1_rmse: 0.64874 |  0:00:06s
epoch 46 | loss: 0.39132 | val_0_rmse: 0.61713 | val_1_rmse: 0.64517 |  0:00:06s
epoch 47 | loss: 0.38496 | val_0_rmse: 0.61521 | val_1_rmse: 0.64519 |  0:00:06s
epoch 48 | loss: 0.38896 | val_0_rmse: 0.62221 | val_1_rmse: 0.6401  |  0:00:06s
epoch 49 | loss: 0.39116 | val_0_rmse: 0.61815 | val_1_rmse: 0.63544 |  0:00:06s
epoch 50 | loss: 0.38494 | val_0_rmse: 0.61524 | val_1_rmse: 0.63887 |  0:00:07s
epoch 51 | loss: 0.38777 | val_0_rmse: 0.61437 | val_1_rmse: 0.64226 |  0:00:07s
epoch 52 | loss: 0.38618 | val_0_rmse: 0.61504 | val_1_rmse: 0.63333 |  0:00:07s
epoch 53 | loss: 0.37793 | val_0_rmse: 0.61037 | val_1_rmse: 0.62525 |  0:00:07s
epoch 54 | loss: 0.37994 | val_0_rmse: 0.6065  | val_1_rmse: 0.6198  |  0:00:07s
epoch 55 | loss: 0.38466 | val_0_rmse: 0.60534 | val_1_rmse: 0.6246  |  0:00:07s
epoch 56 | loss: 0.37638 | val_0_rmse: 0.60382 | val_1_rmse: 0.63419 |  0:00:07s
epoch 57 | loss: 0.38012 | val_0_rmse: 0.60148 | val_1_rmse: 0.63467 |  0:00:07s
epoch 58 | loss: 0.38328 | val_0_rmse: 0.60231 | val_1_rmse: 0.62704 |  0:00:08s
epoch 59 | loss: 0.38046 | val_0_rmse: 0.60105 | val_1_rmse: 0.63107 |  0:00:08s
epoch 60 | loss: 0.37628 | val_0_rmse: 0.60339 | val_1_rmse: 0.63906 |  0:00:08s
epoch 61 | loss: 0.37841 | val_0_rmse: 0.60659 | val_1_rmse: 0.64528 |  0:00:08s
epoch 62 | loss: 0.37771 | val_0_rmse: 0.6052  | val_1_rmse: 0.64723 |  0:00:08s
epoch 63 | loss: 0.3865  | val_0_rmse: 0.60284 | val_1_rmse: 0.6454  |  0:00:08s
epoch 64 | loss: 0.38731 | val_0_rmse: 0.60112 | val_1_rmse: 0.63875 |  0:00:08s
epoch 65 | loss: 0.38673 | val_0_rmse: 0.60299 | val_1_rmse: 0.64213 |  0:00:09s
epoch 66 | loss: 0.37964 | val_0_rmse: 0.60452 | val_1_rmse: 0.64825 |  0:00:09s
epoch 67 | loss: 0.40196 | val_0_rmse: 0.60295 | val_1_rmse: 0.65202 |  0:00:09s
epoch 68 | loss: 0.36458 | val_0_rmse: 0.60006 | val_1_rmse: 0.64353 |  0:00:09s
epoch 69 | loss: 0.37586 | val_0_rmse: 0.60529 | val_1_rmse: 0.63604 |  0:00:09s
epoch 70 | loss: 0.38629 | val_0_rmse: 0.60616 | val_1_rmse: 0.62636 |  0:00:09s
epoch 71 | loss: 0.37955 | val_0_rmse: 0.6037  | val_1_rmse: 0.64431 |  0:00:09s
epoch 72 | loss: 0.38208 | val_0_rmse: 0.61211 | val_1_rmse: 0.65629 |  0:00:09s
epoch 73 | loss: 0.37308 | val_0_rmse: 0.63064 | val_1_rmse: 0.67135 |  0:00:10s
epoch 74 | loss: 0.37502 | val_0_rmse: 0.61612 | val_1_rmse: 0.64852 |  0:00:10s
epoch 75 | loss: 0.38145 | val_0_rmse: 0.60877 | val_1_rmse: 0.63316 |  0:00:10s
epoch 76 | loss: 0.38269 | val_0_rmse: 0.60855 | val_1_rmse: 0.63366 |  0:00:10s
epoch 77 | loss: 0.39305 | val_0_rmse: 0.61336 | val_1_rmse: 0.67279 |  0:00:10s
epoch 78 | loss: 0.38689 | val_0_rmse: 0.62624 | val_1_rmse: 0.66843 |  0:00:10s
epoch 79 | loss: 0.40393 | val_0_rmse: 0.65213 | val_1_rmse: 0.68768 |  0:00:10s
epoch 80 | loss: 0.44534 | val_0_rmse: 0.6617  | val_1_rmse: 0.70887 |  0:00:11s
epoch 81 | loss: 0.41991 | val_0_rmse: 0.669   | val_1_rmse: 0.7198  |  0:00:11s
epoch 82 | loss: 0.41304 | val_0_rmse: 0.64098 | val_1_rmse: 0.67557 |  0:00:11s
epoch 83 | loss: 0.40558 | val_0_rmse: 0.62615 | val_1_rmse: 0.65107 |  0:00:11s
epoch 84 | loss: 0.40595 | val_0_rmse: 0.63115 | val_1_rmse: 0.64642 |  0:00:11s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.6198
Best weights from best epoch are automatically used!
ended training at: 11:00:04
Feature importance:
[('Area', 0.35251821603225353), ('Baths', 0.19371447293393887), ('Beds', 0.1114092714231861), ('Latitude', 0.06586767395218576), ('Longitude', 0.174019322713978), ('Month', 0.035194471628812916), ('Year', 0.06727657131564485)]
Mean squared error is of 3816770205.9608335
Mean absolute error:45155.59420604395
MAPE:0.44538165419880527
R2 score:0.5416288184930602
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:00:04
epoch 0  | loss: 1.36089 | val_0_rmse: 1.15356 | val_1_rmse: 1.10848 |  0:00:00s
epoch 1  | loss: 0.66556 | val_0_rmse: 1.00751 | val_1_rmse: 0.92995 |  0:00:00s
epoch 2  | loss: 0.58092 | val_0_rmse: 0.9134  | val_1_rmse: 0.87665 |  0:00:00s
epoch 3  | loss: 0.5467  | val_0_rmse: 0.95004 | val_1_rmse: 0.8959  |  0:00:00s
epoch 4  | loss: 0.53026 | val_0_rmse: 0.79956 | val_1_rmse: 0.77963 |  0:00:01s
epoch 5  | loss: 0.54232 | val_0_rmse: 0.74088 | val_1_rmse: 0.72474 |  0:00:01s
epoch 6  | loss: 0.50808 | val_0_rmse: 0.7975  | val_1_rmse: 0.79268 |  0:00:01s
epoch 7  | loss: 0.51442 | val_0_rmse: 0.81418 | val_1_rmse: 0.80998 |  0:00:01s
epoch 8  | loss: 0.5043  | val_0_rmse: 0.76196 | val_1_rmse: 0.74624 |  0:00:02s
epoch 9  | loss: 0.50648 | val_0_rmse: 0.74757 | val_1_rmse: 0.72022 |  0:00:02s
epoch 10 | loss: 0.50566 | val_0_rmse: 0.73923 | val_1_rmse: 0.70417 |  0:00:02s
epoch 11 | loss: 0.49143 | val_0_rmse: 0.72848 | val_1_rmse: 0.69342 |  0:00:02s
epoch 12 | loss: 0.49486 | val_0_rmse: 0.71156 | val_1_rmse: 0.68412 |  0:00:02s
epoch 13 | loss: 0.49128 | val_0_rmse: 0.69197 | val_1_rmse: 0.66521 |  0:00:03s
epoch 14 | loss: 0.48605 | val_0_rmse: 0.68345 | val_1_rmse: 0.65671 |  0:00:03s
epoch 15 | loss: 0.47973 | val_0_rmse: 0.6783  | val_1_rmse: 0.65473 |  0:00:03s
epoch 16 | loss: 0.47796 | val_0_rmse: 0.68226 | val_1_rmse: 0.6648  |  0:00:03s
epoch 17 | loss: 0.46669 | val_0_rmse: 0.68324 | val_1_rmse: 0.67294 |  0:00:04s
epoch 18 | loss: 0.47468 | val_0_rmse: 0.70015 | val_1_rmse: 0.70452 |  0:00:04s
epoch 19 | loss: 0.48324 | val_0_rmse: 0.684   | val_1_rmse: 0.67173 |  0:00:04s
epoch 20 | loss: 0.48501 | val_0_rmse: 0.68691 | val_1_rmse: 0.66046 |  0:00:04s
epoch 21 | loss: 0.47258 | val_0_rmse: 0.68234 | val_1_rmse: 0.65817 |  0:00:05s
epoch 22 | loss: 0.48295 | val_0_rmse: 0.68195 | val_1_rmse: 0.66308 |  0:00:05s
epoch 23 | loss: 0.48249 | val_0_rmse: 0.68611 | val_1_rmse: 0.6711  |  0:00:05s
epoch 24 | loss: 0.49152 | val_0_rmse: 0.69452 | val_1_rmse: 0.66746 |  0:00:05s
epoch 25 | loss: 0.48464 | val_0_rmse: 0.68471 | val_1_rmse: 0.66679 |  0:00:05s
epoch 26 | loss: 0.47661 | val_0_rmse: 0.68024 | val_1_rmse: 0.65187 |  0:00:06s
epoch 27 | loss: 0.48244 | val_0_rmse: 0.68527 | val_1_rmse: 0.64635 |  0:00:06s
epoch 28 | loss: 0.48373 | val_0_rmse: 0.68575 | val_1_rmse: 0.66088 |  0:00:06s
epoch 29 | loss: 0.47998 | val_0_rmse: 0.68226 | val_1_rmse: 0.67022 |  0:00:06s
epoch 30 | loss: 0.47301 | val_0_rmse: 0.67872 | val_1_rmse: 0.66851 |  0:00:07s
epoch 31 | loss: 0.47533 | val_0_rmse: 0.6739  | val_1_rmse: 0.66471 |  0:00:07s
epoch 32 | loss: 0.45843 | val_0_rmse: 0.66967 | val_1_rmse: 0.6542  |  0:00:07s
epoch 33 | loss: 0.45011 | val_0_rmse: 0.66748 | val_1_rmse: 0.66846 |  0:00:07s
epoch 34 | loss: 0.45219 | val_0_rmse: 0.64987 | val_1_rmse: 0.65468 |  0:00:07s
epoch 35 | loss: 0.4486  | val_0_rmse: 0.65589 | val_1_rmse: 0.6612  |  0:00:08s
epoch 36 | loss: 0.45763 | val_0_rmse: 0.67323 | val_1_rmse: 0.68507 |  0:00:08s
epoch 37 | loss: 0.44803 | val_0_rmse: 0.67795 | val_1_rmse: 0.69149 |  0:00:08s
epoch 38 | loss: 0.44204 | val_0_rmse: 0.6959  | val_1_rmse: 0.70446 |  0:00:08s
epoch 39 | loss: 0.43394 | val_0_rmse: 0.73136 | val_1_rmse: 0.73141 |  0:00:09s
epoch 40 | loss: 0.42821 | val_0_rmse: 0.68566 | val_1_rmse: 0.71316 |  0:00:09s
epoch 41 | loss: 0.44502 | val_0_rmse: 0.70035 | val_1_rmse: 0.72343 |  0:00:09s
epoch 42 | loss: 0.42962 | val_0_rmse: 0.68386 | val_1_rmse: 0.69381 |  0:00:09s
epoch 43 | loss: 0.42507 | val_0_rmse: 0.66327 | val_1_rmse: 0.67658 |  0:00:09s
epoch 44 | loss: 0.42039 | val_0_rmse: 0.6472  | val_1_rmse: 0.67679 |  0:00:10s
epoch 45 | loss: 0.43325 | val_0_rmse: 0.65019 | val_1_rmse: 0.67208 |  0:00:10s
epoch 46 | loss: 0.41792 | val_0_rmse: 0.63387 | val_1_rmse: 0.63357 |  0:00:10s
epoch 47 | loss: 0.42864 | val_0_rmse: 0.66281 | val_1_rmse: 0.67268 |  0:00:10s
epoch 48 | loss: 0.44387 | val_0_rmse: 0.64092 | val_1_rmse: 0.64301 |  0:00:11s
epoch 49 | loss: 0.42295 | val_0_rmse: 0.64169 | val_1_rmse: 0.65128 |  0:00:11s
epoch 50 | loss: 0.42434 | val_0_rmse: 0.63485 | val_1_rmse: 0.64162 |  0:00:11s
epoch 51 | loss: 0.43513 | val_0_rmse: 0.65145 | val_1_rmse: 0.67077 |  0:00:11s
epoch 52 | loss: 0.43007 | val_0_rmse: 0.62495 | val_1_rmse: 0.64235 |  0:00:11s
epoch 53 | loss: 0.40991 | val_0_rmse: 0.62678 | val_1_rmse: 0.63368 |  0:00:12s
epoch 54 | loss: 0.40306 | val_0_rmse: 0.63932 | val_1_rmse: 0.62742 |  0:00:12s
epoch 55 | loss: 0.41697 | val_0_rmse: 0.63419 | val_1_rmse: 0.64535 |  0:00:12s
epoch 56 | loss: 0.41113 | val_0_rmse: 0.66778 | val_1_rmse: 0.6858  |  0:00:12s
epoch 57 | loss: 0.41861 | val_0_rmse: 0.659   | val_1_rmse: 0.67245 |  0:00:13s
epoch 58 | loss: 0.41958 | val_0_rmse: 0.73313 | val_1_rmse: 0.74311 |  0:00:13s
epoch 59 | loss: 0.40618 | val_0_rmse: 0.68394 | val_1_rmse: 0.68149 |  0:00:13s
epoch 60 | loss: 0.43042 | val_0_rmse: 0.63362 | val_1_rmse: 0.64008 |  0:00:13s
epoch 61 | loss: 0.41325 | val_0_rmse: 0.64834 | val_1_rmse: 0.66238 |  0:00:13s
epoch 62 | loss: 0.41477 | val_0_rmse: 0.65903 | val_1_rmse: 0.67839 |  0:00:14s
epoch 63 | loss: 0.39314 | val_0_rmse: 0.6284  | val_1_rmse: 0.64404 |  0:00:14s
epoch 64 | loss: 0.39001 | val_0_rmse: 0.62948 | val_1_rmse: 0.62451 |  0:00:14s
epoch 65 | loss: 0.39679 | val_0_rmse: 0.61656 | val_1_rmse: 0.61494 |  0:00:14s
epoch 66 | loss: 0.40954 | val_0_rmse: 0.68169 | val_1_rmse: 0.69376 |  0:00:15s
epoch 67 | loss: 0.39325 | val_0_rmse: 0.69007 | val_1_rmse: 0.70466 |  0:00:15s
epoch 68 | loss: 0.39922 | val_0_rmse: 0.61993 | val_1_rmse: 0.6333  |  0:00:15s
epoch 69 | loss: 0.38939 | val_0_rmse: 0.63592 | val_1_rmse: 0.65328 |  0:00:15s
epoch 70 | loss: 0.38508 | val_0_rmse: 0.60944 | val_1_rmse: 0.62124 |  0:00:16s
epoch 71 | loss: 0.40218 | val_0_rmse: 0.62923 | val_1_rmse: 0.64673 |  0:00:16s
epoch 72 | loss: 0.37958 | val_0_rmse: 0.61074 | val_1_rmse: 0.62615 |  0:00:16s
epoch 73 | loss: 0.3829  | val_0_rmse: 0.64188 | val_1_rmse: 0.65407 |  0:00:16s
epoch 74 | loss: 0.39596 | val_0_rmse: 0.64871 | val_1_rmse: 0.63652 |  0:00:16s
epoch 75 | loss: 0.3981  | val_0_rmse: 0.70525 | val_1_rmse: 0.69196 |  0:00:17s
epoch 76 | loss: 0.40374 | val_0_rmse: 0.77331 | val_1_rmse: 0.7563  |  0:00:17s
epoch 77 | loss: 0.39989 | val_0_rmse: 0.65583 | val_1_rmse: 0.65481 |  0:00:17s
epoch 78 | loss: 0.40013 | val_0_rmse: 0.61424 | val_1_rmse: 0.60672 |  0:00:17s
epoch 79 | loss: 0.38777 | val_0_rmse: 0.61978 | val_1_rmse: 0.61329 |  0:00:17s
epoch 80 | loss: 0.38496 | val_0_rmse: 0.61821 | val_1_rmse: 0.61252 |  0:00:18s
epoch 81 | loss: 0.4001  | val_0_rmse: 0.63619 | val_1_rmse: 0.64077 |  0:00:18s
epoch 82 | loss: 0.39056 | val_0_rmse: 0.639   | val_1_rmse: 0.64257 |  0:00:18s
epoch 83 | loss: 0.38283 | val_0_rmse: 0.61899 | val_1_rmse: 0.64118 |  0:00:18s
epoch 84 | loss: 0.39176 | val_0_rmse: 0.61521 | val_1_rmse: 0.63399 |  0:00:19s
epoch 85 | loss: 0.3825  | val_0_rmse: 0.60552 | val_1_rmse: 0.62178 |  0:00:19s
epoch 86 | loss: 0.37478 | val_0_rmse: 0.60836 | val_1_rmse: 0.61304 |  0:00:19s
epoch 87 | loss: 0.37677 | val_0_rmse: 0.61451 | val_1_rmse: 0.61764 |  0:00:19s
epoch 88 | loss: 0.38277 | val_0_rmse: 0.60754 | val_1_rmse: 0.62548 |  0:00:19s
epoch 89 | loss: 0.39375 | val_0_rmse: 0.6046  | val_1_rmse: 0.6225  |  0:00:20s
epoch 90 | loss: 0.38442 | val_0_rmse: 0.6087  | val_1_rmse: 0.6208  |  0:00:20s
epoch 91 | loss: 0.37522 | val_0_rmse: 0.62331 | val_1_rmse: 0.63076 |  0:00:20s
epoch 92 | loss: 0.37831 | val_0_rmse: 0.61295 | val_1_rmse: 0.61901 |  0:00:20s
epoch 93 | loss: 0.35999 | val_0_rmse: 0.613   | val_1_rmse: 0.62706 |  0:00:21s
epoch 94 | loss: 0.38037 | val_0_rmse: 0.6134  | val_1_rmse: 0.62558 |  0:00:21s
epoch 95 | loss: 0.36146 | val_0_rmse: 0.61396 | val_1_rmse: 0.62964 |  0:00:21s
epoch 96 | loss: 0.35566 | val_0_rmse: 0.59901 | val_1_rmse: 0.61319 |  0:00:21s
epoch 97 | loss: 0.36536 | val_0_rmse: 0.62162 | val_1_rmse: 0.63571 |  0:00:21s
epoch 98 | loss: 0.37244 | val_0_rmse: 0.61938 | val_1_rmse: 0.64482 |  0:00:22s
epoch 99 | loss: 0.38335 | val_0_rmse: 0.61357 | val_1_rmse: 0.63931 |  0:00:22s
epoch 100| loss: 0.36671 | val_0_rmse: 0.61835 | val_1_rmse: 0.6438  |  0:00:22s
epoch 101| loss: 0.3743  | val_0_rmse: 0.6293  | val_1_rmse: 0.64768 |  0:00:22s
epoch 102| loss: 0.36729 | val_0_rmse: 0.60841 | val_1_rmse: 0.62433 |  0:00:23s
epoch 103| loss: 0.39169 | val_0_rmse: 0.59452 | val_1_rmse: 0.60078 |  0:00:23s
epoch 104| loss: 0.37537 | val_0_rmse: 0.62112 | val_1_rmse: 0.63037 |  0:00:23s
epoch 105| loss: 0.36783 | val_0_rmse: 0.72732 | val_1_rmse: 0.74511 |  0:00:23s
epoch 106| loss: 0.3729  | val_0_rmse: 0.70633 | val_1_rmse: 0.72385 |  0:00:23s
epoch 107| loss: 0.37031 | val_0_rmse: 0.65612 | val_1_rmse: 0.66743 |  0:00:24s
epoch 108| loss: 0.36053 | val_0_rmse: 0.63071 | val_1_rmse: 0.64849 |  0:00:24s
epoch 109| loss: 0.35439 | val_0_rmse: 0.5918  | val_1_rmse: 0.60289 |  0:00:24s
epoch 110| loss: 0.36904 | val_0_rmse: 0.61877 | val_1_rmse: 0.632   |  0:00:24s
epoch 111| loss: 0.36554 | val_0_rmse: 0.64266 | val_1_rmse: 0.66539 |  0:00:25s
epoch 112| loss: 0.35836 | val_0_rmse: 0.62736 | val_1_rmse: 0.6561  |  0:00:25s
epoch 113| loss: 0.35787 | val_0_rmse: 0.59252 | val_1_rmse: 0.60717 |  0:00:25s
epoch 114| loss: 0.36977 | val_0_rmse: 0.60852 | val_1_rmse: 0.61418 |  0:00:25s
epoch 115| loss: 0.37255 | val_0_rmse: 0.60953 | val_1_rmse: 0.61418 |  0:00:26s
epoch 116| loss: 0.35033 | val_0_rmse: 0.60051 | val_1_rmse: 0.62055 |  0:00:26s
epoch 117| loss: 0.36726 | val_0_rmse: 0.6005  | val_1_rmse: 0.62064 |  0:00:26s
epoch 118| loss: 0.3604  | val_0_rmse: 0.60997 | val_1_rmse: 0.64781 |  0:00:26s
epoch 119| loss: 0.35688 | val_0_rmse: 0.62671 | val_1_rmse: 0.66692 |  0:00:26s
epoch 120| loss: 0.37691 | val_0_rmse: 0.59975 | val_1_rmse: 0.6255  |  0:00:27s
epoch 121| loss: 0.36244 | val_0_rmse: 0.5923  | val_1_rmse: 0.61338 |  0:00:27s
epoch 122| loss: 0.38783 | val_0_rmse: 0.60049 | val_1_rmse: 0.62117 |  0:00:27s
epoch 123| loss: 0.37335 | val_0_rmse: 0.73988 | val_1_rmse: 0.76631 |  0:00:27s
epoch 124| loss: 0.40797 | val_0_rmse: 0.63169 | val_1_rmse: 0.6389  |  0:00:28s
epoch 125| loss: 0.4084  | val_0_rmse: 0.65182 | val_1_rmse: 0.64207 |  0:00:28s
epoch 126| loss: 0.41709 | val_0_rmse: 0.6547  | val_1_rmse: 0.64779 |  0:00:28s
epoch 127| loss: 0.39233 | val_0_rmse: 0.66532 | val_1_rmse: 0.66589 |  0:00:28s
epoch 128| loss: 0.37921 | val_0_rmse: 0.66071 | val_1_rmse: 0.67058 |  0:00:28s
epoch 129| loss: 0.39165 | val_0_rmse: 0.62846 | val_1_rmse: 0.63036 |  0:00:29s
epoch 130| loss: 0.37655 | val_0_rmse: 0.65631 | val_1_rmse: 0.65208 |  0:00:29s
epoch 131| loss: 0.39657 | val_0_rmse: 0.70765 | val_1_rmse: 0.71729 |  0:00:29s
epoch 132| loss: 0.38515 | val_0_rmse: 0.63024 | val_1_rmse: 0.62562 |  0:00:29s
epoch 133| loss: 0.37452 | val_0_rmse: 0.62291 | val_1_rmse: 0.61587 |  0:00:29s

Early stopping occured at epoch 133 with best_epoch = 103 and best_val_1_rmse = 0.60078
Best weights from best epoch are automatically used!
ended training at: 11:00:34
Feature importance:
[('Area', 0.3608124555135672), ('Baths', 0.28589190901932077), ('Beds', 0.06798523863234208), ('Latitude', 0.12661003655702302), ('Longitude', 0.016891606465896155), ('Month', 0.07417571769467461), ('Year', 0.06763303611717614)]
Mean squared error is of 2733259631.676929
Mean absolute error:37773.10240486249
MAPE:0.31825270117680493
R2 score:0.6264241223876542
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:00:35
epoch 0  | loss: 1.32853 | val_0_rmse: 0.98344 | val_1_rmse: 0.95052 |  0:00:00s
epoch 1  | loss: 0.64588 | val_0_rmse: 0.92567 | val_1_rmse: 0.93077 |  0:00:00s
epoch 2  | loss: 0.55847 | val_0_rmse: 0.84121 | val_1_rmse: 0.86385 |  0:00:00s
epoch 3  | loss: 0.55753 | val_0_rmse: 0.7534  | val_1_rmse: 0.7888  |  0:00:00s
epoch 4  | loss: 0.52939 | val_0_rmse: 0.77538 | val_1_rmse: 0.76833 |  0:00:01s
epoch 5  | loss: 0.51242 | val_0_rmse: 0.73439 | val_1_rmse: 0.75652 |  0:00:01s
epoch 6  | loss: 0.50182 | val_0_rmse: 0.6958  | val_1_rmse: 0.72817 |  0:00:01s
epoch 7  | loss: 0.49762 | val_0_rmse: 0.69615 | val_1_rmse: 0.72769 |  0:00:01s
epoch 8  | loss: 0.48642 | val_0_rmse: 0.69741 | val_1_rmse: 0.73305 |  0:00:02s
epoch 9  | loss: 0.46839 | val_0_rmse: 0.69589 | val_1_rmse: 0.72878 |  0:00:02s
epoch 10 | loss: 0.45762 | val_0_rmse: 0.68949 | val_1_rmse: 0.72928 |  0:00:02s
epoch 11 | loss: 0.45048 | val_0_rmse: 0.67439 | val_1_rmse: 0.7137  |  0:00:02s
epoch 12 | loss: 0.45236 | val_0_rmse: 0.68003 | val_1_rmse: 0.71493 |  0:00:02s
epoch 13 | loss: 0.4462  | val_0_rmse: 0.66716 | val_1_rmse: 0.69842 |  0:00:03s
epoch 14 | loss: 0.44715 | val_0_rmse: 0.68325 | val_1_rmse: 0.70863 |  0:00:03s
epoch 15 | loss: 0.4449  | val_0_rmse: 0.7132  | val_1_rmse: 0.7083  |  0:00:03s
epoch 16 | loss: 0.43783 | val_0_rmse: 0.67306 | val_1_rmse: 0.71167 |  0:00:03s
epoch 17 | loss: 0.43458 | val_0_rmse: 0.66373 | val_1_rmse: 0.70465 |  0:00:04s
epoch 18 | loss: 0.44507 | val_0_rmse: 0.64801 | val_1_rmse: 0.67616 |  0:00:04s
epoch 19 | loss: 0.43177 | val_0_rmse: 0.64684 | val_1_rmse: 0.66783 |  0:00:04s
epoch 20 | loss: 0.45929 | val_0_rmse: 0.65225 | val_1_rmse: 0.68924 |  0:00:04s
epoch 21 | loss: 0.44033 | val_0_rmse: 0.69758 | val_1_rmse: 0.7283  |  0:00:04s
epoch 22 | loss: 0.42962 | val_0_rmse: 0.66242 | val_1_rmse: 0.69789 |  0:00:05s
epoch 23 | loss: 0.4436  | val_0_rmse: 0.65364 | val_1_rmse: 0.68411 |  0:00:05s
epoch 24 | loss: 0.44479 | val_0_rmse: 0.65626 | val_1_rmse: 0.69294 |  0:00:05s
epoch 25 | loss: 0.43699 | val_0_rmse: 0.66035 | val_1_rmse: 0.69841 |  0:00:05s
epoch 26 | loss: 0.41223 | val_0_rmse: 0.65221 | val_1_rmse: 0.69589 |  0:00:06s
epoch 27 | loss: 0.43258 | val_0_rmse: 0.64783 | val_1_rmse: 0.70583 |  0:00:06s
epoch 28 | loss: 0.40049 | val_0_rmse: 0.64522 | val_1_rmse: 0.70941 |  0:00:06s
epoch 29 | loss: 0.41273 | val_0_rmse: 0.64853 | val_1_rmse: 0.70801 |  0:00:06s
epoch 30 | loss: 0.41246 | val_0_rmse: 0.62355 | val_1_rmse: 0.68217 |  0:00:07s
epoch 31 | loss: 0.41566 | val_0_rmse: 0.63252 | val_1_rmse: 0.66826 |  0:00:07s
epoch 32 | loss: 0.4044  | val_0_rmse: 0.63932 | val_1_rmse: 0.669   |  0:00:07s
epoch 33 | loss: 0.40765 | val_0_rmse: 0.6279  | val_1_rmse: 0.67185 |  0:00:07s
epoch 34 | loss: 0.40093 | val_0_rmse: 0.61721 | val_1_rmse: 0.6689  |  0:00:07s
epoch 35 | loss: 0.39955 | val_0_rmse: 0.62928 | val_1_rmse: 0.66837 |  0:00:08s
epoch 36 | loss: 0.39446 | val_0_rmse: 0.65974 | val_1_rmse: 0.70804 |  0:00:08s
epoch 37 | loss: 0.39865 | val_0_rmse: 0.64035 | val_1_rmse: 0.71093 |  0:00:08s
epoch 38 | loss: 0.39129 | val_0_rmse: 0.61615 | val_1_rmse: 0.68987 |  0:00:08s
epoch 39 | loss: 0.40417 | val_0_rmse: 0.62483 | val_1_rmse: 0.70506 |  0:00:08s
epoch 40 | loss: 0.3896  | val_0_rmse: 0.61116 | val_1_rmse: 0.76368 |  0:00:09s
epoch 41 | loss: 0.40087 | val_0_rmse: 0.62321 | val_1_rmse: 0.67845 |  0:00:09s
epoch 42 | loss: 0.40967 | val_0_rmse: 0.63207 | val_1_rmse: 0.67394 |  0:00:09s
epoch 43 | loss: 0.403   | val_0_rmse: 0.60469 | val_1_rmse: 0.65794 |  0:00:09s
epoch 44 | loss: 0.38007 | val_0_rmse: 0.62097 | val_1_rmse: 0.66215 |  0:00:10s
epoch 45 | loss: 0.39051 | val_0_rmse: 0.60406 | val_1_rmse: 0.65409 |  0:00:10s
epoch 46 | loss: 0.3816  | val_0_rmse: 0.60032 | val_1_rmse: 0.65052 |  0:00:10s
epoch 47 | loss: 0.38209 | val_0_rmse: 0.59657 | val_1_rmse: 0.63792 |  0:00:10s
epoch 48 | loss: 0.37433 | val_0_rmse: 0.60117 | val_1_rmse: 0.64193 |  0:00:11s
epoch 49 | loss: 0.39679 | val_0_rmse: 0.62125 | val_1_rmse: 0.65891 |  0:00:11s
epoch 50 | loss: 0.38367 | val_0_rmse: 0.61646 | val_1_rmse: 0.64392 |  0:00:11s
epoch 51 | loss: 0.38803 | val_0_rmse: 0.60461 | val_1_rmse: 0.63762 |  0:00:11s
epoch 52 | loss: 0.39167 | val_0_rmse: 0.60425 | val_1_rmse: 0.64512 |  0:00:11s
epoch 53 | loss: 0.36991 | val_0_rmse: 0.61984 | val_1_rmse: 0.65753 |  0:00:12s
epoch 54 | loss: 0.38161 | val_0_rmse: 0.64401 | val_1_rmse: 0.6871  |  0:00:12s
epoch 55 | loss: 0.38809 | val_0_rmse: 0.63028 | val_1_rmse: 0.66395 |  0:00:12s
epoch 56 | loss: 0.40342 | val_0_rmse: 0.6257  | val_1_rmse: 0.65318 |  0:00:12s
epoch 57 | loss: 0.38983 | val_0_rmse: 0.61499 | val_1_rmse: 0.64264 |  0:00:13s
epoch 58 | loss: 0.38473 | val_0_rmse: 0.60546 | val_1_rmse: 0.64746 |  0:00:13s
epoch 59 | loss: 0.37824 | val_0_rmse: 0.60918 | val_1_rmse: 0.65641 |  0:00:13s
epoch 60 | loss: 0.37234 | val_0_rmse: 0.61141 | val_1_rmse: 0.66031 |  0:00:13s
epoch 61 | loss: 0.37482 | val_0_rmse: 0.60499 | val_1_rmse: 0.64539 |  0:00:13s
epoch 62 | loss: 0.37941 | val_0_rmse: 0.60342 | val_1_rmse: 0.64044 |  0:00:14s
epoch 63 | loss: 0.38496 | val_0_rmse: 0.59918 | val_1_rmse: 0.64356 |  0:00:14s
epoch 64 | loss: 0.37348 | val_0_rmse: 0.59768 | val_1_rmse: 0.63529 |  0:00:14s
epoch 65 | loss: 0.36846 | val_0_rmse: 0.60937 | val_1_rmse: 0.65686 |  0:00:14s
epoch 66 | loss: 0.36159 | val_0_rmse: 0.62812 | val_1_rmse: 0.6771  |  0:00:15s
epoch 67 | loss: 0.37086 | val_0_rmse: 0.60592 | val_1_rmse: 0.65226 |  0:00:15s
epoch 68 | loss: 0.38339 | val_0_rmse: 0.60346 | val_1_rmse: 0.64881 |  0:00:15s
epoch 69 | loss: 0.39302 | val_0_rmse: 0.61125 | val_1_rmse: 0.65242 |  0:00:15s
epoch 70 | loss: 0.37876 | val_0_rmse: 0.6037  | val_1_rmse: 0.65115 |  0:00:15s
epoch 71 | loss: 0.38703 | val_0_rmse: 0.60383 | val_1_rmse: 0.65079 |  0:00:16s
epoch 72 | loss: 0.37882 | val_0_rmse: 0.60166 | val_1_rmse: 0.64709 |  0:00:16s
epoch 73 | loss: 0.36352 | val_0_rmse: 0.59841 | val_1_rmse: 0.6437  |  0:00:16s
epoch 74 | loss: 0.35791 | val_0_rmse: 0.5965  | val_1_rmse: 0.64273 |  0:00:16s
epoch 75 | loss: 0.36124 | val_0_rmse: 0.58871 | val_1_rmse: 0.63245 |  0:00:17s
epoch 76 | loss: 0.36804 | val_0_rmse: 0.58942 | val_1_rmse: 0.63673 |  0:00:17s
epoch 77 | loss: 0.36335 | val_0_rmse: 0.59302 | val_1_rmse: 0.63142 |  0:00:17s
epoch 78 | loss: 0.35384 | val_0_rmse: 0.58434 | val_1_rmse: 0.62759 |  0:00:17s
epoch 79 | loss: 0.35138 | val_0_rmse: 0.58826 | val_1_rmse: 0.63209 |  0:00:18s
epoch 80 | loss: 0.35254 | val_0_rmse: 0.59378 | val_1_rmse: 0.64656 |  0:00:18s
epoch 81 | loss: 0.3568  | val_0_rmse: 0.59226 | val_1_rmse: 0.64285 |  0:00:18s
epoch 82 | loss: 0.36336 | val_0_rmse: 0.59191 | val_1_rmse: 0.64385 |  0:00:18s
epoch 83 | loss: 0.35949 | val_0_rmse: 0.59208 | val_1_rmse: 0.64138 |  0:00:18s
epoch 84 | loss: 0.37207 | val_0_rmse: 0.59045 | val_1_rmse: 0.63596 |  0:00:19s
epoch 85 | loss: 0.3704  | val_0_rmse: 0.59319 | val_1_rmse: 0.6397  |  0:00:19s
epoch 86 | loss: 0.36012 | val_0_rmse: 0.60182 | val_1_rmse: 0.6445  |  0:00:19s
epoch 87 | loss: 0.36843 | val_0_rmse: 0.59703 | val_1_rmse: 0.64177 |  0:00:19s
epoch 88 | loss: 0.36411 | val_0_rmse: 0.59787 | val_1_rmse: 0.63939 |  0:00:19s
epoch 89 | loss: 0.3604  | val_0_rmse: 0.583   | val_1_rmse: 0.62909 |  0:00:20s
epoch 90 | loss: 0.35867 | val_0_rmse: 0.59098 | val_1_rmse: 0.63471 |  0:00:20s
epoch 91 | loss: 0.35676 | val_0_rmse: 0.58652 | val_1_rmse: 0.6183  |  0:00:20s
epoch 92 | loss: 0.35764 | val_0_rmse: 0.60354 | val_1_rmse: 0.63804 |  0:00:20s
epoch 93 | loss: 0.36545 | val_0_rmse: 0.59406 | val_1_rmse: 0.63677 |  0:00:21s
epoch 94 | loss: 0.35275 | val_0_rmse: 0.58441 | val_1_rmse: 0.63744 |  0:00:21s
epoch 95 | loss: 0.355   | val_0_rmse: 0.59726 | val_1_rmse: 0.6529  |  0:00:21s
epoch 96 | loss: 0.36168 | val_0_rmse: 0.60776 | val_1_rmse: 0.65995 |  0:00:21s
epoch 97 | loss: 0.36003 | val_0_rmse: 0.59763 | val_1_rmse: 0.65654 |  0:00:21s
epoch 98 | loss: 0.3472  | val_0_rmse: 0.592   | val_1_rmse: 0.65286 |  0:00:22s
epoch 99 | loss: 0.34385 | val_0_rmse: 0.61045 | val_1_rmse: 0.66423 |  0:00:22s
epoch 100| loss: 0.3582  | val_0_rmse: 0.59781 | val_1_rmse: 0.64156 |  0:00:22s
epoch 101| loss: 0.36771 | val_0_rmse: 0.59982 | val_1_rmse: 0.64226 |  0:00:22s
epoch 102| loss: 0.35239 | val_0_rmse: 0.59549 | val_1_rmse: 0.64458 |  0:00:23s
epoch 103| loss: 0.36389 | val_0_rmse: 0.59599 | val_1_rmse: 0.63248 |  0:00:23s
epoch 104| loss: 0.35807 | val_0_rmse: 0.59515 | val_1_rmse: 0.63057 |  0:00:23s
epoch 105| loss: 0.35919 | val_0_rmse: 0.60185 | val_1_rmse: 0.63746 |  0:00:23s
epoch 106| loss: 0.3575  | val_0_rmse: 0.6061  | val_1_rmse: 0.64487 |  0:00:24s
epoch 107| loss: 0.36344 | val_0_rmse: 0.60334 | val_1_rmse: 0.63791 |  0:00:24s
epoch 108| loss: 0.35101 | val_0_rmse: 0.6014  | val_1_rmse: 0.63438 |  0:00:24s
epoch 109| loss: 0.34921 | val_0_rmse: 0.59738 | val_1_rmse: 0.63109 |  0:00:24s
epoch 110| loss: 0.35672 | val_0_rmse: 0.5999  | val_1_rmse: 0.6348  |  0:00:24s
epoch 111| loss: 0.34954 | val_0_rmse: 0.59834 | val_1_rmse: 0.64037 |  0:00:25s
epoch 112| loss: 0.35137 | val_0_rmse: 0.5903  | val_1_rmse: 0.63356 |  0:00:25s
epoch 113| loss: 0.35499 | val_0_rmse: 0.5968  | val_1_rmse: 0.63833 |  0:00:25s
epoch 114| loss: 0.34365 | val_0_rmse: 0.59656 | val_1_rmse: 0.63532 |  0:00:25s
epoch 115| loss: 0.35298 | val_0_rmse: 0.58922 | val_1_rmse: 0.62967 |  0:00:26s
epoch 116| loss: 0.34316 | val_0_rmse: 0.58774 | val_1_rmse: 0.62396 |  0:00:26s
epoch 117| loss: 0.3482  | val_0_rmse: 0.58959 | val_1_rmse: 0.62724 |  0:00:26s
epoch 118| loss: 0.34792 | val_0_rmse: 0.58445 | val_1_rmse: 0.63361 |  0:00:26s
epoch 119| loss: 0.34747 | val_0_rmse: 0.58825 | val_1_rmse: 0.63444 |  0:00:26s
epoch 120| loss: 0.34778 | val_0_rmse: 0.60054 | val_1_rmse: 0.64957 |  0:00:27s
epoch 121| loss: 0.33985 | val_0_rmse: 0.67478 | val_1_rmse: 0.71747 |  0:00:27s

Early stopping occured at epoch 121 with best_epoch = 91 and best_val_1_rmse = 0.6183
Best weights from best epoch are automatically used!
ended training at: 11:01:02
Feature importance:
[('Area', 0.38822647546920347), ('Baths', 0.1405481604688854), ('Beds', 0.037509116955597104), ('Latitude', 0.08879438688486221), ('Longitude', 0.29989562486526655), ('Month', 0.044931244503461296), ('Year', 9.499085272395645e-05)]
Mean squared error is of 2861686559.3148365
Mean absolute error:35824.63630769231
MAPE:0.2851967425781429
R2 score:0.6555066569330092
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:01:02
epoch 0  | loss: 1.40383 | val_0_rmse: 1.00856 | val_1_rmse: 0.92253 |  0:00:00s
epoch 1  | loss: 0.75704 | val_0_rmse: 0.84615 | val_1_rmse: 0.78973 |  0:00:00s
epoch 2  | loss: 0.63122 | val_0_rmse: 0.86466 | val_1_rmse: 0.80705 |  0:00:00s
epoch 3  | loss: 0.57069 | val_0_rmse: 0.88971 | val_1_rmse: 0.81264 |  0:00:00s
epoch 4  | loss: 0.53231 | val_0_rmse: 0.81312 | val_1_rmse: 0.74131 |  0:00:01s
epoch 5  | loss: 0.52076 | val_0_rmse: 0.78467 | val_1_rmse: 0.71447 |  0:00:01s
epoch 6  | loss: 0.50122 | val_0_rmse: 0.76781 | val_1_rmse: 0.71913 |  0:00:01s
epoch 7  | loss: 0.50298 | val_0_rmse: 0.71444 | val_1_rmse: 0.67939 |  0:00:01s
epoch 8  | loss: 0.48987 | val_0_rmse: 0.68781 | val_1_rmse: 0.6245  |  0:00:02s
epoch 9  | loss: 0.4723  | val_0_rmse: 0.69317 | val_1_rmse: 0.65433 |  0:00:02s
epoch 10 | loss: 0.49705 | val_0_rmse: 0.70094 | val_1_rmse: 0.64622 |  0:00:02s
epoch 11 | loss: 0.47678 | val_0_rmse: 0.71237 | val_1_rmse: 0.6387  |  0:00:02s
epoch 12 | loss: 0.46536 | val_0_rmse: 0.67248 | val_1_rmse: 0.61973 |  0:00:02s
epoch 13 | loss: 0.4535  | val_0_rmse: 0.68123 | val_1_rmse: 0.63198 |  0:00:03s
epoch 14 | loss: 0.46085 | val_0_rmse: 0.67498 | val_1_rmse: 0.62448 |  0:00:03s
epoch 15 | loss: 0.46307 | val_0_rmse: 0.67842 | val_1_rmse: 0.62925 |  0:00:03s
epoch 16 | loss: 0.44248 | val_0_rmse: 0.67726 | val_1_rmse: 0.63187 |  0:00:03s
epoch 17 | loss: 0.46493 | val_0_rmse: 0.72219 | val_1_rmse: 0.66272 |  0:00:04s
epoch 18 | loss: 0.45982 | val_0_rmse: 0.66337 | val_1_rmse: 0.61509 |  0:00:04s
epoch 19 | loss: 0.43857 | val_0_rmse: 0.68396 | val_1_rmse: 0.64566 |  0:00:04s
epoch 20 | loss: 0.44507 | val_0_rmse: 0.6581  | val_1_rmse: 0.63521 |  0:00:04s
epoch 21 | loss: 0.4369  | val_0_rmse: 0.65125 | val_1_rmse: 0.62425 |  0:00:04s
epoch 22 | loss: 0.42698 | val_0_rmse: 0.64633 | val_1_rmse: 0.62012 |  0:00:05s
epoch 23 | loss: 0.41575 | val_0_rmse: 0.64855 | val_1_rmse: 0.60294 |  0:00:05s
epoch 24 | loss: 0.42197 | val_0_rmse: 0.66345 | val_1_rmse: 0.61308 |  0:00:05s
epoch 25 | loss: 0.43367 | val_0_rmse: 0.66738 | val_1_rmse: 0.61731 |  0:00:05s
epoch 26 | loss: 0.41839 | val_0_rmse: 0.66465 | val_1_rmse: 0.62266 |  0:00:06s
epoch 27 | loss: 0.41598 | val_0_rmse: 0.64354 | val_1_rmse: 0.61102 |  0:00:06s
epoch 28 | loss: 0.41395 | val_0_rmse: 0.64205 | val_1_rmse: 0.60411 |  0:00:06s
epoch 29 | loss: 0.40893 | val_0_rmse: 0.63654 | val_1_rmse: 0.60847 |  0:00:06s
epoch 30 | loss: 0.40219 | val_0_rmse: 0.63546 | val_1_rmse: 0.61577 |  0:00:07s
epoch 31 | loss: 0.40699 | val_0_rmse: 0.63415 | val_1_rmse: 0.6015  |  0:00:07s
epoch 32 | loss: 0.41059 | val_0_rmse: 0.68687 | val_1_rmse: 0.64877 |  0:00:07s
epoch 33 | loss: 0.42271 | val_0_rmse: 0.62697 | val_1_rmse: 0.58951 |  0:00:07s
epoch 34 | loss: 0.41137 | val_0_rmse: 0.64389 | val_1_rmse: 0.61148 |  0:00:07s
epoch 35 | loss: 0.40186 | val_0_rmse: 0.65914 | val_1_rmse: 0.61363 |  0:00:08s
epoch 36 | loss: 0.39155 | val_0_rmse: 0.6834  | val_1_rmse: 0.6873  |  0:00:08s
epoch 37 | loss: 0.39342 | val_0_rmse: 0.60345 | val_1_rmse: 0.58393 |  0:00:08s
epoch 38 | loss: 0.38508 | val_0_rmse: 0.64694 | val_1_rmse: 0.6522  |  0:00:08s
epoch 39 | loss: 0.4054  | val_0_rmse: 0.60927 | val_1_rmse: 0.63271 |  0:00:09s
epoch 40 | loss: 0.39015 | val_0_rmse: 0.61177 | val_1_rmse: 0.64078 |  0:00:09s
epoch 41 | loss: 0.38211 | val_0_rmse: 0.61372 | val_1_rmse: 0.63282 |  0:00:09s
epoch 42 | loss: 0.38247 | val_0_rmse: 0.60979 | val_1_rmse: 0.61606 |  0:00:09s
epoch 43 | loss: 0.36967 | val_0_rmse: 0.59421 | val_1_rmse: 0.59759 |  0:00:09s
epoch 44 | loss: 0.3684  | val_0_rmse: 0.61383 | val_1_rmse: 0.61428 |  0:00:10s
epoch 45 | loss: 0.38547 | val_0_rmse: 0.61117 | val_1_rmse: 0.62781 |  0:00:10s
epoch 46 | loss: 0.38296 | val_0_rmse: 0.59805 | val_1_rmse: 0.61368 |  0:00:10s
epoch 47 | loss: 0.3889  | val_0_rmse: 0.64179 | val_1_rmse: 0.60466 |  0:00:10s
epoch 48 | loss: 0.37291 | val_0_rmse: 0.63698 | val_1_rmse: 0.61871 |  0:00:11s
epoch 49 | loss: 0.37813 | val_0_rmse: 0.64653 | val_1_rmse: 0.62282 |  0:00:11s
epoch 50 | loss: 0.36658 | val_0_rmse: 0.60457 | val_1_rmse: 0.60417 |  0:00:11s
epoch 51 | loss: 0.36898 | val_0_rmse: 0.58936 | val_1_rmse: 0.62887 |  0:00:11s
epoch 52 | loss: 0.36022 | val_0_rmse: 0.58649 | val_1_rmse: 0.68413 |  0:00:11s
epoch 53 | loss: 0.37948 | val_0_rmse: 0.60718 | val_1_rmse: 0.63872 |  0:00:12s
epoch 54 | loss: 0.37476 | val_0_rmse: 0.61532 | val_1_rmse: 0.62609 |  0:00:12s
epoch 55 | loss: 0.35304 | val_0_rmse: 0.60607 | val_1_rmse: 0.6105  |  0:00:12s
epoch 56 | loss: 0.36252 | val_0_rmse: 0.60417 | val_1_rmse: 0.6101  |  0:00:12s
epoch 57 | loss: 0.36098 | val_0_rmse: 0.62926 | val_1_rmse: 0.64309 |  0:00:13s
epoch 58 | loss: 0.37494 | val_0_rmse: 0.60734 | val_1_rmse: 0.63555 |  0:00:13s
epoch 59 | loss: 0.35216 | val_0_rmse: 0.58652 | val_1_rmse: 0.60638 |  0:00:13s
epoch 60 | loss: 0.36337 | val_0_rmse: 0.62226 | val_1_rmse: 0.60638 |  0:00:13s
epoch 61 | loss: 0.37171 | val_0_rmse: 0.59991 | val_1_rmse: 0.59197 |  0:00:13s
epoch 62 | loss: 0.35147 | val_0_rmse: 0.59549 | val_1_rmse: 0.58486 |  0:00:14s
epoch 63 | loss: 0.36877 | val_0_rmse: 0.60347 | val_1_rmse: 0.6463  |  0:00:14s
epoch 64 | loss: 0.36966 | val_0_rmse: 0.62936 | val_1_rmse: 0.64572 |  0:00:14s
epoch 65 | loss: 0.37244 | val_0_rmse: 0.59184 | val_1_rmse: 0.6236  |  0:00:14s
epoch 66 | loss: 0.36025 | val_0_rmse: 0.58604 | val_1_rmse: 0.60743 |  0:00:15s
epoch 67 | loss: 0.36016 | val_0_rmse: 0.61639 | val_1_rmse: 0.61977 |  0:00:15s

Early stopping occured at epoch 67 with best_epoch = 37 and best_val_1_rmse = 0.58393
Best weights from best epoch are automatically used!
ended training at: 11:01:17
Feature importance:
[('Area', 0.2808219374036062), ('Baths', 0.1978370334225439), ('Beds', 0.0768696617153849), ('Latitude', 0.16898581891661452), ('Longitude', 0.1020406596205174), ('Month', 0.02217744692138194), ('Year', 0.15126744199995115)]
Mean squared error is of 3374977637.35168
Mean absolute error:40310.965620404415
MAPE:0.36746512067029996
R2 score:0.6131405945136837
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:01:18
epoch 0  | loss: 1.28941 | val_0_rmse: 1.249   | val_1_rmse: 1.32371 |  0:00:00s
epoch 1  | loss: 0.74326 | val_0_rmse: 0.87697 | val_1_rmse: 1.11567 |  0:00:00s
epoch 2  | loss: 0.61632 | val_0_rmse: 0.80708 | val_1_rmse: 0.85297 |  0:00:00s
epoch 3  | loss: 0.58504 | val_0_rmse: 0.76001 | val_1_rmse: 0.79596 |  0:00:00s
epoch 4  | loss: 0.54118 | val_0_rmse: 0.83153 | val_1_rmse: 0.8987  |  0:00:01s
epoch 5  | loss: 0.50627 | val_0_rmse: 0.73375 | val_1_rmse: 0.7654  |  0:00:01s
epoch 6  | loss: 0.48563 | val_0_rmse: 0.74451 | val_1_rmse: 0.78911 |  0:00:01s
epoch 7  | loss: 0.45578 | val_0_rmse: 0.66477 | val_1_rmse: 0.71909 |  0:00:01s
epoch 8  | loss: 0.43456 | val_0_rmse: 0.67158 | val_1_rmse: 0.73772 |  0:00:02s
epoch 9  | loss: 0.43945 | val_0_rmse: 0.66352 | val_1_rmse: 0.73753 |  0:00:02s
epoch 10 | loss: 0.43303 | val_0_rmse: 0.67253 | val_1_rmse: 0.7362  |  0:00:02s
epoch 11 | loss: 0.41387 | val_0_rmse: 0.65172 | val_1_rmse: 0.69898 |  0:00:02s
epoch 12 | loss: 0.41609 | val_0_rmse: 0.65824 | val_1_rmse: 0.70417 |  0:00:02s
epoch 13 | loss: 0.4089  | val_0_rmse: 0.67647 | val_1_rmse: 0.72675 |  0:00:03s
epoch 14 | loss: 0.42579 | val_0_rmse: 0.65227 | val_1_rmse: 0.70505 |  0:00:03s
epoch 15 | loss: 0.40398 | val_0_rmse: 0.64623 | val_1_rmse: 0.70663 |  0:00:03s
epoch 16 | loss: 0.3953  | val_0_rmse: 0.65752 | val_1_rmse: 0.70515 |  0:00:03s
epoch 17 | loss: 0.40327 | val_0_rmse: 0.65046 | val_1_rmse: 0.69447 |  0:00:04s
epoch 18 | loss: 0.39852 | val_0_rmse: 0.65281 | val_1_rmse: 0.68979 |  0:00:04s
epoch 19 | loss: 0.4155  | val_0_rmse: 0.64342 | val_1_rmse: 0.69449 |  0:00:04s
epoch 20 | loss: 0.41949 | val_0_rmse: 0.64548 | val_1_rmse: 0.70355 |  0:00:04s
epoch 21 | loss: 0.41271 | val_0_rmse: 0.63338 | val_1_rmse: 0.68643 |  0:00:04s
epoch 22 | loss: 0.39268 | val_0_rmse: 0.65154 | val_1_rmse: 0.70762 |  0:00:05s
epoch 23 | loss: 0.38894 | val_0_rmse: 0.62986 | val_1_rmse: 0.68616 |  0:00:05s
epoch 24 | loss: 0.38074 | val_0_rmse: 0.63468 | val_1_rmse: 0.69659 |  0:00:05s
epoch 25 | loss: 0.36991 | val_0_rmse: 0.62382 | val_1_rmse: 0.68133 |  0:00:05s
epoch 26 | loss: 0.37518 | val_0_rmse: 0.6246  | val_1_rmse: 0.6833  |  0:00:06s
epoch 27 | loss: 0.3682  | val_0_rmse: 0.6168  | val_1_rmse: 0.67952 |  0:00:06s
epoch 28 | loss: 0.36379 | val_0_rmse: 0.61347 | val_1_rmse: 0.6732  |  0:00:06s
epoch 29 | loss: 0.3665  | val_0_rmse: 0.60863 | val_1_rmse: 0.66527 |  0:00:06s
epoch 30 | loss: 0.37963 | val_0_rmse: 0.63101 | val_1_rmse: 0.6887  |  0:00:07s
epoch 31 | loss: 0.37034 | val_0_rmse: 0.62317 | val_1_rmse: 0.68412 |  0:00:07s
epoch 32 | loss: 0.35924 | val_0_rmse: 0.63974 | val_1_rmse: 0.69765 |  0:00:07s
epoch 33 | loss: 0.37243 | val_0_rmse: 0.61236 | val_1_rmse: 0.67586 |  0:00:07s
epoch 34 | loss: 0.36881 | val_0_rmse: 0.6519  | val_1_rmse: 0.72335 |  0:00:07s
epoch 35 | loss: 0.37658 | val_0_rmse: 0.60857 | val_1_rmse: 0.67211 |  0:00:08s
epoch 36 | loss: 0.37608 | val_0_rmse: 0.60402 | val_1_rmse: 0.6739  |  0:00:08s
epoch 37 | loss: 0.37339 | val_0_rmse: 0.63658 | val_1_rmse: 0.69244 |  0:00:08s
epoch 38 | loss: 0.36183 | val_0_rmse: 0.60244 | val_1_rmse: 0.67115 |  0:00:08s
epoch 39 | loss: 0.37024 | val_0_rmse: 0.61009 | val_1_rmse: 0.67857 |  0:00:09s
epoch 40 | loss: 0.35876 | val_0_rmse: 0.61274 | val_1_rmse: 0.67342 |  0:00:09s
epoch 41 | loss: 0.3688  | val_0_rmse: 0.60221 | val_1_rmse: 0.66404 |  0:00:09s
epoch 42 | loss: 0.34354 | val_0_rmse: 0.60191 | val_1_rmse: 0.66297 |  0:00:09s
epoch 43 | loss: 0.36317 | val_0_rmse: 0.60632 | val_1_rmse: 0.67064 |  0:00:09s
epoch 44 | loss: 0.34943 | val_0_rmse: 0.59824 | val_1_rmse: 0.66968 |  0:00:10s
epoch 45 | loss: 0.35002 | val_0_rmse: 0.61931 | val_1_rmse: 0.69255 |  0:00:10s
epoch 46 | loss: 0.34418 | val_0_rmse: 0.6119  | val_1_rmse: 0.6859  |  0:00:10s
epoch 47 | loss: 0.34357 | val_0_rmse: 0.59608 | val_1_rmse: 0.66908 |  0:00:10s
epoch 48 | loss: 0.3504  | val_0_rmse: 0.59554 | val_1_rmse: 0.67271 |  0:00:11s
epoch 49 | loss: 0.33891 | val_0_rmse: 0.60972 | val_1_rmse: 0.6856  |  0:00:11s
epoch 50 | loss: 0.33806 | val_0_rmse: 0.59372 | val_1_rmse: 0.66884 |  0:00:11s
epoch 51 | loss: 0.3319  | val_0_rmse: 0.5893  | val_1_rmse: 0.66085 |  0:00:11s
epoch 52 | loss: 0.34302 | val_0_rmse: 0.59477 | val_1_rmse: 0.67594 |  0:00:11s
epoch 53 | loss: 0.3468  | val_0_rmse: 0.60298 | val_1_rmse: 0.7039  |  0:00:12s
epoch 54 | loss: 0.34172 | val_0_rmse: 0.627   | val_1_rmse: 0.72052 |  0:00:12s
epoch 55 | loss: 0.34202 | val_0_rmse: 0.62337 | val_1_rmse: 0.70556 |  0:00:12s
epoch 56 | loss: 0.3344  | val_0_rmse: 0.61891 | val_1_rmse: 0.692   |  0:00:12s
epoch 57 | loss: 0.34453 | val_0_rmse: 0.59425 | val_1_rmse: 0.66927 |  0:00:13s
epoch 58 | loss: 0.33909 | val_0_rmse: 0.63345 | val_1_rmse: 0.70444 |  0:00:13s
epoch 59 | loss: 0.34277 | val_0_rmse: 0.61067 | val_1_rmse: 0.69231 |  0:00:13s
epoch 60 | loss: 0.33786 | val_0_rmse: 0.62122 | val_1_rmse: 0.694   |  0:00:13s
epoch 61 | loss: 0.33285 | val_0_rmse: 0.59514 | val_1_rmse: 0.6691  |  0:00:13s
epoch 62 | loss: 0.3282  | val_0_rmse: 0.58628 | val_1_rmse: 0.6723  |  0:00:14s
epoch 63 | loss: 0.32754 | val_0_rmse: 0.58812 | val_1_rmse: 0.67249 |  0:00:14s
epoch 64 | loss: 0.33466 | val_0_rmse: 0.58694 | val_1_rmse: 0.67492 |  0:00:14s
epoch 65 | loss: 0.33138 | val_0_rmse: 0.60201 | val_1_rmse: 0.69955 |  0:00:14s
epoch 66 | loss: 0.3475  | val_0_rmse: 0.59726 | val_1_rmse: 0.68489 |  0:00:15s
epoch 67 | loss: 0.33093 | val_0_rmse: 0.60183 | val_1_rmse: 0.68541 |  0:00:15s
epoch 68 | loss: 0.33364 | val_0_rmse: 0.5978  | val_1_rmse: 0.67696 |  0:00:15s
epoch 69 | loss: 0.34062 | val_0_rmse: 0.60089 | val_1_rmse: 0.69593 |  0:00:15s
epoch 70 | loss: 0.33626 | val_0_rmse: 0.59289 | val_1_rmse: 0.68499 |  0:00:15s
epoch 71 | loss: 0.33454 | val_0_rmse: 0.59418 | val_1_rmse: 0.67711 |  0:00:16s
epoch 72 | loss: 0.341   | val_0_rmse: 0.5802  | val_1_rmse: 0.66349 |  0:00:16s
epoch 73 | loss: 0.33277 | val_0_rmse: 0.60366 | val_1_rmse: 0.67755 |  0:00:16s
epoch 74 | loss: 0.32781 | val_0_rmse: 0.58775 | val_1_rmse: 0.67145 |  0:00:16s
epoch 75 | loss: 0.33032 | val_0_rmse: 0.57686 | val_1_rmse: 0.65633 |  0:00:17s
epoch 76 | loss: 0.33091 | val_0_rmse: 0.58092 | val_1_rmse: 0.657   |  0:00:17s
epoch 77 | loss: 0.32746 | val_0_rmse: 0.58684 | val_1_rmse: 0.68715 |  0:00:17s
epoch 78 | loss: 0.32144 | val_0_rmse: 0.58463 | val_1_rmse: 0.68699 |  0:00:17s
epoch 79 | loss: 0.33064 | val_0_rmse: 0.59886 | val_1_rmse: 0.70485 |  0:00:17s
epoch 80 | loss: 0.32806 | val_0_rmse: 0.59831 | val_1_rmse: 0.68148 |  0:00:18s
epoch 81 | loss: 0.32521 | val_0_rmse: 0.5933  | val_1_rmse: 0.67574 |  0:00:18s
epoch 82 | loss: 0.32698 | val_0_rmse: 0.59292 | val_1_rmse: 0.67685 |  0:00:18s
epoch 83 | loss: 0.34102 | val_0_rmse: 0.58568 | val_1_rmse: 0.67335 |  0:00:18s
epoch 84 | loss: 0.32396 | val_0_rmse: 0.60389 | val_1_rmse: 0.6821  |  0:00:19s
epoch 85 | loss: 0.32396 | val_0_rmse: 0.57809 | val_1_rmse: 0.66286 |  0:00:19s
epoch 86 | loss: 0.32724 | val_0_rmse: 0.57749 | val_1_rmse: 0.65867 |  0:00:19s
epoch 87 | loss: 0.31479 | val_0_rmse: 0.57817 | val_1_rmse: 0.6561  |  0:00:19s
epoch 88 | loss: 0.31783 | val_0_rmse: 0.58037 | val_1_rmse: 0.66786 |  0:00:20s
epoch 89 | loss: 0.31173 | val_0_rmse: 0.58014 | val_1_rmse: 0.69817 |  0:00:20s
epoch 90 | loss: 0.31885 | val_0_rmse: 0.59934 | val_1_rmse: 0.71928 |  0:00:20s
epoch 91 | loss: 0.32606 | val_0_rmse: 0.58836 | val_1_rmse: 0.66746 |  0:00:20s
epoch 92 | loss: 0.33407 | val_0_rmse: 0.58621 | val_1_rmse: 0.66745 |  0:00:20s
epoch 93 | loss: 0.32822 | val_0_rmse: 0.58159 | val_1_rmse: 0.65964 |  0:00:21s
epoch 94 | loss: 0.33516 | val_0_rmse: 0.57927 | val_1_rmse: 0.66015 |  0:00:21s
epoch 95 | loss: 0.33001 | val_0_rmse: 0.59443 | val_1_rmse: 0.67543 |  0:00:21s
epoch 96 | loss: 0.33141 | val_0_rmse: 0.59411 | val_1_rmse: 0.70775 |  0:00:21s
epoch 97 | loss: 0.32465 | val_0_rmse: 0.57525 | val_1_rmse: 0.66814 |  0:00:22s
epoch 98 | loss: 0.32896 | val_0_rmse: 0.57933 | val_1_rmse: 0.67123 |  0:00:22s
epoch 99 | loss: 0.34005 | val_0_rmse: 0.59349 | val_1_rmse: 0.67102 |  0:00:22s
epoch 100| loss: 0.33672 | val_0_rmse: 0.61587 | val_1_rmse: 0.70125 |  0:00:22s
epoch 101| loss: 0.3338  | val_0_rmse: 0.59424 | val_1_rmse: 0.69792 |  0:00:22s
epoch 102| loss: 0.32837 | val_0_rmse: 0.60347 | val_1_rmse: 0.69791 |  0:00:23s
epoch 103| loss: 0.3423  | val_0_rmse: 0.59013 | val_1_rmse: 0.68699 |  0:00:23s
epoch 104| loss: 0.33292 | val_0_rmse: 0.62407 | val_1_rmse: 0.71245 |  0:00:23s
epoch 105| loss: 0.32712 | val_0_rmse: 0.58156 | val_1_rmse: 0.67496 |  0:00:23s
epoch 106| loss: 0.32866 | val_0_rmse: 0.60005 | val_1_rmse: 0.69363 |  0:00:24s
epoch 107| loss: 0.34244 | val_0_rmse: 0.57453 | val_1_rmse: 0.66627 |  0:00:24s
epoch 108| loss: 0.33269 | val_0_rmse: 0.59718 | val_1_rmse: 0.70221 |  0:00:24s
epoch 109| loss: 0.3285  | val_0_rmse: 0.65904 | val_1_rmse: 0.74432 |  0:00:24s
epoch 110| loss: 0.32631 | val_0_rmse: 0.65246 | val_1_rmse: 0.7352  |  0:00:24s
epoch 111| loss: 0.33033 | val_0_rmse: 0.5945  | val_1_rmse: 0.68736 |  0:00:25s
epoch 112| loss: 0.31633 | val_0_rmse: 0.57565 | val_1_rmse: 0.69187 |  0:00:25s
epoch 113| loss: 0.33655 | val_0_rmse: 0.59177 | val_1_rmse: 0.69053 |  0:00:25s
epoch 114| loss: 0.32417 | val_0_rmse: 0.58592 | val_1_rmse: 0.68179 |  0:00:25s
epoch 115| loss: 0.33248 | val_0_rmse: 0.5985  | val_1_rmse: 0.69097 |  0:00:26s
epoch 116| loss: 0.32029 | val_0_rmse: 0.61343 | val_1_rmse: 0.68426 |  0:00:26s
epoch 117| loss: 0.32462 | val_0_rmse: 0.62286 | val_1_rmse: 0.6888  |  0:00:26s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.6561
Best weights from best epoch are automatically used!
ended training at: 11:01:44
Feature importance:
[('Area', 0.37648550717156987), ('Baths', 0.10617138072656859), ('Beds', 0.0007285321438400298), ('Latitude', 0.1777367076246694), ('Longitude', 0.2573315955138903), ('Month', 0.0), ('Year', 0.08154627681946179)]
Mean squared error is of 3250778281.362459
Mean absolute error:39622.01624144514
MAPE:0.3607057797726817
R2 score:0.5937740891789915
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:01:44
epoch 0  | loss: 1.32701 | val_0_rmse: 0.97165 | val_1_rmse: 0.93979 |  0:00:00s
epoch 1  | loss: 0.64733 | val_0_rmse: 0.85995 | val_1_rmse: 0.83677 |  0:00:00s
epoch 2  | loss: 0.52876 | val_0_rmse: 1.22284 | val_1_rmse: 0.88139 |  0:00:00s
epoch 3  | loss: 0.55684 | val_0_rmse: 0.81238 | val_1_rmse: 0.83631 |  0:00:00s
epoch 4  | loss: 0.52261 | val_0_rmse: 0.77104 | val_1_rmse: 0.78802 |  0:00:01s
epoch 5  | loss: 0.51372 | val_0_rmse: 0.74422 | val_1_rmse: 0.75184 |  0:00:01s
epoch 6  | loss: 0.51228 | val_0_rmse: 0.74548 | val_1_rmse: 0.76531 |  0:00:01s
epoch 7  | loss: 0.49541 | val_0_rmse: 0.7011  | val_1_rmse: 0.72248 |  0:00:01s
epoch 8  | loss: 0.48111 | val_0_rmse: 0.69515 | val_1_rmse: 0.71845 |  0:00:02s
epoch 9  | loss: 0.46536 | val_0_rmse: 0.69261 | val_1_rmse: 0.72131 |  0:00:02s
epoch 10 | loss: 0.47218 | val_0_rmse: 0.68396 | val_1_rmse: 0.70978 |  0:00:02s
epoch 11 | loss: 0.46772 | val_0_rmse: 0.6723  | val_1_rmse: 0.69913 |  0:00:02s
epoch 12 | loss: 0.45558 | val_0_rmse: 0.66748 | val_1_rmse: 0.70004 |  0:00:02s
epoch 13 | loss: 0.45139 | val_0_rmse: 0.66213 | val_1_rmse: 0.68781 |  0:00:03s
epoch 14 | loss: 0.43741 | val_0_rmse: 0.66146 | val_1_rmse: 0.69631 |  0:00:03s
epoch 15 | loss: 0.45075 | val_0_rmse: 0.65794 | val_1_rmse: 0.69161 |  0:00:03s
epoch 16 | loss: 0.45308 | val_0_rmse: 0.64941 | val_1_rmse: 0.68376 |  0:00:03s
epoch 17 | loss: 0.44071 | val_0_rmse: 0.66504 | val_1_rmse: 0.67842 |  0:00:04s
epoch 18 | loss: 0.43851 | val_0_rmse: 0.67746 | val_1_rmse: 0.6877  |  0:00:04s
epoch 19 | loss: 0.45344 | val_0_rmse: 0.66141 | val_1_rmse: 0.67034 |  0:00:04s
epoch 20 | loss: 0.42961 | val_0_rmse: 0.65196 | val_1_rmse: 0.67496 |  0:00:04s
epoch 21 | loss: 0.43821 | val_0_rmse: 0.65585 | val_1_rmse: 0.6762  |  0:00:05s
epoch 22 | loss: 0.42907 | val_0_rmse: 0.65549 | val_1_rmse: 0.68961 |  0:00:05s
epoch 23 | loss: 0.41368 | val_0_rmse: 0.64529 | val_1_rmse: 0.68057 |  0:00:05s
epoch 24 | loss: 0.41726 | val_0_rmse: 0.65647 | val_1_rmse: 0.69714 |  0:00:05s
epoch 25 | loss: 0.42456 | val_0_rmse: 0.65406 | val_1_rmse: 0.70975 |  0:00:05s
epoch 26 | loss: 0.42077 | val_0_rmse: 0.65287 | val_1_rmse: 0.68797 |  0:00:06s
epoch 27 | loss: 0.41734 | val_0_rmse: 0.63962 | val_1_rmse: 0.6615  |  0:00:06s
epoch 28 | loss: 0.42274 | val_0_rmse: 0.64404 | val_1_rmse: 0.66196 |  0:00:06s
epoch 29 | loss: 0.42589 | val_0_rmse: 0.63742 | val_1_rmse: 0.67809 |  0:00:06s
epoch 30 | loss: 0.41412 | val_0_rmse: 0.64274 | val_1_rmse: 0.67934 |  0:00:07s
epoch 31 | loss: 0.44054 | val_0_rmse: 0.65012 | val_1_rmse: 0.68221 |  0:00:07s
epoch 32 | loss: 0.41197 | val_0_rmse: 0.64462 | val_1_rmse: 0.67242 |  0:00:07s
epoch 33 | loss: 0.40141 | val_0_rmse: 0.65377 | val_1_rmse: 0.67479 |  0:00:07s
epoch 34 | loss: 0.39357 | val_0_rmse: 0.63412 | val_1_rmse: 0.67419 |  0:00:07s
epoch 35 | loss: 0.39647 | val_0_rmse: 0.62572 | val_1_rmse: 0.66855 |  0:00:08s
epoch 36 | loss: 0.40234 | val_0_rmse: 0.63977 | val_1_rmse: 0.68937 |  0:00:08s
epoch 37 | loss: 0.39923 | val_0_rmse: 0.63694 | val_1_rmse: 0.66781 |  0:00:08s
epoch 38 | loss: 0.40846 | val_0_rmse: 0.63839 | val_1_rmse: 0.66219 |  0:00:08s
epoch 39 | loss: 0.40533 | val_0_rmse: 0.63613 | val_1_rmse: 0.66674 |  0:00:09s
epoch 40 | loss: 0.40083 | val_0_rmse: 0.6331  | val_1_rmse: 0.65332 |  0:00:09s
epoch 41 | loss: 0.39929 | val_0_rmse: 0.63041 | val_1_rmse: 0.6524  |  0:00:09s
epoch 42 | loss: 0.39636 | val_0_rmse: 0.62768 | val_1_rmse: 0.65539 |  0:00:09s
epoch 43 | loss: 0.38796 | val_0_rmse: 0.63094 | val_1_rmse: 0.65527 |  0:00:09s
epoch 44 | loss: 0.37997 | val_0_rmse: 0.6269  | val_1_rmse: 0.67557 |  0:00:10s
epoch 45 | loss: 0.40139 | val_0_rmse: 0.63315 | val_1_rmse: 0.66703 |  0:00:10s
epoch 46 | loss: 0.43277 | val_0_rmse: 0.69221 | val_1_rmse: 0.6983  |  0:00:10s
epoch 47 | loss: 0.44292 | val_0_rmse: 0.6729  | val_1_rmse: 0.6839  |  0:00:10s
epoch 48 | loss: 0.42949 | val_0_rmse: 0.62493 | val_1_rmse: 0.66922 |  0:00:11s
epoch 49 | loss: 0.41016 | val_0_rmse: 0.65078 | val_1_rmse: 0.68734 |  0:00:11s
epoch 50 | loss: 0.40145 | val_0_rmse: 0.6464  | val_1_rmse: 0.67988 |  0:00:11s
epoch 51 | loss: 0.39493 | val_0_rmse: 0.62523 | val_1_rmse: 0.66232 |  0:00:11s
epoch 52 | loss: 0.39775 | val_0_rmse: 0.65988 | val_1_rmse: 0.68661 |  0:00:11s
epoch 53 | loss: 0.41333 | val_0_rmse: 0.63742 | val_1_rmse: 0.6695  |  0:00:12s
epoch 54 | loss: 0.40279 | val_0_rmse: 0.62552 | val_1_rmse: 0.66386 |  0:00:12s
epoch 55 | loss: 0.38826 | val_0_rmse: 0.63378 | val_1_rmse: 0.6696  |  0:00:12s
epoch 56 | loss: 0.36928 | val_0_rmse: 0.63025 | val_1_rmse: 0.69805 |  0:00:12s
epoch 57 | loss: 0.37619 | val_0_rmse: 0.61486 | val_1_rmse: 0.69927 |  0:00:13s
epoch 58 | loss: 0.36916 | val_0_rmse: 0.62122 | val_1_rmse: 0.71128 |  0:00:13s
epoch 59 | loss: 0.37359 | val_0_rmse: 0.62763 | val_1_rmse: 0.70066 |  0:00:13s
epoch 60 | loss: 0.37569 | val_0_rmse: 0.61061 | val_1_rmse: 0.67293 |  0:00:13s
epoch 61 | loss: 0.36495 | val_0_rmse: 0.61093 | val_1_rmse: 0.65949 |  0:00:13s
epoch 62 | loss: 0.34886 | val_0_rmse: 0.6033  | val_1_rmse: 0.65617 |  0:00:14s
epoch 63 | loss: 0.35347 | val_0_rmse: 0.59294 | val_1_rmse: 0.64324 |  0:00:14s
epoch 64 | loss: 0.35168 | val_0_rmse: 0.60221 | val_1_rmse: 0.65601 |  0:00:14s
epoch 65 | loss: 0.34894 | val_0_rmse: 0.60732 | val_1_rmse: 0.66134 |  0:00:14s
epoch 66 | loss: 0.35004 | val_0_rmse: 0.59778 | val_1_rmse: 0.6506  |  0:00:15s
epoch 67 | loss: 0.35234 | val_0_rmse: 0.60988 | val_1_rmse: 0.66611 |  0:00:15s
epoch 68 | loss: 0.35541 | val_0_rmse: 0.60567 | val_1_rmse: 0.66898 |  0:00:15s
epoch 69 | loss: 0.35607 | val_0_rmse: 0.59942 | val_1_rmse: 0.66359 |  0:00:15s
epoch 70 | loss: 0.35642 | val_0_rmse: 0.61616 | val_1_rmse: 0.65262 |  0:00:15s
epoch 71 | loss: 0.3707  | val_0_rmse: 0.61726 | val_1_rmse: 0.66487 |  0:00:16s
epoch 72 | loss: 0.37039 | val_0_rmse: 0.6043  | val_1_rmse: 0.63939 |  0:00:16s
epoch 73 | loss: 0.37434 | val_0_rmse: 0.6131  | val_1_rmse: 0.65013 |  0:00:16s
epoch 74 | loss: 0.37367 | val_0_rmse: 0.62656 | val_1_rmse: 0.66339 |  0:00:16s
epoch 75 | loss: 0.36387 | val_0_rmse: 0.5978  | val_1_rmse: 0.64412 |  0:00:17s
epoch 76 | loss: 0.37124 | val_0_rmse: 0.6051  | val_1_rmse: 0.64674 |  0:00:17s
epoch 77 | loss: 0.36181 | val_0_rmse: 0.61239 | val_1_rmse: 0.64988 |  0:00:17s
epoch 78 | loss: 0.35706 | val_0_rmse: 0.62601 | val_1_rmse: 0.66652 |  0:00:17s
epoch 79 | loss: 0.35314 | val_0_rmse: 0.62912 | val_1_rmse: 0.66124 |  0:00:18s
epoch 80 | loss: 0.3662  | val_0_rmse: 0.63237 | val_1_rmse: 0.64497 |  0:00:18s
epoch 81 | loss: 0.36476 | val_0_rmse: 0.60082 | val_1_rmse: 0.62903 |  0:00:18s
epoch 82 | loss: 0.33847 | val_0_rmse: 0.61433 | val_1_rmse: 0.64562 |  0:00:18s
epoch 83 | loss: 0.3645  | val_0_rmse: 0.63813 | val_1_rmse: 0.6558  |  0:00:18s
epoch 84 | loss: 0.35266 | val_0_rmse: 0.63251 | val_1_rmse: 0.64655 |  0:00:19s
epoch 85 | loss: 0.34269 | val_0_rmse: 0.60689 | val_1_rmse: 0.63738 |  0:00:19s
epoch 86 | loss: 0.34515 | val_0_rmse: 0.59873 | val_1_rmse: 0.64208 |  0:00:19s
epoch 87 | loss: 0.34062 | val_0_rmse: 0.58363 | val_1_rmse: 0.63643 |  0:00:19s
epoch 88 | loss: 0.34358 | val_0_rmse: 0.57963 | val_1_rmse: 0.64171 |  0:00:20s
epoch 89 | loss: 0.34131 | val_0_rmse: 0.5892  | val_1_rmse: 0.64449 |  0:00:20s
epoch 90 | loss: 0.34173 | val_0_rmse: 0.61476 | val_1_rmse: 0.64468 |  0:00:20s
epoch 91 | loss: 0.33956 | val_0_rmse: 0.61747 | val_1_rmse: 0.64617 |  0:00:20s
epoch 92 | loss: 0.33956 | val_0_rmse: 0.6006  | val_1_rmse: 0.6254  |  0:00:20s
epoch 93 | loss: 0.34013 | val_0_rmse: 0.59296 | val_1_rmse: 0.62151 |  0:00:21s
epoch 94 | loss: 0.35165 | val_0_rmse: 0.60632 | val_1_rmse: 0.62822 |  0:00:21s
epoch 95 | loss: 0.34111 | val_0_rmse: 0.60061 | val_1_rmse: 0.62486 |  0:00:21s
epoch 96 | loss: 0.33701 | val_0_rmse: 0.61291 | val_1_rmse: 0.63419 |  0:00:21s
epoch 97 | loss: 0.33265 | val_0_rmse: 0.6031  | val_1_rmse: 0.62929 |  0:00:22s
epoch 98 | loss: 0.33802 | val_0_rmse: 0.58746 | val_1_rmse: 0.6284  |  0:00:22s
epoch 99 | loss: 0.32553 | val_0_rmse: 0.58126 | val_1_rmse: 0.62818 |  0:00:22s
epoch 100| loss: 0.33431 | val_0_rmse: 0.58957 | val_1_rmse: 0.6227  |  0:00:22s
epoch 101| loss: 0.33375 | val_0_rmse: 0.57654 | val_1_rmse: 0.62432 |  0:00:22s
epoch 102| loss: 0.34572 | val_0_rmse: 0.60836 | val_1_rmse: 0.62995 |  0:00:23s
epoch 103| loss: 0.33234 | val_0_rmse: 0.5864  | val_1_rmse: 0.61619 |  0:00:23s
epoch 104| loss: 0.32431 | val_0_rmse: 0.61262 | val_1_rmse: 0.63329 |  0:00:23s
epoch 105| loss: 0.33439 | val_0_rmse: 0.60615 | val_1_rmse: 0.62886 |  0:00:23s
epoch 106| loss: 0.31384 | val_0_rmse: 0.61411 | val_1_rmse: 0.63416 |  0:00:24s
epoch 107| loss: 0.33317 | val_0_rmse: 0.58446 | val_1_rmse: 0.61947 |  0:00:24s
epoch 108| loss: 0.31811 | val_0_rmse: 0.58052 | val_1_rmse: 0.62457 |  0:00:24s
epoch 109| loss: 0.32897 | val_0_rmse: 0.56862 | val_1_rmse: 0.60931 |  0:00:24s
epoch 110| loss: 0.3224  | val_0_rmse: 0.56299 | val_1_rmse: 0.61997 |  0:00:24s
epoch 111| loss: 0.32729 | val_0_rmse: 0.56407 | val_1_rmse: 0.61381 |  0:00:25s
epoch 112| loss: 0.31974 | val_0_rmse: 0.57039 | val_1_rmse: 0.60685 |  0:00:25s
epoch 113| loss: 0.3203  | val_0_rmse: 0.5663  | val_1_rmse: 0.6082  |  0:00:25s
epoch 114| loss: 0.31282 | val_0_rmse: 0.57232 | val_1_rmse: 0.60832 |  0:00:25s
epoch 115| loss: 0.31598 | val_0_rmse: 0.57305 | val_1_rmse: 0.61986 |  0:00:26s
epoch 116| loss: 0.32256 | val_0_rmse: 0.57007 | val_1_rmse: 0.60741 |  0:00:26s
epoch 117| loss: 0.3154  | val_0_rmse: 0.57569 | val_1_rmse: 0.61367 |  0:00:26s
epoch 118| loss: 0.31503 | val_0_rmse: 0.57339 | val_1_rmse: 0.61209 |  0:00:26s
epoch 119| loss: 0.30452 | val_0_rmse: 0.56111 | val_1_rmse: 0.61011 |  0:00:27s
epoch 120| loss: 0.3294  | val_0_rmse: 0.5737  | val_1_rmse: 0.62351 |  0:00:27s
epoch 121| loss: 0.32295 | val_0_rmse: 0.56951 | val_1_rmse: 0.60006 |  0:00:27s
epoch 122| loss: 0.31222 | val_0_rmse: 0.58104 | val_1_rmse: 0.61744 |  0:00:27s
epoch 123| loss: 0.31574 | val_0_rmse: 0.5686  | val_1_rmse: 0.61021 |  0:00:27s
epoch 124| loss: 0.30928 | val_0_rmse: 0.56535 | val_1_rmse: 0.6034  |  0:00:28s
epoch 125| loss: 0.31676 | val_0_rmse: 0.58102 | val_1_rmse: 0.6225  |  0:00:28s
epoch 126| loss: 0.30479 | val_0_rmse: 0.55254 | val_1_rmse: 0.60189 |  0:00:28s
epoch 127| loss: 0.30996 | val_0_rmse: 0.56255 | val_1_rmse: 0.60758 |  0:00:28s
epoch 128| loss: 0.31409 | val_0_rmse: 0.57176 | val_1_rmse: 0.62083 |  0:00:29s
epoch 129| loss: 0.29889 | val_0_rmse: 0.55748 | val_1_rmse: 0.6145  |  0:00:29s
epoch 130| loss: 0.31251 | val_0_rmse: 0.5733  | val_1_rmse: 0.63022 |  0:00:29s
epoch 131| loss: 0.31246 | val_0_rmse: 0.55163 | val_1_rmse: 0.60872 |  0:00:29s
epoch 132| loss: 0.31252 | val_0_rmse: 0.57878 | val_1_rmse: 0.61292 |  0:00:29s
epoch 133| loss: 0.31364 | val_0_rmse: 0.58443 | val_1_rmse: 0.62119 |  0:00:30s
epoch 134| loss: 0.31311 | val_0_rmse: 0.56274 | val_1_rmse: 0.607   |  0:00:30s
epoch 135| loss: 0.30438 | val_0_rmse: 0.5859  | val_1_rmse: 0.62787 |  0:00:30s
epoch 136| loss: 0.31182 | val_0_rmse: 0.56986 | val_1_rmse: 0.60888 |  0:00:30s
epoch 137| loss: 0.31004 | val_0_rmse: 0.56761 | val_1_rmse: 0.61551 |  0:00:31s
epoch 138| loss: 0.31364 | val_0_rmse: 0.57192 | val_1_rmse: 0.61998 |  0:00:31s
epoch 139| loss: 0.31285 | val_0_rmse: 0.61176 | val_1_rmse: 0.6444  |  0:00:31s
epoch 140| loss: 0.31567 | val_0_rmse: 0.60547 | val_1_rmse: 0.64001 |  0:00:31s
epoch 141| loss: 0.31248 | val_0_rmse: 0.57318 | val_1_rmse: 0.62114 |  0:00:31s
epoch 142| loss: 0.31293 | val_0_rmse: 0.5863  | val_1_rmse: 0.62022 |  0:00:32s
epoch 143| loss: 0.30636 | val_0_rmse: 0.55969 | val_1_rmse: 0.60655 |  0:00:32s
epoch 144| loss: 0.31084 | val_0_rmse: 0.57023 | val_1_rmse: 0.61655 |  0:00:32s
epoch 145| loss: 0.30293 | val_0_rmse: 0.57129 | val_1_rmse: 0.62706 |  0:00:32s
epoch 146| loss: 0.30931 | val_0_rmse: 0.55618 | val_1_rmse: 0.6303  |  0:00:33s
epoch 147| loss: 0.31062 | val_0_rmse: 0.57851 | val_1_rmse: 0.64653 |  0:00:33s
epoch 148| loss: 0.30609 | val_0_rmse: 0.56738 | val_1_rmse: 0.62362 |  0:00:33s
epoch 149| loss: 0.29371 | val_0_rmse: 0.55944 | val_1_rmse: 0.61094 |  0:00:33s
Stop training because you reached max_epochs = 150 with best_epoch = 121 and best_val_1_rmse = 0.60006
Best weights from best epoch are automatically used!
ended training at: 11:02:18
Feature importance:
[('Area', 0.18332118065736505), ('Baths', 0.10083143578049249), ('Beds', 0.04919568771716193), ('Latitude', 0.26162990811960396), ('Longitude', 0.3597426031030374), ('Month', 0.0), ('Year', 0.04527918462233914)]
Mean squared error is of 3347392084.475862
Mean absolute error:37994.85271365951
MAPE:0.3763473336692048
R2 score:0.587135901055799
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:02:18
epoch 0  | loss: 0.46435 | val_0_rmse: 0.58858 | val_1_rmse: 0.59882 |  0:00:02s
epoch 1  | loss: 0.34518 | val_0_rmse: 0.56869 | val_1_rmse: 0.58181 |  0:00:05s
epoch 2  | loss: 0.3345  | val_0_rmse: 0.57377 | val_1_rmse: 0.58606 |  0:00:08s
epoch 3  | loss: 0.33192 | val_0_rmse: 0.57605 | val_1_rmse: 0.58815 |  0:00:11s
epoch 4  | loss: 0.33256 | val_0_rmse: 0.56838 | val_1_rmse: 0.58126 |  0:00:14s
epoch 5  | loss: 0.33342 | val_0_rmse: 0.567   | val_1_rmse: 0.57993 |  0:00:17s
epoch 6  | loss: 0.33602 | val_0_rmse: 0.566   | val_1_rmse: 0.57851 |  0:00:20s
epoch 7  | loss: 0.33087 | val_0_rmse: 0.57117 | val_1_rmse: 0.58632 |  0:00:23s
epoch 8  | loss: 0.33306 | val_0_rmse: 0.56627 | val_1_rmse: 0.57885 |  0:00:26s
epoch 9  | loss: 0.32529 | val_0_rmse: 0.56375 | val_1_rmse: 0.5754  |  0:00:29s
epoch 10 | loss: 0.32357 | val_0_rmse: 0.57234 | val_1_rmse: 0.5843  |  0:00:32s
epoch 11 | loss: 0.32564 | val_0_rmse: 0.56382 | val_1_rmse: 0.57781 |  0:00:34s
epoch 12 | loss: 0.32421 | val_0_rmse: 0.55906 | val_1_rmse: 0.573   |  0:00:37s
epoch 13 | loss: 0.31932 | val_0_rmse: 0.57066 | val_1_rmse: 0.58398 |  0:00:40s
epoch 14 | loss: 0.32409 | val_0_rmse: 0.55905 | val_1_rmse: 0.57254 |  0:00:43s
epoch 15 | loss: 0.32549 | val_0_rmse: 0.56849 | val_1_rmse: 0.58101 |  0:00:46s
epoch 16 | loss: 0.31975 | val_0_rmse: 0.55843 | val_1_rmse: 0.57129 |  0:00:49s
epoch 17 | loss: 0.31875 | val_0_rmse: 0.56063 | val_1_rmse: 0.57412 |  0:00:52s
epoch 18 | loss: 0.32162 | val_0_rmse: 0.5576  | val_1_rmse: 0.57164 |  0:00:55s
epoch 19 | loss: 0.31911 | val_0_rmse: 0.56661 | val_1_rmse: 0.57986 |  0:00:58s
epoch 20 | loss: 0.31782 | val_0_rmse: 0.55825 | val_1_rmse: 0.57143 |  0:01:01s
epoch 21 | loss: 0.31671 | val_0_rmse: 0.56069 | val_1_rmse: 0.5738  |  0:01:04s
epoch 22 | loss: 0.32062 | val_0_rmse: 0.56421 | val_1_rmse: 0.57795 |  0:01:07s
epoch 23 | loss: 0.31924 | val_0_rmse: 0.56071 | val_1_rmse: 0.57437 |  0:01:09s
epoch 24 | loss: 0.31919 | val_0_rmse: 0.55775 | val_1_rmse: 0.57078 |  0:01:12s
epoch 25 | loss: 0.31656 | val_0_rmse: 0.55846 | val_1_rmse: 0.57185 |  0:01:15s
epoch 26 | loss: 0.31539 | val_0_rmse: 0.5559  | val_1_rmse: 0.56831 |  0:01:18s
epoch 27 | loss: 0.31444 | val_0_rmse: 0.56145 | val_1_rmse: 0.57447 |  0:01:21s
epoch 28 | loss: 0.31658 | val_0_rmse: 0.55608 | val_1_rmse: 0.56953 |  0:01:24s
epoch 29 | loss: 0.31597 | val_0_rmse: 0.55873 | val_1_rmse: 0.57115 |  0:01:27s
epoch 30 | loss: 0.31287 | val_0_rmse: 0.55306 | val_1_rmse: 0.56643 |  0:01:30s
epoch 31 | loss: 0.31212 | val_0_rmse: 0.55235 | val_1_rmse: 0.56441 |  0:01:33s
epoch 32 | loss: 0.31422 | val_0_rmse: 0.55343 | val_1_rmse: 0.56617 |  0:01:36s
epoch 33 | loss: 0.31104 | val_0_rmse: 0.56617 | val_1_rmse: 0.57785 |  0:01:39s
epoch 34 | loss: 0.30805 | val_0_rmse: 0.55814 | val_1_rmse: 0.57223 |  0:01:42s
epoch 35 | loss: 0.30366 | val_0_rmse: 0.57773 | val_1_rmse: 0.58962 |  0:01:44s
epoch 36 | loss: 0.30142 | val_0_rmse: 0.58266 | val_1_rmse: 0.57274 |  0:01:47s
epoch 37 | loss: 0.3031  | val_0_rmse: 0.56922 | val_1_rmse: 0.56791 |  0:01:50s
epoch 38 | loss: 0.30086 | val_0_rmse: 0.60226 | val_1_rmse: 0.57765 |  0:01:53s
epoch 39 | loss: 0.29731 | val_0_rmse: 0.64687 | val_1_rmse: 0.59153 |  0:01:56s
epoch 40 | loss: 0.29932 | val_0_rmse: 0.58953 | val_1_rmse: 0.57725 |  0:01:59s
epoch 41 | loss: 0.29624 | val_0_rmse: 0.59694 | val_1_rmse: 0.59574 |  0:02:02s
epoch 42 | loss: 0.29985 | val_0_rmse: 0.57725 | val_1_rmse: 0.57244 |  0:02:05s
epoch 43 | loss: 0.30126 | val_0_rmse: 0.72146 | val_1_rmse: 0.61794 |  0:02:08s
epoch 44 | loss: 0.29197 | val_0_rmse: 0.87923 | val_1_rmse: 0.57837 |  0:02:11s
epoch 45 | loss: 0.29743 | val_0_rmse: 0.62147 | val_1_rmse: 0.61491 |  0:02:14s
epoch 46 | loss: 0.31196 | val_0_rmse: 0.87422 | val_1_rmse: 0.77621 |  0:02:17s
epoch 47 | loss: 0.29631 | val_0_rmse: 0.62792 | val_1_rmse: 0.58667 |  0:02:19s
epoch 48 | loss: 0.29791 | val_0_rmse: 0.60144 | val_1_rmse: 0.56942 |  0:02:22s
epoch 49 | loss: 0.2893  | val_0_rmse: 0.67732 | val_1_rmse: 0.6147  |  0:02:25s
epoch 50 | loss: 0.2936  | val_0_rmse: 0.8373  | val_1_rmse: 0.59119 |  0:02:28s
epoch 51 | loss: 0.31862 | val_0_rmse: 0.61297 | val_1_rmse: 0.60724 |  0:02:31s
epoch 52 | loss: 0.31134 | val_0_rmse: 0.56887 | val_1_rmse: 0.57671 |  0:02:34s
epoch 53 | loss: 0.30859 | val_0_rmse: 0.56596 | val_1_rmse: 0.56849 |  0:02:37s
epoch 54 | loss: 0.30275 | val_0_rmse: 0.55403 | val_1_rmse: 0.56583 |  0:02:40s
epoch 55 | loss: 0.30163 | val_0_rmse: 0.56517 | val_1_rmse: 0.57647 |  0:02:43s
epoch 56 | loss: 0.30525 | val_0_rmse: 0.56905 | val_1_rmse: 0.58246 |  0:02:46s
epoch 57 | loss: 0.30363 | val_0_rmse: 0.57378 | val_1_rmse: 0.58621 |  0:02:49s
epoch 58 | loss: 0.30305 | val_0_rmse: 0.57748 | val_1_rmse: 0.58332 |  0:02:52s
epoch 59 | loss: 0.3208  | val_0_rmse: 0.57528 | val_1_rmse: 0.58741 |  0:02:55s
epoch 60 | loss: 0.31056 | val_0_rmse: 0.57114 | val_1_rmse: 0.57929 |  0:02:58s
epoch 61 | loss: 0.31054 | val_0_rmse: 0.5786  | val_1_rmse: 0.59004 |  0:03:00s

Early stopping occured at epoch 61 with best_epoch = 31 and best_val_1_rmse = 0.56441
Best weights from best epoch are automatically used!
ended training at: 11:05:20
Feature importance:
[('Area', 0.6527577803944427), ('Baths', 0.0010124017308360054), ('Beds', 0.19036850628897112), ('Latitude', 0.05671458580802587), ('Longitude', 0.05825631466817571), ('Month', 0.040890411109548615), ('Year', 0.0)]
Mean squared error is of 1009014101.239128
Mean absolute error:22227.529069303484
MAPE:0.3660164632454706
R2 score:0.7056521010548995
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:05:21
epoch 0  | loss: 0.45734 | val_0_rmse: 0.59399 | val_1_rmse: 0.59411 |  0:00:02s
epoch 1  | loss: 0.33973 | val_0_rmse: 0.57019 | val_1_rmse: 0.57151 |  0:00:05s
epoch 2  | loss: 0.33514 | val_0_rmse: 0.56536 | val_1_rmse: 0.56606 |  0:00:08s
epoch 3  | loss: 0.32856 | val_0_rmse: 0.58266 | val_1_rmse: 0.58598 |  0:00:11s
epoch 4  | loss: 0.33505 | val_0_rmse: 0.5732  | val_1_rmse: 0.57381 |  0:00:14s
epoch 5  | loss: 0.32845 | val_0_rmse: 0.56129 | val_1_rmse: 0.56083 |  0:00:17s
epoch 6  | loss: 0.32186 | val_0_rmse: 0.57537 | val_1_rmse: 0.5782  |  0:00:20s
epoch 7  | loss: 0.32181 | val_0_rmse: 0.63279 | val_1_rmse: 0.62379 |  0:00:23s
epoch 8  | loss: 0.31611 | val_0_rmse: 0.56285 | val_1_rmse: 0.56661 |  0:00:26s
epoch 9  | loss: 0.31427 | val_0_rmse: 0.69312 | val_1_rmse: 0.69729 |  0:00:29s
epoch 10 | loss: 0.30969 | val_0_rmse: 0.58419 | val_1_rmse: 0.58038 |  0:00:32s
epoch 11 | loss: 0.30393 | val_0_rmse: 0.70818 | val_1_rmse: 0.71476 |  0:00:35s
epoch 12 | loss: 0.30898 | val_0_rmse: 0.59534 | val_1_rmse: 0.59084 |  0:00:37s
epoch 13 | loss: 0.30245 | val_0_rmse: 0.56841 | val_1_rmse: 0.56616 |  0:00:40s
epoch 14 | loss: 0.30129 | val_0_rmse: 0.61997 | val_1_rmse: 0.61382 |  0:00:43s
epoch 15 | loss: 0.2984  | val_0_rmse: 0.65477 | val_1_rmse: 0.66444 |  0:00:46s
epoch 16 | loss: 0.30574 | val_0_rmse: 0.61761 | val_1_rmse: 0.613   |  0:00:49s
epoch 17 | loss: 0.29959 | val_0_rmse: 0.53989 | val_1_rmse: 0.54282 |  0:00:52s
epoch 18 | loss: 0.29701 | val_0_rmse: 0.61979 | val_1_rmse: 0.61899 |  0:00:55s
epoch 19 | loss: 0.29869 | val_0_rmse: 0.62428 | val_1_rmse: 0.63172 |  0:00:58s
epoch 20 | loss: 0.29751 | val_0_rmse: 0.56381 | val_1_rmse: 0.57075 |  0:01:01s
epoch 21 | loss: 0.29394 | val_0_rmse: 0.54307 | val_1_rmse: 0.54788 |  0:01:04s
epoch 22 | loss: 0.29568 | val_0_rmse: 0.55796 | val_1_rmse: 0.55851 |  0:01:07s
epoch 23 | loss: 0.29247 | val_0_rmse: 0.61912 | val_1_rmse: 0.61801 |  0:01:09s
epoch 24 | loss: 0.31753 | val_0_rmse: 0.55527 | val_1_rmse: 0.56001 |  0:01:12s
epoch 25 | loss: 0.3     | val_0_rmse: 0.56209 | val_1_rmse: 0.56505 |  0:01:15s
epoch 26 | loss: 0.29651 | val_0_rmse: 0.57831 | val_1_rmse: 0.58573 |  0:01:18s
epoch 27 | loss: 0.292   | val_0_rmse: 0.54415 | val_1_rmse: 0.54646 |  0:01:21s
epoch 28 | loss: 0.29763 | val_0_rmse: 1.2014  | val_1_rmse: 1.20556 |  0:01:24s
epoch 29 | loss: 0.30085 | val_0_rmse: 0.56382 | val_1_rmse: 0.56564 |  0:01:27s
epoch 30 | loss: 0.29262 | val_0_rmse: 0.58611 | val_1_rmse: 0.58613 |  0:01:30s
epoch 31 | loss: 0.29316 | val_0_rmse: 0.54842 | val_1_rmse: 0.55364 |  0:01:33s
epoch 32 | loss: 0.29383 | val_0_rmse: 0.66402 | val_1_rmse: 0.65573 |  0:01:36s
epoch 33 | loss: 0.2906  | val_0_rmse: 0.52771 | val_1_rmse: 0.53065 |  0:01:39s
epoch 34 | loss: 0.28851 | val_0_rmse: 0.53469 | val_1_rmse: 0.53803 |  0:01:42s
epoch 35 | loss: 0.29149 | val_0_rmse: 0.5748  | val_1_rmse: 0.58064 |  0:01:45s
epoch 36 | loss: 0.29416 | val_0_rmse: 0.89153 | val_1_rmse: 0.89498 |  0:01:48s
epoch 37 | loss: 0.29798 | val_0_rmse: 0.60012 | val_1_rmse: 0.59418 |  0:01:50s
epoch 38 | loss: 0.29829 | val_0_rmse: 0.55762 | val_1_rmse: 0.56157 |  0:01:53s
epoch 39 | loss: 0.29338 | val_0_rmse: 0.5728  | val_1_rmse: 0.57049 |  0:01:56s
epoch 40 | loss: 0.28962 | val_0_rmse: 0.58976 | val_1_rmse: 0.58793 |  0:01:59s
epoch 41 | loss: 0.2896  | val_0_rmse: 1.06695 | val_1_rmse: 1.07985 |  0:02:02s
epoch 42 | loss: 0.28817 | val_0_rmse: 0.54537 | val_1_rmse: 0.54733 |  0:02:05s
epoch 43 | loss: 0.28515 | val_0_rmse: 0.52609 | val_1_rmse: 0.53064 |  0:02:08s
epoch 44 | loss: 0.28637 | val_0_rmse: 0.54225 | val_1_rmse: 0.54896 |  0:02:11s
epoch 45 | loss: 0.2867  | val_0_rmse: 0.55001 | val_1_rmse: 0.55616 |  0:02:14s
epoch 46 | loss: 0.28721 | val_0_rmse: 0.57503 | val_1_rmse: 0.58124 |  0:02:17s
epoch 47 | loss: 0.29521 | val_0_rmse: 2.84094 | val_1_rmse: 2.85281 |  0:02:20s
epoch 48 | loss: 0.29604 | val_0_rmse: 0.53222 | val_1_rmse: 0.53471 |  0:02:22s
epoch 49 | loss: 0.28588 | val_0_rmse: 0.54488 | val_1_rmse: 0.5437  |  0:02:25s
epoch 50 | loss: 0.28924 | val_0_rmse: 0.59887 | val_1_rmse: 0.59925 |  0:02:28s
epoch 51 | loss: 0.28866 | val_0_rmse: 0.53447 | val_1_rmse: 0.53791 |  0:02:31s
epoch 52 | loss: 0.28776 | val_0_rmse: 0.52938 | val_1_rmse: 0.53027 |  0:02:34s
epoch 53 | loss: 0.28443 | val_0_rmse: 0.5567  | val_1_rmse: 0.55705 |  0:02:37s
epoch 54 | loss: 0.28611 | val_0_rmse: 0.5406  | val_1_rmse: 0.54255 |  0:02:40s
epoch 55 | loss: 0.28707 | val_0_rmse: 0.56368 | val_1_rmse: 0.56341 |  0:02:43s
epoch 56 | loss: 0.28642 | val_0_rmse: 0.57698 | val_1_rmse: 0.57873 |  0:02:46s
epoch 57 | loss: 0.28423 | val_0_rmse: 0.54169 | val_1_rmse: 0.54377 |  0:02:49s
epoch 58 | loss: 0.284   | val_0_rmse: 0.54537 | val_1_rmse: 0.54478 |  0:02:52s
epoch 59 | loss: 0.2858  | val_0_rmse: 0.57861 | val_1_rmse: 0.5803  |  0:02:55s
epoch 60 | loss: 0.2837  | val_0_rmse: 0.55915 | val_1_rmse: 0.56567 |  0:02:58s
epoch 61 | loss: 0.28162 | val_0_rmse: 0.53276 | val_1_rmse: 0.53765 |  0:03:00s
epoch 62 | loss: 0.283   | val_0_rmse: 0.53513 | val_1_rmse: 0.53904 |  0:03:03s
epoch 63 | loss: 0.28632 | val_0_rmse: 0.5326  | val_1_rmse: 0.5343  |  0:03:06s
epoch 64 | loss: 0.28727 | val_0_rmse: 0.52472 | val_1_rmse: 0.52682 |  0:03:09s
epoch 65 | loss: 0.28236 | val_0_rmse: 0.60968 | val_1_rmse: 0.61033 |  0:03:12s
epoch 66 | loss: 0.28144 | val_0_rmse: 0.57739 | val_1_rmse: 0.57711 |  0:03:15s
epoch 67 | loss: 0.28435 | val_0_rmse: 0.53735 | val_1_rmse: 0.54107 |  0:03:18s
epoch 68 | loss: 0.28636 | val_0_rmse: 0.53446 | val_1_rmse: 0.53507 |  0:03:21s
epoch 69 | loss: 0.28071 | val_0_rmse: 0.59091 | val_1_rmse: 0.59112 |  0:03:24s
epoch 70 | loss: 0.27831 | val_0_rmse: 0.55669 | val_1_rmse: 0.5604  |  0:03:27s
epoch 71 | loss: 0.28125 | val_0_rmse: 0.59141 | val_1_rmse: 0.58873 |  0:03:30s
epoch 72 | loss: 0.28    | val_0_rmse: 0.57505 | val_1_rmse: 0.58211 |  0:03:33s
epoch 73 | loss: 0.27759 | val_0_rmse: 0.53991 | val_1_rmse: 0.54106 |  0:03:36s
epoch 74 | loss: 0.27952 | val_0_rmse: 0.56347 | val_1_rmse: 0.56416 |  0:03:38s
epoch 75 | loss: 0.28199 | val_0_rmse: 0.54091 | val_1_rmse: 0.54145 |  0:03:41s
epoch 76 | loss: 0.27939 | val_0_rmse: 0.53724 | val_1_rmse: 0.53884 |  0:03:44s
epoch 77 | loss: 0.27978 | val_0_rmse: 0.57626 | val_1_rmse: 0.57702 |  0:03:47s
epoch 78 | loss: 0.27935 | val_0_rmse: 0.53269 | val_1_rmse: 0.53538 |  0:03:50s
epoch 79 | loss: 0.29221 | val_0_rmse: 0.6776  | val_1_rmse: 0.67201 |  0:03:53s
epoch 80 | loss: 0.2999  | val_0_rmse: 0.53238 | val_1_rmse: 0.53648 |  0:03:56s
epoch 81 | loss: 0.2884  | val_0_rmse: 0.67652 | val_1_rmse: 0.6732  |  0:03:59s
epoch 82 | loss: 0.28403 | val_0_rmse: 0.66577 | val_1_rmse: 0.66522 |  0:04:02s
epoch 83 | loss: 0.28302 | val_0_rmse: 0.54969 | val_1_rmse: 0.54934 |  0:04:05s
epoch 84 | loss: 0.28154 | val_0_rmse: 0.56235 | val_1_rmse: 0.56247 |  0:04:08s
epoch 85 | loss: 0.28048 | val_0_rmse: 0.54341 | val_1_rmse: 0.548   |  0:04:11s
epoch 86 | loss: 0.27986 | val_0_rmse: 0.56772 | val_1_rmse: 0.56931 |  0:04:13s
epoch 87 | loss: 0.27778 | val_0_rmse: 0.66261 | val_1_rmse: 0.66198 |  0:04:16s
epoch 88 | loss: 0.27916 | val_0_rmse: 0.5471  | val_1_rmse: 0.55507 |  0:04:19s
epoch 89 | loss: 0.27665 | val_0_rmse: 0.58161 | val_1_rmse: 0.58155 |  0:04:22s
epoch 90 | loss: 0.27369 | val_0_rmse: 0.52827 | val_1_rmse: 0.53318 |  0:04:25s
epoch 91 | loss: 0.27696 | val_0_rmse: 0.53445 | val_1_rmse: 0.53541 |  0:04:28s
epoch 92 | loss: 0.27428 | val_0_rmse: 0.57828 | val_1_rmse: 0.58432 |  0:04:31s
epoch 93 | loss: 0.27384 | val_0_rmse: 0.52862 | val_1_rmse: 0.53086 |  0:04:34s
epoch 94 | loss: 0.27822 | val_0_rmse: 0.6269  | val_1_rmse: 0.63008 |  0:04:37s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.52682
Best weights from best epoch are automatically used!
ended training at: 11:09:59
Feature importance:
[('Area', 0.5392594320281543), ('Baths', 0.0), ('Beds', 0.208006376582026), ('Latitude', 0.10805703307829456), ('Longitude', 0.09943718289915783), ('Month', 0.04523997541236727), ('Year', 0.0)]
Mean squared error is of 936619414.4388475
Mean absolute error:21294.235578218835
MAPE:0.3495658613837946
R2 score:0.726179352476319
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:09:59
epoch 0  | loss: 0.46746 | val_0_rmse: 0.61605 | val_1_rmse: 0.608   |  0:00:02s
epoch 1  | loss: 0.34365 | val_0_rmse: 0.57284 | val_1_rmse: 0.56947 |  0:00:05s
epoch 2  | loss: 0.3358  | val_0_rmse: 0.57287 | val_1_rmse: 0.56693 |  0:00:08s
epoch 3  | loss: 0.33036 | val_0_rmse: 0.57141 | val_1_rmse: 0.56734 |  0:00:11s
epoch 4  | loss: 0.33503 | val_0_rmse: 0.57225 | val_1_rmse: 0.56487 |  0:00:14s
epoch 5  | loss: 0.32999 | val_0_rmse: 0.57731 | val_1_rmse: 0.5718  |  0:00:17s
epoch 6  | loss: 0.33326 | val_0_rmse: 0.56681 | val_1_rmse: 0.56382 |  0:00:20s
epoch 7  | loss: 0.33099 | val_0_rmse: 0.57028 | val_1_rmse: 0.56366 |  0:00:23s
epoch 8  | loss: 0.32959 | val_0_rmse: 0.56716 | val_1_rmse: 0.56135 |  0:00:26s
epoch 9  | loss: 0.32632 | val_0_rmse: 0.57261 | val_1_rmse: 0.56789 |  0:00:29s
epoch 10 | loss: 0.32666 | val_0_rmse: 0.56258 | val_1_rmse: 0.55745 |  0:00:32s
epoch 11 | loss: 0.32265 | val_0_rmse: 0.56232 | val_1_rmse: 0.55888 |  0:00:35s
epoch 12 | loss: 0.32231 | val_0_rmse: 0.56779 | val_1_rmse: 0.56334 |  0:00:38s
epoch 13 | loss: 0.31905 | val_0_rmse: 0.60295 | val_1_rmse: 0.599   |  0:00:41s
epoch 14 | loss: 0.31766 | val_0_rmse: 0.60341 | val_1_rmse: 0.59596 |  0:00:43s
epoch 15 | loss: 0.32893 | val_0_rmse: 0.56996 | val_1_rmse: 0.5652  |  0:00:46s
epoch 16 | loss: 0.31253 | val_0_rmse: 0.9779  | val_1_rmse: 0.98499 |  0:00:49s
epoch 17 | loss: 0.31326 | val_0_rmse: 0.58994 | val_1_rmse: 0.58604 |  0:00:52s
epoch 18 | loss: 0.3135  | val_0_rmse: 0.59187 | val_1_rmse: 0.59132 |  0:00:55s
epoch 19 | loss: 0.31221 | val_0_rmse: 0.56615 | val_1_rmse: 0.56407 |  0:00:58s
epoch 20 | loss: 0.30596 | val_0_rmse: 0.59257 | val_1_rmse: 0.58834 |  0:01:01s
epoch 21 | loss: 0.31541 | val_0_rmse: 0.59124 | val_1_rmse: 0.59204 |  0:01:04s
epoch 22 | loss: 0.32456 | val_0_rmse: 0.61862 | val_1_rmse: 0.61793 |  0:01:07s
epoch 23 | loss: 0.34252 | val_0_rmse: 0.58805 | val_1_rmse: 0.58077 |  0:01:10s
epoch 24 | loss: 0.33374 | val_0_rmse: 0.57234 | val_1_rmse: 0.56536 |  0:01:13s
epoch 25 | loss: 0.33251 | val_0_rmse: 0.57165 | val_1_rmse: 0.56442 |  0:01:16s
epoch 26 | loss: 0.33204 | val_0_rmse: 0.56391 | val_1_rmse: 0.55967 |  0:01:19s
epoch 27 | loss: 0.32514 | val_0_rmse: 0.57325 | val_1_rmse: 0.56696 |  0:01:21s
epoch 28 | loss: 0.33407 | val_0_rmse: 0.5668  | val_1_rmse: 0.5613  |  0:01:24s
epoch 29 | loss: 0.32398 | val_0_rmse: 0.55898 | val_1_rmse: 0.55421 |  0:01:27s
epoch 30 | loss: 0.32076 | val_0_rmse: 0.56352 | val_1_rmse: 0.55724 |  0:01:30s
epoch 31 | loss: 0.32394 | val_0_rmse: 0.56496 | val_1_rmse: 0.56003 |  0:01:33s
epoch 32 | loss: 0.32667 | val_0_rmse: 0.56324 | val_1_rmse: 0.55829 |  0:01:36s
epoch 33 | loss: 0.32862 | val_0_rmse: 0.56402 | val_1_rmse: 0.5592  |  0:01:39s
epoch 34 | loss: 0.32358 | val_0_rmse: 0.56346 | val_1_rmse: 0.55964 |  0:01:42s
epoch 35 | loss: 0.31985 | val_0_rmse: 0.5621  | val_1_rmse: 0.55728 |  0:01:45s
epoch 36 | loss: 0.32076 | val_0_rmse: 0.55978 | val_1_rmse: 0.55593 |  0:01:48s
epoch 37 | loss: 0.323   | val_0_rmse: 0.55828 | val_1_rmse: 0.55527 |  0:01:51s
epoch 38 | loss: 0.31772 | val_0_rmse: 0.55745 | val_1_rmse: 0.55236 |  0:01:54s
epoch 39 | loss: 0.31673 | val_0_rmse: 0.56333 | val_1_rmse: 0.55899 |  0:01:56s
epoch 40 | loss: 0.31805 | val_0_rmse: 0.55724 | val_1_rmse: 0.55182 |  0:01:59s
epoch 41 | loss: 0.31993 | val_0_rmse: 0.56118 | val_1_rmse: 0.55528 |  0:02:02s
epoch 42 | loss: 0.32138 | val_0_rmse: 0.57123 | val_1_rmse: 0.56688 |  0:02:05s
epoch 43 | loss: 0.32431 | val_0_rmse: 0.57117 | val_1_rmse: 0.56368 |  0:02:08s
epoch 44 | loss: 0.32457 | val_0_rmse: 0.56494 | val_1_rmse: 0.56048 |  0:02:11s
epoch 45 | loss: 0.31926 | val_0_rmse: 0.55812 | val_1_rmse: 0.55462 |  0:02:14s
epoch 46 | loss: 0.31864 | val_0_rmse: 0.57022 | val_1_rmse: 0.56467 |  0:02:17s
epoch 47 | loss: 0.31734 | val_0_rmse: 0.56197 | val_1_rmse: 0.55692 |  0:02:20s
epoch 48 | loss: 0.31565 | val_0_rmse: 0.56034 | val_1_rmse: 0.55513 |  0:02:23s
epoch 49 | loss: 0.31629 | val_0_rmse: 0.56447 | val_1_rmse: 0.55932 |  0:02:26s
epoch 50 | loss: 0.31865 | val_0_rmse: 0.56302 | val_1_rmse: 0.55811 |  0:02:29s
epoch 51 | loss: 0.31434 | val_0_rmse: 0.56892 | val_1_rmse: 0.56435 |  0:02:32s
epoch 52 | loss: 0.31567 | val_0_rmse: 0.55838 | val_1_rmse: 0.55373 |  0:02:35s
epoch 53 | loss: 0.31633 | val_0_rmse: 0.5615  | val_1_rmse: 0.55658 |  0:02:38s
epoch 54 | loss: 0.31162 | val_0_rmse: 0.56509 | val_1_rmse: 0.56008 |  0:02:40s
epoch 55 | loss: 0.31343 | val_0_rmse: 0.56774 | val_1_rmse: 0.56264 |  0:02:43s
epoch 56 | loss: 0.314   | val_0_rmse: 0.57083 | val_1_rmse: 0.56529 |  0:02:46s
epoch 57 | loss: 0.31119 | val_0_rmse: 0.5746  | val_1_rmse: 0.56914 |  0:02:49s
epoch 58 | loss: 0.31327 | val_0_rmse: 0.55584 | val_1_rmse: 0.55141 |  0:02:52s
epoch 59 | loss: 0.31034 | val_0_rmse: 0.55818 | val_1_rmse: 0.55305 |  0:02:55s
epoch 60 | loss: 0.31082 | val_0_rmse: 0.5812  | val_1_rmse: 0.57697 |  0:02:58s
epoch 61 | loss: 0.31155 | val_0_rmse: 0.57487 | val_1_rmse: 0.56865 |  0:03:01s
epoch 62 | loss: 0.31051 | val_0_rmse: 0.55896 | val_1_rmse: 0.55581 |  0:03:04s
epoch 63 | loss: 0.30971 | val_0_rmse: 0.55825 | val_1_rmse: 0.55312 |  0:03:07s
epoch 64 | loss: 0.30585 | val_0_rmse: 0.55304 | val_1_rmse: 0.54781 |  0:03:10s
epoch 65 | loss: 0.30449 | val_0_rmse: 0.55038 | val_1_rmse: 0.54577 |  0:03:13s
epoch 66 | loss: 0.30657 | val_0_rmse: 0.59926 | val_1_rmse: 0.59401 |  0:03:16s
epoch 67 | loss: 0.30492 | val_0_rmse: 0.55311 | val_1_rmse: 0.54955 |  0:03:18s
epoch 68 | loss: 0.30168 | val_0_rmse: 0.5814  | val_1_rmse: 0.57685 |  0:03:21s
epoch 69 | loss: 0.30206 | val_0_rmse: 0.57056 | val_1_rmse: 0.56471 |  0:03:24s
epoch 70 | loss: 0.30169 | val_0_rmse: 0.56757 | val_1_rmse: 0.56025 |  0:03:27s
epoch 71 | loss: 0.30158 | val_0_rmse: 0.63774 | val_1_rmse: 0.63477 |  0:03:30s
epoch 72 | loss: 0.30111 | val_0_rmse: 0.60684 | val_1_rmse: 0.60394 |  0:03:33s
epoch 73 | loss: 0.30183 | val_0_rmse: 0.60082 | val_1_rmse: 0.59684 |  0:03:36s
epoch 74 | loss: 0.29731 | val_0_rmse: 0.58682 | val_1_rmse: 0.58154 |  0:03:39s
epoch 75 | loss: 0.29809 | val_0_rmse: 0.57016 | val_1_rmse: 0.56785 |  0:03:42s
epoch 76 | loss: 0.29666 | val_0_rmse: 0.62363 | val_1_rmse: 0.6202  |  0:03:45s
epoch 77 | loss: 0.29644 | val_0_rmse: 0.59577 | val_1_rmse: 0.59162 |  0:03:48s
epoch 78 | loss: 0.2974  | val_0_rmse: 0.54691 | val_1_rmse: 0.54286 |  0:03:51s
epoch 79 | loss: 0.29479 | val_0_rmse: 0.59752 | val_1_rmse: 0.59338 |  0:03:54s
epoch 80 | loss: 0.29546 | val_0_rmse: 0.56209 | val_1_rmse: 0.55439 |  0:03:56s
epoch 81 | loss: 0.29377 | val_0_rmse: 0.57968 | val_1_rmse: 0.5744  |  0:03:59s
epoch 82 | loss: 0.29335 | val_0_rmse: 0.60958 | val_1_rmse: 0.60551 |  0:04:02s
epoch 83 | loss: 0.29446 | val_0_rmse: 0.59748 | val_1_rmse: 0.59583 |  0:04:05s
epoch 84 | loss: 0.29369 | val_0_rmse: 0.61674 | val_1_rmse: 0.61265 |  0:04:08s
epoch 85 | loss: 0.29414 | val_0_rmse: 0.59684 | val_1_rmse: 0.59259 |  0:04:11s
epoch 86 | loss: 0.29302 | val_0_rmse: 0.58655 | val_1_rmse: 0.58202 |  0:04:14s
epoch 87 | loss: 0.2926  | val_0_rmse: 0.56287 | val_1_rmse: 0.55649 |  0:04:17s
epoch 88 | loss: 0.29958 | val_0_rmse: 0.5685  | val_1_rmse: 0.56457 |  0:04:20s
epoch 89 | loss: 0.29524 | val_0_rmse: 0.61211 | val_1_rmse: 0.60844 |  0:04:23s
epoch 90 | loss: 0.29219 | val_0_rmse: 0.58258 | val_1_rmse: 0.57506 |  0:04:26s
epoch 91 | loss: 0.2913  | val_0_rmse: 0.63322 | val_1_rmse: 0.62745 |  0:04:29s
epoch 92 | loss: 0.29122 | val_0_rmse: 0.57599 | val_1_rmse: 0.55493 |  0:04:32s
epoch 93 | loss: 0.29148 | val_0_rmse: 0.59622 | val_1_rmse: 0.59279 |  0:04:34s
epoch 94 | loss: 0.28932 | val_0_rmse: 0.54794 | val_1_rmse: 0.54313 |  0:04:37s
epoch 95 | loss: 0.29077 | val_0_rmse: 0.59244 | val_1_rmse: 0.58948 |  0:04:40s
epoch 96 | loss: 0.29122 | val_0_rmse: 0.58086 | val_1_rmse: 0.5715  |  0:04:43s
epoch 97 | loss: 0.28925 | val_0_rmse: 0.56261 | val_1_rmse: 0.55949 |  0:04:46s
epoch 98 | loss: 0.29226 | val_0_rmse: 0.58622 | val_1_rmse: 0.57983 |  0:04:49s
epoch 99 | loss: 0.29455 | val_0_rmse: 0.61813 | val_1_rmse: 0.61228 |  0:04:52s
epoch 100| loss: 0.29047 | val_0_rmse: 0.60508 | val_1_rmse: 0.60251 |  0:04:55s
epoch 101| loss: 0.29063 | val_0_rmse: 0.62287 | val_1_rmse: 0.61531 |  0:04:58s
epoch 102| loss: 0.29161 | val_0_rmse: 0.62817 | val_1_rmse: 0.62662 |  0:05:01s
epoch 103| loss: 0.29141 | val_0_rmse: 0.5796  | val_1_rmse: 0.57179 |  0:05:04s
epoch 104| loss: 0.28933 | val_0_rmse: 0.57627 | val_1_rmse: 0.57266 |  0:05:07s
epoch 105| loss: 0.293   | val_0_rmse: 0.58286 | val_1_rmse: 0.5766  |  0:05:10s
epoch 106| loss: 0.2949  | val_0_rmse: 0.53528 | val_1_rmse: 0.5305  |  0:05:13s
epoch 107| loss: 0.28896 | val_0_rmse: 0.67767 | val_1_rmse: 0.67103 |  0:05:16s
epoch 108| loss: 0.29041 | val_0_rmse: 0.54984 | val_1_rmse: 0.54381 |  0:05:19s
epoch 109| loss: 0.28829 | val_0_rmse: 0.65658 | val_1_rmse: 0.65423 |  0:05:22s
epoch 110| loss: 0.28943 | val_0_rmse: 0.56993 | val_1_rmse: 0.56769 |  0:05:24s
epoch 111| loss: 0.28626 | val_0_rmse: 0.70101 | val_1_rmse: 0.70068 |  0:05:27s
epoch 112| loss: 0.29073 | val_0_rmse: 0.62515 | val_1_rmse: 0.61962 |  0:05:30s
epoch 113| loss: 0.2891  | val_0_rmse: 0.54727 | val_1_rmse: 0.52966 |  0:05:33s
epoch 114| loss: 0.2855  | val_0_rmse: 0.63083 | val_1_rmse: 0.61699 |  0:05:36s
epoch 115| loss: 0.28768 | val_0_rmse: 0.58986 | val_1_rmse: 0.56082 |  0:05:39s
epoch 116| loss: 0.28934 | val_0_rmse: 0.67973 | val_1_rmse: 0.66192 |  0:05:42s
epoch 117| loss: 0.2877  | val_0_rmse: 0.64998 | val_1_rmse: 0.63364 |  0:05:45s
epoch 118| loss: 0.2881  | val_0_rmse: 0.75385 | val_1_rmse: 0.66312 |  0:05:48s
epoch 119| loss: 0.28793 | val_0_rmse: 0.67814 | val_1_rmse: 0.67664 |  0:05:51s
epoch 120| loss: 0.28817 | val_0_rmse: 0.56224 | val_1_rmse: 0.55906 |  0:05:54s
epoch 121| loss: 0.29224 | val_0_rmse: 0.62057 | val_1_rmse: 0.62049 |  0:05:57s
epoch 122| loss: 0.28779 | val_0_rmse: 0.57856 | val_1_rmse: 0.57111 |  0:05:59s
epoch 123| loss: 0.29055 | val_0_rmse: 0.65198 | val_1_rmse: 0.65025 |  0:06:02s
epoch 124| loss: 0.28771 | val_0_rmse: 0.66065 | val_1_rmse: 0.65713 |  0:06:05s
epoch 125| loss: 0.28702 | val_0_rmse: 0.62112 | val_1_rmse: 0.61424 |  0:06:08s
epoch 126| loss: 0.28815 | val_0_rmse: 0.68195 | val_1_rmse: 0.68032 |  0:06:11s
epoch 127| loss: 0.28617 | val_0_rmse: 0.59312 | val_1_rmse: 0.5893  |  0:06:14s
epoch 128| loss: 0.28875 | val_0_rmse: 0.66619 | val_1_rmse: 0.66437 |  0:06:17s
epoch 129| loss: 0.28408 | val_0_rmse: 0.62166 | val_1_rmse: 0.61982 |  0:06:20s
epoch 130| loss: 0.28461 | val_0_rmse: 0.67832 | val_1_rmse: 0.6771  |  0:06:23s
epoch 131| loss: 0.28869 | val_0_rmse: 0.56089 | val_1_rmse: 0.55528 |  0:06:26s
epoch 132| loss: 0.28454 | val_0_rmse: 0.65191 | val_1_rmse: 0.64279 |  0:06:29s
epoch 133| loss: 0.28585 | val_0_rmse: 0.62552 | val_1_rmse: 0.59755 |  0:06:32s
epoch 134| loss: 0.28894 | val_0_rmse: 0.65918 | val_1_rmse: 0.65075 |  0:06:35s
epoch 135| loss: 0.28691 | val_0_rmse: 0.62243 | val_1_rmse: 0.6068  |  0:06:37s
epoch 136| loss: 0.28601 | val_0_rmse: 0.7057  | val_1_rmse: 0.68017 |  0:06:40s
epoch 137| loss: 0.28426 | val_0_rmse: 0.65987 | val_1_rmse: 0.59842 |  0:06:43s
epoch 138| loss: 0.2845  | val_0_rmse: 0.63851 | val_1_rmse: 0.57665 |  0:06:46s
epoch 139| loss: 0.28398 | val_0_rmse: 0.71635 | val_1_rmse: 0.6985  |  0:06:49s
epoch 140| loss: 0.28729 | val_0_rmse: 0.6275  | val_1_rmse: 0.59558 |  0:06:52s
epoch 141| loss: 0.28554 | val_0_rmse: 0.62477 | val_1_rmse: 0.58121 |  0:06:55s
epoch 142| loss: 0.28699 | val_0_rmse: 0.7752  | val_1_rmse: 0.74347 |  0:06:58s
epoch 143| loss: 0.28686 | val_0_rmse: 0.69754 | val_1_rmse: 0.65457 |  0:07:01s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.52966
Best weights from best epoch are automatically used!
ended training at: 11:17:01
Feature importance:
[('Area', 0.36972672123226846), ('Baths', 0.09881983527703322), ('Beds', 0.1342446997931055), ('Latitude', 0.0), ('Longitude', 0.3293143079948718), ('Month', 0.06789443570272104), ('Year', 0.0)]
Mean squared error is of 985077574.0216057
Mean absolute error:21619.68530002481
MAPE:0.3597752766515646
R2 score:0.7079631916398659
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:17:02
epoch 0  | loss: 0.45448 | val_0_rmse: 0.57652 | val_1_rmse: 0.56197 |  0:00:02s
epoch 1  | loss: 0.3422  | val_0_rmse: 0.57437 | val_1_rmse: 0.56169 |  0:00:05s
epoch 2  | loss: 0.34014 | val_0_rmse: 0.57135 | val_1_rmse: 0.55903 |  0:00:08s
epoch 3  | loss: 0.33695 | val_0_rmse: 0.57193 | val_1_rmse: 0.5588  |  0:00:11s
epoch 4  | loss: 0.33349 | val_0_rmse: 0.57421 | val_1_rmse: 0.56202 |  0:00:14s
epoch 5  | loss: 0.33275 | val_0_rmse: 0.56573 | val_1_rmse: 0.55626 |  0:00:17s
epoch 6  | loss: 0.32793 | val_0_rmse: 0.56534 | val_1_rmse: 0.55357 |  0:00:20s
epoch 7  | loss: 0.33125 | val_0_rmse: 0.56952 | val_1_rmse: 0.55795 |  0:00:23s
epoch 8  | loss: 0.32551 | val_0_rmse: 0.56239 | val_1_rmse: 0.55107 |  0:00:26s
epoch 9  | loss: 0.32224 | val_0_rmse: 0.567   | val_1_rmse: 0.55576 |  0:00:29s
epoch 10 | loss: 0.32363 | val_0_rmse: 0.56264 | val_1_rmse: 0.55081 |  0:00:32s
epoch 11 | loss: 0.326   | val_0_rmse: 0.56619 | val_1_rmse: 0.555   |  0:00:35s
epoch 12 | loss: 0.32199 | val_0_rmse: 0.56375 | val_1_rmse: 0.54986 |  0:00:38s
epoch 13 | loss: 0.3235  | val_0_rmse: 0.56941 | val_1_rmse: 0.55952 |  0:00:41s
epoch 14 | loss: 0.32139 | val_0_rmse: 0.56249 | val_1_rmse: 0.55081 |  0:00:44s
epoch 15 | loss: 0.31973 | val_0_rmse: 0.56866 | val_1_rmse: 0.55586 |  0:00:47s
epoch 16 | loss: 0.32219 | val_0_rmse: 0.55686 | val_1_rmse: 0.54419 |  0:00:50s
epoch 17 | loss: 0.3167  | val_0_rmse: 0.56075 | val_1_rmse: 0.54924 |  0:00:53s
epoch 18 | loss: 0.32147 | val_0_rmse: 0.56069 | val_1_rmse: 0.55065 |  0:00:55s
epoch 19 | loss: 0.31688 | val_0_rmse: 0.56754 | val_1_rmse: 0.56184 |  0:00:58s
epoch 20 | loss: 0.31785 | val_0_rmse: 0.55573 | val_1_rmse: 0.54503 |  0:01:01s
epoch 21 | loss: 0.31868 | val_0_rmse: 0.57315 | val_1_rmse: 0.56294 |  0:01:04s
epoch 22 | loss: 0.31973 | val_0_rmse: 0.55909 | val_1_rmse: 0.54812 |  0:01:07s
epoch 23 | loss: 0.3172  | val_0_rmse: 0.56538 | val_1_rmse: 0.552   |  0:01:10s
epoch 24 | loss: 0.31936 | val_0_rmse: 0.56655 | val_1_rmse: 0.5597  |  0:01:13s
epoch 25 | loss: 0.31421 | val_0_rmse: 0.55424 | val_1_rmse: 0.54222 |  0:01:16s
epoch 26 | loss: 0.31343 | val_0_rmse: 0.55856 | val_1_rmse: 0.54661 |  0:01:19s
epoch 27 | loss: 0.3151  | val_0_rmse: 0.55726 | val_1_rmse: 0.54755 |  0:01:22s
epoch 28 | loss: 0.31655 | val_0_rmse: 0.55942 | val_1_rmse: 0.54742 |  0:01:25s
epoch 29 | loss: 0.31305 | val_0_rmse: 0.56322 | val_1_rmse: 0.54993 |  0:01:28s
epoch 30 | loss: 0.30904 | val_0_rmse: 0.55452 | val_1_rmse: 0.54866 |  0:01:31s
epoch 31 | loss: 0.31029 | val_0_rmse: 0.57076 | val_1_rmse: 0.56134 |  0:01:34s
epoch 32 | loss: 0.31227 | val_0_rmse: 0.57143 | val_1_rmse: 0.56703 |  0:01:37s
epoch 33 | loss: 0.31141 | val_0_rmse: 0.55013 | val_1_rmse: 0.53919 |  0:01:39s
epoch 34 | loss: 0.30889 | val_0_rmse: 0.57249 | val_1_rmse: 0.55936 |  0:01:42s
epoch 35 | loss: 0.31168 | val_0_rmse: 0.57821 | val_1_rmse: 0.56309 |  0:01:45s
epoch 36 | loss: 0.30696 | val_0_rmse: 0.57262 | val_1_rmse: 0.56762 |  0:01:48s
epoch 37 | loss: 0.30639 | val_0_rmse: 0.5557  | val_1_rmse: 0.54303 |  0:01:51s
epoch 38 | loss: 0.30559 | val_0_rmse: 0.56593 | val_1_rmse: 0.55201 |  0:01:54s
epoch 39 | loss: 0.303   | val_0_rmse: 0.56907 | val_1_rmse: 0.55455 |  0:01:57s
epoch 40 | loss: 0.30629 | val_0_rmse: 0.55111 | val_1_rmse: 0.53819 |  0:02:00s
epoch 41 | loss: 0.30255 | val_0_rmse: 0.56639 | val_1_rmse: 0.5525  |  0:02:03s
epoch 42 | loss: 0.30573 | val_0_rmse: 0.54266 | val_1_rmse: 0.53618 |  0:02:06s
epoch 43 | loss: 0.30379 | val_0_rmse: 0.56258 | val_1_rmse: 0.55214 |  0:02:09s
epoch 44 | loss: 0.30268 | val_0_rmse: 0.58572 | val_1_rmse: 0.58125 |  0:02:12s
epoch 45 | loss: 0.31076 | val_0_rmse: 0.58348 | val_1_rmse: 0.5685  |  0:02:15s
epoch 46 | loss: 0.31685 | val_0_rmse: 0.55999 | val_1_rmse: 0.55098 |  0:02:18s
epoch 47 | loss: 0.31056 | val_0_rmse: 0.57108 | val_1_rmse: 0.55628 |  0:02:20s
epoch 48 | loss: 0.30511 | val_0_rmse: 0.58295 | val_1_rmse: 0.56963 |  0:02:23s
epoch 49 | loss: 0.30426 | val_0_rmse: 0.58007 | val_1_rmse: 0.56547 |  0:02:26s
epoch 50 | loss: 0.30227 | val_0_rmse: 0.56294 | val_1_rmse: 0.55809 |  0:02:29s
epoch 51 | loss: 0.30243 | val_0_rmse: 0.58107 | val_1_rmse: 0.56753 |  0:02:32s
epoch 52 | loss: 0.3008  | val_0_rmse: 0.60925 | val_1_rmse: 0.5944  |  0:02:35s
epoch 53 | loss: 0.30223 | val_0_rmse: 0.56856 | val_1_rmse: 0.55584 |  0:02:38s
epoch 54 | loss: 0.30212 | val_0_rmse: 0.54654 | val_1_rmse: 0.53667 |  0:02:41s
epoch 55 | loss: 0.29975 | val_0_rmse: 0.53729 | val_1_rmse: 0.53005 |  0:02:44s
epoch 56 | loss: 0.29743 | val_0_rmse: 0.54096 | val_1_rmse: 0.53455 |  0:02:47s
epoch 57 | loss: 0.2986  | val_0_rmse: 0.57692 | val_1_rmse: 0.56825 |  0:02:50s
epoch 58 | loss: 0.30862 | val_0_rmse: 0.57023 | val_1_rmse: 0.56466 |  0:02:53s
epoch 59 | loss: 0.309   | val_0_rmse: 0.59828 | val_1_rmse: 0.58563 |  0:02:56s
epoch 60 | loss: 0.31053 | val_0_rmse: 0.6154  | val_1_rmse: 0.60743 |  0:02:59s
epoch 61 | loss: 0.30685 | val_0_rmse: 0.57236 | val_1_rmse: 0.56489 |  0:03:02s
epoch 62 | loss: 0.30635 | val_0_rmse: 0.5582  | val_1_rmse: 0.54742 |  0:03:04s
epoch 63 | loss: 0.30884 | val_0_rmse: 0.59034 | val_1_rmse: 0.58021 |  0:03:07s
epoch 64 | loss: 0.30989 | val_0_rmse: 0.55672 | val_1_rmse: 0.54926 |  0:03:10s
epoch 65 | loss: 0.30374 | val_0_rmse: 0.57345 | val_1_rmse: 0.56279 |  0:03:13s
epoch 66 | loss: 0.30356 | val_0_rmse: 0.54837 | val_1_rmse: 0.53889 |  0:03:16s
epoch 67 | loss: 0.30005 | val_0_rmse: 0.5908  | val_1_rmse: 0.5746  |  0:03:19s
epoch 68 | loss: 0.30613 | val_0_rmse: 0.55154 | val_1_rmse: 0.53905 |  0:03:22s
epoch 69 | loss: 0.30202 | val_0_rmse: 0.55624 | val_1_rmse: 0.54865 |  0:03:25s
epoch 70 | loss: 0.30286 | val_0_rmse: 0.5658  | val_1_rmse: 0.56146 |  0:03:28s
epoch 71 | loss: 0.30756 | val_0_rmse: 0.57349 | val_1_rmse: 0.56636 |  0:03:31s
epoch 72 | loss: 0.3111  | val_0_rmse: 0.55094 | val_1_rmse: 0.54448 |  0:03:34s
epoch 73 | loss: 0.33369 | val_0_rmse: 0.56182 | val_1_rmse: 0.54966 |  0:03:37s
epoch 74 | loss: 0.31433 | val_0_rmse: 0.61936 | val_1_rmse: 0.62027 |  0:03:40s
epoch 75 | loss: 0.3098  | val_0_rmse: 0.55142 | val_1_rmse: 0.54118 |  0:03:43s
epoch 76 | loss: 0.30538 | val_0_rmse: 0.5554  | val_1_rmse: 0.5426  |  0:03:46s
epoch 77 | loss: 0.30623 | val_0_rmse: 0.57126 | val_1_rmse: 0.5631  |  0:03:49s
epoch 78 | loss: 0.31167 | val_0_rmse: 0.56701 | val_1_rmse: 0.55603 |  0:03:52s
epoch 79 | loss: 0.30847 | val_0_rmse: 0.67729 | val_1_rmse: 0.67711 |  0:03:54s
epoch 80 | loss: 0.30522 | val_0_rmse: 0.61427 | val_1_rmse: 0.59722 |  0:03:57s
epoch 81 | loss: 0.30501 | val_0_rmse: 0.57447 | val_1_rmse: 0.55824 |  0:04:00s
epoch 82 | loss: 0.30569 | val_0_rmse: 0.55321 | val_1_rmse: 0.54303 |  0:04:03s
epoch 83 | loss: 0.30496 | val_0_rmse: 0.56048 | val_1_rmse: 0.54867 |  0:04:06s
epoch 84 | loss: 0.30706 | val_0_rmse: 0.56145 | val_1_rmse: 0.54951 |  0:04:09s
epoch 85 | loss: 0.30817 | val_0_rmse: 0.55054 | val_1_rmse: 0.54177 |  0:04:12s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.53005
Best weights from best epoch are automatically used!
ended training at: 11:21:15
Feature importance:
[('Area', 0.459125438518611), ('Baths', 0.07942668446379553), ('Beds', 0.14825417268642496), ('Latitude', 0.05020059851295931), ('Longitude', 0.07760328139533296), ('Month', 0.04830325672452485), ('Year', 0.13708656769835142)]
Mean squared error is of 992974414.6963636
Mean absolute error:21267.90934485109
MAPE:0.32883592625538016
R2 score:0.7018680111677764
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:21:15
epoch 0  | loss: 0.44792 | val_0_rmse: 0.59016 | val_1_rmse: 0.579   |  0:00:02s
epoch 1  | loss: 0.3511  | val_0_rmse: 0.58559 | val_1_rmse: 0.57264 |  0:00:05s
epoch 2  | loss: 0.35127 | val_0_rmse: 0.5839  | val_1_rmse: 0.56902 |  0:00:08s
epoch 3  | loss: 0.34092 | val_0_rmse: 0.57566 | val_1_rmse: 0.56443 |  0:00:11s
epoch 4  | loss: 0.33901 | val_0_rmse: 0.57438 | val_1_rmse: 0.56356 |  0:00:14s
epoch 5  | loss: 0.33573 | val_0_rmse: 0.5728  | val_1_rmse: 0.56058 |  0:00:17s
epoch 6  | loss: 0.33284 | val_0_rmse: 0.57355 | val_1_rmse: 0.56067 |  0:00:20s
epoch 7  | loss: 0.33177 | val_0_rmse: 0.57645 | val_1_rmse: 0.56712 |  0:00:23s
epoch 8  | loss: 0.33075 | val_0_rmse: 0.57421 | val_1_rmse: 0.55929 |  0:00:26s
epoch 9  | loss: 0.33779 | val_0_rmse: 0.56981 | val_1_rmse: 0.55877 |  0:00:29s
epoch 10 | loss: 0.33126 | val_0_rmse: 0.57253 | val_1_rmse: 0.56174 |  0:00:32s
epoch 11 | loss: 0.3283  | val_0_rmse: 0.56653 | val_1_rmse: 0.5564  |  0:00:35s
epoch 12 | loss: 0.32623 | val_0_rmse: 0.56928 | val_1_rmse: 0.55828 |  0:00:38s
epoch 13 | loss: 0.3295  | val_0_rmse: 0.56722 | val_1_rmse: 0.55815 |  0:00:41s
epoch 14 | loss: 0.32909 | val_0_rmse: 0.57075 | val_1_rmse: 0.56115 |  0:00:44s
epoch 15 | loss: 0.32999 | val_0_rmse: 0.57481 | val_1_rmse: 0.5632  |  0:00:47s
epoch 16 | loss: 0.32806 | val_0_rmse: 0.56556 | val_1_rmse: 0.5568  |  0:00:49s
epoch 17 | loss: 0.32816 | val_0_rmse: 0.56763 | val_1_rmse: 0.557   |  0:00:52s
epoch 18 | loss: 0.33061 | val_0_rmse: 0.57205 | val_1_rmse: 0.55992 |  0:00:55s
epoch 19 | loss: 0.33335 | val_0_rmse: 0.57072 | val_1_rmse: 0.56054 |  0:00:58s
epoch 20 | loss: 0.33035 | val_0_rmse: 0.56759 | val_1_rmse: 0.55672 |  0:01:01s
epoch 21 | loss: 0.33107 | val_0_rmse: 0.57085 | val_1_rmse: 0.55873 |  0:01:04s
epoch 22 | loss: 0.32916 | val_0_rmse: 0.56898 | val_1_rmse: 0.55939 |  0:01:07s
epoch 23 | loss: 0.32682 | val_0_rmse: 0.56895 | val_1_rmse: 0.5581  |  0:01:10s
epoch 24 | loss: 0.33136 | val_0_rmse: 0.56898 | val_1_rmse: 0.55861 |  0:01:13s
epoch 25 | loss: 0.32647 | val_0_rmse: 0.56756 | val_1_rmse: 0.55615 |  0:01:16s
epoch 26 | loss: 0.32481 | val_0_rmse: 0.566   | val_1_rmse: 0.55527 |  0:01:19s
epoch 27 | loss: 0.32629 | val_0_rmse: 0.56644 | val_1_rmse: 0.55484 |  0:01:22s
epoch 28 | loss: 0.32548 | val_0_rmse: 0.56904 | val_1_rmse: 0.55883 |  0:01:25s
epoch 29 | loss: 0.32627 | val_0_rmse: 0.56775 | val_1_rmse: 0.55749 |  0:01:28s
epoch 30 | loss: 0.32429 | val_0_rmse: 0.57118 | val_1_rmse: 0.5592  |  0:01:30s
epoch 31 | loss: 0.3245  | val_0_rmse: 0.56912 | val_1_rmse: 0.56018 |  0:01:33s
epoch 32 | loss: 0.32694 | val_0_rmse: 0.5701  | val_1_rmse: 0.55974 |  0:01:36s
epoch 33 | loss: 0.32254 | val_0_rmse: 0.56903 | val_1_rmse: 0.55766 |  0:01:39s
epoch 34 | loss: 0.32259 | val_0_rmse: 0.56516 | val_1_rmse: 0.55402 |  0:01:42s
epoch 35 | loss: 0.32232 | val_0_rmse: 0.56376 | val_1_rmse: 0.55307 |  0:01:45s
epoch 36 | loss: 0.32239 | val_0_rmse: 0.56658 | val_1_rmse: 0.55774 |  0:01:48s
epoch 37 | loss: 0.32305 | val_0_rmse: 0.56383 | val_1_rmse: 0.5536  |  0:01:51s
epoch 38 | loss: 0.32056 | val_0_rmse: 0.56342 | val_1_rmse: 0.55325 |  0:01:54s
epoch 39 | loss: 0.32148 | val_0_rmse: 0.56764 | val_1_rmse: 0.55814 |  0:01:57s
epoch 40 | loss: 0.32564 | val_0_rmse: 0.56215 | val_1_rmse: 0.55355 |  0:02:00s
epoch 41 | loss: 0.3205  | val_0_rmse: 0.56347 | val_1_rmse: 0.55312 |  0:02:03s
epoch 42 | loss: 0.3206  | val_0_rmse: 0.56654 | val_1_rmse: 0.55676 |  0:02:06s
epoch 43 | loss: 0.32319 | val_0_rmse: 0.56628 | val_1_rmse: 0.55531 |  0:02:09s
epoch 44 | loss: 0.31967 | val_0_rmse: 0.55973 | val_1_rmse: 0.55052 |  0:02:11s
epoch 45 | loss: 0.32678 | val_0_rmse: 0.56872 | val_1_rmse: 0.5588  |  0:02:14s
epoch 46 | loss: 0.32642 | val_0_rmse: 0.57477 | val_1_rmse: 0.56178 |  0:02:17s
epoch 47 | loss: 0.32796 | val_0_rmse: 0.56658 | val_1_rmse: 0.55536 |  0:02:20s
epoch 48 | loss: 0.32474 | val_0_rmse: 0.56848 | val_1_rmse: 0.55755 |  0:02:23s
epoch 49 | loss: 0.32367 | val_0_rmse: 0.56299 | val_1_rmse: 0.55421 |  0:02:26s
epoch 50 | loss: 0.32205 | val_0_rmse: 0.57623 | val_1_rmse: 0.56358 |  0:02:29s
epoch 51 | loss: 0.32294 | val_0_rmse: 0.56758 | val_1_rmse: 0.55915 |  0:02:32s
epoch 52 | loss: 0.32144 | val_0_rmse: 0.56341 | val_1_rmse: 0.55391 |  0:02:35s
epoch 53 | loss: 0.32089 | val_0_rmse: 0.56485 | val_1_rmse: 0.55402 |  0:02:38s
epoch 54 | loss: 0.32037 | val_0_rmse: 0.56981 | val_1_rmse: 0.5589  |  0:02:41s
epoch 55 | loss: 0.32294 | val_0_rmse: 0.56132 | val_1_rmse: 0.55155 |  0:02:44s
epoch 56 | loss: 0.3216  | val_0_rmse: 0.57361 | val_1_rmse: 0.56098 |  0:02:47s
epoch 57 | loss: 0.32464 | val_0_rmse: 0.56707 | val_1_rmse: 0.55737 |  0:02:50s
epoch 58 | loss: 0.32609 | val_0_rmse: 0.57484 | val_1_rmse: 0.5629  |  0:02:52s
epoch 59 | loss: 0.32084 | val_0_rmse: 0.55877 | val_1_rmse: 0.54882 |  0:02:55s
epoch 60 | loss: 0.3245  | val_0_rmse: 0.56967 | val_1_rmse: 0.55642 |  0:02:58s
epoch 61 | loss: 0.32177 | val_0_rmse: 0.56278 | val_1_rmse: 0.55079 |  0:03:01s
epoch 62 | loss: 0.32553 | val_0_rmse: 0.58337 | val_1_rmse: 0.57168 |  0:03:04s
epoch 63 | loss: 0.32688 | val_0_rmse: 0.56455 | val_1_rmse: 0.5517  |  0:03:07s
epoch 64 | loss: 0.34181 | val_0_rmse: 0.57279 | val_1_rmse: 0.55764 |  0:03:10s
epoch 65 | loss: 0.33379 | val_0_rmse: 0.56835 | val_1_rmse: 0.5563  |  0:03:13s
epoch 66 | loss: 0.33287 | val_0_rmse: 0.57682 | val_1_rmse: 0.56433 |  0:03:16s
epoch 67 | loss: 0.33094 | val_0_rmse: 0.56766 | val_1_rmse: 0.55449 |  0:03:19s
epoch 68 | loss: 0.32667 | val_0_rmse: 0.56772 | val_1_rmse: 0.55536 |  0:03:22s
epoch 69 | loss: 0.32695 | val_0_rmse: 0.57346 | val_1_rmse: 0.56022 |  0:03:25s
epoch 70 | loss: 0.32609 | val_0_rmse: 0.58051 | val_1_rmse: 0.56949 |  0:03:28s
epoch 71 | loss: 0.3234  | val_0_rmse: 0.56484 | val_1_rmse: 0.55242 |  0:03:31s
epoch 72 | loss: 0.32295 | val_0_rmse: 0.5718  | val_1_rmse: 0.56042 |  0:03:34s
epoch 73 | loss: 0.33016 | val_0_rmse: 0.56592 | val_1_rmse: 0.55325 |  0:03:36s
epoch 74 | loss: 0.32954 | val_0_rmse: 0.57874 | val_1_rmse: 0.56798 |  0:03:39s
epoch 75 | loss: 0.3261  | val_0_rmse: 0.57414 | val_1_rmse: 0.564   |  0:03:42s
epoch 76 | loss: 0.32294 | val_0_rmse: 0.56357 | val_1_rmse: 0.55173 |  0:03:45s
epoch 77 | loss: 0.32404 | val_0_rmse: 0.56188 | val_1_rmse: 0.54976 |  0:03:48s
epoch 78 | loss: 0.32288 | val_0_rmse: 0.56294 | val_1_rmse: 0.55065 |  0:03:51s
epoch 79 | loss: 0.32157 | val_0_rmse: 0.56524 | val_1_rmse: 0.55213 |  0:03:54s
epoch 80 | loss: 0.324   | val_0_rmse: 0.5634  | val_1_rmse: 0.55029 |  0:03:57s
epoch 81 | loss: 0.32458 | val_0_rmse: 0.5843  | val_1_rmse: 0.57217 |  0:04:00s
epoch 82 | loss: 0.32242 | val_0_rmse: 0.57259 | val_1_rmse: 0.5621  |  0:04:03s
epoch 83 | loss: 0.3227  | val_0_rmse: 0.56281 | val_1_rmse: 0.55126 |  0:04:06s
epoch 84 | loss: 0.31947 | val_0_rmse: 0.56341 | val_1_rmse: 0.55248 |  0:04:09s
epoch 85 | loss: 0.31967 | val_0_rmse: 0.56664 | val_1_rmse: 0.55604 |  0:04:12s
epoch 86 | loss: 0.32017 | val_0_rmse: 0.5575  | val_1_rmse: 0.54659 |  0:04:15s
epoch 87 | loss: 0.31776 | val_0_rmse: 0.55945 | val_1_rmse: 0.54911 |  0:04:17s
epoch 88 | loss: 0.32328 | val_0_rmse: 0.56516 | val_1_rmse: 0.55335 |  0:04:20s
epoch 89 | loss: 0.3251  | val_0_rmse: 0.58615 | val_1_rmse: 0.57407 |  0:04:23s
epoch 90 | loss: 0.32321 | val_0_rmse: 0.57025 | val_1_rmse: 0.55775 |  0:04:26s
epoch 91 | loss: 0.32461 | val_0_rmse: 0.5732  | val_1_rmse: 0.56218 |  0:04:29s
epoch 92 | loss: 0.31971 | val_0_rmse: 0.56327 | val_1_rmse: 0.55109 |  0:04:32s
epoch 93 | loss: 0.31848 | val_0_rmse: 0.57145 | val_1_rmse: 0.56069 |  0:04:35s
epoch 94 | loss: 0.31792 | val_0_rmse: 0.56679 | val_1_rmse: 0.5555  |  0:04:38s
epoch 95 | loss: 0.3175  | val_0_rmse: 0.55826 | val_1_rmse: 0.54731 |  0:04:41s
epoch 96 | loss: 0.32062 | val_0_rmse: 0.59029 | val_1_rmse: 0.57912 |  0:04:44s
epoch 97 | loss: 0.32241 | val_0_rmse: 0.56319 | val_1_rmse: 0.55149 |  0:04:47s
epoch 98 | loss: 0.31922 | val_0_rmse: 0.56139 | val_1_rmse: 0.5517  |  0:04:50s
epoch 99 | loss: 0.31683 | val_0_rmse: 0.55983 | val_1_rmse: 0.5496  |  0:04:53s
epoch 100| loss: 0.31569 | val_0_rmse: 0.56891 | val_1_rmse: 0.55905 |  0:04:56s
epoch 101| loss: 0.31779 | val_0_rmse: 0.56038 | val_1_rmse: 0.54862 |  0:04:59s
epoch 102| loss: 0.3163  | val_0_rmse: 0.56198 | val_1_rmse: 0.55082 |  0:05:01s
epoch 103| loss: 0.3256  | val_0_rmse: 0.55992 | val_1_rmse: 0.54977 |  0:05:04s
epoch 104| loss: 0.3248  | val_0_rmse: 0.5594  | val_1_rmse: 0.54923 |  0:05:07s
epoch 105| loss: 0.31519 | val_0_rmse: 0.5545  | val_1_rmse: 0.54329 |  0:05:10s
epoch 106| loss: 0.31499 | val_0_rmse: 0.56097 | val_1_rmse: 0.5491  |  0:05:13s
epoch 107| loss: 0.31473 | val_0_rmse: 0.55792 | val_1_rmse: 0.54872 |  0:05:16s
epoch 108| loss: 0.31207 | val_0_rmse: 0.56273 | val_1_rmse: 0.55246 |  0:05:19s
epoch 109| loss: 0.31232 | val_0_rmse: 0.56488 | val_1_rmse: 0.5553  |  0:05:22s
epoch 110| loss: 0.3146  | val_0_rmse: 0.56629 | val_1_rmse: 0.55478 |  0:05:25s
epoch 111| loss: 0.3118  | val_0_rmse: 0.57871 | val_1_rmse: 0.5669  |  0:05:28s
epoch 112| loss: 0.31346 | val_0_rmse: 0.56705 | val_1_rmse: 0.55531 |  0:05:31s
epoch 113| loss: 0.30647 | val_0_rmse: 0.55802 | val_1_rmse: 0.54673 |  0:05:34s
epoch 114| loss: 0.30908 | val_0_rmse: 0.56382 | val_1_rmse: 0.554   |  0:05:37s
epoch 115| loss: 0.30509 | val_0_rmse: 0.58115 | val_1_rmse: 0.572   |  0:05:39s
epoch 116| loss: 0.31152 | val_0_rmse: 0.59581 | val_1_rmse: 0.58588 |  0:05:42s
epoch 117| loss: 0.31297 | val_0_rmse: 0.60099 | val_1_rmse: 0.59269 |  0:05:45s
epoch 118| loss: 0.32287 | val_0_rmse: 0.558   | val_1_rmse: 0.54768 |  0:05:48s
epoch 119| loss: 0.31302 | val_0_rmse: 0.57712 | val_1_rmse: 0.56663 |  0:05:51s
epoch 120| loss: 0.30999 | val_0_rmse: 0.56571 | val_1_rmse: 0.55276 |  0:05:54s
epoch 121| loss: 0.30455 | val_0_rmse: 0.58471 | val_1_rmse: 0.57379 |  0:05:57s
epoch 122| loss: 0.30438 | val_0_rmse: 0.5767  | val_1_rmse: 0.56366 |  0:06:00s
epoch 123| loss: 0.30707 | val_0_rmse: 0.59427 | val_1_rmse: 0.58271 |  0:06:03s
epoch 124| loss: 0.30651 | val_0_rmse: 0.55516 | val_1_rmse: 0.54453 |  0:06:06s
epoch 125| loss: 0.31021 | val_0_rmse: 0.58612 | val_1_rmse: 0.57557 |  0:06:09s
epoch 126| loss: 0.33272 | val_0_rmse: 0.58613 | val_1_rmse: 0.57604 |  0:06:12s
epoch 127| loss: 0.33166 | val_0_rmse: 0.58857 | val_1_rmse: 0.57787 |  0:06:14s
epoch 128| loss: 0.33228 | val_0_rmse: 0.604   | val_1_rmse: 0.59353 |  0:06:17s
epoch 129| loss: 0.33407 | val_0_rmse: 0.58093 | val_1_rmse: 0.57068 |  0:06:20s
epoch 130| loss: 0.32702 | val_0_rmse: 0.59293 | val_1_rmse: 0.58252 |  0:06:23s
epoch 131| loss: 0.33626 | val_0_rmse: 0.58206 | val_1_rmse: 0.56886 |  0:06:26s
epoch 132| loss: 0.34446 | val_0_rmse: 0.61247 | val_1_rmse: 0.59857 |  0:06:29s
epoch 133| loss: 0.34892 | val_0_rmse: 0.60839 | val_1_rmse: 0.59831 |  0:06:32s
epoch 134| loss: 0.34198 | val_0_rmse: 0.57965 | val_1_rmse: 0.5734  |  0:06:35s
epoch 135| loss: 0.34752 | val_0_rmse: 0.59253 | val_1_rmse: 0.58034 |  0:06:38s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.54329
Best weights from best epoch are automatically used!
ended training at: 11:27:55
Feature importance:
[('Area', 0.6607824005727534), ('Baths', 0.016991028771861413), ('Beds', 0.15146536664298704), ('Latitude', 0.0), ('Longitude', 0.1496825020137033), ('Month', 0.0), ('Year', 0.021078701998694865)]
Mean squared error is of 985801766.687796
Mean absolute error:21629.59440956734
MAPE:0.3604460098815414
R2 score:0.703006243281949
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:27:55
epoch 0  | loss: 0.38466 | val_0_rmse: 0.59211 | val_1_rmse: 0.59111 |  0:00:13s
epoch 1  | loss: 0.32946 | val_0_rmse: 0.58029 | val_1_rmse: 0.57929 |  0:00:27s
epoch 2  | loss: 0.32378 | val_0_rmse: 0.56063 | val_1_rmse: 0.5591  |  0:00:40s
epoch 3  | loss: 0.31688 | val_0_rmse: 0.55564 | val_1_rmse: 0.55337 |  0:00:54s
epoch 4  | loss: 0.31034 | val_0_rmse: 0.56212 | val_1_rmse: 0.56013 |  0:01:07s
epoch 5  | loss: 0.30436 | val_0_rmse: 0.56893 | val_1_rmse: 0.56888 |  0:01:21s
epoch 6  | loss: 0.3072  | val_0_rmse: 0.5471  | val_1_rmse: 0.54597 |  0:01:34s
epoch 7  | loss: 0.30203 | val_0_rmse: 0.55446 | val_1_rmse: 0.55489 |  0:01:48s
epoch 8  | loss: 0.30019 | val_0_rmse: 0.53777 | val_1_rmse: 0.5373  |  0:02:01s
epoch 9  | loss: 0.30086 | val_0_rmse: 0.546   | val_1_rmse: 0.5464  |  0:02:15s
epoch 10 | loss: 0.29893 | val_0_rmse: 0.53931 | val_1_rmse: 0.53921 |  0:02:28s
epoch 11 | loss: 0.2971  | val_0_rmse: 0.53872 | val_1_rmse: 0.5394  |  0:02:42s
epoch 12 | loss: 0.30111 | val_0_rmse: 0.54393 | val_1_rmse: 0.54364 |  0:02:55s
epoch 13 | loss: 0.31126 | val_0_rmse: 0.5769  | val_1_rmse: 0.57857 |  0:03:09s
epoch 14 | loss: 0.31528 | val_0_rmse: 0.54742 | val_1_rmse: 0.54818 |  0:03:23s
epoch 15 | loss: 0.31048 | val_0_rmse: 0.54546 | val_1_rmse: 0.54487 |  0:03:36s
epoch 16 | loss: 0.30393 | val_0_rmse: 0.57754 | val_1_rmse: 0.57562 |  0:03:50s
epoch 17 | loss: 0.30187 | val_0_rmse: 0.54722 | val_1_rmse: 0.54723 |  0:04:03s
epoch 18 | loss: 0.2999  | val_0_rmse: 0.55317 | val_1_rmse: 0.55199 |  0:04:17s
epoch 19 | loss: 0.30055 | val_0_rmse: 0.55176 | val_1_rmse: 0.5512  |  0:04:30s
epoch 20 | loss: 0.29788 | val_0_rmse: 0.54131 | val_1_rmse: 0.54141 |  0:04:44s
epoch 21 | loss: 0.29835 | val_0_rmse: 0.5382  | val_1_rmse: 0.53874 |  0:04:57s
epoch 22 | loss: 0.29606 | val_0_rmse: 0.54214 | val_1_rmse: 0.54218 |  0:05:11s
epoch 23 | loss: 0.30409 | val_0_rmse: 0.54098 | val_1_rmse: 0.541   |  0:05:24s
epoch 24 | loss: 0.29963 | val_0_rmse: 0.54302 | val_1_rmse: 0.5441  |  0:05:38s
epoch 25 | loss: 0.30138 | val_0_rmse: 0.53794 | val_1_rmse: 0.53781 |  0:05:52s
epoch 26 | loss: 0.29786 | val_0_rmse: 0.54343 | val_1_rmse: 0.54355 |  0:06:05s
epoch 27 | loss: 0.30274 | val_0_rmse: 0.5506  | val_1_rmse: 0.54999 |  0:06:19s
epoch 28 | loss: 0.30884 | val_0_rmse: 0.55951 | val_1_rmse: 0.55944 |  0:06:32s
epoch 29 | loss: 0.30261 | val_0_rmse: 0.55118 | val_1_rmse: 0.55044 |  0:06:46s
epoch 30 | loss: 0.29784 | val_0_rmse: 0.53765 | val_1_rmse: 0.53767 |  0:06:59s
epoch 31 | loss: 0.29593 | val_0_rmse: 0.53586 | val_1_rmse: 0.53655 |  0:07:13s
epoch 32 | loss: 0.29496 | val_0_rmse: 0.55431 | val_1_rmse: 0.55477 |  0:07:26s
epoch 33 | loss: 0.29646 | val_0_rmse: 0.53342 | val_1_rmse: 0.53462 |  0:07:40s
epoch 34 | loss: 0.29642 | val_0_rmse: 0.53689 | val_1_rmse: 0.53777 |  0:07:53s
epoch 35 | loss: 0.29435 | val_0_rmse: 0.54173 | val_1_rmse: 0.54258 |  0:08:07s
epoch 36 | loss: 0.29369 | val_0_rmse: 0.53875 | val_1_rmse: 0.54052 |  0:08:20s
epoch 37 | loss: 0.29369 | val_0_rmse: 0.53441 | val_1_rmse: 0.53559 |  0:08:34s
epoch 38 | loss: 0.29331 | val_0_rmse: 0.53295 | val_1_rmse: 0.53339 |  0:08:47s
epoch 39 | loss: 0.29467 | val_0_rmse: 0.53507 | val_1_rmse: 0.53648 |  0:09:01s
epoch 40 | loss: 0.29439 | val_0_rmse: 0.55733 | val_1_rmse: 0.55741 |  0:09:14s
epoch 41 | loss: 0.29287 | val_0_rmse: 0.53724 | val_1_rmse: 0.53727 |  0:09:28s
epoch 42 | loss: 0.29159 | val_0_rmse: 0.57914 | val_1_rmse: 0.5789  |  0:09:42s
epoch 43 | loss: 0.2927  | val_0_rmse: 0.62566 | val_1_rmse: 0.62774 |  0:09:55s
epoch 44 | loss: 0.2916  | val_0_rmse: 0.53404 | val_1_rmse: 0.53618 |  0:10:09s
epoch 45 | loss: 0.29059 | val_0_rmse: 0.53535 | val_1_rmse: 0.53723 |  0:10:22s
epoch 46 | loss: 0.29157 | val_0_rmse: 0.54309 | val_1_rmse: 0.54542 |  0:10:35s
epoch 47 | loss: 0.29143 | val_0_rmse: 0.53806 | val_1_rmse: 0.53858 |  0:10:49s
epoch 48 | loss: 0.28992 | val_0_rmse: 0.53287 | val_1_rmse: 0.53492 |  0:11:02s
epoch 49 | loss: 0.29645 | val_0_rmse: 0.54391 | val_1_rmse: 0.54469 |  0:11:16s
epoch 50 | loss: 0.29228 | val_0_rmse: 0.58199 | val_1_rmse: 0.58147 |  0:11:29s
epoch 51 | loss: 0.2908  | val_0_rmse: 0.55633 | val_1_rmse: 0.55815 |  0:11:43s
epoch 52 | loss: 0.29292 | val_0_rmse: 0.54477 | val_1_rmse: 0.54526 |  0:11:56s
epoch 53 | loss: 0.30263 | val_0_rmse: 0.59011 | val_1_rmse: 0.58756 |  0:12:10s
epoch 54 | loss: 0.30258 | val_0_rmse: 0.62602 | val_1_rmse: 0.62477 |  0:12:23s
epoch 55 | loss: 0.30036 | val_0_rmse: 0.56    | val_1_rmse: 0.55753 |  0:12:37s
epoch 56 | loss: 0.2965  | val_0_rmse: 0.53785 | val_1_rmse: 0.53745 |  0:12:50s
epoch 57 | loss: 0.29575 | val_0_rmse: 0.53809 | val_1_rmse: 0.53849 |  0:13:04s
epoch 58 | loss: 0.29469 | val_0_rmse: 0.57668 | val_1_rmse: 0.57499 |  0:13:18s
epoch 59 | loss: 0.29309 | val_0_rmse: 0.58067 | val_1_rmse: 0.57945 |  0:13:31s
epoch 60 | loss: 0.29176 | val_0_rmse: 0.53529 | val_1_rmse: 0.5369  |  0:13:45s
epoch 61 | loss: 0.2927  | val_0_rmse: 0.54152 | val_1_rmse: 0.54326 |  0:13:58s
epoch 62 | loss: 0.292   | val_0_rmse: 0.55365 | val_1_rmse: 0.5543  |  0:14:11s
epoch 63 | loss: 0.29129 | val_0_rmse: 0.57856 | val_1_rmse: 0.57752 |  0:14:25s
epoch 64 | loss: 0.28965 | val_0_rmse: 0.57791 | val_1_rmse: 0.57646 |  0:14:38s
epoch 65 | loss: 0.29093 | val_0_rmse: 0.56912 | val_1_rmse: 0.56818 |  0:14:52s
epoch 66 | loss: 0.2894  | val_0_rmse: 0.56355 | val_1_rmse: 0.56259 |  0:15:06s
epoch 67 | loss: 0.29054 | val_0_rmse: 0.53847 | val_1_rmse: 0.53939 |  0:15:19s
epoch 68 | loss: 0.29025 | val_0_rmse: 0.56589 | val_1_rmse: 0.56567 |  0:15:33s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.53339
Best weights from best epoch are automatically used!
ended training at: 11:43:32
Feature importance:
[('Area', 0.19228773400010024), ('Baths', 0.15856812997048358), ('Beds', 0.0), ('Latitude', 0.3531540957633594), ('Longitude', 0.2959900402660568), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 10857611906.166027
Mean absolute error:65587.64817222282
MAPE:0.4258459944057952
R2 score:0.7158415547217412
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 11:43:34
epoch 0  | loss: 0.41043 | val_0_rmse: 0.56574 | val_1_rmse: 0.56716 |  0:00:13s
epoch 1  | loss: 0.32531 | val_0_rmse: 0.57648 | val_1_rmse: 0.5829  |  0:00:27s
epoch 2  | loss: 0.31385 | val_0_rmse: 0.58982 | val_1_rmse: 0.59518 |  0:00:40s
epoch 3  | loss: 0.30978 | val_0_rmse: 0.56029 | val_1_rmse: 0.56409 |  0:00:54s
epoch 4  | loss: 0.30651 | val_0_rmse: 0.5421  | val_1_rmse: 0.54584 |  0:01:07s
epoch 5  | loss: 0.30534 | val_0_rmse: 0.57749 | val_1_rmse: 0.58273 |  0:01:21s
epoch 6  | loss: 0.30908 | val_0_rmse: 0.54143 | val_1_rmse: 0.54612 |  0:01:35s
epoch 7  | loss: 0.30362 | val_0_rmse: 0.5437  | val_1_rmse: 0.54802 |  0:01:48s
epoch 8  | loss: 0.30359 | val_0_rmse: 0.55838 | val_1_rmse: 0.56315 |  0:02:02s
epoch 9  | loss: 0.30654 | val_0_rmse: 0.54581 | val_1_rmse: 0.54968 |  0:02:15s
epoch 10 | loss: 0.30228 | val_0_rmse: 0.54762 | val_1_rmse: 0.55357 |  0:02:29s
epoch 11 | loss: 0.30021 | val_0_rmse: 0.55965 | val_1_rmse: 0.56305 |  0:02:42s
epoch 12 | loss: 0.30041 | val_0_rmse: 0.64085 | val_1_rmse: 0.64604 |  0:02:56s
epoch 13 | loss: 0.29971 | val_0_rmse: 0.54322 | val_1_rmse: 0.54926 |  0:03:09s
epoch 14 | loss: 0.29738 | val_0_rmse: 0.54336 | val_1_rmse: 0.55025 |  0:03:23s
epoch 15 | loss: 0.29576 | val_0_rmse: 0.53669 | val_1_rmse: 0.54326 |  0:03:37s
epoch 16 | loss: 0.29755 | val_0_rmse: 0.55463 | val_1_rmse: 0.55944 |  0:03:50s
epoch 17 | loss: 0.29612 | val_0_rmse: 0.5402  | val_1_rmse: 0.54556 |  0:04:04s
epoch 18 | loss: 0.29505 | val_0_rmse: 0.54872 | val_1_rmse: 0.55473 |  0:04:17s
epoch 19 | loss: 0.29434 | val_0_rmse: 0.54909 | val_1_rmse: 0.55565 |  0:04:31s
epoch 20 | loss: 0.29549 | val_0_rmse: 0.545   | val_1_rmse: 0.55017 |  0:04:44s
epoch 21 | loss: 0.29348 | val_0_rmse: 0.54602 | val_1_rmse: 0.55043 |  0:04:58s
epoch 22 | loss: 0.29451 | val_0_rmse: 0.53827 | val_1_rmse: 0.54347 |  0:05:11s
epoch 23 | loss: 0.29216 | val_0_rmse: 0.5382  | val_1_rmse: 0.54392 |  0:05:25s
epoch 24 | loss: 0.29266 | val_0_rmse: 0.53633 | val_1_rmse: 0.54135 |  0:05:38s
epoch 25 | loss: 0.29234 | val_0_rmse: 0.53526 | val_1_rmse: 0.53988 |  0:05:52s
epoch 26 | loss: 0.29289 | val_0_rmse: 0.5506  | val_1_rmse: 0.55518 |  0:06:05s
epoch 27 | loss: 0.29137 | val_0_rmse: 0.54425 | val_1_rmse: 0.55025 |  0:06:19s
epoch 28 | loss: 0.29259 | val_0_rmse: 0.54388 | val_1_rmse: 0.54915 |  0:06:32s
epoch 29 | loss: 0.29326 | val_0_rmse: 0.53406 | val_1_rmse: 0.53958 |  0:06:46s
epoch 30 | loss: 0.29189 | val_0_rmse: 0.53233 | val_1_rmse: 0.53768 |  0:07:00s
epoch 31 | loss: 0.2906  | val_0_rmse: 0.53649 | val_1_rmse: 0.54345 |  0:07:13s
epoch 32 | loss: 0.29036 | val_0_rmse: 0.54225 | val_1_rmse: 0.54771 |  0:07:27s
epoch 33 | loss: 0.29031 | val_0_rmse: 0.54018 | val_1_rmse: 0.54659 |  0:07:40s
epoch 34 | loss: 0.29749 | val_0_rmse: 0.55194 | val_1_rmse: 0.55533 |  0:07:54s
epoch 35 | loss: 0.29285 | val_0_rmse: 0.53407 | val_1_rmse: 0.54056 |  0:08:07s
epoch 36 | loss: 0.29109 | val_0_rmse: 0.53539 | val_1_rmse: 0.5415  |  0:08:21s
epoch 37 | loss: 0.29913 | val_0_rmse: 0.53718 | val_1_rmse: 0.5441  |  0:08:34s
epoch 38 | loss: 0.29311 | val_0_rmse: 0.54691 | val_1_rmse: 0.55297 |  0:08:48s
epoch 39 | loss: 0.29388 | val_0_rmse: 0.53934 | val_1_rmse: 0.54388 |  0:09:01s
epoch 40 | loss: 0.29349 | val_0_rmse: 0.5473  | val_1_rmse: 0.55224 |  0:09:15s
epoch 41 | loss: 0.29088 | val_0_rmse: 0.53114 | val_1_rmse: 0.53604 |  0:09:28s
epoch 42 | loss: 0.29142 | val_0_rmse: 0.56161 | val_1_rmse: 0.56669 |  0:09:42s
epoch 43 | loss: 0.2907  | val_0_rmse: 0.53099 | val_1_rmse: 0.53706 |  0:09:55s
epoch 44 | loss: 0.2899  | val_0_rmse: 0.5616  | val_1_rmse: 0.56812 |  0:10:09s
epoch 45 | loss: 0.28964 | val_0_rmse: 0.54091 | val_1_rmse: 0.54689 |  0:10:22s
epoch 46 | loss: 0.28962 | val_0_rmse: 0.53834 | val_1_rmse: 0.54339 |  0:10:36s
epoch 47 | loss: 0.28886 | val_0_rmse: 0.53178 | val_1_rmse: 0.53758 |  0:10:49s
epoch 48 | loss: 0.28838 | val_0_rmse: 0.54224 | val_1_rmse: 0.54885 |  0:11:03s
epoch 49 | loss: 0.2884  | val_0_rmse: 0.53888 | val_1_rmse: 0.54456 |  0:11:17s
epoch 50 | loss: 0.28842 | val_0_rmse: 0.55519 | val_1_rmse: 0.56076 |  0:11:30s
epoch 51 | loss: 0.28973 | val_0_rmse: 0.53445 | val_1_rmse: 0.54123 |  0:11:44s
epoch 52 | loss: 0.28797 | val_0_rmse: 0.5343  | val_1_rmse: 0.54172 |  0:11:57s
epoch 53 | loss: 0.28753 | val_0_rmse: 0.54081 | val_1_rmse: 0.54695 |  0:12:11s
epoch 54 | loss: 0.28956 | val_0_rmse: 0.53049 | val_1_rmse: 0.53645 |  0:12:24s
epoch 55 | loss: 0.28843 | val_0_rmse: 0.52833 | val_1_rmse: 0.53444 |  0:12:38s
epoch 56 | loss: 0.28765 | val_0_rmse: 0.56029 | val_1_rmse: 0.56715 |  0:12:52s
epoch 57 | loss: 0.28725 | val_0_rmse: 0.55252 | val_1_rmse: 0.55846 |  0:13:05s
epoch 58 | loss: 0.28622 | val_0_rmse: 0.53535 | val_1_rmse: 0.54149 |  0:13:19s
epoch 59 | loss: 0.28624 | val_0_rmse: 0.5277  | val_1_rmse: 0.53413 |  0:13:32s
epoch 60 | loss: 0.288   | val_0_rmse: 0.5399  | val_1_rmse: 0.54756 |  0:13:46s
epoch 61 | loss: 0.28609 | val_0_rmse: 0.53226 | val_1_rmse: 0.53817 |  0:13:59s
epoch 62 | loss: 0.28865 | val_0_rmse: 0.53278 | val_1_rmse: 0.53877 |  0:14:13s
epoch 63 | loss: 0.28576 | val_0_rmse: 0.53591 | val_1_rmse: 0.54326 |  0:14:26s
epoch 64 | loss: 0.28569 | val_0_rmse: 0.53011 | val_1_rmse: 0.53504 |  0:14:40s
epoch 65 | loss: 0.28683 | val_0_rmse: 0.52997 | val_1_rmse: 0.53642 |  0:14:53s
epoch 66 | loss: 0.28556 | val_0_rmse: 0.53602 | val_1_rmse: 0.54202 |  0:15:07s
epoch 67 | loss: 0.29281 | val_0_rmse: 0.53455 | val_1_rmse: 0.54061 |  0:15:21s
epoch 68 | loss: 0.29007 | val_0_rmse: 0.5439  | val_1_rmse: 0.54984 |  0:15:34s
epoch 69 | loss: 0.29018 | val_0_rmse: 0.53146 | val_1_rmse: 0.53772 |  0:15:48s
epoch 70 | loss: 0.28788 | val_0_rmse: 0.53864 | val_1_rmse: 0.5458  |  0:16:01s
epoch 71 | loss: 0.28939 | val_0_rmse: 0.54576 | val_1_rmse: 0.55259 |  0:16:15s
epoch 72 | loss: 0.28785 | val_0_rmse: 0.54007 | val_1_rmse: 0.54655 |  0:16:28s
epoch 73 | loss: 0.28689 | val_0_rmse: 0.53491 | val_1_rmse: 0.54201 |  0:16:42s
epoch 74 | loss: 0.28644 | val_0_rmse: 0.53018 | val_1_rmse: 0.53712 |  0:16:56s
epoch 75 | loss: 0.28694 | val_0_rmse: 0.53251 | val_1_rmse: 0.53912 |  0:17:09s
epoch 76 | loss: 0.28727 | val_0_rmse: 0.53323 | val_1_rmse: 0.54019 |  0:17:23s
epoch 77 | loss: 0.28696 | val_0_rmse: 0.53345 | val_1_rmse: 0.5404  |  0:17:36s
epoch 78 | loss: 0.28563 | val_0_rmse: 0.54059 | val_1_rmse: 0.54596 |  0:17:50s
epoch 79 | loss: 0.28812 | val_0_rmse: 0.53905 | val_1_rmse: 0.54559 |  0:18:03s
epoch 80 | loss: 0.28606 | val_0_rmse: 0.53803 | val_1_rmse: 0.54497 |  0:18:17s
epoch 81 | loss: 0.28863 | val_0_rmse: 0.53719 | val_1_rmse: 0.54371 |  0:18:30s
epoch 82 | loss: 0.28585 | val_0_rmse: 0.53505 | val_1_rmse: 0.54211 |  0:18:44s
epoch 83 | loss: 0.28583 | val_0_rmse: 0.53634 | val_1_rmse: 0.54304 |  0:18:57s
epoch 84 | loss: 0.28578 | val_0_rmse: 0.53316 | val_1_rmse: 0.539   |  0:19:11s
epoch 85 | loss: 0.28545 | val_0_rmse: 0.52778 | val_1_rmse: 0.53436 |  0:19:24s
epoch 86 | loss: 0.28625 | val_0_rmse: 0.54083 | val_1_rmse: 0.54847 |  0:19:38s
epoch 87 | loss: 0.28591 | val_0_rmse: 0.55147 | val_1_rmse: 0.55799 |  0:19:51s
epoch 88 | loss: 0.28546 | val_0_rmse: 0.53525 | val_1_rmse: 0.54218 |  0:20:05s
epoch 89 | loss: 0.28582 | val_0_rmse: 0.53058 | val_1_rmse: 0.53851 |  0:20:19s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.53413
Best weights from best epoch are automatically used!
ended training at: 12:03:57
Feature importance:
[('Area', 0.08960109487379639), ('Baths', 0.1535852221480727), ('Beds', 0.008895902619387035), ('Latitude', 0.5069748878391951), ('Longitude', 0.1787517443579571), ('Month', 0.0), ('Year', 0.06219114816159169)]
Mean squared error is of 10846916360.607298
Mean absolute error:66114.61909745014
MAPE:0.45014565547945634
R2 score:0.7204387102623664
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 12:03:58
epoch 0  | loss: 0.41984 | val_0_rmse: 0.58478 | val_1_rmse: 0.58225 |  0:00:13s
epoch 1  | loss: 0.35382 | val_0_rmse: 0.59774 | val_1_rmse: 0.59815 |  0:00:26s
epoch 2  | loss: 0.35671 | val_0_rmse: 0.56251 | val_1_rmse: 0.56351 |  0:00:40s
epoch 3  | loss: 0.32698 | val_0_rmse: 0.56724 | val_1_rmse: 0.56911 |  0:00:53s
epoch 4  | loss: 0.31832 | val_0_rmse: 0.55109 | val_1_rmse: 0.55113 |  0:01:07s
epoch 5  | loss: 0.31574 | val_0_rmse: 0.55748 | val_1_rmse: 0.55862 |  0:01:20s
epoch 6  | loss: 0.31506 | val_0_rmse: 0.54756 | val_1_rmse: 0.54742 |  0:01:34s
epoch 7  | loss: 0.31058 | val_0_rmse: 0.54413 | val_1_rmse: 0.54404 |  0:01:47s
epoch 8  | loss: 0.30931 | val_0_rmse: 0.5471  | val_1_rmse: 0.54727 |  0:02:01s
epoch 9  | loss: 0.3087  | val_0_rmse: 0.55738 | val_1_rmse: 0.55736 |  0:02:14s
epoch 10 | loss: 0.30894 | val_0_rmse: 0.54441 | val_1_rmse: 0.54423 |  0:02:27s
epoch 11 | loss: 0.30581 | val_0_rmse: 0.54581 | val_1_rmse: 0.54679 |  0:02:41s
epoch 12 | loss: 0.30815 | val_0_rmse: 0.55707 | val_1_rmse: 0.55825 |  0:02:54s
epoch 13 | loss: 0.30728 | val_0_rmse: 0.55684 | val_1_rmse: 0.55311 |  0:03:08s
epoch 14 | loss: 0.30601 | val_0_rmse: 0.54309 | val_1_rmse: 0.54229 |  0:03:21s
epoch 15 | loss: 0.3037  | val_0_rmse: 0.5604  | val_1_rmse: 0.56004 |  0:03:35s
epoch 16 | loss: 0.30343 | val_0_rmse: 0.53997 | val_1_rmse: 0.53919 |  0:03:48s
epoch 17 | loss: 0.30117 | val_0_rmse: 0.54115 | val_1_rmse: 0.54105 |  0:04:01s
epoch 18 | loss: 0.30063 | val_0_rmse: 0.54054 | val_1_rmse: 0.54116 |  0:04:15s
epoch 19 | loss: 0.3025  | val_0_rmse: 0.54162 | val_1_rmse: 0.54175 |  0:04:28s
epoch 20 | loss: 0.30112 | val_0_rmse: 0.55592 | val_1_rmse: 0.55617 |  0:04:42s
epoch 21 | loss: 0.29961 | val_0_rmse: 0.54248 | val_1_rmse: 0.54358 |  0:04:55s
epoch 22 | loss: 0.30256 | val_0_rmse: 0.55956 | val_1_rmse: 0.55994 |  0:05:08s
epoch 23 | loss: 0.3023  | val_0_rmse: 0.55617 | val_1_rmse: 0.55844 |  0:05:22s
epoch 24 | loss: 0.29895 | val_0_rmse: 0.5456  | val_1_rmse: 0.54396 |  0:05:35s
epoch 25 | loss: 0.29903 | val_0_rmse: 0.54187 | val_1_rmse: 0.54328 |  0:05:49s
epoch 26 | loss: 0.30099 | val_0_rmse: 0.54882 | val_1_rmse: 0.54652 |  0:06:02s
epoch 27 | loss: 0.29696 | val_0_rmse: 0.53733 | val_1_rmse: 0.53782 |  0:06:16s
epoch 28 | loss: 0.29614 | val_0_rmse: 0.53998 | val_1_rmse: 0.5395  |  0:06:29s
epoch 29 | loss: 0.29453 | val_0_rmse: 0.53604 | val_1_rmse: 0.53565 |  0:06:43s
epoch 30 | loss: 0.2992  | val_0_rmse: 0.53764 | val_1_rmse: 0.53689 |  0:06:56s
epoch 31 | loss: 0.29583 | val_0_rmse: 0.54066 | val_1_rmse: 0.54029 |  0:07:10s
epoch 32 | loss: 0.29915 | val_0_rmse: 0.54349 | val_1_rmse: 0.543   |  0:07:23s
epoch 33 | loss: 0.29678 | val_0_rmse: 0.54901 | val_1_rmse: 0.54942 |  0:07:37s
epoch 34 | loss: 0.2944  | val_0_rmse: 0.54289 | val_1_rmse: 0.54441 |  0:07:50s
epoch 35 | loss: 0.29441 | val_0_rmse: 0.5378  | val_1_rmse: 0.53782 |  0:08:04s
epoch 36 | loss: 0.29281 | val_0_rmse: 0.53329 | val_1_rmse: 0.5333  |  0:08:17s
epoch 37 | loss: 0.29251 | val_0_rmse: 0.53512 | val_1_rmse: 0.53575 |  0:08:31s
epoch 38 | loss: 0.29357 | val_0_rmse: 0.5379  | val_1_rmse: 0.53756 |  0:08:44s
epoch 39 | loss: 0.29123 | val_0_rmse: 0.58098 | val_1_rmse: 0.57765 |  0:08:58s
epoch 40 | loss: 0.29459 | val_0_rmse: 0.54697 | val_1_rmse: 0.54586 |  0:09:11s
epoch 41 | loss: 0.2938  | val_0_rmse: 0.53266 | val_1_rmse: 0.53277 |  0:09:25s
epoch 42 | loss: 0.292   | val_0_rmse: 0.53664 | val_1_rmse: 0.53564 |  0:09:38s
epoch 43 | loss: 0.29087 | val_0_rmse: 0.53154 | val_1_rmse: 0.53088 |  0:09:51s
epoch 44 | loss: 0.28934 | val_0_rmse: 0.53311 | val_1_rmse: 0.53212 |  0:10:05s
epoch 45 | loss: 0.29108 | val_0_rmse: 0.53938 | val_1_rmse: 0.53908 |  0:10:18s
epoch 46 | loss: 0.28995 | val_0_rmse: 0.52921 | val_1_rmse: 0.52872 |  0:10:32s
epoch 47 | loss: 0.29109 | val_0_rmse: 0.53136 | val_1_rmse: 0.53183 |  0:10:45s
epoch 48 | loss: 0.29028 | val_0_rmse: 0.54046 | val_1_rmse: 0.54315 |  0:10:59s
epoch 49 | loss: 0.29069 | val_0_rmse: 0.53892 | val_1_rmse: 0.54065 |  0:11:12s
epoch 50 | loss: 0.29037 | val_0_rmse: 0.53622 | val_1_rmse: 0.5362  |  0:11:26s
epoch 51 | loss: 0.2891  | val_0_rmse: 0.54043 | val_1_rmse: 0.54112 |  0:11:39s
epoch 52 | loss: 0.28828 | val_0_rmse: 0.53286 | val_1_rmse: 0.5326  |  0:11:53s
epoch 53 | loss: 0.28879 | val_0_rmse: 0.54149 | val_1_rmse: 0.53911 |  0:12:06s
epoch 54 | loss: 0.29072 | val_0_rmse: 0.53808 | val_1_rmse: 0.53948 |  0:12:20s
epoch 55 | loss: 0.2894  | val_0_rmse: 0.54221 | val_1_rmse: 0.54124 |  0:12:33s
epoch 56 | loss: 0.29218 | val_0_rmse: 0.53343 | val_1_rmse: 0.5332  |  0:12:47s
epoch 57 | loss: 0.29169 | val_0_rmse: 0.5321  | val_1_rmse: 0.53132 |  0:13:00s
epoch 58 | loss: 0.29114 | val_0_rmse: 0.54996 | val_1_rmse: 0.5517  |  0:13:14s
epoch 59 | loss: 0.29476 | val_0_rmse: 0.54853 | val_1_rmse: 0.55213 |  0:13:27s
epoch 60 | loss: 0.29439 | val_0_rmse: 0.55181 | val_1_rmse: 0.55159 |  0:13:41s
epoch 61 | loss: 0.29205 | val_0_rmse: 0.53322 | val_1_rmse: 0.53476 |  0:13:54s
epoch 62 | loss: 0.29138 | val_0_rmse: 0.53279 | val_1_rmse: 0.53429 |  0:14:08s
epoch 63 | loss: 0.29042 | val_0_rmse: 0.5349  | val_1_rmse: 0.5357  |  0:14:22s
epoch 64 | loss: 0.28888 | val_0_rmse: 0.53202 | val_1_rmse: 0.53231 |  0:14:35s
epoch 65 | loss: 0.29158 | val_0_rmse: 0.54229 | val_1_rmse: 0.54192 |  0:14:49s
epoch 66 | loss: 0.28768 | val_0_rmse: 0.5314  | val_1_rmse: 0.53225 |  0:15:02s
epoch 67 | loss: 0.31381 | val_0_rmse: 0.53058 | val_1_rmse: 0.53055 |  0:15:15s
epoch 68 | loss: 0.2875  | val_0_rmse: 0.5388  | val_1_rmse: 0.5401  |  0:15:29s
epoch 69 | loss: 0.28629 | val_0_rmse: 0.54663 | val_1_rmse: 0.54437 |  0:15:43s
epoch 70 | loss: 0.28639 | val_0_rmse: 0.52927 | val_1_rmse: 0.52901 |  0:15:56s
epoch 71 | loss: 0.28651 | val_0_rmse: 0.533   | val_1_rmse: 0.53395 |  0:16:10s
epoch 72 | loss: 0.2877  | val_0_rmse: 0.53329 | val_1_rmse: 0.53504 |  0:16:23s
epoch 73 | loss: 0.28859 | val_0_rmse: 0.53279 | val_1_rmse: 0.53409 |  0:16:37s
epoch 74 | loss: 0.2873  | val_0_rmse: 0.53175 | val_1_rmse: 0.53254 |  0:16:50s
epoch 75 | loss: 0.28636 | val_0_rmse: 0.53278 | val_1_rmse: 0.53489 |  0:17:03s
epoch 76 | loss: 0.28604 | val_0_rmse: 0.55104 | val_1_rmse: 0.55179 |  0:17:17s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.52872
Best weights from best epoch are automatically used!
ended training at: 12:21:20
Feature importance:
[('Area', 0.1441854546298453), ('Baths', 0.1586830894496514), ('Beds', 0.05801910040495869), ('Latitude', 0.3656446718845627), ('Longitude', 0.17993253001065107), ('Month', 0.0), ('Year', 0.09353515362033085)]
Mean squared error is of 11068704158.25644
Mean absolute error:65889.82690733949
MAPE:0.4183838189284224
R2 score:0.717650353918612
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 12:21:21
epoch 0  | loss: 0.39732 | val_0_rmse: 0.59062 | val_1_rmse: 0.59397 |  0:00:13s
epoch 1  | loss: 0.33104 | val_0_rmse: 0.5609  | val_1_rmse: 0.56366 |  0:00:26s
epoch 2  | loss: 0.32588 | val_0_rmse: 0.58396 | val_1_rmse: 0.58644 |  0:00:40s
epoch 3  | loss: 0.31636 | val_0_rmse: 0.54891 | val_1_rmse: 0.55143 |  0:00:53s
epoch 4  | loss: 0.317   | val_0_rmse: 0.54971 | val_1_rmse: 0.55362 |  0:01:07s
epoch 5  | loss: 0.32024 | val_0_rmse: 0.55464 | val_1_rmse: 0.55752 |  0:01:20s
epoch 6  | loss: 0.3116  | val_0_rmse: 0.55214 | val_1_rmse: 0.55507 |  0:01:34s
epoch 7  | loss: 0.30973 | val_0_rmse: 0.54857 | val_1_rmse: 0.55117 |  0:01:47s
epoch 8  | loss: 0.30979 | val_0_rmse: 0.5511  | val_1_rmse: 0.55267 |  0:02:01s
epoch 9  | loss: 0.30817 | val_0_rmse: 0.6237  | val_1_rmse: 0.62375 |  0:02:14s
epoch 10 | loss: 0.30591 | val_0_rmse: 0.54442 | val_1_rmse: 0.54673 |  0:02:27s
epoch 11 | loss: 0.30613 | val_0_rmse: 0.54772 | val_1_rmse: 0.55011 |  0:02:41s
epoch 12 | loss: 0.30619 | val_0_rmse: 0.56413 | val_1_rmse: 0.56712 |  0:02:54s
epoch 13 | loss: 0.30529 | val_0_rmse: 0.60828 | val_1_rmse: 0.61031 |  0:03:08s
epoch 14 | loss: 0.31853 | val_0_rmse: 0.58954 | val_1_rmse: 0.59066 |  0:03:21s
epoch 15 | loss: 0.31644 | val_0_rmse: 0.54869 | val_1_rmse: 0.55099 |  0:03:34s
epoch 16 | loss: 0.30825 | val_0_rmse: 0.55227 | val_1_rmse: 0.55426 |  0:03:48s
epoch 17 | loss: 0.30531 | val_0_rmse: 0.54381 | val_1_rmse: 0.54532 |  0:04:01s
epoch 18 | loss: 0.30325 | val_0_rmse: 0.58379 | val_1_rmse: 0.58566 |  0:04:15s
epoch 19 | loss: 0.30429 | val_0_rmse: 0.55145 | val_1_rmse: 0.55349 |  0:04:28s
epoch 20 | loss: 0.30358 | val_0_rmse: 0.55457 | val_1_rmse: 0.55756 |  0:04:42s
epoch 21 | loss: 0.30345 | val_0_rmse: 0.54362 | val_1_rmse: 0.5469  |  0:04:55s
epoch 22 | loss: 0.30279 | val_0_rmse: 0.54292 | val_1_rmse: 0.54539 |  0:05:09s
epoch 23 | loss: 0.29974 | val_0_rmse: 0.54675 | val_1_rmse: 0.54909 |  0:05:22s
epoch 24 | loss: 0.30422 | val_0_rmse: 0.54753 | val_1_rmse: 0.54914 |  0:05:36s
epoch 25 | loss: 0.30265 | val_0_rmse: 0.54535 | val_1_rmse: 0.54792 |  0:05:49s
epoch 26 | loss: 0.3035  | val_0_rmse: 0.54506 | val_1_rmse: 0.547   |  0:06:03s
epoch 27 | loss: 0.30051 | val_0_rmse: 0.55645 | val_1_rmse: 0.55775 |  0:06:16s
epoch 28 | loss: 0.29816 | val_0_rmse: 0.54553 | val_1_rmse: 0.54802 |  0:06:30s
epoch 29 | loss: 0.29856 | val_0_rmse: 0.55532 | val_1_rmse: 0.55861 |  0:06:43s
epoch 30 | loss: 0.29662 | val_0_rmse: 0.53853 | val_1_rmse: 0.5402  |  0:06:57s
epoch 31 | loss: 0.30067 | val_0_rmse: 0.60004 | val_1_rmse: 0.60107 |  0:07:10s
epoch 32 | loss: 0.30284 | val_0_rmse: 0.58331 | val_1_rmse: 0.58451 |  0:07:23s
epoch 33 | loss: 0.29918 | val_0_rmse: 0.5843  | val_1_rmse: 0.58433 |  0:07:37s
epoch 34 | loss: 0.30278 | val_0_rmse: 0.54134 | val_1_rmse: 0.5442  |  0:07:50s
epoch 35 | loss: 0.29754 | val_0_rmse: 0.54617 | val_1_rmse: 0.54864 |  0:08:04s
epoch 36 | loss: 0.2972  | val_0_rmse: 0.54698 | val_1_rmse: 0.54976 |  0:08:17s
epoch 37 | loss: 0.29655 | val_0_rmse: 0.54435 | val_1_rmse: 0.54723 |  0:08:31s
epoch 38 | loss: 0.29567 | val_0_rmse: 0.54413 | val_1_rmse: 0.54737 |  0:08:44s
epoch 39 | loss: 0.29481 | val_0_rmse: 0.55141 | val_1_rmse: 0.55501 |  0:08:58s
epoch 40 | loss: 0.29917 | val_0_rmse: 0.5402  | val_1_rmse: 0.54239 |  0:09:11s
epoch 41 | loss: 0.29372 | val_0_rmse: 0.53431 | val_1_rmse: 0.53702 |  0:09:25s
epoch 42 | loss: 0.29353 | val_0_rmse: 0.53883 | val_1_rmse: 0.54254 |  0:09:38s
epoch 43 | loss: 0.29278 | val_0_rmse: 0.53851 | val_1_rmse: 0.54133 |  0:09:52s
epoch 44 | loss: 0.29286 | val_0_rmse: 0.54986 | val_1_rmse: 0.5523  |  0:10:05s
epoch 45 | loss: 0.29407 | val_0_rmse: 0.59123 | val_1_rmse: 0.59274 |  0:10:19s
epoch 46 | loss: 0.30571 | val_0_rmse: 0.65434 | val_1_rmse: 0.65492 |  0:10:32s
epoch 47 | loss: 0.32843 | val_0_rmse: 0.54888 | val_1_rmse: 0.5514  |  0:10:46s
epoch 48 | loss: 0.30435 | val_0_rmse: 0.54342 | val_1_rmse: 0.54578 |  0:10:59s
epoch 49 | loss: 0.30224 | val_0_rmse: 0.54114 | val_1_rmse: 0.54384 |  0:11:13s
epoch 50 | loss: 0.2987  | val_0_rmse: 0.54471 | val_1_rmse: 0.54779 |  0:11:26s
epoch 51 | loss: 0.29664 | val_0_rmse: 0.53709 | val_1_rmse: 0.53923 |  0:11:40s
epoch 52 | loss: 0.29516 | val_0_rmse: 0.53848 | val_1_rmse: 0.54132 |  0:11:53s
epoch 53 | loss: 0.2951  | val_0_rmse: 0.53925 | val_1_rmse: 0.54314 |  0:12:06s
epoch 54 | loss: 0.29491 | val_0_rmse: 0.53818 | val_1_rmse: 0.54051 |  0:12:20s
epoch 55 | loss: 0.29361 | val_0_rmse: 0.53757 | val_1_rmse: 0.53988 |  0:12:33s
epoch 56 | loss: 0.29589 | val_0_rmse: 0.53619 | val_1_rmse: 0.53914 |  0:12:47s
epoch 57 | loss: 0.2947  | val_0_rmse: 0.53658 | val_1_rmse: 0.53911 |  0:13:00s
epoch 58 | loss: 0.29348 | val_0_rmse: 0.53745 | val_1_rmse: 0.54108 |  0:13:14s
epoch 59 | loss: 0.31557 | val_0_rmse: 0.54831 | val_1_rmse: 0.54979 |  0:13:27s
epoch 60 | loss: 0.30047 | val_0_rmse: 0.54267 | val_1_rmse: 0.54586 |  0:13:41s
epoch 61 | loss: 0.29804 | val_0_rmse: 0.54917 | val_1_rmse: 0.55146 |  0:13:54s
epoch 62 | loss: 0.29444 | val_0_rmse: 0.54514 | val_1_rmse: 0.54788 |  0:14:08s
epoch 63 | loss: 0.29396 | val_0_rmse: 0.53577 | val_1_rmse: 0.5393  |  0:14:21s
epoch 64 | loss: 0.29968 | val_0_rmse: 0.53805 | val_1_rmse: 0.54115 |  0:14:35s
epoch 65 | loss: 0.29304 | val_0_rmse: 0.53582 | val_1_rmse: 0.53838 |  0:14:48s
epoch 66 | loss: 0.29363 | val_0_rmse: 0.55281 | val_1_rmse: 0.5563  |  0:15:02s
epoch 67 | loss: 0.291   | val_0_rmse: 0.53839 | val_1_rmse: 0.5426  |  0:15:15s
epoch 68 | loss: 0.29309 | val_0_rmse: 0.53557 | val_1_rmse: 0.53943 |  0:15:28s
epoch 69 | loss: 0.29235 | val_0_rmse: 0.53783 | val_1_rmse: 0.54115 |  0:15:42s
epoch 70 | loss: 0.29182 | val_0_rmse: 0.5413  | val_1_rmse: 0.54409 |  0:15:55s
epoch 71 | loss: 0.29339 | val_0_rmse: 0.53886 | val_1_rmse: 0.54217 |  0:16:09s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.53702
Best weights from best epoch are automatically used!
ended training at: 12:37:34
Feature importance:
[('Area', 0.26723417613794975), ('Baths', 0.0), ('Beds', 0.11532836071636521), ('Latitude', 0.09063930864469393), ('Longitude', 0.3531819806622706), ('Month', 0.0), ('Year', 0.17361617383872052)]
Mean squared error is of 11068389317.561632
Mean absolute error:65993.36783053562
MAPE:0.4369456309713308
R2 score:0.7158640511078893
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 12:37:36
epoch 0  | loss: 0.38905 | val_0_rmse: 0.59992 | val_1_rmse: 0.59818 |  0:00:13s
epoch 1  | loss: 0.34847 | val_0_rmse: 0.58968 | val_1_rmse: 0.58694 |  0:00:26s
epoch 2  | loss: 0.3432  | val_0_rmse: 0.58312 | val_1_rmse: 0.58027 |  0:00:40s
epoch 3  | loss: 0.33874 | val_0_rmse: 0.58532 | val_1_rmse: 0.58599 |  0:00:53s
epoch 4  | loss: 0.3357  | val_0_rmse: 0.56821 | val_1_rmse: 0.56631 |  0:01:07s
epoch 5  | loss: 0.35646 | val_0_rmse: 0.65921 | val_1_rmse: 0.65957 |  0:01:20s
epoch 6  | loss: 0.33843 | val_0_rmse: 0.56613 | val_1_rmse: 0.56474 |  0:01:34s
epoch 7  | loss: 0.3233  | val_0_rmse: 0.57191 | val_1_rmse: 0.57235 |  0:01:47s
epoch 8  | loss: 0.32397 | val_0_rmse: 0.60307 | val_1_rmse: 0.60491 |  0:02:01s
epoch 9  | loss: 0.31947 | val_0_rmse: 0.56222 | val_1_rmse: 0.5601  |  0:02:14s
epoch 10 | loss: 0.32968 | val_0_rmse: 0.56448 | val_1_rmse: 0.56435 |  0:02:28s
epoch 11 | loss: 0.32154 | val_0_rmse: 0.59559 | val_1_rmse: 0.593   |  0:02:41s
epoch 12 | loss: 0.31784 | val_0_rmse: 0.57192 | val_1_rmse: 0.57143 |  0:02:54s
epoch 13 | loss: 0.31711 | val_0_rmse: 0.55367 | val_1_rmse: 0.55237 |  0:03:08s
epoch 14 | loss: 0.3348  | val_0_rmse: 0.56544 | val_1_rmse: 0.56365 |  0:03:21s
epoch 15 | loss: 0.3136  | val_0_rmse: 0.54921 | val_1_rmse: 0.54757 |  0:03:35s
epoch 16 | loss: 0.30642 | val_0_rmse: 0.62516 | val_1_rmse: 0.62126 |  0:03:48s
epoch 17 | loss: 0.30513 | val_0_rmse: 0.55173 | val_1_rmse: 0.55031 |  0:04:02s
epoch 18 | loss: 0.3045  | val_0_rmse: 0.54434 | val_1_rmse: 0.54105 |  0:04:15s
epoch 19 | loss: 0.29902 | val_0_rmse: 0.54824 | val_1_rmse: 0.54709 |  0:04:29s
epoch 20 | loss: 0.2994  | val_0_rmse: 0.54185 | val_1_rmse: 0.53996 |  0:04:42s
epoch 21 | loss: 0.30064 | val_0_rmse: 0.54531 | val_1_rmse: 0.54193 |  0:04:56s
epoch 22 | loss: 0.2995  | val_0_rmse: 0.55117 | val_1_rmse: 0.54882 |  0:05:09s
epoch 23 | loss: 0.29825 | val_0_rmse: 0.54211 | val_1_rmse: 0.54161 |  0:05:22s
epoch 24 | loss: 0.29749 | val_0_rmse: 0.54116 | val_1_rmse: 0.53895 |  0:05:36s
epoch 25 | loss: 0.29871 | val_0_rmse: 0.5378  | val_1_rmse: 0.53618 |  0:05:49s
epoch 26 | loss: 0.29597 | val_0_rmse: 0.54114 | val_1_rmse: 0.53937 |  0:06:03s
epoch 27 | loss: 0.29711 | val_0_rmse: 0.5384  | val_1_rmse: 0.53582 |  0:06:16s
epoch 28 | loss: 0.29756 | val_0_rmse: 0.55892 | val_1_rmse: 0.55597 |  0:06:30s
epoch 29 | loss: 0.29864 | val_0_rmse: 0.71623 | val_1_rmse: 0.71201 |  0:06:43s
epoch 30 | loss: 0.30238 | val_0_rmse: 0.54657 | val_1_rmse: 0.54356 |  0:06:57s
epoch 31 | loss: 0.29727 | val_0_rmse: 0.54452 | val_1_rmse: 0.54389 |  0:07:10s
epoch 32 | loss: 0.29633 | val_0_rmse: 0.54119 | val_1_rmse: 0.53936 |  0:07:23s
epoch 33 | loss: 0.3124  | val_0_rmse: 0.59711 | val_1_rmse: 0.59392 |  0:07:37s
epoch 34 | loss: 0.30896 | val_0_rmse: 0.54425 | val_1_rmse: 0.54218 |  0:07:51s
epoch 35 | loss: 0.3025  | val_0_rmse: 0.54824 | val_1_rmse: 0.54815 |  0:08:04s
epoch 36 | loss: 0.3016  | val_0_rmse: 0.54234 | val_1_rmse: 0.53891 |  0:08:17s
epoch 37 | loss: 0.30119 | val_0_rmse: 0.53983 | val_1_rmse: 0.53714 |  0:08:31s
epoch 38 | loss: 0.2992  | val_0_rmse: 0.54183 | val_1_rmse: 0.54079 |  0:08:44s
epoch 39 | loss: 0.29997 | val_0_rmse: 0.54952 | val_1_rmse: 0.54891 |  0:08:58s
epoch 40 | loss: 0.30054 | val_0_rmse: 0.54821 | val_1_rmse: 0.54665 |  0:09:11s
epoch 41 | loss: 0.30066 | val_0_rmse: 0.54908 | val_1_rmse: 0.54645 |  0:09:25s
epoch 42 | loss: 0.30256 | val_0_rmse: 0.55736 | val_1_rmse: 0.55553 |  0:09:38s
epoch 43 | loss: 0.29771 | val_0_rmse: 0.54636 | val_1_rmse: 0.54524 |  0:09:52s
epoch 44 | loss: 0.29638 | val_0_rmse: 0.56534 | val_1_rmse: 0.56412 |  0:10:05s
epoch 45 | loss: 0.29696 | val_0_rmse: 0.54647 | val_1_rmse: 0.54418 |  0:10:18s
epoch 46 | loss: 0.30024 | val_0_rmse: 0.55527 | val_1_rmse: 0.55533 |  0:10:32s
epoch 47 | loss: 0.29607 | val_0_rmse: 0.54133 | val_1_rmse: 0.5394  |  0:10:46s
epoch 48 | loss: 0.29525 | val_0_rmse: 0.53723 | val_1_rmse: 0.53533 |  0:10:59s
epoch 49 | loss: 0.29644 | val_0_rmse: 0.5424  | val_1_rmse: 0.54211 |  0:11:13s
epoch 50 | loss: 0.29512 | val_0_rmse: 0.53597 | val_1_rmse: 0.53483 |  0:11:26s
epoch 51 | loss: 0.2932  | val_0_rmse: 0.54215 | val_1_rmse: 0.54047 |  0:11:39s
epoch 52 | loss: 0.29384 | val_0_rmse: 0.55146 | val_1_rmse: 0.54988 |  0:11:53s
epoch 53 | loss: 0.29538 | val_0_rmse: 0.53833 | val_1_rmse: 0.53664 |  0:12:06s
epoch 54 | loss: 0.29348 | val_0_rmse: 0.54073 | val_1_rmse: 0.53858 |  0:12:20s
epoch 55 | loss: 0.29272 | val_0_rmse: 0.55017 | val_1_rmse: 0.54837 |  0:12:33s
epoch 56 | loss: 0.29304 | val_0_rmse: 0.53774 | val_1_rmse: 0.537   |  0:12:47s
epoch 57 | loss: 0.2938  | val_0_rmse: 0.53795 | val_1_rmse: 0.53648 |  0:13:00s
epoch 58 | loss: 0.29311 | val_0_rmse: 0.53524 | val_1_rmse: 0.53468 |  0:13:14s
epoch 59 | loss: 0.29105 | val_0_rmse: 0.53574 | val_1_rmse: 0.5349  |  0:13:27s
epoch 60 | loss: 0.33042 | val_0_rmse: 0.57331 | val_1_rmse: 0.57141 |  0:13:41s
epoch 61 | loss: 0.31266 | val_0_rmse: 0.54227 | val_1_rmse: 0.54071 |  0:13:54s
epoch 62 | loss: 0.30426 | val_0_rmse: 0.53923 | val_1_rmse: 0.53606 |  0:14:08s
epoch 63 | loss: 0.30072 | val_0_rmse: 0.54151 | val_1_rmse: 0.53844 |  0:14:21s
epoch 64 | loss: 0.30065 | val_0_rmse: 0.56478 | val_1_rmse: 0.56369 |  0:14:35s
epoch 65 | loss: 0.29831 | val_0_rmse: 0.55039 | val_1_rmse: 0.54819 |  0:14:48s
epoch 66 | loss: 0.29678 | val_0_rmse: 0.55507 | val_1_rmse: 0.55146 |  0:15:02s
epoch 67 | loss: 0.29759 | val_0_rmse: 0.53817 | val_1_rmse: 0.53542 |  0:15:15s
epoch 68 | loss: 0.29623 | val_0_rmse: 0.53613 | val_1_rmse: 0.53292 |  0:15:28s
epoch 69 | loss: 0.29583 | val_0_rmse: 0.54502 | val_1_rmse: 0.543   |  0:15:42s
epoch 70 | loss: 0.2951  | val_0_rmse: 0.53601 | val_1_rmse: 0.53486 |  0:15:55s
epoch 71 | loss: 0.29403 | val_0_rmse: 0.53814 | val_1_rmse: 0.53718 |  0:16:09s
epoch 72 | loss: 0.29588 | val_0_rmse: 0.56215 | val_1_rmse: 0.56109 |  0:16:22s
epoch 73 | loss: 0.29929 | val_0_rmse: 0.59155 | val_1_rmse: 0.58973 |  0:16:36s
epoch 74 | loss: 0.30037 | val_0_rmse: 0.53618 | val_1_rmse: 0.53431 |  0:16:49s
epoch 75 | loss: 0.29574 | val_0_rmse: 0.54231 | val_1_rmse: 0.54047 |  0:17:03s
epoch 76 | loss: 0.29484 | val_0_rmse: 0.53664 | val_1_rmse: 0.53596 |  0:17:16s
epoch 77 | loss: 0.29471 | val_0_rmse: 0.53366 | val_1_rmse: 0.53148 |  0:17:29s
epoch 78 | loss: 0.29394 | val_0_rmse: 0.53297 | val_1_rmse: 0.53178 |  0:17:43s
epoch 79 | loss: 0.29384 | val_0_rmse: 0.53709 | val_1_rmse: 0.53645 |  0:17:56s
epoch 80 | loss: 0.29663 | val_0_rmse: 0.54261 | val_1_rmse: 0.54051 |  0:18:10s
epoch 81 | loss: 0.29256 | val_0_rmse: 0.53945 | val_1_rmse: 0.53772 |  0:18:23s
epoch 82 | loss: 0.29266 | val_0_rmse: 0.54142 | val_1_rmse: 0.5409  |  0:18:37s
epoch 83 | loss: 0.29201 | val_0_rmse: 0.53605 | val_1_rmse: 0.53489 |  0:18:50s
epoch 84 | loss: 0.29198 | val_0_rmse: 0.53316 | val_1_rmse: 0.53129 |  0:19:04s
epoch 85 | loss: 0.29116 | val_0_rmse: 0.53743 | val_1_rmse: 0.53725 |  0:19:17s
epoch 86 | loss: 0.29071 | val_0_rmse: 0.53684 | val_1_rmse: 0.53542 |  0:19:30s
epoch 87 | loss: 0.29067 | val_0_rmse: 0.5578  | val_1_rmse: 0.55814 |  0:19:44s
epoch 88 | loss: 0.28816 | val_0_rmse: 0.53203 | val_1_rmse: 0.53049 |  0:19:57s
epoch 89 | loss: 0.28899 | val_0_rmse: 0.53128 | val_1_rmse: 0.53089 |  0:20:11s
epoch 90 | loss: 0.28909 | val_0_rmse: 0.54542 | val_1_rmse: 0.54293 |  0:20:24s
epoch 91 | loss: 0.29232 | val_0_rmse: 0.53428 | val_1_rmse: 0.53305 |  0:20:38s
epoch 92 | loss: 0.29339 | val_0_rmse: 0.53952 | val_1_rmse: 0.53976 |  0:20:51s
epoch 93 | loss: 0.29353 | val_0_rmse: 0.53372 | val_1_rmse: 0.53323 |  0:21:05s
epoch 94 | loss: 0.29189 | val_0_rmse: 0.53891 | val_1_rmse: 0.53826 |  0:21:18s
epoch 95 | loss: 0.29203 | val_0_rmse: 0.56956 | val_1_rmse: 0.57009 |  0:21:32s
epoch 96 | loss: 0.29132 | val_0_rmse: 0.53661 | val_1_rmse: 0.53527 |  0:21:45s
epoch 97 | loss: 0.29119 | val_0_rmse: 0.53558 | val_1_rmse: 0.53464 |  0:21:59s
epoch 98 | loss: 0.28996 | val_0_rmse: 0.5305  | val_1_rmse: 0.53006 |  0:22:12s
epoch 99 | loss: 0.29235 | val_0_rmse: 0.53655 | val_1_rmse: 0.53608 |  0:22:26s
epoch 100| loss: 0.29    | val_0_rmse: 0.53581 | val_1_rmse: 0.53448 |  0:22:39s
epoch 101| loss: 0.28986 | val_0_rmse: 0.53381 | val_1_rmse: 0.53183 |  0:22:53s
epoch 102| loss: 0.28949 | val_0_rmse: 0.53329 | val_1_rmse: 0.53266 |  0:23:06s
epoch 103| loss: 0.28931 | val_0_rmse: 0.5331  | val_1_rmse: 0.53192 |  0:23:20s
epoch 104| loss: 0.28864 | val_0_rmse: 0.5333  | val_1_rmse: 0.53224 |  0:23:33s
epoch 105| loss: 0.28815 | val_0_rmse: 0.53608 | val_1_rmse: 0.53518 |  0:23:47s
epoch 106| loss: 0.28785 | val_0_rmse: 0.55112 | val_1_rmse: 0.55108 |  0:24:00s
epoch 107| loss: 0.30737 | val_0_rmse: 0.53626 | val_1_rmse: 0.53363 |  0:24:14s
epoch 108| loss: 0.29314 | val_0_rmse: 0.53227 | val_1_rmse: 0.53164 |  0:24:27s
epoch 109| loss: 0.2922  | val_0_rmse: 0.53388 | val_1_rmse: 0.53369 |  0:24:41s
epoch 110| loss: 0.29104 | val_0_rmse: 0.53148 | val_1_rmse: 0.53121 |  0:24:54s
epoch 111| loss: 0.28976 | val_0_rmse: 0.53714 | val_1_rmse: 0.53645 |  0:25:08s
epoch 112| loss: 0.28985 | val_0_rmse: 0.5346  | val_1_rmse: 0.53344 |  0:25:21s
epoch 113| loss: 0.28911 | val_0_rmse: 0.53439 | val_1_rmse: 0.53402 |  0:25:34s
epoch 114| loss: 0.28825 | val_0_rmse: 0.54723 | val_1_rmse: 0.54852 |  0:25:48s
epoch 115| loss: 0.28869 | val_0_rmse: 0.53689 | val_1_rmse: 0.53736 |  0:26:01s
epoch 116| loss: 0.28823 | val_0_rmse: 0.53391 | val_1_rmse: 0.53316 |  0:26:15s
epoch 117| loss: 0.28876 | val_0_rmse: 0.53634 | val_1_rmse: 0.5374  |  0:26:28s
epoch 118| loss: 0.28699 | val_0_rmse: 0.54186 | val_1_rmse: 0.54205 |  0:26:42s
epoch 119| loss: 0.28984 | val_0_rmse: 0.54956 | val_1_rmse: 0.55005 |  0:26:55s
epoch 120| loss: 0.28808 | val_0_rmse: 0.57339 | val_1_rmse: 0.57292 |  0:27:09s
epoch 121| loss: 0.28972 | val_0_rmse: 0.53061 | val_1_rmse: 0.53141 |  0:27:22s
epoch 122| loss: 0.28787 | val_0_rmse: 0.533   | val_1_rmse: 0.53274 |  0:27:36s
epoch 123| loss: 0.28784 | val_0_rmse: 0.54003 | val_1_rmse: 0.54089 |  0:27:49s
epoch 124| loss: 0.28857 | val_0_rmse: 0.5371  | val_1_rmse: 0.53802 |  0:28:03s
epoch 125| loss: 0.28815 | val_0_rmse: 0.58265 | val_1_rmse: 0.58351 |  0:28:16s
epoch 126| loss: 0.28684 | val_0_rmse: 0.53368 | val_1_rmse: 0.535   |  0:28:30s
epoch 127| loss: 0.28699 | val_0_rmse: 0.5435  | val_1_rmse: 0.54504 |  0:28:43s
epoch 128| loss: 0.28705 | val_0_rmse: 0.53413 | val_1_rmse: 0.53531 |  0:28:57s

Early stopping occured at epoch 128 with best_epoch = 98 and best_val_1_rmse = 0.53006
Best weights from best epoch are automatically used!
ended training at: 13:06:37
Feature importance:
[('Area', 0.2684818033263443), ('Baths', 0.002819581414616224), ('Beds', 0.014252797416226459), ('Latitude', 0.3573962276061272), ('Longitude', 0.2600346484704548), ('Month', 0.0), ('Year', 0.097014941766231)]
Mean squared error is of 11051466971.406631
Mean absolute error:67244.8967084655
MAPE:0.47700080225436836
R2 score:0.7170122333714064
------------------------------------------------------------------
