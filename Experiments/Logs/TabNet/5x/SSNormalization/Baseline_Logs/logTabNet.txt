TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:59:28
epoch 0  | loss: 0.77783 | val_0_rmse: 0.80101 | val_1_rmse: 0.77871 |  0:00:05s
epoch 1  | loss: 0.48924 | val_0_rmse: 0.65623 | val_1_rmse: 0.64985 |  0:00:06s
epoch 2  | loss: 0.43088 | val_0_rmse: 0.6186  | val_1_rmse: 0.61557 |  0:00:08s
epoch 3  | loss: 0.40707 | val_0_rmse: 0.61125 | val_1_rmse: 0.60069 |  0:00:10s
epoch 4  | loss: 0.38973 | val_0_rmse: 0.59613 | val_1_rmse: 0.5909  |  0:00:11s
epoch 5  | loss: 0.39038 | val_0_rmse: 0.59036 | val_1_rmse: 0.58112 |  0:00:13s
epoch 6  | loss: 0.37363 | val_0_rmse: 0.58942 | val_1_rmse: 0.58342 |  0:00:14s
epoch 7  | loss: 0.36986 | val_0_rmse: 0.57969 | val_1_rmse: 0.56936 |  0:00:16s
epoch 8  | loss: 0.36936 | val_0_rmse: 0.58125 | val_1_rmse: 0.57468 |  0:00:18s
epoch 9  | loss: 0.36025 | val_0_rmse: 0.59529 | val_1_rmse: 0.58579 |  0:00:19s
epoch 10 | loss: 0.36521 | val_0_rmse: 0.58889 | val_1_rmse: 0.5798  |  0:00:21s
epoch 11 | loss: 0.36429 | val_0_rmse: 0.59477 | val_1_rmse: 0.58225 |  0:00:22s
epoch 12 | loss: 0.36765 | val_0_rmse: 0.58427 | val_1_rmse: 0.57753 |  0:00:24s
epoch 13 | loss: 0.36501 | val_0_rmse: 0.59078 | val_1_rmse: 0.58313 |  0:00:25s
epoch 14 | loss: 0.36343 | val_0_rmse: 0.58497 | val_1_rmse: 0.57621 |  0:00:27s
epoch 15 | loss: 0.35546 | val_0_rmse: 0.56447 | val_1_rmse: 0.55723 |  0:00:29s
epoch 16 | loss: 0.3589  | val_0_rmse: 0.61085 | val_1_rmse: 0.60063 |  0:00:30s
epoch 17 | loss: 0.38388 | val_0_rmse: 0.60979 | val_1_rmse: 0.59744 |  0:00:32s
epoch 18 | loss: 0.36104 | val_0_rmse: 0.5741  | val_1_rmse: 0.56635 |  0:00:33s
epoch 19 | loss: 0.35517 | val_0_rmse: 0.56223 | val_1_rmse: 0.55666 |  0:00:35s
epoch 20 | loss: 0.35191 | val_0_rmse: 0.56075 | val_1_rmse: 0.55598 |  0:00:36s
epoch 21 | loss: 0.33918 | val_0_rmse: 0.5494  | val_1_rmse: 0.54604 |  0:00:38s
epoch 22 | loss: 0.34576 | val_0_rmse: 0.56797 | val_1_rmse: 0.56016 |  0:00:40s
epoch 23 | loss: 0.35103 | val_0_rmse: 0.56523 | val_1_rmse: 0.56192 |  0:00:41s
epoch 24 | loss: 0.34443 | val_0_rmse: 0.56866 | val_1_rmse: 0.55987 |  0:00:43s
epoch 25 | loss: 0.3442  | val_0_rmse: 0.56212 | val_1_rmse: 0.55664 |  0:00:44s
epoch 26 | loss: 0.33919 | val_0_rmse: 0.58741 | val_1_rmse: 0.58096 |  0:00:46s
epoch 27 | loss: 0.34635 | val_0_rmse: 0.55936 | val_1_rmse: 0.55751 |  0:00:48s
epoch 28 | loss: 0.34603 | val_0_rmse: 0.57194 | val_1_rmse: 0.56703 |  0:00:49s
epoch 29 | loss: 0.34606 | val_0_rmse: 0.55512 | val_1_rmse: 0.54856 |  0:00:51s
epoch 30 | loss: 0.33575 | val_0_rmse: 0.56047 | val_1_rmse: 0.55582 |  0:00:52s
epoch 31 | loss: 0.33505 | val_0_rmse: 0.5445  | val_1_rmse: 0.53961 |  0:00:54s
epoch 32 | loss: 0.32725 | val_0_rmse: 0.55965 | val_1_rmse: 0.55372 |  0:00:55s
epoch 33 | loss: 0.32565 | val_0_rmse: 0.54275 | val_1_rmse: 0.53643 |  0:00:57s
epoch 34 | loss: 0.31969 | val_0_rmse: 0.54584 | val_1_rmse: 0.5396  |  0:00:59s
epoch 35 | loss: 0.32457 | val_0_rmse: 0.55822 | val_1_rmse: 0.55046 |  0:01:00s
epoch 36 | loss: 0.32177 | val_0_rmse: 0.54753 | val_1_rmse: 0.53952 |  0:01:02s
epoch 37 | loss: 0.33294 | val_0_rmse: 0.56764 | val_1_rmse: 0.56166 |  0:01:03s
epoch 38 | loss: 0.34048 | val_0_rmse: 0.54665 | val_1_rmse: 0.54206 |  0:01:05s
epoch 39 | loss: 0.31746 | val_0_rmse: 0.53433 | val_1_rmse: 0.52873 |  0:01:07s
epoch 40 | loss: 0.31179 | val_0_rmse: 0.53863 | val_1_rmse: 0.53113 |  0:01:08s
epoch 41 | loss: 0.32028 | val_0_rmse: 0.54405 | val_1_rmse: 0.54151 |  0:01:10s
epoch 42 | loss: 0.32326 | val_0_rmse: 0.54513 | val_1_rmse: 0.54206 |  0:01:11s
epoch 43 | loss: 0.31134 | val_0_rmse: 0.52579 | val_1_rmse: 0.52282 |  0:01:13s
epoch 44 | loss: 0.31144 | val_0_rmse: 0.54089 | val_1_rmse: 0.53839 |  0:01:14s
epoch 45 | loss: 0.3104  | val_0_rmse: 0.5355  | val_1_rmse: 0.53059 |  0:01:16s
epoch 46 | loss: 0.31316 | val_0_rmse: 0.52925 | val_1_rmse: 0.52446 |  0:01:18s
epoch 47 | loss: 0.3203  | val_0_rmse: 0.54511 | val_1_rmse: 0.54076 |  0:01:19s
epoch 48 | loss: 0.3182  | val_0_rmse: 0.53084 | val_1_rmse: 0.52824 |  0:01:21s
epoch 49 | loss: 0.31079 | val_0_rmse: 0.53532 | val_1_rmse: 0.53611 |  0:01:22s
epoch 50 | loss: 0.31192 | val_0_rmse: 0.53592 | val_1_rmse: 0.53376 |  0:01:24s
epoch 51 | loss: 0.30929 | val_0_rmse: 0.55145 | val_1_rmse: 0.54719 |  0:01:26s
epoch 52 | loss: 0.31333 | val_0_rmse: 0.52824 | val_1_rmse: 0.52542 |  0:01:27s
epoch 53 | loss: 0.30619 | val_0_rmse: 0.53478 | val_1_rmse: 0.53317 |  0:01:29s
epoch 54 | loss: 0.30946 | val_0_rmse: 0.55827 | val_1_rmse: 0.55462 |  0:01:30s
epoch 55 | loss: 0.31492 | val_0_rmse: 0.52886 | val_1_rmse: 0.52552 |  0:01:32s
epoch 56 | loss: 0.30682 | val_0_rmse: 0.5275  | val_1_rmse: 0.52478 |  0:01:33s
epoch 57 | loss: 0.30439 | val_0_rmse: 0.52612 | val_1_rmse: 0.52322 |  0:01:35s
epoch 58 | loss: 0.30992 | val_0_rmse: 0.52461 | val_1_rmse: 0.51967 |  0:01:37s
epoch 59 | loss: 0.3034  | val_0_rmse: 0.51886 | val_1_rmse: 0.51815 |  0:01:38s
epoch 60 | loss: 0.30464 | val_0_rmse: 0.51317 | val_1_rmse: 0.51242 |  0:01:40s
epoch 61 | loss: 0.30148 | val_0_rmse: 0.53172 | val_1_rmse: 0.53009 |  0:01:41s
epoch 62 | loss: 0.30785 | val_0_rmse: 0.52094 | val_1_rmse: 0.51868 |  0:01:43s
epoch 63 | loss: 0.30406 | val_0_rmse: 0.52039 | val_1_rmse: 0.51939 |  0:01:44s
epoch 64 | loss: 0.30933 | val_0_rmse: 0.54211 | val_1_rmse: 0.53762 |  0:01:46s
epoch 65 | loss: 0.31419 | val_0_rmse: 0.5553  | val_1_rmse: 0.55438 |  0:01:48s
epoch 66 | loss: 0.30191 | val_0_rmse: 0.52604 | val_1_rmse: 0.52549 |  0:01:49s
epoch 67 | loss: 0.29747 | val_0_rmse: 0.52373 | val_1_rmse: 0.5233  |  0:01:51s
epoch 68 | loss: 0.30982 | val_0_rmse: 0.54395 | val_1_rmse: 0.54105 |  0:01:52s
epoch 69 | loss: 0.29401 | val_0_rmse: 0.51694 | val_1_rmse: 0.52032 |  0:01:54s
epoch 70 | loss: 0.29415 | val_0_rmse: 0.54162 | val_1_rmse: 0.54475 |  0:01:55s
epoch 71 | loss: 0.30857 | val_0_rmse: 0.52361 | val_1_rmse: 0.52428 |  0:01:57s
epoch 72 | loss: 0.29562 | val_0_rmse: 0.51339 | val_1_rmse: 0.51476 |  0:01:59s
epoch 73 | loss: 0.29381 | val_0_rmse: 0.51293 | val_1_rmse: 0.51318 |  0:02:00s
epoch 74 | loss: 0.29468 | val_0_rmse: 0.52546 | val_1_rmse: 0.5252  |  0:02:02s
epoch 75 | loss: 0.30089 | val_0_rmse: 0.52828 | val_1_rmse: 0.52902 |  0:02:03s
epoch 76 | loss: 0.29432 | val_0_rmse: 0.52241 | val_1_rmse: 0.52543 |  0:02:05s
epoch 77 | loss: 0.29569 | val_0_rmse: 0.52641 | val_1_rmse: 0.52973 |  0:02:06s
epoch 78 | loss: 0.29828 | val_0_rmse: 0.52037 | val_1_rmse: 0.51838 |  0:02:08s
epoch 79 | loss: 0.30433 | val_0_rmse: 0.52722 | val_1_rmse: 0.52974 |  0:02:10s
epoch 80 | loss: 0.29762 | val_0_rmse: 0.51749 | val_1_rmse: 0.51876 |  0:02:11s
epoch 81 | loss: 0.29935 | val_0_rmse: 0.52938 | val_1_rmse: 0.52841 |  0:02:13s
epoch 82 | loss: 0.29332 | val_0_rmse: 0.52218 | val_1_rmse: 0.52827 |  0:02:14s
epoch 83 | loss: 0.29478 | val_0_rmse: 0.51751 | val_1_rmse: 0.52097 |  0:02:16s
epoch 84 | loss: 0.29093 | val_0_rmse: 0.52655 | val_1_rmse: 0.52887 |  0:02:17s
epoch 85 | loss: 0.29844 | val_0_rmse: 0.53005 | val_1_rmse: 0.53235 |  0:02:19s
epoch 86 | loss: 0.29583 | val_0_rmse: 0.52997 | val_1_rmse: 0.53422 |  0:02:21s
epoch 87 | loss: 0.29992 | val_0_rmse: 0.52835 | val_1_rmse: 0.52948 |  0:02:22s
epoch 88 | loss: 0.29059 | val_0_rmse: 0.50993 | val_1_rmse: 0.5141  |  0:02:24s
epoch 89 | loss: 0.28498 | val_0_rmse: 0.50646 | val_1_rmse: 0.51652 |  0:02:25s
epoch 90 | loss: 0.29506 | val_0_rmse: 0.51939 | val_1_rmse: 0.52239 |  0:02:27s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.51242
Best weights from best epoch are automatically used!
ended training at: 01:01:56
Feature importance:
[('Area', 0.24799974054674065), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.257510250504231), ('Longitude', 0.3207455623340174), ('Month', 0.0754849453188859), ('Year', 0.09825950129612507)]
Mean squared error is of 6112348893.039663
Mean absolute error:53437.09844331404
MAPE:0.17110319605402752
R2 score:0.7254812708982936
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:01:57
epoch 0  | loss: 0.75357 | val_0_rmse: 0.82811 | val_1_rmse: 0.82662 |  0:00:01s
epoch 1  | loss: 0.49563 | val_0_rmse: 0.67193 | val_1_rmse: 0.66322 |  0:00:03s
epoch 2  | loss: 0.47862 | val_0_rmse: 0.71307 | val_1_rmse: 0.70725 |  0:00:04s
epoch 3  | loss: 0.47028 | val_0_rmse: 0.65816 | val_1_rmse: 0.65809 |  0:00:06s
epoch 4  | loss: 0.43833 | val_0_rmse: 0.6546  | val_1_rmse: 0.66215 |  0:00:07s
epoch 5  | loss: 0.42254 | val_0_rmse: 0.62265 | val_1_rmse: 0.62976 |  0:00:09s
epoch 6  | loss: 0.41682 | val_0_rmse: 0.62742 | val_1_rmse: 0.62811 |  0:00:11s
epoch 7  | loss: 0.40438 | val_0_rmse: 0.61665 | val_1_rmse: 0.62634 |  0:00:12s
epoch 8  | loss: 0.40288 | val_0_rmse: 0.61619 | val_1_rmse: 0.62049 |  0:00:14s
epoch 9  | loss: 0.40678 | val_0_rmse: 0.6145  | val_1_rmse: 0.62213 |  0:00:15s
epoch 10 | loss: 0.39758 | val_0_rmse: 0.60096 | val_1_rmse: 0.60414 |  0:00:17s
epoch 11 | loss: 0.395   | val_0_rmse: 0.5944  | val_1_rmse: 0.58771 |  0:00:19s
epoch 12 | loss: 0.36687 | val_0_rmse: 0.58295 | val_1_rmse: 0.58542 |  0:00:20s
epoch 13 | loss: 0.36703 | val_0_rmse: 0.57901 | val_1_rmse: 0.58064 |  0:00:22s
epoch 14 | loss: 0.35459 | val_0_rmse: 0.57954 | val_1_rmse: 0.57671 |  0:00:23s
epoch 15 | loss: 0.36573 | val_0_rmse: 0.62153 | val_1_rmse: 0.61586 |  0:00:25s
epoch 16 | loss: 0.38094 | val_0_rmse: 0.59258 | val_1_rmse: 0.58834 |  0:00:27s
epoch 17 | loss: 0.37634 | val_0_rmse: 0.61571 | val_1_rmse: 0.61735 |  0:00:28s
epoch 18 | loss: 0.37196 | val_0_rmse: 0.62188 | val_1_rmse: 0.62021 |  0:00:30s
epoch 19 | loss: 0.35426 | val_0_rmse: 0.60485 | val_1_rmse: 0.6026  |  0:00:31s
epoch 20 | loss: 0.37904 | val_0_rmse: 0.59339 | val_1_rmse: 0.59208 |  0:00:33s
epoch 21 | loss: 0.36955 | val_0_rmse: 0.58561 | val_1_rmse: 0.58026 |  0:00:34s
epoch 22 | loss: 0.35698 | val_0_rmse: 0.57469 | val_1_rmse: 0.57161 |  0:00:36s
epoch 23 | loss: 0.35237 | val_0_rmse: 0.57296 | val_1_rmse: 0.565   |  0:00:38s
epoch 24 | loss: 0.34649 | val_0_rmse: 0.564   | val_1_rmse: 0.56162 |  0:00:39s
epoch 25 | loss: 0.34424 | val_0_rmse: 0.55808 | val_1_rmse: 0.5551  |  0:00:41s
epoch 26 | loss: 0.34621 | val_0_rmse: 0.5626  | val_1_rmse: 0.55824 |  0:00:42s
epoch 27 | loss: 0.3407  | val_0_rmse: 0.5586  | val_1_rmse: 0.55453 |  0:00:44s
epoch 28 | loss: 0.33916 | val_0_rmse: 0.56566 | val_1_rmse: 0.55858 |  0:00:46s
epoch 29 | loss: 0.33681 | val_0_rmse: 0.59954 | val_1_rmse: 0.59521 |  0:00:47s
epoch 30 | loss: 0.3421  | val_0_rmse: 0.56849 | val_1_rmse: 0.56262 |  0:00:49s
epoch 31 | loss: 0.33363 | val_0_rmse: 0.55307 | val_1_rmse: 0.54819 |  0:00:50s
epoch 32 | loss: 0.32422 | val_0_rmse: 0.55262 | val_1_rmse: 0.54626 |  0:00:52s
epoch 33 | loss: 0.33772 | val_0_rmse: 0.55075 | val_1_rmse: 0.5489  |  0:00:54s
epoch 34 | loss: 0.33597 | val_0_rmse: 0.56474 | val_1_rmse: 0.55715 |  0:00:55s
epoch 35 | loss: 0.32038 | val_0_rmse: 0.56087 | val_1_rmse: 0.55641 |  0:00:57s
epoch 36 | loss: 0.32496 | val_0_rmse: 0.55472 | val_1_rmse: 0.55398 |  0:00:58s
epoch 37 | loss: 0.33006 | val_0_rmse: 0.54649 | val_1_rmse: 0.53762 |  0:01:00s
epoch 38 | loss: 0.32756 | val_0_rmse: 0.56449 | val_1_rmse: 0.56308 |  0:01:02s
epoch 39 | loss: 0.32598 | val_0_rmse: 0.5563  | val_1_rmse: 0.55578 |  0:01:03s
epoch 40 | loss: 0.3248  | val_0_rmse: 0.57552 | val_1_rmse: 0.57038 |  0:01:05s
epoch 41 | loss: 0.32269 | val_0_rmse: 0.55132 | val_1_rmse: 0.54444 |  0:01:06s
epoch 42 | loss: 0.3315  | val_0_rmse: 0.56537 | val_1_rmse: 0.55548 |  0:01:08s
epoch 43 | loss: 0.32766 | val_0_rmse: 0.55803 | val_1_rmse: 0.5517  |  0:01:09s
epoch 44 | loss: 0.32517 | val_0_rmse: 0.55895 | val_1_rmse: 0.55638 |  0:01:11s
epoch 45 | loss: 0.32598 | val_0_rmse: 0.54681 | val_1_rmse: 0.54637 |  0:01:13s
epoch 46 | loss: 0.32383 | val_0_rmse: 0.57946 | val_1_rmse: 0.57177 |  0:01:14s
epoch 47 | loss: 0.32726 | val_0_rmse: 0.54329 | val_1_rmse: 0.5384  |  0:01:16s
epoch 48 | loss: 0.31228 | val_0_rmse: 0.53746 | val_1_rmse: 0.5401  |  0:01:17s
epoch 49 | loss: 0.31502 | val_0_rmse: 0.53309 | val_1_rmse: 0.5345  |  0:01:19s
epoch 50 | loss: 0.31419 | val_0_rmse: 0.54609 | val_1_rmse: 0.54361 |  0:01:21s
epoch 51 | loss: 0.31706 | val_0_rmse: 0.55842 | val_1_rmse: 0.55557 |  0:01:22s
epoch 52 | loss: 0.32676 | val_0_rmse: 0.58545 | val_1_rmse: 0.58431 |  0:01:24s
epoch 53 | loss: 0.3335  | val_0_rmse: 0.55542 | val_1_rmse: 0.54608 |  0:01:25s
epoch 54 | loss: 0.32932 | val_0_rmse: 0.55212 | val_1_rmse: 0.55178 |  0:01:27s
epoch 55 | loss: 0.32039 | val_0_rmse: 0.55086 | val_1_rmse: 0.54653 |  0:01:28s
epoch 56 | loss: 0.33205 | val_0_rmse: 0.55656 | val_1_rmse: 0.55589 |  0:01:30s
epoch 57 | loss: 0.33626 | val_0_rmse: 0.55686 | val_1_rmse: 0.55244 |  0:01:32s
epoch 58 | loss: 0.33419 | val_0_rmse: 0.54064 | val_1_rmse: 0.54087 |  0:01:33s
epoch 59 | loss: 0.32308 | val_0_rmse: 0.54739 | val_1_rmse: 0.54556 |  0:01:35s
epoch 60 | loss: 0.3238  | val_0_rmse: 0.5701  | val_1_rmse: 0.56524 |  0:01:36s
epoch 61 | loss: 0.32844 | val_0_rmse: 0.54976 | val_1_rmse: 0.54505 |  0:01:38s
epoch 62 | loss: 0.32358 | val_0_rmse: 0.55213 | val_1_rmse: 0.55516 |  0:01:40s
epoch 63 | loss: 0.33061 | val_0_rmse: 0.63572 | val_1_rmse: 0.62804 |  0:01:41s
epoch 64 | loss: 0.33651 | val_0_rmse: 0.5459  | val_1_rmse: 0.54648 |  0:01:43s
epoch 65 | loss: 0.32359 | val_0_rmse: 0.56968 | val_1_rmse: 0.5691  |  0:01:44s
epoch 66 | loss: 0.32298 | val_0_rmse: 0.59945 | val_1_rmse: 0.60019 |  0:01:46s
epoch 67 | loss: 0.33933 | val_0_rmse: 0.56606 | val_1_rmse: 0.55905 |  0:01:48s
epoch 68 | loss: 0.31987 | val_0_rmse: 0.56655 | val_1_rmse: 0.56507 |  0:01:49s
epoch 69 | loss: 0.32914 | val_0_rmse: 0.53878 | val_1_rmse: 0.53655 |  0:01:51s
epoch 70 | loss: 0.32981 | val_0_rmse: 0.54026 | val_1_rmse: 0.54005 |  0:01:52s
epoch 71 | loss: 0.32253 | val_0_rmse: 0.58437 | val_1_rmse: 0.57972 |  0:01:54s
epoch 72 | loss: 0.31504 | val_0_rmse: 0.5416  | val_1_rmse: 0.53503 |  0:01:55s
epoch 73 | loss: 0.30702 | val_0_rmse: 0.57448 | val_1_rmse: 0.57515 |  0:01:57s
epoch 74 | loss: 0.31921 | val_0_rmse: 0.5524  | val_1_rmse: 0.55253 |  0:01:59s
epoch 75 | loss: 0.32212 | val_0_rmse: 0.54557 | val_1_rmse: 0.54902 |  0:02:00s
epoch 76 | loss: 0.31764 | val_0_rmse: 0.57761 | val_1_rmse: 0.58531 |  0:02:02s
epoch 77 | loss: 0.32684 | val_0_rmse: 0.56574 | val_1_rmse: 0.56384 |  0:02:03s
epoch 78 | loss: 0.31474 | val_0_rmse: 0.53638 | val_1_rmse: 0.537   |  0:02:05s
epoch 79 | loss: 0.32126 | val_0_rmse: 0.53399 | val_1_rmse: 0.53278 |  0:02:07s
epoch 80 | loss: 0.31543 | val_0_rmse: 0.5726  | val_1_rmse: 0.57011 |  0:02:08s
epoch 81 | loss: 0.31401 | val_0_rmse: 0.60525 | val_1_rmse: 0.59821 |  0:02:10s
epoch 82 | loss: 0.30427 | val_0_rmse: 0.54228 | val_1_rmse: 0.53879 |  0:02:11s
epoch 83 | loss: 0.30797 | val_0_rmse: 0.53527 | val_1_rmse: 0.53853 |  0:02:13s
epoch 84 | loss: 0.30682 | val_0_rmse: 0.58312 | val_1_rmse: 0.58036 |  0:02:15s
epoch 85 | loss: 0.30974 | val_0_rmse: 0.55588 | val_1_rmse: 0.55229 |  0:02:16s
epoch 86 | loss: 0.31566 | val_0_rmse: 0.54605 | val_1_rmse: 0.54553 |  0:02:18s
epoch 87 | loss: 0.31296 | val_0_rmse: 0.54741 | val_1_rmse: 0.55222 |  0:02:19s
epoch 88 | loss: 0.30591 | val_0_rmse: 0.5361  | val_1_rmse: 0.53118 |  0:02:21s
epoch 89 | loss: 0.30664 | val_0_rmse: 0.52872 | val_1_rmse: 0.53166 |  0:02:22s
epoch 90 | loss: 0.30229 | val_0_rmse: 0.57589 | val_1_rmse: 0.58505 |  0:02:24s
epoch 91 | loss: 0.306   | val_0_rmse: 0.53564 | val_1_rmse: 0.53867 |  0:02:26s
epoch 92 | loss: 0.30611 | val_0_rmse: 0.53158 | val_1_rmse: 0.53115 |  0:02:27s
epoch 93 | loss: 0.29963 | val_0_rmse: 0.52748 | val_1_rmse: 0.5313  |  0:02:29s
epoch 94 | loss: 0.3036  | val_0_rmse: 0.55097 | val_1_rmse: 0.55228 |  0:02:30s
epoch 95 | loss: 0.29875 | val_0_rmse: 0.52274 | val_1_rmse: 0.52554 |  0:02:32s
epoch 96 | loss: 0.30275 | val_0_rmse: 0.54482 | val_1_rmse: 0.54578 |  0:02:34s
epoch 97 | loss: 0.31065 | val_0_rmse: 0.55485 | val_1_rmse: 0.55948 |  0:02:35s
epoch 98 | loss: 0.31122 | val_0_rmse: 0.55467 | val_1_rmse: 0.56363 |  0:02:37s
epoch 99 | loss: 0.29615 | val_0_rmse: 0.52267 | val_1_rmse: 0.52285 |  0:02:38s
epoch 100| loss: 0.30085 | val_0_rmse: 0.54027 | val_1_rmse: 0.53849 |  0:02:40s
epoch 101| loss: 0.29838 | val_0_rmse: 0.56635 | val_1_rmse: 0.55839 |  0:02:42s
epoch 102| loss: 0.30337 | val_0_rmse: 0.59382 | val_1_rmse: 0.59178 |  0:02:43s
epoch 103| loss: 0.30525 | val_0_rmse: 0.52905 | val_1_rmse: 0.53586 |  0:02:45s
epoch 104| loss: 0.29976 | val_0_rmse: 0.54257 | val_1_rmse: 0.54119 |  0:02:46s
epoch 105| loss: 0.29695 | val_0_rmse: 0.54708 | val_1_rmse: 0.54504 |  0:02:48s
epoch 106| loss: 0.30237 | val_0_rmse: 0.53988 | val_1_rmse: 0.53543 |  0:02:49s
epoch 107| loss: 0.30169 | val_0_rmse: 0.54135 | val_1_rmse: 0.54456 |  0:02:51s
epoch 108| loss: 0.30403 | val_0_rmse: 0.56833 | val_1_rmse: 0.56758 |  0:02:53s
epoch 109| loss: 0.30063 | val_0_rmse: 0.54423 | val_1_rmse: 0.54428 |  0:02:54s
epoch 110| loss: 0.29307 | val_0_rmse: 0.56445 | val_1_rmse: 0.56439 |  0:02:56s
epoch 111| loss: 0.29739 | val_0_rmse: 0.52569 | val_1_rmse: 0.52719 |  0:02:57s
epoch 112| loss: 0.32131 | val_0_rmse: 0.54616 | val_1_rmse: 0.54541 |  0:02:59s
epoch 113| loss: 0.30873 | val_0_rmse: 0.5527  | val_1_rmse: 0.55815 |  0:03:01s
epoch 114| loss: 0.30281 | val_0_rmse: 0.53817 | val_1_rmse: 0.54078 |  0:03:02s
epoch 115| loss: 0.29473 | val_0_rmse: 0.52852 | val_1_rmse: 0.5242  |  0:03:04s
epoch 116| loss: 0.29652 | val_0_rmse: 0.60164 | val_1_rmse: 0.60227 |  0:03:05s
epoch 117| loss: 0.29642 | val_0_rmse: 0.54269 | val_1_rmse: 0.54857 |  0:03:07s
epoch 118| loss: 0.29596 | val_0_rmse: 0.51895 | val_1_rmse: 0.52055 |  0:03:09s
epoch 119| loss: 0.29224 | val_0_rmse: 0.51663 | val_1_rmse: 0.52294 |  0:03:10s
epoch 120| loss: 0.29673 | val_0_rmse: 0.55283 | val_1_rmse: 0.55293 |  0:03:12s
epoch 121| loss: 0.29921 | val_0_rmse: 0.52595 | val_1_rmse: 0.52826 |  0:03:13s
epoch 122| loss: 0.29458 | val_0_rmse: 0.55171 | val_1_rmse: 0.55654 |  0:03:15s
epoch 123| loss: 0.30231 | val_0_rmse: 0.52852 | val_1_rmse: 0.5277  |  0:03:16s
epoch 124| loss: 0.30373 | val_0_rmse: 0.55276 | val_1_rmse: 0.55397 |  0:03:18s
epoch 125| loss: 0.30902 | val_0_rmse: 0.55395 | val_1_rmse: 0.55956 |  0:03:20s
epoch 126| loss: 0.30688 | val_0_rmse: 0.52978 | val_1_rmse: 0.52662 |  0:03:21s
epoch 127| loss: 0.30271 | val_0_rmse: 0.57557 | val_1_rmse: 0.57659 |  0:03:23s
epoch 128| loss: 0.30954 | val_0_rmse: 0.54006 | val_1_rmse: 0.54234 |  0:03:24s
epoch 129| loss: 0.30519 | val_0_rmse: 0.52556 | val_1_rmse: 0.52896 |  0:03:26s
epoch 130| loss: 0.30412 | val_0_rmse: 0.53838 | val_1_rmse: 0.5434  |  0:03:28s
epoch 131| loss: 0.30033 | val_0_rmse: 0.56136 | val_1_rmse: 0.56211 |  0:03:29s
epoch 132| loss: 0.29598 | val_0_rmse: 0.52968 | val_1_rmse: 0.5385  |  0:03:31s
epoch 133| loss: 0.29787 | val_0_rmse: 0.61292 | val_1_rmse: 0.60628 |  0:03:32s
epoch 134| loss: 0.29958 | val_0_rmse: 0.52403 | val_1_rmse: 0.53459 |  0:03:34s
epoch 135| loss: 0.31062 | val_0_rmse: 0.53027 | val_1_rmse: 0.53098 |  0:03:36s
epoch 136| loss: 0.30569 | val_0_rmse: 0.67509 | val_1_rmse: 0.67867 |  0:03:37s
epoch 137| loss: 0.29903 | val_0_rmse: 0.53691 | val_1_rmse: 0.54451 |  0:03:39s
epoch 138| loss: 0.29487 | val_0_rmse: 0.56605 | val_1_rmse: 0.57461 |  0:03:40s
epoch 139| loss: 0.29376 | val_0_rmse: 0.52563 | val_1_rmse: 0.53028 |  0:03:42s
epoch 140| loss: 0.2917  | val_0_rmse: 0.55174 | val_1_rmse: 0.55728 |  0:03:43s
epoch 141| loss: 0.29421 | val_0_rmse: 0.5318  | val_1_rmse: 0.54227 |  0:03:45s
epoch 142| loss: 0.30111 | val_0_rmse: 0.54697 | val_1_rmse: 0.54955 |  0:03:47s
epoch 143| loss: 0.30106 | val_0_rmse: 0.52846 | val_1_rmse: 0.532   |  0:03:48s
epoch 144| loss: 0.30046 | val_0_rmse: 0.5174  | val_1_rmse: 0.5183  |  0:03:50s
epoch 145| loss: 0.29534 | val_0_rmse: 0.5121  | val_1_rmse: 0.51824 |  0:03:51s
epoch 146| loss: 0.29642 | val_0_rmse: 0.53092 | val_1_rmse: 0.53059 |  0:03:53s
epoch 147| loss: 0.28592 | val_0_rmse: 0.52164 | val_1_rmse: 0.52832 |  0:03:55s
epoch 148| loss: 0.28542 | val_0_rmse: 0.51268 | val_1_rmse: 0.51459 |  0:03:56s
epoch 149| loss: 0.29018 | val_0_rmse: 0.52633 | val_1_rmse: 0.53761 |  0:03:58s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.51459
Best weights from best epoch are automatically used!
ended training at: 01:05:56
Feature importance:
[('Area', 0.3321360785512901), ('Baths', 4.117258883323871e-05), ('Beds', 0.0), ('Latitude', 0.2409479922437164), ('Longitude', 0.2363412343743576), ('Month', 0.1649174121822471), ('Year', 0.025616110059555493)]
Mean squared error is of 6090546201.835522
Mean absolute error:53789.276004526335
MAPE:0.17495859049562035
R2 score:0.7314333608081216
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:05:56
epoch 0  | loss: 0.78005 | val_0_rmse: 0.79405 | val_1_rmse: 0.78583 |  0:00:01s
epoch 1  | loss: 0.52947 | val_0_rmse: 0.72113 | val_1_rmse: 0.71522 |  0:00:03s
epoch 2  | loss: 0.47758 | val_0_rmse: 0.67205 | val_1_rmse: 0.6634  |  0:00:04s
epoch 3  | loss: 0.4392  | val_0_rmse: 0.62846 | val_1_rmse: 0.62278 |  0:00:06s
epoch 4  | loss: 0.40902 | val_0_rmse: 0.62944 | val_1_rmse: 0.62292 |  0:00:08s
epoch 5  | loss: 0.40625 | val_0_rmse: 0.60236 | val_1_rmse: 0.59743 |  0:00:09s
epoch 6  | loss: 0.40771 | val_0_rmse: 0.64411 | val_1_rmse: 0.64223 |  0:00:11s
epoch 7  | loss: 0.41156 | val_0_rmse: 0.61307 | val_1_rmse: 0.61092 |  0:00:12s
epoch 8  | loss: 0.38385 | val_0_rmse: 0.60151 | val_1_rmse: 0.59778 |  0:00:14s
epoch 9  | loss: 0.36538 | val_0_rmse: 0.59278 | val_1_rmse: 0.58751 |  0:00:16s
epoch 10 | loss: 0.36533 | val_0_rmse: 0.58446 | val_1_rmse: 0.57934 |  0:00:17s
epoch 11 | loss: 0.35404 | val_0_rmse: 0.58045 | val_1_rmse: 0.57287 |  0:00:19s
epoch 12 | loss: 0.35814 | val_0_rmse: 0.59174 | val_1_rmse: 0.58257 |  0:00:20s
epoch 13 | loss: 0.36501 | val_0_rmse: 0.57793 | val_1_rmse: 0.57559 |  0:00:22s
epoch 14 | loss: 0.36938 | val_0_rmse: 0.60704 | val_1_rmse: 0.59708 |  0:00:24s
epoch 15 | loss: 0.36426 | val_0_rmse: 0.58041 | val_1_rmse: 0.57044 |  0:00:25s
epoch 16 | loss: 0.35179 | val_0_rmse: 0.5935  | val_1_rmse: 0.58314 |  0:00:27s
epoch 17 | loss: 0.35305 | val_0_rmse: 0.56773 | val_1_rmse: 0.56292 |  0:00:28s
epoch 18 | loss: 0.34053 | val_0_rmse: 0.57203 | val_1_rmse: 0.56737 |  0:00:30s
epoch 19 | loss: 0.3532  | val_0_rmse: 0.57989 | val_1_rmse: 0.57744 |  0:00:32s
epoch 20 | loss: 0.37862 | val_0_rmse: 0.61118 | val_1_rmse: 0.60629 |  0:00:33s
epoch 21 | loss: 0.37625 | val_0_rmse: 0.59718 | val_1_rmse: 0.5929  |  0:00:35s
epoch 22 | loss: 0.37184 | val_0_rmse: 0.60506 | val_1_rmse: 0.60181 |  0:00:36s
epoch 23 | loss: 0.37275 | val_0_rmse: 0.59    | val_1_rmse: 0.59366 |  0:00:38s
epoch 24 | loss: 0.37127 | val_0_rmse: 0.58668 | val_1_rmse: 0.58764 |  0:00:39s
epoch 25 | loss: 0.37059 | val_0_rmse: 0.62093 | val_1_rmse: 0.62214 |  0:00:41s
epoch 26 | loss: 0.35989 | val_0_rmse: 0.57997 | val_1_rmse: 0.58246 |  0:00:43s
epoch 27 | loss: 0.36548 | val_0_rmse: 0.59595 | val_1_rmse: 0.59102 |  0:00:44s
epoch 28 | loss: 0.36363 | val_0_rmse: 0.58243 | val_1_rmse: 0.58427 |  0:00:46s
epoch 29 | loss: 0.36205 | val_0_rmse: 0.59066 | val_1_rmse: 0.59375 |  0:00:47s
epoch 30 | loss: 0.3567  | val_0_rmse: 0.58321 | val_1_rmse: 0.58309 |  0:00:49s
epoch 31 | loss: 0.35324 | val_0_rmse: 0.57791 | val_1_rmse: 0.57922 |  0:00:51s
epoch 32 | loss: 0.35581 | val_0_rmse: 0.5819  | val_1_rmse: 0.59149 |  0:00:52s
epoch 33 | loss: 0.35945 | val_0_rmse: 0.59281 | val_1_rmse: 0.59203 |  0:00:54s
epoch 34 | loss: 0.35675 | val_0_rmse: 0.57142 | val_1_rmse: 0.57558 |  0:00:55s
epoch 35 | loss: 0.35327 | val_0_rmse: 0.56733 | val_1_rmse: 0.57156 |  0:00:57s
epoch 36 | loss: 0.34572 | val_0_rmse: 0.5765  | val_1_rmse: 0.58212 |  0:00:59s
epoch 37 | loss: 0.34734 | val_0_rmse: 0.59647 | val_1_rmse: 0.59897 |  0:01:00s
epoch 38 | loss: 0.34565 | val_0_rmse: 0.63002 | val_1_rmse: 0.62907 |  0:01:02s
epoch 39 | loss: 0.34225 | val_0_rmse: 0.60071 | val_1_rmse: 0.60153 |  0:01:03s
epoch 40 | loss: 0.34672 | val_0_rmse: 0.56729 | val_1_rmse: 0.56583 |  0:01:05s
epoch 41 | loss: 0.3389  | val_0_rmse: 0.59829 | val_1_rmse: 0.59921 |  0:01:06s
epoch 42 | loss: 0.34433 | val_0_rmse: 0.55877 | val_1_rmse: 0.56272 |  0:01:08s
epoch 43 | loss: 0.3324  | val_0_rmse: 0.57011 | val_1_rmse: 0.56835 |  0:01:10s
epoch 44 | loss: 0.33772 | val_0_rmse: 0.55574 | val_1_rmse: 0.56097 |  0:01:11s
epoch 45 | loss: 0.33062 | val_0_rmse: 0.57477 | val_1_rmse: 0.5767  |  0:01:13s
epoch 46 | loss: 0.32737 | val_0_rmse: 0.57234 | val_1_rmse: 0.57125 |  0:01:14s
epoch 47 | loss: 0.32514 | val_0_rmse: 0.55946 | val_1_rmse: 0.56079 |  0:01:16s
epoch 48 | loss: 0.32288 | val_0_rmse: 0.59172 | val_1_rmse: 0.58968 |  0:01:18s
epoch 49 | loss: 0.33673 | val_0_rmse: 0.56499 | val_1_rmse: 0.56741 |  0:01:19s
epoch 50 | loss: 0.33322 | val_0_rmse: 0.5953  | val_1_rmse: 0.59669 |  0:01:21s
epoch 51 | loss: 0.33829 | val_0_rmse: 0.56979 | val_1_rmse: 0.56774 |  0:01:22s
epoch 52 | loss: 0.32523 | val_0_rmse: 0.56594 | val_1_rmse: 0.56618 |  0:01:24s
epoch 53 | loss: 0.32367 | val_0_rmse: 0.54232 | val_1_rmse: 0.54112 |  0:01:26s
epoch 54 | loss: 0.31781 | val_0_rmse: 0.53873 | val_1_rmse: 0.54125 |  0:01:27s
epoch 55 | loss: 0.32333 | val_0_rmse: 0.56968 | val_1_rmse: 0.57201 |  0:01:29s
epoch 56 | loss: 0.32012 | val_0_rmse: 0.54567 | val_1_rmse: 0.54555 |  0:01:30s
epoch 57 | loss: 0.31467 | val_0_rmse: 0.54649 | val_1_rmse: 0.54965 |  0:01:32s
epoch 58 | loss: 0.32834 | val_0_rmse: 0.54389 | val_1_rmse: 0.54337 |  0:01:33s
epoch 59 | loss: 0.31667 | val_0_rmse: 0.54671 | val_1_rmse: 0.55248 |  0:01:35s
epoch 60 | loss: 0.31705 | val_0_rmse: 0.55016 | val_1_rmse: 0.54919 |  0:01:37s
epoch 61 | loss: 0.31874 | val_0_rmse: 0.55158 | val_1_rmse: 0.55766 |  0:01:38s
epoch 62 | loss: 0.31375 | val_0_rmse: 0.53521 | val_1_rmse: 0.53789 |  0:01:40s
epoch 63 | loss: 0.31406 | val_0_rmse: 0.54784 | val_1_rmse: 0.55226 |  0:01:41s
epoch 64 | loss: 0.31892 | val_0_rmse: 0.55329 | val_1_rmse: 0.56049 |  0:01:43s
epoch 65 | loss: 0.31644 | val_0_rmse: 0.53523 | val_1_rmse: 0.53798 |  0:01:45s
epoch 66 | loss: 0.31595 | val_0_rmse: 0.59134 | val_1_rmse: 0.59645 |  0:01:46s
epoch 67 | loss: 0.31445 | val_0_rmse: 0.53707 | val_1_rmse: 0.54613 |  0:01:48s
epoch 68 | loss: 0.30819 | val_0_rmse: 0.54639 | val_1_rmse: 0.55343 |  0:01:49s
epoch 69 | loss: 0.31689 | val_0_rmse: 0.57232 | val_1_rmse: 0.57835 |  0:01:51s
epoch 70 | loss: 0.30993 | val_0_rmse: 0.53962 | val_1_rmse: 0.54196 |  0:01:53s
epoch 71 | loss: 0.30746 | val_0_rmse: 0.60354 | val_1_rmse: 0.59915 |  0:01:54s
epoch 72 | loss: 0.30856 | val_0_rmse: 0.62673 | val_1_rmse: 0.62817 |  0:01:56s
epoch 73 | loss: 0.30431 | val_0_rmse: 0.53199 | val_1_rmse: 0.53563 |  0:01:57s
epoch 74 | loss: 0.31042 | val_0_rmse: 0.54666 | val_1_rmse: 0.55641 |  0:01:59s
epoch 75 | loss: 0.30481 | val_0_rmse: 0.53388 | val_1_rmse: 0.54011 |  0:02:01s
epoch 76 | loss: 0.30593 | val_0_rmse: 0.54572 | val_1_rmse: 0.55567 |  0:02:02s
epoch 77 | loss: 0.30965 | val_0_rmse: 0.57401 | val_1_rmse: 0.57435 |  0:02:04s
epoch 78 | loss: 0.31506 | val_0_rmse: 0.52981 | val_1_rmse: 0.5373  |  0:02:05s
epoch 79 | loss: 0.30545 | val_0_rmse: 0.51947 | val_1_rmse: 0.5259  |  0:02:07s
epoch 80 | loss: 0.30574 | val_0_rmse: 0.60321 | val_1_rmse: 0.60484 |  0:02:08s
epoch 81 | loss: 0.30078 | val_0_rmse: 0.53547 | val_1_rmse: 0.54202 |  0:02:10s
epoch 82 | loss: 0.31394 | val_0_rmse: 0.54368 | val_1_rmse: 0.54961 |  0:02:12s
epoch 83 | loss: 0.30675 | val_0_rmse: 0.55338 | val_1_rmse: 0.55691 |  0:02:13s
epoch 84 | loss: 0.30473 | val_0_rmse: 0.52433 | val_1_rmse: 0.52494 |  0:02:15s
epoch 85 | loss: 0.29378 | val_0_rmse: 0.55018 | val_1_rmse: 0.55469 |  0:02:16s
epoch 86 | loss: 0.30339 | val_0_rmse: 0.52705 | val_1_rmse: 0.53588 |  0:02:18s
epoch 87 | loss: 0.30457 | val_0_rmse: 0.59485 | val_1_rmse: 0.58772 |  0:02:20s
epoch 88 | loss: 0.312   | val_0_rmse: 0.53343 | val_1_rmse: 0.53668 |  0:02:21s
epoch 89 | loss: 0.30232 | val_0_rmse: 0.54568 | val_1_rmse: 0.55267 |  0:02:23s
epoch 90 | loss: 0.30089 | val_0_rmse: 0.53605 | val_1_rmse: 0.54421 |  0:02:24s
epoch 91 | loss: 0.29776 | val_0_rmse: 0.54912 | val_1_rmse: 0.55002 |  0:02:26s
epoch 92 | loss: 0.29714 | val_0_rmse: 0.53162 | val_1_rmse: 0.53715 |  0:02:28s
epoch 93 | loss: 0.29389 | val_0_rmse: 0.54584 | val_1_rmse: 0.55312 |  0:02:29s
epoch 94 | loss: 0.30002 | val_0_rmse: 0.53246 | val_1_rmse: 0.54114 |  0:02:31s
epoch 95 | loss: 0.30647 | val_0_rmse: 0.52582 | val_1_rmse: 0.53435 |  0:02:32s
epoch 96 | loss: 0.29652 | val_0_rmse: 0.54094 | val_1_rmse: 0.54389 |  0:02:34s
epoch 97 | loss: 0.29491 | val_0_rmse: 0.5209  | val_1_rmse: 0.53044 |  0:02:35s
epoch 98 | loss: 0.29449 | val_0_rmse: 0.52634 | val_1_rmse: 0.53585 |  0:02:37s
epoch 99 | loss: 0.29593 | val_0_rmse: 0.51581 | val_1_rmse: 0.52412 |  0:02:39s
epoch 100| loss: 0.29604 | val_0_rmse: 0.54699 | val_1_rmse: 0.55391 |  0:02:40s
epoch 101| loss: 0.29752 | val_0_rmse: 0.52794 | val_1_rmse: 0.53245 |  0:02:42s
epoch 102| loss: 0.30863 | val_0_rmse: 0.60314 | val_1_rmse: 0.60423 |  0:02:43s
epoch 103| loss: 0.30147 | val_0_rmse: 0.53295 | val_1_rmse: 0.53082 |  0:02:45s
epoch 104| loss: 0.29743 | val_0_rmse: 0.58575 | val_1_rmse: 0.58661 |  0:02:47s
epoch 105| loss: 0.30003 | val_0_rmse: 0.56316 | val_1_rmse: 0.56591 |  0:02:48s
epoch 106| loss: 0.29924 | val_0_rmse: 0.53227 | val_1_rmse: 0.53637 |  0:02:50s
epoch 107| loss: 0.31185 | val_0_rmse: 0.58427 | val_1_rmse: 0.58839 |  0:02:51s
epoch 108| loss: 0.30831 | val_0_rmse: 0.57236 | val_1_rmse: 0.5745  |  0:02:53s
epoch 109| loss: 0.294   | val_0_rmse: 0.5389  | val_1_rmse: 0.53958 |  0:02:55s
epoch 110| loss: 0.3001  | val_0_rmse: 0.5345  | val_1_rmse: 0.53813 |  0:02:56s
epoch 111| loss: 0.29924 | val_0_rmse: 0.54271 | val_1_rmse: 0.55052 |  0:02:58s
epoch 112| loss: 0.31494 | val_0_rmse: 0.53037 | val_1_rmse: 0.53636 |  0:02:59s
epoch 113| loss: 0.30153 | val_0_rmse: 0.52441 | val_1_rmse: 0.52684 |  0:03:01s
epoch 114| loss: 0.30122 | val_0_rmse: 0.52603 | val_1_rmse: 0.53132 |  0:03:03s
epoch 115| loss: 0.2934  | val_0_rmse: 0.52124 | val_1_rmse: 0.52786 |  0:03:04s
epoch 116| loss: 0.29336 | val_0_rmse: 0.51462 | val_1_rmse: 0.51994 |  0:03:06s
epoch 117| loss: 0.304   | val_0_rmse: 0.53665 | val_1_rmse: 0.53887 |  0:03:07s
epoch 118| loss: 0.30151 | val_0_rmse: 0.57014 | val_1_rmse: 0.57105 |  0:03:09s
epoch 119| loss: 0.29132 | val_0_rmse: 0.5287  | val_1_rmse: 0.53516 |  0:03:11s
epoch 120| loss: 0.29416 | val_0_rmse: 0.58159 | val_1_rmse: 0.58796 |  0:03:12s
epoch 121| loss: 0.28707 | val_0_rmse: 0.55699 | val_1_rmse: 0.55978 |  0:03:14s
epoch 122| loss: 0.29535 | val_0_rmse: 0.52629 | val_1_rmse: 0.53012 |  0:03:15s
epoch 123| loss: 0.28787 | val_0_rmse: 0.51466 | val_1_rmse: 0.51872 |  0:03:17s
epoch 124| loss: 0.29163 | val_0_rmse: 0.51535 | val_1_rmse: 0.52296 |  0:03:19s
epoch 125| loss: 0.28632 | val_0_rmse: 0.53774 | val_1_rmse: 0.54592 |  0:03:20s
epoch 126| loss: 0.28307 | val_0_rmse: 0.50754 | val_1_rmse: 0.51203 |  0:03:22s
epoch 127| loss: 0.28623 | val_0_rmse: 0.52988 | val_1_rmse: 0.53867 |  0:03:23s
epoch 128| loss: 0.2955  | val_0_rmse: 0.51296 | val_1_rmse: 0.52214 |  0:03:25s
epoch 129| loss: 0.2873  | val_0_rmse: 0.5729  | val_1_rmse: 0.57685 |  0:03:27s
epoch 130| loss: 0.30408 | val_0_rmse: 0.54677 | val_1_rmse: 0.55711 |  0:03:28s
epoch 131| loss: 0.30117 | val_0_rmse: 0.5297  | val_1_rmse: 0.5319  |  0:03:30s
epoch 132| loss: 0.29227 | val_0_rmse: 0.52716 | val_1_rmse: 0.53067 |  0:03:31s
epoch 133| loss: 0.28425 | val_0_rmse: 0.5169  | val_1_rmse: 0.52508 |  0:03:33s
epoch 134| loss: 0.28487 | val_0_rmse: 0.61006 | val_1_rmse: 0.61469 |  0:03:34s
epoch 135| loss: 0.28738 | val_0_rmse: 0.51966 | val_1_rmse: 0.53153 |  0:03:36s
epoch 136| loss: 0.28871 | val_0_rmse: 0.51011 | val_1_rmse: 0.51932 |  0:03:38s
epoch 137| loss: 0.28451 | val_0_rmse: 0.5789  | val_1_rmse: 0.58407 |  0:03:39s
epoch 138| loss: 0.28746 | val_0_rmse: 0.53059 | val_1_rmse: 0.5377  |  0:03:41s
epoch 139| loss: 0.28284 | val_0_rmse: 0.50708 | val_1_rmse: 0.5134  |  0:03:42s
epoch 140| loss: 0.2811  | val_0_rmse: 0.50615 | val_1_rmse: 0.51318 |  0:03:44s
epoch 141| loss: 0.28157 | val_0_rmse: 0.51104 | val_1_rmse: 0.5249  |  0:03:46s
epoch 142| loss: 0.28292 | val_0_rmse: 0.51262 | val_1_rmse: 0.52654 |  0:03:47s
epoch 143| loss: 0.28754 | val_0_rmse: 0.53464 | val_1_rmse: 0.54349 |  0:03:49s
epoch 144| loss: 0.29061 | val_0_rmse: 0.50382 | val_1_rmse: 0.51178 |  0:03:50s
epoch 145| loss: 0.2864  | val_0_rmse: 0.5247  | val_1_rmse: 0.53256 |  0:03:52s
epoch 146| loss: 0.28713 | val_0_rmse: 0.53827 | val_1_rmse: 0.54801 |  0:03:53s
epoch 147| loss: 0.29174 | val_0_rmse: 0.5537  | val_1_rmse: 0.56075 |  0:03:55s
epoch 148| loss: 0.28756 | val_0_rmse: 0.57476 | val_1_rmse: 0.58115 |  0:03:57s
epoch 149| loss: 0.28166 | val_0_rmse: 0.54177 | val_1_rmse: 0.55324 |  0:03:58s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.51178
Best weights from best epoch are automatically used!
ended training at: 01:09:55
Feature importance:
[('Area', 0.3330297372926367), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.34929895200986977), ('Longitude', 0.316152194615105), ('Month', 0.0015191160823885034), ('Year', 0.0)]
Mean squared error is of 5911252190.931521
Mean absolute error:52596.863068444654
MAPE:0.1763680901380348
R2 score:0.7403660174905725
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:09:55
epoch 0  | loss: 0.79313 | val_0_rmse: 0.87351 | val_1_rmse: 0.87576 |  0:00:01s
epoch 1  | loss: 0.57753 | val_0_rmse: 0.70677 | val_1_rmse: 0.69353 |  0:00:03s
epoch 2  | loss: 0.49745 | val_0_rmse: 0.67352 | val_1_rmse: 0.67265 |  0:00:04s
epoch 3  | loss: 0.44482 | val_0_rmse: 0.64683 | val_1_rmse: 0.64051 |  0:00:06s
epoch 4  | loss: 0.41547 | val_0_rmse: 0.61054 | val_1_rmse: 0.60592 |  0:00:07s
epoch 5  | loss: 0.39862 | val_0_rmse: 0.63901 | val_1_rmse: 0.64108 |  0:00:09s
epoch 6  | loss: 0.37051 | val_0_rmse: 0.57968 | val_1_rmse: 0.57521 |  0:00:11s
epoch 7  | loss: 0.36451 | val_0_rmse: 0.61971 | val_1_rmse: 0.62363 |  0:00:12s
epoch 8  | loss: 0.38355 | val_0_rmse: 0.59347 | val_1_rmse: 0.59367 |  0:00:14s
epoch 9  | loss: 0.3563  | val_0_rmse: 0.5808  | val_1_rmse: 0.58149 |  0:00:15s
epoch 10 | loss: 0.34607 | val_0_rmse: 0.57273 | val_1_rmse: 0.57404 |  0:00:17s
epoch 11 | loss: 0.34038 | val_0_rmse: 0.57494 | val_1_rmse: 0.57357 |  0:00:19s
epoch 12 | loss: 0.3445  | val_0_rmse: 0.57229 | val_1_rmse: 0.57196 |  0:00:20s
epoch 13 | loss: 0.34072 | val_0_rmse: 0.55686 | val_1_rmse: 0.55023 |  0:00:22s
epoch 14 | loss: 0.33662 | val_0_rmse: 0.57016 | val_1_rmse: 0.56779 |  0:00:23s
epoch 15 | loss: 0.34189 | val_0_rmse: 0.55145 | val_1_rmse: 0.55181 |  0:00:25s
epoch 16 | loss: 0.337   | val_0_rmse: 0.56844 | val_1_rmse: 0.56661 |  0:00:27s
epoch 17 | loss: 0.33277 | val_0_rmse: 0.5683  | val_1_rmse: 0.56403 |  0:00:28s
epoch 18 | loss: 0.32764 | val_0_rmse: 0.55296 | val_1_rmse: 0.54694 |  0:00:30s
epoch 19 | loss: 0.32468 | val_0_rmse: 0.55229 | val_1_rmse: 0.55016 |  0:00:31s
epoch 20 | loss: 0.32407 | val_0_rmse: 0.56832 | val_1_rmse: 0.56602 |  0:00:33s
epoch 21 | loss: 0.33573 | val_0_rmse: 0.56591 | val_1_rmse: 0.56289 |  0:00:34s
epoch 22 | loss: 0.33333 | val_0_rmse: 0.56884 | val_1_rmse: 0.56366 |  0:00:36s
epoch 23 | loss: 0.32891 | val_0_rmse: 0.54258 | val_1_rmse: 0.5406  |  0:00:38s
epoch 24 | loss: 0.31851 | val_0_rmse: 0.54412 | val_1_rmse: 0.54114 |  0:00:39s
epoch 25 | loss: 0.32291 | val_0_rmse: 0.54006 | val_1_rmse: 0.54086 |  0:00:41s
epoch 26 | loss: 0.32314 | val_0_rmse: 0.55058 | val_1_rmse: 0.54939 |  0:00:42s
epoch 27 | loss: 0.31811 | val_0_rmse: 0.54646 | val_1_rmse: 0.54209 |  0:00:44s
epoch 28 | loss: 0.31743 | val_0_rmse: 0.55016 | val_1_rmse: 0.55183 |  0:00:46s
epoch 29 | loss: 0.31866 | val_0_rmse: 0.54637 | val_1_rmse: 0.54849 |  0:00:47s
epoch 30 | loss: 0.32358 | val_0_rmse: 0.55242 | val_1_rmse: 0.55379 |  0:00:49s
epoch 31 | loss: 0.32297 | val_0_rmse: 0.55604 | val_1_rmse: 0.55397 |  0:00:50s
epoch 32 | loss: 0.31888 | val_0_rmse: 0.54883 | val_1_rmse: 0.5462  |  0:00:52s
epoch 33 | loss: 0.31451 | val_0_rmse: 0.56824 | val_1_rmse: 0.56587 |  0:00:54s
epoch 34 | loss: 0.32041 | val_0_rmse: 0.54222 | val_1_rmse: 0.5411  |  0:00:55s
epoch 35 | loss: 0.31353 | val_0_rmse: 0.54179 | val_1_rmse: 0.54063 |  0:00:57s
epoch 36 | loss: 0.30997 | val_0_rmse: 0.54869 | val_1_rmse: 0.54999 |  0:00:58s
epoch 37 | loss: 0.32046 | val_0_rmse: 0.5433  | val_1_rmse: 0.53864 |  0:01:00s
epoch 38 | loss: 0.31287 | val_0_rmse: 0.54341 | val_1_rmse: 0.53843 |  0:01:02s
epoch 39 | loss: 0.31474 | val_0_rmse: 0.54343 | val_1_rmse: 0.54487 |  0:01:03s
epoch 40 | loss: 0.31657 | val_0_rmse: 0.577   | val_1_rmse: 0.58101 |  0:01:05s
epoch 41 | loss: 0.3204  | val_0_rmse: 0.54696 | val_1_rmse: 0.5469  |  0:01:06s
epoch 42 | loss: 0.31655 | val_0_rmse: 0.54238 | val_1_rmse: 0.53988 |  0:01:08s
epoch 43 | loss: 0.31756 | val_0_rmse: 0.53901 | val_1_rmse: 0.54049 |  0:01:09s
epoch 44 | loss: 0.31055 | val_0_rmse: 0.53334 | val_1_rmse: 0.5324  |  0:01:11s
epoch 45 | loss: 0.30768 | val_0_rmse: 0.53931 | val_1_rmse: 0.54451 |  0:01:13s
epoch 46 | loss: 0.31344 | val_0_rmse: 0.54271 | val_1_rmse: 0.5403  |  0:01:14s
epoch 47 | loss: 0.31746 | val_0_rmse: 0.55211 | val_1_rmse: 0.549   |  0:01:16s
epoch 48 | loss: 0.31087 | val_0_rmse: 0.53388 | val_1_rmse: 0.53032 |  0:01:17s
epoch 49 | loss: 0.30556 | val_0_rmse: 0.54969 | val_1_rmse: 0.55269 |  0:01:19s
epoch 50 | loss: 0.315   | val_0_rmse: 0.55709 | val_1_rmse: 0.55862 |  0:01:21s
epoch 51 | loss: 0.31949 | val_0_rmse: 0.53183 | val_1_rmse: 0.53131 |  0:01:22s
epoch 52 | loss: 0.31037 | val_0_rmse: 0.54672 | val_1_rmse: 0.53905 |  0:01:24s
epoch 53 | loss: 0.31093 | val_0_rmse: 0.53398 | val_1_rmse: 0.53456 |  0:01:25s
epoch 54 | loss: 0.30655 | val_0_rmse: 0.54339 | val_1_rmse: 0.53736 |  0:01:27s
epoch 55 | loss: 0.29926 | val_0_rmse: 0.5233  | val_1_rmse: 0.52033 |  0:01:28s
epoch 56 | loss: 0.29725 | val_0_rmse: 0.52202 | val_1_rmse: 0.52006 |  0:01:30s
epoch 57 | loss: 0.29792 | val_0_rmse: 0.54244 | val_1_rmse: 0.54055 |  0:01:32s
epoch 58 | loss: 0.31234 | val_0_rmse: 0.52942 | val_1_rmse: 0.5255  |  0:01:33s
epoch 59 | loss: 0.30097 | val_0_rmse: 0.52756 | val_1_rmse: 0.52493 |  0:01:35s
epoch 60 | loss: 0.30475 | val_0_rmse: 0.53607 | val_1_rmse: 0.53271 |  0:01:36s
epoch 61 | loss: 0.30786 | val_0_rmse: 0.52713 | val_1_rmse: 0.52165 |  0:01:38s
epoch 62 | loss: 0.30374 | val_0_rmse: 0.52485 | val_1_rmse: 0.52295 |  0:01:40s
epoch 63 | loss: 0.30129 | val_0_rmse: 0.54324 | val_1_rmse: 0.54306 |  0:01:41s
epoch 64 | loss: 0.30565 | val_0_rmse: 0.5296  | val_1_rmse: 0.52719 |  0:01:43s
epoch 65 | loss: 0.30461 | val_0_rmse: 0.55319 | val_1_rmse: 0.55216 |  0:01:44s
epoch 66 | loss: 0.31558 | val_0_rmse: 0.5352  | val_1_rmse: 0.53598 |  0:01:46s
epoch 67 | loss: 0.30885 | val_0_rmse: 0.52248 | val_1_rmse: 0.52655 |  0:01:48s
epoch 68 | loss: 0.30718 | val_0_rmse: 0.52853 | val_1_rmse: 0.53124 |  0:01:49s
epoch 69 | loss: 0.30997 | val_0_rmse: 0.53844 | val_1_rmse: 0.54273 |  0:01:51s
epoch 70 | loss: 0.31606 | val_0_rmse: 0.54096 | val_1_rmse: 0.54464 |  0:01:52s
epoch 71 | loss: 0.30276 | val_0_rmse: 0.52997 | val_1_rmse: 0.52592 |  0:01:54s
epoch 72 | loss: 0.30891 | val_0_rmse: 0.52704 | val_1_rmse: 0.52417 |  0:01:55s
epoch 73 | loss: 0.31252 | val_0_rmse: 0.53263 | val_1_rmse: 0.53105 |  0:01:57s
epoch 74 | loss: 0.30307 | val_0_rmse: 0.52454 | val_1_rmse: 0.52599 |  0:01:59s
epoch 75 | loss: 0.2995  | val_0_rmse: 0.5378  | val_1_rmse: 0.53559 |  0:02:00s
epoch 76 | loss: 0.29629 | val_0_rmse: 0.53459 | val_1_rmse: 0.53553 |  0:02:02s
epoch 77 | loss: 0.30381 | val_0_rmse: 0.53108 | val_1_rmse: 0.53121 |  0:02:04s
epoch 78 | loss: 0.29867 | val_0_rmse: 0.53436 | val_1_rmse: 0.53107 |  0:02:05s
epoch 79 | loss: 0.30063 | val_0_rmse: 0.52272 | val_1_rmse: 0.52532 |  0:02:07s
epoch 80 | loss: 0.294   | val_0_rmse: 0.5285  | val_1_rmse: 0.5257  |  0:02:08s
epoch 81 | loss: 0.30011 | val_0_rmse: 0.53482 | val_1_rmse: 0.53578 |  0:02:10s
epoch 82 | loss: 0.29673 | val_0_rmse: 0.52125 | val_1_rmse: 0.52355 |  0:02:11s
epoch 83 | loss: 0.29996 | val_0_rmse: 0.54001 | val_1_rmse: 0.5385  |  0:02:13s
epoch 84 | loss: 0.3095  | val_0_rmse: 0.54718 | val_1_rmse: 0.54982 |  0:02:15s
epoch 85 | loss: 0.2957  | val_0_rmse: 0.51573 | val_1_rmse: 0.52131 |  0:02:16s
epoch 86 | loss: 0.29761 | val_0_rmse: 0.51665 | val_1_rmse: 0.52019 |  0:02:18s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.52006
Best weights from best epoch are automatically used!
ended training at: 01:12:14
Feature importance:
[('Area', 0.32965800274888163), ('Baths', 0.06649136937078612), ('Beds', 0.04725183106529398), ('Latitude', 0.2087187190959125), ('Longitude', 0.24865066553125034), ('Month', 0.03323216322164742), ('Year', 0.065997248966228)]
Mean squared error is of 6806310042.2762
Mean absolute error:56841.74234047841
MAPE:0.18319223295285067
R2 score:0.7044581043198969
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:12:14
epoch 0  | loss: 0.77124 | val_0_rmse: 0.83402 | val_1_rmse: 0.84786 |  0:00:01s
epoch 1  | loss: 0.52015 | val_0_rmse: 0.683   | val_1_rmse: 0.68251 |  0:00:03s
epoch 2  | loss: 0.4631  | val_0_rmse: 0.65541 | val_1_rmse: 0.66089 |  0:00:04s
epoch 3  | loss: 0.4324  | val_0_rmse: 0.67163 | val_1_rmse: 0.69061 |  0:00:06s
epoch 4  | loss: 0.41865 | val_0_rmse: 0.63181 | val_1_rmse: 0.6396  |  0:00:08s
epoch 5  | loss: 0.39831 | val_0_rmse: 0.60507 | val_1_rmse: 0.60992 |  0:00:09s
epoch 6  | loss: 0.40229 | val_0_rmse: 0.60673 | val_1_rmse: 0.60518 |  0:00:11s
epoch 7  | loss: 0.38617 | val_0_rmse: 0.59882 | val_1_rmse: 0.59524 |  0:00:12s
epoch 8  | loss: 0.38017 | val_0_rmse: 0.59586 | val_1_rmse: 0.59886 |  0:00:14s
epoch 9  | loss: 0.3632  | val_0_rmse: 0.57895 | val_1_rmse: 0.5786  |  0:00:15s
epoch 10 | loss: 0.35596 | val_0_rmse: 0.57853 | val_1_rmse: 0.58387 |  0:00:17s
epoch 11 | loss: 0.35157 | val_0_rmse: 0.57936 | val_1_rmse: 0.57859 |  0:00:19s
epoch 12 | loss: 0.35519 | val_0_rmse: 0.5888  | val_1_rmse: 0.59377 |  0:00:20s
epoch 13 | loss: 0.35373 | val_0_rmse: 0.57713 | val_1_rmse: 0.57842 |  0:00:22s
epoch 14 | loss: 0.34088 | val_0_rmse: 0.55334 | val_1_rmse: 0.55779 |  0:00:23s
epoch 15 | loss: 0.33827 | val_0_rmse: 0.56494 | val_1_rmse: 0.56363 |  0:00:25s
epoch 16 | loss: 0.33873 | val_0_rmse: 0.55369 | val_1_rmse: 0.55983 |  0:00:27s
epoch 17 | loss: 0.33911 | val_0_rmse: 0.58    | val_1_rmse: 0.57793 |  0:00:28s
epoch 18 | loss: 0.33562 | val_0_rmse: 0.55214 | val_1_rmse: 0.55698 |  0:00:30s
epoch 19 | loss: 0.32812 | val_0_rmse: 0.56125 | val_1_rmse: 0.56755 |  0:00:31s
epoch 20 | loss: 0.3252  | val_0_rmse: 0.54967 | val_1_rmse: 0.54952 |  0:00:33s
epoch 21 | loss: 0.32399 | val_0_rmse: 0.54296 | val_1_rmse: 0.54819 |  0:00:35s
epoch 22 | loss: 0.32421 | val_0_rmse: 0.55036 | val_1_rmse: 0.55067 |  0:00:36s
epoch 23 | loss: 0.3264  | val_0_rmse: 0.53966 | val_1_rmse: 0.54537 |  0:00:38s
epoch 24 | loss: 0.32994 | val_0_rmse: 0.57963 | val_1_rmse: 0.57919 |  0:00:39s
epoch 25 | loss: 0.3327  | val_0_rmse: 0.55578 | val_1_rmse: 0.55253 |  0:00:41s
epoch 26 | loss: 0.33766 | val_0_rmse: 0.55134 | val_1_rmse: 0.55269 |  0:00:42s
epoch 27 | loss: 0.32324 | val_0_rmse: 0.54265 | val_1_rmse: 0.54383 |  0:00:44s
epoch 28 | loss: 0.32848 | val_0_rmse: 0.54929 | val_1_rmse: 0.5475  |  0:00:46s
epoch 29 | loss: 0.32346 | val_0_rmse: 0.55043 | val_1_rmse: 0.54541 |  0:00:47s
epoch 30 | loss: 0.32955 | val_0_rmse: 0.54477 | val_1_rmse: 0.54497 |  0:00:49s
epoch 31 | loss: 0.31735 | val_0_rmse: 0.55185 | val_1_rmse: 0.55616 |  0:00:50s
epoch 32 | loss: 0.3245  | val_0_rmse: 0.54089 | val_1_rmse: 0.54375 |  0:00:52s
epoch 33 | loss: 0.3125  | val_0_rmse: 0.55476 | val_1_rmse: 0.56235 |  0:00:54s
epoch 34 | loss: 0.31631 | val_0_rmse: 0.58777 | val_1_rmse: 0.59127 |  0:00:55s
epoch 35 | loss: 0.32163 | val_0_rmse: 0.54361 | val_1_rmse: 0.54749 |  0:00:57s
epoch 36 | loss: 0.32628 | val_0_rmse: 0.55606 | val_1_rmse: 0.55452 |  0:00:58s
epoch 37 | loss: 0.31473 | val_0_rmse: 0.54022 | val_1_rmse: 0.54605 |  0:01:00s
epoch 38 | loss: 0.31086 | val_0_rmse: 0.54438 | val_1_rmse: 0.54337 |  0:01:02s
epoch 39 | loss: 0.30657 | val_0_rmse: 0.53598 | val_1_rmse: 0.53721 |  0:01:03s
epoch 40 | loss: 0.3101  | val_0_rmse: 0.53394 | val_1_rmse: 0.53194 |  0:01:05s
epoch 41 | loss: 0.30862 | val_0_rmse: 0.53351 | val_1_rmse: 0.53056 |  0:01:06s
epoch 42 | loss: 0.31018 | val_0_rmse: 0.54314 | val_1_rmse: 0.54574 |  0:01:08s
epoch 43 | loss: 0.30535 | val_0_rmse: 0.54144 | val_1_rmse: 0.54128 |  0:01:09s
epoch 44 | loss: 0.30656 | val_0_rmse: 0.5458  | val_1_rmse: 0.54714 |  0:01:11s
epoch 45 | loss: 0.3061  | val_0_rmse: 0.53636 | val_1_rmse: 0.53827 |  0:01:13s
epoch 46 | loss: 0.30649 | val_0_rmse: 0.56768 | val_1_rmse: 0.56746 |  0:01:14s
epoch 47 | loss: 0.31941 | val_0_rmse: 0.56057 | val_1_rmse: 0.561   |  0:01:16s
epoch 48 | loss: 0.3093  | val_0_rmse: 0.53746 | val_1_rmse: 0.53617 |  0:01:17s
epoch 49 | loss: 0.30217 | val_0_rmse: 0.51948 | val_1_rmse: 0.52125 |  0:01:19s
epoch 50 | loss: 0.29313 | val_0_rmse: 0.53786 | val_1_rmse: 0.54212 |  0:01:21s
epoch 51 | loss: 0.30433 | val_0_rmse: 0.55401 | val_1_rmse: 0.55763 |  0:01:22s
epoch 52 | loss: 0.30671 | val_0_rmse: 0.54779 | val_1_rmse: 0.54991 |  0:01:24s
epoch 53 | loss: 0.30398 | val_0_rmse: 0.55926 | val_1_rmse: 0.55758 |  0:01:25s
epoch 54 | loss: 0.30715 | val_0_rmse: 0.53421 | val_1_rmse: 0.53323 |  0:01:27s
epoch 55 | loss: 0.3123  | val_0_rmse: 0.52878 | val_1_rmse: 0.52967 |  0:01:28s
epoch 56 | loss: 0.30052 | val_0_rmse: 0.52392 | val_1_rmse: 0.52437 |  0:01:30s
epoch 57 | loss: 0.30015 | val_0_rmse: 0.53508 | val_1_rmse: 0.53639 |  0:01:32s
epoch 58 | loss: 0.2988  | val_0_rmse: 0.55986 | val_1_rmse: 0.56789 |  0:01:33s
epoch 59 | loss: 0.29946 | val_0_rmse: 0.53702 | val_1_rmse: 0.54417 |  0:01:35s
epoch 60 | loss: 0.29355 | val_0_rmse: 0.51741 | val_1_rmse: 0.51941 |  0:01:36s
epoch 61 | loss: 0.29503 | val_0_rmse: 0.53639 | val_1_rmse: 0.5407  |  0:01:38s
epoch 62 | loss: 0.29355 | val_0_rmse: 0.5175  | val_1_rmse: 0.51839 |  0:01:40s
epoch 63 | loss: 0.29722 | val_0_rmse: 0.53373 | val_1_rmse: 0.5369  |  0:01:41s
epoch 64 | loss: 0.29741 | val_0_rmse: 0.5553  | val_1_rmse: 0.56088 |  0:01:43s
epoch 65 | loss: 0.29359 | val_0_rmse: 0.54257 | val_1_rmse: 0.54729 |  0:01:44s
epoch 66 | loss: 0.2949  | val_0_rmse: 0.52385 | val_1_rmse: 0.52799 |  0:01:46s
epoch 67 | loss: 0.29153 | val_0_rmse: 0.52913 | val_1_rmse: 0.53039 |  0:01:48s
epoch 68 | loss: 0.2944  | val_0_rmse: 0.52541 | val_1_rmse: 0.52276 |  0:01:49s
epoch 69 | loss: 0.29754 | val_0_rmse: 0.53246 | val_1_rmse: 0.54248 |  0:01:51s
epoch 70 | loss: 0.29675 | val_0_rmse: 0.55614 | val_1_rmse: 0.56473 |  0:01:52s
epoch 71 | loss: 0.29861 | val_0_rmse: 0.52203 | val_1_rmse: 0.52673 |  0:01:54s
epoch 72 | loss: 0.28554 | val_0_rmse: 0.52277 | val_1_rmse: 0.52635 |  0:01:55s
epoch 73 | loss: 0.2945  | val_0_rmse: 0.53413 | val_1_rmse: 0.53649 |  0:01:57s
epoch 74 | loss: 0.2881  | val_0_rmse: 0.52455 | val_1_rmse: 0.52823 |  0:01:59s
epoch 75 | loss: 0.29844 | val_0_rmse: 0.55763 | val_1_rmse: 0.55295 |  0:02:00s
epoch 76 | loss: 0.29748 | val_0_rmse: 0.52555 | val_1_rmse: 0.52763 |  0:02:02s
epoch 77 | loss: 0.29474 | val_0_rmse: 0.5219  | val_1_rmse: 0.52331 |  0:02:03s
epoch 78 | loss: 0.29053 | val_0_rmse: 0.51558 | val_1_rmse: 0.51794 |  0:02:05s
epoch 79 | loss: 0.29244 | val_0_rmse: 0.51872 | val_1_rmse: 0.52141 |  0:02:06s
epoch 80 | loss: 0.28775 | val_0_rmse: 0.50846 | val_1_rmse: 0.51361 |  0:02:08s
epoch 81 | loss: 0.28319 | val_0_rmse: 0.52158 | val_1_rmse: 0.52811 |  0:02:10s
epoch 82 | loss: 0.29176 | val_0_rmse: 0.51803 | val_1_rmse: 0.52038 |  0:02:11s
epoch 83 | loss: 0.29207 | val_0_rmse: 0.52373 | val_1_rmse: 0.52851 |  0:02:13s
epoch 84 | loss: 0.29671 | val_0_rmse: 0.52131 | val_1_rmse: 0.52445 |  0:02:14s
epoch 85 | loss: 0.29074 | val_0_rmse: 0.52602 | val_1_rmse: 0.5267  |  0:02:16s
epoch 86 | loss: 0.29846 | val_0_rmse: 0.53324 | val_1_rmse: 0.53764 |  0:02:18s
epoch 87 | loss: 0.29475 | val_0_rmse: 0.55064 | val_1_rmse: 0.55727 |  0:02:19s
epoch 88 | loss: 0.29047 | val_0_rmse: 0.52075 | val_1_rmse: 0.52013 |  0:02:21s
epoch 89 | loss: 0.28529 | val_0_rmse: 0.52372 | val_1_rmse: 0.52605 |  0:02:22s
epoch 90 | loss: 0.29046 | val_0_rmse: 0.53162 | val_1_rmse: 0.53948 |  0:02:24s
epoch 91 | loss: 0.29944 | val_0_rmse: 0.55343 | val_1_rmse: 0.5638  |  0:02:26s
epoch 92 | loss: 0.29326 | val_0_rmse: 0.5211  | val_1_rmse: 0.5227  |  0:02:27s
epoch 93 | loss: 0.29104 | val_0_rmse: 0.5124  | val_1_rmse: 0.51606 |  0:02:29s
epoch 94 | loss: 0.29208 | val_0_rmse: 0.51542 | val_1_rmse: 0.52153 |  0:02:30s
epoch 95 | loss: 0.28498 | val_0_rmse: 0.51583 | val_1_rmse: 0.52127 |  0:02:32s
epoch 96 | loss: 0.28836 | val_0_rmse: 0.51708 | val_1_rmse: 0.52128 |  0:02:33s
epoch 97 | loss: 0.28116 | val_0_rmse: 0.52351 | val_1_rmse: 0.5257  |  0:02:35s
epoch 98 | loss: 0.29167 | val_0_rmse: 0.52129 | val_1_rmse: 0.52411 |  0:02:37s
epoch 99 | loss: 0.30245 | val_0_rmse: 0.55596 | val_1_rmse: 0.55457 |  0:02:38s
epoch 100| loss: 0.28802 | val_0_rmse: 0.50973 | val_1_rmse: 0.51283 |  0:02:40s
epoch 101| loss: 0.29433 | val_0_rmse: 0.53401 | val_1_rmse: 0.54395 |  0:02:41s
epoch 102| loss: 0.30282 | val_0_rmse: 0.51477 | val_1_rmse: 0.51485 |  0:02:43s
epoch 103| loss: 0.28455 | val_0_rmse: 0.5177  | val_1_rmse: 0.52727 |  0:02:44s
epoch 104| loss: 0.28791 | val_0_rmse: 0.51754 | val_1_rmse: 0.52397 |  0:02:46s
epoch 105| loss: 0.28513 | val_0_rmse: 0.51504 | val_1_rmse: 0.52344 |  0:02:48s
epoch 106| loss: 0.28095 | val_0_rmse: 0.51437 | val_1_rmse: 0.52109 |  0:02:49s
epoch 107| loss: 0.28626 | val_0_rmse: 0.51606 | val_1_rmse: 0.51942 |  0:02:51s
epoch 108| loss: 0.29019 | val_0_rmse: 0.54876 | val_1_rmse: 0.55456 |  0:02:52s
epoch 109| loss: 0.29998 | val_0_rmse: 0.53555 | val_1_rmse: 0.53994 |  0:02:54s
epoch 110| loss: 0.29293 | val_0_rmse: 0.53642 | val_1_rmse: 0.53997 |  0:02:56s
epoch 111| loss: 0.30798 | val_0_rmse: 0.52978 | val_1_rmse: 0.53081 |  0:02:57s
epoch 112| loss: 0.29995 | val_0_rmse: 0.5428  | val_1_rmse: 0.54851 |  0:02:59s
epoch 113| loss: 0.28834 | val_0_rmse: 0.52484 | val_1_rmse: 0.53405 |  0:03:00s
epoch 114| loss: 0.2777  | val_0_rmse: 0.52959 | val_1_rmse: 0.53438 |  0:03:02s
epoch 115| loss: 0.28808 | val_0_rmse: 0.58432 | val_1_rmse: 0.58445 |  0:03:03s
epoch 116| loss: 0.29961 | val_0_rmse: 0.54216 | val_1_rmse: 0.53875 |  0:03:05s
epoch 117| loss: 0.30074 | val_0_rmse: 0.5183  | val_1_rmse: 0.52144 |  0:03:07s
epoch 118| loss: 0.28934 | val_0_rmse: 0.53431 | val_1_rmse: 0.53753 |  0:03:08s
epoch 119| loss: 0.27606 | val_0_rmse: 0.52006 | val_1_rmse: 0.52876 |  0:03:10s
epoch 120| loss: 0.28938 | val_0_rmse: 0.55169 | val_1_rmse: 0.56052 |  0:03:11s
epoch 121| loss: 0.28686 | val_0_rmse: 0.52061 | val_1_rmse: 0.52765 |  0:03:13s
epoch 122| loss: 0.29612 | val_0_rmse: 0.56174 | val_1_rmse: 0.56702 |  0:03:15s
epoch 123| loss: 0.30399 | val_0_rmse: 0.53595 | val_1_rmse: 0.53898 |  0:03:16s
epoch 124| loss: 0.2894  | val_0_rmse: 0.52245 | val_1_rmse: 0.52424 |  0:03:18s
epoch 125| loss: 0.28206 | val_0_rmse: 0.50829 | val_1_rmse: 0.51178 |  0:03:19s
epoch 126| loss: 0.29016 | val_0_rmse: 0.53724 | val_1_rmse: 0.5423  |  0:03:21s
epoch 127| loss: 0.30187 | val_0_rmse: 0.5244  | val_1_rmse: 0.53457 |  0:03:23s
epoch 128| loss: 0.30037 | val_0_rmse: 0.53366 | val_1_rmse: 0.53958 |  0:03:24s
epoch 129| loss: 0.30197 | val_0_rmse: 0.53031 | val_1_rmse: 0.5353  |  0:03:26s
epoch 130| loss: 0.29395 | val_0_rmse: 0.52919 | val_1_rmse: 0.53106 |  0:03:27s
epoch 131| loss: 0.29659 | val_0_rmse: 0.5449  | val_1_rmse: 0.5495  |  0:03:29s
epoch 132| loss: 0.29322 | val_0_rmse: 0.53086 | val_1_rmse: 0.53339 |  0:03:30s
epoch 133| loss: 0.29315 | val_0_rmse: 0.53816 | val_1_rmse: 0.53624 |  0:03:32s
epoch 134| loss: 0.28772 | val_0_rmse: 0.53057 | val_1_rmse: 0.53389 |  0:03:34s
epoch 135| loss: 0.2856  | val_0_rmse: 0.51786 | val_1_rmse: 0.52101 |  0:03:35s
epoch 136| loss: 0.28952 | val_0_rmse: 0.53014 | val_1_rmse: 0.53435 |  0:03:37s
epoch 137| loss: 0.29852 | val_0_rmse: 0.57165 | val_1_rmse: 0.57735 |  0:03:38s
epoch 138| loss: 0.29343 | val_0_rmse: 0.51968 | val_1_rmse: 0.52682 |  0:03:40s
epoch 139| loss: 0.2934  | val_0_rmse: 0.51889 | val_1_rmse: 0.52594 |  0:03:42s
epoch 140| loss: 0.28859 | val_0_rmse: 0.52393 | val_1_rmse: 0.53111 |  0:03:43s
epoch 141| loss: 0.28978 | val_0_rmse: 0.55077 | val_1_rmse: 0.55766 |  0:03:45s
epoch 142| loss: 0.30297 | val_0_rmse: 0.52638 | val_1_rmse: 0.5309  |  0:03:46s
epoch 143| loss: 0.28815 | val_0_rmse: 0.53557 | val_1_rmse: 0.53684 |  0:03:48s
epoch 144| loss: 0.28641 | val_0_rmse: 0.51729 | val_1_rmse: 0.51942 |  0:03:49s
epoch 145| loss: 0.27992 | val_0_rmse: 0.50352 | val_1_rmse: 0.51075 |  0:03:51s
epoch 146| loss: 0.28521 | val_0_rmse: 0.53468 | val_1_rmse: 0.53761 |  0:03:53s
epoch 147| loss: 0.28514 | val_0_rmse: 0.51614 | val_1_rmse: 0.52148 |  0:03:54s
epoch 148| loss: 0.27653 | val_0_rmse: 0.50973 | val_1_rmse: 0.52018 |  0:03:56s
epoch 149| loss: 0.28652 | val_0_rmse: 0.51533 | val_1_rmse: 0.52903 |  0:03:57s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.51075
Best weights from best epoch are automatically used!
ended training at: 01:16:13
Feature importance:
[('Area', 0.3139965065121477), ('Baths', 0.03804906453064404), ('Beds', 0.07054177741194932), ('Latitude', 0.20553617886799927), ('Longitude', 0.22011893209756653), ('Month', 0.0), ('Year', 0.1517575405796931)]
Mean squared error is of 6254798665.679723
Mean absolute error:53822.169985453795
MAPE:0.17630381479969234
R2 score:0.726408346777811
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:16:13
epoch 0  | loss: 0.61099 | val_0_rmse: 0.74495 | val_1_rmse: 0.74211 |  0:00:06s
epoch 1  | loss: 0.53914 | val_0_rmse: 0.70059 | val_1_rmse: 0.69709 |  0:00:12s
epoch 2  | loss: 0.46748 | val_0_rmse: 0.66437 | val_1_rmse: 0.66623 |  0:00:18s
epoch 3  | loss: 0.42743 | val_0_rmse: 0.65009 | val_1_rmse: 0.64741 |  0:00:24s
epoch 4  | loss: 0.39035 | val_0_rmse: 0.64879 | val_1_rmse: 0.64861 |  0:00:30s
epoch 5  | loss: 0.3754  | val_0_rmse: 0.58805 | val_1_rmse: 0.58475 |  0:00:36s
epoch 6  | loss: 0.37034 | val_0_rmse: 0.666   | val_1_rmse: 0.66247 |  0:00:42s
epoch 7  | loss: 0.36547 | val_0_rmse: 0.62306 | val_1_rmse: 0.6218  |  0:00:48s
epoch 8  | loss: 0.36417 | val_0_rmse: 0.63744 | val_1_rmse: 0.63467 |  0:00:54s
epoch 9  | loss: 0.36506 | val_0_rmse: 0.59078 | val_1_rmse: 0.58924 |  0:01:00s
epoch 10 | loss: 0.36006 | val_0_rmse: 0.68167 | val_1_rmse: 0.67764 |  0:01:06s
epoch 11 | loss: 0.3617  | val_0_rmse: 0.73959 | val_1_rmse: 0.73403 |  0:01:12s
epoch 12 | loss: 0.35901 | val_0_rmse: 0.63373 | val_1_rmse: 0.62984 |  0:01:18s
epoch 13 | loss: 0.35949 | val_0_rmse: 0.71248 | val_1_rmse: 0.7093  |  0:01:24s
epoch 14 | loss: 0.35069 | val_0_rmse: 0.62041 | val_1_rmse: 0.61789 |  0:01:30s
epoch 15 | loss: 0.35277 | val_0_rmse: 0.58688 | val_1_rmse: 0.58556 |  0:01:36s
epoch 16 | loss: 0.3481  | val_0_rmse: 0.58651 | val_1_rmse: 0.58215 |  0:01:42s
epoch 17 | loss: 0.35194 | val_0_rmse: 0.66391 | val_1_rmse: 0.66135 |  0:01:48s
epoch 18 | loss: 0.36307 | val_0_rmse: 0.61566 | val_1_rmse: 0.61007 |  0:01:54s
epoch 19 | loss: 0.35353 | val_0_rmse: 0.74422 | val_1_rmse: 0.73676 |  0:02:00s
epoch 20 | loss: 0.35027 | val_0_rmse: 0.59116 | val_1_rmse: 0.58679 |  0:02:06s
epoch 21 | loss: 0.35367 | val_0_rmse: 0.61522 | val_1_rmse: 0.61112 |  0:02:11s
epoch 22 | loss: 0.36673 | val_0_rmse: 0.59566 | val_1_rmse: 0.59105 |  0:02:18s
epoch 23 | loss: 0.36528 | val_0_rmse: 0.6675  | val_1_rmse: 0.66299 |  0:02:24s
epoch 24 | loss: 0.35768 | val_0_rmse: 0.61905 | val_1_rmse: 0.61785 |  0:02:30s
epoch 25 | loss: 0.35581 | val_0_rmse: 0.62696 | val_1_rmse: 0.62112 |  0:02:36s
epoch 26 | loss: 0.35047 | val_0_rmse: 0.58471 | val_1_rmse: 0.57974 |  0:02:42s
epoch 27 | loss: 0.35295 | val_0_rmse: 0.66587 | val_1_rmse: 0.66133 |  0:02:47s
epoch 28 | loss: 0.35468 | val_0_rmse: 0.58053 | val_1_rmse: 0.57602 |  0:02:53s
epoch 29 | loss: 0.35268 | val_0_rmse: 0.62172 | val_1_rmse: 0.61672 |  0:03:00s
epoch 30 | loss: 0.3567  | val_0_rmse: 0.61213 | val_1_rmse: 0.60659 |  0:03:06s
epoch 31 | loss: 0.3574  | val_0_rmse: 0.76072 | val_1_rmse: 0.7522  |  0:03:11s
epoch 32 | loss: 0.35957 | val_0_rmse: 0.71665 | val_1_rmse: 0.71189 |  0:03:17s
epoch 33 | loss: 0.34823 | val_0_rmse: 0.60846 | val_1_rmse: 0.6051  |  0:03:23s
epoch 34 | loss: 0.35165 | val_0_rmse: 0.6822  | val_1_rmse: 0.67794 |  0:03:29s
epoch 35 | loss: 0.34567 | val_0_rmse: 0.66972 | val_1_rmse: 0.66059 |  0:03:35s
epoch 36 | loss: 0.35008 | val_0_rmse: 0.74328 | val_1_rmse: 0.7383  |  0:03:41s
epoch 37 | loss: 0.34865 | val_0_rmse: 0.59671 | val_1_rmse: 0.59238 |  0:03:47s
epoch 38 | loss: 0.34461 | val_0_rmse: 0.60244 | val_1_rmse: 0.59803 |  0:03:53s
epoch 39 | loss: 0.34622 | val_0_rmse: 0.65041 | val_1_rmse: 0.64073 |  0:03:59s
epoch 40 | loss: 0.3465  | val_0_rmse: 0.64134 | val_1_rmse: 0.63564 |  0:04:05s
epoch 41 | loss: 0.34565 | val_0_rmse: 0.59754 | val_1_rmse: 0.59503 |  0:04:12s
epoch 42 | loss: 0.34503 | val_0_rmse: 0.58022 | val_1_rmse: 0.57666 |  0:04:18s
epoch 43 | loss: 0.34393 | val_0_rmse: 0.5936  | val_1_rmse: 0.58784 |  0:04:24s
epoch 44 | loss: 0.34034 | val_0_rmse: 0.61116 | val_1_rmse: 0.60307 |  0:04:30s
epoch 45 | loss: 0.34236 | val_0_rmse: 0.72083 | val_1_rmse: 0.71367 |  0:04:36s
epoch 46 | loss: 0.34769 | val_0_rmse: 0.67095 | val_1_rmse: 0.66517 |  0:04:41s
epoch 47 | loss: 0.34175 | val_0_rmse: 0.64795 | val_1_rmse: 0.64678 |  0:04:48s
epoch 48 | loss: 0.34141 | val_0_rmse: 0.68282 | val_1_rmse: 0.67387 |  0:04:54s
epoch 49 | loss: 0.34361 | val_0_rmse: 0.61438 | val_1_rmse: 0.61149 |  0:05:00s
epoch 50 | loss: 0.33624 | val_0_rmse: 0.62349 | val_1_rmse: 0.6227  |  0:05:06s
epoch 51 | loss: 0.34202 | val_0_rmse: 0.63999 | val_1_rmse: 0.63875 |  0:05:11s
epoch 52 | loss: 0.34174 | val_0_rmse: 0.61873 | val_1_rmse: 0.6163  |  0:05:18s
epoch 53 | loss: 0.3386  | val_0_rmse: 0.64175 | val_1_rmse: 0.63872 |  0:05:24s
epoch 54 | loss: 0.33851 | val_0_rmse: 0.69015 | val_1_rmse: 0.6818  |  0:05:30s
epoch 55 | loss: 0.33866 | val_0_rmse: 0.60474 | val_1_rmse: 0.6015  |  0:05:36s
epoch 56 | loss: 0.34072 | val_0_rmse: 0.58136 | val_1_rmse: 0.57718 |  0:05:42s
epoch 57 | loss: 0.33697 | val_0_rmse: 0.58834 | val_1_rmse: 0.58444 |  0:05:48s
epoch 58 | loss: 0.3386  | val_0_rmse: 0.62874 | val_1_rmse: 0.62833 |  0:05:54s

Early stopping occured at epoch 58 with best_epoch = 28 and best_val_1_rmse = 0.57602
Best weights from best epoch are automatically used!
ended training at: 01:22:09
Feature importance:
[('Area', 0.37495600306573856), ('Baths', 0.19459150853930834), ('Beds', 0.0), ('Latitude', 0.21363789563965907), ('Longitude', 0.21224279285234504), ('Month', 0.0), ('Year', 0.0045717999029489835)]
Mean squared error is of 2295762298.2880335
Mean absolute error:34319.22812285486
MAPE:0.33925790305699516
R2 score:0.6606927867488499
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:22:10
epoch 0  | loss: 0.57562 | val_0_rmse: 0.70039 | val_1_rmse: 0.69534 |  0:00:06s
epoch 1  | loss: 0.48143 | val_0_rmse: 0.71051 | val_1_rmse: 0.70705 |  0:00:12s
epoch 2  | loss: 0.45923 | val_0_rmse: 0.69655 | val_1_rmse: 0.69344 |  0:00:18s
epoch 3  | loss: 0.44183 | val_0_rmse: 0.66391 | val_1_rmse: 0.66308 |  0:00:24s
epoch 4  | loss: 0.41393 | val_0_rmse: 0.79491 | val_1_rmse: 0.79039 |  0:00:30s
epoch 5  | loss: 0.40834 | val_0_rmse: 0.69004 | val_1_rmse: 0.68755 |  0:00:36s
epoch 6  | loss: 0.39996 | val_0_rmse: 0.69875 | val_1_rmse: 0.69594 |  0:00:42s
epoch 7  | loss: 0.39639 | val_0_rmse: 0.65574 | val_1_rmse: 0.65119 |  0:00:48s
epoch 8  | loss: 0.39567 | val_0_rmse: 0.61958 | val_1_rmse: 0.61667 |  0:00:54s
epoch 9  | loss: 0.3863  | val_0_rmse: 0.61573 | val_1_rmse: 0.61269 |  0:01:00s
epoch 10 | loss: 0.38461 | val_0_rmse: 0.69287 | val_1_rmse: 0.68834 |  0:01:06s
epoch 11 | loss: 0.37235 | val_0_rmse: 0.67884 | val_1_rmse: 0.67463 |  0:01:12s
epoch 12 | loss: 0.37228 | val_0_rmse: 0.62981 | val_1_rmse: 0.62484 |  0:01:18s
epoch 13 | loss: 0.37449 | val_0_rmse: 0.63834 | val_1_rmse: 0.63791 |  0:01:24s
epoch 14 | loss: 0.37053 | val_0_rmse: 0.62549 | val_1_rmse: 0.6231  |  0:01:30s
epoch 15 | loss: 0.36577 | val_0_rmse: 0.63615 | val_1_rmse: 0.63386 |  0:01:36s
epoch 16 | loss: 0.36548 | val_0_rmse: 0.62597 | val_1_rmse: 0.62264 |  0:01:42s
epoch 17 | loss: 0.36352 | val_0_rmse: 0.68459 | val_1_rmse: 0.68005 |  0:01:48s
epoch 18 | loss: 0.37401 | val_0_rmse: 0.89605 | val_1_rmse: 0.89454 |  0:01:54s
epoch 19 | loss: 0.3809  | val_0_rmse: 0.66498 | val_1_rmse: 0.65999 |  0:02:00s
epoch 20 | loss: 0.36386 | val_0_rmse: 0.60791 | val_1_rmse: 0.60305 |  0:02:06s
epoch 21 | loss: 0.35628 | val_0_rmse: 0.65061 | val_1_rmse: 0.64585 |  0:02:12s
epoch 22 | loss: 0.35493 | val_0_rmse: 0.63724 | val_1_rmse: 0.63481 |  0:02:18s
epoch 23 | loss: 0.35248 | val_0_rmse: 0.6339  | val_1_rmse: 0.63058 |  0:02:24s
epoch 24 | loss: 0.35034 | val_0_rmse: 0.59186 | val_1_rmse: 0.58773 |  0:02:30s
epoch 25 | loss: 0.35257 | val_0_rmse: 0.6384  | val_1_rmse: 0.63484 |  0:02:36s
epoch 26 | loss: 0.35459 | val_0_rmse: 0.62293 | val_1_rmse: 0.6203  |  0:02:42s
epoch 27 | loss: 0.34708 | val_0_rmse: 0.65788 | val_1_rmse: 0.65606 |  0:02:48s
epoch 28 | loss: 0.34922 | val_0_rmse: 0.61911 | val_1_rmse: 0.61505 |  0:02:54s
epoch 29 | loss: 0.34814 | val_0_rmse: 0.61775 | val_1_rmse: 0.61538 |  0:03:00s
epoch 30 | loss: 0.34569 | val_0_rmse: 0.5962  | val_1_rmse: 0.59271 |  0:03:06s
epoch 31 | loss: 0.34174 | val_0_rmse: 0.60836 | val_1_rmse: 0.60465 |  0:03:12s
epoch 32 | loss: 0.34164 | val_0_rmse: 0.67214 | val_1_rmse: 0.67038 |  0:03:18s
epoch 33 | loss: 0.343   | val_0_rmse: 0.62849 | val_1_rmse: 0.62582 |  0:03:24s
epoch 34 | loss: 0.34167 | val_0_rmse: 0.68933 | val_1_rmse: 0.68641 |  0:03:30s
epoch 35 | loss: 0.34166 | val_0_rmse: 0.70869 | val_1_rmse: 0.70218 |  0:03:36s
epoch 36 | loss: 0.36053 | val_0_rmse: 0.7373  | val_1_rmse: 0.73196 |  0:03:42s
epoch 37 | loss: 0.35143 | val_0_rmse: 0.67586 | val_1_rmse: 0.67401 |  0:03:48s
epoch 38 | loss: 0.35318 | val_0_rmse: 0.70387 | val_1_rmse: 0.69907 |  0:03:54s
epoch 39 | loss: 0.34594 | val_0_rmse: 0.63371 | val_1_rmse: 0.63048 |  0:04:00s
epoch 40 | loss: 0.34329 | val_0_rmse: 0.6756  | val_1_rmse: 0.67103 |  0:04:06s
epoch 41 | loss: 0.34124 | val_0_rmse: 0.64423 | val_1_rmse: 0.63836 |  0:04:12s
epoch 42 | loss: 0.34837 | val_0_rmse: 0.59869 | val_1_rmse: 0.59238 |  0:04:18s
epoch 43 | loss: 0.3435  | val_0_rmse: 0.64548 | val_1_rmse: 0.64388 |  0:04:24s
epoch 44 | loss: 0.33873 | val_0_rmse: 0.67057 | val_1_rmse: 0.66943 |  0:04:30s
epoch 45 | loss: 0.34163 | val_0_rmse: 0.58814 | val_1_rmse: 0.58685 |  0:04:36s
epoch 46 | loss: 0.33602 | val_0_rmse: 0.72422 | val_1_rmse: 0.72653 |  0:04:42s
epoch 47 | loss: 0.338   | val_0_rmse: 0.60067 | val_1_rmse: 0.5963  |  0:04:48s
epoch 48 | loss: 0.33591 | val_0_rmse: 0.65681 | val_1_rmse: 0.65054 |  0:04:54s
epoch 49 | loss: 0.3396  | val_0_rmse: 0.58886 | val_1_rmse: 0.5876  |  0:05:01s
epoch 50 | loss: 0.33547 | val_0_rmse: 0.67737 | val_1_rmse: 0.67216 |  0:05:07s
epoch 51 | loss: 0.33835 | val_0_rmse: 0.81034 | val_1_rmse: 0.8022  |  0:05:13s
epoch 52 | loss: 0.33346 | val_0_rmse: 0.66965 | val_1_rmse: 0.66393 |  0:05:19s
epoch 53 | loss: 0.33864 | val_0_rmse: 0.70019 | val_1_rmse: 0.69387 |  0:05:25s
epoch 54 | loss: 0.34142 | val_0_rmse: 0.69368 | val_1_rmse: 0.68523 |  0:05:31s
epoch 55 | loss: 0.33424 | val_0_rmse: 0.72195 | val_1_rmse: 0.71706 |  0:05:37s
epoch 56 | loss: 0.33521 | val_0_rmse: 0.80304 | val_1_rmse: 0.79466 |  0:05:43s
epoch 57 | loss: 0.33486 | val_0_rmse: 0.64524 | val_1_rmse: 0.64324 |  0:05:49s
epoch 58 | loss: 0.3425  | val_0_rmse: 0.66645 | val_1_rmse: 0.65876 |  0:05:55s
epoch 59 | loss: 0.3411  | val_0_rmse: 0.61647 | val_1_rmse: 0.61141 |  0:06:01s
epoch 60 | loss: 0.33702 | val_0_rmse: 0.71303 | val_1_rmse: 0.70789 |  0:06:07s
epoch 61 | loss: 0.34046 | val_0_rmse: 0.61641 | val_1_rmse: 0.61343 |  0:06:13s
epoch 62 | loss: 0.33754 | val_0_rmse: 0.70007 | val_1_rmse: 0.69356 |  0:06:19s
epoch 63 | loss: 0.33933 | val_0_rmse: 0.79773 | val_1_rmse: 0.79249 |  0:06:25s
epoch 64 | loss: 0.33778 | val_0_rmse: 0.6243  | val_1_rmse: 0.62121 |  0:06:31s
epoch 65 | loss: 0.34102 | val_0_rmse: 0.73638 | val_1_rmse: 0.72848 |  0:06:37s
epoch 66 | loss: 0.33606 | val_0_rmse: 0.63679 | val_1_rmse: 0.63103 |  0:06:43s
epoch 67 | loss: 0.33642 | val_0_rmse: 0.58786 | val_1_rmse: 0.58479 |  0:06:49s
epoch 68 | loss: 0.33958 | val_0_rmse: 0.69148 | val_1_rmse: 0.68752 |  0:06:55s
epoch 69 | loss: 0.3377  | val_0_rmse: 0.63779 | val_1_rmse: 0.63271 |  0:07:01s
epoch 70 | loss: 0.33706 | val_0_rmse: 0.64068 | val_1_rmse: 0.63927 |  0:07:07s
epoch 71 | loss: 0.33529 | val_0_rmse: 0.98044 | val_1_rmse: 0.97025 |  0:07:13s
epoch 72 | loss: 0.33269 | val_0_rmse: 0.78167 | val_1_rmse: 0.7722  |  0:07:19s
epoch 73 | loss: 0.34303 | val_0_rmse: 0.80266 | val_1_rmse: 0.96335 |  0:07:25s
epoch 74 | loss: 0.33614 | val_0_rmse: 0.62362 | val_1_rmse: 0.61999 |  0:07:31s
epoch 75 | loss: 0.33374 | val_0_rmse: 0.6276  | val_1_rmse: 0.62964 |  0:07:37s
epoch 76 | loss: 0.32949 | val_0_rmse: 0.78645 | val_1_rmse: 0.78214 |  0:07:43s
epoch 77 | loss: 0.32996 | val_0_rmse: 0.77526 | val_1_rmse: 0.76917 |  0:07:49s
epoch 78 | loss: 0.32978 | val_0_rmse: 0.73864 | val_1_rmse: 0.73817 |  0:07:55s
epoch 79 | loss: 0.33322 | val_0_rmse: 0.63258 | val_1_rmse: 0.63111 |  0:08:01s
epoch 80 | loss: 0.33261 | val_0_rmse: 0.68177 | val_1_rmse: 0.67877 |  0:08:07s
epoch 81 | loss: 0.33025 | val_0_rmse: 0.71878 | val_1_rmse: 0.71616 |  0:08:13s
epoch 82 | loss: 0.32897 | val_0_rmse: 0.64323 | val_1_rmse: 0.64453 |  0:08:19s
epoch 83 | loss: 0.32891 | val_0_rmse: 0.97982 | val_1_rmse: 0.97294 |  0:08:25s
epoch 84 | loss: 0.32801 | val_0_rmse: 0.80716 | val_1_rmse: 0.80173 |  0:08:31s
epoch 85 | loss: 0.3312  | val_0_rmse: 0.62346 | val_1_rmse: 0.62204 |  0:08:37s
epoch 86 | loss: 0.32852 | val_0_rmse: 0.65748 | val_1_rmse: 0.65565 |  0:08:43s
epoch 87 | loss: 0.32569 | val_0_rmse: 0.66327 | val_1_rmse: 0.65811 |  0:08:49s
epoch 88 | loss: 0.32693 | val_0_rmse: 0.72084 | val_1_rmse: 0.7166  |  0:08:55s
epoch 89 | loss: 0.3267  | val_0_rmse: 0.63285 | val_1_rmse: 0.63334 |  0:09:01s
epoch 90 | loss: 0.32882 | val_0_rmse: 0.87307 | val_1_rmse: 0.86623 |  0:09:07s
epoch 91 | loss: 0.32661 | val_0_rmse: 0.72628 | val_1_rmse: 0.7254  |  0:09:13s
epoch 92 | loss: 0.32502 | val_0_rmse: 0.99062 | val_1_rmse: 0.99906 |  0:09:19s
epoch 93 | loss: 0.34262 | val_0_rmse: 0.65439 | val_1_rmse: 0.65205 |  0:09:25s
epoch 94 | loss: 0.34259 | val_0_rmse: 0.78978 | val_1_rmse: 0.7835  |  0:09:31s
epoch 95 | loss: 0.33221 | val_0_rmse: 0.76491 | val_1_rmse: 0.76282 |  0:09:37s
epoch 96 | loss: 0.33005 | val_0_rmse: 0.65156 | val_1_rmse: 0.6458  |  0:09:43s
epoch 97 | loss: 0.32815 | val_0_rmse: 0.68241 | val_1_rmse: 0.6809  |  0:09:49s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.58479
Best weights from best epoch are automatically used!
ended training at: 01:32:01
Feature importance:
[('Area', 0.24573283012930347), ('Baths', 0.3538006848688612), ('Beds', 0.0), ('Latitude', 0.18757929888575386), ('Longitude', 0.2128871861160815), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2379548713.05036
Mean absolute error:35722.46113978314
MAPE:0.3489978655453222
R2 score:0.6545753202398374
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:32:02
epoch 0  | loss: 0.59676 | val_0_rmse: 0.7246  | val_1_rmse: 0.72722 |  0:00:05s
epoch 1  | loss: 0.49982 | val_0_rmse: 0.70866 | val_1_rmse: 0.71277 |  0:00:11s
epoch 2  | loss: 0.47482 | val_0_rmse: 0.67073 | val_1_rmse: 0.67635 |  0:00:17s
epoch 3  | loss: 0.45407 | val_0_rmse: 0.68844 | val_1_rmse: 0.69284 |  0:00:23s
epoch 4  | loss: 0.43759 | val_0_rmse: 0.6509  | val_1_rmse: 0.65589 |  0:00:29s
epoch 5  | loss: 0.41989 | val_0_rmse: 0.6687  | val_1_rmse: 0.6713  |  0:00:35s
epoch 6  | loss: 0.40464 | val_0_rmse: 0.63494 | val_1_rmse: 0.63809 |  0:00:41s
epoch 7  | loss: 0.40299 | val_0_rmse: 0.67363 | val_1_rmse: 0.67982 |  0:00:47s
epoch 8  | loss: 0.40217 | val_0_rmse: 0.64193 | val_1_rmse: 0.64325 |  0:00:53s
epoch 9  | loss: 0.3963  | val_0_rmse: 0.65319 | val_1_rmse: 0.65324 |  0:00:59s
epoch 10 | loss: 0.39342 | val_0_rmse: 0.66281 | val_1_rmse: 0.66447 |  0:01:05s
epoch 11 | loss: 0.38543 | val_0_rmse: 0.79026 | val_1_rmse: 0.79766 |  0:01:11s
epoch 12 | loss: 0.39845 | val_0_rmse: 0.66394 | val_1_rmse: 0.66955 |  0:01:17s
epoch 13 | loss: 0.3947  | val_0_rmse: 0.62916 | val_1_rmse: 0.63218 |  0:01:23s
epoch 14 | loss: 0.38546 | val_0_rmse: 0.62375 | val_1_rmse: 0.62916 |  0:01:29s
epoch 15 | loss: 0.38616 | val_0_rmse: 0.63226 | val_1_rmse: 0.63624 |  0:01:35s
epoch 16 | loss: 0.37993 | val_0_rmse: 0.60129 | val_1_rmse: 0.60316 |  0:01:41s
epoch 17 | loss: 0.37558 | val_0_rmse: 0.6243  | val_1_rmse: 0.62696 |  0:01:47s
epoch 18 | loss: 0.37273 | val_0_rmse: 0.60456 | val_1_rmse: 0.60796 |  0:01:53s
epoch 19 | loss: 0.37185 | val_0_rmse: 0.63317 | val_1_rmse: 0.63651 |  0:01:59s
epoch 20 | loss: 0.36953 | val_0_rmse: 0.61474 | val_1_rmse: 0.61851 |  0:02:05s
epoch 21 | loss: 0.3676  | val_0_rmse: 0.8651  | val_1_rmse: 0.87372 |  0:02:11s
epoch 22 | loss: 0.37336 | val_0_rmse: 0.61473 | val_1_rmse: 0.61647 |  0:02:17s
epoch 23 | loss: 0.36653 | val_0_rmse: 0.64962 | val_1_rmse: 0.65377 |  0:02:23s
epoch 24 | loss: 0.36156 | val_0_rmse: 0.62269 | val_1_rmse: 0.62519 |  0:02:29s
epoch 25 | loss: 0.36937 | val_0_rmse: 0.68649 | val_1_rmse: 0.69178 |  0:02:35s
epoch 26 | loss: 0.37805 | val_0_rmse: 0.72395 | val_1_rmse: 0.72742 |  0:02:41s
epoch 27 | loss: 0.36723 | val_0_rmse: 0.59344 | val_1_rmse: 0.59393 |  0:02:47s
epoch 28 | loss: 0.36204 | val_0_rmse: 0.61831 | val_1_rmse: 0.62183 |  0:02:53s
epoch 29 | loss: 0.362   | val_0_rmse: 0.61646 | val_1_rmse: 0.61513 |  0:02:59s
epoch 30 | loss: 0.36004 | val_0_rmse: 0.58971 | val_1_rmse: 0.59132 |  0:03:05s
epoch 31 | loss: 0.35805 | val_0_rmse: 0.64641 | val_1_rmse: 0.64882 |  0:03:11s
epoch 32 | loss: 0.36212 | val_0_rmse: 0.82381 | val_1_rmse: 0.82919 |  0:03:17s
epoch 33 | loss: 0.35706 | val_0_rmse: 0.69473 | val_1_rmse: 0.69853 |  0:03:23s
epoch 34 | loss: 0.35616 | val_0_rmse: 0.67726 | val_1_rmse: 0.68148 |  0:03:29s
epoch 35 | loss: 0.35291 | val_0_rmse: 0.63113 | val_1_rmse: 0.63331 |  0:03:35s
epoch 36 | loss: 0.35171 | val_0_rmse: 0.62152 | val_1_rmse: 0.62376 |  0:03:41s
epoch 37 | loss: 0.34705 | val_0_rmse: 0.59968 | val_1_rmse: 0.60214 |  0:03:47s
epoch 38 | loss: 0.35299 | val_0_rmse: 0.61711 | val_1_rmse: 0.6196  |  0:03:53s
epoch 39 | loss: 0.34778 | val_0_rmse: 0.6274  | val_1_rmse: 0.63037 |  0:03:59s
epoch 40 | loss: 0.3475  | val_0_rmse: 0.61744 | val_1_rmse: 0.61807 |  0:04:05s
epoch 41 | loss: 0.34674 | val_0_rmse: 0.61813 | val_1_rmse: 0.62048 |  0:04:11s
epoch 42 | loss: 0.3423  | val_0_rmse: 0.60285 | val_1_rmse: 0.60544 |  0:04:17s
epoch 43 | loss: 0.34675 | val_0_rmse: 0.74189 | val_1_rmse: 0.74313 |  0:04:23s
epoch 44 | loss: 0.35279 | val_0_rmse: 0.61204 | val_1_rmse: 0.61711 |  0:04:29s
epoch 45 | loss: 0.34982 | val_0_rmse: 0.6573  | val_1_rmse: 0.65466 |  0:04:35s
epoch 46 | loss: 0.35165 | val_0_rmse: 0.57887 | val_1_rmse: 0.58025 |  0:04:41s
epoch 47 | loss: 0.34749 | val_0_rmse: 0.59316 | val_1_rmse: 0.59693 |  0:04:47s
epoch 48 | loss: 0.34421 | val_0_rmse: 0.58329 | val_1_rmse: 0.58375 |  0:04:53s
epoch 49 | loss: 0.34412 | val_0_rmse: 0.62843 | val_1_rmse: 0.6269  |  0:04:59s
epoch 50 | loss: 0.34242 | val_0_rmse: 0.60261 | val_1_rmse: 0.60565 |  0:05:05s
epoch 51 | loss: 0.34117 | val_0_rmse: 0.70113 | val_1_rmse: 0.70742 |  0:05:11s
epoch 52 | loss: 0.34214 | val_0_rmse: 0.63473 | val_1_rmse: 0.63983 |  0:05:17s
epoch 53 | loss: 0.34322 | val_0_rmse: 0.62171 | val_1_rmse: 0.62198 |  0:05:23s
epoch 54 | loss: 0.34516 | val_0_rmse: 0.64466 | val_1_rmse: 0.64642 |  0:05:29s
epoch 55 | loss: 0.34651 | val_0_rmse: 0.65644 | val_1_rmse: 0.66148 |  0:05:35s
epoch 56 | loss: 0.34439 | val_0_rmse: 0.60922 | val_1_rmse: 0.61134 |  0:05:41s
epoch 57 | loss: 0.34152 | val_0_rmse: 0.60656 | val_1_rmse: 0.60857 |  0:05:47s
epoch 58 | loss: 0.34022 | val_0_rmse: 0.5898  | val_1_rmse: 0.59223 |  0:05:53s
epoch 59 | loss: 0.33978 | val_0_rmse: 0.57514 | val_1_rmse: 0.5776  |  0:05:59s
epoch 60 | loss: 0.34718 | val_0_rmse: 0.62144 | val_1_rmse: 0.62517 |  0:06:05s
epoch 61 | loss: 0.34347 | val_0_rmse: 0.59198 | val_1_rmse: 0.59492 |  0:06:11s
epoch 62 | loss: 0.33673 | val_0_rmse: 0.60826 | val_1_rmse: 0.61285 |  0:06:17s
epoch 63 | loss: 0.33986 | val_0_rmse: 0.79512 | val_1_rmse: 0.7995  |  0:06:23s
epoch 64 | loss: 0.33787 | val_0_rmse: 0.5848  | val_1_rmse: 0.58889 |  0:06:29s
epoch 65 | loss: 0.33635 | val_0_rmse: 0.61024 | val_1_rmse: 0.60977 |  0:06:35s
epoch 66 | loss: 0.34574 | val_0_rmse: 0.75043 | val_1_rmse: 0.75783 |  0:06:41s
epoch 67 | loss: 0.3444  | val_0_rmse: 0.57487 | val_1_rmse: 0.57683 |  0:06:47s
epoch 68 | loss: 0.33702 | val_0_rmse: 0.63901 | val_1_rmse: 0.64294 |  0:06:53s
epoch 69 | loss: 0.33668 | val_0_rmse: 0.6379  | val_1_rmse: 0.64373 |  0:06:59s
epoch 70 | loss: 0.33771 | val_0_rmse: 0.5747  | val_1_rmse: 0.57767 |  0:07:05s
epoch 71 | loss: 0.33561 | val_0_rmse: 0.59534 | val_1_rmse: 0.59477 |  0:07:11s
epoch 72 | loss: 0.33844 | val_0_rmse: 0.61758 | val_1_rmse: 0.62367 |  0:07:17s
epoch 73 | loss: 0.33491 | val_0_rmse: 0.59601 | val_1_rmse: 0.59864 |  0:07:23s
epoch 74 | loss: 0.33419 | val_0_rmse: 0.56975 | val_1_rmse: 0.57308 |  0:07:29s
epoch 75 | loss: 0.33691 | val_0_rmse: 0.57794 | val_1_rmse: 0.58124 |  0:07:35s
epoch 76 | loss: 0.33385 | val_0_rmse: 0.62401 | val_1_rmse: 0.62782 |  0:07:41s
epoch 77 | loss: 0.33676 | val_0_rmse: 0.6193  | val_1_rmse: 0.61918 |  0:07:47s
epoch 78 | loss: 0.3357  | val_0_rmse: 0.58781 | val_1_rmse: 0.59319 |  0:07:53s
epoch 79 | loss: 0.33554 | val_0_rmse: 0.61423 | val_1_rmse: 0.61704 |  0:07:59s
epoch 80 | loss: 0.33863 | val_0_rmse: 0.60733 | val_1_rmse: 0.61079 |  0:08:05s
epoch 81 | loss: 0.33322 | val_0_rmse: 0.58949 | val_1_rmse: 0.59042 |  0:08:11s
epoch 82 | loss: 0.33931 | val_0_rmse: 0.6302  | val_1_rmse: 0.62933 |  0:08:17s
epoch 83 | loss: 0.33716 | val_0_rmse: 0.69942 | val_1_rmse: 0.70121 |  0:08:23s
epoch 84 | loss: 0.33488 | val_0_rmse: 0.63595 | val_1_rmse: 0.64312 |  0:08:29s
epoch 85 | loss: 0.33275 | val_0_rmse: 0.61661 | val_1_rmse: 0.61978 |  0:08:35s
epoch 86 | loss: 0.33468 | val_0_rmse: 0.6437  | val_1_rmse: 0.64874 |  0:08:41s
epoch 87 | loss: 0.33684 | val_0_rmse: 0.70665 | val_1_rmse: 0.71204 |  0:08:47s
epoch 88 | loss: 0.33411 | val_0_rmse: 0.5823  | val_1_rmse: 0.58385 |  0:08:53s
epoch 89 | loss: 0.33293 | val_0_rmse: 0.57419 | val_1_rmse: 0.57557 |  0:08:59s
epoch 90 | loss: 0.33266 | val_0_rmse: 0.61607 | val_1_rmse: 0.61965 |  0:09:05s
epoch 91 | loss: 0.33194 | val_0_rmse: 0.71921 | val_1_rmse: 0.71762 |  0:09:11s
epoch 92 | loss: 0.33504 | val_0_rmse: 0.58601 | val_1_rmse: 0.58816 |  0:09:17s
epoch 93 | loss: 0.33273 | val_0_rmse: 0.62318 | val_1_rmse: 0.62699 |  0:09:23s
epoch 94 | loss: 0.34265 | val_0_rmse: 0.70925 | val_1_rmse: 0.71406 |  0:09:29s
epoch 95 | loss: 0.36168 | val_0_rmse: 0.5882  | val_1_rmse: 0.58822 |  0:09:35s
epoch 96 | loss: 0.38489 | val_0_rmse: 0.67762 | val_1_rmse: 0.67767 |  0:09:41s
epoch 97 | loss: 0.41222 | val_0_rmse: 0.66273 | val_1_rmse: 0.66823 |  0:09:47s
epoch 98 | loss: 0.37782 | val_0_rmse: 0.61834 | val_1_rmse: 0.62012 |  0:09:53s
epoch 99 | loss: 0.36656 | val_0_rmse: 0.61924 | val_1_rmse: 0.62069 |  0:09:59s
epoch 100| loss: 0.3634  | val_0_rmse: 0.60358 | val_1_rmse: 0.60342 |  0:10:05s
epoch 101| loss: 0.36151 | val_0_rmse: 0.61683 | val_1_rmse: 0.6197  |  0:10:11s
epoch 102| loss: 0.36362 | val_0_rmse: 0.61858 | val_1_rmse: 0.62113 |  0:10:17s
epoch 103| loss: 0.35624 | val_0_rmse: 0.59493 | val_1_rmse: 0.59413 |  0:10:23s
epoch 104| loss: 0.35677 | val_0_rmse: 0.64767 | val_1_rmse: 0.65047 |  0:10:29s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.57308
Best weights from best epoch are automatically used!
ended training at: 01:42:33
Feature importance:
[('Area', 0.19289134866021626), ('Baths', 0.2275459327751969), ('Beds', 0.0), ('Latitude', 0.4024521074069667), ('Longitude', 0.09431206978739817), ('Month', 0.08279854137022197), ('Year', 0.0)]
Mean squared error is of 2256164451.451714
Mean absolute error:34122.892568385665
MAPE:0.33457111816129614
R2 score:0.6721844259966354
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:42:33
epoch 0  | loss: 0.59862 | val_0_rmse: 0.75511 | val_1_rmse: 0.74965 |  0:00:05s
epoch 1  | loss: 0.49971 | val_0_rmse: 0.70059 | val_1_rmse: 0.69884 |  0:00:12s
epoch 2  | loss: 0.46664 | val_0_rmse: 0.69408 | val_1_rmse: 0.69225 |  0:00:18s
epoch 3  | loss: 0.44427 | val_0_rmse: 0.72182 | val_1_rmse: 0.72112 |  0:00:24s
epoch 4  | loss: 0.42277 | val_0_rmse: 0.6595  | val_1_rmse: 0.66105 |  0:00:30s
epoch 5  | loss: 0.41426 | val_0_rmse: 0.65015 | val_1_rmse: 0.65175 |  0:00:36s
epoch 6  | loss: 0.39034 | val_0_rmse: 0.62832 | val_1_rmse: 0.63222 |  0:00:42s
epoch 7  | loss: 0.37329 | val_0_rmse: 0.59616 | val_1_rmse: 0.59853 |  0:00:48s
epoch 8  | loss: 0.36378 | val_0_rmse: 0.59929 | val_1_rmse: 0.60284 |  0:00:54s
epoch 9  | loss: 0.35963 | val_0_rmse: 0.60412 | val_1_rmse: 0.60533 |  0:01:00s
epoch 10 | loss: 0.36607 | val_0_rmse: 0.63176 | val_1_rmse: 0.6316  |  0:01:06s
epoch 11 | loss: 0.35767 | val_0_rmse: 0.61819 | val_1_rmse: 0.61933 |  0:01:12s
epoch 12 | loss: 0.35504 | val_0_rmse: 0.61502 | val_1_rmse: 0.61451 |  0:01:18s
epoch 13 | loss: 0.34954 | val_0_rmse: 0.62798 | val_1_rmse: 0.63153 |  0:01:24s
epoch 14 | loss: 0.3486  | val_0_rmse: 0.71265 | val_1_rmse: 0.70927 |  0:01:30s
epoch 15 | loss: 0.34846 | val_0_rmse: 0.59516 | val_1_rmse: 0.59922 |  0:01:36s
epoch 16 | loss: 0.34696 | val_0_rmse: 0.58644 | val_1_rmse: 0.59028 |  0:01:42s
epoch 17 | loss: 0.34526 | val_0_rmse: 0.615   | val_1_rmse: 0.61791 |  0:01:48s
epoch 18 | loss: 0.34796 | val_0_rmse: 0.61181 | val_1_rmse: 0.6158  |  0:01:54s
epoch 19 | loss: 0.35136 | val_0_rmse: 0.64584 | val_1_rmse: 0.64545 |  0:02:00s
epoch 20 | loss: 0.36838 | val_0_rmse: 0.63654 | val_1_rmse: 0.63891 |  0:02:06s
epoch 21 | loss: 0.36391 | val_0_rmse: 0.58625 | val_1_rmse: 0.58794 |  0:02:12s
epoch 22 | loss: 0.36459 | val_0_rmse: 0.76479 | val_1_rmse: 0.76505 |  0:02:18s
epoch 23 | loss: 0.37698 | val_0_rmse: 0.6119  | val_1_rmse: 0.61208 |  0:02:24s
epoch 24 | loss: 0.35845 | val_0_rmse: 0.69487 | val_1_rmse: 0.69167 |  0:02:30s
epoch 25 | loss: 0.35463 | val_0_rmse: 0.62086 | val_1_rmse: 0.62551 |  0:02:36s
epoch 26 | loss: 0.35143 | val_0_rmse: 0.6009  | val_1_rmse: 0.60218 |  0:02:42s
epoch 27 | loss: 0.35264 | val_0_rmse: 0.60057 | val_1_rmse: 0.60412 |  0:02:48s
epoch 28 | loss: 0.35398 | val_0_rmse: 0.63532 | val_1_rmse: 0.63433 |  0:02:54s
epoch 29 | loss: 0.3488  | val_0_rmse: 0.69683 | val_1_rmse: 0.69198 |  0:03:00s
epoch 30 | loss: 0.36175 | val_0_rmse: 0.60404 | val_1_rmse: 0.60405 |  0:03:06s
epoch 31 | loss: 0.3539  | val_0_rmse: 0.62019 | val_1_rmse: 0.62043 |  0:03:12s
epoch 32 | loss: 0.35345 | val_0_rmse: 0.69226 | val_1_rmse: 0.68891 |  0:03:18s
epoch 33 | loss: 0.35781 | val_0_rmse: 0.61978 | val_1_rmse: 0.62244 |  0:03:24s
epoch 34 | loss: 0.35207 | val_0_rmse: 0.6706  | val_1_rmse: 0.668   |  0:03:30s
epoch 35 | loss: 0.35565 | val_0_rmse: 0.5891  | val_1_rmse: 0.58913 |  0:03:36s
epoch 36 | loss: 0.35467 | val_0_rmse: 0.63035 | val_1_rmse: 0.62948 |  0:03:42s
epoch 37 | loss: 0.35356 | val_0_rmse: 0.67468 | val_1_rmse: 0.66874 |  0:03:48s
epoch 38 | loss: 0.34787 | val_0_rmse: 0.59322 | val_1_rmse: 0.59257 |  0:03:54s
epoch 39 | loss: 0.34745 | val_0_rmse: 0.58805 | val_1_rmse: 0.59105 |  0:04:00s
epoch 40 | loss: 0.35192 | val_0_rmse: 0.60429 | val_1_rmse: 0.60219 |  0:04:06s
epoch 41 | loss: 0.35165 | val_0_rmse: 0.66535 | val_1_rmse: 0.66249 |  0:04:12s
epoch 42 | loss: 0.34358 | val_0_rmse: 0.59658 | val_1_rmse: 0.59498 |  0:04:18s
epoch 43 | loss: 0.34108 | val_0_rmse: 0.57933 | val_1_rmse: 0.57966 |  0:04:24s
epoch 44 | loss: 0.3478  | val_0_rmse: 0.65158 | val_1_rmse: 0.65132 |  0:04:30s
epoch 45 | loss: 0.35589 | val_0_rmse: 0.70966 | val_1_rmse: 0.70454 |  0:04:36s
epoch 46 | loss: 0.34804 | val_0_rmse: 0.60186 | val_1_rmse: 0.5999  |  0:04:42s
epoch 47 | loss: 0.34843 | val_0_rmse: 0.62536 | val_1_rmse: 0.6258  |  0:04:48s
epoch 48 | loss: 0.34928 | val_0_rmse: 0.6096  | val_1_rmse: 0.6078  |  0:04:54s
epoch 49 | loss: 0.34848 | val_0_rmse: 0.58091 | val_1_rmse: 0.58034 |  0:05:00s
epoch 50 | loss: 0.34628 | val_0_rmse: 0.60082 | val_1_rmse: 0.60384 |  0:05:06s
epoch 51 | loss: 0.34722 | val_0_rmse: 0.59501 | val_1_rmse: 0.59746 |  0:05:12s
epoch 52 | loss: 0.34591 | val_0_rmse: 0.65037 | val_1_rmse: 0.64857 |  0:05:18s
epoch 53 | loss: 0.34457 | val_0_rmse: 0.60613 | val_1_rmse: 0.60659 |  0:05:24s
epoch 54 | loss: 0.34467 | val_0_rmse: 0.59802 | val_1_rmse: 0.60041 |  0:05:30s
epoch 55 | loss: 0.34365 | val_0_rmse: 0.60251 | val_1_rmse: 0.60369 |  0:05:36s
epoch 56 | loss: 0.34105 | val_0_rmse: 0.68045 | val_1_rmse: 0.6817  |  0:05:42s
epoch 57 | loss: 0.34215 | val_0_rmse: 0.5979  | val_1_rmse: 0.59722 |  0:05:48s
epoch 58 | loss: 0.34091 | val_0_rmse: 0.59689 | val_1_rmse: 0.59729 |  0:05:54s
epoch 59 | loss: 0.34248 | val_0_rmse: 0.70303 | val_1_rmse: 0.70047 |  0:06:00s
epoch 60 | loss: 0.34021 | val_0_rmse: 0.62284 | val_1_rmse: 0.62655 |  0:06:06s
epoch 61 | loss: 0.33829 | val_0_rmse: 0.62627 | val_1_rmse: 0.625   |  0:06:12s
epoch 62 | loss: 0.33647 | val_0_rmse: 0.59563 | val_1_rmse: 0.60016 |  0:06:18s
epoch 63 | loss: 0.33774 | val_0_rmse: 0.57596 | val_1_rmse: 0.57645 |  0:06:24s
epoch 64 | loss: 0.33608 | val_0_rmse: 0.59557 | val_1_rmse: 0.59844 |  0:06:30s
epoch 65 | loss: 0.33868 | val_0_rmse: 0.67662 | val_1_rmse: 0.67344 |  0:06:36s
epoch 66 | loss: 0.33298 | val_0_rmse: 0.57311 | val_1_rmse: 0.57532 |  0:06:42s
epoch 67 | loss: 0.33725 | val_0_rmse: 0.64549 | val_1_rmse: 0.64443 |  0:06:48s
epoch 68 | loss: 0.33913 | val_0_rmse: 0.57029 | val_1_rmse: 0.57273 |  0:06:54s
epoch 69 | loss: 0.33931 | val_0_rmse: 0.61454 | val_1_rmse: 0.61672 |  0:07:00s
epoch 70 | loss: 0.33852 | val_0_rmse: 0.61457 | val_1_rmse: 0.61666 |  0:07:06s
epoch 71 | loss: 0.33448 | val_0_rmse: 0.5827  | val_1_rmse: 0.58256 |  0:07:12s
epoch 72 | loss: 0.33675 | val_0_rmse: 0.66031 | val_1_rmse: 0.65743 |  0:07:18s
epoch 73 | loss: 0.33757 | val_0_rmse: 0.63311 | val_1_rmse: 0.63679 |  0:07:24s
epoch 74 | loss: 0.33609 | val_0_rmse: 0.61787 | val_1_rmse: 0.62105 |  0:07:30s
epoch 75 | loss: 0.33476 | val_0_rmse: 0.57804 | val_1_rmse: 0.58052 |  0:07:36s
epoch 76 | loss: 0.33191 | val_0_rmse: 0.62765 | val_1_rmse: 0.63118 |  0:07:42s
epoch 77 | loss: 0.33404 | val_0_rmse: 0.6163  | val_1_rmse: 0.61895 |  0:07:48s
epoch 78 | loss: 0.33598 | val_0_rmse: 0.63439 | val_1_rmse: 0.63513 |  0:07:54s
epoch 79 | loss: 0.33315 | val_0_rmse: 0.62707 | val_1_rmse: 0.62885 |  0:08:00s
epoch 80 | loss: 0.33139 | val_0_rmse: 0.67356 | val_1_rmse: 0.6705  |  0:08:06s
epoch 81 | loss: 0.33837 | val_0_rmse: 0.603   | val_1_rmse: 0.60444 |  0:08:12s
epoch 82 | loss: 0.33415 | val_0_rmse: 0.5754  | val_1_rmse: 0.57808 |  0:08:18s
epoch 83 | loss: 0.33484 | val_0_rmse: 0.66169 | val_1_rmse: 0.65778 |  0:08:24s
epoch 84 | loss: 0.33475 | val_0_rmse: 0.72556 | val_1_rmse: 0.72439 |  0:08:30s
epoch 85 | loss: 0.3333  | val_0_rmse: 0.62714 | val_1_rmse: 0.6242  |  0:08:36s
epoch 86 | loss: 0.33642 | val_0_rmse: 0.66751 | val_1_rmse: 0.66666 |  0:08:42s
epoch 87 | loss: 0.33232 | val_0_rmse: 0.6726  | val_1_rmse: 0.66997 |  0:08:48s
epoch 88 | loss: 0.33473 | val_0_rmse: 0.65013 | val_1_rmse: 0.64944 |  0:08:54s
epoch 89 | loss: 0.3305  | val_0_rmse: 0.56198 | val_1_rmse: 0.56298 |  0:09:00s
epoch 90 | loss: 0.33074 | val_0_rmse: 0.60199 | val_1_rmse: 0.60117 |  0:09:06s
epoch 91 | loss: 0.33065 | val_0_rmse: 0.81647 | val_1_rmse: 0.81561 |  0:09:12s
epoch 92 | loss: 0.33047 | val_0_rmse: 0.77632 | val_1_rmse: 0.77734 |  0:09:17s
epoch 93 | loss: 0.32921 | val_0_rmse: 0.60431 | val_1_rmse: 0.60749 |  0:09:23s
epoch 94 | loss: 0.33022 | val_0_rmse: 0.86874 | val_1_rmse: 0.87092 |  0:09:29s
epoch 95 | loss: 0.33143 | val_0_rmse: 0.62446 | val_1_rmse: 0.62654 |  0:09:35s
epoch 96 | loss: 0.32867 | val_0_rmse: 0.84175 | val_1_rmse: 0.84413 |  0:09:41s
epoch 97 | loss: 0.32847 | val_0_rmse: 0.66106 | val_1_rmse: 0.66281 |  0:09:47s
epoch 98 | loss: 0.32825 | val_0_rmse: 0.57064 | val_1_rmse: 0.57366 |  0:09:53s
epoch 99 | loss: 0.33021 | val_0_rmse: 0.5703  | val_1_rmse: 0.5737  |  0:09:59s
epoch 100| loss: 0.33011 | val_0_rmse: 0.57411 | val_1_rmse: 0.57751 |  0:10:05s
epoch 101| loss: 0.33027 | val_0_rmse: 0.60285 | val_1_rmse: 0.60202 |  0:10:11s
epoch 102| loss: 0.32936 | val_0_rmse: 0.72088 | val_1_rmse: 0.71947 |  0:10:17s
epoch 103| loss: 0.33201 | val_0_rmse: 0.59143 | val_1_rmse: 0.59439 |  0:10:24s
epoch 104| loss: 0.33349 | val_0_rmse: 0.61112 | val_1_rmse: 0.61146 |  0:10:30s
epoch 105| loss: 0.3313  | val_0_rmse: 0.57931 | val_1_rmse: 0.5833  |  0:10:37s
epoch 106| loss: 0.33088 | val_0_rmse: 0.73737 | val_1_rmse: 0.74108 |  0:10:44s
epoch 107| loss: 0.3312  | val_0_rmse: 0.68753 | val_1_rmse: 0.68504 |  0:10:50s
epoch 108| loss: 0.33194 | val_0_rmse: 0.58123 | val_1_rmse: 0.58268 |  0:10:56s
epoch 109| loss: 0.32979 | val_0_rmse: 0.60951 | val_1_rmse: 0.61152 |  0:11:02s
epoch 110| loss: 0.3341  | val_0_rmse: 0.62752 | val_1_rmse: 0.62578 |  0:11:08s
epoch 111| loss: 0.33282 | val_0_rmse: 0.58027 | val_1_rmse: 0.58241 |  0:11:15s
epoch 112| loss: 0.33325 | val_0_rmse: 0.59781 | val_1_rmse: 0.60109 |  0:11:21s
epoch 113| loss: 0.3319  | val_0_rmse: 0.58222 | val_1_rmse: 0.5834  |  0:11:27s
epoch 114| loss: 0.3325  | val_0_rmse: 0.61707 | val_1_rmse: 0.61849 |  0:11:33s
epoch 115| loss: 0.32725 | val_0_rmse: 0.72374 | val_1_rmse: 0.72634 |  0:11:39s
epoch 116| loss: 0.32836 | val_0_rmse: 0.72369 | val_1_rmse: 0.72618 |  0:11:45s
epoch 117| loss: 0.32808 | val_0_rmse: 0.67214 | val_1_rmse: 0.66924 |  0:11:51s
epoch 118| loss: 0.32891 | val_0_rmse: 0.62175 | val_1_rmse: 0.62079 |  0:11:57s
epoch 119| loss: 0.3393  | val_0_rmse: 0.58798 | val_1_rmse: 0.58944 |  0:12:03s

Early stopping occured at epoch 119 with best_epoch = 89 and best_val_1_rmse = 0.56298
Best weights from best epoch are automatically used!
ended training at: 01:54:38
Feature importance:
[('Area', 0.18301545957549287), ('Baths', 0.17958274862804982), ('Beds', 0.02294303793198628), ('Latitude', 0.2784749982619178), ('Longitude', 0.3359837556025533), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2159594598.0820165
Mean absolute error:33225.67342695783
MAPE:0.31929550251198646
R2 score:0.6846627180631376
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:54:39
epoch 0  | loss: 0.59419 | val_0_rmse: 0.72306 | val_1_rmse: 0.72708 |  0:00:06s
epoch 1  | loss: 0.47269 | val_0_rmse: 0.68412 | val_1_rmse: 0.69194 |  0:00:12s
epoch 2  | loss: 0.41287 | val_0_rmse: 0.64756 | val_1_rmse: 0.65558 |  0:00:18s
epoch 3  | loss: 0.39324 | val_0_rmse: 0.62972 | val_1_rmse: 0.63879 |  0:00:24s
epoch 4  | loss: 0.38316 | val_0_rmse: 0.68265 | val_1_rmse: 0.69072 |  0:00:30s
epoch 5  | loss: 0.37286 | val_0_rmse: 0.62859 | val_1_rmse: 0.63787 |  0:00:36s
epoch 6  | loss: 0.37221 | val_0_rmse: 0.68632 | val_1_rmse: 0.69375 |  0:00:42s
epoch 7  | loss: 0.36844 | val_0_rmse: 0.64687 | val_1_rmse: 0.65727 |  0:00:48s
epoch 8  | loss: 0.37024 | val_0_rmse: 0.69796 | val_1_rmse: 0.70678 |  0:00:54s
epoch 9  | loss: 0.36859 | val_0_rmse: 1.08684 | val_1_rmse: 1.08748 |  0:01:00s
epoch 10 | loss: 0.36595 | val_0_rmse: 0.66777 | val_1_rmse: 0.677   |  0:01:06s
epoch 11 | loss: 0.36384 | val_0_rmse: 0.64596 | val_1_rmse: 0.65555 |  0:01:12s
epoch 12 | loss: 0.36441 | val_0_rmse: 0.7771  | val_1_rmse: 0.78243 |  0:01:18s
epoch 13 | loss: 0.3737  | val_0_rmse: 0.66132 | val_1_rmse: 0.67368 |  0:01:24s
epoch 14 | loss: 0.36053 | val_0_rmse: 0.6238  | val_1_rmse: 0.6375  |  0:01:30s
epoch 15 | loss: 0.36559 | val_0_rmse: 0.64889 | val_1_rmse: 0.66049 |  0:01:36s
epoch 16 | loss: 0.366   | val_0_rmse: 0.97374 | val_1_rmse: 0.97824 |  0:01:43s
epoch 17 | loss: 0.35723 | val_0_rmse: 0.92605 | val_1_rmse: 0.93118 |  0:01:49s
epoch 18 | loss: 0.35682 | val_0_rmse: 0.63193 | val_1_rmse: 0.64493 |  0:01:55s
epoch 19 | loss: 0.3582  | val_0_rmse: 0.6798  | val_1_rmse: 0.69221 |  0:02:01s
epoch 20 | loss: 0.35876 | val_0_rmse: 0.68019 | val_1_rmse: 0.68524 |  0:02:07s
epoch 21 | loss: 0.36002 | val_0_rmse: 0.66377 | val_1_rmse: 0.66618 |  0:02:13s
epoch 22 | loss: 0.35534 | val_0_rmse: 0.61691 | val_1_rmse: 0.62735 |  0:02:19s
epoch 23 | loss: 0.35832 | val_0_rmse: 0.61376 | val_1_rmse: 0.62366 |  0:02:25s
epoch 24 | loss: 0.35364 | val_0_rmse: 0.60114 | val_1_rmse: 0.61135 |  0:02:31s
epoch 25 | loss: 0.35747 | val_0_rmse: 0.99074 | val_1_rmse: 0.99212 |  0:02:37s
epoch 26 | loss: 0.35887 | val_0_rmse: 0.75166 | val_1_rmse: 0.75695 |  0:02:43s
epoch 27 | loss: 0.35368 | val_0_rmse: 1.52348 | val_1_rmse: 1.51736 |  0:02:49s
epoch 28 | loss: 0.35703 | val_0_rmse: 0.61868 | val_1_rmse: 0.62474 |  0:02:55s
epoch 29 | loss: 0.3556  | val_0_rmse: 0.67633 | val_1_rmse: 0.68567 |  0:03:01s
epoch 30 | loss: 0.3525  | val_0_rmse: 0.98716 | val_1_rmse: 0.99079 |  0:03:07s
epoch 31 | loss: 0.35404 | val_0_rmse: 0.78769 | val_1_rmse: 0.79619 |  0:03:13s
epoch 32 | loss: 0.3512  | val_0_rmse: 0.68764 | val_1_rmse: 0.69722 |  0:03:20s
epoch 33 | loss: 0.35192 | val_0_rmse: 0.70424 | val_1_rmse: 0.71121 |  0:03:26s
epoch 34 | loss: 0.35034 | val_0_rmse: 0.70812 | val_1_rmse: 0.71132 |  0:03:32s
epoch 35 | loss: 0.34928 | val_0_rmse: 0.76177 | val_1_rmse: 0.77303 |  0:03:38s
epoch 36 | loss: 0.35217 | val_0_rmse: 1.02691 | val_1_rmse: 1.02985 |  0:03:44s
epoch 37 | loss: 0.3479  | val_0_rmse: 1.28936 | val_1_rmse: 1.29101 |  0:03:50s
epoch 38 | loss: 0.34792 | val_0_rmse: 0.66357 | val_1_rmse: 0.67845 |  0:03:56s
epoch 39 | loss: 0.34911 | val_0_rmse: 0.81426 | val_1_rmse: 0.82146 |  0:04:02s
epoch 40 | loss: 0.34657 | val_0_rmse: 0.86264 | val_1_rmse: 0.86687 |  0:04:08s
epoch 41 | loss: 0.34592 | val_0_rmse: 0.62087 | val_1_rmse: 0.63335 |  0:04:14s
epoch 42 | loss: 0.34794 | val_0_rmse: 0.76809 | val_1_rmse: 0.77766 |  0:04:20s
epoch 43 | loss: 0.34835 | val_0_rmse: 0.8075  | val_1_rmse: 0.81599 |  0:04:26s
epoch 44 | loss: 0.34607 | val_0_rmse: 0.67222 | val_1_rmse: 0.68719 |  0:04:32s
epoch 45 | loss: 0.34677 | val_0_rmse: 0.59036 | val_1_rmse: 0.60161 |  0:04:38s
epoch 46 | loss: 0.34615 | val_0_rmse: 0.69433 | val_1_rmse: 0.70111 |  0:04:44s
epoch 47 | loss: 0.34677 | val_0_rmse: 0.5975  | val_1_rmse: 0.60817 |  0:04:50s
epoch 48 | loss: 0.3452  | val_0_rmse: 0.70975 | val_1_rmse: 0.71431 |  0:04:57s
epoch 49 | loss: 0.34682 | val_0_rmse: 0.79774 | val_1_rmse: 0.80559 |  0:05:03s
epoch 50 | loss: 0.34768 | val_0_rmse: 0.67209 | val_1_rmse: 0.68379 |  0:05:09s
epoch 51 | loss: 0.34625 | val_0_rmse: 0.99405 | val_1_rmse: 0.99968 |  0:05:15s
epoch 52 | loss: 0.34649 | val_0_rmse: 0.67245 | val_1_rmse: 0.68017 |  0:05:21s
epoch 53 | loss: 0.34586 | val_0_rmse: 0.67536 | val_1_rmse: 0.68926 |  0:05:27s
epoch 54 | loss: 0.34509 | val_0_rmse: 0.62361 | val_1_rmse: 0.63693 |  0:05:33s
epoch 55 | loss: 0.34377 | val_0_rmse: 0.69525 | val_1_rmse: 0.70588 |  0:05:39s
epoch 56 | loss: 0.34704 | val_0_rmse: 0.91996 | val_1_rmse: 0.92558 |  0:05:45s
epoch 57 | loss: 0.34442 | val_0_rmse: 0.60974 | val_1_rmse: 0.6176  |  0:05:51s
epoch 58 | loss: 0.3409  | val_0_rmse: 0.76723 | val_1_rmse: 0.77718 |  0:05:57s
epoch 59 | loss: 0.34213 | val_0_rmse: 0.65813 | val_1_rmse: 0.67093 |  0:06:03s
epoch 60 | loss: 0.34354 | val_0_rmse: 1.13222 | val_1_rmse: 1.13404 |  0:06:09s
epoch 61 | loss: 0.34382 | val_0_rmse: 0.73911 | val_1_rmse: 0.74547 |  0:06:15s
epoch 62 | loss: 0.34344 | val_0_rmse: 0.81326 | val_1_rmse: 0.82316 |  0:06:21s
epoch 63 | loss: 0.36826 | val_0_rmse: 0.70328 | val_1_rmse: 0.7151  |  0:06:27s
epoch 64 | loss: 0.37598 | val_0_rmse: 0.77229 | val_1_rmse: 0.78032 |  0:06:33s
epoch 65 | loss: 0.36005 | val_0_rmse: 0.65589 | val_1_rmse: 0.66828 |  0:06:39s
epoch 66 | loss: 0.35613 | val_0_rmse: 0.59191 | val_1_rmse: 0.60276 |  0:06:45s
epoch 67 | loss: 0.35409 | val_0_rmse: 0.59198 | val_1_rmse: 0.60436 |  0:06:51s
epoch 68 | loss: 0.35346 | val_0_rmse: 0.67919 | val_1_rmse: 0.68822 |  0:06:57s
epoch 69 | loss: 0.36025 | val_0_rmse: 0.67127 | val_1_rmse: 0.67839 |  0:07:03s
epoch 70 | loss: 0.35793 | val_0_rmse: 0.6002  | val_1_rmse: 0.6119  |  0:07:09s
epoch 71 | loss: 0.35299 | val_0_rmse: 0.62515 | val_1_rmse: 0.63457 |  0:07:15s
epoch 72 | loss: 0.34913 | val_0_rmse: 0.64827 | val_1_rmse: 0.65331 |  0:07:21s
epoch 73 | loss: 0.35432 | val_0_rmse: 0.7399  | val_1_rmse: 0.75053 |  0:07:27s
epoch 74 | loss: 0.35287 | val_0_rmse: 0.8583  | val_1_rmse: 0.86261 |  0:07:33s
epoch 75 | loss: 0.3542  | val_0_rmse: 0.75749 | val_1_rmse: 0.76248 |  0:07:40s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.60161
Best weights from best epoch are automatically used!
ended training at: 02:02:21
Feature importance:
[('Area', 0.285232346061746), ('Baths', 0.1949667290643135), ('Beds', 0.0), ('Latitude', 0.3455530115181998), ('Longitude', 0.1742479133557407), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2403696359.473044
Mean absolute error:35088.414531222086
MAPE:0.3338813540407646
R2 score:0.6516314720927101
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:02:21
epoch 0  | loss: 0.60113 | val_0_rmse: 0.64547 | val_1_rmse: 0.64316 |  0:00:02s
epoch 1  | loss: 0.35986 | val_0_rmse: 0.56765 | val_1_rmse: 0.56014 |  0:00:04s
epoch 2  | loss: 0.33254 | val_0_rmse: 0.55966 | val_1_rmse: 0.55316 |  0:00:06s
epoch 3  | loss: 0.31931 | val_0_rmse: 0.54682 | val_1_rmse: 0.53891 |  0:00:08s
epoch 4  | loss: 0.30745 | val_0_rmse: 0.54683 | val_1_rmse: 0.54362 |  0:00:10s
epoch 5  | loss: 0.30574 | val_0_rmse: 0.54085 | val_1_rmse: 0.53607 |  0:00:13s
epoch 6  | loss: 0.30185 | val_0_rmse: 0.53786 | val_1_rmse: 0.53408 |  0:00:15s
epoch 7  | loss: 0.29934 | val_0_rmse: 0.54017 | val_1_rmse: 0.53611 |  0:00:17s
epoch 8  | loss: 0.29827 | val_0_rmse: 0.54247 | val_1_rmse: 0.54155 |  0:00:19s
epoch 9  | loss: 0.29431 | val_0_rmse: 0.53781 | val_1_rmse: 0.53321 |  0:00:22s
epoch 10 | loss: 0.29942 | val_0_rmse: 0.53281 | val_1_rmse: 0.5249  |  0:00:24s
epoch 11 | loss: 0.29467 | val_0_rmse: 0.53871 | val_1_rmse: 0.5356  |  0:00:26s
epoch 12 | loss: 0.29054 | val_0_rmse: 0.55528 | val_1_rmse: 0.55527 |  0:00:28s
epoch 13 | loss: 0.29151 | val_0_rmse: 0.5229  | val_1_rmse: 0.52118 |  0:00:30s
epoch 14 | loss: 0.28423 | val_0_rmse: 0.54425 | val_1_rmse: 0.53974 |  0:00:33s
epoch 15 | loss: 0.29397 | val_0_rmse: 0.55084 | val_1_rmse: 0.546   |  0:00:35s
epoch 16 | loss: 0.29842 | val_0_rmse: 0.54062 | val_1_rmse: 0.53586 |  0:00:37s
epoch 17 | loss: 0.30548 | val_0_rmse: 0.56183 | val_1_rmse: 0.55549 |  0:00:39s
epoch 18 | loss: 0.28872 | val_0_rmse: 0.55427 | val_1_rmse: 0.54853 |  0:00:41s
epoch 19 | loss: 0.27845 | val_0_rmse: 0.51834 | val_1_rmse: 0.5146  |  0:00:43s
epoch 20 | loss: 0.29098 | val_0_rmse: 0.61558 | val_1_rmse: 0.61698 |  0:00:46s
epoch 21 | loss: 0.28393 | val_0_rmse: 0.51862 | val_1_rmse: 0.51345 |  0:00:48s
epoch 22 | loss: 0.27751 | val_0_rmse: 0.56378 | val_1_rmse: 0.55787 |  0:00:50s
epoch 23 | loss: 0.27584 | val_0_rmse: 0.51769 | val_1_rmse: 0.51384 |  0:00:52s
epoch 24 | loss: 0.26925 | val_0_rmse: 0.50743 | val_1_rmse: 0.50496 |  0:00:55s
epoch 25 | loss: 0.27722 | val_0_rmse: 0.50409 | val_1_rmse: 0.50081 |  0:00:57s
epoch 26 | loss: 0.27965 | val_0_rmse: 0.5254  | val_1_rmse: 0.51959 |  0:00:59s
epoch 27 | loss: 0.27941 | val_0_rmse: 0.53514 | val_1_rmse: 0.52954 |  0:01:01s
epoch 28 | loss: 0.27126 | val_0_rmse: 0.50187 | val_1_rmse: 0.49559 |  0:01:03s
epoch 29 | loss: 0.26836 | val_0_rmse: 0.50612 | val_1_rmse: 0.50107 |  0:01:06s
epoch 30 | loss: 0.26612 | val_0_rmse: 0.57841 | val_1_rmse: 0.57429 |  0:01:08s
epoch 31 | loss: 0.2638  | val_0_rmse: 0.50099 | val_1_rmse: 0.49743 |  0:01:10s
epoch 32 | loss: 0.25983 | val_0_rmse: 0.49741 | val_1_rmse: 0.49384 |  0:01:12s
epoch 33 | loss: 0.26299 | val_0_rmse: 0.49996 | val_1_rmse: 0.49667 |  0:01:14s
epoch 34 | loss: 0.26154 | val_0_rmse: 0.49836 | val_1_rmse: 0.49621 |  0:01:17s
epoch 35 | loss: 0.2608  | val_0_rmse: 0.50515 | val_1_rmse: 0.50496 |  0:01:19s
epoch 36 | loss: 0.25779 | val_0_rmse: 0.50859 | val_1_rmse: 0.50767 |  0:01:21s
epoch 37 | loss: 0.25886 | val_0_rmse: 0.53455 | val_1_rmse: 0.53205 |  0:01:23s
epoch 38 | loss: 0.25995 | val_0_rmse: 0.50906 | val_1_rmse: 0.5072  |  0:01:25s
epoch 39 | loss: 0.26444 | val_0_rmse: 0.56667 | val_1_rmse: 0.56388 |  0:01:28s
epoch 40 | loss: 0.25658 | val_0_rmse: 0.49687 | val_1_rmse: 0.49844 |  0:01:30s
epoch 41 | loss: 0.26093 | val_0_rmse: 0.50671 | val_1_rmse: 0.5075  |  0:01:32s
epoch 42 | loss: 0.25756 | val_0_rmse: 0.498   | val_1_rmse: 0.49821 |  0:01:34s
epoch 43 | loss: 0.254   | val_0_rmse: 0.51828 | val_1_rmse: 0.51921 |  0:01:36s
epoch 44 | loss: 0.26126 | val_0_rmse: 0.55195 | val_1_rmse: 0.54875 |  0:01:39s
epoch 45 | loss: 0.25987 | val_0_rmse: 0.55469 | val_1_rmse: 0.55285 |  0:01:41s
epoch 46 | loss: 0.25417 | val_0_rmse: 0.55256 | val_1_rmse: 0.55629 |  0:01:43s
epoch 47 | loss: 0.25532 | val_0_rmse: 0.49632 | val_1_rmse: 0.49618 |  0:01:45s
epoch 48 | loss: 0.25288 | val_0_rmse: 0.501   | val_1_rmse: 0.49881 |  0:01:47s
epoch 49 | loss: 0.26043 | val_0_rmse: 0.50756 | val_1_rmse: 0.50638 |  0:01:50s
epoch 50 | loss: 0.25653 | val_0_rmse: 0.48337 | val_1_rmse: 0.48405 |  0:01:52s
epoch 51 | loss: 0.24956 | val_0_rmse: 0.50243 | val_1_rmse: 0.5025  |  0:01:54s
epoch 52 | loss: 0.25243 | val_0_rmse: 0.49374 | val_1_rmse: 0.49539 |  0:01:56s
epoch 53 | loss: 0.25378 | val_0_rmse: 0.50171 | val_1_rmse: 0.50012 |  0:01:59s
epoch 54 | loss: 0.25343 | val_0_rmse: 0.49029 | val_1_rmse: 0.48979 |  0:02:01s
epoch 55 | loss: 0.2535  | val_0_rmse: 0.50531 | val_1_rmse: 0.50753 |  0:02:03s
epoch 56 | loss: 0.25071 | val_0_rmse: 0.4953  | val_1_rmse: 0.49918 |  0:02:05s
epoch 57 | loss: 0.25038 | val_0_rmse: 0.57364 | val_1_rmse: 0.57254 |  0:02:07s
epoch 58 | loss: 0.25066 | val_0_rmse: 0.53947 | val_1_rmse: 0.53973 |  0:02:10s
epoch 59 | loss: 0.25507 | val_0_rmse: 0.55306 | val_1_rmse: 0.55237 |  0:02:12s
epoch 60 | loss: 0.24979 | val_0_rmse: 0.49747 | val_1_rmse: 0.49941 |  0:02:14s
epoch 61 | loss: 0.24938 | val_0_rmse: 0.49788 | val_1_rmse: 0.49688 |  0:02:16s
epoch 62 | loss: 0.24873 | val_0_rmse: 0.49316 | val_1_rmse: 0.49372 |  0:02:18s
epoch 63 | loss: 0.24804 | val_0_rmse: 0.51043 | val_1_rmse: 0.51104 |  0:02:21s
epoch 64 | loss: 0.25229 | val_0_rmse: 0.50291 | val_1_rmse: 0.50607 |  0:02:23s
epoch 65 | loss: 0.24955 | val_0_rmse: 0.49841 | val_1_rmse: 0.4989  |  0:02:25s
epoch 66 | loss: 0.24999 | val_0_rmse: 0.55235 | val_1_rmse: 0.55059 |  0:02:27s
epoch 67 | loss: 0.25372 | val_0_rmse: 0.612   | val_1_rmse: 0.6093  |  0:02:29s
epoch 68 | loss: 0.25899 | val_0_rmse: 0.53334 | val_1_rmse: 0.53614 |  0:02:32s
epoch 69 | loss: 0.25505 | val_0_rmse: 0.54617 | val_1_rmse: 0.5525  |  0:02:34s
epoch 70 | loss: 0.26379 | val_0_rmse: 0.54558 | val_1_rmse: 0.54235 |  0:02:36s
epoch 71 | loss: 0.26516 | val_0_rmse: 0.50943 | val_1_rmse: 0.51286 |  0:02:38s
epoch 72 | loss: 0.25831 | val_0_rmse: 0.51073 | val_1_rmse: 0.51398 |  0:02:41s
epoch 73 | loss: 0.25604 | val_0_rmse: 0.51398 | val_1_rmse: 0.51267 |  0:02:43s
epoch 74 | loss: 0.2563  | val_0_rmse: 0.51011 | val_1_rmse: 0.5155  |  0:02:45s
epoch 75 | loss: 0.25548 | val_0_rmse: 0.49519 | val_1_rmse: 0.49685 |  0:02:47s
epoch 76 | loss: 0.25693 | val_0_rmse: 0.50087 | val_1_rmse: 0.50027 |  0:02:49s
epoch 77 | loss: 0.25495 | val_0_rmse: 0.49179 | val_1_rmse: 0.49185 |  0:02:52s
epoch 78 | loss: 0.25123 | val_0_rmse: 0.54899 | val_1_rmse: 0.55073 |  0:02:54s
epoch 79 | loss: 0.25071 | val_0_rmse: 0.50167 | val_1_rmse: 0.50336 |  0:02:56s
epoch 80 | loss: 0.25306 | val_0_rmse: 0.49567 | val_1_rmse: 0.5016  |  0:02:58s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.48405
Best weights from best epoch are automatically used!
ended training at: 02:05:21
Feature importance:
[('Area', 0.3616298731894538), ('Baths', 0.09473614096970273), ('Beds', 0.03535541078514997), ('Latitude', 0.14991605540656686), ('Longitude', 0.2838101072347357), ('Month', 0.022358108464434357), ('Year', 0.05219430394995661)]
Mean squared error is of 982185585.3481064
Mean absolute error:21421.19707812793
MAPE:0.2722337071107852
R2 score:0.7500639761381951
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:05:21
epoch 0  | loss: 0.55503 | val_0_rmse: 0.60513 | val_1_rmse: 0.60211 |  0:00:02s
epoch 1  | loss: 0.3475  | val_0_rmse: 0.56946 | val_1_rmse: 0.5671  |  0:00:04s
epoch 2  | loss: 0.32343 | val_0_rmse: 0.56566 | val_1_rmse: 0.56387 |  0:00:06s
epoch 3  | loss: 0.3188  | val_0_rmse: 0.55133 | val_1_rmse: 0.55032 |  0:00:08s
epoch 4  | loss: 0.3103  | val_0_rmse: 0.57079 | val_1_rmse: 0.56854 |  0:00:11s
epoch 5  | loss: 0.3003  | val_0_rmse: 0.53174 | val_1_rmse: 0.52989 |  0:00:13s
epoch 6  | loss: 0.29293 | val_0_rmse: 0.52656 | val_1_rmse: 0.52297 |  0:00:15s
epoch 7  | loss: 0.29675 | val_0_rmse: 0.5265  | val_1_rmse: 0.52158 |  0:00:17s
epoch 8  | loss: 0.29658 | val_0_rmse: 0.53617 | val_1_rmse: 0.53832 |  0:00:20s
epoch 9  | loss: 0.2891  | val_0_rmse: 0.55685 | val_1_rmse: 0.55342 |  0:00:22s
epoch 10 | loss: 0.29073 | val_0_rmse: 0.5262  | val_1_rmse: 0.52369 |  0:00:24s
epoch 11 | loss: 0.28719 | val_0_rmse: 0.56295 | val_1_rmse: 0.56123 |  0:00:26s
epoch 12 | loss: 0.27652 | val_0_rmse: 0.53834 | val_1_rmse: 0.53738 |  0:00:28s
epoch 13 | loss: 0.27966 | val_0_rmse: 0.51715 | val_1_rmse: 0.51795 |  0:00:31s
epoch 14 | loss: 0.28451 | val_0_rmse: 0.57593 | val_1_rmse: 0.57973 |  0:00:33s
epoch 15 | loss: 0.28147 | val_0_rmse: 0.52194 | val_1_rmse: 0.52127 |  0:00:35s
epoch 16 | loss: 0.27659 | val_0_rmse: 0.60971 | val_1_rmse: 0.60867 |  0:00:37s
epoch 17 | loss: 0.27346 | val_0_rmse: 0.53005 | val_1_rmse: 0.53005 |  0:00:40s
epoch 18 | loss: 0.27618 | val_0_rmse: 0.52026 | val_1_rmse: 0.52081 |  0:00:42s
epoch 19 | loss: 0.27416 | val_0_rmse: 0.51215 | val_1_rmse: 0.51072 |  0:00:44s
epoch 20 | loss: 0.26828 | val_0_rmse: 0.54681 | val_1_rmse: 0.54487 |  0:00:46s
epoch 21 | loss: 0.27145 | val_0_rmse: 0.56506 | val_1_rmse: 0.56758 |  0:00:48s
epoch 22 | loss: 0.2689  | val_0_rmse: 0.50857 | val_1_rmse: 0.50819 |  0:00:51s
epoch 23 | loss: 0.27104 | val_0_rmse: 0.50706 | val_1_rmse: 0.50981 |  0:00:53s
epoch 24 | loss: 0.26422 | val_0_rmse: 0.50931 | val_1_rmse: 0.50945 |  0:00:55s
epoch 25 | loss: 0.26988 | val_0_rmse: 0.5342  | val_1_rmse: 0.53472 |  0:00:57s
epoch 26 | loss: 0.26962 | val_0_rmse: 0.51474 | val_1_rmse: 0.51317 |  0:01:00s
epoch 27 | loss: 0.26394 | val_0_rmse: 0.56522 | val_1_rmse: 0.56477 |  0:01:02s
epoch 28 | loss: 0.25842 | val_0_rmse: 0.50099 | val_1_rmse: 0.50291 |  0:01:04s
epoch 29 | loss: 0.26593 | val_0_rmse: 0.54017 | val_1_rmse: 0.54031 |  0:01:06s
epoch 30 | loss: 0.26841 | val_0_rmse: 0.51808 | val_1_rmse: 0.51892 |  0:01:08s
epoch 31 | loss: 0.26332 | val_0_rmse: 0.52885 | val_1_rmse: 0.52709 |  0:01:10s
epoch 32 | loss: 0.25679 | val_0_rmse: 0.50476 | val_1_rmse: 0.50512 |  0:01:13s
epoch 33 | loss: 0.259   | val_0_rmse: 0.53034 | val_1_rmse: 0.53084 |  0:01:15s
epoch 34 | loss: 0.25518 | val_0_rmse: 0.49692 | val_1_rmse: 0.49756 |  0:01:17s
epoch 35 | loss: 0.25659 | val_0_rmse: 0.52086 | val_1_rmse: 0.52141 |  0:01:19s
epoch 36 | loss: 0.25495 | val_0_rmse: 0.51464 | val_1_rmse: 0.51684 |  0:01:21s
epoch 37 | loss: 0.25808 | val_0_rmse: 0.50416 | val_1_rmse: 0.50574 |  0:01:23s
epoch 38 | loss: 0.26306 | val_0_rmse: 0.54994 | val_1_rmse: 0.54964 |  0:01:26s
epoch 39 | loss: 0.27214 | val_0_rmse: 0.50597 | val_1_rmse: 0.5076  |  0:01:28s
epoch 40 | loss: 0.2574  | val_0_rmse: 0.51818 | val_1_rmse: 0.51857 |  0:01:30s
epoch 41 | loss: 0.25827 | val_0_rmse: 0.56167 | val_1_rmse: 0.55736 |  0:01:32s
epoch 42 | loss: 0.26591 | val_0_rmse: 0.61321 | val_1_rmse: 0.62123 |  0:01:34s
epoch 43 | loss: 0.25369 | val_0_rmse: 0.51167 | val_1_rmse: 0.51464 |  0:01:37s
epoch 44 | loss: 0.25677 | val_0_rmse: 0.53206 | val_1_rmse: 0.53076 |  0:01:39s
epoch 45 | loss: 0.26014 | val_0_rmse: 0.54182 | val_1_rmse: 0.54178 |  0:01:41s
epoch 46 | loss: 0.26213 | val_0_rmse: 0.55897 | val_1_rmse: 0.55715 |  0:01:43s
epoch 47 | loss: 0.25185 | val_0_rmse: 0.5026  | val_1_rmse: 0.50295 |  0:01:45s
epoch 48 | loss: 0.25264 | val_0_rmse: 0.49882 | val_1_rmse: 0.49872 |  0:01:47s
epoch 49 | loss: 0.24636 | val_0_rmse: 0.53129 | val_1_rmse: 0.53165 |  0:01:50s
epoch 50 | loss: 0.25057 | val_0_rmse: 0.53282 | val_1_rmse: 0.53655 |  0:01:52s
epoch 51 | loss: 0.25251 | val_0_rmse: 0.50824 | val_1_rmse: 0.50847 |  0:01:54s
epoch 52 | loss: 0.24605 | val_0_rmse: 0.52995 | val_1_rmse: 0.52961 |  0:01:56s
epoch 53 | loss: 0.24888 | val_0_rmse: 0.49967 | val_1_rmse: 0.50117 |  0:01:58s
epoch 54 | loss: 0.24615 | val_0_rmse: 0.50954 | val_1_rmse: 0.51477 |  0:02:01s
epoch 55 | loss: 0.24864 | val_0_rmse: 0.52749 | val_1_rmse: 0.52628 |  0:02:03s
epoch 56 | loss: 0.25167 | val_0_rmse: 0.56476 | val_1_rmse: 0.56252 |  0:02:05s
epoch 57 | loss: 0.24872 | val_0_rmse: 0.49611 | val_1_rmse: 0.49996 |  0:02:07s
epoch 58 | loss: 0.24952 | val_0_rmse: 0.49262 | val_1_rmse: 0.49769 |  0:02:09s
epoch 59 | loss: 0.24766 | val_0_rmse: 0.51306 | val_1_rmse: 0.51463 |  0:02:11s
epoch 60 | loss: 0.24771 | val_0_rmse: 0.52544 | val_1_rmse: 0.52576 |  0:02:14s
epoch 61 | loss: 0.26162 | val_0_rmse: 0.51761 | val_1_rmse: 0.51379 |  0:02:16s
epoch 62 | loss: 0.25697 | val_0_rmse: 0.5164  | val_1_rmse: 0.51391 |  0:02:18s
epoch 63 | loss: 0.25375 | val_0_rmse: 0.54253 | val_1_rmse: 0.54254 |  0:02:20s
epoch 64 | loss: 0.26054 | val_0_rmse: 0.56597 | val_1_rmse: 0.56366 |  0:02:22s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.49756
Best weights from best epoch are automatically used!
ended training at: 02:07:45
Feature importance:
[('Area', 0.45688769937021223), ('Baths', 0.023407926386085024), ('Beds', 0.11524378011135657), ('Latitude', 0.07104050434243386), ('Longitude', 0.30724826033822594), ('Month', 0.026170146888960653), ('Year', 1.6825627257152626e-06)]
Mean squared error is of 1000507254.7952336
Mean absolute error:21581.753298945645
MAPE:0.2660838676331217
R2 score:0.7464889340373964
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:07:45
epoch 0  | loss: 0.57181 | val_0_rmse: 0.62444 | val_1_rmse: 0.62468 |  0:00:02s
epoch 1  | loss: 0.35373 | val_0_rmse: 0.5665  | val_1_rmse: 0.573   |  0:00:04s
epoch 2  | loss: 0.31912 | val_0_rmse: 0.57022 | val_1_rmse: 0.56936 |  0:00:06s
epoch 3  | loss: 0.30864 | val_0_rmse: 0.56346 | val_1_rmse: 0.57307 |  0:00:08s
epoch 4  | loss: 0.29365 | val_0_rmse: 0.51951 | val_1_rmse: 0.5282  |  0:00:10s
epoch 5  | loss: 0.28726 | val_0_rmse: 0.59948 | val_1_rmse: 0.60754 |  0:00:13s
epoch 6  | loss: 0.28646 | val_0_rmse: 0.51549 | val_1_rmse: 0.52325 |  0:00:15s
epoch 7  | loss: 0.28475 | val_0_rmse: 0.57003 | val_1_rmse: 0.57668 |  0:00:17s
epoch 8  | loss: 0.28333 | val_0_rmse: 0.60127 | val_1_rmse: 0.60195 |  0:00:19s
epoch 9  | loss: 0.28378 | val_0_rmse: 0.60601 | val_1_rmse: 0.61221 |  0:00:21s
epoch 10 | loss: 0.27566 | val_0_rmse: 0.54811 | val_1_rmse: 0.56077 |  0:00:24s
epoch 11 | loss: 0.27518 | val_0_rmse: 0.56894 | val_1_rmse: 0.57747 |  0:00:26s
epoch 12 | loss: 0.27878 | val_0_rmse: 0.55302 | val_1_rmse: 0.5631  |  0:00:28s
epoch 13 | loss: 0.27654 | val_0_rmse: 0.53891 | val_1_rmse: 0.55056 |  0:00:30s
epoch 14 | loss: 0.27866 | val_0_rmse: 0.52556 | val_1_rmse: 0.53634 |  0:00:32s
epoch 15 | loss: 0.27115 | val_0_rmse: 0.59299 | val_1_rmse: 0.6055  |  0:00:34s
epoch 16 | loss: 0.28206 | val_0_rmse: 0.55748 | val_1_rmse: 0.56672 |  0:00:36s
epoch 17 | loss: 0.27557 | val_0_rmse: 0.51067 | val_1_rmse: 0.52253 |  0:00:39s
epoch 18 | loss: 0.26788 | val_0_rmse: 0.51275 | val_1_rmse: 0.52528 |  0:00:41s
epoch 19 | loss: 0.2656  | val_0_rmse: 0.51006 | val_1_rmse: 0.52306 |  0:00:43s
epoch 20 | loss: 0.26915 | val_0_rmse: 0.56415 | val_1_rmse: 0.57178 |  0:00:45s
epoch 21 | loss: 0.26868 | val_0_rmse: 0.51238 | val_1_rmse: 0.52202 |  0:00:47s
epoch 22 | loss: 0.26439 | val_0_rmse: 0.51147 | val_1_rmse: 0.52337 |  0:00:50s
epoch 23 | loss: 0.26038 | val_0_rmse: 0.60337 | val_1_rmse: 0.61165 |  0:00:52s
epoch 24 | loss: 0.26493 | val_0_rmse: 0.51013 | val_1_rmse: 0.5231  |  0:00:54s
epoch 25 | loss: 0.2532  | val_0_rmse: 0.52113 | val_1_rmse: 0.53122 |  0:00:56s
epoch 26 | loss: 0.25798 | val_0_rmse: 0.50314 | val_1_rmse: 0.51916 |  0:00:58s
epoch 27 | loss: 0.25901 | val_0_rmse: 0.50987 | val_1_rmse: 0.52364 |  0:01:00s
epoch 28 | loss: 0.25265 | val_0_rmse: 0.58121 | val_1_rmse: 0.59005 |  0:01:03s
epoch 29 | loss: 0.25575 | val_0_rmse: 0.62356 | val_1_rmse: 0.61928 |  0:01:05s
epoch 30 | loss: 0.25894 | val_0_rmse: 0.50284 | val_1_rmse: 0.5115  |  0:01:07s
epoch 31 | loss: 0.25006 | val_0_rmse: 0.51109 | val_1_rmse: 0.52067 |  0:01:09s
epoch 32 | loss: 0.2499  | val_0_rmse: 0.57323 | val_1_rmse: 0.5805  |  0:01:11s
epoch 33 | loss: 0.25318 | val_0_rmse: 0.5097  | val_1_rmse: 0.52338 |  0:01:13s
epoch 34 | loss: 0.25166 | val_0_rmse: 0.50442 | val_1_rmse: 0.51416 |  0:01:16s
epoch 35 | loss: 0.25116 | val_0_rmse: 0.57128 | val_1_rmse: 0.57875 |  0:01:18s
epoch 36 | loss: 0.25236 | val_0_rmse: 0.51912 | val_1_rmse: 0.53559 |  0:01:20s
epoch 37 | loss: 0.26202 | val_0_rmse: 0.52191 | val_1_rmse: 0.53064 |  0:01:22s
epoch 38 | loss: 0.25301 | val_0_rmse: 0.519   | val_1_rmse: 0.53317 |  0:01:24s
epoch 39 | loss: 0.25628 | val_0_rmse: 0.5513  | val_1_rmse: 0.56161 |  0:01:26s
epoch 40 | loss: 0.25236 | val_0_rmse: 0.50605 | val_1_rmse: 0.52335 |  0:01:29s
epoch 41 | loss: 0.25318 | val_0_rmse: 0.49224 | val_1_rmse: 0.50747 |  0:01:31s
epoch 42 | loss: 0.25095 | val_0_rmse: 0.50157 | val_1_rmse: 0.51476 |  0:01:33s
epoch 43 | loss: 0.25045 | val_0_rmse: 0.51467 | val_1_rmse: 0.52961 |  0:01:35s
epoch 44 | loss: 0.25278 | val_0_rmse: 0.49272 | val_1_rmse: 0.50697 |  0:01:37s
epoch 45 | loss: 0.25042 | val_0_rmse: 0.5283  | val_1_rmse: 0.53668 |  0:01:40s
epoch 46 | loss: 0.24955 | val_0_rmse: 0.54176 | val_1_rmse: 0.5504  |  0:01:42s
epoch 47 | loss: 0.26025 | val_0_rmse: 0.53062 | val_1_rmse: 0.5406  |  0:01:44s
epoch 48 | loss: 0.26292 | val_0_rmse: 0.50298 | val_1_rmse: 0.51297 |  0:01:46s
epoch 49 | loss: 0.26276 | val_0_rmse: 0.52703 | val_1_rmse: 0.53616 |  0:01:48s
epoch 50 | loss: 0.25383 | val_0_rmse: 0.53674 | val_1_rmse: 0.54635 |  0:01:50s
epoch 51 | loss: 0.2509  | val_0_rmse: 0.49733 | val_1_rmse: 0.51304 |  0:01:53s
epoch 52 | loss: 0.25115 | val_0_rmse: 0.53961 | val_1_rmse: 0.54831 |  0:01:55s
epoch 53 | loss: 0.25295 | val_0_rmse: 0.51878 | val_1_rmse: 0.52999 |  0:01:57s
epoch 54 | loss: 0.24537 | val_0_rmse: 0.51491 | val_1_rmse: 0.52746 |  0:01:59s
epoch 55 | loss: 0.24841 | val_0_rmse: 0.5014  | val_1_rmse: 0.515   |  0:02:01s
epoch 56 | loss: 0.25022 | val_0_rmse: 0.55412 | val_1_rmse: 0.56403 |  0:02:03s
epoch 57 | loss: 0.25168 | val_0_rmse: 0.56239 | val_1_rmse: 0.57005 |  0:02:06s
epoch 58 | loss: 0.24798 | val_0_rmse: 0.51071 | val_1_rmse: 0.52315 |  0:02:08s
epoch 59 | loss: 0.24867 | val_0_rmse: 0.48845 | val_1_rmse: 0.50481 |  0:02:10s
epoch 60 | loss: 0.24687 | val_0_rmse: 0.48818 | val_1_rmse: 0.50534 |  0:02:12s
epoch 61 | loss: 0.25048 | val_0_rmse: 0.5184  | val_1_rmse: 0.52986 |  0:02:14s
epoch 62 | loss: 0.25069 | val_0_rmse: 0.48809 | val_1_rmse: 0.50415 |  0:02:16s
epoch 63 | loss: 0.24686 | val_0_rmse: 0.50296 | val_1_rmse: 0.51942 |  0:02:19s
epoch 64 | loss: 0.24559 | val_0_rmse: 0.56097 | val_1_rmse: 0.5719  |  0:02:21s
epoch 65 | loss: 0.24834 | val_0_rmse: 0.52649 | val_1_rmse: 0.53802 |  0:02:23s
epoch 66 | loss: 0.24556 | val_0_rmse: 0.53806 | val_1_rmse: 0.55184 |  0:02:25s
epoch 67 | loss: 0.2471  | val_0_rmse: 0.51258 | val_1_rmse: 0.52723 |  0:02:27s
epoch 68 | loss: 0.24907 | val_0_rmse: 0.50831 | val_1_rmse: 0.52387 |  0:02:30s
epoch 69 | loss: 0.2528  | val_0_rmse: 0.54549 | val_1_rmse: 0.55177 |  0:02:32s
epoch 70 | loss: 0.24679 | val_0_rmse: 0.57516 | val_1_rmse: 0.58589 |  0:02:34s
epoch 71 | loss: 0.2459  | val_0_rmse: 0.59945 | val_1_rmse: 0.60724 |  0:02:36s
epoch 72 | loss: 0.24665 | val_0_rmse: 0.52897 | val_1_rmse: 0.54287 |  0:02:38s
epoch 73 | loss: 0.24573 | val_0_rmse: 0.56671 | val_1_rmse: 0.57616 |  0:02:40s
epoch 74 | loss: 0.24475 | val_0_rmse: 0.51057 | val_1_rmse: 0.52517 |  0:02:43s
epoch 75 | loss: 0.24478 | val_0_rmse: 0.50826 | val_1_rmse: 0.52208 |  0:02:45s
epoch 76 | loss: 0.24454 | val_0_rmse: 0.5558  | val_1_rmse: 0.55847 |  0:02:47s
epoch 77 | loss: 0.24497 | val_0_rmse: 0.53162 | val_1_rmse: 0.5452  |  0:02:49s
epoch 78 | loss: 0.24484 | val_0_rmse: 0.56973 | val_1_rmse: 0.58026 |  0:02:51s
epoch 79 | loss: 0.2427  | val_0_rmse: 0.56996 | val_1_rmse: 0.57726 |  0:02:53s
epoch 80 | loss: 0.24615 | val_0_rmse: 0.5424  | val_1_rmse: 0.55164 |  0:02:56s
epoch 81 | loss: 0.24295 | val_0_rmse: 0.51435 | val_1_rmse: 0.52772 |  0:02:58s
epoch 82 | loss: 0.24298 | val_0_rmse: 0.5212  | val_1_rmse: 0.53288 |  0:03:00s
epoch 83 | loss: 0.24667 | val_0_rmse: 0.50554 | val_1_rmse: 0.51455 |  0:03:02s
epoch 84 | loss: 0.24641 | val_0_rmse: 0.5431  | val_1_rmse: 0.55156 |  0:03:04s
epoch 85 | loss: 0.24162 | val_0_rmse: 0.52098 | val_1_rmse: 0.53421 |  0:03:06s
epoch 86 | loss: 0.24349 | val_0_rmse: 0.5423  | val_1_rmse: 0.55179 |  0:03:09s
epoch 87 | loss: 0.24721 | val_0_rmse: 0.57812 | val_1_rmse: 0.58736 |  0:03:11s
epoch 88 | loss: 0.24132 | val_0_rmse: 0.48093 | val_1_rmse: 0.49881 |  0:03:13s
epoch 89 | loss: 0.24681 | val_0_rmse: 0.54567 | val_1_rmse: 0.55576 |  0:03:15s
epoch 90 | loss: 0.24663 | val_0_rmse: 0.52456 | val_1_rmse: 0.53769 |  0:03:17s
epoch 91 | loss: 0.24096 | val_0_rmse: 0.48258 | val_1_rmse: 0.50012 |  0:03:19s
epoch 92 | loss: 0.24461 | val_0_rmse: 0.50873 | val_1_rmse: 0.52563 |  0:03:22s
epoch 93 | loss: 0.24074 | val_0_rmse: 0.5182  | val_1_rmse: 0.53098 |  0:03:24s
epoch 94 | loss: 0.24296 | val_0_rmse: 0.49071 | val_1_rmse: 0.50469 |  0:03:26s
epoch 95 | loss: 0.2429  | val_0_rmse: 0.49915 | val_1_rmse: 0.51392 |  0:03:28s
epoch 96 | loss: 0.24839 | val_0_rmse: 0.49672 | val_1_rmse: 0.51054 |  0:03:30s
epoch 97 | loss: 0.24166 | val_0_rmse: 0.49709 | val_1_rmse: 0.51307 |  0:03:32s
epoch 98 | loss: 0.24223 | val_0_rmse: 0.55018 | val_1_rmse: 0.56044 |  0:03:35s
epoch 99 | loss: 0.24624 | val_0_rmse: 0.53354 | val_1_rmse: 0.54137 |  0:03:37s
epoch 100| loss: 0.2421  | val_0_rmse: 0.4887  | val_1_rmse: 0.50115 |  0:03:39s
epoch 101| loss: 0.24059 | val_0_rmse: 0.4824  | val_1_rmse: 0.49682 |  0:03:41s
epoch 102| loss: 0.24551 | val_0_rmse: 0.60676 | val_1_rmse: 0.61191 |  0:03:43s
epoch 103| loss: 0.25302 | val_0_rmse: 0.50759 | val_1_rmse: 0.51912 |  0:03:46s
epoch 104| loss: 0.24429 | val_0_rmse: 0.49583 | val_1_rmse: 0.51374 |  0:03:48s
epoch 105| loss: 0.24412 | val_0_rmse: 0.53623 | val_1_rmse: 0.54801 |  0:03:50s
epoch 106| loss: 0.24087 | val_0_rmse: 0.48179 | val_1_rmse: 0.49774 |  0:03:52s
epoch 107| loss: 0.24064 | val_0_rmse: 0.49171 | val_1_rmse: 0.50966 |  0:03:54s
epoch 108| loss: 0.23865 | val_0_rmse: 0.5643  | val_1_rmse: 0.57244 |  0:03:56s
epoch 109| loss: 0.23914 | val_0_rmse: 0.49784 | val_1_rmse: 0.51286 |  0:03:59s
epoch 110| loss: 0.24092 | val_0_rmse: 0.50439 | val_1_rmse: 0.51778 |  0:04:01s
epoch 111| loss: 0.24461 | val_0_rmse: 0.48908 | val_1_rmse: 0.50512 |  0:04:03s
epoch 112| loss: 0.24082 | val_0_rmse: 0.55067 | val_1_rmse: 0.5613  |  0:04:05s
epoch 113| loss: 0.24136 | val_0_rmse: 0.52825 | val_1_rmse: 0.54616 |  0:04:07s
epoch 114| loss: 0.24142 | val_0_rmse: 0.50509 | val_1_rmse: 0.51685 |  0:04:09s
epoch 115| loss: 0.24561 | val_0_rmse: 0.48344 | val_1_rmse: 0.50015 |  0:04:12s
epoch 116| loss: 0.23979 | val_0_rmse: 0.54316 | val_1_rmse: 0.5484  |  0:04:14s
epoch 117| loss: 0.23925 | val_0_rmse: 0.48895 | val_1_rmse: 0.5068  |  0:04:16s
epoch 118| loss: 0.23935 | val_0_rmse: 0.48928 | val_1_rmse: 0.50651 |  0:04:18s
epoch 119| loss: 0.23763 | val_0_rmse: 0.54663 | val_1_rmse: 0.55966 |  0:04:20s
epoch 120| loss: 0.23964 | val_0_rmse: 0.52545 | val_1_rmse: 0.54081 |  0:04:23s
epoch 121| loss: 0.23826 | val_0_rmse: 0.4852  | val_1_rmse: 0.50234 |  0:04:25s
epoch 122| loss: 0.24007 | val_0_rmse: 0.4889  | val_1_rmse: 0.50746 |  0:04:27s
epoch 123| loss: 0.23794 | val_0_rmse: 0.50083 | val_1_rmse: 0.51475 |  0:04:29s
epoch 124| loss: 0.24032 | val_0_rmse: 0.50189 | val_1_rmse: 0.5182  |  0:04:31s
epoch 125| loss: 0.24581 | val_0_rmse: 0.50364 | val_1_rmse: 0.51657 |  0:04:33s
epoch 126| loss: 0.23786 | val_0_rmse: 0.4913  | val_1_rmse: 0.5066  |  0:04:36s
epoch 127| loss: 0.23925 | val_0_rmse: 0.51292 | val_1_rmse: 0.52483 |  0:04:38s
epoch 128| loss: 0.23585 | val_0_rmse: 0.48345 | val_1_rmse: 0.50081 |  0:04:40s
epoch 129| loss: 0.23887 | val_0_rmse: 0.57866 | val_1_rmse: 0.58721 |  0:04:42s
epoch 130| loss: 0.2444  | val_0_rmse: 0.49067 | val_1_rmse: 0.50446 |  0:04:44s
epoch 131| loss: 0.23941 | val_0_rmse: 0.49304 | val_1_rmse: 0.51022 |  0:04:46s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.49682
Best weights from best epoch are automatically used!
ended training at: 02:12:33
Feature importance:
[('Area', 0.40032592337385), ('Baths', 0.08942402647779124), ('Beds', 0.14693949900151476), ('Latitude', 0.05578583169640142), ('Longitude', 0.2480544414773494), ('Month', 0.05945020997228227), ('Year', 2.006800081089474e-05)]
Mean squared error is of 999091372.3424243
Mean absolute error:21525.493871289244
MAPE:0.267263268898433
R2 score:0.7515923176071733
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:12:33
epoch 0  | loss: 0.56057 | val_0_rmse: 0.64128 | val_1_rmse: 0.64809 |  0:00:02s
epoch 1  | loss: 0.35785 | val_0_rmse: 0.58565 | val_1_rmse: 0.59505 |  0:00:04s
epoch 2  | loss: 0.3354  | val_0_rmse: 0.55951 | val_1_rmse: 0.56823 |  0:00:06s
epoch 3  | loss: 0.31962 | val_0_rmse: 0.55439 | val_1_rmse: 0.56423 |  0:00:08s
epoch 4  | loss: 0.31413 | val_0_rmse: 0.56051 | val_1_rmse: 0.56511 |  0:00:10s
epoch 5  | loss: 0.31173 | val_0_rmse: 0.55764 | val_1_rmse: 0.56656 |  0:00:13s
epoch 6  | loss: 0.30519 | val_0_rmse: 0.53091 | val_1_rmse: 0.534   |  0:00:15s
epoch 7  | loss: 0.28958 | val_0_rmse: 0.51829 | val_1_rmse: 0.5215  |  0:00:17s
epoch 8  | loss: 0.2822  | val_0_rmse: 0.55061 | val_1_rmse: 0.55495 |  0:00:19s
epoch 9  | loss: 0.27509 | val_0_rmse: 0.53667 | val_1_rmse: 0.54387 |  0:00:21s
epoch 10 | loss: 0.28083 | val_0_rmse: 0.53583 | val_1_rmse: 0.53826 |  0:00:24s
epoch 11 | loss: 0.26789 | val_0_rmse: 0.50671 | val_1_rmse: 0.51382 |  0:00:26s
epoch 12 | loss: 0.27251 | val_0_rmse: 0.51839 | val_1_rmse: 0.52277 |  0:00:28s
epoch 13 | loss: 0.26728 | val_0_rmse: 0.65121 | val_1_rmse: 0.65788 |  0:00:30s
epoch 14 | loss: 0.26993 | val_0_rmse: 0.50835 | val_1_rmse: 0.51359 |  0:00:32s
epoch 15 | loss: 0.2686  | val_0_rmse: 0.60747 | val_1_rmse: 0.61632 |  0:00:35s
epoch 16 | loss: 0.27007 | val_0_rmse: 0.55893 | val_1_rmse: 0.56773 |  0:00:37s
epoch 17 | loss: 0.26672 | val_0_rmse: 0.52464 | val_1_rmse: 0.53218 |  0:00:39s
epoch 18 | loss: 0.27156 | val_0_rmse: 0.51527 | val_1_rmse: 0.5173  |  0:00:41s
epoch 19 | loss: 0.26887 | val_0_rmse: 0.56213 | val_1_rmse: 0.57325 |  0:00:43s
epoch 20 | loss: 0.26213 | val_0_rmse: 0.49639 | val_1_rmse: 0.50185 |  0:00:46s
epoch 21 | loss: 0.26039 | val_0_rmse: 0.50687 | val_1_rmse: 0.51472 |  0:00:48s
epoch 22 | loss: 0.25795 | val_0_rmse: 0.61995 | val_1_rmse: 0.63399 |  0:00:50s
epoch 23 | loss: 0.2589  | val_0_rmse: 0.50943 | val_1_rmse: 0.51629 |  0:00:52s
epoch 24 | loss: 0.25956 | val_0_rmse: 0.54186 | val_1_rmse: 0.55139 |  0:00:54s
epoch 25 | loss: 0.25656 | val_0_rmse: 0.5024  | val_1_rmse: 0.51144 |  0:00:56s
epoch 26 | loss: 0.25593 | val_0_rmse: 0.51291 | val_1_rmse: 0.52098 |  0:00:59s
epoch 27 | loss: 0.25859 | val_0_rmse: 0.6673  | val_1_rmse: 0.68156 |  0:01:01s
epoch 28 | loss: 0.26001 | val_0_rmse: 0.55581 | val_1_rmse: 0.56246 |  0:01:03s
epoch 29 | loss: 0.25892 | val_0_rmse: 0.51875 | val_1_rmse: 0.52827 |  0:01:05s
epoch 30 | loss: 0.25748 | val_0_rmse: 0.5046  | val_1_rmse: 0.5111  |  0:01:07s
epoch 31 | loss: 0.25623 | val_0_rmse: 0.58862 | val_1_rmse: 0.59453 |  0:01:10s
epoch 32 | loss: 0.26065 | val_0_rmse: 0.54154 | val_1_rmse: 0.55032 |  0:01:12s
epoch 33 | loss: 0.26067 | val_0_rmse: 0.55089 | val_1_rmse: 0.56009 |  0:01:14s
epoch 34 | loss: 0.25469 | val_0_rmse: 0.52235 | val_1_rmse: 0.52662 |  0:01:16s
epoch 35 | loss: 0.25276 | val_0_rmse: 0.50112 | val_1_rmse: 0.50808 |  0:01:18s
epoch 36 | loss: 0.25653 | val_0_rmse: 0.53724 | val_1_rmse: 0.5469  |  0:01:21s
epoch 37 | loss: 0.25356 | val_0_rmse: 0.52044 | val_1_rmse: 0.53084 |  0:01:23s
epoch 38 | loss: 0.25446 | val_0_rmse: 0.50814 | val_1_rmse: 0.5141  |  0:01:25s
epoch 39 | loss: 0.2529  | val_0_rmse: 0.51565 | val_1_rmse: 0.52124 |  0:01:27s
epoch 40 | loss: 0.25359 | val_0_rmse: 0.61325 | val_1_rmse: 0.62552 |  0:01:29s
epoch 41 | loss: 0.25195 | val_0_rmse: 0.52039 | val_1_rmse: 0.52597 |  0:01:31s
epoch 42 | loss: 0.25248 | val_0_rmse: 0.55428 | val_1_rmse: 0.55914 |  0:01:34s
epoch 43 | loss: 0.25454 | val_0_rmse: 0.51233 | val_1_rmse: 0.52164 |  0:01:36s
epoch 44 | loss: 0.25091 | val_0_rmse: 0.60696 | val_1_rmse: 0.6222  |  0:01:38s
epoch 45 | loss: 0.25285 | val_0_rmse: 0.49429 | val_1_rmse: 0.50036 |  0:01:40s
epoch 46 | loss: 0.25033 | val_0_rmse: 0.5096  | val_1_rmse: 0.51214 |  0:01:42s
epoch 47 | loss: 0.25146 | val_0_rmse: 0.5435  | val_1_rmse: 0.55432 |  0:01:45s
epoch 48 | loss: 0.25301 | val_0_rmse: 0.48794 | val_1_rmse: 0.49787 |  0:01:47s
epoch 49 | loss: 0.24739 | val_0_rmse: 0.51066 | val_1_rmse: 0.51709 |  0:01:49s
epoch 50 | loss: 0.25014 | val_0_rmse: 0.54372 | val_1_rmse: 0.54843 |  0:01:51s
epoch 51 | loss: 0.24994 | val_0_rmse: 0.54643 | val_1_rmse: 0.55627 |  0:01:53s
epoch 52 | loss: 0.2505  | val_0_rmse: 0.53163 | val_1_rmse: 0.53921 |  0:01:55s
epoch 53 | loss: 0.24647 | val_0_rmse: 0.49615 | val_1_rmse: 0.50446 |  0:01:58s
epoch 54 | loss: 0.25095 | val_0_rmse: 0.55598 | val_1_rmse: 0.5642  |  0:02:00s
epoch 55 | loss: 0.25359 | val_0_rmse: 0.4953  | val_1_rmse: 0.50605 |  0:02:02s
epoch 56 | loss: 0.2512  | val_0_rmse: 0.50661 | val_1_rmse: 0.51053 |  0:02:04s
epoch 57 | loss: 0.2517  | val_0_rmse: 0.55615 | val_1_rmse: 0.56573 |  0:02:06s
epoch 58 | loss: 0.25226 | val_0_rmse: 0.53265 | val_1_rmse: 0.54326 |  0:02:08s
epoch 59 | loss: 0.24662 | val_0_rmse: 0.55751 | val_1_rmse: 0.56797 |  0:02:11s
epoch 60 | loss: 0.25269 | val_0_rmse: 0.53757 | val_1_rmse: 0.54756 |  0:02:13s
epoch 61 | loss: 0.24647 | val_0_rmse: 0.5279  | val_1_rmse: 0.53946 |  0:02:15s
epoch 62 | loss: 0.25112 | val_0_rmse: 0.58211 | val_1_rmse: 0.59626 |  0:02:17s
epoch 63 | loss: 0.24627 | val_0_rmse: 0.49936 | val_1_rmse: 0.51247 |  0:02:19s
epoch 64 | loss: 0.24215 | val_0_rmse: 0.5421  | val_1_rmse: 0.54773 |  0:02:22s
epoch 65 | loss: 0.24569 | val_0_rmse: 0.536   | val_1_rmse: 0.54573 |  0:02:24s
epoch 66 | loss: 0.24305 | val_0_rmse: 0.51339 | val_1_rmse: 0.52546 |  0:02:26s
epoch 67 | loss: 0.24157 | val_0_rmse: 0.50793 | val_1_rmse: 0.5144  |  0:02:28s
epoch 68 | loss: 0.24277 | val_0_rmse: 0.50077 | val_1_rmse: 0.51111 |  0:02:30s
epoch 69 | loss: 0.24044 | val_0_rmse: 0.48749 | val_1_rmse: 0.4921  |  0:02:32s
epoch 70 | loss: 0.24442 | val_0_rmse: 0.50828 | val_1_rmse: 0.51785 |  0:02:35s
epoch 71 | loss: 0.24446 | val_0_rmse: 0.5173  | val_1_rmse: 0.52817 |  0:02:37s
epoch 72 | loss: 0.24638 | val_0_rmse: 0.55055 | val_1_rmse: 0.55994 |  0:02:39s
epoch 73 | loss: 0.24448 | val_0_rmse: 0.53411 | val_1_rmse: 0.54776 |  0:02:41s
epoch 74 | loss: 0.23758 | val_0_rmse: 0.4928  | val_1_rmse: 0.4966  |  0:02:43s
epoch 75 | loss: 0.24242 | val_0_rmse: 0.59853 | val_1_rmse: 0.61157 |  0:02:46s
epoch 76 | loss: 0.24196 | val_0_rmse: 0.52042 | val_1_rmse: 0.53129 |  0:02:48s
epoch 77 | loss: 0.24069 | val_0_rmse: 0.50109 | val_1_rmse: 0.51289 |  0:02:50s
epoch 78 | loss: 0.23638 | val_0_rmse: 0.58873 | val_1_rmse: 0.60349 |  0:02:52s
epoch 79 | loss: 0.23545 | val_0_rmse: 0.49177 | val_1_rmse: 0.50117 |  0:02:54s
epoch 80 | loss: 0.23747 | val_0_rmse: 0.49408 | val_1_rmse: 0.50572 |  0:02:56s
epoch 81 | loss: 0.23822 | val_0_rmse: 0.52515 | val_1_rmse: 0.53997 |  0:02:59s
epoch 82 | loss: 0.23648 | val_0_rmse: 0.60923 | val_1_rmse: 0.62442 |  0:03:01s
epoch 83 | loss: 0.23612 | val_0_rmse: 0.57015 | val_1_rmse: 0.57894 |  0:03:03s
epoch 84 | loss: 0.24602 | val_0_rmse: 0.7449  | val_1_rmse: 0.7546  |  0:03:05s
epoch 85 | loss: 0.24667 | val_0_rmse: 0.51532 | val_1_rmse: 0.52913 |  0:03:07s
epoch 86 | loss: 0.23719 | val_0_rmse: 0.5707  | val_1_rmse: 0.58751 |  0:03:10s
epoch 87 | loss: 0.23967 | val_0_rmse: 0.58414 | val_1_rmse: 0.60315 |  0:03:12s
epoch 88 | loss: 0.23665 | val_0_rmse: 0.478   | val_1_rmse: 0.48734 |  0:03:14s
epoch 89 | loss: 0.23834 | val_0_rmse: 0.62133 | val_1_rmse: 0.63692 |  0:03:16s
epoch 90 | loss: 0.23761 | val_0_rmse: 0.5229  | val_1_rmse: 0.53936 |  0:03:18s
epoch 91 | loss: 0.2377  | val_0_rmse: 0.52248 | val_1_rmse: 0.53176 |  0:03:20s
epoch 92 | loss: 0.23465 | val_0_rmse: 0.50069 | val_1_rmse: 0.51126 |  0:03:23s
epoch 93 | loss: 0.23637 | val_0_rmse: 0.51209 | val_1_rmse: 0.52065 |  0:03:25s
epoch 94 | loss: 0.23788 | val_0_rmse: 0.52279 | val_1_rmse: 0.53823 |  0:03:27s
epoch 95 | loss: 0.23492 | val_0_rmse: 0.56085 | val_1_rmse: 0.57507 |  0:03:29s
epoch 96 | loss: 0.23446 | val_0_rmse: 0.49493 | val_1_rmse: 0.50875 |  0:03:31s
epoch 97 | loss: 0.23352 | val_0_rmse: 0.60282 | val_1_rmse: 0.61778 |  0:03:33s
epoch 98 | loss: 0.2346  | val_0_rmse: 0.55323 | val_1_rmse: 0.57014 |  0:03:36s
epoch 99 | loss: 0.2373  | val_0_rmse: 0.62559 | val_1_rmse: 0.6428  |  0:03:38s
epoch 100| loss: 0.23394 | val_0_rmse: 0.61262 | val_1_rmse: 0.62897 |  0:03:40s
epoch 101| loss: 0.23516 | val_0_rmse: 0.5117  | val_1_rmse: 0.52259 |  0:03:42s
epoch 102| loss: 0.23376 | val_0_rmse: 0.5344  | val_1_rmse: 0.54612 |  0:03:44s
epoch 103| loss: 0.23392 | val_0_rmse: 0.56393 | val_1_rmse: 0.57919 |  0:03:46s
epoch 104| loss: 0.23708 | val_0_rmse: 0.53762 | val_1_rmse: 0.5521  |  0:03:49s
epoch 105| loss: 0.24529 | val_0_rmse: 0.58618 | val_1_rmse: 0.59678 |  0:03:51s
epoch 106| loss: 0.24042 | val_0_rmse: 0.5985  | val_1_rmse: 0.61604 |  0:03:53s
epoch 107| loss: 0.23679 | val_0_rmse: 0.52075 | val_1_rmse: 0.5387  |  0:03:55s
epoch 108| loss: 0.23197 | val_0_rmse: 0.56177 | val_1_rmse: 0.57802 |  0:03:57s
epoch 109| loss: 0.23207 | val_0_rmse: 0.53801 | val_1_rmse: 0.5518  |  0:04:00s
epoch 110| loss: 0.2368  | val_0_rmse: 0.54277 | val_1_rmse: 0.56175 |  0:04:02s
epoch 111| loss: 0.23717 | val_0_rmse: 0.54974 | val_1_rmse: 0.56487 |  0:04:04s
epoch 112| loss: 0.23225 | val_0_rmse: 0.5124  | val_1_rmse: 0.52821 |  0:04:06s
epoch 113| loss: 0.23484 | val_0_rmse: 0.5155  | val_1_rmse: 0.52741 |  0:04:08s
epoch 114| loss: 0.23252 | val_0_rmse: 0.53159 | val_1_rmse: 0.54267 |  0:04:10s
epoch 115| loss: 0.23233 | val_0_rmse: 0.6137  | val_1_rmse: 0.62921 |  0:04:13s
epoch 116| loss: 0.23244 | val_0_rmse: 0.54752 | val_1_rmse: 0.56315 |  0:04:15s
epoch 117| loss: 0.23673 | val_0_rmse: 0.47876 | val_1_rmse: 0.49307 |  0:04:17s
epoch 118| loss: 0.23272 | val_0_rmse: 0.54476 | val_1_rmse: 0.55734 |  0:04:19s

Early stopping occured at epoch 118 with best_epoch = 88 and best_val_1_rmse = 0.48734
Best weights from best epoch are automatically used!
ended training at: 02:16:53
Feature importance:
[('Area', 0.3196028552592662), ('Baths', 0.14589311399235677), ('Beds', 0.03904002512778776), ('Latitude', 0.08617278385767964), ('Longitude', 0.23761682942687803), ('Month', 0.09225455831300615), ('Year', 0.07941983402302549)]
Mean squared error is of 975687474.8612099
Mean absolute error:21420.71624933976
MAPE:0.2605805049225849
R2 score:0.7534977400458484
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:16:53
epoch 0  | loss: 0.53394 | val_0_rmse: 0.60845 | val_1_rmse: 0.59924 |  0:00:02s
epoch 1  | loss: 0.36096 | val_0_rmse: 0.57886 | val_1_rmse: 0.56956 |  0:00:04s
epoch 2  | loss: 0.33788 | val_0_rmse: 0.58415 | val_1_rmse: 0.58088 |  0:00:06s
epoch 3  | loss: 0.32054 | val_0_rmse: 0.56237 | val_1_rmse: 0.55586 |  0:00:08s
epoch 4  | loss: 0.3106  | val_0_rmse: 0.5515  | val_1_rmse: 0.53691 |  0:00:10s
epoch 5  | loss: 0.31265 | val_0_rmse: 0.54763 | val_1_rmse: 0.53719 |  0:00:13s
epoch 6  | loss: 0.31309 | val_0_rmse: 0.5422  | val_1_rmse: 0.5342  |  0:00:15s
epoch 7  | loss: 0.30812 | val_0_rmse: 0.54084 | val_1_rmse: 0.53281 |  0:00:17s
epoch 8  | loss: 0.30404 | val_0_rmse: 0.56286 | val_1_rmse: 0.55316 |  0:00:19s
epoch 9  | loss: 0.30982 | val_0_rmse: 0.5478  | val_1_rmse: 0.54194 |  0:00:21s
epoch 10 | loss: 0.29815 | val_0_rmse: 0.55037 | val_1_rmse: 0.54469 |  0:00:24s
epoch 11 | loss: 0.29678 | val_0_rmse: 0.52574 | val_1_rmse: 0.51846 |  0:00:26s
epoch 12 | loss: 0.28828 | val_0_rmse: 0.52537 | val_1_rmse: 0.5153  |  0:00:28s
epoch 13 | loss: 0.28738 | val_0_rmse: 0.5279  | val_1_rmse: 0.52489 |  0:00:30s
epoch 14 | loss: 0.28625 | val_0_rmse: 0.53028 | val_1_rmse: 0.52639 |  0:00:32s
epoch 15 | loss: 0.28278 | val_0_rmse: 0.54948 | val_1_rmse: 0.54858 |  0:00:34s
epoch 16 | loss: 0.28485 | val_0_rmse: 0.52309 | val_1_rmse: 0.5168  |  0:00:37s
epoch 17 | loss: 0.28037 | val_0_rmse: 0.52368 | val_1_rmse: 0.51769 |  0:00:39s
epoch 18 | loss: 0.27804 | val_0_rmse: 0.54056 | val_1_rmse: 0.54083 |  0:00:41s
epoch 19 | loss: 0.27359 | val_0_rmse: 0.55713 | val_1_rmse: 0.55602 |  0:00:43s
epoch 20 | loss: 0.27011 | val_0_rmse: 0.53313 | val_1_rmse: 0.52649 |  0:00:45s
epoch 21 | loss: 0.26959 | val_0_rmse: 0.52177 | val_1_rmse: 0.51914 |  0:00:47s
epoch 22 | loss: 0.27118 | val_0_rmse: 0.51058 | val_1_rmse: 0.50738 |  0:00:50s
epoch 23 | loss: 0.27371 | val_0_rmse: 0.56334 | val_1_rmse: 0.56086 |  0:00:52s
epoch 24 | loss: 0.26627 | val_0_rmse: 0.50931 | val_1_rmse: 0.51142 |  0:00:54s
epoch 25 | loss: 0.2662  | val_0_rmse: 0.52364 | val_1_rmse: 0.51625 |  0:00:56s
epoch 26 | loss: 0.26515 | val_0_rmse: 0.52663 | val_1_rmse: 0.51955 |  0:00:58s
epoch 27 | loss: 0.26723 | val_0_rmse: 0.51859 | val_1_rmse: 0.51164 |  0:01:01s
epoch 28 | loss: 0.2626  | val_0_rmse: 0.57129 | val_1_rmse: 0.57438 |  0:01:03s
epoch 29 | loss: 0.2675  | val_0_rmse: 0.56468 | val_1_rmse: 0.55487 |  0:01:05s
epoch 30 | loss: 0.2652  | val_0_rmse: 0.61867 | val_1_rmse: 0.60361 |  0:01:07s
epoch 31 | loss: 0.26438 | val_0_rmse: 0.51325 | val_1_rmse: 0.50948 |  0:01:09s
epoch 32 | loss: 0.26515 | val_0_rmse: 0.53988 | val_1_rmse: 0.54616 |  0:01:11s
epoch 33 | loss: 0.2626  | val_0_rmse: 0.56925 | val_1_rmse: 0.56758 |  0:01:14s
epoch 34 | loss: 0.26067 | val_0_rmse: 0.49462 | val_1_rmse: 0.49509 |  0:01:16s
epoch 35 | loss: 0.25931 | val_0_rmse: 0.50767 | val_1_rmse: 0.50142 |  0:01:18s
epoch 36 | loss: 0.2579  | val_0_rmse: 0.53484 | val_1_rmse: 0.53865 |  0:01:20s
epoch 37 | loss: 0.26257 | val_0_rmse: 0.50904 | val_1_rmse: 0.51037 |  0:01:22s
epoch 38 | loss: 0.25721 | val_0_rmse: 0.52084 | val_1_rmse: 0.51227 |  0:01:24s
epoch 39 | loss: 0.25487 | val_0_rmse: 0.54116 | val_1_rmse: 0.53122 |  0:01:27s
epoch 40 | loss: 0.25891 | val_0_rmse: 0.55487 | val_1_rmse: 0.55238 |  0:01:29s
epoch 41 | loss: 0.25407 | val_0_rmse: 0.49994 | val_1_rmse: 0.49372 |  0:01:31s
epoch 42 | loss: 0.25332 | val_0_rmse: 0.49501 | val_1_rmse: 0.50076 |  0:01:33s
epoch 43 | loss: 0.25715 | val_0_rmse: 0.5272  | val_1_rmse: 0.53367 |  0:01:35s
epoch 44 | loss: 0.25477 | val_0_rmse: 0.50233 | val_1_rmse: 0.49761 |  0:01:38s
epoch 45 | loss: 0.25442 | val_0_rmse: 0.53236 | val_1_rmse: 0.52805 |  0:01:40s
epoch 46 | loss: 0.25737 | val_0_rmse: 0.52639 | val_1_rmse: 0.52052 |  0:01:42s
epoch 47 | loss: 0.25321 | val_0_rmse: 0.50266 | val_1_rmse: 0.50528 |  0:01:44s
epoch 48 | loss: 0.25849 | val_0_rmse: 0.53615 | val_1_rmse: 0.54185 |  0:01:46s
epoch 49 | loss: 0.25744 | val_0_rmse: 0.51079 | val_1_rmse: 0.51471 |  0:01:48s
epoch 50 | loss: 0.25293 | val_0_rmse: 0.55331 | val_1_rmse: 0.54329 |  0:01:51s
epoch 51 | loss: 0.25419 | val_0_rmse: 0.54611 | val_1_rmse: 0.53536 |  0:01:53s
epoch 52 | loss: 0.25127 | val_0_rmse: 0.50418 | val_1_rmse: 0.49814 |  0:01:55s
epoch 53 | loss: 0.25458 | val_0_rmse: 0.68164 | val_1_rmse: 0.67566 |  0:01:57s
epoch 54 | loss: 0.25681 | val_0_rmse: 0.49809 | val_1_rmse: 0.49661 |  0:01:59s
epoch 55 | loss: 0.25494 | val_0_rmse: 0.54132 | val_1_rmse: 0.53144 |  0:02:02s
epoch 56 | loss: 0.25129 | val_0_rmse: 0.50629 | val_1_rmse: 0.50286 |  0:02:04s
epoch 57 | loss: 0.25026 | val_0_rmse: 0.60927 | val_1_rmse: 0.59528 |  0:02:06s
epoch 58 | loss: 0.24919 | val_0_rmse: 0.54906 | val_1_rmse: 0.54373 |  0:02:08s
epoch 59 | loss: 0.25281 | val_0_rmse: 0.51848 | val_1_rmse: 0.51091 |  0:02:10s
epoch 60 | loss: 0.2512  | val_0_rmse: 0.49491 | val_1_rmse: 0.49791 |  0:02:12s
epoch 61 | loss: 0.2524  | val_0_rmse: 0.50144 | val_1_rmse: 0.50753 |  0:02:15s
epoch 62 | loss: 0.25479 | val_0_rmse: 0.53251 | val_1_rmse: 0.52463 |  0:02:17s
epoch 63 | loss: 0.25095 | val_0_rmse: 0.49666 | val_1_rmse: 0.49966 |  0:02:19s
epoch 64 | loss: 0.24832 | val_0_rmse: 0.52755 | val_1_rmse: 0.5187  |  0:02:21s
epoch 65 | loss: 0.24539 | val_0_rmse: 0.51388 | val_1_rmse: 0.51291 |  0:02:23s
epoch 66 | loss: 0.25444 | val_0_rmse: 0.49605 | val_1_rmse: 0.49938 |  0:02:25s
epoch 67 | loss: 0.25379 | val_0_rmse: 0.5218  | val_1_rmse: 0.51224 |  0:02:28s
epoch 68 | loss: 0.2544  | val_0_rmse: 0.58611 | val_1_rmse: 0.57183 |  0:02:30s
epoch 69 | loss: 0.25017 | val_0_rmse: 0.49105 | val_1_rmse: 0.49218 |  0:02:32s
epoch 70 | loss: 0.24957 | val_0_rmse: 0.51216 | val_1_rmse: 0.51018 |  0:02:34s
epoch 71 | loss: 0.25061 | val_0_rmse: 0.55548 | val_1_rmse: 0.54453 |  0:02:36s
epoch 72 | loss: 0.25028 | val_0_rmse: 0.55396 | val_1_rmse: 0.54104 |  0:02:38s
epoch 73 | loss: 0.24602 | val_0_rmse: 0.52143 | val_1_rmse: 0.52492 |  0:02:41s
epoch 74 | loss: 0.24632 | val_0_rmse: 0.51365 | val_1_rmse: 0.50967 |  0:02:43s
epoch 75 | loss: 0.24816 | val_0_rmse: 0.53017 | val_1_rmse: 0.52071 |  0:02:45s
epoch 76 | loss: 0.24834 | val_0_rmse: 0.48568 | val_1_rmse: 0.48764 |  0:02:47s
epoch 77 | loss: 0.24678 | val_0_rmse: 0.49973 | val_1_rmse: 0.50468 |  0:02:49s
epoch 78 | loss: 0.24526 | val_0_rmse: 0.49706 | val_1_rmse: 0.50406 |  0:02:52s
epoch 79 | loss: 0.24862 | val_0_rmse: 0.52735 | val_1_rmse: 0.53225 |  0:02:54s
epoch 80 | loss: 0.24945 | val_0_rmse: 0.54059 | val_1_rmse: 0.52715 |  0:02:56s
epoch 81 | loss: 0.24935 | val_0_rmse: 0.49478 | val_1_rmse: 0.49825 |  0:02:58s
epoch 82 | loss: 0.24762 | val_0_rmse: 0.50715 | val_1_rmse: 0.51782 |  0:03:00s
epoch 83 | loss: 0.24781 | val_0_rmse: 0.50574 | val_1_rmse: 0.51561 |  0:03:02s
epoch 84 | loss: 0.24839 | val_0_rmse: 0.49944 | val_1_rmse: 0.50643 |  0:03:05s
epoch 85 | loss: 0.24718 | val_0_rmse: 0.50932 | val_1_rmse: 0.5046  |  0:03:07s
epoch 86 | loss: 0.25039 | val_0_rmse: 0.52145 | val_1_rmse: 0.52756 |  0:03:09s
epoch 87 | loss: 0.24991 | val_0_rmse: 0.52606 | val_1_rmse: 0.51793 |  0:03:11s
epoch 88 | loss: 0.24773 | val_0_rmse: 0.49064 | val_1_rmse: 0.49367 |  0:03:13s
epoch 89 | loss: 0.24985 | val_0_rmse: 0.51058 | val_1_rmse: 0.50839 |  0:03:15s
epoch 90 | loss: 0.24998 | val_0_rmse: 0.52076 | val_1_rmse: 0.51486 |  0:03:18s
epoch 91 | loss: 0.25251 | val_0_rmse: 0.48613 | val_1_rmse: 0.49171 |  0:03:20s
epoch 92 | loss: 0.24831 | val_0_rmse: 0.48576 | val_1_rmse: 0.4911  |  0:03:22s
epoch 93 | loss: 0.24669 | val_0_rmse: 0.5311  | val_1_rmse: 0.52734 |  0:03:24s
epoch 94 | loss: 0.2466  | val_0_rmse: 0.57029 | val_1_rmse: 0.55854 |  0:03:26s
epoch 95 | loss: 0.24494 | val_0_rmse: 0.49514 | val_1_rmse: 0.49878 |  0:03:28s
epoch 96 | loss: 0.24887 | val_0_rmse: 0.50039 | val_1_rmse: 0.49835 |  0:03:31s
epoch 97 | loss: 0.24603 | val_0_rmse: 0.53572 | val_1_rmse: 0.52801 |  0:03:33s
epoch 98 | loss: 0.24249 | val_0_rmse: 0.51655 | val_1_rmse: 0.51048 |  0:03:35s
epoch 99 | loss: 0.24328 | val_0_rmse: 0.49208 | val_1_rmse: 0.49175 |  0:03:37s
epoch 100| loss: 0.24424 | val_0_rmse: 0.54009 | val_1_rmse: 0.53787 |  0:03:39s
epoch 101| loss: 0.241   | val_0_rmse: 0.55594 | val_1_rmse: 0.5441  |  0:03:41s
epoch 102| loss: 0.24363 | val_0_rmse: 0.49163 | val_1_rmse: 0.49346 |  0:03:44s
epoch 103| loss: 0.24376 | val_0_rmse: 0.55411 | val_1_rmse: 0.54522 |  0:03:46s
epoch 104| loss: 0.2456  | val_0_rmse: 0.55139 | val_1_rmse: 0.54135 |  0:03:48s
epoch 105| loss: 0.24082 | val_0_rmse: 0.49903 | val_1_rmse: 0.50223 |  0:03:50s
epoch 106| loss: 0.24088 | val_0_rmse: 0.53291 | val_1_rmse: 0.52465 |  0:03:52s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.48764
Best weights from best epoch are automatically used!
ended training at: 02:20:47
Feature importance:
[('Area', 0.444429490221117), ('Baths', 0.11018733764353655), ('Beds', 0.08788847720628662), ('Latitude', 0.15093918031282885), ('Longitude', 0.032430063887624504), ('Month', 0.07730095059583426), ('Year', 0.0968245001327722)]
Mean squared error is of 993020919.7775975
Mean absolute error:21315.49329681752
MAPE:0.2583815896109169
R2 score:0.7577813619856822
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:20:47
epoch 0  | loss: 0.43223 | val_0_rmse: 0.54751 | val_1_rmse: 0.54517 |  0:00:03s
epoch 1  | loss: 0.28583 | val_0_rmse: 0.53769 | val_1_rmse: 0.54149 |  0:00:07s
epoch 2  | loss: 0.26491 | val_0_rmse: 0.47218 | val_1_rmse: 0.47477 |  0:00:10s
epoch 3  | loss: 0.24648 | val_0_rmse: 0.473   | val_1_rmse: 0.47374 |  0:00:14s
epoch 4  | loss: 0.24356 | val_0_rmse: 0.47512 | val_1_rmse: 0.47965 |  0:00:17s
epoch 5  | loss: 0.23619 | val_0_rmse: 0.46094 | val_1_rmse: 0.46533 |  0:00:21s
epoch 6  | loss: 0.23314 | val_0_rmse: 0.45689 | val_1_rmse: 0.46499 |  0:00:24s
epoch 7  | loss: 0.22454 | val_0_rmse: 0.45872 | val_1_rmse: 0.46627 |  0:00:28s
epoch 8  | loss: 0.22892 | val_0_rmse: 0.45906 | val_1_rmse: 0.4625  |  0:00:31s
epoch 9  | loss: 0.21896 | val_0_rmse: 0.46255 | val_1_rmse: 0.46947 |  0:00:35s
epoch 10 | loss: 0.22027 | val_0_rmse: 0.45401 | val_1_rmse: 0.46628 |  0:00:38s
epoch 11 | loss: 0.21332 | val_0_rmse: 0.43339 | val_1_rmse: 0.44175 |  0:00:42s
epoch 12 | loss: 0.21402 | val_0_rmse: 0.44848 | val_1_rmse: 0.45739 |  0:00:45s
epoch 13 | loss: 0.21419 | val_0_rmse: 0.45235 | val_1_rmse: 0.459   |  0:00:49s
epoch 14 | loss: 0.21354 | val_0_rmse: 0.43495 | val_1_rmse: 0.44555 |  0:00:52s
epoch 15 | loss: 0.21629 | val_0_rmse: 0.45759 | val_1_rmse: 0.46554 |  0:00:56s
epoch 16 | loss: 0.20712 | val_0_rmse: 0.42967 | val_1_rmse: 0.43868 |  0:00:59s
epoch 17 | loss: 0.20445 | val_0_rmse: 0.4368  | val_1_rmse: 0.44617 |  0:01:03s
epoch 18 | loss: 0.20101 | val_0_rmse: 0.43688 | val_1_rmse: 0.44231 |  0:01:06s
epoch 19 | loss: 0.20462 | val_0_rmse: 0.43904 | val_1_rmse: 0.44987 |  0:01:10s
epoch 20 | loss: 0.20931 | val_0_rmse: 0.43186 | val_1_rmse: 0.44047 |  0:01:13s
epoch 21 | loss: 0.19979 | val_0_rmse: 0.42381 | val_1_rmse: 0.43352 |  0:01:17s
epoch 22 | loss: 0.20523 | val_0_rmse: 0.43443 | val_1_rmse: 0.44425 |  0:01:20s
epoch 23 | loss: 0.19979 | val_0_rmse: 0.42568 | val_1_rmse: 0.43441 |  0:01:24s
epoch 24 | loss: 0.20826 | val_0_rmse: 0.44136 | val_1_rmse: 0.44893 |  0:01:27s
epoch 25 | loss: 0.20181 | val_0_rmse: 0.42596 | val_1_rmse: 0.43654 |  0:01:31s
epoch 26 | loss: 0.20168 | val_0_rmse: 0.46799 | val_1_rmse: 0.47627 |  0:01:34s
epoch 27 | loss: 0.21264 | val_0_rmse: 0.43368 | val_1_rmse: 0.44639 |  0:01:38s
epoch 28 | loss: 0.20289 | val_0_rmse: 0.43029 | val_1_rmse: 0.43971 |  0:01:41s
epoch 29 | loss: 0.20025 | val_0_rmse: 0.43282 | val_1_rmse: 0.44163 |  0:01:45s
epoch 30 | loss: 0.1983  | val_0_rmse: 0.44961 | val_1_rmse: 0.4571  |  0:01:48s
epoch 31 | loss: 0.20002 | val_0_rmse: 0.43271 | val_1_rmse: 0.44073 |  0:01:52s
epoch 32 | loss: 0.19458 | val_0_rmse: 0.42384 | val_1_rmse: 0.43403 |  0:01:55s
epoch 33 | loss: 0.19893 | val_0_rmse: 0.43666 | val_1_rmse: 0.44874 |  0:01:59s
epoch 34 | loss: 0.19715 | val_0_rmse: 0.44085 | val_1_rmse: 0.4537  |  0:02:02s
epoch 35 | loss: 0.1973  | val_0_rmse: 0.42152 | val_1_rmse: 0.43226 |  0:02:06s
epoch 36 | loss: 0.19685 | val_0_rmse: 0.46434 | val_1_rmse: 0.47119 |  0:02:09s
epoch 37 | loss: 0.19895 | val_0_rmse: 0.44016 | val_1_rmse: 0.44834 |  0:02:13s
epoch 38 | loss: 0.19415 | val_0_rmse: 0.41552 | val_1_rmse: 0.42817 |  0:02:16s
epoch 39 | loss: 0.19151 | val_0_rmse: 0.42254 | val_1_rmse: 0.43294 |  0:02:20s
epoch 40 | loss: 0.1962  | val_0_rmse: 0.43049 | val_1_rmse: 0.44045 |  0:02:23s
epoch 41 | loss: 0.19728 | val_0_rmse: 0.44833 | val_1_rmse: 0.4552  |  0:02:27s
epoch 42 | loss: 0.20383 | val_0_rmse: 0.4612  | val_1_rmse: 0.47407 |  0:02:30s
epoch 43 | loss: 0.19376 | val_0_rmse: 0.42529 | val_1_rmse: 0.43464 |  0:02:34s
epoch 44 | loss: 0.19512 | val_0_rmse: 0.42585 | val_1_rmse: 0.43727 |  0:02:37s
epoch 45 | loss: 0.19521 | val_0_rmse: 0.43385 | val_1_rmse: 0.44512 |  0:02:41s
epoch 46 | loss: 0.1936  | val_0_rmse: 0.4177  | val_1_rmse: 0.42955 |  0:02:44s
epoch 47 | loss: 0.19251 | val_0_rmse: 0.41909 | val_1_rmse: 0.4304  |  0:02:48s
epoch 48 | loss: 0.19354 | val_0_rmse: 0.42207 | val_1_rmse: 0.433   |  0:02:51s
epoch 49 | loss: 0.18961 | val_0_rmse: 0.41672 | val_1_rmse: 0.42876 |  0:02:55s
epoch 50 | loss: 0.19103 | val_0_rmse: 0.42267 | val_1_rmse: 0.43401 |  0:02:58s
epoch 51 | loss: 0.18727 | val_0_rmse: 0.41569 | val_1_rmse: 0.42765 |  0:03:02s
epoch 52 | loss: 0.19656 | val_0_rmse: 0.42786 | val_1_rmse: 0.44192 |  0:03:05s
epoch 53 | loss: 0.19366 | val_0_rmse: 0.43895 | val_1_rmse: 0.44936 |  0:03:09s
epoch 54 | loss: 0.18822 | val_0_rmse: 0.42492 | val_1_rmse: 0.43511 |  0:03:12s
epoch 55 | loss: 0.18735 | val_0_rmse: 0.41302 | val_1_rmse: 0.42747 |  0:03:16s
epoch 56 | loss: 0.18457 | val_0_rmse: 0.4132  | val_1_rmse: 0.42673 |  0:03:19s
epoch 57 | loss: 0.18469 | val_0_rmse: 0.41937 | val_1_rmse: 0.4334  |  0:03:23s
epoch 58 | loss: 0.1856  | val_0_rmse: 0.41179 | val_1_rmse: 0.42769 |  0:03:26s
epoch 59 | loss: 0.18682 | val_0_rmse: 0.41517 | val_1_rmse: 0.43016 |  0:03:30s
epoch 60 | loss: 0.18809 | val_0_rmse: 0.4104  | val_1_rmse: 0.42587 |  0:03:33s
epoch 61 | loss: 0.1856  | val_0_rmse: 0.42633 | val_1_rmse: 0.44112 |  0:03:37s
epoch 62 | loss: 0.18929 | val_0_rmse: 0.4297  | val_1_rmse: 0.44088 |  0:03:40s
epoch 63 | loss: 0.18842 | val_0_rmse: 0.42351 | val_1_rmse: 0.43637 |  0:03:44s
epoch 64 | loss: 0.18383 | val_0_rmse: 0.42602 | val_1_rmse: 0.43777 |  0:03:47s
epoch 65 | loss: 0.18909 | val_0_rmse: 0.41484 | val_1_rmse: 0.42858 |  0:03:51s
epoch 66 | loss: 0.18665 | val_0_rmse: 0.41217 | val_1_rmse: 0.42857 |  0:03:54s
epoch 67 | loss: 0.18584 | val_0_rmse: 0.40652 | val_1_rmse: 0.42433 |  0:03:58s
epoch 68 | loss: 0.18558 | val_0_rmse: 0.41828 | val_1_rmse: 0.42961 |  0:04:01s
epoch 69 | loss: 0.18563 | val_0_rmse: 0.42252 | val_1_rmse: 0.43669 |  0:04:05s
epoch 70 | loss: 0.18951 | val_0_rmse: 0.41301 | val_1_rmse: 0.42618 |  0:04:08s
epoch 71 | loss: 0.18393 | val_0_rmse: 0.4101  | val_1_rmse: 0.42616 |  0:04:12s
epoch 72 | loss: 0.18052 | val_0_rmse: 0.41188 | val_1_rmse: 0.42919 |  0:04:15s
epoch 73 | loss: 0.1837  | val_0_rmse: 0.4252  | val_1_rmse: 0.44455 |  0:04:19s
epoch 74 | loss: 0.18501 | val_0_rmse: 0.41326 | val_1_rmse: 0.42672 |  0:04:22s
epoch 75 | loss: 0.18219 | val_0_rmse: 0.40864 | val_1_rmse: 0.42215 |  0:04:26s
epoch 76 | loss: 0.18638 | val_0_rmse: 0.40941 | val_1_rmse: 0.42352 |  0:04:29s
epoch 77 | loss: 0.18054 | val_0_rmse: 0.41245 | val_1_rmse: 0.42641 |  0:04:33s
epoch 78 | loss: 0.184   | val_0_rmse: 0.41171 | val_1_rmse: 0.42578 |  0:04:36s
epoch 79 | loss: 0.18518 | val_0_rmse: 0.40934 | val_1_rmse: 0.42265 |  0:04:40s
epoch 80 | loss: 0.18315 | val_0_rmse: 0.40492 | val_1_rmse: 0.42305 |  0:04:43s
epoch 81 | loss: 0.17851 | val_0_rmse: 0.41212 | val_1_rmse: 0.42628 |  0:04:47s
epoch 82 | loss: 0.18037 | val_0_rmse: 0.40922 | val_1_rmse: 0.4252  |  0:04:50s
epoch 83 | loss: 0.1815  | val_0_rmse: 0.40484 | val_1_rmse: 0.42163 |  0:04:54s
epoch 84 | loss: 0.17761 | val_0_rmse: 0.39737 | val_1_rmse: 0.41506 |  0:04:57s
epoch 85 | loss: 0.18409 | val_0_rmse: 0.4057  | val_1_rmse: 0.42046 |  0:05:01s
epoch 86 | loss: 0.1822  | val_0_rmse: 0.41896 | val_1_rmse: 0.43813 |  0:05:04s
epoch 87 | loss: 0.1806  | val_0_rmse: 0.40704 | val_1_rmse: 0.42168 |  0:05:08s
epoch 88 | loss: 0.17969 | val_0_rmse: 0.40199 | val_1_rmse: 0.41826 |  0:05:11s
epoch 89 | loss: 0.17881 | val_0_rmse: 0.41106 | val_1_rmse: 0.42831 |  0:05:15s
epoch 90 | loss: 0.18126 | val_0_rmse: 0.41921 | val_1_rmse: 0.43356 |  0:05:18s
epoch 91 | loss: 0.18096 | val_0_rmse: 0.40798 | val_1_rmse: 0.42413 |  0:05:22s
epoch 92 | loss: 0.17753 | val_0_rmse: 0.40595 | val_1_rmse: 0.42407 |  0:05:26s
epoch 93 | loss: 0.18249 | val_0_rmse: 0.40869 | val_1_rmse: 0.4271  |  0:05:29s
epoch 94 | loss: 0.18008 | val_0_rmse: 0.42083 | val_1_rmse: 0.43738 |  0:05:33s
epoch 95 | loss: 0.18073 | val_0_rmse: 0.40665 | val_1_rmse: 0.42395 |  0:05:36s
epoch 96 | loss: 0.182   | val_0_rmse: 0.40971 | val_1_rmse: 0.42372 |  0:05:40s
epoch 97 | loss: 0.17777 | val_0_rmse: 0.40139 | val_1_rmse: 0.41888 |  0:05:43s
epoch 98 | loss: 0.17999 | val_0_rmse: 0.40305 | val_1_rmse: 0.41916 |  0:05:47s
epoch 99 | loss: 0.17749 | val_0_rmse: 0.40966 | val_1_rmse: 0.42765 |  0:05:50s
epoch 100| loss: 0.17962 | val_0_rmse: 0.40183 | val_1_rmse: 0.42295 |  0:05:54s
epoch 101| loss: 0.1795  | val_0_rmse: 0.40647 | val_1_rmse: 0.42431 |  0:05:57s
epoch 102| loss: 0.17928 | val_0_rmse: 0.39752 | val_1_rmse: 0.41726 |  0:06:01s
epoch 103| loss: 0.17974 | val_0_rmse: 0.41359 | val_1_rmse: 0.42633 |  0:06:04s
epoch 104| loss: 0.17834 | val_0_rmse: 0.404   | val_1_rmse: 0.42338 |  0:06:08s
epoch 105| loss: 0.17434 | val_0_rmse: 0.39525 | val_1_rmse: 0.41575 |  0:06:12s
epoch 106| loss: 0.17946 | val_0_rmse: 0.40238 | val_1_rmse: 0.41944 |  0:06:15s
epoch 107| loss: 0.17621 | val_0_rmse: 0.40512 | val_1_rmse: 0.42628 |  0:06:19s
epoch 108| loss: 0.17765 | val_0_rmse: 0.40269 | val_1_rmse: 0.42322 |  0:06:22s
epoch 109| loss: 0.1812  | val_0_rmse: 0.4081  | val_1_rmse: 0.42563 |  0:06:26s
epoch 110| loss: 0.17726 | val_0_rmse: 0.40196 | val_1_rmse: 0.422   |  0:06:29s
epoch 111| loss: 0.17766 | val_0_rmse: 0.39868 | val_1_rmse: 0.4176  |  0:06:33s
epoch 112| loss: 0.17701 | val_0_rmse: 0.41719 | val_1_rmse: 0.43292 |  0:06:36s
epoch 113| loss: 0.17911 | val_0_rmse: 0.40141 | val_1_rmse: 0.42092 |  0:06:40s
epoch 114| loss: 0.17601 | val_0_rmse: 0.40097 | val_1_rmse: 0.41883 |  0:06:43s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.41506
Best weights from best epoch are automatically used!
ended training at: 02:27:32
Feature importance:
[('Area', 0.10970804519931318), ('Baths', 0.058681506183044345), ('Beds', 0.12602156080776344), ('Latitude', 0.19621352252947966), ('Longitude', 0.2557077580624996), ('Month', 0.0), ('Year', 0.25366760721789977)]
Mean squared error is of 9865604268.486237
Mean absolute error:68028.8754719721
MAPE:0.2685425452113265
R2 score:0.8283019846393679
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:27:33
epoch 0  | loss: 0.43044 | val_0_rmse: 0.5285  | val_1_rmse: 0.53706 |  0:00:03s
epoch 1  | loss: 0.27936 | val_0_rmse: 0.48249 | val_1_rmse: 0.48893 |  0:00:07s
epoch 2  | loss: 0.24595 | val_0_rmse: 0.491   | val_1_rmse: 0.49855 |  0:00:10s
epoch 3  | loss: 0.2393  | val_0_rmse: 0.51825 | val_1_rmse: 0.52524 |  0:00:14s
epoch 4  | loss: 0.23326 | val_0_rmse: 0.45524 | val_1_rmse: 0.46272 |  0:00:17s
epoch 5  | loss: 0.22909 | val_0_rmse: 0.46378 | val_1_rmse: 0.47164 |  0:00:21s
epoch 6  | loss: 0.22925 | val_0_rmse: 0.50187 | val_1_rmse: 0.51236 |  0:00:24s
epoch 7  | loss: 0.24504 | val_0_rmse: 0.47812 | val_1_rmse: 0.48949 |  0:00:28s
epoch 8  | loss: 0.22772 | val_0_rmse: 0.45717 | val_1_rmse: 0.46609 |  0:00:31s
epoch 9  | loss: 0.2225  | val_0_rmse: 0.45204 | val_1_rmse: 0.46185 |  0:00:35s
epoch 10 | loss: 0.22609 | val_0_rmse: 0.47252 | val_1_rmse: 0.47846 |  0:00:38s
epoch 11 | loss: 0.22477 | val_0_rmse: 0.46399 | val_1_rmse: 0.46908 |  0:00:42s
epoch 12 | loss: 0.22254 | val_0_rmse: 0.46031 | val_1_rmse: 0.46642 |  0:00:45s
epoch 13 | loss: 0.2162  | val_0_rmse: 0.43574 | val_1_rmse: 0.44473 |  0:00:49s
epoch 14 | loss: 0.21093 | val_0_rmse: 0.45294 | val_1_rmse: 0.46059 |  0:00:52s
epoch 15 | loss: 0.20603 | val_0_rmse: 0.44695 | val_1_rmse: 0.4536  |  0:00:56s
epoch 16 | loss: 0.20732 | val_0_rmse: 0.43695 | val_1_rmse: 0.44296 |  0:00:59s
epoch 17 | loss: 0.20577 | val_0_rmse: 0.43105 | val_1_rmse: 0.43824 |  0:01:03s
epoch 18 | loss: 0.20731 | val_0_rmse: 0.44659 | val_1_rmse: 0.45567 |  0:01:06s
epoch 19 | loss: 0.20287 | val_0_rmse: 0.43969 | val_1_rmse: 0.44705 |  0:01:10s
epoch 20 | loss: 0.21487 | val_0_rmse: 0.53391 | val_1_rmse: 0.53944 |  0:01:13s
epoch 21 | loss: 0.2314  | val_0_rmse: 0.4491  | val_1_rmse: 0.45736 |  0:01:17s
epoch 22 | loss: 0.22419 | val_0_rmse: 0.44636 | val_1_rmse: 0.45497 |  0:01:20s
epoch 23 | loss: 0.21595 | val_0_rmse: 0.44571 | val_1_rmse: 0.45532 |  0:01:24s
epoch 24 | loss: 0.21293 | val_0_rmse: 0.45225 | val_1_rmse: 0.45938 |  0:01:27s
epoch 25 | loss: 0.21216 | val_0_rmse: 0.44978 | val_1_rmse: 0.4576  |  0:01:31s
epoch 26 | loss: 0.21025 | val_0_rmse: 0.42364 | val_1_rmse: 0.43295 |  0:01:34s
epoch 27 | loss: 0.20097 | val_0_rmse: 0.42963 | val_1_rmse: 0.43825 |  0:01:38s
epoch 28 | loss: 0.20523 | val_0_rmse: 0.42774 | val_1_rmse: 0.43585 |  0:01:41s
epoch 29 | loss: 0.20056 | val_0_rmse: 0.4383  | val_1_rmse: 0.44644 |  0:01:45s
epoch 30 | loss: 0.20327 | val_0_rmse: 0.43352 | val_1_rmse: 0.44046 |  0:01:48s
epoch 31 | loss: 0.20221 | val_0_rmse: 0.43212 | val_1_rmse: 0.4404  |  0:01:52s
epoch 32 | loss: 0.20424 | val_0_rmse: 0.44382 | val_1_rmse: 0.45161 |  0:01:55s
epoch 33 | loss: 0.21036 | val_0_rmse: 0.44209 | val_1_rmse: 0.45058 |  0:01:59s
epoch 34 | loss: 0.20882 | val_0_rmse: 0.45062 | val_1_rmse: 0.45766 |  0:02:02s
epoch 35 | loss: 0.20945 | val_0_rmse: 0.44527 | val_1_rmse: 0.45622 |  0:02:06s
epoch 36 | loss: 0.20936 | val_0_rmse: 0.44398 | val_1_rmse: 0.45448 |  0:02:09s
epoch 37 | loss: 0.21587 | val_0_rmse: 0.47687 | val_1_rmse: 0.48377 |  0:02:13s
epoch 38 | loss: 0.21396 | val_0_rmse: 0.45112 | val_1_rmse: 0.46234 |  0:02:17s
epoch 39 | loss: 0.23667 | val_0_rmse: 0.47012 | val_1_rmse: 0.47505 |  0:02:20s
epoch 40 | loss: 0.22112 | val_0_rmse: 0.45285 | val_1_rmse: 0.46151 |  0:02:24s
epoch 41 | loss: 0.2108  | val_0_rmse: 0.43211 | val_1_rmse: 0.43779 |  0:02:27s
epoch 42 | loss: 0.21461 | val_0_rmse: 0.46057 | val_1_rmse: 0.46696 |  0:02:30s
epoch 43 | loss: 0.22348 | val_0_rmse: 0.44876 | val_1_rmse: 0.45531 |  0:02:34s
epoch 44 | loss: 0.21124 | val_0_rmse: 0.4308  | val_1_rmse: 0.4373  |  0:02:37s
epoch 45 | loss: 0.20146 | val_0_rmse: 0.44648 | val_1_rmse: 0.45195 |  0:02:41s
epoch 46 | loss: 0.20098 | val_0_rmse: 0.45386 | val_1_rmse: 0.46135 |  0:02:44s
epoch 47 | loss: 0.19888 | val_0_rmse: 0.41916 | val_1_rmse: 0.42954 |  0:02:48s
epoch 48 | loss: 0.19889 | val_0_rmse: 0.43862 | val_1_rmse: 0.44687 |  0:02:52s
epoch 49 | loss: 0.19833 | val_0_rmse: 0.43268 | val_1_rmse: 0.44486 |  0:02:55s
epoch 50 | loss: 0.20844 | val_0_rmse: 0.4378  | val_1_rmse: 0.44528 |  0:02:59s
epoch 51 | loss: 0.20593 | val_0_rmse: 0.42711 | val_1_rmse: 0.43721 |  0:03:02s
epoch 52 | loss: 0.19718 | val_0_rmse: 0.43904 | val_1_rmse: 0.44653 |  0:03:06s
epoch 53 | loss: 0.21717 | val_0_rmse: 0.44066 | val_1_rmse: 0.44799 |  0:03:09s
epoch 54 | loss: 0.20222 | val_0_rmse: 0.44624 | val_1_rmse: 0.45541 |  0:03:13s
epoch 55 | loss: 0.19851 | val_0_rmse: 0.43695 | val_1_rmse: 0.44696 |  0:03:16s
epoch 56 | loss: 0.19853 | val_0_rmse: 0.43295 | val_1_rmse: 0.43965 |  0:03:20s
epoch 57 | loss: 0.20095 | val_0_rmse: 0.4402  | val_1_rmse: 0.44987 |  0:03:23s
epoch 58 | loss: 0.19592 | val_0_rmse: 0.43475 | val_1_rmse: 0.44467 |  0:03:27s
epoch 59 | loss: 0.19547 | val_0_rmse: 0.44112 | val_1_rmse: 0.4487  |  0:03:30s
epoch 60 | loss: 0.19249 | val_0_rmse: 0.44122 | val_1_rmse: 0.45061 |  0:03:34s
epoch 61 | loss: 0.19553 | val_0_rmse: 0.43781 | val_1_rmse: 0.44407 |  0:03:37s
epoch 62 | loss: 0.19847 | val_0_rmse: 0.44237 | val_1_rmse: 0.45297 |  0:03:41s
epoch 63 | loss: 0.19479 | val_0_rmse: 0.44131 | val_1_rmse: 0.45172 |  0:03:44s
epoch 64 | loss: 0.1977  | val_0_rmse: 0.42959 | val_1_rmse: 0.44018 |  0:03:48s
epoch 65 | loss: 0.19216 | val_0_rmse: 0.41751 | val_1_rmse: 0.42706 |  0:03:51s
epoch 66 | loss: 0.19095 | val_0_rmse: 0.42189 | val_1_rmse: 0.43086 |  0:03:55s
epoch 67 | loss: 0.18883 | val_0_rmse: 0.41454 | val_1_rmse: 0.42697 |  0:03:58s
epoch 68 | loss: 0.19296 | val_0_rmse: 0.41861 | val_1_rmse: 0.43041 |  0:04:02s
epoch 69 | loss: 0.18735 | val_0_rmse: 0.42181 | val_1_rmse: 0.43142 |  0:04:05s
epoch 70 | loss: 0.18969 | val_0_rmse: 0.41641 | val_1_rmse: 0.42621 |  0:04:09s
epoch 71 | loss: 0.18966 | val_0_rmse: 0.40992 | val_1_rmse: 0.42156 |  0:04:12s
epoch 72 | loss: 0.18797 | val_0_rmse: 0.41489 | val_1_rmse: 0.42668 |  0:04:16s
epoch 73 | loss: 0.18986 | val_0_rmse: 0.41399 | val_1_rmse: 0.4277  |  0:04:19s
epoch 74 | loss: 0.19076 | val_0_rmse: 0.45106 | val_1_rmse: 0.45984 |  0:04:23s
epoch 75 | loss: 0.19085 | val_0_rmse: 0.45965 | val_1_rmse: 0.47032 |  0:04:26s
epoch 76 | loss: 0.18736 | val_0_rmse: 0.42351 | val_1_rmse: 0.43351 |  0:04:30s
epoch 77 | loss: 0.18659 | val_0_rmse: 0.41895 | val_1_rmse: 0.43137 |  0:04:33s
epoch 78 | loss: 0.18957 | val_0_rmse: 0.41737 | val_1_rmse: 0.42935 |  0:04:37s
epoch 79 | loss: 0.18892 | val_0_rmse: 0.41619 | val_1_rmse: 0.42805 |  0:04:40s
epoch 80 | loss: 0.19727 | val_0_rmse: 0.4428  | val_1_rmse: 0.45206 |  0:04:44s
epoch 81 | loss: 0.19452 | val_0_rmse: 0.43419 | val_1_rmse: 0.4474  |  0:04:47s
epoch 82 | loss: 0.19499 | val_0_rmse: 0.43075 | val_1_rmse: 0.44094 |  0:04:51s
epoch 83 | loss: 0.18905 | val_0_rmse: 0.41778 | val_1_rmse: 0.42934 |  0:04:54s
epoch 84 | loss: 0.19767 | val_0_rmse: 0.43015 | val_1_rmse: 0.44296 |  0:04:58s
epoch 85 | loss: 0.19585 | val_0_rmse: 0.45964 | val_1_rmse: 0.46823 |  0:05:01s
epoch 86 | loss: 0.20352 | val_0_rmse: 0.45465 | val_1_rmse: 0.46305 |  0:05:05s
epoch 87 | loss: 0.25467 | val_0_rmse: 0.53292 | val_1_rmse: 0.53644 |  0:05:08s
epoch 88 | loss: 0.21673 | val_0_rmse: 0.4741  | val_1_rmse: 0.44989 |  0:05:12s
epoch 89 | loss: 0.2077  | val_0_rmse: 0.5018  | val_1_rmse: 0.49458 |  0:05:16s
epoch 90 | loss: 0.20182 | val_0_rmse: 0.43907 | val_1_rmse: 0.445   |  0:05:19s
epoch 91 | loss: 0.19821 | val_0_rmse: 0.43688 | val_1_rmse: 0.44685 |  0:05:23s
epoch 92 | loss: 0.19658 | val_0_rmse: 0.42083 | val_1_rmse: 0.43169 |  0:05:26s
epoch 93 | loss: 0.19    | val_0_rmse: 0.44802 | val_1_rmse: 0.45753 |  0:05:30s
epoch 94 | loss: 0.18741 | val_0_rmse: 0.41148 | val_1_rmse: 0.42481 |  0:05:33s
epoch 95 | loss: 0.19256 | val_0_rmse: 0.41895 | val_1_rmse: 0.43085 |  0:05:37s
epoch 96 | loss: 0.18734 | val_0_rmse: 0.44975 | val_1_rmse: 0.45942 |  0:05:40s
epoch 97 | loss: 0.18813 | val_0_rmse: 0.41362 | val_1_rmse: 0.42504 |  0:05:44s
epoch 98 | loss: 0.18763 | val_0_rmse: 0.41096 | val_1_rmse: 0.42242 |  0:05:47s
epoch 99 | loss: 0.18664 | val_0_rmse: 0.43437 | val_1_rmse: 0.44363 |  0:05:51s
epoch 100| loss: 0.18679 | val_0_rmse: 0.43632 | val_1_rmse: 0.44563 |  0:05:54s
epoch 101| loss: 0.18673 | val_0_rmse: 0.42114 | val_1_rmse: 0.43441 |  0:05:58s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.42156
Best weights from best epoch are automatically used!
ended training at: 02:33:32
Feature importance:
[('Area', 0.0), ('Baths', 0.0), ('Beds', 0.22320828788597064), ('Latitude', 0.17477350862862784), ('Longitude', 0.26943468537577325), ('Month', 0.09412877694760277), ('Year', 0.23845474116202553)]
Mean squared error is of 9915154251.01539
Mean absolute error:68584.01748586752
MAPE:0.26868651387703213
R2 score:0.8319684756977432
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:33:32
epoch 0  | loss: 0.4586  | val_0_rmse: 0.56152 | val_1_rmse: 0.55674 |  0:00:03s
epoch 1  | loss: 0.28897 | val_0_rmse: 0.5156  | val_1_rmse: 0.51667 |  0:00:07s
epoch 2  | loss: 0.26409 | val_0_rmse: 0.50837 | val_1_rmse: 0.50916 |  0:00:10s
epoch 3  | loss: 0.25493 | val_0_rmse: 0.48515 | val_1_rmse: 0.48663 |  0:00:13s
epoch 4  | loss: 0.23823 | val_0_rmse: 0.45798 | val_1_rmse: 0.45922 |  0:00:17s
epoch 5  | loss: 0.22764 | val_0_rmse: 0.46686 | val_1_rmse: 0.46791 |  0:00:20s
epoch 6  | loss: 0.23127 | val_0_rmse: 0.4697  | val_1_rmse: 0.46942 |  0:00:24s
epoch 7  | loss: 0.22269 | val_0_rmse: 0.46044 | val_1_rmse: 0.46158 |  0:00:27s
epoch 8  | loss: 0.21919 | val_0_rmse: 0.45485 | val_1_rmse: 0.45714 |  0:00:31s
epoch 9  | loss: 0.21491 | val_0_rmse: 0.43719 | val_1_rmse: 0.43865 |  0:00:34s
epoch 10 | loss: 0.22353 | val_0_rmse: 0.44062 | val_1_rmse: 0.44505 |  0:00:38s
epoch 11 | loss: 0.21275 | val_0_rmse: 0.44039 | val_1_rmse: 0.44237 |  0:00:41s
epoch 12 | loss: 0.21508 | val_0_rmse: 0.47013 | val_1_rmse: 0.47534 |  0:00:45s
epoch 13 | loss: 0.21636 | val_0_rmse: 0.43572 | val_1_rmse: 0.4385  |  0:00:48s
epoch 14 | loss: 0.20931 | val_0_rmse: 0.43232 | val_1_rmse: 0.43367 |  0:00:52s
epoch 15 | loss: 0.20596 | val_0_rmse: 0.45322 | val_1_rmse: 0.45426 |  0:00:55s
epoch 16 | loss: 0.21035 | val_0_rmse: 0.43544 | val_1_rmse: 0.44066 |  0:00:59s
epoch 17 | loss: 0.20299 | val_0_rmse: 0.44184 | val_1_rmse: 0.44808 |  0:01:02s
epoch 18 | loss: 0.20512 | val_0_rmse: 0.46614 | val_1_rmse: 0.47014 |  0:01:06s
epoch 19 | loss: 0.2036  | val_0_rmse: 0.45205 | val_1_rmse: 0.45921 |  0:01:09s
epoch 20 | loss: 0.20173 | val_0_rmse: 0.43081 | val_1_rmse: 0.43613 |  0:01:13s
epoch 21 | loss: 0.20105 | val_0_rmse: 0.43765 | val_1_rmse: 0.44291 |  0:01:16s
epoch 22 | loss: 0.1993  | val_0_rmse: 0.43006 | val_1_rmse: 0.43411 |  0:01:20s
epoch 23 | loss: 0.19835 | val_0_rmse: 0.42659 | val_1_rmse: 0.42989 |  0:01:23s
epoch 24 | loss: 0.20115 | val_0_rmse: 0.44852 | val_1_rmse: 0.45332 |  0:01:27s
epoch 25 | loss: 0.20054 | val_0_rmse: 0.42623 | val_1_rmse: 0.43161 |  0:01:30s
epoch 26 | loss: 0.19749 | val_0_rmse: 0.45516 | val_1_rmse: 0.45925 |  0:01:34s
epoch 27 | loss: 0.19799 | val_0_rmse: 0.45545 | val_1_rmse: 0.45941 |  0:01:37s
epoch 28 | loss: 0.19805 | val_0_rmse: 0.42401 | val_1_rmse: 0.42907 |  0:01:41s
epoch 29 | loss: 0.19723 | val_0_rmse: 0.42    | val_1_rmse: 0.42539 |  0:01:44s
epoch 30 | loss: 0.19588 | val_0_rmse: 0.42605 | val_1_rmse: 0.43118 |  0:01:48s
epoch 31 | loss: 0.19675 | val_0_rmse: 0.45008 | val_1_rmse: 0.45278 |  0:01:51s
epoch 32 | loss: 0.1968  | val_0_rmse: 0.43973 | val_1_rmse: 0.444   |  0:01:55s
epoch 33 | loss: 0.19668 | val_0_rmse: 0.42075 | val_1_rmse: 0.43053 |  0:01:58s
epoch 34 | loss: 0.19179 | val_0_rmse: 0.42536 | val_1_rmse: 0.4303  |  0:02:02s
epoch 35 | loss: 0.19412 | val_0_rmse: 0.42098 | val_1_rmse: 0.43025 |  0:02:05s
epoch 36 | loss: 0.19545 | val_0_rmse: 0.42804 | val_1_rmse: 0.43375 |  0:02:09s
epoch 37 | loss: 0.19287 | val_0_rmse: 0.42465 | val_1_rmse: 0.43238 |  0:02:13s
epoch 38 | loss: 0.19468 | val_0_rmse: 0.42954 | val_1_rmse: 0.43481 |  0:02:16s
epoch 39 | loss: 0.18949 | val_0_rmse: 0.4361  | val_1_rmse: 0.44376 |  0:02:20s
epoch 40 | loss: 0.19498 | val_0_rmse: 0.42943 | val_1_rmse: 0.43684 |  0:02:23s
epoch 41 | loss: 0.1932  | val_0_rmse: 0.4277  | val_1_rmse: 0.43403 |  0:02:27s
epoch 42 | loss: 0.19468 | val_0_rmse: 0.42188 | val_1_rmse: 0.42778 |  0:02:30s
epoch 43 | loss: 0.19304 | val_0_rmse: 0.42182 | val_1_rmse: 0.42878 |  0:02:34s
epoch 44 | loss: 0.19536 | val_0_rmse: 0.41701 | val_1_rmse: 0.42444 |  0:02:37s
epoch 45 | loss: 0.19124 | val_0_rmse: 0.42688 | val_1_rmse: 0.43338 |  0:02:41s
epoch 46 | loss: 0.19312 | val_0_rmse: 0.41892 | val_1_rmse: 0.4265  |  0:02:44s
epoch 47 | loss: 0.1887  | val_0_rmse: 0.4269  | val_1_rmse: 0.43337 |  0:02:48s
epoch 48 | loss: 0.19161 | val_0_rmse: 0.42568 | val_1_rmse: 0.436   |  0:02:51s
epoch 49 | loss: 0.19375 | val_0_rmse: 0.43446 | val_1_rmse: 0.44285 |  0:02:55s
epoch 50 | loss: 0.19384 | val_0_rmse: 0.41447 | val_1_rmse: 0.42262 |  0:02:58s
epoch 51 | loss: 0.18914 | val_0_rmse: 0.42032 | val_1_rmse: 0.42878 |  0:03:02s
epoch 52 | loss: 0.18734 | val_0_rmse: 0.4329  | val_1_rmse: 0.44123 |  0:03:05s
epoch 53 | loss: 0.19309 | val_0_rmse: 0.42211 | val_1_rmse: 0.43117 |  0:03:09s
epoch 54 | loss: 0.18782 | val_0_rmse: 0.41228 | val_1_rmse: 0.42263 |  0:03:12s
epoch 55 | loss: 0.18976 | val_0_rmse: 0.42178 | val_1_rmse: 0.42979 |  0:03:16s
epoch 56 | loss: 0.1842  | val_0_rmse: 0.41583 | val_1_rmse: 0.42468 |  0:03:19s
epoch 57 | loss: 0.19088 | val_0_rmse: 0.4269  | val_1_rmse: 0.43512 |  0:03:23s
epoch 58 | loss: 0.1949  | val_0_rmse: 0.41952 | val_1_rmse: 0.42997 |  0:03:26s
epoch 59 | loss: 0.20662 | val_0_rmse: 0.4513  | val_1_rmse: 0.45586 |  0:03:30s
epoch 60 | loss: 0.21129 | val_0_rmse: 0.45409 | val_1_rmse: 0.45707 |  0:03:33s
epoch 61 | loss: 0.24695 | val_0_rmse: 0.48976 | val_1_rmse: 0.49079 |  0:03:37s
epoch 62 | loss: 0.22838 | val_0_rmse: 0.5177  | val_1_rmse: 0.51813 |  0:03:40s
epoch 63 | loss: 0.21458 | val_0_rmse: 0.4561  | val_1_rmse: 0.45825 |  0:03:44s
epoch 64 | loss: 0.21403 | val_0_rmse: 0.47296 | val_1_rmse: 0.47297 |  0:03:47s
epoch 65 | loss: 0.20914 | val_0_rmse: 0.44304 | val_1_rmse: 0.44408 |  0:03:51s
epoch 66 | loss: 0.20187 | val_0_rmse: 0.42712 | val_1_rmse: 0.4315  |  0:03:55s
epoch 67 | loss: 0.20604 | val_0_rmse: 0.42424 | val_1_rmse: 0.42744 |  0:03:58s
epoch 68 | loss: 0.20056 | val_0_rmse: 0.43515 | val_1_rmse: 0.43992 |  0:04:01s
epoch 69 | loss: 0.19632 | val_0_rmse: 0.43003 | val_1_rmse: 0.43272 |  0:04:05s
epoch 70 | loss: 0.1942  | val_0_rmse: 0.42536 | val_1_rmse: 0.43052 |  0:04:09s
epoch 71 | loss: 0.19699 | val_0_rmse: 0.43138 | val_1_rmse: 0.4336  |  0:04:12s
epoch 72 | loss: 0.1939  | val_0_rmse: 0.41985 | val_1_rmse: 0.42447 |  0:04:16s
epoch 73 | loss: 0.19448 | val_0_rmse: 0.4412  | val_1_rmse: 0.44775 |  0:04:19s
epoch 74 | loss: 0.19321 | val_0_rmse: 0.42672 | val_1_rmse: 0.43445 |  0:04:23s
epoch 75 | loss: 0.19317 | val_0_rmse: 0.43031 | val_1_rmse: 0.43791 |  0:04:26s
epoch 76 | loss: 0.19324 | val_0_rmse: 0.41948 | val_1_rmse: 0.42751 |  0:04:30s
epoch 77 | loss: 0.18691 | val_0_rmse: 0.41035 | val_1_rmse: 0.41751 |  0:04:33s
epoch 78 | loss: 0.18982 | val_0_rmse: 0.46815 | val_1_rmse: 0.47256 |  0:04:37s
epoch 79 | loss: 0.18877 | val_0_rmse: 0.41232 | val_1_rmse: 0.41948 |  0:04:40s
epoch 80 | loss: 0.18567 | val_0_rmse: 0.4293  | val_1_rmse: 0.43668 |  0:04:44s
epoch 81 | loss: 0.18831 | val_0_rmse: 0.42821 | val_1_rmse: 0.43361 |  0:04:47s
epoch 82 | loss: 0.18881 | val_0_rmse: 0.42381 | val_1_rmse: 0.43007 |  0:04:51s
epoch 83 | loss: 0.18497 | val_0_rmse: 0.41348 | val_1_rmse: 0.42093 |  0:04:54s
epoch 84 | loss: 0.18411 | val_0_rmse: 0.41382 | val_1_rmse: 0.4221  |  0:04:58s
epoch 85 | loss: 0.18939 | val_0_rmse: 0.42763 | val_1_rmse: 0.43278 |  0:05:01s
epoch 86 | loss: 0.18617 | val_0_rmse: 0.43694 | val_1_rmse: 0.4432  |  0:05:05s
epoch 87 | loss: 0.18336 | val_0_rmse: 0.40906 | val_1_rmse: 0.4188  |  0:05:08s
epoch 88 | loss: 0.18327 | val_0_rmse: 0.42368 | val_1_rmse: 0.43013 |  0:05:12s
epoch 89 | loss: 0.18336 | val_0_rmse: 0.41408 | val_1_rmse: 0.42354 |  0:05:15s
epoch 90 | loss: 0.18331 | val_0_rmse: 0.41371 | val_1_rmse: 0.42231 |  0:05:19s
epoch 91 | loss: 0.18868 | val_0_rmse: 0.41464 | val_1_rmse: 0.42248 |  0:05:22s
epoch 92 | loss: 0.18776 | val_0_rmse: 0.42483 | val_1_rmse: 0.43227 |  0:05:26s
epoch 93 | loss: 0.18555 | val_0_rmse: 0.42542 | val_1_rmse: 0.4331  |  0:05:30s
epoch 94 | loss: 0.18377 | val_0_rmse: 0.42334 | val_1_rmse: 0.43319 |  0:05:33s
epoch 95 | loss: 0.18641 | val_0_rmse: 0.43224 | val_1_rmse: 0.4405  |  0:05:37s
epoch 96 | loss: 0.18519 | val_0_rmse: 0.42231 | val_1_rmse: 0.43319 |  0:05:40s
epoch 97 | loss: 0.1829  | val_0_rmse: 0.43636 | val_1_rmse: 0.44335 |  0:05:44s
epoch 98 | loss: 0.17917 | val_0_rmse: 0.4135  | val_1_rmse: 0.42237 |  0:05:47s
epoch 99 | loss: 0.17975 | val_0_rmse: 0.40963 | val_1_rmse: 0.41788 |  0:05:51s
epoch 100| loss: 0.17882 | val_0_rmse: 0.41941 | val_1_rmse: 0.42841 |  0:05:54s
epoch 101| loss: 0.181   | val_0_rmse: 0.40877 | val_1_rmse: 0.42058 |  0:05:58s
epoch 102| loss: 0.18359 | val_0_rmse: 0.41345 | val_1_rmse: 0.42361 |  0:06:01s
epoch 103| loss: 0.18106 | val_0_rmse: 0.41395 | val_1_rmse: 0.42647 |  0:06:05s
epoch 104| loss: 0.17723 | val_0_rmse: 0.40672 | val_1_rmse: 0.41713 |  0:06:08s
epoch 105| loss: 0.17797 | val_0_rmse: 0.4076  | val_1_rmse: 0.41781 |  0:06:12s
epoch 106| loss: 0.18444 | val_0_rmse: 0.42483 | val_1_rmse: 0.43307 |  0:06:15s
epoch 107| loss: 0.1814  | val_0_rmse: 0.40725 | val_1_rmse: 0.41513 |  0:06:19s
epoch 108| loss: 0.18072 | val_0_rmse: 0.40847 | val_1_rmse: 0.41804 |  0:06:22s
epoch 109| loss: 0.17669 | val_0_rmse: 0.41563 | val_1_rmse: 0.42862 |  0:06:26s
epoch 110| loss: 0.17909 | val_0_rmse: 0.40811 | val_1_rmse: 0.41883 |  0:06:29s
epoch 111| loss: 0.17795 | val_0_rmse: 0.40759 | val_1_rmse: 0.41798 |  0:06:33s
epoch 112| loss: 0.1764  | val_0_rmse: 0.41834 | val_1_rmse: 0.42806 |  0:06:36s
epoch 113| loss: 0.17989 | val_0_rmse: 0.40665 | val_1_rmse: 0.41741 |  0:06:40s
epoch 114| loss: 0.18098 | val_0_rmse: 0.40319 | val_1_rmse: 0.41492 |  0:06:43s
epoch 115| loss: 0.17713 | val_0_rmse: 0.41645 | val_1_rmse: 0.4275  |  0:06:47s
epoch 116| loss: 0.17886 | val_0_rmse: 0.40316 | val_1_rmse: 0.41596 |  0:06:50s
epoch 117| loss: 0.1753  | val_0_rmse: 0.40636 | val_1_rmse: 0.4185  |  0:06:54s
epoch 118| loss: 0.18002 | val_0_rmse: 0.40823 | val_1_rmse: 0.41792 |  0:06:57s
epoch 119| loss: 0.17995 | val_0_rmse: 0.40524 | val_1_rmse: 0.41902 |  0:07:01s
epoch 120| loss: 0.17502 | val_0_rmse: 0.40118 | val_1_rmse: 0.4146  |  0:07:04s
epoch 121| loss: 0.17512 | val_0_rmse: 0.42094 | val_1_rmse: 0.43474 |  0:07:08s
epoch 122| loss: 0.17878 | val_0_rmse: 0.40391 | val_1_rmse: 0.41449 |  0:07:12s
epoch 123| loss: 0.17593 | val_0_rmse: 0.40244 | val_1_rmse: 0.41462 |  0:07:15s
epoch 124| loss: 0.17682 | val_0_rmse: 0.41391 | val_1_rmse: 0.42612 |  0:07:19s
epoch 125| loss: 0.17721 | val_0_rmse: 0.41271 | val_1_rmse: 0.42711 |  0:07:22s
epoch 126| loss: 0.1762  | val_0_rmse: 0.40229 | val_1_rmse: 0.41315 |  0:07:26s
epoch 127| loss: 0.1802  | val_0_rmse: 0.42596 | val_1_rmse: 0.43781 |  0:07:29s
epoch 128| loss: 0.17851 | val_0_rmse: 0.41278 | val_1_rmse: 0.42267 |  0:07:33s
epoch 129| loss: 0.17515 | val_0_rmse: 0.41118 | val_1_rmse: 0.42228 |  0:07:36s
epoch 130| loss: 0.18148 | val_0_rmse: 0.42186 | val_1_rmse: 0.43026 |  0:07:40s
epoch 131| loss: 0.17975 | val_0_rmse: 0.40378 | val_1_rmse: 0.41376 |  0:07:43s
epoch 132| loss: 0.17384 | val_0_rmse: 0.41114 | val_1_rmse: 0.42369 |  0:07:47s
epoch 133| loss: 0.17863 | val_0_rmse: 0.41372 | val_1_rmse: 0.42516 |  0:07:50s
epoch 134| loss: 0.17635 | val_0_rmse: 0.41314 | val_1_rmse: 0.42419 |  0:07:54s
epoch 135| loss: 0.17729 | val_0_rmse: 0.4056  | val_1_rmse: 0.41719 |  0:07:57s
epoch 136| loss: 0.17526 | val_0_rmse: 0.41436 | val_1_rmse: 0.42382 |  0:08:01s
epoch 137| loss: 0.17586 | val_0_rmse: 0.39987 | val_1_rmse: 0.41137 |  0:08:04s
epoch 138| loss: 0.17377 | val_0_rmse: 0.39749 | val_1_rmse: 0.41144 |  0:08:08s
epoch 139| loss: 0.17604 | val_0_rmse: 0.41175 | val_1_rmse: 0.4237  |  0:08:11s
epoch 140| loss: 0.17346 | val_0_rmse: 0.45552 | val_1_rmse: 0.46611 |  0:08:15s
epoch 141| loss: 0.17774 | val_0_rmse: 0.39396 | val_1_rmse: 0.40807 |  0:08:18s
epoch 142| loss: 0.17551 | val_0_rmse: 0.4009  | val_1_rmse: 0.41445 |  0:08:22s
epoch 143| loss: 0.17116 | val_0_rmse: 0.39789 | val_1_rmse: 0.41086 |  0:08:25s
epoch 144| loss: 0.17269 | val_0_rmse: 0.39913 | val_1_rmse: 0.4112  |  0:08:29s
epoch 145| loss: 0.17292 | val_0_rmse: 0.42919 | val_1_rmse: 0.44197 |  0:08:32s
epoch 146| loss: 0.17344 | val_0_rmse: 0.43371 | val_1_rmse: 0.44719 |  0:08:36s
epoch 147| loss: 0.17567 | val_0_rmse: 0.41599 | val_1_rmse: 0.42457 |  0:08:39s
epoch 148| loss: 0.1769  | val_0_rmse: 0.40574 | val_1_rmse: 0.41591 |  0:08:43s
epoch 149| loss: 0.17667 | val_0_rmse: 0.40302 | val_1_rmse: 0.41766 |  0:08:47s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.40807
Best weights from best epoch are automatically used!
ended training at: 02:42:20
Feature importance:
[('Area', 0.04828079672667552), ('Baths', 0.056390994207370954), ('Beds', 0.14378497645505464), ('Latitude', 0.1108956783511459), ('Longitude', 0.24541585407240749), ('Month', 0.054168261600251684), ('Year', 0.3410634385870938)]
Mean squared error is of 9440955108.468178
Mean absolute error:67077.87688905811
MAPE:0.27520776302481903
R2 score:0.8375982107118749
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:42:21
epoch 0  | loss: 0.4421  | val_0_rmse: 0.54839 | val_1_rmse: 0.55864 |  0:00:03s
epoch 1  | loss: 0.29498 | val_0_rmse: 0.51496 | val_1_rmse: 0.53164 |  0:00:07s
epoch 2  | loss: 0.26119 | val_0_rmse: 0.51027 | val_1_rmse: 0.52485 |  0:00:10s
epoch 3  | loss: 0.25352 | val_0_rmse: 0.54752 | val_1_rmse: 0.56525 |  0:00:14s
epoch 4  | loss: 0.24487 | val_0_rmse: 0.4635  | val_1_rmse: 0.48185 |  0:00:17s
epoch 5  | loss: 0.24106 | val_0_rmse: 0.46967 | val_1_rmse: 0.48626 |  0:00:21s
epoch 6  | loss: 0.23755 | val_0_rmse: 0.4765  | val_1_rmse: 0.49714 |  0:00:24s
epoch 7  | loss: 0.23126 | val_0_rmse: 0.45961 | val_1_rmse: 0.47764 |  0:00:28s
epoch 8  | loss: 0.22915 | val_0_rmse: 0.48939 | val_1_rmse: 0.50467 |  0:00:31s
epoch 9  | loss: 0.22635 | val_0_rmse: 0.45043 | val_1_rmse: 0.46859 |  0:00:35s
epoch 10 | loss: 0.22996 | val_0_rmse: 0.46392 | val_1_rmse: 0.4823  |  0:00:38s
epoch 11 | loss: 0.22353 | val_0_rmse: 0.46526 | val_1_rmse: 0.48522 |  0:00:42s
epoch 12 | loss: 0.21824 | val_0_rmse: 0.45021 | val_1_rmse: 0.46888 |  0:00:45s
epoch 13 | loss: 0.21555 | val_0_rmse: 0.44067 | val_1_rmse: 0.4584  |  0:00:49s
epoch 14 | loss: 0.21304 | val_0_rmse: 0.44334 | val_1_rmse: 0.45913 |  0:00:52s
epoch 15 | loss: 0.21076 | val_0_rmse: 0.45898 | val_1_rmse: 0.47541 |  0:00:56s
epoch 16 | loss: 0.21236 | val_0_rmse: 0.435   | val_1_rmse: 0.45432 |  0:01:00s
epoch 17 | loss: 0.2063  | val_0_rmse: 0.43404 | val_1_rmse: 0.45394 |  0:01:03s
epoch 18 | loss: 0.2047  | val_0_rmse: 0.42413 | val_1_rmse: 0.44318 |  0:01:07s
epoch 19 | loss: 0.2063  | val_0_rmse: 0.46911 | val_1_rmse: 0.48874 |  0:01:10s
epoch 20 | loss: 0.20571 | val_0_rmse: 0.43199 | val_1_rmse: 0.45112 |  0:01:14s
epoch 21 | loss: 0.20377 | val_0_rmse: 0.44199 | val_1_rmse: 0.45806 |  0:01:17s
epoch 22 | loss: 0.19995 | val_0_rmse: 0.42711 | val_1_rmse: 0.44642 |  0:01:21s
epoch 23 | loss: 0.20034 | val_0_rmse: 0.42636 | val_1_rmse: 0.44669 |  0:01:24s
epoch 24 | loss: 0.19548 | val_0_rmse: 0.4266  | val_1_rmse: 0.44668 |  0:01:28s
epoch 25 | loss: 0.20059 | val_0_rmse: 0.44473 | val_1_rmse: 0.46163 |  0:01:31s
epoch 26 | loss: 0.19717 | val_0_rmse: 0.43082 | val_1_rmse: 0.44908 |  0:01:35s
epoch 27 | loss: 0.19867 | val_0_rmse: 0.42447 | val_1_rmse: 0.44316 |  0:01:39s
epoch 28 | loss: 0.20141 | val_0_rmse: 0.42805 | val_1_rmse: 0.44931 |  0:01:42s
epoch 29 | loss: 0.19947 | val_0_rmse: 0.4236  | val_1_rmse: 0.44423 |  0:01:46s
epoch 30 | loss: 0.19344 | val_0_rmse: 0.42761 | val_1_rmse: 0.45032 |  0:01:49s
epoch 31 | loss: 0.19678 | val_0_rmse: 0.41914 | val_1_rmse: 0.44049 |  0:01:53s
epoch 32 | loss: 0.19375 | val_0_rmse: 0.43371 | val_1_rmse: 0.45298 |  0:01:56s
epoch 33 | loss: 0.19966 | val_0_rmse: 0.41639 | val_1_rmse: 0.43772 |  0:02:00s
epoch 34 | loss: 0.19413 | val_0_rmse: 0.4245  | val_1_rmse: 0.44329 |  0:02:03s
epoch 35 | loss: 0.1939  | val_0_rmse: 0.43453 | val_1_rmse: 0.45748 |  0:02:07s
epoch 36 | loss: 0.19584 | val_0_rmse: 0.42138 | val_1_rmse: 0.44133 |  0:02:10s
epoch 37 | loss: 0.20044 | val_0_rmse: 0.42048 | val_1_rmse: 0.43883 |  0:02:14s
epoch 38 | loss: 0.19034 | val_0_rmse: 0.41853 | val_1_rmse: 0.44077 |  0:02:17s
epoch 39 | loss: 0.19335 | val_0_rmse: 0.43031 | val_1_rmse: 0.45069 |  0:02:21s
epoch 40 | loss: 0.19253 | val_0_rmse: 0.42217 | val_1_rmse: 0.44179 |  0:02:24s
epoch 41 | loss: 0.1848  | val_0_rmse: 0.41608 | val_1_rmse: 0.43671 |  0:02:28s
epoch 42 | loss: 0.1892  | val_0_rmse: 0.40851 | val_1_rmse: 0.4283  |  0:02:31s
epoch 43 | loss: 0.18541 | val_0_rmse: 0.41292 | val_1_rmse: 0.4331  |  0:02:35s
epoch 44 | loss: 0.18771 | val_0_rmse: 0.41861 | val_1_rmse: 0.43914 |  0:02:38s
epoch 45 | loss: 0.18812 | val_0_rmse: 0.4159  | val_1_rmse: 0.43684 |  0:02:42s
epoch 46 | loss: 0.18463 | val_0_rmse: 0.42008 | val_1_rmse: 0.43996 |  0:02:45s
epoch 47 | loss: 0.18563 | val_0_rmse: 0.41227 | val_1_rmse: 0.43496 |  0:02:49s
epoch 48 | loss: 0.18524 | val_0_rmse: 0.41255 | val_1_rmse: 0.43374 |  0:02:53s
epoch 49 | loss: 0.18645 | val_0_rmse: 0.41009 | val_1_rmse: 0.43554 |  0:02:56s
epoch 50 | loss: 0.18716 | val_0_rmse: 0.41983 | val_1_rmse: 0.44033 |  0:03:00s
epoch 51 | loss: 0.1859  | val_0_rmse: 0.40982 | val_1_rmse: 0.43438 |  0:03:03s
epoch 52 | loss: 0.18533 | val_0_rmse: 0.40853 | val_1_rmse: 0.43293 |  0:03:07s
epoch 53 | loss: 0.19136 | val_0_rmse: 0.42466 | val_1_rmse: 0.4453  |  0:03:10s
epoch 54 | loss: 0.18796 | val_0_rmse: 0.40878 | val_1_rmse: 0.42925 |  0:03:14s
epoch 55 | loss: 0.18405 | val_0_rmse: 0.41927 | val_1_rmse: 0.44196 |  0:03:17s
epoch 56 | loss: 0.18768 | val_0_rmse: 0.42801 | val_1_rmse: 0.44884 |  0:03:21s
epoch 57 | loss: 0.18575 | val_0_rmse: 0.4125  | val_1_rmse: 0.43371 |  0:03:24s
epoch 58 | loss: 0.18266 | val_0_rmse: 0.41207 | val_1_rmse: 0.43499 |  0:03:28s
epoch 59 | loss: 0.18375 | val_0_rmse: 0.41431 | val_1_rmse: 0.43571 |  0:03:31s
epoch 60 | loss: 0.18576 | val_0_rmse: 0.41502 | val_1_rmse: 0.43747 |  0:03:35s
epoch 61 | loss: 0.18399 | val_0_rmse: 0.40971 | val_1_rmse: 0.4312  |  0:03:38s
epoch 62 | loss: 0.18304 | val_0_rmse: 0.42032 | val_1_rmse: 0.43889 |  0:03:42s
epoch 63 | loss: 0.18405 | val_0_rmse: 0.40563 | val_1_rmse: 0.42822 |  0:03:45s
epoch 64 | loss: 0.18107 | val_0_rmse: 0.41206 | val_1_rmse: 0.43187 |  0:03:49s
epoch 65 | loss: 0.18522 | val_0_rmse: 0.40865 | val_1_rmse: 0.43158 |  0:03:52s
epoch 66 | loss: 0.18485 | val_0_rmse: 0.41706 | val_1_rmse: 0.43798 |  0:03:56s
epoch 67 | loss: 0.17802 | val_0_rmse: 0.41485 | val_1_rmse: 0.43755 |  0:03:59s
epoch 68 | loss: 0.18129 | val_0_rmse: 0.40939 | val_1_rmse: 0.43442 |  0:04:03s
epoch 69 | loss: 0.17824 | val_0_rmse: 0.41001 | val_1_rmse: 0.4346  |  0:04:07s
epoch 70 | loss: 0.18224 | val_0_rmse: 0.41008 | val_1_rmse: 0.43359 |  0:04:10s
epoch 71 | loss: 0.17922 | val_0_rmse: 0.40285 | val_1_rmse: 0.42595 |  0:04:14s
epoch 72 | loss: 0.18104 | val_0_rmse: 0.40793 | val_1_rmse: 0.43253 |  0:04:17s
epoch 73 | loss: 0.18267 | val_0_rmse: 0.41434 | val_1_rmse: 0.43658 |  0:04:21s
epoch 74 | loss: 0.18094 | val_0_rmse: 0.40579 | val_1_rmse: 0.42926 |  0:04:24s
epoch 75 | loss: 0.18045 | val_0_rmse: 0.41053 | val_1_rmse: 0.43296 |  0:04:28s
epoch 76 | loss: 0.18047 | val_0_rmse: 0.40869 | val_1_rmse: 0.43314 |  0:04:31s
epoch 77 | loss: 0.1792  | val_0_rmse: 0.4128  | val_1_rmse: 0.43373 |  0:04:35s
epoch 78 | loss: 0.17978 | val_0_rmse: 0.42276 | val_1_rmse: 0.4432  |  0:04:38s
epoch 79 | loss: 0.1858  | val_0_rmse: 0.41292 | val_1_rmse: 0.43302 |  0:04:42s
epoch 80 | loss: 0.18177 | val_0_rmse: 0.41133 | val_1_rmse: 0.43176 |  0:04:45s
epoch 81 | loss: 0.18009 | val_0_rmse: 0.41572 | val_1_rmse: 0.43948 |  0:04:49s
epoch 82 | loss: 0.18056 | val_0_rmse: 0.42019 | val_1_rmse: 0.43879 |  0:04:52s
epoch 83 | loss: 0.18122 | val_0_rmse: 0.40131 | val_1_rmse: 0.4239  |  0:04:56s
epoch 84 | loss: 0.17815 | val_0_rmse: 0.40488 | val_1_rmse: 0.42739 |  0:04:59s
epoch 85 | loss: 0.17896 | val_0_rmse: 0.41051 | val_1_rmse: 0.43291 |  0:05:03s
epoch 86 | loss: 0.17739 | val_0_rmse: 0.41    | val_1_rmse: 0.43162 |  0:05:06s
epoch 87 | loss: 0.18058 | val_0_rmse: 0.40213 | val_1_rmse: 0.42583 |  0:05:10s
epoch 88 | loss: 0.17483 | val_0_rmse: 0.40409 | val_1_rmse: 0.4275  |  0:05:13s
epoch 89 | loss: 0.17753 | val_0_rmse: 0.41576 | val_1_rmse: 0.43869 |  0:05:17s
epoch 90 | loss: 0.17796 | val_0_rmse: 0.40396 | val_1_rmse: 0.4283  |  0:05:21s
epoch 91 | loss: 0.17484 | val_0_rmse: 0.408   | val_1_rmse: 0.43201 |  0:05:24s
epoch 92 | loss: 0.17661 | val_0_rmse: 0.43254 | val_1_rmse: 0.45675 |  0:05:28s
epoch 93 | loss: 0.17925 | val_0_rmse: 0.40307 | val_1_rmse: 0.42825 |  0:05:31s
epoch 94 | loss: 0.177   | val_0_rmse: 0.40238 | val_1_rmse: 0.42683 |  0:05:35s
epoch 95 | loss: 0.17871 | val_0_rmse: 0.40781 | val_1_rmse: 0.4274  |  0:05:38s
epoch 96 | loss: 0.17652 | val_0_rmse: 0.39889 | val_1_rmse: 0.42412 |  0:05:42s
epoch 97 | loss: 0.17599 | val_0_rmse: 0.41047 | val_1_rmse: 0.43575 |  0:05:45s
epoch 98 | loss: 0.179   | val_0_rmse: 0.40114 | val_1_rmse: 0.42681 |  0:05:49s
epoch 99 | loss: 0.17868 | val_0_rmse: 0.4085  | val_1_rmse: 0.43195 |  0:05:52s
epoch 100| loss: 0.17582 | val_0_rmse: 0.41289 | val_1_rmse: 0.43687 |  0:05:56s
epoch 101| loss: 0.17598 | val_0_rmse: 0.40278 | val_1_rmse: 0.42754 |  0:05:59s
epoch 102| loss: 0.17665 | val_0_rmse: 0.40527 | val_1_rmse: 0.43065 |  0:06:03s
epoch 103| loss: 0.17545 | val_0_rmse: 0.40689 | val_1_rmse: 0.43289 |  0:06:06s
epoch 104| loss: 0.17699 | val_0_rmse: 0.40116 | val_1_rmse: 0.42746 |  0:06:10s
epoch 105| loss: 0.17354 | val_0_rmse: 0.40144 | val_1_rmse: 0.42513 |  0:06:13s
epoch 106| loss: 0.17544 | val_0_rmse: 0.39575 | val_1_rmse: 0.41918 |  0:06:17s
epoch 107| loss: 0.17746 | val_0_rmse: 0.40548 | val_1_rmse: 0.42861 |  0:06:20s
epoch 108| loss: 0.17628 | val_0_rmse: 0.40143 | val_1_rmse: 0.42735 |  0:06:24s
epoch 109| loss: 0.17522 | val_0_rmse: 0.40891 | val_1_rmse: 0.42948 |  0:06:27s
epoch 110| loss: 0.17582 | val_0_rmse: 0.40419 | val_1_rmse: 0.42911 |  0:06:31s
epoch 111| loss: 0.1756  | val_0_rmse: 0.41365 | val_1_rmse: 0.44003 |  0:06:34s
epoch 112| loss: 0.17392 | val_0_rmse: 0.40014 | val_1_rmse: 0.42461 |  0:06:38s
epoch 113| loss: 0.17617 | val_0_rmse: 0.40379 | val_1_rmse: 0.42921 |  0:06:41s
epoch 114| loss: 0.17732 | val_0_rmse: 0.40376 | val_1_rmse: 0.43362 |  0:06:45s
epoch 115| loss: 0.17421 | val_0_rmse: 0.39985 | val_1_rmse: 0.42502 |  0:06:48s
epoch 116| loss: 0.17618 | val_0_rmse: 0.40681 | val_1_rmse: 0.43274 |  0:06:52s
epoch 117| loss: 0.17407 | val_0_rmse: 0.40358 | val_1_rmse: 0.43131 |  0:06:55s
epoch 118| loss: 0.17449 | val_0_rmse: 0.40723 | val_1_rmse: 0.43234 |  0:06:59s
epoch 119| loss: 0.17371 | val_0_rmse: 0.41527 | val_1_rmse: 0.44019 |  0:07:03s
epoch 120| loss: 0.17643 | val_0_rmse: 0.40422 | val_1_rmse: 0.42976 |  0:07:06s
epoch 121| loss: 0.17411 | val_0_rmse: 0.40896 | val_1_rmse: 0.43661 |  0:07:10s
epoch 122| loss: 0.17248 | val_0_rmse: 0.39347 | val_1_rmse: 0.42129 |  0:07:13s
epoch 123| loss: 0.1751  | val_0_rmse: 0.40448 | val_1_rmse: 0.42894 |  0:07:17s
epoch 124| loss: 0.17671 | val_0_rmse: 0.40233 | val_1_rmse: 0.42896 |  0:07:20s
epoch 125| loss: 0.17438 | val_0_rmse: 0.39977 | val_1_rmse: 0.42371 |  0:07:24s
epoch 126| loss: 0.17543 | val_0_rmse: 0.39659 | val_1_rmse: 0.42226 |  0:07:27s
epoch 127| loss: 0.17078 | val_0_rmse: 0.39833 | val_1_rmse: 0.42487 |  0:07:31s
epoch 128| loss: 0.17164 | val_0_rmse: 0.38825 | val_1_rmse: 0.41489 |  0:07:34s
epoch 129| loss: 0.17142 | val_0_rmse: 0.40847 | val_1_rmse: 0.43606 |  0:07:38s
epoch 130| loss: 0.17449 | val_0_rmse: 0.40147 | val_1_rmse: 0.42943 |  0:07:41s
epoch 131| loss: 0.17392 | val_0_rmse: 0.39579 | val_1_rmse: 0.42312 |  0:07:45s
epoch 132| loss: 0.17008 | val_0_rmse: 0.40332 | val_1_rmse: 0.43219 |  0:07:48s
epoch 133| loss: 0.16882 | val_0_rmse: 0.39436 | val_1_rmse: 0.42258 |  0:07:52s
epoch 134| loss: 0.17434 | val_0_rmse: 0.39714 | val_1_rmse: 0.4251  |  0:07:55s
epoch 135| loss: 0.17158 | val_0_rmse: 0.39884 | val_1_rmse: 0.42569 |  0:07:59s
epoch 136| loss: 0.17101 | val_0_rmse: 0.39495 | val_1_rmse: 0.42392 |  0:08:02s
epoch 137| loss: 0.16964 | val_0_rmse: 0.40449 | val_1_rmse: 0.43279 |  0:08:06s
epoch 138| loss: 0.17127 | val_0_rmse: 0.41229 | val_1_rmse: 0.44144 |  0:08:09s
epoch 139| loss: 0.17263 | val_0_rmse: 0.3984  | val_1_rmse: 0.42863 |  0:08:13s
epoch 140| loss: 0.16953 | val_0_rmse: 0.39938 | val_1_rmse: 0.43029 |  0:08:16s
epoch 141| loss: 0.17107 | val_0_rmse: 0.40105 | val_1_rmse: 0.42743 |  0:08:20s
epoch 142| loss: 0.17459 | val_0_rmse: 0.40242 | val_1_rmse: 0.42912 |  0:08:23s
epoch 143| loss: 0.17036 | val_0_rmse: 0.39728 | val_1_rmse: 0.42475 |  0:08:27s
epoch 144| loss: 0.16961 | val_0_rmse: 0.39813 | val_1_rmse: 0.42482 |  0:08:30s
epoch 145| loss: 0.17094 | val_0_rmse: 0.39905 | val_1_rmse: 0.42591 |  0:08:34s
epoch 146| loss: 0.17153 | val_0_rmse: 0.40017 | val_1_rmse: 0.42598 |  0:08:38s
epoch 147| loss: 0.17493 | val_0_rmse: 0.40515 | val_1_rmse: 0.43194 |  0:08:41s
epoch 148| loss: 0.1705  | val_0_rmse: 0.39744 | val_1_rmse: 0.42263 |  0:08:45s
epoch 149| loss: 0.17225 | val_0_rmse: 0.39347 | val_1_rmse: 0.42189 |  0:08:48s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.41489
Best weights from best epoch are automatically used!
ended training at: 02:51:10
Feature importance:
[('Area', 0.0), ('Baths', 0.21425373116172383), ('Beds', 0.09049241609387822), ('Latitude', 0.09589310799667963), ('Longitude', 0.22098899974362599), ('Month', 0.10832172338924083), ('Year', 0.2700500216148515)]
Mean squared error is of 9524447617.422451
Mean absolute error:67112.46488193773
MAPE:0.2736038699221276
R2 score:0.8333470377500031
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:51:11
epoch 0  | loss: 0.44767 | val_0_rmse: 0.57959 | val_1_rmse: 0.57766 |  0:00:03s
epoch 1  | loss: 0.2978  | val_0_rmse: 0.49454 | val_1_rmse: 0.49189 |  0:00:07s
epoch 2  | loss: 0.26105 | val_0_rmse: 0.48258 | val_1_rmse: 0.47647 |  0:00:10s
epoch 3  | loss: 0.24665 | val_0_rmse: 0.51504 | val_1_rmse: 0.5112  |  0:00:14s
epoch 4  | loss: 0.24192 | val_0_rmse: 0.46419 | val_1_rmse: 0.46039 |  0:00:17s
epoch 5  | loss: 0.22852 | val_0_rmse: 0.46044 | val_1_rmse: 0.45733 |  0:00:21s
epoch 6  | loss: 0.22753 | val_0_rmse: 0.46031 | val_1_rmse: 0.45672 |  0:00:24s
epoch 7  | loss: 0.2198  | val_0_rmse: 0.45032 | val_1_rmse: 0.44641 |  0:00:28s
epoch 8  | loss: 0.22558 | val_0_rmse: 0.47118 | val_1_rmse: 0.46916 |  0:00:31s
epoch 9  | loss: 0.22096 | val_0_rmse: 0.44716 | val_1_rmse: 0.44157 |  0:00:35s
epoch 10 | loss: 0.21169 | val_0_rmse: 0.45447 | val_1_rmse: 0.44854 |  0:00:38s
epoch 11 | loss: 0.22038 | val_0_rmse: 0.4659  | val_1_rmse: 0.46387 |  0:00:42s
epoch 12 | loss: 0.21411 | val_0_rmse: 0.45809 | val_1_rmse: 0.45368 |  0:00:45s
epoch 13 | loss: 0.22362 | val_0_rmse: 0.44948 | val_1_rmse: 0.44543 |  0:00:49s
epoch 14 | loss: 0.21768 | val_0_rmse: 0.45965 | val_1_rmse: 0.45861 |  0:00:52s
epoch 15 | loss: 0.20828 | val_0_rmse: 0.43499 | val_1_rmse: 0.43121 |  0:00:56s
epoch 16 | loss: 0.20539 | val_0_rmse: 0.43494 | val_1_rmse: 0.43614 |  0:00:59s
epoch 17 | loss: 0.21102 | val_0_rmse: 0.42965 | val_1_rmse: 0.42725 |  0:01:03s
epoch 18 | loss: 0.20357 | val_0_rmse: 0.43687 | val_1_rmse: 0.43642 |  0:01:06s
epoch 19 | loss: 0.20179 | val_0_rmse: 0.43264 | val_1_rmse: 0.43128 |  0:01:10s
epoch 20 | loss: 0.20197 | val_0_rmse: 0.44407 | val_1_rmse: 0.44497 |  0:01:13s
epoch 21 | loss: 0.19996 | val_0_rmse: 0.4261  | val_1_rmse: 0.42768 |  0:01:17s
epoch 22 | loss: 0.2019  | val_0_rmse: 0.438   | val_1_rmse: 0.43907 |  0:01:20s
epoch 23 | loss: 0.20512 | val_0_rmse: 0.42399 | val_1_rmse: 0.42224 |  0:01:24s
epoch 24 | loss: 0.1959  | val_0_rmse: 0.42467 | val_1_rmse: 0.42619 |  0:01:28s
epoch 25 | loss: 0.19895 | val_0_rmse: 0.44074 | val_1_rmse: 0.43994 |  0:01:31s
epoch 26 | loss: 0.19802 | val_0_rmse: 0.42376 | val_1_rmse: 0.4237  |  0:01:35s
epoch 27 | loss: 0.19972 | val_0_rmse: 0.43566 | val_1_rmse: 0.43471 |  0:01:38s
epoch 28 | loss: 0.19709 | val_0_rmse: 0.43579 | val_1_rmse: 0.43612 |  0:01:42s
epoch 29 | loss: 0.19459 | val_0_rmse: 0.43095 | val_1_rmse: 0.43339 |  0:01:45s
epoch 30 | loss: 0.19488 | val_0_rmse: 0.42535 | val_1_rmse: 0.42735 |  0:01:49s
epoch 31 | loss: 0.19374 | val_0_rmse: 0.42351 | val_1_rmse: 0.42617 |  0:01:52s
epoch 32 | loss: 0.19607 | val_0_rmse: 0.41742 | val_1_rmse: 0.42055 |  0:01:56s
epoch 33 | loss: 0.1926  | val_0_rmse: 0.4256  | val_1_rmse: 0.42554 |  0:01:59s
epoch 34 | loss: 0.19547 | val_0_rmse: 0.41857 | val_1_rmse: 0.41962 |  0:02:03s
epoch 35 | loss: 0.19179 | val_0_rmse: 0.42101 | val_1_rmse: 0.42088 |  0:02:06s
epoch 36 | loss: 0.19314 | val_0_rmse: 0.42182 | val_1_rmse: 0.42295 |  0:02:10s
epoch 37 | loss: 0.19286 | val_0_rmse: 0.42118 | val_1_rmse: 0.42254 |  0:02:13s
epoch 38 | loss: 0.19037 | val_0_rmse: 0.42216 | val_1_rmse: 0.42384 |  0:02:17s
epoch 39 | loss: 0.19083 | val_0_rmse: 0.42043 | val_1_rmse: 0.42202 |  0:02:20s
epoch 40 | loss: 0.19765 | val_0_rmse: 0.4225  | val_1_rmse: 0.42219 |  0:02:24s
epoch 41 | loss: 0.19152 | val_0_rmse: 0.40981 | val_1_rmse: 0.41331 |  0:02:27s
epoch 42 | loss: 0.18874 | val_0_rmse: 0.42634 | val_1_rmse: 0.42931 |  0:02:31s
epoch 43 | loss: 0.18767 | val_0_rmse: 0.43029 | val_1_rmse: 0.43382 |  0:02:34s
epoch 44 | loss: 0.18859 | val_0_rmse: 0.43053 | val_1_rmse: 0.43219 |  0:02:38s
epoch 45 | loss: 0.18947 | val_0_rmse: 0.44446 | val_1_rmse: 0.4466  |  0:02:41s
epoch 46 | loss: 0.18836 | val_0_rmse: 0.42479 | val_1_rmse: 0.42706 |  0:02:45s
epoch 47 | loss: 0.18822 | val_0_rmse: 0.42413 | val_1_rmse: 0.42727 |  0:02:48s
epoch 48 | loss: 0.18981 | val_0_rmse: 0.4189  | val_1_rmse: 0.42233 |  0:02:52s
epoch 49 | loss: 0.18679 | val_0_rmse: 0.42001 | val_1_rmse: 0.42316 |  0:02:56s
epoch 50 | loss: 0.19    | val_0_rmse: 0.42081 | val_1_rmse: 0.42264 |  0:02:59s
epoch 51 | loss: 0.19256 | val_0_rmse: 0.41941 | val_1_rmse: 0.42144 |  0:03:03s
epoch 52 | loss: 0.18609 | val_0_rmse: 0.42493 | val_1_rmse: 0.42736 |  0:03:06s
epoch 53 | loss: 0.19289 | val_0_rmse: 0.42657 | val_1_rmse: 0.42817 |  0:03:10s
epoch 54 | loss: 0.18827 | val_0_rmse: 0.4127  | val_1_rmse: 0.41368 |  0:03:13s
epoch 55 | loss: 0.18855 | val_0_rmse: 0.41886 | val_1_rmse: 0.42146 |  0:03:17s
epoch 56 | loss: 0.19269 | val_0_rmse: 0.42671 | val_1_rmse: 0.42921 |  0:03:20s
epoch 57 | loss: 0.18956 | val_0_rmse: 0.42861 | val_1_rmse: 0.43217 |  0:03:24s
epoch 58 | loss: 0.18625 | val_0_rmse: 0.40979 | val_1_rmse: 0.4133  |  0:03:27s
epoch 59 | loss: 0.18682 | val_0_rmse: 0.42199 | val_1_rmse: 0.42628 |  0:03:31s
epoch 60 | loss: 0.18739 | val_0_rmse: 0.40901 | val_1_rmse: 0.4143  |  0:03:34s
epoch 61 | loss: 0.18589 | val_0_rmse: 0.43055 | val_1_rmse: 0.43558 |  0:03:38s
epoch 62 | loss: 0.18704 | val_0_rmse: 0.41089 | val_1_rmse: 0.41499 |  0:03:41s
epoch 63 | loss: 0.18518 | val_0_rmse: 0.41266 | val_1_rmse: 0.41789 |  0:03:45s
epoch 64 | loss: 0.18601 | val_0_rmse: 0.4179  | val_1_rmse: 0.41878 |  0:03:48s
epoch 65 | loss: 0.18405 | val_0_rmse: 0.40878 | val_1_rmse: 0.4108  |  0:03:52s
epoch 66 | loss: 0.18429 | val_0_rmse: 0.41645 | val_1_rmse: 0.4187  |  0:03:55s
epoch 67 | loss: 0.18429 | val_0_rmse: 0.45297 | val_1_rmse: 0.45686 |  0:03:59s
epoch 68 | loss: 0.18618 | val_0_rmse: 0.42161 | val_1_rmse: 0.42845 |  0:04:02s
epoch 69 | loss: 0.18189 | val_0_rmse: 0.41612 | val_1_rmse: 0.41995 |  0:04:06s
epoch 70 | loss: 0.18325 | val_0_rmse: 0.4045  | val_1_rmse: 0.40953 |  0:04:09s
epoch 71 | loss: 0.18177 | val_0_rmse: 0.42623 | val_1_rmse: 0.42882 |  0:04:13s
epoch 72 | loss: 0.18261 | val_0_rmse: 0.41048 | val_1_rmse: 0.41756 |  0:04:16s
epoch 73 | loss: 0.18147 | val_0_rmse: 0.42254 | val_1_rmse: 0.43093 |  0:04:20s
epoch 74 | loss: 0.18198 | val_0_rmse: 0.4164  | val_1_rmse: 0.42235 |  0:04:23s
epoch 75 | loss: 0.1863  | val_0_rmse: 0.41042 | val_1_rmse: 0.41645 |  0:04:27s
epoch 76 | loss: 0.18683 | val_0_rmse: 0.40945 | val_1_rmse: 0.41628 |  0:04:30s
epoch 77 | loss: 0.18115 | val_0_rmse: 0.43253 | val_1_rmse: 0.4377  |  0:04:34s
epoch 78 | loss: 0.18412 | val_0_rmse: 0.41713 | val_1_rmse: 0.42198 |  0:04:38s
epoch 79 | loss: 0.18417 | val_0_rmse: 0.42029 | val_1_rmse: 0.42479 |  0:04:41s
epoch 80 | loss: 0.18347 | val_0_rmse: 0.42796 | val_1_rmse: 0.43187 |  0:04:45s
epoch 81 | loss: 0.1862  | val_0_rmse: 0.42946 | val_1_rmse: 0.43447 |  0:04:48s
epoch 82 | loss: 0.18565 | val_0_rmse: 0.41637 | val_1_rmse: 0.41956 |  0:04:52s
epoch 83 | loss: 0.18336 | val_0_rmse: 0.40661 | val_1_rmse: 0.41234 |  0:04:55s
epoch 84 | loss: 0.17875 | val_0_rmse: 0.4078  | val_1_rmse: 0.41313 |  0:04:59s
epoch 85 | loss: 0.18021 | val_0_rmse: 0.41453 | val_1_rmse: 0.42177 |  0:05:02s
epoch 86 | loss: 0.18907 | val_0_rmse: 0.44024 | val_1_rmse: 0.44697 |  0:05:06s
epoch 87 | loss: 0.18864 | val_0_rmse: 0.42081 | val_1_rmse: 0.42598 |  0:05:09s
epoch 88 | loss: 0.18459 | val_0_rmse: 0.40857 | val_1_rmse: 0.41447 |  0:05:13s
epoch 89 | loss: 0.18168 | val_0_rmse: 0.41621 | val_1_rmse: 0.42544 |  0:05:16s
epoch 90 | loss: 0.18316 | val_0_rmse: 0.41201 | val_1_rmse: 0.41633 |  0:05:20s
epoch 91 | loss: 0.18237 | val_0_rmse: 0.41516 | val_1_rmse: 0.41956 |  0:05:23s
epoch 92 | loss: 0.1827  | val_0_rmse: 0.403   | val_1_rmse: 0.40934 |  0:05:27s
epoch 93 | loss: 0.17899 | val_0_rmse: 0.4056  | val_1_rmse: 0.41083 |  0:05:30s
epoch 94 | loss: 0.175   | val_0_rmse: 0.39993 | val_1_rmse: 0.40959 |  0:05:34s
epoch 95 | loss: 0.18068 | val_0_rmse: 0.40959 | val_1_rmse: 0.41801 |  0:05:37s
epoch 96 | loss: 0.18106 | val_0_rmse: 0.40579 | val_1_rmse: 0.41264 |  0:05:41s
epoch 97 | loss: 0.17707 | val_0_rmse: 0.404   | val_1_rmse: 0.41308 |  0:05:44s
epoch 98 | loss: 0.1779  | val_0_rmse: 0.40922 | val_1_rmse: 0.41369 |  0:05:48s
epoch 99 | loss: 0.18271 | val_0_rmse: 0.41281 | val_1_rmse: 0.42077 |  0:05:51s
epoch 100| loss: 0.1811  | val_0_rmse: 0.40333 | val_1_rmse: 0.41123 |  0:05:55s
epoch 101| loss: 0.18225 | val_0_rmse: 0.42056 | val_1_rmse: 0.42557 |  0:05:58s
epoch 102| loss: 0.17982 | val_0_rmse: 0.41548 | val_1_rmse: 0.41739 |  0:06:02s
epoch 103| loss: 0.1847  | val_0_rmse: 0.41681 | val_1_rmse: 0.42092 |  0:06:06s
epoch 104| loss: 0.18442 | val_0_rmse: 0.40049 | val_1_rmse: 0.40753 |  0:06:09s
epoch 105| loss: 0.17932 | val_0_rmse: 0.40426 | val_1_rmse: 0.41132 |  0:06:13s
epoch 106| loss: 0.17666 | val_0_rmse: 0.41327 | val_1_rmse: 0.41943 |  0:06:16s
epoch 107| loss: 0.18306 | val_0_rmse: 0.41447 | val_1_rmse: 0.42041 |  0:06:20s
epoch 108| loss: 0.18058 | val_0_rmse: 0.40346 | val_1_rmse: 0.41088 |  0:06:23s
epoch 109| loss: 0.17921 | val_0_rmse: 0.41942 | val_1_rmse: 0.42585 |  0:06:27s
epoch 110| loss: 0.17816 | val_0_rmse: 0.4066  | val_1_rmse: 0.41515 |  0:06:30s
epoch 111| loss: 0.1796  | val_0_rmse: 0.42152 | val_1_rmse: 0.43044 |  0:06:34s
epoch 112| loss: 0.1866  | val_0_rmse: 0.42118 | val_1_rmse: 0.42717 |  0:06:37s
epoch 113| loss: 0.1817  | val_0_rmse: 0.41083 | val_1_rmse: 0.41761 |  0:06:41s
epoch 114| loss: 0.1764  | val_0_rmse: 0.40005 | val_1_rmse: 0.40823 |  0:06:44s
epoch 115| loss: 0.17704 | val_0_rmse: 0.39801 | val_1_rmse: 0.40776 |  0:06:48s
epoch 116| loss: 0.17806 | val_0_rmse: 0.40246 | val_1_rmse: 0.41146 |  0:06:51s
epoch 117| loss: 0.17891 | val_0_rmse: 0.40365 | val_1_rmse: 0.4122  |  0:06:55s
epoch 118| loss: 0.17509 | val_0_rmse: 0.39744 | val_1_rmse: 0.40794 |  0:06:58s
epoch 119| loss: 0.17836 | val_0_rmse: 0.39892 | val_1_rmse: 0.40662 |  0:07:02s
epoch 120| loss: 0.17726 | val_0_rmse: 0.40953 | val_1_rmse: 0.41771 |  0:07:06s
epoch 121| loss: 0.17563 | val_0_rmse: 0.40321 | val_1_rmse: 0.41301 |  0:07:09s
epoch 122| loss: 0.17609 | val_0_rmse: 0.39709 | val_1_rmse: 0.4059  |  0:07:13s
epoch 123| loss: 0.17378 | val_0_rmse: 0.40826 | val_1_rmse: 0.4199  |  0:07:16s
epoch 124| loss: 0.17441 | val_0_rmse: 0.40258 | val_1_rmse: 0.41271 |  0:07:20s
epoch 125| loss: 0.17547 | val_0_rmse: 0.41472 | val_1_rmse: 0.42624 |  0:07:23s
epoch 126| loss: 0.17347 | val_0_rmse: 0.39786 | val_1_rmse: 0.40889 |  0:07:27s
epoch 127| loss: 0.17254 | val_0_rmse: 0.39457 | val_1_rmse: 0.40518 |  0:07:30s
epoch 128| loss: 0.17377 | val_0_rmse: 0.43021 | val_1_rmse: 0.43844 |  0:07:34s
epoch 129| loss: 0.1831  | val_0_rmse: 0.41107 | val_1_rmse: 0.41638 |  0:07:37s
epoch 130| loss: 0.1806  | val_0_rmse: 0.40851 | val_1_rmse: 0.41536 |  0:07:41s
epoch 131| loss: 0.17352 | val_0_rmse: 0.40666 | val_1_rmse: 0.41848 |  0:07:44s
epoch 132| loss: 0.17785 | val_0_rmse: 0.40523 | val_1_rmse: 0.41401 |  0:07:48s
epoch 133| loss: 0.17764 | val_0_rmse: 0.40045 | val_1_rmse: 0.40984 |  0:07:51s
epoch 134| loss: 0.17336 | val_0_rmse: 0.39961 | val_1_rmse: 0.41117 |  0:07:55s
epoch 135| loss: 0.17391 | val_0_rmse: 0.39524 | val_1_rmse: 0.40311 |  0:07:58s
epoch 136| loss: 0.18    | val_0_rmse: 0.40908 | val_1_rmse: 0.41648 |  0:08:02s
epoch 137| loss: 0.17445 | val_0_rmse: 0.40158 | val_1_rmse: 0.41396 |  0:08:05s
epoch 138| loss: 0.17428 | val_0_rmse: 0.40686 | val_1_rmse: 0.41776 |  0:08:09s
epoch 139| loss: 0.17433 | val_0_rmse: 0.39501 | val_1_rmse: 0.40341 |  0:08:12s
epoch 140| loss: 0.1718  | val_0_rmse: 0.39791 | val_1_rmse: 0.40856 |  0:08:16s
epoch 141| loss: 0.17531 | val_0_rmse: 0.40332 | val_1_rmse: 0.41402 |  0:08:19s
epoch 142| loss: 0.17091 | val_0_rmse: 0.40254 | val_1_rmse: 0.41405 |  0:08:23s
epoch 143| loss: 0.17315 | val_0_rmse: 0.40635 | val_1_rmse: 0.41495 |  0:08:26s
epoch 144| loss: 0.17294 | val_0_rmse: 0.39459 | val_1_rmse: 0.40578 |  0:08:29s
epoch 145| loss: 0.17328 | val_0_rmse: 0.40291 | val_1_rmse: 0.41423 |  0:08:33s
epoch 146| loss: 0.17287 | val_0_rmse: 0.41677 | val_1_rmse: 0.42718 |  0:08:36s
epoch 147| loss: 0.18536 | val_0_rmse: 0.40611 | val_1_rmse: 0.41105 |  0:08:40s
epoch 148| loss: 0.17384 | val_0_rmse: 0.40518 | val_1_rmse: 0.41362 |  0:08:43s
epoch 149| loss: 0.17598 | val_0_rmse: 0.40162 | val_1_rmse: 0.4102  |  0:08:47s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.40311
Best weights from best epoch are automatically used!
ended training at: 02:59:59
Feature importance:
[('Area', 0.13283927265260653), ('Baths', 0.22735721946793042), ('Beds', 0.06482339715579889), ('Latitude', 0.046055901808763955), ('Longitude', 0.23333188102396538), ('Month', 0.02920382299759211), ('Year', 0.2663885048933427)]
Mean squared error is of 10266875218.31373
Mean absolute error:68562.58750407693
MAPE:0.29205057952634783
R2 score:0.8212381947036214
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 02:59:59
epoch 0  | loss: 0.70112 | val_0_rmse: 0.71146 | val_1_rmse: 0.71552 |  0:00:01s
epoch 1  | loss: 0.36484 | val_0_rmse: 0.67715 | val_1_rmse: 0.68539 |  0:00:02s
epoch 2  | loss: 0.32664 | val_0_rmse: 0.56364 | val_1_rmse: 0.57041 |  0:00:03s
epoch 3  | loss: 0.31733 | val_0_rmse: 0.5662  | val_1_rmse: 0.57784 |  0:00:04s
epoch 4  | loss: 0.31042 | val_0_rmse: 0.53959 | val_1_rmse: 0.55153 |  0:00:05s
epoch 5  | loss: 0.30104 | val_0_rmse: 0.54873 | val_1_rmse: 0.56142 |  0:00:06s
epoch 6  | loss: 0.29644 | val_0_rmse: 0.54554 | val_1_rmse: 0.55769 |  0:00:07s
epoch 7  | loss: 0.30544 | val_0_rmse: 0.53559 | val_1_rmse: 0.54833 |  0:00:08s
epoch 8  | loss: 0.29451 | val_0_rmse: 0.53014 | val_1_rmse: 0.54743 |  0:00:09s
epoch 9  | loss: 0.2952  | val_0_rmse: 0.5368  | val_1_rmse: 0.55275 |  0:00:10s
epoch 10 | loss: 0.30532 | val_0_rmse: 0.53066 | val_1_rmse: 0.53986 |  0:00:11s
epoch 11 | loss: 0.30617 | val_0_rmse: 0.54124 | val_1_rmse: 0.55557 |  0:00:12s
epoch 12 | loss: 0.30949 | val_0_rmse: 0.54085 | val_1_rmse: 0.55737 |  0:00:13s
epoch 13 | loss: 0.30469 | val_0_rmse: 0.54648 | val_1_rmse: 0.56321 |  0:00:14s
epoch 14 | loss: 0.30635 | val_0_rmse: 0.55337 | val_1_rmse: 0.56856 |  0:00:15s
epoch 15 | loss: 0.29595 | val_0_rmse: 0.52359 | val_1_rmse: 0.53726 |  0:00:16s
epoch 16 | loss: 0.28359 | val_0_rmse: 0.5212  | val_1_rmse: 0.53146 |  0:00:17s
epoch 17 | loss: 0.28501 | val_0_rmse: 0.53084 | val_1_rmse: 0.54525 |  0:00:18s
epoch 18 | loss: 0.28257 | val_0_rmse: 0.54244 | val_1_rmse: 0.55148 |  0:00:19s
epoch 19 | loss: 0.28429 | val_0_rmse: 0.52243 | val_1_rmse: 0.534   |  0:00:20s
epoch 20 | loss: 0.27711 | val_0_rmse: 0.51302 | val_1_rmse: 0.52229 |  0:00:21s
epoch 21 | loss: 0.27835 | val_0_rmse: 0.51423 | val_1_rmse: 0.52768 |  0:00:22s
epoch 22 | loss: 0.27951 | val_0_rmse: 0.51971 | val_1_rmse: 0.53398 |  0:00:23s
epoch 23 | loss: 0.27823 | val_0_rmse: 0.51108 | val_1_rmse: 0.52578 |  0:00:24s
epoch 24 | loss: 0.27264 | val_0_rmse: 0.51379 | val_1_rmse: 0.52443 |  0:00:25s
epoch 25 | loss: 0.27215 | val_0_rmse: 0.51163 | val_1_rmse: 0.52294 |  0:00:26s
epoch 26 | loss: 0.27157 | val_0_rmse: 0.50403 | val_1_rmse: 0.51624 |  0:00:27s
epoch 27 | loss: 0.27419 | val_0_rmse: 0.50262 | val_1_rmse: 0.51599 |  0:00:29s
epoch 28 | loss: 0.27103 | val_0_rmse: 0.53944 | val_1_rmse: 0.55298 |  0:00:30s
epoch 29 | loss: 0.27943 | val_0_rmse: 0.52034 | val_1_rmse: 0.53531 |  0:00:31s
epoch 30 | loss: 0.27982 | val_0_rmse: 0.52239 | val_1_rmse: 0.53859 |  0:00:32s
epoch 31 | loss: 0.27556 | val_0_rmse: 0.51225 | val_1_rmse: 0.527   |  0:00:33s
epoch 32 | loss: 0.26722 | val_0_rmse: 0.50608 | val_1_rmse: 0.51987 |  0:00:34s
epoch 33 | loss: 0.26399 | val_0_rmse: 0.50769 | val_1_rmse: 0.51801 |  0:00:35s
epoch 34 | loss: 0.26832 | val_0_rmse: 0.51241 | val_1_rmse: 0.52161 |  0:00:36s
epoch 35 | loss: 0.27589 | val_0_rmse: 0.50616 | val_1_rmse: 0.52064 |  0:00:37s
epoch 36 | loss: 0.26828 | val_0_rmse: 0.53235 | val_1_rmse: 0.54713 |  0:00:38s
epoch 37 | loss: 0.26364 | val_0_rmse: 0.51475 | val_1_rmse: 0.53548 |  0:00:39s
epoch 38 | loss: 0.26414 | val_0_rmse: 0.49915 | val_1_rmse: 0.51828 |  0:00:40s
epoch 39 | loss: 0.26548 | val_0_rmse: 0.50946 | val_1_rmse: 0.52871 |  0:00:41s
epoch 40 | loss: 0.2601  | val_0_rmse: 0.4969  | val_1_rmse: 0.51457 |  0:00:42s
epoch 41 | loss: 0.25842 | val_0_rmse: 0.50334 | val_1_rmse: 0.52462 |  0:00:43s
epoch 42 | loss: 0.26012 | val_0_rmse: 0.49423 | val_1_rmse: 0.51223 |  0:00:44s
epoch 43 | loss: 0.25289 | val_0_rmse: 0.49037 | val_1_rmse: 0.50871 |  0:00:45s
epoch 44 | loss: 0.25479 | val_0_rmse: 0.4877  | val_1_rmse: 0.50532 |  0:00:46s
epoch 45 | loss: 0.2554  | val_0_rmse: 0.48969 | val_1_rmse: 0.50391 |  0:00:47s
epoch 46 | loss: 0.25747 | val_0_rmse: 0.50752 | val_1_rmse: 0.52117 |  0:00:48s
epoch 47 | loss: 0.25474 | val_0_rmse: 0.48045 | val_1_rmse: 0.49838 |  0:00:49s
epoch 48 | loss: 0.24973 | val_0_rmse: 0.48963 | val_1_rmse: 0.50772 |  0:00:50s
epoch 49 | loss: 0.25282 | val_0_rmse: 0.48605 | val_1_rmse: 0.50661 |  0:00:51s
epoch 50 | loss: 0.25296 | val_0_rmse: 0.48081 | val_1_rmse: 0.49701 |  0:00:52s
epoch 51 | loss: 0.2491  | val_0_rmse: 0.49369 | val_1_rmse: 0.50976 |  0:00:53s
epoch 52 | loss: 0.2498  | val_0_rmse: 0.48435 | val_1_rmse: 0.49956 |  0:00:54s
epoch 53 | loss: 0.24838 | val_0_rmse: 0.49201 | val_1_rmse: 0.50908 |  0:00:55s
epoch 54 | loss: 0.24481 | val_0_rmse: 0.47747 | val_1_rmse: 0.49351 |  0:00:56s
epoch 55 | loss: 0.2493  | val_0_rmse: 0.48167 | val_1_rmse: 0.49979 |  0:00:57s
epoch 56 | loss: 0.24813 | val_0_rmse: 0.48254 | val_1_rmse: 0.4995  |  0:00:58s
epoch 57 | loss: 0.24337 | val_0_rmse: 0.48446 | val_1_rmse: 0.50251 |  0:00:59s
epoch 58 | loss: 0.24839 | val_0_rmse: 0.47663 | val_1_rmse: 0.49464 |  0:01:00s
epoch 59 | loss: 0.24894 | val_0_rmse: 0.4901  | val_1_rmse: 0.503   |  0:01:02s
epoch 60 | loss: 0.25235 | val_0_rmse: 0.48058 | val_1_rmse: 0.49593 |  0:01:03s
epoch 61 | loss: 0.2571  | val_0_rmse: 0.48583 | val_1_rmse: 0.50618 |  0:01:04s
epoch 62 | loss: 0.24895 | val_0_rmse: 0.48076 | val_1_rmse: 0.49877 |  0:01:05s
epoch 63 | loss: 0.24438 | val_0_rmse: 0.49262 | val_1_rmse: 0.50975 |  0:01:06s
epoch 64 | loss: 0.24609 | val_0_rmse: 0.48707 | val_1_rmse: 0.50493 |  0:01:07s
epoch 65 | loss: 0.24917 | val_0_rmse: 0.4798  | val_1_rmse: 0.49873 |  0:01:08s
epoch 66 | loss: 0.24699 | val_0_rmse: 0.48142 | val_1_rmse: 0.50039 |  0:01:09s
epoch 67 | loss: 0.24276 | val_0_rmse: 0.47215 | val_1_rmse: 0.49091 |  0:01:10s
epoch 68 | loss: 0.24243 | val_0_rmse: 0.47804 | val_1_rmse: 0.49488 |  0:01:11s
epoch 69 | loss: 0.24234 | val_0_rmse: 0.47169 | val_1_rmse: 0.4907  |  0:01:12s
epoch 70 | loss: 0.23771 | val_0_rmse: 0.48189 | val_1_rmse: 0.50533 |  0:01:13s
epoch 71 | loss: 0.24337 | val_0_rmse: 0.47897 | val_1_rmse: 0.49665 |  0:01:14s
epoch 72 | loss: 0.24508 | val_0_rmse: 0.47939 | val_1_rmse: 0.49535 |  0:01:15s
epoch 73 | loss: 0.24431 | val_0_rmse: 0.47581 | val_1_rmse: 0.4944  |  0:01:16s
epoch 74 | loss: 0.24532 | val_0_rmse: 0.47635 | val_1_rmse: 0.49891 |  0:01:17s
epoch 75 | loss: 0.2427  | val_0_rmse: 0.48121 | val_1_rmse: 0.50173 |  0:01:18s
epoch 76 | loss: 0.24669 | val_0_rmse: 0.48727 | val_1_rmse: 0.50611 |  0:01:19s
epoch 77 | loss: 0.24306 | val_0_rmse: 0.48794 | val_1_rmse: 0.50739 |  0:01:20s
epoch 78 | loss: 0.23936 | val_0_rmse: 0.47245 | val_1_rmse: 0.49085 |  0:01:21s
epoch 79 | loss: 0.24261 | val_0_rmse: 0.47322 | val_1_rmse: 0.48802 |  0:01:22s
epoch 80 | loss: 0.23833 | val_0_rmse: 0.4752  | val_1_rmse: 0.49302 |  0:01:23s
epoch 81 | loss: 0.236   | val_0_rmse: 0.48146 | val_1_rmse: 0.49705 |  0:01:24s
epoch 82 | loss: 0.24308 | val_0_rmse: 0.47482 | val_1_rmse: 0.49166 |  0:01:25s
epoch 83 | loss: 0.2448  | val_0_rmse: 0.5062  | val_1_rmse: 0.52065 |  0:01:26s
epoch 84 | loss: 0.24487 | val_0_rmse: 0.47292 | val_1_rmse: 0.48982 |  0:01:27s
epoch 85 | loss: 0.23716 | val_0_rmse: 0.46745 | val_1_rmse: 0.48738 |  0:01:28s
epoch 86 | loss: 0.23367 | val_0_rmse: 0.47161 | val_1_rmse: 0.48874 |  0:01:29s
epoch 87 | loss: 0.24462 | val_0_rmse: 0.47093 | val_1_rmse: 0.49022 |  0:01:31s
epoch 88 | loss: 0.24123 | val_0_rmse: 0.47566 | val_1_rmse: 0.49577 |  0:01:32s
epoch 89 | loss: 0.24221 | val_0_rmse: 0.47484 | val_1_rmse: 0.49508 |  0:01:33s
epoch 90 | loss: 0.23791 | val_0_rmse: 0.47429 | val_1_rmse: 0.49559 |  0:01:34s
epoch 91 | loss: 0.24266 | val_0_rmse: 0.47582 | val_1_rmse: 0.49908 |  0:01:35s
epoch 92 | loss: 0.23813 | val_0_rmse: 0.48119 | val_1_rmse: 0.4986  |  0:01:36s
epoch 93 | loss: 0.24485 | val_0_rmse: 0.48033 | val_1_rmse: 0.49349 |  0:01:37s
epoch 94 | loss: 0.23878 | val_0_rmse: 0.47157 | val_1_rmse: 0.49408 |  0:01:38s
epoch 95 | loss: 0.24115 | val_0_rmse: 0.4791  | val_1_rmse: 0.50142 |  0:01:39s
epoch 96 | loss: 0.24221 | val_0_rmse: 0.4706  | val_1_rmse: 0.49075 |  0:01:40s
epoch 97 | loss: 0.24312 | val_0_rmse: 0.47881 | val_1_rmse: 0.49733 |  0:01:41s
epoch 98 | loss: 0.24286 | val_0_rmse: 0.47281 | val_1_rmse: 0.49094 |  0:01:42s
epoch 99 | loss: 0.24041 | val_0_rmse: 0.47765 | val_1_rmse: 0.49698 |  0:01:43s
epoch 100| loss: 0.23952 | val_0_rmse: 0.46932 | val_1_rmse: 0.48678 |  0:01:44s
epoch 101| loss: 0.23715 | val_0_rmse: 0.46583 | val_1_rmse: 0.48691 |  0:01:45s
epoch 102| loss: 0.23819 | val_0_rmse: 0.46508 | val_1_rmse: 0.48535 |  0:01:46s
epoch 103| loss: 0.23733 | val_0_rmse: 0.47226 | val_1_rmse: 0.49685 |  0:01:47s
epoch 104| loss: 0.23771 | val_0_rmse: 0.47616 | val_1_rmse: 0.50026 |  0:01:48s
epoch 105| loss: 0.24601 | val_0_rmse: 0.48812 | val_1_rmse: 0.50944 |  0:01:49s
epoch 106| loss: 0.24145 | val_0_rmse: 0.47462 | val_1_rmse: 0.49373 |  0:01:50s
epoch 107| loss: 0.23941 | val_0_rmse: 0.47454 | val_1_rmse: 0.49291 |  0:01:51s
epoch 108| loss: 0.24522 | val_0_rmse: 0.47451 | val_1_rmse: 0.4971  |  0:01:52s
epoch 109| loss: 0.23477 | val_0_rmse: 0.47529 | val_1_rmse: 0.49546 |  0:01:53s
epoch 110| loss: 0.23475 | val_0_rmse: 0.46799 | val_1_rmse: 0.48688 |  0:01:54s
epoch 111| loss: 0.24319 | val_0_rmse: 0.46775 | val_1_rmse: 0.48601 |  0:01:55s
epoch 112| loss: 0.23822 | val_0_rmse: 0.4643  | val_1_rmse: 0.48289 |  0:01:56s
epoch 113| loss: 0.23116 | val_0_rmse: 0.4689  | val_1_rmse: 0.48933 |  0:01:57s
epoch 114| loss: 0.23649 | val_0_rmse: 0.47592 | val_1_rmse: 0.49409 |  0:01:58s
epoch 115| loss: 0.2437  | val_0_rmse: 0.48929 | val_1_rmse: 0.50703 |  0:01:59s
epoch 116| loss: 0.24972 | val_0_rmse: 0.48017 | val_1_rmse: 0.49686 |  0:02:00s
epoch 117| loss: 0.24529 | val_0_rmse: 0.47655 | val_1_rmse: 0.49423 |  0:02:01s
epoch 118| loss: 0.23758 | val_0_rmse: 0.47128 | val_1_rmse: 0.49296 |  0:02:03s
epoch 119| loss: 0.23709 | val_0_rmse: 0.50775 | val_1_rmse: 0.52286 |  0:02:04s
epoch 120| loss: 0.24296 | val_0_rmse: 0.4773  | val_1_rmse: 0.49245 |  0:02:05s
epoch 121| loss: 0.2389  | val_0_rmse: 0.47221 | val_1_rmse: 0.48929 |  0:02:06s
epoch 122| loss: 0.23932 | val_0_rmse: 0.47384 | val_1_rmse: 0.49238 |  0:02:07s
epoch 123| loss: 0.24053 | val_0_rmse: 0.46732 | val_1_rmse: 0.48546 |  0:02:08s
epoch 124| loss: 0.23844 | val_0_rmse: 0.47002 | val_1_rmse: 0.48902 |  0:02:09s
epoch 125| loss: 0.23368 | val_0_rmse: 0.47347 | val_1_rmse: 0.48904 |  0:02:10s
epoch 126| loss: 0.23135 | val_0_rmse: 0.47088 | val_1_rmse: 0.49559 |  0:02:11s
epoch 127| loss: 0.23479 | val_0_rmse: 0.47915 | val_1_rmse: 0.4965  |  0:02:12s
epoch 128| loss: 0.24215 | val_0_rmse: 0.47093 | val_1_rmse: 0.49033 |  0:02:13s
epoch 129| loss: 0.23791 | val_0_rmse: 0.47094 | val_1_rmse: 0.49086 |  0:02:14s
epoch 130| loss: 0.23668 | val_0_rmse: 0.48395 | val_1_rmse: 0.50334 |  0:02:15s
epoch 131| loss: 0.24058 | val_0_rmse: 0.48607 | val_1_rmse: 0.50452 |  0:02:16s
epoch 132| loss: 0.24266 | val_0_rmse: 0.47187 | val_1_rmse: 0.48943 |  0:02:17s
epoch 133| loss: 0.2369  | val_0_rmse: 0.48327 | val_1_rmse: 0.49987 |  0:02:18s
epoch 134| loss: 0.24123 | val_0_rmse: 0.46631 | val_1_rmse: 0.48515 |  0:02:19s
epoch 135| loss: 0.23761 | val_0_rmse: 0.47001 | val_1_rmse: 0.48898 |  0:02:20s
epoch 136| loss: 0.23692 | val_0_rmse: 0.48246 | val_1_rmse: 0.50193 |  0:02:21s
epoch 137| loss: 0.24203 | val_0_rmse: 0.47599 | val_1_rmse: 0.49632 |  0:02:22s
epoch 138| loss: 0.23558 | val_0_rmse: 0.46742 | val_1_rmse: 0.48766 |  0:02:23s
epoch 139| loss: 0.23163 | val_0_rmse: 0.46262 | val_1_rmse: 0.48647 |  0:02:24s
epoch 140| loss: 0.23833 | val_0_rmse: 0.47488 | val_1_rmse: 0.49924 |  0:02:25s
epoch 141| loss: 0.2315  | val_0_rmse: 0.47253 | val_1_rmse: 0.49547 |  0:02:26s
epoch 142| loss: 0.22915 | val_0_rmse: 0.45986 | val_1_rmse: 0.48301 |  0:02:27s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.48289
Best weights from best epoch are automatically used!
ended training at: 03:02:27
Feature importance:
[('Area', 0.34070094133530204), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.5055569039207407), ('Longitude', 0.13176423987610025), ('Month', 0.0), ('Year', 0.02197791486785691)]
Mean squared error is of 7209319301.763938
Mean absolute error:61077.92786814293
MAPE:0.16699445493085194
R2 score:0.7640782649414495
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:02:28
epoch 0  | loss: 0.66227 | val_0_rmse: 0.7346  | val_1_rmse: 0.73722 |  0:00:01s
epoch 1  | loss: 0.35843 | val_0_rmse: 0.61056 | val_1_rmse: 0.59975 |  0:00:02s
epoch 2  | loss: 0.31726 | val_0_rmse: 0.55439 | val_1_rmse: 0.54975 |  0:00:03s
epoch 3  | loss: 0.30162 | val_0_rmse: 0.53271 | val_1_rmse: 0.52509 |  0:00:04s
epoch 4  | loss: 0.2966  | val_0_rmse: 0.53636 | val_1_rmse: 0.53252 |  0:00:05s
epoch 5  | loss: 0.29906 | val_0_rmse: 0.5226  | val_1_rmse: 0.5172  |  0:00:06s
epoch 6  | loss: 0.28624 | val_0_rmse: 0.51556 | val_1_rmse: 0.51007 |  0:00:07s
epoch 7  | loss: 0.28355 | val_0_rmse: 0.51577 | val_1_rmse: 0.50574 |  0:00:08s
epoch 8  | loss: 0.27443 | val_0_rmse: 0.54453 | val_1_rmse: 0.54841 |  0:00:09s
epoch 9  | loss: 0.27673 | val_0_rmse: 0.50412 | val_1_rmse: 0.50269 |  0:00:10s
epoch 10 | loss: 0.26362 | val_0_rmse: 0.50743 | val_1_rmse: 0.50414 |  0:00:11s
epoch 11 | loss: 0.26369 | val_0_rmse: 0.52821 | val_1_rmse: 0.53126 |  0:00:12s
epoch 12 | loss: 0.28798 | val_0_rmse: 0.53569 | val_1_rmse: 0.53553 |  0:00:13s
epoch 13 | loss: 0.27271 | val_0_rmse: 0.52984 | val_1_rmse: 0.51797 |  0:00:14s
epoch 14 | loss: 0.27696 | val_0_rmse: 0.51543 | val_1_rmse: 0.50769 |  0:00:15s
epoch 15 | loss: 0.27181 | val_0_rmse: 0.49601 | val_1_rmse: 0.48991 |  0:00:16s
epoch 16 | loss: 0.25748 | val_0_rmse: 0.50461 | val_1_rmse: 0.49766 |  0:00:17s
epoch 17 | loss: 0.25593 | val_0_rmse: 0.49483 | val_1_rmse: 0.48858 |  0:00:18s
epoch 18 | loss: 0.26042 | val_0_rmse: 0.499   | val_1_rmse: 0.48557 |  0:00:19s
epoch 19 | loss: 0.25254 | val_0_rmse: 0.49218 | val_1_rmse: 0.48478 |  0:00:20s
epoch 20 | loss: 0.24732 | val_0_rmse: 0.4776  | val_1_rmse: 0.47567 |  0:00:21s
epoch 21 | loss: 0.24599 | val_0_rmse: 0.48536 | val_1_rmse: 0.47579 |  0:00:22s
epoch 22 | loss: 0.24422 | val_0_rmse: 0.49523 | val_1_rmse: 0.48987 |  0:00:23s
epoch 23 | loss: 0.24067 | val_0_rmse: 0.47672 | val_1_rmse: 0.47082 |  0:00:24s
epoch 24 | loss: 0.2452  | val_0_rmse: 0.48329 | val_1_rmse: 0.47926 |  0:00:25s
epoch 25 | loss: 0.23722 | val_0_rmse: 0.47787 | val_1_rmse: 0.47549 |  0:00:26s
epoch 26 | loss: 0.25583 | val_0_rmse: 0.48235 | val_1_rmse: 0.47731 |  0:00:27s
epoch 27 | loss: 0.25127 | val_0_rmse: 0.4953  | val_1_rmse: 0.49198 |  0:00:29s
epoch 28 | loss: 0.25905 | val_0_rmse: 0.48544 | val_1_rmse: 0.4803  |  0:00:30s
epoch 29 | loss: 0.24849 | val_0_rmse: 0.51534 | val_1_rmse: 0.50776 |  0:00:31s
epoch 30 | loss: 0.26708 | val_0_rmse: 0.50328 | val_1_rmse: 0.49987 |  0:00:32s
epoch 31 | loss: 0.26471 | val_0_rmse: 0.49425 | val_1_rmse: 0.49128 |  0:00:33s
epoch 32 | loss: 0.25054 | val_0_rmse: 0.47711 | val_1_rmse: 0.47602 |  0:00:34s
epoch 33 | loss: 0.23952 | val_0_rmse: 0.47405 | val_1_rmse: 0.47083 |  0:00:35s
epoch 34 | loss: 0.23586 | val_0_rmse: 0.47766 | val_1_rmse: 0.47289 |  0:00:36s
epoch 35 | loss: 0.23203 | val_0_rmse: 0.47436 | val_1_rmse: 0.46942 |  0:00:37s
epoch 36 | loss: 0.23575 | val_0_rmse: 0.47622 | val_1_rmse: 0.47298 |  0:00:38s
epoch 37 | loss: 0.24126 | val_0_rmse: 0.46322 | val_1_rmse: 0.46185 |  0:00:39s
epoch 38 | loss: 0.23302 | val_0_rmse: 0.49266 | val_1_rmse: 0.48811 |  0:00:40s
epoch 39 | loss: 0.2358  | val_0_rmse: 0.47099 | val_1_rmse: 0.47311 |  0:00:41s
epoch 40 | loss: 0.22665 | val_0_rmse: 0.46072 | val_1_rmse: 0.46302 |  0:00:42s
epoch 41 | loss: 0.22412 | val_0_rmse: 0.45937 | val_1_rmse: 0.45807 |  0:00:43s
epoch 42 | loss: 0.22815 | val_0_rmse: 0.46337 | val_1_rmse: 0.46666 |  0:00:44s
epoch 43 | loss: 0.22669 | val_0_rmse: 0.46632 | val_1_rmse: 0.46186 |  0:00:45s
epoch 44 | loss: 0.22482 | val_0_rmse: 0.45544 | val_1_rmse: 0.45516 |  0:00:46s
epoch 45 | loss: 0.2234  | val_0_rmse: 0.46199 | val_1_rmse: 0.45964 |  0:00:47s
epoch 46 | loss: 0.2299  | val_0_rmse: 0.45557 | val_1_rmse: 0.45335 |  0:00:48s
epoch 47 | loss: 0.22525 | val_0_rmse: 0.45586 | val_1_rmse: 0.45204 |  0:00:49s
epoch 48 | loss: 0.22263 | val_0_rmse: 0.46255 | val_1_rmse: 0.46278 |  0:00:50s
epoch 49 | loss: 0.24254 | val_0_rmse: 0.47431 | val_1_rmse: 0.47019 |  0:00:51s
epoch 50 | loss: 0.24463 | val_0_rmse: 0.49567 | val_1_rmse: 0.49105 |  0:00:52s
epoch 51 | loss: 0.25421 | val_0_rmse: 0.47976 | val_1_rmse: 0.47724 |  0:00:53s
epoch 52 | loss: 0.24187 | val_0_rmse: 0.47186 | val_1_rmse: 0.47292 |  0:00:54s
epoch 53 | loss: 0.24419 | val_0_rmse: 0.47668 | val_1_rmse: 0.47425 |  0:00:56s
epoch 54 | loss: 0.24391 | val_0_rmse: 0.46741 | val_1_rmse: 0.46787 |  0:00:57s
epoch 55 | loss: 0.2313  | val_0_rmse: 0.46341 | val_1_rmse: 0.46533 |  0:00:58s
epoch 56 | loss: 0.23555 | val_0_rmse: 0.48807 | val_1_rmse: 0.48456 |  0:00:59s
epoch 57 | loss: 0.23382 | val_0_rmse: 0.46713 | val_1_rmse: 0.46639 |  0:01:00s
epoch 58 | loss: 0.23439 | val_0_rmse: 0.46103 | val_1_rmse: 0.46178 |  0:01:01s
epoch 59 | loss: 0.23219 | val_0_rmse: 0.4586  | val_1_rmse: 0.45939 |  0:01:02s
epoch 60 | loss: 0.22768 | val_0_rmse: 0.45697 | val_1_rmse: 0.45697 |  0:01:03s
epoch 61 | loss: 0.22376 | val_0_rmse: 0.46755 | val_1_rmse: 0.4727  |  0:01:04s
epoch 62 | loss: 0.23989 | val_0_rmse: 0.49908 | val_1_rmse: 0.49748 |  0:01:05s
epoch 63 | loss: 0.23894 | val_0_rmse: 0.46573 | val_1_rmse: 0.46508 |  0:01:06s
epoch 64 | loss: 0.22443 | val_0_rmse: 0.45828 | val_1_rmse: 0.45863 |  0:01:07s
epoch 65 | loss: 0.22788 | val_0_rmse: 0.47224 | val_1_rmse: 0.47917 |  0:01:08s
epoch 66 | loss: 0.23844 | val_0_rmse: 0.53274 | val_1_rmse: 0.52508 |  0:01:09s
epoch 67 | loss: 0.25335 | val_0_rmse: 0.48977 | val_1_rmse: 0.48115 |  0:01:10s
epoch 68 | loss: 0.25535 | val_0_rmse: 0.50244 | val_1_rmse: 0.50541 |  0:01:11s
epoch 69 | loss: 0.24259 | val_0_rmse: 0.47505 | val_1_rmse: 0.47472 |  0:01:12s
epoch 70 | loss: 0.23699 | val_0_rmse: 0.47179 | val_1_rmse: 0.46323 |  0:01:13s
epoch 71 | loss: 0.2384  | val_0_rmse: 0.479   | val_1_rmse: 0.47236 |  0:01:14s
epoch 72 | loss: 0.23575 | val_0_rmse: 0.50352 | val_1_rmse: 0.49228 |  0:01:15s
epoch 73 | loss: 0.26487 | val_0_rmse: 0.50401 | val_1_rmse: 0.50514 |  0:01:16s
epoch 74 | loss: 0.26291 | val_0_rmse: 0.49053 | val_1_rmse: 0.48483 |  0:01:17s
epoch 75 | loss: 0.2475  | val_0_rmse: 0.49112 | val_1_rmse: 0.49377 |  0:01:18s
epoch 76 | loss: 0.24936 | val_0_rmse: 0.48111 | val_1_rmse: 0.47736 |  0:01:19s
epoch 77 | loss: 0.24564 | val_0_rmse: 0.492   | val_1_rmse: 0.48433 |  0:01:20s

Early stopping occured at epoch 77 with best_epoch = 47 and best_val_1_rmse = 0.45204
Best weights from best epoch are automatically used!
ended training at: 03:03:49
Feature importance:
[('Area', 0.5002979568156368), ('Baths', 0.01760510905450022), ('Beds', 0.013536496128928727), ('Latitude', 0.29519574515681685), ('Longitude', 0.0964647830513169), ('Month', 0.03483696216071136), ('Year', 0.042062947632089204)]
Mean squared error is of 6740096832.359406
Mean absolute error:58055.59135880096
MAPE:0.15440781791071834
R2 score:0.785242707068118
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:03:49
epoch 0  | loss: 0.63209 | val_0_rmse: 0.73745 | val_1_rmse: 0.74855 |  0:00:01s
epoch 1  | loss: 0.36505 | val_0_rmse: 0.63488 | val_1_rmse: 0.64258 |  0:00:02s
epoch 2  | loss: 0.32131 | val_0_rmse: 0.53492 | val_1_rmse: 0.5384  |  0:00:03s
epoch 3  | loss: 0.30643 | val_0_rmse: 0.52417 | val_1_rmse: 0.53106 |  0:00:04s
epoch 4  | loss: 0.29166 | val_0_rmse: 0.53919 | val_1_rmse: 0.54511 |  0:00:05s
epoch 5  | loss: 0.28233 | val_0_rmse: 0.52674 | val_1_rmse: 0.53309 |  0:00:06s
epoch 6  | loss: 0.28029 | val_0_rmse: 0.51888 | val_1_rmse: 0.5192  |  0:00:07s
epoch 7  | loss: 0.28309 | val_0_rmse: 0.51364 | val_1_rmse: 0.51438 |  0:00:08s
epoch 8  | loss: 0.27153 | val_0_rmse: 0.51356 | val_1_rmse: 0.51401 |  0:00:09s
epoch 9  | loss: 0.27898 | val_0_rmse: 0.50554 | val_1_rmse: 0.50901 |  0:00:10s
epoch 10 | loss: 0.27204 | val_0_rmse: 0.5039  | val_1_rmse: 0.50888 |  0:00:11s
epoch 11 | loss: 0.27089 | val_0_rmse: 0.50521 | val_1_rmse: 0.51183 |  0:00:12s
epoch 12 | loss: 0.26459 | val_0_rmse: 0.49501 | val_1_rmse: 0.50123 |  0:00:13s
epoch 13 | loss: 0.25711 | val_0_rmse: 0.49946 | val_1_rmse: 0.50502 |  0:00:14s
epoch 14 | loss: 0.25594 | val_0_rmse: 0.48916 | val_1_rmse: 0.4957  |  0:00:15s
epoch 15 | loss: 0.25683 | val_0_rmse: 0.49671 | val_1_rmse: 0.50379 |  0:00:16s
epoch 16 | loss: 0.25322 | val_0_rmse: 0.48998 | val_1_rmse: 0.49573 |  0:00:17s
epoch 17 | loss: 0.25432 | val_0_rmse: 0.48227 | val_1_rmse: 0.48718 |  0:00:18s
epoch 18 | loss: 0.24916 | val_0_rmse: 0.48505 | val_1_rmse: 0.49413 |  0:00:19s
epoch 19 | loss: 0.24707 | val_0_rmse: 0.48391 | val_1_rmse: 0.49463 |  0:00:20s
epoch 20 | loss: 0.24702 | val_0_rmse: 0.49353 | val_1_rmse: 0.50148 |  0:00:21s
epoch 21 | loss: 0.25177 | val_0_rmse: 0.52049 | val_1_rmse: 0.52672 |  0:00:22s
epoch 22 | loss: 0.25978 | val_0_rmse: 0.49017 | val_1_rmse: 0.49397 |  0:00:24s
epoch 23 | loss: 0.25327 | val_0_rmse: 0.48796 | val_1_rmse: 0.49676 |  0:00:25s
epoch 24 | loss: 0.2481  | val_0_rmse: 0.47447 | val_1_rmse: 0.48158 |  0:00:26s
epoch 25 | loss: 0.23846 | val_0_rmse: 0.47279 | val_1_rmse: 0.48236 |  0:00:27s
epoch 26 | loss: 0.23868 | val_0_rmse: 0.4636  | val_1_rmse: 0.46732 |  0:00:28s
epoch 27 | loss: 0.23134 | val_0_rmse: 0.48411 | val_1_rmse: 0.49408 |  0:00:29s
epoch 28 | loss: 0.23915 | val_0_rmse: 0.46827 | val_1_rmse: 0.4732  |  0:00:30s
epoch 29 | loss: 0.23201 | val_0_rmse: 0.47418 | val_1_rmse: 0.48333 |  0:00:31s
epoch 30 | loss: 0.23982 | val_0_rmse: 0.49658 | val_1_rmse: 0.50665 |  0:00:32s
epoch 31 | loss: 0.23876 | val_0_rmse: 0.46656 | val_1_rmse: 0.47564 |  0:00:33s
epoch 32 | loss: 0.22686 | val_0_rmse: 0.45613 | val_1_rmse: 0.46237 |  0:00:34s
epoch 33 | loss: 0.23205 | val_0_rmse: 0.46015 | val_1_rmse: 0.47031 |  0:00:35s
epoch 34 | loss: 0.22803 | val_0_rmse: 0.45993 | val_1_rmse: 0.47102 |  0:00:36s
epoch 35 | loss: 0.23529 | val_0_rmse: 0.47358 | val_1_rmse: 0.48001 |  0:00:37s
epoch 36 | loss: 0.23083 | val_0_rmse: 0.46502 | val_1_rmse: 0.47474 |  0:00:38s
epoch 37 | loss: 0.22721 | val_0_rmse: 0.45822 | val_1_rmse: 0.46629 |  0:00:39s
epoch 38 | loss: 0.22398 | val_0_rmse: 0.46751 | val_1_rmse: 0.47117 |  0:00:40s
epoch 39 | loss: 0.22719 | val_0_rmse: 0.44616 | val_1_rmse: 0.454   |  0:00:41s
epoch 40 | loss: 0.22376 | val_0_rmse: 0.4606  | val_1_rmse: 0.47135 |  0:00:42s
epoch 41 | loss: 0.22262 | val_0_rmse: 0.44819 | val_1_rmse: 0.45812 |  0:00:43s
epoch 42 | loss: 0.23211 | val_0_rmse: 0.46406 | val_1_rmse: 0.4671  |  0:00:44s
epoch 43 | loss: 0.22621 | val_0_rmse: 0.46031 | val_1_rmse: 0.47124 |  0:00:45s
epoch 44 | loss: 0.22598 | val_0_rmse: 0.45847 | val_1_rmse: 0.46365 |  0:00:46s
epoch 45 | loss: 0.22843 | val_0_rmse: 0.45023 | val_1_rmse: 0.45837 |  0:00:47s
epoch 46 | loss: 0.22678 | val_0_rmse: 0.44962 | val_1_rmse: 0.46155 |  0:00:48s
epoch 47 | loss: 0.22689 | val_0_rmse: 0.45801 | val_1_rmse: 0.46484 |  0:00:49s
epoch 48 | loss: 0.22127 | val_0_rmse: 0.4544  | val_1_rmse: 0.46521 |  0:00:51s
epoch 49 | loss: 0.21617 | val_0_rmse: 0.4502  | val_1_rmse: 0.46058 |  0:00:52s
epoch 50 | loss: 0.21563 | val_0_rmse: 0.45789 | val_1_rmse: 0.46184 |  0:00:53s
epoch 51 | loss: 0.21539 | val_0_rmse: 0.44713 | val_1_rmse: 0.45726 |  0:00:54s
epoch 52 | loss: 0.21509 | val_0_rmse: 0.4591  | val_1_rmse: 0.46949 |  0:00:55s
epoch 53 | loss: 0.21807 | val_0_rmse: 0.4438  | val_1_rmse: 0.45481 |  0:00:56s
epoch 54 | loss: 0.21702 | val_0_rmse: 0.44931 | val_1_rmse: 0.45953 |  0:00:57s
epoch 55 | loss: 0.22154 | val_0_rmse: 0.4485  | val_1_rmse: 0.46217 |  0:00:58s
epoch 56 | loss: 0.21637 | val_0_rmse: 0.47295 | val_1_rmse: 0.4843  |  0:00:59s
epoch 57 | loss: 0.21658 | val_0_rmse: 0.45378 | val_1_rmse: 0.46838 |  0:01:00s
epoch 58 | loss: 0.22168 | val_0_rmse: 0.46039 | val_1_rmse: 0.47047 |  0:01:01s
epoch 59 | loss: 0.21366 | val_0_rmse: 0.47951 | val_1_rmse: 0.49044 |  0:01:02s
epoch 60 | loss: 0.22047 | val_0_rmse: 0.43892 | val_1_rmse: 0.44568 |  0:01:03s
epoch 61 | loss: 0.21261 | val_0_rmse: 0.45887 | val_1_rmse: 0.46519 |  0:01:04s
epoch 62 | loss: 0.22002 | val_0_rmse: 0.44709 | val_1_rmse: 0.45812 |  0:01:05s
epoch 63 | loss: 0.21428 | val_0_rmse: 0.43984 | val_1_rmse: 0.45014 |  0:01:06s
epoch 64 | loss: 0.21535 | val_0_rmse: 0.44558 | val_1_rmse: 0.45408 |  0:01:07s
epoch 65 | loss: 0.21633 | val_0_rmse: 0.43512 | val_1_rmse: 0.44656 |  0:01:08s
epoch 66 | loss: 0.21143 | val_0_rmse: 0.43908 | val_1_rmse: 0.45075 |  0:01:09s
epoch 67 | loss: 0.21561 | val_0_rmse: 0.45549 | val_1_rmse: 0.46611 |  0:01:10s
epoch 68 | loss: 0.21672 | val_0_rmse: 0.4468  | val_1_rmse: 0.46249 |  0:01:11s
epoch 69 | loss: 0.21472 | val_0_rmse: 0.45055 | val_1_rmse: 0.4629  |  0:01:12s
epoch 70 | loss: 0.21564 | val_0_rmse: 0.4501  | val_1_rmse: 0.46141 |  0:01:13s
epoch 71 | loss: 0.21417 | val_0_rmse: 0.45227 | val_1_rmse: 0.46362 |  0:01:14s
epoch 72 | loss: 0.21115 | val_0_rmse: 0.44087 | val_1_rmse: 0.45722 |  0:01:15s
epoch 73 | loss: 0.21277 | val_0_rmse: 0.44345 | val_1_rmse: 0.45649 |  0:01:16s
epoch 74 | loss: 0.20854 | val_0_rmse: 0.43765 | val_1_rmse: 0.44909 |  0:01:17s
epoch 75 | loss: 0.20831 | val_0_rmse: 0.44101 | val_1_rmse: 0.45457 |  0:01:18s
epoch 76 | loss: 0.2106  | val_0_rmse: 0.43744 | val_1_rmse: 0.44986 |  0:01:19s
epoch 77 | loss: 0.2156  | val_0_rmse: 0.45572 | val_1_rmse: 0.46889 |  0:01:20s
epoch 78 | loss: 0.21628 | val_0_rmse: 0.44792 | val_1_rmse: 0.46242 |  0:01:21s
epoch 79 | loss: 0.2187  | val_0_rmse: 0.47136 | val_1_rmse: 0.47043 |  0:01:22s
epoch 80 | loss: 0.21356 | val_0_rmse: 0.45593 | val_1_rmse: 0.4709  |  0:01:24s
epoch 81 | loss: 0.22493 | val_0_rmse: 0.47158 | val_1_rmse: 0.48145 |  0:01:25s
epoch 82 | loss: 0.22584 | val_0_rmse: 0.47283 | val_1_rmse: 0.47588 |  0:01:26s
epoch 83 | loss: 0.23768 | val_0_rmse: 0.45669 | val_1_rmse: 0.4695  |  0:01:27s
epoch 84 | loss: 0.22354 | val_0_rmse: 0.46087 | val_1_rmse: 0.46908 |  0:01:28s
epoch 85 | loss: 0.22171 | val_0_rmse: 0.44386 | val_1_rmse: 0.45578 |  0:01:29s
epoch 86 | loss: 0.21138 | val_0_rmse: 0.4484  | val_1_rmse: 0.45977 |  0:01:30s
epoch 87 | loss: 0.22855 | val_0_rmse: 0.45028 | val_1_rmse: 0.46331 |  0:01:31s
epoch 88 | loss: 0.21778 | val_0_rmse: 0.44883 | val_1_rmse: 0.46306 |  0:01:32s
epoch 89 | loss: 0.21373 | val_0_rmse: 0.46019 | val_1_rmse: 0.47587 |  0:01:33s
epoch 90 | loss: 0.21402 | val_0_rmse: 0.44301 | val_1_rmse: 0.45643 |  0:01:34s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.44568
Best weights from best epoch are automatically used!
ended training at: 03:05:24
Feature importance:
[('Area', 0.4300318811684952), ('Baths', 0.03149553027294969), ('Beds', 0.02367532693198217), ('Latitude', 0.38371824663944504), ('Longitude', 0.06547425372713456), ('Month', 0.0), ('Year', 0.0656047612599933)]
Mean squared error is of 6159285055.075853
Mean absolute error:56704.21633621666
MAPE:0.15197279738315284
R2 score:0.8009574808996425
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:05:24
epoch 0  | loss: 0.65084 | val_0_rmse: 0.77016 | val_1_rmse: 0.78178 |  0:00:01s
epoch 1  | loss: 0.33262 | val_0_rmse: 0.6097  | val_1_rmse: 0.62293 |  0:00:02s
epoch 2  | loss: 0.30712 | val_0_rmse: 0.54871 | val_1_rmse: 0.55432 |  0:00:03s
epoch 3  | loss: 0.29632 | val_0_rmse: 0.59189 | val_1_rmse: 0.60018 |  0:00:04s
epoch 4  | loss: 0.2967  | val_0_rmse: 0.55228 | val_1_rmse: 0.55666 |  0:00:05s
epoch 5  | loss: 0.29332 | val_0_rmse: 0.53126 | val_1_rmse: 0.53383 |  0:00:06s
epoch 6  | loss: 0.2909  | val_0_rmse: 0.52065 | val_1_rmse: 0.5221  |  0:00:07s
epoch 7  | loss: 0.28621 | val_0_rmse: 0.52767 | val_1_rmse: 0.53335 |  0:00:08s
epoch 8  | loss: 0.28298 | val_0_rmse: 0.5164  | val_1_rmse: 0.52043 |  0:00:09s
epoch 9  | loss: 0.27295 | val_0_rmse: 0.52525 | val_1_rmse: 0.53624 |  0:00:10s
epoch 10 | loss: 0.27759 | val_0_rmse: 0.51709 | val_1_rmse: 0.52627 |  0:00:11s
epoch 11 | loss: 0.28212 | val_0_rmse: 0.53203 | val_1_rmse: 0.53315 |  0:00:12s
epoch 12 | loss: 0.28108 | val_0_rmse: 0.51905 | val_1_rmse: 0.52406 |  0:00:13s
epoch 13 | loss: 0.27761 | val_0_rmse: 0.51237 | val_1_rmse: 0.51736 |  0:00:14s
epoch 14 | loss: 0.27506 | val_0_rmse: 0.50856 | val_1_rmse: 0.51632 |  0:00:15s
epoch 15 | loss: 0.26442 | val_0_rmse: 0.52191 | val_1_rmse: 0.52216 |  0:00:16s
epoch 16 | loss: 0.26604 | val_0_rmse: 0.51713 | val_1_rmse: 0.51801 |  0:00:17s
epoch 17 | loss: 0.27468 | val_0_rmse: 0.51763 | val_1_rmse: 0.51212 |  0:00:18s
epoch 18 | loss: 0.26352 | val_0_rmse: 0.4947  | val_1_rmse: 0.4945  |  0:00:19s
epoch 19 | loss: 0.25437 | val_0_rmse: 0.4966  | val_1_rmse: 0.49553 |  0:00:20s
epoch 20 | loss: 0.25677 | val_0_rmse: 0.49307 | val_1_rmse: 0.49375 |  0:00:21s
epoch 21 | loss: 0.25382 | val_0_rmse: 0.49355 | val_1_rmse: 0.49548 |  0:00:22s
epoch 22 | loss: 0.25718 | val_0_rmse: 0.49043 | val_1_rmse: 0.49168 |  0:00:23s
epoch 23 | loss: 0.25071 | val_0_rmse: 0.48732 | val_1_rmse: 0.48977 |  0:00:24s
epoch 24 | loss: 0.24881 | val_0_rmse: 0.52154 | val_1_rmse: 0.51822 |  0:00:25s
epoch 25 | loss: 0.24973 | val_0_rmse: 0.4868  | val_1_rmse: 0.48567 |  0:00:26s
epoch 26 | loss: 0.24829 | val_0_rmse: 0.47941 | val_1_rmse: 0.48052 |  0:00:27s
epoch 27 | loss: 0.25039 | val_0_rmse: 0.47739 | val_1_rmse: 0.47926 |  0:00:29s
epoch 28 | loss: 0.24214 | val_0_rmse: 0.47573 | val_1_rmse: 0.48082 |  0:00:30s
epoch 29 | loss: 0.2442  | val_0_rmse: 0.4805  | val_1_rmse: 0.48099 |  0:00:31s
epoch 30 | loss: 0.25009 | val_0_rmse: 0.48929 | val_1_rmse: 0.49467 |  0:00:32s
epoch 31 | loss: 0.25143 | val_0_rmse: 0.47778 | val_1_rmse: 0.48001 |  0:00:33s
epoch 32 | loss: 0.2412  | val_0_rmse: 0.49727 | val_1_rmse: 0.50373 |  0:00:34s
epoch 33 | loss: 0.24322 | val_0_rmse: 0.46675 | val_1_rmse: 0.47202 |  0:00:35s
epoch 34 | loss: 0.2385  | val_0_rmse: 0.47589 | val_1_rmse: 0.47745 |  0:00:36s
epoch 35 | loss: 0.24101 | val_0_rmse: 0.49711 | val_1_rmse: 0.50867 |  0:00:37s
epoch 36 | loss: 0.23906 | val_0_rmse: 0.47256 | val_1_rmse: 0.47237 |  0:00:38s
epoch 37 | loss: 0.23456 | val_0_rmse: 0.47346 | val_1_rmse: 0.47971 |  0:00:39s
epoch 38 | loss: 0.24193 | val_0_rmse: 0.4675  | val_1_rmse: 0.47335 |  0:00:40s
epoch 39 | loss: 0.23405 | val_0_rmse: 0.46945 | val_1_rmse: 0.47292 |  0:00:41s
epoch 40 | loss: 0.23519 | val_0_rmse: 0.47855 | val_1_rmse: 0.48118 |  0:00:42s
epoch 41 | loss: 0.23931 | val_0_rmse: 0.49146 | val_1_rmse: 0.49497 |  0:00:43s
epoch 42 | loss: 0.24564 | val_0_rmse: 0.4831  | val_1_rmse: 0.48903 |  0:00:44s
epoch 43 | loss: 0.24112 | val_0_rmse: 0.4861  | val_1_rmse: 0.48927 |  0:00:45s
epoch 44 | loss: 0.24083 | val_0_rmse: 0.47813 | val_1_rmse: 0.48087 |  0:00:46s
epoch 45 | loss: 0.23935 | val_0_rmse: 0.47221 | val_1_rmse: 0.48056 |  0:00:47s
epoch 46 | loss: 0.23588 | val_0_rmse: 0.46979 | val_1_rmse: 0.47501 |  0:00:48s
epoch 47 | loss: 0.23375 | val_0_rmse: 0.47769 | val_1_rmse: 0.48642 |  0:00:49s
epoch 48 | loss: 0.23641 | val_0_rmse: 0.47582 | val_1_rmse: 0.47731 |  0:00:50s
epoch 49 | loss: 0.23756 | val_0_rmse: 0.47368 | val_1_rmse: 0.47939 |  0:00:51s
epoch 50 | loss: 0.2308  | val_0_rmse: 0.48258 | val_1_rmse: 0.48564 |  0:00:52s
epoch 51 | loss: 0.24239 | val_0_rmse: 0.48171 | val_1_rmse: 0.48856 |  0:00:53s
epoch 52 | loss: 0.23835 | val_0_rmse: 0.49386 | val_1_rmse: 0.50425 |  0:00:54s
epoch 53 | loss: 0.23201 | val_0_rmse: 0.46985 | val_1_rmse: 0.47371 |  0:00:55s
epoch 54 | loss: 0.23257 | val_0_rmse: 0.47009 | val_1_rmse: 0.47041 |  0:00:56s
epoch 55 | loss: 0.23613 | val_0_rmse: 0.4753  | val_1_rmse: 0.4812  |  0:00:57s
epoch 56 | loss: 0.2292  | val_0_rmse: 0.46907 | val_1_rmse: 0.47445 |  0:00:58s
epoch 57 | loss: 0.2331  | val_0_rmse: 0.46344 | val_1_rmse: 0.4643  |  0:01:00s
epoch 58 | loss: 0.23033 | val_0_rmse: 0.46699 | val_1_rmse: 0.47181 |  0:01:01s
epoch 59 | loss: 0.23138 | val_0_rmse: 0.47856 | val_1_rmse: 0.4817  |  0:01:02s
epoch 60 | loss: 0.22967 | val_0_rmse: 0.45884 | val_1_rmse: 0.46276 |  0:01:03s
epoch 61 | loss: 0.22951 | val_0_rmse: 0.46696 | val_1_rmse: 0.47253 |  0:01:04s
epoch 62 | loss: 0.23319 | val_0_rmse: 0.48673 | val_1_rmse: 0.48993 |  0:01:05s
epoch 63 | loss: 0.23478 | val_0_rmse: 0.4748  | val_1_rmse: 0.483   |  0:01:06s
epoch 64 | loss: 0.22825 | val_0_rmse: 0.47102 | val_1_rmse: 0.47121 |  0:01:07s
epoch 65 | loss: 0.23639 | val_0_rmse: 0.47553 | val_1_rmse: 0.47912 |  0:01:08s
epoch 66 | loss: 0.23471 | val_0_rmse: 0.47196 | val_1_rmse: 0.47719 |  0:01:09s
epoch 67 | loss: 0.23346 | val_0_rmse: 0.47239 | val_1_rmse: 0.47503 |  0:01:10s
epoch 68 | loss: 0.22948 | val_0_rmse: 0.47058 | val_1_rmse: 0.47811 |  0:01:11s
epoch 69 | loss: 0.23104 | val_0_rmse: 0.46802 | val_1_rmse: 0.47149 |  0:01:12s
epoch 70 | loss: 0.22737 | val_0_rmse: 0.46042 | val_1_rmse: 0.46101 |  0:01:13s
epoch 71 | loss: 0.22877 | val_0_rmse: 0.4548  | val_1_rmse: 0.45883 |  0:01:14s
epoch 72 | loss: 0.22269 | val_0_rmse: 0.46378 | val_1_rmse: 0.47405 |  0:01:15s
epoch 73 | loss: 0.22593 | val_0_rmse: 0.47311 | val_1_rmse: 0.48311 |  0:01:16s
epoch 74 | loss: 0.22774 | val_0_rmse: 0.46597 | val_1_rmse: 0.47185 |  0:01:17s
epoch 75 | loss: 0.22552 | val_0_rmse: 0.46056 | val_1_rmse: 0.46495 |  0:01:18s
epoch 76 | loss: 0.22558 | val_0_rmse: 0.46394 | val_1_rmse: 0.46839 |  0:01:19s
epoch 77 | loss: 0.22432 | val_0_rmse: 0.45317 | val_1_rmse: 0.45912 |  0:01:20s
epoch 78 | loss: 0.22702 | val_0_rmse: 0.45462 | val_1_rmse: 0.46057 |  0:01:21s
epoch 79 | loss: 0.23042 | val_0_rmse: 0.46798 | val_1_rmse: 0.47565 |  0:01:22s
epoch 80 | loss: 0.22744 | val_0_rmse: 0.45441 | val_1_rmse: 0.46147 |  0:01:23s
epoch 81 | loss: 0.22766 | val_0_rmse: 0.45769 | val_1_rmse: 0.46532 |  0:01:24s
epoch 82 | loss: 0.22539 | val_0_rmse: 0.46483 | val_1_rmse: 0.47048 |  0:01:25s
epoch 83 | loss: 0.23234 | val_0_rmse: 0.46737 | val_1_rmse: 0.46931 |  0:01:27s
epoch 84 | loss: 0.23554 | val_0_rmse: 0.46    | val_1_rmse: 0.46297 |  0:01:28s
epoch 85 | loss: 0.23561 | val_0_rmse: 0.45601 | val_1_rmse: 0.46172 |  0:01:29s
epoch 86 | loss: 0.23282 | val_0_rmse: 0.46463 | val_1_rmse: 0.47405 |  0:01:30s
epoch 87 | loss: 0.22674 | val_0_rmse: 0.46025 | val_1_rmse: 0.46674 |  0:01:31s
epoch 88 | loss: 0.22977 | val_0_rmse: 0.45675 | val_1_rmse: 0.46414 |  0:01:32s
epoch 89 | loss: 0.22205 | val_0_rmse: 0.45716 | val_1_rmse: 0.46488 |  0:01:33s
epoch 90 | loss: 0.22811 | val_0_rmse: 0.46862 | val_1_rmse: 0.47981 |  0:01:34s
epoch 91 | loss: 0.22894 | val_0_rmse: 0.45371 | val_1_rmse: 0.45864 |  0:01:35s
epoch 92 | loss: 0.2183  | val_0_rmse: 0.45189 | val_1_rmse: 0.45603 |  0:01:36s
epoch 93 | loss: 0.22271 | val_0_rmse: 0.45989 | val_1_rmse: 0.4669  |  0:01:37s
epoch 94 | loss: 0.21889 | val_0_rmse: 0.45973 | val_1_rmse: 0.46426 |  0:01:38s
epoch 95 | loss: 0.22297 | val_0_rmse: 0.45224 | val_1_rmse: 0.45745 |  0:01:39s
epoch 96 | loss: 0.23227 | val_0_rmse: 0.46495 | val_1_rmse: 0.47212 |  0:01:40s
epoch 97 | loss: 0.22559 | val_0_rmse: 0.4538  | val_1_rmse: 0.45779 |  0:01:41s
epoch 98 | loss: 0.22441 | val_0_rmse: 0.44825 | val_1_rmse: 0.45759 |  0:01:42s
epoch 99 | loss: 0.21988 | val_0_rmse: 0.44804 | val_1_rmse: 0.45636 |  0:01:43s
epoch 100| loss: 0.2211  | val_0_rmse: 0.44944 | val_1_rmse: 0.4539  |  0:01:44s
epoch 101| loss: 0.22021 | val_0_rmse: 0.45922 | val_1_rmse: 0.46426 |  0:01:45s
epoch 102| loss: 0.21862 | val_0_rmse: 0.44902 | val_1_rmse: 0.45647 |  0:01:46s
epoch 103| loss: 0.22868 | val_0_rmse: 0.46852 | val_1_rmse: 0.46932 |  0:01:48s
epoch 104| loss: 0.22267 | val_0_rmse: 0.45251 | val_1_rmse: 0.4556  |  0:01:49s
epoch 105| loss: 0.22592 | val_0_rmse: 0.46518 | val_1_rmse: 0.46799 |  0:01:50s
epoch 106| loss: 0.22266 | val_0_rmse: 0.45411 | val_1_rmse: 0.45538 |  0:01:51s
epoch 107| loss: 0.22321 | val_0_rmse: 0.44826 | val_1_rmse: 0.45303 |  0:01:52s
epoch 108| loss: 0.22056 | val_0_rmse: 0.46477 | val_1_rmse: 0.47256 |  0:01:53s
epoch 109| loss: 0.22248 | val_0_rmse: 0.46834 | val_1_rmse: 0.47356 |  0:01:54s
epoch 110| loss: 0.22908 | val_0_rmse: 0.47149 | val_1_rmse: 0.47504 |  0:01:55s
epoch 111| loss: 0.22021 | val_0_rmse: 0.44668 | val_1_rmse: 0.45172 |  0:01:56s
epoch 112| loss: 0.21495 | val_0_rmse: 0.46062 | val_1_rmse: 0.46254 |  0:01:57s
epoch 113| loss: 0.22197 | val_0_rmse: 0.4458  | val_1_rmse: 0.45294 |  0:01:58s
epoch 114| loss: 0.22525 | val_0_rmse: 0.45889 | val_1_rmse: 0.46539 |  0:01:59s
epoch 115| loss: 0.21927 | val_0_rmse: 0.44845 | val_1_rmse: 0.45338 |  0:02:00s
epoch 116| loss: 0.22403 | val_0_rmse: 0.45743 | val_1_rmse: 0.4643  |  0:02:01s
epoch 117| loss: 0.22132 | val_0_rmse: 0.45032 | val_1_rmse: 0.45346 |  0:02:02s
epoch 118| loss: 0.2183  | val_0_rmse: 0.46322 | val_1_rmse: 0.47109 |  0:02:03s
epoch 119| loss: 0.22017 | val_0_rmse: 0.45276 | val_1_rmse: 0.46191 |  0:02:04s
epoch 120| loss: 0.22306 | val_0_rmse: 0.45191 | val_1_rmse: 0.46078 |  0:02:05s
epoch 121| loss: 0.21739 | val_0_rmse: 0.44776 | val_1_rmse: 0.45362 |  0:02:06s
epoch 122| loss: 0.21846 | val_0_rmse: 0.45693 | val_1_rmse: 0.4652  |  0:02:07s
epoch 123| loss: 0.21926 | val_0_rmse: 0.44426 | val_1_rmse: 0.45525 |  0:02:08s
epoch 124| loss: 0.21998 | val_0_rmse: 0.4544  | val_1_rmse: 0.46402 |  0:02:09s
epoch 125| loss: 0.21789 | val_0_rmse: 0.45089 | val_1_rmse: 0.461   |  0:02:10s
epoch 126| loss: 0.2203  | val_0_rmse: 0.44706 | val_1_rmse: 0.45657 |  0:02:12s
epoch 127| loss: 0.21328 | val_0_rmse: 0.46024 | val_1_rmse: 0.46943 |  0:02:13s
epoch 128| loss: 0.21463 | val_0_rmse: 0.44381 | val_1_rmse: 0.45532 |  0:02:14s
epoch 129| loss: 0.21737 | val_0_rmse: 0.44669 | val_1_rmse: 0.45497 |  0:02:15s
epoch 130| loss: 0.21922 | val_0_rmse: 0.44298 | val_1_rmse: 0.45309 |  0:02:16s
epoch 131| loss: 0.21474 | val_0_rmse: 0.45357 | val_1_rmse: 0.46664 |  0:02:17s
epoch 132| loss: 0.21493 | val_0_rmse: 0.44456 | val_1_rmse: 0.45116 |  0:02:18s
epoch 133| loss: 0.21594 | val_0_rmse: 0.45201 | val_1_rmse: 0.4577  |  0:02:19s
epoch 134| loss: 0.22033 | val_0_rmse: 0.47079 | val_1_rmse: 0.48249 |  0:02:20s
epoch 135| loss: 0.21487 | val_0_rmse: 0.45035 | val_1_rmse: 0.45757 |  0:02:21s
epoch 136| loss: 0.21585 | val_0_rmse: 0.44492 | val_1_rmse: 0.45525 |  0:02:22s
epoch 137| loss: 0.21428 | val_0_rmse: 0.43993 | val_1_rmse: 0.45432 |  0:02:23s
epoch 138| loss: 0.22729 | val_0_rmse: 0.45309 | val_1_rmse: 0.46223 |  0:02:24s
epoch 139| loss: 0.22073 | val_0_rmse: 0.45863 | val_1_rmse: 0.4691  |  0:02:25s
epoch 140| loss: 0.21816 | val_0_rmse: 0.44661 | val_1_rmse: 0.45654 |  0:02:26s
epoch 141| loss: 0.22161 | val_0_rmse: 0.45513 | val_1_rmse: 0.46413 |  0:02:27s
epoch 142| loss: 0.22008 | val_0_rmse: 0.45043 | val_1_rmse: 0.45955 |  0:02:28s
epoch 143| loss: 0.21894 | val_0_rmse: 0.45683 | val_1_rmse: 0.46928 |  0:02:29s
epoch 144| loss: 0.22028 | val_0_rmse: 0.45147 | val_1_rmse: 0.46351 |  0:02:30s
epoch 145| loss: 0.22005 | val_0_rmse: 0.44861 | val_1_rmse: 0.45651 |  0:02:31s
epoch 146| loss: 0.22123 | val_0_rmse: 0.45253 | val_1_rmse: 0.45652 |  0:02:32s
epoch 147| loss: 0.21972 | val_0_rmse: 0.44731 | val_1_rmse: 0.45652 |  0:02:34s
epoch 148| loss: 0.21667 | val_0_rmse: 0.43979 | val_1_rmse: 0.45044 |  0:02:35s
epoch 149| loss: 0.21146 | val_0_rmse: 0.44132 | val_1_rmse: 0.44968 |  0:02:36s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.44968
Best weights from best epoch are automatically used!
ended training at: 03:08:00
Feature importance:
[('Area', 0.41886311636991025), ('Baths', 0.03806168654268614), ('Beds', 0.04848683803001332), ('Latitude', 0.40637955347407195), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.08820880558331833)]
Mean squared error is of 6487897276.49817
Mean absolute error:57266.27767174133
MAPE:0.15412668140580857
R2 score:0.784642043467409
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:08:00
epoch 0  | loss: 0.68966 | val_0_rmse: 0.68957 | val_1_rmse: 0.67602 |  0:00:01s
epoch 1  | loss: 0.35951 | val_0_rmse: 0.59508 | val_1_rmse: 0.59262 |  0:00:02s
epoch 2  | loss: 0.3071  | val_0_rmse: 0.54967 | val_1_rmse: 0.54842 |  0:00:03s
epoch 3  | loss: 0.29539 | val_0_rmse: 0.52592 | val_1_rmse: 0.52575 |  0:00:04s
epoch 4  | loss: 0.28903 | val_0_rmse: 0.53344 | val_1_rmse: 0.52909 |  0:00:05s
epoch 5  | loss: 0.28163 | val_0_rmse: 0.50899 | val_1_rmse: 0.50754 |  0:00:06s
epoch 6  | loss: 0.27842 | val_0_rmse: 0.5065  | val_1_rmse: 0.50151 |  0:00:07s
epoch 7  | loss: 0.27798 | val_0_rmse: 0.52793 | val_1_rmse: 0.52917 |  0:00:08s
epoch 8  | loss: 0.28381 | val_0_rmse: 0.51432 | val_1_rmse: 0.51209 |  0:00:09s
epoch 9  | loss: 0.27437 | val_0_rmse: 0.51619 | val_1_rmse: 0.51735 |  0:00:10s
epoch 10 | loss: 0.27415 | val_0_rmse: 0.50925 | val_1_rmse: 0.50879 |  0:00:11s
epoch 11 | loss: 0.27041 | val_0_rmse: 0.50359 | val_1_rmse: 0.50579 |  0:00:12s
epoch 12 | loss: 0.2604  | val_0_rmse: 0.49751 | val_1_rmse: 0.50141 |  0:00:13s
epoch 13 | loss: 0.26112 | val_0_rmse: 0.52213 | val_1_rmse: 0.51928 |  0:00:14s
epoch 14 | loss: 0.2615  | val_0_rmse: 0.49095 | val_1_rmse: 0.49022 |  0:00:15s
epoch 15 | loss: 0.25291 | val_0_rmse: 0.4977  | val_1_rmse: 0.50244 |  0:00:16s
epoch 16 | loss: 0.25312 | val_0_rmse: 0.49073 | val_1_rmse: 0.4886  |  0:00:17s
epoch 17 | loss: 0.25125 | val_0_rmse: 0.49104 | val_1_rmse: 0.49773 |  0:00:18s
epoch 18 | loss: 0.25037 | val_0_rmse: 0.48925 | val_1_rmse: 0.49541 |  0:00:19s
epoch 19 | loss: 0.25235 | val_0_rmse: 0.4854  | val_1_rmse: 0.48717 |  0:00:21s
epoch 20 | loss: 0.24578 | val_0_rmse: 0.48069 | val_1_rmse: 0.48466 |  0:00:22s
epoch 21 | loss: 0.24148 | val_0_rmse: 0.4996  | val_1_rmse: 0.50692 |  0:00:23s
epoch 22 | loss: 0.25239 | val_0_rmse: 0.48814 | val_1_rmse: 0.49527 |  0:00:24s
epoch 23 | loss: 0.24842 | val_0_rmse: 0.47812 | val_1_rmse: 0.48072 |  0:00:25s
epoch 24 | loss: 0.24659 | val_0_rmse: 0.47323 | val_1_rmse: 0.47987 |  0:00:26s
epoch 25 | loss: 0.24116 | val_0_rmse: 0.47989 | val_1_rmse: 0.48766 |  0:00:27s
epoch 26 | loss: 0.24397 | val_0_rmse: 0.48328 | val_1_rmse: 0.49146 |  0:00:28s
epoch 27 | loss: 0.24124 | val_0_rmse: 0.47693 | val_1_rmse: 0.48094 |  0:00:29s
epoch 28 | loss: 0.24223 | val_0_rmse: 0.47299 | val_1_rmse: 0.47773 |  0:00:30s
epoch 29 | loss: 0.239   | val_0_rmse: 0.49058 | val_1_rmse: 0.49558 |  0:00:31s
epoch 30 | loss: 0.23835 | val_0_rmse: 0.47768 | val_1_rmse: 0.49089 |  0:00:32s
epoch 31 | loss: 0.23777 | val_0_rmse: 0.47179 | val_1_rmse: 0.4795  |  0:00:33s
epoch 32 | loss: 0.2312  | val_0_rmse: 0.45999 | val_1_rmse: 0.46723 |  0:00:34s
epoch 33 | loss: 0.23058 | val_0_rmse: 0.47828 | val_1_rmse: 0.48067 |  0:00:35s
epoch 34 | loss: 0.2438  | val_0_rmse: 0.46735 | val_1_rmse: 0.47758 |  0:00:36s
epoch 35 | loss: 0.24218 | val_0_rmse: 0.48879 | val_1_rmse: 0.49961 |  0:00:37s
epoch 36 | loss: 0.24522 | val_0_rmse: 0.46001 | val_1_rmse: 0.47008 |  0:00:38s
epoch 37 | loss: 0.22896 | val_0_rmse: 0.46388 | val_1_rmse: 0.47312 |  0:00:39s
epoch 38 | loss: 0.22987 | val_0_rmse: 0.48202 | val_1_rmse: 0.48595 |  0:00:40s
epoch 39 | loss: 0.23051 | val_0_rmse: 0.4707  | val_1_rmse: 0.48196 |  0:00:41s
epoch 40 | loss: 0.23021 | val_0_rmse: 0.47349 | val_1_rmse: 0.48488 |  0:00:42s
epoch 41 | loss: 0.23556 | val_0_rmse: 0.4721  | val_1_rmse: 0.47525 |  0:00:43s
epoch 42 | loss: 0.23002 | val_0_rmse: 0.46637 | val_1_rmse: 0.47523 |  0:00:45s
epoch 43 | loss: 0.23018 | val_0_rmse: 0.46003 | val_1_rmse: 0.4768  |  0:00:46s
epoch 44 | loss: 0.23401 | val_0_rmse: 0.45942 | val_1_rmse: 0.47293 |  0:00:47s
epoch 45 | loss: 0.22697 | val_0_rmse: 0.4728  | val_1_rmse: 0.48019 |  0:00:48s
epoch 46 | loss: 0.22523 | val_0_rmse: 0.46205 | val_1_rmse: 0.47058 |  0:00:49s
epoch 47 | loss: 0.22492 | val_0_rmse: 0.46474 | val_1_rmse: 0.47679 |  0:00:50s
epoch 48 | loss: 0.24372 | val_0_rmse: 0.46511 | val_1_rmse: 0.47499 |  0:00:51s
epoch 49 | loss: 0.23508 | val_0_rmse: 0.45915 | val_1_rmse: 0.47583 |  0:00:52s
epoch 50 | loss: 0.22076 | val_0_rmse: 0.4631  | val_1_rmse: 0.48071 |  0:00:53s
epoch 51 | loss: 0.22561 | val_0_rmse: 0.45697 | val_1_rmse: 0.47073 |  0:00:54s
epoch 52 | loss: 0.22423 | val_0_rmse: 0.44899 | val_1_rmse: 0.45967 |  0:00:55s
epoch 53 | loss: 0.22102 | val_0_rmse: 0.47088 | val_1_rmse: 0.48189 |  0:00:56s
epoch 54 | loss: 0.21824 | val_0_rmse: 0.45241 | val_1_rmse: 0.46668 |  0:00:57s
epoch 55 | loss: 0.2162  | val_0_rmse: 0.44298 | val_1_rmse: 0.45957 |  0:00:58s
epoch 56 | loss: 0.22384 | val_0_rmse: 0.46265 | val_1_rmse: 0.4803  |  0:00:59s
epoch 57 | loss: 0.22319 | val_0_rmse: 0.47537 | val_1_rmse: 0.48765 |  0:01:00s
epoch 58 | loss: 0.22566 | val_0_rmse: 0.45286 | val_1_rmse: 0.46553 |  0:01:01s
epoch 59 | loss: 0.224   | val_0_rmse: 0.4674  | val_1_rmse: 0.47585 |  0:01:02s
epoch 60 | loss: 0.22661 | val_0_rmse: 0.44992 | val_1_rmse: 0.46246 |  0:01:03s
epoch 61 | loss: 0.21853 | val_0_rmse: 0.458   | val_1_rmse: 0.47933 |  0:01:04s
epoch 62 | loss: 0.22041 | val_0_rmse: 0.45854 | val_1_rmse: 0.47167 |  0:01:05s
epoch 63 | loss: 0.22316 | val_0_rmse: 0.45224 | val_1_rmse: 0.46716 |  0:01:06s
epoch 64 | loss: 0.21755 | val_0_rmse: 0.47874 | val_1_rmse: 0.49008 |  0:01:07s
epoch 65 | loss: 0.22136 | val_0_rmse: 0.45063 | val_1_rmse: 0.46255 |  0:01:08s
epoch 66 | loss: 0.21988 | val_0_rmse: 0.44171 | val_1_rmse: 0.45782 |  0:01:10s
epoch 67 | loss: 0.21542 | val_0_rmse: 0.4478  | val_1_rmse: 0.46218 |  0:01:11s
epoch 68 | loss: 0.21849 | val_0_rmse: 0.45518 | val_1_rmse: 0.46665 |  0:01:12s
epoch 69 | loss: 0.21975 | val_0_rmse: 0.46087 | val_1_rmse: 0.46952 |  0:01:13s
epoch 70 | loss: 0.2183  | val_0_rmse: 0.46996 | val_1_rmse: 0.48126 |  0:01:14s
epoch 71 | loss: 0.22487 | val_0_rmse: 0.45502 | val_1_rmse: 0.46939 |  0:01:15s
epoch 72 | loss: 0.22152 | val_0_rmse: 0.47017 | val_1_rmse: 0.48265 |  0:01:16s
epoch 73 | loss: 0.2174  | val_0_rmse: 0.44413 | val_1_rmse: 0.46259 |  0:01:17s
epoch 74 | loss: 0.21941 | val_0_rmse: 0.46353 | val_1_rmse: 0.47451 |  0:01:18s
epoch 75 | loss: 0.21865 | val_0_rmse: 0.45733 | val_1_rmse: 0.475   |  0:01:19s
epoch 76 | loss: 0.22138 | val_0_rmse: 0.447   | val_1_rmse: 0.46092 |  0:01:20s
epoch 77 | loss: 0.21441 | val_0_rmse: 0.4438  | val_1_rmse: 0.45743 |  0:01:21s
epoch 78 | loss: 0.21475 | val_0_rmse: 0.43808 | val_1_rmse: 0.45433 |  0:01:22s
epoch 79 | loss: 0.21593 | val_0_rmse: 0.44869 | val_1_rmse: 0.46501 |  0:01:23s
epoch 80 | loss: 0.21749 | val_0_rmse: 0.45067 | val_1_rmse: 0.4652  |  0:01:24s
epoch 81 | loss: 0.21561 | val_0_rmse: 0.44429 | val_1_rmse: 0.45843 |  0:01:25s
epoch 82 | loss: 0.21352 | val_0_rmse: 0.43492 | val_1_rmse: 0.45388 |  0:01:26s
epoch 83 | loss: 0.21026 | val_0_rmse: 0.43867 | val_1_rmse: 0.45808 |  0:01:27s
epoch 84 | loss: 0.20916 | val_0_rmse: 0.44606 | val_1_rmse: 0.4594  |  0:01:28s
epoch 85 | loss: 0.20939 | val_0_rmse: 0.43943 | val_1_rmse: 0.45498 |  0:01:29s
epoch 86 | loss: 0.20726 | val_0_rmse: 0.44401 | val_1_rmse: 0.45916 |  0:01:30s
epoch 87 | loss: 0.2173  | val_0_rmse: 0.44459 | val_1_rmse: 0.45828 |  0:01:31s
epoch 88 | loss: 0.21811 | val_0_rmse: 0.46149 | val_1_rmse: 0.47466 |  0:01:32s
epoch 89 | loss: 0.21798 | val_0_rmse: 0.45148 | val_1_rmse: 0.46866 |  0:01:33s
epoch 90 | loss: 0.21377 | val_0_rmse: 0.44746 | val_1_rmse: 0.46358 |  0:01:35s
epoch 91 | loss: 0.21691 | val_0_rmse: 0.44525 | val_1_rmse: 0.45585 |  0:01:36s
epoch 92 | loss: 0.21125 | val_0_rmse: 0.43669 | val_1_rmse: 0.45329 |  0:01:37s
epoch 93 | loss: 0.21262 | val_0_rmse: 0.44119 | val_1_rmse: 0.45838 |  0:01:38s
epoch 94 | loss: 0.20869 | val_0_rmse: 0.44081 | val_1_rmse: 0.45808 |  0:01:39s
epoch 95 | loss: 0.21518 | val_0_rmse: 0.43946 | val_1_rmse: 0.4552  |  0:01:40s
epoch 96 | loss: 0.21046 | val_0_rmse: 0.44449 | val_1_rmse: 0.45642 |  0:01:41s
epoch 97 | loss: 0.21033 | val_0_rmse: 0.43589 | val_1_rmse: 0.45172 |  0:01:42s
epoch 98 | loss: 0.21286 | val_0_rmse: 0.45142 | val_1_rmse: 0.46592 |  0:01:43s
epoch 99 | loss: 0.21508 | val_0_rmse: 0.45641 | val_1_rmse: 0.46878 |  0:01:44s
epoch 100| loss: 0.21606 | val_0_rmse: 0.44556 | val_1_rmse: 0.46396 |  0:01:45s
epoch 101| loss: 0.20972 | val_0_rmse: 0.43441 | val_1_rmse: 0.44907 |  0:01:46s
epoch 102| loss: 0.21087 | val_0_rmse: 0.44686 | val_1_rmse: 0.45948 |  0:01:47s
epoch 103| loss: 0.21138 | val_0_rmse: 0.44892 | val_1_rmse: 0.46621 |  0:01:48s
epoch 104| loss: 0.21325 | val_0_rmse: 0.44392 | val_1_rmse: 0.45824 |  0:01:49s
epoch 105| loss: 0.20831 | val_0_rmse: 0.44234 | val_1_rmse: 0.45712 |  0:01:50s
epoch 106| loss: 0.21354 | val_0_rmse: 0.45909 | val_1_rmse: 0.47309 |  0:01:51s
epoch 107| loss: 0.21185 | val_0_rmse: 0.43671 | val_1_rmse: 0.44957 |  0:01:52s
epoch 108| loss: 0.21146 | val_0_rmse: 0.44486 | val_1_rmse: 0.46077 |  0:01:53s
epoch 109| loss: 0.21017 | val_0_rmse: 0.46391 | val_1_rmse: 0.47277 |  0:01:54s
epoch 110| loss: 0.20866 | val_0_rmse: 0.4483  | val_1_rmse: 0.46737 |  0:01:56s
epoch 111| loss: 0.21151 | val_0_rmse: 0.46053 | val_1_rmse: 0.46891 |  0:01:57s
epoch 112| loss: 0.21016 | val_0_rmse: 0.43703 | val_1_rmse: 0.45019 |  0:01:58s
epoch 113| loss: 0.20936 | val_0_rmse: 0.44216 | val_1_rmse: 0.45328 |  0:01:59s
epoch 114| loss: 0.21102 | val_0_rmse: 0.44404 | val_1_rmse: 0.46167 |  0:02:00s
epoch 115| loss: 0.20914 | val_0_rmse: 0.4449  | val_1_rmse: 0.45787 |  0:02:01s
epoch 116| loss: 0.2155  | val_0_rmse: 0.44903 | val_1_rmse: 0.46407 |  0:02:02s
epoch 117| loss: 0.21656 | val_0_rmse: 0.43868 | val_1_rmse: 0.45787 |  0:02:03s
epoch 118| loss: 0.20834 | val_0_rmse: 0.45948 | val_1_rmse: 0.47834 |  0:02:04s
epoch 119| loss: 0.21798 | val_0_rmse: 0.45764 | val_1_rmse: 0.47193 |  0:02:05s
epoch 120| loss: 0.2103  | val_0_rmse: 0.45439 | val_1_rmse: 0.46688 |  0:02:06s
epoch 121| loss: 0.2205  | val_0_rmse: 0.47359 | val_1_rmse: 0.48009 |  0:02:07s
epoch 122| loss: 0.22458 | val_0_rmse: 0.46085 | val_1_rmse: 0.46763 |  0:02:08s
epoch 123| loss: 0.21565 | val_0_rmse: 0.44508 | val_1_rmse: 0.45413 |  0:02:09s
epoch 124| loss: 0.20581 | val_0_rmse: 0.43772 | val_1_rmse: 0.44964 |  0:02:10s
epoch 125| loss: 0.20979 | val_0_rmse: 0.43214 | val_1_rmse: 0.44971 |  0:02:11s
epoch 126| loss: 0.20239 | val_0_rmse: 0.45346 | val_1_rmse: 0.4683  |  0:02:12s
epoch 127| loss: 0.20368 | val_0_rmse: 0.45361 | val_1_rmse: 0.47161 |  0:02:13s
epoch 128| loss: 0.20837 | val_0_rmse: 0.43113 | val_1_rmse: 0.44795 |  0:02:14s
epoch 129| loss: 0.20895 | val_0_rmse: 0.43365 | val_1_rmse: 0.44632 |  0:02:15s
epoch 130| loss: 0.20608 | val_0_rmse: 0.42886 | val_1_rmse: 0.44511 |  0:02:16s
epoch 131| loss: 0.20747 | val_0_rmse: 0.44316 | val_1_rmse: 0.45286 |  0:02:17s
epoch 132| loss: 0.2053  | val_0_rmse: 0.43297 | val_1_rmse: 0.44788 |  0:02:18s
epoch 133| loss: 0.20421 | val_0_rmse: 0.43266 | val_1_rmse: 0.4478  |  0:02:19s
epoch 134| loss: 0.20706 | val_0_rmse: 0.44845 | val_1_rmse: 0.462   |  0:02:21s
epoch 135| loss: 0.20845 | val_0_rmse: 0.43614 | val_1_rmse: 0.45169 |  0:02:22s
epoch 136| loss: 0.20927 | val_0_rmse: 0.43323 | val_1_rmse: 0.44771 |  0:02:23s
epoch 137| loss: 0.20677 | val_0_rmse: 0.43758 | val_1_rmse: 0.45412 |  0:02:24s
epoch 138| loss: 0.20578 | val_0_rmse: 0.43262 | val_1_rmse: 0.446   |  0:02:25s
epoch 139| loss: 0.19842 | val_0_rmse: 0.43456 | val_1_rmse: 0.45076 |  0:02:26s
epoch 140| loss: 0.19705 | val_0_rmse: 0.42952 | val_1_rmse: 0.44842 |  0:02:27s
epoch 141| loss: 0.1988  | val_0_rmse: 0.43019 | val_1_rmse: 0.44394 |  0:02:28s
epoch 142| loss: 0.20573 | val_0_rmse: 0.44101 | val_1_rmse: 0.45538 |  0:02:29s
epoch 143| loss: 0.22223 | val_0_rmse: 0.50975 | val_1_rmse: 0.51718 |  0:02:30s
epoch 144| loss: 0.22243 | val_0_rmse: 0.45968 | val_1_rmse: 0.47758 |  0:02:31s
epoch 145| loss: 0.23644 | val_0_rmse: 0.62027 | val_1_rmse: 0.62276 |  0:02:32s
epoch 146| loss: 0.26775 | val_0_rmse: 0.51362 | val_1_rmse: 0.52484 |  0:02:33s
epoch 147| loss: 0.25155 | val_0_rmse: 0.56755 | val_1_rmse: 0.5633  |  0:02:34s
epoch 148| loss: 0.25356 | val_0_rmse: 0.49735 | val_1_rmse: 0.49541 |  0:02:35s
epoch 149| loss: 0.23822 | val_0_rmse: 0.46948 | val_1_rmse: 0.47584 |  0:02:36s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.44394
Best weights from best epoch are automatically used!
ended training at: 03:10:38
Feature importance:
[('Area', 0.439277972906912), ('Baths', 0.06177834400746661), ('Beds', 0.0), ('Latitude', 0.3960293544135039), ('Longitude', 0.03379915849296636), ('Month', 0.0), ('Year', 0.06911517017915116)]
Mean squared error is of 6272438770.491362
Mean absolute error:55961.475314638905
MAPE:0.14446500399117088
R2 score:0.8020286441190538
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:10:38
epoch 0  | loss: 0.98038 | val_0_rmse: 0.92846 | val_1_rmse: 0.94485 |  0:00:00s
epoch 1  | loss: 0.5409  | val_0_rmse: 0.81297 | val_1_rmse: 0.82679 |  0:00:00s
epoch 2  | loss: 0.46803 | val_0_rmse: 0.73342 | val_1_rmse: 0.75704 |  0:00:01s
epoch 3  | loss: 0.42663 | val_0_rmse: 0.63977 | val_1_rmse: 0.67585 |  0:00:01s
epoch 4  | loss: 0.37261 | val_0_rmse: 0.66396 | val_1_rmse: 0.68485 |  0:00:02s
epoch 5  | loss: 0.35014 | val_0_rmse: 0.59806 | val_1_rmse: 0.62179 |  0:00:03s
epoch 6  | loss: 0.32609 | val_0_rmse: 0.59371 | val_1_rmse: 0.61589 |  0:00:03s
epoch 7  | loss: 0.32959 | val_0_rmse: 0.548   | val_1_rmse: 0.57251 |  0:00:04s
epoch 8  | loss: 0.31312 | val_0_rmse: 0.55255 | val_1_rmse: 0.57483 |  0:00:04s
epoch 9  | loss: 0.30972 | val_0_rmse: 0.54237 | val_1_rmse: 0.56832 |  0:00:05s
epoch 10 | loss: 0.30174 | val_0_rmse: 0.52388 | val_1_rmse: 0.55459 |  0:00:05s
epoch 11 | loss: 0.30308 | val_0_rmse: 0.51949 | val_1_rmse: 0.54801 |  0:00:06s
epoch 12 | loss: 0.2925  | val_0_rmse: 0.51906 | val_1_rmse: 0.54871 |  0:00:06s
epoch 13 | loss: 0.28906 | val_0_rmse: 0.51136 | val_1_rmse: 0.54625 |  0:00:07s
epoch 14 | loss: 0.2794  | val_0_rmse: 0.51312 | val_1_rmse: 0.54737 |  0:00:07s
epoch 15 | loss: 0.2826  | val_0_rmse: 0.51365 | val_1_rmse: 0.54134 |  0:00:08s
epoch 16 | loss: 0.27796 | val_0_rmse: 0.5112  | val_1_rmse: 0.53612 |  0:00:08s
epoch 17 | loss: 0.27325 | val_0_rmse: 0.50745 | val_1_rmse: 0.52446 |  0:00:09s
epoch 18 | loss: 0.27952 | val_0_rmse: 0.5104  | val_1_rmse: 0.53406 |  0:00:09s
epoch 19 | loss: 0.27256 | val_0_rmse: 0.50178 | val_1_rmse: 0.52692 |  0:00:10s
epoch 20 | loss: 0.27029 | val_0_rmse: 0.50675 | val_1_rmse: 0.54104 |  0:00:10s
epoch 21 | loss: 0.26527 | val_0_rmse: 0.49822 | val_1_rmse: 0.53332 |  0:00:11s
epoch 22 | loss: 0.26955 | val_0_rmse: 0.49372 | val_1_rmse: 0.5242  |  0:00:11s
epoch 23 | loss: 0.27008 | val_0_rmse: 0.50717 | val_1_rmse: 0.54059 |  0:00:12s
epoch 24 | loss: 0.26218 | val_0_rmse: 0.49267 | val_1_rmse: 0.52383 |  0:00:12s
epoch 25 | loss: 0.2619  | val_0_rmse: 0.50341 | val_1_rmse: 0.53439 |  0:00:13s
epoch 26 | loss: 0.26711 | val_0_rmse: 0.49601 | val_1_rmse: 0.52342 |  0:00:13s
epoch 27 | loss: 0.27643 | val_0_rmse: 0.49101 | val_1_rmse: 0.5202  |  0:00:14s
epoch 28 | loss: 0.26163 | val_0_rmse: 0.49261 | val_1_rmse: 0.52125 |  0:00:14s
epoch 29 | loss: 0.26203 | val_0_rmse: 0.49381 | val_1_rmse: 0.52398 |  0:00:15s
epoch 30 | loss: 0.26277 | val_0_rmse: 0.49266 | val_1_rmse: 0.52497 |  0:00:15s
epoch 31 | loss: 0.25691 | val_0_rmse: 0.4892  | val_1_rmse: 0.52022 |  0:00:16s
epoch 32 | loss: 0.25507 | val_0_rmse: 0.49106 | val_1_rmse: 0.52029 |  0:00:16s
epoch 33 | loss: 0.25131 | val_0_rmse: 0.48339 | val_1_rmse: 0.51594 |  0:00:17s
epoch 34 | loss: 0.25142 | val_0_rmse: 0.48238 | val_1_rmse: 0.5158  |  0:00:17s
epoch 35 | loss: 0.25217 | val_0_rmse: 0.4802  | val_1_rmse: 0.51303 |  0:00:18s
epoch 36 | loss: 0.25391 | val_0_rmse: 0.4849  | val_1_rmse: 0.51761 |  0:00:18s
epoch 37 | loss: 0.25574 | val_0_rmse: 0.4832  | val_1_rmse: 0.51565 |  0:00:19s
epoch 38 | loss: 0.25328 | val_0_rmse: 0.48906 | val_1_rmse: 0.52453 |  0:00:19s
epoch 39 | loss: 0.25478 | val_0_rmse: 0.48639 | val_1_rmse: 0.51569 |  0:00:20s
epoch 40 | loss: 0.25347 | val_0_rmse: 0.48206 | val_1_rmse: 0.51801 |  0:00:20s
epoch 41 | loss: 0.25507 | val_0_rmse: 0.48327 | val_1_rmse: 0.51529 |  0:00:21s
epoch 42 | loss: 0.25698 | val_0_rmse: 0.4958  | val_1_rmse: 0.53154 |  0:00:21s
epoch 43 | loss: 0.2545  | val_0_rmse: 0.48409 | val_1_rmse: 0.52404 |  0:00:22s
epoch 44 | loss: 0.24974 | val_0_rmse: 0.48954 | val_1_rmse: 0.53162 |  0:00:22s
epoch 45 | loss: 0.25639 | val_0_rmse: 0.49773 | val_1_rmse: 0.53635 |  0:00:23s
epoch 46 | loss: 0.25465 | val_0_rmse: 0.49736 | val_1_rmse: 0.53877 |  0:00:23s
epoch 47 | loss: 0.25581 | val_0_rmse: 0.49098 | val_1_rmse: 0.5245  |  0:00:24s
epoch 48 | loss: 0.2561  | val_0_rmse: 0.47869 | val_1_rmse: 0.51858 |  0:00:24s
epoch 49 | loss: 0.25604 | val_0_rmse: 0.48676 | val_1_rmse: 0.52473 |  0:00:25s
epoch 50 | loss: 0.24766 | val_0_rmse: 0.47495 | val_1_rmse: 0.51528 |  0:00:25s
epoch 51 | loss: 0.24345 | val_0_rmse: 0.47524 | val_1_rmse: 0.51308 |  0:00:26s
epoch 52 | loss: 0.2495  | val_0_rmse: 0.47904 | val_1_rmse: 0.51365 |  0:00:26s
epoch 53 | loss: 0.25398 | val_0_rmse: 0.47737 | val_1_rmse: 0.51212 |  0:00:27s
epoch 54 | loss: 0.2436  | val_0_rmse: 0.48286 | val_1_rmse: 0.52055 |  0:00:27s
epoch 55 | loss: 0.24618 | val_0_rmse: 0.47415 | val_1_rmse: 0.51049 |  0:00:28s
epoch 56 | loss: 0.23897 | val_0_rmse: 0.46942 | val_1_rmse: 0.50785 |  0:00:28s
epoch 57 | loss: 0.23917 | val_0_rmse: 0.46888 | val_1_rmse: 0.50297 |  0:00:29s
epoch 58 | loss: 0.23605 | val_0_rmse: 0.47465 | val_1_rmse: 0.51713 |  0:00:29s
epoch 59 | loss: 0.24221 | val_0_rmse: 0.47232 | val_1_rmse: 0.5115  |  0:00:30s
epoch 60 | loss: 0.23973 | val_0_rmse: 0.47635 | val_1_rmse: 0.51335 |  0:00:30s
epoch 61 | loss: 0.24493 | val_0_rmse: 0.47238 | val_1_rmse: 0.50907 |  0:00:31s
epoch 62 | loss: 0.25384 | val_0_rmse: 0.47018 | val_1_rmse: 0.51168 |  0:00:31s
epoch 63 | loss: 0.24346 | val_0_rmse: 0.47352 | val_1_rmse: 0.51123 |  0:00:32s
epoch 64 | loss: 0.24227 | val_0_rmse: 0.47239 | val_1_rmse: 0.51588 |  0:00:32s
epoch 65 | loss: 0.24131 | val_0_rmse: 0.49781 | val_1_rmse: 0.54059 |  0:00:33s
epoch 66 | loss: 0.25001 | val_0_rmse: 0.49624 | val_1_rmse: 0.53901 |  0:00:33s
epoch 67 | loss: 0.25611 | val_0_rmse: 0.47812 | val_1_rmse: 0.52063 |  0:00:34s
epoch 68 | loss: 0.24859 | val_0_rmse: 0.48113 | val_1_rmse: 0.5194  |  0:00:34s
epoch 69 | loss: 0.24451 | val_0_rmse: 0.49095 | val_1_rmse: 0.52673 |  0:00:35s
epoch 70 | loss: 0.25524 | val_0_rmse: 0.47523 | val_1_rmse: 0.51209 |  0:00:35s
epoch 71 | loss: 0.24952 | val_0_rmse: 0.47373 | val_1_rmse: 0.51006 |  0:00:36s
epoch 72 | loss: 0.24491 | val_0_rmse: 0.46723 | val_1_rmse: 0.50853 |  0:00:36s
epoch 73 | loss: 0.24377 | val_0_rmse: 0.47547 | val_1_rmse: 0.51957 |  0:00:37s
epoch 74 | loss: 0.24145 | val_0_rmse: 0.47031 | val_1_rmse: 0.50602 |  0:00:37s
epoch 75 | loss: 0.24201 | val_0_rmse: 0.47798 | val_1_rmse: 0.51811 |  0:00:38s
epoch 76 | loss: 0.24176 | val_0_rmse: 0.46952 | val_1_rmse: 0.51064 |  0:00:38s
epoch 77 | loss: 0.24364 | val_0_rmse: 0.4652  | val_1_rmse: 0.50627 |  0:00:38s
epoch 78 | loss: 0.24746 | val_0_rmse: 0.46878 | val_1_rmse: 0.51445 |  0:00:39s
epoch 79 | loss: 0.24401 | val_0_rmse: 0.46332 | val_1_rmse: 0.50723 |  0:00:39s
epoch 80 | loss: 0.23889 | val_0_rmse: 0.47367 | val_1_rmse: 0.50943 |  0:00:40s
epoch 81 | loss: 0.24118 | val_0_rmse: 0.47099 | val_1_rmse: 0.51532 |  0:00:40s
epoch 82 | loss: 0.23633 | val_0_rmse: 0.46524 | val_1_rmse: 0.51065 |  0:00:41s
epoch 83 | loss: 0.23784 | val_0_rmse: 0.47557 | val_1_rmse: 0.51439 |  0:00:41s
epoch 84 | loss: 0.23823 | val_0_rmse: 0.4676  | val_1_rmse: 0.5122  |  0:00:42s
epoch 85 | loss: 0.23612 | val_0_rmse: 0.47138 | val_1_rmse: 0.51383 |  0:00:42s
epoch 86 | loss: 0.24254 | val_0_rmse: 0.46357 | val_1_rmse: 0.50984 |  0:00:43s
epoch 87 | loss: 0.23289 | val_0_rmse: 0.46087 | val_1_rmse: 0.50698 |  0:00:43s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.50297
Best weights from best epoch are automatically used!
ended training at: 03:11:22
Feature importance:
[('Area', 0.21951824442673354), ('Baths', 0.008321358478640462), ('Beds', 0.09832659539039838), ('Latitude', 0.2913419998504293), ('Longitude', 0.3084106034025839), ('Month', 0.01731011710267096), ('Year', 0.05677108134854346)]
Mean squared error is of 20450050325.631184
Mean absolute error:103551.97315910885
MAPE:0.17080680375288668
R2 score:0.7534493394621004
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:11:22
epoch 0  | loss: 1.13328 | val_0_rmse: 1.13441 | val_1_rmse: 1.10505 |  0:00:00s
epoch 1  | loss: 0.61366 | val_0_rmse: 0.85509 | val_1_rmse: 0.85711 |  0:00:01s
epoch 2  | loss: 0.51283 | val_0_rmse: 0.71722 | val_1_rmse: 0.72415 |  0:00:01s
epoch 3  | loss: 0.44189 | val_0_rmse: 0.68938 | val_1_rmse: 0.71854 |  0:00:02s
epoch 4  | loss: 0.40777 | val_0_rmse: 0.65516 | val_1_rmse: 0.67595 |  0:00:02s
epoch 5  | loss: 0.39035 | val_0_rmse: 0.60954 | val_1_rmse: 0.62092 |  0:00:03s
epoch 6  | loss: 0.36793 | val_0_rmse: 0.58965 | val_1_rmse: 0.59718 |  0:00:03s
epoch 7  | loss: 0.36448 | val_0_rmse: 0.62636 | val_1_rmse: 0.63504 |  0:00:04s
epoch 8  | loss: 0.35636 | val_0_rmse: 0.60423 | val_1_rmse: 0.61497 |  0:00:04s
epoch 9  | loss: 0.35396 | val_0_rmse: 0.56467 | val_1_rmse: 0.57275 |  0:00:05s
epoch 10 | loss: 0.32924 | val_0_rmse: 0.54444 | val_1_rmse: 0.55415 |  0:00:05s
epoch 11 | loss: 0.33135 | val_0_rmse: 0.54927 | val_1_rmse: 0.55773 |  0:00:06s
epoch 12 | loss: 0.32218 | val_0_rmse: 0.54208 | val_1_rmse: 0.55258 |  0:00:06s
epoch 13 | loss: 0.31019 | val_0_rmse: 0.54129 | val_1_rmse: 0.55659 |  0:00:07s
epoch 14 | loss: 0.30312 | val_0_rmse: 0.53654 | val_1_rmse: 0.55715 |  0:00:07s
epoch 15 | loss: 0.29672 | val_0_rmse: 0.52637 | val_1_rmse: 0.54632 |  0:00:08s
epoch 16 | loss: 0.29841 | val_0_rmse: 0.51938 | val_1_rmse: 0.53816 |  0:00:08s
epoch 17 | loss: 0.2988  | val_0_rmse: 0.52086 | val_1_rmse: 0.54029 |  0:00:09s
epoch 18 | loss: 0.29201 | val_0_rmse: 0.51136 | val_1_rmse: 0.53809 |  0:00:09s
epoch 19 | loss: 0.28481 | val_0_rmse: 0.5185  | val_1_rmse: 0.53173 |  0:00:10s
epoch 20 | loss: 0.28972 | val_0_rmse: 0.51801 | val_1_rmse: 0.53951 |  0:00:10s
epoch 21 | loss: 0.28405 | val_0_rmse: 0.5169  | val_1_rmse: 0.53797 |  0:00:11s
epoch 22 | loss: 0.28555 | val_0_rmse: 0.52048 | val_1_rmse: 0.53824 |  0:00:11s
epoch 23 | loss: 0.27692 | val_0_rmse: 0.50991 | val_1_rmse: 0.53338 |  0:00:12s
epoch 24 | loss: 0.28253 | val_0_rmse: 0.50983 | val_1_rmse: 0.52451 |  0:00:12s
epoch 25 | loss: 0.2799  | val_0_rmse: 0.50934 | val_1_rmse: 0.5212  |  0:00:13s
epoch 26 | loss: 0.28118 | val_0_rmse: 0.50933 | val_1_rmse: 0.52498 |  0:00:13s
epoch 27 | loss: 0.2825  | val_0_rmse: 0.51715 | val_1_rmse: 0.53627 |  0:00:14s
epoch 28 | loss: 0.27818 | val_0_rmse: 0.50623 | val_1_rmse: 0.52623 |  0:00:14s
epoch 29 | loss: 0.27754 | val_0_rmse: 0.5095  | val_1_rmse: 0.52765 |  0:00:15s
epoch 30 | loss: 0.28434 | val_0_rmse: 0.52671 | val_1_rmse: 0.54696 |  0:00:15s
epoch 31 | loss: 0.28895 | val_0_rmse: 0.52528 | val_1_rmse: 0.5434  |  0:00:16s
epoch 32 | loss: 0.27624 | val_0_rmse: 0.51437 | val_1_rmse: 0.53168 |  0:00:16s
epoch 33 | loss: 0.27502 | val_0_rmse: 0.50472 | val_1_rmse: 0.52572 |  0:00:17s
epoch 34 | loss: 0.27432 | val_0_rmse: 0.51518 | val_1_rmse: 0.5363  |  0:00:17s
epoch 35 | loss: 0.27688 | val_0_rmse: 0.49854 | val_1_rmse: 0.51573 |  0:00:18s
epoch 36 | loss: 0.27676 | val_0_rmse: 0.49468 | val_1_rmse: 0.51561 |  0:00:18s
epoch 37 | loss: 0.2647  | val_0_rmse: 0.4962  | val_1_rmse: 0.51744 |  0:00:19s
epoch 38 | loss: 0.26193 | val_0_rmse: 0.50696 | val_1_rmse: 0.53193 |  0:00:19s
epoch 39 | loss: 0.26622 | val_0_rmse: 0.5005  | val_1_rmse: 0.5249  |  0:00:20s
epoch 40 | loss: 0.26593 | val_0_rmse: 0.49044 | val_1_rmse: 0.5228  |  0:00:20s
epoch 41 | loss: 0.26376 | val_0_rmse: 0.49182 | val_1_rmse: 0.52128 |  0:00:21s
epoch 42 | loss: 0.25999 | val_0_rmse: 0.49932 | val_1_rmse: 0.52674 |  0:00:21s
epoch 43 | loss: 0.26829 | val_0_rmse: 0.49901 | val_1_rmse: 0.52353 |  0:00:22s
epoch 44 | loss: 0.26935 | val_0_rmse: 0.49857 | val_1_rmse: 0.52722 |  0:00:22s
epoch 45 | loss: 0.26585 | val_0_rmse: 0.50665 | val_1_rmse: 0.53663 |  0:00:23s
epoch 46 | loss: 0.26567 | val_0_rmse: 0.51969 | val_1_rmse: 0.54813 |  0:00:23s
epoch 47 | loss: 0.27211 | val_0_rmse: 0.51925 | val_1_rmse: 0.54811 |  0:00:24s
epoch 48 | loss: 0.26573 | val_0_rmse: 0.48786 | val_1_rmse: 0.51543 |  0:00:24s
epoch 49 | loss: 0.26004 | val_0_rmse: 0.48781 | val_1_rmse: 0.51499 |  0:00:25s
epoch 50 | loss: 0.27132 | val_0_rmse: 0.52116 | val_1_rmse: 0.5494  |  0:00:25s
epoch 51 | loss: 0.27135 | val_0_rmse: 0.4973  | val_1_rmse: 0.52315 |  0:00:26s
epoch 52 | loss: 0.26342 | val_0_rmse: 0.49836 | val_1_rmse: 0.5277  |  0:00:26s
epoch 53 | loss: 0.26308 | val_0_rmse: 0.49294 | val_1_rmse: 0.51864 |  0:00:27s
epoch 54 | loss: 0.26605 | val_0_rmse: 0.50586 | val_1_rmse: 0.52934 |  0:00:27s
epoch 55 | loss: 0.26078 | val_0_rmse: 0.48635 | val_1_rmse: 0.51538 |  0:00:28s
epoch 56 | loss: 0.26436 | val_0_rmse: 0.49856 | val_1_rmse: 0.52445 |  0:00:28s
epoch 57 | loss: 0.26727 | val_0_rmse: 0.4835  | val_1_rmse: 0.51184 |  0:00:29s
epoch 58 | loss: 0.26389 | val_0_rmse: 0.48827 | val_1_rmse: 0.51303 |  0:00:29s
epoch 59 | loss: 0.25728 | val_0_rmse: 0.49311 | val_1_rmse: 0.52628 |  0:00:30s
epoch 60 | loss: 0.26037 | val_0_rmse: 0.48799 | val_1_rmse: 0.52521 |  0:00:30s
epoch 61 | loss: 0.2592  | val_0_rmse: 0.48294 | val_1_rmse: 0.51075 |  0:00:31s
epoch 62 | loss: 0.25539 | val_0_rmse: 0.4792  | val_1_rmse: 0.50868 |  0:00:31s
epoch 63 | loss: 0.25431 | val_0_rmse: 0.47808 | val_1_rmse: 0.50651 |  0:00:32s
epoch 64 | loss: 0.25406 | val_0_rmse: 0.48204 | val_1_rmse: 0.51517 |  0:00:32s
epoch 65 | loss: 0.26143 | val_0_rmse: 0.49288 | val_1_rmse: 0.52303 |  0:00:33s
epoch 66 | loss: 0.25787 | val_0_rmse: 0.48348 | val_1_rmse: 0.51451 |  0:00:33s
epoch 67 | loss: 0.2516  | val_0_rmse: 0.47743 | val_1_rmse: 0.51028 |  0:00:34s
epoch 68 | loss: 0.24721 | val_0_rmse: 0.47649 | val_1_rmse: 0.50369 |  0:00:34s
epoch 69 | loss: 0.24841 | val_0_rmse: 0.48094 | val_1_rmse: 0.51335 |  0:00:35s
epoch 70 | loss: 0.25661 | val_0_rmse: 0.49481 | val_1_rmse: 0.52409 |  0:00:35s
epoch 71 | loss: 0.25265 | val_0_rmse: 0.48594 | val_1_rmse: 0.51597 |  0:00:36s
epoch 72 | loss: 0.25989 | val_0_rmse: 0.51013 | val_1_rmse: 0.53658 |  0:00:36s
epoch 73 | loss: 0.25895 | val_0_rmse: 0.4865  | val_1_rmse: 0.51166 |  0:00:37s
epoch 74 | loss: 0.26217 | val_0_rmse: 0.50485 | val_1_rmse: 0.53673 |  0:00:37s
epoch 75 | loss: 0.26412 | val_0_rmse: 0.52191 | val_1_rmse: 0.55753 |  0:00:38s
epoch 76 | loss: 0.27939 | val_0_rmse: 0.49584 | val_1_rmse: 0.53755 |  0:00:38s
epoch 77 | loss: 0.26633 | val_0_rmse: 0.48666 | val_1_rmse: 0.52108 |  0:00:39s
epoch 78 | loss: 0.2599  | val_0_rmse: 0.4974  | val_1_rmse: 0.53542 |  0:00:39s
epoch 79 | loss: 0.26425 | val_0_rmse: 0.47603 | val_1_rmse: 0.50814 |  0:00:40s
epoch 80 | loss: 0.24894 | val_0_rmse: 0.52014 | val_1_rmse: 0.55585 |  0:00:40s
epoch 81 | loss: 0.25914 | val_0_rmse: 0.48861 | val_1_rmse: 0.52044 |  0:00:41s
epoch 82 | loss: 0.23915 | val_0_rmse: 0.48009 | val_1_rmse: 0.51742 |  0:00:41s
epoch 83 | loss: 0.23892 | val_0_rmse: 0.47187 | val_1_rmse: 0.51105 |  0:00:42s
epoch 84 | loss: 0.24736 | val_0_rmse: 0.48693 | val_1_rmse: 0.5207  |  0:00:42s
epoch 85 | loss: 0.24857 | val_0_rmse: 0.48166 | val_1_rmse: 0.52365 |  0:00:43s
epoch 86 | loss: 0.24213 | val_0_rmse: 0.46813 | val_1_rmse: 0.50939 |  0:00:43s
epoch 87 | loss: 0.25094 | val_0_rmse: 0.47213 | val_1_rmse: 0.50985 |  0:00:44s
epoch 88 | loss: 0.24166 | val_0_rmse: 0.47805 | val_1_rmse: 0.51548 |  0:00:44s
epoch 89 | loss: 0.24256 | val_0_rmse: 0.47744 | val_1_rmse: 0.51111 |  0:00:45s
epoch 90 | loss: 0.24611 | val_0_rmse: 0.49428 | val_1_rmse: 0.52781 |  0:00:45s
epoch 91 | loss: 0.26324 | val_0_rmse: 0.47705 | val_1_rmse: 0.52194 |  0:00:46s
epoch 92 | loss: 0.2524  | val_0_rmse: 0.49811 | val_1_rmse: 0.53616 |  0:00:46s
epoch 93 | loss: 0.25257 | val_0_rmse: 0.50494 | val_1_rmse: 0.53028 |  0:00:47s
epoch 94 | loss: 0.25467 | val_0_rmse: 0.48188 | val_1_rmse: 0.51677 |  0:00:47s
epoch 95 | loss: 0.23976 | val_0_rmse: 0.47945 | val_1_rmse: 0.51303 |  0:00:48s
epoch 96 | loss: 0.23875 | val_0_rmse: 0.46941 | val_1_rmse: 0.52224 |  0:00:48s
epoch 97 | loss: 0.24739 | val_0_rmse: 0.47834 | val_1_rmse: 0.51437 |  0:00:49s
epoch 98 | loss: 0.24655 | val_0_rmse: 0.47259 | val_1_rmse: 0.51559 |  0:00:49s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.50369
Best weights from best epoch are automatically used!
ended training at: 03:12:12
Feature importance:
[('Area', 0.19318135032669162), ('Baths', 0.08250598665407381), ('Beds', 0.14527492607638812), ('Latitude', 0.30210971777683593), ('Longitude', 0.21509835682006273), ('Month', 0.021507900905242846), ('Year', 0.040321761440704894)]
Mean squared error is of 21095852802.790386
Mean absolute error:106155.75978157861
MAPE:0.18499336947768918
R2 score:0.7393392794043887
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:12:12
epoch 0  | loss: 1.04434 | val_0_rmse: 1.06058 | val_1_rmse: 0.9929  |  0:00:00s
epoch 1  | loss: 0.54926 | val_0_rmse: 0.78251 | val_1_rmse: 0.7842  |  0:00:01s
epoch 2  | loss: 0.45379 | val_0_rmse: 0.70101 | val_1_rmse: 0.71649 |  0:00:01s
epoch 3  | loss: 0.4019  | val_0_rmse: 0.66259 | val_1_rmse: 0.68421 |  0:00:02s
epoch 4  | loss: 0.38825 | val_0_rmse: 0.63984 | val_1_rmse: 0.64826 |  0:00:02s
epoch 5  | loss: 0.37005 | val_0_rmse: 0.61188 | val_1_rmse: 0.63127 |  0:00:03s
epoch 6  | loss: 0.36092 | val_0_rmse: 0.59198 | val_1_rmse: 0.60397 |  0:00:03s
epoch 7  | loss: 0.35314 | val_0_rmse: 0.57591 | val_1_rmse: 0.59662 |  0:00:04s
epoch 8  | loss: 0.33842 | val_0_rmse: 0.57257 | val_1_rmse: 0.59206 |  0:00:04s
epoch 9  | loss: 0.33734 | val_0_rmse: 0.55087 | val_1_rmse: 0.56838 |  0:00:05s
epoch 10 | loss: 0.3324  | val_0_rmse: 0.54604 | val_1_rmse: 0.56936 |  0:00:05s
epoch 11 | loss: 0.31973 | val_0_rmse: 0.53541 | val_1_rmse: 0.55137 |  0:00:06s
epoch 12 | loss: 0.31083 | val_0_rmse: 0.53785 | val_1_rmse: 0.55295 |  0:00:06s
epoch 13 | loss: 0.30723 | val_0_rmse: 0.5405  | val_1_rmse: 0.5595  |  0:00:07s
epoch 14 | loss: 0.31294 | val_0_rmse: 0.5371  | val_1_rmse: 0.56112 |  0:00:07s
epoch 15 | loss: 0.31289 | val_0_rmse: 0.5582  | val_1_rmse: 0.57283 |  0:00:08s
epoch 16 | loss: 0.31406 | val_0_rmse: 0.55072 | val_1_rmse: 0.57531 |  0:00:08s
epoch 17 | loss: 0.30233 | val_0_rmse: 0.54129 | val_1_rmse: 0.56902 |  0:00:09s
epoch 18 | loss: 0.30297 | val_0_rmse: 0.54556 | val_1_rmse: 0.57202 |  0:00:09s
epoch 19 | loss: 0.29548 | val_0_rmse: 0.54513 | val_1_rmse: 0.5614  |  0:00:10s
epoch 20 | loss: 0.29834 | val_0_rmse: 0.52387 | val_1_rmse: 0.5501  |  0:00:10s
epoch 21 | loss: 0.29187 | val_0_rmse: 0.52093 | val_1_rmse: 0.53854 |  0:00:11s
epoch 22 | loss: 0.29192 | val_0_rmse: 0.52466 | val_1_rmse: 0.54568 |  0:00:11s
epoch 23 | loss: 0.29033 | val_0_rmse: 0.52041 | val_1_rmse: 0.53832 |  0:00:12s
epoch 24 | loss: 0.28574 | val_0_rmse: 0.53343 | val_1_rmse: 0.55835 |  0:00:12s
epoch 25 | loss: 0.29857 | val_0_rmse: 0.53345 | val_1_rmse: 0.55182 |  0:00:13s
epoch 26 | loss: 0.29059 | val_0_rmse: 0.51989 | val_1_rmse: 0.55198 |  0:00:13s
epoch 27 | loss: 0.28582 | val_0_rmse: 0.53522 | val_1_rmse: 0.56217 |  0:00:14s
epoch 28 | loss: 0.28889 | val_0_rmse: 0.52067 | val_1_rmse: 0.5496  |  0:00:14s
epoch 29 | loss: 0.27936 | val_0_rmse: 0.5159  | val_1_rmse: 0.55085 |  0:00:15s
epoch 30 | loss: 0.28171 | val_0_rmse: 0.51803 | val_1_rmse: 0.54233 |  0:00:15s
epoch 31 | loss: 0.29168 | val_0_rmse: 0.51394 | val_1_rmse: 0.53134 |  0:00:16s
epoch 32 | loss: 0.28273 | val_0_rmse: 0.52574 | val_1_rmse: 0.55775 |  0:00:16s
epoch 33 | loss: 0.28164 | val_0_rmse: 0.52435 | val_1_rmse: 0.53273 |  0:00:17s
epoch 34 | loss: 0.28856 | val_0_rmse: 0.52749 | val_1_rmse: 0.53701 |  0:00:17s
epoch 35 | loss: 0.2915  | val_0_rmse: 0.53072 | val_1_rmse: 0.55531 |  0:00:18s
epoch 36 | loss: 0.30135 | val_0_rmse: 0.51471 | val_1_rmse: 0.52417 |  0:00:18s
epoch 37 | loss: 0.28619 | val_0_rmse: 0.51305 | val_1_rmse: 0.54262 |  0:00:19s
epoch 38 | loss: 0.2881  | val_0_rmse: 0.51335 | val_1_rmse: 0.5359  |  0:00:19s
epoch 39 | loss: 0.27862 | val_0_rmse: 0.50109 | val_1_rmse: 0.51426 |  0:00:20s
epoch 40 | loss: 0.27291 | val_0_rmse: 0.52232 | val_1_rmse: 0.54706 |  0:00:20s
epoch 41 | loss: 0.27021 | val_0_rmse: 0.50196 | val_1_rmse: 0.51804 |  0:00:21s
epoch 42 | loss: 0.27053 | val_0_rmse: 0.49489 | val_1_rmse: 0.51717 |  0:00:21s
epoch 43 | loss: 0.27224 | val_0_rmse: 0.49834 | val_1_rmse: 0.51687 |  0:00:22s
epoch 44 | loss: 0.26731 | val_0_rmse: 0.50133 | val_1_rmse: 0.52109 |  0:00:22s
epoch 45 | loss: 0.26709 | val_0_rmse: 0.49609 | val_1_rmse: 0.51793 |  0:00:23s
epoch 46 | loss: 0.26647 | val_0_rmse: 0.48921 | val_1_rmse: 0.50645 |  0:00:23s
epoch 47 | loss: 0.25986 | val_0_rmse: 0.49173 | val_1_rmse: 0.51414 |  0:00:24s
epoch 48 | loss: 0.2534  | val_0_rmse: 0.48821 | val_1_rmse: 0.51036 |  0:00:24s
epoch 49 | loss: 0.26065 | val_0_rmse: 0.49256 | val_1_rmse: 0.5198  |  0:00:25s
epoch 50 | loss: 0.25785 | val_0_rmse: 0.48135 | val_1_rmse: 0.49755 |  0:00:25s
epoch 51 | loss: 0.25684 | val_0_rmse: 0.48468 | val_1_rmse: 0.50168 |  0:00:26s
epoch 52 | loss: 0.25825 | val_0_rmse: 0.47911 | val_1_rmse: 0.50104 |  0:00:26s
epoch 53 | loss: 0.25804 | val_0_rmse: 0.48246 | val_1_rmse: 0.50212 |  0:00:27s
epoch 54 | loss: 0.25645 | val_0_rmse: 0.47661 | val_1_rmse: 0.49253 |  0:00:27s
epoch 55 | loss: 0.25157 | val_0_rmse: 0.5025  | val_1_rmse: 0.52645 |  0:00:28s
epoch 56 | loss: 0.26103 | val_0_rmse: 0.48295 | val_1_rmse: 0.5101  |  0:00:28s
epoch 57 | loss: 0.26566 | val_0_rmse: 0.49095 | val_1_rmse: 0.51079 |  0:00:29s
epoch 58 | loss: 0.26615 | val_0_rmse: 0.52853 | val_1_rmse: 0.55166 |  0:00:29s
epoch 59 | loss: 0.25877 | val_0_rmse: 0.49021 | val_1_rmse: 0.49971 |  0:00:30s
epoch 60 | loss: 0.25783 | val_0_rmse: 0.48349 | val_1_rmse: 0.50365 |  0:00:30s
epoch 61 | loss: 0.25976 | val_0_rmse: 0.49699 | val_1_rmse: 0.51326 |  0:00:31s
epoch 62 | loss: 0.26061 | val_0_rmse: 0.49531 | val_1_rmse: 0.51412 |  0:00:31s
epoch 63 | loss: 0.26203 | val_0_rmse: 0.48916 | val_1_rmse: 0.50292 |  0:00:32s
epoch 64 | loss: 0.2677  | val_0_rmse: 0.49134 | val_1_rmse: 0.51028 |  0:00:32s
epoch 65 | loss: 0.25845 | val_0_rmse: 0.49448 | val_1_rmse: 0.5157  |  0:00:33s
epoch 66 | loss: 0.25998 | val_0_rmse: 0.48643 | val_1_rmse: 0.5008  |  0:00:33s
epoch 67 | loss: 0.26212 | val_0_rmse: 0.49683 | val_1_rmse: 0.51999 |  0:00:34s
epoch 68 | loss: 0.25924 | val_0_rmse: 0.50231 | val_1_rmse: 0.52205 |  0:00:34s
epoch 69 | loss: 0.2568  | val_0_rmse: 0.47907 | val_1_rmse: 0.49637 |  0:00:35s
epoch 70 | loss: 0.24872 | val_0_rmse: 0.48559 | val_1_rmse: 0.50267 |  0:00:35s
epoch 71 | loss: 0.25173 | val_0_rmse: 0.479   | val_1_rmse: 0.49202 |  0:00:35s
epoch 72 | loss: 0.2462  | val_0_rmse: 0.47486 | val_1_rmse: 0.49748 |  0:00:36s
epoch 73 | loss: 0.24369 | val_0_rmse: 0.48111 | val_1_rmse: 0.49329 |  0:00:36s
epoch 74 | loss: 0.24747 | val_0_rmse: 0.47788 | val_1_rmse: 0.49645 |  0:00:37s
epoch 75 | loss: 0.25255 | val_0_rmse: 0.48252 | val_1_rmse: 0.50303 |  0:00:37s
epoch 76 | loss: 0.2439  | val_0_rmse: 0.4743  | val_1_rmse: 0.49988 |  0:00:38s
epoch 77 | loss: 0.24706 | val_0_rmse: 0.48735 | val_1_rmse: 0.50819 |  0:00:39s
epoch 78 | loss: 0.2485  | val_0_rmse: 0.47545 | val_1_rmse: 0.49521 |  0:00:39s
epoch 79 | loss: 0.24783 | val_0_rmse: 0.4721  | val_1_rmse: 0.49298 |  0:00:40s
epoch 80 | loss: 0.25083 | val_0_rmse: 0.49313 | val_1_rmse: 0.51786 |  0:00:40s
epoch 81 | loss: 0.25057 | val_0_rmse: 0.49488 | val_1_rmse: 0.51001 |  0:00:40s
epoch 82 | loss: 0.24452 | val_0_rmse: 0.48173 | val_1_rmse: 0.50517 |  0:00:41s
epoch 83 | loss: 0.25255 | val_0_rmse: 0.48103 | val_1_rmse: 0.50924 |  0:00:41s
epoch 84 | loss: 0.25114 | val_0_rmse: 0.47717 | val_1_rmse: 0.49317 |  0:00:42s
epoch 85 | loss: 0.24657 | val_0_rmse: 0.47429 | val_1_rmse: 0.49208 |  0:00:42s
epoch 86 | loss: 0.24127 | val_0_rmse: 0.475   | val_1_rmse: 0.49606 |  0:00:43s
epoch 87 | loss: 0.24251 | val_0_rmse: 0.47453 | val_1_rmse: 0.49914 |  0:00:43s
epoch 88 | loss: 0.24313 | val_0_rmse: 0.47422 | val_1_rmse: 0.49338 |  0:00:44s
epoch 89 | loss: 0.24099 | val_0_rmse: 0.46993 | val_1_rmse: 0.50009 |  0:00:44s
epoch 90 | loss: 0.24799 | val_0_rmse: 0.47252 | val_1_rmse: 0.49864 |  0:00:45s
epoch 91 | loss: 0.24878 | val_0_rmse: 0.47807 | val_1_rmse: 0.50281 |  0:00:45s
epoch 92 | loss: 0.24594 | val_0_rmse: 0.47999 | val_1_rmse: 0.50523 |  0:00:46s
epoch 93 | loss: 0.24045 | val_0_rmse: 0.49712 | val_1_rmse: 0.51469 |  0:00:46s
epoch 94 | loss: 0.24018 | val_0_rmse: 0.49198 | val_1_rmse: 0.51656 |  0:00:47s
epoch 95 | loss: 0.24263 | val_0_rmse: 0.48181 | val_1_rmse: 0.50387 |  0:00:47s
epoch 96 | loss: 0.24365 | val_0_rmse: 0.48678 | val_1_rmse: 0.51379 |  0:00:48s
epoch 97 | loss: 0.24273 | val_0_rmse: 0.46455 | val_1_rmse: 0.48479 |  0:00:48s
epoch 98 | loss: 0.24053 | val_0_rmse: 0.46828 | val_1_rmse: 0.49368 |  0:00:49s
epoch 99 | loss: 0.23812 | val_0_rmse: 0.46458 | val_1_rmse: 0.49181 |  0:00:49s
epoch 100| loss: 0.24181 | val_0_rmse: 0.46378 | val_1_rmse: 0.49158 |  0:00:50s
epoch 101| loss: 0.23518 | val_0_rmse: 0.47084 | val_1_rmse: 0.49779 |  0:00:50s
epoch 102| loss: 0.24143 | val_0_rmse: 0.50526 | val_1_rmse: 0.52368 |  0:00:51s
epoch 103| loss: 0.24234 | val_0_rmse: 0.47994 | val_1_rmse: 0.50966 |  0:00:51s
epoch 104| loss: 0.2399  | val_0_rmse: 0.46217 | val_1_rmse: 0.49089 |  0:00:52s
epoch 105| loss: 0.23634 | val_0_rmse: 0.46838 | val_1_rmse: 0.49014 |  0:00:52s
epoch 106| loss: 0.23938 | val_0_rmse: 0.46602 | val_1_rmse: 0.49716 |  0:00:53s
epoch 107| loss: 0.24229 | val_0_rmse: 0.47856 | val_1_rmse: 0.507   |  0:00:53s
epoch 108| loss: 0.23725 | val_0_rmse: 0.47432 | val_1_rmse: 0.50402 |  0:00:54s
epoch 109| loss: 0.24311 | val_0_rmse: 0.47197 | val_1_rmse: 0.50048 |  0:00:54s
epoch 110| loss: 0.2367  | val_0_rmse: 0.46588 | val_1_rmse: 0.49784 |  0:00:55s
epoch 111| loss: 0.23269 | val_0_rmse: 0.46473 | val_1_rmse: 0.49896 |  0:00:55s
epoch 112| loss: 0.2366  | val_0_rmse: 0.46918 | val_1_rmse: 0.49998 |  0:00:56s
epoch 113| loss: 0.24006 | val_0_rmse: 0.47193 | val_1_rmse: 0.50802 |  0:00:56s
epoch 114| loss: 0.23554 | val_0_rmse: 0.46826 | val_1_rmse: 0.49018 |  0:00:57s
epoch 115| loss: 0.23522 | val_0_rmse: 0.46025 | val_1_rmse: 0.491   |  0:00:57s
epoch 116| loss: 0.24084 | val_0_rmse: 0.47624 | val_1_rmse: 0.5045  |  0:00:58s
epoch 117| loss: 0.24217 | val_0_rmse: 0.46457 | val_1_rmse: 0.49432 |  0:00:58s
epoch 118| loss: 0.23061 | val_0_rmse: 0.45982 | val_1_rmse: 0.48619 |  0:00:59s
epoch 119| loss: 0.23969 | val_0_rmse: 0.469   | val_1_rmse: 0.49957 |  0:00:59s
epoch 120| loss: 0.23316 | val_0_rmse: 0.48341 | val_1_rmse: 0.51324 |  0:01:00s
epoch 121| loss: 0.2417  | val_0_rmse: 0.46897 | val_1_rmse: 0.49733 |  0:01:00s
epoch 122| loss: 0.24648 | val_0_rmse: 0.48018 | val_1_rmse: 0.50877 |  0:01:01s
epoch 123| loss: 0.23876 | val_0_rmse: 0.46359 | val_1_rmse: 0.4903  |  0:01:01s
epoch 124| loss: 0.23314 | val_0_rmse: 0.46416 | val_1_rmse: 0.50383 |  0:01:02s
epoch 125| loss: 0.23128 | val_0_rmse: 0.49507 | val_1_rmse: 0.52312 |  0:01:02s
epoch 126| loss: 0.24638 | val_0_rmse: 0.46893 | val_1_rmse: 0.50277 |  0:01:03s
epoch 127| loss: 0.24726 | val_0_rmse: 0.48923 | val_1_rmse: 0.51689 |  0:01:03s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.48479
Best weights from best epoch are automatically used!
ended training at: 03:13:16
Feature importance:
[('Area', 0.31248645973238504), ('Baths', 0.018118534223665165), ('Beds', 0.10191541024413366), ('Latitude', 0.14923470793097454), ('Longitude', 0.31577548530357014), ('Month', 0.014364936545449802), ('Year', 0.08810446601982166)]
Mean squared error is of 20831716189.89775
Mean absolute error:104124.95344416241
MAPE:0.18449992699177833
R2 score:0.7534563601764304
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:13:16
epoch 0  | loss: 1.06106 | val_0_rmse: 0.84206 | val_1_rmse: 0.86087 |  0:00:00s
epoch 1  | loss: 0.57532 | val_0_rmse: 0.72433 | val_1_rmse: 0.74647 |  0:00:00s
epoch 2  | loss: 0.44492 | val_0_rmse: 0.66597 | val_1_rmse: 0.6849  |  0:00:01s
epoch 3  | loss: 0.40565 | val_0_rmse: 0.63352 | val_1_rmse: 0.64902 |  0:00:02s
epoch 4  | loss: 0.36923 | val_0_rmse: 0.60577 | val_1_rmse: 0.6243  |  0:00:02s
epoch 5  | loss: 0.35883 | val_0_rmse: 0.60128 | val_1_rmse: 0.62927 |  0:00:03s
epoch 6  | loss: 0.3482  | val_0_rmse: 0.57072 | val_1_rmse: 0.6032  |  0:00:03s
epoch 7  | loss: 0.33207 | val_0_rmse: 0.54629 | val_1_rmse: 0.57941 |  0:00:04s
epoch 8  | loss: 0.31959 | val_0_rmse: 0.54719 | val_1_rmse: 0.57549 |  0:00:04s
epoch 9  | loss: 0.32133 | val_0_rmse: 0.54277 | val_1_rmse: 0.572   |  0:00:05s
epoch 10 | loss: 0.31386 | val_0_rmse: 0.55061 | val_1_rmse: 0.58232 |  0:00:05s
epoch 11 | loss: 0.31109 | val_0_rmse: 0.54473 | val_1_rmse: 0.57789 |  0:00:06s
epoch 12 | loss: 0.31206 | val_0_rmse: 0.53984 | val_1_rmse: 0.57635 |  0:00:06s
epoch 13 | loss: 0.31043 | val_0_rmse: 0.5302  | val_1_rmse: 0.56696 |  0:00:07s
epoch 14 | loss: 0.31004 | val_0_rmse: 0.57534 | val_1_rmse: 0.61437 |  0:00:07s
epoch 15 | loss: 0.31478 | val_0_rmse: 0.58547 | val_1_rmse: 0.62837 |  0:00:08s
epoch 16 | loss: 0.31165 | val_0_rmse: 0.53696 | val_1_rmse: 0.57661 |  0:00:08s
epoch 17 | loss: 0.30498 | val_0_rmse: 0.52362 | val_1_rmse: 0.55502 |  0:00:09s
epoch 18 | loss: 0.29232 | val_0_rmse: 0.52325 | val_1_rmse: 0.55845 |  0:00:09s
epoch 19 | loss: 0.29174 | val_0_rmse: 0.52518 | val_1_rmse: 0.55442 |  0:00:10s
epoch 20 | loss: 0.29275 | val_0_rmse: 0.52431 | val_1_rmse: 0.55713 |  0:00:10s
epoch 21 | loss: 0.28792 | val_0_rmse: 0.52751 | val_1_rmse: 0.55522 |  0:00:11s
epoch 22 | loss: 0.28411 | val_0_rmse: 0.51214 | val_1_rmse: 0.54679 |  0:00:11s
epoch 23 | loss: 0.28143 | val_0_rmse: 0.51005 | val_1_rmse: 0.54389 |  0:00:12s
epoch 24 | loss: 0.27889 | val_0_rmse: 0.5099  | val_1_rmse: 0.54587 |  0:00:12s
epoch 25 | loss: 0.27779 | val_0_rmse: 0.51017 | val_1_rmse: 0.54844 |  0:00:13s
epoch 26 | loss: 0.27547 | val_0_rmse: 0.50281 | val_1_rmse: 0.5339  |  0:00:13s
epoch 27 | loss: 0.27539 | val_0_rmse: 0.51694 | val_1_rmse: 0.55026 |  0:00:14s
epoch 28 | loss: 0.27687 | val_0_rmse: 0.49394 | val_1_rmse: 0.53382 |  0:00:14s
epoch 29 | loss: 0.268   | val_0_rmse: 0.50303 | val_1_rmse: 0.5428  |  0:00:15s
epoch 30 | loss: 0.27516 | val_0_rmse: 0.49144 | val_1_rmse: 0.53316 |  0:00:15s
epoch 31 | loss: 0.26708 | val_0_rmse: 0.5096  | val_1_rmse: 0.5487  |  0:00:16s
epoch 32 | loss: 0.27194 | val_0_rmse: 0.50718 | val_1_rmse: 0.54322 |  0:00:16s
epoch 33 | loss: 0.27206 | val_0_rmse: 0.49639 | val_1_rmse: 0.53546 |  0:00:17s
epoch 34 | loss: 0.26592 | val_0_rmse: 0.49418 | val_1_rmse: 0.53462 |  0:00:17s
epoch 35 | loss: 0.26599 | val_0_rmse: 0.49889 | val_1_rmse: 0.54228 |  0:00:18s
epoch 36 | loss: 0.26918 | val_0_rmse: 0.49137 | val_1_rmse: 0.52991 |  0:00:18s
epoch 37 | loss: 0.26735 | val_0_rmse: 0.50763 | val_1_rmse: 0.54593 |  0:00:19s
epoch 38 | loss: 0.27554 | val_0_rmse: 0.51643 | val_1_rmse: 0.55107 |  0:00:19s
epoch 39 | loss: 0.26599 | val_0_rmse: 0.49497 | val_1_rmse: 0.53415 |  0:00:20s
epoch 40 | loss: 0.26505 | val_0_rmse: 0.49985 | val_1_rmse: 0.54131 |  0:00:20s
epoch 41 | loss: 0.26585 | val_0_rmse: 0.49266 | val_1_rmse: 0.53504 |  0:00:21s
epoch 42 | loss: 0.2659  | val_0_rmse: 0.49286 | val_1_rmse: 0.53203 |  0:00:21s
epoch 43 | loss: 0.26522 | val_0_rmse: 0.49383 | val_1_rmse: 0.5321  |  0:00:22s
epoch 44 | loss: 0.2643  | val_0_rmse: 0.48893 | val_1_rmse: 0.52783 |  0:00:22s
epoch 45 | loss: 0.25845 | val_0_rmse: 0.48619 | val_1_rmse: 0.527   |  0:00:23s
epoch 46 | loss: 0.25805 | val_0_rmse: 0.48924 | val_1_rmse: 0.52942 |  0:00:23s
epoch 47 | loss: 0.25507 | val_0_rmse: 0.48436 | val_1_rmse: 0.525   |  0:00:24s
epoch 48 | loss: 0.25859 | val_0_rmse: 0.50036 | val_1_rmse: 0.53907 |  0:00:24s
epoch 49 | loss: 0.26399 | val_0_rmse: 0.5148  | val_1_rmse: 0.55244 |  0:00:25s
epoch 50 | loss: 0.26408 | val_0_rmse: 0.48881 | val_1_rmse: 0.52638 |  0:00:25s
epoch 51 | loss: 0.25685 | val_0_rmse: 0.48277 | val_1_rmse: 0.52296 |  0:00:26s
epoch 52 | loss: 0.26395 | val_0_rmse: 0.49605 | val_1_rmse: 0.53701 |  0:00:26s
epoch 53 | loss: 0.25378 | val_0_rmse: 0.48122 | val_1_rmse: 0.51901 |  0:00:27s
epoch 54 | loss: 0.25856 | val_0_rmse: 0.48944 | val_1_rmse: 0.53298 |  0:00:27s
epoch 55 | loss: 0.25559 | val_0_rmse: 0.47954 | val_1_rmse: 0.52672 |  0:00:28s
epoch 56 | loss: 0.25727 | val_0_rmse: 0.48653 | val_1_rmse: 0.52824 |  0:00:28s
epoch 57 | loss: 0.25474 | val_0_rmse: 0.48063 | val_1_rmse: 0.5207  |  0:00:29s
epoch 58 | loss: 0.26962 | val_0_rmse: 0.48587 | val_1_rmse: 0.52424 |  0:00:29s
epoch 59 | loss: 0.25877 | val_0_rmse: 0.48599 | val_1_rmse: 0.52886 |  0:00:30s
epoch 60 | loss: 0.25375 | val_0_rmse: 0.49333 | val_1_rmse: 0.5302  |  0:00:30s
epoch 61 | loss: 0.26369 | val_0_rmse: 0.52473 | val_1_rmse: 0.56717 |  0:00:31s
epoch 62 | loss: 0.26542 | val_0_rmse: 0.4928  | val_1_rmse: 0.53663 |  0:00:31s
epoch 63 | loss: 0.26064 | val_0_rmse: 0.48529 | val_1_rmse: 0.52788 |  0:00:32s
epoch 64 | loss: 0.24738 | val_0_rmse: 0.48028 | val_1_rmse: 0.52568 |  0:00:32s
epoch 65 | loss: 0.25249 | val_0_rmse: 0.48716 | val_1_rmse: 0.53449 |  0:00:33s
epoch 66 | loss: 0.25764 | val_0_rmse: 0.47934 | val_1_rmse: 0.52536 |  0:00:33s
epoch 67 | loss: 0.25109 | val_0_rmse: 0.47603 | val_1_rmse: 0.52335 |  0:00:34s
epoch 68 | loss: 0.25301 | val_0_rmse: 0.49572 | val_1_rmse: 0.5409  |  0:00:34s
epoch 69 | loss: 0.25661 | val_0_rmse: 0.48499 | val_1_rmse: 0.52857 |  0:00:35s
epoch 70 | loss: 0.25301 | val_0_rmse: 0.48128 | val_1_rmse: 0.52751 |  0:00:35s
epoch 71 | loss: 0.25482 | val_0_rmse: 0.49276 | val_1_rmse: 0.53677 |  0:00:36s
epoch 72 | loss: 0.25555 | val_0_rmse: 0.48089 | val_1_rmse: 0.52416 |  0:00:36s
epoch 73 | loss: 0.24719 | val_0_rmse: 0.47433 | val_1_rmse: 0.52317 |  0:00:37s
epoch 74 | loss: 0.24969 | val_0_rmse: 0.47648 | val_1_rmse: 0.5263  |  0:00:37s
epoch 75 | loss: 0.25007 | val_0_rmse: 0.47213 | val_1_rmse: 0.5197  |  0:00:38s
epoch 76 | loss: 0.25012 | val_0_rmse: 0.47457 | val_1_rmse: 0.52158 |  0:00:38s
epoch 77 | loss: 0.2457  | val_0_rmse: 0.47592 | val_1_rmse: 0.52631 |  0:00:39s
epoch 78 | loss: 0.24517 | val_0_rmse: 0.47555 | val_1_rmse: 0.52532 |  0:00:39s
epoch 79 | loss: 0.24971 | val_0_rmse: 0.48192 | val_1_rmse: 0.52696 |  0:00:40s
epoch 80 | loss: 0.25406 | val_0_rmse: 0.48493 | val_1_rmse: 0.52908 |  0:00:40s
epoch 81 | loss: 0.25767 | val_0_rmse: 0.48145 | val_1_rmse: 0.52416 |  0:00:41s
epoch 82 | loss: 0.24668 | val_0_rmse: 0.48879 | val_1_rmse: 0.5339  |  0:00:41s
epoch 83 | loss: 0.25489 | val_0_rmse: 0.49698 | val_1_rmse: 0.5428  |  0:00:42s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.51901
Best weights from best epoch are automatically used!
ended training at: 03:13:58
Feature importance:
[('Area', 0.23470004502821393), ('Baths', 6.96794618176785e-07), ('Beds', 0.09903904578405238), ('Latitude', 0.27246005612314), ('Longitude', 0.3869592754220656), ('Month', 0.0), ('Year', 0.006840880847909914)]
Mean squared error is of 21091993735.067436
Mean absolute error:107198.28265782728
MAPE:0.1839337370125912
R2 score:0.7427287825214761
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:13:58
epoch 0  | loss: 1.04581 | val_0_rmse: 1.26262 | val_1_rmse: 1.26343 |  0:00:00s
epoch 1  | loss: 0.5696  | val_0_rmse: 0.77811 | val_1_rmse: 0.75431 |  0:00:00s
epoch 2  | loss: 0.44449 | val_0_rmse: 0.74433 | val_1_rmse: 0.73144 |  0:00:01s
epoch 3  | loss: 0.3852  | val_0_rmse: 0.71631 | val_1_rmse: 0.7175  |  0:00:01s
epoch 4  | loss: 0.36662 | val_0_rmse: 0.67607 | val_1_rmse: 0.68088 |  0:00:02s
epoch 5  | loss: 0.34634 | val_0_rmse: 0.60651 | val_1_rmse: 0.60453 |  0:00:03s
epoch 6  | loss: 0.33013 | val_0_rmse: 0.58173 | val_1_rmse: 0.58349 |  0:00:03s
epoch 7  | loss: 0.31733 | val_0_rmse: 0.55218 | val_1_rmse: 0.55105 |  0:00:04s
epoch 8  | loss: 0.30439 | val_0_rmse: 0.54083 | val_1_rmse: 0.54903 |  0:00:04s
epoch 9  | loss: 0.30449 | val_0_rmse: 0.53641 | val_1_rmse: 0.55271 |  0:00:05s
epoch 10 | loss: 0.30438 | val_0_rmse: 0.54562 | val_1_rmse: 0.55367 |  0:00:05s
epoch 11 | loss: 0.30247 | val_0_rmse: 0.54274 | val_1_rmse: 0.55267 |  0:00:06s
epoch 12 | loss: 0.30596 | val_0_rmse: 0.53121 | val_1_rmse: 0.53771 |  0:00:06s
epoch 13 | loss: 0.30683 | val_0_rmse: 0.52102 | val_1_rmse: 0.53252 |  0:00:07s
epoch 14 | loss: 0.28999 | val_0_rmse: 0.51038 | val_1_rmse: 0.52471 |  0:00:07s
epoch 15 | loss: 0.27889 | val_0_rmse: 0.51921 | val_1_rmse: 0.53399 |  0:00:08s
epoch 16 | loss: 0.27474 | val_0_rmse: 0.50462 | val_1_rmse: 0.5187  |  0:00:08s
epoch 17 | loss: 0.28769 | val_0_rmse: 0.50594 | val_1_rmse: 0.51764 |  0:00:09s
epoch 18 | loss: 0.28703 | val_0_rmse: 0.52173 | val_1_rmse: 0.53356 |  0:00:09s
epoch 19 | loss: 0.28516 | val_0_rmse: 0.50581 | val_1_rmse: 0.52199 |  0:00:10s
epoch 20 | loss: 0.28176 | val_0_rmse: 0.50644 | val_1_rmse: 0.52119 |  0:00:10s
epoch 21 | loss: 0.28338 | val_0_rmse: 0.51174 | val_1_rmse: 0.52278 |  0:00:11s
epoch 22 | loss: 0.27799 | val_0_rmse: 0.50603 | val_1_rmse: 0.51241 |  0:00:11s
epoch 23 | loss: 0.27123 | val_0_rmse: 0.50218 | val_1_rmse: 0.51793 |  0:00:12s
epoch 24 | loss: 0.2784  | val_0_rmse: 0.51082 | val_1_rmse: 0.52519 |  0:00:12s
epoch 25 | loss: 0.27311 | val_0_rmse: 0.49897 | val_1_rmse: 0.517   |  0:00:13s
epoch 26 | loss: 0.27791 | val_0_rmse: 0.49686 | val_1_rmse: 0.51189 |  0:00:13s
epoch 27 | loss: 0.27199 | val_0_rmse: 0.49823 | val_1_rmse: 0.51916 |  0:00:14s
epoch 28 | loss: 0.26802 | val_0_rmse: 0.50717 | val_1_rmse: 0.52186 |  0:00:14s
epoch 29 | loss: 0.27823 | val_0_rmse: 0.50681 | val_1_rmse: 0.521   |  0:00:15s
epoch 30 | loss: 0.27519 | val_0_rmse: 0.4939  | val_1_rmse: 0.51693 |  0:00:15s
epoch 31 | loss: 0.26409 | val_0_rmse: 0.49069 | val_1_rmse: 0.51325 |  0:00:16s
epoch 32 | loss: 0.26992 | val_0_rmse: 0.49362 | val_1_rmse: 0.51215 |  0:00:16s
epoch 33 | loss: 0.26388 | val_0_rmse: 0.49841 | val_1_rmse: 0.52068 |  0:00:17s
epoch 34 | loss: 0.26382 | val_0_rmse: 0.4917  | val_1_rmse: 0.5216  |  0:00:17s
epoch 35 | loss: 0.25784 | val_0_rmse: 0.49065 | val_1_rmse: 0.51938 |  0:00:18s
epoch 36 | loss: 0.25776 | val_0_rmse: 0.48838 | val_1_rmse: 0.51468 |  0:00:18s
epoch 37 | loss: 0.25601 | val_0_rmse: 0.51392 | val_1_rmse: 0.54785 |  0:00:19s
epoch 38 | loss: 0.26959 | val_0_rmse: 0.51189 | val_1_rmse: 0.53998 |  0:00:19s
epoch 39 | loss: 0.26104 | val_0_rmse: 0.49357 | val_1_rmse: 0.52292 |  0:00:20s
epoch 40 | loss: 0.26109 | val_0_rmse: 0.49109 | val_1_rmse: 0.51434 |  0:00:20s
epoch 41 | loss: 0.26264 | val_0_rmse: 0.48802 | val_1_rmse: 0.51983 |  0:00:21s
epoch 42 | loss: 0.25826 | val_0_rmse: 0.49452 | val_1_rmse: 0.51911 |  0:00:21s
epoch 43 | loss: 0.26876 | val_0_rmse: 0.50562 | val_1_rmse: 0.53491 |  0:00:22s
epoch 44 | loss: 0.25831 | val_0_rmse: 0.49742 | val_1_rmse: 0.52755 |  0:00:22s
epoch 45 | loss: 0.25987 | val_0_rmse: 0.51478 | val_1_rmse: 0.54339 |  0:00:23s
epoch 46 | loss: 0.2617  | val_0_rmse: 0.49338 | val_1_rmse: 0.52081 |  0:00:23s
epoch 47 | loss: 0.26279 | val_0_rmse: 0.48816 | val_1_rmse: 0.52381 |  0:00:24s
epoch 48 | loss: 0.26215 | val_0_rmse: 0.49655 | val_1_rmse: 0.5182  |  0:00:24s
epoch 49 | loss: 0.25588 | val_0_rmse: 0.4932  | val_1_rmse: 0.52205 |  0:00:25s
epoch 50 | loss: 0.25982 | val_0_rmse: 0.4972  | val_1_rmse: 0.52201 |  0:00:25s
epoch 51 | loss: 0.26499 | val_0_rmse: 0.48674 | val_1_rmse: 0.52495 |  0:00:26s
epoch 52 | loss: 0.25683 | val_0_rmse: 0.48533 | val_1_rmse: 0.51506 |  0:00:26s
epoch 53 | loss: 0.25469 | val_0_rmse: 0.49629 | val_1_rmse: 0.52742 |  0:00:27s
epoch 54 | loss: 0.25809 | val_0_rmse: 0.49036 | val_1_rmse: 0.51951 |  0:00:27s
epoch 55 | loss: 0.2517  | val_0_rmse: 0.47866 | val_1_rmse: 0.51643 |  0:00:28s
epoch 56 | loss: 0.25568 | val_0_rmse: 0.48871 | val_1_rmse: 0.52875 |  0:00:28s

Early stopping occured at epoch 56 with best_epoch = 26 and best_val_1_rmse = 0.51189
Best weights from best epoch are automatically used!
ended training at: 03:14:27
Feature importance:
[('Area', 0.2636015370961178), ('Baths', 0.045356145114197194), ('Beds', 0.1693383549784206), ('Latitude', 0.16021165878086657), ('Longitude', 0.31890556075454857), ('Month', 0.0053330569617905145), ('Year', 0.03725368631405876)]
Mean squared error is of 22144971400.751022
Mean absolute error:107870.05517310725
MAPE:0.1823329246642292
R2 score:0.724646698827244
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:14:27
epoch 0  | loss: 1.64662 | val_0_rmse: 1.10876 | val_1_rmse: 1.02441 |  0:00:00s
epoch 1  | loss: 0.85422 | val_0_rmse: 1.01425 | val_1_rmse: 0.9068  |  0:00:00s
epoch 2  | loss: 0.76497 | val_0_rmse: 0.81092 | val_1_rmse: 0.72567 |  0:00:00s
epoch 3  | loss: 0.57535 | val_0_rmse: 0.86592 | val_1_rmse: 0.71379 |  0:00:00s
epoch 4  | loss: 0.5466  | val_0_rmse: 0.82817 | val_1_rmse: 0.69526 |  0:00:00s
epoch 5  | loss: 0.54411 | val_0_rmse: 0.90608 | val_1_rmse: 0.7525  |  0:00:00s
epoch 6  | loss: 0.50073 | val_0_rmse: 0.83298 | val_1_rmse: 0.71946 |  0:00:01s
epoch 7  | loss: 0.51114 | val_0_rmse: 0.78628 | val_1_rmse: 0.65717 |  0:00:01s
epoch 8  | loss: 0.49608 | val_0_rmse: 0.77123 | val_1_rmse: 0.63136 |  0:00:01s
epoch 9  | loss: 0.51444 | val_0_rmse: 0.74044 | val_1_rmse: 0.60824 |  0:00:01s
epoch 10 | loss: 0.50929 | val_0_rmse: 0.74504 | val_1_rmse: 0.62214 |  0:00:01s
epoch 11 | loss: 0.49722 | val_0_rmse: 0.75869 | val_1_rmse: 0.65749 |  0:00:01s
epoch 12 | loss: 0.49672 | val_0_rmse: 0.76132 | val_1_rmse: 0.67162 |  0:00:01s
epoch 13 | loss: 0.50165 | val_0_rmse: 0.74523 | val_1_rmse: 0.65235 |  0:00:02s
epoch 14 | loss: 0.50283 | val_0_rmse: 0.72267 | val_1_rmse: 0.62804 |  0:00:02s
epoch 15 | loss: 0.47911 | val_0_rmse: 0.69869 | val_1_rmse: 0.60374 |  0:00:02s
epoch 16 | loss: 0.47904 | val_0_rmse: 0.68808 | val_1_rmse: 0.60192 |  0:00:02s
epoch 17 | loss: 0.47915 | val_0_rmse: 0.67957 | val_1_rmse: 0.59943 |  0:00:02s
epoch 18 | loss: 0.46499 | val_0_rmse: 0.67268 | val_1_rmse: 0.5883  |  0:00:02s
epoch 19 | loss: 0.46687 | val_0_rmse: 0.6745  | val_1_rmse: 0.58357 |  0:00:02s
epoch 20 | loss: 0.45492 | val_0_rmse: 0.67191 | val_1_rmse: 0.57106 |  0:00:03s
epoch 21 | loss: 0.45232 | val_0_rmse: 0.67824 | val_1_rmse: 0.5856  |  0:00:03s
epoch 22 | loss: 0.47967 | val_0_rmse: 0.67963 | val_1_rmse: 0.5956  |  0:00:03s
epoch 23 | loss: 0.46298 | val_0_rmse: 0.67932 | val_1_rmse: 0.62016 |  0:00:03s
epoch 24 | loss: 0.46041 | val_0_rmse: 0.68105 | val_1_rmse: 0.63876 |  0:00:03s
epoch 25 | loss: 0.46805 | val_0_rmse: 0.68055 | val_1_rmse: 0.62879 |  0:00:03s
epoch 26 | loss: 0.47056 | val_0_rmse: 0.66985 | val_1_rmse: 0.60728 |  0:00:03s
epoch 27 | loss: 0.45598 | val_0_rmse: 0.67081 | val_1_rmse: 0.61344 |  0:00:04s
epoch 28 | loss: 0.46476 | val_0_rmse: 0.67908 | val_1_rmse: 0.61238 |  0:00:04s
epoch 29 | loss: 0.47332 | val_0_rmse: 0.68723 | val_1_rmse: 0.6177  |  0:00:04s
epoch 30 | loss: 0.47346 | val_0_rmse: 0.67914 | val_1_rmse: 0.60988 |  0:00:04s
epoch 31 | loss: 0.46287 | val_0_rmse: 0.67748 | val_1_rmse: 0.60809 |  0:00:04s
epoch 32 | loss: 0.45527 | val_0_rmse: 0.67271 | val_1_rmse: 0.616   |  0:00:04s
epoch 33 | loss: 0.45636 | val_0_rmse: 0.67078 | val_1_rmse: 0.60817 |  0:00:04s
epoch 34 | loss: 0.47509 | val_0_rmse: 0.67253 | val_1_rmse: 0.62634 |  0:00:05s
epoch 35 | loss: 0.46085 | val_0_rmse: 0.67415 | val_1_rmse: 0.63323 |  0:00:05s
epoch 36 | loss: 0.4547  | val_0_rmse: 0.66016 | val_1_rmse: 0.61408 |  0:00:05s
epoch 37 | loss: 0.43794 | val_0_rmse: 0.65401 | val_1_rmse: 0.60822 |  0:00:05s
epoch 38 | loss: 0.43322 | val_0_rmse: 0.65582 | val_1_rmse: 0.61674 |  0:00:05s
epoch 39 | loss: 0.43993 | val_0_rmse: 0.65293 | val_1_rmse: 0.61864 |  0:00:05s
epoch 40 | loss: 0.4276  | val_0_rmse: 0.64592 | val_1_rmse: 0.60055 |  0:00:05s
epoch 41 | loss: 0.42862 | val_0_rmse: 0.64877 | val_1_rmse: 0.59622 |  0:00:06s
epoch 42 | loss: 0.4383  | val_0_rmse: 0.64523 | val_1_rmse: 0.59283 |  0:00:06s
epoch 43 | loss: 0.42501 | val_0_rmse: 0.64403 | val_1_rmse: 0.59171 |  0:00:06s
epoch 44 | loss: 0.42903 | val_0_rmse: 0.64358 | val_1_rmse: 0.60429 |  0:00:06s
epoch 45 | loss: 0.41531 | val_0_rmse: 0.64913 | val_1_rmse: 0.62473 |  0:00:06s
epoch 46 | loss: 0.42538 | val_0_rmse: 0.63983 | val_1_rmse: 0.61715 |  0:00:06s
epoch 47 | loss: 0.40881 | val_0_rmse: 0.63544 | val_1_rmse: 0.60808 |  0:00:06s
epoch 48 | loss: 0.42726 | val_0_rmse: 0.64159 | val_1_rmse: 0.61769 |  0:00:07s
epoch 49 | loss: 0.41694 | val_0_rmse: 0.6437  | val_1_rmse: 0.62812 |  0:00:07s
epoch 50 | loss: 0.41724 | val_0_rmse: 0.63261 | val_1_rmse: 0.60969 |  0:00:07s

Early stopping occured at epoch 50 with best_epoch = 20 and best_val_1_rmse = 0.57106
Best weights from best epoch are automatically used!
ended training at: 03:14:34
Feature importance:
[('Area', 0.38344005783081975), ('Baths', 0.17801223092646035), ('Beds', 0.03594441322503373), ('Latitude', 0.11686147768396599), ('Longitude', 0.15794463407882206), ('Month', 0.09316946827813133), ('Year', 0.03462771797676676)]
Mean squared error is of 3476460786.062093
Mean absolute error:42680.92249701665
MAPE:0.37720286656911545
R2 score:0.47947012132512934
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:14:35
epoch 0  | loss: 1.56192 | val_0_rmse: 0.94967 | val_1_rmse: 0.96844 |  0:00:00s
epoch 1  | loss: 0.75748 | val_0_rmse: 0.90761 | val_1_rmse: 0.91411 |  0:00:00s
epoch 2  | loss: 0.58717 | val_0_rmse: 1.0292  | val_1_rmse: 1.0479  |  0:00:00s
epoch 3  | loss: 0.53955 | val_0_rmse: 1.03601 | val_1_rmse: 1.02065 |  0:00:00s
epoch 4  | loss: 0.49331 | val_0_rmse: 1.00295 | val_1_rmse: 1.03352 |  0:00:00s
epoch 5  | loss: 0.50336 | val_0_rmse: 0.76665 | val_1_rmse: 0.77004 |  0:00:00s
epoch 6  | loss: 0.5025  | val_0_rmse: 0.75826 | val_1_rmse: 0.75131 |  0:00:01s
epoch 7  | loss: 0.50114 | val_0_rmse: 0.7681  | val_1_rmse: 0.7549  |  0:00:01s
epoch 8  | loss: 0.49264 | val_0_rmse: 0.75559 | val_1_rmse: 0.74522 |  0:00:01s
epoch 9  | loss: 0.48972 | val_0_rmse: 0.73342 | val_1_rmse: 0.71552 |  0:00:01s
epoch 10 | loss: 0.46418 | val_0_rmse: 0.7152  | val_1_rmse: 0.70556 |  0:00:01s
epoch 11 | loss: 0.46555 | val_0_rmse: 0.70022 | val_1_rmse: 0.69545 |  0:00:01s
epoch 12 | loss: 0.46472 | val_0_rmse: 0.68854 | val_1_rmse: 0.66772 |  0:00:02s
epoch 13 | loss: 0.45233 | val_0_rmse: 0.68141 | val_1_rmse: 0.65662 |  0:00:02s
epoch 14 | loss: 0.44554 | val_0_rmse: 0.67518 | val_1_rmse: 0.65465 |  0:00:02s
epoch 15 | loss: 0.44276 | val_0_rmse: 0.66372 | val_1_rmse: 0.64491 |  0:00:02s
epoch 16 | loss: 0.43386 | val_0_rmse: 0.66321 | val_1_rmse: 0.6474  |  0:00:02s
epoch 17 | loss: 0.4333  | val_0_rmse: 0.65405 | val_1_rmse: 0.64314 |  0:00:02s
epoch 18 | loss: 0.423   | val_0_rmse: 0.63915 | val_1_rmse: 0.63568 |  0:00:02s
epoch 19 | loss: 0.4117  | val_0_rmse: 0.63163 | val_1_rmse: 0.62561 |  0:00:03s
epoch 20 | loss: 0.40575 | val_0_rmse: 0.62568 | val_1_rmse: 0.62122 |  0:00:03s
epoch 21 | loss: 0.38582 | val_0_rmse: 0.61728 | val_1_rmse: 0.61207 |  0:00:03s
epoch 22 | loss: 0.39704 | val_0_rmse: 0.62489 | val_1_rmse: 0.62172 |  0:00:03s
epoch 23 | loss: 0.39403 | val_0_rmse: 0.62154 | val_1_rmse: 0.61948 |  0:00:03s
epoch 24 | loss: 0.39145 | val_0_rmse: 0.61406 | val_1_rmse: 0.60253 |  0:00:03s
epoch 25 | loss: 0.39102 | val_0_rmse: 0.61764 | val_1_rmse: 0.61049 |  0:00:03s
epoch 26 | loss: 0.3625  | val_0_rmse: 0.61016 | val_1_rmse: 0.60205 |  0:00:04s
epoch 27 | loss: 0.37168 | val_0_rmse: 0.59709 | val_1_rmse: 0.58615 |  0:00:04s
epoch 28 | loss: 0.36022 | val_0_rmse: 0.59621 | val_1_rmse: 0.58056 |  0:00:04s
epoch 29 | loss: 0.35401 | val_0_rmse: 0.5968  | val_1_rmse: 0.57952 |  0:00:04s
epoch 30 | loss: 0.3553  | val_0_rmse: 0.59228 | val_1_rmse: 0.58132 |  0:00:04s
epoch 31 | loss: 0.35078 | val_0_rmse: 0.60938 | val_1_rmse: 0.5955  |  0:00:04s
epoch 32 | loss: 0.34495 | val_0_rmse: 0.58935 | val_1_rmse: 0.57588 |  0:00:04s
epoch 33 | loss: 0.3364  | val_0_rmse: 0.5864  | val_1_rmse: 0.58816 |  0:00:05s
epoch 34 | loss: 0.35001 | val_0_rmse: 0.56789 | val_1_rmse: 0.5615  |  0:00:05s
epoch 35 | loss: 0.3442  | val_0_rmse: 0.55967 | val_1_rmse: 0.53491 |  0:00:05s
epoch 36 | loss: 0.34021 | val_0_rmse: 0.56219 | val_1_rmse: 0.53338 |  0:00:05s
epoch 37 | loss: 0.33923 | val_0_rmse: 0.5666  | val_1_rmse: 0.54117 |  0:00:05s
epoch 38 | loss: 0.32183 | val_0_rmse: 0.57272 | val_1_rmse: 0.55738 |  0:00:05s
epoch 39 | loss: 0.32501 | val_0_rmse: 0.56789 | val_1_rmse: 0.56066 |  0:00:05s
epoch 40 | loss: 0.31826 | val_0_rmse: 0.56111 | val_1_rmse: 0.55262 |  0:00:06s
epoch 41 | loss: 0.31763 | val_0_rmse: 0.5609  | val_1_rmse: 0.5473  |  0:00:06s
epoch 42 | loss: 0.32024 | val_0_rmse: 0.55248 | val_1_rmse: 0.5407  |  0:00:06s
epoch 43 | loss: 0.33697 | val_0_rmse: 0.56328 | val_1_rmse: 0.546   |  0:00:06s
epoch 44 | loss: 0.31913 | val_0_rmse: 0.57113 | val_1_rmse: 0.54972 |  0:00:06s
epoch 45 | loss: 0.31515 | val_0_rmse: 0.54609 | val_1_rmse: 0.53136 |  0:00:06s
epoch 46 | loss: 0.325   | val_0_rmse: 0.55688 | val_1_rmse: 0.54647 |  0:00:06s
epoch 47 | loss: 0.32652 | val_0_rmse: 0.55788 | val_1_rmse: 0.54207 |  0:00:07s
epoch 48 | loss: 0.32937 | val_0_rmse: 0.54771 | val_1_rmse: 0.52574 |  0:00:07s
epoch 49 | loss: 0.32701 | val_0_rmse: 0.55005 | val_1_rmse: 0.52718 |  0:00:07s
epoch 50 | loss: 0.32062 | val_0_rmse: 0.61147 | val_1_rmse: 0.60188 |  0:00:07s
epoch 51 | loss: 0.32751 | val_0_rmse: 0.60808 | val_1_rmse: 0.59541 |  0:00:07s
epoch 52 | loss: 0.32661 | val_0_rmse: 0.55115 | val_1_rmse: 0.52128 |  0:00:07s
epoch 53 | loss: 0.32131 | val_0_rmse: 0.54277 | val_1_rmse: 0.51975 |  0:00:08s
epoch 54 | loss: 0.31838 | val_0_rmse: 0.55562 | val_1_rmse: 0.54504 |  0:00:08s
epoch 55 | loss: 0.3138  | val_0_rmse: 0.56347 | val_1_rmse: 0.56699 |  0:00:08s
epoch 56 | loss: 0.30828 | val_0_rmse: 0.56402 | val_1_rmse: 0.56882 |  0:00:08s
epoch 57 | loss: 0.30912 | val_0_rmse: 0.55702 | val_1_rmse: 0.55971 |  0:00:08s
epoch 58 | loss: 0.30691 | val_0_rmse: 0.54086 | val_1_rmse: 0.53449 |  0:00:08s
epoch 59 | loss: 0.29392 | val_0_rmse: 0.56211 | val_1_rmse: 0.55721 |  0:00:08s
epoch 60 | loss: 0.30546 | val_0_rmse: 0.69299 | val_1_rmse: 0.7038  |  0:00:09s
epoch 61 | loss: 0.30994 | val_0_rmse: 0.74791 | val_1_rmse: 0.7655  |  0:00:09s
epoch 62 | loss: 0.31079 | val_0_rmse: 0.60242 | val_1_rmse: 0.60962 |  0:00:09s
epoch 63 | loss: 0.30831 | val_0_rmse: 0.53102 | val_1_rmse: 0.52646 |  0:00:09s
epoch 64 | loss: 0.31969 | val_0_rmse: 0.53455 | val_1_rmse: 0.5345  |  0:00:09s
epoch 65 | loss: 0.2986  | val_0_rmse: 0.53488 | val_1_rmse: 0.5194  |  0:00:09s
epoch 66 | loss: 0.29926 | val_0_rmse: 0.53741 | val_1_rmse: 0.524   |  0:00:09s
epoch 67 | loss: 0.28171 | val_0_rmse: 0.52359 | val_1_rmse: 0.52021 |  0:00:10s
epoch 68 | loss: 0.30819 | val_0_rmse: 0.52218 | val_1_rmse: 0.52134 |  0:00:10s
epoch 69 | loss: 0.28759 | val_0_rmse: 0.52748 | val_1_rmse: 0.53018 |  0:00:10s
epoch 70 | loss: 0.29796 | val_0_rmse: 0.52811 | val_1_rmse: 0.53322 |  0:00:10s
epoch 71 | loss: 0.2973  | val_0_rmse: 0.56673 | val_1_rmse: 0.57203 |  0:00:10s
epoch 72 | loss: 0.29445 | val_0_rmse: 0.55681 | val_1_rmse: 0.56337 |  0:00:10s
epoch 73 | loss: 0.2926  | val_0_rmse: 0.56939 | val_1_rmse: 0.57958 |  0:00:10s
epoch 74 | loss: 0.28496 | val_0_rmse: 0.54391 | val_1_rmse: 0.56162 |  0:00:11s
epoch 75 | loss: 0.28506 | val_0_rmse: 0.53887 | val_1_rmse: 0.55878 |  0:00:11s
epoch 76 | loss: 0.29179 | val_0_rmse: 0.55076 | val_1_rmse: 0.57032 |  0:00:11s
epoch 77 | loss: 0.28657 | val_0_rmse: 0.51988 | val_1_rmse: 0.53304 |  0:00:11s
epoch 78 | loss: 0.27326 | val_0_rmse: 0.50965 | val_1_rmse: 0.524   |  0:00:11s
epoch 79 | loss: 0.27476 | val_0_rmse: 0.50748 | val_1_rmse: 0.52961 |  0:00:11s
epoch 80 | loss: 0.28102 | val_0_rmse: 0.51265 | val_1_rmse: 0.54306 |  0:00:11s
epoch 81 | loss: 0.28556 | val_0_rmse: 0.51304 | val_1_rmse: 0.53852 |  0:00:12s
epoch 82 | loss: 0.27853 | val_0_rmse: 0.51818 | val_1_rmse: 0.53405 |  0:00:12s
epoch 83 | loss: 0.29053 | val_0_rmse: 0.5052  | val_1_rmse: 0.52409 |  0:00:12s
epoch 84 | loss: 0.28456 | val_0_rmse: 0.53133 | val_1_rmse: 0.56111 |  0:00:12s
epoch 85 | loss: 0.27285 | val_0_rmse: 0.55928 | val_1_rmse: 0.5881  |  0:00:12s
epoch 86 | loss: 0.27592 | val_0_rmse: 0.50325 | val_1_rmse: 0.54056 |  0:00:12s
epoch 87 | loss: 0.27613 | val_0_rmse: 0.49933 | val_1_rmse: 0.53523 |  0:00:12s
epoch 88 | loss: 0.27605 | val_0_rmse: 0.52679 | val_1_rmse: 0.54414 |  0:00:13s
epoch 89 | loss: 0.29647 | val_0_rmse: 0.51447 | val_1_rmse: 0.54859 |  0:00:13s
epoch 90 | loss: 0.27742 | val_0_rmse: 0.50729 | val_1_rmse: 0.54982 |  0:00:13s
epoch 91 | loss: 0.28502 | val_0_rmse: 0.53751 | val_1_rmse: 0.57003 |  0:00:13s
epoch 92 | loss: 0.30523 | val_0_rmse: 0.52839 | val_1_rmse: 0.55851 |  0:00:13s
epoch 93 | loss: 0.26566 | val_0_rmse: 0.52814 | val_1_rmse: 0.57034 |  0:00:13s
epoch 94 | loss: 0.29128 | val_0_rmse: 0.56595 | val_1_rmse: 0.60816 |  0:00:13s
epoch 95 | loss: 0.28791 | val_0_rmse: 0.54403 | val_1_rmse: 0.56908 |  0:00:14s

Early stopping occured at epoch 95 with best_epoch = 65 and best_val_1_rmse = 0.5194
Best weights from best epoch are automatically used!
ended training at: 03:14:49
Feature importance:
[('Area', 0.30467589934020894), ('Baths', 0.09887339250593484), ('Beds', 0.13554682547993718), ('Latitude', 0.2953568816702949), ('Longitude', 0.06774690426078678), ('Month', 0.0543740493224623), ('Year', 0.04342604742037508)]
Mean squared error is of 4117241187.3970156
Mean absolute error:41268.72382115384
MAPE:0.3638354516952309
R2 score:0.469472765921245
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:14:49
epoch 0  | loss: 1.52578 | val_0_rmse: 1.378   | val_1_rmse: 1.17899 |  0:00:00s
epoch 1  | loss: 0.76829 | val_0_rmse: 1.34071 | val_1_rmse: 1.16274 |  0:00:00s
epoch 2  | loss: 0.56653 | val_0_rmse: 1.11298 | val_1_rmse: 1.07195 |  0:00:00s
epoch 3  | loss: 0.5425  | val_0_rmse: 0.85912 | val_1_rmse: 0.87393 |  0:00:00s
epoch 4  | loss: 0.56246 | val_0_rmse: 0.82645 | val_1_rmse: 0.80508 |  0:00:00s
epoch 5  | loss: 0.5269  | val_0_rmse: 0.89237 | val_1_rmse: 0.79143 |  0:00:00s
epoch 6  | loss: 0.52904 | val_0_rmse: 0.77818 | val_1_rmse: 0.75577 |  0:00:01s
epoch 7  | loss: 0.49822 | val_0_rmse: 0.76453 | val_1_rmse: 0.73603 |  0:00:01s
epoch 8  | loss: 0.50066 | val_0_rmse: 0.74864 | val_1_rmse: 0.72753 |  0:00:01s
epoch 9  | loss: 0.49575 | val_0_rmse: 0.70117 | val_1_rmse: 0.68362 |  0:00:01s
epoch 10 | loss: 0.48111 | val_0_rmse: 0.71912 | val_1_rmse: 0.70091 |  0:00:01s
epoch 11 | loss: 0.46394 | val_0_rmse: 0.7413  | val_1_rmse: 0.72463 |  0:00:01s
epoch 12 | loss: 0.45088 | val_0_rmse: 0.70936 | val_1_rmse: 0.6916  |  0:00:01s
epoch 13 | loss: 0.46254 | val_0_rmse: 0.68879 | val_1_rmse: 0.67679 |  0:00:02s
epoch 14 | loss: 0.44973 | val_0_rmse: 0.68543 | val_1_rmse: 0.68057 |  0:00:02s
epoch 15 | loss: 0.46203 | val_0_rmse: 0.69257 | val_1_rmse: 0.67548 |  0:00:02s
epoch 16 | loss: 0.45054 | val_0_rmse: 0.68891 | val_1_rmse: 0.68343 |  0:00:02s
epoch 17 | loss: 0.44473 | val_0_rmse: 0.66942 | val_1_rmse: 0.67092 |  0:00:02s
epoch 18 | loss: 0.45266 | val_0_rmse: 0.67351 | val_1_rmse: 0.66798 |  0:00:02s
epoch 19 | loss: 0.45246 | val_0_rmse: 0.6693  | val_1_rmse: 0.67794 |  0:00:03s
epoch 20 | loss: 0.42937 | val_0_rmse: 0.6632  | val_1_rmse: 0.67634 |  0:00:03s
epoch 21 | loss: 0.44345 | val_0_rmse: 0.65646 | val_1_rmse: 0.67737 |  0:00:03s
epoch 22 | loss: 0.43199 | val_0_rmse: 0.65414 | val_1_rmse: 0.67735 |  0:00:03s
epoch 23 | loss: 0.43304 | val_0_rmse: 0.65057 | val_1_rmse: 0.6731  |  0:00:03s
epoch 24 | loss: 0.43867 | val_0_rmse: 0.6614  | val_1_rmse: 0.67719 |  0:00:03s
epoch 25 | loss: 0.45477 | val_0_rmse: 0.66148 | val_1_rmse: 0.68197 |  0:00:03s
epoch 26 | loss: 0.45011 | val_0_rmse: 0.6535  | val_1_rmse: 0.68085 |  0:00:04s
epoch 27 | loss: 0.43093 | val_0_rmse: 0.65067 | val_1_rmse: 0.67851 |  0:00:04s
epoch 28 | loss: 0.43276 | val_0_rmse: 0.64472 | val_1_rmse: 0.66965 |  0:00:04s
epoch 29 | loss: 0.42925 | val_0_rmse: 0.64786 | val_1_rmse: 0.6667  |  0:00:04s
epoch 30 | loss: 0.43204 | val_0_rmse: 0.65006 | val_1_rmse: 0.66526 |  0:00:04s
epoch 31 | loss: 0.42081 | val_0_rmse: 0.64533 | val_1_rmse: 0.66644 |  0:00:04s
epoch 32 | loss: 0.42423 | val_0_rmse: 0.64551 | val_1_rmse: 0.66605 |  0:00:04s
epoch 33 | loss: 0.41813 | val_0_rmse: 0.64538 | val_1_rmse: 0.66056 |  0:00:05s
epoch 34 | loss: 0.42102 | val_0_rmse: 0.64137 | val_1_rmse: 0.66176 |  0:00:05s
epoch 35 | loss: 0.42213 | val_0_rmse: 0.64548 | val_1_rmse: 0.66925 |  0:00:05s
epoch 36 | loss: 0.42751 | val_0_rmse: 0.63705 | val_1_rmse: 0.66648 |  0:00:05s
epoch 37 | loss: 0.4207  | val_0_rmse: 0.63196 | val_1_rmse: 0.67341 |  0:00:05s
epoch 38 | loss: 0.42391 | val_0_rmse: 0.63249 | val_1_rmse: 0.67703 |  0:00:05s
epoch 39 | loss: 0.42223 | val_0_rmse: 0.63309 | val_1_rmse: 0.67806 |  0:00:05s
epoch 40 | loss: 0.41661 | val_0_rmse: 0.63174 | val_1_rmse: 0.68326 |  0:00:06s
epoch 41 | loss: 0.40645 | val_0_rmse: 0.63267 | val_1_rmse: 0.67299 |  0:00:06s
epoch 42 | loss: 0.40839 | val_0_rmse: 0.63182 | val_1_rmse: 0.67544 |  0:00:06s
epoch 43 | loss: 0.40814 | val_0_rmse: 0.62091 | val_1_rmse: 0.68158 |  0:00:06s
epoch 44 | loss: 0.40597 | val_0_rmse: 0.62513 | val_1_rmse: 0.69281 |  0:00:06s
epoch 45 | loss: 0.41532 | val_0_rmse: 0.62267 | val_1_rmse: 0.69773 |  0:00:06s
epoch 46 | loss: 0.4076  | val_0_rmse: 0.62044 | val_1_rmse: 0.70606 |  0:00:06s
epoch 47 | loss: 0.39153 | val_0_rmse: 0.62205 | val_1_rmse: 0.69536 |  0:00:07s
epoch 48 | loss: 0.40799 | val_0_rmse: 0.62221 | val_1_rmse: 0.6863  |  0:00:07s
epoch 49 | loss: 0.40314 | val_0_rmse: 0.62174 | val_1_rmse: 0.67852 |  0:00:07s
epoch 50 | loss: 0.39854 | val_0_rmse: 0.62695 | val_1_rmse: 0.68499 |  0:00:07s
epoch 51 | loss: 0.40717 | val_0_rmse: 0.63252 | val_1_rmse: 0.7005  |  0:00:07s
epoch 52 | loss: 0.4071  | val_0_rmse: 0.63736 | val_1_rmse: 0.70113 |  0:00:07s
epoch 53 | loss: 0.40257 | val_0_rmse: 0.63308 | val_1_rmse: 0.69175 |  0:00:07s
epoch 54 | loss: 0.39321 | val_0_rmse: 0.6369  | val_1_rmse: 0.68294 |  0:00:08s
epoch 55 | loss: 0.39998 | val_0_rmse: 0.63488 | val_1_rmse: 0.67077 |  0:00:08s
epoch 56 | loss: 0.40702 | val_0_rmse: 0.63427 | val_1_rmse: 0.68013 |  0:00:08s
epoch 57 | loss: 0.40729 | val_0_rmse: 0.62844 | val_1_rmse: 0.69029 |  0:00:08s
epoch 58 | loss: 0.42282 | val_0_rmse: 0.62937 | val_1_rmse: 0.69603 |  0:00:08s
epoch 59 | loss: 0.42071 | val_0_rmse: 0.62815 | val_1_rmse: 0.69974 |  0:00:08s
epoch 60 | loss: 0.40819 | val_0_rmse: 0.63599 | val_1_rmse: 0.6966  |  0:00:08s
epoch 61 | loss: 0.40932 | val_0_rmse: 0.64916 | val_1_rmse: 0.69725 |  0:00:09s
epoch 62 | loss: 0.41479 | val_0_rmse: 0.66247 | val_1_rmse: 0.69616 |  0:00:09s
epoch 63 | loss: 0.42891 | val_0_rmse: 0.68803 | val_1_rmse: 0.7154  |  0:00:09s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.66056
Best weights from best epoch are automatically used!
ended training at: 03:14:59
Feature importance:
[('Area', 0.2947168522435132), ('Baths', 0.12907940330160317), ('Beds', 0.11551199654310555), ('Latitude', 0.2562037003323106), ('Longitude', 0.09881325972821577), ('Month', 0.07045421680166379), ('Year', 0.035220571049587916)]
Mean squared error is of 3143766708.7950788
Mean absolute error:40470.5068282967
MAPE:0.3546169666956209
R2 score:0.5829564382820904
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:14:59
epoch 0  | loss: 1.68725 | val_0_rmse: 1.11337 | val_1_rmse: 1.15772 |  0:00:00s
epoch 1  | loss: 1.05607 | val_0_rmse: 0.92605 | val_1_rmse: 0.9695  |  0:00:00s
epoch 2  | loss: 0.71226 | val_0_rmse: 0.92885 | val_1_rmse: 1.01544 |  0:00:00s
epoch 3  | loss: 0.59031 | val_0_rmse: 0.93087 | val_1_rmse: 1.05302 |  0:00:00s
epoch 4  | loss: 0.54797 | val_0_rmse: 0.8695  | val_1_rmse: 0.94911 |  0:00:00s
epoch 5  | loss: 0.51067 | val_0_rmse: 0.78595 | val_1_rmse: 0.79321 |  0:00:00s
epoch 6  | loss: 0.49515 | val_0_rmse: 0.79105 | val_1_rmse: 0.81128 |  0:00:01s
epoch 7  | loss: 0.47361 | val_0_rmse: 0.75219 | val_1_rmse: 0.76807 |  0:00:01s
epoch 8  | loss: 0.46473 | val_0_rmse: 0.73131 | val_1_rmse: 0.83128 |  0:00:01s
epoch 9  | loss: 0.46707 | val_0_rmse: 0.72897 | val_1_rmse: 0.83735 |  0:00:01s
epoch 10 | loss: 0.45722 | val_0_rmse: 0.72117 | val_1_rmse: 0.8307  |  0:00:01s
epoch 11 | loss: 0.45435 | val_0_rmse: 0.70327 | val_1_rmse: 0.79938 |  0:00:01s
epoch 12 | loss: 0.44765 | val_0_rmse: 0.68118 | val_1_rmse: 0.77975 |  0:00:01s
epoch 13 | loss: 0.4404  | val_0_rmse: 0.67862 | val_1_rmse: 0.77323 |  0:00:02s
epoch 14 | loss: 0.44147 | val_0_rmse: 0.66265 | val_1_rmse: 0.74578 |  0:00:02s
epoch 15 | loss: 0.44693 | val_0_rmse: 0.65416 | val_1_rmse: 0.73703 |  0:00:02s
epoch 16 | loss: 0.4249  | val_0_rmse: 0.65401 | val_1_rmse: 0.73714 |  0:00:02s
epoch 17 | loss: 0.43366 | val_0_rmse: 0.66041 | val_1_rmse: 0.73897 |  0:00:02s
epoch 18 | loss: 0.42475 | val_0_rmse: 0.65781 | val_1_rmse: 0.73311 |  0:00:02s
epoch 19 | loss: 0.41716 | val_0_rmse: 0.6467  | val_1_rmse: 0.73639 |  0:00:02s
epoch 20 | loss: 0.42393 | val_0_rmse: 0.64428 | val_1_rmse: 0.74083 |  0:00:03s
epoch 21 | loss: 0.4154  | val_0_rmse: 0.64827 | val_1_rmse: 0.74022 |  0:00:03s
epoch 22 | loss: 0.41647 | val_0_rmse: 0.64478 | val_1_rmse: 0.73432 |  0:00:03s
epoch 23 | loss: 0.42605 | val_0_rmse: 0.63754 | val_1_rmse: 0.72522 |  0:00:03s
epoch 24 | loss: 0.41655 | val_0_rmse: 0.64382 | val_1_rmse: 0.73375 |  0:00:03s
epoch 25 | loss: 0.42094 | val_0_rmse: 0.63414 | val_1_rmse: 0.71721 |  0:00:03s
epoch 26 | loss: 0.40775 | val_0_rmse: 0.63716 | val_1_rmse: 0.70796 |  0:00:03s
epoch 27 | loss: 0.41471 | val_0_rmse: 0.62942 | val_1_rmse: 0.71147 |  0:00:04s
epoch 28 | loss: 0.41737 | val_0_rmse: 0.63225 | val_1_rmse: 0.72457 |  0:00:04s
epoch 29 | loss: 0.41371 | val_0_rmse: 0.63026 | val_1_rmse: 0.72181 |  0:00:04s
epoch 30 | loss: 0.41651 | val_0_rmse: 0.62968 | val_1_rmse: 0.72438 |  0:00:04s
epoch 31 | loss: 0.41699 | val_0_rmse: 0.65353 | val_1_rmse: 0.73721 |  0:00:04s
epoch 32 | loss: 0.41651 | val_0_rmse: 0.65751 | val_1_rmse: 0.73952 |  0:00:04s
epoch 33 | loss: 0.41726 | val_0_rmse: 0.63747 | val_1_rmse: 0.70973 |  0:00:05s
epoch 34 | loss: 0.40868 | val_0_rmse: 0.6359  | val_1_rmse: 0.70575 |  0:00:05s
epoch 35 | loss: 0.41081 | val_0_rmse: 0.63072 | val_1_rmse: 0.69493 |  0:00:05s
epoch 36 | loss: 0.4142  | val_0_rmse: 0.63121 | val_1_rmse: 0.69372 |  0:00:05s
epoch 37 | loss: 0.41385 | val_0_rmse: 0.63144 | val_1_rmse: 0.69429 |  0:00:05s
epoch 38 | loss: 0.39873 | val_0_rmse: 0.62037 | val_1_rmse: 0.6834  |  0:00:05s
epoch 39 | loss: 0.40083 | val_0_rmse: 0.61937 | val_1_rmse: 0.68337 |  0:00:05s
epoch 40 | loss: 0.39673 | val_0_rmse: 0.67492 | val_1_rmse: 0.73457 |  0:00:06s
epoch 41 | loss: 0.40686 | val_0_rmse: 0.69613 | val_1_rmse: 0.75716 |  0:00:06s
epoch 42 | loss: 0.38918 | val_0_rmse: 0.61343 | val_1_rmse: 0.69238 |  0:00:06s
epoch 43 | loss: 0.38148 | val_0_rmse: 0.62823 | val_1_rmse: 0.70964 |  0:00:06s
epoch 44 | loss: 0.39188 | val_0_rmse: 0.63532 | val_1_rmse: 0.70996 |  0:00:06s
epoch 45 | loss: 0.37483 | val_0_rmse: 0.602   | val_1_rmse: 0.66462 |  0:00:06s
epoch 46 | loss: 0.37346 | val_0_rmse: 0.60596 | val_1_rmse: 0.67379 |  0:00:06s
epoch 47 | loss: 0.37237 | val_0_rmse: 0.6075  | val_1_rmse: 0.68473 |  0:00:07s
epoch 48 | loss: 0.38695 | val_0_rmse: 0.68061 | val_1_rmse: 0.7375  |  0:00:07s
epoch 49 | loss: 0.37116 | val_0_rmse: 0.67895 | val_1_rmse: 0.7321  |  0:00:07s
epoch 50 | loss: 0.38076 | val_0_rmse: 0.61429 | val_1_rmse: 0.6859  |  0:00:07s
epoch 51 | loss: 0.38206 | val_0_rmse: 0.5941  | val_1_rmse: 0.66221 |  0:00:07s
epoch 52 | loss: 0.36409 | val_0_rmse: 0.61115 | val_1_rmse: 0.67677 |  0:00:07s
epoch 53 | loss: 0.36295 | val_0_rmse: 0.58868 | val_1_rmse: 0.67101 |  0:00:07s
epoch 54 | loss: 0.35529 | val_0_rmse: 0.59227 | val_1_rmse: 0.67345 |  0:00:08s
epoch 55 | loss: 0.37319 | val_0_rmse: 0.58487 | val_1_rmse: 0.66592 |  0:00:08s
epoch 56 | loss: 0.35034 | val_0_rmse: 0.58029 | val_1_rmse: 0.66363 |  0:00:08s
epoch 57 | loss: 0.34721 | val_0_rmse: 0.59062 | val_1_rmse: 0.66369 |  0:00:08s
epoch 58 | loss: 0.35446 | val_0_rmse: 0.57881 | val_1_rmse: 0.65925 |  0:00:08s
epoch 59 | loss: 0.34778 | val_0_rmse: 0.59218 | val_1_rmse: 0.67482 |  0:00:08s
epoch 60 | loss: 0.34804 | val_0_rmse: 0.59844 | val_1_rmse: 0.68683 |  0:00:08s
epoch 61 | loss: 0.35268 | val_0_rmse: 0.57709 | val_1_rmse: 0.6686  |  0:00:09s
epoch 62 | loss: 0.35858 | val_0_rmse: 0.62841 | val_1_rmse: 0.681   |  0:00:09s
epoch 63 | loss: 0.35541 | val_0_rmse: 0.6267  | val_1_rmse: 0.67684 |  0:00:09s
epoch 64 | loss: 0.35344 | val_0_rmse: 0.57408 | val_1_rmse: 0.64985 |  0:00:09s
epoch 65 | loss: 0.34549 | val_0_rmse: 0.57119 | val_1_rmse: 0.65098 |  0:00:09s
epoch 66 | loss: 0.34363 | val_0_rmse: 0.73081 | val_1_rmse: 0.77649 |  0:00:09s
epoch 67 | loss: 0.3502  | val_0_rmse: 0.76747 | val_1_rmse: 0.80015 |  0:00:09s
epoch 68 | loss: 0.3418  | val_0_rmse: 0.57633 | val_1_rmse: 0.64811 |  0:00:10s
epoch 69 | loss: 0.34675 | val_0_rmse: 0.59107 | val_1_rmse: 0.65268 |  0:00:10s
epoch 70 | loss: 0.34906 | val_0_rmse: 0.5706  | val_1_rmse: 0.65518 |  0:00:10s
epoch 71 | loss: 0.33222 | val_0_rmse: 0.5989  | val_1_rmse: 0.67925 |  0:00:10s
epoch 72 | loss: 0.34211 | val_0_rmse: 0.65177 | val_1_rmse: 0.72506 |  0:00:10s
epoch 73 | loss: 0.33869 | val_0_rmse: 0.56589 | val_1_rmse: 0.65884 |  0:00:10s
epoch 74 | loss: 0.33559 | val_0_rmse: 0.56447 | val_1_rmse: 0.63927 |  0:00:10s
epoch 75 | loss: 0.33787 | val_0_rmse: 0.73808 | val_1_rmse: 0.78166 |  0:00:11s
epoch 76 | loss: 0.32905 | val_0_rmse: 0.93893 | val_1_rmse: 0.9784  |  0:00:11s
epoch 77 | loss: 0.34625 | val_0_rmse: 0.76418 | val_1_rmse: 0.81938 |  0:00:11s
epoch 78 | loss: 0.34652 | val_0_rmse: 0.57479 | val_1_rmse: 0.64745 |  0:00:11s
epoch 79 | loss: 0.33189 | val_0_rmse: 0.56157 | val_1_rmse: 0.64785 |  0:00:11s
epoch 80 | loss: 0.35592 | val_0_rmse: 0.62234 | val_1_rmse: 0.70602 |  0:00:11s
epoch 81 | loss: 0.34384 | val_0_rmse: 0.72146 | val_1_rmse: 0.79101 |  0:00:11s
epoch 82 | loss: 0.33961 | val_0_rmse: 0.76373 | val_1_rmse: 0.82558 |  0:00:12s
epoch 83 | loss: 0.34435 | val_0_rmse: 0.62086 | val_1_rmse: 0.69349 |  0:00:12s
epoch 84 | loss: 0.32495 | val_0_rmse: 0.59261 | val_1_rmse: 0.66501 |  0:00:12s
epoch 85 | loss: 0.34299 | val_0_rmse: 0.57057 | val_1_rmse: 0.64694 |  0:00:12s
epoch 86 | loss: 0.34768 | val_0_rmse: 0.61039 | val_1_rmse: 0.68747 |  0:00:12s
epoch 87 | loss: 0.33199 | val_0_rmse: 0.59986 | val_1_rmse: 0.67327 |  0:00:12s
epoch 88 | loss: 0.33695 | val_0_rmse: 0.57736 | val_1_rmse: 0.63962 |  0:00:13s
epoch 89 | loss: 0.34139 | val_0_rmse: 0.57499 | val_1_rmse: 0.62676 |  0:00:13s
epoch 90 | loss: 0.34763 | val_0_rmse: 0.56219 | val_1_rmse: 0.62279 |  0:00:13s
epoch 91 | loss: 0.33237 | val_0_rmse: 0.58003 | val_1_rmse: 0.66169 |  0:00:13s
epoch 92 | loss: 0.3242  | val_0_rmse: 0.5651  | val_1_rmse: 0.65579 |  0:00:13s
epoch 93 | loss: 0.3446  | val_0_rmse: 0.56821 | val_1_rmse: 0.6498  |  0:00:13s
epoch 94 | loss: 0.33739 | val_0_rmse: 0.56071 | val_1_rmse: 0.63389 |  0:00:13s
epoch 95 | loss: 0.33131 | val_0_rmse: 0.57771 | val_1_rmse: 0.64689 |  0:00:14s
epoch 96 | loss: 0.33097 | val_0_rmse: 0.57978 | val_1_rmse: 0.66682 |  0:00:14s
epoch 97 | loss: 0.33384 | val_0_rmse: 0.55643 | val_1_rmse: 0.65398 |  0:00:14s
epoch 98 | loss: 0.32396 | val_0_rmse: 0.55034 | val_1_rmse: 0.64531 |  0:00:14s
epoch 99 | loss: 0.31315 | val_0_rmse: 0.56417 | val_1_rmse: 0.64365 |  0:00:14s
epoch 100| loss: 0.3113  | val_0_rmse: 0.55103 | val_1_rmse: 0.63346 |  0:00:14s
epoch 101| loss: 0.3155  | val_0_rmse: 0.5628  | val_1_rmse: 0.63231 |  0:00:14s
epoch 102| loss: 0.32399 | val_0_rmse: 0.64633 | val_1_rmse: 0.69538 |  0:00:15s
epoch 103| loss: 0.323   | val_0_rmse: 0.68377 | val_1_rmse: 0.72993 |  0:00:15s
epoch 104| loss: 0.33318 | val_0_rmse: 0.64458 | val_1_rmse: 0.67393 |  0:00:15s
epoch 105| loss: 0.34135 | val_0_rmse: 0.58647 | val_1_rmse: 0.62015 |  0:00:15s
epoch 106| loss: 0.32758 | val_0_rmse: 0.57115 | val_1_rmse: 0.65497 |  0:00:15s
epoch 107| loss: 0.32383 | val_0_rmse: 0.59018 | val_1_rmse: 0.68371 |  0:00:15s
epoch 108| loss: 0.32105 | val_0_rmse: 0.56039 | val_1_rmse: 0.65096 |  0:00:15s
epoch 109| loss: 0.3282  | val_0_rmse: 0.571   | val_1_rmse: 0.666   |  0:00:16s
epoch 110| loss: 0.34948 | val_0_rmse: 0.61867 | val_1_rmse: 0.7044  |  0:00:16s
epoch 111| loss: 0.33722 | val_0_rmse: 0.69213 | val_1_rmse: 0.74285 |  0:00:16s
epoch 112| loss: 0.34058 | val_0_rmse: 0.61821 | val_1_rmse: 0.75235 |  0:00:16s
epoch 113| loss: 0.35314 | val_0_rmse: 0.57659 | val_1_rmse: 0.69003 |  0:00:16s
epoch 114| loss: 0.35005 | val_0_rmse: 0.56875 | val_1_rmse: 0.66153 |  0:00:16s
epoch 115| loss: 0.35306 | val_0_rmse: 0.56636 | val_1_rmse: 0.66208 |  0:00:16s
epoch 116| loss: 0.33283 | val_0_rmse: 0.58212 | val_1_rmse: 0.65098 |  0:00:17s
epoch 117| loss: 0.34958 | val_0_rmse: 0.6842  | val_1_rmse: 0.74441 |  0:00:17s
epoch 118| loss: 0.33567 | val_0_rmse: 0.7572  | val_1_rmse: 0.83127 |  0:00:17s
epoch 119| loss: 0.33618 | val_0_rmse: 0.80586 | val_1_rmse: 0.84719 |  0:00:17s
epoch 120| loss: 0.34261 | val_0_rmse: 0.65446 | val_1_rmse: 0.68055 |  0:00:17s
epoch 121| loss: 0.32706 | val_0_rmse: 0.56458 | val_1_rmse: 0.61747 |  0:00:17s
epoch 122| loss: 0.32958 | val_0_rmse: 0.57406 | val_1_rmse: 0.62496 |  0:00:17s
epoch 123| loss: 0.32238 | val_0_rmse: 0.59059 | val_1_rmse: 0.63822 |  0:00:18s
epoch 124| loss: 0.31972 | val_0_rmse: 0.55523 | val_1_rmse: 0.63623 |  0:00:18s
epoch 125| loss: 0.3215  | val_0_rmse: 0.55674 | val_1_rmse: 0.63186 |  0:00:18s
epoch 126| loss: 0.33554 | val_0_rmse: 0.55811 | val_1_rmse: 0.62147 |  0:00:18s
epoch 127| loss: 0.36552 | val_0_rmse: 0.55909 | val_1_rmse: 0.62517 |  0:00:18s
epoch 128| loss: 0.33311 | val_0_rmse: 0.5723  | val_1_rmse: 0.64356 |  0:00:18s
epoch 129| loss: 0.34573 | val_0_rmse: 0.61836 | val_1_rmse: 0.65191 |  0:00:18s
epoch 130| loss: 0.35274 | val_0_rmse: 0.63774 | val_1_rmse: 0.67831 |  0:00:19s
epoch 131| loss: 0.35263 | val_0_rmse: 0.60425 | val_1_rmse: 0.64909 |  0:00:19s
epoch 132| loss: 0.34383 | val_0_rmse: 0.62556 | val_1_rmse: 0.67837 |  0:00:19s
epoch 133| loss: 0.33391 | val_0_rmse: 0.58306 | val_1_rmse: 0.65762 |  0:00:19s
epoch 134| loss: 0.34062 | val_0_rmse: 0.69868 | val_1_rmse: 0.74153 |  0:00:19s
epoch 135| loss: 0.34455 | val_0_rmse: 0.81352 | val_1_rmse: 0.83536 |  0:00:19s
epoch 136| loss: 0.34653 | val_0_rmse: 0.63832 | val_1_rmse: 0.70152 |  0:00:19s
epoch 137| loss: 0.33654 | val_0_rmse: 0.57263 | val_1_rmse: 0.65485 |  0:00:20s
epoch 138| loss: 0.33633 | val_0_rmse: 0.56206 | val_1_rmse: 0.63605 |  0:00:20s
epoch 139| loss: 0.32418 | val_0_rmse: 0.55716 | val_1_rmse: 0.6229  |  0:00:20s
epoch 140| loss: 0.31672 | val_0_rmse: 0.60277 | val_1_rmse: 0.67711 |  0:00:20s
epoch 141| loss: 0.34178 | val_0_rmse: 0.58529 | val_1_rmse: 0.6736  |  0:00:20s
epoch 142| loss: 0.32435 | val_0_rmse: 0.5973  | val_1_rmse: 0.65879 |  0:00:20s
epoch 143| loss: 0.31442 | val_0_rmse: 0.61042 | val_1_rmse: 0.66265 |  0:00:20s
epoch 144| loss: 0.31759 | val_0_rmse: 0.56368 | val_1_rmse: 0.64149 |  0:00:21s
epoch 145| loss: 0.32255 | val_0_rmse: 0.65027 | val_1_rmse: 0.70834 |  0:00:21s
epoch 146| loss: 0.31045 | val_0_rmse: 0.72669 | val_1_rmse: 0.76958 |  0:00:21s
epoch 147| loss: 0.33119 | val_0_rmse: 0.74129 | val_1_rmse: 0.78688 |  0:00:21s
epoch 148| loss: 0.31602 | val_0_rmse: 0.68218 | val_1_rmse: 0.74322 |  0:00:21s
epoch 149| loss: 0.31863 | val_0_rmse: 0.67605 | val_1_rmse: 0.73826 |  0:00:21s
Stop training because you reached max_epochs = 150 with best_epoch = 121 and best_val_1_rmse = 0.61747
Best weights from best epoch are automatically used!
ended training at: 03:15:21
Feature importance:
[('Area', 0.3030588455834388), ('Baths', 0.14313878795615176), ('Beds', 0.01401221430491095), ('Latitude', 0.4799638079965748), ('Longitude', 0.03141203479515933), ('Month', 0.00417683535126602), ('Year', 0.024237474012498317)]
Mean squared error is of 2464900783.524795
Mean absolute error:35857.68684739011
MAPE:0.28249406582437
R2 score:0.6169590622268197
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:15:21
epoch 0  | loss: 1.57651 | val_0_rmse: 0.86529 | val_1_rmse: 0.83024 |  0:00:00s
epoch 1  | loss: 0.94952 | val_0_rmse: 1.02779 | val_1_rmse: 1.07002 |  0:00:00s
epoch 2  | loss: 0.66102 | val_0_rmse: 0.8499  | val_1_rmse: 0.80931 |  0:00:00s
epoch 3  | loss: 0.57764 | val_0_rmse: 2.8196  | val_1_rmse: 0.78607 |  0:00:00s
epoch 4  | loss: 0.5323  | val_0_rmse: 1.9922  | val_1_rmse: 0.78666 |  0:00:00s
epoch 5  | loss: 0.52411 | val_0_rmse: 1.01995 | val_1_rmse: 0.72531 |  0:00:00s
epoch 6  | loss: 0.50664 | val_0_rmse: 0.78247 | val_1_rmse: 0.70825 |  0:00:01s
epoch 7  | loss: 0.50487 | val_0_rmse: 0.74235 | val_1_rmse: 0.69088 |  0:00:01s
epoch 8  | loss: 0.50194 | val_0_rmse: 0.74399 | val_1_rmse: 0.69236 |  0:00:01s
epoch 9  | loss: 0.51051 | val_0_rmse: 0.71288 | val_1_rmse: 0.66317 |  0:00:01s
epoch 10 | loss: 0.47795 | val_0_rmse: 0.69856 | val_1_rmse: 0.66861 |  0:00:01s
epoch 11 | loss: 0.48648 | val_0_rmse: 0.71046 | val_1_rmse: 0.67472 |  0:00:01s
epoch 12 | loss: 0.48705 | val_0_rmse: 0.69593 | val_1_rmse: 0.65179 |  0:00:01s
epoch 13 | loss: 0.48049 | val_0_rmse: 0.68927 | val_1_rmse: 0.63856 |  0:00:02s
epoch 14 | loss: 0.46494 | val_0_rmse: 0.68618 | val_1_rmse: 0.61616 |  0:00:02s
epoch 15 | loss: 0.47565 | val_0_rmse: 0.68629 | val_1_rmse: 0.63977 |  0:00:02s
epoch 16 | loss: 0.47465 | val_0_rmse: 0.68712 | val_1_rmse: 0.63693 |  0:00:02s
epoch 17 | loss: 0.45271 | val_0_rmse: 0.67388 | val_1_rmse: 0.62855 |  0:00:02s
epoch 18 | loss: 0.45927 | val_0_rmse: 0.68814 | val_1_rmse: 0.64854 |  0:00:02s
epoch 19 | loss: 0.45514 | val_0_rmse: 0.66804 | val_1_rmse: 0.61046 |  0:00:02s
epoch 20 | loss: 0.44962 | val_0_rmse: 0.6574  | val_1_rmse: 0.60142 |  0:00:03s
epoch 21 | loss: 0.47618 | val_0_rmse: 0.66521 | val_1_rmse: 0.62082 |  0:00:03s
epoch 22 | loss: 0.44299 | val_0_rmse: 0.66983 | val_1_rmse: 0.61511 |  0:00:03s
epoch 23 | loss: 0.45953 | val_0_rmse: 0.65454 | val_1_rmse: 0.60752 |  0:00:03s
epoch 24 | loss: 0.44239 | val_0_rmse: 0.66056 | val_1_rmse: 0.63234 |  0:00:03s
epoch 25 | loss: 0.4312  | val_0_rmse: 0.66045 | val_1_rmse: 0.64125 |  0:00:03s
epoch 26 | loss: 0.4371  | val_0_rmse: 0.65284 | val_1_rmse: 0.61981 |  0:00:03s
epoch 27 | loss: 0.43567 | val_0_rmse: 0.66188 | val_1_rmse: 0.63384 |  0:00:04s
epoch 28 | loss: 0.43768 | val_0_rmse: 0.66083 | val_1_rmse: 0.62452 |  0:00:04s
epoch 29 | loss: 0.43224 | val_0_rmse: 0.65335 | val_1_rmse: 0.60733 |  0:00:04s
epoch 30 | loss: 0.44943 | val_0_rmse: 0.66079 | val_1_rmse: 0.61129 |  0:00:04s
epoch 31 | loss: 0.44055 | val_0_rmse: 0.66048 | val_1_rmse: 0.61939 |  0:00:04s
epoch 32 | loss: 0.44351 | val_0_rmse: 0.65915 | val_1_rmse: 0.6193  |  0:00:04s
epoch 33 | loss: 0.43475 | val_0_rmse: 0.65024 | val_1_rmse: 0.61482 |  0:00:04s
epoch 34 | loss: 0.42585 | val_0_rmse: 0.6506  | val_1_rmse: 0.6121  |  0:00:05s
epoch 35 | loss: 0.43714 | val_0_rmse: 0.67079 | val_1_rmse: 0.63236 |  0:00:05s
epoch 36 | loss: 0.44935 | val_0_rmse: 0.66857 | val_1_rmse: 0.62561 |  0:00:05s
epoch 37 | loss: 0.44875 | val_0_rmse: 0.6598  | val_1_rmse: 0.61134 |  0:00:05s
epoch 38 | loss: 0.43946 | val_0_rmse: 0.66001 | val_1_rmse: 0.61676 |  0:00:05s
epoch 39 | loss: 0.4557  | val_0_rmse: 0.68791 | val_1_rmse: 0.65582 |  0:00:05s
epoch 40 | loss: 0.45061 | val_0_rmse: 0.66475 | val_1_rmse: 0.62165 |  0:00:05s
epoch 41 | loss: 0.44933 | val_0_rmse: 0.67857 | val_1_rmse: 0.62364 |  0:00:06s
epoch 42 | loss: 0.44736 | val_0_rmse: 0.65811 | val_1_rmse: 0.60039 |  0:00:06s
epoch 43 | loss: 0.43591 | val_0_rmse: 0.65819 | val_1_rmse: 0.61419 |  0:00:06s
epoch 44 | loss: 0.44378 | val_0_rmse: 0.6568  | val_1_rmse: 0.61227 |  0:00:06s
epoch 45 | loss: 0.43687 | val_0_rmse: 0.66213 | val_1_rmse: 0.62418 |  0:00:06s
epoch 46 | loss: 0.43417 | val_0_rmse: 0.65952 | val_1_rmse: 0.61828 |  0:00:06s
epoch 47 | loss: 0.44062 | val_0_rmse: 0.6577  | val_1_rmse: 0.61541 |  0:00:06s
epoch 48 | loss: 0.42531 | val_0_rmse: 0.66097 | val_1_rmse: 0.62572 |  0:00:07s
epoch 49 | loss: 0.43422 | val_0_rmse: 0.65424 | val_1_rmse: 0.61927 |  0:00:07s
epoch 50 | loss: 0.41998 | val_0_rmse: 0.65776 | val_1_rmse: 0.60913 |  0:00:07s
epoch 51 | loss: 0.4424  | val_0_rmse: 0.66204 | val_1_rmse: 0.59229 |  0:00:07s
epoch 52 | loss: 0.4275  | val_0_rmse: 0.66128 | val_1_rmse: 0.59801 |  0:00:07s
epoch 53 | loss: 0.42134 | val_0_rmse: 0.66294 | val_1_rmse: 0.61166 |  0:00:07s
epoch 54 | loss: 0.42377 | val_0_rmse: 0.65168 | val_1_rmse: 0.61315 |  0:00:08s
epoch 55 | loss: 0.43007 | val_0_rmse: 0.64134 | val_1_rmse: 0.60415 |  0:00:08s
epoch 56 | loss: 0.42397 | val_0_rmse: 0.64164 | val_1_rmse: 0.60173 |  0:00:08s
epoch 57 | loss: 0.41907 | val_0_rmse: 0.6453  | val_1_rmse: 0.60359 |  0:00:08s
epoch 58 | loss: 0.41248 | val_0_rmse: 0.65414 | val_1_rmse: 0.60945 |  0:00:08s
epoch 59 | loss: 0.41215 | val_0_rmse: 0.67046 | val_1_rmse: 0.6424  |  0:00:08s
epoch 60 | loss: 0.40147 | val_0_rmse: 0.65625 | val_1_rmse: 0.61647 |  0:00:08s
epoch 61 | loss: 0.3988  | val_0_rmse: 0.6511  | val_1_rmse: 0.60331 |  0:00:09s
epoch 62 | loss: 0.41286 | val_0_rmse: 0.65802 | val_1_rmse: 0.62435 |  0:00:09s
epoch 63 | loss: 0.38913 | val_0_rmse: 0.69557 | val_1_rmse: 0.64706 |  0:00:09s
epoch 64 | loss: 0.40134 | val_0_rmse: 0.68759 | val_1_rmse: 0.65239 |  0:00:09s
epoch 65 | loss: 0.39348 | val_0_rmse: 0.64651 | val_1_rmse: 0.63082 |  0:00:09s
epoch 66 | loss: 0.39067 | val_0_rmse: 0.62974 | val_1_rmse: 0.61319 |  0:00:09s
epoch 67 | loss: 0.38497 | val_0_rmse: 0.61164 | val_1_rmse: 0.59656 |  0:00:09s
epoch 68 | loss: 0.37277 | val_0_rmse: 0.61295 | val_1_rmse: 0.60501 |  0:00:10s
epoch 69 | loss: 0.38152 | val_0_rmse: 0.62303 | val_1_rmse: 0.60284 |  0:00:10s
epoch 70 | loss: 0.37425 | val_0_rmse: 0.60685 | val_1_rmse: 0.5879  |  0:00:10s
epoch 71 | loss: 0.35712 | val_0_rmse: 0.61903 | val_1_rmse: 0.59446 |  0:00:10s
epoch 72 | loss: 0.37259 | val_0_rmse: 0.66057 | val_1_rmse: 0.64217 |  0:00:10s
epoch 73 | loss: 0.37077 | val_0_rmse: 0.68632 | val_1_rmse: 0.66193 |  0:00:10s
epoch 74 | loss: 0.37839 | val_0_rmse: 0.60219 | val_1_rmse: 0.57894 |  0:00:10s
epoch 75 | loss: 0.38614 | val_0_rmse: 0.59135 | val_1_rmse: 0.54985 |  0:00:11s
epoch 76 | loss: 0.38099 | val_0_rmse: 0.60864 | val_1_rmse: 0.56413 |  0:00:11s
epoch 77 | loss: 0.39251 | val_0_rmse: 0.60082 | val_1_rmse: 0.55682 |  0:00:11s
epoch 78 | loss: 0.37839 | val_0_rmse: 0.61732 | val_1_rmse: 0.58658 |  0:00:11s
epoch 79 | loss: 0.37775 | val_0_rmse: 0.61048 | val_1_rmse: 0.58801 |  0:00:11s
epoch 80 | loss: 0.36128 | val_0_rmse: 0.60883 | val_1_rmse: 0.5868  |  0:00:11s
epoch 81 | loss: 0.36662 | val_0_rmse: 0.61905 | val_1_rmse: 0.60046 |  0:00:11s
epoch 82 | loss: 0.35791 | val_0_rmse: 0.61222 | val_1_rmse: 0.58763 |  0:00:12s
epoch 83 | loss: 0.36996 | val_0_rmse: 0.63033 | val_1_rmse: 0.60161 |  0:00:12s
epoch 84 | loss: 0.3524  | val_0_rmse: 0.64336 | val_1_rmse: 0.60407 |  0:00:12s
epoch 85 | loss: 0.35948 | val_0_rmse: 0.64547 | val_1_rmse: 0.6025  |  0:00:12s
epoch 86 | loss: 0.34581 | val_0_rmse: 0.63286 | val_1_rmse: 0.59482 |  0:00:12s
epoch 87 | loss: 0.33795 | val_0_rmse: 0.607   | val_1_rmse: 0.5789  |  0:00:12s
epoch 88 | loss: 0.35294 | val_0_rmse: 0.59023 | val_1_rmse: 0.55231 |  0:00:12s
epoch 89 | loss: 0.34351 | val_0_rmse: 0.58967 | val_1_rmse: 0.54734 |  0:00:13s
epoch 90 | loss: 0.3513  | val_0_rmse: 0.61469 | val_1_rmse: 0.58305 |  0:00:13s
epoch 91 | loss: 0.34197 | val_0_rmse: 0.62975 | val_1_rmse: 0.60873 |  0:00:13s
epoch 92 | loss: 0.32915 | val_0_rmse: 0.63461 | val_1_rmse: 0.61182 |  0:00:13s
epoch 93 | loss: 0.33368 | val_0_rmse: 0.64049 | val_1_rmse: 0.61829 |  0:00:13s
epoch 94 | loss: 0.33995 | val_0_rmse: 0.63173 | val_1_rmse: 0.60967 |  0:00:13s
epoch 95 | loss: 0.33043 | val_0_rmse: 0.60559 | val_1_rmse: 0.58109 |  0:00:13s
epoch 96 | loss: 0.32804 | val_0_rmse: 0.60251 | val_1_rmse: 0.5753  |  0:00:14s
epoch 97 | loss: 0.34294 | val_0_rmse: 0.59888 | val_1_rmse: 0.5845  |  0:00:14s
epoch 98 | loss: 0.33952 | val_0_rmse: 0.56882 | val_1_rmse: 0.55374 |  0:00:14s
epoch 99 | loss: 0.34455 | val_0_rmse: 0.59987 | val_1_rmse: 0.57065 |  0:00:14s
epoch 100| loss: 0.33051 | val_0_rmse: 0.65418 | val_1_rmse: 0.62743 |  0:00:14s
epoch 101| loss: 0.33536 | val_0_rmse: 0.67973 | val_1_rmse: 0.64712 |  0:00:14s
epoch 102| loss: 0.33828 | val_0_rmse: 0.69586 | val_1_rmse: 0.65307 |  0:00:14s
epoch 103| loss: 0.32233 | val_0_rmse: 0.56384 | val_1_rmse: 0.5188  |  0:00:15s
epoch 104| loss: 0.3254  | val_0_rmse: 0.6018  | val_1_rmse: 0.5714  |  0:00:15s
epoch 105| loss: 0.33432 | val_0_rmse: 0.62573 | val_1_rmse: 0.61126 |  0:00:15s
epoch 106| loss: 0.32792 | val_0_rmse: 0.62773 | val_1_rmse: 0.608   |  0:00:15s
epoch 107| loss: 0.32206 | val_0_rmse: 0.62086 | val_1_rmse: 0.60599 |  0:00:15s
epoch 108| loss: 0.32137 | val_0_rmse: 0.60206 | val_1_rmse: 0.59222 |  0:00:15s
epoch 109| loss: 0.3188  | val_0_rmse: 0.57881 | val_1_rmse: 0.56531 |  0:00:15s
epoch 110| loss: 0.30775 | val_0_rmse: 0.58653 | val_1_rmse: 0.57366 |  0:00:16s
epoch 111| loss: 0.31531 | val_0_rmse: 0.59532 | val_1_rmse: 0.5794  |  0:00:16s
epoch 112| loss: 0.32153 | val_0_rmse: 0.60474 | val_1_rmse: 0.5838  |  0:00:16s
epoch 113| loss: 0.31886 | val_0_rmse: 0.56804 | val_1_rmse: 0.54225 |  0:00:16s
epoch 114| loss: 0.30986 | val_0_rmse: 0.55393 | val_1_rmse: 0.53571 |  0:00:16s
epoch 115| loss: 0.30817 | val_0_rmse: 0.64604 | val_1_rmse: 0.63596 |  0:00:16s
epoch 116| loss: 0.31461 | val_0_rmse: 0.67998 | val_1_rmse: 0.67007 |  0:00:17s
epoch 117| loss: 0.3285  | val_0_rmse: 0.54844 | val_1_rmse: 0.5125  |  0:00:17s
epoch 118| loss: 0.31946 | val_0_rmse: 0.5775  | val_1_rmse: 0.54831 |  0:00:17s
epoch 119| loss: 0.31261 | val_0_rmse: 0.57368 | val_1_rmse: 0.54433 |  0:00:17s
epoch 120| loss: 0.30294 | val_0_rmse: 0.543   | val_1_rmse: 0.50549 |  0:00:17s
epoch 121| loss: 0.3035  | val_0_rmse: 0.54257 | val_1_rmse: 0.50864 |  0:00:17s
epoch 122| loss: 0.30344 | val_0_rmse: 0.53486 | val_1_rmse: 0.50799 |  0:00:17s
epoch 123| loss: 0.30571 | val_0_rmse: 0.53135 | val_1_rmse: 0.50989 |  0:00:18s
epoch 124| loss: 0.30486 | val_0_rmse: 0.53109 | val_1_rmse: 0.51449 |  0:00:18s
epoch 125| loss: 0.30751 | val_0_rmse: 0.52964 | val_1_rmse: 0.51828 |  0:00:18s
epoch 126| loss: 0.30307 | val_0_rmse: 0.59336 | val_1_rmse: 0.59243 |  0:00:18s
epoch 127| loss: 0.2971  | val_0_rmse: 0.66    | val_1_rmse: 0.65636 |  0:00:18s
epoch 128| loss: 0.31381 | val_0_rmse: 0.58831 | val_1_rmse: 0.60476 |  0:00:18s
epoch 129| loss: 0.30036 | val_0_rmse: 0.53182 | val_1_rmse: 0.53318 |  0:00:18s
epoch 130| loss: 0.29985 | val_0_rmse: 0.55041 | val_1_rmse: 0.54059 |  0:00:19s
epoch 131| loss: 0.31878 | val_0_rmse: 0.55847 | val_1_rmse: 0.56926 |  0:00:19s
epoch 132| loss: 0.30239 | val_0_rmse: 0.5288  | val_1_rmse: 0.5274  |  0:00:19s
epoch 133| loss: 0.29155 | val_0_rmse: 0.56884 | val_1_rmse: 0.55871 |  0:00:19s
epoch 134| loss: 0.30529 | val_0_rmse: 0.57412 | val_1_rmse: 0.5656  |  0:00:19s
epoch 135| loss: 0.2936  | val_0_rmse: 0.53746 | val_1_rmse: 0.52854 |  0:00:19s
epoch 136| loss: 0.30139 | val_0_rmse: 0.54508 | val_1_rmse: 0.54403 |  0:00:19s
epoch 137| loss: 0.29505 | val_0_rmse: 0.52525 | val_1_rmse: 0.51461 |  0:00:20s
epoch 138| loss: 0.2964  | val_0_rmse: 0.57317 | val_1_rmse: 0.57572 |  0:00:20s
epoch 139| loss: 0.29529 | val_0_rmse: 0.60034 | val_1_rmse: 0.60917 |  0:00:20s
epoch 140| loss: 0.28683 | val_0_rmse: 0.57171 | val_1_rmse: 0.58242 |  0:00:20s
epoch 141| loss: 0.29188 | val_0_rmse: 0.54812 | val_1_rmse: 0.55838 |  0:00:20s
epoch 142| loss: 0.29874 | val_0_rmse: 0.57467 | val_1_rmse: 0.58422 |  0:00:20s
epoch 143| loss: 0.29858 | val_0_rmse: 0.62717 | val_1_rmse: 0.64735 |  0:00:20s
epoch 144| loss: 0.30168 | val_0_rmse: 0.65117 | val_1_rmse: 0.66573 |  0:00:21s
epoch 145| loss: 0.29437 | val_0_rmse: 0.55828 | val_1_rmse: 0.57561 |  0:00:21s
epoch 146| loss: 0.28644 | val_0_rmse: 0.52855 | val_1_rmse: 0.55039 |  0:00:21s
epoch 147| loss: 0.29175 | val_0_rmse: 0.56959 | val_1_rmse: 0.58946 |  0:00:21s
epoch 148| loss: 0.29864 | val_0_rmse: 0.56406 | val_1_rmse: 0.57538 |  0:00:21s
epoch 149| loss: 0.28657 | val_0_rmse: 0.52675 | val_1_rmse: 0.532   |  0:00:21s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.50549
Best weights from best epoch are automatically used!
ended training at: 03:15:42
Feature importance:
[('Area', 0.24982125092770388), ('Baths', 0.13044945864346624), ('Beds', 0.038105708834234864), ('Latitude', 0.35009593257575683), ('Longitude', 0.17758722897835888), ('Month', 0.01759721116224021), ('Year', 0.036343208878239136)]
Mean squared error is of 2916941749.079424
Mean absolute error:36094.52278798077
MAPE:0.30118044891348844
R2 score:0.6302195680258639
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:15:42
epoch 0  | loss: 1.14457 | val_0_rmse: 0.98418 | val_1_rmse: 1.00308 |  0:00:00s
epoch 1  | loss: 0.66381 | val_0_rmse: 0.86677 | val_1_rmse: 0.85775 |  0:00:00s
epoch 2  | loss: 0.58507 | val_0_rmse: 0.78006 | val_1_rmse: 0.79483 |  0:00:00s
epoch 3  | loss: 0.52759 | val_0_rmse: 0.80606 | val_1_rmse: 0.82693 |  0:00:01s
epoch 4  | loss: 0.52264 | val_0_rmse: 0.72107 | val_1_rmse: 0.7478  |  0:00:01s
epoch 5  | loss: 0.50902 | val_0_rmse: 0.70888 | val_1_rmse: 0.73245 |  0:00:01s
epoch 6  | loss: 0.49083 | val_0_rmse: 0.70929 | val_1_rmse: 0.71642 |  0:00:02s
epoch 7  | loss: 0.49207 | val_0_rmse: 0.69732 | val_1_rmse: 0.69815 |  0:00:02s
epoch 8  | loss: 0.48443 | val_0_rmse: 0.68435 | val_1_rmse: 0.68556 |  0:00:02s
epoch 9  | loss: 0.46608 | val_0_rmse: 0.68261 | val_1_rmse: 0.68638 |  0:00:03s
epoch 10 | loss: 0.45738 | val_0_rmse: 0.67658 | val_1_rmse: 0.68021 |  0:00:03s
epoch 11 | loss: 0.45225 | val_0_rmse: 0.67786 | val_1_rmse: 0.67594 |  0:00:03s
epoch 12 | loss: 0.45065 | val_0_rmse: 0.66695 | val_1_rmse: 0.66691 |  0:00:04s
epoch 13 | loss: 0.4374  | val_0_rmse: 0.66181 | val_1_rmse: 0.6621  |  0:00:04s
epoch 14 | loss: 0.44463 | val_0_rmse: 0.66551 | val_1_rmse: 0.67462 |  0:00:04s
epoch 15 | loss: 0.42637 | val_0_rmse: 0.66217 | val_1_rmse: 0.66822 |  0:00:05s
epoch 16 | loss: 0.41879 | val_0_rmse: 0.65851 | val_1_rmse: 0.66523 |  0:00:05s
epoch 17 | loss: 0.42947 | val_0_rmse: 0.65566 | val_1_rmse: 0.66819 |  0:00:05s
epoch 18 | loss: 0.43111 | val_0_rmse: 0.66038 | val_1_rmse: 0.66752 |  0:00:06s
epoch 19 | loss: 0.42732 | val_0_rmse: 0.65897 | val_1_rmse: 0.65872 |  0:00:06s
epoch 20 | loss: 0.43249 | val_0_rmse: 0.6464  | val_1_rmse: 0.6489  |  0:00:06s
epoch 21 | loss: 0.42385 | val_0_rmse: 0.68458 | val_1_rmse: 0.69527 |  0:00:07s
epoch 22 | loss: 0.41134 | val_0_rmse: 0.63542 | val_1_rmse: 0.65312 |  0:00:07s
epoch 23 | loss: 0.4038  | val_0_rmse: 0.70367 | val_1_rmse: 0.7036  |  0:00:07s
epoch 24 | loss: 0.39671 | val_0_rmse: 0.61612 | val_1_rmse: 0.6112  |  0:00:08s
epoch 25 | loss: 0.38793 | val_0_rmse: 0.65431 | val_1_rmse: 0.64964 |  0:00:08s
epoch 26 | loss: 0.40254 | val_0_rmse: 0.6247  | val_1_rmse: 0.62224 |  0:00:08s
epoch 27 | loss: 0.38412 | val_0_rmse: 0.63865 | val_1_rmse: 0.63184 |  0:00:09s
epoch 28 | loss: 0.39332 | val_0_rmse: 0.63541 | val_1_rmse: 0.63403 |  0:00:09s
epoch 29 | loss: 0.41099 | val_0_rmse: 0.64067 | val_1_rmse: 0.6317  |  0:00:09s
epoch 30 | loss: 0.39638 | val_0_rmse: 0.61361 | val_1_rmse: 0.61785 |  0:00:10s
epoch 31 | loss: 0.3877  | val_0_rmse: 0.61395 | val_1_rmse: 0.61341 |  0:00:10s
epoch 32 | loss: 0.39391 | val_0_rmse: 0.63703 | val_1_rmse: 0.63612 |  0:00:10s
epoch 33 | loss: 0.38355 | val_0_rmse: 0.62769 | val_1_rmse: 0.63537 |  0:00:10s
epoch 34 | loss: 0.37441 | val_0_rmse: 0.65718 | val_1_rmse: 0.65699 |  0:00:11s
epoch 35 | loss: 0.38034 | val_0_rmse: 0.64568 | val_1_rmse: 0.64473 |  0:00:11s
epoch 36 | loss: 0.36566 | val_0_rmse: 0.61695 | val_1_rmse: 0.60605 |  0:00:11s
epoch 37 | loss: 0.36849 | val_0_rmse: 0.67436 | val_1_rmse: 0.67416 |  0:00:12s
epoch 38 | loss: 0.36754 | val_0_rmse: 0.61399 | val_1_rmse: 0.61831 |  0:00:12s
epoch 39 | loss: 0.37172 | val_0_rmse: 0.64996 | val_1_rmse: 0.64471 |  0:00:12s
epoch 40 | loss: 0.37187 | val_0_rmse: 0.63765 | val_1_rmse: 0.631   |  0:00:13s
epoch 41 | loss: 0.37521 | val_0_rmse: 0.63998 | val_1_rmse: 0.63872 |  0:00:13s
epoch 42 | loss: 0.36393 | val_0_rmse: 0.6323  | val_1_rmse: 0.62924 |  0:00:13s
epoch 43 | loss: 0.36884 | val_0_rmse: 0.62592 | val_1_rmse: 0.61779 |  0:00:14s
epoch 44 | loss: 0.36164 | val_0_rmse: 0.60583 | val_1_rmse: 0.60061 |  0:00:14s
epoch 45 | loss: 0.35497 | val_0_rmse: 0.66378 | val_1_rmse: 0.65864 |  0:00:14s
epoch 46 | loss: 0.36074 | val_0_rmse: 0.60259 | val_1_rmse: 0.59335 |  0:00:15s
epoch 47 | loss: 0.36933 | val_0_rmse: 0.61852 | val_1_rmse: 0.61339 |  0:00:15s
epoch 48 | loss: 0.38706 | val_0_rmse: 0.62144 | val_1_rmse: 0.60469 |  0:00:15s
epoch 49 | loss: 0.38688 | val_0_rmse: 0.63101 | val_1_rmse: 0.63064 |  0:00:16s
epoch 50 | loss: 0.37542 | val_0_rmse: 0.60549 | val_1_rmse: 0.59609 |  0:00:16s
epoch 51 | loss: 0.37998 | val_0_rmse: 0.74334 | val_1_rmse: 0.73562 |  0:00:16s
epoch 52 | loss: 0.37023 | val_0_rmse: 0.60452 | val_1_rmse: 0.59524 |  0:00:17s
epoch 53 | loss: 0.37711 | val_0_rmse: 0.65152 | val_1_rmse: 0.65131 |  0:00:17s
epoch 54 | loss: 0.36582 | val_0_rmse: 0.59556 | val_1_rmse: 0.60057 |  0:00:17s
epoch 55 | loss: 0.37239 | val_0_rmse: 0.64214 | val_1_rmse: 0.65546 |  0:00:18s
epoch 56 | loss: 0.368   | val_0_rmse: 0.60653 | val_1_rmse: 0.60979 |  0:00:18s
epoch 57 | loss: 0.35931 | val_0_rmse: 0.65791 | val_1_rmse: 0.66499 |  0:00:18s
epoch 58 | loss: 0.35072 | val_0_rmse: 0.62518 | val_1_rmse: 0.62019 |  0:00:18s
epoch 59 | loss: 0.35073 | val_0_rmse: 0.65768 | val_1_rmse: 0.65765 |  0:00:19s
epoch 60 | loss: 0.36228 | val_0_rmse: 0.60162 | val_1_rmse: 0.60306 |  0:00:19s
epoch 61 | loss: 0.34918 | val_0_rmse: 0.61893 | val_1_rmse: 0.6231  |  0:00:19s
epoch 62 | loss: 0.35821 | val_0_rmse: 0.59384 | val_1_rmse: 0.59856 |  0:00:20s
epoch 63 | loss: 0.34932 | val_0_rmse: 0.59539 | val_1_rmse: 0.58936 |  0:00:20s
epoch 64 | loss: 0.34698 | val_0_rmse: 0.60676 | val_1_rmse: 0.60213 |  0:00:20s
epoch 65 | loss: 0.33703 | val_0_rmse: 0.59372 | val_1_rmse: 0.59483 |  0:00:21s
epoch 66 | loss: 0.34407 | val_0_rmse: 0.61705 | val_1_rmse: 0.61615 |  0:00:21s
epoch 67 | loss: 0.36483 | val_0_rmse: 0.69946 | val_1_rmse: 0.6959  |  0:00:21s
epoch 68 | loss: 0.35574 | val_0_rmse: 0.5907  | val_1_rmse: 0.59444 |  0:00:22s
epoch 69 | loss: 0.35054 | val_0_rmse: 0.64651 | val_1_rmse: 0.65828 |  0:00:22s
epoch 70 | loss: 0.35667 | val_0_rmse: 0.59959 | val_1_rmse: 0.60038 |  0:00:22s
epoch 71 | loss: 0.3582  | val_0_rmse: 0.63718 | val_1_rmse: 0.63104 |  0:00:23s
epoch 72 | loss: 0.35816 | val_0_rmse: 0.65948 | val_1_rmse: 0.64697 |  0:00:23s
epoch 73 | loss: 0.3518  | val_0_rmse: 0.61947 | val_1_rmse: 0.62076 |  0:00:23s
epoch 74 | loss: 0.35165 | val_0_rmse: 0.68074 | val_1_rmse: 0.68195 |  0:00:24s
epoch 75 | loss: 0.34981 | val_0_rmse: 0.58805 | val_1_rmse: 0.59468 |  0:00:24s
epoch 76 | loss: 0.34489 | val_0_rmse: 0.5854  | val_1_rmse: 0.59722 |  0:00:24s
epoch 77 | loss: 0.35327 | val_0_rmse: 0.58107 | val_1_rmse: 0.58006 |  0:00:25s
epoch 78 | loss: 0.34579 | val_0_rmse: 0.57534 | val_1_rmse: 0.58443 |  0:00:25s
epoch 79 | loss: 0.34807 | val_0_rmse: 0.58769 | val_1_rmse: 0.59785 |  0:00:25s
epoch 80 | loss: 0.35237 | val_0_rmse: 0.63163 | val_1_rmse: 0.63409 |  0:00:26s
epoch 81 | loss: 0.36276 | val_0_rmse: 0.767   | val_1_rmse: 0.75881 |  0:00:26s
epoch 82 | loss: 0.35302 | val_0_rmse: 0.60053 | val_1_rmse: 0.60312 |  0:00:26s
epoch 83 | loss: 0.35187 | val_0_rmse: 0.62198 | val_1_rmse: 0.62958 |  0:00:26s
epoch 84 | loss: 0.33413 | val_0_rmse: 0.5943  | val_1_rmse: 0.60144 |  0:00:27s
epoch 85 | loss: 0.35428 | val_0_rmse: 0.57602 | val_1_rmse: 0.58217 |  0:00:27s
epoch 86 | loss: 0.33916 | val_0_rmse: 0.58573 | val_1_rmse: 0.59915 |  0:00:27s
epoch 87 | loss: 0.34143 | val_0_rmse: 0.575   | val_1_rmse: 0.5848  |  0:00:28s
epoch 88 | loss: 0.33637 | val_0_rmse: 0.59829 | val_1_rmse: 0.60658 |  0:00:28s
epoch 89 | loss: 0.33529 | val_0_rmse: 0.58492 | val_1_rmse: 0.58689 |  0:00:28s
epoch 90 | loss: 0.32811 | val_0_rmse: 0.5705  | val_1_rmse: 0.58296 |  0:00:29s
epoch 91 | loss: 0.32936 | val_0_rmse: 0.58662 | val_1_rmse: 0.59361 |  0:00:29s
epoch 92 | loss: 0.34298 | val_0_rmse: 0.61396 | val_1_rmse: 0.6186  |  0:00:29s
epoch 93 | loss: 0.32776 | val_0_rmse: 0.59068 | val_1_rmse: 0.60226 |  0:00:30s
epoch 94 | loss: 0.32847 | val_0_rmse: 0.58956 | val_1_rmse: 0.60937 |  0:00:30s
epoch 95 | loss: 0.32752 | val_0_rmse: 0.599   | val_1_rmse: 0.61395 |  0:00:30s
epoch 96 | loss: 0.3301  | val_0_rmse: 0.62919 | val_1_rmse: 0.63931 |  0:00:31s
epoch 97 | loss: 0.32295 | val_0_rmse: 0.57676 | val_1_rmse: 0.59511 |  0:00:31s
epoch 98 | loss: 0.33534 | val_0_rmse: 0.61896 | val_1_rmse: 0.63098 |  0:00:31s
epoch 99 | loss: 0.34465 | val_0_rmse: 0.5885  | val_1_rmse: 0.59489 |  0:00:32s
epoch 100| loss: 0.33317 | val_0_rmse: 0.6028  | val_1_rmse: 0.60626 |  0:00:32s
epoch 101| loss: 0.33981 | val_0_rmse: 0.57264 | val_1_rmse: 0.5789  |  0:00:32s
epoch 102| loss: 0.34168 | val_0_rmse: 0.61473 | val_1_rmse: 0.62915 |  0:00:33s
epoch 103| loss: 0.33369 | val_0_rmse: 0.59909 | val_1_rmse: 0.60935 |  0:00:33s
epoch 104| loss: 0.3245  | val_0_rmse: 0.57454 | val_1_rmse: 0.59574 |  0:00:33s
epoch 105| loss: 0.32464 | val_0_rmse: 0.61058 | val_1_rmse: 0.61833 |  0:00:34s
epoch 106| loss: 0.32274 | val_0_rmse: 0.59807 | val_1_rmse: 0.60616 |  0:00:34s
epoch 107| loss: 0.32963 | val_0_rmse: 0.60239 | val_1_rmse: 0.60614 |  0:00:34s
epoch 108| loss: 0.32685 | val_0_rmse: 0.6116  | val_1_rmse: 0.61975 |  0:00:34s
epoch 109| loss: 0.31662 | val_0_rmse: 0.6185  | val_1_rmse: 0.61571 |  0:00:35s
epoch 110| loss: 0.32685 | val_0_rmse: 0.57496 | val_1_rmse: 0.57898 |  0:00:35s
epoch 111| loss: 0.32474 | val_0_rmse: 0.61585 | val_1_rmse: 0.62794 |  0:00:35s
epoch 112| loss: 0.33597 | val_0_rmse: 0.57632 | val_1_rmse: 0.59224 |  0:00:36s
epoch 113| loss: 0.34134 | val_0_rmse: 0.56425 | val_1_rmse: 0.57975 |  0:00:36s
epoch 114| loss: 0.33123 | val_0_rmse: 0.6217  | val_1_rmse: 0.62302 |  0:00:36s
epoch 115| loss: 0.32615 | val_0_rmse: 0.58677 | val_1_rmse: 0.59055 |  0:00:37s
epoch 116| loss: 0.3292  | val_0_rmse: 0.57812 | val_1_rmse: 0.58731 |  0:00:37s
epoch 117| loss: 0.32281 | val_0_rmse: 0.65761 | val_1_rmse: 0.65848 |  0:00:37s
epoch 118| loss: 0.3323  | val_0_rmse: 0.60553 | val_1_rmse: 0.60011 |  0:00:38s
epoch 119| loss: 0.3263  | val_0_rmse: 0.57229 | val_1_rmse: 0.58067 |  0:00:38s
epoch 120| loss: 0.34026 | val_0_rmse: 0.60439 | val_1_rmse: 0.60353 |  0:00:38s
epoch 121| loss: 0.32665 | val_0_rmse: 0.61712 | val_1_rmse: 0.62329 |  0:00:39s
epoch 122| loss: 0.32721 | val_0_rmse: 0.62783 | val_1_rmse: 0.63378 |  0:00:39s
epoch 123| loss: 0.32257 | val_0_rmse: 0.6261  | val_1_rmse: 0.62187 |  0:00:39s
epoch 124| loss: 0.32555 | val_0_rmse: 0.65539 | val_1_rmse: 0.6442  |  0:00:40s
epoch 125| loss: 0.32747 | val_0_rmse: 0.60672 | val_1_rmse: 0.60512 |  0:00:40s
epoch 126| loss: 0.32606 | val_0_rmse: 0.57401 | val_1_rmse: 0.57741 |  0:00:40s
epoch 127| loss: 0.32927 | val_0_rmse: 0.57364 | val_1_rmse: 0.58368 |  0:00:41s
epoch 128| loss: 0.32637 | val_0_rmse: 0.56942 | val_1_rmse: 0.57757 |  0:00:41s
epoch 129| loss: 0.32978 | val_0_rmse: 0.59207 | val_1_rmse: 0.59959 |  0:00:41s
epoch 130| loss: 0.32348 | val_0_rmse: 0.6392  | val_1_rmse: 0.64096 |  0:00:41s
epoch 131| loss: 0.32346 | val_0_rmse: 0.60046 | val_1_rmse: 0.60448 |  0:00:42s
epoch 132| loss: 0.32754 | val_0_rmse: 0.55826 | val_1_rmse: 0.57777 |  0:00:42s
epoch 133| loss: 0.31602 | val_0_rmse: 0.58753 | val_1_rmse: 0.59325 |  0:00:42s
epoch 134| loss: 0.32194 | val_0_rmse: 0.59106 | val_1_rmse: 0.6086  |  0:00:43s
epoch 135| loss: 0.32796 | val_0_rmse: 0.56445 | val_1_rmse: 0.58495 |  0:00:43s
epoch 136| loss: 0.32316 | val_0_rmse: 0.57299 | val_1_rmse: 0.5923  |  0:00:43s
epoch 137| loss: 0.3339  | val_0_rmse: 0.60072 | val_1_rmse: 0.60335 |  0:00:44s
epoch 138| loss: 0.33126 | val_0_rmse: 0.62774 | val_1_rmse: 0.63433 |  0:00:44s
epoch 139| loss: 0.32117 | val_0_rmse: 0.56186 | val_1_rmse: 0.58756 |  0:00:44s
epoch 140| loss: 0.32298 | val_0_rmse: 0.59392 | val_1_rmse: 0.60665 |  0:00:45s
epoch 141| loss: 0.32851 | val_0_rmse: 0.59269 | val_1_rmse: 0.59715 |  0:00:45s
epoch 142| loss: 0.3371  | val_0_rmse: 0.57563 | val_1_rmse: 0.5891  |  0:00:45s
epoch 143| loss: 0.31955 | val_0_rmse: 0.62393 | val_1_rmse: 0.62747 |  0:00:46s
epoch 144| loss: 0.32638 | val_0_rmse: 0.57174 | val_1_rmse: 0.57947 |  0:00:46s
epoch 145| loss: 0.32562 | val_0_rmse: 0.55644 | val_1_rmse: 0.56722 |  0:00:46s
epoch 146| loss: 0.32969 | val_0_rmse: 0.57716 | val_1_rmse: 0.58248 |  0:00:47s
epoch 147| loss: 0.33967 | val_0_rmse: 0.64236 | val_1_rmse: 0.6386  |  0:00:47s
epoch 148| loss: 0.32728 | val_0_rmse: 0.57979 | val_1_rmse: 0.59238 |  0:00:47s
epoch 149| loss: 0.32512 | val_0_rmse: 0.60909 | val_1_rmse: 0.61391 |  0:00:48s
Stop training because you reached max_epochs = 150 with best_epoch = 145 and best_val_1_rmse = 0.56722
Best weights from best epoch are automatically used!
ended training at: 03:16:31
Feature importance:
[('Area', 0.4124567722164748), ('Baths', 0.11604946926730211), ('Beds', 0.029679189456742944), ('Latitude', 0.163511069114876), ('Longitude', 0.2096996201851378), ('Month', 0.0), ('Year', 0.06860387975946632)]
Mean squared error is of 2500295281.565708
Mean absolute error:34117.072625053064
MAPE:0.3090968310326941
R2 score:0.692210696286055
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:16:31
epoch 0  | loss: 1.2731  | val_0_rmse: 0.95067 | val_1_rmse: 0.95793 |  0:00:00s
epoch 1  | loss: 0.60915 | val_0_rmse: 0.81127 | val_1_rmse: 0.87789 |  0:00:00s
epoch 2  | loss: 0.52123 | val_0_rmse: 0.7837  | val_1_rmse: 0.82262 |  0:00:00s
epoch 3  | loss: 0.4983  | val_0_rmse: 0.71739 | val_1_rmse: 0.77643 |  0:00:01s
epoch 4  | loss: 0.46009 | val_0_rmse: 0.74657 | val_1_rmse: 0.80578 |  0:00:01s
epoch 5  | loss: 0.44794 | val_0_rmse: 0.67134 | val_1_rmse: 0.73347 |  0:00:01s
epoch 6  | loss: 0.44947 | val_0_rmse: 0.67073 | val_1_rmse: 0.73878 |  0:00:02s
epoch 7  | loss: 0.43413 | val_0_rmse: 0.65664 | val_1_rmse: 0.70407 |  0:00:02s
epoch 8  | loss: 0.44055 | val_0_rmse: 0.65765 | val_1_rmse: 0.69434 |  0:00:02s
epoch 9  | loss: 0.42805 | val_0_rmse: 0.64665 | val_1_rmse: 0.70008 |  0:00:03s
epoch 10 | loss: 0.42287 | val_0_rmse: 0.64267 | val_1_rmse: 0.69479 |  0:00:03s
epoch 11 | loss: 0.42541 | val_0_rmse: 0.63548 | val_1_rmse: 0.69742 |  0:00:03s
epoch 12 | loss: 0.41288 | val_0_rmse: 0.6343  | val_1_rmse: 0.6866  |  0:00:04s
epoch 13 | loss: 0.41946 | val_0_rmse: 0.63865 | val_1_rmse: 0.69112 |  0:00:04s
epoch 14 | loss: 0.39982 | val_0_rmse: 0.65238 | val_1_rmse: 0.70517 |  0:00:04s
epoch 15 | loss: 0.38371 | val_0_rmse: 0.60674 | val_1_rmse: 0.659   |  0:00:05s
epoch 16 | loss: 0.39118 | val_0_rmse: 0.6225  | val_1_rmse: 0.67301 |  0:00:05s
epoch 17 | loss: 0.3675  | val_0_rmse: 0.62138 | val_1_rmse: 0.67093 |  0:00:05s
epoch 18 | loss: 0.36395 | val_0_rmse: 0.60042 | val_1_rmse: 0.65098 |  0:00:06s
epoch 19 | loss: 0.3756  | val_0_rmse: 0.65182 | val_1_rmse: 0.69644 |  0:00:06s
epoch 20 | loss: 0.36589 | val_0_rmse: 0.60352 | val_1_rmse: 0.6616  |  0:00:06s
epoch 21 | loss: 0.37083 | val_0_rmse: 0.59296 | val_1_rmse: 0.66611 |  0:00:07s
epoch 22 | loss: 0.37504 | val_0_rmse: 0.60729 | val_1_rmse: 0.6692  |  0:00:07s
epoch 23 | loss: 0.37085 | val_0_rmse: 0.64874 | val_1_rmse: 0.69587 |  0:00:07s
epoch 24 | loss: 0.37302 | val_0_rmse: 0.61851 | val_1_rmse: 0.66576 |  0:00:08s
epoch 25 | loss: 0.36226 | val_0_rmse: 0.58053 | val_1_rmse: 0.63225 |  0:00:08s
epoch 26 | loss: 0.35728 | val_0_rmse: 0.58163 | val_1_rmse: 0.64318 |  0:00:08s
epoch 27 | loss: 0.34759 | val_0_rmse: 0.58496 | val_1_rmse: 0.64386 |  0:00:08s
epoch 28 | loss: 0.33914 | val_0_rmse: 0.60116 | val_1_rmse: 0.65271 |  0:00:09s
epoch 29 | loss: 0.36001 | val_0_rmse: 0.61063 | val_1_rmse: 0.66502 |  0:00:09s
epoch 30 | loss: 0.36071 | val_0_rmse: 0.63682 | val_1_rmse: 0.68802 |  0:00:09s
epoch 31 | loss: 0.35611 | val_0_rmse: 0.58197 | val_1_rmse: 0.63996 |  0:00:10s
epoch 32 | loss: 0.34449 | val_0_rmse: 0.65652 | val_1_rmse: 0.70621 |  0:00:10s
epoch 33 | loss: 0.34456 | val_0_rmse: 0.59436 | val_1_rmse: 0.65355 |  0:00:10s
epoch 34 | loss: 0.34265 | val_0_rmse: 0.56656 | val_1_rmse: 0.62386 |  0:00:11s
epoch 35 | loss: 0.3335  | val_0_rmse: 0.59589 | val_1_rmse: 0.6534  |  0:00:11s
epoch 36 | loss: 0.34223 | val_0_rmse: 0.58041 | val_1_rmse: 0.64285 |  0:00:11s
epoch 37 | loss: 0.33886 | val_0_rmse: 0.57926 | val_1_rmse: 0.7808  |  0:00:12s
epoch 38 | loss: 0.34482 | val_0_rmse: 0.58603 | val_1_rmse: 0.66843 |  0:00:12s
epoch 39 | loss: 0.33479 | val_0_rmse: 0.60021 | val_1_rmse: 0.65235 |  0:00:12s
epoch 40 | loss: 0.33483 | val_0_rmse: 0.57496 | val_1_rmse: 0.62105 |  0:00:13s
epoch 41 | loss: 0.33599 | val_0_rmse: 0.57528 | val_1_rmse: 0.62808 |  0:00:13s
epoch 42 | loss: 0.33652 | val_0_rmse: 0.58246 | val_1_rmse: 0.63902 |  0:00:13s
epoch 43 | loss: 0.33898 | val_0_rmse: 0.60239 | val_1_rmse: 0.65572 |  0:00:14s
epoch 44 | loss: 0.34409 | val_0_rmse: 0.59272 | val_1_rmse: 0.64357 |  0:00:14s
epoch 45 | loss: 0.34989 | val_0_rmse: 0.59294 | val_1_rmse: 0.63681 |  0:00:14s
epoch 46 | loss: 0.33904 | val_0_rmse: 0.59901 | val_1_rmse: 0.63582 |  0:00:15s
epoch 47 | loss: 0.35715 | val_0_rmse: 0.60468 | val_1_rmse: 0.64522 |  0:00:15s
epoch 48 | loss: 0.35592 | val_0_rmse: 0.60496 | val_1_rmse: 0.64454 |  0:00:15s
epoch 49 | loss: 0.34716 | val_0_rmse: 0.58088 | val_1_rmse: 0.62458 |  0:00:16s
epoch 50 | loss: 0.34219 | val_0_rmse: 0.58529 | val_1_rmse: 0.63298 |  0:00:16s
epoch 51 | loss: 0.33947 | val_0_rmse: 0.58937 | val_1_rmse: 0.63461 |  0:00:16s
epoch 52 | loss: 0.34826 | val_0_rmse: 0.61182 | val_1_rmse: 0.65915 |  0:00:16s
epoch 53 | loss: 0.3405  | val_0_rmse: 0.6042  | val_1_rmse: 0.65068 |  0:00:17s
epoch 54 | loss: 0.35881 | val_0_rmse: 0.65443 | val_1_rmse: 0.68662 |  0:00:17s
epoch 55 | loss: 0.35231 | val_0_rmse: 0.58509 | val_1_rmse: 0.63507 |  0:00:17s
epoch 56 | loss: 0.35593 | val_0_rmse: 0.58883 | val_1_rmse: 0.63465 |  0:00:18s
epoch 57 | loss: 0.34896 | val_0_rmse: 0.58259 | val_1_rmse: 0.62813 |  0:00:18s
epoch 58 | loss: 0.34181 | val_0_rmse: 0.62779 | val_1_rmse: 0.66517 |  0:00:18s
epoch 59 | loss: 0.34206 | val_0_rmse: 0.59523 | val_1_rmse: 0.63544 |  0:00:19s
epoch 60 | loss: 0.34068 | val_0_rmse: 0.57138 | val_1_rmse: 0.62197 |  0:00:19s
epoch 61 | loss: 0.33955 | val_0_rmse: 0.58358 | val_1_rmse: 0.63631 |  0:00:19s
epoch 62 | loss: 0.33899 | val_0_rmse: 0.61994 | val_1_rmse: 0.66357 |  0:00:20s
epoch 63 | loss: 0.32855 | val_0_rmse: 0.62162 | val_1_rmse: 0.66423 |  0:00:20s
epoch 64 | loss: 0.33484 | val_0_rmse: 0.5794  | val_1_rmse: 0.62933 |  0:00:20s
epoch 65 | loss: 0.33876 | val_0_rmse: 0.57354 | val_1_rmse: 0.63115 |  0:00:21s
epoch 66 | loss: 0.33197 | val_0_rmse: 0.61548 | val_1_rmse: 0.65701 |  0:00:21s
epoch 67 | loss: 0.32462 | val_0_rmse: 0.62781 | val_1_rmse: 0.67385 |  0:00:21s
epoch 68 | loss: 0.31727 | val_0_rmse: 0.63956 | val_1_rmse: 0.68505 |  0:00:21s
epoch 69 | loss: 0.33068 | val_0_rmse: 0.58107 | val_1_rmse: 0.63237 |  0:00:22s
epoch 70 | loss: 0.32084 | val_0_rmse: 0.55854 | val_1_rmse: 0.61827 |  0:00:22s
epoch 71 | loss: 0.31743 | val_0_rmse: 0.56633 | val_1_rmse: 0.62434 |  0:00:22s
epoch 72 | loss: 0.30893 | val_0_rmse: 0.56788 | val_1_rmse: 0.62055 |  0:00:23s
epoch 73 | loss: 0.31708 | val_0_rmse: 0.55941 | val_1_rmse: 0.60927 |  0:00:23s
epoch 74 | loss: 0.32325 | val_0_rmse: 0.56925 | val_1_rmse: 0.62191 |  0:00:23s
epoch 75 | loss: 0.32704 | val_0_rmse: 0.55728 | val_1_rmse: 0.61802 |  0:00:24s
epoch 76 | loss: 0.31777 | val_0_rmse: 0.55236 | val_1_rmse: 0.61235 |  0:00:24s
epoch 77 | loss: 0.31602 | val_0_rmse: 0.55512 | val_1_rmse: 0.61723 |  0:00:24s
epoch 78 | loss: 0.31639 | val_0_rmse: 0.58125 | val_1_rmse: 0.6435  |  0:00:25s
epoch 79 | loss: 0.31866 | val_0_rmse: 0.57292 | val_1_rmse: 0.64635 |  0:00:25s
epoch 80 | loss: 0.31172 | val_0_rmse: 0.58742 | val_1_rmse: 0.64533 |  0:00:25s
epoch 81 | loss: 0.31469 | val_0_rmse: 0.56858 | val_1_rmse: 0.62507 |  0:00:26s
epoch 82 | loss: 0.32346 | val_0_rmse: 0.59234 | val_1_rmse: 0.65507 |  0:00:26s
epoch 83 | loss: 0.30963 | val_0_rmse: 0.5775  | val_1_rmse: 0.6425  |  0:00:26s
epoch 84 | loss: 0.30306 | val_0_rmse: 0.57361 | val_1_rmse: 0.64366 |  0:00:27s
epoch 85 | loss: 0.31996 | val_0_rmse: 0.61155 | val_1_rmse: 0.66734 |  0:00:27s
epoch 86 | loss: 0.31843 | val_0_rmse: 0.57988 | val_1_rmse: 0.6345  |  0:00:27s
epoch 87 | loss: 0.31175 | val_0_rmse: 0.56958 | val_1_rmse: 0.62284 |  0:00:28s
epoch 88 | loss: 0.32073 | val_0_rmse: 0.56574 | val_1_rmse: 0.6284  |  0:00:28s
epoch 89 | loss: 0.32228 | val_0_rmse: 0.59844 | val_1_rmse: 0.65166 |  0:00:28s
epoch 90 | loss: 0.31628 | val_0_rmse: 0.58226 | val_1_rmse: 0.64186 |  0:00:28s
epoch 91 | loss: 0.32066 | val_0_rmse: 0.59975 | val_1_rmse: 0.66157 |  0:00:29s
epoch 92 | loss: 0.30528 | val_0_rmse: 0.57339 | val_1_rmse: 0.63263 |  0:00:29s
epoch 93 | loss: 0.30964 | val_0_rmse: 0.56329 | val_1_rmse: 0.62313 |  0:00:30s
epoch 94 | loss: 0.31095 | val_0_rmse: 0.57445 | val_1_rmse: 0.63107 |  0:00:30s
epoch 95 | loss: 0.30876 | val_0_rmse: 0.59031 | val_1_rmse: 0.6485  |  0:00:30s
epoch 96 | loss: 0.30243 | val_0_rmse: 0.57006 | val_1_rmse: 0.63347 |  0:00:30s
epoch 97 | loss: 0.29222 | val_0_rmse: 0.56693 | val_1_rmse: 0.63482 |  0:00:31s
epoch 98 | loss: 0.30178 | val_0_rmse: 0.55001 | val_1_rmse: 0.61993 |  0:00:31s
epoch 99 | loss: 0.3071  | val_0_rmse: 0.5341  | val_1_rmse: 0.60578 |  0:00:31s
epoch 100| loss: 0.29507 | val_0_rmse: 0.5394  | val_1_rmse: 0.60401 |  0:00:32s
epoch 101| loss: 0.28382 | val_0_rmse: 0.54357 | val_1_rmse: 0.60624 |  0:00:32s
epoch 102| loss: 0.3003  | val_0_rmse: 0.58448 | val_1_rmse: 0.64384 |  0:00:32s
epoch 103| loss: 0.28913 | val_0_rmse: 0.55618 | val_1_rmse: 0.62021 |  0:00:33s
epoch 104| loss: 0.28478 | val_0_rmse: 0.57186 | val_1_rmse: 0.63402 |  0:00:33s
epoch 105| loss: 0.29147 | val_0_rmse: 0.56506 | val_1_rmse: 0.62503 |  0:00:33s
epoch 106| loss: 0.29826 | val_0_rmse: 0.53871 | val_1_rmse: 0.60261 |  0:00:34s
epoch 107| loss: 0.3041  | val_0_rmse: 0.54093 | val_1_rmse: 0.61172 |  0:00:34s
epoch 108| loss: 0.30232 | val_0_rmse: 0.60005 | val_1_rmse: 0.66081 |  0:00:34s
epoch 109| loss: 0.29733 | val_0_rmse: 0.5854  | val_1_rmse: 0.64858 |  0:00:35s
epoch 110| loss: 0.29456 | val_0_rmse: 0.60486 | val_1_rmse: 0.66144 |  0:00:35s
epoch 111| loss: 0.29634 | val_0_rmse: 0.56014 | val_1_rmse: 0.61827 |  0:00:35s
epoch 112| loss: 0.29607 | val_0_rmse: 0.53207 | val_1_rmse: 0.59876 |  0:00:36s
epoch 113| loss: 0.30093 | val_0_rmse: 0.54568 | val_1_rmse: 0.61139 |  0:00:36s
epoch 114| loss: 0.28583 | val_0_rmse: 0.54408 | val_1_rmse: 0.61195 |  0:00:36s
epoch 115| loss: 0.29198 | val_0_rmse: 0.54068 | val_1_rmse: 0.60577 |  0:00:36s
epoch 116| loss: 0.28409 | val_0_rmse: 0.54602 | val_1_rmse: 0.60985 |  0:00:37s
epoch 117| loss: 0.29337 | val_0_rmse: 0.53669 | val_1_rmse: 0.6023  |  0:00:37s
epoch 118| loss: 0.29647 | val_0_rmse: 0.56084 | val_1_rmse: 0.62392 |  0:00:38s
epoch 119| loss: 0.28521 | val_0_rmse: 0.5462  | val_1_rmse: 0.61069 |  0:00:38s
epoch 120| loss: 0.30119 | val_0_rmse: 0.54386 | val_1_rmse: 0.60654 |  0:00:38s
epoch 121| loss: 0.29329 | val_0_rmse: 0.56618 | val_1_rmse: 0.62227 |  0:00:38s
epoch 122| loss: 0.30286 | val_0_rmse: 0.54565 | val_1_rmse: 0.60684 |  0:00:39s
epoch 123| loss: 0.30961 | val_0_rmse: 0.58592 | val_1_rmse: 0.64048 |  0:00:39s
epoch 124| loss: 0.29804 | val_0_rmse: 0.55521 | val_1_rmse: 0.62213 |  0:00:39s
epoch 125| loss: 0.28425 | val_0_rmse: 0.54544 | val_1_rmse: 0.61756 |  0:00:40s
epoch 126| loss: 0.28387 | val_0_rmse: 0.57022 | val_1_rmse: 0.63637 |  0:00:40s
epoch 127| loss: 0.28288 | val_0_rmse: 0.57606 | val_1_rmse: 0.65087 |  0:00:40s
epoch 128| loss: 0.30597 | val_0_rmse: 0.61775 | val_1_rmse: 0.6792  |  0:00:41s
epoch 129| loss: 0.28932 | val_0_rmse: 0.58215 | val_1_rmse: 0.64649 |  0:00:41s
epoch 130| loss: 0.29412 | val_0_rmse: 0.59502 | val_1_rmse: 0.65558 |  0:00:41s
epoch 131| loss: 0.28522 | val_0_rmse: 0.53447 | val_1_rmse: 0.60525 |  0:00:42s
epoch 132| loss: 0.29068 | val_0_rmse: 0.5519  | val_1_rmse: 0.6209  |  0:00:42s
epoch 133| loss: 0.28415 | val_0_rmse: 0.56007 | val_1_rmse: 0.6279  |  0:00:42s
epoch 134| loss: 0.2929  | val_0_rmse: 0.55128 | val_1_rmse: 0.6152  |  0:00:43s
epoch 135| loss: 0.28178 | val_0_rmse: 0.53474 | val_1_rmse: 0.60472 |  0:00:43s
epoch 136| loss: 0.28374 | val_0_rmse: 0.53095 | val_1_rmse: 0.60405 |  0:00:43s
epoch 137| loss: 0.2896  | val_0_rmse: 0.53054 | val_1_rmse: 0.60256 |  0:00:44s
epoch 138| loss: 0.2864  | val_0_rmse: 0.54365 | val_1_rmse: 0.62355 |  0:00:44s
epoch 139| loss: 0.28594 | val_0_rmse: 0.54087 | val_1_rmse: 0.60235 |  0:00:44s
epoch 140| loss: 0.29033 | val_0_rmse: 0.52931 | val_1_rmse: 0.60467 |  0:00:45s
epoch 141| loss: 0.29691 | val_0_rmse: 0.57314 | val_1_rmse: 0.64181 |  0:00:45s
epoch 142| loss: 0.29108 | val_0_rmse: 0.55067 | val_1_rmse: 0.62348 |  0:00:45s

Early stopping occured at epoch 142 with best_epoch = 112 and best_val_1_rmse = 0.59876
Best weights from best epoch are automatically used!
ended training at: 03:17:17
Feature importance:
[('Area', 0.29715637592976707), ('Baths', 0.21435388128968796), ('Beds', 0.018551953395816927), ('Latitude', 0.1330935135519209), ('Longitude', 0.297236799390614), ('Month', 0.003425777107866464), ('Year', 0.03618169933432672)]
Mean squared error is of 2974166070.821259
Mean absolute error:36221.2373681823
MAPE:0.3973129892393474
R2 score:0.6592310827521358
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:17:17
epoch 0  | loss: 1.20736 | val_0_rmse: 0.97103 | val_1_rmse: 0.92862 |  0:00:00s
epoch 1  | loss: 0.59072 | val_0_rmse: 0.75136 | val_1_rmse: 0.78194 |  0:00:00s
epoch 2  | loss: 0.52253 | val_0_rmse: 0.73468 | val_1_rmse: 0.75481 |  0:00:00s
epoch 3  | loss: 0.51182 | val_0_rmse: 0.76411 | val_1_rmse: 0.77831 |  0:00:01s
epoch 4  | loss: 0.50596 | val_0_rmse: 0.76145 | val_1_rmse: 0.76694 |  0:00:01s
epoch 5  | loss: 0.50284 | val_0_rmse: 0.72671 | val_1_rmse: 0.74324 |  0:00:01s
epoch 6  | loss: 0.4927  | val_0_rmse: 0.71348 | val_1_rmse: 0.74405 |  0:00:02s
epoch 7  | loss: 0.4704  | val_0_rmse: 0.69879 | val_1_rmse: 0.71436 |  0:00:02s
epoch 8  | loss: 0.45379 | val_0_rmse: 0.69849 | val_1_rmse: 0.7204  |  0:00:02s
epoch 9  | loss: 0.44811 | val_0_rmse: 0.66505 | val_1_rmse: 0.68598 |  0:00:03s
epoch 10 | loss: 0.43486 | val_0_rmse: 0.64671 | val_1_rmse: 0.67363 |  0:00:03s
epoch 11 | loss: 0.43306 | val_0_rmse: 0.65776 | val_1_rmse: 0.68401 |  0:00:03s
epoch 12 | loss: 0.4384  | val_0_rmse: 0.66211 | val_1_rmse: 0.68325 |  0:00:04s
epoch 13 | loss: 0.43183 | val_0_rmse: 0.67031 | val_1_rmse: 0.70041 |  0:00:04s
epoch 14 | loss: 0.42485 | val_0_rmse: 0.63085 | val_1_rmse: 0.65019 |  0:00:04s
epoch 15 | loss: 0.3985  | val_0_rmse: 0.61085 | val_1_rmse: 0.64089 |  0:00:05s
epoch 16 | loss: 0.40094 | val_0_rmse: 0.62108 | val_1_rmse: 0.64572 |  0:00:05s
epoch 17 | loss: 0.39556 | val_0_rmse: 0.63074 | val_1_rmse: 0.64489 |  0:00:05s
epoch 18 | loss: 0.38743 | val_0_rmse: 0.61463 | val_1_rmse: 0.63898 |  0:00:06s
epoch 19 | loss: 0.37257 | val_0_rmse: 0.60215 | val_1_rmse: 0.63561 |  0:00:06s
epoch 20 | loss: 0.38058 | val_0_rmse: 0.64751 | val_1_rmse: 0.65865 |  0:00:06s
epoch 21 | loss: 0.36865 | val_0_rmse: 0.6542  | val_1_rmse: 0.66813 |  0:00:07s
epoch 22 | loss: 0.36236 | val_0_rmse: 0.62388 | val_1_rmse: 0.64132 |  0:00:07s
epoch 23 | loss: 0.37081 | val_0_rmse: 0.61737 | val_1_rmse: 0.64634 |  0:00:07s
epoch 24 | loss: 0.37477 | val_0_rmse: 0.60814 | val_1_rmse: 0.63231 |  0:00:08s
epoch 25 | loss: 0.35763 | val_0_rmse: 0.60917 | val_1_rmse: 0.63918 |  0:00:08s
epoch 26 | loss: 0.3509  | val_0_rmse: 0.60165 | val_1_rmse: 0.63073 |  0:00:08s
epoch 27 | loss: 0.36045 | val_0_rmse: 0.603   | val_1_rmse: 0.62852 |  0:00:09s
epoch 28 | loss: 0.34475 | val_0_rmse: 0.64032 | val_1_rmse: 0.65654 |  0:00:09s
epoch 29 | loss: 0.34195 | val_0_rmse: 0.61016 | val_1_rmse: 0.63932 |  0:00:09s
epoch 30 | loss: 0.35264 | val_0_rmse: 0.64287 | val_1_rmse: 0.66406 |  0:00:09s
epoch 31 | loss: 0.34374 | val_0_rmse: 0.66424 | val_1_rmse: 0.69285 |  0:00:10s
epoch 32 | loss: 0.3427  | val_0_rmse: 0.59225 | val_1_rmse: 0.62063 |  0:00:10s
epoch 33 | loss: 0.33907 | val_0_rmse: 0.63712 | val_1_rmse: 0.64747 |  0:00:10s
epoch 34 | loss: 0.34041 | val_0_rmse: 0.60311 | val_1_rmse: 0.63071 |  0:00:11s
epoch 35 | loss: 0.35717 | val_0_rmse: 0.59725 | val_1_rmse: 0.62307 |  0:00:11s
epoch 36 | loss: 0.33823 | val_0_rmse: 0.59508 | val_1_rmse: 0.62568 |  0:00:11s
epoch 37 | loss: 0.33632 | val_0_rmse: 0.60064 | val_1_rmse: 0.62734 |  0:00:12s
epoch 38 | loss: 0.3422  | val_0_rmse: 0.60934 | val_1_rmse: 0.64615 |  0:00:12s
epoch 39 | loss: 0.34094 | val_0_rmse: 0.61521 | val_1_rmse: 0.65609 |  0:00:12s
epoch 40 | loss: 0.33453 | val_0_rmse: 0.59068 | val_1_rmse: 0.62859 |  0:00:13s
epoch 41 | loss: 0.33716 | val_0_rmse: 0.6237  | val_1_rmse: 0.65296 |  0:00:13s
epoch 42 | loss: 0.33289 | val_0_rmse: 0.59752 | val_1_rmse: 0.6276  |  0:00:13s
epoch 43 | loss: 0.33316 | val_0_rmse: 0.61769 | val_1_rmse: 0.65005 |  0:00:14s
epoch 44 | loss: 0.33744 | val_0_rmse: 0.59559 | val_1_rmse: 0.6218  |  0:00:14s
epoch 45 | loss: 0.34691 | val_0_rmse: 0.59457 | val_1_rmse: 0.62472 |  0:00:14s
epoch 46 | loss: 0.33857 | val_0_rmse: 0.59605 | val_1_rmse: 0.62645 |  0:00:15s
epoch 47 | loss: 0.34495 | val_0_rmse: 0.6071  | val_1_rmse: 0.64051 |  0:00:15s
epoch 48 | loss: 0.34176 | val_0_rmse: 0.58587 | val_1_rmse: 0.62257 |  0:00:15s
epoch 49 | loss: 0.35468 | val_0_rmse: 0.62281 | val_1_rmse: 0.64648 |  0:00:16s
epoch 50 | loss: 0.34013 | val_0_rmse: 0.60104 | val_1_rmse: 0.64401 |  0:00:16s
epoch 51 | loss: 0.34471 | val_0_rmse: 0.63107 | val_1_rmse: 0.6647  |  0:00:16s
epoch 52 | loss: 0.34485 | val_0_rmse: 0.61524 | val_1_rmse: 0.65003 |  0:00:17s
epoch 53 | loss: 0.33885 | val_0_rmse: 0.65173 | val_1_rmse: 0.67115 |  0:00:17s
epoch 54 | loss: 0.34394 | val_0_rmse: 0.62352 | val_1_rmse: 0.6618  |  0:00:17s
epoch 55 | loss: 0.33849 | val_0_rmse: 0.63099 | val_1_rmse: 0.6633  |  0:00:17s
epoch 56 | loss: 0.33899 | val_0_rmse: 0.60111 | val_1_rmse: 0.63626 |  0:00:18s
epoch 57 | loss: 0.34229 | val_0_rmse: 0.60994 | val_1_rmse: 0.63725 |  0:00:18s
epoch 58 | loss: 0.33213 | val_0_rmse: 0.59392 | val_1_rmse: 0.62821 |  0:00:18s
epoch 59 | loss: 0.33641 | val_0_rmse: 0.61831 | val_1_rmse: 0.64608 |  0:00:19s
epoch 60 | loss: 0.32955 | val_0_rmse: 0.60164 | val_1_rmse: 0.63827 |  0:00:19s
epoch 61 | loss: 0.32923 | val_0_rmse: 0.57974 | val_1_rmse: 0.61497 |  0:00:19s
epoch 62 | loss: 0.32531 | val_0_rmse: 0.58603 | val_1_rmse: 0.61048 |  0:00:20s
epoch 63 | loss: 0.34328 | val_0_rmse: 0.59205 | val_1_rmse: 0.61676 |  0:00:20s
epoch 64 | loss: 0.3252  | val_0_rmse: 0.57884 | val_1_rmse: 0.60886 |  0:00:20s
epoch 65 | loss: 0.3321  | val_0_rmse: 0.60225 | val_1_rmse: 0.63512 |  0:00:21s
epoch 66 | loss: 0.32008 | val_0_rmse: 0.59339 | val_1_rmse: 0.6325  |  0:00:21s
epoch 67 | loss: 0.3409  | val_0_rmse: 0.58247 | val_1_rmse: 0.62197 |  0:00:21s
epoch 68 | loss: 0.33971 | val_0_rmse: 0.59345 | val_1_rmse: 0.63225 |  0:00:22s
epoch 69 | loss: 0.34076 | val_0_rmse: 0.61276 | val_1_rmse: 0.64397 |  0:00:22s
epoch 70 | loss: 0.32747 | val_0_rmse: 0.59281 | val_1_rmse: 0.62911 |  0:00:22s
epoch 71 | loss: 0.33122 | val_0_rmse: 0.58703 | val_1_rmse: 0.61689 |  0:00:23s
epoch 72 | loss: 0.3261  | val_0_rmse: 0.59761 | val_1_rmse: 0.62859 |  0:00:23s
epoch 73 | loss: 0.32308 | val_0_rmse: 0.57316 | val_1_rmse: 0.60973 |  0:00:23s
epoch 74 | loss: 0.32509 | val_0_rmse: 0.6059  | val_1_rmse: 0.62995 |  0:00:24s
epoch 75 | loss: 0.32208 | val_0_rmse: 0.61662 | val_1_rmse: 0.6368  |  0:00:24s
epoch 76 | loss: 0.32436 | val_0_rmse: 0.60182 | val_1_rmse: 0.62491 |  0:00:24s
epoch 77 | loss: 0.3163  | val_0_rmse: 0.59513 | val_1_rmse: 0.61677 |  0:00:25s
epoch 78 | loss: 0.33464 | val_0_rmse: 0.63238 | val_1_rmse: 0.65561 |  0:00:25s
epoch 79 | loss: 0.32186 | val_0_rmse: 0.6169  | val_1_rmse: 0.64802 |  0:00:25s
epoch 80 | loss: 0.31958 | val_0_rmse: 0.62429 | val_1_rmse: 0.65119 |  0:00:25s
epoch 81 | loss: 0.33634 | val_0_rmse: 0.5912  | val_1_rmse: 0.6206  |  0:00:26s
epoch 82 | loss: 0.36065 | val_0_rmse: 0.61686 | val_1_rmse: 0.6478  |  0:00:26s
epoch 83 | loss: 0.33486 | val_0_rmse: 0.63499 | val_1_rmse: 0.6805  |  0:00:26s
epoch 84 | loss: 0.35168 | val_0_rmse: 0.68625 | val_1_rmse: 0.70322 |  0:00:27s
epoch 85 | loss: 0.3534  | val_0_rmse: 0.61211 | val_1_rmse: 0.66233 |  0:00:27s
epoch 86 | loss: 0.33604 | val_0_rmse: 0.57886 | val_1_rmse: 0.62007 |  0:00:27s
epoch 87 | loss: 0.32849 | val_0_rmse: 0.59262 | val_1_rmse: 0.62971 |  0:00:28s
epoch 88 | loss: 0.32899 | val_0_rmse: 0.58382 | val_1_rmse: 0.62325 |  0:00:28s
epoch 89 | loss: 0.33925 | val_0_rmse: 0.63344 | val_1_rmse: 0.65941 |  0:00:28s
epoch 90 | loss: 0.33601 | val_0_rmse: 0.59694 | val_1_rmse: 0.633   |  0:00:29s
epoch 91 | loss: 0.32967 | val_0_rmse: 0.62075 | val_1_rmse: 0.66168 |  0:00:29s
epoch 92 | loss: 0.33158 | val_0_rmse: 0.58216 | val_1_rmse: 0.62152 |  0:00:29s
epoch 93 | loss: 0.33581 | val_0_rmse: 0.60007 | val_1_rmse: 0.63116 |  0:00:30s
epoch 94 | loss: 0.33826 | val_0_rmse: 0.66608 | val_1_rmse: 0.68126 |  0:00:30s

Early stopping occured at epoch 94 with best_epoch = 64 and best_val_1_rmse = 0.60886
Best weights from best epoch are automatically used!
ended training at: 03:17:47
Feature importance:
[('Area', 0.31563521747871337), ('Baths', 0.1228960562484473), ('Beds', 0.0018710067205209436), ('Latitude', 0.1890986233373808), ('Longitude', 0.29007307003000654), ('Month', 0.04939135167905634), ('Year', 0.031034674505874677)]
Mean squared error is of 3312189126.971546
Mean absolute error:38220.252538041175
MAPE:0.3305332528640234
R2 score:0.5946431701924825
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:17:47
epoch 0  | loss: 1.14029 | val_0_rmse: 1.12301 | val_1_rmse: 1.1607  |  0:00:00s
epoch 1  | loss: 0.63797 | val_0_rmse: 0.86183 | val_1_rmse: 0.91049 |  0:00:00s
epoch 2  | loss: 0.53549 | val_0_rmse: 0.76598 | val_1_rmse: 0.81921 |  0:00:00s
epoch 3  | loss: 0.52944 | val_0_rmse: 0.75425 | val_1_rmse: 0.78747 |  0:00:01s
epoch 4  | loss: 0.52318 | val_0_rmse: 0.73459 | val_1_rmse: 0.74988 |  0:00:01s
epoch 5  | loss: 0.50836 | val_0_rmse: 0.72739 | val_1_rmse: 0.76572 |  0:00:01s
epoch 6  | loss: 0.49401 | val_0_rmse: 0.72633 | val_1_rmse: 0.75125 |  0:00:02s
epoch 7  | loss: 0.49673 | val_0_rmse: 0.70119 | val_1_rmse: 0.72499 |  0:00:02s
epoch 8  | loss: 0.48175 | val_0_rmse: 0.73308 | val_1_rmse: 0.76682 |  0:00:02s
epoch 9  | loss: 0.47528 | val_0_rmse: 0.71945 | val_1_rmse: 0.7555  |  0:00:03s
epoch 10 | loss: 0.472   | val_0_rmse: 0.68674 | val_1_rmse: 0.71708 |  0:00:03s
epoch 11 | loss: 0.46776 | val_0_rmse: 0.68078 | val_1_rmse: 0.71202 |  0:00:03s
epoch 12 | loss: 0.46195 | val_0_rmse: 0.66863 | val_1_rmse: 0.69746 |  0:00:04s
epoch 13 | loss: 0.44817 | val_0_rmse: 0.67291 | val_1_rmse: 0.71835 |  0:00:04s
epoch 14 | loss: 0.44455 | val_0_rmse: 0.6697  | val_1_rmse: 0.70399 |  0:00:04s
epoch 15 | loss: 0.45165 | val_0_rmse: 0.67308 | val_1_rmse: 0.71355 |  0:00:05s
epoch 16 | loss: 0.44634 | val_0_rmse: 0.65606 | val_1_rmse: 0.68767 |  0:00:05s
epoch 17 | loss: 0.44307 | val_0_rmse: 0.66778 | val_1_rmse: 0.70175 |  0:00:05s
epoch 18 | loss: 0.45634 | val_0_rmse: 0.66924 | val_1_rmse: 0.6974  |  0:00:06s
epoch 19 | loss: 0.44046 | val_0_rmse: 0.67338 | val_1_rmse: 0.70122 |  0:00:06s
epoch 20 | loss: 0.44109 | val_0_rmse: 0.66246 | val_1_rmse: 0.69355 |  0:00:06s
epoch 21 | loss: 0.44256 | val_0_rmse: 0.66741 | val_1_rmse: 0.69778 |  0:00:07s
epoch 22 | loss: 0.43923 | val_0_rmse: 0.67537 | val_1_rmse: 0.71221 |  0:00:07s
epoch 23 | loss: 0.43278 | val_0_rmse: 0.66025 | val_1_rmse: 0.68623 |  0:00:07s
epoch 24 | loss: 0.43271 | val_0_rmse: 0.66228 | val_1_rmse: 0.69456 |  0:00:08s
epoch 25 | loss: 0.43059 | val_0_rmse: 0.663   | val_1_rmse: 0.69402 |  0:00:08s
epoch 26 | loss: 0.43787 | val_0_rmse: 0.66741 | val_1_rmse: 0.69354 |  0:00:08s
epoch 27 | loss: 0.44156 | val_0_rmse: 0.66978 | val_1_rmse: 0.70051 |  0:00:08s
epoch 28 | loss: 0.43232 | val_0_rmse: 0.6672  | val_1_rmse: 0.70091 |  0:00:09s
epoch 29 | loss: 0.43734 | val_0_rmse: 0.66309 | val_1_rmse: 0.69308 |  0:00:09s
epoch 30 | loss: 0.43162 | val_0_rmse: 0.6629  | val_1_rmse: 0.68882 |  0:00:10s
epoch 31 | loss: 0.43205 | val_0_rmse: 0.66935 | val_1_rmse: 0.69936 |  0:00:10s
epoch 32 | loss: 0.43092 | val_0_rmse: 0.66659 | val_1_rmse: 0.69101 |  0:00:10s
epoch 33 | loss: 0.42934 | val_0_rmse: 0.67655 | val_1_rmse: 0.70507 |  0:00:10s
epoch 34 | loss: 0.42452 | val_0_rmse: 0.66591 | val_1_rmse: 0.68867 |  0:00:11s
epoch 35 | loss: 0.43093 | val_0_rmse: 0.67956 | val_1_rmse: 0.70902 |  0:00:11s
epoch 36 | loss: 0.4306  | val_0_rmse: 0.6881  | val_1_rmse: 0.70908 |  0:00:11s
epoch 37 | loss: 0.42691 | val_0_rmse: 0.66926 | val_1_rmse: 0.69001 |  0:00:12s
epoch 38 | loss: 0.4289  | val_0_rmse: 0.66972 | val_1_rmse: 0.69457 |  0:00:12s
epoch 39 | loss: 0.43101 | val_0_rmse: 0.68313 | val_1_rmse: 0.70511 |  0:00:12s
epoch 40 | loss: 0.43158 | val_0_rmse: 0.68149 | val_1_rmse: 0.70878 |  0:00:13s
epoch 41 | loss: 0.44566 | val_0_rmse: 0.69374 | val_1_rmse: 0.71564 |  0:00:13s
epoch 42 | loss: 0.42529 | val_0_rmse: 0.67716 | val_1_rmse: 0.69845 |  0:00:13s
epoch 43 | loss: 0.42632 | val_0_rmse: 0.6816  | val_1_rmse: 0.70921 |  0:00:14s
epoch 44 | loss: 0.43474 | val_0_rmse: 0.67228 | val_1_rmse: 0.69838 |  0:00:14s
epoch 45 | loss: 0.43543 | val_0_rmse: 0.66319 | val_1_rmse: 0.68781 |  0:00:14s
epoch 46 | loss: 0.41928 | val_0_rmse: 0.66667 | val_1_rmse: 0.69101 |  0:00:15s
epoch 47 | loss: 0.41843 | val_0_rmse: 0.66712 | val_1_rmse: 0.68305 |  0:00:15s
epoch 48 | loss: 0.42007 | val_0_rmse: 0.66712 | val_1_rmse: 0.69108 |  0:00:15s
epoch 49 | loss: 0.41657 | val_0_rmse: 0.67002 | val_1_rmse: 0.69874 |  0:00:16s
epoch 50 | loss: 0.42125 | val_0_rmse: 0.65735 | val_1_rmse: 0.68327 |  0:00:16s
epoch 51 | loss: 0.42379 | val_0_rmse: 0.65896 | val_1_rmse: 0.67645 |  0:00:16s
epoch 52 | loss: 0.41992 | val_0_rmse: 0.67375 | val_1_rmse: 0.69434 |  0:00:17s
epoch 53 | loss: 0.41816 | val_0_rmse: 0.67157 | val_1_rmse: 0.68929 |  0:00:17s
epoch 54 | loss: 0.4178  | val_0_rmse: 0.66564 | val_1_rmse: 0.67831 |  0:00:17s
epoch 55 | loss: 0.40751 | val_0_rmse: 0.66647 | val_1_rmse: 0.68515 |  0:00:17s
epoch 56 | loss: 0.41608 | val_0_rmse: 0.66745 | val_1_rmse: 0.68477 |  0:00:18s
epoch 57 | loss: 0.41241 | val_0_rmse: 0.66469 | val_1_rmse: 0.66896 |  0:00:18s
epoch 58 | loss: 0.41121 | val_0_rmse: 0.67098 | val_1_rmse: 0.69299 |  0:00:18s
epoch 59 | loss: 0.40661 | val_0_rmse: 0.67034 | val_1_rmse: 0.6912  |  0:00:19s
epoch 60 | loss: 0.40735 | val_0_rmse: 0.66188 | val_1_rmse: 0.68332 |  0:00:19s
epoch 61 | loss: 0.41056 | val_0_rmse: 0.65955 | val_1_rmse: 0.67199 |  0:00:19s
epoch 62 | loss: 0.41616 | val_0_rmse: 0.67785 | val_1_rmse: 0.70083 |  0:00:20s
epoch 63 | loss: 0.41874 | val_0_rmse: 0.68122 | val_1_rmse: 0.69949 |  0:00:20s
epoch 64 | loss: 0.41039 | val_0_rmse: 0.67679 | val_1_rmse: 0.69567 |  0:00:20s
epoch 65 | loss: 0.4127  | val_0_rmse: 0.68453 | val_1_rmse: 0.70508 |  0:00:21s
epoch 66 | loss: 0.41015 | val_0_rmse: 0.68571 | val_1_rmse: 0.70883 |  0:00:21s
epoch 67 | loss: 0.40333 | val_0_rmse: 0.67847 | val_1_rmse: 0.70243 |  0:00:21s
epoch 68 | loss: 0.39776 | val_0_rmse: 0.66545 | val_1_rmse: 0.68457 |  0:00:22s
epoch 69 | loss: 0.40944 | val_0_rmse: 0.66598 | val_1_rmse: 0.68743 |  0:00:22s
epoch 70 | loss: 0.40218 | val_0_rmse: 0.66874 | val_1_rmse: 0.69137 |  0:00:22s
epoch 71 | loss: 0.4065  | val_0_rmse: 0.67645 | val_1_rmse: 0.68784 |  0:00:23s
epoch 72 | loss: 0.41898 | val_0_rmse: 0.67821 | val_1_rmse: 0.68912 |  0:00:23s
epoch 73 | loss: 0.4264  | val_0_rmse: 0.6939  | val_1_rmse: 0.71742 |  0:00:23s
epoch 74 | loss: 0.42433 | val_0_rmse: 0.68792 | val_1_rmse: 0.69338 |  0:00:24s
epoch 75 | loss: 0.4106  | val_0_rmse: 0.70259 | val_1_rmse: 0.71481 |  0:00:24s
epoch 76 | loss: 0.41089 | val_0_rmse: 0.70068 | val_1_rmse: 0.70029 |  0:00:24s
epoch 77 | loss: 0.41726 | val_0_rmse: 0.69216 | val_1_rmse: 0.7017  |  0:00:24s
epoch 78 | loss: 0.42207 | val_0_rmse: 0.68863 | val_1_rmse: 0.70079 |  0:00:25s
epoch 79 | loss: 0.41959 | val_0_rmse: 0.68109 | val_1_rmse: 0.68385 |  0:00:25s
epoch 80 | loss: 0.41378 | val_0_rmse: 0.68895 | val_1_rmse: 0.70336 |  0:00:25s
epoch 81 | loss: 0.41381 | val_0_rmse: 0.6945  | val_1_rmse: 0.70354 |  0:00:26s
epoch 82 | loss: 0.42382 | val_0_rmse: 0.69948 | val_1_rmse: 0.71632 |  0:00:26s
epoch 83 | loss: 0.41936 | val_0_rmse: 0.69694 | val_1_rmse: 0.70503 |  0:00:26s
epoch 84 | loss: 0.41114 | val_0_rmse: 0.68503 | val_1_rmse: 0.70439 |  0:00:27s
epoch 85 | loss: 0.39943 | val_0_rmse: 0.68819 | val_1_rmse: 0.70781 |  0:00:27s
epoch 86 | loss: 0.40377 | val_0_rmse: 0.6784  | val_1_rmse: 0.69534 |  0:00:27s
epoch 87 | loss: 0.41537 | val_0_rmse: 0.70033 | val_1_rmse: 0.71054 |  0:00:28s

Early stopping occured at epoch 87 with best_epoch = 57 and best_val_1_rmse = 0.66896
Best weights from best epoch are automatically used!
ended training at: 03:18:16
Feature importance:
[('Area', 0.2644856470979815), ('Baths', 0.182168043016646), ('Beds', 0.026293657422367226), ('Latitude', 0.39254088733713477), ('Longitude', 0.08944329707201519), ('Month', 0.04067731172720749), ('Year', 0.00439115632664783)]
Mean squared error is of 3894419876.898106
Mean absolute error:42297.96959122984
MAPE:0.3934447852264408
R2 score:0.5128413199110962
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:18:16
epoch 0  | loss: 1.13306 | val_0_rmse: 1.21634 | val_1_rmse: 1.10563 |  0:00:00s
epoch 1  | loss: 0.62279 | val_0_rmse: 1.05991 | val_1_rmse: 0.84963 |  0:00:00s
epoch 2  | loss: 0.54179 | val_0_rmse: 0.75773 | val_1_rmse: 0.75671 |  0:00:00s
epoch 3  | loss: 0.51498 | val_0_rmse: 0.78901 | val_1_rmse: 0.75939 |  0:00:01s
epoch 4  | loss: 0.49987 | val_0_rmse: 0.71441 | val_1_rmse: 0.70097 |  0:00:01s
epoch 5  | loss: 0.49066 | val_0_rmse: 0.70668 | val_1_rmse: 0.70688 |  0:00:01s
epoch 6  | loss: 0.4901  | val_0_rmse: 0.73052 | val_1_rmse: 0.70509 |  0:00:02s
epoch 7  | loss: 0.47747 | val_0_rmse: 0.7631  | val_1_rmse: 0.7134  |  0:00:02s
epoch 8  | loss: 0.47903 | val_0_rmse: 0.76787 | val_1_rmse: 0.68238 |  0:00:02s
epoch 9  | loss: 0.46169 | val_0_rmse: 0.80951 | val_1_rmse: 0.66994 |  0:00:03s
epoch 10 | loss: 0.46059 | val_0_rmse: 0.74961 | val_1_rmse: 0.67555 |  0:00:03s
epoch 11 | loss: 0.45084 | val_0_rmse: 0.71366 | val_1_rmse: 0.68394 |  0:00:03s
epoch 12 | loss: 0.44645 | val_0_rmse: 0.81225 | val_1_rmse: 0.6813  |  0:00:04s
epoch 13 | loss: 0.44061 | val_0_rmse: 1.14593 | val_1_rmse: 0.66863 |  0:00:04s
epoch 14 | loss: 0.44495 | val_0_rmse: 1.15874 | val_1_rmse: 0.67148 |  0:00:04s
epoch 15 | loss: 0.447   | val_0_rmse: 0.80731 | val_1_rmse: 0.67474 |  0:00:05s
epoch 16 | loss: 0.45628 | val_0_rmse: 0.75602 | val_1_rmse: 0.69332 |  0:00:05s
epoch 17 | loss: 0.44051 | val_0_rmse: 0.75998 | val_1_rmse: 0.68786 |  0:00:05s
epoch 18 | loss: 0.44257 | val_0_rmse: 0.74241 | val_1_rmse: 0.6754  |  0:00:06s
epoch 19 | loss: 0.43679 | val_0_rmse: 0.71582 | val_1_rmse: 0.67557 |  0:00:06s
epoch 20 | loss: 0.43826 | val_0_rmse: 0.71766 | val_1_rmse: 0.67163 |  0:00:06s
epoch 21 | loss: 0.453   | val_0_rmse: 0.71479 | val_1_rmse: 0.67672 |  0:00:07s
epoch 22 | loss: 0.44471 | val_0_rmse: 0.71946 | val_1_rmse: 0.68165 |  0:00:07s
epoch 23 | loss: 0.45801 | val_0_rmse: 0.72363 | val_1_rmse: 0.68632 |  0:00:07s
epoch 24 | loss: 0.45116 | val_0_rmse: 0.70422 | val_1_rmse: 0.68117 |  0:00:08s
epoch 25 | loss: 0.43808 | val_0_rmse: 0.70689 | val_1_rmse: 0.67434 |  0:00:08s
epoch 26 | loss: 0.42401 | val_0_rmse: 0.71358 | val_1_rmse: 0.6841  |  0:00:08s
epoch 27 | loss: 0.42375 | val_0_rmse: 0.69484 | val_1_rmse: 0.66648 |  0:00:09s
epoch 28 | loss: 0.41157 | val_0_rmse: 0.67336 | val_1_rmse: 0.65235 |  0:00:09s
epoch 29 | loss: 0.409   | val_0_rmse: 0.67777 | val_1_rmse: 0.66218 |  0:00:09s
epoch 30 | loss: 0.40595 | val_0_rmse: 0.65273 | val_1_rmse: 0.65726 |  0:00:10s
epoch 31 | loss: 0.40147 | val_0_rmse: 0.63787 | val_1_rmse: 0.65043 |  0:00:10s
epoch 32 | loss: 0.39202 | val_0_rmse: 0.66372 | val_1_rmse: 0.67643 |  0:00:10s
epoch 33 | loss: 0.40129 | val_0_rmse: 0.69945 | val_1_rmse: 0.63989 |  0:00:10s
epoch 34 | loss: 0.42565 | val_0_rmse: 0.71224 | val_1_rmse: 0.67849 |  0:00:11s
epoch 35 | loss: 0.40538 | val_0_rmse: 0.74378 | val_1_rmse: 0.67692 |  0:00:11s
epoch 36 | loss: 0.40676 | val_0_rmse: 0.62556 | val_1_rmse: 0.64374 |  0:00:11s
epoch 37 | loss: 0.39651 | val_0_rmse: 0.61543 | val_1_rmse: 0.62625 |  0:00:12s
epoch 38 | loss: 0.40105 | val_0_rmse: 0.62958 | val_1_rmse: 0.6475  |  0:00:12s
epoch 39 | loss: 0.39885 | val_0_rmse: 0.9329  | val_1_rmse: 0.65773 |  0:00:12s
epoch 40 | loss: 0.41101 | val_0_rmse: 0.83964 | val_1_rmse: 0.69643 |  0:00:13s
epoch 41 | loss: 0.45403 | val_0_rmse: 0.78322 | val_1_rmse: 0.73976 |  0:00:13s
epoch 42 | loss: 0.4498  | val_0_rmse: 0.68783 | val_1_rmse: 0.67869 |  0:00:13s
epoch 43 | loss: 0.42988 | val_0_rmse: 0.69401 | val_1_rmse: 0.66981 |  0:00:14s
epoch 44 | loss: 0.41897 | val_0_rmse: 0.69823 | val_1_rmse: 0.68764 |  0:00:14s
epoch 45 | loss: 0.42378 | val_0_rmse: 0.69648 | val_1_rmse: 0.65221 |  0:00:14s
epoch 46 | loss: 0.43409 | val_0_rmse: 0.66605 | val_1_rmse: 0.64921 |  0:00:15s
epoch 47 | loss: 0.41434 | val_0_rmse: 0.66454 | val_1_rmse: 0.65424 |  0:00:15s
epoch 48 | loss: 0.41867 | val_0_rmse: 0.66213 | val_1_rmse: 0.6493  |  0:00:15s
epoch 49 | loss: 0.41267 | val_0_rmse: 0.65811 | val_1_rmse: 0.64725 |  0:00:16s
epoch 50 | loss: 0.4079  | val_0_rmse: 0.64113 | val_1_rmse: 0.64262 |  0:00:16s
epoch 51 | loss: 0.4008  | val_0_rmse: 0.63857 | val_1_rmse: 0.64379 |  0:00:16s
epoch 52 | loss: 0.39037 | val_0_rmse: 0.63154 | val_1_rmse: 0.6413  |  0:00:17s
epoch 53 | loss: 0.38218 | val_0_rmse: 0.64246 | val_1_rmse: 0.6539  |  0:00:17s
epoch 54 | loss: 0.37107 | val_0_rmse: 0.65702 | val_1_rmse: 0.66916 |  0:00:17s
epoch 55 | loss: 0.37381 | val_0_rmse: 0.6272  | val_1_rmse: 0.63716 |  0:00:17s
epoch 56 | loss: 0.37395 | val_0_rmse: 0.63268 | val_1_rmse: 0.63943 |  0:00:18s
epoch 57 | loss: 0.36799 | val_0_rmse: 0.63631 | val_1_rmse: 0.635   |  0:00:18s
epoch 58 | loss: 0.37017 | val_0_rmse: 0.65229 | val_1_rmse: 0.65688 |  0:00:18s
epoch 59 | loss: 0.36979 | val_0_rmse: 0.60974 | val_1_rmse: 0.61458 |  0:00:19s
epoch 60 | loss: 0.35267 | val_0_rmse: 0.65292 | val_1_rmse: 0.6584  |  0:00:19s
epoch 61 | loss: 0.34841 | val_0_rmse: 0.60141 | val_1_rmse: 0.61412 |  0:00:19s
epoch 62 | loss: 0.36854 | val_0_rmse: 0.64478 | val_1_rmse: 0.65367 |  0:00:20s
epoch 63 | loss: 0.34366 | val_0_rmse: 0.63707 | val_1_rmse: 0.65576 |  0:00:20s
epoch 64 | loss: 0.3332  | val_0_rmse: 0.62964 | val_1_rmse: 0.64868 |  0:00:20s
epoch 65 | loss: 0.34339 | val_0_rmse: 0.59517 | val_1_rmse: 0.62522 |  0:00:21s
epoch 66 | loss: 0.35143 | val_0_rmse: 0.60888 | val_1_rmse: 0.6267  |  0:00:21s
epoch 67 | loss: 0.3467  | val_0_rmse: 0.58872 | val_1_rmse: 0.60702 |  0:00:21s
epoch 68 | loss: 0.35235 | val_0_rmse: 0.59883 | val_1_rmse: 0.60962 |  0:00:22s
epoch 69 | loss: 0.35214 | val_0_rmse: 0.65598 | val_1_rmse: 0.66656 |  0:00:22s
epoch 70 | loss: 0.34891 | val_0_rmse: 0.66416 | val_1_rmse: 0.67535 |  0:00:22s
epoch 71 | loss: 0.35273 | val_0_rmse: 0.61173 | val_1_rmse: 0.61505 |  0:00:23s
epoch 72 | loss: 0.35164 | val_0_rmse: 0.59726 | val_1_rmse: 0.59347 |  0:00:23s
epoch 73 | loss: 0.35329 | val_0_rmse: 0.6058  | val_1_rmse: 0.61466 |  0:00:23s
epoch 74 | loss: 0.35285 | val_0_rmse: 0.60173 | val_1_rmse: 0.61618 |  0:00:24s
epoch 75 | loss: 0.34576 | val_0_rmse: 0.63005 | val_1_rmse: 0.63666 |  0:00:24s
epoch 76 | loss: 0.34365 | val_0_rmse: 0.59035 | val_1_rmse: 0.6053  |  0:00:24s
epoch 77 | loss: 0.35351 | val_0_rmse: 0.60228 | val_1_rmse: 0.61958 |  0:00:24s
epoch 78 | loss: 0.33327 | val_0_rmse: 0.58036 | val_1_rmse: 0.61024 |  0:00:25s
epoch 79 | loss: 0.33749 | val_0_rmse: 0.60195 | val_1_rmse: 0.62672 |  0:00:25s
epoch 80 | loss: 0.33496 | val_0_rmse: 0.58476 | val_1_rmse: 0.60764 |  0:00:25s
epoch 81 | loss: 0.34059 | val_0_rmse: 0.57125 | val_1_rmse: 0.59308 |  0:00:26s
epoch 82 | loss: 0.33779 | val_0_rmse: 0.61739 | val_1_rmse: 0.62698 |  0:00:26s
epoch 83 | loss: 0.34451 | val_0_rmse: 0.60653 | val_1_rmse: 0.62023 |  0:00:26s
epoch 84 | loss: 0.33567 | val_0_rmse: 0.59389 | val_1_rmse: 0.60877 |  0:00:27s
epoch 85 | loss: 0.33925 | val_0_rmse: 0.60776 | val_1_rmse: 0.62495 |  0:00:27s
epoch 86 | loss: 0.33588 | val_0_rmse: 0.59589 | val_1_rmse: 0.61927 |  0:00:27s
epoch 87 | loss: 0.33873 | val_0_rmse: 0.58992 | val_1_rmse: 0.60958 |  0:00:28s
epoch 88 | loss: 0.33069 | val_0_rmse: 0.57389 | val_1_rmse: 0.59855 |  0:00:28s
epoch 89 | loss: 0.32412 | val_0_rmse: 0.58876 | val_1_rmse: 0.60918 |  0:00:28s
epoch 90 | loss: 0.33035 | val_0_rmse: 0.56574 | val_1_rmse: 0.59158 |  0:00:29s
epoch 91 | loss: 0.33252 | val_0_rmse: 0.58307 | val_1_rmse: 0.60943 |  0:00:29s
epoch 92 | loss: 0.326   | val_0_rmse: 0.56804 | val_1_rmse: 0.59738 |  0:00:29s
epoch 93 | loss: 0.33103 | val_0_rmse: 0.61261 | val_1_rmse: 0.63312 |  0:00:30s
epoch 94 | loss: 0.33366 | val_0_rmse: 0.59097 | val_1_rmse: 0.61241 |  0:00:30s
epoch 95 | loss: 0.33138 | val_0_rmse: 0.58081 | val_1_rmse: 0.60378 |  0:00:30s
epoch 96 | loss: 0.33038 | val_0_rmse: 0.5776  | val_1_rmse: 0.5968  |  0:00:31s
epoch 97 | loss: 0.33171 | val_0_rmse: 0.59961 | val_1_rmse: 0.62005 |  0:00:31s
epoch 98 | loss: 0.33915 | val_0_rmse: 0.6095  | val_1_rmse: 0.62737 |  0:00:31s
epoch 99 | loss: 0.3288  | val_0_rmse: 0.59601 | val_1_rmse: 0.61591 |  0:00:31s
epoch 100| loss: 0.32421 | val_0_rmse: 0.62974 | val_1_rmse: 0.64485 |  0:00:32s
epoch 101| loss: 0.32627 | val_0_rmse: 0.63733 | val_1_rmse: 0.66025 |  0:00:32s
epoch 102| loss: 0.32791 | val_0_rmse: 0.58651 | val_1_rmse: 0.61109 |  0:00:32s
epoch 103| loss: 0.32624 | val_0_rmse: 0.59296 | val_1_rmse: 0.60731 |  0:00:33s
epoch 104| loss: 0.33644 | val_0_rmse: 0.6039  | val_1_rmse: 0.62295 |  0:00:33s
epoch 105| loss: 0.34312 | val_0_rmse: 0.60644 | val_1_rmse: 0.61401 |  0:00:33s
epoch 106| loss: 0.3375  | val_0_rmse: 0.59381 | val_1_rmse: 0.61401 |  0:00:34s
epoch 107| loss: 0.34027 | val_0_rmse: 0.5808  | val_1_rmse: 0.61206 |  0:00:34s
epoch 108| loss: 0.33197 | val_0_rmse: 0.58723 | val_1_rmse: 0.60556 |  0:00:34s
epoch 109| loss: 0.33506 | val_0_rmse: 0.57794 | val_1_rmse: 0.60377 |  0:00:35s
epoch 110| loss: 0.32815 | val_0_rmse: 0.5913  | val_1_rmse: 0.61591 |  0:00:35s
epoch 111| loss: 0.33172 | val_0_rmse: 0.59463 | val_1_rmse: 0.61735 |  0:00:35s
epoch 112| loss: 0.32491 | val_0_rmse: 0.62718 | val_1_rmse: 0.63789 |  0:00:36s
epoch 113| loss: 0.36513 | val_0_rmse: 0.62389 | val_1_rmse: 0.64235 |  0:00:36s
epoch 114| loss: 0.34873 | val_0_rmse: 0.64432 | val_1_rmse: 0.65449 |  0:00:36s
epoch 115| loss: 0.35381 | val_0_rmse: 0.62914 | val_1_rmse: 0.64236 |  0:00:37s
epoch 116| loss: 0.34882 | val_0_rmse: 0.60155 | val_1_rmse: 0.6218  |  0:00:37s
epoch 117| loss: 0.33262 | val_0_rmse: 0.60203 | val_1_rmse: 0.62342 |  0:00:37s
epoch 118| loss: 0.33883 | val_0_rmse: 0.6042  | val_1_rmse: 0.63514 |  0:00:38s
epoch 119| loss: 0.32556 | val_0_rmse: 0.62063 | val_1_rmse: 0.64732 |  0:00:38s
epoch 120| loss: 0.32964 | val_0_rmse: 0.60687 | val_1_rmse: 0.63339 |  0:00:38s

Early stopping occured at epoch 120 with best_epoch = 90 and best_val_1_rmse = 0.59158
Best weights from best epoch are automatically used!
ended training at: 03:18:55
Feature importance:
[('Area', 0.32525067477993347), ('Baths', 0.26717985123457383), ('Beds', 0.028703982854229218), ('Latitude', 0.24846080144845017), ('Longitude', 0.09752205289234628), ('Month', 2.8015550214847525e-05), ('Year', 0.0328546212402522)]
Mean squared error is of 2768962813.810314
Mean absolute error:36343.94841635718
MAPE:0.32870168833283675
R2 score:0.6650906493804783
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:18:55
epoch 0  | loss: 0.43423 | val_0_rmse: 0.58809 | val_1_rmse: 0.59197 |  0:00:04s
epoch 1  | loss: 0.33928 | val_0_rmse: 0.5755  | val_1_rmse: 0.57784 |  0:00:08s
epoch 2  | loss: 0.33072 | val_0_rmse: 0.56754 | val_1_rmse: 0.57138 |  0:00:12s
epoch 3  | loss: 0.32738 | val_0_rmse: 0.57717 | val_1_rmse: 0.58283 |  0:00:16s
epoch 4  | loss: 0.32984 | val_0_rmse: 0.57769 | val_1_rmse: 0.58428 |  0:00:21s
epoch 5  | loss: 0.32168 | val_0_rmse: 1.04435 | val_1_rmse: 1.05946 |  0:00:25s
epoch 6  | loss: 0.31653 | val_0_rmse: 0.59609 | val_1_rmse: 0.60167 |  0:00:29s
epoch 7  | loss: 0.30797 | val_0_rmse: 0.54469 | val_1_rmse: 0.54966 |  0:00:33s
epoch 8  | loss: 0.30607 | val_0_rmse: 0.57784 | val_1_rmse: 0.58381 |  0:00:37s
epoch 9  | loss: 0.30539 | val_0_rmse: 0.59138 | val_1_rmse: 0.59825 |  0:00:42s
epoch 10 | loss: 0.30265 | val_0_rmse: 0.5409  | val_1_rmse: 0.5451  |  0:00:46s
epoch 11 | loss: 0.2994  | val_0_rmse: 0.54656 | val_1_rmse: 0.55034 |  0:00:51s
epoch 12 | loss: 0.29961 | val_0_rmse: 0.53802 | val_1_rmse: 0.54271 |  0:00:55s
epoch 13 | loss: 0.29509 | val_0_rmse: 0.53919 | val_1_rmse: 0.54441 |  0:00:59s
epoch 14 | loss: 0.30559 | val_0_rmse: 1.13449 | val_1_rmse: 1.15088 |  0:01:03s
epoch 15 | loss: 0.30565 | val_0_rmse: 0.54703 | val_1_rmse: 0.55283 |  0:01:08s
epoch 16 | loss: 0.29795 | val_0_rmse: 0.54546 | val_1_rmse: 0.55233 |  0:01:12s
epoch 17 | loss: 0.29682 | val_0_rmse: 0.55662 | val_1_rmse: 0.56079 |  0:01:16s
epoch 18 | loss: 0.2974  | val_0_rmse: 0.62197 | val_1_rmse: 0.6286  |  0:01:20s
epoch 19 | loss: 0.297   | val_0_rmse: 0.53652 | val_1_rmse: 0.54182 |  0:01:24s
epoch 20 | loss: 0.30221 | val_0_rmse: 0.54416 | val_1_rmse: 0.54792 |  0:01:29s
epoch 21 | loss: 0.30019 | val_0_rmse: 0.6254  | val_1_rmse: 0.63286 |  0:01:33s
epoch 22 | loss: 0.29841 | val_0_rmse: 0.55305 | val_1_rmse: 0.55951 |  0:01:37s
epoch 23 | loss: 0.30307 | val_0_rmse: 0.59097 | val_1_rmse: 0.59613 |  0:01:41s
epoch 24 | loss: 0.2967  | val_0_rmse: 0.57782 | val_1_rmse: 0.58355 |  0:01:45s
epoch 25 | loss: 0.29409 | val_0_rmse: 0.61204 | val_1_rmse: 0.62106 |  0:01:50s
epoch 26 | loss: 0.29141 | val_0_rmse: 0.56555 | val_1_rmse: 0.57221 |  0:01:54s
epoch 27 | loss: 0.28985 | val_0_rmse: 0.62835 | val_1_rmse: 0.63695 |  0:01:58s
epoch 28 | loss: 0.29005 | val_0_rmse: 0.54635 | val_1_rmse: 0.55246 |  0:02:02s
epoch 29 | loss: 0.29022 | val_0_rmse: 0.55171 | val_1_rmse: 0.55694 |  0:02:07s
epoch 30 | loss: 0.29545 | val_0_rmse: 0.72577 | val_1_rmse: 0.73784 |  0:02:11s
epoch 31 | loss: 0.30877 | val_0_rmse: 0.64566 | val_1_rmse: 0.65129 |  0:02:15s
epoch 32 | loss: 0.31019 | val_0_rmse: 1.20326 | val_1_rmse: 1.22071 |  0:02:19s
epoch 33 | loss: 0.30128 | val_0_rmse: 0.55111 | val_1_rmse: 0.55602 |  0:02:23s
epoch 34 | loss: 0.29979 | val_0_rmse: 0.54878 | val_1_rmse: 0.55479 |  0:02:27s
epoch 35 | loss: 0.29412 | val_0_rmse: 0.58474 | val_1_rmse: 0.59381 |  0:02:32s
epoch 36 | loss: 0.28987 | val_0_rmse: 0.53441 | val_1_rmse: 0.5412  |  0:02:36s
epoch 37 | loss: 0.28879 | val_0_rmse: 0.55935 | val_1_rmse: 0.56466 |  0:02:40s
epoch 38 | loss: 0.28758 | val_0_rmse: 0.52949 | val_1_rmse: 0.53495 |  0:02:44s
epoch 39 | loss: 0.2899  | val_0_rmse: 0.542   | val_1_rmse: 0.5473  |  0:02:48s
epoch 40 | loss: 0.28559 | val_0_rmse: 0.58374 | val_1_rmse: 0.59211 |  0:02:53s
epoch 41 | loss: 0.28857 | val_0_rmse: 0.56439 | val_1_rmse: 0.5707  |  0:02:57s
epoch 42 | loss: 0.28454 | val_0_rmse: 0.53706 | val_1_rmse: 0.54379 |  0:03:01s
epoch 43 | loss: 0.28328 | val_0_rmse: 0.53626 | val_1_rmse: 0.54088 |  0:03:05s
epoch 44 | loss: 0.2878  | val_0_rmse: 0.56099 | val_1_rmse: 0.56578 |  0:03:09s
epoch 45 | loss: 0.28551 | val_0_rmse: 0.60751 | val_1_rmse: 0.61488 |  0:03:13s
epoch 46 | loss: 0.28367 | val_0_rmse: 0.54072 | val_1_rmse: 0.54383 |  0:03:18s
epoch 47 | loss: 0.28493 | val_0_rmse: 0.53069 | val_1_rmse: 0.53469 |  0:03:22s
epoch 48 | loss: 0.2937  | val_0_rmse: 0.62712 | val_1_rmse: 0.63169 |  0:03:26s
epoch 49 | loss: 0.30264 | val_0_rmse: 0.83881 | val_1_rmse: 0.85183 |  0:03:30s
epoch 50 | loss: 0.29116 | val_0_rmse: 0.64148 | val_1_rmse: 0.65171 |  0:03:34s
epoch 51 | loss: 0.28787 | val_0_rmse: 0.53258 | val_1_rmse: 0.5365  |  0:03:38s
epoch 52 | loss: 0.28809 | val_0_rmse: 0.55212 | val_1_rmse: 0.55689 |  0:03:43s
epoch 53 | loss: 0.28513 | val_0_rmse: 0.5252  | val_1_rmse: 0.5298  |  0:03:47s
epoch 54 | loss: 0.29084 | val_0_rmse: 0.62928 | val_1_rmse: 0.63755 |  0:03:51s
epoch 55 | loss: 0.29029 | val_0_rmse: 0.5965  | val_1_rmse: 0.60174 |  0:03:55s
epoch 56 | loss: 0.28721 | val_0_rmse: 0.61892 | val_1_rmse: 0.62565 |  0:03:59s
epoch 57 | loss: 0.28517 | val_0_rmse: 0.56315 | val_1_rmse: 0.56972 |  0:04:04s
epoch 58 | loss: 0.28102 | val_0_rmse: 0.54317 | val_1_rmse: 0.54921 |  0:04:08s
epoch 59 | loss: 0.28426 | val_0_rmse: 0.52653 | val_1_rmse: 0.52971 |  0:04:12s
epoch 60 | loss: 0.28024 | val_0_rmse: 0.51801 | val_1_rmse: 0.52249 |  0:04:16s
epoch 61 | loss: 0.27804 | val_0_rmse: 0.61769 | val_1_rmse: 0.62572 |  0:04:20s
epoch 62 | loss: 0.27764 | val_0_rmse: 0.57671 | val_1_rmse: 0.58453 |  0:04:25s
epoch 63 | loss: 0.27903 | val_0_rmse: 0.51497 | val_1_rmse: 0.521   |  0:04:29s
epoch 64 | loss: 0.27768 | val_0_rmse: 0.51615 | val_1_rmse: 0.52142 |  0:04:33s
epoch 65 | loss: 0.27443 | val_0_rmse: 0.5515  | val_1_rmse: 0.55686 |  0:04:37s
epoch 66 | loss: 0.27475 | val_0_rmse: 0.59244 | val_1_rmse: 0.59926 |  0:04:41s
epoch 67 | loss: 0.27396 | val_0_rmse: 0.54795 | val_1_rmse: 0.55296 |  0:04:45s
epoch 68 | loss: 0.2752  | val_0_rmse: 0.67776 | val_1_rmse: 0.68623 |  0:04:50s
epoch 69 | loss: 0.27339 | val_0_rmse: 0.55915 | val_1_rmse: 0.56525 |  0:04:54s
epoch 70 | loss: 0.2736  | val_0_rmse: 0.52496 | val_1_rmse: 0.52988 |  0:04:58s
epoch 71 | loss: 0.27298 | val_0_rmse: 0.51981 | val_1_rmse: 0.52681 |  0:05:02s
epoch 72 | loss: 0.27318 | val_0_rmse: 0.51751 | val_1_rmse: 0.52487 |  0:05:07s
epoch 73 | loss: 0.27296 | val_0_rmse: 0.63146 | val_1_rmse: 0.64008 |  0:05:11s
epoch 74 | loss: 0.2712  | val_0_rmse: 0.5167  | val_1_rmse: 0.52047 |  0:05:15s
epoch 75 | loss: 0.27109 | val_0_rmse: 0.60731 | val_1_rmse: 0.61606 |  0:05:19s
epoch 76 | loss: 0.27308 | val_0_rmse: 0.54777 | val_1_rmse: 0.55456 |  0:05:23s
epoch 77 | loss: 0.26987 | val_0_rmse: 0.70868 | val_1_rmse: 0.71637 |  0:05:27s
epoch 78 | loss: 0.27109 | val_0_rmse: 0.53878 | val_1_rmse: 0.54377 |  0:05:32s
epoch 79 | loss: 0.27238 | val_0_rmse: 0.61222 | val_1_rmse: 0.62068 |  0:05:36s
epoch 80 | loss: 0.26973 | val_0_rmse: 0.52893 | val_1_rmse: 0.53539 |  0:05:40s
epoch 81 | loss: 0.26828 | val_0_rmse: 0.53814 | val_1_rmse: 0.54475 |  0:05:44s
epoch 82 | loss: 0.30988 | val_0_rmse: 0.57034 | val_1_rmse: 0.57547 |  0:05:48s
epoch 83 | loss: 0.32176 | val_0_rmse: 0.55567 | val_1_rmse: 0.55953 |  0:05:53s
epoch 84 | loss: 0.31933 | val_0_rmse: 0.55492 | val_1_rmse: 0.55912 |  0:05:57s
epoch 85 | loss: 0.31416 | val_0_rmse: 0.55477 | val_1_rmse: 0.55965 |  0:06:01s
epoch 86 | loss: 0.31139 | val_0_rmse: 0.55932 | val_1_rmse: 0.56282 |  0:06:05s
epoch 87 | loss: 0.31183 | val_0_rmse: 0.552   | val_1_rmse: 0.55652 |  0:06:09s
epoch 88 | loss: 0.31045 | val_0_rmse: 0.55053 | val_1_rmse: 0.55358 |  0:06:14s
epoch 89 | loss: 0.31186 | val_0_rmse: 0.56218 | val_1_rmse: 0.56762 |  0:06:18s
epoch 90 | loss: 0.31234 | val_0_rmse: 0.55175 | val_1_rmse: 0.55618 |  0:06:22s
epoch 91 | loss: 0.30754 | val_0_rmse: 0.54994 | val_1_rmse: 0.55577 |  0:06:26s
epoch 92 | loss: 0.30751 | val_0_rmse: 0.57562 | val_1_rmse: 0.57703 |  0:06:30s
epoch 93 | loss: 0.30334 | val_0_rmse: 0.53199 | val_1_rmse: 0.53606 |  0:06:34s
epoch 94 | loss: 0.3042  | val_0_rmse: 0.53841 | val_1_rmse: 0.54211 |  0:06:39s
epoch 95 | loss: 0.29764 | val_0_rmse: 0.55134 | val_1_rmse: 0.5564  |  0:06:43s
epoch 96 | loss: 0.29169 | val_0_rmse: 0.55287 | val_1_rmse: 0.55732 |  0:06:47s
epoch 97 | loss: 0.28725 | val_0_rmse: 0.61311 | val_1_rmse: 0.62081 |  0:06:51s
epoch 98 | loss: 0.28856 | val_0_rmse: 0.59004 | val_1_rmse: 0.59658 |  0:06:55s
epoch 99 | loss: 0.28667 | val_0_rmse: 0.59876 | val_1_rmse: 0.60469 |  0:07:00s
epoch 100| loss: 0.28557 | val_0_rmse: 0.60366 | val_1_rmse: 0.60975 |  0:07:04s
epoch 101| loss: 0.2825  | val_0_rmse: 0.56679 | val_1_rmse: 0.56903 |  0:07:08s
epoch 102| loss: 0.2849  | val_0_rmse: 0.56672 | val_1_rmse: 0.57376 |  0:07:12s
epoch 103| loss: 0.28056 | val_0_rmse: 0.54626 | val_1_rmse: 0.55279 |  0:07:16s
epoch 104| loss: 0.27999 | val_0_rmse: 0.5666  | val_1_rmse: 0.57277 |  0:07:21s

Early stopping occured at epoch 104 with best_epoch = 74 and best_val_1_rmse = 0.52047
Best weights from best epoch are automatically used!
ended training at: 03:26:17
Feature importance:
[('Area', 0.6610416920958998), ('Baths', 0.0), ('Beds', 0.1670152644524413), ('Latitude', 0.15547737629219835), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.016465667159460524)]
Mean squared error is of 917682357.9926834
Mean absolute error:20771.779040756646
MAPE:0.346269232953205
R2 score:0.7206226665161259
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:26:18
epoch 0  | loss: 0.42245 | val_0_rmse: 0.57924 | val_1_rmse: 0.59924 |  0:00:04s
epoch 1  | loss: 0.33411 | val_0_rmse: 0.56671 | val_1_rmse: 0.57448 |  0:00:08s
epoch 2  | loss: 0.33212 | val_0_rmse: 0.56679 | val_1_rmse: 0.5736  |  0:00:12s
epoch 3  | loss: 0.32798 | val_0_rmse: 0.57001 | val_1_rmse: 0.57744 |  0:00:16s
epoch 4  | loss: 0.32673 | val_0_rmse: 0.56905 | val_1_rmse: 0.5758  |  0:00:20s
epoch 5  | loss: 0.32414 | val_0_rmse: 0.55733 | val_1_rmse: 0.5641  |  0:00:25s
epoch 6  | loss: 0.31995 | val_0_rmse: 0.56055 | val_1_rmse: 0.5754  |  0:00:29s
epoch 7  | loss: 0.31441 | val_0_rmse: 0.5707  | val_1_rmse: 0.63939 |  0:00:33s
epoch 8  | loss: 0.31134 | val_0_rmse: 0.55404 | val_1_rmse: 0.66817 |  0:00:37s
epoch 9  | loss: 0.30485 | val_0_rmse: 0.55032 | val_1_rmse: 0.79236 |  0:00:41s
epoch 10 | loss: 0.3031  | val_0_rmse: 0.54321 | val_1_rmse: 0.71924 |  0:00:45s
epoch 11 | loss: 0.29792 | val_0_rmse: 0.69152 | val_1_rmse: 0.75853 |  0:00:50s
epoch 12 | loss: 0.29088 | val_0_rmse: 0.5571  | val_1_rmse: 0.56298 |  0:00:54s
epoch 13 | loss: 0.29125 | val_0_rmse: 0.53371 | val_1_rmse: 0.54022 |  0:00:58s
epoch 14 | loss: 0.29635 | val_0_rmse: 0.53325 | val_1_rmse: 0.53893 |  0:01:02s
epoch 15 | loss: 0.29107 | val_0_rmse: 0.72982 | val_1_rmse: 0.72317 |  0:01:06s
epoch 16 | loss: 0.28829 | val_0_rmse: 0.54738 | val_1_rmse: 0.54988 |  0:01:11s
epoch 17 | loss: 0.28938 | val_0_rmse: 0.54502 | val_1_rmse: 0.55242 |  0:01:15s
epoch 18 | loss: 0.28634 | val_0_rmse: 0.5865  | val_1_rmse: 0.58452 |  0:01:19s
epoch 19 | loss: 0.29215 | val_0_rmse: 0.55604 | val_1_rmse: 0.56287 |  0:01:23s
epoch 20 | loss: 0.28632 | val_0_rmse: 0.55853 | val_1_rmse: 0.57223 |  0:01:27s
epoch 21 | loss: 0.28792 | val_0_rmse: 0.53439 | val_1_rmse: 0.53654 |  0:01:32s
epoch 22 | loss: 0.29313 | val_0_rmse: 0.5364  | val_1_rmse: 0.53956 |  0:01:36s
epoch 23 | loss: 0.28987 | val_0_rmse: 0.5299  | val_1_rmse: 0.53574 |  0:01:40s
epoch 24 | loss: 0.28961 | val_0_rmse: 0.57917 | val_1_rmse: 0.58675 |  0:01:44s
epoch 25 | loss: 0.2896  | val_0_rmse: 0.60967 | val_1_rmse: 0.61734 |  0:01:48s
epoch 26 | loss: 0.28656 | val_0_rmse: 0.57014 | val_1_rmse: 0.57611 |  0:01:53s
epoch 27 | loss: 0.28366 | val_0_rmse: 0.64217 | val_1_rmse: 0.63868 |  0:01:57s
epoch 28 | loss: 0.28668 | val_0_rmse: 0.53514 | val_1_rmse: 0.54189 |  0:02:01s
epoch 29 | loss: 0.28096 | val_0_rmse: 0.53729 | val_1_rmse: 0.54011 |  0:02:05s
epoch 30 | loss: 0.28398 | val_0_rmse: 0.5803  | val_1_rmse: 0.58098 |  0:02:09s
epoch 31 | loss: 0.28078 | val_0_rmse: 0.53328 | val_1_rmse: 0.53419 |  0:02:14s
epoch 32 | loss: 0.279   | val_0_rmse: 0.56751 | val_1_rmse: 0.57512 |  0:02:18s
epoch 33 | loss: 0.2802  | val_0_rmse: 0.53673 | val_1_rmse: 0.54221 |  0:02:22s
epoch 34 | loss: 0.28004 | val_0_rmse: 0.61423 | val_1_rmse: 0.62276 |  0:02:26s
epoch 35 | loss: 0.28125 | val_0_rmse: 0.52325 | val_1_rmse: 0.52663 |  0:02:30s
epoch 36 | loss: 0.27958 | val_0_rmse: 0.52795 | val_1_rmse: 0.53389 |  0:02:35s
epoch 37 | loss: 0.27772 | val_0_rmse: 0.56869 | val_1_rmse: 0.57695 |  0:02:39s
epoch 38 | loss: 0.27619 | val_0_rmse: 0.54456 | val_1_rmse: 0.54831 |  0:02:43s
epoch 39 | loss: 0.27517 | val_0_rmse: 0.52839 | val_1_rmse: 0.53244 |  0:02:47s
epoch 40 | loss: 0.27713 | val_0_rmse: 0.57354 | val_1_rmse: 0.58141 |  0:02:51s
epoch 41 | loss: 0.27486 | val_0_rmse: 0.52739 | val_1_rmse: 0.53137 |  0:02:55s
epoch 42 | loss: 0.27681 | val_0_rmse: 0.56069 | val_1_rmse: 0.56664 |  0:03:00s
epoch 43 | loss: 0.27523 | val_0_rmse: 0.56702 | val_1_rmse: 0.57353 |  0:03:04s
epoch 44 | loss: 0.27591 | val_0_rmse: 0.53895 | val_1_rmse: 0.54445 |  0:03:08s
epoch 45 | loss: 0.2752  | val_0_rmse: 0.54911 | val_1_rmse: 0.55768 |  0:03:12s
epoch 46 | loss: 0.27411 | val_0_rmse: 0.56569 | val_1_rmse: 0.57372 |  0:03:17s
epoch 47 | loss: 0.27408 | val_0_rmse: 0.57887 | val_1_rmse: 0.58489 |  0:03:21s
epoch 48 | loss: 0.27339 | val_0_rmse: 0.5237  | val_1_rmse: 0.52847 |  0:03:25s
epoch 49 | loss: 0.27122 | val_0_rmse: 0.5373  | val_1_rmse: 0.54436 |  0:03:29s
epoch 50 | loss: 0.27106 | val_0_rmse: 0.67024 | val_1_rmse: 0.66604 |  0:03:33s
epoch 51 | loss: 0.27109 | val_0_rmse: 0.52789 | val_1_rmse: 0.53343 |  0:03:38s
epoch 52 | loss: 0.27237 | val_0_rmse: 0.57913 | val_1_rmse: 0.58527 |  0:03:42s
epoch 53 | loss: 0.27024 | val_0_rmse: 0.54139 | val_1_rmse: 0.54441 |  0:03:46s
epoch 54 | loss: 0.2726  | val_0_rmse: 0.51427 | val_1_rmse: 0.52009 |  0:03:51s
epoch 55 | loss: 0.27198 | val_0_rmse: 0.55984 | val_1_rmse: 0.56709 |  0:03:55s
epoch 56 | loss: 0.26937 | val_0_rmse: 0.55963 | val_1_rmse: 0.56618 |  0:03:59s
epoch 57 | loss: 0.2726  | val_0_rmse: 0.52023 | val_1_rmse: 0.52502 |  0:04:03s
epoch 58 | loss: 0.27009 | val_0_rmse: 0.61714 | val_1_rmse: 0.62657 |  0:04:07s
epoch 59 | loss: 0.27234 | val_0_rmse: 0.58606 | val_1_rmse: 0.59351 |  0:04:12s
epoch 60 | loss: 0.271   | val_0_rmse: 0.52168 | val_1_rmse: 0.5271  |  0:04:16s
epoch 61 | loss: 0.27685 | val_0_rmse: 0.61193 | val_1_rmse: 0.62201 |  0:04:20s
epoch 62 | loss: 0.27447 | val_0_rmse: 0.53658 | val_1_rmse: 0.54332 |  0:04:24s
epoch 63 | loss: 0.27075 | val_0_rmse: 0.62203 | val_1_rmse: 0.62923 |  0:04:28s
epoch 64 | loss: 0.27044 | val_0_rmse: 0.55535 | val_1_rmse: 0.56172 |  0:04:33s
epoch 65 | loss: 0.27284 | val_0_rmse: 0.63343 | val_1_rmse: 0.63988 |  0:04:37s
epoch 66 | loss: 0.27623 | val_0_rmse: 0.53443 | val_1_rmse: 0.53874 |  0:04:41s
epoch 67 | loss: 0.27069 | val_0_rmse: 0.57995 | val_1_rmse: 0.58831 |  0:04:45s
epoch 68 | loss: 0.27032 | val_0_rmse: 0.61631 | val_1_rmse: 0.62433 |  0:04:49s
epoch 69 | loss: 0.26855 | val_0_rmse: 0.52055 | val_1_rmse: 0.52819 |  0:04:54s
epoch 70 | loss: 0.26974 | val_0_rmse: 0.97264 | val_1_rmse: 0.96019 |  0:04:58s
epoch 71 | loss: 0.26912 | val_0_rmse: 0.56818 | val_1_rmse: 0.57707 |  0:05:02s
epoch 72 | loss: 0.26752 | val_0_rmse: 0.56891 | val_1_rmse: 0.57753 |  0:05:06s
epoch 73 | loss: 0.26762 | val_0_rmse: 0.58992 | val_1_rmse: 0.59838 |  0:05:10s
epoch 74 | loss: 0.26805 | val_0_rmse: 0.51418 | val_1_rmse: 0.52004 |  0:05:15s
epoch 75 | loss: 0.26917 | val_0_rmse: 0.58843 | val_1_rmse: 0.59654 |  0:05:19s
epoch 76 | loss: 0.26677 | val_0_rmse: 0.99552 | val_1_rmse: 0.98113 |  0:05:23s
epoch 77 | loss: 0.27003 | val_0_rmse: 0.57003 | val_1_rmse: 0.57699 |  0:05:27s
epoch 78 | loss: 0.26434 | val_0_rmse: 0.61997 | val_1_rmse: 0.62866 |  0:05:32s
epoch 79 | loss: 0.26552 | val_0_rmse: 0.73799 | val_1_rmse: 0.73756 |  0:05:36s
epoch 80 | loss: 0.26683 | val_0_rmse: 0.52551 | val_1_rmse: 0.53262 |  0:05:40s
epoch 81 | loss: 0.26484 | val_0_rmse: 0.63787 | val_1_rmse: 0.64686 |  0:05:44s
epoch 82 | loss: 0.26811 | val_0_rmse: 0.56335 | val_1_rmse: 0.62908 |  0:05:48s
epoch 83 | loss: 0.26351 | val_0_rmse: 0.54727 | val_1_rmse: 0.59929 |  0:05:53s
epoch 84 | loss: 0.26322 | val_0_rmse: 0.52923 | val_1_rmse: 0.56973 |  0:05:57s
epoch 85 | loss: 0.26118 | val_0_rmse: 0.5826  | val_1_rmse: 0.58451 |  0:06:01s
epoch 86 | loss: 0.26247 | val_0_rmse: 0.51755 | val_1_rmse: 0.51707 |  0:06:05s
epoch 87 | loss: 0.26392 | val_0_rmse: 0.56948 | val_1_rmse: 0.58187 |  0:06:09s
epoch 88 | loss: 0.2643  | val_0_rmse: 0.5388  | val_1_rmse: 0.54613 |  0:06:14s
epoch 89 | loss: 0.26119 | val_0_rmse: 0.6274  | val_1_rmse: 0.63723 |  0:06:18s
epoch 90 | loss: 0.25996 | val_0_rmse: 0.60613 | val_1_rmse: 0.63518 |  0:06:22s
epoch 91 | loss: 0.26474 | val_0_rmse: 0.55011 | val_1_rmse: 0.57765 |  0:06:26s
epoch 92 | loss: 0.26115 | val_0_rmse: 0.53085 | val_1_rmse: 0.57064 |  0:06:30s
epoch 93 | loss: 0.26223 | val_0_rmse: 0.55026 | val_1_rmse: 0.59834 |  0:06:35s
epoch 94 | loss: 0.25835 | val_0_rmse: 0.56781 | val_1_rmse: 0.69114 |  0:06:39s
epoch 95 | loss: 0.25782 | val_0_rmse: 0.56963 | val_1_rmse: 0.69491 |  0:06:43s
epoch 96 | loss: 0.26098 | val_0_rmse: 0.5682  | val_1_rmse: 0.57624 |  0:06:47s
epoch 97 | loss: 0.26534 | val_0_rmse: 0.55508 | val_1_rmse: 0.55501 |  0:06:51s
epoch 98 | loss: 0.26697 | val_0_rmse: 0.61535 | val_1_rmse: 0.62423 |  0:06:56s
epoch 99 | loss: 0.25606 | val_0_rmse: 0.50405 | val_1_rmse: 0.50749 |  0:07:00s
epoch 100| loss: 0.25636 | val_0_rmse: 0.57898 | val_1_rmse: 0.58634 |  0:07:04s
epoch 101| loss: 0.25501 | val_0_rmse: 0.62457 | val_1_rmse: 0.62945 |  0:07:08s
epoch 102| loss: 0.25548 | val_0_rmse: 0.5846  | val_1_rmse: 0.583   |  0:07:13s
epoch 103| loss: 0.25612 | val_0_rmse: 0.5246  | val_1_rmse: 0.52689 |  0:07:17s
epoch 104| loss: 0.25602 | val_0_rmse: 0.53473 | val_1_rmse: 0.5398  |  0:07:21s
epoch 105| loss: 0.25771 | val_0_rmse: 1.2051  | val_1_rmse: 1.1887  |  0:07:25s
epoch 106| loss: 0.26217 | val_0_rmse: 0.53132 | val_1_rmse: 0.53687 |  0:07:29s
epoch 107| loss: 0.26016 | val_0_rmse: 0.56148 | val_1_rmse: 0.57097 |  0:07:34s
epoch 108| loss: 0.25754 | val_0_rmse: 0.53855 | val_1_rmse: 0.54383 |  0:07:38s
epoch 109| loss: 0.2548  | val_0_rmse: 0.64677 | val_1_rmse: 0.64645 |  0:07:42s
epoch 110| loss: 0.25503 | val_0_rmse: 0.53686 | val_1_rmse: 0.54107 |  0:07:46s
epoch 111| loss: 0.25322 | val_0_rmse: 0.60294 | val_1_rmse: 0.59975 |  0:07:50s
epoch 112| loss: 0.25315 | val_0_rmse: 0.57225 | val_1_rmse: 0.57988 |  0:07:55s
epoch 113| loss: 0.25544 | val_0_rmse: 0.6216  | val_1_rmse: 0.62572 |  0:07:59s
epoch 114| loss: 0.25739 | val_0_rmse: 0.56079 | val_1_rmse: 0.56869 |  0:08:03s
epoch 115| loss: 0.2539  | val_0_rmse: 0.54751 | val_1_rmse: 0.55537 |  0:08:07s
epoch 116| loss: 0.25308 | val_0_rmse: 0.56837 | val_1_rmse: 0.57601 |  0:08:12s
epoch 117| loss: 0.25457 | val_0_rmse: 0.52763 | val_1_rmse: 0.53181 |  0:08:16s
epoch 118| loss: 0.25136 | val_0_rmse: 0.52151 | val_1_rmse: 0.53    |  0:08:20s
epoch 119| loss: 0.25154 | val_0_rmse: 0.65137 | val_1_rmse: 0.65974 |  0:08:24s
epoch 120| loss: 0.25366 | val_0_rmse: 0.58188 | val_1_rmse: 0.58972 |  0:08:29s
epoch 121| loss: 0.25416 | val_0_rmse: 0.6544  | val_1_rmse: 0.66263 |  0:08:33s
epoch 122| loss: 0.25309 | val_0_rmse: 0.51607 | val_1_rmse: 0.52245 |  0:08:37s
epoch 123| loss: 0.25111 | val_0_rmse: 0.6785  | val_1_rmse: 0.68621 |  0:08:41s
epoch 124| loss: 0.25375 | val_0_rmse: 0.63481 | val_1_rmse: 0.7072  |  0:08:45s
epoch 125| loss: 0.25624 | val_0_rmse: 0.56292 | val_1_rmse: 0.56772 |  0:08:49s
epoch 126| loss: 0.25093 | val_0_rmse: 0.55616 | val_1_rmse: 0.62395 |  0:08:54s
epoch 127| loss: 0.24926 | val_0_rmse: 0.5946  | val_1_rmse: 0.64999 |  0:08:58s
epoch 128| loss: 0.25301 | val_0_rmse: 0.57247 | val_1_rmse: 0.64587 |  0:09:02s
epoch 129| loss: 0.25126 | val_0_rmse: 0.66978 | val_1_rmse: 0.67977 |  0:09:06s

Early stopping occured at epoch 129 with best_epoch = 99 and best_val_1_rmse = 0.50749
Best weights from best epoch are automatically used!
ended training at: 03:35:26
Feature importance:
[('Area', 0.4613269105895326), ('Baths', 0.0), ('Beds', 0.23446409899204165), ('Latitude', 0.10350045464105176), ('Longitude', 0.10339149502298976), ('Month', 0.03517298795006792), ('Year', 0.06214405280431627)]
Mean squared error is of 869509747.2043715
Mean absolute error:20146.235040783027
MAPE:0.3243422145590969
R2 score:0.7393968616386816
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:35:26
epoch 0  | loss: 0.42581 | val_0_rmse: 0.57477 | val_1_rmse: 0.5784  |  0:00:04s
epoch 1  | loss: 0.33652 | val_0_rmse: 0.56561 | val_1_rmse: 0.56923 |  0:00:08s
epoch 2  | loss: 0.33029 | val_0_rmse: 0.57048 | val_1_rmse: 0.57396 |  0:00:12s
epoch 3  | loss: 0.32803 | val_0_rmse: 0.56336 | val_1_rmse: 0.56691 |  0:00:16s
epoch 4  | loss: 0.3263  | val_0_rmse: 0.56505 | val_1_rmse: 0.56988 |  0:00:21s
epoch 5  | loss: 0.32254 | val_0_rmse: 0.56348 | val_1_rmse: 0.56612 |  0:00:25s
epoch 6  | loss: 0.32226 | val_0_rmse: 0.56619 | val_1_rmse: 0.56954 |  0:00:29s
epoch 7  | loss: 0.31946 | val_0_rmse: 0.55908 | val_1_rmse: 0.56391 |  0:00:33s
epoch 8  | loss: 0.32135 | val_0_rmse: 0.56118 | val_1_rmse: 0.56546 |  0:00:37s
epoch 9  | loss: 0.31979 | val_0_rmse: 0.55665 | val_1_rmse: 0.56105 |  0:00:42s
epoch 10 | loss: 0.31687 | val_0_rmse: 0.55864 | val_1_rmse: 0.56301 |  0:00:46s
epoch 11 | loss: 0.32068 | val_0_rmse: 0.56152 | val_1_rmse: 0.56677 |  0:00:50s
epoch 12 | loss: 0.32001 | val_0_rmse: 0.55932 | val_1_rmse: 0.56349 |  0:00:54s
epoch 13 | loss: 0.31428 | val_0_rmse: 0.55881 | val_1_rmse: 0.56327 |  0:00:58s
epoch 14 | loss: 0.31069 | val_0_rmse: 0.55084 | val_1_rmse: 0.5548  |  0:01:03s
epoch 15 | loss: 0.31486 | val_0_rmse: 0.558   | val_1_rmse: 0.56199 |  0:01:07s
epoch 16 | loss: 0.31671 | val_0_rmse: 0.57794 | val_1_rmse: 0.58062 |  0:01:11s
epoch 17 | loss: 0.3097  | val_0_rmse: 0.55522 | val_1_rmse: 0.55976 |  0:01:15s
epoch 18 | loss: 0.30428 | val_0_rmse: 0.5464  | val_1_rmse: 0.55225 |  0:01:19s
epoch 19 | loss: 0.30044 | val_0_rmse: 0.5798  | val_1_rmse: 0.58308 |  0:01:24s
epoch 20 | loss: 0.30069 | val_0_rmse: 0.55864 | val_1_rmse: 0.56192 |  0:01:28s
epoch 21 | loss: 0.30242 | val_0_rmse: 0.55651 | val_1_rmse: 0.56118 |  0:01:32s
epoch 22 | loss: 0.30269 | val_0_rmse: 0.56355 | val_1_rmse: 0.56813 |  0:01:36s
epoch 23 | loss: 0.29822 | val_0_rmse: 0.58727 | val_1_rmse: 0.58911 |  0:01:40s
epoch 24 | loss: 0.2961  | val_0_rmse: 0.5543  | val_1_rmse: 0.55938 |  0:01:45s
epoch 25 | loss: 0.29744 | val_0_rmse: 0.55782 | val_1_rmse: 0.56213 |  0:01:49s
epoch 26 | loss: 0.29943 | val_0_rmse: 0.57769 | val_1_rmse: 0.58137 |  0:01:53s
epoch 27 | loss: 0.2971  | val_0_rmse: 0.59025 | val_1_rmse: 0.59491 |  0:01:57s
epoch 28 | loss: 0.2963  | val_0_rmse: 0.54874 | val_1_rmse: 0.55369 |  0:02:01s
epoch 29 | loss: 0.29436 | val_0_rmse: 0.53786 | val_1_rmse: 0.54179 |  0:02:06s
epoch 30 | loss: 0.29474 | val_0_rmse: 0.57017 | val_1_rmse: 0.57264 |  0:02:10s
epoch 31 | loss: 0.29432 | val_0_rmse: 0.6321  | val_1_rmse: 0.63344 |  0:02:14s
epoch 32 | loss: 0.29389 | val_0_rmse: 0.54756 | val_1_rmse: 0.55275 |  0:02:18s
epoch 33 | loss: 0.29334 | val_0_rmse: 0.5376  | val_1_rmse: 0.54234 |  0:02:23s
epoch 34 | loss: 0.29258 | val_0_rmse: 0.55476 | val_1_rmse: 0.56105 |  0:02:27s
epoch 35 | loss: 0.29158 | val_0_rmse: 0.5387  | val_1_rmse: 0.54434 |  0:02:31s
epoch 36 | loss: 0.29294 | val_0_rmse: 0.54449 | val_1_rmse: 0.54893 |  0:02:35s
epoch 37 | loss: 0.29211 | val_0_rmse: 0.57614 | val_1_rmse: 0.58019 |  0:02:39s
epoch 38 | loss: 0.28916 | val_0_rmse: 0.55363 | val_1_rmse: 0.55865 |  0:02:44s
epoch 39 | loss: 0.28797 | val_0_rmse: 0.56157 | val_1_rmse: 0.56454 |  0:02:48s
epoch 40 | loss: 0.28703 | val_0_rmse: 0.5395  | val_1_rmse: 0.54435 |  0:02:52s
epoch 41 | loss: 0.28702 | val_0_rmse: 0.53387 | val_1_rmse: 0.53724 |  0:02:56s
epoch 42 | loss: 0.28879 | val_0_rmse: 0.63181 | val_1_rmse: 0.63723 |  0:03:00s
epoch 43 | loss: 0.28758 | val_0_rmse: 0.5968  | val_1_rmse: 0.60009 |  0:03:05s
epoch 44 | loss: 0.2873  | val_0_rmse: 0.59952 | val_1_rmse: 0.60301 |  0:03:09s
epoch 45 | loss: 0.28467 | val_0_rmse: 0.56708 | val_1_rmse: 0.57291 |  0:03:13s
epoch 46 | loss: 0.28717 | val_0_rmse: 0.54102 | val_1_rmse: 0.54591 |  0:03:17s
epoch 47 | loss: 0.28669 | val_0_rmse: 0.81749 | val_1_rmse: 0.81819 |  0:03:21s
epoch 48 | loss: 0.28524 | val_0_rmse: 0.61612 | val_1_rmse: 0.62079 |  0:03:26s
epoch 49 | loss: 0.2822  | val_0_rmse: 0.53275 | val_1_rmse: 0.53749 |  0:03:30s
epoch 50 | loss: 0.28044 | val_0_rmse: 0.54378 | val_1_rmse: 0.54959 |  0:03:34s
epoch 51 | loss: 0.27158 | val_0_rmse: 0.57031 | val_1_rmse: 0.57248 |  0:03:38s
epoch 52 | loss: 0.27067 | val_0_rmse: 0.53336 | val_1_rmse: 0.53964 |  0:03:42s
epoch 53 | loss: 0.26998 | val_0_rmse: 0.61118 | val_1_rmse: 0.61461 |  0:03:47s
epoch 54 | loss: 0.2714  | val_0_rmse: 0.54253 | val_1_rmse: 0.54607 |  0:03:51s
epoch 55 | loss: 0.26741 | val_0_rmse: 0.60713 | val_1_rmse: 0.61034 |  0:03:55s
epoch 56 | loss: 0.26612 | val_0_rmse: 0.50904 | val_1_rmse: 0.51678 |  0:03:59s
epoch 57 | loss: 0.26367 | val_0_rmse: 0.54802 | val_1_rmse: 0.55304 |  0:04:03s
epoch 58 | loss: 0.26832 | val_0_rmse: 0.52731 | val_1_rmse: 0.53417 |  0:04:08s
epoch 59 | loss: 0.26359 | val_0_rmse: 0.51263 | val_1_rmse: 0.51846 |  0:04:12s
epoch 60 | loss: 0.26305 | val_0_rmse: 0.57584 | val_1_rmse: 0.57978 |  0:04:16s
epoch 61 | loss: 0.26425 | val_0_rmse: 0.53911 | val_1_rmse: 0.54387 |  0:04:20s
epoch 62 | loss: 0.26536 | val_0_rmse: 0.51981 | val_1_rmse: 0.52591 |  0:04:25s
epoch 63 | loss: 0.2641  | val_0_rmse: 0.52695 | val_1_rmse: 0.53288 |  0:04:29s
epoch 64 | loss: 0.26049 | val_0_rmse: 0.51524 | val_1_rmse: 0.52322 |  0:04:33s
epoch 65 | loss: 0.26274 | val_0_rmse: 0.53817 | val_1_rmse: 0.5454  |  0:04:37s
epoch 66 | loss: 0.26272 | val_0_rmse: 0.51915 | val_1_rmse: 0.52697 |  0:04:41s
epoch 67 | loss: 0.26212 | val_0_rmse: 0.55924 | val_1_rmse: 0.56533 |  0:04:46s
epoch 68 | loss: 0.26112 | val_0_rmse: 0.52614 | val_1_rmse: 0.53582 |  0:04:50s
epoch 69 | loss: 0.26121 | val_0_rmse: 0.56666 | val_1_rmse: 0.56999 |  0:04:54s
epoch 70 | loss: 0.26069 | val_0_rmse: 0.53399 | val_1_rmse: 0.53869 |  0:04:58s
epoch 71 | loss: 0.25853 | val_0_rmse: 0.53707 | val_1_rmse: 0.54285 |  0:05:02s
epoch 72 | loss: 0.25838 | val_0_rmse: 0.56189 | val_1_rmse: 0.56549 |  0:05:07s
epoch 73 | loss: 0.2622  | val_0_rmse: 0.54105 | val_1_rmse: 0.54624 |  0:05:11s
epoch 74 | loss: 0.25742 | val_0_rmse: 0.52831 | val_1_rmse: 0.53634 |  0:05:15s
epoch 75 | loss: 0.25812 | val_0_rmse: 0.51002 | val_1_rmse: 0.5169  |  0:05:19s
epoch 76 | loss: 0.25909 | val_0_rmse: 0.53869 | val_1_rmse: 0.54404 |  0:05:23s
epoch 77 | loss: 0.25674 | val_0_rmse: 0.53746 | val_1_rmse: 0.54301 |  0:05:28s
epoch 78 | loss: 0.25692 | val_0_rmse: 0.60166 | val_1_rmse: 0.60253 |  0:05:32s
epoch 79 | loss: 0.25765 | val_0_rmse: 0.53512 | val_1_rmse: 0.54187 |  0:05:36s
epoch 80 | loss: 0.25716 | val_0_rmse: 0.54648 | val_1_rmse: 0.55277 |  0:05:40s
epoch 81 | loss: 0.25641 | val_0_rmse: 0.52304 | val_1_rmse: 0.52925 |  0:05:44s
epoch 82 | loss: 0.25778 | val_0_rmse: 0.57379 | val_1_rmse: 0.57807 |  0:05:49s
epoch 83 | loss: 0.25719 | val_0_rmse: 0.5699  | val_1_rmse: 0.57438 |  0:05:53s
epoch 84 | loss: 0.2582  | val_0_rmse: 0.56808 | val_1_rmse: 0.57264 |  0:05:57s
epoch 85 | loss: 0.25922 | val_0_rmse: 0.50276 | val_1_rmse: 0.50889 |  0:06:01s
epoch 86 | loss: 0.25717 | val_0_rmse: 0.63067 | val_1_rmse: 0.63358 |  0:06:06s
epoch 87 | loss: 0.25837 | val_0_rmse: 0.49891 | val_1_rmse: 0.50492 |  0:06:10s
epoch 88 | loss: 0.25409 | val_0_rmse: 0.59656 | val_1_rmse: 0.5983  |  0:06:14s
epoch 89 | loss: 0.25578 | val_0_rmse: 0.59234 | val_1_rmse: 0.59421 |  0:06:18s
epoch 90 | loss: 0.25393 | val_0_rmse: 0.56722 | val_1_rmse: 0.57238 |  0:06:22s
epoch 91 | loss: 0.2689  | val_0_rmse: 0.60323 | val_1_rmse: 0.60794 |  0:06:27s
epoch 92 | loss: 0.29765 | val_0_rmse: 0.58167 | val_1_rmse: 0.5841  |  0:06:31s
epoch 93 | loss: 0.2778  | val_0_rmse: 1.63019 | val_1_rmse: 1.60846 |  0:06:35s
epoch 94 | loss: 0.26661 | val_0_rmse: 0.56894 | val_1_rmse: 0.57388 |  0:06:39s
epoch 95 | loss: 0.2638  | val_0_rmse: 0.55098 | val_1_rmse: 0.55492 |  0:06:43s
epoch 96 | loss: 0.26058 | val_0_rmse: 0.5457  | val_1_rmse: 0.55204 |  0:06:48s
epoch 97 | loss: 0.25768 | val_0_rmse: 0.58877 | val_1_rmse: 0.59518 |  0:06:52s
epoch 98 | loss: 0.25629 | val_0_rmse: 0.55039 | val_1_rmse: 0.55721 |  0:06:56s
epoch 99 | loss: 0.25677 | val_0_rmse: 0.55556 | val_1_rmse: 0.56141 |  0:07:00s
epoch 100| loss: 0.25553 | val_0_rmse: 0.56604 | val_1_rmse: 0.57147 |  0:07:04s
epoch 101| loss: 0.25685 | val_0_rmse: 0.57397 | val_1_rmse: 0.58095 |  0:07:09s
epoch 102| loss: 0.25528 | val_0_rmse: 0.54928 | val_1_rmse: 0.55604 |  0:07:13s
epoch 103| loss: 0.25603 | val_0_rmse: 0.56617 | val_1_rmse: 0.57223 |  0:07:17s
epoch 104| loss: 0.25499 | val_0_rmse: 0.57774 | val_1_rmse: 0.58257 |  0:07:21s
epoch 105| loss: 0.25366 | val_0_rmse: 0.51988 | val_1_rmse: 0.52749 |  0:07:25s
epoch 106| loss: 0.25776 | val_0_rmse: 0.56399 | val_1_rmse: 0.56946 |  0:07:30s
epoch 107| loss: 0.25636 | val_0_rmse: 0.53221 | val_1_rmse: 0.5387  |  0:07:34s
epoch 108| loss: 0.2547  | val_0_rmse: 0.57354 | val_1_rmse: 0.57945 |  0:07:38s
epoch 109| loss: 0.25278 | val_0_rmse: 0.51691 | val_1_rmse: 0.52189 |  0:07:42s
epoch 110| loss: 0.2537  | val_0_rmse: 0.56184 | val_1_rmse: 0.56805 |  0:07:46s
epoch 111| loss: 0.25202 | val_0_rmse: 0.55225 | val_1_rmse: 0.55946 |  0:07:51s
epoch 112| loss: 0.25201 | val_0_rmse: 0.63398 | val_1_rmse: 0.63873 |  0:07:55s
epoch 113| loss: 0.25439 | val_0_rmse: 0.54279 | val_1_rmse: 0.54967 |  0:07:59s
epoch 114| loss: 0.25138 | val_0_rmse: 0.52023 | val_1_rmse: 0.52617 |  0:08:04s
epoch 115| loss: 0.25175 | val_0_rmse: 0.53568 | val_1_rmse: 0.54046 |  0:08:08s
epoch 116| loss: 0.25469 | val_0_rmse: 0.54924 | val_1_rmse: 0.55277 |  0:08:12s
epoch 117| loss: 0.25326 | val_0_rmse: 0.58232 | val_1_rmse: 0.58659 |  0:08:16s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.50492
Best weights from best epoch are automatically used!
ended training at: 03:43:44
Feature importance:
[('Area', 0.4454963183488901), ('Baths', 0.014519450350615854), ('Beds', 0.3061351232692533), ('Latitude', 0.10863340201220163), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.12521570601903914)]
Mean squared error is of 840951887.4545739
Mean absolute error:20202.100334204384
MAPE:0.35268820738830753
R2 score:0.7532764224109044
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:43:45
epoch 0  | loss: 0.4119  | val_0_rmse: 0.579   | val_1_rmse: 0.58418 |  0:00:04s
epoch 1  | loss: 0.33575 | val_0_rmse: 0.58316 | val_1_rmse: 0.58828 |  0:00:08s
epoch 2  | loss: 0.32936 | val_0_rmse: 0.56499 | val_1_rmse: 0.57173 |  0:00:12s
epoch 3  | loss: 0.32719 | val_0_rmse: 0.57214 | val_1_rmse: 0.57538 |  0:00:16s
epoch 4  | loss: 0.33189 | val_0_rmse: 0.56646 | val_1_rmse: 0.57161 |  0:00:21s
epoch 5  | loss: 0.32508 | val_0_rmse: 0.5648  | val_1_rmse: 0.56793 |  0:00:25s
epoch 6  | loss: 0.32497 | val_0_rmse: 0.56331 | val_1_rmse: 0.56779 |  0:00:29s
epoch 7  | loss: 0.32886 | val_0_rmse: 0.56911 | val_1_rmse: 0.57233 |  0:00:33s
epoch 8  | loss: 0.32106 | val_0_rmse: 0.56084 | val_1_rmse: 0.56525 |  0:00:38s
epoch 9  | loss: 0.31756 | val_0_rmse: 0.56013 | val_1_rmse: 0.56363 |  0:00:42s
epoch 10 | loss: 0.31741 | val_0_rmse: 0.56103 | val_1_rmse: 0.56564 |  0:00:46s
epoch 11 | loss: 0.3186  | val_0_rmse: 0.55945 | val_1_rmse: 0.56455 |  0:00:50s
epoch 12 | loss: 0.31943 | val_0_rmse: 0.56461 | val_1_rmse: 0.56941 |  0:00:55s
epoch 13 | loss: 0.32023 | val_0_rmse: 0.56257 | val_1_rmse: 0.56552 |  0:00:59s
epoch 14 | loss: 0.32167 | val_0_rmse: 0.56026 | val_1_rmse: 0.56459 |  0:01:03s
epoch 15 | loss: 0.3192  | val_0_rmse: 0.59834 | val_1_rmse: 0.60219 |  0:01:07s
epoch 16 | loss: 0.31663 | val_0_rmse: 0.5628  | val_1_rmse: 0.56544 |  0:01:11s
epoch 17 | loss: 0.31499 | val_0_rmse: 0.565   | val_1_rmse: 0.56835 |  0:01:16s
epoch 18 | loss: 0.31433 | val_0_rmse: 0.56008 | val_1_rmse: 0.56711 |  0:01:20s
epoch 19 | loss: 0.31458 | val_0_rmse: 0.55221 | val_1_rmse: 0.55936 |  0:01:24s
epoch 20 | loss: 0.3093  | val_0_rmse: 0.58275 | val_1_rmse: 0.58262 |  0:01:28s
epoch 21 | loss: 0.3092  | val_0_rmse: 0.62758 | val_1_rmse: 0.55918 |  0:01:33s
epoch 22 | loss: 0.30934 | val_0_rmse: 0.71661 | val_1_rmse: 0.67754 |  0:01:37s
epoch 23 | loss: 0.30606 | val_0_rmse: 0.55994 | val_1_rmse: 0.56017 |  0:01:41s
epoch 24 | loss: 0.30707 | val_0_rmse: 0.58467 | val_1_rmse: 0.55999 |  0:01:45s
epoch 25 | loss: 0.30604 | val_0_rmse: 0.5663  | val_1_rmse: 0.56352 |  0:01:50s
epoch 26 | loss: 0.30364 | val_0_rmse: 0.64589 | val_1_rmse: 0.6189  |  0:01:54s
epoch 27 | loss: 0.30221 | val_0_rmse: 0.58323 | val_1_rmse: 0.58216 |  0:01:58s
epoch 28 | loss: 0.30091 | val_0_rmse: 0.60026 | val_1_rmse: 0.5835  |  0:02:02s
epoch 29 | loss: 0.3026  | val_0_rmse: 0.5957  | val_1_rmse: 0.59613 |  0:02:07s
epoch 30 | loss: 0.30289 | val_0_rmse: 0.61492 | val_1_rmse: 0.5821  |  0:02:11s
epoch 31 | loss: 0.30151 | val_0_rmse: 1.24709 | val_1_rmse: 0.57722 |  0:02:15s
epoch 32 | loss: 0.30426 | val_0_rmse: 0.57971 | val_1_rmse: 0.56004 |  0:02:19s
epoch 33 | loss: 0.30201 | val_0_rmse: 0.61136 | val_1_rmse: 0.59462 |  0:02:24s
epoch 34 | loss: 0.30097 | val_0_rmse: 0.65546 | val_1_rmse: 0.58277 |  0:02:28s
epoch 35 | loss: 0.2998  | val_0_rmse: 0.77008 | val_1_rmse: 0.56367 |  0:02:32s
epoch 36 | loss: 0.3027  | val_0_rmse: 0.71065 | val_1_rmse: 0.57629 |  0:02:36s
epoch 37 | loss: 0.30171 | val_0_rmse: 1.06733 | val_1_rmse: 0.89968 |  0:02:41s
epoch 38 | loss: 0.29818 | val_0_rmse: 0.65836 | val_1_rmse: 0.55283 |  0:02:45s
epoch 39 | loss: 0.2983  | val_0_rmse: 0.62771 | val_1_rmse: 0.56731 |  0:02:49s
epoch 40 | loss: 0.29911 | val_0_rmse: 0.61942 | val_1_rmse: 0.58026 |  0:02:53s
epoch 41 | loss: 0.3005  | val_0_rmse: 0.58004 | val_1_rmse: 0.55911 |  0:02:58s
epoch 42 | loss: 0.29889 | val_0_rmse: 0.60117 | val_1_rmse: 0.55617 |  0:03:02s
epoch 43 | loss: 0.30269 | val_0_rmse: 0.61528 | val_1_rmse: 0.59392 |  0:03:06s
epoch 44 | loss: 0.30152 | val_0_rmse: 0.60369 | val_1_rmse: 0.57113 |  0:03:10s
epoch 45 | loss: 0.29969 | val_0_rmse: 0.6078  | val_1_rmse: 0.60494 |  0:03:15s
epoch 46 | loss: 0.30074 | val_0_rmse: 0.62219 | val_1_rmse: 0.55534 |  0:03:19s
epoch 47 | loss: 0.29694 | val_0_rmse: 0.66651 | val_1_rmse: 0.61891 |  0:03:23s
epoch 48 | loss: 0.29776 | val_0_rmse: 0.58161 | val_1_rmse: 0.5706  |  0:03:27s
epoch 49 | loss: 0.29627 | val_0_rmse: 0.58679 | val_1_rmse: 0.56171 |  0:03:32s
epoch 50 | loss: 0.29682 | val_0_rmse: 0.57467 | val_1_rmse: 0.56106 |  0:03:36s
epoch 51 | loss: 0.2975  | val_0_rmse: 0.65339 | val_1_rmse: 0.61113 |  0:03:40s
epoch 52 | loss: 0.29731 | val_0_rmse: 0.63937 | val_1_rmse: 0.57634 |  0:03:44s
epoch 53 | loss: 0.30243 | val_0_rmse: 0.58803 | val_1_rmse: 0.58447 |  0:03:49s
epoch 54 | loss: 0.2994  | val_0_rmse: 0.60578 | val_1_rmse: 0.55214 |  0:03:53s
epoch 55 | loss: 0.29737 | val_0_rmse: 0.56215 | val_1_rmse: 0.56061 |  0:03:57s
epoch 56 | loss: 0.29763 | val_0_rmse: 0.58826 | val_1_rmse: 0.55992 |  0:04:01s
epoch 57 | loss: 0.29715 | val_0_rmse: 0.63437 | val_1_rmse: 0.55804 |  0:04:06s
epoch 58 | loss: 0.30541 | val_0_rmse: 0.57387 | val_1_rmse: 0.55664 |  0:04:10s
epoch 59 | loss: 0.30942 | val_0_rmse: 0.60141 | val_1_rmse: 0.59099 |  0:04:14s
epoch 60 | loss: 0.31513 | val_0_rmse: 0.56334 | val_1_rmse: 0.56112 |  0:04:18s
epoch 61 | loss: 0.31378 | val_0_rmse: 0.60088 | val_1_rmse: 0.60263 |  0:04:23s
epoch 62 | loss: 0.31385 | val_0_rmse: 0.56557 | val_1_rmse: 0.56726 |  0:04:27s
epoch 63 | loss: 0.33031 | val_0_rmse: 0.57172 | val_1_rmse: 0.57485 |  0:04:31s
epoch 64 | loss: 0.32243 | val_0_rmse: 0.56447 | val_1_rmse: 0.56543 |  0:04:35s
epoch 65 | loss: 0.32009 | val_0_rmse: 0.5597  | val_1_rmse: 0.56294 |  0:04:40s
epoch 66 | loss: 0.32009 | val_0_rmse: 0.56289 | val_1_rmse: 0.56509 |  0:04:44s
epoch 67 | loss: 0.31811 | val_0_rmse: 0.55801 | val_1_rmse: 0.56083 |  0:04:48s
epoch 68 | loss: 0.31459 | val_0_rmse: 0.5596  | val_1_rmse: 0.56273 |  0:04:52s
epoch 69 | loss: 0.31486 | val_0_rmse: 0.56895 | val_1_rmse: 0.56955 |  0:04:57s
epoch 70 | loss: 0.31224 | val_0_rmse: 0.56087 | val_1_rmse: 0.56359 |  0:05:01s
epoch 71 | loss: 0.30939 | val_0_rmse: 0.55668 | val_1_rmse: 0.55839 |  0:05:05s
epoch 72 | loss: 0.30479 | val_0_rmse: 0.63915 | val_1_rmse: 0.64076 |  0:05:09s
epoch 73 | loss: 0.29189 | val_0_rmse: 0.60737 | val_1_rmse: 0.61346 |  0:05:14s
epoch 74 | loss: 0.28977 | val_0_rmse: 0.54836 | val_1_rmse: 0.55117 |  0:05:18s
epoch 75 | loss: 0.28794 | val_0_rmse: 0.54479 | val_1_rmse: 0.55255 |  0:05:22s
epoch 76 | loss: 0.28382 | val_0_rmse: 0.56152 | val_1_rmse: 0.56539 |  0:05:26s
epoch 77 | loss: 0.28227 | val_0_rmse: 0.56851 | val_1_rmse: 0.57264 |  0:05:31s
epoch 78 | loss: 0.28416 | val_0_rmse: 0.52411 | val_1_rmse: 0.52456 |  0:05:35s
epoch 79 | loss: 0.28109 | val_0_rmse: 0.53064 | val_1_rmse: 0.53183 |  0:05:39s
epoch 80 | loss: 0.27943 | val_0_rmse: 0.59353 | val_1_rmse: 0.59851 |  0:05:43s
epoch 81 | loss: 0.28044 | val_0_rmse: 0.53167 | val_1_rmse: 0.53398 |  0:05:48s
epoch 82 | loss: 0.28071 | val_0_rmse: 0.58861 | val_1_rmse: 0.59553 |  0:05:52s
epoch 83 | loss: 0.27802 | val_0_rmse: 0.52765 | val_1_rmse: 0.53021 |  0:05:56s
epoch 84 | loss: 0.2775  | val_0_rmse: 0.52441 | val_1_rmse: 0.52607 |  0:06:00s
epoch 85 | loss: 0.2798  | val_0_rmse: 0.55364 | val_1_rmse: 0.55792 |  0:06:04s
epoch 86 | loss: 0.2796  | val_0_rmse: 0.56283 | val_1_rmse: 0.56704 |  0:06:09s
epoch 87 | loss: 0.27942 | val_0_rmse: 0.56567 | val_1_rmse: 0.56745 |  0:06:13s
epoch 88 | loss: 0.27667 | val_0_rmse: 0.60852 | val_1_rmse: 0.61206 |  0:06:17s
epoch 89 | loss: 0.27665 | val_0_rmse: 0.54613 | val_1_rmse: 0.54989 |  0:06:21s
epoch 90 | loss: 0.27738 | val_0_rmse: 0.60465 | val_1_rmse: 0.60937 |  0:06:25s
epoch 91 | loss: 0.27668 | val_0_rmse: 0.52995 | val_1_rmse: 0.53264 |  0:06:30s
epoch 92 | loss: 0.27527 | val_0_rmse: 0.56729 | val_1_rmse: 0.57247 |  0:06:34s
epoch 93 | loss: 0.27525 | val_0_rmse: 0.54421 | val_1_rmse: 0.54722 |  0:06:38s
epoch 94 | loss: 0.27588 | val_0_rmse: 0.62294 | val_1_rmse: 0.62654 |  0:06:42s
epoch 95 | loss: 0.2746  | val_0_rmse: 0.5525  | val_1_rmse: 0.55381 |  0:06:46s
epoch 96 | loss: 0.27516 | val_0_rmse: 0.5736  | val_1_rmse: 0.57651 |  0:06:51s
epoch 97 | loss: 0.27734 | val_0_rmse: 0.54214 | val_1_rmse: 0.54515 |  0:06:55s
epoch 98 | loss: 0.27416 | val_0_rmse: 0.53828 | val_1_rmse: 0.54299 |  0:06:59s
epoch 99 | loss: 0.2726  | val_0_rmse: 0.58254 | val_1_rmse: 0.58876 |  0:07:03s
epoch 100| loss: 0.27301 | val_0_rmse: 0.51403 | val_1_rmse: 0.51908 |  0:07:07s
epoch 101| loss: 0.27089 | val_0_rmse: 0.55675 | val_1_rmse: 0.56062 |  0:07:12s
epoch 102| loss: 0.27138 | val_0_rmse: 0.5499  | val_1_rmse: 0.55447 |  0:07:16s
epoch 103| loss: 0.27289 | val_0_rmse: 0.59378 | val_1_rmse: 0.60029 |  0:07:20s
epoch 104| loss: 0.27347 | val_0_rmse: 0.51906 | val_1_rmse: 0.52425 |  0:07:24s
epoch 105| loss: 0.27339 | val_0_rmse: 0.53091 | val_1_rmse: 0.53447 |  0:07:29s
epoch 106| loss: 0.27312 | val_0_rmse: 0.54211 | val_1_rmse: 0.54579 |  0:07:33s
epoch 107| loss: 0.27325 | val_0_rmse: 0.60194 | val_1_rmse: 0.60952 |  0:07:37s
epoch 108| loss: 0.27212 | val_0_rmse: 0.57976 | val_1_rmse: 0.586   |  0:07:41s
epoch 109| loss: 0.271   | val_0_rmse: 0.57782 | val_1_rmse: 0.58321 |  0:07:46s
epoch 110| loss: 0.26953 | val_0_rmse: 0.56885 | val_1_rmse: 0.57511 |  0:07:50s
epoch 111| loss: 0.2692  | val_0_rmse: 0.58872 | val_1_rmse: 0.59388 |  0:07:54s
epoch 112| loss: 0.27083 | val_0_rmse: 0.5312  | val_1_rmse: 0.53706 |  0:07:58s
epoch 113| loss: 0.26626 | val_0_rmse: 0.58518 | val_1_rmse: 0.59152 |  0:08:02s
epoch 114| loss: 0.26853 | val_0_rmse: 0.61674 | val_1_rmse: 0.62206 |  0:08:07s
epoch 115| loss: 0.2695  | val_0_rmse: 0.57174 | val_1_rmse: 0.57665 |  0:08:11s
epoch 116| loss: 0.26798 | val_0_rmse: 0.54824 | val_1_rmse: 0.55412 |  0:08:15s
epoch 117| loss: 0.27087 | val_0_rmse: 0.60099 | val_1_rmse: 0.60673 |  0:08:19s
epoch 118| loss: 0.27392 | val_0_rmse: 0.61025 | val_1_rmse: 0.61498 |  0:08:23s
epoch 119| loss: 0.27115 | val_0_rmse: 0.57137 | val_1_rmse: 0.57601 |  0:08:28s
epoch 120| loss: 0.26933 | val_0_rmse: 0.58673 | val_1_rmse: 0.59159 |  0:08:32s
epoch 121| loss: 0.26746 | val_0_rmse: 0.60643 | val_1_rmse: 0.61026 |  0:08:36s
epoch 122| loss: 0.26859 | val_0_rmse: 0.52773 | val_1_rmse: 0.53086 |  0:08:40s
epoch 123| loss: 0.26597 | val_0_rmse: 0.54412 | val_1_rmse: 0.54941 |  0:08:45s
epoch 124| loss: 0.26776 | val_0_rmse: 0.59    | val_1_rmse: 0.59462 |  0:08:49s
epoch 125| loss: 0.26526 | val_0_rmse: 0.51446 | val_1_rmse: 0.51969 |  0:08:53s
epoch 126| loss: 0.26492 | val_0_rmse: 0.56186 | val_1_rmse: 0.56722 |  0:08:57s
epoch 127| loss: 0.27137 | val_0_rmse: 0.50785 | val_1_rmse: 0.51354 |  0:09:01s
epoch 128| loss: 0.26567 | val_0_rmse: 0.50813 | val_1_rmse: 0.51275 |  0:09:06s
epoch 129| loss: 0.2633  | val_0_rmse: 0.60215 | val_1_rmse: 0.60709 |  0:09:10s
epoch 130| loss: 0.26145 | val_0_rmse: 0.51621 | val_1_rmse: 0.52389 |  0:09:14s
epoch 131| loss: 0.26095 | val_0_rmse: 0.57187 | val_1_rmse: 0.57581 |  0:09:18s
epoch 132| loss: 0.26177 | val_0_rmse: 0.5572  | val_1_rmse: 0.5628  |  0:09:22s
epoch 133| loss: 0.26009 | val_0_rmse: 0.53135 | val_1_rmse: 0.53397 |  0:09:27s
epoch 134| loss: 0.26081 | val_0_rmse: 0.52879 | val_1_rmse: 0.53137 |  0:09:31s
epoch 135| loss: 0.26038 | val_0_rmse: 0.57336 | val_1_rmse: 0.57561 |  0:09:35s
epoch 136| loss: 0.26349 | val_0_rmse: 0.61393 | val_1_rmse: 0.61759 |  0:09:39s
epoch 137| loss: 0.26363 | val_0_rmse: 0.51637 | val_1_rmse: 0.52431 |  0:09:44s
epoch 138| loss: 0.25918 | val_0_rmse: 0.56146 | val_1_rmse: 0.5671  |  0:09:48s
epoch 139| loss: 0.25971 | val_0_rmse: 0.57963 | val_1_rmse: 0.58557 |  0:09:52s
epoch 140| loss: 0.26012 | val_0_rmse: 0.53713 | val_1_rmse: 0.54258 |  0:09:56s
epoch 141| loss: 0.26479 | val_0_rmse: 0.51309 | val_1_rmse: 0.51928 |  0:10:01s
epoch 142| loss: 0.27713 | val_0_rmse: 0.53591 | val_1_rmse: 0.54235 |  0:10:05s
epoch 143| loss: 0.26951 | val_0_rmse: 0.5631  | val_1_rmse: 0.56737 |  0:10:09s
epoch 144| loss: 0.26603 | val_0_rmse: 0.57124 | val_1_rmse: 0.57752 |  0:10:13s
epoch 145| loss: 0.26513 | val_0_rmse: 0.59713 | val_1_rmse: 0.60344 |  0:10:17s
epoch 146| loss: 0.2645  | val_0_rmse: 0.58122 | val_1_rmse: 0.58755 |  0:10:22s
epoch 147| loss: 0.2643  | val_0_rmse: 0.5249  | val_1_rmse: 0.52906 |  0:10:26s
epoch 148| loss: 0.26142 | val_0_rmse: 0.57295 | val_1_rmse: 0.58271 |  0:10:30s
epoch 149| loss: 0.25991 | val_0_rmse: 0.56945 | val_1_rmse: 0.57591 |  0:10:34s
Stop training because you reached max_epochs = 150 with best_epoch = 128 and best_val_1_rmse = 0.51275
Best weights from best epoch are automatically used!
ended training at: 03:54:21
Feature importance:
[('Area', 0.675539740421301), ('Baths', 0.03465030363297521), ('Beds', 0.11398016879849751), ('Latitude', 0.0), ('Longitude', 0.055235558510560334), ('Month', 0.07642469642002016), ('Year', 0.044169532216645706)]
Mean squared error is of 869089222.4665284
Mean absolute error:20313.219480155818
MAPE:0.3294698301266583
R2 score:0.7364582842479093
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:54:21
epoch 0  | loss: 0.42766 | val_0_rmse: 0.57387 | val_1_rmse: 0.56888 |  0:00:04s
epoch 1  | loss: 0.34196 | val_0_rmse: 0.56821 | val_1_rmse: 0.5619  |  0:00:08s
epoch 2  | loss: 0.33144 | val_0_rmse: 0.56685 | val_1_rmse: 0.56355 |  0:00:12s
epoch 3  | loss: 0.32533 | val_0_rmse: 0.56342 | val_1_rmse: 0.55922 |  0:00:16s
epoch 4  | loss: 0.32373 | val_0_rmse: 0.56474 | val_1_rmse: 0.55726 |  0:00:21s
epoch 5  | loss: 0.33188 | val_0_rmse: 0.56261 | val_1_rmse: 0.55801 |  0:00:25s
epoch 6  | loss: 0.3226  | val_0_rmse: 0.55865 | val_1_rmse: 0.55332 |  0:00:29s
epoch 7  | loss: 0.31975 | val_0_rmse: 0.5591  | val_1_rmse: 0.5533  |  0:00:33s
epoch 8  | loss: 0.31967 | val_0_rmse: 0.56487 | val_1_rmse: 0.55651 |  0:00:37s
epoch 9  | loss: 0.31936 | val_0_rmse: 0.5748  | val_1_rmse: 0.57015 |  0:00:42s
epoch 10 | loss: 0.31754 | val_0_rmse: 0.56352 | val_1_rmse: 0.55737 |  0:00:46s
epoch 11 | loss: 0.31965 | val_0_rmse: 0.56593 | val_1_rmse: 0.55187 |  0:00:50s
epoch 12 | loss: 0.31451 | val_0_rmse: 0.58054 | val_1_rmse: 0.55754 |  0:00:54s
epoch 13 | loss: 0.31297 | val_0_rmse: 0.65591 | val_1_rmse: 0.5659  |  0:00:59s
epoch 14 | loss: 0.31859 | val_0_rmse: 0.60026 | val_1_rmse: 0.55572 |  0:01:03s
epoch 15 | loss: 0.31155 | val_0_rmse: 0.64073 | val_1_rmse: 0.54967 |  0:01:07s
epoch 16 | loss: 0.30682 | val_0_rmse: 0.64234 | val_1_rmse: 0.57394 |  0:01:11s
epoch 17 | loss: 0.31057 | val_0_rmse: 0.62946 | val_1_rmse: 0.57283 |  0:01:15s
epoch 18 | loss: 0.30962 | val_0_rmse: 0.55791 | val_1_rmse: 0.54829 |  0:01:20s
epoch 19 | loss: 0.3078  | val_0_rmse: 0.6271  | val_1_rmse: 0.55086 |  0:01:24s
epoch 20 | loss: 0.3022  | val_0_rmse: 0.63184 | val_1_rmse: 0.54624 |  0:01:28s
epoch 21 | loss: 0.30214 | val_0_rmse: 0.82975 | val_1_rmse: 0.54889 |  0:01:32s
epoch 22 | loss: 0.29835 | val_0_rmse: 0.81122 | val_1_rmse: 0.58777 |  0:01:36s
epoch 23 | loss: 0.30259 | val_0_rmse: 0.83455 | val_1_rmse: 0.57105 |  0:01:41s
epoch 24 | loss: 0.30197 | val_0_rmse: 0.78778 | val_1_rmse: 0.55064 |  0:01:45s
epoch 25 | loss: 0.29975 | val_0_rmse: 0.71507 | val_1_rmse: 0.54057 |  0:01:49s
epoch 26 | loss: 0.29998 | val_0_rmse: 0.74016 | val_1_rmse: 0.55953 |  0:01:53s
epoch 27 | loss: 0.29875 | val_0_rmse: 0.64788 | val_1_rmse: 0.56216 |  0:01:58s
epoch 28 | loss: 0.29377 | val_0_rmse: 0.77255 | val_1_rmse: 0.53541 |  0:02:02s
epoch 29 | loss: 0.29864 | val_0_rmse: 0.94599 | val_1_rmse: 0.58071 |  0:02:06s
epoch 30 | loss: 0.29664 | val_0_rmse: 0.75345 | val_1_rmse: 0.75161 |  0:02:10s
epoch 31 | loss: 0.2924  | val_0_rmse: 0.59898 | val_1_rmse: 0.59762 |  0:02:14s
epoch 32 | loss: 0.29124 | val_0_rmse: 0.62701 | val_1_rmse: 0.62569 |  0:02:19s
epoch 33 | loss: 0.28956 | val_0_rmse: 0.58682 | val_1_rmse: 0.58454 |  0:02:23s
epoch 34 | loss: 0.28925 | val_0_rmse: 0.5327  | val_1_rmse: 0.52899 |  0:02:27s
epoch 35 | loss: 0.28979 | val_0_rmse: 0.56336 | val_1_rmse: 0.56072 |  0:02:31s
epoch 36 | loss: 0.28685 | val_0_rmse: 0.64161 | val_1_rmse: 0.63912 |  0:02:35s
epoch 37 | loss: 0.28624 | val_0_rmse: 0.58966 | val_1_rmse: 0.58902 |  0:02:40s
epoch 38 | loss: 0.28706 | val_0_rmse: 0.64479 | val_1_rmse: 0.6439  |  0:02:44s
epoch 39 | loss: 0.28614 | val_0_rmse: 0.60604 | val_1_rmse: 0.60278 |  0:02:48s
epoch 40 | loss: 0.28411 | val_0_rmse: 0.64842 | val_1_rmse: 0.6475  |  0:02:52s
epoch 41 | loss: 0.28503 | val_0_rmse: 0.5846  | val_1_rmse: 0.58334 |  0:02:56s
epoch 42 | loss: 0.28493 | val_0_rmse: 0.60431 | val_1_rmse: 0.60294 |  0:03:01s
epoch 43 | loss: 0.28413 | val_0_rmse: 0.68412 | val_1_rmse: 0.68135 |  0:03:05s
epoch 44 | loss: 0.28762 | val_0_rmse: 0.61401 | val_1_rmse: 0.61174 |  0:03:09s
epoch 45 | loss: 0.29917 | val_0_rmse: 0.67729 | val_1_rmse: 0.67434 |  0:03:13s
epoch 46 | loss: 0.29867 | val_0_rmse: 0.65513 | val_1_rmse: 0.6541  |  0:03:17s
epoch 47 | loss: 0.2902  | val_0_rmse: 0.56899 | val_1_rmse: 0.56798 |  0:03:22s
epoch 48 | loss: 0.28862 | val_0_rmse: 0.63568 | val_1_rmse: 0.63418 |  0:03:26s
epoch 49 | loss: 0.28841 | val_0_rmse: 0.67852 | val_1_rmse: 0.67892 |  0:03:30s
epoch 50 | loss: 0.2871  | val_0_rmse: 0.64189 | val_1_rmse: 0.64109 |  0:03:34s
epoch 51 | loss: 0.28623 | val_0_rmse: 0.66883 | val_1_rmse: 0.66914 |  0:03:38s
epoch 52 | loss: 0.28648 | val_0_rmse: 0.6712  | val_1_rmse: 0.67103 |  0:03:43s
epoch 53 | loss: 0.28688 | val_0_rmse: 3.51673 | val_1_rmse: 3.51562 |  0:03:47s
epoch 54 | loss: 0.29547 | val_0_rmse: 0.7151  | val_1_rmse: 0.71134 |  0:03:51s
epoch 55 | loss: 0.29346 | val_0_rmse: 0.64727 | val_1_rmse: 0.64559 |  0:03:55s
epoch 56 | loss: 0.29044 | val_0_rmse: 0.7345  | val_1_rmse: 0.72988 |  0:03:59s
epoch 57 | loss: 0.30564 | val_0_rmse: 0.61469 | val_1_rmse: 0.61066 |  0:04:04s
epoch 58 | loss: 0.32522 | val_0_rmse: 0.5719  | val_1_rmse: 0.56854 |  0:04:08s
epoch 59 | loss: 0.32167 | val_0_rmse: 0.57741 | val_1_rmse: 0.57355 |  0:04:12s
epoch 60 | loss: 0.30932 | val_0_rmse: 0.56099 | val_1_rmse: 0.55675 |  0:04:16s
epoch 61 | loss: 0.308   | val_0_rmse: 0.58893 | val_1_rmse: 0.58203 |  0:04:20s
epoch 62 | loss: 0.30931 | val_0_rmse: 0.56313 | val_1_rmse: 0.55879 |  0:04:25s
epoch 63 | loss: 0.30483 | val_0_rmse: 0.56584 | val_1_rmse: 0.56296 |  0:04:29s
epoch 64 | loss: 0.29504 | val_0_rmse: 0.78178 | val_1_rmse: 0.77586 |  0:04:33s

Early stopping occured at epoch 64 with best_epoch = 34 and best_val_1_rmse = 0.52899
Best weights from best epoch are automatically used!
ended training at: 03:58:56
Feature importance:
[('Area', 0.42351156367957893), ('Baths', 0.1197620392100733), ('Beds', 0.17951519943358793), ('Latitude', 0.26697729437083356), ('Longitude', 0.003288505290734858), ('Month', 0.0), ('Year', 0.00694539801519143)]
Mean squared error is of 992293579.1537623
Mean absolute error:22129.642552725814
MAPE:0.36373502357195536
R2 score:0.7064257337570186
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 03:58:56
epoch 0  | loss: 0.39134 | val_0_rmse: 0.58368 | val_1_rmse: 0.57911 |  0:00:19s
epoch 1  | loss: 0.33361 | val_0_rmse: 0.56359 | val_1_rmse: 0.56139 |  0:00:38s
epoch 2  | loss: 0.32308 | val_0_rmse: 0.55469 | val_1_rmse: 0.55123 |  0:00:58s
epoch 3  | loss: 0.31869 | val_0_rmse: 0.55879 | val_1_rmse: 0.55836 |  0:01:17s
epoch 4  | loss: 0.31474 | val_0_rmse: 0.55023 | val_1_rmse: 0.54564 |  0:01:37s
epoch 5  | loss: 0.30756 | val_0_rmse: 0.56602 | val_1_rmse: 0.56467 |  0:01:56s
epoch 6  | loss: 0.30837 | val_0_rmse: 0.56166 | val_1_rmse: 0.55913 |  0:02:16s
epoch 7  | loss: 0.3063  | val_0_rmse: 0.55641 | val_1_rmse: 0.55412 |  0:02:35s
epoch 8  | loss: 0.31988 | val_0_rmse: 0.56251 | val_1_rmse: 0.55937 |  0:02:55s
epoch 9  | loss: 0.30665 | val_0_rmse: 0.54246 | val_1_rmse: 0.54054 |  0:03:14s
epoch 10 | loss: 0.3042  | val_0_rmse: 0.54738 | val_1_rmse: 0.54499 |  0:03:34s
epoch 11 | loss: 0.30286 | val_0_rmse: 0.54223 | val_1_rmse: 0.54031 |  0:03:53s
epoch 12 | loss: 0.30122 | val_0_rmse: 0.55963 | val_1_rmse: 0.55569 |  0:04:13s
epoch 13 | loss: 0.30069 | val_0_rmse: 0.54026 | val_1_rmse: 0.53823 |  0:04:33s
epoch 14 | loss: 0.2993  | val_0_rmse: 0.55053 | val_1_rmse: 0.5465  |  0:04:52s
epoch 15 | loss: 0.29864 | val_0_rmse: 0.55334 | val_1_rmse: 0.55306 |  0:05:12s
epoch 16 | loss: 0.30338 | val_0_rmse: 0.54558 | val_1_rmse: 0.54526 |  0:05:31s
epoch 17 | loss: 0.29722 | val_0_rmse: 0.5411  | val_1_rmse: 0.53858 |  0:05:51s
epoch 18 | loss: 0.3004  | val_0_rmse: 0.53875 | val_1_rmse: 0.53762 |  0:06:11s
epoch 19 | loss: 0.29651 | val_0_rmse: 0.54511 | val_1_rmse: 0.54251 |  0:06:30s
epoch 20 | loss: 0.29619 | val_0_rmse: 0.54053 | val_1_rmse: 0.53837 |  0:06:50s
epoch 21 | loss: 0.29522 | val_0_rmse: 0.54317 | val_1_rmse: 0.54134 |  0:07:09s
epoch 22 | loss: 0.2946  | val_0_rmse: 0.53752 | val_1_rmse: 0.53655 |  0:07:29s
epoch 23 | loss: 0.30387 | val_0_rmse: 0.55264 | val_1_rmse: 0.5518  |  0:07:49s
epoch 24 | loss: 0.29892 | val_0_rmse: 0.544   | val_1_rmse: 0.54404 |  0:08:08s
epoch 25 | loss: 0.29734 | val_0_rmse: 0.55706 | val_1_rmse: 0.55222 |  0:08:28s
epoch 26 | loss: 0.29598 | val_0_rmse: 0.54741 | val_1_rmse: 0.54566 |  0:08:47s
epoch 27 | loss: 0.29468 | val_0_rmse: 0.55064 | val_1_rmse: 0.54843 |  0:09:07s
epoch 28 | loss: 0.29698 | val_0_rmse: 0.54887 | val_1_rmse: 0.54549 |  0:09:27s
epoch 29 | loss: 0.29424 | val_0_rmse: 0.5388  | val_1_rmse: 0.53654 |  0:09:46s
epoch 30 | loss: 0.29479 | val_0_rmse: 0.54025 | val_1_rmse: 0.5382  |  0:10:06s
epoch 31 | loss: 0.29497 | val_0_rmse: 0.53736 | val_1_rmse: 0.53707 |  0:10:26s
epoch 32 | loss: 0.29317 | val_0_rmse: 0.53982 | val_1_rmse: 0.54017 |  0:10:45s
epoch 33 | loss: 0.29319 | val_0_rmse: 0.5363  | val_1_rmse: 0.5364  |  0:11:05s
epoch 34 | loss: 0.29395 | val_0_rmse: 0.54441 | val_1_rmse: 0.54361 |  0:11:24s
epoch 35 | loss: 0.29365 | val_0_rmse: 0.53597 | val_1_rmse: 0.53612 |  0:11:44s
epoch 36 | loss: 0.29302 | val_0_rmse: 0.54514 | val_1_rmse: 0.54667 |  0:12:03s
epoch 37 | loss: 0.29139 | val_0_rmse: 0.54047 | val_1_rmse: 0.53933 |  0:12:23s
epoch 38 | loss: 0.29304 | val_0_rmse: 0.53445 | val_1_rmse: 0.53563 |  0:12:42s
epoch 39 | loss: 0.29259 | val_0_rmse: 0.5402  | val_1_rmse: 0.5416  |  0:13:02s
epoch 40 | loss: 0.29149 | val_0_rmse: 0.57484 | val_1_rmse: 0.57591 |  0:13:21s
epoch 41 | loss: 0.29136 | val_0_rmse: 0.56064 | val_1_rmse: 0.56237 |  0:13:41s
epoch 42 | loss: 0.29221 | val_0_rmse: 0.53398 | val_1_rmse: 0.53326 |  0:14:00s
epoch 43 | loss: 0.30406 | val_0_rmse: 0.54653 | val_1_rmse: 0.54309 |  0:14:20s
epoch 44 | loss: 0.30173 | val_0_rmse: 0.55095 | val_1_rmse: 0.54977 |  0:14:40s
epoch 45 | loss: 0.29656 | val_0_rmse: 0.54161 | val_1_rmse: 0.54293 |  0:14:59s
epoch 46 | loss: 0.29131 | val_0_rmse: 0.5321  | val_1_rmse: 0.53255 |  0:15:18s
epoch 47 | loss: 0.29389 | val_0_rmse: 0.55029 | val_1_rmse: 0.54988 |  0:15:38s
epoch 48 | loss: 0.29766 | val_0_rmse: 0.53786 | val_1_rmse: 0.53688 |  0:15:58s
epoch 49 | loss: 0.29226 | val_0_rmse: 0.55303 | val_1_rmse: 0.55486 |  0:16:17s
epoch 50 | loss: 0.29145 | val_0_rmse: 0.53772 | val_1_rmse: 0.53776 |  0:16:37s
epoch 51 | loss: 0.29127 | val_0_rmse: 0.53599 | val_1_rmse: 0.53608 |  0:16:56s
epoch 52 | loss: 0.28966 | val_0_rmse: 0.53232 | val_1_rmse: 0.53325 |  0:17:16s
epoch 53 | loss: 0.29034 | val_0_rmse: 0.54588 | val_1_rmse: 0.54521 |  0:17:35s
epoch 54 | loss: 0.29422 | val_0_rmse: 0.54108 | val_1_rmse: 0.54152 |  0:17:55s
epoch 55 | loss: 0.2915  | val_0_rmse: 0.54385 | val_1_rmse: 0.54476 |  0:18:14s
epoch 56 | loss: 0.28948 | val_0_rmse: 0.53794 | val_1_rmse: 0.53731 |  0:18:34s
epoch 57 | loss: 0.29432 | val_0_rmse: 0.54414 | val_1_rmse: 0.54442 |  0:18:53s
epoch 58 | loss: 0.28963 | val_0_rmse: 0.53942 | val_1_rmse: 0.53828 |  0:19:13s
epoch 59 | loss: 0.28933 | val_0_rmse: 0.54291 | val_1_rmse: 0.54462 |  0:19:32s
epoch 60 | loss: 0.2888  | val_0_rmse: 0.53498 | val_1_rmse: 0.5346  |  0:19:52s
epoch 61 | loss: 0.28811 | val_0_rmse: 0.53632 | val_1_rmse: 0.53476 |  0:20:12s
epoch 62 | loss: 0.2891  | val_0_rmse: 0.53437 | val_1_rmse: 0.53333 |  0:20:31s
epoch 63 | loss: 0.28796 | val_0_rmse: 0.53111 | val_1_rmse: 0.53254 |  0:20:51s
epoch 64 | loss: 0.28712 | val_0_rmse: 0.53278 | val_1_rmse: 0.53271 |  0:21:11s
epoch 65 | loss: 0.28847 | val_0_rmse: 0.55276 | val_1_rmse: 0.555   |  0:21:30s
epoch 66 | loss: 0.28796 | val_0_rmse: 0.54584 | val_1_rmse: 0.54832 |  0:21:50s
epoch 67 | loss: 0.28664 | val_0_rmse: 0.53276 | val_1_rmse: 0.53287 |  0:22:09s
epoch 68 | loss: 0.28956 | val_0_rmse: 0.57208 | val_1_rmse: 0.5716  |  0:22:29s
epoch 69 | loss: 0.29083 | val_0_rmse: 0.54416 | val_1_rmse: 0.54395 |  0:22:49s
epoch 70 | loss: 0.2946  | val_0_rmse: 0.53636 | val_1_rmse: 0.53747 |  0:23:08s
epoch 71 | loss: 0.28885 | val_0_rmse: 0.53476 | val_1_rmse: 0.53548 |  0:23:27s
epoch 72 | loss: 0.28895 | val_0_rmse: 0.53156 | val_1_rmse: 0.53139 |  0:23:47s
epoch 73 | loss: 0.28979 | val_0_rmse: 0.53618 | val_1_rmse: 0.53645 |  0:24:06s
epoch 74 | loss: 0.28824 | val_0_rmse: 0.55585 | val_1_rmse: 0.55833 |  0:24:26s
epoch 75 | loss: 0.28741 | val_0_rmse: 0.53282 | val_1_rmse: 0.53394 |  0:24:45s
epoch 76 | loss: 0.28709 | val_0_rmse: 0.53152 | val_1_rmse: 0.53159 |  0:25:04s
epoch 77 | loss: 0.28895 | val_0_rmse: 0.53365 | val_1_rmse: 0.53343 |  0:25:24s
epoch 78 | loss: 0.28753 | val_0_rmse: 0.53182 | val_1_rmse: 0.53146 |  0:25:43s
epoch 79 | loss: 0.28597 | val_0_rmse: 0.54373 | val_1_rmse: 0.54445 |  0:26:03s
epoch 80 | loss: 0.28949 | val_0_rmse: 0.53157 | val_1_rmse: 0.53173 |  0:26:22s
epoch 81 | loss: 0.28838 | val_0_rmse: 0.67403 | val_1_rmse: 0.6739  |  0:26:42s
epoch 82 | loss: 0.2881  | val_0_rmse: 0.53376 | val_1_rmse: 0.53442 |  0:27:01s
epoch 83 | loss: 0.28657 | val_0_rmse: 0.53205 | val_1_rmse: 0.5337  |  0:27:20s
epoch 84 | loss: 0.29002 | val_0_rmse: 0.53171 | val_1_rmse: 0.53271 |  0:27:40s
epoch 85 | loss: 0.28714 | val_0_rmse: 0.53403 | val_1_rmse: 0.53739 |  0:27:59s
epoch 86 | loss: 0.28582 | val_0_rmse: 0.53318 | val_1_rmse: 0.53336 |  0:28:19s
epoch 87 | loss: 0.28546 | val_0_rmse: 0.52948 | val_1_rmse: 0.52971 |  0:28:38s
epoch 88 | loss: 0.28574 | val_0_rmse: 0.53386 | val_1_rmse: 0.53367 |  0:28:58s
epoch 89 | loss: 0.28605 | val_0_rmse: 0.55769 | val_1_rmse: 0.55741 |  0:29:17s
epoch 90 | loss: 0.28502 | val_0_rmse: 0.55113 | val_1_rmse: 0.55248 |  0:29:37s
epoch 91 | loss: 0.28709 | val_0_rmse: 0.53251 | val_1_rmse: 0.53424 |  0:29:56s
epoch 92 | loss: 0.28544 | val_0_rmse: 0.53186 | val_1_rmse: 0.53458 |  0:30:15s
epoch 93 | loss: 0.28653 | val_0_rmse: 0.53864 | val_1_rmse: 0.5387  |  0:30:35s
epoch 94 | loss: 0.28464 | val_0_rmse: 0.53324 | val_1_rmse: 0.53511 |  0:30:54s
epoch 95 | loss: 0.2851  | val_0_rmse: 0.53493 | val_1_rmse: 0.53621 |  0:31:14s
epoch 96 | loss: 0.28518 | val_0_rmse: 0.52891 | val_1_rmse: 0.53152 |  0:31:33s
epoch 97 | loss: 0.299   | val_0_rmse: 0.54879 | val_1_rmse: 0.54658 |  0:31:53s
epoch 98 | loss: 0.29129 | val_0_rmse: 0.53768 | val_1_rmse: 0.53857 |  0:32:12s
epoch 99 | loss: 0.28943 | val_0_rmse: 0.55595 | val_1_rmse: 0.55381 |  0:32:32s
epoch 100| loss: 0.28738 | val_0_rmse: 0.54046 | val_1_rmse: 0.53976 |  0:32:51s
epoch 101| loss: 0.28702 | val_0_rmse: 0.53461 | val_1_rmse: 0.53707 |  0:33:11s
epoch 102| loss: 0.28714 | val_0_rmse: 0.53284 | val_1_rmse: 0.53156 |  0:33:30s
epoch 103| loss: 0.28756 | val_0_rmse: 0.54114 | val_1_rmse: 0.54206 |  0:33:50s
epoch 104| loss: 0.28772 | val_0_rmse: 0.53103 | val_1_rmse: 0.53158 |  0:34:09s
epoch 105| loss: 0.28813 | val_0_rmse: 0.5294  | val_1_rmse: 0.52941 |  0:34:29s
epoch 106| loss: 0.28612 | val_0_rmse: 0.55658 | val_1_rmse: 0.55762 |  0:34:48s
epoch 107| loss: 0.28626 | val_0_rmse: 0.53408 | val_1_rmse: 0.53511 |  0:35:08s
epoch 108| loss: 0.28708 | val_0_rmse: 0.52886 | val_1_rmse: 0.53084 |  0:35:27s
epoch 109| loss: 0.28604 | val_0_rmse: 0.52935 | val_1_rmse: 0.53035 |  0:35:47s
epoch 110| loss: 0.28506 | val_0_rmse: 0.53021 | val_1_rmse: 0.53168 |  0:36:06s
epoch 111| loss: 0.28597 | val_0_rmse: 0.60372 | val_1_rmse: 0.60088 |  0:36:26s
epoch 112| loss: 0.29098 | val_0_rmse: 0.55548 | val_1_rmse: 0.55942 |  0:36:45s
epoch 113| loss: 0.28975 | val_0_rmse: 0.53851 | val_1_rmse: 0.5381  |  0:37:05s
epoch 114| loss: 0.28523 | val_0_rmse: 0.53533 | val_1_rmse: 0.53809 |  0:37:24s
epoch 115| loss: 0.28477 | val_0_rmse: 0.5329  | val_1_rmse: 0.5327  |  0:37:44s
epoch 116| loss: 0.28521 | val_0_rmse: 0.53535 | val_1_rmse: 0.53729 |  0:38:04s
epoch 117| loss: 0.30067 | val_0_rmse: 0.5417  | val_1_rmse: 0.54431 |  0:38:23s
epoch 118| loss: 0.29471 | val_0_rmse: 0.53947 | val_1_rmse: 0.54052 |  0:38:43s
epoch 119| loss: 0.29306 | val_0_rmse: 0.53207 | val_1_rmse: 0.5325  |  0:39:02s
epoch 120| loss: 0.2911  | val_0_rmse: 0.54378 | val_1_rmse: 0.54528 |  0:39:22s
epoch 121| loss: 0.29081 | val_0_rmse: 0.54122 | val_1_rmse: 0.54013 |  0:39:41s
epoch 122| loss: 0.29032 | val_0_rmse: 0.54804 | val_1_rmse: 0.55121 |  0:40:01s
epoch 123| loss: 0.29011 | val_0_rmse: 0.53654 | val_1_rmse: 0.5386  |  0:40:21s
epoch 124| loss: 0.28933 | val_0_rmse: 0.5406  | val_1_rmse: 0.54271 |  0:40:40s
epoch 125| loss: 0.28797 | val_0_rmse: 0.52962 | val_1_rmse: 0.53135 |  0:41:00s
epoch 126| loss: 0.28785 | val_0_rmse: 0.53847 | val_1_rmse: 0.5421  |  0:41:19s
epoch 127| loss: 0.28795 | val_0_rmse: 0.53949 | val_1_rmse: 0.54257 |  0:41:39s
epoch 128| loss: 0.28717 | val_0_rmse: 0.53051 | val_1_rmse: 0.53164 |  0:41:59s
epoch 129| loss: 0.28814 | val_0_rmse: 0.53093 | val_1_rmse: 0.53446 |  0:42:19s
epoch 130| loss: 0.28786 | val_0_rmse: 0.53037 | val_1_rmse: 0.53364 |  0:42:39s
epoch 131| loss: 0.2882  | val_0_rmse: 0.53374 | val_1_rmse: 0.53729 |  0:42:58s
epoch 132| loss: 0.28781 | val_0_rmse: 0.53575 | val_1_rmse: 0.53574 |  0:43:18s
epoch 133| loss: 0.28627 | val_0_rmse: 0.5292  | val_1_rmse: 0.53077 |  0:43:38s
epoch 134| loss: 0.28801 | val_0_rmse: 0.54991 | val_1_rmse: 0.5519  |  0:43:57s
epoch 135| loss: 0.28741 | val_0_rmse: 0.54998 | val_1_rmse: 0.55458 |  0:44:17s

Early stopping occured at epoch 135 with best_epoch = 105 and best_val_1_rmse = 0.52941
Best weights from best epoch are automatically used!
ended training at: 04:43:19
Feature importance:
[('Area', 0.06314352139876692), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.28974974251944446), ('Longitude', 0.4015091199062406), ('Month', 0.028110812445985525), ('Year', 0.21748680372956247)]
Mean squared error is of 10936095247.291368
Mean absolute error:65205.38587093444
MAPE:0.4360733335980014
R2 score:0.7213556016818503
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 04:43:22
epoch 0  | loss: 0.38909 | val_0_rmse: 0.63052 | val_1_rmse: 0.6335  |  0:00:19s
epoch 1  | loss: 0.34396 | val_0_rmse: 0.58092 | val_1_rmse: 0.58732 |  0:00:39s
epoch 2  | loss: 0.34091 | val_0_rmse: 0.57055 | val_1_rmse: 0.57421 |  0:00:58s
epoch 3  | loss: 0.33505 | val_0_rmse: 0.57567 | val_1_rmse: 0.57988 |  0:01:17s
epoch 4  | loss: 0.32738 | val_0_rmse: 0.56781 | val_1_rmse: 0.57052 |  0:01:37s
epoch 5  | loss: 0.33238 | val_0_rmse: 0.56527 | val_1_rmse: 0.57105 |  0:01:56s
epoch 6  | loss: 0.32453 | val_0_rmse: 0.57302 | val_1_rmse: 0.57859 |  0:02:16s
epoch 7  | loss: 0.31942 | val_0_rmse: 0.73992 | val_1_rmse: 0.73846 |  0:02:35s
epoch 8  | loss: 0.32469 | val_0_rmse: 0.55654 | val_1_rmse: 0.56161 |  0:02:55s
epoch 9  | loss: 0.32175 | val_0_rmse: 0.57124 | val_1_rmse: 0.57704 |  0:03:14s
epoch 10 | loss: 0.31603 | val_0_rmse: 0.55559 | val_1_rmse: 0.55942 |  0:03:34s
epoch 11 | loss: 0.31459 | val_0_rmse: 0.55591 | val_1_rmse: 0.55955 |  0:03:53s
epoch 12 | loss: 0.31344 | val_0_rmse: 0.5486  | val_1_rmse: 0.55285 |  0:04:12s
epoch 13 | loss: 0.30663 | val_0_rmse: 0.57035 | val_1_rmse: 0.57479 |  0:04:32s
epoch 14 | loss: 0.33221 | val_0_rmse: 0.56079 | val_1_rmse: 0.56691 |  0:04:51s
epoch 15 | loss: 0.32083 | val_0_rmse: 0.56053 | val_1_rmse: 0.56536 |  0:05:11s
epoch 16 | loss: 0.31195 | val_0_rmse: 0.54983 | val_1_rmse: 0.55399 |  0:05:30s
epoch 17 | loss: 0.30957 | val_0_rmse: 0.55193 | val_1_rmse: 0.5562  |  0:05:49s
epoch 18 | loss: 0.30362 | val_0_rmse: 0.55906 | val_1_rmse: 0.56511 |  0:06:09s
epoch 19 | loss: 0.3033  | val_0_rmse: 0.55221 | val_1_rmse: 0.55863 |  0:06:28s
epoch 20 | loss: 0.3005  | val_0_rmse: 0.57222 | val_1_rmse: 0.57674 |  0:06:48s
epoch 21 | loss: 0.2988  | val_0_rmse: 0.55659 | val_1_rmse: 0.56265 |  0:07:07s
epoch 22 | loss: 0.29857 | val_0_rmse: 0.56939 | val_1_rmse: 0.57499 |  0:07:27s
epoch 23 | loss: 0.31765 | val_0_rmse: 0.58433 | val_1_rmse: 0.58933 |  0:07:47s
epoch 24 | loss: 0.32379 | val_0_rmse: 0.5555  | val_1_rmse: 0.56217 |  0:08:06s
epoch 25 | loss: 0.3231  | val_0_rmse: 0.61313 | val_1_rmse: 0.61744 |  0:08:26s
epoch 26 | loss: 0.321   | val_0_rmse: 0.56687 | val_1_rmse: 0.57353 |  0:08:45s
epoch 27 | loss: 0.31517 | val_0_rmse: 0.55265 | val_1_rmse: 0.56031 |  0:09:05s
epoch 28 | loss: 0.31119 | val_0_rmse: 0.56665 | val_1_rmse: 0.56959 |  0:09:25s
epoch 29 | loss: 0.31533 | val_0_rmse: 0.62409 | val_1_rmse: 0.62916 |  0:09:44s
epoch 30 | loss: 0.31577 | val_0_rmse: 0.56951 | val_1_rmse: 0.57757 |  0:10:04s
epoch 31 | loss: 0.30203 | val_0_rmse: 0.5517  | val_1_rmse: 0.55855 |  0:10:24s
epoch 32 | loss: 0.29573 | val_0_rmse: 0.53172 | val_1_rmse: 0.53752 |  0:10:43s
epoch 33 | loss: 0.29495 | val_0_rmse: 0.55052 | val_1_rmse: 0.55688 |  0:11:03s
epoch 34 | loss: 0.29245 | val_0_rmse: 0.53475 | val_1_rmse: 0.54266 |  0:11:22s
epoch 35 | loss: 0.29945 | val_0_rmse: 0.53559 | val_1_rmse: 0.54156 |  0:11:42s
epoch 36 | loss: 0.29508 | val_0_rmse: 0.55339 | val_1_rmse: 0.55764 |  0:12:02s
epoch 37 | loss: 0.29239 | val_0_rmse: 0.58427 | val_1_rmse: 0.58968 |  0:12:21s
epoch 38 | loss: 0.29131 | val_0_rmse: 0.54139 | val_1_rmse: 0.54652 |  0:12:41s
epoch 39 | loss: 0.29009 | val_0_rmse: 0.53544 | val_1_rmse: 0.54161 |  0:13:00s
epoch 40 | loss: 0.28943 | val_0_rmse: 0.55083 | val_1_rmse: 0.55796 |  0:13:19s
epoch 41 | loss: 0.28987 | val_0_rmse: 0.53785 | val_1_rmse: 0.54368 |  0:13:39s
epoch 42 | loss: 0.2894  | val_0_rmse: 0.54201 | val_1_rmse: 0.54928 |  0:13:59s
epoch 43 | loss: 0.28846 | val_0_rmse: 0.53125 | val_1_rmse: 0.5377  |  0:14:18s
epoch 44 | loss: 0.28782 | val_0_rmse: 0.53231 | val_1_rmse: 0.53917 |  0:14:38s
epoch 45 | loss: 0.28854 | val_0_rmse: 0.59795 | val_1_rmse: 0.60412 |  0:14:57s
epoch 46 | loss: 0.28725 | val_0_rmse: 0.53584 | val_1_rmse: 0.54126 |  0:15:17s
epoch 47 | loss: 0.28677 | val_0_rmse: 0.60882 | val_1_rmse: 0.61183 |  0:15:36s
epoch 48 | loss: 0.28658 | val_0_rmse: 0.54546 | val_1_rmse: 0.55252 |  0:15:56s
epoch 49 | loss: 0.28786 | val_0_rmse: 0.5614  | val_1_rmse: 0.56913 |  0:16:15s
epoch 50 | loss: 0.28691 | val_0_rmse: 0.5584  | val_1_rmse: 0.56333 |  0:16:35s
epoch 51 | loss: 0.28542 | val_0_rmse: 0.5349  | val_1_rmse: 0.5419  |  0:16:54s
epoch 52 | loss: 0.2898  | val_0_rmse: 0.54078 | val_1_rmse: 0.54867 |  0:17:14s
epoch 53 | loss: 0.2862  | val_0_rmse: 0.54666 | val_1_rmse: 0.55562 |  0:17:33s
epoch 54 | loss: 0.28458 | val_0_rmse: 0.54526 | val_1_rmse: 0.55395 |  0:17:53s
epoch 55 | loss: 0.28598 | val_0_rmse: 0.58864 | val_1_rmse: 0.59234 |  0:18:12s
epoch 56 | loss: 0.28521 | val_0_rmse: 0.55054 | val_1_rmse: 0.55981 |  0:18:32s
epoch 57 | loss: 0.28426 | val_0_rmse: 0.53173 | val_1_rmse: 0.5397  |  0:18:51s
epoch 58 | loss: 0.28423 | val_0_rmse: 0.53062 | val_1_rmse: 0.53831 |  0:19:11s
epoch 59 | loss: 0.2865  | val_0_rmse: 0.52788 | val_1_rmse: 0.53424 |  0:19:30s
epoch 60 | loss: 0.28378 | val_0_rmse: 0.54326 | val_1_rmse: 0.54904 |  0:19:50s
epoch 61 | loss: 0.28461 | val_0_rmse: 0.53469 | val_1_rmse: 0.54239 |  0:20:09s
epoch 62 | loss: 0.28414 | val_0_rmse: 0.61589 | val_1_rmse: 0.61811 |  0:20:29s
epoch 63 | loss: 0.28423 | val_0_rmse: 0.5275  | val_1_rmse: 0.53505 |  0:20:48s
epoch 64 | loss: 0.28309 | val_0_rmse: 0.53276 | val_1_rmse: 0.54101 |  0:21:08s
epoch 65 | loss: 0.28468 | val_0_rmse: 0.52815 | val_1_rmse: 0.53624 |  0:21:27s
epoch 66 | loss: 0.28311 | val_0_rmse: 0.57169 | val_1_rmse: 0.57887 |  0:21:47s
epoch 67 | loss: 0.28374 | val_0_rmse: 0.52499 | val_1_rmse: 0.53294 |  0:22:06s
epoch 68 | loss: 0.28295 | val_0_rmse: 0.5348  | val_1_rmse: 0.5422  |  0:22:26s
epoch 69 | loss: 0.2827  | val_0_rmse: 0.53518 | val_1_rmse: 0.54216 |  0:22:45s
epoch 70 | loss: 0.28283 | val_0_rmse: 0.53048 | val_1_rmse: 0.53798 |  0:23:04s
epoch 71 | loss: 0.28354 | val_0_rmse: 0.57423 | val_1_rmse: 0.57948 |  0:23:24s
epoch 72 | loss: 0.28312 | val_0_rmse: 0.52844 | val_1_rmse: 0.53705 |  0:23:43s
epoch 73 | loss: 0.2824  | val_0_rmse: 0.53014 | val_1_rmse: 0.53829 |  0:24:03s
epoch 74 | loss: 0.28395 | val_0_rmse: 0.5338  | val_1_rmse: 0.54222 |  0:24:22s
epoch 75 | loss: 0.28232 | val_0_rmse: 0.52491 | val_1_rmse: 0.53326 |  0:24:42s
epoch 76 | loss: 0.2811  | val_0_rmse: 0.57278 | val_1_rmse: 0.57729 |  0:25:01s
epoch 77 | loss: 0.2821  | val_0_rmse: 0.54044 | val_1_rmse: 0.54946 |  0:25:21s
epoch 78 | loss: 0.28288 | val_0_rmse: 0.53883 | val_1_rmse: 0.54655 |  0:25:40s
epoch 79 | loss: 0.28208 | val_0_rmse: 0.54251 | val_1_rmse: 0.55227 |  0:26:00s
epoch 80 | loss: 0.28182 | val_0_rmse: 0.59859 | val_1_rmse: 0.60218 |  0:26:19s
epoch 81 | loss: 0.28221 | val_0_rmse: 0.53845 | val_1_rmse: 0.54681 |  0:26:39s
epoch 82 | loss: 0.2808  | val_0_rmse: 0.55106 | val_1_rmse: 0.55803 |  0:26:58s
epoch 83 | loss: 0.28269 | val_0_rmse: 0.52936 | val_1_rmse: 0.53713 |  0:27:18s
epoch 84 | loss: 0.28209 | val_0_rmse: 0.5304  | val_1_rmse: 0.53897 |  0:27:37s
epoch 85 | loss: 0.28081 | val_0_rmse: 0.5317  | val_1_rmse: 0.54133 |  0:27:57s
epoch 86 | loss: 0.28089 | val_0_rmse: 0.5244  | val_1_rmse: 0.53174 |  0:28:16s
epoch 87 | loss: 0.2809  | val_0_rmse: 0.54552 | val_1_rmse: 0.55348 |  0:28:36s
epoch 88 | loss: 0.28188 | val_0_rmse: 0.52735 | val_1_rmse: 0.53692 |  0:28:56s
epoch 89 | loss: 0.28025 | val_0_rmse: 0.54984 | val_1_rmse: 0.55497 |  0:29:16s
epoch 90 | loss: 0.28213 | val_0_rmse: 0.52385 | val_1_rmse: 0.53144 |  0:29:35s
epoch 91 | loss: 0.28037 | val_0_rmse: 0.71002 | val_1_rmse: 0.71917 |  0:29:55s
epoch 92 | loss: 0.28183 | val_0_rmse: 0.55014 | val_1_rmse: 0.55633 |  0:30:15s
epoch 93 | loss: 0.28168 | val_0_rmse: 0.53137 | val_1_rmse: 0.53962 |  0:30:34s
epoch 94 | loss: 0.27977 | val_0_rmse: 0.52612 | val_1_rmse: 0.53454 |  0:30:54s
epoch 95 | loss: 0.28094 | val_0_rmse: 0.52493 | val_1_rmse: 0.53445 |  0:31:14s
epoch 96 | loss: 0.27957 | val_0_rmse: 0.53095 | val_1_rmse: 0.53753 |  0:31:33s
epoch 97 | loss: 0.28056 | val_0_rmse: 0.55062 | val_1_rmse: 0.55919 |  0:31:52s
epoch 98 | loss: 0.28018 | val_0_rmse: 0.53386 | val_1_rmse: 0.54271 |  0:32:12s
epoch 99 | loss: 0.2809  | val_0_rmse: 0.53711 | val_1_rmse: 0.54571 |  0:32:31s
epoch 100| loss: 0.28012 | val_0_rmse: 0.52192 | val_1_rmse: 0.52961 |  0:32:51s
epoch 101| loss: 0.28062 | val_0_rmse: 0.52744 | val_1_rmse: 0.53559 |  0:33:10s
epoch 102| loss: 0.27923 | val_0_rmse: 0.55833 | val_1_rmse: 0.56636 |  0:33:29s
epoch 103| loss: 0.27964 | val_0_rmse: 0.55386 | val_1_rmse: 0.56117 |  0:33:49s
epoch 104| loss: 0.2794  | val_0_rmse: 0.5541  | val_1_rmse: 0.56369 |  0:34:08s
epoch 105| loss: 0.28074 | val_0_rmse: 0.56265 | val_1_rmse: 0.57153 |  0:34:28s
epoch 106| loss: 0.28025 | val_0_rmse: 0.62285 | val_1_rmse: 0.63264 |  0:34:47s
epoch 107| loss: 0.27946 | val_0_rmse: 0.56222 | val_1_rmse: 0.56775 |  0:35:06s
epoch 108| loss: 0.27932 | val_0_rmse: 0.53696 | val_1_rmse: 0.54392 |  0:35:26s
epoch 109| loss: 0.28012 | val_0_rmse: 0.52832 | val_1_rmse: 0.53706 |  0:35:45s
epoch 110| loss: 0.27947 | val_0_rmse: 0.58399 | val_1_rmse: 0.58927 |  0:36:05s
epoch 111| loss: 0.2803  | val_0_rmse: 0.54071 | val_1_rmse: 0.5491  |  0:36:24s
epoch 112| loss: 0.2799  | val_0_rmse: 0.53329 | val_1_rmse: 0.54029 |  0:36:43s
epoch 113| loss: 0.28006 | val_0_rmse: 0.52885 | val_1_rmse: 0.5378  |  0:37:03s
epoch 114| loss: 0.28023 | val_0_rmse: 0.52369 | val_1_rmse: 0.53135 |  0:37:22s
epoch 115| loss: 0.28233 | val_0_rmse: 0.52934 | val_1_rmse: 0.53765 |  0:37:42s
epoch 116| loss: 0.28    | val_0_rmse: 0.52972 | val_1_rmse: 0.53901 |  0:38:01s
epoch 117| loss: 0.27932 | val_0_rmse: 0.52748 | val_1_rmse: 0.5352  |  0:38:20s
epoch 118| loss: 0.27837 | val_0_rmse: 0.58167 | val_1_rmse: 0.58627 |  0:38:40s
epoch 119| loss: 0.2795  | val_0_rmse: 0.53632 | val_1_rmse: 0.54559 |  0:38:59s
epoch 120| loss: 0.27901 | val_0_rmse: 0.52371 | val_1_rmse: 0.53362 |  0:39:19s
epoch 121| loss: 0.27882 | val_0_rmse: 0.53932 | val_1_rmse: 0.54733 |  0:39:38s
epoch 122| loss: 0.27935 | val_0_rmse: 0.57108 | val_1_rmse: 0.58032 |  0:39:57s
epoch 123| loss: 0.2795  | val_0_rmse: 0.52854 | val_1_rmse: 0.53652 |  0:40:17s
epoch 124| loss: 0.27861 | val_0_rmse: 0.52529 | val_1_rmse: 0.53497 |  0:40:36s
epoch 125| loss: 0.27991 | val_0_rmse: 0.54353 | val_1_rmse: 0.54949 |  0:40:56s
epoch 126| loss: 0.27915 | val_0_rmse: 0.52523 | val_1_rmse: 0.53495 |  0:41:15s
epoch 127| loss: 0.27795 | val_0_rmse: 0.53299 | val_1_rmse: 0.53992 |  0:41:34s
epoch 128| loss: 0.27979 | val_0_rmse: 0.54138 | val_1_rmse: 0.5484  |  0:41:54s
epoch 129| loss: 0.27933 | val_0_rmse: 1.61233 | val_1_rmse: 1.61337 |  0:42:13s
epoch 130| loss: 0.29237 | val_0_rmse: 0.52875 | val_1_rmse: 0.53608 |  0:42:32s

Early stopping occured at epoch 130 with best_epoch = 100 and best_val_1_rmse = 0.52961
Best weights from best epoch are automatically used!
ended training at: 05:26:00
Feature importance:
[('Area', 0.1260460970412161), ('Baths', 0.0), ('Beds', 0.0691756797419929), ('Latitude', 0.4848802230538712), ('Longitude', 0.3198980001629198), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 10991364333.2451
Mean absolute error:65569.92686445551
MAPE:0.43341346193020336
R2 score:0.7219863296129871
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:26:02
epoch 0  | loss: 0.38239 | val_0_rmse: 0.57158 | val_1_rmse: 0.56684 |  0:00:19s
epoch 1  | loss: 0.32777 | val_0_rmse: 0.56133 | val_1_rmse: 0.5546  |  0:00:38s
epoch 2  | loss: 0.32026 | val_0_rmse: 0.55395 | val_1_rmse: 0.54893 |  0:00:57s
epoch 3  | loss: 0.31375 | val_0_rmse: 0.55488 | val_1_rmse: 0.54888 |  0:01:17s
epoch 4  | loss: 0.3157  | val_0_rmse: 0.54616 | val_1_rmse: 0.54165 |  0:01:37s
epoch 5  | loss: 0.3123  | val_0_rmse: 0.56063 | val_1_rmse: 0.55806 |  0:01:56s
epoch 6  | loss: 0.30971 | val_0_rmse: 0.55805 | val_1_rmse: 0.5561  |  0:02:15s
epoch 7  | loss: 0.30702 | val_0_rmse: 0.54854 | val_1_rmse: 0.54542 |  0:02:35s
epoch 8  | loss: 0.3038  | val_0_rmse: 0.55748 | val_1_rmse: 0.55561 |  0:02:54s
epoch 9  | loss: 0.30341 | val_0_rmse: 0.55018 | val_1_rmse: 0.54808 |  0:03:13s
epoch 10 | loss: 0.30298 | val_0_rmse: 0.57415 | val_1_rmse: 0.57173 |  0:03:33s
epoch 11 | loss: 0.30284 | val_0_rmse: 0.54912 | val_1_rmse: 0.54644 |  0:03:52s
epoch 12 | loss: 0.30023 | val_0_rmse: 0.54064 | val_1_rmse: 0.53703 |  0:04:12s
epoch 13 | loss: 0.30196 | val_0_rmse: 0.54497 | val_1_rmse: 0.54083 |  0:04:31s
epoch 14 | loss: 0.29924 | val_0_rmse: 0.54062 | val_1_rmse: 0.53703 |  0:04:51s
epoch 15 | loss: 0.29846 | val_0_rmse: 0.54522 | val_1_rmse: 0.54372 |  0:05:10s
epoch 16 | loss: 0.29779 | val_0_rmse: 0.54413 | val_1_rmse: 0.53965 |  0:05:29s
epoch 17 | loss: 0.2982  | val_0_rmse: 0.55621 | val_1_rmse: 0.5531  |  0:05:49s
epoch 18 | loss: 0.29706 | val_0_rmse: 0.54601 | val_1_rmse: 0.54376 |  0:06:08s
epoch 19 | loss: 0.30747 | val_0_rmse: 0.65324 | val_1_rmse: 0.65103 |  0:06:27s
epoch 20 | loss: 0.31132 | val_0_rmse: 0.56599 | val_1_rmse: 0.56313 |  0:06:47s
epoch 21 | loss: 0.30249 | val_0_rmse: 0.57061 | val_1_rmse: 0.56983 |  0:07:06s
epoch 22 | loss: 0.29972 | val_0_rmse: 0.55353 | val_1_rmse: 0.55073 |  0:07:26s
epoch 23 | loss: 0.29838 | val_0_rmse: 0.54629 | val_1_rmse: 0.54255 |  0:07:45s
epoch 24 | loss: 0.29732 | val_0_rmse: 0.56914 | val_1_rmse: 0.56693 |  0:08:05s
epoch 25 | loss: 0.29571 | val_0_rmse: 0.54307 | val_1_rmse: 0.54056 |  0:08:24s
epoch 26 | loss: 0.29638 | val_0_rmse: 0.54072 | val_1_rmse: 0.53676 |  0:08:44s
epoch 27 | loss: 0.29918 | val_0_rmse: 0.57714 | val_1_rmse: 0.57469 |  0:09:04s
epoch 28 | loss: 0.29566 | val_0_rmse: 0.56798 | val_1_rmse: 0.56535 |  0:09:23s
epoch 29 | loss: 0.2949  | val_0_rmse: 0.53742 | val_1_rmse: 0.53457 |  0:09:43s
epoch 30 | loss: 0.29378 | val_0_rmse: 0.57964 | val_1_rmse: 0.57863 |  0:10:02s
epoch 31 | loss: 0.29345 | val_0_rmse: 0.58707 | val_1_rmse: 0.58605 |  0:10:22s
epoch 32 | loss: 0.29342 | val_0_rmse: 0.54334 | val_1_rmse: 0.54097 |  0:10:41s
epoch 33 | loss: 0.29465 | val_0_rmse: 0.54551 | val_1_rmse: 0.54401 |  0:11:01s
epoch 34 | loss: 0.29564 | val_0_rmse: 0.56043 | val_1_rmse: 0.55943 |  0:11:20s
epoch 35 | loss: 0.29396 | val_0_rmse: 0.54685 | val_1_rmse: 0.54601 |  0:11:39s
epoch 36 | loss: 0.2948  | val_0_rmse: 0.53613 | val_1_rmse: 0.53398 |  0:11:59s
epoch 37 | loss: 0.2927  | val_0_rmse: 0.57939 | val_1_rmse: 0.5776  |  0:12:18s
epoch 38 | loss: 0.29255 | val_0_rmse: 0.53797 | val_1_rmse: 0.53532 |  0:12:37s
epoch 39 | loss: 0.29375 | val_0_rmse: 0.58073 | val_1_rmse: 0.5788  |  0:12:57s
epoch 40 | loss: 0.29253 | val_0_rmse: 0.58124 | val_1_rmse: 0.5805  |  0:13:16s
epoch 41 | loss: 0.29177 | val_0_rmse: 0.55609 | val_1_rmse: 0.5539  |  0:13:35s
epoch 42 | loss: 0.2908  | val_0_rmse: 0.594   | val_1_rmse: 0.59202 |  0:13:55s
epoch 43 | loss: 0.29052 | val_0_rmse: 0.56514 | val_1_rmse: 0.56444 |  0:14:14s
epoch 44 | loss: 0.29036 | val_0_rmse: 0.699   | val_1_rmse: 0.69821 |  0:14:33s
epoch 45 | loss: 0.29204 | val_0_rmse: 0.58844 | val_1_rmse: 0.58657 |  0:14:53s
epoch 46 | loss: 0.2912  | val_0_rmse: 0.59139 | val_1_rmse: 0.58918 |  0:15:12s
epoch 47 | loss: 0.29076 | val_0_rmse: 0.53863 | val_1_rmse: 0.53633 |  0:15:32s
epoch 48 | loss: 0.29015 | val_0_rmse: 0.53741 | val_1_rmse: 0.53604 |  0:15:51s
epoch 49 | loss: 0.28976 | val_0_rmse: 0.57199 | val_1_rmse: 0.57063 |  0:16:11s
epoch 50 | loss: 0.28964 | val_0_rmse: 0.54894 | val_1_rmse: 0.54636 |  0:16:30s
epoch 51 | loss: 0.29021 | val_0_rmse: 0.52983 | val_1_rmse: 0.52788 |  0:16:49s
epoch 52 | loss: 0.28912 | val_0_rmse: 0.54443 | val_1_rmse: 0.54309 |  0:17:08s
epoch 53 | loss: 0.28976 | val_0_rmse: 0.5427  | val_1_rmse: 0.54024 |  0:17:28s
epoch 54 | loss: 0.28981 | val_0_rmse: 0.55737 | val_1_rmse: 0.5561  |  0:17:47s
epoch 55 | loss: 0.2885  | val_0_rmse: 0.60773 | val_1_rmse: 0.60594 |  0:18:07s
epoch 56 | loss: 0.28871 | val_0_rmse: 0.53842 | val_1_rmse: 0.53639 |  0:18:26s
epoch 57 | loss: 0.28945 | val_0_rmse: 0.5454  | val_1_rmse: 0.5441  |  0:18:45s
epoch 58 | loss: 0.28862 | val_0_rmse: 0.57878 | val_1_rmse: 0.57585 |  0:19:05s
epoch 59 | loss: 0.28918 | val_0_rmse: 0.53998 | val_1_rmse: 0.53783 |  0:19:24s
epoch 60 | loss: 0.28836 | val_0_rmse: 0.5354  | val_1_rmse: 0.5331  |  0:19:43s
epoch 61 | loss: 0.2881  | val_0_rmse: 0.54847 | val_1_rmse: 0.54835 |  0:20:03s
epoch 62 | loss: 0.29132 | val_0_rmse: 0.55567 | val_1_rmse: 0.55487 |  0:20:22s
epoch 63 | loss: 0.28949 | val_0_rmse: 0.54056 | val_1_rmse: 0.53812 |  0:20:41s
epoch 64 | loss: 0.28911 | val_0_rmse: 0.56142 | val_1_rmse: 0.55962 |  0:21:01s
epoch 65 | loss: 0.28877 | val_0_rmse: 0.54622 | val_1_rmse: 0.5437  |  0:21:20s
epoch 66 | loss: 0.28844 | val_0_rmse: 0.55625 | val_1_rmse: 0.55525 |  0:21:39s
epoch 67 | loss: 0.28893 | val_0_rmse: 0.56392 | val_1_rmse: 0.5608  |  0:21:59s
epoch 68 | loss: 0.28808 | val_0_rmse: 0.53958 | val_1_rmse: 0.53789 |  0:22:18s
epoch 69 | loss: 0.28853 | val_0_rmse: 0.5449  | val_1_rmse: 0.54533 |  0:22:37s
epoch 70 | loss: 0.28798 | val_0_rmse: 0.55241 | val_1_rmse: 0.55048 |  0:22:57s
epoch 71 | loss: 0.28755 | val_0_rmse: 0.53923 | val_1_rmse: 0.53618 |  0:23:16s
epoch 72 | loss: 0.2875  | val_0_rmse: 0.55865 | val_1_rmse: 0.55684 |  0:23:35s
epoch 73 | loss: 0.28754 | val_0_rmse: 0.55034 | val_1_rmse: 0.54867 |  0:23:55s
epoch 74 | loss: 0.28812 | val_0_rmse: 0.58387 | val_1_rmse: 0.58293 |  0:24:14s
epoch 75 | loss: 0.29106 | val_0_rmse: 0.54503 | val_1_rmse: 0.54333 |  0:24:33s
epoch 76 | loss: 0.28987 | val_0_rmse: 0.55889 | val_1_rmse: 0.55876 |  0:24:53s
epoch 77 | loss: 0.28862 | val_0_rmse: 0.57556 | val_1_rmse: 0.57357 |  0:25:12s
epoch 78 | loss: 0.28753 | val_0_rmse: 0.55648 | val_1_rmse: 0.55594 |  0:25:31s
epoch 79 | loss: 0.28929 | val_0_rmse: 0.55969 | val_1_rmse: 0.55799 |  0:25:51s
epoch 80 | loss: 0.29972 | val_0_rmse: 0.54781 | val_1_rmse: 0.54722 |  0:26:10s
epoch 81 | loss: 0.29365 | val_0_rmse: 0.57627 | val_1_rmse: 0.57617 |  0:26:29s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.52788
Best weights from best epoch are automatically used!
ended training at: 05:52:37
Feature importance:
[('Area', 0.27954234705599146), ('Baths', 0.20154641081128868), ('Beds', 0.0), ('Latitude', 0.24293452875895039), ('Longitude', 0.27597671337376944), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 11072444409.228563
Mean absolute error:66116.72802099743
MAPE:0.4334897075870091
R2 score:0.7165643169644544
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 05:52:39
epoch 0  | loss: 0.37726 | val_0_rmse: 0.57385 | val_1_rmse: 0.57354 |  0:00:19s
epoch 1  | loss: 0.325   | val_0_rmse: 0.56487 | val_1_rmse: 0.5646  |  0:00:38s
epoch 2  | loss: 0.31603 | val_0_rmse: 0.55665 | val_1_rmse: 0.55736 |  0:00:57s
epoch 3  | loss: 0.32604 | val_0_rmse: 0.56322 | val_1_rmse: 0.56456 |  0:01:17s
epoch 4  | loss: 0.31316 | val_0_rmse: 0.54856 | val_1_rmse: 0.54921 |  0:01:36s
epoch 5  | loss: 0.3072  | val_0_rmse: 0.54976 | val_1_rmse: 0.55084 |  0:01:55s
epoch 6  | loss: 0.30687 | val_0_rmse: 0.54194 | val_1_rmse: 0.54369 |  0:02:15s
epoch 7  | loss: 0.30486 | val_0_rmse: 0.55082 | val_1_rmse: 0.55224 |  0:02:34s
epoch 8  | loss: 0.31201 | val_0_rmse: 0.55198 | val_1_rmse: 0.5534  |  0:02:53s
epoch 9  | loss: 0.30259 | val_0_rmse: 0.54312 | val_1_rmse: 0.5434  |  0:03:13s
epoch 10 | loss: 0.30116 | val_0_rmse: 0.55506 | val_1_rmse: 0.55631 |  0:03:32s
epoch 11 | loss: 0.29952 | val_0_rmse: 0.54608 | val_1_rmse: 0.54753 |  0:03:52s
epoch 12 | loss: 0.30126 | val_0_rmse: 0.55019 | val_1_rmse: 0.55233 |  0:04:11s
epoch 13 | loss: 0.30513 | val_0_rmse: 0.55096 | val_1_rmse: 0.551   |  0:04:31s
epoch 14 | loss: 0.34771 | val_0_rmse: 0.60048 | val_1_rmse: 0.60368 |  0:04:50s
epoch 15 | loss: 0.32323 | val_0_rmse: 0.55963 | val_1_rmse: 0.5603  |  0:05:10s
epoch 16 | loss: 0.30749 | val_0_rmse: 0.55445 | val_1_rmse: 0.55599 |  0:05:29s
epoch 17 | loss: 0.30594 | val_0_rmse: 0.5515  | val_1_rmse: 0.5509  |  0:05:49s
epoch 18 | loss: 0.3056  | val_0_rmse: 0.56613 | val_1_rmse: 0.56589 |  0:06:09s
epoch 19 | loss: 0.30084 | val_0_rmse: 0.54055 | val_1_rmse: 0.54163 |  0:06:28s
epoch 20 | loss: 0.29792 | val_0_rmse: 0.53885 | val_1_rmse: 0.53933 |  0:06:48s
epoch 21 | loss: 0.29673 | val_0_rmse: 0.54647 | val_1_rmse: 0.54771 |  0:07:07s
epoch 22 | loss: 0.29553 | val_0_rmse: 0.53958 | val_1_rmse: 0.53943 |  0:07:27s
epoch 23 | loss: 0.29663 | val_0_rmse: 0.54281 | val_1_rmse: 0.54351 |  0:07:46s
epoch 24 | loss: 0.29403 | val_0_rmse: 0.54018 | val_1_rmse: 0.54153 |  0:08:06s
epoch 25 | loss: 0.29436 | val_0_rmse: 0.54553 | val_1_rmse: 0.54715 |  0:08:25s
epoch 26 | loss: 0.29432 | val_0_rmse: 0.56189 | val_1_rmse: 0.56399 |  0:08:45s
epoch 27 | loss: 0.29462 | val_0_rmse: 0.53747 | val_1_rmse: 0.53789 |  0:09:05s
epoch 28 | loss: 0.29398 | val_0_rmse: 0.55163 | val_1_rmse: 0.55299 |  0:09:24s
epoch 29 | loss: 0.29386 | val_0_rmse: 0.53701 | val_1_rmse: 0.53777 |  0:09:44s
epoch 30 | loss: 0.29458 | val_0_rmse: 0.53973 | val_1_rmse: 0.54192 |  0:10:03s
epoch 31 | loss: 0.29275 | val_0_rmse: 0.54066 | val_1_rmse: 0.54169 |  0:10:23s
epoch 32 | loss: 0.29662 | val_0_rmse: 0.54238 | val_1_rmse: 0.5437  |  0:10:42s
epoch 33 | loss: 0.29334 | val_0_rmse: 0.53705 | val_1_rmse: 0.5385  |  0:11:02s
epoch 34 | loss: 0.2922  | val_0_rmse: 0.53832 | val_1_rmse: 0.5393  |  0:11:21s
epoch 35 | loss: 0.29137 | val_0_rmse: 0.54066 | val_1_rmse: 0.54273 |  0:11:41s
epoch 36 | loss: 0.29191 | val_0_rmse: 0.53765 | val_1_rmse: 0.53933 |  0:12:01s
epoch 37 | loss: 0.29073 | val_0_rmse: 0.5375  | val_1_rmse: 0.53886 |  0:12:20s
epoch 38 | loss: 0.29271 | val_0_rmse: 0.54844 | val_1_rmse: 0.54982 |  0:12:40s
epoch 39 | loss: 0.29024 | val_0_rmse: 0.53209 | val_1_rmse: 0.5334  |  0:12:59s
epoch 40 | loss: 0.29314 | val_0_rmse: 0.55998 | val_1_rmse: 0.56181 |  0:13:18s
epoch 41 | loss: 0.29188 | val_0_rmse: 0.54449 | val_1_rmse: 0.54602 |  0:13:37s
epoch 42 | loss: 0.29118 | val_0_rmse: 0.54351 | val_1_rmse: 0.54516 |  0:13:57s
epoch 43 | loss: 0.29539 | val_0_rmse: 0.5437  | val_1_rmse: 0.5456  |  0:14:16s
epoch 44 | loss: 0.29289 | val_0_rmse: 0.53786 | val_1_rmse: 0.53919 |  0:14:36s
epoch 45 | loss: 0.29128 | val_0_rmse: 0.56008 | val_1_rmse: 0.56265 |  0:14:55s
epoch 46 | loss: 0.29113 | val_0_rmse: 0.53878 | val_1_rmse: 0.54057 |  0:15:15s
epoch 47 | loss: 0.29189 | val_0_rmse: 0.54627 | val_1_rmse: 0.54908 |  0:15:34s
epoch 48 | loss: 0.29152 | val_0_rmse: 0.55843 | val_1_rmse: 0.55977 |  0:15:54s
epoch 49 | loss: 0.29095 | val_0_rmse: 0.53212 | val_1_rmse: 0.53411 |  0:16:14s
epoch 50 | loss: 0.29063 | val_0_rmse: 0.53177 | val_1_rmse: 0.53357 |  0:16:33s
epoch 51 | loss: 0.2929  | val_0_rmse: 0.54754 | val_1_rmse: 0.54869 |  0:16:53s
epoch 52 | loss: 0.29202 | val_0_rmse: 0.54517 | val_1_rmse: 0.54633 |  0:17:12s
epoch 53 | loss: 0.29033 | val_0_rmse: 0.5418  | val_1_rmse: 0.54311 |  0:17:32s
epoch 54 | loss: 0.31612 | val_0_rmse: 0.55136 | val_1_rmse: 0.55321 |  0:17:52s
epoch 55 | loss: 0.30934 | val_0_rmse: 0.56251 | val_1_rmse: 0.56468 |  0:18:11s
epoch 56 | loss: 0.30916 | val_0_rmse: 0.56042 | val_1_rmse: 0.56183 |  0:18:31s
epoch 57 | loss: 0.30844 | val_0_rmse: 0.55342 | val_1_rmse: 0.5554  |  0:18:50s
epoch 58 | loss: 0.31962 | val_0_rmse: 0.56637 | val_1_rmse: 0.56885 |  0:19:10s
epoch 59 | loss: 0.3083  | val_0_rmse: 0.56869 | val_1_rmse: 0.57104 |  0:19:29s
epoch 60 | loss: 0.30863 | val_0_rmse: 0.55546 | val_1_rmse: 0.55788 |  0:19:49s
epoch 61 | loss: 0.30857 | val_0_rmse: 0.55228 | val_1_rmse: 0.55499 |  0:20:09s
epoch 62 | loss: 0.30837 | val_0_rmse: 0.55267 | val_1_rmse: 0.55412 |  0:20:28s
epoch 63 | loss: 0.30855 | val_0_rmse: 0.55515 | val_1_rmse: 0.5568  |  0:20:48s
epoch 64 | loss: 0.30812 | val_0_rmse: 0.55021 | val_1_rmse: 0.55167 |  0:21:07s
epoch 65 | loss: 0.30943 | val_0_rmse: 0.56011 | val_1_rmse: 0.56223 |  0:21:27s
epoch 66 | loss: 0.30785 | val_0_rmse: 0.55155 | val_1_rmse: 0.5535  |  0:21:47s
epoch 67 | loss: 0.31095 | val_0_rmse: 0.55779 | val_1_rmse: 0.56043 |  0:22:06s
epoch 68 | loss: 0.30753 | val_0_rmse: 0.56958 | val_1_rmse: 0.57155 |  0:22:26s
epoch 69 | loss: 0.30738 | val_0_rmse: 0.55592 | val_1_rmse: 0.55801 |  0:22:45s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.5334
Best weights from best epoch are automatically used!
ended training at: 06:15:30
Feature importance:
[('Area', 0.3243140376196559), ('Baths', 0.1911660739614402), ('Beds', 0.0), ('Latitude', 0.1261133800713585), ('Longitude', 0.1430807172400056), ('Month', 0.0707480841725127), ('Year', 0.1445777069350271)]
Mean squared error is of 11081249110.465141
Mean absolute error:65868.88562899744
MAPE:0.44024949706461775
R2 score:0.716462982296913
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Tab Net params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 06:15:32
epoch 0  | loss: 0.37575 | val_0_rmse: 0.5833  | val_1_rmse: 0.5841  |  0:00:19s
epoch 1  | loss: 0.32827 | val_0_rmse: 0.55627 | val_1_rmse: 0.55464 |  0:00:39s
epoch 2  | loss: 0.31959 | val_0_rmse: 0.55475 | val_1_rmse: 0.55373 |  0:00:58s
epoch 3  | loss: 0.3162  | val_0_rmse: 0.56145 | val_1_rmse: 0.5584  |  0:01:18s
epoch 4  | loss: 0.31291 | val_0_rmse: 0.55299 | val_1_rmse: 0.55218 |  0:01:37s
epoch 5  | loss: 0.30948 | val_0_rmse: 0.54514 | val_1_rmse: 0.54351 |  0:01:57s
epoch 6  | loss: 0.30812 | val_0_rmse: 0.55901 | val_1_rmse: 0.55402 |  0:02:16s
epoch 7  | loss: 0.30634 | val_0_rmse: 0.54591 | val_1_rmse: 0.54209 |  0:02:36s
epoch 8  | loss: 0.30303 | val_0_rmse: 0.5514  | val_1_rmse: 0.54877 |  0:02:55s
epoch 9  | loss: 0.30205 | val_0_rmse: 0.55916 | val_1_rmse: 0.55801 |  0:03:15s
epoch 10 | loss: 0.29974 | val_0_rmse: 0.56029 | val_1_rmse: 0.5591  |  0:03:34s
epoch 11 | loss: 0.29755 | val_0_rmse: 0.55786 | val_1_rmse: 0.55804 |  0:03:54s
epoch 12 | loss: 0.29503 | val_0_rmse: 0.5382  | val_1_rmse: 0.53488 |  0:04:14s
epoch 13 | loss: 0.29478 | val_0_rmse: 0.54169 | val_1_rmse: 0.53858 |  0:04:33s
epoch 14 | loss: 0.29214 | val_0_rmse: 0.5362  | val_1_rmse: 0.53208 |  0:04:53s
epoch 15 | loss: 0.29329 | val_0_rmse: 0.53504 | val_1_rmse: 0.5319  |  0:05:12s
epoch 16 | loss: 0.29344 | val_0_rmse: 0.53603 | val_1_rmse: 0.53328 |  0:05:32s
epoch 17 | loss: 0.29267 | val_0_rmse: 0.56185 | val_1_rmse: 0.55703 |  0:05:51s
epoch 18 | loss: 0.29147 | val_0_rmse: 0.53745 | val_1_rmse: 0.53401 |  0:06:11s
epoch 19 | loss: 0.29038 | val_0_rmse: 0.54205 | val_1_rmse: 0.53782 |  0:06:30s
epoch 20 | loss: 0.29015 | val_0_rmse: 0.54501 | val_1_rmse: 0.54041 |  0:06:49s
epoch 21 | loss: 0.29047 | val_0_rmse: 0.53528 | val_1_rmse: 0.53352 |  0:07:08s
epoch 22 | loss: 0.28965 | val_0_rmse: 0.56136 | val_1_rmse: 0.5583  |  0:07:28s
epoch 23 | loss: 0.29183 | val_0_rmse: 0.54007 | val_1_rmse: 0.53644 |  0:07:47s
epoch 24 | loss: 0.28976 | val_0_rmse: 0.5337  | val_1_rmse: 0.53205 |  0:08:06s
epoch 25 | loss: 0.29073 | val_0_rmse: 0.57634 | val_1_rmse: 0.57033 |  0:08:26s
epoch 26 | loss: 0.29163 | val_0_rmse: 0.54738 | val_1_rmse: 0.54121 |  0:08:45s
epoch 27 | loss: 0.29029 | val_0_rmse: 0.53355 | val_1_rmse: 0.53071 |  0:09:05s
epoch 28 | loss: 0.28845 | val_0_rmse: 0.53622 | val_1_rmse: 0.53195 |  0:09:24s
epoch 29 | loss: 0.28867 | val_0_rmse: 0.54604 | val_1_rmse: 0.54281 |  0:09:43s
epoch 30 | loss: 0.28882 | val_0_rmse: 0.53513 | val_1_rmse: 0.53368 |  0:10:03s
epoch 31 | loss: 0.28681 | val_0_rmse: 0.53084 | val_1_rmse: 0.52893 |  0:10:22s
epoch 32 | loss: 0.29001 | val_0_rmse: 0.54363 | val_1_rmse: 0.54124 |  0:10:41s
epoch 33 | loss: 0.28703 | val_0_rmse: 0.53157 | val_1_rmse: 0.52836 |  0:11:01s
epoch 34 | loss: 0.28735 | val_0_rmse: 0.52995 | val_1_rmse: 0.5274  |  0:11:20s
epoch 35 | loss: 0.28746 | val_0_rmse: 0.5338  | val_1_rmse: 0.53129 |  0:11:39s
epoch 36 | loss: 0.28638 | val_0_rmse: 0.5398  | val_1_rmse: 0.53514 |  0:11:59s
epoch 37 | loss: 0.28821 | val_0_rmse: 0.53355 | val_1_rmse: 0.53148 |  0:12:18s
epoch 38 | loss: 0.28636 | val_0_rmse: 0.58965 | val_1_rmse: 0.58338 |  0:12:37s
epoch 39 | loss: 0.28696 | val_0_rmse: 0.54883 | val_1_rmse: 0.54504 |  0:12:57s
epoch 40 | loss: 0.28688 | val_0_rmse: 0.52967 | val_1_rmse: 0.52728 |  0:13:16s
epoch 41 | loss: 0.28754 | val_0_rmse: 0.53974 | val_1_rmse: 0.53826 |  0:13:35s
epoch 42 | loss: 0.2855  | val_0_rmse: 0.5357  | val_1_rmse: 0.53211 |  0:13:55s
epoch 43 | loss: 0.2872  | val_0_rmse: 0.54541 | val_1_rmse: 0.54061 |  0:14:14s
epoch 44 | loss: 0.28546 | val_0_rmse: 0.54449 | val_1_rmse: 0.54069 |  0:14:33s
epoch 45 | loss: 0.28576 | val_0_rmse: 0.52855 | val_1_rmse: 0.52632 |  0:14:53s
epoch 46 | loss: 0.28505 | val_0_rmse: 0.52595 | val_1_rmse: 0.52321 |  0:15:12s
epoch 47 | loss: 0.28567 | val_0_rmse: 0.54158 | val_1_rmse: 0.53796 |  0:15:31s
epoch 48 | loss: 0.28514 | val_0_rmse: 0.53393 | val_1_rmse: 0.53234 |  0:15:51s
epoch 49 | loss: 0.2847  | val_0_rmse: 0.53576 | val_1_rmse: 0.53553 |  0:16:10s
epoch 50 | loss: 0.28645 | val_0_rmse: 0.54686 | val_1_rmse: 0.54302 |  0:16:29s
epoch 51 | loss: 0.28545 | val_0_rmse: 0.53555 | val_1_rmse: 0.53212 |  0:16:49s
epoch 52 | loss: 0.28529 | val_0_rmse: 0.53437 | val_1_rmse: 0.53058 |  0:17:08s
epoch 53 | loss: 0.2841  | val_0_rmse: 0.53433 | val_1_rmse: 0.53121 |  0:17:27s
epoch 54 | loss: 0.2834  | val_0_rmse: 0.52912 | val_1_rmse: 0.52628 |  0:17:47s
epoch 55 | loss: 0.28494 | val_0_rmse: 0.52813 | val_1_rmse: 0.52583 |  0:18:06s
epoch 56 | loss: 0.28628 | val_0_rmse: 0.52784 | val_1_rmse: 0.5254  |  0:18:25s
epoch 57 | loss: 0.28476 | val_0_rmse: 0.52973 | val_1_rmse: 0.52666 |  0:18:45s
epoch 58 | loss: 0.28488 | val_0_rmse: 0.53407 | val_1_rmse: 0.5301  |  0:19:04s
epoch 59 | loss: 0.28432 | val_0_rmse: 0.53208 | val_1_rmse: 0.5307  |  0:19:24s
epoch 60 | loss: 0.28487 | val_0_rmse: 0.53194 | val_1_rmse: 0.5283  |  0:19:43s
epoch 61 | loss: 0.28448 | val_0_rmse: 0.53254 | val_1_rmse: 0.52935 |  0:20:02s
epoch 62 | loss: 0.28401 | val_0_rmse: 0.53997 | val_1_rmse: 0.53726 |  0:20:22s
epoch 63 | loss: 0.28363 | val_0_rmse: 0.53097 | val_1_rmse: 0.52848 |  0:20:41s
epoch 64 | loss: 0.28352 | val_0_rmse: 0.52733 | val_1_rmse: 0.52474 |  0:21:00s
epoch 65 | loss: 0.28356 | val_0_rmse: 0.53643 | val_1_rmse: 0.53371 |  0:21:19s
epoch 66 | loss: 0.28376 | val_0_rmse: 0.53253 | val_1_rmse: 0.529   |  0:21:39s
epoch 67 | loss: 0.28383 | val_0_rmse: 0.52893 | val_1_rmse: 0.52676 |  0:21:58s
epoch 68 | loss: 0.28274 | val_0_rmse: 0.53248 | val_1_rmse: 0.52951 |  0:22:18s
epoch 69 | loss: 0.28275 | val_0_rmse: 0.52953 | val_1_rmse: 0.52629 |  0:22:37s
epoch 70 | loss: 0.28334 | val_0_rmse: 0.53056 | val_1_rmse: 0.52814 |  0:22:56s
epoch 71 | loss: 0.28286 | val_0_rmse: 0.58188 | val_1_rmse: 0.57701 |  0:23:16s
epoch 72 | loss: 0.2831  | val_0_rmse: 0.53463 | val_1_rmse: 0.53094 |  0:23:35s
epoch 73 | loss: 0.28204 | val_0_rmse: 0.53273 | val_1_rmse: 0.53189 |  0:23:54s
epoch 74 | loss: 0.28193 | val_0_rmse: 0.52861 | val_1_rmse: 0.52735 |  0:24:14s
epoch 75 | loss: 0.28419 | val_0_rmse: 0.53732 | val_1_rmse: 0.53433 |  0:24:33s
epoch 76 | loss: 0.28436 | val_0_rmse: 0.53374 | val_1_rmse: 0.53102 |  0:24:52s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.52321
Best weights from best epoch are automatically used!
ended training at: 06:40:31
Feature importance:
[('Area', 0.41910329326635015), ('Baths', 0.17485582956195025), ('Beds', 0.009903173240550404), ('Latitude', 0.31306091252913293), ('Longitude', 0.0005515577821877076), ('Month', 0.0), ('Year', 0.08252523361982851)]
Mean squared error is of 10993210510.001955
Mean absolute error:65434.021547169585
MAPE:0.43923829845673235
R2 score:0.7190985618451199
------------------------------------------------------------------
