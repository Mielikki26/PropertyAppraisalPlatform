TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:18:42
epoch 0  | loss: 0.80364 | val_0_rmse: 0.81442 | val_1_rmse: 0.81089 |  0:00:04s
epoch 1  | loss: 0.63812 | val_0_rmse: 0.78672 | val_1_rmse: 0.79623 |  0:00:06s
epoch 2  | loss: 0.56545 | val_0_rmse: 0.7465  | val_1_rmse: 0.74165 |  0:00:08s
epoch 3  | loss: 0.52874 | val_0_rmse: 0.70584 | val_1_rmse: 0.7119  |  0:00:09s
epoch 4  | loss: 0.49607 | val_0_rmse: 0.67415 | val_1_rmse: 0.67563 |  0:00:11s
epoch 5  | loss: 0.46218 | val_0_rmse: 0.66669 | val_1_rmse: 0.67291 |  0:00:13s
epoch 6  | loss: 0.46795 | val_0_rmse: 0.66831 | val_1_rmse: 0.67653 |  0:00:15s
epoch 7  | loss: 0.44213 | val_0_rmse: 0.63711 | val_1_rmse: 0.64315 |  0:00:17s
epoch 8  | loss: 0.43938 | val_0_rmse: 0.69578 | val_1_rmse: 0.70378 |  0:00:19s
epoch 9  | loss: 0.4265  | val_0_rmse: 0.64073 | val_1_rmse: 0.64693 |  0:00:21s
epoch 10 | loss: 0.42002 | val_0_rmse: 0.61723 | val_1_rmse: 0.62095 |  0:00:23s
epoch 11 | loss: 0.41859 | val_0_rmse: 0.63535 | val_1_rmse: 0.63111 |  0:00:25s
epoch 12 | loss: 0.41754 | val_0_rmse: 0.62626 | val_1_rmse: 0.63146 |  0:00:27s
epoch 13 | loss: 0.41349 | val_0_rmse: 0.6339  | val_1_rmse: 0.6381  |  0:00:29s
epoch 14 | loss: 0.42397 | val_0_rmse: 0.62296 | val_1_rmse: 0.62846 |  0:00:31s
epoch 15 | loss: 0.40784 | val_0_rmse: 0.73253 | val_1_rmse: 0.74522 |  0:00:33s
epoch 16 | loss: 0.4078  | val_0_rmse: 0.61103 | val_1_rmse: 0.6177  |  0:00:35s
epoch 17 | loss: 0.40026 | val_0_rmse: 0.60811 | val_1_rmse: 0.61468 |  0:00:37s
epoch 18 | loss: 0.39733 | val_0_rmse: 0.59346 | val_1_rmse: 0.60039 |  0:00:39s
epoch 19 | loss: 0.38647 | val_0_rmse: 0.60213 | val_1_rmse: 0.60778 |  0:00:41s
epoch 20 | loss: 0.39805 | val_0_rmse: 0.64464 | val_1_rmse: 0.64478 |  0:00:43s
epoch 21 | loss: 0.41616 | val_0_rmse: 0.65961 | val_1_rmse: 0.66433 |  0:00:45s
epoch 22 | loss: 0.43618 | val_0_rmse: 0.67106 | val_1_rmse: 0.67221 |  0:00:47s
epoch 23 | loss: 0.46151 | val_0_rmse: 0.70822 | val_1_rmse: 0.70641 |  0:00:49s
epoch 24 | loss: 0.47337 | val_0_rmse: 0.66148 | val_1_rmse: 0.66531 |  0:00:51s
epoch 25 | loss: 0.43374 | val_0_rmse: 0.63208 | val_1_rmse: 0.64118 |  0:00:53s
epoch 26 | loss: 0.42563 | val_0_rmse: 0.68016 | val_1_rmse: 0.68241 |  0:00:55s
epoch 27 | loss: 0.41734 | val_0_rmse: 0.65997 | val_1_rmse: 0.6743  |  0:00:57s
epoch 28 | loss: 0.42171 | val_0_rmse: 0.67204 | val_1_rmse: 0.67884 |  0:00:59s
epoch 29 | loss: 0.41662 | val_0_rmse: 0.66461 | val_1_rmse: 0.66696 |  0:01:01s
epoch 30 | loss: 0.40826 | val_0_rmse: 0.65715 | val_1_rmse: 0.66368 |  0:01:03s
epoch 31 | loss: 0.39919 | val_0_rmse: 0.62924 | val_1_rmse: 0.63641 |  0:01:05s
epoch 32 | loss: 0.39689 | val_0_rmse: 0.76755 | val_1_rmse: 0.77276 |  0:01:07s
epoch 33 | loss: 0.39846 | val_0_rmse: 0.61705 | val_1_rmse: 0.63223 |  0:01:09s
epoch 34 | loss: 0.39131 | val_0_rmse: 0.64139 | val_1_rmse: 0.64954 |  0:01:11s
epoch 35 | loss: 0.38462 | val_0_rmse: 0.65405 | val_1_rmse: 0.67159 |  0:01:13s
epoch 36 | loss: 0.4054  | val_0_rmse: 0.92955 | val_1_rmse: 0.94489 |  0:01:15s
epoch 37 | loss: 0.3931  | val_0_rmse: 0.83703 | val_1_rmse: 0.83878 |  0:01:17s
epoch 38 | loss: 0.40163 | val_0_rmse: 0.69946 | val_1_rmse: 0.69252 |  0:01:19s
epoch 39 | loss: 0.40262 | val_0_rmse: 0.67197 | val_1_rmse: 0.67204 |  0:01:21s
epoch 40 | loss: 0.38978 | val_0_rmse: 0.58785 | val_1_rmse: 0.60038 |  0:01:23s
epoch 41 | loss: 0.38594 | val_0_rmse: 0.60074 | val_1_rmse: 0.61166 |  0:01:25s
epoch 42 | loss: 0.37948 | val_0_rmse: 0.63837 | val_1_rmse: 0.64508 |  0:01:27s
epoch 43 | loss: 0.37995 | val_0_rmse: 0.60642 | val_1_rmse: 0.61814 |  0:01:29s
epoch 44 | loss: 0.37396 | val_0_rmse: 0.59901 | val_1_rmse: 0.60809 |  0:01:31s
epoch 45 | loss: 0.37908 | val_0_rmse: 0.58839 | val_1_rmse: 0.59976 |  0:01:33s
epoch 46 | loss: 0.38024 | val_0_rmse: 0.59614 | val_1_rmse: 0.6015  |  0:01:35s
epoch 47 | loss: 0.3696  | val_0_rmse: 0.59994 | val_1_rmse: 0.60774 |  0:01:37s
epoch 48 | loss: 0.36354 | val_0_rmse: 0.59862 | val_1_rmse: 0.60467 |  0:01:39s
epoch 49 | loss: 0.36561 | val_0_rmse: 0.68162 | val_1_rmse: 0.69736 |  0:01:41s
epoch 50 | loss: 0.37537 | val_0_rmse: 0.5884  | val_1_rmse: 0.59938 |  0:01:43s
epoch 51 | loss: 0.36244 | val_0_rmse: 0.57464 | val_1_rmse: 0.58565 |  0:01:45s
epoch 52 | loss: 0.35869 | val_0_rmse: 0.58395 | val_1_rmse: 0.59064 |  0:01:47s
epoch 53 | loss: 0.38129 | val_0_rmse: 0.59356 | val_1_rmse: 0.60177 |  0:01:49s
epoch 54 | loss: 0.36929 | val_0_rmse: 0.59134 | val_1_rmse: 0.60346 |  0:01:51s
epoch 55 | loss: 0.36736 | val_0_rmse: 0.58555 | val_1_rmse: 0.59428 |  0:01:53s
epoch 56 | loss: 0.36388 | val_0_rmse: 0.58542 | val_1_rmse: 0.59766 |  0:01:55s
epoch 57 | loss: 0.35698 | val_0_rmse: 0.57889 | val_1_rmse: 0.59145 |  0:01:56s
epoch 58 | loss: 0.36143 | val_0_rmse: 0.57791 | val_1_rmse: 0.59392 |  0:01:58s
epoch 59 | loss: 0.36829 | val_0_rmse: 0.57798 | val_1_rmse: 0.59052 |  0:02:00s
epoch 60 | loss: 0.37207 | val_0_rmse: 0.60768 | val_1_rmse: 0.61603 |  0:02:02s
epoch 61 | loss: 0.36672 | val_0_rmse: 0.58506 | val_1_rmse: 0.59484 |  0:02:04s
epoch 62 | loss: 0.36165 | val_0_rmse: 0.58202 | val_1_rmse: 0.59551 |  0:02:06s
epoch 63 | loss: 0.35373 | val_0_rmse: 0.58183 | val_1_rmse: 0.59578 |  0:02:08s
epoch 64 | loss: 0.35374 | val_0_rmse: 0.57236 | val_1_rmse: 0.58564 |  0:02:10s
epoch 65 | loss: 0.35382 | val_0_rmse: 0.57728 | val_1_rmse: 0.59343 |  0:02:12s
epoch 66 | loss: 0.35116 | val_0_rmse: 0.56469 | val_1_rmse: 0.57728 |  0:02:14s
epoch 67 | loss: 0.35051 | val_0_rmse: 0.66815 | val_1_rmse: 0.68579 |  0:02:16s
epoch 68 | loss: 0.36315 | val_0_rmse: 0.56814 | val_1_rmse: 0.58135 |  0:02:18s
epoch 69 | loss: 0.34989 | val_0_rmse: 0.60507 | val_1_rmse: 0.6093  |  0:02:20s
epoch 70 | loss: 0.35817 | val_0_rmse: 0.56952 | val_1_rmse: 0.58212 |  0:02:22s
epoch 71 | loss: 0.35548 | val_0_rmse: 0.5911  | val_1_rmse: 0.60497 |  0:02:24s
epoch 72 | loss: 0.3559  | val_0_rmse: 0.60223 | val_1_rmse: 0.6106  |  0:02:26s
epoch 73 | loss: 0.34714 | val_0_rmse: 0.60185 | val_1_rmse: 0.6217  |  0:02:28s
epoch 74 | loss: 0.3503  | val_0_rmse: 0.56942 | val_1_rmse: 0.57989 |  0:02:30s
epoch 75 | loss: 0.34949 | val_0_rmse: 0.75819 | val_1_rmse: 0.76637 |  0:02:32s
epoch 76 | loss: 0.35507 | val_0_rmse: 0.62594 | val_1_rmse: 0.63326 |  0:02:34s
epoch 77 | loss: 0.36079 | val_0_rmse: 0.71768 | val_1_rmse: 0.72819 |  0:02:36s
epoch 78 | loss: 0.35641 | val_0_rmse: 0.58212 | val_1_rmse: 0.59427 |  0:02:38s
epoch 79 | loss: 0.359   | val_0_rmse: 0.57699 | val_1_rmse: 0.58723 |  0:02:40s
epoch 80 | loss: 0.35561 | val_0_rmse: 0.58818 | val_1_rmse: 0.60045 |  0:02:42s
epoch 81 | loss: 0.35052 | val_0_rmse: 0.56285 | val_1_rmse: 0.57751 |  0:02:44s
epoch 82 | loss: 0.34655 | val_0_rmse: 0.59929 | val_1_rmse: 0.6071  |  0:02:46s
epoch 83 | loss: 0.35679 | val_0_rmse: 0.57537 | val_1_rmse: 0.58603 |  0:02:48s
epoch 84 | loss: 0.35756 | val_0_rmse: 0.56749 | val_1_rmse: 0.58084 |  0:02:50s
epoch 85 | loss: 0.3432  | val_0_rmse: 0.57116 | val_1_rmse: 0.58889 |  0:02:52s
epoch 86 | loss: 0.3509  | val_0_rmse: 0.67572 | val_1_rmse: 0.69207 |  0:02:54s
epoch 87 | loss: 0.35892 | val_0_rmse: 0.59771 | val_1_rmse: 0.61488 |  0:02:56s
epoch 88 | loss: 0.36197 | val_0_rmse: 0.56053 | val_1_rmse: 0.5793  |  0:02:58s
epoch 89 | loss: 0.35292 | val_0_rmse: 0.57211 | val_1_rmse: 0.58824 |  0:03:00s
epoch 90 | loss: 0.35361 | val_0_rmse: 0.5976  | val_1_rmse: 0.62182 |  0:03:02s
epoch 91 | loss: 0.35999 | val_0_rmse: 0.58019 | val_1_rmse: 0.59231 |  0:03:04s
epoch 92 | loss: 0.34295 | val_0_rmse: 0.55768 | val_1_rmse: 0.57552 |  0:03:06s
epoch 93 | loss: 0.34604 | val_0_rmse: 0.58783 | val_1_rmse: 0.60549 |  0:03:08s
epoch 94 | loss: 0.36951 | val_0_rmse: 0.59459 | val_1_rmse: 0.60744 |  0:03:10s
epoch 95 | loss: 0.35488 | val_0_rmse: 0.6495  | val_1_rmse: 0.66358 |  0:03:12s
epoch 96 | loss: 0.36939 | val_0_rmse: 0.59726 | val_1_rmse: 0.6036  |  0:03:14s
epoch 97 | loss: 0.35611 | val_0_rmse: 0.64419 | val_1_rmse: 0.65993 |  0:03:16s
epoch 98 | loss: 0.34954 | val_0_rmse: 0.59527 | val_1_rmse: 0.61289 |  0:03:18s
epoch 99 | loss: 0.34613 | val_0_rmse: 0.59401 | val_1_rmse: 0.61053 |  0:03:20s
epoch 100| loss: 0.34554 | val_0_rmse: 0.60331 | val_1_rmse: 0.62332 |  0:03:22s
epoch 101| loss: 0.34808 | val_0_rmse: 0.56781 | val_1_rmse: 0.58415 |  0:03:24s
epoch 102| loss: 0.34537 | val_0_rmse: 0.57528 | val_1_rmse: 0.59311 |  0:03:26s
epoch 103| loss: 0.34839 | val_0_rmse: 0.63738 | val_1_rmse: 0.65846 |  0:03:28s
epoch 104| loss: 0.36216 | val_0_rmse: 0.66035 | val_1_rmse: 0.67183 |  0:03:30s
epoch 105| loss: 0.35365 | val_0_rmse: 0.58683 | val_1_rmse: 0.60661 |  0:03:32s
epoch 106| loss: 0.3524  | val_0_rmse: 0.60629 | val_1_rmse: 0.62477 |  0:03:34s
epoch 107| loss: 0.34701 | val_0_rmse: 0.56987 | val_1_rmse: 0.58925 |  0:03:36s
epoch 108| loss: 0.34043 | val_0_rmse: 0.58179 | val_1_rmse: 0.59205 |  0:03:38s
epoch 109| loss: 0.33894 | val_0_rmse: 0.59135 | val_1_rmse: 0.61586 |  0:03:40s
epoch 110| loss: 0.3417  | val_0_rmse: 0.564   | val_1_rmse: 0.58088 |  0:03:42s
epoch 111| loss: 0.33821 | val_0_rmse: 0.59265 | val_1_rmse: 0.60428 |  0:03:44s
epoch 112| loss: 0.34377 | val_0_rmse: 0.57113 | val_1_rmse: 0.58996 |  0:03:46s
epoch 113| loss: 0.35467 | val_0_rmse: 0.57652 | val_1_rmse: 0.59249 |  0:03:47s
epoch 114| loss: 0.34485 | val_0_rmse: 0.59762 | val_1_rmse: 0.61239 |  0:03:49s
epoch 115| loss: 0.34613 | val_0_rmse: 0.55908 | val_1_rmse: 0.57774 |  0:03:51s
epoch 116| loss: 0.34381 | val_0_rmse: 0.5498  | val_1_rmse: 0.56982 |  0:03:53s
epoch 117| loss: 0.34043 | val_0_rmse: 0.65233 | val_1_rmse: 0.6725  |  0:03:55s
epoch 118| loss: 0.34637 | val_0_rmse: 0.57688 | val_1_rmse: 0.58628 |  0:03:57s
epoch 119| loss: 0.37082 | val_0_rmse: 0.58887 | val_1_rmse: 0.5948  |  0:03:59s
epoch 120| loss: 0.36404 | val_0_rmse: 0.57713 | val_1_rmse: 0.58944 |  0:04:01s
epoch 121| loss: 0.36075 | val_0_rmse: 0.62349 | val_1_rmse: 0.62548 |  0:04:03s
epoch 122| loss: 0.35776 | val_0_rmse: 0.59221 | val_1_rmse: 0.60342 |  0:04:05s
epoch 123| loss: 0.35223 | val_0_rmse: 0.55962 | val_1_rmse: 0.5742  |  0:04:07s
epoch 124| loss: 0.34583 | val_0_rmse: 0.61826 | val_1_rmse: 0.63604 |  0:04:09s
epoch 125| loss: 0.35377 | val_0_rmse: 0.56829 | val_1_rmse: 0.57704 |  0:04:11s
epoch 126| loss: 0.35195 | val_0_rmse: 0.60379 | val_1_rmse: 0.61188 |  0:04:13s
epoch 127| loss: 0.35853 | val_0_rmse: 0.57343 | val_1_rmse: 0.58488 |  0:04:15s
epoch 128| loss: 0.34606 | val_0_rmse: 0.57751 | val_1_rmse: 0.5876  |  0:04:18s
epoch 129| loss: 0.35008 | val_0_rmse: 0.57228 | val_1_rmse: 0.58276 |  0:04:19s
epoch 130| loss: 0.36353 | val_0_rmse: 0.65551 | val_1_rmse: 0.66313 |  0:04:21s
epoch 131| loss: 0.35669 | val_0_rmse: 0.59301 | val_1_rmse: 0.60513 |  0:04:23s
epoch 132| loss: 0.34995 | val_0_rmse: 0.56372 | val_1_rmse: 0.57906 |  0:04:25s
epoch 133| loss: 0.3554  | val_0_rmse: 0.65433 | val_1_rmse: 0.65954 |  0:04:27s
epoch 134| loss: 0.35438 | val_0_rmse: 0.57996 | val_1_rmse: 0.58957 |  0:04:29s
epoch 135| loss: 0.34967 | val_0_rmse: 0.55289 | val_1_rmse: 0.56091 |  0:04:31s
epoch 136| loss: 0.35698 | val_0_rmse: 0.64466 | val_1_rmse: 0.65435 |  0:04:33s
epoch 137| loss: 0.36256 | val_0_rmse: 0.60252 | val_1_rmse: 0.60648 |  0:04:35s
epoch 138| loss: 0.358   | val_0_rmse: 0.5748  | val_1_rmse: 0.58647 |  0:04:37s
epoch 139| loss: 0.34761 | val_0_rmse: 0.55793 | val_1_rmse: 0.56796 |  0:04:39s
epoch 140| loss: 0.34109 | val_0_rmse: 0.61261 | val_1_rmse: 0.62507 |  0:04:41s
epoch 141| loss: 0.34605 | val_0_rmse: 0.59294 | val_1_rmse: 0.60194 |  0:04:43s
epoch 142| loss: 0.3607  | val_0_rmse: 0.72056 | val_1_rmse: 0.73213 |  0:04:45s
epoch 143| loss: 0.35375 | val_0_rmse: 0.61887 | val_1_rmse: 0.63483 |  0:04:47s
epoch 144| loss: 0.34181 | val_0_rmse: 0.64703 | val_1_rmse: 0.65418 |  0:04:49s
epoch 145| loss: 0.34583 | val_0_rmse: 0.66736 | val_1_rmse: 0.67554 |  0:04:51s
epoch 146| loss: 0.35372 | val_0_rmse: 0.56916 | val_1_rmse: 0.58421 |  0:04:53s
epoch 147| loss: 0.34648 | val_0_rmse: 0.66866 | val_1_rmse: 0.68158 |  0:04:55s
epoch 148| loss: 0.34866 | val_0_rmse: 0.57344 | val_1_rmse: 0.5836  |  0:04:57s
epoch 149| loss: 0.36304 | val_0_rmse: 0.67059 | val_1_rmse: 0.67991 |  0:04:59s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.56091
Best weights from best epoch are automatically used!
ended training at: 13:23:43
Feature importance:
[('Area', 0.20077983987308662), ('Baths', 0.0), ('Beds', 0.003988128055997788), ('Latitude', 0.42707797319684015), ('Longitude', 0.358072415935534), ('Month', 0.010081642938541422), ('Year', 0.0)]
Mean squared error is of 6923923831.922724
Mean absolute error:57518.676567086375
MAPE:0.19103102231191063
R2 score:0.6904417841085217
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:23:44
epoch 0  | loss: 0.81031 | val_0_rmse: 0.81707 | val_1_rmse: 0.81334 |  0:00:02s
epoch 1  | loss: 0.66306 | val_0_rmse: 0.79443 | val_1_rmse: 0.78806 |  0:00:04s
epoch 2  | loss: 0.59665 | val_0_rmse: 0.74901 | val_1_rmse: 0.75032 |  0:00:06s
epoch 3  | loss: 0.54537 | val_0_rmse: 0.71379 | val_1_rmse: 0.7187  |  0:00:08s
epoch 4  | loss: 0.53362 | val_0_rmse: 0.70964 | val_1_rmse: 0.7167  |  0:00:10s
epoch 5  | loss: 0.48021 | val_0_rmse: 0.69456 | val_1_rmse: 0.71007 |  0:00:12s
epoch 6  | loss: 0.45994 | val_0_rmse: 0.6507  | val_1_rmse: 0.65544 |  0:00:14s
epoch 7  | loss: 0.45109 | val_0_rmse: 0.64011 | val_1_rmse: 0.64654 |  0:00:16s
epoch 8  | loss: 0.46603 | val_0_rmse: 0.69692 | val_1_rmse: 0.70439 |  0:00:18s
epoch 9  | loss: 0.45703 | val_0_rmse: 0.64125 | val_1_rmse: 0.64347 |  0:00:20s
epoch 10 | loss: 0.43969 | val_0_rmse: 0.63234 | val_1_rmse: 0.63525 |  0:00:21s
epoch 11 | loss: 0.43682 | val_0_rmse: 0.6602  | val_1_rmse: 0.65774 |  0:00:24s
epoch 12 | loss: 0.42619 | val_0_rmse: 0.63009 | val_1_rmse: 0.63536 |  0:00:25s
epoch 13 | loss: 0.41999 | val_0_rmse: 0.63543 | val_1_rmse: 0.63699 |  0:00:27s
epoch 14 | loss: 0.41877 | val_0_rmse: 0.61911 | val_1_rmse: 0.61962 |  0:00:30s
epoch 15 | loss: 0.41079 | val_0_rmse: 0.62794 | val_1_rmse: 0.62749 |  0:00:31s
epoch 16 | loss: 0.4134  | val_0_rmse: 0.68931 | val_1_rmse: 0.68888 |  0:00:33s
epoch 17 | loss: 0.42142 | val_0_rmse: 0.62859 | val_1_rmse: 0.63132 |  0:00:35s
epoch 18 | loss: 0.43008 | val_0_rmse: 0.6473  | val_1_rmse: 0.64917 |  0:00:37s
epoch 19 | loss: 0.41329 | val_0_rmse: 0.62368 | val_1_rmse: 0.62568 |  0:00:39s
epoch 20 | loss: 0.4103  | val_0_rmse: 0.6195  | val_1_rmse: 0.62214 |  0:00:41s
epoch 21 | loss: 0.41019 | val_0_rmse: 0.62097 | val_1_rmse: 0.62434 |  0:00:43s
epoch 22 | loss: 0.40688 | val_0_rmse: 0.61995 | val_1_rmse: 0.62266 |  0:00:45s
epoch 23 | loss: 0.40478 | val_0_rmse: 0.6432  | val_1_rmse: 0.64373 |  0:00:47s
epoch 24 | loss: 0.40218 | val_0_rmse: 0.60177 | val_1_rmse: 0.60705 |  0:00:49s
epoch 25 | loss: 0.39889 | val_0_rmse: 0.64364 | val_1_rmse: 0.6481  |  0:00:51s
epoch 26 | loss: 0.3951  | val_0_rmse: 0.60904 | val_1_rmse: 0.60909 |  0:00:53s
epoch 27 | loss: 0.39386 | val_0_rmse: 0.63239 | val_1_rmse: 0.63111 |  0:00:55s
epoch 28 | loss: 0.40771 | val_0_rmse: 0.6272  | val_1_rmse: 0.63551 |  0:00:57s
epoch 29 | loss: 0.40197 | val_0_rmse: 0.61501 | val_1_rmse: 0.61681 |  0:00:59s
epoch 30 | loss: 0.3959  | val_0_rmse: 0.60826 | val_1_rmse: 0.61646 |  0:01:01s
epoch 31 | loss: 0.40112 | val_0_rmse: 0.62565 | val_1_rmse: 0.63089 |  0:01:03s
epoch 32 | loss: 0.41113 | val_0_rmse: 0.60011 | val_1_rmse: 0.60602 |  0:01:05s
epoch 33 | loss: 0.39745 | val_0_rmse: 0.60957 | val_1_rmse: 0.6161  |  0:01:07s
epoch 34 | loss: 0.41715 | val_0_rmse: 0.62241 | val_1_rmse: 0.62526 |  0:01:09s
epoch 35 | loss: 0.41958 | val_0_rmse: 0.61759 | val_1_rmse: 0.62462 |  0:01:11s
epoch 36 | loss: 0.39392 | val_0_rmse: 0.59682 | val_1_rmse: 0.60454 |  0:01:13s
epoch 37 | loss: 0.39857 | val_0_rmse: 0.61383 | val_1_rmse: 0.61812 |  0:01:15s
epoch 38 | loss: 0.4038  | val_0_rmse: 0.66663 | val_1_rmse: 0.67616 |  0:01:17s
epoch 39 | loss: 0.41411 | val_0_rmse: 0.60349 | val_1_rmse: 0.60706 |  0:01:19s
epoch 40 | loss: 0.39323 | val_0_rmse: 0.59806 | val_1_rmse: 0.60562 |  0:01:21s
epoch 41 | loss: 0.38974 | val_0_rmse: 0.61174 | val_1_rmse: 0.61181 |  0:01:23s
epoch 42 | loss: 0.39213 | val_0_rmse: 0.61386 | val_1_rmse: 0.62271 |  0:01:25s
epoch 43 | loss: 0.40142 | val_0_rmse: 0.61244 | val_1_rmse: 0.61928 |  0:01:27s
epoch 44 | loss: 0.40298 | val_0_rmse: 0.64252 | val_1_rmse: 0.64559 |  0:01:29s
epoch 45 | loss: 0.42433 | val_0_rmse: 0.64748 | val_1_rmse: 0.64267 |  0:01:31s
epoch 46 | loss: 0.42512 | val_0_rmse: 0.65218 | val_1_rmse: 0.65713 |  0:01:33s
epoch 47 | loss: 0.42354 | val_0_rmse: 0.62796 | val_1_rmse: 0.6314  |  0:01:35s
epoch 48 | loss: 0.41938 | val_0_rmse: 0.66051 | val_1_rmse: 0.65883 |  0:01:37s
epoch 49 | loss: 0.41011 | val_0_rmse: 0.61641 | val_1_rmse: 0.6201  |  0:01:39s
epoch 50 | loss: 0.41949 | val_0_rmse: 0.64964 | val_1_rmse: 0.64854 |  0:01:41s
epoch 51 | loss: 0.41734 | val_0_rmse: 0.64239 | val_1_rmse: 0.64016 |  0:01:43s
epoch 52 | loss: 0.41358 | val_0_rmse: 0.62722 | val_1_rmse: 0.62907 |  0:01:45s
epoch 53 | loss: 0.40536 | val_0_rmse: 0.67744 | val_1_rmse: 0.67669 |  0:01:47s
epoch 54 | loss: 0.40822 | val_0_rmse: 0.68484 | val_1_rmse: 0.68279 |  0:01:49s
epoch 55 | loss: 0.40519 | val_0_rmse: 0.61359 | val_1_rmse: 0.61269 |  0:01:51s
epoch 56 | loss: 0.40003 | val_0_rmse: 0.63566 | val_1_rmse: 0.63659 |  0:01:53s
epoch 57 | loss: 0.39649 | val_0_rmse: 0.62976 | val_1_rmse: 0.63583 |  0:01:55s
epoch 58 | loss: 0.4025  | val_0_rmse: 0.63883 | val_1_rmse: 0.64088 |  0:01:57s
epoch 59 | loss: 0.39327 | val_0_rmse: 0.6079  | val_1_rmse: 0.61198 |  0:01:59s
epoch 60 | loss: 0.38489 | val_0_rmse: 0.60997 | val_1_rmse: 0.6128  |  0:02:01s
epoch 61 | loss: 0.38827 | val_0_rmse: 0.60787 | val_1_rmse: 0.60569 |  0:02:03s
epoch 62 | loss: 0.39059 | val_0_rmse: 0.62435 | val_1_rmse: 0.62112 |  0:02:05s
epoch 63 | loss: 0.38169 | val_0_rmse: 0.62973 | val_1_rmse: 0.62815 |  0:02:07s
epoch 64 | loss: 0.38306 | val_0_rmse: 0.60919 | val_1_rmse: 0.608   |  0:02:09s
epoch 65 | loss: 0.37185 | val_0_rmse: 0.6085  | val_1_rmse: 0.61622 |  0:02:11s
epoch 66 | loss: 0.38505 | val_0_rmse: 0.61949 | val_1_rmse: 0.62181 |  0:02:13s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.60454
Best weights from best epoch are automatically used!
ended training at: 13:25:58
Feature importance:
[('Area', 0.3528141746804882), ('Baths', 0.0), ('Beds', 0.01947918891088844), ('Latitude', 0.1899657797666394), ('Longitude', 0.3767140080269297), ('Month', 0.0), ('Year', 0.06102684861505425)]
Mean squared error is of 8079916063.55647
Mean absolute error:62987.214267453666
MAPE:0.20317130103164996
R2 score:0.6398125581590005
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:25:58
epoch 0  | loss: 0.82338 | val_0_rmse: 0.8507  | val_1_rmse: 0.85316 |  0:00:02s
epoch 1  | loss: 0.65581 | val_0_rmse: 0.78665 | val_1_rmse: 0.79649 |  0:00:03s
epoch 2  | loss: 0.64994 | val_0_rmse: 0.79879 | val_1_rmse: 0.80365 |  0:00:06s
epoch 3  | loss: 0.63255 | val_0_rmse: 0.77574 | val_1_rmse: 0.77671 |  0:00:08s
epoch 4  | loss: 0.59626 | val_0_rmse: 0.77102 | val_1_rmse: 0.77525 |  0:00:10s
epoch 5  | loss: 0.55595 | val_0_rmse: 0.7098  | val_1_rmse: 0.71922 |  0:00:11s
epoch 6  | loss: 0.49734 | val_0_rmse: 0.67773 | val_1_rmse: 0.68709 |  0:00:14s
epoch 7  | loss: 0.46656 | val_0_rmse: 0.65926 | val_1_rmse: 0.66053 |  0:00:16s
epoch 8  | loss: 0.45286 | val_0_rmse: 0.64615 | val_1_rmse: 0.65181 |  0:00:18s
epoch 9  | loss: 0.45855 | val_0_rmse: 0.64145 | val_1_rmse: 0.64827 |  0:00:20s
epoch 10 | loss: 0.43061 | val_0_rmse: 0.64472 | val_1_rmse: 0.64903 |  0:00:22s
epoch 11 | loss: 0.43357 | val_0_rmse: 0.65864 | val_1_rmse: 0.6666  |  0:00:24s
epoch 12 | loss: 0.42637 | val_0_rmse: 0.64001 | val_1_rmse: 0.64413 |  0:00:26s
epoch 13 | loss: 0.42336 | val_0_rmse: 0.62839 | val_1_rmse: 0.63549 |  0:00:28s
epoch 14 | loss: 0.42071 | val_0_rmse: 0.63232 | val_1_rmse: 0.63747 |  0:00:30s
epoch 15 | loss: 0.42651 | val_0_rmse: 0.63835 | val_1_rmse: 0.6464  |  0:00:32s
epoch 16 | loss: 0.42758 | val_0_rmse: 0.62663 | val_1_rmse: 0.63541 |  0:00:34s
epoch 17 | loss: 0.41129 | val_0_rmse: 0.61675 | val_1_rmse: 0.6288  |  0:00:36s
epoch 18 | loss: 0.41092 | val_0_rmse: 0.62178 | val_1_rmse: 0.63222 |  0:00:38s
epoch 19 | loss: 0.41131 | val_0_rmse: 0.70127 | val_1_rmse: 0.70551 |  0:00:40s
epoch 20 | loss: 0.42018 | val_0_rmse: 0.63199 | val_1_rmse: 0.64049 |  0:00:42s
epoch 21 | loss: 0.40892 | val_0_rmse: 0.61666 | val_1_rmse: 0.62863 |  0:00:44s
epoch 22 | loss: 0.41092 | val_0_rmse: 0.64911 | val_1_rmse: 0.66062 |  0:00:46s
epoch 23 | loss: 0.41266 | val_0_rmse: 0.63137 | val_1_rmse: 0.64321 |  0:00:48s
epoch 24 | loss: 0.42953 | val_0_rmse: 0.64581 | val_1_rmse: 0.6565  |  0:00:50s
epoch 25 | loss: 0.4295  | val_0_rmse: 0.62651 | val_1_rmse: 0.63327 |  0:00:52s
epoch 26 | loss: 0.41739 | val_0_rmse: 0.63689 | val_1_rmse: 0.65002 |  0:00:53s
epoch 27 | loss: 0.40264 | val_0_rmse: 0.61617 | val_1_rmse: 0.62634 |  0:00:56s
epoch 28 | loss: 0.39686 | val_0_rmse: 0.62791 | val_1_rmse: 0.63936 |  0:00:57s
epoch 29 | loss: 0.39884 | val_0_rmse: 0.61047 | val_1_rmse: 0.62063 |  0:01:00s
epoch 30 | loss: 0.40353 | val_0_rmse: 0.62461 | val_1_rmse: 0.63415 |  0:01:02s
epoch 31 | loss: 0.40219 | val_0_rmse: 0.61483 | val_1_rmse: 0.63009 |  0:01:03s
epoch 32 | loss: 0.3951  | val_0_rmse: 0.63469 | val_1_rmse: 0.65028 |  0:01:05s
epoch 33 | loss: 0.39684 | val_0_rmse: 0.60329 | val_1_rmse: 0.61404 |  0:01:07s
epoch 34 | loss: 0.39479 | val_0_rmse: 0.63076 | val_1_rmse: 0.63186 |  0:01:09s
epoch 35 | loss: 0.39034 | val_0_rmse: 0.63258 | val_1_rmse: 0.6414  |  0:01:11s
epoch 36 | loss: 0.40084 | val_0_rmse: 0.63688 | val_1_rmse: 0.64669 |  0:01:13s
epoch 37 | loss: 0.39826 | val_0_rmse: 0.62165 | val_1_rmse: 0.63074 |  0:01:15s
epoch 38 | loss: 0.38789 | val_0_rmse: 0.59271 | val_1_rmse: 0.60913 |  0:01:17s
epoch 39 | loss: 0.39057 | val_0_rmse: 0.59905 | val_1_rmse: 0.60966 |  0:01:19s
epoch 40 | loss: 0.37738 | val_0_rmse: 0.59137 | val_1_rmse: 0.60012 |  0:01:21s
epoch 41 | loss: 0.3829  | val_0_rmse: 0.60081 | val_1_rmse: 0.61087 |  0:01:23s
epoch 42 | loss: 0.38197 | val_0_rmse: 0.59483 | val_1_rmse: 0.60485 |  0:01:25s
epoch 43 | loss: 0.38435 | val_0_rmse: 0.60673 | val_1_rmse: 0.61826 |  0:01:27s
epoch 44 | loss: 0.38468 | val_0_rmse: 0.61803 | val_1_rmse: 0.63526 |  0:01:29s
epoch 45 | loss: 0.3814  | val_0_rmse: 0.62456 | val_1_rmse: 0.63499 |  0:01:31s
epoch 46 | loss: 0.37981 | val_0_rmse: 0.58965 | val_1_rmse: 0.60358 |  0:01:33s
epoch 47 | loss: 0.36528 | val_0_rmse: 0.58749 | val_1_rmse: 0.59972 |  0:01:35s
epoch 48 | loss: 0.37739 | val_0_rmse: 0.61219 | val_1_rmse: 0.62235 |  0:01:37s
epoch 49 | loss: 0.36427 | val_0_rmse: 0.59487 | val_1_rmse: 0.60629 |  0:01:39s
epoch 50 | loss: 0.3742  | val_0_rmse: 0.6179  | val_1_rmse: 0.62999 |  0:01:42s
epoch 51 | loss: 0.37799 | val_0_rmse: 0.60811 | val_1_rmse: 0.61874 |  0:01:43s
epoch 52 | loss: 0.37446 | val_0_rmse: 0.57538 | val_1_rmse: 0.58875 |  0:01:45s
epoch 53 | loss: 0.36859 | val_0_rmse: 0.61828 | val_1_rmse: 0.62823 |  0:01:47s
epoch 54 | loss: 0.36791 | val_0_rmse: 0.58696 | val_1_rmse: 0.59958 |  0:01:49s
epoch 55 | loss: 0.38582 | val_0_rmse: 0.592   | val_1_rmse: 0.60382 |  0:01:51s
epoch 56 | loss: 0.37444 | val_0_rmse: 0.58931 | val_1_rmse: 0.60349 |  0:01:53s
epoch 57 | loss: 0.37346 | val_0_rmse: 0.60939 | val_1_rmse: 0.61971 |  0:01:55s
epoch 58 | loss: 0.37499 | val_0_rmse: 0.61835 | val_1_rmse: 0.6267  |  0:01:57s
epoch 59 | loss: 0.37714 | val_0_rmse: 0.60252 | val_1_rmse: 0.61689 |  0:01:59s
epoch 60 | loss: 0.37107 | val_0_rmse: 0.62656 | val_1_rmse: 0.63897 |  0:02:01s
epoch 61 | loss: 0.37793 | val_0_rmse: 0.60982 | val_1_rmse: 0.61629 |  0:02:03s
epoch 62 | loss: 0.39216 | val_0_rmse: 0.61625 | val_1_rmse: 0.62712 |  0:02:05s
epoch 63 | loss: 0.37643 | val_0_rmse: 0.618   | val_1_rmse: 0.62743 |  0:02:07s
epoch 64 | loss: 0.36911 | val_0_rmse: 0.58077 | val_1_rmse: 0.59526 |  0:02:09s
epoch 65 | loss: 0.36568 | val_0_rmse: 0.59916 | val_1_rmse: 0.61337 |  0:02:11s
epoch 66 | loss: 0.36617 | val_0_rmse: 0.58841 | val_1_rmse: 0.60133 |  0:02:13s
epoch 67 | loss: 0.3676  | val_0_rmse: 0.59097 | val_1_rmse: 0.6048  |  0:02:15s
epoch 68 | loss: 0.36566 | val_0_rmse: 0.58213 | val_1_rmse: 0.59655 |  0:02:17s
epoch 69 | loss: 0.36056 | val_0_rmse: 0.58452 | val_1_rmse: 0.60078 |  0:02:19s
epoch 70 | loss: 0.35335 | val_0_rmse: 0.56912 | val_1_rmse: 0.58864 |  0:02:21s
epoch 71 | loss: 0.36537 | val_0_rmse: 0.59441 | val_1_rmse: 0.60761 |  0:02:23s
epoch 72 | loss: 0.36156 | val_0_rmse: 0.6152  | val_1_rmse: 0.62746 |  0:02:25s
epoch 73 | loss: 0.36373 | val_0_rmse: 0.59183 | val_1_rmse: 0.60625 |  0:02:27s
epoch 74 | loss: 0.36894 | val_0_rmse: 0.59007 | val_1_rmse: 0.60245 |  0:02:29s
epoch 75 | loss: 0.36381 | val_0_rmse: 0.59577 | val_1_rmse: 0.60606 |  0:02:31s
epoch 76 | loss: 0.36738 | val_0_rmse: 0.5885  | val_1_rmse: 0.60691 |  0:02:33s
epoch 77 | loss: 0.36929 | val_0_rmse: 0.58331 | val_1_rmse: 0.59377 |  0:02:35s
epoch 78 | loss: 0.36894 | val_0_rmse: 0.57816 | val_1_rmse: 0.59084 |  0:02:37s
epoch 79 | loss: 0.36709 | val_0_rmse: 0.62912 | val_1_rmse: 0.64567 |  0:02:39s
epoch 80 | loss: 0.36724 | val_0_rmse: 0.58604 | val_1_rmse: 0.60107 |  0:02:41s
epoch 81 | loss: 0.35856 | val_0_rmse: 0.59931 | val_1_rmse: 0.61809 |  0:02:43s
epoch 82 | loss: 0.36215 | val_0_rmse: 0.57411 | val_1_rmse: 0.58903 |  0:02:45s
epoch 83 | loss: 0.35809 | val_0_rmse: 0.60814 | val_1_rmse: 0.6273  |  0:02:47s
epoch 84 | loss: 0.36574 | val_0_rmse: 0.60138 | val_1_rmse: 0.61672 |  0:02:49s
epoch 85 | loss: 0.35857 | val_0_rmse: 0.57349 | val_1_rmse: 0.58851 |  0:02:51s
epoch 86 | loss: 0.34956 | val_0_rmse: 0.57052 | val_1_rmse: 0.58815 |  0:02:53s
epoch 87 | loss: 0.36008 | val_0_rmse: 0.5938  | val_1_rmse: 0.6124  |  0:02:55s
epoch 88 | loss: 0.35312 | val_0_rmse: 0.60605 | val_1_rmse: 0.61873 |  0:02:57s
epoch 89 | loss: 0.35871 | val_0_rmse: 0.57274 | val_1_rmse: 0.59025 |  0:02:59s
epoch 90 | loss: 0.35757 | val_0_rmse: 0.57854 | val_1_rmse: 0.59446 |  0:03:01s
epoch 91 | loss: 0.35672 | val_0_rmse: 0.59088 | val_1_rmse: 0.60447 |  0:03:03s
epoch 92 | loss: 0.35453 | val_0_rmse: 0.57127 | val_1_rmse: 0.58575 |  0:03:05s
epoch 93 | loss: 0.35829 | val_0_rmse: 0.59054 | val_1_rmse: 0.59686 |  0:03:07s
epoch 94 | loss: 0.35565 | val_0_rmse: 0.58235 | val_1_rmse: 0.5951  |  0:03:09s
epoch 95 | loss: 0.35726 | val_0_rmse: 0.58151 | val_1_rmse: 0.60072 |  0:03:11s
epoch 96 | loss: 0.36303 | val_0_rmse: 0.58457 | val_1_rmse: 0.59766 |  0:03:13s
epoch 97 | loss: 0.35925 | val_0_rmse: 0.56002 | val_1_rmse: 0.57646 |  0:03:15s
epoch 98 | loss: 0.35931 | val_0_rmse: 0.58426 | val_1_rmse: 0.59993 |  0:03:17s
epoch 99 | loss: 0.35691 | val_0_rmse: 0.61362 | val_1_rmse: 0.62878 |  0:03:19s
epoch 100| loss: 0.34713 | val_0_rmse: 0.57104 | val_1_rmse: 0.58849 |  0:03:21s
epoch 101| loss: 0.34899 | val_0_rmse: 0.56751 | val_1_rmse: 0.58502 |  0:03:23s
epoch 102| loss: 0.34896 | val_0_rmse: 0.58367 | val_1_rmse: 0.59888 |  0:03:25s
epoch 103| loss: 0.35865 | val_0_rmse: 0.60912 | val_1_rmse: 0.62174 |  0:03:27s
epoch 104| loss: 0.35956 | val_0_rmse: 0.58301 | val_1_rmse: 0.59736 |  0:03:29s
epoch 105| loss: 0.35707 | val_0_rmse: 0.62001 | val_1_rmse: 0.63248 |  0:03:31s
epoch 106| loss: 0.3594  | val_0_rmse: 0.5714  | val_1_rmse: 0.58534 |  0:03:33s
epoch 107| loss: 0.34428 | val_0_rmse: 0.58381 | val_1_rmse: 0.60035 |  0:03:35s
epoch 108| loss: 0.3516  | val_0_rmse: 0.58721 | val_1_rmse: 0.60191 |  0:03:37s
epoch 109| loss: 0.35625 | val_0_rmse: 0.59595 | val_1_rmse: 0.60796 |  0:03:39s
epoch 110| loss: 0.35939 | val_0_rmse: 0.60093 | val_1_rmse: 0.61579 |  0:03:41s
epoch 111| loss: 0.35123 | val_0_rmse: 0.57021 | val_1_rmse: 0.58516 |  0:03:43s
epoch 112| loss: 0.34902 | val_0_rmse: 0.59134 | val_1_rmse: 0.60988 |  0:03:45s
epoch 113| loss: 0.35553 | val_0_rmse: 0.57182 | val_1_rmse: 0.59016 |  0:03:47s
epoch 114| loss: 0.35348 | val_0_rmse: 0.57597 | val_1_rmse: 0.59139 |  0:03:49s
epoch 115| loss: 0.34767 | val_0_rmse: 0.56439 | val_1_rmse: 0.57839 |  0:03:51s
epoch 116| loss: 0.34539 | val_0_rmse: 0.56998 | val_1_rmse: 0.58773 |  0:03:53s
epoch 117| loss: 0.3496  | val_0_rmse: 0.58352 | val_1_rmse: 0.59881 |  0:03:55s
epoch 118| loss: 0.35411 | val_0_rmse: 0.63166 | val_1_rmse: 0.64631 |  0:03:57s
epoch 119| loss: 0.34475 | val_0_rmse: 0.58003 | val_1_rmse: 0.59788 |  0:03:59s
epoch 120| loss: 0.35322 | val_0_rmse: 0.56245 | val_1_rmse: 0.57836 |  0:04:01s
epoch 121| loss: 0.34931 | val_0_rmse: 0.59512 | val_1_rmse: 0.61445 |  0:04:03s
epoch 122| loss: 0.35082 | val_0_rmse: 0.5922  | val_1_rmse: 0.6103  |  0:04:05s
epoch 123| loss: 0.34925 | val_0_rmse: 0.56933 | val_1_rmse: 0.5869  |  0:04:07s
epoch 124| loss: 0.35351 | val_0_rmse: 0.60341 | val_1_rmse: 0.61507 |  0:04:09s
epoch 125| loss: 0.33944 | val_0_rmse: 0.57636 | val_1_rmse: 0.59149 |  0:04:11s
epoch 126| loss: 0.34544 | val_0_rmse: 0.58325 | val_1_rmse: 0.5963  |  0:04:13s
epoch 127| loss: 0.35571 | val_0_rmse: 0.57926 | val_1_rmse: 0.59061 |  0:04:15s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.57646
Best weights from best epoch are automatically used!
ended training at: 13:30:14
Feature importance:
[('Area', 0.4198005959428509), ('Baths', 0.045190327668346214), ('Beds', 0.0), ('Latitude', 0.19298752885828505), ('Longitude', 0.2420412874726094), ('Month', 0.0), ('Year', 0.09998026005790847)]
Mean squared error is of 7038002580.003975
Mean absolute error:60187.542334000835
MAPE:0.1983869851001774
R2 score:0.6819677286747974
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:30:15
epoch 0  | loss: 0.79419 | val_0_rmse: 0.84252 | val_1_rmse: 0.84005 |  0:00:02s
epoch 1  | loss: 0.64347 | val_0_rmse: 0.75728 | val_1_rmse: 0.75334 |  0:00:04s
epoch 2  | loss: 0.55015 | val_0_rmse: 0.72996 | val_1_rmse: 0.73462 |  0:00:06s
epoch 3  | loss: 0.50241 | val_0_rmse: 0.7185  | val_1_rmse: 0.7119  |  0:00:08s
epoch 4  | loss: 0.48707 | val_0_rmse: 0.67425 | val_1_rmse: 0.67042 |  0:00:10s
epoch 5  | loss: 0.48093 | val_0_rmse: 0.68524 | val_1_rmse: 0.68493 |  0:00:12s
epoch 6  | loss: 0.46462 | val_0_rmse: 0.65771 | val_1_rmse: 0.65269 |  0:00:14s
epoch 7  | loss: 0.45101 | val_0_rmse: 0.67162 | val_1_rmse: 0.67143 |  0:00:16s
epoch 8  | loss: 0.44965 | val_0_rmse: 0.65789 | val_1_rmse: 0.65681 |  0:00:18s
epoch 9  | loss: 0.45702 | val_0_rmse: 0.65086 | val_1_rmse: 0.64909 |  0:00:20s
epoch 10 | loss: 0.44916 | val_0_rmse: 0.64566 | val_1_rmse: 0.64677 |  0:00:22s
epoch 11 | loss: 0.44538 | val_0_rmse: 0.64449 | val_1_rmse: 0.6435  |  0:00:24s
epoch 12 | loss: 0.45074 | val_0_rmse: 0.66563 | val_1_rmse: 0.66347 |  0:00:26s
epoch 13 | loss: 0.45276 | val_0_rmse: 0.72197 | val_1_rmse: 0.72269 |  0:00:28s
epoch 14 | loss: 0.44166 | val_0_rmse: 0.64767 | val_1_rmse: 0.64658 |  0:00:30s
epoch 15 | loss: 0.43253 | val_0_rmse: 0.65675 | val_1_rmse: 0.65895 |  0:00:32s
epoch 16 | loss: 0.43349 | val_0_rmse: 0.63501 | val_1_rmse: 0.63562 |  0:00:34s
epoch 17 | loss: 0.4307  | val_0_rmse: 0.63554 | val_1_rmse: 0.63675 |  0:00:36s
epoch 18 | loss: 0.42191 | val_0_rmse: 0.64389 | val_1_rmse: 0.64849 |  0:00:38s
epoch 19 | loss: 0.42553 | val_0_rmse: 0.64815 | val_1_rmse: 0.64993 |  0:00:40s
epoch 20 | loss: 0.42103 | val_0_rmse: 0.63988 | val_1_rmse: 0.63985 |  0:00:42s
epoch 21 | loss: 0.40892 | val_0_rmse: 0.63881 | val_1_rmse: 0.63828 |  0:00:44s
epoch 22 | loss: 0.41725 | val_0_rmse: 0.64706 | val_1_rmse: 0.64447 |  0:00:46s
epoch 23 | loss: 0.42462 | val_0_rmse: 0.65571 | val_1_rmse: 0.65712 |  0:00:48s
epoch 24 | loss: 0.41438 | val_0_rmse: 0.63888 | val_1_rmse: 0.64269 |  0:00:50s
epoch 25 | loss: 0.41355 | val_0_rmse: 0.6143  | val_1_rmse: 0.61546 |  0:00:52s
epoch 26 | loss: 0.40818 | val_0_rmse: 0.6356  | val_1_rmse: 0.63634 |  0:00:54s
epoch 27 | loss: 0.41329 | val_0_rmse: 0.68076 | val_1_rmse: 0.68037 |  0:00:56s
epoch 28 | loss: 0.4299  | val_0_rmse: 0.64387 | val_1_rmse: 0.64691 |  0:00:58s
epoch 29 | loss: 0.41302 | val_0_rmse: 0.62211 | val_1_rmse: 0.62326 |  0:01:00s
epoch 30 | loss: 0.41247 | val_0_rmse: 0.61391 | val_1_rmse: 0.61468 |  0:01:02s
epoch 31 | loss: 0.40209 | val_0_rmse: 0.61031 | val_1_rmse: 0.61026 |  0:01:04s
epoch 32 | loss: 0.39566 | val_0_rmse: 0.60839 | val_1_rmse: 0.61002 |  0:01:06s
epoch 33 | loss: 0.39732 | val_0_rmse: 0.61075 | val_1_rmse: 0.61198 |  0:01:08s
epoch 34 | loss: 0.39622 | val_0_rmse: 0.6067  | val_1_rmse: 0.60776 |  0:01:10s
epoch 35 | loss: 0.39655 | val_0_rmse: 0.62486 | val_1_rmse: 0.62225 |  0:01:12s
epoch 36 | loss: 0.40364 | val_0_rmse: 0.64361 | val_1_rmse: 0.64172 |  0:01:14s
epoch 37 | loss: 0.40592 | val_0_rmse: 0.60677 | val_1_rmse: 0.60678 |  0:01:16s
epoch 38 | loss: 0.4066  | val_0_rmse: 0.61274 | val_1_rmse: 0.61478 |  0:01:18s
epoch 39 | loss: 0.3935  | val_0_rmse: 0.60322 | val_1_rmse: 0.60492 |  0:01:20s
epoch 40 | loss: 0.39565 | val_0_rmse: 0.66178 | val_1_rmse: 0.66511 |  0:01:22s
epoch 41 | loss: 0.43648 | val_0_rmse: 0.63992 | val_1_rmse: 0.64854 |  0:01:24s
epoch 42 | loss: 0.42309 | val_0_rmse: 0.66082 | val_1_rmse: 0.6651  |  0:01:26s
epoch 43 | loss: 0.4152  | val_0_rmse: 0.62841 | val_1_rmse: 0.63386 |  0:01:28s
epoch 44 | loss: 0.40471 | val_0_rmse: 0.65    | val_1_rmse: 0.65348 |  0:01:30s
epoch 45 | loss: 0.39938 | val_0_rmse: 0.62647 | val_1_rmse: 0.63328 |  0:01:32s
epoch 46 | loss: 0.40945 | val_0_rmse: 0.69155 | val_1_rmse: 0.69823 |  0:01:34s
epoch 47 | loss: 0.40484 | val_0_rmse: 0.62615 | val_1_rmse: 0.629   |  0:01:36s
epoch 48 | loss: 0.40938 | val_0_rmse: 0.60812 | val_1_rmse: 0.61119 |  0:01:38s
epoch 49 | loss: 0.40246 | val_0_rmse: 0.63714 | val_1_rmse: 0.64358 |  0:01:40s
epoch 50 | loss: 0.40534 | val_0_rmse: 0.65518 | val_1_rmse: 0.65287 |  0:01:42s
epoch 51 | loss: 0.39322 | val_0_rmse: 0.64404 | val_1_rmse: 0.6482  |  0:01:44s
epoch 52 | loss: 0.38729 | val_0_rmse: 0.62291 | val_1_rmse: 0.62372 |  0:01:46s
epoch 53 | loss: 0.39729 | val_0_rmse: 0.5959  | val_1_rmse: 0.60141 |  0:01:48s
epoch 54 | loss: 0.39161 | val_0_rmse: 0.6017  | val_1_rmse: 0.60714 |  0:01:50s
epoch 55 | loss: 0.38742 | val_0_rmse: 0.59515 | val_1_rmse: 0.59659 |  0:01:52s
epoch 56 | loss: 0.38753 | val_0_rmse: 0.59449 | val_1_rmse: 0.59674 |  0:01:54s
epoch 57 | loss: 0.38514 | val_0_rmse: 0.59901 | val_1_rmse: 0.60058 |  0:01:56s
epoch 58 | loss: 0.38105 | val_0_rmse: 0.5989  | val_1_rmse: 0.59778 |  0:01:58s
epoch 59 | loss: 0.38135 | val_0_rmse: 0.59011 | val_1_rmse: 0.59346 |  0:02:00s
epoch 60 | loss: 0.38138 | val_0_rmse: 0.63326 | val_1_rmse: 0.63752 |  0:02:02s
epoch 61 | loss: 0.38517 | val_0_rmse: 0.66043 | val_1_rmse: 0.66693 |  0:02:04s
epoch 62 | loss: 0.38621 | val_0_rmse: 0.61949 | val_1_rmse: 0.61882 |  0:02:06s
epoch 63 | loss: 0.37458 | val_0_rmse: 0.58555 | val_1_rmse: 0.5892  |  0:02:08s
epoch 64 | loss: 0.37558 | val_0_rmse: 0.59803 | val_1_rmse: 0.59922 |  0:02:10s
epoch 65 | loss: 0.37464 | val_0_rmse: 0.58599 | val_1_rmse: 0.58695 |  0:02:12s
epoch 66 | loss: 0.37604 | val_0_rmse: 0.64812 | val_1_rmse: 0.65033 |  0:02:14s
epoch 67 | loss: 0.37387 | val_0_rmse: 0.59748 | val_1_rmse: 0.60039 |  0:02:16s
epoch 68 | loss: 0.36234 | val_0_rmse: 0.58966 | val_1_rmse: 0.59188 |  0:02:18s
epoch 69 | loss: 0.37155 | val_0_rmse: 0.59987 | val_1_rmse: 0.60234 |  0:02:20s
epoch 70 | loss: 0.36786 | val_0_rmse: 0.57779 | val_1_rmse: 0.58114 |  0:02:22s
epoch 71 | loss: 0.36781 | val_0_rmse: 0.5837  | val_1_rmse: 0.58711 |  0:02:24s
epoch 72 | loss: 0.36468 | val_0_rmse: 0.5742  | val_1_rmse: 0.57625 |  0:02:26s
epoch 73 | loss: 0.36266 | val_0_rmse: 0.58759 | val_1_rmse: 0.59405 |  0:02:28s
epoch 74 | loss: 0.36461 | val_0_rmse: 0.59837 | val_1_rmse: 0.60713 |  0:02:30s
epoch 75 | loss: 0.3741  | val_0_rmse: 0.5933  | val_1_rmse: 0.59785 |  0:02:32s
epoch 76 | loss: 0.36018 | val_0_rmse: 0.59228 | val_1_rmse: 0.5924  |  0:02:34s
epoch 77 | loss: 0.36606 | val_0_rmse: 0.59254 | val_1_rmse: 0.59424 |  0:02:36s
epoch 78 | loss: 0.36671 | val_0_rmse: 0.59196 | val_1_rmse: 0.59311 |  0:02:38s
epoch 79 | loss: 0.36757 | val_0_rmse: 0.57969 | val_1_rmse: 0.58798 |  0:02:40s
epoch 80 | loss: 0.35898 | val_0_rmse: 0.59852 | val_1_rmse: 0.60205 |  0:02:42s
epoch 81 | loss: 0.36137 | val_0_rmse: 0.60317 | val_1_rmse: 0.60412 |  0:02:44s
epoch 82 | loss: 0.36584 | val_0_rmse: 0.60667 | val_1_rmse: 0.61252 |  0:02:46s
epoch 83 | loss: 0.36823 | val_0_rmse: 0.59558 | val_1_rmse: 0.59496 |  0:02:48s
epoch 84 | loss: 0.36383 | val_0_rmse: 0.59345 | val_1_rmse: 0.59146 |  0:02:50s
epoch 85 | loss: 0.35918 | val_0_rmse: 0.62192 | val_1_rmse: 0.627   |  0:02:52s
epoch 86 | loss: 0.3636  | val_0_rmse: 0.58952 | val_1_rmse: 0.59463 |  0:02:54s
epoch 87 | loss: 0.35913 | val_0_rmse: 0.61681 | val_1_rmse: 0.6211  |  0:02:56s
epoch 88 | loss: 0.36124 | val_0_rmse: 0.58073 | val_1_rmse: 0.58704 |  0:02:58s
epoch 89 | loss: 0.36082 | val_0_rmse: 0.62345 | val_1_rmse: 0.62413 |  0:03:00s
epoch 90 | loss: 0.35864 | val_0_rmse: 0.59277 | val_1_rmse: 0.59954 |  0:03:02s
epoch 91 | loss: 0.35371 | val_0_rmse: 0.57957 | val_1_rmse: 0.58624 |  0:03:04s
epoch 92 | loss: 0.35824 | val_0_rmse: 0.58172 | val_1_rmse: 0.58713 |  0:03:06s
epoch 93 | loss: 0.35286 | val_0_rmse: 0.57934 | val_1_rmse: 0.58337 |  0:03:08s
epoch 94 | loss: 0.36244 | val_0_rmse: 0.60519 | val_1_rmse: 0.60834 |  0:03:10s
epoch 95 | loss: 0.35091 | val_0_rmse: 0.59043 | val_1_rmse: 0.59346 |  0:03:12s
epoch 96 | loss: 0.35268 | val_0_rmse: 0.59253 | val_1_rmse: 0.5963  |  0:03:14s
epoch 97 | loss: 0.35529 | val_0_rmse: 0.57043 | val_1_rmse: 0.57764 |  0:03:16s
epoch 98 | loss: 0.35293 | val_0_rmse: 0.57618 | val_1_rmse: 0.58711 |  0:03:18s
epoch 99 | loss: 0.35104 | val_0_rmse: 0.56347 | val_1_rmse: 0.57019 |  0:03:20s
epoch 100| loss: 0.35045 | val_0_rmse: 0.57592 | val_1_rmse: 0.58646 |  0:03:22s
epoch 101| loss: 0.34815 | val_0_rmse: 0.57731 | val_1_rmse: 0.58345 |  0:03:25s
epoch 102| loss: 0.35419 | val_0_rmse: 0.56992 | val_1_rmse: 0.57923 |  0:03:27s
epoch 103| loss: 0.35366 | val_0_rmse: 0.57841 | val_1_rmse: 0.58126 |  0:03:29s
epoch 104| loss: 0.3576  | val_0_rmse: 0.59407 | val_1_rmse: 0.60018 |  0:03:31s
epoch 105| loss: 0.35078 | val_0_rmse: 0.57383 | val_1_rmse: 0.581   |  0:03:33s
epoch 106| loss: 0.34862 | val_0_rmse: 0.59207 | val_1_rmse: 0.59915 |  0:03:35s
epoch 107| loss: 0.35257 | val_0_rmse: 0.56511 | val_1_rmse: 0.56964 |  0:03:37s
epoch 108| loss: 0.34367 | val_0_rmse: 0.55374 | val_1_rmse: 0.56082 |  0:03:39s
epoch 109| loss: 0.34457 | val_0_rmse: 0.56596 | val_1_rmse: 0.5721  |  0:03:41s
epoch 110| loss: 0.34072 | val_0_rmse: 0.56349 | val_1_rmse: 0.57556 |  0:03:43s
epoch 111| loss: 0.35108 | val_0_rmse: 0.59523 | val_1_rmse: 0.6024  |  0:03:45s
epoch 112| loss: 0.34079 | val_0_rmse: 0.56572 | val_1_rmse: 0.57287 |  0:03:47s
epoch 113| loss: 0.34584 | val_0_rmse: 0.55736 | val_1_rmse: 0.56454 |  0:03:49s
epoch 114| loss: 0.34949 | val_0_rmse: 0.58382 | val_1_rmse: 0.5872  |  0:03:51s
epoch 115| loss: 0.34654 | val_0_rmse: 0.61682 | val_1_rmse: 0.62056 |  0:03:53s
epoch 116| loss: 0.35101 | val_0_rmse: 0.59245 | val_1_rmse: 0.59595 |  0:03:55s
epoch 117| loss: 0.34962 | val_0_rmse: 0.60578 | val_1_rmse: 0.61274 |  0:03:57s
epoch 118| loss: 0.3423  | val_0_rmse: 0.56785 | val_1_rmse: 0.57502 |  0:03:59s
epoch 119| loss: 0.34313 | val_0_rmse: 0.5862  | val_1_rmse: 0.59016 |  0:04:01s
epoch 120| loss: 0.34664 | val_0_rmse: 0.58117 | val_1_rmse: 0.58861 |  0:04:03s
epoch 121| loss: 0.34013 | val_0_rmse: 0.57528 | val_1_rmse: 0.58215 |  0:04:05s
epoch 122| loss: 0.35508 | val_0_rmse: 0.57638 | val_1_rmse: 0.57783 |  0:04:07s
epoch 123| loss: 0.34545 | val_0_rmse: 0.56855 | val_1_rmse: 0.57354 |  0:04:09s
epoch 124| loss: 0.3559  | val_0_rmse: 0.58508 | val_1_rmse: 0.58912 |  0:04:11s
epoch 125| loss: 0.33542 | val_0_rmse: 0.55111 | val_1_rmse: 0.5572  |  0:04:13s
epoch 126| loss: 0.33947 | val_0_rmse: 0.56024 | val_1_rmse: 0.56329 |  0:04:15s
epoch 127| loss: 0.33718 | val_0_rmse: 0.57412 | val_1_rmse: 0.5787  |  0:04:17s
epoch 128| loss: 0.34738 | val_0_rmse: 0.55879 | val_1_rmse: 0.56292 |  0:04:19s
epoch 129| loss: 0.34252 | val_0_rmse: 0.56525 | val_1_rmse: 0.56997 |  0:04:21s
epoch 130| loss: 0.34941 | val_0_rmse: 0.56276 | val_1_rmse: 0.56955 |  0:04:23s
epoch 131| loss: 0.34146 | val_0_rmse: 0.5704  | val_1_rmse: 0.57707 |  0:04:25s
epoch 132| loss: 0.33861 | val_0_rmse: 0.5739  | val_1_rmse: 0.58261 |  0:04:27s
epoch 133| loss: 0.34847 | val_0_rmse: 0.56225 | val_1_rmse: 0.56896 |  0:04:29s
epoch 134| loss: 0.3436  | val_0_rmse: 0.55616 | val_1_rmse: 0.56315 |  0:04:31s
epoch 135| loss: 0.34254 | val_0_rmse: 0.59064 | val_1_rmse: 0.60087 |  0:04:33s
epoch 136| loss: 0.34337 | val_0_rmse: 0.56472 | val_1_rmse: 0.57658 |  0:04:35s
epoch 137| loss: 0.33622 | val_0_rmse: 0.56254 | val_1_rmse: 0.56799 |  0:04:37s
epoch 138| loss: 0.33747 | val_0_rmse: 0.5784  | val_1_rmse: 0.58529 |  0:04:39s
epoch 139| loss: 0.33723 | val_0_rmse: 0.56605 | val_1_rmse: 0.57146 |  0:04:41s
epoch 140| loss: 0.33882 | val_0_rmse: 0.55489 | val_1_rmse: 0.56411 |  0:04:43s
epoch 141| loss: 0.33785 | val_0_rmse: 0.5655  | val_1_rmse: 0.57904 |  0:04:45s
epoch 142| loss: 0.34157 | val_0_rmse: 0.57022 | val_1_rmse: 0.57663 |  0:04:47s
epoch 143| loss: 0.33598 | val_0_rmse: 0.575   | val_1_rmse: 0.58435 |  0:04:49s
epoch 144| loss: 0.34625 | val_0_rmse: 0.54755 | val_1_rmse: 0.55722 |  0:04:51s
epoch 145| loss: 0.33009 | val_0_rmse: 0.55644 | val_1_rmse: 0.5699  |  0:04:53s
epoch 146| loss: 0.33375 | val_0_rmse: 0.6032  | val_1_rmse: 0.61177 |  0:04:55s
epoch 147| loss: 0.33616 | val_0_rmse: 0.54991 | val_1_rmse: 0.56185 |  0:04:57s
epoch 148| loss: 0.34136 | val_0_rmse: 0.56359 | val_1_rmse: 0.56985 |  0:04:59s
epoch 149| loss: 0.34649 | val_0_rmse: 0.56014 | val_1_rmse: 0.56855 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.5572
Best weights from best epoch are automatically used!
ended training at: 13:35:17
Feature importance:
[('Area', 0.38440977229012185), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.20819266706115547), ('Longitude', 0.4073975606487226), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 7358839410.700334
Mean absolute error:61018.58573311067
MAPE:0.20628905357515362
R2 score:0.6735892433873368
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:35:17
epoch 0  | loss: 0.78603 | val_0_rmse: 0.83977 | val_1_rmse: 0.84017 |  0:00:01s
epoch 1  | loss: 0.63375 | val_0_rmse: 0.76756 | val_1_rmse: 0.77125 |  0:00:03s
epoch 2  | loss: 0.54494 | val_0_rmse: 0.72349 | val_1_rmse: 0.7335  |  0:00:05s
epoch 3  | loss: 0.49336 | val_0_rmse: 0.67595 | val_1_rmse: 0.69046 |  0:00:07s
epoch 4  | loss: 0.45534 | val_0_rmse: 0.68079 | val_1_rmse: 0.69037 |  0:00:09s
epoch 5  | loss: 0.46073 | val_0_rmse: 0.64979 | val_1_rmse: 0.6579  |  0:00:11s
epoch 6  | loss: 0.43753 | val_0_rmse: 0.65752 | val_1_rmse: 0.6632  |  0:00:13s
epoch 7  | loss: 0.43476 | val_0_rmse: 0.6582  | val_1_rmse: 0.66556 |  0:00:15s
epoch 8  | loss: 0.42363 | val_0_rmse: 0.63168 | val_1_rmse: 0.64236 |  0:00:17s
epoch 9  | loss: 0.42214 | val_0_rmse: 0.62987 | val_1_rmse: 0.64365 |  0:00:19s
epoch 10 | loss: 0.42635 | val_0_rmse: 0.64475 | val_1_rmse: 0.65489 |  0:00:21s
epoch 11 | loss: 0.41835 | val_0_rmse: 0.62836 | val_1_rmse: 0.6468  |  0:00:23s
epoch 12 | loss: 0.40436 | val_0_rmse: 0.61963 | val_1_rmse: 0.6313  |  0:00:25s
epoch 13 | loss: 0.39721 | val_0_rmse: 0.61403 | val_1_rmse: 0.62653 |  0:00:27s
epoch 14 | loss: 0.40252 | val_0_rmse: 0.62249 | val_1_rmse: 0.63514 |  0:00:29s
epoch 15 | loss: 0.39335 | val_0_rmse: 0.63177 | val_1_rmse: 0.64251 |  0:00:31s
epoch 16 | loss: 0.40584 | val_0_rmse: 0.63791 | val_1_rmse: 0.64987 |  0:00:33s
epoch 17 | loss: 0.41423 | val_0_rmse: 0.61237 | val_1_rmse: 0.62822 |  0:00:35s
epoch 18 | loss: 0.39582 | val_0_rmse: 0.62523 | val_1_rmse: 0.63817 |  0:00:37s
epoch 19 | loss: 0.39666 | val_0_rmse: 0.60612 | val_1_rmse: 0.61805 |  0:00:39s
epoch 20 | loss: 0.39716 | val_0_rmse: 0.61879 | val_1_rmse: 0.63136 |  0:00:41s
epoch 21 | loss: 0.39285 | val_0_rmse: 0.63925 | val_1_rmse: 0.65174 |  0:00:43s
epoch 22 | loss: 0.3979  | val_0_rmse: 0.60001 | val_1_rmse: 0.61101 |  0:00:45s
epoch 23 | loss: 0.39011 | val_0_rmse: 0.61645 | val_1_rmse: 0.62653 |  0:00:47s
epoch 24 | loss: 0.39111 | val_0_rmse: 0.60088 | val_1_rmse: 0.61338 |  0:00:49s
epoch 25 | loss: 0.38688 | val_0_rmse: 0.6161  | val_1_rmse: 0.62812 |  0:00:51s
epoch 26 | loss: 0.38952 | val_0_rmse: 0.62087 | val_1_rmse: 0.62977 |  0:00:53s
epoch 27 | loss: 0.38343 | val_0_rmse: 0.60655 | val_1_rmse: 0.61621 |  0:00:55s
epoch 28 | loss: 0.3848  | val_0_rmse: 0.59741 | val_1_rmse: 0.60569 |  0:00:57s
epoch 29 | loss: 0.394   | val_0_rmse: 0.64036 | val_1_rmse: 0.64377 |  0:00:59s
epoch 30 | loss: 0.40938 | val_0_rmse: 0.65018 | val_1_rmse: 0.66636 |  0:01:01s
epoch 31 | loss: 0.39928 | val_0_rmse: 0.64192 | val_1_rmse: 0.65443 |  0:01:03s
epoch 32 | loss: 0.41654 | val_0_rmse: 0.65734 | val_1_rmse: 0.66624 |  0:01:05s
epoch 33 | loss: 0.40905 | val_0_rmse: 0.6162  | val_1_rmse: 0.62353 |  0:01:07s
epoch 34 | loss: 0.39535 | val_0_rmse: 0.59661 | val_1_rmse: 0.60662 |  0:01:09s
epoch 35 | loss: 0.38867 | val_0_rmse: 0.62721 | val_1_rmse: 0.6367  |  0:01:11s
epoch 36 | loss: 0.39052 | val_0_rmse: 0.6127  | val_1_rmse: 0.61793 |  0:01:13s
epoch 37 | loss: 0.39219 | val_0_rmse: 0.60776 | val_1_rmse: 0.61352 |  0:01:15s
epoch 38 | loss: 0.39165 | val_0_rmse: 0.60107 | val_1_rmse: 0.60958 |  0:01:17s
epoch 39 | loss: 0.38465 | val_0_rmse: 0.59952 | val_1_rmse: 0.60879 |  0:01:19s
epoch 40 | loss: 0.38165 | val_0_rmse: 0.61042 | val_1_rmse: 0.62007 |  0:01:21s
epoch 41 | loss: 0.39403 | val_0_rmse: 0.60418 | val_1_rmse: 0.61612 |  0:01:23s
epoch 42 | loss: 0.38526 | val_0_rmse: 0.60094 | val_1_rmse: 0.60776 |  0:01:25s
epoch 43 | loss: 0.38376 | val_0_rmse: 0.59682 | val_1_rmse: 0.60596 |  0:01:27s
epoch 44 | loss: 0.39213 | val_0_rmse: 0.59631 | val_1_rmse: 0.60366 |  0:01:29s
epoch 45 | loss: 0.38082 | val_0_rmse: 0.60595 | val_1_rmse: 0.6154  |  0:01:31s
epoch 46 | loss: 0.37417 | val_0_rmse: 0.60907 | val_1_rmse: 0.61589 |  0:01:33s
epoch 47 | loss: 0.37646 | val_0_rmse: 0.6031  | val_1_rmse: 0.61175 |  0:01:35s
epoch 48 | loss: 0.37706 | val_0_rmse: 0.61909 | val_1_rmse: 0.62721 |  0:01:37s
epoch 49 | loss: 0.37785 | val_0_rmse: 0.5914  | val_1_rmse: 0.60307 |  0:01:39s
epoch 50 | loss: 0.376   | val_0_rmse: 0.58677 | val_1_rmse: 0.597   |  0:01:41s
epoch 51 | loss: 0.37387 | val_0_rmse: 0.59533 | val_1_rmse: 0.6029  |  0:01:43s
epoch 52 | loss: 0.38484 | val_0_rmse: 0.69382 | val_1_rmse: 0.69918 |  0:01:45s
epoch 53 | loss: 0.373   | val_0_rmse: 0.61198 | val_1_rmse: 0.6236  |  0:01:47s
epoch 54 | loss: 0.37115 | val_0_rmse: 0.5878  | val_1_rmse: 0.59536 |  0:01:49s
epoch 55 | loss: 0.37038 | val_0_rmse: 0.60527 | val_1_rmse: 0.61258 |  0:01:51s
epoch 56 | loss: 0.36869 | val_0_rmse: 0.60799 | val_1_rmse: 0.62198 |  0:01:53s
epoch 57 | loss: 0.37059 | val_0_rmse: 0.5817  | val_1_rmse: 0.59398 |  0:01:55s
epoch 58 | loss: 0.36404 | val_0_rmse: 0.63793 | val_1_rmse: 0.6528  |  0:01:57s
epoch 59 | loss: 0.37119 | val_0_rmse: 0.60264 | val_1_rmse: 0.6147  |  0:01:59s
epoch 60 | loss: 0.37014 | val_0_rmse: 0.61466 | val_1_rmse: 0.62641 |  0:02:01s
epoch 61 | loss: 0.36566 | val_0_rmse: 0.58667 | val_1_rmse: 0.59636 |  0:02:03s
epoch 62 | loss: 0.36203 | val_0_rmse: 0.58068 | val_1_rmse: 0.59159 |  0:02:05s
epoch 63 | loss: 0.36189 | val_0_rmse: 0.58986 | val_1_rmse: 0.59906 |  0:02:07s
epoch 64 | loss: 0.36385 | val_0_rmse: 0.59475 | val_1_rmse: 0.60755 |  0:02:09s
epoch 65 | loss: 0.35473 | val_0_rmse: 0.57833 | val_1_rmse: 0.58984 |  0:02:11s
epoch 66 | loss: 0.36212 | val_0_rmse: 0.57148 | val_1_rmse: 0.58179 |  0:02:13s
epoch 67 | loss: 0.366   | val_0_rmse: 0.59717 | val_1_rmse: 0.61087 |  0:02:15s
epoch 68 | loss: 0.36643 | val_0_rmse: 0.59574 | val_1_rmse: 0.60425 |  0:02:17s
epoch 69 | loss: 0.35833 | val_0_rmse: 0.58412 | val_1_rmse: 0.59448 |  0:02:19s
epoch 70 | loss: 0.36325 | val_0_rmse: 0.57915 | val_1_rmse: 0.58983 |  0:02:21s
epoch 71 | loss: 0.3592  | val_0_rmse: 0.57228 | val_1_rmse: 0.58715 |  0:02:23s
epoch 72 | loss: 0.36063 | val_0_rmse: 0.58596 | val_1_rmse: 0.59919 |  0:02:25s
epoch 73 | loss: 0.35806 | val_0_rmse: 0.58349 | val_1_rmse: 0.59818 |  0:02:27s
epoch 74 | loss: 0.36982 | val_0_rmse: 0.58989 | val_1_rmse: 0.60497 |  0:02:29s
epoch 75 | loss: 0.359   | val_0_rmse: 0.5695  | val_1_rmse: 0.58139 |  0:02:31s
epoch 76 | loss: 0.35535 | val_0_rmse: 0.58909 | val_1_rmse: 0.60618 |  0:02:33s
epoch 77 | loss: 0.35045 | val_0_rmse: 0.57497 | val_1_rmse: 0.58611 |  0:02:35s
epoch 78 | loss: 0.3642  | val_0_rmse: 0.59082 | val_1_rmse: 0.60509 |  0:02:37s
epoch 79 | loss: 0.36529 | val_0_rmse: 0.57593 | val_1_rmse: 0.59058 |  0:02:39s
epoch 80 | loss: 0.3523  | val_0_rmse: 0.5895  | val_1_rmse: 0.60268 |  0:02:41s
epoch 81 | loss: 0.35565 | val_0_rmse: 0.60324 | val_1_rmse: 0.61514 |  0:02:43s
epoch 82 | loss: 0.35002 | val_0_rmse: 0.58919 | val_1_rmse: 0.6028  |  0:02:45s
epoch 83 | loss: 0.34933 | val_0_rmse: 0.59767 | val_1_rmse: 0.60858 |  0:02:47s
epoch 84 | loss: 0.35134 | val_0_rmse: 0.59598 | val_1_rmse: 0.60505 |  0:02:49s
epoch 85 | loss: 0.35155 | val_0_rmse: 0.59014 | val_1_rmse: 0.60495 |  0:02:51s
epoch 86 | loss: 0.35936 | val_0_rmse: 0.57134 | val_1_rmse: 0.58875 |  0:02:53s
epoch 87 | loss: 0.35167 | val_0_rmse: 0.61042 | val_1_rmse: 0.62064 |  0:02:55s
epoch 88 | loss: 0.34677 | val_0_rmse: 0.58087 | val_1_rmse: 0.59444 |  0:02:57s
epoch 89 | loss: 0.33976 | val_0_rmse: 0.56276 | val_1_rmse: 0.57957 |  0:02:59s
epoch 90 | loss: 0.34777 | val_0_rmse: 0.57133 | val_1_rmse: 0.58419 |  0:03:01s
epoch 91 | loss: 0.34485 | val_0_rmse: 0.58866 | val_1_rmse: 0.60373 |  0:03:03s
epoch 92 | loss: 0.36101 | val_0_rmse: 0.59983 | val_1_rmse: 0.61702 |  0:03:05s
epoch 93 | loss: 0.3503  | val_0_rmse: 0.58987 | val_1_rmse: 0.6034  |  0:03:07s
epoch 94 | loss: 0.34987 | val_0_rmse: 0.55857 | val_1_rmse: 0.57335 |  0:03:09s
epoch 95 | loss: 0.34532 | val_0_rmse: 0.57395 | val_1_rmse: 0.58973 |  0:03:11s
epoch 96 | loss: 0.35052 | val_0_rmse: 0.59222 | val_1_rmse: 0.61051 |  0:03:13s
epoch 97 | loss: 0.34142 | val_0_rmse: 0.59449 | val_1_rmse: 0.6079  |  0:03:15s
epoch 98 | loss: 0.33811 | val_0_rmse: 0.56765 | val_1_rmse: 0.58261 |  0:03:17s
epoch 99 | loss: 0.33817 | val_0_rmse: 0.5638  | val_1_rmse: 0.57966 |  0:03:19s
epoch 100| loss: 0.34005 | val_0_rmse: 0.57428 | val_1_rmse: 0.59193 |  0:03:21s
epoch 101| loss: 0.33958 | val_0_rmse: 0.57379 | val_1_rmse: 0.58464 |  0:03:23s
epoch 102| loss: 0.33908 | val_0_rmse: 0.55496 | val_1_rmse: 0.57066 |  0:03:25s
epoch 103| loss: 0.33763 | val_0_rmse: 0.5744  | val_1_rmse: 0.58937 |  0:03:27s
epoch 104| loss: 0.34675 | val_0_rmse: 0.55611 | val_1_rmse: 0.56886 |  0:03:29s
epoch 105| loss: 0.33636 | val_0_rmse: 0.54643 | val_1_rmse: 0.56423 |  0:03:31s
epoch 106| loss: 0.34019 | val_0_rmse: 0.60437 | val_1_rmse: 0.62186 |  0:03:33s
epoch 107| loss: 0.34225 | val_0_rmse: 0.56861 | val_1_rmse: 0.585   |  0:03:35s
epoch 108| loss: 0.33793 | val_0_rmse: 0.56593 | val_1_rmse: 0.58431 |  0:03:37s
epoch 109| loss: 0.33859 | val_0_rmse: 0.5547  | val_1_rmse: 0.57221 |  0:03:39s
epoch 110| loss: 0.35839 | val_0_rmse: 0.58734 | val_1_rmse: 0.60455 |  0:03:41s
epoch 111| loss: 0.35703 | val_0_rmse: 0.62116 | val_1_rmse: 0.6366  |  0:03:43s
epoch 112| loss: 0.34873 | val_0_rmse: 0.58313 | val_1_rmse: 0.59873 |  0:03:45s
epoch 113| loss: 0.34205 | val_0_rmse: 0.57047 | val_1_rmse: 0.58859 |  0:03:47s
epoch 114| loss: 0.34259 | val_0_rmse: 0.55504 | val_1_rmse: 0.57282 |  0:03:49s
epoch 115| loss: 0.33605 | val_0_rmse: 0.56288 | val_1_rmse: 0.57743 |  0:03:51s
epoch 116| loss: 0.34074 | val_0_rmse: 0.59366 | val_1_rmse: 0.61009 |  0:03:53s
epoch 117| loss: 0.34904 | val_0_rmse: 0.55913 | val_1_rmse: 0.57366 |  0:03:55s
epoch 118| loss: 0.33336 | val_0_rmse: 0.55584 | val_1_rmse: 0.57138 |  0:03:57s
epoch 119| loss: 0.3301  | val_0_rmse: 0.56516 | val_1_rmse: 0.58246 |  0:03:59s
epoch 120| loss: 0.33237 | val_0_rmse: 0.5803  | val_1_rmse: 0.59654 |  0:04:01s
epoch 121| loss: 0.33784 | val_0_rmse: 0.55727 | val_1_rmse: 0.57353 |  0:04:03s
epoch 122| loss: 0.33157 | val_0_rmse: 0.58731 | val_1_rmse: 0.60145 |  0:04:05s
epoch 123| loss: 0.33141 | val_0_rmse: 0.56466 | val_1_rmse: 0.57889 |  0:04:07s
epoch 124| loss: 0.32973 | val_0_rmse: 0.55083 | val_1_rmse: 0.56667 |  0:04:09s
epoch 125| loss: 0.33222 | val_0_rmse: 0.56109 | val_1_rmse: 0.57464 |  0:04:11s
epoch 126| loss: 0.32488 | val_0_rmse: 0.5534  | val_1_rmse: 0.5729  |  0:04:13s
epoch 127| loss: 0.33046 | val_0_rmse: 0.58337 | val_1_rmse: 0.59701 |  0:04:15s
epoch 128| loss: 0.33342 | val_0_rmse: 0.56923 | val_1_rmse: 0.58959 |  0:04:17s
epoch 129| loss: 0.3244  | val_0_rmse: 0.59702 | val_1_rmse: 0.61438 |  0:04:19s
epoch 130| loss: 0.32545 | val_0_rmse: 0.56436 | val_1_rmse: 0.57697 |  0:04:21s
epoch 131| loss: 0.33313 | val_0_rmse: 0.57537 | val_1_rmse: 0.59451 |  0:04:23s
epoch 132| loss: 0.32752 | val_0_rmse: 0.54021 | val_1_rmse: 0.56011 |  0:04:25s
epoch 133| loss: 0.32751 | val_0_rmse: 0.59562 | val_1_rmse: 0.61626 |  0:04:27s
epoch 134| loss: 0.32871 | val_0_rmse: 0.56604 | val_1_rmse: 0.58727 |  0:04:29s
epoch 135| loss: 0.32849 | val_0_rmse: 0.54669 | val_1_rmse: 0.56163 |  0:04:31s
epoch 136| loss: 0.32021 | val_0_rmse: 0.57085 | val_1_rmse: 0.59058 |  0:04:33s
epoch 137| loss: 0.33346 | val_0_rmse: 0.5988  | val_1_rmse: 0.61889 |  0:04:35s
epoch 138| loss: 0.33779 | val_0_rmse: 0.55428 | val_1_rmse: 0.57215 |  0:04:37s
epoch 139| loss: 0.32737 | val_0_rmse: 0.56543 | val_1_rmse: 0.58246 |  0:04:39s
epoch 140| loss: 0.32387 | val_0_rmse: 0.56719 | val_1_rmse: 0.58499 |  0:04:41s
epoch 141| loss: 0.33002 | val_0_rmse: 0.59376 | val_1_rmse: 0.60697 |  0:04:43s
epoch 142| loss: 0.32648 | val_0_rmse: 0.54238 | val_1_rmse: 0.56465 |  0:04:45s
epoch 143| loss: 0.32511 | val_0_rmse: 0.55548 | val_1_rmse: 0.57544 |  0:04:47s
epoch 144| loss: 0.32932 | val_0_rmse: 0.59209 | val_1_rmse: 0.60912 |  0:04:49s
epoch 145| loss: 0.32086 | val_0_rmse: 0.58417 | val_1_rmse: 0.60342 |  0:04:51s
epoch 146| loss: 0.31854 | val_0_rmse: 0.54832 | val_1_rmse: 0.56794 |  0:04:53s
epoch 147| loss: 0.32145 | val_0_rmse: 0.58681 | val_1_rmse: 0.60658 |  0:04:55s
epoch 148| loss: 0.32838 | val_0_rmse: 0.56309 | val_1_rmse: 0.57886 |  0:04:57s
epoch 149| loss: 0.32713 | val_0_rmse: 0.5778  | val_1_rmse: 0.59366 |  0:04:59s
Stop training because you reached max_epochs = 150 with best_epoch = 132 and best_val_1_rmse = 0.56011
Best weights from best epoch are automatically used!
ended training at: 13:40:18
Feature importance:
[('Area', 0.23574146164902643), ('Baths', 0.20235117682046133), ('Beds', 0.030368000027202413), ('Latitude', 0.19707327248753667), ('Longitude', 0.23351080809947758), ('Month', 0.0), ('Year', 0.10095528091629559)]
Mean squared error is of 6640033921.613871
Mean absolute error:56309.84622643519
MAPE:0.17629220073658125
R2 score:0.7015020239266234
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:40:18
epoch 0  | loss: 0.61136 | val_0_rmse: 0.73522 | val_1_rmse: 0.73042 |  0:00:05s
epoch 1  | loss: 0.51906 | val_0_rmse: 0.73702 | val_1_rmse: 0.73112 |  0:00:11s
epoch 2  | loss: 0.4606  | val_0_rmse: 0.66219 | val_1_rmse: 0.65532 |  0:00:16s
epoch 3  | loss: 0.41236 | val_0_rmse: 0.6382  | val_1_rmse: 0.63036 |  0:00:22s
epoch 4  | loss: 0.40131 | val_0_rmse: 0.69944 | val_1_rmse: 0.68695 |  0:00:27s
epoch 5  | loss: 0.39691 | val_0_rmse: 0.64183 | val_1_rmse: 0.6347  |  0:00:33s
epoch 6  | loss: 0.38649 | val_0_rmse: 0.64762 | val_1_rmse: 0.64137 |  0:00:38s
epoch 7  | loss: 0.37748 | val_0_rmse: 0.60924 | val_1_rmse: 0.6011  |  0:00:44s
epoch 8  | loss: 0.37668 | val_0_rmse: 0.64706 | val_1_rmse: 0.6423  |  0:00:49s
epoch 9  | loss: 0.37629 | val_0_rmse: 0.62198 | val_1_rmse: 0.61628 |  0:00:55s
epoch 10 | loss: 0.37285 | val_0_rmse: 0.62726 | val_1_rmse: 0.62273 |  0:01:01s
epoch 11 | loss: 0.36667 | val_0_rmse: 0.60553 | val_1_rmse: 0.59896 |  0:01:06s
epoch 12 | loss: 0.36662 | val_0_rmse: 0.64431 | val_1_rmse: 0.64252 |  0:01:12s
epoch 13 | loss: 0.36668 | val_0_rmse: 0.64819 | val_1_rmse: 0.64135 |  0:01:17s
epoch 14 | loss: 0.37184 | val_0_rmse: 0.60154 | val_1_rmse: 0.59435 |  0:01:23s
epoch 15 | loss: 0.36672 | val_0_rmse: 0.63776 | val_1_rmse: 0.63315 |  0:01:28s
epoch 16 | loss: 0.36657 | val_0_rmse: 0.60828 | val_1_rmse: 0.59818 |  0:01:34s
epoch 17 | loss: 0.3638  | val_0_rmse: 0.62312 | val_1_rmse: 0.61826 |  0:01:39s
epoch 18 | loss: 0.36101 | val_0_rmse: 0.6094  | val_1_rmse: 0.6026  |  0:01:45s
epoch 19 | loss: 0.363   | val_0_rmse: 0.59986 | val_1_rmse: 0.59022 |  0:01:50s
epoch 20 | loss: 0.35942 | val_0_rmse: 0.61379 | val_1_rmse: 0.60509 |  0:01:56s
epoch 21 | loss: 0.35864 | val_0_rmse: 0.61306 | val_1_rmse: 0.60922 |  0:02:02s
epoch 22 | loss: 0.35796 | val_0_rmse: 0.64399 | val_1_rmse: 0.63768 |  0:02:07s
epoch 23 | loss: 0.35974 | val_0_rmse: 0.60977 | val_1_rmse: 0.6048  |  0:02:13s
epoch 24 | loss: 0.35829 | val_0_rmse: 0.61225 | val_1_rmse: 0.60711 |  0:02:18s
epoch 25 | loss: 0.35972 | val_0_rmse: 0.61966 | val_1_rmse: 0.6108  |  0:02:24s
epoch 26 | loss: 0.35264 | val_0_rmse: 0.60409 | val_1_rmse: 0.59647 |  0:02:29s
epoch 27 | loss: 0.35478 | val_0_rmse: 0.63612 | val_1_rmse: 0.62584 |  0:02:35s
epoch 28 | loss: 0.35719 | val_0_rmse: 0.6097  | val_1_rmse: 0.60584 |  0:02:40s
epoch 29 | loss: 0.35418 | val_0_rmse: 0.59506 | val_1_rmse: 0.58763 |  0:02:46s
epoch 30 | loss: 0.35731 | val_0_rmse: 0.61884 | val_1_rmse: 0.61592 |  0:02:51s
epoch 31 | loss: 0.34765 | val_0_rmse: 0.59574 | val_1_rmse: 0.59014 |  0:02:57s
epoch 32 | loss: 0.34899 | val_0_rmse: 0.58998 | val_1_rmse: 0.5844  |  0:03:02s
epoch 33 | loss: 0.34736 | val_0_rmse: 0.63226 | val_1_rmse: 0.62359 |  0:03:08s
epoch 34 | loss: 0.34917 | val_0_rmse: 0.63282 | val_1_rmse: 0.62219 |  0:03:13s
epoch 35 | loss: 0.34601 | val_0_rmse: 0.58601 | val_1_rmse: 0.57799 |  0:03:19s
epoch 36 | loss: 0.34622 | val_0_rmse: 0.58526 | val_1_rmse: 0.57869 |  0:03:25s
epoch 37 | loss: 0.34348 | val_0_rmse: 0.63354 | val_1_rmse: 0.63216 |  0:03:30s
epoch 38 | loss: 0.34906 | val_0_rmse: 0.64864 | val_1_rmse: 0.64713 |  0:03:36s
epoch 39 | loss: 0.3553  | val_0_rmse: 0.71522 | val_1_rmse: 0.71392 |  0:03:41s
epoch 40 | loss: 0.35237 | val_0_rmse: 0.62122 | val_1_rmse: 0.61224 |  0:03:47s
epoch 41 | loss: 0.34597 | val_0_rmse: 0.63235 | val_1_rmse: 0.62585 |  0:03:52s
epoch 42 | loss: 0.34417 | val_0_rmse: 0.5819  | val_1_rmse: 0.57774 |  0:03:58s
epoch 43 | loss: 0.34597 | val_0_rmse: 0.6179  | val_1_rmse: 0.61572 |  0:04:03s
epoch 44 | loss: 0.34291 | val_0_rmse: 0.62087 | val_1_rmse: 0.61921 |  0:04:09s
epoch 45 | loss: 0.34162 | val_0_rmse: 0.57665 | val_1_rmse: 0.57004 |  0:04:14s
epoch 46 | loss: 0.34149 | val_0_rmse: 0.62656 | val_1_rmse: 0.61831 |  0:04:20s
epoch 47 | loss: 0.34175 | val_0_rmse: 0.6026  | val_1_rmse: 0.59839 |  0:04:26s
epoch 48 | loss: 0.35282 | val_0_rmse: 0.62165 | val_1_rmse: 0.61405 |  0:04:31s
epoch 49 | loss: 0.34003 | val_0_rmse: 0.59875 | val_1_rmse: 0.59428 |  0:04:37s
epoch 50 | loss: 0.37587 | val_0_rmse: 0.68626 | val_1_rmse: 0.68277 |  0:04:42s
epoch 51 | loss: 0.37973 | val_0_rmse: 0.59428 | val_1_rmse: 0.585   |  0:04:48s
epoch 52 | loss: 0.35627 | val_0_rmse: 0.67622 | val_1_rmse: 0.67233 |  0:04:53s
epoch 53 | loss: 0.34887 | val_0_rmse: 0.59018 | val_1_rmse: 0.58204 |  0:04:59s
epoch 54 | loss: 0.34192 | val_0_rmse: 0.57122 | val_1_rmse: 0.56588 |  0:05:04s
epoch 55 | loss: 0.34286 | val_0_rmse: 0.74405 | val_1_rmse: 0.74332 |  0:05:10s
epoch 56 | loss: 0.33913 | val_0_rmse: 0.6275  | val_1_rmse: 0.62322 |  0:05:16s
epoch 57 | loss: 0.34121 | val_0_rmse: 0.59799 | val_1_rmse: 0.59474 |  0:05:21s
epoch 58 | loss: 0.33971 | val_0_rmse: 0.76514 | val_1_rmse: 0.75314 |  0:05:27s
epoch 59 | loss: 0.33853 | val_0_rmse: 0.70396 | val_1_rmse: 0.70305 |  0:05:33s
epoch 60 | loss: 0.33726 | val_0_rmse: 0.6162  | val_1_rmse: 0.61515 |  0:05:38s
epoch 61 | loss: 0.33463 | val_0_rmse: 0.60132 | val_1_rmse: 0.59702 |  0:05:44s
epoch 62 | loss: 0.34212 | val_0_rmse: 0.60895 | val_1_rmse: 0.60715 |  0:05:49s
epoch 63 | loss: 0.33557 | val_0_rmse: 0.61313 | val_1_rmse: 0.61252 |  0:05:55s
epoch 64 | loss: 0.33628 | val_0_rmse: 0.5776  | val_1_rmse: 0.57342 |  0:06:00s
epoch 65 | loss: 0.33499 | val_0_rmse: 0.57633 | val_1_rmse: 0.57291 |  0:06:06s
epoch 66 | loss: 0.33554 | val_0_rmse: 0.58697 | val_1_rmse: 0.58162 |  0:06:11s
epoch 67 | loss: 0.3345  | val_0_rmse: 0.6238  | val_1_rmse: 0.62352 |  0:06:17s
epoch 68 | loss: 0.33578 | val_0_rmse: 0.61357 | val_1_rmse: 0.60945 |  0:06:22s
epoch 69 | loss: 0.33306 | val_0_rmse: 0.58245 | val_1_rmse: 0.5793  |  0:06:28s
epoch 70 | loss: 0.33872 | val_0_rmse: 0.59185 | val_1_rmse: 0.5899  |  0:06:33s
epoch 71 | loss: 0.33343 | val_0_rmse: 0.57423 | val_1_rmse: 0.56983 |  0:06:39s
epoch 72 | loss: 0.33365 | val_0_rmse: 0.8025  | val_1_rmse: 0.79111 |  0:06:45s
epoch 73 | loss: 0.33156 | val_0_rmse: 0.62122 | val_1_rmse: 0.61779 |  0:06:50s
epoch 74 | loss: 0.33356 | val_0_rmse: 0.61701 | val_1_rmse: 0.61599 |  0:06:56s
epoch 75 | loss: 0.33266 | val_0_rmse: 0.56819 | val_1_rmse: 0.56433 |  0:07:01s
epoch 76 | loss: 0.33542 | val_0_rmse: 0.6018  | val_1_rmse: 0.59592 |  0:07:07s
epoch 77 | loss: 0.3319  | val_0_rmse: 0.58117 | val_1_rmse: 0.57946 |  0:07:12s
epoch 78 | loss: 0.32903 | val_0_rmse: 0.63963 | val_1_rmse: 0.63973 |  0:07:18s
epoch 79 | loss: 0.33596 | val_0_rmse: 0.63545 | val_1_rmse: 0.63432 |  0:07:23s
epoch 80 | loss: 0.33104 | val_0_rmse: 0.60516 | val_1_rmse: 0.60137 |  0:07:29s
epoch 81 | loss: 0.32939 | val_0_rmse: 0.63597 | val_1_rmse: 0.63658 |  0:07:34s
epoch 82 | loss: 0.33189 | val_0_rmse: 0.59516 | val_1_rmse: 0.59112 |  0:07:40s
epoch 83 | loss: 0.32685 | val_0_rmse: 0.58647 | val_1_rmse: 0.58495 |  0:07:46s
epoch 84 | loss: 0.32722 | val_0_rmse: 0.67218 | val_1_rmse: 0.66791 |  0:07:51s
epoch 85 | loss: 0.3284  | val_0_rmse: 0.609   | val_1_rmse: 0.60709 |  0:07:57s
epoch 86 | loss: 0.32748 | val_0_rmse: 0.59217 | val_1_rmse: 0.59026 |  0:08:02s
epoch 87 | loss: 0.3338  | val_0_rmse: 0.62348 | val_1_rmse: 0.62084 |  0:08:08s
epoch 88 | loss: 0.32687 | val_0_rmse: 0.6837  | val_1_rmse: 0.67738 |  0:08:13s
epoch 89 | loss: 0.32658 | val_0_rmse: 1.00547 | val_1_rmse: 0.99828 |  0:08:19s
epoch 90 | loss: 0.33051 | val_0_rmse: 0.59426 | val_1_rmse: 0.59156 |  0:08:24s
epoch 91 | loss: 0.32821 | val_0_rmse: 0.622   | val_1_rmse: 0.61631 |  0:08:30s
epoch 92 | loss: 0.32727 | val_0_rmse: 0.59405 | val_1_rmse: 0.58984 |  0:08:35s
epoch 93 | loss: 0.32733 | val_0_rmse: 0.73589 | val_1_rmse: 0.72996 |  0:08:41s
epoch 94 | loss: 0.32524 | val_0_rmse: 0.68442 | val_1_rmse: 0.67879 |  0:08:47s
epoch 95 | loss: 0.32654 | val_0_rmse: 0.56025 | val_1_rmse: 0.55883 |  0:08:52s
epoch 96 | loss: 0.32703 | val_0_rmse: 0.56012 | val_1_rmse: 0.55923 |  0:08:58s
epoch 97 | loss: 0.32636 | val_0_rmse: 0.68186 | val_1_rmse: 0.67498 |  0:09:03s
epoch 98 | loss: 0.3306  | val_0_rmse: 0.60125 | val_1_rmse: 0.59956 |  0:09:09s
epoch 99 | loss: 0.32506 | val_0_rmse: 0.62771 | val_1_rmse: 0.62272 |  0:09:14s
epoch 100| loss: 0.32556 | val_0_rmse: 0.58809 | val_1_rmse: 0.58524 |  0:09:20s
epoch 101| loss: 0.33253 | val_0_rmse: 0.62555 | val_1_rmse: 0.62326 |  0:09:25s
epoch 102| loss: 0.33132 | val_0_rmse: 0.61415 | val_1_rmse: 0.61281 |  0:09:31s
epoch 103| loss: 0.32934 | val_0_rmse: 0.6254  | val_1_rmse: 0.61998 |  0:09:36s
epoch 104| loss: 0.32536 | val_0_rmse: 0.59483 | val_1_rmse: 0.59235 |  0:09:42s
epoch 105| loss: 0.32818 | val_0_rmse: 0.60477 | val_1_rmse: 0.60074 |  0:09:48s
epoch 106| loss: 0.33242 | val_0_rmse: 0.58745 | val_1_rmse: 0.58711 |  0:09:53s
epoch 107| loss: 0.32536 | val_0_rmse: 0.60408 | val_1_rmse: 0.59909 |  0:09:59s
epoch 108| loss: 0.32657 | val_0_rmse: 0.76941 | val_1_rmse: 0.75784 |  0:10:04s
epoch 109| loss: 0.32875 | val_0_rmse: 0.62309 | val_1_rmse: 0.62222 |  0:10:10s
epoch 110| loss: 0.3234  | val_0_rmse: 0.92421 | val_1_rmse: 0.91369 |  0:10:15s
epoch 111| loss: 0.3239  | val_0_rmse: 0.60976 | val_1_rmse: 0.60735 |  0:10:21s
epoch 112| loss: 0.32413 | val_0_rmse: 0.63472 | val_1_rmse: 0.62955 |  0:10:26s
epoch 113| loss: 0.32145 | val_0_rmse: 0.63104 | val_1_rmse: 0.62663 |  0:10:32s
epoch 114| loss: 0.3216  | val_0_rmse: 0.61885 | val_1_rmse: 0.61528 |  0:10:37s
epoch 115| loss: 0.32316 | val_0_rmse: 0.66325 | val_1_rmse: 0.65841 |  0:10:43s
epoch 116| loss: 0.32283 | val_0_rmse: 0.61593 | val_1_rmse: 0.61591 |  0:10:48s
epoch 117| loss: 0.32268 | val_0_rmse: 0.59478 | val_1_rmse: 0.5927  |  0:10:54s
epoch 118| loss: 0.3249  | val_0_rmse: 0.84869 | val_1_rmse: 0.83854 |  0:10:59s
epoch 119| loss: 0.32666 | val_0_rmse: 0.5826  | val_1_rmse: 0.57958 |  0:11:05s
epoch 120| loss: 0.32062 | val_0_rmse: 0.60236 | val_1_rmse: 0.59967 |  0:11:11s
epoch 121| loss: 0.32128 | val_0_rmse: 0.56919 | val_1_rmse: 0.56739 |  0:11:16s
epoch 122| loss: 0.31945 | val_0_rmse: 0.61278 | val_1_rmse: 0.61167 |  0:11:22s
epoch 123| loss: 0.32181 | val_0_rmse: 0.64212 | val_1_rmse: 0.63973 |  0:11:27s
epoch 124| loss: 0.32172 | val_0_rmse: 0.67133 | val_1_rmse: 0.66884 |  0:11:33s
epoch 125| loss: 0.32028 | val_0_rmse: 0.62383 | val_1_rmse: 0.6212  |  0:11:38s

Early stopping occured at epoch 125 with best_epoch = 95 and best_val_1_rmse = 0.55883
Best weights from best epoch are automatically used!
ended training at: 13:51:59
Feature importance:
[('Area', 0.31858787901418223), ('Baths', 0.26359276473843896), ('Beds', 0.0), ('Latitude', 0.24512138165891398), ('Longitude', 0.17263088558320885), ('Month', 3.3526892323953914e-05), ('Year', 3.356211293203085e-05)]
Mean squared error is of 2113506107.5322917
Mean absolute error:33104.590383725365
MAPE:0.3252812736410166
R2 score:0.6861153574928612
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 13:52:00
epoch 0  | loss: 0.5942  | val_0_rmse: 0.73096 | val_1_rmse: 0.74082 |  0:00:05s
epoch 1  | loss: 0.49828 | val_0_rmse: 0.70858 | val_1_rmse: 0.71404 |  0:00:11s
epoch 2  | loss: 0.44479 | val_0_rmse: 0.65449 | val_1_rmse: 0.65796 |  0:00:16s
epoch 3  | loss: 0.42365 | val_0_rmse: 0.67333 | val_1_rmse: 0.67399 |  0:00:22s
epoch 4  | loss: 0.4126  | val_0_rmse: 0.63346 | val_1_rmse: 0.63561 |  0:00:27s
epoch 5  | loss: 0.40448 | val_0_rmse: 0.67317 | val_1_rmse: 0.67431 |  0:00:33s
epoch 6  | loss: 0.3943  | val_0_rmse: 0.65818 | val_1_rmse: 0.65875 |  0:00:38s
epoch 7  | loss: 0.3889  | val_0_rmse: 0.66004 | val_1_rmse: 0.65871 |  0:00:44s
epoch 8  | loss: 0.37797 | val_0_rmse: 0.64819 | val_1_rmse: 0.64775 |  0:00:49s
epoch 9  | loss: 0.38342 | val_0_rmse: 0.64285 | val_1_rmse: 0.64286 |  0:00:55s
epoch 10 | loss: 0.37403 | val_0_rmse: 0.63035 | val_1_rmse: 0.62973 |  0:01:01s
epoch 11 | loss: 0.37449 | val_0_rmse: 0.64257 | val_1_rmse: 0.64591 |  0:01:06s
epoch 12 | loss: 0.3738  | val_0_rmse: 0.61573 | val_1_rmse: 0.61586 |  0:01:12s
epoch 13 | loss: 0.3729  | val_0_rmse: 0.60544 | val_1_rmse: 0.60379 |  0:01:17s
epoch 14 | loss: 0.36609 | val_0_rmse: 0.63331 | val_1_rmse: 0.63469 |  0:01:23s
epoch 15 | loss: 0.36516 | val_0_rmse: 0.60699 | val_1_rmse: 0.60551 |  0:01:29s
epoch 16 | loss: 0.35938 | val_0_rmse: 0.59673 | val_1_rmse: 0.5943  |  0:01:34s
epoch 17 | loss: 0.35507 | val_0_rmse: 0.60291 | val_1_rmse: 0.60059 |  0:01:40s
epoch 18 | loss: 0.35749 | val_0_rmse: 0.60309 | val_1_rmse: 0.60098 |  0:01:45s
epoch 19 | loss: 0.35287 | val_0_rmse: 0.66891 | val_1_rmse: 0.67502 |  0:01:51s
epoch 20 | loss: 0.35322 | val_0_rmse: 0.63455 | val_1_rmse: 0.63098 |  0:01:56s
epoch 21 | loss: 0.35645 | val_0_rmse: 0.67454 | val_1_rmse: 0.6774  |  0:02:02s
epoch 22 | loss: 0.35151 | val_0_rmse: 0.62707 | val_1_rmse: 0.62505 |  0:02:07s
epoch 23 | loss: 0.34856 | val_0_rmse: 0.61175 | val_1_rmse: 0.6088  |  0:02:13s
epoch 24 | loss: 0.35091 | val_0_rmse: 0.5839  | val_1_rmse: 0.58377 |  0:02:19s
epoch 25 | loss: 0.35314 | val_0_rmse: 0.62538 | val_1_rmse: 0.62697 |  0:02:24s
epoch 26 | loss: 0.34959 | val_0_rmse: 0.62193 | val_1_rmse: 0.62145 |  0:02:30s
epoch 27 | loss: 0.34799 | val_0_rmse: 0.61099 | val_1_rmse: 0.6117  |  0:02:35s
epoch 28 | loss: 0.34704 | val_0_rmse: 0.63929 | val_1_rmse: 0.64416 |  0:02:41s
epoch 29 | loss: 0.35267 | val_0_rmse: 0.68255 | val_1_rmse: 0.68851 |  0:02:46s
epoch 30 | loss: 0.34672 | val_0_rmse: 0.66979 | val_1_rmse: 0.66648 |  0:02:52s
epoch 31 | loss: 0.34459 | val_0_rmse: 0.61095 | val_1_rmse: 0.61404 |  0:02:57s
epoch 32 | loss: 0.34495 | val_0_rmse: 0.62624 | val_1_rmse: 0.6229  |  0:03:03s
epoch 33 | loss: 0.34609 | val_0_rmse: 0.62873 | val_1_rmse: 0.62693 |  0:03:08s
epoch 34 | loss: 0.34481 | val_0_rmse: 0.67418 | val_1_rmse: 0.67866 |  0:03:14s
epoch 35 | loss: 0.35036 | val_0_rmse: 0.61365 | val_1_rmse: 0.61574 |  0:03:20s
epoch 36 | loss: 0.34314 | val_0_rmse: 0.5964  | val_1_rmse: 0.5945  |  0:03:25s
epoch 37 | loss: 0.34065 | val_0_rmse: 0.6488  | val_1_rmse: 0.65272 |  0:03:31s
epoch 38 | loss: 0.34403 | val_0_rmse: 0.65642 | val_1_rmse: 0.65888 |  0:03:36s
epoch 39 | loss: 0.34154 | val_0_rmse: 0.65433 | val_1_rmse: 0.65604 |  0:03:42s
epoch 40 | loss: 0.34326 | val_0_rmse: 0.68368 | val_1_rmse: 0.68062 |  0:03:47s
epoch 41 | loss: 0.34213 | val_0_rmse: 0.62436 | val_1_rmse: 0.62942 |  0:03:53s
epoch 42 | loss: 0.34345 | val_0_rmse: 0.6042  | val_1_rmse: 0.60068 |  0:03:58s
epoch 43 | loss: 0.3428  | val_0_rmse: 0.59196 | val_1_rmse: 0.58867 |  0:04:04s
epoch 44 | loss: 0.33996 | val_0_rmse: 0.60848 | val_1_rmse: 0.60654 |  0:04:10s
epoch 45 | loss: 0.33737 | val_0_rmse: 0.62454 | val_1_rmse: 0.62475 |  0:04:15s
epoch 46 | loss: 0.33718 | val_0_rmse: 0.71964 | val_1_rmse: 0.72329 |  0:04:21s
epoch 47 | loss: 0.34073 | val_0_rmse: 0.6284  | val_1_rmse: 0.63246 |  0:04:26s
epoch 48 | loss: 0.3405  | val_0_rmse: 0.6359  | val_1_rmse: 0.64188 |  0:04:32s
epoch 49 | loss: 0.33604 | val_0_rmse: 0.63248 | val_1_rmse: 0.63336 |  0:04:37s
epoch 50 | loss: 0.34013 | val_0_rmse: 0.58699 | val_1_rmse: 0.58653 |  0:04:43s
epoch 51 | loss: 0.33749 | val_0_rmse: 0.58143 | val_1_rmse: 0.57964 |  0:04:49s
epoch 52 | loss: 0.3396  | val_0_rmse: 0.64285 | val_1_rmse: 0.6467  |  0:04:54s
epoch 53 | loss: 0.33761 | val_0_rmse: 0.65057 | val_1_rmse: 0.64698 |  0:05:00s
epoch 54 | loss: 0.3349  | val_0_rmse: 0.63465 | val_1_rmse: 0.6375  |  0:05:05s
epoch 55 | loss: 0.33704 | val_0_rmse: 0.61638 | val_1_rmse: 0.61693 |  0:05:11s
epoch 56 | loss: 0.33675 | val_0_rmse: 0.61298 | val_1_rmse: 0.61791 |  0:05:16s
epoch 57 | loss: 0.34382 | val_0_rmse: 0.5872  | val_1_rmse: 0.58619 |  0:05:22s
epoch 58 | loss: 0.3395  | val_0_rmse: 0.58213 | val_1_rmse: 0.58096 |  0:05:28s
epoch 59 | loss: 0.33906 | val_0_rmse: 0.70001 | val_1_rmse: 0.70259 |  0:05:33s
epoch 60 | loss: 0.33568 | val_0_rmse: 0.63451 | val_1_rmse: 0.64025 |  0:05:39s
epoch 61 | loss: 0.33742 | val_0_rmse: 0.58012 | val_1_rmse: 0.58229 |  0:05:44s
epoch 62 | loss: 0.34111 | val_0_rmse: 0.59917 | val_1_rmse: 0.59831 |  0:05:50s
epoch 63 | loss: 0.35304 | val_0_rmse: 0.67822 | val_1_rmse: 0.67668 |  0:05:55s
epoch 64 | loss: 0.34905 | val_0_rmse: 0.68713 | val_1_rmse: 0.6868  |  0:06:01s
epoch 65 | loss: 0.34851 | val_0_rmse: 0.66138 | val_1_rmse: 0.65703 |  0:06:06s
epoch 66 | loss: 0.34448 | val_0_rmse: 0.66096 | val_1_rmse: 0.66376 |  0:06:12s
epoch 67 | loss: 0.34432 | val_0_rmse: 0.65008 | val_1_rmse: 0.64893 |  0:06:17s
epoch 68 | loss: 0.34485 | val_0_rmse: 0.734   | val_1_rmse: 0.73127 |  0:06:23s
epoch 69 | loss: 0.34101 | val_0_rmse: 0.59439 | val_1_rmse: 0.59492 |  0:06:28s
epoch 70 | loss: 0.33922 | val_0_rmse: 0.64502 | val_1_rmse: 0.64394 |  0:06:34s
epoch 71 | loss: 0.34068 | val_0_rmse: 0.60473 | val_1_rmse: 0.60889 |  0:06:40s
epoch 72 | loss: 0.33657 | val_0_rmse: 0.64828 | val_1_rmse: 0.64458 |  0:06:45s
epoch 73 | loss: 0.3385  | val_0_rmse: 0.61616 | val_1_rmse: 0.62094 |  0:06:51s
epoch 74 | loss: 0.33817 | val_0_rmse: 0.59615 | val_1_rmse: 0.59455 |  0:06:56s
epoch 75 | loss: 0.34009 | val_0_rmse: 0.60641 | val_1_rmse: 0.60566 |  0:07:02s
epoch 76 | loss: 0.33934 | val_0_rmse: 0.58204 | val_1_rmse: 0.57963 |  0:07:07s
epoch 77 | loss: 0.33687 | val_0_rmse: 0.58127 | val_1_rmse: 0.58241 |  0:07:13s
epoch 78 | loss: 0.33759 | val_0_rmse: 0.75704 | val_1_rmse: 0.75415 |  0:07:18s
epoch 79 | loss: 0.3373  | val_0_rmse: 0.62336 | val_1_rmse: 0.62919 |  0:07:24s
epoch 80 | loss: 0.33673 | val_0_rmse: 0.74665 | val_1_rmse: 0.74431 |  0:07:29s
epoch 81 | loss: 0.33771 | val_0_rmse: 0.66883 | val_1_rmse: 0.67402 |  0:07:35s
epoch 82 | loss: 0.33757 | val_0_rmse: 0.58764 | val_1_rmse: 0.58994 |  0:07:41s
epoch 83 | loss: 0.33514 | val_0_rmse: 0.60744 | val_1_rmse: 0.61016 |  0:07:46s
epoch 84 | loss: 0.33696 | val_0_rmse: 0.64279 | val_1_rmse: 0.64452 |  0:07:52s
epoch 85 | loss: 0.33732 | val_0_rmse: 0.66404 | val_1_rmse: 0.66486 |  0:07:57s
epoch 86 | loss: 0.33963 | val_0_rmse: 0.58378 | val_1_rmse: 0.58026 |  0:08:03s
epoch 87 | loss: 0.33746 | val_0_rmse: 0.65945 | val_1_rmse: 0.66487 |  0:08:08s
epoch 88 | loss: 0.33812 | val_0_rmse: 0.61955 | val_1_rmse: 0.61776 |  0:08:14s
epoch 89 | loss: 0.34004 | val_0_rmse: 0.68189 | val_1_rmse: 0.67983 |  0:08:19s
epoch 90 | loss: 0.33988 | val_0_rmse: 0.69428 | val_1_rmse: 0.69599 |  0:08:25s
epoch 91 | loss: 0.34069 | val_0_rmse: 0.65789 | val_1_rmse: 0.65608 |  0:08:30s
epoch 92 | loss: 0.33762 | val_0_rmse: 0.66714 | val_1_rmse: 0.66553 |  0:08:36s
epoch 93 | loss: 0.33796 | val_0_rmse: 0.56751 | val_1_rmse: 0.56804 |  0:08:42s
epoch 94 | loss: 0.33603 | val_0_rmse: 0.57465 | val_1_rmse: 0.57382 |  0:08:47s
epoch 95 | loss: 0.33727 | val_0_rmse: 0.8412  | val_1_rmse: 0.83529 |  0:08:53s
epoch 96 | loss: 0.33641 | val_0_rmse: 0.69353 | val_1_rmse: 0.68997 |  0:08:58s
epoch 97 | loss: 0.33454 | val_0_rmse: 0.59877 | val_1_rmse: 0.59811 |  0:09:04s
epoch 98 | loss: 0.33504 | val_0_rmse: 0.68772 | val_1_rmse: 0.6891  |  0:09:09s
epoch 99 | loss: 0.34079 | val_0_rmse: 0.74271 | val_1_rmse: 0.74051 |  0:09:15s
epoch 100| loss: 0.33859 | val_0_rmse: 0.60568 | val_1_rmse: 0.6044  |  0:09:20s
epoch 101| loss: 0.33891 | val_0_rmse: 0.59164 | val_1_rmse: 0.59145 |  0:09:26s
epoch 102| loss: 0.33894 | val_0_rmse: 0.60239 | val_1_rmse: 0.60593 |  0:09:31s
epoch 103| loss: 0.33627 | val_0_rmse: 0.65972 | val_1_rmse: 0.66182 |  0:09:37s
epoch 104| loss: 0.33561 | val_0_rmse: 0.61044 | val_1_rmse: 0.60832 |  0:09:43s
epoch 105| loss: 0.33581 | val_0_rmse: 0.61318 | val_1_rmse: 0.61554 |  0:09:48s
epoch 106| loss: 0.33645 | val_0_rmse: 0.59546 | val_1_rmse: 0.59708 |  0:09:54s
epoch 107| loss: 0.33612 | val_0_rmse: 0.7151  | val_1_rmse: 0.71132 |  0:09:59s
epoch 108| loss: 0.33384 | val_0_rmse: 0.61155 | val_1_rmse: 0.60797 |  0:10:05s
epoch 109| loss: 0.33554 | val_0_rmse: 0.60763 | val_1_rmse: 0.60464 |  0:10:10s
epoch 110| loss: 0.33243 | val_0_rmse: 0.57231 | val_1_rmse: 0.56987 |  0:10:16s
epoch 111| loss: 0.33444 | val_0_rmse: 0.61013 | val_1_rmse: 0.60678 |  0:10:22s
epoch 112| loss: 0.33118 | val_0_rmse: 0.57491 | val_1_rmse: 0.57214 |  0:10:27s
epoch 113| loss: 0.3337  | val_0_rmse: 0.67875 | val_1_rmse: 0.68533 |  0:10:33s
epoch 114| loss: 0.33141 | val_0_rmse: 0.60531 | val_1_rmse: 0.60211 |  0:10:38s
epoch 115| loss: 0.33414 | val_0_rmse: 0.68209 | val_1_rmse: 0.68431 |  0:10:44s
epoch 116| loss: 0.33183 | val_0_rmse: 0.59998 | val_1_rmse: 0.5952  |  0:10:49s
epoch 117| loss: 0.33165 | val_0_rmse: 0.60821 | val_1_rmse: 0.60949 |  0:10:55s
epoch 118| loss: 0.33537 | val_0_rmse: 0.61305 | val_1_rmse: 0.61235 |  0:11:00s
epoch 119| loss: 0.33465 | val_0_rmse: 0.57064 | val_1_rmse: 0.57039 |  0:11:06s
epoch 120| loss: 0.33191 | val_0_rmse: 0.62917 | val_1_rmse: 0.62636 |  0:11:11s
epoch 121| loss: 0.33241 | val_0_rmse: 0.56464 | val_1_rmse: 0.56512 |  0:11:17s
epoch 122| loss: 0.33315 | val_0_rmse: 0.62189 | val_1_rmse: 0.61886 |  0:11:23s
epoch 123| loss: 0.33277 | val_0_rmse: 0.59122 | val_1_rmse: 0.59164 |  0:11:28s
epoch 124| loss: 0.33327 | val_0_rmse: 0.64102 | val_1_rmse: 0.64591 |  0:11:34s
epoch 125| loss: 0.33211 | val_0_rmse: 0.65404 | val_1_rmse: 0.65292 |  0:11:39s
epoch 126| loss: 0.33178 | val_0_rmse: 0.58623 | val_1_rmse: 0.5884  |  0:11:45s
epoch 127| loss: 0.33307 | val_0_rmse: 0.71875 | val_1_rmse: 0.71651 |  0:11:50s
epoch 128| loss: 0.3381  | val_0_rmse: 0.58875 | val_1_rmse: 0.5894  |  0:11:56s
epoch 129| loss: 0.34967 | val_0_rmse: 0.72998 | val_1_rmse: 0.72572 |  0:12:01s
epoch 130| loss: 0.34244 | val_0_rmse: 0.64512 | val_1_rmse: 0.6424  |  0:12:07s
epoch 131| loss: 0.34273 | val_0_rmse: 0.63317 | val_1_rmse: 0.63472 |  0:12:12s
epoch 132| loss: 0.34035 | val_0_rmse: 0.6783  | val_1_rmse: 0.67697 |  0:12:18s
epoch 133| loss: 0.34012 | val_0_rmse: 0.64588 | val_1_rmse: 0.64649 |  0:12:24s
epoch 134| loss: 0.33661 | val_0_rmse: 0.62155 | val_1_rmse: 0.6245  |  0:12:29s
epoch 135| loss: 0.33633 | val_0_rmse: 0.73905 | val_1_rmse: 0.73673 |  0:12:35s
epoch 136| loss: 0.33798 | val_0_rmse: 0.57692 | val_1_rmse: 0.5745  |  0:12:40s
epoch 137| loss: 0.33733 | val_0_rmse: 0.5889  | val_1_rmse: 0.58611 |  0:12:46s
epoch 138| loss: 0.33747 | val_0_rmse: 0.65453 | val_1_rmse: 0.65005 |  0:12:51s
epoch 139| loss: 0.33648 | val_0_rmse: 0.60096 | val_1_rmse: 0.60203 |  0:12:57s
epoch 140| loss: 0.33537 | val_0_rmse: 0.60339 | val_1_rmse: 0.598   |  0:13:02s
epoch 141| loss: 0.33256 | val_0_rmse: 0.7049  | val_1_rmse: 0.70217 |  0:13:08s
epoch 142| loss: 0.33375 | val_0_rmse: 0.59137 | val_1_rmse: 0.5905  |  0:13:13s
epoch 143| loss: 0.33471 | val_0_rmse: 0.67844 | val_1_rmse: 0.68283 |  0:13:19s
epoch 144| loss: 0.33977 | val_0_rmse: 0.59302 | val_1_rmse: 0.59032 |  0:13:25s
epoch 145| loss: 0.33624 | val_0_rmse: 0.63549 | val_1_rmse: 0.63186 |  0:13:30s
epoch 146| loss: 0.33563 | val_0_rmse: 0.66056 | val_1_rmse: 0.66365 |  0:13:36s
epoch 147| loss: 0.33283 | val_0_rmse: 0.63907 | val_1_rmse: 0.64134 |  0:13:41s
epoch 148| loss: 0.33366 | val_0_rmse: 0.58011 | val_1_rmse: 0.57635 |  0:13:47s
epoch 149| loss: 0.33173 | val_0_rmse: 0.64466 | val_1_rmse: 0.64328 |  0:13:52s
Stop training because you reached max_epochs = 150 with best_epoch = 121 and best_val_1_rmse = 0.56512
Best weights from best epoch are automatically used!
ended training at: 14:05:54
Feature importance:
[('Area', 0.4712145329720242), ('Baths', 0.13335527261138413), ('Beds', 0.0), ('Latitude', 0.2148458997769376), ('Longitude', 0.18058429463965409), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2156491588.9813323
Mean absolute error:33678.65816690157
MAPE:0.33481589449518956
R2 score:0.680426968266311
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:05:55
epoch 0  | loss: 0.5946  | val_0_rmse: 0.71409 | val_1_rmse: 0.72405 |  0:00:05s
epoch 1  | loss: 0.47918 | val_0_rmse: 0.68294 | val_1_rmse: 0.68887 |  0:00:11s
epoch 2  | loss: 0.44354 | val_0_rmse: 0.73247 | val_1_rmse: 0.73737 |  0:00:16s
epoch 3  | loss: 0.40926 | val_0_rmse: 0.65377 | val_1_rmse: 0.6556  |  0:00:22s
epoch 4  | loss: 0.40654 | val_0_rmse: 0.69253 | val_1_rmse: 0.69619 |  0:00:27s
epoch 5  | loss: 0.40188 | val_0_rmse: 0.66832 | val_1_rmse: 0.67192 |  0:00:33s
epoch 6  | loss: 0.40414 | val_0_rmse: 0.66876 | val_1_rmse: 0.67235 |  0:00:38s
epoch 7  | loss: 0.39787 | val_0_rmse: 0.68731 | val_1_rmse: 0.69245 |  0:00:44s
epoch 8  | loss: 0.39374 | val_0_rmse: 0.67494 | val_1_rmse: 0.67709 |  0:00:49s
epoch 9  | loss: 0.39297 | val_0_rmse: 0.65694 | val_1_rmse: 0.65766 |  0:00:55s
epoch 10 | loss: 0.38998 | val_0_rmse: 0.70183 | val_1_rmse: 0.70469 |  0:01:00s
epoch 11 | loss: 0.37766 | val_0_rmse: 0.67591 | val_1_rmse: 0.67751 |  0:01:06s
epoch 12 | loss: 0.37356 | val_0_rmse: 0.66354 | val_1_rmse: 0.66619 |  0:01:11s
epoch 13 | loss: 0.36916 | val_0_rmse: 0.64469 | val_1_rmse: 0.64663 |  0:01:17s
epoch 14 | loss: 0.37063 | val_0_rmse: 0.66186 | val_1_rmse: 0.66033 |  0:01:22s
epoch 15 | loss: 0.36311 | val_0_rmse: 0.63124 | val_1_rmse: 0.63076 |  0:01:28s
epoch 16 | loss: 0.36445 | val_0_rmse: 0.6933  | val_1_rmse: 0.6953  |  0:01:34s
epoch 17 | loss: 0.35987 | val_0_rmse: 0.64145 | val_1_rmse: 0.64313 |  0:01:39s
epoch 18 | loss: 0.36419 | val_0_rmse: 0.60235 | val_1_rmse: 0.60257 |  0:01:45s
epoch 19 | loss: 0.36467 | val_0_rmse: 0.62756 | val_1_rmse: 0.62733 |  0:01:50s
epoch 20 | loss: 0.36295 | val_0_rmse: 0.62243 | val_1_rmse: 0.62323 |  0:01:56s
epoch 21 | loss: 0.361   | val_0_rmse: 0.69603 | val_1_rmse: 0.69894 |  0:02:01s
epoch 22 | loss: 0.35595 | val_0_rmse: 0.64473 | val_1_rmse: 0.64698 |  0:02:07s
epoch 23 | loss: 0.35776 | val_0_rmse: 0.68478 | val_1_rmse: 0.68921 |  0:02:12s
epoch 24 | loss: 0.35981 | val_0_rmse: 0.63756 | val_1_rmse: 0.64012 |  0:02:18s
epoch 25 | loss: 0.35592 | val_0_rmse: 0.64353 | val_1_rmse: 0.64634 |  0:02:24s
epoch 26 | loss: 0.35575 | val_0_rmse: 0.63446 | val_1_rmse: 0.63245 |  0:02:29s
epoch 27 | loss: 0.35671 | val_0_rmse: 0.58831 | val_1_rmse: 0.58583 |  0:02:35s
epoch 28 | loss: 0.35644 | val_0_rmse: 0.63958 | val_1_rmse: 0.63969 |  0:02:40s
epoch 29 | loss: 0.35482 | val_0_rmse: 0.65872 | val_1_rmse: 0.66353 |  0:02:46s
epoch 30 | loss: 0.35413 | val_0_rmse: 0.68918 | val_1_rmse: 0.6911  |  0:02:51s
epoch 31 | loss: 0.35765 | val_0_rmse: 0.73608 | val_1_rmse: 0.74064 |  0:02:57s
epoch 32 | loss: 0.36613 | val_0_rmse: 0.61019 | val_1_rmse: 0.61044 |  0:03:02s
epoch 33 | loss: 0.36135 | val_0_rmse: 0.63314 | val_1_rmse: 0.63157 |  0:03:08s
epoch 34 | loss: 0.3565  | val_0_rmse: 0.78477 | val_1_rmse: 0.78913 |  0:03:13s
epoch 35 | loss: 0.3572  | val_0_rmse: 0.64598 | val_1_rmse: 0.6455  |  0:03:19s
epoch 36 | loss: 0.35542 | val_0_rmse: 0.60184 | val_1_rmse: 0.60163 |  0:03:25s
epoch 37 | loss: 0.35472 | val_0_rmse: 0.73261 | val_1_rmse: 0.73617 |  0:03:30s
epoch 38 | loss: 0.35518 | val_0_rmse: 0.64307 | val_1_rmse: 0.64346 |  0:03:36s
epoch 39 | loss: 0.35228 | val_0_rmse: 0.61882 | val_1_rmse: 0.61835 |  0:03:41s
epoch 40 | loss: 0.3543  | val_0_rmse: 0.65066 | val_1_rmse: 0.65322 |  0:03:47s
epoch 41 | loss: 0.35333 | val_0_rmse: 0.63665 | val_1_rmse: 0.63743 |  0:03:52s
epoch 42 | loss: 0.35162 | val_0_rmse: 0.71844 | val_1_rmse: 0.72259 |  0:03:58s
epoch 43 | loss: 0.3546  | val_0_rmse: 0.71004 | val_1_rmse: 0.71394 |  0:04:03s
epoch 44 | loss: 0.35354 | val_0_rmse: 0.67613 | val_1_rmse: 0.67943 |  0:04:09s
epoch 45 | loss: 0.352   | val_0_rmse: 0.6307  | val_1_rmse: 0.63084 |  0:04:15s
epoch 46 | loss: 0.34797 | val_0_rmse: 0.65528 | val_1_rmse: 0.65595 |  0:04:20s
epoch 47 | loss: 0.34915 | val_0_rmse: 0.73431 | val_1_rmse: 0.73661 |  0:04:26s
epoch 48 | loss: 0.34845 | val_0_rmse: 0.60653 | val_1_rmse: 0.60452 |  0:04:31s
epoch 49 | loss: 0.35033 | val_0_rmse: 0.72573 | val_1_rmse: 0.73154 |  0:04:37s
epoch 50 | loss: 0.35147 | val_0_rmse: 0.68132 | val_1_rmse: 0.68078 |  0:04:42s
epoch 51 | loss: 0.34829 | val_0_rmse: 0.65966 | val_1_rmse: 0.66193 |  0:04:48s
epoch 52 | loss: 0.34713 | val_0_rmse: 0.69339 | val_1_rmse: 0.69672 |  0:04:53s
epoch 53 | loss: 0.34848 | val_0_rmse: 0.65628 | val_1_rmse: 0.65763 |  0:04:59s
epoch 54 | loss: 0.34492 | val_0_rmse: 0.61875 | val_1_rmse: 0.61827 |  0:05:04s
epoch 55 | loss: 0.34794 | val_0_rmse: 0.65317 | val_1_rmse: 0.65565 |  0:05:10s
epoch 56 | loss: 0.35032 | val_0_rmse: 0.64373 | val_1_rmse: 0.64451 |  0:05:15s
epoch 57 | loss: 0.34991 | val_0_rmse: 0.60853 | val_1_rmse: 0.60798 |  0:05:21s

Early stopping occured at epoch 57 with best_epoch = 27 and best_val_1_rmse = 0.58583
Best weights from best epoch are automatically used!
ended training at: 14:11:18
Feature importance:
[('Area', 0.3273865109505822), ('Baths', 0.1846116376247244), ('Beds', 0.0), ('Latitude', 0.2927258449946752), ('Longitude', 0.1437259572021224), ('Month', 0.0), ('Year', 0.05155004922789586)]
Mean squared error is of 2378074177.1036363
Mean absolute error:35290.115357348746
MAPE:0.3443432854779429
R2 score:0.6495805326628048
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:11:19
epoch 0  | loss: 0.60037 | val_0_rmse: 0.70053 | val_1_rmse: 0.69482 |  0:00:05s
epoch 1  | loss: 0.47427 | val_0_rmse: 0.68209 | val_1_rmse: 0.6751  |  0:00:11s
epoch 2  | loss: 0.42872 | val_0_rmse: 0.65072 | val_1_rmse: 0.64688 |  0:00:16s
epoch 3  | loss: 0.40864 | val_0_rmse: 0.6257  | val_1_rmse: 0.62061 |  0:00:22s
epoch 4  | loss: 0.39298 | val_0_rmse: 0.62235 | val_1_rmse: 0.61529 |  0:00:27s
epoch 5  | loss: 0.39008 | val_0_rmse: 0.62767 | val_1_rmse: 0.62022 |  0:00:33s
epoch 6  | loss: 0.37791 | val_0_rmse: 0.60412 | val_1_rmse: 0.59762 |  0:00:38s
epoch 7  | loss: 0.37496 | val_0_rmse: 0.68134 | val_1_rmse: 0.6752  |  0:00:44s
epoch 8  | loss: 0.37212 | val_0_rmse: 0.6481  | val_1_rmse: 0.6447  |  0:00:49s
epoch 9  | loss: 0.37355 | val_0_rmse: 0.63455 | val_1_rmse: 0.62709 |  0:00:55s
epoch 10 | loss: 0.36617 | val_0_rmse: 0.68412 | val_1_rmse: 0.68173 |  0:01:01s
epoch 11 | loss: 0.36685 | val_0_rmse: 0.59674 | val_1_rmse: 0.59179 |  0:01:06s
epoch 12 | loss: 0.36376 | val_0_rmse: 0.63394 | val_1_rmse: 0.62895 |  0:01:12s
epoch 13 | loss: 0.36139 | val_0_rmse: 0.59094 | val_1_rmse: 0.58604 |  0:01:17s
epoch 14 | loss: 0.36088 | val_0_rmse: 0.62706 | val_1_rmse: 0.6195  |  0:01:23s
epoch 15 | loss: 0.36095 | val_0_rmse: 0.82383 | val_1_rmse: 0.82349 |  0:01:28s
epoch 16 | loss: 0.36023 | val_0_rmse: 0.60777 | val_1_rmse: 0.60242 |  0:01:34s
epoch 17 | loss: 0.3661  | val_0_rmse: 0.62774 | val_1_rmse: 0.62296 |  0:01:39s
epoch 18 | loss: 0.36826 | val_0_rmse: 0.67755 | val_1_rmse: 0.67205 |  0:01:45s
epoch 19 | loss: 0.36418 | val_0_rmse: 0.58336 | val_1_rmse: 0.57689 |  0:01:50s
epoch 20 | loss: 0.36223 | val_0_rmse: 0.69116 | val_1_rmse: 0.68359 |  0:01:56s
epoch 21 | loss: 0.35698 | val_0_rmse: 0.62706 | val_1_rmse: 0.61947 |  0:02:02s
epoch 22 | loss: 0.35141 | val_0_rmse: 0.60422 | val_1_rmse: 0.5987  |  0:02:07s
epoch 23 | loss: 0.35581 | val_0_rmse: 0.63632 | val_1_rmse: 0.63173 |  0:02:13s
epoch 24 | loss: 0.35258 | val_0_rmse: 0.59379 | val_1_rmse: 0.58636 |  0:02:18s
epoch 25 | loss: 0.35379 | val_0_rmse: 0.65332 | val_1_rmse: 0.64784 |  0:02:24s
epoch 26 | loss: 0.35191 | val_0_rmse: 0.63589 | val_1_rmse: 0.63209 |  0:02:29s
epoch 27 | loss: 0.35454 | val_0_rmse: 0.67221 | val_1_rmse: 0.66319 |  0:02:35s
epoch 28 | loss: 0.35506 | val_0_rmse: 0.598   | val_1_rmse: 0.59022 |  0:02:40s
epoch 29 | loss: 0.35383 | val_0_rmse: 0.65404 | val_1_rmse: 0.64462 |  0:02:46s
epoch 30 | loss: 0.35339 | val_0_rmse: 0.59068 | val_1_rmse: 0.58667 |  0:02:51s
epoch 31 | loss: 0.34564 | val_0_rmse: 0.62312 | val_1_rmse: 0.6147  |  0:02:57s
epoch 32 | loss: 0.3459  | val_0_rmse: 0.64317 | val_1_rmse: 0.63646 |  0:03:02s
epoch 33 | loss: 0.34687 | val_0_rmse: 0.61565 | val_1_rmse: 0.6077  |  0:03:08s
epoch 34 | loss: 0.35172 | val_0_rmse: 0.71311 | val_1_rmse: 0.70487 |  0:03:14s
epoch 35 | loss: 0.34497 | val_0_rmse: 0.61104 | val_1_rmse: 0.60507 |  0:03:19s
epoch 36 | loss: 0.34812 | val_0_rmse: 0.58824 | val_1_rmse: 0.58214 |  0:03:25s
epoch 37 | loss: 0.34661 | val_0_rmse: 0.6617  | val_1_rmse: 0.65667 |  0:03:30s
epoch 38 | loss: 0.34509 | val_0_rmse: 0.62581 | val_1_rmse: 0.62226 |  0:03:36s
epoch 39 | loss: 0.34684 | val_0_rmse: 0.58012 | val_1_rmse: 0.57427 |  0:03:41s
epoch 40 | loss: 0.34632 | val_0_rmse: 0.60995 | val_1_rmse: 0.60378 |  0:03:47s
epoch 41 | loss: 0.34406 | val_0_rmse: 0.6434  | val_1_rmse: 0.63913 |  0:03:52s
epoch 42 | loss: 0.34494 | val_0_rmse: 0.57744 | val_1_rmse: 0.57117 |  0:03:58s
epoch 43 | loss: 0.34258 | val_0_rmse: 0.74954 | val_1_rmse: 0.7438  |  0:04:03s
epoch 44 | loss: 0.34879 | val_0_rmse: 0.6039  | val_1_rmse: 0.59744 |  0:04:09s
epoch 45 | loss: 0.34596 | val_0_rmse: 0.70854 | val_1_rmse: 0.70223 |  0:04:14s
epoch 46 | loss: 0.34281 | val_0_rmse: 0.61937 | val_1_rmse: 0.61003 |  0:04:20s
epoch 47 | loss: 0.34287 | val_0_rmse: 0.68404 | val_1_rmse: 0.67984 |  0:04:25s
epoch 48 | loss: 0.34037 | val_0_rmse: 0.58369 | val_1_rmse: 0.57732 |  0:04:31s
epoch 49 | loss: 0.34236 | val_0_rmse: 0.61996 | val_1_rmse: 0.61182 |  0:04:36s
epoch 50 | loss: 0.343   | val_0_rmse: 0.61781 | val_1_rmse: 0.61068 |  0:04:42s
epoch 51 | loss: 0.34161 | val_0_rmse: 0.65864 | val_1_rmse: 0.65276 |  0:04:48s
epoch 52 | loss: 0.34012 | val_0_rmse: 0.68019 | val_1_rmse: 0.67222 |  0:04:53s
epoch 53 | loss: 0.34253 | val_0_rmse: 0.56979 | val_1_rmse: 0.56293 |  0:04:59s
epoch 54 | loss: 0.33755 | val_0_rmse: 0.62507 | val_1_rmse: 0.61712 |  0:05:04s
epoch 55 | loss: 0.34459 | val_0_rmse: 0.62614 | val_1_rmse: 0.61812 |  0:05:10s
epoch 56 | loss: 0.33907 | val_0_rmse: 0.78838 | val_1_rmse: 0.78693 |  0:05:15s
epoch 57 | loss: 0.33822 | val_0_rmse: 0.598   | val_1_rmse: 0.59085 |  0:05:21s
epoch 58 | loss: 0.33643 | val_0_rmse: 0.59547 | val_1_rmse: 0.58841 |  0:05:26s
epoch 59 | loss: 0.34233 | val_0_rmse: 0.68675 | val_1_rmse: 0.68155 |  0:05:32s
epoch 60 | loss: 0.34228 | val_0_rmse: 0.60425 | val_1_rmse: 0.59741 |  0:05:37s
epoch 61 | loss: 0.33714 | val_0_rmse: 0.6766  | val_1_rmse: 0.67296 |  0:05:43s
epoch 62 | loss: 0.34002 | val_0_rmse: 0.65911 | val_1_rmse: 0.65321 |  0:05:49s
epoch 63 | loss: 0.33822 | val_0_rmse: 0.67707 | val_1_rmse: 0.66869 |  0:05:54s
epoch 64 | loss: 0.33982 | val_0_rmse: 0.62569 | val_1_rmse: 0.61827 |  0:06:00s
epoch 65 | loss: 0.33584 | val_0_rmse: 0.58336 | val_1_rmse: 0.57866 |  0:06:05s
epoch 66 | loss: 0.34541 | val_0_rmse: 0.63228 | val_1_rmse: 0.62772 |  0:06:11s
epoch 67 | loss: 0.34064 | val_0_rmse: 0.62754 | val_1_rmse: 0.62141 |  0:06:16s
epoch 68 | loss: 0.336   | val_0_rmse: 0.59164 | val_1_rmse: 0.58562 |  0:06:22s
epoch 69 | loss: 0.34434 | val_0_rmse: 0.64466 | val_1_rmse: 0.63675 |  0:06:27s
epoch 70 | loss: 0.33736 | val_0_rmse: 0.62637 | val_1_rmse: 0.62198 |  0:06:33s
epoch 71 | loss: 0.33828 | val_0_rmse: 0.61028 | val_1_rmse: 0.60169 |  0:06:38s
epoch 72 | loss: 0.33928 | val_0_rmse: 0.67037 | val_1_rmse: 0.66507 |  0:06:44s
epoch 73 | loss: 0.33639 | val_0_rmse: 0.59448 | val_1_rmse: 0.58787 |  0:06:50s
epoch 74 | loss: 0.33503 | val_0_rmse: 0.60818 | val_1_rmse: 0.6039  |  0:06:55s
epoch 75 | loss: 0.33463 | val_0_rmse: 0.78266 | val_1_rmse: 0.78438 |  0:07:01s
epoch 76 | loss: 0.34034 | val_0_rmse: 0.6927  | val_1_rmse: 0.68848 |  0:07:06s
epoch 77 | loss: 0.33793 | val_0_rmse: 0.60791 | val_1_rmse: 0.60154 |  0:07:12s
epoch 78 | loss: 0.33824 | val_0_rmse: 0.70829 | val_1_rmse: 0.70282 |  0:07:17s
epoch 79 | loss: 0.33499 | val_0_rmse: 0.66488 | val_1_rmse: 0.65481 |  0:07:23s
epoch 80 | loss: 0.33398 | val_0_rmse: 0.62687 | val_1_rmse: 0.6182  |  0:07:28s
epoch 81 | loss: 0.33207 | val_0_rmse: 0.59208 | val_1_rmse: 0.58507 |  0:07:34s
epoch 82 | loss: 0.33958 | val_0_rmse: 0.57405 | val_1_rmse: 0.56909 |  0:07:39s
epoch 83 | loss: 0.33646 | val_0_rmse: 0.69965 | val_1_rmse: 0.69459 |  0:07:45s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.56293
Best weights from best epoch are automatically used!
ended training at: 14:19:07
Feature importance:
[('Area', 0.29652452463043494), ('Baths', 0.08731355320801325), ('Beds', 0.015742994393460493), ('Latitude', 0.12656263672928764), ('Longitude', 0.22205766666757548), ('Month', 0.1951084548625676), ('Year', 0.05669016950866061)]
Mean squared error is of 2209262756.740299
Mean absolute error:34007.55077236036
MAPE:0.3150673981682711
R2 score:0.6757309900175366
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:19:08
epoch 0  | loss: 0.60054 | val_0_rmse: 0.71686 | val_1_rmse: 0.71447 |  0:00:05s
epoch 1  | loss: 0.49923 | val_0_rmse: 0.69392 | val_1_rmse: 0.68959 |  0:00:11s
epoch 2  | loss: 0.4715  | val_0_rmse: 0.67194 | val_1_rmse: 0.67457 |  0:00:16s
epoch 3  | loss: 0.44739 | val_0_rmse: 0.65939 | val_1_rmse: 0.65972 |  0:00:22s
epoch 4  | loss: 0.41294 | val_0_rmse: 0.64348 | val_1_rmse: 0.64624 |  0:00:27s
epoch 5  | loss: 0.4023  | val_0_rmse: 0.64241 | val_1_rmse: 0.64504 |  0:00:33s
epoch 6  | loss: 0.39518 | val_0_rmse: 0.62914 | val_1_rmse: 0.63064 |  0:00:38s
epoch 7  | loss: 0.39008 | val_0_rmse: 0.61529 | val_1_rmse: 0.61794 |  0:00:44s
epoch 8  | loss: 0.38385 | val_0_rmse: 0.65382 | val_1_rmse: 0.6559  |  0:00:50s
epoch 9  | loss: 0.38522 | val_0_rmse: 0.72719 | val_1_rmse: 0.73198 |  0:00:55s
epoch 10 | loss: 0.38118 | val_0_rmse: 0.67067 | val_1_rmse: 0.67321 |  0:01:01s
epoch 11 | loss: 0.37559 | val_0_rmse: 0.63248 | val_1_rmse: 0.6355  |  0:01:06s
epoch 12 | loss: 0.37612 | val_0_rmse: 0.61295 | val_1_rmse: 0.61536 |  0:01:12s
epoch 13 | loss: 0.3762  | val_0_rmse: 0.60428 | val_1_rmse: 0.60643 |  0:01:17s
epoch 14 | loss: 0.38192 | val_0_rmse: 0.66545 | val_1_rmse: 0.66648 |  0:01:23s
epoch 15 | loss: 0.37559 | val_0_rmse: 0.61284 | val_1_rmse: 0.61547 |  0:01:28s
epoch 16 | loss: 0.36732 | val_0_rmse: 0.61531 | val_1_rmse: 0.62016 |  0:01:34s
epoch 17 | loss: 0.36887 | val_0_rmse: 0.62408 | val_1_rmse: 0.62873 |  0:01:39s
epoch 18 | loss: 0.36946 | val_0_rmse: 0.68479 | val_1_rmse: 0.68617 |  0:01:45s
epoch 19 | loss: 0.36595 | val_0_rmse: 0.66274 | val_1_rmse: 0.66879 |  0:01:51s
epoch 20 | loss: 0.36582 | val_0_rmse: 0.61524 | val_1_rmse: 0.62142 |  0:01:56s
epoch 21 | loss: 0.36364 | val_0_rmse: 0.6516  | val_1_rmse: 0.65229 |  0:02:02s
epoch 22 | loss: 0.35997 | val_0_rmse: 0.62783 | val_1_rmse: 0.62519 |  0:02:07s
epoch 23 | loss: 0.36546 | val_0_rmse: 0.66225 | val_1_rmse: 0.66935 |  0:02:13s
epoch 24 | loss: 0.3628  | val_0_rmse: 0.60266 | val_1_rmse: 0.60886 |  0:02:18s
epoch 25 | loss: 0.35956 | val_0_rmse: 0.60932 | val_1_rmse: 0.61465 |  0:02:24s
epoch 26 | loss: 0.3609  | val_0_rmse: 0.61608 | val_1_rmse: 0.61987 |  0:02:29s
epoch 27 | loss: 0.36249 | val_0_rmse: 0.69396 | val_1_rmse: 0.69516 |  0:02:35s
epoch 28 | loss: 0.36171 | val_0_rmse: 0.58715 | val_1_rmse: 0.59033 |  0:02:40s
epoch 29 | loss: 0.35863 | val_0_rmse: 0.58997 | val_1_rmse: 0.59109 |  0:02:46s
epoch 30 | loss: 0.36072 | val_0_rmse: 0.63985 | val_1_rmse: 0.63911 |  0:02:51s
epoch 31 | loss: 0.35801 | val_0_rmse: 0.69871 | val_1_rmse: 0.69873 |  0:02:57s
epoch 32 | loss: 0.35492 | val_0_rmse: 0.58163 | val_1_rmse: 0.58592 |  0:03:03s
epoch 33 | loss: 0.35444 | val_0_rmse: 0.60671 | val_1_rmse: 0.6079  |  0:03:08s
epoch 34 | loss: 0.35345 | val_0_rmse: 0.60235 | val_1_rmse: 0.6039  |  0:03:14s
epoch 35 | loss: 0.35411 | val_0_rmse: 0.63944 | val_1_rmse: 0.64532 |  0:03:19s
epoch 36 | loss: 0.35835 | val_0_rmse: 0.61071 | val_1_rmse: 0.61456 |  0:03:25s
epoch 37 | loss: 0.35085 | val_0_rmse: 0.60621 | val_1_rmse: 0.60721 |  0:03:30s
epoch 38 | loss: 0.3509  | val_0_rmse: 0.60072 | val_1_rmse: 0.6018  |  0:03:36s
epoch 39 | loss: 0.35284 | val_0_rmse: 0.65943 | val_1_rmse: 0.6579  |  0:03:42s
epoch 40 | loss: 0.35232 | val_0_rmse: 0.61258 | val_1_rmse: 0.61344 |  0:03:47s
epoch 41 | loss: 0.35218 | val_0_rmse: 0.71437 | val_1_rmse: 0.71281 |  0:03:53s
epoch 42 | loss: 0.35255 | val_0_rmse: 0.63251 | val_1_rmse: 0.63337 |  0:03:59s
epoch 43 | loss: 0.34902 | val_0_rmse: 0.58521 | val_1_rmse: 0.58792 |  0:04:04s
epoch 44 | loss: 0.35271 | val_0_rmse: 0.63491 | val_1_rmse: 0.63641 |  0:04:10s
epoch 45 | loss: 0.35275 | val_0_rmse: 0.58862 | val_1_rmse: 0.59172 |  0:04:15s
epoch 46 | loss: 0.34795 | val_0_rmse: 0.65204 | val_1_rmse: 0.65197 |  0:04:21s
epoch 47 | loss: 0.35239 | val_0_rmse: 0.62869 | val_1_rmse: 0.62991 |  0:04:26s
epoch 48 | loss: 0.36814 | val_0_rmse: 0.62482 | val_1_rmse: 0.62733 |  0:04:32s
epoch 49 | loss: 0.36231 | val_0_rmse: 0.64657 | val_1_rmse: 0.65233 |  0:04:37s
epoch 50 | loss: 0.36165 | val_0_rmse: 0.60974 | val_1_rmse: 0.61033 |  0:04:43s
epoch 51 | loss: 0.35204 | val_0_rmse: 0.64577 | val_1_rmse: 0.65177 |  0:04:48s
epoch 52 | loss: 0.35223 | val_0_rmse: 0.65351 | val_1_rmse: 0.65989 |  0:04:54s
epoch 53 | loss: 0.34997 | val_0_rmse: 0.60452 | val_1_rmse: 0.60493 |  0:04:59s
epoch 54 | loss: 0.34828 | val_0_rmse: 0.71401 | val_1_rmse: 0.71987 |  0:05:05s
epoch 55 | loss: 0.34746 | val_0_rmse: 0.62165 | val_1_rmse: 0.62915 |  0:05:10s
epoch 56 | loss: 0.34505 | val_0_rmse: 0.58271 | val_1_rmse: 0.5831  |  0:05:16s
epoch 57 | loss: 0.34764 | val_0_rmse: 0.59166 | val_1_rmse: 0.59361 |  0:05:22s
epoch 58 | loss: 0.345   | val_0_rmse: 0.61604 | val_1_rmse: 0.61701 |  0:05:27s
epoch 59 | loss: 0.34908 | val_0_rmse: 0.66698 | val_1_rmse: 0.66727 |  0:05:33s
epoch 60 | loss: 0.34336 | val_0_rmse: 0.58586 | val_1_rmse: 0.59208 |  0:05:38s
epoch 61 | loss: 0.34411 | val_0_rmse: 0.67939 | val_1_rmse: 0.67824 |  0:05:44s
epoch 62 | loss: 0.34674 | val_0_rmse: 0.70535 | val_1_rmse: 0.70536 |  0:05:49s
epoch 63 | loss: 0.35235 | val_0_rmse: 0.69906 | val_1_rmse: 0.69735 |  0:05:55s
epoch 64 | loss: 0.34931 | val_0_rmse: 0.62072 | val_1_rmse: 0.62253 |  0:06:00s
epoch 65 | loss: 0.34301 | val_0_rmse: 0.58826 | val_1_rmse: 0.5919  |  0:06:06s
epoch 66 | loss: 0.35401 | val_0_rmse: 0.63805 | val_1_rmse: 0.64128 |  0:06:12s
epoch 67 | loss: 0.34942 | val_0_rmse: 0.60088 | val_1_rmse: 0.60569 |  0:06:17s
epoch 68 | loss: 0.34569 | val_0_rmse: 0.58666 | val_1_rmse: 0.59159 |  0:06:23s
epoch 69 | loss: 0.34693 | val_0_rmse: 0.67973 | val_1_rmse: 0.68407 |  0:06:28s
epoch 70 | loss: 0.34611 | val_0_rmse: 0.58084 | val_1_rmse: 0.58471 |  0:06:34s
epoch 71 | loss: 0.34204 | val_0_rmse: 0.66286 | val_1_rmse: 0.66223 |  0:06:39s
epoch 72 | loss: 0.34238 | val_0_rmse: 0.58575 | val_1_rmse: 0.58996 |  0:06:45s
epoch 73 | loss: 0.34079 | val_0_rmse: 0.6135  | val_1_rmse: 0.62177 |  0:06:50s
epoch 74 | loss: 0.33972 | val_0_rmse: 0.57112 | val_1_rmse: 0.57552 |  0:06:56s
epoch 75 | loss: 0.34042 | val_0_rmse: 0.62828 | val_1_rmse: 0.62781 |  0:07:01s
epoch 76 | loss: 0.34361 | val_0_rmse: 0.60849 | val_1_rmse: 0.60725 |  0:07:07s
epoch 77 | loss: 0.34296 | val_0_rmse: 0.58449 | val_1_rmse: 0.58878 |  0:07:12s
epoch 78 | loss: 0.34181 | val_0_rmse: 0.67208 | val_1_rmse: 0.6797  |  0:07:18s
epoch 79 | loss: 0.34855 | val_0_rmse: 0.62856 | val_1_rmse: 0.63446 |  0:07:23s
epoch 80 | loss: 0.35019 | val_0_rmse: 0.63138 | val_1_rmse: 0.62978 |  0:07:29s
epoch 81 | loss: 0.34476 | val_0_rmse: 0.67566 | val_1_rmse: 0.67581 |  0:07:34s
epoch 82 | loss: 0.35642 | val_0_rmse: 0.63012 | val_1_rmse: 0.62676 |  0:07:40s
epoch 83 | loss: 0.35387 | val_0_rmse: 0.63862 | val_1_rmse: 0.64427 |  0:07:46s
epoch 84 | loss: 0.3457  | val_0_rmse: 0.60037 | val_1_rmse: 0.60125 |  0:07:51s
epoch 85 | loss: 0.34271 | val_0_rmse: 0.59672 | val_1_rmse: 0.59598 |  0:07:57s
epoch 86 | loss: 0.34345 | val_0_rmse: 0.63765 | val_1_rmse: 0.64154 |  0:08:02s
epoch 87 | loss: 0.33872 | val_0_rmse: 0.62658 | val_1_rmse: 0.63304 |  0:08:08s
epoch 88 | loss: 0.33784 | val_0_rmse: 0.6259  | val_1_rmse: 0.62466 |  0:08:13s
epoch 89 | loss: 0.337   | val_0_rmse: 0.62542 | val_1_rmse: 0.62554 |  0:08:19s
epoch 90 | loss: 0.33605 | val_0_rmse: 0.57726 | val_1_rmse: 0.58078 |  0:08:24s
epoch 91 | loss: 0.33536 | val_0_rmse: 0.60787 | val_1_rmse: 0.61462 |  0:08:30s
epoch 92 | loss: 0.33679 | val_0_rmse: 0.58384 | val_1_rmse: 0.58678 |  0:08:35s
epoch 93 | loss: 0.3381  | val_0_rmse: 0.56904 | val_1_rmse: 0.57254 |  0:08:41s
epoch 94 | loss: 0.33552 | val_0_rmse: 0.63245 | val_1_rmse: 0.63994 |  0:08:46s
epoch 95 | loss: 0.33302 | val_0_rmse: 0.59804 | val_1_rmse: 0.6046  |  0:08:52s
epoch 96 | loss: 0.3338  | val_0_rmse: 0.75266 | val_1_rmse: 0.76019 |  0:08:58s
epoch 97 | loss: 0.33375 | val_0_rmse: 0.67379 | val_1_rmse: 0.68028 |  0:09:03s
epoch 98 | loss: 0.33478 | val_0_rmse: 0.61347 | val_1_rmse: 0.6206  |  0:09:09s
epoch 99 | loss: 0.33489 | val_0_rmse: 0.60426 | val_1_rmse: 0.60699 |  0:09:14s
epoch 100| loss: 0.33801 | val_0_rmse: 0.61807 | val_1_rmse: 0.61792 |  0:09:20s
epoch 101| loss: 0.34429 | val_0_rmse: 0.5903  | val_1_rmse: 0.59623 |  0:09:25s
epoch 102| loss: 0.34307 | val_0_rmse: 0.59121 | val_1_rmse: 0.59521 |  0:09:31s
epoch 103| loss: 0.33935 | val_0_rmse: 0.63186 | val_1_rmse: 0.63377 |  0:09:37s
epoch 104| loss: 0.33712 | val_0_rmse: 0.64187 | val_1_rmse: 0.65044 |  0:09:42s
epoch 105| loss: 0.3383  | val_0_rmse: 0.60553 | val_1_rmse: 0.61219 |  0:09:48s
epoch 106| loss: 0.33964 | val_0_rmse: 0.59804 | val_1_rmse: 0.6032  |  0:09:53s
epoch 107| loss: 0.33726 | val_0_rmse: 0.60349 | val_1_rmse: 0.60451 |  0:09:59s
epoch 108| loss: 0.3357  | val_0_rmse: 0.60602 | val_1_rmse: 0.6078  |  0:10:04s
epoch 109| loss: 0.33588 | val_0_rmse: 0.59304 | val_1_rmse: 0.59493 |  0:10:10s
epoch 110| loss: 0.33569 | val_0_rmse: 0.60196 | val_1_rmse: 0.60298 |  0:10:15s
epoch 111| loss: 0.33737 | val_0_rmse: 0.60233 | val_1_rmse: 0.60314 |  0:10:21s
epoch 112| loss: 0.33586 | val_0_rmse: 0.63473 | val_1_rmse: 0.63917 |  0:10:26s
epoch 113| loss: 0.33585 | val_0_rmse: 0.69539 | val_1_rmse: 0.7047  |  0:10:32s
epoch 114| loss: 0.33283 | val_0_rmse: 0.62279 | val_1_rmse: 0.62909 |  0:10:37s
epoch 115| loss: 0.3343  | val_0_rmse: 0.60446 | val_1_rmse: 0.60643 |  0:10:43s
epoch 116| loss: 0.33431 | val_0_rmse: 0.62768 | val_1_rmse: 0.63711 |  0:10:48s
epoch 117| loss: 0.33416 | val_0_rmse: 0.59162 | val_1_rmse: 0.59647 |  0:10:54s
epoch 118| loss: 0.33659 | val_0_rmse: 0.64549 | val_1_rmse: 0.64548 |  0:10:59s
epoch 119| loss: 0.33286 | val_0_rmse: 0.5882  | val_1_rmse: 0.59118 |  0:11:05s
epoch 120| loss: 0.33469 | val_0_rmse: 0.57672 | val_1_rmse: 0.58268 |  0:11:11s
epoch 121| loss: 0.33205 | val_0_rmse: 0.64956 | val_1_rmse: 0.64961 |  0:11:16s
epoch 122| loss: 0.33343 | val_0_rmse: 0.59163 | val_1_rmse: 0.59857 |  0:11:22s
epoch 123| loss: 0.33245 | val_0_rmse: 0.57698 | val_1_rmse: 0.58142 |  0:11:27s

Early stopping occured at epoch 123 with best_epoch = 93 and best_val_1_rmse = 0.57254
Best weights from best epoch are automatically used!
ended training at: 14:30:37
Feature importance:
[('Area', 0.2048937269141754), ('Baths', 0.22146171067453282), ('Beds', 0.0), ('Latitude', 0.3740356165738987), ('Longitude', 0.19960894583739303), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2185073092.291946
Mean absolute error:33726.39465148944
MAPE:0.32268177835800205
R2 score:0.6778290784604812
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:30:38
epoch 0  | loss: 0.54483 | val_0_rmse: 0.6007  | val_1_rmse: 0.60372 |  0:00:01s
epoch 1  | loss: 0.35036 | val_0_rmse: 0.57266 | val_1_rmse: 0.56749 |  0:00:03s
epoch 2  | loss: 0.32767 | val_0_rmse: 0.5816  | val_1_rmse: 0.56979 |  0:00:06s
epoch 3  | loss: 0.32967 | val_0_rmse: 0.56504 | val_1_rmse: 0.56101 |  0:00:08s
epoch 4  | loss: 0.32141 | val_0_rmse: 0.56173 | val_1_rmse: 0.55683 |  0:00:10s
epoch 5  | loss: 0.30961 | val_0_rmse: 0.54299 | val_1_rmse: 0.54022 |  0:00:12s
epoch 6  | loss: 0.30718 | val_0_rmse: 0.5319  | val_1_rmse: 0.52728 |  0:00:14s
epoch 7  | loss: 0.2835  | val_0_rmse: 0.55647 | val_1_rmse: 0.55629 |  0:00:16s
epoch 8  | loss: 0.28109 | val_0_rmse: 0.54409 | val_1_rmse: 0.54517 |  0:00:18s
epoch 9  | loss: 0.27669 | val_0_rmse: 0.52524 | val_1_rmse: 0.52222 |  0:00:20s
epoch 10 | loss: 0.27768 | val_0_rmse: 0.52218 | val_1_rmse: 0.51488 |  0:00:22s
epoch 11 | loss: 0.2694  | val_0_rmse: 0.53023 | val_1_rmse: 0.53355 |  0:00:24s
epoch 12 | loss: 0.26015 | val_0_rmse: 0.5195  | val_1_rmse: 0.51922 |  0:00:26s
epoch 13 | loss: 0.26413 | val_0_rmse: 0.55254 | val_1_rmse: 0.54628 |  0:00:28s
epoch 14 | loss: 0.26802 | val_0_rmse: 0.51542 | val_1_rmse: 0.51283 |  0:00:30s
epoch 15 | loss: 0.26369 | val_0_rmse: 0.54051 | val_1_rmse: 0.53559 |  0:00:32s
epoch 16 | loss: 0.26278 | val_0_rmse: 0.54509 | val_1_rmse: 0.54815 |  0:00:34s
epoch 17 | loss: 0.25773 | val_0_rmse: 0.50441 | val_1_rmse: 0.50246 |  0:00:36s
epoch 18 | loss: 0.25948 | val_0_rmse: 0.50998 | val_1_rmse: 0.50934 |  0:00:38s
epoch 19 | loss: 0.25916 | val_0_rmse: 0.52968 | val_1_rmse: 0.53358 |  0:00:40s
epoch 20 | loss: 0.25294 | val_0_rmse: 0.50261 | val_1_rmse: 0.50469 |  0:00:42s
epoch 21 | loss: 0.2578  | val_0_rmse: 0.51789 | val_1_rmse: 0.51793 |  0:00:44s
epoch 22 | loss: 0.2651  | val_0_rmse: 0.53236 | val_1_rmse: 0.53013 |  0:00:46s
epoch 23 | loss: 0.26717 | val_0_rmse: 0.60996 | val_1_rmse: 0.61274 |  0:00:48s
epoch 24 | loss: 0.25741 | val_0_rmse: 0.50667 | val_1_rmse: 0.5057  |  0:00:50s
epoch 25 | loss: 0.25437 | val_0_rmse: 0.54432 | val_1_rmse: 0.54318 |  0:00:52s
epoch 26 | loss: 0.25045 | val_0_rmse: 0.51339 | val_1_rmse: 0.51252 |  0:00:54s
epoch 27 | loss: 0.24935 | val_0_rmse: 0.49031 | val_1_rmse: 0.48914 |  0:00:56s
epoch 28 | loss: 0.25005 | val_0_rmse: 0.49349 | val_1_rmse: 0.49563 |  0:00:58s
epoch 29 | loss: 0.25061 | val_0_rmse: 0.52503 | val_1_rmse: 0.52406 |  0:01:00s
epoch 30 | loss: 0.25135 | val_0_rmse: 0.51588 | val_1_rmse: 0.51803 |  0:01:02s
epoch 31 | loss: 0.2527  | val_0_rmse: 0.50446 | val_1_rmse: 0.50027 |  0:01:04s
epoch 32 | loss: 0.24839 | val_0_rmse: 0.48377 | val_1_rmse: 0.48344 |  0:01:06s
epoch 33 | loss: 0.24726 | val_0_rmse: 0.51993 | val_1_rmse: 0.52022 |  0:01:08s
epoch 34 | loss: 0.24938 | val_0_rmse: 0.49614 | val_1_rmse: 0.49678 |  0:01:10s
epoch 35 | loss: 0.24786 | val_0_rmse: 0.52497 | val_1_rmse: 0.52764 |  0:01:12s
epoch 36 | loss: 0.24637 | val_0_rmse: 0.49142 | val_1_rmse: 0.49357 |  0:01:14s
epoch 37 | loss: 0.24865 | val_0_rmse: 0.51384 | val_1_rmse: 0.51631 |  0:01:16s
epoch 38 | loss: 0.25281 | val_0_rmse: 0.51365 | val_1_rmse: 0.51109 |  0:01:18s
epoch 39 | loss: 0.25016 | val_0_rmse: 0.51534 | val_1_rmse: 0.51376 |  0:01:20s
epoch 40 | loss: 0.25128 | val_0_rmse: 0.50037 | val_1_rmse: 0.50104 |  0:01:22s
epoch 41 | loss: 0.24615 | val_0_rmse: 0.49247 | val_1_rmse: 0.49056 |  0:01:24s
epoch 42 | loss: 0.24271 | val_0_rmse: 0.51235 | val_1_rmse: 0.51501 |  0:01:26s
epoch 43 | loss: 0.24687 | val_0_rmse: 0.50716 | val_1_rmse: 0.50686 |  0:01:28s
epoch 44 | loss: 0.24332 | val_0_rmse: 0.53595 | val_1_rmse: 0.53747 |  0:01:30s
epoch 45 | loss: 0.24654 | val_0_rmse: 0.50225 | val_1_rmse: 0.50072 |  0:01:32s
epoch 46 | loss: 0.24316 | val_0_rmse: 0.55272 | val_1_rmse: 0.55675 |  0:01:34s
epoch 47 | loss: 0.24165 | val_0_rmse: 0.50303 | val_1_rmse: 0.50574 |  0:01:36s
epoch 48 | loss: 0.24288 | val_0_rmse: 0.50925 | val_1_rmse: 0.50832 |  0:01:38s
epoch 49 | loss: 0.24496 | val_0_rmse: 0.48258 | val_1_rmse: 0.48581 |  0:01:40s
epoch 50 | loss: 0.24625 | val_0_rmse: 0.482   | val_1_rmse: 0.48078 |  0:01:42s
epoch 51 | loss: 0.24368 | val_0_rmse: 0.50664 | val_1_rmse: 0.50266 |  0:01:44s
epoch 52 | loss: 0.2428  | val_0_rmse: 0.51432 | val_1_rmse: 0.51592 |  0:01:46s
epoch 53 | loss: 0.2409  | val_0_rmse: 0.52568 | val_1_rmse: 0.52592 |  0:01:48s
epoch 54 | loss: 0.24225 | val_0_rmse: 0.48949 | val_1_rmse: 0.48714 |  0:01:50s
epoch 55 | loss: 0.24749 | val_0_rmse: 0.56602 | val_1_rmse: 0.56887 |  0:01:52s
epoch 56 | loss: 0.2445  | val_0_rmse: 0.4849  | val_1_rmse: 0.48575 |  0:01:54s
epoch 57 | loss: 0.24856 | val_0_rmse: 0.52958 | val_1_rmse: 0.53298 |  0:01:56s
epoch 58 | loss: 0.24205 | val_0_rmse: 0.55121 | val_1_rmse: 0.54523 |  0:01:58s
epoch 59 | loss: 0.24605 | val_0_rmse: 0.49123 | val_1_rmse: 0.49093 |  0:02:00s
epoch 60 | loss: 0.24163 | val_0_rmse: 0.50171 | val_1_rmse: 0.5003  |  0:02:02s
epoch 61 | loss: 0.24431 | val_0_rmse: 0.5166  | val_1_rmse: 0.51602 |  0:02:04s
epoch 62 | loss: 0.24535 | val_0_rmse: 0.48974 | val_1_rmse: 0.48699 |  0:02:06s
epoch 63 | loss: 0.24316 | val_0_rmse: 0.54046 | val_1_rmse: 0.54328 |  0:02:08s
epoch 64 | loss: 0.2453  | val_0_rmse: 0.5088  | val_1_rmse: 0.50787 |  0:02:10s
epoch 65 | loss: 0.2422  | val_0_rmse: 0.48205 | val_1_rmse: 0.48244 |  0:02:12s
epoch 66 | loss: 0.24374 | val_0_rmse: 0.51956 | val_1_rmse: 0.5207  |  0:02:14s
epoch 67 | loss: 0.24128 | val_0_rmse: 0.51342 | val_1_rmse: 0.5174  |  0:02:16s
epoch 68 | loss: 0.24078 | val_0_rmse: 0.48953 | val_1_rmse: 0.49126 |  0:02:18s
epoch 69 | loss: 0.23806 | val_0_rmse: 0.49104 | val_1_rmse: 0.49066 |  0:02:20s
epoch 70 | loss: 0.23993 | val_0_rmse: 0.50688 | val_1_rmse: 0.50596 |  0:02:22s
epoch 71 | loss: 0.24605 | val_0_rmse: 0.53448 | val_1_rmse: 0.53547 |  0:02:24s
epoch 72 | loss: 0.24465 | val_0_rmse: 0.54749 | val_1_rmse: 0.55088 |  0:02:26s
epoch 73 | loss: 0.24502 | val_0_rmse: 0.49257 | val_1_rmse: 0.49479 |  0:02:28s
epoch 74 | loss: 0.23642 | val_0_rmse: 0.48902 | val_1_rmse: 0.49367 |  0:02:30s
epoch 75 | loss: 0.23943 | val_0_rmse: 0.5175  | val_1_rmse: 0.52322 |  0:02:32s
epoch 76 | loss: 0.23899 | val_0_rmse: 0.48408 | val_1_rmse: 0.48689 |  0:02:34s
epoch 77 | loss: 0.23809 | val_0_rmse: 0.52725 | val_1_rmse: 0.53007 |  0:02:36s
epoch 78 | loss: 0.24575 | val_0_rmse: 0.48788 | val_1_rmse: 0.49006 |  0:02:38s
epoch 79 | loss: 0.25244 | val_0_rmse: 0.53988 | val_1_rmse: 0.53656 |  0:02:40s
epoch 80 | loss: 0.24519 | val_0_rmse: 0.51603 | val_1_rmse: 0.51608 |  0:02:42s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.48078
Best weights from best epoch are automatically used!
ended training at: 14:33:22
Feature importance:
[('Area', 0.3620071791556163), ('Baths', 0.08976966754176921), ('Beds', 0.03086597778449981), ('Latitude', 0.15740179906397087), ('Longitude', 0.18634530859521098), ('Month', 0.10120756913008407), ('Year', 0.07240249872884874)]
Mean squared error is of 1009914611.2140639
Mean absolute error:21399.371879235372
MAPE:0.2554992989091872
R2 score:0.753309150675476
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:33:22
epoch 0  | loss: 0.5167  | val_0_rmse: 0.64662 | val_1_rmse: 0.6652  |  0:00:01s
epoch 1  | loss: 0.36689 | val_0_rmse: 0.58575 | val_1_rmse: 0.60205 |  0:00:03s
epoch 2  | loss: 0.35371 | val_0_rmse: 0.57153 | val_1_rmse: 0.59173 |  0:00:05s
epoch 3  | loss: 0.32719 | val_0_rmse: 0.57503 | val_1_rmse: 0.58617 |  0:00:08s
epoch 4  | loss: 0.31767 | val_0_rmse: 0.55949 | val_1_rmse: 0.5745  |  0:00:10s
epoch 5  | loss: 0.31893 | val_0_rmse: 0.55262 | val_1_rmse: 0.56494 |  0:00:12s
epoch 6  | loss: 0.30107 | val_0_rmse: 0.53566 | val_1_rmse: 0.54698 |  0:00:13s
epoch 7  | loss: 0.29941 | val_0_rmse: 0.53209 | val_1_rmse: 0.53839 |  0:00:16s
epoch 8  | loss: 0.28735 | val_0_rmse: 0.63382 | val_1_rmse: 0.63368 |  0:00:18s
epoch 9  | loss: 0.28141 | val_0_rmse: 0.51272 | val_1_rmse: 0.51437 |  0:00:20s
epoch 10 | loss: 0.27755 | val_0_rmse: 0.50938 | val_1_rmse: 0.51289 |  0:00:22s
epoch 11 | loss: 0.28236 | val_0_rmse: 0.57313 | val_1_rmse: 0.573   |  0:00:24s
epoch 12 | loss: 0.28397 | val_0_rmse: 0.53004 | val_1_rmse: 0.53435 |  0:00:26s
epoch 13 | loss: 0.27396 | val_0_rmse: 0.52799 | val_1_rmse: 0.52788 |  0:00:28s
epoch 14 | loss: 0.27186 | val_0_rmse: 0.65674 | val_1_rmse: 0.6682  |  0:00:30s
epoch 15 | loss: 0.27922 | val_0_rmse: 0.55213 | val_1_rmse: 0.56211 |  0:00:32s
epoch 16 | loss: 0.27196 | val_0_rmse: 0.52738 | val_1_rmse: 0.52736 |  0:00:34s
epoch 17 | loss: 0.26313 | val_0_rmse: 0.49981 | val_1_rmse: 0.50437 |  0:00:36s
epoch 18 | loss: 0.27429 | val_0_rmse: 0.52193 | val_1_rmse: 0.52372 |  0:00:38s
epoch 19 | loss: 0.26565 | val_0_rmse: 0.52304 | val_1_rmse: 0.52881 |  0:00:40s
epoch 20 | loss: 0.26407 | val_0_rmse: 0.54549 | val_1_rmse: 0.54348 |  0:00:42s
epoch 21 | loss: 0.26265 | val_0_rmse: 0.49545 | val_1_rmse: 0.49873 |  0:00:44s
epoch 22 | loss: 0.26372 | val_0_rmse: 0.55707 | val_1_rmse: 0.56288 |  0:00:46s
epoch 23 | loss: 0.26761 | val_0_rmse: 0.52403 | val_1_rmse: 0.52479 |  0:00:48s
epoch 24 | loss: 0.26253 | val_0_rmse: 0.4997  | val_1_rmse: 0.50261 |  0:00:50s
epoch 25 | loss: 0.26092 | val_0_rmse: 0.50052 | val_1_rmse: 0.50738 |  0:00:52s
epoch 26 | loss: 0.2594  | val_0_rmse: 0.52088 | val_1_rmse: 0.52731 |  0:00:54s
epoch 27 | loss: 0.25565 | val_0_rmse: 0.51529 | val_1_rmse: 0.51875 |  0:00:56s
epoch 28 | loss: 0.25594 | val_0_rmse: 0.50763 | val_1_rmse: 0.5114  |  0:00:58s
epoch 29 | loss: 0.25861 | val_0_rmse: 0.50187 | val_1_rmse: 0.50532 |  0:01:00s
epoch 30 | loss: 0.25995 | val_0_rmse: 0.61463 | val_1_rmse: 0.61297 |  0:01:02s
epoch 31 | loss: 0.25763 | val_0_rmse: 0.55636 | val_1_rmse: 0.56115 |  0:01:04s
epoch 32 | loss: 0.26489 | val_0_rmse: 0.50432 | val_1_rmse: 0.50654 |  0:01:06s
epoch 33 | loss: 0.2659  | val_0_rmse: 0.58495 | val_1_rmse: 0.58291 |  0:01:08s
epoch 34 | loss: 0.25899 | val_0_rmse: 0.50965 | val_1_rmse: 0.51941 |  0:01:10s
epoch 35 | loss: 0.25331 | val_0_rmse: 0.50539 | val_1_rmse: 0.50715 |  0:01:12s
epoch 36 | loss: 0.2536  | val_0_rmse: 0.49392 | val_1_rmse: 0.50059 |  0:01:14s
epoch 37 | loss: 0.25215 | val_0_rmse: 0.49313 | val_1_rmse: 0.50318 |  0:01:16s
epoch 38 | loss: 0.25255 | val_0_rmse: 0.5028  | val_1_rmse: 0.50955 |  0:01:18s
epoch 39 | loss: 0.25557 | val_0_rmse: 0.5091  | val_1_rmse: 0.51169 |  0:01:20s
epoch 40 | loss: 0.2547  | val_0_rmse: 0.50896 | val_1_rmse: 0.51857 |  0:01:22s
epoch 41 | loss: 0.25327 | val_0_rmse: 0.54641 | val_1_rmse: 0.55295 |  0:01:24s
epoch 42 | loss: 0.2588  | val_0_rmse: 0.49712 | val_1_rmse: 0.50524 |  0:01:26s
epoch 43 | loss: 0.253   | val_0_rmse: 0.49294 | val_1_rmse: 0.49801 |  0:01:28s
epoch 44 | loss: 0.25357 | val_0_rmse: 0.60238 | val_1_rmse: 0.60216 |  0:01:30s
epoch 45 | loss: 0.2516  | val_0_rmse: 0.55975 | val_1_rmse: 0.56921 |  0:01:32s
epoch 46 | loss: 0.25291 | val_0_rmse: 0.54282 | val_1_rmse: 0.54885 |  0:01:34s
epoch 47 | loss: 0.2535  | val_0_rmse: 0.51126 | val_1_rmse: 0.51725 |  0:01:36s
epoch 48 | loss: 0.25542 | val_0_rmse: 0.49314 | val_1_rmse: 0.49737 |  0:01:38s
epoch 49 | loss: 0.25104 | val_0_rmse: 0.50265 | val_1_rmse: 0.51063 |  0:01:40s
epoch 50 | loss: 0.25697 | val_0_rmse: 0.54635 | val_1_rmse: 0.55789 |  0:01:42s
epoch 51 | loss: 0.25656 | val_0_rmse: 0.56106 | val_1_rmse: 0.56786 |  0:01:44s
epoch 52 | loss: 0.25844 | val_0_rmse: 0.57429 | val_1_rmse: 0.57649 |  0:01:46s
epoch 53 | loss: 0.25424 | val_0_rmse: 0.50435 | val_1_rmse: 0.51106 |  0:01:48s
epoch 54 | loss: 0.25232 | val_0_rmse: 0.55844 | val_1_rmse: 0.5666  |  0:01:50s
epoch 55 | loss: 0.25272 | val_0_rmse: 0.48749 | val_1_rmse: 0.49759 |  0:01:52s
epoch 56 | loss: 0.24595 | val_0_rmse: 0.50214 | val_1_rmse: 0.51173 |  0:01:54s
epoch 57 | loss: 0.24719 | val_0_rmse: 0.55025 | val_1_rmse: 0.55371 |  0:01:56s
epoch 58 | loss: 0.24782 | val_0_rmse: 0.48767 | val_1_rmse: 0.49608 |  0:01:58s
epoch 59 | loss: 0.2513  | val_0_rmse: 0.52697 | val_1_rmse: 0.53511 |  0:02:00s
epoch 60 | loss: 0.24982 | val_0_rmse: 0.4907  | val_1_rmse: 0.49451 |  0:02:02s
epoch 61 | loss: 0.25619 | val_0_rmse: 0.52523 | val_1_rmse: 0.53263 |  0:02:04s
epoch 62 | loss: 0.24804 | val_0_rmse: 0.51643 | val_1_rmse: 0.52387 |  0:02:06s
epoch 63 | loss: 0.24635 | val_0_rmse: 0.49522 | val_1_rmse: 0.50111 |  0:02:08s
epoch 64 | loss: 0.24808 | val_0_rmse: 0.52199 | val_1_rmse: 0.52384 |  0:02:10s
epoch 65 | loss: 0.24906 | val_0_rmse: 0.59214 | val_1_rmse: 0.59963 |  0:02:12s
epoch 66 | loss: 0.24654 | val_0_rmse: 0.54245 | val_1_rmse: 0.54919 |  0:02:14s
epoch 67 | loss: 0.24542 | val_0_rmse: 0.51325 | val_1_rmse: 0.51215 |  0:02:16s
epoch 68 | loss: 0.24573 | val_0_rmse: 0.48963 | val_1_rmse: 0.49569 |  0:02:18s
epoch 69 | loss: 0.24314 | val_0_rmse: 0.54034 | val_1_rmse: 0.54545 |  0:02:20s
epoch 70 | loss: 0.25066 | val_0_rmse: 0.53649 | val_1_rmse: 0.54161 |  0:02:22s
epoch 71 | loss: 0.24779 | val_0_rmse: 0.54817 | val_1_rmse: 0.55385 |  0:02:24s
epoch 72 | loss: 0.24677 | val_0_rmse: 0.51946 | val_1_rmse: 0.52966 |  0:02:26s
epoch 73 | loss: 0.2456  | val_0_rmse: 0.53432 | val_1_rmse: 0.53849 |  0:02:28s
epoch 74 | loss: 0.24261 | val_0_rmse: 0.53245 | val_1_rmse: 0.53997 |  0:02:30s
epoch 75 | loss: 0.24242 | val_0_rmse: 0.48786 | val_1_rmse: 0.49565 |  0:02:32s
epoch 76 | loss: 0.24634 | val_0_rmse: 0.59244 | val_1_rmse: 0.60224 |  0:02:34s
epoch 77 | loss: 0.25292 | val_0_rmse: 0.56927 | val_1_rmse: 0.57229 |  0:02:36s
epoch 78 | loss: 0.24558 | val_0_rmse: 0.60357 | val_1_rmse: 0.61282 |  0:02:38s
epoch 79 | loss: 0.24493 | val_0_rmse: 0.60125 | val_1_rmse: 0.60505 |  0:02:40s
epoch 80 | loss: 0.24465 | val_0_rmse: 0.549   | val_1_rmse: 0.54844 |  0:02:42s
epoch 81 | loss: 0.24377 | val_0_rmse: 0.55256 | val_1_rmse: 0.56379 |  0:02:44s
epoch 82 | loss: 0.24512 | val_0_rmse: 0.51258 | val_1_rmse: 0.51449 |  0:02:46s
epoch 83 | loss: 0.24518 | val_0_rmse: 0.66643 | val_1_rmse: 0.66315 |  0:02:48s
epoch 84 | loss: 0.248   | val_0_rmse: 0.50843 | val_1_rmse: 0.51714 |  0:02:50s
epoch 85 | loss: 0.24505 | val_0_rmse: 0.51757 | val_1_rmse: 0.52344 |  0:02:52s
epoch 86 | loss: 0.24634 | val_0_rmse: 0.54226 | val_1_rmse: 0.54673 |  0:02:54s
epoch 87 | loss: 0.24153 | val_0_rmse: 0.54473 | val_1_rmse: 0.54967 |  0:02:56s
epoch 88 | loss: 0.24089 | val_0_rmse: 0.5352  | val_1_rmse: 0.54001 |  0:02:58s
epoch 89 | loss: 0.2423  | val_0_rmse: 0.5373  | val_1_rmse: 0.54685 |  0:03:00s
epoch 90 | loss: 0.24969 | val_0_rmse: 0.53022 | val_1_rmse: 0.53317 |  0:03:02s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.49451
Best weights from best epoch are automatically used!
ended training at: 14:36:26
Feature importance:
[('Area', 0.33331731834557937), ('Baths', 0.10211727691520055), ('Beds', 0.19059117642442117), ('Latitude', 0.14094339064041964), ('Longitude', 0.12455857084291477), ('Month', 0.08165128546069593), ('Year', 0.026820981370768587)]
Mean squared error is of 962665880.3419535
Mean absolute error:21395.742749051442
MAPE:0.27166277088334584
R2 score:0.761459769017134
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:36:26
epoch 0  | loss: 0.5406  | val_0_rmse: 0.65362 | val_1_rmse: 0.65223 |  0:00:02s
epoch 1  | loss: 0.33409 | val_0_rmse: 0.56454 | val_1_rmse: 0.55831 |  0:00:04s
epoch 2  | loss: 0.31011 | val_0_rmse: 0.55748 | val_1_rmse: 0.55194 |  0:00:06s
epoch 3  | loss: 0.30824 | val_0_rmse: 0.54286 | val_1_rmse: 0.53911 |  0:00:08s
epoch 4  | loss: 0.29683 | val_0_rmse: 0.52803 | val_1_rmse: 0.52502 |  0:00:10s
epoch 5  | loss: 0.29671 | val_0_rmse: 0.55844 | val_1_rmse: 0.55446 |  0:00:12s
epoch 6  | loss: 0.29828 | val_0_rmse: 0.53349 | val_1_rmse: 0.52927 |  0:00:14s
epoch 7  | loss: 0.30263 | val_0_rmse: 0.5347  | val_1_rmse: 0.52882 |  0:00:16s
epoch 8  | loss: 0.29655 | val_0_rmse: 0.52345 | val_1_rmse: 0.51848 |  0:00:18s
epoch 9  | loss: 0.29056 | val_0_rmse: 0.52562 | val_1_rmse: 0.52169 |  0:00:20s
epoch 10 | loss: 0.28426 | val_0_rmse: 0.52126 | val_1_rmse: 0.51778 |  0:00:22s
epoch 11 | loss: 0.28605 | val_0_rmse: 0.53091 | val_1_rmse: 0.52893 |  0:00:24s
epoch 12 | loss: 0.28482 | val_0_rmse: 0.51611 | val_1_rmse: 0.51147 |  0:00:26s
epoch 13 | loss: 0.28468 | val_0_rmse: 0.52431 | val_1_rmse: 0.51852 |  0:00:29s
epoch 14 | loss: 0.28093 | val_0_rmse: 0.52722 | val_1_rmse: 0.52508 |  0:00:31s
epoch 15 | loss: 0.27871 | val_0_rmse: 0.51882 | val_1_rmse: 0.51695 |  0:00:33s
epoch 16 | loss: 0.27966 | val_0_rmse: 0.51346 | val_1_rmse: 0.50856 |  0:00:35s
epoch 17 | loss: 0.2761  | val_0_rmse: 0.55149 | val_1_rmse: 0.5435  |  0:00:37s
epoch 18 | loss: 0.28038 | val_0_rmse: 0.52675 | val_1_rmse: 0.52152 |  0:00:39s
epoch 19 | loss: 0.27959 | val_0_rmse: 0.51302 | val_1_rmse: 0.5096  |  0:00:41s
epoch 20 | loss: 0.27803 | val_0_rmse: 0.52104 | val_1_rmse: 0.51847 |  0:00:43s
epoch 21 | loss: 0.27303 | val_0_rmse: 0.51784 | val_1_rmse: 0.51506 |  0:00:45s
epoch 22 | loss: 0.27265 | val_0_rmse: 0.57179 | val_1_rmse: 0.56901 |  0:00:47s
epoch 23 | loss: 0.27394 | val_0_rmse: 0.52389 | val_1_rmse: 0.5171  |  0:00:49s
epoch 24 | loss: 0.27133 | val_0_rmse: 0.51975 | val_1_rmse: 0.52063 |  0:00:51s
epoch 25 | loss: 0.26672 | val_0_rmse: 0.51932 | val_1_rmse: 0.51846 |  0:00:53s
epoch 26 | loss: 0.26939 | val_0_rmse: 0.53413 | val_1_rmse: 0.53363 |  0:00:55s
epoch 27 | loss: 0.27086 | val_0_rmse: 0.60905 | val_1_rmse: 0.61063 |  0:00:57s
epoch 28 | loss: 0.26731 | val_0_rmse: 0.51217 | val_1_rmse: 0.51122 |  0:00:59s
epoch 29 | loss: 0.26447 | val_0_rmse: 0.50619 | val_1_rmse: 0.50777 |  0:01:01s
epoch 30 | loss: 0.26404 | val_0_rmse: 0.52553 | val_1_rmse: 0.52082 |  0:01:03s
epoch 31 | loss: 0.2657  | val_0_rmse: 0.50099 | val_1_rmse: 0.49957 |  0:01:05s
epoch 32 | loss: 0.25794 | val_0_rmse: 0.50666 | val_1_rmse: 0.50881 |  0:01:07s
epoch 33 | loss: 0.26408 | val_0_rmse: 0.52935 | val_1_rmse: 0.52637 |  0:01:09s
epoch 34 | loss: 0.25894 | val_0_rmse: 0.51527 | val_1_rmse: 0.51683 |  0:01:11s
epoch 35 | loss: 0.25763 | val_0_rmse: 0.50844 | val_1_rmse: 0.50622 |  0:01:13s
epoch 36 | loss: 0.26162 | val_0_rmse: 0.52012 | val_1_rmse: 0.52103 |  0:01:15s
epoch 37 | loss: 0.25957 | val_0_rmse: 0.51016 | val_1_rmse: 0.50722 |  0:01:17s
epoch 38 | loss: 0.2592  | val_0_rmse: 0.50198 | val_1_rmse: 0.49876 |  0:01:19s
epoch 39 | loss: 0.25693 | val_0_rmse: 0.55401 | val_1_rmse: 0.55584 |  0:01:21s
epoch 40 | loss: 0.25818 | val_0_rmse: 0.50496 | val_1_rmse: 0.50618 |  0:01:23s
epoch 41 | loss: 0.25833 | val_0_rmse: 0.52191 | val_1_rmse: 0.52339 |  0:01:25s
epoch 42 | loss: 0.26017 | val_0_rmse: 0.50654 | val_1_rmse: 0.50574 |  0:01:27s
epoch 43 | loss: 0.25806 | val_0_rmse: 0.50529 | val_1_rmse: 0.50701 |  0:01:29s
epoch 44 | loss: 0.25854 | val_0_rmse: 0.49999 | val_1_rmse: 0.50158 |  0:01:31s
epoch 45 | loss: 0.25719 | val_0_rmse: 0.52994 | val_1_rmse: 0.52819 |  0:01:33s
epoch 46 | loss: 0.25512 | val_0_rmse: 0.52653 | val_1_rmse: 0.52449 |  0:01:35s
epoch 47 | loss: 0.25491 | val_0_rmse: 0.5084  | val_1_rmse: 0.50683 |  0:01:37s
epoch 48 | loss: 0.25182 | val_0_rmse: 0.49941 | val_1_rmse: 0.49836 |  0:01:39s
epoch 49 | loss: 0.25627 | val_0_rmse: 0.50114 | val_1_rmse: 0.50012 |  0:01:41s
epoch 50 | loss: 0.2605  | val_0_rmse: 0.5003  | val_1_rmse: 0.50105 |  0:01:44s
epoch 51 | loss: 0.25507 | val_0_rmse: 0.52069 | val_1_rmse: 0.51658 |  0:01:46s
epoch 52 | loss: 0.25415 | val_0_rmse: 0.52167 | val_1_rmse: 0.51915 |  0:01:48s
epoch 53 | loss: 0.25964 | val_0_rmse: 0.54137 | val_1_rmse: 0.53568 |  0:01:50s
epoch 54 | loss: 0.2569  | val_0_rmse: 0.50357 | val_1_rmse: 0.50224 |  0:01:52s
epoch 55 | loss: 0.25316 | val_0_rmse: 0.53487 | val_1_rmse: 0.53702 |  0:01:54s
epoch 56 | loss: 0.24969 | val_0_rmse: 0.54515 | val_1_rmse: 0.54314 |  0:01:56s
epoch 57 | loss: 0.25195 | val_0_rmse: 0.51113 | val_1_rmse: 0.51162 |  0:01:58s
epoch 58 | loss: 0.25422 | val_0_rmse: 0.49652 | val_1_rmse: 0.49488 |  0:02:00s
epoch 59 | loss: 0.25026 | val_0_rmse: 0.54116 | val_1_rmse: 0.53793 |  0:02:02s
epoch 60 | loss: 0.25559 | val_0_rmse: 0.54546 | val_1_rmse: 0.54194 |  0:02:04s
epoch 61 | loss: 0.24985 | val_0_rmse: 0.50562 | val_1_rmse: 0.49866 |  0:02:06s
epoch 62 | loss: 0.24859 | val_0_rmse: 0.57682 | val_1_rmse: 0.5741  |  0:02:08s
epoch 63 | loss: 0.25523 | val_0_rmse: 0.55706 | val_1_rmse: 0.55559 |  0:02:10s
epoch 64 | loss: 0.25123 | val_0_rmse: 0.51247 | val_1_rmse: 0.50976 |  0:02:12s
epoch 65 | loss: 0.24843 | val_0_rmse: 0.51156 | val_1_rmse: 0.51057 |  0:02:14s
epoch 66 | loss: 0.24896 | val_0_rmse: 0.48461 | val_1_rmse: 0.48243 |  0:02:16s
epoch 67 | loss: 0.25123 | val_0_rmse: 0.54545 | val_1_rmse: 0.54236 |  0:02:18s
epoch 68 | loss: 0.25102 | val_0_rmse: 0.52018 | val_1_rmse: 0.51892 |  0:02:20s
epoch 69 | loss: 0.25073 | val_0_rmse: 0.62024 | val_1_rmse: 0.61536 |  0:02:22s
epoch 70 | loss: 0.24574 | val_0_rmse: 0.49575 | val_1_rmse: 0.49508 |  0:02:24s
epoch 71 | loss: 0.25018 | val_0_rmse: 0.53729 | val_1_rmse: 0.53557 |  0:02:26s
epoch 72 | loss: 0.24369 | val_0_rmse: 0.54816 | val_1_rmse: 0.5446  |  0:02:28s
epoch 73 | loss: 0.24676 | val_0_rmse: 0.50156 | val_1_rmse: 0.50053 |  0:02:30s
epoch 74 | loss: 0.24567 | val_0_rmse: 0.50736 | val_1_rmse: 0.50798 |  0:02:32s
epoch 75 | loss: 0.24332 | val_0_rmse: 0.54216 | val_1_rmse: 0.54158 |  0:02:34s
epoch 76 | loss: 0.24843 | val_0_rmse: 0.56206 | val_1_rmse: 0.56558 |  0:02:36s
epoch 77 | loss: 0.24407 | val_0_rmse: 0.49984 | val_1_rmse: 0.49604 |  0:02:38s
epoch 78 | loss: 0.24002 | val_0_rmse: 0.47732 | val_1_rmse: 0.47616 |  0:02:40s
epoch 79 | loss: 0.23694 | val_0_rmse: 0.47933 | val_1_rmse: 0.47942 |  0:02:42s
epoch 80 | loss: 0.24081 | val_0_rmse: 0.49399 | val_1_rmse: 0.49306 |  0:02:44s
epoch 81 | loss: 0.2408  | val_0_rmse: 0.51696 | val_1_rmse: 0.51469 |  0:02:46s
epoch 82 | loss: 0.23893 | val_0_rmse: 0.52348 | val_1_rmse: 0.52224 |  0:02:48s
epoch 83 | loss: 0.24035 | val_0_rmse: 0.51979 | val_1_rmse: 0.51779 |  0:02:50s
epoch 84 | loss: 0.23694 | val_0_rmse: 0.48954 | val_1_rmse: 0.48614 |  0:02:52s
epoch 85 | loss: 0.2396  | val_0_rmse: 0.53999 | val_1_rmse: 0.54515 |  0:02:54s
epoch 86 | loss: 0.24067 | val_0_rmse: 0.49784 | val_1_rmse: 0.49591 |  0:02:56s
epoch 87 | loss: 0.23885 | val_0_rmse: 0.47899 | val_1_rmse: 0.48052 |  0:02:58s
epoch 88 | loss: 0.24028 | val_0_rmse: 0.48048 | val_1_rmse: 0.47925 |  0:03:00s
epoch 89 | loss: 0.23796 | val_0_rmse: 0.50839 | val_1_rmse: 0.50691 |  0:03:02s
epoch 90 | loss: 0.23947 | val_0_rmse: 0.56701 | val_1_rmse: 0.57096 |  0:03:04s
epoch 91 | loss: 0.24088 | val_0_rmse: 0.52415 | val_1_rmse: 0.52524 |  0:03:06s
epoch 92 | loss: 0.23938 | val_0_rmse: 0.49617 | val_1_rmse: 0.49716 |  0:03:09s
epoch 93 | loss: 0.24079 | val_0_rmse: 0.55962 | val_1_rmse: 0.55774 |  0:03:11s
epoch 94 | loss: 0.24072 | val_0_rmse: 0.53446 | val_1_rmse: 0.53308 |  0:03:13s
epoch 95 | loss: 0.238   | val_0_rmse: 0.48853 | val_1_rmse: 0.48668 |  0:03:15s
epoch 96 | loss: 0.24019 | val_0_rmse: 0.49733 | val_1_rmse: 0.49573 |  0:03:17s
epoch 97 | loss: 0.23475 | val_0_rmse: 0.56767 | val_1_rmse: 0.56614 |  0:03:19s
epoch 98 | loss: 0.23516 | val_0_rmse: 0.50798 | val_1_rmse: 0.51111 |  0:03:21s
epoch 99 | loss: 0.23745 | val_0_rmse: 0.53482 | val_1_rmse: 0.5404  |  0:03:23s
epoch 100| loss: 0.23512 | val_0_rmse: 0.58004 | val_1_rmse: 0.58012 |  0:03:25s
epoch 101| loss: 0.23656 | val_0_rmse: 0.47821 | val_1_rmse: 0.47868 |  0:03:27s
epoch 102| loss: 0.23489 | val_0_rmse: 0.51689 | val_1_rmse: 0.521   |  0:03:29s
epoch 103| loss: 0.23741 | val_0_rmse: 0.605   | val_1_rmse: 0.60537 |  0:03:31s
epoch 104| loss: 0.23276 | val_0_rmse: 0.48184 | val_1_rmse: 0.48047 |  0:03:33s
epoch 105| loss: 0.23843 | val_0_rmse: 0.56466 | val_1_rmse: 0.56392 |  0:03:35s
epoch 106| loss: 0.23602 | val_0_rmse: 0.48793 | val_1_rmse: 0.48857 |  0:03:37s
epoch 107| loss: 0.23354 | val_0_rmse: 0.47994 | val_1_rmse: 0.47953 |  0:03:39s
epoch 108| loss: 0.23731 | val_0_rmse: 0.51618 | val_1_rmse: 0.51456 |  0:03:41s

Early stopping occured at epoch 108 with best_epoch = 78 and best_val_1_rmse = 0.47616
Best weights from best epoch are automatically used!
ended training at: 14:40:08
Feature importance:
[('Area', 0.5429021564260347), ('Baths', 0.1250136615579651), ('Beds', 0.0), ('Latitude', 0.17193849725974), ('Longitude', 0.07026888009887364), ('Month', 0.08374490507291989), ('Year', 0.006131899584466729)]
Mean squared error is of 940142150.1302767
Mean absolute error:20599.642103235925
MAPE:0.25083583064527576
R2 score:0.7704208896699789
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:40:09
epoch 0  | loss: 0.52947 | val_0_rmse: 0.63814 | val_1_rmse: 0.62809 |  0:00:02s
epoch 1  | loss: 0.35515 | val_0_rmse: 0.58214 | val_1_rmse: 0.57594 |  0:00:04s
epoch 2  | loss: 0.3375  | val_0_rmse: 0.56896 | val_1_rmse: 0.55748 |  0:00:06s
epoch 3  | loss: 0.32952 | val_0_rmse: 0.55844 | val_1_rmse: 0.54717 |  0:00:08s
epoch 4  | loss: 0.31773 | val_0_rmse: 0.55254 | val_1_rmse: 0.53773 |  0:00:10s
epoch 5  | loss: 0.31649 | val_0_rmse: 0.5619  | val_1_rmse: 0.55325 |  0:00:12s
epoch 6  | loss: 0.32061 | val_0_rmse: 0.54861 | val_1_rmse: 0.53577 |  0:00:14s
epoch 7  | loss: 0.3153  | val_0_rmse: 0.54864 | val_1_rmse: 0.53495 |  0:00:16s
epoch 8  | loss: 0.30558 | val_0_rmse: 0.54672 | val_1_rmse: 0.53843 |  0:00:18s
epoch 9  | loss: 0.30657 | val_0_rmse: 0.54396 | val_1_rmse: 0.53215 |  0:00:20s
epoch 10 | loss: 0.30547 | val_0_rmse: 0.53947 | val_1_rmse: 0.52971 |  0:00:22s
epoch 11 | loss: 0.30328 | val_0_rmse: 0.54071 | val_1_rmse: 0.52587 |  0:00:24s
epoch 12 | loss: 0.29442 | val_0_rmse: 0.53815 | val_1_rmse: 0.53156 |  0:00:26s
epoch 13 | loss: 0.29249 | val_0_rmse: 0.53426 | val_1_rmse: 0.52172 |  0:00:28s
epoch 14 | loss: 0.29158 | val_0_rmse: 0.53223 | val_1_rmse: 0.5248  |  0:00:30s
epoch 15 | loss: 0.28595 | val_0_rmse: 0.5289  | val_1_rmse: 0.51877 |  0:00:32s
epoch 16 | loss: 0.2853  | val_0_rmse: 0.51896 | val_1_rmse: 0.51225 |  0:00:34s
epoch 17 | loss: 0.28213 | val_0_rmse: 0.53265 | val_1_rmse: 0.52284 |  0:00:36s
epoch 18 | loss: 0.28162 | val_0_rmse: 0.52201 | val_1_rmse: 0.50943 |  0:00:38s
epoch 19 | loss: 0.27851 | val_0_rmse: 0.53266 | val_1_rmse: 0.51988 |  0:00:40s
epoch 20 | loss: 0.27684 | val_0_rmse: 0.50732 | val_1_rmse: 0.50103 |  0:00:42s
epoch 21 | loss: 0.26985 | val_0_rmse: 0.51723 | val_1_rmse: 0.50843 |  0:00:44s
epoch 22 | loss: 0.27122 | val_0_rmse: 0.60682 | val_1_rmse: 0.5947  |  0:00:46s
epoch 23 | loss: 0.27115 | val_0_rmse: 0.51133 | val_1_rmse: 0.50219 |  0:00:48s
epoch 24 | loss: 0.2704  | val_0_rmse: 0.52298 | val_1_rmse: 0.51102 |  0:00:50s
epoch 25 | loss: 0.27049 | val_0_rmse: 0.51264 | val_1_rmse: 0.50352 |  0:00:52s
epoch 26 | loss: 0.26623 | val_0_rmse: 0.54562 | val_1_rmse: 0.54149 |  0:00:54s
epoch 27 | loss: 0.27036 | val_0_rmse: 0.5071  | val_1_rmse: 0.5001  |  0:00:56s
epoch 28 | loss: 0.26492 | val_0_rmse: 0.49965 | val_1_rmse: 0.49414 |  0:00:58s
epoch 29 | loss: 0.2643  | val_0_rmse: 0.553   | val_1_rmse: 0.55396 |  0:01:00s
epoch 30 | loss: 0.27718 | val_0_rmse: 0.52279 | val_1_rmse: 0.51015 |  0:01:02s
epoch 31 | loss: 0.26914 | val_0_rmse: 0.51225 | val_1_rmse: 0.50542 |  0:01:04s
epoch 32 | loss: 0.27316 | val_0_rmse: 0.50969 | val_1_rmse: 0.50044 |  0:01:06s
epoch 33 | loss: 0.2652  | val_0_rmse: 0.516   | val_1_rmse: 0.50887 |  0:01:08s
epoch 34 | loss: 0.26012 | val_0_rmse: 0.52295 | val_1_rmse: 0.51655 |  0:01:10s
epoch 35 | loss: 0.26083 | val_0_rmse: 0.52037 | val_1_rmse: 0.51258 |  0:01:12s
epoch 36 | loss: 0.26407 | val_0_rmse: 0.55211 | val_1_rmse: 0.54274 |  0:01:14s
epoch 37 | loss: 0.26776 | val_0_rmse: 0.51698 | val_1_rmse: 0.50774 |  0:01:16s
epoch 38 | loss: 0.26158 | val_0_rmse: 0.52157 | val_1_rmse: 0.51424 |  0:01:18s
epoch 39 | loss: 0.26174 | val_0_rmse: 0.56241 | val_1_rmse: 0.55039 |  0:01:20s
epoch 40 | loss: 0.25933 | val_0_rmse: 0.50621 | val_1_rmse: 0.50061 |  0:01:22s
epoch 41 | loss: 0.26023 | val_0_rmse: 0.5023  | val_1_rmse: 0.49959 |  0:01:25s
epoch 42 | loss: 0.26041 | val_0_rmse: 0.51347 | val_1_rmse: 0.5039  |  0:01:27s
epoch 43 | loss: 0.26136 | val_0_rmse: 0.57965 | val_1_rmse: 0.57421 |  0:01:29s
epoch 44 | loss: 0.25661 | val_0_rmse: 0.49626 | val_1_rmse: 0.49139 |  0:01:31s
epoch 45 | loss: 0.25498 | val_0_rmse: 0.58686 | val_1_rmse: 0.58483 |  0:01:33s
epoch 46 | loss: 0.26104 | val_0_rmse: 0.54671 | val_1_rmse: 0.53661 |  0:01:35s
epoch 47 | loss: 0.25897 | val_0_rmse: 0.50342 | val_1_rmse: 0.50248 |  0:01:37s
epoch 48 | loss: 0.25902 | val_0_rmse: 0.52678 | val_1_rmse: 0.51901 |  0:01:39s
epoch 49 | loss: 0.25887 | val_0_rmse: 0.49297 | val_1_rmse: 0.48968 |  0:01:41s
epoch 50 | loss: 0.25646 | val_0_rmse: 0.55021 | val_1_rmse: 0.54354 |  0:01:43s
epoch 51 | loss: 0.25345 | val_0_rmse: 0.50706 | val_1_rmse: 0.50144 |  0:01:45s
epoch 52 | loss: 0.25642 | val_0_rmse: 0.49218 | val_1_rmse: 0.48705 |  0:01:47s
epoch 53 | loss: 0.25837 | val_0_rmse: 0.50787 | val_1_rmse: 0.50388 |  0:01:49s
epoch 54 | loss: 0.25636 | val_0_rmse: 0.51422 | val_1_rmse: 0.50952 |  0:01:51s
epoch 55 | loss: 0.25397 | val_0_rmse: 0.50346 | val_1_rmse: 0.50208 |  0:01:53s
epoch 56 | loss: 0.25615 | val_0_rmse: 0.528   | val_1_rmse: 0.52927 |  0:01:55s
epoch 57 | loss: 0.25836 | val_0_rmse: 0.50377 | val_1_rmse: 0.49947 |  0:01:57s
epoch 58 | loss: 0.25426 | val_0_rmse: 0.50921 | val_1_rmse: 0.5066  |  0:01:59s
epoch 59 | loss: 0.25143 | val_0_rmse: 0.49766 | val_1_rmse: 0.4918  |  0:02:01s
epoch 60 | loss: 0.25266 | val_0_rmse: 0.49233 | val_1_rmse: 0.48807 |  0:02:03s
epoch 61 | loss: 0.25795 | val_0_rmse: 0.50049 | val_1_rmse: 0.49576 |  0:02:05s
epoch 62 | loss: 0.25283 | val_0_rmse: 0.50345 | val_1_rmse: 0.49993 |  0:02:07s
epoch 63 | loss: 0.25231 | val_0_rmse: 0.51059 | val_1_rmse: 0.50808 |  0:02:09s
epoch 64 | loss: 0.25432 | val_0_rmse: 0.54601 | val_1_rmse: 0.54176 |  0:02:11s
epoch 65 | loss: 0.25142 | val_0_rmse: 0.50789 | val_1_rmse: 0.50332 |  0:02:13s
epoch 66 | loss: 0.25237 | val_0_rmse: 0.49911 | val_1_rmse: 0.49574 |  0:02:15s
epoch 67 | loss: 0.25232 | val_0_rmse: 0.52394 | val_1_rmse: 0.52058 |  0:02:17s
epoch 68 | loss: 0.2553  | val_0_rmse: 0.51167 | val_1_rmse: 0.50731 |  0:02:19s
epoch 69 | loss: 0.2502  | val_0_rmse: 0.56181 | val_1_rmse: 0.55522 |  0:02:21s
epoch 70 | loss: 0.25142 | val_0_rmse: 0.51403 | val_1_rmse: 0.5106  |  0:02:23s
epoch 71 | loss: 0.2523  | val_0_rmse: 0.52886 | val_1_rmse: 0.52109 |  0:02:25s
epoch 72 | loss: 0.25354 | val_0_rmse: 0.49866 | val_1_rmse: 0.49883 |  0:02:27s
epoch 73 | loss: 0.25428 | val_0_rmse: 0.4968  | val_1_rmse: 0.49498 |  0:02:29s
epoch 74 | loss: 0.24825 | val_0_rmse: 0.53914 | val_1_rmse: 0.53433 |  0:02:31s
epoch 75 | loss: 0.2493  | val_0_rmse: 0.51791 | val_1_rmse: 0.51562 |  0:02:33s
epoch 76 | loss: 0.24856 | val_0_rmse: 0.49183 | val_1_rmse: 0.49072 |  0:02:35s
epoch 77 | loss: 0.24787 | val_0_rmse: 0.52033 | val_1_rmse: 0.51466 |  0:02:37s
epoch 78 | loss: 0.2539  | val_0_rmse: 0.52265 | val_1_rmse: 0.52423 |  0:02:39s
epoch 79 | loss: 0.24979 | val_0_rmse: 0.49007 | val_1_rmse: 0.48882 |  0:02:41s
epoch 80 | loss: 0.24849 | val_0_rmse: 0.50548 | val_1_rmse: 0.50518 |  0:02:43s
epoch 81 | loss: 0.24417 | val_0_rmse: 0.53123 | val_1_rmse: 0.52674 |  0:02:45s
epoch 82 | loss: 0.24838 | val_0_rmse: 0.49251 | val_1_rmse: 0.49089 |  0:02:47s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.48705
Best weights from best epoch are automatically used!
ended training at: 14:42:57
Feature importance:
[('Area', 0.37590647973560787), ('Baths', 0.13230187929209264), ('Beds', 0.14148959555718854), ('Latitude', 0.0), ('Longitude', 0.2605163323903421), ('Month', 0.007532461616470183), ('Year', 0.08225325140829863)]
Mean squared error is of 952353947.0631635
Mean absolute error:21055.71115988387
MAPE:0.25687973488857957
R2 score:0.7609163254002549
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:42:58
epoch 0  | loss: 0.55914 | val_0_rmse: 0.59328 | val_1_rmse: 0.58404 |  0:00:02s
epoch 1  | loss: 0.33904 | val_0_rmse: 0.58014 | val_1_rmse: 0.57665 |  0:00:04s
epoch 2  | loss: 0.32168 | val_0_rmse: 0.55232 | val_1_rmse: 0.5501  |  0:00:06s
epoch 3  | loss: 0.3139  | val_0_rmse: 0.5452  | val_1_rmse: 0.54359 |  0:00:08s
epoch 4  | loss: 0.3139  | val_0_rmse: 0.54369 | val_1_rmse: 0.53989 |  0:00:10s
epoch 5  | loss: 0.30509 | val_0_rmse: 0.55839 | val_1_rmse: 0.56077 |  0:00:12s
epoch 6  | loss: 0.30295 | val_0_rmse: 0.54861 | val_1_rmse: 0.54954 |  0:00:14s
epoch 7  | loss: 0.29673 | val_0_rmse: 0.53226 | val_1_rmse: 0.53066 |  0:00:16s
epoch 8  | loss: 0.29248 | val_0_rmse: 0.53324 | val_1_rmse: 0.53123 |  0:00:18s
epoch 9  | loss: 0.29626 | val_0_rmse: 0.55272 | val_1_rmse: 0.55311 |  0:00:20s
epoch 10 | loss: 0.29439 | val_0_rmse: 0.53082 | val_1_rmse: 0.52773 |  0:00:22s
epoch 11 | loss: 0.28994 | val_0_rmse: 0.53175 | val_1_rmse: 0.53065 |  0:00:24s
epoch 12 | loss: 0.2872  | val_0_rmse: 0.52252 | val_1_rmse: 0.51706 |  0:00:26s
epoch 13 | loss: 0.28702 | val_0_rmse: 0.52399 | val_1_rmse: 0.52346 |  0:00:28s
epoch 14 | loss: 0.28415 | val_0_rmse: 0.52449 | val_1_rmse: 0.52369 |  0:00:30s
epoch 15 | loss: 0.2828  | val_0_rmse: 0.52492 | val_1_rmse: 0.52543 |  0:00:32s
epoch 16 | loss: 0.28382 | val_0_rmse: 0.52973 | val_1_rmse: 0.52931 |  0:00:34s
epoch 17 | loss: 0.28439 | val_0_rmse: 0.52272 | val_1_rmse: 0.52145 |  0:00:36s
epoch 18 | loss: 0.28162 | val_0_rmse: 0.5336  | val_1_rmse: 0.53482 |  0:00:38s
epoch 19 | loss: 0.28411 | val_0_rmse: 0.53544 | val_1_rmse: 0.53309 |  0:00:40s
epoch 20 | loss: 0.28164 | val_0_rmse: 0.53242 | val_1_rmse: 0.53442 |  0:00:42s
epoch 21 | loss: 0.27743 | val_0_rmse: 0.52941 | val_1_rmse: 0.52689 |  0:00:44s
epoch 22 | loss: 0.2786  | val_0_rmse: 0.64081 | val_1_rmse: 0.64071 |  0:00:46s
epoch 23 | loss: 0.27781 | val_0_rmse: 0.52669 | val_1_rmse: 0.52491 |  0:00:48s
epoch 24 | loss: 0.27457 | val_0_rmse: 0.55095 | val_1_rmse: 0.55054 |  0:00:50s
epoch 25 | loss: 0.2652  | val_0_rmse: 0.51285 | val_1_rmse: 0.50673 |  0:00:52s
epoch 26 | loss: 0.26068 | val_0_rmse: 0.55178 | val_1_rmse: 0.54682 |  0:00:54s
epoch 27 | loss: 0.26033 | val_0_rmse: 0.50381 | val_1_rmse: 0.50376 |  0:00:56s
epoch 28 | loss: 0.26601 | val_0_rmse: 0.54437 | val_1_rmse: 0.54555 |  0:00:58s
epoch 29 | loss: 0.2642  | val_0_rmse: 0.5345  | val_1_rmse: 0.53636 |  0:01:00s
epoch 30 | loss: 0.25935 | val_0_rmse: 0.50307 | val_1_rmse: 0.50172 |  0:01:02s
epoch 31 | loss: 0.25849 | val_0_rmse: 0.5103  | val_1_rmse: 0.50465 |  0:01:04s
epoch 32 | loss: 0.25676 | val_0_rmse: 0.50397 | val_1_rmse: 0.5035  |  0:01:06s
epoch 33 | loss: 0.2582  | val_0_rmse: 0.55873 | val_1_rmse: 0.55958 |  0:01:08s
epoch 34 | loss: 0.26527 | val_0_rmse: 0.61518 | val_1_rmse: 0.62372 |  0:01:11s
epoch 35 | loss: 0.26002 | val_0_rmse: 0.54005 | val_1_rmse: 0.53863 |  0:01:13s
epoch 36 | loss: 0.2552  | val_0_rmse: 0.50576 | val_1_rmse: 0.50631 |  0:01:15s
epoch 37 | loss: 0.2551  | val_0_rmse: 0.53658 | val_1_rmse: 0.53973 |  0:01:17s
epoch 38 | loss: 0.25395 | val_0_rmse: 0.56144 | val_1_rmse: 0.55746 |  0:01:19s
epoch 39 | loss: 0.25665 | val_0_rmse: 0.50128 | val_1_rmse: 0.50232 |  0:01:21s
epoch 40 | loss: 0.25735 | val_0_rmse: 0.58551 | val_1_rmse: 0.58632 |  0:01:23s
epoch 41 | loss: 0.25483 | val_0_rmse: 0.49805 | val_1_rmse: 0.49995 |  0:01:25s
epoch 42 | loss: 0.25259 | val_0_rmse: 0.58456 | val_1_rmse: 0.58576 |  0:01:27s
epoch 43 | loss: 0.2519  | val_0_rmse: 0.57722 | val_1_rmse: 0.58425 |  0:01:29s
epoch 44 | loss: 0.25167 | val_0_rmse: 0.51073 | val_1_rmse: 0.50914 |  0:01:31s
epoch 45 | loss: 0.26031 | val_0_rmse: 0.50886 | val_1_rmse: 0.51295 |  0:01:33s
epoch 46 | loss: 0.25287 | val_0_rmse: 0.53516 | val_1_rmse: 0.5395  |  0:01:35s
epoch 47 | loss: 0.25161 | val_0_rmse: 0.52083 | val_1_rmse: 0.52348 |  0:01:37s
epoch 48 | loss: 0.25182 | val_0_rmse: 0.57266 | val_1_rmse: 0.57623 |  0:01:39s
epoch 49 | loss: 0.25192 | val_0_rmse: 0.60926 | val_1_rmse: 0.61153 |  0:01:41s
epoch 50 | loss: 0.24987 | val_0_rmse: 0.6784  | val_1_rmse: 0.66992 |  0:01:43s
epoch 51 | loss: 0.24827 | val_0_rmse: 0.49488 | val_1_rmse: 0.49661 |  0:01:45s
epoch 52 | loss: 0.25043 | val_0_rmse: 0.5062  | val_1_rmse: 0.5092  |  0:01:47s
epoch 53 | loss: 0.24887 | val_0_rmse: 0.51236 | val_1_rmse: 0.51949 |  0:01:49s
epoch 54 | loss: 0.2486  | val_0_rmse: 0.53077 | val_1_rmse: 0.53175 |  0:01:51s
epoch 55 | loss: 0.24959 | val_0_rmse: 0.63918 | val_1_rmse: 0.63528 |  0:01:53s
epoch 56 | loss: 0.24857 | val_0_rmse: 0.54391 | val_1_rmse: 0.54539 |  0:01:55s
epoch 57 | loss: 0.24838 | val_0_rmse: 0.49715 | val_1_rmse: 0.49702 |  0:01:57s
epoch 58 | loss: 0.24673 | val_0_rmse: 0.50121 | val_1_rmse: 0.50069 |  0:01:59s
epoch 59 | loss: 0.24782 | val_0_rmse: 0.51744 | val_1_rmse: 0.51885 |  0:02:01s
epoch 60 | loss: 0.25125 | val_0_rmse: 0.57077 | val_1_rmse: 0.5818  |  0:02:03s
epoch 61 | loss: 0.24918 | val_0_rmse: 0.49906 | val_1_rmse: 0.50193 |  0:02:05s
epoch 62 | loss: 0.24542 | val_0_rmse: 0.59593 | val_1_rmse: 0.60326 |  0:02:07s
epoch 63 | loss: 0.24664 | val_0_rmse: 0.5175  | val_1_rmse: 0.52192 |  0:02:09s
epoch 64 | loss: 0.24436 | val_0_rmse: 0.54029 | val_1_rmse: 0.54558 |  0:02:11s
epoch 65 | loss: 0.24404 | val_0_rmse: 0.53207 | val_1_rmse: 0.5396  |  0:02:13s
epoch 66 | loss: 0.24213 | val_0_rmse: 0.60388 | val_1_rmse: 0.6047  |  0:02:15s
epoch 67 | loss: 0.24165 | val_0_rmse: 0.51444 | val_1_rmse: 0.51837 |  0:02:17s
epoch 68 | loss: 0.24521 | val_0_rmse: 0.50186 | val_1_rmse: 0.50632 |  0:02:19s
epoch 69 | loss: 0.24257 | val_0_rmse: 0.56391 | val_1_rmse: 0.55704 |  0:02:21s
epoch 70 | loss: 0.2438  | val_0_rmse: 0.55075 | val_1_rmse: 0.55783 |  0:02:23s
epoch 71 | loss: 0.24112 | val_0_rmse: 0.51499 | val_1_rmse: 0.52179 |  0:02:25s
epoch 72 | loss: 0.24093 | val_0_rmse: 0.49963 | val_1_rmse: 0.50472 |  0:02:27s
epoch 73 | loss: 0.24089 | val_0_rmse: 0.5456  | val_1_rmse: 0.54328 |  0:02:29s
epoch 74 | loss: 0.24855 | val_0_rmse: 0.49922 | val_1_rmse: 0.50425 |  0:02:31s
epoch 75 | loss: 0.24189 | val_0_rmse: 0.54534 | val_1_rmse: 0.55299 |  0:02:33s
epoch 76 | loss: 0.24907 | val_0_rmse: 0.52351 | val_1_rmse: 0.52321 |  0:02:35s
epoch 77 | loss: 0.2454  | val_0_rmse: 0.50406 | val_1_rmse: 0.50859 |  0:02:37s
epoch 78 | loss: 0.23828 | val_0_rmse: 0.71864 | val_1_rmse: 0.72033 |  0:02:39s
epoch 79 | loss: 0.23889 | val_0_rmse: 0.5148  | val_1_rmse: 0.51941 |  0:02:41s
epoch 80 | loss: 0.23662 | val_0_rmse: 0.59308 | val_1_rmse: 0.6028  |  0:02:43s
epoch 81 | loss: 0.23911 | val_0_rmse: 0.52219 | val_1_rmse: 0.53046 |  0:02:45s

Early stopping occured at epoch 81 with best_epoch = 51 and best_val_1_rmse = 0.49661
Best weights from best epoch are automatically used!
ended training at: 14:45:44
Feature importance:
[('Area', 0.34864756338604), ('Baths', 0.25708886663464625), ('Beds', 0.12486028181801752), ('Latitude', 0.19895965750196223), ('Longitude', 0.041161260320280314), ('Month', 0.005432161201809279), ('Year', 0.02385020913724437)]
Mean squared error is of 986742817.8854777
Mean absolute error:21439.27877604374
MAPE:0.26196653581819124
R2 score:0.7550431651995086
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:45:45
epoch 0  | loss: 0.48613 | val_0_rmse: 0.58795 | val_1_rmse: 0.58439 |  0:00:03s
epoch 1  | loss: 0.32235 | val_0_rmse: 0.53444 | val_1_rmse: 0.52998 |  0:00:06s
epoch 2  | loss: 0.2953  | val_0_rmse: 0.53454 | val_1_rmse: 0.53303 |  0:00:09s
epoch 3  | loss: 0.27265 | val_0_rmse: 0.50206 | val_1_rmse: 0.49907 |  0:00:13s
epoch 4  | loss: 0.25445 | val_0_rmse: 0.47279 | val_1_rmse: 0.46911 |  0:00:16s
epoch 5  | loss: 0.24055 | val_0_rmse: 0.46211 | val_1_rmse: 0.45881 |  0:00:19s
epoch 6  | loss: 0.22956 | val_0_rmse: 0.46679 | val_1_rmse: 0.46297 |  0:00:22s
epoch 7  | loss: 0.22674 | val_0_rmse: 0.46585 | val_1_rmse: 0.46215 |  0:00:26s
epoch 8  | loss: 0.22212 | val_0_rmse: 0.44696 | val_1_rmse: 0.44485 |  0:00:29s
epoch 9  | loss: 0.22159 | val_0_rmse: 0.45063 | val_1_rmse: 0.44848 |  0:00:32s
epoch 10 | loss: 0.21741 | val_0_rmse: 0.45834 | val_1_rmse: 0.45444 |  0:00:35s
epoch 11 | loss: 0.21021 | val_0_rmse: 0.43496 | val_1_rmse: 0.43183 |  0:00:39s
epoch 12 | loss: 0.21115 | val_0_rmse: 0.43469 | val_1_rmse: 0.43311 |  0:00:42s
epoch 13 | loss: 0.20959 | val_0_rmse: 0.44091 | val_1_rmse: 0.43789 |  0:00:45s
epoch 14 | loss: 0.20786 | val_0_rmse: 0.44659 | val_1_rmse: 0.44456 |  0:00:48s
epoch 15 | loss: 0.20929 | val_0_rmse: 0.4312  | val_1_rmse: 0.42792 |  0:00:52s
epoch 16 | loss: 0.20495 | val_0_rmse: 0.44505 | val_1_rmse: 0.44224 |  0:00:55s
epoch 17 | loss: 0.2104  | val_0_rmse: 0.4252  | val_1_rmse: 0.42464 |  0:00:58s
epoch 18 | loss: 0.20427 | val_0_rmse: 0.43545 | val_1_rmse: 0.43355 |  0:01:01s
epoch 19 | loss: 0.20517 | val_0_rmse: 0.4373  | val_1_rmse: 0.43495 |  0:01:05s
epoch 20 | loss: 0.20197 | val_0_rmse: 0.43927 | val_1_rmse: 0.43515 |  0:01:08s
epoch 21 | loss: 0.20268 | val_0_rmse: 0.42191 | val_1_rmse: 0.42127 |  0:01:11s
epoch 22 | loss: 0.19787 | val_0_rmse: 0.4317  | val_1_rmse: 0.42891 |  0:01:14s
epoch 23 | loss: 0.19723 | val_0_rmse: 0.43458 | val_1_rmse: 0.43481 |  0:01:18s
epoch 24 | loss: 0.19603 | val_0_rmse: 0.42641 | val_1_rmse: 0.42681 |  0:01:21s
epoch 25 | loss: 0.20022 | val_0_rmse: 0.43815 | val_1_rmse: 0.43743 |  0:01:24s
epoch 26 | loss: 0.19974 | val_0_rmse: 0.42335 | val_1_rmse: 0.42454 |  0:01:27s
epoch 27 | loss: 0.1994  | val_0_rmse: 0.43111 | val_1_rmse: 0.43273 |  0:01:31s
epoch 28 | loss: 0.19781 | val_0_rmse: 0.41778 | val_1_rmse: 0.41811 |  0:01:34s
epoch 29 | loss: 0.19926 | val_0_rmse: 0.42889 | val_1_rmse: 0.42876 |  0:01:37s
epoch 30 | loss: 0.19561 | val_0_rmse: 0.42186 | val_1_rmse: 0.42218 |  0:01:40s
epoch 31 | loss: 0.19434 | val_0_rmse: 0.42013 | val_1_rmse: 0.4193  |  0:01:44s
epoch 32 | loss: 0.19658 | val_0_rmse: 0.41493 | val_1_rmse: 0.41708 |  0:01:47s
epoch 33 | loss: 0.19924 | val_0_rmse: 0.42076 | val_1_rmse: 0.42024 |  0:01:50s
epoch 34 | loss: 0.19685 | val_0_rmse: 0.42382 | val_1_rmse: 0.42376 |  0:01:53s
epoch 35 | loss: 0.19728 | val_0_rmse: 0.41791 | val_1_rmse: 0.41848 |  0:01:57s
epoch 36 | loss: 0.19365 | val_0_rmse: 0.42053 | val_1_rmse: 0.42008 |  0:02:00s
epoch 37 | loss: 0.1952  | val_0_rmse: 0.42882 | val_1_rmse: 0.4311  |  0:02:03s
epoch 38 | loss: 0.19487 | val_0_rmse: 0.4291  | val_1_rmse: 0.43078 |  0:02:06s
epoch 39 | loss: 0.19261 | val_0_rmse: 0.41814 | val_1_rmse: 0.42143 |  0:02:10s
epoch 40 | loss: 0.19254 | val_0_rmse: 0.42613 | val_1_rmse: 0.42862 |  0:02:13s
epoch 41 | loss: 0.19455 | val_0_rmse: 0.41149 | val_1_rmse: 0.41182 |  0:02:16s
epoch 42 | loss: 0.19116 | val_0_rmse: 0.42156 | val_1_rmse: 0.42224 |  0:02:20s
epoch 43 | loss: 0.19457 | val_0_rmse: 0.41163 | val_1_rmse: 0.41383 |  0:02:23s
epoch 44 | loss: 0.19454 | val_0_rmse: 0.42977 | val_1_rmse: 0.43227 |  0:02:26s
epoch 45 | loss: 0.19292 | val_0_rmse: 0.42061 | val_1_rmse: 0.42478 |  0:02:29s
epoch 46 | loss: 0.19041 | val_0_rmse: 0.41367 | val_1_rmse: 0.41593 |  0:02:32s
epoch 47 | loss: 0.19606 | val_0_rmse: 0.41877 | val_1_rmse: 0.42247 |  0:02:36s
epoch 48 | loss: 0.1926  | val_0_rmse: 0.41947 | val_1_rmse: 0.42062 |  0:02:39s
epoch 49 | loss: 0.19154 | val_0_rmse: 0.41867 | val_1_rmse: 0.42073 |  0:02:42s
epoch 50 | loss: 0.19311 | val_0_rmse: 0.42656 | val_1_rmse: 0.42969 |  0:02:45s
epoch 51 | loss: 0.19584 | val_0_rmse: 0.43456 | val_1_rmse: 0.43755 |  0:02:49s
epoch 52 | loss: 0.19144 | val_0_rmse: 0.42382 | val_1_rmse: 0.4269  |  0:02:52s
epoch 53 | loss: 0.18918 | val_0_rmse: 0.42043 | val_1_rmse: 0.4221  |  0:02:55s
epoch 54 | loss: 0.19215 | val_0_rmse: 0.42    | val_1_rmse: 0.42502 |  0:02:58s
epoch 55 | loss: 0.18904 | val_0_rmse: 0.42013 | val_1_rmse: 0.42265 |  0:03:02s
epoch 56 | loss: 0.19421 | val_0_rmse: 0.42568 | val_1_rmse: 0.42667 |  0:03:05s
epoch 57 | loss: 0.18789 | val_0_rmse: 0.41481 | val_1_rmse: 0.41775 |  0:03:08s
epoch 58 | loss: 0.18621 | val_0_rmse: 0.41418 | val_1_rmse: 0.41905 |  0:03:11s
epoch 59 | loss: 0.18825 | val_0_rmse: 0.41891 | val_1_rmse: 0.42296 |  0:03:15s
epoch 60 | loss: 0.18566 | val_0_rmse: 0.41071 | val_1_rmse: 0.41416 |  0:03:18s
epoch 61 | loss: 0.19045 | val_0_rmse: 0.41711 | val_1_rmse: 0.41884 |  0:03:21s
epoch 62 | loss: 0.19047 | val_0_rmse: 0.42547 | val_1_rmse: 0.42588 |  0:03:24s
epoch 63 | loss: 0.18905 | val_0_rmse: 0.41976 | val_1_rmse: 0.42274 |  0:03:28s
epoch 64 | loss: 0.18675 | val_0_rmse: 0.41979 | val_1_rmse: 0.42384 |  0:03:31s
epoch 65 | loss: 0.19101 | val_0_rmse: 0.4157  | val_1_rmse: 0.4177  |  0:03:34s
epoch 66 | loss: 0.18753 | val_0_rmse: 0.43468 | val_1_rmse: 0.43773 |  0:03:37s
epoch 67 | loss: 0.18789 | val_0_rmse: 0.42329 | val_1_rmse: 0.42692 |  0:03:41s
epoch 68 | loss: 0.18528 | val_0_rmse: 0.42403 | val_1_rmse: 0.42541 |  0:03:44s
epoch 69 | loss: 0.18852 | val_0_rmse: 0.4165  | val_1_rmse: 0.41941 |  0:03:47s
epoch 70 | loss: 0.18841 | val_0_rmse: 0.41703 | val_1_rmse: 0.4192  |  0:03:50s
epoch 71 | loss: 0.18489 | val_0_rmse: 0.41985 | val_1_rmse: 0.42389 |  0:03:53s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.41182
Best weights from best epoch are automatically used!
ended training at: 14:49:40
Feature importance:
[('Area', 0.0201353674132501), ('Baths', 0.19362245311205592), ('Beds', 0.03345588201241402), ('Latitude', 0.13986711021744758), ('Longitude', 0.19067193143693695), ('Month', 0.08062135546154427), ('Year', 0.34162590034635115)]
Mean squared error is of 9785361448.163956
Mean absolute error:68246.15338409902
MAPE:0.2773404354592682
R2 score:0.8279290150385072
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:49:41
epoch 0  | loss: 0.4502  | val_0_rmse: 0.56322 | val_1_rmse: 0.5539  |  0:00:03s
epoch 1  | loss: 0.28453 | val_0_rmse: 0.50644 | val_1_rmse: 0.49629 |  0:00:06s
epoch 2  | loss: 0.26328 | val_0_rmse: 0.47913 | val_1_rmse: 0.46731 |  0:00:09s
epoch 3  | loss: 0.24554 | val_0_rmse: 0.48592 | val_1_rmse: 0.47539 |  0:00:12s
epoch 4  | loss: 0.24031 | val_0_rmse: 0.46909 | val_1_rmse: 0.45783 |  0:00:16s
epoch 5  | loss: 0.23626 | val_0_rmse: 0.45534 | val_1_rmse: 0.43939 |  0:00:19s
epoch 6  | loss: 0.22793 | val_0_rmse: 0.44766 | val_1_rmse: 0.43707 |  0:00:22s
epoch 7  | loss: 0.22594 | val_0_rmse: 0.45237 | val_1_rmse: 0.44183 |  0:00:26s
epoch 8  | loss: 0.22065 | val_0_rmse: 0.45467 | val_1_rmse: 0.44075 |  0:00:29s
epoch 9  | loss: 0.22204 | val_0_rmse: 0.48229 | val_1_rmse: 0.46611 |  0:00:33s
epoch 10 | loss: 0.21904 | val_0_rmse: 0.44083 | val_1_rmse: 0.4286  |  0:00:36s
epoch 11 | loss: 0.22101 | val_0_rmse: 0.44902 | val_1_rmse: 0.4404  |  0:00:40s
epoch 12 | loss: 0.21363 | val_0_rmse: 0.4572  | val_1_rmse: 0.44736 |  0:00:43s
epoch 13 | loss: 0.21459 | val_0_rmse: 0.44621 | val_1_rmse: 0.43602 |  0:00:46s
epoch 14 | loss: 0.21545 | val_0_rmse: 0.43775 | val_1_rmse: 0.42693 |  0:00:50s
epoch 15 | loss: 0.20804 | val_0_rmse: 0.43437 | val_1_rmse: 0.42612 |  0:00:53s
epoch 16 | loss: 0.2082  | val_0_rmse: 0.43968 | val_1_rmse: 0.43381 |  0:00:58s
epoch 17 | loss: 0.20662 | val_0_rmse: 0.45215 | val_1_rmse: 0.4418  |  0:01:01s
epoch 18 | loss: 0.20674 | val_0_rmse: 0.44371 | val_1_rmse: 0.43523 |  0:01:05s
epoch 19 | loss: 0.2036  | val_0_rmse: 0.44225 | val_1_rmse: 0.43487 |  0:01:08s
epoch 20 | loss: 0.20488 | val_0_rmse: 0.42681 | val_1_rmse: 0.42155 |  0:01:11s
epoch 21 | loss: 0.20507 | val_0_rmse: 0.42452 | val_1_rmse: 0.41668 |  0:01:14s
epoch 22 | loss: 0.20459 | val_0_rmse: 0.4377  | val_1_rmse: 0.43253 |  0:01:18s
epoch 23 | loss: 0.20135 | val_0_rmse: 0.45365 | val_1_rmse: 0.44714 |  0:01:21s
epoch 24 | loss: 0.20466 | val_0_rmse: 0.4436  | val_1_rmse: 0.43693 |  0:01:24s
epoch 25 | loss: 0.20237 | val_0_rmse: 0.44707 | val_1_rmse: 0.43973 |  0:01:27s
epoch 26 | loss: 0.2054  | val_0_rmse: 0.4313  | val_1_rmse: 0.4234  |  0:01:31s
epoch 27 | loss: 0.2055  | val_0_rmse: 0.43467 | val_1_rmse: 0.42657 |  0:01:34s
epoch 28 | loss: 0.20294 | val_0_rmse: 0.43172 | val_1_rmse: 0.42446 |  0:01:37s
epoch 29 | loss: 0.21077 | val_0_rmse: 0.45144 | val_1_rmse: 0.44087 |  0:01:40s
epoch 30 | loss: 0.21001 | val_0_rmse: 0.44747 | val_1_rmse: 0.43903 |  0:01:43s
epoch 31 | loss: 0.20264 | val_0_rmse: 0.42644 | val_1_rmse: 0.41936 |  0:01:47s
epoch 32 | loss: 0.20041 | val_0_rmse: 0.44824 | val_1_rmse: 0.4417  |  0:01:50s
epoch 33 | loss: 0.19896 | val_0_rmse: 0.41911 | val_1_rmse: 0.41354 |  0:01:53s
epoch 34 | loss: 0.19536 | val_0_rmse: 0.41872 | val_1_rmse: 0.4118  |  0:01:56s
epoch 35 | loss: 0.19518 | val_0_rmse: 0.42224 | val_1_rmse: 0.4141  |  0:02:00s
epoch 36 | loss: 0.1948  | val_0_rmse: 0.45058 | val_1_rmse: 0.44367 |  0:02:03s
epoch 37 | loss: 0.1982  | val_0_rmse: 0.41896 | val_1_rmse: 0.41201 |  0:02:06s
epoch 38 | loss: 0.19266 | val_0_rmse: 0.41781 | val_1_rmse: 0.41306 |  0:02:09s
epoch 39 | loss: 0.19569 | val_0_rmse: 0.43356 | val_1_rmse: 0.42595 |  0:02:13s
epoch 40 | loss: 0.19749 | val_0_rmse: 0.4226  | val_1_rmse: 0.41641 |  0:02:16s
epoch 41 | loss: 0.1968  | val_0_rmse: 0.4319  | val_1_rmse: 0.42473 |  0:02:19s
epoch 42 | loss: 0.19206 | val_0_rmse: 0.41711 | val_1_rmse: 0.41042 |  0:02:22s
epoch 43 | loss: 0.18905 | val_0_rmse: 0.41952 | val_1_rmse: 0.41228 |  0:02:26s
epoch 44 | loss: 0.18958 | val_0_rmse: 0.42498 | val_1_rmse: 0.41819 |  0:02:29s
epoch 45 | loss: 0.18887 | val_0_rmse: 0.42548 | val_1_rmse: 0.41923 |  0:02:32s
epoch 46 | loss: 0.19338 | val_0_rmse: 0.42699 | val_1_rmse: 0.42207 |  0:02:35s
epoch 47 | loss: 0.18821 | val_0_rmse: 0.42016 | val_1_rmse: 0.41508 |  0:02:38s
epoch 48 | loss: 0.18789 | val_0_rmse: 0.42582 | val_1_rmse: 0.42037 |  0:02:42s
epoch 49 | loss: 0.18827 | val_0_rmse: 0.45465 | val_1_rmse: 0.44838 |  0:02:45s
epoch 50 | loss: 0.19083 | val_0_rmse: 0.40957 | val_1_rmse: 0.40369 |  0:02:48s
epoch 51 | loss: 0.18944 | val_0_rmse: 0.42902 | val_1_rmse: 0.4251  |  0:02:51s
epoch 52 | loss: 0.18679 | val_0_rmse: 0.43024 | val_1_rmse: 0.42431 |  0:02:55s
epoch 53 | loss: 0.19767 | val_0_rmse: 0.42877 | val_1_rmse: 0.42302 |  0:02:58s
epoch 54 | loss: 0.19357 | val_0_rmse: 0.42007 | val_1_rmse: 0.41396 |  0:03:01s
epoch 55 | loss: 0.18871 | val_0_rmse: 0.44981 | val_1_rmse: 0.44393 |  0:03:04s
epoch 56 | loss: 0.19044 | val_0_rmse: 0.41675 | val_1_rmse: 0.41279 |  0:03:08s
epoch 57 | loss: 0.18925 | val_0_rmse: 0.4291  | val_1_rmse: 0.42134 |  0:03:11s
epoch 58 | loss: 0.19796 | val_0_rmse: 0.41168 | val_1_rmse: 0.40702 |  0:03:14s
epoch 59 | loss: 0.18904 | val_0_rmse: 0.42186 | val_1_rmse: 0.41481 |  0:03:17s
epoch 60 | loss: 0.18729 | val_0_rmse: 0.41315 | val_1_rmse: 0.40776 |  0:03:20s
epoch 61 | loss: 0.18362 | val_0_rmse: 0.4111  | val_1_rmse: 0.4071  |  0:03:24s
epoch 62 | loss: 0.1835  | val_0_rmse: 0.41277 | val_1_rmse: 0.40812 |  0:03:27s
epoch 63 | loss: 0.18437 | val_0_rmse: 0.41203 | val_1_rmse: 0.41196 |  0:03:30s
epoch 64 | loss: 0.18534 | val_0_rmse: 0.40781 | val_1_rmse: 0.40572 |  0:03:33s
epoch 65 | loss: 0.18361 | val_0_rmse: 0.4199  | val_1_rmse: 0.41708 |  0:03:37s
epoch 66 | loss: 0.18471 | val_0_rmse: 0.42095 | val_1_rmse: 0.41525 |  0:03:40s
epoch 67 | loss: 0.18445 | val_0_rmse: 0.40709 | val_1_rmse: 0.40575 |  0:03:43s
epoch 68 | loss: 0.19073 | val_0_rmse: 0.42613 | val_1_rmse: 0.4209  |  0:03:46s
epoch 69 | loss: 0.18957 | val_0_rmse: 0.4229  | val_1_rmse: 0.41648 |  0:03:50s
epoch 70 | loss: 0.18767 | val_0_rmse: 0.41989 | val_1_rmse: 0.41816 |  0:03:53s
epoch 71 | loss: 0.18155 | val_0_rmse: 0.41223 | val_1_rmse: 0.40951 |  0:03:56s
epoch 72 | loss: 0.18233 | val_0_rmse: 0.41766 | val_1_rmse: 0.41708 |  0:03:59s
epoch 73 | loss: 0.18586 | val_0_rmse: 0.4278  | val_1_rmse: 0.4241  |  0:04:02s
epoch 74 | loss: 0.18589 | val_0_rmse: 0.41844 | val_1_rmse: 0.41655 |  0:04:06s
epoch 75 | loss: 0.1812  | val_0_rmse: 0.41101 | val_1_rmse: 0.40912 |  0:04:09s
epoch 76 | loss: 0.18446 | val_0_rmse: 0.40163 | val_1_rmse: 0.39946 |  0:04:12s
epoch 77 | loss: 0.17947 | val_0_rmse: 0.40245 | val_1_rmse: 0.40172 |  0:04:15s
epoch 78 | loss: 0.17997 | val_0_rmse: 0.40627 | val_1_rmse: 0.40416 |  0:04:19s
epoch 79 | loss: 0.18323 | val_0_rmse: 0.40539 | val_1_rmse: 0.4011  |  0:04:22s
epoch 80 | loss: 0.18028 | val_0_rmse: 0.40713 | val_1_rmse: 0.40542 |  0:04:25s
epoch 81 | loss: 0.18044 | val_0_rmse: 0.4157  | val_1_rmse: 0.41221 |  0:04:28s
epoch 82 | loss: 0.1814  | val_0_rmse: 0.40853 | val_1_rmse: 0.40752 |  0:04:32s
epoch 83 | loss: 0.17912 | val_0_rmse: 0.41128 | val_1_rmse: 0.40917 |  0:04:35s
epoch 84 | loss: 0.17777 | val_0_rmse: 0.40875 | val_1_rmse: 0.40762 |  0:04:38s
epoch 85 | loss: 0.18088 | val_0_rmse: 0.41362 | val_1_rmse: 0.4146  |  0:04:41s
epoch 86 | loss: 0.18142 | val_0_rmse: 0.40945 | val_1_rmse: 0.40875 |  0:04:44s
epoch 87 | loss: 0.18147 | val_0_rmse: 0.42188 | val_1_rmse: 0.42073 |  0:04:48s
epoch 88 | loss: 0.18229 | val_0_rmse: 0.43329 | val_1_rmse: 0.43133 |  0:04:51s
epoch 89 | loss: 0.17875 | val_0_rmse: 0.4428  | val_1_rmse: 0.44194 |  0:04:54s
epoch 90 | loss: 0.17862 | val_0_rmse: 0.41857 | val_1_rmse: 0.41694 |  0:04:57s
epoch 91 | loss: 0.17773 | val_0_rmse: 0.40018 | val_1_rmse: 0.3989  |  0:05:01s
epoch 92 | loss: 0.17594 | val_0_rmse: 0.40045 | val_1_rmse: 0.40026 |  0:05:04s
epoch 93 | loss: 0.17335 | val_0_rmse: 0.4051  | val_1_rmse: 0.40382 |  0:05:07s
epoch 94 | loss: 0.17817 | val_0_rmse: 0.40187 | val_1_rmse: 0.40144 |  0:05:10s
epoch 95 | loss: 0.17989 | val_0_rmse: 0.41654 | val_1_rmse: 0.41702 |  0:05:14s
epoch 96 | loss: 0.17889 | val_0_rmse: 0.41776 | val_1_rmse: 0.41742 |  0:05:17s
epoch 97 | loss: 0.17525 | val_0_rmse: 0.40442 | val_1_rmse: 0.4029  |  0:05:20s
epoch 98 | loss: 0.17722 | val_0_rmse: 0.42227 | val_1_rmse: 0.42092 |  0:05:23s
epoch 99 | loss: 0.18006 | val_0_rmse: 0.41606 | val_1_rmse: 0.41669 |  0:05:27s
epoch 100| loss: 0.17848 | val_0_rmse: 0.39894 | val_1_rmse: 0.39851 |  0:05:30s
epoch 101| loss: 0.17913 | val_0_rmse: 0.40994 | val_1_rmse: 0.40687 |  0:05:33s
epoch 102| loss: 0.179   | val_0_rmse: 0.40061 | val_1_rmse: 0.39834 |  0:05:36s
epoch 103| loss: 0.1758  | val_0_rmse: 0.41054 | val_1_rmse: 0.41007 |  0:05:40s
epoch 104| loss: 0.18018 | val_0_rmse: 0.4216  | val_1_rmse: 0.42282 |  0:05:43s
epoch 105| loss: 0.1801  | val_0_rmse: 0.41292 | val_1_rmse: 0.41133 |  0:05:46s
epoch 106| loss: 0.1772  | val_0_rmse: 0.40544 | val_1_rmse: 0.40604 |  0:05:49s
epoch 107| loss: 0.17385 | val_0_rmse: 0.39955 | val_1_rmse: 0.39844 |  0:05:53s
epoch 108| loss: 0.17949 | val_0_rmse: 0.40062 | val_1_rmse: 0.40029 |  0:05:56s
epoch 109| loss: 0.17404 | val_0_rmse: 0.41349 | val_1_rmse: 0.41381 |  0:05:59s
epoch 110| loss: 0.17477 | val_0_rmse: 0.39854 | val_1_rmse: 0.39874 |  0:06:02s
epoch 111| loss: 0.17767 | val_0_rmse: 0.40311 | val_1_rmse: 0.40562 |  0:06:06s
epoch 112| loss: 0.17494 | val_0_rmse: 0.40482 | val_1_rmse: 0.40618 |  0:06:09s
epoch 113| loss: 0.17484 | val_0_rmse: 0.39755 | val_1_rmse: 0.40054 |  0:06:12s
epoch 114| loss: 0.17512 | val_0_rmse: 0.40572 | val_1_rmse: 0.40464 |  0:06:15s
epoch 115| loss: 0.17479 | val_0_rmse: 0.40958 | val_1_rmse: 0.40825 |  0:06:18s
epoch 116| loss: 0.17748 | val_0_rmse: 0.40152 | val_1_rmse: 0.40096 |  0:06:22s
epoch 117| loss: 0.1768  | val_0_rmse: 0.41156 | val_1_rmse: 0.41043 |  0:06:25s
epoch 118| loss: 0.17528 | val_0_rmse: 0.40028 | val_1_rmse: 0.4016  |  0:06:28s
epoch 119| loss: 0.17433 | val_0_rmse: 0.4023  | val_1_rmse: 0.40266 |  0:06:31s
epoch 120| loss: 0.17007 | val_0_rmse: 0.41045 | val_1_rmse: 0.4123  |  0:06:35s
epoch 121| loss: 0.17563 | val_0_rmse: 0.40736 | val_1_rmse: 0.40813 |  0:06:38s
epoch 122| loss: 0.1753  | val_0_rmse: 0.39444 | val_1_rmse: 0.3972  |  0:06:41s
epoch 123| loss: 0.17144 | val_0_rmse: 0.40396 | val_1_rmse: 0.40293 |  0:06:44s
epoch 124| loss: 0.17317 | val_0_rmse: 0.39648 | val_1_rmse: 0.39743 |  0:06:48s
epoch 125| loss: 0.17422 | val_0_rmse: 0.40201 | val_1_rmse: 0.40202 |  0:06:51s
epoch 126| loss: 0.17439 | val_0_rmse: 0.40348 | val_1_rmse: 0.40623 |  0:06:54s
epoch 127| loss: 0.1745  | val_0_rmse: 0.40772 | val_1_rmse: 0.40798 |  0:06:57s
epoch 128| loss: 0.17492 | val_0_rmse: 0.40182 | val_1_rmse: 0.40417 |  0:07:00s
epoch 129| loss: 0.17288 | val_0_rmse: 0.44608 | val_1_rmse: 0.44978 |  0:07:04s
epoch 130| loss: 0.1766  | val_0_rmse: 0.40017 | val_1_rmse: 0.40113 |  0:07:07s
epoch 131| loss: 0.17458 | val_0_rmse: 0.40055 | val_1_rmse: 0.39915 |  0:07:10s
epoch 132| loss: 0.17445 | val_0_rmse: 0.40791 | val_1_rmse: 0.40628 |  0:07:13s
epoch 133| loss: 0.17709 | val_0_rmse: 0.41677 | val_1_rmse: 0.41541 |  0:07:17s
epoch 134| loss: 0.1864  | val_0_rmse: 0.4271  | val_1_rmse: 0.42587 |  0:07:20s
epoch 135| loss: 0.19389 | val_0_rmse: 0.41296 | val_1_rmse: 0.40775 |  0:07:23s
epoch 136| loss: 0.19001 | val_0_rmse: 0.40593 | val_1_rmse: 0.40392 |  0:07:26s
epoch 137| loss: 0.18892 | val_0_rmse: 0.41967 | val_1_rmse: 0.4165  |  0:07:30s
epoch 138| loss: 0.18348 | val_0_rmse: 0.40379 | val_1_rmse: 0.40404 |  0:07:33s
epoch 139| loss: 0.18406 | val_0_rmse: 0.40012 | val_1_rmse: 0.39942 |  0:07:36s
epoch 140| loss: 0.18325 | val_0_rmse: 0.41736 | val_1_rmse: 0.41313 |  0:07:39s
epoch 141| loss: 0.18907 | val_0_rmse: 0.43141 | val_1_rmse: 0.42604 |  0:07:43s
epoch 142| loss: 0.20215 | val_0_rmse: 0.52207 | val_1_rmse: 0.51423 |  0:07:46s
epoch 143| loss: 0.19372 | val_0_rmse: 0.42338 | val_1_rmse: 0.41956 |  0:07:49s
epoch 144| loss: 0.18641 | val_0_rmse: 0.41003 | val_1_rmse: 0.41055 |  0:07:52s
epoch 145| loss: 0.18137 | val_0_rmse: 0.44009 | val_1_rmse: 0.43553 |  0:07:56s
epoch 146| loss: 0.18289 | val_0_rmse: 0.41045 | val_1_rmse: 0.40826 |  0:07:59s
epoch 147| loss: 0.18205 | val_0_rmse: 0.40511 | val_1_rmse: 0.40382 |  0:08:02s
epoch 148| loss: 0.17847 | val_0_rmse: 0.42114 | val_1_rmse: 0.41717 |  0:08:05s
epoch 149| loss: 0.18083 | val_0_rmse: 0.4074  | val_1_rmse: 0.40834 |  0:08:08s
Stop training because you reached max_epochs = 150 with best_epoch = 122 and best_val_1_rmse = 0.3972
Best weights from best epoch are automatically used!
ended training at: 14:57:51
Feature importance:
[('Area', 0.018519778900932586), ('Baths', 0.13307494638247627), ('Beds', 0.0), ('Latitude', 0.20104544838093993), ('Longitude', 0.2676215140452663), ('Month', 0.1790537235521191), ('Year', 0.2006845887382658)]
Mean squared error is of 10079640721.920048
Mean absolute error:68226.18318851326
MAPE:0.2867349631451419
R2 score:0.8266281541284883
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 14:57:51
epoch 0  | loss: 0.44138 | val_0_rmse: 0.5583  | val_1_rmse: 0.56192 |  0:00:03s
epoch 1  | loss: 0.30218 | val_0_rmse: 0.51821 | val_1_rmse: 0.52303 |  0:00:06s
epoch 2  | loss: 0.28359 | val_0_rmse: 0.49915 | val_1_rmse: 0.50561 |  0:00:09s
epoch 3  | loss: 0.25982 | val_0_rmse: 0.48534 | val_1_rmse: 0.49021 |  0:00:13s
epoch 4  | loss: 0.25274 | val_0_rmse: 0.48217 | val_1_rmse: 0.48603 |  0:00:16s
epoch 5  | loss: 0.24499 | val_0_rmse: 0.46384 | val_1_rmse: 0.46814 |  0:00:19s
epoch 6  | loss: 0.23694 | val_0_rmse: 0.46777 | val_1_rmse: 0.47092 |  0:00:22s
epoch 7  | loss: 0.22921 | val_0_rmse: 0.4517  | val_1_rmse: 0.45513 |  0:00:26s
epoch 8  | loss: 0.22305 | val_0_rmse: 0.44935 | val_1_rmse: 0.4533  |  0:00:29s
epoch 9  | loss: 0.22509 | val_0_rmse: 0.45373 | val_1_rmse: 0.45576 |  0:00:32s
epoch 10 | loss: 0.2162  | val_0_rmse: 0.45521 | val_1_rmse: 0.45742 |  0:00:35s
epoch 11 | loss: 0.21805 | val_0_rmse: 0.44401 | val_1_rmse: 0.44874 |  0:00:39s
epoch 12 | loss: 0.20851 | val_0_rmse: 0.44671 | val_1_rmse: 0.45136 |  0:00:42s
epoch 13 | loss: 0.20982 | val_0_rmse: 0.43649 | val_1_rmse: 0.44266 |  0:00:45s
epoch 14 | loss: 0.21235 | val_0_rmse: 0.43529 | val_1_rmse: 0.44045 |  0:00:48s
epoch 15 | loss: 0.20697 | val_0_rmse: 0.46525 | val_1_rmse: 0.46743 |  0:00:52s
epoch 16 | loss: 0.21674 | val_0_rmse: 0.44569 | val_1_rmse: 0.45149 |  0:00:55s
epoch 17 | loss: 0.20649 | val_0_rmse: 0.43061 | val_1_rmse: 0.43722 |  0:00:58s
epoch 18 | loss: 0.20621 | val_0_rmse: 0.45496 | val_1_rmse: 0.46003 |  0:01:01s
epoch 19 | loss: 0.21079 | val_0_rmse: 0.42408 | val_1_rmse: 0.42903 |  0:01:05s
epoch 20 | loss: 0.2047  | val_0_rmse: 0.42742 | val_1_rmse: 0.43022 |  0:01:08s
epoch 21 | loss: 0.20641 | val_0_rmse: 0.43938 | val_1_rmse: 0.44599 |  0:01:11s
epoch 22 | loss: 0.20699 | val_0_rmse: 0.43951 | val_1_rmse: 0.44516 |  0:01:14s
epoch 23 | loss: 0.2014  | val_0_rmse: 0.43155 | val_1_rmse: 0.43574 |  0:01:18s
epoch 24 | loss: 0.20608 | val_0_rmse: 0.42357 | val_1_rmse: 0.42924 |  0:01:21s
epoch 25 | loss: 0.20057 | val_0_rmse: 0.43527 | val_1_rmse: 0.43964 |  0:01:24s
epoch 26 | loss: 0.20483 | val_0_rmse: 0.42961 | val_1_rmse: 0.43644 |  0:01:27s
epoch 27 | loss: 0.20309 | val_0_rmse: 0.42716 | val_1_rmse: 0.43631 |  0:01:31s
epoch 28 | loss: 0.19956 | val_0_rmse: 0.42842 | val_1_rmse: 0.43756 |  0:01:34s
epoch 29 | loss: 0.19774 | val_0_rmse: 0.43204 | val_1_rmse: 0.43734 |  0:01:37s
epoch 30 | loss: 0.19693 | val_0_rmse: 0.42345 | val_1_rmse: 0.43209 |  0:01:40s
epoch 31 | loss: 0.19805 | val_0_rmse: 0.42941 | val_1_rmse: 0.4359  |  0:01:44s
epoch 32 | loss: 0.19732 | val_0_rmse: 0.4286  | val_1_rmse: 0.4342  |  0:01:47s
epoch 33 | loss: 0.2063  | val_0_rmse: 0.43781 | val_1_rmse: 0.44566 |  0:01:50s
epoch 34 | loss: 0.1982  | val_0_rmse: 0.42763 | val_1_rmse: 0.43191 |  0:01:53s
epoch 35 | loss: 0.19788 | val_0_rmse: 0.42192 | val_1_rmse: 0.42902 |  0:01:57s
epoch 36 | loss: 0.19367 | val_0_rmse: 0.43218 | val_1_rmse: 0.43938 |  0:02:00s
epoch 37 | loss: 0.19867 | val_0_rmse: 0.42268 | val_1_rmse: 0.42894 |  0:02:03s
epoch 38 | loss: 0.19685 | val_0_rmse: 0.43335 | val_1_rmse: 0.43931 |  0:02:06s
epoch 39 | loss: 0.19539 | val_0_rmse: 0.42936 | val_1_rmse: 0.43784 |  0:02:10s
epoch 40 | loss: 0.19584 | val_0_rmse: 0.42841 | val_1_rmse: 0.43296 |  0:02:13s
epoch 41 | loss: 0.19392 | val_0_rmse: 0.42471 | val_1_rmse: 0.43162 |  0:02:16s
epoch 42 | loss: 0.19464 | val_0_rmse: 0.4458  | val_1_rmse: 0.45084 |  0:02:19s
epoch 43 | loss: 0.19325 | val_0_rmse: 0.42522 | val_1_rmse: 0.43071 |  0:02:22s
epoch 44 | loss: 0.1933  | val_0_rmse: 0.42775 | val_1_rmse: 0.43448 |  0:02:26s
epoch 45 | loss: 0.19625 | val_0_rmse: 0.42181 | val_1_rmse: 0.42708 |  0:02:29s
epoch 46 | loss: 0.18884 | val_0_rmse: 0.41233 | val_1_rmse: 0.4185  |  0:02:32s
epoch 47 | loss: 0.1906  | val_0_rmse: 0.42185 | val_1_rmse: 0.43246 |  0:02:35s
epoch 48 | loss: 0.19358 | val_0_rmse: 0.4334  | val_1_rmse: 0.44236 |  0:02:39s
epoch 49 | loss: 0.19426 | val_0_rmse: 0.43483 | val_1_rmse: 0.44061 |  0:02:42s
epoch 50 | loss: 0.19316 | val_0_rmse: 0.42549 | val_1_rmse: 0.43079 |  0:02:45s
epoch 51 | loss: 0.19394 | val_0_rmse: 0.45598 | val_1_rmse: 0.46442 |  0:02:49s
epoch 52 | loss: 0.18758 | val_0_rmse: 0.41823 | val_1_rmse: 0.42594 |  0:02:52s
epoch 53 | loss: 0.18897 | val_0_rmse: 0.41794 | val_1_rmse: 0.42594 |  0:02:55s
epoch 54 | loss: 0.19431 | val_0_rmse: 0.42214 | val_1_rmse: 0.42991 |  0:02:58s
epoch 55 | loss: 0.19158 | val_0_rmse: 0.41753 | val_1_rmse: 0.42243 |  0:03:02s
epoch 56 | loss: 0.19068 | val_0_rmse: 0.41351 | val_1_rmse: 0.42151 |  0:03:05s
epoch 57 | loss: 0.18993 | val_0_rmse: 0.44621 | val_1_rmse: 0.45222 |  0:03:08s
epoch 58 | loss: 0.19003 | val_0_rmse: 0.43201 | val_1_rmse: 0.44186 |  0:03:11s
epoch 59 | loss: 0.19226 | val_0_rmse: 0.42606 | val_1_rmse: 0.43318 |  0:03:15s
epoch 60 | loss: 0.18993 | val_0_rmse: 0.41572 | val_1_rmse: 0.42395 |  0:03:18s
epoch 61 | loss: 0.18936 | val_0_rmse: 0.44115 | val_1_rmse: 0.44789 |  0:03:21s
epoch 62 | loss: 0.18815 | val_0_rmse: 0.41446 | val_1_rmse: 0.42224 |  0:03:24s
epoch 63 | loss: 0.18486 | val_0_rmse: 0.42555 | val_1_rmse: 0.43368 |  0:03:28s
epoch 64 | loss: 0.19098 | val_0_rmse: 0.43115 | val_1_rmse: 0.44009 |  0:03:31s
epoch 65 | loss: 0.1874  | val_0_rmse: 0.42654 | val_1_rmse: 0.43418 |  0:03:34s
epoch 66 | loss: 0.19248 | val_0_rmse: 0.4184  | val_1_rmse: 0.42481 |  0:03:37s
epoch 67 | loss: 0.18741 | val_0_rmse: 0.41741 | val_1_rmse: 0.42802 |  0:03:41s
epoch 68 | loss: 0.18513 | val_0_rmse: 0.42635 | val_1_rmse: 0.43124 |  0:03:44s
epoch 69 | loss: 0.1863  | val_0_rmse: 0.41648 | val_1_rmse: 0.42397 |  0:03:47s
epoch 70 | loss: 0.18681 | val_0_rmse: 0.42137 | val_1_rmse: 0.43055 |  0:03:50s
epoch 71 | loss: 0.19276 | val_0_rmse: 0.42397 | val_1_rmse: 0.43091 |  0:03:54s
epoch 72 | loss: 0.19201 | val_0_rmse: 0.4289  | val_1_rmse: 0.43656 |  0:03:57s
epoch 73 | loss: 0.1925  | val_0_rmse: 0.42619 | val_1_rmse: 0.43369 |  0:04:00s
epoch 74 | loss: 0.19246 | val_0_rmse: 0.41601 | val_1_rmse: 0.42316 |  0:04:03s
epoch 75 | loss: 0.18447 | val_0_rmse: 0.40611 | val_1_rmse: 0.41545 |  0:04:07s
epoch 76 | loss: 0.18449 | val_0_rmse: 0.40466 | val_1_rmse: 0.41376 |  0:04:10s
epoch 77 | loss: 0.18506 | val_0_rmse: 0.41436 | val_1_rmse: 0.42362 |  0:04:13s
epoch 78 | loss: 0.18464 | val_0_rmse: 0.44547 | val_1_rmse: 0.45725 |  0:04:16s
epoch 79 | loss: 0.18392 | val_0_rmse: 0.41079 | val_1_rmse: 0.41928 |  0:04:20s
epoch 80 | loss: 0.18427 | val_0_rmse: 0.41718 | val_1_rmse: 0.42434 |  0:04:23s
epoch 81 | loss: 0.18484 | val_0_rmse: 0.41717 | val_1_rmse: 0.42432 |  0:04:26s
epoch 82 | loss: 0.18893 | val_0_rmse: 0.41319 | val_1_rmse: 0.4226  |  0:04:29s
epoch 83 | loss: 0.18569 | val_0_rmse: 0.41301 | val_1_rmse: 0.42497 |  0:04:33s
epoch 84 | loss: 0.1895  | val_0_rmse: 0.41971 | val_1_rmse: 0.42844 |  0:04:36s
epoch 85 | loss: 0.19107 | val_0_rmse: 0.41699 | val_1_rmse: 0.42541 |  0:04:39s
epoch 86 | loss: 0.18712 | val_0_rmse: 0.4155  | val_1_rmse: 0.42482 |  0:04:42s
epoch 87 | loss: 0.18688 | val_0_rmse: 0.4133  | val_1_rmse: 0.4229  |  0:04:46s
epoch 88 | loss: 0.18073 | val_0_rmse: 0.40685 | val_1_rmse: 0.41915 |  0:04:49s
epoch 89 | loss: 0.18171 | val_0_rmse: 0.40771 | val_1_rmse: 0.42054 |  0:04:52s
epoch 90 | loss: 0.18249 | val_0_rmse: 0.41932 | val_1_rmse: 0.42862 |  0:04:55s
epoch 91 | loss: 0.18172 | val_0_rmse: 0.41654 | val_1_rmse: 0.42553 |  0:04:58s
epoch 92 | loss: 0.18553 | val_0_rmse: 0.41714 | val_1_rmse: 0.42694 |  0:05:02s
epoch 93 | loss: 0.18511 | val_0_rmse: 0.43009 | val_1_rmse: 0.44084 |  0:05:05s
epoch 94 | loss: 0.18547 | val_0_rmse: 0.41444 | val_1_rmse: 0.42321 |  0:05:08s
epoch 95 | loss: 0.18647 | val_0_rmse: 0.44937 | val_1_rmse: 0.45998 |  0:05:12s
epoch 96 | loss: 0.18811 | val_0_rmse: 0.41518 | val_1_rmse: 0.42125 |  0:05:15s
epoch 97 | loss: 0.18602 | val_0_rmse: 0.42647 | val_1_rmse: 0.43553 |  0:05:18s
epoch 98 | loss: 0.18347 | val_0_rmse: 0.42216 | val_1_rmse: 0.43144 |  0:05:21s
epoch 99 | loss: 0.18627 | val_0_rmse: 0.4292  | val_1_rmse: 0.43865 |  0:05:24s
epoch 100| loss: 0.18263 | val_0_rmse: 0.4189  | val_1_rmse: 0.43179 |  0:05:28s
epoch 101| loss: 0.18383 | val_0_rmse: 0.41038 | val_1_rmse: 0.42142 |  0:05:31s
epoch 102| loss: 0.18255 | val_0_rmse: 0.42886 | val_1_rmse: 0.43929 |  0:05:34s
epoch 103| loss: 0.18572 | val_0_rmse: 0.41057 | val_1_rmse: 0.42039 |  0:05:37s
epoch 104| loss: 0.18773 | val_0_rmse: 0.45321 | val_1_rmse: 0.46367 |  0:05:41s
epoch 105| loss: 0.18346 | val_0_rmse: 0.40703 | val_1_rmse: 0.41872 |  0:05:44s
epoch 106| loss: 0.18223 | val_0_rmse: 0.42506 | val_1_rmse: 0.43665 |  0:05:47s

Early stopping occured at epoch 106 with best_epoch = 76 and best_val_1_rmse = 0.41376
Best weights from best epoch are automatically used!
ended training at: 15:03:40
Feature importance:
[('Area', 0.020481172224118827), ('Baths', 0.09358623974257661), ('Beds', 0.12987317350290126), ('Latitude', 0.2678965576690879), ('Longitude', 0.18290498067540656), ('Month', 0.009755108928247947), ('Year', 0.2955027672576609)]
Mean squared error is of 10265617009.295702
Mean absolute error:69323.92366514157
MAPE:0.29422136088713874
R2 score:0.8248215731014599
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:03:41
epoch 0  | loss: 0.42308 | val_0_rmse: 0.57172 | val_1_rmse: 0.57927 |  0:00:03s
epoch 1  | loss: 0.27165 | val_0_rmse: 0.49892 | val_1_rmse: 0.50247 |  0:00:06s
epoch 2  | loss: 0.25056 | val_0_rmse: 0.46311 | val_1_rmse: 0.46796 |  0:00:09s
epoch 3  | loss: 0.23671 | val_0_rmse: 0.45821 | val_1_rmse: 0.46127 |  0:00:12s
epoch 4  | loss: 0.235   | val_0_rmse: 0.46131 | val_1_rmse: 0.46595 |  0:00:16s
epoch 5  | loss: 0.2294  | val_0_rmse: 0.48641 | val_1_rmse: 0.4913  |  0:00:19s
epoch 6  | loss: 0.22746 | val_0_rmse: 0.45037 | val_1_rmse: 0.45523 |  0:00:22s
epoch 7  | loss: 0.219   | val_0_rmse: 0.44148 | val_1_rmse: 0.44963 |  0:00:26s
epoch 8  | loss: 0.21659 | val_0_rmse: 0.44732 | val_1_rmse: 0.45286 |  0:00:29s
epoch 9  | loss: 0.21111 | val_0_rmse: 0.43944 | val_1_rmse: 0.44563 |  0:00:32s
epoch 10 | loss: 0.21626 | val_0_rmse: 0.46163 | val_1_rmse: 0.46459 |  0:00:35s
epoch 11 | loss: 0.2128  | val_0_rmse: 0.43318 | val_1_rmse: 0.43961 |  0:00:38s
epoch 12 | loss: 0.21158 | val_0_rmse: 0.45851 | val_1_rmse: 0.46226 |  0:00:42s
epoch 13 | loss: 0.21017 | val_0_rmse: 0.47293 | val_1_rmse: 0.47913 |  0:00:45s
epoch 14 | loss: 0.20727 | val_0_rmse: 0.43909 | val_1_rmse: 0.44598 |  0:00:48s
epoch 15 | loss: 0.20563 | val_0_rmse: 0.42875 | val_1_rmse: 0.43617 |  0:00:51s
epoch 16 | loss: 0.20143 | val_0_rmse: 0.43823 | val_1_rmse: 0.44497 |  0:00:55s
epoch 17 | loss: 0.20498 | val_0_rmse: 0.4329  | val_1_rmse: 0.44133 |  0:00:58s
epoch 18 | loss: 0.20118 | val_0_rmse: 0.42888 | val_1_rmse: 0.43472 |  0:01:01s
epoch 19 | loss: 0.20589 | val_0_rmse: 0.43275 | val_1_rmse: 0.43658 |  0:01:04s
epoch 20 | loss: 0.20291 | val_0_rmse: 0.44095 | val_1_rmse: 0.44592 |  0:01:08s
epoch 21 | loss: 0.20102 | val_0_rmse: 0.43408 | val_1_rmse: 0.44054 |  0:01:11s
epoch 22 | loss: 0.20078 | val_0_rmse: 0.42246 | val_1_rmse: 0.42802 |  0:01:14s
epoch 23 | loss: 0.20308 | val_0_rmse: 0.42547 | val_1_rmse: 0.43364 |  0:01:17s
epoch 24 | loss: 0.19749 | val_0_rmse: 0.42817 | val_1_rmse: 0.4362  |  0:01:21s
epoch 25 | loss: 0.19962 | val_0_rmse: 0.42702 | val_1_rmse: 0.43303 |  0:01:24s
epoch 26 | loss: 0.19609 | val_0_rmse: 0.43467 | val_1_rmse: 0.44474 |  0:01:27s
epoch 27 | loss: 0.19977 | val_0_rmse: 0.42126 | val_1_rmse: 0.43114 |  0:01:30s
epoch 28 | loss: 0.19548 | val_0_rmse: 0.4243  | val_1_rmse: 0.43057 |  0:01:34s
epoch 29 | loss: 0.19683 | val_0_rmse: 0.43152 | val_1_rmse: 0.43918 |  0:01:37s
epoch 30 | loss: 0.19442 | val_0_rmse: 0.42994 | val_1_rmse: 0.43724 |  0:01:40s
epoch 31 | loss: 0.19611 | val_0_rmse: 0.43226 | val_1_rmse: 0.4389  |  0:01:43s
epoch 32 | loss: 0.1929  | val_0_rmse: 0.43019 | val_1_rmse: 0.43835 |  0:01:47s
epoch 33 | loss: 0.19442 | val_0_rmse: 0.42056 | val_1_rmse: 0.42906 |  0:01:50s
epoch 34 | loss: 0.19048 | val_0_rmse: 0.42003 | val_1_rmse: 0.42778 |  0:01:53s
epoch 35 | loss: 0.192   | val_0_rmse: 0.43095 | val_1_rmse: 0.44056 |  0:01:56s
epoch 36 | loss: 0.19385 | val_0_rmse: 0.41479 | val_1_rmse: 0.42254 |  0:02:00s
epoch 37 | loss: 0.19351 | val_0_rmse: 0.41268 | val_1_rmse: 0.42163 |  0:02:03s
epoch 38 | loss: 0.19091 | val_0_rmse: 0.40974 | val_1_rmse: 0.41954 |  0:02:06s
epoch 39 | loss: 0.19111 | val_0_rmse: 0.43238 | val_1_rmse: 0.44304 |  0:02:09s
epoch 40 | loss: 0.19166 | val_0_rmse: 0.42866 | val_1_rmse: 0.43811 |  0:02:13s
epoch 41 | loss: 0.18928 | val_0_rmse: 0.41826 | val_1_rmse: 0.42702 |  0:02:16s
epoch 42 | loss: 0.19107 | val_0_rmse: 0.41776 | val_1_rmse: 0.42612 |  0:02:19s
epoch 43 | loss: 0.19516 | val_0_rmse: 0.42302 | val_1_rmse: 0.43048 |  0:02:22s
epoch 44 | loss: 0.19125 | val_0_rmse: 0.41892 | val_1_rmse: 0.42808 |  0:02:26s
epoch 45 | loss: 0.18991 | val_0_rmse: 0.41995 | val_1_rmse: 0.43022 |  0:02:29s
epoch 46 | loss: 0.18949 | val_0_rmse: 0.41904 | val_1_rmse: 0.4292  |  0:02:32s
epoch 47 | loss: 0.18873 | val_0_rmse: 0.43251 | val_1_rmse: 0.442   |  0:02:35s
epoch 48 | loss: 0.18901 | val_0_rmse: 0.41207 | val_1_rmse: 0.4234  |  0:02:39s
epoch 49 | loss: 0.18616 | val_0_rmse: 0.40817 | val_1_rmse: 0.41835 |  0:02:42s
epoch 50 | loss: 0.19019 | val_0_rmse: 0.42133 | val_1_rmse: 0.4311  |  0:02:45s
epoch 51 | loss: 0.18695 | val_0_rmse: 0.40999 | val_1_rmse: 0.42314 |  0:02:49s
epoch 52 | loss: 0.18957 | val_0_rmse: 0.45148 | val_1_rmse: 0.4581  |  0:02:52s
epoch 53 | loss: 0.18942 | val_0_rmse: 0.41411 | val_1_rmse: 0.42531 |  0:02:55s
epoch 54 | loss: 0.18627 | val_0_rmse: 0.41651 | val_1_rmse: 0.4272  |  0:02:58s
epoch 55 | loss: 0.18783 | val_0_rmse: 0.42347 | val_1_rmse: 0.43135 |  0:03:01s
epoch 56 | loss: 0.18559 | val_0_rmse: 0.41034 | val_1_rmse: 0.42034 |  0:03:05s
epoch 57 | loss: 0.18573 | val_0_rmse: 0.41013 | val_1_rmse: 0.42072 |  0:03:08s
epoch 58 | loss: 0.18569 | val_0_rmse: 0.41014 | val_1_rmse: 0.42199 |  0:03:11s
epoch 59 | loss: 0.18779 | val_0_rmse: 0.41454 | val_1_rmse: 0.42825 |  0:03:15s
epoch 60 | loss: 0.18492 | val_0_rmse: 0.41177 | val_1_rmse: 0.4255  |  0:03:18s
epoch 61 | loss: 0.18524 | val_0_rmse: 0.4078  | val_1_rmse: 0.41777 |  0:03:21s
epoch 62 | loss: 0.1835  | val_0_rmse: 0.41217 | val_1_rmse: 0.42392 |  0:03:24s
epoch 63 | loss: 0.18447 | val_0_rmse: 0.4053  | val_1_rmse: 0.41625 |  0:03:27s
epoch 64 | loss: 0.18434 | val_0_rmse: 0.40814 | val_1_rmse: 0.41964 |  0:03:31s
epoch 65 | loss: 0.18411 | val_0_rmse: 0.41862 | val_1_rmse: 0.42997 |  0:03:34s
epoch 66 | loss: 0.18373 | val_0_rmse: 0.40508 | val_1_rmse: 0.41672 |  0:03:37s
epoch 67 | loss: 0.18224 | val_0_rmse: 0.41926 | val_1_rmse: 0.43213 |  0:03:40s
epoch 68 | loss: 0.18154 | val_0_rmse: 0.4171  | val_1_rmse: 0.4316  |  0:03:44s
epoch 69 | loss: 0.18315 | val_0_rmse: 0.40968 | val_1_rmse: 0.42183 |  0:03:47s
epoch 70 | loss: 0.18566 | val_0_rmse: 0.41412 | val_1_rmse: 0.42537 |  0:03:50s
epoch 71 | loss: 0.18608 | val_0_rmse: 0.41346 | val_1_rmse: 0.42709 |  0:03:53s
epoch 72 | loss: 0.18324 | val_0_rmse: 0.40853 | val_1_rmse: 0.42169 |  0:03:57s
epoch 73 | loss: 0.18513 | val_0_rmse: 0.41545 | val_1_rmse: 0.42611 |  0:04:00s
epoch 74 | loss: 0.18014 | val_0_rmse: 0.41899 | val_1_rmse: 0.43151 |  0:04:03s
epoch 75 | loss: 0.17962 | val_0_rmse: 0.41226 | val_1_rmse: 0.42415 |  0:04:06s
epoch 76 | loss: 0.18753 | val_0_rmse: 0.44257 | val_1_rmse: 0.45339 |  0:04:10s
epoch 77 | loss: 0.18408 | val_0_rmse: 0.4085  | val_1_rmse: 0.42093 |  0:04:13s
epoch 78 | loss: 0.18278 | val_0_rmse: 0.40352 | val_1_rmse: 0.41586 |  0:04:16s
epoch 79 | loss: 0.18262 | val_0_rmse: 0.4049  | val_1_rmse: 0.41815 |  0:04:19s
epoch 80 | loss: 0.18324 | val_0_rmse: 0.40836 | val_1_rmse: 0.41824 |  0:04:23s
epoch 81 | loss: 0.18256 | val_0_rmse: 0.46045 | val_1_rmse: 0.47248 |  0:04:26s
epoch 82 | loss: 0.18076 | val_0_rmse: 0.39941 | val_1_rmse: 0.41214 |  0:04:29s
epoch 83 | loss: 0.18081 | val_0_rmse: 0.40196 | val_1_rmse: 0.41613 |  0:04:32s
epoch 84 | loss: 0.17933 | val_0_rmse: 0.40159 | val_1_rmse: 0.41469 |  0:04:36s
epoch 85 | loss: 0.18083 | val_0_rmse: 0.41082 | val_1_rmse: 0.42552 |  0:04:39s
epoch 86 | loss: 0.18133 | val_0_rmse: 0.40817 | val_1_rmse: 0.42009 |  0:04:42s
epoch 87 | loss: 0.17724 | val_0_rmse: 0.4076  | val_1_rmse: 0.42082 |  0:04:45s
epoch 88 | loss: 0.17794 | val_0_rmse: 0.40549 | val_1_rmse: 0.42167 |  0:04:49s
epoch 89 | loss: 0.18027 | val_0_rmse: 0.40256 | val_1_rmse: 0.41571 |  0:04:52s
epoch 90 | loss: 0.17922 | val_0_rmse: 0.40867 | val_1_rmse: 0.42311 |  0:04:55s
epoch 91 | loss: 0.17974 | val_0_rmse: 0.42155 | val_1_rmse: 0.43557 |  0:04:58s
epoch 92 | loss: 0.17972 | val_0_rmse: 0.40311 | val_1_rmse: 0.41613 |  0:05:02s
epoch 93 | loss: 0.18085 | val_0_rmse: 0.41249 | val_1_rmse: 0.42789 |  0:05:05s
epoch 94 | loss: 0.18266 | val_0_rmse: 0.40169 | val_1_rmse: 0.41616 |  0:05:08s
epoch 95 | loss: 0.1776  | val_0_rmse: 0.40442 | val_1_rmse: 0.41864 |  0:05:11s
epoch 96 | loss: 0.17963 | val_0_rmse: 0.40088 | val_1_rmse: 0.41729 |  0:05:15s
epoch 97 | loss: 0.18412 | val_0_rmse: 0.40327 | val_1_rmse: 0.41683 |  0:05:18s
epoch 98 | loss: 0.17443 | val_0_rmse: 0.40638 | val_1_rmse: 0.41921 |  0:05:21s
epoch 99 | loss: 0.17547 | val_0_rmse: 0.41767 | val_1_rmse: 0.4294  |  0:05:24s
epoch 100| loss: 0.17756 | val_0_rmse: 0.40215 | val_1_rmse: 0.41468 |  0:05:28s
epoch 101| loss: 0.17774 | val_0_rmse: 0.42265 | val_1_rmse: 0.43592 |  0:05:31s
epoch 102| loss: 0.17585 | val_0_rmse: 0.40021 | val_1_rmse: 0.41732 |  0:05:34s
epoch 103| loss: 0.17896 | val_0_rmse: 0.40524 | val_1_rmse: 0.41966 |  0:05:37s
epoch 104| loss: 0.17598 | val_0_rmse: 0.42305 | val_1_rmse: 0.43687 |  0:05:41s
epoch 105| loss: 0.17637 | val_0_rmse: 0.40508 | val_1_rmse: 0.42221 |  0:05:44s
epoch 106| loss: 0.17485 | val_0_rmse: 0.40395 | val_1_rmse: 0.41996 |  0:05:47s
epoch 107| loss: 0.17652 | val_0_rmse: 0.42354 | val_1_rmse: 0.43651 |  0:05:50s
epoch 108| loss: 0.17793 | val_0_rmse: 0.41382 | val_1_rmse: 0.42876 |  0:05:54s
epoch 109| loss: 0.18578 | val_0_rmse: 0.40807 | val_1_rmse: 0.41835 |  0:05:57s
epoch 110| loss: 0.17963 | val_0_rmse: 0.40748 | val_1_rmse: 0.42279 |  0:06:00s
epoch 111| loss: 0.1803  | val_0_rmse: 0.4023  | val_1_rmse: 0.41786 |  0:06:03s
epoch 112| loss: 0.17363 | val_0_rmse: 0.41528 | val_1_rmse: 0.42872 |  0:06:06s

Early stopping occured at epoch 112 with best_epoch = 82 and best_val_1_rmse = 0.41214
Best weights from best epoch are automatically used!
ended training at: 15:09:49
Feature importance:
[('Area', 0.03579372893346976), ('Baths', 0.26813279401845097), ('Beds', 0.0668211026357501), ('Latitude', 0.12078777129545713), ('Longitude', 0.18953889069022162), ('Month', 0.0435060102881574), ('Year', 0.27541970213849304)]
Mean squared error is of 9785118443.534613
Mean absolute error:68647.8984169857
MAPE:0.2807613115362552
R2 score:0.8315938521644345
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:09:50
epoch 0  | loss: 0.41215 | val_0_rmse: 0.52178 | val_1_rmse: 0.52927 |  0:00:03s
epoch 1  | loss: 0.27148 | val_0_rmse: 0.48985 | val_1_rmse: 0.49247 |  0:00:06s
epoch 2  | loss: 0.25067 | val_0_rmse: 0.48445 | val_1_rmse: 0.48924 |  0:00:09s
epoch 3  | loss: 0.24768 | val_0_rmse: 0.47288 | val_1_rmse: 0.47772 |  0:00:13s
epoch 4  | loss: 0.24283 | val_0_rmse: 0.46878 | val_1_rmse: 0.47245 |  0:00:16s
epoch 5  | loss: 0.23459 | val_0_rmse: 0.47926 | val_1_rmse: 0.4835  |  0:00:19s
epoch 6  | loss: 0.22997 | val_0_rmse: 0.45609 | val_1_rmse: 0.46179 |  0:00:22s
epoch 7  | loss: 0.2221  | val_0_rmse: 0.44869 | val_1_rmse: 0.45393 |  0:00:26s
epoch 8  | loss: 0.2199  | val_0_rmse: 0.4469  | val_1_rmse: 0.45113 |  0:00:29s
epoch 9  | loss: 0.21427 | val_0_rmse: 0.44867 | val_1_rmse: 0.45422 |  0:00:32s
epoch 10 | loss: 0.21407 | val_0_rmse: 0.43768 | val_1_rmse: 0.44501 |  0:00:35s
epoch 11 | loss: 0.21575 | val_0_rmse: 0.4659  | val_1_rmse: 0.47286 |  0:00:39s
epoch 12 | loss: 0.20943 | val_0_rmse: 0.44981 | val_1_rmse: 0.45547 |  0:00:42s
epoch 13 | loss: 0.2058  | val_0_rmse: 0.43489 | val_1_rmse: 0.43897 |  0:00:45s
epoch 14 | loss: 0.2065  | val_0_rmse: 0.43474 | val_1_rmse: 0.43993 |  0:00:48s
epoch 15 | loss: 0.2082  | val_0_rmse: 0.4375  | val_1_rmse: 0.44228 |  0:00:51s
epoch 16 | loss: 0.20926 | val_0_rmse: 0.44134 | val_1_rmse: 0.44548 |  0:00:55s
epoch 17 | loss: 0.20732 | val_0_rmse: 0.42815 | val_1_rmse: 0.43329 |  0:00:58s
epoch 18 | loss: 0.20765 | val_0_rmse: 0.44924 | val_1_rmse: 0.453   |  0:01:01s
epoch 19 | loss: 0.20208 | val_0_rmse: 0.43407 | val_1_rmse: 0.43891 |  0:01:04s
epoch 20 | loss: 0.2003  | val_0_rmse: 0.44547 | val_1_rmse: 0.45185 |  0:01:08s
epoch 21 | loss: 0.20012 | val_0_rmse: 0.42742 | val_1_rmse: 0.43466 |  0:01:11s
epoch 22 | loss: 0.2042  | val_0_rmse: 0.43162 | val_1_rmse: 0.43947 |  0:01:14s
epoch 23 | loss: 0.20106 | val_0_rmse: 0.42186 | val_1_rmse: 0.42973 |  0:01:17s
epoch 24 | loss: 0.20305 | val_0_rmse: 0.42902 | val_1_rmse: 0.43655 |  0:01:21s
epoch 25 | loss: 0.20002 | val_0_rmse: 0.43427 | val_1_rmse: 0.44102 |  0:01:24s
epoch 26 | loss: 0.19554 | val_0_rmse: 0.42419 | val_1_rmse: 0.43157 |  0:01:27s
epoch 27 | loss: 0.20154 | val_0_rmse: 0.4369  | val_1_rmse: 0.44434 |  0:01:30s
epoch 28 | loss: 0.20072 | val_0_rmse: 0.43758 | val_1_rmse: 0.44409 |  0:01:34s
epoch 29 | loss: 0.19759 | val_0_rmse: 0.4248  | val_1_rmse: 0.43246 |  0:01:37s
epoch 30 | loss: 0.19973 | val_0_rmse: 0.42308 | val_1_rmse: 0.42908 |  0:01:40s
epoch 31 | loss: 0.19637 | val_0_rmse: 0.42752 | val_1_rmse: 0.43502 |  0:01:43s
epoch 32 | loss: 0.20155 | val_0_rmse: 0.43256 | val_1_rmse: 0.44062 |  0:01:47s
epoch 33 | loss: 0.19482 | val_0_rmse: 0.4377  | val_1_rmse: 0.44553 |  0:01:50s
epoch 34 | loss: 0.19419 | val_0_rmse: 0.42675 | val_1_rmse: 0.43414 |  0:01:53s
epoch 35 | loss: 0.19584 | val_0_rmse: 0.4473  | val_1_rmse: 0.45351 |  0:01:56s
epoch 36 | loss: 0.19553 | val_0_rmse: 0.42049 | val_1_rmse: 0.42752 |  0:02:00s
epoch 37 | loss: 0.19293 | val_0_rmse: 0.42118 | val_1_rmse: 0.43022 |  0:02:03s
epoch 38 | loss: 0.19425 | val_0_rmse: 0.42856 | val_1_rmse: 0.4361  |  0:02:06s
epoch 39 | loss: 0.19476 | val_0_rmse: 0.42499 | val_1_rmse: 0.43377 |  0:02:09s
epoch 40 | loss: 0.19169 | val_0_rmse: 0.41933 | val_1_rmse: 0.42876 |  0:02:12s
epoch 41 | loss: 0.19221 | val_0_rmse: 0.41764 | val_1_rmse: 0.42672 |  0:02:16s
epoch 42 | loss: 0.19082 | val_0_rmse: 0.42738 | val_1_rmse: 0.43569 |  0:02:19s
epoch 43 | loss: 0.1917  | val_0_rmse: 0.42849 | val_1_rmse: 0.43719 |  0:02:22s
epoch 44 | loss: 0.20096 | val_0_rmse: 0.42753 | val_1_rmse: 0.43454 |  0:02:25s
epoch 45 | loss: 0.19639 | val_0_rmse: 0.43659 | val_1_rmse: 0.44451 |  0:02:29s
epoch 46 | loss: 0.19619 | val_0_rmse: 0.42787 | val_1_rmse: 0.43544 |  0:02:32s
epoch 47 | loss: 0.19413 | val_0_rmse: 0.42132 | val_1_rmse: 0.43103 |  0:02:35s
epoch 48 | loss: 0.19301 | val_0_rmse: 0.42549 | val_1_rmse: 0.43471 |  0:02:38s
epoch 49 | loss: 0.18837 | val_0_rmse: 0.41864 | val_1_rmse: 0.42859 |  0:02:42s
epoch 50 | loss: 0.1891  | val_0_rmse: 0.42816 | val_1_rmse: 0.43588 |  0:02:45s
epoch 51 | loss: 0.18917 | val_0_rmse: 0.44306 | val_1_rmse: 0.45091 |  0:02:48s
epoch 52 | loss: 0.19017 | val_0_rmse: 0.43761 | val_1_rmse: 0.44672 |  0:02:51s
epoch 53 | loss: 0.19738 | val_0_rmse: 0.45758 | val_1_rmse: 0.46578 |  0:02:55s
epoch 54 | loss: 0.19588 | val_0_rmse: 0.41409 | val_1_rmse: 0.42392 |  0:02:58s
epoch 55 | loss: 0.18994 | val_0_rmse: 0.42095 | val_1_rmse: 0.4293  |  0:03:01s
epoch 56 | loss: 0.18948 | val_0_rmse: 0.42388 | val_1_rmse: 0.43419 |  0:03:04s
epoch 57 | loss: 0.19148 | val_0_rmse: 0.42638 | val_1_rmse: 0.43566 |  0:03:08s
epoch 58 | loss: 0.19048 | val_0_rmse: 0.41275 | val_1_rmse: 0.42149 |  0:03:11s
epoch 59 | loss: 0.1841  | val_0_rmse: 0.42203 | val_1_rmse: 0.43123 |  0:03:14s
epoch 60 | loss: 0.18748 | val_0_rmse: 0.40845 | val_1_rmse: 0.41642 |  0:03:17s
epoch 61 | loss: 0.18659 | val_0_rmse: 0.43212 | val_1_rmse: 0.44333 |  0:03:21s
epoch 62 | loss: 0.19793 | val_0_rmse: 0.44372 | val_1_rmse: 0.44822 |  0:03:24s
epoch 63 | loss: 0.19136 | val_0_rmse: 0.41699 | val_1_rmse: 0.42625 |  0:03:27s
epoch 64 | loss: 0.18932 | val_0_rmse: 0.41595 | val_1_rmse: 0.42662 |  0:03:30s
epoch 65 | loss: 0.18638 | val_0_rmse: 0.4106  | val_1_rmse: 0.42115 |  0:03:33s
epoch 66 | loss: 0.18696 | val_0_rmse: 0.41422 | val_1_rmse: 0.42376 |  0:03:37s
epoch 67 | loss: 0.18391 | val_0_rmse: 0.42151 | val_1_rmse: 0.43181 |  0:03:40s
epoch 68 | loss: 0.1849  | val_0_rmse: 0.42144 | val_1_rmse: 0.43264 |  0:03:43s
epoch 69 | loss: 0.19218 | val_0_rmse: 0.41492 | val_1_rmse: 0.42491 |  0:03:46s
epoch 70 | loss: 0.18642 | val_0_rmse: 0.41506 | val_1_rmse: 0.4232  |  0:03:50s
epoch 71 | loss: 0.18452 | val_0_rmse: 0.41211 | val_1_rmse: 0.4211  |  0:03:53s
epoch 72 | loss: 0.18098 | val_0_rmse: 0.41988 | val_1_rmse: 0.42827 |  0:03:56s
epoch 73 | loss: 0.18124 | val_0_rmse: 0.41369 | val_1_rmse: 0.42652 |  0:03:59s
epoch 74 | loss: 0.18302 | val_0_rmse: 0.4237  | val_1_rmse: 0.43394 |  0:04:03s
epoch 75 | loss: 0.18179 | val_0_rmse: 0.41083 | val_1_rmse: 0.42223 |  0:04:06s
epoch 76 | loss: 0.18144 | val_0_rmse: 0.40655 | val_1_rmse: 0.41993 |  0:04:09s
epoch 77 | loss: 0.18107 | val_0_rmse: 0.41752 | val_1_rmse: 0.42677 |  0:04:12s
epoch 78 | loss: 0.18305 | val_0_rmse: 0.41898 | val_1_rmse: 0.43206 |  0:04:15s
epoch 79 | loss: 0.1814  | val_0_rmse: 0.41574 | val_1_rmse: 0.42688 |  0:04:19s
epoch 80 | loss: 0.18352 | val_0_rmse: 0.41255 | val_1_rmse: 0.42519 |  0:04:22s
epoch 81 | loss: 0.17829 | val_0_rmse: 0.4086  | val_1_rmse: 0.42061 |  0:04:25s
epoch 82 | loss: 0.18258 | val_0_rmse: 0.41979 | val_1_rmse: 0.43147 |  0:04:28s
epoch 83 | loss: 0.17964 | val_0_rmse: 0.41656 | val_1_rmse: 0.42537 |  0:04:32s
epoch 84 | loss: 0.18038 | val_0_rmse: 0.40795 | val_1_rmse: 0.41892 |  0:04:35s
epoch 85 | loss: 0.18323 | val_0_rmse: 0.40308 | val_1_rmse: 0.41508 |  0:04:38s
epoch 86 | loss: 0.17931 | val_0_rmse: 0.41006 | val_1_rmse: 0.42362 |  0:04:41s
epoch 87 | loss: 0.18031 | val_0_rmse: 0.4036  | val_1_rmse: 0.41672 |  0:04:45s
epoch 88 | loss: 0.18039 | val_0_rmse: 0.42128 | val_1_rmse: 0.43014 |  0:04:48s
epoch 89 | loss: 0.17968 | val_0_rmse: 0.40635 | val_1_rmse: 0.42154 |  0:04:51s
epoch 90 | loss: 0.17881 | val_0_rmse: 0.40725 | val_1_rmse: 0.41795 |  0:04:54s
epoch 91 | loss: 0.18101 | val_0_rmse: 0.40732 | val_1_rmse: 0.42059 |  0:04:57s
epoch 92 | loss: 0.17669 | val_0_rmse: 0.40386 | val_1_rmse: 0.41612 |  0:05:01s
epoch 93 | loss: 0.18159 | val_0_rmse: 0.40503 | val_1_rmse: 0.41811 |  0:05:04s
epoch 94 | loss: 0.18089 | val_0_rmse: 0.41911 | val_1_rmse: 0.4304  |  0:05:07s
epoch 95 | loss: 0.17796 | val_0_rmse: 0.40407 | val_1_rmse: 0.41528 |  0:05:10s
epoch 96 | loss: 0.17575 | val_0_rmse: 0.41827 | val_1_rmse: 0.43182 |  0:05:14s
epoch 97 | loss: 0.18386 | val_0_rmse: 0.43644 | val_1_rmse: 0.44616 |  0:05:17s
epoch 98 | loss: 0.19121 | val_0_rmse: 0.42847 | val_1_rmse: 0.43813 |  0:05:20s
epoch 99 | loss: 0.18275 | val_0_rmse: 0.41826 | val_1_rmse: 0.43075 |  0:05:23s
epoch 100| loss: 0.17915 | val_0_rmse: 0.42466 | val_1_rmse: 0.43412 |  0:05:26s
epoch 101| loss: 0.1831  | val_0_rmse: 0.42849 | val_1_rmse: 0.43893 |  0:05:30s
epoch 102| loss: 0.17824 | val_0_rmse: 0.40863 | val_1_rmse: 0.42131 |  0:05:33s
epoch 103| loss: 0.18298 | val_0_rmse: 0.42539 | val_1_rmse: 0.43665 |  0:05:36s
epoch 104| loss: 0.1824  | val_0_rmse: 0.41854 | val_1_rmse: 0.43103 |  0:05:39s
epoch 105| loss: 0.18062 | val_0_rmse: 0.41178 | val_1_rmse: 0.42472 |  0:05:43s
epoch 106| loss: 0.18271 | val_0_rmse: 0.41785 | val_1_rmse: 0.42792 |  0:05:46s
epoch 107| loss: 0.17958 | val_0_rmse: 0.40879 | val_1_rmse: 0.42119 |  0:05:49s
epoch 108| loss: 0.17678 | val_0_rmse: 0.40848 | val_1_rmse: 0.42359 |  0:05:52s
epoch 109| loss: 0.17896 | val_0_rmse: 0.40804 | val_1_rmse: 0.42374 |  0:05:55s
epoch 110| loss: 0.17911 | val_0_rmse: 0.41045 | val_1_rmse: 0.42448 |  0:05:59s
epoch 111| loss: 0.17692 | val_0_rmse: 0.40304 | val_1_rmse: 0.41864 |  0:06:02s
epoch 112| loss: 0.18101 | val_0_rmse: 0.48273 | val_1_rmse: 0.49066 |  0:06:05s
epoch 113| loss: 0.17814 | val_0_rmse: 0.4046  | val_1_rmse: 0.41723 |  0:06:08s
epoch 114| loss: 0.17497 | val_0_rmse: 0.39651 | val_1_rmse: 0.41049 |  0:06:12s
epoch 115| loss: 0.17746 | val_0_rmse: 0.4236  | val_1_rmse: 0.43362 |  0:06:15s
epoch 116| loss: 0.18545 | val_0_rmse: 0.4154  | val_1_rmse: 0.42764 |  0:06:18s
epoch 117| loss: 0.18126 | val_0_rmse: 0.40632 | val_1_rmse: 0.41771 |  0:06:21s
epoch 118| loss: 0.1751  | val_0_rmse: 0.39791 | val_1_rmse: 0.41335 |  0:06:25s
epoch 119| loss: 0.17695 | val_0_rmse: 0.4269  | val_1_rmse: 0.44019 |  0:06:28s
epoch 120| loss: 0.20631 | val_0_rmse: 0.43188 | val_1_rmse: 0.44022 |  0:06:31s
epoch 121| loss: 0.19011 | val_0_rmse: 0.40724 | val_1_rmse: 0.41534 |  0:06:34s
epoch 122| loss: 0.18248 | val_0_rmse: 0.41413 | val_1_rmse: 0.42281 |  0:06:38s
epoch 123| loss: 0.18555 | val_0_rmse: 0.43196 | val_1_rmse: 0.44141 |  0:06:41s
epoch 124| loss: 0.18308 | val_0_rmse: 0.41851 | val_1_rmse: 0.42948 |  0:06:44s
epoch 125| loss: 0.18348 | val_0_rmse: 0.41172 | val_1_rmse: 0.42426 |  0:06:47s
epoch 126| loss: 0.17651 | val_0_rmse: 0.40885 | val_1_rmse: 0.42133 |  0:06:51s
epoch 127| loss: 0.18475 | val_0_rmse: 0.41721 | val_1_rmse: 0.42592 |  0:06:54s
epoch 128| loss: 0.17798 | val_0_rmse: 0.40462 | val_1_rmse: 0.41719 |  0:06:57s
epoch 129| loss: 0.17811 | val_0_rmse: 0.4116  | val_1_rmse: 0.42152 |  0:07:00s
epoch 130| loss: 0.17708 | val_0_rmse: 0.40989 | val_1_rmse: 0.42209 |  0:07:04s
epoch 131| loss: 0.17704 | val_0_rmse: 0.39843 | val_1_rmse: 0.41457 |  0:07:07s
epoch 132| loss: 0.17853 | val_0_rmse: 0.40618 | val_1_rmse: 0.41944 |  0:07:10s
epoch 133| loss: 0.17563 | val_0_rmse: 0.4031  | val_1_rmse: 0.41507 |  0:07:13s
epoch 134| loss: 0.17589 | val_0_rmse: 0.3992  | val_1_rmse: 0.41367 |  0:07:16s
epoch 135| loss: 0.17716 | val_0_rmse: 0.40096 | val_1_rmse: 0.41634 |  0:07:20s
epoch 136| loss: 0.17908 | val_0_rmse: 0.40652 | val_1_rmse: 0.41988 |  0:07:23s
epoch 137| loss: 0.17685 | val_0_rmse: 0.40394 | val_1_rmse: 0.41678 |  0:07:26s
epoch 138| loss: 0.17591 | val_0_rmse: 0.40259 | val_1_rmse: 0.41519 |  0:07:29s
epoch 139| loss: 0.17369 | val_0_rmse: 0.40309 | val_1_rmse: 0.41612 |  0:07:33s
epoch 140| loss: 0.17737 | val_0_rmse: 0.40577 | val_1_rmse: 0.42067 |  0:07:36s
epoch 141| loss: 0.17408 | val_0_rmse: 0.42218 | val_1_rmse: 0.43586 |  0:07:39s
epoch 142| loss: 0.17725 | val_0_rmse: 0.41055 | val_1_rmse: 0.42605 |  0:07:42s
epoch 143| loss: 0.1774  | val_0_rmse: 0.40958 | val_1_rmse: 0.42494 |  0:07:46s
epoch 144| loss: 0.17547 | val_0_rmse: 0.40554 | val_1_rmse: 0.41969 |  0:07:49s

Early stopping occured at epoch 144 with best_epoch = 114 and best_val_1_rmse = 0.41049
Best weights from best epoch are automatically used!
ended training at: 15:17:40
Feature importance:
[('Area', 0.0010616820646241719), ('Baths', 0.1755797845725575), ('Beds', 0.08173551975424308), ('Latitude', 0.18235252866898602), ('Longitude', 0.2126767437944874), ('Month', 0.0290986152011769), ('Year', 0.3174951259439249)]
Mean squared error is of 9734605754.651394
Mean absolute error:68071.06507332661
MAPE:0.27279793606431496
R2 score:0.8299199638623261
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:18:49
epoch 0  | loss: 0.72269 | val_0_rmse: 0.74091 | val_1_rmse: 0.74402 |  0:00:01s
epoch 1  | loss: 0.49005 | val_0_rmse: 0.65965 | val_1_rmse: 0.65677 |  0:00:04s
epoch 2  | loss: 0.45629 | val_0_rmse: 0.65931 | val_1_rmse: 0.66048 |  0:00:06s
epoch 3  | loss: 0.42557 | val_0_rmse: 0.6222  | val_1_rmse: 0.62392 |  0:00:08s
epoch 4  | loss: 0.41117 | val_0_rmse: 0.62924 | val_1_rmse: 0.63075 |  0:00:10s
epoch 5  | loss: 0.37324 | val_0_rmse: 0.58371 | val_1_rmse: 0.58194 |  0:00:12s
epoch 6  | loss: 0.35302 | val_0_rmse: 0.57021 | val_1_rmse: 0.56753 |  0:00:14s
epoch 7  | loss: 0.35124 | val_0_rmse: 0.58826 | val_1_rmse: 0.58693 |  0:00:16s
epoch 8  | loss: 0.34407 | val_0_rmse: 0.56555 | val_1_rmse: 0.5629  |  0:00:18s
epoch 9  | loss: 0.3415  | val_0_rmse: 0.61659 | val_1_rmse: 0.61581 |  0:00:20s
epoch 10 | loss: 0.34528 | val_0_rmse: 0.6308  | val_1_rmse: 0.63014 |  0:00:22s
epoch 11 | loss: 0.3447  | val_0_rmse: 0.57353 | val_1_rmse: 0.57157 |  0:00:24s
epoch 12 | loss: 0.33243 | val_0_rmse: 0.59026 | val_1_rmse: 0.59118 |  0:00:26s
epoch 13 | loss: 0.33309 | val_0_rmse: 0.58081 | val_1_rmse: 0.58017 |  0:00:28s
epoch 14 | loss: 0.32503 | val_0_rmse: 0.57416 | val_1_rmse: 0.57555 |  0:00:30s
epoch 15 | loss: 0.32229 | val_0_rmse: 0.58491 | val_1_rmse: 0.58769 |  0:00:32s
epoch 16 | loss: 0.31983 | val_0_rmse: 0.5994  | val_1_rmse: 0.5942  |  0:00:34s
epoch 17 | loss: 0.31954 | val_0_rmse: 0.55156 | val_1_rmse: 0.54785 |  0:00:36s
epoch 18 | loss: 0.31996 | val_0_rmse: 0.59766 | val_1_rmse: 0.59887 |  0:00:38s
epoch 19 | loss: 0.32541 | val_0_rmse: 0.57083 | val_1_rmse: 0.56575 |  0:00:40s
epoch 20 | loss: 0.32569 | val_0_rmse: 0.55299 | val_1_rmse: 0.54828 |  0:00:42s
epoch 21 | loss: 0.32101 | val_0_rmse: 0.56578 | val_1_rmse: 0.56303 |  0:00:44s
epoch 22 | loss: 0.31368 | val_0_rmse: 0.59974 | val_1_rmse: 0.59231 |  0:00:46s
epoch 23 | loss: 0.31307 | val_0_rmse: 0.56137 | val_1_rmse: 0.55798 |  0:00:48s
epoch 24 | loss: 0.32635 | val_0_rmse: 0.54623 | val_1_rmse: 0.54297 |  0:00:50s
epoch 25 | loss: 0.32181 | val_0_rmse: 0.5511  | val_1_rmse: 0.54576 |  0:00:52s
epoch 26 | loss: 0.31797 | val_0_rmse: 0.55514 | val_1_rmse: 0.5567  |  0:00:54s
epoch 27 | loss: 0.31525 | val_0_rmse: 0.57792 | val_1_rmse: 0.56869 |  0:00:56s
epoch 28 | loss: 0.31368 | val_0_rmse: 0.56674 | val_1_rmse: 0.56115 |  0:00:58s
epoch 29 | loss: 0.30975 | val_0_rmse: 0.54916 | val_1_rmse: 0.54212 |  0:01:00s
epoch 30 | loss: 0.30872 | val_0_rmse: 0.53736 | val_1_rmse: 0.53204 |  0:01:02s
epoch 31 | loss: 0.31754 | val_0_rmse: 0.59134 | val_1_rmse: 0.58821 |  0:01:04s
epoch 32 | loss: 0.3108  | val_0_rmse: 0.58176 | val_1_rmse: 0.57451 |  0:01:06s
epoch 33 | loss: 0.31595 | val_0_rmse: 0.56875 | val_1_rmse: 0.56184 |  0:01:08s
epoch 34 | loss: 0.3058  | val_0_rmse: 0.57624 | val_1_rmse: 0.57098 |  0:01:10s
epoch 35 | loss: 0.31421 | val_0_rmse: 0.53778 | val_1_rmse: 0.53252 |  0:01:12s
epoch 36 | loss: 0.3112  | val_0_rmse: 0.56396 | val_1_rmse: 0.56217 |  0:01:14s
epoch 37 | loss: 0.31314 | val_0_rmse: 0.56746 | val_1_rmse: 0.56218 |  0:01:16s
epoch 38 | loss: 0.31418 | val_0_rmse: 0.5572  | val_1_rmse: 0.55117 |  0:01:18s
epoch 39 | loss: 0.30738 | val_0_rmse: 0.5317  | val_1_rmse: 0.5318  |  0:01:20s
epoch 40 | loss: 0.30763 | val_0_rmse: 0.57771 | val_1_rmse: 0.57264 |  0:01:22s
epoch 41 | loss: 0.3017  | val_0_rmse: 0.53632 | val_1_rmse: 0.53227 |  0:01:24s
epoch 42 | loss: 0.30041 | val_0_rmse: 0.5613  | val_1_rmse: 0.56398 |  0:01:26s
epoch 43 | loss: 0.31047 | val_0_rmse: 0.53982 | val_1_rmse: 0.53754 |  0:01:28s
epoch 44 | loss: 0.30688 | val_0_rmse: 0.5271  | val_1_rmse: 0.52303 |  0:01:30s
epoch 45 | loss: 0.30912 | val_0_rmse: 0.57135 | val_1_rmse: 0.57625 |  0:01:32s
epoch 46 | loss: 0.30196 | val_0_rmse: 0.54675 | val_1_rmse: 0.54624 |  0:01:34s
epoch 47 | loss: 0.29584 | val_0_rmse: 0.53389 | val_1_rmse: 0.53186 |  0:01:36s
epoch 48 | loss: 0.30559 | val_0_rmse: 0.56221 | val_1_rmse: 0.55623 |  0:01:38s
epoch 49 | loss: 0.29317 | val_0_rmse: 0.5482  | val_1_rmse: 0.54474 |  0:01:40s
epoch 50 | loss: 0.29665 | val_0_rmse: 0.51824 | val_1_rmse: 0.51763 |  0:01:42s
epoch 51 | loss: 0.3012  | val_0_rmse: 0.52744 | val_1_rmse: 0.52741 |  0:01:44s
epoch 52 | loss: 0.29919 | val_0_rmse: 0.54196 | val_1_rmse: 0.54466 |  0:01:46s
epoch 53 | loss: 0.29714 | val_0_rmse: 0.53928 | val_1_rmse: 0.53873 |  0:01:48s
epoch 54 | loss: 0.29672 | val_0_rmse: 0.53338 | val_1_rmse: 0.52817 |  0:01:50s
epoch 55 | loss: 0.29699 | val_0_rmse: 0.56212 | val_1_rmse: 0.56888 |  0:01:52s
epoch 56 | loss: 0.30224 | val_0_rmse: 0.57453 | val_1_rmse: 0.57665 |  0:01:54s
epoch 57 | loss: 0.29072 | val_0_rmse: 0.53921 | val_1_rmse: 0.54442 |  0:01:56s
epoch 58 | loss: 0.29579 | val_0_rmse: 0.53268 | val_1_rmse: 0.52988 |  0:01:58s
epoch 59 | loss: 0.29668 | val_0_rmse: 0.5577  | val_1_rmse: 0.55607 |  0:02:00s
epoch 60 | loss: 0.29471 | val_0_rmse: 0.51224 | val_1_rmse: 0.51381 |  0:02:02s
epoch 61 | loss: 0.28698 | val_0_rmse: 0.52094 | val_1_rmse: 0.52151 |  0:02:04s
epoch 62 | loss: 0.28828 | val_0_rmse: 0.51492 | val_1_rmse: 0.51341 |  0:02:06s
epoch 63 | loss: 0.29538 | val_0_rmse: 0.54842 | val_1_rmse: 0.55068 |  0:02:08s
epoch 64 | loss: 0.29125 | val_0_rmse: 0.56514 | val_1_rmse: 0.57162 |  0:02:10s
epoch 65 | loss: 0.29255 | val_0_rmse: 0.53892 | val_1_rmse: 0.5403  |  0:02:12s
epoch 66 | loss: 0.29318 | val_0_rmse: 0.52756 | val_1_rmse: 0.52732 |  0:02:14s
epoch 67 | loss: 0.28761 | val_0_rmse: 0.51018 | val_1_rmse: 0.50949 |  0:02:16s
epoch 68 | loss: 0.28521 | val_0_rmse: 0.51527 | val_1_rmse: 0.50981 |  0:02:18s
epoch 69 | loss: 0.28999 | val_0_rmse: 0.54796 | val_1_rmse: 0.547   |  0:02:20s
epoch 70 | loss: 0.29059 | val_0_rmse: 0.53961 | val_1_rmse: 0.53757 |  0:02:22s
epoch 71 | loss: 0.2877  | val_0_rmse: 0.53444 | val_1_rmse: 0.53127 |  0:02:24s
epoch 72 | loss: 0.28562 | val_0_rmse: 0.54074 | val_1_rmse: 0.53953 |  0:02:27s
epoch 73 | loss: 0.28766 | val_0_rmse: 0.54467 | val_1_rmse: 0.54645 |  0:02:29s
epoch 74 | loss: 0.28818 | val_0_rmse: 0.53167 | val_1_rmse: 0.53178 |  0:02:30s
epoch 75 | loss: 0.29004 | val_0_rmse: 0.51438 | val_1_rmse: 0.51486 |  0:02:32s
epoch 76 | loss: 0.28929 | val_0_rmse: 0.544   | val_1_rmse: 0.54279 |  0:02:35s
epoch 77 | loss: 0.2834  | val_0_rmse: 0.52681 | val_1_rmse: 0.53235 |  0:02:37s
epoch 78 | loss: 0.28198 | val_0_rmse: 0.5273  | val_1_rmse: 0.53073 |  0:02:39s
epoch 79 | loss: 0.28126 | val_0_rmse: 0.56456 | val_1_rmse: 0.56301 |  0:02:41s
epoch 80 | loss: 0.29327 | val_0_rmse: 0.55308 | val_1_rmse: 0.55642 |  0:02:43s
epoch 81 | loss: 0.28394 | val_0_rmse: 0.50713 | val_1_rmse: 0.50601 |  0:02:45s
epoch 82 | loss: 0.28215 | val_0_rmse: 0.51874 | val_1_rmse: 0.51989 |  0:02:47s
epoch 83 | loss: 0.28447 | val_0_rmse: 0.56671 | val_1_rmse: 0.57297 |  0:02:49s
epoch 84 | loss: 0.29169 | val_0_rmse: 0.52582 | val_1_rmse: 0.52796 |  0:02:51s
epoch 85 | loss: 0.28385 | val_0_rmse: 0.50531 | val_1_rmse: 0.50653 |  0:02:53s
epoch 86 | loss: 0.29004 | val_0_rmse: 0.5488  | val_1_rmse: 0.55455 |  0:02:55s
epoch 87 | loss: 0.28343 | val_0_rmse: 0.52104 | val_1_rmse: 0.51912 |  0:02:57s
epoch 88 | loss: 0.28345 | val_0_rmse: 0.5622  | val_1_rmse: 0.56106 |  0:02:59s
epoch 89 | loss: 0.29451 | val_0_rmse: 0.56391 | val_1_rmse: 0.5606  |  0:03:01s
epoch 90 | loss: 0.28182 | val_0_rmse: 0.54032 | val_1_rmse: 0.54239 |  0:03:03s
epoch 91 | loss: 0.28054 | val_0_rmse: 0.55461 | val_1_rmse: 0.5521  |  0:03:05s
epoch 92 | loss: 0.28148 | val_0_rmse: 0.51409 | val_1_rmse: 0.51626 |  0:03:07s
epoch 93 | loss: 0.28518 | val_0_rmse: 0.51744 | val_1_rmse: 0.52024 |  0:03:09s
epoch 94 | loss: 0.27382 | val_0_rmse: 0.5069  | val_1_rmse: 0.50752 |  0:03:11s
epoch 95 | loss: 0.27611 | val_0_rmse: 0.53149 | val_1_rmse: 0.53502 |  0:03:13s
epoch 96 | loss: 0.28011 | val_0_rmse: 0.52142 | val_1_rmse: 0.52197 |  0:03:15s
epoch 97 | loss: 0.27925 | val_0_rmse: 0.51902 | val_1_rmse: 0.52391 |  0:03:17s
epoch 98 | loss: 0.27884 | val_0_rmse: 0.53636 | val_1_rmse: 0.53397 |  0:03:19s
epoch 99 | loss: 0.2793  | val_0_rmse: 0.50766 | val_1_rmse: 0.50595 |  0:03:21s
epoch 100| loss: 0.27309 | val_0_rmse: 0.53716 | val_1_rmse: 0.54295 |  0:03:23s
epoch 101| loss: 0.2822  | val_0_rmse: 0.5319  | val_1_rmse: 0.54027 |  0:03:25s
epoch 102| loss: 0.28108 | val_0_rmse: 0.52123 | val_1_rmse: 0.52275 |  0:03:27s
epoch 103| loss: 0.27975 | val_0_rmse: 0.51727 | val_1_rmse: 0.51835 |  0:03:29s
epoch 104| loss: 0.27709 | val_0_rmse: 0.56198 | val_1_rmse: 0.57086 |  0:03:31s
epoch 105| loss: 0.27702 | val_0_rmse: 0.53061 | val_1_rmse: 0.52933 |  0:03:33s
epoch 106| loss: 0.28164 | val_0_rmse: 0.51749 | val_1_rmse: 0.51938 |  0:03:35s
epoch 107| loss: 0.27848 | val_0_rmse: 0.52349 | val_1_rmse: 0.52757 |  0:03:37s
epoch 108| loss: 0.27857 | val_0_rmse: 0.53493 | val_1_rmse: 0.5334  |  0:03:39s
epoch 109| loss: 0.2832  | val_0_rmse: 0.51661 | val_1_rmse: 0.51689 |  0:03:41s
epoch 110| loss: 0.27735 | val_0_rmse: 0.51429 | val_1_rmse: 0.51291 |  0:03:43s
epoch 111| loss: 0.28174 | val_0_rmse: 0.51462 | val_1_rmse: 0.51916 |  0:03:45s
epoch 112| loss: 0.28065 | val_0_rmse: 0.51035 | val_1_rmse: 0.51476 |  0:03:47s
epoch 113| loss: 0.2783  | val_0_rmse: 0.4945  | val_1_rmse: 0.49828 |  0:03:49s
epoch 114| loss: 0.27185 | val_0_rmse: 0.546   | val_1_rmse: 0.54911 |  0:03:51s
epoch 115| loss: 0.27538 | val_0_rmse: 0.52833 | val_1_rmse: 0.5262  |  0:03:53s
epoch 116| loss: 0.27645 | val_0_rmse: 0.52171 | val_1_rmse: 0.52003 |  0:03:55s
epoch 117| loss: 0.27484 | val_0_rmse: 0.51082 | val_1_rmse: 0.516   |  0:03:57s
epoch 118| loss: 0.28138 | val_0_rmse: 0.50496 | val_1_rmse: 0.509   |  0:03:59s
epoch 119| loss: 0.27659 | val_0_rmse: 0.53354 | val_1_rmse: 0.54075 |  0:04:01s
epoch 120| loss: 0.27789 | val_0_rmse: 0.5506  | val_1_rmse: 0.54875 |  0:04:03s
epoch 121| loss: 0.27101 | val_0_rmse: 0.55642 | val_1_rmse: 0.56479 |  0:04:05s
epoch 122| loss: 0.28302 | val_0_rmse: 0.51522 | val_1_rmse: 0.51335 |  0:04:07s
epoch 123| loss: 0.28175 | val_0_rmse: 0.50453 | val_1_rmse: 0.50708 |  0:04:09s
epoch 124| loss: 0.27367 | val_0_rmse: 0.5004  | val_1_rmse: 0.50139 |  0:04:11s
epoch 125| loss: 0.27355 | val_0_rmse: 0.52316 | val_1_rmse: 0.5211  |  0:04:13s
epoch 126| loss: 0.2784  | val_0_rmse: 0.55538 | val_1_rmse: 0.55457 |  0:04:15s
epoch 127| loss: 0.27661 | val_0_rmse: 0.50516 | val_1_rmse: 0.50202 |  0:04:17s
epoch 128| loss: 0.28277 | val_0_rmse: 0.5075  | val_1_rmse: 0.50842 |  0:04:19s
epoch 129| loss: 0.2807  | val_0_rmse: 0.51699 | val_1_rmse: 0.51857 |  0:04:21s
epoch 130| loss: 0.27478 | val_0_rmse: 0.51876 | val_1_rmse: 0.51889 |  0:04:23s
epoch 131| loss: 0.27502 | val_0_rmse: 0.50774 | val_1_rmse: 0.5089  |  0:04:25s
epoch 132| loss: 0.2744  | val_0_rmse: 0.51109 | val_1_rmse: 0.50913 |  0:04:27s
epoch 133| loss: 0.27488 | val_0_rmse: 0.52978 | val_1_rmse: 0.52538 |  0:04:29s
epoch 134| loss: 0.26871 | val_0_rmse: 0.56016 | val_1_rmse: 0.5589  |  0:04:31s
epoch 135| loss: 0.27594 | val_0_rmse: 0.52283 | val_1_rmse: 0.52812 |  0:04:33s
epoch 136| loss: 0.27303 | val_0_rmse: 0.53548 | val_1_rmse: 0.5337  |  0:04:35s
epoch 137| loss: 0.27408 | val_0_rmse: 0.53373 | val_1_rmse: 0.54052 |  0:04:37s
epoch 138| loss: 0.27542 | val_0_rmse: 0.52445 | val_1_rmse: 0.52082 |  0:04:39s
epoch 139| loss: 0.2675  | val_0_rmse: 0.50667 | val_1_rmse: 0.50418 |  0:04:41s
epoch 140| loss: 0.27175 | val_0_rmse: 0.53936 | val_1_rmse: 0.54749 |  0:04:43s
epoch 141| loss: 0.27901 | val_0_rmse: 0.50553 | val_1_rmse: 0.50755 |  0:04:45s
epoch 142| loss: 0.27144 | val_0_rmse: 0.51273 | val_1_rmse: 0.51364 |  0:04:47s
epoch 143| loss: 0.28068 | val_0_rmse: 0.51225 | val_1_rmse: 0.5109  |  0:04:49s

Early stopping occured at epoch 143 with best_epoch = 113 and best_val_1_rmse = 0.49828
Best weights from best epoch are automatically used!
ended training at: 15:23:40
Feature importance:
[('Area', 0.3555806586370875), ('Baths', 0.044026257151989615), ('Beds', 0.0), ('Latitude', 0.34148865879858625), ('Longitude', 0.2589044254123367), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 7717930492.147809
Mean absolute error:63085.838680237146
MAPE:0.1673638864192943
R2 score:0.7432507400747359
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:23:40
epoch 0  | loss: 0.68501 | val_0_rmse: 0.73656 | val_1_rmse: 0.72739 |  0:00:02s
epoch 1  | loss: 0.47814 | val_0_rmse: 0.65235 | val_1_rmse: 0.64585 |  0:00:04s
epoch 2  | loss: 0.42508 | val_0_rmse: 0.61669 | val_1_rmse: 0.61412 |  0:00:06s
epoch 3  | loss: 0.37472 | val_0_rmse: 0.61165 | val_1_rmse: 0.60594 |  0:00:08s
epoch 4  | loss: 0.36078 | val_0_rmse: 0.58209 | val_1_rmse: 0.57666 |  0:00:10s
epoch 5  | loss: 0.35596 | val_0_rmse: 0.59183 | val_1_rmse: 0.58445 |  0:00:12s
epoch 6  | loss: 0.33729 | val_0_rmse: 0.56924 | val_1_rmse: 0.56323 |  0:00:14s
epoch 7  | loss: 0.35156 | val_0_rmse: 0.60627 | val_1_rmse: 0.596   |  0:00:16s
epoch 8  | loss: 0.34423 | val_0_rmse: 0.56791 | val_1_rmse: 0.5613  |  0:00:18s
epoch 9  | loss: 0.33845 | val_0_rmse: 0.55456 | val_1_rmse: 0.54494 |  0:00:20s
epoch 10 | loss: 0.33196 | val_0_rmse: 0.56998 | val_1_rmse: 0.56212 |  0:00:22s
epoch 11 | loss: 0.33764 | val_0_rmse: 0.57953 | val_1_rmse: 0.57187 |  0:00:24s
epoch 12 | loss: 0.33604 | val_0_rmse: 0.56169 | val_1_rmse: 0.55572 |  0:00:26s
epoch 13 | loss: 0.33088 | val_0_rmse: 0.56101 | val_1_rmse: 0.55116 |  0:00:28s
epoch 14 | loss: 0.32609 | val_0_rmse: 0.54886 | val_1_rmse: 0.542   |  0:00:30s
epoch 15 | loss: 0.32438 | val_0_rmse: 0.54639 | val_1_rmse: 0.538   |  0:00:32s
epoch 16 | loss: 0.33155 | val_0_rmse: 0.57056 | val_1_rmse: 0.5607  |  0:00:34s
epoch 17 | loss: 0.33622 | val_0_rmse: 0.5453  | val_1_rmse: 0.53509 |  0:00:36s
epoch 18 | loss: 0.3149  | val_0_rmse: 0.54465 | val_1_rmse: 0.535   |  0:00:38s
epoch 19 | loss: 0.32608 | val_0_rmse: 0.5521  | val_1_rmse: 0.54457 |  0:00:40s
epoch 20 | loss: 0.33159 | val_0_rmse: 0.56085 | val_1_rmse: 0.55376 |  0:00:42s
epoch 21 | loss: 0.32263 | val_0_rmse: 0.54749 | val_1_rmse: 0.54147 |  0:00:44s
epoch 22 | loss: 0.3093  | val_0_rmse: 0.53788 | val_1_rmse: 0.52972 |  0:00:46s
epoch 23 | loss: 0.31347 | val_0_rmse: 0.54333 | val_1_rmse: 0.53324 |  0:00:48s
epoch 24 | loss: 0.31318 | val_0_rmse: 0.55363 | val_1_rmse: 0.54599 |  0:00:50s
epoch 25 | loss: 0.31092 | val_0_rmse: 0.54244 | val_1_rmse: 0.5321  |  0:00:52s
epoch 26 | loss: 0.31071 | val_0_rmse: 0.52235 | val_1_rmse: 0.51369 |  0:00:54s
epoch 27 | loss: 0.3053  | val_0_rmse: 0.52425 | val_1_rmse: 0.51265 |  0:00:56s
epoch 28 | loss: 0.30919 | val_0_rmse: 0.57928 | val_1_rmse: 0.57132 |  0:00:58s
epoch 29 | loss: 0.31372 | val_0_rmse: 0.53549 | val_1_rmse: 0.5284  |  0:01:00s
epoch 30 | loss: 0.30253 | val_0_rmse: 0.5319  | val_1_rmse: 0.52126 |  0:01:02s
epoch 31 | loss: 0.30101 | val_0_rmse: 0.52347 | val_1_rmse: 0.51299 |  0:01:04s
epoch 32 | loss: 0.29524 | val_0_rmse: 0.52713 | val_1_rmse: 0.51992 |  0:01:06s
epoch 33 | loss: 0.30961 | val_0_rmse: 0.54265 | val_1_rmse: 0.53204 |  0:01:08s
epoch 34 | loss: 0.30175 | val_0_rmse: 0.51887 | val_1_rmse: 0.51185 |  0:01:10s
epoch 35 | loss: 0.29918 | val_0_rmse: 0.53671 | val_1_rmse: 0.52944 |  0:01:12s
epoch 36 | loss: 0.29993 | val_0_rmse: 0.51766 | val_1_rmse: 0.50951 |  0:01:14s
epoch 37 | loss: 0.29824 | val_0_rmse: 0.53533 | val_1_rmse: 0.5287  |  0:01:16s
epoch 38 | loss: 0.30446 | val_0_rmse: 0.54027 | val_1_rmse: 0.53208 |  0:01:18s
epoch 39 | loss: 0.30218 | val_0_rmse: 0.54215 | val_1_rmse: 0.53327 |  0:01:20s
epoch 40 | loss: 0.29545 | val_0_rmse: 0.51728 | val_1_rmse: 0.51069 |  0:01:22s
epoch 41 | loss: 0.29652 | val_0_rmse: 0.52026 | val_1_rmse: 0.51305 |  0:01:24s
epoch 42 | loss: 0.30232 | val_0_rmse: 0.54021 | val_1_rmse: 0.53437 |  0:01:26s
epoch 43 | loss: 0.30265 | val_0_rmse: 0.52416 | val_1_rmse: 0.51581 |  0:01:28s
epoch 44 | loss: 0.29199 | val_0_rmse: 0.52091 | val_1_rmse: 0.51147 |  0:01:30s
epoch 45 | loss: 0.29239 | val_0_rmse: 0.52386 | val_1_rmse: 0.51682 |  0:01:32s
epoch 46 | loss: 0.29699 | val_0_rmse: 0.53991 | val_1_rmse: 0.53371 |  0:01:34s
epoch 47 | loss: 0.2978  | val_0_rmse: 0.53107 | val_1_rmse: 0.52193 |  0:01:36s
epoch 48 | loss: 0.29414 | val_0_rmse: 0.51872 | val_1_rmse: 0.51183 |  0:01:38s
epoch 49 | loss: 0.29408 | val_0_rmse: 0.52243 | val_1_rmse: 0.51532 |  0:01:40s
epoch 50 | loss: 0.29419 | val_0_rmse: 0.52648 | val_1_rmse: 0.5182  |  0:01:42s
epoch 51 | loss: 0.29413 | val_0_rmse: 0.515   | val_1_rmse: 0.51124 |  0:01:44s
epoch 52 | loss: 0.29262 | val_0_rmse: 0.52078 | val_1_rmse: 0.51135 |  0:01:46s
epoch 53 | loss: 0.29189 | val_0_rmse: 0.51348 | val_1_rmse: 0.50422 |  0:01:48s
epoch 54 | loss: 0.29302 | val_0_rmse: 0.52222 | val_1_rmse: 0.51576 |  0:01:50s
epoch 55 | loss: 0.29709 | val_0_rmse: 0.52068 | val_1_rmse: 0.51406 |  0:01:52s
epoch 56 | loss: 0.29285 | val_0_rmse: 0.52945 | val_1_rmse: 0.52355 |  0:01:54s
epoch 57 | loss: 0.29922 | val_0_rmse: 0.52905 | val_1_rmse: 0.52167 |  0:01:57s
epoch 58 | loss: 0.29746 | val_0_rmse: 0.54226 | val_1_rmse: 0.54098 |  0:01:58s
epoch 59 | loss: 0.28739 | val_0_rmse: 0.51745 | val_1_rmse: 0.5125  |  0:02:00s
epoch 60 | loss: 0.28586 | val_0_rmse: 0.51775 | val_1_rmse: 0.51398 |  0:02:02s
epoch 61 | loss: 0.28473 | val_0_rmse: 0.51893 | val_1_rmse: 0.50971 |  0:02:05s
epoch 62 | loss: 0.28758 | val_0_rmse: 0.52301 | val_1_rmse: 0.51814 |  0:02:07s
epoch 63 | loss: 0.2852  | val_0_rmse: 0.52608 | val_1_rmse: 0.52425 |  0:02:09s
epoch 64 | loss: 0.28344 | val_0_rmse: 0.55067 | val_1_rmse: 0.54681 |  0:02:11s
epoch 65 | loss: 0.28442 | val_0_rmse: 0.5381  | val_1_rmse: 0.53302 |  0:02:13s
epoch 66 | loss: 0.28884 | val_0_rmse: 0.5316  | val_1_rmse: 0.52731 |  0:02:15s
epoch 67 | loss: 0.28804 | val_0_rmse: 0.51715 | val_1_rmse: 0.51179 |  0:02:17s
epoch 68 | loss: 0.28042 | val_0_rmse: 0.51218 | val_1_rmse: 0.50722 |  0:02:19s
epoch 69 | loss: 0.28124 | val_0_rmse: 0.52343 | val_1_rmse: 0.51908 |  0:02:21s
epoch 70 | loss: 0.28757 | val_0_rmse: 0.51132 | val_1_rmse: 0.50516 |  0:02:23s
epoch 71 | loss: 0.28491 | val_0_rmse: 0.50397 | val_1_rmse: 0.49876 |  0:02:25s
epoch 72 | loss: 0.28565 | val_0_rmse: 0.53545 | val_1_rmse: 0.53014 |  0:02:27s
epoch 73 | loss: 0.28099 | val_0_rmse: 0.52622 | val_1_rmse: 0.5197  |  0:02:29s
epoch 74 | loss: 0.29311 | val_0_rmse: 0.51876 | val_1_rmse: 0.51527 |  0:02:31s
epoch 75 | loss: 0.2824  | val_0_rmse: 0.51984 | val_1_rmse: 0.51693 |  0:02:33s
epoch 76 | loss: 0.28054 | val_0_rmse: 0.52467 | val_1_rmse: 0.52097 |  0:02:35s
epoch 77 | loss: 0.28147 | val_0_rmse: 0.50954 | val_1_rmse: 0.5062  |  0:02:37s
epoch 78 | loss: 0.28049 | val_0_rmse: 0.52096 | val_1_rmse: 0.51451 |  0:02:39s
epoch 79 | loss: 0.28272 | val_0_rmse: 0.52514 | val_1_rmse: 0.51888 |  0:02:41s
epoch 80 | loss: 0.28294 | val_0_rmse: 0.529   | val_1_rmse: 0.529   |  0:02:43s
epoch 81 | loss: 0.28268 | val_0_rmse: 0.4997  | val_1_rmse: 0.49418 |  0:02:45s
epoch 82 | loss: 0.29062 | val_0_rmse: 0.51709 | val_1_rmse: 0.50963 |  0:02:47s
epoch 83 | loss: 0.28084 | val_0_rmse: 0.52158 | val_1_rmse: 0.51721 |  0:02:49s
epoch 84 | loss: 0.28032 | val_0_rmse: 0.49718 | val_1_rmse: 0.49319 |  0:02:51s
epoch 85 | loss: 0.27507 | val_0_rmse: 0.52068 | val_1_rmse: 0.51757 |  0:02:53s
epoch 86 | loss: 0.2793  | val_0_rmse: 0.4969  | val_1_rmse: 0.49312 |  0:02:55s
epoch 87 | loss: 0.28263 | val_0_rmse: 0.51913 | val_1_rmse: 0.51665 |  0:02:57s
epoch 88 | loss: 0.27332 | val_0_rmse: 0.51544 | val_1_rmse: 0.51543 |  0:02:59s
epoch 89 | loss: 0.28038 | val_0_rmse: 0.52756 | val_1_rmse: 0.52472 |  0:03:01s
epoch 90 | loss: 0.27672 | val_0_rmse: 0.50555 | val_1_rmse: 0.49761 |  0:03:03s
epoch 91 | loss: 0.27233 | val_0_rmse: 0.50226 | val_1_rmse: 0.49874 |  0:03:05s
epoch 92 | loss: 0.2726  | val_0_rmse: 0.5045  | val_1_rmse: 0.4998  |  0:03:07s
epoch 93 | loss: 0.27204 | val_0_rmse: 0.49448 | val_1_rmse: 0.49222 |  0:03:09s
epoch 94 | loss: 0.27187 | val_0_rmse: 0.49815 | val_1_rmse: 0.49428 |  0:03:11s
epoch 95 | loss: 0.29124 | val_0_rmse: 0.50767 | val_1_rmse: 0.50355 |  0:03:13s
epoch 96 | loss: 0.27902 | val_0_rmse: 0.51527 | val_1_rmse: 0.51163 |  0:03:15s
epoch 97 | loss: 0.27179 | val_0_rmse: 0.51184 | val_1_rmse: 0.50562 |  0:03:17s
epoch 98 | loss: 0.27612 | val_0_rmse: 0.50547 | val_1_rmse: 0.50061 |  0:03:19s
epoch 99 | loss: 0.27622 | val_0_rmse: 0.5007  | val_1_rmse: 0.49647 |  0:03:21s
epoch 100| loss: 0.27596 | val_0_rmse: 0.51461 | val_1_rmse: 0.50969 |  0:03:23s
epoch 101| loss: 0.27359 | val_0_rmse: 0.51119 | val_1_rmse: 0.50507 |  0:03:25s
epoch 102| loss: 0.27326 | val_0_rmse: 0.5049  | val_1_rmse: 0.50061 |  0:03:27s
epoch 103| loss: 0.27139 | val_0_rmse: 0.50298 | val_1_rmse: 0.49859 |  0:03:29s
epoch 104| loss: 0.27417 | val_0_rmse: 0.50401 | val_1_rmse: 0.49973 |  0:03:31s
epoch 105| loss: 0.26631 | val_0_rmse: 0.49572 | val_1_rmse: 0.4903  |  0:03:33s
epoch 106| loss: 0.26997 | val_0_rmse: 0.50238 | val_1_rmse: 0.49921 |  0:03:35s
epoch 107| loss: 0.27528 | val_0_rmse: 0.4978  | val_1_rmse: 0.49753 |  0:03:37s
epoch 108| loss: 0.27223 | val_0_rmse: 0.51829 | val_1_rmse: 0.51265 |  0:03:39s
epoch 109| loss: 0.27807 | val_0_rmse: 0.56732 | val_1_rmse: 0.56235 |  0:03:41s
epoch 110| loss: 0.28028 | val_0_rmse: 0.52151 | val_1_rmse: 0.51757 |  0:03:43s
epoch 111| loss: 0.27455 | val_0_rmse: 0.49263 | val_1_rmse: 0.48903 |  0:03:45s
epoch 112| loss: 0.27668 | val_0_rmse: 0.51602 | val_1_rmse: 0.51453 |  0:03:47s
epoch 113| loss: 0.2729  | val_0_rmse: 0.49428 | val_1_rmse: 0.49052 |  0:03:49s
epoch 114| loss: 0.27064 | val_0_rmse: 0.50802 | val_1_rmse: 0.50673 |  0:03:51s
epoch 115| loss: 0.27278 | val_0_rmse: 0.50835 | val_1_rmse: 0.50393 |  0:03:53s
epoch 116| loss: 0.27122 | val_0_rmse: 0.50862 | val_1_rmse: 0.50295 |  0:03:55s
epoch 117| loss: 0.27714 | val_0_rmse: 0.51412 | val_1_rmse: 0.51172 |  0:03:57s
epoch 118| loss: 0.26625 | val_0_rmse: 0.50798 | val_1_rmse: 0.50749 |  0:03:59s
epoch 119| loss: 0.28691 | val_0_rmse: 0.51207 | val_1_rmse: 0.50995 |  0:04:01s
epoch 120| loss: 0.28444 | val_0_rmse: 0.50606 | val_1_rmse: 0.50008 |  0:04:03s
epoch 121| loss: 0.2672  | val_0_rmse: 0.50327 | val_1_rmse: 0.49933 |  0:04:05s
epoch 122| loss: 0.27104 | val_0_rmse: 0.5185  | val_1_rmse: 0.51634 |  0:04:07s
epoch 123| loss: 0.27869 | val_0_rmse: 0.51126 | val_1_rmse: 0.50916 |  0:04:09s
epoch 124| loss: 0.2733  | val_0_rmse: 0.5097  | val_1_rmse: 0.50558 |  0:04:11s
epoch 125| loss: 0.26879 | val_0_rmse: 0.50307 | val_1_rmse: 0.49942 |  0:04:13s
epoch 126| loss: 0.26781 | val_0_rmse: 0.49662 | val_1_rmse: 0.49453 |  0:04:15s
epoch 127| loss: 0.27102 | val_0_rmse: 0.49717 | val_1_rmse: 0.4944  |  0:04:17s
epoch 128| loss: 0.27444 | val_0_rmse: 0.51865 | val_1_rmse: 0.51635 |  0:04:19s
epoch 129| loss: 0.2702  | val_0_rmse: 0.51614 | val_1_rmse: 0.51222 |  0:04:21s
epoch 130| loss: 0.2792  | val_0_rmse: 0.51489 | val_1_rmse: 0.51207 |  0:04:23s
epoch 131| loss: 0.2739  | val_0_rmse: 0.50673 | val_1_rmse: 0.50603 |  0:04:25s
epoch 132| loss: 0.27365 | val_0_rmse: 0.48706 | val_1_rmse: 0.48609 |  0:04:27s
epoch 133| loss: 0.27129 | val_0_rmse: 0.51392 | val_1_rmse: 0.51352 |  0:04:29s
epoch 134| loss: 0.27229 | val_0_rmse: 0.52082 | val_1_rmse: 0.51756 |  0:04:31s
epoch 135| loss: 0.2706  | val_0_rmse: 0.48729 | val_1_rmse: 0.4844  |  0:04:33s
epoch 136| loss: 0.269   | val_0_rmse: 0.5018  | val_1_rmse: 0.50087 |  0:04:35s
epoch 137| loss: 0.26226 | val_0_rmse: 0.50054 | val_1_rmse: 0.49915 |  0:04:37s
epoch 138| loss: 0.26483 | val_0_rmse: 0.52909 | val_1_rmse: 0.52418 |  0:04:39s
epoch 139| loss: 0.27385 | val_0_rmse: 0.49072 | val_1_rmse: 0.48942 |  0:04:41s
epoch 140| loss: 0.27208 | val_0_rmse: 0.53051 | val_1_rmse: 0.523   |  0:04:43s
epoch 141| loss: 0.26495 | val_0_rmse: 0.52582 | val_1_rmse: 0.52506 |  0:04:45s
epoch 142| loss: 0.27337 | val_0_rmse: 0.49894 | val_1_rmse: 0.4957  |  0:04:47s
epoch 143| loss: 0.27338 | val_0_rmse: 0.49015 | val_1_rmse: 0.48922 |  0:04:49s
epoch 144| loss: 0.26682 | val_0_rmse: 0.49868 | val_1_rmse: 0.49501 |  0:04:51s
epoch 145| loss: 0.26812 | val_0_rmse: 0.50603 | val_1_rmse: 0.50589 |  0:04:53s
epoch 146| loss: 0.27024 | val_0_rmse: 0.48785 | val_1_rmse: 0.48811 |  0:04:55s
epoch 147| loss: 0.26664 | val_0_rmse: 0.51505 | val_1_rmse: 0.51456 |  0:04:57s
epoch 148| loss: 0.2666  | val_0_rmse: 0.50073 | val_1_rmse: 0.49996 |  0:04:59s
epoch 149| loss: 0.26511 | val_0_rmse: 0.4959  | val_1_rmse: 0.49358 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 135 and best_val_1_rmse = 0.4844
Best weights from best epoch are automatically used!
ended training at: 15:28:43
Feature importance:
[('Area', 0.45878461370476065), ('Baths', 0.0008303047553983642), ('Beds', 0.0), ('Latitude', 0.349008282638109), ('Longitude', 0.10754979524958712), ('Month', 0.03571249711571464), ('Year', 0.048114506536430256)]
Mean squared error is of 7654400745.643314
Mean absolute error:62343.846350618805
MAPE:0.16649803880352948
R2 score:0.7494757471019458
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:28:44
epoch 0  | loss: 0.7175  | val_0_rmse: 0.76898 | val_1_rmse: 0.77114 |  0:00:01s
epoch 1  | loss: 0.50556 | val_0_rmse: 0.64753 | val_1_rmse: 0.65266 |  0:00:03s
epoch 2  | loss: 0.41506 | val_0_rmse: 0.60501 | val_1_rmse: 0.61195 |  0:00:05s
epoch 3  | loss: 0.39595 | val_0_rmse: 0.61078 | val_1_rmse: 0.61934 |  0:00:07s
epoch 4  | loss: 0.3674  | val_0_rmse: 0.60662 | val_1_rmse: 0.61598 |  0:00:10s
epoch 5  | loss: 0.37509 | val_0_rmse: 0.58257 | val_1_rmse: 0.59486 |  0:00:12s
epoch 6  | loss: 0.36504 | val_0_rmse: 0.57635 | val_1_rmse: 0.5861  |  0:00:14s
epoch 7  | loss: 0.35599 | val_0_rmse: 0.5681  | val_1_rmse: 0.57977 |  0:00:16s
epoch 8  | loss: 0.3439  | val_0_rmse: 0.58107 | val_1_rmse: 0.59156 |  0:00:18s
epoch 9  | loss: 0.34227 | val_0_rmse: 0.58862 | val_1_rmse: 0.59508 |  0:00:20s
epoch 10 | loss: 0.34088 | val_0_rmse: 0.56583 | val_1_rmse: 0.5737  |  0:00:22s
epoch 11 | loss: 0.34246 | val_0_rmse: 0.56216 | val_1_rmse: 0.56798 |  0:00:24s
epoch 12 | loss: 0.34145 | val_0_rmse: 0.56053 | val_1_rmse: 0.56814 |  0:00:26s
epoch 13 | loss: 0.33329 | val_0_rmse: 0.55231 | val_1_rmse: 0.56162 |  0:00:28s
epoch 14 | loss: 0.33385 | val_0_rmse: 0.56093 | val_1_rmse: 0.57193 |  0:00:30s
epoch 15 | loss: 0.33529 | val_0_rmse: 0.57327 | val_1_rmse: 0.58333 |  0:00:32s
epoch 16 | loss: 0.32919 | val_0_rmse: 0.56161 | val_1_rmse: 0.56728 |  0:00:34s
epoch 17 | loss: 0.3308  | val_0_rmse: 0.57332 | val_1_rmse: 0.57872 |  0:00:36s
epoch 18 | loss: 0.32826 | val_0_rmse: 0.57441 | val_1_rmse: 0.58111 |  0:00:38s
epoch 19 | loss: 0.33051 | val_0_rmse: 0.60022 | val_1_rmse: 0.60845 |  0:00:40s
epoch 20 | loss: 0.32504 | val_0_rmse: 0.557   | val_1_rmse: 0.56769 |  0:00:42s
epoch 21 | loss: 0.33228 | val_0_rmse: 0.5665  | val_1_rmse: 0.57857 |  0:00:44s
epoch 22 | loss: 0.33095 | val_0_rmse: 0.61006 | val_1_rmse: 0.61837 |  0:00:46s
epoch 23 | loss: 0.40594 | val_0_rmse: 0.59267 | val_1_rmse: 0.59582 |  0:00:48s
epoch 24 | loss: 0.36628 | val_0_rmse: 0.65103 | val_1_rmse: 0.65797 |  0:00:50s
epoch 25 | loss: 0.36666 | val_0_rmse: 0.56948 | val_1_rmse: 0.57586 |  0:00:52s
epoch 26 | loss: 0.35068 | val_0_rmse: 0.61513 | val_1_rmse: 0.62097 |  0:00:54s
epoch 27 | loss: 0.35078 | val_0_rmse: 0.66224 | val_1_rmse: 0.66252 |  0:00:56s
epoch 28 | loss: 0.35765 | val_0_rmse: 0.59345 | val_1_rmse: 0.5995  |  0:00:58s
epoch 29 | loss: 0.34257 | val_0_rmse: 0.58545 | val_1_rmse: 0.59624 |  0:01:00s
epoch 30 | loss: 0.33882 | val_0_rmse: 0.582   | val_1_rmse: 0.58926 |  0:01:02s
epoch 31 | loss: 0.32958 | val_0_rmse: 0.54659 | val_1_rmse: 0.557   |  0:01:04s
epoch 32 | loss: 0.32404 | val_0_rmse: 0.57452 | val_1_rmse: 0.58033 |  0:01:06s
epoch 33 | loss: 0.32949 | val_0_rmse: 0.5563  | val_1_rmse: 0.5641  |  0:01:08s
epoch 34 | loss: 0.32957 | val_0_rmse: 0.56754 | val_1_rmse: 0.57483 |  0:01:10s
epoch 35 | loss: 0.31703 | val_0_rmse: 0.55724 | val_1_rmse: 0.57207 |  0:01:12s
epoch 36 | loss: 0.31478 | val_0_rmse: 0.54615 | val_1_rmse: 0.55292 |  0:01:14s
epoch 37 | loss: 0.31956 | val_0_rmse: 0.57478 | val_1_rmse: 0.58082 |  0:01:16s
epoch 38 | loss: 0.3225  | val_0_rmse: 0.5464  | val_1_rmse: 0.55744 |  0:01:18s
epoch 39 | loss: 0.32223 | val_0_rmse: 0.55842 | val_1_rmse: 0.56655 |  0:01:20s
epoch 40 | loss: 0.31945 | val_0_rmse: 0.54571 | val_1_rmse: 0.55372 |  0:01:22s
epoch 41 | loss: 0.31992 | val_0_rmse: 0.55149 | val_1_rmse: 0.56065 |  0:01:24s
epoch 42 | loss: 0.32641 | val_0_rmse: 0.56732 | val_1_rmse: 0.57974 |  0:01:26s
epoch 43 | loss: 0.32866 | val_0_rmse: 0.54451 | val_1_rmse: 0.55372 |  0:01:28s
epoch 44 | loss: 0.32069 | val_0_rmse: 0.53397 | val_1_rmse: 0.54329 |  0:01:30s
epoch 45 | loss: 0.31765 | val_0_rmse: 0.55321 | val_1_rmse: 0.56142 |  0:01:32s
epoch 46 | loss: 0.33615 | val_0_rmse: 0.59785 | val_1_rmse: 0.60723 |  0:01:34s
epoch 47 | loss: 0.37629 | val_0_rmse: 0.62064 | val_1_rmse: 0.62795 |  0:01:36s
epoch 48 | loss: 0.34716 | val_0_rmse: 0.58246 | val_1_rmse: 0.59459 |  0:01:38s
epoch 49 | loss: 0.34068 | val_0_rmse: 0.62747 | val_1_rmse: 0.63497 |  0:01:40s
epoch 50 | loss: 0.32958 | val_0_rmse: 0.55007 | val_1_rmse: 0.55624 |  0:01:42s
epoch 51 | loss: 0.31983 | val_0_rmse: 0.55398 | val_1_rmse: 0.56707 |  0:01:44s
epoch 52 | loss: 0.31599 | val_0_rmse: 0.55385 | val_1_rmse: 0.56284 |  0:01:46s
epoch 53 | loss: 0.32076 | val_0_rmse: 0.56476 | val_1_rmse: 0.57305 |  0:01:48s
epoch 54 | loss: 0.32026 | val_0_rmse: 0.5598  | val_1_rmse: 0.56683 |  0:01:50s
epoch 55 | loss: 0.31662 | val_0_rmse: 0.54338 | val_1_rmse: 0.55561 |  0:01:52s
epoch 56 | loss: 0.31834 | val_0_rmse: 0.54807 | val_1_rmse: 0.55978 |  0:01:54s
epoch 57 | loss: 0.32028 | val_0_rmse: 0.54238 | val_1_rmse: 0.55462 |  0:01:56s
epoch 58 | loss: 0.30904 | val_0_rmse: 0.55355 | val_1_rmse: 0.56672 |  0:01:58s
epoch 59 | loss: 0.31094 | val_0_rmse: 0.53566 | val_1_rmse: 0.54462 |  0:02:00s
epoch 60 | loss: 0.30909 | val_0_rmse: 0.53562 | val_1_rmse: 0.5406  |  0:02:02s
epoch 61 | loss: 0.32058 | val_0_rmse: 0.58098 | val_1_rmse: 0.58197 |  0:02:04s
epoch 62 | loss: 0.3195  | val_0_rmse: 0.54882 | val_1_rmse: 0.55557 |  0:02:06s
epoch 63 | loss: 0.3184  | val_0_rmse: 0.5456  | val_1_rmse: 0.55595 |  0:02:08s
epoch 64 | loss: 0.31087 | val_0_rmse: 0.53899 | val_1_rmse: 0.54729 |  0:02:10s
epoch 65 | loss: 0.30454 | val_0_rmse: 0.54144 | val_1_rmse: 0.54866 |  0:02:12s
epoch 66 | loss: 0.30685 | val_0_rmse: 0.5329  | val_1_rmse: 0.54093 |  0:02:14s
epoch 67 | loss: 0.31621 | val_0_rmse: 0.5454  | val_1_rmse: 0.55525 |  0:02:16s
epoch 68 | loss: 0.31296 | val_0_rmse: 0.5299  | val_1_rmse: 0.54076 |  0:02:18s
epoch 69 | loss: 0.31877 | val_0_rmse: 0.54614 | val_1_rmse: 0.55392 |  0:02:20s
epoch 70 | loss: 0.30924 | val_0_rmse: 0.54415 | val_1_rmse: 0.55241 |  0:02:22s
epoch 71 | loss: 0.31137 | val_0_rmse: 0.54852 | val_1_rmse: 0.55547 |  0:02:24s
epoch 72 | loss: 0.3108  | val_0_rmse: 0.53206 | val_1_rmse: 0.5418  |  0:02:26s
epoch 73 | loss: 0.30387 | val_0_rmse: 0.53021 | val_1_rmse: 0.54144 |  0:02:28s
epoch 74 | loss: 0.30173 | val_0_rmse: 0.54542 | val_1_rmse: 0.55896 |  0:02:30s
epoch 75 | loss: 0.30283 | val_0_rmse: 0.54133 | val_1_rmse: 0.54855 |  0:02:32s
epoch 76 | loss: 0.31381 | val_0_rmse: 0.53626 | val_1_rmse: 0.54691 |  0:02:34s
epoch 77 | loss: 0.31232 | val_0_rmse: 0.55213 | val_1_rmse: 0.56082 |  0:02:36s
epoch 78 | loss: 0.31791 | val_0_rmse: 0.53421 | val_1_rmse: 0.54701 |  0:02:38s
epoch 79 | loss: 0.30336 | val_0_rmse: 0.54697 | val_1_rmse: 0.55969 |  0:02:40s
epoch 80 | loss: 0.30925 | val_0_rmse: 0.55212 | val_1_rmse: 0.55898 |  0:02:42s
epoch 81 | loss: 0.31709 | val_0_rmse: 0.55121 | val_1_rmse: 0.56409 |  0:02:44s
epoch 82 | loss: 0.31872 | val_0_rmse: 0.55081 | val_1_rmse: 0.56273 |  0:02:46s
epoch 83 | loss: 0.31928 | val_0_rmse: 0.53922 | val_1_rmse: 0.55404 |  0:02:48s
epoch 84 | loss: 0.30553 | val_0_rmse: 0.5436  | val_1_rmse: 0.55188 |  0:02:50s
epoch 85 | loss: 0.30533 | val_0_rmse: 0.56206 | val_1_rmse: 0.57689 |  0:02:52s
epoch 86 | loss: 0.30699 | val_0_rmse: 0.53054 | val_1_rmse: 0.53711 |  0:02:54s
epoch 87 | loss: 0.30623 | val_0_rmse: 0.5517  | val_1_rmse: 0.5688  |  0:02:56s
epoch 88 | loss: 0.30772 | val_0_rmse: 0.52586 | val_1_rmse: 0.53628 |  0:02:58s
epoch 89 | loss: 0.30918 | val_0_rmse: 0.55317 | val_1_rmse: 0.55779 |  0:03:00s
epoch 90 | loss: 0.31428 | val_0_rmse: 0.53324 | val_1_rmse: 0.54389 |  0:03:03s
epoch 91 | loss: 0.30997 | val_0_rmse: 0.56423 | val_1_rmse: 0.57366 |  0:03:05s
epoch 92 | loss: 0.30793 | val_0_rmse: 0.54303 | val_1_rmse: 0.55569 |  0:03:07s
epoch 93 | loss: 0.30755 | val_0_rmse: 0.54064 | val_1_rmse: 0.54928 |  0:03:09s
epoch 94 | loss: 0.30344 | val_0_rmse: 0.52376 | val_1_rmse: 0.53311 |  0:03:11s
epoch 95 | loss: 0.30758 | val_0_rmse: 0.54378 | val_1_rmse: 0.55504 |  0:03:13s
epoch 96 | loss: 0.33824 | val_0_rmse: 0.60265 | val_1_rmse: 0.61517 |  0:03:15s
epoch 97 | loss: 0.33328 | val_0_rmse: 0.54464 | val_1_rmse: 0.54887 |  0:03:17s
epoch 98 | loss: 0.30707 | val_0_rmse: 0.54037 | val_1_rmse: 0.54974 |  0:03:19s
epoch 99 | loss: 0.30621 | val_0_rmse: 0.53996 | val_1_rmse: 0.55276 |  0:03:21s
epoch 100| loss: 0.30179 | val_0_rmse: 0.5255  | val_1_rmse: 0.53628 |  0:03:23s
epoch 101| loss: 0.29946 | val_0_rmse: 0.5264  | val_1_rmse: 0.53686 |  0:03:25s
epoch 102| loss: 0.29978 | val_0_rmse: 0.54058 | val_1_rmse: 0.55333 |  0:03:27s
epoch 103| loss: 0.29821 | val_0_rmse: 0.5389  | val_1_rmse: 0.55711 |  0:03:29s
epoch 104| loss: 0.30278 | val_0_rmse: 0.52942 | val_1_rmse: 0.5381  |  0:03:31s
epoch 105| loss: 0.30473 | val_0_rmse: 0.54062 | val_1_rmse: 0.55725 |  0:03:33s
epoch 106| loss: 0.29944 | val_0_rmse: 0.53663 | val_1_rmse: 0.54564 |  0:03:35s
epoch 107| loss: 0.29765 | val_0_rmse: 0.52977 | val_1_rmse: 0.54442 |  0:03:37s
epoch 108| loss: 0.29573 | val_0_rmse: 0.52623 | val_1_rmse: 0.53775 |  0:03:39s
epoch 109| loss: 0.30695 | val_0_rmse: 0.53073 | val_1_rmse: 0.53815 |  0:03:41s
epoch 110| loss: 0.3009  | val_0_rmse: 0.54187 | val_1_rmse: 0.55024 |  0:03:43s
epoch 111| loss: 0.30338 | val_0_rmse: 0.53968 | val_1_rmse: 0.55454 |  0:03:45s
epoch 112| loss: 0.29912 | val_0_rmse: 0.53642 | val_1_rmse: 0.55059 |  0:03:47s
epoch 113| loss: 0.30086 | val_0_rmse: 0.5273  | val_1_rmse: 0.54129 |  0:03:49s
epoch 114| loss: 0.3023  | val_0_rmse: 0.54431 | val_1_rmse: 0.55686 |  0:03:51s
epoch 115| loss: 0.30525 | val_0_rmse: 0.52669 | val_1_rmse: 0.53791 |  0:03:53s
epoch 116| loss: 0.30306 | val_0_rmse: 0.52702 | val_1_rmse: 0.53796 |  0:03:55s
epoch 117| loss: 0.30241 | val_0_rmse: 0.54771 | val_1_rmse: 0.5679  |  0:03:57s
epoch 118| loss: 0.30133 | val_0_rmse: 0.52414 | val_1_rmse: 0.53312 |  0:03:59s
epoch 119| loss: 0.29764 | val_0_rmse: 0.53719 | val_1_rmse: 0.55134 |  0:04:01s
epoch 120| loss: 0.30057 | val_0_rmse: 0.56363 | val_1_rmse: 0.57028 |  0:04:03s
epoch 121| loss: 0.31799 | val_0_rmse: 0.56483 | val_1_rmse: 0.58088 |  0:04:05s
epoch 122| loss: 0.31207 | val_0_rmse: 0.53834 | val_1_rmse: 0.5464  |  0:04:07s
epoch 123| loss: 0.30395 | val_0_rmse: 0.53726 | val_1_rmse: 0.5513  |  0:04:09s
epoch 124| loss: 0.30345 | val_0_rmse: 0.53724 | val_1_rmse: 0.54869 |  0:04:11s

Early stopping occured at epoch 124 with best_epoch = 94 and best_val_1_rmse = 0.53311
Best weights from best epoch are automatically used!
ended training at: 15:32:56
Feature importance:
[('Area', 0.33590924550563744), ('Baths', 0.003988994650266511), ('Beds', 0.06105512056689292), ('Latitude', 0.3231780636762488), ('Longitude', 0.24570114860752), ('Month', 0.0), ('Year', 0.03016742699343432)]
Mean squared error is of 8385683160.02597
Mean absolute error:66140.99515506005
MAPE:0.17665878161443868
R2 score:0.7242701520599979
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:32:56
epoch 0  | loss: 0.65547 | val_0_rmse: 0.69678 | val_1_rmse: 0.71548 |  0:00:02s
epoch 1  | loss: 0.43333 | val_0_rmse: 0.62327 | val_1_rmse: 0.63979 |  0:00:04s
epoch 2  | loss: 0.40879 | val_0_rmse: 0.63441 | val_1_rmse: 0.65097 |  0:00:06s
epoch 3  | loss: 0.39943 | val_0_rmse: 0.62332 | val_1_rmse: 0.64383 |  0:00:08s
epoch 4  | loss: 0.39531 | val_0_rmse: 0.6087  | val_1_rmse: 0.62433 |  0:00:10s
epoch 5  | loss: 0.38452 | val_0_rmse: 0.59113 | val_1_rmse: 0.60783 |  0:00:12s
epoch 6  | loss: 0.3809  | val_0_rmse: 0.63699 | val_1_rmse: 0.65318 |  0:00:14s
epoch 7  | loss: 0.38264 | val_0_rmse: 0.66553 | val_1_rmse: 0.68152 |  0:00:16s
epoch 8  | loss: 0.38127 | val_0_rmse: 0.58989 | val_1_rmse: 0.60633 |  0:00:18s
epoch 9  | loss: 0.35784 | val_0_rmse: 0.57405 | val_1_rmse: 0.58868 |  0:00:20s
epoch 10 | loss: 0.36121 | val_0_rmse: 0.58196 | val_1_rmse: 0.60064 |  0:00:22s
epoch 11 | loss: 0.34879 | val_0_rmse: 0.69931 | val_1_rmse: 0.70827 |  0:00:24s
epoch 12 | loss: 0.35164 | val_0_rmse: 0.58992 | val_1_rmse: 0.60555 |  0:00:26s
epoch 13 | loss: 0.32867 | val_0_rmse: 0.56804 | val_1_rmse: 0.57772 |  0:00:28s
epoch 14 | loss: 0.34365 | val_0_rmse: 0.58229 | val_1_rmse: 0.60151 |  0:00:30s
epoch 15 | loss: 0.3297  | val_0_rmse: 0.5414  | val_1_rmse: 0.558   |  0:00:32s
epoch 16 | loss: 0.31435 | val_0_rmse: 0.53646 | val_1_rmse: 0.54951 |  0:00:34s
epoch 17 | loss: 0.31133 | val_0_rmse: 0.53816 | val_1_rmse: 0.55511 |  0:00:36s
epoch 18 | loss: 0.30653 | val_0_rmse: 0.54201 | val_1_rmse: 0.55819 |  0:00:38s
epoch 19 | loss: 0.30724 | val_0_rmse: 0.53712 | val_1_rmse: 0.55227 |  0:00:40s
epoch 20 | loss: 0.30455 | val_0_rmse: 0.5328  | val_1_rmse: 0.54642 |  0:00:42s
epoch 21 | loss: 0.29864 | val_0_rmse: 0.53481 | val_1_rmse: 0.55049 |  0:00:44s
epoch 22 | loss: 0.29765 | val_0_rmse: 0.55076 | val_1_rmse: 0.57096 |  0:00:46s
epoch 23 | loss: 0.29932 | val_0_rmse: 0.52656 | val_1_rmse: 0.54625 |  0:00:48s
epoch 24 | loss: 0.29986 | val_0_rmse: 0.5339  | val_1_rmse: 0.5532  |  0:00:50s
epoch 25 | loss: 0.29733 | val_0_rmse: 0.52815 | val_1_rmse: 0.5464  |  0:00:52s
epoch 26 | loss: 0.29824 | val_0_rmse: 0.52551 | val_1_rmse: 0.5412  |  0:00:54s
epoch 27 | loss: 0.29553 | val_0_rmse: 0.52116 | val_1_rmse: 0.53893 |  0:00:56s
epoch 28 | loss: 0.28197 | val_0_rmse: 0.51836 | val_1_rmse: 0.53264 |  0:00:58s
epoch 29 | loss: 0.29047 | val_0_rmse: 0.55354 | val_1_rmse: 0.56576 |  0:01:00s
epoch 30 | loss: 0.29505 | val_0_rmse: 0.54078 | val_1_rmse: 0.55334 |  0:01:02s
epoch 31 | loss: 0.28836 | val_0_rmse: 0.50842 | val_1_rmse: 0.52569 |  0:01:04s
epoch 32 | loss: 0.28775 | val_0_rmse: 0.5157  | val_1_rmse: 0.53158 |  0:01:06s
epoch 33 | loss: 0.28651 | val_0_rmse: 0.52811 | val_1_rmse: 0.54527 |  0:01:08s
epoch 34 | loss: 0.2815  | val_0_rmse: 0.53845 | val_1_rmse: 0.5577  |  0:01:10s
epoch 35 | loss: 0.28423 | val_0_rmse: 0.52633 | val_1_rmse: 0.54633 |  0:01:12s
epoch 36 | loss: 0.28684 | val_0_rmse: 0.51759 | val_1_rmse: 0.53702 |  0:01:14s
epoch 37 | loss: 0.28663 | val_0_rmse: 0.51719 | val_1_rmse: 0.53382 |  0:01:16s
epoch 38 | loss: 0.28157 | val_0_rmse: 0.51912 | val_1_rmse: 0.53467 |  0:01:18s
epoch 39 | loss: 0.28543 | val_0_rmse: 0.5295  | val_1_rmse: 0.54472 |  0:01:20s
epoch 40 | loss: 0.28546 | val_0_rmse: 0.52732 | val_1_rmse: 0.5421  |  0:01:22s
epoch 41 | loss: 0.28577 | val_0_rmse: 0.5115  | val_1_rmse: 0.52943 |  0:01:24s
epoch 42 | loss: 0.28483 | val_0_rmse: 0.53929 | val_1_rmse: 0.55722 |  0:01:26s
epoch 43 | loss: 0.28088 | val_0_rmse: 0.55426 | val_1_rmse: 0.56909 |  0:01:28s
epoch 44 | loss: 0.27819 | val_0_rmse: 0.53325 | val_1_rmse: 0.55197 |  0:01:30s
epoch 45 | loss: 0.27684 | val_0_rmse: 0.51824 | val_1_rmse: 0.53761 |  0:01:32s
epoch 46 | loss: 0.2807  | val_0_rmse: 0.52259 | val_1_rmse: 0.54074 |  0:01:34s
epoch 47 | loss: 0.27088 | val_0_rmse: 0.50646 | val_1_rmse: 0.5258  |  0:01:36s
epoch 48 | loss: 0.27456 | val_0_rmse: 0.54451 | val_1_rmse: 0.56244 |  0:01:38s
epoch 49 | loss: 0.28014 | val_0_rmse: 0.51218 | val_1_rmse: 0.53209 |  0:01:40s
epoch 50 | loss: 0.27404 | val_0_rmse: 0.49815 | val_1_rmse: 0.51691 |  0:01:42s
epoch 51 | loss: 0.27228 | val_0_rmse: 0.53536 | val_1_rmse: 0.55441 |  0:01:44s
epoch 52 | loss: 0.27183 | val_0_rmse: 0.54526 | val_1_rmse: 0.56268 |  0:01:46s
epoch 53 | loss: 0.283   | val_0_rmse: 0.54021 | val_1_rmse: 0.55759 |  0:01:48s
epoch 54 | loss: 0.28257 | val_0_rmse: 0.55247 | val_1_rmse: 0.56965 |  0:01:50s
epoch 55 | loss: 0.27795 | val_0_rmse: 0.5127  | val_1_rmse: 0.53065 |  0:01:52s
epoch 56 | loss: 0.2801  | val_0_rmse: 0.5465  | val_1_rmse: 0.56254 |  0:01:54s
epoch 57 | loss: 0.275   | val_0_rmse: 0.50818 | val_1_rmse: 0.52761 |  0:01:56s
epoch 58 | loss: 0.27616 | val_0_rmse: 0.51153 | val_1_rmse: 0.53275 |  0:01:58s
epoch 59 | loss: 0.27591 | val_0_rmse: 0.52585 | val_1_rmse: 0.54466 |  0:02:00s
epoch 60 | loss: 0.27996 | val_0_rmse: 0.57268 | val_1_rmse: 0.59094 |  0:02:02s
epoch 61 | loss: 0.2755  | val_0_rmse: 0.50304 | val_1_rmse: 0.52406 |  0:02:04s
epoch 62 | loss: 0.27214 | val_0_rmse: 0.5604  | val_1_rmse: 0.57339 |  0:02:06s
epoch 63 | loss: 0.27458 | val_0_rmse: 0.50747 | val_1_rmse: 0.52443 |  0:02:08s
epoch 64 | loss: 0.27228 | val_0_rmse: 0.49327 | val_1_rmse: 0.51296 |  0:02:10s
epoch 65 | loss: 0.27214 | val_0_rmse: 0.50003 | val_1_rmse: 0.51642 |  0:02:12s
epoch 66 | loss: 0.27726 | val_0_rmse: 0.5129  | val_1_rmse: 0.53213 |  0:02:14s
epoch 67 | loss: 0.27133 | val_0_rmse: 0.50473 | val_1_rmse: 0.52268 |  0:02:16s
epoch 68 | loss: 0.26794 | val_0_rmse: 0.50779 | val_1_rmse: 0.52591 |  0:02:18s
epoch 69 | loss: 0.27188 | val_0_rmse: 0.49356 | val_1_rmse: 0.51184 |  0:02:21s
epoch 70 | loss: 0.27266 | val_0_rmse: 0.53106 | val_1_rmse: 0.55356 |  0:02:23s
epoch 71 | loss: 0.27306 | val_0_rmse: 0.508   | val_1_rmse: 0.52587 |  0:02:24s
epoch 72 | loss: 0.27084 | val_0_rmse: 0.51443 | val_1_rmse: 0.53531 |  0:02:26s
epoch 73 | loss: 0.268   | val_0_rmse: 0.49248 | val_1_rmse: 0.5123  |  0:02:28s
epoch 74 | loss: 0.26768 | val_0_rmse: 0.51702 | val_1_rmse: 0.53782 |  0:02:31s
epoch 75 | loss: 0.27755 | val_0_rmse: 0.51579 | val_1_rmse: 0.53672 |  0:02:33s
epoch 76 | loss: 0.27222 | val_0_rmse: 0.49532 | val_1_rmse: 0.51911 |  0:02:35s
epoch 77 | loss: 0.26651 | val_0_rmse: 0.50811 | val_1_rmse: 0.53044 |  0:02:37s
epoch 78 | loss: 0.27446 | val_0_rmse: 0.49889 | val_1_rmse: 0.5217  |  0:02:39s
epoch 79 | loss: 0.26385 | val_0_rmse: 0.51834 | val_1_rmse: 0.53598 |  0:02:41s
epoch 80 | loss: 0.26916 | val_0_rmse: 0.49788 | val_1_rmse: 0.51653 |  0:02:43s
epoch 81 | loss: 0.27101 | val_0_rmse: 0.49501 | val_1_rmse: 0.5169  |  0:02:45s
epoch 82 | loss: 0.2714  | val_0_rmse: 0.50451 | val_1_rmse: 0.52203 |  0:02:47s
epoch 83 | loss: 0.27075 | val_0_rmse: 0.50051 | val_1_rmse: 0.52144 |  0:02:49s
epoch 84 | loss: 0.26204 | val_0_rmse: 0.50226 | val_1_rmse: 0.52116 |  0:02:51s
epoch 85 | loss: 0.26951 | val_0_rmse: 0.51217 | val_1_rmse: 0.53285 |  0:02:53s
epoch 86 | loss: 0.26847 | val_0_rmse: 0.55083 | val_1_rmse: 0.56878 |  0:02:55s
epoch 87 | loss: 0.26725 | val_0_rmse: 0.52423 | val_1_rmse: 0.54043 |  0:02:57s
epoch 88 | loss: 0.2666  | val_0_rmse: 0.48995 | val_1_rmse: 0.51263 |  0:02:59s
epoch 89 | loss: 0.26064 | val_0_rmse: 0.51381 | val_1_rmse: 0.53198 |  0:03:01s
epoch 90 | loss: 0.26341 | val_0_rmse: 0.57216 | val_1_rmse: 0.58817 |  0:03:03s
epoch 91 | loss: 0.26057 | val_0_rmse: 0.49767 | val_1_rmse: 0.51663 |  0:03:05s
epoch 92 | loss: 0.26384 | val_0_rmse: 0.53383 | val_1_rmse: 0.5531  |  0:03:07s
epoch 93 | loss: 0.26265 | val_0_rmse: 0.48589 | val_1_rmse: 0.50949 |  0:03:09s
epoch 94 | loss: 0.26731 | val_0_rmse: 0.54411 | val_1_rmse: 0.564   |  0:03:11s
epoch 95 | loss: 0.25937 | val_0_rmse: 0.4994  | val_1_rmse: 0.52122 |  0:03:13s
epoch 96 | loss: 0.26362 | val_0_rmse: 0.50514 | val_1_rmse: 0.52404 |  0:03:15s
epoch 97 | loss: 0.27142 | val_0_rmse: 0.49533 | val_1_rmse: 0.51608 |  0:03:17s
epoch 98 | loss: 0.25857 | val_0_rmse: 0.48808 | val_1_rmse: 0.51139 |  0:03:19s
epoch 99 | loss: 0.25896 | val_0_rmse: 0.52407 | val_1_rmse: 0.54325 |  0:03:21s
epoch 100| loss: 0.26464 | val_0_rmse: 0.50164 | val_1_rmse: 0.52013 |  0:03:23s
epoch 101| loss: 0.26099 | val_0_rmse: 0.55161 | val_1_rmse: 0.57116 |  0:03:25s
epoch 102| loss: 0.26444 | val_0_rmse: 0.54245 | val_1_rmse: 0.55984 |  0:03:27s
epoch 103| loss: 0.25782 | val_0_rmse: 0.49949 | val_1_rmse: 0.52161 |  0:03:29s
epoch 104| loss: 0.26257 | val_0_rmse: 0.50964 | val_1_rmse: 0.5294  |  0:03:31s
epoch 105| loss: 0.25817 | val_0_rmse: 0.51801 | val_1_rmse: 0.53835 |  0:03:33s
epoch 106| loss: 0.26245 | val_0_rmse: 0.57883 | val_1_rmse: 0.59978 |  0:03:35s
epoch 107| loss: 0.26597 | val_0_rmse: 0.49195 | val_1_rmse: 0.51369 |  0:03:37s
epoch 108| loss: 0.25663 | val_0_rmse: 0.50128 | val_1_rmse: 0.52227 |  0:03:39s
epoch 109| loss: 0.25624 | val_0_rmse: 0.54997 | val_1_rmse: 0.56648 |  0:03:41s
epoch 110| loss: 0.25595 | val_0_rmse: 0.50717 | val_1_rmse: 0.52928 |  0:03:43s
epoch 111| loss: 0.26372 | val_0_rmse: 0.50999 | val_1_rmse: 0.53053 |  0:03:45s
epoch 112| loss: 0.26394 | val_0_rmse: 0.52614 | val_1_rmse: 0.54919 |  0:03:47s
epoch 113| loss: 0.26256 | val_0_rmse: 0.5113  | val_1_rmse: 0.5301  |  0:03:49s
epoch 114| loss: 0.26256 | val_0_rmse: 0.69293 | val_1_rmse: 0.70042 |  0:03:51s
epoch 115| loss: 0.26713 | val_0_rmse: 0.50918 | val_1_rmse: 0.52869 |  0:03:53s
epoch 116| loss: 0.25843 | val_0_rmse: 0.50528 | val_1_rmse: 0.52355 |  0:03:55s
epoch 117| loss: 0.25621 | val_0_rmse: 0.51358 | val_1_rmse: 0.53383 |  0:03:57s
epoch 118| loss: 0.26135 | val_0_rmse: 0.49133 | val_1_rmse: 0.51529 |  0:03:59s
epoch 119| loss: 0.25401 | val_0_rmse: 0.49298 | val_1_rmse: 0.514   |  0:04:01s
epoch 120| loss: 0.25404 | val_0_rmse: 0.48087 | val_1_rmse: 0.50752 |  0:04:03s
epoch 121| loss: 0.26076 | val_0_rmse: 0.48961 | val_1_rmse: 0.51394 |  0:04:05s
epoch 122| loss: 0.2541  | val_0_rmse: 0.48336 | val_1_rmse: 0.50629 |  0:04:07s
epoch 123| loss: 0.25104 | val_0_rmse: 0.5326  | val_1_rmse: 0.5531  |  0:04:09s
epoch 124| loss: 0.25606 | val_0_rmse: 0.51009 | val_1_rmse: 0.53041 |  0:04:11s
epoch 125| loss: 0.25557 | val_0_rmse: 0.47741 | val_1_rmse: 0.50282 |  0:04:13s
epoch 126| loss: 0.2537  | val_0_rmse: 0.49277 | val_1_rmse: 0.51488 |  0:04:15s
epoch 127| loss: 0.25498 | val_0_rmse: 0.49506 | val_1_rmse: 0.51692 |  0:04:17s
epoch 128| loss: 0.25275 | val_0_rmse: 0.50803 | val_1_rmse: 0.53438 |  0:04:19s
epoch 129| loss: 0.25086 | val_0_rmse: 0.48429 | val_1_rmse: 0.5096  |  0:04:21s
epoch 130| loss: 0.25364 | val_0_rmse: 0.47634 | val_1_rmse: 0.50198 |  0:04:23s
epoch 131| loss: 0.25021 | val_0_rmse: 0.48483 | val_1_rmse: 0.50847 |  0:04:25s
epoch 132| loss: 0.25789 | val_0_rmse: 0.47962 | val_1_rmse: 0.50745 |  0:04:27s
epoch 133| loss: 0.2578  | val_0_rmse: 0.52288 | val_1_rmse: 0.54445 |  0:04:29s
epoch 134| loss: 0.25686 | val_0_rmse: 0.47598 | val_1_rmse: 0.50433 |  0:04:31s
epoch 135| loss: 0.25477 | val_0_rmse: 0.48471 | val_1_rmse: 0.51084 |  0:04:33s
epoch 136| loss: 0.24482 | val_0_rmse: 0.4741  | val_1_rmse: 0.5031  |  0:04:35s
epoch 137| loss: 0.24994 | val_0_rmse: 0.50082 | val_1_rmse: 0.52326 |  0:04:37s
epoch 138| loss: 0.25435 | val_0_rmse: 0.48594 | val_1_rmse: 0.50565 |  0:04:39s
epoch 139| loss: 0.25086 | val_0_rmse: 0.48034 | val_1_rmse: 0.5052  |  0:04:41s
epoch 140| loss: 0.24796 | val_0_rmse: 0.49022 | val_1_rmse: 0.5117  |  0:04:43s
epoch 141| loss: 0.25119 | val_0_rmse: 0.50901 | val_1_rmse: 0.53459 |  0:04:45s
epoch 142| loss: 0.25645 | val_0_rmse: 0.48783 | val_1_rmse: 0.51383 |  0:04:47s
epoch 143| loss: 0.25536 | val_0_rmse: 0.47653 | val_1_rmse: 0.50397 |  0:04:49s
epoch 144| loss: 0.24712 | val_0_rmse: 0.49727 | val_1_rmse: 0.52155 |  0:04:51s
epoch 145| loss: 0.25352 | val_0_rmse: 0.51295 | val_1_rmse: 0.53873 |  0:04:53s
epoch 146| loss: 0.25145 | val_0_rmse: 0.48857 | val_1_rmse: 0.51415 |  0:04:55s
epoch 147| loss: 0.24901 | val_0_rmse: 0.49633 | val_1_rmse: 0.52    |  0:04:57s
epoch 148| loss: 0.25166 | val_0_rmse: 0.47669 | val_1_rmse: 0.50379 |  0:04:59s
epoch 149| loss: 0.25762 | val_0_rmse: 0.47975 | val_1_rmse: 0.50592 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 130 and best_val_1_rmse = 0.50198
Best weights from best epoch are automatically used!
ended training at: 15:37:59
Feature importance:
[('Area', 0.41468841847372195), ('Baths', 0.0), ('Beds', 0.14529868981655283), ('Latitude', 0.2024380557452824), ('Longitude', 0.1799486824174622), ('Month', 0.057626153546980585), ('Year', 0.0)]
Mean squared error is of 7199153811.471695
Mean absolute error:60982.333677095674
MAPE:0.16600294734406248
R2 score:0.7681232262513589
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:37:59
epoch 0  | loss: 0.68039 | val_0_rmse: 0.76802 | val_1_rmse: 0.76307 |  0:00:02s
epoch 1  | loss: 0.46525 | val_0_rmse: 0.66021 | val_1_rmse: 0.66139 |  0:00:03s
epoch 2  | loss: 0.40157 | val_0_rmse: 0.63495 | val_1_rmse: 0.63188 |  0:00:05s
epoch 3  | loss: 0.39188 | val_0_rmse: 0.72154 | val_1_rmse: 0.72748 |  0:00:08s
epoch 4  | loss: 0.37896 | val_0_rmse: 0.60238 | val_1_rmse: 0.61085 |  0:00:10s
epoch 5  | loss: 0.35661 | val_0_rmse: 0.57235 | val_1_rmse: 0.57729 |  0:00:12s
epoch 6  | loss: 0.34316 | val_0_rmse: 0.56452 | val_1_rmse: 0.57263 |  0:00:14s
epoch 7  | loss: 0.33999 | val_0_rmse: 0.57331 | val_1_rmse: 0.57559 |  0:00:16s
epoch 8  | loss: 0.34044 | val_0_rmse: 0.58292 | val_1_rmse: 0.58985 |  0:00:18s
epoch 9  | loss: 0.33709 | val_0_rmse: 0.5586  | val_1_rmse: 0.56224 |  0:00:20s
epoch 10 | loss: 0.34816 | val_0_rmse: 0.5531  | val_1_rmse: 0.55465 |  0:00:22s
epoch 11 | loss: 0.32819 | val_0_rmse: 0.55131 | val_1_rmse: 0.55473 |  0:00:24s
epoch 12 | loss: 0.33137 | val_0_rmse: 0.56462 | val_1_rmse: 0.57898 |  0:00:26s
epoch 13 | loss: 0.31849 | val_0_rmse: 0.5425  | val_1_rmse: 0.55121 |  0:00:28s
epoch 14 | loss: 0.3226  | val_0_rmse: 0.56724 | val_1_rmse: 0.5696  |  0:00:30s
epoch 15 | loss: 0.32725 | val_0_rmse: 0.54823 | val_1_rmse: 0.55133 |  0:00:32s
epoch 16 | loss: 0.31604 | val_0_rmse: 0.53754 | val_1_rmse: 0.54317 |  0:00:34s
epoch 17 | loss: 0.30995 | val_0_rmse: 0.54841 | val_1_rmse: 0.55091 |  0:00:36s
epoch 18 | loss: 0.31928 | val_0_rmse: 0.55893 | val_1_rmse: 0.5683  |  0:00:38s
epoch 19 | loss: 0.32862 | val_0_rmse: 0.61742 | val_1_rmse: 0.6243  |  0:00:40s
epoch 20 | loss: 0.33937 | val_0_rmse: 0.55701 | val_1_rmse: 0.56009 |  0:00:42s
epoch 21 | loss: 0.32025 | val_0_rmse: 0.54415 | val_1_rmse: 0.54693 |  0:00:44s
epoch 22 | loss: 0.33144 | val_0_rmse: 0.56351 | val_1_rmse: 0.57154 |  0:00:46s
epoch 23 | loss: 0.32985 | val_0_rmse: 0.59396 | val_1_rmse: 0.59949 |  0:00:48s
epoch 24 | loss: 0.3346  | val_0_rmse: 0.54427 | val_1_rmse: 0.55042 |  0:00:50s
epoch 25 | loss: 0.31434 | val_0_rmse: 0.55327 | val_1_rmse: 0.56019 |  0:00:52s
epoch 26 | loss: 0.32168 | val_0_rmse: 0.55171 | val_1_rmse: 0.55761 |  0:00:54s
epoch 27 | loss: 0.31526 | val_0_rmse: 0.54642 | val_1_rmse: 0.55344 |  0:00:56s
epoch 28 | loss: 0.31393 | val_0_rmse: 0.55544 | val_1_rmse: 0.56541 |  0:00:58s
epoch 29 | loss: 0.32044 | val_0_rmse: 0.54537 | val_1_rmse: 0.54833 |  0:01:00s
epoch 30 | loss: 0.30888 | val_0_rmse: 0.55933 | val_1_rmse: 0.56559 |  0:01:02s
epoch 31 | loss: 0.33869 | val_0_rmse: 0.56805 | val_1_rmse: 0.57142 |  0:01:04s
epoch 32 | loss: 0.33829 | val_0_rmse: 0.59482 | val_1_rmse: 0.59836 |  0:01:06s
epoch 33 | loss: 0.35019 | val_0_rmse: 0.55131 | val_1_rmse: 0.55622 |  0:01:08s
epoch 34 | loss: 0.32845 | val_0_rmse: 0.54215 | val_1_rmse: 0.54769 |  0:01:10s
epoch 35 | loss: 0.32881 | val_0_rmse: 0.54131 | val_1_rmse: 0.54622 |  0:01:12s
epoch 36 | loss: 0.31734 | val_0_rmse: 0.55464 | val_1_rmse: 0.56021 |  0:01:14s
epoch 37 | loss: 0.31823 | val_0_rmse: 0.53854 | val_1_rmse: 0.5409  |  0:01:16s
epoch 38 | loss: 0.30888 | val_0_rmse: 0.5347  | val_1_rmse: 0.54099 |  0:01:18s
epoch 39 | loss: 0.31468 | val_0_rmse: 0.53294 | val_1_rmse: 0.53712 |  0:01:20s
epoch 40 | loss: 0.30637 | val_0_rmse: 0.54773 | val_1_rmse: 0.55716 |  0:01:22s
epoch 41 | loss: 0.31046 | val_0_rmse: 0.53424 | val_1_rmse: 0.53956 |  0:01:24s
epoch 42 | loss: 0.31159 | val_0_rmse: 0.55223 | val_1_rmse: 0.56036 |  0:01:26s
epoch 43 | loss: 0.30824 | val_0_rmse: 0.53256 | val_1_rmse: 0.53895 |  0:01:28s
epoch 44 | loss: 0.31661 | val_0_rmse: 0.57503 | val_1_rmse: 0.57968 |  0:01:30s
epoch 45 | loss: 0.31995 | val_0_rmse: 0.57307 | val_1_rmse: 0.57659 |  0:01:32s
epoch 46 | loss: 0.33008 | val_0_rmse: 0.5483  | val_1_rmse: 0.5536  |  0:01:34s
epoch 47 | loss: 0.32205 | val_0_rmse: 0.57451 | val_1_rmse: 0.57599 |  0:01:36s
epoch 48 | loss: 0.32841 | val_0_rmse: 0.555   | val_1_rmse: 0.56347 |  0:01:38s
epoch 49 | loss: 0.3152  | val_0_rmse: 0.5541  | val_1_rmse: 0.55679 |  0:01:40s
epoch 50 | loss: 0.33446 | val_0_rmse: 0.55179 | val_1_rmse: 0.55578 |  0:01:42s
epoch 51 | loss: 0.31395 | val_0_rmse: 0.52406 | val_1_rmse: 0.53071 |  0:01:44s
epoch 52 | loss: 0.30303 | val_0_rmse: 0.56207 | val_1_rmse: 0.56725 |  0:01:46s
epoch 53 | loss: 0.31846 | val_0_rmse: 0.55918 | val_1_rmse: 0.56211 |  0:01:48s
epoch 54 | loss: 0.30658 | val_0_rmse: 0.553   | val_1_rmse: 0.55557 |  0:01:50s
epoch 55 | loss: 0.30375 | val_0_rmse: 0.53634 | val_1_rmse: 0.54131 |  0:01:52s
epoch 56 | loss: 0.29323 | val_0_rmse: 0.5493  | val_1_rmse: 0.5581  |  0:01:54s
epoch 57 | loss: 0.29189 | val_0_rmse: 0.51902 | val_1_rmse: 0.52663 |  0:01:56s
epoch 58 | loss: 0.30954 | val_0_rmse: 0.52528 | val_1_rmse: 0.52863 |  0:01:58s
epoch 59 | loss: 0.30341 | val_0_rmse: 0.57272 | val_1_rmse: 0.57984 |  0:02:00s
epoch 60 | loss: 0.31518 | val_0_rmse: 0.56016 | val_1_rmse: 0.56549 |  0:02:02s
epoch 61 | loss: 0.29599 | val_0_rmse: 0.53342 | val_1_rmse: 0.53878 |  0:02:04s
epoch 62 | loss: 0.30152 | val_0_rmse: 0.55413 | val_1_rmse: 0.55731 |  0:02:06s
epoch 63 | loss: 0.30626 | val_0_rmse: 0.55115 | val_1_rmse: 0.55657 |  0:02:08s
epoch 64 | loss: 0.35354 | val_0_rmse: 0.60568 | val_1_rmse: 0.60899 |  0:02:10s
epoch 65 | loss: 0.3382  | val_0_rmse: 0.56701 | val_1_rmse: 0.57212 |  0:02:12s
epoch 66 | loss: 0.32356 | val_0_rmse: 0.556   | val_1_rmse: 0.56224 |  0:02:14s
epoch 67 | loss: 0.3073  | val_0_rmse: 0.54177 | val_1_rmse: 0.54782 |  0:02:16s
epoch 68 | loss: 0.3089  | val_0_rmse: 0.55824 | val_1_rmse: 0.56377 |  0:02:18s
epoch 69 | loss: 0.31179 | val_0_rmse: 0.53935 | val_1_rmse: 0.54497 |  0:02:20s
epoch 70 | loss: 0.30694 | val_0_rmse: 0.52429 | val_1_rmse: 0.52749 |  0:02:22s
epoch 71 | loss: 0.30123 | val_0_rmse: 0.54602 | val_1_rmse: 0.55072 |  0:02:24s
epoch 72 | loss: 0.29734 | val_0_rmse: 0.56619 | val_1_rmse: 0.56766 |  0:02:26s
epoch 73 | loss: 0.30411 | val_0_rmse: 0.52045 | val_1_rmse: 0.52578 |  0:02:28s
epoch 74 | loss: 0.29751 | val_0_rmse: 0.52115 | val_1_rmse: 0.52559 |  0:02:30s
epoch 75 | loss: 0.29131 | val_0_rmse: 0.53026 | val_1_rmse: 0.53138 |  0:02:32s
epoch 76 | loss: 0.29192 | val_0_rmse: 0.51775 | val_1_rmse: 0.52573 |  0:02:34s
epoch 77 | loss: 0.30182 | val_0_rmse: 0.57138 | val_1_rmse: 0.58372 |  0:02:36s
epoch 78 | loss: 0.29481 | val_0_rmse: 0.53532 | val_1_rmse: 0.54299 |  0:02:38s
epoch 79 | loss: 0.29755 | val_0_rmse: 0.53477 | val_1_rmse: 0.53985 |  0:02:40s
epoch 80 | loss: 0.2976  | val_0_rmse: 0.54794 | val_1_rmse: 0.55298 |  0:02:42s
epoch 81 | loss: 0.30657 | val_0_rmse: 0.536   | val_1_rmse: 0.54192 |  0:02:44s
epoch 82 | loss: 0.30218 | val_0_rmse: 0.52025 | val_1_rmse: 0.52772 |  0:02:46s
epoch 83 | loss: 0.29562 | val_0_rmse: 0.51486 | val_1_rmse: 0.52187 |  0:02:48s
epoch 84 | loss: 0.2888  | val_0_rmse: 0.5225  | val_1_rmse: 0.52806 |  0:02:50s
epoch 85 | loss: 0.29454 | val_0_rmse: 0.53485 | val_1_rmse: 0.54516 |  0:02:52s
epoch 86 | loss: 0.3     | val_0_rmse: 0.53881 | val_1_rmse: 0.54789 |  0:02:54s
epoch 87 | loss: 0.28933 | val_0_rmse: 0.52678 | val_1_rmse: 0.53308 |  0:02:56s
epoch 88 | loss: 0.30027 | val_0_rmse: 0.53337 | val_1_rmse: 0.54324 |  0:02:58s
epoch 89 | loss: 0.3052  | val_0_rmse: 0.52276 | val_1_rmse: 0.52987 |  0:03:00s
epoch 90 | loss: 0.29684 | val_0_rmse: 0.53115 | val_1_rmse: 0.53885 |  0:03:02s
epoch 91 | loss: 0.29118 | val_0_rmse: 0.53303 | val_1_rmse: 0.54172 |  0:03:04s
epoch 92 | loss: 0.29592 | val_0_rmse: 0.52285 | val_1_rmse: 0.53059 |  0:03:06s
epoch 93 | loss: 0.29385 | val_0_rmse: 0.52615 | val_1_rmse: 0.53416 |  0:03:08s
epoch 94 | loss: 0.29951 | val_0_rmse: 0.53397 | val_1_rmse: 0.54143 |  0:03:10s
epoch 95 | loss: 0.29861 | val_0_rmse: 0.5488  | val_1_rmse: 0.55428 |  0:03:12s
epoch 96 | loss: 0.30122 | val_0_rmse: 0.55251 | val_1_rmse: 0.56289 |  0:03:14s
epoch 97 | loss: 0.29998 | val_0_rmse: 0.55751 | val_1_rmse: 0.56359 |  0:03:16s
epoch 98 | loss: 0.29536 | val_0_rmse: 0.52512 | val_1_rmse: 0.53326 |  0:03:18s
epoch 99 | loss: 0.28255 | val_0_rmse: 0.52477 | val_1_rmse: 0.5298  |  0:03:20s
epoch 100| loss: 0.28043 | val_0_rmse: 0.51437 | val_1_rmse: 0.52286 |  0:03:22s
epoch 101| loss: 0.28415 | val_0_rmse: 0.5194  | val_1_rmse: 0.5255  |  0:03:24s
epoch 102| loss: 0.28034 | val_0_rmse: 0.5382  | val_1_rmse: 0.54611 |  0:03:26s
epoch 103| loss: 0.28455 | val_0_rmse: 0.52166 | val_1_rmse: 0.52989 |  0:03:28s
epoch 104| loss: 0.28383 | val_0_rmse: 0.51703 | val_1_rmse: 0.52574 |  0:03:31s
epoch 105| loss: 0.28728 | val_0_rmse: 0.52129 | val_1_rmse: 0.52976 |  0:03:33s
epoch 106| loss: 0.2849  | val_0_rmse: 0.51275 | val_1_rmse: 0.52144 |  0:03:35s
epoch 107| loss: 0.27797 | val_0_rmse: 0.54375 | val_1_rmse: 0.55234 |  0:03:37s
epoch 108| loss: 0.28725 | val_0_rmse: 0.51571 | val_1_rmse: 0.52231 |  0:03:39s
epoch 109| loss: 0.28075 | val_0_rmse: 0.54346 | val_1_rmse: 0.54936 |  0:03:41s
epoch 110| loss: 0.29652 | val_0_rmse: 0.53454 | val_1_rmse: 0.54433 |  0:03:43s
epoch 111| loss: 0.29836 | val_0_rmse: 0.51878 | val_1_rmse: 0.52776 |  0:03:45s
epoch 112| loss: 0.28165 | val_0_rmse: 0.55102 | val_1_rmse: 0.55931 |  0:03:47s
epoch 113| loss: 0.28831 | val_0_rmse: 0.51103 | val_1_rmse: 0.51917 |  0:03:49s
epoch 114| loss: 0.28687 | val_0_rmse: 0.52909 | val_1_rmse: 0.539   |  0:03:51s
epoch 115| loss: 0.29228 | val_0_rmse: 0.52066 | val_1_rmse: 0.5278  |  0:03:53s
epoch 116| loss: 0.28559 | val_0_rmse: 0.57085 | val_1_rmse: 0.57743 |  0:03:55s
epoch 117| loss: 0.29588 | val_0_rmse: 0.5487  | val_1_rmse: 0.55708 |  0:03:57s
epoch 118| loss: 0.29382 | val_0_rmse: 0.52146 | val_1_rmse: 0.52927 |  0:03:59s
epoch 119| loss: 0.28561 | val_0_rmse: 0.51277 | val_1_rmse: 0.52079 |  0:04:01s
epoch 120| loss: 0.27944 | val_0_rmse: 0.51871 | val_1_rmse: 0.52625 |  0:04:03s
epoch 121| loss: 0.28052 | val_0_rmse: 0.51652 | val_1_rmse: 0.52616 |  0:04:05s
epoch 122| loss: 0.27866 | val_0_rmse: 0.51089 | val_1_rmse: 0.51938 |  0:04:07s
epoch 123| loss: 0.30874 | val_0_rmse: 0.5521  | val_1_rmse: 0.55996 |  0:04:09s
epoch 124| loss: 0.29397 | val_0_rmse: 0.51378 | val_1_rmse: 0.52062 |  0:04:11s
epoch 125| loss: 0.27906 | val_0_rmse: 0.51457 | val_1_rmse: 0.52302 |  0:04:13s
epoch 126| loss: 0.27885 | val_0_rmse: 0.50406 | val_1_rmse: 0.51015 |  0:04:15s
epoch 127| loss: 0.29348 | val_0_rmse: 0.55772 | val_1_rmse: 0.56072 |  0:04:17s
epoch 128| loss: 0.28566 | val_0_rmse: 0.51264 | val_1_rmse: 0.52056 |  0:04:19s
epoch 129| loss: 0.27993 | val_0_rmse: 0.50232 | val_1_rmse: 0.50871 |  0:04:21s
epoch 130| loss: 0.28097 | val_0_rmse: 0.53918 | val_1_rmse: 0.54829 |  0:04:23s
epoch 131| loss: 0.28346 | val_0_rmse: 0.50367 | val_1_rmse: 0.50958 |  0:04:25s
epoch 132| loss: 0.28574 | val_0_rmse: 0.52939 | val_1_rmse: 0.53564 |  0:04:27s
epoch 133| loss: 0.2942  | val_0_rmse: 0.60106 | val_1_rmse: 0.60839 |  0:04:29s
epoch 134| loss: 0.29641 | val_0_rmse: 0.52329 | val_1_rmse: 0.53236 |  0:04:31s
epoch 135| loss: 0.28793 | val_0_rmse: 0.52337 | val_1_rmse: 0.52656 |  0:04:33s
epoch 136| loss: 0.2754  | val_0_rmse: 0.52154 | val_1_rmse: 0.52995 |  0:04:35s
epoch 137| loss: 0.27099 | val_0_rmse: 0.5186  | val_1_rmse: 0.52577 |  0:04:37s
epoch 138| loss: 0.28572 | val_0_rmse: 0.5174  | val_1_rmse: 0.52334 |  0:04:39s
epoch 139| loss: 0.27179 | val_0_rmse: 0.50015 | val_1_rmse: 0.50781 |  0:04:41s
epoch 140| loss: 0.27182 | val_0_rmse: 0.50042 | val_1_rmse: 0.50867 |  0:04:43s
epoch 141| loss: 0.27624 | val_0_rmse: 0.49958 | val_1_rmse: 0.50601 |  0:04:45s
epoch 142| loss: 0.27224 | val_0_rmse: 0.49648 | val_1_rmse: 0.50414 |  0:04:47s
epoch 143| loss: 0.271   | val_0_rmse: 0.50877 | val_1_rmse: 0.51512 |  0:04:49s
epoch 144| loss: 0.27447 | val_0_rmse: 0.52091 | val_1_rmse: 0.52898 |  0:04:51s
epoch 145| loss: 0.27265 | val_0_rmse: 0.51159 | val_1_rmse: 0.5175  |  0:04:53s
epoch 146| loss: 0.27339 | val_0_rmse: 0.50448 | val_1_rmse: 0.51174 |  0:04:55s
epoch 147| loss: 0.26967 | val_0_rmse: 0.50419 | val_1_rmse: 0.5138  |  0:04:57s
epoch 148| loss: 0.2694  | val_0_rmse: 0.4956  | val_1_rmse: 0.50202 |  0:04:59s
epoch 149| loss: 0.26948 | val_0_rmse: 0.50462 | val_1_rmse: 0.51226 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.50202
Best weights from best epoch are automatically used!
ended training at: 15:43:01
Feature importance:
[('Area', 0.23576203473516064), ('Baths', 0.24736769088223243), ('Beds', 0.016336833621752065), ('Latitude', 0.22886375904149808), ('Longitude', 0.2716696817193568), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 7092057763.575833
Mean absolute error:61211.784997850686
MAPE:0.1677745358375435
R2 score:0.758954760768958
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:44:34
epoch 0  | loss: 0.8218  | val_0_rmse: 0.83581 | val_1_rmse: 0.83058 |  0:00:02s
epoch 1  | loss: 0.69116 | val_0_rmse: 0.83658 | val_1_rmse: 0.83351 |  0:00:04s
epoch 2  | loss: 0.68384 | val_0_rmse: 0.82715 | val_1_rmse: 0.82463 |  0:00:06s
epoch 3  | loss: 0.67485 | val_0_rmse: 0.81082 | val_1_rmse: 0.80719 |  0:00:08s
epoch 4  | loss: 0.67185 | val_0_rmse: 0.81291 | val_1_rmse: 0.80711 |  0:00:10s
epoch 5  | loss: 0.66461 | val_0_rmse: 0.8092  | val_1_rmse: 0.80342 |  0:00:12s
epoch 6  | loss: 0.66106 | val_0_rmse: 0.80788 | val_1_rmse: 0.80173 |  0:00:14s
epoch 7  | loss: 0.6573  | val_0_rmse: 0.80095 | val_1_rmse: 0.79951 |  0:00:16s
epoch 8  | loss: 0.64931 | val_0_rmse: 0.8014  | val_1_rmse: 0.79521 |  0:00:18s
epoch 9  | loss: 0.64218 | val_0_rmse: 0.78875 | val_1_rmse: 0.78264 |  0:00:20s
epoch 10 | loss: 0.62926 | val_0_rmse: 0.78403 | val_1_rmse: 0.78049 |  0:00:22s
epoch 11 | loss: 0.61803 | val_0_rmse: 0.76985 | val_1_rmse: 0.76548 |  0:00:24s
epoch 12 | loss: 0.61744 | val_0_rmse: 0.7597  | val_1_rmse: 0.75534 |  0:00:26s
epoch 13 | loss: 0.6144  | val_0_rmse: 0.77051 | val_1_rmse: 0.76462 |  0:00:28s
epoch 14 | loss: 0.60956 | val_0_rmse: 0.77188 | val_1_rmse: 0.77006 |  0:00:30s
epoch 15 | loss: 0.60472 | val_0_rmse: 0.76078 | val_1_rmse: 0.75766 |  0:00:32s
epoch 16 | loss: 0.58486 | val_0_rmse: 0.75691 | val_1_rmse: 0.75067 |  0:00:34s
epoch 17 | loss: 0.5787  | val_0_rmse: 0.76493 | val_1_rmse: 0.76642 |  0:00:36s
epoch 18 | loss: 0.58048 | val_0_rmse: 0.74754 | val_1_rmse: 0.74421 |  0:00:38s
epoch 19 | loss: 0.57951 | val_0_rmse: 0.76227 | val_1_rmse: 0.75447 |  0:00:40s
epoch 20 | loss: 0.58083 | val_0_rmse: 0.74423 | val_1_rmse: 0.73804 |  0:00:42s
epoch 21 | loss: 0.56708 | val_0_rmse: 0.73469 | val_1_rmse: 0.73248 |  0:00:44s
epoch 22 | loss: 0.56647 | val_0_rmse: 0.73509 | val_1_rmse: 0.7332  |  0:00:46s
epoch 23 | loss: 0.56576 | val_0_rmse: 0.73446 | val_1_rmse: 0.73326 |  0:00:48s
epoch 24 | loss: 0.55891 | val_0_rmse: 0.73135 | val_1_rmse: 0.7295  |  0:00:50s
epoch 25 | loss: 0.56488 | val_0_rmse: 0.74322 | val_1_rmse: 0.74131 |  0:00:52s
epoch 26 | loss: 0.5645  | val_0_rmse: 0.74475 | val_1_rmse: 0.74201 |  0:00:54s
epoch 27 | loss: 0.56088 | val_0_rmse: 0.73194 | val_1_rmse: 0.72532 |  0:00:56s
epoch 28 | loss: 0.56193 | val_0_rmse: 0.72944 | val_1_rmse: 0.72944 |  0:00:58s
epoch 29 | loss: 0.56465 | val_0_rmse: 0.7324  | val_1_rmse: 0.72946 |  0:01:00s
epoch 30 | loss: 0.56286 | val_0_rmse: 0.76475 | val_1_rmse: 0.76097 |  0:01:02s
epoch 31 | loss: 0.5757  | val_0_rmse: 0.77438 | val_1_rmse: 0.77483 |  0:01:04s
epoch 32 | loss: 0.55483 | val_0_rmse: 0.72658 | val_1_rmse: 0.72437 |  0:01:06s
epoch 33 | loss: 0.54902 | val_0_rmse: 0.72452 | val_1_rmse: 0.71844 |  0:01:08s
epoch 34 | loss: 0.54384 | val_0_rmse: 0.74122 | val_1_rmse: 0.73303 |  0:01:10s
epoch 35 | loss: 0.53856 | val_0_rmse: 0.71672 | val_1_rmse: 0.71327 |  0:01:12s
epoch 36 | loss: 0.54085 | val_0_rmse: 0.73103 | val_1_rmse: 0.72734 |  0:01:14s
epoch 37 | loss: 0.54065 | val_0_rmse: 0.72633 | val_1_rmse: 0.72037 |  0:01:16s
epoch 38 | loss: 0.53819 | val_0_rmse: 0.73512 | val_1_rmse: 0.73319 |  0:01:18s
epoch 39 | loss: 0.55308 | val_0_rmse: 0.70572 | val_1_rmse: 0.70464 |  0:01:20s
epoch 40 | loss: 0.51371 | val_0_rmse: 0.72211 | val_1_rmse: 0.72147 |  0:01:22s
epoch 41 | loss: 0.51258 | val_0_rmse: 0.71528 | val_1_rmse: 0.71546 |  0:01:24s
epoch 42 | loss: 0.52487 | val_0_rmse: 0.70751 | val_1_rmse: 0.70851 |  0:01:26s
epoch 43 | loss: 0.51721 | val_0_rmse: 0.72647 | val_1_rmse: 0.72706 |  0:01:28s
epoch 44 | loss: 0.52303 | val_0_rmse: 0.71695 | val_1_rmse: 0.71114 |  0:01:30s
epoch 45 | loss: 0.50298 | val_0_rmse: 0.71366 | val_1_rmse: 0.70749 |  0:01:32s
epoch 46 | loss: 0.51494 | val_0_rmse: 0.74703 | val_1_rmse: 0.75138 |  0:01:34s
epoch 47 | loss: 0.56716 | val_0_rmse: 0.77911 | val_1_rmse: 0.77541 |  0:01:36s
epoch 48 | loss: 0.56565 | val_0_rmse: 0.7562  | val_1_rmse: 0.75092 |  0:01:38s
epoch 49 | loss: 0.57095 | val_0_rmse: 0.73043 | val_1_rmse: 0.72436 |  0:01:40s
epoch 50 | loss: 0.63142 | val_0_rmse: 0.79021 | val_1_rmse: 0.78419 |  0:01:42s
epoch 51 | loss: 0.57929 | val_0_rmse: 0.74598 | val_1_rmse: 0.74774 |  0:01:44s
epoch 52 | loss: 0.55229 | val_0_rmse: 0.7498  | val_1_rmse: 0.744   |  0:01:46s
epoch 53 | loss: 0.54979 | val_0_rmse: 0.74965 | val_1_rmse: 0.74709 |  0:01:48s
epoch 54 | loss: 0.578   | val_0_rmse: 0.75083 | val_1_rmse: 0.75087 |  0:01:50s
epoch 55 | loss: 0.55965 | val_0_rmse: 0.72412 | val_1_rmse: 0.72039 |  0:01:53s
epoch 56 | loss: 0.53282 | val_0_rmse: 0.71118 | val_1_rmse: 0.70963 |  0:01:55s
epoch 57 | loss: 0.53717 | val_0_rmse: 0.75714 | val_1_rmse: 0.7532  |  0:01:57s
epoch 58 | loss: 0.61967 | val_0_rmse: 0.7787  | val_1_rmse: 0.76989 |  0:01:59s
epoch 59 | loss: 0.60696 | val_0_rmse: 0.8095  | val_1_rmse: 0.80724 |  0:02:01s
epoch 60 | loss: 0.66033 | val_0_rmse: 0.79908 | val_1_rmse: 0.79297 |  0:02:03s
epoch 61 | loss: 0.63606 | val_0_rmse: 0.79468 | val_1_rmse: 0.79226 |  0:02:05s
epoch 62 | loss: 0.62962 | val_0_rmse: 0.77658 | val_1_rmse: 0.77565 |  0:02:07s
epoch 63 | loss: 0.61897 | val_0_rmse: 0.77085 | val_1_rmse: 0.76754 |  0:02:09s
epoch 64 | loss: 0.60934 | val_0_rmse: 0.77399 | val_1_rmse: 0.77528 |  0:02:11s
epoch 65 | loss: 0.61755 | val_0_rmse: 0.78037 | val_1_rmse: 0.77169 |  0:02:13s
epoch 66 | loss: 0.59423 | val_0_rmse: 0.76128 | val_1_rmse: 0.76378 |  0:02:15s
epoch 67 | loss: 0.57635 | val_0_rmse: 0.74758 | val_1_rmse: 0.74927 |  0:02:17s
epoch 68 | loss: 0.56361 | val_0_rmse: 0.74034 | val_1_rmse: 0.7355  |  0:02:19s
epoch 69 | loss: 0.56187 | val_0_rmse: 0.74457 | val_1_rmse: 0.74449 |  0:02:21s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.70464
Best weights from best epoch are automatically used!
ended training at: 15:46:56
Feature importance:
[('Area', 0.3716706830061029), ('Baths', 0.07876068584511761), ('Beds', 0.009283577419098124), ('Latitude', 0.26008511119726146), ('Longitude', 0.27951245448738), ('Month', 0.0006874880450399425), ('Year', 0.0)]
Mean squared error is of 40876746237.20086
Mean absolute error:154796.89585024462
MAPE:0.2824891293066189
R2 score:0.49167622483779627
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:46:56
epoch 0  | loss: 0.81155 | val_0_rmse: 0.84428 | val_1_rmse: 0.85088 |  0:00:02s
epoch 1  | loss: 0.68322 | val_0_rmse: 0.82149 | val_1_rmse: 0.82763 |  0:00:04s
epoch 2  | loss: 0.67039 | val_0_rmse: 0.81663 | val_1_rmse: 0.82623 |  0:00:06s
epoch 3  | loss: 0.6501  | val_0_rmse: 0.7888  | val_1_rmse: 0.79529 |  0:00:08s
epoch 4  | loss: 0.60527 | val_0_rmse: 0.76193 | val_1_rmse: 0.76599 |  0:00:10s
epoch 5  | loss: 0.58589 | val_0_rmse: 0.75701 | val_1_rmse: 0.76186 |  0:00:12s
epoch 6  | loss: 0.56391 | val_0_rmse: 0.75077 | val_1_rmse: 0.75543 |  0:00:14s
epoch 7  | loss: 0.56196 | val_0_rmse: 0.76088 | val_1_rmse: 0.7667  |  0:00:16s
epoch 8  | loss: 0.56178 | val_0_rmse: 0.72841 | val_1_rmse: 0.73653 |  0:00:18s
epoch 9  | loss: 0.54827 | val_0_rmse: 0.72071 | val_1_rmse: 0.73032 |  0:00:20s
epoch 10 | loss: 0.55171 | val_0_rmse: 0.7494  | val_1_rmse: 0.76769 |  0:00:22s
epoch 11 | loss: 0.52145 | val_0_rmse: 0.71235 | val_1_rmse: 0.72813 |  0:00:24s
epoch 12 | loss: 0.51461 | val_0_rmse: 0.71777 | val_1_rmse: 0.72826 |  0:00:26s
epoch 13 | loss: 0.5169  | val_0_rmse: 0.73898 | val_1_rmse: 0.74367 |  0:00:28s
epoch 14 | loss: 0.51738 | val_0_rmse: 0.72528 | val_1_rmse: 0.74029 |  0:00:30s
epoch 15 | loss: 0.52587 | val_0_rmse: 0.78725 | val_1_rmse: 0.79312 |  0:00:32s
epoch 16 | loss: 0.55886 | val_0_rmse: 0.82126 | val_1_rmse: 0.81779 |  0:00:34s
epoch 17 | loss: 0.59503 | val_0_rmse: 0.73087 | val_1_rmse: 0.73745 |  0:00:36s
epoch 18 | loss: 0.51841 | val_0_rmse: 0.71433 | val_1_rmse: 0.7173  |  0:00:38s
epoch 19 | loss: 0.50655 | val_0_rmse: 0.69505 | val_1_rmse: 0.70008 |  0:00:40s
epoch 20 | loss: 0.49294 | val_0_rmse: 0.68659 | val_1_rmse: 0.6897  |  0:00:42s
epoch 21 | loss: 0.46966 | val_0_rmse: 0.6716  | val_1_rmse: 0.68157 |  0:00:44s
epoch 22 | loss: 0.48036 | val_0_rmse: 0.69891 | val_1_rmse: 0.70587 |  0:00:46s
epoch 23 | loss: 0.46935 | val_0_rmse: 0.63861 | val_1_rmse: 0.64627 |  0:00:48s
epoch 24 | loss: 0.44917 | val_0_rmse: 0.64229 | val_1_rmse: 0.64826 |  0:00:50s
epoch 25 | loss: 0.44619 | val_0_rmse: 0.65949 | val_1_rmse: 0.66285 |  0:00:52s
epoch 26 | loss: 0.43875 | val_0_rmse: 0.69156 | val_1_rmse: 0.69883 |  0:00:54s
epoch 27 | loss: 0.43064 | val_0_rmse: 0.63388 | val_1_rmse: 0.63996 |  0:00:56s
epoch 28 | loss: 0.43066 | val_0_rmse: 0.67417 | val_1_rmse: 0.6772  |  0:00:58s
epoch 29 | loss: 0.4326  | val_0_rmse: 0.64294 | val_1_rmse: 0.65153 |  0:01:00s
epoch 30 | loss: 0.42371 | val_0_rmse: 0.66639 | val_1_rmse: 0.67691 |  0:01:02s
epoch 31 | loss: 0.42087 | val_0_rmse: 0.67279 | val_1_rmse: 0.68572 |  0:01:04s
epoch 32 | loss: 0.42895 | val_0_rmse: 0.65515 | val_1_rmse: 0.6611  |  0:01:06s
epoch 33 | loss: 0.41548 | val_0_rmse: 0.67608 | val_1_rmse: 0.68848 |  0:01:08s
epoch 34 | loss: 0.41269 | val_0_rmse: 0.66186 | val_1_rmse: 0.66244 |  0:01:10s
epoch 35 | loss: 0.42826 | val_0_rmse: 0.65354 | val_1_rmse: 0.65359 |  0:01:12s
epoch 36 | loss: 0.41917 | val_0_rmse: 0.63776 | val_1_rmse: 0.6458  |  0:01:14s
epoch 37 | loss: 0.40976 | val_0_rmse: 0.63277 | val_1_rmse: 0.63603 |  0:01:16s
epoch 38 | loss: 0.41821 | val_0_rmse: 0.67127 | val_1_rmse: 0.67582 |  0:01:18s
epoch 39 | loss: 0.42002 | val_0_rmse: 0.6587  | val_1_rmse: 0.66721 |  0:01:20s
epoch 40 | loss: 0.41194 | val_0_rmse: 0.6588  | val_1_rmse: 0.66276 |  0:01:22s
epoch 41 | loss: 0.44398 | val_0_rmse: 0.72167 | val_1_rmse: 0.7209  |  0:01:24s
epoch 42 | loss: 0.43765 | val_0_rmse: 0.64779 | val_1_rmse: 0.65536 |  0:01:26s
epoch 43 | loss: 0.41944 | val_0_rmse: 0.65488 | val_1_rmse: 0.65123 |  0:01:28s
epoch 44 | loss: 0.41474 | val_0_rmse: 0.65364 | val_1_rmse: 0.65673 |  0:01:30s
epoch 45 | loss: 0.41208 | val_0_rmse: 0.6199  | val_1_rmse: 0.6207  |  0:01:32s
epoch 46 | loss: 0.40127 | val_0_rmse: 0.65933 | val_1_rmse: 0.66529 |  0:01:34s
epoch 47 | loss: 0.43336 | val_0_rmse: 0.66962 | val_1_rmse: 0.66748 |  0:01:36s
epoch 48 | loss: 0.42348 | val_0_rmse: 0.64314 | val_1_rmse: 0.6555  |  0:01:38s
epoch 49 | loss: 0.42934 | val_0_rmse: 0.69868 | val_1_rmse: 0.70215 |  0:01:40s
epoch 50 | loss: 0.43584 | val_0_rmse: 0.64286 | val_1_rmse: 0.64896 |  0:01:42s
epoch 51 | loss: 0.4152  | val_0_rmse: 0.63867 | val_1_rmse: 0.63708 |  0:01:44s
epoch 52 | loss: 0.40848 | val_0_rmse: 0.60661 | val_1_rmse: 0.61412 |  0:01:46s
epoch 53 | loss: 0.39555 | val_0_rmse: 0.6614  | val_1_rmse: 0.66545 |  0:01:48s
epoch 54 | loss: 0.40257 | val_0_rmse: 0.6376  | val_1_rmse: 0.63605 |  0:01:50s
epoch 55 | loss: 0.40051 | val_0_rmse: 0.66094 | val_1_rmse: 0.66539 |  0:01:52s
epoch 56 | loss: 0.40806 | val_0_rmse: 0.62564 | val_1_rmse: 0.62215 |  0:01:54s
epoch 57 | loss: 0.39501 | val_0_rmse: 0.63812 | val_1_rmse: 0.63859 |  0:01:56s
epoch 58 | loss: 0.40264 | val_0_rmse: 0.63307 | val_1_rmse: 0.63931 |  0:01:58s
epoch 59 | loss: 0.39837 | val_0_rmse: 0.6309  | val_1_rmse: 0.63596 |  0:02:00s
epoch 60 | loss: 0.39457 | val_0_rmse: 0.63663 | val_1_rmse: 0.63918 |  0:02:02s
epoch 61 | loss: 0.40834 | val_0_rmse: 0.63408 | val_1_rmse: 0.63709 |  0:02:04s
epoch 62 | loss: 0.42078 | val_0_rmse: 0.66141 | val_1_rmse: 0.66456 |  0:02:06s
epoch 63 | loss: 0.39742 | val_0_rmse: 0.68984 | val_1_rmse: 0.68778 |  0:02:08s
epoch 64 | loss: 0.40935 | val_0_rmse: 0.62863 | val_1_rmse: 0.63007 |  0:02:10s
epoch 65 | loss: 0.39985 | val_0_rmse: 0.64733 | val_1_rmse: 0.646   |  0:02:12s
epoch 66 | loss: 0.40782 | val_0_rmse: 0.66199 | val_1_rmse: 0.6722  |  0:02:14s
epoch 67 | loss: 0.42272 | val_0_rmse: 0.6774  | val_1_rmse: 0.6683  |  0:02:16s
epoch 68 | loss: 0.41122 | val_0_rmse: 0.69398 | val_1_rmse: 0.6889  |  0:02:18s
epoch 69 | loss: 0.41853 | val_0_rmse: 0.75964 | val_1_rmse: 0.76824 |  0:02:20s
epoch 70 | loss: 0.42033 | val_0_rmse: 0.77042 | val_1_rmse: 0.76017 |  0:02:22s
epoch 71 | loss: 0.40627 | val_0_rmse: 0.72356 | val_1_rmse: 0.73738 |  0:02:24s
epoch 72 | loss: 0.40476 | val_0_rmse: 0.64087 | val_1_rmse: 0.63911 |  0:02:26s
epoch 73 | loss: 0.39299 | val_0_rmse: 0.63692 | val_1_rmse: 0.64136 |  0:02:28s
epoch 74 | loss: 0.40756 | val_0_rmse: 0.68459 | val_1_rmse: 0.69041 |  0:02:30s
epoch 75 | loss: 0.41221 | val_0_rmse: 0.64482 | val_1_rmse: 0.6436  |  0:02:32s
epoch 76 | loss: 0.39146 | val_0_rmse: 0.63284 | val_1_rmse: 0.64097 |  0:02:34s
epoch 77 | loss: 0.39508 | val_0_rmse: 0.67336 | val_1_rmse: 0.67452 |  0:02:37s
epoch 78 | loss: 0.39959 | val_0_rmse: 0.66691 | val_1_rmse: 0.66588 |  0:02:39s
epoch 79 | loss: 0.42354 | val_0_rmse: 0.68566 | val_1_rmse: 0.69265 |  0:02:41s
epoch 80 | loss: 0.42145 | val_0_rmse: 0.65626 | val_1_rmse: 0.65974 |  0:02:43s
epoch 81 | loss: 0.43476 | val_0_rmse: 0.63802 | val_1_rmse: 0.64024 |  0:02:44s
epoch 82 | loss: 0.41615 | val_0_rmse: 0.62622 | val_1_rmse: 0.63485 |  0:02:46s

Early stopping occured at epoch 82 with best_epoch = 52 and best_val_1_rmse = 0.61412
Best weights from best epoch are automatically used!
ended training at: 15:49:44
Feature importance:
[('Area', 0.31547613787450535), ('Baths', 0.029704648580054845), ('Beds', 0.05389669429511337), ('Latitude', 0.13953732915343073), ('Longitude', 0.26813584449448213), ('Month', 0.07283954298621652), ('Year', 0.12040980261619708)]
Mean squared error is of 30927049603.47345
Mean absolute error:134272.67372583374
MAPE:0.24597998703509075
R2 score:0.6249191320786037
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:49:45
epoch 0  | loss: 0.80077 | val_0_rmse: 0.83856 | val_1_rmse: 0.83974 |  0:00:02s
epoch 1  | loss: 0.68221 | val_0_rmse: 0.81595 | val_1_rmse: 0.81532 |  0:00:04s
epoch 2  | loss: 0.67097 | val_0_rmse: 0.81528 | val_1_rmse: 0.81672 |  0:00:06s
epoch 3  | loss: 0.65513 | val_0_rmse: 0.79714 | val_1_rmse: 0.7981  |  0:00:08s
epoch 4  | loss: 0.65716 | val_0_rmse: 0.8107  | val_1_rmse: 0.80803 |  0:00:10s
epoch 5  | loss: 0.64087 | val_0_rmse: 0.78737 | val_1_rmse: 0.78408 |  0:00:12s
epoch 6  | loss: 0.63478 | val_0_rmse: 0.78248 | val_1_rmse: 0.78008 |  0:00:14s
epoch 7  | loss: 0.61553 | val_0_rmse: 0.76859 | val_1_rmse: 0.7676  |  0:00:16s
epoch 8  | loss: 0.60269 | val_0_rmse: 0.75695 | val_1_rmse: 0.75454 |  0:00:18s
epoch 9  | loss: 0.6019  | val_0_rmse: 0.76389 | val_1_rmse: 0.76161 |  0:00:20s
epoch 10 | loss: 0.5967  | val_0_rmse: 0.76172 | val_1_rmse: 0.75801 |  0:00:22s
epoch 11 | loss: 0.58793 | val_0_rmse: 0.74722 | val_1_rmse: 0.74366 |  0:00:24s
epoch 12 | loss: 0.58709 | val_0_rmse: 0.7467  | val_1_rmse: 0.74409 |  0:00:26s
epoch 13 | loss: 0.56296 | val_0_rmse: 0.71834 | val_1_rmse: 0.71389 |  0:00:28s
epoch 14 | loss: 0.53908 | val_0_rmse: 0.7218  | val_1_rmse: 0.71815 |  0:00:30s
epoch 15 | loss: 0.51976 | val_0_rmse: 0.72021 | val_1_rmse: 0.71631 |  0:00:32s
epoch 16 | loss: 0.49239 | val_0_rmse: 0.68534 | val_1_rmse: 0.68067 |  0:00:34s
epoch 17 | loss: 0.48675 | val_0_rmse: 0.70619 | val_1_rmse: 0.70379 |  0:00:36s
epoch 18 | loss: 0.48377 | val_0_rmse: 0.71058 | val_1_rmse: 0.7058  |  0:00:38s
epoch 19 | loss: 0.47678 | val_0_rmse: 0.66573 | val_1_rmse: 0.65917 |  0:00:40s
epoch 20 | loss: 0.47184 | val_0_rmse: 0.71924 | val_1_rmse: 0.71303 |  0:00:42s
epoch 21 | loss: 0.46486 | val_0_rmse: 0.6697  | val_1_rmse: 0.66529 |  0:00:44s
epoch 22 | loss: 0.46911 | val_0_rmse: 0.67829 | val_1_rmse: 0.67333 |  0:00:46s
epoch 23 | loss: 0.46186 | val_0_rmse: 0.66664 | val_1_rmse: 0.66483 |  0:00:48s
epoch 24 | loss: 0.45682 | val_0_rmse: 0.69198 | val_1_rmse: 0.68624 |  0:00:50s
epoch 25 | loss: 0.45035 | val_0_rmse: 0.67882 | val_1_rmse: 0.675   |  0:00:52s
epoch 26 | loss: 0.4571  | val_0_rmse: 0.66608 | val_1_rmse: 0.66537 |  0:00:54s
epoch 27 | loss: 0.44867 | val_0_rmse: 0.68786 | val_1_rmse: 0.6865  |  0:00:56s
epoch 28 | loss: 0.44518 | val_0_rmse: 0.66298 | val_1_rmse: 0.66099 |  0:00:58s
epoch 29 | loss: 0.43784 | val_0_rmse: 0.67206 | val_1_rmse: 0.67215 |  0:01:00s
epoch 30 | loss: 0.44267 | val_0_rmse: 0.73707 | val_1_rmse: 0.73697 |  0:01:02s
epoch 31 | loss: 0.4346  | val_0_rmse: 0.68409 | val_1_rmse: 0.67806 |  0:01:04s
epoch 32 | loss: 0.42809 | val_0_rmse: 0.69882 | val_1_rmse: 0.69626 |  0:01:06s
epoch 33 | loss: 0.4301  | val_0_rmse: 0.65285 | val_1_rmse: 0.65207 |  0:01:08s
epoch 34 | loss: 0.42201 | val_0_rmse: 0.65247 | val_1_rmse: 0.64627 |  0:01:10s
epoch 35 | loss: 0.41924 | val_0_rmse: 0.67486 | val_1_rmse: 0.67423 |  0:01:12s
epoch 36 | loss: 0.41077 | val_0_rmse: 0.67825 | val_1_rmse: 0.67831 |  0:01:14s
epoch 37 | loss: 0.40827 | val_0_rmse: 0.63303 | val_1_rmse: 0.63215 |  0:01:16s
epoch 38 | loss: 0.40982 | val_0_rmse: 0.61537 | val_1_rmse: 0.61751 |  0:01:18s
epoch 39 | loss: 0.40108 | val_0_rmse: 0.64505 | val_1_rmse: 0.64687 |  0:01:20s
epoch 40 | loss: 0.40535 | val_0_rmse: 0.64639 | val_1_rmse: 0.63951 |  0:01:22s
epoch 41 | loss: 0.39461 | val_0_rmse: 0.62896 | val_1_rmse: 0.6254  |  0:01:24s
epoch 42 | loss: 0.39586 | val_0_rmse: 0.65697 | val_1_rmse: 0.6525  |  0:01:26s
epoch 43 | loss: 0.39912 | val_0_rmse: 0.70093 | val_1_rmse: 0.69971 |  0:01:28s
epoch 44 | loss: 0.39923 | val_0_rmse: 0.64604 | val_1_rmse: 0.64703 |  0:01:30s
epoch 45 | loss: 0.39384 | val_0_rmse: 0.62086 | val_1_rmse: 0.62005 |  0:01:32s
epoch 46 | loss: 0.38979 | val_0_rmse: 0.65032 | val_1_rmse: 0.64876 |  0:01:34s
epoch 47 | loss: 0.38617 | val_0_rmse: 0.61892 | val_1_rmse: 0.61941 |  0:01:36s
epoch 48 | loss: 0.38729 | val_0_rmse: 0.62261 | val_1_rmse: 0.62513 |  0:01:38s
epoch 49 | loss: 0.38483 | val_0_rmse: 0.60915 | val_1_rmse: 0.61254 |  0:01:40s
epoch 50 | loss: 0.38005 | val_0_rmse: 0.58884 | val_1_rmse: 0.59197 |  0:01:42s
epoch 51 | loss: 0.38335 | val_0_rmse: 0.61013 | val_1_rmse: 0.61446 |  0:01:44s
epoch 52 | loss: 0.38154 | val_0_rmse: 0.61457 | val_1_rmse: 0.61394 |  0:01:47s
epoch 53 | loss: 0.38292 | val_0_rmse: 0.61619 | val_1_rmse: 0.61608 |  0:01:49s
epoch 54 | loss: 0.38435 | val_0_rmse: 0.60635 | val_1_rmse: 0.60909 |  0:01:51s
epoch 55 | loss: 0.3772  | val_0_rmse: 0.60806 | val_1_rmse: 0.61312 |  0:01:53s
epoch 56 | loss: 0.39452 | val_0_rmse: 0.6416  | val_1_rmse: 0.64175 |  0:01:55s
epoch 57 | loss: 0.36979 | val_0_rmse: 0.62637 | val_1_rmse: 0.63054 |  0:01:57s
epoch 58 | loss: 0.37194 | val_0_rmse: 0.59158 | val_1_rmse: 0.60096 |  0:01:59s
epoch 59 | loss: 0.37368 | val_0_rmse: 0.60591 | val_1_rmse: 0.61249 |  0:02:01s
epoch 60 | loss: 0.37754 | val_0_rmse: 0.60374 | val_1_rmse: 0.60646 |  0:02:03s
epoch 61 | loss: 0.37149 | val_0_rmse: 0.62152 | val_1_rmse: 0.62675 |  0:02:05s
epoch 62 | loss: 0.378   | val_0_rmse: 0.63894 | val_1_rmse: 0.64407 |  0:02:07s
epoch 63 | loss: 0.37416 | val_0_rmse: 0.64599 | val_1_rmse: 0.65235 |  0:02:09s
epoch 64 | loss: 0.37172 | val_0_rmse: 0.60299 | val_1_rmse: 0.61051 |  0:02:11s
epoch 65 | loss: 0.36604 | val_0_rmse: 0.58873 | val_1_rmse: 0.5947  |  0:02:13s
epoch 66 | loss: 0.36378 | val_0_rmse: 0.60134 | val_1_rmse: 0.60651 |  0:02:15s
epoch 67 | loss: 0.36292 | val_0_rmse: 0.63921 | val_1_rmse: 0.64215 |  0:02:17s
epoch 68 | loss: 0.36976 | val_0_rmse: 0.58758 | val_1_rmse: 0.59474 |  0:02:19s
epoch 69 | loss: 0.36107 | val_0_rmse: 0.60559 | val_1_rmse: 0.6095  |  0:02:21s
epoch 70 | loss: 0.36241 | val_0_rmse: 0.63261 | val_1_rmse: 0.63352 |  0:02:23s
epoch 71 | loss: 0.38109 | val_0_rmse: 0.62836 | val_1_rmse: 0.63489 |  0:02:25s
epoch 72 | loss: 0.3695  | val_0_rmse: 0.61648 | val_1_rmse: 0.62486 |  0:02:27s
epoch 73 | loss: 0.3693  | val_0_rmse: 0.57474 | val_1_rmse: 0.58058 |  0:02:29s
epoch 74 | loss: 0.36621 | val_0_rmse: 0.63281 | val_1_rmse: 0.63686 |  0:02:31s
epoch 75 | loss: 0.36493 | val_0_rmse: 0.59773 | val_1_rmse: 0.60436 |  0:02:33s
epoch 76 | loss: 0.36133 | val_0_rmse: 0.59127 | val_1_rmse: 0.60023 |  0:02:35s
epoch 77 | loss: 0.36234 | val_0_rmse: 0.60058 | val_1_rmse: 0.60743 |  0:02:37s
epoch 78 | loss: 0.35621 | val_0_rmse: 0.57028 | val_1_rmse: 0.57703 |  0:02:39s
epoch 79 | loss: 0.35592 | val_0_rmse: 0.57398 | val_1_rmse: 0.58053 |  0:02:41s
epoch 80 | loss: 0.35803 | val_0_rmse: 0.6007  | val_1_rmse: 0.60645 |  0:02:43s
epoch 81 | loss: 0.36133 | val_0_rmse: 0.60042 | val_1_rmse: 0.61077 |  0:02:45s
epoch 82 | loss: 0.3567  | val_0_rmse: 0.58172 | val_1_rmse: 0.59092 |  0:02:47s
epoch 83 | loss: 0.35135 | val_0_rmse: 0.595   | val_1_rmse: 0.60595 |  0:02:49s
epoch 84 | loss: 0.35976 | val_0_rmse: 0.60593 | val_1_rmse: 0.61548 |  0:02:51s
epoch 85 | loss: 0.35734 | val_0_rmse: 0.62195 | val_1_rmse: 0.62582 |  0:02:53s
epoch 86 | loss: 0.35012 | val_0_rmse: 0.58802 | val_1_rmse: 0.595   |  0:02:55s
epoch 87 | loss: 0.35243 | val_0_rmse: 0.58694 | val_1_rmse: 0.59301 |  0:02:57s
epoch 88 | loss: 0.35889 | val_0_rmse: 0.6433  | val_1_rmse: 0.6507  |  0:02:59s
epoch 89 | loss: 0.34881 | val_0_rmse: 0.58055 | val_1_rmse: 0.58983 |  0:03:01s
epoch 90 | loss: 0.36061 | val_0_rmse: 0.58715 | val_1_rmse: 0.59524 |  0:03:03s
epoch 91 | loss: 0.35119 | val_0_rmse: 0.57703 | val_1_rmse: 0.5892  |  0:03:05s
epoch 92 | loss: 0.34716 | val_0_rmse: 0.58901 | val_1_rmse: 0.59876 |  0:03:07s
epoch 93 | loss: 0.35718 | val_0_rmse: 0.58741 | val_1_rmse: 0.59764 |  0:03:09s
epoch 94 | loss: 0.35165 | val_0_rmse: 0.61164 | val_1_rmse: 0.62358 |  0:03:11s
epoch 95 | loss: 0.34671 | val_0_rmse: 0.60145 | val_1_rmse: 0.61137 |  0:03:13s
epoch 96 | loss: 0.35226 | val_0_rmse: 0.66193 | val_1_rmse: 0.66537 |  0:03:15s
epoch 97 | loss: 0.34885 | val_0_rmse: 0.6398  | val_1_rmse: 0.65492 |  0:03:17s
epoch 98 | loss: 0.3403  | val_0_rmse: 0.60833 | val_1_rmse: 0.61771 |  0:03:19s
epoch 99 | loss: 0.34068 | val_0_rmse: 0.55409 | val_1_rmse: 0.56634 |  0:03:21s
epoch 100| loss: 0.34506 | val_0_rmse: 0.57337 | val_1_rmse: 0.5817  |  0:03:23s
epoch 101| loss: 0.34758 | val_0_rmse: 0.58413 | val_1_rmse: 0.59208 |  0:03:25s
epoch 102| loss: 0.34462 | val_0_rmse: 0.56579 | val_1_rmse: 0.57697 |  0:03:27s
epoch 103| loss: 0.34329 | val_0_rmse: 0.6041  | val_1_rmse: 0.61063 |  0:03:29s
epoch 104| loss: 0.33841 | val_0_rmse: 0.565   | val_1_rmse: 0.57496 |  0:03:31s
epoch 105| loss: 0.3417  | val_0_rmse: 0.582   | val_1_rmse: 0.59666 |  0:03:33s
epoch 106| loss: 0.34115 | val_0_rmse: 0.56971 | val_1_rmse: 0.58239 |  0:03:35s
epoch 107| loss: 0.34404 | val_0_rmse: 0.56851 | val_1_rmse: 0.57828 |  0:03:37s
epoch 108| loss: 0.33716 | val_0_rmse: 0.56951 | val_1_rmse: 0.58508 |  0:03:39s
epoch 109| loss: 0.33895 | val_0_rmse: 0.61179 | val_1_rmse: 0.6257  |  0:03:42s
epoch 110| loss: 0.34267 | val_0_rmse: 0.58138 | val_1_rmse: 0.59565 |  0:03:44s
epoch 111| loss: 0.34436 | val_0_rmse: 0.55669 | val_1_rmse: 0.56964 |  0:03:46s
epoch 112| loss: 0.3402  | val_0_rmse: 0.59894 | val_1_rmse: 0.60642 |  0:03:48s
epoch 113| loss: 0.3393  | val_0_rmse: 0.56869 | val_1_rmse: 0.58053 |  0:03:50s
epoch 114| loss: 0.3368  | val_0_rmse: 0.56196 | val_1_rmse: 0.57481 |  0:03:52s
epoch 115| loss: 0.3356  | val_0_rmse: 0.54907 | val_1_rmse: 0.56694 |  0:03:54s
epoch 116| loss: 0.33044 | val_0_rmse: 0.54793 | val_1_rmse: 0.56279 |  0:03:56s
epoch 117| loss: 0.33309 | val_0_rmse: 0.54535 | val_1_rmse: 0.56003 |  0:03:58s
epoch 118| loss: 0.3291  | val_0_rmse: 0.55199 | val_1_rmse: 0.56939 |  0:04:00s
epoch 119| loss: 0.33312 | val_0_rmse: 0.5817  | val_1_rmse: 0.59579 |  0:04:02s
epoch 120| loss: 0.33488 | val_0_rmse: 0.57395 | val_1_rmse: 0.58758 |  0:04:04s
epoch 121| loss: 0.3317  | val_0_rmse: 0.57353 | val_1_rmse: 0.58698 |  0:04:06s
epoch 122| loss: 0.32944 | val_0_rmse: 0.56511 | val_1_rmse: 0.58394 |  0:04:08s
epoch 123| loss: 0.33625 | val_0_rmse: 0.55496 | val_1_rmse: 0.57233 |  0:04:10s
epoch 124| loss: 0.33608 | val_0_rmse: 0.58288 | val_1_rmse: 0.59905 |  0:04:12s
epoch 125| loss: 0.32908 | val_0_rmse: 0.58203 | val_1_rmse: 0.59737 |  0:04:14s
epoch 126| loss: 0.3418  | val_0_rmse: 0.58102 | val_1_rmse: 0.59982 |  0:04:16s
epoch 127| loss: 0.32819 | val_0_rmse: 0.55223 | val_1_rmse: 0.5678  |  0:04:18s
epoch 128| loss: 0.32428 | val_0_rmse: 0.56874 | val_1_rmse: 0.57688 |  0:04:20s
epoch 129| loss: 0.32447 | val_0_rmse: 0.62182 | val_1_rmse: 0.63168 |  0:04:22s
epoch 130| loss: 0.32331 | val_0_rmse: 0.53834 | val_1_rmse: 0.55435 |  0:04:24s
epoch 131| loss: 0.3254  | val_0_rmse: 0.54827 | val_1_rmse: 0.56167 |  0:04:26s
epoch 132| loss: 0.32114 | val_0_rmse: 0.55421 | val_1_rmse: 0.56951 |  0:04:28s
epoch 133| loss: 0.32548 | val_0_rmse: 0.54545 | val_1_rmse: 0.55961 |  0:04:30s
epoch 134| loss: 0.32436 | val_0_rmse: 0.56195 | val_1_rmse: 0.57822 |  0:04:32s
epoch 135| loss: 0.32263 | val_0_rmse: 0.53122 | val_1_rmse: 0.54831 |  0:04:34s
epoch 136| loss: 0.32014 | val_0_rmse: 0.57758 | val_1_rmse: 0.59771 |  0:04:36s
epoch 137| loss: 0.321   | val_0_rmse: 0.574   | val_1_rmse: 0.58861 |  0:04:38s
epoch 138| loss: 0.31914 | val_0_rmse: 0.55722 | val_1_rmse: 0.56959 |  0:04:40s
epoch 139| loss: 0.32779 | val_0_rmse: 0.58868 | val_1_rmse: 0.60675 |  0:04:42s
epoch 140| loss: 0.3197  | val_0_rmse: 0.60319 | val_1_rmse: 0.61579 |  0:04:44s
epoch 141| loss: 0.32536 | val_0_rmse: 0.5197  | val_1_rmse: 0.53716 |  0:04:46s
epoch 142| loss: 0.31253 | val_0_rmse: 0.56858 | val_1_rmse: 0.58529 |  0:04:48s
epoch 143| loss: 0.3169  | val_0_rmse: 0.55857 | val_1_rmse: 0.5752  |  0:04:50s
epoch 144| loss: 0.3266  | val_0_rmse: 0.54388 | val_1_rmse: 0.56169 |  0:04:52s
epoch 145| loss: 0.31471 | val_0_rmse: 0.53325 | val_1_rmse: 0.54918 |  0:04:54s
epoch 146| loss: 0.31509 | val_0_rmse: 0.58781 | val_1_rmse: 0.59991 |  0:04:56s
epoch 147| loss: 0.32063 | val_0_rmse: 0.55388 | val_1_rmse: 0.56911 |  0:04:58s
epoch 148| loss: 0.31451 | val_0_rmse: 0.58688 | val_1_rmse: 0.60444 |  0:05:00s
epoch 149| loss: 0.31537 | val_0_rmse: 0.54157 | val_1_rmse: 0.55964 |  0:05:03s
Stop training because you reached max_epochs = 150 with best_epoch = 141 and best_val_1_rmse = 0.53716
Best weights from best epoch are automatically used!
ended training at: 15:54:48
Feature importance:
[('Area', 0.2788686206071796), ('Baths', 0.17674085841331122), ('Beds', 0.021304896427286852), ('Latitude', 0.16819461756554396), ('Longitude', 0.3330440599358732), ('Month', 0.021846947050805126), ('Year', 0.0)]
Mean squared error is of 23670082256.437927
Mean absolute error:112945.04558737787
MAPE:0.20018114672048368
R2 score:0.7054810605701992
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:54:49
epoch 0  | loss: 0.83084 | val_0_rmse: 0.86194 | val_1_rmse: 0.86481 |  0:00:02s
epoch 1  | loss: 0.69988 | val_0_rmse: 0.82704 | val_1_rmse: 0.8261  |  0:00:04s
epoch 2  | loss: 0.696   | val_0_rmse: 0.8272  | val_1_rmse: 0.82395 |  0:00:05s
epoch 3  | loss: 0.68892 | val_0_rmse: 0.8212  | val_1_rmse: 0.82105 |  0:00:08s
epoch 4  | loss: 0.67286 | val_0_rmse: 0.81446 | val_1_rmse: 0.8192  |  0:00:10s
epoch 5  | loss: 0.67242 | val_0_rmse: 0.81161 | val_1_rmse: 0.81827 |  0:00:12s
epoch 6  | loss: 0.67823 | val_0_rmse: 0.81853 | val_1_rmse: 0.81912 |  0:00:14s
epoch 7  | loss: 0.65914 | val_0_rmse: 0.79256 | val_1_rmse: 0.79748 |  0:00:16s
epoch 8  | loss: 0.64032 | val_0_rmse: 0.8023  | val_1_rmse: 0.80492 |  0:00:18s
epoch 9  | loss: 0.62641 | val_0_rmse: 0.80582 | val_1_rmse: 0.80471 |  0:00:20s
epoch 10 | loss: 0.61451 | val_0_rmse: 0.77283 | val_1_rmse: 0.77784 |  0:00:22s
epoch 11 | loss: 0.61157 | val_0_rmse: 0.78947 | val_1_rmse: 0.79878 |  0:00:24s
epoch 12 | loss: 0.6034  | val_0_rmse: 0.76502 | val_1_rmse: 0.76757 |  0:00:26s
epoch 13 | loss: 0.59516 | val_0_rmse: 0.76051 | val_1_rmse: 0.76646 |  0:00:28s
epoch 14 | loss: 0.58601 | val_0_rmse: 0.75268 | val_1_rmse: 0.75435 |  0:00:30s
epoch 15 | loss: 0.5773  | val_0_rmse: 0.7654  | val_1_rmse: 0.77287 |  0:00:32s
epoch 16 | loss: 0.58755 | val_0_rmse: 0.75582 | val_1_rmse: 0.76345 |  0:00:34s
epoch 17 | loss: 0.56146 | val_0_rmse: 0.72606 | val_1_rmse: 0.73634 |  0:00:36s
epoch 18 | loss: 0.54971 | val_0_rmse: 0.72413 | val_1_rmse: 0.73004 |  0:00:38s
epoch 19 | loss: 0.53146 | val_0_rmse: 0.69304 | val_1_rmse: 0.7012  |  0:00:40s
epoch 20 | loss: 0.51099 | val_0_rmse: 0.7211  | val_1_rmse: 0.72739 |  0:00:42s
epoch 21 | loss: 0.48894 | val_0_rmse: 0.67458 | val_1_rmse: 0.69058 |  0:00:44s
epoch 22 | loss: 0.48526 | val_0_rmse: 0.71054 | val_1_rmse: 0.7178  |  0:00:46s
epoch 23 | loss: 0.48312 | val_0_rmse: 0.67779 | val_1_rmse: 0.68453 |  0:00:48s
epoch 24 | loss: 0.46502 | val_0_rmse: 0.66231 | val_1_rmse: 0.67378 |  0:00:50s
epoch 25 | loss: 0.47088 | val_0_rmse: 0.79423 | val_1_rmse: 0.79025 |  0:00:52s
epoch 26 | loss: 0.46405 | val_0_rmse: 0.73771 | val_1_rmse: 0.7446  |  0:00:54s
epoch 27 | loss: 0.45661 | val_0_rmse: 0.68456 | val_1_rmse: 0.6965  |  0:00:56s
epoch 28 | loss: 0.45171 | val_0_rmse: 0.6904  | val_1_rmse: 0.69779 |  0:00:58s
epoch 29 | loss: 0.44475 | val_0_rmse: 0.74702 | val_1_rmse: 0.75119 |  0:01:00s
epoch 30 | loss: 0.43817 | val_0_rmse: 0.64924 | val_1_rmse: 0.65884 |  0:01:02s
epoch 31 | loss: 0.43261 | val_0_rmse: 0.70422 | val_1_rmse: 0.71006 |  0:01:04s
epoch 32 | loss: 0.44334 | val_0_rmse: 0.67732 | val_1_rmse: 0.68892 |  0:01:06s
epoch 33 | loss: 0.42673 | val_0_rmse: 0.7917  | val_1_rmse: 0.79789 |  0:01:08s
epoch 34 | loss: 0.45082 | val_0_rmse: 0.63741 | val_1_rmse: 0.64746 |  0:01:10s
epoch 35 | loss: 0.42578 | val_0_rmse: 0.68684 | val_1_rmse: 0.70706 |  0:01:12s
epoch 36 | loss: 0.41786 | val_0_rmse: 0.72056 | val_1_rmse: 0.72575 |  0:01:15s
epoch 37 | loss: 0.41912 | val_0_rmse: 0.67662 | val_1_rmse: 0.69549 |  0:01:17s
epoch 38 | loss: 0.42894 | val_0_rmse: 0.68651 | val_1_rmse: 0.69124 |  0:01:19s
epoch 39 | loss: 0.41973 | val_0_rmse: 0.65221 | val_1_rmse: 0.67103 |  0:01:21s
epoch 40 | loss: 0.42384 | val_0_rmse: 0.62787 | val_1_rmse: 0.63846 |  0:01:23s
epoch 41 | loss: 0.41924 | val_0_rmse: 0.63352 | val_1_rmse: 0.6465  |  0:01:25s
epoch 42 | loss: 0.41492 | val_0_rmse: 0.72665 | val_1_rmse: 0.73804 |  0:01:27s
epoch 43 | loss: 0.41353 | val_0_rmse: 0.64497 | val_1_rmse: 0.65469 |  0:01:29s
epoch 44 | loss: 0.40632 | val_0_rmse: 0.63146 | val_1_rmse: 0.64549 |  0:01:31s
epoch 45 | loss: 0.40022 | val_0_rmse: 0.63141 | val_1_rmse: 0.64027 |  0:01:33s
epoch 46 | loss: 0.40033 | val_0_rmse: 0.66438 | val_1_rmse: 0.67371 |  0:01:35s
epoch 47 | loss: 0.40969 | val_0_rmse: 0.65751 | val_1_rmse: 0.67038 |  0:01:37s
epoch 48 | loss: 0.40976 | val_0_rmse: 0.71948 | val_1_rmse: 0.73987 |  0:01:39s
epoch 49 | loss: 0.41071 | val_0_rmse: 0.70819 | val_1_rmse: 0.72326 |  0:01:41s
epoch 50 | loss: 0.42987 | val_0_rmse: 0.65707 | val_1_rmse: 0.67176 |  0:01:43s
epoch 51 | loss: 0.41338 | val_0_rmse: 0.66358 | val_1_rmse: 0.67106 |  0:01:45s
epoch 52 | loss: 0.40132 | val_0_rmse: 0.62856 | val_1_rmse: 0.64075 |  0:01:47s
epoch 53 | loss: 0.40308 | val_0_rmse: 0.79538 | val_1_rmse: 0.80395 |  0:01:49s
epoch 54 | loss: 0.40539 | val_0_rmse: 0.63056 | val_1_rmse: 0.64318 |  0:01:51s
epoch 55 | loss: 0.41198 | val_0_rmse: 0.79037 | val_1_rmse: 0.80952 |  0:01:53s
epoch 56 | loss: 0.41049 | val_0_rmse: 0.64779 | val_1_rmse: 0.66532 |  0:01:55s
epoch 57 | loss: 0.41237 | val_0_rmse: 0.86222 | val_1_rmse: 0.87806 |  0:01:57s
epoch 58 | loss: 0.41038 | val_0_rmse: 0.67171 | val_1_rmse: 0.6717  |  0:01:59s
epoch 59 | loss: 0.4104  | val_0_rmse: 0.68916 | val_1_rmse: 0.70676 |  0:02:01s
epoch 60 | loss: 0.40519 | val_0_rmse: 0.63674 | val_1_rmse: 0.64416 |  0:02:03s
epoch 61 | loss: 0.39932 | val_0_rmse: 0.61179 | val_1_rmse: 0.62369 |  0:02:05s
epoch 62 | loss: 0.39622 | val_0_rmse: 0.6163  | val_1_rmse: 0.62753 |  0:02:07s
epoch 63 | loss: 0.39289 | val_0_rmse: 0.72672 | val_1_rmse: 0.74821 |  0:02:09s
epoch 64 | loss: 0.39746 | val_0_rmse: 0.72263 | val_1_rmse: 0.73063 |  0:02:11s
epoch 65 | loss: 0.41554 | val_0_rmse: 0.67376 | val_1_rmse: 0.67913 |  0:02:13s
epoch 66 | loss: 0.4042  | val_0_rmse: 0.70758 | val_1_rmse: 0.7151  |  0:02:15s
epoch 67 | loss: 0.40326 | val_0_rmse: 0.61807 | val_1_rmse: 0.63396 |  0:02:17s
epoch 68 | loss: 0.4033  | val_0_rmse: 0.62082 | val_1_rmse: 0.63248 |  0:02:19s
epoch 69 | loss: 0.40813 | val_0_rmse: 0.6708  | val_1_rmse: 0.67638 |  0:02:21s
epoch 70 | loss: 0.39739 | val_0_rmse: 0.66724 | val_1_rmse: 0.67294 |  0:02:23s
epoch 71 | loss: 0.39204 | val_0_rmse: 0.69907 | val_1_rmse: 0.71359 |  0:02:25s
epoch 72 | loss: 0.39204 | val_0_rmse: 0.68761 | val_1_rmse: 0.69846 |  0:02:27s
epoch 73 | loss: 0.40367 | val_0_rmse: 0.71352 | val_1_rmse: 0.7195  |  0:02:29s
epoch 74 | loss: 0.40213 | val_0_rmse: 0.80318 | val_1_rmse: 0.80018 |  0:02:31s
epoch 75 | loss: 0.39519 | val_0_rmse: 0.71631 | val_1_rmse: 0.72192 |  0:02:33s
epoch 76 | loss: 0.39072 | val_0_rmse: 0.76862 | val_1_rmse: 0.7649  |  0:02:35s
epoch 77 | loss: 0.38298 | val_0_rmse: 0.61    | val_1_rmse: 0.62177 |  0:02:37s
epoch 78 | loss: 0.38565 | val_0_rmse: 0.77024 | val_1_rmse: 0.77034 |  0:02:39s
epoch 79 | loss: 0.40714 | val_0_rmse: 0.66874 | val_1_rmse: 0.68883 |  0:02:41s
epoch 80 | loss: 0.39563 | val_0_rmse: 0.6002  | val_1_rmse: 0.61039 |  0:02:43s
epoch 81 | loss: 0.38363 | val_0_rmse: 0.60633 | val_1_rmse: 0.62073 |  0:02:45s
epoch 82 | loss: 0.38919 | val_0_rmse: 0.73651 | val_1_rmse: 0.74977 |  0:02:47s
epoch 83 | loss: 0.39052 | val_0_rmse: 0.59571 | val_1_rmse: 0.60815 |  0:02:49s
epoch 84 | loss: 0.38351 | val_0_rmse: 0.70121 | val_1_rmse: 0.7148  |  0:02:51s
epoch 85 | loss: 0.38873 | val_0_rmse: 0.71767 | val_1_rmse: 0.72163 |  0:02:53s
epoch 86 | loss: 0.39926 | val_0_rmse: 0.69652 | val_1_rmse: 0.7081  |  0:02:55s
epoch 87 | loss: 0.39416 | val_0_rmse: 0.61863 | val_1_rmse: 0.62913 |  0:02:57s
epoch 88 | loss: 0.3889  | val_0_rmse: 0.6448  | val_1_rmse: 0.65564 |  0:02:59s
epoch 89 | loss: 0.3717  | val_0_rmse: 0.60369 | val_1_rmse: 0.61795 |  0:03:01s
epoch 90 | loss: 0.37476 | val_0_rmse: 0.65188 | val_1_rmse: 0.66238 |  0:03:03s
epoch 91 | loss: 0.38055 | val_0_rmse: 0.80509 | val_1_rmse: 0.81869 |  0:03:05s
epoch 92 | loss: 0.38535 | val_0_rmse: 0.67312 | val_1_rmse: 0.6933  |  0:03:07s
epoch 93 | loss: 0.37723 | val_0_rmse: 0.61836 | val_1_rmse: 0.62747 |  0:03:09s
epoch 94 | loss: 0.37487 | val_0_rmse: 0.61953 | val_1_rmse: 0.62842 |  0:03:11s
epoch 95 | loss: 0.39843 | val_0_rmse: 0.70114 | val_1_rmse: 0.7049  |  0:03:13s
epoch 96 | loss: 0.40173 | val_0_rmse: 0.62467 | val_1_rmse: 0.63409 |  0:03:15s
epoch 97 | loss: 0.37513 | val_0_rmse: 0.59922 | val_1_rmse: 0.61231 |  0:03:17s
epoch 98 | loss: 0.3866  | val_0_rmse: 0.70799 | val_1_rmse: 0.72731 |  0:03:20s
epoch 99 | loss: 0.37542 | val_0_rmse: 0.61622 | val_1_rmse: 0.62623 |  0:03:22s
epoch 100| loss: 0.38341 | val_0_rmse: 0.64628 | val_1_rmse: 0.65324 |  0:03:24s
epoch 101| loss: 0.4023  | val_0_rmse: 0.90176 | val_1_rmse: 0.89575 |  0:03:26s
epoch 102| loss: 0.39307 | val_0_rmse: 0.59683 | val_1_rmse: 0.61142 |  0:03:28s
epoch 103| loss: 0.39974 | val_0_rmse: 0.60272 | val_1_rmse: 0.61774 |  0:03:29s
epoch 104| loss: 0.38931 | val_0_rmse: 0.76399 | val_1_rmse: 0.78068 |  0:03:31s
epoch 105| loss: 0.39743 | val_0_rmse: 0.63136 | val_1_rmse: 0.64256 |  0:03:34s
epoch 106| loss: 0.38468 | val_0_rmse: 0.67805 | val_1_rmse: 0.68285 |  0:03:36s
epoch 107| loss: 0.39202 | val_0_rmse: 0.60056 | val_1_rmse: 0.61171 |  0:03:38s
epoch 108| loss: 0.3969  | val_0_rmse: 0.67718 | val_1_rmse: 0.68093 |  0:03:40s
epoch 109| loss: 0.38888 | val_0_rmse: 0.59594 | val_1_rmse: 0.60523 |  0:03:42s
epoch 110| loss: 0.3844  | val_0_rmse: 0.68586 | val_1_rmse: 0.69017 |  0:03:44s
epoch 111| loss: 0.37878 | val_0_rmse: 0.60146 | val_1_rmse: 0.61359 |  0:03:46s
epoch 112| loss: 0.38114 | val_0_rmse: 0.63625 | val_1_rmse: 0.65172 |  0:03:48s
epoch 113| loss: 0.37848 | val_0_rmse: 0.6065  | val_1_rmse: 0.6142  |  0:03:50s
epoch 114| loss: 0.37459 | val_0_rmse: 0.61514 | val_1_rmse: 0.62287 |  0:03:52s
epoch 115| loss: 0.37578 | val_0_rmse: 0.60537 | val_1_rmse: 0.61553 |  0:03:54s
epoch 116| loss: 0.37597 | val_0_rmse: 0.70244 | val_1_rmse: 0.70769 |  0:03:56s
epoch 117| loss: 0.3751  | val_0_rmse: 0.59152 | val_1_rmse: 0.60409 |  0:03:58s
epoch 118| loss: 0.36768 | val_0_rmse: 0.74921 | val_1_rmse: 0.75158 |  0:04:00s
epoch 119| loss: 0.38578 | val_0_rmse: 0.67989 | val_1_rmse: 0.68482 |  0:04:02s
epoch 120| loss: 0.3946  | val_0_rmse: 0.70875 | val_1_rmse: 0.72015 |  0:04:04s
epoch 121| loss: 0.39489 | val_0_rmse: 0.64275 | val_1_rmse: 0.65069 |  0:04:06s
epoch 122| loss: 0.37564 | val_0_rmse: 0.59543 | val_1_rmse: 0.60509 |  0:04:08s
epoch 123| loss: 0.37564 | val_0_rmse: 0.59333 | val_1_rmse: 0.60496 |  0:04:10s
epoch 124| loss: 0.37567 | val_0_rmse: 0.61124 | val_1_rmse: 0.61791 |  0:04:12s
epoch 125| loss: 0.37354 | val_0_rmse: 0.6271  | val_1_rmse: 0.64093 |  0:04:14s
epoch 126| loss: 0.37622 | val_0_rmse: 0.66161 | val_1_rmse: 0.67683 |  0:04:16s
epoch 127| loss: 0.38459 | val_0_rmse: 0.60322 | val_1_rmse: 0.61243 |  0:04:18s
epoch 128| loss: 0.377   | val_0_rmse: 0.65171 | val_1_rmse: 0.66191 |  0:04:20s
epoch 129| loss: 0.37654 | val_0_rmse: 0.72899 | val_1_rmse: 0.73536 |  0:04:22s
epoch 130| loss: 0.37396 | val_0_rmse: 0.63276 | val_1_rmse: 0.63877 |  0:04:24s
epoch 131| loss: 0.3641  | val_0_rmse: 0.63009 | val_1_rmse: 0.64189 |  0:04:26s
epoch 132| loss: 0.36942 | val_0_rmse: 0.68218 | val_1_rmse: 0.69716 |  0:04:28s
epoch 133| loss: 0.36597 | val_0_rmse: 0.60253 | val_1_rmse: 0.61137 |  0:04:30s
epoch 134| loss: 0.36846 | val_0_rmse: 0.71763 | val_1_rmse: 0.73577 |  0:04:32s
epoch 135| loss: 0.35881 | val_0_rmse: 0.72836 | val_1_rmse: 0.74354 |  0:04:34s
epoch 136| loss: 0.35774 | val_0_rmse: 0.59477 | val_1_rmse: 0.60781 |  0:04:36s
epoch 137| loss: 0.36747 | val_0_rmse: 0.5914  | val_1_rmse: 0.60319 |  0:04:38s
epoch 138| loss: 0.37602 | val_0_rmse: 0.72459 | val_1_rmse: 0.74088 |  0:04:40s
epoch 139| loss: 0.3709  | val_0_rmse: 0.61405 | val_1_rmse: 0.62737 |  0:04:42s
epoch 140| loss: 0.35923 | val_0_rmse: 0.58817 | val_1_rmse: 0.59292 |  0:04:44s
epoch 141| loss: 0.36608 | val_0_rmse: 0.643   | val_1_rmse: 0.65151 |  0:04:46s
epoch 142| loss: 0.35721 | val_0_rmse: 0.61281 | val_1_rmse: 0.62323 |  0:04:48s
epoch 143| loss: 0.36208 | val_0_rmse: 0.63816 | val_1_rmse: 0.65169 |  0:04:50s
epoch 144| loss: 0.36149 | val_0_rmse: 0.93306 | val_1_rmse: 0.94219 |  0:04:52s
epoch 145| loss: 0.37183 | val_0_rmse: 0.64835 | val_1_rmse: 0.6645  |  0:04:54s
epoch 146| loss: 0.35883 | val_0_rmse: 0.69494 | val_1_rmse: 0.71552 |  0:04:56s
epoch 147| loss: 0.3542  | val_0_rmse: 0.8526  | val_1_rmse: 0.87292 |  0:04:58s
epoch 148| loss: 0.3651  | val_0_rmse: 0.68266 | val_1_rmse: 0.68962 |  0:05:00s
epoch 149| loss: 0.35109 | val_0_rmse: 0.68763 | val_1_rmse: 0.70131 |  0:05:02s
Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_val_1_rmse = 0.59292
Best weights from best epoch are automatically used!
ended training at: 15:59:52
Feature importance:
[('Area', 0.27552243146211897), ('Baths', 0.0), ('Beds', 0.1684726822403144), ('Latitude', 0.28711656929142426), ('Longitude', 0.0), ('Month', 0.165265726827084), ('Year', 0.10362259017905837)]
Mean squared error is of 27410649564.70341
Mean absolute error:123815.47252779458
MAPE:0.22566024997267328
R2 score:0.6567157900307652
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 15:59:53
epoch 0  | loss: 0.82458 | val_0_rmse: 0.84624 | val_1_rmse: 0.85383 |  0:00:01s
epoch 1  | loss: 0.7061  | val_0_rmse: 0.83644 | val_1_rmse: 0.84433 |  0:00:03s
epoch 2  | loss: 0.69529 | val_0_rmse: 0.82845 | val_1_rmse: 0.8352  |  0:00:06s
epoch 3  | loss: 0.68751 | val_0_rmse: 0.81895 | val_1_rmse: 0.82785 |  0:00:08s
epoch 4  | loss: 0.67481 | val_0_rmse: 0.8141  | val_1_rmse: 0.81883 |  0:00:10s
epoch 5  | loss: 0.67366 | val_0_rmse: 0.81486 | val_1_rmse: 0.8214  |  0:00:12s
epoch 6  | loss: 0.67383 | val_0_rmse: 0.82074 | val_1_rmse: 0.82924 |  0:00:14s
epoch 7  | loss: 0.66459 | val_0_rmse: 0.8038  | val_1_rmse: 0.81134 |  0:00:16s
epoch 8  | loss: 0.65585 | val_0_rmse: 0.79719 | val_1_rmse: 0.80198 |  0:00:18s
epoch 9  | loss: 0.63994 | val_0_rmse: 0.78387 | val_1_rmse: 0.78921 |  0:00:20s
epoch 10 | loss: 0.61546 | val_0_rmse: 0.78982 | val_1_rmse: 0.79253 |  0:00:22s
epoch 11 | loss: 0.58137 | val_0_rmse: 0.73943 | val_1_rmse: 0.74401 |  0:00:24s
epoch 12 | loss: 0.56037 | val_0_rmse: 0.73548 | val_1_rmse: 0.73783 |  0:00:26s
epoch 13 | loss: 0.55145 | val_0_rmse: 0.75727 | val_1_rmse: 0.76385 |  0:00:28s
epoch 14 | loss: 0.53355 | val_0_rmse: 0.71701 | val_1_rmse: 0.71761 |  0:00:30s
epoch 15 | loss: 0.52111 | val_0_rmse: 0.7182  | val_1_rmse: 0.71616 |  0:00:32s
epoch 16 | loss: 0.53653 | val_0_rmse: 0.74974 | val_1_rmse: 0.74794 |  0:00:34s
epoch 17 | loss: 0.52871 | val_0_rmse: 0.72418 | val_1_rmse: 0.72465 |  0:00:36s
epoch 18 | loss: 0.52038 | val_0_rmse: 0.75054 | val_1_rmse: 0.75007 |  0:00:38s
epoch 19 | loss: 0.51616 | val_0_rmse: 0.93728 | val_1_rmse: 0.93228 |  0:00:40s
epoch 20 | loss: 0.51816 | val_0_rmse: 0.68967 | val_1_rmse: 0.69213 |  0:00:42s
epoch 21 | loss: 0.5171  | val_0_rmse: 0.71771 | val_1_rmse: 0.7156  |  0:00:44s
epoch 22 | loss: 0.50276 | val_0_rmse: 0.70093 | val_1_rmse: 0.70225 |  0:00:46s
epoch 23 | loss: 0.50191 | val_0_rmse: 0.70575 | val_1_rmse: 0.70628 |  0:00:48s
epoch 24 | loss: 0.50339 | val_0_rmse: 0.70022 | val_1_rmse: 0.70504 |  0:00:50s
epoch 25 | loss: 0.49036 | val_0_rmse: 0.7003  | val_1_rmse: 0.70387 |  0:00:52s
epoch 26 | loss: 0.49349 | val_0_rmse: 0.70216 | val_1_rmse: 0.70638 |  0:00:54s
epoch 27 | loss: 0.49324 | val_0_rmse: 0.69091 | val_1_rmse: 0.69784 |  0:00:56s
epoch 28 | loss: 0.49625 | val_0_rmse: 0.7154  | val_1_rmse: 0.71746 |  0:00:58s
epoch 29 | loss: 0.49375 | val_0_rmse: 0.72565 | val_1_rmse: 0.73046 |  0:01:00s
epoch 30 | loss: 0.48819 | val_0_rmse: 0.7047  | val_1_rmse: 0.70462 |  0:01:02s
epoch 31 | loss: 0.48572 | val_0_rmse: 0.67302 | val_1_rmse: 0.67605 |  0:01:04s
epoch 32 | loss: 0.4832  | val_0_rmse: 0.75363 | val_1_rmse: 0.75493 |  0:01:06s
epoch 33 | loss: 0.50998 | val_0_rmse: 0.73063 | val_1_rmse: 0.72845 |  0:01:08s
epoch 34 | loss: 0.49168 | val_0_rmse: 0.69346 | val_1_rmse: 0.69914 |  0:01:10s
epoch 35 | loss: 0.47751 | val_0_rmse: 0.66598 | val_1_rmse: 0.669   |  0:01:12s
epoch 36 | loss: 0.47049 | val_0_rmse: 0.67784 | val_1_rmse: 0.68634 |  0:01:14s
epoch 37 | loss: 0.47429 | val_0_rmse: 0.66888 | val_1_rmse: 0.67227 |  0:01:16s
epoch 38 | loss: 0.45971 | val_0_rmse: 0.66948 | val_1_rmse: 0.67182 |  0:01:18s
epoch 39 | loss: 0.44893 | val_0_rmse: 0.67655 | val_1_rmse: 0.6805  |  0:01:20s
epoch 40 | loss: 0.44291 | val_0_rmse: 0.67133 | val_1_rmse: 0.67075 |  0:01:22s
epoch 41 | loss: 0.43865 | val_0_rmse: 0.67587 | val_1_rmse: 0.67462 |  0:01:24s
epoch 42 | loss: 0.43433 | val_0_rmse: 0.70391 | val_1_rmse: 0.70389 |  0:01:26s
epoch 43 | loss: 0.43737 | val_0_rmse: 0.64261 | val_1_rmse: 0.64839 |  0:01:28s
epoch 44 | loss: 0.4271  | val_0_rmse: 0.7121  | val_1_rmse: 0.71468 |  0:01:30s
epoch 45 | loss: 0.44087 | val_0_rmse: 0.68623 | val_1_rmse: 0.68843 |  0:01:32s
epoch 46 | loss: 0.43175 | val_0_rmse: 0.64523 | val_1_rmse: 0.6515  |  0:01:34s
epoch 47 | loss: 0.42525 | val_0_rmse: 0.72314 | val_1_rmse: 0.72457 |  0:01:37s
epoch 48 | loss: 0.43108 | val_0_rmse: 0.6445  | val_1_rmse: 0.65146 |  0:01:39s
epoch 49 | loss: 0.41302 | val_0_rmse: 0.6458  | val_1_rmse: 0.64791 |  0:01:41s
epoch 50 | loss: 0.42303 | val_0_rmse: 0.63819 | val_1_rmse: 0.64539 |  0:01:43s
epoch 51 | loss: 0.42232 | val_0_rmse: 0.65829 | val_1_rmse: 0.65971 |  0:01:45s
epoch 52 | loss: 0.40592 | val_0_rmse: 0.6298  | val_1_rmse: 0.63548 |  0:01:47s
epoch 53 | loss: 0.41918 | val_0_rmse: 0.65641 | val_1_rmse: 0.66082 |  0:01:49s
epoch 54 | loss: 0.40657 | val_0_rmse: 0.6355  | val_1_rmse: 0.63707 |  0:01:51s
epoch 55 | loss: 0.41873 | val_0_rmse: 0.6424  | val_1_rmse: 0.64598 |  0:01:53s
epoch 56 | loss: 0.40676 | val_0_rmse: 0.67621 | val_1_rmse: 0.6867  |  0:01:55s
epoch 57 | loss: 0.40779 | val_0_rmse: 0.70559 | val_1_rmse: 0.70704 |  0:01:57s
epoch 58 | loss: 0.39804 | val_0_rmse: 0.63843 | val_1_rmse: 0.64417 |  0:01:59s
epoch 59 | loss: 0.38247 | val_0_rmse: 0.62396 | val_1_rmse: 0.63096 |  0:02:01s
epoch 60 | loss: 0.38071 | val_0_rmse: 0.59325 | val_1_rmse: 0.60022 |  0:02:03s
epoch 61 | loss: 0.38006 | val_0_rmse: 0.60036 | val_1_rmse: 0.60245 |  0:02:05s
epoch 62 | loss: 0.37257 | val_0_rmse: 0.62339 | val_1_rmse: 0.62546 |  0:02:07s
epoch 63 | loss: 0.37041 | val_0_rmse: 0.67907 | val_1_rmse: 0.6929  |  0:02:09s
epoch 64 | loss: 0.38326 | val_0_rmse: 0.6774  | val_1_rmse: 0.67787 |  0:02:11s
epoch 65 | loss: 0.38181 | val_0_rmse: 0.61845 | val_1_rmse: 0.62918 |  0:02:13s
epoch 66 | loss: 0.38824 | val_0_rmse: 0.75783 | val_1_rmse: 0.76077 |  0:02:15s
epoch 67 | loss: 0.38614 | val_0_rmse: 0.61479 | val_1_rmse: 0.61892 |  0:02:17s
epoch 68 | loss: 0.39109 | val_0_rmse: 0.69127 | val_1_rmse: 0.6979  |  0:02:19s
epoch 69 | loss: 0.38775 | val_0_rmse: 0.64065 | val_1_rmse: 0.64342 |  0:02:21s
epoch 70 | loss: 0.37328 | val_0_rmse: 0.60008 | val_1_rmse: 0.60659 |  0:02:23s
epoch 71 | loss: 0.38657 | val_0_rmse: 0.70363 | val_1_rmse: 0.70416 |  0:02:25s
epoch 72 | loss: 0.38387 | val_0_rmse: 0.62377 | val_1_rmse: 0.62972 |  0:02:27s
epoch 73 | loss: 0.3686  | val_0_rmse: 0.61525 | val_1_rmse: 0.62445 |  0:02:29s
epoch 74 | loss: 0.36054 | val_0_rmse: 0.72496 | val_1_rmse: 0.73616 |  0:02:31s
epoch 75 | loss: 0.36891 | val_0_rmse: 0.62798 | val_1_rmse: 0.63637 |  0:02:33s
epoch 76 | loss: 0.36242 | val_0_rmse: 0.84339 | val_1_rmse: 0.84392 |  0:02:35s
epoch 77 | loss: 0.35998 | val_0_rmse: 0.5718  | val_1_rmse: 0.58273 |  0:02:37s
epoch 78 | loss: 0.35129 | val_0_rmse: 0.60712 | val_1_rmse: 0.61743 |  0:02:39s
epoch 79 | loss: 0.35034 | val_0_rmse: 0.60057 | val_1_rmse: 0.60644 |  0:02:41s
epoch 80 | loss: 0.35086 | val_0_rmse: 0.58858 | val_1_rmse: 0.59935 |  0:02:43s
epoch 81 | loss: 0.34824 | val_0_rmse: 0.5833  | val_1_rmse: 0.59023 |  0:02:45s
epoch 82 | loss: 0.35063 | val_0_rmse: 0.58227 | val_1_rmse: 0.58846 |  0:02:47s
epoch 83 | loss: 0.37311 | val_0_rmse: 0.60627 | val_1_rmse: 0.61141 |  0:02:49s
epoch 84 | loss: 0.35786 | val_0_rmse: 0.68434 | val_1_rmse: 0.6903  |  0:02:51s
epoch 85 | loss: 0.34535 | val_0_rmse: 0.60863 | val_1_rmse: 0.61606 |  0:02:53s
epoch 86 | loss: 0.34637 | val_0_rmse: 0.56413 | val_1_rmse: 0.575   |  0:02:55s
epoch 87 | loss: 0.35587 | val_0_rmse: 0.57773 | val_1_rmse: 0.59103 |  0:02:57s
epoch 88 | loss: 0.35052 | val_0_rmse: 0.67987 | val_1_rmse: 0.68307 |  0:02:59s
epoch 89 | loss: 0.34808 | val_0_rmse: 0.59174 | val_1_rmse: 0.60218 |  0:03:01s
epoch 90 | loss: 0.34721 | val_0_rmse: 0.62381 | val_1_rmse: 0.63111 |  0:03:03s
epoch 91 | loss: 0.3546  | val_0_rmse: 0.60214 | val_1_rmse: 0.60667 |  0:03:05s
epoch 92 | loss: 0.34158 | val_0_rmse: 0.57808 | val_1_rmse: 0.58469 |  0:03:07s
epoch 93 | loss: 0.34321 | val_0_rmse: 0.63484 | val_1_rmse: 0.63978 |  0:03:09s
epoch 94 | loss: 0.34364 | val_0_rmse: 0.62822 | val_1_rmse: 0.63331 |  0:03:11s
epoch 95 | loss: 0.34787 | val_0_rmse: 0.58223 | val_1_rmse: 0.59237 |  0:03:13s
epoch 96 | loss: 0.3396  | val_0_rmse: 0.58898 | val_1_rmse: 0.59427 |  0:03:15s
epoch 97 | loss: 0.34165 | val_0_rmse: 0.58951 | val_1_rmse: 0.59838 |  0:03:17s
epoch 98 | loss: 0.337   | val_0_rmse: 0.73219 | val_1_rmse: 0.74324 |  0:03:19s
epoch 99 | loss: 0.33204 | val_0_rmse: 0.56453 | val_1_rmse: 0.57642 |  0:03:21s
epoch 100| loss: 0.34937 | val_0_rmse: 0.59316 | val_1_rmse: 0.5966  |  0:03:23s
epoch 101| loss: 0.33447 | val_0_rmse: 0.60486 | val_1_rmse: 0.62027 |  0:03:25s
epoch 102| loss: 0.33467 | val_0_rmse: 0.61557 | val_1_rmse: 0.62112 |  0:03:28s
epoch 103| loss: 0.33916 | val_0_rmse: 0.59194 | val_1_rmse: 0.59689 |  0:03:30s
epoch 104| loss: 0.33418 | val_0_rmse: 0.62259 | val_1_rmse: 0.63699 |  0:03:32s
epoch 105| loss: 0.33023 | val_0_rmse: 0.66301 | val_1_rmse: 0.68202 |  0:03:34s
epoch 106| loss: 0.33154 | val_0_rmse: 0.59062 | val_1_rmse: 0.60349 |  0:03:36s
epoch 107| loss: 0.33169 | val_0_rmse: 0.60019 | val_1_rmse: 0.60938 |  0:03:38s
epoch 108| loss: 0.3377  | val_0_rmse: 0.6681  | val_1_rmse: 0.67532 |  0:03:40s
epoch 109| loss: 0.32782 | val_0_rmse: 0.5731  | val_1_rmse: 0.58732 |  0:03:42s
epoch 110| loss: 0.33008 | val_0_rmse: 0.61935 | val_1_rmse: 0.63116 |  0:03:44s
epoch 111| loss: 0.33904 | val_0_rmse: 0.55863 | val_1_rmse: 0.5721  |  0:03:46s
epoch 112| loss: 0.34285 | val_0_rmse: 0.62013 | val_1_rmse: 0.626   |  0:03:48s
epoch 113| loss: 0.33839 | val_0_rmse: 0.59829 | val_1_rmse: 0.60435 |  0:03:50s
epoch 114| loss: 0.32711 | val_0_rmse: 0.57799 | val_1_rmse: 0.5931  |  0:03:52s
epoch 115| loss: 0.32966 | val_0_rmse: 0.68673 | val_1_rmse: 0.69444 |  0:03:54s
epoch 116| loss: 0.33234 | val_0_rmse: 0.61796 | val_1_rmse: 0.62703 |  0:03:56s
epoch 117| loss: 0.33597 | val_0_rmse: 0.5485  | val_1_rmse: 0.56141 |  0:03:58s
epoch 118| loss: 0.32793 | val_0_rmse: 0.55785 | val_1_rmse: 0.57082 |  0:04:00s
epoch 119| loss: 0.33133 | val_0_rmse: 0.5882  | val_1_rmse: 0.59827 |  0:04:02s
epoch 120| loss: 0.32895 | val_0_rmse: 0.69513 | val_1_rmse: 0.70032 |  0:04:04s
epoch 121| loss: 0.34363 | val_0_rmse: 0.57042 | val_1_rmse: 0.58278 |  0:04:06s
epoch 122| loss: 0.33527 | val_0_rmse: 0.64645 | val_1_rmse: 0.65507 |  0:04:08s
epoch 123| loss: 0.32926 | val_0_rmse: 0.61728 | val_1_rmse: 0.62461 |  0:04:10s
epoch 124| loss: 0.32869 | val_0_rmse: 0.54994 | val_1_rmse: 0.5606  |  0:04:12s
epoch 125| loss: 0.3287  | val_0_rmse: 0.63218 | val_1_rmse: 0.63659 |  0:04:14s
epoch 126| loss: 0.32801 | val_0_rmse: 0.58263 | val_1_rmse: 0.59003 |  0:04:16s
epoch 127| loss: 0.32708 | val_0_rmse: 0.73984 | val_1_rmse: 0.75062 |  0:04:18s
epoch 128| loss: 0.33866 | val_0_rmse: 0.5516  | val_1_rmse: 0.56603 |  0:04:20s
epoch 129| loss: 0.33018 | val_0_rmse: 0.57958 | val_1_rmse: 0.58891 |  0:04:22s
epoch 130| loss: 0.32474 | val_0_rmse: 0.55546 | val_1_rmse: 0.56553 |  0:04:24s
epoch 131| loss: 0.32154 | val_0_rmse: 0.56317 | val_1_rmse: 0.57755 |  0:04:26s
epoch 132| loss: 0.33424 | val_0_rmse: 0.70154 | val_1_rmse: 0.70306 |  0:04:28s
epoch 133| loss: 0.33252 | val_0_rmse: 0.72736 | val_1_rmse: 0.73546 |  0:04:30s
epoch 134| loss: 0.33712 | val_0_rmse: 0.62305 | val_1_rmse: 0.62763 |  0:04:32s
epoch 135| loss: 0.32869 | val_0_rmse: 0.70514 | val_1_rmse: 0.70223 |  0:04:34s
epoch 136| loss: 0.34106 | val_0_rmse: 0.59616 | val_1_rmse: 0.60838 |  0:04:36s
epoch 137| loss: 0.32992 | val_0_rmse: 0.57801 | val_1_rmse: 0.58537 |  0:04:38s
epoch 138| loss: 0.33231 | val_0_rmse: 0.73753 | val_1_rmse: 0.73752 |  0:04:40s
epoch 139| loss: 0.33194 | val_0_rmse: 0.58725 | val_1_rmse: 0.59706 |  0:04:42s
epoch 140| loss: 0.33398 | val_0_rmse: 0.60051 | val_1_rmse: 0.61028 |  0:04:44s
epoch 141| loss: 0.32851 | val_0_rmse: 0.64878 | val_1_rmse: 0.65853 |  0:04:46s
epoch 142| loss: 0.31741 | val_0_rmse: 0.56589 | val_1_rmse: 0.5741  |  0:04:48s
epoch 143| loss: 0.31431 | val_0_rmse: 0.54149 | val_1_rmse: 0.55027 |  0:04:50s
epoch 144| loss: 0.31829 | val_0_rmse: 0.61543 | val_1_rmse: 0.63039 |  0:04:52s
epoch 145| loss: 0.32324 | val_0_rmse: 0.55186 | val_1_rmse: 0.5639  |  0:04:54s
epoch 146| loss: 0.3215  | val_0_rmse: 0.57318 | val_1_rmse: 0.58582 |  0:04:56s
epoch 147| loss: 0.32322 | val_0_rmse: 0.72369 | val_1_rmse: 0.73474 |  0:04:58s
epoch 148| loss: 0.317   | val_0_rmse: 0.62118 | val_1_rmse: 0.63194 |  0:05:00s
epoch 149| loss: 0.32408 | val_0_rmse: 0.73204 | val_1_rmse: 0.73321 |  0:05:02s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.55027
Best weights from best epoch are automatically used!
ended training at: 16:04:56
Feature importance:
[('Area', 0.2213246395698156), ('Baths', 0.0), ('Beds', 0.13079673890271312), ('Latitude', 0.24236942599353373), ('Longitude', 0.05992670572947429), ('Month', 0.2774914228617828), ('Year', 0.06809106694268043)]
Mean squared error is of 25410846045.63341
Mean absolute error:119857.65312233391
MAPE:0.21752030583619186
R2 score:0.6958970581230752
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:06:43
epoch 0  | loss: 0.59573 | val_0_rmse: 0.67217 | val_1_rmse: 0.67276 |  0:00:01s
epoch 1  | loss: 0.42664 | val_0_rmse: 0.63548 | val_1_rmse: 0.63779 |  0:00:03s
epoch 2  | loss: 0.39284 | val_0_rmse: 0.60829 | val_1_rmse: 0.61022 |  0:00:06s
epoch 3  | loss: 0.3839  | val_0_rmse: 0.61552 | val_1_rmse: 0.61035 |  0:00:08s
epoch 4  | loss: 0.37653 | val_0_rmse: 0.59245 | val_1_rmse: 0.58994 |  0:00:10s
epoch 5  | loss: 0.36323 | val_0_rmse: 0.58672 | val_1_rmse: 0.58575 |  0:00:12s
epoch 6  | loss: 0.36489 | val_0_rmse: 0.5993  | val_1_rmse: 0.60232 |  0:00:14s
epoch 7  | loss: 0.36826 | val_0_rmse: 0.59724 | val_1_rmse: 0.59767 |  0:00:16s
epoch 8  | loss: 0.36133 | val_0_rmse: 0.59209 | val_1_rmse: 0.5962  |  0:00:18s
epoch 9  | loss: 0.35245 | val_0_rmse: 0.58989 | val_1_rmse: 0.59513 |  0:00:20s
epoch 10 | loss: 0.36069 | val_0_rmse: 0.59218 | val_1_rmse: 0.58904 |  0:00:22s
epoch 11 | loss: 0.35317 | val_0_rmse: 0.57784 | val_1_rmse: 0.58201 |  0:00:24s
epoch 12 | loss: 0.34642 | val_0_rmse: 0.57658 | val_1_rmse: 0.57899 |  0:00:26s
epoch 13 | loss: 0.34472 | val_0_rmse: 0.56373 | val_1_rmse: 0.57198 |  0:00:28s
epoch 14 | loss: 0.33641 | val_0_rmse: 0.56344 | val_1_rmse: 0.56883 |  0:00:30s
epoch 15 | loss: 0.33754 | val_0_rmse: 0.57553 | val_1_rmse: 0.58062 |  0:00:32s
epoch 16 | loss: 0.3405  | val_0_rmse: 0.56036 | val_1_rmse: 0.56629 |  0:00:34s
epoch 17 | loss: 0.33822 | val_0_rmse: 0.56312 | val_1_rmse: 0.57389 |  0:00:36s
epoch 18 | loss: 0.32138 | val_0_rmse: 0.56512 | val_1_rmse: 0.57171 |  0:00:38s
epoch 19 | loss: 0.32602 | val_0_rmse: 0.5472  | val_1_rmse: 0.55398 |  0:00:40s
epoch 20 | loss: 0.3222  | val_0_rmse: 0.55366 | val_1_rmse: 0.5634  |  0:00:42s
epoch 21 | loss: 0.3342  | val_0_rmse: 0.55726 | val_1_rmse: 0.56099 |  0:00:44s
epoch 22 | loss: 0.32841 | val_0_rmse: 0.55728 | val_1_rmse: 0.56815 |  0:00:46s
epoch 23 | loss: 0.32927 | val_0_rmse: 0.57868 | val_1_rmse: 0.57848 |  0:00:48s
epoch 24 | loss: 0.33889 | val_0_rmse: 0.57126 | val_1_rmse: 0.57597 |  0:00:50s
epoch 25 | loss: 0.33553 | val_0_rmse: 0.55386 | val_1_rmse: 0.56184 |  0:00:52s
epoch 26 | loss: 0.32068 | val_0_rmse: 0.56178 | val_1_rmse: 0.56398 |  0:00:54s
epoch 27 | loss: 0.32315 | val_0_rmse: 0.56245 | val_1_rmse: 0.56946 |  0:00:56s
epoch 28 | loss: 0.32503 | val_0_rmse: 0.57416 | val_1_rmse: 0.57921 |  0:00:58s
epoch 29 | loss: 0.32886 | val_0_rmse: 0.54339 | val_1_rmse: 0.55169 |  0:01:00s
epoch 30 | loss: 0.32072 | val_0_rmse: 0.57451 | val_1_rmse: 0.58103 |  0:01:02s
epoch 31 | loss: 0.32004 | val_0_rmse: 0.55394 | val_1_rmse: 0.56085 |  0:01:04s
epoch 32 | loss: 0.31425 | val_0_rmse: 0.5377  | val_1_rmse: 0.54269 |  0:01:06s
epoch 33 | loss: 0.31277 | val_0_rmse: 0.54243 | val_1_rmse: 0.55319 |  0:01:08s
epoch 34 | loss: 0.30207 | val_0_rmse: 0.54312 | val_1_rmse: 0.54586 |  0:01:10s
epoch 35 | loss: 0.30212 | val_0_rmse: 0.54132 | val_1_rmse: 0.55106 |  0:01:12s
epoch 36 | loss: 0.29747 | val_0_rmse: 0.54042 | val_1_rmse: 0.55025 |  0:01:14s
epoch 37 | loss: 0.29399 | val_0_rmse: 0.52247 | val_1_rmse: 0.53326 |  0:01:16s
epoch 38 | loss: 0.28656 | val_0_rmse: 0.52077 | val_1_rmse: 0.53036 |  0:01:18s
epoch 39 | loss: 0.2815  | val_0_rmse: 0.52104 | val_1_rmse: 0.52657 |  0:01:20s
epoch 40 | loss: 0.28687 | val_0_rmse: 0.51827 | val_1_rmse: 0.52624 |  0:01:22s
epoch 41 | loss: 0.28186 | val_0_rmse: 0.53558 | val_1_rmse: 0.54772 |  0:01:24s
epoch 42 | loss: 0.28681 | val_0_rmse: 0.53239 | val_1_rmse: 0.54056 |  0:01:26s
epoch 43 | loss: 0.27932 | val_0_rmse: 0.49524 | val_1_rmse: 0.50623 |  0:01:28s
epoch 44 | loss: 0.25621 | val_0_rmse: 0.63502 | val_1_rmse: 0.64308 |  0:01:30s
epoch 45 | loss: 0.26297 | val_0_rmse: 0.52508 | val_1_rmse: 0.52275 |  0:01:32s
epoch 46 | loss: 0.26198 | val_0_rmse: 0.50323 | val_1_rmse: 0.50878 |  0:01:34s
epoch 47 | loss: 0.26471 | val_0_rmse: 0.48121 | val_1_rmse: 0.49158 |  0:01:36s
epoch 48 | loss: 0.25007 | val_0_rmse: 0.62765 | val_1_rmse: 0.63156 |  0:01:38s
epoch 49 | loss: 0.24648 | val_0_rmse: 0.51295 | val_1_rmse: 0.51501 |  0:01:40s
epoch 50 | loss: 0.24021 | val_0_rmse: 0.73796 | val_1_rmse: 0.73626 |  0:01:42s
epoch 51 | loss: 0.25762 | val_0_rmse: 0.47971 | val_1_rmse: 0.48804 |  0:01:44s
epoch 52 | loss: 0.25503 | val_0_rmse: 0.53639 | val_1_rmse: 0.53644 |  0:01:46s
epoch 53 | loss: 0.25212 | val_0_rmse: 0.48757 | val_1_rmse: 0.49096 |  0:01:48s
epoch 54 | loss: 0.24251 | val_0_rmse: 0.48396 | val_1_rmse: 0.49058 |  0:01:50s
epoch 55 | loss: 0.23965 | val_0_rmse: 0.53726 | val_1_rmse: 0.53691 |  0:01:52s
epoch 56 | loss: 0.23147 | val_0_rmse: 0.65385 | val_1_rmse: 0.65178 |  0:01:54s
epoch 57 | loss: 0.22615 | val_0_rmse: 0.52283 | val_1_rmse: 0.52449 |  0:01:56s
epoch 58 | loss: 0.22721 | val_0_rmse: 0.49995 | val_1_rmse: 0.51103 |  0:01:58s
epoch 59 | loss: 0.25472 | val_0_rmse: 0.47809 | val_1_rmse: 0.48189 |  0:02:00s
epoch 60 | loss: 0.24029 | val_0_rmse: 0.48574 | val_1_rmse: 0.48852 |  0:02:02s
epoch 61 | loss: 0.23204 | val_0_rmse: 0.47286 | val_1_rmse: 0.48219 |  0:02:04s
epoch 62 | loss: 0.23744 | val_0_rmse: 0.50732 | val_1_rmse: 0.51592 |  0:02:06s
epoch 63 | loss: 0.22586 | val_0_rmse: 0.55479 | val_1_rmse: 0.54908 |  0:02:08s
epoch 64 | loss: 0.21458 | val_0_rmse: 0.5244  | val_1_rmse: 0.52522 |  0:02:10s
epoch 65 | loss: 0.23977 | val_0_rmse: 0.47083 | val_1_rmse: 0.47304 |  0:02:12s
epoch 66 | loss: 0.22182 | val_0_rmse: 0.45257 | val_1_rmse: 0.45966 |  0:02:14s
epoch 67 | loss: 0.20852 | val_0_rmse: 0.46565 | val_1_rmse: 0.4753  |  0:02:16s
epoch 68 | loss: 0.21809 | val_0_rmse: 0.50083 | val_1_rmse: 0.50771 |  0:02:18s
epoch 69 | loss: 0.22155 | val_0_rmse: 0.47339 | val_1_rmse: 0.47923 |  0:02:20s
epoch 70 | loss: 0.20818 | val_0_rmse: 0.44113 | val_1_rmse: 0.44662 |  0:02:22s
epoch 71 | loss: 0.22664 | val_0_rmse: 0.4817  | val_1_rmse: 0.49026 |  0:02:24s
epoch 72 | loss: 0.21774 | val_0_rmse: 0.52963 | val_1_rmse: 0.53039 |  0:02:26s
epoch 73 | loss: 0.22067 | val_0_rmse: 0.47808 | val_1_rmse: 0.48364 |  0:02:28s
epoch 74 | loss: 0.21139 | val_0_rmse: 0.70714 | val_1_rmse: 0.7075  |  0:02:30s
epoch 75 | loss: 0.19876 | val_0_rmse: 0.50079 | val_1_rmse: 0.51052 |  0:02:33s
epoch 76 | loss: 0.20489 | val_0_rmse: 0.53098 | val_1_rmse: 0.53289 |  0:02:35s
epoch 77 | loss: 0.20552 | val_0_rmse: 0.48239 | val_1_rmse: 0.48353 |  0:02:37s
epoch 78 | loss: 0.18803 | val_0_rmse: 0.6138  | val_1_rmse: 0.6133  |  0:02:39s
epoch 79 | loss: 0.18644 | val_0_rmse: 0.4235  | val_1_rmse: 0.43566 |  0:02:41s
epoch 80 | loss: 0.19593 | val_0_rmse: 0.41416 | val_1_rmse: 0.42333 |  0:02:43s
epoch 81 | loss: 0.19644 | val_0_rmse: 0.41617 | val_1_rmse: 0.42492 |  0:02:45s
epoch 82 | loss: 0.19147 | val_0_rmse: 0.43674 | val_1_rmse: 0.44428 |  0:02:47s
epoch 83 | loss: 0.19109 | val_0_rmse: 0.43347 | val_1_rmse: 0.44356 |  0:02:49s
epoch 84 | loss: 0.19443 | val_0_rmse: 0.47577 | val_1_rmse: 0.478   |  0:02:51s
epoch 85 | loss: 0.19989 | val_0_rmse: 0.57452 | val_1_rmse: 0.57843 |  0:02:53s
epoch 86 | loss: 0.18739 | val_0_rmse: 0.54344 | val_1_rmse: 0.54286 |  0:02:55s
epoch 87 | loss: 0.18682 | val_0_rmse: 0.48801 | val_1_rmse: 0.48733 |  0:02:57s
epoch 88 | loss: 0.17958 | val_0_rmse: 0.47527 | val_1_rmse: 0.4811  |  0:02:59s
epoch 89 | loss: 0.20127 | val_0_rmse: 0.42042 | val_1_rmse: 0.43145 |  0:03:01s
epoch 90 | loss: 0.19572 | val_0_rmse: 0.45415 | val_1_rmse: 0.46811 |  0:03:03s
epoch 91 | loss: 0.18142 | val_0_rmse: 0.60962 | val_1_rmse: 0.6088  |  0:03:05s
epoch 92 | loss: 0.19344 | val_0_rmse: 0.45831 | val_1_rmse: 0.46851 |  0:03:07s
epoch 93 | loss: 0.20131 | val_0_rmse: 0.44886 | val_1_rmse: 0.45558 |  0:03:09s
epoch 94 | loss: 0.1919  | val_0_rmse: 0.49112 | val_1_rmse: 0.4977  |  0:03:11s
epoch 95 | loss: 0.1825  | val_0_rmse: 0.39996 | val_1_rmse: 0.40567 |  0:03:13s
epoch 96 | loss: 0.17618 | val_0_rmse: 0.397   | val_1_rmse: 0.40738 |  0:03:15s
epoch 97 | loss: 0.17942 | val_0_rmse: 0.41645 | val_1_rmse: 0.42013 |  0:03:17s
epoch 98 | loss: 0.1771  | val_0_rmse: 0.48401 | val_1_rmse: 0.48283 |  0:03:19s
epoch 99 | loss: 0.17604 | val_0_rmse: 0.43008 | val_1_rmse: 0.43809 |  0:03:21s
epoch 100| loss: 0.17116 | val_0_rmse: 0.44809 | val_1_rmse: 0.44787 |  0:03:23s
epoch 101| loss: 0.16577 | val_0_rmse: 0.53094 | val_1_rmse: 0.52997 |  0:03:25s
epoch 102| loss: 0.16421 | val_0_rmse: 0.37271 | val_1_rmse: 0.38687 |  0:03:27s
epoch 103| loss: 0.17253 | val_0_rmse: 0.59831 | val_1_rmse: 0.597   |  0:03:29s
epoch 104| loss: 0.16909 | val_0_rmse: 0.39947 | val_1_rmse: 0.40564 |  0:03:31s
epoch 105| loss: 0.16681 | val_0_rmse: 0.39977 | val_1_rmse: 0.40993 |  0:03:33s
epoch 106| loss: 0.16562 | val_0_rmse: 0.36027 | val_1_rmse: 0.37102 |  0:03:35s
epoch 107| loss: 0.15719 | val_0_rmse: 0.38117 | val_1_rmse: 0.39098 |  0:03:37s
epoch 108| loss: 0.16375 | val_0_rmse: 0.40797 | val_1_rmse: 0.41353 |  0:03:39s
epoch 109| loss: 0.16535 | val_0_rmse: 0.37069 | val_1_rmse: 0.38401 |  0:03:41s
epoch 110| loss: 0.15746 | val_0_rmse: 0.49744 | val_1_rmse: 0.49673 |  0:03:43s
epoch 111| loss: 0.16212 | val_0_rmse: 0.36195 | val_1_rmse: 0.3747  |  0:03:45s
epoch 112| loss: 0.17034 | val_0_rmse: 0.37626 | val_1_rmse: 0.38243 |  0:03:47s
epoch 113| loss: 0.16713 | val_0_rmse: 0.39199 | val_1_rmse: 0.40817 |  0:03:49s
epoch 114| loss: 0.1575  | val_0_rmse: 0.37449 | val_1_rmse: 0.38622 |  0:03:51s
epoch 115| loss: 0.18206 | val_0_rmse: 0.42879 | val_1_rmse: 0.43738 |  0:03:53s
epoch 116| loss: 0.18685 | val_0_rmse: 0.49797 | val_1_rmse: 0.44994 |  0:03:55s
epoch 117| loss: 0.16897 | val_0_rmse: 0.46635 | val_1_rmse: 0.44842 |  0:03:57s
epoch 118| loss: 0.16318 | val_0_rmse: 0.42354 | val_1_rmse: 0.38187 |  0:03:59s
epoch 119| loss: 0.15958 | val_0_rmse: 0.65172 | val_1_rmse: 0.56049 |  0:04:01s
epoch 120| loss: 0.16563 | val_0_rmse: 0.50054 | val_1_rmse: 0.4995  |  0:04:03s
epoch 121| loss: 0.15997 | val_0_rmse: 0.39861 | val_1_rmse: 0.38446 |  0:04:05s
epoch 122| loss: 0.15377 | val_0_rmse: 0.37307 | val_1_rmse: 0.38302 |  0:04:07s
epoch 123| loss: 0.15843 | val_0_rmse: 0.40885 | val_1_rmse: 0.41587 |  0:04:09s
epoch 124| loss: 0.15002 | val_0_rmse: 0.61099 | val_1_rmse: 0.59557 |  0:04:11s
epoch 125| loss: 0.15036 | val_0_rmse: 0.5562  | val_1_rmse: 0.44123 |  0:04:13s
epoch 126| loss: 0.16264 | val_0_rmse: 0.48347 | val_1_rmse: 0.47949 |  0:04:15s
epoch 127| loss: 0.16078 | val_0_rmse: 0.5666  | val_1_rmse: 0.5376  |  0:04:17s
epoch 128| loss: 0.16455 | val_0_rmse: 0.58535 | val_1_rmse: 0.56055 |  0:04:19s
epoch 129| loss: 0.15676 | val_0_rmse: 0.47648 | val_1_rmse: 0.41013 |  0:04:21s
epoch 130| loss: 0.15064 | val_0_rmse: 0.44184 | val_1_rmse: 0.38207 |  0:04:23s
epoch 131| loss: 0.14538 | val_0_rmse: 0.38834 | val_1_rmse: 0.36963 |  0:04:25s
epoch 132| loss: 0.14965 | val_0_rmse: 0.42894 | val_1_rmse: 0.41194 |  0:04:27s
epoch 133| loss: 0.15314 | val_0_rmse: 0.54479 | val_1_rmse: 0.52874 |  0:04:29s
epoch 134| loss: 0.14833 | val_0_rmse: 0.39912 | val_1_rmse: 0.39101 |  0:04:31s
epoch 135| loss: 0.14891 | val_0_rmse: 0.37542 | val_1_rmse: 0.36766 |  0:04:33s
epoch 136| loss: 0.15516 | val_0_rmse: 0.58887 | val_1_rmse: 0.56628 |  0:04:35s
epoch 137| loss: 0.15094 | val_0_rmse: 0.45874 | val_1_rmse: 0.45803 |  0:04:37s
epoch 138| loss: 0.14948 | val_0_rmse: 0.4071  | val_1_rmse: 0.40379 |  0:04:39s
epoch 139| loss: 0.15265 | val_0_rmse: 0.39688 | val_1_rmse: 0.40924 |  0:04:41s
epoch 140| loss: 0.15254 | val_0_rmse: 0.42856 | val_1_rmse: 0.44332 |  0:04:43s
epoch 141| loss: 0.14564 | val_0_rmse: 0.39642 | val_1_rmse: 0.36883 |  0:04:45s
epoch 142| loss: 0.15335 | val_0_rmse: 0.3656  | val_1_rmse: 0.37496 |  0:04:47s
epoch 143| loss: 0.15368 | val_0_rmse: 0.65525 | val_1_rmse: 0.49387 |  0:04:49s
epoch 144| loss: 0.15072 | val_0_rmse: 0.47404 | val_1_rmse: 0.48799 |  0:04:51s
epoch 145| loss: 0.14094 | val_0_rmse: 0.355   | val_1_rmse: 0.3596  |  0:04:53s
epoch 146| loss: 0.14572 | val_0_rmse: 0.35633 | val_1_rmse: 0.35211 |  0:04:55s
epoch 147| loss: 0.14742 | val_0_rmse: 0.36287 | val_1_rmse: 0.37568 |  0:04:57s
epoch 148| loss: 0.15368 | val_0_rmse: 0.39263 | val_1_rmse: 0.39712 |  0:05:00s
epoch 149| loss: 0.14903 | val_0_rmse: 0.42362 | val_1_rmse: 0.37391 |  0:05:02s
Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_val_1_rmse = 0.35211
Best weights from best epoch are automatically used!
ended training at: 16:11:46
Feature importance:
[('Area', 0.35742886197319834), ('Baths', 0.17105160029509442), ('Beds', 0.11167438567436727), ('Latitude', 0.1773452510163504), ('Longitude', 0.1192973488281974), ('Month', 0.02156637609149586), ('Year', 0.041636176121296324)]
Mean squared error is of 970495671.9475031
Mean absolute error:20865.38066297798
MAPE:0.1897398326564518
R2 score:0.8680343592434343
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:11:46
epoch 0  | loss: 0.58871 | val_0_rmse: 0.67248 | val_1_rmse: 0.67898 |  0:00:02s
epoch 1  | loss: 0.40981 | val_0_rmse: 0.62998 | val_1_rmse: 0.63042 |  0:00:04s
epoch 2  | loss: 0.40546 | val_0_rmse: 0.6962  | val_1_rmse: 0.62218 |  0:00:06s
epoch 3  | loss: 0.3922  | val_0_rmse: 0.68306 | val_1_rmse: 0.62305 |  0:00:08s
epoch 4  | loss: 0.38396 | val_0_rmse: 0.62272 | val_1_rmse: 0.61569 |  0:00:10s
epoch 5  | loss: 0.37819 | val_0_rmse: 0.62012 | val_1_rmse: 0.6238  |  0:00:12s
epoch 6  | loss: 0.37654 | val_0_rmse: 0.59514 | val_1_rmse: 0.59695 |  0:00:14s
epoch 7  | loss: 0.36529 | val_0_rmse: 0.59037 | val_1_rmse: 0.59    |  0:00:16s
epoch 8  | loss: 0.36823 | val_0_rmse: 0.59672 | val_1_rmse: 0.59687 |  0:00:18s
epoch 9  | loss: 0.36019 | val_0_rmse: 0.59022 | val_1_rmse: 0.59404 |  0:00:20s
epoch 10 | loss: 0.36036 | val_0_rmse: 0.58966 | val_1_rmse: 0.58904 |  0:00:22s
epoch 11 | loss: 0.36894 | val_0_rmse: 0.58934 | val_1_rmse: 0.58603 |  0:00:24s
epoch 12 | loss: 0.35171 | val_0_rmse: 0.59559 | val_1_rmse: 0.5966  |  0:00:26s
epoch 13 | loss: 0.36481 | val_0_rmse: 0.58419 | val_1_rmse: 0.58496 |  0:00:28s
epoch 14 | loss: 0.3673  | val_0_rmse: 0.60395 | val_1_rmse: 0.60313 |  0:00:30s
epoch 15 | loss: 0.36315 | val_0_rmse: 0.57821 | val_1_rmse: 0.57581 |  0:00:32s
epoch 16 | loss: 0.35469 | val_0_rmse: 0.58835 | val_1_rmse: 0.58551 |  0:00:34s
epoch 17 | loss: 0.36508 | val_0_rmse: 0.58667 | val_1_rmse: 0.58076 |  0:00:36s
epoch 18 | loss: 0.35033 | val_0_rmse: 0.58674 | val_1_rmse: 0.57887 |  0:00:38s
epoch 19 | loss: 0.35039 | val_0_rmse: 0.58309 | val_1_rmse: 0.58213 |  0:00:40s
epoch 20 | loss: 0.34948 | val_0_rmse: 0.5797  | val_1_rmse: 0.57548 |  0:00:42s
epoch 21 | loss: 0.3419  | val_0_rmse: 0.57275 | val_1_rmse: 0.56944 |  0:00:44s
epoch 22 | loss: 0.34    | val_0_rmse: 0.56907 | val_1_rmse: 0.56685 |  0:00:46s
epoch 23 | loss: 0.33497 | val_0_rmse: 0.56932 | val_1_rmse: 0.57004 |  0:00:48s
epoch 24 | loss: 0.3447  | val_0_rmse: 0.57461 | val_1_rmse: 0.576   |  0:00:50s
epoch 25 | loss: 0.33723 | val_0_rmse: 0.57272 | val_1_rmse: 0.56902 |  0:00:52s
epoch 26 | loss: 0.34226 | val_0_rmse: 0.57392 | val_1_rmse: 0.57241 |  0:00:54s
epoch 27 | loss: 0.33681 | val_0_rmse: 0.57987 | val_1_rmse: 0.58208 |  0:00:56s
epoch 28 | loss: 0.3393  | val_0_rmse: 0.57052 | val_1_rmse: 0.56453 |  0:00:58s
epoch 29 | loss: 0.34059 | val_0_rmse: 0.57145 | val_1_rmse: 0.56911 |  0:01:00s
epoch 30 | loss: 0.337   | val_0_rmse: 0.57103 | val_1_rmse: 0.56941 |  0:01:02s
epoch 31 | loss: 0.33473 | val_0_rmse: 0.56985 | val_1_rmse: 0.56758 |  0:01:04s
epoch 32 | loss: 0.3326  | val_0_rmse: 0.56074 | val_1_rmse: 0.56036 |  0:01:06s
epoch 33 | loss: 0.3238  | val_0_rmse: 0.54692 | val_1_rmse: 0.54483 |  0:01:08s
epoch 34 | loss: 0.33327 | val_0_rmse: 0.57422 | val_1_rmse: 0.57475 |  0:01:10s
epoch 35 | loss: 0.33377 | val_0_rmse: 0.56323 | val_1_rmse: 0.56145 |  0:01:12s
epoch 36 | loss: 0.3285  | val_0_rmse: 0.54776 | val_1_rmse: 0.55122 |  0:01:14s
epoch 37 | loss: 0.32142 | val_0_rmse: 0.55519 | val_1_rmse: 0.55809 |  0:01:16s
epoch 38 | loss: 0.33104 | val_0_rmse: 0.5769  | val_1_rmse: 0.57751 |  0:01:19s
epoch 39 | loss: 0.3354  | val_0_rmse: 0.55298 | val_1_rmse: 0.55267 |  0:01:21s
epoch 40 | loss: 0.31873 | val_0_rmse: 0.55901 | val_1_rmse: 0.56296 |  0:01:23s
epoch 41 | loss: 0.32446 | val_0_rmse: 0.54068 | val_1_rmse: 0.54484 |  0:01:25s
epoch 42 | loss: 0.31435 | val_0_rmse: 0.54419 | val_1_rmse: 0.54612 |  0:01:27s
epoch 43 | loss: 0.3161  | val_0_rmse: 0.55733 | val_1_rmse: 0.5625  |  0:01:29s
epoch 44 | loss: 0.31571 | val_0_rmse: 0.55854 | val_1_rmse: 0.56227 |  0:01:31s
epoch 45 | loss: 0.32797 | val_0_rmse: 0.57463 | val_1_rmse: 0.57329 |  0:01:33s
epoch 46 | loss: 0.3368  | val_0_rmse: 0.56192 | val_1_rmse: 0.56243 |  0:01:35s
epoch 47 | loss: 0.32787 | val_0_rmse: 0.58386 | val_1_rmse: 0.5804  |  0:01:37s
epoch 48 | loss: 0.33711 | val_0_rmse: 0.55427 | val_1_rmse: 0.5555  |  0:01:39s
epoch 49 | loss: 0.32798 | val_0_rmse: 0.54549 | val_1_rmse: 0.5474  |  0:01:41s
epoch 50 | loss: 0.32116 | val_0_rmse: 0.55832 | val_1_rmse: 0.55738 |  0:01:43s
epoch 51 | loss: 0.31423 | val_0_rmse: 0.54216 | val_1_rmse: 0.54511 |  0:01:45s
epoch 52 | loss: 0.31556 | val_0_rmse: 0.55185 | val_1_rmse: 0.55473 |  0:01:47s
epoch 53 | loss: 0.30546 | val_0_rmse: 0.54036 | val_1_rmse: 0.5406  |  0:01:49s
epoch 54 | loss: 0.3028  | val_0_rmse: 0.53474 | val_1_rmse: 0.53647 |  0:01:51s
epoch 55 | loss: 0.2997  | val_0_rmse: 0.52723 | val_1_rmse: 0.53064 |  0:01:53s
epoch 56 | loss: 0.30373 | val_0_rmse: 0.54567 | val_1_rmse: 0.5472  |  0:01:55s
epoch 57 | loss: 0.30351 | val_0_rmse: 0.52442 | val_1_rmse: 0.52654 |  0:01:57s
epoch 58 | loss: 0.29126 | val_0_rmse: 0.52419 | val_1_rmse: 0.52449 |  0:01:59s
epoch 59 | loss: 0.28973 | val_0_rmse: 0.54381 | val_1_rmse: 0.55021 |  0:02:01s
epoch 60 | loss: 0.29454 | val_0_rmse: 0.53406 | val_1_rmse: 0.53625 |  0:02:03s
epoch 61 | loss: 0.29744 | val_0_rmse: 0.52877 | val_1_rmse: 0.53598 |  0:02:05s
epoch 62 | loss: 0.29443 | val_0_rmse: 0.5245  | val_1_rmse: 0.52964 |  0:02:07s
epoch 63 | loss: 0.29533 | val_0_rmse: 0.53589 | val_1_rmse: 0.53712 |  0:02:09s
epoch 64 | loss: 0.29545 | val_0_rmse: 0.53125 | val_1_rmse: 0.53597 |  0:02:11s
epoch 65 | loss: 0.29382 | val_0_rmse: 0.51814 | val_1_rmse: 0.52263 |  0:02:13s
epoch 66 | loss: 0.28963 | val_0_rmse: 0.51627 | val_1_rmse: 0.51816 |  0:02:15s
epoch 67 | loss: 0.29527 | val_0_rmse: 0.53561 | val_1_rmse: 0.5373  |  0:02:17s
epoch 68 | loss: 0.30016 | val_0_rmse: 0.5338  | val_1_rmse: 0.53519 |  0:02:19s
epoch 69 | loss: 0.28748 | val_0_rmse: 0.51952 | val_1_rmse: 0.52221 |  0:02:21s
epoch 70 | loss: 0.28676 | val_0_rmse: 0.52125 | val_1_rmse: 0.527   |  0:02:23s
epoch 71 | loss: 0.28403 | val_0_rmse: 0.53183 | val_1_rmse: 0.53574 |  0:02:25s
epoch 72 | loss: 0.28911 | val_0_rmse: 0.5214  | val_1_rmse: 0.5262  |  0:02:27s
epoch 73 | loss: 0.28185 | val_0_rmse: 0.51931 | val_1_rmse: 0.51897 |  0:02:29s
epoch 74 | loss: 0.29098 | val_0_rmse: 0.52585 | val_1_rmse: 0.52823 |  0:02:31s
epoch 75 | loss: 0.28147 | val_0_rmse: 0.50263 | val_1_rmse: 0.50987 |  0:02:33s
epoch 76 | loss: 0.27973 | val_0_rmse: 0.50722 | val_1_rmse: 0.50877 |  0:02:35s
epoch 77 | loss: 0.29629 | val_0_rmse: 0.55282 | val_1_rmse: 0.55659 |  0:02:37s
epoch 78 | loss: 0.29049 | val_0_rmse: 0.52817 | val_1_rmse: 0.53199 |  0:02:39s
epoch 79 | loss: 0.28589 | val_0_rmse: 0.49866 | val_1_rmse: 0.50309 |  0:02:41s
epoch 80 | loss: 0.27979 | val_0_rmse: 0.55367 | val_1_rmse: 0.55421 |  0:02:43s
epoch 81 | loss: 0.29712 | val_0_rmse: 0.52721 | val_1_rmse: 0.53016 |  0:02:45s
epoch 82 | loss: 0.28153 | val_0_rmse: 0.50938 | val_1_rmse: 0.51084 |  0:02:47s
epoch 83 | loss: 0.28427 | val_0_rmse: 0.51009 | val_1_rmse: 0.51262 |  0:02:49s
epoch 84 | loss: 0.27138 | val_0_rmse: 0.48409 | val_1_rmse: 0.4943  |  0:02:51s
epoch 85 | loss: 0.27687 | val_0_rmse: 0.51011 | val_1_rmse: 0.51525 |  0:02:53s
epoch 86 | loss: 0.30498 | val_0_rmse: 0.54045 | val_1_rmse: 0.53911 |  0:02:55s
epoch 87 | loss: 0.29136 | val_0_rmse: 0.56053 | val_1_rmse: 0.55995 |  0:02:57s
epoch 88 | loss: 0.30427 | val_0_rmse: 0.54099 | val_1_rmse: 0.54118 |  0:02:59s
epoch 89 | loss: 0.28714 | val_0_rmse: 0.51236 | val_1_rmse: 0.51031 |  0:03:02s
epoch 90 | loss: 0.2786  | val_0_rmse: 0.50128 | val_1_rmse: 0.50374 |  0:03:04s
epoch 91 | loss: 0.26784 | val_0_rmse: 0.49486 | val_1_rmse: 0.49201 |  0:03:06s
epoch 92 | loss: 0.25799 | val_0_rmse: 0.46519 | val_1_rmse: 0.46791 |  0:03:08s
epoch 93 | loss: 0.24314 | val_0_rmse: 0.48747 | val_1_rmse: 0.48757 |  0:03:10s
epoch 94 | loss: 0.25262 | val_0_rmse: 0.48246 | val_1_rmse: 0.48435 |  0:03:12s
epoch 95 | loss: 0.26238 | val_0_rmse: 0.49279 | val_1_rmse: 0.49295 |  0:03:14s
epoch 96 | loss: 0.25509 | val_0_rmse: 0.52252 | val_1_rmse: 0.52121 |  0:03:16s
epoch 97 | loss: 0.25158 | val_0_rmse: 0.45416 | val_1_rmse: 0.46074 |  0:03:18s
epoch 98 | loss: 0.27736 | val_0_rmse: 0.50599 | val_1_rmse: 0.50308 |  0:03:20s
epoch 99 | loss: 0.27827 | val_0_rmse: 0.5551  | val_1_rmse: 0.55642 |  0:03:22s
epoch 100| loss: 0.31087 | val_0_rmse: 0.55568 | val_1_rmse: 0.55563 |  0:03:24s
epoch 101| loss: 0.28091 | val_0_rmse: 0.57724 | val_1_rmse: 0.57799 |  0:03:26s
epoch 102| loss: 0.26316 | val_0_rmse: 0.47612 | val_1_rmse: 0.47728 |  0:03:28s
epoch 103| loss: 0.25761 | val_0_rmse: 0.48289 | val_1_rmse: 0.48159 |  0:03:30s
epoch 104| loss: 0.24948 | val_0_rmse: 0.46557 | val_1_rmse: 0.46445 |  0:03:32s
epoch 105| loss: 0.249   | val_0_rmse: 0.47596 | val_1_rmse: 0.47589 |  0:03:34s
epoch 106| loss: 0.23836 | val_0_rmse: 0.48746 | val_1_rmse: 0.49012 |  0:03:36s
epoch 107| loss: 0.25794 | val_0_rmse: 0.48915 | val_1_rmse: 0.49209 |  0:03:38s
epoch 108| loss: 0.23391 | val_0_rmse: 0.52479 | val_1_rmse: 0.53449 |  0:03:40s
epoch 109| loss: 0.23217 | val_0_rmse: 0.44642 | val_1_rmse: 0.44915 |  0:03:42s
epoch 110| loss: 0.22418 | val_0_rmse: 0.50539 | val_1_rmse: 0.50902 |  0:03:44s
epoch 111| loss: 0.21806 | val_0_rmse: 0.49916 | val_1_rmse: 0.50516 |  0:03:46s
epoch 112| loss: 0.21354 | val_0_rmse: 0.48595 | val_1_rmse: 0.49791 |  0:03:48s
epoch 113| loss: 0.21173 | val_0_rmse: 0.46252 | val_1_rmse: 0.47112 |  0:03:50s
epoch 114| loss: 0.21956 | val_0_rmse: 0.46283 | val_1_rmse: 0.47129 |  0:03:52s
epoch 115| loss: 0.21201 | val_0_rmse: 0.43739 | val_1_rmse: 0.45038 |  0:03:54s
epoch 116| loss: 0.21963 | val_0_rmse: 0.52305 | val_1_rmse: 0.53323 |  0:03:56s
epoch 117| loss: 0.22757 | val_0_rmse: 0.44756 | val_1_rmse: 0.45615 |  0:03:58s
epoch 118| loss: 0.21467 | val_0_rmse: 0.43703 | val_1_rmse: 0.4407  |  0:04:00s
epoch 119| loss: 0.20752 | val_0_rmse: 0.50259 | val_1_rmse: 0.50907 |  0:04:02s
epoch 120| loss: 0.20402 | val_0_rmse: 0.48388 | val_1_rmse: 0.48857 |  0:04:04s
epoch 121| loss: 0.19674 | val_0_rmse: 0.45955 | val_1_rmse: 0.46922 |  0:04:06s
epoch 122| loss: 0.20454 | val_0_rmse: 0.41974 | val_1_rmse: 0.43143 |  0:04:08s
epoch 123| loss: 0.20341 | val_0_rmse: 0.45071 | val_1_rmse: 0.45853 |  0:04:10s
epoch 124| loss: 0.20614 | val_0_rmse: 0.48419 | val_1_rmse: 0.49132 |  0:04:12s
epoch 125| loss: 0.20606 | val_0_rmse: 0.44551 | val_1_rmse: 0.44968 |  0:04:14s
epoch 126| loss: 0.20682 | val_0_rmse: 0.68475 | val_1_rmse: 0.69967 |  0:04:16s
epoch 127| loss: 0.19896 | val_0_rmse: 0.43605 | val_1_rmse: 0.44373 |  0:04:18s
epoch 128| loss: 0.19435 | val_0_rmse: 0.51137 | val_1_rmse: 0.51947 |  0:04:20s
epoch 129| loss: 0.19261 | val_0_rmse: 0.67904 | val_1_rmse: 0.68707 |  0:04:22s
epoch 130| loss: 0.19894 | val_0_rmse: 0.45005 | val_1_rmse: 0.45646 |  0:04:24s
epoch 131| loss: 0.19452 | val_0_rmse: 0.42037 | val_1_rmse: 0.42818 |  0:04:26s
epoch 132| loss: 0.1992  | val_0_rmse: 0.41101 | val_1_rmse: 0.42029 |  0:04:28s
epoch 133| loss: 0.19712 | val_0_rmse: 0.48446 | val_1_rmse: 0.49448 |  0:04:31s
epoch 134| loss: 0.194   | val_0_rmse: 0.49453 | val_1_rmse: 0.50125 |  0:04:32s
epoch 135| loss: 0.19522 | val_0_rmse: 0.41338 | val_1_rmse: 0.42486 |  0:04:34s
epoch 136| loss: 0.192   | val_0_rmse: 0.55418 | val_1_rmse: 0.56112 |  0:04:36s
epoch 137| loss: 0.20758 | val_0_rmse: 0.47156 | val_1_rmse: 0.48504 |  0:04:38s
epoch 138| loss: 0.19484 | val_0_rmse: 0.41704 | val_1_rmse: 0.41988 |  0:04:40s
epoch 139| loss: 0.18967 | val_0_rmse: 0.58897 | val_1_rmse: 0.59477 |  0:04:43s
epoch 140| loss: 0.18871 | val_0_rmse: 0.4686  | val_1_rmse: 0.47311 |  0:04:45s
epoch 141| loss: 0.18452 | val_0_rmse: 0.61493 | val_1_rmse: 0.62453 |  0:04:47s
epoch 142| loss: 0.18865 | val_0_rmse: 0.46387 | val_1_rmse: 0.4735  |  0:04:49s
epoch 143| loss: 0.17791 | val_0_rmse: 0.383   | val_1_rmse: 0.39668 |  0:04:51s
epoch 144| loss: 0.19655 | val_0_rmse: 0.43984 | val_1_rmse: 0.44756 |  0:04:53s
epoch 145| loss: 0.20095 | val_0_rmse: 0.41987 | val_1_rmse: 0.42839 |  0:04:55s
epoch 146| loss: 0.18707 | val_0_rmse: 0.39587 | val_1_rmse: 0.40705 |  0:04:57s
epoch 147| loss: 0.18497 | val_0_rmse: 0.39304 | val_1_rmse: 0.40653 |  0:04:59s
epoch 148| loss: 0.1915  | val_0_rmse: 0.41845 | val_1_rmse: 0.43112 |  0:05:01s
epoch 149| loss: 0.18414 | val_0_rmse: 0.41875 | val_1_rmse: 0.42528 |  0:05:03s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.39668
Best weights from best epoch are automatically used!
ended training at: 16:16:50
Feature importance:
[('Area', 0.2530619036658808), ('Baths', 0.1946658420201704), ('Beds', 0.1388037588770426), ('Latitude', 0.09861299091629847), ('Longitude', 0.20546998176143724), ('Month', 0.08141637236809622), ('Year', 0.027969150391074223)]
Mean squared error is of 1169587917.9134007
Mean absolute error:23587.951761262164
MAPE:0.20620149211286012
R2 score:0.8417720366082868
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:16:51
epoch 0  | loss: 0.59704 | val_0_rmse: 0.71084 | val_1_rmse: 0.70556 |  0:00:01s
epoch 1  | loss: 0.41917 | val_0_rmse: 0.64076 | val_1_rmse: 0.63996 |  0:00:04s
epoch 2  | loss: 0.4043  | val_0_rmse: 0.61371 | val_1_rmse: 0.61088 |  0:00:06s
epoch 3  | loss: 0.39087 | val_0_rmse: 0.61231 | val_1_rmse: 0.60753 |  0:00:08s
epoch 4  | loss: 0.38258 | val_0_rmse: 0.59962 | val_1_rmse: 0.59565 |  0:00:10s
epoch 5  | loss: 0.38566 | val_0_rmse: 0.60411 | val_1_rmse: 0.60049 |  0:00:12s
epoch 6  | loss: 0.37436 | val_0_rmse: 0.60089 | val_1_rmse: 0.59778 |  0:00:14s
epoch 7  | loss: 0.37139 | val_0_rmse: 0.59327 | val_1_rmse: 0.5895  |  0:00:16s
epoch 8  | loss: 0.37398 | val_0_rmse: 0.59259 | val_1_rmse: 0.59137 |  0:00:18s
epoch 9  | loss: 0.36368 | val_0_rmse: 0.60018 | val_1_rmse: 0.59462 |  0:00:20s
epoch 10 | loss: 0.36632 | val_0_rmse: 0.58984 | val_1_rmse: 0.58871 |  0:00:22s
epoch 11 | loss: 0.36074 | val_0_rmse: 0.59048 | val_1_rmse: 0.58801 |  0:00:24s
epoch 12 | loss: 0.36659 | val_0_rmse: 0.59512 | val_1_rmse: 0.59672 |  0:00:26s
epoch 13 | loss: 0.34869 | val_0_rmse: 0.5719  | val_1_rmse: 0.57437 |  0:00:28s
epoch 14 | loss: 0.34323 | val_0_rmse: 0.56818 | val_1_rmse: 0.56847 |  0:00:30s
epoch 15 | loss: 0.33254 | val_0_rmse: 0.55754 | val_1_rmse: 0.56003 |  0:00:32s
epoch 16 | loss: 0.33708 | val_0_rmse: 0.55752 | val_1_rmse: 0.56164 |  0:00:34s
epoch 17 | loss: 0.33092 | val_0_rmse: 0.5582  | val_1_rmse: 0.56023 |  0:00:36s
epoch 18 | loss: 0.313   | val_0_rmse: 0.62799 | val_1_rmse: 0.62417 |  0:00:38s
epoch 19 | loss: 0.3192  | val_0_rmse: 0.56041 | val_1_rmse: 0.56588 |  0:00:40s
epoch 20 | loss: 0.30173 | val_0_rmse: 0.58062 | val_1_rmse: 0.57843 |  0:00:42s
epoch 21 | loss: 0.30841 | val_0_rmse: 0.56102 | val_1_rmse: 0.56361 |  0:00:44s
epoch 22 | loss: 0.2991  | val_0_rmse: 0.52878 | val_1_rmse: 0.53361 |  0:00:46s
epoch 23 | loss: 0.29695 | val_0_rmse: 0.51045 | val_1_rmse: 0.51521 |  0:00:48s
epoch 24 | loss: 0.31698 | val_0_rmse: 0.57499 | val_1_rmse: 0.58098 |  0:00:50s
epoch 25 | loss: 0.32226 | val_0_rmse: 0.5601  | val_1_rmse: 0.56593 |  0:00:52s
epoch 26 | loss: 0.29483 | val_0_rmse: 0.57765 | val_1_rmse: 0.58183 |  0:00:54s
epoch 27 | loss: 0.27287 | val_0_rmse: 0.61472 | val_1_rmse: 0.61205 |  0:00:56s
epoch 28 | loss: 0.27805 | val_0_rmse: 0.51313 | val_1_rmse: 0.51797 |  0:00:58s
epoch 29 | loss: 0.276   | val_0_rmse: 0.51836 | val_1_rmse: 0.52889 |  0:01:00s
epoch 30 | loss: 0.26447 | val_0_rmse: 0.64812 | val_1_rmse: 0.64595 |  0:01:02s
epoch 31 | loss: 0.27421 | val_0_rmse: 0.51778 | val_1_rmse: 0.52464 |  0:01:04s
epoch 32 | loss: 0.28336 | val_0_rmse: 0.57103 | val_1_rmse: 0.57486 |  0:01:06s
epoch 33 | loss: 0.27424 | val_0_rmse: 0.57647 | val_1_rmse: 0.57957 |  0:01:08s
epoch 34 | loss: 0.2617  | val_0_rmse: 0.53917 | val_1_rmse: 0.54482 |  0:01:10s
epoch 35 | loss: 0.26345 | val_0_rmse: 0.50252 | val_1_rmse: 0.51035 |  0:01:12s
epoch 36 | loss: 0.26422 | val_0_rmse: 0.62006 | val_1_rmse: 0.62142 |  0:01:14s
epoch 37 | loss: 0.26958 | val_0_rmse: 0.50716 | val_1_rmse: 0.51744 |  0:01:16s
epoch 38 | loss: 0.27696 | val_0_rmse: 0.51404 | val_1_rmse: 0.52029 |  0:01:18s
epoch 39 | loss: 0.27029 | val_0_rmse: 0.63684 | val_1_rmse: 0.63715 |  0:01:20s
epoch 40 | loss: 0.27511 | val_0_rmse: 0.49804 | val_1_rmse: 0.50367 |  0:01:22s
epoch 41 | loss: 0.25692 | val_0_rmse: 0.49445 | val_1_rmse: 0.5036  |  0:01:24s
epoch 42 | loss: 0.26356 | val_0_rmse: 0.52178 | val_1_rmse: 0.52965 |  0:01:26s
epoch 43 | loss: 0.27155 | val_0_rmse: 0.49785 | val_1_rmse: 0.49835 |  0:01:28s
epoch 44 | loss: 0.27195 | val_0_rmse: 0.55754 | val_1_rmse: 0.56456 |  0:01:30s
epoch 45 | loss: 0.26216 | val_0_rmse: 0.51125 | val_1_rmse: 0.52555 |  0:01:32s
epoch 46 | loss: 0.2567  | val_0_rmse: 0.62997 | val_1_rmse: 0.62463 |  0:01:34s
epoch 47 | loss: 0.24378 | val_0_rmse: 0.50012 | val_1_rmse: 0.50568 |  0:01:36s
epoch 48 | loss: 0.25415 | val_0_rmse: 0.54109 | val_1_rmse: 0.5445  |  0:01:38s
epoch 49 | loss: 0.25071 | val_0_rmse: 0.49106 | val_1_rmse: 0.49693 |  0:01:40s
epoch 50 | loss: 0.25514 | val_0_rmse: 0.53471 | val_1_rmse: 0.53889 |  0:01:42s
epoch 51 | loss: 0.25146 | val_0_rmse: 0.51855 | val_1_rmse: 0.52602 |  0:01:44s
epoch 52 | loss: 0.254   | val_0_rmse: 0.50511 | val_1_rmse: 0.51082 |  0:01:47s
epoch 53 | loss: 0.24323 | val_0_rmse: 0.59088 | val_1_rmse: 0.59223 |  0:01:49s
epoch 54 | loss: 0.26561 | val_0_rmse: 0.61902 | val_1_rmse: 0.6147  |  0:01:51s
epoch 55 | loss: 0.28999 | val_0_rmse: 0.53781 | val_1_rmse: 0.54172 |  0:01:53s
epoch 56 | loss: 0.27522 | val_0_rmse: 0.52061 | val_1_rmse: 0.53021 |  0:01:55s
epoch 57 | loss: 0.25816 | val_0_rmse: 0.51303 | val_1_rmse: 0.51767 |  0:01:57s
epoch 58 | loss: 0.26603 | val_0_rmse: 0.72394 | val_1_rmse: 0.71992 |  0:01:59s
epoch 59 | loss: 0.25455 | val_0_rmse: 0.55525 | val_1_rmse: 0.55827 |  0:02:01s
epoch 60 | loss: 0.25757 | val_0_rmse: 0.56205 | val_1_rmse: 0.56539 |  0:02:03s
epoch 61 | loss: 0.24834 | val_0_rmse: 0.49488 | val_1_rmse: 0.50187 |  0:02:05s
epoch 62 | loss: 0.24295 | val_0_rmse: 0.51231 | val_1_rmse: 0.51649 |  0:02:07s
epoch 63 | loss: 0.25224 | val_0_rmse: 0.54672 | val_1_rmse: 0.54811 |  0:02:09s
epoch 64 | loss: 0.26518 | val_0_rmse: 0.71835 | val_1_rmse: 0.72153 |  0:02:11s
epoch 65 | loss: 0.25844 | val_0_rmse: 0.51044 | val_1_rmse: 0.51986 |  0:02:13s
epoch 66 | loss: 0.27921 | val_0_rmse: 0.5556  | val_1_rmse: 0.55652 |  0:02:15s
epoch 67 | loss: 0.262   | val_0_rmse: 0.49721 | val_1_rmse: 0.50588 |  0:02:17s
epoch 68 | loss: 0.25118 | val_0_rmse: 0.56277 | val_1_rmse: 0.57167 |  0:02:19s
epoch 69 | loss: 0.24275 | val_0_rmse: 0.46837 | val_1_rmse: 0.47746 |  0:02:21s
epoch 70 | loss: 0.24201 | val_0_rmse: 0.50154 | val_1_rmse: 0.50968 |  0:02:23s
epoch 71 | loss: 0.259   | val_0_rmse: 0.48978 | val_1_rmse: 0.49557 |  0:02:25s
epoch 72 | loss: 0.27963 | val_0_rmse: 0.63606 | val_1_rmse: 0.6171  |  0:02:27s
epoch 73 | loss: 0.28447 | val_0_rmse: 0.56797 | val_1_rmse: 0.57392 |  0:02:29s
epoch 74 | loss: 0.29036 | val_0_rmse: 0.60044 | val_1_rmse: 0.60413 |  0:02:31s
epoch 75 | loss: 0.28468 | val_0_rmse: 0.59196 | val_1_rmse: 0.59691 |  0:02:34s
epoch 76 | loss: 0.27871 | val_0_rmse: 0.79551 | val_1_rmse: 0.7778  |  0:02:36s
epoch 77 | loss: 0.30689 | val_0_rmse: 0.53845 | val_1_rmse: 0.53972 |  0:02:38s
epoch 78 | loss: 0.28904 | val_0_rmse: 0.56869 | val_1_rmse: 0.56628 |  0:02:41s
epoch 79 | loss: 0.28437 | val_0_rmse: 0.50919 | val_1_rmse: 0.5115  |  0:02:43s
epoch 80 | loss: 0.27635 | val_0_rmse: 0.49736 | val_1_rmse: 0.49812 |  0:02:45s
epoch 81 | loss: 0.26183 | val_0_rmse: 0.55575 | val_1_rmse: 0.55646 |  0:02:47s
epoch 82 | loss: 0.266   | val_0_rmse: 0.65598 | val_1_rmse: 0.65649 |  0:02:49s
epoch 83 | loss: 0.26019 | val_0_rmse: 0.63854 | val_1_rmse: 0.63847 |  0:02:51s
epoch 84 | loss: 0.26967 | val_0_rmse: 0.58795 | val_1_rmse: 0.58399 |  0:02:53s
epoch 85 | loss: 0.26045 | val_0_rmse: 0.63289 | val_1_rmse: 0.6292  |  0:02:56s
epoch 86 | loss: 0.25585 | val_0_rmse: 0.48512 | val_1_rmse: 0.48684 |  0:02:59s
epoch 87 | loss: 0.25421 | val_0_rmse: 0.52254 | val_1_rmse: 0.52785 |  0:03:01s
epoch 88 | loss: 0.25949 | val_0_rmse: 0.57877 | val_1_rmse: 0.57713 |  0:03:03s
epoch 89 | loss: 0.258   | val_0_rmse: 0.67426 | val_1_rmse: 0.66589 |  0:03:05s
epoch 90 | loss: 0.25443 | val_0_rmse: 0.49323 | val_1_rmse: 0.4946  |  0:03:07s
epoch 91 | loss: 0.25373 | val_0_rmse: 0.5679  | val_1_rmse: 0.56421 |  0:03:09s
epoch 92 | loss: 0.25249 | val_0_rmse: 0.56843 | val_1_rmse: 0.56753 |  0:03:11s
epoch 93 | loss: 0.25414 | val_0_rmse: 0.51019 | val_1_rmse: 0.5118  |  0:03:13s
epoch 94 | loss: 0.25447 | val_0_rmse: 0.50077 | val_1_rmse: 0.50234 |  0:03:15s
epoch 95 | loss: 0.24397 | val_0_rmse: 0.48294 | val_1_rmse: 0.4841  |  0:03:17s
epoch 96 | loss: 0.24874 | val_0_rmse: 0.63491 | val_1_rmse: 0.6286  |  0:03:19s
epoch 97 | loss: 0.24125 | val_0_rmse: 0.66507 | val_1_rmse: 0.65979 |  0:03:21s
epoch 98 | loss: 0.237   | val_0_rmse: 0.56948 | val_1_rmse: 0.56942 |  0:03:23s
epoch 99 | loss: 0.23597 | val_0_rmse: 0.48005 | val_1_rmse: 0.48416 |  0:03:25s

Early stopping occured at epoch 99 with best_epoch = 69 and best_val_1_rmse = 0.47746
Best weights from best epoch are automatically used!
ended training at: 16:20:17
Feature importance:
[('Area', 0.30303212681566166), ('Baths', 0.12788763497683056), ('Beds', 0.03134163201101388), ('Latitude', 0.26936893530954764), ('Longitude', 0.21789336968302386), ('Month', 0.02002725197887846), ('Year', 0.030449049225043944)]
Mean squared error is of 1630707492.8177388
Mean absolute error:29133.397286075
MAPE:0.2508758115122927
R2 score:0.7766671208425227
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:20:18
epoch 0  | loss: 0.62208 | val_0_rmse: 0.6755  | val_1_rmse: 0.66453 |  0:00:01s
epoch 1  | loss: 0.42644 | val_0_rmse: 0.6878  | val_1_rmse: 0.6437  |  0:00:03s
epoch 2  | loss: 0.40626 | val_0_rmse: 0.62174 | val_1_rmse: 0.61413 |  0:00:05s
epoch 3  | loss: 0.39753 | val_0_rmse: 0.63354 | val_1_rmse: 0.62766 |  0:00:07s
epoch 4  | loss: 0.3881  | val_0_rmse: 0.61753 | val_1_rmse: 0.61176 |  0:00:09s
epoch 5  | loss: 0.3853  | val_0_rmse: 0.60863 | val_1_rmse: 0.59771 |  0:00:12s
epoch 6  | loss: 0.38267 | val_0_rmse: 0.61078 | val_1_rmse: 0.60268 |  0:00:14s
epoch 7  | loss: 0.37772 | val_0_rmse: 0.60614 | val_1_rmse: 0.60095 |  0:00:16s
epoch 8  | loss: 0.371   | val_0_rmse: 0.60508 | val_1_rmse: 0.60023 |  0:00:18s
epoch 9  | loss: 0.37187 | val_0_rmse: 0.59106 | val_1_rmse: 0.58393 |  0:00:19s
epoch 10 | loss: 0.35977 | val_0_rmse: 0.63354 | val_1_rmse: 0.62654 |  0:00:21s
epoch 11 | loss: 0.35606 | val_0_rmse: 0.65532 | val_1_rmse: 0.65366 |  0:00:23s
epoch 12 | loss: 0.33847 | val_0_rmse: 0.58645 | val_1_rmse: 0.57879 |  0:00:26s
epoch 13 | loss: 0.32651 | val_0_rmse: 0.5539  | val_1_rmse: 0.54207 |  0:00:28s
epoch 14 | loss: 0.32884 | val_0_rmse: 0.56126 | val_1_rmse: 0.55422 |  0:00:30s
epoch 15 | loss: 0.31268 | val_0_rmse: 0.55357 | val_1_rmse: 0.54105 |  0:00:31s
epoch 16 | loss: 0.30647 | val_0_rmse: 0.55818 | val_1_rmse: 0.55104 |  0:00:33s
epoch 17 | loss: 0.30917 | val_0_rmse: 0.66084 | val_1_rmse: 0.65704 |  0:00:35s
epoch 18 | loss: 0.30349 | val_0_rmse: 0.55259 | val_1_rmse: 0.5521  |  0:00:38s
epoch 19 | loss: 0.28182 | val_0_rmse: 0.69265 | val_1_rmse: 0.6998  |  0:00:40s
epoch 20 | loss: 0.27624 | val_0_rmse: 0.6811  | val_1_rmse: 0.67928 |  0:00:42s
epoch 21 | loss: 0.28462 | val_0_rmse: 0.64723 | val_1_rmse: 0.64515 |  0:00:44s
epoch 22 | loss: 0.28131 | val_0_rmse: 0.54018 | val_1_rmse: 0.52534 |  0:00:46s
epoch 23 | loss: 0.28738 | val_0_rmse: 0.69885 | val_1_rmse: 0.70397 |  0:00:47s
epoch 24 | loss: 0.28562 | val_0_rmse: 0.50704 | val_1_rmse: 0.50294 |  0:00:50s
epoch 25 | loss: 0.27394 | val_0_rmse: 0.53528 | val_1_rmse: 0.52956 |  0:00:52s
epoch 26 | loss: 0.26667 | val_0_rmse: 0.55344 | val_1_rmse: 0.5559  |  0:00:54s
epoch 27 | loss: 0.26406 | val_0_rmse: 0.57094 | val_1_rmse: 0.5573  |  0:00:56s
epoch 28 | loss: 0.25617 | val_0_rmse: 0.73011 | val_1_rmse: 0.73682 |  0:00:58s
epoch 29 | loss: 0.26565 | val_0_rmse: 0.6267  | val_1_rmse: 0.62996 |  0:01:00s
epoch 30 | loss: 0.2818  | val_0_rmse: 0.63786 | val_1_rmse: 0.63821 |  0:01:02s
epoch 31 | loss: 0.28452 | val_0_rmse: 0.56957 | val_1_rmse: 0.56275 |  0:01:04s
epoch 32 | loss: 0.26902 | val_0_rmse: 0.56776 | val_1_rmse: 0.55166 |  0:01:06s
epoch 33 | loss: 0.29696 | val_0_rmse: 0.52443 | val_1_rmse: 0.51294 |  0:01:08s
epoch 34 | loss: 0.27551 | val_0_rmse: 0.63284 | val_1_rmse: 0.63604 |  0:01:10s
epoch 35 | loss: 0.27555 | val_0_rmse: 0.49177 | val_1_rmse: 0.48228 |  0:01:12s
epoch 36 | loss: 0.26837 | val_0_rmse: 0.52002 | val_1_rmse: 0.51098 |  0:01:14s
epoch 37 | loss: 0.27667 | val_0_rmse: 0.53537 | val_1_rmse: 0.53137 |  0:01:16s
epoch 38 | loss: 0.2775  | val_0_rmse: 0.76831 | val_1_rmse: 0.77259 |  0:01:18s
epoch 39 | loss: 0.26516 | val_0_rmse: 0.52584 | val_1_rmse: 0.51208 |  0:01:20s
epoch 40 | loss: 0.25343 | val_0_rmse: 0.8715  | val_1_rmse: 0.87661 |  0:01:22s
epoch 41 | loss: 0.29733 | val_0_rmse: 0.64038 | val_1_rmse: 0.62729 |  0:01:24s
epoch 42 | loss: 0.27315 | val_0_rmse: 0.52373 | val_1_rmse: 0.51552 |  0:01:26s
epoch 43 | loss: 0.27354 | val_0_rmse: 0.51182 | val_1_rmse: 0.49866 |  0:01:28s
epoch 44 | loss: 0.26108 | val_0_rmse: 0.80158 | val_1_rmse: 0.79475 |  0:01:30s
epoch 45 | loss: 0.25395 | val_0_rmse: 0.59368 | val_1_rmse: 0.60565 |  0:01:32s
epoch 46 | loss: 0.25441 | val_0_rmse: 0.85979 | val_1_rmse: 0.86571 |  0:01:34s
epoch 47 | loss: 0.27092 | val_0_rmse: 0.68957 | val_1_rmse: 0.69821 |  0:01:36s
epoch 48 | loss: 0.26038 | val_0_rmse: 0.52888 | val_1_rmse: 0.52254 |  0:01:38s
epoch 49 | loss: 0.24671 | val_0_rmse: 0.55422 | val_1_rmse: 0.54628 |  0:01:40s
epoch 50 | loss: 0.26623 | val_0_rmse: 0.57078 | val_1_rmse: 0.55516 |  0:01:42s
epoch 51 | loss: 0.25328 | val_0_rmse: 0.50308 | val_1_rmse: 0.50402 |  0:01:44s
epoch 52 | loss: 0.23879 | val_0_rmse: 0.47173 | val_1_rmse: 0.46402 |  0:01:46s
epoch 53 | loss: 0.23757 | val_0_rmse: 0.66084 | val_1_rmse: 0.66547 |  0:01:48s
epoch 54 | loss: 0.23806 | val_0_rmse: 0.47074 | val_1_rmse: 0.46444 |  0:01:50s
epoch 55 | loss: 0.23768 | val_0_rmse: 0.55065 | val_1_rmse: 0.53679 |  0:01:52s
epoch 56 | loss: 0.25522 | val_0_rmse: 0.49608 | val_1_rmse: 0.48533 |  0:01:54s
epoch 57 | loss: 0.2507  | val_0_rmse: 0.58451 | val_1_rmse: 0.59053 |  0:01:56s
epoch 58 | loss: 0.22996 | val_0_rmse: 0.5598  | val_1_rmse: 0.52048 |  0:01:58s
epoch 59 | loss: 0.22524 | val_0_rmse: 0.49949 | val_1_rmse: 0.48849 |  0:02:00s
epoch 60 | loss: 0.23849 | val_0_rmse: 0.655   | val_1_rmse: 0.67205 |  0:02:02s
epoch 61 | loss: 0.23119 | val_0_rmse: 0.46218 | val_1_rmse: 0.45437 |  0:02:04s
epoch 62 | loss: 0.23505 | val_0_rmse: 0.54131 | val_1_rmse: 0.52799 |  0:02:06s
epoch 63 | loss: 0.23186 | val_0_rmse: 0.47749 | val_1_rmse: 0.46725 |  0:02:08s
epoch 64 | loss: 0.23035 | val_0_rmse: 0.45056 | val_1_rmse: 0.44565 |  0:02:10s
epoch 65 | loss: 0.23046 | val_0_rmse: 0.50932 | val_1_rmse: 0.50263 |  0:02:12s
epoch 66 | loss: 0.2313  | val_0_rmse: 0.57633 | val_1_rmse: 0.58498 |  0:02:14s
epoch 67 | loss: 0.22779 | val_0_rmse: 0.54662 | val_1_rmse: 0.53815 |  0:02:16s
epoch 68 | loss: 0.2252  | val_0_rmse: 0.45029 | val_1_rmse: 0.44359 |  0:02:18s
epoch 69 | loss: 0.23029 | val_0_rmse: 0.60122 | val_1_rmse: 0.61036 |  0:02:20s
epoch 70 | loss: 0.2274  | val_0_rmse: 0.46753 | val_1_rmse: 0.45904 |  0:02:22s
epoch 71 | loss: 0.23575 | val_0_rmse: 0.47833 | val_1_rmse: 0.47884 |  0:02:24s
epoch 72 | loss: 0.22946 | val_0_rmse: 0.54579 | val_1_rmse: 0.53594 |  0:02:26s
epoch 73 | loss: 0.22971 | val_0_rmse: 0.50304 | val_1_rmse: 0.50732 |  0:02:28s
epoch 74 | loss: 0.22699 | val_0_rmse: 0.82096 | val_1_rmse: 0.83258 |  0:02:30s
epoch 75 | loss: 0.22603 | val_0_rmse: 0.49001 | val_1_rmse: 0.48191 |  0:02:32s
epoch 76 | loss: 0.24724 | val_0_rmse: 0.4838  | val_1_rmse: 0.48608 |  0:02:34s
epoch 77 | loss: 0.22992 | val_0_rmse: 0.44495 | val_1_rmse: 0.43769 |  0:02:36s
epoch 78 | loss: 0.21701 | val_0_rmse: 0.43801 | val_1_rmse: 0.43977 |  0:02:38s
epoch 79 | loss: 0.21535 | val_0_rmse: 0.51334 | val_1_rmse: 0.52261 |  0:02:40s
epoch 80 | loss: 0.21646 | val_0_rmse: 0.43957 | val_1_rmse: 0.43675 |  0:02:42s
epoch 81 | loss: 0.21651 | val_0_rmse: 0.45884 | val_1_rmse: 0.46144 |  0:02:44s
epoch 82 | loss: 0.2127  | val_0_rmse: 0.4429  | val_1_rmse: 0.44258 |  0:02:46s
epoch 83 | loss: 0.21419 | val_0_rmse: 0.47691 | val_1_rmse: 0.47443 |  0:02:48s
epoch 84 | loss: 0.2172  | val_0_rmse: 0.48412 | val_1_rmse: 0.4729  |  0:02:50s
epoch 85 | loss: 0.20808 | val_0_rmse: 0.5194  | val_1_rmse: 0.52526 |  0:02:52s
epoch 86 | loss: 0.22288 | val_0_rmse: 0.46209 | val_1_rmse: 0.46144 |  0:02:54s
epoch 87 | loss: 0.23269 | val_0_rmse: 0.49006 | val_1_rmse: 0.48805 |  0:02:56s
epoch 88 | loss: 0.22874 | val_0_rmse: 0.94005 | val_1_rmse: 0.95132 |  0:02:58s
epoch 89 | loss: 0.26993 | val_0_rmse: 0.95613 | val_1_rmse: 0.95586 |  0:03:00s
epoch 90 | loss: 0.24983 | val_0_rmse: 0.63454 | val_1_rmse: 0.64388 |  0:03:02s
epoch 91 | loss: 0.23729 | val_0_rmse: 0.51886 | val_1_rmse: 0.51803 |  0:03:04s
epoch 92 | loss: 0.24448 | val_0_rmse: 0.5274  | val_1_rmse: 0.53    |  0:03:06s
epoch 93 | loss: 0.23457 | val_0_rmse: 0.45692 | val_1_rmse: 0.44756 |  0:03:08s
epoch 94 | loss: 0.22707 | val_0_rmse: 0.50147 | val_1_rmse: 0.50556 |  0:03:10s
epoch 95 | loss: 0.22968 | val_0_rmse: 0.50154 | val_1_rmse: 0.48989 |  0:03:12s
epoch 96 | loss: 0.23614 | val_0_rmse: 0.50135 | val_1_rmse: 0.49939 |  0:03:14s
epoch 97 | loss: 0.22716 | val_0_rmse: 0.46871 | val_1_rmse: 0.46113 |  0:03:16s
epoch 98 | loss: 0.2279  | val_0_rmse: 0.53278 | val_1_rmse: 0.53603 |  0:03:18s
epoch 99 | loss: 0.22384 | val_0_rmse: 0.47149 | val_1_rmse: 0.4639  |  0:03:20s
epoch 100| loss: 0.21756 | val_0_rmse: 0.4708  | val_1_rmse: 0.46136 |  0:03:22s
epoch 101| loss: 0.22514 | val_0_rmse: 0.53477 | val_1_rmse: 0.53931 |  0:03:24s
epoch 102| loss: 0.22428 | val_0_rmse: 0.74402 | val_1_rmse: 0.7528  |  0:03:26s
epoch 103| loss: 0.2292  | val_0_rmse: 0.48111 | val_1_rmse: 0.46686 |  0:03:28s
epoch 104| loss: 0.22554 | val_0_rmse: 0.45437 | val_1_rmse: 0.44525 |  0:03:30s
epoch 105| loss: 0.21236 | val_0_rmse: 0.57335 | val_1_rmse: 0.58145 |  0:03:32s
epoch 106| loss: 0.21413 | val_0_rmse: 0.49275 | val_1_rmse: 0.46231 |  0:03:34s
epoch 107| loss: 0.20715 | val_0_rmse: 0.58552 | val_1_rmse: 0.5712  |  0:03:36s
epoch 108| loss: 0.22209 | val_0_rmse: 0.4658  | val_1_rmse: 0.45991 |  0:03:38s
epoch 109| loss: 0.2173  | val_0_rmse: 0.44846 | val_1_rmse: 0.43856 |  0:03:40s
epoch 110| loss: 0.20819 | val_0_rmse: 0.45101 | val_1_rmse: 0.44506 |  0:03:42s

Early stopping occured at epoch 110 with best_epoch = 80 and best_val_1_rmse = 0.43675
Best weights from best epoch are automatically used!
ended training at: 16:24:01
Feature importance:
[('Area', 0.3761047214951565), ('Baths', 0.1676202558980649), ('Beds', 0.06684076686338054), ('Latitude', 0.04315713023246344), ('Longitude', 0.04713939617055987), ('Month', 0.08864382788319355), ('Year', 0.2104939014571812)]
Mean squared error is of 1422308968.4335134
Mean absolute error:26593.802510746893
MAPE:0.235579268817112
R2 score:0.8042590945651845
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:24:02
epoch 0  | loss: 0.57583 | val_0_rmse: 0.66824 | val_1_rmse: 0.67586 |  0:00:02s
epoch 1  | loss: 0.42318 | val_0_rmse: 0.62253 | val_1_rmse: 0.62987 |  0:00:04s
epoch 2  | loss: 0.40624 | val_0_rmse: 0.61538 | val_1_rmse: 0.62449 |  0:00:06s
epoch 3  | loss: 0.39744 | val_0_rmse: 0.60843 | val_1_rmse: 0.61626 |  0:00:07s
epoch 4  | loss: 0.38995 | val_0_rmse: 0.62324 | val_1_rmse: 0.62921 |  0:00:09s
epoch 5  | loss: 0.39277 | val_0_rmse: 0.60859 | val_1_rmse: 0.61547 |  0:00:12s
epoch 6  | loss: 0.38319 | val_0_rmse: 0.60536 | val_1_rmse: 0.61499 |  0:00:14s
epoch 7  | loss: 0.37674 | val_0_rmse: 0.59882 | val_1_rmse: 0.6058  |  0:00:16s
epoch 8  | loss: 0.36508 | val_0_rmse: 0.59363 | val_1_rmse: 0.60254 |  0:00:18s
epoch 9  | loss: 0.35668 | val_0_rmse: 0.59088 | val_1_rmse: 0.6     |  0:00:20s
epoch 10 | loss: 0.36151 | val_0_rmse: 0.59824 | val_1_rmse: 0.60453 |  0:00:22s
epoch 11 | loss: 0.36371 | val_0_rmse: 0.59286 | val_1_rmse: 0.60373 |  0:00:24s
epoch 12 | loss: 0.3627  | val_0_rmse: 0.58577 | val_1_rmse: 0.5993  |  0:00:26s
epoch 13 | loss: 0.35994 | val_0_rmse: 0.58727 | val_1_rmse: 0.59614 |  0:00:28s
epoch 14 | loss: 0.35321 | val_0_rmse: 0.59256 | val_1_rmse: 0.6044  |  0:00:30s
epoch 15 | loss: 0.36144 | val_0_rmse: 0.59985 | val_1_rmse: 0.60839 |  0:00:32s
epoch 16 | loss: 0.35906 | val_0_rmse: 0.59177 | val_1_rmse: 0.60207 |  0:00:34s
epoch 17 | loss: 0.34934 | val_0_rmse: 0.58038 | val_1_rmse: 0.59348 |  0:00:36s
epoch 18 | loss: 0.3511  | val_0_rmse: 0.58749 | val_1_rmse: 0.60314 |  0:00:38s
epoch 19 | loss: 0.34471 | val_0_rmse: 0.57643 | val_1_rmse: 0.58643 |  0:00:40s
epoch 20 | loss: 0.33706 | val_0_rmse: 0.56293 | val_1_rmse: 0.57242 |  0:00:42s
epoch 21 | loss: 0.33611 | val_0_rmse: 0.56582 | val_1_rmse: 0.57538 |  0:00:44s
epoch 22 | loss: 0.33511 | val_0_rmse: 0.57137 | val_1_rmse: 0.58148 |  0:00:46s
epoch 23 | loss: 0.33026 | val_0_rmse: 0.55971 | val_1_rmse: 0.57119 |  0:00:48s
epoch 24 | loss: 0.33235 | val_0_rmse: 0.56022 | val_1_rmse: 0.56921 |  0:00:50s
epoch 25 | loss: 0.33005 | val_0_rmse: 0.55681 | val_1_rmse: 0.56559 |  0:00:52s
epoch 26 | loss: 0.32257 | val_0_rmse: 0.55532 | val_1_rmse: 0.56481 |  0:00:54s
epoch 27 | loss: 0.31827 | val_0_rmse: 0.56054 | val_1_rmse: 0.57049 |  0:00:56s
epoch 28 | loss: 0.32749 | val_0_rmse: 0.55679 | val_1_rmse: 0.56785 |  0:00:58s
epoch 29 | loss: 0.33535 | val_0_rmse: 0.56787 | val_1_rmse: 0.57789 |  0:01:00s
epoch 30 | loss: 0.33421 | val_0_rmse: 0.55938 | val_1_rmse: 0.57326 |  0:01:02s
epoch 31 | loss: 0.32952 | val_0_rmse: 0.55397 | val_1_rmse: 0.56319 |  0:01:04s
epoch 32 | loss: 0.32218 | val_0_rmse: 0.60184 | val_1_rmse: 0.57572 |  0:01:07s
epoch 33 | loss: 0.3233  | val_0_rmse: 0.56328 | val_1_rmse: 0.57454 |  0:01:09s
epoch 34 | loss: 0.32262 | val_0_rmse: 0.57519 | val_1_rmse: 0.56638 |  0:01:11s
epoch 35 | loss: 0.32507 | val_0_rmse: 0.55411 | val_1_rmse: 0.56566 |  0:01:13s
epoch 36 | loss: 0.31946 | val_0_rmse: 0.5515  | val_1_rmse: 0.56076 |  0:01:15s
epoch 37 | loss: 0.31499 | val_0_rmse: 0.55758 | val_1_rmse: 0.56728 |  0:01:17s
epoch 38 | loss: 0.31594 | val_0_rmse: 0.55754 | val_1_rmse: 0.57246 |  0:01:19s
epoch 39 | loss: 0.32202 | val_0_rmse: 0.5437  | val_1_rmse: 0.55357 |  0:01:21s
epoch 40 | loss: 0.31534 | val_0_rmse: 0.55175 | val_1_rmse: 0.56454 |  0:01:23s
epoch 41 | loss: 0.31515 | val_0_rmse: 0.55245 | val_1_rmse: 0.56238 |  0:01:25s
epoch 42 | loss: 0.31196 | val_0_rmse: 0.53693 | val_1_rmse: 0.5465  |  0:01:27s
epoch 43 | loss: 0.30923 | val_0_rmse: 0.5609  | val_1_rmse: 0.57215 |  0:01:29s
epoch 44 | loss: 0.31419 | val_0_rmse: 0.54827 | val_1_rmse: 0.56075 |  0:01:31s
epoch 45 | loss: 0.30904 | val_0_rmse: 0.55173 | val_1_rmse: 0.56323 |  0:01:33s
epoch 46 | loss: 0.32241 | val_0_rmse: 0.56713 | val_1_rmse: 0.57585 |  0:01:35s
epoch 47 | loss: 0.33388 | val_0_rmse: 0.58755 | val_1_rmse: 0.59948 |  0:01:37s
epoch 48 | loss: 0.35261 | val_0_rmse: 0.57915 | val_1_rmse: 0.59129 |  0:01:39s
epoch 49 | loss: 0.35249 | val_0_rmse: 0.5815  | val_1_rmse: 0.5943  |  0:01:41s
epoch 50 | loss: 0.33707 | val_0_rmse: 0.5787  | val_1_rmse: 0.58711 |  0:01:43s
epoch 51 | loss: 0.33463 | val_0_rmse: 0.57738 | val_1_rmse: 0.58844 |  0:01:45s
epoch 52 | loss: 0.32841 | val_0_rmse: 0.59581 | val_1_rmse: 0.61143 |  0:01:47s
epoch 53 | loss: 0.33123 | val_0_rmse: 0.5534  | val_1_rmse: 0.56228 |  0:01:49s
epoch 54 | loss: 0.33966 | val_0_rmse: 0.57583 | val_1_rmse: 0.59008 |  0:01:51s
epoch 55 | loss: 0.32966 | val_0_rmse: 0.57172 | val_1_rmse: 0.58562 |  0:01:53s
epoch 56 | loss: 0.32786 | val_0_rmse: 0.55941 | val_1_rmse: 0.57753 |  0:01:55s
epoch 57 | loss: 0.32627 | val_0_rmse: 0.57269 | val_1_rmse: 0.58643 |  0:01:58s
epoch 58 | loss: 0.32416 | val_0_rmse: 0.56265 | val_1_rmse: 0.5752  |  0:02:00s
epoch 59 | loss: 0.31539 | val_0_rmse: 0.55637 | val_1_rmse: 0.57063 |  0:02:02s
epoch 60 | loss: 0.31215 | val_0_rmse: 0.54185 | val_1_rmse: 0.55419 |  0:02:04s
epoch 61 | loss: 0.31369 | val_0_rmse: 0.54446 | val_1_rmse: 0.5566  |  0:02:06s
epoch 62 | loss: 0.31045 | val_0_rmse: 0.53637 | val_1_rmse: 0.54688 |  0:02:08s
epoch 63 | loss: 0.30995 | val_0_rmse: 0.54203 | val_1_rmse: 0.55721 |  0:02:10s
epoch 64 | loss: 0.30551 | val_0_rmse: 0.54757 | val_1_rmse: 0.55884 |  0:02:12s
epoch 65 | loss: 0.30273 | val_0_rmse: 0.54406 | val_1_rmse: 0.55798 |  0:02:14s
epoch 66 | loss: 0.30688 | val_0_rmse: 0.54825 | val_1_rmse: 0.55713 |  0:02:16s
epoch 67 | loss: 0.3081  | val_0_rmse: 0.53251 | val_1_rmse: 0.54398 |  0:02:18s
epoch 68 | loss: 0.30047 | val_0_rmse: 0.53909 | val_1_rmse: 0.55259 |  0:02:20s
epoch 69 | loss: 0.30084 | val_0_rmse: 0.53813 | val_1_rmse: 0.54685 |  0:02:22s
epoch 70 | loss: 0.29687 | val_0_rmse: 0.53925 | val_1_rmse: 0.55416 |  0:02:24s
epoch 71 | loss: 0.30234 | val_0_rmse: 0.60465 | val_1_rmse: 0.61179 |  0:02:26s
epoch 72 | loss: 0.34285 | val_0_rmse: 0.56382 | val_1_rmse: 0.57165 |  0:02:28s
epoch 73 | loss: 0.3169  | val_0_rmse: 0.56045 | val_1_rmse: 0.5694  |  0:02:30s
epoch 74 | loss: 0.30924 | val_0_rmse: 0.53068 | val_1_rmse: 0.54091 |  0:02:32s
epoch 75 | loss: 0.29592 | val_0_rmse: 0.53646 | val_1_rmse: 0.54372 |  0:02:34s
epoch 76 | loss: 0.29694 | val_0_rmse: 0.52733 | val_1_rmse: 0.53684 |  0:02:36s
epoch 77 | loss: 0.2905  | val_0_rmse: 0.53227 | val_1_rmse: 0.54009 |  0:02:38s
epoch 78 | loss: 0.29894 | val_0_rmse: 0.56732 | val_1_rmse: 0.57089 |  0:02:40s
epoch 79 | loss: 0.30407 | val_0_rmse: 0.5655  | val_1_rmse: 0.57216 |  0:02:42s
epoch 80 | loss: 0.2994  | val_0_rmse: 0.52839 | val_1_rmse: 0.53753 |  0:02:44s
epoch 81 | loss: 0.29384 | val_0_rmse: 0.5463  | val_1_rmse: 0.55053 |  0:02:46s
epoch 82 | loss: 0.28462 | val_0_rmse: 0.55204 | val_1_rmse: 0.55972 |  0:02:48s
epoch 83 | loss: 0.28256 | val_0_rmse: 0.52381 | val_1_rmse: 0.53275 |  0:02:50s
epoch 84 | loss: 0.27482 | val_0_rmse: 0.519   | val_1_rmse: 0.53143 |  0:02:52s
epoch 85 | loss: 0.2787  | val_0_rmse: 0.53269 | val_1_rmse: 0.54021 |  0:02:54s
epoch 86 | loss: 0.29334 | val_0_rmse: 0.5701  | val_1_rmse: 0.5757  |  0:02:56s
epoch 87 | loss: 0.31029 | val_0_rmse: 0.53857 | val_1_rmse: 0.54556 |  0:02:58s
epoch 88 | loss: 0.28831 | val_0_rmse: 0.52189 | val_1_rmse: 0.52972 |  0:03:00s
epoch 89 | loss: 0.28667 | val_0_rmse: 0.50762 | val_1_rmse: 0.5206  |  0:03:02s
epoch 90 | loss: 0.27946 | val_0_rmse: 0.5362  | val_1_rmse: 0.55081 |  0:03:04s
epoch 91 | loss: 0.27898 | val_0_rmse: 0.49344 | val_1_rmse: 0.50273 |  0:03:06s
epoch 92 | loss: 0.28492 | val_0_rmse: 0.54552 | val_1_rmse: 0.5554  |  0:03:08s
epoch 93 | loss: 0.29132 | val_0_rmse: 0.53475 | val_1_rmse: 0.54253 |  0:03:11s
epoch 94 | loss: 0.26964 | val_0_rmse: 0.53093 | val_1_rmse: 0.54186 |  0:03:12s
epoch 95 | loss: 0.27131 | val_0_rmse: 0.50488 | val_1_rmse: 0.51695 |  0:03:14s
epoch 96 | loss: 0.2697  | val_0_rmse: 0.49674 | val_1_rmse: 0.50472 |  0:03:17s
epoch 97 | loss: 0.27295 | val_0_rmse: 0.54108 | val_1_rmse: 0.55041 |  0:03:19s
epoch 98 | loss: 0.26639 | val_0_rmse: 0.50552 | val_1_rmse: 0.51203 |  0:03:21s
epoch 99 | loss: 0.26838 | val_0_rmse: 0.56018 | val_1_rmse: 0.56516 |  0:03:23s
epoch 100| loss: 0.27005 | val_0_rmse: 0.49777 | val_1_rmse: 0.50497 |  0:03:25s
epoch 101| loss: 0.26212 | val_0_rmse: 0.49218 | val_1_rmse: 0.49895 |  0:03:27s
epoch 102| loss: 0.25988 | val_0_rmse: 0.49578 | val_1_rmse: 0.50578 |  0:03:29s
epoch 103| loss: 0.25821 | val_0_rmse: 0.51474 | val_1_rmse: 0.52033 |  0:03:31s
epoch 104| loss: 0.24531 | val_0_rmse: 0.49275 | val_1_rmse: 0.5007  |  0:03:33s
epoch 105| loss: 0.25256 | val_0_rmse: 0.51066 | val_1_rmse: 0.51998 |  0:03:35s
epoch 106| loss: 0.26035 | val_0_rmse: 0.4927  | val_1_rmse: 0.50232 |  0:03:37s
epoch 107| loss: 0.26446 | val_0_rmse: 0.49478 | val_1_rmse: 0.50289 |  0:03:39s
epoch 108| loss: 0.26334 | val_0_rmse: 0.5199  | val_1_rmse: 0.53015 |  0:03:41s
epoch 109| loss: 0.25068 | val_0_rmse: 0.52584 | val_1_rmse: 0.5354  |  0:03:43s
epoch 110| loss: 0.24833 | val_0_rmse: 0.48261 | val_1_rmse: 0.49104 |  0:03:45s
epoch 111| loss: 0.26477 | val_0_rmse: 0.54105 | val_1_rmse: 0.54419 |  0:03:47s
epoch 112| loss: 0.26398 | val_0_rmse: 0.67115 | val_1_rmse: 0.68502 |  0:03:49s
epoch 113| loss: 0.25958 | val_0_rmse: 0.56911 | val_1_rmse: 0.57913 |  0:03:51s
epoch 114| loss: 0.25899 | val_0_rmse: 0.50417 | val_1_rmse: 0.5157  |  0:03:53s
epoch 115| loss: 0.25158 | val_0_rmse: 0.50176 | val_1_rmse: 0.50833 |  0:03:55s
epoch 116| loss: 0.24447 | val_0_rmse: 0.45968 | val_1_rmse: 0.4715  |  0:03:57s
epoch 117| loss: 0.24223 | val_0_rmse: 0.48451 | val_1_rmse: 0.49444 |  0:03:59s
epoch 118| loss: 0.23391 | val_0_rmse: 0.50642 | val_1_rmse: 0.51428 |  0:04:01s
epoch 119| loss: 0.24112 | val_0_rmse: 0.48559 | val_1_rmse: 0.49231 |  0:04:03s
epoch 120| loss: 0.24389 | val_0_rmse: 0.49924 | val_1_rmse: 0.51344 |  0:04:05s
epoch 121| loss: 0.23443 | val_0_rmse: 0.54253 | val_1_rmse: 0.55414 |  0:04:07s
epoch 122| loss: 0.22824 | val_0_rmse: 0.61423 | val_1_rmse: 0.63452 |  0:04:09s
epoch 123| loss: 0.24931 | val_0_rmse: 0.46706 | val_1_rmse: 0.47389 |  0:04:11s
epoch 124| loss: 0.23456 | val_0_rmse: 0.497   | val_1_rmse: 0.50678 |  0:04:13s
epoch 125| loss: 0.24426 | val_0_rmse: 0.49216 | val_1_rmse: 0.50358 |  0:04:15s
epoch 126| loss: 0.23313 | val_0_rmse: 0.45699 | val_1_rmse: 0.46811 |  0:04:17s
epoch 127| loss: 0.22616 | val_0_rmse: 0.4784  | val_1_rmse: 0.49309 |  0:04:19s
epoch 128| loss: 0.22431 | val_0_rmse: 0.463   | val_1_rmse: 0.46972 |  0:04:21s
epoch 129| loss: 0.22134 | val_0_rmse: 0.44639 | val_1_rmse: 0.45648 |  0:04:23s
epoch 130| loss: 0.22138 | val_0_rmse: 0.48694 | val_1_rmse: 0.49724 |  0:04:25s
epoch 131| loss: 0.2323  | val_0_rmse: 0.48492 | val_1_rmse: 0.49858 |  0:04:27s
epoch 132| loss: 0.24024 | val_0_rmse: 0.55853 | val_1_rmse: 0.56358 |  0:04:29s
epoch 133| loss: 0.23229 | val_0_rmse: 0.49651 | val_1_rmse: 0.50113 |  0:04:31s
epoch 134| loss: 0.21922 | val_0_rmse: 0.51968 | val_1_rmse: 0.52805 |  0:04:34s
epoch 135| loss: 0.22207 | val_0_rmse: 0.43683 | val_1_rmse: 0.44505 |  0:04:36s
epoch 136| loss: 0.21283 | val_0_rmse: 0.46401 | val_1_rmse: 0.47182 |  0:04:38s
epoch 137| loss: 0.21652 | val_0_rmse: 0.5148  | val_1_rmse: 0.52216 |  0:04:40s
epoch 138| loss: 0.22136 | val_0_rmse: 0.52623 | val_1_rmse: 0.53929 |  0:04:42s
epoch 139| loss: 0.20489 | val_0_rmse: 0.51936 | val_1_rmse: 0.52113 |  0:04:44s
epoch 140| loss: 0.22003 | val_0_rmse: 0.50868 | val_1_rmse: 0.51496 |  0:04:46s
epoch 141| loss: 0.20976 | val_0_rmse: 0.45108 | val_1_rmse: 0.45745 |  0:04:48s
epoch 142| loss: 0.23049 | val_0_rmse: 0.65719 | val_1_rmse: 0.66231 |  0:04:50s
epoch 143| loss: 0.21331 | val_0_rmse: 0.42801 | val_1_rmse: 0.44392 |  0:04:52s
epoch 144| loss: 0.21057 | val_0_rmse: 0.42785 | val_1_rmse: 0.4364  |  0:04:54s
epoch 145| loss: 0.22667 | val_0_rmse: 0.5833  | val_1_rmse: 0.59146 |  0:04:56s
epoch 146| loss: 0.22367 | val_0_rmse: 0.44198 | val_1_rmse: 0.45036 |  0:04:58s
epoch 147| loss: 0.20792 | val_0_rmse: 0.45593 | val_1_rmse: 0.46979 |  0:05:00s
epoch 148| loss: 0.21519 | val_0_rmse: 0.63644 | val_1_rmse: 0.64072 |  0:05:02s
epoch 149| loss: 0.2232  | val_0_rmse: 0.50155 | val_1_rmse: 0.50455 |  0:05:04s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.4364
Best weights from best epoch are automatically used!
ended training at: 16:29:07
Feature importance:
[('Area', 0.36598339809425334), ('Baths', 0.19652739746516934), ('Beds', 0.058562862893512785), ('Latitude', 0.14943587512311177), ('Longitude', 0.1401878970040587), ('Month', 0.02804692100842839), ('Year', 0.06125564841146567)]
Mean squared error is of 1432446519.4584177
Mean absolute error:26181.25505359521
MAPE:0.23495259093771742
R2 score:0.8053043396758047
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:30:47
epoch 0  | loss: 0.65764 | val_0_rmse: 0.71746 | val_1_rmse: 0.71814 |  0:00:01s
epoch 1  | loss: 0.48833 | val_0_rmse: 0.69377 | val_1_rmse: 0.69283 |  0:00:03s
epoch 2  | loss: 0.48214 | val_0_rmse: 0.68273 | val_1_rmse: 0.68081 |  0:00:05s
epoch 3  | loss: 0.47646 | val_0_rmse: 0.69188 | val_1_rmse: 0.69101 |  0:00:08s
epoch 4  | loss: 0.48057 | val_0_rmse: 0.68138 | val_1_rmse: 0.68116 |  0:00:10s
epoch 5  | loss: 0.47714 | val_0_rmse: 0.6842  | val_1_rmse: 0.68443 |  0:00:12s
epoch 6  | loss: 0.47439 | val_0_rmse: 0.68788 | val_1_rmse: 0.68953 |  0:00:14s
epoch 7  | loss: 0.47736 | val_0_rmse: 0.68744 | val_1_rmse: 0.68781 |  0:00:16s
epoch 8  | loss: 0.47338 | val_0_rmse: 0.67999 | val_1_rmse: 0.68131 |  0:00:18s
epoch 9  | loss: 0.46946 | val_0_rmse: 0.68875 | val_1_rmse: 0.69295 |  0:00:20s
epoch 10 | loss: 0.46856 | val_0_rmse: 0.67424 | val_1_rmse: 0.677   |  0:00:22s
epoch 11 | loss: 0.46361 | val_0_rmse: 0.68209 | val_1_rmse: 0.68359 |  0:00:24s
epoch 12 | loss: 0.477   | val_0_rmse: 0.68704 | val_1_rmse: 0.68587 |  0:00:26s
epoch 13 | loss: 0.47197 | val_0_rmse: 0.67935 | val_1_rmse: 0.67814 |  0:00:28s
epoch 14 | loss: 0.46863 | val_0_rmse: 0.68024 | val_1_rmse: 0.67778 |  0:00:30s
epoch 15 | loss: 0.46771 | val_0_rmse: 0.67791 | val_1_rmse: 0.67798 |  0:00:32s
epoch 16 | loss: 0.46425 | val_0_rmse: 0.67865 | val_1_rmse: 0.67945 |  0:00:34s
epoch 17 | loss: 0.46601 | val_0_rmse: 0.6774  | val_1_rmse: 0.6777  |  0:00:36s
epoch 18 | loss: 0.46787 | val_0_rmse: 0.68028 | val_1_rmse: 0.67847 |  0:00:38s
epoch 19 | loss: 0.47218 | val_0_rmse: 0.6807  | val_1_rmse: 0.67781 |  0:00:40s
epoch 20 | loss: 0.46491 | val_0_rmse: 0.67679 | val_1_rmse: 0.67541 |  0:00:42s
epoch 21 | loss: 0.46518 | val_0_rmse: 0.67754 | val_1_rmse: 0.67817 |  0:00:44s
epoch 22 | loss: 0.46269 | val_0_rmse: 0.67791 | val_1_rmse: 0.67742 |  0:00:46s
epoch 23 | loss: 0.45657 | val_0_rmse: 0.67227 | val_1_rmse: 0.67434 |  0:00:48s
epoch 24 | loss: 0.4582  | val_0_rmse: 0.67222 | val_1_rmse: 0.67229 |  0:00:50s
epoch 25 | loss: 0.45388 | val_0_rmse: 0.66839 | val_1_rmse: 0.6712  |  0:00:52s
epoch 26 | loss: 0.45625 | val_0_rmse: 0.67567 | val_1_rmse: 0.67629 |  0:00:54s
epoch 27 | loss: 0.46591 | val_0_rmse: 0.68602 | val_1_rmse: 0.68626 |  0:00:56s
epoch 28 | loss: 0.46474 | val_0_rmse: 0.67967 | val_1_rmse: 0.67892 |  0:00:58s
epoch 29 | loss: 0.4566  | val_0_rmse: 0.67638 | val_1_rmse: 0.67562 |  0:01:00s
epoch 30 | loss: 0.45384 | val_0_rmse: 0.72759 | val_1_rmse: 0.73117 |  0:01:02s
epoch 31 | loss: 0.44856 | val_0_rmse: 0.68192 | val_1_rmse: 0.68514 |  0:01:04s
epoch 32 | loss: 0.45128 | val_0_rmse: 0.68921 | val_1_rmse: 0.68932 |  0:01:06s
epoch 33 | loss: 0.45918 | val_0_rmse: 0.67088 | val_1_rmse: 0.6676  |  0:01:08s
epoch 34 | loss: 0.44948 | val_0_rmse: 0.67024 | val_1_rmse: 0.66942 |  0:01:10s
epoch 35 | loss: 0.44775 | val_0_rmse: 0.6705  | val_1_rmse: 0.67103 |  0:01:12s
epoch 36 | loss: 0.44654 | val_0_rmse: 0.74217 | val_1_rmse: 0.74501 |  0:01:14s
epoch 37 | loss: 0.44841 | val_0_rmse: 0.67068 | val_1_rmse: 0.67148 |  0:01:16s
epoch 38 | loss: 0.4444  | val_0_rmse: 0.65054 | val_1_rmse: 0.65318 |  0:01:18s
epoch 39 | loss: 0.43986 | val_0_rmse: 0.6431  | val_1_rmse: 0.64297 |  0:01:20s
epoch 40 | loss: 0.43555 | val_0_rmse: 0.70361 | val_1_rmse: 0.70069 |  0:01:22s
epoch 41 | loss: 0.44155 | val_0_rmse: 0.66047 | val_1_rmse: 0.66206 |  0:01:24s
epoch 42 | loss: 0.43245 | val_0_rmse: 0.64887 | val_1_rmse: 0.64632 |  0:01:26s
epoch 43 | loss: 0.4332  | val_0_rmse: 0.65778 | val_1_rmse: 0.6587  |  0:01:28s
epoch 44 | loss: 0.41789 | val_0_rmse: 0.67332 | val_1_rmse: 0.67101 |  0:01:30s
epoch 45 | loss: 0.41343 | val_0_rmse: 0.68267 | val_1_rmse: 0.687   |  0:01:32s
epoch 46 | loss: 0.40711 | val_0_rmse: 0.65104 | val_1_rmse: 0.65165 |  0:01:34s
epoch 47 | loss: 0.40975 | val_0_rmse: 0.63488 | val_1_rmse: 0.63499 |  0:01:36s
epoch 48 | loss: 0.40076 | val_0_rmse: 0.6578  | val_1_rmse: 0.65942 |  0:01:39s
epoch 49 | loss: 0.40393 | val_0_rmse: 0.6996  | val_1_rmse: 0.70019 |  0:01:41s
epoch 50 | loss: 0.40423 | val_0_rmse: 0.61301 | val_1_rmse: 0.61357 |  0:01:43s
epoch 51 | loss: 0.39918 | val_0_rmse: 0.62349 | val_1_rmse: 0.62477 |  0:01:45s
epoch 52 | loss: 0.39687 | val_0_rmse: 0.65665 | val_1_rmse: 0.66117 |  0:01:47s
epoch 53 | loss: 0.39335 | val_0_rmse: 0.6269  | val_1_rmse: 0.62554 |  0:01:49s
epoch 54 | loss: 0.39291 | val_0_rmse: 0.6114  | val_1_rmse: 0.61443 |  0:01:51s
epoch 55 | loss: 0.38605 | val_0_rmse: 0.65319 | val_1_rmse: 0.65674 |  0:01:53s
epoch 56 | loss: 0.38848 | val_0_rmse: 0.61158 | val_1_rmse: 0.60924 |  0:01:55s
epoch 57 | loss: 0.3735  | val_0_rmse: 0.62241 | val_1_rmse: 0.62182 |  0:01:57s
epoch 58 | loss: 0.37217 | val_0_rmse: 0.59439 | val_1_rmse: 0.59586 |  0:01:59s
epoch 59 | loss: 0.36143 | val_0_rmse: 0.61335 | val_1_rmse: 0.61542 |  0:02:01s
epoch 60 | loss: 0.364   | val_0_rmse: 0.63941 | val_1_rmse: 0.63562 |  0:02:03s
epoch 61 | loss: 0.36234 | val_0_rmse: 0.60095 | val_1_rmse: 0.59943 |  0:02:05s
epoch 62 | loss: 0.36105 | val_0_rmse: 0.5901  | val_1_rmse: 0.58958 |  0:02:07s
epoch 63 | loss: 0.35572 | val_0_rmse: 0.62823 | val_1_rmse: 0.63057 |  0:02:09s
epoch 64 | loss: 0.35437 | val_0_rmse: 0.62186 | val_1_rmse: 0.62123 |  0:02:11s
epoch 65 | loss: 0.35882 | val_0_rmse: 0.63348 | val_1_rmse: 0.63177 |  0:02:13s
epoch 66 | loss: 0.35954 | val_0_rmse: 0.59793 | val_1_rmse: 0.60094 |  0:02:15s
epoch 67 | loss: 0.41358 | val_0_rmse: 0.75964 | val_1_rmse: 0.75547 |  0:02:17s
epoch 68 | loss: 0.3823  | val_0_rmse: 0.65342 | val_1_rmse: 0.65008 |  0:02:19s
epoch 69 | loss: 0.36159 | val_0_rmse: 0.60884 | val_1_rmse: 0.61021 |  0:02:21s
epoch 70 | loss: 0.36261 | val_0_rmse: 0.63877 | val_1_rmse: 0.63776 |  0:02:23s
epoch 71 | loss: 0.36375 | val_0_rmse: 0.61145 | val_1_rmse: 0.61125 |  0:02:25s
epoch 72 | loss: 0.37596 | val_0_rmse: 0.62043 | val_1_rmse: 0.61903 |  0:02:27s
epoch 73 | loss: 0.35845 | val_0_rmse: 0.62728 | val_1_rmse: 0.62413 |  0:02:29s
epoch 74 | loss: 0.36821 | val_0_rmse: 0.6311  | val_1_rmse: 0.63403 |  0:02:31s
epoch 75 | loss: 0.36312 | val_0_rmse: 0.57906 | val_1_rmse: 0.57954 |  0:02:33s
epoch 76 | loss: 0.35026 | val_0_rmse: 0.60712 | val_1_rmse: 0.60996 |  0:02:35s
epoch 77 | loss: 0.34967 | val_0_rmse: 0.59385 | val_1_rmse: 0.592   |  0:02:37s
epoch 78 | loss: 0.34887 | val_0_rmse: 0.62935 | val_1_rmse: 0.62713 |  0:02:39s
epoch 79 | loss: 0.34667 | val_0_rmse: 0.67597 | val_1_rmse: 0.67906 |  0:02:41s
epoch 80 | loss: 0.34199 | val_0_rmse: 0.62117 | val_1_rmse: 0.61644 |  0:02:43s
epoch 81 | loss: 0.33949 | val_0_rmse: 0.61005 | val_1_rmse: 0.61174 |  0:02:45s
epoch 82 | loss: 0.34143 | val_0_rmse: 0.60032 | val_1_rmse: 0.59737 |  0:02:47s
epoch 83 | loss: 0.33887 | val_0_rmse: 0.57326 | val_1_rmse: 0.571   |  0:02:49s
epoch 84 | loss: 0.34334 | val_0_rmse: 0.61438 | val_1_rmse: 0.61473 |  0:02:51s
epoch 85 | loss: 0.35086 | val_0_rmse: 0.59051 | val_1_rmse: 0.59253 |  0:02:53s
epoch 86 | loss: 0.40184 | val_0_rmse: 0.63553 | val_1_rmse: 0.63981 |  0:02:55s
epoch 87 | loss: 0.3676  | val_0_rmse: 0.62002 | val_1_rmse: 0.6239  |  0:02:57s
epoch 88 | loss: 0.3469  | val_0_rmse: 0.60073 | val_1_rmse: 0.59646 |  0:02:59s
epoch 89 | loss: 0.36237 | val_0_rmse: 0.59454 | val_1_rmse: 0.59488 |  0:03:01s
epoch 90 | loss: 0.35034 | val_0_rmse: 0.57374 | val_1_rmse: 0.5711  |  0:03:04s
epoch 91 | loss: 0.34302 | val_0_rmse: 0.60966 | val_1_rmse: 0.60985 |  0:03:06s
epoch 92 | loss: 0.35381 | val_0_rmse: 0.60506 | val_1_rmse: 0.61151 |  0:03:08s
epoch 93 | loss: 0.34763 | val_0_rmse: 0.56934 | val_1_rmse: 0.5692  |  0:03:10s
epoch 94 | loss: 0.33902 | val_0_rmse: 0.57237 | val_1_rmse: 0.57206 |  0:03:12s
epoch 95 | loss: 0.34515 | val_0_rmse: 0.57156 | val_1_rmse: 0.57455 |  0:03:14s
epoch 96 | loss: 0.3412  | val_0_rmse: 0.62983 | val_1_rmse: 0.63104 |  0:03:16s
epoch 97 | loss: 0.33529 | val_0_rmse: 0.66377 | val_1_rmse: 0.66163 |  0:03:18s
epoch 98 | loss: 0.33729 | val_0_rmse: 0.59313 | val_1_rmse: 0.59321 |  0:03:20s
epoch 99 | loss: 0.33775 | val_0_rmse: 0.57168 | val_1_rmse: 0.57565 |  0:03:22s
epoch 100| loss: 0.34198 | val_0_rmse: 0.58225 | val_1_rmse: 0.58306 |  0:03:24s
epoch 101| loss: 0.33877 | val_0_rmse: 0.5715  | val_1_rmse: 0.57013 |  0:03:26s
epoch 102| loss: 0.33843 | val_0_rmse: 0.59385 | val_1_rmse: 0.59814 |  0:03:28s
epoch 103| loss: 0.33639 | val_0_rmse: 0.7418  | val_1_rmse: 0.74178 |  0:03:30s
epoch 104| loss: 0.37495 | val_0_rmse: 0.60252 | val_1_rmse: 0.59991 |  0:03:32s
epoch 105| loss: 0.36515 | val_0_rmse: 0.59799 | val_1_rmse: 0.5998  |  0:03:34s
epoch 106| loss: 0.3536  | val_0_rmse: 0.6175  | val_1_rmse: 0.62031 |  0:03:36s
epoch 107| loss: 0.34563 | val_0_rmse: 0.59723 | val_1_rmse: 0.59946 |  0:03:38s
epoch 108| loss: 0.33764 | val_0_rmse: 0.57813 | val_1_rmse: 0.57355 |  0:03:40s
epoch 109| loss: 0.34397 | val_0_rmse: 0.64212 | val_1_rmse: 0.6294  |  0:03:42s
epoch 110| loss: 0.33505 | val_0_rmse: 0.58355 | val_1_rmse: 0.58512 |  0:03:44s
epoch 111| loss: 0.34459 | val_0_rmse: 0.60226 | val_1_rmse: 0.60209 |  0:03:46s
epoch 112| loss: 0.32988 | val_0_rmse: 0.61624 | val_1_rmse: 0.61678 |  0:03:48s
epoch 113| loss: 0.3421  | val_0_rmse: 0.57574 | val_1_rmse: 0.57165 |  0:03:50s
epoch 114| loss: 0.33512 | val_0_rmse: 0.56528 | val_1_rmse: 0.5633  |  0:03:52s
epoch 115| loss: 0.33256 | val_0_rmse: 0.56539 | val_1_rmse: 0.56786 |  0:03:54s
epoch 116| loss: 0.3287  | val_0_rmse: 0.58593 | val_1_rmse: 0.59161 |  0:03:56s
epoch 117| loss: 0.3292  | val_0_rmse: 0.57108 | val_1_rmse: 0.57096 |  0:03:58s
epoch 118| loss: 0.32718 | val_0_rmse: 0.6203  | val_1_rmse: 0.62314 |  0:04:00s
epoch 119| loss: 0.33012 | val_0_rmse: 0.59688 | val_1_rmse: 0.60025 |  0:04:02s
epoch 120| loss: 0.3311  | val_0_rmse: 0.61374 | val_1_rmse: 0.62039 |  0:04:04s
epoch 121| loss: 0.32753 | val_0_rmse: 0.60396 | val_1_rmse: 0.60694 |  0:04:06s
epoch 122| loss: 0.32681 | val_0_rmse: 0.56854 | val_1_rmse: 0.57361 |  0:04:08s
epoch 123| loss: 0.33056 | val_0_rmse: 0.59114 | val_1_rmse: 0.5885  |  0:04:10s
epoch 124| loss: 0.32623 | val_0_rmse: 0.5753  | val_1_rmse: 0.57545 |  0:04:12s
epoch 125| loss: 0.32422 | val_0_rmse: 0.59845 | val_1_rmse: 0.60677 |  0:04:14s
epoch 126| loss: 0.323   | val_0_rmse: 0.58771 | val_1_rmse: 0.58783 |  0:04:16s
epoch 127| loss: 0.32904 | val_0_rmse: 0.6191  | val_1_rmse: 0.62317 |  0:04:18s
epoch 128| loss: 0.32415 | val_0_rmse: 0.56305 | val_1_rmse: 0.5643  |  0:04:20s
epoch 129| loss: 0.31824 | val_0_rmse: 0.62121 | val_1_rmse: 0.62414 |  0:04:22s
epoch 130| loss: 0.32547 | val_0_rmse: 0.57213 | val_1_rmse: 0.57521 |  0:04:24s
epoch 131| loss: 0.32211 | val_0_rmse: 0.58706 | val_1_rmse: 0.58625 |  0:04:26s
epoch 132| loss: 0.32117 | val_0_rmse: 0.59142 | val_1_rmse: 0.59342 |  0:04:28s
epoch 133| loss: 0.32174 | val_0_rmse: 0.5805  | val_1_rmse: 0.58086 |  0:04:30s
epoch 134| loss: 0.32372 | val_0_rmse: 0.59359 | val_1_rmse: 0.59385 |  0:04:32s
epoch 135| loss: 0.32346 | val_0_rmse: 0.6033  | val_1_rmse: 0.60697 |  0:04:34s
epoch 136| loss: 0.31947 | val_0_rmse: 0.58159 | val_1_rmse: 0.58139 |  0:04:36s
epoch 137| loss: 0.32004 | val_0_rmse: 0.56576 | val_1_rmse: 0.56808 |  0:04:38s
epoch 138| loss: 0.32374 | val_0_rmse: 0.5868  | val_1_rmse: 0.58963 |  0:04:40s
epoch 139| loss: 0.3194  | val_0_rmse: 0.61895 | val_1_rmse: 0.6191  |  0:04:42s
epoch 140| loss: 0.32851 | val_0_rmse: 0.5744  | val_1_rmse: 0.57387 |  0:04:44s
epoch 141| loss: 0.31985 | val_0_rmse: 0.55577 | val_1_rmse: 0.55827 |  0:04:46s
epoch 142| loss: 0.31356 | val_0_rmse: 0.56205 | val_1_rmse: 0.56314 |  0:04:48s
epoch 143| loss: 0.32068 | val_0_rmse: 0.5611  | val_1_rmse: 0.55894 |  0:04:50s
epoch 144| loss: 0.31992 | val_0_rmse: 0.55413 | val_1_rmse: 0.55657 |  0:04:52s
epoch 145| loss: 0.31854 | val_0_rmse: 0.56845 | val_1_rmse: 0.56966 |  0:04:54s
epoch 146| loss: 0.31272 | val_0_rmse: 0.55819 | val_1_rmse: 0.55939 |  0:04:57s
epoch 147| loss: 0.31559 | val_0_rmse: 0.5558  | val_1_rmse: 0.55647 |  0:04:59s
epoch 148| loss: 0.31899 | val_0_rmse: 0.55499 | val_1_rmse: 0.56173 |  0:05:01s
epoch 149| loss: 0.31666 | val_0_rmse: 0.591   | val_1_rmse: 0.58974 |  0:05:03s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.55647
Best weights from best epoch are automatically used!
ended training at: 16:35:51
Feature importance:
[('Area', 0.4328700141031919), ('Baths', 0.28668647173934164), ('Beds', 0.1972168025868226), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.03896765002267023), ('Year', 0.044259061547973634)]
Mean squared error is of 2739290921.139269
Mean absolute error:36139.32580199587
MAPE:0.29931417199593424
R2 score:0.6721897963374919
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:35:52
epoch 0  | loss: 0.64495 | val_0_rmse: 0.71033 | val_1_rmse: 0.7292  |  0:00:01s
epoch 1  | loss: 0.49938 | val_0_rmse: 0.69908 | val_1_rmse: 0.71368 |  0:00:03s
epoch 2  | loss: 0.49142 | val_0_rmse: 0.68826 | val_1_rmse: 0.70433 |  0:00:06s
epoch 3  | loss: 0.47432 | val_0_rmse: 0.68476 | val_1_rmse: 0.70248 |  0:00:08s
epoch 4  | loss: 0.47598 | val_0_rmse: 0.68699 | val_1_rmse: 0.70584 |  0:00:10s
epoch 5  | loss: 0.47957 | val_0_rmse: 0.68355 | val_1_rmse: 0.69938 |  0:00:12s
epoch 6  | loss: 0.47676 | val_0_rmse: 0.68975 | val_1_rmse: 0.70512 |  0:00:14s
epoch 7  | loss: 0.47282 | val_0_rmse: 0.68385 | val_1_rmse: 0.69949 |  0:00:16s
epoch 8  | loss: 0.47485 | val_0_rmse: 0.68424 | val_1_rmse: 0.69991 |  0:00:18s
epoch 9  | loss: 0.47091 | val_0_rmse: 0.68536 | val_1_rmse: 0.6986  |  0:00:20s
epoch 10 | loss: 0.47141 | val_0_rmse: 0.6896  | val_1_rmse: 0.70889 |  0:00:22s
epoch 11 | loss: 0.47097 | val_0_rmse: 0.68182 | val_1_rmse: 0.69842 |  0:00:24s
epoch 12 | loss: 0.4675  | val_0_rmse: 0.68206 | val_1_rmse: 0.69758 |  0:00:26s
epoch 13 | loss: 0.46986 | val_0_rmse: 0.68161 | val_1_rmse: 0.69571 |  0:00:28s
epoch 14 | loss: 0.47038 | val_0_rmse: 0.68838 | val_1_rmse: 0.70538 |  0:00:30s
epoch 15 | loss: 0.4699  | val_0_rmse: 0.68459 | val_1_rmse: 0.70081 |  0:00:32s
epoch 16 | loss: 0.47027 | val_0_rmse: 0.6814  | val_1_rmse: 0.69772 |  0:00:34s
epoch 17 | loss: 0.47358 | val_0_rmse: 0.69266 | val_1_rmse: 0.70755 |  0:00:36s
epoch 18 | loss: 0.47198 | val_0_rmse: 0.68144 | val_1_rmse: 0.69806 |  0:00:38s
epoch 19 | loss: 0.47033 | val_0_rmse: 0.68079 | val_1_rmse: 0.69648 |  0:00:40s
epoch 20 | loss: 0.46986 | val_0_rmse: 0.68248 | val_1_rmse: 0.69933 |  0:00:42s
epoch 21 | loss: 0.46954 | val_0_rmse: 0.68368 | val_1_rmse: 0.69876 |  0:00:44s
epoch 22 | loss: 0.47011 | val_0_rmse: 0.68915 | val_1_rmse: 0.70304 |  0:00:46s
epoch 23 | loss: 0.48189 | val_0_rmse: 0.68381 | val_1_rmse: 0.70079 |  0:00:48s
epoch 24 | loss: 0.4721  | val_0_rmse: 0.6856  | val_1_rmse: 0.70011 |  0:00:50s
epoch 25 | loss: 0.46741 | val_0_rmse: 0.6795  | val_1_rmse: 0.69558 |  0:00:52s
epoch 26 | loss: 0.4655  | val_0_rmse: 0.67867 | val_1_rmse: 0.69512 |  0:00:54s
epoch 27 | loss: 0.46501 | val_0_rmse: 0.67905 | val_1_rmse: 0.69842 |  0:00:56s
epoch 28 | loss: 0.47074 | val_0_rmse: 0.68404 | val_1_rmse: 0.70004 |  0:00:58s
epoch 29 | loss: 0.46669 | val_0_rmse: 0.68714 | val_1_rmse: 0.70118 |  0:01:00s
epoch 30 | loss: 0.46693 | val_0_rmse: 0.6783  | val_1_rmse: 0.6948  |  0:01:02s
epoch 31 | loss: 0.46863 | val_0_rmse: 0.67737 | val_1_rmse: 0.69613 |  0:01:04s
epoch 32 | loss: 0.46674 | val_0_rmse: 0.67551 | val_1_rmse: 0.69098 |  0:01:06s
epoch 33 | loss: 0.46248 | val_0_rmse: 0.67688 | val_1_rmse: 0.69317 |  0:01:08s
epoch 34 | loss: 0.46125 | val_0_rmse: 0.67594 | val_1_rmse: 0.69344 |  0:01:10s
epoch 35 | loss: 0.46328 | val_0_rmse: 0.67557 | val_1_rmse: 0.69171 |  0:01:12s
epoch 36 | loss: 0.46276 | val_0_rmse: 0.67426 | val_1_rmse: 0.69128 |  0:01:14s
epoch 37 | loss: 0.46169 | val_0_rmse: 0.67945 | val_1_rmse: 0.69328 |  0:01:16s
epoch 38 | loss: 0.4632  | val_0_rmse: 0.68039 | val_1_rmse: 0.69601 |  0:01:18s
epoch 39 | loss: 0.4643  | val_0_rmse: 0.67638 | val_1_rmse: 0.69531 |  0:01:20s
epoch 40 | loss: 0.45927 | val_0_rmse: 0.67874 | val_1_rmse: 0.6964  |  0:01:22s
epoch 41 | loss: 0.46343 | val_0_rmse: 0.67419 | val_1_rmse: 0.69204 |  0:01:24s
epoch 42 | loss: 0.45998 | val_0_rmse: 0.67812 | val_1_rmse: 0.69354 |  0:01:26s
epoch 43 | loss: 0.46153 | val_0_rmse: 0.67685 | val_1_rmse: 0.69192 |  0:01:28s
epoch 44 | loss: 0.46012 | val_0_rmse: 0.67385 | val_1_rmse: 0.69043 |  0:01:30s
epoch 45 | loss: 0.45814 | val_0_rmse: 0.67395 | val_1_rmse: 0.68708 |  0:01:32s
epoch 46 | loss: 0.46109 | val_0_rmse: 0.67443 | val_1_rmse: 0.6903  |  0:01:34s
epoch 47 | loss: 0.46272 | val_0_rmse: 0.67413 | val_1_rmse: 0.69152 |  0:01:36s
epoch 48 | loss: 0.46117 | val_0_rmse: 0.67736 | val_1_rmse: 0.69397 |  0:01:38s
epoch 49 | loss: 0.46005 | val_0_rmse: 0.67938 | val_1_rmse: 0.69625 |  0:01:40s
epoch 50 | loss: 0.46051 | val_0_rmse: 0.67471 | val_1_rmse: 0.69468 |  0:01:42s
epoch 51 | loss: 0.45823 | val_0_rmse: 0.67326 | val_1_rmse: 0.6913  |  0:01:44s
epoch 52 | loss: 0.4582  | val_0_rmse: 0.67389 | val_1_rmse: 0.69237 |  0:01:46s
epoch 53 | loss: 0.4584  | val_0_rmse: 0.67082 | val_1_rmse: 0.6899  |  0:01:48s
epoch 54 | loss: 0.45659 | val_0_rmse: 0.67085 | val_1_rmse: 0.68698 |  0:01:50s
epoch 55 | loss: 0.45394 | val_0_rmse: 0.67743 | val_1_rmse: 0.69417 |  0:01:52s
epoch 56 | loss: 0.45603 | val_0_rmse: 0.67017 | val_1_rmse: 0.68471 |  0:01:54s
epoch 57 | loss: 0.45011 | val_0_rmse: 0.66379 | val_1_rmse: 0.68027 |  0:01:56s
epoch 58 | loss: 0.45151 | val_0_rmse: 0.67511 | val_1_rmse: 0.69369 |  0:01:58s
epoch 59 | loss: 0.44632 | val_0_rmse: 0.67405 | val_1_rmse: 0.68827 |  0:02:00s
epoch 60 | loss: 0.44676 | val_0_rmse: 0.66212 | val_1_rmse: 0.68035 |  0:02:02s
epoch 61 | loss: 0.44546 | val_0_rmse: 0.65723 | val_1_rmse: 0.67499 |  0:02:04s
epoch 62 | loss: 0.44056 | val_0_rmse: 0.70926 | val_1_rmse: 0.72456 |  0:02:06s
epoch 63 | loss: 0.44634 | val_0_rmse: 0.66143 | val_1_rmse: 0.67706 |  0:02:08s
epoch 64 | loss: 0.43922 | val_0_rmse: 0.66007 | val_1_rmse: 0.67389 |  0:02:10s
epoch 65 | loss: 0.43823 | val_0_rmse: 0.66786 | val_1_rmse: 0.67853 |  0:02:12s
epoch 66 | loss: 0.43762 | val_0_rmse: 0.65488 | val_1_rmse: 0.6682  |  0:02:14s
epoch 67 | loss: 0.44809 | val_0_rmse: 0.69313 | val_1_rmse: 0.70736 |  0:02:16s
epoch 68 | loss: 0.44236 | val_0_rmse: 0.66388 | val_1_rmse: 0.68232 |  0:02:18s
epoch 69 | loss: 0.44868 | val_0_rmse: 0.65266 | val_1_rmse: 0.67062 |  0:02:20s
epoch 70 | loss: 0.4378  | val_0_rmse: 0.64169 | val_1_rmse: 0.6626  |  0:02:23s
epoch 71 | loss: 0.40498 | val_0_rmse: 0.69183 | val_1_rmse: 0.69675 |  0:02:25s
epoch 72 | loss: 0.40426 | val_0_rmse: 0.65179 | val_1_rmse: 0.67336 |  0:02:26s
epoch 73 | loss: 0.38784 | val_0_rmse: 0.62389 | val_1_rmse: 0.64059 |  0:02:28s
epoch 74 | loss: 0.38861 | val_0_rmse: 0.59606 | val_1_rmse: 0.61042 |  0:02:30s
epoch 75 | loss: 0.38014 | val_0_rmse: 0.67051 | val_1_rmse: 0.68781 |  0:02:32s
epoch 76 | loss: 0.39307 | val_0_rmse: 0.64496 | val_1_rmse: 0.66505 |  0:02:34s
epoch 77 | loss: 0.37734 | val_0_rmse: 0.61086 | val_1_rmse: 0.62299 |  0:02:37s
epoch 78 | loss: 0.37167 | val_0_rmse: 0.58147 | val_1_rmse: 0.5978  |  0:02:39s
epoch 79 | loss: 0.36369 | val_0_rmse: 0.61628 | val_1_rmse: 0.63193 |  0:02:41s
epoch 80 | loss: 0.36041 | val_0_rmse: 0.6397  | val_1_rmse: 0.65506 |  0:02:43s
epoch 81 | loss: 0.3791  | val_0_rmse: 0.59721 | val_1_rmse: 0.61423 |  0:02:45s
epoch 82 | loss: 0.35981 | val_0_rmse: 0.62485 | val_1_rmse: 0.64585 |  0:02:47s
epoch 83 | loss: 0.35252 | val_0_rmse: 0.61316 | val_1_rmse: 0.62563 |  0:02:49s
epoch 84 | loss: 0.34812 | val_0_rmse: 0.60873 | val_1_rmse: 0.62696 |  0:02:51s
epoch 85 | loss: 0.34593 | val_0_rmse: 0.60533 | val_1_rmse: 0.62372 |  0:02:53s
epoch 86 | loss: 0.33779 | val_0_rmse: 0.57863 | val_1_rmse: 0.59599 |  0:02:55s
epoch 87 | loss: 0.33992 | val_0_rmse: 0.67388 | val_1_rmse: 0.73805 |  0:02:57s
epoch 88 | loss: 0.34712 | val_0_rmse: 0.60119 | val_1_rmse: 0.61968 |  0:02:59s
epoch 89 | loss: 0.34    | val_0_rmse: 0.60968 | val_1_rmse: 0.6333  |  0:03:01s
epoch 90 | loss: 0.34816 | val_0_rmse: 0.63967 | val_1_rmse: 0.66267 |  0:03:03s
epoch 91 | loss: 0.35223 | val_0_rmse: 0.65326 | val_1_rmse: 0.67551 |  0:03:05s
epoch 92 | loss: 0.33289 | val_0_rmse: 0.66006 | val_1_rmse: 0.6502  |  0:03:07s
epoch 93 | loss: 0.33539 | val_0_rmse: 0.6316  | val_1_rmse: 0.66137 |  0:03:09s
epoch 94 | loss: 0.33614 | val_0_rmse: 0.59419 | val_1_rmse: 0.61122 |  0:03:11s
epoch 95 | loss: 0.34001 | val_0_rmse: 0.61006 | val_1_rmse: 0.63637 |  0:03:13s
epoch 96 | loss: 0.32997 | val_0_rmse: 0.55072 | val_1_rmse: 0.57379 |  0:03:15s
epoch 97 | loss: 0.3247  | val_0_rmse: 0.65742 | val_1_rmse: 0.68271 |  0:03:17s
epoch 98 | loss: 0.33118 | val_0_rmse: 0.57281 | val_1_rmse: 0.58886 |  0:03:19s
epoch 99 | loss: 0.33423 | val_0_rmse: 0.56731 | val_1_rmse: 0.58662 |  0:03:21s
epoch 100| loss: 0.32517 | val_0_rmse: 0.64614 | val_1_rmse: 0.67411 |  0:03:23s
epoch 101| loss: 0.3284  | val_0_rmse: 0.57672 | val_1_rmse: 0.6019  |  0:03:25s
epoch 102| loss: 0.32732 | val_0_rmse: 0.58077 | val_1_rmse: 0.60299 |  0:03:27s
epoch 103| loss: 0.33778 | val_0_rmse: 0.62839 | val_1_rmse: 0.64966 |  0:03:29s
epoch 104| loss: 0.34172 | val_0_rmse: 0.57426 | val_1_rmse: 0.59756 |  0:03:31s
epoch 105| loss: 0.32433 | val_0_rmse: 0.58846 | val_1_rmse: 0.60772 |  0:03:33s
epoch 106| loss: 0.31983 | val_0_rmse: 0.60084 | val_1_rmse: 0.62705 |  0:03:35s
epoch 107| loss: 0.31131 | val_0_rmse: 0.54864 | val_1_rmse: 0.57466 |  0:03:37s
epoch 108| loss: 0.30987 | val_0_rmse: 0.54929 | val_1_rmse: 0.57989 |  0:03:39s
epoch 109| loss: 0.32138 | val_0_rmse: 0.61337 | val_1_rmse: 0.64078 |  0:03:41s
epoch 110| loss: 0.32399 | val_0_rmse: 0.56734 | val_1_rmse: 0.59598 |  0:03:43s
epoch 111| loss: 0.31618 | val_0_rmse: 0.56382 | val_1_rmse: 0.58383 |  0:03:45s
epoch 112| loss: 0.31748 | val_0_rmse: 0.65137 | val_1_rmse: 0.6735  |  0:03:47s
epoch 113| loss: 0.31941 | val_0_rmse: 0.61122 | val_1_rmse: 0.63284 |  0:03:49s
epoch 114| loss: 0.3179  | val_0_rmse: 0.55406 | val_1_rmse: 0.57797 |  0:03:51s
epoch 115| loss: 0.30739 | val_0_rmse: 0.55233 | val_1_rmse: 0.57883 |  0:03:53s
epoch 116| loss: 0.30273 | val_0_rmse: 0.53332 | val_1_rmse: 0.55449 |  0:03:55s
epoch 117| loss: 0.30885 | val_0_rmse: 0.57113 | val_1_rmse: 0.60071 |  0:03:57s
epoch 118| loss: 0.30397 | val_0_rmse: 0.56628 | val_1_rmse: 0.58866 |  0:03:59s
epoch 119| loss: 0.30605 | val_0_rmse: 0.62269 | val_1_rmse: 0.64834 |  0:04:01s
epoch 120| loss: 0.30225 | val_0_rmse: 0.61473 | val_1_rmse: 0.63938 |  0:04:03s
epoch 121| loss: 0.30336 | val_0_rmse: 0.57436 | val_1_rmse: 0.59157 |  0:04:05s
epoch 122| loss: 0.29831 | val_0_rmse: 0.55363 | val_1_rmse: 0.58472 |  0:04:07s
epoch 123| loss: 0.29813 | val_0_rmse: 0.60045 | val_1_rmse: 0.62341 |  0:04:09s
epoch 124| loss: 0.29993 | val_0_rmse: 0.5444  | val_1_rmse: 0.568   |  0:04:11s
epoch 125| loss: 0.29193 | val_0_rmse: 0.53365 | val_1_rmse: 0.55903 |  0:04:13s
epoch 126| loss: 0.28805 | val_0_rmse: 0.55172 | val_1_rmse: 0.57658 |  0:04:15s
epoch 127| loss: 0.29315 | val_0_rmse: 0.57499 | val_1_rmse: 0.59648 |  0:04:17s
epoch 128| loss: 0.28966 | val_0_rmse: 0.51991 | val_1_rmse: 0.54982 |  0:04:19s
epoch 129| loss: 0.28611 | val_0_rmse: 0.53981 | val_1_rmse: 0.56795 |  0:04:21s
epoch 130| loss: 0.28622 | val_0_rmse: 0.57951 | val_1_rmse: 0.60638 |  0:04:23s
epoch 131| loss: 0.29338 | val_0_rmse: 0.55787 | val_1_rmse: 0.57726 |  0:04:25s
epoch 132| loss: 0.28808 | val_0_rmse: 0.56678 | val_1_rmse: 0.59007 |  0:04:27s
epoch 133| loss: 0.29922 | val_0_rmse: 0.62117 | val_1_rmse: 0.64005 |  0:04:29s
epoch 134| loss: 0.2931  | val_0_rmse: 0.5741  | val_1_rmse: 0.59898 |  0:04:31s
epoch 135| loss: 0.28531 | val_0_rmse: 0.53595 | val_1_rmse: 0.56737 |  0:04:33s
epoch 136| loss: 0.28407 | val_0_rmse: 0.53737 | val_1_rmse: 0.56203 |  0:04:35s
epoch 137| loss: 0.29305 | val_0_rmse: 0.55807 | val_1_rmse: 0.57369 |  0:04:37s
epoch 138| loss: 0.28529 | val_0_rmse: 0.55367 | val_1_rmse: 0.57414 |  0:04:39s
epoch 139| loss: 0.29382 | val_0_rmse: 0.59752 | val_1_rmse: 0.61996 |  0:04:41s
epoch 140| loss: 0.29855 | val_0_rmse: 0.6069  | val_1_rmse: 0.62637 |  0:04:43s
epoch 141| loss: 0.29715 | val_0_rmse: 0.53472 | val_1_rmse: 0.56041 |  0:04:45s
epoch 142| loss: 0.28998 | val_0_rmse: 0.54063 | val_1_rmse: 0.56636 |  0:04:47s
epoch 143| loss: 0.28447 | val_0_rmse: 0.59574 | val_1_rmse: 0.61174 |  0:04:49s
epoch 144| loss: 0.2746  | val_0_rmse: 0.54806 | val_1_rmse: 0.57431 |  0:04:51s
epoch 145| loss: 0.27956 | val_0_rmse: 0.54059 | val_1_rmse: 0.56182 |  0:04:53s
epoch 146| loss: 0.28541 | val_0_rmse: 0.60916 | val_1_rmse: 0.63392 |  0:04:55s
epoch 147| loss: 0.27678 | val_0_rmse: 0.52764 | val_1_rmse: 0.55435 |  0:04:57s
epoch 148| loss: 0.28551 | val_0_rmse: 0.5626  | val_1_rmse: 0.57958 |  0:04:59s
epoch 149| loss: 0.27826 | val_0_rmse: 0.51889 | val_1_rmse: 0.54704 |  0:05:01s
Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_val_1_rmse = 0.54704
Best weights from best epoch are automatically used!
ended training at: 16:40:54
Feature importance:
[('Area', 0.5239490735463055), ('Baths', 0.10642595231892851), ('Beds', 0.03838612620865721), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.0396407361961766), ('Year', 0.29159811172993216)]
Mean squared error is of 2404654903.8434334
Mean absolute error:33740.986097653666
MAPE:0.3012629368481378
R2 score:0.7040263168420635
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:40:55
epoch 0  | loss: 0.62158 | val_0_rmse: 0.73521 | val_1_rmse: 0.7238  |  0:00:01s
epoch 1  | loss: 0.51908 | val_0_rmse: 0.71168 | val_1_rmse: 0.6958  |  0:00:03s
epoch 2  | loss: 0.50446 | val_0_rmse: 0.7058  | val_1_rmse: 0.69441 |  0:00:05s
epoch 3  | loss: 0.49313 | val_0_rmse: 0.69917 | val_1_rmse: 0.68553 |  0:00:07s
epoch 4  | loss: 0.48795 | val_0_rmse: 0.70334 | val_1_rmse: 0.69406 |  0:00:09s
epoch 5  | loss: 0.48945 | val_0_rmse: 0.69341 | val_1_rmse: 0.6802  |  0:00:11s
epoch 6  | loss: 0.48814 | val_0_rmse: 0.69102 | val_1_rmse: 0.68398 |  0:00:14s
epoch 7  | loss: 0.48749 | val_0_rmse: 0.69506 | val_1_rmse: 0.68414 |  0:00:16s
epoch 8  | loss: 0.48972 | val_0_rmse: 0.72254 | val_1_rmse: 0.71016 |  0:00:18s
epoch 9  | loss: 0.4857  | val_0_rmse: 0.69316 | val_1_rmse: 0.68227 |  0:00:19s
epoch 10 | loss: 0.48052 | val_0_rmse: 0.69254 | val_1_rmse: 0.6823  |  0:00:21s
epoch 11 | loss: 0.47616 | val_0_rmse: 0.72417 | val_1_rmse: 0.71582 |  0:00:23s
epoch 12 | loss: 0.48355 | val_0_rmse: 0.69129 | val_1_rmse: 0.67974 |  0:00:26s
epoch 13 | loss: 0.47877 | val_0_rmse: 0.68803 | val_1_rmse: 0.67657 |  0:00:28s
epoch 14 | loss: 0.47805 | val_0_rmse: 0.68178 | val_1_rmse: 0.67006 |  0:00:30s
epoch 15 | loss: 0.47484 | val_0_rmse: 0.68533 | val_1_rmse: 0.66972 |  0:00:32s
epoch 16 | loss: 0.47407 | val_0_rmse: 0.68381 | val_1_rmse: 0.67337 |  0:00:34s
epoch 17 | loss: 0.46817 | val_0_rmse: 0.68004 | val_1_rmse: 0.66574 |  0:00:36s
epoch 18 | loss: 0.46732 | val_0_rmse: 0.67584 | val_1_rmse: 0.6614  |  0:00:38s
epoch 19 | loss: 0.4653  | val_0_rmse: 0.68197 | val_1_rmse: 0.67056 |  0:00:40s
epoch 20 | loss: 0.46825 | val_0_rmse: 0.6799  | val_1_rmse: 0.66959 |  0:00:42s
epoch 21 | loss: 0.46238 | val_0_rmse: 0.68009 | val_1_rmse: 0.66835 |  0:00:44s
epoch 22 | loss: 0.47802 | val_0_rmse: 0.6863  | val_1_rmse: 0.67488 |  0:00:46s
epoch 23 | loss: 0.47289 | val_0_rmse: 0.6833  | val_1_rmse: 0.67101 |  0:00:48s
epoch 24 | loss: 0.47112 | val_0_rmse: 0.69374 | val_1_rmse: 0.68274 |  0:00:50s
epoch 25 | loss: 0.47923 | val_0_rmse: 0.69    | val_1_rmse: 0.67633 |  0:00:52s
epoch 26 | loss: 0.47898 | val_0_rmse: 0.69168 | val_1_rmse: 0.68041 |  0:00:54s
epoch 27 | loss: 0.47412 | val_0_rmse: 0.6788  | val_1_rmse: 0.66733 |  0:00:56s
epoch 28 | loss: 0.46746 | val_0_rmse: 0.684   | val_1_rmse: 0.67177 |  0:00:58s
epoch 29 | loss: 0.46742 | val_0_rmse: 0.67957 | val_1_rmse: 0.67092 |  0:01:00s
epoch 30 | loss: 0.46233 | val_0_rmse: 0.67821 | val_1_rmse: 0.67316 |  0:01:02s
epoch 31 | loss: 0.45831 | val_0_rmse: 0.67808 | val_1_rmse: 0.67118 |  0:01:04s
epoch 32 | loss: 0.45916 | val_0_rmse: 0.67287 | val_1_rmse: 0.66518 |  0:01:06s
epoch 33 | loss: 0.45922 | val_0_rmse: 0.67683 | val_1_rmse: 0.66743 |  0:01:08s
epoch 34 | loss: 0.461   | val_0_rmse: 0.6836  | val_1_rmse: 0.67431 |  0:01:10s
epoch 35 | loss: 0.45917 | val_0_rmse: 0.66636 | val_1_rmse: 0.65818 |  0:01:12s
epoch 36 | loss: 0.45077 | val_0_rmse: 0.66797 | val_1_rmse: 0.65969 |  0:01:14s
epoch 37 | loss: 0.45345 | val_0_rmse: 0.66347 | val_1_rmse: 0.65838 |  0:01:16s
epoch 38 | loss: 0.44957 | val_0_rmse: 0.66704 | val_1_rmse: 0.65944 |  0:01:18s
epoch 39 | loss: 0.45093 | val_0_rmse: 0.66266 | val_1_rmse: 0.65418 |  0:01:20s
epoch 40 | loss: 0.44289 | val_0_rmse: 0.66273 | val_1_rmse: 0.6595  |  0:01:22s
epoch 41 | loss: 0.44416 | val_0_rmse: 0.66626 | val_1_rmse: 0.65782 |  0:01:24s
epoch 42 | loss: 0.44444 | val_0_rmse: 0.65863 | val_1_rmse: 0.65261 |  0:01:26s
epoch 43 | loss: 0.44113 | val_0_rmse: 0.65865 | val_1_rmse: 0.65565 |  0:01:28s
epoch 44 | loss: 0.45738 | val_0_rmse: 0.68392 | val_1_rmse: 0.67857 |  0:01:30s
epoch 45 | loss: 0.45135 | val_0_rmse: 0.66018 | val_1_rmse: 0.65586 |  0:01:32s
epoch 46 | loss: 0.44038 | val_0_rmse: 0.65199 | val_1_rmse: 0.64    |  0:01:34s
epoch 47 | loss: 0.43612 | val_0_rmse: 0.65327 | val_1_rmse: 0.64724 |  0:01:36s
epoch 48 | loss: 0.43312 | val_0_rmse: 0.64652 | val_1_rmse: 0.64111 |  0:01:38s
epoch 49 | loss: 0.42872 | val_0_rmse: 0.64733 | val_1_rmse: 0.64295 |  0:01:40s
epoch 50 | loss: 0.42285 | val_0_rmse: 0.66375 | val_1_rmse: 0.65878 |  0:01:42s
epoch 51 | loss: 0.42417 | val_0_rmse: 0.64809 | val_1_rmse: 0.64237 |  0:01:44s
epoch 52 | loss: 0.4336  | val_0_rmse: 0.65969 | val_1_rmse: 0.64943 |  0:01:46s
epoch 53 | loss: 0.42157 | val_0_rmse: 0.68055 | val_1_rmse: 0.67389 |  0:01:48s
epoch 54 | loss: 0.42372 | val_0_rmse: 0.63765 | val_1_rmse: 0.63278 |  0:01:50s
epoch 55 | loss: 0.40619 | val_0_rmse: 0.64097 | val_1_rmse: 0.63714 |  0:01:52s
epoch 56 | loss: 0.3992  | val_0_rmse: 0.64311 | val_1_rmse: 0.64345 |  0:01:54s
epoch 57 | loss: 0.39448 | val_0_rmse: 0.68469 | val_1_rmse: 0.68087 |  0:01:56s
epoch 58 | loss: 0.40101 | val_0_rmse: 0.6701  | val_1_rmse: 0.66232 |  0:01:58s
epoch 59 | loss: 0.38534 | val_0_rmse: 0.61361 | val_1_rmse: 0.61492 |  0:02:00s
epoch 60 | loss: 0.39506 | val_0_rmse: 0.61068 | val_1_rmse: 0.61091 |  0:02:02s
epoch 61 | loss: 0.38583 | val_0_rmse: 0.64033 | val_1_rmse: 0.63833 |  0:02:04s
epoch 62 | loss: 0.37498 | val_0_rmse: 0.68094 | val_1_rmse: 0.6782  |  0:02:06s
epoch 63 | loss: 0.37992 | val_0_rmse: 0.64498 | val_1_rmse: 0.64623 |  0:02:08s
epoch 64 | loss: 0.37785 | val_0_rmse: 0.64041 | val_1_rmse: 0.6416  |  0:02:10s
epoch 65 | loss: 0.36085 | val_0_rmse: 0.62499 | val_1_rmse: 0.62839 |  0:02:12s
epoch 66 | loss: 0.3625  | val_0_rmse: 0.60752 | val_1_rmse: 0.61315 |  0:02:14s
epoch 67 | loss: 0.36159 | val_0_rmse: 0.6019  | val_1_rmse: 0.59938 |  0:02:16s
epoch 68 | loss: 0.34796 | val_0_rmse: 0.64943 | val_1_rmse: 0.65252 |  0:02:18s
epoch 69 | loss: 0.35308 | val_0_rmse: 0.61555 | val_1_rmse: 0.61871 |  0:02:20s
epoch 70 | loss: 0.35143 | val_0_rmse: 0.62723 | val_1_rmse: 0.62706 |  0:02:22s
epoch 71 | loss: 0.34105 | val_0_rmse: 0.56393 | val_1_rmse: 0.56876 |  0:02:24s
epoch 72 | loss: 0.34096 | val_0_rmse: 0.58539 | val_1_rmse: 0.58924 |  0:02:26s
epoch 73 | loss: 0.3307  | val_0_rmse: 0.58048 | val_1_rmse: 0.58583 |  0:02:28s
epoch 74 | loss: 0.32275 | val_0_rmse: 0.61393 | val_1_rmse: 0.6208  |  0:02:30s
epoch 75 | loss: 0.33325 | val_0_rmse: 0.5594  | val_1_rmse: 0.56759 |  0:02:32s
epoch 76 | loss: 0.33305 | val_0_rmse: 0.57992 | val_1_rmse: 0.58689 |  0:02:34s
epoch 77 | loss: 0.31855 | val_0_rmse: 0.54528 | val_1_rmse: 0.55384 |  0:02:36s
epoch 78 | loss: 0.31891 | val_0_rmse: 0.58113 | val_1_rmse: 0.5852  |  0:02:38s
epoch 79 | loss: 0.31668 | val_0_rmse: 0.6196  | val_1_rmse: 0.62574 |  0:02:40s
epoch 80 | loss: 0.32062 | val_0_rmse: 0.55335 | val_1_rmse: 0.55749 |  0:02:42s
epoch 81 | loss: 0.31666 | val_0_rmse: 0.57541 | val_1_rmse: 0.58608 |  0:02:44s
epoch 82 | loss: 0.31613 | val_0_rmse: 0.55458 | val_1_rmse: 0.56569 |  0:02:46s
epoch 83 | loss: 0.31672 | val_0_rmse: 0.61366 | val_1_rmse: 0.6197  |  0:02:48s
epoch 84 | loss: 0.31799 | val_0_rmse: 0.57658 | val_1_rmse: 0.58406 |  0:02:50s
epoch 85 | loss: 0.31431 | val_0_rmse: 0.59103 | val_1_rmse: 0.59986 |  0:02:52s
epoch 86 | loss: 0.31607 | val_0_rmse: 0.54042 | val_1_rmse: 0.54972 |  0:02:54s
epoch 87 | loss: 0.30452 | val_0_rmse: 0.55796 | val_1_rmse: 0.56286 |  0:02:56s
epoch 88 | loss: 0.30116 | val_0_rmse: 0.54107 | val_1_rmse: 0.54851 |  0:02:58s
epoch 89 | loss: 0.30998 | val_0_rmse: 0.55744 | val_1_rmse: 0.5673  |  0:03:00s
epoch 90 | loss: 0.30942 | val_0_rmse: 0.59704 | val_1_rmse: 0.60742 |  0:03:02s
epoch 91 | loss: 0.29917 | val_0_rmse: 0.53749 | val_1_rmse: 0.54716 |  0:03:04s
epoch 92 | loss: 0.30345 | val_0_rmse: 0.54535 | val_1_rmse: 0.5543  |  0:03:06s
epoch 93 | loss: 0.30424 | val_0_rmse: 0.55192 | val_1_rmse: 0.56517 |  0:03:08s
epoch 94 | loss: 0.29827 | val_0_rmse: 0.53743 | val_1_rmse: 0.54774 |  0:03:10s
epoch 95 | loss: 0.29967 | val_0_rmse: 0.54722 | val_1_rmse: 0.55053 |  0:03:12s
epoch 96 | loss: 0.29887 | val_0_rmse: 0.54901 | val_1_rmse: 0.56196 |  0:03:14s
epoch 97 | loss: 0.2925  | val_0_rmse: 0.52677 | val_1_rmse: 0.53809 |  0:03:16s
epoch 98 | loss: 0.29888 | val_0_rmse: 0.53162 | val_1_rmse: 0.54186 |  0:03:18s
epoch 99 | loss: 0.30578 | val_0_rmse: 0.53666 | val_1_rmse: 0.55201 |  0:03:20s
epoch 100| loss: 0.3016  | val_0_rmse: 0.59672 | val_1_rmse: 0.59975 |  0:03:22s
epoch 101| loss: 0.29983 | val_0_rmse: 0.54001 | val_1_rmse: 0.54838 |  0:03:24s
epoch 102| loss: 0.29569 | val_0_rmse: 0.57894 | val_1_rmse: 0.58243 |  0:03:26s
epoch 103| loss: 0.29657 | val_0_rmse: 0.54724 | val_1_rmse: 0.55589 |  0:03:28s
epoch 104| loss: 0.29649 | val_0_rmse: 0.53526 | val_1_rmse: 0.54455 |  0:03:30s
epoch 105| loss: 0.29565 | val_0_rmse: 0.55176 | val_1_rmse: 0.56315 |  0:03:32s
epoch 106| loss: 0.29554 | val_0_rmse: 0.54604 | val_1_rmse: 0.55281 |  0:03:34s
epoch 107| loss: 0.29017 | val_0_rmse: 0.53697 | val_1_rmse: 0.54684 |  0:03:36s
epoch 108| loss: 0.28818 | val_0_rmse: 0.60064 | val_1_rmse: 0.61302 |  0:03:38s
epoch 109| loss: 0.29095 | val_0_rmse: 0.51684 | val_1_rmse: 0.52381 |  0:03:40s
epoch 110| loss: 0.28117 | val_0_rmse: 0.54092 | val_1_rmse: 0.5477  |  0:03:42s
epoch 111| loss: 0.28121 | val_0_rmse: 0.52183 | val_1_rmse: 0.53236 |  0:03:44s
epoch 112| loss: 0.28682 | val_0_rmse: 0.51937 | val_1_rmse: 0.52714 |  0:03:46s
epoch 113| loss: 0.29103 | val_0_rmse: 0.53321 | val_1_rmse: 0.53696 |  0:03:48s
epoch 114| loss: 0.28443 | val_0_rmse: 0.56398 | val_1_rmse: 0.57193 |  0:03:50s
epoch 115| loss: 0.29018 | val_0_rmse: 0.6725  | val_1_rmse: 0.67849 |  0:03:52s
epoch 116| loss: 0.28942 | val_0_rmse: 0.55845 | val_1_rmse: 0.56536 |  0:03:54s
epoch 117| loss: 0.2808  | val_0_rmse: 0.52168 | val_1_rmse: 0.53156 |  0:03:56s
epoch 118| loss: 0.29013 | val_0_rmse: 0.68855 | val_1_rmse: 0.68939 |  0:03:58s
epoch 119| loss: 0.29458 | val_0_rmse: 0.67141 | val_1_rmse: 0.67978 |  0:04:00s
epoch 120| loss: 0.28493 | val_0_rmse: 0.53145 | val_1_rmse: 0.53851 |  0:04:02s
epoch 121| loss: 0.28874 | val_0_rmse: 0.60484 | val_1_rmse: 0.61863 |  0:04:04s
epoch 122| loss: 0.28582 | val_0_rmse: 0.51126 | val_1_rmse: 0.52598 |  0:04:06s
epoch 123| loss: 0.27939 | val_0_rmse: 0.50785 | val_1_rmse: 0.51875 |  0:04:08s
epoch 124| loss: 0.27957 | val_0_rmse: 0.58214 | val_1_rmse: 0.59469 |  0:04:10s
epoch 125| loss: 0.28039 | val_0_rmse: 0.54681 | val_1_rmse: 0.55578 |  0:04:12s
epoch 126| loss: 0.27643 | val_0_rmse: 0.53786 | val_1_rmse: 0.53828 |  0:04:14s
epoch 127| loss: 0.27785 | val_0_rmse: 0.52562 | val_1_rmse: 0.5337  |  0:04:16s
epoch 128| loss: 0.27862 | val_0_rmse: 0.54843 | val_1_rmse: 0.55625 |  0:04:18s
epoch 129| loss: 0.27912 | val_0_rmse: 0.59348 | val_1_rmse: 0.60164 |  0:04:20s
epoch 130| loss: 0.28479 | val_0_rmse: 0.52749 | val_1_rmse: 0.53232 |  0:04:22s
epoch 131| loss: 0.27611 | val_0_rmse: 0.5238  | val_1_rmse: 0.53118 |  0:04:24s
epoch 132| loss: 0.28358 | val_0_rmse: 0.64376 | val_1_rmse: 0.65471 |  0:04:26s
epoch 133| loss: 0.28481 | val_0_rmse: 0.53558 | val_1_rmse: 0.54152 |  0:04:28s
epoch 134| loss: 0.28607 | val_0_rmse: 0.56342 | val_1_rmse: 0.57162 |  0:04:30s
epoch 135| loss: 0.27929 | val_0_rmse: 0.55668 | val_1_rmse: 0.57347 |  0:04:32s
epoch 136| loss: 0.27471 | val_0_rmse: 0.51488 | val_1_rmse: 0.52297 |  0:04:34s
epoch 137| loss: 0.27493 | val_0_rmse: 0.56561 | val_1_rmse: 0.57416 |  0:04:36s
epoch 138| loss: 0.27236 | val_0_rmse: 0.54373 | val_1_rmse: 0.55683 |  0:04:38s
epoch 139| loss: 0.27185 | val_0_rmse: 0.52822 | val_1_rmse: 0.54265 |  0:04:40s
epoch 140| loss: 0.26884 | val_0_rmse: 0.61734 | val_1_rmse: 0.62102 |  0:04:42s
epoch 141| loss: 0.28358 | val_0_rmse: 0.50156 | val_1_rmse: 0.51206 |  0:04:44s
epoch 142| loss: 0.2713  | val_0_rmse: 0.56016 | val_1_rmse: 0.56768 |  0:04:46s
epoch 143| loss: 0.28887 | val_0_rmse: 0.59438 | val_1_rmse: 0.59774 |  0:04:48s
epoch 144| loss: 0.27555 | val_0_rmse: 0.50923 | val_1_rmse: 0.5136  |  0:04:50s
epoch 145| loss: 0.26925 | val_0_rmse: 0.50976 | val_1_rmse: 0.52235 |  0:04:52s
epoch 146| loss: 0.27039 | val_0_rmse: 0.49533 | val_1_rmse: 0.50854 |  0:04:54s
epoch 147| loss: 0.26663 | val_0_rmse: 0.50259 | val_1_rmse: 0.5133  |  0:04:56s
epoch 148| loss: 0.2653  | val_0_rmse: 0.48776 | val_1_rmse: 0.5034  |  0:04:58s
epoch 149| loss: 0.26252 | val_0_rmse: 0.58359 | val_1_rmse: 0.58957 |  0:05:00s
Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_val_1_rmse = 0.5034
Best weights from best epoch are automatically used!
ended training at: 16:45:56
Feature importance:
[('Area', 0.3728279061593107), ('Baths', 0.38692342305132355), ('Beds', 0.0), ('Latitude', 0.0), ('Longitude', 0.06714000813319607), ('Month', 0.1731086626561697), ('Year', 0.0)]
Mean squared error is of 2206167617.465573
Mean absolute error:31831.01844718018
MAPE:0.28637285701609255
R2 score:0.7409383635636231
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:45:57
epoch 0  | loss: 0.6456  | val_0_rmse: 0.72341 | val_1_rmse: 0.73906 |  0:00:01s
epoch 1  | loss: 0.49904 | val_0_rmse: 0.72238 | val_1_rmse: 0.73802 |  0:00:03s
epoch 2  | loss: 0.49411 | val_0_rmse: 0.69647 | val_1_rmse: 0.70313 |  0:00:06s
epoch 3  | loss: 0.47787 | val_0_rmse: 0.68214 | val_1_rmse: 0.69007 |  0:00:08s
epoch 4  | loss: 0.47856 | val_0_rmse: 0.68506 | val_1_rmse: 0.6912  |  0:00:10s
epoch 5  | loss: 0.47547 | val_0_rmse: 0.68504 | val_1_rmse: 0.69486 |  0:00:12s
epoch 6  | loss: 0.467   | val_0_rmse: 0.68405 | val_1_rmse: 0.69806 |  0:00:14s
epoch 7  | loss: 0.47182 | val_0_rmse: 0.67806 | val_1_rmse: 0.68329 |  0:00:16s
epoch 8  | loss: 0.46474 | val_0_rmse: 0.67861 | val_1_rmse: 0.68885 |  0:00:18s
epoch 9  | loss: 0.46598 | val_0_rmse: 0.68239 | val_1_rmse: 0.68898 |  0:00:20s
epoch 10 | loss: 0.46015 | val_0_rmse: 0.67566 | val_1_rmse: 0.68435 |  0:00:22s
epoch 11 | loss: 0.45708 | val_0_rmse: 0.6874  | val_1_rmse: 0.70276 |  0:00:24s
epoch 12 | loss: 0.45948 | val_0_rmse: 0.66904 | val_1_rmse: 0.67808 |  0:00:26s
epoch 13 | loss: 0.44215 | val_0_rmse: 0.65846 | val_1_rmse: 0.66573 |  0:00:28s
epoch 14 | loss: 0.42411 | val_0_rmse: 0.66865 | val_1_rmse: 0.67131 |  0:00:30s
epoch 15 | loss: 0.43739 | val_0_rmse: 0.66361 | val_1_rmse: 0.66669 |  0:00:32s
epoch 16 | loss: 0.40724 | val_0_rmse: 0.64541 | val_1_rmse: 0.65098 |  0:00:34s
epoch 17 | loss: 0.40912 | val_0_rmse: 0.66084 | val_1_rmse: 0.66352 |  0:00:36s
epoch 18 | loss: 0.40283 | val_0_rmse: 0.65338 | val_1_rmse: 0.65599 |  0:00:38s
epoch 19 | loss: 0.42147 | val_0_rmse: 0.6703  | val_1_rmse: 0.67484 |  0:00:40s
epoch 20 | loss: 0.41407 | val_0_rmse: 0.65006 | val_1_rmse: 0.65772 |  0:00:42s
epoch 21 | loss: 0.4104  | val_0_rmse: 0.61954 | val_1_rmse: 0.62717 |  0:00:44s
epoch 22 | loss: 0.39227 | val_0_rmse: 0.63092 | val_1_rmse: 0.63776 |  0:00:46s
epoch 23 | loss: 0.39504 | val_0_rmse: 0.62295 | val_1_rmse: 0.62845 |  0:00:48s
epoch 24 | loss: 0.39845 | val_0_rmse: 0.624   | val_1_rmse: 0.62432 |  0:00:50s
epoch 25 | loss: 0.40007 | val_0_rmse: 0.63348 | val_1_rmse: 0.63936 |  0:00:52s
epoch 26 | loss: 0.39461 | val_0_rmse: 0.63078 | val_1_rmse: 0.64259 |  0:00:54s
epoch 27 | loss: 0.39952 | val_0_rmse: 0.62416 | val_1_rmse: 0.63227 |  0:00:56s
epoch 28 | loss: 0.38486 | val_0_rmse: 0.6208  | val_1_rmse: 0.62148 |  0:00:58s
epoch 29 | loss: 0.38728 | val_0_rmse: 0.71901 | val_1_rmse: 0.72646 |  0:01:00s
epoch 30 | loss: 0.3879  | val_0_rmse: 0.64197 | val_1_rmse: 0.64545 |  0:01:02s
epoch 31 | loss: 0.38041 | val_0_rmse: 0.63086 | val_1_rmse: 0.63414 |  0:01:04s
epoch 32 | loss: 0.38021 | val_0_rmse: 0.61742 | val_1_rmse: 0.61584 |  0:01:06s
epoch 33 | loss: 0.36903 | val_0_rmse: 0.63668 | val_1_rmse: 0.63858 |  0:01:08s
epoch 34 | loss: 0.36103 | val_0_rmse: 0.6324  | val_1_rmse: 0.63222 |  0:01:10s
epoch 35 | loss: 0.35883 | val_0_rmse: 0.65313 | val_1_rmse: 0.6613  |  0:01:12s
epoch 36 | loss: 0.36449 | val_0_rmse: 0.60683 | val_1_rmse: 0.60907 |  0:01:14s
epoch 37 | loss: 0.35887 | val_0_rmse: 0.66193 | val_1_rmse: 0.66703 |  0:01:16s
epoch 38 | loss: 0.35935 | val_0_rmse: 0.63329 | val_1_rmse: 0.63791 |  0:01:18s
epoch 39 | loss: 0.36766 | val_0_rmse: 0.65399 | val_1_rmse: 0.65921 |  0:01:20s
epoch 40 | loss: 0.36327 | val_0_rmse: 0.60032 | val_1_rmse: 0.59939 |  0:01:22s
epoch 41 | loss: 0.35631 | val_0_rmse: 0.60975 | val_1_rmse: 0.61041 |  0:01:24s
epoch 42 | loss: 0.35943 | val_0_rmse: 0.6277  | val_1_rmse: 0.628   |  0:01:26s
epoch 43 | loss: 0.36181 | val_0_rmse: 0.62944 | val_1_rmse: 0.63254 |  0:01:28s
epoch 44 | loss: 0.35813 | val_0_rmse: 0.68798 | val_1_rmse: 0.68969 |  0:01:30s
epoch 45 | loss: 0.36189 | val_0_rmse: 0.66077 | val_1_rmse: 0.66891 |  0:01:32s
epoch 46 | loss: 0.35581 | val_0_rmse: 0.68919 | val_1_rmse: 0.69893 |  0:01:34s
epoch 47 | loss: 0.38623 | val_0_rmse: 0.63794 | val_1_rmse: 0.63797 |  0:01:36s
epoch 48 | loss: 0.36124 | val_0_rmse: 0.65164 | val_1_rmse: 0.64991 |  0:01:38s
epoch 49 | loss: 0.36059 | val_0_rmse: 0.61076 | val_1_rmse: 0.61148 |  0:01:40s
epoch 50 | loss: 0.35796 | val_0_rmse: 0.63249 | val_1_rmse: 0.63328 |  0:01:42s
epoch 51 | loss: 0.35092 | val_0_rmse: 0.59847 | val_1_rmse: 0.59767 |  0:01:44s
epoch 52 | loss: 0.35188 | val_0_rmse: 0.63634 | val_1_rmse: 0.63546 |  0:01:46s
epoch 53 | loss: 0.35122 | val_0_rmse: 0.62379 | val_1_rmse: 0.6216  |  0:01:48s
epoch 54 | loss: 0.34677 | val_0_rmse: 0.67477 | val_1_rmse: 0.68183 |  0:01:50s
epoch 55 | loss: 0.34871 | val_0_rmse: 0.59781 | val_1_rmse: 0.59485 |  0:01:52s
epoch 56 | loss: 0.34312 | val_0_rmse: 0.59575 | val_1_rmse: 0.59348 |  0:01:54s
epoch 57 | loss: 0.34587 | val_0_rmse: 0.60285 | val_1_rmse: 0.59798 |  0:01:56s
epoch 58 | loss: 0.34846 | val_0_rmse: 0.62756 | val_1_rmse: 0.6332  |  0:01:58s
epoch 59 | loss: 0.35922 | val_0_rmse: 0.65828 | val_1_rmse: 0.65973 |  0:02:00s
epoch 60 | loss: 0.3523  | val_0_rmse: 0.63492 | val_1_rmse: 0.64502 |  0:02:02s
epoch 61 | loss: 0.34539 | val_0_rmse: 0.63012 | val_1_rmse: 0.63404 |  0:02:04s
epoch 62 | loss: 0.34772 | val_0_rmse: 0.59259 | val_1_rmse: 0.59366 |  0:02:06s
epoch 63 | loss: 0.34368 | val_0_rmse: 0.57406 | val_1_rmse: 0.57543 |  0:02:08s
epoch 64 | loss: 0.34592 | val_0_rmse: 0.66867 | val_1_rmse: 0.67254 |  0:02:10s
epoch 65 | loss: 0.35008 | val_0_rmse: 0.59281 | val_1_rmse: 0.59237 |  0:02:12s
epoch 66 | loss: 0.34269 | val_0_rmse: 0.59179 | val_1_rmse: 0.58808 |  0:02:14s
epoch 67 | loss: 0.34217 | val_0_rmse: 0.65262 | val_1_rmse: 0.65819 |  0:02:16s
epoch 68 | loss: 0.34423 | val_0_rmse: 0.61087 | val_1_rmse: 0.61068 |  0:02:18s
epoch 69 | loss: 0.34811 | val_0_rmse: 0.63722 | val_1_rmse: 0.63987 |  0:02:20s
epoch 70 | loss: 0.3425  | val_0_rmse: 0.6011  | val_1_rmse: 0.6002  |  0:02:22s
epoch 71 | loss: 0.34276 | val_0_rmse: 0.59239 | val_1_rmse: 0.59204 |  0:02:24s
epoch 72 | loss: 0.3418  | val_0_rmse: 0.6041  | val_1_rmse: 0.60231 |  0:02:26s
epoch 73 | loss: 0.34371 | val_0_rmse: 0.61994 | val_1_rmse: 0.62253 |  0:02:28s
epoch 74 | loss: 0.34102 | val_0_rmse: 0.62092 | val_1_rmse: 0.61467 |  0:02:30s
epoch 75 | loss: 0.34172 | val_0_rmse: 0.60299 | val_1_rmse: 0.6075  |  0:02:32s
epoch 76 | loss: 0.33611 | val_0_rmse: 0.69684 | val_1_rmse: 0.70092 |  0:02:34s
epoch 77 | loss: 0.34298 | val_0_rmse: 0.62785 | val_1_rmse: 0.62784 |  0:02:36s
epoch 78 | loss: 0.3366  | val_0_rmse: 0.57965 | val_1_rmse: 0.58009 |  0:02:38s
epoch 79 | loss: 0.33994 | val_0_rmse: 0.59248 | val_1_rmse: 0.59341 |  0:02:41s
epoch 80 | loss: 0.33925 | val_0_rmse: 0.62728 | val_1_rmse: 0.63638 |  0:02:43s
epoch 81 | loss: 0.33869 | val_0_rmse: 0.58765 | val_1_rmse: 0.58648 |  0:02:44s
epoch 82 | loss: 0.33912 | val_0_rmse: 0.57715 | val_1_rmse: 0.57658 |  0:02:46s
epoch 83 | loss: 0.34309 | val_0_rmse: 0.60955 | val_1_rmse: 0.60576 |  0:02:48s
epoch 84 | loss: 0.34331 | val_0_rmse: 0.66018 | val_1_rmse: 0.66547 |  0:02:50s
epoch 85 | loss: 0.33567 | val_0_rmse: 0.68515 | val_1_rmse: 0.68811 |  0:02:52s
epoch 86 | loss: 0.32801 | val_0_rmse: 0.63691 | val_1_rmse: 0.63652 |  0:02:54s
epoch 87 | loss: 0.33538 | val_0_rmse: 0.58894 | val_1_rmse: 0.5915  |  0:02:56s
epoch 88 | loss: 0.33476 | val_0_rmse: 0.57969 | val_1_rmse: 0.58392 |  0:02:59s
epoch 89 | loss: 0.33398 | val_0_rmse: 0.61449 | val_1_rmse: 0.6113  |  0:03:00s
epoch 90 | loss: 0.3331  | val_0_rmse: 0.68094 | val_1_rmse: 0.68773 |  0:03:02s
epoch 91 | loss: 0.33734 | val_0_rmse: 0.5795  | val_1_rmse: 0.58202 |  0:03:04s
epoch 92 | loss: 0.33399 | val_0_rmse: 0.6341  | val_1_rmse: 0.63863 |  0:03:06s
epoch 93 | loss: 0.33824 | val_0_rmse: 0.5974  | val_1_rmse: 0.59527 |  0:03:08s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.57543
Best weights from best epoch are automatically used!
ended training at: 16:49:06
Feature importance:
[('Area', 0.437606960071268), ('Baths', 0.22227171479480476), ('Beds', 0.24761570198069302), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.08753744929752223), ('Year', 0.004968173855712018)]
Mean squared error is of 2749792152.0151033
Mean absolute error:36111.443733788794
MAPE:0.3266232730469745
R2 score:0.6584684610517015
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:49:07
epoch 0  | loss: 0.64284 | val_0_rmse: 0.74218 | val_1_rmse: 0.73221 |  0:00:01s
epoch 1  | loss: 0.50113 | val_0_rmse: 0.70281 | val_1_rmse: 0.69716 |  0:00:03s
epoch 2  | loss: 0.49494 | val_0_rmse: 0.69899 | val_1_rmse: 0.68832 |  0:00:05s
epoch 3  | loss: 0.4841  | val_0_rmse: 0.69307 | val_1_rmse: 0.68386 |  0:00:07s
epoch 4  | loss: 0.48421 | val_0_rmse: 0.68221 | val_1_rmse: 0.67328 |  0:00:09s
epoch 5  | loss: 0.47664 | val_0_rmse: 0.70085 | val_1_rmse: 0.69305 |  0:00:11s
epoch 6  | loss: 0.48423 | val_0_rmse: 0.68494 | val_1_rmse: 0.6732  |  0:00:13s
epoch 7  | loss: 0.47388 | val_0_rmse: 0.69005 | val_1_rmse: 0.6785  |  0:00:16s
epoch 8  | loss: 0.47366 | val_0_rmse: 0.6857  | val_1_rmse: 0.67969 |  0:00:18s
epoch 9  | loss: 0.47101 | val_0_rmse: 0.68169 | val_1_rmse: 0.67228 |  0:00:20s
epoch 10 | loss: 0.47366 | val_0_rmse: 0.68802 | val_1_rmse: 0.67998 |  0:00:22s
epoch 11 | loss: 0.46969 | val_0_rmse: 0.68251 | val_1_rmse: 0.66976 |  0:00:24s
epoch 12 | loss: 0.46633 | val_0_rmse: 0.67944 | val_1_rmse: 0.67167 |  0:00:26s
epoch 13 | loss: 0.46675 | val_0_rmse: 0.68628 | val_1_rmse: 0.6735  |  0:00:28s
epoch 14 | loss: 0.46748 | val_0_rmse: 0.68009 | val_1_rmse: 0.66899 |  0:00:30s
epoch 15 | loss: 0.46928 | val_0_rmse: 0.67869 | val_1_rmse: 0.66815 |  0:00:32s
epoch 16 | loss: 0.46328 | val_0_rmse: 0.67685 | val_1_rmse: 0.66989 |  0:00:34s
epoch 17 | loss: 0.46446 | val_0_rmse: 0.67525 | val_1_rmse: 0.66672 |  0:00:36s
epoch 18 | loss: 0.46375 | val_0_rmse: 0.67632 | val_1_rmse: 0.66809 |  0:00:38s
epoch 19 | loss: 0.46266 | val_0_rmse: 0.67654 | val_1_rmse: 0.6719  |  0:00:40s
epoch 20 | loss: 0.46488 | val_0_rmse: 0.68392 | val_1_rmse: 0.67773 |  0:00:42s
epoch 21 | loss: 0.47038 | val_0_rmse: 0.67145 | val_1_rmse: 0.65908 |  0:00:44s
epoch 22 | loss: 0.45702 | val_0_rmse: 0.67453 | val_1_rmse: 0.66612 |  0:00:46s
epoch 23 | loss: 0.45951 | val_0_rmse: 0.66734 | val_1_rmse: 0.65827 |  0:00:48s
epoch 24 | loss: 0.45261 | val_0_rmse: 0.66805 | val_1_rmse: 0.65932 |  0:00:50s
epoch 25 | loss: 0.45067 | val_0_rmse: 0.67065 | val_1_rmse: 0.65998 |  0:00:52s
epoch 26 | loss: 0.4508  | val_0_rmse: 0.67199 | val_1_rmse: 0.66631 |  0:00:54s
epoch 27 | loss: 0.4489  | val_0_rmse: 0.66339 | val_1_rmse: 0.6575  |  0:00:56s
epoch 28 | loss: 0.44714 | val_0_rmse: 0.67217 | val_1_rmse: 0.66258 |  0:00:58s
epoch 29 | loss: 0.45273 | val_0_rmse: 0.66552 | val_1_rmse: 0.65883 |  0:01:00s
epoch 30 | loss: 0.44293 | val_0_rmse: 0.65433 | val_1_rmse: 0.64271 |  0:01:02s
epoch 31 | loss: 0.44108 | val_0_rmse: 0.6659  | val_1_rmse: 0.6616  |  0:01:04s
epoch 32 | loss: 0.43662 | val_0_rmse: 0.66311 | val_1_rmse: 0.6517  |  0:01:06s
epoch 33 | loss: 0.44168 | val_0_rmse: 0.66397 | val_1_rmse: 0.65474 |  0:01:08s
epoch 34 | loss: 0.4424  | val_0_rmse: 0.66303 | val_1_rmse: 0.65121 |  0:01:10s
epoch 35 | loss: 0.43881 | val_0_rmse: 0.65378 | val_1_rmse: 0.64703 |  0:01:12s
epoch 36 | loss: 0.43873 | val_0_rmse: 0.66469 | val_1_rmse: 0.65792 |  0:01:14s
epoch 37 | loss: 0.43562 | val_0_rmse: 0.64511 | val_1_rmse: 0.63808 |  0:01:16s
epoch 38 | loss: 0.45411 | val_0_rmse: 0.66734 | val_1_rmse: 0.65643 |  0:01:18s
epoch 39 | loss: 0.44024 | val_0_rmse: 0.66097 | val_1_rmse: 0.6523  |  0:01:20s
epoch 40 | loss: 0.43682 | val_0_rmse: 0.67335 | val_1_rmse: 0.65885 |  0:01:22s
epoch 41 | loss: 0.42281 | val_0_rmse: 0.66689 | val_1_rmse: 0.66392 |  0:01:24s
epoch 42 | loss: 0.44401 | val_0_rmse: 0.6618  | val_1_rmse: 0.64683 |  0:01:26s
epoch 43 | loss: 0.42172 | val_0_rmse: 0.66532 | val_1_rmse: 0.64939 |  0:01:28s
epoch 44 | loss: 0.43439 | val_0_rmse: 0.642   | val_1_rmse: 0.62628 |  0:01:30s
epoch 45 | loss: 0.42195 | val_0_rmse: 0.64319 | val_1_rmse: 0.63025 |  0:01:32s
epoch 46 | loss: 0.40643 | val_0_rmse: 0.62293 | val_1_rmse: 0.60798 |  0:01:34s
epoch 47 | loss: 0.41499 | val_0_rmse: 0.64667 | val_1_rmse: 0.63755 |  0:01:36s
epoch 48 | loss: 0.40169 | val_0_rmse: 0.62541 | val_1_rmse: 0.61068 |  0:01:38s
epoch 49 | loss: 0.39919 | val_0_rmse: 0.63595 | val_1_rmse: 0.61967 |  0:01:40s
epoch 50 | loss: 0.38101 | val_0_rmse: 0.6072  | val_1_rmse: 0.59574 |  0:01:42s
epoch 51 | loss: 0.38286 | val_0_rmse: 0.60068 | val_1_rmse: 0.58845 |  0:01:44s
epoch 52 | loss: 0.3861  | val_0_rmse: 0.62201 | val_1_rmse: 0.6116  |  0:01:46s
epoch 53 | loss: 0.39562 | val_0_rmse: 0.63738 | val_1_rmse: 0.62865 |  0:01:48s
epoch 54 | loss: 0.39007 | val_0_rmse: 0.66964 | val_1_rmse: 0.65593 |  0:01:50s
epoch 55 | loss: 0.38712 | val_0_rmse: 0.64584 | val_1_rmse: 0.62456 |  0:01:52s
epoch 56 | loss: 0.38276 | val_0_rmse: 0.63626 | val_1_rmse: 0.62333 |  0:01:54s
epoch 57 | loss: 0.37553 | val_0_rmse: 0.60527 | val_1_rmse: 0.59112 |  0:01:56s
epoch 58 | loss: 0.37512 | val_0_rmse: 0.63442 | val_1_rmse: 0.62045 |  0:01:58s
epoch 59 | loss: 0.42116 | val_0_rmse: 0.76849 | val_1_rmse: 0.75775 |  0:02:00s
epoch 60 | loss: 0.47264 | val_0_rmse: 0.67748 | val_1_rmse: 0.66599 |  0:02:02s
epoch 61 | loss: 0.44095 | val_0_rmse: 0.64576 | val_1_rmse: 0.63524 |  0:02:04s
epoch 62 | loss: 0.41793 | val_0_rmse: 0.65606 | val_1_rmse: 0.63806 |  0:02:06s
epoch 63 | loss: 0.39312 | val_0_rmse: 0.61461 | val_1_rmse: 0.59605 |  0:02:08s
epoch 64 | loss: 0.38676 | val_0_rmse: 0.60714 | val_1_rmse: 0.59285 |  0:02:10s
epoch 65 | loss: 0.38728 | val_0_rmse: 0.62034 | val_1_rmse: 0.60702 |  0:02:12s
epoch 66 | loss: 0.38992 | val_0_rmse: 0.61755 | val_1_rmse: 0.6054  |  0:02:14s
epoch 67 | loss: 0.38093 | val_0_rmse: 0.6527  | val_1_rmse: 0.64136 |  0:02:16s
epoch 68 | loss: 0.38324 | val_0_rmse: 0.67061 | val_1_rmse: 0.65484 |  0:02:18s
epoch 69 | loss: 0.37898 | val_0_rmse: 0.6082  | val_1_rmse: 0.59476 |  0:02:20s
epoch 70 | loss: 0.36797 | val_0_rmse: 0.61938 | val_1_rmse: 0.60195 |  0:02:22s
epoch 71 | loss: 0.36361 | val_0_rmse: 0.63607 | val_1_rmse: 0.61915 |  0:02:24s
epoch 72 | loss: 0.36044 | val_0_rmse: 0.60781 | val_1_rmse: 0.59499 |  0:02:26s
epoch 73 | loss: 0.35685 | val_0_rmse: 0.60076 | val_1_rmse: 0.58484 |  0:02:28s
epoch 74 | loss: 0.34943 | val_0_rmse: 0.58418 | val_1_rmse: 0.57039 |  0:02:30s
epoch 75 | loss: 0.35087 | val_0_rmse: 0.59828 | val_1_rmse: 0.58238 |  0:02:32s
epoch 76 | loss: 0.34832 | val_0_rmse: 0.61622 | val_1_rmse: 0.594   |  0:02:34s
epoch 77 | loss: 0.34642 | val_0_rmse: 0.62835 | val_1_rmse: 0.61444 |  0:02:37s
epoch 78 | loss: 0.3445  | val_0_rmse: 0.66108 | val_1_rmse: 0.65309 |  0:02:39s
epoch 79 | loss: 0.36204 | val_0_rmse: 0.6152  | val_1_rmse: 0.60307 |  0:02:40s
epoch 80 | loss: 0.35227 | val_0_rmse: 0.58114 | val_1_rmse: 0.5636  |  0:02:42s
epoch 81 | loss: 0.35554 | val_0_rmse: 0.60477 | val_1_rmse: 0.58607 |  0:02:44s
epoch 82 | loss: 0.3444  | val_0_rmse: 0.5745  | val_1_rmse: 0.56088 |  0:02:46s
epoch 83 | loss: 0.34423 | val_0_rmse: 0.57562 | val_1_rmse: 0.55654 |  0:02:48s
epoch 84 | loss: 0.33888 | val_0_rmse: 0.60521 | val_1_rmse: 0.58626 |  0:02:50s
epoch 85 | loss: 0.34745 | val_0_rmse: 0.60124 | val_1_rmse: 0.58497 |  0:02:53s
epoch 86 | loss: 0.36008 | val_0_rmse: 0.62242 | val_1_rmse: 0.61132 |  0:02:55s
epoch 87 | loss: 0.37199 | val_0_rmse: 0.60873 | val_1_rmse: 0.5901  |  0:02:57s
epoch 88 | loss: 0.38485 | val_0_rmse: 0.6472  | val_1_rmse: 0.63556 |  0:02:59s
epoch 89 | loss: 0.37801 | val_0_rmse: 0.6067  | val_1_rmse: 0.58968 |  0:03:01s
epoch 90 | loss: 0.37185 | val_0_rmse: 0.60506 | val_1_rmse: 0.59111 |  0:03:03s
epoch 91 | loss: 0.37553 | val_0_rmse: 0.61444 | val_1_rmse: 0.59804 |  0:03:05s
epoch 92 | loss: 0.36445 | val_0_rmse: 0.60686 | val_1_rmse: 0.58892 |  0:03:07s
epoch 93 | loss: 0.36376 | val_0_rmse: 0.6062  | val_1_rmse: 0.59012 |  0:03:09s
epoch 94 | loss: 0.36462 | val_0_rmse: 0.60403 | val_1_rmse: 0.59092 |  0:03:11s
epoch 95 | loss: 0.36807 | val_0_rmse: 0.64924 | val_1_rmse: 0.63471 |  0:03:13s
epoch 96 | loss: 0.36516 | val_0_rmse: 0.62564 | val_1_rmse: 0.61232 |  0:03:15s
epoch 97 | loss: 0.36251 | val_0_rmse: 0.59363 | val_1_rmse: 0.57814 |  0:03:17s
epoch 98 | loss: 0.35378 | val_0_rmse: 0.61747 | val_1_rmse: 0.59894 |  0:03:19s
epoch 99 | loss: 0.35233 | val_0_rmse: 0.58699 | val_1_rmse: 0.56684 |  0:03:21s
epoch 100| loss: 0.34603 | val_0_rmse: 0.61014 | val_1_rmse: 0.5939  |  0:03:23s
epoch 101| loss: 0.35354 | val_0_rmse: 0.58614 | val_1_rmse: 0.57129 |  0:03:25s
epoch 102| loss: 0.34893 | val_0_rmse: 0.62117 | val_1_rmse: 0.5984  |  0:03:27s
epoch 103| loss: 0.3492  | val_0_rmse: 0.62527 | val_1_rmse: 0.60249 |  0:03:29s
epoch 104| loss: 0.35271 | val_0_rmse: 0.66865 | val_1_rmse: 0.6593  |  0:03:31s
epoch 105| loss: 0.37988 | val_0_rmse: 0.74055 | val_1_rmse: 0.72326 |  0:03:33s
epoch 106| loss: 0.3528  | val_0_rmse: 0.58845 | val_1_rmse: 0.5696  |  0:03:35s
epoch 107| loss: 0.35844 | val_0_rmse: 0.62549 | val_1_rmse: 0.60973 |  0:03:37s
epoch 108| loss: 0.34717 | val_0_rmse: 0.57931 | val_1_rmse: 0.55914 |  0:03:39s
epoch 109| loss: 0.34227 | val_0_rmse: 0.57696 | val_1_rmse: 0.56074 |  0:03:41s
epoch 110| loss: 0.33954 | val_0_rmse: 0.58336 | val_1_rmse: 0.56793 |  0:03:43s
epoch 111| loss: 0.34087 | val_0_rmse: 0.63355 | val_1_rmse: 0.61586 |  0:03:45s
epoch 112| loss: 0.33615 | val_0_rmse: 0.59427 | val_1_rmse: 0.58289 |  0:03:47s
epoch 113| loss: 0.33705 | val_0_rmse: 0.57822 | val_1_rmse: 0.566   |  0:03:49s

Early stopping occured at epoch 113 with best_epoch = 83 and best_val_1_rmse = 0.55654
Best weights from best epoch are automatically used!
ended training at: 16:52:57
Feature importance:
[('Area', 0.2917699602021942), ('Baths', 0.3206227628213733), ('Beds', 0.2611983369945078), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.12640893998192465)]
Mean squared error is of 2706865512.249878
Mean absolute error:36180.297128619284
MAPE:0.3059246597902524
R2 score:0.6723416559346198
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:52:58
epoch 0  | loss: 0.44141 | val_0_rmse: 0.57591 | val_1_rmse: 0.58518 |  0:00:04s
epoch 1  | loss: 0.33899 | val_0_rmse: 0.56827 | val_1_rmse: 0.5771  |  0:00:07s
epoch 2  | loss: 0.33201 | val_0_rmse: 0.57098 | val_1_rmse: 0.57766 |  0:00:11s
epoch 3  | loss: 0.32984 | val_0_rmse: 0.56953 | val_1_rmse: 0.5766  |  0:00:15s
epoch 4  | loss: 0.33328 | val_0_rmse: 0.57472 | val_1_rmse: 0.58056 |  0:00:19s
epoch 5  | loss: 0.32859 | val_0_rmse: 0.56695 | val_1_rmse: 0.57503 |  0:00:23s
epoch 6  | loss: 0.32677 | val_0_rmse: 0.56161 | val_1_rmse: 0.56795 |  0:00:27s
epoch 7  | loss: 0.32423 | val_0_rmse: 0.5594  | val_1_rmse: 0.56633 |  0:00:30s
epoch 8  | loss: 0.32311 | val_0_rmse: 0.55938 | val_1_rmse: 0.56696 |  0:00:34s
epoch 9  | loss: 0.31777 | val_0_rmse: 0.56399 | val_1_rmse: 0.56706 |  0:00:38s
epoch 10 | loss: 0.32016 | val_0_rmse: 0.56652 | val_1_rmse: 0.5741  |  0:00:42s
epoch 11 | loss: 0.31802 | val_0_rmse: 0.55254 | val_1_rmse: 0.55832 |  0:00:46s
epoch 12 | loss: 0.31441 | val_0_rmse: 0.55596 | val_1_rmse: 0.56077 |  0:00:50s
epoch 13 | loss: 0.31204 | val_0_rmse: 0.55106 | val_1_rmse: 0.55757 |  0:00:54s
epoch 14 | loss: 0.31345 | val_0_rmse: 0.55435 | val_1_rmse: 0.55987 |  0:00:58s
epoch 15 | loss: 0.31556 | val_0_rmse: 0.55504 | val_1_rmse: 0.55773 |  0:01:02s
epoch 16 | loss: 0.31228 | val_0_rmse: 0.55145 | val_1_rmse: 0.55937 |  0:01:05s
epoch 17 | loss: 0.31466 | val_0_rmse: 0.55229 | val_1_rmse: 0.55637 |  0:01:09s
epoch 18 | loss: 0.30853 | val_0_rmse: 0.55887 | val_1_rmse: 0.56234 |  0:01:13s
epoch 19 | loss: 0.30797 | val_0_rmse: 0.54923 | val_1_rmse: 0.55713 |  0:01:17s
epoch 20 | loss: 0.30173 | val_0_rmse: 0.54738 | val_1_rmse: 0.55588 |  0:01:21s
epoch 21 | loss: 0.29501 | val_0_rmse: 0.53525 | val_1_rmse: 0.53776 |  0:01:25s
epoch 22 | loss: 0.30008 | val_0_rmse: 0.54621 | val_1_rmse: 0.54707 |  0:01:28s
epoch 23 | loss: 0.29342 | val_0_rmse: 0.52817 | val_1_rmse: 0.53238 |  0:01:32s
epoch 24 | loss: 0.29105 | val_0_rmse: 0.56326 | val_1_rmse: 0.56033 |  0:01:36s
epoch 25 | loss: 0.28815 | val_0_rmse: 0.57479 | val_1_rmse: 0.5842  |  0:01:40s
epoch 26 | loss: 0.28772 | val_0_rmse: 0.58129 | val_1_rmse: 0.59233 |  0:01:44s
epoch 27 | loss: 0.29813 | val_0_rmse: 0.59355 | val_1_rmse: 0.59303 |  0:01:48s
epoch 28 | loss: 0.29347 | val_0_rmse: 0.53321 | val_1_rmse: 0.53753 |  0:01:52s
epoch 29 | loss: 0.288   | val_0_rmse: 0.55391 | val_1_rmse: 0.56134 |  0:01:56s
epoch 30 | loss: 0.28484 | val_0_rmse: 0.53763 | val_1_rmse: 0.5426  |  0:01:59s
epoch 31 | loss: 0.28697 | val_0_rmse: 0.56691 | val_1_rmse: 0.57548 |  0:02:03s
epoch 32 | loss: 0.29185 | val_0_rmse: 0.53831 | val_1_rmse: 0.54567 |  0:02:07s
epoch 33 | loss: 0.28652 | val_0_rmse: 0.52502 | val_1_rmse: 0.52588 |  0:02:11s
epoch 34 | loss: 0.28816 | val_0_rmse: 0.54311 | val_1_rmse: 0.5481  |  0:02:15s
epoch 35 | loss: 0.28394 | val_0_rmse: 0.58534 | val_1_rmse: 0.59657 |  0:02:19s
epoch 36 | loss: 0.28321 | val_0_rmse: 0.52714 | val_1_rmse: 0.53062 |  0:02:23s
epoch 37 | loss: 0.28114 | val_0_rmse: 0.52623 | val_1_rmse: 0.53182 |  0:02:27s
epoch 38 | loss: 0.28316 | val_0_rmse: 0.5258  | val_1_rmse: 0.53039 |  0:02:30s
epoch 39 | loss: 0.28111 | val_0_rmse: 0.56754 | val_1_rmse: 0.57587 |  0:02:34s
epoch 40 | loss: 0.27986 | val_0_rmse: 0.61868 | val_1_rmse: 0.62915 |  0:02:38s
epoch 41 | loss: 0.28656 | val_0_rmse: 0.52518 | val_1_rmse: 0.52701 |  0:02:42s
epoch 42 | loss: 0.28008 | val_0_rmse: 0.59856 | val_1_rmse: 0.60855 |  0:02:46s
epoch 43 | loss: 0.28344 | val_0_rmse: 0.52701 | val_1_rmse: 0.53025 |  0:02:50s
epoch 44 | loss: 0.28307 | val_0_rmse: 0.54228 | val_1_rmse: 0.54961 |  0:02:54s
epoch 45 | loss: 0.28156 | val_0_rmse: 0.5277  | val_1_rmse: 0.52873 |  0:02:57s
epoch 46 | loss: 0.27955 | val_0_rmse: 0.53751 | val_1_rmse: 0.53364 |  0:03:01s
epoch 47 | loss: 0.2778  | val_0_rmse: 0.5361  | val_1_rmse: 0.5423  |  0:03:05s
epoch 48 | loss: 0.28303 | val_0_rmse: 0.56692 | val_1_rmse: 0.57631 |  0:03:09s
epoch 49 | loss: 0.29027 | val_0_rmse: 0.56284 | val_1_rmse: 0.56301 |  0:03:13s
epoch 50 | loss: 0.29702 | val_0_rmse: 0.53198 | val_1_rmse: 0.53272 |  0:03:17s
epoch 51 | loss: 0.29517 | val_0_rmse: 0.54828 | val_1_rmse: 0.55256 |  0:03:21s
epoch 52 | loss: 0.2936  | val_0_rmse: 0.61208 | val_1_rmse: 0.60982 |  0:03:25s
epoch 53 | loss: 0.28974 | val_0_rmse: 0.53697 | val_1_rmse: 0.54231 |  0:03:28s
epoch 54 | loss: 0.29173 | val_0_rmse: 0.59296 | val_1_rmse: 0.59152 |  0:03:32s
epoch 55 | loss: 0.29359 | val_0_rmse: 0.53017 | val_1_rmse: 0.53118 |  0:03:36s
epoch 56 | loss: 0.28401 | val_0_rmse: 0.60425 | val_1_rmse: 0.61455 |  0:03:40s
epoch 57 | loss: 0.28545 | val_0_rmse: 0.56689 | val_1_rmse: 0.56452 |  0:03:44s
epoch 58 | loss: 0.2829  | val_0_rmse: 0.57721 | val_1_rmse: 0.58627 |  0:03:48s
epoch 59 | loss: 0.28263 | val_0_rmse: 0.55949 | val_1_rmse: 0.56929 |  0:03:52s
epoch 60 | loss: 0.28327 | val_0_rmse: 0.52365 | val_1_rmse: 0.52742 |  0:03:55s
epoch 61 | loss: 0.28088 | val_0_rmse: 0.52683 | val_1_rmse: 0.52908 |  0:03:59s
epoch 62 | loss: 0.28034 | val_0_rmse: 0.53048 | val_1_rmse: 0.53101 |  0:04:03s
epoch 63 | loss: 0.28009 | val_0_rmse: 0.52671 | val_1_rmse: 0.52963 |  0:04:07s

Early stopping occured at epoch 63 with best_epoch = 33 and best_val_1_rmse = 0.52588
Best weights from best epoch are automatically used!
ended training at: 16:57:06
Feature importance:
[('Area', 0.5669451110032815), ('Baths', 0.12428950129997468), ('Beds', 0.1998418916628063), ('Latitude', 0.039978874523011035), ('Longitude', 0.0), ('Month', 0.010324512601964311), ('Year', 0.058620108908962236)]
Mean squared error is of 953208533.8300021
Mean absolute error:21374.02455575998
MAPE:0.3425592875758805
R2 score:0.715756856517901
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 16:57:07
epoch 0  | loss: 0.43301 | val_0_rmse: 0.57587 | val_1_rmse: 0.56794 |  0:00:03s
epoch 1  | loss: 0.34202 | val_0_rmse: 0.57956 | val_1_rmse: 0.57014 |  0:00:07s
epoch 2  | loss: 0.3402  | val_0_rmse: 0.5713  | val_1_rmse: 0.56633 |  0:00:11s
epoch 3  | loss: 0.33408 | val_0_rmse: 0.57128 | val_1_rmse: 0.56596 |  0:00:15s
epoch 4  | loss: 0.33253 | val_0_rmse: 0.56501 | val_1_rmse: 0.55913 |  0:00:19s
epoch 5  | loss: 0.32473 | val_0_rmse: 0.56521 | val_1_rmse: 0.55862 |  0:00:23s
epoch 6  | loss: 0.32261 | val_0_rmse: 0.58879 | val_1_rmse: 0.58529 |  0:00:27s
epoch 7  | loss: 0.31518 | val_0_rmse: 0.64823 | val_1_rmse: 0.64235 |  0:00:30s
epoch 8  | loss: 0.31007 | val_0_rmse: 0.58057 | val_1_rmse: 0.57382 |  0:00:34s
epoch 9  | loss: 0.30789 | val_0_rmse: 0.62441 | val_1_rmse: 0.621   |  0:00:38s
epoch 10 | loss: 0.3121  | val_0_rmse: 0.56018 | val_1_rmse: 0.55411 |  0:00:42s
epoch 11 | loss: 0.30934 | val_0_rmse: 0.63278 | val_1_rmse: 0.6267  |  0:00:46s
epoch 12 | loss: 0.30019 | val_0_rmse: 0.62221 | val_1_rmse: 0.61513 |  0:00:50s
epoch 13 | loss: 0.29533 | val_0_rmse: 0.55463 | val_1_rmse: 0.5478  |  0:00:53s
epoch 14 | loss: 0.30087 | val_0_rmse: 0.54557 | val_1_rmse: 0.53964 |  0:00:58s
epoch 15 | loss: 0.29391 | val_0_rmse: 0.60936 | val_1_rmse: 0.60234 |  0:01:01s
epoch 16 | loss: 0.28896 | val_0_rmse: 0.5382  | val_1_rmse: 0.53381 |  0:01:05s
epoch 17 | loss: 0.28872 | val_0_rmse: 1.29401 | val_1_rmse: 1.29641 |  0:01:09s
epoch 18 | loss: 0.28797 | val_0_rmse: 0.58581 | val_1_rmse: 0.58487 |  0:01:13s
epoch 19 | loss: 0.28633 | val_0_rmse: 0.59955 | val_1_rmse: 0.5927  |  0:01:17s
epoch 20 | loss: 0.28942 | val_0_rmse: 0.52663 | val_1_rmse: 0.523   |  0:01:21s
epoch 21 | loss: 0.28226 | val_0_rmse: 0.55106 | val_1_rmse: 0.5456  |  0:01:24s
epoch 22 | loss: 0.28156 | val_0_rmse: 0.53014 | val_1_rmse: 0.52474 |  0:01:28s
epoch 23 | loss: 0.28238 | val_0_rmse: 0.63483 | val_1_rmse: 0.6269  |  0:01:32s
epoch 24 | loss: 0.28107 | val_0_rmse: 0.52299 | val_1_rmse: 0.52069 |  0:01:36s
epoch 25 | loss: 0.28051 | val_0_rmse: 0.54524 | val_1_rmse: 0.54666 |  0:01:40s
epoch 26 | loss: 0.28084 | val_0_rmse: 0.52292 | val_1_rmse: 0.51917 |  0:01:44s
epoch 27 | loss: 0.28261 | val_0_rmse: 0.76525 | val_1_rmse: 0.77079 |  0:01:48s
epoch 28 | loss: 0.28301 | val_0_rmse: 2.83952 | val_1_rmse: 2.83932 |  0:01:52s
epoch 29 | loss: 0.28683 | val_0_rmse: 0.61466 | val_1_rmse: 0.60751 |  0:01:55s
epoch 30 | loss: 0.28143 | val_0_rmse: 0.56926 | val_1_rmse: 0.56306 |  0:01:59s
epoch 31 | loss: 0.28088 | val_0_rmse: 0.52862 | val_1_rmse: 0.52831 |  0:02:03s
epoch 32 | loss: 0.27841 | val_0_rmse: 0.5496  | val_1_rmse: 0.55116 |  0:02:07s
epoch 33 | loss: 0.28247 | val_0_rmse: 0.52973 | val_1_rmse: 0.5259  |  0:02:11s
epoch 34 | loss: 0.28144 | val_0_rmse: 0.58773 | val_1_rmse: 0.58116 |  0:02:15s
epoch 35 | loss: 0.28169 | val_0_rmse: 0.52766 | val_1_rmse: 0.52739 |  0:02:19s
epoch 36 | loss: 0.27887 | val_0_rmse: 0.53441 | val_1_rmse: 0.53464 |  0:02:22s
epoch 37 | loss: 0.27897 | val_0_rmse: 0.52607 | val_1_rmse: 0.52    |  0:02:26s
epoch 38 | loss: 0.2787  | val_0_rmse: 0.55715 | val_1_rmse: 0.5512  |  0:02:30s
epoch 39 | loss: 0.27914 | val_0_rmse: 0.60933 | val_1_rmse: 0.6033  |  0:02:34s
epoch 40 | loss: 0.27729 | val_0_rmse: 0.55966 | val_1_rmse: 0.55197 |  0:02:38s
epoch 41 | loss: 0.27752 | val_0_rmse: 0.51607 | val_1_rmse: 0.513   |  0:02:42s
epoch 42 | loss: 0.27672 | val_0_rmse: 0.64168 | val_1_rmse: 0.64432 |  0:02:46s
epoch 43 | loss: 0.27795 | val_0_rmse: 0.54707 | val_1_rmse: 0.54145 |  0:02:50s
epoch 44 | loss: 0.27531 | val_0_rmse: 0.61835 | val_1_rmse: 0.61177 |  0:02:53s
epoch 45 | loss: 0.27674 | val_0_rmse: 0.55795 | val_1_rmse: 0.55296 |  0:02:57s
epoch 46 | loss: 0.27757 | val_0_rmse: 0.55559 | val_1_rmse: 0.54968 |  0:03:01s
epoch 47 | loss: 0.27611 | val_0_rmse: 0.52536 | val_1_rmse: 0.52322 |  0:03:05s
epoch 48 | loss: 0.27789 | val_0_rmse: 0.55924 | val_1_rmse: 0.55408 |  0:03:09s
epoch 49 | loss: 0.27849 | val_0_rmse: 0.5945  | val_1_rmse: 0.58729 |  0:03:13s
epoch 50 | loss: 0.2799  | val_0_rmse: 0.52648 | val_1_rmse: 0.52185 |  0:03:17s
epoch 51 | loss: 0.27692 | val_0_rmse: 0.61845 | val_1_rmse: 0.61146 |  0:03:20s
epoch 52 | loss: 0.27843 | val_0_rmse: 0.51697 | val_1_rmse: 0.51194 |  0:03:24s
epoch 53 | loss: 0.27566 | val_0_rmse: 0.63627 | val_1_rmse: 0.64254 |  0:03:28s
epoch 54 | loss: 0.27388 | val_0_rmse: 0.55632 | val_1_rmse: 0.5511  |  0:03:32s
epoch 55 | loss: 0.27312 | val_0_rmse: 0.52108 | val_1_rmse: 0.5164  |  0:03:36s
epoch 56 | loss: 0.27286 | val_0_rmse: 0.56916 | val_1_rmse: 0.57212 |  0:03:40s
epoch 57 | loss: 0.27234 | val_0_rmse: 0.59071 | val_1_rmse: 0.58511 |  0:03:44s
epoch 58 | loss: 0.27706 | val_0_rmse: 0.64398 | val_1_rmse: 0.63786 |  0:03:48s
epoch 59 | loss: 0.27188 | val_0_rmse: 0.59543 | val_1_rmse: 0.58938 |  0:03:53s
epoch 60 | loss: 0.27187 | val_0_rmse: 0.62406 | val_1_rmse: 0.61896 |  0:03:57s
epoch 61 | loss: 0.27155 | val_0_rmse: 0.57805 | val_1_rmse: 0.57312 |  0:04:02s
epoch 62 | loss: 0.27258 | val_0_rmse: 0.51503 | val_1_rmse: 0.51199 |  0:04:06s
epoch 63 | loss: 0.27012 | val_0_rmse: 0.61656 | val_1_rmse: 0.62139 |  0:04:10s
epoch 64 | loss: 0.27166 | val_0_rmse: 0.51727 | val_1_rmse: 0.51466 |  0:04:14s
epoch 65 | loss: 0.26949 | val_0_rmse: 0.52973 | val_1_rmse: 0.52558 |  0:04:18s
epoch 66 | loss: 0.2714  | val_0_rmse: 0.51706 | val_1_rmse: 0.51339 |  0:04:22s
epoch 67 | loss: 0.27084 | val_0_rmse: 0.54614 | val_1_rmse: 0.54087 |  0:04:26s
epoch 68 | loss: 0.27284 | val_0_rmse: 0.51354 | val_1_rmse: 0.51223 |  0:04:30s
epoch 69 | loss: 0.27305 | val_0_rmse: 0.51752 | val_1_rmse: 0.51223 |  0:04:34s
epoch 70 | loss: 0.27211 | val_0_rmse: 0.52077 | val_1_rmse: 0.51944 |  0:04:38s
epoch 71 | loss: 0.27252 | val_0_rmse: 0.57282 | val_1_rmse: 0.56733 |  0:04:41s
epoch 72 | loss: 0.27286 | val_0_rmse: 0.74305 | val_1_rmse: 0.7481  |  0:04:45s
epoch 73 | loss: 0.27157 | val_0_rmse: 0.51545 | val_1_rmse: 0.5133  |  0:04:49s
epoch 74 | loss: 0.26784 | val_0_rmse: 0.58301 | val_1_rmse: 0.58555 |  0:04:53s
epoch 75 | loss: 0.26856 | val_0_rmse: 0.57988 | val_1_rmse: 0.57457 |  0:04:57s
epoch 76 | loss: 0.26768 | val_0_rmse: 0.59002 | val_1_rmse: 0.58543 |  0:05:01s
epoch 77 | loss: 0.26922 | val_0_rmse: 0.66368 | val_1_rmse: 0.65872 |  0:05:05s
epoch 78 | loss: 0.26775 | val_0_rmse: 0.50988 | val_1_rmse: 0.50883 |  0:05:08s
epoch 79 | loss: 0.26957 | val_0_rmse: 4.4139  | val_1_rmse: 4.4176  |  0:05:12s
epoch 80 | loss: 0.27366 | val_0_rmse: 0.53422 | val_1_rmse: 0.52802 |  0:05:16s
epoch 81 | loss: 0.26835 | val_0_rmse: 0.51775 | val_1_rmse: 0.51546 |  0:05:20s
epoch 82 | loss: 0.26854 | val_0_rmse: 0.51312 | val_1_rmse: 0.51174 |  0:05:24s
epoch 83 | loss: 0.2648  | val_0_rmse: 0.52373 | val_1_rmse: 0.5194  |  0:05:28s
epoch 84 | loss: 0.26831 | val_0_rmse: 0.50706 | val_1_rmse: 0.50593 |  0:05:32s
epoch 85 | loss: 0.26445 | val_0_rmse: 0.50735 | val_1_rmse: 0.50515 |  0:05:35s
epoch 86 | loss: 0.26439 | val_0_rmse: 0.51496 | val_1_rmse: 0.51218 |  0:05:39s
epoch 87 | loss: 0.26493 | val_0_rmse: 0.58778 | val_1_rmse: 0.57967 |  0:05:43s
epoch 88 | loss: 0.26291 | val_0_rmse: 0.5314  | val_1_rmse: 0.52538 |  0:05:47s
epoch 89 | loss: 0.26274 | val_0_rmse: 0.52173 | val_1_rmse: 0.51681 |  0:05:51s
epoch 90 | loss: 0.26219 | val_0_rmse: 0.59594 | val_1_rmse: 0.58949 |  0:05:55s
epoch 91 | loss: 0.26506 | val_0_rmse: 0.57524 | val_1_rmse: 0.58151 |  0:05:59s
epoch 92 | loss: 0.26428 | val_0_rmse: 0.53944 | val_1_rmse: 0.53509 |  0:06:03s
epoch 93 | loss: 0.26257 | val_0_rmse: 0.74977 | val_1_rmse: 0.75652 |  0:06:06s
epoch 94 | loss: 0.2606  | val_0_rmse: 0.6138  | val_1_rmse: 0.60893 |  0:06:10s
epoch 95 | loss: 0.26249 | val_0_rmse: 0.51756 | val_1_rmse: 0.51561 |  0:06:14s
epoch 96 | loss: 0.25837 | val_0_rmse: 0.56559 | val_1_rmse: 0.56735 |  0:06:18s
epoch 97 | loss: 0.26013 | val_0_rmse: 0.53514 | val_1_rmse: 0.53214 |  0:06:22s
epoch 98 | loss: 0.25928 | val_0_rmse: 0.54653 | val_1_rmse: 0.54116 |  0:06:26s
epoch 99 | loss: 0.25822 | val_0_rmse: 0.66957 | val_1_rmse: 0.67726 |  0:06:29s
epoch 100| loss: 0.25802 | val_0_rmse: 0.58771 | val_1_rmse: 0.58219 |  0:06:33s
epoch 101| loss: 0.25629 | val_0_rmse: 0.57524 | val_1_rmse: 0.56961 |  0:06:37s
epoch 102| loss: 0.25745 | val_0_rmse: 0.62304 | val_1_rmse: 0.61798 |  0:06:41s
epoch 103| loss: 0.25376 | val_0_rmse: 0.52057 | val_1_rmse: 0.51712 |  0:06:45s
epoch 104| loss: 0.25568 | val_0_rmse: 0.51183 | val_1_rmse: 0.51298 |  0:06:49s
epoch 105| loss: 0.2511  | val_0_rmse: 0.54625 | val_1_rmse: 0.5428  |  0:06:53s
epoch 106| loss: 0.25365 | val_0_rmse: 0.52657 | val_1_rmse: 0.53062 |  0:06:57s
epoch 107| loss: 0.25336 | val_0_rmse: 0.55724 | val_1_rmse: 0.55066 |  0:07:00s
epoch 108| loss: 0.25103 | val_0_rmse: 0.55673 | val_1_rmse: 0.55679 |  0:07:04s
epoch 109| loss: 0.25289 | val_0_rmse: 0.49632 | val_1_rmse: 0.4939  |  0:07:08s
epoch 110| loss: 0.25263 | val_0_rmse: 0.58168 | val_1_rmse: 0.57535 |  0:07:12s
epoch 111| loss: 0.25017 | val_0_rmse: 0.52871 | val_1_rmse: 0.52833 |  0:07:16s
epoch 112| loss: 0.24817 | val_0_rmse: 0.69127 | val_1_rmse: 0.69279 |  0:07:20s
epoch 113| loss: 0.24868 | val_0_rmse: 0.53259 | val_1_rmse: 0.52778 |  0:07:24s
epoch 114| loss: 0.24833 | val_0_rmse: 0.54345 | val_1_rmse: 0.53849 |  0:07:27s
epoch 115| loss: 0.24904 | val_0_rmse: 0.57765 | val_1_rmse: 0.57299 |  0:07:31s
epoch 116| loss: 0.24925 | val_0_rmse: 0.60335 | val_1_rmse: 0.59737 |  0:07:35s
epoch 117| loss: 0.24886 | val_0_rmse: 0.59814 | val_1_rmse: 0.59236 |  0:07:39s
epoch 118| loss: 0.24825 | val_0_rmse: 0.56771 | val_1_rmse: 0.56516 |  0:07:43s
epoch 119| loss: 0.24938 | val_0_rmse: 0.63557 | val_1_rmse: 0.63112 |  0:07:47s
epoch 120| loss: 0.24644 | val_0_rmse: 0.54005 | val_1_rmse: 0.53647 |  0:07:51s
epoch 121| loss: 0.24949 | val_0_rmse: 0.50995 | val_1_rmse: 0.50683 |  0:07:54s
epoch 122| loss: 0.24705 | val_0_rmse: 0.52735 | val_1_rmse: 0.52428 |  0:07:58s
epoch 123| loss: 0.24674 | val_0_rmse: 0.5946  | val_1_rmse: 0.58757 |  0:08:02s
epoch 124| loss: 0.25964 | val_0_rmse: 0.56405 | val_1_rmse: 0.55663 |  0:08:06s
epoch 125| loss: 0.27821 | val_0_rmse: 0.52099 | val_1_rmse: 0.51993 |  0:08:10s
epoch 126| loss: 0.27127 | val_0_rmse: 0.51051 | val_1_rmse: 0.50822 |  0:08:14s
epoch 127| loss: 0.26705 | val_0_rmse: 0.53956 | val_1_rmse: 0.535   |  0:08:17s
epoch 128| loss: 0.26544 | val_0_rmse: 0.53401 | val_1_rmse: 0.53708 |  0:08:21s
epoch 129| loss: 0.26343 | val_0_rmse: 0.51301 | val_1_rmse: 0.50951 |  0:08:25s
epoch 130| loss: 0.26113 | val_0_rmse: 0.54404 | val_1_rmse: 0.53849 |  0:08:29s
epoch 131| loss: 0.25872 | val_0_rmse: 0.54643 | val_1_rmse: 0.54664 |  0:08:33s
epoch 132| loss: 0.25843 | val_0_rmse: 0.50759 | val_1_rmse: 0.50577 |  0:08:37s
epoch 133| loss: 0.25265 | val_0_rmse: 0.52966 | val_1_rmse: 0.52554 |  0:08:41s
epoch 134| loss: 0.24919 | val_0_rmse: 0.49594 | val_1_rmse: 0.49741 |  0:08:45s
epoch 135| loss: 0.24945 | val_0_rmse: 0.55791 | val_1_rmse: 0.55445 |  0:08:48s
epoch 136| loss: 0.24577 | val_0_rmse: 0.54971 | val_1_rmse: 0.5451  |  0:08:52s
epoch 137| loss: 0.24725 | val_0_rmse: 0.53406 | val_1_rmse: 0.53147 |  0:08:56s
epoch 138| loss: 0.24698 | val_0_rmse: 0.49683 | val_1_rmse: 0.499   |  0:09:00s
epoch 139| loss: 0.24973 | val_0_rmse: 0.49925 | val_1_rmse: 0.49979 |  0:09:04s

Early stopping occured at epoch 139 with best_epoch = 109 and best_val_1_rmse = 0.4939
Best weights from best epoch are automatically used!
ended training at: 17:06:12
Feature importance:
[('Area', 0.6315877023934018), ('Baths', 0.04232629853008673), ('Beds', 0.19089986683368046), ('Latitude', 0.09668055085439906), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.03850558138843201)]
Mean squared error is of 825070572.7936875
Mean absolute error:19566.75533823743
MAPE:0.3032033686226216
R2 score:0.750646046323026
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:06:13
epoch 0  | loss: 0.42214 | val_0_rmse: 0.5698  | val_1_rmse: 0.57175 |  0:00:03s
epoch 1  | loss: 0.33502 | val_0_rmse: 0.5717  | val_1_rmse: 0.57175 |  0:00:07s
epoch 2  | loss: 0.33153 | val_0_rmse: 0.57073 | val_1_rmse: 0.5734  |  0:00:11s
epoch 3  | loss: 0.32447 | val_0_rmse: 0.56734 | val_1_rmse: 0.57306 |  0:00:15s
epoch 4  | loss: 0.32218 | val_0_rmse: 0.56069 | val_1_rmse: 0.56399 |  0:00:19s
epoch 5  | loss: 0.31733 | val_0_rmse: 0.55248 | val_1_rmse: 0.55784 |  0:00:23s
epoch 6  | loss: 0.31518 | val_0_rmse: 0.55765 | val_1_rmse: 0.56413 |  0:00:26s
epoch 7  | loss: 0.31362 | val_0_rmse: 0.55799 | val_1_rmse: 0.56202 |  0:00:30s
epoch 8  | loss: 0.31229 | val_0_rmse: 0.55476 | val_1_rmse: 0.55769 |  0:00:34s
epoch 9  | loss: 0.31544 | val_0_rmse: 0.55425 | val_1_rmse: 0.55662 |  0:00:38s
epoch 10 | loss: 0.3139  | val_0_rmse: 0.57759 | val_1_rmse: 0.58213 |  0:00:42s
epoch 11 | loss: 0.31094 | val_0_rmse: 0.58077 | val_1_rmse: 0.57572 |  0:00:46s
epoch 12 | loss: 0.30605 | val_0_rmse: 0.99416 | val_1_rmse: 0.99138 |  0:00:50s
epoch 13 | loss: 0.30635 | val_0_rmse: 0.54799 | val_1_rmse: 0.55285 |  0:00:54s
epoch 14 | loss: 0.3059  | val_0_rmse: 0.54893 | val_1_rmse: 0.55416 |  0:00:57s
epoch 15 | loss: 0.30405 | val_0_rmse: 0.60442 | val_1_rmse: 0.58816 |  0:01:01s
epoch 16 | loss: 0.30276 | val_0_rmse: 0.5549  | val_1_rmse: 0.55012 |  0:01:05s
epoch 17 | loss: 0.30059 | val_0_rmse: 0.59461 | val_1_rmse: 0.59967 |  0:01:09s
epoch 18 | loss: 0.30477 | val_0_rmse: 0.58836 | val_1_rmse: 0.59231 |  0:01:13s
epoch 19 | loss: 0.29913 | val_0_rmse: 0.60848 | val_1_rmse: 0.59833 |  0:01:17s
epoch 20 | loss: 0.30009 | val_0_rmse: 0.56243 | val_1_rmse: 0.5663  |  0:01:21s
epoch 21 | loss: 0.29737 | val_0_rmse: 0.60417 | val_1_rmse: 0.56221 |  0:01:25s
epoch 22 | loss: 0.29682 | val_0_rmse: 0.57865 | val_1_rmse: 0.57712 |  0:01:28s
epoch 23 | loss: 0.30171 | val_0_rmse: 0.63331 | val_1_rmse: 0.5949  |  0:01:32s
epoch 24 | loss: 0.29827 | val_0_rmse: 0.57189 | val_1_rmse: 0.57628 |  0:01:36s
epoch 25 | loss: 0.29545 | val_0_rmse: 0.60159 | val_1_rmse: 0.60695 |  0:01:40s
epoch 26 | loss: 0.29759 | val_0_rmse: 0.57843 | val_1_rmse: 0.583   |  0:01:44s
epoch 27 | loss: 0.29984 | val_0_rmse: 0.88297 | val_1_rmse: 0.87964 |  0:01:48s
epoch 28 | loss: 0.29851 | val_0_rmse: 0.55927 | val_1_rmse: 0.5631  |  0:01:52s
epoch 29 | loss: 0.29969 | val_0_rmse: 0.563   | val_1_rmse: 0.56693 |  0:01:55s
epoch 30 | loss: 0.31222 | val_0_rmse: 0.59275 | val_1_rmse: 0.59933 |  0:01:59s
epoch 31 | loss: 0.30688 | val_0_rmse: 0.58032 | val_1_rmse: 0.584   |  0:02:03s
epoch 32 | loss: 0.30403 | val_0_rmse: 0.5647  | val_1_rmse: 0.56984 |  0:02:07s
epoch 33 | loss: 0.29899 | val_0_rmse: 0.58872 | val_1_rmse: 0.59163 |  0:02:11s
epoch 34 | loss: 0.29914 | val_0_rmse: 0.57362 | val_1_rmse: 0.58208 |  0:02:15s
epoch 35 | loss: 0.29934 | val_0_rmse: 0.59373 | val_1_rmse: 0.59798 |  0:02:19s
epoch 36 | loss: 0.29936 | val_0_rmse: 0.55026 | val_1_rmse: 0.55119 |  0:02:22s
epoch 37 | loss: 0.31026 | val_0_rmse: 0.55605 | val_1_rmse: 0.55993 |  0:02:26s
epoch 38 | loss: 0.31139 | val_0_rmse: 0.58321 | val_1_rmse: 0.58928 |  0:02:30s
epoch 39 | loss: 0.3088  | val_0_rmse: 0.5962  | val_1_rmse: 0.60055 |  0:02:34s
epoch 40 | loss: 0.30701 | val_0_rmse: 0.55386 | val_1_rmse: 0.55714 |  0:02:38s
epoch 41 | loss: 0.30583 | val_0_rmse: 0.59706 | val_1_rmse: 0.6009  |  0:02:42s
epoch 42 | loss: 0.30949 | val_0_rmse: 0.57396 | val_1_rmse: 0.57679 |  0:02:46s
epoch 43 | loss: 0.30803 | val_0_rmse: 0.58788 | val_1_rmse: 0.59226 |  0:02:49s
epoch 44 | loss: 0.30493 | val_0_rmse: 0.56185 | val_1_rmse: 0.56502 |  0:02:53s
epoch 45 | loss: 0.30401 | val_0_rmse: 0.59045 | val_1_rmse: 0.59422 |  0:02:57s
epoch 46 | loss: 0.3024  | val_0_rmse: 0.5786  | val_1_rmse: 0.58251 |  0:03:01s

Early stopping occured at epoch 46 with best_epoch = 16 and best_val_1_rmse = 0.55012
Best weights from best epoch are automatically used!
ended training at: 17:09:16
Feature importance:
[('Area', 0.5125935155328566), ('Baths', 0.060543504106715315), ('Beds', 0.13085734523702788), ('Latitude', 0.041128874786343646), ('Longitude', 0.17192181993972616), ('Month', 0.03680902544083457), ('Year', 0.046145914956495844)]
Mean squared error is of 1028506059.2177591
Mean absolute error:22100.409611784933
MAPE:0.37614444449270684
R2 score:0.691890952546934
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:09:17
epoch 0  | loss: 0.4495  | val_0_rmse: 0.59891 | val_1_rmse: 0.59785 |  0:00:03s
epoch 1  | loss: 0.34607 | val_0_rmse: 0.57314 | val_1_rmse: 0.57308 |  0:00:07s
epoch 2  | loss: 0.33362 | val_0_rmse: 0.5715  | val_1_rmse: 0.57146 |  0:00:11s
epoch 3  | loss: 0.33455 | val_0_rmse: 0.5839  | val_1_rmse: 0.58747 |  0:00:15s
epoch 4  | loss: 0.34315 | val_0_rmse: 0.57168 | val_1_rmse: 0.57271 |  0:00:19s
epoch 5  | loss: 0.33299 | val_0_rmse: 0.56887 | val_1_rmse: 0.56958 |  0:00:23s
epoch 6  | loss: 0.33187 | val_0_rmse: 0.57554 | val_1_rmse: 0.5739  |  0:00:27s
epoch 7  | loss: 0.33022 | val_0_rmse: 0.56542 | val_1_rmse: 0.56729 |  0:00:31s
epoch 8  | loss: 0.32712 | val_0_rmse: 0.56738 | val_1_rmse: 0.56897 |  0:00:35s
epoch 9  | loss: 0.32526 | val_0_rmse: 0.56224 | val_1_rmse: 0.56423 |  0:00:38s
epoch 10 | loss: 0.32216 | val_0_rmse: 0.56103 | val_1_rmse: 0.56169 |  0:00:42s
epoch 11 | loss: 0.32363 | val_0_rmse: 0.5608  | val_1_rmse: 0.56163 |  0:00:46s
epoch 12 | loss: 0.32088 | val_0_rmse: 0.56237 | val_1_rmse: 0.56368 |  0:00:50s
epoch 13 | loss: 0.32288 | val_0_rmse: 0.56287 | val_1_rmse: 0.56163 |  0:00:54s
epoch 14 | loss: 0.32295 | val_0_rmse: 0.56367 | val_1_rmse: 0.56413 |  0:00:58s
epoch 15 | loss: 0.31964 | val_0_rmse: 0.55991 | val_1_rmse: 0.56243 |  0:01:02s
epoch 16 | loss: 0.32356 | val_0_rmse: 0.55867 | val_1_rmse: 0.56202 |  0:01:06s
epoch 17 | loss: 0.31838 | val_0_rmse: 0.55975 | val_1_rmse: 0.56172 |  0:01:10s
epoch 18 | loss: 0.31894 | val_0_rmse: 0.5579  | val_1_rmse: 0.55894 |  0:01:13s
epoch 19 | loss: 0.31756 | val_0_rmse: 0.56103 | val_1_rmse: 0.56347 |  0:01:17s
epoch 20 | loss: 0.32    | val_0_rmse: 0.55744 | val_1_rmse: 0.55888 |  0:01:21s
epoch 21 | loss: 0.31817 | val_0_rmse: 0.56195 | val_1_rmse: 0.56654 |  0:01:25s
epoch 22 | loss: 0.32094 | val_0_rmse: 0.56285 | val_1_rmse: 0.56388 |  0:01:29s
epoch 23 | loss: 0.3217  | val_0_rmse: 0.56822 | val_1_rmse: 0.57223 |  0:01:33s
epoch 24 | loss: 0.32004 | val_0_rmse: 0.55866 | val_1_rmse: 0.56174 |  0:01:37s
epoch 25 | loss: 0.32262 | val_0_rmse: 0.56119 | val_1_rmse: 0.56342 |  0:01:41s
epoch 26 | loss: 0.3206  | val_0_rmse: 0.56756 | val_1_rmse: 0.56941 |  0:01:44s
epoch 27 | loss: 0.32383 | val_0_rmse: 0.56338 | val_1_rmse: 0.56677 |  0:01:48s
epoch 28 | loss: 0.32392 | val_0_rmse: 0.57144 | val_1_rmse: 0.57191 |  0:01:52s
epoch 29 | loss: 0.32435 | val_0_rmse: 0.56285 | val_1_rmse: 0.5647  |  0:01:56s
epoch 30 | loss: 0.32238 | val_0_rmse: 0.56199 | val_1_rmse: 0.56326 |  0:02:00s
epoch 31 | loss: 0.31799 | val_0_rmse: 0.56224 | val_1_rmse: 0.56544 |  0:02:04s
epoch 32 | loss: 0.32027 | val_0_rmse: 0.55883 | val_1_rmse: 0.56094 |  0:02:08s
epoch 33 | loss: 0.31892 | val_0_rmse: 0.55848 | val_1_rmse: 0.56007 |  0:02:12s
epoch 34 | loss: 0.31626 | val_0_rmse: 0.55672 | val_1_rmse: 0.55923 |  0:02:15s
epoch 35 | loss: 0.31583 | val_0_rmse: 0.55612 | val_1_rmse: 0.55788 |  0:02:19s
epoch 36 | loss: 0.31473 | val_0_rmse: 0.55851 | val_1_rmse: 0.56176 |  0:02:23s
epoch 37 | loss: 0.31666 | val_0_rmse: 0.55843 | val_1_rmse: 0.56144 |  0:02:27s
epoch 38 | loss: 0.31786 | val_0_rmse: 0.55606 | val_1_rmse: 0.55964 |  0:02:31s
epoch 39 | loss: 0.31485 | val_0_rmse: 0.55743 | val_1_rmse: 0.55925 |  0:02:35s
epoch 40 | loss: 0.31514 | val_0_rmse: 0.55905 | val_1_rmse: 0.56155 |  0:02:39s
epoch 41 | loss: 0.31823 | val_0_rmse: 0.56145 | val_1_rmse: 0.56421 |  0:02:43s
epoch 42 | loss: 0.31666 | val_0_rmse: 0.5628  | val_1_rmse: 0.56579 |  0:02:46s
epoch 43 | loss: 0.31589 | val_0_rmse: 0.56011 | val_1_rmse: 0.5605  |  0:02:50s
epoch 44 | loss: 0.31557 | val_0_rmse: 0.55813 | val_1_rmse: 0.55963 |  0:02:54s
epoch 45 | loss: 0.31654 | val_0_rmse: 0.55895 | val_1_rmse: 0.56245 |  0:02:58s
epoch 46 | loss: 0.3178  | val_0_rmse: 0.55978 | val_1_rmse: 0.56255 |  0:03:02s
epoch 47 | loss: 0.31908 | val_0_rmse: 0.55703 | val_1_rmse: 0.55986 |  0:03:06s
epoch 48 | loss: 0.3162  | val_0_rmse: 0.55889 | val_1_rmse: 0.56037 |  0:03:10s
epoch 49 | loss: 0.31643 | val_0_rmse: 0.55879 | val_1_rmse: 0.56073 |  0:03:14s
epoch 50 | loss: 0.3168  | val_0_rmse: 0.56451 | val_1_rmse: 0.56646 |  0:03:17s
epoch 51 | loss: 0.31869 | val_0_rmse: 0.55908 | val_1_rmse: 0.56034 |  0:03:21s
epoch 52 | loss: 0.32007 | val_0_rmse: 0.56053 | val_1_rmse: 0.56083 |  0:03:25s
epoch 53 | loss: 0.31733 | val_0_rmse: 0.56095 | val_1_rmse: 0.56319 |  0:03:29s
epoch 54 | loss: 0.31926 | val_0_rmse: 0.55682 | val_1_rmse: 0.5578  |  0:03:33s
epoch 55 | loss: 0.31971 | val_0_rmse: 0.56178 | val_1_rmse: 0.56135 |  0:03:37s
epoch 56 | loss: 0.3173  | val_0_rmse: 0.55813 | val_1_rmse: 0.5599  |  0:03:41s
epoch 57 | loss: 0.31757 | val_0_rmse: 0.55919 | val_1_rmse: 0.56045 |  0:03:45s
epoch 58 | loss: 0.31501 | val_0_rmse: 0.55442 | val_1_rmse: 0.55658 |  0:03:48s
epoch 59 | loss: 0.31382 | val_0_rmse: 0.55518 | val_1_rmse: 0.55609 |  0:03:52s
epoch 60 | loss: 0.31675 | val_0_rmse: 0.56421 | val_1_rmse: 0.56316 |  0:03:56s
epoch 61 | loss: 0.32114 | val_0_rmse: 0.56443 | val_1_rmse: 0.56524 |  0:04:00s
epoch 62 | loss: 0.31871 | val_0_rmse: 0.55767 | val_1_rmse: 0.55912 |  0:04:04s
epoch 63 | loss: 0.31788 | val_0_rmse: 0.55847 | val_1_rmse: 0.5582  |  0:04:08s
epoch 64 | loss: 0.31599 | val_0_rmse: 0.56093 | val_1_rmse: 0.56208 |  0:04:12s
epoch 65 | loss: 0.31607 | val_0_rmse: 0.55813 | val_1_rmse: 0.55964 |  0:04:16s
epoch 66 | loss: 0.3159  | val_0_rmse: 0.55816 | val_1_rmse: 0.55746 |  0:04:19s
epoch 67 | loss: 0.31436 | val_0_rmse: 0.55904 | val_1_rmse: 0.56175 |  0:04:23s
epoch 68 | loss: 0.31621 | val_0_rmse: 0.5568  | val_1_rmse: 0.55607 |  0:04:27s
epoch 69 | loss: 0.31607 | val_0_rmse: 0.557   | val_1_rmse: 0.55732 |  0:04:31s
epoch 70 | loss: 0.31732 | val_0_rmse: 0.55425 | val_1_rmse: 0.55563 |  0:04:35s
epoch 71 | loss: 0.31317 | val_0_rmse: 0.55692 | val_1_rmse: 0.55826 |  0:04:39s
epoch 72 | loss: 0.31446 | val_0_rmse: 0.55581 | val_1_rmse: 0.55639 |  0:04:43s
epoch 73 | loss: 0.3114  | val_0_rmse: 0.55301 | val_1_rmse: 0.55455 |  0:04:47s
epoch 74 | loss: 0.31212 | val_0_rmse: 0.55715 | val_1_rmse: 0.55859 |  0:04:50s
epoch 75 | loss: 0.31512 | val_0_rmse: 0.55679 | val_1_rmse: 0.55697 |  0:04:54s
epoch 76 | loss: 0.31535 | val_0_rmse: 0.55665 | val_1_rmse: 0.55711 |  0:04:58s
epoch 77 | loss: 0.31225 | val_0_rmse: 0.55387 | val_1_rmse: 0.55473 |  0:05:02s
epoch 78 | loss: 0.31111 | val_0_rmse: 0.55777 | val_1_rmse: 0.55825 |  0:05:06s
epoch 79 | loss: 0.31404 | val_0_rmse: 0.55857 | val_1_rmse: 0.55855 |  0:05:10s
epoch 80 | loss: 0.31426 | val_0_rmse: 0.55505 | val_1_rmse: 0.55499 |  0:05:14s
epoch 81 | loss: 0.31567 | val_0_rmse: 0.55744 | val_1_rmse: 0.55725 |  0:05:18s
epoch 82 | loss: 0.31374 | val_0_rmse: 0.55333 | val_1_rmse: 0.55583 |  0:05:21s
epoch 83 | loss: 0.31257 | val_0_rmse: 0.5574  | val_1_rmse: 0.5576  |  0:05:25s
epoch 84 | loss: 0.31188 | val_0_rmse: 0.54996 | val_1_rmse: 0.54964 |  0:05:29s
epoch 85 | loss: 0.35357 | val_0_rmse: 0.58334 | val_1_rmse: 0.57789 |  0:05:33s
epoch 86 | loss: 0.34947 | val_0_rmse: 0.58415 | val_1_rmse: 0.58183 |  0:05:37s
epoch 87 | loss: 0.35045 | val_0_rmse: 0.59126 | val_1_rmse: 0.58794 |  0:05:41s
epoch 88 | loss: 0.34801 | val_0_rmse: 0.58374 | val_1_rmse: 0.58084 |  0:05:45s
epoch 89 | loss: 0.34648 | val_0_rmse: 0.58471 | val_1_rmse: 0.5806  |  0:05:49s
epoch 90 | loss: 0.35215 | val_0_rmse: 0.59506 | val_1_rmse: 0.59894 |  0:05:52s
epoch 91 | loss: 0.36623 | val_0_rmse: 0.59831 | val_1_rmse: 0.59941 |  0:05:56s
epoch 92 | loss: 0.36293 | val_0_rmse: 0.59023 | val_1_rmse: 0.59542 |  0:06:00s
epoch 93 | loss: 0.36153 | val_0_rmse: 0.60212 | val_1_rmse: 0.60934 |  0:06:04s
epoch 94 | loss: 0.36315 | val_0_rmse: 0.6168  | val_1_rmse: 0.6168  |  0:06:08s
epoch 95 | loss: 0.35906 | val_0_rmse: 0.61525 | val_1_rmse: 0.61935 |  0:06:12s
epoch 96 | loss: 0.35565 | val_0_rmse: 0.58687 | val_1_rmse: 0.59729 |  0:06:15s
epoch 97 | loss: 0.35083 | val_0_rmse: 0.60102 | val_1_rmse: 0.61549 |  0:06:19s
epoch 98 | loss: 0.34997 | val_0_rmse: 0.59278 | val_1_rmse: 0.64323 |  0:06:23s
epoch 99 | loss: 0.34627 | val_0_rmse: 0.58599 | val_1_rmse: 0.62328 |  0:06:27s
epoch 100| loss: 0.34127 | val_0_rmse: 0.63106 | val_1_rmse: 0.85537 |  0:06:31s
epoch 101| loss: 0.33969 | val_0_rmse: 0.63455 | val_1_rmse: 0.94231 |  0:06:35s
epoch 102| loss: 0.33699 | val_0_rmse: 0.58706 | val_1_rmse: 1.21055 |  0:06:39s
epoch 103| loss: 0.33645 | val_0_rmse: 0.60405 | val_1_rmse: 1.08042 |  0:06:43s
epoch 104| loss: 0.33571 | val_0_rmse: 0.62453 | val_1_rmse: 1.02784 |  0:06:46s
epoch 105| loss: 0.33423 | val_0_rmse: 0.61321 | val_1_rmse: 1.00273 |  0:06:50s
epoch 106| loss: 0.33368 | val_0_rmse: 0.58416 | val_1_rmse: 1.06889 |  0:06:54s
epoch 107| loss: 0.33183 | val_0_rmse: 0.67373 | val_1_rmse: 0.92221 |  0:06:58s
epoch 108| loss: 0.33248 | val_0_rmse: 0.58761 | val_1_rmse: 0.8367  |  0:07:02s
epoch 109| loss: 0.33066 | val_0_rmse: 0.57395 | val_1_rmse: 0.72661 |  0:07:06s
epoch 110| loss: 0.33106 | val_0_rmse: 0.59705 | val_1_rmse: 0.76264 |  0:07:10s
epoch 111| loss: 0.32889 | val_0_rmse: 0.57074 | val_1_rmse: 0.66936 |  0:07:14s
epoch 112| loss: 0.32907 | val_0_rmse: 0.57258 | val_1_rmse: 0.65883 |  0:07:17s
epoch 113| loss: 0.33062 | val_0_rmse: 0.59347 | val_1_rmse: 0.65882 |  0:07:22s
epoch 114| loss: 0.33031 | val_0_rmse: 0.61086 | val_1_rmse: 0.69213 |  0:07:25s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.54964
Best weights from best epoch are automatically used!
ended training at: 17:16:44
Feature importance:
[('Area', 0.6420904976915163), ('Baths', 0.05490844491354056), ('Beds', 0.17360302517647452), ('Latitude', 0.0416216772944958), ('Longitude', 0.013033032420205166), ('Month', 1.2647796012455298e-05), ('Year', 0.07473067470775524)]
Mean squared error is of 999916612.0697079
Mean absolute error:21967.172585456712
MAPE:0.3535352889775189
R2 score:0.700311669165258
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:16:45
epoch 0  | loss: 0.41891 | val_0_rmse: 0.57874 | val_1_rmse: 0.58105 |  0:00:03s
epoch 1  | loss: 0.33973 | val_0_rmse: 0.59152 | val_1_rmse: 0.59265 |  0:00:07s
epoch 2  | loss: 0.3375  | val_0_rmse: 0.57504 | val_1_rmse: 0.57651 |  0:00:11s
epoch 3  | loss: 0.33121 | val_0_rmse: 0.57944 | val_1_rmse: 0.57942 |  0:00:15s
epoch 4  | loss: 0.33125 | val_0_rmse: 0.56834 | val_1_rmse: 0.56751 |  0:00:19s
epoch 5  | loss: 0.33142 | val_0_rmse: 0.56972 | val_1_rmse: 0.56925 |  0:00:23s
epoch 6  | loss: 0.32932 | val_0_rmse: 0.56689 | val_1_rmse: 0.56654 |  0:00:27s
epoch 7  | loss: 0.3256  | val_0_rmse: 0.56608 | val_1_rmse: 0.56539 |  0:00:30s
epoch 8  | loss: 0.32525 | val_0_rmse: 0.56705 | val_1_rmse: 0.56609 |  0:00:34s
epoch 9  | loss: 0.32697 | val_0_rmse: 0.56157 | val_1_rmse: 0.56172 |  0:00:38s
epoch 10 | loss: 0.3247  | val_0_rmse: 0.59055 | val_1_rmse: 0.57632 |  0:00:42s
epoch 11 | loss: 0.32501 | val_0_rmse: 0.56817 | val_1_rmse: 0.56767 |  0:00:46s
epoch 12 | loss: 0.32491 | val_0_rmse: 0.57039 | val_1_rmse: 0.57059 |  0:00:50s
epoch 13 | loss: 0.32741 | val_0_rmse: 0.58005 | val_1_rmse: 0.5655  |  0:00:54s
epoch 14 | loss: 0.32236 | val_0_rmse: 0.57967 | val_1_rmse: 0.58107 |  0:00:57s
epoch 15 | loss: 0.31729 | val_0_rmse: 0.55229 | val_1_rmse: 0.55249 |  0:01:01s
epoch 16 | loss: 0.32059 | val_0_rmse: 0.58093 | val_1_rmse: 0.58201 |  0:01:05s
epoch 17 | loss: 0.31592 | val_0_rmse: 0.58796 | val_1_rmse: 0.58994 |  0:01:09s
epoch 18 | loss: 0.31329 | val_0_rmse: 0.55107 | val_1_rmse: 0.55315 |  0:01:13s
epoch 19 | loss: 0.31381 | val_0_rmse: 0.58176 | val_1_rmse: 0.58411 |  0:01:17s
epoch 20 | loss: 0.31454 | val_0_rmse: 0.56984 | val_1_rmse: 0.57181 |  0:01:21s
epoch 21 | loss: 0.31283 | val_0_rmse: 0.63211 | val_1_rmse: 0.63843 |  0:01:25s
epoch 22 | loss: 0.30986 | val_0_rmse: 0.55401 | val_1_rmse: 0.5564  |  0:01:28s
epoch 23 | loss: 0.3121  | val_0_rmse: 0.55186 | val_1_rmse: 0.55326 |  0:01:32s
epoch 24 | loss: 0.31439 | val_0_rmse: 0.58351 | val_1_rmse: 0.58718 |  0:01:36s
epoch 25 | loss: 0.31022 | val_0_rmse: 0.57795 | val_1_rmse: 0.57994 |  0:01:40s
epoch 26 | loss: 0.30875 | val_0_rmse: 0.57725 | val_1_rmse: 0.5804  |  0:01:44s
epoch 27 | loss: 0.30806 | val_0_rmse: 0.58444 | val_1_rmse: 0.5866  |  0:01:48s
epoch 28 | loss: 0.31307 | val_0_rmse: 0.56776 | val_1_rmse: 0.56916 |  0:01:52s
epoch 29 | loss: 0.31631 | val_0_rmse: 0.55224 | val_1_rmse: 0.5556  |  0:01:55s
epoch 30 | loss: 0.3072  | val_0_rmse: 0.55454 | val_1_rmse: 0.5565  |  0:01:59s
epoch 31 | loss: 0.30723 | val_0_rmse: 0.5555  | val_1_rmse: 0.55793 |  0:02:03s
epoch 32 | loss: 0.31688 | val_0_rmse: 0.57753 | val_1_rmse: 0.5768  |  0:02:07s
epoch 33 | loss: 0.3126  | val_0_rmse: 0.57527 | val_1_rmse: 0.57593 |  0:02:11s
epoch 34 | loss: 0.31239 | val_0_rmse: 0.60277 | val_1_rmse: 0.60207 |  0:02:15s
epoch 35 | loss: 0.30709 | val_0_rmse: 0.55426 | val_1_rmse: 0.55674 |  0:02:19s
epoch 36 | loss: 0.30566 | val_0_rmse: 0.54985 | val_1_rmse: 0.55103 |  0:02:23s
epoch 37 | loss: 0.30639 | val_0_rmse: 0.55375 | val_1_rmse: 0.55393 |  0:02:26s
epoch 38 | loss: 0.30377 | val_0_rmse: 0.57446 | val_1_rmse: 0.56981 |  0:02:30s
epoch 39 | loss: 0.30332 | val_0_rmse: 0.56659 | val_1_rmse: 0.56544 |  0:02:34s
epoch 40 | loss: 0.30233 | val_0_rmse: 0.56824 | val_1_rmse: 0.56947 |  0:02:38s
epoch 41 | loss: 0.30231 | val_0_rmse: 0.54696 | val_1_rmse: 0.55006 |  0:02:42s
epoch 42 | loss: 0.30202 | val_0_rmse: 0.63356 | val_1_rmse: 0.60612 |  0:02:46s
epoch 43 | loss: 0.30145 | val_0_rmse: 0.57886 | val_1_rmse: 0.58086 |  0:02:49s
epoch 44 | loss: 0.30004 | val_0_rmse: 0.59066 | val_1_rmse: 0.59376 |  0:02:53s
epoch 45 | loss: 0.30052 | val_0_rmse: 0.54562 | val_1_rmse: 0.54694 |  0:02:57s
epoch 46 | loss: 0.30021 | val_0_rmse: 0.57384 | val_1_rmse: 0.57469 |  0:03:01s
epoch 47 | loss: 0.29841 | val_0_rmse: 0.5665  | val_1_rmse: 0.55643 |  0:03:05s
epoch 48 | loss: 0.2977  | val_0_rmse: 0.54365 | val_1_rmse: 0.54539 |  0:03:09s
epoch 49 | loss: 0.29744 | val_0_rmse: 0.54165 | val_1_rmse: 0.54279 |  0:03:13s
epoch 50 | loss: 0.29824 | val_0_rmse: 0.58837 | val_1_rmse: 0.59327 |  0:03:17s
epoch 51 | loss: 0.30026 | val_0_rmse: 0.59428 | val_1_rmse: 0.59884 |  0:03:20s
epoch 52 | loss: 0.29632 | val_0_rmse: 0.54507 | val_1_rmse: 0.54901 |  0:03:24s
epoch 53 | loss: 0.29663 | val_0_rmse: 0.55858 | val_1_rmse: 0.55982 |  0:03:28s
epoch 54 | loss: 0.29728 | val_0_rmse: 0.61569 | val_1_rmse: 0.5701  |  0:03:32s
epoch 55 | loss: 0.29609 | val_0_rmse: 0.58177 | val_1_rmse: 0.57038 |  0:03:36s
epoch 56 | loss: 0.29693 | val_0_rmse: 0.60512 | val_1_rmse: 0.6092  |  0:03:40s
epoch 57 | loss: 0.29563 | val_0_rmse: 0.59761 | val_1_rmse: 0.56196 |  0:03:44s
epoch 58 | loss: 0.29593 | val_0_rmse: 0.67698 | val_1_rmse: 0.55332 |  0:03:48s
epoch 59 | loss: 0.29533 | val_0_rmse: 0.95529 | val_1_rmse: 0.54639 |  0:03:51s
epoch 60 | loss: 0.29541 | val_0_rmse: 0.57045 | val_1_rmse: 0.57318 |  0:03:55s
epoch 61 | loss: 0.29707 | val_0_rmse: 0.68784 | val_1_rmse: 0.55599 |  0:03:59s
epoch 62 | loss: 0.29661 | val_0_rmse: 0.55962 | val_1_rmse: 0.55972 |  0:04:03s
epoch 63 | loss: 0.29605 | val_0_rmse: 0.56048 | val_1_rmse: 0.56148 |  0:04:07s
epoch 64 | loss: 0.29894 | val_0_rmse: 0.58106 | val_1_rmse: 0.57951 |  0:04:11s
epoch 65 | loss: 0.29798 | val_0_rmse: 0.5638  | val_1_rmse: 0.56557 |  0:04:15s
epoch 66 | loss: 0.29557 | val_0_rmse: 0.54734 | val_1_rmse: 0.5492  |  0:04:19s
epoch 67 | loss: 0.29519 | val_0_rmse: 0.54346 | val_1_rmse: 0.54494 |  0:04:22s
epoch 68 | loss: 0.29285 | val_0_rmse: 0.54027 | val_1_rmse: 0.542   |  0:04:26s
epoch 69 | loss: 0.29469 | val_0_rmse: 0.56044 | val_1_rmse: 0.5619  |  0:04:30s
epoch 70 | loss: 0.29375 | val_0_rmse: 0.56024 | val_1_rmse: 0.56278 |  0:04:34s
epoch 71 | loss: 0.29332 | val_0_rmse: 0.55627 | val_1_rmse: 0.55866 |  0:04:38s
epoch 72 | loss: 0.2927  | val_0_rmse: 0.56586 | val_1_rmse: 0.56846 |  0:04:42s
epoch 73 | loss: 0.2962  | val_0_rmse: 0.57336 | val_1_rmse: 0.57663 |  0:04:46s
epoch 74 | loss: 0.29516 | val_0_rmse: 0.56188 | val_1_rmse: 0.56229 |  0:04:50s
epoch 75 | loss: 0.29259 | val_0_rmse: 0.54136 | val_1_rmse: 0.5439  |  0:04:53s
epoch 76 | loss: 0.29288 | val_0_rmse: 0.5548  | val_1_rmse: 0.557   |  0:04:57s
epoch 77 | loss: 0.29227 | val_0_rmse: 0.5512  | val_1_rmse: 0.55321 |  0:05:01s
epoch 78 | loss: 0.29097 | val_0_rmse: 0.583   | val_1_rmse: 0.58721 |  0:05:05s
epoch 79 | loss: 0.28997 | val_0_rmse: 0.56444 | val_1_rmse: 0.56767 |  0:05:09s
epoch 80 | loss: 0.28972 | val_0_rmse: 0.5818  | val_1_rmse: 0.58505 |  0:05:13s
epoch 81 | loss: 0.287   | val_0_rmse: 0.58066 | val_1_rmse: 0.58357 |  0:05:17s
epoch 82 | loss: 0.2889  | val_0_rmse: 0.62185 | val_1_rmse: 0.62778 |  0:05:20s
epoch 83 | loss: 0.28857 | val_0_rmse: 0.56836 | val_1_rmse: 0.57284 |  0:05:24s
epoch 84 | loss: 0.28836 | val_0_rmse: 0.55745 | val_1_rmse: 0.56046 |  0:05:28s
epoch 85 | loss: 0.28727 | val_0_rmse: 0.55819 | val_1_rmse: 0.56262 |  0:05:32s
epoch 86 | loss: 0.2868  | val_0_rmse: 0.56882 | val_1_rmse: 0.57194 |  0:05:36s
epoch 87 | loss: 0.28555 | val_0_rmse: 0.5661  | val_1_rmse: 0.57002 |  0:05:40s
epoch 88 | loss: 0.28807 | val_0_rmse: 0.57457 | val_1_rmse: 0.57638 |  0:05:44s
epoch 89 | loss: 0.29436 | val_0_rmse: 0.59942 | val_1_rmse: 0.60129 |  0:05:48s
epoch 90 | loss: 0.30875 | val_0_rmse: 0.56649 | val_1_rmse: 0.56784 |  0:05:52s
epoch 91 | loss: 0.29914 | val_0_rmse: 0.55967 | val_1_rmse: 0.56111 |  0:05:56s
epoch 92 | loss: 0.29653 | val_0_rmse: 0.67002 | val_1_rmse: 0.67195 |  0:06:00s
epoch 93 | loss: 0.2943  | val_0_rmse: 0.62127 | val_1_rmse: 0.62566 |  0:06:03s
epoch 94 | loss: 0.29274 | val_0_rmse: 0.59007 | val_1_rmse: 0.59468 |  0:06:07s
epoch 95 | loss: 0.28901 | val_0_rmse: 0.56968 | val_1_rmse: 0.57212 |  0:06:11s
epoch 96 | loss: 0.2897  | val_0_rmse: 0.56695 | val_1_rmse: 0.56937 |  0:06:15s
epoch 97 | loss: 0.29101 | val_0_rmse: 0.56942 | val_1_rmse: 0.57228 |  0:06:19s
epoch 98 | loss: 0.29106 | val_0_rmse: 0.58985 | val_1_rmse: 0.59182 |  0:06:23s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.542
Best weights from best epoch are automatically used!
ended training at: 17:23:09
Feature importance:
[('Area', 0.4374295230359429), ('Baths', 0.021838990671110642), ('Beds', 0.3125088344540501), ('Latitude', 0.0), ('Longitude', 0.10319376675235675), ('Month', 0.008965318442173765), ('Year', 0.11606356664436582)]
Mean squared error is of 993055163.588205
Mean absolute error:21515.41249433786
MAPE:0.35462154642530375
R2 score:0.7023365638987764
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 17:23:10
epoch 0  | loss: 0.35172 | val_0_rmse: 0.95157 | val_1_rmse: 0.95291 |  0:00:24s
epoch 1  | loss: 0.30377 | val_0_rmse: 0.71778 | val_1_rmse: 0.71731 |  0:00:49s
epoch 2  | loss: 0.30086 | val_0_rmse: 0.54945 | val_1_rmse: 0.54817 |  0:01:14s
epoch 3  | loss: 0.29841 | val_0_rmse: 0.53851 | val_1_rmse: 0.53715 |  0:01:38s
epoch 4  | loss: 0.29999 | val_0_rmse: 0.58125 | val_1_rmse: 0.58082 |  0:02:03s
epoch 5  | loss: 0.30285 | val_0_rmse: 0.53414 | val_1_rmse: 0.53278 |  0:02:28s
epoch 6  | loss: 0.29231 | val_0_rmse: 0.53412 | val_1_rmse: 0.53152 |  0:02:53s
epoch 7  | loss: 0.31243 | val_0_rmse: 0.55824 | val_1_rmse: 0.55837 |  0:03:17s
epoch 8  | loss: 0.31814 | val_0_rmse: 0.56219 | val_1_rmse: 0.56088 |  0:03:42s
epoch 9  | loss: 0.30113 | val_0_rmse: 0.53984 | val_1_rmse: 0.53944 |  0:04:07s
epoch 10 | loss: 0.29545 | val_0_rmse: 0.53756 | val_1_rmse: 0.53711 |  0:04:31s
epoch 11 | loss: 0.29215 | val_0_rmse: 0.53879 | val_1_rmse: 0.53733 |  0:04:56s
epoch 12 | loss: 0.29319 | val_0_rmse: 0.54191 | val_1_rmse: 0.54137 |  0:05:21s
epoch 13 | loss: 0.2954  | val_0_rmse: 0.54902 | val_1_rmse: 0.54887 |  0:05:46s
epoch 14 | loss: 0.2901  | val_0_rmse: 0.53211 | val_1_rmse: 0.53209 |  0:06:10s
epoch 15 | loss: 0.28721 | val_0_rmse: 0.5293  | val_1_rmse: 0.5289  |  0:06:35s
epoch 16 | loss: 0.29298 | val_0_rmse: 0.60073 | val_1_rmse: 0.59936 |  0:07:00s
epoch 17 | loss: 0.29022 | val_0_rmse: 0.53145 | val_1_rmse: 0.53039 |  0:07:25s
epoch 18 | loss: 0.28728 | val_0_rmse: 0.53737 | val_1_rmse: 0.53538 |  0:07:49s
epoch 19 | loss: 0.29716 | val_0_rmse: 0.54095 | val_1_rmse: 0.53986 |  0:08:14s
epoch 20 | loss: 0.28816 | val_0_rmse: 0.5312  | val_1_rmse: 0.53043 |  0:08:39s
epoch 21 | loss: 0.28685 | val_0_rmse: 0.53672 | val_1_rmse: 0.53639 |  0:09:04s
epoch 22 | loss: 0.31214 | val_0_rmse: 0.64533 | val_1_rmse: 0.6417  |  0:09:28s
epoch 23 | loss: 0.31849 | val_0_rmse: 0.60587 | val_1_rmse: 0.60482 |  0:09:53s
epoch 24 | loss: 0.29862 | val_0_rmse: 0.54189 | val_1_rmse: 0.54165 |  0:10:18s
epoch 25 | loss: 0.29771 | val_0_rmse: 0.53998 | val_1_rmse: 0.53924 |  0:10:43s
epoch 26 | loss: 0.29299 | val_0_rmse: 0.53912 | val_1_rmse: 0.53835 |  0:11:07s
epoch 27 | loss: 0.29195 | val_0_rmse: 0.54223 | val_1_rmse: 0.54154 |  0:11:32s
epoch 28 | loss: 0.31806 | val_0_rmse: 0.54019 | val_1_rmse: 0.53957 |  0:11:57s
epoch 29 | loss: 0.2974  | val_0_rmse: 0.54314 | val_1_rmse: 0.54229 |  0:12:21s
epoch 30 | loss: 0.29274 | val_0_rmse: 0.53488 | val_1_rmse: 0.53462 |  0:12:46s
epoch 31 | loss: 0.29007 | val_0_rmse: 0.61528 | val_1_rmse: 0.61338 |  0:13:11s
epoch 32 | loss: 0.28718 | val_0_rmse: 0.54014 | val_1_rmse: 0.54055 |  0:13:36s
epoch 33 | loss: 0.2848  | val_0_rmse: 0.62715 | val_1_rmse: 0.62373 |  0:14:00s
epoch 34 | loss: 0.28361 | val_0_rmse: 0.5678  | val_1_rmse: 0.56703 |  0:14:25s
epoch 35 | loss: 0.28314 | val_0_rmse: 0.55944 | val_1_rmse: 0.55799 |  0:14:50s
epoch 36 | loss: 0.28211 | val_0_rmse: 0.63198 | val_1_rmse: 0.62776 |  0:15:15s
epoch 37 | loss: 0.28235 | val_0_rmse: 0.5677  | val_1_rmse: 0.56591 |  0:15:39s
epoch 38 | loss: 0.2839  | val_0_rmse: 0.53579 | val_1_rmse: 0.53506 |  0:16:04s
epoch 39 | loss: 0.29232 | val_0_rmse: 0.61551 | val_1_rmse: 0.61239 |  0:16:29s
epoch 40 | loss: 0.28534 | val_0_rmse: 0.56338 | val_1_rmse: 0.56135 |  0:16:54s
epoch 41 | loss: 0.28459 | val_0_rmse: 0.65242 | val_1_rmse: 0.64788 |  0:17:18s
epoch 42 | loss: 0.28888 | val_0_rmse: 0.57196 | val_1_rmse: 0.57043 |  0:17:43s
epoch 43 | loss: 0.29903 | val_0_rmse: 0.58779 | val_1_rmse: 0.5856  |  0:18:08s
epoch 44 | loss: 0.29456 | val_0_rmse: 0.62835 | val_1_rmse: 0.62555 |  0:18:33s
epoch 45 | loss: 0.28748 | val_0_rmse: 0.52802 | val_1_rmse: 0.52675 |  0:18:57s
epoch 46 | loss: 0.28571 | val_0_rmse: 0.59072 | val_1_rmse: 0.5879  |  0:19:22s
epoch 47 | loss: 0.28352 | val_0_rmse: 0.63268 | val_1_rmse: 0.62999 |  0:19:47s
epoch 48 | loss: 0.28275 | val_0_rmse: 0.56152 | val_1_rmse: 0.55956 |  0:20:11s
epoch 49 | loss: 0.28098 | val_0_rmse: 0.53359 | val_1_rmse: 0.53246 |  0:20:36s
epoch 50 | loss: 0.27983 | val_0_rmse: 0.55105 | val_1_rmse: 0.54988 |  0:21:01s
epoch 51 | loss: 0.27898 | val_0_rmse: 0.64064 | val_1_rmse: 0.63775 |  0:21:25s
epoch 52 | loss: 0.27936 | val_0_rmse: 0.53104 | val_1_rmse: 0.531   |  0:21:50s
epoch 53 | loss: 0.27891 | val_0_rmse: 0.52509 | val_1_rmse: 0.52488 |  0:22:15s
epoch 54 | loss: 0.27822 | val_0_rmse: 0.52148 | val_1_rmse: 0.52147 |  0:22:40s
epoch 55 | loss: 0.27778 | val_0_rmse: 0.53853 | val_1_rmse: 0.53761 |  0:23:04s
epoch 56 | loss: 0.27729 | val_0_rmse: 0.626   | val_1_rmse: 0.62392 |  0:23:29s
epoch 57 | loss: 0.27641 | val_0_rmse: 0.53279 | val_1_rmse: 0.53215 |  0:23:54s
epoch 58 | loss: 0.27701 | val_0_rmse: 0.5597  | val_1_rmse: 0.55845 |  0:24:18s
epoch 59 | loss: 0.27657 | val_0_rmse: 0.54769 | val_1_rmse: 0.54704 |  0:24:43s
epoch 60 | loss: 0.27682 | val_0_rmse: 0.54498 | val_1_rmse: 0.5446  |  0:25:08s
epoch 61 | loss: 0.2771  | val_0_rmse: 0.52523 | val_1_rmse: 0.52411 |  0:25:33s
epoch 62 | loss: 0.2764  | val_0_rmse: 0.54247 | val_1_rmse: 0.54231 |  0:25:58s
epoch 63 | loss: 0.27722 | val_0_rmse: 0.51984 | val_1_rmse: 0.5191  |  0:26:23s
epoch 64 | loss: 0.27541 | val_0_rmse: 0.57251 | val_1_rmse: 0.57032 |  0:26:47s
epoch 65 | loss: 0.27605 | val_0_rmse: 0.55346 | val_1_rmse: 0.55197 |  0:27:12s
epoch 66 | loss: 0.27555 | val_0_rmse: 0.5242  | val_1_rmse: 0.52343 |  0:27:37s
epoch 67 | loss: 0.27516 | val_0_rmse: 0.5757  | val_1_rmse: 0.57332 |  0:28:02s
epoch 68 | loss: 0.27417 | val_0_rmse: 0.61784 | val_1_rmse: 0.61472 |  0:28:26s
epoch 69 | loss: 0.27512 | val_0_rmse: 0.52443 | val_1_rmse: 0.52346 |  0:28:51s
epoch 70 | loss: 0.27417 | val_0_rmse: 0.53561 | val_1_rmse: 0.53455 |  0:29:16s
epoch 71 | loss: 0.27455 | val_0_rmse: 0.51991 | val_1_rmse: 0.51944 |  0:29:40s
epoch 72 | loss: 0.27507 | val_0_rmse: 0.51729 | val_1_rmse: 0.51743 |  0:30:05s
epoch 73 | loss: 0.27491 | val_0_rmse: 0.52517 | val_1_rmse: 0.52467 |  0:30:30s
epoch 74 | loss: 0.27409 | val_0_rmse: 0.52437 | val_1_rmse: 0.52355 |  0:30:55s
epoch 75 | loss: 0.27368 | val_0_rmse: 0.52813 | val_1_rmse: 0.52756 |  0:31:20s
epoch 76 | loss: 0.27431 | val_0_rmse: 0.52193 | val_1_rmse: 0.52101 |  0:31:44s
epoch 77 | loss: 0.27364 | val_0_rmse: 0.52384 | val_1_rmse: 0.52236 |  0:32:09s
epoch 78 | loss: 0.27397 | val_0_rmse: 0.63042 | val_1_rmse: 0.62721 |  0:32:34s
epoch 79 | loss: 0.27353 | val_0_rmse: 0.52236 | val_1_rmse: 0.52261 |  0:32:59s
epoch 80 | loss: 0.2742  | val_0_rmse: 0.53121 | val_1_rmse: 0.53051 |  0:33:23s
epoch 81 | loss: 0.27389 | val_0_rmse: 0.61863 | val_1_rmse: 0.61518 |  0:33:48s
epoch 82 | loss: 0.27298 | val_0_rmse: 0.51586 | val_1_rmse: 0.51598 |  0:34:13s
epoch 83 | loss: 0.2727  | val_0_rmse: 0.52331 | val_1_rmse: 0.52203 |  0:34:38s
epoch 84 | loss: 0.27244 | val_0_rmse: 0.60498 | val_1_rmse: 0.60162 |  0:35:02s
epoch 85 | loss: 0.27318 | val_0_rmse: 0.58944 | val_1_rmse: 0.58751 |  0:35:27s
epoch 86 | loss: 0.2722  | val_0_rmse: 0.59649 | val_1_rmse: 0.59377 |  0:35:52s
epoch 87 | loss: 0.27263 | val_0_rmse: 0.53218 | val_1_rmse: 0.53136 |  0:36:17s
epoch 88 | loss: 0.2718  | val_0_rmse: 0.51916 | val_1_rmse: 0.51916 |  0:36:41s
epoch 89 | loss: 0.27254 | val_0_rmse: 0.60081 | val_1_rmse: 0.59831 |  0:37:06s
epoch 90 | loss: 0.27257 | val_0_rmse: 0.52593 | val_1_rmse: 0.52521 |  0:37:31s
epoch 91 | loss: 0.2721  | val_0_rmse: 0.51432 | val_1_rmse: 0.51393 |  0:37:56s
epoch 92 | loss: 0.27131 | val_0_rmse: 0.51768 | val_1_rmse: 0.51714 |  0:38:21s
epoch 93 | loss: 0.27289 | val_0_rmse: 0.51788 | val_1_rmse: 0.51734 |  0:38:45s
epoch 94 | loss: 0.2826  | val_0_rmse: 0.52298 | val_1_rmse: 0.52266 |  0:39:10s
epoch 95 | loss: 0.27836 | val_0_rmse: 0.51774 | val_1_rmse: 0.51757 |  0:39:35s
epoch 96 | loss: 0.27634 | val_0_rmse: 0.51932 | val_1_rmse: 0.51846 |  0:40:00s
epoch 97 | loss: 0.27303 | val_0_rmse: 0.51574 | val_1_rmse: 0.51598 |  0:40:25s
epoch 98 | loss: 0.27279 | val_0_rmse: 0.51792 | val_1_rmse: 0.51764 |  0:40:49s
epoch 99 | loss: 0.27217 | val_0_rmse: 0.52457 | val_1_rmse: 0.52413 |  0:41:14s
epoch 100| loss: 0.27211 | val_0_rmse: 0.52329 | val_1_rmse: 0.52225 |  0:41:38s
epoch 101| loss: 0.27537 | val_0_rmse: 0.5203  | val_1_rmse: 0.5204  |  0:42:03s
epoch 102| loss: 0.27948 | val_0_rmse: 0.52143 | val_1_rmse: 0.52112 |  0:42:28s
epoch 103| loss: 0.2809  | val_0_rmse: 0.53555 | val_1_rmse: 0.53486 |  0:42:53s
epoch 104| loss: 0.28001 | val_0_rmse: 0.63629 | val_1_rmse: 0.63483 |  0:43:18s
epoch 105| loss: 0.27623 | val_0_rmse: 0.5255  | val_1_rmse: 0.52504 |  0:43:42s
epoch 106| loss: 0.27525 | val_0_rmse: 0.53619 | val_1_rmse: 0.53674 |  0:44:07s
epoch 107| loss: 0.27541 | val_0_rmse: 0.56864 | val_1_rmse: 0.56934 |  0:44:32s
epoch 108| loss: 0.27662 | val_0_rmse: 0.51972 | val_1_rmse: 0.52014 |  0:44:56s
epoch 109| loss: 0.27758 | val_0_rmse: 0.52952 | val_1_rmse: 0.52856 |  0:45:21s
epoch 110| loss: 0.2794  | val_0_rmse: 0.53743 | val_1_rmse: 0.53704 |  0:45:46s
epoch 111| loss: 0.27273 | val_0_rmse: 0.52177 | val_1_rmse: 0.52157 |  0:46:11s
epoch 112| loss: 0.27335 | val_0_rmse: 0.51523 | val_1_rmse: 0.51537 |  0:46:35s
epoch 113| loss: 0.27163 | val_0_rmse: 0.51922 | val_1_rmse: 0.51907 |  0:47:00s
epoch 114| loss: 0.27107 | val_0_rmse: 0.51553 | val_1_rmse: 0.51535 |  0:47:25s
epoch 115| loss: 0.27105 | val_0_rmse: 0.52188 | val_1_rmse: 0.52165 |  0:47:50s
epoch 116| loss: 0.27011 | val_0_rmse: 0.61388 | val_1_rmse: 0.61121 |  0:48:14s
epoch 117| loss: 0.27433 | val_0_rmse: 0.51314 | val_1_rmse: 0.51334 |  0:48:39s
epoch 118| loss: 0.27209 | val_0_rmse: 0.51638 | val_1_rmse: 0.51638 |  0:49:04s
epoch 119| loss: 0.2706  | val_0_rmse: 0.5154  | val_1_rmse: 0.51508 |  0:49:28s
epoch 120| loss: 0.27037 | val_0_rmse: 0.51995 | val_1_rmse: 0.52052 |  0:49:53s
epoch 121| loss: 0.27127 | val_0_rmse: 0.52173 | val_1_rmse: 0.52166 |  0:50:18s
epoch 122| loss: 0.27039 | val_0_rmse: 0.51229 | val_1_rmse: 0.51292 |  0:50:43s
epoch 123| loss: 0.27005 | val_0_rmse: 0.51755 | val_1_rmse: 0.5174  |  0:51:07s
epoch 124| loss: 0.27017 | val_0_rmse: 0.57031 | val_1_rmse: 0.56961 |  0:51:32s
epoch 125| loss: 0.27018 | val_0_rmse: 0.51522 | val_1_rmse: 0.51508 |  0:51:57s
epoch 126| loss: 0.2705  | val_0_rmse: 0.51827 | val_1_rmse: 0.51801 |  0:52:21s
epoch 127| loss: 0.2707  | val_0_rmse: 0.53682 | val_1_rmse: 0.53709 |  0:52:46s
epoch 128| loss: 0.27129 | val_0_rmse: 0.51446 | val_1_rmse: 0.51474 |  0:53:11s
epoch 129| loss: 0.27285 | val_0_rmse: 0.54268 | val_1_rmse: 0.54301 |  0:53:35s
epoch 130| loss: 0.27051 | val_0_rmse: 0.51311 | val_1_rmse: 0.51316 |  0:54:00s
epoch 131| loss: 0.27085 | val_0_rmse: 0.51204 | val_1_rmse: 0.51281 |  0:54:25s
epoch 132| loss: 0.2696  | val_0_rmse: 0.51908 | val_1_rmse: 0.51927 |  0:54:50s
epoch 133| loss: 0.27009 | val_0_rmse: 0.51922 | val_1_rmse: 0.51871 |  0:55:14s
epoch 134| loss: 0.26961 | val_0_rmse: 0.51617 | val_1_rmse: 0.51659 |  0:55:39s
epoch 135| loss: 0.26996 | val_0_rmse: 0.52074 | val_1_rmse: 0.52049 |  0:56:04s
epoch 136| loss: 0.26984 | val_0_rmse: 0.51763 | val_1_rmse: 0.51788 |  0:56:28s
epoch 137| loss: 0.26972 | val_0_rmse: 0.62118 | val_1_rmse: 0.62133 |  0:56:53s
epoch 138| loss: 0.27103 | val_0_rmse: 0.51265 | val_1_rmse: 0.51286 |  0:57:18s
epoch 139| loss: 0.27181 | val_0_rmse: 0.53069 | val_1_rmse: 0.531   |  0:57:42s
epoch 140| loss: 0.26965 | val_0_rmse: 0.51478 | val_1_rmse: 0.51572 |  0:58:07s
epoch 141| loss: 0.26994 | val_0_rmse: 0.52478 | val_1_rmse: 0.52539 |  0:58:32s
epoch 142| loss: 0.26937 | val_0_rmse: 0.52368 | val_1_rmse: 0.52392 |  0:58:56s
epoch 143| loss: 0.26888 | val_0_rmse: 0.55102 | val_1_rmse: 0.54981 |  0:59:21s
epoch 144| loss: 0.26942 | val_0_rmse: 0.52378 | val_1_rmse: 0.52515 |  0:59:46s
epoch 145| loss: 0.27062 | val_0_rmse: 0.55518 | val_1_rmse: 0.5552  |  1:00:11s
epoch 146| loss: 0.26937 | val_0_rmse: 0.51478 | val_1_rmse: 0.5149  |  1:00:35s
epoch 147| loss: 0.26975 | val_0_rmse: 0.52799 | val_1_rmse: 0.52861 |  1:01:00s
epoch 148| loss: 0.26889 | val_0_rmse: 0.51308 | val_1_rmse: 0.51387 |  1:01:25s
epoch 149| loss: 0.26884 | val_0_rmse: 0.51673 | val_1_rmse: 0.51681 |  1:01:49s
Stop training because you reached max_epochs = 150 with best_epoch = 131 and best_val_1_rmse = 0.51281
Best weights from best epoch are automatically used!
ended training at: 18:25:07
Feature importance:
[('Area', 0.12734552358192402), ('Baths', 0.20115930837265153), ('Beds', 0.0), ('Latitude', 6.692267866654959e-05), ('Longitude', 0.4574506082863864), ('Month', 0.0), ('Year', 0.21397763708037149)]
Mean squared error is of 12581226063.701704
Mean absolute error:70395.2457233556
MAPE:0.3899933182439337
R2 score:0.7398873056049102
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:25:10
epoch 0  | loss: 0.36152 | val_0_rmse: 0.59359 | val_1_rmse: 0.59132 |  0:00:24s
epoch 1  | loss: 0.31555 | val_0_rmse: 0.55565 | val_1_rmse: 0.55225 |  0:00:49s
epoch 2  | loss: 0.30468 | val_0_rmse: 0.54672 | val_1_rmse: 0.54502 |  0:01:14s
epoch 3  | loss: 0.30179 | val_0_rmse: 0.54521 | val_1_rmse: 0.54161 |  0:01:38s
epoch 4  | loss: 0.29554 | val_0_rmse: 0.53741 | val_1_rmse: 0.53488 |  0:02:03s
epoch 5  | loss: 0.2926  | val_0_rmse: 0.56184 | val_1_rmse: 0.56088 |  0:02:28s
epoch 6  | loss: 0.29483 | val_0_rmse: 0.65795 | val_1_rmse: 0.65677 |  0:02:53s
epoch 7  | loss: 0.29696 | val_0_rmse: 0.81022 | val_1_rmse: 0.80851 |  0:03:18s
epoch 8  | loss: 0.31304 | val_0_rmse: 0.55146 | val_1_rmse: 0.54967 |  0:03:42s
epoch 9  | loss: 0.29401 | val_0_rmse: 0.53673 | val_1_rmse: 0.53379 |  0:04:07s
epoch 10 | loss: 0.28956 | val_0_rmse: 0.54007 | val_1_rmse: 0.53777 |  0:04:32s
epoch 11 | loss: 0.28752 | val_0_rmse: 0.59366 | val_1_rmse: 0.5921  |  0:04:56s
epoch 12 | loss: 0.28858 | val_0_rmse: 0.61281 | val_1_rmse: 0.61117 |  0:05:21s
epoch 13 | loss: 0.28927 | val_0_rmse: 0.53376 | val_1_rmse: 0.53032 |  0:05:48s
epoch 14 | loss: 0.28603 | val_0_rmse: 0.52643 | val_1_rmse: 0.52371 |  0:06:13s
epoch 15 | loss: 0.28607 | val_0_rmse: 0.53135 | val_1_rmse: 0.52843 |  0:06:38s
epoch 16 | loss: 0.28535 | val_0_rmse: 0.52848 | val_1_rmse: 0.52624 |  0:07:02s
epoch 17 | loss: 0.28338 | val_0_rmse: 0.52656 | val_1_rmse: 0.52402 |  0:07:27s
epoch 18 | loss: 0.28324 | val_0_rmse: 0.65672 | val_1_rmse: 0.65525 |  0:07:51s
epoch 19 | loss: 0.28289 | val_0_rmse: 0.53442 | val_1_rmse: 0.53168 |  0:08:16s
epoch 20 | loss: 0.28185 | val_0_rmse: 0.53032 | val_1_rmse: 0.52809 |  0:08:41s
epoch 21 | loss: 0.28356 | val_0_rmse: 0.53031 | val_1_rmse: 0.52806 |  0:09:05s
epoch 22 | loss: 0.2826  | val_0_rmse: 0.52689 | val_1_rmse: 0.52459 |  0:09:30s
epoch 23 | loss: 0.28164 | val_0_rmse: 0.54263 | val_1_rmse: 0.54071 |  0:09:55s
epoch 24 | loss: 0.28323 | val_0_rmse: 0.54248 | val_1_rmse: 0.53951 |  0:10:19s
epoch 25 | loss: 0.28158 | val_0_rmse: 0.54031 | val_1_rmse: 0.53852 |  0:10:44s
epoch 26 | loss: 0.28472 | val_0_rmse: 0.53638 | val_1_rmse: 0.53359 |  0:11:09s
epoch 27 | loss: 0.28494 | val_0_rmse: 0.53132 | val_1_rmse: 0.5291  |  0:11:33s
epoch 28 | loss: 0.28472 | val_0_rmse: 0.54698 | val_1_rmse: 0.54423 |  0:11:58s
epoch 29 | loss: 0.28165 | val_0_rmse: 0.52647 | val_1_rmse: 0.52372 |  0:12:22s
epoch 30 | loss: 0.28099 | val_0_rmse: 0.61336 | val_1_rmse: 0.61323 |  0:12:47s
epoch 31 | loss: 0.28345 | val_0_rmse: 0.52651 | val_1_rmse: 0.52337 |  0:13:12s
epoch 32 | loss: 0.2866  | val_0_rmse: 0.53966 | val_1_rmse: 0.53781 |  0:13:37s
epoch 33 | loss: 0.28227 | val_0_rmse: 0.52524 | val_1_rmse: 0.52303 |  0:14:01s
epoch 34 | loss: 0.28085 | val_0_rmse: 0.52732 | val_1_rmse: 0.5241  |  0:14:26s
epoch 35 | loss: 0.28151 | val_0_rmse: 0.52552 | val_1_rmse: 0.5231  |  0:14:51s
epoch 36 | loss: 0.29184 | val_0_rmse: 0.52362 | val_1_rmse: 0.52032 |  0:15:16s
epoch 37 | loss: 0.28221 | val_0_rmse: 0.53051 | val_1_rmse: 0.52913 |  0:15:40s
epoch 38 | loss: 0.28071 | val_0_rmse: 0.52704 | val_1_rmse: 0.52445 |  0:16:05s
epoch 39 | loss: 0.27873 | val_0_rmse: 0.5354  | val_1_rmse: 0.53288 |  0:16:30s
epoch 40 | loss: 0.28212 | val_0_rmse: 0.53332 | val_1_rmse: 0.53003 |  0:16:55s
epoch 41 | loss: 0.28002 | val_0_rmse: 0.52888 | val_1_rmse: 0.52668 |  0:17:20s
epoch 42 | loss: 0.27692 | val_0_rmse: 0.52356 | val_1_rmse: 0.52175 |  0:17:44s
epoch 43 | loss: 0.27779 | val_0_rmse: 0.52625 | val_1_rmse: 0.52475 |  0:18:09s
epoch 44 | loss: 0.27673 | val_0_rmse: 0.52517 | val_1_rmse: 0.52326 |  0:18:34s
epoch 45 | loss: 0.27519 | val_0_rmse: 0.53236 | val_1_rmse: 0.53082 |  0:18:59s
epoch 46 | loss: 0.27486 | val_0_rmse: 0.53167 | val_1_rmse: 0.53022 |  0:19:24s
epoch 47 | loss: 0.28194 | val_0_rmse: 0.53145 | val_1_rmse: 0.52955 |  0:19:48s
epoch 48 | loss: 0.28147 | val_0_rmse: 0.53293 | val_1_rmse: 0.53142 |  0:20:13s
epoch 49 | loss: 0.27835 | val_0_rmse: 0.54458 | val_1_rmse: 0.54318 |  0:20:38s
epoch 50 | loss: 0.27915 | val_0_rmse: 0.55814 | val_1_rmse: 0.55736 |  0:21:03s
epoch 51 | loss: 0.28135 | val_0_rmse: 0.53954 | val_1_rmse: 0.53832 |  0:21:28s
epoch 52 | loss: 0.28266 | val_0_rmse: 0.58281 | val_1_rmse: 0.51948 |  0:21:52s
epoch 53 | loss: 0.27924 | val_0_rmse: 0.52696 | val_1_rmse: 0.52024 |  0:22:17s
epoch 54 | loss: 0.27829 | val_0_rmse: 0.51961 | val_1_rmse: 0.51756 |  0:22:42s
epoch 55 | loss: 0.27745 | val_0_rmse: 0.52023 | val_1_rmse: 0.51808 |  0:23:06s
epoch 56 | loss: 0.2764  | val_0_rmse: 0.53089 | val_1_rmse: 0.52875 |  0:23:31s
epoch 57 | loss: 0.27608 | val_0_rmse: 0.52016 | val_1_rmse: 0.51776 |  0:23:56s
epoch 58 | loss: 0.27487 | val_0_rmse: 0.51979 | val_1_rmse: 0.5168  |  0:24:20s
epoch 59 | loss: 0.27505 | val_0_rmse: 0.53013 | val_1_rmse: 0.52897 |  0:24:45s
epoch 60 | loss: 0.27471 | val_0_rmse: 0.52613 | val_1_rmse: 0.52423 |  0:25:10s
epoch 61 | loss: 0.27411 | val_0_rmse: 0.53439 | val_1_rmse: 0.53146 |  0:25:34s
epoch 62 | loss: 0.27349 | val_0_rmse: 0.5215  | val_1_rmse: 0.51818 |  0:25:59s
epoch 63 | loss: 0.27504 | val_0_rmse: 0.52166 | val_1_rmse: 0.51894 |  0:26:24s
epoch 64 | loss: 0.27425 | val_0_rmse: 0.52447 | val_1_rmse: 0.52191 |  0:26:48s
epoch 65 | loss: 0.27598 | val_0_rmse: 0.53484 | val_1_rmse: 0.52605 |  0:27:13s
epoch 66 | loss: 0.27526 | val_0_rmse: 0.53164 | val_1_rmse: 0.51955 |  0:27:38s
epoch 67 | loss: 0.27481 | val_0_rmse: 0.52521 | val_1_rmse: 0.52189 |  0:28:03s
epoch 68 | loss: 0.27397 | val_0_rmse: 0.53641 | val_1_rmse: 0.52488 |  0:28:27s
epoch 69 | loss: 0.27361 | val_0_rmse: 0.53107 | val_1_rmse: 0.52737 |  0:28:52s
epoch 70 | loss: 0.27272 | val_0_rmse: 0.53039 | val_1_rmse: 0.52806 |  0:29:17s
epoch 71 | loss: 0.27186 | val_0_rmse: 0.52611 | val_1_rmse: 0.52447 |  0:29:42s
epoch 72 | loss: 0.27209 | val_0_rmse: 0.53921 | val_1_rmse: 0.53791 |  0:30:06s
epoch 73 | loss: 0.27216 | val_0_rmse: 0.52721 | val_1_rmse: 0.52577 |  0:30:31s
epoch 74 | loss: 0.2715  | val_0_rmse: 0.52228 | val_1_rmse: 0.5205  |  0:30:56s
epoch 75 | loss: 0.27193 | val_0_rmse: 0.55303 | val_1_rmse: 0.55177 |  0:31:21s
epoch 76 | loss: 0.27136 | val_0_rmse: 0.52842 | val_1_rmse: 0.52709 |  0:31:45s
epoch 77 | loss: 0.27071 | val_0_rmse: 0.52179 | val_1_rmse: 0.52027 |  0:32:10s
epoch 78 | loss: 0.27203 | val_0_rmse: 0.53979 | val_1_rmse: 0.53792 |  0:32:35s
epoch 79 | loss: 0.27139 | val_0_rmse: 0.52838 | val_1_rmse: 0.52693 |  0:33:00s
epoch 80 | loss: 0.27102 | val_0_rmse: 0.52215 | val_1_rmse: 0.52083 |  0:33:24s
epoch 81 | loss: 0.27033 | val_0_rmse: 0.5185  | val_1_rmse: 0.51678 |  0:33:49s
epoch 82 | loss: 0.27181 | val_0_rmse: 0.51917 | val_1_rmse: 0.51703 |  0:34:14s
epoch 83 | loss: 0.27114 | val_0_rmse: 0.5593  | val_1_rmse: 0.55836 |  0:34:39s
epoch 84 | loss: 0.27169 | val_0_rmse: 0.51628 | val_1_rmse: 0.51495 |  0:35:03s
epoch 85 | loss: 0.27144 | val_0_rmse: 0.51762 | val_1_rmse: 0.51642 |  0:35:28s
epoch 86 | loss: 0.27005 | val_0_rmse: 0.53554 | val_1_rmse: 0.53446 |  0:35:53s
epoch 87 | loss: 0.27016 | val_0_rmse: 0.52463 | val_1_rmse: 0.52344 |  0:36:18s
epoch 88 | loss: 0.27042 | val_0_rmse: 0.53275 | val_1_rmse: 0.53124 |  0:36:43s
epoch 89 | loss: 0.2701  | val_0_rmse: 0.52283 | val_1_rmse: 0.5217  |  0:37:08s
epoch 90 | loss: 0.26952 | val_0_rmse: 0.52676 | val_1_rmse: 0.52552 |  0:37:33s
epoch 91 | loss: 0.26988 | val_0_rmse: 0.52103 | val_1_rmse: 0.52024 |  0:37:57s
epoch 92 | loss: 0.27015 | val_0_rmse: 0.5789  | val_1_rmse: 0.57973 |  0:38:22s
epoch 93 | loss: 0.26999 | val_0_rmse: 0.52176 | val_1_rmse: 0.5206  |  0:38:47s
epoch 94 | loss: 0.26969 | val_0_rmse: 0.56051 | val_1_rmse: 0.56063 |  0:39:12s
epoch 95 | loss: 0.27015 | val_0_rmse: 0.5348  | val_1_rmse: 0.52755 |  0:39:37s
epoch 96 | loss: 0.26951 | val_0_rmse: 0.52718 | val_1_rmse: 0.52524 |  0:40:02s
epoch 97 | loss: 0.27099 | val_0_rmse: 0.54955 | val_1_rmse: 0.54217 |  0:40:27s
epoch 98 | loss: 0.26943 | val_0_rmse: 0.53228 | val_1_rmse: 0.51749 |  0:40:52s
epoch 99 | loss: 0.26933 | val_0_rmse: 0.55508 | val_1_rmse: 0.55524 |  0:41:16s
epoch 100| loss: 0.26922 | val_0_rmse: 0.53498 | val_1_rmse: 0.51829 |  0:41:41s
epoch 101| loss: 0.26883 | val_0_rmse: 0.52208 | val_1_rmse: 0.51996 |  0:42:06s
epoch 102| loss: 0.26896 | val_0_rmse: 0.53133 | val_1_rmse: 0.52793 |  0:42:31s
epoch 103| loss: 0.26916 | val_0_rmse: 0.52526 | val_1_rmse: 0.52385 |  0:42:56s
epoch 104| loss: 0.2685  | val_0_rmse: 0.52471 | val_1_rmse: 0.52373 |  0:43:21s
epoch 105| loss: 0.26853 | val_0_rmse: 0.51691 | val_1_rmse: 0.51535 |  0:43:45s
epoch 106| loss: 0.26985 | val_0_rmse: 0.51879 | val_1_rmse: 0.51803 |  0:44:10s
epoch 107| loss: 0.26942 | val_0_rmse: 0.52274 | val_1_rmse: 0.52208 |  0:44:35s
epoch 108| loss: 0.26872 | val_0_rmse: 0.53316 | val_1_rmse: 0.53362 |  0:45:00s
epoch 109| loss: 0.26862 | val_0_rmse: 0.53482 | val_1_rmse: 0.53435 |  0:45:25s
epoch 110| loss: 0.26913 | val_0_rmse: 0.54115 | val_1_rmse: 0.53993 |  0:45:50s
epoch 111| loss: 0.26872 | val_0_rmse: 0.52601 | val_1_rmse: 0.52318 |  0:46:14s
epoch 112| loss: 0.26842 | val_0_rmse: 0.52145 | val_1_rmse: 0.52073 |  0:46:39s
epoch 113| loss: 0.26815 | val_0_rmse: 0.52734 | val_1_rmse: 0.52488 |  0:47:04s
epoch 114| loss: 0.26818 | val_0_rmse: 0.53571 | val_1_rmse: 0.52138 |  0:47:29s

Early stopping occured at epoch 114 with best_epoch = 84 and best_val_1_rmse = 0.51495
Best weights from best epoch are automatically used!
ended training at: 19:12:47
Feature importance:
[('Area', 0.23808396647408953), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.33081413194352716), ('Longitude', 0.19452956634345844), ('Month', 0.0), ('Year', 0.23657233523892487)]
Mean squared error is of 12838894699.558363
Mean absolute error:72292.98285903185
MAPE:0.42744640333621353
R2 score:0.7312896902753176
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:12:50
epoch 0  | loss: 0.35791 | val_0_rmse: 0.55136 | val_1_rmse: 0.5537  |  0:00:25s
epoch 1  | loss: 0.30496 | val_0_rmse: 0.54353 | val_1_rmse: 0.54564 |  0:00:52s
epoch 2  | loss: 0.29837 | val_0_rmse: 0.54287 | val_1_rmse: 0.5444  |  0:01:19s
epoch 3  | loss: 0.29338 | val_0_rmse: 0.53298 | val_1_rmse: 0.53445 |  0:01:43s
epoch 4  | loss: 0.29214 | val_0_rmse: 0.54217 | val_1_rmse: 0.54385 |  0:02:09s
epoch 5  | loss: 0.29046 | val_0_rmse: 0.5441  | val_1_rmse: 0.5466  |  0:02:33s
epoch 6  | loss: 0.28799 | val_0_rmse: 0.60687 | val_1_rmse: 0.6097  |  0:02:58s
epoch 7  | loss: 0.28783 | val_0_rmse: 0.52706 | val_1_rmse: 0.5284  |  0:03:22s
epoch 8  | loss: 0.28668 | val_0_rmse: 0.53592 | val_1_rmse: 0.5365  |  0:03:47s
epoch 9  | loss: 0.28594 | val_0_rmse: 0.53907 | val_1_rmse: 0.53989 |  0:04:12s
epoch 10 | loss: 0.28562 | val_0_rmse: 0.54819 | val_1_rmse: 0.55126 |  0:04:37s
epoch 11 | loss: 0.28446 | val_0_rmse: 0.5369  | val_1_rmse: 0.53921 |  0:05:02s
epoch 12 | loss: 0.2829  | val_0_rmse: 0.52449 | val_1_rmse: 0.52456 |  0:05:27s
epoch 13 | loss: 0.28239 | val_0_rmse: 0.54054 | val_1_rmse: 0.54174 |  0:05:51s
epoch 14 | loss: 0.28916 | val_0_rmse: 0.53568 | val_1_rmse: 0.53701 |  0:06:16s
epoch 15 | loss: 0.28585 | val_0_rmse: 0.53475 | val_1_rmse: 0.53521 |  0:06:41s
epoch 16 | loss: 0.28255 | val_0_rmse: 0.55333 | val_1_rmse: 0.55343 |  0:07:06s
epoch 17 | loss: 0.28125 | val_0_rmse: 0.52428 | val_1_rmse: 0.52541 |  0:07:31s
epoch 18 | loss: 0.28101 | val_0_rmse: 0.52253 | val_1_rmse: 0.52373 |  0:07:56s
epoch 19 | loss: 0.28024 | val_0_rmse: 0.52514 | val_1_rmse: 0.52643 |  0:08:20s
epoch 20 | loss: 0.27909 | val_0_rmse: 0.53133 | val_1_rmse: 0.53278 |  0:08:45s
epoch 21 | loss: 0.27827 | val_0_rmse: 0.53257 | val_1_rmse: 0.53341 |  0:09:10s
epoch 22 | loss: 0.27835 | val_0_rmse: 0.53118 | val_1_rmse: 0.53285 |  0:09:34s
epoch 23 | loss: 0.27852 | val_0_rmse: 0.53155 | val_1_rmse: 0.53247 |  0:09:59s
epoch 24 | loss: 0.2774  | val_0_rmse: 0.53246 | val_1_rmse: 0.53404 |  0:10:24s
epoch 25 | loss: 0.2771  | val_0_rmse: 0.57372 | val_1_rmse: 0.57668 |  0:10:48s
epoch 26 | loss: 0.27665 | val_0_rmse: 0.5274  | val_1_rmse: 0.52877 |  0:11:13s
epoch 27 | loss: 0.27587 | val_0_rmse: 0.52138 | val_1_rmse: 0.52273 |  0:11:38s
epoch 28 | loss: 0.27563 | val_0_rmse: 0.51873 | val_1_rmse: 0.52008 |  0:12:02s
epoch 29 | loss: 0.27521 | val_0_rmse: 0.53391 | val_1_rmse: 0.53518 |  0:12:27s
epoch 30 | loss: 0.27445 | val_0_rmse: 0.52847 | val_1_rmse: 0.53069 |  0:12:52s
epoch 31 | loss: 0.27404 | val_0_rmse: 0.55077 | val_1_rmse: 0.55169 |  0:13:16s
epoch 32 | loss: 0.27427 | val_0_rmse: 0.52423 | val_1_rmse: 0.52489 |  0:13:41s
epoch 33 | loss: 0.27327 | val_0_rmse: 0.52197 | val_1_rmse: 0.52335 |  0:14:06s
epoch 34 | loss: 0.27405 | val_0_rmse: 0.53758 | val_1_rmse: 0.53959 |  0:14:31s
epoch 35 | loss: 0.2724  | val_0_rmse: 0.53721 | val_1_rmse: 0.53959 |  0:14:55s
epoch 36 | loss: 0.27294 | val_0_rmse: 0.53938 | val_1_rmse: 0.54116 |  0:15:20s
epoch 37 | loss: 0.27599 | val_0_rmse: 0.52015 | val_1_rmse: 0.52039 |  0:15:45s
epoch 38 | loss: 0.27181 | val_0_rmse: 0.53586 | val_1_rmse: 0.53799 |  0:16:10s
epoch 39 | loss: 0.27294 | val_0_rmse: 0.52782 | val_1_rmse: 0.52921 |  0:16:34s
epoch 40 | loss: 0.27093 | val_0_rmse: 0.52032 | val_1_rmse: 0.52111 |  0:16:59s
epoch 41 | loss: 0.27095 | val_0_rmse: 0.5331  | val_1_rmse: 0.53438 |  0:17:24s
epoch 42 | loss: 0.27234 | val_0_rmse: 0.53027 | val_1_rmse: 0.53093 |  0:17:49s
epoch 43 | loss: 0.27198 | val_0_rmse: 0.52384 | val_1_rmse: 0.52535 |  0:18:13s
epoch 44 | loss: 0.27038 | val_0_rmse: 0.54343 | val_1_rmse: 0.54492 |  0:18:38s
epoch 45 | loss: 0.27112 | val_0_rmse: 0.52239 | val_1_rmse: 0.52407 |  0:19:02s
epoch 46 | loss: 0.27204 | val_0_rmse: 0.51903 | val_1_rmse: 0.5197  |  0:19:27s
epoch 47 | loss: 0.27111 | val_0_rmse: 0.51597 | val_1_rmse: 0.51712 |  0:19:52s
epoch 48 | loss: 0.27002 | val_0_rmse: 0.51687 | val_1_rmse: 0.51772 |  0:20:17s
epoch 49 | loss: 0.2705  | val_0_rmse: 0.51656 | val_1_rmse: 0.51769 |  0:20:41s
epoch 50 | loss: 0.2713  | val_0_rmse: 0.5404  | val_1_rmse: 0.54182 |  0:21:06s
epoch 51 | loss: 0.27051 | val_0_rmse: 0.53702 | val_1_rmse: 0.53876 |  0:21:31s
epoch 52 | loss: 0.27025 | val_0_rmse: 0.52062 | val_1_rmse: 0.52176 |  0:21:56s
epoch 53 | loss: 0.27021 | val_0_rmse: 0.52432 | val_1_rmse: 0.52615 |  0:22:20s
epoch 54 | loss: 0.26914 | val_0_rmse: 0.52464 | val_1_rmse: 0.52705 |  0:22:45s
epoch 55 | loss: 0.26884 | val_0_rmse: 0.51732 | val_1_rmse: 0.51839 |  0:23:10s
epoch 56 | loss: 0.26826 | val_0_rmse: 0.55234 | val_1_rmse: 0.55404 |  0:23:34s
epoch 57 | loss: 0.26847 | val_0_rmse: 0.51947 | val_1_rmse: 0.52091 |  0:23:59s
epoch 58 | loss: 0.26827 | val_0_rmse: 0.51917 | val_1_rmse: 0.52021 |  0:24:24s
epoch 59 | loss: 0.2685  | val_0_rmse: 0.52009 | val_1_rmse: 0.5224  |  0:24:48s
epoch 60 | loss: 0.26791 | val_0_rmse: 0.51852 | val_1_rmse: 0.52044 |  0:25:13s
epoch 61 | loss: 0.26881 | val_0_rmse: 0.58809 | val_1_rmse: 0.59123 |  0:25:38s
epoch 62 | loss: 0.26838 | val_0_rmse: 0.51015 | val_1_rmse: 0.51144 |  0:26:03s
epoch 63 | loss: 0.26771 | val_0_rmse: 0.52241 | val_1_rmse: 0.5251  |  0:26:27s
epoch 64 | loss: 0.26775 | val_0_rmse: 0.53206 | val_1_rmse: 0.53376 |  0:26:52s
epoch 65 | loss: 0.26875 | val_0_rmse: 0.51332 | val_1_rmse: 0.51537 |  0:27:16s
epoch 66 | loss: 0.26878 | val_0_rmse: 0.52945 | val_1_rmse: 0.53203 |  0:27:41s
epoch 67 | loss: 0.2683  | val_0_rmse: 0.51972 | val_1_rmse: 0.52162 |  0:28:06s
epoch 68 | loss: 0.26901 | val_0_rmse: 0.5709  | val_1_rmse: 0.57249 |  0:28:31s
epoch 69 | loss: 0.27043 | val_0_rmse: 0.51602 | val_1_rmse: 0.51825 |  0:28:56s
epoch 70 | loss: 0.26823 | val_0_rmse: 0.5153  | val_1_rmse: 0.51748 |  0:29:20s
epoch 71 | loss: 0.26804 | val_0_rmse: 0.52611 | val_1_rmse: 0.52789 |  0:29:45s
epoch 72 | loss: 0.26748 | val_0_rmse: 0.51349 | val_1_rmse: 0.51573 |  0:30:10s
epoch 73 | loss: 0.26768 | val_0_rmse: 0.52706 | val_1_rmse: 0.52953 |  0:30:34s
epoch 74 | loss: 0.26826 | val_0_rmse: 0.52493 | val_1_rmse: 0.52738 |  0:30:59s
epoch 75 | loss: 0.26779 | val_0_rmse: 0.54349 | val_1_rmse: 0.54443 |  0:31:24s
epoch 76 | loss: 0.26959 | val_0_rmse: 0.5468  | val_1_rmse: 0.54923 |  0:31:49s
epoch 77 | loss: 0.26949 | val_0_rmse: 0.51367 | val_1_rmse: 0.51533 |  0:32:13s
epoch 78 | loss: 0.26826 | val_0_rmse: 0.51604 | val_1_rmse: 0.51778 |  0:32:39s
epoch 79 | loss: 0.26712 | val_0_rmse: 0.51676 | val_1_rmse: 0.51824 |  0:33:03s
epoch 80 | loss: 0.2675  | val_0_rmse: 0.51232 | val_1_rmse: 0.5138  |  0:33:28s
epoch 81 | loss: 0.26657 | val_0_rmse: 0.52853 | val_1_rmse: 0.53041 |  0:33:53s
epoch 82 | loss: 0.26821 | val_0_rmse: 0.53075 | val_1_rmse: 0.53322 |  0:34:18s
epoch 83 | loss: 0.26716 | val_0_rmse: 0.52824 | val_1_rmse: 0.53047 |  0:34:42s
epoch 84 | loss: 0.26696 | val_0_rmse: 0.51056 | val_1_rmse: 0.51204 |  0:35:11s
epoch 85 | loss: 0.26657 | val_0_rmse: 0.51493 | val_1_rmse: 0.51692 |  0:35:40s
epoch 86 | loss: 0.26601 | val_0_rmse: 0.5377  | val_1_rmse: 0.53967 |  0:36:10s
epoch 87 | loss: 0.26706 | val_0_rmse: 0.51688 | val_1_rmse: 0.51977 |  0:36:42s
epoch 88 | loss: 0.26566 | val_0_rmse: 0.54554 | val_1_rmse: 0.54831 |  0:37:16s
epoch 89 | loss: 0.26576 | val_0_rmse: 0.51751 | val_1_rmse: 0.51953 |  0:37:54s
epoch 90 | loss: 0.27095 | val_0_rmse: 0.5218  | val_1_rmse: 0.5236  |  0:38:41s
epoch 91 | loss: 0.27415 | val_0_rmse: 0.53441 | val_1_rmse: 0.53546 |  0:39:26s
epoch 92 | loss: 0.27023 | val_0_rmse: 0.521   | val_1_rmse: 0.52171 |  0:40:12s

Early stopping occured at epoch 92 with best_epoch = 62 and best_val_1_rmse = 0.51144
Best weights from best epoch are automatically used!
ended training at: 19:53:18
Feature importance:
[('Area', 0.346424347417591), ('Baths', 0.0), ('Beds', 0.0), ('Latitude', 0.3631706897667153), ('Longitude', 0.1714106455934445), ('Month', 0.0), ('Year', 0.11899431722224922)]
Mean squared error is of 12679277088.86868
Mean absolute error:70931.0487743248
MAPE:0.4107993192392146
R2 score:0.7379503917562124
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:53:23
epoch 0  | loss: 0.36243 | val_0_rmse: 0.55501 | val_1_rmse: 0.55187 |  0:00:45s
epoch 1  | loss: 0.30925 | val_0_rmse: 0.55871 | val_1_rmse: 0.55448 |  0:01:31s
epoch 2  | loss: 0.30169 | val_0_rmse: 0.56842 | val_1_rmse: 0.56339 |  0:02:17s
epoch 3  | loss: 0.30022 | val_0_rmse: 0.54575 | val_1_rmse: 0.54221 |  0:03:02s
epoch 4  | loss: 0.29836 | val_0_rmse: 0.85568 | val_1_rmse: 0.84334 |  0:03:49s
epoch 5  | loss: 0.30249 | val_0_rmse: 0.54454 | val_1_rmse: 0.54218 |  0:04:35s
epoch 6  | loss: 0.29353 | val_0_rmse: 0.53988 | val_1_rmse: 0.53669 |  0:05:21s
epoch 7  | loss: 0.29359 | val_0_rmse: 0.53921 | val_1_rmse: 0.53498 |  0:06:07s
epoch 8  | loss: 0.29193 | val_0_rmse: 0.54895 | val_1_rmse: 0.54592 |  0:06:54s
epoch 9  | loss: 0.29461 | val_0_rmse: 0.53447 | val_1_rmse: 0.53168 |  0:07:40s
epoch 10 | loss: 0.29026 | val_0_rmse: 0.53577 | val_1_rmse: 0.53332 |  0:08:26s
epoch 11 | loss: 0.28891 | val_0_rmse: 0.56762 | val_1_rmse: 0.56221 |  0:09:12s
epoch 12 | loss: 0.29468 | val_0_rmse: 0.54965 | val_1_rmse: 0.54587 |  0:09:59s
epoch 13 | loss: 0.29301 | val_0_rmse: 0.54871 | val_1_rmse: 0.54437 |  0:10:45s
epoch 14 | loss: 0.2894  | val_0_rmse: 0.7049  | val_1_rmse: 0.69662 |  0:11:31s
epoch 15 | loss: 0.28703 | val_0_rmse: 0.53185 | val_1_rmse: 0.52918 |  0:12:17s
epoch 16 | loss: 0.28711 | val_0_rmse: 0.52914 | val_1_rmse: 0.52637 |  0:13:03s
epoch 17 | loss: 0.28721 | val_0_rmse: 0.5417  | val_1_rmse: 0.53778 |  0:13:49s
epoch 18 | loss: 0.28592 | val_0_rmse: 0.53015 | val_1_rmse: 0.52675 |  0:14:35s
epoch 19 | loss: 0.28654 | val_0_rmse: 0.54322 | val_1_rmse: 0.53855 |  0:15:21s
epoch 20 | loss: 0.28518 | val_0_rmse: 0.53697 | val_1_rmse: 0.53292 |  0:16:07s
epoch 21 | loss: 0.28416 | val_0_rmse: 0.57288 | val_1_rmse: 0.57111 |  0:16:52s
epoch 22 | loss: 0.28368 | val_0_rmse: 0.528   | val_1_rmse: 0.52469 |  0:17:36s
epoch 23 | loss: 0.28359 | val_0_rmse: 0.53091 | val_1_rmse: 0.52814 |  0:18:20s
epoch 24 | loss: 0.28442 | val_0_rmse: 0.57137 | val_1_rmse: 0.56532 |  0:19:04s
epoch 25 | loss: 0.28284 | val_0_rmse: 0.55023 | val_1_rmse: 0.5468  |  0:19:48s
epoch 26 | loss: 0.28454 | val_0_rmse: 0.53388 | val_1_rmse: 0.53099 |  0:20:33s
epoch 27 | loss: 0.28434 | val_0_rmse: 0.53685 | val_1_rmse: 0.53317 |  0:21:20s
epoch 28 | loss: 0.28534 | val_0_rmse: 0.53034 | val_1_rmse: 0.52663 |  0:22:00s
epoch 29 | loss: 0.28239 | val_0_rmse: 0.55196 | val_1_rmse: 0.54701 |  0:22:45s
epoch 30 | loss: 0.28437 | val_0_rmse: 0.53423 | val_1_rmse: 0.53052 |  0:23:18s
epoch 31 | loss: 0.28109 | val_0_rmse: 0.54064 | val_1_rmse: 0.53756 |  0:23:43s
epoch 32 | loss: 0.2821  | val_0_rmse: 0.53967 | val_1_rmse: 0.53485 |  0:24:09s
epoch 33 | loss: 0.28094 | val_0_rmse: 0.53987 | val_1_rmse: 0.53501 |  0:24:34s
epoch 34 | loss: 0.28044 | val_0_rmse: 0.53951 | val_1_rmse: 0.53601 |  0:24:59s
epoch 35 | loss: 0.28384 | val_0_rmse: 0.53993 | val_1_rmse: 0.53675 |  0:25:24s
epoch 36 | loss: 0.2814  | val_0_rmse: 0.55092 | val_1_rmse: 0.54538 |  0:25:49s
epoch 37 | loss: 0.28104 | val_0_rmse: 0.5358  | val_1_rmse: 0.53212 |  0:26:14s
epoch 38 | loss: 0.28012 | val_0_rmse: 0.52822 | val_1_rmse: 0.52422 |  0:26:39s
epoch 39 | loss: 0.28076 | val_0_rmse: 0.5367  | val_1_rmse: 0.53248 |  0:27:04s
epoch 40 | loss: 0.28289 | val_0_rmse: 0.54261 | val_1_rmse: 0.53786 |  0:27:29s
epoch 41 | loss: 0.28146 | val_0_rmse: 0.523   | val_1_rmse: 0.52045 |  0:27:54s
epoch 42 | loss: 0.28074 | val_0_rmse: 0.52813 | val_1_rmse: 0.52529 |  0:28:19s
epoch 43 | loss: 0.27992 | val_0_rmse: 0.53998 | val_1_rmse: 0.53514 |  0:28:44s
epoch 44 | loss: 0.27786 | val_0_rmse: 0.55674 | val_1_rmse: 0.55097 |  0:29:09s
epoch 45 | loss: 0.28096 | val_0_rmse: 0.54772 | val_1_rmse: 0.54246 |  0:29:34s
epoch 46 | loss: 0.2797  | val_0_rmse: 0.55082 | val_1_rmse: 0.54632 |  0:29:58s
epoch 47 | loss: 0.27964 | val_0_rmse: 0.55991 | val_1_rmse: 0.55449 |  0:30:23s
epoch 48 | loss: 0.27951 | val_0_rmse: 0.53664 | val_1_rmse: 0.53446 |  0:30:48s
epoch 49 | loss: 0.27926 | val_0_rmse: 0.53978 | val_1_rmse: 0.53601 |  0:31:13s
epoch 50 | loss: 0.27787 | val_0_rmse: 0.53298 | val_1_rmse: 0.53035 |  0:31:38s
epoch 51 | loss: 0.2796  | val_0_rmse: 0.52834 | val_1_rmse: 0.52528 |  0:32:02s
epoch 52 | loss: 0.27794 | val_0_rmse: 0.57452 | val_1_rmse: 0.56715 |  0:32:27s
epoch 53 | loss: 0.27979 | val_0_rmse: 0.52777 | val_1_rmse: 0.52497 |  0:32:52s
epoch 54 | loss: 0.27776 | val_0_rmse: 0.54389 | val_1_rmse: 0.53843 |  0:33:17s
epoch 55 | loss: 0.27729 | val_0_rmse: 0.54217 | val_1_rmse: 0.53804 |  0:33:42s
epoch 56 | loss: 0.28434 | val_0_rmse: 0.60964 | val_1_rmse: 0.60024 |  0:34:06s
epoch 57 | loss: 0.28249 | val_0_rmse: 0.52582 | val_1_rmse: 0.52285 |  0:34:31s
epoch 58 | loss: 0.28292 | val_0_rmse: 0.533   | val_1_rmse: 0.52949 |  0:34:56s
epoch 59 | loss: 0.28172 | val_0_rmse: 0.52927 | val_1_rmse: 0.52632 |  0:35:21s
epoch 60 | loss: 0.27876 | val_0_rmse: 0.54187 | val_1_rmse: 0.5376  |  0:35:45s
epoch 61 | loss: 0.27793 | val_0_rmse: 0.54129 | val_1_rmse: 0.53645 |  0:36:10s
epoch 62 | loss: 0.27815 | val_0_rmse: 0.59523 | val_1_rmse: 0.59193 |  0:36:35s
epoch 63 | loss: 0.28078 | val_0_rmse: 0.75804 | val_1_rmse: 0.74752 |  0:37:00s
epoch 64 | loss: 0.27969 | val_0_rmse: 0.52708 | val_1_rmse: 0.52435 |  0:37:24s
epoch 65 | loss: 0.279   | val_0_rmse: 0.53906 | val_1_rmse: 0.53621 |  0:37:49s
epoch 66 | loss: 0.27801 | val_0_rmse: 0.53888 | val_1_rmse: 0.53463 |  0:38:14s
epoch 67 | loss: 0.27803 | val_0_rmse: 0.52409 | val_1_rmse: 0.52165 |  0:38:38s
epoch 68 | loss: 0.27732 | val_0_rmse: 0.53779 | val_1_rmse: 0.53438 |  0:39:03s
epoch 69 | loss: 0.27712 | val_0_rmse: 0.5259  | val_1_rmse: 0.52332 |  0:39:28s
epoch 70 | loss: 0.27829 | val_0_rmse: 0.53893 | val_1_rmse: 0.5343  |  0:39:53s
epoch 71 | loss: 0.27547 | val_0_rmse: 0.53729 | val_1_rmse: 0.53269 |  0:40:17s

Early stopping occured at epoch 71 with best_epoch = 41 and best_val_1_rmse = 0.52045
Best weights from best epoch are automatically used!
ended training at: 20:33:49
Feature importance:
[('Area', 0.12469187751131469), ('Baths', 0.013119526288306784), ('Beds', 0.05502510498836638), ('Latitude', 0.17465186054950763), ('Longitude', 0.23161481563912298), ('Month', 0.08308526798157026), ('Year', 0.31781154704181125)]
Mean squared error is of 13315893862.635895
Mean absolute error:72266.74687636191
MAPE:0.4036011701221904
R2 score:0.7258383290505168
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:33:52
epoch 0  | loss: 0.3517  | val_0_rmse: 0.5526  | val_1_rmse: 0.55379 |  0:00:24s
epoch 1  | loss: 0.30607 | val_0_rmse: 0.54331 | val_1_rmse: 0.54465 |  0:00:49s
epoch 2  | loss: 0.30036 | val_0_rmse: 0.58459 | val_1_rmse: 0.58756 |  0:01:14s
epoch 3  | loss: 0.3026  | val_0_rmse: 0.62248 | val_1_rmse: 0.62274 |  0:01:39s
epoch 4  | loss: 0.30453 | val_0_rmse: 0.56522 | val_1_rmse: 0.56798 |  0:02:04s
epoch 5  | loss: 0.29431 | val_0_rmse: 0.53483 | val_1_rmse: 0.53668 |  0:02:29s
epoch 6  | loss: 0.29191 | val_0_rmse: 0.53356 | val_1_rmse: 0.53519 |  0:02:53s
epoch 7  | loss: 0.2899  | val_0_rmse: 0.52846 | val_1_rmse: 0.52923 |  0:03:18s
epoch 8  | loss: 0.28847 | val_0_rmse: 0.53151 | val_1_rmse: 0.53176 |  0:03:43s
epoch 9  | loss: 0.28588 | val_0_rmse: 0.54188 | val_1_rmse: 0.54194 |  0:04:08s
epoch 10 | loss: 0.2854  | val_0_rmse: 0.53933 | val_1_rmse: 0.5391  |  0:04:33s
epoch 11 | loss: 0.28379 | val_0_rmse: 0.52741 | val_1_rmse: 0.52778 |  0:04:58s
epoch 12 | loss: 0.28328 | val_0_rmse: 0.55124 | val_1_rmse: 0.55181 |  0:05:23s
epoch 13 | loss: 0.28324 | val_0_rmse: 0.52879 | val_1_rmse: 0.52996 |  0:05:47s
epoch 14 | loss: 0.28226 | val_0_rmse: 0.55133 | val_1_rmse: 0.5508  |  0:06:12s
epoch 15 | loss: 0.28214 | val_0_rmse: 0.53199 | val_1_rmse: 0.53263 |  0:06:37s
epoch 16 | loss: 0.28112 | val_0_rmse: 0.57039 | val_1_rmse: 0.57051 |  0:07:02s
epoch 17 | loss: 0.28081 | val_0_rmse: 0.5364  | val_1_rmse: 0.53721 |  0:07:27s
epoch 18 | loss: 0.28187 | val_0_rmse: 0.55514 | val_1_rmse: 0.55489 |  0:07:52s
epoch 19 | loss: 0.28049 | val_0_rmse: 0.6176  | val_1_rmse: 0.62175 |  0:08:17s
epoch 20 | loss: 0.28885 | val_0_rmse: 0.525   | val_1_rmse: 0.52625 |  0:08:41s
epoch 21 | loss: 0.28095 | val_0_rmse: 0.55183 | val_1_rmse: 0.55247 |  0:09:06s
epoch 22 | loss: 0.28062 | val_0_rmse: 0.5267  | val_1_rmse: 0.52773 |  0:09:31s
epoch 23 | loss: 0.2795  | val_0_rmse: 0.54428 | val_1_rmse: 0.54609 |  0:09:56s
epoch 24 | loss: 0.27906 | val_0_rmse: 0.5778  | val_1_rmse: 0.58076 |  0:10:20s
epoch 25 | loss: 0.2779  | val_0_rmse: 0.61461 | val_1_rmse: 0.61749 |  0:10:45s
epoch 26 | loss: 0.27968 | val_0_rmse: 0.52403 | val_1_rmse: 0.5251  |  0:11:10s
epoch 27 | loss: 0.27829 | val_0_rmse: 0.54352 | val_1_rmse: 0.54384 |  0:11:35s
epoch 28 | loss: 0.27785 | val_0_rmse: 0.5275  | val_1_rmse: 0.52843 |  0:12:00s
epoch 29 | loss: 0.27733 | val_0_rmse: 0.52093 | val_1_rmse: 0.52148 |  0:12:25s
epoch 30 | loss: 0.27719 | val_0_rmse: 0.52334 | val_1_rmse: 0.52541 |  0:12:50s
epoch 31 | loss: 0.2767  | val_0_rmse: 0.53607 | val_1_rmse: 0.5379  |  0:13:14s
epoch 32 | loss: 0.27582 | val_0_rmse: 0.52708 | val_1_rmse: 0.52838 |  0:13:39s
epoch 33 | loss: 0.27615 | val_0_rmse: 0.52118 | val_1_rmse: 0.52175 |  0:14:04s
epoch 34 | loss: 0.27499 | val_0_rmse: 0.5254  | val_1_rmse: 0.52637 |  0:14:29s
epoch 35 | loss: 0.27615 | val_0_rmse: 0.51887 | val_1_rmse: 0.52005 |  0:14:54s
epoch 36 | loss: 0.27712 | val_0_rmse: 0.54419 | val_1_rmse: 0.54575 |  0:15:19s
epoch 37 | loss: 0.28385 | val_0_rmse: 0.53889 | val_1_rmse: 0.53899 |  0:15:43s
epoch 38 | loss: 0.27873 | val_0_rmse: 0.53761 | val_1_rmse: 0.53928 |  0:16:08s
epoch 39 | loss: 0.27724 | val_0_rmse: 0.51816 | val_1_rmse: 0.51977 |  0:16:33s
epoch 40 | loss: 0.27537 | val_0_rmse: 0.5267  | val_1_rmse: 0.52697 |  0:16:58s
epoch 41 | loss: 0.27532 | val_0_rmse: 0.52279 | val_1_rmse: 0.52499 |  0:17:23s
epoch 42 | loss: 0.27427 | val_0_rmse: 0.54142 | val_1_rmse: 0.54365 |  0:17:48s
epoch 43 | loss: 0.27682 | val_0_rmse: 0.52203 | val_1_rmse: 0.523   |  0:18:13s
epoch 44 | loss: 0.27462 | val_0_rmse: 0.54327 | val_1_rmse: 0.54477 |  0:18:38s
epoch 45 | loss: 0.27461 | val_0_rmse: 0.57803 | val_1_rmse: 0.58051 |  0:19:02s
epoch 46 | loss: 0.28843 | val_0_rmse: 0.53132 | val_1_rmse: 0.5337  |  0:19:27s
epoch 47 | loss: 0.28001 | val_0_rmse: 0.54217 | val_1_rmse: 0.54368 |  0:19:52s
epoch 48 | loss: 0.27774 | val_0_rmse: 0.51945 | val_1_rmse: 0.52035 |  0:20:17s
epoch 49 | loss: 0.2746  | val_0_rmse: 0.5222  | val_1_rmse: 0.52469 |  0:20:42s
epoch 50 | loss: 0.27569 | val_0_rmse: 0.54591 | val_1_rmse: 0.54921 |  0:21:07s
epoch 51 | loss: 0.27459 | val_0_rmse: 0.56768 | val_1_rmse: 0.57097 |  0:21:32s
epoch 52 | loss: 0.27653 | val_0_rmse: 0.52499 | val_1_rmse: 0.52631 |  0:21:56s
epoch 53 | loss: 0.27635 | val_0_rmse: 0.52367 | val_1_rmse: 0.52537 |  0:22:21s
epoch 54 | loss: 0.27542 | val_0_rmse: 0.51979 | val_1_rmse: 0.5217  |  0:22:46s
epoch 55 | loss: 0.27446 | val_0_rmse: 0.55211 | val_1_rmse: 0.55522 |  0:23:11s
epoch 56 | loss: 0.27584 | val_0_rmse: 0.53281 | val_1_rmse: 0.5333  |  0:23:36s
epoch 57 | loss: 0.27443 | val_0_rmse: 0.52278 | val_1_rmse: 0.52434 |  0:24:01s
epoch 58 | loss: 0.27468 | val_0_rmse: 0.53482 | val_1_rmse: 0.5354  |  0:24:26s
epoch 59 | loss: 0.27512 | val_0_rmse: 0.54547 | val_1_rmse: 0.54728 |  0:24:50s
epoch 60 | loss: 0.27541 | val_0_rmse: 0.52228 | val_1_rmse: 0.52418 |  0:25:15s
epoch 61 | loss: 0.27486 | val_0_rmse: 0.52219 | val_1_rmse: 0.52419 |  0:25:40s
epoch 62 | loss: 0.27573 | val_0_rmse: 0.51848 | val_1_rmse: 0.52035 |  0:26:05s
epoch 63 | loss: 0.27341 | val_0_rmse: 0.52773 | val_1_rmse: 0.52894 |  0:26:30s
epoch 64 | loss: 0.27414 | val_0_rmse: 0.52768 | val_1_rmse: 0.52861 |  0:26:55s
epoch 65 | loss: 0.27258 | val_0_rmse: 0.53062 | val_1_rmse: 0.53172 |  0:27:19s
epoch 66 | loss: 0.27351 | val_0_rmse: 0.52562 | val_1_rmse: 0.52779 |  0:27:44s
epoch 67 | loss: 0.27427 | val_0_rmse: 0.52285 | val_1_rmse: 0.52494 |  0:28:09s
epoch 68 | loss: 0.27319 | val_0_rmse: 0.52423 | val_1_rmse: 0.5261  |  0:28:34s
epoch 69 | loss: 0.27227 | val_0_rmse: 0.52705 | val_1_rmse: 0.5286  |  0:28:59s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.51977
Best weights from best epoch are automatically used!
ended training at: 21:02:59
Feature importance:
[('Area', 0.22078073430893044), ('Baths', 0.0934797661467944), ('Beds', 0.0), ('Latitude', 0.2900377924316571), ('Longitude', 0.35726944635867697), ('Month', 0.0), ('Year', 0.03843226075394111)]
Mean squared error is of 13185697762.30734
Mean absolute error:73588.69463369841
MAPE:0.4159113238096037
R2 score:0.7282852102487498
------------------------------------------------------------------
