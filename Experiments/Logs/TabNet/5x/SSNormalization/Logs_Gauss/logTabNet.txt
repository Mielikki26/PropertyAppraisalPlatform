TabNet Logs:

Saving copy of script...
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 18:57:23
epoch 0  | loss: 0.76469 | val_0_rmse: 0.80897 | val_1_rmse: 0.82155 |  0:00:13s
epoch 1  | loss: 0.61643 | val_0_rmse: 0.77023 | val_1_rmse: 0.78218 |  0:00:19s
epoch 2  | loss: 0.60035 | val_0_rmse: 0.76681 | val_1_rmse: 0.77909 |  0:00:26s
epoch 3  | loss: 0.60334 | val_0_rmse: 0.7791  | val_1_rmse: 0.79122 |  0:00:33s
epoch 4  | loss: 0.60761 | val_0_rmse: 0.76423 | val_1_rmse: 0.78027 |  0:00:40s
epoch 5  | loss: 0.58919 | val_0_rmse: 0.75647 | val_1_rmse: 0.77507 |  0:00:47s
epoch 6  | loss: 0.58174 | val_0_rmse: 0.75782 | val_1_rmse: 0.77616 |  0:00:54s
epoch 7  | loss: 0.58653 | val_0_rmse: 0.75185 | val_1_rmse: 0.77175 |  0:01:01s
epoch 8  | loss: 0.58485 | val_0_rmse: 0.75623 | val_1_rmse: 0.77236 |  0:01:07s
epoch 9  | loss: 0.58179 | val_0_rmse: 0.75411 | val_1_rmse: 0.77073 |  0:01:13s
epoch 10 | loss: 0.57574 | val_0_rmse: 0.74869 | val_1_rmse: 0.76723 |  0:01:19s
epoch 11 | loss: 0.574   | val_0_rmse: 0.7569  | val_1_rmse: 0.77313 |  0:01:26s
epoch 12 | loss: 0.57673 | val_0_rmse: 0.76301 | val_1_rmse: 0.77796 |  0:01:32s
epoch 13 | loss: 0.58612 | val_0_rmse: 0.76257 | val_1_rmse: 0.77547 |  0:01:39s
epoch 14 | loss: 0.58344 | val_0_rmse: 0.7548  | val_1_rmse: 0.77266 |  0:01:44s
epoch 15 | loss: 0.58304 | val_0_rmse: 0.76382 | val_1_rmse: 0.78207 |  0:01:51s
epoch 16 | loss: 0.57863 | val_0_rmse: 0.75163 | val_1_rmse: 0.76716 |  0:01:58s
epoch 17 | loss: 0.58193 | val_0_rmse: 0.76204 | val_1_rmse: 0.7747  |  0:02:05s
epoch 18 | loss: 0.58249 | val_0_rmse: 0.75229 | val_1_rmse: 0.76516 |  0:02:13s
epoch 19 | loss: 0.57652 | val_0_rmse: 0.7504  | val_1_rmse: 0.76927 |  0:02:19s
epoch 20 | loss: 0.5744  | val_0_rmse: 0.75176 | val_1_rmse: 0.76533 |  0:02:26s
epoch 21 | loss: 0.57489 | val_0_rmse: 0.77325 | val_1_rmse: 0.79027 |  0:02:32s
epoch 22 | loss: 0.57332 | val_0_rmse: 0.75398 | val_1_rmse: 0.7705  |  0:02:39s
epoch 23 | loss: 0.5723  | val_0_rmse: 0.74884 | val_1_rmse: 0.76592 |  0:02:45s
epoch 24 | loss: 0.57085 | val_0_rmse: 0.74773 | val_1_rmse: 0.7624  |  0:02:52s
epoch 25 | loss: 0.57079 | val_0_rmse: 0.74628 | val_1_rmse: 0.76418 |  0:02:59s
epoch 26 | loss: 0.56834 | val_0_rmse: 0.74791 | val_1_rmse: 0.76414 |  0:03:05s
epoch 27 | loss: 0.57468 | val_0_rmse: 0.75566 | val_1_rmse: 0.76799 |  0:03:12s
epoch 28 | loss: 0.57138 | val_0_rmse: 0.74421 | val_1_rmse: 0.75977 |  0:03:18s
epoch 29 | loss: 0.56229 | val_0_rmse: 0.7427  | val_1_rmse: 0.76077 |  0:03:25s
epoch 30 | loss: 0.56129 | val_0_rmse: 0.74586 | val_1_rmse: 0.76268 |  0:03:31s
epoch 31 | loss: 0.56384 | val_0_rmse: 0.74309 | val_1_rmse: 0.76393 |  0:03:38s
epoch 32 | loss: 0.56547 | val_0_rmse: 0.77796 | val_1_rmse: 0.79689 |  0:03:45s
epoch 33 | loss: 0.56538 | val_0_rmse: 0.75017 | val_1_rmse: 0.76366 |  0:03:52s
epoch 34 | loss: 0.55452 | val_0_rmse: 0.74316 | val_1_rmse: 0.75853 |  0:03:59s
epoch 35 | loss: 0.54403 | val_0_rmse: 0.74211 | val_1_rmse: 0.75796 |  0:04:06s
epoch 36 | loss: 0.52935 | val_0_rmse: 0.76047 | val_1_rmse: 0.77907 |  0:04:13s
epoch 37 | loss: 0.51978 | val_0_rmse: 0.7476  | val_1_rmse: 0.75415 |  0:04:20s
epoch 38 | loss: 0.51828 | val_0_rmse: 0.80023 | val_1_rmse: 0.8213  |  0:04:26s
epoch 39 | loss: 0.52605 | val_0_rmse: 0.7715  | val_1_rmse: 0.78315 |  0:04:33s
epoch 40 | loss: 0.53219 | val_0_rmse: 0.74188 | val_1_rmse: 0.76114 |  0:04:40s
epoch 41 | loss: 0.50795 | val_0_rmse: 0.74707 | val_1_rmse: 0.76001 |  0:04:47s
epoch 42 | loss: 0.5103  | val_0_rmse: 0.75816 | val_1_rmse: 0.77597 |  0:04:54s
epoch 43 | loss: 0.49498 | val_0_rmse: 0.88546 | val_1_rmse: 0.91016 |  0:05:01s
epoch 44 | loss: 0.50575 | val_0_rmse: 0.74819 | val_1_rmse: 0.75705 |  0:05:07s
epoch 45 | loss: 0.50252 | val_0_rmse: 0.75209 | val_1_rmse: 0.7717  |  0:05:15s
epoch 46 | loss: 0.51131 | val_0_rmse: 0.72258 | val_1_rmse: 0.7329  |  0:05:22s
epoch 47 | loss: 0.49084 | val_0_rmse: 0.75885 | val_1_rmse: 0.76502 |  0:05:28s
epoch 48 | loss: 0.48837 | val_0_rmse: 0.75825 | val_1_rmse: 0.77887 |  0:05:34s
epoch 49 | loss: 0.48768 | val_0_rmse: 0.8699  | val_1_rmse: 0.88012 |  0:05:42s
epoch 50 | loss: 0.47813 | val_0_rmse: 0.70092 | val_1_rmse: 0.71138 |  0:05:48s
epoch 51 | loss: 0.47585 | val_0_rmse: 0.75765 | val_1_rmse: 0.77717 |  0:05:54s
epoch 52 | loss: 0.47588 | val_0_rmse: 0.73676 | val_1_rmse: 0.74968 |  0:06:01s
epoch 53 | loss: 0.47609 | val_0_rmse: 0.76505 | val_1_rmse: 0.77303 |  0:06:07s
epoch 54 | loss: 0.47576 | val_0_rmse: 0.72872 | val_1_rmse: 0.74658 |  0:06:13s
epoch 55 | loss: 0.47085 | val_0_rmse: 0.72282 | val_1_rmse: 0.73951 |  0:06:19s
epoch 56 | loss: 0.46803 | val_0_rmse: 0.74229 | val_1_rmse: 0.76222 |  0:06:24s
epoch 57 | loss: 0.48198 | val_0_rmse: 0.77622 | val_1_rmse: 0.79345 |  0:06:29s
epoch 58 | loss: 0.47655 | val_0_rmse: 0.74765 | val_1_rmse: 0.75186 |  0:06:34s
epoch 59 | loss: 0.47087 | val_0_rmse: 0.7717  | val_1_rmse: 0.78427 |  0:06:39s
epoch 60 | loss: 0.46049 | val_0_rmse: 0.71193 | val_1_rmse: 0.72644 |  0:06:44s
epoch 61 | loss: 0.46356 | val_0_rmse: 0.74352 | val_1_rmse: 0.75191 |  0:06:49s
epoch 62 | loss: 0.47057 | val_0_rmse: 0.73164 | val_1_rmse: 0.74874 |  0:06:54s
epoch 63 | loss: 0.46951 | val_0_rmse: 0.776   | val_1_rmse: 0.79275 |  0:07:01s
epoch 64 | loss: 0.46494 | val_0_rmse: 0.71333 | val_1_rmse: 0.72671 |  0:07:07s
epoch 65 | loss: 0.45928 | val_0_rmse: 0.75832 | val_1_rmse: 0.77813 |  0:07:12s
epoch 66 | loss: 0.46199 | val_0_rmse: 0.72357 | val_1_rmse: 0.73298 |  0:07:17s
epoch 67 | loss: 0.4668  | val_0_rmse: 0.75185 | val_1_rmse: 0.7723  |  0:07:23s
epoch 68 | loss: 0.45667 | val_0_rmse: 0.70025 | val_1_rmse: 0.71361 |  0:07:28s
epoch 69 | loss: 0.45546 | val_0_rmse: 0.76491 | val_1_rmse: 0.77236 |  0:07:34s
epoch 70 | loss: 0.45015 | val_0_rmse: 0.79466 | val_1_rmse: 0.80889 |  0:07:40s
epoch 71 | loss: 0.44756 | val_0_rmse: 0.82452 | val_1_rmse: 0.82435 |  0:07:46s
epoch 72 | loss: 0.46691 | val_0_rmse: 0.74149 | val_1_rmse: 0.75552 |  0:07:53s
epoch 73 | loss: 0.46472 | val_0_rmse: 0.71481 | val_1_rmse: 0.72941 |  0:07:59s
epoch 74 | loss: 0.46083 | val_0_rmse: 0.80183 | val_1_rmse: 0.81694 |  0:08:04s
epoch 75 | loss: 0.45537 | val_0_rmse: 0.74641 | val_1_rmse: 0.76523 |  0:08:11s
epoch 76 | loss: 0.45534 | val_0_rmse: 0.72753 | val_1_rmse: 0.74491 |  0:08:16s
epoch 77 | loss: 0.44945 | val_0_rmse: 0.80768 | val_1_rmse: 0.82127 |  0:08:23s
epoch 78 | loss: 0.44846 | val_0_rmse: 0.71821 | val_1_rmse: 0.72786 |  0:08:29s
epoch 79 | loss: 0.45231 | val_0_rmse: 0.80949 | val_1_rmse: 0.81428 |  0:08:35s
epoch 80 | loss: 0.45951 | val_0_rmse: 0.714   | val_1_rmse: 0.72667 |  0:08:41s

Early stopping occured at epoch 80 with best_epoch = 50 and best_val_1_rmse = 0.71138
Best weights from best epoch are automatically used!
ended training at: 19:06:07
Feature importance:
[('Area', 0.23374515128560874), ('Baths', 0.17008935579090856), ('Beds', 0.06067924730487629), ('Latitude', 0.04454638360363336), ('Longitude', 0.34388722991033527), ('Month', 0.005891953082293216), ('Year', 0.14116067902234455)]
Mean squared error is of 11743095954.089157
Mean absolute error:79366.6046735796
MAPE:0.26347249453106536
R2 score:0.5044008247032171
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:06:08
epoch 0  | loss: 0.78656 | val_0_rmse: 0.81513 | val_1_rmse: 0.8033  |  0:00:05s
epoch 1  | loss: 0.64821 | val_0_rmse: 0.79585 | val_1_rmse: 0.7909  |  0:00:11s
epoch 2  | loss: 0.64872 | val_0_rmse: 0.81149 | val_1_rmse: 0.81589 |  0:00:16s
epoch 3  | loss: 0.6509  | val_0_rmse: 0.7978  | val_1_rmse: 0.79553 |  0:00:22s
epoch 4  | loss: 0.64188 | val_0_rmse: 0.81591 | val_1_rmse: 0.79697 |  0:00:28s
epoch 5  | loss: 0.63075 | val_0_rmse: 0.78195 | val_1_rmse: 0.77938 |  0:00:33s
epoch 6  | loss: 0.61266 | val_0_rmse: 0.76868 | val_1_rmse: 0.76268 |  0:00:39s
epoch 7  | loss: 0.60402 | val_0_rmse: 0.7837  | val_1_rmse: 0.77745 |  0:00:45s
epoch 8  | loss: 0.59953 | val_0_rmse: 0.76456 | val_1_rmse: 0.76123 |  0:00:51s
epoch 9  | loss: 0.59542 | val_0_rmse: 0.7642  | val_1_rmse: 0.76101 |  0:00:57s
epoch 10 | loss: 0.59608 | val_0_rmse: 0.76269 | val_1_rmse: 0.75943 |  0:01:05s
epoch 11 | loss: 0.59274 | val_0_rmse: 0.77787 | val_1_rmse: 0.77324 |  0:01:14s
epoch 12 | loss: 0.59393 | val_0_rmse: 0.76604 | val_1_rmse: 0.76083 |  0:01:21s
epoch 13 | loss: 0.58959 | val_0_rmse: 0.76339 | val_1_rmse: 0.75689 |  0:01:28s
epoch 14 | loss: 0.59563 | val_0_rmse: 0.76867 | val_1_rmse: 0.76211 |  0:01:34s
epoch 15 | loss: 0.59424 | val_0_rmse: 0.75763 | val_1_rmse: 0.74997 |  0:01:40s
epoch 16 | loss: 0.59348 | val_0_rmse: 0.79565 | val_1_rmse: 0.79516 |  0:01:46s
epoch 17 | loss: 0.60102 | val_0_rmse: 0.78437 | val_1_rmse: 0.77587 |  0:01:52s
epoch 18 | loss: 0.60873 | val_0_rmse: 0.77561 | val_1_rmse: 0.77    |  0:01:57s
epoch 19 | loss: 0.61416 | val_0_rmse: 0.76501 | val_1_rmse: 0.76292 |  0:02:03s
epoch 20 | loss: 0.59317 | val_0_rmse: 0.76375 | val_1_rmse: 0.75777 |  0:02:09s
epoch 21 | loss: 0.60234 | val_0_rmse: 0.76158 | val_1_rmse: 0.75633 |  0:02:15s
epoch 22 | loss: 0.58859 | val_0_rmse: 0.76379 | val_1_rmse: 0.76152 |  0:02:22s
epoch 23 | loss: 0.59011 | val_0_rmse: 0.77022 | val_1_rmse: 0.7696  |  0:02:27s
epoch 24 | loss: 0.58836 | val_0_rmse: 0.76203 | val_1_rmse: 0.76238 |  0:02:32s
epoch 25 | loss: 0.59012 | val_0_rmse: 0.76064 | val_1_rmse: 0.75649 |  0:02:39s
epoch 26 | loss: 0.59342 | val_0_rmse: 0.757   | val_1_rmse: 0.75469 |  0:02:46s
epoch 27 | loss: 0.59759 | val_0_rmse: 0.76251 | val_1_rmse: 0.75497 |  0:02:53s
epoch 28 | loss: 0.59488 | val_0_rmse: 0.77164 | val_1_rmse: 0.76698 |  0:02:59s
epoch 29 | loss: 0.58541 | val_0_rmse: 0.75664 | val_1_rmse: 0.75395 |  0:03:05s
epoch 30 | loss: 0.59049 | val_0_rmse: 0.75616 | val_1_rmse: 0.75318 |  0:03:10s
epoch 31 | loss: 0.5859  | val_0_rmse: 0.75317 | val_1_rmse: 0.75104 |  0:03:16s
epoch 32 | loss: 0.5841  | val_0_rmse: 0.75869 | val_1_rmse: 0.7574  |  0:03:22s
epoch 33 | loss: 0.58564 | val_0_rmse: 0.75258 | val_1_rmse: 0.75028 |  0:03:27s
epoch 34 | loss: 0.58497 | val_0_rmse: 0.75275 | val_1_rmse: 0.74974 |  0:03:33s
epoch 35 | loss: 0.58752 | val_0_rmse: 0.77029 | val_1_rmse: 0.76641 |  0:03:39s
epoch 36 | loss: 0.58585 | val_0_rmse: 0.76559 | val_1_rmse: 0.76456 |  0:03:44s
epoch 37 | loss: 0.58796 | val_0_rmse: 0.76695 | val_1_rmse: 0.76263 |  0:03:49s
epoch 38 | loss: 0.58294 | val_0_rmse: 0.76567 | val_1_rmse: 0.76123 |  0:03:55s
epoch 39 | loss: 0.58149 | val_0_rmse: 0.75628 | val_1_rmse: 0.75477 |  0:04:00s
epoch 40 | loss: 0.58507 | val_0_rmse: 0.76632 | val_1_rmse: 0.76188 |  0:04:05s
epoch 41 | loss: 0.59047 | val_0_rmse: 0.76775 | val_1_rmse: 0.76574 |  0:04:10s
epoch 42 | loss: 0.59344 | val_0_rmse: 0.76316 | val_1_rmse: 0.75965 |  0:04:16s
epoch 43 | loss: 0.59185 | val_0_rmse: 0.75899 | val_1_rmse: 0.75663 |  0:04:22s
epoch 44 | loss: 0.58627 | val_0_rmse: 0.75898 | val_1_rmse: 0.75719 |  0:04:30s
epoch 45 | loss: 0.59443 | val_0_rmse: 0.7782  | val_1_rmse: 0.77367 |  0:04:38s
epoch 46 | loss: 0.59231 | val_0_rmse: 0.77293 | val_1_rmse: 0.76324 |  0:04:46s
epoch 47 | loss: 0.6125  | val_0_rmse: 0.77484 | val_1_rmse: 0.76666 |  0:04:52s
epoch 48 | loss: 0.60681 | val_0_rmse: 0.77106 | val_1_rmse: 0.76198 |  0:04:59s
epoch 49 | loss: 0.6004  | val_0_rmse: 0.76096 | val_1_rmse: 0.7538  |  0:05:07s
epoch 50 | loss: 0.59596 | val_0_rmse: 0.76697 | val_1_rmse: 0.76065 |  0:05:14s
epoch 51 | loss: 0.59926 | val_0_rmse: 0.76082 | val_1_rmse: 0.75529 |  0:05:21s
epoch 52 | loss: 0.59907 | val_0_rmse: 0.76609 | val_1_rmse: 0.75799 |  0:05:29s
epoch 53 | loss: 0.59848 | val_0_rmse: 0.77401 | val_1_rmse: 0.76279 |  0:05:37s
epoch 54 | loss: 0.59501 | val_0_rmse: 0.75925 | val_1_rmse: 0.75212 |  0:05:43s
epoch 55 | loss: 0.59789 | val_0_rmse: 0.76368 | val_1_rmse: 0.75697 |  0:05:50s
epoch 56 | loss: 0.59354 | val_0_rmse: 0.7577  | val_1_rmse: 0.74937 |  0:05:56s
epoch 57 | loss: 0.59528 | val_0_rmse: 0.76296 | val_1_rmse: 0.75494 |  0:06:03s
epoch 58 | loss: 0.59713 | val_0_rmse: 0.79022 | val_1_rmse: 0.78816 |  0:06:09s
epoch 59 | loss: 0.63722 | val_0_rmse: 0.79664 | val_1_rmse: 0.79696 |  0:06:14s
epoch 60 | loss: 0.6396  | val_0_rmse: 0.79666 | val_1_rmse: 0.79238 |  0:06:19s
epoch 61 | loss: 0.63644 | val_0_rmse: 0.79536 | val_1_rmse: 0.79375 |  0:06:24s
epoch 62 | loss: 0.63333 | val_0_rmse: 0.7904  | val_1_rmse: 0.78802 |  0:06:29s
epoch 63 | loss: 0.6396  | val_0_rmse: 0.79439 | val_1_rmse: 0.79247 |  0:06:34s
epoch 64 | loss: 0.63401 | val_0_rmse: 0.79047 | val_1_rmse: 0.78769 |  0:06:39s
epoch 65 | loss: 0.63362 | val_0_rmse: 0.79572 | val_1_rmse: 0.79377 |  0:06:44s
epoch 66 | loss: 0.63397 | val_0_rmse: 0.79051 | val_1_rmse: 0.78709 |  0:06:50s
epoch 67 | loss: 0.63449 | val_0_rmse: 0.78996 | val_1_rmse: 0.78649 |  0:06:55s
epoch 68 | loss: 0.63554 | val_0_rmse: 0.79046 | val_1_rmse: 0.78672 |  0:07:01s
epoch 69 | loss: 0.63602 | val_0_rmse: 0.79132 | val_1_rmse: 0.78792 |  0:07:06s
epoch 70 | loss: 0.63629 | val_0_rmse: 0.79752 | val_1_rmse: 0.79277 |  0:07:12s
epoch 71 | loss: 0.63389 | val_0_rmse: 0.78875 | val_1_rmse: 0.78615 |  0:07:18s
epoch 72 | loss: 0.63871 | val_0_rmse: 0.79045 | val_1_rmse: 0.78853 |  0:07:23s
epoch 73 | loss: 0.63646 | val_0_rmse: 0.78809 | val_1_rmse: 0.78365 |  0:07:28s
epoch 74 | loss: 0.63496 | val_0_rmse: 0.78869 | val_1_rmse: 0.78546 |  0:07:33s
epoch 75 | loss: 0.63412 | val_0_rmse: 0.78896 | val_1_rmse: 0.78346 |  0:07:38s
epoch 76 | loss: 0.63314 | val_0_rmse: 0.78831 | val_1_rmse: 0.78549 |  0:07:43s
epoch 77 | loss: 0.63547 | val_0_rmse: 0.78773 | val_1_rmse: 0.78559 |  0:07:49s
epoch 78 | loss: 0.63372 | val_0_rmse: 0.79808 | val_1_rmse: 0.79415 |  0:07:53s
epoch 79 | loss: 0.63438 | val_0_rmse: 0.7859  | val_1_rmse: 0.78391 |  0:07:57s
epoch 80 | loss: 0.63186 | val_0_rmse: 0.79109 | val_1_rmse: 0.78698 |  0:08:02s
epoch 81 | loss: 0.63493 | val_0_rmse: 0.78719 | val_1_rmse: 0.78456 |  0:08:07s
epoch 82 | loss: 0.63233 | val_0_rmse: 0.79171 | val_1_rmse: 0.78816 |  0:08:12s
epoch 83 | loss: 0.63224 | val_0_rmse: 0.79017 | val_1_rmse: 0.78984 |  0:08:16s
epoch 84 | loss: 0.63161 | val_0_rmse: 0.78797 | val_1_rmse: 0.78448 |  0:08:20s
epoch 85 | loss: 0.63125 | val_0_rmse: 0.78938 | val_1_rmse: 0.78747 |  0:08:24s
epoch 86 | loss: 0.63425 | val_0_rmse: 0.78933 | val_1_rmse: 0.78601 |  0:08:27s

Early stopping occured at epoch 86 with best_epoch = 56 and best_val_1_rmse = 0.74937
Best weights from best epoch are automatically used!
ended training at: 19:14:37
Feature importance:
[('Area', 0.48905766358484737), ('Baths', 0.0815143121271423), ('Beds', 0.04373519128312428), ('Latitude', 0.3792650882537274), ('Longitude', 0.0), ('Month', 6.08327919655724e-07), ('Year', 0.006427136423238987)]
Mean squared error is of 13384930511.658068
Mean absolute error:85030.32364494848
MAPE:0.2719946860459682
R2 score:0.41116633449392825
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:14:39
epoch 0  | loss: 0.8087  | val_0_rmse: 0.82048 | val_1_rmse: 0.83255 |  0:00:04s
epoch 1  | loss: 0.6435  | val_0_rmse: 0.78952 | val_1_rmse: 0.79881 |  0:00:09s
epoch 2  | loss: 0.62729 | val_0_rmse: 0.76766 | val_1_rmse: 0.77509 |  0:00:14s
epoch 3  | loss: 0.60675 | val_0_rmse: 0.77671 | val_1_rmse: 0.78532 |  0:00:19s
epoch 4  | loss: 0.59795 | val_0_rmse: 0.77234 | val_1_rmse: 0.78313 |  0:00:24s
epoch 5  | loss: 0.5972  | val_0_rmse: 0.76421 | val_1_rmse: 0.77402 |  0:00:29s
epoch 6  | loss: 0.60336 | val_0_rmse: 0.76928 | val_1_rmse: 0.77754 |  0:00:34s
epoch 7  | loss: 0.5956  | val_0_rmse: 0.76926 | val_1_rmse: 0.7821  |  0:00:39s
epoch 8  | loss: 0.59009 | val_0_rmse: 0.7585  | val_1_rmse: 0.76834 |  0:00:44s
epoch 9  | loss: 0.59582 | val_0_rmse: 0.76408 | val_1_rmse: 0.77624 |  0:00:49s
epoch 10 | loss: 0.58717 | val_0_rmse: 0.76266 | val_1_rmse: 0.77406 |  0:00:54s
epoch 11 | loss: 0.59052 | val_0_rmse: 0.76854 | val_1_rmse: 0.78052 |  0:00:59s
epoch 12 | loss: 0.58459 | val_0_rmse: 0.7715  | val_1_rmse: 0.78087 |  0:01:05s
epoch 13 | loss: 0.58095 | val_0_rmse: 0.75926 | val_1_rmse: 0.77036 |  0:01:10s
epoch 14 | loss: 0.58008 | val_0_rmse: 0.75882 | val_1_rmse: 0.76885 |  0:01:15s
epoch 15 | loss: 0.58172 | val_0_rmse: 0.77117 | val_1_rmse: 0.77316 |  0:01:19s
epoch 16 | loss: 0.58185 | val_0_rmse: 0.76523 | val_1_rmse: 0.774   |  0:01:23s
epoch 17 | loss: 0.57643 | val_0_rmse: 0.76015 | val_1_rmse: 0.77219 |  0:01:27s
epoch 18 | loss: 0.55039 | val_0_rmse: 0.77885 | val_1_rmse: 0.7919  |  0:01:31s
epoch 19 | loss: 0.54181 | val_0_rmse: 0.76871 | val_1_rmse: 0.77762 |  0:01:34s
epoch 20 | loss: 0.52763 | val_0_rmse: 0.76449 | val_1_rmse: 0.77169 |  0:01:38s
epoch 21 | loss: 0.51903 | val_0_rmse: 0.77235 | val_1_rmse: 0.77903 |  0:01:42s
epoch 22 | loss: 0.51299 | val_0_rmse: 0.77161 | val_1_rmse: 0.77661 |  0:01:46s
epoch 23 | loss: 0.51077 | val_0_rmse: 0.77286 | val_1_rmse: 0.78237 |  0:01:50s
epoch 24 | loss: 0.50688 | val_0_rmse: 0.75279 | val_1_rmse: 0.75914 |  0:01:54s
epoch 25 | loss: 0.50111 | val_0_rmse: 0.79432 | val_1_rmse: 0.80407 |  0:01:59s
epoch 26 | loss: 0.49889 | val_0_rmse: 0.75608 | val_1_rmse: 0.76169 |  0:02:04s
epoch 27 | loss: 0.50164 | val_0_rmse: 0.7721  | val_1_rmse: 0.78031 |  0:02:09s
epoch 28 | loss: 0.48789 | val_0_rmse: 0.80672 | val_1_rmse: 0.81477 |  0:02:13s
epoch 29 | loss: 0.49847 | val_0_rmse: 0.77743 | val_1_rmse: 0.78789 |  0:02:18s
epoch 30 | loss: 0.49542 | val_0_rmse: 0.76301 | val_1_rmse: 0.76957 |  0:02:22s
epoch 31 | loss: 0.49028 | val_0_rmse: 0.73886 | val_1_rmse: 0.74642 |  0:02:27s
epoch 32 | loss: 0.49023 | val_0_rmse: 0.74572 | val_1_rmse: 0.75336 |  0:02:32s
epoch 33 | loss: 0.48561 | val_0_rmse: 0.73833 | val_1_rmse: 0.74445 |  0:02:36s
epoch 34 | loss: 0.48366 | val_0_rmse: 0.77344 | val_1_rmse: 0.78229 |  0:02:41s
epoch 35 | loss: 0.48173 | val_0_rmse: 0.79253 | val_1_rmse: 0.80106 |  0:02:46s
epoch 36 | loss: 0.48826 | val_0_rmse: 0.77697 | val_1_rmse: 0.78197 |  0:02:50s
epoch 37 | loss: 0.48427 | val_0_rmse: 0.77292 | val_1_rmse: 0.78258 |  0:02:55s
epoch 38 | loss: 0.48642 | val_0_rmse: 0.81404 | val_1_rmse: 0.82179 |  0:02:59s
epoch 39 | loss: 0.49074 | val_0_rmse: 0.74702 | val_1_rmse: 0.75633 |  0:03:04s
epoch 40 | loss: 0.4792  | val_0_rmse: 0.74029 | val_1_rmse: 0.7473  |  0:03:08s
epoch 41 | loss: 0.47613 | val_0_rmse: 0.74311 | val_1_rmse: 0.74933 |  0:03:14s
epoch 42 | loss: 0.47619 | val_0_rmse: 0.74901 | val_1_rmse: 0.75841 |  0:03:18s
epoch 43 | loss: 0.4753  | val_0_rmse: 0.75351 | val_1_rmse: 0.76192 |  0:03:23s
epoch 44 | loss: 0.47674 | val_0_rmse: 0.75049 | val_1_rmse: 0.75709 |  0:03:28s
epoch 45 | loss: 0.4795  | val_0_rmse: 0.83463 | val_1_rmse: 0.84171 |  0:03:33s
epoch 46 | loss: 0.47539 | val_0_rmse: 0.73517 | val_1_rmse: 0.74042 |  0:03:38s
epoch 47 | loss: 0.47292 | val_0_rmse: 0.74593 | val_1_rmse: 0.75309 |  0:03:42s
epoch 48 | loss: 0.47781 | val_0_rmse: 0.81265 | val_1_rmse: 0.82139 |  0:03:46s
epoch 49 | loss: 0.47761 | val_0_rmse: 0.75739 | val_1_rmse: 0.76355 |  0:03:49s
epoch 50 | loss: 0.48351 | val_0_rmse: 0.78121 | val_1_rmse: 0.79095 |  0:03:53s
epoch 51 | loss: 0.47434 | val_0_rmse: 0.75706 | val_1_rmse: 0.76728 |  0:03:57s
epoch 52 | loss: 0.47183 | val_0_rmse: 0.83143 | val_1_rmse: 0.83984 |  0:04:01s
epoch 53 | loss: 0.4719  | val_0_rmse: 0.76871 | val_1_rmse: 0.78214 |  0:04:06s
epoch 54 | loss: 0.47309 | val_0_rmse: 0.77674 | val_1_rmse: 0.78217 |  0:04:10s
epoch 55 | loss: 0.46853 | val_0_rmse: 0.7478  | val_1_rmse: 0.75746 |  0:04:14s
epoch 56 | loss: 0.47787 | val_0_rmse: 0.77203 | val_1_rmse: 0.77741 |  0:04:18s
epoch 57 | loss: 0.4687  | val_0_rmse: 0.74099 | val_1_rmse: 0.74775 |  0:04:22s
epoch 58 | loss: 0.47781 | val_0_rmse: 0.7399  | val_1_rmse: 0.74928 |  0:04:27s
epoch 59 | loss: 0.46719 | val_0_rmse: 0.74795 | val_1_rmse: 0.75875 |  0:04:31s
epoch 60 | loss: 0.47087 | val_0_rmse: 0.76522 | val_1_rmse: 0.77193 |  0:04:35s
epoch 61 | loss: 0.4675  | val_0_rmse: 1.01946 | val_1_rmse: 1.03154 |  0:04:40s
epoch 62 | loss: 0.48358 | val_0_rmse: 0.79517 | val_1_rmse: 0.80342 |  0:04:44s
epoch 63 | loss: 0.47439 | val_0_rmse: 0.76352 | val_1_rmse: 0.77403 |  0:04:48s
epoch 64 | loss: 0.4643  | val_0_rmse: 0.73813 | val_1_rmse: 0.74719 |  0:04:52s
epoch 65 | loss: 0.46775 | val_0_rmse: 0.78412 | val_1_rmse: 0.79337 |  0:04:57s
epoch 66 | loss: 0.46376 | val_0_rmse: 0.73553 | val_1_rmse: 0.7433  |  0:05:01s
epoch 67 | loss: 0.46256 | val_0_rmse: 0.79146 | val_1_rmse: 0.8062  |  0:05:05s
epoch 68 | loss: 0.46464 | val_0_rmse: 0.75301 | val_1_rmse: 0.76354 |  0:05:10s
epoch 69 | loss: 0.4718  | val_0_rmse: 0.77251 | val_1_rmse: 0.78213 |  0:05:15s
epoch 70 | loss: 0.46559 | val_0_rmse: 0.78794 | val_1_rmse: 0.79848 |  0:05:20s
epoch 71 | loss: 0.46456 | val_0_rmse: 0.76168 | val_1_rmse: 0.77358 |  0:05:25s
epoch 72 | loss: 0.46434 | val_0_rmse: 0.75784 | val_1_rmse: 0.76923 |  0:05:30s
epoch 73 | loss: 0.45671 | val_0_rmse: 0.77036 | val_1_rmse: 0.78008 |  0:05:35s
epoch 74 | loss: 0.46218 | val_0_rmse: 0.78221 | val_1_rmse: 0.78776 |  0:05:42s
epoch 75 | loss: 0.46326 | val_0_rmse: 0.77744 | val_1_rmse: 0.78545 |  0:05:49s
epoch 76 | loss: 0.46572 | val_0_rmse: 0.83252 | val_1_rmse: 0.84437 |  0:05:54s

Early stopping occured at epoch 76 with best_epoch = 46 and best_val_1_rmse = 0.74042
Best weights from best epoch are automatically used!
ended training at: 19:20:34
Feature importance:
[('Area', 0.4507790760009084), ('Baths', 0.09612913697382962), ('Beds', 0.09755826503104904), ('Latitude', 0.20158647825552736), ('Longitude', 0.0), ('Month', 0.006301442718676782), ('Year', 0.1476456010200088)]
Mean squared error is of 12095672081.114412
Mean absolute error:82723.93630303626
MAPE:0.2730246277296213
R2 score:0.44991837972504367
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:20:36
epoch 0  | loss: 0.8063  | val_0_rmse: 0.83178 | val_1_rmse: 0.82158 |  0:00:06s
epoch 1  | loss: 0.67879 | val_0_rmse: 0.80115 | val_1_rmse: 0.78838 |  0:00:13s
epoch 2  | loss: 0.6467  | val_0_rmse: 0.79776 | val_1_rmse: 0.78539 |  0:00:19s
epoch 3  | loss: 0.6302  | val_0_rmse: 0.78001 | val_1_rmse: 0.77172 |  0:00:25s
epoch 4  | loss: 0.61752 | val_0_rmse: 0.77526 | val_1_rmse: 0.76494 |  0:00:31s
epoch 5  | loss: 0.60767 | val_0_rmse: 0.77887 | val_1_rmse: 0.77132 |  0:00:36s
epoch 6  | loss: 0.60446 | val_0_rmse: 0.76965 | val_1_rmse: 0.76086 |  0:00:42s
epoch 7  | loss: 0.6015  | val_0_rmse: 0.76109 | val_1_rmse: 0.75342 |  0:00:48s
epoch 8  | loss: 0.60244 | val_0_rmse: 0.76328 | val_1_rmse: 0.75428 |  0:00:54s
epoch 9  | loss: 0.60242 | val_0_rmse: 0.77158 | val_1_rmse: 0.7645  |  0:01:00s
epoch 10 | loss: 0.60066 | val_0_rmse: 0.76651 | val_1_rmse: 0.75926 |  0:01:06s
epoch 11 | loss: 0.59693 | val_0_rmse: 0.77114 | val_1_rmse: 0.76538 |  0:01:12s
epoch 12 | loss: 0.59815 | val_0_rmse: 0.76106 | val_1_rmse: 0.75508 |  0:01:18s
epoch 13 | loss: 0.59637 | val_0_rmse: 0.76236 | val_1_rmse: 0.75462 |  0:01:24s
epoch 14 | loss: 0.5944  | val_0_rmse: 0.76228 | val_1_rmse: 0.75391 |  0:01:30s
epoch 15 | loss: 0.5899  | val_0_rmse: 0.75983 | val_1_rmse: 0.75042 |  0:01:36s
epoch 16 | loss: 0.60294 | val_0_rmse: 0.7718  | val_1_rmse: 0.76115 |  0:01:42s
epoch 17 | loss: 0.60251 | val_0_rmse: 0.77201 | val_1_rmse: 0.76392 |  0:01:48s
epoch 18 | loss: 0.60424 | val_0_rmse: 0.76143 | val_1_rmse: 0.75359 |  0:01:54s
epoch 19 | loss: 0.59384 | val_0_rmse: 0.76869 | val_1_rmse: 0.76113 |  0:02:00s
epoch 20 | loss: 0.60169 | val_0_rmse: 0.78493 | val_1_rmse: 0.7827  |  0:02:06s
epoch 21 | loss: 0.59468 | val_0_rmse: 0.76408 | val_1_rmse: 0.75856 |  0:02:11s
epoch 22 | loss: 0.5886  | val_0_rmse: 0.76567 | val_1_rmse: 0.75876 |  0:02:17s
epoch 23 | loss: 0.59017 | val_0_rmse: 0.77084 | val_1_rmse: 0.75866 |  0:02:23s
epoch 24 | loss: 0.58979 | val_0_rmse: 0.75683 | val_1_rmse: 0.74862 |  0:02:29s
epoch 25 | loss: 0.58706 | val_0_rmse: 0.7553  | val_1_rmse: 0.74872 |  0:02:36s
epoch 26 | loss: 0.58683 | val_0_rmse: 0.76    | val_1_rmse: 0.75294 |  0:02:42s
epoch 27 | loss: 0.58455 | val_0_rmse: 0.7682  | val_1_rmse: 0.76356 |  0:02:47s
epoch 28 | loss: 0.58614 | val_0_rmse: 0.75522 | val_1_rmse: 0.74669 |  0:02:52s
epoch 29 | loss: 0.59102 | val_0_rmse: 0.76014 | val_1_rmse: 0.7519  |  0:02:58s
epoch 30 | loss: 0.59066 | val_0_rmse: 0.76071 | val_1_rmse: 0.75611 |  0:03:04s
epoch 31 | loss: 0.5844  | val_0_rmse: 0.7564  | val_1_rmse: 0.75035 |  0:03:09s
epoch 32 | loss: 0.58195 | val_0_rmse: 0.76146 | val_1_rmse: 0.75519 |  0:03:12s
epoch 33 | loss: 0.58948 | val_0_rmse: 0.75932 | val_1_rmse: 0.75225 |  0:03:16s
epoch 34 | loss: 0.58532 | val_0_rmse: 0.75843 | val_1_rmse: 0.75103 |  0:03:21s
epoch 35 | loss: 0.58543 | val_0_rmse: 0.75307 | val_1_rmse: 0.74585 |  0:03:26s
epoch 36 | loss: 0.58493 | val_0_rmse: 0.76328 | val_1_rmse: 0.75634 |  0:03:31s
epoch 37 | loss: 0.58914 | val_0_rmse: 0.75793 | val_1_rmse: 0.74852 |  0:03:36s
epoch 38 | loss: 0.58188 | val_0_rmse: 0.75902 | val_1_rmse: 0.75399 |  0:03:41s
epoch 39 | loss: 0.59079 | val_0_rmse: 0.7583  | val_1_rmse: 0.75174 |  0:03:45s
epoch 40 | loss: 0.58375 | val_0_rmse: 0.75881 | val_1_rmse: 0.75261 |  0:03:49s
epoch 41 | loss: 0.5833  | val_0_rmse: 0.75647 | val_1_rmse: 0.75316 |  0:03:54s
epoch 42 | loss: 0.586   | val_0_rmse: 0.75993 | val_1_rmse: 0.75315 |  0:04:00s
epoch 43 | loss: 0.58563 | val_0_rmse: 0.75618 | val_1_rmse: 0.75163 |  0:04:06s
epoch 44 | loss: 0.58656 | val_0_rmse: 0.76395 | val_1_rmse: 0.75716 |  0:04:12s
epoch 45 | loss: 0.59079 | val_0_rmse: 0.76741 | val_1_rmse: 0.76262 |  0:04:18s
epoch 46 | loss: 0.58619 | val_0_rmse: 0.75594 | val_1_rmse: 0.74856 |  0:04:23s
epoch 47 | loss: 0.5797  | val_0_rmse: 0.75925 | val_1_rmse: 0.75159 |  0:04:29s
epoch 48 | loss: 0.58472 | val_0_rmse: 0.76329 | val_1_rmse: 0.75736 |  0:04:34s
epoch 49 | loss: 0.59365 | val_0_rmse: 0.76515 | val_1_rmse: 0.75818 |  0:04:39s
epoch 50 | loss: 0.59172 | val_0_rmse: 0.75499 | val_1_rmse: 0.74747 |  0:04:45s
epoch 51 | loss: 0.58365 | val_0_rmse: 0.75729 | val_1_rmse: 0.75105 |  0:04:50s
epoch 52 | loss: 0.5797  | val_0_rmse: 0.75911 | val_1_rmse: 0.75367 |  0:04:56s
epoch 53 | loss: 0.58121 | val_0_rmse: 0.76043 | val_1_rmse: 0.75472 |  0:05:01s
epoch 54 | loss: 0.58652 | val_0_rmse: 0.76738 | val_1_rmse: 0.75998 |  0:05:07s
epoch 55 | loss: 0.58537 | val_0_rmse: 0.75929 | val_1_rmse: 0.7521  |  0:05:13s
epoch 56 | loss: 0.58429 | val_0_rmse: 0.76311 | val_1_rmse: 0.75732 |  0:05:16s
epoch 57 | loss: 0.58726 | val_0_rmse: 0.7759  | val_1_rmse: 0.76933 |  0:05:20s
epoch 58 | loss: 0.58514 | val_0_rmse: 0.75593 | val_1_rmse: 0.74896 |  0:05:24s
epoch 59 | loss: 0.57939 | val_0_rmse: 0.76436 | val_1_rmse: 0.75882 |  0:05:29s
epoch 60 | loss: 0.58048 | val_0_rmse: 0.75328 | val_1_rmse: 0.74645 |  0:05:34s
epoch 61 | loss: 0.58039 | val_0_rmse: 0.74989 | val_1_rmse: 0.74154 |  0:05:38s
epoch 62 | loss: 0.57715 | val_0_rmse: 0.75422 | val_1_rmse: 0.74858 |  0:05:45s
epoch 63 | loss: 0.57794 | val_0_rmse: 0.77105 | val_1_rmse: 0.76135 |  0:05:50s
epoch 64 | loss: 0.57892 | val_0_rmse: 0.75091 | val_1_rmse: 0.7433  |  0:05:56s
epoch 65 | loss: 0.57603 | val_0_rmse: 0.75158 | val_1_rmse: 0.74403 |  0:06:02s
epoch 66 | loss: 0.5798  | val_0_rmse: 0.75486 | val_1_rmse: 0.74962 |  0:06:09s
epoch 67 | loss: 0.58573 | val_0_rmse: 0.75742 | val_1_rmse: 0.7513  |  0:06:14s
epoch 68 | loss: 0.57808 | val_0_rmse: 0.75247 | val_1_rmse: 0.74685 |  0:06:20s
epoch 69 | loss: 0.57956 | val_0_rmse: 0.75637 | val_1_rmse: 0.74888 |  0:06:26s
epoch 70 | loss: 0.58164 | val_0_rmse: 0.75472 | val_1_rmse: 0.74703 |  0:06:32s
epoch 71 | loss: 0.58575 | val_0_rmse: 0.75725 | val_1_rmse: 0.75104 |  0:06:38s
epoch 72 | loss: 0.57952 | val_0_rmse: 0.75283 | val_1_rmse: 0.74545 |  0:06:43s
epoch 73 | loss: 0.58507 | val_0_rmse: 0.75529 | val_1_rmse: 0.75058 |  0:06:50s
epoch 74 | loss: 0.57778 | val_0_rmse: 0.76277 | val_1_rmse: 0.75701 |  0:06:55s
epoch 75 | loss: 0.57857 | val_0_rmse: 0.75763 | val_1_rmse: 0.75186 |  0:07:01s
epoch 76 | loss: 0.58011 | val_0_rmse: 0.75608 | val_1_rmse: 0.75167 |  0:07:08s
epoch 77 | loss: 0.58316 | val_0_rmse: 0.76929 | val_1_rmse: 0.76341 |  0:07:13s
epoch 78 | loss: 0.58151 | val_0_rmse: 0.76008 | val_1_rmse: 0.75268 |  0:07:16s
epoch 79 | loss: 0.5846  | val_0_rmse: 0.7652  | val_1_rmse: 0.75919 |  0:07:21s
epoch 80 | loss: 0.5852  | val_0_rmse: 0.75945 | val_1_rmse: 0.75306 |  0:07:26s
epoch 81 | loss: 0.58896 | val_0_rmse: 0.76621 | val_1_rmse: 0.75924 |  0:07:31s
epoch 82 | loss: 0.58135 | val_0_rmse: 0.75291 | val_1_rmse: 0.74551 |  0:07:35s
epoch 83 | loss: 0.59081 | val_0_rmse: 0.78886 | val_1_rmse: 0.78287 |  0:07:41s
epoch 84 | loss: 0.59604 | val_0_rmse: 0.7626  | val_1_rmse: 0.75502 |  0:07:47s
epoch 85 | loss: 0.5875  | val_0_rmse: 0.76121 | val_1_rmse: 0.7554  |  0:07:52s
epoch 86 | loss: 0.58668 | val_0_rmse: 0.76537 | val_1_rmse: 0.75931 |  0:07:59s
epoch 87 | loss: 0.58746 | val_0_rmse: 0.76627 | val_1_rmse: 0.7634  |  0:08:05s
epoch 88 | loss: 0.58765 | val_0_rmse: 0.76373 | val_1_rmse: 0.75907 |  0:08:10s
epoch 89 | loss: 0.58365 | val_0_rmse: 0.75519 | val_1_rmse: 0.74917 |  0:08:16s
epoch 90 | loss: 0.58967 | val_0_rmse: 0.77031 | val_1_rmse: 0.76589 |  0:08:21s
epoch 91 | loss: 0.58778 | val_0_rmse: 0.76096 | val_1_rmse: 0.75541 |  0:08:27s

Early stopping occured at epoch 91 with best_epoch = 61 and best_val_1_rmse = 0.74154
Best weights from best epoch are automatically used!
ended training at: 19:29:05
Feature importance:
[('Area', 0.29314297911255804), ('Baths', 0.09431298987562059), ('Beds', 0.18209573180041566), ('Latitude', 0.3115168554630451), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.11893144374836064)]
Mean squared error is of 12747010407.885557
Mean absolute error:82858.2320447852
MAPE:0.2646084745850894
R2 score:0.4396065473246453
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all perth.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:29:07
epoch 0  | loss: 0.80823 | val_0_rmse: 0.82426 | val_1_rmse: 0.8126  |  0:00:06s
epoch 1  | loss: 0.65184 | val_0_rmse: 0.80949 | val_1_rmse: 0.7953  |  0:00:12s
epoch 2  | loss: 0.64261 | val_0_rmse: 0.80361 | val_1_rmse: 0.79238 |  0:00:18s
epoch 3  | loss: 0.6303  | val_0_rmse: 0.7942  | val_1_rmse: 0.77839 |  0:00:24s
epoch 4  | loss: 0.62038 | val_0_rmse: 0.78335 | val_1_rmse: 0.77187 |  0:00:30s
epoch 5  | loss: 0.61537 | val_0_rmse: 0.78467 | val_1_rmse: 0.77427 |  0:00:36s
epoch 6  | loss: 0.61311 | val_0_rmse: 0.77328 | val_1_rmse: 0.7642  |  0:00:42s
epoch 7  | loss: 0.61027 | val_0_rmse: 0.77395 | val_1_rmse: 0.7641  |  0:00:48s
epoch 8  | loss: 0.61262 | val_0_rmse: 0.7696  | val_1_rmse: 0.75883 |  0:00:54s
epoch 9  | loss: 0.61058 | val_0_rmse: 0.77226 | val_1_rmse: 0.76054 |  0:01:01s
epoch 10 | loss: 0.60848 | val_0_rmse: 0.77264 | val_1_rmse: 0.76265 |  0:01:07s
epoch 11 | loss: 0.60811 | val_0_rmse: 0.7667  | val_1_rmse: 0.75651 |  0:01:13s
epoch 12 | loss: 0.60416 | val_0_rmse: 0.76568 | val_1_rmse: 0.7569  |  0:01:18s
epoch 13 | loss: 0.59923 | val_0_rmse: 0.76404 | val_1_rmse: 0.75579 |  0:01:24s
epoch 14 | loss: 0.5995  | val_0_rmse: 0.76486 | val_1_rmse: 0.75651 |  0:01:30s
epoch 15 | loss: 0.60107 | val_0_rmse: 0.76289 | val_1_rmse: 0.7555  |  0:01:37s
epoch 16 | loss: 0.59818 | val_0_rmse: 0.76651 | val_1_rmse: 0.75827 |  0:01:43s
epoch 17 | loss: 0.59743 | val_0_rmse: 0.76152 | val_1_rmse: 0.7559  |  0:01:49s
epoch 18 | loss: 0.60002 | val_0_rmse: 0.76877 | val_1_rmse: 0.76423 |  0:01:56s
epoch 19 | loss: 0.59993 | val_0_rmse: 0.76833 | val_1_rmse: 0.76148 |  0:02:02s
epoch 20 | loss: 0.5971  | val_0_rmse: 0.76441 | val_1_rmse: 0.75904 |  0:02:09s
epoch 21 | loss: 0.59337 | val_0_rmse: 0.77148 | val_1_rmse: 0.76654 |  0:02:16s
epoch 22 | loss: 0.5944  | val_0_rmse: 0.76101 | val_1_rmse: 0.75457 |  0:02:23s
epoch 23 | loss: 0.59203 | val_0_rmse: 0.76473 | val_1_rmse: 0.75921 |  0:02:30s
epoch 24 | loss: 0.5921  | val_0_rmse: 0.75873 | val_1_rmse: 0.75156 |  0:02:36s
epoch 25 | loss: 0.59159 | val_0_rmse: 0.76178 | val_1_rmse: 0.75777 |  0:02:43s
epoch 26 | loss: 0.5939  | val_0_rmse: 0.76779 | val_1_rmse: 0.76328 |  0:02:49s
epoch 27 | loss: 0.5938  | val_0_rmse: 0.75939 | val_1_rmse: 0.75618 |  0:02:55s
epoch 28 | loss: 0.58943 | val_0_rmse: 0.76115 | val_1_rmse: 0.75754 |  0:03:02s
epoch 29 | loss: 0.58935 | val_0_rmse: 0.76084 | val_1_rmse: 0.75727 |  0:03:09s
epoch 30 | loss: 0.5922  | val_0_rmse: 0.75657 | val_1_rmse: 0.75481 |  0:03:15s
epoch 31 | loss: 0.58907 | val_0_rmse: 0.76887 | val_1_rmse: 0.76703 |  0:03:22s
epoch 32 | loss: 0.59081 | val_0_rmse: 0.76199 | val_1_rmse: 0.75424 |  0:03:26s
epoch 33 | loss: 0.59047 | val_0_rmse: 0.75805 | val_1_rmse: 0.75203 |  0:03:29s
epoch 34 | loss: 0.59114 | val_0_rmse: 0.75832 | val_1_rmse: 0.75352 |  0:03:34s
epoch 35 | loss: 0.58894 | val_0_rmse: 0.75893 | val_1_rmse: 0.75584 |  0:03:39s
epoch 36 | loss: 0.58813 | val_0_rmse: 0.76389 | val_1_rmse: 0.7596  |  0:03:44s
epoch 37 | loss: 0.5925  | val_0_rmse: 0.75746 | val_1_rmse: 0.75217 |  0:03:49s
epoch 38 | loss: 0.58635 | val_0_rmse: 0.76243 | val_1_rmse: 0.75658 |  0:03:53s
epoch 39 | loss: 0.59276 | val_0_rmse: 0.7552  | val_1_rmse: 0.74952 |  0:03:58s
epoch 40 | loss: 0.58846 | val_0_rmse: 0.76135 | val_1_rmse: 0.75611 |  0:04:02s
epoch 41 | loss: 0.58356 | val_0_rmse: 0.76089 | val_1_rmse: 0.75567 |  0:04:08s
epoch 42 | loss: 0.58318 | val_0_rmse: 0.76201 | val_1_rmse: 0.75748 |  0:04:14s
epoch 43 | loss: 0.57959 | val_0_rmse: 0.76707 | val_1_rmse: 0.76264 |  0:04:20s
epoch 44 | loss: 0.57551 | val_0_rmse: 0.75966 | val_1_rmse: 0.75434 |  0:04:26s
epoch 45 | loss: 0.5598  | val_0_rmse: 0.75426 | val_1_rmse: 0.747   |  0:04:32s
epoch 46 | loss: 0.54037 | val_0_rmse: 0.74692 | val_1_rmse: 0.7358  |  0:04:38s
epoch 47 | loss: 0.54277 | val_0_rmse: 0.74693 | val_1_rmse: 0.73762 |  0:04:44s
epoch 48 | loss: 0.54399 | val_0_rmse: 0.77294 | val_1_rmse: 0.76562 |  0:04:49s
epoch 49 | loss: 0.5361  | val_0_rmse: 0.75573 | val_1_rmse: 0.74523 |  0:04:52s
epoch 50 | loss: 0.53299 | val_0_rmse: 0.82927 | val_1_rmse: 0.82593 |  0:04:57s
epoch 51 | loss: 0.53432 | val_0_rmse: 0.788   | val_1_rmse: 0.78361 |  0:05:02s
epoch 52 | loss: 0.52633 | val_0_rmse: 0.76494 | val_1_rmse: 0.75707 |  0:05:07s
epoch 53 | loss: 0.52591 | val_0_rmse: 0.78679 | val_1_rmse: 0.78059 |  0:05:12s
epoch 54 | loss: 0.5168  | val_0_rmse: 0.81487 | val_1_rmse: 0.81087 |  0:05:17s
epoch 55 | loss: 0.5232  | val_0_rmse: 0.75348 | val_1_rmse: 0.74863 |  0:05:22s
epoch 56 | loss: 0.51751 | val_0_rmse: 0.76867 | val_1_rmse: 0.7662  |  0:05:28s
epoch 57 | loss: 0.51088 | val_0_rmse: 0.78651 | val_1_rmse: 0.77313 |  0:05:34s
epoch 58 | loss: 0.50507 | val_0_rmse: 0.77718 | val_1_rmse: 0.7726  |  0:05:39s
epoch 59 | loss: 0.50837 | val_0_rmse: 0.80194 | val_1_rmse: 0.79912 |  0:05:45s
epoch 60 | loss: 0.50498 | val_0_rmse: 0.73804 | val_1_rmse: 0.73308 |  0:05:51s
epoch 61 | loss: 0.50269 | val_0_rmse: 0.82518 | val_1_rmse: 0.80957 |  0:05:57s
epoch 62 | loss: 0.49871 | val_0_rmse: 0.76093 | val_1_rmse: 0.75383 |  0:06:03s
epoch 63 | loss: 0.49882 | val_0_rmse: 0.75151 | val_1_rmse: 0.74479 |  0:06:09s
epoch 64 | loss: 0.49697 | val_0_rmse: 0.80971 | val_1_rmse: 0.80292 |  0:06:15s
epoch 65 | loss: 0.50314 | val_0_rmse: 0.75364 | val_1_rmse: 0.74622 |  0:06:21s
epoch 66 | loss: 0.48168 | val_0_rmse: 0.80816 | val_1_rmse: 0.80695 |  0:06:28s
epoch 67 | loss: 0.49031 | val_0_rmse: 0.80765 | val_1_rmse: 0.80407 |  0:06:33s
epoch 68 | loss: 0.49147 | val_0_rmse: 0.76239 | val_1_rmse: 0.75653 |  0:06:36s
epoch 69 | loss: 0.49581 | val_0_rmse: 0.95135 | val_1_rmse: 0.95117 |  0:06:40s
epoch 70 | loss: 0.48985 | val_0_rmse: 0.73033 | val_1_rmse: 0.72368 |  0:06:45s
epoch 71 | loss: 0.49212 | val_0_rmse: 0.82261 | val_1_rmse: 0.82199 |  0:06:50s
epoch 72 | loss: 0.49278 | val_0_rmse: 0.76534 | val_1_rmse: 0.76214 |  0:06:55s
epoch 73 | loss: 0.48905 | val_0_rmse: 0.78458 | val_1_rmse: 0.78453 |  0:07:01s
epoch 74 | loss: 0.48781 | val_0_rmse: 0.7939  | val_1_rmse: 0.79501 |  0:07:07s
epoch 75 | loss: 0.48169 | val_0_rmse: 0.78416 | val_1_rmse: 0.78241 |  0:07:12s
epoch 76 | loss: 0.48225 | val_0_rmse: 0.77097 | val_1_rmse: 0.76414 |  0:07:18s
epoch 77 | loss: 0.48463 | val_0_rmse: 0.73426 | val_1_rmse: 0.72801 |  0:07:24s
epoch 78 | loss: 0.48444 | val_0_rmse: 0.7532  | val_1_rmse: 0.74275 |  0:07:30s
epoch 79 | loss: 0.48332 | val_0_rmse: 0.77076 | val_1_rmse: 0.76429 |  0:07:36s
epoch 80 | loss: 0.47577 | val_0_rmse: 0.79542 | val_1_rmse: 0.75815 |  0:07:39s
epoch 81 | loss: 0.48086 | val_0_rmse: 0.76927 | val_1_rmse: 0.76819 |  0:07:43s
epoch 82 | loss: 0.48271 | val_0_rmse: 0.76833 | val_1_rmse: 0.76594 |  0:07:48s
epoch 83 | loss: 0.47611 | val_0_rmse: 0.78831 | val_1_rmse: 0.7911  |  0:07:54s
epoch 84 | loss: 0.48042 | val_0_rmse: 0.75219 | val_1_rmse: 0.74945 |  0:07:59s
epoch 85 | loss: 0.47375 | val_0_rmse: 0.76656 | val_1_rmse: 0.76364 |  0:08:03s
epoch 86 | loss: 0.48045 | val_0_rmse: 0.74797 | val_1_rmse: 0.74331 |  0:08:08s
epoch 87 | loss: 0.47391 | val_0_rmse: 0.79229 | val_1_rmse: 0.79448 |  0:08:14s
epoch 88 | loss: 0.46674 | val_0_rmse: 0.7572  | val_1_rmse: 0.75484 |  0:08:19s
epoch 89 | loss: 0.47722 | val_0_rmse: 0.7426  | val_1_rmse: 0.73764 |  0:08:25s
epoch 90 | loss: 0.47287 | val_0_rmse: 0.79717 | val_1_rmse: 0.80516 |  0:08:30s
epoch 91 | loss: 0.47974 | val_0_rmse: 0.77028 | val_1_rmse: 0.77178 |  0:08:36s
epoch 92 | loss: 0.47284 | val_0_rmse: 0.81727 | val_1_rmse: 0.81604 |  0:08:42s
epoch 93 | loss: 0.46912 | val_0_rmse: 0.73893 | val_1_rmse: 0.73335 |  0:08:48s
epoch 94 | loss: 0.47141 | val_0_rmse: 0.74639 | val_1_rmse: 0.74277 |  0:08:54s
epoch 95 | loss: 0.46492 | val_0_rmse: 0.81031 | val_1_rmse: 0.76199 |  0:08:59s
epoch 96 | loss: 0.46741 | val_0_rmse: 0.73425 | val_1_rmse: 0.72845 |  0:09:05s
epoch 97 | loss: 0.4605  | val_0_rmse: 0.76097 | val_1_rmse: 0.76312 |  0:09:10s
epoch 98 | loss: 0.47086 | val_0_rmse: 0.8137  | val_1_rmse: 0.81039 |  0:09:15s
epoch 99 | loss: 0.46412 | val_0_rmse: 0.74002 | val_1_rmse: 0.73504 |  0:09:21s
epoch 100| loss: 0.46802 | val_0_rmse: 0.76616 | val_1_rmse: 0.75972 |  0:09:27s

Early stopping occured at epoch 100 with best_epoch = 70 and best_val_1_rmse = 0.72368
Best weights from best epoch are automatically used!
ended training at: 19:38:36
Feature importance:
[('Area', 0.5100896375227658), ('Baths', 0.0), ('Beds', 0.13846321167586986), ('Latitude', 0.16812017970782453), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.18332697109353982)]
Mean squared error is of 11390533973.23191
Mean absolute error:77968.9404492582
MAPE:0.25232602977104945
R2 score:0.4846343756583351
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 19:38:37
epoch 0  | loss: 0.6079  | val_0_rmse: 0.73439 | val_1_rmse: 0.73917 |  0:00:12s
epoch 1  | loss: 0.48958 | val_0_rmse: 0.65932 | val_1_rmse: 0.66938 |  0:00:25s
epoch 2  | loss: 0.43898 | val_0_rmse: 0.7736  | val_1_rmse: 0.77749 |  0:00:40s
epoch 3  | loss: 0.41751 | val_0_rmse: 0.66014 | val_1_rmse: 0.66409 |  0:00:57s
epoch 4  | loss: 0.40847 | val_0_rmse: 0.64237 | val_1_rmse: 0.64942 |  0:01:13s
epoch 5  | loss: 0.39703 | val_0_rmse: 0.63189 | val_1_rmse: 0.6381  |  0:01:28s
epoch 6  | loss: 0.39027 | val_0_rmse: 0.61215 | val_1_rmse: 0.61715 |  0:01:43s
epoch 7  | loss: 0.38852 | val_0_rmse: 0.67963 | val_1_rmse: 0.6862  |  0:02:00s
epoch 8  | loss: 0.38592 | val_0_rmse: 0.76883 | val_1_rmse: 0.77075 |  0:02:17s
epoch 9  | loss: 0.3911  | val_0_rmse: 0.7056  | val_1_rmse: 0.71001 |  0:02:28s
epoch 10 | loss: 0.38274 | val_0_rmse: 0.60837 | val_1_rmse: 0.61153 |  0:02:41s
epoch 11 | loss: 0.38225 | val_0_rmse: 0.73533 | val_1_rmse: 0.73902 |  0:02:55s
epoch 12 | loss: 0.3802  | val_0_rmse: 0.61464 | val_1_rmse: 0.61846 |  0:03:10s
epoch 13 | loss: 0.38158 | val_0_rmse: 0.63422 | val_1_rmse: 0.63828 |  0:03:26s
epoch 14 | loss: 0.37471 | val_0_rmse: 0.62164 | val_1_rmse: 0.62562 |  0:03:42s
epoch 15 | loss: 0.37468 | val_0_rmse: 0.61653 | val_1_rmse: 0.62114 |  0:03:58s
epoch 16 | loss: 0.37484 | val_0_rmse: 0.65795 | val_1_rmse: 0.65975 |  0:04:15s
epoch 17 | loss: 0.37156 | val_0_rmse: 0.60763 | val_1_rmse: 0.61159 |  0:04:31s
epoch 18 | loss: 0.369   | val_0_rmse: 0.61586 | val_1_rmse: 0.62251 |  0:04:46s
epoch 19 | loss: 0.37134 | val_0_rmse: 0.64383 | val_1_rmse: 0.64763 |  0:04:57s
epoch 20 | loss: 0.36835 | val_0_rmse: 0.62794 | val_1_rmse: 0.63224 |  0:05:10s
epoch 21 | loss: 0.369   | val_0_rmse: 0.65247 | val_1_rmse: 0.65869 |  0:05:26s
epoch 22 | loss: 0.36643 | val_0_rmse: 0.64803 | val_1_rmse: 0.65239 |  0:05:42s
epoch 23 | loss: 0.36185 | val_0_rmse: 0.59472 | val_1_rmse: 0.60035 |  0:05:59s
epoch 24 | loss: 0.35921 | val_0_rmse: 0.80991 | val_1_rmse: 0.81186 |  0:06:17s
epoch 25 | loss: 0.36493 | val_0_rmse: 0.64513 | val_1_rmse: 0.65101 |  0:06:33s
epoch 26 | loss: 0.37006 | val_0_rmse: 0.62195 | val_1_rmse: 0.62397 |  0:06:49s
epoch 27 | loss: 0.36118 | val_0_rmse: 0.68405 | val_1_rmse: 0.68925 |  0:07:05s
epoch 28 | loss: 0.36345 | val_0_rmse: 0.60372 | val_1_rmse: 0.60879 |  0:07:22s
epoch 29 | loss: 0.35723 | val_0_rmse: 0.62995 | val_1_rmse: 0.63336 |  0:07:38s
epoch 30 | loss: 0.3588  | val_0_rmse: 0.66074 | val_1_rmse: 0.66613 |  0:07:54s
epoch 31 | loss: 0.35648 | val_0_rmse: 0.59463 | val_1_rmse: 0.59705 |  0:08:05s
epoch 32 | loss: 0.35692 | val_0_rmse: 0.58847 | val_1_rmse: 0.59224 |  0:08:19s
epoch 33 | loss: 0.35353 | val_0_rmse: 0.6046  | val_1_rmse: 0.60975 |  0:08:34s
epoch 34 | loss: 0.35343 | val_0_rmse: 0.67529 | val_1_rmse: 0.67945 |  0:08:50s
epoch 35 | loss: 0.35581 | val_0_rmse: 0.60813 | val_1_rmse: 0.61341 |  0:09:07s
epoch 36 | loss: 0.35883 | val_0_rmse: 0.60166 | val_1_rmse: 0.60369 |  0:09:24s
epoch 37 | loss: 0.36198 | val_0_rmse: 0.6122  | val_1_rmse: 0.6147  |  0:09:39s
epoch 38 | loss: 0.35775 | val_0_rmse: 0.60274 | val_1_rmse: 0.60835 |  0:09:55s
epoch 39 | loss: 0.35407 | val_0_rmse: 0.5936  | val_1_rmse: 0.5966  |  0:10:11s
epoch 40 | loss: 0.35337 | val_0_rmse: 0.59003 | val_1_rmse: 0.59462 |  0:10:28s
epoch 41 | loss: 0.35071 | val_0_rmse: 0.59156 | val_1_rmse: 0.5942  |  0:10:44s
epoch 42 | loss: 0.35601 | val_0_rmse: 0.60215 | val_1_rmse: 0.60611 |  0:11:02s
epoch 43 | loss: 0.35563 | val_0_rmse: 0.75424 | val_1_rmse: 0.75676 |  0:11:13s
epoch 44 | loss: 0.35802 | val_0_rmse: 0.61106 | val_1_rmse: 0.61515 |  0:11:26s
epoch 45 | loss: 0.35472 | val_0_rmse: 0.61095 | val_1_rmse: 0.61563 |  0:11:41s
epoch 46 | loss: 0.35605 | val_0_rmse: 0.60859 | val_1_rmse: 0.61275 |  0:11:55s
epoch 47 | loss: 0.35384 | val_0_rmse: 0.62391 | val_1_rmse: 0.63022 |  0:12:09s
epoch 48 | loss: 0.35017 | val_0_rmse: 0.65054 | val_1_rmse: 0.65209 |  0:12:26s
epoch 49 | loss: 0.35228 | val_0_rmse: 0.64791 | val_1_rmse: 0.65163 |  0:12:43s
epoch 50 | loss: 0.3495  | val_0_rmse: 0.71266 | val_1_rmse: 0.71682 |  0:12:58s
epoch 51 | loss: 0.34831 | val_0_rmse: 0.66823 | val_1_rmse: 0.67478 |  0:13:15s
epoch 52 | loss: 0.34911 | val_0_rmse: 0.59038 | val_1_rmse: 0.59387 |  0:13:31s
epoch 53 | loss: 0.34888 | val_0_rmse: 0.5864  | val_1_rmse: 0.59154 |  0:13:46s
epoch 54 | loss: 0.34614 | val_0_rmse: 0.67273 | val_1_rmse: 0.67709 |  0:14:02s
epoch 55 | loss: 0.34876 | val_0_rmse: 0.62532 | val_1_rmse: 0.62852 |  0:14:14s
epoch 56 | loss: 0.34993 | val_0_rmse: 0.90393 | val_1_rmse: 0.90407 |  0:14:29s
epoch 57 | loss: 0.34953 | val_0_rmse: 0.59595 | val_1_rmse: 0.59603 |  0:14:43s
epoch 58 | loss: 0.35182 | val_0_rmse: 0.59969 | val_1_rmse: 0.60404 |  0:14:55s
epoch 59 | loss: 0.35118 | val_0_rmse: 0.65047 | val_1_rmse: 0.65228 |  0:15:05s
epoch 60 | loss: 0.34723 | val_0_rmse: 0.61555 | val_1_rmse: 0.62136 |  0:15:24s
epoch 61 | loss: 0.34823 | val_0_rmse: 0.60927 | val_1_rmse: 0.6129  |  0:15:40s
epoch 62 | loss: 0.3443  | val_0_rmse: 0.59072 | val_1_rmse: 0.59713 |  0:15:55s
epoch 63 | loss: 0.34704 | val_0_rmse: 0.60868 | val_1_rmse: 0.61478 |  0:16:10s
epoch 64 | loss: 0.33989 | val_0_rmse: 0.59899 | val_1_rmse: 0.60252 |  0:16:21s
epoch 65 | loss: 0.33952 | val_0_rmse: 0.58371 | val_1_rmse: 0.58844 |  0:16:39s
epoch 66 | loss: 0.3435  | val_0_rmse: 0.72686 | val_1_rmse: 0.73101 |  0:16:56s
epoch 67 | loss: 0.33961 | val_0_rmse: 0.59844 | val_1_rmse: 0.60063 |  0:17:12s
epoch 68 | loss: 0.33905 | val_0_rmse: 0.59875 | val_1_rmse: 0.6015  |  0:17:31s
epoch 69 | loss: 0.33783 | val_0_rmse: 0.637   | val_1_rmse: 0.64314 |  0:17:49s
epoch 70 | loss: 0.34472 | val_0_rmse: 0.71341 | val_1_rmse: 0.71897 |  0:18:10s
epoch 71 | loss: 0.34863 | val_0_rmse: 0.58626 | val_1_rmse: 0.58805 |  0:18:30s
epoch 72 | loss: 0.34087 | val_0_rmse: 0.66262 | val_1_rmse: 0.66335 |  0:18:50s
epoch 73 | loss: 0.33833 | val_0_rmse: 0.65109 | val_1_rmse: 0.65497 |  0:19:09s
epoch 74 | loss: 0.34001 | val_0_rmse: 0.64942 | val_1_rmse: 0.65041 |  0:19:30s
epoch 75 | loss: 0.33495 | val_0_rmse: 0.59986 | val_1_rmse: 0.60273 |  0:19:42s
epoch 76 | loss: 0.34554 | val_0_rmse: 0.6533  | val_1_rmse: 0.65782 |  0:19:59s
epoch 77 | loss: 0.34236 | val_0_rmse: 0.59752 | val_1_rmse: 0.60094 |  0:20:18s
epoch 78 | loss: 0.33971 | val_0_rmse: 0.60022 | val_1_rmse: 0.60337 |  0:20:38s
epoch 79 | loss: 0.33764 | val_0_rmse: 0.63712 | val_1_rmse: 0.64259 |  0:20:59s
epoch 80 | loss: 0.33786 | val_0_rmse: 0.59722 | val_1_rmse: 0.59824 |  0:21:20s
epoch 81 | loss: 0.33921 | val_0_rmse: 0.62692 | val_1_rmse: 0.63107 |  0:21:41s
epoch 82 | loss: 0.34489 | val_0_rmse: 0.59924 | val_1_rmse: 0.60136 |  0:22:02s
epoch 83 | loss: 0.33989 | val_0_rmse: 0.62308 | val_1_rmse: 0.62843 |  0:22:22s
epoch 84 | loss: 0.33391 | val_0_rmse: 0.57759 | val_1_rmse: 0.58313 |  0:22:42s
epoch 85 | loss: 0.33317 | val_0_rmse: 0.66464 | val_1_rmse: 0.66667 |  0:23:03s
epoch 86 | loss: 0.33141 | val_0_rmse: 0.64245 | val_1_rmse: 0.64458 |  0:23:21s
epoch 87 | loss: 0.33275 | val_0_rmse: 0.64043 | val_1_rmse: 0.64486 |  0:23:41s
epoch 88 | loss: 0.32992 | val_0_rmse: 0.62799 | val_1_rmse: 0.63094 |  0:24:01s
epoch 89 | loss: 0.3349  | val_0_rmse: 0.6029  | val_1_rmse: 0.6067  |  0:24:21s
epoch 90 | loss: 0.33465 | val_0_rmse: 0.66675 | val_1_rmse: 0.67353 |  0:24:32s
epoch 91 | loss: 0.34106 | val_0_rmse: 0.61407 | val_1_rmse: 0.61702 |  0:24:48s
epoch 92 | loss: 0.33492 | val_0_rmse: 0.64744 | val_1_rmse: 0.65078 |  0:25:00s
epoch 93 | loss: 0.3297  | val_0_rmse: 0.59358 | val_1_rmse: 0.59728 |  0:25:13s
epoch 94 | loss: 0.3338  | val_0_rmse: 0.60614 | val_1_rmse: 0.61195 |  0:25:28s
epoch 95 | loss: 0.33482 | val_0_rmse: 0.77691 | val_1_rmse: 0.78124 |  0:25:43s
epoch 96 | loss: 0.33449 | val_0_rmse: 0.85237 | val_1_rmse: 0.85281 |  0:25:57s
epoch 97 | loss: 0.32892 | val_0_rmse: 0.57689 | val_1_rmse: 0.58036 |  0:26:12s
epoch 98 | loss: 0.32952 | val_0_rmse: 0.64166 | val_1_rmse: 0.64567 |  0:26:29s
epoch 99 | loss: 0.33136 | val_0_rmse: 0.62287 | val_1_rmse: 0.6237  |  0:26:44s
epoch 100| loss: 0.32994 | val_0_rmse: 0.70458 | val_1_rmse: 0.708   |  0:27:02s
epoch 101| loss: 0.32957 | val_0_rmse: 0.64886 | val_1_rmse: 0.65378 |  0:27:21s
epoch 102| loss: 0.33504 | val_0_rmse: 0.6599  | val_1_rmse: 0.66396 |  0:27:37s
epoch 103| loss: 0.33094 | val_0_rmse: 0.66928 | val_1_rmse: 0.67324 |  0:27:49s
epoch 104| loss: 0.32873 | val_0_rmse: 0.60363 | val_1_rmse: 0.60389 |  0:27:58s
epoch 105| loss: 0.33006 | val_0_rmse: 0.6288  | val_1_rmse: 0.63167 |  0:28:08s
epoch 106| loss: 0.32966 | val_0_rmse: 0.62802 | val_1_rmse: 0.63033 |  0:28:15s
epoch 107| loss: 0.33139 | val_0_rmse: 1.0584  | val_1_rmse: 1.05996 |  0:28:21s
epoch 108| loss: 0.33051 | val_0_rmse: 0.73524 | val_1_rmse: 0.7406  |  0:28:27s
epoch 109| loss: 0.33663 | val_0_rmse: 0.81508 | val_1_rmse: 0.8188  |  0:28:33s
epoch 110| loss: 0.33176 | val_0_rmse: 0.59703 | val_1_rmse: 0.60195 |  0:28:40s
epoch 111| loss: 0.3292  | val_0_rmse: 0.58152 | val_1_rmse: 0.58495 |  0:28:47s
epoch 112| loss: 0.34391 | val_0_rmse: 0.71471 | val_1_rmse: 0.71739 |  0:28:53s
epoch 113| loss: 0.356   | val_0_rmse: 0.60013 | val_1_rmse: 0.60577 |  0:28:58s
epoch 114| loss: 0.34559 | val_0_rmse: 0.61492 | val_1_rmse: 0.61769 |  0:29:04s
epoch 115| loss: 0.34775 | val_0_rmse: 0.64121 | val_1_rmse: 0.64798 |  0:29:09s
epoch 116| loss: 0.34112 | val_0_rmse: 0.7212  | val_1_rmse: 0.72396 |  0:29:15s
epoch 117| loss: 0.33993 | val_0_rmse: 0.66116 | val_1_rmse: 0.66275 |  0:29:21s
epoch 118| loss: 0.33989 | val_0_rmse: 0.64663 | val_1_rmse: 0.65021 |  0:29:26s
epoch 119| loss: 0.34083 | val_0_rmse: 0.69173 | val_1_rmse: 0.69334 |  0:29:32s
epoch 120| loss: 0.34162 | val_0_rmse: 0.65011 | val_1_rmse: 0.65262 |  0:29:38s
epoch 121| loss: 0.34097 | val_0_rmse: 0.73489 | val_1_rmse: 0.73748 |  0:29:43s
epoch 122| loss: 0.34061 | val_0_rmse: 0.59501 | val_1_rmse: 0.5962  |  0:29:49s
epoch 123| loss: 0.33681 | val_0_rmse: 0.67274 | val_1_rmse: 0.67604 |  0:29:55s
epoch 124| loss: 0.34886 | val_0_rmse: 0.69307 | val_1_rmse: 0.69731 |  0:30:00s
epoch 125| loss: 0.33817 | val_0_rmse: 0.67327 | val_1_rmse: 0.67588 |  0:30:06s
epoch 126| loss: 0.33786 | val_0_rmse: 0.65014 | val_1_rmse: 0.65574 |  0:30:12s
epoch 127| loss: 0.33725 | val_0_rmse: 0.62417 | val_1_rmse: 0.63372 |  0:30:17s

Early stopping occured at epoch 127 with best_epoch = 97 and best_val_1_rmse = 0.58036
Best weights from best epoch are automatically used!
ended training at: 20:08:56
Feature importance:
[('Area', 0.31486553298654263), ('Baths', 0.17717393956520153), ('Beds', 0.0), ('Latitude', 0.22183427576699308), ('Longitude', 0.22864275977614426), ('Month', 0.0), ('Year', 0.0574834919051185)]
Mean squared error is of 2251668619.254611
Mean absolute error:33864.748676640724
MAPE:0.3201531696873984
R2 score:0.6680785688590534
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:08:57
epoch 0  | loss: 0.58302 | val_0_rmse: 0.71536 | val_1_rmse: 0.72091 |  0:00:05s
epoch 1  | loss: 0.49673 | val_0_rmse: 0.6589  | val_1_rmse: 0.66694 |  0:00:11s
epoch 2  | loss: 0.44246 | val_0_rmse: 0.62782 | val_1_rmse: 0.63609 |  0:00:16s
epoch 3  | loss: 0.39834 | val_0_rmse: 0.68963 | val_1_rmse: 0.70224 |  0:00:22s
epoch 4  | loss: 0.39169 | val_0_rmse: 0.68504 | val_1_rmse: 0.69674 |  0:00:27s
epoch 5  | loss: 0.38656 | val_0_rmse: 0.63507 | val_1_rmse: 0.64759 |  0:00:33s
epoch 6  | loss: 0.37842 | val_0_rmse: 0.61317 | val_1_rmse: 0.62311 |  0:00:38s
epoch 7  | loss: 0.37326 | val_0_rmse: 0.64507 | val_1_rmse: 0.64707 |  0:00:44s
epoch 8  | loss: 0.37104 | val_0_rmse: 0.59468 | val_1_rmse: 0.6014  |  0:00:49s
epoch 9  | loss: 0.36596 | val_0_rmse: 0.59092 | val_1_rmse: 0.5983  |  0:00:55s
epoch 10 | loss: 0.35892 | val_0_rmse: 0.59042 | val_1_rmse: 0.5988  |  0:01:00s
epoch 11 | loss: 0.36006 | val_0_rmse: 0.60655 | val_1_rmse: 0.61007 |  0:01:06s
epoch 12 | loss: 0.3554  | val_0_rmse: 0.59637 | val_1_rmse: 0.60339 |  0:01:11s
epoch 13 | loss: 0.35781 | val_0_rmse: 0.62186 | val_1_rmse: 0.62412 |  0:01:17s
epoch 14 | loss: 0.35359 | val_0_rmse: 0.57935 | val_1_rmse: 0.58686 |  0:01:22s
epoch 15 | loss: 0.3523  | val_0_rmse: 0.61577 | val_1_rmse: 0.62496 |  0:01:28s
epoch 16 | loss: 0.35309 | val_0_rmse: 0.60481 | val_1_rmse: 0.61241 |  0:01:34s
epoch 17 | loss: 0.34938 | val_0_rmse: 0.61022 | val_1_rmse: 0.61335 |  0:01:39s
epoch 18 | loss: 0.34604 | val_0_rmse: 0.57851 | val_1_rmse: 0.58572 |  0:01:45s
epoch 19 | loss: 0.3501  | val_0_rmse: 0.60919 | val_1_rmse: 0.61279 |  0:01:50s
epoch 20 | loss: 0.35068 | val_0_rmse: 0.65696 | val_1_rmse: 0.66336 |  0:01:56s
epoch 21 | loss: 0.34558 | val_0_rmse: 0.58707 | val_1_rmse: 0.59436 |  0:02:01s
epoch 22 | loss: 0.34381 | val_0_rmse: 0.60152 | val_1_rmse: 0.60881 |  0:02:07s
epoch 23 | loss: 0.34388 | val_0_rmse: 0.63022 | val_1_rmse: 0.6379  |  0:02:12s
epoch 24 | loss: 0.3447  | val_0_rmse: 0.62873 | val_1_rmse: 0.63578 |  0:02:18s
epoch 25 | loss: 0.34248 | val_0_rmse: 0.60285 | val_1_rmse: 0.60826 |  0:02:23s
epoch 26 | loss: 0.33982 | val_0_rmse: 0.68247 | val_1_rmse: 0.69019 |  0:02:29s
epoch 27 | loss: 0.34183 | val_0_rmse: 0.60156 | val_1_rmse: 0.61077 |  0:02:35s
epoch 28 | loss: 0.34079 | val_0_rmse: 0.64925 | val_1_rmse: 0.6567  |  0:02:40s
epoch 29 | loss: 0.33814 | val_0_rmse: 0.93521 | val_1_rmse: 0.9342  |  0:02:46s
epoch 30 | loss: 0.33939 | val_0_rmse: 0.66269 | val_1_rmse: 0.67336 |  0:02:51s
epoch 31 | loss: 0.33812 | val_0_rmse: 0.8178  | val_1_rmse: 0.81287 |  0:02:57s
epoch 32 | loss: 0.34039 | val_0_rmse: 0.87193 | val_1_rmse: 0.87806 |  0:03:02s
epoch 33 | loss: 0.3383  | val_0_rmse: 0.74034 | val_1_rmse: 0.75047 |  0:03:08s
epoch 34 | loss: 0.33623 | val_0_rmse: 0.57074 | val_1_rmse: 0.57562 |  0:03:13s
epoch 35 | loss: 0.33456 | val_0_rmse: 0.65592 | val_1_rmse: 0.65385 |  0:03:19s
epoch 36 | loss: 0.33461 | val_0_rmse: 0.60613 | val_1_rmse: 0.61233 |  0:03:24s
epoch 37 | loss: 0.33396 | val_0_rmse: 0.59269 | val_1_rmse: 0.60087 |  0:03:30s
epoch 38 | loss: 0.33383 | val_0_rmse: 0.566   | val_1_rmse: 0.57293 |  0:03:35s
epoch 39 | loss: 0.33193 | val_0_rmse: 1.02234 | val_1_rmse: 1.01772 |  0:03:41s
epoch 40 | loss: 0.33691 | val_0_rmse: 0.64273 | val_1_rmse: 0.64029 |  0:03:47s
epoch 41 | loss: 0.32869 | val_0_rmse: 0.61764 | val_1_rmse: 0.62552 |  0:03:52s
epoch 42 | loss: 0.33045 | val_0_rmse: 1.27444 | val_1_rmse: 1.26779 |  0:03:58s
epoch 43 | loss: 0.33186 | val_0_rmse: 0.75383 | val_1_rmse: 0.75297 |  0:04:03s
epoch 44 | loss: 0.32717 | val_0_rmse: 0.6103  | val_1_rmse: 0.61831 |  0:04:09s
epoch 45 | loss: 0.32665 | val_0_rmse: 0.58285 | val_1_rmse: 0.58808 |  0:04:14s
epoch 46 | loss: 0.32892 | val_0_rmse: 0.59296 | val_1_rmse: 0.59888 |  0:04:20s
epoch 47 | loss: 0.32796 | val_0_rmse: 0.57963 | val_1_rmse: 0.58493 |  0:04:25s
epoch 48 | loss: 0.32756 | val_0_rmse: 0.77214 | val_1_rmse: 0.76774 |  0:04:31s
epoch 49 | loss: 0.32855 | val_0_rmse: 0.83281 | val_1_rmse: 0.83524 |  0:04:36s
epoch 50 | loss: 0.32822 | val_0_rmse: 0.67368 | val_1_rmse: 0.68145 |  0:04:42s
epoch 51 | loss: 0.32802 | val_0_rmse: 0.58571 | val_1_rmse: 0.59087 |  0:04:47s
epoch 52 | loss: 0.32748 | val_0_rmse: 0.93489 | val_1_rmse: 0.93245 |  0:04:53s
epoch 53 | loss: 0.32495 | val_0_rmse: 0.87533 | val_1_rmse: 0.86707 |  0:04:59s
epoch 54 | loss: 0.32279 | val_0_rmse: 0.61445 | val_1_rmse: 0.626   |  0:05:04s
epoch 55 | loss: 0.32673 | val_0_rmse: 0.67509 | val_1_rmse: 0.6839  |  0:05:10s
epoch 56 | loss: 0.32736 | val_0_rmse: 0.74895 | val_1_rmse: 0.74621 |  0:05:15s
epoch 57 | loss: 0.32465 | val_0_rmse: 0.59292 | val_1_rmse: 0.59965 |  0:05:21s
epoch 58 | loss: 0.32492 | val_0_rmse: 0.66846 | val_1_rmse: 0.67055 |  0:05:26s
epoch 59 | loss: 0.32774 | val_0_rmse: 0.68822 | val_1_rmse: 0.68498 |  0:05:32s
epoch 60 | loss: 0.32315 | val_0_rmse: 0.67833 | val_1_rmse: 0.68686 |  0:05:37s
epoch 61 | loss: 0.32627 | val_0_rmse: 0.58425 | val_1_rmse: 0.59019 |  0:05:43s
epoch 62 | loss: 0.32326 | val_0_rmse: 0.69183 | val_1_rmse: 0.70164 |  0:05:48s
epoch 63 | loss: 0.32092 | val_0_rmse: 0.60602 | val_1_rmse: 0.61137 |  0:05:54s
epoch 64 | loss: 0.32758 | val_0_rmse: 0.94862 | val_1_rmse: 0.94631 |  0:05:59s
epoch 65 | loss: 0.33562 | val_0_rmse: 0.85918 | val_1_rmse: 0.87253 |  0:06:05s
epoch 66 | loss: 0.36344 | val_0_rmse: 0.74112 | val_1_rmse: 0.7529  |  0:06:10s
epoch 67 | loss: 0.35093 | val_0_rmse: 0.60027 | val_1_rmse: 0.60692 |  0:06:16s
epoch 68 | loss: 0.3418  | val_0_rmse: 0.59094 | val_1_rmse: 0.59628 |  0:06:21s

Early stopping occured at epoch 68 with best_epoch = 38 and best_val_1_rmse = 0.57293
Best weights from best epoch are automatically used!
ended training at: 20:15:21
Feature importance:
[('Area', 0.2554782960647297), ('Baths', 0.1818863447797617), ('Beds', 0.0), ('Latitude', 0.3350402182060477), ('Longitude', 0.22582814472469626), ('Month', 0.0017669962247646557), ('Year', 0.0)]
Mean squared error is of 2261133845.3761497
Mean absolute error:34369.72852231871
MAPE:0.3275067029740174
R2 score:0.6744966102477183
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:15:22
epoch 0  | loss: 0.60686 | val_0_rmse: 0.71541 | val_1_rmse: 0.71526 |  0:00:05s
epoch 1  | loss: 0.48613 | val_0_rmse: 0.72254 | val_1_rmse: 0.72104 |  0:00:11s
epoch 2  | loss: 0.42087 | val_0_rmse: 0.64703 | val_1_rmse: 0.64586 |  0:00:16s
epoch 3  | loss: 0.39436 | val_0_rmse: 0.7676  | val_1_rmse: 0.7633  |  0:00:22s
epoch 4  | loss: 0.387   | val_0_rmse: 0.73155 | val_1_rmse: 0.72729 |  0:00:27s
epoch 5  | loss: 0.38092 | val_0_rmse: 0.63217 | val_1_rmse: 0.63045 |  0:00:33s
epoch 6  | loss: 0.37443 | val_0_rmse: 0.61056 | val_1_rmse: 0.60883 |  0:00:38s
epoch 7  | loss: 0.37801 | val_0_rmse: 0.61968 | val_1_rmse: 0.61879 |  0:00:44s
epoch 8  | loss: 0.37299 | val_0_rmse: 0.6313  | val_1_rmse: 0.62909 |  0:00:50s
epoch 9  | loss: 0.37095 | val_0_rmse: 0.5912  | val_1_rmse: 0.58897 |  0:00:55s
epoch 10 | loss: 0.37185 | val_0_rmse: 0.62323 | val_1_rmse: 0.61989 |  0:01:01s
epoch 11 | loss: 0.36874 | val_0_rmse: 0.73216 | val_1_rmse: 0.72994 |  0:01:06s
epoch 12 | loss: 0.37364 | val_0_rmse: 0.63914 | val_1_rmse: 0.63712 |  0:01:12s
epoch 13 | loss: 0.36935 | val_0_rmse: 0.63714 | val_1_rmse: 0.63574 |  0:01:17s
epoch 14 | loss: 0.37033 | val_0_rmse: 0.66857 | val_1_rmse: 0.66729 |  0:01:23s
epoch 15 | loss: 0.36362 | val_0_rmse: 0.63935 | val_1_rmse: 0.63727 |  0:01:28s
epoch 16 | loss: 0.3662  | val_0_rmse: 0.71246 | val_1_rmse: 0.70938 |  0:01:34s
epoch 17 | loss: 0.36496 | val_0_rmse: 0.6162  | val_1_rmse: 0.61578 |  0:01:40s
epoch 18 | loss: 0.36138 | val_0_rmse: 0.61223 | val_1_rmse: 0.61354 |  0:01:45s
epoch 19 | loss: 0.36115 | val_0_rmse: 0.63803 | val_1_rmse: 0.63634 |  0:01:51s
epoch 20 | loss: 0.3598  | val_0_rmse: 0.65892 | val_1_rmse: 0.65592 |  0:01:56s
epoch 21 | loss: 0.36113 | val_0_rmse: 0.65454 | val_1_rmse: 0.65285 |  0:02:02s
epoch 22 | loss: 0.36011 | val_0_rmse: 0.61156 | val_1_rmse: 0.60963 |  0:02:07s
epoch 23 | loss: 0.3556  | val_0_rmse: 0.60611 | val_1_rmse: 0.60941 |  0:02:13s
epoch 24 | loss: 0.35568 | val_0_rmse: 0.59217 | val_1_rmse: 0.59301 |  0:02:19s
epoch 25 | loss: 0.35551 | val_0_rmse: 0.63898 | val_1_rmse: 0.64043 |  0:02:24s
epoch 26 | loss: 0.35747 | val_0_rmse: 0.60383 | val_1_rmse: 0.60543 |  0:02:30s
epoch 27 | loss: 0.35829 | val_0_rmse: 0.59532 | val_1_rmse: 0.59269 |  0:02:35s
epoch 28 | loss: 0.35199 | val_0_rmse: 0.60003 | val_1_rmse: 0.59864 |  0:02:41s
epoch 29 | loss: 0.35359 | val_0_rmse: 0.64032 | val_1_rmse: 0.6421  |  0:02:46s
epoch 30 | loss: 0.35159 | val_0_rmse: 0.59613 | val_1_rmse: 0.59471 |  0:02:52s
epoch 31 | loss: 0.35327 | val_0_rmse: 0.59108 | val_1_rmse: 0.59194 |  0:02:58s
epoch 32 | loss: 0.35198 | val_0_rmse: 0.61327 | val_1_rmse: 0.61083 |  0:03:03s
epoch 33 | loss: 0.35112 | val_0_rmse: 0.64419 | val_1_rmse: 0.64749 |  0:03:09s
epoch 34 | loss: 0.34811 | val_0_rmse: 0.61255 | val_1_rmse: 0.61404 |  0:03:14s
epoch 35 | loss: 0.35148 | val_0_rmse: 0.63515 | val_1_rmse: 0.63365 |  0:03:20s
epoch 36 | loss: 0.35364 | val_0_rmse: 0.62013 | val_1_rmse: 0.61665 |  0:03:25s
epoch 37 | loss: 0.34854 | val_0_rmse: 0.58297 | val_1_rmse: 0.58218 |  0:03:31s
epoch 38 | loss: 0.34818 | val_0_rmse: 0.61198 | val_1_rmse: 0.61498 |  0:03:36s
epoch 39 | loss: 0.34471 | val_0_rmse: 0.59114 | val_1_rmse: 0.58862 |  0:03:42s
epoch 40 | loss: 0.34691 | val_0_rmse: 0.6015  | val_1_rmse: 0.59987 |  0:03:47s
epoch 41 | loss: 0.34659 | val_0_rmse: 0.6085  | val_1_rmse: 0.60881 |  0:03:53s
epoch 42 | loss: 0.35318 | val_0_rmse: 0.61666 | val_1_rmse: 0.61569 |  0:03:58s
epoch 43 | loss: 0.35411 | val_0_rmse: 0.66342 | val_1_rmse: 0.66082 |  0:04:04s
epoch 44 | loss: 0.34885 | val_0_rmse: 0.62462 | val_1_rmse: 0.62444 |  0:04:10s
epoch 45 | loss: 0.34354 | val_0_rmse: 0.62297 | val_1_rmse: 0.62561 |  0:04:15s
epoch 46 | loss: 0.34751 | val_0_rmse: 0.58223 | val_1_rmse: 0.58212 |  0:04:21s
epoch 47 | loss: 0.34426 | val_0_rmse: 0.63516 | val_1_rmse: 0.63358 |  0:04:26s
epoch 48 | loss: 0.34521 | val_0_rmse: 0.63791 | val_1_rmse: 0.63366 |  0:04:32s
epoch 49 | loss: 0.3477  | val_0_rmse: 0.72948 | val_1_rmse: 0.7316  |  0:04:37s
epoch 50 | loss: 0.3523  | val_0_rmse: 0.64294 | val_1_rmse: 0.64341 |  0:04:43s
epoch 51 | loss: 0.34864 | val_0_rmse: 0.62975 | val_1_rmse: 0.63167 |  0:04:48s
epoch 52 | loss: 0.34714 | val_0_rmse: 0.61512 | val_1_rmse: 0.61182 |  0:04:54s
epoch 53 | loss: 0.34665 | val_0_rmse: 0.63243 | val_1_rmse: 0.634   |  0:04:59s
epoch 54 | loss: 0.34323 | val_0_rmse: 0.58415 | val_1_rmse: 0.58302 |  0:05:05s
epoch 55 | loss: 0.3442  | val_0_rmse: 0.58219 | val_1_rmse: 0.58115 |  0:05:11s
epoch 56 | loss: 0.34294 | val_0_rmse: 0.71537 | val_1_rmse: 0.7133  |  0:05:16s
epoch 57 | loss: 0.34512 | val_0_rmse: 0.7314  | val_1_rmse: 0.73089 |  0:05:22s
epoch 58 | loss: 0.34086 | val_0_rmse: 0.59907 | val_1_rmse: 0.59994 |  0:05:27s
epoch 59 | loss: 0.33797 | val_0_rmse: 0.59711 | val_1_rmse: 0.59887 |  0:05:33s
epoch 60 | loss: 0.34452 | val_0_rmse: 0.57483 | val_1_rmse: 0.57244 |  0:05:38s
epoch 61 | loss: 0.34295 | val_0_rmse: 0.58859 | val_1_rmse: 0.59076 |  0:05:44s
epoch 62 | loss: 0.34002 | val_0_rmse: 0.63081 | val_1_rmse: 0.62975 |  0:05:49s
epoch 63 | loss: 0.34238 | val_0_rmse: 0.61578 | val_1_rmse: 0.61611 |  0:05:55s
epoch 64 | loss: 0.34052 | val_0_rmse: 0.66511 | val_1_rmse: 0.66303 |  0:06:00s
epoch 65 | loss: 0.34149 | val_0_rmse: 0.60628 | val_1_rmse: 0.60864 |  0:06:06s
epoch 66 | loss: 0.33981 | val_0_rmse: 0.57838 | val_1_rmse: 0.57943 |  0:06:12s
epoch 67 | loss: 0.34004 | val_0_rmse: 0.69267 | val_1_rmse: 0.69323 |  0:06:17s
epoch 68 | loss: 0.34046 | val_0_rmse: 0.62827 | val_1_rmse: 0.62913 |  0:06:23s
epoch 69 | loss: 0.33798 | val_0_rmse: 0.61267 | val_1_rmse: 0.61453 |  0:06:28s
epoch 70 | loss: 0.34134 | val_0_rmse: 0.63413 | val_1_rmse: 0.63153 |  0:06:34s
epoch 71 | loss: 0.34083 | val_0_rmse: 0.61155 | val_1_rmse: 0.61146 |  0:06:39s
epoch 72 | loss: 0.34051 | val_0_rmse: 0.669   | val_1_rmse: 0.67218 |  0:06:45s
epoch 73 | loss: 0.33781 | val_0_rmse: 0.66771 | val_1_rmse: 0.66754 |  0:06:50s
epoch 74 | loss: 0.33924 | val_0_rmse: 0.58857 | val_1_rmse: 0.58687 |  0:06:56s
epoch 75 | loss: 0.33982 | val_0_rmse: 0.58827 | val_1_rmse: 0.5879  |  0:07:02s
epoch 76 | loss: 0.33614 | val_0_rmse: 0.66175 | val_1_rmse: 0.66178 |  0:07:07s
epoch 77 | loss: 0.33886 | val_0_rmse: 0.80772 | val_1_rmse: 0.80387 |  0:07:13s
epoch 78 | loss: 0.33738 | val_0_rmse: 0.63504 | val_1_rmse: 0.63222 |  0:07:18s
epoch 79 | loss: 0.34069 | val_0_rmse: 0.61208 | val_1_rmse: 0.6125  |  0:07:24s
epoch 80 | loss: 0.34578 | val_0_rmse: 0.80713 | val_1_rmse: 0.80442 |  0:07:29s
epoch 81 | loss: 0.34167 | val_0_rmse: 0.59777 | val_1_rmse: 0.59665 |  0:07:35s
epoch 82 | loss: 0.33918 | val_0_rmse: 0.66389 | val_1_rmse: 0.66263 |  0:07:41s
epoch 83 | loss: 0.33934 | val_0_rmse: 0.60523 | val_1_rmse: 0.60345 |  0:07:46s
epoch 84 | loss: 0.33686 | val_0_rmse: 0.69059 | val_1_rmse: 0.69186 |  0:07:52s
epoch 85 | loss: 0.33857 | val_0_rmse: 0.64959 | val_1_rmse: 0.65131 |  0:07:57s
epoch 86 | loss: 0.33939 | val_0_rmse: 0.6032  | val_1_rmse: 0.60359 |  0:08:03s
epoch 87 | loss: 0.42856 | val_0_rmse: 0.74229 | val_1_rmse: 0.73932 |  0:08:08s
epoch 88 | loss: 0.48098 | val_0_rmse: 0.75499 | val_1_rmse: 0.74972 |  0:08:14s
epoch 89 | loss: 0.46834 | val_0_rmse: 0.70408 | val_1_rmse: 0.70184 |  0:08:19s
epoch 90 | loss: 0.5231  | val_0_rmse: 0.73057 | val_1_rmse: 0.73027 |  0:08:25s

Early stopping occured at epoch 90 with best_epoch = 60 and best_val_1_rmse = 0.57244
Best weights from best epoch are automatically used!
ended training at: 20:23:49
Feature importance:
[('Area', 0.4597939519707971), ('Baths', 0.09547998612683574), ('Beds', 0.0500112745684454), ('Latitude', 0.0653167553305923), ('Longitude', 0.3293980320033295), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2256603843.2487206
Mean absolute error:34047.53061360956
MAPE:0.323144326550047
R2 score:0.6712713194983552
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:23:50
epoch 0  | loss: 0.61071 | val_0_rmse: 0.73892 | val_1_rmse: 0.73502 |  0:00:05s
epoch 1  | loss: 0.50335 | val_0_rmse: 0.75614 | val_1_rmse: 0.74786 |  0:00:11s
epoch 2  | loss: 0.44331 | val_0_rmse: 0.73978 | val_1_rmse: 0.73619 |  0:00:16s
epoch 3  | loss: 0.41693 | val_0_rmse: 0.70406 | val_1_rmse: 0.69777 |  0:00:22s
epoch 4  | loss: 0.39832 | val_0_rmse: 0.66057 | val_1_rmse: 0.65733 |  0:00:27s
epoch 5  | loss: 0.40259 | val_0_rmse: 0.6387  | val_1_rmse: 0.6374  |  0:00:33s
epoch 6  | loss: 0.3978  | val_0_rmse: 0.65998 | val_1_rmse: 0.6581  |  0:00:38s
epoch 7  | loss: 0.38859 | val_0_rmse: 0.69152 | val_1_rmse: 0.68755 |  0:00:44s
epoch 8  | loss: 0.38193 | val_0_rmse: 0.66282 | val_1_rmse: 0.65921 |  0:00:49s
epoch 9  | loss: 0.38022 | val_0_rmse: 0.65093 | val_1_rmse: 0.65044 |  0:00:55s
epoch 10 | loss: 0.37829 | val_0_rmse: 0.64479 | val_1_rmse: 0.6423  |  0:01:01s
epoch 11 | loss: 0.37885 | val_0_rmse: 0.61278 | val_1_rmse: 0.61344 |  0:01:06s
epoch 12 | loss: 0.37137 | val_0_rmse: 0.63211 | val_1_rmse: 0.63309 |  0:01:12s
epoch 13 | loss: 0.37403 | val_0_rmse: 0.60981 | val_1_rmse: 0.61092 |  0:01:17s
epoch 14 | loss: 0.36814 | val_0_rmse: 0.62263 | val_1_rmse: 0.62147 |  0:01:23s
epoch 15 | loss: 0.36886 | val_0_rmse: 0.65168 | val_1_rmse: 0.65022 |  0:01:28s
epoch 16 | loss: 0.36911 | val_0_rmse: 0.6926  | val_1_rmse: 0.6897  |  0:01:34s
epoch 17 | loss: 0.36905 | val_0_rmse: 0.6242  | val_1_rmse: 0.62437 |  0:01:39s
epoch 18 | loss: 0.37461 | val_0_rmse: 0.77963 | val_1_rmse: 0.77277 |  0:01:45s
epoch 19 | loss: 0.36306 | val_0_rmse: 0.75349 | val_1_rmse: 0.7476  |  0:01:50s
epoch 20 | loss: 0.3656  | val_0_rmse: 0.59825 | val_1_rmse: 0.59779 |  0:01:56s
epoch 21 | loss: 0.36649 | val_0_rmse: 0.58924 | val_1_rmse: 0.58734 |  0:02:02s
epoch 22 | loss: 0.3588  | val_0_rmse: 0.58638 | val_1_rmse: 0.58514 |  0:02:07s
epoch 23 | loss: 0.36079 | val_0_rmse: 0.63332 | val_1_rmse: 0.63005 |  0:02:13s
epoch 24 | loss: 0.35417 | val_0_rmse: 0.61718 | val_1_rmse: 0.61315 |  0:02:18s
epoch 25 | loss: 0.35626 | val_0_rmse: 0.69072 | val_1_rmse: 0.69215 |  0:02:24s
epoch 26 | loss: 0.35856 | val_0_rmse: 0.62595 | val_1_rmse: 0.62271 |  0:02:29s
epoch 27 | loss: 0.35536 | val_0_rmse: 0.70537 | val_1_rmse: 0.70782 |  0:02:35s
epoch 28 | loss: 0.35645 | val_0_rmse: 0.86362 | val_1_rmse: 0.857   |  0:02:41s
epoch 29 | loss: 0.35639 | val_0_rmse: 0.58063 | val_1_rmse: 0.57834 |  0:02:46s
epoch 30 | loss: 0.35073 | val_0_rmse: 0.60379 | val_1_rmse: 0.60339 |  0:02:52s
epoch 31 | loss: 0.35476 | val_0_rmse: 0.72128 | val_1_rmse: 0.7238  |  0:02:57s
epoch 32 | loss: 0.35083 | val_0_rmse: 1.00718 | val_1_rmse: 1.00609 |  0:03:03s
epoch 33 | loss: 0.34858 | val_0_rmse: 0.61085 | val_1_rmse: 0.61149 |  0:03:08s
epoch 34 | loss: 0.34939 | val_0_rmse: 0.65719 | val_1_rmse: 0.6527  |  0:03:14s
epoch 35 | loss: 0.34767 | val_0_rmse: 0.65918 | val_1_rmse: 0.65446 |  0:03:19s
epoch 36 | loss: 0.35059 | val_0_rmse: 0.59082 | val_1_rmse: 0.58817 |  0:03:25s
epoch 37 | loss: 0.34977 | val_0_rmse: 0.61421 | val_1_rmse: 0.61523 |  0:03:30s
epoch 38 | loss: 0.35712 | val_0_rmse: 0.59102 | val_1_rmse: 0.59028 |  0:03:36s
epoch 39 | loss: 0.35837 | val_0_rmse: 0.66831 | val_1_rmse: 0.66479 |  0:03:41s
epoch 40 | loss: 0.35151 | val_0_rmse: 0.72117 | val_1_rmse: 0.71532 |  0:03:47s
epoch 41 | loss: 0.35204 | val_0_rmse: 0.59676 | val_1_rmse: 0.59645 |  0:03:52s
epoch 42 | loss: 0.34947 | val_0_rmse: 0.63054 | val_1_rmse: 0.6273  |  0:03:58s
epoch 43 | loss: 0.3511  | val_0_rmse: 0.67199 | val_1_rmse: 0.66958 |  0:04:03s
epoch 44 | loss: 0.35201 | val_0_rmse: 0.60861 | val_1_rmse: 0.60786 |  0:04:09s
epoch 45 | loss: 0.35082 | val_0_rmse: 0.61632 | val_1_rmse: 0.61257 |  0:04:14s
epoch 46 | loss: 0.34554 | val_0_rmse: 0.57623 | val_1_rmse: 0.57544 |  0:04:20s
epoch 47 | loss: 0.35108 | val_0_rmse: 0.58541 | val_1_rmse: 0.58262 |  0:04:26s
epoch 48 | loss: 0.34591 | val_0_rmse: 0.61413 | val_1_rmse: 0.61127 |  0:04:31s
epoch 49 | loss: 0.34951 | val_0_rmse: 0.59155 | val_1_rmse: 0.59016 |  0:04:37s
epoch 50 | loss: 0.35397 | val_0_rmse: 0.59651 | val_1_rmse: 0.59578 |  0:04:42s
epoch 51 | loss: 0.35443 | val_0_rmse: 0.60442 | val_1_rmse: 0.60336 |  0:04:48s
epoch 52 | loss: 0.34867 | val_0_rmse: 0.6107  | val_1_rmse: 0.60797 |  0:04:53s
epoch 53 | loss: 0.34784 | val_0_rmse: 0.58684 | val_1_rmse: 0.58623 |  0:04:59s
epoch 54 | loss: 0.34755 | val_0_rmse: 0.66834 | val_1_rmse: 0.66596 |  0:05:04s
epoch 55 | loss: 0.34692 | val_0_rmse: 0.5743  | val_1_rmse: 0.5733  |  0:05:10s
epoch 56 | loss: 0.34436 | val_0_rmse: 0.6946  | val_1_rmse: 0.68994 |  0:05:15s
epoch 57 | loss: 0.34388 | val_0_rmse: 0.57924 | val_1_rmse: 0.57687 |  0:05:21s
epoch 58 | loss: 0.3463  | val_0_rmse: 0.61409 | val_1_rmse: 0.61134 |  0:05:27s
epoch 59 | loss: 0.34729 | val_0_rmse: 0.61403 | val_1_rmse: 0.61333 |  0:05:32s
epoch 60 | loss: 0.34253 | val_0_rmse: 0.64385 | val_1_rmse: 0.64155 |  0:05:38s
epoch 61 | loss: 0.3458  | val_0_rmse: 0.62191 | val_1_rmse: 0.62006 |  0:05:43s
epoch 62 | loss: 0.34585 | val_0_rmse: 0.676   | val_1_rmse: 0.67472 |  0:05:49s
epoch 63 | loss: 0.34878 | val_0_rmse: 0.62149 | val_1_rmse: 0.62036 |  0:05:54s
epoch 64 | loss: 0.34828 | val_0_rmse: 0.61146 | val_1_rmse: 0.61127 |  0:06:00s
epoch 65 | loss: 0.34681 | val_0_rmse: 0.67673 | val_1_rmse: 0.67396 |  0:06:05s
epoch 66 | loss: 0.34559 | val_0_rmse: 0.68842 | val_1_rmse: 0.68299 |  0:06:11s
epoch 67 | loss: 0.34329 | val_0_rmse: 0.58514 | val_1_rmse: 0.58341 |  0:06:16s
epoch 68 | loss: 0.35197 | val_0_rmse: 0.63588 | val_1_rmse: 0.63291 |  0:06:22s
epoch 69 | loss: 0.34353 | val_0_rmse: 0.68054 | val_1_rmse: 0.67576 |  0:06:28s
epoch 70 | loss: 0.34412 | val_0_rmse: 0.62251 | val_1_rmse: 0.62321 |  0:06:33s
epoch 71 | loss: 0.3489  | val_0_rmse: 0.58611 | val_1_rmse: 0.58533 |  0:06:39s
epoch 72 | loss: 0.35373 | val_0_rmse: 0.6042  | val_1_rmse: 0.6027  |  0:06:44s
epoch 73 | loss: 0.34728 | val_0_rmse: 0.62885 | val_1_rmse: 0.6244  |  0:06:50s
epoch 74 | loss: 0.34608 | val_0_rmse: 0.66559 | val_1_rmse: 0.6615  |  0:06:55s
epoch 75 | loss: 0.34264 | val_0_rmse: 0.63896 | val_1_rmse: 0.63936 |  0:07:01s
epoch 76 | loss: 0.34569 | val_0_rmse: 0.6867  | val_1_rmse: 0.67904 |  0:07:06s
epoch 77 | loss: 0.33928 | val_0_rmse: 0.68649 | val_1_rmse: 0.68953 |  0:07:12s
epoch 78 | loss: 0.34816 | val_0_rmse: 0.61822 | val_1_rmse: 0.61997 |  0:07:17s
epoch 79 | loss: 0.35255 | val_0_rmse: 0.58406 | val_1_rmse: 0.58041 |  0:07:23s
epoch 80 | loss: 0.34595 | val_0_rmse: 0.62916 | val_1_rmse: 0.62608 |  0:07:28s
epoch 81 | loss: 0.34111 | val_0_rmse: 0.6184  | val_1_rmse: 0.61793 |  0:07:34s
epoch 82 | loss: 0.34417 | val_0_rmse: 0.62863 | val_1_rmse: 0.627   |  0:07:40s
epoch 83 | loss: 0.34138 | val_0_rmse: 0.59939 | val_1_rmse: 0.59664 |  0:07:45s
epoch 84 | loss: 0.33972 | val_0_rmse: 0.73162 | val_1_rmse: 0.73298 |  0:07:51s
epoch 85 | loss: 0.33945 | val_0_rmse: 0.67399 | val_1_rmse: 0.66938 |  0:07:56s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.5733
Best weights from best epoch are automatically used!
ended training at: 20:31:48
Feature importance:
[('Area', 0.22028435026300158), ('Baths', 0.22063460869620807), ('Beds', 0.11264963960899545), ('Latitude', 0.25370638954023306), ('Longitude', 0.1927250118915618), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2255508441.198446
Mean absolute error:34165.89929966709
MAPE:0.3251498913593967
R2 score:0.6655764461204512
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: ar properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:31:49
epoch 0  | loss: 0.59318 | val_0_rmse: 0.71685 | val_1_rmse: 0.7142  |  0:00:05s
epoch 1  | loss: 0.50139 | val_0_rmse: 0.71531 | val_1_rmse: 0.71364 |  0:00:11s
epoch 2  | loss: 0.44462 | val_0_rmse: 0.64343 | val_1_rmse: 0.64151 |  0:00:16s
epoch 3  | loss: 0.4252  | val_0_rmse: 0.68391 | val_1_rmse: 0.68174 |  0:00:22s
epoch 4  | loss: 0.41163 | val_0_rmse: 0.65957 | val_1_rmse: 0.65624 |  0:00:27s
epoch 5  | loss: 0.40434 | val_0_rmse: 0.62703 | val_1_rmse: 0.62519 |  0:00:33s
epoch 6  | loss: 0.39937 | val_0_rmse: 0.64124 | val_1_rmse: 0.64109 |  0:00:38s
epoch 7  | loss: 0.39573 | val_0_rmse: 0.64672 | val_1_rmse: 0.64446 |  0:00:44s
epoch 8  | loss: 0.39491 | val_0_rmse: 0.66797 | val_1_rmse: 0.66555 |  0:00:49s
epoch 9  | loss: 0.39594 | val_0_rmse: 0.63312 | val_1_rmse: 0.63204 |  0:00:55s
epoch 10 | loss: 0.3902  | val_0_rmse: 0.65234 | val_1_rmse: 0.65157 |  0:01:00s
epoch 11 | loss: 0.38608 | val_0_rmse: 0.66372 | val_1_rmse: 0.66262 |  0:01:06s
epoch 12 | loss: 0.38863 | val_0_rmse: 0.63367 | val_1_rmse: 0.63527 |  0:01:11s
epoch 13 | loss: 0.39054 | val_0_rmse: 0.63539 | val_1_rmse: 0.63529 |  0:01:17s
epoch 14 | loss: 0.38934 | val_0_rmse: 0.65333 | val_1_rmse: 0.65243 |  0:01:23s
epoch 15 | loss: 0.38657 | val_0_rmse: 0.64241 | val_1_rmse: 0.64046 |  0:01:28s
epoch 16 | loss: 0.38263 | val_0_rmse: 0.68392 | val_1_rmse: 0.68344 |  0:01:34s
epoch 17 | loss: 0.39093 | val_0_rmse: 0.65227 | val_1_rmse: 0.65153 |  0:01:39s
epoch 18 | loss: 0.3846  | val_0_rmse: 0.63267 | val_1_rmse: 0.63457 |  0:01:45s
epoch 19 | loss: 0.37808 | val_0_rmse: 0.65693 | val_1_rmse: 0.65307 |  0:01:50s
epoch 20 | loss: 0.37413 | val_0_rmse: 0.64436 | val_1_rmse: 0.64405 |  0:01:56s
epoch 21 | loss: 0.37249 | val_0_rmse: 0.63692 | val_1_rmse: 0.63485 |  0:02:01s
epoch 22 | loss: 0.37158 | val_0_rmse: 0.65585 | val_1_rmse: 0.65682 |  0:02:07s
epoch 23 | loss: 0.37061 | val_0_rmse: 0.69086 | val_1_rmse: 0.68954 |  0:02:12s
epoch 24 | loss: 0.36446 | val_0_rmse: 0.65341 | val_1_rmse: 0.65563 |  0:02:18s
epoch 25 | loss: 0.36906 | val_0_rmse: 0.61967 | val_1_rmse: 0.61993 |  0:02:23s
epoch 26 | loss: 0.36564 | val_0_rmse: 0.68747 | val_1_rmse: 0.691   |  0:02:29s
epoch 27 | loss: 0.36258 | val_0_rmse: 0.62632 | val_1_rmse: 0.62656 |  0:02:34s
epoch 28 | loss: 0.36342 | val_0_rmse: 0.62068 | val_1_rmse: 0.62153 |  0:02:40s
epoch 29 | loss: 0.36275 | val_0_rmse: 0.70592 | val_1_rmse: 0.70639 |  0:02:45s
epoch 30 | loss: 0.36103 | val_0_rmse: 0.62053 | val_1_rmse: 0.61863 |  0:02:51s
epoch 31 | loss: 0.3577  | val_0_rmse: 0.62546 | val_1_rmse: 0.62717 |  0:02:57s
epoch 32 | loss: 0.36522 | val_0_rmse: 0.6665  | val_1_rmse: 0.66526 |  0:03:02s
epoch 33 | loss: 0.35667 | val_0_rmse: 0.65241 | val_1_rmse: 0.65459 |  0:03:08s
epoch 34 | loss: 0.35771 | val_0_rmse: 0.61598 | val_1_rmse: 0.61634 |  0:03:13s
epoch 35 | loss: 0.35606 | val_0_rmse: 0.64212 | val_1_rmse: 0.64054 |  0:03:19s
epoch 36 | loss: 0.35735 | val_0_rmse: 0.61656 | val_1_rmse: 0.61764 |  0:03:24s
epoch 37 | loss: 0.35923 | val_0_rmse: 0.68526 | val_1_rmse: 0.68426 |  0:03:30s
epoch 38 | loss: 0.35429 | val_0_rmse: 0.63523 | val_1_rmse: 0.63523 |  0:03:35s
epoch 39 | loss: 0.35476 | val_0_rmse: 0.68337 | val_1_rmse: 0.68265 |  0:03:41s
epoch 40 | loss: 0.35584 | val_0_rmse: 0.63298 | val_1_rmse: 0.6338  |  0:03:47s
epoch 41 | loss: 0.35705 | val_0_rmse: 0.62918 | val_1_rmse: 0.62982 |  0:03:52s
epoch 42 | loss: 0.35362 | val_0_rmse: 0.62349 | val_1_rmse: 0.62087 |  0:03:58s
epoch 43 | loss: 0.35614 | val_0_rmse: 0.61019 | val_1_rmse: 0.61189 |  0:04:03s
epoch 44 | loss: 0.3524  | val_0_rmse: 0.6778  | val_1_rmse: 0.67594 |  0:04:09s
epoch 45 | loss: 0.35513 | val_0_rmse: 0.63362 | val_1_rmse: 0.63418 |  0:04:14s
epoch 46 | loss: 0.35348 | val_0_rmse: 0.66752 | val_1_rmse: 0.66601 |  0:04:20s
epoch 47 | loss: 0.35521 | val_0_rmse: 0.65061 | val_1_rmse: 0.65216 |  0:04:25s
epoch 48 | loss: 0.35373 | val_0_rmse: 0.67061 | val_1_rmse: 0.67063 |  0:04:31s
epoch 49 | loss: 0.35882 | val_0_rmse: 0.79999 | val_1_rmse: 0.79403 |  0:04:36s
epoch 50 | loss: 0.35632 | val_0_rmse: 0.61353 | val_1_rmse: 0.61568 |  0:04:42s
epoch 51 | loss: 0.35055 | val_0_rmse: 0.60841 | val_1_rmse: 0.61078 |  0:04:48s
epoch 52 | loss: 0.35601 | val_0_rmse: 0.65665 | val_1_rmse: 0.65423 |  0:04:53s
epoch 53 | loss: 0.35171 | val_0_rmse: 0.60688 | val_1_rmse: 0.6081  |  0:04:59s
epoch 54 | loss: 0.3511  | val_0_rmse: 0.69576 | val_1_rmse: 0.69249 |  0:05:04s
epoch 55 | loss: 0.35045 | val_0_rmse: 0.59232 | val_1_rmse: 0.59375 |  0:05:10s
epoch 56 | loss: 0.34903 | val_0_rmse: 0.68108 | val_1_rmse: 0.67978 |  0:05:15s
epoch 57 | loss: 0.35101 | val_0_rmse: 0.68523 | val_1_rmse: 0.6846  |  0:05:21s
epoch 58 | loss: 0.35281 | val_0_rmse: 0.60335 | val_1_rmse: 0.60493 |  0:05:26s
epoch 59 | loss: 0.35719 | val_0_rmse: 0.74408 | val_1_rmse: 0.75128 |  0:05:32s
epoch 60 | loss: 0.3547  | val_0_rmse: 0.68229 | val_1_rmse: 0.68464 |  0:05:38s
epoch 61 | loss: 0.35085 | val_0_rmse: 0.64287 | val_1_rmse: 0.64227 |  0:05:43s
epoch 62 | loss: 0.35059 | val_0_rmse: 0.6804  | val_1_rmse: 0.67938 |  0:05:49s
epoch 63 | loss: 0.3512  | val_0_rmse: 0.64289 | val_1_rmse: 0.64294 |  0:05:54s
epoch 64 | loss: 0.35032 | val_0_rmse: 0.6816  | val_1_rmse: 0.68004 |  0:06:00s
epoch 65 | loss: 0.35124 | val_0_rmse: 0.67376 | val_1_rmse: 0.67045 |  0:06:05s
epoch 66 | loss: 0.34897 | val_0_rmse: 0.61581 | val_1_rmse: 0.61605 |  0:06:11s
epoch 67 | loss: 0.3494  | val_0_rmse: 0.61126 | val_1_rmse: 0.61257 |  0:06:16s
epoch 68 | loss: 0.34828 | val_0_rmse: 0.65572 | val_1_rmse: 0.65647 |  0:06:22s
epoch 69 | loss: 0.34808 | val_0_rmse: 0.65156 | val_1_rmse: 0.64995 |  0:06:27s
epoch 70 | loss: 0.34998 | val_0_rmse: 0.62463 | val_1_rmse: 0.62735 |  0:06:33s
epoch 71 | loss: 0.34726 | val_0_rmse: 0.62484 | val_1_rmse: 0.624   |  0:06:39s
epoch 72 | loss: 0.34846 | val_0_rmse: 0.64765 | val_1_rmse: 0.64909 |  0:06:44s
epoch 73 | loss: 0.34807 | val_0_rmse: 0.62662 | val_1_rmse: 0.6263  |  0:06:50s
epoch 74 | loss: 0.34579 | val_0_rmse: 0.60565 | val_1_rmse: 0.60413 |  0:06:55s
epoch 75 | loss: 0.34783 | val_0_rmse: 0.65469 | val_1_rmse: 0.6531  |  0:07:01s
epoch 76 | loss: 0.3481  | val_0_rmse: 0.65581 | val_1_rmse: 0.656   |  0:07:06s
epoch 77 | loss: 0.3495  | val_0_rmse: 0.73399 | val_1_rmse: 0.7303  |  0:07:12s
epoch 78 | loss: 0.35113 | val_0_rmse: 0.94251 | val_1_rmse: 0.93645 |  0:07:17s
epoch 79 | loss: 0.35044 | val_0_rmse: 0.61289 | val_1_rmse: 0.61492 |  0:07:23s
epoch 80 | loss: 0.34757 | val_0_rmse: 0.61031 | val_1_rmse: 0.61036 |  0:07:28s
epoch 81 | loss: 0.3475  | val_0_rmse: 0.75103 | val_1_rmse: 0.74795 |  0:07:34s
epoch 82 | loss: 0.35199 | val_0_rmse: 0.76974 | val_1_rmse: 0.76617 |  0:07:40s
epoch 83 | loss: 0.37421 | val_0_rmse: 0.69852 | val_1_rmse: 0.69927 |  0:07:45s
epoch 84 | loss: 0.39627 | val_0_rmse: 0.64548 | val_1_rmse: 0.64404 |  0:07:51s
epoch 85 | loss: 0.37532 | val_0_rmse: 0.63607 | val_1_rmse: 0.63521 |  0:07:56s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.59375
Best weights from best epoch are automatically used!
ended training at: 20:39:47
Feature importance:
[('Area', 0.3938492660077412), ('Baths', 0.09586525800411584), ('Beds', 0.04160244042703246), ('Latitude', 0.2514086687223222), ('Longitude', 0.21727436683878826), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 2430658192.8465114
Mean absolute error:35784.93349321159
MAPE:0.34116341230438074
R2 score:0.6449934678134399
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:39:48
epoch 0  | loss: 0.56234 | val_0_rmse: 0.61808 | val_1_rmse: 0.6212  |  0:00:02s
epoch 1  | loss: 0.34769 | val_0_rmse: 0.56371 | val_1_rmse: 0.56376 |  0:00:04s
epoch 2  | loss: 0.3333  | val_0_rmse: 0.58452 | val_1_rmse: 0.57806 |  0:00:06s
epoch 3  | loss: 0.31531 | val_0_rmse: 0.5438  | val_1_rmse: 0.53767 |  0:00:08s
epoch 4  | loss: 0.30143 | val_0_rmse: 0.53345 | val_1_rmse: 0.52612 |  0:00:10s
epoch 5  | loss: 0.29744 | val_0_rmse: 0.53765 | val_1_rmse: 0.53286 |  0:00:12s
epoch 6  | loss: 0.29499 | val_0_rmse: 0.53518 | val_1_rmse: 0.53016 |  0:00:14s
epoch 7  | loss: 0.29044 | val_0_rmse: 0.52816 | val_1_rmse: 0.52256 |  0:00:16s
epoch 8  | loss: 0.29193 | val_0_rmse: 0.53326 | val_1_rmse: 0.52936 |  0:00:18s
epoch 9  | loss: 0.29086 | val_0_rmse: 0.52397 | val_1_rmse: 0.51948 |  0:00:20s
epoch 10 | loss: 0.2847  | val_0_rmse: 0.52326 | val_1_rmse: 0.51721 |  0:00:22s
epoch 11 | loss: 0.28626 | val_0_rmse: 0.56316 | val_1_rmse: 0.56623 |  0:00:24s
epoch 12 | loss: 0.28346 | val_0_rmse: 0.53866 | val_1_rmse: 0.53451 |  0:00:26s
epoch 13 | loss: 0.28272 | val_0_rmse: 0.51568 | val_1_rmse: 0.51013 |  0:00:28s
epoch 14 | loss: 0.27332 | val_0_rmse: 0.54702 | val_1_rmse: 0.54723 |  0:00:30s
epoch 15 | loss: 0.27008 | val_0_rmse: 0.57573 | val_1_rmse: 0.57639 |  0:00:32s
epoch 16 | loss: 0.28171 | val_0_rmse: 0.52289 | val_1_rmse: 0.51008 |  0:00:34s
epoch 17 | loss: 0.28967 | val_0_rmse: 0.54286 | val_1_rmse: 0.5447  |  0:00:36s
epoch 18 | loss: 0.2775  | val_0_rmse: 0.60942 | val_1_rmse: 0.61151 |  0:00:38s
epoch 19 | loss: 0.26961 | val_0_rmse: 0.51103 | val_1_rmse: 0.50486 |  0:00:40s
epoch 20 | loss: 0.26549 | val_0_rmse: 0.57318 | val_1_rmse: 0.5769  |  0:00:42s
epoch 21 | loss: 0.26559 | val_0_rmse: 0.57334 | val_1_rmse: 0.57374 |  0:00:44s
epoch 22 | loss: 0.26437 | val_0_rmse: 0.52583 | val_1_rmse: 0.52088 |  0:00:46s
epoch 23 | loss: 0.26058 | val_0_rmse: 0.51004 | val_1_rmse: 0.50584 |  0:00:48s
epoch 24 | loss: 0.26308 | val_0_rmse: 0.56932 | val_1_rmse: 0.5736  |  0:00:50s
epoch 25 | loss: 0.25533 | val_0_rmse: 0.57026 | val_1_rmse: 0.57391 |  0:00:52s
epoch 26 | loss: 0.25629 | val_0_rmse: 0.56289 | val_1_rmse: 0.56833 |  0:00:54s
epoch 27 | loss: 0.254   | val_0_rmse: 0.49802 | val_1_rmse: 0.4927  |  0:00:56s
epoch 28 | loss: 0.26051 | val_0_rmse: 0.51968 | val_1_rmse: 0.5096  |  0:00:58s
epoch 29 | loss: 0.25947 | val_0_rmse: 0.51477 | val_1_rmse: 0.50924 |  0:01:00s
epoch 30 | loss: 0.26176 | val_0_rmse: 0.62997 | val_1_rmse: 0.63615 |  0:01:02s
epoch 31 | loss: 0.26519 | val_0_rmse: 0.52129 | val_1_rmse: 0.51923 |  0:01:04s
epoch 32 | loss: 0.27046 | val_0_rmse: 0.54344 | val_1_rmse: 0.53756 |  0:01:06s
epoch 33 | loss: 0.26902 | val_0_rmse: 0.54182 | val_1_rmse: 0.54384 |  0:01:08s
epoch 34 | loss: 0.25684 | val_0_rmse: 0.54994 | val_1_rmse: 0.54723 |  0:01:10s
epoch 35 | loss: 0.25499 | val_0_rmse: 0.56968 | val_1_rmse: 0.57364 |  0:01:12s
epoch 36 | loss: 0.25801 | val_0_rmse: 0.52454 | val_1_rmse: 0.52443 |  0:01:14s
epoch 37 | loss: 0.25981 | val_0_rmse: 0.61741 | val_1_rmse: 0.6217  |  0:01:17s
epoch 38 | loss: 0.25411 | val_0_rmse: 0.56379 | val_1_rmse: 0.56846 |  0:01:19s
epoch 39 | loss: 0.25322 | val_0_rmse: 0.4889  | val_1_rmse: 0.47914 |  0:01:21s
epoch 40 | loss: 0.25415 | val_0_rmse: 0.49402 | val_1_rmse: 0.49206 |  0:01:23s
epoch 41 | loss: 0.25927 | val_0_rmse: 0.57137 | val_1_rmse: 0.56334 |  0:01:25s
epoch 42 | loss: 0.27071 | val_0_rmse: 0.51163 | val_1_rmse: 0.50555 |  0:01:27s
epoch 43 | loss: 0.25764 | val_0_rmse: 0.54386 | val_1_rmse: 0.53282 |  0:01:29s
epoch 44 | loss: 0.26093 | val_0_rmse: 0.55675 | val_1_rmse: 0.55633 |  0:01:31s
epoch 45 | loss: 0.25621 | val_0_rmse: 0.6614  | val_1_rmse: 0.66765 |  0:01:33s
epoch 46 | loss: 0.26512 | val_0_rmse: 0.5443  | val_1_rmse: 0.54449 |  0:01:35s
epoch 47 | loss: 0.25648 | val_0_rmse: 0.50381 | val_1_rmse: 0.49898 |  0:01:37s
epoch 48 | loss: 0.25732 | val_0_rmse: 0.52533 | val_1_rmse: 0.51486 |  0:01:39s
epoch 49 | loss: 0.26949 | val_0_rmse: 0.63571 | val_1_rmse: 0.64476 |  0:01:41s
epoch 50 | loss: 0.26178 | val_0_rmse: 0.55981 | val_1_rmse: 0.5636  |  0:01:43s
epoch 51 | loss: 0.25791 | val_0_rmse: 0.5681  | val_1_rmse: 0.55271 |  0:01:45s
epoch 52 | loss: 0.25213 | val_0_rmse: 0.58718 | val_1_rmse: 0.59111 |  0:01:47s
epoch 53 | loss: 0.25566 | val_0_rmse: 0.57752 | val_1_rmse: 0.58341 |  0:01:49s
epoch 54 | loss: 0.25205 | val_0_rmse: 0.50886 | val_1_rmse: 0.4984  |  0:01:51s
epoch 55 | loss: 0.251   | val_0_rmse: 0.52928 | val_1_rmse: 0.52614 |  0:01:53s
epoch 56 | loss: 0.24805 | val_0_rmse: 0.60631 | val_1_rmse: 0.61607 |  0:01:55s
epoch 57 | loss: 0.24922 | val_0_rmse: 0.58591 | val_1_rmse: 0.59292 |  0:01:57s
epoch 58 | loss: 0.24424 | val_0_rmse: 0.49892 | val_1_rmse: 0.49297 |  0:01:59s
epoch 59 | loss: 0.24665 | val_0_rmse: 0.55004 | val_1_rmse: 0.53987 |  0:02:01s
epoch 60 | loss: 0.24767 | val_0_rmse: 0.65968 | val_1_rmse: 0.6699  |  0:02:03s
epoch 61 | loss: 0.2601  | val_0_rmse: 0.58778 | val_1_rmse: 0.59429 |  0:02:05s
epoch 62 | loss: 0.25767 | val_0_rmse: 0.52075 | val_1_rmse: 0.51221 |  0:02:07s
epoch 63 | loss: 0.26462 | val_0_rmse: 0.5407  | val_1_rmse: 0.53564 |  0:02:09s
epoch 64 | loss: 0.2575  | val_0_rmse: 0.5278  | val_1_rmse: 0.52314 |  0:02:11s
epoch 65 | loss: 0.26267 | val_0_rmse: 0.54643 | val_1_rmse: 0.5466  |  0:02:13s
epoch 66 | loss: 0.25898 | val_0_rmse: 0.52996 | val_1_rmse: 0.52274 |  0:02:15s
epoch 67 | loss: 0.24981 | val_0_rmse: 0.51974 | val_1_rmse: 0.51957 |  0:02:17s
epoch 68 | loss: 0.25705 | val_0_rmse: 0.66299 | val_1_rmse: 0.67197 |  0:02:19s
epoch 69 | loss: 0.26089 | val_0_rmse: 0.50865 | val_1_rmse: 0.50665 |  0:02:21s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.47914
Best weights from best epoch are automatically used!
ended training at: 20:42:11
Feature importance:
[('Area', 0.3008073148750006), ('Baths', 0.06681052454605212), ('Beds', 0.03666963280205741), ('Latitude', 0.1705118833443853), ('Longitude', 0.336136970037758), ('Month', 0.08338354520516944), ('Year', 0.0056801291895770736)]
Mean squared error is of 993685859.8127464
Mean absolute error:21342.3385055244
MAPE:0.2553634740577339
R2 score:0.7539786555656904
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:42:11
epoch 0  | loss: 0.54445 | val_0_rmse: 0.60963 | val_1_rmse: 0.60367 |  0:00:02s
epoch 1  | loss: 0.34878 | val_0_rmse: 0.57795 | val_1_rmse: 0.57911 |  0:00:04s
epoch 2  | loss: 0.32969 | val_0_rmse: 0.56222 | val_1_rmse: 0.56382 |  0:00:06s
epoch 3  | loss: 0.31465 | val_0_rmse: 0.55082 | val_1_rmse: 0.54913 |  0:00:08s
epoch 4  | loss: 0.31061 | val_0_rmse: 0.55424 | val_1_rmse: 0.55611 |  0:00:10s
epoch 5  | loss: 0.30456 | val_0_rmse: 0.53836 | val_1_rmse: 0.54069 |  0:00:12s
epoch 6  | loss: 0.29774 | val_0_rmse: 0.53483 | val_1_rmse: 0.53691 |  0:00:14s
epoch 7  | loss: 0.29277 | val_0_rmse: 0.53011 | val_1_rmse: 0.53179 |  0:00:16s
epoch 8  | loss: 0.2934  | val_0_rmse: 0.53867 | val_1_rmse: 0.53986 |  0:00:18s
epoch 9  | loss: 0.29256 | val_0_rmse: 0.52568 | val_1_rmse: 0.52565 |  0:00:20s
epoch 10 | loss: 0.28851 | val_0_rmse: 0.52896 | val_1_rmse: 0.52993 |  0:00:22s
epoch 11 | loss: 0.28462 | val_0_rmse: 0.53245 | val_1_rmse: 0.53682 |  0:00:24s
epoch 12 | loss: 0.29349 | val_0_rmse: 0.5496  | val_1_rmse: 0.55055 |  0:00:26s
epoch 13 | loss: 0.28981 | val_0_rmse: 0.52948 | val_1_rmse: 0.53146 |  0:00:28s
epoch 14 | loss: 0.28884 | val_0_rmse: 0.52979 | val_1_rmse: 0.53052 |  0:00:30s
epoch 15 | loss: 0.29229 | val_0_rmse: 0.54156 | val_1_rmse: 0.54493 |  0:00:32s
epoch 16 | loss: 0.2816  | val_0_rmse: 0.52342 | val_1_rmse: 0.52568 |  0:00:34s
epoch 17 | loss: 0.28375 | val_0_rmse: 0.52007 | val_1_rmse: 0.52404 |  0:00:36s
epoch 18 | loss: 0.28235 | val_0_rmse: 0.53077 | val_1_rmse: 0.53259 |  0:00:38s
epoch 19 | loss: 0.28349 | val_0_rmse: 0.51937 | val_1_rmse: 0.5242  |  0:00:40s
epoch 20 | loss: 0.27787 | val_0_rmse: 0.52267 | val_1_rmse: 0.529   |  0:00:42s
epoch 21 | loss: 0.27645 | val_0_rmse: 0.51946 | val_1_rmse: 0.52641 |  0:00:44s
epoch 22 | loss: 0.28185 | val_0_rmse: 0.51892 | val_1_rmse: 0.52613 |  0:00:46s
epoch 23 | loss: 0.28003 | val_0_rmse: 0.51762 | val_1_rmse: 0.52465 |  0:00:48s
epoch 24 | loss: 0.27958 | val_0_rmse: 0.52554 | val_1_rmse: 0.52733 |  0:00:50s
epoch 25 | loss: 0.28145 | val_0_rmse: 0.52343 | val_1_rmse: 0.52993 |  0:00:52s
epoch 26 | loss: 0.2739  | val_0_rmse: 0.51771 | val_1_rmse: 0.52331 |  0:00:54s
epoch 27 | loss: 0.27448 | val_0_rmse: 0.51765 | val_1_rmse: 0.52384 |  0:00:56s
epoch 28 | loss: 0.27744 | val_0_rmse: 0.51583 | val_1_rmse: 0.52002 |  0:00:58s
epoch 29 | loss: 0.27399 | val_0_rmse: 0.51282 | val_1_rmse: 0.51836 |  0:01:00s
epoch 30 | loss: 0.27272 | val_0_rmse: 0.51401 | val_1_rmse: 0.5191  |  0:01:02s
epoch 31 | loss: 0.27187 | val_0_rmse: 0.528   | val_1_rmse: 0.53437 |  0:01:04s
epoch 32 | loss: 0.2718  | val_0_rmse: 0.51379 | val_1_rmse: 0.52048 |  0:01:06s
epoch 33 | loss: 0.27224 | val_0_rmse: 0.5167  | val_1_rmse: 0.52567 |  0:01:08s
epoch 34 | loss: 0.27106 | val_0_rmse: 0.5163  | val_1_rmse: 0.52401 |  0:01:10s
epoch 35 | loss: 0.26906 | val_0_rmse: 0.56617 | val_1_rmse: 0.57082 |  0:01:12s
epoch 36 | loss: 0.26775 | val_0_rmse: 0.54143 | val_1_rmse: 0.54571 |  0:01:14s
epoch 37 | loss: 0.26522 | val_0_rmse: 0.50823 | val_1_rmse: 0.51169 |  0:01:16s
epoch 38 | loss: 0.26467 | val_0_rmse: 0.58333 | val_1_rmse: 0.59139 |  0:01:18s
epoch 39 | loss: 0.2672  | val_0_rmse: 0.51755 | val_1_rmse: 0.5201  |  0:01:20s
epoch 40 | loss: 0.26448 | val_0_rmse: 0.57908 | val_1_rmse: 0.58438 |  0:01:22s
epoch 41 | loss: 0.26677 | val_0_rmse: 0.52404 | val_1_rmse: 0.52371 |  0:01:24s
epoch 42 | loss: 0.26352 | val_0_rmse: 0.51922 | val_1_rmse: 0.51937 |  0:01:26s
epoch 43 | loss: 0.26079 | val_0_rmse: 0.52143 | val_1_rmse: 0.51901 |  0:01:28s
epoch 44 | loss: 0.2627  | val_0_rmse: 0.54562 | val_1_rmse: 0.54934 |  0:01:30s
epoch 45 | loss: 0.26212 | val_0_rmse: 0.5333  | val_1_rmse: 0.53755 |  0:01:32s
epoch 46 | loss: 0.25672 | val_0_rmse: 0.53603 | val_1_rmse: 0.53929 |  0:01:34s
epoch 47 | loss: 0.25232 | val_0_rmse: 0.5092  | val_1_rmse: 0.51252 |  0:01:36s
epoch 48 | loss: 0.25011 | val_0_rmse: 0.53264 | val_1_rmse: 0.53448 |  0:01:39s
epoch 49 | loss: 0.26369 | val_0_rmse: 0.51593 | val_1_rmse: 0.51428 |  0:01:40s
epoch 50 | loss: 0.25759 | val_0_rmse: 0.50313 | val_1_rmse: 0.50564 |  0:01:42s
epoch 51 | loss: 0.25776 | val_0_rmse: 0.4945  | val_1_rmse: 0.49651 |  0:01:44s
epoch 52 | loss: 0.25631 | val_0_rmse: 0.49944 | val_1_rmse: 0.50178 |  0:01:46s
epoch 53 | loss: 0.2525  | val_0_rmse: 0.49457 | val_1_rmse: 0.49739 |  0:01:49s
epoch 54 | loss: 0.25806 | val_0_rmse: 0.54604 | val_1_rmse: 0.55044 |  0:01:51s
epoch 55 | loss: 0.25249 | val_0_rmse: 0.49567 | val_1_rmse: 0.4965  |  0:01:53s
epoch 56 | loss: 0.25129 | val_0_rmse: 0.58105 | val_1_rmse: 0.58927 |  0:01:55s
epoch 57 | loss: 0.25042 | val_0_rmse: 0.49059 | val_1_rmse: 0.49686 |  0:01:57s
epoch 58 | loss: 0.24852 | val_0_rmse: 0.50332 | val_1_rmse: 0.50523 |  0:01:59s
epoch 59 | loss: 0.25165 | val_0_rmse: 0.51181 | val_1_rmse: 0.51785 |  0:02:01s
epoch 60 | loss: 0.25438 | val_0_rmse: 0.49454 | val_1_rmse: 0.49756 |  0:02:03s
epoch 61 | loss: 0.2575  | val_0_rmse: 0.56061 | val_1_rmse: 0.56507 |  0:02:05s
epoch 62 | loss: 0.25796 | val_0_rmse: 0.50519 | val_1_rmse: 0.50923 |  0:02:07s
epoch 63 | loss: 0.26085 | val_0_rmse: 0.50213 | val_1_rmse: 0.5028  |  0:02:09s
epoch 64 | loss: 0.25208 | val_0_rmse: 0.53647 | val_1_rmse: 0.54365 |  0:02:11s
epoch 65 | loss: 0.25583 | val_0_rmse: 0.51285 | val_1_rmse: 0.51838 |  0:02:13s
epoch 66 | loss: 0.25074 | val_0_rmse: 0.51983 | val_1_rmse: 0.52756 |  0:02:15s
epoch 67 | loss: 0.24736 | val_0_rmse: 0.51938 | val_1_rmse: 0.52527 |  0:02:17s
epoch 68 | loss: 0.2499  | val_0_rmse: 0.49523 | val_1_rmse: 0.50093 |  0:02:19s
epoch 69 | loss: 0.25091 | val_0_rmse: 0.51857 | val_1_rmse: 0.52458 |  0:02:21s
epoch 70 | loss: 0.25617 | val_0_rmse: 0.50421 | val_1_rmse: 0.50789 |  0:02:23s
epoch 71 | loss: 0.25182 | val_0_rmse: 0.53082 | val_1_rmse: 0.53268 |  0:02:25s
epoch 72 | loss: 0.25116 | val_0_rmse: 0.517   | val_1_rmse: 0.52247 |  0:02:27s
epoch 73 | loss: 0.25624 | val_0_rmse: 0.5127  | val_1_rmse: 0.51709 |  0:02:29s
epoch 74 | loss: 0.25934 | val_0_rmse: 0.50724 | val_1_rmse: 0.51211 |  0:02:31s
epoch 75 | loss: 0.24783 | val_0_rmse: 0.54963 | val_1_rmse: 0.5557  |  0:02:33s
epoch 76 | loss: 0.24987 | val_0_rmse: 0.59434 | val_1_rmse: 0.60248 |  0:02:35s
epoch 77 | loss: 0.26374 | val_0_rmse: 0.58061 | val_1_rmse: 0.57495 |  0:02:37s
epoch 78 | loss: 0.28856 | val_0_rmse: 0.61531 | val_1_rmse: 0.60902 |  0:02:39s
epoch 79 | loss: 0.30151 | val_0_rmse: 0.55024 | val_1_rmse: 0.54959 |  0:02:41s
epoch 80 | loss: 0.27882 | val_0_rmse: 0.52864 | val_1_rmse: 0.52597 |  0:02:43s
epoch 81 | loss: 0.27322 | val_0_rmse: 0.52289 | val_1_rmse: 0.52351 |  0:02:45s
epoch 82 | loss: 0.27098 | val_0_rmse: 0.53119 | val_1_rmse: 0.52888 |  0:02:47s
epoch 83 | loss: 0.26188 | val_0_rmse: 0.55451 | val_1_rmse: 0.5578  |  0:02:49s
epoch 84 | loss: 0.25454 | val_0_rmse: 0.63105 | val_1_rmse: 0.63855 |  0:02:51s
epoch 85 | loss: 0.2816  | val_0_rmse: 0.59644 | val_1_rmse: 0.60192 |  0:02:53s

Early stopping occured at epoch 85 with best_epoch = 55 and best_val_1_rmse = 0.4965
Best weights from best epoch are automatically used!
ended training at: 20:45:06
Feature importance:
[('Area', 0.40368285636621243), ('Baths', 0.07505086620592692), ('Beds', 0.06685538011901249), ('Latitude', 0.06762931316124238), ('Longitude', 0.22227829358426496), ('Month', 0.03208960739463237), ('Year', 0.13241368316870847)]
Mean squared error is of 1007449093.5342441
Mean absolute error:21682.07933403929
MAPE:0.27103342897874044
R2 score:0.7541794579008891
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:45:07
epoch 0  | loss: 0.52937 | val_0_rmse: 0.61068 | val_1_rmse: 0.61229 |  0:00:02s
epoch 1  | loss: 0.34937 | val_0_rmse: 0.57472 | val_1_rmse: 0.58304 |  0:00:04s
epoch 2  | loss: 0.32852 | val_0_rmse: 0.56808 | val_1_rmse: 0.57245 |  0:00:06s
epoch 3  | loss: 0.32642 | val_0_rmse: 0.58785 | val_1_rmse: 0.58774 |  0:00:08s
epoch 4  | loss: 0.31356 | val_0_rmse: 0.54195 | val_1_rmse: 0.5447  |  0:00:10s
epoch 5  | loss: 0.30075 | val_0_rmse: 0.53952 | val_1_rmse: 0.54203 |  0:00:12s
epoch 6  | loss: 0.30192 | val_0_rmse: 0.53337 | val_1_rmse: 0.53628 |  0:00:14s
epoch 7  | loss: 0.29591 | val_0_rmse: 0.53091 | val_1_rmse: 0.53186 |  0:00:16s
epoch 8  | loss: 0.29146 | val_0_rmse: 0.52876 | val_1_rmse: 0.53372 |  0:00:18s
epoch 9  | loss: 0.29078 | val_0_rmse: 0.53139 | val_1_rmse: 0.53477 |  0:00:20s
epoch 10 | loss: 0.29683 | val_0_rmse: 0.52683 | val_1_rmse: 0.53214 |  0:00:22s
epoch 11 | loss: 0.28953 | val_0_rmse: 0.52707 | val_1_rmse: 0.53439 |  0:00:24s
epoch 12 | loss: 0.2882  | val_0_rmse: 0.52175 | val_1_rmse: 0.52631 |  0:00:26s
epoch 13 | loss: 0.29179 | val_0_rmse: 0.5269  | val_1_rmse: 0.53033 |  0:00:28s
epoch 14 | loss: 0.28222 | val_0_rmse: 0.52386 | val_1_rmse: 0.52845 |  0:00:30s
epoch 15 | loss: 0.28593 | val_0_rmse: 0.53016 | val_1_rmse: 0.53457 |  0:00:32s
epoch 16 | loss: 0.28273 | val_0_rmse: 0.53428 | val_1_rmse: 0.53908 |  0:00:34s
epoch 17 | loss: 0.29061 | val_0_rmse: 0.52701 | val_1_rmse: 0.52906 |  0:00:36s
epoch 18 | loss: 0.28156 | val_0_rmse: 0.59566 | val_1_rmse: 0.59731 |  0:00:38s
epoch 19 | loss: 0.27942 | val_0_rmse: 0.52785 | val_1_rmse: 0.53458 |  0:00:40s
epoch 20 | loss: 0.28516 | val_0_rmse: 0.55002 | val_1_rmse: 0.55678 |  0:00:42s
epoch 21 | loss: 0.2797  | val_0_rmse: 0.52126 | val_1_rmse: 0.52498 |  0:00:44s
epoch 22 | loss: 0.27863 | val_0_rmse: 0.53859 | val_1_rmse: 0.54303 |  0:00:46s
epoch 23 | loss: 0.27871 | val_0_rmse: 0.51713 | val_1_rmse: 0.52314 |  0:00:48s
epoch 24 | loss: 0.27684 | val_0_rmse: 0.51958 | val_1_rmse: 0.52344 |  0:00:50s
epoch 25 | loss: 0.27556 | val_0_rmse: 0.52461 | val_1_rmse: 0.53014 |  0:00:52s
epoch 26 | loss: 0.27237 | val_0_rmse: 0.51657 | val_1_rmse: 0.52164 |  0:00:54s
epoch 27 | loss: 0.2762  | val_0_rmse: 0.56376 | val_1_rmse: 0.56778 |  0:00:56s
epoch 28 | loss: 0.27373 | val_0_rmse: 0.51537 | val_1_rmse: 0.52206 |  0:00:58s
epoch 29 | loss: 0.27365 | val_0_rmse: 0.5177  | val_1_rmse: 0.52341 |  0:01:00s
epoch 30 | loss: 0.27557 | val_0_rmse: 0.51362 | val_1_rmse: 0.51821 |  0:01:02s
epoch 31 | loss: 0.27725 | val_0_rmse: 0.51298 | val_1_rmse: 0.51797 |  0:01:04s
epoch 32 | loss: 0.27679 | val_0_rmse: 0.53058 | val_1_rmse: 0.53435 |  0:01:06s
epoch 33 | loss: 0.27934 | val_0_rmse: 0.52842 | val_1_rmse: 0.53731 |  0:01:08s
epoch 34 | loss: 0.2736  | val_0_rmse: 0.51164 | val_1_rmse: 0.51774 |  0:01:10s
epoch 35 | loss: 0.27345 | val_0_rmse: 0.52287 | val_1_rmse: 0.52926 |  0:01:12s
epoch 36 | loss: 0.27086 | val_0_rmse: 0.51579 | val_1_rmse: 0.52212 |  0:01:14s
epoch 37 | loss: 0.27283 | val_0_rmse: 0.51444 | val_1_rmse: 0.5222  |  0:01:16s
epoch 38 | loss: 0.26891 | val_0_rmse: 0.55378 | val_1_rmse: 0.557   |  0:01:18s
epoch 39 | loss: 0.27214 | val_0_rmse: 0.56881 | val_1_rmse: 0.5712  |  0:01:20s
epoch 40 | loss: 0.27151 | val_0_rmse: 0.50655 | val_1_rmse: 0.51271 |  0:01:22s
epoch 41 | loss: 0.26902 | val_0_rmse: 0.55884 | val_1_rmse: 0.56251 |  0:01:24s
epoch 42 | loss: 0.27382 | val_0_rmse: 0.52002 | val_1_rmse: 0.52659 |  0:01:26s
epoch 43 | loss: 0.26805 | val_0_rmse: 0.51074 | val_1_rmse: 0.51663 |  0:01:28s
epoch 44 | loss: 0.26185 | val_0_rmse: 0.50588 | val_1_rmse: 0.5125  |  0:01:30s
epoch 45 | loss: 0.26085 | val_0_rmse: 0.54765 | val_1_rmse: 0.55324 |  0:01:32s
epoch 46 | loss: 0.26197 | val_0_rmse: 0.49799 | val_1_rmse: 0.50197 |  0:01:34s
epoch 47 | loss: 0.26201 | val_0_rmse: 0.5326  | val_1_rmse: 0.53372 |  0:01:36s
epoch 48 | loss: 0.25751 | val_0_rmse: 0.51726 | val_1_rmse: 0.52147 |  0:01:38s
epoch 49 | loss: 0.25968 | val_0_rmse: 0.52278 | val_1_rmse: 0.52563 |  0:01:41s
epoch 50 | loss: 0.25512 | val_0_rmse: 0.49806 | val_1_rmse: 0.50436 |  0:01:43s
epoch 51 | loss: 0.25286 | val_0_rmse: 0.49939 | val_1_rmse: 0.50388 |  0:01:45s
epoch 52 | loss: 0.25227 | val_0_rmse: 0.49653 | val_1_rmse: 0.50231 |  0:01:47s
epoch 53 | loss: 0.25479 | val_0_rmse: 0.52339 | val_1_rmse: 0.52427 |  0:01:49s
epoch 54 | loss: 0.25013 | val_0_rmse: 0.64912 | val_1_rmse: 0.64943 |  0:01:51s
epoch 55 | loss: 0.24708 | val_0_rmse: 0.49296 | val_1_rmse: 0.50018 |  0:01:53s
epoch 56 | loss: 0.25348 | val_0_rmse: 0.57747 | val_1_rmse: 0.57991 |  0:01:55s
epoch 57 | loss: 0.25224 | val_0_rmse: 0.507   | val_1_rmse: 0.51319 |  0:01:57s
epoch 58 | loss: 0.25257 | val_0_rmse: 0.49989 | val_1_rmse: 0.50871 |  0:01:59s
epoch 59 | loss: 0.24871 | val_0_rmse: 0.4974  | val_1_rmse: 0.50266 |  0:02:01s
epoch 60 | loss: 0.24708 | val_0_rmse: 0.55763 | val_1_rmse: 0.56347 |  0:02:03s
epoch 61 | loss: 0.24769 | val_0_rmse: 0.49533 | val_1_rmse: 0.50324 |  0:02:05s
epoch 62 | loss: 0.24806 | val_0_rmse: 0.57695 | val_1_rmse: 0.57964 |  0:02:07s
epoch 63 | loss: 0.24827 | val_0_rmse: 0.4895  | val_1_rmse: 0.49533 |  0:02:09s
epoch 64 | loss: 0.24917 | val_0_rmse: 0.54238 | val_1_rmse: 0.54561 |  0:02:11s
epoch 65 | loss: 0.24612 | val_0_rmse: 0.4888  | val_1_rmse: 0.49813 |  0:02:13s
epoch 66 | loss: 0.24521 | val_0_rmse: 0.60638 | val_1_rmse: 0.61021 |  0:02:15s
epoch 67 | loss: 0.24543 | val_0_rmse: 0.48598 | val_1_rmse: 0.4924  |  0:02:17s
epoch 68 | loss: 0.24312 | val_0_rmse: 0.52641 | val_1_rmse: 0.52927 |  0:02:19s
epoch 69 | loss: 0.24514 | val_0_rmse: 0.49794 | val_1_rmse: 0.50371 |  0:02:21s
epoch 70 | loss: 0.24737 | val_0_rmse: 0.48548 | val_1_rmse: 0.49318 |  0:02:23s
epoch 71 | loss: 0.24267 | val_0_rmse: 0.50203 | val_1_rmse: 0.50843 |  0:02:25s
epoch 72 | loss: 0.23995 | val_0_rmse: 0.51763 | val_1_rmse: 0.52446 |  0:02:27s
epoch 73 | loss: 0.24295 | val_0_rmse: 0.48694 | val_1_rmse: 0.49542 |  0:02:29s
epoch 74 | loss: 0.23942 | val_0_rmse: 0.53145 | val_1_rmse: 0.54023 |  0:02:31s
epoch 75 | loss: 0.24106 | val_0_rmse: 0.58325 | val_1_rmse: 0.58878 |  0:02:33s
epoch 76 | loss: 0.24358 | val_0_rmse: 0.4857  | val_1_rmse: 0.49532 |  0:02:35s
epoch 77 | loss: 0.2435  | val_0_rmse: 0.52364 | val_1_rmse: 0.52917 |  0:02:37s
epoch 78 | loss: 0.24614 | val_0_rmse: 0.50782 | val_1_rmse: 0.51512 |  0:02:39s
epoch 79 | loss: 0.24279 | val_0_rmse: 0.53671 | val_1_rmse: 0.54581 |  0:02:41s
epoch 80 | loss: 0.24417 | val_0_rmse: 0.49073 | val_1_rmse: 0.49917 |  0:02:43s
epoch 81 | loss: 0.23821 | val_0_rmse: 0.50722 | val_1_rmse: 0.51621 |  0:02:45s
epoch 82 | loss: 0.2386  | val_0_rmse: 0.50443 | val_1_rmse: 0.51208 |  0:02:47s
epoch 83 | loss: 0.2415  | val_0_rmse: 0.54859 | val_1_rmse: 0.55345 |  0:02:49s
epoch 84 | loss: 0.24262 | val_0_rmse: 0.49505 | val_1_rmse: 0.50215 |  0:02:51s
epoch 85 | loss: 0.24413 | val_0_rmse: 0.51146 | val_1_rmse: 0.52531 |  0:02:53s
epoch 86 | loss: 0.24744 | val_0_rmse: 0.48443 | val_1_rmse: 0.49577 |  0:02:55s
epoch 87 | loss: 0.24181 | val_0_rmse: 0.57873 | val_1_rmse: 0.58557 |  0:02:57s
epoch 88 | loss: 0.24417 | val_0_rmse: 0.48565 | val_1_rmse: 0.4956  |  0:02:59s
epoch 89 | loss: 0.24384 | val_0_rmse: 0.51669 | val_1_rmse: 0.52476 |  0:03:01s
epoch 90 | loss: 0.23921 | val_0_rmse: 0.48393 | val_1_rmse: 0.49632 |  0:03:03s
epoch 91 | loss: 0.24305 | val_0_rmse: 0.51796 | val_1_rmse: 0.5263  |  0:03:05s
epoch 92 | loss: 0.2421  | val_0_rmse: 0.50375 | val_1_rmse: 0.51464 |  0:03:07s
epoch 93 | loss: 0.2388  | val_0_rmse: 0.49379 | val_1_rmse: 0.50417 |  0:03:09s
epoch 94 | loss: 0.23636 | val_0_rmse: 0.47785 | val_1_rmse: 0.48993 |  0:03:11s
epoch 95 | loss: 0.23884 | val_0_rmse: 0.50325 | val_1_rmse: 0.5164  |  0:03:13s
epoch 96 | loss: 0.23997 | val_0_rmse: 0.47659 | val_1_rmse: 0.48792 |  0:03:15s
epoch 97 | loss: 0.24572 | val_0_rmse: 0.48588 | val_1_rmse: 0.49387 |  0:03:17s
epoch 98 | loss: 0.24133 | val_0_rmse: 0.48605 | val_1_rmse: 0.49716 |  0:03:19s
epoch 99 | loss: 0.23929 | val_0_rmse: 0.51707 | val_1_rmse: 0.5248  |  0:03:21s
epoch 100| loss: 0.23681 | val_0_rmse: 0.56567 | val_1_rmse: 0.57109 |  0:03:23s
epoch 101| loss: 0.24254 | val_0_rmse: 0.55197 | val_1_rmse: 0.56098 |  0:03:25s
epoch 102| loss: 0.24065 | val_0_rmse: 0.50003 | val_1_rmse: 0.50851 |  0:03:27s
epoch 103| loss: 0.23686 | val_0_rmse: 0.50632 | val_1_rmse: 0.51942 |  0:03:29s
epoch 104| loss: 0.23379 | val_0_rmse: 0.48735 | val_1_rmse: 0.49845 |  0:03:31s
epoch 105| loss: 0.23741 | val_0_rmse: 0.49853 | val_1_rmse: 0.51321 |  0:03:33s
epoch 106| loss: 0.2369  | val_0_rmse: 0.52755 | val_1_rmse: 0.53796 |  0:03:35s
epoch 107| loss: 0.2333  | val_0_rmse: 0.50871 | val_1_rmse: 0.52036 |  0:03:37s
epoch 108| loss: 0.23364 | val_0_rmse: 0.48376 | val_1_rmse: 0.49926 |  0:03:39s
epoch 109| loss: 0.23954 | val_0_rmse: 0.49195 | val_1_rmse: 0.50789 |  0:03:41s
epoch 110| loss: 0.23238 | val_0_rmse: 0.52316 | val_1_rmse: 0.53306 |  0:03:43s
epoch 111| loss: 0.234   | val_0_rmse: 0.48366 | val_1_rmse: 0.49796 |  0:03:45s
epoch 112| loss: 0.23738 | val_0_rmse: 0.57368 | val_1_rmse: 0.58191 |  0:03:47s
epoch 113| loss: 0.23478 | val_0_rmse: 0.48769 | val_1_rmse: 0.50187 |  0:03:49s
epoch 114| loss: 0.23602 | val_0_rmse: 0.494   | val_1_rmse: 0.50756 |  0:03:51s
epoch 115| loss: 0.23289 | val_0_rmse: 0.52437 | val_1_rmse: 0.53945 |  0:03:54s
epoch 116| loss: 0.23373 | val_0_rmse: 0.47277 | val_1_rmse: 0.48615 |  0:03:56s
epoch 117| loss: 0.23635 | val_0_rmse: 0.58263 | val_1_rmse: 0.59198 |  0:03:58s
epoch 118| loss: 0.23573 | val_0_rmse: 0.50007 | val_1_rmse: 0.51395 |  0:04:00s
epoch 119| loss: 0.23551 | val_0_rmse: 0.53358 | val_1_rmse: 0.54315 |  0:04:02s
epoch 120| loss: 0.23376 | val_0_rmse: 0.49899 | val_1_rmse: 0.51017 |  0:04:04s
epoch 121| loss: 0.24502 | val_0_rmse: 0.49936 | val_1_rmse: 0.51281 |  0:04:06s
epoch 122| loss: 0.24313 | val_0_rmse: 0.48634 | val_1_rmse: 0.49843 |  0:04:08s
epoch 123| loss: 0.24333 | val_0_rmse: 0.48255 | val_1_rmse: 0.49129 |  0:04:10s
epoch 124| loss: 0.23725 | val_0_rmse: 0.48101 | val_1_rmse: 0.49299 |  0:04:12s
epoch 125| loss: 0.24795 | val_0_rmse: 0.52414 | val_1_rmse: 0.53036 |  0:04:14s
epoch 126| loss: 0.2432  | val_0_rmse: 0.49415 | val_1_rmse: 0.50672 |  0:04:16s
epoch 127| loss: 0.24194 | val_0_rmse: 0.47951 | val_1_rmse: 0.4865  |  0:04:18s
epoch 128| loss: 0.24154 | val_0_rmse: 0.54966 | val_1_rmse: 0.55823 |  0:04:20s
epoch 129| loss: 0.24119 | val_0_rmse: 0.48999 | val_1_rmse: 0.49912 |  0:04:22s
epoch 130| loss: 0.23981 | val_0_rmse: 0.48635 | val_1_rmse: 0.49397 |  0:04:24s
epoch 131| loss: 0.23729 | val_0_rmse: 0.5084  | val_1_rmse: 0.51961 |  0:04:26s
epoch 132| loss: 0.23476 | val_0_rmse: 0.53533 | val_1_rmse: 0.54516 |  0:04:28s
epoch 133| loss: 0.23189 | val_0_rmse: 0.55238 | val_1_rmse: 0.56549 |  0:04:30s
epoch 134| loss: 0.23164 | val_0_rmse: 0.53585 | val_1_rmse: 0.54646 |  0:04:32s
epoch 135| loss: 0.2287  | val_0_rmse: 0.56276 | val_1_rmse: 0.57304 |  0:04:34s
epoch 136| loss: 0.22971 | val_0_rmse: 0.47383 | val_1_rmse: 0.49337 |  0:04:36s
epoch 137| loss: 0.23343 | val_0_rmse: 0.51708 | val_1_rmse: 0.53182 |  0:04:38s
epoch 138| loss: 0.23689 | val_0_rmse: 0.48235 | val_1_rmse: 0.4966  |  0:04:40s
epoch 139| loss: 0.22826 | val_0_rmse: 0.47799 | val_1_rmse: 0.48974 |  0:04:42s
epoch 140| loss: 0.23042 | val_0_rmse: 0.46994 | val_1_rmse: 0.48698 |  0:04:44s
epoch 141| loss: 0.23109 | val_0_rmse: 0.48785 | val_1_rmse: 0.50434 |  0:04:46s
epoch 142| loss: 0.23295 | val_0_rmse: 0.53226 | val_1_rmse: 0.54492 |  0:04:48s
epoch 143| loss: 0.23064 | val_0_rmse: 0.47533 | val_1_rmse: 0.48962 |  0:04:50s
epoch 144| loss: 0.2299  | val_0_rmse: 0.49058 | val_1_rmse: 0.50578 |  0:04:52s
epoch 145| loss: 0.22956 | val_0_rmse: 0.53267 | val_1_rmse: 0.54522 |  0:04:54s
epoch 146| loss: 0.23135 | val_0_rmse: 0.49256 | val_1_rmse: 0.50389 |  0:04:56s

Early stopping occured at epoch 146 with best_epoch = 116 and best_val_1_rmse = 0.48615
Best weights from best epoch are automatically used!
ended training at: 20:50:04
Feature importance:
[('Area', 0.3621846020846194), ('Baths', 0.07916484241989641), ('Beds', 9.756480077191768e-06), ('Latitude', 2.1532699337994177e-07), ('Longitude', 0.38412085001097457), ('Month', 0.12512058817807498), ('Year', 0.04939914549936407)]
Mean squared error is of 948417797.2820073
Mean absolute error:21268.80310880791
MAPE:0.27443331273162597
R2 score:0.7679203004023456
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:50:04
epoch 0  | loss: 0.58807 | val_0_rmse: 0.65153 | val_1_rmse: 0.6543  |  0:00:02s
epoch 1  | loss: 0.37831 | val_0_rmse: 0.60153 | val_1_rmse: 0.59893 |  0:00:04s
epoch 2  | loss: 0.3477  | val_0_rmse: 0.56854 | val_1_rmse: 0.57292 |  0:00:06s
epoch 3  | loss: 0.32558 | val_0_rmse: 0.56383 | val_1_rmse: 0.56354 |  0:00:08s
epoch 4  | loss: 0.3143  | val_0_rmse: 0.56081 | val_1_rmse: 0.56479 |  0:00:10s
epoch 5  | loss: 0.30427 | val_0_rmse: 0.53381 | val_1_rmse: 0.53736 |  0:00:12s
epoch 6  | loss: 0.30314 | val_0_rmse: 0.54348 | val_1_rmse: 0.54581 |  0:00:14s
epoch 7  | loss: 0.2994  | val_0_rmse: 0.53233 | val_1_rmse: 0.5338  |  0:00:16s
epoch 8  | loss: 0.29327 | val_0_rmse: 0.53337 | val_1_rmse: 0.53727 |  0:00:18s
epoch 9  | loss: 0.29151 | val_0_rmse: 0.53517 | val_1_rmse: 0.53994 |  0:00:20s
epoch 10 | loss: 0.28894 | val_0_rmse: 0.55217 | val_1_rmse: 0.55377 |  0:00:22s
epoch 11 | loss: 0.28992 | val_0_rmse: 0.52593 | val_1_rmse: 0.52979 |  0:00:24s
epoch 12 | loss: 0.285   | val_0_rmse: 0.52808 | val_1_rmse: 0.53299 |  0:00:26s
epoch 13 | loss: 0.28039 | val_0_rmse: 0.5155  | val_1_rmse: 0.52094 |  0:00:28s
epoch 14 | loss: 0.27873 | val_0_rmse: 0.51455 | val_1_rmse: 0.52176 |  0:00:30s
epoch 15 | loss: 0.2787  | val_0_rmse: 0.53299 | val_1_rmse: 0.54165 |  0:00:32s
epoch 16 | loss: 0.28131 | val_0_rmse: 0.53302 | val_1_rmse: 0.53721 |  0:00:34s
epoch 17 | loss: 0.27759 | val_0_rmse: 0.52807 | val_1_rmse: 0.53008 |  0:00:36s
epoch 18 | loss: 0.27815 | val_0_rmse: 0.54248 | val_1_rmse: 0.54389 |  0:00:38s
epoch 19 | loss: 0.27518 | val_0_rmse: 0.52018 | val_1_rmse: 0.51887 |  0:00:40s
epoch 20 | loss: 0.27428 | val_0_rmse: 0.57005 | val_1_rmse: 0.56088 |  0:00:42s
epoch 21 | loss: 0.27252 | val_0_rmse: 0.54143 | val_1_rmse: 0.54051 |  0:00:44s
epoch 22 | loss: 0.27181 | val_0_rmse: 0.5165  | val_1_rmse: 0.51857 |  0:00:46s
epoch 23 | loss: 0.27652 | val_0_rmse: 0.54392 | val_1_rmse: 0.54168 |  0:00:48s
epoch 24 | loss: 0.27486 | val_0_rmse: 0.52918 | val_1_rmse: 0.5269  |  0:00:50s
epoch 25 | loss: 0.27035 | val_0_rmse: 0.59623 | val_1_rmse: 0.5943  |  0:00:52s
epoch 26 | loss: 0.27334 | val_0_rmse: 0.54128 | val_1_rmse: 0.53989 |  0:00:54s
epoch 27 | loss: 0.27014 | val_0_rmse: 0.58151 | val_1_rmse: 0.58518 |  0:00:56s
epoch 28 | loss: 0.27218 | val_0_rmse: 0.58775 | val_1_rmse: 0.585   |  0:00:58s
epoch 29 | loss: 0.27111 | val_0_rmse: 0.52625 | val_1_rmse: 0.52289 |  0:01:00s
epoch 30 | loss: 0.26463 | val_0_rmse: 0.64358 | val_1_rmse: 0.64274 |  0:01:02s
epoch 31 | loss: 0.26728 | val_0_rmse: 0.53777 | val_1_rmse: 0.53507 |  0:01:04s
epoch 32 | loss: 0.25783 | val_0_rmse: 0.54282 | val_1_rmse: 0.54216 |  0:01:06s
epoch 33 | loss: 0.26174 | val_0_rmse: 0.55383 | val_1_rmse: 0.55463 |  0:01:08s
epoch 34 | loss: 0.25709 | val_0_rmse: 0.52817 | val_1_rmse: 0.52905 |  0:01:10s
epoch 35 | loss: 0.25627 | val_0_rmse: 0.59576 | val_1_rmse: 0.59579 |  0:01:12s
epoch 36 | loss: 0.26095 | val_0_rmse: 0.54653 | val_1_rmse: 0.54584 |  0:01:14s
epoch 37 | loss: 0.25223 | val_0_rmse: 0.54206 | val_1_rmse: 0.54138 |  0:01:16s
epoch 38 | loss: 0.25716 | val_0_rmse: 0.52759 | val_1_rmse: 0.5284  |  0:01:18s
epoch 39 | loss: 0.25032 | val_0_rmse: 0.51961 | val_1_rmse: 0.51762 |  0:01:20s
epoch 40 | loss: 0.2541  | val_0_rmse: 0.59982 | val_1_rmse: 0.59689 |  0:01:22s
epoch 41 | loss: 0.25418 | val_0_rmse: 0.54443 | val_1_rmse: 0.54255 |  0:01:24s
epoch 42 | loss: 0.25428 | val_0_rmse: 0.5253  | val_1_rmse: 0.52248 |  0:01:26s
epoch 43 | loss: 0.24575 | val_0_rmse: 0.5266  | val_1_rmse: 0.527   |  0:01:28s
epoch 44 | loss: 0.25235 | val_0_rmse: 0.51174 | val_1_rmse: 0.51341 |  0:01:30s
epoch 45 | loss: 0.24914 | val_0_rmse: 0.54844 | val_1_rmse: 0.54844 |  0:01:32s
epoch 46 | loss: 0.25588 | val_0_rmse: 0.67244 | val_1_rmse: 0.66809 |  0:01:34s
epoch 47 | loss: 0.25629 | val_0_rmse: 0.51596 | val_1_rmse: 0.51473 |  0:01:37s
epoch 48 | loss: 0.25213 | val_0_rmse: 0.53639 | val_1_rmse: 0.53433 |  0:01:39s
epoch 49 | loss: 0.25161 | val_0_rmse: 0.49282 | val_1_rmse: 0.49708 |  0:01:41s
epoch 50 | loss: 0.25127 | val_0_rmse: 0.56253 | val_1_rmse: 0.55764 |  0:01:42s
epoch 51 | loss: 0.25364 | val_0_rmse: 0.52719 | val_1_rmse: 0.52798 |  0:01:45s
epoch 52 | loss: 0.24712 | val_0_rmse: 0.51297 | val_1_rmse: 0.51585 |  0:01:47s
epoch 53 | loss: 0.24823 | val_0_rmse: 0.54982 | val_1_rmse: 0.55046 |  0:01:49s
epoch 54 | loss: 0.25202 | val_0_rmse: 0.5246  | val_1_rmse: 0.52407 |  0:01:51s
epoch 55 | loss: 0.24744 | val_0_rmse: 0.54103 | val_1_rmse: 0.54338 |  0:01:53s
epoch 56 | loss: 0.24646 | val_0_rmse: 0.51746 | val_1_rmse: 0.51973 |  0:01:55s
epoch 57 | loss: 0.25181 | val_0_rmse: 0.6093  | val_1_rmse: 0.60799 |  0:01:57s
epoch 58 | loss: 0.25274 | val_0_rmse: 0.5363  | val_1_rmse: 0.5351  |  0:01:59s
epoch 59 | loss: 0.25231 | val_0_rmse: 0.53738 | val_1_rmse: 0.53659 |  0:02:01s
epoch 60 | loss: 0.25076 | val_0_rmse: 0.49177 | val_1_rmse: 0.49501 |  0:02:03s
epoch 61 | loss: 0.25113 | val_0_rmse: 0.50203 | val_1_rmse: 0.50618 |  0:02:05s
epoch 62 | loss: 0.24899 | val_0_rmse: 0.56151 | val_1_rmse: 0.56065 |  0:02:07s
epoch 63 | loss: 0.24448 | val_0_rmse: 0.53263 | val_1_rmse: 0.53505 |  0:02:09s
epoch 64 | loss: 0.24733 | val_0_rmse: 0.51778 | val_1_rmse: 0.51901 |  0:02:11s
epoch 65 | loss: 0.24714 | val_0_rmse: 0.49636 | val_1_rmse: 0.49532 |  0:02:13s
epoch 66 | loss: 0.243   | val_0_rmse: 0.54374 | val_1_rmse: 0.5478  |  0:02:15s
epoch 67 | loss: 0.24317 | val_0_rmse: 0.57489 | val_1_rmse: 0.57533 |  0:02:17s
epoch 68 | loss: 0.24421 | val_0_rmse: 0.48034 | val_1_rmse: 0.4849  |  0:02:19s
epoch 69 | loss: 0.24673 | val_0_rmse: 0.54844 | val_1_rmse: 0.54847 |  0:02:21s
epoch 70 | loss: 0.24474 | val_0_rmse: 0.51485 | val_1_rmse: 0.51562 |  0:02:23s
epoch 71 | loss: 0.24448 | val_0_rmse: 0.52436 | val_1_rmse: 0.53035 |  0:02:25s
epoch 72 | loss: 0.24401 | val_0_rmse: 0.62315 | val_1_rmse: 0.62183 |  0:02:27s
epoch 73 | loss: 0.24329 | val_0_rmse: 0.54753 | val_1_rmse: 0.54709 |  0:02:29s
epoch 74 | loss: 0.24359 | val_0_rmse: 0.53886 | val_1_rmse: 0.53718 |  0:02:31s
epoch 75 | loss: 0.24345 | val_0_rmse: 0.47852 | val_1_rmse: 0.48548 |  0:02:33s
epoch 76 | loss: 0.25655 | val_0_rmse: 0.72826 | val_1_rmse: 0.72381 |  0:02:35s
epoch 77 | loss: 0.25893 | val_0_rmse: 0.53873 | val_1_rmse: 0.53674 |  0:02:37s
epoch 78 | loss: 0.25469 | val_0_rmse: 0.66307 | val_1_rmse: 0.66082 |  0:02:39s
epoch 79 | loss: 0.25013 | val_0_rmse: 0.50913 | val_1_rmse: 0.51083 |  0:02:41s
epoch 80 | loss: 0.2489  | val_0_rmse: 0.61604 | val_1_rmse: 0.61448 |  0:02:43s
epoch 81 | loss: 0.24252 | val_0_rmse: 0.51892 | val_1_rmse: 0.52244 |  0:02:45s
epoch 82 | loss: 0.24891 | val_0_rmse: 0.63985 | val_1_rmse: 0.63735 |  0:02:47s
epoch 83 | loss: 0.24716 | val_0_rmse: 0.56884 | val_1_rmse: 0.56917 |  0:02:49s
epoch 84 | loss: 0.2467  | val_0_rmse: 0.59639 | val_1_rmse: 0.59369 |  0:02:51s
epoch 85 | loss: 0.2452  | val_0_rmse: 0.55272 | val_1_rmse: 0.55336 |  0:02:53s
epoch 86 | loss: 0.24097 | val_0_rmse: 0.50078 | val_1_rmse: 0.50258 |  0:02:55s
epoch 87 | loss: 0.24502 | val_0_rmse: 0.52102 | val_1_rmse: 0.52655 |  0:02:57s
epoch 88 | loss: 0.241   | val_0_rmse: 0.51602 | val_1_rmse: 0.51678 |  0:02:59s
epoch 89 | loss: 0.23873 | val_0_rmse: 0.51951 | val_1_rmse: 0.52427 |  0:03:01s
epoch 90 | loss: 0.24202 | val_0_rmse: 0.54079 | val_1_rmse: 0.53859 |  0:03:03s
epoch 91 | loss: 0.24064 | val_0_rmse: 0.55839 | val_1_rmse: 0.55725 |  0:03:05s
epoch 92 | loss: 0.23712 | val_0_rmse: 0.51033 | val_1_rmse: 0.51126 |  0:03:07s
epoch 93 | loss: 0.2401  | val_0_rmse: 0.52867 | val_1_rmse: 0.5319  |  0:03:09s
epoch 94 | loss: 0.2385  | val_0_rmse: 0.52846 | val_1_rmse: 0.53082 |  0:03:11s
epoch 95 | loss: 0.24093 | val_0_rmse: 0.50708 | val_1_rmse: 0.51199 |  0:03:13s
epoch 96 | loss: 0.2385  | val_0_rmse: 0.53135 | val_1_rmse: 0.53555 |  0:03:15s
epoch 97 | loss: 0.24199 | val_0_rmse: 0.49464 | val_1_rmse: 0.49623 |  0:03:17s
epoch 98 | loss: 0.23797 | val_0_rmse: 0.52978 | val_1_rmse: 0.53342 |  0:03:20s

Early stopping occured at epoch 98 with best_epoch = 68 and best_val_1_rmse = 0.4849
Best weights from best epoch are automatically used!
ended training at: 20:53:25
Feature importance:
[('Area', 0.44397998990266474), ('Baths', 0.08336442381672096), ('Beds', 0.030976119497535287), ('Latitude', 0.1597720285883349), ('Longitude', 0.20293633734475772), ('Month', 0.00894414244301887), ('Year', 0.07002695840696756)]
Mean squared error is of 971027166.8873785
Mean absolute error:20920.247380448247
MAPE:0.24253506185267953
R2 score:0.7566427756401074
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: co properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:53:26
epoch 0  | loss: 0.57414 | val_0_rmse: 0.64738 | val_1_rmse: 0.64682 |  0:00:02s
epoch 1  | loss: 0.37148 | val_0_rmse: 0.58795 | val_1_rmse: 0.58099 |  0:00:04s
epoch 2  | loss: 0.34234 | val_0_rmse: 0.57799 | val_1_rmse: 0.5746  |  0:00:06s
epoch 3  | loss: 0.33715 | val_0_rmse: 0.55669 | val_1_rmse: 0.55356 |  0:00:08s
epoch 4  | loss: 0.33092 | val_0_rmse: 0.56739 | val_1_rmse: 0.56727 |  0:00:10s
epoch 5  | loss: 0.32422 | val_0_rmse: 0.55791 | val_1_rmse: 0.55505 |  0:00:12s
epoch 6  | loss: 0.31691 | val_0_rmse: 0.55733 | val_1_rmse: 0.55548 |  0:00:14s
epoch 7  | loss: 0.31823 | val_0_rmse: 0.56543 | val_1_rmse: 0.56057 |  0:00:16s
epoch 8  | loss: 0.3126  | val_0_rmse: 0.55039 | val_1_rmse: 0.54548 |  0:00:18s
epoch 9  | loss: 0.3108  | val_0_rmse: 0.54339 | val_1_rmse: 0.54091 |  0:00:20s
epoch 10 | loss: 0.30571 | val_0_rmse: 0.57663 | val_1_rmse: 0.57265 |  0:00:22s
epoch 11 | loss: 0.30837 | val_0_rmse: 0.54385 | val_1_rmse: 0.53969 |  0:00:24s
epoch 12 | loss: 0.30444 | val_0_rmse: 0.53739 | val_1_rmse: 0.53443 |  0:00:26s
epoch 13 | loss: 0.29664 | val_0_rmse: 0.58763 | val_1_rmse: 0.58149 |  0:00:28s
epoch 14 | loss: 0.29257 | val_0_rmse: 0.52101 | val_1_rmse: 0.5217  |  0:00:30s
epoch 15 | loss: 0.28325 | val_0_rmse: 0.54279 | val_1_rmse: 0.53918 |  0:00:32s
epoch 16 | loss: 0.28722 | val_0_rmse: 0.59008 | val_1_rmse: 0.58851 |  0:00:34s
epoch 17 | loss: 0.28074 | val_0_rmse: 0.56636 | val_1_rmse: 0.56404 |  0:00:36s
epoch 18 | loss: 0.28335 | val_0_rmse: 0.52361 | val_1_rmse: 0.52667 |  0:00:38s
epoch 19 | loss: 0.27881 | val_0_rmse: 0.56145 | val_1_rmse: 0.56471 |  0:00:40s
epoch 20 | loss: 0.28047 | val_0_rmse: 0.52489 | val_1_rmse: 0.52656 |  0:00:42s
epoch 21 | loss: 0.28268 | val_0_rmse: 0.56856 | val_1_rmse: 0.56541 |  0:00:44s
epoch 22 | loss: 0.27995 | val_0_rmse: 0.53057 | val_1_rmse: 0.53422 |  0:00:46s
epoch 23 | loss: 0.27327 | val_0_rmse: 0.50984 | val_1_rmse: 0.51354 |  0:00:48s
epoch 24 | loss: 0.272   | val_0_rmse: 0.52981 | val_1_rmse: 0.52678 |  0:00:50s
epoch 25 | loss: 0.27614 | val_0_rmse: 0.51666 | val_1_rmse: 0.52113 |  0:00:52s
epoch 26 | loss: 0.27116 | val_0_rmse: 0.55459 | val_1_rmse: 0.55806 |  0:00:54s
epoch 27 | loss: 0.26988 | val_0_rmse: 0.54899 | val_1_rmse: 0.54922 |  0:00:56s
epoch 28 | loss: 0.26739 | val_0_rmse: 0.60918 | val_1_rmse: 0.60662 |  0:00:58s
epoch 29 | loss: 0.26377 | val_0_rmse: 0.52181 | val_1_rmse: 0.51947 |  0:01:01s
epoch 30 | loss: 0.27098 | val_0_rmse: 0.52256 | val_1_rmse: 0.52594 |  0:01:03s
epoch 31 | loss: 0.26849 | val_0_rmse: 0.51537 | val_1_rmse: 0.51453 |  0:01:05s
epoch 32 | loss: 0.26265 | val_0_rmse: 0.49951 | val_1_rmse: 0.50239 |  0:01:07s
epoch 33 | loss: 0.26079 | val_0_rmse: 0.53326 | val_1_rmse: 0.53498 |  0:01:09s
epoch 34 | loss: 0.2769  | val_0_rmse: 0.51944 | val_1_rmse: 0.52083 |  0:01:11s
epoch 35 | loss: 0.26689 | val_0_rmse: 0.51345 | val_1_rmse: 0.51682 |  0:01:13s
epoch 36 | loss: 0.25946 | val_0_rmse: 0.51574 | val_1_rmse: 0.51477 |  0:01:15s
epoch 37 | loss: 0.25395 | val_0_rmse: 0.52074 | val_1_rmse: 0.52148 |  0:01:17s
epoch 38 | loss: 0.25857 | val_0_rmse: 0.56763 | val_1_rmse: 0.56452 |  0:01:19s
epoch 39 | loss: 0.2518  | val_0_rmse: 0.53466 | val_1_rmse: 0.53676 |  0:01:21s
epoch 40 | loss: 0.25949 | val_0_rmse: 0.56734 | val_1_rmse: 0.56529 |  0:01:23s
epoch 41 | loss: 0.25581 | val_0_rmse: 0.55848 | val_1_rmse: 0.55976 |  0:01:25s
epoch 42 | loss: 0.251   | val_0_rmse: 0.54103 | val_1_rmse: 0.54282 |  0:01:27s
epoch 43 | loss: 0.25027 | val_0_rmse: 0.59637 | val_1_rmse: 0.59815 |  0:01:29s
epoch 44 | loss: 0.2487  | val_0_rmse: 0.49112 | val_1_rmse: 0.49459 |  0:01:31s
epoch 45 | loss: 0.2462  | val_0_rmse: 0.59135 | val_1_rmse: 0.59086 |  0:01:33s
epoch 46 | loss: 0.25556 | val_0_rmse: 0.51599 | val_1_rmse: 0.52036 |  0:01:35s
epoch 47 | loss: 0.2461  | val_0_rmse: 0.5696  | val_1_rmse: 0.56752 |  0:01:37s
epoch 48 | loss: 0.2599  | val_0_rmse: 0.54391 | val_1_rmse: 0.54053 |  0:01:39s
epoch 49 | loss: 0.26086 | val_0_rmse: 0.53833 | val_1_rmse: 0.54245 |  0:01:41s
epoch 50 | loss: 0.24997 | val_0_rmse: 0.54858 | val_1_rmse: 0.54787 |  0:01:43s
epoch 51 | loss: 0.25375 | val_0_rmse: 0.50537 | val_1_rmse: 0.50713 |  0:01:45s
epoch 52 | loss: 0.25069 | val_0_rmse: 0.50363 | val_1_rmse: 0.50482 |  0:01:47s
epoch 53 | loss: 0.25064 | val_0_rmse: 0.50963 | val_1_rmse: 0.51075 |  0:01:49s
epoch 54 | loss: 0.25097 | val_0_rmse: 0.61668 | val_1_rmse: 0.61677 |  0:01:51s
epoch 55 | loss: 0.25062 | val_0_rmse: 0.51764 | val_1_rmse: 0.51455 |  0:01:53s
epoch 56 | loss: 0.25325 | val_0_rmse: 0.58184 | val_1_rmse: 0.58077 |  0:01:55s
epoch 57 | loss: 0.24936 | val_0_rmse: 0.54615 | val_1_rmse: 0.54614 |  0:01:57s
epoch 58 | loss: 0.24896 | val_0_rmse: 0.49324 | val_1_rmse: 0.49932 |  0:01:59s
epoch 59 | loss: 0.2461  | val_0_rmse: 0.50384 | val_1_rmse: 0.51075 |  0:02:01s
epoch 60 | loss: 0.24385 | val_0_rmse: 0.54483 | val_1_rmse: 0.54976 |  0:02:03s
epoch 61 | loss: 0.24407 | val_0_rmse: 0.49301 | val_1_rmse: 0.50286 |  0:02:05s
epoch 62 | loss: 0.24623 | val_0_rmse: 0.5536  | val_1_rmse: 0.5572  |  0:02:07s
epoch 63 | loss: 0.24817 | val_0_rmse: 0.54654 | val_1_rmse: 0.54995 |  0:02:09s
epoch 64 | loss: 0.24652 | val_0_rmse: 0.54484 | val_1_rmse: 0.54085 |  0:02:12s
epoch 65 | loss: 0.24812 | val_0_rmse: 0.49523 | val_1_rmse: 0.50059 |  0:02:14s
epoch 66 | loss: 0.24505 | val_0_rmse: 0.49336 | val_1_rmse: 0.49853 |  0:02:16s
epoch 67 | loss: 0.2504  | val_0_rmse: 0.51368 | val_1_rmse: 0.51916 |  0:02:18s
epoch 68 | loss: 0.25318 | val_0_rmse: 0.53444 | val_1_rmse: 0.53373 |  0:02:20s
epoch 69 | loss: 0.25252 | val_0_rmse: 0.49987 | val_1_rmse: 0.50304 |  0:02:22s
epoch 70 | loss: 0.24551 | val_0_rmse: 0.49527 | val_1_rmse: 0.50195 |  0:02:24s
epoch 71 | loss: 0.24268 | val_0_rmse: 0.53312 | val_1_rmse: 0.54265 |  0:02:26s
epoch 72 | loss: 0.24393 | val_0_rmse: 0.48773 | val_1_rmse: 0.49521 |  0:02:28s
epoch 73 | loss: 0.24647 | val_0_rmse: 0.48897 | val_1_rmse: 0.49298 |  0:02:30s
epoch 74 | loss: 0.246   | val_0_rmse: 0.49617 | val_1_rmse: 0.50328 |  0:02:32s
epoch 75 | loss: 0.24424 | val_0_rmse: 0.48294 | val_1_rmse: 0.48876 |  0:02:34s
epoch 76 | loss: 0.24109 | val_0_rmse: 0.57746 | val_1_rmse: 0.57993 |  0:02:36s
epoch 77 | loss: 0.24192 | val_0_rmse: 0.48557 | val_1_rmse: 0.49387 |  0:02:38s
epoch 78 | loss: 0.24393 | val_0_rmse: 0.50092 | val_1_rmse: 0.50492 |  0:02:40s
epoch 79 | loss: 0.25182 | val_0_rmse: 0.53981 | val_1_rmse: 0.54317 |  0:02:42s
epoch 80 | loss: 0.24675 | val_0_rmse: 0.58317 | val_1_rmse: 0.58466 |  0:02:44s
epoch 81 | loss: 0.24485 | val_0_rmse: 0.49995 | val_1_rmse: 0.50811 |  0:02:46s
epoch 82 | loss: 0.24476 | val_0_rmse: 0.48923 | val_1_rmse: 0.49714 |  0:02:48s
epoch 83 | loss: 0.24565 | val_0_rmse: 0.50519 | val_1_rmse: 0.51195 |  0:02:50s
epoch 84 | loss: 0.25917 | val_0_rmse: 0.66111 | val_1_rmse: 0.66198 |  0:02:52s
epoch 85 | loss: 0.24856 | val_0_rmse: 0.50962 | val_1_rmse: 0.5129  |  0:02:54s
epoch 86 | loss: 0.24268 | val_0_rmse: 0.49192 | val_1_rmse: 0.49356 |  0:02:56s
epoch 87 | loss: 0.24383 | val_0_rmse: 0.48726 | val_1_rmse: 0.49494 |  0:02:58s
epoch 88 | loss: 0.24684 | val_0_rmse: 0.50264 | val_1_rmse: 0.50551 |  0:03:00s
epoch 89 | loss: 0.24698 | val_0_rmse: 0.48708 | val_1_rmse: 0.49859 |  0:03:02s
epoch 90 | loss: 0.24513 | val_0_rmse: 0.51978 | val_1_rmse: 0.52354 |  0:03:04s
epoch 91 | loss: 0.24758 | val_0_rmse: 0.53035 | val_1_rmse: 0.53026 |  0:03:06s
epoch 92 | loss: 0.24194 | val_0_rmse: 0.50293 | val_1_rmse: 0.51027 |  0:03:08s
epoch 93 | loss: 0.24132 | val_0_rmse: 0.50645 | val_1_rmse: 0.50757 |  0:03:10s
epoch 94 | loss: 0.2435  | val_0_rmse: 0.48924 | val_1_rmse: 0.49609 |  0:03:12s
epoch 95 | loss: 0.24179 | val_0_rmse: 0.49202 | val_1_rmse: 0.50016 |  0:03:14s
epoch 96 | loss: 0.24167 | val_0_rmse: 0.51347 | val_1_rmse: 0.52245 |  0:03:16s
epoch 97 | loss: 0.24096 | val_0_rmse: 0.49929 | val_1_rmse: 0.50977 |  0:03:18s
epoch 98 | loss: 0.24434 | val_0_rmse: 0.50425 | val_1_rmse: 0.51114 |  0:03:20s
epoch 99 | loss: 0.24145 | val_0_rmse: 0.48943 | val_1_rmse: 0.50034 |  0:03:23s
epoch 100| loss: 0.24572 | val_0_rmse: 0.50302 | val_1_rmse: 0.50671 |  0:03:25s
epoch 101| loss: 0.24005 | val_0_rmse: 0.61536 | val_1_rmse: 0.61869 |  0:03:27s
epoch 102| loss: 0.2478  | val_0_rmse: 0.57589 | val_1_rmse: 0.58773 |  0:03:29s
epoch 103| loss: 0.24422 | val_0_rmse: 0.53882 | val_1_rmse: 0.5471  |  0:03:31s
epoch 104| loss: 0.24089 | val_0_rmse: 0.48288 | val_1_rmse: 0.49307 |  0:03:33s
epoch 105| loss: 0.2405  | val_0_rmse: 0.51281 | val_1_rmse: 0.5273  |  0:03:35s

Early stopping occured at epoch 105 with best_epoch = 75 and best_val_1_rmse = 0.48876
Best weights from best epoch are automatically used!
ended training at: 20:57:01
Feature importance:
[('Area', 0.23805983714094164), ('Baths', 0.2506126315417686), ('Beds', 0.08166789285935873), ('Latitude', 0.1903776470734488), ('Longitude', 0.22891018778292196), ('Month', 0.0), ('Year', 0.010371803601560248)]
Mean squared error is of 962968841.5175651
Mean absolute error:21048.80510535636
MAPE:0.26595374129804855
R2 score:0.7585718504983794
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 20:57:02
epoch 0  | loss: 0.43403 | val_0_rmse: 0.54008 | val_1_rmse: 0.52887 |  0:00:03s
epoch 1  | loss: 0.28353 | val_0_rmse: 0.49283 | val_1_rmse: 0.4824  |  0:00:06s
epoch 2  | loss: 0.25726 | val_0_rmse: 0.48629 | val_1_rmse: 0.47677 |  0:00:09s
epoch 3  | loss: 0.24951 | val_0_rmse: 0.48209 | val_1_rmse: 0.4728  |  0:00:13s
epoch 4  | loss: 0.24301 | val_0_rmse: 0.50277 | val_1_rmse: 0.49414 |  0:00:16s
epoch 5  | loss: 0.23075 | val_0_rmse: 0.46307 | val_1_rmse: 0.45529 |  0:00:19s
epoch 6  | loss: 0.22886 | val_0_rmse: 0.45349 | val_1_rmse: 0.44466 |  0:00:22s
epoch 7  | loss: 0.226   | val_0_rmse: 0.45846 | val_1_rmse: 0.45152 |  0:00:25s
epoch 8  | loss: 0.22674 | val_0_rmse: 0.44831 | val_1_rmse: 0.44102 |  0:00:29s
epoch 9  | loss: 0.22631 | val_0_rmse: 0.48385 | val_1_rmse: 0.47539 |  0:00:32s
epoch 10 | loss: 0.22566 | val_0_rmse: 0.4453  | val_1_rmse: 0.4371  |  0:00:35s
epoch 11 | loss: 0.21663 | val_0_rmse: 0.44494 | val_1_rmse: 0.43796 |  0:00:39s
epoch 12 | loss: 0.21937 | val_0_rmse: 0.45041 | val_1_rmse: 0.44166 |  0:00:42s
epoch 13 | loss: 0.21411 | val_0_rmse: 0.47387 | val_1_rmse: 0.4686  |  0:00:45s
epoch 14 | loss: 0.21629 | val_0_rmse: 0.4533  | val_1_rmse: 0.44622 |  0:00:48s
epoch 15 | loss: 0.21548 | val_0_rmse: 0.44109 | val_1_rmse: 0.43405 |  0:00:52s
epoch 16 | loss: 0.21427 | val_0_rmse: 0.43947 | val_1_rmse: 0.43328 |  0:00:55s
epoch 17 | loss: 0.21251 | val_0_rmse: 0.45931 | val_1_rmse: 0.45347 |  0:00:58s
epoch 18 | loss: 0.21042 | val_0_rmse: 0.43015 | val_1_rmse: 0.42248 |  0:01:01s
epoch 19 | loss: 0.20768 | val_0_rmse: 0.45208 | val_1_rmse: 0.44473 |  0:01:05s
epoch 20 | loss: 0.20942 | val_0_rmse: 0.44803 | val_1_rmse: 0.44328 |  0:01:08s
epoch 21 | loss: 0.2104  | val_0_rmse: 0.45396 | val_1_rmse: 0.44742 |  0:01:11s
epoch 22 | loss: 0.20881 | val_0_rmse: 0.44522 | val_1_rmse: 0.44018 |  0:01:14s
epoch 23 | loss: 0.20495 | val_0_rmse: 0.43661 | val_1_rmse: 0.43076 |  0:01:18s
epoch 24 | loss: 0.20814 | val_0_rmse: 0.43272 | val_1_rmse: 0.42851 |  0:01:21s
epoch 25 | loss: 0.20925 | val_0_rmse: 0.43826 | val_1_rmse: 0.43306 |  0:01:24s
epoch 26 | loss: 0.20646 | val_0_rmse: 0.44651 | val_1_rmse: 0.44343 |  0:01:27s
epoch 27 | loss: 0.20513 | val_0_rmse: 0.43707 | val_1_rmse: 0.43    |  0:01:31s
epoch 28 | loss: 0.20109 | val_0_rmse: 0.44214 | val_1_rmse: 0.43811 |  0:01:34s
epoch 29 | loss: 0.20153 | val_0_rmse: 0.44387 | val_1_rmse: 0.43998 |  0:01:37s
epoch 30 | loss: 0.20116 | val_0_rmse: 0.43767 | val_1_rmse: 0.43419 |  0:01:40s
epoch 31 | loss: 0.2082  | val_0_rmse: 0.45436 | val_1_rmse: 0.44925 |  0:01:44s
epoch 32 | loss: 0.20538 | val_0_rmse: 0.42906 | val_1_rmse: 0.42725 |  0:01:47s
epoch 33 | loss: 0.20546 | val_0_rmse: 0.429   | val_1_rmse: 0.42619 |  0:01:50s
epoch 34 | loss: 0.19996 | val_0_rmse: 0.44251 | val_1_rmse: 0.43891 |  0:01:53s
epoch 35 | loss: 0.20192 | val_0_rmse: 0.42403 | val_1_rmse: 0.42172 |  0:01:57s
epoch 36 | loss: 0.19722 | val_0_rmse: 0.43579 | val_1_rmse: 0.43259 |  0:02:00s
epoch 37 | loss: 0.19837 | val_0_rmse: 0.45359 | val_1_rmse: 0.45179 |  0:02:03s
epoch 38 | loss: 0.20605 | val_0_rmse: 0.43343 | val_1_rmse: 0.42923 |  0:02:06s
epoch 39 | loss: 0.19686 | val_0_rmse: 0.42552 | val_1_rmse: 0.4248  |  0:02:10s
epoch 40 | loss: 0.19506 | val_0_rmse: 0.42196 | val_1_rmse: 0.41849 |  0:02:13s
epoch 41 | loss: 0.19526 | val_0_rmse: 0.42231 | val_1_rmse: 0.42075 |  0:02:16s
epoch 42 | loss: 0.19985 | val_0_rmse: 0.41857 | val_1_rmse: 0.41591 |  0:02:19s
epoch 43 | loss: 0.19353 | val_0_rmse: 0.42707 | val_1_rmse: 0.42369 |  0:02:23s
epoch 44 | loss: 0.19625 | val_0_rmse: 0.43634 | val_1_rmse: 0.4335  |  0:02:26s
epoch 45 | loss: 0.2008  | val_0_rmse: 0.42105 | val_1_rmse: 0.421   |  0:02:29s
epoch 46 | loss: 0.19719 | val_0_rmse: 0.42892 | val_1_rmse: 0.42754 |  0:02:32s
epoch 47 | loss: 0.1993  | val_0_rmse: 0.42559 | val_1_rmse: 0.42391 |  0:02:36s
epoch 48 | loss: 0.194   | val_0_rmse: 0.42973 | val_1_rmse: 0.4282  |  0:02:39s
epoch 49 | loss: 0.1936  | val_0_rmse: 0.42305 | val_1_rmse: 0.42101 |  0:02:42s
epoch 50 | loss: 0.19042 | val_0_rmse: 0.43593 | val_1_rmse: 0.43289 |  0:02:45s
epoch 51 | loss: 0.19283 | val_0_rmse: 0.41652 | val_1_rmse: 0.41716 |  0:02:49s
epoch 52 | loss: 0.19233 | val_0_rmse: 0.43201 | val_1_rmse: 0.42959 |  0:02:52s
epoch 53 | loss: 0.19421 | val_0_rmse: 0.41696 | val_1_rmse: 0.4169  |  0:02:55s
epoch 54 | loss: 0.19064 | val_0_rmse: 0.41856 | val_1_rmse: 0.42032 |  0:02:58s
epoch 55 | loss: 0.19349 | val_0_rmse: 0.44025 | val_1_rmse: 0.43891 |  0:03:02s
epoch 56 | loss: 0.19171 | val_0_rmse: 0.4163  | val_1_rmse: 0.41802 |  0:03:05s
epoch 57 | loss: 0.19063 | val_0_rmse: 0.42063 | val_1_rmse: 0.42131 |  0:03:08s
epoch 58 | loss: 0.19177 | val_0_rmse: 0.43163 | val_1_rmse: 0.43143 |  0:03:12s
epoch 59 | loss: 0.19149 | val_0_rmse: 0.42347 | val_1_rmse: 0.42428 |  0:03:15s
epoch 60 | loss: 0.19247 | val_0_rmse: 0.4253  | val_1_rmse: 0.42486 |  0:03:18s
epoch 61 | loss: 0.1921  | val_0_rmse: 0.43634 | val_1_rmse: 0.43509 |  0:03:21s
epoch 62 | loss: 0.19298 | val_0_rmse: 0.41953 | val_1_rmse: 0.41996 |  0:03:24s
epoch 63 | loss: 0.1904  | val_0_rmse: 0.41801 | val_1_rmse: 0.41875 |  0:03:28s
epoch 64 | loss: 0.19426 | val_0_rmse: 0.42558 | val_1_rmse: 0.42523 |  0:03:31s
epoch 65 | loss: 0.19018 | val_0_rmse: 0.43118 | val_1_rmse: 0.42945 |  0:03:34s
epoch 66 | loss: 0.18876 | val_0_rmse: 0.41493 | val_1_rmse: 0.41617 |  0:03:37s
epoch 67 | loss: 0.18733 | val_0_rmse: 0.41776 | val_1_rmse: 0.41677 |  0:03:41s
epoch 68 | loss: 0.19056 | val_0_rmse: 0.41391 | val_1_rmse: 0.41626 |  0:03:44s
epoch 69 | loss: 0.19107 | val_0_rmse: 0.42811 | val_1_rmse: 0.43165 |  0:03:47s
epoch 70 | loss: 0.18933 | val_0_rmse: 0.41143 | val_1_rmse: 0.4172  |  0:03:50s
epoch 71 | loss: 0.18699 | val_0_rmse: 0.41645 | val_1_rmse: 0.41851 |  0:03:54s
epoch 72 | loss: 0.18597 | val_0_rmse: 0.41392 | val_1_rmse: 0.41607 |  0:03:57s

Early stopping occured at epoch 72 with best_epoch = 42 and best_val_1_rmse = 0.41591
Best weights from best epoch are automatically used!
ended training at: 21:01:00
Feature importance:
[('Area', 0.08357751058408544), ('Baths', 0.12285649630174567), ('Beds', 0.060902677557168265), ('Latitude', 0.1205372646475976), ('Longitude', 0.22548917163725202), ('Month', 0.017524221568953197), ('Year', 0.3691126577031978)]
Mean squared error is of 10165310009.781672
Mean absolute error:69915.07278717306
MAPE:0.2772268568525985
R2 score:0.8228389492520628
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:01:01
epoch 0  | loss: 0.47153 | val_0_rmse: 0.59914 | val_1_rmse: 0.59159 |  0:00:03s
epoch 1  | loss: 0.31769 | val_0_rmse: 0.52762 | val_1_rmse: 0.52368 |  0:00:06s
epoch 2  | loss: 0.26666 | val_0_rmse: 0.49205 | val_1_rmse: 0.48442 |  0:00:09s
epoch 3  | loss: 0.24768 | val_0_rmse: 0.48413 | val_1_rmse: 0.47961 |  0:00:13s
epoch 4  | loss: 0.2395  | val_0_rmse: 0.46001 | val_1_rmse: 0.45424 |  0:00:16s
epoch 5  | loss: 0.23251 | val_0_rmse: 0.45004 | val_1_rmse: 0.44397 |  0:00:19s
epoch 6  | loss: 0.22717 | val_0_rmse: 0.45104 | val_1_rmse: 0.44802 |  0:00:22s
epoch 7  | loss: 0.21877 | val_0_rmse: 0.47118 | val_1_rmse: 0.47122 |  0:00:26s
epoch 8  | loss: 0.22024 | val_0_rmse: 0.4777  | val_1_rmse: 0.47744 |  0:00:29s
epoch 9  | loss: 0.22238 | val_0_rmse: 0.47832 | val_1_rmse: 0.47722 |  0:00:32s
epoch 10 | loss: 0.21858 | val_0_rmse: 0.44761 | val_1_rmse: 0.44853 |  0:00:35s
epoch 11 | loss: 0.21269 | val_0_rmse: 0.44129 | val_1_rmse: 0.44215 |  0:00:39s
epoch 12 | loss: 0.21436 | val_0_rmse: 0.43646 | val_1_rmse: 0.43704 |  0:00:42s
epoch 13 | loss: 0.21535 | val_0_rmse: 0.45247 | val_1_rmse: 0.45112 |  0:00:45s
epoch 14 | loss: 0.21479 | val_0_rmse: 0.43612 | val_1_rmse: 0.43666 |  0:00:48s
epoch 15 | loss: 0.20887 | val_0_rmse: 0.43843 | val_1_rmse: 0.43834 |  0:00:52s
epoch 16 | loss: 0.21501 | val_0_rmse: 0.43785 | val_1_rmse: 0.4375  |  0:00:55s
epoch 17 | loss: 0.21052 | val_0_rmse: 0.43772 | val_1_rmse: 0.44073 |  0:00:58s
epoch 18 | loss: 0.20606 | val_0_rmse: 0.43776 | val_1_rmse: 0.43729 |  0:01:01s
epoch 19 | loss: 0.20381 | val_0_rmse: 0.4494  | val_1_rmse: 0.4511  |  0:01:05s
epoch 20 | loss: 0.21319 | val_0_rmse: 0.46971 | val_1_rmse: 0.4684  |  0:01:08s
epoch 21 | loss: 0.20583 | val_0_rmse: 0.42869 | val_1_rmse: 0.43068 |  0:01:11s
epoch 22 | loss: 0.20425 | val_0_rmse: 0.45232 | val_1_rmse: 0.45413 |  0:01:14s
epoch 23 | loss: 0.2037  | val_0_rmse: 0.4375  | val_1_rmse: 0.43853 |  0:01:18s
epoch 24 | loss: 0.20605 | val_0_rmse: 0.43613 | val_1_rmse: 0.43719 |  0:01:21s
epoch 25 | loss: 0.20525 | val_0_rmse: 0.44315 | val_1_rmse: 0.44575 |  0:01:24s
epoch 26 | loss: 0.19972 | val_0_rmse: 0.43699 | val_1_rmse: 0.43796 |  0:01:27s
epoch 27 | loss: 0.20178 | val_0_rmse: 0.43    | val_1_rmse: 0.43244 |  0:01:31s
epoch 28 | loss: 0.19812 | val_0_rmse: 0.42965 | val_1_rmse: 0.4298  |  0:01:34s
epoch 29 | loss: 0.19773 | val_0_rmse: 0.43676 | val_1_rmse: 0.43851 |  0:01:37s
epoch 30 | loss: 0.20045 | val_0_rmse: 0.42804 | val_1_rmse: 0.42915 |  0:01:41s
epoch 31 | loss: 0.19988 | val_0_rmse: 0.4435  | val_1_rmse: 0.4485  |  0:01:44s
epoch 32 | loss: 0.199   | val_0_rmse: 0.42951 | val_1_rmse: 0.43092 |  0:01:47s
epoch 33 | loss: 0.19759 | val_0_rmse: 0.42574 | val_1_rmse: 0.4294  |  0:01:50s
epoch 34 | loss: 0.20047 | val_0_rmse: 0.44754 | val_1_rmse: 0.4492  |  0:01:54s
epoch 35 | loss: 0.19993 | val_0_rmse: 0.42839 | val_1_rmse: 0.4299  |  0:01:57s
epoch 36 | loss: 0.19975 | val_0_rmse: 0.4262  | val_1_rmse: 0.42708 |  0:02:00s
epoch 37 | loss: 0.19572 | val_0_rmse: 0.42815 | val_1_rmse: 0.4297  |  0:02:03s
epoch 38 | loss: 0.20337 | val_0_rmse: 0.43446 | val_1_rmse: 0.43704 |  0:02:07s
epoch 39 | loss: 0.20055 | val_0_rmse: 0.41942 | val_1_rmse: 0.42163 |  0:02:10s
epoch 40 | loss: 0.19352 | val_0_rmse: 0.42408 | val_1_rmse: 0.42897 |  0:02:13s
epoch 41 | loss: 0.19271 | val_0_rmse: 0.42155 | val_1_rmse: 0.42438 |  0:02:16s
epoch 42 | loss: 0.19642 | val_0_rmse: 0.4389  | val_1_rmse: 0.44379 |  0:02:20s
epoch 43 | loss: 0.19934 | val_0_rmse: 0.43052 | val_1_rmse: 0.43345 |  0:02:23s
epoch 44 | loss: 0.19465 | val_0_rmse: 0.41994 | val_1_rmse: 0.4229  |  0:02:26s
epoch 45 | loss: 0.1997  | val_0_rmse: 0.42231 | val_1_rmse: 0.42674 |  0:02:29s
epoch 46 | loss: 0.19496 | val_0_rmse: 0.42685 | val_1_rmse: 0.4312  |  0:02:33s
epoch 47 | loss: 0.18994 | val_0_rmse: 0.41637 | val_1_rmse: 0.42091 |  0:02:36s
epoch 48 | loss: 0.19969 | val_0_rmse: 0.42024 | val_1_rmse: 0.42246 |  0:02:39s
epoch 49 | loss: 0.19737 | val_0_rmse: 0.42699 | val_1_rmse: 0.43064 |  0:02:42s
epoch 50 | loss: 0.19847 | val_0_rmse: 0.47457 | val_1_rmse: 0.47555 |  0:02:46s
epoch 51 | loss: 0.21384 | val_0_rmse: 0.43634 | val_1_rmse: 0.43919 |  0:02:49s
epoch 52 | loss: 0.19959 | val_0_rmse: 0.42481 | val_1_rmse: 0.42848 |  0:02:52s
epoch 53 | loss: 0.19692 | val_0_rmse: 0.42872 | val_1_rmse: 0.43041 |  0:02:55s
epoch 54 | loss: 0.19726 | val_0_rmse: 0.42641 | val_1_rmse: 0.42947 |  0:02:59s
epoch 55 | loss: 0.19467 | val_0_rmse: 0.41772 | val_1_rmse: 0.42106 |  0:03:02s
epoch 56 | loss: 0.19377 | val_0_rmse: 0.42613 | val_1_rmse: 0.42871 |  0:03:05s
epoch 57 | loss: 0.19797 | val_0_rmse: 0.42483 | val_1_rmse: 0.42567 |  0:03:08s
epoch 58 | loss: 0.19198 | val_0_rmse: 0.43604 | val_1_rmse: 0.43862 |  0:03:12s
epoch 59 | loss: 0.19325 | val_0_rmse: 0.41087 | val_1_rmse: 0.4146  |  0:03:15s
epoch 60 | loss: 0.19129 | val_0_rmse: 0.41424 | val_1_rmse: 0.41702 |  0:03:18s
epoch 61 | loss: 0.19143 | val_0_rmse: 0.41996 | val_1_rmse: 0.42424 |  0:03:21s
epoch 62 | loss: 0.1926  | val_0_rmse: 0.44111 | val_1_rmse: 0.44218 |  0:03:25s
epoch 63 | loss: 0.19766 | val_0_rmse: 0.44717 | val_1_rmse: 0.45016 |  0:03:28s
epoch 64 | loss: 0.20455 | val_0_rmse: 0.43634 | val_1_rmse: 0.43744 |  0:03:31s
epoch 65 | loss: 0.19371 | val_0_rmse: 0.42316 | val_1_rmse: 0.42524 |  0:03:34s
epoch 66 | loss: 0.19043 | val_0_rmse: 0.43039 | val_1_rmse: 0.43198 |  0:03:38s
epoch 67 | loss: 0.19985 | val_0_rmse: 0.43576 | val_1_rmse: 0.43648 |  0:03:41s
epoch 68 | loss: 0.20308 | val_0_rmse: 0.43038 | val_1_rmse: 0.43286 |  0:03:44s
epoch 69 | loss: 0.2011  | val_0_rmse: 0.43454 | val_1_rmse: 0.4377  |  0:03:47s
epoch 70 | loss: 0.19874 | val_0_rmse: 0.42945 | val_1_rmse: 0.43432 |  0:03:51s
epoch 71 | loss: 0.19078 | val_0_rmse: 0.42365 | val_1_rmse: 0.43088 |  0:03:54s
epoch 72 | loss: 0.19294 | val_0_rmse: 0.42014 | val_1_rmse: 0.42464 |  0:03:57s
epoch 73 | loss: 0.18938 | val_0_rmse: 0.41829 | val_1_rmse: 0.42171 |  0:04:00s
epoch 74 | loss: 0.19029 | val_0_rmse: 0.42656 | val_1_rmse: 0.42676 |  0:04:04s
epoch 75 | loss: 0.20132 | val_0_rmse: 0.43431 | val_1_rmse: 0.4326  |  0:04:07s
epoch 76 | loss: 0.19645 | val_0_rmse: 0.43355 | val_1_rmse: 0.43427 |  0:04:10s
epoch 77 | loss: 0.19276 | val_0_rmse: 0.42843 | val_1_rmse: 0.43028 |  0:04:13s
epoch 78 | loss: 0.1927  | val_0_rmse: 0.42833 | val_1_rmse: 0.43007 |  0:04:17s
epoch 79 | loss: 0.18835 | val_0_rmse: 0.42393 | val_1_rmse: 0.42744 |  0:04:20s
epoch 80 | loss: 0.19096 | val_0_rmse: 0.42007 | val_1_rmse: 0.42506 |  0:04:23s
epoch 81 | loss: 0.18886 | val_0_rmse: 0.4157  | val_1_rmse: 0.41885 |  0:04:27s
epoch 82 | loss: 0.19001 | val_0_rmse: 0.41861 | val_1_rmse: 0.42125 |  0:04:30s
epoch 83 | loss: 0.18772 | val_0_rmse: 0.42228 | val_1_rmse: 0.42622 |  0:04:33s
epoch 84 | loss: 0.19225 | val_0_rmse: 0.41415 | val_1_rmse: 0.4164  |  0:04:36s
epoch 85 | loss: 0.18684 | val_0_rmse: 0.43377 | val_1_rmse: 0.43784 |  0:04:40s
epoch 86 | loss: 0.18765 | val_0_rmse: 0.42934 | val_1_rmse: 0.43363 |  0:04:43s
epoch 87 | loss: 0.18684 | val_0_rmse: 0.42426 | val_1_rmse: 0.42644 |  0:04:46s
epoch 88 | loss: 0.19062 | val_0_rmse: 0.41465 | val_1_rmse: 0.4177  |  0:04:49s
epoch 89 | loss: 0.19122 | val_0_rmse: 0.42679 | val_1_rmse: 0.43083 |  0:04:53s

Early stopping occured at epoch 89 with best_epoch = 59 and best_val_1_rmse = 0.4146
Best weights from best epoch are automatically used!
ended training at: 21:05:55
Feature importance:
[('Area', 0.028662283198938245), ('Baths', 0.22971521798726946), ('Beds', 0.13158042649426988), ('Latitude', 0.14667631760453387), ('Longitude', 0.19042603228044244), ('Month', 0.0522465574534327), ('Year', 0.2206931649811134)]
Mean squared error is of 10180310076.836512
Mean absolute error:69641.14711660222
MAPE:0.29580320467984117
R2 score:0.8257324262009182
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:05:56
epoch 0  | loss: 0.4355  | val_0_rmse: 0.5418  | val_1_rmse: 0.54205 |  0:00:03s
epoch 1  | loss: 0.28354 | val_0_rmse: 0.49239 | val_1_rmse: 0.49356 |  0:00:06s
epoch 2  | loss: 0.25214 | val_0_rmse: 0.48363 | val_1_rmse: 0.48553 |  0:00:09s
epoch 3  | loss: 0.24197 | val_0_rmse: 0.48016 | val_1_rmse: 0.47666 |  0:00:12s
epoch 4  | loss: 0.24343 | val_0_rmse: 0.46495 | val_1_rmse: 0.4669  |  0:00:16s
epoch 5  | loss: 0.23209 | val_0_rmse: 0.46134 | val_1_rmse: 0.46226 |  0:00:19s
epoch 6  | loss: 0.22654 | val_0_rmse: 0.45274 | val_1_rmse: 0.45359 |  0:00:22s
epoch 7  | loss: 0.22156 | val_0_rmse: 0.4524  | val_1_rmse: 0.45084 |  0:00:25s
epoch 8  | loss: 0.22468 | val_0_rmse: 0.45185 | val_1_rmse: 0.45674 |  0:00:29s
epoch 9  | loss: 0.21984 | val_0_rmse: 0.44845 | val_1_rmse: 0.45082 |  0:00:32s
epoch 10 | loss: 0.21398 | val_0_rmse: 0.44108 | val_1_rmse: 0.44394 |  0:00:35s
epoch 11 | loss: 0.21324 | val_0_rmse: 0.43759 | val_1_rmse: 0.43808 |  0:00:39s
epoch 12 | loss: 0.21497 | val_0_rmse: 0.44332 | val_1_rmse: 0.44889 |  0:00:42s
epoch 13 | loss: 0.2095  | val_0_rmse: 0.4378  | val_1_rmse: 0.43938 |  0:00:45s
epoch 14 | loss: 0.21723 | val_0_rmse: 0.45222 | val_1_rmse: 0.45989 |  0:00:48s
epoch 15 | loss: 0.21157 | val_0_rmse: 0.43757 | val_1_rmse: 0.44287 |  0:00:52s
epoch 16 | loss: 0.21075 | val_0_rmse: 0.44194 | val_1_rmse: 0.44519 |  0:00:55s
epoch 17 | loss: 0.20494 | val_0_rmse: 0.4567  | val_1_rmse: 0.46493 |  0:00:58s
epoch 18 | loss: 0.20942 | val_0_rmse: 0.42959 | val_1_rmse: 0.4357  |  0:01:01s
epoch 19 | loss: 0.20301 | val_0_rmse: 0.44267 | val_1_rmse: 0.4475  |  0:01:05s
epoch 20 | loss: 0.20792 | val_0_rmse: 0.43324 | val_1_rmse: 0.43594 |  0:01:08s
epoch 21 | loss: 0.20437 | val_0_rmse: 0.42966 | val_1_rmse: 0.43258 |  0:01:11s
epoch 22 | loss: 0.20535 | val_0_rmse: 0.45043 | val_1_rmse: 0.44932 |  0:01:14s
epoch 23 | loss: 0.20083 | val_0_rmse: 0.43134 | val_1_rmse: 0.43739 |  0:01:18s
epoch 24 | loss: 0.20316 | val_0_rmse: 0.4298  | val_1_rmse: 0.43618 |  0:01:21s
epoch 25 | loss: 0.20533 | val_0_rmse: 0.42893 | val_1_rmse: 0.43582 |  0:01:24s
epoch 26 | loss: 0.20016 | val_0_rmse: 0.42548 | val_1_rmse: 0.43045 |  0:01:27s
epoch 27 | loss: 0.19796 | val_0_rmse: 0.42929 | val_1_rmse: 0.43412 |  0:01:31s
epoch 28 | loss: 0.20091 | val_0_rmse: 0.42489 | val_1_rmse: 0.43276 |  0:01:34s
epoch 29 | loss: 0.19731 | val_0_rmse: 0.4298  | val_1_rmse: 0.43572 |  0:01:37s
epoch 30 | loss: 0.19773 | val_0_rmse: 0.42754 | val_1_rmse: 0.4332  |  0:01:40s
epoch 31 | loss: 0.19343 | val_0_rmse: 0.41964 | val_1_rmse: 0.42741 |  0:01:44s
epoch 32 | loss: 0.19284 | val_0_rmse: 0.4445  | val_1_rmse: 0.44951 |  0:01:47s
epoch 33 | loss: 0.19822 | val_0_rmse: 0.42565 | val_1_rmse: 0.43291 |  0:01:50s
epoch 34 | loss: 0.19615 | val_0_rmse: 0.42419 | val_1_rmse: 0.4321  |  0:01:53s
epoch 35 | loss: 0.19326 | val_0_rmse: 0.42256 | val_1_rmse: 0.43181 |  0:01:57s
epoch 36 | loss: 0.1925  | val_0_rmse: 0.41988 | val_1_rmse: 0.4303  |  0:02:00s
epoch 37 | loss: 0.1896  | val_0_rmse: 0.42967 | val_1_rmse: 0.43281 |  0:02:03s
epoch 38 | loss: 0.19177 | val_0_rmse: 0.42847 | val_1_rmse: 0.43738 |  0:02:06s
epoch 39 | loss: 0.19169 | val_0_rmse: 0.41791 | val_1_rmse: 0.42664 |  0:02:10s
epoch 40 | loss: 0.20177 | val_0_rmse: 0.5054  | val_1_rmse: 0.50881 |  0:02:13s
epoch 41 | loss: 0.19594 | val_0_rmse: 0.44121 | val_1_rmse: 0.45147 |  0:02:16s
epoch 42 | loss: 0.19181 | val_0_rmse: 0.43175 | val_1_rmse: 0.44224 |  0:02:19s
epoch 43 | loss: 0.19086 | val_0_rmse: 0.4319  | val_1_rmse: 0.44283 |  0:02:22s
epoch 44 | loss: 0.19618 | val_0_rmse: 0.43042 | val_1_rmse: 0.44162 |  0:02:26s
epoch 45 | loss: 0.18713 | val_0_rmse: 0.43414 | val_1_rmse: 0.4388  |  0:02:29s
epoch 46 | loss: 0.18885 | val_0_rmse: 0.41799 | val_1_rmse: 0.42772 |  0:02:32s
epoch 47 | loss: 0.19303 | val_0_rmse: 0.41372 | val_1_rmse: 0.42155 |  0:02:36s
epoch 48 | loss: 0.18883 | val_0_rmse: 0.41918 | val_1_rmse: 0.43019 |  0:02:39s
epoch 49 | loss: 0.18572 | val_0_rmse: 0.41587 | val_1_rmse: 0.4251  |  0:02:42s
epoch 50 | loss: 0.18853 | val_0_rmse: 0.41788 | val_1_rmse: 0.42658 |  0:02:46s
epoch 51 | loss: 0.18553 | val_0_rmse: 0.42273 | val_1_rmse: 0.4298  |  0:02:49s
epoch 52 | loss: 0.18591 | val_0_rmse: 0.4123  | val_1_rmse: 0.42322 |  0:02:52s
epoch 53 | loss: 0.1852  | val_0_rmse: 0.43523 | val_1_rmse: 0.44301 |  0:02:55s
epoch 54 | loss: 0.18592 | val_0_rmse: 0.40978 | val_1_rmse: 0.42009 |  0:02:59s
epoch 55 | loss: 0.18797 | val_0_rmse: 0.41582 | val_1_rmse: 0.42616 |  0:03:02s
epoch 56 | loss: 0.19063 | val_0_rmse: 0.41648 | val_1_rmse: 0.4272  |  0:03:05s
epoch 57 | loss: 0.18474 | val_0_rmse: 0.40932 | val_1_rmse: 0.41793 |  0:03:08s
epoch 58 | loss: 0.18493 | val_0_rmse: 0.41622 | val_1_rmse: 0.42284 |  0:03:11s
epoch 59 | loss: 0.18732 | val_0_rmse: 0.42161 | val_1_rmse: 0.43038 |  0:03:15s
epoch 60 | loss: 0.18545 | val_0_rmse: 0.41965 | val_1_rmse: 0.43251 |  0:03:18s
epoch 61 | loss: 0.18157 | val_0_rmse: 0.41267 | val_1_rmse: 0.42392 |  0:03:21s
epoch 62 | loss: 0.18141 | val_0_rmse: 0.41199 | val_1_rmse: 0.4266  |  0:03:25s
epoch 63 | loss: 0.18747 | val_0_rmse: 0.42219 | val_1_rmse: 0.43047 |  0:03:28s
epoch 64 | loss: 0.18451 | val_0_rmse: 0.42781 | val_1_rmse: 0.43603 |  0:03:31s
epoch 65 | loss: 0.18748 | val_0_rmse: 0.42612 | val_1_rmse: 0.43971 |  0:03:34s
epoch 66 | loss: 0.18228 | val_0_rmse: 0.40541 | val_1_rmse: 0.41666 |  0:03:38s
epoch 67 | loss: 0.18348 | val_0_rmse: 0.42222 | val_1_rmse: 0.42877 |  0:03:41s
epoch 68 | loss: 0.18848 | val_0_rmse: 0.41523 | val_1_rmse: 0.42457 |  0:03:44s
epoch 69 | loss: 0.18582 | val_0_rmse: 0.41814 | val_1_rmse: 0.42741 |  0:03:47s
epoch 70 | loss: 0.18506 | val_0_rmse: 0.42151 | val_1_rmse: 0.43298 |  0:03:51s
epoch 71 | loss: 0.1844  | val_0_rmse: 0.41499 | val_1_rmse: 0.42724 |  0:03:54s
epoch 72 | loss: 0.18288 | val_0_rmse: 0.41652 | val_1_rmse: 0.43162 |  0:03:57s
epoch 73 | loss: 0.1809  | val_0_rmse: 0.42011 | val_1_rmse: 0.43017 |  0:04:00s
epoch 74 | loss: 0.18282 | val_0_rmse: 0.40844 | val_1_rmse: 0.41832 |  0:04:04s
epoch 75 | loss: 0.18362 | val_0_rmse: 0.4137  | val_1_rmse: 0.42275 |  0:04:07s
epoch 76 | loss: 0.18285 | val_0_rmse: 0.41006 | val_1_rmse: 0.42147 |  0:04:10s
epoch 77 | loss: 0.18121 | val_0_rmse: 0.41248 | val_1_rmse: 0.42509 |  0:04:13s
epoch 78 | loss: 0.18025 | val_0_rmse: 0.40514 | val_1_rmse: 0.41591 |  0:04:17s
epoch 79 | loss: 0.18016 | val_0_rmse: 0.40672 | val_1_rmse: 0.42055 |  0:04:20s
epoch 80 | loss: 0.18113 | val_0_rmse: 0.41213 | val_1_rmse: 0.42381 |  0:04:23s
epoch 81 | loss: 0.18546 | val_0_rmse: 0.41019 | val_1_rmse: 0.42295 |  0:04:26s
epoch 82 | loss: 0.18099 | val_0_rmse: 0.41166 | val_1_rmse: 0.4235  |  0:04:30s
epoch 83 | loss: 0.17923 | val_0_rmse: 0.40809 | val_1_rmse: 0.42291 |  0:04:33s
epoch 84 | loss: 0.17828 | val_0_rmse: 0.4024  | val_1_rmse: 0.41533 |  0:04:36s
epoch 85 | loss: 0.18111 | val_0_rmse: 0.41331 | val_1_rmse: 0.42644 |  0:04:39s
epoch 86 | loss: 0.17732 | val_0_rmse: 0.41073 | val_1_rmse: 0.42381 |  0:04:43s
epoch 87 | loss: 0.18093 | val_0_rmse: 0.41564 | val_1_rmse: 0.42483 |  0:04:46s
epoch 88 | loss: 0.17773 | val_0_rmse: 0.39528 | val_1_rmse: 0.40987 |  0:04:49s
epoch 89 | loss: 0.17829 | val_0_rmse: 0.41469 | val_1_rmse: 0.42979 |  0:04:52s
epoch 90 | loss: 0.18155 | val_0_rmse: 0.4116  | val_1_rmse: 0.42774 |  0:04:56s
epoch 91 | loss: 0.17867 | val_0_rmse: 0.40111 | val_1_rmse: 0.41549 |  0:04:59s
epoch 92 | loss: 0.17609 | val_0_rmse: 0.39913 | val_1_rmse: 0.41429 |  0:05:02s
epoch 93 | loss: 0.18018 | val_0_rmse: 0.4058  | val_1_rmse: 0.41612 |  0:05:05s
epoch 94 | loss: 0.17702 | val_0_rmse: 0.40638 | val_1_rmse: 0.41685 |  0:05:09s
epoch 95 | loss: 0.17747 | val_0_rmse: 0.40645 | val_1_rmse: 0.42158 |  0:05:12s
epoch 96 | loss: 0.18152 | val_0_rmse: 0.40625 | val_1_rmse: 0.4181  |  0:05:15s
epoch 97 | loss: 0.17773 | val_0_rmse: 0.40752 | val_1_rmse: 0.4196  |  0:05:18s
epoch 98 | loss: 0.17787 | val_0_rmse: 0.40728 | val_1_rmse: 0.41794 |  0:05:22s
epoch 99 | loss: 0.18308 | val_0_rmse: 0.45499 | val_1_rmse: 0.46999 |  0:05:25s
epoch 100| loss: 0.17969 | val_0_rmse: 0.4123  | val_1_rmse: 0.42605 |  0:05:28s
epoch 101| loss: 0.17782 | val_0_rmse: 0.39613 | val_1_rmse: 0.40884 |  0:05:32s
epoch 102| loss: 0.17992 | val_0_rmse: 0.4061  | val_1_rmse: 0.41987 |  0:05:35s
epoch 103| loss: 0.17722 | val_0_rmse: 0.40516 | val_1_rmse: 0.41727 |  0:05:38s
epoch 104| loss: 0.17983 | val_0_rmse: 0.40512 | val_1_rmse: 0.42017 |  0:05:41s
epoch 105| loss: 0.17867 | val_0_rmse: 0.40404 | val_1_rmse: 0.41969 |  0:05:44s
epoch 106| loss: 0.17568 | val_0_rmse: 0.39942 | val_1_rmse: 0.4145  |  0:05:48s
epoch 107| loss: 0.17525 | val_0_rmse: 0.40083 | val_1_rmse: 0.41529 |  0:05:51s
epoch 108| loss: 0.17692 | val_0_rmse: 0.40739 | val_1_rmse: 0.42022 |  0:05:54s
epoch 109| loss: 0.17333 | val_0_rmse: 0.39845 | val_1_rmse: 0.41328 |  0:05:57s
epoch 110| loss: 0.17484 | val_0_rmse: 0.39501 | val_1_rmse: 0.40939 |  0:06:01s
epoch 111| loss: 0.17283 | val_0_rmse: 0.40124 | val_1_rmse: 0.41757 |  0:06:04s
epoch 112| loss: 0.1765  | val_0_rmse: 0.40142 | val_1_rmse: 0.4148  |  0:06:07s
epoch 113| loss: 0.1783  | val_0_rmse: 0.41223 | val_1_rmse: 0.4224  |  0:06:10s
epoch 114| loss: 0.17889 | val_0_rmse: 0.41083 | val_1_rmse: 0.42292 |  0:06:14s
epoch 115| loss: 0.17816 | val_0_rmse: 0.40695 | val_1_rmse: 0.42141 |  0:06:17s
epoch 116| loss: 0.17892 | val_0_rmse: 0.39899 | val_1_rmse: 0.41561 |  0:06:20s
epoch 117| loss: 0.17786 | val_0_rmse: 0.39725 | val_1_rmse: 0.41019 |  0:06:24s
epoch 118| loss: 0.17325 | val_0_rmse: 0.4087  | val_1_rmse: 0.42302 |  0:06:27s
epoch 119| loss: 0.17496 | val_0_rmse: 0.40192 | val_1_rmse: 0.41715 |  0:06:30s
epoch 120| loss: 0.17563 | val_0_rmse: 0.43565 | val_1_rmse: 0.45236 |  0:06:33s
epoch 121| loss: 0.17435 | val_0_rmse: 0.42543 | val_1_rmse: 0.44271 |  0:06:36s
epoch 122| loss: 0.17694 | val_0_rmse: 0.39912 | val_1_rmse: 0.41476 |  0:06:40s
epoch 123| loss: 0.17499 | val_0_rmse: 0.39723 | val_1_rmse: 0.41496 |  0:06:43s
epoch 124| loss: 0.17383 | val_0_rmse: 0.39927 | val_1_rmse: 0.4188  |  0:06:46s
epoch 125| loss: 0.17573 | val_0_rmse: 0.40887 | val_1_rmse: 0.41894 |  0:06:49s
epoch 126| loss: 0.18036 | val_0_rmse: 0.40576 | val_1_rmse: 0.42169 |  0:06:53s
epoch 127| loss: 0.18122 | val_0_rmse: 0.39821 | val_1_rmse: 0.41239 |  0:06:56s
epoch 128| loss: 0.17447 | val_0_rmse: 0.40044 | val_1_rmse: 0.41251 |  0:06:59s
epoch 129| loss: 0.17339 | val_0_rmse: 0.42187 | val_1_rmse: 0.43321 |  0:07:02s
epoch 130| loss: 0.17513 | val_0_rmse: 0.40162 | val_1_rmse: 0.42009 |  0:07:06s
epoch 131| loss: 0.17395 | val_0_rmse: 0.40554 | val_1_rmse: 0.42131 |  0:07:09s

Early stopping occured at epoch 131 with best_epoch = 101 and best_val_1_rmse = 0.40884
Best weights from best epoch are automatically used!
ended training at: 21:13:06
Feature importance:
[('Area', 0.09495940470686058), ('Baths', 0.08672506452120353), ('Beds', 0.12270690128086402), ('Latitude', 0.18096322014626404), ('Longitude', 0.2623185961512689), ('Month', 0.040579525404928984), ('Year', 0.21174728778860996)]
Mean squared error is of 9740502158.25389
Mean absolute error:67983.51148458045
MAPE:0.274581082837828
R2 score:0.8315349946056086
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:13:07
epoch 0  | loss: 0.43868 | val_0_rmse: 0.52297 | val_1_rmse: 0.52723 |  0:00:03s
epoch 1  | loss: 0.27196 | val_0_rmse: 0.48857 | val_1_rmse: 0.49774 |  0:00:06s
epoch 2  | loss: 0.25295 | val_0_rmse: 0.48893 | val_1_rmse: 0.49706 |  0:00:09s
epoch 3  | loss: 0.23662 | val_0_rmse: 0.49638 | val_1_rmse: 0.50079 |  0:00:13s
epoch 4  | loss: 0.24222 | val_0_rmse: 0.47559 | val_1_rmse: 0.48356 |  0:00:16s
epoch 5  | loss: 0.23175 | val_0_rmse: 0.4491  | val_1_rmse: 0.45824 |  0:00:19s
epoch 6  | loss: 0.22521 | val_0_rmse: 0.50789 | val_1_rmse: 0.519   |  0:00:22s
epoch 7  | loss: 0.22276 | val_0_rmse: 0.45655 | val_1_rmse: 0.46742 |  0:00:26s
epoch 8  | loss: 0.21979 | val_0_rmse: 0.45828 | val_1_rmse: 0.46707 |  0:00:29s
epoch 9  | loss: 0.21646 | val_0_rmse: 0.43799 | val_1_rmse: 0.44783 |  0:00:32s
epoch 10 | loss: 0.21582 | val_0_rmse: 0.44506 | val_1_rmse: 0.45447 |  0:00:35s
epoch 11 | loss: 0.21488 | val_0_rmse: 0.44668 | val_1_rmse: 0.45663 |  0:00:39s
epoch 12 | loss: 0.21049 | val_0_rmse: 0.44832 | val_1_rmse: 0.45754 |  0:00:42s
epoch 13 | loss: 0.20813 | val_0_rmse: 0.44135 | val_1_rmse: 0.45365 |  0:00:45s
epoch 14 | loss: 0.2103  | val_0_rmse: 0.43793 | val_1_rmse: 0.44624 |  0:00:48s
epoch 15 | loss: 0.20715 | val_0_rmse: 0.44471 | val_1_rmse: 0.45633 |  0:00:52s
epoch 16 | loss: 0.21119 | val_0_rmse: 0.44409 | val_1_rmse: 0.45354 |  0:00:55s
epoch 17 | loss: 0.20738 | val_0_rmse: 0.45451 | val_1_rmse: 0.47035 |  0:00:58s
epoch 18 | loss: 0.20721 | val_0_rmse: 0.44993 | val_1_rmse: 0.46686 |  0:01:01s
epoch 19 | loss: 0.20329 | val_0_rmse: 0.42386 | val_1_rmse: 0.43589 |  0:01:05s
epoch 20 | loss: 0.2022  | val_0_rmse: 0.44756 | val_1_rmse: 0.46059 |  0:01:08s
epoch 21 | loss: 0.20742 | val_0_rmse: 0.43148 | val_1_rmse: 0.44628 |  0:01:11s
epoch 22 | loss: 0.20059 | val_0_rmse: 0.42807 | val_1_rmse: 0.44402 |  0:01:14s
epoch 23 | loss: 0.20139 | val_0_rmse: 0.42892 | val_1_rmse: 0.44498 |  0:01:18s
epoch 24 | loss: 0.2023  | val_0_rmse: 0.42725 | val_1_rmse: 0.44069 |  0:01:21s
epoch 25 | loss: 0.20271 | val_0_rmse: 0.43368 | val_1_rmse: 0.44867 |  0:01:24s
epoch 26 | loss: 0.19621 | val_0_rmse: 0.43402 | val_1_rmse: 0.4497  |  0:01:27s
epoch 27 | loss: 0.19696 | val_0_rmse: 0.4298  | val_1_rmse: 0.445   |  0:01:31s
epoch 28 | loss: 0.19992 | val_0_rmse: 0.42819 | val_1_rmse: 0.44028 |  0:01:34s
epoch 29 | loss: 0.19739 | val_0_rmse: 0.42997 | val_1_rmse: 0.44145 |  0:01:37s
epoch 30 | loss: 0.1951  | val_0_rmse: 0.41836 | val_1_rmse: 0.43599 |  0:01:40s
epoch 31 | loss: 0.19201 | val_0_rmse: 0.42286 | val_1_rmse: 0.43632 |  0:01:44s
epoch 32 | loss: 0.20051 | val_0_rmse: 0.44601 | val_1_rmse: 0.45899 |  0:01:47s
epoch 33 | loss: 0.20056 | val_0_rmse: 0.42872 | val_1_rmse: 0.44251 |  0:01:50s
epoch 34 | loss: 0.19618 | val_0_rmse: 0.44684 | val_1_rmse: 0.46046 |  0:01:53s
epoch 35 | loss: 0.19646 | val_0_rmse: 0.42815 | val_1_rmse: 0.44039 |  0:01:57s
epoch 36 | loss: 0.1948  | val_0_rmse: 0.42801 | val_1_rmse: 0.44381 |  0:02:00s
epoch 37 | loss: 0.19374 | val_0_rmse: 0.42988 | val_1_rmse: 0.4429  |  0:02:03s
epoch 38 | loss: 0.19418 | val_0_rmse: 0.42166 | val_1_rmse: 0.43822 |  0:02:06s
epoch 39 | loss: 0.19228 | val_0_rmse: 0.4261  | val_1_rmse: 0.43809 |  0:02:10s
epoch 40 | loss: 0.19332 | val_0_rmse: 0.42064 | val_1_rmse: 0.43184 |  0:02:13s
epoch 41 | loss: 0.19361 | val_0_rmse: 0.42006 | val_1_rmse: 0.4337  |  0:02:16s
epoch 42 | loss: 0.18933 | val_0_rmse: 0.41755 | val_1_rmse: 0.43318 |  0:02:20s
epoch 43 | loss: 0.19799 | val_0_rmse: 0.41379 | val_1_rmse: 0.4268  |  0:02:23s
epoch 44 | loss: 0.19291 | val_0_rmse: 0.42314 | val_1_rmse: 0.44186 |  0:02:27s
epoch 45 | loss: 0.19524 | val_0_rmse: 0.41976 | val_1_rmse: 0.43436 |  0:02:30s
epoch 46 | loss: 0.1925  | val_0_rmse: 0.42636 | val_1_rmse: 0.44055 |  0:02:34s
epoch 47 | loss: 0.19726 | val_0_rmse: 0.42421 | val_1_rmse: 0.43244 |  0:02:37s
epoch 48 | loss: 0.19478 | val_0_rmse: 0.42101 | val_1_rmse: 0.43622 |  0:02:41s
epoch 49 | loss: 0.19315 | val_0_rmse: 0.42318 | val_1_rmse: 0.43781 |  0:02:44s
epoch 50 | loss: 0.18976 | val_0_rmse: 0.42736 | val_1_rmse: 0.44013 |  0:02:47s
epoch 51 | loss: 0.19034 | val_0_rmse: 0.41345 | val_1_rmse: 0.42827 |  0:02:50s
epoch 52 | loss: 0.18967 | val_0_rmse: 0.42097 | val_1_rmse: 0.43458 |  0:02:54s
epoch 53 | loss: 0.1891  | val_0_rmse: 0.42303 | val_1_rmse: 0.43659 |  0:02:57s
epoch 54 | loss: 0.19321 | val_0_rmse: 0.42911 | val_1_rmse: 0.44038 |  0:03:00s
epoch 55 | loss: 0.19244 | val_0_rmse: 0.41973 | val_1_rmse: 0.43481 |  0:03:03s
epoch 56 | loss: 0.18892 | val_0_rmse: 0.41536 | val_1_rmse: 0.42944 |  0:03:07s
epoch 57 | loss: 0.18703 | val_0_rmse: 0.42105 | val_1_rmse: 0.43687 |  0:03:10s
epoch 58 | loss: 0.18875 | val_0_rmse: 0.42251 | val_1_rmse: 0.43639 |  0:03:13s
epoch 59 | loss: 0.18571 | val_0_rmse: 0.42059 | val_1_rmse: 0.43916 |  0:03:16s
epoch 60 | loss: 0.18949 | val_0_rmse: 0.43041 | val_1_rmse: 0.44432 |  0:03:20s
epoch 61 | loss: 0.18649 | val_0_rmse: 0.41718 | val_1_rmse: 0.4315  |  0:03:23s
epoch 62 | loss: 0.19661 | val_0_rmse: 0.4355  | val_1_rmse: 0.4479  |  0:03:26s
epoch 63 | loss: 0.18936 | val_0_rmse: 0.40352 | val_1_rmse: 0.42146 |  0:03:29s
epoch 64 | loss: 0.18662 | val_0_rmse: 0.41809 | val_1_rmse: 0.43394 |  0:03:33s
epoch 65 | loss: 0.18748 | val_0_rmse: 0.42581 | val_1_rmse: 0.43895 |  0:03:36s
epoch 66 | loss: 0.18864 | val_0_rmse: 0.40646 | val_1_rmse: 0.42326 |  0:03:39s
epoch 67 | loss: 0.18635 | val_0_rmse: 0.41938 | val_1_rmse: 0.43563 |  0:03:42s
epoch 68 | loss: 0.18713 | val_0_rmse: 0.42285 | val_1_rmse: 0.43934 |  0:03:46s
epoch 69 | loss: 0.19031 | val_0_rmse: 0.40994 | val_1_rmse: 0.42592 |  0:03:49s
epoch 70 | loss: 0.18744 | val_0_rmse: 0.4096  | val_1_rmse: 0.4267  |  0:03:52s
epoch 71 | loss: 0.18581 | val_0_rmse: 0.42012 | val_1_rmse: 0.43489 |  0:03:56s
epoch 72 | loss: 0.19158 | val_0_rmse: 0.41048 | val_1_rmse: 0.42833 |  0:03:59s
epoch 73 | loss: 0.18737 | val_0_rmse: 0.41969 | val_1_rmse: 0.43486 |  0:04:02s
epoch 74 | loss: 0.18891 | val_0_rmse: 0.418   | val_1_rmse: 0.43286 |  0:04:05s
epoch 75 | loss: 0.1879  | val_0_rmse: 0.42835 | val_1_rmse: 0.44644 |  0:04:09s
epoch 76 | loss: 0.19019 | val_0_rmse: 0.41835 | val_1_rmse: 0.43336 |  0:04:12s
epoch 77 | loss: 0.18893 | val_0_rmse: 0.41097 | val_1_rmse: 0.42812 |  0:04:15s
epoch 78 | loss: 0.18735 | val_0_rmse: 0.41605 | val_1_rmse: 0.43211 |  0:04:18s
epoch 79 | loss: 0.18381 | val_0_rmse: 0.40703 | val_1_rmse: 0.42509 |  0:04:22s
epoch 80 | loss: 0.18448 | val_0_rmse: 0.41902 | val_1_rmse: 0.43321 |  0:04:25s
epoch 81 | loss: 0.18707 | val_0_rmse: 0.42393 | val_1_rmse: 0.43789 |  0:04:28s
epoch 82 | loss: 0.18924 | val_0_rmse: 0.41309 | val_1_rmse: 0.42875 |  0:04:31s
epoch 83 | loss: 0.19409 | val_0_rmse: 0.41729 | val_1_rmse: 0.43494 |  0:04:34s
epoch 84 | loss: 0.18867 | val_0_rmse: 0.41147 | val_1_rmse: 0.42758 |  0:04:38s
epoch 85 | loss: 0.18974 | val_0_rmse: 0.42222 | val_1_rmse: 0.44003 |  0:04:41s
epoch 86 | loss: 0.18468 | val_0_rmse: 0.40594 | val_1_rmse: 0.42409 |  0:04:44s
epoch 87 | loss: 0.18397 | val_0_rmse: 0.40279 | val_1_rmse: 0.42175 |  0:04:48s
epoch 88 | loss: 0.1893  | val_0_rmse: 0.41291 | val_1_rmse: 0.43033 |  0:04:51s
epoch 89 | loss: 0.18215 | val_0_rmse: 0.41344 | val_1_rmse: 0.43381 |  0:04:54s
epoch 90 | loss: 0.18305 | val_0_rmse: 0.41661 | val_1_rmse: 0.43548 |  0:04:57s
epoch 91 | loss: 0.18143 | val_0_rmse: 0.42218 | val_1_rmse: 0.44135 |  0:05:01s
epoch 92 | loss: 0.1856  | val_0_rmse: 0.41348 | val_1_rmse: 0.4297  |  0:05:04s
epoch 93 | loss: 0.18376 | val_0_rmse: 0.41528 | val_1_rmse: 0.43109 |  0:05:07s

Early stopping occured at epoch 93 with best_epoch = 63 and best_val_1_rmse = 0.42146
Best weights from best epoch are automatically used!
ended training at: 21:18:16
Feature importance:
[('Area', 0.10731932810392346), ('Baths', 0.23223670279990605), ('Beds', 0.08318839885001546), ('Latitude', 0.09267447773631728), ('Longitude', 0.11208699684309287), ('Month', 0.044420707299516825), ('Year', 0.3280733883672281)]
Mean squared error is of 9830353394.477901
Mean absolute error:68420.02660643756
MAPE:0.2860610809900255
R2 score:0.828562148734077
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: DC Properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:18:16
epoch 0  | loss: 0.43745 | val_0_rmse: 0.56038 | val_1_rmse: 0.56088 |  0:00:03s
epoch 1  | loss: 0.29505 | val_0_rmse: 0.50664 | val_1_rmse: 0.51249 |  0:00:06s
epoch 2  | loss: 0.26136 | val_0_rmse: 0.48914 | val_1_rmse: 0.49641 |  0:00:09s
epoch 3  | loss: 0.24789 | val_0_rmse: 0.48967 | val_1_rmse: 0.4924  |  0:00:13s
epoch 4  | loss: 0.23698 | val_0_rmse: 0.48122 | val_1_rmse: 0.48618 |  0:00:16s
epoch 5  | loss: 0.23614 | val_0_rmse: 0.47214 | val_1_rmse: 0.47417 |  0:00:19s
epoch 6  | loss: 0.23063 | val_0_rmse: 0.48371 | val_1_rmse: 0.4846  |  0:00:22s
epoch 7  | loss: 0.23037 | val_0_rmse: 0.45835 | val_1_rmse: 0.46328 |  0:00:25s
epoch 8  | loss: 0.22999 | val_0_rmse: 0.47284 | val_1_rmse: 0.4785  |  0:00:29s
epoch 9  | loss: 0.22489 | val_0_rmse: 0.46252 | val_1_rmse: 0.46608 |  0:00:32s
epoch 10 | loss: 0.21736 | val_0_rmse: 0.44818 | val_1_rmse: 0.45366 |  0:00:35s
epoch 11 | loss: 0.22352 | val_0_rmse: 0.44969 | val_1_rmse: 0.45584 |  0:00:39s
epoch 12 | loss: 0.21926 | val_0_rmse: 0.44125 | val_1_rmse: 0.44798 |  0:00:42s
epoch 13 | loss: 0.21301 | val_0_rmse: 0.44891 | val_1_rmse: 0.45631 |  0:00:45s
epoch 14 | loss: 0.21813 | val_0_rmse: 0.4386  | val_1_rmse: 0.44318 |  0:00:48s
epoch 15 | loss: 0.21403 | val_0_rmse: 0.45177 | val_1_rmse: 0.45674 |  0:00:51s
epoch 16 | loss: 0.2169  | val_0_rmse: 0.46635 | val_1_rmse: 0.47175 |  0:00:55s
epoch 17 | loss: 0.21273 | val_0_rmse: 0.44203 | val_1_rmse: 0.44683 |  0:00:58s
epoch 18 | loss: 0.21104 | val_0_rmse: 0.43449 | val_1_rmse: 0.44251 |  0:01:01s
epoch 19 | loss: 0.20639 | val_0_rmse: 0.44243 | val_1_rmse: 0.45092 |  0:01:05s
epoch 20 | loss: 0.20808 | val_0_rmse: 0.44227 | val_1_rmse: 0.44512 |  0:01:08s
epoch 21 | loss: 0.20443 | val_0_rmse: 0.44241 | val_1_rmse: 0.44753 |  0:01:11s
epoch 22 | loss: 0.20361 | val_0_rmse: 0.43789 | val_1_rmse: 0.44212 |  0:01:14s
epoch 23 | loss: 0.2067  | val_0_rmse: 0.42887 | val_1_rmse: 0.43453 |  0:01:17s
epoch 24 | loss: 0.21354 | val_0_rmse: 0.45825 | val_1_rmse: 0.46392 |  0:01:21s
epoch 25 | loss: 0.2018  | val_0_rmse: 0.43815 | val_1_rmse: 0.44748 |  0:01:24s
epoch 26 | loss: 0.20154 | val_0_rmse: 0.44392 | val_1_rmse: 0.45121 |  0:01:27s
epoch 27 | loss: 0.20222 | val_0_rmse: 0.42824 | val_1_rmse: 0.43353 |  0:01:31s
epoch 28 | loss: 0.19687 | val_0_rmse: 0.42712 | val_1_rmse: 0.43489 |  0:01:34s
epoch 29 | loss: 0.20008 | val_0_rmse: 0.43302 | val_1_rmse: 0.43937 |  0:01:37s
epoch 30 | loss: 0.20186 | val_0_rmse: 0.42683 | val_1_rmse: 0.43583 |  0:01:40s
epoch 31 | loss: 0.1994  | val_0_rmse: 0.44397 | val_1_rmse: 0.45135 |  0:01:44s
epoch 32 | loss: 0.19784 | val_0_rmse: 0.42841 | val_1_rmse: 0.43521 |  0:01:47s
epoch 33 | loss: 0.19873 | val_0_rmse: 0.41947 | val_1_rmse: 0.42865 |  0:01:50s
epoch 34 | loss: 0.1943  | val_0_rmse: 0.43756 | val_1_rmse: 0.44387 |  0:01:53s
epoch 35 | loss: 0.20053 | val_0_rmse: 0.44582 | val_1_rmse: 0.45624 |  0:01:57s
epoch 36 | loss: 0.19773 | val_0_rmse: 0.44403 | val_1_rmse: 0.45274 |  0:02:00s
epoch 37 | loss: 0.19434 | val_0_rmse: 0.41982 | val_1_rmse: 0.42981 |  0:02:03s
epoch 38 | loss: 0.19298 | val_0_rmse: 0.42723 | val_1_rmse: 0.43843 |  0:02:06s
epoch 39 | loss: 0.19421 | val_0_rmse: 0.42346 | val_1_rmse: 0.43093 |  0:02:10s
epoch 40 | loss: 0.19611 | val_0_rmse: 0.42468 | val_1_rmse: 0.43306 |  0:02:13s
epoch 41 | loss: 0.19556 | val_0_rmse: 0.42571 | val_1_rmse: 0.43384 |  0:02:16s
epoch 42 | loss: 0.19246 | val_0_rmse: 0.42382 | val_1_rmse: 0.43144 |  0:02:19s
epoch 43 | loss: 0.19274 | val_0_rmse: 0.41966 | val_1_rmse: 0.42817 |  0:02:23s
epoch 44 | loss: 0.19702 | val_0_rmse: 0.43198 | val_1_rmse: 0.44299 |  0:02:26s
epoch 45 | loss: 0.19163 | val_0_rmse: 0.41223 | val_1_rmse: 0.42231 |  0:02:29s
epoch 46 | loss: 0.19324 | val_0_rmse: 0.45723 | val_1_rmse: 0.46104 |  0:02:32s
epoch 47 | loss: 0.20058 | val_0_rmse: 0.4245  | val_1_rmse: 0.4344  |  0:02:35s
epoch 48 | loss: 0.20174 | val_0_rmse: 0.42519 | val_1_rmse: 0.43613 |  0:02:39s
epoch 49 | loss: 0.19311 | val_0_rmse: 0.42177 | val_1_rmse: 0.4332  |  0:02:42s
epoch 50 | loss: 0.19456 | val_0_rmse: 0.44881 | val_1_rmse: 0.45637 |  0:02:45s
epoch 51 | loss: 0.1914  | val_0_rmse: 0.42103 | val_1_rmse: 0.4309  |  0:02:48s
epoch 52 | loss: 0.19169 | val_0_rmse: 0.42778 | val_1_rmse: 0.43675 |  0:02:52s
epoch 53 | loss: 0.19354 | val_0_rmse: 0.48024 | val_1_rmse: 0.48571 |  0:02:55s
epoch 54 | loss: 0.19618 | val_0_rmse: 0.4243  | val_1_rmse: 0.43266 |  0:02:58s
epoch 55 | loss: 0.18671 | val_0_rmse: 0.41291 | val_1_rmse: 0.42323 |  0:03:01s
epoch 56 | loss: 0.18807 | val_0_rmse: 0.42261 | val_1_rmse: 0.43321 |  0:03:05s
epoch 57 | loss: 0.19245 | val_0_rmse: 0.42058 | val_1_rmse: 0.43239 |  0:03:08s
epoch 58 | loss: 0.19129 | val_0_rmse: 0.42179 | val_1_rmse: 0.43061 |  0:03:11s
epoch 59 | loss: 0.19056 | val_0_rmse: 0.43144 | val_1_rmse: 0.44535 |  0:03:14s
epoch 60 | loss: 0.19012 | val_0_rmse: 0.42044 | val_1_rmse: 0.43358 |  0:03:18s
epoch 61 | loss: 0.18701 | val_0_rmse: 0.41396 | val_1_rmse: 0.42551 |  0:03:21s
epoch 62 | loss: 0.18775 | val_0_rmse: 0.41494 | val_1_rmse: 0.42384 |  0:03:24s
epoch 63 | loss: 0.18836 | val_0_rmse: 0.44461 | val_1_rmse: 0.45612 |  0:03:27s
epoch 64 | loss: 0.19216 | val_0_rmse: 0.41827 | val_1_rmse: 0.42943 |  0:03:30s
epoch 65 | loss: 0.18766 | val_0_rmse: 0.41913 | val_1_rmse: 0.42842 |  0:03:34s
epoch 66 | loss: 0.18921 | val_0_rmse: 0.42268 | val_1_rmse: 0.43187 |  0:03:37s
epoch 67 | loss: 0.18944 | val_0_rmse: 0.4119  | val_1_rmse: 0.42336 |  0:03:40s
epoch 68 | loss: 0.19031 | val_0_rmse: 0.42051 | val_1_rmse: 0.43172 |  0:03:44s
epoch 69 | loss: 0.18723 | val_0_rmse: 0.41308 | val_1_rmse: 0.42269 |  0:03:47s
epoch 70 | loss: 0.18756 | val_0_rmse: 0.42748 | val_1_rmse: 0.43669 |  0:03:50s
epoch 71 | loss: 0.19025 | val_0_rmse: 0.40919 | val_1_rmse: 0.41927 |  0:03:53s
epoch 72 | loss: 0.18749 | val_0_rmse: 0.42191 | val_1_rmse: 0.43298 |  0:03:57s
epoch 73 | loss: 0.18249 | val_0_rmse: 0.41691 | val_1_rmse: 0.42964 |  0:04:00s
epoch 74 | loss: 0.18412 | val_0_rmse: 0.40995 | val_1_rmse: 0.42134 |  0:04:03s
epoch 75 | loss: 0.18636 | val_0_rmse: 0.41191 | val_1_rmse: 0.41962 |  0:04:06s
epoch 76 | loss: 0.19177 | val_0_rmse: 0.41067 | val_1_rmse: 0.42191 |  0:04:10s
epoch 77 | loss: 0.18258 | val_0_rmse: 0.40883 | val_1_rmse: 0.42095 |  0:04:13s
epoch 78 | loss: 0.1834  | val_0_rmse: 0.41583 | val_1_rmse: 0.427   |  0:04:16s
epoch 79 | loss: 0.18998 | val_0_rmse: 0.42665 | val_1_rmse: 0.43535 |  0:04:19s
epoch 80 | loss: 0.20776 | val_0_rmse: 0.4663  | val_1_rmse: 0.47378 |  0:04:22s
epoch 81 | loss: 0.21261 | val_0_rmse: 0.44127 | val_1_rmse: 0.44931 |  0:04:26s
epoch 82 | loss: 0.20338 | val_0_rmse: 0.43525 | val_1_rmse: 0.44491 |  0:04:29s
epoch 83 | loss: 0.19391 | val_0_rmse: 0.43194 | val_1_rmse: 0.43702 |  0:04:32s
epoch 84 | loss: 0.19455 | val_0_rmse: 0.43952 | val_1_rmse: 0.44488 |  0:04:35s
epoch 85 | loss: 0.19159 | val_0_rmse: 0.4226  | val_1_rmse: 0.4319  |  0:04:39s
epoch 86 | loss: 0.18686 | val_0_rmse: 0.417   | val_1_rmse: 0.42845 |  0:04:42s
epoch 87 | loss: 0.18805 | val_0_rmse: 0.41747 | val_1_rmse: 0.42671 |  0:04:45s
epoch 88 | loss: 0.19202 | val_0_rmse: 0.43426 | val_1_rmse: 0.44431 |  0:04:48s
epoch 89 | loss: 0.18977 | val_0_rmse: 0.41714 | val_1_rmse: 0.42778 |  0:04:52s
epoch 90 | loss: 0.2052  | val_0_rmse: 0.52299 | val_1_rmse: 0.5268  |  0:04:55s
epoch 91 | loss: 0.20575 | val_0_rmse: 0.44579 | val_1_rmse: 0.45253 |  0:04:58s
epoch 92 | loss: 0.19756 | val_0_rmse: 0.4228  | val_1_rmse: 0.43213 |  0:05:01s
epoch 93 | loss: 0.19044 | val_0_rmse: 0.42603 | val_1_rmse: 0.4327  |  0:05:05s
epoch 94 | loss: 0.1905  | val_0_rmse: 0.43225 | val_1_rmse: 0.43853 |  0:05:08s
epoch 95 | loss: 0.18621 | val_0_rmse: 0.41873 | val_1_rmse: 0.42903 |  0:05:11s
epoch 96 | loss: 0.19638 | val_0_rmse: 0.43308 | val_1_rmse: 0.4401  |  0:05:14s
epoch 97 | loss: 0.19245 | val_0_rmse: 0.43193 | val_1_rmse: 0.44132 |  0:05:18s
epoch 98 | loss: 0.18829 | val_0_rmse: 0.42074 | val_1_rmse: 0.42848 |  0:05:21s
epoch 99 | loss: 0.19101 | val_0_rmse: 0.41673 | val_1_rmse: 0.42755 |  0:05:24s
epoch 100| loss: 0.19239 | val_0_rmse: 0.42821 | val_1_rmse: 0.43586 |  0:05:27s
epoch 101| loss: 0.19219 | val_0_rmse: 0.42812 | val_1_rmse: 0.43669 |  0:05:31s

Early stopping occured at epoch 101 with best_epoch = 71 and best_val_1_rmse = 0.41927
Best weights from best epoch are automatically used!
ended training at: 21:23:48
Feature importance:
[('Area', 0.0), ('Baths', 0.12014355100571114), ('Beds', 0.17402582071024847), ('Latitude', 0.23591605219416215), ('Longitude', 0.22377125784878707), ('Month', 0.0), ('Year', 0.24614331824109117)]
Mean squared error is of 9682772348.644068
Mean absolute error:69321.75590104585
MAPE:0.29558969711834593
R2 score:0.8308052483131825
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:25:54
epoch 0  | loss: 0.74002 | val_0_rmse: 0.7981  | val_1_rmse: 0.80167 |  0:00:02s
epoch 1  | loss: 0.60721 | val_0_rmse: 0.77314 | val_1_rmse: 0.77505 |  0:00:04s
epoch 2  | loss: 0.57323 | val_0_rmse: 0.75315 | val_1_rmse: 0.75301 |  0:00:06s
epoch 3  | loss: 0.54245 | val_0_rmse: 0.74171 | val_1_rmse: 0.7373  |  0:00:08s
epoch 4  | loss: 0.53134 | val_0_rmse: 0.78539 | val_1_rmse: 0.77986 |  0:00:10s
epoch 5  | loss: 0.52602 | val_0_rmse: 0.7406  | val_1_rmse: 0.73883 |  0:00:12s
epoch 6  | loss: 0.52719 | val_0_rmse: 0.7442  | val_1_rmse: 0.74064 |  0:00:14s
epoch 7  | loss: 0.52659 | val_0_rmse: 0.7614  | val_1_rmse: 0.75869 |  0:00:16s
epoch 8  | loss: 0.52125 | val_0_rmse: 0.74953 | val_1_rmse: 0.74211 |  0:00:18s
epoch 9  | loss: 0.51864 | val_0_rmse: 0.73508 | val_1_rmse: 0.73149 |  0:00:20s
epoch 10 | loss: 0.52109 | val_0_rmse: 0.73064 | val_1_rmse: 0.72708 |  0:00:22s
epoch 11 | loss: 0.52284 | val_0_rmse: 0.73864 | val_1_rmse: 0.73519 |  0:00:24s
epoch 12 | loss: 0.51706 | val_0_rmse: 0.7372  | val_1_rmse: 0.73527 |  0:00:26s
epoch 13 | loss: 0.51769 | val_0_rmse: 0.74746 | val_1_rmse: 0.74513 |  0:00:28s
epoch 14 | loss: 0.51735 | val_0_rmse: 0.80152 | val_1_rmse: 0.74704 |  0:00:30s
epoch 15 | loss: 0.51376 | val_0_rmse: 0.74321 | val_1_rmse: 0.73393 |  0:00:32s
epoch 16 | loss: 0.51364 | val_0_rmse: 0.73963 | val_1_rmse: 0.73531 |  0:00:34s
epoch 17 | loss: 0.51428 | val_0_rmse: 0.73063 | val_1_rmse: 0.72744 |  0:00:36s
epoch 18 | loss: 0.51256 | val_0_rmse: 0.74035 | val_1_rmse: 0.73654 |  0:00:38s
epoch 19 | loss: 0.51035 | val_0_rmse: 0.77327 | val_1_rmse: 0.7761  |  0:00:40s
epoch 20 | loss: 0.51155 | val_0_rmse: 0.77676 | val_1_rmse: 0.77167 |  0:00:42s
epoch 21 | loss: 0.512   | val_0_rmse: 0.75026 | val_1_rmse: 0.74615 |  0:00:44s
epoch 22 | loss: 0.51229 | val_0_rmse: 0.74718 | val_1_rmse: 0.73663 |  0:00:46s
epoch 23 | loss: 0.5181  | val_0_rmse: 0.75434 | val_1_rmse: 0.7537  |  0:00:48s
epoch 24 | loss: 0.51169 | val_0_rmse: 0.74423 | val_1_rmse: 0.73773 |  0:00:50s
epoch 25 | loss: 0.52014 | val_0_rmse: 0.76752 | val_1_rmse: 0.76042 |  0:00:52s
epoch 26 | loss: 0.51551 | val_0_rmse: 0.73826 | val_1_rmse: 0.73204 |  0:00:54s
epoch 27 | loss: 0.51141 | val_0_rmse: 0.74282 | val_1_rmse: 0.73766 |  0:00:56s
epoch 28 | loss: 0.51062 | val_0_rmse: 0.74601 | val_1_rmse: 0.74204 |  0:00:58s
epoch 29 | loss: 0.50852 | val_0_rmse: 0.75291 | val_1_rmse: 0.74841 |  0:01:00s
epoch 30 | loss: 0.50837 | val_0_rmse: 0.75517 | val_1_rmse: 0.7477  |  0:01:02s
epoch 31 | loss: 0.50889 | val_0_rmse: 0.74656 | val_1_rmse: 0.74074 |  0:01:04s
epoch 32 | loss: 0.50948 | val_0_rmse: 0.74322 | val_1_rmse: 0.73921 |  0:01:06s
epoch 33 | loss: 0.50829 | val_0_rmse: 0.75287 | val_1_rmse: 0.74752 |  0:01:08s
epoch 34 | loss: 0.50763 | val_0_rmse: 0.79507 | val_1_rmse: 0.78875 |  0:01:10s
epoch 35 | loss: 0.51252 | val_0_rmse: 0.77851 | val_1_rmse: 0.7715  |  0:01:12s
epoch 36 | loss: 0.51266 | val_0_rmse: 0.76773 | val_1_rmse: 0.76291 |  0:01:14s
epoch 37 | loss: 0.51268 | val_0_rmse: 0.83054 | val_1_rmse: 0.82501 |  0:01:16s
epoch 38 | loss: 0.51356 | val_0_rmse: 0.79208 | val_1_rmse: 0.78551 |  0:01:18s
epoch 39 | loss: 0.51058 | val_0_rmse: 0.7539  | val_1_rmse: 0.75182 |  0:01:20s
epoch 40 | loss: 0.50994 | val_0_rmse: 0.7544  | val_1_rmse: 0.75066 |  0:01:23s

Early stopping occured at epoch 40 with best_epoch = 10 and best_val_1_rmse = 0.72708
Best weights from best epoch are automatically used!
ended training at: 21:27:17
Feature importance:
[('Area', 0.48414265059082107), ('Baths', 0.00023404594464192417), ('Beds', 0.0933941702420543), ('Latitude', 0.38885248587228793), ('Longitude', 0.02230887271292091), ('Month', 0.0015835132045827031), ('Year', 0.009484261432691148)]
Mean squared error is of 16387978457.445496
Mean absolute error:99861.27397152342
MAPE:0.28389849197579514
R2 score:0.4585073355252949
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:27:18
epoch 0  | loss: 0.74954 | val_0_rmse: 0.80429 | val_1_rmse: 0.79416 |  0:00:01s
epoch 1  | loss: 0.61249 | val_0_rmse: 0.78017 | val_1_rmse: 0.77394 |  0:00:03s
epoch 2  | loss: 0.59453 | val_0_rmse: 0.77945 | val_1_rmse: 0.77193 |  0:00:06s
epoch 3  | loss: 0.56954 | val_0_rmse: 0.77967 | val_1_rmse: 0.78672 |  0:00:08s
epoch 4  | loss: 0.55087 | val_0_rmse: 0.76243 | val_1_rmse: 0.76071 |  0:00:10s
epoch 5  | loss: 0.54522 | val_0_rmse: 0.77613 | val_1_rmse: 0.77354 |  0:00:12s
epoch 6  | loss: 0.53593 | val_0_rmse: 0.74038 | val_1_rmse: 0.72992 |  0:00:14s
epoch 7  | loss: 0.52833 | val_0_rmse: 0.74054 | val_1_rmse: 0.73618 |  0:00:16s
epoch 8  | loss: 0.52415 | val_0_rmse: 0.72998 | val_1_rmse: 0.72248 |  0:00:18s
epoch 9  | loss: 0.52498 | val_0_rmse: 0.73677 | val_1_rmse: 0.73063 |  0:00:20s
epoch 10 | loss: 0.52971 | val_0_rmse: 0.76191 | val_1_rmse: 0.76126 |  0:00:22s
epoch 11 | loss: 0.52998 | val_0_rmse: 0.73597 | val_1_rmse: 0.72767 |  0:00:24s
epoch 12 | loss: 0.51579 | val_0_rmse: 0.73066 | val_1_rmse: 0.7215  |  0:00:26s
epoch 13 | loss: 0.51339 | val_0_rmse: 0.75529 | val_1_rmse: 0.74191 |  0:00:28s
epoch 14 | loss: 0.52007 | val_0_rmse: 0.7215  | val_1_rmse: 0.71535 |  0:00:30s
epoch 15 | loss: 0.5151  | val_0_rmse: 0.72309 | val_1_rmse: 0.71493 |  0:00:32s
epoch 16 | loss: 0.5105  | val_0_rmse: 0.75855 | val_1_rmse: 0.75396 |  0:00:34s
epoch 17 | loss: 0.50493 | val_0_rmse: 0.76295 | val_1_rmse: 0.75212 |  0:00:36s
epoch 18 | loss: 0.51634 | val_0_rmse: 0.74317 | val_1_rmse: 0.73079 |  0:00:38s
epoch 19 | loss: 0.50746 | val_0_rmse: 0.74947 | val_1_rmse: 0.74208 |  0:00:40s
epoch 20 | loss: 0.50835 | val_0_rmse: 0.74446 | val_1_rmse: 0.73507 |  0:00:42s
epoch 21 | loss: 0.50691 | val_0_rmse: 0.73915 | val_1_rmse: 0.73345 |  0:00:44s
epoch 22 | loss: 0.49776 | val_0_rmse: 0.723   | val_1_rmse: 0.71567 |  0:00:46s
epoch 23 | loss: 0.49398 | val_0_rmse: 0.75231 | val_1_rmse: 0.74829 |  0:00:48s
epoch 24 | loss: 0.49933 | val_0_rmse: 0.85838 | val_1_rmse: 0.8369  |  0:00:50s
epoch 25 | loss: 0.49526 | val_0_rmse: 0.75464 | val_1_rmse: 0.74406 |  0:00:52s
epoch 26 | loss: 0.49254 | val_0_rmse: 0.73022 | val_1_rmse: 0.72363 |  0:00:54s
epoch 27 | loss: 0.49076 | val_0_rmse: 0.74532 | val_1_rmse: 0.7412  |  0:00:56s
epoch 28 | loss: 0.49175 | val_0_rmse: 0.77407 | val_1_rmse: 0.76734 |  0:00:58s
epoch 29 | loss: 0.48965 | val_0_rmse: 0.75233 | val_1_rmse: 0.74512 |  0:01:00s
epoch 30 | loss: 0.49406 | val_0_rmse: 0.72051 | val_1_rmse: 0.70996 |  0:01:02s
epoch 31 | loss: 0.49019 | val_0_rmse: 0.74064 | val_1_rmse: 0.73082 |  0:01:04s
epoch 32 | loss: 0.49892 | val_0_rmse: 0.76029 | val_1_rmse: 0.75704 |  0:01:06s
epoch 33 | loss: 0.49308 | val_0_rmse: 0.75631 | val_1_rmse: 0.74355 |  0:01:08s
epoch 34 | loss: 0.49326 | val_0_rmse: 0.74225 | val_1_rmse: 0.73097 |  0:01:10s
epoch 35 | loss: 0.49141 | val_0_rmse: 0.72878 | val_1_rmse: 0.72069 |  0:01:12s
epoch 36 | loss: 0.48714 | val_0_rmse: 0.76544 | val_1_rmse: 0.76036 |  0:01:14s
epoch 37 | loss: 0.48899 | val_0_rmse: 0.76149 | val_1_rmse: 0.74319 |  0:01:16s
epoch 38 | loss: 0.48916 | val_0_rmse: 0.74868 | val_1_rmse: 0.74068 |  0:01:18s
epoch 39 | loss: 0.48195 | val_0_rmse: 0.75225 | val_1_rmse: 0.74741 |  0:01:20s
epoch 40 | loss: 0.48291 | val_0_rmse: 0.74122 | val_1_rmse: 0.73268 |  0:01:22s
epoch 41 | loss: 0.48916 | val_0_rmse: 0.76275 | val_1_rmse: 0.74995 |  0:01:24s
epoch 42 | loss: 0.48703 | val_0_rmse: 0.73    | val_1_rmse: 0.71648 |  0:01:26s
epoch 43 | loss: 0.48675 | val_0_rmse: 0.72904 | val_1_rmse: 0.72105 |  0:01:28s
epoch 44 | loss: 0.48509 | val_0_rmse: 0.75293 | val_1_rmse: 0.74413 |  0:01:30s
epoch 45 | loss: 0.4846  | val_0_rmse: 0.73766 | val_1_rmse: 0.72839 |  0:01:32s
epoch 46 | loss: 0.48052 | val_0_rmse: 0.78877 | val_1_rmse: 0.7852  |  0:01:34s
epoch 47 | loss: 0.48869 | val_0_rmse: 0.79136 | val_1_rmse: 0.78764 |  0:01:36s
epoch 48 | loss: 0.48753 | val_0_rmse: 0.73617 | val_1_rmse: 0.73035 |  0:01:38s
epoch 49 | loss: 0.48496 | val_0_rmse: 0.78134 | val_1_rmse: 0.7775  |  0:01:40s
epoch 50 | loss: 0.48647 | val_0_rmse: 0.75456 | val_1_rmse: 0.74974 |  0:01:42s
epoch 51 | loss: 0.47985 | val_0_rmse: 0.75549 | val_1_rmse: 0.74957 |  0:01:44s
epoch 52 | loss: 0.48099 | val_0_rmse: 0.79846 | val_1_rmse: 0.79222 |  0:01:46s
epoch 53 | loss: 0.4805  | val_0_rmse: 0.77487 | val_1_rmse: 0.76816 |  0:01:48s
epoch 54 | loss: 0.48149 | val_0_rmse: 0.75027 | val_1_rmse: 0.74108 |  0:01:50s
epoch 55 | loss: 0.4796  | val_0_rmse: 0.73862 | val_1_rmse: 0.73095 |  0:01:52s
epoch 56 | loss: 0.48436 | val_0_rmse: 0.77117 | val_1_rmse: 0.76151 |  0:01:54s
epoch 57 | loss: 0.48546 | val_0_rmse: 0.78052 | val_1_rmse: 0.77426 |  0:01:56s
epoch 58 | loss: 0.48196 | val_0_rmse: 0.74918 | val_1_rmse: 0.74146 |  0:01:58s
epoch 59 | loss: 0.48034 | val_0_rmse: 0.74314 | val_1_rmse: 0.73171 |  0:02:00s
epoch 60 | loss: 0.47675 | val_0_rmse: 0.74684 | val_1_rmse: 0.73209 |  0:02:02s

Early stopping occured at epoch 60 with best_epoch = 30 and best_val_1_rmse = 0.70996
Best weights from best epoch are automatically used!
ended training at: 21:29:22
Feature importance:
[('Area', 0.4401842206746916), ('Baths', 0.09720372334926128), ('Beds', 0.12100713095266821), ('Latitude', 0.11510792104221397), ('Longitude', 0.2247953375008545), ('Month', 0.0), ('Year', 0.0017016664803104636)]
Mean squared error is of 15620373398.86882
Mean absolute error:95980.38594372994
MAPE:0.26861923756856454
R2 score:0.48591107468203554
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:29:22
epoch 0  | loss: 0.73698 | val_0_rmse: 0.82439 | val_1_rmse: 0.8291  |  0:00:01s
epoch 1  | loss: 0.63594 | val_0_rmse: 0.79019 | val_1_rmse: 0.78886 |  0:00:03s
epoch 2  | loss: 0.61189 | val_0_rmse: 0.78054 | val_1_rmse: 0.78129 |  0:00:06s
epoch 3  | loss: 0.59108 | val_0_rmse: 0.78935 | val_1_rmse: 0.7818  |  0:00:08s
epoch 4  | loss: 0.56711 | val_0_rmse: 0.75997 | val_1_rmse: 0.75903 |  0:00:10s
epoch 5  | loss: 0.53884 | val_0_rmse: 0.77609 | val_1_rmse: 0.77239 |  0:00:12s
epoch 6  | loss: 0.52767 | val_0_rmse: 0.74253 | val_1_rmse: 0.74128 |  0:00:14s
epoch 7  | loss: 0.52904 | val_0_rmse: 0.75175 | val_1_rmse: 0.74874 |  0:00:16s
epoch 8  | loss: 0.52547 | val_0_rmse: 0.77568 | val_1_rmse: 0.77541 |  0:00:18s
epoch 9  | loss: 0.523   | val_0_rmse: 0.77549 | val_1_rmse: 0.77481 |  0:00:20s
epoch 10 | loss: 0.52315 | val_0_rmse: 0.76211 | val_1_rmse: 0.7613  |  0:00:22s
epoch 11 | loss: 0.52104 | val_0_rmse: 0.75473 | val_1_rmse: 0.75327 |  0:00:24s
epoch 12 | loss: 0.51886 | val_0_rmse: 0.76035 | val_1_rmse: 0.75888 |  0:00:26s
epoch 13 | loss: 0.51531 | val_0_rmse: 0.79531 | val_1_rmse: 0.79169 |  0:00:28s
epoch 14 | loss: 0.51831 | val_0_rmse: 0.77931 | val_1_rmse: 0.77674 |  0:00:30s
epoch 15 | loss: 0.5256  | val_0_rmse: 0.76254 | val_1_rmse: 0.76119 |  0:00:32s
epoch 16 | loss: 0.52189 | val_0_rmse: 0.76848 | val_1_rmse: 0.7653  |  0:00:34s
epoch 17 | loss: 0.51674 | val_0_rmse: 0.7798  | val_1_rmse: 0.77748 |  0:00:36s
epoch 18 | loss: 0.51499 | val_0_rmse: 0.78211 | val_1_rmse: 0.77906 |  0:00:38s
epoch 19 | loss: 0.51382 | val_0_rmse: 0.76004 | val_1_rmse: 0.75895 |  0:00:40s
epoch 20 | loss: 0.5143  | val_0_rmse: 0.75808 | val_1_rmse: 0.75701 |  0:00:42s
epoch 21 | loss: 0.51506 | val_0_rmse: 0.80991 | val_1_rmse: 0.80887 |  0:00:44s
epoch 22 | loss: 0.52354 | val_0_rmse: 0.7497  | val_1_rmse: 0.74917 |  0:00:46s
epoch 23 | loss: 0.51465 | val_0_rmse: 0.77889 | val_1_rmse: 0.77638 |  0:00:48s
epoch 24 | loss: 0.51664 | val_0_rmse: 0.7684  | val_1_rmse: 0.76783 |  0:00:50s
epoch 25 | loss: 0.51345 | val_0_rmse: 0.77064 | val_1_rmse: 0.76981 |  0:00:52s
epoch 26 | loss: 0.51266 | val_0_rmse: 0.74665 | val_1_rmse: 0.74753 |  0:00:54s
epoch 27 | loss: 0.51265 | val_0_rmse: 0.79465 | val_1_rmse: 0.79283 |  0:00:56s
epoch 28 | loss: 0.50987 | val_0_rmse: 0.79616 | val_1_rmse: 0.79433 |  0:00:58s
epoch 29 | loss: 0.50928 | val_0_rmse: 0.75959 | val_1_rmse: 0.75842 |  0:01:00s
epoch 30 | loss: 0.51173 | val_0_rmse: 0.75419 | val_1_rmse: 0.75387 |  0:01:02s
epoch 31 | loss: 0.50898 | val_0_rmse: 0.7735  | val_1_rmse: 0.77227 |  0:01:04s
epoch 32 | loss: 0.5106  | val_0_rmse: 0.75005 | val_1_rmse: 0.74934 |  0:01:07s
epoch 33 | loss: 0.50861 | val_0_rmse: 0.74136 | val_1_rmse: 0.74363 |  0:01:09s
epoch 34 | loss: 0.51586 | val_0_rmse: 0.74944 | val_1_rmse: 0.74868 |  0:01:11s
epoch 35 | loss: 0.51187 | val_0_rmse: 0.74668 | val_1_rmse: 0.74795 |  0:01:13s
epoch 36 | loss: 0.50874 | val_0_rmse: 0.75609 | val_1_rmse: 0.7567  |  0:01:15s

Early stopping occured at epoch 36 with best_epoch = 6 and best_val_1_rmse = 0.74128
Best weights from best epoch are automatically used!
ended training at: 21:30:38
Feature importance:
[('Area', 0.393814772233742), ('Baths', 0.023352976225030835), ('Beds', 0.1552040676461288), ('Latitude', 0.35493815322619204), ('Longitude', 0.0009047670515901064), ('Month', 0.03185385800440777), ('Year', 0.039931405612908484)]
Mean squared error is of 16671556760.394896
Mean absolute error:101373.62273352293
MAPE:0.2886245234017015
R2 score:0.45121488068728155
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:30:39
epoch 0  | loss: 0.76261 | val_0_rmse: 0.80501 | val_1_rmse: 0.86122 |  0:00:01s
epoch 1  | loss: 0.6022  | val_0_rmse: 0.78775 | val_1_rmse: 0.78284 |  0:00:04s
epoch 2  | loss: 0.56072 | val_0_rmse: 0.76915 | val_1_rmse: 0.76367 |  0:00:05s
epoch 3  | loss: 0.54454 | val_0_rmse: 0.74156 | val_1_rmse: 0.76655 |  0:00:08s
epoch 4  | loss: 0.54461 | val_0_rmse: 0.75168 | val_1_rmse: 0.75108 |  0:00:10s
epoch 5  | loss: 0.53953 | val_0_rmse: 0.73616 | val_1_rmse: 0.74818 |  0:00:12s
epoch 6  | loss: 0.5514  | val_0_rmse: 0.7917  | val_1_rmse: 0.7975  |  0:00:14s
epoch 7  | loss: 0.5335  | val_0_rmse: 0.76657 | val_1_rmse: 0.76197 |  0:00:16s
epoch 8  | loss: 0.52965 | val_0_rmse: 0.73919 | val_1_rmse: 0.73869 |  0:00:18s
epoch 9  | loss: 0.52734 | val_0_rmse: 0.76421 | val_1_rmse: 0.7633  |  0:00:20s
epoch 10 | loss: 0.52461 | val_0_rmse: 0.7451  | val_1_rmse: 0.74328 |  0:00:22s
epoch 11 | loss: 0.51915 | val_0_rmse: 0.74692 | val_1_rmse: 0.74696 |  0:00:24s
epoch 12 | loss: 0.52218 | val_0_rmse: 0.73389 | val_1_rmse: 0.73314 |  0:00:26s
epoch 13 | loss: 0.51293 | val_0_rmse: 0.73819 | val_1_rmse: 0.74448 |  0:00:28s
epoch 14 | loss: 0.51649 | val_0_rmse: 0.73551 | val_1_rmse: 0.7383  |  0:00:30s
epoch 15 | loss: 0.52113 | val_0_rmse: 0.74029 | val_1_rmse: 0.73712 |  0:00:32s
epoch 16 | loss: 0.51398 | val_0_rmse: 0.75128 | val_1_rmse: 0.75316 |  0:00:35s
epoch 17 | loss: 0.51701 | val_0_rmse: 0.74237 | val_1_rmse: 0.74391 |  0:00:37s
epoch 18 | loss: 0.51973 | val_0_rmse: 0.76156 | val_1_rmse: 0.75847 |  0:00:39s
epoch 19 | loss: 0.51522 | val_0_rmse: 0.74605 | val_1_rmse: 0.74414 |  0:00:41s
epoch 20 | loss: 0.51429 | val_0_rmse: 0.74049 | val_1_rmse: 0.78191 |  0:00:43s
epoch 21 | loss: 0.52467 | val_0_rmse: 0.75232 | val_1_rmse: 0.77779 |  0:00:46s
epoch 22 | loss: 0.52627 | val_0_rmse: 0.72646 | val_1_rmse: 0.73165 |  0:00:48s
epoch 23 | loss: 0.51523 | val_0_rmse: 0.75552 | val_1_rmse: 0.7732  |  0:00:50s
epoch 24 | loss: 0.51871 | val_0_rmse: 0.75944 | val_1_rmse: 0.76213 |  0:00:53s
epoch 25 | loss: 0.5177  | val_0_rmse: 0.74514 | val_1_rmse: 0.87434 |  0:00:55s
epoch 26 | loss: 0.52114 | val_0_rmse: 0.75714 | val_1_rmse: 0.82414 |  0:00:57s
epoch 27 | loss: 0.51699 | val_0_rmse: 0.74962 | val_1_rmse: 0.94872 |  0:00:59s
epoch 28 | loss: 0.51985 | val_0_rmse: 0.75401 | val_1_rmse: 0.75177 |  0:01:02s
epoch 29 | loss: 0.51279 | val_0_rmse: 0.76335 | val_1_rmse: 0.76631 |  0:01:04s
epoch 30 | loss: 0.51177 | val_0_rmse: 0.77372 | val_1_rmse: 0.77406 |  0:01:06s
epoch 31 | loss: 0.52406 | val_0_rmse: 0.73875 | val_1_rmse: 0.73388 |  0:01:09s
epoch 32 | loss: 0.51793 | val_0_rmse: 0.79619 | val_1_rmse: 0.81864 |  0:01:11s
epoch 33 | loss: 0.5123  | val_0_rmse: 0.75261 | val_1_rmse: 0.75249 |  0:01:13s
epoch 34 | loss: 0.5202  | val_0_rmse: 0.78177 | val_1_rmse: 0.78324 |  0:01:15s
epoch 35 | loss: 0.53098 | val_0_rmse: 0.7581  | val_1_rmse: 0.7545  |  0:01:17s
epoch 36 | loss: 0.52512 | val_0_rmse: 0.75626 | val_1_rmse: 0.75626 |  0:01:20s
epoch 37 | loss: 0.52213 | val_0_rmse: 0.76358 | val_1_rmse: 0.76248 |  0:01:22s
epoch 38 | loss: 0.51224 | val_0_rmse: 0.76572 | val_1_rmse: 0.76411 |  0:01:25s
epoch 39 | loss: 0.51292 | val_0_rmse: 0.74106 | val_1_rmse: 0.73884 |  0:01:27s
epoch 40 | loss: 0.51619 | val_0_rmse: 0.76258 | val_1_rmse: 0.76277 |  0:01:29s
epoch 41 | loss: 0.51116 | val_0_rmse: 0.74714 | val_1_rmse: 0.74453 |  0:01:32s
epoch 42 | loss: 0.51217 | val_0_rmse: 0.74484 | val_1_rmse: 0.74634 |  0:01:34s
epoch 43 | loss: 0.5111  | val_0_rmse: 0.77823 | val_1_rmse: 0.78653 |  0:01:37s
epoch 44 | loss: 0.51858 | val_0_rmse: 0.74661 | val_1_rmse: 0.74915 |  0:01:39s
epoch 45 | loss: 0.51971 | val_0_rmse: 0.74841 | val_1_rmse: 0.7647  |  0:01:41s
epoch 46 | loss: 0.52425 | val_0_rmse: 0.76762 | val_1_rmse: 0.76345 |  0:01:44s
epoch 47 | loss: 0.51924 | val_0_rmse: 0.79285 | val_1_rmse: 0.80084 |  0:01:47s
epoch 48 | loss: 0.51975 | val_0_rmse: 0.77898 | val_1_rmse: 0.78113 |  0:01:49s
epoch 49 | loss: 0.51633 | val_0_rmse: 0.76352 | val_1_rmse: 0.76626 |  0:01:52s
epoch 50 | loss: 0.51474 | val_0_rmse: 0.74922 | val_1_rmse: 0.74701 |  0:01:54s
epoch 51 | loss: 0.50757 | val_0_rmse: 0.76902 | val_1_rmse: 0.76792 |  0:01:58s
epoch 52 | loss: 0.51121 | val_0_rmse: 0.75298 | val_1_rmse: 0.75062 |  0:02:01s

Early stopping occured at epoch 52 with best_epoch = 22 and best_val_1_rmse = 0.73165
Best weights from best epoch are automatically used!
ended training at: 21:32:41
Feature importance:
[('Area', 0.31095688504541524), ('Baths', 0.0052946542461963005), ('Beds', 0.14183721725736714), ('Latitude', 0.4715023763221977), ('Longitude', 0.0), ('Month', 0.02377283781196324), ('Year', 0.046636029316860415)]
Mean squared error is of 16081628783.398663
Mean absolute error:99411.55575389093
MAPE:0.28672237864004363
R2 score:0.4717954918981253
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: kc house data.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:32:42
epoch 0  | loss: 0.76246 | val_0_rmse: 0.81737 | val_1_rmse: 0.8075  |  0:00:02s
epoch 1  | loss: 0.63172 | val_0_rmse: 0.78466 | val_1_rmse: 0.77341 |  0:00:05s
epoch 2  | loss: 0.58622 | val_0_rmse: 0.77398 | val_1_rmse: 0.76171 |  0:00:08s
epoch 3  | loss: 0.56373 | val_0_rmse: 0.7768  | val_1_rmse: 0.76666 |  0:00:11s
epoch 4  | loss: 0.56083 | val_0_rmse: 0.76929 | val_1_rmse: 0.76362 |  0:00:14s
epoch 5  | loss: 0.55629 | val_0_rmse: 0.73917 | val_1_rmse: 0.72357 |  0:00:16s
epoch 6  | loss: 0.54909 | val_0_rmse: 0.76892 | val_1_rmse: 0.76124 |  0:00:19s
epoch 7  | loss: 0.54004 | val_0_rmse: 0.75463 | val_1_rmse: 0.74142 |  0:00:21s
epoch 8  | loss: 0.53404 | val_0_rmse: 0.74141 | val_1_rmse: 0.73113 |  0:00:24s
epoch 9  | loss: 0.52755 | val_0_rmse: 0.75501 | val_1_rmse: 0.74895 |  0:00:27s
epoch 10 | loss: 0.53475 | val_0_rmse: 0.77386 | val_1_rmse: 0.76328 |  0:00:30s
epoch 11 | loss: 0.53793 | val_0_rmse: 0.75646 | val_1_rmse: 0.74258 |  0:00:33s
epoch 12 | loss: 0.52562 | val_0_rmse: 0.75452 | val_1_rmse: 0.7441  |  0:00:36s
epoch 13 | loss: 0.52694 | val_0_rmse: 0.7492  | val_1_rmse: 0.73805 |  0:00:38s
epoch 14 | loss: 0.52455 | val_0_rmse: 0.80306 | val_1_rmse: 0.79856 |  0:00:41s
epoch 15 | loss: 0.52654 | val_0_rmse: 0.78595 | val_1_rmse: 0.77997 |  0:00:44s
epoch 16 | loss: 0.53119 | val_0_rmse: 0.76759 | val_1_rmse: 0.75866 |  0:00:46s
epoch 17 | loss: 0.5278  | val_0_rmse: 0.75918 | val_1_rmse: 0.74768 |  0:00:49s
epoch 18 | loss: 0.52856 | val_0_rmse: 0.77296 | val_1_rmse: 0.76475 |  0:00:52s
epoch 19 | loss: 0.52499 | val_0_rmse: 0.76412 | val_1_rmse: 0.75623 |  0:00:55s
epoch 20 | loss: 0.52228 | val_0_rmse: 0.78368 | val_1_rmse: 0.77785 |  0:00:58s
epoch 21 | loss: 0.52082 | val_0_rmse: 0.7458  | val_1_rmse: 0.73381 |  0:01:01s
epoch 22 | loss: 0.52434 | val_0_rmse: 0.79819 | val_1_rmse: 0.7926  |  0:01:04s
epoch 23 | loss: 0.52387 | val_0_rmse: 0.78914 | val_1_rmse: 0.7832  |  0:01:06s
epoch 24 | loss: 0.52056 | val_0_rmse: 0.82295 | val_1_rmse: 0.82096 |  0:01:09s
epoch 25 | loss: 0.52445 | val_0_rmse: 0.78421 | val_1_rmse: 0.77769 |  0:01:12s
epoch 26 | loss: 0.51971 | val_0_rmse: 0.76681 | val_1_rmse: 0.75831 |  0:01:15s
epoch 27 | loss: 0.52074 | val_0_rmse: 0.75792 | val_1_rmse: 0.74553 |  0:01:17s
epoch 28 | loss: 0.52025 | val_0_rmse: 0.78333 | val_1_rmse: 0.77416 |  0:01:20s
epoch 29 | loss: 0.5211  | val_0_rmse: 0.78492 | val_1_rmse: 0.7773  |  0:01:23s
epoch 30 | loss: 0.51916 | val_0_rmse: 0.76183 | val_1_rmse: 0.75239 |  0:01:26s
epoch 31 | loss: 0.51787 | val_0_rmse: 0.78915 | val_1_rmse: 0.78255 |  0:01:28s
epoch 32 | loss: 0.5204  | val_0_rmse: 0.77797 | val_1_rmse: 0.77004 |  0:01:31s
epoch 33 | loss: 0.52    | val_0_rmse: 0.77209 | val_1_rmse: 0.76312 |  0:01:34s
epoch 34 | loss: 0.51714 | val_0_rmse: 0.75816 | val_1_rmse: 0.7479  |  0:01:37s
epoch 35 | loss: 0.51647 | val_0_rmse: 0.78421 | val_1_rmse: 0.77679 |  0:01:39s

Early stopping occured at epoch 35 with best_epoch = 5 and best_val_1_rmse = 0.72357
Best weights from best epoch are automatically used!
ended training at: 21:34:23
Feature importance:
[('Area', 0.4890465262592557), ('Baths', 9.924581323289766e-07), ('Beds', 0.07918786696492938), ('Latitude', 0.42522760641141116), ('Longitude', 0.0), ('Month', 0.004858787706418239), ('Year', 0.001678220199853209)]
Mean squared error is of 16523510774.572662
Mean absolute error:100060.08635138706
MAPE:0.28022481763183593
R2 score:0.4592610557648493
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:38:31
epoch 0  | loss: 0.82659 | val_0_rmse: 0.92701 | val_1_rmse: 0.87573 |  0:00:03s
epoch 1  | loss: 0.73142 | val_0_rmse: 0.87537 | val_1_rmse: 0.85491 |  0:00:06s
epoch 2  | loss: 0.72017 | val_0_rmse: 0.86611 | val_1_rmse: 0.84526 |  0:00:09s
epoch 3  | loss: 0.71455 | val_0_rmse: 0.85724 | val_1_rmse: 0.8432  |  0:00:12s
epoch 4  | loss: 0.71662 | val_0_rmse: 0.85421 | val_1_rmse: 0.84108 |  0:00:15s
epoch 5  | loss: 0.70627 | val_0_rmse: 0.84911 | val_1_rmse: 0.83963 |  0:00:18s
epoch 6  | loss: 0.70385 | val_0_rmse: 0.84199 | val_1_rmse: 0.84385 |  0:00:20s
epoch 7  | loss: 0.70683 | val_0_rmse: 0.86287 | val_1_rmse: 0.84523 |  0:00:24s
epoch 8  | loss: 0.70555 | val_0_rmse: 0.84686 | val_1_rmse: 0.83821 |  0:00:27s
epoch 9  | loss: 0.70364 | val_0_rmse: 0.84758 | val_1_rmse: 0.83901 |  0:00:30s
epoch 10 | loss: 0.70551 | val_0_rmse: 0.83341 | val_1_rmse: 0.83291 |  0:00:33s
epoch 11 | loss: 0.70143 | val_0_rmse: 0.83322 | val_1_rmse: 0.83575 |  0:00:36s
epoch 12 | loss: 0.69955 | val_0_rmse: 0.8339  | val_1_rmse: 0.83698 |  0:00:39s
epoch 13 | loss: 0.70035 | val_0_rmse: 0.82809 | val_1_rmse: 0.8304  |  0:00:42s
epoch 14 | loss: 0.69547 | val_0_rmse: 0.83178 | val_1_rmse: 0.83522 |  0:00:44s
epoch 15 | loss: 0.69827 | val_0_rmse: 0.82835 | val_1_rmse: 0.82807 |  0:00:47s
epoch 16 | loss: 0.69165 | val_0_rmse: 0.82712 | val_1_rmse: 0.82815 |  0:00:50s
epoch 17 | loss: 0.68955 | val_0_rmse: 0.82349 | val_1_rmse: 0.82443 |  0:00:53s
epoch 18 | loss: 0.69544 | val_0_rmse: 0.83423 | val_1_rmse: 0.83524 |  0:00:56s
epoch 19 | loss: 0.703   | val_0_rmse: 0.84739 | val_1_rmse: 0.84992 |  0:01:00s
epoch 20 | loss: 0.70579 | val_0_rmse: 0.84046 | val_1_rmse: 0.8465  |  0:01:03s
epoch 21 | loss: 0.70198 | val_0_rmse: 0.8306  | val_1_rmse: 0.83685 |  0:01:06s
epoch 22 | loss: 0.70526 | val_0_rmse: 0.83616 | val_1_rmse: 0.84084 |  0:01:09s
epoch 23 | loss: 0.70136 | val_0_rmse: 0.82991 | val_1_rmse: 0.83051 |  0:01:12s
epoch 24 | loss: 0.70017 | val_0_rmse: 0.83388 | val_1_rmse: 0.83791 |  0:01:16s
epoch 25 | loss: 0.69905 | val_0_rmse: 0.83144 | val_1_rmse: 0.83516 |  0:01:19s
epoch 26 | loss: 0.69947 | val_0_rmse: 0.83258 | val_1_rmse: 0.83494 |  0:01:22s
epoch 27 | loss: 0.70038 | val_0_rmse: 0.83163 | val_1_rmse: 0.83392 |  0:01:25s
epoch 28 | loss: 0.69781 | val_0_rmse: 0.83446 | val_1_rmse: 0.83687 |  0:01:28s
epoch 29 | loss: 0.70419 | val_0_rmse: 0.8358  | val_1_rmse: 0.84003 |  0:01:31s
epoch 30 | loss: 0.70449 | val_0_rmse: 0.83036 | val_1_rmse: 0.83429 |  0:01:35s
epoch 31 | loss: 0.70215 | val_0_rmse: 0.83189 | val_1_rmse: 0.83215 |  0:01:38s
epoch 32 | loss: 0.69849 | val_0_rmse: 0.83171 | val_1_rmse: 0.83333 |  0:01:42s
epoch 33 | loss: 0.696   | val_0_rmse: 0.82989 | val_1_rmse: 0.83165 |  0:01:45s
epoch 34 | loss: 0.69424 | val_0_rmse: 0.83492 | val_1_rmse: 0.83707 |  0:01:48s
epoch 35 | loss: 0.695   | val_0_rmse: 0.8306  | val_1_rmse: 0.83426 |  0:01:51s
epoch 36 | loss: 0.69721 | val_0_rmse: 0.83247 | val_1_rmse: 0.83457 |  0:01:55s
epoch 37 | loss: 0.70473 | val_0_rmse: 0.83655 | val_1_rmse: 0.83975 |  0:01:58s
epoch 38 | loss: 0.69765 | val_0_rmse: 0.83196 | val_1_rmse: 0.83458 |  0:02:01s
epoch 39 | loss: 0.6966  | val_0_rmse: 0.82644 | val_1_rmse: 0.82869 |  0:02:04s
epoch 40 | loss: 0.69323 | val_0_rmse: 0.83889 | val_1_rmse: 0.84044 |  0:02:07s
epoch 41 | loss: 0.70773 | val_0_rmse: 0.84372 | val_1_rmse: 0.84698 |  0:02:10s
epoch 42 | loss: 0.70421 | val_0_rmse: 0.83651 | val_1_rmse: 0.84237 |  0:02:13s
epoch 43 | loss: 0.70822 | val_0_rmse: 0.83524 | val_1_rmse: 0.84015 |  0:02:16s
epoch 44 | loss: 0.70513 | val_0_rmse: 0.83515 | val_1_rmse: 0.84126 |  0:02:19s
epoch 45 | loss: 0.70607 | val_0_rmse: 0.83555 | val_1_rmse: 0.84154 |  0:02:22s
epoch 46 | loss: 0.69898 | val_0_rmse: 0.83467 | val_1_rmse: 0.84032 |  0:02:26s
epoch 47 | loss: 0.70092 | val_0_rmse: 0.83427 | val_1_rmse: 0.83897 |  0:02:29s

Early stopping occured at epoch 47 with best_epoch = 17 and best_val_1_rmse = 0.82443
Best weights from best epoch are automatically used!
ended training at: 21:41:02
Feature importance:
[('Area', 0.5071222767192783), ('Baths', 0.017013949035643572), ('Beds', 0.03894444649324827), ('Latitude', 0.02494135887750752), ('Longitude', 0.11289390726477513), ('Month', 0.21582968212966935), ('Year', 0.08325437947987788)]
Mean squared error is of 52892674116.77758
Mean absolute error:180400.16727242488
MAPE:0.33571572015792817
R2 score:0.3325614789700154
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:41:02
epoch 0  | loss: 0.84371 | val_0_rmse: 0.88111 | val_1_rmse: 1.20394 |  0:00:03s
epoch 1  | loss: 0.72672 | val_0_rmse: 0.84475 | val_1_rmse: 0.82611 |  0:00:06s
epoch 2  | loss: 0.71258 | val_0_rmse: 0.84375 | val_1_rmse: 0.82489 |  0:00:10s
epoch 3  | loss: 0.70894 | val_0_rmse: 0.83954 | val_1_rmse: 0.83324 |  0:00:13s
epoch 4  | loss: 0.7119  | val_0_rmse: 0.86564 | val_1_rmse: 0.85625 |  0:00:16s
epoch 5  | loss: 0.71218 | val_0_rmse: 0.83694 | val_1_rmse: 0.82555 |  0:00:19s
epoch 6  | loss: 0.70356 | val_0_rmse: 0.83694 | val_1_rmse: 0.82176 |  0:00:23s
epoch 7  | loss: 0.70015 | val_0_rmse: 0.83306 | val_1_rmse: 0.82697 |  0:00:26s
epoch 8  | loss: 0.70263 | val_0_rmse: 0.83636 | val_1_rmse: 0.82253 |  0:00:29s
epoch 9  | loss: 0.69998 | val_0_rmse: 0.83351 | val_1_rmse: 0.82042 |  0:00:32s
epoch 10 | loss: 0.6996  | val_0_rmse: 0.83043 | val_1_rmse: 0.81237 |  0:00:35s
epoch 11 | loss: 0.69663 | val_0_rmse: 0.83534 | val_1_rmse: 0.82189 |  0:00:38s
epoch 12 | loss: 0.69733 | val_0_rmse: 0.83053 | val_1_rmse: 0.86835 |  0:00:41s
epoch 13 | loss: 0.69229 | val_0_rmse: 0.82964 | val_1_rmse: 0.83887 |  0:00:45s
epoch 14 | loss: 0.69274 | val_0_rmse: 0.82689 | val_1_rmse: 0.85112 |  0:00:48s
epoch 15 | loss: 0.69429 | val_0_rmse: 0.83288 | val_1_rmse: 0.81359 |  0:00:51s
epoch 16 | loss: 0.69352 | val_0_rmse: 0.82746 | val_1_rmse: 0.80974 |  0:00:54s
epoch 17 | loss: 0.69147 | val_0_rmse: 0.82531 | val_1_rmse: 0.80968 |  0:00:57s
epoch 18 | loss: 0.69192 | val_0_rmse: 0.83019 | val_1_rmse: 1.72729 |  0:01:00s
epoch 19 | loss: 0.68995 | val_0_rmse: 0.82459 | val_1_rmse: 0.87994 |  0:01:03s
epoch 20 | loss: 0.68403 | val_0_rmse: 0.8248  | val_1_rmse: 0.81325 |  0:01:07s
epoch 21 | loss: 0.68857 | val_0_rmse: 0.8266  | val_1_rmse: 0.81148 |  0:01:10s
epoch 22 | loss: 0.68881 | val_0_rmse: 0.82633 | val_1_rmse: 0.81392 |  0:01:13s
epoch 23 | loss: 0.68893 | val_0_rmse: 0.83463 | val_1_rmse: 0.81751 |  0:01:16s
epoch 24 | loss: 0.68375 | val_0_rmse: 0.83184 | val_1_rmse: 0.82012 |  0:01:19s
epoch 25 | loss: 0.68474 | val_0_rmse: 0.8311  | val_1_rmse: 0.81876 |  0:01:22s
epoch 26 | loss: 0.6843  | val_0_rmse: 0.82497 | val_1_rmse: 0.80958 |  0:01:25s
epoch 27 | loss: 0.68502 | val_0_rmse: 0.82138 | val_1_rmse: 0.80627 |  0:01:28s
epoch 28 | loss: 0.68578 | val_0_rmse: 0.82537 | val_1_rmse: 0.81132 |  0:01:31s
epoch 29 | loss: 0.68551 | val_0_rmse: 0.83545 | val_1_rmse: 0.82407 |  0:01:34s
epoch 30 | loss: 0.68327 | val_0_rmse: 0.82308 | val_1_rmse: 0.81127 |  0:01:37s
epoch 31 | loss: 0.67831 | val_0_rmse: 0.81841 | val_1_rmse: 0.81403 |  0:01:40s
epoch 32 | loss: 0.68254 | val_0_rmse: 0.82742 | val_1_rmse: 0.81233 |  0:01:43s
epoch 33 | loss: 0.68599 | val_0_rmse: 0.82299 | val_1_rmse: 0.81909 |  0:01:46s
epoch 34 | loss: 0.68247 | val_0_rmse: 0.81985 | val_1_rmse: 0.80875 |  0:01:49s
epoch 35 | loss: 0.6829  | val_0_rmse: 0.82184 | val_1_rmse: 0.80804 |  0:01:52s
epoch 36 | loss: 0.68635 | val_0_rmse: 0.83942 | val_1_rmse: 0.82531 |  0:01:55s
epoch 37 | loss: 0.68962 | val_0_rmse: 0.83066 | val_1_rmse: 0.81701 |  0:01:59s
epoch 38 | loss: 0.68848 | val_0_rmse: 0.82534 | val_1_rmse: 0.80863 |  0:02:03s
epoch 39 | loss: 0.69072 | val_0_rmse: 0.84472 | val_1_rmse: 0.83083 |  0:02:06s
epoch 40 | loss: 0.70435 | val_0_rmse: 0.83727 | val_1_rmse: 0.8246  |  0:02:10s
epoch 41 | loss: 0.69853 | val_0_rmse: 0.85363 | val_1_rmse: 0.89399 |  0:02:13s
epoch 42 | loss: 0.69713 | val_0_rmse: 0.82837 | val_1_rmse: 0.82459 |  0:02:16s
epoch 43 | loss: 0.70077 | val_0_rmse: 0.82891 | val_1_rmse: 0.81872 |  0:02:20s
epoch 44 | loss: 0.69555 | val_0_rmse: 0.82938 | val_1_rmse: 0.81467 |  0:02:23s
epoch 45 | loss: 0.6945  | val_0_rmse: 0.8237  | val_1_rmse: 0.81965 |  0:02:27s
epoch 46 | loss: 0.68931 | val_0_rmse: 0.83194 | val_1_rmse: 0.81994 |  0:02:30s
epoch 47 | loss: 0.68846 | val_0_rmse: 0.82749 | val_1_rmse: 0.81366 |  0:02:34s
epoch 48 | loss: 0.69085 | val_0_rmse: 0.83286 | val_1_rmse: 0.81554 |  0:02:37s
epoch 49 | loss: 0.68578 | val_0_rmse: 0.82703 | val_1_rmse: 0.81479 |  0:02:40s
epoch 50 | loss: 0.68759 | val_0_rmse: 0.82155 | val_1_rmse: 0.80989 |  0:02:44s
epoch 51 | loss: 0.68566 | val_0_rmse: 0.82222 | val_1_rmse: 0.80998 |  0:02:47s
epoch 52 | loss: 0.68489 | val_0_rmse: 0.82479 | val_1_rmse: 0.8105  |  0:02:50s
epoch 53 | loss: 0.68308 | val_0_rmse: 0.82187 | val_1_rmse: 0.80864 |  0:02:54s
epoch 54 | loss: 0.68334 | val_0_rmse: 0.82319 | val_1_rmse: 0.80932 |  0:02:57s
epoch 55 | loss: 0.68086 | val_0_rmse: 0.81979 | val_1_rmse: 0.80616 |  0:03:00s
epoch 56 | loss: 0.67904 | val_0_rmse: 0.82385 | val_1_rmse: 0.81247 |  0:03:03s
epoch 57 | loss: 0.67688 | val_0_rmse: 0.81855 | val_1_rmse: 0.80734 |  0:03:06s
epoch 58 | loss: 0.6794  | val_0_rmse: 0.8189  | val_1_rmse: 0.80593 |  0:03:10s
epoch 59 | loss: 0.68049 | val_0_rmse: 0.82396 | val_1_rmse: 0.81127 |  0:03:13s
epoch 60 | loss: 0.68511 | val_0_rmse: 0.82476 | val_1_rmse: 0.81154 |  0:03:17s
epoch 61 | loss: 0.68242 | val_0_rmse: 0.82094 | val_1_rmse: 0.80598 |  0:03:20s
epoch 62 | loss: 0.68084 | val_0_rmse: 0.81918 | val_1_rmse: 0.80656 |  0:03:23s
epoch 63 | loss: 0.67883 | val_0_rmse: 0.81923 | val_1_rmse: 0.80588 |  0:03:27s
epoch 64 | loss: 0.67887 | val_0_rmse: 0.82682 | val_1_rmse: 0.81263 |  0:03:30s
epoch 65 | loss: 0.68036 | val_0_rmse: 0.81754 | val_1_rmse: 0.80577 |  0:03:33s
epoch 66 | loss: 0.67824 | val_0_rmse: 0.82075 | val_1_rmse: 0.80678 |  0:03:36s
epoch 67 | loss: 0.67897 | val_0_rmse: 0.81781 | val_1_rmse: 0.80433 |  0:03:39s
epoch 68 | loss: 0.67585 | val_0_rmse: 0.82518 | val_1_rmse: 0.8111  |  0:03:43s
epoch 69 | loss: 0.68411 | val_0_rmse: 0.82019 | val_1_rmse: 0.80752 |  0:03:46s
epoch 70 | loss: 0.68119 | val_0_rmse: 0.82903 | val_1_rmse: 0.81425 |  0:03:49s
epoch 71 | loss: 0.68343 | val_0_rmse: 0.82482 | val_1_rmse: 0.81186 |  0:03:52s
epoch 72 | loss: 0.68023 | val_0_rmse: 0.81897 | val_1_rmse: 0.80578 |  0:03:55s
epoch 73 | loss: 0.68507 | val_0_rmse: 0.81758 | val_1_rmse: 0.80594 |  0:03:59s
epoch 74 | loss: 0.67985 | val_0_rmse: 0.82466 | val_1_rmse: 0.80997 |  0:04:02s
epoch 75 | loss: 0.68054 | val_0_rmse: 0.82922 | val_1_rmse: 0.81936 |  0:04:05s
epoch 76 | loss: 0.67834 | val_0_rmse: 0.81733 | val_1_rmse: 0.80437 |  0:04:08s
epoch 77 | loss: 0.68184 | val_0_rmse: 0.82517 | val_1_rmse: 0.81117 |  0:04:11s
epoch 78 | loss: 0.68185 | val_0_rmse: 0.82029 | val_1_rmse: 0.8082  |  0:04:14s
epoch 79 | loss: 0.6768  | val_0_rmse: 0.82836 | val_1_rmse: 0.819   |  0:04:17s
epoch 80 | loss: 0.68462 | val_0_rmse: 0.82961 | val_1_rmse: 0.81691 |  0:04:20s
epoch 81 | loss: 0.68243 | val_0_rmse: 0.82055 | val_1_rmse: 0.80834 |  0:04:23s
epoch 82 | loss: 0.67936 | val_0_rmse: 0.81785 | val_1_rmse: 0.8064  |  0:04:26s
epoch 83 | loss: 0.6784  | val_0_rmse: 0.82305 | val_1_rmse: 0.81313 |  0:04:29s
epoch 84 | loss: 0.6833  | val_0_rmse: 0.82703 | val_1_rmse: 0.81261 |  0:04:32s
epoch 85 | loss: 0.6839  | val_0_rmse: 0.82082 | val_1_rmse: 0.80678 |  0:04:36s
epoch 86 | loss: 0.68442 | val_0_rmse: 0.82075 | val_1_rmse: 0.80789 |  0:04:39s
epoch 87 | loss: 0.68259 | val_0_rmse: 0.82093 | val_1_rmse: 0.8087  |  0:04:43s
epoch 88 | loss: 0.68084 | val_0_rmse: 0.82011 | val_1_rmse: 0.80829 |  0:04:46s
epoch 89 | loss: 0.67848 | val_0_rmse: 0.8188  | val_1_rmse: 0.80827 |  0:04:49s
epoch 90 | loss: 0.6743  | val_0_rmse: 0.82491 | val_1_rmse: 0.81292 |  0:04:52s
epoch 91 | loss: 0.67624 | val_0_rmse: 0.81859 | val_1_rmse: 0.80742 |  0:04:55s
epoch 92 | loss: 0.67725 | val_0_rmse: 0.81737 | val_1_rmse: 0.80533 |  0:04:59s
epoch 93 | loss: 0.67811 | val_0_rmse: 0.82129 | val_1_rmse: 0.80847 |  0:05:02s
epoch 94 | loss: 0.67895 | val_0_rmse: 0.8184  | val_1_rmse: 0.80673 |  0:05:05s
epoch 95 | loss: 0.6782  | val_0_rmse: 0.81736 | val_1_rmse: 0.80504 |  0:05:08s
epoch 96 | loss: 0.67707 | val_0_rmse: 0.8159  | val_1_rmse: 0.80509 |  0:05:12s
epoch 97 | loss: 0.67679 | val_0_rmse: 0.81875 | val_1_rmse: 0.80621 |  0:05:15s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.80433
Best weights from best epoch are automatically used!
ended training at: 21:46:19
Feature importance:
[('Area', 0.4974784669497171), ('Baths', 0.07595653728665165), ('Beds', 0.045338538567763834), ('Latitude', 0.0126248013514704), ('Longitude', 0.03412118244255034), ('Month', 0.179283117034129), ('Year', 0.1551973563677177)]
Mean squared error is of 55343153283.88005
Mean absolute error:185210.44475216823
MAPE:0.33954195542898047
R2 score:0.3290329363260196
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:46:20
epoch 0  | loss: 0.83095 | val_0_rmse: 0.8674  | val_1_rmse: 0.9177  |  0:00:03s
epoch 1  | loss: 0.72252 | val_0_rmse: 0.83649 | val_1_rmse: 0.84998 |  0:00:06s
epoch 2  | loss: 0.7119  | val_0_rmse: 0.8526  | val_1_rmse: 0.85591 |  0:00:09s
epoch 3  | loss: 0.71076 | val_0_rmse: 0.84769 | val_1_rmse: 0.84694 |  0:00:12s
epoch 4  | loss: 0.70391 | val_0_rmse: 0.83028 | val_1_rmse: 0.90047 |  0:00:16s
epoch 5  | loss: 0.70034 | val_0_rmse: 0.83372 | val_1_rmse: 0.84184 |  0:00:19s
epoch 6  | loss: 0.69358 | val_0_rmse: 0.82857 | val_1_rmse: 0.83738 |  0:00:22s
epoch 7  | loss: 0.69941 | val_0_rmse: 0.83858 | val_1_rmse: 0.84524 |  0:00:25s
epoch 8  | loss: 0.70674 | val_0_rmse: 0.8327  | val_1_rmse: 0.84324 |  0:00:27s
epoch 9  | loss: 0.69824 | val_0_rmse: 0.83219 | val_1_rmse: 0.84032 |  0:00:30s
epoch 10 | loss: 0.6948  | val_0_rmse: 0.83238 | val_1_rmse: 0.84037 |  0:00:33s
epoch 11 | loss: 0.69582 | val_0_rmse: 0.83188 | val_1_rmse: 0.83922 |  0:00:36s
epoch 12 | loss: 0.69205 | val_0_rmse: 0.82797 | val_1_rmse: 0.83516 |  0:00:39s
epoch 13 | loss: 0.68889 | val_0_rmse: 0.83079 | val_1_rmse: 0.83726 |  0:00:42s
epoch 14 | loss: 0.69051 | val_0_rmse: 0.82796 | val_1_rmse: 0.83784 |  0:00:45s
epoch 15 | loss: 0.69174 | val_0_rmse: 0.82752 | val_1_rmse: 0.83309 |  0:00:48s
epoch 16 | loss: 0.69126 | val_0_rmse: 0.82373 | val_1_rmse: 0.83234 |  0:00:52s
epoch 17 | loss: 0.686   | val_0_rmse: 0.82672 | val_1_rmse: 0.83632 |  0:00:55s
epoch 18 | loss: 0.68838 | val_0_rmse: 0.83779 | val_1_rmse: 0.83882 |  0:00:58s
epoch 19 | loss: 0.69187 | val_0_rmse: 0.82866 | val_1_rmse: 0.84411 |  0:01:01s
epoch 20 | loss: 0.6896  | val_0_rmse: 0.82883 | val_1_rmse: 0.83883 |  0:01:04s
epoch 21 | loss: 0.69315 | val_0_rmse: 0.82601 | val_1_rmse: 0.83523 |  0:01:08s
epoch 22 | loss: 0.69357 | val_0_rmse: 0.82762 | val_1_rmse: 0.83573 |  0:01:11s
epoch 23 | loss: 0.68812 | val_0_rmse: 0.82388 | val_1_rmse: 0.83348 |  0:01:14s
epoch 24 | loss: 0.68705 | val_0_rmse: 0.82694 | val_1_rmse: 0.83504 |  0:01:17s
epoch 25 | loss: 0.69225 | val_0_rmse: 0.82947 | val_1_rmse: 0.83871 |  0:01:20s
epoch 26 | loss: 0.6894  | val_0_rmse: 0.82111 | val_1_rmse: 0.83164 |  0:01:23s
epoch 27 | loss: 0.68496 | val_0_rmse: 0.82514 | val_1_rmse: 0.83652 |  0:01:26s
epoch 28 | loss: 0.68501 | val_0_rmse: 0.82723 | val_1_rmse: 0.83964 |  0:01:29s
epoch 29 | loss: 0.68259 | val_0_rmse: 0.82327 | val_1_rmse: 0.83681 |  0:01:32s
epoch 30 | loss: 0.67994 | val_0_rmse: 0.82331 | val_1_rmse: 0.83727 |  0:01:35s
epoch 31 | loss: 0.68563 | val_0_rmse: 0.82926 | val_1_rmse: 0.83774 |  0:01:38s
epoch 32 | loss: 0.6878  | val_0_rmse: 0.82292 | val_1_rmse: 0.83573 |  0:01:41s
epoch 33 | loss: 0.68335 | val_0_rmse: 0.82294 | val_1_rmse: 0.8347  |  0:01:43s
epoch 34 | loss: 0.68366 | val_0_rmse: 0.82343 | val_1_rmse: 0.83523 |  0:01:46s
epoch 35 | loss: 0.68245 | val_0_rmse: 0.81989 | val_1_rmse: 0.83097 |  0:01:49s
epoch 36 | loss: 0.67882 | val_0_rmse: 0.82092 | val_1_rmse: 0.83335 |  0:01:52s
epoch 37 | loss: 0.68006 | val_0_rmse: 0.82178 | val_1_rmse: 0.83477 |  0:01:55s
epoch 38 | loss: 0.67924 | val_0_rmse: 0.82092 | val_1_rmse: 0.83338 |  0:01:58s
epoch 39 | loss: 0.67924 | val_0_rmse: 0.81817 | val_1_rmse: 0.83025 |  0:02:01s
epoch 40 | loss: 0.67728 | val_0_rmse: 0.82858 | val_1_rmse: 0.83813 |  0:02:04s
epoch 41 | loss: 0.6902  | val_0_rmse: 0.83577 | val_1_rmse: 0.844   |  0:02:07s
epoch 42 | loss: 0.7032  | val_0_rmse: 0.83615 | val_1_rmse: 0.86549 |  0:02:10s
epoch 43 | loss: 0.69305 | val_0_rmse: 0.83292 | val_1_rmse: 0.84363 |  0:02:14s
epoch 44 | loss: 0.69406 | val_0_rmse: 0.83046 | val_1_rmse: 0.84095 |  0:02:17s
epoch 45 | loss: 0.69633 | val_0_rmse: 0.83106 | val_1_rmse: 0.8429  |  0:02:20s
epoch 46 | loss: 0.69189 | val_0_rmse: 0.82886 | val_1_rmse: 0.85007 |  0:02:23s
epoch 47 | loss: 0.69781 | val_0_rmse: 0.82875 | val_1_rmse: 0.83627 |  0:02:26s
epoch 48 | loss: 0.69167 | val_0_rmse: 0.8302  | val_1_rmse: 0.94021 |  0:02:29s
epoch 49 | loss: 0.691   | val_0_rmse: 0.825   | val_1_rmse: 0.85439 |  0:02:32s
epoch 50 | loss: 0.68907 | val_0_rmse: 0.82562 | val_1_rmse: 0.85433 |  0:02:36s
epoch 51 | loss: 0.6874  | val_0_rmse: 0.82474 | val_1_rmse: 0.83372 |  0:02:39s
epoch 52 | loss: 0.68834 | val_0_rmse: 0.82649 | val_1_rmse: 0.83539 |  0:02:42s
epoch 53 | loss: 0.69201 | val_0_rmse: 0.83225 | val_1_rmse: 0.84333 |  0:02:45s
epoch 54 | loss: 0.7054  | val_0_rmse: 0.83273 | val_1_rmse: 0.88801 |  0:02:48s
epoch 55 | loss: 0.69491 | val_0_rmse: 0.82838 | val_1_rmse: 0.84495 |  0:02:51s
epoch 56 | loss: 0.69918 | val_0_rmse: 0.84173 | val_1_rmse: 0.86866 |  0:02:54s
epoch 57 | loss: 0.70757 | val_0_rmse: 0.83873 | val_1_rmse: 0.8624  |  0:02:57s
epoch 58 | loss: 0.70505 | val_0_rmse: 0.83631 | val_1_rmse: 0.8578  |  0:03:00s
epoch 59 | loss: 0.7049  | val_0_rmse: 0.83574 | val_1_rmse: 0.84967 |  0:03:03s
epoch 60 | loss: 0.70461 | val_0_rmse: 0.83685 | val_1_rmse: 0.84625 |  0:03:06s
epoch 61 | loss: 0.70519 | val_0_rmse: 0.84603 | val_1_rmse: 0.85388 |  0:03:09s
epoch 62 | loss: 0.70796 | val_0_rmse: 0.84123 | val_1_rmse: 0.89351 |  0:03:12s
epoch 63 | loss: 0.70581 | val_0_rmse: 0.83805 | val_1_rmse: 1.01889 |  0:03:15s
epoch 64 | loss: 0.70639 | val_0_rmse: 0.83714 | val_1_rmse: 0.87716 |  0:03:18s
epoch 65 | loss: 0.7015  | val_0_rmse: 0.83458 | val_1_rmse: 0.89645 |  0:03:21s
epoch 66 | loss: 0.69852 | val_0_rmse: 0.83623 | val_1_rmse: 0.90368 |  0:03:24s
epoch 67 | loss: 0.69657 | val_0_rmse: 0.83076 | val_1_rmse: 0.8785  |  0:03:27s
epoch 68 | loss: 0.69543 | val_0_rmse: 0.83072 | val_1_rmse: 0.86754 |  0:03:30s
epoch 69 | loss: 0.70546 | val_0_rmse: 0.8332  | val_1_rmse: 0.85554 |  0:03:33s

Early stopping occured at epoch 69 with best_epoch = 39 and best_val_1_rmse = 0.83025
Best weights from best epoch are automatically used!
ended training at: 21:49:55
Feature importance:
[('Area', 0.5951430112325282), ('Baths', 0.10143082151371495), ('Beds', 0.09094077683181531), ('Latitude', 0.09396163249925976), ('Longitude', 0.0), ('Month', 0.09852174744336935), ('Year', 0.020002010479312354)]
Mean squared error is of 55229022997.42957
Mean absolute error:182550.16639821194
MAPE:0.331477599413497
R2 score:0.3171191325712458
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:49:56
epoch 0  | loss: 0.85063 | val_0_rmse: 0.86863 | val_1_rmse: 0.86595 |  0:00:03s
epoch 1  | loss: 0.73544 | val_0_rmse: 0.85045 | val_1_rmse: 0.85357 |  0:00:06s
epoch 2  | loss: 0.71578 | val_0_rmse: 0.83673 | val_1_rmse: 0.83978 |  0:00:09s
epoch 3  | loss: 0.70369 | val_0_rmse: 0.83323 | val_1_rmse: 0.83849 |  0:00:12s
epoch 4  | loss: 0.70448 | val_0_rmse: 0.83564 | val_1_rmse: 0.83992 |  0:00:15s
epoch 5  | loss: 0.70098 | val_0_rmse: 0.83624 | val_1_rmse: 0.83866 |  0:00:18s
epoch 6  | loss: 0.70425 | val_0_rmse: 0.8366  | val_1_rmse: 0.8382  |  0:00:21s
epoch 7  | loss: 0.6976  | val_0_rmse: 0.8288  | val_1_rmse: 0.83166 |  0:00:24s
epoch 8  | loss: 0.69735 | val_0_rmse: 0.8344  | val_1_rmse: 0.83689 |  0:00:27s
epoch 9  | loss: 0.69742 | val_0_rmse: 0.83523 | val_1_rmse: 0.83653 |  0:00:30s
epoch 10 | loss: 0.69888 | val_0_rmse: 0.82805 | val_1_rmse: 0.83218 |  0:00:33s
epoch 11 | loss: 0.69124 | val_0_rmse: 0.8257  | val_1_rmse: 0.82981 |  0:00:36s
epoch 12 | loss: 0.69035 | val_0_rmse: 0.83356 | val_1_rmse: 0.83945 |  0:00:40s
epoch 13 | loss: 0.69893 | val_0_rmse: 0.83501 | val_1_rmse: 0.83607 |  0:00:43s
epoch 14 | loss: 0.69552 | val_0_rmse: 0.82878 | val_1_rmse: 0.83249 |  0:00:46s
epoch 15 | loss: 0.69556 | val_0_rmse: 0.83041 | val_1_rmse: 0.83228 |  0:00:49s
epoch 16 | loss: 0.69097 | val_0_rmse: 0.82675 | val_1_rmse: 0.83182 |  0:00:51s
epoch 17 | loss: 0.69254 | val_0_rmse: 0.82762 | val_1_rmse: 0.83167 |  0:00:54s
epoch 18 | loss: 0.69008 | val_0_rmse: 0.82322 | val_1_rmse: 0.82777 |  0:00:57s
epoch 19 | loss: 0.69272 | val_0_rmse: 0.82324 | val_1_rmse: 0.83019 |  0:01:00s
epoch 20 | loss: 0.69422 | val_0_rmse: 0.82762 | val_1_rmse: 0.83158 |  0:01:03s
epoch 21 | loss: 0.69092 | val_0_rmse: 0.83304 | val_1_rmse: 0.83352 |  0:01:06s
epoch 22 | loss: 0.68796 | val_0_rmse: 0.82425 | val_1_rmse: 0.82587 |  0:01:09s
epoch 23 | loss: 0.68513 | val_0_rmse: 0.8328  | val_1_rmse: 0.83658 |  0:01:13s
epoch 24 | loss: 0.69084 | val_0_rmse: 0.82248 | val_1_rmse: 0.82517 |  0:01:15s
epoch 25 | loss: 0.68628 | val_0_rmse: 0.82613 | val_1_rmse: 0.8333  |  0:01:18s
epoch 26 | loss: 0.68716 | val_0_rmse: 0.82562 | val_1_rmse: 0.8314  |  0:01:21s
epoch 27 | loss: 0.68742 | val_0_rmse: 0.82279 | val_1_rmse: 0.82662 |  0:01:24s
epoch 28 | loss: 0.68426 | val_0_rmse: 0.8324  | val_1_rmse: 0.83468 |  0:01:27s
epoch 29 | loss: 0.68935 | val_0_rmse: 0.82446 | val_1_rmse: 0.83061 |  0:01:31s
epoch 30 | loss: 0.68354 | val_0_rmse: 0.82123 | val_1_rmse: 0.82612 |  0:01:34s
epoch 31 | loss: 0.68315 | val_0_rmse: 0.82394 | val_1_rmse: 0.83152 |  0:01:37s
epoch 32 | loss: 0.68655 | val_0_rmse: 0.82856 | val_1_rmse: 0.83423 |  0:01:40s
epoch 33 | loss: 0.68133 | val_0_rmse: 0.82176 | val_1_rmse: 0.82584 |  0:01:43s
epoch 34 | loss: 0.68049 | val_0_rmse: 0.82968 | val_1_rmse: 0.83464 |  0:01:46s
epoch 35 | loss: 0.69766 | val_0_rmse: 0.83242 | val_1_rmse: 0.84029 |  0:01:49s
epoch 36 | loss: 0.68975 | val_0_rmse: 0.82646 | val_1_rmse: 0.83113 |  0:01:52s
epoch 37 | loss: 0.68261 | val_0_rmse: 0.82336 | val_1_rmse: 0.82815 |  0:01:55s
epoch 38 | loss: 0.68    | val_0_rmse: 0.81641 | val_1_rmse: 0.82337 |  0:01:58s
epoch 39 | loss: 0.68233 | val_0_rmse: 0.82566 | val_1_rmse: 0.83113 |  0:02:01s
epoch 40 | loss: 0.68537 | val_0_rmse: 0.82379 | val_1_rmse: 0.83224 |  0:02:04s
epoch 41 | loss: 0.68329 | val_0_rmse: 0.82202 | val_1_rmse: 0.82613 |  0:02:07s
epoch 42 | loss: 0.68001 | val_0_rmse: 0.8255  | val_1_rmse: 0.82832 |  0:02:11s
epoch 43 | loss: 0.68069 | val_0_rmse: 0.81705 | val_1_rmse: 0.82411 |  0:02:14s
epoch 44 | loss: 0.67796 | val_0_rmse: 0.81491 | val_1_rmse: 0.82065 |  0:02:17s
epoch 45 | loss: 0.67806 | val_0_rmse: 0.81961 | val_1_rmse: 0.8292  |  0:02:20s
epoch 46 | loss: 0.67751 | val_0_rmse: 0.82053 | val_1_rmse: 0.82829 |  0:02:24s
epoch 47 | loss: 0.68395 | val_0_rmse: 0.82086 | val_1_rmse: 0.82738 |  0:02:26s
epoch 48 | loss: 0.69767 | val_0_rmse: 0.82983 | val_1_rmse: 0.83282 |  0:02:29s
epoch 49 | loss: 0.68716 | val_0_rmse: 0.82473 | val_1_rmse: 0.82664 |  0:02:32s
epoch 50 | loss: 0.6816  | val_0_rmse: 0.859   | val_1_rmse: 0.82864 |  0:02:36s
epoch 51 | loss: 0.68603 | val_0_rmse: 0.8246  | val_1_rmse: 0.82918 |  0:02:39s
epoch 52 | loss: 0.68415 | val_0_rmse: 0.82262 | val_1_rmse: 0.8281  |  0:02:43s
epoch 53 | loss: 0.67746 | val_0_rmse: 0.82015 | val_1_rmse: 0.82835 |  0:02:46s
epoch 54 | loss: 0.67557 | val_0_rmse: 0.82047 | val_1_rmse: 0.8265  |  0:02:49s
epoch 55 | loss: 0.68277 | val_0_rmse: 0.81776 | val_1_rmse: 0.82407 |  0:02:52s
epoch 56 | loss: 0.67545 | val_0_rmse: 0.81718 | val_1_rmse: 0.82508 |  0:02:55s
epoch 57 | loss: 0.67542 | val_0_rmse: 0.81592 | val_1_rmse: 0.823   |  0:02:58s
epoch 58 | loss: 0.67471 | val_0_rmse: 0.81745 | val_1_rmse: 0.82618 |  0:03:01s
epoch 59 | loss: 0.67511 | val_0_rmse: 0.81829 | val_1_rmse: 0.8262  |  0:03:04s
epoch 60 | loss: 0.67972 | val_0_rmse: 0.81587 | val_1_rmse: 0.82129 |  0:03:08s
epoch 61 | loss: 0.67934 | val_0_rmse: 0.82018 | val_1_rmse: 0.82796 |  0:03:11s
epoch 62 | loss: 0.68614 | val_0_rmse: 0.82236 | val_1_rmse: 0.82797 |  0:03:14s
epoch 63 | loss: 0.69283 | val_0_rmse: 0.8363  | val_1_rmse: 0.84008 |  0:03:17s
epoch 64 | loss: 0.69388 | val_0_rmse: 0.82576 | val_1_rmse: 0.83434 |  0:03:20s
epoch 65 | loss: 0.68762 | val_0_rmse: 0.82483 | val_1_rmse: 0.83282 |  0:03:23s
epoch 66 | loss: 0.68753 | val_0_rmse: 0.82885 | val_1_rmse: 0.83493 |  0:03:26s
epoch 67 | loss: 0.69187 | val_0_rmse: 0.82458 | val_1_rmse: 0.83111 |  0:03:29s
epoch 68 | loss: 0.68586 | val_0_rmse: 0.82197 | val_1_rmse: 0.82945 |  0:03:32s
epoch 69 | loss: 0.68393 | val_0_rmse: 0.82415 | val_1_rmse: 0.83052 |  0:03:35s
epoch 70 | loss: 0.68277 | val_0_rmse: 0.82556 | val_1_rmse: 0.83387 |  0:03:38s
epoch 71 | loss: 0.68167 | val_0_rmse: 0.83622 | val_1_rmse: 0.8449  |  0:03:41s
epoch 72 | loss: 0.68766 | val_0_rmse: 0.82026 | val_1_rmse: 0.82847 |  0:03:44s
epoch 73 | loss: 0.68107 | val_0_rmse: 0.81695 | val_1_rmse: 0.82766 |  0:03:47s
epoch 74 | loss: 0.67762 | val_0_rmse: 0.81661 | val_1_rmse: 0.82689 |  0:03:49s

Early stopping occured at epoch 74 with best_epoch = 44 and best_val_1_rmse = 0.82065
Best weights from best epoch are automatically used!
ended training at: 21:53:47
Feature importance:
[('Area', 0.5442400875880016), ('Baths', 0.04992428885200464), ('Beds', 0.05602113733175751), ('Latitude', 0.009597319787346924), ('Longitude', 0.0), ('Month', 0.17094209399010746), ('Year', 0.16927507245078183)]
Mean squared error is of 54425486916.36518
Mean absolute error:180921.70974977728
MAPE:0.32695318933656004
R2 score:0.32571382554674155
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Melbourne housing.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 21:53:48
epoch 0  | loss: 0.8265  | val_0_rmse: 1.23607 | val_1_rmse: 0.8714  |  0:00:03s
epoch 1  | loss: 0.72368 | val_0_rmse: 0.8464  | val_1_rmse: 0.84914 |  0:00:06s
epoch 2  | loss: 0.71726 | val_0_rmse: 1.0429  | val_1_rmse: 0.84061 |  0:00:09s
epoch 3  | loss: 0.71141 | val_0_rmse: 0.84041 | val_1_rmse: 0.84448 |  0:00:12s
epoch 4  | loss: 0.71832 | val_0_rmse: 0.84252 | val_1_rmse: 0.84995 |  0:00:15s
epoch 5  | loss: 0.71473 | val_0_rmse: 0.84098 | val_1_rmse: 0.8457  |  0:00:18s
epoch 6  | loss: 0.70325 | val_0_rmse: 0.83916 | val_1_rmse: 0.84346 |  0:00:21s
epoch 7  | loss: 0.70298 | val_0_rmse: 0.83019 | val_1_rmse: 0.83488 |  0:00:24s
epoch 8  | loss: 0.69552 | val_0_rmse: 0.83877 | val_1_rmse: 0.84164 |  0:00:27s
epoch 9  | loss: 0.69319 | val_0_rmse: 0.82927 | val_1_rmse: 0.83489 |  0:00:30s
epoch 10 | loss: 0.69387 | val_0_rmse: 0.82844 | val_1_rmse: 0.83488 |  0:00:33s
epoch 11 | loss: 0.69429 | val_0_rmse: 0.82923 | val_1_rmse: 0.83538 |  0:00:36s
epoch 12 | loss: 0.69538 | val_0_rmse: 0.82757 | val_1_rmse: 0.83164 |  0:00:40s
epoch 13 | loss: 0.6932  | val_0_rmse: 0.82904 | val_1_rmse: 0.83369 |  0:00:42s
epoch 14 | loss: 0.69373 | val_0_rmse: 0.82909 | val_1_rmse: 0.83395 |  0:00:46s
epoch 15 | loss: 0.69176 | val_0_rmse: 1.24539 | val_1_rmse: 0.83353 |  0:00:49s
epoch 16 | loss: 0.69025 | val_0_rmse: 1.04351 | val_1_rmse: 0.83031 |  0:00:52s
epoch 17 | loss: 0.69402 | val_0_rmse: 0.8972  | val_1_rmse: 0.83435 |  0:00:55s
epoch 18 | loss: 0.69059 | val_0_rmse: 0.84686 | val_1_rmse: 0.83612 |  0:00:58s
epoch 19 | loss: 0.69105 | val_0_rmse: 0.84652 | val_1_rmse: 0.83259 |  0:01:01s
epoch 20 | loss: 0.68954 | val_0_rmse: 0.83553 | val_1_rmse: 0.84263 |  0:01:04s
epoch 21 | loss: 0.69047 | val_0_rmse: 0.82954 | val_1_rmse: 0.83053 |  0:01:07s
epoch 22 | loss: 0.68893 | val_0_rmse: 0.82766 | val_1_rmse: 0.83493 |  0:01:10s
epoch 23 | loss: 0.69001 | val_0_rmse: 0.82425 | val_1_rmse: 0.83063 |  0:01:13s
epoch 24 | loss: 0.69024 | val_0_rmse: 0.825   | val_1_rmse: 0.82838 |  0:01:16s
epoch 25 | loss: 0.68859 | val_0_rmse: 0.82622 | val_1_rmse: 0.83046 |  0:01:19s
epoch 26 | loss: 0.68802 | val_0_rmse: 0.82457 | val_1_rmse: 0.82936 |  0:01:22s
epoch 27 | loss: 0.68384 | val_0_rmse: 0.83224 | val_1_rmse: 0.83603 |  0:01:25s
epoch 28 | loss: 0.68532 | val_0_rmse: 0.82785 | val_1_rmse: 0.8347  |  0:01:28s
epoch 29 | loss: 0.69049 | val_0_rmse: 0.82833 | val_1_rmse: 0.83355 |  0:01:31s
epoch 30 | loss: 0.6905  | val_0_rmse: 0.8238  | val_1_rmse: 0.83251 |  0:01:33s
epoch 31 | loss: 0.68629 | val_0_rmse: 0.83007 | val_1_rmse: 0.83429 |  0:01:36s
epoch 32 | loss: 0.68478 | val_0_rmse: 0.82763 | val_1_rmse: 0.83406 |  0:01:39s
epoch 33 | loss: 0.68246 | val_0_rmse: 0.82025 | val_1_rmse: 0.82601 |  0:01:42s
epoch 34 | loss: 0.68183 | val_0_rmse: 0.83333 | val_1_rmse: 0.84002 |  0:01:45s
epoch 35 | loss: 0.68721 | val_0_rmse: 0.82368 | val_1_rmse: 0.83121 |  0:01:48s
epoch 36 | loss: 0.68108 | val_0_rmse: 0.82958 | val_1_rmse: 0.84116 |  0:01:51s
epoch 37 | loss: 0.67828 | val_0_rmse: 0.81839 | val_1_rmse: 0.829   |  0:01:55s
epoch 38 | loss: 0.678   | val_0_rmse: 0.82357 | val_1_rmse: 0.83132 |  0:01:58s
epoch 39 | loss: 0.67541 | val_0_rmse: 0.82118 | val_1_rmse: 0.83132 |  0:02:01s
epoch 40 | loss: 0.67951 | val_0_rmse: 0.82592 | val_1_rmse: 0.83469 |  0:02:04s
epoch 41 | loss: 0.67934 | val_0_rmse: 0.82073 | val_1_rmse: 0.83063 |  0:02:07s
epoch 42 | loss: 0.67496 | val_0_rmse: 0.81747 | val_1_rmse: 0.82606 |  0:02:11s
epoch 43 | loss: 0.67215 | val_0_rmse: 0.81706 | val_1_rmse: 0.82609 |  0:02:14s
epoch 44 | loss: 0.6738  | val_0_rmse: 0.82973 | val_1_rmse: 0.8342  |  0:02:17s
epoch 45 | loss: 0.67598 | val_0_rmse: 0.82234 | val_1_rmse: 0.82995 |  0:02:20s
epoch 46 | loss: 0.67622 | val_0_rmse: 0.81849 | val_1_rmse: 0.82584 |  0:02:23s
epoch 47 | loss: 0.66828 | val_0_rmse: 0.81825 | val_1_rmse: 0.82642 |  0:02:26s
epoch 48 | loss: 0.66993 | val_0_rmse: 0.81905 | val_1_rmse: 0.82261 |  0:02:28s
epoch 49 | loss: 0.66999 | val_0_rmse: 0.81985 | val_1_rmse: 0.82665 |  0:02:32s
epoch 50 | loss: 0.67501 | val_0_rmse: 0.81616 | val_1_rmse: 0.8286  |  0:02:35s
epoch 51 | loss: 0.67084 | val_0_rmse: 0.82343 | val_1_rmse: 0.82978 |  0:02:38s
epoch 52 | loss: 0.67006 | val_0_rmse: 0.81486 | val_1_rmse: 0.82288 |  0:02:42s
epoch 53 | loss: 0.67072 | val_0_rmse: 0.81247 | val_1_rmse: 0.82055 |  0:02:45s
epoch 54 | loss: 0.66549 | val_0_rmse: 0.81454 | val_1_rmse: 0.82637 |  0:02:48s
epoch 55 | loss: 0.6668  | val_0_rmse: 0.81298 | val_1_rmse: 0.82065 |  0:02:51s
epoch 56 | loss: 0.66323 | val_0_rmse: 0.81755 | val_1_rmse: 0.82311 |  0:02:54s
epoch 57 | loss: 0.66961 | val_0_rmse: 0.82177 | val_1_rmse: 0.83084 |  0:02:57s
epoch 58 | loss: 0.66951 | val_0_rmse: 0.82161 | val_1_rmse: 0.82979 |  0:03:01s
epoch 59 | loss: 0.67204 | val_0_rmse: 0.82168 | val_1_rmse: 0.83042 |  0:03:04s
epoch 60 | loss: 0.66864 | val_0_rmse: 0.8241  | val_1_rmse: 0.82823 |  0:03:07s
epoch 61 | loss: 0.67116 | val_0_rmse: 0.82784 | val_1_rmse: 0.8398  |  0:03:10s
epoch 62 | loss: 0.69062 | val_0_rmse: 0.83809 | val_1_rmse: 0.84478 |  0:03:13s
epoch 63 | loss: 0.69472 | val_0_rmse: 0.82989 | val_1_rmse: 0.83491 |  0:03:16s
epoch 64 | loss: 0.70329 | val_0_rmse: 0.83168 | val_1_rmse: 0.83641 |  0:03:19s
epoch 65 | loss: 0.69948 | val_0_rmse: 0.82678 | val_1_rmse: 0.82962 |  0:03:22s
epoch 66 | loss: 0.68602 | val_0_rmse: 0.82547 | val_1_rmse: 0.83508 |  0:03:25s
epoch 67 | loss: 0.68944 | val_0_rmse: 0.82542 | val_1_rmse: 0.83265 |  0:03:28s
epoch 68 | loss: 0.68353 | val_0_rmse: 0.82105 | val_1_rmse: 0.82909 |  0:03:31s
epoch 69 | loss: 0.68738 | val_0_rmse: 0.82571 | val_1_rmse: 0.83245 |  0:03:34s
epoch 70 | loss: 0.68377 | val_0_rmse: 0.8374  | val_1_rmse: 0.84157 |  0:03:37s
epoch 71 | loss: 0.69536 | val_0_rmse: 0.84355 | val_1_rmse: 0.85089 |  0:03:40s
epoch 72 | loss: 0.68775 | val_0_rmse: 0.82182 | val_1_rmse: 0.83318 |  0:03:43s
epoch 73 | loss: 0.67901 | val_0_rmse: 0.8162  | val_1_rmse: 0.82361 |  0:03:46s
epoch 74 | loss: 0.6801  | val_0_rmse: 0.81858 | val_1_rmse: 0.82439 |  0:03:49s
epoch 75 | loss: 0.68376 | val_0_rmse: 0.8211  | val_1_rmse: 0.82788 |  0:03:52s
epoch 76 | loss: 0.68288 | val_0_rmse: 0.81575 | val_1_rmse: 0.82503 |  0:03:55s
epoch 77 | loss: 0.67496 | val_0_rmse: 0.81537 | val_1_rmse: 0.82409 |  0:03:58s
epoch 78 | loss: 0.67479 | val_0_rmse: 0.81459 | val_1_rmse: 0.82293 |  0:04:01s
epoch 79 | loss: 0.67528 | val_0_rmse: 0.82106 | val_1_rmse: 0.82683 |  0:04:05s
epoch 80 | loss: 0.67858 | val_0_rmse: 0.81597 | val_1_rmse: 0.82445 |  0:04:08s
epoch 81 | loss: 0.67656 | val_0_rmse: 0.82991 | val_1_rmse: 0.83999 |  0:04:11s
epoch 82 | loss: 0.6776  | val_0_rmse: 0.82271 | val_1_rmse: 0.82875 |  0:04:14s
epoch 83 | loss: 0.67646 | val_0_rmse: 0.81815 | val_1_rmse: 0.82432 |  0:04:17s

Early stopping occured at epoch 83 with best_epoch = 53 and best_val_1_rmse = 0.82055
Best weights from best epoch are automatically used!
ended training at: 21:58:07
Feature importance:
[('Area', 0.5525652203572853), ('Baths', 0.07332455539985623), ('Beds', 0.02963599106343942), ('Latitude', 0.05121395410907642), ('Longitude', 0.01013502452015751), ('Month', 0.11289677987822284), ('Year', 0.17022847467196225)]
Mean squared error is of 54162077816.911896
Mean absolute error:179861.76429446362
MAPE:0.3202166990881588
R2 score:0.328285864997489
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:02:57
epoch 0  | loss: 0.62264 | val_0_rmse: 0.68749 | val_1_rmse: 0.67759 |  0:00:03s
epoch 1  | loss: 0.46074 | val_0_rmse: 0.64504 | val_1_rmse: 0.63906 |  0:00:06s
epoch 2  | loss: 0.41509 | val_0_rmse: 0.63044 | val_1_rmse: 0.62238 |  0:00:09s
epoch 3  | loss: 0.38779 | val_0_rmse: 0.60566 | val_1_rmse: 0.60197 |  0:00:12s
epoch 4  | loss: 0.35636 | val_0_rmse: 0.5891  | val_1_rmse: 0.58491 |  0:00:15s
epoch 5  | loss: 0.35975 | val_0_rmse: 0.61873 | val_1_rmse: 0.61192 |  0:00:18s
epoch 6  | loss: 0.36579 | val_0_rmse: 0.67988 | val_1_rmse: 0.6755  |  0:00:21s
epoch 7  | loss: 0.38318 | val_0_rmse: 0.59205 | val_1_rmse: 0.5872  |  0:00:24s
epoch 8  | loss: 0.35189 | val_0_rmse: 0.56884 | val_1_rmse: 0.5607  |  0:00:28s
epoch 9  | loss: 0.32456 | val_0_rmse: 0.56962 | val_1_rmse: 0.5596  |  0:00:31s
epoch 10 | loss: 0.31474 | val_0_rmse: 0.54149 | val_1_rmse: 0.53094 |  0:00:34s
epoch 11 | loss: 0.30876 | val_0_rmse: 0.68801 | val_1_rmse: 0.68496 |  0:00:37s
epoch 12 | loss: 0.30943 | val_0_rmse: 0.54635 | val_1_rmse: 0.54108 |  0:00:40s
epoch 13 | loss: 0.31273 | val_0_rmse: 0.57935 | val_1_rmse: 0.57443 |  0:00:44s
epoch 14 | loss: 0.30173 | val_0_rmse: 0.56216 | val_1_rmse: 0.55922 |  0:00:47s
epoch 15 | loss: 0.29556 | val_0_rmse: 0.55738 | val_1_rmse: 0.55239 |  0:00:50s
epoch 16 | loss: 0.3071  | val_0_rmse: 0.55142 | val_1_rmse: 0.54864 |  0:00:53s
epoch 17 | loss: 0.30432 | val_0_rmse: 0.51864 | val_1_rmse: 0.51338 |  0:00:56s
epoch 18 | loss: 0.29042 | val_0_rmse: 0.71127 | val_1_rmse: 0.70153 |  0:00:59s
epoch 19 | loss: 0.29363 | val_0_rmse: 0.56027 | val_1_rmse: 0.55891 |  0:01:02s
epoch 20 | loss: 0.28434 | val_0_rmse: 0.55228 | val_1_rmse: 0.54549 |  0:01:05s
epoch 21 | loss: 0.28089 | val_0_rmse: 0.5276  | val_1_rmse: 0.51909 |  0:01:08s
epoch 22 | loss: 0.27105 | val_0_rmse: 0.60117 | val_1_rmse: 0.59907 |  0:01:11s
epoch 23 | loss: 0.26037 | val_0_rmse: 0.66018 | val_1_rmse: 0.65026 |  0:01:14s
epoch 24 | loss: 0.26501 | val_0_rmse: 0.52924 | val_1_rmse: 0.52174 |  0:01:17s
epoch 25 | loss: 0.26104 | val_0_rmse: 0.62415 | val_1_rmse: 0.61832 |  0:01:20s
epoch 26 | loss: 0.2662  | val_0_rmse: 0.65763 | val_1_rmse: 0.64644 |  0:01:23s
epoch 27 | loss: 0.26341 | val_0_rmse: 0.57713 | val_1_rmse: 0.5701  |  0:01:26s
epoch 28 | loss: 0.25548 | val_0_rmse: 0.64598 | val_1_rmse: 0.63561 |  0:01:29s
epoch 29 | loss: 0.25775 | val_0_rmse: 0.63502 | val_1_rmse: 0.62774 |  0:01:32s
epoch 30 | loss: 0.24779 | val_0_rmse: 0.62007 | val_1_rmse: 0.61642 |  0:01:35s
epoch 31 | loss: 0.24907 | val_0_rmse: 0.50716 | val_1_rmse: 0.50205 |  0:01:38s
epoch 32 | loss: 0.24392 | val_0_rmse: 0.5091  | val_1_rmse: 0.50402 |  0:01:41s
epoch 33 | loss: 0.24338 | val_0_rmse: 0.54771 | val_1_rmse: 0.54335 |  0:01:44s
epoch 34 | loss: 0.24847 | val_0_rmse: 0.52645 | val_1_rmse: 0.52059 |  0:01:47s
epoch 35 | loss: 0.24092 | val_0_rmse: 0.64802 | val_1_rmse: 0.64043 |  0:01:50s
epoch 36 | loss: 0.23732 | val_0_rmse: 0.46877 | val_1_rmse: 0.45891 |  0:01:53s
epoch 37 | loss: 0.23232 | val_0_rmse: 0.47365 | val_1_rmse: 0.47189 |  0:01:56s
epoch 38 | loss: 0.24185 | val_0_rmse: 0.46324 | val_1_rmse: 0.4634  |  0:01:59s
epoch 39 | loss: 0.23957 | val_0_rmse: 0.63009 | val_1_rmse: 0.62379 |  0:02:02s
epoch 40 | loss: 0.23646 | val_0_rmse: 0.449   | val_1_rmse: 0.44811 |  0:02:05s
epoch 41 | loss: 0.23874 | val_0_rmse: 0.66065 | val_1_rmse: 0.6546  |  0:02:09s
epoch 42 | loss: 0.2249  | val_0_rmse: 0.48435 | val_1_rmse: 0.48353 |  0:02:12s
epoch 43 | loss: 0.2273  | val_0_rmse: 0.58614 | val_1_rmse: 0.5804  |  0:02:15s
epoch 44 | loss: 0.22593 | val_0_rmse: 0.57293 | val_1_rmse: 0.56916 |  0:02:18s
epoch 45 | loss: 0.23881 | val_0_rmse: 0.52262 | val_1_rmse: 0.51342 |  0:02:21s
epoch 46 | loss: 0.23063 | val_0_rmse: 0.51541 | val_1_rmse: 0.50986 |  0:02:25s
epoch 47 | loss: 0.23128 | val_0_rmse: 0.69192 | val_1_rmse: 0.68778 |  0:02:27s
epoch 48 | loss: 0.22735 | val_0_rmse: 0.53561 | val_1_rmse: 0.52975 |  0:02:30s
epoch 49 | loss: 0.22494 | val_0_rmse: 0.51364 | val_1_rmse: 0.50572 |  0:02:33s
epoch 50 | loss: 0.22976 | val_0_rmse: 0.48614 | val_1_rmse: 0.48417 |  0:02:36s
epoch 51 | loss: 0.23033 | val_0_rmse: 0.48756 | val_1_rmse: 0.48038 |  0:02:39s
epoch 52 | loss: 0.22981 | val_0_rmse: 0.47143 | val_1_rmse: 0.45998 |  0:02:42s
epoch 53 | loss: 0.24058 | val_0_rmse: 0.66326 | val_1_rmse: 0.65556 |  0:02:45s
epoch 54 | loss: 0.22882 | val_0_rmse: 0.45604 | val_1_rmse: 0.45186 |  0:02:48s
epoch 55 | loss: 0.22801 | val_0_rmse: 0.45191 | val_1_rmse: 0.45075 |  0:02:51s
epoch 56 | loss: 0.2213  | val_0_rmse: 0.63308 | val_1_rmse: 0.62586 |  0:02:53s
epoch 57 | loss: 0.21972 | val_0_rmse: 0.4428  | val_1_rmse: 0.43473 |  0:02:56s
epoch 58 | loss: 0.23163 | val_0_rmse: 0.53736 | val_1_rmse: 0.53212 |  0:02:59s
epoch 59 | loss: 0.21957 | val_0_rmse: 0.49589 | val_1_rmse: 0.49198 |  0:03:01s
epoch 60 | loss: 0.21692 | val_0_rmse: 0.44181 | val_1_rmse: 0.43874 |  0:03:04s
epoch 61 | loss: 0.21073 | val_0_rmse: 0.44445 | val_1_rmse: 0.44027 |  0:03:06s
epoch 62 | loss: 0.213   | val_0_rmse: 0.44222 | val_1_rmse: 0.43509 |  0:03:09s
epoch 63 | loss: 0.2092  | val_0_rmse: 0.5587  | val_1_rmse: 0.54983 |  0:03:11s
epoch 64 | loss: 0.21456 | val_0_rmse: 0.52685 | val_1_rmse: 0.51434 |  0:03:14s
epoch 65 | loss: 0.21755 | val_0_rmse: 0.45624 | val_1_rmse: 0.45303 |  0:03:17s
epoch 66 | loss: 0.21049 | val_0_rmse: 0.48323 | val_1_rmse: 0.47742 |  0:03:19s
epoch 67 | loss: 0.21842 | val_0_rmse: 0.55679 | val_1_rmse: 0.55119 |  0:03:22s
epoch 68 | loss: 0.21102 | val_0_rmse: 0.43361 | val_1_rmse: 0.43385 |  0:03:25s
epoch 69 | loss: 0.22675 | val_0_rmse: 0.78569 | val_1_rmse: 0.78236 |  0:03:27s
epoch 70 | loss: 0.21418 | val_0_rmse: 0.59364 | val_1_rmse: 0.59006 |  0:03:30s
epoch 71 | loss: 0.20532 | val_0_rmse: 0.65764 | val_1_rmse: 0.64999 |  0:03:33s
epoch 72 | loss: 0.2223  | val_0_rmse: 0.44608 | val_1_rmse: 0.43761 |  0:03:36s
epoch 73 | loss: 0.21711 | val_0_rmse: 0.43354 | val_1_rmse: 0.4291  |  0:03:38s
epoch 74 | loss: 0.20664 | val_0_rmse: 0.43176 | val_1_rmse: 0.43041 |  0:03:41s
epoch 75 | loss: 0.21485 | val_0_rmse: 0.73362 | val_1_rmse: 0.73167 |  0:03:44s
epoch 76 | loss: 0.21389 | val_0_rmse: 0.69224 | val_1_rmse: 0.68537 |  0:03:46s
epoch 77 | loss: 0.21245 | val_0_rmse: 0.45497 | val_1_rmse: 0.45273 |  0:03:49s
epoch 78 | loss: 0.20613 | val_0_rmse: 0.42745 | val_1_rmse: 0.42308 |  0:03:52s
epoch 79 | loss: 0.21106 | val_0_rmse: 0.44566 | val_1_rmse: 0.44071 |  0:03:54s
epoch 80 | loss: 0.20882 | val_0_rmse: 0.49984 | val_1_rmse: 0.49241 |  0:03:57s
epoch 81 | loss: 0.21209 | val_0_rmse: 0.456   | val_1_rmse: 0.44967 |  0:04:00s
epoch 82 | loss: 0.21418 | val_0_rmse: 0.58627 | val_1_rmse: 0.58046 |  0:04:02s
epoch 83 | loss: 0.21351 | val_0_rmse: 0.44115 | val_1_rmse: 0.43823 |  0:04:05s
epoch 84 | loss: 0.20328 | val_0_rmse: 0.41981 | val_1_rmse: 0.41926 |  0:04:08s
epoch 85 | loss: 0.20527 | val_0_rmse: 0.60744 | val_1_rmse: 0.6012  |  0:04:10s
epoch 86 | loss: 0.20011 | val_0_rmse: 0.4171  | val_1_rmse: 0.41764 |  0:04:13s
epoch 87 | loss: 0.1937  | val_0_rmse: 0.41931 | val_1_rmse: 0.41487 |  0:04:15s
epoch 88 | loss: 0.2018  | val_0_rmse: 0.47354 | val_1_rmse: 0.46693 |  0:04:18s
epoch 89 | loss: 0.20022 | val_0_rmse: 0.67471 | val_1_rmse: 0.67093 |  0:04:21s
epoch 90 | loss: 0.21041 | val_0_rmse: 0.52875 | val_1_rmse: 0.52099 |  0:04:23s
epoch 91 | loss: 0.19676 | val_0_rmse: 0.42534 | val_1_rmse: 0.42467 |  0:04:26s
epoch 92 | loss: 0.21703 | val_0_rmse: 0.66622 | val_1_rmse: 0.66574 |  0:04:29s
epoch 93 | loss: 0.22681 | val_0_rmse: 0.46138 | val_1_rmse: 0.45766 |  0:04:31s
epoch 94 | loss: 0.21331 | val_0_rmse: 0.43778 | val_1_rmse: 0.43596 |  0:04:34s
epoch 95 | loss: 0.20367 | val_0_rmse: 0.43127 | val_1_rmse: 0.42845 |  0:04:37s
epoch 96 | loss: 0.1941  | val_0_rmse: 0.50174 | val_1_rmse: 0.49679 |  0:04:39s
epoch 97 | loss: 0.19517 | val_0_rmse: 0.82943 | val_1_rmse: 0.83087 |  0:04:41s
epoch 98 | loss: 0.19518 | val_0_rmse: 0.44973 | val_1_rmse: 0.44272 |  0:04:44s
epoch 99 | loss: 0.19368 | val_0_rmse: 0.60678 | val_1_rmse: 0.59838 |  0:04:46s
epoch 100| loss: 0.19388 | val_0_rmse: 0.45239 | val_1_rmse: 0.44997 |  0:04:49s
epoch 101| loss: 0.19833 | val_0_rmse: 0.51706 | val_1_rmse: 0.51246 |  0:04:52s
epoch 102| loss: 0.20258 | val_0_rmse: 0.49925 | val_1_rmse: 0.489   |  0:04:55s
epoch 103| loss: 0.2078  | val_0_rmse: 0.42013 | val_1_rmse: 0.41927 |  0:04:58s
epoch 104| loss: 0.21376 | val_0_rmse: 0.4417  | val_1_rmse: 0.44183 |  0:05:01s
epoch 105| loss: 0.21032 | val_0_rmse: 0.41827 | val_1_rmse: 0.41726 |  0:05:03s
epoch 106| loss: 0.19321 | val_0_rmse: 0.64673 | val_1_rmse: 0.63921 |  0:05:06s
epoch 107| loss: 0.19503 | val_0_rmse: 0.72389 | val_1_rmse: 0.7227  |  0:05:09s
epoch 108| loss: 0.19845 | val_0_rmse: 0.62584 | val_1_rmse: 0.62101 |  0:05:12s
epoch 109| loss: 0.18638 | val_0_rmse: 0.58603 | val_1_rmse: 0.57887 |  0:05:14s
epoch 110| loss: 0.21637 | val_0_rmse: 0.53619 | val_1_rmse: 0.52732 |  0:05:17s
epoch 111| loss: 0.20702 | val_0_rmse: 0.43314 | val_1_rmse: 0.42769 |  0:05:19s
epoch 112| loss: 0.20413 | val_0_rmse: 0.42722 | val_1_rmse: 0.42602 |  0:05:22s
epoch 113| loss: 0.19208 | val_0_rmse: 0.46777 | val_1_rmse: 0.46645 |  0:05:25s
epoch 114| loss: 0.19883 | val_0_rmse: 0.6105  | val_1_rmse: 0.6033  |  0:05:28s
epoch 115| loss: 0.19541 | val_0_rmse: 0.59067 | val_1_rmse: 0.58741 |  0:05:30s
epoch 116| loss: 0.19362 | val_0_rmse: 0.46866 | val_1_rmse: 0.46661 |  0:05:33s
epoch 117| loss: 0.18908 | val_0_rmse: 0.43488 | val_1_rmse: 0.43204 |  0:05:35s

Early stopping occured at epoch 117 with best_epoch = 87 and best_val_1_rmse = 0.41487
Best weights from best epoch are automatically used!
ended training at: 22:08:34
Feature importance:
[('Area', 0.22867093820070175), ('Baths', 0.151675888913508), ('Beds', 0.12299091155885677), ('Latitude', 0.40978231557053235), ('Longitude', 0.016203971796883813), ('Month', 0.07067597395951729), ('Year', 0.0)]
Mean squared error is of 1281176538.3372364
Mean absolute error:25353.09866793487
MAPE:0.21500531691278352
R2 score:0.8230724007646112
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:08:35
epoch 0  | loss: 0.60046 | val_0_rmse: 0.65374 | val_1_rmse: 0.64905 |  0:00:02s
epoch 1  | loss: 0.43628 | val_0_rmse: 0.64377 | val_1_rmse: 0.63886 |  0:00:05s
epoch 2  | loss: 0.40552 | val_0_rmse: 0.66787 | val_1_rmse: 0.67195 |  0:00:08s
epoch 3  | loss: 0.36899 | val_0_rmse: 0.83124 | val_1_rmse: 0.8385  |  0:00:11s
epoch 4  | loss: 0.36503 | val_0_rmse: 0.57462 | val_1_rmse: 0.57298 |  0:00:14s
epoch 5  | loss: 0.34482 | val_0_rmse: 0.57027 | val_1_rmse: 0.56556 |  0:00:17s
epoch 6  | loss: 0.32689 | val_0_rmse: 0.65181 | val_1_rmse: 0.6506  |  0:00:20s
epoch 7  | loss: 0.31367 | val_0_rmse: 0.74284 | val_1_rmse: 0.74845 |  0:00:22s
epoch 8  | loss: 0.31878 | val_0_rmse: 0.65393 | val_1_rmse: 0.64968 |  0:00:25s
epoch 9  | loss: 0.32066 | val_0_rmse: 0.86458 | val_1_rmse: 0.88311 |  0:00:28s
epoch 10 | loss: 0.3103  | val_0_rmse: 1.08992 | val_1_rmse: 1.11096 |  0:00:31s
epoch 11 | loss: 0.30372 | val_0_rmse: 0.52132 | val_1_rmse: 0.51642 |  0:00:34s
epoch 12 | loss: 0.29331 | val_0_rmse: 0.54404 | val_1_rmse: 0.53971 |  0:00:37s
epoch 13 | loss: 0.31138 | val_0_rmse: 0.94958 | val_1_rmse: 0.9609  |  0:00:40s
epoch 14 | loss: 0.30368 | val_0_rmse: 0.82805 | val_1_rmse: 0.83121 |  0:00:43s
epoch 15 | loss: 0.29633 | val_0_rmse: 0.60404 | val_1_rmse: 0.60187 |  0:00:46s
epoch 16 | loss: 0.29136 | val_0_rmse: 0.71883 | val_1_rmse: 0.72314 |  0:00:49s
epoch 17 | loss: 0.29451 | val_0_rmse: 0.72042 | val_1_rmse: 0.71517 |  0:00:52s
epoch 18 | loss: 0.31078 | val_0_rmse: 0.70019 | val_1_rmse: 0.70135 |  0:00:55s
epoch 19 | loss: 0.29597 | val_0_rmse: 0.62498 | val_1_rmse: 0.62494 |  0:00:58s
epoch 20 | loss: 0.29433 | val_0_rmse: 0.60952 | val_1_rmse: 0.61009 |  0:01:01s
epoch 21 | loss: 0.2816  | val_0_rmse: 0.85065 | val_1_rmse: 0.86516 |  0:01:04s
epoch 22 | loss: 0.2769  | val_0_rmse: 0.66422 | val_1_rmse: 0.66841 |  0:01:07s
epoch 23 | loss: 0.27768 | val_0_rmse: 0.5372  | val_1_rmse: 0.53036 |  0:01:10s
epoch 24 | loss: 0.27556 | val_0_rmse: 0.70059 | val_1_rmse: 0.69854 |  0:01:13s
epoch 25 | loss: 0.27537 | val_0_rmse: 0.67563 | val_1_rmse: 0.681   |  0:01:16s
epoch 26 | loss: 0.27299 | val_0_rmse: 0.58498 | val_1_rmse: 0.58072 |  0:01:19s
epoch 27 | loss: 0.26661 | val_0_rmse: 0.50228 | val_1_rmse: 0.49742 |  0:01:22s
epoch 28 | loss: 0.26733 | val_0_rmse: 0.53867 | val_1_rmse: 0.5346  |  0:01:26s
epoch 29 | loss: 0.27487 | val_0_rmse: 0.69739 | val_1_rmse: 0.6963  |  0:01:29s
epoch 30 | loss: 0.26713 | val_0_rmse: 0.67529 | val_1_rmse: 0.6772  |  0:01:33s
epoch 31 | loss: 0.26612 | val_0_rmse: 0.56683 | val_1_rmse: 0.56835 |  0:01:36s
epoch 32 | loss: 0.27461 | val_0_rmse: 0.51099 | val_1_rmse: 0.51235 |  0:01:39s
epoch 33 | loss: 0.27284 | val_0_rmse: 0.53758 | val_1_rmse: 0.53273 |  0:01:43s
epoch 34 | loss: 0.26845 | val_0_rmse: 0.71015 | val_1_rmse: 0.71082 |  0:01:46s
epoch 35 | loss: 0.26562 | val_0_rmse: 0.49035 | val_1_rmse: 0.48465 |  0:01:49s
epoch 36 | loss: 0.25363 | val_0_rmse: 0.48813 | val_1_rmse: 0.48363 |  0:01:52s
epoch 37 | loss: 0.25367 | val_0_rmse: 0.4962  | val_1_rmse: 0.49501 |  0:01:55s
epoch 38 | loss: 0.24947 | val_0_rmse: 0.51523 | val_1_rmse: 0.51252 |  0:01:59s
epoch 39 | loss: 0.26093 | val_0_rmse: 0.54046 | val_1_rmse: 0.5329  |  0:02:02s
epoch 40 | loss: 0.2644  | val_0_rmse: 0.70117 | val_1_rmse: 0.70983 |  0:02:06s
epoch 41 | loss: 0.25764 | val_0_rmse: 0.55983 | val_1_rmse: 0.5555  |  0:02:09s
epoch 42 | loss: 0.25484 | val_0_rmse: 0.58957 | val_1_rmse: 0.58158 |  0:02:12s
epoch 43 | loss: 0.26603 | val_0_rmse: 0.51143 | val_1_rmse: 0.50445 |  0:02:15s
epoch 44 | loss: 0.26054 | val_0_rmse: 0.58893 | val_1_rmse: 0.58102 |  0:02:18s
epoch 45 | loss: 0.26113 | val_0_rmse: 0.69382 | val_1_rmse: 0.69633 |  0:02:21s
epoch 46 | loss: 0.25581 | val_0_rmse: 0.56752 | val_1_rmse: 0.5671  |  0:02:24s
epoch 47 | loss: 0.25127 | val_0_rmse: 0.66813 | val_1_rmse: 0.66499 |  0:02:27s
epoch 48 | loss: 0.25955 | val_0_rmse: 0.58542 | val_1_rmse: 0.58328 |  0:02:30s
epoch 49 | loss: 0.25598 | val_0_rmse: 0.78986 | val_1_rmse: 0.7989  |  0:02:33s
epoch 50 | loss: 0.24486 | val_0_rmse: 0.50744 | val_1_rmse: 0.50052 |  0:02:36s
epoch 51 | loss: 0.24176 | val_0_rmse: 0.47969 | val_1_rmse: 0.47542 |  0:02:39s
epoch 52 | loss: 0.2382  | val_0_rmse: 0.53689 | val_1_rmse: 0.53777 |  0:02:41s
epoch 53 | loss: 0.24759 | val_0_rmse: 0.56911 | val_1_rmse: 0.55707 |  0:02:44s
epoch 54 | loss: 0.23629 | val_0_rmse: 0.46557 | val_1_rmse: 0.45881 |  0:02:47s
epoch 55 | loss: 0.2365  | val_0_rmse: 0.60017 | val_1_rmse: 0.60282 |  0:02:50s
epoch 56 | loss: 0.24259 | val_0_rmse: 0.55864 | val_1_rmse: 0.55958 |  0:02:53s
epoch 57 | loss: 0.24298 | val_0_rmse: 0.77722 | val_1_rmse: 0.78881 |  0:02:56s
epoch 58 | loss: 0.23876 | val_0_rmse: 0.49884 | val_1_rmse: 0.491   |  0:02:59s
epoch 59 | loss: 0.23999 | val_0_rmse: 0.52577 | val_1_rmse: 0.51949 |  0:03:02s
epoch 60 | loss: 0.24596 | val_0_rmse: 0.86573 | val_1_rmse: 0.87732 |  0:03:05s
epoch 61 | loss: 0.24311 | val_0_rmse: 0.66737 | val_1_rmse: 0.66026 |  0:03:08s
epoch 62 | loss: 0.25009 | val_0_rmse: 0.53082 | val_1_rmse: 0.52024 |  0:03:11s
epoch 63 | loss: 0.23654 | val_0_rmse: 0.57373 | val_1_rmse: 0.56724 |  0:03:14s
epoch 64 | loss: 0.23423 | val_0_rmse: 0.45161 | val_1_rmse: 0.44465 |  0:03:17s
epoch 65 | loss: 0.23024 | val_0_rmse: 0.49176 | val_1_rmse: 0.48759 |  0:03:20s
epoch 66 | loss: 0.2289  | val_0_rmse: 0.48832 | val_1_rmse: 0.47866 |  0:03:23s
epoch 67 | loss: 0.23898 | val_0_rmse: 0.4811  | val_1_rmse: 0.47056 |  0:03:26s
epoch 68 | loss: 0.22915 | val_0_rmse: 0.54184 | val_1_rmse: 0.53464 |  0:03:29s
epoch 69 | loss: 0.22411 | val_0_rmse: 0.50393 | val_1_rmse: 0.49128 |  0:03:32s
epoch 70 | loss: 0.22974 | val_0_rmse: 0.4931  | val_1_rmse: 0.48952 |  0:03:36s
epoch 71 | loss: 0.22208 | val_0_rmse: 0.71704 | val_1_rmse: 0.7234  |  0:03:39s
epoch 72 | loss: 0.22363 | val_0_rmse: 0.54256 | val_1_rmse: 0.53667 |  0:03:42s
epoch 73 | loss: 0.22391 | val_0_rmse: 0.48245 | val_1_rmse: 0.46886 |  0:03:45s
epoch 74 | loss: 0.23514 | val_0_rmse: 0.59109 | val_1_rmse: 0.58216 |  0:03:48s
epoch 75 | loss: 0.24009 | val_0_rmse: 0.59722 | val_1_rmse: 0.58296 |  0:03:51s
epoch 76 | loss: 0.23543 | val_0_rmse: 0.47078 | val_1_rmse: 0.46682 |  0:03:55s
epoch 77 | loss: 0.22683 | val_0_rmse: 0.57827 | val_1_rmse: 0.57377 |  0:03:58s
epoch 78 | loss: 0.22026 | val_0_rmse: 0.44711 | val_1_rmse: 0.43866 |  0:04:02s
epoch 79 | loss: 0.21521 | val_0_rmse: 0.43478 | val_1_rmse: 0.42952 |  0:04:05s
epoch 80 | loss: 0.21342 | val_0_rmse: 0.44915 | val_1_rmse: 0.44299 |  0:04:09s
epoch 81 | loss: 0.21256 | val_0_rmse: 0.48525 | val_1_rmse: 0.47833 |  0:04:12s
epoch 82 | loss: 0.21318 | val_0_rmse: 0.43905 | val_1_rmse: 0.42964 |  0:04:15s
epoch 83 | loss: 0.21303 | val_0_rmse: 0.45602 | val_1_rmse: 0.45241 |  0:04:18s
epoch 84 | loss: 0.21965 | val_0_rmse: 0.5334  | val_1_rmse: 0.53382 |  0:04:22s
epoch 85 | loss: 0.21801 | val_0_rmse: 0.5494  | val_1_rmse: 0.54212 |  0:04:24s
epoch 86 | loss: 0.21893 | val_0_rmse: 0.45927 | val_1_rmse: 0.45874 |  0:04:27s
epoch 87 | loss: 0.20912 | val_0_rmse: 0.57639 | val_1_rmse: 0.57315 |  0:04:30s
epoch 88 | loss: 0.21167 | val_0_rmse: 0.45644 | val_1_rmse: 0.45036 |  0:04:34s
epoch 89 | loss: 0.21188 | val_0_rmse: 0.5075  | val_1_rmse: 0.50475 |  0:04:37s
epoch 90 | loss: 0.20963 | val_0_rmse: 0.45072 | val_1_rmse: 0.44461 |  0:04:40s
epoch 91 | loss: 0.21552 | val_0_rmse: 0.67723 | val_1_rmse: 0.68205 |  0:04:43s
epoch 92 | loss: 0.21931 | val_0_rmse: 0.52758 | val_1_rmse: 0.51969 |  0:04:47s
epoch 93 | loss: 0.21048 | val_0_rmse: 0.47602 | val_1_rmse: 0.47289 |  0:04:50s
epoch 94 | loss: 0.214   | val_0_rmse: 0.42851 | val_1_rmse: 0.42192 |  0:04:54s
epoch 95 | loss: 0.20302 | val_0_rmse: 0.45284 | val_1_rmse: 0.4455  |  0:04:57s
epoch 96 | loss: 0.21078 | val_0_rmse: 0.47334 | val_1_rmse: 0.46003 |  0:05:00s
epoch 97 | loss: 0.21486 | val_0_rmse: 0.56508 | val_1_rmse: 0.56205 |  0:05:03s
epoch 98 | loss: 0.20271 | val_0_rmse: 0.4426  | val_1_rmse: 0.43457 |  0:05:06s
epoch 99 | loss: 0.20242 | val_0_rmse: 0.43399 | val_1_rmse: 0.42964 |  0:05:09s
epoch 100| loss: 0.2033  | val_0_rmse: 0.45178 | val_1_rmse: 0.44813 |  0:05:12s
epoch 101| loss: 0.20561 | val_0_rmse: 0.44028 | val_1_rmse: 0.42738 |  0:05:15s
epoch 102| loss: 0.20258 | val_0_rmse: 0.44225 | val_1_rmse: 0.43773 |  0:05:18s
epoch 103| loss: 0.20163 | val_0_rmse: 0.45703 | val_1_rmse: 0.44689 |  0:05:22s
epoch 104| loss: 0.20694 | val_0_rmse: 0.60637 | val_1_rmse: 0.60286 |  0:05:25s
epoch 105| loss: 0.20342 | val_0_rmse: 0.4746  | val_1_rmse: 0.4721  |  0:05:28s
epoch 106| loss: 0.19948 | val_0_rmse: 0.52234 | val_1_rmse: 0.51364 |  0:05:32s
epoch 107| loss: 0.1995  | val_0_rmse: 0.48655 | val_1_rmse: 0.47466 |  0:05:36s
epoch 108| loss: 0.20499 | val_0_rmse: 0.48419 | val_1_rmse: 0.48533 |  0:05:39s
epoch 109| loss: 0.20042 | val_0_rmse: 0.47396 | val_1_rmse: 0.47254 |  0:05:42s
epoch 110| loss: 0.19944 | val_0_rmse: 0.41369 | val_1_rmse: 0.40732 |  0:05:45s
epoch 111| loss: 0.20141 | val_0_rmse: 0.61619 | val_1_rmse: 0.61864 |  0:05:48s
epoch 112| loss: 0.19985 | val_0_rmse: 0.59382 | val_1_rmse: 0.59258 |  0:05:51s
epoch 113| loss: 0.19421 | val_0_rmse: 0.45682 | val_1_rmse: 0.45515 |  0:05:55s
epoch 114| loss: 0.19788 | val_0_rmse: 0.47161 | val_1_rmse: 0.46341 |  0:05:58s
epoch 115| loss: 0.19385 | val_0_rmse: 0.45835 | val_1_rmse: 0.45601 |  0:06:02s
epoch 116| loss: 0.19686 | val_0_rmse: 0.47371 | val_1_rmse: 0.46327 |  0:06:05s
epoch 117| loss: 0.19689 | val_0_rmse: 0.48197 | val_1_rmse: 0.47667 |  0:06:08s
epoch 118| loss: 0.19404 | val_0_rmse: 0.4597  | val_1_rmse: 0.46024 |  0:06:11s
epoch 119| loss: 0.19266 | val_0_rmse: 0.42044 | val_1_rmse: 0.41204 |  0:06:14s
epoch 120| loss: 0.19953 | val_0_rmse: 0.59948 | val_1_rmse: 0.59268 |  0:06:17s
epoch 121| loss: 0.20588 | val_0_rmse: 0.50993 | val_1_rmse: 0.50427 |  0:06:20s
epoch 122| loss: 0.20378 | val_0_rmse: 0.46082 | val_1_rmse: 0.45714 |  0:06:23s
epoch 123| loss: 0.21068 | val_0_rmse: 0.50241 | val_1_rmse: 0.50315 |  0:06:27s
epoch 124| loss: 0.20627 | val_0_rmse: 0.59595 | val_1_rmse: 0.60243 |  0:06:30s
epoch 125| loss: 0.21227 | val_0_rmse: 0.50049 | val_1_rmse: 0.48539 |  0:06:33s
epoch 126| loss: 0.20085 | val_0_rmse: 0.57842 | val_1_rmse: 0.57773 |  0:06:36s
epoch 127| loss: 0.20581 | val_0_rmse: 0.56635 | val_1_rmse: 0.56945 |  0:06:39s
epoch 128| loss: 0.20636 | val_0_rmse: 0.50026 | val_1_rmse: 0.493   |  0:06:43s
epoch 129| loss: 0.19178 | val_0_rmse: 0.50581 | val_1_rmse: 0.50786 |  0:06:46s
epoch 130| loss: 0.21192 | val_0_rmse: 0.42948 | val_1_rmse: 0.42414 |  0:06:49s
epoch 131| loss: 0.21172 | val_0_rmse: 0.48917 | val_1_rmse: 0.47698 |  0:06:52s
epoch 132| loss: 0.20386 | val_0_rmse: 0.46855 | val_1_rmse: 0.46945 |  0:06:55s
epoch 133| loss: 0.19763 | val_0_rmse: 0.41077 | val_1_rmse: 0.40795 |  0:06:58s
epoch 134| loss: 0.19632 | val_0_rmse: 0.44966 | val_1_rmse: 0.44507 |  0:07:01s
epoch 135| loss: 0.19754 | val_0_rmse: 0.43528 | val_1_rmse: 0.43454 |  0:07:04s
epoch 136| loss: 0.19019 | val_0_rmse: 0.44389 | val_1_rmse: 0.4403  |  0:07:07s
epoch 137| loss: 0.19027 | val_0_rmse: 0.46986 | val_1_rmse: 0.47121 |  0:07:11s
epoch 138| loss: 0.18405 | val_0_rmse: 0.40509 | val_1_rmse: 0.39856 |  0:07:14s
epoch 139| loss: 0.18375 | val_0_rmse: 0.62207 | val_1_rmse: 0.62368 |  0:07:18s
epoch 140| loss: 0.1799  | val_0_rmse: 0.43024 | val_1_rmse: 0.42556 |  0:07:22s
epoch 141| loss: 0.18555 | val_0_rmse: 0.53126 | val_1_rmse: 0.52805 |  0:07:25s
epoch 142| loss: 0.18739 | val_0_rmse: 0.54397 | val_1_rmse: 0.53784 |  0:07:29s
epoch 143| loss: 0.1894  | val_0_rmse: 0.43781 | val_1_rmse: 0.43034 |  0:07:32s
epoch 144| loss: 0.18929 | val_0_rmse: 0.6034  | val_1_rmse: 0.60579 |  0:07:35s
epoch 145| loss: 0.17991 | val_0_rmse: 0.53688 | val_1_rmse: 0.53825 |  0:07:39s
epoch 146| loss: 0.18903 | val_0_rmse: 0.67612 | val_1_rmse: 0.67827 |  0:07:42s
epoch 147| loss: 0.18497 | val_0_rmse: 0.38802 | val_1_rmse: 0.38045 |  0:07:45s
epoch 148| loss: 0.18391 | val_0_rmse: 0.41063 | val_1_rmse: 0.39587 |  0:07:48s
epoch 149| loss: 0.18229 | val_0_rmse: 0.4247  | val_1_rmse: 0.41984 |  0:07:52s
Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_val_1_rmse = 0.38045
Best weights from best epoch are automatically used!
ended training at: 22:16:28
Feature importance:
[('Area', 0.2540176243087318), ('Baths', 0.1509658804242645), ('Beds', 0.039977008904543025), ('Latitude', 0.4430396954547323), ('Longitude', 0.003911909797927336), ('Month', 0.03564816462648085), ('Year', 0.07243971648332016)]
Mean squared error is of 1139164455.907743
Mean absolute error:23647.939259916686
MAPE:0.2085436456647547
R2 score:0.8422146508552507
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:16:29
epoch 0  | loss: 0.59786 | val_0_rmse: 0.71156 | val_1_rmse: 0.71924 |  0:00:02s
epoch 1  | loss: 0.44369 | val_0_rmse: 0.64419 | val_1_rmse: 0.64656 |  0:00:06s
epoch 2  | loss: 0.42705 | val_0_rmse: 0.65061 | val_1_rmse: 0.65364 |  0:00:09s
epoch 3  | loss: 0.42271 | val_0_rmse: 0.62793 | val_1_rmse: 0.63132 |  0:00:12s
epoch 4  | loss: 0.40709 | val_0_rmse: 0.63027 | val_1_rmse: 0.65827 |  0:00:15s
epoch 5  | loss: 0.38338 | val_0_rmse: 0.83087 | val_1_rmse: 0.83679 |  0:00:18s
epoch 6  | loss: 0.37821 | val_0_rmse: 0.61138 | val_1_rmse: 0.61474 |  0:00:21s
epoch 7  | loss: 0.35002 | val_0_rmse: 0.76428 | val_1_rmse: 0.76082 |  0:00:24s
epoch 8  | loss: 0.35968 | val_0_rmse: 0.58413 | val_1_rmse: 0.58992 |  0:00:27s
epoch 9  | loss: 0.36008 | val_0_rmse: 0.7297  | val_1_rmse: 0.72945 |  0:00:30s
epoch 10 | loss: 0.3508  | val_0_rmse: 0.79532 | val_1_rmse: 0.79292 |  0:00:33s
epoch 11 | loss: 0.34982 | val_0_rmse: 0.5883  | val_1_rmse: 0.58894 |  0:00:36s
epoch 12 | loss: 0.33862 | val_0_rmse: 0.56785 | val_1_rmse: 0.56953 |  0:00:39s
epoch 13 | loss: 0.3327  | val_0_rmse: 0.55687 | val_1_rmse: 0.5592  |  0:00:42s
epoch 14 | loss: 0.32236 | val_0_rmse: 0.55158 | val_1_rmse: 0.55488 |  0:00:45s
epoch 15 | loss: 0.33138 | val_0_rmse: 0.63932 | val_1_rmse: 0.64727 |  0:00:48s
epoch 16 | loss: 0.338   | val_0_rmse: 0.72573 | val_1_rmse: 0.72744 |  0:00:52s
epoch 17 | loss: 0.33507 | val_0_rmse: 0.58069 | val_1_rmse: 0.58353 |  0:00:56s
epoch 18 | loss: 0.34116 | val_0_rmse: 0.60108 | val_1_rmse: 0.60514 |  0:00:59s
epoch 19 | loss: 0.32938 | val_0_rmse: 0.69184 | val_1_rmse: 0.69654 |  0:01:03s
epoch 20 | loss: 0.32839 | val_0_rmse: 0.54809 | val_1_rmse: 0.55176 |  0:01:06s
epoch 21 | loss: 0.33475 | val_0_rmse: 0.59927 | val_1_rmse: 0.59783 |  0:01:09s
epoch 22 | loss: 0.3498  | val_0_rmse: 0.90407 | val_1_rmse: 0.90377 |  0:01:13s
epoch 23 | loss: 0.3465  | val_0_rmse: 0.66424 | val_1_rmse: 0.67103 |  0:01:16s
epoch 24 | loss: 0.3386  | val_0_rmse: 0.65212 | val_1_rmse: 0.65552 |  0:01:19s
epoch 25 | loss: 0.32295 | val_0_rmse: 0.63949 | val_1_rmse: 0.64351 |  0:01:23s
epoch 26 | loss: 0.31961 | val_0_rmse: 0.59359 | val_1_rmse: 0.5993  |  0:01:26s
epoch 27 | loss: 0.31582 | val_0_rmse: 0.56079 | val_1_rmse: 0.56795 |  0:01:29s
epoch 28 | loss: 0.31436 | val_0_rmse: 0.65787 | val_1_rmse: 0.65938 |  0:01:33s
epoch 29 | loss: 0.31728 | val_0_rmse: 0.53989 | val_1_rmse: 0.54434 |  0:01:36s
epoch 30 | loss: 0.31124 | val_0_rmse: 0.73683 | val_1_rmse: 0.7404  |  0:01:39s
epoch 31 | loss: 0.30634 | val_0_rmse: 0.55039 | val_1_rmse: 0.55469 |  0:01:43s
epoch 32 | loss: 0.31039 | val_0_rmse: 0.54321 | val_1_rmse: 0.54883 |  0:01:47s
epoch 33 | loss: 0.30641 | val_0_rmse: 0.58475 | val_1_rmse: 0.59042 |  0:01:50s
epoch 34 | loss: 0.30455 | val_0_rmse: 0.53848 | val_1_rmse: 0.54341 |  0:01:53s
epoch 35 | loss: 0.30485 | val_0_rmse: 0.71768 | val_1_rmse: 0.72176 |  0:01:57s
epoch 36 | loss: 0.30136 | val_0_rmse: 0.59786 | val_1_rmse: 0.59936 |  0:02:00s
epoch 37 | loss: 0.30448 | val_0_rmse: 0.56283 | val_1_rmse: 0.5701  |  0:02:03s
epoch 38 | loss: 0.29874 | val_0_rmse: 0.53261 | val_1_rmse: 0.53652 |  0:02:06s
epoch 39 | loss: 0.30732 | val_0_rmse: 0.55931 | val_1_rmse: 0.5627  |  0:02:09s
epoch 40 | loss: 0.301   | val_0_rmse: 0.53397 | val_1_rmse: 0.5414  |  0:02:12s
epoch 41 | loss: 0.29959 | val_0_rmse: 0.57417 | val_1_rmse: 0.5804  |  0:02:16s
epoch 42 | loss: 0.30349 | val_0_rmse: 0.76928 | val_1_rmse: 0.77173 |  0:02:19s
epoch 43 | loss: 0.30462 | val_0_rmse: 0.53403 | val_1_rmse: 0.53518 |  0:02:22s
epoch 44 | loss: 0.29655 | val_0_rmse: 0.56419 | val_1_rmse: 0.56618 |  0:02:25s
epoch 45 | loss: 0.29924 | val_0_rmse: 0.61504 | val_1_rmse: 0.62231 |  0:02:28s
epoch 46 | loss: 0.30009 | val_0_rmse: 0.56774 | val_1_rmse: 0.57336 |  0:02:31s
epoch 47 | loss: 0.29628 | val_0_rmse: 0.66911 | val_1_rmse: 0.67405 |  0:02:34s
epoch 48 | loss: 0.2949  | val_0_rmse: 0.56162 | val_1_rmse: 0.56736 |  0:02:37s
epoch 49 | loss: 0.29469 | val_0_rmse: 0.53336 | val_1_rmse: 0.5388  |  0:02:40s
epoch 50 | loss: 0.29223 | val_0_rmse: 0.55371 | val_1_rmse: 0.55752 |  0:02:43s
epoch 51 | loss: 0.2998  | val_0_rmse: 0.61759 | val_1_rmse: 0.62088 |  0:02:46s
epoch 52 | loss: 0.29436 | val_0_rmse: 0.66924 | val_1_rmse: 0.67366 |  0:02:50s
epoch 53 | loss: 0.29333 | val_0_rmse: 0.63925 | val_1_rmse: 0.64225 |  0:02:53s
epoch 54 | loss: 0.28868 | val_0_rmse: 0.52558 | val_1_rmse: 0.52699 |  0:02:56s
epoch 55 | loss: 0.29569 | val_0_rmse: 0.64106 | val_1_rmse: 0.64746 |  0:02:59s
epoch 56 | loss: 0.29734 | val_0_rmse: 0.69058 | val_1_rmse: 0.69656 |  0:03:02s
epoch 57 | loss: 0.38057 | val_0_rmse: 0.69103 | val_1_rmse: 0.68996 |  0:03:05s
epoch 58 | loss: 0.35003 | val_0_rmse: 0.87032 | val_1_rmse: 0.86839 |  0:03:08s
epoch 59 | loss: 0.33349 | val_0_rmse: 0.59401 | val_1_rmse: 0.59623 |  0:03:11s
epoch 60 | loss: 0.32396 | val_0_rmse: 0.77738 | val_1_rmse: 0.77646 |  0:03:14s
epoch 61 | loss: 0.32088 | val_0_rmse: 0.62577 | val_1_rmse: 0.63046 |  0:03:17s
epoch 62 | loss: 0.31355 | val_0_rmse: 0.75957 | val_1_rmse: 0.75972 |  0:03:21s
epoch 63 | loss: 0.30797 | val_0_rmse: 0.61734 | val_1_rmse: 0.62366 |  0:03:24s
epoch 64 | loss: 0.30981 | val_0_rmse: 0.62779 | val_1_rmse: 0.63395 |  0:03:27s
epoch 65 | loss: 0.30588 | val_0_rmse: 0.62604 | val_1_rmse: 0.62623 |  0:03:31s
epoch 66 | loss: 0.30702 | val_0_rmse: 0.52693 | val_1_rmse: 0.52915 |  0:03:34s
epoch 67 | loss: 0.2974  | val_0_rmse: 0.57258 | val_1_rmse: 0.58255 |  0:03:37s
epoch 68 | loss: 0.31526 | val_0_rmse: 0.75796 | val_1_rmse: 0.76163 |  0:03:41s
epoch 69 | loss: 0.30544 | val_0_rmse: 0.81661 | val_1_rmse: 0.81915 |  0:03:44s
epoch 70 | loss: 0.30069 | val_0_rmse: 0.60308 | val_1_rmse: 0.60873 |  0:03:48s
epoch 71 | loss: 0.29937 | val_0_rmse: 0.5667  | val_1_rmse: 0.57469 |  0:03:51s
epoch 72 | loss: 0.29679 | val_0_rmse: 0.54632 | val_1_rmse: 0.54895 |  0:03:55s
epoch 73 | loss: 0.29995 | val_0_rmse: 0.73835 | val_1_rmse: 0.73891 |  0:03:58s
epoch 74 | loss: 0.2989  | val_0_rmse: 0.76947 | val_1_rmse: 0.76995 |  0:04:02s
epoch 75 | loss: 0.2959  | val_0_rmse: 0.56889 | val_1_rmse: 0.57775 |  0:04:05s
epoch 76 | loss: 0.29339 | val_0_rmse: 0.7072  | val_1_rmse: 0.71127 |  0:04:09s
epoch 77 | loss: 0.29455 | val_0_rmse: 0.6968  | val_1_rmse: 0.70112 |  0:04:12s
epoch 78 | loss: 0.29358 | val_0_rmse: 0.52191 | val_1_rmse: 0.52975 |  0:04:15s
epoch 79 | loss: 0.29086 | val_0_rmse: 0.5526  | val_1_rmse: 0.56045 |  0:04:18s
epoch 80 | loss: 0.2932  | val_0_rmse: 0.75279 | val_1_rmse: 0.75729 |  0:04:21s
epoch 81 | loss: 0.28762 | val_0_rmse: 0.56122 | val_1_rmse: 0.56179 |  0:04:24s
epoch 82 | loss: 0.29174 | val_0_rmse: 0.6357  | val_1_rmse: 0.6479  |  0:04:27s
epoch 83 | loss: 0.29778 | val_0_rmse: 0.86506 | val_1_rmse: 0.86504 |  0:04:30s
epoch 84 | loss: 0.2997  | val_0_rmse: 0.59187 | val_1_rmse: 0.60116 |  0:04:34s

Early stopping occured at epoch 84 with best_epoch = 54 and best_val_1_rmse = 0.52699
Best weights from best epoch are automatically used!
ended training at: 22:21:04
Feature importance:
[('Area', 0.22716159086975654), ('Baths', 0.19731210436405228), ('Beds', 0.03738254996977328), ('Latitude', 0.3731601041366576), ('Longitude', 0.16474309463902054), ('Month', 0.0), ('Year', 0.00024055602073975643)]
Mean squared error is of 1966913210.0089703
Mean absolute error:32572.813037481497
MAPE:0.2919790955092108
R2 score:0.733454572013029
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:21:05
epoch 0  | loss: 0.62063 | val_0_rmse: 0.70037 | val_1_rmse: 0.71439 |  0:00:03s
epoch 1  | loss: 0.44362 | val_0_rmse: 0.64399 | val_1_rmse: 0.65793 |  0:00:06s
epoch 2  | loss: 0.41848 | val_0_rmse: 0.62935 | val_1_rmse: 0.64739 |  0:00:10s
epoch 3  | loss: 0.4074  | val_0_rmse: 0.67191 | val_1_rmse: 0.68269 |  0:00:13s
epoch 4  | loss: 0.39299 | val_0_rmse: 0.61047 | val_1_rmse: 0.63069 |  0:00:16s
epoch 5  | loss: 0.37889 | val_0_rmse: 0.59011 | val_1_rmse: 0.60756 |  0:00:19s
epoch 6  | loss: 0.35171 | val_0_rmse: 0.97286 | val_1_rmse: 0.98369 |  0:00:23s
epoch 7  | loss: 0.34494 | val_0_rmse: 0.62596 | val_1_rmse: 0.64426 |  0:00:26s
epoch 8  | loss: 0.3675  | val_0_rmse: 0.58762 | val_1_rmse: 0.60763 |  0:00:29s
epoch 9  | loss: 0.35722 | val_0_rmse: 0.63691 | val_1_rmse: 0.65175 |  0:00:33s
epoch 10 | loss: 0.34554 | val_0_rmse: 0.79859 | val_1_rmse: 0.80859 |  0:00:36s
epoch 11 | loss: 0.32472 | val_0_rmse: 0.62678 | val_1_rmse: 0.64097 |  0:00:39s
epoch 12 | loss: 0.31334 | val_0_rmse: 0.59715 | val_1_rmse: 0.6078  |  0:00:42s
epoch 13 | loss: 0.33085 | val_0_rmse: 1.22518 | val_1_rmse: 1.22694 |  0:00:46s
epoch 14 | loss: 0.3176  | val_0_rmse: 0.85057 | val_1_rmse: 0.85177 |  0:00:49s
epoch 15 | loss: 0.29644 | val_0_rmse: 0.69149 | val_1_rmse: 0.71003 |  0:00:52s
epoch 16 | loss: 0.29581 | val_0_rmse: 0.56852 | val_1_rmse: 0.58152 |  0:00:56s
epoch 17 | loss: 0.29053 | val_0_rmse: 0.5495  | val_1_rmse: 0.56388 |  0:00:59s
epoch 18 | loss: 0.28488 | val_0_rmse: 0.8466  | val_1_rmse: 0.84999 |  0:01:03s
epoch 19 | loss: 0.28285 | val_0_rmse: 0.57977 | val_1_rmse: 0.59359 |  0:01:07s
epoch 20 | loss: 0.27857 | val_0_rmse: 0.57753 | val_1_rmse: 0.58964 |  0:01:10s
epoch 21 | loss: 0.27134 | val_0_rmse: 0.56545 | val_1_rmse: 0.57559 |  0:01:13s
epoch 22 | loss: 0.26737 | val_0_rmse: 0.51498 | val_1_rmse: 0.52511 |  0:01:17s
epoch 23 | loss: 0.2719  | val_0_rmse: 0.56159 | val_1_rmse: 0.5703  |  0:01:20s
epoch 24 | loss: 0.25988 | val_0_rmse: 0.5532  | val_1_rmse: 0.5655  |  0:01:23s
epoch 25 | loss: 0.2583  | val_0_rmse: 0.5042  | val_1_rmse: 0.51475 |  0:01:27s
epoch 26 | loss: 0.25708 | val_0_rmse: 0.51956 | val_1_rmse: 0.52926 |  0:01:30s
epoch 27 | loss: 0.26113 | val_0_rmse: 0.55969 | val_1_rmse: 0.56659 |  0:01:33s
epoch 28 | loss: 0.25809 | val_0_rmse: 0.6127  | val_1_rmse: 0.61937 |  0:01:36s
epoch 29 | loss: 0.25678 | val_0_rmse: 0.55044 | val_1_rmse: 0.55967 |  0:01:40s
epoch 30 | loss: 0.26813 | val_0_rmse: 0.63358 | val_1_rmse: 0.64071 |  0:01:43s
epoch 31 | loss: 0.26705 | val_0_rmse: 0.71345 | val_1_rmse: 0.72531 |  0:01:46s
epoch 32 | loss: 0.28827 | val_0_rmse: 0.68402 | val_1_rmse: 0.68498 |  0:01:49s
epoch 33 | loss: 0.26239 | val_0_rmse: 0.85226 | val_1_rmse: 0.85604 |  0:01:52s
epoch 34 | loss: 0.27515 | val_0_rmse: 0.94904 | val_1_rmse: 0.94812 |  0:01:55s
epoch 35 | loss: 0.26313 | val_0_rmse: 0.49755 | val_1_rmse: 0.50969 |  0:01:58s
epoch 36 | loss: 0.25802 | val_0_rmse: 0.58423 | val_1_rmse: 0.58879 |  0:02:01s
epoch 37 | loss: 0.25581 | val_0_rmse: 0.62349 | val_1_rmse: 0.63533 |  0:02:04s
epoch 38 | loss: 0.25178 | val_0_rmse: 0.5443  | val_1_rmse: 0.55564 |  0:02:07s
epoch 39 | loss: 0.24474 | val_0_rmse: 0.52699 | val_1_rmse: 0.53818 |  0:02:10s
epoch 40 | loss: 0.25509 | val_0_rmse: 0.67389 | val_1_rmse: 0.6722  |  0:02:14s
epoch 41 | loss: 0.24058 | val_0_rmse: 0.50655 | val_1_rmse: 0.51581 |  0:02:17s
epoch 42 | loss: 0.23825 | val_0_rmse: 0.54145 | val_1_rmse: 0.55707 |  0:02:20s
epoch 43 | loss: 0.23639 | val_0_rmse: 0.60485 | val_1_rmse: 0.61596 |  0:02:23s
epoch 44 | loss: 0.2392  | val_0_rmse: 0.67144 | val_1_rmse: 0.6856  |  0:02:26s
epoch 45 | loss: 0.23448 | val_0_rmse: 0.49549 | val_1_rmse: 0.50344 |  0:02:29s
epoch 46 | loss: 0.23392 | val_0_rmse: 0.46615 | val_1_rmse: 0.48046 |  0:02:32s
epoch 47 | loss: 0.23332 | val_0_rmse: 0.46886 | val_1_rmse: 0.48003 |  0:02:36s
epoch 48 | loss: 0.22979 | val_0_rmse: 0.55547 | val_1_rmse: 0.56007 |  0:02:39s
epoch 49 | loss: 0.22951 | val_0_rmse: 0.69077 | val_1_rmse: 0.70229 |  0:02:42s
epoch 50 | loss: 0.22497 | val_0_rmse: 0.82377 | val_1_rmse: 0.82053 |  0:02:45s
epoch 51 | loss: 0.23041 | val_0_rmse: 0.57092 | val_1_rmse: 0.57783 |  0:02:48s
epoch 52 | loss: 0.23624 | val_0_rmse: 0.49996 | val_1_rmse: 0.51298 |  0:02:51s
epoch 53 | loss: 0.22761 | val_0_rmse: 0.82029 | val_1_rmse: 0.82854 |  0:02:54s
epoch 54 | loss: 0.21752 | val_0_rmse: 0.46424 | val_1_rmse: 0.47954 |  0:02:57s
epoch 55 | loss: 0.21942 | val_0_rmse: 0.76833 | val_1_rmse: 0.77543 |  0:03:00s
epoch 56 | loss: 0.22164 | val_0_rmse: 0.50163 | val_1_rmse: 0.51095 |  0:03:04s
epoch 57 | loss: 0.22293 | val_0_rmse: 0.45532 | val_1_rmse: 0.47005 |  0:03:07s
epoch 58 | loss: 0.21425 | val_0_rmse: 0.46213 | val_1_rmse: 0.47462 |  0:03:10s
epoch 59 | loss: 0.21687 | val_0_rmse: 0.5208  | val_1_rmse: 0.53025 |  0:03:13s
epoch 60 | loss: 0.21433 | val_0_rmse: 0.69623 | val_1_rmse: 0.70612 |  0:03:16s
epoch 61 | loss: 0.22016 | val_0_rmse: 0.44911 | val_1_rmse: 0.45962 |  0:03:20s
epoch 62 | loss: 0.2272  | val_0_rmse: 0.73186 | val_1_rmse: 0.72681 |  0:03:23s
epoch 63 | loss: 0.21834 | val_0_rmse: 0.89751 | val_1_rmse: 0.89971 |  0:03:26s
epoch 64 | loss: 0.21306 | val_0_rmse: 0.51385 | val_1_rmse: 0.52795 |  0:03:29s
epoch 65 | loss: 0.20956 | val_0_rmse: 0.58026 | val_1_rmse: 0.5955  |  0:03:32s
epoch 66 | loss: 0.20865 | val_0_rmse: 0.58445 | val_1_rmse: 0.58962 |  0:03:35s
epoch 67 | loss: 0.21462 | val_0_rmse: 0.46931 | val_1_rmse: 0.48058 |  0:03:38s
epoch 68 | loss: 0.21122 | val_0_rmse: 0.46553 | val_1_rmse: 0.47696 |  0:03:42s
epoch 69 | loss: 0.20482 | val_0_rmse: 0.53594 | val_1_rmse: 0.54484 |  0:03:45s
epoch 70 | loss: 0.21169 | val_0_rmse: 0.75161 | val_1_rmse: 0.76372 |  0:03:48s
epoch 71 | loss: 0.21197 | val_0_rmse: 0.76484 | val_1_rmse: 0.75991 |  0:03:51s
epoch 72 | loss: 0.21858 | val_0_rmse: 0.50902 | val_1_rmse: 0.52045 |  0:03:54s
epoch 73 | loss: 0.21109 | val_0_rmse: 0.70037 | val_1_rmse: 0.69585 |  0:03:58s
epoch 74 | loss: 0.20905 | val_0_rmse: 0.4774  | val_1_rmse: 0.48892 |  0:04:01s
epoch 75 | loss: 0.20724 | val_0_rmse: 0.44948 | val_1_rmse: 0.4661  |  0:04:04s
epoch 76 | loss: 0.20548 | val_0_rmse: 0.45285 | val_1_rmse: 0.47037 |  0:04:07s
epoch 77 | loss: 0.20976 | val_0_rmse: 0.78156 | val_1_rmse: 0.78034 |  0:04:11s
epoch 78 | loss: 0.2056  | val_0_rmse: 0.41745 | val_1_rmse: 0.43158 |  0:04:14s
epoch 79 | loss: 0.20056 | val_0_rmse: 0.45967 | val_1_rmse: 0.47009 |  0:04:17s
epoch 80 | loss: 0.2059  | val_0_rmse: 0.6414  | val_1_rmse: 0.65096 |  0:04:20s
epoch 81 | loss: 0.20779 | val_0_rmse: 0.42648 | val_1_rmse: 0.43761 |  0:04:23s
epoch 82 | loss: 0.20324 | val_0_rmse: 0.4556  | val_1_rmse: 0.46912 |  0:04:27s
epoch 83 | loss: 0.20221 | val_0_rmse: 0.62971 | val_1_rmse: 0.63133 |  0:04:30s
epoch 84 | loss: 0.20009 | val_0_rmse: 0.82489 | val_1_rmse: 0.83312 |  0:04:33s
epoch 85 | loss: 0.19891 | val_0_rmse: 0.42132 | val_1_rmse: 0.43625 |  0:04:37s
epoch 86 | loss: 0.19446 | val_0_rmse: 0.46038 | val_1_rmse: 0.47403 |  0:04:40s
epoch 87 | loss: 0.19595 | val_0_rmse: 0.51422 | val_1_rmse: 0.52896 |  0:04:43s
epoch 88 | loss: 0.19406 | val_0_rmse: 0.51432 | val_1_rmse: 0.52425 |  0:04:47s
epoch 89 | loss: 0.20556 | val_0_rmse: 0.62155 | val_1_rmse: 0.62032 |  0:04:50s
epoch 90 | loss: 0.19124 | val_0_rmse: 0.68524 | val_1_rmse: 0.69916 |  0:04:54s
epoch 91 | loss: 0.19315 | val_0_rmse: 0.43324 | val_1_rmse: 0.44698 |  0:04:57s
epoch 92 | loss: 0.2023  | val_0_rmse: 0.78786 | val_1_rmse: 0.79612 |  0:05:01s
epoch 93 | loss: 0.19797 | val_0_rmse: 0.46118 | val_1_rmse: 0.47231 |  0:05:04s
epoch 94 | loss: 0.19317 | val_0_rmse: 0.53153 | val_1_rmse: 0.5418  |  0:05:07s
epoch 95 | loss: 0.19638 | val_0_rmse: 0.83929 | val_1_rmse: 0.83778 |  0:05:10s
epoch 96 | loss: 0.19924 | val_0_rmse: 0.93395 | val_1_rmse: 0.93277 |  0:05:14s
epoch 97 | loss: 0.19477 | val_0_rmse: 0.59182 | val_1_rmse: 0.59166 |  0:05:17s
epoch 98 | loss: 0.1932  | val_0_rmse: 0.48934 | val_1_rmse: 0.50034 |  0:05:20s
epoch 99 | loss: 0.19051 | val_0_rmse: 0.9935  | val_1_rmse: 0.98847 |  0:05:23s
epoch 100| loss: 0.19663 | val_0_rmse: 0.52725 | val_1_rmse: 0.53734 |  0:05:26s
epoch 101| loss: 0.1968  | val_0_rmse: 0.48511 | val_1_rmse: 0.49122 |  0:05:29s
epoch 102| loss: 0.18972 | val_0_rmse: 0.39854 | val_1_rmse: 0.41178 |  0:05:33s
epoch 103| loss: 0.18441 | val_0_rmse: 0.45312 | val_1_rmse: 0.46514 |  0:05:36s
epoch 104| loss: 0.1909  | val_0_rmse: 0.5781  | val_1_rmse: 0.59102 |  0:05:39s
epoch 105| loss: 0.19569 | val_0_rmse: 0.4114  | val_1_rmse: 0.4259  |  0:05:42s
epoch 106| loss: 0.19497 | val_0_rmse: 0.54497 | val_1_rmse: 0.55027 |  0:05:46s
epoch 107| loss: 0.20071 | val_0_rmse: 0.4693  | val_1_rmse: 0.47629 |  0:05:49s
epoch 108| loss: 0.19266 | val_0_rmse: 0.70064 | val_1_rmse: 0.71348 |  0:05:52s
epoch 109| loss: 0.18851 | val_0_rmse: 0.94529 | val_1_rmse: 0.94655 |  0:05:55s
epoch 110| loss: 0.18831 | val_0_rmse: 0.75486 | val_1_rmse: 0.75266 |  0:05:59s
epoch 111| loss: 0.18726 | val_0_rmse: 0.45996 | val_1_rmse: 0.47446 |  0:06:02s
epoch 112| loss: 0.18992 | val_0_rmse: 0.55993 | val_1_rmse: 0.57339 |  0:06:05s
epoch 113| loss: 0.19658 | val_0_rmse: 0.42285 | val_1_rmse: 0.43145 |  0:06:08s
epoch 114| loss: 0.19389 | val_0_rmse: 0.60293 | val_1_rmse: 0.60744 |  0:06:12s
epoch 115| loss: 0.18747 | val_0_rmse: 0.55092 | val_1_rmse: 0.56584 |  0:06:15s
epoch 116| loss: 0.18489 | val_0_rmse: 0.47687 | val_1_rmse: 0.49341 |  0:06:18s
epoch 117| loss: 0.1867  | val_0_rmse: 0.67997 | val_1_rmse: 0.68037 |  0:06:21s
epoch 118| loss: 0.18805 | val_0_rmse: 0.46103 | val_1_rmse: 0.47152 |  0:06:24s
epoch 119| loss: 0.18714 | val_0_rmse: 0.43951 | val_1_rmse: 0.45288 |  0:06:27s
epoch 120| loss: 0.18415 | val_0_rmse: 0.39552 | val_1_rmse: 0.40415 |  0:06:30s
epoch 121| loss: 0.18098 | val_0_rmse: 0.83005 | val_1_rmse: 0.83004 |  0:06:34s
epoch 122| loss: 0.18461 | val_0_rmse: 0.47456 | val_1_rmse: 0.48787 |  0:06:37s
epoch 123| loss: 0.18031 | val_0_rmse: 0.52826 | val_1_rmse: 0.54276 |  0:06:40s
epoch 124| loss: 0.17781 | val_0_rmse: 0.59332 | val_1_rmse: 0.60077 |  0:06:43s
epoch 125| loss: 0.18918 | val_0_rmse: 0.4145  | val_1_rmse: 0.4271  |  0:06:47s
epoch 126| loss: 0.18545 | val_0_rmse: 0.68602 | val_1_rmse: 0.68023 |  0:06:50s
epoch 127| loss: 0.18383 | val_0_rmse: 0.51188 | val_1_rmse: 0.51575 |  0:06:54s
epoch 128| loss: 0.17742 | val_0_rmse: 0.63295 | val_1_rmse: 0.64542 |  0:06:57s
epoch 129| loss: 0.17527 | val_0_rmse: 0.45126 | val_1_rmse: 0.46505 |  0:07:00s
epoch 130| loss: 0.1827  | val_0_rmse: 0.41149 | val_1_rmse: 0.42347 |  0:07:04s
epoch 131| loss: 0.19175 | val_0_rmse: 0.45509 | val_1_rmse: 0.46099 |  0:07:07s
epoch 132| loss: 0.17922 | val_0_rmse: 0.69618 | val_1_rmse: 0.70407 |  0:07:11s
epoch 133| loss: 0.18265 | val_0_rmse: 0.9353  | val_1_rmse: 0.93371 |  0:07:14s
epoch 134| loss: 0.18157 | val_0_rmse: 0.55272 | val_1_rmse: 0.55698 |  0:07:17s
epoch 135| loss: 0.18349 | val_0_rmse: 0.42355 | val_1_rmse: 0.4337  |  0:07:21s
epoch 136| loss: 0.17654 | val_0_rmse: 0.45947 | val_1_rmse: 0.47115 |  0:07:24s
epoch 137| loss: 0.17412 | val_0_rmse: 0.72012 | val_1_rmse: 0.72181 |  0:07:27s
epoch 138| loss: 0.17567 | val_0_rmse: 0.48919 | val_1_rmse: 0.49537 |  0:07:31s
epoch 139| loss: 0.18542 | val_0_rmse: 0.4039  | val_1_rmse: 0.41753 |  0:07:34s
epoch 140| loss: 0.17695 | val_0_rmse: 0.60386 | val_1_rmse: 0.60235 |  0:07:38s
epoch 141| loss: 0.18597 | val_0_rmse: 0.40985 | val_1_rmse: 0.41763 |  0:07:41s
epoch 142| loss: 0.17729 | val_0_rmse: 0.49933 | val_1_rmse: 0.50169 |  0:07:44s
epoch 143| loss: 0.18048 | val_0_rmse: 0.42864 | val_1_rmse: 0.44063 |  0:07:48s
epoch 144| loss: 0.17165 | val_0_rmse: 0.42939 | val_1_rmse: 0.44236 |  0:07:51s
epoch 145| loss: 0.17927 | val_0_rmse: 0.62757 | val_1_rmse: 0.62022 |  0:07:55s
epoch 146| loss: 0.1723  | val_0_rmse: 0.43894 | val_1_rmse: 0.44302 |  0:07:58s
epoch 147| loss: 0.18314 | val_0_rmse: 0.48617 | val_1_rmse: 0.49341 |  0:08:01s
epoch 148| loss: 0.17484 | val_0_rmse: 0.71488 | val_1_rmse: 0.72394 |  0:08:05s
epoch 149| loss: 0.16937 | val_0_rmse: 0.74231 | val_1_rmse: 0.73319 |  0:08:08s
Stop training because you reached max_epochs = 150 with best_epoch = 120 and best_val_1_rmse = 0.40415
Best weights from best epoch are automatically used!
ended training at: 22:29:14
Feature importance:
[('Area', 0.42123708005774435), ('Baths', 0.03227055589255007), ('Beds', 0.19206344541556078), ('Latitude', 0.22239238690471266), ('Longitude', 0.12303935538942498), ('Month', 0.0030322413157280502), ('Year', 0.005964935024279101)]
Mean squared error is of 1202799469.8991597
Mean absolute error:23924.102876425684
MAPE:0.2130082166942824
R2 score:0.8351345749672012
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: pe properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:29:15
epoch 0  | loss: 0.61408 | val_0_rmse: 0.79462 | val_1_rmse: 0.7519  |  0:00:03s
epoch 1  | loss: 0.45842 | val_0_rmse: 0.6601  | val_1_rmse: 0.66359 |  0:00:06s
epoch 2  | loss: 0.42788 | val_0_rmse: 0.65368 | val_1_rmse: 0.65639 |  0:00:09s
epoch 3  | loss: 0.38535 | val_0_rmse: 0.90101 | val_1_rmse: 0.90094 |  0:00:13s
epoch 4  | loss: 0.35473 | val_0_rmse: 0.62497 | val_1_rmse: 0.62917 |  0:00:16s
epoch 5  | loss: 0.3404  | val_0_rmse: 0.58349 | val_1_rmse: 0.58366 |  0:00:19s
epoch 6  | loss: 0.31809 | val_0_rmse: 0.66028 | val_1_rmse: 0.6628  |  0:00:23s
epoch 7  | loss: 0.30548 | val_0_rmse: 0.59818 | val_1_rmse: 0.60299 |  0:00:26s
epoch 8  | loss: 0.30076 | val_0_rmse: 0.64895 | val_1_rmse: 0.65524 |  0:00:29s
epoch 9  | loss: 0.30631 | val_0_rmse: 0.77332 | val_1_rmse: 0.77246 |  0:00:33s
epoch 10 | loss: 0.31308 | val_0_rmse: 0.60784 | val_1_rmse: 0.61539 |  0:00:36s
epoch 11 | loss: 0.30828 | val_0_rmse: 0.78143 | val_1_rmse: 0.782   |  0:00:39s
epoch 12 | loss: 0.29233 | val_0_rmse: 0.72798 | val_1_rmse: 0.72992 |  0:00:42s
epoch 13 | loss: 0.29326 | val_0_rmse: 0.62678 | val_1_rmse: 0.63214 |  0:00:46s
epoch 14 | loss: 0.28323 | val_0_rmse: 0.80059 | val_1_rmse: 0.80486 |  0:00:48s
epoch 15 | loss: 0.28384 | val_0_rmse: 0.52386 | val_1_rmse: 0.52958 |  0:00:51s
epoch 16 | loss: 0.27944 | val_0_rmse: 0.54172 | val_1_rmse: 0.55039 |  0:00:54s
epoch 17 | loss: 0.27776 | val_0_rmse: 0.7038  | val_1_rmse: 0.71051 |  0:00:58s
epoch 18 | loss: 0.28463 | val_0_rmse: 0.89412 | val_1_rmse: 0.9     |  0:01:01s
epoch 19 | loss: 0.27856 | val_0_rmse: 0.56298 | val_1_rmse: 0.56909 |  0:01:03s
epoch 20 | loss: 0.2786  | val_0_rmse: 0.54673 | val_1_rmse: 0.56226 |  0:01:06s
epoch 21 | loss: 0.2866  | val_0_rmse: 0.84227 | val_1_rmse: 0.84683 |  0:01:09s
epoch 22 | loss: 0.29538 | val_0_rmse: 0.53025 | val_1_rmse: 0.53884 |  0:01:12s
epoch 23 | loss: 0.28514 | val_0_rmse: 0.77459 | val_1_rmse: 0.77877 |  0:01:16s
epoch 24 | loss: 0.28793 | val_0_rmse: 0.74051 | val_1_rmse: 0.74406 |  0:01:18s
epoch 25 | loss: 0.28405 | val_0_rmse: 0.5167  | val_1_rmse: 0.52315 |  0:01:22s
epoch 26 | loss: 0.27676 | val_0_rmse: 0.7111  | val_1_rmse: 0.71088 |  0:01:24s
epoch 27 | loss: 0.27794 | val_0_rmse: 0.79083 | val_1_rmse: 0.795   |  0:01:28s
epoch 28 | loss: 0.28321 | val_0_rmse: 0.51449 | val_1_rmse: 0.52715 |  0:01:31s
epoch 29 | loss: 0.2742  | val_0_rmse: 0.51294 | val_1_rmse: 0.52161 |  0:01:34s
epoch 30 | loss: 0.27717 | val_0_rmse: 0.54797 | val_1_rmse: 0.55532 |  0:01:38s
epoch 31 | loss: 0.27915 | val_0_rmse: 0.74308 | val_1_rmse: 0.7479  |  0:01:41s
epoch 32 | loss: 0.27484 | val_0_rmse: 0.77104 | val_1_rmse: 0.77553 |  0:01:44s
epoch 33 | loss: 0.26851 | val_0_rmse: 0.58644 | val_1_rmse: 0.58966 |  0:01:47s
epoch 34 | loss: 0.27315 | val_0_rmse: 0.52383 | val_1_rmse: 0.53234 |  0:01:50s
epoch 35 | loss: 0.2723  | val_0_rmse: 0.51707 | val_1_rmse: 0.52926 |  0:01:54s
epoch 36 | loss: 0.27217 | val_0_rmse: 0.51299 | val_1_rmse: 0.51923 |  0:01:57s
epoch 37 | loss: 0.26382 | val_0_rmse: 0.86943 | val_1_rmse: 0.87077 |  0:02:00s
epoch 38 | loss: 0.26447 | val_0_rmse: 0.63972 | val_1_rmse: 0.64547 |  0:02:04s
epoch 39 | loss: 0.26407 | val_0_rmse: 0.51453 | val_1_rmse: 0.52632 |  0:02:07s
epoch 40 | loss: 0.26546 | val_0_rmse: 0.54616 | val_1_rmse: 0.55836 |  0:02:10s
epoch 41 | loss: 0.2664  | val_0_rmse: 0.75098 | val_1_rmse: 0.75947 |  0:02:13s
epoch 42 | loss: 0.25263 | val_0_rmse: 0.53438 | val_1_rmse: 0.54259 |  0:02:17s
epoch 43 | loss: 0.25695 | val_0_rmse: 0.58064 | val_1_rmse: 0.59061 |  0:02:21s
epoch 44 | loss: 0.25247 | val_0_rmse: 0.7581  | val_1_rmse: 0.7658  |  0:02:24s
epoch 45 | loss: 0.2596  | val_0_rmse: 0.55323 | val_1_rmse: 0.56216 |  0:02:28s
epoch 46 | loss: 0.26104 | val_0_rmse: 0.72684 | val_1_rmse: 0.73132 |  0:02:31s
epoch 47 | loss: 0.25968 | val_0_rmse: 0.63344 | val_1_rmse: 0.6415  |  0:02:34s
epoch 48 | loss: 0.26009 | val_0_rmse: 0.62749 | val_1_rmse: 0.63771 |  0:02:37s
epoch 49 | loss: 0.25034 | val_0_rmse: 0.4953  | val_1_rmse: 0.50476 |  0:02:40s
epoch 50 | loss: 0.25714 | val_0_rmse: 0.60658 | val_1_rmse: 0.61224 |  0:02:43s
epoch 51 | loss: 0.26285 | val_0_rmse: 0.73728 | val_1_rmse: 0.74244 |  0:02:46s
epoch 52 | loss: 0.2617  | val_0_rmse: 0.49477 | val_1_rmse: 0.50849 |  0:02:49s
epoch 53 | loss: 0.25413 | val_0_rmse: 0.57372 | val_1_rmse: 0.58286 |  0:02:52s
epoch 54 | loss: 0.25262 | val_0_rmse: 0.5001  | val_1_rmse: 0.51345 |  0:02:55s
epoch 55 | loss: 0.25656 | val_0_rmse: 0.59635 | val_1_rmse: 0.609   |  0:02:58s
epoch 56 | loss: 0.2552  | val_0_rmse: 0.49478 | val_1_rmse: 0.50495 |  0:03:01s
epoch 57 | loss: 0.25002 | val_0_rmse: 0.55034 | val_1_rmse: 0.56044 |  0:03:04s
epoch 58 | loss: 0.24859 | val_0_rmse: 0.83056 | val_1_rmse: 0.83749 |  0:03:07s
epoch 59 | loss: 0.25745 | val_0_rmse: 0.48362 | val_1_rmse: 0.4959  |  0:03:10s
epoch 60 | loss: 0.25188 | val_0_rmse: 0.74303 | val_1_rmse: 0.74926 |  0:03:13s
epoch 61 | loss: 0.24914 | val_0_rmse: 0.63562 | val_1_rmse: 0.64092 |  0:03:17s
epoch 62 | loss: 0.24146 | val_0_rmse: 0.7685  | val_1_rmse: 0.77311 |  0:03:20s
epoch 63 | loss: 0.24692 | val_0_rmse: 0.50666 | val_1_rmse: 0.51273 |  0:03:23s
epoch 64 | loss: 0.24818 | val_0_rmse: 0.55518 | val_1_rmse: 0.56317 |  0:03:27s
epoch 65 | loss: 0.24635 | val_0_rmse: 0.66729 | val_1_rmse: 0.67398 |  0:03:30s
epoch 66 | loss: 0.24497 | val_0_rmse: 0.47431 | val_1_rmse: 0.48471 |  0:03:33s
epoch 67 | loss: 0.24604 | val_0_rmse: 0.5214  | val_1_rmse: 0.53377 |  0:03:36s
epoch 68 | loss: 0.24293 | val_0_rmse: 0.72924 | val_1_rmse: 0.73683 |  0:03:39s
epoch 69 | loss: 0.23844 | val_0_rmse: 0.74126 | val_1_rmse: 0.7464  |  0:03:42s
epoch 70 | loss: 0.25035 | val_0_rmse: 0.8301  | val_1_rmse: 0.84112 |  0:03:46s
epoch 71 | loss: 0.24085 | val_0_rmse: 0.47924 | val_1_rmse: 0.49037 |  0:03:49s
epoch 72 | loss: 0.23948 | val_0_rmse: 0.67112 | val_1_rmse: 0.67406 |  0:03:53s
epoch 73 | loss: 0.248   | val_0_rmse: 0.6999  | val_1_rmse: 0.70615 |  0:03:56s
epoch 74 | loss: 0.24036 | val_0_rmse: 0.47918 | val_1_rmse: 0.48883 |  0:03:59s
epoch 75 | loss: 0.23647 | val_0_rmse: 0.5293  | val_1_rmse: 0.54    |  0:04:02s
epoch 76 | loss: 0.23368 | val_0_rmse: 0.52404 | val_1_rmse: 0.53238 |  0:04:06s
epoch 77 | loss: 0.23607 | val_0_rmse: 0.61438 | val_1_rmse: 0.61954 |  0:04:09s
epoch 78 | loss: 0.239   | val_0_rmse: 0.6451  | val_1_rmse: 0.65082 |  0:04:13s
epoch 79 | loss: 0.23629 | val_0_rmse: 0.48496 | val_1_rmse: 0.49373 |  0:04:17s
epoch 80 | loss: 0.24211 | val_0_rmse: 0.65278 | val_1_rmse: 0.66213 |  0:04:20s
epoch 81 | loss: 0.2384  | val_0_rmse: 0.68267 | val_1_rmse: 0.68876 |  0:04:23s
epoch 82 | loss: 0.24164 | val_0_rmse: 0.64916 | val_1_rmse: 0.65241 |  0:04:26s
epoch 83 | loss: 0.23753 | val_0_rmse: 0.47968 | val_1_rmse: 0.48841 |  0:04:29s
epoch 84 | loss: 0.23674 | val_0_rmse: 0.52649 | val_1_rmse: 0.53171 |  0:04:33s
epoch 85 | loss: 0.23919 | val_0_rmse: 0.49794 | val_1_rmse: 0.5039  |  0:04:36s
epoch 86 | loss: 0.23425 | val_0_rmse: 0.45684 | val_1_rmse: 0.46758 |  0:04:39s
epoch 87 | loss: 0.2306  | val_0_rmse: 0.58471 | val_1_rmse: 0.59222 |  0:04:42s
epoch 88 | loss: 0.23965 | val_0_rmse: 0.47092 | val_1_rmse: 0.48354 |  0:04:45s
epoch 89 | loss: 0.22883 | val_0_rmse: 0.47848 | val_1_rmse: 0.48212 |  0:04:48s
epoch 90 | loss: 0.24026 | val_0_rmse: 0.47022 | val_1_rmse: 0.47593 |  0:04:51s
epoch 91 | loss: 0.25053 | val_0_rmse: 0.50036 | val_1_rmse: 0.5047  |  0:04:55s
epoch 92 | loss: 0.23731 | val_0_rmse: 0.75951 | val_1_rmse: 0.76332 |  0:04:58s
epoch 93 | loss: 0.24088 | val_0_rmse: 0.56322 | val_1_rmse: 0.57326 |  0:05:01s
epoch 94 | loss: 0.23562 | val_0_rmse: 0.45345 | val_1_rmse: 0.46275 |  0:05:04s
epoch 95 | loss: 0.22923 | val_0_rmse: 0.48395 | val_1_rmse: 0.49185 |  0:05:07s
epoch 96 | loss: 0.23231 | val_0_rmse: 0.65443 | val_1_rmse: 0.6579  |  0:05:10s
epoch 97 | loss: 0.2344  | val_0_rmse: 0.72075 | val_1_rmse: 0.72436 |  0:05:13s
epoch 98 | loss: 0.23413 | val_0_rmse: 0.58293 | val_1_rmse: 0.58642 |  0:05:17s
epoch 99 | loss: 0.23013 | val_0_rmse: 0.68324 | val_1_rmse: 0.68163 |  0:05:20s
epoch 100| loss: 0.24371 | val_0_rmse: 0.63106 | val_1_rmse: 0.63931 |  0:05:23s
epoch 101| loss: 0.2388  | val_0_rmse: 0.45512 | val_1_rmse: 0.46497 |  0:05:25s
epoch 102| loss: 0.22923 | val_0_rmse: 0.64719 | val_1_rmse: 0.65421 |  0:05:29s
epoch 103| loss: 0.22731 | val_0_rmse: 0.54372 | val_1_rmse: 0.55194 |  0:05:32s
epoch 104| loss: 0.22519 | val_0_rmse: 0.46251 | val_1_rmse: 0.47336 |  0:05:35s
epoch 105| loss: 0.22392 | val_0_rmse: 0.6537  | val_1_rmse: 0.66065 |  0:05:38s
epoch 106| loss: 0.22108 | val_0_rmse: 0.49108 | val_1_rmse: 0.50417 |  0:05:41s
epoch 107| loss: 0.22736 | val_0_rmse: 0.68875 | val_1_rmse: 0.69522 |  0:05:45s
epoch 108| loss: 0.22601 | val_0_rmse: 0.58117 | val_1_rmse: 0.59416 |  0:05:48s
epoch 109| loss: 0.23167 | val_0_rmse: 0.61857 | val_1_rmse: 0.62634 |  0:05:51s
epoch 110| loss: 0.22183 | val_0_rmse: 0.5097  | val_1_rmse: 0.51887 |  0:05:55s
epoch 111| loss: 0.22552 | val_0_rmse: 0.46549 | val_1_rmse: 0.47335 |  0:05:58s
epoch 112| loss: 0.22065 | val_0_rmse: 0.51689 | val_1_rmse: 0.52415 |  0:06:01s
epoch 113| loss: 0.21848 | val_0_rmse: 0.51251 | val_1_rmse: 0.51749 |  0:06:04s
epoch 114| loss: 0.22231 | val_0_rmse: 0.5216  | val_1_rmse: 0.52969 |  0:06:08s
epoch 115| loss: 0.21915 | val_0_rmse: 0.51542 | val_1_rmse: 0.52246 |  0:06:11s
epoch 116| loss: 0.21654 | val_0_rmse: 0.56327 | val_1_rmse: 0.56683 |  0:06:15s
epoch 117| loss: 0.22688 | val_0_rmse: 0.72597 | val_1_rmse: 0.72612 |  0:06:18s
epoch 118| loss: 0.21702 | val_0_rmse: 0.43269 | val_1_rmse: 0.44496 |  0:06:21s
epoch 119| loss: 0.21297 | val_0_rmse: 0.44578 | val_1_rmse: 0.45329 |  0:06:24s
epoch 120| loss: 0.22718 | val_0_rmse: 0.48731 | val_1_rmse: 0.48621 |  0:06:27s
epoch 121| loss: 0.23156 | val_0_rmse: 0.70373 | val_1_rmse: 0.708   |  0:06:30s
epoch 122| loss: 0.22927 | val_0_rmse: 0.9008  | val_1_rmse: 0.90319 |  0:06:34s
epoch 123| loss: 0.22793 | val_0_rmse: 0.7528  | val_1_rmse: 0.76122 |  0:06:37s
epoch 124| loss: 0.22138 | val_0_rmse: 0.45871 | val_1_rmse: 0.46873 |  0:06:41s
epoch 125| loss: 0.22097 | val_0_rmse: 0.56182 | val_1_rmse: 0.57216 |  0:06:44s
epoch 126| loss: 0.22331 | val_0_rmse: 0.56251 | val_1_rmse: 0.56957 |  0:06:47s
epoch 127| loss: 0.22002 | val_0_rmse: 0.49019 | val_1_rmse: 0.49671 |  0:06:51s
epoch 128| loss: 0.21691 | val_0_rmse: 0.49631 | val_1_rmse: 0.50618 |  0:06:54s
epoch 129| loss: 0.21922 | val_0_rmse: 0.47669 | val_1_rmse: 0.47992 |  0:06:57s
epoch 130| loss: 0.2214  | val_0_rmse: 0.48073 | val_1_rmse: 0.49219 |  0:07:00s
epoch 131| loss: 0.22539 | val_0_rmse: 0.46488 | val_1_rmse: 0.4699  |  0:07:04s
epoch 132| loss: 0.21946 | val_0_rmse: 0.51951 | val_1_rmse: 0.52415 |  0:07:07s
epoch 133| loss: 0.22388 | val_0_rmse: 0.68244 | val_1_rmse: 0.68634 |  0:07:10s
epoch 134| loss: 0.21362 | val_0_rmse: 0.66969 | val_1_rmse: 0.67263 |  0:07:13s
epoch 135| loss: 0.21787 | val_0_rmse: 0.45092 | val_1_rmse: 0.45884 |  0:07:17s
epoch 136| loss: 0.22177 | val_0_rmse: 0.44695 | val_1_rmse: 0.45643 |  0:07:20s
epoch 137| loss: 0.21693 | val_0_rmse: 0.46735 | val_1_rmse: 0.48009 |  0:07:23s
epoch 138| loss: 0.21684 | val_0_rmse: 0.44522 | val_1_rmse: 0.45732 |  0:07:26s
epoch 139| loss: 0.20891 | val_0_rmse: 0.64555 | val_1_rmse: 0.65463 |  0:07:29s
epoch 140| loss: 0.2163  | val_0_rmse: 0.54149 | val_1_rmse: 0.54808 |  0:07:32s
epoch 141| loss: 0.2391  | val_0_rmse: 0.54652 | val_1_rmse: 0.55119 |  0:07:35s
epoch 142| loss: 0.23033 | val_0_rmse: 0.59349 | val_1_rmse: 0.59092 |  0:07:38s
epoch 143| loss: 0.23328 | val_0_rmse: 0.56494 | val_1_rmse: 0.57075 |  0:07:41s
epoch 144| loss: 0.22131 | val_0_rmse: 0.48534 | val_1_rmse: 0.49179 |  0:07:45s
epoch 145| loss: 0.21792 | val_0_rmse: 0.46845 | val_1_rmse: 0.48063 |  0:07:48s
epoch 146| loss: 0.21944 | val_0_rmse: 0.5258  | val_1_rmse: 0.53089 |  0:07:51s
epoch 147| loss: 0.22814 | val_0_rmse: 0.67784 | val_1_rmse: 0.68333 |  0:07:55s
epoch 148| loss: 0.23904 | val_0_rmse: 0.78222 | val_1_rmse: 0.78536 |  0:07:58s

Early stopping occured at epoch 148 with best_epoch = 118 and best_val_1_rmse = 0.44496
Best weights from best epoch are automatically used!
ended training at: 22:37:14
Feature importance:
[('Area', 0.2496503770790991), ('Baths', 0.23022563321440664), ('Beds', 0.0), ('Latitude', 0.42393326098765627), ('Longitude', 0.010905108258661651), ('Month', 0.05742647871750802), ('Year', 0.027859141742668323)]
Mean squared error is of 1433722914.144003
Mean absolute error:26980.283314679084
MAPE:0.23622702497263567
R2 score:0.8041967724394588
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:41:50
epoch 0  | loss: 0.65857 | val_0_rmse: 0.71862 | val_1_rmse: 0.74241 |  0:00:03s
epoch 1  | loss: 0.50867 | val_0_rmse: 0.70113 | val_1_rmse: 0.72274 |  0:00:06s
epoch 2  | loss: 0.49629 | val_0_rmse: 0.69211 | val_1_rmse: 0.71347 |  0:00:09s
epoch 3  | loss: 0.50071 | val_0_rmse: 0.7095  | val_1_rmse: 0.73008 |  0:00:12s
epoch 4  | loss: 0.50364 | val_0_rmse: 0.75444 | val_1_rmse: 0.86394 |  0:00:16s
epoch 5  | loss: 0.50208 | val_0_rmse: 0.73947 | val_1_rmse: 1.05255 |  0:00:19s
epoch 6  | loss: 0.49708 | val_0_rmse: 0.69937 | val_1_rmse: 0.71511 |  0:00:22s
epoch 7  | loss: 0.48541 | val_0_rmse: 0.69206 | val_1_rmse: 0.70716 |  0:00:25s
epoch 8  | loss: 0.48258 | val_0_rmse: 0.69138 | val_1_rmse: 0.71461 |  0:00:29s
epoch 9  | loss: 0.48416 | val_0_rmse: 0.68787 | val_1_rmse: 0.71051 |  0:00:33s
epoch 10 | loss: 0.48345 | val_0_rmse: 0.6849  | val_1_rmse: 0.70461 |  0:00:36s
epoch 11 | loss: 0.47831 | val_0_rmse: 0.68649 | val_1_rmse: 0.70531 |  0:00:40s
epoch 12 | loss: 0.47715 | val_0_rmse: 0.68631 | val_1_rmse: 0.70401 |  0:00:43s
epoch 13 | loss: 0.4785  | val_0_rmse: 0.68441 | val_1_rmse: 0.70372 |  0:00:46s
epoch 14 | loss: 0.47597 | val_0_rmse: 0.67926 | val_1_rmse: 0.69942 |  0:00:50s
epoch 15 | loss: 0.47124 | val_0_rmse: 0.67876 | val_1_rmse: 0.69802 |  0:00:53s
epoch 16 | loss: 0.4723  | val_0_rmse: 0.67937 | val_1_rmse: 0.70353 |  0:00:56s
epoch 17 | loss: 0.46858 | val_0_rmse: 0.69038 | val_1_rmse: 0.71347 |  0:01:00s
epoch 18 | loss: 0.482   | val_0_rmse: 0.68382 | val_1_rmse: 0.70103 |  0:01:03s
epoch 19 | loss: 0.47807 | val_0_rmse: 0.69154 | val_1_rmse: 0.71312 |  0:01:06s
epoch 20 | loss: 0.47778 | val_0_rmse: 0.68768 | val_1_rmse: 0.70666 |  0:01:08s
epoch 21 | loss: 0.47562 | val_0_rmse: 0.68107 | val_1_rmse: 0.70459 |  0:01:11s
epoch 22 | loss: 0.47473 | val_0_rmse: 0.68232 | val_1_rmse: 0.70312 |  0:01:14s
epoch 23 | loss: 0.47226 | val_0_rmse: 0.6829  | val_1_rmse: 0.70428 |  0:01:18s
epoch 24 | loss: 0.47524 | val_0_rmse: 0.68818 | val_1_rmse: 0.70879 |  0:01:21s
epoch 25 | loss: 0.47506 | val_0_rmse: 0.68214 | val_1_rmse: 0.69927 |  0:01:24s
epoch 26 | loss: 0.47607 | val_0_rmse: 0.68919 | val_1_rmse: 0.71094 |  0:01:27s
epoch 27 | loss: 0.47768 | val_0_rmse: 0.69026 | val_1_rmse: 0.71222 |  0:01:30s
epoch 28 | loss: 0.47835 | val_0_rmse: 0.68895 | val_1_rmse: 0.70866 |  0:01:33s
epoch 29 | loss: 0.48558 | val_0_rmse: 0.68977 | val_1_rmse: 0.7099  |  0:01:36s
epoch 30 | loss: 0.48255 | val_0_rmse: 0.69269 | val_1_rmse: 0.71368 |  0:01:39s
epoch 31 | loss: 0.48531 | val_0_rmse: 0.68717 | val_1_rmse: 0.70998 |  0:01:42s
epoch 32 | loss: 0.47712 | val_0_rmse: 0.68179 | val_1_rmse: 0.70314 |  0:01:45s
epoch 33 | loss: 0.47256 | val_0_rmse: 0.68227 | val_1_rmse: 0.70139 |  0:01:48s
epoch 34 | loss: 0.47944 | val_0_rmse: 0.69431 | val_1_rmse: 0.71796 |  0:01:51s
epoch 35 | loss: 0.48302 | val_0_rmse: 0.68845 | val_1_rmse: 0.71078 |  0:01:55s
epoch 36 | loss: 0.47666 | val_0_rmse: 0.68503 | val_1_rmse: 0.70177 |  0:01:58s
epoch 37 | loss: 0.47739 | val_0_rmse: 0.68613 | val_1_rmse: 0.7085  |  0:02:01s
epoch 38 | loss: 0.47429 | val_0_rmse: 0.69384 | val_1_rmse: 0.71641 |  0:02:05s
epoch 39 | loss: 0.47598 | val_0_rmse: 0.68417 | val_1_rmse: 0.70538 |  0:02:08s
epoch 40 | loss: 0.46815 | val_0_rmse: 0.68609 | val_1_rmse: 0.71222 |  0:02:11s
epoch 41 | loss: 0.46709 | val_0_rmse: 0.68514 | val_1_rmse: 0.70742 |  0:02:14s
epoch 42 | loss: 0.46506 | val_0_rmse: 0.67458 | val_1_rmse: 0.69709 |  0:02:17s
epoch 43 | loss: 0.46492 | val_0_rmse: 0.67449 | val_1_rmse: 0.70337 |  0:02:21s
epoch 44 | loss: 0.46732 | val_0_rmse: 0.67646 | val_1_rmse: 0.69965 |  0:02:24s
epoch 45 | loss: 0.45981 | val_0_rmse: 0.67172 | val_1_rmse: 0.69846 |  0:02:27s
epoch 46 | loss: 0.45882 | val_0_rmse: 0.6697  | val_1_rmse: 0.6933  |  0:02:31s
epoch 47 | loss: 0.4581  | val_0_rmse: 0.67117 | val_1_rmse: 0.69523 |  0:02:34s
epoch 48 | loss: 0.45836 | val_0_rmse: 0.67323 | val_1_rmse: 0.69739 |  0:02:37s
epoch 49 | loss: 0.458   | val_0_rmse: 0.66813 | val_1_rmse: 0.69236 |  0:02:40s
epoch 50 | loss: 0.45155 | val_0_rmse: 0.67003 | val_1_rmse: 0.69514 |  0:02:43s
epoch 51 | loss: 0.45493 | val_0_rmse: 0.66816 | val_1_rmse: 0.68873 |  0:02:47s
epoch 52 | loss: 0.45828 | val_0_rmse: 0.66725 | val_1_rmse: 0.68663 |  0:02:50s
epoch 53 | loss: 0.45484 | val_0_rmse: 0.66911 | val_1_rmse: 0.68739 |  0:02:53s
epoch 54 | loss: 0.4502  | val_0_rmse: 0.67154 | val_1_rmse: 0.69392 |  0:02:57s
epoch 55 | loss: 0.463   | val_0_rmse: 0.66684 | val_1_rmse: 0.69036 |  0:03:00s
epoch 56 | loss: 0.45471 | val_0_rmse: 0.67758 | val_1_rmse: 0.6958  |  0:03:04s
epoch 57 | loss: 0.46386 | val_0_rmse: 0.67372 | val_1_rmse: 0.69778 |  0:03:08s
epoch 58 | loss: 0.45433 | val_0_rmse: 0.68248 | val_1_rmse: 0.70982 |  0:03:11s
epoch 59 | loss: 0.46438 | val_0_rmse: 0.66994 | val_1_rmse: 0.69215 |  0:03:14s
epoch 60 | loss: 0.4555  | val_0_rmse: 0.67686 | val_1_rmse: 0.69795 |  0:03:18s
epoch 61 | loss: 0.46138 | val_0_rmse: 0.67713 | val_1_rmse: 0.70139 |  0:03:21s
epoch 62 | loss: 0.45883 | val_0_rmse: 0.68082 | val_1_rmse: 0.69969 |  0:03:25s
epoch 63 | loss: 0.46766 | val_0_rmse: 0.675   | val_1_rmse: 0.69663 |  0:03:28s
epoch 64 | loss: 0.45652 | val_0_rmse: 0.66627 | val_1_rmse: 0.69074 |  0:03:32s
epoch 65 | loss: 0.45245 | val_0_rmse: 0.66361 | val_1_rmse: 0.68236 |  0:03:35s
epoch 66 | loss: 0.45679 | val_0_rmse: 0.67045 | val_1_rmse: 0.68894 |  0:03:38s
epoch 67 | loss: 0.45945 | val_0_rmse: 0.66987 | val_1_rmse: 0.69364 |  0:03:42s
epoch 68 | loss: 0.45069 | val_0_rmse: 0.66409 | val_1_rmse: 0.68429 |  0:03:45s
epoch 69 | loss: 0.44914 | val_0_rmse: 0.66724 | val_1_rmse: 0.69144 |  0:03:48s
epoch 70 | loss: 0.44974 | val_0_rmse: 0.66764 | val_1_rmse: 0.68656 |  0:03:51s
epoch 71 | loss: 0.45687 | val_0_rmse: 0.67197 | val_1_rmse: 0.69272 |  0:03:54s
epoch 72 | loss: 0.45573 | val_0_rmse: 0.68345 | val_1_rmse: 0.70506 |  0:03:58s
epoch 73 | loss: 0.44976 | val_0_rmse: 0.67132 | val_1_rmse: 0.69477 |  0:04:01s
epoch 74 | loss: 0.45931 | val_0_rmse: 0.67169 | val_1_rmse: 0.69613 |  0:04:05s
epoch 75 | loss: 0.45491 | val_0_rmse: 0.66382 | val_1_rmse: 0.69098 |  0:04:08s
epoch 76 | loss: 0.45217 | val_0_rmse: 0.66878 | val_1_rmse: 0.69393 |  0:04:12s
epoch 77 | loss: 0.44781 | val_0_rmse: 0.6647  | val_1_rmse: 0.69382 |  0:04:15s
epoch 78 | loss: 0.45149 | val_0_rmse: 0.66584 | val_1_rmse: 0.68726 |  0:04:19s
epoch 79 | loss: 0.45673 | val_0_rmse: 0.66237 | val_1_rmse: 0.6841  |  0:04:22s
epoch 80 | loss: 0.44966 | val_0_rmse: 0.67174 | val_1_rmse: 0.6952  |  0:04:26s
epoch 81 | loss: 0.45763 | val_0_rmse: 0.67824 | val_1_rmse: 0.70192 |  0:04:29s
epoch 82 | loss: 0.45474 | val_0_rmse: 0.66819 | val_1_rmse: 0.69249 |  0:04:32s
epoch 83 | loss: 0.46236 | val_0_rmse: 0.67938 | val_1_rmse: 0.69695 |  0:04:35s
epoch 84 | loss: 0.45908 | val_0_rmse: 0.68236 | val_1_rmse: 0.69375 |  0:04:39s
epoch 85 | loss: 0.46759 | val_0_rmse: 0.68666 | val_1_rmse: 0.71123 |  0:04:42s
epoch 86 | loss: 0.46051 | val_0_rmse: 0.66617 | val_1_rmse: 0.6904  |  0:04:45s
epoch 87 | loss: 0.45477 | val_0_rmse: 0.66666 | val_1_rmse: 0.68677 |  0:04:48s
epoch 88 | loss: 0.45271 | val_0_rmse: 0.66711 | val_1_rmse: 0.68988 |  0:04:51s
epoch 89 | loss: 0.45062 | val_0_rmse: 0.67132 | val_1_rmse: 0.69451 |  0:04:54s
epoch 90 | loss: 0.46165 | val_0_rmse: 0.6761  | val_1_rmse: 0.69661 |  0:04:58s
epoch 91 | loss: 0.45181 | val_0_rmse: 0.66597 | val_1_rmse: 0.68144 |  0:05:01s
epoch 92 | loss: 0.44954 | val_0_rmse: 0.67661 | val_1_rmse: 0.69881 |  0:05:04s
epoch 93 | loss: 0.45077 | val_0_rmse: 0.66713 | val_1_rmse: 0.69273 |  0:05:07s
epoch 94 | loss: 0.44945 | val_0_rmse: 0.66617 | val_1_rmse: 0.68343 |  0:05:11s
epoch 95 | loss: 0.4561  | val_0_rmse: 0.66409 | val_1_rmse: 0.68343 |  0:05:14s
epoch 96 | loss: 0.4475  | val_0_rmse: 0.65923 | val_1_rmse: 0.68171 |  0:05:17s
epoch 97 | loss: 0.44635 | val_0_rmse: 0.6576  | val_1_rmse: 0.67936 |  0:05:21s
epoch 98 | loss: 0.44236 | val_0_rmse: 0.65857 | val_1_rmse: 0.68086 |  0:05:24s
epoch 99 | loss: 0.44272 | val_0_rmse: 0.67149 | val_1_rmse: 0.69397 |  0:05:27s
epoch 100| loss: 0.45674 | val_0_rmse: 0.67487 | val_1_rmse: 0.70054 |  0:05:30s
epoch 101| loss: 0.44446 | val_0_rmse: 0.65838 | val_1_rmse: 0.68409 |  0:05:33s
epoch 102| loss: 0.44194 | val_0_rmse: 0.65678 | val_1_rmse: 0.68196 |  0:05:36s
epoch 103| loss: 0.44165 | val_0_rmse: 0.65964 | val_1_rmse: 0.68108 |  0:05:39s
epoch 104| loss: 0.44368 | val_0_rmse: 0.66265 | val_1_rmse: 0.6877  |  0:05:42s
epoch 105| loss: 0.45114 | val_0_rmse: 0.66206 | val_1_rmse: 0.68199 |  0:05:45s
epoch 106| loss: 0.44872 | val_0_rmse: 0.66516 | val_1_rmse: 0.68973 |  0:05:49s
epoch 107| loss: 0.44479 | val_0_rmse: 0.66069 | val_1_rmse: 0.67998 |  0:05:52s
epoch 108| loss: 0.44517 | val_0_rmse: 0.6658  | val_1_rmse: 0.68593 |  0:05:57s
epoch 109| loss: 0.44262 | val_0_rmse: 0.6595  | val_1_rmse: 0.68159 |  0:06:01s
epoch 110| loss: 0.44333 | val_0_rmse: 0.65611 | val_1_rmse: 0.67992 |  0:06:05s
epoch 111| loss: 0.44102 | val_0_rmse: 0.65714 | val_1_rmse: 0.67869 |  0:06:08s
epoch 112| loss: 0.44097 | val_0_rmse: 0.65542 | val_1_rmse: 0.67716 |  0:06:12s
epoch 113| loss: 0.44071 | val_0_rmse: 0.66086 | val_1_rmse: 0.68383 |  0:06:15s
epoch 114| loss: 0.43836 | val_0_rmse: 0.65315 | val_1_rmse: 0.67663 |  0:06:19s
epoch 115| loss: 0.43755 | val_0_rmse: 0.66093 | val_1_rmse: 0.68165 |  0:06:22s
epoch 116| loss: 0.43871 | val_0_rmse: 0.65365 | val_1_rmse: 0.6744  |  0:06:25s
epoch 117| loss: 0.43406 | val_0_rmse: 0.65352 | val_1_rmse: 0.676   |  0:06:29s
epoch 118| loss: 0.43655 | val_0_rmse: 0.65652 | val_1_rmse: 0.67684 |  0:06:32s
epoch 119| loss: 0.43709 | val_0_rmse: 0.65437 | val_1_rmse: 0.67741 |  0:06:35s
epoch 120| loss: 0.43982 | val_0_rmse: 0.65271 | val_1_rmse: 0.67536 |  0:06:38s
epoch 121| loss: 0.43454 | val_0_rmse: 0.65218 | val_1_rmse: 0.67533 |  0:06:40s
epoch 122| loss: 0.44135 | val_0_rmse: 0.65866 | val_1_rmse: 0.68295 |  0:06:43s
epoch 123| loss: 0.44248 | val_0_rmse: 0.65438 | val_1_rmse: 0.67342 |  0:06:45s
epoch 124| loss: 0.43482 | val_0_rmse: 0.65537 | val_1_rmse: 0.67659 |  0:06:48s
epoch 125| loss: 0.43378 | val_0_rmse: 0.65159 | val_1_rmse: 0.67026 |  0:06:50s
epoch 126| loss: 0.44195 | val_0_rmse: 0.66607 | val_1_rmse: 0.68949 |  0:06:53s
epoch 127| loss: 0.44087 | val_0_rmse: 0.6595  | val_1_rmse: 0.68295 |  0:06:55s
epoch 128| loss: 0.43987 | val_0_rmse: 0.65784 | val_1_rmse: 0.68029 |  0:06:58s
epoch 129| loss: 0.43713 | val_0_rmse: 0.65112 | val_1_rmse: 0.67517 |  0:07:01s
epoch 130| loss: 0.43359 | val_0_rmse: 0.65251 | val_1_rmse: 0.6751  |  0:07:04s
epoch 131| loss: 0.44284 | val_0_rmse: 0.65614 | val_1_rmse: 0.67843 |  0:07:06s
epoch 132| loss: 0.44305 | val_0_rmse: 0.66888 | val_1_rmse: 0.68511 |  0:07:09s
epoch 133| loss: 0.44213 | val_0_rmse: 0.66912 | val_1_rmse: 0.68724 |  0:07:12s
epoch 134| loss: 0.4391  | val_0_rmse: 0.65564 | val_1_rmse: 0.67625 |  0:07:15s
epoch 135| loss: 0.44092 | val_0_rmse: 0.66127 | val_1_rmse: 0.68533 |  0:07:17s
epoch 136| loss: 0.44361 | val_0_rmse: 0.66461 | val_1_rmse: 0.68653 |  0:07:20s
epoch 137| loss: 0.44315 | val_0_rmse: 0.6672  | val_1_rmse: 0.687   |  0:07:23s
epoch 138| loss: 0.4483  | val_0_rmse: 0.65887 | val_1_rmse: 0.67641 |  0:07:25s
epoch 139| loss: 0.43672 | val_0_rmse: 0.66221 | val_1_rmse: 0.6859  |  0:07:28s
epoch 140| loss: 0.43348 | val_0_rmse: 0.6537  | val_1_rmse: 0.67361 |  0:07:31s
epoch 141| loss: 0.44175 | val_0_rmse: 0.65931 | val_1_rmse: 0.67971 |  0:07:33s
epoch 142| loss: 0.43684 | val_0_rmse: 0.65458 | val_1_rmse: 0.67754 |  0:07:36s
epoch 143| loss: 0.43725 | val_0_rmse: 0.65792 | val_1_rmse: 0.67944 |  0:07:38s
epoch 144| loss: 0.43275 | val_0_rmse: 0.6507  | val_1_rmse: 0.67222 |  0:07:41s
epoch 145| loss: 0.43634 | val_0_rmse: 0.65834 | val_1_rmse: 0.68352 |  0:07:44s
epoch 146| loss: 0.43458 | val_0_rmse: 0.64914 | val_1_rmse: 0.67167 |  0:07:46s
epoch 147| loss: 0.43321 | val_0_rmse: 0.65122 | val_1_rmse: 0.67261 |  0:07:49s
epoch 148| loss: 0.43165 | val_0_rmse: 0.65007 | val_1_rmse: 0.67359 |  0:07:51s
epoch 149| loss: 0.43173 | val_0_rmse: 0.65237 | val_1_rmse: 0.67583 |  0:07:54s
Stop training because you reached max_epochs = 150 with best_epoch = 125 and best_val_1_rmse = 0.67026
Best weights from best epoch are automatically used!
ended training at: 22:49:45
Feature importance:
[('Area', 0.39042912123685275), ('Baths', 0.2783714284312641), ('Beds', 0.08109676884968785), ('Latitude', 0.0), ('Longitude', 0.05444279428102668), ('Month', 0.15370988866286908), ('Year', 0.04194999853829953)]
Mean squared error is of 3633678881.9211736
Mean absolute error:42174.529289176666
MAPE:0.3760230428193429
R2 score:0.5615576498279709
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:49:46
epoch 0  | loss: 0.65664 | val_0_rmse: 0.75774 | val_1_rmse: 0.76435 |  0:00:02s
epoch 1  | loss: 0.50773 | val_0_rmse: 0.70062 | val_1_rmse: 0.72559 |  0:00:05s
epoch 2  | loss: 0.50036 | val_0_rmse: 0.70788 | val_1_rmse: 0.73303 |  0:00:07s
epoch 3  | loss: 0.50004 | val_0_rmse: 0.71318 | val_1_rmse: 0.73751 |  0:00:10s
epoch 4  | loss: 0.51839 | val_0_rmse: 0.71941 | val_1_rmse: 0.73768 |  0:00:13s
epoch 5  | loss: 0.51927 | val_0_rmse: 0.71119 | val_1_rmse: 0.73119 |  0:00:16s
epoch 6  | loss: 0.49993 | val_0_rmse: 0.70124 | val_1_rmse: 0.72554 |  0:00:18s
epoch 7  | loss: 0.50015 | val_0_rmse: 0.70125 | val_1_rmse: 0.72499 |  0:00:21s
epoch 8  | loss: 0.48947 | val_0_rmse: 0.70291 | val_1_rmse: 0.72706 |  0:00:24s
epoch 9  | loss: 0.4875  | val_0_rmse: 0.6974  | val_1_rmse: 0.71588 |  0:00:26s
epoch 10 | loss: 0.4838  | val_0_rmse: 0.68859 | val_1_rmse: 0.70887 |  0:00:29s
epoch 11 | loss: 0.4822  | val_0_rmse: 0.69028 | val_1_rmse: 0.71013 |  0:00:32s
epoch 12 | loss: 0.48408 | val_0_rmse: 0.70362 | val_1_rmse: 0.72359 |  0:00:35s
epoch 13 | loss: 0.48449 | val_0_rmse: 0.69825 | val_1_rmse: 0.72332 |  0:00:37s
epoch 14 | loss: 0.4798  | val_0_rmse: 0.68473 | val_1_rmse: 0.70783 |  0:00:40s
epoch 15 | loss: 0.47678 | val_0_rmse: 0.69391 | val_1_rmse: 0.71869 |  0:00:42s
epoch 16 | loss: 0.47974 | val_0_rmse: 0.69522 | val_1_rmse: 0.71427 |  0:00:45s
epoch 17 | loss: 0.48088 | val_0_rmse: 0.68345 | val_1_rmse: 0.70635 |  0:00:48s
epoch 18 | loss: 0.47777 | val_0_rmse: 0.69007 | val_1_rmse: 0.71495 |  0:00:50s
epoch 19 | loss: 0.48012 | val_0_rmse: 0.68891 | val_1_rmse: 0.71264 |  0:00:53s
epoch 20 | loss: 0.47473 | val_0_rmse: 0.68829 | val_1_rmse: 0.71353 |  0:00:56s
epoch 21 | loss: 0.4783  | val_0_rmse: 0.68212 | val_1_rmse: 0.70454 |  0:00:59s
epoch 22 | loss: 0.47096 | val_0_rmse: 0.6863  | val_1_rmse: 0.70726 |  0:01:01s
epoch 23 | loss: 0.46806 | val_0_rmse: 0.68193 | val_1_rmse: 0.70476 |  0:01:04s
epoch 24 | loss: 0.48143 | val_0_rmse: 0.69233 | val_1_rmse: 0.71327 |  0:01:06s
epoch 25 | loss: 0.48364 | val_0_rmse: 0.69136 | val_1_rmse: 0.7113  |  0:01:09s
epoch 26 | loss: 0.48596 | val_0_rmse: 0.69345 | val_1_rmse: 0.7164  |  0:01:12s
epoch 27 | loss: 0.47997 | val_0_rmse: 0.68511 | val_1_rmse: 0.70571 |  0:01:14s
epoch 28 | loss: 0.47261 | val_0_rmse: 0.6827  | val_1_rmse: 0.7075  |  0:01:17s
epoch 29 | loss: 0.47289 | val_0_rmse: 0.6853  | val_1_rmse: 0.70979 |  0:01:20s
epoch 30 | loss: 0.47355 | val_0_rmse: 0.68959 | val_1_rmse: 0.71003 |  0:01:23s
epoch 31 | loss: 0.47487 | val_0_rmse: 0.68878 | val_1_rmse: 0.70708 |  0:01:25s
epoch 32 | loss: 0.47094 | val_0_rmse: 0.70089 | val_1_rmse: 0.7214  |  0:01:28s
epoch 33 | loss: 0.47246 | val_0_rmse: 0.68236 | val_1_rmse: 0.70491 |  0:01:31s
epoch 34 | loss: 0.49249 | val_0_rmse: 0.70886 | val_1_rmse: 0.71511 |  0:01:33s
epoch 35 | loss: 0.48797 | val_0_rmse: 0.70543 | val_1_rmse: 0.72203 |  0:01:36s
epoch 36 | loss: 0.48322 | val_0_rmse: 0.68873 | val_1_rmse: 0.70834 |  0:01:39s
epoch 37 | loss: 0.47492 | val_0_rmse: 0.68295 | val_1_rmse: 0.70317 |  0:01:41s
epoch 38 | loss: 0.47237 | val_0_rmse: 0.68845 | val_1_rmse: 0.70868 |  0:01:44s
epoch 39 | loss: 0.47445 | val_0_rmse: 0.68549 | val_1_rmse: 0.70349 |  0:01:47s
epoch 40 | loss: 0.47905 | val_0_rmse: 0.68747 | val_1_rmse: 0.70857 |  0:01:49s
epoch 41 | loss: 0.47294 | val_0_rmse: 0.68875 | val_1_rmse: 0.70745 |  0:01:52s
epoch 42 | loss: 0.47792 | val_0_rmse: 0.68726 | val_1_rmse: 0.70719 |  0:01:55s
epoch 43 | loss: 0.47488 | val_0_rmse: 0.68595 | val_1_rmse: 0.70529 |  0:01:57s
epoch 44 | loss: 0.47204 | val_0_rmse: 0.68811 | val_1_rmse: 0.70505 |  0:02:00s
epoch 45 | loss: 0.47716 | val_0_rmse: 0.68302 | val_1_rmse: 0.70395 |  0:02:03s
epoch 46 | loss: 0.47277 | val_0_rmse: 0.68972 | val_1_rmse: 0.70528 |  0:02:06s
epoch 47 | loss: 0.47008 | val_0_rmse: 0.68145 | val_1_rmse: 0.70275 |  0:02:08s
epoch 48 | loss: 0.47149 | val_0_rmse: 0.68672 | val_1_rmse: 0.70615 |  0:02:11s
epoch 49 | loss: 0.47029 | val_0_rmse: 0.67693 | val_1_rmse: 0.69685 |  0:02:14s
epoch 50 | loss: 0.46773 | val_0_rmse: 0.6914  | val_1_rmse: 0.71205 |  0:02:16s
epoch 51 | loss: 0.47261 | val_0_rmse: 0.68565 | val_1_rmse: 0.70211 |  0:02:19s
epoch 52 | loss: 0.47143 | val_0_rmse: 0.67657 | val_1_rmse: 0.6957  |  0:02:21s
epoch 53 | loss: 0.46676 | val_0_rmse: 0.6808  | val_1_rmse: 0.7012  |  0:02:24s
epoch 54 | loss: 0.46821 | val_0_rmse: 0.68437 | val_1_rmse: 0.70421 |  0:02:27s
epoch 55 | loss: 0.46758 | val_0_rmse: 0.67981 | val_1_rmse: 0.69885 |  0:02:29s
epoch 56 | loss: 0.46495 | val_0_rmse: 0.68195 | val_1_rmse: 0.69939 |  0:02:32s
epoch 57 | loss: 0.46645 | val_0_rmse: 0.68153 | val_1_rmse: 0.6994  |  0:02:35s
epoch 58 | loss: 0.46487 | val_0_rmse: 0.6762  | val_1_rmse: 0.69393 |  0:02:37s
epoch 59 | loss: 0.46295 | val_0_rmse: 0.67813 | val_1_rmse: 0.69842 |  0:02:40s
epoch 60 | loss: 0.46274 | val_0_rmse: 0.67791 | val_1_rmse: 0.69534 |  0:02:43s
epoch 61 | loss: 0.46549 | val_0_rmse: 0.67992 | val_1_rmse: 0.69754 |  0:02:45s
epoch 62 | loss: 0.46307 | val_0_rmse: 0.68051 | val_1_rmse: 0.70198 |  0:02:48s
epoch 63 | loss: 0.46162 | val_0_rmse: 0.67853 | val_1_rmse: 0.69655 |  0:02:51s
epoch 64 | loss: 0.46954 | val_0_rmse: 0.68744 | val_1_rmse: 0.71014 |  0:02:53s
epoch 65 | loss: 0.46989 | val_0_rmse: 0.67833 | val_1_rmse: 0.69901 |  0:02:56s
epoch 66 | loss: 0.46427 | val_0_rmse: 0.68067 | val_1_rmse: 0.70203 |  0:02:59s
epoch 67 | loss: 0.46548 | val_0_rmse: 0.68022 | val_1_rmse: 0.6995  |  0:03:02s
epoch 68 | loss: 0.46854 | val_0_rmse: 0.68466 | val_1_rmse: 0.70516 |  0:03:04s
epoch 69 | loss: 0.47153 | val_0_rmse: 0.68314 | val_1_rmse: 0.70105 |  0:03:07s
epoch 70 | loss: 0.47045 | val_0_rmse: 0.68191 | val_1_rmse: 0.70199 |  0:03:10s
epoch 71 | loss: 0.46689 | val_0_rmse: 0.68383 | val_1_rmse: 0.70318 |  0:03:12s
epoch 72 | loss: 0.4665  | val_0_rmse: 0.67622 | val_1_rmse: 0.69721 |  0:03:15s
epoch 73 | loss: 0.46324 | val_0_rmse: 0.68056 | val_1_rmse: 0.70086 |  0:03:18s
epoch 74 | loss: 0.4676  | val_0_rmse: 0.69742 | val_1_rmse: 0.71597 |  0:03:20s
epoch 75 | loss: 0.46517 | val_0_rmse: 0.67665 | val_1_rmse: 0.69809 |  0:03:23s
epoch 76 | loss: 0.46555 | val_0_rmse: 0.68086 | val_1_rmse: 0.69853 |  0:03:26s
epoch 77 | loss: 0.46635 | val_0_rmse: 0.68609 | val_1_rmse: 0.70362 |  0:03:28s
epoch 78 | loss: 0.46886 | val_0_rmse: 0.68117 | val_1_rmse: 0.70058 |  0:03:31s
epoch 79 | loss: 0.47024 | val_0_rmse: 0.67904 | val_1_rmse: 0.69959 |  0:03:34s
epoch 80 | loss: 0.46381 | val_0_rmse: 0.67806 | val_1_rmse: 0.69703 |  0:03:37s
epoch 81 | loss: 0.46555 | val_0_rmse: 0.67714 | val_1_rmse: 0.69622 |  0:03:39s
epoch 82 | loss: 0.46255 | val_0_rmse: 0.67469 | val_1_rmse: 0.69524 |  0:03:42s
epoch 83 | loss: 0.46127 | val_0_rmse: 0.67753 | val_1_rmse: 0.69654 |  0:03:45s
epoch 84 | loss: 0.46569 | val_0_rmse: 0.68439 | val_1_rmse: 0.7058  |  0:03:47s
epoch 85 | loss: 0.46825 | val_0_rmse: 0.67608 | val_1_rmse: 0.69784 |  0:03:50s
epoch 86 | loss: 0.46463 | val_0_rmse: 0.67518 | val_1_rmse: 0.6961  |  0:03:53s
epoch 87 | loss: 0.46285 | val_0_rmse: 0.67628 | val_1_rmse: 0.69698 |  0:03:55s
epoch 88 | loss: 0.46398 | val_0_rmse: 0.67854 | val_1_rmse: 0.69872 |  0:03:58s

Early stopping occured at epoch 88 with best_epoch = 58 and best_val_1_rmse = 0.69393
Best weights from best epoch are automatically used!
ended training at: 22:53:46
Feature importance:
[('Area', 0.40666988297196255), ('Baths', 0.2732360884095251), ('Beds', 0.15743552858207788), ('Latitude', 0.0), ('Longitude', 0.01861773583156594), ('Month', 0.04930976054272361), ('Year', 0.09473100366214493)]
Mean squared error is of 3754222761.7534084
Mean absolute error:42883.03745109599
MAPE:0.3883288375278809
R2 score:0.5539253657310743
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 22:53:46
epoch 0  | loss: 0.65396 | val_0_rmse: 0.7366  | val_1_rmse: 0.74039 |  0:00:02s
epoch 1  | loss: 0.5059  | val_0_rmse: 0.70154 | val_1_rmse: 0.71001 |  0:00:05s
epoch 2  | loss: 0.4913  | val_0_rmse: 0.69131 | val_1_rmse: 0.70089 |  0:00:07s
epoch 3  | loss: 0.48889 | val_0_rmse: 0.69439 | val_1_rmse: 0.70561 |  0:00:10s
epoch 4  | loss: 0.48787 | val_0_rmse: 0.68936 | val_1_rmse: 0.70019 |  0:00:13s
epoch 5  | loss: 0.48563 | val_0_rmse: 0.69499 | val_1_rmse: 0.7068  |  0:00:16s
epoch 6  | loss: 0.48338 | val_0_rmse: 0.69125 | val_1_rmse: 0.70285 |  0:00:18s
epoch 7  | loss: 0.48017 | val_0_rmse: 0.68673 | val_1_rmse: 0.69606 |  0:00:21s
epoch 8  | loss: 0.47753 | val_0_rmse: 0.68888 | val_1_rmse: 0.70235 |  0:00:23s
epoch 9  | loss: 0.47837 | val_0_rmse: 0.68845 | val_1_rmse: 0.70114 |  0:00:26s
epoch 10 | loss: 0.47767 | val_0_rmse: 0.68697 | val_1_rmse: 0.70074 |  0:00:29s
epoch 11 | loss: 0.48126 | val_0_rmse: 0.69326 | val_1_rmse: 0.7069  |  0:00:32s
epoch 12 | loss: 0.47559 | val_0_rmse: 0.68278 | val_1_rmse: 0.69535 |  0:00:34s
epoch 13 | loss: 0.47278 | val_0_rmse: 0.68374 | val_1_rmse: 0.69725 |  0:00:37s
epoch 14 | loss: 0.47147 | val_0_rmse: 0.68651 | val_1_rmse: 0.69897 |  0:00:40s
epoch 15 | loss: 0.47136 | val_0_rmse: 0.67967 | val_1_rmse: 0.69177 |  0:00:42s
epoch 16 | loss: 0.465   | val_0_rmse: 0.67911 | val_1_rmse: 0.69387 |  0:00:45s
epoch 17 | loss: 0.46546 | val_0_rmse: 0.68081 | val_1_rmse: 0.69275 |  0:00:48s
epoch 18 | loss: 0.46804 | val_0_rmse: 0.67633 | val_1_rmse: 0.68944 |  0:00:51s
epoch 19 | loss: 0.4644  | val_0_rmse: 0.68246 | val_1_rmse: 0.69666 |  0:00:53s
epoch 20 | loss: 0.46106 | val_0_rmse: 0.67198 | val_1_rmse: 0.68413 |  0:00:56s
epoch 21 | loss: 0.46176 | val_0_rmse: 0.67257 | val_1_rmse: 0.68979 |  0:00:59s
epoch 22 | loss: 0.45875 | val_0_rmse: 0.67283 | val_1_rmse: 0.68473 |  0:01:01s
epoch 23 | loss: 0.46033 | val_0_rmse: 0.67291 | val_1_rmse: 0.6849  |  0:01:04s
epoch 24 | loss: 0.45838 | val_0_rmse: 0.67349 | val_1_rmse: 0.69048 |  0:01:07s
epoch 25 | loss: 0.4566  | val_0_rmse: 0.6709  | val_1_rmse: 0.68843 |  0:01:10s
epoch 26 | loss: 0.4525  | val_0_rmse: 0.66884 | val_1_rmse: 0.68393 |  0:01:12s
epoch 27 | loss: 0.45584 | val_0_rmse: 0.66966 | val_1_rmse: 0.68673 |  0:01:15s
epoch 28 | loss: 0.45438 | val_0_rmse: 0.67233 | val_1_rmse: 0.68252 |  0:01:18s
epoch 29 | loss: 0.45877 | val_0_rmse: 0.67336 | val_1_rmse: 0.68565 |  0:01:20s
epoch 30 | loss: 0.45902 | val_0_rmse: 0.66651 | val_1_rmse: 0.68107 |  0:01:23s
epoch 31 | loss: 0.45075 | val_0_rmse: 0.66414 | val_1_rmse: 0.67732 |  0:01:26s
epoch 32 | loss: 0.4508  | val_0_rmse: 0.6718  | val_1_rmse: 0.68299 |  0:01:28s
epoch 33 | loss: 0.4518  | val_0_rmse: 0.66565 | val_1_rmse: 0.679   |  0:01:31s
epoch 34 | loss: 0.4533  | val_0_rmse: 0.6699  | val_1_rmse: 0.68169 |  0:01:34s
epoch 35 | loss: 0.45228 | val_0_rmse: 0.66562 | val_1_rmse: 0.67845 |  0:01:36s
epoch 36 | loss: 0.44905 | val_0_rmse: 0.66675 | val_1_rmse: 0.68221 |  0:01:39s
epoch 37 | loss: 0.45097 | val_0_rmse: 0.66233 | val_1_rmse: 0.67809 |  0:01:42s
epoch 38 | loss: 0.45031 | val_0_rmse: 0.66549 | val_1_rmse: 0.68361 |  0:01:45s
epoch 39 | loss: 0.45008 | val_0_rmse: 0.66371 | val_1_rmse: 0.67717 |  0:01:47s
epoch 40 | loss: 0.44758 | val_0_rmse: 0.68424 | val_1_rmse: 0.69713 |  0:01:50s
epoch 41 | loss: 0.45459 | val_0_rmse: 0.6689  | val_1_rmse: 0.68045 |  0:01:53s
epoch 42 | loss: 0.44834 | val_0_rmse: 0.66694 | val_1_rmse: 0.68086 |  0:01:55s
epoch 43 | loss: 0.45023 | val_0_rmse: 0.66784 | val_1_rmse: 0.68644 |  0:01:58s
epoch 44 | loss: 0.44764 | val_0_rmse: 0.66181 | val_1_rmse: 0.67645 |  0:02:01s
epoch 45 | loss: 0.44929 | val_0_rmse: 0.67009 | val_1_rmse: 0.68251 |  0:02:03s
epoch 46 | loss: 0.44903 | val_0_rmse: 0.66615 | val_1_rmse: 0.67717 |  0:02:06s
epoch 47 | loss: 0.44445 | val_0_rmse: 0.65721 | val_1_rmse: 0.67183 |  0:02:09s
epoch 48 | loss: 0.44806 | val_0_rmse: 0.66548 | val_1_rmse: 0.67631 |  0:02:11s
epoch 49 | loss: 0.44614 | val_0_rmse: 0.66291 | val_1_rmse: 0.67782 |  0:02:14s
epoch 50 | loss: 0.44716 | val_0_rmse: 0.66497 | val_1_rmse: 0.67886 |  0:02:17s
epoch 51 | loss: 0.44385 | val_0_rmse: 0.66298 | val_1_rmse: 0.6774  |  0:02:20s
epoch 52 | loss: 0.44686 | val_0_rmse: 0.66561 | val_1_rmse: 0.6769  |  0:02:22s
epoch 53 | loss: 0.44495 | val_0_rmse: 0.66108 | val_1_rmse: 0.67586 |  0:02:25s
epoch 54 | loss: 0.44925 | val_0_rmse: 0.66146 | val_1_rmse: 0.67663 |  0:02:28s
epoch 55 | loss: 0.44757 | val_0_rmse: 0.66989 | val_1_rmse: 0.68254 |  0:02:30s
epoch 56 | loss: 0.44761 | val_0_rmse: 0.66951 | val_1_rmse: 0.68245 |  0:02:33s
epoch 57 | loss: 0.44802 | val_0_rmse: 0.66427 | val_1_rmse: 0.67358 |  0:02:36s
epoch 58 | loss: 0.44733 | val_0_rmse: 0.66184 | val_1_rmse: 0.67578 |  0:02:38s
epoch 59 | loss: 0.45121 | val_0_rmse: 0.67298 | val_1_rmse: 0.68963 |  0:02:41s
epoch 60 | loss: 0.45304 | val_0_rmse: 0.66459 | val_1_rmse: 0.68329 |  0:02:44s
epoch 61 | loss: 0.44466 | val_0_rmse: 0.66137 | val_1_rmse: 0.67821 |  0:02:46s
epoch 62 | loss: 0.44904 | val_0_rmse: 0.66262 | val_1_rmse: 0.67882 |  0:02:49s
epoch 63 | loss: 0.44246 | val_0_rmse: 0.65673 | val_1_rmse: 0.67411 |  0:02:52s
epoch 64 | loss: 0.44363 | val_0_rmse: 0.65968 | val_1_rmse: 0.67196 |  0:02:54s
epoch 65 | loss: 0.44514 | val_0_rmse: 0.65601 | val_1_rmse: 0.67035 |  0:02:57s
epoch 66 | loss: 0.44458 | val_0_rmse: 0.66473 | val_1_rmse: 0.68385 |  0:03:00s
epoch 67 | loss: 0.44418 | val_0_rmse: 0.66403 | val_1_rmse: 0.67683 |  0:03:03s
epoch 68 | loss: 0.44135 | val_0_rmse: 0.6585  | val_1_rmse: 0.67474 |  0:03:05s
epoch 69 | loss: 0.44539 | val_0_rmse: 0.65877 | val_1_rmse: 0.67135 |  0:03:08s
epoch 70 | loss: 0.44281 | val_0_rmse: 0.657   | val_1_rmse: 0.67071 |  0:03:11s
epoch 71 | loss: 0.44272 | val_0_rmse: 0.65688 | val_1_rmse: 0.67083 |  0:03:13s
epoch 72 | loss: 0.44465 | val_0_rmse: 0.65913 | val_1_rmse: 0.66947 |  0:03:16s
epoch 73 | loss: 0.44464 | val_0_rmse: 0.65725 | val_1_rmse: 0.67084 |  0:03:19s
epoch 74 | loss: 0.45066 | val_0_rmse: 0.65761 | val_1_rmse: 0.67328 |  0:03:21s
epoch 75 | loss: 0.4426  | val_0_rmse: 0.66045 | val_1_rmse: 0.67574 |  0:03:24s
epoch 76 | loss: 0.4444  | val_0_rmse: 0.65831 | val_1_rmse: 0.67366 |  0:03:27s
epoch 77 | loss: 0.44236 | val_0_rmse: 0.65337 | val_1_rmse: 0.66898 |  0:03:29s
epoch 78 | loss: 0.44306 | val_0_rmse: 0.6606  | val_1_rmse: 0.67594 |  0:03:32s
epoch 79 | loss: 0.44612 | val_0_rmse: 0.65425 | val_1_rmse: 0.66964 |  0:03:35s
epoch 80 | loss: 0.44392 | val_0_rmse: 0.66424 | val_1_rmse: 0.67577 |  0:03:37s
epoch 81 | loss: 0.44656 | val_0_rmse: 0.65702 | val_1_rmse: 0.67235 |  0:03:40s
epoch 82 | loss: 0.4403  | val_0_rmse: 0.65806 | val_1_rmse: 0.67242 |  0:03:43s
epoch 83 | loss: 0.44671 | val_0_rmse: 0.66067 | val_1_rmse: 0.67666 |  0:03:45s
epoch 84 | loss: 0.44585 | val_0_rmse: 0.65819 | val_1_rmse: 0.67618 |  0:03:48s
epoch 85 | loss: 0.44004 | val_0_rmse: 0.65359 | val_1_rmse: 0.67063 |  0:03:51s
epoch 86 | loss: 0.43666 | val_0_rmse: 0.65557 | val_1_rmse: 0.66935 |  0:03:53s
epoch 87 | loss: 0.44052 | val_0_rmse: 0.65818 | val_1_rmse: 0.67449 |  0:03:56s
epoch 88 | loss: 0.44564 | val_0_rmse: 0.65737 | val_1_rmse: 0.67136 |  0:03:59s
epoch 89 | loss: 0.4379  | val_0_rmse: 0.65361 | val_1_rmse: 0.67009 |  0:04:02s
epoch 90 | loss: 0.44271 | val_0_rmse: 0.65187 | val_1_rmse: 0.66522 |  0:04:04s
epoch 91 | loss: 0.44099 | val_0_rmse: 0.65643 | val_1_rmse: 0.67241 |  0:04:07s
epoch 92 | loss: 0.43786 | val_0_rmse: 0.65901 | val_1_rmse: 0.67515 |  0:04:10s
epoch 93 | loss: 0.44    | val_0_rmse: 0.65583 | val_1_rmse: 0.67419 |  0:04:12s
epoch 94 | loss: 0.43738 | val_0_rmse: 0.65134 | val_1_rmse: 0.66741 |  0:04:15s
epoch 95 | loss: 0.435   | val_0_rmse: 0.65184 | val_1_rmse: 0.66545 |  0:04:18s
epoch 96 | loss: 0.43846 | val_0_rmse: 0.65753 | val_1_rmse: 0.66903 |  0:04:20s
epoch 97 | loss: 0.43869 | val_0_rmse: 0.65708 | val_1_rmse: 0.66838 |  0:04:23s
epoch 98 | loss: 0.44217 | val_0_rmse: 0.64939 | val_1_rmse: 0.66338 |  0:04:26s
epoch 99 | loss: 0.44064 | val_0_rmse: 0.66772 | val_1_rmse: 0.68457 |  0:04:28s
epoch 100| loss: 0.44511 | val_0_rmse: 0.65888 | val_1_rmse: 0.67323 |  0:04:31s
epoch 101| loss: 0.43898 | val_0_rmse: 0.65178 | val_1_rmse: 0.66449 |  0:04:34s
epoch 102| loss: 0.43808 | val_0_rmse: 0.65539 | val_1_rmse: 0.66962 |  0:04:37s
epoch 103| loss: 0.43745 | val_0_rmse: 0.65948 | val_1_rmse: 0.67612 |  0:04:39s
epoch 104| loss: 0.43792 | val_0_rmse: 0.65091 | val_1_rmse: 0.66495 |  0:04:42s
epoch 105| loss: 0.43518 | val_0_rmse: 0.65172 | val_1_rmse: 0.66623 |  0:04:45s
epoch 106| loss: 0.43589 | val_0_rmse: 0.65645 | val_1_rmse: 0.66829 |  0:04:47s
epoch 107| loss: 0.432   | val_0_rmse: 0.65049 | val_1_rmse: 0.66309 |  0:04:50s
epoch 108| loss: 0.43675 | val_0_rmse: 0.65245 | val_1_rmse: 0.66541 |  0:04:53s
epoch 109| loss: 0.44158 | val_0_rmse: 0.65261 | val_1_rmse: 0.66932 |  0:04:55s
epoch 110| loss: 0.43307 | val_0_rmse: 0.65157 | val_1_rmse: 0.66578 |  0:04:58s
epoch 111| loss: 0.43415 | val_0_rmse: 0.65276 | val_1_rmse: 0.66585 |  0:05:01s
epoch 112| loss: 0.4349  | val_0_rmse: 0.65591 | val_1_rmse: 0.67205 |  0:05:03s
epoch 113| loss: 0.44072 | val_0_rmse: 0.65791 | val_1_rmse: 0.67335 |  0:05:06s
epoch 114| loss: 0.43473 | val_0_rmse: 0.6467  | val_1_rmse: 0.66207 |  0:05:09s
epoch 115| loss: 0.43206 | val_0_rmse: 0.64756 | val_1_rmse: 0.66503 |  0:05:11s
epoch 116| loss: 0.43899 | val_0_rmse: 0.65572 | val_1_rmse: 0.67281 |  0:05:14s
epoch 117| loss: 0.44042 | val_0_rmse: 0.65146 | val_1_rmse: 0.66565 |  0:05:17s
epoch 118| loss: 0.43893 | val_0_rmse: 0.65488 | val_1_rmse: 0.6681  |  0:05:19s
epoch 119| loss: 0.43869 | val_0_rmse: 0.65235 | val_1_rmse: 0.66584 |  0:05:22s
epoch 120| loss: 0.43288 | val_0_rmse: 0.64793 | val_1_rmse: 0.66185 |  0:05:25s
epoch 121| loss: 0.4338  | val_0_rmse: 0.67636 | val_1_rmse: 0.68735 |  0:05:28s
epoch 122| loss: 0.43545 | val_0_rmse: 0.64523 | val_1_rmse: 0.66131 |  0:05:31s
epoch 123| loss: 0.42852 | val_0_rmse: 0.65387 | val_1_rmse: 0.66698 |  0:05:34s
epoch 124| loss: 0.43278 | val_0_rmse: 0.64846 | val_1_rmse: 0.66215 |  0:05:36s
epoch 125| loss: 0.43257 | val_0_rmse: 0.64788 | val_1_rmse: 0.66578 |  0:05:39s
epoch 126| loss: 0.44117 | val_0_rmse: 0.66527 | val_1_rmse: 0.68105 |  0:05:42s
epoch 127| loss: 0.44299 | val_0_rmse: 0.65372 | val_1_rmse: 0.66846 |  0:05:45s
epoch 128| loss: 0.43842 | val_0_rmse: 0.65169 | val_1_rmse: 0.67116 |  0:05:47s
epoch 129| loss: 0.43099 | val_0_rmse: 0.64838 | val_1_rmse: 0.66115 |  0:05:50s
epoch 130| loss: 0.43404 | val_0_rmse: 0.64988 | val_1_rmse: 0.66625 |  0:05:53s
epoch 131| loss: 0.43533 | val_0_rmse: 0.64885 | val_1_rmse: 0.66496 |  0:05:56s
epoch 132| loss: 0.43023 | val_0_rmse: 0.64145 | val_1_rmse: 0.65851 |  0:05:59s
epoch 133| loss: 0.42684 | val_0_rmse: 0.64858 | val_1_rmse: 0.66866 |  0:06:02s
epoch 134| loss: 0.43401 | val_0_rmse: 0.64541 | val_1_rmse: 0.66129 |  0:06:04s
epoch 135| loss: 0.4273  | val_0_rmse: 0.64712 | val_1_rmse: 0.6608  |  0:06:07s
epoch 136| loss: 0.43269 | val_0_rmse: 0.6511  | val_1_rmse: 0.66731 |  0:06:10s
epoch 137| loss: 0.43085 | val_0_rmse: 0.64964 | val_1_rmse: 0.67011 |  0:06:13s
epoch 138| loss: 0.43219 | val_0_rmse: 0.64804 | val_1_rmse: 0.66529 |  0:06:16s
epoch 139| loss: 0.42737 | val_0_rmse: 0.64359 | val_1_rmse: 0.65979 |  0:06:19s
epoch 140| loss: 0.43141 | val_0_rmse: 0.65301 | val_1_rmse: 0.66571 |  0:06:22s
epoch 141| loss: 0.43181 | val_0_rmse: 0.64902 | val_1_rmse: 0.66602 |  0:06:24s
epoch 142| loss: 0.42541 | val_0_rmse: 0.64545 | val_1_rmse: 0.66306 |  0:06:27s
epoch 143| loss: 0.42889 | val_0_rmse: 0.64471 | val_1_rmse: 0.66119 |  0:06:30s
epoch 144| loss: 0.42621 | val_0_rmse: 0.64141 | val_1_rmse: 0.65835 |  0:06:33s
epoch 145| loss: 0.42674 | val_0_rmse: 0.64733 | val_1_rmse: 0.6609  |  0:06:36s
epoch 146| loss: 0.42764 | val_0_rmse: 0.64466 | val_1_rmse: 0.66128 |  0:06:39s
epoch 147| loss: 0.42815 | val_0_rmse: 0.64779 | val_1_rmse: 0.66264 |  0:06:41s
epoch 148| loss: 0.43272 | val_0_rmse: 0.64871 | val_1_rmse: 0.66218 |  0:06:44s
epoch 149| loss: 0.42634 | val_0_rmse: 0.64658 | val_1_rmse: 0.66146 |  0:06:47s
Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_val_1_rmse = 0.65835
Best weights from best epoch are automatically used!
ended training at: 23:00:35
Feature importance:
[('Area', 0.37045857651780073), ('Baths', 0.29153087576677256), ('Beds', 0.08795678665025258), ('Latitude', 0.0), ('Longitude', 0.06344532968718611), ('Month', 0.14631245867570788), ('Year', 0.0402959727022802)]
Mean squared error is of 3646148839.4897814
Mean absolute error:42039.094153231556
MAPE:0.393702704930061
R2 score:0.5698810668364438
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:00:36
epoch 0  | loss: 0.64235 | val_0_rmse: 0.72165 | val_1_rmse: 0.72715 |  0:00:02s
epoch 1  | loss: 0.49528 | val_0_rmse: 0.7125  | val_1_rmse: 0.7231  |  0:00:05s
epoch 2  | loss: 0.48852 | val_0_rmse: 0.70759 | val_1_rmse: 0.71804 |  0:00:08s
epoch 3  | loss: 0.484   | val_0_rmse: 0.68615 | val_1_rmse: 0.69442 |  0:00:11s
epoch 4  | loss: 0.476   | val_0_rmse: 0.68593 | val_1_rmse: 0.69403 |  0:00:14s
epoch 5  | loss: 0.47358 | val_0_rmse: 0.70963 | val_1_rmse: 0.71342 |  0:00:17s
epoch 6  | loss: 0.47911 | val_0_rmse: 0.70964 | val_1_rmse: 0.71835 |  0:00:20s
epoch 7  | loss: 0.47634 | val_0_rmse: 0.69934 | val_1_rmse: 0.70292 |  0:00:23s
epoch 8  | loss: 0.48092 | val_0_rmse: 0.68599 | val_1_rmse: 0.69178 |  0:00:26s
epoch 9  | loss: 0.4695  | val_0_rmse: 0.67475 | val_1_rmse: 0.68033 |  0:00:29s
epoch 10 | loss: 0.46613 | val_0_rmse: 0.68035 | val_1_rmse: 0.68614 |  0:00:31s
epoch 11 | loss: 0.47094 | val_0_rmse: 0.67918 | val_1_rmse: 0.68534 |  0:00:35s
epoch 12 | loss: 0.46781 | val_0_rmse: 0.6737  | val_1_rmse: 0.67748 |  0:00:38s
epoch 13 | loss: 0.46771 | val_0_rmse: 0.6806  | val_1_rmse: 0.685   |  0:00:41s
epoch 14 | loss: 0.46937 | val_0_rmse: 0.6791  | val_1_rmse: 0.68261 |  0:00:43s
epoch 15 | loss: 0.46407 | val_0_rmse: 0.67873 | val_1_rmse: 0.68337 |  0:00:46s
epoch 16 | loss: 0.46638 | val_0_rmse: 0.67836 | val_1_rmse: 0.68647 |  0:00:49s
epoch 17 | loss: 0.46349 | val_0_rmse: 0.67365 | val_1_rmse: 0.67846 |  0:00:51s
epoch 18 | loss: 0.46277 | val_0_rmse: 0.67743 | val_1_rmse: 0.68559 |  0:00:54s
epoch 19 | loss: 0.4619  | val_0_rmse: 0.67995 | val_1_rmse: 0.68649 |  0:00:57s
epoch 20 | loss: 0.46231 | val_0_rmse: 0.6729  | val_1_rmse: 0.68042 |  0:01:00s
epoch 21 | loss: 0.4601  | val_0_rmse: 0.68641 | val_1_rmse: 0.70198 |  0:01:03s
epoch 22 | loss: 0.46471 | val_0_rmse: 0.6762  | val_1_rmse: 1.93955 |  0:01:05s
epoch 23 | loss: 0.46162 | val_0_rmse: 0.67275 | val_1_rmse: 0.67911 |  0:01:08s
epoch 24 | loss: 0.46075 | val_0_rmse: 0.67577 | val_1_rmse: 0.67989 |  0:01:11s
epoch 25 | loss: 0.46025 | val_0_rmse: 0.69043 | val_1_rmse: 1.72302 |  0:01:15s
epoch 26 | loss: 0.46598 | val_0_rmse: 0.67976 | val_1_rmse: 0.68543 |  0:01:18s
epoch 27 | loss: 0.47379 | val_0_rmse: 0.68904 | val_1_rmse: 0.69356 |  0:01:21s
epoch 28 | loss: 0.47678 | val_0_rmse: 0.68648 | val_1_rmse: 0.69575 |  0:01:25s
epoch 29 | loss: 0.47463 | val_0_rmse: 0.68112 | val_1_rmse: 0.68889 |  0:01:29s
epoch 30 | loss: 0.46907 | val_0_rmse: 0.67703 | val_1_rmse: 0.68843 |  0:01:32s
epoch 31 | loss: 0.46365 | val_0_rmse: 0.68117 | val_1_rmse: 0.68964 |  0:01:35s
epoch 32 | loss: 0.46493 | val_0_rmse: 0.67776 | val_1_rmse: 0.68561 |  0:01:39s
epoch 33 | loss: 0.46835 | val_0_rmse: 0.69378 | val_1_rmse: 0.69486 |  0:01:42s
epoch 34 | loss: 0.46776 | val_0_rmse: 0.68467 | val_1_rmse: 0.68672 |  0:01:45s
epoch 35 | loss: 0.46501 | val_0_rmse: 0.67714 | val_1_rmse: 0.68513 |  0:01:49s
epoch 36 | loss: 0.46279 | val_0_rmse: 0.68676 | val_1_rmse: 0.69175 |  0:01:52s
epoch 37 | loss: 0.47911 | val_0_rmse: 0.69081 | val_1_rmse: 0.6944  |  0:01:55s
epoch 38 | loss: 0.47708 | val_0_rmse: 0.69182 | val_1_rmse: 0.69731 |  0:01:59s
epoch 39 | loss: 0.47586 | val_0_rmse: 0.70043 | val_1_rmse: 0.70506 |  0:02:02s
epoch 40 | loss: 0.49669 | val_0_rmse: 0.69843 | val_1_rmse: 0.70751 |  0:02:05s
epoch 41 | loss: 0.483   | val_0_rmse: 0.69774 | val_1_rmse: 0.7037  |  0:02:08s
epoch 42 | loss: 0.48411 | val_0_rmse: 0.69741 | val_1_rmse: 0.70576 |  0:02:11s

Early stopping occured at epoch 42 with best_epoch = 12 and best_val_1_rmse = 0.67748
Best weights from best epoch are automatically used!
ended training at: 23:02:49
Feature importance:
[('Area', 0.4364440957852792), ('Baths', 0.15525836008707664), ('Beds', 0.062146995989440665), ('Latitude', 0.1550205648684732), ('Longitude', 0.06010189310637434), ('Month', 0.04876534381490678), ('Year', 0.08226274634844917)]
Mean squared error is of 4007935809.288321
Mean absolute error:44135.31179726298
MAPE:0.39761467821231267
R2 score:0.5246191370808106
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: uy properties.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:02:50
epoch 0  | loss: 0.63513 | val_0_rmse: 0.70566 | val_1_rmse: 0.70919 |  0:00:03s
epoch 1  | loss: 0.4991  | val_0_rmse: 0.69409 | val_1_rmse: 0.69983 |  0:00:07s
epoch 2  | loss: 0.49531 | val_0_rmse: 0.703   | val_1_rmse: 0.70639 |  0:00:10s
epoch 3  | loss: 0.50006 | val_0_rmse: 0.69786 | val_1_rmse: 0.70014 |  0:00:14s
epoch 4  | loss: 0.49456 | val_0_rmse: 0.69596 | val_1_rmse: 0.69805 |  0:00:17s
epoch 5  | loss: 0.49102 | val_0_rmse: 0.69375 | val_1_rmse: 0.69645 |  0:00:20s
epoch 6  | loss: 0.48369 | val_0_rmse: 0.69028 | val_1_rmse: 0.6965  |  0:00:24s
epoch 7  | loss: 0.47988 | val_0_rmse: 0.69425 | val_1_rmse: 0.69941 |  0:00:27s
epoch 8  | loss: 0.47775 | val_0_rmse: 0.68616 | val_1_rmse: 0.69262 |  0:00:30s
epoch 9  | loss: 0.47844 | val_0_rmse: 0.6854  | val_1_rmse: 0.69131 |  0:00:33s
epoch 10 | loss: 0.47553 | val_0_rmse: 0.70101 | val_1_rmse: 0.70472 |  0:00:36s
epoch 11 | loss: 0.4849  | val_0_rmse: 0.68674 | val_1_rmse: 0.69126 |  0:00:39s
epoch 12 | loss: 0.47293 | val_0_rmse: 0.68361 | val_1_rmse: 0.68974 |  0:00:42s
epoch 13 | loss: 0.47067 | val_0_rmse: 0.68275 | val_1_rmse: 0.68925 |  0:00:45s
epoch 14 | loss: 0.47121 | val_0_rmse: 0.68845 | val_1_rmse: 0.69474 |  0:00:48s
epoch 15 | loss: 0.47854 | val_0_rmse: 0.68211 | val_1_rmse: 0.68794 |  0:00:52s
epoch 16 | loss: 0.47138 | val_0_rmse: 0.68214 | val_1_rmse: 0.6899  |  0:00:55s
epoch 17 | loss: 0.47324 | val_0_rmse: 0.68378 | val_1_rmse: 0.69132 |  0:00:58s
epoch 18 | loss: 0.4727  | val_0_rmse: 0.68997 | val_1_rmse: 0.70006 |  0:01:02s
epoch 19 | loss: 0.47064 | val_0_rmse: 0.67965 | val_1_rmse: 0.68533 |  0:01:05s
epoch 20 | loss: 0.46542 | val_0_rmse: 0.6784  | val_1_rmse: 0.68294 |  0:01:09s
epoch 21 | loss: 0.47024 | val_0_rmse: 0.67956 | val_1_rmse: 0.68453 |  0:01:12s
epoch 22 | loss: 0.46859 | val_0_rmse: 0.67974 | val_1_rmse: 0.68644 |  0:01:16s
epoch 23 | loss: 0.46776 | val_0_rmse: 0.69468 | val_1_rmse: 0.68759 |  0:01:20s
epoch 24 | loss: 0.46653 | val_0_rmse: 0.70501 | val_1_rmse: 0.68276 |  0:01:24s
epoch 25 | loss: 0.46251 | val_0_rmse: 0.67926 | val_1_rmse: 0.68285 |  0:01:27s
epoch 26 | loss: 0.46477 | val_0_rmse: 0.67295 | val_1_rmse: 0.67914 |  0:01:31s
epoch 27 | loss: 0.46273 | val_0_rmse: 0.67792 | val_1_rmse: 0.68559 |  0:01:35s
epoch 28 | loss: 0.46542 | val_0_rmse: 0.6748  | val_1_rmse: 0.68217 |  0:01:38s
epoch 29 | loss: 0.46444 | val_0_rmse: 0.68281 | val_1_rmse: 0.69104 |  0:01:41s
epoch 30 | loss: 0.46646 | val_0_rmse: 0.68341 | val_1_rmse: 0.68956 |  0:01:45s
epoch 31 | loss: 0.47187 | val_0_rmse: 0.6801  | val_1_rmse: 0.68629 |  0:01:48s
epoch 32 | loss: 0.45957 | val_0_rmse: 0.68427 | val_1_rmse: 0.6771  |  0:01:51s
epoch 33 | loss: 0.45861 | val_0_rmse: 0.67316 | val_1_rmse: 0.68093 |  0:01:55s
epoch 34 | loss: 0.45798 | val_0_rmse: 0.67327 | val_1_rmse: 0.68162 |  0:01:58s
epoch 35 | loss: 0.45745 | val_0_rmse: 0.67383 | val_1_rmse: 0.67958 |  0:02:01s
epoch 36 | loss: 0.45777 | val_0_rmse: 0.67473 | val_1_rmse: 0.68338 |  0:02:05s
epoch 37 | loss: 0.46086 | val_0_rmse: 0.67222 | val_1_rmse: 0.68008 |  0:02:08s
epoch 38 | loss: 0.45697 | val_0_rmse: 0.66855 | val_1_rmse: 0.67501 |  0:02:11s
epoch 39 | loss: 0.46306 | val_0_rmse: 0.68181 | val_1_rmse: 0.69237 |  0:02:14s
epoch 40 | loss: 0.46565 | val_0_rmse: 0.67554 | val_1_rmse: 0.68122 |  0:02:17s
epoch 41 | loss: 0.46045 | val_0_rmse: 0.67304 | val_1_rmse: 0.67997 |  0:02:20s
epoch 42 | loss: 0.45629 | val_0_rmse: 0.67087 | val_1_rmse: 0.67943 |  0:02:24s
epoch 43 | loss: 0.46196 | val_0_rmse: 0.67255 | val_1_rmse: 0.68101 |  0:02:27s
epoch 44 | loss: 0.45932 | val_0_rmse: 0.67184 | val_1_rmse: 0.68205 |  0:02:30s
epoch 45 | loss: 0.45389 | val_0_rmse: 0.66667 | val_1_rmse: 0.67565 |  0:02:33s
epoch 46 | loss: 0.45274 | val_0_rmse: 0.67529 | val_1_rmse: 0.68424 |  0:02:36s
epoch 47 | loss: 0.4584  | val_0_rmse: 0.67131 | val_1_rmse: 0.68058 |  0:02:39s
epoch 48 | loss: 0.45644 | val_0_rmse: 0.67738 | val_1_rmse: 0.68058 |  0:02:42s
epoch 49 | loss: 0.45272 | val_0_rmse: 0.66826 | val_1_rmse: 0.67512 |  0:02:45s
epoch 50 | loss: 0.45303 | val_0_rmse: 0.6672  | val_1_rmse: 0.67359 |  0:02:49s
epoch 51 | loss: 0.45554 | val_0_rmse: 0.6691  | val_1_rmse: 0.67724 |  0:02:53s
epoch 52 | loss: 0.45367 | val_0_rmse: 0.6775  | val_1_rmse: 0.68328 |  0:02:57s
epoch 53 | loss: 0.45529 | val_0_rmse: 0.66815 | val_1_rmse: 0.67601 |  0:03:00s
epoch 54 | loss: 0.456   | val_0_rmse: 0.6703  | val_1_rmse: 0.6794  |  0:03:04s
epoch 55 | loss: 0.45784 | val_0_rmse: 0.66909 | val_1_rmse: 0.67913 |  0:03:07s
epoch 56 | loss: 0.45589 | val_0_rmse: 0.66873 | val_1_rmse: 0.67509 |  0:03:10s
epoch 57 | loss: 0.4518  | val_0_rmse: 0.67076 | val_1_rmse: 0.67896 |  0:03:13s
epoch 58 | loss: 0.45405 | val_0_rmse: 0.67108 | val_1_rmse: 0.67955 |  0:03:17s
epoch 59 | loss: 0.45452 | val_0_rmse: 0.66905 | val_1_rmse: 0.67877 |  0:03:20s
epoch 60 | loss: 0.45403 | val_0_rmse: 0.6681  | val_1_rmse: 0.6756  |  0:03:23s
epoch 61 | loss: 0.45526 | val_0_rmse: 0.66557 | val_1_rmse: 0.67544 |  0:03:26s
epoch 62 | loss: 0.45419 | val_0_rmse: 0.67247 | val_1_rmse: 0.68046 |  0:03:29s
epoch 63 | loss: 0.4535  | val_0_rmse: 0.69182 | val_1_rmse: 0.7012  |  0:03:33s
epoch 64 | loss: 0.4703  | val_0_rmse: 0.67086 | val_1_rmse: 0.67886 |  0:03:37s
epoch 65 | loss: 0.45404 | val_0_rmse: 0.67292 | val_1_rmse: 0.67859 |  0:03:40s
epoch 66 | loss: 0.45381 | val_0_rmse: 0.66569 | val_1_rmse: 0.67289 |  0:03:44s
epoch 67 | loss: 0.45131 | val_0_rmse: 0.67201 | val_1_rmse: 0.68114 |  0:03:49s
epoch 68 | loss: 0.45414 | val_0_rmse: 0.66509 | val_1_rmse: 0.67296 |  0:03:52s
epoch 69 | loss: 0.45341 | val_0_rmse: 0.67082 | val_1_rmse: 0.67761 |  0:03:56s
epoch 70 | loss: 0.45209 | val_0_rmse: 0.66999 | val_1_rmse: 0.67885 |  0:04:00s
epoch 71 | loss: 0.45887 | val_0_rmse: 0.67415 | val_1_rmse: 0.68095 |  0:04:03s
epoch 72 | loss: 0.45944 | val_0_rmse: 0.67347 | val_1_rmse: 0.68274 |  0:04:06s
epoch 73 | loss: 0.44947 | val_0_rmse: 0.66426 | val_1_rmse: 0.67352 |  0:04:09s
epoch 74 | loss: 0.45353 | val_0_rmse: 0.66629 | val_1_rmse: 0.6735  |  0:04:13s
epoch 75 | loss: 0.44936 | val_0_rmse: 0.66501 | val_1_rmse: 0.67221 |  0:04:16s
epoch 76 | loss: 0.44574 | val_0_rmse: 0.66223 | val_1_rmse: 0.6709  |  0:04:19s
epoch 77 | loss: 0.44315 | val_0_rmse: 0.66006 | val_1_rmse: 0.66974 |  0:04:22s
epoch 78 | loss: 0.4421  | val_0_rmse: 0.65476 | val_1_rmse: 0.6646  |  0:04:26s
epoch 79 | loss: 0.4424  | val_0_rmse: 0.66701 | val_1_rmse: 0.67539 |  0:04:29s
epoch 80 | loss: 0.45328 | val_0_rmse: 0.66213 | val_1_rmse: 0.66889 |  0:04:33s
epoch 81 | loss: 0.44622 | val_0_rmse: 0.65396 | val_1_rmse: 0.66301 |  0:04:36s
epoch 82 | loss: 0.44271 | val_0_rmse: 0.66183 | val_1_rmse: 0.67141 |  0:04:42s
epoch 83 | loss: 0.44402 | val_0_rmse: 0.65942 | val_1_rmse: 0.66856 |  0:04:46s
epoch 84 | loss: 0.44221 | val_0_rmse: 0.65571 | val_1_rmse: 0.66537 |  0:04:49s
epoch 85 | loss: 0.44249 | val_0_rmse: 0.66342 | val_1_rmse: 0.66968 |  0:04:53s
epoch 86 | loss: 0.4427  | val_0_rmse: 0.65507 | val_1_rmse: 0.66676 |  0:04:56s
epoch 87 | loss: 0.44557 | val_0_rmse: 0.65926 | val_1_rmse: 0.66848 |  0:05:00s
epoch 88 | loss: 0.46023 | val_0_rmse: 0.6723  | val_1_rmse: 0.68046 |  0:05:04s
epoch 89 | loss: 0.46413 | val_0_rmse: 0.67968 | val_1_rmse: 0.6918  |  0:05:07s
epoch 90 | loss: 0.46203 | val_0_rmse: 0.67014 | val_1_rmse: 0.67768 |  0:05:10s
epoch 91 | loss: 0.45365 | val_0_rmse: 0.66841 | val_1_rmse: 0.6772  |  0:05:15s
epoch 92 | loss: 0.45304 | val_0_rmse: 0.66533 | val_1_rmse: 0.67414 |  0:05:18s
epoch 93 | loss: 0.44951 | val_0_rmse: 0.66214 | val_1_rmse: 0.67385 |  0:05:22s
epoch 94 | loss: 0.44749 | val_0_rmse: 0.65935 | val_1_rmse: 0.67153 |  0:05:25s
epoch 95 | loss: 0.45242 | val_0_rmse: 0.66148 | val_1_rmse: 0.67291 |  0:05:29s
epoch 96 | loss: 0.44166 | val_0_rmse: 0.65314 | val_1_rmse: 0.66524 |  0:05:32s
epoch 97 | loss: 0.43921 | val_0_rmse: 0.65267 | val_1_rmse: 0.66337 |  0:05:35s
epoch 98 | loss: 0.4389  | val_0_rmse: 0.65643 | val_1_rmse: 0.66639 |  0:05:39s
epoch 99 | loss: 0.44395 | val_0_rmse: 0.65445 | val_1_rmse: 0.66212 |  0:05:42s
epoch 100| loss: 0.44399 | val_0_rmse: 0.65956 | val_1_rmse: 0.66912 |  0:05:45s
epoch 101| loss: 0.43642 | val_0_rmse: 0.65005 | val_1_rmse: 0.66276 |  0:05:48s
epoch 102| loss: 0.43869 | val_0_rmse: 0.65275 | val_1_rmse: 0.66336 |  0:05:51s
epoch 103| loss: 0.43746 | val_0_rmse: 0.65607 | val_1_rmse: 0.66289 |  0:05:55s
epoch 104| loss: 0.4341  | val_0_rmse: 0.64982 | val_1_rmse: 0.66141 |  0:05:58s
epoch 105| loss: 0.4386  | val_0_rmse: 0.65189 | val_1_rmse: 0.66344 |  0:06:01s
epoch 106| loss: 0.43472 | val_0_rmse: 0.65402 | val_1_rmse: 0.66463 |  0:06:05s
epoch 107| loss: 0.43555 | val_0_rmse: 0.65062 | val_1_rmse: 0.66233 |  0:06:08s
epoch 108| loss: 0.4367  | val_0_rmse: 0.64958 | val_1_rmse: 0.66502 |  0:06:11s
epoch 109| loss: 0.43171 | val_0_rmse: 0.64548 | val_1_rmse: 0.65626 |  0:06:14s
epoch 110| loss: 0.43441 | val_0_rmse: 0.65081 | val_1_rmse: 0.66204 |  0:06:18s
epoch 111| loss: 0.43465 | val_0_rmse: 0.65023 | val_1_rmse: 0.66204 |  0:06:21s
epoch 112| loss: 0.43437 | val_0_rmse: 0.6479  | val_1_rmse: 0.65994 |  0:06:25s
epoch 113| loss: 0.43169 | val_0_rmse: 0.64409 | val_1_rmse: 0.65529 |  0:06:29s
epoch 114| loss: 0.43016 | val_0_rmse: 0.64846 | val_1_rmse: 0.65795 |  0:06:32s
epoch 115| loss: 0.42806 | val_0_rmse: 0.65184 | val_1_rmse: 0.66493 |  0:06:36s
epoch 116| loss: 0.4325  | val_0_rmse: 0.64731 | val_1_rmse: 0.65865 |  0:06:39s
epoch 117| loss: 0.42948 | val_0_rmse: 0.65994 | val_1_rmse: 0.66997 |  0:06:42s
epoch 118| loss: 0.43577 | val_0_rmse: 0.64301 | val_1_rmse: 0.65364 |  0:06:45s
epoch 119| loss: 0.42767 | val_0_rmse: 0.64583 | val_1_rmse: 0.65838 |  0:06:48s
epoch 120| loss: 0.43079 | val_0_rmse: 0.64463 | val_1_rmse: 0.65595 |  0:06:52s
epoch 121| loss: 0.42955 | val_0_rmse: 0.64085 | val_1_rmse: 0.65365 |  0:06:55s
epoch 122| loss: 0.42614 | val_0_rmse: 0.64932 | val_1_rmse: 0.66119 |  0:06:58s
epoch 123| loss: 0.42317 | val_0_rmse: 0.638   | val_1_rmse: 0.65498 |  0:07:01s
epoch 124| loss: 0.41908 | val_0_rmse: 0.64434 | val_1_rmse: 0.6566  |  0:07:04s
epoch 125| loss: 0.42799 | val_0_rmse: 0.64214 | val_1_rmse: 0.65396 |  0:07:08s
epoch 126| loss: 0.42617 | val_0_rmse: 0.64979 | val_1_rmse: 0.66336 |  0:07:11s
epoch 127| loss: 0.4293  | val_0_rmse: 0.65356 | val_1_rmse: 0.6717  |  0:07:14s
epoch 128| loss: 0.43656 | val_0_rmse: 0.65351 | val_1_rmse: 0.66645 |  0:07:17s
epoch 129| loss: 0.43195 | val_0_rmse: 0.64722 | val_1_rmse: 0.65755 |  0:07:20s
epoch 130| loss: 0.43383 | val_0_rmse: 0.64788 | val_1_rmse: 0.66352 |  0:07:23s
epoch 131| loss: 0.4366  | val_0_rmse: 0.67084 | val_1_rmse: 0.68145 |  0:07:26s
epoch 132| loss: 0.4294  | val_0_rmse: 0.65141 | val_1_rmse: 0.66825 |  0:07:29s
epoch 133| loss: 0.42322 | val_0_rmse: 0.65758 | val_1_rmse: 0.67029 |  0:07:32s
epoch 134| loss: 0.43021 | val_0_rmse: 0.63973 | val_1_rmse: 0.65401 |  0:07:35s
epoch 135| loss: 0.43133 | val_0_rmse: 0.64677 | val_1_rmse: 0.65749 |  0:07:38s
epoch 136| loss: 0.42845 | val_0_rmse: 0.66395 | val_1_rmse: 0.67681 |  0:07:42s
epoch 137| loss: 0.42789 | val_0_rmse: 0.6567  | val_1_rmse: 0.67005 |  0:07:46s
epoch 138| loss: 0.4217  | val_0_rmse: 0.63548 | val_1_rmse: 0.64914 |  0:07:49s
epoch 139| loss: 0.41747 | val_0_rmse: 0.63374 | val_1_rmse: 0.64798 |  0:07:52s
epoch 140| loss: 0.41796 | val_0_rmse: 0.62872 | val_1_rmse: 0.64745 |  0:07:56s
epoch 141| loss: 0.41759 | val_0_rmse: 0.63379 | val_1_rmse: 0.64944 |  0:07:59s
epoch 142| loss: 0.42051 | val_0_rmse: 0.65468 | val_1_rmse: 0.66581 |  0:08:03s
epoch 143| loss: 0.42005 | val_0_rmse: 0.6353  | val_1_rmse: 0.64486 |  0:08:06s
epoch 144| loss: 0.41761 | val_0_rmse: 0.63901 | val_1_rmse: 0.6499  |  0:08:09s
epoch 145| loss: 0.41503 | val_0_rmse: 0.64302 | val_1_rmse: 0.64961 |  0:08:13s
epoch 146| loss: 0.42575 | val_0_rmse: 0.64197 | val_1_rmse: 0.65356 |  0:08:16s
epoch 147| loss: 0.41975 | val_0_rmse: 0.63541 | val_1_rmse: 0.65003 |  0:08:19s
epoch 148| loss: 0.41919 | val_0_rmse: 0.65314 | val_1_rmse: 0.66747 |  0:08:22s
epoch 149| loss: 0.43392 | val_0_rmse: 0.66165 | val_1_rmse: 0.6844  |  0:08:26s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.64486
Best weights from best epoch are automatically used!
ended training at: 23:11:17
Feature importance:
[('Area', 0.43121517853365565), ('Baths', 0.2439732193795263), ('Beds', 0.13540717835230984), ('Latitude', 0.0), ('Longitude', 0.0), ('Month', 0.014230456340874359), ('Year', 0.17517396739363386)]
Mean squared error is of 3502635844.289603
Mean absolute error:41355.01810424587
MAPE:0.390770686418577
R2 score:0.5823460544248957
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:11:18
epoch 0  | loss: 0.41909 | val_0_rmse: 0.57681 | val_1_rmse: 0.58393 |  0:00:06s
epoch 1  | loss: 0.33958 | val_0_rmse: 0.58366 | val_1_rmse: 0.59039 |  0:00:12s
epoch 2  | loss: 0.33575 | val_0_rmse: 0.56416 | val_1_rmse: 0.57116 |  0:00:17s
epoch 3  | loss: 0.32829 | val_0_rmse: 0.56941 | val_1_rmse: 0.5761  |  0:00:23s
epoch 4  | loss: 0.32887 | val_0_rmse: 0.579   | val_1_rmse: 0.58536 |  0:00:29s
epoch 5  | loss: 0.33011 | val_0_rmse: 0.56942 | val_1_rmse: 0.57042 |  0:00:35s
epoch 6  | loss: 0.31744 | val_0_rmse: 0.62061 | val_1_rmse: 0.59373 |  0:00:41s
epoch 7  | loss: 0.31481 | val_0_rmse: 0.57235 | val_1_rmse: 0.57259 |  0:00:47s
epoch 8  | loss: 0.31074 | val_0_rmse: 0.59586 | val_1_rmse: 0.59415 |  0:00:52s
epoch 9  | loss: 0.30689 | val_0_rmse: 0.57382 | val_1_rmse: 0.5711  |  0:00:58s
epoch 10 | loss: 0.30616 | val_0_rmse: 0.70757 | val_1_rmse: 0.70198 |  0:01:06s
epoch 11 | loss: 0.30283 | val_0_rmse: 0.62716 | val_1_rmse: 0.6307  |  0:01:13s
epoch 12 | loss: 0.29707 | val_0_rmse: 0.5297  | val_1_rmse: 0.53666 |  0:01:19s
epoch 13 | loss: 0.28935 | val_0_rmse: 0.56215 | val_1_rmse: 0.57173 |  0:01:25s
epoch 14 | loss: 0.28788 | val_0_rmse: 0.55552 | val_1_rmse: 0.5673  |  0:01:32s
epoch 15 | loss: 0.30739 | val_0_rmse: 0.5844  | val_1_rmse: 0.59358 |  0:01:38s
epoch 16 | loss: 0.29231 | val_0_rmse: 0.55742 | val_1_rmse: 0.56574 |  0:01:45s
epoch 17 | loss: 0.29059 | val_0_rmse: 0.61001 | val_1_rmse: 0.6182  |  0:01:52s
epoch 18 | loss: 0.28804 | val_0_rmse: 0.64319 | val_1_rmse: 0.65437 |  0:01:59s
epoch 19 | loss: 0.28449 | val_0_rmse: 0.52708 | val_1_rmse: 0.53681 |  0:02:05s
epoch 20 | loss: 0.28207 | val_0_rmse: 0.56179 | val_1_rmse: 0.5733  |  0:02:11s
epoch 21 | loss: 0.28084 | val_0_rmse: 0.64015 | val_1_rmse: 0.6503  |  0:02:18s
epoch 22 | loss: 0.27854 | val_0_rmse: 0.52247 | val_1_rmse: 0.53287 |  0:02:25s
epoch 23 | loss: 0.2891  | val_0_rmse: 0.57778 | val_1_rmse: 0.58632 |  0:02:31s
epoch 24 | loss: 0.29147 | val_0_rmse: 0.54243 | val_1_rmse: 0.55268 |  0:02:37s
epoch 25 | loss: 0.28671 | val_0_rmse: 0.6748  | val_1_rmse: 0.68521 |  0:02:44s
epoch 26 | loss: 0.28369 | val_0_rmse: 0.57733 | val_1_rmse: 0.58651 |  0:02:51s
epoch 27 | loss: 0.28135 | val_0_rmse: 0.58825 | val_1_rmse: 0.59669 |  0:02:56s
epoch 28 | loss: 0.28374 | val_0_rmse: 0.52631 | val_1_rmse: 0.53755 |  0:03:03s
epoch 29 | loss: 0.27985 | val_0_rmse: 0.54018 | val_1_rmse: 0.54933 |  0:03:10s
epoch 30 | loss: 0.28391 | val_0_rmse: 0.54157 | val_1_rmse: 0.55157 |  0:03:16s
epoch 31 | loss: 0.28345 | val_0_rmse: 0.56064 | val_1_rmse: 0.57072 |  0:03:22s
epoch 32 | loss: 0.27979 | val_0_rmse: 0.5262  | val_1_rmse: 0.53683 |  0:03:29s
epoch 33 | loss: 0.27887 | val_0_rmse: 0.52653 | val_1_rmse: 0.53668 |  0:03:35s
epoch 34 | loss: 0.27501 | val_0_rmse: 0.54392 | val_1_rmse: 0.55457 |  0:03:42s
epoch 35 | loss: 0.27913 | val_0_rmse: 0.5235  | val_1_rmse: 0.53322 |  0:03:49s
epoch 36 | loss: 0.28049 | val_0_rmse: 0.54094 | val_1_rmse: 0.55083 |  0:03:57s
epoch 37 | loss: 0.27675 | val_0_rmse: 0.52441 | val_1_rmse: 0.53535 |  0:04:05s
epoch 38 | loss: 0.2767  | val_0_rmse: 0.53025 | val_1_rmse: 0.54089 |  0:04:12s
epoch 39 | loss: 0.27634 | val_0_rmse: 0.52847 | val_1_rmse: 0.53871 |  0:04:19s
epoch 40 | loss: 0.27734 | val_0_rmse: 0.52594 | val_1_rmse: 0.5357  |  0:04:26s
epoch 41 | loss: 0.27711 | val_0_rmse: 0.51986 | val_1_rmse: 0.53034 |  0:04:33s
epoch 42 | loss: 0.27604 | val_0_rmse: 0.52601 | val_1_rmse: 0.53709 |  0:04:40s
epoch 43 | loss: 0.27641 | val_0_rmse: 0.70161 | val_1_rmse: 0.71517 |  0:04:47s
epoch 44 | loss: 0.27591 | val_0_rmse: 0.5285  | val_1_rmse: 0.53923 |  0:04:54s
epoch 45 | loss: 0.27617 | val_0_rmse: 0.51882 | val_1_rmse: 0.5293  |  0:05:00s
epoch 46 | loss: 0.27475 | val_0_rmse: 0.54706 | val_1_rmse: 0.55786 |  0:05:07s
epoch 47 | loss: 0.27674 | val_0_rmse: 0.57259 | val_1_rmse: 0.58157 |  0:05:13s
epoch 48 | loss: 0.27458 | val_0_rmse: 0.55988 | val_1_rmse: 0.56973 |  0:05:20s
epoch 49 | loss: 0.27422 | val_0_rmse: 0.514   | val_1_rmse: 0.52409 |  0:05:27s
epoch 50 | loss: 0.27399 | val_0_rmse: 0.59179 | val_1_rmse: 0.601   |  0:05:34s
epoch 51 | loss: 0.27484 | val_0_rmse: 0.55082 | val_1_rmse: 0.56102 |  0:05:41s
epoch 52 | loss: 0.27484 | val_0_rmse: 0.5265  | val_1_rmse: 0.53713 |  0:05:47s
epoch 53 | loss: 0.27315 | val_0_rmse: 0.54292 | val_1_rmse: 0.55333 |  0:05:54s
epoch 54 | loss: 0.27147 | val_0_rmse: 0.52792 | val_1_rmse: 0.53848 |  0:06:00s
epoch 55 | loss: 0.27307 | val_0_rmse: 0.55604 | val_1_rmse: 0.56742 |  0:06:06s
epoch 56 | loss: 0.27359 | val_0_rmse: 0.51945 | val_1_rmse: 0.53094 |  0:06:11s
epoch 57 | loss: 0.27469 | val_0_rmse: 0.57014 | val_1_rmse: 0.58024 |  0:06:18s
epoch 58 | loss: 0.27392 | val_0_rmse: 0.5221  | val_1_rmse: 0.53349 |  0:06:25s
epoch 59 | loss: 0.27408 | val_0_rmse: 0.5413  | val_1_rmse: 0.55284 |  0:06:32s
epoch 60 | loss: 0.27254 | val_0_rmse: 0.54357 | val_1_rmse: 0.55491 |  0:06:38s
epoch 61 | loss: 0.27258 | val_0_rmse: 0.55142 | val_1_rmse: 0.563   |  0:06:45s
epoch 62 | loss: 0.27332 | val_0_rmse: 0.5123  | val_1_rmse: 0.5249  |  0:06:51s
epoch 63 | loss: 0.27212 | val_0_rmse: 0.53314 | val_1_rmse: 0.54541 |  0:06:57s
epoch 64 | loss: 0.27228 | val_0_rmse: 0.57025 | val_1_rmse: 0.58139 |  0:07:05s
epoch 65 | loss: 0.272   | val_0_rmse: 0.51251 | val_1_rmse: 0.52395 |  0:07:11s
epoch 66 | loss: 0.27059 | val_0_rmse: 0.51693 | val_1_rmse: 0.52841 |  0:07:18s
epoch 67 | loss: 0.2721  | val_0_rmse: 0.59575 | val_1_rmse: 0.60566 |  0:07:25s
epoch 68 | loss: 0.27379 | val_0_rmse: 0.52467 | val_1_rmse: 0.53663 |  0:07:32s
epoch 69 | loss: 0.27035 | val_0_rmse: 0.52259 | val_1_rmse: 0.53344 |  0:07:39s
epoch 70 | loss: 0.27228 | val_0_rmse: 0.51915 | val_1_rmse: 0.52844 |  0:07:45s
epoch 71 | loss: 0.27058 | val_0_rmse: 0.54183 | val_1_rmse: 0.55343 |  0:07:52s
epoch 72 | loss: 0.27037 | val_0_rmse: 0.53205 | val_1_rmse: 0.54193 |  0:07:58s
epoch 73 | loss: 0.26999 | val_0_rmse: 0.51677 | val_1_rmse: 0.52747 |  0:08:05s
epoch 74 | loss: 0.26966 | val_0_rmse: 0.51181 | val_1_rmse: 0.5233  |  0:08:11s
epoch 75 | loss: 0.27102 | val_0_rmse: 0.52011 | val_1_rmse: 0.53117 |  0:08:18s
epoch 76 | loss: 0.27283 | val_0_rmse: 0.54678 | val_1_rmse: 0.55794 |  0:08:24s
epoch 77 | loss: 0.27352 | val_0_rmse: 0.52076 | val_1_rmse: 0.53219 |  0:08:31s
epoch 78 | loss: 0.27139 | val_0_rmse: 0.51835 | val_1_rmse: 0.5275  |  0:08:38s
epoch 79 | loss: 0.2726  | val_0_rmse: 0.53849 | val_1_rmse: 0.54916 |  0:08:44s
epoch 80 | loss: 0.27221 | val_0_rmse: 0.51986 | val_1_rmse: 0.53126 |  0:08:51s
epoch 81 | loss: 0.27143 | val_0_rmse: 0.52666 | val_1_rmse: 0.5382  |  0:08:58s
epoch 82 | loss: 0.27419 | val_0_rmse: 0.73904 | val_1_rmse: 0.75132 |  0:09:05s
epoch 83 | loss: 0.27584 | val_0_rmse: 0.52202 | val_1_rmse: 0.53085 |  0:09:11s
epoch 84 | loss: 0.27666 | val_0_rmse: 0.53607 | val_1_rmse: 0.54701 |  0:09:18s
epoch 85 | loss: 0.27418 | val_0_rmse: 0.53835 | val_1_rmse: 0.54917 |  0:09:25s
epoch 86 | loss: 0.2728  | val_0_rmse: 0.58887 | val_1_rmse: 0.59762 |  0:09:31s
epoch 87 | loss: 0.27807 | val_0_rmse: 0.5233  | val_1_rmse: 0.53625 |  0:09:39s
epoch 88 | loss: 0.27618 | val_0_rmse: 0.51601 | val_1_rmse: 0.52786 |  0:09:45s
epoch 89 | loss: 0.27039 | val_0_rmse: 0.58486 | val_1_rmse: 0.59443 |  0:09:51s
epoch 90 | loss: 0.27157 | val_0_rmse: 0.76028 | val_1_rmse: 0.7739  |  0:09:58s
epoch 91 | loss: 0.27262 | val_0_rmse: 0.61986 | val_1_rmse: 0.63063 |  0:10:05s
epoch 92 | loss: 0.26926 | val_0_rmse: 0.5143  | val_1_rmse: 0.52703 |  0:10:11s
epoch 93 | loss: 0.27004 | val_0_rmse: 0.52596 | val_1_rmse: 0.53803 |  0:10:16s
epoch 94 | loss: 0.28007 | val_0_rmse: 0.53543 | val_1_rmse: 0.54588 |  0:10:22s
epoch 95 | loss: 0.28059 | val_0_rmse: 0.52905 | val_1_rmse: 0.53877 |  0:10:28s
epoch 96 | loss: 0.27639 | val_0_rmse: 0.54835 | val_1_rmse: 0.55699 |  0:10:34s
epoch 97 | loss: 0.2767  | val_0_rmse: 0.54559 | val_1_rmse: 0.55599 |  0:10:39s
epoch 98 | loss: 0.27521 | val_0_rmse: 0.52012 | val_1_rmse: 0.53083 |  0:10:45s
epoch 99 | loss: 0.27312 | val_0_rmse: 0.51671 | val_1_rmse: 0.52741 |  0:10:52s
epoch 100| loss: 0.27108 | val_0_rmse: 0.5241  | val_1_rmse: 0.53659 |  0:10:58s
epoch 101| loss: 0.27233 | val_0_rmse: 0.52185 | val_1_rmse: 0.53159 |  0:11:05s
epoch 102| loss: 0.27072 | val_0_rmse: 0.55495 | val_1_rmse: 0.56631 |  0:11:11s
epoch 103| loss: 0.27036 | val_0_rmse: 0.60947 | val_1_rmse: 0.62353 |  0:11:18s
epoch 104| loss: 0.27085 | val_0_rmse: 0.51114 | val_1_rmse: 0.52228 |  0:11:24s
epoch 105| loss: 0.27388 | val_0_rmse: 0.55276 | val_1_rmse: 0.56264 |  0:11:31s
epoch 106| loss: 0.27034 | val_0_rmse: 0.58413 | val_1_rmse: 0.59336 |  0:11:37s
epoch 107| loss: 0.26915 | val_0_rmse: 0.51636 | val_1_rmse: 0.52726 |  0:11:43s
epoch 108| loss: 0.26847 | val_0_rmse: 0.5314  | val_1_rmse: 0.54218 |  0:11:50s
epoch 109| loss: 0.2669  | val_0_rmse: 0.52853 | val_1_rmse: 0.53968 |  0:11:56s
epoch 110| loss: 0.2694  | val_0_rmse: 0.52675 | val_1_rmse: 0.53787 |  0:12:03s
epoch 111| loss: 0.27022 | val_0_rmse: 0.5373  | val_1_rmse: 0.54897 |  0:12:10s
epoch 112| loss: 0.27057 | val_0_rmse: 0.50963 | val_1_rmse: 0.52065 |  0:12:16s
epoch 113| loss: 0.27059 | val_0_rmse: 0.51055 | val_1_rmse: 0.52156 |  0:12:22s
epoch 114| loss: 0.26741 | val_0_rmse: 0.51446 | val_1_rmse: 0.52604 |  0:12:29s
epoch 115| loss: 0.26606 | val_0_rmse: 0.52824 | val_1_rmse: 0.53652 |  0:12:35s
epoch 116| loss: 0.26806 | val_0_rmse: 0.55811 | val_1_rmse: 0.56969 |  0:12:41s
epoch 117| loss: 0.2722  | val_0_rmse: 0.53896 | val_1_rmse: 0.54972 |  0:12:47s
epoch 118| loss: 0.26897 | val_0_rmse: 0.52516 | val_1_rmse: 0.53496 |  0:12:53s
epoch 119| loss: 0.26843 | val_0_rmse: 0.50957 | val_1_rmse: 0.52065 |  0:12:59s
epoch 120| loss: 0.26737 | val_0_rmse: 0.59522 | val_1_rmse: 0.60433 |  0:13:05s
epoch 121| loss: 0.26887 | val_0_rmse: 0.52845 | val_1_rmse: 0.53946 |  0:13:12s
epoch 122| loss: 0.26811 | val_0_rmse: 0.59663 | val_1_rmse: 0.60575 |  0:13:18s
epoch 123| loss: 0.26716 | val_0_rmse: 0.66653 | val_1_rmse: 0.68201 |  0:13:25s
epoch 124| loss: 0.26681 | val_0_rmse: 0.50886 | val_1_rmse: 0.52124 |  0:13:32s
epoch 125| loss: 0.26564 | val_0_rmse: 0.55002 | val_1_rmse: 0.56455 |  0:13:39s
epoch 126| loss: 0.26707 | val_0_rmse: 0.55205 | val_1_rmse: 0.56201 |  0:13:46s
epoch 127| loss: 0.26737 | val_0_rmse: 0.54301 | val_1_rmse: 0.55578 |  0:13:52s
epoch 128| loss: 0.26726 | val_0_rmse: 0.54376 | val_1_rmse: 0.55586 |  0:13:58s
epoch 129| loss: 0.27026 | val_0_rmse: 0.59817 | val_1_rmse: 0.60741 |  0:14:04s
epoch 130| loss: 0.26562 | val_0_rmse: 0.51196 | val_1_rmse: 0.52304 |  0:14:10s
epoch 131| loss: 0.26551 | val_0_rmse: 0.50569 | val_1_rmse: 0.51744 |  0:14:17s
epoch 132| loss: 0.26719 | val_0_rmse: 0.62226 | val_1_rmse: 0.63276 |  0:14:24s
epoch 133| loss: 0.26492 | val_0_rmse: 0.51065 | val_1_rmse: 0.52216 |  0:14:31s
epoch 134| loss: 0.27401 | val_0_rmse: 0.55061 | val_1_rmse: 0.56135 |  0:14:37s
epoch 135| loss: 0.26927 | val_0_rmse: 0.51639 | val_1_rmse: 0.52794 |  0:14:43s
epoch 136| loss: 0.27086 | val_0_rmse: 0.5315  | val_1_rmse: 0.5425  |  0:14:49s
epoch 137| loss: 0.26697 | val_0_rmse: 0.53693 | val_1_rmse: 0.54704 |  0:14:54s
epoch 138| loss: 0.2664  | val_0_rmse: 0.57898 | val_1_rmse: 0.58927 |  0:15:00s
epoch 139| loss: 0.27318 | val_0_rmse: 0.51672 | val_1_rmse: 0.5274  |  0:15:06s
epoch 140| loss: 0.27313 | val_0_rmse: 0.51355 | val_1_rmse: 0.52328 |  0:15:12s
epoch 141| loss: 0.27097 | val_0_rmse: 0.52462 | val_1_rmse: 0.53513 |  0:15:18s
epoch 142| loss: 0.26944 | val_0_rmse: 0.52874 | val_1_rmse: 0.54026 |  0:15:25s
epoch 143| loss: 0.26713 | val_0_rmse: 0.50638 | val_1_rmse: 0.51566 |  0:15:32s
epoch 144| loss: 0.27177 | val_0_rmse: 0.58674 | val_1_rmse: 0.59432 |  0:15:38s
epoch 145| loss: 0.26669 | val_0_rmse: 0.51222 | val_1_rmse: 0.52317 |  0:15:45s
epoch 146| loss: 0.26563 | val_0_rmse: 0.53115 | val_1_rmse: 0.54253 |  0:15:51s
epoch 147| loss: 0.26553 | val_0_rmse: 0.55894 | val_1_rmse: 0.56873 |  0:15:57s
epoch 148| loss: 0.26815 | val_0_rmse: 0.51965 | val_1_rmse: 0.52945 |  0:16:04s
epoch 149| loss: 0.26623 | val_0_rmse: 0.58522 | val_1_rmse: 0.59305 |  0:16:10s
Stop training because you reached max_epochs = 150 with best_epoch = 143 and best_val_1_rmse = 0.51566
Best weights from best epoch are automatically used!
ended training at: 23:27:31
Feature importance:
[('Area', 0.6020309687633606), ('Baths', 0.07845051007870427), ('Beds', 0.16986207661733652), ('Latitude', 0.0), ('Longitude', 0.14965644454059865), ('Month', 0.0), ('Year', 0.0)]
Mean squared error is of 869983341.7427256
Mean absolute error:20626.77669025044
MAPE:0.3551916757936825
R2 score:0.7374017180729827
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:27:33
epoch 0  | loss: 0.42482 | val_0_rmse: 0.57251 | val_1_rmse: 0.56789 |  0:00:06s
epoch 1  | loss: 0.33316 | val_0_rmse: 0.57056 | val_1_rmse: 0.56522 |  0:00:13s
epoch 2  | loss: 0.33024 | val_0_rmse: 0.56502 | val_1_rmse: 0.56029 |  0:00:20s
epoch 3  | loss: 0.32821 | val_0_rmse: 0.57426 | val_1_rmse: 0.56869 |  0:00:26s
epoch 4  | loss: 0.32895 | val_0_rmse: 0.58071 | val_1_rmse: 0.57404 |  0:00:32s
epoch 5  | loss: 0.3272  | val_0_rmse: 0.56622 | val_1_rmse: 0.56299 |  0:00:39s
epoch 6  | loss: 0.32323 | val_0_rmse: 0.56172 | val_1_rmse: 0.55615 |  0:00:46s
epoch 7  | loss: 0.31959 | val_0_rmse: 0.56331 | val_1_rmse: 0.55949 |  0:00:52s
epoch 8  | loss: 0.32195 | val_0_rmse: 0.56239 | val_1_rmse: 0.55919 |  0:00:58s
epoch 9  | loss: 0.31917 | val_0_rmse: 0.56116 | val_1_rmse: 0.55768 |  0:01:04s
epoch 10 | loss: 0.32141 | val_0_rmse: 0.56248 | val_1_rmse: 0.55762 |  0:01:10s
epoch 11 | loss: 0.32197 | val_0_rmse: 0.5599  | val_1_rmse: 0.55532 |  0:01:17s
epoch 12 | loss: 0.3189  | val_0_rmse: 0.56429 | val_1_rmse: 0.55963 |  0:01:24s
epoch 13 | loss: 0.32447 | val_0_rmse: 0.56481 | val_1_rmse: 0.56003 |  0:01:30s
epoch 14 | loss: 0.32655 | val_0_rmse: 0.57089 | val_1_rmse: 0.56428 |  0:01:37s
epoch 15 | loss: 0.32367 | val_0_rmse: 0.56818 | val_1_rmse: 0.56463 |  0:01:43s
epoch 16 | loss: 0.32257 | val_0_rmse: 0.56221 | val_1_rmse: 0.55838 |  0:01:50s
epoch 17 | loss: 0.31878 | val_0_rmse: 0.56079 | val_1_rmse: 0.55619 |  0:01:56s
epoch 18 | loss: 0.32075 | val_0_rmse: 0.56076 | val_1_rmse: 0.55536 |  0:02:03s
epoch 19 | loss: 0.3194  | val_0_rmse: 0.56376 | val_1_rmse: 0.55884 |  0:02:09s
epoch 20 | loss: 0.32257 | val_0_rmse: 0.56245 | val_1_rmse: 0.5576  |  0:02:15s
epoch 21 | loss: 0.31742 | val_0_rmse: 0.55621 | val_1_rmse: 0.55245 |  0:02:23s
epoch 22 | loss: 0.31468 | val_0_rmse: 0.55316 | val_1_rmse: 0.5483  |  0:02:30s
epoch 23 | loss: 0.31358 | val_0_rmse: 0.55672 | val_1_rmse: 0.55254 |  0:02:36s
epoch 24 | loss: 0.31325 | val_0_rmse: 0.55537 | val_1_rmse: 0.5519  |  0:02:42s
epoch 25 | loss: 0.31441 | val_0_rmse: 0.55955 | val_1_rmse: 0.55548 |  0:02:49s
epoch 26 | loss: 0.31846 | val_0_rmse: 0.55286 | val_1_rmse: 0.54764 |  0:02:55s
epoch 27 | loss: 0.31406 | val_0_rmse: 0.55291 | val_1_rmse: 0.54831 |  0:03:02s
epoch 28 | loss: 0.31194 | val_0_rmse: 0.56165 | val_1_rmse: 0.55744 |  0:03:09s
epoch 29 | loss: 0.31064 | val_0_rmse: 0.55683 | val_1_rmse: 0.55145 |  0:03:16s
epoch 30 | loss: 0.3177  | val_0_rmse: 0.57135 | val_1_rmse: 0.56678 |  0:03:22s
epoch 31 | loss: 0.31402 | val_0_rmse: 0.55708 | val_1_rmse: 0.54896 |  0:03:29s
epoch 32 | loss: 0.31235 | val_0_rmse: 0.55906 | val_1_rmse: 0.55461 |  0:03:35s
epoch 33 | loss: 0.31436 | val_0_rmse: 0.55855 | val_1_rmse: 0.55366 |  0:03:42s
epoch 34 | loss: 0.30885 | val_0_rmse: 0.56286 | val_1_rmse: 0.54748 |  0:03:48s
epoch 35 | loss: 0.3113  | val_0_rmse: 0.55652 | val_1_rmse: 0.54432 |  0:03:54s
epoch 36 | loss: 0.31132 | val_0_rmse: 0.56816 | val_1_rmse: 0.56283 |  0:04:00s
epoch 37 | loss: 0.30482 | val_0_rmse: 0.59459 | val_1_rmse: 0.58959 |  0:04:06s
epoch 38 | loss: 0.31216 | val_0_rmse: 0.56715 | val_1_rmse: 0.54904 |  0:04:12s
epoch 39 | loss: 0.31139 | val_0_rmse: 0.55427 | val_1_rmse: 0.5483  |  0:04:18s
epoch 40 | loss: 0.30773 | val_0_rmse: 0.56888 | val_1_rmse: 0.55481 |  0:04:23s
epoch 41 | loss: 0.30754 | val_0_rmse: 0.54915 | val_1_rmse: 0.54482 |  0:04:29s
epoch 42 | loss: 0.30705 | val_0_rmse: 0.54778 | val_1_rmse: 0.54378 |  0:04:35s
epoch 43 | loss: 0.30504 | val_0_rmse: 0.55258 | val_1_rmse: 0.5449  |  0:04:41s
epoch 44 | loss: 0.30326 | val_0_rmse: 0.56864 | val_1_rmse: 0.56605 |  0:04:47s
epoch 45 | loss: 0.30242 | val_0_rmse: 0.55335 | val_1_rmse: 0.54953 |  0:04:53s
epoch 46 | loss: 0.3013  | val_0_rmse: 0.5417  | val_1_rmse: 0.53828 |  0:05:00s
epoch 47 | loss: 0.29934 | val_0_rmse: 0.54459 | val_1_rmse: 0.53961 |  0:05:06s
epoch 48 | loss: 0.3007  | val_0_rmse: 0.54673 | val_1_rmse: 0.54308 |  0:05:13s
epoch 49 | loss: 0.30203 | val_0_rmse: 0.557   | val_1_rmse: 0.5524  |  0:05:19s
epoch 50 | loss: 0.29819 | val_0_rmse: 0.55203 | val_1_rmse: 0.54847 |  0:05:26s
epoch 51 | loss: 0.29844 | val_0_rmse: 0.58261 | val_1_rmse: 0.582   |  0:05:32s
epoch 52 | loss: 0.29659 | val_0_rmse: 0.57408 | val_1_rmse: 0.57221 |  0:05:39s
epoch 53 | loss: 0.29876 | val_0_rmse: 0.55233 | val_1_rmse: 0.54733 |  0:05:46s
epoch 54 | loss: 0.29562 | val_0_rmse: 0.54253 | val_1_rmse: 0.53837 |  0:05:52s
epoch 55 | loss: 0.29883 | val_0_rmse: 0.57819 | val_1_rmse: 0.57335 |  0:05:58s
epoch 56 | loss: 0.29911 | val_0_rmse: 0.57126 | val_1_rmse: 0.56669 |  0:06:03s
epoch 57 | loss: 0.29829 | val_0_rmse: 0.5815  | val_1_rmse: 0.57451 |  0:06:09s
epoch 58 | loss: 0.30077 | val_0_rmse: 0.54592 | val_1_rmse: 0.54257 |  0:06:16s
epoch 59 | loss: 0.29651 | val_0_rmse: 0.57844 | val_1_rmse: 0.57359 |  0:06:23s
epoch 60 | loss: 0.29749 | val_0_rmse: 0.59253 | val_1_rmse: 0.58804 |  0:06:30s
epoch 61 | loss: 0.29578 | val_0_rmse: 0.53737 | val_1_rmse: 0.53297 |  0:06:36s
epoch 62 | loss: 0.29577 | val_0_rmse: 0.54592 | val_1_rmse: 0.54071 |  0:06:42s
epoch 63 | loss: 0.29543 | val_0_rmse: 0.55311 | val_1_rmse: 0.54813 |  0:06:49s
epoch 64 | loss: 0.2972  | val_0_rmse: 0.55048 | val_1_rmse: 0.5478  |  0:06:56s
epoch 65 | loss: 0.29547 | val_0_rmse: 0.57045 | val_1_rmse: 0.56633 |  0:07:02s
epoch 66 | loss: 0.29576 | val_0_rmse: 0.5745  | val_1_rmse: 0.56987 |  0:07:08s
epoch 67 | loss: 0.29635 | val_0_rmse: 0.5419  | val_1_rmse: 0.53847 |  0:07:14s
epoch 68 | loss: 0.29417 | val_0_rmse: 0.55195 | val_1_rmse: 0.54759 |  0:07:21s
epoch 69 | loss: 0.29336 | val_0_rmse: 0.56006 | val_1_rmse: 0.55547 |  0:07:26s
epoch 70 | loss: 0.29243 | val_0_rmse: 0.54833 | val_1_rmse: 0.54456 |  0:07:31s
epoch 71 | loss: 0.29484 | val_0_rmse: 0.56936 | val_1_rmse: 0.56553 |  0:07:36s
epoch 72 | loss: 0.295   | val_0_rmse: 0.54426 | val_1_rmse: 0.54221 |  0:07:41s
epoch 73 | loss: 0.29577 | val_0_rmse: 0.54928 | val_1_rmse: 0.54526 |  0:07:46s
epoch 74 | loss: 0.2946  | val_0_rmse: 0.57528 | val_1_rmse: 0.57253 |  0:07:51s
epoch 75 | loss: 0.29174 | val_0_rmse: 0.57836 | val_1_rmse: 0.57376 |  0:07:56s
epoch 76 | loss: 0.29568 | val_0_rmse: 0.54598 | val_1_rmse: 0.54198 |  0:08:00s
epoch 77 | loss: 0.29377 | val_0_rmse: 0.53364 | val_1_rmse: 0.53045 |  0:08:05s
epoch 78 | loss: 0.29409 | val_0_rmse: 0.58771 | val_1_rmse: 0.58117 |  0:08:10s
epoch 79 | loss: 0.30699 | val_0_rmse: 0.56433 | val_1_rmse: 0.55697 |  0:08:14s
epoch 80 | loss: 0.29929 | val_0_rmse: 0.53854 | val_1_rmse: 0.53463 |  0:08:19s
epoch 81 | loss: 0.29614 | val_0_rmse: 0.5889  | val_1_rmse: 0.5867  |  0:08:24s
epoch 82 | loss: 0.30041 | val_0_rmse: 0.57685 | val_1_rmse: 0.57192 |  0:08:28s
epoch 83 | loss: 0.29271 | val_0_rmse: 0.5892  | val_1_rmse: 0.58262 |  0:08:33s
epoch 84 | loss: 0.29298 | val_0_rmse: 0.58404 | val_1_rmse: 0.57996 |  0:08:37s
epoch 85 | loss: 0.29837 | val_0_rmse: 0.57086 | val_1_rmse: 0.56519 |  0:08:42s
epoch 86 | loss: 0.2955  | val_0_rmse: 0.53934 | val_1_rmse: 0.53488 |  0:08:47s
epoch 87 | loss: 0.29337 | val_0_rmse: 0.5506  | val_1_rmse: 0.54667 |  0:08:51s
epoch 88 | loss: 0.29477 | val_0_rmse: 0.55781 | val_1_rmse: 0.55435 |  0:08:56s
epoch 89 | loss: 0.29482 | val_0_rmse: 0.54086 | val_1_rmse: 0.53802 |  0:09:01s
epoch 90 | loss: 0.29472 | val_0_rmse: 0.54098 | val_1_rmse: 0.53755 |  0:09:05s
epoch 91 | loss: 0.29163 | val_0_rmse: 0.53593 | val_1_rmse: 0.53096 |  0:09:11s
epoch 92 | loss: 0.29373 | val_0_rmse: 0.54814 | val_1_rmse: 0.54395 |  0:09:15s
epoch 93 | loss: 0.29027 | val_0_rmse: 0.55786 | val_1_rmse: 0.55298 |  0:09:21s
epoch 94 | loss: 0.29077 | val_0_rmse: 0.56696 | val_1_rmse: 0.56319 |  0:09:26s
epoch 95 | loss: 0.28981 | val_0_rmse: 0.5461  | val_1_rmse: 0.54109 |  0:09:31s
epoch 96 | loss: 0.29211 | val_0_rmse: 0.53693 | val_1_rmse: 0.5342  |  0:09:36s
epoch 97 | loss: 0.29196 | val_0_rmse: 0.55459 | val_1_rmse: 0.55034 |  0:09:41s
epoch 98 | loss: 0.29031 | val_0_rmse: 0.56172 | val_1_rmse: 0.5585  |  0:09:46s
epoch 99 | loss: 0.29139 | val_0_rmse: 0.59785 | val_1_rmse: 0.59396 |  0:09:51s
epoch 100| loss: 0.29033 | val_0_rmse: 0.56533 | val_1_rmse: 0.56088 |  0:09:56s
epoch 101| loss: 0.28991 | val_0_rmse: 0.55497 | val_1_rmse: 0.55046 |  0:10:01s
epoch 102| loss: 0.2922  | val_0_rmse: 0.58637 | val_1_rmse: 0.58166 |  0:10:07s
epoch 103| loss: 0.29387 | val_0_rmse: 0.56191 | val_1_rmse: 0.56004 |  0:10:12s
epoch 104| loss: 0.2983  | val_0_rmse: 0.54948 | val_1_rmse: 0.54509 |  0:10:17s
epoch 105| loss: 0.29485 | val_0_rmse: 0.59745 | val_1_rmse: 0.59617 |  0:10:23s
epoch 106| loss: 0.29325 | val_0_rmse: 0.54242 | val_1_rmse: 0.53947 |  0:10:28s
epoch 107| loss: 0.2963  | val_0_rmse: 0.57274 | val_1_rmse: 0.57046 |  0:10:34s

Early stopping occured at epoch 107 with best_epoch = 77 and best_val_1_rmse = 0.53045
Best weights from best epoch are automatically used!
ended training at: 23:38:09
Feature importance:
[('Area', 0.5327428307723937), ('Baths', 0.06574385241088106), ('Beds', 0.23654344308588468), ('Latitude', 0.0), ('Longitude', 0.12142618681418221), ('Month', 0.0), ('Year', 0.043543686916658324)]
Mean squared error is of 952678063.6675671
Mean absolute error:21466.56036339635
MAPE:0.35613398897631793
R2 score:0.7208240937811037
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:38:10
epoch 0  | loss: 0.45307 | val_0_rmse: 0.58131 | val_1_rmse: 0.58051 |  0:00:05s
epoch 1  | loss: 0.34668 | val_0_rmse: 0.57578 | val_1_rmse: 0.57521 |  0:00:12s
epoch 2  | loss: 0.33871 | val_0_rmse: 0.56787 | val_1_rmse: 0.56765 |  0:00:17s
epoch 3  | loss: 0.33126 | val_0_rmse: 0.56618 | val_1_rmse: 0.56577 |  0:00:22s
epoch 4  | loss: 0.33038 | val_0_rmse: 0.56719 | val_1_rmse: 0.56754 |  0:00:28s
epoch 5  | loss: 0.32752 | val_0_rmse: 0.56787 | val_1_rmse: 0.56807 |  0:00:33s
epoch 6  | loss: 0.32762 | val_0_rmse: 0.56962 | val_1_rmse: 0.56901 |  0:00:39s
epoch 7  | loss: 0.33241 | val_0_rmse: 0.56958 | val_1_rmse: 0.56981 |  0:00:45s
epoch 8  | loss: 0.32803 | val_0_rmse: 0.55979 | val_1_rmse: 0.56051 |  0:00:51s
epoch 9  | loss: 0.32176 | val_0_rmse: 0.56465 | val_1_rmse: 0.56481 |  0:00:57s
epoch 10 | loss: 0.32316 | val_0_rmse: 0.561   | val_1_rmse: 0.56409 |  0:01:03s
epoch 11 | loss: 0.32181 | val_0_rmse: 0.55846 | val_1_rmse: 0.55922 |  0:01:09s
epoch 12 | loss: 0.32154 | val_0_rmse: 0.5578  | val_1_rmse: 0.55804 |  0:01:15s
epoch 13 | loss: 0.32293 | val_0_rmse: 0.56208 | val_1_rmse: 0.56235 |  0:01:22s
epoch 14 | loss: 0.31763 | val_0_rmse: 0.56066 | val_1_rmse: 0.56194 |  0:01:28s
epoch 15 | loss: 0.32661 | val_0_rmse: 0.57385 | val_1_rmse: 0.57214 |  0:01:34s
epoch 16 | loss: 0.33162 | val_0_rmse: 0.56503 | val_1_rmse: 0.56352 |  0:01:40s
epoch 17 | loss: 0.32783 | val_0_rmse: 0.56198 | val_1_rmse: 0.5622  |  0:01:47s
epoch 18 | loss: 0.3245  | val_0_rmse: 0.55918 | val_1_rmse: 0.56009 |  0:01:54s
epoch 19 | loss: 0.32073 | val_0_rmse: 0.55793 | val_1_rmse: 0.55949 |  0:02:00s
epoch 20 | loss: 0.31681 | val_0_rmse: 0.55649 | val_1_rmse: 0.55769 |  0:02:06s
epoch 21 | loss: 0.31683 | val_0_rmse: 0.55696 | val_1_rmse: 0.55813 |  0:02:13s
epoch 22 | loss: 0.31841 | val_0_rmse: 0.56711 | val_1_rmse: 0.56626 |  0:02:19s
epoch 23 | loss: 0.32222 | val_0_rmse: 0.56051 | val_1_rmse: 0.56212 |  0:02:25s
epoch 24 | loss: 0.32292 | val_0_rmse: 0.56151 | val_1_rmse: 0.56325 |  0:02:31s
epoch 25 | loss: 0.32044 | val_0_rmse: 0.56728 | val_1_rmse: 0.56796 |  0:02:36s
epoch 26 | loss: 0.32316 | val_0_rmse: 0.56398 | val_1_rmse: 0.56607 |  0:02:42s
epoch 27 | loss: 0.32063 | val_0_rmse: 0.55697 | val_1_rmse: 0.55715 |  0:02:48s
epoch 28 | loss: 0.31661 | val_0_rmse: 0.5555  | val_1_rmse: 0.55764 |  0:02:54s
epoch 29 | loss: 0.31511 | val_0_rmse: 0.55525 | val_1_rmse: 0.55594 |  0:03:01s
epoch 30 | loss: 0.31484 | val_0_rmse: 0.56209 | val_1_rmse: 0.56344 |  0:03:08s
epoch 31 | loss: 0.31468 | val_0_rmse: 0.55843 | val_1_rmse: 0.56088 |  0:03:13s
epoch 32 | loss: 0.31351 | val_0_rmse: 0.55608 | val_1_rmse: 0.55771 |  0:03:19s
epoch 33 | loss: 0.31203 | val_0_rmse: 0.55338 | val_1_rmse: 0.55544 |  0:03:26s
epoch 34 | loss: 0.31521 | val_0_rmse: 0.55292 | val_1_rmse: 0.5543  |  0:03:32s
epoch 35 | loss: 0.31249 | val_0_rmse: 0.55265 | val_1_rmse: 0.55461 |  0:03:39s
epoch 36 | loss: 0.31164 | val_0_rmse: 0.55175 | val_1_rmse: 0.55496 |  0:03:45s
epoch 37 | loss: 0.31714 | val_0_rmse: 0.55123 | val_1_rmse: 0.55498 |  0:03:51s
epoch 38 | loss: 0.30998 | val_0_rmse: 0.55251 | val_1_rmse: 0.5564  |  0:03:58s
epoch 39 | loss: 0.31057 | val_0_rmse: 0.55206 | val_1_rmse: 0.55369 |  0:04:04s
epoch 40 | loss: 0.30971 | val_0_rmse: 0.5511  | val_1_rmse: 0.55319 |  0:04:10s
epoch 41 | loss: 0.30845 | val_0_rmse: 0.54771 | val_1_rmse: 0.54945 |  0:04:16s
epoch 42 | loss: 0.3092  | val_0_rmse: 0.54802 | val_1_rmse: 0.54966 |  0:04:22s
epoch 43 | loss: 0.30871 | val_0_rmse: 0.55101 | val_1_rmse: 0.55483 |  0:04:28s
epoch 44 | loss: 0.30724 | val_0_rmse: 0.55205 | val_1_rmse: 0.55556 |  0:04:34s
epoch 45 | loss: 0.30964 | val_0_rmse: 0.54844 | val_1_rmse: 0.54908 |  0:04:40s
epoch 46 | loss: 0.30999 | val_0_rmse: 0.58001 | val_1_rmse: 0.58199 |  0:04:47s
epoch 47 | loss: 0.30407 | val_0_rmse: 0.55535 | val_1_rmse: 0.55798 |  0:04:53s
epoch 48 | loss: 0.30071 | val_0_rmse: 0.53603 | val_1_rmse: 0.53703 |  0:04:59s
epoch 49 | loss: 0.29935 | val_0_rmse: 0.542   | val_1_rmse: 0.54413 |  0:05:05s
epoch 50 | loss: 0.30037 | val_0_rmse: 0.66815 | val_1_rmse: 0.67035 |  0:05:11s
epoch 51 | loss: 0.29472 | val_0_rmse: 0.53568 | val_1_rmse: 0.53639 |  0:05:17s
epoch 52 | loss: 0.29069 | val_0_rmse: 0.60628 | val_1_rmse: 0.61016 |  0:05:23s
epoch 53 | loss: 0.29079 | val_0_rmse: 0.55334 | val_1_rmse: 0.55358 |  0:05:29s
epoch 54 | loss: 0.31717 | val_0_rmse: 0.5732  | val_1_rmse: 0.57478 |  0:05:35s
epoch 55 | loss: 0.32888 | val_0_rmse: 0.60509 | val_1_rmse: 0.60478 |  0:05:41s
epoch 56 | loss: 0.30489 | val_0_rmse: 0.53638 | val_1_rmse: 0.53819 |  0:05:46s
epoch 57 | loss: 0.29054 | val_0_rmse: 0.57355 | val_1_rmse: 0.57291 |  0:05:52s
epoch 58 | loss: 0.28676 | val_0_rmse: 0.54556 | val_1_rmse: 0.5476  |  0:05:58s
epoch 59 | loss: 0.28497 | val_0_rmse: 0.54499 | val_1_rmse: 0.55146 |  0:06:05s
epoch 60 | loss: 0.29732 | val_0_rmse: 0.63671 | val_1_rmse: 0.63096 |  0:06:11s
epoch 61 | loss: 0.2992  | val_0_rmse: 0.55033 | val_1_rmse: 0.55448 |  0:06:17s
epoch 62 | loss: 0.28994 | val_0_rmse: 0.54327 | val_1_rmse: 0.54492 |  0:06:23s
epoch 63 | loss: 0.28477 | val_0_rmse: 0.53662 | val_1_rmse: 0.53912 |  0:06:29s
epoch 64 | loss: 0.28381 | val_0_rmse: 0.60434 | val_1_rmse: 0.60007 |  0:06:35s
epoch 65 | loss: 0.28413 | val_0_rmse: 0.57879 | val_1_rmse: 0.57783 |  0:06:41s
epoch 66 | loss: 0.28207 | val_0_rmse: 0.55967 | val_1_rmse: 0.56072 |  0:06:48s
epoch 67 | loss: 0.27922 | val_0_rmse: 0.51773 | val_1_rmse: 0.52078 |  0:06:54s
epoch 68 | loss: 0.27987 | val_0_rmse: 0.52064 | val_1_rmse: 0.52223 |  0:07:01s
epoch 69 | loss: 0.2792  | val_0_rmse: 0.52307 | val_1_rmse: 0.52622 |  0:07:07s
epoch 70 | loss: 0.28231 | val_0_rmse: 0.56934 | val_1_rmse: 0.56871 |  0:07:13s
epoch 71 | loss: 0.29742 | val_0_rmse: 0.61799 | val_1_rmse: 0.61913 |  0:07:19s
epoch 72 | loss: 0.29751 | val_0_rmse: 0.56126 | val_1_rmse: 0.55947 |  0:07:25s
epoch 73 | loss: 0.29076 | val_0_rmse: 0.59916 | val_1_rmse: 0.60132 |  0:07:31s
epoch 74 | loss: 0.29563 | val_0_rmse: 0.62983 | val_1_rmse: 0.63324 |  0:07:38s
epoch 75 | loss: 0.30005 | val_0_rmse: 0.5347  | val_1_rmse: 0.53711 |  0:07:45s
epoch 76 | loss: 0.29327 | val_0_rmse: 0.52469 | val_1_rmse: 0.52736 |  0:07:51s
epoch 77 | loss: 0.29156 | val_0_rmse: 0.54177 | val_1_rmse: 0.54206 |  0:07:57s
epoch 78 | loss: 0.30458 | val_0_rmse: 0.53652 | val_1_rmse: 0.53713 |  0:08:03s
epoch 79 | loss: 0.29033 | val_0_rmse: 0.5864  | val_1_rmse: 0.59284 |  0:08:09s
epoch 80 | loss: 0.29087 | val_0_rmse: 0.61887 | val_1_rmse: 0.622   |  0:08:14s
epoch 81 | loss: 0.2873  | val_0_rmse: 0.58648 | val_1_rmse: 0.58442 |  0:08:20s
epoch 82 | loss: 0.28585 | val_0_rmse: 0.61658 | val_1_rmse: 0.6143  |  0:08:26s
epoch 83 | loss: 0.28498 | val_0_rmse: 0.5743  | val_1_rmse: 0.57452 |  0:08:32s
epoch 84 | loss: 0.28223 | val_0_rmse: 0.57494 | val_1_rmse: 0.57622 |  0:08:38s
epoch 85 | loss: 0.2816  | val_0_rmse: 0.52878 | val_1_rmse: 0.53028 |  0:08:44s
epoch 86 | loss: 0.28872 | val_0_rmse: 0.53289 | val_1_rmse: 0.5325  |  0:08:51s
epoch 87 | loss: 0.28189 | val_0_rmse: 0.59284 | val_1_rmse: 0.59316 |  0:08:57s
epoch 88 | loss: 0.28013 | val_0_rmse: 0.5907  | val_1_rmse: 0.59022 |  0:09:03s
epoch 89 | loss: 0.28106 | val_0_rmse: 0.57366 | val_1_rmse: 0.57278 |  0:09:09s
epoch 90 | loss: 0.28026 | val_0_rmse: 0.57542 | val_1_rmse: 0.57716 |  0:09:15s
epoch 91 | loss: 0.28046 | val_0_rmse: 0.57436 | val_1_rmse: 0.57541 |  0:09:22s
epoch 92 | loss: 0.27905 | val_0_rmse: 0.64225 | val_1_rmse: 0.63938 |  0:09:28s
epoch 93 | loss: 0.2867  | val_0_rmse: 0.5212  | val_1_rmse: 0.5228  |  0:09:34s
epoch 94 | loss: 0.28292 | val_0_rmse: 0.65533 | val_1_rmse: 0.65303 |  0:09:40s
epoch 95 | loss: 0.28768 | val_0_rmse: 0.62378 | val_1_rmse: 0.62243 |  0:09:46s
epoch 96 | loss: 0.27973 | val_0_rmse: 0.5197  | val_1_rmse: 0.52158 |  0:09:52s
epoch 97 | loss: 0.27643 | val_0_rmse: 0.52473 | val_1_rmse: 0.52677 |  0:09:57s

Early stopping occured at epoch 97 with best_epoch = 67 and best_val_1_rmse = 0.52078
Best weights from best epoch are automatically used!
ended training at: 23:48:10
Feature importance:
[('Area', 0.7722725321502597), ('Baths', 8.10512712505799e-05), ('Beds', 0.09538817048460049), ('Latitude', 0.0775942512553381), ('Longitude', 0.0030492239182594084), ('Month', 0.0), ('Year', 0.051614770920291674)]
Mean squared error is of 892475267.6111499
Mean absolute error:20922.98178296693
MAPE:0.35031792452202176
R2 score:0.7315428701644732
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:48:11
epoch 0  | loss: 0.43449 | val_0_rmse: 0.5869  | val_1_rmse: 0.58356 |  0:00:05s
epoch 1  | loss: 0.3382  | val_0_rmse: 0.57697 | val_1_rmse: 0.57721 |  0:00:11s
epoch 2  | loss: 0.33933 | val_0_rmse: 0.58691 | val_1_rmse: 0.58911 |  0:00:17s
epoch 3  | loss: 0.33726 | val_0_rmse: 0.57311 | val_1_rmse: 0.57239 |  0:00:22s
epoch 4  | loss: 0.33418 | val_0_rmse: 0.57359 | val_1_rmse: 0.57581 |  0:00:28s
epoch 5  | loss: 0.32863 | val_0_rmse: 0.56587 | val_1_rmse: 0.56551 |  0:00:34s
epoch 6  | loss: 0.3313  | val_0_rmse: 0.56808 | val_1_rmse: 0.56849 |  0:00:40s
epoch 7  | loss: 0.32798 | val_0_rmse: 0.5679  | val_1_rmse: 0.56865 |  0:00:45s
epoch 8  | loss: 0.33015 | val_0_rmse: 0.56531 | val_1_rmse: 0.56579 |  0:00:52s
epoch 9  | loss: 0.32629 | val_0_rmse: 0.5623  | val_1_rmse: 0.56246 |  0:00:58s
epoch 10 | loss: 0.32418 | val_0_rmse: 0.56247 | val_1_rmse: 0.5624  |  0:01:04s
epoch 11 | loss: 0.32364 | val_0_rmse: 0.5615  | val_1_rmse: 0.56067 |  0:01:10s
epoch 12 | loss: 0.32228 | val_0_rmse: 0.56729 | val_1_rmse: 0.56527 |  0:01:17s
epoch 13 | loss: 0.32139 | val_0_rmse: 0.5588  | val_1_rmse: 0.5587  |  0:01:23s
epoch 14 | loss: 0.32206 | val_0_rmse: 0.56751 | val_1_rmse: 0.565   |  0:01:30s
epoch 15 | loss: 0.31902 | val_0_rmse: 0.55777 | val_1_rmse: 0.55528 |  0:01:36s
epoch 16 | loss: 0.31806 | val_0_rmse: 0.55932 | val_1_rmse: 0.55468 |  0:01:42s
epoch 17 | loss: 0.31784 | val_0_rmse: 0.55601 | val_1_rmse: 0.55381 |  0:01:48s
epoch 18 | loss: 0.31334 | val_0_rmse: 0.55396 | val_1_rmse: 0.54993 |  0:01:55s
epoch 19 | loss: 0.31591 | val_0_rmse: 0.56794 | val_1_rmse: 0.56454 |  0:02:01s
epoch 20 | loss: 0.3217  | val_0_rmse: 0.56104 | val_1_rmse: 0.55835 |  0:02:08s
epoch 21 | loss: 0.3212  | val_0_rmse: 0.55892 | val_1_rmse: 0.55624 |  0:02:14s
epoch 22 | loss: 0.31632 | val_0_rmse: 0.55739 | val_1_rmse: 0.55527 |  0:02:19s
epoch 23 | loss: 0.31701 | val_0_rmse: 0.56259 | val_1_rmse: 0.5602  |  0:02:25s
epoch 24 | loss: 0.30951 | val_0_rmse: 0.62628 | val_1_rmse: 0.61783 |  0:02:32s
epoch 25 | loss: 0.30593 | val_0_rmse: 0.66764 | val_1_rmse: 0.66234 |  0:02:38s
epoch 26 | loss: 0.3057  | val_0_rmse: 0.58777 | val_1_rmse: 0.59018 |  0:02:44s
epoch 27 | loss: 0.29868 | val_0_rmse: 0.596   | val_1_rmse: 0.59659 |  0:02:52s
epoch 28 | loss: 0.29595 | val_0_rmse: 0.58399 | val_1_rmse: 0.58457 |  0:02:58s
epoch 29 | loss: 0.30162 | val_0_rmse: 0.61806 | val_1_rmse: 0.61852 |  0:03:05s
epoch 30 | loss: 0.29609 | val_0_rmse: 0.61305 | val_1_rmse: 0.6149  |  0:03:11s
epoch 31 | loss: 0.29629 | val_0_rmse: 0.8339  | val_1_rmse: 0.82939 |  0:03:17s
epoch 32 | loss: 0.29378 | val_0_rmse: 0.82732 | val_1_rmse: 0.82462 |  0:03:24s
epoch 33 | loss: 0.29356 | val_0_rmse: 0.92874 | val_1_rmse: 0.92103 |  0:03:31s
epoch 34 | loss: 0.29504 | val_0_rmse: 0.61811 | val_1_rmse: 0.61971 |  0:03:37s
epoch 35 | loss: 0.29259 | val_0_rmse: 0.76505 | val_1_rmse: 0.77134 |  0:03:43s
epoch 36 | loss: 0.28928 | val_0_rmse: 0.54808 | val_1_rmse: 0.5446  |  0:03:49s
epoch 37 | loss: 0.28704 | val_0_rmse: 1.01192 | val_1_rmse: 1.00095 |  0:03:55s
epoch 38 | loss: 0.28516 | val_0_rmse: 0.66498 | val_1_rmse: 0.6687  |  0:04:01s
epoch 39 | loss: 0.28188 | val_0_rmse: 0.78565 | val_1_rmse: 0.79146 |  0:04:07s
epoch 40 | loss: 0.28191 | val_0_rmse: 0.6952  | val_1_rmse: 0.69743 |  0:04:13s
epoch 41 | loss: 0.28149 | val_0_rmse: 0.68775 | val_1_rmse: 0.6616  |  0:04:19s
epoch 42 | loss: 0.2816  | val_0_rmse: 4.83026 | val_1_rmse: 4.82274 |  0:04:25s
epoch 43 | loss: 0.2887  | val_0_rmse: 0.69555 | val_1_rmse: 0.67808 |  0:04:31s
epoch 44 | loss: 0.29118 | val_0_rmse: 0.75256 | val_1_rmse: 0.74547 |  0:04:37s
epoch 45 | loss: 0.28467 | val_0_rmse: 0.7704  | val_1_rmse: 0.76954 |  0:04:43s
epoch 46 | loss: 0.28695 | val_0_rmse: 0.70501 | val_1_rmse: 0.70425 |  0:04:50s
epoch 47 | loss: 0.28127 | val_0_rmse: 3.23279 | val_1_rmse: 3.22649 |  0:04:56s
epoch 48 | loss: 0.28284 | val_0_rmse: 0.70694 | val_1_rmse: 0.70842 |  0:05:02s
epoch 49 | loss: 0.28427 | val_0_rmse: 0.76407 | val_1_rmse: 0.77084 |  0:05:08s
epoch 50 | loss: 0.28157 | val_0_rmse: 0.63021 | val_1_rmse: 0.63331 |  0:05:15s
epoch 51 | loss: 0.27952 | val_0_rmse: 2.79121 | val_1_rmse: 2.78612 |  0:05:21s
epoch 52 | loss: 0.27793 | val_0_rmse: 0.68771 | val_1_rmse: 0.68673 |  0:05:27s
epoch 53 | loss: 0.28224 | val_0_rmse: 0.637   | val_1_rmse: 0.6238  |  0:05:33s
epoch 54 | loss: 0.27574 | val_0_rmse: 0.77525 | val_1_rmse: 0.78121 |  0:05:39s
epoch 55 | loss: 0.28342 | val_0_rmse: 0.57015 | val_1_rmse: 0.56888 |  0:05:45s
epoch 56 | loss: 0.28812 | val_0_rmse: 0.87015 | val_1_rmse: 0.84726 |  0:05:51s
epoch 57 | loss: 0.32437 | val_0_rmse: 2.14337 | val_1_rmse: 2.13328 |  0:05:57s
epoch 58 | loss: 0.32166 | val_0_rmse: 0.58734 | val_1_rmse: 0.58554 |  0:06:04s
epoch 59 | loss: 0.30364 | val_0_rmse: 0.5778  | val_1_rmse: 0.57745 |  0:06:09s
epoch 60 | loss: 0.30185 | val_0_rmse: 0.61731 | val_1_rmse: 0.61811 |  0:06:15s
epoch 61 | loss: 0.3075  | val_0_rmse: 0.64584 | val_1_rmse: 0.64485 |  0:06:22s
epoch 62 | loss: 0.30236 | val_0_rmse: 0.61495 | val_1_rmse: 0.61527 |  0:06:28s
epoch 63 | loss: 0.29853 | val_0_rmse: 0.71307 | val_1_rmse: 0.71323 |  0:06:35s
epoch 64 | loss: 0.30111 | val_0_rmse: 0.6055  | val_1_rmse: 0.60498 |  0:06:41s
epoch 65 | loss: 0.31069 | val_0_rmse: 0.64098 | val_1_rmse: 0.6397  |  0:06:48s
epoch 66 | loss: 0.30822 | val_0_rmse: 0.61168 | val_1_rmse: 0.6114  |  0:06:54s

Early stopping occured at epoch 66 with best_epoch = 36 and best_val_1_rmse = 0.5446
Best weights from best epoch are automatically used!
ended training at: 23:55:08
Feature importance:
[('Area', 0.521248015272349), ('Baths', 0.06355478665294773), ('Beds', 0.09359671103694275), ('Latitude', 0.25212933678629734), ('Longitude', 0.021329451007145957), ('Month', 0.0), ('Year', 0.04814169924431723)]
Mean squared error is of 1009184713.5677476
Mean absolute error:23012.69204422924
MAPE:0.4216613663685359
R2 score:0.6974025829130723
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: Zameen Property.csv
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 23:55:09
epoch 0  | loss: 0.42599 | val_0_rmse: 0.59059 | val_1_rmse: 0.59396 |  0:00:06s
epoch 1  | loss: 0.3339  | val_0_rmse: 0.57038 | val_1_rmse: 0.57501 |  0:00:12s
epoch 2  | loss: 0.32666 | val_0_rmse: 0.56476 | val_1_rmse: 0.56974 |  0:00:19s
epoch 3  | loss: 0.32427 | val_0_rmse: 0.56764 | val_1_rmse: 0.57244 |  0:00:25s
epoch 4  | loss: 0.32467 | val_0_rmse: 0.55703 | val_1_rmse: 0.5594  |  0:00:31s
epoch 5  | loss: 0.31218 | val_0_rmse: 0.55414 | val_1_rmse: 0.55893 |  0:00:37s
epoch 6  | loss: 0.31608 | val_0_rmse: 0.54684 | val_1_rmse: 0.55021 |  0:00:44s
epoch 7  | loss: 0.30081 | val_0_rmse: 0.55468 | val_1_rmse: 0.55755 |  0:00:50s
epoch 8  | loss: 0.29524 | val_0_rmse: 0.60925 | val_1_rmse: 0.61674 |  0:00:57s
epoch 9  | loss: 0.29447 | val_0_rmse: 0.67287 | val_1_rmse: 0.66947 |  0:01:04s
epoch 10 | loss: 0.29156 | val_0_rmse: 0.53798 | val_1_rmse: 0.54185 |  0:01:10s
epoch 11 | loss: 0.29244 | val_0_rmse: 0.58003 | val_1_rmse: 0.58395 |  0:01:17s
epoch 12 | loss: 0.28899 | val_0_rmse: 0.65245 | val_1_rmse: 0.65273 |  0:01:23s
epoch 13 | loss: 0.28933 | val_0_rmse: 0.53782 | val_1_rmse: 0.54147 |  0:01:29s
epoch 14 | loss: 0.28927 | val_0_rmse: 0.62561 | val_1_rmse: 0.62267 |  0:01:35s
epoch 15 | loss: 0.28959 | val_0_rmse: 0.5462  | val_1_rmse: 0.54895 |  0:01:41s
epoch 16 | loss: 0.2871  | val_0_rmse: 0.53306 | val_1_rmse: 0.53409 |  0:01:48s
epoch 17 | loss: 0.28981 | val_0_rmse: 0.5337  | val_1_rmse: 0.53365 |  0:01:54s
epoch 18 | loss: 0.28614 | val_0_rmse: 0.53171 | val_1_rmse: 0.53263 |  0:02:00s
epoch 19 | loss: 0.28657 | val_0_rmse: 0.52765 | val_1_rmse: 0.53016 |  0:02:06s
epoch 20 | loss: 0.28546 | val_0_rmse: 0.8156  | val_1_rmse: 0.80883 |  0:02:12s
epoch 21 | loss: 0.2864  | val_0_rmse: 0.53279 | val_1_rmse: 0.53701 |  0:02:19s
epoch 22 | loss: 0.28231 | val_0_rmse: 0.53978 | val_1_rmse: 0.54499 |  0:02:25s
epoch 23 | loss: 0.28484 | val_0_rmse: 0.67168 | val_1_rmse: 0.66404 |  0:02:31s
epoch 24 | loss: 0.2819  | val_0_rmse: 0.53094 | val_1_rmse: 0.53486 |  0:02:37s
epoch 25 | loss: 0.2826  | val_0_rmse: 0.54944 | val_1_rmse: 0.55501 |  0:02:43s
epoch 26 | loss: 0.2823  | val_0_rmse: 0.53429 | val_1_rmse: 0.53434 |  0:02:50s
epoch 27 | loss: 0.27983 | val_0_rmse: 0.54071 | val_1_rmse: 0.54886 |  0:02:56s
epoch 28 | loss: 0.28163 | val_0_rmse: 0.54937 | val_1_rmse: 0.5499  |  0:03:03s
epoch 29 | loss: 0.28071 | val_0_rmse: 0.57908 | val_1_rmse: 0.5769  |  0:03:09s
epoch 30 | loss: 0.27671 | val_0_rmse: 0.55119 | val_1_rmse: 0.55683 |  0:03:16s
epoch 31 | loss: 0.28028 | val_0_rmse: 0.54804 | val_1_rmse: 0.55622 |  0:03:21s
epoch 32 | loss: 0.27781 | val_0_rmse: 0.56394 | val_1_rmse: 0.56407 |  0:03:27s
epoch 33 | loss: 0.28214 | val_0_rmse: 0.56685 | val_1_rmse: 0.57625 |  0:03:33s
epoch 34 | loss: 0.27727 | val_0_rmse: 0.54301 | val_1_rmse: 0.54987 |  0:03:40s
epoch 35 | loss: 0.27698 | val_0_rmse: 0.5423  | val_1_rmse: 0.54915 |  0:03:46s
epoch 36 | loss: 0.27799 | val_0_rmse: 0.65427 | val_1_rmse: 0.64883 |  0:03:52s
epoch 37 | loss: 0.27788 | val_0_rmse: 0.51728 | val_1_rmse: 0.52157 |  0:03:59s
epoch 38 | loss: 0.27522 | val_0_rmse: 0.5318  | val_1_rmse: 0.53746 |  0:04:05s
epoch 39 | loss: 0.27423 | val_0_rmse: 0.68707 | val_1_rmse: 0.68176 |  0:04:11s
epoch 40 | loss: 0.27433 | val_0_rmse: 0.51372 | val_1_rmse: 0.51764 |  0:04:18s
epoch 41 | loss: 0.27483 | val_0_rmse: 0.55674 | val_1_rmse: 0.56404 |  0:04:24s
epoch 42 | loss: 0.27553 | val_0_rmse: 0.5524  | val_1_rmse: 0.55932 |  0:04:31s
epoch 43 | loss: 0.27668 | val_0_rmse: 0.55341 | val_1_rmse: 0.55936 |  0:04:37s
epoch 44 | loss: 0.2754  | val_0_rmse: 0.52048 | val_1_rmse: 0.52592 |  0:04:44s
epoch 45 | loss: 0.27265 | val_0_rmse: 0.51263 | val_1_rmse: 0.51508 |  0:04:50s
epoch 46 | loss: 0.27549 | val_0_rmse: 0.52313 | val_1_rmse: 0.52937 |  0:04:56s
epoch 47 | loss: 0.27458 | val_0_rmse: 0.51907 | val_1_rmse: 0.51986 |  0:05:03s
epoch 48 | loss: 0.27745 | val_0_rmse: 0.57953 | val_1_rmse: 0.58793 |  0:05:09s
epoch 49 | loss: 0.27479 | val_0_rmse: 0.61146 | val_1_rmse: 0.60931 |  0:05:14s
epoch 50 | loss: 0.27293 | val_0_rmse: 0.52215 | val_1_rmse: 0.52685 |  0:05:20s
epoch 51 | loss: 0.2746  | val_0_rmse: 0.58961 | val_1_rmse: 0.59818 |  0:05:27s
epoch 52 | loss: 0.27312 | val_0_rmse: 0.55811 | val_1_rmse: 0.56542 |  0:05:33s
epoch 53 | loss: 0.27278 | val_0_rmse: 0.53785 | val_1_rmse: 0.54092 |  0:05:38s
epoch 54 | loss: 0.27228 | val_0_rmse: 0.60356 | val_1_rmse: 0.60219 |  0:05:45s
epoch 55 | loss: 0.27618 | val_0_rmse: 0.56459 | val_1_rmse: 0.57284 |  0:05:51s
epoch 56 | loss: 0.27301 | val_0_rmse: 0.51515 | val_1_rmse: 0.51933 |  0:05:57s
epoch 57 | loss: 0.27513 | val_0_rmse: 0.57452 | val_1_rmse: 0.58158 |  0:06:04s
epoch 58 | loss: 0.27188 | val_0_rmse: 0.51425 | val_1_rmse: 0.51677 |  0:06:10s
epoch 59 | loss: 0.26997 | val_0_rmse: 0.51739 | val_1_rmse: 0.52088 |  0:06:16s
epoch 60 | loss: 0.26997 | val_0_rmse: 0.53684 | val_1_rmse: 0.54349 |  0:06:22s
epoch 61 | loss: 0.27166 | val_0_rmse: 0.54907 | val_1_rmse: 0.55639 |  0:06:28s
epoch 62 | loss: 0.2731  | val_0_rmse: 0.55629 | val_1_rmse: 0.56285 |  0:06:34s
epoch 63 | loss: 0.27188 | val_0_rmse: 0.55162 | val_1_rmse: 0.55874 |  0:06:41s
epoch 64 | loss: 0.26973 | val_0_rmse: 0.62243 | val_1_rmse: 0.61792 |  0:06:46s
epoch 65 | loss: 0.27319 | val_0_rmse: 0.52106 | val_1_rmse: 0.5274  |  0:06:52s
epoch 66 | loss: 0.27022 | val_0_rmse: 0.54423 | val_1_rmse: 0.55166 |  0:06:58s
epoch 67 | loss: 0.26865 | val_0_rmse: 0.51711 | val_1_rmse: 0.52172 |  0:07:04s
epoch 68 | loss: 0.27084 | val_0_rmse: 0.51742 | val_1_rmse: 0.51907 |  0:07:10s
epoch 69 | loss: 0.27105 | val_0_rmse: 0.52307 | val_1_rmse: 0.52824 |  0:07:17s
epoch 70 | loss: 0.2698  | val_0_rmse: 0.5562  | val_1_rmse: 0.55659 |  0:07:23s
epoch 71 | loss: 0.27123 | val_0_rmse: 0.54834 | val_1_rmse: 0.55739 |  0:07:30s
epoch 72 | loss: 0.27082 | val_0_rmse: 0.55362 | val_1_rmse: 0.56124 |  0:07:36s
epoch 73 | loss: 0.27126 | val_0_rmse: 0.62803 | val_1_rmse: 0.62496 |  0:07:42s
epoch 74 | loss: 0.27147 | val_0_rmse: 0.56062 | val_1_rmse: 0.56078 |  0:07:49s
epoch 75 | loss: 0.27064 | val_0_rmse: 0.55237 | val_1_rmse: 0.55913 |  0:07:56s

Early stopping occured at epoch 75 with best_epoch = 45 and best_val_1_rmse = 0.51508
Best weights from best epoch are automatically used!
ended training at: 00:03:08
Feature importance:
[('Area', 0.4604757990734267), ('Baths', 0.1853682204074685), ('Beds', 0.21085228854634283), ('Latitude', 0.061176152856600705), ('Longitude', 0.00900734018629593), ('Month', 0.0), ('Year', 0.07312019892986538)]
Mean squared error is of 886360432.8397245
Mean absolute error:20469.492377944673
MAPE:0.32638610978746296
R2 score:0.7386794680363846
------------------------------------------------------------------
Normalization used is StandardScaler
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 0
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:03:09
epoch 0  | loss: 0.45576 | val_0_rmse: 0.63619 | val_1_rmse: 0.63349 |  0:00:37s
epoch 1  | loss: 0.40752 | val_0_rmse: 0.65308 | val_1_rmse: 0.63693 |  0:01:13s
epoch 2  | loss: 0.39805 | val_0_rmse: 0.72525 | val_1_rmse: 0.62711 |  0:01:52s
epoch 3  | loss: 0.40446 | val_0_rmse: 0.73221 | val_1_rmse: 0.62772 |  0:02:32s
epoch 4  | loss: 0.38439 | val_0_rmse: 1.35338 | val_1_rmse: 0.64384 |  0:03:10s
epoch 5  | loss: 0.37069 | val_0_rmse: 6.15229 | val_1_rmse: 0.70361 |  0:03:45s
epoch 6  | loss: 0.36301 | val_0_rmse: 0.70789 | val_1_rmse: 0.70872 |  0:04:15s
epoch 7  | loss: 0.35983 | val_0_rmse: 7.13069 | val_1_rmse: 0.75037 |  0:04:41s
epoch 8  | loss: 0.35341 | val_0_rmse: 0.67295 | val_1_rmse: 0.67331 |  0:05:07s
epoch 9  | loss: 0.35082 | val_0_rmse: 5.30957 | val_1_rmse: 0.8712  |  0:05:34s
epoch 10 | loss: 0.348   | val_0_rmse: 0.79959 | val_1_rmse: 0.80114 |  0:06:00s
epoch 11 | loss: 0.33395 | val_0_rmse: 0.91327 | val_1_rmse: 0.90832 |  0:06:25s
epoch 12 | loss: 0.31993 | val_0_rmse: 0.74175 | val_1_rmse: 0.73504 |  0:06:51s
epoch 13 | loss: 0.31691 | val_0_rmse: 0.78124 | val_1_rmse: 0.74416 |  0:07:16s
epoch 14 | loss: 0.31536 | val_0_rmse: 40.42877| val_1_rmse: 0.7073  |  0:07:43s
epoch 15 | loss: 0.31378 | val_0_rmse: 41.36291| val_1_rmse: 0.88739 |  0:08:10s
epoch 16 | loss: 0.32191 | val_0_rmse: 0.90706 | val_1_rmse: 0.90604 |  0:08:35s
epoch 17 | loss: 0.30569 | val_0_rmse: 0.83236 | val_1_rmse: 0.82768 |  0:09:02s
epoch 18 | loss: 0.3086  | val_0_rmse: 10.81632| val_1_rmse: 0.76696 |  0:09:27s
epoch 19 | loss: 0.30343 | val_0_rmse: 11.72219| val_1_rmse: 0.80334 |  0:09:52s
epoch 20 | loss: 0.30224 | val_0_rmse: 0.76087 | val_1_rmse: 0.74257 |  0:10:17s
epoch 21 | loss: 0.30306 | val_0_rmse: 7.9171  | val_1_rmse: 0.69298 |  0:10:43s
epoch 22 | loss: 0.30253 | val_0_rmse: 6.26802 | val_1_rmse: 0.90466 |  0:11:09s
epoch 23 | loss: 0.34048 | val_0_rmse: 20.41874| val_1_rmse: 0.759   |  0:11:34s
epoch 24 | loss: 0.31954 | val_0_rmse: 0.72022 | val_1_rmse: 0.71616 |  0:11:59s
epoch 25 | loss: 0.31078 | val_0_rmse: 20.10571| val_1_rmse: 0.78772 |  0:12:24s
epoch 26 | loss: 0.32358 | val_0_rmse: 34.34022| val_1_rmse: 0.70827 |  0:12:50s
epoch 27 | loss: 0.31383 | val_0_rmse: 102.08624| val_1_rmse: 0.7041  |  0:13:16s
epoch 28 | loss: 0.31451 | val_0_rmse: 0.76035 | val_1_rmse: 0.75446 |  0:13:41s
epoch 29 | loss: 0.31966 | val_0_rmse: 0.9333  | val_1_rmse: 0.93513 |  0:14:06s
epoch 30 | loss: 0.31884 | val_0_rmse: 0.73279 | val_1_rmse: 0.72935 |  0:14:32s
epoch 31 | loss: 0.31452 | val_0_rmse: 0.71646 | val_1_rmse: 0.71309 |  0:14:57s
epoch 32 | loss: 0.30832 | val_0_rmse: 24.69968| val_1_rmse: 1.07231 |  0:15:23s

Early stopping occured at epoch 32 with best_epoch = 2 and best_val_1_rmse = 0.62711
Best weights from best epoch are automatically used!
ended training at: 00:18:40
Feature importance:
[('Area', 0.2100525443781098), ('Baths', 0.10962234832416413), ('Beds', 0.04158234870625415), ('Latitude', 0.1288873578584256), ('Longitude', 0.0), ('Month', 0.07153135746065951), ('Year', 0.43832404327238683)]
Mean squared error is of 19225019359.485863
Mean absolute error:93445.31911071249
MAPE:0.6312657577560179
R2 score:0.6014467614583745
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 1
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:18:43
epoch 0  | loss: 0.46174 | val_0_rmse: 0.64457 | val_1_rmse: 0.64999 |  0:00:25s
epoch 1  | loss: 0.41348 | val_0_rmse: 0.65759 | val_1_rmse: 437.02378|  0:00:50s
epoch 2  | loss: 0.37906 | val_0_rmse: 0.67365 | val_1_rmse: 2073.32038|  0:01:16s
epoch 3  | loss: 0.36992 | val_0_rmse: 0.67412 | val_1_rmse: 3994.73576|  0:01:41s
epoch 4  | loss: 0.36374 | val_0_rmse: 0.75846 | val_1_rmse: 2478.1095|  0:02:07s
epoch 5  | loss: 0.35409 | val_0_rmse: 0.934   | val_1_rmse: 9040.85992|  0:02:32s
epoch 6  | loss: 0.36276 | val_0_rmse: 0.71384 | val_1_rmse: 2262.0342|  0:02:57s
epoch 7  | loss: 0.34653 | val_0_rmse: 1.26361 | val_1_rmse: 14702.1046|  0:03:23s
epoch 8  | loss: 0.35205 | val_0_rmse: 0.92053 | val_1_rmse: 9152.26471|  0:03:48s
epoch 9  | loss: 0.40747 | val_0_rmse: 0.62802 | val_1_rmse: 0.62862 |  0:04:13s
epoch 10 | loss: 0.39868 | val_0_rmse: 0.80072 | val_1_rmse: 12339.86529|  0:04:38s
epoch 11 | loss: 0.38146 | val_0_rmse: 1.18819 | val_1_rmse: 14683.3746|  0:05:04s
epoch 12 | loss: 0.37806 | val_0_rmse: 0.72989 | val_1_rmse: 3292.44945|  0:05:29s
epoch 13 | loss: 0.37039 | val_0_rmse: 0.76742 | val_1_rmse: 6530.41281|  0:05:54s
epoch 14 | loss: 0.36394 | val_0_rmse: 1.95803 | val_1_rmse: 26588.14568|  0:06:20s
epoch 15 | loss: 0.35941 | val_0_rmse: 0.80065 | val_1_rmse: 5983.68786|  0:06:45s
epoch 16 | loss: 0.35567 | val_0_rmse: 3.55211 | val_1_rmse: 50208.63252|  0:07:10s
epoch 17 | loss: 0.35612 | val_0_rmse: 0.691   | val_1_rmse: 847.60485|  0:07:35s
epoch 18 | loss: 0.35334 | val_0_rmse: 0.75615 | val_1_rmse: 4932.84579|  0:08:01s
epoch 19 | loss: 0.35346 | val_0_rmse: 1.41988 | val_1_rmse: 18062.56161|  0:08:26s
epoch 20 | loss: 0.35449 | val_0_rmse: 0.74432 | val_1_rmse: 4051.25004|  0:08:51s
epoch 21 | loss: 0.353   | val_0_rmse: 0.63626 | val_1_rmse: 712.61062|  0:09:16s
epoch 22 | loss: 0.35045 | val_0_rmse: 0.72213 | val_1_rmse: 6232.32261|  0:09:42s
epoch 23 | loss: 0.35894 | val_0_rmse: 0.63721 | val_1_rmse: 849.73302|  0:10:07s
epoch 24 | loss: 0.35598 | val_0_rmse: 0.67394 | val_1_rmse: 185.87105|  0:10:32s
epoch 25 | loss: 0.35464 | val_0_rmse: 1.81784 | val_1_rmse: 24360.18405|  0:10:57s
epoch 26 | loss: 0.35194 | val_0_rmse: 0.72371 | val_1_rmse: 4431.65848|  0:11:22s
epoch 27 | loss: 0.35619 | val_0_rmse: 0.6634  | val_1_rmse: 4560.24612|  0:11:48s
epoch 28 | loss: 0.34944 | val_0_rmse: 0.8428  | val_1_rmse: 7811.10838|  0:12:13s
epoch 29 | loss: 0.36122 | val_0_rmse: 0.72872 | val_1_rmse: 784.19952|  0:12:38s
epoch 30 | loss: 0.39591 | val_0_rmse: 0.68216 | val_1_rmse: 38618.78984|  0:13:03s
epoch 31 | loss: 0.38611 | val_0_rmse: 0.68307 | val_1_rmse: 3351.71523|  0:13:28s
epoch 32 | loss: 0.41353 | val_0_rmse: 1.26097 | val_1_rmse: 832.89633|  0:13:53s
epoch 33 | loss: 0.44166 | val_0_rmse: 0.65264 | val_1_rmse: 0.64938 |  0:14:19s
epoch 34 | loss: 0.41337 | val_0_rmse: 0.63378 | val_1_rmse: 806.59839|  0:14:45s
epoch 35 | loss: 0.39739 | val_0_rmse: 0.62458 | val_1_rmse: 908.80196|  0:15:13s
epoch 36 | loss: 0.39316 | val_0_rmse: 0.72688 | val_1_rmse: 4545.19624|  0:15:39s
epoch 37 | loss: 0.40659 | val_0_rmse: 0.70656 | val_1_rmse: 4146.56572|  0:16:04s
epoch 38 | loss: 0.39471 | val_0_rmse: 0.79606 | val_1_rmse: 6539.65908|  0:16:29s
epoch 39 | loss: 0.39105 | val_0_rmse: 0.96042 | val_1_rmse: 10649.71612|  0:16:54s

Early stopping occured at epoch 39 with best_epoch = 9 and best_val_1_rmse = 0.62862
Best weights from best epoch are automatically used!
ended training at: 00:35:45
Feature importance:
[('Area', 0.1613896795458691), ('Baths', 0.15144589436785597), ('Beds', 0.09531277873513258), ('Latitude', 0.10923755431158447), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.4826140930395579)]
Mean squared error is of 19204499485.8448
Mean absolute error:92879.74902119763
MAPE:0.619508269133832
R2 score:0.6031734805490516
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 2
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:35:48
epoch 0  | loss: 0.46448 | val_0_rmse: 0.64828 | val_1_rmse: 0.64965 |  0:00:25s
epoch 1  | loss: 0.41296 | val_0_rmse: 0.63597 | val_1_rmse: 7.6675  |  0:00:50s
epoch 2  | loss: 0.3981  | val_0_rmse: 0.66912 | val_1_rmse: 808.20947|  0:01:15s
epoch 3  | loss: 0.41438 | val_0_rmse: 0.76026 | val_1_rmse: 335.05314|  0:01:41s
epoch 4  | loss: 0.39228 | val_0_rmse: 0.64314 | val_1_rmse: 62.40631|  0:02:06s
epoch 5  | loss: 0.38049 | val_0_rmse: 0.71753 | val_1_rmse: 436.31037|  0:02:31s
epoch 6  | loss: 0.37742 | val_0_rmse: 0.65976 | val_1_rmse: 68.04568|  0:02:57s
epoch 7  | loss: 0.37366 | val_0_rmse: 0.65459 | val_1_rmse: 103.35826|  0:03:22s
epoch 8  | loss: 0.37242 | val_0_rmse: 0.6225  | val_1_rmse: 1.66227 |  0:03:47s
epoch 9  | loss: 0.37    | val_0_rmse: 0.7239  | val_1_rmse: 1308.32758|  0:04:13s
epoch 10 | loss: 0.36304 | val_0_rmse: 0.7116  | val_1_rmse: 0.68586 |  0:04:38s
epoch 11 | loss: 0.3839  | val_0_rmse: 0.65318 | val_1_rmse: 873.15719|  0:05:03s
epoch 12 | loss: 0.37349 | val_0_rmse: 0.59809 | val_1_rmse: 905.13503|  0:05:29s
epoch 13 | loss: 0.3663  | val_0_rmse: 4.11399 | val_1_rmse: 59416.88074|  0:05:54s
epoch 14 | loss: 0.37576 | val_0_rmse: 0.64721 | val_1_rmse: 1457.23418|  0:06:19s
epoch 15 | loss: 0.40308 | val_0_rmse: 0.687   | val_1_rmse: 2395.55526|  0:06:45s
epoch 16 | loss: 0.38634 | val_0_rmse: 0.6723  | val_1_rmse: 1797.2016|  0:07:10s
epoch 17 | loss: 0.37404 | val_0_rmse: 0.65482 | val_1_rmse: 773.06719|  0:07:35s
epoch 18 | loss: 0.36442 | val_0_rmse: 0.65902 | val_1_rmse: 39.80346|  0:08:01s
epoch 19 | loss: 0.36386 | val_0_rmse: 3.22129 | val_1_rmse: 45711.02428|  0:08:26s
epoch 20 | loss: 0.3781  | val_0_rmse: 0.81772 | val_1_rmse: 6505.78611|  0:08:51s
epoch 21 | loss: 0.38067 | val_0_rmse: 0.91789 | val_1_rmse: 1644.95806|  0:09:17s
epoch 22 | loss: 0.36815 | val_0_rmse: 0.70351 | val_1_rmse: 3274.0979|  0:09:42s
epoch 23 | loss: 0.36944 | val_0_rmse: 0.67909 | val_1_rmse: 1280.72976|  0:10:07s
epoch 24 | loss: 0.39148 | val_0_rmse: 0.82244 | val_1_rmse: 1237.33698|  0:10:33s
epoch 25 | loss: 0.38855 | val_0_rmse: 0.69025 | val_1_rmse: 2805.93423|  0:10:58s
epoch 26 | loss: 0.39338 | val_0_rmse: 0.68055 | val_1_rmse: 4099.8187|  0:11:23s
epoch 27 | loss: 0.39396 | val_0_rmse: 0.73883 | val_1_rmse: 5442.0809|  0:11:49s
epoch 28 | loss: 0.39289 | val_0_rmse: 0.66283 | val_1_rmse: 914.10419|  0:12:14s
epoch 29 | loss: 0.39543 | val_0_rmse: 0.65604 | val_1_rmse: 547.69862|  0:12:40s
epoch 30 | loss: 0.39321 | val_0_rmse: 0.81916 | val_1_rmse: 337.2859|  0:13:05s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 0.64965
Best weights from best epoch are automatically used!
ended training at: 00:49:01
Feature importance:
[('Area', 0.11654851407683312), ('Baths', 0.2192454486296281), ('Beds', 0.14087385558598586), ('Latitude', 0.15545669160618913), ('Longitude', 0.022654433286432435), ('Month', 0.02829280005832207), ('Year', 0.3169282567566093)]
Mean squared error is of 20095448419.61355
Mean absolute error:94680.18064609625
MAPE:0.7128221523639174
R2 score:0.5834183998761302
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 3
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 00:49:05
epoch 0  | loss: 0.46437 | val_0_rmse: 0.64324 | val_1_rmse: 0.64262 |  0:00:25s
epoch 1  | loss: 0.4197  | val_0_rmse: 10.02495| val_1_rmse: 0.6676  |  0:00:50s
epoch 2  | loss: 0.3921  | val_0_rmse: 4.48801 | val_1_rmse: 0.76914 |  0:01:15s
epoch 3  | loss: 0.38029 | val_0_rmse: 0.85859 | val_1_rmse: 0.78437 |  0:01:41s
epoch 4  | loss: 0.37434 | val_0_rmse: 0.79758 | val_1_rmse: 0.69308 |  0:02:06s
epoch 5  | loss: 0.37387 | val_0_rmse: 0.78767 | val_1_rmse: 0.66814 |  0:02:31s
epoch 6  | loss: 0.36974 | val_0_rmse: 2.68123 | val_1_rmse: 0.73464 |  0:02:57s
epoch 7  | loss: 0.38917 | val_0_rmse: 0.85094 | val_1_rmse: 0.81417 |  0:03:22s
epoch 8  | loss: 0.40735 | val_0_rmse: 3.10227 | val_1_rmse: 0.68679 |  0:03:49s
epoch 9  | loss: 0.42969 | val_0_rmse: 0.80507 | val_1_rmse: 0.80595 |  0:04:15s
epoch 10 | loss: 0.403   | val_0_rmse: 1.67983 | val_1_rmse: 0.68335 |  0:04:40s
epoch 11 | loss: 0.39226 | val_0_rmse: 7.56148 | val_1_rmse: 0.70843 |  0:05:06s
epoch 12 | loss: 0.41096 | val_0_rmse: 2.90799 | val_1_rmse: 0.6841  |  0:05:31s
epoch 13 | loss: 0.40044 | val_0_rmse: 2.5129  | val_1_rmse: 0.89212 |  0:05:56s
epoch 14 | loss: 0.39787 | val_0_rmse: 13.07119| val_1_rmse: 0.78614 |  0:06:22s
epoch 15 | loss: 0.39041 | val_0_rmse: 0.99466 | val_1_rmse: 0.70269 |  0:06:47s
epoch 16 | loss: 0.40057 | val_0_rmse: 0.72713 | val_1_rmse: 0.72694 |  0:07:13s
epoch 17 | loss: 0.40056 | val_0_rmse: 0.71052 | val_1_rmse: 0.7116  |  0:07:38s
epoch 18 | loss: 0.39605 | val_0_rmse: 0.80224 | val_1_rmse: 0.80243 |  0:08:03s
epoch 19 | loss: 0.39479 | val_0_rmse: 27.72499| val_1_rmse: 0.72251 |  0:08:29s
epoch 20 | loss: 0.39448 | val_0_rmse: 25.14872| val_1_rmse: 0.7291  |  0:08:54s
epoch 21 | loss: 0.37235 | val_0_rmse: 17.5391 | val_1_rmse: 0.87857 |  0:09:20s
epoch 22 | loss: 0.36406 | val_0_rmse: 41.62185| val_1_rmse: 0.70759 |  0:09:45s
epoch 23 | loss: 0.35776 | val_0_rmse: 0.79514 | val_1_rmse: 0.76364 |  0:10:10s
epoch 24 | loss: 0.36089 | val_0_rmse: 1.16661 | val_1_rmse: 0.78217 |  0:10:36s
epoch 25 | loss: 0.35262 | val_0_rmse: 2.05247 | val_1_rmse: 0.77087 |  0:11:01s
epoch 26 | loss: 0.34914 | val_0_rmse: 1.40458 | val_1_rmse: 0.85941 |  0:11:26s
epoch 27 | loss: 0.34767 | val_0_rmse: 19.78752| val_1_rmse: 0.68148 |  0:11:52s
epoch 28 | loss: 0.3494  | val_0_rmse: 36.74791| val_1_rmse: 0.83642 |  0:12:18s
epoch 29 | loss: 0.34674 | val_0_rmse: 7.66452 | val_1_rmse: 0.85433 |  0:12:43s
epoch 30 | loss: 0.35227 | val_0_rmse: 4.34095 | val_1_rmse: 0.70323 |  0:13:08s

Early stopping occured at epoch 30 with best_epoch = 0 and best_val_1_rmse = 0.64262
Best weights from best epoch are automatically used!
ended training at: 01:02:21
Feature importance:
[('Area', 0.17718442439119994), ('Baths', 0.053487942244718534), ('Beds', 0.047609249977435764), ('Latitude', 0.11455232488812213), ('Longitude', 0.03400484461631996), ('Month', 0.0), ('Year', 0.5731612138822036)]
Mean squared error is of 20357401808.316994
Mean absolute error:97756.7053272695
MAPE:0.7845265106089468
R2 score:0.5867205715656515
------------------------------------------------------------------
------------------------------------------------------------------
Using the dataset: all_datasets
Using seed = 4
Train/val/test division is 64/18/18
Device used : cuda
Random Forest params:

{'cat_dims': [], 'cat_emb_dim': 1, 'cat_idxs': [], 'clip_value': 1, 'device_name': 'auto', 'epsilon': 1e-15, 'gamma': 1.3, 'input_dim': None, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'momentum': 0.02, 'n_a': 12, 'n_d': 12, 'n_independent': 2, 'n_shared': 2, 'n_steps': 3, 'optimizer_fn': <class 'torch.optim.adam.Adam'>, 'optimizer_params': {'lr': 0.05}, 'output_dim': None, 'scheduler_fn': None, 'scheduler_params': {}, 'seed': 0, 'verbose': 1}
started training at: 01:02:24
epoch 0  | loss: 0.47343 | val_0_rmse: 0.66239 | val_1_rmse: 0.66194 |  0:00:25s
epoch 1  | loss: 0.41434 | val_0_rmse: 0.69022 | val_1_rmse: 0.68897 |  0:00:50s
epoch 2  | loss: 0.41302 | val_0_rmse: 0.7238  | val_1_rmse: 0.72443 |  0:01:16s
epoch 3  | loss: 0.37229 | val_0_rmse: 0.86095 | val_1_rmse: 0.86137 |  0:01:41s
epoch 4  | loss: 0.34673 | val_0_rmse: 0.73919 | val_1_rmse: 0.73718 |  0:02:06s
epoch 5  | loss: 0.33368 | val_0_rmse: 0.83106 | val_1_rmse: 0.82986 |  0:02:32s
epoch 6  | loss: 0.32517 | val_0_rmse: 0.80946 | val_1_rmse: 0.80918 |  0:02:57s
epoch 7  | loss: 0.31903 | val_0_rmse: 0.70019 | val_1_rmse: 0.69876 |  0:03:22s
epoch 8  | loss: 0.31695 | val_0_rmse: 0.74682 | val_1_rmse: 0.74546 |  0:03:48s
epoch 9  | loss: 0.31829 | val_0_rmse: 0.68103 | val_1_rmse: 0.67941 |  0:04:13s
epoch 10 | loss: 0.31194 | val_0_rmse: 0.73408 | val_1_rmse: 0.73204 |  0:04:39s
epoch 11 | loss: 0.31104 | val_0_rmse: 0.71468 | val_1_rmse: 0.71178 |  0:05:04s
epoch 12 | loss: 0.30976 | val_0_rmse: 0.67852 | val_1_rmse: 0.67545 |  0:05:29s
epoch 13 | loss: 0.31034 | val_0_rmse: 0.75463 | val_1_rmse: 0.75365 |  0:05:55s
epoch 14 | loss: 0.30704 | val_0_rmse: 1.01191 | val_1_rmse: 1.01272 |  0:06:20s
epoch 15 | loss: 0.30696 | val_0_rmse: 162.85119| val_1_rmse: 0.64867 |  0:06:45s
epoch 16 | loss: 0.30741 | val_0_rmse: 0.7046  | val_1_rmse: 0.70244 |  0:07:11s
epoch 17 | loss: 0.30512 | val_0_rmse: 0.66872 | val_1_rmse: 0.6669  |  0:07:36s
epoch 18 | loss: 0.30322 | val_0_rmse: 0.89741 | val_1_rmse: 0.90034 |  0:08:01s
epoch 19 | loss: 0.31592 | val_0_rmse: 0.69428 | val_1_rmse: 0.69261 |  0:08:27s
epoch 20 | loss: 0.31096 | val_0_rmse: 0.79526 | val_1_rmse: 0.79269 |  0:08:52s
epoch 21 | loss: 0.3141  | val_0_rmse: 0.91965 | val_1_rmse: 0.92024 |  0:09:18s
epoch 22 | loss: 0.30953 | val_0_rmse: 0.67507 | val_1_rmse: 0.67478 |  0:09:45s
epoch 23 | loss: 0.30382 | val_0_rmse: 0.66564 | val_1_rmse: 0.66503 |  0:10:11s
epoch 24 | loss: 0.30231 | val_0_rmse: 0.73192 | val_1_rmse: 0.7318  |  0:10:37s
epoch 25 | loss: 0.30155 | val_0_rmse: 0.7271  | val_1_rmse: 0.72659 |  0:11:02s
epoch 26 | loss: 0.30604 | val_0_rmse: 0.68962 | val_1_rmse: 0.68804 |  0:11:27s
epoch 27 | loss: 0.30413 | val_0_rmse: 1.42918 | val_1_rmse: 1.43328 |  0:11:53s
epoch 28 | loss: 0.30532 | val_0_rmse: 0.74033 | val_1_rmse: 0.74071 |  0:12:18s
epoch 29 | loss: 0.3047  | val_0_rmse: 0.68931 | val_1_rmse: 24.47846|  0:12:44s
epoch 30 | loss: 0.3028  | val_0_rmse: 0.69111 | val_1_rmse: 0.68958 |  0:13:09s
epoch 31 | loss: 0.29835 | val_0_rmse: 0.68694 | val_1_rmse: 0.68739 |  0:13:34s
epoch 32 | loss: 0.29784 | val_0_rmse: 0.74375 | val_1_rmse: 0.74244 |  0:14:00s
epoch 33 | loss: 0.29795 | val_0_rmse: 1.29595 | val_1_rmse: 1.29978 |  0:14:25s
epoch 34 | loss: 0.29626 | val_0_rmse: 0.80444 | val_1_rmse: 0.80225 |  0:14:51s
epoch 35 | loss: 0.29539 | val_0_rmse: 0.71306 | val_1_rmse: 0.71162 |  0:15:16s
epoch 36 | loss: 0.29491 | val_0_rmse: 0.79702 | val_1_rmse: 0.79554 |  0:15:41s
epoch 37 | loss: 0.29341 | val_0_rmse: 0.68965 | val_1_rmse: 0.68826 |  0:16:07s
epoch 38 | loss: 0.29558 | val_0_rmse: 0.70296 | val_1_rmse: 0.70087 |  0:16:32s
epoch 39 | loss: 0.29564 | val_0_rmse: 0.74601 | val_1_rmse: 0.74397 |  0:16:58s
epoch 40 | loss: 0.29329 | val_0_rmse: 0.80357 | val_1_rmse: 0.80258 |  0:17:23s
epoch 41 | loss: 0.29375 | val_0_rmse: 0.70571 | val_1_rmse: 0.70614 |  0:17:49s
epoch 42 | loss: 0.29307 | val_0_rmse: 0.71407 | val_1_rmse: 98.9086 |  0:18:14s
epoch 43 | loss: 0.29199 | val_0_rmse: 0.70562 | val_1_rmse: 0.70383 |  0:18:40s
epoch 44 | loss: 0.29666 | val_0_rmse: 0.67793 | val_1_rmse: 0.678   |  0:19:05s
epoch 45 | loss: 0.29729 | val_0_rmse: 0.81934 | val_1_rmse: 0.82133 |  0:19:30s

Early stopping occured at epoch 45 with best_epoch = 15 and best_val_1_rmse = 0.64867
Best weights from best epoch are automatically used!
ended training at: 01:22:02
Feature importance:
[('Area', 0.2647795338863667), ('Baths', 0.08737490035576519), ('Beds', 0.2667463333794764), ('Latitude', 0.2642355557104564), ('Longitude', 0.0), ('Month', 0.0), ('Year', 0.11686367666793533)]
Mean squared error is of 20413385851.335632
Mean absolute error:98307.50964246038
MAPE:0.7633665142795157
R2 score:0.5792949604915597
------------------------------------------------------------------
